Emergence, Complexity and Computation ECC
ISCS 2014: 
Interdisciplinary 
Symposium on 
Complex Systems
Ali Sanayei
Otto E. Rössler
Ivan Zelinka   Editors

Emergence, Complexity and Computation
Volume 14
Series editors
Ivan Zelinka, Technical University of Ostrava, Ostrava, Czech Republic
e-mail: ivan.zelinka@vsb.cz
Andrew Adamatzky, University of the West of England, Bristol, United Kingdom
e-mail: gchen@ee.cityu.edu.hk
Guanrong Chen, City University of Hong Kong, Hong Kong
e-mail: andrew.adamatzky@uwe.ac.uk
Editorial Board
Ajith Abraham, MirLabs, USA
Ana Lucia C. Bazzan, Universidade Federal do Rio Grande do Sul, Porto Alegre
RS Brasil
Juan C. Burguillo, University of Vigo, Spain
Sergej ˇCelikovský, Academy of Sciences of the Czech Republic, Czech Republic
Mohammed Chadli, University of Jules Verne, France
Emilio Corchado, University of Salamanca, Spain
Donald Davendra, Technical University of Ostrava, Czech Republic
Andrew Ilachinski, Center for Naval Analyses, USA
Jouni Lampinen, University of Vaasa, Finland
Martin Middendorf, University of Leipzig, Germany
Edward Ott, University of Maryland, USA
Linqiang Pan, Huazhong University of Science and Technology, Wuhan, China
Gheorghe P˘aun, Romanian Academy, Bucharest, Romania
Hendrik Richter, HTWK Leipzig University of Applied Sciences, Germany
Juan A. Rodriguez-Aguilar, IIIA-CSIC, Spain
Otto Rössler, Institute of Physical and Theoretical Chemistry, Tübingen, Germany
Vaclav Snasel, Technical University of Ostrava, Czech Republic
Ivo Vondrák, Technical University of Ostrava, Czech Republic
Hector Zenil, Karolinska Institute, Sweden

About this Series
The Emergence, Complexity and Computation (ECC) series publishes new develop-
ments, advancements and selected topics in the ﬁelds of complexity, computation and
emergence. The series focuses on all aspects of reality-based computation approaches
from an interdisciplinary point of view especially from applied sciences, biology,
physics, or Chemistry. It presents new ideas and interdisciplinary insight on the mu-
tual intersection of subareas of computation, complexity and emergence and its impact
and limits to any computing based on physical limits (thermodynamic and quantum
limits, Bremermann’s limit, Seth Lloyd limits...) as well as algorithmic limits (Gödel’s
proof and its impact on calculation, algorithmic complexity, the Chaitin’s Omega num-
ber and Kolmogorov complexity, non-traditional calculations like Turing machine pro-
cess and its consequences,...) and limitations arising in artiﬁcial intelligence ﬁeld. The
topics are (but not limited to) membrane computing, DNA computing, immune comput-
ing, quantum computing, swarm computing, analogic computing, chaos computing and
computing on the edge of chaos, computational aspects of dynamics of complex sys-
tems (systems with self-organization, multiagent systems, cellular automata, artiﬁcial
life,...), emergence of complex systems and its computational aspects, and agent based
computation. The main aim of this series it to discuss the above mentioned topics from
an interdisciplinary point of view and present new ideas coming from mutual intersec-
tion of classical as well as modern methods of computation. Within the scope of the
series are monographs, lecture notes, selected contributions from specialized confer-
ences and workshops, special contribution from international experts. More information
about this series at http://www.springer.com/series/10624

Ali Sanayei · Otto E. Rössler
Ivan Zelinka
Editors
ISCS 2014:
Interdisciplinary
Symposium on
Complex Systems
ABC

Editors
Ali Sanayei
Institute for Theoretical Physics
Tübingen
Germany
Otto E. Rössler
Institute for Physical and Theoretical
Chemistry
University of Tübingen
Tübingen
Germany
Ivan Zelinka
Department of Computer Science
Faculty of Elect. Eng. and Comp.Sci.
Ostrava-Poruba
Czech Republic
ISSN 2194-7287
ISSN 2194-7295
(electronic)
ISBN 978-3-319-10758-5
ISBN 978-3-319-10759-2
(eBook)
DOI 10.1007/978-3-319-10759-2
Library of Congress Control Number: 2014949171
Springer Cham Heidelberg New York Dordrecht London
c⃝Springer International Publishing Switzerland 2014
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broad-
casting, reproduction on microﬁlms or in any other physical way, and transmission or information storage
and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known
or hereafter developed. Exempted from this legal reservation are brief excerpts in connection with reviews
or scholarly analysis or material supplied speciﬁcally for the purpose of being entered and executed on a
computer system, for exclusive use by the purchaser of the work. Duplication of this publication or parts
thereof is permitted only under the provisions of the Copyright Law of the Publisher’s location, in its cur-
rent version, and permission for use must always be obtained from Springer. Permissions for use may be
obtained through RightsLink at the Copyright Clearance Center. Violations are liable to prosecution under
the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of publication,
neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or
omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material
contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface
The present book is the upshot of the “2014 Interdisciplinary Symposium on Complex
Systems,” held at the “Athens of the Middle Ages” as a continuation of our symposia
series. Florence, birthplace of the Italian Renaissance, has a unique genius loci to it.
It sufﬁces to point to the eminent physicist who was buried there after playing here
his pivotal role in the modern scientiﬁc revolution. He was a “Sire” in the word of his
faithful daughter Virginia (Sister Maria Celste) and was “the father of modern physics
– indeed of modern science altogether” in Einstein’s words. When Galileo Galilei was
68 years old he wrote the epoch making book “A Dialogue Concerning the Two Chief
World Systems: Ptolemaic and Copernican.” Because of this he found himself “vehe-
mently suspected of heresy” and got forced down on his knees to eat his words and was
sentenced to permanent house arrest. He can also be seen in the tradition of an earlier
more eastern school, Rumi’s, which held that “there is no limit to thinking and there is
a world of freedom inside every person.”
In accordance with Galilei, the main aim of the symposium was to bring together
many diverse points of view by scientists working on complex systems in a both di-
vergent and convergent manner. The reader – student or professional scientist – will
encounter four general categories of papers: Physical modeling of complex systems,
Evolutionary computations, Complex biological systems, and Complex networks. The
presentations comprise innovative ideas, philosophical overviews or state-of-the-art ap-
plications in miscellaneous ﬁelds.
At this point, we would like to express our special gratitude to our participants, our
program committee, the keynote speakers (Yuri Manin, Rudolf Kalman, Karoline Wies-
ner and Antonio Politi), the University of Florence and the Galileo Galilei Institute for
Theoretical Physics who helped us a lot to continue our symposium series in an efﬁcient
way.
The special event of this year’s symposium was a “Galilei-Turing Round-table” held
among our keynote speakers, accompanied by John Symons, and chaired by Barry
Cooper. The round-table was devoted to diverse open problems in the physics and
computation of complex systems. We accordingly dedicate the book to the memory of

VI
Preface
Alan Turing who believed that “mathematical reasoning may be regarded rather
schematically as the exercise of a combination of two facilities which we may call
intuition and ingenuity.”
July 2014
Ali Sanayei
Ivan Zelinka
Otto E. Rössler

Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
V
Part I: Physical Modeling of Complex Systems
Physics in the World of Ideas: Complexity as Energy . . . . . . . . . . . . . . . . . . . .
3
Yuri I. Manin
Complexity Measures and Physical Principles. . . . . . . . . . . . . . . . . . . . . . . . . .
15
Karoline Wiesner
Collective Dynamics in Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
Antonio Politi
Transient Sequences in a Network of Excitatory Coupled Morris-Lecar
Neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
Dmitry V. Kasatkin, Aleksey Dmitrichev, Vladimir I. Nekorkin
The Development of Chemical Artiﬁcial Intelligence Processing Fuzzy
Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
Pier Luigi Gentili
Weak Sensitivity to Initial Conditions for Generating Temporal Patterns
in Recurrent Neural Networks: A Reservoir Computing Approach . . . . . . . .
47
Hiromichi Suetani
Synchronization and Control in Modular Networks of Spiking Neurons . . . .
57
Oleg V. Maslennikov, Dmitry V. Kasatkin, Vladimir I. Nekorkin
Birth-Death Models of Information Spread in Structured Populations . . . . .
67
Burton Voorhees

VIII
Contents
Coexistence of Deterministic and Stochastic Bistability in a 1-D
Birth-Death Process with Hill Type Nonlinear Birth Rates . . . . . . . . . . . . . . .
77
Neslihan Avcu, Nihal Pekergin, Ferhan Pekergin, Güleser Kalaycı Demir,
Cüneyt Güzeli¸s
Intellectualized Home Environment as a Complex System. . . . . . . . . . . . . . . .
87
Raimundas Jasinevicius, Egidijus Kazanavicius, Vytautas Petrauskas
Desertiﬁcation Transition in Semi-arid Ecosystems and Directed
Percolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
Raffaele Corrado, Anna Maria Cherubini, Cecilia Pennetta
Lognormality Observed for Additive Processes: Application to
Turbulence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
Hideaki Mouri
The Prediction of Tropospheric Ozone Using a Radial Basis Function
Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
Kˇríž Radko, Šedek Pavel
Physical Experiments and Stochastic Modeling to Clarify the System
Containing the Seismic Source and the Ground . . . . . . . . . . . . . . . . . . . . . . . .
125
Alexander V. Smaglichenko, Maria K. Sayankina, Tatyana A. Smaglichenko,
Igor A. Volodin
Cosmology 2.0: Convergent Implication of Cryodynamics and Global-c
General Relativity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
Otto E. Rossler
Hidden Quantum Markov Models and Open Quantum Systems with
Instantaneous Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
Lewis A. Clark, Wei Huang, Thomas M. Barlow, Almut Beige
An Efﬁcient Strategy to Handle Complex Datasets Having Multimodal
Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
Samira Ghodratnama, Reza Boostani
Maximum Likelihood Estimation and Integration Algorithm for
Modeling Complex Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
Yoshinao Shiraki
On Dynamics of an Electromechanical System Supported by Cylindrical
Helical Spring Damped by an Impact Damper . . . . . . . . . . . . . . . . . . . . . . . . .
173
Marek Lampart, Jaroslav Zapomˇel
Petri Net Models of Purposeful Complex Dynamic Systems . . . . . . . . . . . . . .
183
Felipe Lara-Rosano

Contents
IX
Part II: Evolutionary Computations
Does Evolutionary Dynamics Need Randomness, Complexity or
Determinism? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
Ivan Zelinka, Roman Senkerik
A Brief Survey on the Chaotic Systems as the Pseudo Random Number
Generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
Roman Senkerik, Michal Pluhacek, Ivan Zelinka, Donald Davendra,
Zuzana Kominkova Oplatkova
Emergent Behaviors on Coevolutionary Networks of Chaotic Dynamical
Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
A. Anzo, J.G. Barajas-Ramírez
Chaos Driven PSO – On the Inﬂuence of Various CPRNG
Implementations – An Initial Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
Michal Pluhacek, Roman Senkerik, Ivan Zelinka, Donald Davendra
Evolutionary Based ARIMA Models for Stock Price Forecasting . . . . . . . . . .
239
Tomas Vantuch, Ivan Zelinka
On Some False Chaos Indicators When Analyzing Sampled Data . . . . . . . . .
249
Petra Augustová, Zdenˇek Beran, Sergej ˇCelikovský
Part III: Complex Biological Systems
Multifractality in Imaging: Application of Information Entropy for
Observation of Inner Dynamics Inside of an Unlabeled Living Cell in
Bright-Field Microscopy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
Renata Rychtarikova, Tomas Nahlik, Rebecca Smaha, Jan Urban,
Dalibor Stys Jr., Petr Cisar, Dalibor Stys
Trajectory Tracking for Genetic Networks Using Control Theory . . . . . . . . .
269
Natalja Strelkowa
Modeling and Optimization of Microalgae Growth in Photobioreactors:
A Multidisciplinary Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
Štˇepán Papáˇcek, Jiˇri Jablonský, Karel Petera, Branislav Rehák,
Ctirad Matonoha
Digital Processing of Toxoplasma gondii Cysts in Meat Samples for
Human Consumption in Colombia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
Quiñones Armando, Juez-C Graciela
Modeling of Tumour Growth Induced by Circadian Rhythm Disruption
in Epithelial Tissue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
Dmitry Bratsun, Andrey Zakharov, Len Pismen

X
Contents
Part IV: Complex Networks
Comparing Overlapping Properties of Real Bipartite Networks . . . . . . . . . . .
309
Fabien Tarissan
Risk Perception and Epidemic Spreading in Multiplex Networks . . . . . . . . . .
319
Franco Bagnoli, Emanuele Massaro
Applications of Multifractal Diffusion Entropy Analysis to Daily and
Intraday Financial Time Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
Petr Jizba, Jan Korbel
An Economic Approach to the Evolution of an Ecology . . . . . . . . . . . . . . . . . .
343
Doug McLeod
Foraging Multi-Agent System Simulation Based on Attachment Theory . . . .
359
Carlos Alberto Riveros Varela, Ferney Beltrán Velandia,
Miguel Alberto Melgarejo Rey, Nadya González Romero,
Nelson Obregón Neira
A Spatial Model for Infrastructure Network Generation and Evolution . . . .
365
Gaihua Fu, Sean Wilkinson, Richard Dawson
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Part I 
 
Physical Modeling of Complex Systems 
 
 
 
 
 
 
 
 
 
 
 
 

Physics in the World of Ideas:
Complexity as Energy⋆
Yuri I. Manin
Max–Planck–Institut f¨ur Mathematik, Bonn, Germany
Introduction: Two Faces of Computation
Since the origins of civilisation, computation methods adopted in various cultures
could be roughly subdivided into two types.
One relied upon a system of notation, initially only for numbers, and rules of
performing arithmetical operations on these notations. I will refer to this type as
the linguistic one. Gradually it developed into algebra where generic or speciﬁc
names could be ascribed to other mathematical objects, and then to operations
on them as well.
The second type of computation methods stressed their manipulation with
objects, such as arithmetical computations using abacus, Chinese counting sticks,
arithmometer, slide rule, geometric measurements etc.
The discovery of the abstract theory of computability in the 1930’s followed
this pattern as well: Alonzo Church and Andrey Markov used linguistic models
whereas Alan Turing introduced his celebrated “machine”. In the powerful image
of Turing machine a computation is “a physical process performing linguistic
tasks”. Finally, the opposition software/hardware in applied computer science is
the contemporary avatar of this duality.
Introduction of entropy in the information theory by C. Shannon in 1948 was
probably the key moment in the development of “physics of the world of ideas”.
However, at least before 1980’s, theory of computability did not really use
conceptual richness of theoretical physics: principle of minimal action, conserva-
tion laws, basic symmetry groups of fundamental physical models. For example,
space–time of a Turing machine is “Galilean” one: space dimension is stretched
along the tape whereas time dimension is embodied in the sequence of states
and positions of the head.
With advent of the idea of quantum computing, the necessity of bridging
world of physics and Platonic world of (un)computable became urgent. There
were many attempts to do this: see a lively survey [A1], and [Man4], [Man5]
motivated by path integration in quantum ﬁeld theory.
However, most of these attempts were dedicated to “mental engineering”:
attempts to devise quantum (or mixed quantum/classical, cf. [Man3]) gadgets
capable to solve, say, NP–complete problems in polynomial time. S. Aaronson in
⋆Keynote talk at the 2014 Interdisciplinary Symposium on Complex Systems
(ISCS’14), University of Florence, September 15–18, 2014.
c
⃝Springer International Publishing Switzerland 2015
3
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_1

4
Y.I. Manin
[A1] eloquently puts forward the idea that P ̸= NP principle might be elevated
to the status of an important law of physics, cf. also [A2].
In my work of the last years, I was trying to develop the beautiful idea of
Andrey Kolmogorov who connected the physical idea of “randomness” of, say,
a ﬁnite sequence of bits (“string”) w, with the mathematical idea of “incom-
pessibility” of this string. Incompressibility here means impossibility to get w as
the output of a Kolmogorov optimal Turing machine taking as input a consider-
ably shorter than w sequence of bits, whereas “randomness” means absence of
all nontrivial regularities or correlations. See [ZvLe] for an early development of
this idea.
As I will explain in this talk, the Kolmogorov complexity of a combinatorial
object (and more generally, of a semicomputable function) has strong similar-
ities to the physical notion of energy level of an isolated system. I will show
that partition functions and quantum evolution operators involving Kolmogorov
complexity throw new light upon such problems as computation of asymptoti-
cal bounds of error–correcting codes ([VlaNoTsfa], [ManMar1], [ManMar3]), and
origins of Zipf’s law ([Zi1], [Zi2], [Mand], [Ma1], [Ma2]).
1
Complexity as Energy
1.1
Energy, Partition Function and Hamiltonian Evolution
Here I brieﬂy remind two basic contexts in which the notion of energy is used in
theoretical physics.
(i) In statistical thermodynamics, we imagine that a system can occupy an
enumerable set of states, with energy En at the state number n = 1, 2, 3, . . .. Its
interaction with environment is described via thermal contact; the temperature
of the system is denoted T . Put β := −1/kT where k is the Boltzman constant.
The partition function of the system
Z :=

n
e−βEn
(1.1)
deﬁnes the probability pn := e−βEn/Z for the system to occupy the n–the state.
Using this probability distribution, one calculates mean values of various observ-
ables as functions of temperature, volume etc. Singularities of such mean values
determine phase transitions.
(ii) In quantum mechanics, (En) appear as eigenvalues of the Hamiltonian
operator H acting upon a Hilbert space with orthonormal basis ( |n⟩) of eigen-
vectors of H. The Hamiltonian deﬁnes the time evolution which is given by the
operator eiHt. Formally replacing in H time t by the imaginary inverse temper-
ature i/kT , we get an interpretation of the partition function (1.1) as “trace of
of the quantum evolution operator in imaginary time”.
In the recent paper [ManMar2], we suggested that this formal substitution
may have a physical meaning in the “thermodynamics of the Universe”, where
one of the versions of global cosmological time is given by the inverse temperature

Physics in the World of Ideas: Complexity as Energy
5
of the cosmic microwave background (CMB) radiation, and transition of time
(or temperature) to the imaginary axis may describe the statistical physics of
the Big Bang.
Below we argue that the values of Kolmogorov complexity in the computabil-
ity theory can be fruitfully interpreted as such “energy levels” En.
1.2
Constructive Worlds
Markov algorithms process ﬁnite strings in a given ﬁnite alphabet. Turing ma-
chines do the same, although usually the respective alphabet consists of bits
{0, 1}. Church’s lambda–calculus processes certain syntactically correct ﬁnite
strings. Partial recursive functions are certain “(semi)computable” partial func-
tions f : D(f) →Zn
+, with deﬁnition domains D(f) ⊂Zm
+ . Positive integers Z+
here are not supposed to be encoded in any speciﬁc way.
Proofs that any two of these (and other) methods calculate “the same” class
of partial functions can be reduced to the following general scheme.
Deﬁne an (inﬁnite) constructive world as a countable set X (usually of some
ﬁnite Bourbaki structures, such as the set of all error–correcting codes in a
ﬁxed alphabet) given together with a set of structural numberings: bijections
ν : Z+ →X. This set must have the following formal property: for any two
numberings ν1, ν2 there exists a total recursive permutation σ : Z+ →Z+ such
that ν2 = ν1 ◦σ, and conversely, if ν is structural numbering, all ν ◦σ are also
such numberings.
Informally, structural numberings must be algorithmically computable, to-
gether with their inverses.
A ﬁnite constructive world is any ﬁnite set.
Categorical Church–Turing thesis, Part I. Let X, Y be two inﬁnite construc-
tive worlds, νX : Z+ →X νY : Z+ →X their structural numberings, and F
an (intuitive) algorithm that takes as input an object x ∈X and produces an
object F(x) ∈Y whenever x lies in the domain of deﬁnition of F; otherwise it
outputs “NO” or works indeﬁnitely.
Conversely, any algorithmically (semi) computable partial map F : X →Y
is induced by a partial recursive function f in this way.
Then f := ν−1
Y
◦F ◦νX : Z+ →Z+ is a partial recursive function.
Categorical Church–Turing thesis, Part II. Let C be a category, whose objects
are some inﬁnite constructive worlds, and some ﬁnite constructive worlds of all
ﬁnite cardinalities. Deﬁne the set of morphisms C(X, Y ) to be the set of all
partial maps that can be algorithmically (semi)computed in an intuitive sense.
Then C is equivalent to the category having one inﬁnite object Z, one ﬁnite ob-
ject {1, . . . , n} of each cardinality, and partial recursive functions as morphisms.
If X is ﬁnite, then C(X, Y ) consists of all partial maps.
One must keep in mind, that if X is inﬁnite, then C(X, Y ) has no natural
structure of a constructive world. In particular, various numberings of partial
recursive functions discussed in the literature are not structural numberings in
our sense, they enumerate “programs” rather than functions themselves, and the

6
Y.I. Manin
binary relations “two programs calculate one and the same partial function” is
undecidable.
1.3
Kolmogorov Complexity of Constructive Objects
Consider now an inﬁnite a constructive world. For any (semi)–computable func-
tion u : Z+ →X, the (exponential) complexity of an object x ∈X relative to u
is
Ku(x) := min {m ∈Z+ | u(m) = x}.
If such m does not exist, we put Ku(x) = ∞.
Claim: there exists such u (“an optimal Kolmogorov numbering”, or “decom-
pressor”) that for each other v : Z+ →X, some constant cu,v > 0, and all
x ∈X,
Ku(x) ≤cu,vKv(x).
This Ku(x) is called Kolmogorov complexity of x.
A Kolmogorov order of a constructive world X is a bijection K = Ku : X →Z
arranging elements of X in the increasing order of their complexities Ku.
Notice that any optimal numbering is only partial function, and its deﬁnition
domain is not decidable. Moreover, the Kolmogorov complexity Ku itself is not
computable: it is the lower bound of a sequence of computable functions.
The same can be said about the Kolmogorov order. Moreover, on Z+ it car-
dinally diﬀers from the natural order in the following sense: it puts in the initial
segments very large numbers that can be at the same time Kolmogorov simple.
For example, let an := nn...n
(n times). Then Ku(an) ≤cn for some c > 0.
Finally, the indeterminacy of the complexity related to diﬀerent choices of
optimal functions u, v is multiplicatively exp( O(1)). The same is true for the
Kolmogorov order.
For a concise treatment of Kolmogorov complexity, see [Man1], pp. 226–231,
and for a thorough one, see [LiVi]. Notice that in the literature one often uses the
logarithmic Kolmogorov complexity which is deﬁned as the length of the binary
presentation of Ku(x). It is interpreted as the length of the maximally compressed
description of x. For our purposes, exponential version is more convenient, in
particular, because as soon as an optimal programming method u is chosen, Ku
allows us to deﬁne an unambiguous Kolmogorov order on Z+ or on any inﬁnite
constructive world given together with its structural numbering.
1.4
Fractality and Symmetries of the Kolmogorov Complexity
In [LiVi], pp. 103, 105, 178, one can ﬁnd a schematic graph of logarithmic com-
plexity of integers. The visible“continuity” of this graph reﬂects the fact that
complexity of n+1 in any reasonable encoding is almost the same as complexity
of n. The graph “most of the time” follows closely the graph of log2 n: as soon
as an optimal family is chosen, the respective complexity diﬀers from log2Ku(n)
by a bounded function. However, inﬁnitely often the graph drops down, lower

Physics in the World of Ideas: Complexity as Energy
7
than any given computable function of n. We will sometimes write K in place
of Ku when change of optimal u is not essential.
Looking only at such graphs one does not see or suspect any kind of self–
similarity. But it is there: if one restricts this graph onto any inﬁnite decidable
subset of Z+ in increasing order, one will get the same complexity relief as for
the whole Z+: in fact, for any recursive bijection f of Z+ with a subset of Z+
we have K(f(x)) = exp(O(1)) · K(x).
The natural symmetry group of the logarithmic Kolmogorov complexity is the
group Scomp
∞
of total recursive permutations of Z+: for each such permutation
σ, there exists a constant c(σ) such that for all n we have
| log2K(σ(n)) −log2K(n) | < c(σ).
This symmetry reﬂects the essential independence of complexity on a chosen
initial structural numbering. The group Scomp
∞
is pretty mysterious one.
However, an inner conjugation inside the total permutation group of Z+ al-
lows one to embed it into a less mysterious group of permutations Slin
∞of Z+,
consisting of all permutations which together with their inverses have no more
than linear growth.
To achieve this, it suﬃces to replace the natural order of Z+ by its Kolmogorov
order. This remark is essentially used in the proof of the Theorem 3.5 of the next
section.
2
Probability Distributions on Constructive Worlds and
Zipf’s Law
2.1
Zipf’s Law
Zipf ([Zi1], [Zi2]) studied statistics of usage of words in natural languages. His
main empirical observation was this: if all words {wk} of a language in a repre-
sentative corpus of texts are ranked according to decreasing frequency of their
appearance, and the rank k and the number of the occurrences of wk are plotted
in the logarithmic scale, then the data approximately ﬁt on a line. This means
that the frequency of usage of the word of rank k is approximatetely C/kα where
C and α are some constants. Moreover, for natural languages α is close to one:
see e. g. Fig. 1 in [Ma1] based upon a corpus containing 4 · 107 Russian words.
This general power law, and its particular α = 1 realization turned out to
be very universal, see [MurSo], [Pi]. It was observed in the texts of an extinct
and not deciphered Meroitic language ([Sm]), and in the frequency of certain
patterns in databases of ﬁnancial reports of businesses: see [HuYeYaHua]. In the
latter article, it was suggested that systemtic deviations from Zipf’s law might
be useful for fraud detection.
Zipf himself ([Zi2], [Zi1]) conjectured that his distribution “minimizes eﬀort”.
Mandelbrot in [Mand] made this mathematically precise in the following way.
If we postulate and denote by Ck a certain “cost” (of producing, using etc.) of
the word of rank k, then the frequency distribution pk ∼2−h−1Ck minimizes

8
Y.I. Manin
the ratio h = C/H, where C := 
k pkCk is the average cost per word, and
H := −
k pklog2pk is the average entropy: see a discussion in [Ma2]. This
produces a power law, if Ck ∼log k. An additional problem, explanation of
α = −1, must be addressed separately. For one possibility, see [MurSo], sec. III.
In [Man7], I suggested a similar background but replaced logarithm in Ck ∼
log k by the logarithmic Kolmogorov complexity. This means, in particular, that
Zipf’s rank corresponds to the ranking in order of growing complexity, and iden-
tiﬁes complexity with Zipf’s “eﬀort”. Underlying this metaphor is the image of
brain/society dealing with compressed descriptions of units of information. Intu-
itively, whenever an individual mind, or a society, ﬁnds a compressed description
of something, this something becomes usable, and is used more often than other
”something” whose description length is longer. In experimental psychology, this
corresponds to the “availability bias” studied by A. Tversky and D. Kahneman:
as they write in [TvKa], this is “a judgemental heuristic in which a person eval-
uates the frequency of classes or the probability of events by availability, i. e., by
the ease with which relevant instances come to mind.”
For an expanded version of this metaphor applied to the history of science,
see [Man6], and for related suggestions see [DeMe], [Del], [Ve].
In the context of [Man7], the α = −1 enigma is solved by appealing to another
mathematical discovery. Namely, L. Levin has established that the power law
with α = −1 appears as the maximal (up to a multiplicative constant) probability
distribution that is computable from below: see [Lev1], [Lev2].
In all theoretical discussions, it is more or less implicitly assumed that empir-
ically observed distributions concern fragments of a potential countable inﬁnity
of objects. I also postulate this, and work in a “constructive world”.
2.2
Zipf’s Law from Complexity
Here is the summary of my arguments. Zipf’s law emerges as the combined eﬀect
of two factors:
(A) Zipf’s rank ordering of a constructive world coincides with the ordering
with respect to the growing (exponential) Kolmogorov complexity K(w).
(B) The probability distribution producing Zipf’s law (with exponent −1) is
(an approximation to) the L. Levin maximal computable from below distribution:
see [ZvLe], [Le] and [LiVi].
The α = −1 power law follows from the fact that Levin’s distribution assigns
to an object w probability ∼KP(w)−1 where KP is the exponentiated preﬁx
Kolmogorov complexity, and we have, up to exp (O(1))–factors,
K(w) ⪯KP(w) ⪯K(w) · log1+ε K(w)
with arbitrary ε > 0.
Discrepancy between the growth orders of K and KP is the reason why a
probability distribution on inﬁnity of objects cannot be constructed from K:
the series 
m K(m)−1 diverges. However, on ﬁnite sets of data this small dis-
crepancy is additionally masked by the dependence of both K and KP on the

Physics in the World of Ideas: Complexity as Energy
9
choice of an optimal encoding. Therefore, when speaking about Zipf’s Law, we
will mostly disregard this diﬀerence. One could however argue that it is at least
partly responsible for “numerous minima and maxima in the error of [Zipf’s]
ﬁt”: see [Pi], Sec. 2.
3
Error–Correcting Codes and Their Asymptotic Bounds
3.1
Codes and Code Points
Fix an alphabet A which a ﬁnite set of cardinality q ≥2. A code C ⊂An is a
subset of words of length n. Hamming distance between two words of the same
length is deﬁned as
d((ai), (bi)) := card{i ∈(1, . . . , n) | ai ̸= bi}.
Code parameters are the cardinality of the alphabet q and the numbers n(C),
k(C), d(C) deﬁned by:
n(C) := n,
k(C) := k := [logqcard(C)],
where [x] is the maximal integer ≤x;
d(C) := d = min {d(a, b) | a, b ∈C, a ̸= b}.
Brieﬂy, C is an [n, k, d]q–code. Its code point is the point
x(C) :=
k(C)
n(C), d(C)
n(C),

∈[0, 1]2
Coordinates of x(C) = (R(C), δ(C)) are called transmission rate and relative
distance respectively.
The idealized scheme of using error–correcting codes for information trans-
mission can be described as follows. Some source data are encoded by a sequence
of code words. After transmission through a noisy channel at the receiving end
we will get a sequence of possibly corrupted words. If we know probability of
corruption of a single letter, we can calculate, how many corrupted letters in
a word we may allow for safe transmission; pairs of code words must be then
separated by a larger Hamming distance. This necessity puts an upper bound
on the achievable transmission rate.
A good code must maximize minimal relative distance when the transmission
rate is chosen.
Our discussion up to now was restricted to unstructured codes: arbitrary sub-
sets of words. Arguably, one more property of good codes is the existence of
eﬃcient algorithms of encoding and decoding. This can be achieved by intro-
duction of structured codes. A typical choice is represented by linear codes: for
them, A is a ﬁnite ﬁeld of q elements, and C is a linear subspace of Fn
q .
Now, let us call the multiplicity of a code point the number of codes that
project onto it.

10
Y.I. Manin
All codes (with ﬁxed q), and all rational points in [0, 1]2 constitute examples
of constructive worlds. The map C →x(C) is a computable function. The same
refers to structured codes considered in the literature, in particular, to linear
codes.
Theorem 1. (Yu. M., 1981 + 2011). There exists a continuous function αq(δ),
δ ∈[0, 1], with the following properties:
(i) The set of code points of inﬁnite multiplicity is exactly the set of rational
points (R, δ) ∈[0, 1]2 satisfying R ≤αq(δ). The curve R = αq(δ) is called the
asymptotic bound.
(ii) Code points x of ﬁnite multiplicity all lie strictly above the asymptotic
bound and are called isolated ones: for each such point there is an open neigh-
borhood containing x as the only code point.
(iii) The same statements are true for linear codes, with a possibly diﬀerent
asymptotic bound R = αlin
q (δ).
In a paper published in 1981, I proved that all limit points of the set of code
points (q being ﬁxed) lie under the graph of a monotone function αq that was
later called the asymptotic bound. Its characterisation via multiplicity used here
was proved in [Man2].
3.2
Can One Compute an Asymptotic Bound?
During the thirty years since the discovery of the asymptotic bounds, many
upper and lower estimates were established for them, especially for the linear
case: see the monograph [VlaNoTsfa]. Upper bounds helped pinpoint a number
of isolated codes.
However, the following most natural problems remain unsolved: – To ﬁnd an
explicit formula for αq or αlin
q .
– To ﬁnd any single value of αq(δ) or αlin
q (δ) for 0 < δ < 1 −q−1 (at the end
segment [1 −q−1, 1] these function vanish).
– To ﬁnd any method of approximate computation of αq(δ) or αlin
q (δ).
– Clearly, αlin
q
≤αq. Is this inequaliy strict somewhere?
3.3
A Brief Survey of Some Known Results
(i) One can count the number of codes of bounded block length n and plot
their code points. The standard probabilistic methods then give the following
Gilbert–Varshamov bounds.
Most unstructured q–ary codes lie lower or only slightly above the Hamming
curve
R = 1 −Hq(δ/2),
Hq(δ) = δlogq(q −1) −δlogqδ −(1 −δ)logq(1 −δ).
Most linear q–ary codes lie near or only slightly above the Gilbert–Varshamov
bound
R = 1 −Hq(δ).

Physics in the World of Ideas: Complexity as Energy
11
In particular,
αq(R) ≥αlin
q (R) ≥1 −Hq(δ)
(ii) A useful combinatorial upper estimate is the Singleton bound:
R(C) + δ(C) ≤1 +
1
n(C).
Hence
αq(δ) ≤1 −δ.
It follows that code points lying above this bound are isolated. The following
Reed–Solomon (linear) codes C ⊂Fn
q belong to this group.
Choose parameters 1 ≤k ≤n ≤q, d = n + 1 −k. Choose pairwise distinct
x1, . . . , xn ∈Fq, Embed the space of polynomials f(x) ∈Fq[x] of degree ≤k −1
into Fn
q by
f →(f(x1), . . . , f(xn)) ∈Fn
q .
After works of Goppa, this construction was generalized. Points x1, . . . , xn ∈
Fq were replaced by rational points of any smooth algebraic curve over Fq, and
polynomials by sections of an invertible sheaf. This allowed one to construct non–
isolated linear codes lying partly strictly above the Gilbert–Varshamov bound.
This implies that we cannot “see” the asymptotic bound, plotting the set of
(linear) code points of bounded size: we will see a cloud of points, whose upper
bound concentrates near the Hamming or Varshamov–Gilbert bounds.
The proof of the following theorem given in [ManMar1] again uses reordering
codes by their growing Kolmogorov complexity and basic properties of the Levin
distribution (in this context the diﬀerence between K and KP becomes negli-
gible). Imaginatively, one can say that the asymptotic bound becomes plottable
with help of oracle–assisted computation: oracle should produce for us codes in
their Kolmogorov order.
In order to state our theorem, notice that the function αq(δ) is continuous and
strictly decreasing for δ ∈[1, 1 −q−1). Hence the limit points domain R ≤αq(δ)
can be equally well described by the inequality δ ≤βq(R) where βq is the function
inverse to αq.
Fix an R ∈Q ∩(0, 1). For Δ ∈Q ∩(0, 1), put
Z(R, Δ; β) :=

C: R(C)=R, Δ≤δ(C)≤1
Ku(C)−β+δ(C)−1,
where Ku is an (exponential) Kolmogorov complexity on the constructive world
of all codes in a given alphabet of cardinality q.
Theorem 2. (i) If Δ > βq(R), then Z(R, Δ; β) is a real analytic function of β.
(ii) If Δ < βq(R), then Z(R, Δ; β) is a real analytic function of β for β >
βq(R) such that its limit for β −βq(R) →+0 does not exist.
The following thermodynamical analogies justify our interpretation of the
asymptotic bound as a phase transition curve in the “statistical physics of Pla-
tonian world.” a) The argument β of the partition function corresponds to the
inverse temperature.

12
Y.I. Manin
b) The transmission rate R corresponds to the density ρ.
c) Our asymptotic bound transported into (T = β−1, R)–plane as T = βq(R)−1
becomes the phase transition boundary in the (temperature, density)–plane.
For other mathematical contexts in which our partition function and other
characteristics of codes appear, in particular, for relevant quantum mechanical
analogies, see [ManMar3].
References
[A1]
Aaronson,
S.:
NP–complete
problems
and
physical
reality,
23.
arXiv:quant–ph/0502072
[A2]
Aaronson, S.: Why philosophers should care about computational com-
plexity, 58. arXiv:1108.1791
[DeMe]
Dehaene, S., Mehler, J.: Cross–linguistic regularities in the frequency of
number words. Cognition 43, 1–29 (1992)
[Del]
Delahaye, J.–P.: Les entiers ne naissent pas ´egaux. Pour la Science (421),
80–85 (2012)
[HuYeYaHua] Huang, S.-M., Yen, D.C., Yang, L.-W., Hua, J.-S.: An investigation of
Zipf’s Law for fraud detection. Decision Support Systems 46, 70–83
(2008)
[Lev1]
Levin, L.A.: Various measures of complexity for ﬁnite objects (axiomatic
description). Soviet Math. Dokl. 17(2), 522–526 (1976)
[Lev2]
Levin, L.A.: Randomness conservation inequalities: information and in-
dependence in mathematical theories. Information and Control 61, 15–
37 (1984)
[LiVi]
Li, M., Vit´anyi, P.: An introduction to Kolmogorov complexity and its
applications. Springer (1993)
[Mand]
Mandelbrot, B.: An information theory of the statistical structure of
languages. In: Jackson, W. (ed.) Communication Theory, pp. 486–502.
Butterworth, Woburn (1953)
[Ma1]
Manin, D.Y.: Zipf’s Law and Avoidance of Excessive Synonymy. Cog-
nitive Science 32(7), 1075–1078 (2008), arXiv:0710.0105
[Ma2]
Manin, D.Y.: Mandelbrot’s model for Zipf’s Law. Can Mandelbrot’s
model explain Zipf’s Law for language? Journ of Quantitative Linguis-
tics 16(3), 274–285 (2009)
[Man1]
Manin, Y.I.: A Course in Mathematical Logic for Mathematicians, 2nd
edn. Graduate Texts in Mathematics. Springer (2010)
[Man2]
Manin, Y.I.: A computability challenge: Asymptotic bounds and iso-
lated error-correcting codes. In: Dinneen, M.J., Khoussainov, B., Nies,
A. (eds.) WTCS 2012 (Calude Festschrift). LNCS, vol. 7160, pp. 174–
182. Springer, Heidelberg (2012)
[Man3]
Manin, Y.I.: Classical computing, quantum computing, and Shor’s fac-
toring algorithm, S´eminaire Bourbaki, no. 862, vol. 266, pp. 375–404.
Ast´erisque (June 1999/2000), arXiv: quant-ph/9903008
[Man4]
Manin, Y.I.: Renormalization and computation I. Motivation and back-
ground. In: Loday, J., Vallette, B. (eds.) Proceedings OPERADS 2009.
S´eminaires et Congr`es, vol. 26, pp. 181–223. Soc. Math. de France
(2012), math.QA/0904.492

Physics in the World of Ideas: Complexity as Energy
13
[Man5]
Manin, Y.I.: Renormalization and computation II: Time cut–oﬀand the
Halting Problem. Math. Struct. in Comp. Science 22, 729–751 (2012),
Special issue, Cambridge UP. math.QA/0908.3430
[Man6]
Manin, Y.I.: Kolmogorov complexity as a hidden factor of scientiﬁc dis-
course: From Newton’s law to data mining. In: Talk at the Plenary Ses-
sion of the Pontiﬁcal Academy of Sciences on Complexity and Analogy
in Science: Theoretical, Methodological and Epistemological Aspects,
Vatican, November 5–7 (2012) arXiv:1301.0081
[Man7]
Manin, Y.I.: Zipf’s law and L. Levin’s probability distributions. Func-
tional Analysis and its Applications, Vol. 48(2) (2014), arXiv:1301.0427
[ManMar1]
Manin, Y., Marcoll, M.: Kolmogorov complexity and the asymptotic
bound for error-correcting codes. Journ. of Diﬀ. Geometry (to appear
in 2014), arXiv:1203.0653
[ManMar2]
Manin, Y., Marcolli, M.: Big Bang, Blow Up, and Modular Curves:
Algebraic Geometry in Cosmology. arXiv:1402.2158
[ManMar3]
Manin, Y., Marcolli, M.: Error–correcting codes and phase transitions.
Math. in Comp. Science 5, 133–170 (2011), arXiv:mat.QA/0910.5135
[MurSo]
Murtra, B.C., Sol´e, R.: On the Universality of Zipf’s Law. Phys. Rev.
E 82 (2010)
[Pi]
Piantadosi,
S.T.:
Zipf’s
word
frequency
law
in
natural
lan-
guage: A critical review and future directions, http://colala.bcs.
rochester.edu/papers/piantadosi2014zipfs.pdf
[Sm]
Smith, R.: Investigation of the Zipf–plot of the extinct Meroitic lan-
guage. Glottometrics 15, 53–61 (2007)
[TvKa]
Tversky, A., Kahneman, D.: Availability; A Heuristic for Judging Fre-
quency and Probability. Cognitive Psychology 5, 207–232 (1973)
[Ve]
Todd, L.: Veldhuizen. Software Libraries and Their Reuse: Entropy, Kol-
mogorov Complexity, and Zipf’s Law. arXiv:cs/0508023
[VlaNoTsfa]
Vladut, S.G., Nogin, D.Y., Tsfasman, M.A.: Algebraic geometric codes:
Basic notions. Mathematical Surveys and Monographs. American Math-
ematical Society, Providence (2007)
[Zi1]
Zipf, G.K.: The psycho–biology of language. Routledge, London (1936)
[Zi2]
Zipf, G.K.: Human behavior and the principle of least eﬀort. Addison–
Wesley (1949)
[ZvLe]
Zvonkin, A.K., Levin, L.A.: The complexity of ﬁnite objects and the
basing of the concepts of information and randomness on the theory of
algorithms. Uspehi Mat. Nauk 25(6(156)), 8–127 (1970), (Russian)

Complexity Measures and Physical Principles
Karoline Wiesner
School of Mathematics, University of Bristol, Bristol, U.K.
Abstract. I summarise some recent results on physics of information
and discuss their impact on our understanding of complexity. A surpris-
ingly coherent picture of measures of complexity, information theoretic
functions, and thermodynamical quantities arises.
1
Introduction
Complexity is a concept thoroughly defeating any unique deﬁnition. One no-
tion is that a complex system is an open system exchanging matter and / or
signals with the environment while being robust in its structure and adapting
to environmental circumstances without any internal central control unit [11].
One can take this view into a more quantitative realm by considering a complex
system as an information processor. The above description can then be recast
as follows: A complex system takes in and puts out information, it has an in-
ternal memory, and robust and decentralised information processing capacity.
For complex systems this is a very helpful starting point as it allows quantita-
tive statements about systems of which we do not know the Hamiltonian or any
other form of energetic characterisation. The idea to put physics – which deals
with matter and energy – and information – which deals with bits and memory
on the same footing is not new. The most pointed statement in this context is
probably Wheeler’s “It from bit”, implying that every physical quantity derives
its function from answers to yes-no questions [20].
In the following I will sketch some recent developments in the area of physics of
information and outline parallels to information-theoretic treatments of complex
systems.
2
Measures of Complexity
One way of measuring the complexity of a system is to quantify the amount of
correlation a system exhibits in its temporal behaviour. The temporal behaviour
is captured in observations over time and can be formalised as a stochastic pro-
cess. The most generally applicable measure of correlation in a stochastic process
is the mutual information which is a function of the joint probability distribu-
tion of two random varibles1. We will consider the past data to be one random
1 For deﬁnitions of Shannon entropy, conditional entropy, and mutual information, see
for example [2].
c
⃝Springer International Publishing Switzerland 2015
15
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_2

16
K. Wiesner
variable, ←−
X, and the future data to be another random variable, −→
X. In the limit
of inﬁnite data the mutual information between these two variables has been
taken as a measure of complexity ([13,4]):
E = I[←−
X; −→
X] .
(1)
E has been given diﬀerent names, such as predictive information, excess entropy,
eﬀective measure complexity, and stored information (see [4] and refs. therein).
We can interpret E as the amount of communication between a system’s past
and its future. Any positive mutual information means that the system remem-
bers some of its past and its future behaviour is inﬂuenced by that memory. In
this sense, E is the system’s complexity, measured in bits.
I now turn to a second measure of complexity, the statistical complexity, as in-
troduced by Crutchﬁeld and Young [5]. It is related to the true metric complexity
initially discussed by Grassberger [8]. Given a stochastic process, the provably
unique minimal (in terms of information stored) and optimal (in terms of predic-
tion) computation-theoretic representation summarising the regularities of the
process is the so-called ϵ-machine [5,13]. It consists of an output alphabet X, a
set of states S and stochastic transitions between them. For every pair of states
s, s′ ∈S probabilities Pr(Si = s′, Xi = x|Si−1 = s) are deﬁned for going from
state s to state s′ while outputting symbol x ∈X. The statistical complexity of
a process is deﬁned as the Shannon entropy over the stationary distribution of
its ϵ-machine’s states:
Cμ := −

s
Pr(s) log Pr(s) .
(2)
Cμ is the number of bits required to specify a particular state of the ϵ-machine.
Since knowing the state means knowing everything there is to know about future
observations, Cμ is the number of bits that need to be stored to optimally predict
future data points from the ϵ-machine.
The statistical complexity is bounded from below by the predictive informa-
tion: E ≤Cμ [13]. It is intuitive that the amount of memory of a predicting
device should be at least as big as the number of bits it is predicting about
a system. There have been diﬀerent approaches to derive the exact diﬀerence
between E and Cμ analytically. Crutchﬁeld and co-workers explained it through
the asymmetry between prediction and retrodiction [6]. They were able to show
that the predictive information is equal to the mutual information between the
states predicting the stochastic process forward in time and the states retrod-
icting the process backward in time. Wiesner et al. took a diﬀerent approach
to explaining the diﬀerence between the two complexity measures [19]. They
considered the computational irreversibility of the ϵ-machine and showed that it
accounts exactly for the diﬀerence between the two complexity measures. The
extra memory kept by the ϵ-machine which is not predictive incurs an entropic
cost to the computation. Computing E (Eq. 1) requires numerical approximation
from ﬁnite data. Finding an analytic expression for the predictive information
from the predictive states is an open problem.

Complexity Measures and Physical Principles
17
Gu et al. extended the framework of ϵ-machines by allowing the states to have
quantum mechanical properties [9]. Once the (classical) states S are known the
quantum mechanical states (here in ket notation) can be easily constructed as
|s⟩=

s′∈S

x∈X

Pr(Si = s′, Xi = x|Si−1 = s)|x⟩|s′⟩,
(3)
This is related to an earlier quantum model of a stochastic process [12]. Gu et
al. deﬁne the quantum complexity of the process as the von Neumann entropy
over the resulting density matrix ρ = 
s Pr(s)|s⟩⟨s|:
Cq := −Trρ log ρ .
(4)
It can be easily shown that Cq is bounded above and below by the statistical
complexity and the predictive information, respectively:
E ≤Cq ≤Cμ.
(5)
Equality holds if and only if the upper and lower bound coincide [9,19]. Whenever
Cq is strictly less than Cμ the quantum states |s⟩are non-orthogonal. This means
that there exists no quantum measurement which could perfectly distinguish
between them. Since the machines both predict E bits about future observations
the additional bits stored in the classical machine are non-predictive. This, again,
indicates that the diﬀerence between E and Cμ represents an ineﬃciency in the
model. Explaining the gap between Cq and E remains an open problem.
3
Finding an Optimal Model under Constraints
Finding an eﬃcient model is a classical optimisation problem under costraints:
One wants to minimise the size of a model while maximising its predictive power.
Any model is some form of summary of past observations. A good model retains
as much relevant information about the past as possible and discards anything
beyond that. The so-called information bottleneck method, introduced by Tishby,
Pereira and Bialek, provides the tools for ﬁnding a minimal model under the
constraint of maximal predictive power [18]. They formalise this task as ﬁnd-
ing an optimal compression of data (the “summary”) under the constraint of
minimal error upon decompression (equivalent to “prediction” of observations).
Furthermore, the bottleneck method allows for the case where the data, X,
are descriptions of another, correlated variable Y . The optimisation task they
solve is to compress the data X into ˜X as much as possible while retaining
as much information about Y as needed. Tishby et al. were able to show that
the optimal compression minimises the relative entropy (also known as Kullback-
Leibler divergence) between the conditional distributions Pr(Y |X) and Pr(Y | ˜X),
DKL[Pr(Y |X)|| Pr(Y | ˜X)].
Still et al. applied the bottleneck method to the problem of ﬁnding a predictive
model for a stochastic process as discussed above [16,15], see also [14]. Here the

18
K. Wiesner
future behaviour −→
X of the system corresponds to the variable of interest Y . Past
observations ←−
X are analogous to the data X to be compressed. The states S and
associated transition probabilities are the compressed version ˜X of the data. A
minimal model under the constraint of maximal predictive power can be found
by solving the following optimisation problem:
min
Pr(S|←
−
X)

I(←−
X; S) + βI(←−
X; −→
X|S)

,
(6)
where β is the remaining parameter (Lagrange multiplier) representing the
trade-oﬀbetween model complexity and predictive power. It turns out that
the solution to this optimisation problem minimises the relative entropy
DKL[Pr(−→
X|←−
X)|| Pr(−→
X|S)]. In the limit of β →∞equivalent to unrestricted
model complexity, the states of the ϵ-machine are recovered [16].
Still et al. expanded this framework to a more general setting [17]. The view
point taken is slightly diﬀerent from that of optimally modeling observations
of a complex system’s dynamics. Rather, they consider a complex system as
making a model of the world. The system remembers some of the past experi-
ences in the world and tries to predict future experiences. Mathematically this
framework is identical to the one discussed above. Still et al. deﬁne predictive
power as the mutual information between the system’s state st at time t and
the environmental signal xt+1 at time t + 1. Similarly, instantaneous memory is
deﬁned as the mutual information between the system’s state st at time t and
the environmental signal xt at time t:
Imem(t) = I(st; xt) ,
(7)
Ipred = I(st; xt+1) .
(8)
Both Imem and Ipred are instantaneous quantities, i.e. they extend over one time
point into the future or the past. The instantaneous nonpredictive information
is then deﬁned as the diﬀerence between instantaneous memory and predictive
power, Imem −Ipred. Given these deﬁnitions, Still et al. ﬁnd that the unnecessary
retention of past information is fundamentally equivalent to energetic ineﬃciency
which results in energy dissipation Wdiss. This dissipation is an upper bound to
the diﬀerence between memory and predictive information [17]:
Imem −Ipred ≤βWdiss
(9)
The inequality relies on a result by Jarzynski who found a mathematically
exact relation between the amount of work put into a system and the resulting
average free energy change which holds for systems driven arbitrarily far away
from the initial equilibrium: ⟨e−βW ⟩= e−βΔF [10]. Here, W is the total work
performed on the system, the average is taken over an ensemble of measure-
ments, and ΔF is the total change in free energy between the initial and ﬁnal
state. Crooks was able to show that the assumptions of Markovian dynamics
and microscopic reversibility lead to the same result [3].

Complexity Measures and Physical Principles
19
4
Discussion
We have seen that the complexity of a stochastic process can be measured in
various ways. While the predictive information E is the number of bits stored
by the process itself, the statistical complexity Cμ and the quantum statistical
complexity Cq quantify the minimum amount of memory necessary to optimally
predict the process. In general all three measures diﬀer for any given stochastic
process. While the minimum amount of information required for optimal predic-
tion is E, the actual information stored to make that prediction is usually higher
due to the architecture of the storage device, i.e. the discreteness and physical
nature of the states. Hence, the physical architecture of the prediction device
matters.
A slight change in perspective shed new light on the physical implications
of this information processing ineﬃciency. Instead of a complex system being
predicted by a modeler as best she can, Still et al. considered a complex system
which stores information about past environmental signals to predict future en-
vironmental signals. The assumption is that the system, just like the modeler
before, wants to minimise the amount of information stored while predicting as
much about the next environmental state as possible. Based on a direct propor-
tionality between free energy and relative entropy they established an equiva-
lence between the non-predictive information which the system holds and the
energy dissipation during state changes of the system.
Whether and how energy dissipation caused by ineﬃcient prediction can be
measured experimentally is an open question at this point. The equivalence be-
tween complexity measures and physical principles yields lower bounds only.
Heat dissipation in real complex systems might be orders of magnitude higher.
The basic unit of dissipation in a logical operation is kT ln 2 where k is the Boltz-
mann constant and T is the temperature. The heat dissipated during a logical
operation in a modern processing chip, for example, is an estimated 1011kT ln 2
at room temperature2.
However, there exist experiments in this direction. Energy dissipation has been
measured in small molecular machines performing logical operations [1]. The
measured values were only a few ten times larger than the basic unit of kT ln 2.
Furthermore, a free energy principle has been proposed accounting for perception
and learning in the brain [7]. This, again, is based on the proportionality between
relative entropy and free energy which suggests that the above discussion on
information and energy (dissipation) might be highly relevant in this context.
Much work is still to be done to close the gap between rigorous mathematical
results in the physics of information and characterisation of complexity. I hope
to have conveyed to the reader that some very promising steps have been taken.
Acknowledgements. The Author would like to thank A. Houston for very
helpful comments on a draft of this article.
2 This is an order-of-magnitude estimate based on thermal speciﬁcations of an Intel
processor.

20
K. Wiesner
References
1. Benenson, Y., Adar, R., Paz-Elizur, T., Livneh, Z., Shapiro, E.: DNA molecule
provides a computing machine with both data and fuel. Proceedings of the National
Academy of Sciences 100(5), 2191–2196 (2003)
2. Cover, T.M., Thomas, J.A.: Elements of Information Theory. WileyBlackwell
(2006)
3. Crooks, G.E.: Entropy production ﬂuctuation theorem and the nonequilibrium
work relation for free energy diﬀerences. Physical Review E 60(3), 2721–2726 (1999)
4. Crutchﬁeld, J.P., Feldman, D.P.: Regularities unseen, randomness observed: Levels
of entropy convergence. Chaos 13(1), 25–54 (2003)
5. Crutchﬁeld, J.P., Young, K.: Inferring statistical complexity. Physical Revue Let-
ters 63(2), 105 (1989)
6. Crutchﬁeld, J.P., Ellison, C.J., Mahoney, J.R.: Time’s barbed arrow: Irreversibility,
crypticity, and stored information. Physical Review Letters 103(9), 94101–94104
(2009)
7. Friston, K., Kilner, J., Harrison, L.: A free energy principle for the brain. Journal
of Physiology-Paris 100(1-3), 70–87 (2006)
8. Grassberger, P.: Toward a quantitative theory of self-generated complexity. Inter-
national Journal of Theoretical Physics 25(9), 907–938 (1986)
9. Gu, M., Wiesner, K., Rieper, E., Vedral, V.: Quantum mechanics can reduce the
complexity of classical models. Nature Communications 3, 762 (2012)
10. Jarzynski, C.: Nonequilibrium equality for free energy diﬀerences. Physical Review
Letters 78(14), 2690–2693 (1997)
11. Ladyman, J., Lambert, J., Wiesner, K.: What is a complex system? European
Journal for Philosophy of Science 3(1), 33–67 (2013)
12. Monras, A., Beige, A., Wiesner, K.: Hidden quantum markov models and non-
adaptive read-out of many-body states. Applied Mathematical and Computational
Sciences 3, 93 (2011)
13. Shalizi, C.R., Crutchﬁeld, J.P.: Computational mechanics: Pattern and prediction,
structure and simplicity. Journal of Statistical Physics 104(3), 817–879 (2001)
14. Still, S.: Information bottleneck approach to predictive inference. Entropy 16(2),
968–989 (2014)
15. Still, S., Crutchﬁeld, J.P.: Structure or noise? arXiv/0708.0654 (2007)
16. Still, S., Crutchﬁeld, J.P., Ellison, C.J.: Optimal causal inference: Estimating stored
information and approximating causal architecture. arXiv/0708.1580 (2007)
17. Still, S., Sivak, D.A., Bell, A.J., Crooks, G.E.: Thermodynamics of prediction.
Physical Review Letters 109(12), 120604 (2012)
18. Tishby, N., Pereira, F.C., Bialek, W.: The information bottleneck method.
arXiv/physics/0004057 (2000)
19. Wiesner, K., Gu, M., Rieper, E., Vedral, V.: Information-theoretic lower bound
on energy cost of stochastic computation. Proceedings of the Royal Society A:
Mathematical, Physical and Engineering Science 468(2148), 4058–4066 (2012)
20. Zurek, W.H. (ed.): Information, physics, quantum: The search for links. Santa
Fe Institute Studies in the Sciences of Complex Systems, vol. 8. Advanced Book
Programme (1990)

Collective Dynamics in Neural Networks
Antonio Politi
Institute for Complex Systems and Mathematical Biology,
University of Aberdeen, UK
1
Introduction
A wealth of natural and artiﬁcial systems are composed of many coupled sub-
units: examples are electric, metabolic, and neural networks that are encountered
in engineering and biological contexts, but also the granular media and ﬂuids
studied in physics. In such cases, it is natural to expect substantial changes in-
duced by the mutual coupling and it is customary to qualify the overall behaviour
as collective. There exists, however, a deeper notion of the term collective, that
is related to the concept of thermodynamic phase within equilibrium statistical
mechanics. The same set of microscopic equations may generate and sustain dif-
ferent macroscopic states (see, e.g., the gas, liquid, and solid phases), that may
be selected by varying a suitable control parameter (e.g., the temperature, or an
external ﬁeld). At equilibrium, the macroscopic phases are necessarily stationary
(time-independent) but, out-of-equilibrium, detailed balance is violated, currents
are generated and macroscopic oscillations may appear as well. The emergence
of collective motion is particularly relevant in the context of neural networks,
where its properties are likely to be connected to information processing in a
way still to be understood.
As a general theory is still lacking, it is convenient to review a series of various
examples, starting from the simple and yet general synchronization transition
occurring in the Kuramoto model [1]. It describes an ensemble of sinusoidally
coupled phase-oscillators
˙φi = ωi + g
N
N

j
sin(φj −φi) = ωi + gR sin(ψ −φi) ,
where each oscillator is characterized by a diﬀerent bare frequency ωi and
Reiψ = 1
N
N

j
eiφj
represents the order parameter. Upon increasing the coupling constant g from
zero, the system undergoes a phase transition from an asynchronous regime
characterized by R = 0 (in the N →∞limit), to a coherent phase, where R > 0,
meaning that a ﬁnite fraction of the oscillators are mutually locked. This mean-
ﬁeld model can be properly analysed by writing an evolution equation for the
c
⃝Springer International Publishing Switzerland 2015
21
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_3

22
A. Politi
probability density P(φ, ω, t)dφdω of the oscillator phases at time t. Although
such a nonlinear equation is a functional and hence inﬁnite-dimensional, the
resulting scenario is not qualitatively diﬀerent from that of a typical equilibrium
transition: above transition, R is indeed constant and the only time-dependence
is a trivial homogeneous rotation of the phase ψ which can be eliminated by
suitably choosing the reference frame.
A by far less trivial scenario arises in models of globally coupled logistic
maps [2]
xn+1(i) = f [(1 −g)xn(i) + g⟨xn⟩]
,
f(x) = ax(1 −x)
where the angular brackets denote an ensemble average over the N dynamical
units (the mean ﬁeld). Here, for some a and g values, the “microscopic” chaotic
dynamics induced by the logistic map is accompanied by an irregular behaviour
of the collective variable ⟨xn⟩. This observation could be mistaken for a violation
of the law of large numbers, if one (incorrectly) assumed that the single maps are
statistically independent (because of the chaotic dynamics). In practice, analo-
gously to the Kuramoto case, one should more properly look at the evolution
of the probability distribution P(x, n), which is again described by a nonlinear
functional equation. At variance with the previous case, however, here P(x, n)
does not converge towards a ﬁxed point, but keeps showing irregular oscillations
which appear even to be inﬁnite dimensional [2,3].
It is natural to ask whether there are general rules, which can allow predicting
the complexity of the macroscopic dynamics, given the microscopic equations of
motion. The comparison between the Kuramoto setup and the logistic maps
seems to suggest that a more complex microscopic dynamics is necessary to gen-
erate a more complex collective motion, but such a conclusion is untimely, as the
study of various phase-oscillator networks has revealed. Let us start by brieﬂy re-
viewing the expected asymptotic regimes in the case of identical oscillators. First
of all, there exist two highly symmetric solutions: (i) a fully synchronous state,
where all units evolve periodically with the same phase, so that the Kuramoto
order parameter is R = 1; (ii) the so-called splay state, where the single units
follow the same periodic dynamics, but their phases are uniformly distributed,
so that R = 0. Furthermore, it may happen that the oscillator phases group in
clusters, each characterized by a single value [4]. This phenomenon is rather gen-
eral, but a corresponding theory has not yet been fully elaborated. Finally, there
exists the less known phenomenon of partial synchronization. In this regime the
(identical) oscillators follow a quasiperiodic dynamics, while the overall proba-
bility density exhibits periodic oscillations: in practice the macroscopic density
rotates with a frequency that happens to be larger than the average frequency
of the single oscillators. This regime was ﬁrst discovered by van Vreeswijk in a
network of leaky-integrate-and-ﬁre (LIF) neurons [5]
˙vi = a −vi + gE(t) :
when the membrane potential vi reaches the threshold vi = 1, its value is reset
to vi = 0, while a pulse W(t) is sent to all other oscillators with some delay τ. All
such pulses contribute to deﬁning the mean ﬁeld E = 1
N

n|tn<t W(t −tn −τ).

Collective Dynamics in Neural Networks
23
For many years, it was believed that partial synchronization is a special regime
which arises only in the LIF model, until a generalized Kuramoto model with
a nonlinear dependence on the order parameter R was shown to give rise to
a similar scenario [6]. Partial synchronization is, however, a yet more general
regime and its properties can be fully appreciated by discussing the relationship
between pulse-coupled oscillators, such as the LIF model, and Kuramoto-like
systems.
At a ﬁrst sight, there are two main diﬀerences between the two models: (i)
in the LIF case, the membrane potential vi is not a true phase variable (it
does not rotate uniformly); (ii) the mean-ﬁeld E has its own dynamics (it is
indeed necessary to add some equations which account for the pulse shape W).
The ﬁrst diﬀerence can be eliminated by a suitable change of variables which
straighten the phase dynamics [7]. As for the second one, one can eliminate
the ﬁeld dynamics, by transforming the pulse height at time t with a phase-
dependence coupling strength. Upon following this strategy, one can transform
the LIF dynamics into a Winfree model [8]
˙φi = ν + gΓ(φi) 1
N

j
S(φj).
This is perhaps the ﬁrst model ever proposed to describe the evolution of cou-
pled oscillators: Γ(φ) is the so-called phase-response curve, which expresses the
eﬀectiveness of a perturbation; S(φ) is the coupling function, which, in this case,
encodes the pulse shape. Upon suitably choosing the functions Γ and S, it is
possible to observe the onset of partial synchronization [10]. This conﬁrms a
substantial equivalence with the original pulse-coupled model.
The Winfree model is closer to the Kuramoto model in that the coupling
is instantaneous. There is still, however, a distinguishing feature: the velocity
ﬁeld depends on the absolute phases of the oscillators. This is due to the weak-
coupling limit implicitly assumed while deriving the Kuramoto model. In such a
limit, with the help of averaging techniques, it is possible to map Winfree model
onto the so-called Kuramoto-Daido structure
˙φi = ν + g 1
N

j
G(φi −φj) .
where the interaction structure is fully encoded into the periodic phase-interaction
function G that is obtained by convolving Γ and S [8]. The stadard Kuramoto
model would correspond to G(φ) = sin φ. The shape of the phase-interaction curve
which corresponds to the original LIF model (for some parameter values which
support partial synchronization) is reported in Fig. 1.
There, we see that it is not drastically diﬀerent from the sinusoidal shape of
the original Kuramoto model (the presence of additional harmonics make G a
bit skewed). In fact, it is possible to prove that the Kuramoto model is rather
singular: although it cannot give rise to partial synchronization, the addition of
one harmonic suﬃces to make this possible [10]. As a result, the missed observa-
tion of this regime is more to be attributed to the fact most of the studies have

24
A. Politi
0
0.2
0.4
0.6
0.8
1
φ
-0.2
-0.1
0
0.1
0.2
G
Fig. 1. The phase-interaction function which corresponds to the LIF model for g = 0.1,
a = 1.3 and W (t) = α2 exp(−αt) with α = 6 and with zero delay
been focused on the original Kuramoto model, rather than to the speciﬁcity of
the phenomenon.
Interestingly enough, numerical studies have revealed that partial synchro-
nization is also rather robust. First of all it surives to the addition of a ﬁnite
amount of noise [11] (independently on each oscillator). This suggests that it
should be observable in experimental setups, that are unavoidably aﬀected by ex-
ternal and internal noise. Furthermore, this phenomenon survives even in sparse
networks, when each oscillator is coupled only to K other oscillators. It appears
that an average connectivity K = 10 is suﬃcient to sustain stable periodic peri-
odic collective oscillations [12]. In other worlds, the phenomenology goes beyond
the simple mean-ﬁeld setup where it has been originally observed. Moreover, the
relatively small connectivity that is required suggests that collective phenomena
should play a crucial role in more realistic neural networks.
A last crucial point that is worth considering is the role of disoder. In the orig-
inal Kuramoto model, the “partial” synchronization signalled by intermediate R
values is the result of a distribution of the bare frequencies: those oscillators with
either too-large or too-small frequencies cannot lock. In the LIF setup, all oscil-
lators are identical and none of them is locked with the mean ﬁeld: the onset of
partial synchronization is the result of an atypical Hopf bifurcation, which leads
to the onset of two frequencies. It is therefore legitimate to ask what happens in
pulse-coupled oscillators in the presence of additional disorder. Strange enough,
this question has not been thoroughly explored in the literature. The most ap-
propriate setup is that of δ pulses with a small delay (about 10% of the average
interspike interval). Numerical studies have revealed that a standard synchroniza-
tion transition (similar to that occurring in the Kuramoto model) is accompanied
by the emergence of a nontrivial collective motion [13]. This conﬁrms that the LIF

Collective Dynamics in Neural Networks
25
model (and pulse-coupled oscillators, in gneral) is a richer dynamical system than
the standard Kuramoto model, where, at most, periodic oscillations of the order
parameter R can be observed. Moreover, we see that the complexity of the collec-
tive dynamics is not directly linked to the complexity of the single units.
Finally, for the sake of completeness, I wish to brieﬂy recall another collec-
tive behaviour that may arise in (identical) phase oscillators. In the presence
of a global coupling which, nevertheless depends on the physical distance, the
symmetry among all oscillators may break giving birth to the so-called chimera
states [14] characterized by the spontaneous birth of two sub-populations: those
which are synchronous with the mean ﬁeld and the unlocked ones. Altogether,
much progress is still needed to build a general theory of collective dynamics
in neural networks, especially in the presence of disordered and/or nontrivial
topological structures.
References
1. Kuramoto, Y.: Chemical Oscillations, Waves, and Turbulence. Springer, Berlin
(1984)
2. Shibata, T., Kaneko, K.: Phys. Rev. Lett. 81, 4116 (1998)
3. Takeuchi, K.A., Chat´e, H.: J. Phys. A 46, 254007 (2013)
4. Golomb, D., Rinzel, J.: Physica D 72, 259 (1994)
5. van Vreeswijk, C.: Phys. Rev. E 54, 5522 (1996)
6. Rosenblum, M., Pikovsky, A.: Phys. Rev. Lett. 98, 64101 (2007)
7. Abbott, L.F., van Vreeswijk, C.: Phys. Rev. E 48, 1483–1490 (1993)
8. Hansel, D., Mato, G., Meunier, C.: Neural Comput. 7, 307 (1995)
9. Winfree, A.T.: J. Theor. Biol. 16, 15 (1967)
10. Politi, A., Rosenblum, M.: in preparation
11. Mohanty, P.K., Politi, A.: J. Phys. A 39, L415 (2006)
12. Luccioli, S., Olmi, S., Politi, A., Torcini, A.: Phys. Rev. Lett. 109, 138103 (2012)
13. Luccioli, S., Politi, A.: Phys. Rev. Lett. 105, 158104 (2010)
14. Abrams, D.M., Strogatz, S.H.: Phys. Rev. Lett. 93, 174102 (2004)

Transient Sequences in a Network of Excitatory
Coupled Morris-Lecar Neurons
Dmitry V. Kasatkin, Aleksey Dmitrichev, and Vladimir I. Nekorkin
Institute of Applied Physics of the Russian Academy of Sciences,
Ulyanov str. 46, 603950 Nizhny Novgorod, Russia
{kasatkin,admitry,vnekorkin}@neuron.appl.sci-nnov.ru
Abstract. We propose a model of neural network demonstrating vari-
ety of sequential activity modes. Unlike the previously known models
of transient dynamics, in the present model transient sequential modes
are formed by means of dynamical bifurcations and not directly related
to the existence of heteroclinic channels. It is shown that network be-
ing initially at rest generates a sequence of metastable oscillatory states
of activity in response to an external stimulus. We study the inﬂuence
of the parameters characterized the initial times of synaptic activation
processes caused by input information signals on the network dynamics.
Numerical simulation of the model has shown, that these parameters de-
termine not only the structure of the set of oscillatory metastable states
and the sequence of transitions between them, but also the temporal
characteristics of the transition sequences such as the time duration of
the oscillatory metastable states.
Keywords: neural network, transient dynamics, oscillatory metastable
states, dynamical bifurcation.
1
Introduction
Many neurophysiological experiments [1–4] have indicated that some neural pro-
cesses related, for example, with performing of diﬀerent cognitive tasks (mem-
ory, attention, psychomotor coordination, and so on) are accompanied only by
transient activity at the level of individual neurons or small enough groups of
neurons. As a result of such processes a certain sequence of transitional activity
phases appears in neural network. It is clear that such activity of neural net-
works cannot be understood within the framework of classical models of nonlin-
ear dynamics which are based on the concept of attractor because here the main
eﬀect is achieved long before the system reaches its neighborhood. Therefore,
to adequately describe transient and sequential neural processes a novel class
of models should be developed. One of the popular points of view is that such
models should be based primarily on the concept of metastable states. Transient
dynamics here is formed by sequential switching between these states. Recently
several phenomenological models based on the metastable states concept have
been proposed [5–9]. These models operate with variables characterizing aver-
aged activity of neural networks. To describe the dynamics of such variables
c⃝Springer International Publishing Switzerland 2015
27
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_4

28
D.V. Kasatkin, A. Dmitrichev, and V.I. Nekorkin
either a generalized Lotka-Volterra system or a Wilson-Cowan system is used. It
has been shown that under appropriate conditions a stable heteroclinic channel
appears in the phase space of the models. This channel represents a mathematical
image of transient activity. Indeed, the channel is formed by a set of trajectories
in the vicinity of a heteroclinic skeleton which consists of saddles and unstable
separatrices which connect their surroundings. Each trajectory of the channel
sequentially passes the neighborhoods of saddle ﬁxed points ”staying” there for
some (ﬁnite) time. However, the models have signiﬁcant limitations. First of all,
they do not account for dynamical properties of individual neurons. Another
limitation consists in signiﬁcant simpliﬁcation of architecture and properties of
interneuron synaptic couplings.
In this work we propose a model demonstrating various structurally stable
transient dynamics and taking into account individual properties of neurons. The
model represents a network of Morris-Lecar neurons [10] coupled by means of
excitatory chemical synapses. In this model sequential activity modes are formed
by means of dynamical bifurcation and not directly related to the existence of
heteroclinic channels.
2
Model
Consider a neural network consisting of N neurons coupled by excitatory synapses.
The dynamics of such a network is described by the following system of ODEs:
C dvi
dt = −gL(vi −vL) −gCaM∞(vi)(vi −vCa) −
(1)
−gKni(vi −vK) + Iext
i
−gsynsi(vi −vrev),
dni
dt = n∞(vi) −ni
τn(vi)
,
(2)
dri
dt = f1(ri) −si −k1,
(3)
dsi
dt = ε(f2(ri) −si −k2 −μ
N

j=1,
j̸=i
H(vj −θji)),
(4)
i, j = 1, N.
The neurons dynamics is given by the two-variable Morris-Lecar (ML) neu-
ronal model (1)-(2), equations (3)-(4) describe synaptic couplings between neu-
rons. Here, the variables vi, ni correspond to the membrane potential and the
state of activation of K+ channels of the i-th neuron, C is the capacitance of
the membrane. The parameters gL, gCa, gK are the maximal conductances as-
sociated with the leak, Ca2+ and K+ transmembrane currents, respectively; vL,
vCa, vK are the corresponding reversal potentials; Iext
i
are the external currents.
The voltage-dependent gating functions M∞(v), n∞(v) and τn(v) are given by:
M∞(v) = 0.5

1 + tanh
v −v(1)
v(2)

,

Transient Sequences in a Network
29
n∞(v) = 0.5

1 + tanh
v −v(3)
v(4)

,
τn(v) =

φ cosh
v −v(3)
2v(4)
−1
,
where v(1) = −0.01, v(2) = 0.15, v(3) = 0, v(4) = 0.3, φ = 1. The coupling be-
tween neurons is deﬁned by adding the synaptic current term −gsynsi(vi −vrev)
to the right side of equation (1). The parameter gsyn is the maximal synaptic
conductance and vrev is the synaptic reversal potential. The variable si charac-
terizes how the postsynaptic conductance depends on the presynaptic potentials
vj, (j ̸= i). To describe the dynamics of synaptic interneuron interaction we
introduce a phenomenological model in the form of equations (3),(4), where
f1(r) = r −r3
3 ,
f2(r) =

αr,
r < 0
βr,
r ≥0 ,
H(x) = (1 + exp(−x
Kp
))−1,
Kp = 0.001, α = 0.5, β = 2, ε = 0.005, μ = 0.07, k1 = −0.666, k2 = −0.5.
The parameters θji characterize the initial times of synaptic activation pro-
cesses caused by input information signals. In the ML neuronal model (equations
(1),(2)) values of parameters are ﬁxed as follows: C = 1, gL = 0.1, gCa = 1.1,
gK = 2, vL = −0.5, vCa = 1, vK = −0.7, gsyn = 0.0409, vrev = 0.5, Iext
i
= 0.13,
(i = 1, N). With these parameter values the uncoupled single neuron is in the
rest state.
3
Synaptic Coupling Dynamics
The term −sigsyn(vi−vrev) in the right side of equation (1) models synaptic cur-
rent that can changes the membrane potential of the i-th neuron vi. Activation
variable si shows the dependence of postsynaptic conductance on presynaptic
potential vj. Here, we propose a new phenomenological dynamical model of
synapse possessing the threshold features and realistic form of synaptic current
as well as synaptic delay.
The FitzHugh-Nagumo system (equations (3),(4) with μ = 0) in the param-
eters region, where it has three equilibrium states, was used here for describing
dynamics of variable si. Let us consider in detail an activation process of synap-
tic coupling in our model. If the condition vj < θji is satisﬁed for all the neurons
membrane potentials, then all the functions H(vj−θji) ≈0 and equations (3),(4)
can be rewritten as follows:
dri
dt = f1(ri) −si −k1,
(5)
dsi
dt = ε(f2(ri) −si −k2).

30
D.V. Kasatkin, A. Dmitrichev, and V.I. Nekorkin
(a)
(b)
(c)
(d)
Fig. 1. Phase portraits of system (3),(4) for: (a) vj < θji; (b) vj > θji. (c) Fragment
of trajectory forming in system (3),(4) as a result of processes of its activation and
inactivation; (d) Evolution of variable si(t) caused by changing of membrane potential
vj.
Under the chosen parameter values, the system (5) has a phase portrait presented
in ﬁg. 1a. In this case, for any initial conditions the system comes to the rest
state associated with stable equilibrium state O1. Parameters k1 and k2 are
chosen such, that the ordinate of this equilibrium state is equal to zero. There is
also saddle equilibrium state O2 in phase plane. Stable separatrix of this state
deﬁnes the activation threshold of the system. Assume that at some moment
the potential of the j-th neuron exceeds θji value. Then, function H(vj −θji)
rapidly takes on the value equal to unity (H is close to Heaviside function) and
for throughout the period when vj > θji, equations (3),(4) are written in the
form:
dri
dt = f1(ri) −si −k1,
(6)
dsi
dt = ε(f2(ri) −si −k2 −μ)).
At the same time, the initial conditions for system (3),(4) are deﬁned by dy-
namics of the previous system (5) and correspond to its equilibrium state O1. In
ﬁg. 1b the phase portrait of system (6) is presented. In this case there is only one
attractor of the system – stable limit cycle L. Note, that transformation of the
phase portrait, presented in ﬁg. 1a, into the one, illustrated in ﬁg. 1b is realized
through Andronov-Hopf bifurcation, loop of separatrix W s
1 and W u
1 and saddle-

Transient Sequences in a Network
31
node bifurcation of limit cycle. However, appearing modes have no signiﬁcant
inﬂuence on dynamics of system (3),(4), because the function H(vj −θji) takes
one of two possible values 0 or 1 rather quickly. The trajectory of system (6),
originating from equilibrium state O1 returns to the vicinity of stable limit cycle
L in accordance with dynamics of system (6). When the variable vj becomes less
than θji, the system dynamics is described by equations (5), i.e. the trajectory
returns to the vicinity of equilibrium state O1. Therefore, dynamics of synapse
is caused by both systems (5) and (6) in turn. If the excess of potential vj over
θji is multiple, then some trajectory T (ﬁg. 1c) appears in the phase plane of
variables (ri, si). This trajectory consists of fragments of trajectories of systems
(6) (dot line) and system (5) (solid line) in turn. If the representation point ﬁnds
itself to the right of separatrix W s
1 after the presynaptic potential ceases to in-
crease, then the system returns to the vicinity of equilibrium state O1 according
to system (5) dynamics. This process is followed by “powerful” pulse change of
variable r, i.e. activation of synaptic coupling. In this case evolution of variable
s has a typical form for synaptic current (ﬁg. 1d). Separatrix W s
1 deﬁnes acti-
vation threshold for synaptic coupling. The time needed for the representation
point to ﬁnd itself to the right of separatrix W s
1 determines synaptic delay in our
model. Thus, the parameters θji in system (1)-(4) control the start of synaptic
processes and synaptic delay is deﬁned by dynamical properties of two “joining”
nonlinear systems.
4
Oscillatory Metastable States
Consider dynamics of the model (1)-(4). All the neurons are in the rest state
at the initial moment. Suppose one of the neurons (for example, the second
one) to be aﬀected by external current (information signal). It causes change
of membrane potential v2(t). When the variable v2(t) exceeds the value of θ2i
synaptic coupling between the second and the i-th neurons may be activated. In
the phase plane of system (3),(4) there appears trajectory T which in some time
exceeds the threshold corresponding to separatrix W s
1 , and causes an increase
of variable si (ﬁg. 1d). Increasing of variable si means appearance of synaptic
current coming to the i-th neuron. Under the action of the synaptic current, the
dynamical system (1),(2), describing the i-th neuron, is non-autonomous and
is deﬁned in three-dimensional phase space. The system (3),(4) have a small
parameter ε and therefore the evolution of the si-variable is slow with respect
to the evolution of variables vi, ni. Hence, in the ﬁrst approximation one can
suppose variable si(t) in system (1),(2) to be a quasistatically varying param-
eter. The bifurcation diagram of system (1),(2) where si is playing a role of
parameter is presented in ﬁg. 2a. For si < sd ≈0.724 the only stable equilibrium
state P1 exists. For si = sd the saddle-node bifurcation of limit cycle holds,
and two limit cycles – stable Cs and unstable Cu appear on the phase plane
(ﬁg. 2a). Further increasing of si leads to the shrinking of Cu to the equilibrium
state P1, and for si = sh ≈1.092, the equilibrium state P1 changes its stabil-
ity due to the subcritical Andronov-Hopf bifurcation. For si > sf ≈1.327, two

32
D.V. Kasatkin, A. Dmitrichev, and V.I. Nekorkin
0,0
0,4
0,8
1,2
1,6
-0,4
-0,2
0,0
0,2
P1
vi
si
si
max
si
d
si
h
max
min
P2
P3
si
f
(a)
(b)
Fig. 2. Dynamics of system (1),(2) for changing si value. (a) One-parametric bifurca-
tion diagram; (b) The trajectory forming in non-autonomous phase space for changing
si(t).
more equilibrium states appear - saddle P2 and unstable P3 (ﬁg. 2a). There-
fore, if si > sh, the only attractor of system (1),(2) is stable limit cycle Cs. In
non-autonomous phase space, the stable equilibrium states P1(si) form a stable
one-dimensional integral manifold M s
st(0) and the stable limit cycles Cs form
a stable two-dimensional integral manifold M s
c (0). According to the theorem of
persistence of integral manifolds [11] there are integral manifolds M s
st(ε) and
M s
c (ε) close to M s
st(0) and M s
c (0) correspondingly. Since initially the i-th neu-
ron is at the rest, the representative point in the non-autonomous phase space
is situated on the manifold M s
st(ε). Under a synaptic action the representative
point starts moving along M s
st(ε) until it reaches the boundary of M s
st(ε). After

Transient Sequences in a Network
33
that the representative point passes rapidly into a vicinity of M s
c (ε) forming an
oscillatory motion. The oscillating process is over when the representative point
reaches the boundary of the manifold M s
c (ε). Fig. 2b illustrates a numerically
obtained trajectory in non-autonomous phase space laying in manifolds M s
st(ε)
and M s
c (ε). The trajectory includes an oscillating part related to the phase of
transient oscillating activity of a neuron, and the part corresponding to the phase
of rest state of a neuron.
5
Transient Sequences
We considered the process of activation of a neuron and described dynamical
mechanism of arising of the process. Activation of other neurons of the network
occurs analogously. As a result, a sequence of oscillation activity phases of dif-
ferent neurons of the network emerges successively in time. Numerical study of
system (1)-(4) has shown, that any set of parameters θij corresponds to some
sequence (“path”) of transient oscillatory states of neurons of the network. Fig-
ure 3 depicts partition of (θ13, θ32)-parameters plane into domains corresponding
to diﬀerent transient dynamical modes of the network (1)-(4) for N = 3.
-0,20
-0,15
-0,10
-0,05
0,00
0,05
0,10
0,15
-0,20
-0,15
-0,10
-0,05
0,00
0,05
0,10
0,15
1
2
3
4
5
6
7


Fig. 3. Diagram of transient dynamical modes of system (1)-(4) for N = 3 in the plane
of parameters (θ13, θ32), θ21 = −0.2, θ12 = θ23 = θ31 = 0.2
In domain 1, sequential excitation of the ﬁrst, third and second neurons (ba-
sic sequence) is observed in the system (ﬁg. 4a). If the parameters θ13 and θ32
belong to the domain 2 there is only the successive excitation of the ﬁrst and
third neuron, but the second neuron remains unexcited. Domain 3 corresponds
to the excitation of only the ﬁrst neuron, while the others remain at the rest.

34
D.V. Kasatkin, A. Dmitrichev, and V.I. Nekorkin
0
300
600
900
1200
-0.4
-0.2
0.0
0.2
time
vi
v1v2v3
(a)
(b)
(c)
(d)
Fig. 4. Examples of transient sequences generated by a network of three Morris-
Lecar neurons with excitatory synaptic connections for diﬀerent parameters θij: θ13 =
−0.146, θ32 = −0.146 (a); θ13 = −0.05, θ32 = −0.05 (b); θ13 = 0.05, θ32 = −0.05 (c);
θ13 = −0.175, θ32 = −0.05 (d). The other parameters θij are chosen as in ﬁg. 3.
Transient sequences realized in such a network represent not only processes of
single activation of neurons. It has been found, that the phases of activity of
neurons may be not single and may take place irregularly. The examples of such
transient sequences are presented in ﬁg. 4b-d. These modes are realized for pa-
rameter values from domains 4 and 5 (ﬁg. 3). Domain 6 corresponds to the mode
when the basic sequence is repeated three times. Domain 7 has suﬃcient com-
plex structure and contains a set of subdomains that is not shown here because
they are relatively narrow. In general, the dynamics in this domain is charac-
terized by nontrivial transient modes with multiply repetition of basis sequence.
Parameters θij determine not only the structure of the set of metastable states
and the sequence of transitions between them, but also the temporal charac-
teristics of the transition sequences such as the time duration of the oscillatory
metastable state. Furthermore it was established that a set of metastable states
and the transient sequence among them are also sensitive to initial conditions
(input information signals). Transient sequences may vary depending on ampli-
tude and duration of an external initial action and element of network where
external action is applied.
All results presented here for the three coupled neurons will take place in the
case of large neural network. Numerical study showed that the more elements
are in the network the more diﬀerent sequential modes the network can form.
Also the sequences themselves become more complex. For example, the ﬁgure 5
shows one of the sequential modes formed in the network of 100 Morris-Lecar
neurons coupled randomly (Erdos-Renyi model with probability 0.2) by means
of excitatory synapses with identical activation thresholds.

Transient Sequences in a Network
35
Fig. 5. Spatiotemporal pattern of transient activity generated by a network of hundred
Morris-Lecar neurons with excitatory synaptic connections. Points indicate the spikes
generated by neurons of the network.
6
Conclusion
Sequential activity inherent to a wide range of neural systems is associated with
cognitive processes like memory, attention, decision making, motor actions. In
the present work we develop an approach for describing sequential dynamics.
We propose a model of neural network demonstrating variety of sequential ac-
tivity modes which are formed by means of dynamical bifurcations. Unlike the
previously known models [5, 8, 9] of transient dynamics, in the present model
transient sequential modes are not directly related to the existence of hetero-
clinic channels. To simulate excitatory synaptic couplings between neurons we
propose two-dimensional dynamical system with nonunique equilibrium state.
It is shown that a sequence of metastable oscillatory states of activity occurs
in the network initially at rest in response to an external stimulus. We study
the inﬂuence of the parameters characterized the initial times of synaptic acti-
vation processes caused by input information signals on the network dynamics.
Numerical simulation of the model has shown, that these parameters determine
the structure of the set of oscillatory metastable states, the sequence of tran-
sitions between them and temporal characteristics of the transition sequences.
It has been established that the transient sequences realized in the network are
selective to input information signals and robust against small perturbations of
initial conditions. Application of the proposed approach to the problem of mod-
eling sequential activity allow us to describe, explain and predict the possible
spatio-temporal activation processes of some neural systems in the brain.
Acknowledgment. The authors would like to thank the support of the Russian
Foundation for Basic Research (grants Nos. 13-02-00858, 12-02-00526).

36
D.V. Kasatkin, A. Dmitrichev, and V.I. Nekorkin
References
1. Stopfer, M., Jayaraman, V., Laurent, G.: Intensity versus Identity Coding in an
Olfactory System. Neuron 39, 991–1004 (2003)
2. Mazur, O., Laurent, G.: Transient Dynamics versus Fixed Points in Odor Rep-
resentations by Locust Antennal Lobe Projection Neurons. Neuron 48, 661–673
(2005)
3. Jones, L.M., Fontanini, A., Sadacca, B.F., Miller, P., Katz, D.B.: Natural Stimuli
Evoke Dynamic Sequences of States in Sensory Cortical Ensembles. Proc. Natl.
Acad. Sci. USA 104, 18772–18777 (2007)
4. Orlandi, J.G., Soriano, J., Alvarez-Lacalle, E., Teller, S., Casademunt, J.: Noise
Focusing and the Emergence of Coherent Activity in Neuronal Cultures. Nature
Phys. 9, 582–590 (2013)
5. Afraimovich, V.S., Zhigulin, V.P., Rabinovich, M.I.: On the Origin of Reproducible
Sequential Activity in Neural Circuits. Chaos 14, 1123–1129 (2004)
6. Rabinovich, M.I., Huerta, R., Afraimovich, V.S.: Dynamics of Sequential Decision
Making. Phys. Rev. Lett. 97, 188103 (2006)
7. Rabinovich, M.I., Huerta, R., Varona, P., Afraimovich, V.S.: Generation and Re-
shaping of Sequences in Neural Systems. Biol. Cybern. 95, 519–536 (2006)
8. Rabinovich, M.I., Varona, P., Tristan, I., Afraimovich, V.S.: Chunking dynamics:
heteroclinics in mind. Front. Comput. Neurosci. 8, 1–10 (2014)
9. Komarov, M.A., Osipov, G.V., Zhou, C.S.: Heteroclinic contours in oscillatory
ensembles. Phys. Rev. E. 87, 022909 (2013)
10. Morris, C., Lecar, H.: Voltage Oscillations in the Barnacle Giant Muscle Fiber.
Biophys. J. 35, 193–213 (1981)
11. Hirsh, M.V., Pugh, C.C., Shub, M.: Invariant manifolds. In: Dold, A., Eckmann,
B. (eds.) LNM, vol. 583, pp. 2–148. Springer, Berlin (1977)

 
© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
37
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_5 
 
The Development of Chemical Artificial Intelligence 
Processing Fuzzy Logic 
Pier Luigi Gentili 
Department of Chemistry, Biology and Biotechnology, University of Perugia, Italy 
pierluigi.gentili@unipg.it 
Abstract. The Human Nervous System is an outstanding example of natural 
complex system. Its hierarchical architecture and its basic nonlinear working 
principles store the secrets of Complexity. Of course, a scrutiny of the Human 
Nervous System is going to have a profound impact on the challenges to Com-
plexity. In this contribution, we present the first results in our analysis of the 
human nervous system at the “computational”, “algorithmic” and “implementa-
tion” levels. Such analysis will probably bring to the development of a new 
generation of computing machines imitating the human intelligence that com-
putes with words and solves quite easily computational problems like the 
recognition of variable patterns. 
Keywords: Complexity, Fuzzy Information, Bayes theory, Chromogenic mate-
rials, Bistable reactions, Belousov-Zhabotinsky reaction.  
1 
Introduction 
Nowadays, one of the most exciting and compelling goal of science is trying to win 
the Complexity Challenges. There are two types of Complexity Challenges. The first 
type regards Natural Complexity and the development of a unifying theory which 
should describe and predict the behavior of any Natural Complex System. Whatever 
is the character of the Complex System, that is if it is of either chemical or physical, 
or biological or geological, or social or economic, etc…, character, science is spurred 
to unveil the shared working principles. The second type of Complexity Challenges 
concerns about the Computational Complexity; we need to devise effective ways to 
find accurate solutions of NP-problems in reasonable time. Moreover, we need to 
search for universally acceptable solutions of pattern-recognition problems.  
Two main strategies are being followed to face the Complexity Challenges. The 
first strategy consists in developing always more powerful electronic supercomputers 
[1], because facing both Natural and Computational Complexity means handling a 
huge number of data. The second strategy is the interdisciplinary endeavor known as 
Natural Computing [2]. It involves mathematicians, computer scientists, biologists, 
physicists, chemists, engineers, etc…, and its goals are to propose (1) new models to 
describe Natural Complex Systems; (2) new algorithms to face the Computational 
Complexity Challenges; (3) new materials, alternative to the inorganic semiconduc-
tors used in current computers, to make easier the implementation of the new  

38 
P.L. Gentili 
 
algorithms. The rationale of this interdisciplinary research line is that every natural 
process is a kind of computation. It derives that many natural systems become proto-
types of new computational machines. For instance, the human nervous system is a 
wealthy source of inspiration for the development of Artificial Intelligence. The teams 
working in the field of Artificial Intelligence are driven by the ambitious project of 
understanding the foundations and running mechanisms of human mind in order to try 
to reproduce them artificially. Any success in the field of Artificial Intelligence is 
having positive impact on the challenges to Complexity. In fact, a deeper understating 
of human brain, being a prototype of Complex System, allows secrets of Natural 
Complexity to be unveiled. Moreover, a computational machine, mocking human 
intelligence, will be a new effective tool to tackle at least some of the Computational 
Complexity challenges. 
In a fairly recent book [3], the cognitive scientists Gallistel and King, in agreement 
with the neuroscientist Marr [4], argue that to understand a complex biological system 
like our brain, it is necessary to perform an analysis at three distinct levels. The first is 
the “computational level” and consists in describing the inputs, outputs and the task of 
the system. The second is the “algorithmic level” and consists in formulating algo-
rithms that might carry out those computations. Finally, the third is the “implementa-
tion level” and consists in searching for mechanisms of the kind that would make the 
algorithm work.  
In this contribution, we are presenting the results achieved so far in our group in 
the development of Chemical Artificial Intelligence [5] by following the methodology 
proposed by Gallistel, King and Marr. In paragraph 2, we are introducing the first 
results on the analysis of the Human Nervous System at the “computational” and 
“algorithmic” level. Specifically, I propose a combination of Fuzzy logic with the 
theory that describes human inference as Bayesian probabilistic event [6]. In the ensu-
ing paragraphs, we are describing the properties of few chemical systems that are 
useful in imitating the working mechanism and performances of the elements of the 
Human Nervous System. Since fuzzy logic is a good model of the human power of 
“computing” with words and taking decision in Complex situations [7], in paragraphs 
3 and 4 we are presenting the ways of implementing fuzzy logic at the molecular lev-
el. Any success in this field will push the Chemical Artificial Intelligence towards the 
human abilities of (i) taking decisions in Complex situations and (ii) recognizing vari-
able patterns.  
2 
Computational and Algorithmic Analysis of Human Nervous 
System 
The human nervous system [8] is a complex network of billions of nerve cells une-
venly spread throughout the body. It controls and rules the behavior of the entire body 
by catching, retrieving, processing, memorizing, sending either accurate or vague 
information. It allows humans to take decisions as responses to incoming stimuli and 
signals; it computes by using words or handling at the same time numbers and words. 
The human nervous system comprises three main elements: (a) the sensory system 

 
The Development of Chemical Artificial Intelligence Processing Fuzzy Logic 
39 
 
(SS), (b) the central nervous system (CNS), and (c) the effectors’ system (ES) [6] and 
it can be structurally and intrinsically compared with a Fuzzy Logic System (FLS). A 
FLS consists of three main elements (see Figure 1): (a’) the Fuzzifier, based on the 
partition of the input variables in fuzzy sets, and transforming crisp inputs in vectors 
of degrees of membership to the input fuzzy sets; (b’) the Fuzzy Inference Engine that 
contains IF-THEN rules linking input fuzzy sets with output fuzzy sets; (c’) the 
Defuzzifier, transforming the activated output fuzzy sets in crisp outputs. 
 
Fig. 1. The three elements of a FLS built according to the Mamdani’s method (see [9]) are the 
Fuzzifier, the Fuzzy Inference Engine and the Defuzzifier 
The sensory system collects specific physical and chemical stimuli. It codes for 
four aspects of a stimulus: modality (M), intensity (IM), time-evolution (IM(t)) and 
spatial distribution (IM(t,x,y,z)). The power to distinguish different modalities is due 
to the existence of specialized receptor cells. Receptor cells in our SS can be sorted 
out in photoreceptors collecting visible light; mechanoreceptors sensing mechanical 
stimuli; thermo-receptors perceiving thermal stimuli; chemo-receptors detecting and 
distinguishing several chemicals. Each sensory organ has a hierarchical structure. At 
the smallest level, that is at the molecular level, the fundamental elements are macro-
molecular switches, i.e. proteins. Then, at an upper level, i.e. at the cellular level, 
there are the sensory cells containing many replicas of the macromolecular switches 
that are spatially organized. Finally, at the highest level, i.e. at the organ level, there 
are many copies of the different cells, properly distributed in space according to the 
architectures of the organ itself. Each sensory organ plays like a Fuzzifier. The mani-
fold content of information of a stimulus, i.e. its value, intensity, time-evolution and 
spatial distribution is encoded as fuzzy information. The interaction between a stimu-
lus and each type of molecular switches allows the value of the stimulus to be en-
coded as degrees of membership of the stimulus to the different molecular switches. 
Such information is encoded as a vector of degrees of membership 
ML
μ
 (where ML 
stands for Molecular Level). The information of the intensity and its temporal evolu-
tion are encoded as degrees of membership of the stimulus to the sensory cells, i.e. as 

40 
P.L. Gentili 
 
the vector 
CL
μ
 (where CL stands for Cellular Level). The value of the membership 
function at the cellular level corresponds to the ratio of molecular switches activated 
per unit cell and per unit of time. Finally, the information regarding the spatial distri-
bution is encoded as degrees of membership of the stimulus to the properly arranged 
array of sensory cells, 
OL
μ
 (where OL stands for Organ Level). The overall fuzzy 
information 
T
μ  (where T stands for Total) is a composition of the three contribu-
tions, 
ML
μ
, 
CL
μ
, and
OL
μ
; it is deterministic and should be, in principle, reproduci-
ble and universal. However, there are many factors that contribute to limiting the 
reliability of sensory information about the world. Structural constraints on neural 
representations and computations, neural noise introduced in early stages of sensory 
coding, past experiences, and the temporary physiological state of the fellow receiv-
ing the stimuli contribute to limiting the reliability, reproducibility and the universal-
ity of sensory information about the world. Close values of 
T
μ  become indistin-
guishable and are grouped together into granules. Every human brain must deal with 
the resulting uncertainty to generate perceptual representations of the world. Cox’s 
theorem along with the Dutch Book argument tell us that the probability theory con-
stitutes a coherent way of reasoning under uncertainty [10, 11]. This leads to the idea 
that human brain performs probabilistic reasoning, whether for sensory processing, 
motor control or cognitive computing. Therefore, perception can be described as a 
subjective process of Bayesian probabilistic inference [12, 13]. If IM is the intensity of 
the stimulus of modality M, and XM represents the collection of cells sensitive to it, 
applying the Bayes’ rule, it derives that 
 
)
(
)
(
)
(
)
(
M
M
M
M
M
M
X
p
I
X
p
I
p
X
I
p
=
 
(1) 
The term 
)
(
M
M X
I
p
 represents the “posterior probability”. It is proportional to the 
product of the “prior probability” 
)
( M
I
p
 and the “likelihood” 
)
(
M
M I
X
p
. 
)
( M
I
p
 is 
the probability of each perception prior to receiving the stimulus: it represents knowl-
edge of the regularities and it is strongly subjective. In agreement with the theory of 
Bayesian inference generalized in fuzzy context [14], 
)
(
M
M I
X
p
 can be identified 
with the fuzzy information 
T
μ . The term 
)
(
M
X
p
 is the “plausibility” and is merely a 
normalization factor. Bayes formula implies that human perception is a trade-off be-
tween 
)
(
M
M I
X
p
 and 
)
(
M
I
p
[15]. Some perceptions may be more “prior probability” 
driven, and others more data (that is 
T
μ ) driven. The more noisy and ambiguous the 
perception features, the more relevant the role played by 
)
( M
I
p
, and the less repro-
ducible is the sensation. The population of XM sensory neurons activated by the 
stimulus, evokes the activity of a population of N’ downstream neurons (RM). The 
terms IM, XM and RM form a chain communication system. At each step along  
the communication chain, the uncertainty does not decrease. It rather increases due to 
the noise operating in the communication among neurons. Thus, the information that 

 
The Development of Chemical Artificial Intelligence Processing Fuzzy Logic 
41 
 
RM gives about IM cannot be greater than the information XM gives about IM, in 
agreement with the Law of Diminishing Information [16]: 
 
inf(RM@IM) ≤ inf(XM@IM) 
 (2) 
Finally, after one or more of these communication steps, our brain receives an infor-
mation about a stimulus of a specific modality. Such information is fuzzy granular 
due to the uncertainty and indistinguishability, and it is properly described by words. 
Most often, we receive multimodal stimuli that interact with more than one sensory 
system. Multisensory processing pieces signals of different modality if stimuli fall on 
(i) the same or adjacent receptive fields (according to the “spatial rule”) and (ii) 
within close temporal proximity (according to the “temporal rule”). Multisensory 
processing forms a total picture that differs from the sum of its unimodal contribu-
tions, a phenomenon called multisensory enhancement in neuroscience [17] or colli-
gation in the Information theory [16]. The principle of inverse effectiveness states that 
the multisensory enhancement is stronger for weaker stimuli. Physiological and be-
havioural experiments have shown that a number of brain areas, such as the Superior 
Colliculus [17], contain distinct monosensory, as well as multisensory neurons. 
Neuro-physiological data have revealed also cross-modal influences on unimodal 
brain areas. All these evidences suggest a role of both feed-forward connections from 
unimodal to multimodal areas and feedback connections from multimodal to unimo-
dal areas [17]. The human brain results a collection of intertwined Fuzzy Inference 
Engines, whose working principles still need to be more deeply studied and rational-
ized. So far, it is evident that since sensory modalities are not equally reliable and 
their reliability can change with context, multisensory integration may be assumed a 
Bayesian probabilistic inference. The posterior probabilities of Bayes formula (see 
equation 1) are actually granules of fuzzy information properly described by the use 
of words. The computation carried out by our brain triggers a response on the effec-
tors’ system. The ES can be assumed to be a collection of mutually interacting De-
fuzzifiers, whose properties still need to be analyzed.  
3 
Chromogenic Materials as Artificial Sensory Elements to 
Process Fuzzy Logic 
In human sensory systems there are many types of responsive molecules catching 
physical and chemical stimuli [5], as said before. These natural responsive molecules 
can be artificially imitated by chromogenic and fluorogenic materials that are switch-
able molecules sending signals of transmitted and emitted light, respectively. Before 
receiving any perturbation, chromogenic compounds are in a structural A state that is 
quite often uncolored. After a perturbation, they acquire a new structure B, and hence 
a new color. This chemical transformation is reversible: through a suitable stimulus, 
the system may be transformed back to the initial state. Chromogenic materials are 
classified according to the kind of external perturbation that promotes the conversion 
from A to B. There are photochromic species that are sensitive to UV-visible radia-
tion; piezochromic compounds that respond to mechanical forces; thermochromic 

42 
P.L. Gentili 
 
molecules sensitive to heat; electrochromic species responding to electric fields; 
acidichromic and metallochromic compounds that change colors in response to pro-
tons and metal cations, respectively; and so on [5]. To quantify the information a 
chromogenic species may send, it is useful to define its Colorability (C) through the 
equation below [19]: 
)
(
log
)
(
log
)
(
log
)
(
log
)
(
log
)
(
log
2
2
2
2
2
2
A
A
B
B
A
A
B
B
A
A
B
B
A
B
z
z
z
z
y
y
y
y
x
x
x
x
I
I
C
−
+
−
+
−
=
−
=
  
(3)  
where xi, yi, zi are the chromaticity’s coordinates of the i-th state, and Ii is the infor-
mation sent by the i-th state. From equation (3), it is easy to infer that the Colourabil-
ity of a chromogenic compound is large when there is a sharp color contrast between 
A and B; for example, when A is transparent in the visible region and B transmits 
radiations belonging only to a narrow band of visible wavelengths. 
The spectral features of chromogenic materials and hence the values of the chro-
maticity coordinates can be changed in continuous way by proper selection of the type 
of stimulus and by modulating its intensity. This property makes chromogenic com-
pounds the right candidates for processing the infinite-valued Fuzzy logic. Two strat-
egies may be followed [19, 20], herein described by using the same spirooxazine SpO 
(whose structure is shown in the upper left part of Figure 2).  
0.00.20.40.60.8 1.0
1.2 1.4
0.00
0.02
0.04
0.06
0.08
0.10
0.0
0.2
0.4
0.60.8
1.01.21.4
 
SpO
Cu
n
n
/
2 +
 
SpO
H
n
n
/
+
C
0.0 0.2 0.4
0.6
0.8
1.0
1.2
1.4
0.00
0.02
0.04
0.06
0.0
0.2
0.4
0.6
0.8
1.0
SpO
Al
n
n
/
3+
C
SpO
H
n
n
/
+
N
O
N
NO2
+
N
N
-
O
NO2
SpO
MC
+hν
Δ
300
400
500
600
700
800
0.0
0.2
0.4
0.6
0.8
1.0
A
λ (nm)
+hν
+H
+ OR +Al
+3
+Cu
+2
SpO
 
Fig. 2. Chromogenic properties of the spirooxazine SpO that under UV irradiation produces the 
merocyanine MC depicted in the upper left part. The absorption spectra recorded before any 
perturbation (black), after UV (blue), after UV and addition of H+ or Al3+ (orange), and after 
UV and addition of Cu2+ (yellow) are shown in the upper right plot. The lower plots show the 
trend of C as function of the molar ratios nH
+/nSpO, nCu
2+/nSpO (on the left), and nH
+/nSpO, 
nAl
3+/nSpO (on the right).  

 
The Development of Chemical Artificial Intelligence Processing Fuzzy Logic 
43 
 
SpO is photochromic: under UV irradiation, it produces MC which is blue colored. 
It is also acidichromic: if it is UV irradiated in the presence of H+ it gives rise to 
MCH+ which is orange. The same orange is obtained in the presence of Al3+, when the 
complex Al3+MC is produced. On the other hand, in the presence of Cu2+, the solution 
becomes yellow when Cu2+MC is formed. The plot of C versus the molar ratios 
nH
+/nSpO, nCu
2+/nSpO, and that of C versus nH
+/nSpO and nAl
3+/nSpO are shown in the low-
er part of Figure 2. They are nonlinear smooth functions [21] that can be exploited to 
build two Fuzzy Logic Systems. The shapes of the functions along with the partitions 
of the variables in fuzzy sets impose what fuzzy rules can be formulated. In particular, 
the asymmetric shape of C as function of nH
+/nSpO, nCu
2+/nSpO allows rules with the 
AND operator to be defined. On the other hand, the symmetric trend of C as function 
of nH
+/nSpO and nAl
3+/nSpO allows also rules with the OR operator to be formulated.  
The second strategy to process Fuzzy logic at the molecular level may be explained 
by exploiting further chromogenic and computational properties of SpO. These may 
be unfolded using physical and chemical inputs different from UV radiation, H+, Al3+ 
and Cu2+ ions. For instance, SpO has a Colorability that increases by reducing tem-
perature [19]. In fact, the kinetic constant of the MC→SpO process (kΔ) decreases by 
cooling. Therefore, the photostationary concentration of MC may be higher at lower 
temperature (T). SpO has also the ability to distinguish an α-aminoacid 
RCHNH3
+COO- with a small R group from one with a bulky R. The 
photomerocyanine MC interplays appreciably with glycine (where R is the tiny hy-
drogen atom), but not with tryptophan (where R is the large indole) [22, 19]. The 
supramolecular adduct between MC and glycine does not produce any spectral shift 
of the color band, but it exerts a “braking effect” on the bleaching kinetic constant of 
merocyanine and an increase in its Colorability [19]. Moreover, the analysis of the 
bleaching kinetics through the Maximum Entropy method [19] proves that 
merocyanine exists as a collection of many conformers. The properties of the confor-
mational distribution, i.e. its broadness and shape, may be influenced by T and gly-
cine. A high T produces a broad distribution of MC conformational substates. On the 
other hand, a high concentration of glycine shrinks the number of MC conformers 
because aminoacidic molecules duck only to few substates [19]. The set of conform-
ers under which MC exists is actually fuzzy. Its characteristics are context dependent. 
The fuzziness or vagueness of the set of conformers may be featured by the measure 
of its fuzzy entropy, through equation (4): 
 

=
−
−
+
−
=
n
i
i
i
i
i
K
H
1
10
10
))
1(
log
)
1(
)
(
log
(
μ
μ
μ
μ
 
 (4) 
where K is a constant equal to 1/n [23] with n being the number of conformers, and μi 
is the relative weight of the i-th substate. Therefore, a compound such as MC does 
exist as a context-dependent ensemble of substates, each one reacting with its own 
kinetic constant. If we represent the absorption spectra of the chromogenic species as 
vectors in the space of the La*b* color coordinates, a cycle of coloration and bleach-
ing for the photochromic SpO becomes a reversible rotation, shrinking and stretching 
of the vector accompanied by the production and erasure of fuzzy information [19]. 

44 
P.L. Gentili 
 
This latter case shows that molecules actually process Fuzzy logic. A compound 
does not react with only one kinetic constant, but with as many ki as are the different 
conformers. The set of conformers is a fuzzy set whose characteristics are context 
dependent. A chemical reaction will correspond to an event of fuzzy information pro-
cessing, which is ruled by the inter- and intramolecular forces. The Fuzziness of a 
chemical compound becomes particularly important when we consider macromole-
cules, like those we have in our cells, where we can have a huge number of conforma-
tional substates. The cellular processes should now be analyzed under this new point 
of view to appreciate the molecular language of living cells. 
4 
Excitable and Oscillatory Reactions as Prototypes of 
Artificial Neurons 
The nonlinear dynamical properties of neurons in the brain can be imitated through 
particular chemical transformations such as the Belousov-Zhabotinsky (BZ) reaction, 
which, in the whole, is a catalytic oxidative bromination of malonic acid by bromate 
in acidic medium [5]:  
 
2
2
2
.
2
2
3
3
4
)
(
2
)
(
3
2
2
CO
O
H
COOH
BrCH
COOH
CH
BrO
H
cat
+
+
⎯→
⎯
+
+
−
+
 
(5)  
Various metal ions or metallo-complexes, for example cerious ions or ferroin (i.e. tris-
(1,10-phenantroline)-iron(II)), can serve as the catalyst. Although the chemistry of the 
BZ reaction does not share anything with the chemistry involved in the workings of a 
neuron, the BZ reaction can reproduce the main neural dynamical behaviours. A neu-
ron is an example of a very far-from-equilibrium system, which, in its phase space, 
can trace a limit cycle or stand on a stable stationary state. In the former case, the 
neuron will show periodic spiking, whereas in the latter, it will be resting unless a 
strong perturbation will promote a provisional shift away from the initial condition, 
corresponding to the release of an action potential. The same holds for the BZ reac-
tion. When the BZ reaction is in oscillatory regime, it can be formally compared to 
the dynamical behaviour of pacemaker cells, i.e. neurons which spontaneously depo-
larize their axon hillock and fire action potentials, often at a regular rate. Although 
such natural oscillators have their own internal rhythm, external stimuli can alter their 
timing. In pacemaker cells, information about a stimulus is encoded by changes in the 
timing of individual action potentials, and it is used to rule proprioception and motor 
coordination for running, swimming, and flying. Similarly to neurons, the BZ reaction 
in oscillatory regime can be perturbed in its timing by both inhibitors and activators. 
The response of the system is phase dependent, where for phase of addition we mean 
the ratio φ = τ/T0: τ is the “time delay”, i.e. the time since the most recent spike oc-
curred, and T0 is the period of the previous oscillations. The addition of Br- leads 
always to a delay in the appearance of a spike. In other words, ΔT=Tpert-T0 (Tpert is the 
period of the perturbed oscillation) is always positive. The higher the phase of addi-
tion of Br-, the larger the ΔT. The addition of silver ions decreases the period because 
Ag+ removes bromide, forming a AgBr precipitate. In other words, Ag+ is an activator 

 
The Development of Chemical Artificial Intelligence Processing Fuzzy Logic 
45 
 
unless it is injected in small quantities and at low phase, inducing a slight lengthening 
of the period of oscillations [24]. An analytic investigation of the dependences of ΔT 
on the volumes of the KBr and AgNO3 solutions and their phase of addition, reveals 
that such relations are smooth and almost linear. Therefore, these relations are suit-
able to process fuzzy logic according to the first strategy described in the previous 
paragraph. Such possibility suggests that a real pacemaker cell could process fuzzy 
logic in much the same fashion as the BZ reaction does.  
Since the amazing computational performances of our brain relies mainly on the 
emergent properties of neural networks, it is important to study the coupling between 
artificial neurons. In real neurons, signal transmission is typically unidirectional and 
takes place by discrete pulses of neurotransmitters at synapses. Therefore, the attention 
must be focused on pulse-coupled oscillators. When two ferroin-catalyzed BZ oscilla-
tors, operating in continuously fed stirred tank reactors (CSTRs), are pulse-coupled 
through symmetric, reciprocal release of either inhibitor or activator or mixed, and at 
variable time delay, many dynamical regimes can be obtained [25]. Such dynamical 
regimes are examples of temporal patterns. They are achievable for different combina-
tions of effectors and different delay times of their release. These temporal patterns 
constitute the reasoning code for neurons and their nets, according to the neuroscience 
axiom that “neurons that fire together, wire together” [26]. The next step along this 
research path is to increase the number of coupled artificial neurons, study the emerging 
temporal patterns and see if fuzzy logic can ground on them. 
5 
Conclusion 
The computational and algorithmic investigation of the human nervous system is 
unveiling important properties of Complexity that is rooted on hierarchical processing 
of Information. The analysis of the human nervous system at the implementation level 
is spurring the design of a new generation of computing machines imitating the per-
formances of human intelligence. The new computing machines will be probably 
three-dimensional wetware containing encapsulated bistable reactions [5] playing like 
neurons. They will be encompassed by chromogenic and fluorogenic compounds [5] 
playing like sensory elements. The chromogenic and fluorogenic compounds will 
collect the external stimuli and they will encode them as fuzzy information. The 
communication inside the 3-D wetware will be based on light and photo/chemical 
waves. An intertwined system of fuzzy inference engine will allow to integrate gran-
ules of information and to compute with words and numbers. A success in the project 
of reverse engineering the nervous system will provide great benefits to solve the 
computational problems of variable patterns recognition.       
References 
1. TOP500 Project, http://www.top500.org 
2. de Castro, L.N.: Fundamentals of natural computing: an overview. Phys. of Life Rev. 4,  
1–36 (2007) 

46 
P.L. Gentili 
 
3. Gallistel, C.R., King, A.: Memory and the computational brain: Why cognitive science 
will transform neuroscience. Blackwell/Wiley, New York (2009) 
4. Marr, D.: Vision. A Computational Investigation into the Human Representation and Pro-
cessing of Visual Information. The MIT Press (2010) 
5. Gentili, P.L.: Small Steps towards the Development of Chemical Artificial Intelligent Sys-
tems. RSC Adv. 3, 25523–25549 (2013) 
6. Gentili, P.L.: The Human Sensory System as a collection of specialized Fuzzifiers: a con-
ceptual framework to inspire new Artificial Intelligent Systems computing with words. J. 
Intel. & Fuzzy Sys. (2014), doi:10.3233/IFS-141179 
7. Zadeh, L.A.: A New Direction in AI. AI Magazine 22, 73–84 (2001) 
8. Paxinos, G., Mai, J.K.: The Human Nervous System, 2nd edn. Elsevier (2004) 
9. Mendel, J.M.: Fuzzy logic systems for engineering: a tutorial. Proc. IEEE 83(3), 345–377 
(1995) 
10. Van Horn, K.S.: Constructing a logic of plausible inference: a guide to Cox’s theorem. Int. 
J. Approx. Reason. 34, 3–24 (2003) 
11. De Finetti, B., Machi, A., Smith, A.: Theory of Probability: a Critical Introductory Treat-
ment. Wiley, New York (1993) 
12. Mach, E.: Contributions to the Analysis of the Sensations (C. M. Williams, Trans.). Open 
Court Publishing Co., Chicago (1980) 
13. Knill, D.C., Pouget, A.: The Bayesian brain: the role of uncertainty in neural coding and 
computation. Trends Neur. 27(12), 712–719 (2004) 
14. Coletti, G., Scozzafava, R.: Conditional probability, fuzzy sets, and possibility: a unifying 
view. Fuzzy Sets and Systems 144(1), 227–249 (2004) 
15. Kersten, D., Mamassian, P., Yuille, A.: Object Perception as Bayesian Inference. Annu. 
Rev. Psychol. 55, 271–304 (2004) 
16. Kåhre, J.: The Mathematical Theory of Information. Kluwer Academic Publishers, Nor-
well (2002) 
17. Stein, B.E., Meredith, M.A.: The merging of the senses. MIT Press, Cambridge (1993) 
18. Driver, J., Spense, C.: Multisensory perception: beyond modularity and convergence. Curr. 
Biol. 10, R731–R735 (2000) 
19. Gentili, P.L.: The fuzziness of a chromogenic spirooxazine. Dyes and Pigments 110, 235–
248 (2014) 
20. Gentili, P.L.: The fundamental Fuzzy logic operators and some complex Boolean logic cir-
cuits implemented by the chromogenism of a spirooxazine. Phys. Chem. Chem. Phys. 13, 
20335–20344 (2011) 
21. Gentili, P.L.: Molecular Processors: From Qubits to Fuzzy Logic. ChemPhysChem. 12, 
739–745 (2011) 
22. Gentili, P.L., Ortica, F., Favaro, G.: Supramolecular interaction of a spirooxazine with 
amino acids. Chem. Phys. Lett. 444, 135–139 (2007) 
23. Al-sharhan, S., Karray, F., Gueaieb, W., Basir, O.: Fuzzy entropy: a brief survey. In: IEEE 
International Fuzzy Systems Conference, pp. 1135–1139 (2001) 
24. Gentili, P.L., Horvath, V., Vanag, V.K., Epstein, I.R.: Belousov-Zhabotinsky “chemical 
neuron” as a binary and fuzzy logic processor. Int. Journ. of Unconventional Computing 8, 
177–192 (2012) 
25. Horvath, V., Gentili, P.L., Vanag, V.K., Epstein, I.R.: Pulse-Coupled Chemical Os-
cillators with Time Delay. Angew. Chem. Int. Ed. 51, 6878–6881 (2012) 
26. Rabinovich, M.I., Varona, P., Selverston, A.I., Abardanel, H.D.I.: Dynamical Principles in 
Neuroscience. Rev. Mod. Phys. 78, 1213–1265 (2006) 

Weak Sensitivity to Initial Conditions for Generating
Temporal Patterns in Recurrent Neural Networks:
A Reservoir Computing Approach
Hiromichi Suetani
Department of Physics and Astronomy, Kagoshima University,
Kagoshima 890-0065, Japan
Abstract. A function for generating temporal patterns such as melody of the
music and motor commands for body movements is one of major roles in the
brain. In this paper, we study how such temporal patterns can be generated from
nonlinear dynamics of recurrent neural networks (RNNs) and clarify the hidden
mechanism that supports the functional ability of RNNs from reservoir comput-
ing (RC) approach. We show that when the reservoir (random recurrent neural
network) shows weak instability to initial conditions, the error of the output from
the reservoir and the target pattern is suﬃciently small and robust to noise. It is
also shown that the output from the spontaneous activity of the trained system
intermittently exhibits response-like activity to the trigger input, which may be
related to recent experimental ﬁndings in the neuroscience.
1
Introduction
A function for generating temporal patterns is one of major roles in the central nervous
system (CNS) ranging from melodies of music to sequences of commands for the body
movements such speech and hand-writing of letters. On the other hand, it has been
identiﬁed that there is a rich variety of nonlinear dynamics ubiquitously in the brain
from the single neural cell to the cortex[1]. Therefore, a question how generations of
functional patterns can be understood in terms of nonlinear dynamics is very important
from the viewpoint of neuroscience.
In real biological brains, asymmetry is more plausible than symmetry as a way of
connectivity among neurons in networks, and it has been also known that asymmetrical
recurrent neural networks exhibit chaotic dynamics[2] whereas symmetrical networks
such as Hopﬁeld network only yielding ﬁxed point attractors.
Recently, random recurrent neural networks (RNNs) have attracted considerable at-
tentions under the name of “reservoir computing” (RC) paradigm[3,4]. In the frame-
work of RC, connectivity matrices within a RNN and from the input to the RNN are
given at ﬁrst and they do not change with time, only the connectivity from the RNN to
readout neurons is updated so as to minimize, for example, the root-mean-squared error
between the output from the neural network and a given target (supervised) signal. Here,
RC basically learns the input-ouput relationship, i.e., a static nonlinear function from
the input to the output, so it is required that the dynamics in the RNN is “echo state”,
i.e., the state of the RNN is activated by the injection of the input ﬁrst (echo of the
c⃝Springer International Publishing Switzerland 2015
47
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_6

48
H. Suetani
input in the RNN), then going to the static state, so that there is a deterministic function
between the shape of the wave in the RNN and that of the input signal. Furthermore, a
signiﬁcant progress has been done by Sussillo and Abbott. In their paper[5], they pro-
posed an online learning extension RCs, called FORCE learning, and they explores that
not relaxation dynamics but sustained chaotic activity in RNNs plays a constructive role
for generating temporal patterns.
In this paper, we study how the performance of temporal pattern generations depends
on parameters that determine the dynamical properties of RNNs and clarify the hidden
mechanism supporting the functional ability of RNNs based on the RC paradigm. We
show that when the reservoir shows “weak” instability to initial conditions, the error
of the output from the reservoir and the target pattern is suﬃciently small and robust
to noise. It is also shown that the output from the spontaneous activity of the trained
system intermittently exhibits response-like activity to the trigger input, which may be
related to recent experimental ﬁndings in the neuroscience.
2
Model and Method
First, we introduce the architecture of a model used in this study. We consider the fol-
lowing system consisting of a continuous-time RNN (called a “reservoir”) with M input
units, N internal units, and L readout units deﬁned as
τ˙xn(t) = −xn(t) + g
N

n′=1
wrec
nn′ tanh(xn′(t))
+
M

m=1
win
nmum(t) +
L

l=1
wfb
nlzl(t) + bn, n = 1, ..., N,
(1)
(see Fig. 1) . Here, xn is the state variable of the n-th internal unit, um is that of the m-th
input unit, g is the parameter of gain control from other units, and γ is the parameter de-
termining the characteristic time-scale of dynamics. Also, as readout units, we consider
the following linear projection deﬁned as
zl(t) =
N

n=1
wout
ln tanh(xn(t)), l = 1, ..., L,
(2)
where zl is the state variable of the l-th readout unit. The connection weights are given
in a N × N matrix Wrec = (wrec
nn′) for connections between the internal units, a N ×
M matrix Win = (win
nm) for those between the input and the internal units, a N × L
matrix Wfb = (wfb
nl) for a feedback from the readout to the internal units, and a L × N
matrix Wout = (wout
ln ) for those between the internal and the readout units. The term bn
denotes bias of the n-th internal unit and is given by a Gaussian random number obeying
N(0, 10−3)
In the framework of RC, the matrices Wrec, Win, and Wfb are given at ﬁrst and do
not change with time, only the readout matrix Wout is updated so as to minimize, for
example, the root-mean-squared error between the output z(t) from the neural network

Weak Sensitivity to Initial Conditions for Generating Temporal Patterns
49
and the target (supervised) signal ztarget(t). In FORCE learning[5], synaptic weights in
Wout are updated in an online way using, e.g., the recursive least squares (RLS) method.
We also use RLS in this paper. It is supposed here that the matrix Wrec is a sparse one,
i.e., the probability of the existence of connection between two units is p = 0.1, and its
intensity obeys Gaussian distribution as wrec
nn′ ∼(g/
√
N)N(0, 1) if there is a connection
between the n and the n′-th units. Also, elements in Win and Wfb are supposed to obey
the uniform distribution as win
nm ∼U(−0.75, 0.75) and wfb
nl ∼U(−0.5afb, 0.5afb), re-
spectively. The values of g and afb are varied in the following as the control parameters,
and other parameters are set to τ = 10 [msec] of the characteristic time scale of the
reservoir, N = 103 units in the reservoir, and M = 1 input channel.
 on    
input
output                         teacher
gradient learning
/recursive least square
Fig. 1. Schematic plot of the reservoir network employed in this paper
The purpose of this network system now is to generate an output signal z(t) close
to a target signal f(t) as possible as one can with precise timing. Here, the system is
stimulated by a pulse with the duration ΔT = 50[msec] as the input trigger in order to
train the network to generate the desired output at this timing. And, this network system
is trained from the beginning of the trigger stimulus followed by the presentation of the
target signal f(t). We repeat this process(called an “episode”) up to 102 times, then we
check whether the desired temporal pattern can be generated by giving a trigger pulse to
the trained network. In particular, we focus on how the performance of learning depends
on the parameters of the system such as the nonlinear gain g and the feedback strength
afb, and what kinds of dynamical properties supporting the performance.
3
Numerical Simulations and Results
As a demonstration, we consider a linear combination of sinusoidal signals (i.e., L = 1
in Eqs. (1) and (2) ) f(t), shown in Fig. 2(a), as the target pattern. Figure 2(b) shows
the outputs z(t) of the system after training for the parameters g = 1.3 and afb = 0.5.
In training process, we add small Gaussian white noise with the standard deviation
σ1 = 10−4, as well as noise with σ2 = 10−2 in generating process. Here, time series
depicted in diﬀerent colors indicate diﬀerent trials (trajectories starting from diﬀerent

50
H. Suetani
initial conditions) and a short trigger pulse is given at t = 103 [msec] for each trial. We
can see that the RNN can generate the temporal patterns which are very similar to the
target f(t) over trials starting from diﬀerent initial conditions. Then, after generating
the desired pattern, the system goes back to chaotic state again.
(a)
(b)
Fig. 2. (a) Target signal f (t) and (b) outputs z(t) from the trained network for several diﬀerent
trials (initial conditions). Time series in diﬀerent colors indicate diﬀerent trials. Gaussian white
noise with the standard deviation σ = 10−2 is injected to the system.
A success of learning is shown in Fig. 2 (b), but the performance depends on the
choice of parameters of the system. Figure 3(a) shows the outputs z(t) of the system
after training for three diﬀerent values of g (afb is set to 0.5). In Ref.[2], it is said that a
reservoir recurrent network in Eq. (1) exhibits chaos for g > 1, so the results in Fig. 3(a)
indicate that neither “no”-chaos and “strong”-chaos in the inside RNN can generate the
desired pattern nor even have reproducibility over diﬀerent trials. We note here that the
existence of noise and the feedback term yields sustained oscillatory behaviors in the
case of g < 1 after training. Figure 3(b) also shows the outputs z(t) of the system after
training for three diﬀerent values of afb (g is set to 1.3). We can see that the feedback
term induces the reproducibility of the output.
Figure 4 shows the performance of the system as a function of g for three diﬀerent
noise strength σ2 in generation process. Here the “performance” means the average of
the root-mean-squared error between the output z(t) from the RNN and the target f(t)
over diﬀerent trials, deﬁned as
E(z, f) = ⟨

1
T
 T
0
(z(t) −f(t))2dt ⟩,
(3)
where T = 1.2 [sec] (the length of the target signal) and the average ⟨· · · ⟩is taken over
102 diﬀerent trials. In the noiseless case (σ2 = 0), the reservoir can generate the desired
pattern successfully even for g ∼1. It is, however, seen that the error becomes quite
large for the larger noise (σ2 = 10−2) for g ∼1, i.e., susceptible to noise for coherent
generation of the pattern. On the other hand, the error grows rapidly for g > 1.4 for
all noise cases. There is an optimal value of g around at g = 1.2 which keeps the error
suﬃciently small and robust to noise.

Weak Sensitivity to Initial Conditions for Generating Temporal Patterns
51
(a)
(b)
Fig. 3. (a) Outputs z(t) from the trained network for several diﬀerent trials and for three diﬀerent
values of g = 0.9, 1.2 and 1.5 (afb is set to 0.5). (a) Outputs z(t) from the trained network for
several diﬀerent trials and for three diﬀerent values of afb = 0, 0.5 and 1.0 (g is set to 1.3).
2
2
2
Fig. 4. The averaged error E(z, f ) vs. the nonlinear gain g for three diﬀerent noise cases. afb = 0.5.
The number of 102 trials is used for taking the average for each parameter case.
Furthermore, we investigate how the performance depends on the feedback term.
Figure 5(a) shows that the norm of the readout matrix (or vector if the target signal
is scalar) ∥Wout∥as a function of the number of episodes in training process for three
diﬀerent values of afb. The norm ∥Wout∥converges to stationary state after suﬃcient
trainings, and the value for the no-feedback case (afb = 0) is quite large compared to
other cases (afb = 0.5 and 1.0). This result indicates that the frequency that coeﬃcients

52
H. Suetani
in Wout for afb = 0 take both large positive and negative values is high compared to
those for the cases of afb = 0.5 and 1.0 because the amplitude of the output z(t) is same
order for all three cases, which causes a susceptibility to noise. Figure 5(b) shows the
error averaged over 102 trials against the strength afb t of the feedback term. We see that
the existence of the feedback actually reduces the error. But, we also note that too strong
feedback may also induce the instability to the system so the performance decreases in
such a case.
# of episodes
error
(a)
(b)
Fig. 5. Eﬀects of the feedback term. (a) the norm ∥Wout∥vs. the number of episodes for three
diﬀerent values of afb and (b) the averaged error vs. afb.
Now, we turn our interests to a problem what kinds of dynamical properties deter-
mine the “optimality” of the performance (g ∼1.2 in the current case). In order to
explore this problem, we ﬁrst investigate how a cloud of points (each point corresponds
to a single trial) on the attractor of the reservoir is condensed by the trigger pulse in-
put. Figure 6 shows the time evolution of the “volume” ρ of the cloud of points after
g=1.6
g=1.5
g=1.4
g=1.3
g=1.55
g=1.45
g=1.35
g=1.25
g<1.2
1 [sec]
Fig. 6. Time evolution of the volume ρ of the reservoir for several diﬀerent values of g

Weak Sensitivity to Initial Conditions for Generating Temporal Patterns
53
injecting the trigger pulse input at time t = 0 for several diﬀerent values of g. Here, the
volume ρ(t) at time t is determined as
ρ(t) =
N

j=1
λ j(t),
(4)
where λ j(t) is the j-th eigenvalue of principle component analysis (PCA) at time t. We
prepare J = 103 diﬀerent initial points and evolve these points under the time evolution
law Eq. (1) for Wout = 0 (i.e., there is no feedback) to perform PCA and calculate
ρ(t) at each time step. In Fig. 6, ﬁrst the volume ρ(t) decreases rapidly for all cases,
which indicates that the reduction of the sensitivity to initial conditions is induced by
the trigger pulse input. After such process, the volume ρ(t) increases rapidly again and
converges to the initial volumes ρ(0) with time for g > 1.4 due to the existence of
chaotic nature of the reservoir. Remarkably, however, ρ(t) does not recover to ρ(0) for
g < 1.2, this fact means that the reservoir “remembers” the trigger input for long time.
For 1.25 ≤g ≤1.35, ρ(t) seems recover to ρ(0), but their relaxations are slow.
g=1.2
g=1.2
g=1.3
1 [sec]
(a)
(b)
g=1.2
1 [sec]
Fig. 7. Growth of the error of initial conditions (a) before training and (b) after training

54
H. Suetani
Therefore, we also investigate temporal change of the initial error δx(t) of the reser-
voir for longer time periods. Figure 7(a) shows the error growth δx(t) of the initial
conditions for the reservoir, which is averaged over J = 102 trials up to 10 seconds.
The value of δx(t) seems ﬂuctuate around some constant for g = 1.2 whereas the error
increases with time for g = 1.3. But, for more longer time scale, up to 5 × 102 seconds,
the error δx(t) grows with ﬂuctuation even for g = 1.2 as shown in the inside ﬁgure of
Fig. 7 (a). Although we have not clariﬁed whether this sensitivity initial conditions ex-
hibits power-law behavior[6] or not, this result suggests that there is “weak” sensitivity
to initial conditions for g = 1.2 yielding optimal performance for generating temporal
patterns. Figure 7(b) also shows the error growth δx(t) of the initial conditions for the
system after training, i.e., the system also includes the feedback term that reﬂects the
eﬀect of training. Here, we can see that there are large ﬂuctuations, divergence or con-
vergence of the error growth, which reﬂects the existence of a coherent object associated
with the target pattern f(t) is embedded in the state space.
Finally, we compare between responses to the trigger input and “spontaneous activ-
ity” (no input signal) of the trained system. Figure 8 shows such a comparison. There is
a remarkable ﬁnding that a time-delay embedding of the spontaneous activity (Fig. 8(b))
is very similar to that of responses to the trigger input (Fig. 8(a), here diﬀerent colors
indicates diﬀerent trials). This result suggests that the system sometimes “recalls” the
target pattern even if there is no trigger input to the system. Figure 8(c) shows the time
series of z(t) in the case the spontaneous activity. Actually, there is a resonant-like be-
havior, i.e., there is intermittent traces of the target pattern with a few cycles.
(a)
(b)
(c)
resonant like behavior
Fig. 8. Comparison between (a) response to the trigger pulse (diﬀerent colors indicate diﬀerent
trials) and (b) spontaneous activity of the output z(t) in the time-delay embedding space. (c) Time
series of the output z(t) in the case of spontaneous activity.

Weak Sensitivity to Initial Conditions for Generating Temporal Patterns
55
4
Conclusion
In conclusion, we have investigated a problem about generation of coherent temporal
patterns using nonlinear dynamics of recurrent neural networks based on the reservoir
computing approach. In this study, we considered a situation that a short pulse is em-
ployed to reset trajectories to a “pseudo” stable ﬁxed point and the transient dynamics
starting from this ﬁxed point-like state is harnessed so that the corresponding readout
pattern z(t) trace the target pattern f(t) by FORCE learning. For an example of the tar-
get pattern, we found that there is an optimal value of g where the error of the output
from the reservoir and the target pattern is suﬃciently small and robust to noise. Here,
weak instability to initial conditions of the reservoir is important. We do not show re-
sults against other examples of the target pattern, but similar optimality is obtained if
the length and characteristic time-scale of the target patterns are same as those used in
this paper. We also showed that the output from the spontaneous activity of the trained
system intermittently exhibits response-like activity to the trigger input. This ﬁnding
may relate to experimental observations by Arieli et al.[7] and Kenet et al.[8]
Acknowledgements. This study is supported by “Decoding and Controlling Brain In-
formation”, PRESTO, JST, Japan. We thank M. Kawato (Supervisor of the project) for
supporting. MATLAB codes for numerical simulations in this paper are based on pre-
liminary ones prepared by D. Rodriguez. We thank him for preparations to start this
study. We also thank I. Tsuda, Y. Yamaguchi, S. Kuroda, Y. Katori, T. Leleu, and K.
Aihara for fruitful discussions and comments.
References
1. Tsuda, I.: Behavioral and Brain Sciences 24, 793–810 (2001)
2. Sompolinsky, H., Crisanti, A., Sommers, H.J.: Phys. Rev. Lett. 61, 259 (1988)
3. Maas, W., Natschl¨ager, T., Markam, H.: Neural Comp. 14, 2351 (2002)
4. Jaeger, H., Haas, H.: Science 304, 78 (2004)
5. Sussillo, D., Abbott, L.F.: Neuron 63, 544 (2009)
6. Costa, U.M.S., Lyra, M.L., Plastino, A.R., Tsallis, C.: Phys. Rev. E 56, 245 (1997)
7. Arieli, A., Sterkin, A., Grinvald, A., Aertsen, A.: Science 273, 1868 (1996)
8. Kenet, T., Bibitchko, D., Tsodyks, M., Grinvald, A., Arieli, A.: Nature 425, 954 (2003)

Synchronization and Control
in Modular Networks of Spiking Neurons
Oleg V. Maslennikov⋆, Dmitry V. Kasatkin, and Vladimir I. Nekorkin
Institute of Applied Physics of the Russian Academy of Sciences,
Nizhny Novgorod, Russia
{olmaov,kasatkin,vnekorkin}@neuron.appl.sci-nnov.ru
Abstract. In this paper, we consider the dynamics of two types of
modular neural networks. The ﬁrst network consists of two modules of
non-interacting neurons while each neuron inhibits all the neurons of an
opposite module. We explain the mechanism for emergence of anti-phase
group bursts in the network and showed that the collective behavior un-
derlies a regular response of the system to external pulse stimulation.
The networks of the second type contain modules with complex topol-
ogy which are connected by relatively sparse excitatory delayed coupling.
We found a dual role of the inter-module coupling delay in the collective
network dynamics. First, with increasing time delay, in-phase and anti-
phase regimes, where individual spikes form rhythmic modular burst-like
oscillations, alternate with each other. Second, the average frequency of
the collective oscillations in each of these regimes decreases with increas-
ing inter-module coupling delay.
Keywords: Complex networks, nonlinear dynamics, neurodynamics,
synchronization, delayed coupling, maps.
1
Introduction
Many natural systems, including biological ones, are generally organized as com-
plex networks consisting of several modules (or groups, populations)—sets of
somehow grouped elements [1–3]. There are several features characterizing such
modular networks. First, each module can often be considered as a unit which op-
erates and functions as a whole. Second, the distance between modules sometimes
results in a coupling delay which in turn inﬂuences the network dynamics. Third,
the connectivity in modular networks is frequently organized in such a way that
inter-module connections are sparser than intra-module ones. In this paper, we
consider two types of modular networks representing important neural systems.
In the ﬁrst case the network consists of two modules coupled by inhibitory con-
nections while the neurons within the modules are not connected. Such network
models an essential part of so-called half-center oscillators—common circuits of
central pattern generators responsible for production and control of rhythmic
⋆Corresponding author.
c
⃝Springer International Publishing Switzerland 2015
57
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_7

58
O.V. Maslennikov, D.V. Kasatkin, and V.I. Nekorkin
motor activity in animals. The second network consists of two modules con-
nected by excitatory links with delay and each module is a subnetwork with
complex topology. Such a conﬁguration reproduces main properties of local cor-
tical structures or in vitro neuronal cultures. In all cases a neuron is modeled by a
discrete-time two-dimensional system which generates irregular spiking activity,
and a coherent collective behavior emerges in the network due to the interplay
between connectivity and the individual oscillations.
2
Map-Based Model of Neural Activity
In the study we use a dynamical map model with stochastic perturbations in
which an isolated neuron exhibits occasional spikes at random moments of time.
The dynamics of a neuron is described by the following two-dimensional discrete-
time model [4–6]:

xn+1 = xn + F(xn) −βH(xn −d) −yn + Isyn
n
,
yn+1 = yn + ε(xn −J).
(1)
where x mimics the qualitative behavior of the neural membrane potential and
y models the eﬀects of relatively slow ionic currents governing the transient
behavior of the neuron. The nonlinear functions F(x) and H(x) are written
F(x) = x(x −a)(1 −x),
(2)
H(x) =

1,
x ≥0,
0,
x < 0.
(3)
where 0 < a < 1. The parameters β and d control the shape of bursting oscil-
lations, ε is a time scale for y, and J deﬁnes neural excitability. For J < Jmin,
where Jmin = (1 + a −(1 −a + a2)1/2)/3 and the function F(x) has a minimum
at x = Jmin [Fig. 1a], the neuron is in a resting state (excitable regime). In
contrast, for J > Jmin the neuron generates spike sequences and the larger J
is, the more spikes in a sequence. The phase portrait of the typical dynamics
generated by the model is shown in Fig. 1a. Note that the neurons within the
networks diﬀer by the value of J so they have diﬀerent average spiking frequen-
cies. Waveforms with two diﬀerent excitation levels are shown in Fig. 1b and
1c. Coupling between the neurons is modeled using the input synaptic current
as deﬁned by
Isyn
n
= −gn(xpost
n
−ν),
(4)
where g is the synaptic conductance, xpost is the membrane potential of the post-
synaptic neuron, and ν is the reversal potential. The dynamics of the synaptic
conductance g is triggered by a presynaptic spike and relaxes in accordance with
the dynamics of the following one-dimensional map:
gn+1 = γgn + (1 −γ)gmaxH(xpre
n
−θ).
(5)
Here γ (0 < γ < 1) deﬁnes a relaxation rate for the conductance, gmax is a
maximum synaptic conductance, θ is a threshold parameter.

Synchronization and Control in Modular Networks of Spiking Neurons
59
Fig. 1. Phase portrait of the map (1) for a = 0.1, ε = 10−4, β = 0.5, d = 0.4,
J = 0.04, with small additive Gaussian noise. (b,c) Waveforms of the map (1) for
diﬀerent excitation levels (b) J = 0.04 and (c) J = 0.044.
3
Modular Network with Inhibitory Coupling
The onset of switching activity in groups of reciprocally coupled populations of
neurons is a typical element of many central pattern generators responsible for
production and control of rhythmic motor activity in animals. A rather com-
mon circuit found in many central pattern generators networks is a half-center
oscillator that acts as a core of rhythm generation and control. Many half-center
oscillators are formed by two neurons or two groups of neurons linked together
by a reciprocal inhibition. Such simple networks play an important role in the
control of motor activity patterns such as walking, ﬂying, and swimming. In
this section we study the dynamical properties of half-center oscillators in the
populations of reciprocally inhibiting neurons. We focus on the case of intrinsi-
cally not bursting neurons in which the onset of switching anti-phase oscillations
is supported by the eﬀect of post-inhibitory rebound. The neurons within each
group do not interact with each other; however, each neuron in one group inhibits
all neurons in the other group. Consider the network of two neuron populations
schematically shown in Fig. 2. In our example each group contains 100 uncoupled
Fig. 2. Modular network in the form of two groups of neurons inhibiting each other. For
simplicity, synaptic links from only one presynaptic neuron in each module is shown.

60
O.V. Maslennikov, D.V. Kasatkin, and V.I. Nekorkin
neurons and each neuron inhibits all the elements from the other group [7]. First
we consider the collective dynamics of the network operating autonomously. Due
to the intergroup coupling, irregular action potentials of one group arise in time
intervals that do not overlap with those of another group, causing the alternation
of group activities. Therefore, the waveforms of the average activities of the two
groups are anti-phase bursts. It should be noted that an increase of the value of
Jmean leads to an increase of the bursting period. To study how the excitability
level of the network and the strength of inhibitory coupling aﬀect the param-
eters of collective activity, we have calculated spectrograms of group bursting
oscillations for diﬀerent Jmean and gmax. We set the sampling frequency equal
to 500. Some results are illustrated in Figs. 3a and 3b, which correspond to two
diﬀerent values of the coupling strength gmax. One can clearly see that there is
a threshold value of Jmean corresponding to the emergence of bursting activity.
This threshold level decreases as the value of gmax increases. Another property
of these oscillations is that the average bursting frequency decreases with an in-
crease of the excitation level of the network, Jmean. One of the most important
elements in capturing the behavior of central pattern generators is the dynam-
ics of its response to the external sensory or control stimulus. In the case of
the half-center oscillator, this dynamics is related to the phase reset properties.
Considering the eﬀects of phase reset in the network, we apply a short stimulus
with A = −0.01 and △T = 20ms to all the elements in one group and examine
the phase of new bursting oscillations after the transient is complete. Figure 4a
shows that the network responds diﬀerently depending on the network state
when it receives a stimulus. There are some moments (during the active phase
of bursts) when the stimulus hardly disturbs the network oscillations; however,
there are moments (during the passive phase of bursts) when the stimulus can
lead to a signiﬁcant phase shift. To quantify this property we plotted the phase
resetting curve shown in Fig. 4b. An interesting question that we address in this
study is how the model of such a large network of reciprocally inhibiting groups
of neurons can be replaced with a pair of reciprocally inhibiting units. Further,
we ask whether the units in the pair have parameters similar to the neurons in
0
5
10
15
20
Frequency (Hz)
0
0.05
0.1
Jmean
Power (dB)
0
0.05
0.1
Jmean
-160
-120
-80
-40
-160
-120
-80
-40
(a)
(b)
0
5
10
15
20
Frequency (Hz)
Fig. 3. Spectrograms of group bursting for (a) gmax = 10−3, (b) gmax = 5 × 10−3,
σJ = 10−2

Synchronization and Control in Modular Networks of Spiking Neurons
61
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0
2
4
t (s)
0
0.2
1
φs
Δφ
φs
0.4
0.6
0.8
-0.1
0.2
0
0.1
x
(a)
(b)
Fig. 4. (a) Phase reset in two reciprocally inhibiting groups. Average activity of one
group is shown for diﬀerent moments of stimulation. (b) Corresponding phase resetting
curve.
the group. The simulations show that there are no such pronounced oscillatory
properties in a pair of spiking neurons mutually inhibiting each other as in the
network of two groups. The neurons that produce a robust rhythm of bursting
operating in networks generate irregular bursts with ﬂuctuating duration oper-
ating in the case of a reduced network model. We argue that it is reasonable
to consider two mutually inhibiting units with spike-bursting behavior that is
observed in the group average activity. To replicate this regime, we choose values
of β and d in such a way that each unit generates a sequence of spikes. By tuning
other parameters one can obtain the similar dynamical properties in the reduced
model as in the network model. This allows us to reveal the dynamical mecha-
nisms of emergence of anti-phase bursting and the phase reset in the interacting
groups.
4
Modular Networks with Excitatory Delayed Coupling
In this section we consider a modular network where intra-module coupling is in-
stantaneous while the elements of diﬀerent modules interact with time delay [8].
Schematically the examples of modular networks are shown in Fig. 5. Such mod-
els reﬂect the property of cortical networks that are spatially remote from each
other and thus they interact with time delay. As a module, we use complex net-
works with three diﬀerent topologies—a random Erd¨os-R´enyi network (Fig. 5a),
a small-world Watts-Strogatz network (Fig. 5b), and a scale-free Barab´asi-Albert
network (Fig. 5c)—each of them more or less reproduces structure properties of
real neural networks. The interaction between the modules is performed by rela-
tively sparse connections with time delay. For the intra-module topologies indi-
cated we show that with increasing time delay, in-phase and anti-phase regimes
of modular synchronization alternate with each other. Moreover, with increas-
ing time delay the network oscillations change their frequency, namely, within
every delay interval corresponding to a certain synchronization regime the fre-
quency decreases on the average. Also we study how the collective activity is

62
O.V. Maslennikov, D.V. Kasatkin, and V.I. Nekorkin
(a)
(b)
( )
с
Fig. 5. Schematic structure of a modular network consisting of two interacting sub-
networks. The nodes within each module are coupled by directed links into a network
with a certain topology: (a) a random Erd¨os–R´enyi network, (b) a small-world Watts–
Strogatz network, and (c) a scalefree Barab´asi–Albert network. Intermodule interaction
is organized by a relatively small number of random excitatory connections.
inﬂuenced by the inter-module coupling strength and the average spike rate of
individual neurons. Consider a network which is constituted by two interacting
subnetworks-modules schematically shown in Fig. 5. Each module consists of
N = 100 active nodes coupled with each other by directed links which model
the chemical synapses. Following the experimental observations, we set 80%
of the total number of neurons sending excitatory connections while the rest
20% neurons—inhibitory. We consider three diﬀerent algorithms for generating
an intra-module connection topology: a random Erd¨os-R´enyi network, a small-
world Watts-Strogatz network and a scale-free Barab´asi-Albert network, in every
case with directed links. The connections between the modules are random and
relatively sparse: 5% of nodes in one module send directed excitatory links to
random nodes of another module (note that the nodes are chosen from those
80% which send excitatory connections inside their module they belong to). We
analyze the inﬂuence of network modularity on the collective dynamics so we
are mainly interested in a role of the parameters of inter-module interaction: the
coupling strength and delay. The parameters of intra-module coupling and indi-
vidual dynamics are chosen in such a way that each module separately displays
incoherent activity [see collective dynamics of non-interacting small-world mod-
ules in Fig. 6a,b] which is achieved by relatively weak intra-module connections
and diﬀerent J-values for network nodes.

Synchronization and Control in Modular Networks of Spiking Neurons
63
0.03
0.04
0.05
0.06
xmean
1,2
-0.04
0
0.04
0.08
0.12
0.16
xmean
1,2
0.02
0.04
0.06
0.08
0
0.04
0.08
0.12
0.16
0
1000
2000
xmean
1,2
xmean
1,2
(a)
(b)
( )
с
(d)
(e)
(f)
(g)
(h)
100
200
0
0.4
# neuron
x
100
200
0
0.4
# neuron
x
100
200
0
0.4
# neuron
x
0
1000
2000
100
200
0
0.4
n
# neuron
x
n
Fig. 6. Spike rastrograms (left) and the average module activity (right). (a,b) Two
uncoupled modules; (c,d) coupled modules for go = 0.02 and in the absence of time
delay; (e,f) the same coupling strength, the delay is τ = 10; (g,h) the same coupling
strength, the delay is τ = 50.
Adding intermodule connections (so far without delay), as it was described
above, leads to that the average module activity becomes synchronous rhythmic
oscillations when the coupling strength go exceeds some critical value [Fig. 6c,d].
In what follows these oscillations are called bursts of average activity, or group
bursts, and we distinguish in them active phases—time intervals during which
the averaged module activity is above x = J0, and passive phases in between
the active ones. At the nodal level, spike events mainly happen during the active
phases of group bursts, however irregularity of the individual dynamics remains
and there is no strict periodicity of the individual oscillations.
If a small time delay is introduced into the intermodule coupling then the
regime of synchronous group bursts remains almost unchanged. For some crit-
ical value of the time delay, the neuronal dynamics becomes incoherent [see
Fig. 6e,f] as it was in the case of uncoupled modules but in long enough time
series one can ﬁnd short intervals during which synchronous group bursts ap-
pear. Further increasing of the time delay leads to a regime of rhythmic oscilla-
tions [Fig. 6g,h], however, in the form of anti-phase group bursts whose average
frequency is markedly higher than that of the in-phase bursts at small delays
[Fig. 6c,d]. Thus, with increasing time delay in the inter-module coupling, the
in-phase group activity is replaced by the anti-phase one as well as one observes
the change in the frequency of group oscillations. We have analyzed the spectra
of group activity and the regimes of modular synchronization depending on the
time delay for three intra-module topologies considered, the results are shown in

64
O.V. Maslennikov, D.V. Kasatkin, and V.I. Nekorkin
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
τ
σ
0
50
100
150
200
τ
0
50
100
150
200
τ
-140
-100
-60
P (dB)
0.01
0.02
0.03
F
(a)
(b)
( )
с
(d)
(e)
(f)
I
II
III
IV
V
VI
I
II
III
IV
V
VIVII
I
II
III
IV
V
VI
VII
Fig. 7. (a),(c),(e) Spectrogram of the average module activity depending on the inter-
module coupling delay τ; (b),(d),(f) module synchronization degree σ(τ) for J0 = 0.045
and go = 0.02. The intra-module connection topology is: (a),(b) random Erd¨os–R´enyi,
(c),(d) small-world Watts–Strogatz and(e),(f) scale-free Barab´asi–Albert.
Fig. 7. Despite the diﬀerence in the intra-module connectivity, the impact of the
delayed coupling is qualitatively similar for all three cases. One can distinguish
in Fig. 7 the delay intervals labeled by the Roman numerals corresponding to
diﬀerent types of collective dynamics. In the intervals I and V where the val-
ues of the synchronization parameter σ are close to one, there is the in-phase
regime of modular synchronization. In the intervals III and VI where σ is close
to zero, there is the anti-phase regime. The intervals II, IV, and VI correspond
to transient regimes where the modules display incoherent activity without any
pronounced rhythm. Note that in the intervals of the in-phase and anti-phase
regimes, the frequency of lower harmonic as well as its multiples decreases with
increasing delay. At the transition from one regime (e.g., anti-phase in the inter-
val III) to another (in-phase in the interval V), the frequency abruptly rises, and
the magnitude of the jump is higher for smaller time delays. It is interesting to
note that the width of the interval I corresponding to the in-phase group bursts
at a small delay, is always less than that of other intervals, in particular, the in-
terval V where there is also the in-phase regime. The impact of the inter-module
coupling delay and strength on a synchronization regime and the frequency of
bursts generated is summarized in Fig. 8. The two-parameter diagrams show
the alternation of the in-phase and anti-phase regimes [Fig. 8a], as well as the
decrease in the frequency of the ﬁrst harmonic of group bursts within each of
these regimes with increasing τ [Fig. 8b]. Note that an unobvious result follows

Synchronization and Control in Modular Networks of Spiking Neurons
65
0
50
100
150
200
0
0.005
0.01
0.015
0.02
0.025
0.03
τ
go
0
0.5
1
σ
3
4
5
6
7
8
9
10
x 10
-3
0
50
100
150
200
τ
Fb
(a)
(b)
Fig. 8. (a) Dependence of the synchronization degree σ and (b) the frequency of the
ﬁrst harmonic of group bursts Fb on τ = 10 and go
from Fig. 8b: with increasing inter-module coupling strength go, the frequency of
group bursts increases for a ﬁxed delay, while in the ﬁrst zone of in-phase regime
the opposite case takes place—a decrease in the frequency with increasing go.
5
Conclusions
In this paper, we considered modular networks of interacting neuron-like ele-
ments which generate irregular spike sequences. In the ﬁrst example the mod-
ules are groups of non-interacting neurons which inhibit all the neuron of another
module. We explained the mechanism for emergence of anti-phase group bursts
in the network and showed that the collective behavior underlies a regular re-
sponse of the system to external pulse stimulation. In the second example the
modules are subnetworks with complex topology which are connected by sparse
excitatory delayed coupling. We found a dual role of the inter-module coupling
delay in the collective network dynamics. First, with increasing time delay, in-
phase and anti-phase regimes, where individual spikes form rhythmic modular
burst-like oscillations, alternate with each other. Second, the average frequency
of the collective oscillations in each of these regimes decreases with increasing
inter-module coupling delay. An increase in the inter-module coupling strength
and the network excitation level leads to an enhancement of synchronization in
the in-phase as well as anti-phase regimes. The frequency of group bursts rises
with increasing inter-module coupling strength for all delay intervals except the
ﬁrst one: for a small time delay in the in-phase regime the frequency decreases
with increasing coupling strength.
Acknowledgments. This work was supported by the Russian Science Founda-
tion (Project No. 14-12-01358).

66
O.V. Maslennikov, D.V. Kasatkin, and V.I. Nekorkin
References
1. Arenas, A., Diaz-Guilera, A., Kurths, J., Moreno, Y., Zhou, C.: Synchronization in
complex networks. Phys. Rep. 469, 93–153 (2008)
2. Bullmore, E., Sporns, O.: Complex brain networks: graph theoretical analysis of
structural and functional systems. Nat. Rev. Neurosci. 10, 186–198 (2009)
3. Girvan, M., Newman, M.E.J.: Community structure in social and biological net-
works. Proc. Natl. Acad. Sci. USA 99(12), 7821–7826 (2002)
4. Nekorkin, V.I., Vdovin, L.V.: Diskretnaya model neironnoy aktivnosti. Izvest. vys.
ucheb. zaved. Prikladnaya Nelineinaya Dinamika 15, 36–60 (2007) (in Russian)
5. Courbage, M., Nekorkin, V.I., Vdovin, L.V.: Chaotic oscillations in a map-based
model of neural activity. Chaos 17, 043109 (2007)
6. Courbage, M., Nekorkin, V.I.: Map-based models in neurodynamics. Int. J. Bifur-
cation and Chaos 20, 1631–1651 (2010)
7. Maslennikov, O.V., Kasatkin, D.V., Rulkov, N.F., Nekorkin, V.I.: Emergence of
antiphase bursting in two populations of randomly spiking elements. Phys. Rev.
E. 88, 042907 (2013)
8. Maslennikov, O.V., Nekorkin, V.I.: Modular networks with delayed coupling: Syn-
chronization and frequency control. Phys. Rev. E. 90, 012901 (2014)

 
© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
67
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_8 
 
Birth-Death Models of Information Spread  
in Structured Populations  
Burton Voorhees  
Center for Science, Athabasca University,  
1 University Drive Athabasca, AB Canada T9S 3A3 
burt@athabascau.ca 
1 
Introduction  
Following on the seminal work of Lieberman, et al [1], analysis of information spread 
in structured populations has become a major area of research interest.  This work, 
together with research on general networks [e.g., 2 – 8], has inspired a number of 
graph based evolutionary models [e.g. 9 – 21].  The generality of such models can be 
seen through a partial listing of questions to which they have been applied: the spread 
of information, gossip, and rumors [22 – 25]; the spread of ideas and innovations [26 
– 30]; the probability of a mutant gene becoming fixed in a population [31 – 34]; 
models of defenses against cancer and epidemics [35, 36]; models of the evolution of 
cooperation [37 – 39]; and, tracking rumor sources and terrorists [40 - 42]. 
2 
Birth-Death Models and the Moran Process  
Four of the more popular modeling assumptions are birth-death, death-birth, voter 
models, and probabilistic voter models, together with game theoretic versions.  The 
focus here is on birth-death models.  Members of a population, who either do or do 
not possess a particular characteristic (e.g., a mutant gene, a particular belief or 
opinion, an innovative practice, or a disease) occupies each vertex of a graph.  
Vertices occupied by population members with the characteristic are labeled 1 and 
those occupied by members not possessing it are labeled 0.  The distinguishing 
characteristic is assigned fitness r ≥ 0 as compared to fitness 1 of the normal 
population.   
The birth-death process is a discrete time process in which, at each iterate, a vertex 
is chosen, biased by fitness, to “reproduce.”  Following on this choice, an adjacent 
vertex is chosen to die and be replaced by a copy of the reproducing vertex. The edge 
from vertex i to vertex j is labeled with the probability that if vertex i is chosen for 
reproduction then vertex j is chosen for death. 
This process was introduced by Moran [43] to find the fixation probability of a 
single mutant gene introduced into an otherwise homogeneous population.  It is a 
biased random drift Markov process on the state space {0,1,…,N}, where the state m 
indicates a population state with m mutants and N – m normals.  States 0 and N are 

68  
B. Voorhees 
 
absorbing states and, following Nowak [44], if pm,m-1 and pm,m+1 are the respective 
probabilities of the state transitions m  m – 1 and m  m + 1 and if xk is the 
probability of reaching fixation (state N) from state k then  
 
x0 = 0
xk = pk,k−1xk−1 + pk,kxk + pk,k+1xk+1
xN = 1
 
 (1) 
and the single vertex fixation probability ρ is x1, which is given by  
 
ρ =
1
1+
pk,k+1
pk,k−1
k=1
j
∏
j=1
N−1

 
 (2)       
For the Moran process,  
 
pm,m−1 =
m
N




1
N −m + rm ,
pm,m+1 =
N −m
N




rm
N −m + rm
 
 (3) 
and  
 
ρ =
1−1
r
1−1
r N
 
 (4) 
Any graph with fixation probability given by equation (5) is fixation equivalent to a 
Moran process – the population structure it describes has no effect on single mutant 
fixation probability, although it may influence time to fixation [45 – 47].   
The Effect of Population Structure  
The obvious next question is whether or not population structure can influence 
fixation probability.  Initial work seemed to indicate there was no influence [48, 49].  
Recent results, however, have demonstrated that this is not the general case.  In their 
seminal paper, Lieberman, et al [1] give examples of graphs that suppress or enhance 
fixation probabilities relative to the Moran probability of equation (4). 
Other questions relating to information spread in populations have also been 
seriously investigated.  Two questions of particular interest are:  
1. What initial node (or set of nodes) leads to the greatest fixation probability?  
2. Given an observed distribution of mutants, what is the most likely starting 
node (or set of nodes) producing this distribution? 
The first of these questions becomes significant for inhomogeneous population 
structures since the fixation probability itself is defined as the average of the fixation 
probabilities over all nodes, and the fixation probability at different nodes may differ.  
The following theorem makes this specific.   

 
Birth-Death Models of Information Spread in Structured Populations 
69 
 
Theorem 1 [1, 20, 32] 
A graph is fixation equivalent to a Moran process if and only if all single vertex 
fixation probabilities are equal. 
A State Space Approach  
In [50, 51], computation of birth-death fixation probabilities is developed using a state 
space approach: vectors a(v),

b(v) are defined by a(v) = v ⋅W,

b(v) = ′v ⋅W  where 
vi′ =1−vi . For state vector v , aj( v ) is the probability that an edge originating at a 
mutant vertex terminates at vertex j and bj( v ) is the probability that an edge 
originating at a normal vertex terminates at vertex j.   
Theorem 2 [50] 
Let a birth-death process be defined on a graph G with probabilistic edge weight  
matrix W.  Then for all j, 1 ≤ j ≤ N and for all states v with m =
vi
i=1
N

:  
1. 
The probability of a transition from the state (v1,…,vj-1,0,vj+1,…,vN) to the 
state (v1,…,vj-1,1,vj+1,…,vN) is given by  
 
raj(v)
N −m + rm
.  
(5) 
2. 
The probability of transition from the state (v1,…,vj-1,1,vj+1,…,vN) to the 
state (v1,…,vj-1,0,vj+1,…,vN) is given by 
 
bj(v)
N −M + rm
.  
(6)  
3. 
The probability that v remains unchanged is  
 
ra(v)⋅v +

b(v)⋅′v
N −m + rm
.  
(7) 
Based on this theorem a state transition matrix T = (Tuv) can be constructed.  
Elements on the state space are indexed with letters chosen from the latter part of the 
alphabet (e.g., u,v) while labels of vertices of the graph  are indexed with letters i, j.   
Any set of mutants less than the full state space will goes to extinction or fixation, 
hence the Markov process has only two steady states: extinction, represented by the 
vector 0 of all zeros; and, fixation, represented by the vector 1 of all ones.  Writing 
v =
vi2N−i
i=1
N

 with xv the probability that the state v  goes to fixation, the equation  
for the Markov steady state, with boundary conditions x0 = 0 and x2N −1 = 1, yields  
 
N + (r −1)m −ra(v)⋅v −

b(v)⋅′v

xv −r
ai(v)vi′xv+2N−i −
i=1
N

bi(v)vixv−2N−i = 0
i=1
N

.  
(8)  

70  
B. Voorhees 
 
These are linear equations, which can be solved by standard packages in Maple, 
Mathematica, and Matlab.  Solution, however, is only possible for small values of N 
(say N < 20), and even in these cases, exact solutions are cumbersome. For example, 
the graph of Figure 1 represents a cycle with a constriction.   
 
 
 
 
 
 
 
 
Fig. 1. N = 5 Cycle of Width 2 With Constriction 
Symmetry conditions reduce the 30 equations arising from equation (8) to 16 
equations in 16 unknowns.  The fixation probability is given by the ratio of two 
degree 16 polynomials in the fitness r, with coefficients of up to seven digits.  It is 
best represented graphically in terms of the difference between this probability and 
the corre sponding Moran probability on five vertices, as indicated in Figure 2: 
 
 
Fig. 2. Difference between Fixation Probability for Graphs of Figure 1 and the Equivalent 
Moran Probability 
The fixation probability for the graph of Figure 1 is greater than that of a Moran 
process for 1< r < 2.267235117.  This suggests that the introduction of funnel or star-
like constrictions in cycles of width n may lead to enhancement of selection for only 
particular ranges of fitness, and similar results show up for other graphs.  
Circular Flows  
In [50, 51] a class of graphs called circular flows is defined.  This class contains 
many of the graphs that have been studied in the literature, including cycles, funnels, 
cascades, and layered networks [3].  Figure 3 shows an example such a graph.  

 
Birth-Death Models of Information Spread in Structured Populations 
71 
 
 
Fig. 3. Illustration of a 5-Level Circular Flow 
In this figure, the numbers nk indicate the number of vertices in equivalence classes 
of vertices while the arrows indicate that every node in class nk+1 is connected to 
every node in class nk with probability 1/nk (with indices taken mod s+1 where s is the 
maximum number of classes).  If there are k+1 equivalence classes in a circular flow 
with ni vertices in the i-th class then this is indicated by the k+1 tuple (n0,n1,…,nk).  
All vertices in each class are equivalent and this symmetry condition reduces the 
number of equations and unknowns involved to the product  
(ni +1)
i=0
k
∏
 
Given a class label (m0,…,ms-1,ms,ms+1,…,mk) where mk denotes the number of 
mutants at level k, define  
 
ι m
( ) = mk +
ms
ns+ j +1
(
).
j=1
k−s
∏
s=0
k−1

 
 (9) 
Using the notation  (݉଴, … , ݉௦ିଵ, ݉௦± 1, ݉௦ାଵ, … , ݉௞) →ߡ(݉,
ሬሬሬሬԦ ݉௦± 1) equation 
(8) becomes 
 
ms(ns+1 −ms+1) + rms+1(ns −ms)
ns
s=0
k




xι(m)
−
ms(ns+1 −ms+1)
ns
s=0
k




xι(m,ms−1) −r
ms+1(ns −ms)
ns
s=0
k




xι(m,ms+1) = 0
  
(10) 
and the single vertex fixation probability is  
 
ρ = 1
N nkx1 +
nk−sxι(s)
s=1
k




, ι(s) =
(nj+1 +1)
j=k−s
k−1
∏
  
(11) 
with summation indices taken mod(k+1).   
     n0  
             n1  
       n2  
        n3  
       n4 
          

72  
B. Voorhees 
 
The question of the sign
be explicitly illustrated wit
three level funnel graphs (
average fixation probabilit
limited values of r > 1 (for 
in Figure 4a.  Figure 4b, h
initial mutant placement in 
probability is enhanced rela
in the n2 equivalence class (
 
                      a  
Fig. 4. (a) ρG - ρM
The State Transition Matrix
Useful results can be obta
state u = (u1,u2,...,uN )  de
denary form given by u = 
down in numerical order 
consequence we have: 
Lemma 1:  
The probability that vertex
initial state u, is given by [
Theorem 6:  
Given initial state u, the pro
Proof:   
Since the Markov process 
fixation, for large k, [Tk]uv ~
 
lim
k→∞T k

where T * = lim
k→∞T k.  
ificance of the initial site has been studied in [12], and 
h solutions to equations (8).  Figure 4 shows examples
n0,n1,n2) = (1,2,n).  For the (1,2,n) graphs with n < 6 
ty is enhanced with respect to a Moran process only 
n ≥ 6 it appears to be enhanced for all r > 1), as indica
owever, shows the corresponding fixation probabilities
the n0, n1, and n2 equivalence classes of vertices.  Fixat
ative to a Moran process only if the initial mutant is pla
(upper curves).   
 
                                   b 
, (b) single vertex probabilities minus ρM for G(1,2,n) 
x  
ained by consideration of the state transition matrix T
fines the binary number u1u2…uN with correspond
ui2N−i
i=1
N

.  The 2N population states are listed from the 
to define a 2N × N matrix S with Sui = ui.  As a dir
x i will contain a mutant after k iterations, starting fr
[TkS]ui.  
obability of fixation is xu = lim
k→∞T k ⋅S

ui .  
with transition matrix T converges to either extinction
~ 0 unless v  is 0 or 1, and  
k ⋅Sui = Tu0
* 00 + Tu1
* 12N −1 = Tu1
* = xu,  
(
can 
s of 
the 
for 
ated 
 for 
tion 
aced 
T. A 
ding 
top 
rect 
rom 
n or 
(12) 

 
Birth-Death Models of Information Spread in Structured Populations 
73 
 
This also provides a determination of upper and lower bounds for the fixation 
probability of any state u.   
Theorem 7:  
Given an initial state u and a number of birth-death iterations k,  
 
T k

u1 ≤xu ≤1−T k

u0.
  
(13)
 
Lemma 1 and Theorem 7 are reported in [20, 21] in different language.   
Interpolation of equation (20) yields an estimate for the fixation probability of u: 
set ∆u(k) = 1 – [Tk]u0 – [Tk]u1 and write the linear interpolation estimate for the 
fixation probability as:  
 
estk(xu) = T k

u1 +
Δu(k)
T k

u0 + T k

u1
   
(14) 
This makes it possible to at least tentatively answer our question (1): given that the 
observed state after k iterations is v , what are the most likely initial states to produce 
this observed configuration?  The answer is obtained by finding max Tuv
k u ∈V
{
} for 
sufficiently large values of k 
The Graph Laplacian 
Our question (2) can be approached by making use of a suitably defined graph 
Laplacian [52, 53].  If the eigenvalues of this matrix are ordered from smallest to 
largest, the first is always 0 and the remaining eigenvalues satisfy 0 ≤ λ1 ≤ λ2 ≤ … ≤ 
λN-1 ≤ 2 for an N vertex graph.  The number of 0 eigenvalues equal the number of 
connected components of the graph, hence λ1 > 0 if and only if the graph is connected 
and the value of λ1 provides information about the difficulty of cutting the graph into 
disconnected parts.  If λ1 is found to be decreasing in time, for example, it may 
suggest increasing polarization or conflict in the represented population. Figure 5 
shows a simple example.  
 
Fig. 5. Example of Graph with Disconnect Parameter p 
In this graph, the parameter p ranges between zero and one.  If p = 0 the graph has 
been severed into two disconnected components. The edge weight matrix and graph 
Laplacian for this graph are 
              1                     p                     1-p 
   1-p                   p                      1 

74  
B. Voorhees 
 
W =
0
1
0
0
1−p
0
p
0
0
p
0
1−p
0
0
1
0












,
∆=
1
−1−p
0
0
−1−p
1
−p
0
0
−p
1
−1−p
0
0
−1−p
1
















 
  
 
 
 
 
 
 
    (22)   
The eigenvalues of ∆ are 0, p, 2-p, and 2.  Thus p measures the degree of contact 
between the first two and final two vertices.  
Discussion  
The study of information spread in finite populations is a broad and rapidly 
developing field, subsuming a wide variety of topics and directions of research.  The 
use of edge-weighted graphs to model interaction patterns in heterogeneous 
populations is a significant tool in this research.  One of the questions this tool has 
been used to study is the fixation probability, that is, the probability that an innovation 
introduced into a population will eventually take over the entire population.  
Intuitively, the expectation is that non-homogeneous structure in a population can 
influence fixation probability and it is now clear that there are indeed population 
structures that can exert a strong influence, either suppressing or enhancing the effects 
of selection relative to drift in a homogeneous population.  Further, there are sound 
biological and social reasons to explore these heterogeneous population structures.   
A point of interest is the existence of structures that provide suppression or 
enhancement of selection for limited ranges of a fitness parameter.  This places 
fitness (or any measure of constraint matching) in the role of control parameter.   
From a graph theoretic perspective, the graph Laplacian for the population matrix 
provides a link to research in spectral graph theory [52, 53].  Thus, λ1, called the 
algebraic connectivity of the graph, is non-zero if and only if the graph is connected.  
Its value provides information about the difficulty of cutting the graph into 
disconnected parts, giving a measure of the degree of connectivity within a 
population.  
References 
[1] 
Lieberman, E., Hauert, C., Nowak, M.A.: Evolutionary dynamics on graphs. 
Nature 433(7023), 312–316 (2005) 
[2] 
Kauffman, S.A.: Origins of Order: Self-Organization and Selection in Evolution. Oxford 
University Press, Oxford (1993) 
[3] 
Barbosa, V.C., Donangelo, R., Sousa, S.R.: Network growth for enhanced natural 
selection. Physical Review E 80(2), 026115 (2009) 
[4] 
Birkholz, J.M., Bakhshi, R., Harige, R., van Steen, M., Groenewegen, P. (2012) Scalable 
analysis of socially informed network models. arXiv:1209.6615v1 (cs.SI) (September 
28, 2012) 
[5] 
Chazelle, B. (2012) The dynamics of influence systems. arXiv:1204.3946 (nlin.AO) 
(April 17, 2012) 
 
 

 
Birth-Death Models of Information Spread in Structured Populations 
75 
 
[6] 
Díaz, J., Goldberg, L., Mertzios, G., Richerby, D., Serna, M., Spirakis, P.: 
Approximating fixation probabilities in the generalized Moran process. In: Proceedings 
of the ACM-SIAM Symposium on Discrete Algorithms (SODA), Kyoto, Japan. ACM 
(2012), http://arxiv.org/abs/1111.3321 
[7] 
Ghanbarnejad, F., Klemm, K.: Impact of individual nodes in Boolean network 
dynamics. arXiv:1111/5334v1 (q-bio.MN) (November 22, 2011) 
[8] 
Mossel, E., Sly, A., Tamuz, O.: Strategic learning and the topology of social networks. 
arXiv:1209.5527 (cs.GT) (September 25, 2012) 
[9] 
Antal, T., Redner, S., Sood, V.: Evolutionary dynamics on degree-heterogeneous graphs. 
Physical Review Letters 96(18), 188104 (2006) 
[10] 
Broom, M., Hadjichrysanthou, C., Rychtár, J., Stadler, B.T.: Two results on 
evolutionary processes on general non-directed graphs. Proceedings of the Royal 
Society A: Mathematical, Physical and Engineering Sciences 466, 2795–2798 (2010) 
[11] 
Broom, M., Rychtár, J., Stadler, B.T.: Evolutionary dynamics on small-order graphs. 
Journal of Interdisciplinary Mathematic 12(2), 129–140 (2009) 
[12] 
Broom, M., Rychtár, J., Stadler, B.T.: Evolutionary dynamics on graphs – the effect of 
graph structure and initial placement on mutant spread. Journal of Statistical Theory and 
Practice 5(3), 369–381 (2011) 
[13] 
Champagnat, N., Lambert, A., Richard, M.: Birth and death processes with neutral 
mutations. arXiv:1209.6205.v1 (math.PR) (September 27, 2012) 
[14] 
Fu, F., Wang, L., Nowak, M.A., Hauert, C.: Evolutionary dynamics on graphs: Efficient 
methods for weak selection. Physical Review E 79(4), 046707 (2009) 
[15] 
Hauert, C.: Evolutionary dynamics. In: Skjeltorp, A.T., Belushkin, A.V. (eds.) 
Proceedings of the NATO Advanced Study Institute on Evolution From Cellular to 
Social Scales, pp. 11–44. Springer, Dordrecht (2008) 
[16] 
Masuda, N., Ohtsuki, H.: Evolutionary dynamics and fixation probabilities in directed 
networks. New Journal of Physics 11, 033012 (2009) 
[17] 
Masuda, N.: Directionality of contact networks suppresses selection pressure in 
evolutionary dynamics. Journal of Theoretical Biology 258(2), 323–334 (2009) 
[18] 
Nowak, M.A., Tarnita, C.E., Antal, T.: Evolutionary dynamics in structured populations. 
Philosophical Transactions of the Royal Society B 365, 19–30 (2010) 
[19] 
Santos, F.C., Pacheco, J.M., Lenaerts, T.: Evolutionary dynamics of social dilemmas in 
structured heterogeneous populations. PNAS 103(9), 3490–3494 (2006) 
[20] 
Shakarian, P., Roos, P., Johnson, A.: A review of evolutionary graph theory with 
applications to game theory. Biosystems 107(2), 66–80 (2012) 
[21] 
Sharkarian, P., Roos, P., Moores, G.: A novel analytical method for evolutionary graph 
theory problems. Biosystems (2012) (to appear) 
[22] 
Barbour, A.D., Reinert, G.: Asymptotic behavior of gossip processes and small world 
networks. arXiv:1202.5895v2 (math.PR) (February 28, 2012) 
[23] 
Coletti, C.F., Rodriguez, P.M., Schinazi, R.B.: A spatial stochastic model for rumor 
transmission. arXiv:1202.1491v1 (math.PR) (February 17, 2012) 
[24] 
Haeupler, B.: Simple, fast and deterministic gossip and rumor spreading. 
arXiv:1210.1193v1 (cs.DS) (October 3, 2012) 
[25] 
Shi, G., Johansson, M., Johansson, K.H.: How agreement and disagreement evolve over 
random dynamic networks. arXiv:1208.3398v1 (cs.SI) (August 16, 2012) 
[26] 
Banos, R.A., Borge-Holthoefer, J., Moreno, Y.: The role of hidden influentials in the 
diffusion of online information cascades. EJP Data Science 2(6) (2013) 
[27] 
Montanari, A., Saberi, A.: The spread of innovation in social networks. PNAS 107(47), 
20196–20201 (2010) 
[28] 
Tu, S.-Y., Sayed, A.H.: On the influence of informed agents on learning and adaptation 
over networks. arXiv:1203.1524 (cs.IT) (March 7, 2012) 

76  
B. Voorhees 
 
[29] 
Wang, Y., Xiao, G., Liu, J.: Dynamics of competing ideas in complex social networks. 
arXiv: 1112.5534v1 (2011) 
[30] 
Lang, J., De Sterck, H.: The Arab Spring: A simple compartmental model for the 
dynamics of a revolution. ArXiv:1210.1841v1 (math.DS) (October 5, 2012) 
[31] 
Barbosa, V.C., Donangelo, R., Souza, S.R.: Early appraisal of the fixation probability in 
directed networks. Physical Review E8 2(4), 046114 (2010) 
[32] 
Broom, M., Rychtár, J.: An analysis of the fixation probability of a mutant on special 
classes of non-directed graphs. Proceedings of the Royal Society A 464, 2609–2627 
(2008) 
[33] 
Taylor, P.D., Day, T., Wild, G.: From inclusive fitness to fixation probability in 
homogeneous structured populations. Journal of Theoretical Biology 249, 101–110 (2007) 
[34] 
Zhang, C., Wu, Y., Liu, W., Yang, X.: Fixation probabilities on complete star and 
bipartite digraphs. Discrete Dynamics in Nature and Society 940465 (2012) 
[35] 
Nowak, M.A., Michor, F., Iwasa, Y.: The linear process of somatic evolution. 
Proceedings of the National Academy of Science USA 100, 14966–14969 (2003) 
[36] 
Simon, P.L., Taylor, M., Kiss, I.Z.: Exact epidemic models on graphs using graph 
automorphism driven lumping. Journal of Mathematical Biology 62(4), 479–508 (2011) 
[37] 
Chen, Y.-T.: Robustness of two simple rules for the evolution of cooperation on regular 
graphs. arXiv:1107.1513v1 (math.PR) (July 7, 2011) 
[38] 
Dong-Ping, Y., Hai, L., Chen-Xu, W., Jian-Wei, S.: Modeling Moran process with 
network dynamics for the evolution of cooperation. Chinese Physics Letters 26(6) (2009) 
[39] 
Taylor, P.D., Day, T., Wild, G.: Evolution of cooperation in a finite homogeneous 
graph. Nature 447, 469–472 (2007) 
[40] 
Dong, W., Zhang, W., Tan, C.W.: Rooting out the rumor culprit from suspects. 
arXiv:1301.6312v1 (cs.SI) (January 27, 2013) 
[41] 
Shah, D., Zaman, T.R.: Rumors in a network: Who’s the culprit? IEEE Transactions on 
Information Theory 57, 5163–5181 (2011) 
[42] 
Shah, D., Zaman, T.R.: Finding rumor sources on random graphs. arXvi:1110.6230v2 
(math.PR) (November 3, 2011) 
[43] 
Moran, P.: Random processes in genetics. Mathematical Proceedings of the Cambridge 
Philosophical Society 54, 60–71 (1958) 
[44] 
Nowak, M.A.: Evolutionary Dynamics. Harvard University Press, Cambridge (2006) 
[45] 
Broom, M., Hadjichrysanthou, C., Rychtár, J., Stadler, B.T.: Evolutionary games on 
graphs and the speed of the evolutionary process. Proceedings of the Royal Society A: 
Mathematical, Physical and Engineering Sciences 466, 1327–1346 (2009) 
[46] 
Altrock, P.M., Traulsen, A.: Fixation times in evolutionary games under weak selection. 
New Journal of Physics 11, 013012 (2009) 
[47] 
Taylor, C., Iwasa, Y., Nowak, M.A.: A symmetry of fixation times in evolutionary 
dynamics. Journal of Theoretical Biology 243, 245–251 (2006) 
[48] 
Maruyama, T.: A simple proof that certain quantities are independent of the 
geographical structure of populations. Theoretical Population Biology 5(2), 148–154 
(1974) 
[49] 
Slatkin, M.: Fixation probabilities and fixation times in a subdivided population. 
Evolution 35, 477–488 (1981) 
[50] 
Voorhees, B.: Birth-death fixation probabilities for structured populations. Proceedings 
of the Royal Society A 469, 2153 (2013) 
[51] 
Voorhees, B., Murray, A.: Fixation probabilities for simple digraphs. Proceedings of the 
Royal Society A 469, 2154 (2013) 
[52] 
Banerjee, A.: The Spectrum of the Graph Laplacian as a Tool for Analyzing Structure 
and Evolution of Networks. Dissertation (Dr. rer. nat.), University of Leipzig (2008) 
[53] 
Li, Y., Zhang, Z.: Digraph Laplacian and the degree of asymmetry. Internet 
Mathematics 8(4), 381–401 (2012) 

Coexistence of Deterministic and Stochastic
Bistability in a 1-D Birth-Death Process
with Hill Type Nonlinear Birth Rates
Neslihan Avcu1,⋆, Nihal Pekergin2, Ferhan Pekergin3, G¨uleser Kalaycı Demir1,
and C¨uneyt G¨uzeli¸s4
1 Dokuz Eyl¨ul University, Faculty of Engineering, Department of Electrical and
Electronics Engineering, ˙Izmir, Turkey
{neslihan.avcu,guleser.kalayci}@deu.edu.tr
2 Universit´e Paris-Est Cr´eteil, LACL, Cr´eteil, France
nihal.pekergin@u-pec.fr
3 Universit´e Paris 13, Sorbonne Paris Cit´e, LIPN, CNRS, Paris, France
ferhan.pekergin@lipn.univ-paris13.fr
4 Izmir University of Economics, Faculty of Engineering and Computer Sciences,
Department of Electrical and Electronics Engineering, ˙Izmir, Turkey
cuneyt.guzelis@ieu.edu.tr
Abstract. The paper shows that, for a speciﬁc 1 −D birth-death pro-
cess, the parameter ranges for the existence of the deterministic bista-
bility exactly coincide with the parameter ranges for the existence of the
stochastic bistability, namely bimodality. The considered 1 −D birth-
death process is a reduced model of TMG induced lactose operon of
Escherichia coli in which the birth rate for intra cellular TMG molecules
is of Hill type. As opposed to the results reported by some works for
other 1 −D birth-death processes in the literature, such as the obser-
vation called Keizer paradox, no bistability without bimodality and also
no bimodality without bistability are obtained for any set of model pa-
rameters. Bistability without bimodality is observed to occur only when
invalid calculations of steady-state probabilities due to i) big number
problems related to very small probabilities corresponding to troughs
between two peaks and/or ii) inappropriately low choice of molecule
numbers are not prevented.
1
Introduction
Bistability is a nonlinear dynamical behavior observed in diverse areas of biolog-
ical systems including gene regulatory networks [5]. For a deterministic system
model, the bistability appears with having two stable and one unstable equi-
librium points such that the unstable one actually separates the domain of at-
tractions of the stable ones [1]. The deterministic models are usually deﬁned in
⋆This work was supported in part by the Turkish Scientiﬁc and Technological Re-
search Council and Centre National de la Recherche Scientiﬁque in the framework
of Bosphorus PIA Program.
c
⃝Springer International Publishing Switzerland 2015
77
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_9

78
N. Avcu et al.
terms of Ordinary Diﬀerential Equations (ODEs) which are derived by the law of
mass action for the case of Gene Regulatory Networks (GRNs) [9], [10], [11], [16].
These ODE models of GRNs represent the change of concentrations of molecules
which are the enzyme products, in general, as the sum of a nonlinear birth term
and a linear death term; both are functions of molecule concentrations. It is
well known [1], [3], [9], [16] that the bistability occurs for some values of model
parameters when the birth rates are modeled with Hill type nonlinearity.
It is argued by many researchers [1], [3], [9], [16] that ODE models predict
well the average behavior of a population or the behavior of a single cell or a
gene regulatory network for high molecule numbers, but their predictions may
not be valid for the cases of small molecule numbers. It is further argued [2]
that stochastic models such as Chemical Master Equations (CMEs) describing
the continuous-time evaluations of probabilities of model’s discrete states, i.e.
molecule numbers, provide better predictions for small numbers of molecules. It
is shown that, for the cases of linear propensities, i.e. birth and death rates, the
prediction by a CME model provides a mean value of the steady-state probability
distribution identical to the stable state of the corresponding ODE Model. It is
further shown in many works [4], [14] for some bistable ODE models such as
Schl¨ogl model with a third order polynomial reaction rate that the peaks, i.e.
local maxima, of the steady-state probability distribution which is the steady-
state solution to the corresponding CME take place at the stable equilibria of
the bistable ODE model and there is a trough, i.e. a local minimum, of the
steady-state probability distribution at the unstable equilibrium of ODE model.
On the other hand, some works [12] considering ODE models with second
order polynomial reaction rates show that the CME counterpart shows bimodal-
ity with having a peak at zero molecule number whereas the ODE model has
no capacity for possessing a bistable behavior since the quadratic nonlinearity
deﬁning the right hand side of the ODE model can provide a positive stable
equilibrium and an unstable equilibrium point at zero, only. These observations
lead many researchers to investigate the similarities and diﬀerences between the
ODE and corresponding CME birth-death models. This paper presents a work
in this direction for 1-D birth-death processes deﬁned with a Hill type nonlinear
birth rate and a linear death rate. The importance of the considered birth death
process is that Hill type interaction rates are commonly used for describing the
bistable behavior of gene regulatory networks, e.g. lac operon of Escherichia coli.
The main contributions of this paper are given as follows.
• As in accordance with the results obtained in the literature for Schl¨ogl model,
the considered 1-D birth death process with Hill type birth rate are shown
to demonstrate the bistability and bimodality for the same ranges of the
model parameters, i.e. no bistability without bimodality and no bimodality
without bistability are shown to occur, if biologically valid values of model
parameters are used.
• It is shown that, for a bistable ODE model, bimodality may disappear in
the corresponding CME model for some parameter values when the vol-
ume of a cell and thus the number of molecules are chosen biologically

Coexistence of Deterministic and Stochastic Bistability
79
unrealistically low or the big number problems due to low probabilities occur-
ring at the trough of steady-state probability distribution are not overcome
in calculating the steady-state probabilities.
• In addition to the 1-D birth death lac operon model analyzed in the paper,
a similar 1-D birth death model with Hill type birth rate is also considered
and it is shown that, as opposed to the conclusions made in the literature [8]
that a second peak whose disappearance removes bimodality is observable
for the parameter values yielding bistability if the steady-state probabilities
are calculated by supposing suﬃciently large numbers of molecules and by
avoiding numerical problems, in precisely, the bottleneck caused by very low
probability at the molecule number corresponding to unstable equilibrium.
• Bimodality without bistability which is observed in some quadratic 1-D birth
death process [12] is numerically studied for the considered 1-D birth death
lac operon model and such a phenomenon is reported to never occur for
the considered model. It is argued in the paper that bimodality without
bistability may occur when unstable equilibrium of ODE model with one
stable and one unstable equilibria is transformed to an absorbent in the
CME counterpart as in the models in which Keizer paradox is observed [12].
• The considered 1-D birth death model is also studied in slow time scale by
calculating absorption times. So, transition times from uninduced state, i.e.
the small stable equilibrium point to induced state, i.e. the large equilibrium
point are calculated and it is concluded that the CME counterpart of the
ODE model behaves similar to ODE model for fast time scales such that
the trajectories originated from the basin of stable equilibrium tends to this
stable equilibrium and any trajectory around or at a stable state switches
to the other stable state for fast time scales.
The paper is structured as follows. In Section 2, a reduced lac operon model
is presented as a 1-D birth death ODE model and the bistability ranges of the
model parameters are given based on a discriminant based analysis [1]. CME
counterpart of the reduced lac operon model is derived in Section 3. The calcu-
lation of steady-state probabilities based on closed form solution and then the
bimodality ranges of the model parameters are described in Section 3. Section
4 is devoted to discuss the results obtained for ODE and CME models and also
provides a comparison on bistability versus bimodality for a similar 1-D birth
death model which is studied in the literature [8] for the sake of emphasizing
the importance of choosing an appropriate number of molecules in solving CME
and also avoiding numerical problems involved with very small probabilities at
some molecule numbers when calculating steady-state probability distribution.
Conclusions are made in Section 5.
2
Reduced ODE lac Operon Model
The lac operon is a paradigm for genetic regulation due to the positive feedback
resulting bistable behavior [5]. There are lots of experimental and theoretical
studies about the bistable behavior of lac operon in the literature for nearly half

80
N. Avcu et al.
a century. Diﬀerent types of mathematical models and analysis methods have
been developed to explain the genetic switching mechanism of lac operon [6], [7],
[8], [9], [16].
The lac operon includes three structural genes lacZ, lacY and lacA which
respectively encode β-galactosidase, galactoside permease and thiogalactoside
transacetylase enzymes. The permease enzyme transports lactose or its non-
metabolizable analogs methyl-1-thio-β-D-galactoside (TMG) or isopropyl-β-D-
thio-galactoside (IPTG) into the cell. β-galactosidase hydrolyzes the transported
lactose into galactose and glucose to be metabolized, and also catalyzes the con-
version of internal lactose to allolactose to induce the transcription of lac operon
by binding to lac repressor. Thiogalactoside transacetylase is thought to have
roles in the sugar metabolism and detoxiﬁcation [1]. This study includes a re-
duced model of TMG induced lac operon with one ordinary diﬀerential equation
whose state variable denoted by T . This model is obtained from the lac operon
model with three state variables in [1] by assuming that mRNA transcription
and permease translation occur more rapidly according to the transient response
of TMG. In quasi steady-state, the rate of the change of molar concentration of
TMG is written as in (1) by eliminating mRNA and permease concentrations
in terms of TMG concentrations. The reduced TMG model involves the catabo-
lite repression and inducer exclusion eﬀects of the extracellular glucose and the
positive eﬀect of TMG on the transcription of mRNA. The translational and
transcriptional delays are ignored in the lac operon mechanism.
1
γT
dT
dt = pfM,T (T ) −T
(1)
where p is deﬁned as the combination of the model parameters such as degrada-
tion and production constants, the eﬀects of external glucose and TMG in [1] to
simplify the analysis of the model and fM,T(T ) = (1 + K1T n)/(K + K1T n) is
the Hill type production function of mRNA under TMG concentration. It is an
allosteric interaction [15] and n is the required number of TMG for inactivating
a repressor protein, K1 is the equilibrium constant of TMG-repressor protein
interaction, and K−1 is the basal level of mRNA transcription in E.coli [9], [15].
To inactivate the repressor protein, at least two TMG molecules have to bind
the repressor, thus n is taken as 2. The Equation (1) can be rearranged and the
characteristic equation can be derived for the steady-state case as in (2).
K1T 3 −pK1T 2 + KT −p = 0
(2)
It has been shown that the considered model has either one or three equilibrium
points depending on the model parameters [1], [9]. The model always has a
real root irrespective of the parameter values. The remaining two roots of (2)
are either real or a pair of complex conjugates. In three real roots case, there
are three diﬀerent possibilities: A triple root, two roots such that one of them
is double, and three diﬀerent roots. The bistable behaviour occurs when there
exist three equilibria corresponding to the three diﬀerent real roots of (2). In
the previous work [1], to determine the ranges of the parameters ensuring the

Coexistence of Deterministic and Stochastic Bistability
81
existence of three real roots, the discriminant based bistability analysis is applied
to the characteristic equation in (2). The bistability condition with three real
roots is satisﬁed by the parameter values that make the discriminant of (2) in
(3) is positive, Δ > 0.
Δ = −4K3
1p4 + (18K2
1K −27K2
1 + K2
1K2)p2 −4K1K3
(3)
The bistability parameter ranges are calculated for each parameter by substitut-
ing the nominal values of other two parameters. To deﬁne the bistability range
for p parameter, the nominal values of K = 167.1 and K1 = 1(μM)−2 are cho-
sen from [9] and it is obtained as p ∈(25.7μM, 84.1μM). In the calculations
of the other parameter ranges, p is taken as 74.59μM which is calculated from
the formula of p in [1] for the input values 20μM external TMG, Te = 20μM,
and 0μM external glucose, Ge = 0μM, which the bistability is observed ex-
perimentally in [9]. The bistability range for K parameter is determined as
K ∈(148, 1390) by assuming K1 = 1(μM)−2 and p = 74.59μM. By taking
K = 167.1 and p = 74.59μM in similar calculation, the bistability range is ob-
tained as K1 ∈(0.119(μM)−2, 1.27(μM)−2) for K1 parameter. It is seen that,
the nominal values are in the bistability ranges of each parameter deﬁned from
reduced ODE model.
3
Reduced CME lac Operon Model
The quasi steady-state model in (1) is indeed a one-dimensional birth-death
process. The change rate of the TMG concentration depends on two terms. The
ﬁrst one is the birth term, f(T ) = γT p 1+K1T 2
K+K1T 2 , explaining the rate of going to the
next state and the second one is the death term, g(T ) = γT T , expressing the rate
of going to the previous state. The loss constant, γT , only aﬀects the transient
behavior of the system and has no eﬀect on the steady-state (see Equation (1)).
So, the loss constant can be ignored in the steady-state analysis. In the stochastic
model derived from deterministic quasi steady-state model, the birth and death
rates can be deﬁned as state-dependent and time-homogenous. The stochastic
1-D birth death lac operon model can be written in Chemical Master Equation
form as in (4).
dPr(T, t)
dt
= f(T −1)Pr(T −1, t) −f(T )Pr(T,t) + g(T + 1)Pr(T + 1, t) −g(T )Pr(T,t)
(4)
where, T represent the number of TMG molecule and is a discrete random
variable taking values in {0 · · · N}. N is the maximum number of T molecules.
dP r(T,t)
dt
is the rate of change of the probability of TMG molecule in time. In the
matrix form, this can be written as follows.
d
dt
⎡
⎢⎢⎢⎣
Pr(0, t)
Pr(1, t)
...
Pr(N, t)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
−f(0)
g(1)
0
· · ·
0
f(0) −f(1) −g(1)
g(2)
· · ·
0
0
f(1)
−f(2) −g(2) · · ·
0
...
...
...
...
...
0
0
0
· · ·
g(N)
0
0
0
· · · −g(N)
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
Pr(0, t)
Pr(1, t)
...
Pr(N, t)
⎤
⎥⎥⎥⎦

82
N. Avcu et al.
A steady-state probability distribution, π, exists for this time homogeneous,
ﬁnite, irreducible Continuous Time Markov Chain (CTMC) [13]. The unique
steady-state distribution πT = [π0 π1 · · · πN] where πi = limt→∞Pr(i, t)∀i ∈
{0, 1, 2, · · · , N} can be found by taking the derivatives equal to 0 as 0 = QT π or
0 = πT Q. In steady-state, a detailed balance equation between each connected
state couple is obtained by assuming that there is no transition between states,
πi−1f(i−1) = πig(i). According to these balance equations, all state probabilities
can be written iteratively in terms of the probability of the ﬁrst state as in (5).
πi = π0
i−1

j=0
f(j)
g(j + 1)
(5)
The steady-state probability distribution, π, is a probability mass function and
the sum of all state probabilities must be equal to 1, N
i=0 πi = 1. By using this
property, the probability of the ﬁrst state, π0, can be obtained as in (6). Then,
all the steady-state probabilities are calculated according to (5).
π0 =
1
1 + N
i=1
i−1
j=0
f(j)
g(j+1)
(6)
The closed form steady-state solution of 1-D birth death lac operon model with
the nominal parameter values, K = 167.1 and K1 = 1(μM)2, are calculated
in MATLAB. In these calculations, the range of T values are chosen in molar
concentration, T ∈[0M, 150M], to compare the results of two models [17]. The
p parameter value is chosen as 74.59(μM)2, this value is obtained under the
absence of external glucose and for Te = 20μM external TMG used as a mid-
value of bistable interval in the experiments by [9]. Even though the lac operon
has a bistable behavior with these parameter values in experimental studies,
the closed-form steady-state solution has a unimodal probability distribution for
stochastic model in Figure.1. When the closed form steady-state solutions are
calculated with the p values in the bistability range determined from the de-
terministic model and consistent with the experimental studies, it is observed
that there is a narrower range for p parameter to satisfy the bistable behav-
ior with bimodal probability distribution. The bistability range for p parameter
is found as respectively p ∈(25.7μM, 84.1μM) and p ∈(30μM, 40μM) in the
deterministic and stochastic mathematical models. The analyses in K and K1
parameter spaces give a similar contraction in the ranges with respect to the
results of the deterministic model. The bistability range for K parameter is ob-
tained K ∈(80, 250) from the analysis of stochastic model when p = 35μM, the
middle point of the stochastic bistability ranges for p and K ∈(148, 1390) from
the deterministic one for p = 74.59μM. In K1 parameter space, the range for
bistable behaviour of lac operon is identiﬁed as K1 ∈(0.6(μM)−2, 2.4(μM)−2)
with p = 35μM and K1 ∈(0.119(μM)−2, 1.27(μM)−2) with p = 74.59μM
from the stochastic and deterministic models, respectively. The reason of these
contractions in the parameter ranges is related to the granularity of the state
space of T . Recall that in a CTMC, while the time is continuous, the states are

Coexistence of Deterministic and Stochastic Bistability
83
discrete valued. When the state representation is in molar concentration, it is
not possible to capture the bistability behavior for some parameter values. It is
given in the following ﬁgure that the steady-state distribution for K = 167.1,
K1 = 1(μM)−2, and p = 74.59μM is unimodal. The roots of the characteristic
equation, equilibrium points, are T1,2,3 = {72.162μM, 1.6639μM, 0.6199μM}.
The ﬁrst and last ones of roots are stable equilibrium points and the middle
one is unstable. The domain of attraction point between the smaller stable and
unstable points does not include any states. So it is not possible to capture the
ﬁrst trough of bimodal distribution. The peak states of unimodal distribution
in Figure.1 corresponds to induced states. If we consider the ﬁrst mean passage
time (computed as given in [13]) from other states to induced states made ab-
sorbing, it is observed that the mean number of steps to reach induced states
from states in the domain of smaller stable equilibrium point are bigger (ten
times more than the other states). The situation shows that there must be an
other probability accumulation in this region. This is not only parameter set to
observe unimodal steady-state probability distribution in stochastic model but
bistable behaviour in deterministic model. When two roots become closer to each
other, the second trough does not appear and a unimodal distribution observed
in steady-state. Thus, it is not obtained the same bistability parameter range in
stochastic model. The model parameters in molar concentration are transformed
in number of molecules.
Deterministic Model
Stochastic Model
Parameters Units
p∗
Molar concentration
K∗
Unitless
K∗
1
(Molar concentration)−2
Parameters Units
p
Number of molecule
K
Unitless
K1
(Number of molecule)−2
To convert the parameter to the number of molecule from molar concentra-
tion, each parameter value is multiplied by Avogadro constant and the average
volume of E. coli. The minimum and maximum volume values are calculated by
taking two diﬀerent diameters of cylindrical shape of E. coli given similar in [8]
and the average of these two volumes is found as 1 × 10−18m3. The unitless pa-
rameter in deterministic model, K, is taken as the same in stochastic model and
the parameter with (μM)−2 unit, K1, is multiplied by two times by Avogadro
constant and volume in stochastic model. The parameters in deterministic model
are noted with asterix and the stochastic model parameters are noted without
asterix.
The steady-state probability distribution of 1-D birth death model with trans-
formed parameter values can not be calculated in terms of number of molecule by
MATLAB due to the numerical problems. The representation limit of MATLAB
is reached: some probability values become smaller or greater than the extreme
values represented by MATLAB. To overcome this numerical problem, the steady-
state probabilities of the stochastic model are calculated in C by using the GNU
Multiple Precision Arithmetic Library. The bistability regions for each parame-
ter are obtained by the nominal parameters values as in deterministic model. For

84
N. Avcu et al.
Fig. 1. Simulation for molar concen-
tration, absorption time, simulation for
number of molecule
Fig. 2. Bistability ranges for each pa-
rameter
a better graphical representation of the probability distribution, the ratio of the
successive state probabilities are drawn in logarithmic scale in Figure.2. As simi-
lar in deterministic reduced lac operon model, the bistability ranges for p, K and
K1 parameters in 1-D birth death model are obtained as p ∈(25.7μM, 84.1μM),
K ∈(148, 1390) and K1 ∈(0.119(μM)−2, 1.27(μM)−2), respectively.
4
A Comparative Study on a Benchmark 1-D Birth
Death Model
A similar 1-D birth death model is studied in [8] with a Hill type nonlinear birth
rate, f(x) = v(α + kxn)/(1 + kxn), and linear death rate, g(x) = βx. The Hill
coeﬃcient n is chosen as 3. The ODE model for this 1-D birth death process is
deﬁned as ˙x = f(x) −g(x) where the model parameters are given as v = 40,
α = 0.125, and k = 0.093750x10−3 in [8]. In this study, it is reported that the
deterministic and stochastic model give diﬀerent results for the same parameter
sets. It is stated in [8] that the bimodal distribution could not be obtained in
the stochastic model for some parameter values ensuring bistability for the ODE
model, and further that bimodality occurs without having bistability for some
parameter values. However, the results derived by choosing the convenient trans-
formation to the number of molecule shows that the bistability and bimodality
respectively in deterministic and stochastic models can be obtained for the same
parameter sets even with doubling the volume. The diﬀerence between the re-
sults reported in [8] and the one presented in this paper is due to the followings:
The ODE model and the corresponding stochastic model are solved in [8] for the

Coexistence of Deterministic and Stochastic Bistability
85
Fig. 3. Simulation results of 1 −D birth death model
same x variable without applying any transformation from molecular concentra-
tion into molecular number, however, this paper uses the scaled versions of x
variable in the stocastic model as a kind of transformation from concentration
into molecular numbers with choosing the volume as a scaled parameter. On the
other hand, in [8], the steady-state solution to the master equation is probably
calculated without running the solution algorithm for a suﬃciently long time
period allowing the transitions from one stable state to the other, so making the
visible both of the peaks in the steady-state distributions.
5
Conclusion
The work presented in the paper shows that, for two speciﬁc 1 −D birth death
models with Hill type birth and linear death rates, the bistability and bimodal-
ity exist for the same parameter ranges. The work may be extended to general
classes of 1−D and also n−D with n > 1 birth death models with Hill type and
also other type nonlinearities such as Michealies-Menten and polynomial types.
It is worth to study the eﬀect of the existence of unstable equilibrium of ODE
models, on the occurrence of bimodality without bistability and also the qual-
itative properties of the ODE models causing the disappearance of bimodality
under the existence of bistability.
References
1. Avcu, N., Demir, G.K., Pekergin, F., Aly¨ur¨uk, H., C¸ava¸s, L., G¨uzeli¸s, C.: Discrimi-
nant based bistability analysis of a TMG induced lac operon model supported with
boundedness and local stability results. Turkish Jour. of Elec. Eng.and Comp. Sci-
ences, 1–11 (2013), doi:10.3906/elk-1305-264

86
N. Avcu et al.
2. Baras, F., Mansour, M.M., Pearson, J.E.: Microscopic simulation of chemical bista-
bility in homogeneous systems. J. Chem. Phys. 105, 8257–8261 (1996)
3. van Hoek, M., Hogeweg, P.: The Eﬀect of Stochasticity on the Lac Operon: An
Evolutionary Perspective. Plos Comp. Biology 3, 1071–1082 (2007)
4. Gillespie, D.T.: Stochastic Simulation of Chemical Kinetics. Annu. Rev. Phys.
Chem. 58, 35–55 (2007)
5. Jacob, F., Perrin, D., Sanchez, C., Monod, J.: Operon: a group of genes with the
expression coordinated by an operator. C. R. Acad. Sci. 250, 1727–1729 (1960)
6. Julius, A., Halasz, A., Sakar, S., Harvey, R., Pappas, G.J.: Stochastic modeling
and control of biological systems: the lactose regulation system of E. coli. IEEE
Trans. Auto. Cont. 53, 51–65 (2008)
7. Kim, H., Gelenbe, E.: Stochastic Gene Expression Modeling with Hill Function for
Switch-like Gene Responses. IEEE/ACM Transactions on Computational Biology
and Bioinformatics 9, 973–9979 (2012)
8. Lestas, I., Paulsson, J., Ross, N.E., Vinnicombe, G.: Noise in gene regulatory net-
works. IEEE Trans. Automatic Control 53, 189–200 (2008)
9. ¨Ozbudak, E.M., Thattai, M., Lim, H.N., Shraiman, B.I., van Oudenaarden, A.:
Multistability in the lactose utilization network of Escherichia coli. Nature 427,
737–740 (2004)
10. Stamatakis, M., Mantzaris, N.V.: Comparison of deterministic and stochastic mod-
els of the lac operon genetic network. Biophys. J. 96, 887–906 (2009)
11. Stamatakis, M., Zygourakis, K.: Deterministic and stochastic population-level sim-
ulations of an artiﬁcial lac operon genetic network. BMC Bioinformatics 12, 301–
317 (2011)
12. Qian, H., Bishop, L.M.: The Chemical Master Equation Approach to Nonequilib-
rium Steady-State of Open Biochemical Systems: Linear Single-Molecule Enzyme
Kinetics and Nonlinear Biochemical Reaction Networks. International Journal of
Molecular Sciences 11, 3472–3500 (2010), doi:10.3390/ijms11093472
13. Trivedi, K.S.: Probability and statistics with reliability, queuing and computer
science applications. John Wiley and Sons Ltd., Chichester (2001)
14. Vellela, M., Qian, H.: Stochastic dynamics and non-equilibrium thermodynamics
of a bistable chemical system: the Schl¨ogl model revisited. J. R. Soc. Interface 6,
925–940 (2008)
15. Yagil, G., Yagil, E.: On the relation between eﬀector concentration and the rate of
induced enzyme synthesis. Biophys. J. 11, 11–27 (1971)
16. Yıldırım, N., Santill`an, M., Horike, D., Mackey, M.C.: Dynamics and bistability in
a reduced model of the lac operon. Chaos 14, 279–292 (2004)
17. Yıldırım, N., Kazancı, C.: Deterministic and stochastic simulation and analysis of
biochemical reaction networks: The lactose operon example. Methods Enz. 487,
371–395 (2011)

© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
87
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_10 
 
Intellectualized Home Environment  
as a Complex System 
Raimundas Jasinevicius, Egidijus Kazanavicius, and Vytautas Petrauskas 
Real Time Computer Systems Centre, Kaunas University of Technology,  
Studentu 50-204a, Kaunas, LT-51368, Lithuania  
{raimundas.jasinevicius,egidijus.kazanavicius, 
vytautas.petrauskas}@ktu.lt 
Abstract. In the analytical part of this report several EC projects, developing an 
idea to create the intellectualized home environment (IHE) serving for peoples’ 
comfort on the base of multiple internet things and services (IoT&S), are  
discussed, and the EC HORIZON 2020 program perspectives in this field are 
presented.  
The IHE itself is presented as a complex sociotechnical fabric with the  
inherited mixture of real human and elements of artificial intellect as well. Mul-
ti-agent-system-based intellectics is used for the IHE’s training, retraining, self-
training, and functional behavior. Three aspects of human being’s behavioral 
features, usually considered as intellectual actions, were practically realized, 
and they introduced new additional dimension to the systems complexity.  
The IHE’s complex systems model implementation was simulated to 
demonstrate the practical vitality and efficiency of the theoretical approach to 
the realization of intelligent environment of IoT&S for usre‘s comfort in two 
projects: „Research and Development of Internet Infrastructure for IoT&S in 
the Smart Environment (IDAPI)“, and “Research on Smart Home Environment 
and Development of Intelligent Technologies (BIATech)”. 
Keywords: Complex systems science, Intellectualized home environment, In-
ternet of things and services, Human being’s intellectics, Multi agent system, 
Systems training, retraining, self-training.  
1 
Introduction: State-of-the-Art 
In spite of the fact that the idea of an Internet of Things (IoT) has born many years 
ago, after Nikola Tesla‘s interview with Colliers magazine (1926 [1]) where was stat-
ed: "When wireless is perfectly applied the whole earth will be converted into a huge 
brain, which in fact it is, all things being particles of a real and rhythmic whole ... and 
the instruments through which we shall be able to do this will be amazingly simple 
compared with our present telephone,” we were forced to wait approximately a hun-
dred years to be able to start the full-fledged implementation of the idea in our home 
ambience. Huge amounts of research publications devoted to the IoT problems solu-
tions can be find today; for example, Google presents 39 600 000 citations in the year 

88 
R. Jasinevicius, E. Kazanavicius, and V. Petrauskas 
2008, and 130 000 000 - in the year 2013. Five commonly agreed areas of human life 
are considered as ready for IoT involvement: Smart transport, Smart health, Smart 
shopping, Smart city and Smart home/office are on the top of the list of broad applica-
tions of different IoT computerized gadgets, artefacts and services ([2], [3]). World-
widely recognized significance of the problem under consideration is witnessed not 
only by the yearly organized IoT forums under hospice of the IEEE (The IEEE Stand-
ards Association; an Internet of Things (IoT); 5-6 November 2013 in Silicon Valley, 
Calif. [4] , and IEEE World Forum on Internet of Things (WF-IoT); 6-8 March 2014, 
Seoul, Korea [5]), but by a very serious attitude of the European Commission (EC) to 
the problem expressed by organizing the IERC - European Research cluster on the 
IoT&S consisting of 33 research and development projects [6] as well. In general, an 
European road map for the future research and innovations in the field of Internet of 
things and services for smart home environment is discussed and delivered in the 
Horizon 2020 programs and documents (see, for example, [7], [8], and [9])  
Such an approach to various engineering applications, especially to those which 
involve IT – enabled technologies, is very popular nowadays for two reasons. The 
first one – the term “agent” itself is very attractive, as mysterious and suitable for a 
purpose of advertising new products, and sounds scientifically enough in the research 
people circles. The second reason – a fuzziness of agent’s definitions. [10] – [18] and 
many others have produced a lot of agents’ definitions, some of which are cited here. 
According to these an agent is: 
“an entity that senses its environment and acts upon it”; 
“an entity that functions continuously and autonomously in an environment in 
which other processes take place and other agents exist”; 
“a computer system that is situated in some environment, and that is capable of au-
tonomous action in this environment in order to meet its design objectives”. 
“Intelligent agent is a computer system capable of flexible autonomous actions in 
order to meet its design objectives”. 
 “Intelligent agents are software entities that carry out some set of operations on 
behalf of a user or another program, with some degree of independence and autono-
my, and in so doing, employ some knowledge or representation of the user’s goals or 
desires”. 
So, according to those popular definitions an agent is – an entity, a piece of soft-
ware or a computer system, which functions in environment in order to meet its de-
sign objectives. And if this behavior is autonomous, the agent is called intelligent. 
In this paper we consider such a definition too fuzzy, and the agent’s intelligence 
definition even too poor. We understand the danger of even the authors’ attempt to 
precisiate any term, but we take a risk to start to disclose the meaning of agents’ 
intellectics or at least to put in it very clearly understandable content and to demon-
strate its practicability in case of the intellectualized home environment (IHE). In case 
of presence of several interacting agents we have a multi agent system. IHE multi 
agent system’s intellectics is hidden in those two terms: “autonomous” and “design 
objectives”. We pretend to cover those terms by putting some contemporary mathe-
matics and soft computing modeling three most simple human beings’ features widely 
considered as his/her intellectual activity: 1) the recognition and classification (of 

 
Intellectualized Home Environment as a Complex System 
89 
patterns, processes, situations); 2) the behavior according to the set of fuzzy rules and 
3) the functioning according to some prescribed tendency. Those three activities men-
tioned above from a mathematical point of view are covered by mathematical pro-
gramming, fuzzy logic and soft computing, and stochastic approximation correspond-
ingly. 
We claim that our approach to the IHE intellectics is different than those proposed, 
for example, in [10] – [19] and others. And the purpose of this paper is to demonstrate 
on the model the practical vitality and efficiency of this approach to the realization of 
intelligent home environment of IoT&S for usre‘s comfort.  
The reminder of this paper is organized as follows: Sections 2 is devoted to the de-
scription of the IHE; in Section 3 the complex systems model’s implementation is 
presented; in the Section 4; in Section 5 the experimental simulation results are dis-
cussed. Concluding remarks are given in Section 5. 
2 
IHE: The Descriptive Approach 
In a modern world it is increasingly realized that complex systems are messy combi-
nation of many physical, technical and social systems as it is symbolically shown in 
Figure 1. 
 
Fig. 1. IHE as a system of control systems 
Here we see several evolving interacting entities each of which has a certain level 
of autonomy, heterogeneity, intelligence, uncertainty and so on, as it is written in the 
ω1. Dynamic coexistence of all those entities (for example: ω1- ω5) creates an emer-
gent behavior Ω – a “life” of the complex system [20]. As a matter of fact each home 
environment to be intellectualized is such a complex system: it is a system of control 
systems consisting not only of a certain set of sensors, actuators and means of inter  
 

90 
R. Jasinevicius, E. Kazanavicius, and V. Petrauskas 
and intra communication and smart things, but also a complex of sociotechnical fab-
rics with the inherited mixture of real human and elements of artificial intelligence as 
well. Such a model emphasizes the significance of its structural and uncertainty-based 
complexity and the behavioral complexity as well ([21]).   
An elaborated structure of the typical smart home environment and its functional 
organization is presented in Figure 2. On the level of a real smart home environment a 
lot of internet things and services (IoT&S) are presented under agents’ control. Sys-
tem of agents is shown on a real multi agent environment level (R_MAS). This MAS 
communicates with the IoT&Ss through the data normalization and conversion level. 
On this level available real data ({X}, {Y}, {K}) are converted into computerized 
format ([X], [Y], [K]) and vice versa. It must be emphasized that X, Y, and K mean 
input, output and internal state variables of each element under consideration. Real 
implementation of the R_MAS depends on concrete type of the IHS itself, and must 
be concentrated in one server, or distributed over a certain specialized network, Inter-
net or cloud, or even in a network of smart things on the real smart home environment 
level.  It is naturally to predict that in a case of complex IHE this R_MAS implemen-
tation will have a combined functional organization of solutions mentioned above. 
 
Fig. 2. Structure of the typical smart home environment and its functional organization 
Whatever the R_MAS implementation is, its intellectics must be designed and test-
ed on a certain virtual multi agent environment level (V_MAS, as it is shown in Fig-
ure 2.) according to users’ requirements, guessed application knowledge and available 
information using one of the most popular standards, for example, the IEEE FIPA [4], 
[22].  

 
Intellectualized Home Environment as a Complex System 
91 
Considering the IHE as a complex system, and keeping in mind that a number of 
the IoT&Ss is very high (in general case - thousands and thousands), and such must 
be the number of controlling agents, it is worth to use a unified description for every 
element of the system: for things, services, agents, and even for their subsystems. 
Such an approach permits us to create IHE model from those unified elements as it is 
commonly appropriate using Lego bricks.  
The best attempt to create such “Lego bricks” for complex systems description in 
space – time coordinate system was proposed by prof. Gerhard Wunch from Dresden 
University in 1975 [23]. According to him, each “brick” is presented by a raw consist-
ing of five sets of variables X, K, Y, R, T and two functional transformations Φ, Ψ: 
 
A= {X, K, Y, R, T, Φ, Ψ}; 
(1) 
here X – a set of inputs I, K – a set of “brick’s” internal states S, Y – a set of its out-
puts O, R – an independent space variable, and T – an independent time variable. 
 
Φ: X×K×R×T→K 
(2)  
and  
 
Ψ: K×R×T→Y  
 (3) 
are the transformations of “brick’s” internal states and the transformation of its out-
puts correspondingly (see figure 3). Here we mean that all dependent variables are 
functions of space and time coordinates: X(ρ, t), Y(ρ, t) and K(ρ, t). 
 
Fig. 3. Functional organization of the unified IHE element (“Lego brick”) 
Parameters (X=∝ሬሬԦ ) emitted by each element of the home environment (and by per-
sons/IHE users as well), create conditions for a certain actions ܻ= ܸሬԦ according to 
those Φ and Ψ. Any reasonable sequence of actions and their durations create one or 
several services ܻ= ܸሬԦ  . On the other hand a reasonable sequence of separate actions 
together with a set of services may be considered as IHE user’s life scenario. A vector 
type symbolic structure producing ܸሬԦ and ΠሬሬԦ is delivered in Figure 4.  
Set of evolving and changing at a time scenarios actually is a “real life” in this 
IoT&S based IHE under MAS intellectics control. The intellectics itself for actions as 
well as for services are usually transferred from virtual “V and Π agencies” into cor-
responding Φ and Ψ software/hardware based entities as it is shown in Figure 4.  
 

92 
R. Jasinevicius, E. Kazanavicius, and V. Petrauskas 
 
Fig. 4. A vector type symbolic structure producing V and Π 
It must be emphasized that real agents’ algorithms and machine codes are created 
and tested in corresponding “agencies” using abstract, model and/or real situations’ 
descriptions (X) for the V-agents, and using abstract, model and/or real actions’ de-
scriptions (V) for the Π-agents. The activities of “V and Π agencies’ ” usually are 
organized according to the users’ requirements and IEEE FIPA standards.  
3 
Intellectual Complex Systems Model Implementation 
The generalized description of the whole IHE development process is presented in 
Figure 5. Usually the team (a), consisting of users, professional experts and IT people 
accumulate, discuss and prepare a set or requirements (b) to be formalized (c) in the 
design computer BIAcomp and presented in the form of a special type of user cases 
(UC) and systems functional requirements (FR) matrix (d) [2], [6], [8]. At the same 
time an analogous FRxUC matrix must be created, which serves as an information 
necessary to produce a virtual home environment (f) in the BIAsim where the user 
and IT people are able to model, visualize and simulate in 3D-coordinate system all 
effects and consequences of requirements’ changes. The “tailor made, on a shelf 
ready” approach based software and hardware artefacts’ prototypes for the smart 
home environment’s intellectualization are selected from the data base (DB) and are 
transferred to the virtual multi agent system (MAS) (g) to be implemented in the real 
MAS (h) serving for the direct intellectualized smart home environment’s control.  
The approach based on the considerations that the IHE’s practical development is 
similar to the hierarchical complex system’s implementation leads to the following 
future steps: 1) top-down elaboration of the FRxUC matrix until we get the smallest  
 

 
Intellectualized Home Environment as a Complex System 
93 
 
Fig. 5. IHE development process’ block- diagram 
“tailor made” software/hardware artefacts as “on a shelf ready” items to be transferred 
into corresponding agents; 2) the “agents community’s” control actions, caused by the 
real life situations observed in the RHE, and the possible consequences of those  
actions,  must be tested on-line in the VHE before they are supplied for the real  
activity. 
Must be outlined that the information and knowledge obtained during the concrete 
real IHE design and implementation serves as a background for systems’ multiplication 
and industrial producing of systems’ components in a certain IT products’ supply chain. 
4 
Experimental Simulation Results 
Modeling and simulation of the simplified IHE under MAS control was performed at 
the Real Time Computer Systems Center of the Kaunas University of Technology.  
Four different intellectics technologies were used in systems’ agents for situation 
analysis and decision making: NN (artificial neural network based), FL (fuzzy reason-
ing and fuzzy control based), LP (linear programming based) and BA (Bayes method-
ology based) technologies. They were used for a virtual IHE training, retraining 
(when suddenly any user changes his/her wishes or habits), and self training (when 
the system has no a priori information and learns from the observations of user’s ac-
tivity in the HE to be intellectualized). Block diagrams of a functional organization 
for all four cases are presented in Figure 6 (a, b, c, and d). 
 

94 
R. Jasinevicius, E. Kazanavicius, and V. Petrauskas 
   
  
a) 
b) 
   
 
c) 
d) 
Fig. 6. IHE model’s functional organization for NN, FL, LP and BA cases: a) NN_technology; 
b) FL_technology; c) LP_technology; d) BA_technology 
Worth of mentioning that agents were able to increase, decrease and keep them-
selves out of any actions concerning certain parameters and characteristics under con-
trol according to the learned user’s wishes and expectations.  
All four technologies were tested according to the speed and data amount used for 
the action performance when:1) the training process takes place; 2) decision is made 
for action; 3) the retraining process takes place in two different regimes – when user’s 
wishes change momentarily and when those changes are accumulated gradually; 4) 
when the intellectualized smart environment is under permanent observation starting 
“from zero information” (the self training regime) and obtains possibility to react 
adequately to the implicit user’s wishes. The received simulation results were evalu-
ated by eight independent decision making experts according to the 10 degree system. 
Their average evaluations are presented in the Table 1- before and Table 2 after the 
round up procedure. 

 
Intellectualized Home Environment as a Complex System 
95 
Table 1. Expert evaluations before round up procedure 
 
Table 2. Expert evaluations after round up procedure 
 
The Table 1 clearly shows that according to the average performance speed the 
preference must be given to the BA technology, but the total average summary (Table 
2) confirms the opinion that all for technologies are approximately equal for the pur-
pose under investigation.   

96 
R. Jasinevicius, E. Kazanavicius, and V. Petrauskas 
The modeling and simulation confirmed a vitality of this approach and efficiency 
of the investigated MAS intellectics algorithms and technology. 
The main research ideology was developed and implemented under guidance of the 
COST Action IC0702 „Combining Soft Computing Techniques and Statistical Meth-
ods to Improve Data Analysis Solutions (SOFTSTAT)“. The research itself was per-
formed under support of two EU Structural Funds projects: a) VP1 - 3.1-ŠMM-08-K-
01-018 “Research and Development of Internet Infrastructure for IoT& S in the Smart 
Environment (IDAPI)“, and b) VP1-3.1-ŠMM-10-V-02-020 "Research on Smart 
Home Environment and Development of Intelligent Technologies (BIATech)". 
5 
Concluding Remarks 
As a matter of fact, we expect about 50 bn things and social services (IoT&SS) to be 
connected by 2030. We count on multi agent systems (MAS) as a sort of complex 
systems or „soft infrastructure“ for such systems. And in such a situation we face at 
least three approaching challenges. 
The first challenge: we need standardized and documented procedures and recom-
mendations how to create agent’s intellectics and how to implement it in a computer-
ized environment. Shortly, we need world-wide standards. 
The second challenge: we must decide, how we can cope with the three main prob-
lems accompanying the globalized approach of the IoT&SS complex systems such as: 
1. Total emission of electromagnetic signals (RFID; EPC; ...) and peoples’ safety. 
2. Who intends to control the centralized/decentralized assignment of names, IDs, 
tags and labels for things and services. 
3. What sort of social and cultural values remain stable, or will be harmed, or in-
tend to be newly developed, delivered and accepted.  
The third challenge: we must discuss and decide, what sort of researchers (and 
what sort of people in general) does the EU (and our world in general) need to cope 
with the IoT&SS complex systems problems (the task for our universities). 
And the answer is like this: THE COMPLEX SYSTEM SCIENCE is the essential 
21st Century science. This new science is providing radical new way of understand-
ing, modeling, predicting, managing the physical, biological, ecological, and social 
universe of the IoT&SS.  
References 
1. History of the Internet of Things (postscapes.com/internet-of-things-history) 
2. D1.1 - Requirements and Exploitation Strategy/BUTLER, ALBLF, No 287901, p. 178 
(2012), http://www.iot-butler.eu 
3. D3.2 Integrated System Architecture and Initial Pervasive BUTLER proof of concept/ 
BUTLER, ERC, No 287901, p. 191 (2013), http://www.iot-butler.eu 
4. The IEEE Standards Association (IEEE-SA); Internet of Things (IoT) Workshop, Novem-
ber 5-6, Silicon Valley, Calif. (2013), http://sites.ieee.org/wf-iot/ 
5. IEEE World Forum on Internet of Things (WF-IoT); March 6-8, Seoul, Korea (2014), 
http://sites.ieee.org/wf-iot/ 

 
Intellectualized Home Environment as a Complex System 
97 
6. IERC - European Research Cluster on the Internet of Things, Future Network Technolo-
gies Research and Innovation in HORIZON2020, Consultation Workshop, June 29 2012, 
Brussels, Avenue de Beaulieu 25; Dr. Ovidiu Vermesan, Coordinator of IERC, Chief Sci-
entist, SINTEF, Norway; Seventh Framework Programme, p. 54 (2012) 
7. Internet of Things in 2020. A Roadmap For The Future Infso D.4Networked Enterprise & 
Rfid Infso G.2Micro &Nanosystem. In: Co-Operation With The Rfidworking Group Of 
The European Technology Platform On Smart Systems Integration (Eposs); 05, European 
Commission; Information Society and Media, p. 32 (September 2008) 
8. Smith, I.G.: The Internet of Things 2012, New Horizons, Technical Editors: Ovidiu 
Vermesan Peter Friess Anthony Furness, p. 360. Platinum, Halifax (2012) 
9. Future Network Technologies Research and Innovation in HORIZON2020, Consultation 
Workshop, June 29, Brussels, Avenue de Beaulieu 25 Dr. Ovidiu Vermesan, Coordinator 
of IERC Chief Scientist, SINTEF, Norway (2012), http://www.internet-of-
things-research.eu 
10. Wooldridge, M.: An Introduction to Multi-Agent Systems, p. 366. John Willey & Sons, 
ltd. (2008) 
11. Shoham, Y., Tennenholtz, M.: On the emergence of social conversions: modeling, analy-
sis, and simulations. Artificial Intelligence, 165–200 (1997) 
12. Jennings, N.R., Sierra, C., Sonenberg, L., Tambe, M.: Proceedings of the Third Interna-
tional Joint Conference on Autonomous Agents and Multi Agent Systems. ACM Press, 
NY (2004) 
13. Russel, S., Norvig, P.: Artificial Intelligence: A Modern Approach. Prentice Hall (2003) 
14. Weiss, G., Sen, S.: Adaptation and Learning in Multiagent Systems. Springer, Berlin 
(1996) 
15. Shoham, Y., Leyton-Brown, K.: Multiagent Systems: Algorithmic, Game-Theoretic, and 
Logical Foundations, p. 504. Cambridge University Press (2009) 
16. Ferber, J.: Multi-agent systems; an introduction to distributed artificial intelligence, p. 509. 
Addison Wesley (1999) 
17. Hayes-Roth, B., Brownston, L., van Gent, R.: Multiagent collaboration in direct improvi-
sation. In: Proceedings of the First International Conference on Multi-Agent Systems, pp. 
148–154. AAAI Press, Menlo Park (1995) 
18. Dickinson, I.J.: Agent Standards. Living of the Vision HP Laboratories, Bristol, HPL-97-
156, p. 8 (1997) 
19. Vasseur, J.-P., Dunkels, A.: Interconnecting Smart Objects with IP - The Next Internet, p. 
432. Morgan Kaufmann (2010) 
20. Jasinevicius, R., Petrauskas, V.: On Fundamentals of Global Systems Control Science 
(GSCS). In: ISCS 2013: Interdisciplainary Symposium on Complex Systems, pp. 77–88. 
Springer (2014) 
21. Sanayei, A.: Complexity as a Linguistic Variable. Complex Systems 20(3), 253–263 
(2012) 
22. Foundation For Intelligent Physical Agents, FIPA Ontology Service Specification, Gene-
va, Switzerland, p. 58. Foundation for Intelligent Physical Agents (2001),  
http://www.fipa.org/ 
23. Wunch, G.: Systemtheorie. Akademische Verlag-sgesellschaft Geest& Porting K.- G. 
Leipcig, p. 286 (1975) 

Desertiﬁcation Transition in Semi-arid
Ecosystems and Directed Percolation
Raﬀaele Corrado1, Anna Maria Cherubini2, and Cecilia Pennetta2,3,⋆
1 PhD School on Climate Change Sciences, Universit`a del Salento, I-73100 Lecce and
Istituto di Scienze dell’Atmosfera e del Clima, CNR, I-73100 Lecce, Italy
2 Dipartimento di Matematica e Fisica “Ennio De Giorgi”, Universit`a del Salento,
I-73100 Lecce, Italy
3 Istituto Nazionale di Fisica Nucleare (INFN), Italy
cecilia.pennetta@unisalento.it
Abstract. Regime shifts in ecosystems caused by climatic or anthro-
pogenic factors can happen on a relatively short timescale with relevant
economic and social eﬀects, a consideration which motivates the large in-
terest in the literature to this topic. A special case of regime shift is given
by desertiﬁcation transitions in semi-arid ecosystems. One desertiﬁcation
model, recently proposed, seems particularly eﬀective in describing sev-
eral ecological landscapes, taking into account diﬀerent ecological mech-
anisms. This model simulates an ecosystem undergoing a desertiﬁcation
transition in term of a stochastic cellular automaton (SCA) subjected to
a damage spreading (DS) transition. On the other hand, it is well known
that many DS transitions belong to the directed percolation (DP) univer-
sality class under certain rather general conditions. Here we investigate
the universality class of the SCA model and we identify the region of
parameters space inside which it belongs to the DP class.
Keywords: Desertiﬁcation, Percolation in phase transitions, Critical
transitions, Complex biological systems.
1
Introduction
Climatic or anthropogenic factors can induce or accelerate the regime shift of
an ecosystem [1, 2]. This kind of transitions often implies relevant economic and
social eﬀects, in particular when they occur on a relatively short timescale, a
consideration which motivates the large interest in the literature to this topic
[1–16]. An important case of regime shift is given by the desertiﬁcation transi-
tion in arid or semi-arid ecosystems [1]. Concerning this transition, many eﬀorts
are devoted to the identiﬁcation of early warning signals and early transition
indicators [10–18]. Among the diﬀerent approaches that have been proposed
to study desertiﬁcation processes [10–16], the model introduced by K´eﬁet al.
[15] seems particularly eﬀective in describing several ecological landscapes, tak-
ing into account diﬀerent ecological mechanisms [14, 15] and providing a good
⋆Corresponding author.
c
⃝Springer International Publishing Switzerland 2015
99
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_11

100
R. Corrado, A.M. Cherubini, and C. Pennetta
starting point for the identiﬁcation of early warning signals [10, 17, 18]. This
model simulates an ecosystem undergoing a desertiﬁcation transition in term of
a stochastic cellular automaton (SCA) subjected to a damage spreading (DS)
transition [19–24]. On the other hand, it is well known that many DS transi-
tions belong to the directed percolation (DP) universality class under certain
rather general conditions [20–23]. In fact, according to the so called Grassberger
conjecture [23], short-range interacting systems undergoing continuous phase
transition into an absorbing state, in absence of frozen disorder and without
additional symmetries, belong to the DP class [20–23].
Actually for a wide region of the parameters space the SCA model of K´eﬁ
et al. predicts continuous transitions into a state characterized by the complete
extinction of vegetation (full desertiﬁcation transition) [17, 25]. The model does
not contain explicitly long-range interactions, frozen disorder or additional sym-
metries. However the critical exponents, β and γ′, characterizing the approach to
the extinction threshold respectively of the vegetation density and its variance,
are found diﬀerent from the DP values [17, 26] and strongly dependent on the
model parameters [25]. So here we investigate the universality class of the SCA
model and we identify the region of parameter space inside which it belongs to
the DP class.
2
Model and Discussion
The desertiﬁcation model of K´eﬁet al. is a two-dimensional SCA [14, 15]. The
ecosystem is modeled as a L × L square lattice, where the linear size of the
lattice L is multiple of the size of the elementary cell. Each cell can be found
in three states: {+} (cell covered by vegetation or living cell); {0} (cell empty
but colonizable or dead cell); {−} (cell empty and degraded). A degraded cell
cannot be colonized before recovering, i.e. moving ﬁrst to the {0} state. Similarly,
also the reverse transition {+} →{−} is forbidden. The rates of the allowed
transitions are:
W+0 = m .
(1)
W0+ =

δρ+ + (1 −δ) q+|0

(b −cρ+) .
(2)
W0−= d .
(3)
W−0 = r + fq+|−.
(4)
Equations (1), (2),(3) and (4) deﬁne respectively the rates of the mortality,
colonization, degradation and recovery processes. The mortality parameter m
measures the strength of external stresses and is taken here as the control pa-
rameter of the transition. ρ+ is the global vegetation mass fraction or living
cells density (i.e. total number of living cells normalized to the total number of

Desertiﬁcation Transition in Semi-arid Ecosystems
101
lattice cells) and it plays the role of order parameter of the transition. ρ0 and
ρ−are the densities of {0} and {−} cells respectively (ρ+ + ρ0 + ρ−= 1). qi|j
is the fraction of ﬁrst neighbors in the state i around a cell in the state j (the
average of qi|j over all the pairs of cells gives the joint probability of ﬁnding a
{i}-cell as a ﬁrst neighbor of a {j}-cell). δ gives the fraction of seeds globally
dispersed by wind, animals etc. b is the colonization parameter, controlling sev-
eral intrinsic properties of a vegetated cell, such as seed production rate, seed
survival, germination and survival probabilities (intrinsic properties because b
accounts for these features without considering global competition eﬀects among
plants). The strength of competition eﬀects is instead controlled by the param-
eter c. The parameter d, representing the rate of soil degradation, depends on
the intrinsic soil characteristics, on climatics and/or anthropogenic factors. f is
the local facilitation parameter which describes positive local interactions among
plants and between soil and plants. Finally, r is the spontaneous regenerative
rate of a degraded cell in absence of vegetation covering ﬁrst neighbor cells.
The evolutions of the CA are obtained by numerical simulations with syn-
cronous update of the states of all the lattice cells and by associating each
iteration step with an elementary time step in an appropriate time-scale. Con-
sequently, transition rates are expressed in arbitrary units. Periodic boundary
conditions are used for the calculation of the transition rates at the boundaries.
The initial conﬁgurations of the CA are taken corresponding to ρ+(t0) = 0.5,
ρ0(t0) = 0.2, with random distribution of {+} and {0} cells. In the region of
parameter space explored, the initial conﬁguration do not aﬀect the statistical
properties of the stationary regime discussed here. The results reported below
for ⟨ρ+⟩and ⟨Δρ2
+⟩are obtained by averaging over the stationary portion of the
time series, after the relaxation of the initial transient. The typical length of the
time series was 1÷ 5 × 104 records. In the following we consider lattices of linear
size L = 100. More details on the model can be found in [14, 15] and in [17]).
To clarify the relation of the SCA model with the DP universality class, we ﬁrst
have considered the question of the eﬀective range of the interactions among the
diﬀerent cells. Diﬀerently from Eq. (4), which concerns the recovery process and
allows only ﬁrst-neighbor interactions, Eq. (2) which describes the colonization
process includes two contributions dependent on the global density ρ+. These
contributions account for two diﬀerent ecological mechanisms: seed dispersion
(when δ ̸= 0) and competition among plants for global resources (when c ̸= 0).
Both these mechanisms are expected to act on a long or at least on an inter-
mediate range of distances, even if they are described in Eq. (2) by the average
quantity ρ+, thus by adopting a sort of mean ﬁeld approximation. To suppress
these eﬀects, keeping only strictly short-range interactions, we performed nu-
merical simulations of the CA by taking δ = 0 and c = 0. Figure 1 compares the
two plots of ⟨ρ+⟩as a function of m obtained for δ = 0 and c = 0 (blue circles)
and δ = 0.1 and c = 0.3 (green trianges). The values of the other parameters
are: b = 0.6, f = 0.9, d = 0.2, r = 0.0004 in both cases. These values of the
parameters have been chosen as reasonable values to simulate ”real world” ﬁeld
data [14, 10]. Similarly, Fig. 2 compares the behavior of the variance ⟨Δρ2
+⟩vs.

102
R. Corrado, A.M. Cherubini, and C. Pennetta
0
0.05
0.1
0.15
0.2
m
0.0
0.2
0.4
0.6
0.8
1.0
<ρ+>
 δ=0.1, c=0.3, d=0.2, r=0.0004
 δ=0,  c=0,  d=0.2, r=0.0004
b=0.6, f=0.9
Fig. 1. Average density of the living cells ⟨ρ+⟩vs. mortality rate m. The values of the
other parameters are reported in the ﬁgure. Triangles and circles show the numerical
data obtained respectively by including (δ ̸= 0 and c ̸= 0) or not (δ = 0 and c = 0) the
eﬀect of the living cell density on the colonization rate.
0
0.05
0.1
0.15
0.2
m
0.0000
0.0004
0.0008
0.0012
<Δρ+
2>
 δ=0.1, c=0.3, d=0.2, r=0.0004
 δ=0,  c=0,  d=0.2, r=0.0004
b=0.6, f=0.9
Fig. 2. Variance of the living cells density ⟨Δρ2
+⟩vs. mortality rate m. The values of
the other parameters are reported in the ﬁgure. Triangles and circles show the data
obtained respectively by including (δ ̸= 0 and c ̸= 0) or not (δ = 0 and c = 0) the
eﬀect of the living cell density on the colonization rate.

Desertiﬁcation Transition in Semi-arid Ecosystems
103
m for the two sets of data. It must be noted that the variance sharply increases
in both cases, approaching the critical value of mortality rate mc associated
with the full extinction of the vegetation, i.e. the threshold value of m above
which ⟨ρ+⟩= 0 [17]. Actually, the rise in the variance of the vegetation density
provides a well recognized indicator of the desertiﬁcation transition [3, 10, 17].
It must also be remarked that competitive eﬀects, which limit the colonization
process, accelerate the vegetation extinction, lowering the critical threshold mc
from 0.1860 ± 0.0001 to 0.1685 ± 0.0001 (Figs. 1 and 2).
0.0001
0.0010
0.0100
0.1000
1.0000
mc−m
0.10
1.00
<ρ+>
δ=0.1, c=0.3, d=0.2, r=0.0004, mc=0.1685
δ=0,   c=0,  d=0.2, r=0.0004, mc=0.1860
δ=0.1, c=0.3, d=0, r=0, mc=0.4750
δ=0,  c=0, d=0, r=0, mc=0.4102
slope=0.39
slope=0.28
slope=0.99
slope=0.58
Fig. 3. Log-Log plot of the average density of living cells ⟨ρ+⟩vs. |m−mc|. The values
of the parameters are reported in the ﬁgure together with the corresponding values of
the critical mortality mc. The two upper curves correspond to d ̸= 0: triangles and
open circles are obtained respectively by including (δ ̸= 0 and c ̸= 0) or not (δ = 0
and c = 0) the eﬀect of the living cell density on the colonization rate. The two lower
curves show the behavior for d = 0: diamonds are obtained for δ ̸= 0 and c ̸= 0 and
full circles for δ = 0 and c = 0. The straight lines are the best-ﬁt with power laws.
Figure 3 displays on a double logarithmic scale the same ⟨ρ+⟩data sets of Fig.
1 as a function of |m−mc|. The straight lines are the best-ﬁt with power laws of
exponent β = 0.28 ± 0.01 (for δ = 0 and c = 0) and β = 0.39 ± 0.01 (for δ = 0.1
and c = 0.3), where β is the critical exponent characterizing the scaling of the
average living cell density when the system approaches the mortality threshold:
⟨ρ+⟩= Cρ|m −mc|β .
(5)
The double logarithmic plot of the variance (same data of Fig. 2) as a function
of |1 −mc/m| is shown in Fig. 4. The straight lines are the best-ﬁt with power
laws of slope −0.59 ± 0.01 (for δ = 0 and c = 0) and −0.50 ± 0.01 (for δ = 0.1
and c = 0.3). Thus the variance can be written as:

104
R. Corrado, A.M. Cherubini, and C. Pennetta
10
−4
10
−3
10
−2
10
−1
10
0
10
1
|1−mC/m|
10
−5
10
−4
10
−3
10
−2
<Δρ+
2>
 δ=0.1, c=0.3, d=0.2, r=0.0004, mc=0.1685
 δ=0,  c=0,  d=0.2, r=0.0004, mc=0.1860
 δ=0,  c=0,   d=0,  r=0, mC=0.4102
 δ=0.1, c=0.3, d=0, r=0, mc=0.4750
slope=−0.33
slope=−0.59
slope=−0.50
slope=−0.044
Fig. 4. Log-Log plot of the variance ⟨Δρ2
+⟩vs. |1 −mc/m|. The values of the param-
eters are reported in the ﬁgure together with the corresponding values of the critical
mortality mc. Triangles and open circles show the data obtained for d ̸= 0. Precisely:
triangles correspond to δ ̸= 0 and c ̸= 0 and open circles to δ = 0 and c = 0. Diamonds
and full circles show the behavior for d = 0. Diamonds: δ ̸= 0 and c ̸= 0; full circles:
δ = 0 and c = 0. The straight lines are the best-ﬁt with power laws.
⟨Δρ2
+⟩= CΔ
1 −mc
m

−γ′
.
(6)
where γ′ is the critical exponent of the variance. Therefore, the values of β and
γ′ obtained for δ = 0 and c = 0, strongly diﬀer from the values reported in
the literature [20–22, 26] for the corresponding critical exponents of the DP
class in 2D systems: βDP = 0.583 ÷ 0.584 and γ′
DP = 0.30 ÷ 0.35. In spite of
the suppression of the ρ+ dependent terms in the expression of the colonization
rate, Eq. (2), which explicitely limits the range of interactions to ﬁrst neighbors
only, the SCA desertiﬁcation model does not belong to the DP class.
On the other hand, it should be noted that numerical simulations [25] have
proved the independence of both β and γ′ from the value of the facilitation
parameter f associated with short-range interactions. Similarly, we expect that
when δ = 0 and c = 0, the exponents β and γ′ would be independent also of
b, intrinsic colonization parameter. In other terms, we expect that when the
evolution of the CA is controlled uniquely by short-range interactions, the only
parameters controlling the universality class of the model are d and r, as we
discuss below. For contrast, when δ ̸= 0 and c ̸= 0, the model provides non
universal exponents, depending not only on d and r but also on δ, c, b (which
determine the interplay between short and long-range interactions).
Actually, the model under considerationis a three states CA. However dead cells
are mainly conﬁned to the interface between living and degraded cells [17]. There-
fore, approaching the desertiﬁcation transition, dead cells essentially represent the

Desertiﬁcation Transition in Semi-arid Ecosystems
105
perimeter of the living cell clusters embedded in the ”sea” of degraded cells. As a
consequence, close to the transition, when living cell clusters are few and small, de-
graded cells dominate. The absorbing state, over the threshold mc, is characterized
by: ρ+ = 0 and ⟨ρ−⟩>> ⟨ρ0⟩. However, in general ⟨ρ0⟩̸= 0. In fact, according to
Eq. (4), even when ρ+ = 0 and q+|−= 0, there is a spontaneous recovery proba-
bility if r ̸= 0. After the complete vegetation extinction, the system reaches an ab-
sorbing state which is not completely frozen but which corresponds to a two state
CA, characterized by purely random (white noise) ﬂuctuations of the number of de-
graded cells: {0} ↔{−}. In particular, over the vegetation extinction threshold,
⟨ρ−⟩and ⟨ρ0⟩depend only on the degradation and recovery rates:
⟨ρ0⟩=
r
r + d
;
⟨ρ−⟩=
d
r + d
(7)
Thus a single frozen absorbing state exists only when: i) r = 0 and d ̸= 0
(the absorbing state is a completely degraded lattice); ii) d = 0 and r ̸= 0 (the
absorbing state is an empty lattice); iii) r = 0 and d = 0. The last two conditions
actually deﬁne a two states CA with only {+} and {0} cells (it should be noted
that when d = 0 and ρ+(t0) ̸= 0, even if there is an initial non zero concentration
of degraded cells, the stationary state of the lattice contains only {+} and {0}
cells, as a consequence of Eq. (4)). In all the three cases, the critical exponents
are expected to be the same of the directed percolation in 2D, at least when
only short-range interactions are present. Figures 3 and 4 respectively display
⟨ρ+⟩and its variance ⟨Δρ2
+⟩calculated by taking: δ = 0, c = 0, d = 0, r = 0
and δ = 0.1, c = 0.3, d = 0, r = 0. Both curves are obtained by considering
b = 0.6 and f = 0.9 (when c = 0 and d = 0, f only determines the relaxation
time to achieve the stationary state of the two states CA). The ﬁrst set of
parameter values actually provides β = 0.58 ± 0.01 and γ′ = 0.33 ± 0.01, in good
agreement with the values reported in the literature for the DP universality
class. On the contrary, the second set of parameters gives β = 0.99 ± 0.01 and
γ′ = 0.044 ± 0.001.
3
Conclusions
Among the diﬀerent approaches that have been proposed to study desertiﬁcation
processes, the model introduced by K´eﬁet al. [15] seems particularly eﬀective in
describing several ecological landscapes, taking into account diﬀerent ecological
mechanisms [14, 15] and providing a good starting point for the identiﬁcation of
early warning signals [10, 17, 18]. This model simulates an ecosystem undergoing
a desertiﬁcation transition in term of a SCA subjected to a damage spreading
transition [19–24]. Here we have investigated the universality class of the SCA
model, looking for its relation with the directed percolation [20–23]. The results
of our numerical simulations show that two features of the SCA desertiﬁcation
model are responsible of its non universal critical behavior and deviation from
the DP class: the presence of the ρ+ dependent terms in the colonization rate,
which destroy the requirement of short-range interactions, and the existence of
a not frozen absorbing state when d ̸= 0 and r ̸= 0.

106
R. Corrado, A.M. Cherubini, and C. Pennetta
Our results, clarifying some features of the SCA model and in particular the
behavior of its critical exponents can help the research of early indicators of
the desertiﬁcation transition. In particular, since the rise in the variance of the
vegetation density is a well recognized indicator of the desertiﬁcation transition
[3, 10, 17] and the steepness of this rise is determined by γ′, the study of the rel-
ative impact of the diﬀerent ecological factors (rates of colonization, facilitation,
degradation, etc.) on the critical exponent of the variance is of help to identify
the most relevant actions which can contrast desertiﬁcation phenomena.
Acknowledgments. A.M.C. thanks the members of the Dynamical Systems
Group (DynamIC) at the Department of Mathematics of Imperical College Lon-
don for friendly collaboration during her academic visit, R.C. acknowledges
the support from the Euro-Mediterranean Center on Climate Change (CMCC).
The authors thank Profs. Nadav Shenrb and Haim Weissmann (Department of
Physics, Bar-Ilan University) for stimulating comments.
References
1. Reynolds, J.F., Staﬀord Smith, D.M., Lambin, E.F., Turner II, B.L., Mortimore,
M., Batterbury, S.P.J., Downing, T.E., Dowlatabadi, H., Fern´andez, R.J., Her-
rick, J.E., Huber-Sannwald, E., Jiang, H., Leemans, R., Lynam, T., Maestre, F.T.,
Ayarza, M., Walker, B.: Global desertiﬁcation: building a science for dryland de-
velopment. Science 316, 847–851 (2007)
2. Rietkerk, M., Dekeer, S.C., de. Ruiter, P.C., van de Koppel, J.: Self-organized
patchiness and catastrophic shifts in ecosystems. Science 305, 1926–1929 (2004)
3. Carpenter, S.R., Brock, W.A.: Rising variance: a leading indicator of ecological
transition. Ecology Lett. 9, 311–318 (2006)
4. Dakos, V., Scheﬀer, M., van Nes, E.H., Brovkin, V., Petoukhov, V., Held, H.:
Slowing down as an early warning signal for abroupt climate change. PNAS 105,
14308–14312 (2008)
5. Scheﬀer, M., Bascompte, J., Brock, W.A., Brovkin, V., Carpenter, S.R., Dakos,
V., Held, H., van Nes, E.H., Rietkerk, M., Sugihara, G.: Early-warning signals for
critical transitions. Nature 461, 53–59 (2009)
6. Asner, G.P., Knapp, D.E., Balaji, A., P´aez-Acosta, G.: Automated mapping of
tropical deforestation and forest degration: CLASlite. J. of Appl. Remote Sensing 3,
033543-1–13 (2009)
7. Donangelo, R., Fort, H., Dakos, V., Scheﬀer, M., van Nes, E.H.: Early warnings
for catastrophic shifts in ecosystems: comparison between spatial and temporal
indicators. Int. J. of Bifurcation and Chaos 20, 315–321 (2010)
8. Dakos, V., van Nes, E.H., Donangelo, R., Fort, H., Scheﬀer, M.: Spatial correlation
as leading indicator of catastrophic shifts. Theor. Ecol. 3, 163–174 (2010)
9. Dahlin, K.M., Asner, G.P., Field, C.B.: Environmental and community controls
on plant canopy chemistry in a Mediterranean-type ecosystem. Procs. Nat. Ac. of
Science 110, 6895–6900 (2013)
10. K´eﬁ, S., Guttal, V., Brock, W.A., Carpenter, S.R., Ellison, A.M., Livina, V.N.,
Seekell, D.A., Scheﬀer, M., van Nes, E.H., Dakos, V.: Early warnings signals of
ecological transitions: methods for spatial patterns. PLoS One 9, e41010 (2012)

Desertiﬁcation Transition in Semi-arid Ecosystems
107
11. von Hardenberg, J., Meron, E., Shachak, M., Zarmi, Y.: Diversity of vegetation
patterns and desertiﬁcation. Phys. Rev. Lett. 87, 198101–198104 (2001)
12. Rietkerk, M., Boerlijst, M.C., van Langvelde, F., HilleRis Lambers, R., van de
Koppel, J., et al.: Self-organization of vegetation in arid ecosystems. American
Naturalist 160, 524–530 (2002)
13. Shnerb, N.M., Sarah, P., Lavee, H., Solomon, S.: Reactive glass and vegetation
patterns. Phys. Rev. Lett. 90, 038101–1–4 (2003)
14. K´eﬁ, S., Rietkerk, M., Alados, C.L., Pueyo, Y., Papanastasis, V.P., ElAich, A., de
Ruiter, P.C.: Spatial vegetation patterns and imminent desertiﬁcation in Mediter-
ranean arid ecosystems. Nature 449, 213–217 (2007)
15. K´eﬁ, S., Rietkerk, M., van Baalen, M., Loreau, M.: Local facilitation, bistability
and transitions in arid ecosystems. Theor. Popul. Biolog 71, 367–379 (2007)
16. Scanlon, T.M., Caylor, K.K., Levin, S.A., Rodriguez-Iturbe, I.: Positive feedbacks
promote power-law clustering of Kalahari vegetation. Nature 449, 209–212 (2007)
17. Corrado, R., Cherubini, A.M., Pennetta, C.: Early warning signals of desertiﬁcation
transitions in semi-arid ecosystems (submitted for publication)
18. Corrado, R., Cherubini, A.M., Pennetta, C.: Signals of Critical Transitions in
Ecosystems Associated with Fluctuations of Spatial Patterns. In: 22nd Int. Conf.
Noise and Fluctuations (ICNF), pp. 1–4. IEEE Press, New York (2013)
19. Ilachinski, A.: Cellular Automata, a Discrete Universe. World Scientiﬁc, Singapore
(2002)
20. Henkel, M., Hinrichsen, H., L˜ubeck, S.: Non-Equilibrium Phase Transitions.
Springer, Berlin (2008)
21. ´Odor, G.: Universality classes in nonequilibrium lattice systems. Review of Modern
Physics 76, 663–724 (2004)
22. Hinrichsen, H.: Non-equilibrium critical phenomena and phase transitions into ab-
sorbing states. Advances in Phys. 49, 815–958 (2000)
23. Grassberger, P.: Are damage spreading transitions generically in the universality
class of directed percolation. Journ. Stat. Phys. 79, 13–23 (1995)
24. Bagnoli, F.: On damage spreading transitions. Journal Stat. Phys. 85, 151–164
(1996)
25. Corrado, R., Cherubini, A.M., Pennetta, C.: Critical Desertiﬁcation Transition
in Semi-Arid Ecosystems: The Role of Local Facilitation and Colonization Rate
(submitted for publication)
26. Munoz, M.A., Dickman, R., Vespignani, A., Zapperi, S.: Avalanche and spreading
exponents in systems with absorbing states. Phys. Review E 59, 6175–6179 (1999)

Lognormality Observed for Additive Processes:
Application to Turbulence
Hideaki Mouri
Meteorological Research Institute, Nagamine, Tsukuba 305-0052, Japan
Abstract. The lognormal distribution has been observed across the nat-
ural and social sciences. It is always ascribed to a multiplicative process,
i.e., a product of random variables, or equivalently to nonlinearity of the
system, which always appears to be complex. However, we ﬁnd that the
lognormal distribution is also observable for a sum of random variables,
i.e., an additive process in a linear system. The application is shown for
large-scale ﬂuctuations of ﬂuid turbulence.
Keywords: statistical description of complex systems.
1
Introduction
If the logarithm of a positive random variable z > 0 is Gaussian, z is said to obey
a lognormal distribution, which has been observed across the natural and social
sciences [1]. The current paradigm is that any lognormality is caused by a mul-
tiplicative process [2], i.e., a product of random variables N
n=1 zn. Its logarithm
is a sum of random variables N
n=1 ln zn. This tends to a Gaussian distribution
as N →∞, according to the central limit theorem [3], if the summands have
ﬁnite variances, if none of them dominates the others, and also if they do not so
depend on one another. As a result, N
n=1 zn tends to a lognormal distribution.
The multiplicative process is in turn ascribed to some nonlinearity of the system,
which always appears to be complex.
However, for a class of positive random variables, we ﬁnd that the sum tends
faster to a lognormal distribution [4]. While the sum tends eventually to a Gaus-
sian distribution, the distribution of the sum is close to a lognormal distribution
rather than to any Gaussian distribution so far as the number of the summands
is large enough. Thus, the lognormal distribution is also observable for a sum,
i.e., an additive process in a linear system that consists of independent subsys-
tems. We discuss the condition for such a sum [4], an analytical example [4], and
the application to large-scale ﬂuctuations of ﬂuid turbulence [5].
2
Condition for Additive Processes
To study an additive process, we consider an average of positive random variables
zn > 0,
¯zN = 1
N
N

n=1
zn .
(1a)
c
⃝Springer International Publishing Switzerland 2015
109
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_12

110
H. Mouri
Instead of moments ⟨zm⟩, we use cumulants ⟨zm⟩κ. They are ⟨z2⟩κ = ⟨(z−⟨z⟩)2⟩,
⟨z3⟩κ = ⟨(z −⟨z⟩)3⟩, ⟨z4⟩κ = ⟨(z −⟨z⟩)4⟩−3⟨(z −⟨z⟩)2⟩2, and so on [3]. The
cumulant for m = 2 is the variance.
We explain the central limit theorem in a simple form, by assuming that the
variables zn are independent and identically distributed with the distribution of
some variable z∗. Its cumulants are all assumed to be ﬁnite. These assumptions
lead to ⟨¯zm
N ⟩κ = N
n=1⟨zm
n ⟩κ/N m = ⟨zm
∗⟩κ/N m−1. The skewness and kurtosis of
¯zN are obtained as
⟨¯z3
N⟩κ
⟨¯z2
N⟩1.5
κ
=
1
N 0.5
⟨z3
∗⟩κ
⟨z2∗⟩1.5
κ
and
⟨¯z4
N⟩κ
⟨¯z2
N⟩2κ
= 1
N
⟨z4
∗⟩κ
⟨z2∗⟩2κ
.
(1b)
With an increase in N, they decay to the Gaussian value of 0. A faster decay to
this value is found for ⟨¯zm
N ⟩κ/⟨¯z2
N⟩m/2
κ
∝1/N m/2−1 at m ≥5. Thus, ¯zN tends to
a Gaussian distribution.
The central limit theorem is approximate in that it ignores the tails of the
distribution at deviations of ¯zN from ⟨¯zN⟩that are larger than several of ⟨¯z2
N⟩0.5
κ .
It is still a practical approximation because those tails are not exactly determined
in any actual observation. Also, in any actual system, N is not inﬁnite. We deﬁne
that ¯zN is observed to be Gaussian, irrespective of its exact distribution, if its
skewness and kurtosis are close enough to 0.
We regard the additive process (1a) as if it were multiplicative. By ignoring
large deviations of zn from ⟨zn⟩, the arithmetic average of zn is approximated
as a geometric average, N
n=1 zn/N ≃(N
n=1 zn)1/N. Then,
¯zN ≃
 N

n=1
zn
1/N
and
ln ¯zN ≃1
N
N

n=1
ln zn .
(2a)
The above approximation is used at large N as an exact relation. We do expect
the exactness in the limit N →∞if zn is distributed within a ﬁnite range. The
skewness and kurtosis of ln ¯zN are
⟨(ln ¯zN)3⟩κ
⟨(ln ¯zN)2⟩1.5
κ
=
1
N 0.5
⟨(ln z∗)3⟩κ
⟨(ln z∗)2⟩1.5
κ
and
⟨(ln ¯zN)4⟩κ
⟨(ln ¯zN)2⟩2κ
= 1
N
⟨(ln z∗)4⟩κ
⟨(ln z∗)2⟩2κ
.
(2b)
With an increase in N, they decay to the Gaussian value of 0. Thus, although ¯zN
is observed to become Gaussian, ¯zN is also observed to become lognormal. The
large deviations ignored to observe the lognormal distribution are not always the
same as those ignored to observe the Gaussian distribution.
For ¯zN to tend to a lognormal distribution faster than to any Gaussian dis-
tribution, a necessary condition is obtained as

⟨(ln z∗)3⟩κ
⟨(ln z∗)2⟩1.5
κ
 <

⟨z3
∗⟩κ
⟨z2∗⟩1.5
κ

and

⟨(ln z∗)4⟩κ
⟨(ln z∗)2⟩2κ
 <

⟨z4
∗⟩κ
⟨z2∗⟩2κ
 .
(3)
This is also a practically suﬃcient condition, which implies that ¯zN at large N
is observed to be lognormal rather than to be Gaussian so far as the observation
is based on the skewness and kurtosis as deﬁned above. Since the condition (3)
is not strict, it would hold for a wide class of positive random variables.

Lognormality Observed for Additive Processes
111
Fig. 1. Skewness and kurtosis of z and of ln z as a function of γ for the gamma dis-
tribution (4a). The dotted curves are the asymptotes (4c). The dot-dashed lines stand
for the Gaussian value of 0.
3
Example: Gamma Distribution
We discuss the gamma distribution of a positive random variable z > 0 [3] as
an example to observe the lognormality for an additive process. Its probability
density is
f(z) =
1
Γ(γ)zγ−1 exp(−z) .
(4a)
Here Γ is the gamma function. The parameter γ > 0 determines the shape of the
distribution. While γ = 1/2 is for the distribution of the square of a zero-mean
Gaussian random variable, γ = 1 is for an exponential distribution.
Since the cumulant of z is ⟨zm⟩κ = γ(m −1)!, its skewness and kurtosis are
obtained as
⟨z3⟩κ
⟨z2⟩1.5
κ
=
2
γ0.5
and
⟨z4⟩κ
⟨z2⟩2κ
= 6
γ .
(4b)
The cumulant of ln z is written as ⟨(ln z)m⟩κ = dm ln⟨exp(iξ ln z)⟩/d(iξ)m|ξ=0 =
dm ln Γ(γ)/dγm [3]. This is a poly-gamma function. From its asymptote for γ →
∞[6], we ﬁnd ⟨(ln z)m⟩κ →(−1)m(m −2)!/γm−1 at m ≥2. The skewness and
kurtosis of ln z are
⟨(ln z)3⟩κ
⟨(ln z)2⟩1.5
κ
→−1
γ0.5
and
⟨(ln z)4⟩κ
⟨(ln z)2⟩2κ
→2
γ
as
γ →∞.
(4c)
These are good approximations for γ ≳100 as shown by dotted and solid curves
in Fig. 1.
The gamma distribution is reproductive [3]. Recall the additive process (1a)
for independent random variables zn. If each of them obeys a gamma distribution
for γ = γ∗, the sum N
n=1 zn obeys a gamma distribution for γ = Nγ∗. The
distribution of the average ¯zN = N
n=1 zn/N has the same shape.

112
H. Mouri
Fig. 2. Energy budget in three-dimensional turbulence. The open arrows are the local
and instantaneous energy transfer. The ﬁlled arrows are the mean energy transfer along
with the energy injection and dissipation.
With an increase in N = γ/γ∗, we ﬁnd that ⟨(ln ¯zN)m⟩κ/⟨(ln ¯zN)2⟩m/2
κ
at
m ≥3 decays to the Gaussian value of 0 faster than ⟨¯zm
N ⟩κ/⟨¯z2
N⟩m/2
κ
by a factor
of m −1 in units of N m/2−1. In particular, the skewness and kurtosis of ln ¯zN
decay faster than those of ¯zN by factors of 2 and 3. Thus, ¯zN tends to a lognormal
distribution faster than to any Gaussian distribution. If N is large, ¯zN is observed
to be lognormal on the basis of our deﬁnition. The condition (3) actually holds
for any value of γ∗> 0 in Fig. 1.
The gamma distribution is caused by many processes. We accordingly expect
that it explains at least some of the observations of the lognormality in terms of
the additive process. For example, the duration τ of an event of a complex system
is often observed to be lognormal [1], which could be reproduced by a series of
subevents, N
n=1 τn, if each of them is exponential, f(τn) ∝exp(−τn/τ∗), or
equivalently obeys the gamma distribution for γ∗= 1.
4
Application: Fluid Turbulence
The kinetic energy of turbulence is injected at some length scale L. As sketched in
Fig. 2, if the turbulence is three dimensional, the energy is on average transferred
to smaller and smaller scales because it is eventually dissipated into heat at the
smallest scale of the Kolmogorov length η. This is known as the energy cascade
[7]. However, locally and instantaneously, the energy is transferred to the larger
scales as well as to the smaller scales. As a result, it is transferred to scales much
larger than L and cause ﬂuctuations there [8]. The presence of such large-scale
ﬂuctuations was pointed out for the ﬁrst time by L.D. Landau [9].
If the turbulence is homogeneous, any spatial correlation is negligible at the
large scales. The kinetic energy there is additive. Its value for a large-scale region
is the sum of its values for the yet large-scale subregions that are not correlated
at all. This situation is in contrast to the situation of small-scale motions, which
are correlated, multiplicative, and thus typical of complex systems [7]. We are to

Lognormality Observed for Additive Processes
113
Fig. 3. Probability densities of ¯v2
R/⟨¯v2
R⟩and of (ln ¯v2
R −⟨ln ¯v2
R⟩)/⟨(ln ¯v2
R −⟨ln ¯v2
R⟩)2⟩0.5
at R = 60L for the data of a ﬂow of grid turbulence G3 (circles), a turbulent boundary
layer B3 (triangles), and a turbulent jet J3 (squares) taken from our past experiments
[8]. The solid curve stands for the Gaussian distribution.
demonstrate that the energy is ﬂuctuating at those large scales in a lognormal
distribution.
Consider that a velocity component v(x) is obtained along a one-dimensional
section x of the three-dimensional space, as is usual in experiments of turbulent
ﬂows. The average ⟨v⟩is subtracted to have ⟨v⟩= 0. As an estimate of the scale
L of the energy injection, we use the correlation length of the energy v2 [8]:
L =
	 ∞
0 ⟨[v2(x + r) −⟨v2⟩][v2(x) −⟨v2⟩]⟩dr
2⟨v2⟩2
.
(5a)
The one-dimensional section is divided into segments with length R. For each
segment, the energy v2 is coarse-grained as
¯v2
R(x∗) = 1
R

 +R/2
−R/2
v2(x∗+ x) dx .
(5b)
This corresponds to the additive process (1a) in case of R ≫L, where the corre-
lations of v2 are negligible. The segment serves as a linear system that consists
of independent subsegments. In each of them, we regard v2 as a constant. Since
v is closely Gaussian, the distribution of v2 is close to the gamma distribution
(4a) for γ = 1/2. It satisﬁes the condition (3) for a lognormal distribution of ¯v2
R
to be observable among the segments.
Figure 3 shows the distributions of ¯v2
R and of ln ¯v2
R at large R for the data
of some diﬀerent ﬂows of turbulence taken from our past experiments in a wind
tunnel [8]. They are consistent with the lognormal distribution of ¯v2
R. Since the
data points for the diﬀerent ﬂows collapse into single curves, it would be possible
to formulate a theory of large-scale ﬂuctuations that is also applicable to random
ﬁelds other than those of turbulence [4,8].

114
H. Mouri
5
Conclusion and Prospects
The central limit theorem says that a sum of random variables is observed to
become Gaussian with an increase in the number of the variables, if we ignore
too large deviations of the sum from its average [3]. However, also if the variables
satisfy the condition (3), the sum is observed to become lognormal before it is
observed to become Gaussian [4]. The large-scale ﬂuctuations of kinetic energy
of ﬂuid turbulence is just the case [5].
It is known that the lognormal distribution is inﬁnitely divisible [10], i.e.,
written as the distribution of a sum of an arbitrary number of independent and
identically distributed random variables. It is also known that the sum of exactly
lognormal variables is approximately lognormal [11]. These are features of special
sums and are consistent with those found in our study.
The present result contradicts the current paradigm that any lognormal dis-
tribution is due to a multiplicative process, i.e., a product of random variables,
or equivalently to nonlinearity of the system [2]. In fact, some are due to additive
processes in linear systems. It is desirable to explore such an additive process
among the observations of the lognormal distribution [1]. The exploration would
be to study the distribution outside the range of the central limit theorem, i.e.,
the large deviations. Although complexity and hence the nonlinearity appear to
exist in any system that exhibits the lognormality, we would have to reconsider
whether the system is eﬀectively linear or not.
Acknowledgments. This work was supported in part by KAKENHI 25340018.
References
1. Limpert, E., Stahel, W.A., Abbt, M.: Log-Normal Distributions across the Sciences:
Keys and Clues. BioScience 51, 341–352 (2001)
2. Mitzenmacher, M.: A Brief History of Generative Models for Power Law and Log-
normal Distributions. Internet Math. 1, 226–251 (2004)
3. Kendall, M., Stuart, A.: The Advanced Theory of Statistics, 4th edn., vol. 1. Griﬃn,
London (1977)
4. Mouri, H.: Log-Normal Distribution from a Process that is not Multiplicative but
is Additive. Phys. Rev. E 88, 042124 (2013)
5. Mouri, H., Hori, A., Takaoka, M.: Large-Scale Lognormal Fluctuations in Turbu-
lence Velocity Fields. Phys. Fluids 21, 065107 (2009)
6. Abramowitz, M., Stegun, I.A.: Handbook of Mathematical Functions with Formu-
las, Graphs, and Mathematical Tables. Dover, New York (1965)
7. Monin, A.S., Yaglom, A.M.: Statistical Fluid Mechanics: Mechanics of Turbulence,
vol. 2. MIT Press, Cambridge (1975)
8. Mouri, H., Hori, A., Kawashima, Y., Hashimoto, K.: Statistical Mechanics and
Large-Scale Velocity Fluctuations of Turbulence. Phys. Fluids 23, 125110 (2011)
9. Landau, L.D., Lifshitz, E.M.: Fluid Mechanics. Pergamon, London (1959)
10. Thorin, O.: On the Inﬁnite Divisibility of the Lognormal Distribution. Scand. Ac-
tuarial J. 1977, 121–148 (1977)
11. Fenton, L.F.: The Sum of Log-Normal Probability Distributions in Scatter Trans-
mission Systems. IRE Trans. Commun. Syst. 8, 57–67 (1960)

© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
115
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_13 
 
The Prediction of Tropospheric Ozone  
Using a Radial Basis Function Network 
Kříž Radko1 and Šedek Pavel2 
1 University of Pardubice, Faculty of Economics and Administration, Institute of  
Administrative and Social Sciences, Studentská 84, 532 10 Pardubice Czech Republic 
2 Czech Technical University, Faculty of Electrical Engineering, Dept. of Economics,  
Management and Humanities, Technická 2, 166 27 Praha, Czech Republic 
Radko.Kriz@upce.cz, sedekpav@fel.cvut.cz 
Abstract. The goal of this paper is to analyze the tropospheric ozone (O3) 
concentration time series and its prediction using artificial neural networks 
(ANNs). Tropospheric ozone has harmful effects on human health and on the 
environment. This study was based on daily averaged tropospheric ozone (O3) 
data from Pardubice in the Czech Republic. In this study, daily averaged ozone 
concentrations in Pardubice were predicted using a radial basis function 
network (RBFN) with three pollutant parameters and three meteorological 
factors in selected areas. We used a three-layer ANN, which consists of input, 
hidden, and output layers. 
Keywords: Tropospheric ozone, Time series analysis, Artificial neural 
network, Prediction, Radial basis function network. 
1 
Introduction 
Tropospheric ozone (O3) is one of the major air pollutants. O3 is one of the secondary 
pollutants.  Secondary pollutants are not emitted directly to the air, water, or soil. 
Secondary pollutants are synthesized in the environment by chemical reactions 
involving primary or emitted chemicals. The formation of ozone in the troposphere is 
a complex system involving the reactions of hundreds of precursors depending on the 
physical conditions. To predict ozone you must understand not only the ozone itself, 
but also the conditions that contribute to its creation. In addition, ozone 
concentrations are strongly associated with meteorological conditions. To predict the 
concentration of ozone, it is necessary to use a model that contains complex 
relationships between ozone and other variables [1]. Photochemical pollution is 
formed from emissions of nitrogen oxides (NOx, where NOx = NO + NO2), volatile 
organic compounds (VOCs) and carbon monoxide (CO) in the presence of sunlight 
[2]. The most prominent sources of NO2 are internal combustion engines, thermal 
power stations and, to a lesser extent, pulp mills. The burning of biomass also creates 
additional pollutants including NOx [3]. More information analysis of Nitrogen 
Dioxide (NO2) concentration time series is in [4]. 

116 
K. Radko and Š. Pavel 
Ozone has strong oxidant properties. Exposure to O3 induces effects on health and 
the environment, causing respiratory difficulties in sensitive people and possible 
damage to vegetation and ecosystems. There is evidence from controlled human and 
animal exposure studies of the potential for O3 to cause harmful health effects. There 
is a great deal of evidence to show that O3 can harm lung function and irritate the 
respiratory system. Exposure to ozone and the pollutants that produce it is linked to 
premature death, asthma, bronchitis, heart attack, and other cardiopulmonary 
problems. [5] Public announcements, or forecasts, of potential unhealthy O3 air 
quality for the next day can be of great benefit to those at risk of respiratory distress. 
In addition, some cities have voluntary episodic emission control programs, typically 
called “ozone action” programs, which are initiated on days of forecasted high O3. 
Accurate forecasts of O3 are an essential component of such programs, particularly on 
the high O3 days when the health effects are most acute. [6] 
ANNs have become a frequently used tool among various industries during the 
past decades, especially because the computational performance is easily available. 
Their absolutely unique ability to learn from collected data and then to apply the 
acquired knowledge enables us to solve complicated computational tasks, where other 
convenience methods would fail. The modeling of O3 concentration has been studied 
in several works based on ANN [1, 4, 6, 7, 8, 9, 10, 11, 12]. 
2 
Artificial Neural Networks 
There exist several similar definitions of what an artificial neural network (ANN) is. 
For example Gurney [13] states that an ANN is an interconnected assembly of simple 
processing elements, units or nodes, whose functionality is loosely based on the 
animal neuron. The processing ability of the network is stored in the interunit 
connection strengths, or weights, obtained by a process of adaptation to, or learning 
from, a set of training patterns. In the Encyclopedia of Machine Learning [14] an 
ANN term is defined as a computational model based on biological neural networks. 
It consists of an interconnected group of artificial neurons and processes information 
using a connectionist approach to computation. In most cases an ANN is an adaptive 
system that changes its structure based on external or internal information that flows 
through the network during the learning phase. Generally, an ANN is a computational 
model inspired by the biological structure of neuron. 
The ANN consists of neurons, connected by edges. These neurons are organized 
into layers. The first layer processes input data and passes it to the next layer. This 
process is repeated until the last layer in the network is reached and the calculated 
output is returned. Each neuron may recieve input data from more edges, which has 
different weights, sums them and applies an activation function to calculate it’s output 
value, which may be an input to the other layer, or the network output. Thus the 
neuron output value could be described by the following formula: 
 








+
=

=
b
y
w
f
o
N
j
j
j
1
  
(1) 

 
The Prediction of Tropospheric Ozone Using a Radial Basis Function Network 
117 
The structure of ANN is shown on Fig. 1. During the learning process, the network 
modifies the connection weights to achieve a target. Obviously, the performance is 
measured by an error function and the target is to minimize the error. 
Unlike other statistical methods, an ANN makes no prior assumption concerning 
the data distribution. 
 
Fig. 1. Structure of artificial neural network [15] 
2.1 
Radial Basis Function Network 
Generally, a radial basis function network (RBFN) is an ANN that uses radial basis 
functions (RBF) as activation functions. They were first formulated by Broomhead 
and Lowe [16]. The output of the network is a linear combination of radial basis 
functions (RBF) of the inputs and neuron parameters. The RBFN model is an ANN 
model with the multilayer feed-forward network topology. As stated in [17, p. 2], 
despite of the identical model, the node activation function and the training algorithms 
are very different. In the RBFN model, the RBF is used as the activation function 
instead of the sigmoidal functions obviously used in multilayer feed-forward network.  
A RBF is a real-valued function whose value depends only on the distance from 
the origin, or alternatively on the distance from some other point xi, called a center, so 
that 
 
)
(
)
,
(
i
i
x
x
x
x
−
Φ
=
Φ
. 
(2) 
Gaussian kernel is used in this analysis.  

118 
K. Radko and Š. Pavel 
 
2
2
)
,
(
ix
x
c
i
e
x
x
−
−
=
Φ
. 
(3) 
Thus the network output is calculated using formula [13, p. 171]: 
 


=
−
−
=
=
Φ
=
M
i
c
x
ij
M
i
i
ij
j
i
i
e
w
x
w
y
1
2
1
2
2
)
(
ˆ
σ
 
(4) 
where M is number of neurons in hidden layer, ci is the center vector for neuron i and 
wij is the weight of neuron i in the linear output neuron.  
2.2 
Training Algorithm 
Literature states many different approaches of training RBFN as Orthogonal forward 
regression, Hierarchically Self-Organizin Learning, Resource Allocating Network or 
Heuristic Called Forward Selection [17]. Construction and training of the ANN is an 
iterative rather than a deterministic process, requiring a certain amount of expertise. 
In this case, the reduction in the number of input nodes greatly reduced the training 
time and improved the model fit. [6] The MathWorks Matlab software 
implementation of RBFN was used. Brief schema of the network topology is shown 
on Fig. 2.  
 
Fig. 2. Schema of the network topology [18] 
The training algorithm used with this implementation is shown on Fig. 3 and 
described in the software documentation [19] as follows: 
Initially the hidden layer has no neurons. The following steps are repeated until the 
network’s mean squared error falls below goal. 
• The network is simulated. 
• The input vector with the greatest error is found. 
• A new neuron is added to the hidden layer with weights equal to that vector. 

 
The Prediction of Tropospheric Ozone Using a Radial Basis Function Network 
119 
• The weights are redesigned to minimize error. 
The network performance function used for training is mean square error described 
by following formula [20, p. 171]: 
 
(
)
2
1
1
1
2
)
(
ˆ



=
=
=






Φ
−
=
−
=
M
j
M
i
i
ij
j
P
j
j
j
x
w
y
y
y
E
  
(5) 
 
Fig. 3. Training algorithm used with the implementation 
3 
Case Study 
3.1 
Input Data 
This study was based on daily averaged tropospheric ozone (O3) data from 
Pardubice-Rosice from 1.1.2005 to 31.12.2011. Dates, which have missing data, are 
removed. The data were provided by the Czech Hydrometeorological Institute for a 
related diploma thesis that is aimed at nonlinear and chaotic behavior of air pollutants 
time series. Pardubice is the capital of the Pardubice Region and lies on the river Elbe, 
100 km east of Prague. 
3.2 
Description of Model 
In the prediction of the concentrations, input variables of the model are arranged on 
time t and outputs on time t +1. Here, t defines the average daily concentration. The 

120 
K. Radko and Š. Pavel 
neural network model program is trained and tested using MATLAB 2014, a three-
layer neural network that consists of an input, hidden, and output layer. 
Table 1. Model parameters  
Parameter 
Unit 
O3 
μg.m-3 
NO2 
μg.m-3 
NOx 
μg.m-3 
Temperature (in 2 m high) 
K 
Global radiation 
W.m-2 
Wind spead 
m.s-1 
3.3 
Network Training 
As the experimental data for the network, the 3 days of all parameters were used, 
adding one input representing the day of year. That means, the network has 22 inputs. 
The predicted value should be the concentration of O3 one day forever. For the 
training purpose, 90% of the time series were employed as the training data and the 
rest 10% as the testing data. 
During the training process the mean square error steadily decreases (Fig. 4) when 
a neuron is added to the hidden layer. However adding unlimited number of neurons 
causes the network to learn exact patterns instead of the generalization ability. 
Because the training set had approximately 2000 samples, the training process was 
constrained by adding at most 100 neurons to the hidden layer. 
 
Fig. 4. Root mean square error (RMSE) during training process 

 
The Prediction of Tropospheric Ozone Using a Radial Basis Function Network 
121 
3.4 
Testing Results 
The rest 10% of input data were simulated using the trained network and the result was 
compared to the inputs. On the Fig. 5 and Fig. 6 showing prediction it could be seen, that 
the predicted values are merely the same as the target. Root mean square error is 13,7648. 
 
Fig. 5. Prediction (red line) of O3 concentration time series using RBFN  
 
Fig. 6. Prediction (red line) of O3 concentration time series using RBFN  

122 
K. Radko and Š. Pavel 
Performance of the model is determined by comparing the produced model 
predictions and observed results c.f. Fig. 7. 
 
 
Fig. 7. Comparison between the measured (x-axis) and the estimated (y-axis) O3 concentration 
values 
4 
Conclusion 
Obtained results revealed that although the prediction of ozone is a highly complex and 
nonlinear problem, the ANN produces a quite good estimation of the ozone 
concentration level. Considering all these findings, we would recommend the RBF 
network as one of the methods used for prediction. As it may not be reliable under 
certain circumstances, it should be used in combination with other prediction methods. 
References 
[1] Abdul-Wahab, S.A., Al-Alawi, S.M.: Assessment and prediction of tropospheric ozone 
concentration levels using artificial neural networks. Environmental Modelling & 
Software 17(3), 219–228 (2002) 

 
The Prediction of Tropospheric Ozone Using a Radial Basis Function Network 
123 
[2] Tropospheric ozone in the European Union, The consolidated Report, Luxembourg 
(1999) 
[3] Baťa, R., Půlkrábková, P.: The importance of Modelling the Environmental Impacts of a 
Biomass Based Electric Power Generation for public safety. WSEAS Transactions on 
Environment & Development 9(4) (2013) 
[4] Kříž, R.: Chaos in Nitrogen Dioxide Concentration Time Series and Its Prediction. In: 
Nostradamus 2014: Prediction, Modeling and Analysis of Complex Systems, pp. 365–
376. Springer International Publishing (2014) 
[5] Health Aspects of Air Pollution with Particulate Matter, Ozone and Nitrogen Dioxide. 
WHO-Europe report, Bonn (2003) 
[6] Cobourn, W.G., Dolcine, L., French, M., Hubbard, M.C.: A comparison of nonlinear 
regression and neural network models for ground-level ozone forecasting. Journal of the 
Air & Waste Management Association 50(11), 1999–2009 (2000) 
[7] Sousa, S.I.V., Martins, F.G., Alvim-Ferraz, M.C.M., Pereira, M.C.: Multiple linear 
regression and artificial neural networks based on principal components to predict ozone 
concentrations. Environmental Modelling & Software 22(1), 97–103 (2007) 
[8] Bandyopadhyay, G., Chattopadhyay, S.: Single hidden layer artificial neural network 
models versus multiple linear regression model in forecasting the time series of total 
ozone. International Journal of Environmental Science & Technology 4, 141–149 (2007) 
[9] Ozdemir, H., et al.: Prediction of tropospheric ozone concentration by employing 
artificial neural networks. Environmental Engineering Science 25(9), 1249–1254 (2008) 
[10] Al-Alawi, S.M., Abdul-Wahab, S.A., Bakheit, C.S.: Combining principal component 
regression and artificial neural networks for more accurate predictions of ground-level 
ozone. Environmental Modelling & Software 23(4), 396–403 (2008) 
[11] Yi, J., Prybutok, V.R.: A neural network model forecasting for prediction of daily 
maximum ozone concentration in an industrialized urban area. Environmental 
Pollution 92(3), 349–357 (1996) 
[12] Yi, J., Prybutok, R.: A Neural Network Model Forecasting for Prediction of Daily 
Maximum Ozone Concentration in an Industrialized Urban Area. Environ. Pollut. 92, 
349–357 (1996) 
[13] Gurney, K.: An introduction to neural networks. CRC Press (1997) 
[14] Artificial Neural Networks. In: Sammut, C., Webb, G.I. (eds.) Encyclopedia of Machine 
Learning SE - 35, p. 44. Springer, US (2010),  
http://dx.doi.org/10.1007/978-0-387-30164-8_35,  
doi:10.1007/978-0-387-30164-8_35, ISBN: 978-0-387-30768-8 
[15] http://www.cs.bgu.ac.il/~ben-shahar/Teaching/Computational-
Vision/StudentProjects/ICBV061/ICBV-2006-1-TorIvry-
ShaharMichal/index.php 
[16] Broomhead, D.S., Lowe, D.: Radial basis functions, multi-variable functional 
interpolation and adaptive networks. No. RSRE-MEMO-4148. Royal Signals And Radar 
Establishment Malvern, United Kingdom (1988) 
[17] Sundararajan, N., Saratchandran, P., Lu, Y.W.: Radial Basis Function Neural Networks 
with Sequential Learning: MRAN and Its Applications. Progress in neural processing. 
World Scientific (1999) 
[18] http://www.cs.hmc.edu/~bfeinste/rbf/ 
[19] Mathworks Inc.: MATLAB Functions in Neural Network Toolbox 
[20] Mathworks Inc.: Radial Basis Neural Networks – MATLAB & Simulink 

 
© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,  
125
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_14 
 
Physical Experiments and Stochastic Modeling  
to Clarify the System Containing the Seismic Source  
and the Ground 
Alexander V. Smaglichenko1, Maria K. Sayankina2,  
Tatyana A. Smaglichenko2, and Igor A. Volodin2 
1 RTSoft Company “Tools and Automation Systems”, Moscow, Russia 
asmaglichenko@dev.rtsoft.ru 
2 Research Oil and Gas Institute of Russian Academy of Sciences, Moscow, Russia 
{t.a.smaglichenko,msayankina}@gmail.com, volodin@ipng.ru 
Abstract. The nonlinearity of the system containing the source of seismic 
energy and the ground creates a difficulty for application of geophysical 
methods. The initial reason is in a distortion of signals radiated by the source. In 
this article the results of physical experiments using of an impulse source have 
been investigated. It has been demonstrated that in order to get a reliable 
Earth’s images is necessary to process signal series records from repeated 
experiments instead of traditional using of the record of one selected signal. The 
proposed technique is based on an application of the method of microseismic 
sounding, stochastic modeling and calculation of arithmetic mean values. The 
technique can be extended for other sources of seismic energy, where the 
radiated signal can be transformed to series of impulses. 
Keywords: Rayleigh surface waves, recording of seismic signals, inhomogeneous 
structure, stochastic seismic process.  
1 
Introduction 
Exploration of Earth’s structure can be made by using controlled seismic sources. 
There are two popular types of devices that produce vibrations into the ground and 
generate the seismic waves: a seismic vibrator and an impulse hammer. The generated 
waves from a source travel down into a soil until change its characteristics depending 
on properties of earth materials. Seismic records are collected by using receivers 
(seismometers or accelerometers) located at known distances from the source. The 
exploration techniques are applied by processing the registration data to determine 
Earth’s structure, which can contain deposits, karsts cavities, karsts related to water 
resources or other heterogeneities.   
However it often happens that it is very difficult to do the interpretation of the 
exploration method results. The reason can be in the nonlinearity of the system that 
involves the source of seismic energy and the ground. Namely, the record of receiver 
(outgoing signal) does not equal to the linear sum of the record of source (radiated 
signal) and the record that corresponds to the existing heterogeneity. The initial 

126 
A.V. Smaglichenko et al. 
 
reason may be in the distortion of the radiated signal. When using a seismic vibrator 
the nonlinear distortion of outgoing waves has been observed and investigated by 
many authors [1, 2, 3, 4]. However fundamental investigation of signals radiated by 
an impulse hammer is not available. In the work [5] it was pointed that repetition of a 
number of hammer blows at the same location allows us to find an appropriate signal 
and improve the signal-to-noise ratio. In this article it has been illustrated that using of 
the average value of the stochastic parameter that was obtained by processing series 
of seismic signals from the increasing number of hammer blows can provide more 
reliable result of the exploration method than in the case of one selected signal. Thus 
the processing of seismic records of many repeating experiments, their stochastic 
analysis are needed to overcome practical issues connected with the nonlinearity of 
the system containing the impulse source of seismic energy and the ground.  
The exploration method that is applied to seismic records is the method of micro-
seismic sounding [6]. This method uses Rayleigh waves. In order to estimate the 
effectiveness of the method we use the next physical model. The plastic water bottle 
(heterogeneity) is located to soil sand at known depth. Rayleigh waves are created by 
an impulse hammer, which is needed for shallow investigation. In the following 
section, we briefly describe the measuring equipment, which was made to get high 
frequency signals radiated by the hammer. Next, on the base of the method of 
microseismic sounding we propose the stochastic parameter, numerical values of 
which characterize the presence of heterogeneity in a soil. We compare outcomes of 
the stochastic parameter for the two cases: 1) when one signal was selected from a set 
of records; 2) when series of signals from repeating of the number of hammer blows 
are simultaneously processed.                   
2 
The Measuring Equipment 
The review of publications about spectrum analysis (see for example [7]) shows that it 
is difficult to generate and record high-frequency elastic vibrations. However high 
frequency data should be analyzed if we place the physical object at a depth of a half 
of the meter. The deeper the penetration of the Rayleigh surface wave, the lower the 
frequency of this wave.  Therefore it is necessary to measure high frequency waves 
and apply the exploration method to “catch” the process of interaction of these waves 
with heterogeneity. Figure 1 illustrates the measuring equipment that was made by the 
first author of this article [8].  
The measuring equipment consist of four receivers (accelerometers), which have 
universal design to remove the possibility of noise and to connect with Analog-to-
Digital Converter (ADC), debug stabilization board, battery, glass textolite base 
having slots for each accelerometer. The upper boundary of a bandwidth for the 
measurement system is equal to 20000 Hz. The software developed by company “L-
Card” is used when processing the ADC data. It permits to create the unlimited size of 
record, multi-module regime of work, any frequency of interrogation when recording. 
All parts of the equipment are detachable that provides its small-size and versatility in 
land exploration.  
 

 
Physical Exp
 
 
 
 
 
Fig. 1. Measuring eq
Measuring channels hav
produced by musical tuning
Hz (Fig. 2). This original m
by the first author of this 
arithmetic mean value of de
Fig. 2. Musical tunin
3 
Physical Experim
Physical science experime
village Aniskino, Shelkovo
homogeneous and it consi
meters. The physical model
bottle is 0.4 x 0.2 x 0.2 met
 
Fig. 3. Heterogeneity is imita
along the profiles denoted by 
depth of a half of the meter (sh
periments and Stochastic Modeling to Clarify the System 
 
quipment for recording high-frequency elastic vibrations 
ve been checked by using records of elastic vibrati
g fork. The musical fork makes a sound A 1st octave 
method for metrological verification has also been propo
work. Frequency was measured several ten of times. 
eviation of the frequency from 440 Hz  was 1.2 Hz. 
 
ng fork used to test measuring channels of the equipment 
ments 
ents were conducted in sand quarry located near by 
o’s district, Moscow’s province. A geological medium w
sts of sand. The investigated area had a size of 10 x
l was presented by the plastic water bottle. The size of 
ters (Fig.3).  
ated by the plastic water bottle. Measurements were perform
blue lines (shown on the left). The water bottle has placed at
hown on the right). 
127 
ions 
440 
osed 
The 
the 
was 
x 10 
f the 
 
med 
t the 

128 
A.V. Smaglichenko e
 
A small seismic energy s
seismic waves. Two pairs o
hammer strikes. The pair in
other accelerometer, which 
schematic diagram of the 
heterogeneity and the sourc
accelerometers register seis
debug stabilization board co
Fig. 4. Schematic diagram o
conditions (shown on the right
4 
Automatic Proc
The recording of signals req
the information in compact
of the article.  Figure 5 illu
digital records and selects t
files. Each experiment has t
Fig. 5. Selection of dif
et al. 
source that is the impulse hummer was used to generate 
of accelerometers were utilized to record signals radiated
cludes the base accelerometer located at some place and 
moves along a profile with some step. Fig. 4 illustrate
experiment and its performing in real conditions. T
e are shown by blue and black colors on the diagram. F
smic signals, which are going to the ADC and then to 
onnected with a notebook.  
of experiments (shown on the left) and its preparing in f
t) 
essing of Experimental Data 
quires the storing of huge digital data sets. In order to s
t form the software has been developed by the first au
ustrates the program outcome, which detects “the breaks
the seismic signal record. All records are kept in separa
their “tree of files”.         
 
fferent seismic signals as the result of software application 
the 
d by 
 the 
es a 
The 
Four 
the 
 
field 
store 
thor 
s” in 
ated 

 
Physical Experiments and Stochastic Modeling to Clarify the System 
129 
 
Note that there are other works that also simplify the signal records (see for 
example [9]). Nevertheless in practice the data corresponding to seismic signals are 
often manually chosen. 
5 
A Proposed Stochastic Analysis  
In the work [10] authors Lim and Harris introduced a new spectrum presentation for 
stochastic signals. They called it “the ratio spectrum”. The ratio spectrum R(m)  is 
determined as the ratio of the output power that was filtered for some frequencies to 
the input power having all frequencies range. In the work [11] authors Skowronski 
and Harris determined the analytical expression of this parameter for a white noise 
model. It has been shown that the ratio spectrum can be used “to extract features 
from regions of high energy”. It is known that a white noise describes the stochastic 
process having uniform probability distribution. Models of a white noise correspond 
to an appearance of distortions of the uniform stochastic process.  
If we study the stochastic seismic process we can consider the appearance of 
inhomogeneous structure having contrast properties for some medium as the 
appearance of a real distortion for this medium. In our assumption, the interaction of 
the Rayleigh waves with the heterogeneity can be detected by studying the ratio 
spectrum of seismic signals. By analogy with the ratio spectrum, which was defined 
in [10, 11], we propose to introduce a parameter that is formed by taking the ratio of 
the power of a signal from the moving accelerometer to the power of a signal from the 
base accelerometer. Thus in the case of the microseismic sounding method the 
parameter can be described in the following form: 
 
2
1
0
)
(
1
1
0
2
)
(
2
)
(

−
=

−
=
=
=
N
k
k
f
A
N
k
k
f
A
ter
accelerome
base
the
from
power
the
ter
accelerome
moving
the
from
power
the
j
f
r
  
(1) 
where j is the registration point of the pair “the moving accelerometer – the base 
accelerometer”,  
1
f
A и  
2
f
A  are values in the domain of Rayleigh wave frequencies 
f  that were found due to the Fourier transformation of signal records registered by 
using the pair of accelerometers in the j-th point, N is the length of the discrete time 
series.   
In accordance with the base of the method of microseismic sounding we will study 
the variations of the power spectrum in each registration points from the arithmetical  
 
 
 
 

130 
A.V. Smaglichenko et al. 
 
mean value of the power spectrum for all registration points. For each frequency f the 
arithmetical mean value 
)
( f
r
is equal to:   
 

=
=
K
j
f
jr
f
r
1
)
(
)
(
 ,  
(2) 
where j is the registration point number. Then the parameter that characterizes a 
presence of the heterogeneity having contrast property is determined as following:   
 
)
(
)
(
)
(
f
r
j
f
r
j
f
S
−
=
  
(3) 
We will call the parameter 
j
f
S
)
(
 by the stochastic parameter.  
6 
A Comparison of Results for One Selected Signal and for 
Series of Repeating Signals  
Let us analyze outcomes of the method based on using of the stochastic parameter. The 
first image of heterogeneity has been obtained processing the record of one seismic 
signal.  This signal was selected from a set of signals that were obtained due to repeating 
of a number of hammer blows at the same location (Fig.6-a). The next image was 
restored using data of 10 seismic signals (Fig 6-b). One can see that the shape of the 
original heterogeneity that is denoted by a rectangle is better manifested in the given 
case. Numerical values of the stochastic parameter in the domain of a rectangle are in the 
range from 0.3 to 1.0. These values are maximal absolute values among values of the 
parameter that were determined for all points of the investigated area. At the same time it 
should be noted the presence of values in the range from -0.8 to -0.3, absolute values of 
which close to the maximum values (Table 1). These values can point a false position of 
the heterogeneity if researcher does not know where it is.   
Table 1. Values of the stochastic parameter. Data of 10 seismic signals.   
 

 
Physical Experiments and Stochastic Modeling to Clarify the System 
131 
 
In order to understand how the number of outgoing seismic signals and a distance 
between accelerometers affects on reconstruction image quality we conducted 
additional experiments. The distance between accelerometers has been reduced from 
20 cm to 12.5 cm. 20 seismic signals were generated by the impulse hammer. Thus 
the amount of signals exceeds the previous amount in 2 times. Fig. 6-c demonstrates 
the improvement of the heterogeneity shape quality. Numerical values of the 
stochastic parameter in the domain of a rectangle are in the range from 0.7 to 1.0 
(Table 2). 
Table 2. Values of the stochastic parameter. Data of 20 seismic signals.  
 
 
However the problem of fuzzy boundaries remains. In order to overcome this 
problem we applied the data emulation. Namely, data obtained from a real experiment 
were duplicated from the other side of the profile. The final result was reliable (Fig. 6-
d). Values of the stochastic parameter are equal to 1.0 in the heterogeneity domain. 
There are no artifacts.     
 

132 
A.V. Smaglichenko e
 
Fig. 6. Images of interaction 
following conditions: a) dat
accelerometers is 20 cm; b) d
data of 20 signals have been p
used due to the data emulation
7 
Conclusion  
In this article we briefly d
frequency signals radiated
Earth’s heterogeneity, phy
has been sounded by seism
illustrated.  
On the base of microsei
seismic process as a stocha
that was developed in work
been obtained using the s
reconstructed when proces
partly reconstructed when 
exists. If we use records of
the shape of heterogeneity 
image well reconstructed if
profile.        
et al. 
of the Rayleigh surface waves with the heterogeneity under
a of one signal have been processed; the distance betw
data of 10 signals have been processed; the distance is 20 cm
processed; the distance is 12.5 cm; d) data of 40 signals have b
n; the distance is 12.5 cm.     
described the measuring equipment for recording of h
d by the impulse hammer, the physical model imitat
sical experiments, in which the inhomogeneous medi
mic impulses.  Processing of experimental data has b
ismic method we proposed the technique, which prese
astic process. This permits us to apply the novel parame
ks [10, 11] for a white noise model.  The next results h
stochastic technique. The shape of heterogeneity is 
ssing one selected signal. The image of heterogeneity
using of series of 10 signals. However significant artif
f 20 signals from repeating number of hammer strikes t
can be restored having a small artifact. The heterogene
f records of signals are added from an opposite side of 
 
r the 
ween 
m; c) 
been 
high 
ting 
ium 
been 
ents 
eter 
have 
not 
y is 
fact 
then 
eity 
the 

 
Physical Experiments and Stochastic Modeling to Clarify the System 
133 
 
Thus we conclude that the processing of numerous records of seismic signals 
radiated by the impulse source from different sides of the seismic profile leads to 
reliable results of the exploration method. This reduces the influence of the nonlinear 
system containing the source of seismic energy and the ground on the exploration 
method application.       
We assume that the technique can be used for other seismic sources, for instance 
for the vibrator, if its extended signal will be converted to separated impulses.     
 
Acknowledgments. We are very grateful to Marina Stepanova (Institute of Physics of 
the Earth, RAS) for her help in using of the microseismic sounding method. We 
sincerely thank to an anonymous reviewer and an editor for their comments that 
helped to clarify the basic purpose of the manuscript.        
References 
1. Martin, J.E., Jack, I.G.: The behavior of a seismic vibrator using different phase control 
methods and drive level. First Break 8, 404–414 (1990) 
2. Walker, D.: Harmonic resonance structure and chaotic dynamics in the earth-vibrator 
system. Geophysical Prospecting 43, 487–507 (1995) 
3. Jeffryes, B.P.: Far-field harmonic measurement for seismic vibrators. In: 66th Annual 
International Meeting, SEG, Expanded Abstracts, pp. 60–63 (1996) 
4. Lebedev, A., Beresnev, I.: Nonlinear distortion of signals radiated by vibroseis sources. 
Geophysics 69(4), 968–977 (2004) 
5. Wightman, W., Jalinoos, F., Sirles, P., Hanna, K.: Application of geophysical methods to 
high way related problems. Technical report, DTFH 68-02-P-00083 (2003) 
6. Gorbatikov, A.V., Montesinos, F.G., Arnoso, J., Stepanova, M.Y., Benavent, M., 
Tsukanov, A.A.: New Features in the Subsurface Structure Model of El Hierro Island 
(Canaries) from Low-Frequency Microseismic Sounding: An Insight into the 2011 
Seismo-Volcanic Crisis. Surveys in Geophysics 34(3), 463–489 (2013) 
7. Milsom, J.A., Eriksen, A.: Field Geophysics. Geological Field Guide. Book 28, 304 p. 
Willey (2011) 
8. Smaglichenko, A.V.: The complex of measuring equipment and software to estimate the 
location of inhomogeneity using a seismic data. Seismic Instruments 50(2), 20–38 (2014) 
(in Russian) 
9. Abancó, C., Hürlimann, M., Fritschi, B., Graf, C., Moya, J.: Transformation of Ground 
Vibration Signal for Debris-Flow Monitoring and Detection in Alarm Systems. Sensors 
(Basel) 12(4), 4870–4891 (2012) 
10. Lim, S., Harris, J.G.: Analog implementation of ratio spectrum. In: Proc. of the IEEE 
International Symposium on Circuits and Systems, Monterey, pp. 277–280 (1998) 
11. Skowronski, M.D., Harris, J.A.: Probabilistic Analysis of the Ratio Spectrum. In: Proc. of 
the IEEE Adaptive Systems for Signal Processing, Communications, and Control 
Symposium, Lake Louise, Alta, pp. 333–336 (2000) 

 
© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
135
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_15 
 
Cosmology 2.0: Convergent Implication  
of Cryodynamics and Global-c General Relativity 
Otto E. Rossler 
Faculty of Science, University of Tübingen, Auf der Morgenstelle 8  
D-72076 Tübingen, Germany 
oeross00@yahoo.com 
Abstract. Two building blocks valid in two unrelated physical disciplines, 
statistical mechanics and general relativity, respectively, have convergent 
implications for cosmology. Firstly, cryodynamics – the recent new sister 
discipline to thermodynamics which applies to gases that are made up from 
mutually attractive rather than repulsive particles – is anti-entropic. Its 
statistical equilibrium is unstable rather than stable. This fact confirms Fritz 
Zwicky’s 1929 intuitive explanation of cosmological redshift as generated by 
the randomly distributed moving galaxies. Secondly, global-c general relativity 
– the unfinished global-c version of general relativity – implies that 
gravitational redshift is no longer accompanied by a proportional reduction in 
the speed of light c. Rather, the constant recession speed of the bottom of the 
Einstein rocketship relative to the tip implies a proportional local size increase 
that is optically masked from above. The new global c excludes cosmic 
expansion directly, since the global space expansion of the Big Bang involves 
superluminal speeds by definition. Thus the cosmological standard model is 
refuted regarding its main assumption of space expansion in two independent 
ways. The second pillar of the cosmological standard model besides expansion, 
the “cosmological background radiation,” now necessarily comes from a close-
by source, the Milky Way galaxy’s halo. A Clifford-Einstein-Mandelbrot 
cosmos comes in sight along with an eternal cosmological metabolism 
(Heraclitus). Thus the non-experimental (observational) physical discipline – 
cosmology – presents itself in a new light.  
Keywords: Cosmological Standard model, superluminality, tired light, 
deterministic 
statistical 
mechanics, 
cryodynamics, 
global-c, 
Clifford, 
Mandelbrot, Heraclitus. 
1 
Introduction 
Statistical mechanics and general relativity currently undergo an update: In Stat 
Mech, on one hand, the existence of a second fundamental paradigm based on 
mutually attractive rather than repulsive particles was discovered: cryodynamics, 
sister discipline to thermodynamics [1] (crýos means cold, thermós means hot in 
Greek). In general relativity, on the other hand, the existence of a new global-c 

136 
O.E. Rossler 
 
transform of the Einstein equation follows from the discovery of a global-c version of 
the Schwarzschild metric [2,3] and the equivalence principle [4]. The two 
independent results converge in ruling out the cosmological standard model founded 
on the assumption of cosmic expansion. “Cosmology 2.0” is an active construction 
site. 
2 
Cryodynamics 
Thermodynamics is applicable at all temperatures despite its hot name as is well-
known. So is cryodynamics in spite of its cold name. The difference lies only in the 
numerical sign of the inter-particle potential. While thermodynamics is based on 
(short-range) repulsion between the gas particles, cryodynamics is based on (long-
range) attraction. Since thermodynamics still works when the repulsive inter-particle 
potential is made long-range (anti-Newtonian) [5], cryodynamics with its long-range 
attractive Newtonian (or post-Newtonian) potentials is a genuine dual to 
thermodynamics, arrived at after a 150 years long delay. 
Cryodynamics, like thermodynamics, admits an entropy-like macroscopic 
observable which differs from the latter’s Sackur-Tetrode  deterministic entropy 
formula (see [6]) only by a sign flip. This new macro observable can be called 
“ectropy.” While entropy is spontaneously increasing or constant in deterministic 
statistical thermodynamics, ectropy is spontaneously increasing or constant in 
deterministic statistical cryodynamics. Time’s arrow is inverted. Apart from the sky, 
cryodynamics possesses also an applied domain down on earth: Hotter electrons can 
be used in principle to locally cool-down a run-away plasma instability in a hot-fusion 
reactor [7]. 
The main implication of cryodynamics, however, concerns cosmology. In 
retrospect it is not surprising that when cryodynamics was first intuitively envisioned 
by Zwicky in 1929 [8] it was met with ridicule as “tired light theory.” This response 
from the part of the community prevented Zwicky from making his intuition formally 
consistent. The at first similarly naïve re-discovery of the idea of an intergalactic 
cooling of photons 74 years later [9] was initially met with silence.  
At this point, the reader no doubt asks for numerical evidence. After all, a huge 
number of numerical simulations of multi-particle celestial mechanical systems have 
been performed during the last two thirds of a century. If Zwicky’s brainchild were 
for real, the phenomenon ought to be well-known empirically. No “anti-entropic” 
effects were observed. Only in 2010 quite recently, positive numerical evidence was 
found in a high-accuracy symplectic two-particle simulation in a formally one-
dimensional T-tube configuration [10]. An independent analytical confirmation also 
exists [11]. The evidence so far points to the interpretation that, unlike statistical 
thermodynamics, statistical cryodynamics is numerically unstable. 
Recently a mathematical proof of cryodynamics (smoothed inverted Sinai 
paradigm, cf. Sinai [12]) was offered: the “breathing plane-tree alley paradigm” [13]. 
A first textbook on cryodynamics in the footsteps of Richard Becker’s classic 
introductory treatise on deterministic thermodynamics [14] waits to be written.  

 
Convergent Implication of Cryodynamics and Global-c General Relativity 
137 
 
3 
Global-c General Relativity 
The very existence of general relativity is a miracle of scientific tenacity; the time gap 
of four years between the discovery of the equivalence principle by Einstein in 1907 
[15] (marred by the loss of c-global) and the first follow-up paper on the way towards 
general relativity [16] could as well have lasted indefinitely. The finished theory five 
more years later had to be built carefully around the obstacle that c was assumed 
reduced in proportion to the local gravitational potential – the very drawback which 
had crept-in in 1907 (a fact which is not widely known). 
The full Einstein equation of 1916 [17] still waits to be written down in its global-c 
transform.  The global-c transform of the Schwarzschild metric was the first evidence 
found in 2007 as mentioned [2, 3]. Eventually, the Noether theorem (see [18]) and 
quantum electrodynamics (see [19]) confirmed the generality of the finding. The plain 
special-relativistic fact that a constantly receding light clock mimics the presence of a 
reduced speed of light c inside when watched from behind, offers the best explanation 
for the analogous shrinking of the speed of light c seen by Einstein at the bottom of 
his extended long rocketship in outer space in 1907 [15]. However, without the aid of 
quantum electrodynamics [19], the error may have remained as unrecognizable as it 
was in 1907.    
The new convergence between quantum mechanics [19] and general relativity, 
valid in the wake of c-global, is most encouraging for fundamental particle physics. 
Big theoretical progress can be expected. At the present moment in time, black hole 
theory is particularly affected by c-global. Black holes cease to “Hawking evaporate 
[20]”, see [3], and cease to be finished in finite outer time in accordance with 
Oppenheimer and Snyder’s original prediction of 1939 [21].  c-global makes the 
distance to the horizon infinite, not only in time as is well known [21], but also in 
space (while the short proper times of in-falling observers lose their erroneously 
assumed equivalent status). The altered features of black holes due to c-global make 
any attempt at producing them down on earth counter advisable [3]. 
c-global has many further implications, see [4]. The main point in our context is 
that it pays to return to the young Einstein. Near the beginnings of a new gold vein in 
science, one always finds further gold when digging there, Maxwell advised.  In this 
way a seemingly “undergraduate” type of approach (like the present one) can prove 
superior to formally advanced later work, no matter how paradoxical this looks at first 
sight. 
4 
Convergent Implications for Cosmology 
Each 
of 
the 
two 
mathematically 
unrelated 
approaches 
discussed 
above 
(cryodynamics; c-global) implies absence of cosmological expansion. The Big Bang 
has become a redundant hypothesis in light of cryodynamics just as Zwicky had 
conjectured; and the Big Bang has become an inadmissible hypothesis in light of the 
repaired constant-c element of general relativity (Schwarzschild metric [3], 
equivalence principle [4]). 

138 
O.E. Rossler 
 
The lack of expansion is the main new feature of Cosmology 2.0. It allows the 
prediction that an unbounded “Clifford-Einstein cosmos” can be formulated 
eventually. Many new and novel questions will need to be answered along the way. 
Since black holes are no longer finished in finite outer time: can it be that the bigger 
partner in a merger of two black holes “recycles” the likewise unfinished smaller one? 
This is the hypothesis of a “blue-sky catastrophe” in the sense of differential topology 
(see Ralph Abraham [22]) leading to black hole recycling [23]. Alternatively, the 
quasar ejection mechanism [24] will need to be shown to be more efficient than is 
currently known. 
In the absence of some efficient recycling mechanism like the one just mentioned, 
the cosmos would have long ceased to be an active ballpark. Thus a major new open 
problem – the nature of the recycling mechanism – deserves urgent attention. 
The second major point to be answered is the origin of the so-called cosmic 
background radiation (CBR) in a non-expanding cosmos. It cannot come from very 
far away any more. The radiation of the CBR was long before its empirical discovery 
by Wilson and Penzias [25] predicted by Guillaume in 1896 [26], and later Nernst 
[27] and Born [28], as Assis and Neves [29,30] showed. It reflects the presence of a 
virtually constant-temperature shell around our galaxy – the galactic halo. The 
“anomalies” recently discovered in the Planck mission [31] can now be faced 
squarely. 
The amazing linearity of the Hubble line over orders of magnitude also remains to 
be explained. The famous “wiggle” at its end [32] follows predictably from the huge 
voids in the cosmos whose size becomes bigger with distance statistically in a fractal 
Fournier-Mandelbrot [33] sponge. 
5 
Discussion 
Two recent classical-in-spirit physical discoveries, cryodynamics and global-c general 
relativity (the latter still unfinished), were reported on with an eye to their 
cosmological implications. A whole “bouleversement” of 20th century cosmology 
followed. Such “swerves” are rare in science. 
In a sense, cryodynamics and c-global each rest on a naïve question: (a) How about 
inverting the sign of the interaction-generating forces in classical statistical 
mechanics?; and (b) How about reconciling constant-c special relativity with the 
apparent non-constant-c implication so reluctantly accepted in the equivalence 
principle and hence in gravitation by Einstein in 1907 [15]? The two questions prove 
to be far-reaching. Both moreover lead to convergent implications regarding the 
scientific picture of the cosmos. A potentially infinite and eternal fractal Clifford-
Einstein-Fournier-Mandelbrot cosmos appears to take shape in their wake [23]. 
This conclusion is very baffling since it is a long time that a consensus held by 
virtually the whole scientific community over many decades, had to be given up. The 
phlogiston theory is the last example that comes to mind. An excuse why such could 
occur in our own time with the Big Bang theory will be needed. Here chaos theory 
comes to mind. Its overwhelming, transfinitely exact features go back to Poincaré – 
the same Poincaré who first wrote down the Lorentz transformation and who had 

 
Convergent Implication of Cryodynamics and Global-c General Relativity 
139 
 
named and in parallel to Einstein invented the theory of relativity. His deterministic 
spirit got underestimated in the following decades, cf. [34]. If Poincaré had not died 
prematurely in 1912, the development sketched above would most likely have taken 
place a century earlier. It goes without saying that all of the above is mathematically 
trivial compared to QED and modern field theories.  
To conclude, a synopsis of two recent developments in classical fundamental 
physics was offered. It comes as a surprise that thereby a return to an earlier stage of 
physics became necessary. On the one hand, the deterministic Boltzmann-Sinai model 
of classical statistical mechanics admits a generalization towards attractive Newtonian 
(and post-Newtonian) potentials. On the other, a return to the special-relativistic roots 
of general relativity enables a revival of the global constancy of the speed of light c in 
the vacuum. Both recent findings independently enforce the same radically new 
picture of the cosmos. The new picture is maximally old at the same time in light of 
Heraclitus’ now no longer cryptic phrase metabállon anapaúetai – “metabolizing it 
rests.” 
Acknowledgements. I thank the organizer for the kind invitation and advice. For 
J.O.R. 
References 
1. Rössler, O.E.: The new science of cryodynamics and its connection to cosmology. 
Complex Systems 20, 105–113 (2011); Open access 
2. Rössler, O.E.: Abraham-solution to Schwarzschild metric implies that CERN miniblack 
holes pose a planetary risk. In: Plath, P.J., Hass, E.C. (eds.) Vernetzte Wissenschaften: 
Crosslinks in Natural and Social Sciences, pp. 263–270. Logos Verlag, Berlin (2008), 
http://www.wissensnavigator.ch/documents/OTTOROESSLERMINIBLA
CKHOLE.pdf 
3. Rossler, O.E.: Abraham-like return to constant c in general relativity: “ℜ-theorem” 
demonstrated in Schwarzschild metric. Fractal Spacetime and Noncommutative Geometry 
in Quantum and High Energy Physics 2, 1–14 (2012), Preprint 2008:  
http://www.wissensnavigator.com/documents/chaos.pdf 
4. Rossler, O.E.: Einstein’s equivalence principle has three further implications besides 
affecting time: T-L-M-Ch theorem (“Telemach”). African Journal of Mathematics and 
Computer Science Research 5, 44–47 (2012); Open access 
5. Diebner, H.H.: Time-dependent Deterministic Entropies and Dissipative Structures in 
exactly reversible Newtonian Molecular-dynamical Universes. Doctoral thesis. Verlag 
Ulrich Grauer, Stuttgart (1999) (in German) 
6. Diebner, H.H., Rössler, O.E.: A deterministic entropy based on the instantaneous phase 
space volume. Z. Naturforsch. 53, 51–60 (1998); Open access 
7. Rössler, O.E., Sanayei, A., Zelinka, I.: Is hot fusion made feasible by the discovery of 
cryodynamics? In: Zelinka, I., Snasel, V., Rössler, O.E., Abraham, A., Corchado, E.S. 
(eds.) Nostradamus: Mod. Meth. of Prediction, Modeling. AISC, vol. 192, pp. 1–4. 
Springer, Heidelberg (2013),  
http://link.springer.com/chapter/10.1007/978-3-642-3 
...ccess=true 
8. Zwicky, F.: On the red shift of spectral lines through interstellar space. Proc. Nat. Acad. 
Sci. 15, 773–779 (1929) 

140 
O.E. Rossler 
 
9. Rössler, O.E., Fröhlich, D., Kleiner, N.: A time-symmetric Hubble-like law: light rays 
grazing 
randomly 
moving 
galaxies 
show 
distance 
proportional 
redshift. 
Z. 
Naturforsch. 58a, 807–809 (2003) 
10. Sonnleitner, K.: StV4: A symplectic 4th order Störmer-Verlet Algorithm for Hamiltonian 
multi-particle Systems with two applied Examples (Gas, T-tube Configuration). PhD 
dissertation, University of Tubingen (2010),  
http://www.wissensnavigator.com/documents/StV4-
universell.pdf (in German) 
11. Movassagh, R.: A time-asymmetric process in central force scatterings. arxiv.org 2010 
(2013) 
12. Sinai, Y.G.: Dynamical systems with elastic reflections. Russ. Math. Surv. 25, 137–189 
(1970) 
13. Rossler, O.E.: Rolling ball in breathing plane-tree alley paradigm. European Scientific 
Journal 9(27), 1–7 (2013); Open access 
14. Becker, R.: Theory of Heat. Springer, New York (1967) 
15. Einstein, A.: On the relativity principle and the conclusions drawn from it. Jahrbuch der 
Radioaktivität 4, 411–462, 458 (1907), English translation:  
http://www.pitt.edu/~jdnorton/teaching/GR&Grav_2007/pdf/ 
Einstein_1907.pdf (second-but-last page) 
16. Einstein, A.: On the influence of gravity on the dispersion of light (Über den Einfluss der 
Schwerkraft auf die Ausbreitung des Lichtes). Annalen der Physik 35, 898–908 (1911) 
17. Einstein, A.: The foundation of the general theory of relativity (Die Grundlage der 
allgemeinen Relativitätstheorie). Annalen der Physik 49, 769–822 (1916) 
18. Rossler, O.E.: Olemach theorem: angular-momentum conservation implies gravitational-
redshift proportional change of length, mass and charge. European Scientific Journal 9(6), 
38–45 (2013); Open access 
19. Rossler, O.E.: “Schwinger theorem”: ascending photons in equivalence principle imply 
globally constant c in general relativity. European Scientific Journal 10(3), 26–30 (2014); 
Open access 
20. Hawking, S.W.: Black hole explosions. Nature 248, 30–31 (1974) 
21. Oppenheimer, J.R., Snyder, H.: On continued gravitational attraction. Phys. Rev. 56, 455–
459 (1939) 
22. Abraham, R.: Chaostrophes, intermittency, and noise. In: Fischer, P., Smith, W.R. (eds.) 
Chaos, Fractals and Dynamics, pp. 3–22. Marcel Decker, New York (1985) 
23. Rossler, O.E.: Complexity decomplexified: a list of 200+ new results encountered over 55 
years. In: Zelinka, I., Sanayei, A., Zenil, H., Rössler, O.E. (eds.) How Nature Works, pp. 
1–18. Springer, Heidelberg (2014), see # CXCI, CLXXVII, CLXXXVI 
24. Blandford, R.D.: Active Galactic Nuclei. Springer, Heidelberg (2006) 
25. Penzias, A.A., Wilson, R.W.: A measurement of excess antenna temperature at 4080 Mc/s. 
Astrophysical Journal Letters 142, 419–421 (1965) 
26. Guillaume, C.E.: La température de l’espace (The temperature of space). La Nature 24(2), 
210–234 (1896); Quote: We conclude that the radiation of the stars alone would maintain 
the test particle, that we suppose might have been placed at different points in the sky, at a 
temperature of 338/60 = 5.6 abs. 
27. Nernst, W.: Further examination of the assumption of a stationary state of the universe. 
Zeits. Phys. 106, 633–661 (1937); 2.8 K predicted (in German) 
28. Born, M.: On the interpretation of Freundlich’s red-shift formula. Proc. Phys. Soc. A 67, 
193–194 (1954) 

 
Convergent Implication of Cryodynamics and Global-c General Relativity 
141 
 
29. Assis, A.K.T., Neves, M.C.D.: The redshift revisited. Astrophysics and Space Science 227, 
13–24 (1995) 
30. Assis, A.K.T., Neves, M.C.D.: History of the 2.7 K temperature prior to Penzias and 
Wilson. Apeiron 2, 79–84 (1995) 
31. ESA, Planck reveals an almost perfect universe (select page 2 there, which contains the 
decisive figure titled, Asymmetry and cold spot) (2013),  
http://www.esa.int/Our_Activities/Space_Science/Planck/ 
Planck_reveals_an_almost_perfect_Universe 
32. Interview with Saul Perlmutter, Brian P. Schmidt and Adam G. Riess (37 minutes), 
http://www.nobelprize.org/mediaplayer/index.php?id=1745 
33. Mandelbrot, B.B.: Fractal Geometry of Nature. Freeman, San Francisco (1977) 
34. Sanayei, A., Rössler, O.E.: Chaotic Harmony – A Dialog about Physics, Complexity and 
Life. Springer, Heidelberg (2014),  
http://link.springer.com/book/10.1007/978-3-319-06781-0/ 
page/1 

Hidden Quantum Markov Models
and Open Quantum Systems
with Instantaneous Feedback
Lewis A. Clark1, Wei Huang2, Thomas M. Barlow1, and Almut Beige1
1 The School of Physics and Astronomy, University of Leeds,
Leeds, LS2 9JT, United Kingdom
2 20 Dover Drive Singapore, Singapore University of Technology & Design,
Singapore, 138682, Singapore
Abstract. Hidden Markov Models are widely used in classical computer
science to model stochastic processes with a wide range of applications.
This paper concerns the quantum analogues of these machines — so-
called Hidden Quantum Markov Models (HQMMs). Using the proper-
ties of Quantum Physics, HQMMs are able to generate more complex
random output sequences than their classical counterparts, even when
using the same number of internal states. They are therefore expected to
ﬁnd applications as quantum simulators of stochastic processes. Here, we
emphasise that open quantum systems with instantaneous feedback are
examples of HQMMs, thereby identifying a novel application of quantum
feedback control.
Keywords: Stochastic Processes, Hidden Markov Models, Quantum
Simulations, Quantum Feedback.
1
Introduction
In classical computer science, a Markov chain is a memoryless stochastic ma-
chine, which progresses from one state to another on a discrete time scale.
Since their introduction in 1906 by Andrey Markov, the properties of Markov
chains have been studied in great detail by mathematicians, computer scientists,
and physicists alike [1]. In the meantime, more complex versions of stochastic
machines, like Hidden Markov Models (HMMs), have been introduced. These
progress randomly from one internal state to another, which remains unob-
served (hidden), while producing a stochastic output sequence. HMMs are widely
used for the simulation of stochastic processes [2–4]. Applications include speech
recognition, image analysis and the modelling of biological systems.
Over recent years, several attempts have been made to extend the deﬁnition of
HMMs into the quantum world and to utilise the properties of quantum systems
to generate more complex stochastic output sequences [5–9]. For example, in
2011, Monras et al. [6] introduced so-called Hidden Quantum Markov Models
(HQMMs). These are machines that progress from one quantum state to another,
c
⃝Springer International Publishing Switzerland 2015
143
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_16

144
L.A. Clark et al.
while generating classical output symbols. To produce an output symbol, a so-
called generalised measurement or Kraus operation [10] is performed on the
internal state of the machine. One way of implementing a Kraus operation is to
use an auxiliary quantum system — a so-called ancilla. In every time step, the
internal state of the HQMM interacts with its ancilla, which is then read out by
a projective measurement. After every measurement, the ancilla is reset into its
initial state, while the internal state of the HQMM remains hidden.
A Kraus operation is the most general operation that a quantum system can
experience, which is why Kraus operations are a vital part of the deﬁnition of
a HQMM given by Monras et al. [6]. In a previous attempt to introduce quan-
tum analogues of HMMs, Wiesner and Crutchﬁeld [5] deﬁned so-called quantum
ﬁnite-state generators, which only involved unitary operations and projective
measurements. This is the most basic way in which a quantum system may be
evolved and measured. The state evolves according to the given unitary opera-
tor and is then measured, collapsing the state onto the measurement outcome.
In this way, they only obtained a subset of HQMMs, which are less powerful
than their classical analogues. Diﬀerent from quantum ﬁnite-state generators,
HQMMs are able to produce more complex output sequences than HMMs with
the same number of internal states.
Several ways of implementing HQMMs have already been identiﬁed:
1. As pointed out in Ref. [6], one way of implementing HQMMs is the successive,
non-adaptive read-out of entangled many-body states.
2. Another example of a HQMM is the time evolution of an open quantum
system on a coarse grained time scale, Δt, which produces a random se-
quence of classical output symbols. Indeed, in Ref. [9], Sweke, Sinayskiy, and
Petruccione use the language of HMMs to model open quantum systems.
3. The purpose of this paper is to highlight the connection between HQMMs
and open quantum systems with instantaneous feedback [11]. In this way, we
identify a way of implementing an even wider set of HQMMs.
Like HQMMs, open quantum systems evolve randomly in time. Taking this
perspective, the open quantum system itself provides the internal states of a
HQMM, while its surrounding bath plays the role of the ancilla, which is con-
stantly reset into an environmentally preferred, or einselected, state [12]. By this,
we mean the state that the environment would naturally evolve into if left alone.
The continuous interaction between the internal states and the bath moves the
bath away from its einselected state, thereby usually producing a measurable
response that manifests itself as a random classical symbol. The eﬀective dy-
namics of such a machine, when averaged over all possible trajectories, can be
described by a Markovian master equation [13–15]. When describing an open
quantum system in this way, its accompanying output sequence is ignored. Here
we suggest not to do so and to use the output sequences of open quantum sys-
tems to simulate stochastic processes. Like HMMs, we expect HQMMs to ﬁnd a
wide range of applications [6, 16, 17].
Quantum feedback is a process in which the classical output symbols pro-
duced by an open quantum system are used to change its internal dynamics.

Hidden Quantum Markov Models and Open Quantum Systems
145
Applications of quantum feedback control can be found, for example, in Quan-
tum Information Processing [11], where it is especially used to control state
preparation [18] and quantum transport [19]. In these applications, the feedback
is used to guide the internal dynamics of a quantum system. In contrast to this,
this paper proposes to use quantum feedback to manipulate the classical output
sequences of open quantum systems.
There are ﬁve sections in this paper. Sect. 2 shortly reviews HQMMs. After-
wards, in Sect. 3, we review the master equations of Markovian open quantum
systems with and without instantaneous feedback. Sect. 4 shows that open quan-
tum systems with instantaneous feedback are examples of HQMMs. Finally, we
summarise our ﬁndings in Sect. 5.
2
Hidden Quantum Markov Models
Hidden Markov Models (HMMs) are machines that evolve randomly from one
internal state to another. In every time step, an output symbol is produced. Only
the output symbol is detected externally, while the internal state of the machine
remains hidden. Consequently, the time evolution of a HMM is characterised
through a set of transition matrices Tm, where m denotes the output symbol
generated during the respective time step. For example, if the initial probability
distribution of the internal states of the HMM is given by a vector p0, then
the probability to obtain the outputs abc . . . def, where a is the ﬁrst symbol
produced and f is the last, is given by (see eg. Ref. [6])
p(abc . . . def) = η TfTeTd . . . TcTbTa p0 .
(1)
Here, η is a vector with all of its coeﬃcients equal to 1.
Analogously, a Hidden Quantum Markov Model (HQMM) with a certain prob-
ability distribution of its internal state populations can be described by a density
matrix, ρS. In every time step, the system evolves and produces an output sym-
bol. Again, only the output symbol is detected externally, while the internal
state of the machine remains hidden. In contrast to HMMs, the time evolution
of a HQMM is governed by a set of Kraus operators Km, where the subscripts m
coincide again with the output symbols of the machine. Using the same example
as above, the probability of the output abc . . . def occurring is now given by
p(abc . . . def) = Tr

KfKeKd . . . KcKbKa ρS K†
aK†
bK†
c . . . K†
dK†
eK†
f

.
(2)
If the output symbol is ignored, then the density matrix ρS(t) of a HQMM
evolves within a time step (t, t + Δt) such that
ρS(t + Δt) =
∞

m=0
Km ρS(t) K†
m .
(3)
The above Kraus operators Km should form a complete set. This means they
need to obey the condition
∞

m=0
K†
mKm = 1
(4)

146
L.A. Clark et al.
for the density matrix ρS(t + Δt) to be normalised. More details can be found
in Ref. [6].
3
Open Quantum Systems
In the following, we describe how master equations can be used to model the time
evolution of open quantum systems with linear couplings between the quantum
system and its surrounding bath. Adopting the ideas of Zurek and others [12, 20],
we assume that the bath possesses an environmentally preferred state, a so-
called einselected or pointer state. While the internal states of the open quantum
system evolve on a relatively slow time scale, the bath relaxes rapidly back into its
preferred state whenever its state is perturbed by the system-bath interaction.
The more microscopic approach to the derivation of master equations, which
we review here, makes it easy to incorporate instantaneous feedback into the
dynamics of the open quantum system.
Our starting point is the Hamiltonian, H, for system and bath, which can be
split into four parts,
H = HS + Hint + HB + HSB .
(5)
Here, HS describes the free energy of the system and Hint allows for some internal
system dynamics. Moreover, HB represents the free energy of the bath and HSB
accounts for the system-bath interaction. When denoting the energy eigenstates
of system and bath by |n⟩S and |m⟩B, respectively, and assuming a linear coupling
between system and bath, HS, HB, and HSB can be written as
HS =
N

n=1
¯hωn|n⟩SS⟨n| ,
HB =
∞

m=0
¯hωm|m⟩BB⟨m| ,
HSB =
∞

m,m′=0
N

n,n′=1
¯hgnm,n′m′|n′m′⟩SB SB⟨nm| + H.c.
(6)
without loss of generality. Because of being a bath, an inﬁnite number of highly
degenerate energy levels ¯hωm may occur. Finally, the g’s are system-bath cou-
pling constants. Here, we assume for simplicity that these are time independent,
though this is not always the case.
Since we are interested in identifying the relatively slow, eﬀective internal
dynamics of the open quantum system, we now move into the interaction picture
with respect to the free system H0 = HB+HS, giving the interaction Hamiltonian
HI (t) =
∞

m,m′=0
N

n,n′=1
¯hgnm,n′m′|n′m′⟩SB SB⟨nm| e−i(ωm−ωm′+ωn−ωn′)t
+H.c. + Hint I (t)
(7)

Hidden Quantum Markov Models and Open Quantum Systems
147
with Hint I (t) describing the internal dynamics of the system in the interaction
picture. The Hamiltonian HI no longer contains free energy terms. The time
evolution of system and environment in the interaction picture is hence much
slower than in the Schr¨odinger picture.
Suppose the environment, which the bath couples to, thermalises very rapidly,
thereby relaxing the bath into an environmentally preferred state — a so-called
pointer state. This state minimises the entropy of the bath and does not evolve
in time unless there is a very strong system-bath interaction. In the following,
we denote the corresponding bath state by |0⟩B. Without restrictions, the above
introduced notation can indeed be chosen such that the pointer state corresponds
to m = 0. Moreover, we can choose the free energy of the pointer state such that
ω0 = 0, again without loss of generality.
Next we assume that the initial state of the open quantum system at a time t
is given by the density matrix ρS (t), while the bath is in |0⟩B. Over a short time
Δt, the system and bath then evolve in the interaction picture into the density
matrix ρSB(t + Δt) with
ρSB(t + Δt) = UI (t + Δt, t) |0⟩B ρS(t) B⟨0|U †
I (t + Δt, t) .
(8)
Subsequently, on a time scale that is fast compared to Δt, the surrounding
bath thermalises again, which transforms it back into its pointer state. Due to
locality, this process should only aﬀect the bath and not the quantum system
itself. All the expectation values of the system should remain the same during the
relaxation process. Consequently, the time evolution of system and bath within
Δt can be summarised as
ρSB(t + Δt) −→|0⟩BρS(t + Δt)B⟨0|
(9)
with
ρS (t + Δt) = TrB (ρSB(t + Δt)) .
(10)
This equation describes an eﬀective Markovian system dynamics within the time
interval (t, t+Δt). To summarise the eﬀective time evolution of the open quantum
system in a more compact form, i.e. in form of a master equation, we now
calculate the time derivative of ρS(t),
˙ρS = lim
Δt→0
1
Δt (ρS (t + Δt) −ρS (t)) .
(11)
Using the above mentioned time scale separation between the internal and ex-
ternal dynamics of the open quantum system allows us to evaluate equation (10)
using second order perturbation theory, which implies
UI(Δt, 0) = 1 −i
¯h
 Δt
0
dt HI (t) −1
¯h2
 Δt
0
dt
 t
0
dt′ HI (t) HI (t′) .
(12)
Substituting Eqs. (7), (10) and (12) into equation (11) eventually yields a master
equation of the general form (see e.g. Refs. [13–15, 20] for more details)
˙ρS = −i
¯h [Hint I, ρS] −1
2
N

n,n′,n′′,n′′′=1
ξnn′ξ∗
n′′n′′′

L†
n′′n′′′Lnn′, ρS

+

148
L.A. Clark et al.
+
N

n,n′,n′′,n′′′=1
ξnn′ξ∗
n′′n′′′ Lnn′ ρS L†
n′′n′′′ .
(13)
The L’s in this equation are operators that act on the internal states of the
open quantum system and the ξ’s are constants. In addition, one can show that
the above equation is of Lindblad form [21], which is the most general way of
expressing the master equation for a Markovian quantum system.
Equation (13) describes open quantum systems without feedback. These are
quantum systems, where the environment does nothing else but constantly resets
the bath that surrounds the system back into its pointer state. However, this is
not necessarily the case. Open quantum systems can be designed such that the
population of a state |m⟩B that is not environmentally preferred, triggers a back
action, which changes the density matrix ρS(t) by a certain unitary operation
Rm. Such a back action is known as feedback. If the feedback is so fast such that
its time scale is short compared to the time scale on which ρS(t) evolves, then we
talk about instantaneous feedback. Using the same arguments as above, one can
show that the open quantum system evolves in this case according to a master
equation of the general form
˙ρS = −i
¯h [Hint I, ρS]
−1
2
∞

m=1
N

n,n′,n′′,n′′′=1
ξnn′,mξ∗
n′′n′′′,m

L†
n′′n′′′,mLnn′,m, ρS

+
+
∞

m=1
N

n,n′,n′′,n′′′=1
ξnn′,mξ∗
n′′n′′′,m Lnn′,m ρS L†
n′′n′′′,m
(14)
with the Lnn′,m operators deﬁned such that
Lnn′,m = Rm Lnn′ .
(15)
This equation is of exactly the same form as the master equation for open quan-
tum systems with instantaneous feedback in Ref. [11].
4
Open Quantum Systems as HQMMs
Comparing the above description of open quantum systems with the deﬁnition
of the HQMM in Sect. 2, it becomes relatively straightforward to see that open
quantum systems with instantaneous feedback are concrete examples of HQMMs.
To illustrate this in more detail, we notice that ρS(t + Δt) in equation (10) is a
statistical mixture of diﬀerent subensembles and distinguish two cases.
4.1
Energy Exchange with Bath and Environment
The ﬁrst case is the one, where the bath has been reset into its pointer state, |0⟩B,
within (t, t + Δt) after having evolved into |m⟩B and experiencing the feedback

Hidden Quantum Markov Models and Open Quantum Systems
149
operation Rm. The above equations and their given interpretation tell us that
the density matrix of the corresponding subensemble equals
ρS(t + Δt|m ≥1) = Km ρS(t) K†
m
(16)
in this case, with the operator Km given by
Km =
N

n,n′=1
ξnn′,m Lnn′m
√
Δt
(17)
for m ≥1. As we shall see below, Km is a Kraus operator, which acts on the
internal state of the open quantum system.
4.2
No Energy Exchange between Bath and Environment
The remaining terms in the above master equation correspond to m = 0 and
describe the time evolution of the open quantum system under the condition
that the surrounding bath remains in its environmentally preferred state |0⟩B.
In this case, ρS(t) evolves within Δt into
ρS(t + Δt|m = 0) = K0 ρS(t) K†
0 .
(18)
Up to ﬁrst order in Δt, the corresponding operator K0 can be written as
K0 = exp

−i
¯hHcondΔt
	
(19)
with the non-Hermitian Hamiltonian Hcond given by
Hcond = Hint I −i
2¯h
∞

m=1
N

n,n′,n′′,n′′′=1
ξnn′,mξ∗
n′′n′′′,m L†
n′′n′′′,mLnn′,m .
(20)
The last term in this equation is crucial for the density matrix ρS(t + Δt) in
equation (14) to remain normalised.
4.3
Comparison of Kraus Operators
To show that open quantum systems with instantaneous feedback are examples
of HQMMs, we now only need to identify the operators Km in Eqs. (17) and
(19) with the Kraus operators in equation (3). Summing over all of the above
described subensembles with their respective output symbols given by m =
0, 1, ..., we immediately see that equation (3) applies. Since a density matrix
ρS(t), which evolves according to the master equation of an open quantum system
in Lindblad form remains normalised, we moreover have
TrS

 ∞

m=0
Km ρS K†
m

= TrS

 ∞

m=0
K†
mKm ρS

= 1 .
(21)
This means, equation (4) too is satisﬁed. Open quantum systems with instanta-
neous feedback are indeed examples of HQMMs.

150
L.A. Clark et al.
5
Conclusions
Motivated by the popularity of Hidden Markov Models (HMMs) in classical com-
puter science, this paper has a closer look at the quantum analogues of these
machines — so-called Hidden Quantum Markov Models (HQMMs) [6]. Sect. 2
deﬁnes HQMMs in terms of Kraus operators. Sect. 3 gives an overview of how
to model open quantum systems with and without instantaneous feedback with
the help of master equations in Lindblad form [11, 13–15, 21]. When comparing
Sects. 2 and 3 in Sect. 4, it becomes obvious open quantum systems with ran-
dom classical output sequences are examples of HQMMs. This paper proposes
not to ignore the random classical output sequences of open quantum systems,
since they could ﬁnd interesting applications as quantum simulators of stochastic
processes.
Finally, it might be worth noting that the above analysis of open quantum sys-
tems with instantaneous feedback only allows for an environmental back action
when the system-bath interaction changes the bath into a state that is diﬀerent
from its environmentally preferred state, |0⟩B. This need not be the case. Phys-
ically, it is possible to design open quantum systems, which experience feedback
also, when no exchange of energy occurs between system and bath. In this case,
the open quantum system can no longer be modelled by a master equation.
However, the eﬀective system dynamics would remain Markovian and could be
described using the language of HQMMs.
Acknowledgement. T. B. acknowledges ﬁnancial support from a White Rose
Studentship Network on Optimising Quantum Processes and Quantum Devices
for future Digital Economy.
References
1. Norris, J.R.: Markov chains, Cambridge University Press (1998)
2. Rabiner, L.R.: A tutorial on hidden Markov models and selected applications in
speech recognition. Proc. IEEE 77, 257 (1989)
3. Xue, H.: Hidden Markov Models Combining Discrete Symbols and Continuous
Attributes in Handwriting Recognition. IEEE Transactions on Pattern Analysis
and Machine Intelligence 28, 458 (2006)
4. Vanluyten, B., Willems, J.C., Moor, B.D.: Equivalence of State Representations
for Hidden Markov Models. Systems and Control Letters 57, 410 (2008)
5. Wiesner, K., Crutchﬁeld, C.P.: Computation in ﬁnitary stochastic and quantum
processes. Physica D 237, 1173 (2008)
6. Monras, A., Beige, A., Wiesner, K.: Hidden Quantum Markov Models and non-
adaptive read-out of many-body states. App. Math. and Comp. Sciences 3, 93
(2011)
7. Gmeiner, P.: Equality conditions for internal entropies of certain classical and
quantum models, arXiv:1108.5303 (2011)
8. O‘Neill, B., Barlow, T.M., Safranek, D., Beige, A.: Hidden Quantum Markov Mod-
els with one qubit. In: AIP Conf. Proc., vol. 1479, p. 667 (2012)

Hidden Quantum Markov Models and Open Quantum Systems
151
9. Sweke, R., Sinayskiy, I., Petruccione, F.: Simulation of Single-Qubit Open Quan-
tum Systems, arXiv:1405.6049 (2014)
10. Kraus, K.: States, Eﬀects and Operations. Lecture Notes in Physics, vol. 190.
Springer, Berlin (1983)
11. Wiseman, H.M., Milburn, G.J.: Quantum Measurement and Control. Cambridge
University Press (2010)
12. Zurek, W.H.: Decoherence, einselection, and the quantum origins of the classical.
Rev. Mod. Phys. 75, 715 (2003)
13. Dalibard, J., Castin, Y., Molmer, K.: Wave-function approach to dissipative pro-
cesses in quantum optics. Phys. Rev. Lett. 68, 580 (1992)
14. Hegerfeldt, G.C.: How to reset an atom after a photon detection. Applications to
photon counting processes. Phys. Rev. A 47, 449 (1993)
15. Carmichael, H.: An Open Systems Approach to Quantum Optics. Lecture Notes
in Physics, vol. 18. Springer, Berlin (1993)
16. Goldenfeld, N., Woese, C.: Life is Physics: evolution as a collective phenomenon
far from equilibrium. Ann. Rev. Cond. Matt. Phys. 2, 375 (2011)
17. Schuld, M., Sinayskiy, I., Petruccione, F.: Quantum walks on graphs representing
the ﬁring patterns of a quantum neural network, arXiv:1404.0159 (2014)
18. Kiesslich, G., Emary, C., Schaller, G., Brandes, T.: Reverse quantum state engi-
neering using electronic feedback loops. New. J. Phys. 14, 123036 (2012)
19. Emary, C.: Delayed feedback control in quantum transport. Phil. Trans. R. Soc.
A 371, 1999 (2013)
20. Stokes, A., Kurcz, A., Spiller, T.P., Beige, A.: Extending the validity range of
quantum optical master equations. Phys. Rev. A 85, 053805 (2012)
21. Lindblad, G.: On the generators of quantum dynamical semigroups. Comm. Math.
Phys. 48, 119 (1976)

 
© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
153
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_17 
 
An Efficient Strategy to Handle Complex Datasets 
Having Multimodal Distribution  
Samira Ghodratnama and Reza Boostani 
School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran  
sghodratnama@cse.shirazu.ac.ir, boostani@shirazu.ac.ir 
Abstract. One of the main shortcomings of the conventional classifiers is 
appeared when facing with datasets having multimodal distribution. To 
overcome this drawback, here, an efficient strategy is proposed in which a 
clustering phase is firstly executed over all class samples to partition the feature 
space into separate subspaces (clusters). Since in clustering label of samples are 
not considered, each cluster contains impure samples belonging to different 
classes. The next phase is to apply a classifier to each of the created clusters. 
The main advantage of this proposed distributed approach is to simplify a 
complex pattern recognition problem by training a specific classifier for each 
subspace. It is expected applying an efficient classifier to a local cluster leads to 
better results compared to apply it to several scattered clusters. In the validation 
and test phases, before make a decision about which classifier should be 
applied, we should find the nearest cluster to the input sample and then utilize 
the corresponding trained classifier. Experimental results over different UCI 
datasets demonstrate a significant supremacy of the proposed distributed 
classifier system in comparison with single classifier approaches. 
Keywords: Distributed classifiers, classifier ensembles, subspace classification, 
distributed learning, complex systems. 
1 
Introduction 
Classification is the most popular machine learning task in different applications. 
Although there are a number of different models on this major, it is still a challenge 
for experts to discover a model that can efficiently work on data with unknown 
distribution. Recently, researchers come to this agreement that a single classifier does 
not have enough ability, in terms of capacity and generalization, to classify a complex 
dataset. Among different models of classification, ensemble structures have recently 
attracted much attention because in many real applications they provide suitable 
results. As far as ensemble learners (e.g. boosting) are highly sensitive to noisy 
samples, this paper is aimed at proposing a completely new idea to handle datasets 
having multimodal density distribution. The main idea of this paper is to convert a big 
complex problem into some small problems with lower complexity and apply a 
learner to each subspace. It is obvious that the probability of getting good result by 
applying a classifier to a certain partition is more than expecting good results by 
applying just one learner for some scattered cluster of samples.  

154 
S. Ghodratnama and R. Boostani 
 
2 
Related Work    
There are different approaches to combine several methods to achieve a more 
comprehensive model. Different methods for combining classifiers have been 
proposed in terms of Bagging [1], Boosting [2] and Stacking [3]. In the most 
ensemble methods like Boosting, a series of learners are sequentially trained that each 
tries to cover the former’s shortcoming. Nevertheless, boosting use the benefit of 
parallel decision making via combining its learner’s vote [2]. It can be said that each 
learner of Boosting is more focused on the former misclassified samples but consider 
the other samples with lower attention. Nevertheless, each learner of Boosting is 
biased to some samples which are not necessarily localized in a certain subspace.  
Therefore, we can say that the idea of partitioning the feature space into non-
overlapping regions and make a distributed classifier system is not fully employed in the 
Boosting framework. Moreover, Bagging employs several classifiers but there is no 
guarantee that each learner works on a certain subspace because the selected samples 
(63% from all samples) for each learner are randomly chosen with replacement. 
Stacking also uses a parallel structure for decision making and has no constraint on 
using just one type of learner. Stacking yields better performance than any single 
classifier and has been successfully used to both supervised [4] and unsupervised 
learning [5] tasks; however, the difficulty of finding a good approach for meta-level 
makes it difficult to apply this approach in practice. In distributed classification methods 
which had been proposed, the common ways for partitioning the feature space are r-
sampling and d-sampling [6] that suffer from some weaknesses, one of the most 
obvious limitations is its need of a complete list of all the members of the population, 
this list is usually not available for large populations. The other is that there may be 
practical constraints in terms of time available and access to certain parts of the study 
area .In this paper, we suggest using a clustering algorithm to partition the feature space 
into local areas and apply a suitable classifier to each of them. 
3 
Methodology 
This paper presents a new strategy to handle datasets with multimodal distributions. In 
this method we first partition the feature space by a clustering method and then train 
proper classifiers on each subspace based on some statistical features of subspaces. 
Using a proper clustering method to partition the feature space can play an 
important role in the final result. Among variety of clustering methods, density based 
clustering methods are of interest due to their properties such as interpretability, 
creating clusters with arbitrarily shapes, insensitivity to the order of the data points 
and noise [7, 8]. Some of the state of the art density based clustering methods are 
DBSCAN [9], OPTICS [10], DBCLASD [11] and DENCLUE [12]. It should be 
mentioned that in this paper DENCLUE is selected because it is not sensitive to the 
input dimension while the other mentioned density based clustering methods have 
serious problem with high dimensional data. To improve the performance of 
clustering, as many researchers do, a feature selection method is applied to achieve 
more separable clusters.       

 
An Efficient Strategy to Handle Complex Datasets Having Multimodal Distribution 
155 
 
After partitioning the feature space by applying a good clustering method, a proper 
classifier based on statistical properties of each cluster should be chosen (similar to 
Stacking which employs different types of classifiers). The situation of samples in 
different subspaces (clusters) are not equal, for instance, some of them contain noisy 
samples, the population samples belonging to different classes in the created clusters 
is not the same. In the test phase, each sample has to find its nearest cluster based on 
the Mahalanobis distance and then the corresponding classifier is used to assign the 
label to that sample. To evaluate the proposed method, state of the art methods are 
implemented and all applied to some selected datasets driven form UCI database 
repository [13]. An overview of our method in the train and test phases is 
demonstrated in Fig. 1. 
 
Fig. 1. An overview of proposed method  
3.1 
Feature Selection 
Reducing the number of features to find the relevant feature subset of the original 
feature set is important in statistical learning. For many data sets with a large number 
of features and a limited number of observations, some of the features may usually 
considered as redundant ones when they do not carry any discriminative information 
for classifying the samples belong to  different classes. Feature selection can also help 
to avoid the risk of over-fitting and sensitivity to the noisy features. Reducing features 
can also save storage and computation time and increase the comprehensibility. In this 
paper, MCFS [14] is used to select the discriminative and informative features. The 
result of performing this method is illustrated in Table 2. 
3.2 
Clustering Phase 
Detecting clusters of points is a challenging issue, especially when the clusters are of 
different size, density and shape. Many of these issues become even more significant 
when facing with high dimensional samples when the dataset contains noisy and 
outlier samples. Density based clustering algorithm is one of the primary methods for 
clustering. Among the density based algorithms two popular methods are DENCLUE 
and DBSCAN. 
In this paper, DENCLUE algorithm is used as preprocess approach to make 
subspaces. The advantages of this approach are: (a) it has a firm mathematical basis, 
(b) it has good clustering properties in data sets with large amounts of noise, (c) it 
allows a compact mathematical description of arbitrarily shaped clusters in high-
dimensional datasets and (d) it is significantly faster than the existing algorithms [12]. 
 
 
 
 
 
 
Select a proper classifier based 
for the subspaces 
Trained 
classifiers  
Train Samples 
Cluster the train 
Samples 
Assign samples to 
suitable cluster based on 
mahalanobis distance 
Predictive 
label 
Classify test samples by the 
corresponding trained classifier 
to that cluster 
Test 
Samples 
Feature 
Selection by 
MCFC 

156 
S. Ghodratnama and R. Boostani 
 
Although DENCLUE has some advantages, its problem is the high number of input 
parameters. We handle this problem by determining the validity of the clusters as it is 
discussed in next section. 
3.2.1 Validation of the Created Clusters 
A major challenge in cluster analysis is estimating the optimal number of clusters. 
There are various methods to determine the validity of the clusters including: Gap 
statistic [15], Elbows [16], silhouette [17] and prediction strength [18] methods. 
We first perform the Gap statistic method to determine optimal number of clusters. 
When we find out this number, we select the input parameters of DENCLUE method 
such that the number of clusters become close to the optimal number of clusters 
achieved by the Gap statistic method. Then we perform Silhouette method to measure 
how well the samples are clustered.  
3.3 
Classification Phase 
The state of the art classifiers are Decision Trees (DT), Random Forest (RF), Support 
Vector Machine (SVM), k-Nearest Neighbor (KNN), Artificial Neural Networks 
(ANN), AdaBoost, Bagging and Stacking, and so on. Each of the classification 
methods shows different efficacy and accuracy based on the statistical characteristics 
of datasets [19]. Among the mentioned classifiers, SVM, KNN, Neural Network, 
Decision Tree, Random Forest and AdaBoost are chosen in this study. These selected 
classifiers can help us having different shape of classes’ boundary based on shapes of 
the data distribution.  
Stages of the proposed method are described below in terms of pseudo code.  
1. Perform MCFC algorithm to select the discriminative features 
2. Run Gap statistic method to find optimal number of clusters 
3. Perform DENCLUE Algorithm for clustering and adjust its parameter such that 
the result of clustering become close to the result of Gap statistic method 
4.  The silhouette algorithm is run to determine how valid each of the clusters find 
out validation of clustering 
5. Train different classifiers on each cluster  
6. Use Mahalanobis distance to find the proper cluster of each test data 
7. Predict the label of test data by classifying the input sample using the trained 
classifier of the corresponding cluster. 
8. Use Freidman Test to evaluate the proposed method. 
4 
Experimental Results 
In order to evaluate the proposed method, a set of experiments are run using 12 well-
known machine learning datasets driven from the UCI Machine Learning Repository. 
The details of these data sets are described in Table 1. In selecting the datasets, we 
tried to cover the following states: low and high dimensional samples, two and 
multiclass problems, containing noisy and outlier samples, balance and imbalance 
datasets. 

 
An Efficient Strategy to Handle Complex Datasets Having Multimodal Distribution 
157 
 
Table 1. Selected datasets from UCI database [13]  
Data Set 
Number 
of 
Features 
Number of 
Samples 
Number of  
classes 
Iris 
4 
150 
3 
Glass 
9 
214 
6 
Diabetes 
8 
768 
2 
Bupa 
6 
345 
2 
Tic_Tac_Toe 
9 
958 
2 
Heart 
13 
270 
2 
Wine 
13 
178 
3 
Seed 
7 
210 
3 
Satellite 
36 
6435 
6 
Sonar 
60 
208 
2 
Blood 
4 
748 
2 
Letter 
Recognition 
16 
20000 
26 
Table 2. Result of Feature Selection Method 
Data Set 
Number 
of 
Original 
Features 
Number of 
Features 
Selected by 
MFCF 
Iris 
4 
4 
Glass 
9 
9 
Diabetes 
8 
8 
Bupa 
6 
6 
Tic_Tac_Toe 
9 
9 
Heart 
13 
5 
Wine 
13 
9 
Seed 
7 
7 
Satellite 
36 
22 
Sonar 
60 
42 
Blood 
4 
4 
Letter 
Recognition 
16 
16 
 
 
 

158 
S. Ghodratnama and R. Boostani 
 
First we applied Gap statistic method to find the suitable number of clusters. The 
result of Gap statistic, showed in the Table 3, is the median over 10 times running. 
Then, we execute DENCLUE to cluster the datasets. The number of clusters achieved 
by this method is also shown in Table 3. After clustering the data points, we perform 
silhouette method to evaluate the clustering method. The results are illustrated in 
Table 3. 
Table 3. The Estimated number of clusters by Gap-Statistics, DENCLUE and the Silhouette 
value belong to the all samples of DENCLUE clusters 
Data Set 
Gap-
Statistic 
Results 
Number of 
Clusters by 
DENCLUE 
Silhouette 
Results 
Iris 
3 
3 
0.6992 
Glass 
2 
3 
0.6178 
Diabetes 
3 
5 
0.5809 
Bupa 
2 
4 
0.6513 
Tic_Tac_Toe 
3 
6 
0.7139 
Heart 
3 
4 
0.6073 
Wine 
3 
3 
0.7051 
Satellite 
2 
5 
0.6647 
Sonar 
3 
2 
0.7015 
Seed 
2 
3 
0.6814 
Blood 
3 
4 
0.7080 
Letter 
Recognition 
3 
5 
0.6450 
 
After achieving the clusters, we want to classify each cluster to evaluate the 
proposed method. We first run single classifiers on the selected features of datasets, 
the results of train and test phases are demonstrated in Tables 3 and 4. In all 
experiments to validate the results, 10-times 10-fold cross validation is used. 
Table 4. Result of single classifiers in the train phase (in %) 
Data Set
SVM 
AdaBoost
Decision 
Tree 
Random 
Forest 
Neural 
Network  
Iris 
97.4962± 
0.6569 
97.0666± 
0.0063 
93.8962± 
0.0136 
99.9770± 
0.0012 
97.7472± 
1.0404 
Glass 
60.5499± 
0.6569 
50.2610± 
0.2793 
70.31855± 
1.1931 
81.7359± 
0.0006 
60.9025± 
1.9111 
Diabetes
83.0541± 
0.0100 
80.1373± 
0.0224 
76.4018± 
0.3696 
99.9594± 
0.0009 
77.7748± 
0.2660 

 
An Efficient Strategy to Handle Complex Datasets Having Multimodal Distribution 
159 
 
Table 5. (continued) 
Bupa 
83.9002± 
0.0457 
82.3961± 
0.1302 
66.8894± 
1.7618 
99.9742± 
0.0008 
71.1108± 
1.2184 
Tic_Tac_Toe 
99.1336± 
0.0006 
78.6882± 
0.0738 
80.2866± 
0.3596 
99.9976± 
0.0002 
74.8288± 
1.5964 
heart 
84.0123± 
0.0569 
84.4526± 
0.0631 
67.1563± 
1.2886 
99.8847± 
0.0063 
80.7637± 
1.2052 
wine 
97.2223± 
0.0123 
99.1260± 
0.0129 
98.7655± 
1.1827 
100.0000± 
0.0000 
97.7037± 
0.1522 
seed 
92.7566± 
0.0363 
93.6031± 
0.0257 
92.1957± 
0.7081 
99.9894± 
0.0004 
92.5192± 
0.3394 
Satellite
80.0800± 
0.0010 
81.1048± 
0.0300 
87.1025± 
0.0011 
100.0000± 
0.0000 
77.6099± 
0.0000 
Sonar 
99.9821± 
0.0430 
99.9700± 
0.045 
89.5176± 
1.3840 
100.0000± 
0.0000 
85.9441± 
1.5579 
Blood 
72.7109± 
0.0531 
81.0975± 
0.0082 
60.2559± 
0.1475 
91.8375± 
0.0016 
78.7553± 
0.0414 
Letter 
Recognition 
89.0918± 
0.0008 
97.0283± 
0.0819 
79.7195± 
0.0198 
99.3471± 
0.00001 
78.9804± 
0.0710 
Table 6. Result of single classifiers in the test phase (in %) 
Data Set 
SVM 
1NN 
AdaBoost Decision 
Tree 
Random 
Forest 
Neural 
Network(MLP) 
Iris 
95.4000± 
1.3407 
95.8666±
0.0790 
95.2000± 
0.2765 
91.7346± 
1.0977 
94.7333± 
0.4395 
93.6571± 
1.4427 
Glass 
59.5648± 
0.1455 
72.4896±
0.6218 
48.4976± 
0.6600 
53.2963± 
2.317 
79.1295± 
0.8609 
52.0831± 
2.2531 
Diabetes 
81.9648± 
0.0121 
67.5622±
0.6604 
75.4564± 
0.3907 
68.3848± 
1.1385 
76.5085± 
0.3660 
75.9161± 
0.7625 
Bupa 
82.2405± 
0.0392 
61.8470±
0.6851 
74.5747± 
1.6581 
51.4974± 
2.0210 
72.3739± 
1.7591 
67.6941± 
2.0331 
Tic_Tac_Toe 97.5876± 
0.0011 
69.8941±
0.1024 
77.1872± 
0.2511 
71.2274± 
1.2520 
94.9783± 
0.4466 
71.0396± 
2.1951 
heart 
83.1888± 
0.0446 
61.1851±
0.7559 
76.8888± 
0.4328 
60.1111± 
1.2174 
75.1851± 
1.1315 
77.5389± 
1.5282 
wine 
95.5134± 
0.3623 
93.2062±
0.1843 
94.9830± 
0.4030 
92.6662± 
1.1959 
96.8383± 
0.6869 
92.4379± 
1.6759 
seed 
91.3809± 
0.4258 
90.6190±
0.1033 
92.1904± 
0.4132 
84.6190± 
1.3707 
93.4761± 
0.5568 
85.7228± 
1.2500 
Satellite 
78.6500± 
0.0000 
89.8000±
0.0000 
78.7000± 
0.0000 
75.9500± 
0.0000 
90.8500± 
0.0000 
74.7500± 
0.0000 
Sonar 
96.0480± 
0.0009 
86.8798±
0.6868 
82.2930± 
1.4187 
67.6783± 
1.4597 
80.6517± 
1.9204 
72.9322± 
2.9881 
Blood 
72.4639± 
0.0544 
68.7176±
0.6703 
79.3063± 
0.3253 
55.0266± 
0.9982 
75.1318± 
0.8841 
77.6245± 
0.1308 
Letter 
Recognition 
87.2308± 
0.0190 
96.0129±
0.0026 
95.2000± 
0.2765 
70.8276± 
0.0499 
92.9871± 
0.0054 
78.5000± 
0.4213 

160 
S. Ghodratnama and R. Boostani 
 
Table 7. Result of the proposed method in the train phase (in %) 
Data Set 
SVM 
AdaBoost
Decision 
Tree
Random 
Forest
Neural   
Network(MLP) 
Iris 
97.5980± 
0.1022 
99.8460±
0.0634 
97.5944±
0.1319 
99.9760±
0.0252 
88.0865± 
1.0405 
Glass 
65.4055± 
0.2130 
68.8846±
0.6423
71.9866±
1.4389
85.9819±
0.0255
67.3290± 
1.0151 
Diabetes 
97.1680± 
0.0580 
95.1057±
0.1391 
73.2398±
1.1360 
99.9740±
0.0240 
83.1820± 
1.0012 
Bupa 
92.1111± 
0.2772 
97.4543±
0.1108 
60.1328±
1.0227 
99.9609±
0.0577 
66.8888± 
1.0902 
Tic_Tac_Toe
99.9388± 
0.0080 
87.4204±
0.1628 
76.6748±
0.9267 
99.9596±
0.0268 
67.0606± 
1.0132 
Heart 
92.4563± 
0.1586 
91.4914±
0.2394 
85.4047±
0.5509 
99.9441±
0.0577 
71.7695± 
1.0335 
Wine 
100.000± 
0.0000 
99.9085±
0.0574 
96.3697±
0.21059 
100.0000±
0.0000 
75.6064± 
1.0002 
Seed 
98.4510± 
 0.0957 
98.4873±
0.1518
82.2980±
1.2573
99.9829±
0.0289
66.6320± 
1.0122 
Satellite 
74.7766± 
  0.0038 
79.8715±
0.1002
77.5391±
0.5245
99.9689±
0.0388
72.5204± 
1.0295 
Sonar 
100.000± 
0.0000 
100.0000±
0.0000
82.5325±
1.3477
99.9947±
0.0166
61.0849± 
1.0015 
Blood 
76.4389± 
0.3329 
90.0878±
0.0824
61.7175±
0.8544
96.5688±
0.1884
81.1207± 
1.0661 
Letter 
Recognition 
90.0412± 
0.0320 
97.9805±
0.0058
90.0753±
0.0011
99.0810±
0.0000
79.9901± 
1.0031 
Table 8. Results of the proposed method in the test phase (in %) 
Data Set 
SVM 
1NN 
AdaBoost
Decision 
Tree 
Random 
Forest 
Neural 
Network 
Iris 
97.6703± 
0.1395 
95.3303±
0.8905 
93.5122±
0.8060 
92.1648±
0.2852 
95.2504±
0.4867 
85.8000± 
1.0094 
Glass 
60.9208± 
0.4279 
73.1790±
0.4487 
60.9159±
0.7094 
59.0869±
0.0460 
81.4069±
0.8499 
56.0029± 
2.0901 
Diabetes 
87.8929± 
0.9643 
68.3357±
0.6509 
83.7097±
0.9545 
70.3452±
0.4882 
78.1142±
0.7783 
75.0981± 
0.0021 
Bupa 
88.2606± 
0.6160 
64.0374±
0.0209 
80.6944±
0.6032 
58.3839±
0.2683 
82.1417±
1.4208 
61.9761± 
1.2387 
Tic_Tac_Toe
98.8454± 
0.4492 
72.0063±
0.5805
83.6441±
0.8285
74.7350±
0.8476
95.7643±
0.6645
65.0971± 
0.0461 
heart 
85.2965± 
0.2371 
66.3667±
0.9837
79.0835±
0.1559
73.2512±
1.7372
88.0271±
0.0906
68.6043± 
1.0790 
wine 
95.4987± 
1.4510 
94.1081±
0.3755
94.8566±
0.8298
93.4226±
0.7543
96.5191±
0.3363
71.0512± 
1.0816 
Seed 
93.6824± 
0.6576 
90.3891±
0.0876
92.9188±
0.5909
80.0596±
1.5077
94.2951±
0.1912
62.3809± 
1.0300 
Satellite 
71.4349± 
0.1326 
90.7558±
0.3425
78.3063±
0.0612
75.0782±
0.4934
91.0341±
0.2092
70.0028± 
0.1200 
Sonar 
97.9587± 
0.5514 
89.1825±
0.3919
81.3501±
0.1334
62.7111±
0.7949
78.1362±
0.4637
59.3451± 
0.5419 
Blood 
75.3054± 
0.5344 
70.6725±
0.8280
83.0599±
0.4114
58.0657±
0.4191
77.2757±
0.8171
73.2415± 
0.0002 
Letter 
Recognition
88.9871± 
0.0001 
98.8466±
0.1854
95.9099±
0.0065
73.1114±
0.0098
93.9104±
0.0041
75.0817± 
0.0361 

 
An Efficient Strategy to Handle Complex Datasets Having Multimodal Distribution 
161 
 
Results of executing classifiers on each cluster of a dataset are shown in Table 5 
and Table 6. The final result is the mean over the performance of each cluster. 
Our results indicate that ensemble of individual partitions can yield better results 
than learning one classifier over the entire dataset.  
5 
Evaluation 
To measure the performance of the proposed method, especially for multiclass 
problems, we execute the Friedman test [20]. We compare the result of each single 
classifier to considering that classifier in the proposed framework in both test and 
train phase. The results illustrate that 1NN, AdaBoost and Random Forest have 
remarkable improvement in almost datasets in both train and test phases but Neural 
Network and SVM do not perform that well.  
To demonstrate the supremacy of the proposed distributed method, we plot the 
performance of each single classifier versus corresponding distributed classifier over 
mean of all datasets in Fig. 2 As we see in most classifiers, the proposed distributed 
method (DMethod) is performed better, although the rate of improvements are 
different depending on the structure of classifiers. As an example the KNN method is 
locally acted, it is clear that in the distributed KNN method (DKNN) which locality is 
considered, the rate of the improvement would be much more significant compare to 
methods like decision tree that locality is not that important. In Fig. 2, it is showed 
that MLP performs not well, it is not surprising since MLP is a classifier that learns 
from samples, reducing sample size leads to information loss and therefore do not 
provide a remarkable result. The other method which is not performed well is SVM; 
since using nonlinear kernel is somehow a blind process, we cannot exactly figure out 
how the input distribution is changed in the higher dimensional space (kernel space). 
Based on cover’s theorem [21], mapping the original data by a suitable kernel leads to 
re-distribute the transformed samples such that they are more likely to be linearly 
separable in transformed space. Nevertheless, there is no guaranty that a proper kernel 
can act well on all existing datasets. Thus, as we can see from Fig. 3, the results for 
some datasets are improved and for some others are diminished. 
In Fig. 2, we plot for each point a square which illustrates the standard deviation 
(mean+SD and mean-SD). As it is showed, the standard deviation in the proposed 
method is a very small value such that in the figure it cannot be seen but in single 
classifier is noticeable. This result presents that the proposed distributed method is 
more stable than the single classifiers. 
The other strength of our method is that by reducing the sample size and also 
features size, it is clear that the complexity of each classifier for each local cluster is 
reduced and therefore the overall complexity is reduced.  
To analysis the results based on the selected features of datasets, we choose the 
best classifier (from Fig. 2) and compare the result of this classifier in the single mode 
(KNN) and distributed (DKNN) mode for all 12 datasets, separately. Although the 
rate of improvement is different over datasets, in all datasets, the distributed method 
mode has perform better except for the Glass dataset that the results get worse and in 
 

162 
S. Ghodratnama and R. Boostani 
 
 
Fig. 2. Compare the results of single classifier and the proposed method over all datasets 
 
Fig. 3. Compare the results of KNN and DKNN for each dataset separately 
Satimage (Satellite) which the results are near the same. Since we remove the noise in 
the clustering phase, in datasets contains noisy samples and also in datasets where 
data points are not well-separated (highly overlapped), the results show significant 
improvement but in datasets which points are well-separated, the improvement is not 
that significant. 
6 
Conclusions 
In this paper, we present a novel approach to improve the performance of distributed 
classifier systems based on local learning. The idea of this approach is to simplify a 
complex problem into some simpler problems by clustering the entire dataset and 
assign a classifier to classify the samples within each cluster. Experimental results 

 
An Efficient Strategy to Handle Complex Datasets Having Multimodal Distribution 
163 
 
demonstrate a significant supremacy of the proposed distributed classifier system in 
comparison with single classifier approaches, especially in the test phase as discussed 
in section 5. As the future work, distance metric should be selected specifically for 
each cluster corresponding to its density shape and its statistical properties. For this 
purpose, a distance learning process can be considered to improve the results. 
References 
1. Breiman, L.: Bagging predictors. Journal of Machine Learning 24(2), 123–140 (1996) 
2. Schapiro, R.E.: The Strength of Weak Learnability. Journal of Machine Learning 5(2), 
197–227 (1990) 
3. Wolpert, D.H.: Stacked Generalization. Journal of Neural Networks 5(2), 241–259 (1992) 
4. Breiman, L.: Stacked Regression. Journal of Machine Learning 24(1), 49–64 (1996) 
5. Smyth, P., Wolpert, D.: Linearly Combining Density Estimators via Stacking. Journal of 
Machine Learning 36(1), 59–83 (1999) 
6. Lazarevic, A., Obradovic, Z.: Boosting Algorithms for Parallel and Distributed Learning. 
Journal of Distributed and Parallel Databases 11(2), 101–229 (2002) 
7. Parimala, M., Lopez, D., Senthilkumar, N.C.: A Survey on Density Based Clustering 
Algorithms for Mining Large Spatial Databases. International Journal of Advanced 
Science and Technology 31 (2011) 
8. Nagpal, P., Mann, P.: Comparative Study of Density based Clustering Algorithms. 
International Journal of Computer Applications 27(11), 421–435 (2011) 
9. Ester, M., Kriegel, H.P., Sander, J., Xu, X.: A Density-based algorithm for discovering 
clusters in large spatial databases with noise. KDD 96(34), 226–231 (1996) 
10. Ankrest, M., Breunig, M., Kriegel, H., Sander, J.: OPTICS: Ordering Points to Identify the 
Clustering Structure. In: International Conference on Management of Data, pp. 49–60 (1999) 
11. Xiaowei, X., Jägerand, J., Kriegel, H.P.: A fast parallel clustering algorithm - for large 
spatial databases. Journal of Data Mining and Knowledge Discovery 3, 263–290 (1999) 
12. Hinneburg, A., Keim, D.A.: A General Approach to Clustering in Large Databases with 
Noise. Journal of Knowledge and Information Systems (KAIS) 5(4), 387–415 (2003) 
13. Bache, K., Lichman, M.: UCI Machine Learning Repository. University of California, 
School of Information and Computer Science, Irvine (2013),  
http://archive.ics.uci.edu/ml 
14. Cai, D., Zhang, C., He, X.: Unsupervised Feature Selection for Multi-cluster Data. In: 16th 
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2010) (July 
2010) 
15. Tibshirani, R., Walther, G., Hastie, T.: Estimating the number of clusters in a dataset via 
the Gap statistic. Journal of the Royal Statistical Society 63(2), 411–423 (2001) 
16. Thorndike, R.L.: Who Belongs in the Family? Journal of Psychometrika 18, 267–276 
(1953) 
17. Rousseeuw, P.J.: Silhouettes: a graphical aid to the interpretation and validation of cluster 
analysis. Journal of Computational and Applied Mathematics 20(1), 53–65 (1987) 
18. Tibshirani, R., Walther, G.: Cluster Validation by Prediction Strength. Journal of 
Computational and Graphical Statistics 14(3), 511–528 (2005) 
19. Entezari-Maleki, R., Rezaei, A., Minaei-Bidgoli, B.: Comparison of Classification 
Methods Based on the Type of Attributes and Sample Size. Journal of Convergence 
Information Technology (JCIT) 4(3), 94–102 (2009) 
20. Friedman, M.: A correction: The use of ranks to avoid the assumption of normality 
implicit in the analysis of variance. Journal of the American Statistical Association 
32(200), 675–701 (1937) 
21. Ting, K., Zhu, L., Wells, J.R.: Local Models—The Key to Boosting Stable Learners 
Successfully. Journal of Computational Intelligence 29(2), 331–356 (2013) 

Maximum Likelihood Estimation and Integration
Algorithm for Modeling Complex Systems
Yoshinao Shiraki
Toho University, Chiba, 274-8510, Japan
shiraki@is.sci.toho-u.ac.jp
Abstract. The holonomic gradient descent (HGD) method has been
proposed as a means for calculating the maximum likelihood estimate
(MLE), and its eﬀectiveness has, in recent years, been reported within
the statistics community. The purpose of HGD calculations is to reduce
the calculation of the maximum likelihood estimate (MLE) of particular
types of functions to calculating the minimum value of the holonomic
function. As is well known, the maximum likelihood estimate (MLE)
plays an important role in complex systems theory. In the complex sys-
tems community, however, little is known about the holonomic gradient
descent (HGD) method. In this article, we introduce this method to the
complex systems community and review the calculation mechanism of
HGD.
Keywords: Maximum Likelihood Estimate (MLE), Groebner Basis,
Holonomic Gradient Descent (HGD), Integration algorithm.
1
Introduction
In order to calculate the maximum likelihood estimate (MLE) it has been pro-
posed to use the holonomic gradient descent (HGD) method, and its eﬀectiveness
has, in recent years, been reported within the statistics community ([3,1,12]). The
purpose of this method is to reduce the calculation of the maximum likelihood
estimate (MLE) of particular types of functions to calculating the minimum
value of the holonomic function. As is well known, the maximum likelihood es-
timate (MLE) plays an important role in optimal control theory (see cf. [5]). In
the control community, however, little is known about the holonomic gradient
descent (HGD) method. Therefore, in this article, we introduce this method to
the complex systems community and review the calculation mechanism of HGD.
The holonomic gradient descent (HGD) method turns an analytical problem
into an algebraic one. HGD eases the diﬃculties of calculating the integrals that
appear in likelihood calculations, by utilizing a hlinear algebraic calculationh as
follows: First, HGD uses the fact that most probability density functions are
holonomic functions[2]. It then reduces the calculation of the deﬁnite integral of
the probability density function to a hlinear algebraic calculationh on a holo-
nomic function. hThe deﬁnite integral in a probability density functionh is the
normalization constant, and the hlinear algebraic calculationh is simpliﬁed by
c
⃝Springer International Publishing Switzerland 2015
165
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_18

166
Y. Shiraki
hcalculating the Groebner basis[9]”. For a diﬀerential operatorL of rational coef-
ﬁcients, when the equation L • f = 0 holds, the function f is called a holonomic
function.
Next, let’s look back on the likelihood calculation. First, the probability den-
sity function p(x) can be written as follows:
p(x) =
f(x)

D f(x)dx.
(1)
Here, the deﬁnite integral

D f(x)dx is the normalization constant and is the
central element in the likelihood calculation. It often happens that calculating
the normalization constant from the deﬁnite integral is diﬃcult. In many cases,
the normalization constant cannot be expressed by the simple integral given in
equation (1). If θ represents the dispersion parameters, such as the average, then
the deﬁnite integral becomes F(θ) =

D f(θ, x)dx. In this case, the conventional
likelihood calculation becomes diﬃcult, and it is much better to use the holo-
nomic gradient descent (HGD) method. By taking advantage of the fact that
the probability density function is a holonomic function, the diﬃculties in the
calculation can be eased as follows:
1 The normalization constant F(θ) =

D f(θ, x)dx is also a holonomic function
of θ.
2 The use of Groebner basis calculations for the diﬀerential equation system
becomes applicable.
Fortunately, the fact that hmost probability density functions are holonomic
functions h means that a good deal of attention has been focused on the holo-
nomic gradient descent(HGD) method. Of course, rational or polynomial func-
tions are holonomic functions. In addition, holonomic functions have hdesirable
propertiesh and are equivalent or superior to trigonometric and polynomial func-
tions. hThe desirable properties of holonomic functionsh are that they are closed
under deﬁnite integral calculations and sum, diﬀerence, and product operations.
hThe motivation for thinking about things that have desirable properties such
as holonomic functionshis because hpeople want to make concrete things easy
to handle algebraicallyh (thought attributed to Mikio Sato) [8].
In Section 2, we review the holonomic gradient descent (HGD) method. In
Section 3, we examine the eﬀectiveness of the method, including the eﬀectiveness
of using Groebner basis calculations for the algorithm to calculate the minimum
value of the holonomic function ([6,7]). Concerning a comparison with the Maxi-
mum likelihood method, please refer to [3]. It should be noted that, in this article,
we mainly describe the application of the HGD method to maximum likelihood
estimates. However, the method is also applicable to other estimation meth-
ods, for example, refer to [10] for applications to MAP (Maximum-a-Posteriori
(MAP)) estimation, etc..

Integration Algorithm
167
2
Holonomic Gradient Descent (HGD)
In this section, we review the holonomic gradient descent method, focusing on
the motivation for its introduction [11].
2.1
The Gradient Descent Method and Its Computational
Diﬃculties
The gradient descent method is a powerful tool for determining the maximum
or minimum value of a given function g, by sequentially updating the solution
z(k). When updating the solution, the gradient descent method primarily uses
the gradient ∇g to provide information about increases or decreases in g and
also uses the Hessian matrix Hk of the function g as auxiliary information. The
solution z(k) is updated according to the following equation:
z(k+1) = z(k) + akd(k).
(2)
Here, d(k) = −H−1
k ∇g(z(k)) is called the search direction and ak is the updated
width. As already mentioned in Section 1, it often occurs that, depending on the
given function g, we cannot explicitly obtain the Hessian matrix Hk and/or the
gradient ∇g. For example, when the normalization constant of the probability
density function cannot be obtained analytically, accurate calculations of Hk
and ∇g are diﬃcult.
2.2
Holonomic Functions and Their Computational Superiority
On the other hand, when the given function g is a holonomic function, Hk and
gradient ∇g can be easily represented ([12], p.355, Lemma 6.5.1). By calculating
the Groebner basis of the diﬀerential equation system, it is possible to calculate
the Hk and ∇g sequentially. Fortunately, many functions are holonomic func-
tions. In particular, most probability density functions are holonomic functions.
Thus, the maximum likelihood estimate (MLE) is a typical example for which
the function is a holonomic function. In MAP (Maximum-a-Posteriori (MAP))
estimation, also, the function is similar to a holonomic function[10].
2.3
Holonomic Function and Calculation of the Groebner Basis
As described in Section 1, the holonomic gradient descent (HGD) method turns
an analytical problem into an algebraic one. HGD eases the diﬃculties of calcu-
lating the integrals that appear in likelihood calculations, by utilizing a hlinear
algebraic calculationh as follows: First, HGD uses the fact that most probability
density functions are holonomic functions. It then reduces the calculation of the
deﬁnite integral of the probability density function to a hlinear algebraic calcu-
lationh on a holonomic function. In this subsection, we ﬁrst describe examples
of holonomic functions. Then we describe how the problem of calculating the ex-
treme values of a holonomic function can be reduced to calculating a Groebner
basis.

168
Y. Shiraki
2.4
Examples of Holonomic Functions
We show an example of a two-variable holonomic function below. Here, the
symbol ∂θi represents a partial derivative;
∂
∂θi . When a two-variable function
f(θ1, θ2) satisﬁes the following linear diﬀerential equation, it is called a two-
variable holonomic function:
(ap(θ1, θ2)∂p
θ1 + · · · + a1(θ1, θ2)∂θ1 + a0(θ1, θ2)) • f = 0,
(bp(θ1, θ2)∂p
θ2 + · · · + b1(θ1, θ2)∂θ2 + b0(θ1, θ2)) • f = 0.
(3)
Here, the coeﬃcients ai; bi are polynomial functions of two variables θ1, θ2. For
example, function f is a holonomic function because the function f = exp(θ1θ2)
satisﬁes the following equation:
(∂θ1 −θ2) • f = 0,
(∂θ2 −θ1) • f = 0.
(4)
The following functions are examples of holonomic functions.
f(x, y, z) = sin(x) + sin(y) cos(z),
f(x, y) = exp(x + y),
f(x, y) =
 2π
0
exp(x cos(t) + y sin(t))dt,
f(θ) = exp(θ1 cos(x) + θ2 sin(x))/F(θ), F(θ) =
 2π
0
exp(θ1 cos(x) + θ2 sin(x))dx.
(5)
Here, the function f(θ) is the von Mises distribution([4]). In Section 3, we deal
with the calculation of the normalization constant of the von Mises distribution.
2.5
Maximum Value of the Holonomic Function and the System of
Diﬀerential Equations
In order to demonstrate how to ﬁnd the maximum value of a single variable
function f(x) = sin (x), we shall outline how to use the system of diﬀerential
equations of the function f(x). Here, ∂=
∂
∂x. First, the function f(x) = sin (x)
satisﬁes the following equation:
(∂2 + 1) • sin (x) = 0.
(6)
Therefore, the function f(x) = sin (x) is a holonomic function. By using this
fact, the following holds:
∂• sin (x)
∂2 • sin (x)

=
∂• sin (x)
−sin (x)

.
(7)
The following equation holds by recursively rewriting the above equation:
∂•

sin (x)
∂• sin (x)

=

0 1
−1 0
 
sin (x)
∂• sin (x)

.
(8)

Integration Algorithm
169
Here, we set
F(x) =

sin (x)
∂• sin (x)

,
then, the following equation holds:
∂• F =
 0 1
−1 0

F.
(9)
Using the Taylor expansion formula and equation (9), the following equation
holds:
F (x + h) = F (x) + h∂• F (x) + o(h) = F (x) + h

0
1
−1 0

F (x) + o(h).
(10)
Therefore, by using the value of F(x), we obtain an approximate value of F(x +
h). Thus, the things we require in order to calculate the maximum value of the
function f(x) = sin (x) are the following: The vector function F(x);
F(x) =

sin (x)
∂• sin (x)

,
and the system of diﬀerential equations of F(x);
∂• F =
 0 1
−1 0

F.
In addition, we need the vector value F(x0) at a certain point x0 of the function
F(x).
[Note 1] In order to reduce the calculation of the maximum value of the
function into the calculation of a Groebner basis, it is necessary to convert them
into Pfaﬃan equations. Fortunately, such transforms exist and are conﬁgurable
(see Chapter 6 in [12]).
3
Integration Algorithm
In Section 2, we described the diﬃculty of calculating the deﬁnite integral ap-
pearing in the maximum likelihood estimate, and how we could reduce calcula-
tion of the deﬁnite integral F(θ) =

D f(θ, x)dx into calculation of a holonomic
function. However, that description was only the ﬁrst part. Now, we describe the
calculation for a system of diﬀerential equations of the holonomic function f. In
fact, in order to create a system of diﬀerential equations of F from the system of
diﬀerential equations of f, a new algorithm is needed. In this Section, we describe
how we reduce the calculation of the holonomic function F(θ) into calculation
of a Groebner basis. The second part is called the integration algorithm in the
diﬀerential ring. In this Section, we outline one point in [11].

170
Y. Shiraki
3.1
Variable Transformation of the Normalization Constant of a
von Mises Distribution
For the maximum likelihood estimate of the von Mises distribution, let us con-
sider the following normalization constant:
F(θ) =
 2π
0
exp(θ1 cos(x) + θ2 sin(x))dx.
(11)
Here, by using the variable transformation t1 = cos(x), t2 = sin(x), we get the
following equation:
F(θ) =
 2π
0
exp(θ1t1 + θ2t2)dx.
(12)
By using the calculation for the hyperfunction h(t) = δ(t2
1+t2
2+1) i[11],pp.43-44),
we obtain the following equations for f = exp(θ1t1 + θ2t2)h:
(t2
1 + t2
2 −1)f = 0,
(13)
(t2(∂1 −θ1) −t1(∂2 −θ2)f = 0,
(14)
(ti(ti(∂i −θi) + tj(∂j −θj) + 2) −(∂i −θi))f = 0,
(15)
(∂θi −ti)f = 0.
(16)
3.2
Integration Algorithm for D-Modules
Using the four equations from (13) to (16) obtained above, we can calculate
a system of diﬀerential equations which the function F(θ) satisﬁes. When per-
forming this calculation, integration algorithms for D-modules are required [6,7].
Here, D = C < θ1, θ2, ∂θ1, ∂θ2 > represents the diﬀerential ring with polynomial
coeﬃcients with two-variables θ1, θ2.
[Calculation ﬂow of integration algorithm]
Let us consider the left-ideal I of the diﬀerential ring D. By using the ideal I,
we calculate the following integral-ideal:
(I + ∂1D + ∂2D) ∩C < θ1, θ2, ∂θ1, ∂θ2 > .
Let I represent an element of the integral-ideal. Then, by deﬁnition, I can be
written in the following form: l0 + ∂1l1 + ∂2l2, l0 ∈I, l1, l2 ∈D. By modifying
equation (14) using equation (16), we have the following:
∂1t2 + ∂2(−t2) −θ1t2 + θ2t1 ≡−θ1∂θ2 + θ2∂θ1 mod I + ∂1D + ∂2D.
(17)
Therefore, the following is one of the elements of the integral ideal:
−θ1∂θ2 + θ2∂θ1.
(18)
By modifying equation (15) using equation (16), we have the following:
∂1t2
1 + ∂2t1t2 −t1 −(∂1 −θ1) −θ1t2
2 −θ2t1t2.
(19)

Integration Algorithm
171
Here, by using@t2
i = ∂2
θi for modI + ∂1D + ∂2D, the following is one of the
elements of the integral ideal:
−∂θ1 + θ1 −θ1∂2
θ1 −θ2t1t2.
(20)
From equation (13), the following is one of the elements of the integral ideal:
∂2
θ1 + ∂2
θ2 −1.
(21)
Finally, we have the integral ideal: equations (18), (20), and (21).
4
Conclusion
In this article, we reviewed the application of the holonomic gradient descent
(HGD) method to a calculation of the maximum likelihood estimate (MLE). We
also examined the eﬀectiveness of using the Groebner basis of the calculation
algorithm for calculating the minimum value of the holonomic function. The
main topic described is the application of the holonomic gradient descent (HGD)
method to the maximum likelihood estimation method.
References
1. Hashiguchi, H., Numata, Y., Takayama, N., Takemura, A.: The holonomic gradient
method for the distribution function of the largest root of a Wishart matrix. Journal
of Multivariate Analysis 117, 296–312 (2013)
2. Hibi, T.: Harmony of Groebner basis and modern industrial society. Sugaku 63(3)
(July 2013) (in Japanese)
3. Nakayama, H., Nishiyama, K., Noro, M., Ohara, K., Sei, T., Takayama, N., Take-
mura, A.: Holonomic Gradient Descent and its Application to the Fisher-Bingham
Integral. Advances in Applied Mathematics 47(3), 639–658 (2011)
4. Mardia, K.V., Jupp, P.E.: Directional statistics. Wiley, New York (2000)
5. Milstein, G.N., Nussbaum, M.: Maximum likelihood estimate for nonparametric
signal in white noise by optimal control, WIAS-Preprint No. 596, Berlin (2000)
6. Oaku, T.: Algorithms for b-functions, restrictions, and algebraic local cohomology
groups of D-modules. Adv. Appl. Math. 19, 61–105 (1997)
7. Oaku, T., Shiraki, Y., Takayama, N.: Algebraic algorithms for D-modules and
numerical analysis. In: Li, Z.M., Sit, W. (eds.) Proceedings of the Sixth Asian
Symposium on Computer Mathematics, pp. 23–39. World scientiﬁc (2003)
8. Sato, M., Kawai, T., Kashiwara, M.: Microfunctions and Pseudo-diﬀerential Equa-
tions. Lecture Note in Mathematics, vol. 287, pp. 265–529. Springer (1973)
9. Saito, M., Sturmfels, B., Takayama, N.: Gr¨obner Deformations of Hypergeometric
Diﬀerential Equations. Springer (2000)
10. Takayama, N.: MAP estimation based on the holonomic gradient method, Harmony
of Groebner basis and modern industrial society. Jst Crest (September 8, 2011) (in
Japanese)
11. Takayama, N.: Integration algorithm for D-modules and estimation theory. Sugaku
Seminar, 41–46 (February 2012) (in Japanese)
12. Edited by JST CREST Hibi team, Gr¨obner Dojo, Kyoritsu (2011) (in Japanese)

On Dynamics of an Electromechanical System
Supported by Cylindrical Helical Spring
Damped by an Impact Damper
Marek Lampart and Jaroslav Zapomˇel
Department of Applied Mathematics & IT4Innovations,
VˇSB - Technical University of Ostrava, Czech Republic,
Department of Mechanics, VˇSB - Technical University of Ostrava, Czech Republic
{marek.lampart,jaroslav.zapomel}@vsb.cz
http://www.vsb.cz
Abstract. This paper focuses on vibrations attenuation of an electrome-
chanical system ﬂexibly coupled with a baseplate by cylindrical helical
springs and damped by an element that can work either in inertia or
impact regime. The model is constructed with three degrees of freedom
in the mechanical oscillating part, two translational and one rotational.
The system movement is described by three mutually coupled second-
order ordinary diﬀerential equations. The nonlinearities that signiﬁcantly
inﬂuence behavior of the system are impacts if the impact regime is set
on. Several important results were obtained by means of computational
simulations. Character of the system motion and amplitude of its oscil-
lations strongly depend on the width of clearances between the damping
element and the rotor frame. The damping element operating in inertia
regime must have precisely chosen mass and reduces eﬃciently the oscil-
lations amplitude only in a narrow frequency interval. In contrast, the
damping device working in impact regime attenuates vibrations of the
rotor frame in a wide range of the excitation frequency and the system
is showing periodic, quasi periodic and chaotic movements.
Keywords: electromechanical system, inertia damper, impact damper,
nonlinear stiﬀness, impacts, vibration attenuation.
1
Introduction
One of the most important mechanical phenomena are impacts of solid bodies.
The existence of such impacts can be noticed during a large number of natural
and technological processes. The impacts are characterized by short duration
body collisions, very large impact forces and by near sudden changes of the
system state parameters. The experience and theoretical analyses show that
the behavior of the impact systems is highly nonlinear, very sensitive to initial
conditions and instantaneous excitation eﬀects, leading frequently to irregular
vibrations and nearly unpredictable movements. The behavior of each system
where the body collisions take place is diﬀerent, and therefore, each of them
must be investigated individually.
c
⃝Springer International Publishing Switzerland 2015
173
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_19

174
M. Lampart and J. Zapomˇel
Due to the practical importance, a great deal of attention is focused on analysis
of vibro-impact systems, where the vibrations are governed by the momentum
transfer and mechanical energy dissipation through the body collisions. This is
utilized for impact dampers applied to attenuate high-amplitude oscillations,
such as those appearing in subharmonic, self-excited and chaotic vibrations.
Despite the fact that the problem of impacts is very old, the new possibili-
ties of its investigation enabled by eﬃcient computational simulations appeared
at the end of the 20th century. The authors in [2] studied the dynamic behav-
ior of an impact damper for vibration attenuation of an externally loaded and
self-excited cart that moves in one direction (see also [1]). The damping was
produced by impacts of a point body colliding with the cart walls. A number of
authors have dealt with the so-called non-ideal problem, which means that the
power of the source exciting the oscillator is limited. Into this category belongs
the article [10], who extended Chaterjee’s model with the cart by attaching a
rotor driven by a motor. Application of a non-ideal model to the gear rattling
dynamics was done by [7]. Later on, a new mechanical model with clearances for
a gear transmission was reported in [6]. Their model has time varying bound-
aries and impacts between two gears occur at diﬀerent locations. The horizontal
movement of a cart excited by a rotating particle and damped by an impact
damper formed by a point body bouncing on the cart walls was investigated by
[8]. A chaotic pattern of the oscillations was conﬁrmed by the computational
simulations. Thus, the inﬂuence of the impact body to the cart mass ratio on its
suppression was examined. The non-ideal impact problem completed by ﬂexible
stops was analyzed by [12]. The main objective was to study the character of
the vibration of the oscillator excited by an unbalanced rotor driven by a motor
of limited power. The mutual interaction of a mechanical and electrical system
was analyzed by [9]. The investigated unbalanced rotor of an electric motor was
attached to a cantilever beam. The amplitude of its bending vibration was lim-
ited by a ﬂexible stop. The performed simulations were aimed at studying the
character of the induced vibration and at ﬂuctuations of the current in the elec-
tric circuit. Vibrations of a rotor supported by bearings with nonlinear stiﬀness
and damping characteristics considering its impacts against the stationary part
were investigated by [11]. The shaft was represented by a beam like-body and
rotation of the disc was taken into account. The impacts were described both by
collisions of rigid bodies utilizing the Newton theory and by impacts with soft
stops. Vibration reduction of an electromechanical system by an impact damper
having rigid stops was investigated by [3]. Emphasis was put on observing the
inﬂuence of the inner impacts on the character and reduction of the system
vibration dependent on the geometric parameters.
The aim of this paper is an analysis of a system formed by a rotor and its
casing ﬂexibly coupled with a baseplate and of an impact body. This impact
body is separated from the casing by two gaps, lower and upper ones. The
rotor is driven by a motor of limited power and from this point of view the
investigated model system can be classiﬁed as non-ideal. A new contribution of
the presented work consists of investigating the system oscillations as a result of

Electromechanical System Damped by Impact Damper
175
a combined time variable loading caused by two sources, the rotor unbalance and
the baseplate vibrations, and in investigating the interaction between the motor
and its feeding electric circuit. Emphasis is put on observing the inﬂuence of the
inner impacts on the character and reduction of the system vibration dependent
on the width of the upper and lower clearances between the rotor frame and
the impact body. The investigated system is of great practical importance as
it represents a simpliﬁed model of a rotating machine, which is excited by a
ground vibration and unbalance of the rotating parts and damped by an impact
damper. Results of the performed simulations contribute to better understanding
of the dynamic behavior of such technological devices and of impact systems with
complicated loading, in general.
Behavior of a similar system, where the rotor frame was coupled with the
baseplate by plate springs having nonlinear stiﬀness, was analyzed in [5].
2
The Vibrating System
The considered system consists of a rotor (body 1, Figure 1), of its casing (body
2, Figure 1) and of a baseplate (body 3, Figure 1), with which the rotor casing
is coupled by a spring and damping element (body 4, Figure 1). The casing and
the baseplate can move in a vertical direction and the rotor can rotate and slide
together with its casing. Vibration of the baseplate and unbalance of the rotor
are the main sources of the casing excitation. To attenuate its oscillation an
impact damper was proposed. It consists of a housing ﬁxed to the rotor casing
(body 2, Figure 1) and of an impact element (body 4, Figure 1), which is coupled
with the housing by a linear spring. The impact body can move only in a vertical
direction and is separated from the housing by the lower and upper clearances
that limit its vibration amplitude. The rotor is loaded by an external moment
produced by a DC motor. Its behavior is described by a moment characteristic,
which enables implementation of the inﬂuence of the electric parameters of the
motor feeding circuit into the mathematical model of the investigated impact
system.
The task was to analyze the inﬂuence of the upper and lower clearances and
the mass of the impact body respectively on attenuation of the rotor frame
oscillation and character of its motion.
In the computational model all bodies are considered as absolutely rigid ex-
cept the contact areas between the impact element and the rotor frame. The
cylindrical helical springs coupling the rotor casing and the baseplate have lin-
ear characteristic
FK = k1Δ
(1)
where FK is the spring force, k1 is the stiﬀness parameter and Δ is the spring
deformation (compression or extension). The damper between the rotor frame
and the baseplate and the spring coupling the impact body with the damper
housing are linear. The Hertz theory has been accepted to describe the impacts.

176
M. Lampart and J. Zapomˇel
yz
yt
y
k1
b
ϕ
kt
4
3
2
1
eT
c1
c2
4
Fig. 1. Model of vibrating system
The nonlinear contact stiﬀness and damping were linearized in the expected
range of the contact deformation.
The investigated system has three mechanical degrees of freedom. Its instan-
taneous position is deﬁned by three generalized coordinates: Y - vertical dis-
placement of the rotor casing, Yt - vertical displacement of the impact body and
Φ - angular rotation of the rotor:
(m + mR)¨Y + mReT cos(Φ) ¨Φ = mReT ˙Φ2 sin(Φ) −b( ˙Y −˙yz)−
−k1(Y −yz) −kt(Y −Yt)−
−(m + mR)g −FI1 −FI2,
mt ¨Yt = FI1 + FI2 −kt(Yt −Y) −mtg,
(JRT + mRe2
T) ¨Φ + mReT cos(Φ)¨Y = −mRgeT cos(Φ) + MZ −kM ˙Φ
(2)
where ¨Y, ¨Yt and ¨Φ denote the second derivative of Y, Yt and Φ respectively, and
˙Y the ﬁrst derivative of Y with respect to time. Parameters of the system (2)
are summarized in Table 1.
It holds for the impact forces FI1 and FI2
FI1 =

−kc(Yt −Y −c1) −bc( ˙Yt −˙Y) if condition (4) is satisﬁed,
0
if condition (4) is not satisﬁed
(3)
where condition (4) is deﬁned by
Yt −Y −c1 > 0 and −kc(Yt −Y −c1) −bc( ˙Yt −˙Y) < 0
(4)
and
FI2 =

−kc(Yt −Y + c2) −bc( ˙Yt −˙Y) if condition (6) is satisﬁed,
0
if condition (6) is not satisﬁed
(5)
where condition (6) is deﬁned by
Yt −Y + c2 < 0 and −kc(Yt −Y + c2) −bc( ˙Yt −˙Y) > 0
(6)

Electromechanical System Damped by Impact Damper
177
Here kc, bc denote the contact stiﬀness and damping and c1, c2 stand for
the upper and lower clearances. Conditions (4) and (6) express that the contact
forces act only if the impact body and the rotor frame are in contact and that
they can be only compressive.
The vibration of the baseplate is deﬁned by the map
yz(t) = A (1 −e−αt) sin(ωt)
(7)
where A is the amplitude, α is the constant determining how fast the vibration of
the baseplate becomes a steady state and ω stands for the excitation frequency.
To perform the simulations, the following initial conditions were accepted
˙Y(0) = 0,
˙Yt(0) = 0,
˙Φ(0) = 0, Φ(0) = 3/2,
Y(0) = −m + mR + mt
k1
g,
Yt(0) = −m + mR + mt
k1
g −mtg
kt
.
This corresponds to the state when in the beginning no moment is applied on
the rotor of the electric motor, no contacts between the impact body and the
frame occur, the system is in rest, including the baseplate, and takes the stable
equilibrium position.
3
Main Results
It was shown by [3] that there is a resonance peak for the baseplate excitation
frequency of ω = 102 rad s−1 for the system parameters summarized in Table 1.
Next, parameters c1 and c2 are assumed to be equal for simplicity and together
with the mass of the impact element mt were taken as variables. In the following,
the situations are discussed in detail, dependent on clearances between the rotor
casing and the impact element, the mass of the impact element and the baseplate
excitation frequency.
In the linear system it would always be possible to ﬁnd the mass of the impact
element that would work then as an inertia damper and considerably attenuate
the vibration of the rotor frame without any impacts occurring. The amplitude
of vibrations of the rotor frame would be considerably attenuated and no impacts
would take place. In this cases such a damper can be eﬀective only in a small
range of excitation frequencies.
To extend the interval, the mass of the damping element would have to be
changed and such manipulation is not easy to accomplish from the technological
point of view.
On the contrary it is easy to ﬁnd a technological solution of the damping device
that would make it possible to actively change the clearance width dependent
on the excitation frequency to get maximum attenuation of the rotor vibrations.
This is suitable especially in the cases when the excitation frequency is not
constant but when it can slightly vary in a wider frequency interval.

178
M. Lampart and J. Zapomˇel
Table 1. Parameters of the system (2)
value quantity format
description
m
100
kg
mass of the damping body
mR
40
kg
mass of the rotor
mt
25
kg
mass of the impact element
k1
1.42 ×106 N m−1
linear stiﬀness coeﬃcient
JRT
5
kg m2
moment of inertia of the rotor
b
1.5 ×103 N s m−1
damping coeﬃcient of the suspension
kt
8 ×104
N m−1
coupling stiﬀness of the impact element
eT
2
mm
eccentricity of the rotor center of gravity
Φ
rad
rotation angle of the rotor
MZ
100
N m
starting moment
kM
8
N m s rad−1 negative of the motor characteristic slope
α
1
s−1
parameter of the baseplate excitation
ω
102
rad s−1
baseplate excitation frequency
A
1
mm
amplitude of yz
kc
4×107
N m−1
contact stiﬀness
bc
3×103
N s m−1
coeﬃcient of contact damping
90
95
100
105
110
115
120
0
2
4
6
8
10
12
14
16
18
20
t [rad/s]
ï [mm]
 
 
mt=7 [kg]
mt=8 [kg]
mt=9 [kg]
Fig. 2. Peak-to-peak vibrations amplitudes of the rotor frame dependent on the exci-
tation frequency for the mass of the impact element mt from 7 kg to 9 kg. There are
no impacts in all these cases.

Electromechanical System Damped by Impact Damper
179
Figure 3 shows that it is possible to change the clearance width dependent
on the excitation frequency intentionally to get maximal attenuation. More pre-
cisely, the simulations show (see Figure 2) that for the mass of the impact el-
ement mt = 8 kg (corresponding to the resonance peak) the range of the exci-
tation frequency is only (104.5, 108.5) where the attenuation is remarkable (i.e.
maxY −minY < 2 mm.) In this situation the inertia damper works in the range
of 6 rad s−1. On the other hand, in Figure 3 it is conﬁrmed that for the mass
of the impact element mt = 25 kg and the controlled clearance width the range
of excitation frequency overlaps 60 rad s−1. That is if the damping element is in
active control, the clearance is changed dependent on the excitation frequency,
attenuation is meaningful for the excitation frequency in the interval (87, 151).
90
100
110
120
130
140
150
160
0
2
4
6
7 mm
6 mm
5 mm
5 mm
4 mm
3 mm
3 mm
2 mm
2 mm
2 mm
2 mm
1 mm
1 mm
1 mm
 max Y − minY [mm]
ω [rad/s]
Fig. 3. Optimal choice of the clearance c1 = c2 for the maximal attenuation of vibra-
tions of the rotor body. The attenuation is signiﬁcant for the range of the excitation
frequency ω from 87 rad s−1 to 151 rad s−1 and the mass of the impact element
mt = 25 kg.
For simulations of the angular frequency ω = 100 rad s−1 and the upper and
lower clearances between the rotor casing and the impact body c1 = c2 = 6 mm,
ω = 110 rad s−1 and c1 = c2 = 5 mm, ω = 150 rad s−1 and c1 = c2 = 1 mm,
respectively the Fourier spectra and phase trajectories show that the movement is
formed only by components having the basic, sub and ultra harmonic frequencies
as shown on Figures 4 and 5 which corresponds to periodic movements.
Fourier spectrum of the response referred to the excitation frequency ω =
110 rad s−1 (Figures 6) shows the side bands around the frequency peaks re-
lated to the principal and ultra harmonic components of the motion that is char-
acteristic for quasi-periodic oscillations. The corresponding phase trajectory is
depicted in Figure 6.
Finally, Fourier spectrum of the motion excited by the baseplate vibrating
with the frequency of ω = 185 rad s−1 (Figure 7) has a band character which
conﬁrms a chaotic pattern of the system oscillations. The high degree of ir-
regularity of the motion is also evident from the phase trajectory depicted in
Figure 7.
The above described situations are also conﬁrmed by the Bifurcation diagram
in Figure 8 with respect to the angular frequency ω.

180
M. Lampart and J. Zapomˇel
−4
−2
0
2
4
6
8
10
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
˙Y- ˙Yt [m s-1]
Y-Yt [mm]
50
100
150
200
250
300
350
400
0
5
10
15
20
25
30
35
40
45
50
FFT(Y-Yt) [m]
Angular frequency [rad s -1]
Fig. 4. Phase portrait (left), Y −Yt versus ˙Y −˙Yt and Fourier spectra (right) for the
angular frequency ω = 100 rad s−1 and the upper and lower clearances between the
rotor casing and the impact body c1 = c2 = 6 mm
1
1.5
2
2.5
3
3.5
4
4.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
˙Y- ˙Yt [m s-1]
Y-Yt [mm]
50
100
150
200
250
300
350
400
0
5
10
15
20
25
30
35
40
45
50
FFT(Y-Yt) [m]
Angular frequency [rad s -1]
Fig. 5. Phase portrait (left), Y −Yt versus ˙Y −˙Yt and Fourier spectra (right) for the
angular frequency ω = 150 rad s−1 and the upper and lower clearances between the
rotor casing and the impact body c1 = c2 = 1 mm
−4
−2
0
2
4
6
8
10
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
˙Y- ˙Yt [m s-1]
Y-Yt [mm]
50
100
150
200
250
300
350
400
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
FFT(Y-Yt) [m]
Angular frequency [rad s -1]
Fig. 6. Phase portrait (left), Y −Yt versus ˙Y −˙Yt and Fourier spectra (right) for the
angular frequency ω = 110 rad s−1 and the upper and lower clearances between the
rotor casing and the impact body c1 = c2 = 5 mm

Electromechanical System Damped by Impact Damper
181
1
1.5
2
2.5
3
3.5
4
4.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
˙Y- ˙Yt [m s-1]
Y-Yt [mm]
50
100
150
200
250
300
350
400
0
5
10
15
20
25
30
35
40
45
50
FFT(Y-Yt) [m]
Angular frequency [rad s -1]
Fig. 7. Phase portrait (left), Y −Yt versus ˙Y −˙Yt and Fourier spectra (right) for the
angular frequency ω = 185 rad s−1 and the upper and lower clearances between the
rotor casing and the impact body c1 = c2 = 1 mm
100
110
120
130
140
150
160
170
180
190
200
−8
−6
−4
−2
0
2
4
ω [rad/s]
Y-Yt [mm]
Fig. 8. Bifurcation diagram with respect to the angular frequency ω
4
Conclusions
In this paper, it was developed and analyzed a new electromechanical system
damped by impact element with soft stops dependent on parameters, namely
the weight of the impact element, the clearance between the impact body and
rotor casing and ﬁnally the excitation frequency. This model was inspired by
real frequently occurring technological problems when electromechanical rotat-
ing machines are excited by a combined loading produced by the rotor unbalance
and ground vibrations. The equations of motions were solved numerically by the
explicit Runge-Kutta method. The computational simulations showed that the
vibration of the baseplate played a key role here and proved that application of
the impact body arrived at a signiﬁcant decrease of vibration amplitude of the
rotor frame.
It was observed that for given parameters of the model, the damping element
can work as an inertia (there are no impacts) or as an impact damper (impacts
occur). Both situations were investigated and commented on. The ranges of
parameters for which the attenuation is meaningful were detected. In the case of
the inertia damper, the range of the excitation frequency is very narrow. On the
other hand, it is quite wider in the case of the impact damping device. Finally, if
the damping element is in active control, the clearance is changed dependent on
the excitation frequency, attenuation is meaningful in the interval which is ten

182
M. Lampart and J. Zapomˇel
times wider then those in the case of the inertia damper. It was also proved by
simulations that movements corresponding to the inertia damper show periodic
pattern.
Acknowledgments. This work was supported by the European Regional De-
velopment
Fund
in
the
IT4Innovations
Centre
of
Excellence
Project
(CZ.1.05/1.1.00/02.0070). The work was also supported by the Grant Agency of
the Czech Republic, Grant No. P201/10/0887.
References
1. Chaterjee, A.K., Mallik, A., Ghosh, A.: Impact dampers for controlling self-excited
oscillations. Journal of Sound and Vibration 193, 1003–1014 (1995)
2. Chaterjee, A.K., Mallik, A., Ghosh, A.: On impact dampers for non-linear vibration
systems. Journal of Sound and Vibration 187, 403–420 (1995)
3. Lampart, M., Zapomˇel, J.: Dynamics of the electromechanical system with impact
element. Journal of Sound and Vibration 332, 701–713 (2013)
4. Lampart, M., Zapomˇel, J.: Dynamical properties of the electromechanical system
damped by impact element with soft stops. International Journal of Applied Me-
chanics 6, 1450016 (2014)
5. Lampart, M., Zapomˇel, J.: Dynamics and eﬃciency of an impact damper. In: Pro-
ceedings of the NOSTRADAMUS 2014 Conference (to appear, 2014)
6. Luo, A.C.J., O’ Connor, D.: Periodic motions with impacting chatter and stick in
a gear transmission system. ASME Journal of Vibration and Acoustics 131 (2009)
7. de Souza, S.L.T., Caldas, I.L., Balthazar, J.M., Brasil, R.M.L.R.F.: Analysis of
regular and irregular dynamics of a non ideal gear rattling problem. Journal of
Brayilian Society of Mechanical Sciencies 24, 111–114 (2002)
8. de Souza, S.L.T., Caldas, I.L., Viana, R.L., Balthazar, J.M., Brasil, R.M.L.R.F.:
Impact dampers for controlling chaos in systems with limited power supply. Journal
of Sound and Vibration 279, 955–967 (2005)
9. P˚ust, L.: Electro-mechanical impact system excited by a source of limited power.
Engineering Mechanics 6, 391–400 (2008)
10. Warminski, J., Balthazar, J.M., Brasil, R.M.L.R.F.: Vibrations of a non-ideal para-
metrically and self-excited model. Journal of Sound and Vibration 245, 363–374
(2001)
11. Zapomˇel, J., Fox, C.H.J., Malenovsk´y, E.: Numerical investigation of a rotor system
with disc-housing impact. Journal of Sound and Vibration 243, 215–240 (2001)
12. Zukovic, M., Cveticanim, L.: Chaos in non-ideal mechanical system with clearance.
Journal of Vibration and Control 8, 1229–1246 (2009)

 
© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
183
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_20 
 
Petri Net Models of Purposeful Complex Dynamic 
Systems 
Felipe Lara-Rosano 
Centro de Ciencias Aplicadas y Desarrollo Tecnologico 
Universidad Nacional Autonoma de Mexico 
Mexico-City, Mexico 
flararosano@gmail.com 
Abstract. In this paper a Petri nets modeling approach for purposeful complex 
dynamic systems is proposed. This approach is based on considering a 
hierarchy of interrelated systemic goals/functions and emergent outcomes. The 
system model is a Petri net with two types of elements: goals/functions whose 
accomplishment is desirable and emergent outcomes that represent systemic 
properties. 
Keywords: Purposeful complex dynamic systems, Petri Net Models, Goals and 
emergent outcomes modeling, Synthetic micro-analysis of purposeful complex 
systems. 
1 
Introduction 
Sources of complexity in the social and behavioral sciences are not only the mechanistic 
interrelated, asinchronous, non-linearly coupled dynamic elements we find in the human 
and the social world but the plentiful positive and negative feedback loops (Buckley 
1968, Modern Systems Research for the Behavioral Scientist, pp xxiv), and a purposeful 
active behavior of dynamic elements that cannot be captured through common causality 
(Rosenblueth, Wiener & Bigelow 1943, Behavior, Purpose and Teleology, pp 5). 
This fact had already been  recognized by the economist Ludwig von Mises (1949, 
Human Action: A Treatise on Economics pp 11):   
 
“Human action is purposeful behavior. Or we may say: Action is will 
put into operation and transformed into an agency, is aiming at ends and 
goals, is the ego's meaningful response to stimuli and to the conditions 
of its environment, is a person's conscious adjustment to the state of the 
universe that determines his life. Such paraphrases may clarify the 
definition given and prevent possible misinterpretations. But the 
definition itself is adequate and does not need complement of 
commentary.” 

184 
F. Lara-Rosano 
 
According to Rosenblueth, Wiener and Bigelow (1950, Purposeful and 
Non.Purposeful Behavior, pp 32) the analysis of behavior in social and behavioral 
sciences requires to introduce a teleological method of study: 
It may be mentioned parenthetically that the theory of games is a 
chapter in the study of two- or more-way purposeful activity ⁄the 
adoption of a teleological approach simplifies the analysis of goal-
directed behavior and enlarges the scope of this analysis. This 
methodological approach does not imply the philosophical belief in 
final causes.‰ 
Richard Taylor (1950, Purposeful and Non-Purposeful Behavior: A Rejoinder, pp 
331) discusses the paper by Rosenblueth et al (1943), critizising some of the 
cybernetic conceptions but stating the conditions to be fulfilled to regard any behavior 
pattern as purposive: 
„I now suggest the following as necessary and sufficient conditions to 
be fulfilled, or to be assumed to be fulfilled, in order appropriately to 
regard any given behavior pattern as purposive: There must be, on the 
part of the behaving entity, i.e., the agent: (a) a desire, whether actually 
felt or not for some object, event, or state of affairs as yet future; (b) the 
belief, wether tacit or explicit, that a given behavioral sequence will be 
efficacious as a means to the realization of that object, event or state of 
affairs; and c) the behavior pattern in question. Less precisely, this 
means that to say of a given behavior pattern that is purposeful, is to say 
that the entity exhibiting that behavior desires some goal, and is 
behaving in a manner it believes appropriate to the attainment of it. 
In 1972 Russell L. Ackoff  and Fred Emery, pionners and influential thinkers on the 
applications of the systems approach to organization diagnostics and planning in his 
book 1972, On Purposeful Systems (pp 4) state: 
In this book we take a holistic view of human behavior and hence 
necesarily a functional, teleological, or purposeful view. Yet, following 
Singer (1924, 1959) and Rosenblueth and Wiener (1943, 1950) and 
Sommerhoff (1950), we try to make all the functional concepts 
employed as objective, as measurable, as capable of use in 
experimentation as any structural concepts produced by the mechanistic 
(so-called behavioristic) view of human behavior. 
This book develops the systems theory of purposeful systems as a new foundation for 
the behavioral sciences: 
This book provides a completely new foundation for the behavioral 
sciences; a new way of looking at human and social behavior – as  a 
system opf purposefuil (teleological) events. It uses a systems 
theoretical approach for the study of these phenomena, and thus 

 
Petri Net Models of Purposeful Complex Dynamic Systems 
185 
 
illuminates and extends general system theory. It treats individual 
purposeful behavior, interactions of purposeful individuals, and 
purposeful social groups, in a way that is complementary to the 
traditional mechanistic concept of the world; it makes teleology and 
mechanism compatible. Only the systems approach, however, can deal 
with the full richness and complexity of human behavior. (Ackoff and 
Emery 1972, On Purposeful Systems 1972, pp Cover 2) 
In his paper Towards a System of Systems Concepts (1971, pp 667) Russell L. Ackoff 
defines a set of systems concepts including the basic definitions for the teleological 
analysis of complex systems. According to this school of thought the goal of a 
purposeful system in a particular situation is a preferred outcome that can be obtained 
within a specified time period. The objective of a purposeful system in a particular 
situation is a preferred outcome that cannot be obtained within a specified period but 
which can be obtained over a longer time period. Pursuit of an objective requires an 
ability to change goals once a goal has been obtained. This is why such pursuit is 
possible only for a purposeful system. An ideal is an objective which cannot be 
obtained in any time period but which can be approached without limit. For example, 
an ideal of scientists is errorless observations. The amount of observer error can be 
reduced without limit but can never be reduced to zero. Omniscience is another such 
ideal. Just as goals can be ordered with respect to objectives, objectives can be 
ordered with respect to ideals.  
Consider a set of possible outcomes ordered along one or more scales 
(e.g., increasing speeds of travel). Then each outcome is closer to the 
final one than those which precede it. Each of these outcomes can be a 
goal in some time period after the 'preceding' goal has been obtained, 
leading eventually to attainment of the last outcome, the objective. For 
example, a high-school freshman's goal in his first year is to be 
promoted to his second (sophomore) year. Passing his second year is a 
subsequent goal. And so on to graduation, which is his objective. 
Then Ackoff defines the different types of teleological systems (Ackoff 1971, pp 666): 
A goal-seeking system is one that can respond differently to one or 
more different external or internal events in one or more different 
external or internal states and that can respond differently to a 
particular event in an unchanging environment until it produces a 
particular state (outcome). Production of this state is its goal. Thus 
such a system has a choice of behavior. 
A multi-goal-seeking system is one that is goal-seeking in each of 
two or more different (initial) external or internal states, and which 
seeks different goals in at least two different states, the goal being 
determined by the initial state. 
A purposive system is a multi-goal-seeking system the different goals 
of which have a common property called the system's purpose.  These 

186 
F. Lara-Rosano 
 
types of system can pursue different goals but they do not select the 
goal to be pursued. The goal is determined by the initiating event. But 
such a system does choose the means by which to pursue its goals. 
A purposeful system is one which can change its goals under constant 
conditions; it selects ends as well as means and thus displays will. 
Human beings are the most familiar examples of such systems.  
An ideal-seeking system is a purposeful system which, on attainment of 
any of its goals or objectives, then seeks another goal and objective 
which more closely approximates its ideal. An ideal-seeking system is 
thus one which has a concept of 'perfection' or the 'ultimately desirable' 
and pursues it systematically; that is, in interrelated steps. 
The purposeful systems approach of Ackoff has been followed and adopted by the 
most organizational theorists. According to Marion (1999, The Edge of Organization: 
Chaos and Complexity Theories of Formal Social Systems, pp 115): 
Organizational theorists, and social theorists in general, don't share 
biology's consternation over conventional teleology. We embrace it for 
a number of reasons, one of which is because we want to feel in control 
of our organizations. There are entire models of organization built 
around the assumption that leaders determine the destiny of their firms 
and institutions”.   
This point of view is also shared by social sciences and humanities (Mainzer 2007, 
Thinking in Complexity: The Computational Dynamics of Matter, Mind and Mankind, 
pp 373): 
In the social sciences and humanities one usually distinguishes strictly 
between biological evolution and the history of human cultures. The 
main reason is that the development of nations and cultures is obviously 
guided by the intentional behavior of humans with their attitudes, 
emotions, plans, and ideals, while systems in biological evolution are 
assumed to be driven by unintended self-organization. From a 
microscopic view, we observe single human individuals with their 
intentions and desires. Even in biological systems like animal ecologies 
there are individuals with intentional behavior of some degree. 
Finally we must remark that contemporary Social Complexity Science has 
incorporated purposefulness and intent in the study of social and human dynamics. In 
fact Beautement & Broenner (2011, Complexity Demystified: A guide for 
practitioners pp 70) ask: 
what is the significance of purpose and intent in relation to dynamic 
phenomena? Well, it's a simple one it provides a way of disturbing 
conditions to either move the zone or trigger a phase change such that a 
new order, and therefore new zones, can arise. Some of the givens and 
realities that are relevant here include the following:  

 
Petri Net Models of Purposeful Complex Dynamic Systems 
187 
 
A. Purposeful Change Does Not Come by Chance. Complexity science 
provides a way of identifying the class/type of phenomena one is 
dealing with and so provides the basis, through purpose and intent, for 
selecting from a range of options for influencing, shaping and seeding 
the type of emergence so that what one anticipates is more likely to 
come about. Hence, putting complexity to work requires a radically 
different emphasis from reductionist approaches to an emphasis on an 
awareness of the tensions and drivers of change in real-world 
phenomena and on having fitness and readiness to adjust to external 
changes quickly without major internal dislocation. With complexity 
science having 'explained' the mechanisms by which complex 
phenomena come about, practitioners can take these insights, add 
intentionality, and be more aware of how and why they can affect 
change by getting a better feel of how to tune these properties. 
B. Intentions are Part of the Context. Purpose and intent are considered 
by complexity science to be modifiers of the properties of the 
components, or of things that would affect their interactions  such as the 
degree of people's involvement or withdrawal and these will change 
outcomes. So, part of putting complexity to work, where everything 
seems different depending on where you stand in relation to the 
situation, is to be able to appreciate issues from the points-ofview of the 
other actors who have their own purpose and intent….” 
D. Modelling Intentions needs to be done Appropriately. Though 
people use models to inform their thinking, another reality is the limited 
ability of models to accommodate issues related to purpose and intent in 
practice within the models. 
After this short review over the inclusion of teleological methodology in complexity 
studies in the social, behavioral sciences and humanities in this paper we present a 
Petri net approach to model purposeful complex dynamic social systems and their 
interactions with emergent outcomes. 
2 
Synthetic Microanalysis of Purposeful Complex Dynamic 
Systems  
Synthetic Microanalysis is a general theoretical and methodological framework in 
which concepts describing system elements and composite systems are related to 
explain the function, structure and behavior of physical, biological, psychological and 
social complex systems (Auyang 1998) employing, in a complementary way, two 
consecutive procedures: the systemic decomposition method and the systemic 
composition method.  
The systemic decomposition method begins with a holistic view of the system and 
its environment, to capture a global view of the system, lowering its complexity. Then 
it decomposes the system into a hierarchy of successive functional subsystem levels 
down to find and define the basic components of the system, and detecting all 

188 
F. Lara-Rosano 
 
emergent properties appearing at each level. This operation firstly identifies the supra-
system to which the focal system belongs and the functions that this focal system 
fulfills in the supra-system. Then it identifies from the environment the peer systems 
belonging to the same supra-system and the interrelations that have with the focal 
system. The successive application of this method downward the systemic hierarchy 
allows to reach the basic elements, and explain their interrelationships. In the case of 
social systems, all these elements may be also social systems except the basic ones, 
which can be individuals. Once the hierarchy of systems and subsystems is identified, 
these elements must be analyzed by associating them with the system dynamics 
through the application of disciplinary and theoretical frameworks to the data.  
In order to analyze the system dynamics is necessary to identify the goals that 
motivate the goal-oriented, purposive and purposeful system components. Firstly 
identify the highest priority goals/functions of the overall system and the conditions to 
be fulfilled by its first level subsystems in order to accomplish those goals/functions. 
These conditions define the goals and/or functions to be accomplished by these first 
level subsystems (strategy level). Then, for each of these first level goals/functions 
identify the conditions to be fulfilled by the second level subsystems (tactical level) to 
accomplish the corresponding first level goal/function. The process is repeated until 
all the levels up to the basic one are covered. The identified goals/functions at each 
level must be necessary and sufficient to produce as emergent outcomes the fulfilling 
of the goals of the next higher level. 
The systemic composition method allows then interrelating the component 
goals/functions at the various levels, starting with the basic level, to find the actions 
responsible for producing the emergent outcomes at each level up to the overall 
goals/functions of the whole system whose attainment guarantees the proper system 
behavior.  
In this paper a Petri net approach is proposed to model purposeful complex 
dynamic systems that can be used to make inferences and to solve problems related to 
the purposeful complex system dynamics. 
3 
A Petri Net Model for Purposeful Complex Dynamic Systems 
In order to model purposeful complex dynamic systems, we give the model the 
structure of a Petri net (Lara-Rosano 1994 pp126).  Formally a Petri net N is a four-
tuple, N = (P,T,I,O) where P = {p1, p2,...,pn} is a finite set of places,  n ≥ 0.  T = {t1, 
t2,...,tm} is a finite set of transitions, m ≥ 0.   The set of places and the set of 
transitions are disjoint,  P ∩ T = ∅.  I: T → P∞  is the input function, a mapping from 
transitions to sets of places.   O: T → P∞ is the output function, a mapping from 
transitions to sets of places.  A place pi is an input place of a transition tj if  pi ∈ I(tj);  
pi is an output place of a transition tj if pi ∈ O(tj).  A Petri net graph is a graphical 
representation of a Petri net and therefore has two types of nodes: places and 
transitions.  A circle O represents a place and a bar | represents a transition.  Some 
directed arcs connect the places with the transitions and some other directed arcs 
connect the transitions with the places.  There are no arcs connecting places with 

 
Petri Net Models of Purposeful Complex Dynamic Systems 
189 
 
places or transitions with transitions, thus the graph representing the system is a 
bipartite graph. 
To model a purposeful complex system as a Petri net we define following structure: 
a) There are two kinds of system elements: the system goals/functions whose 
accomplishment define the state of the system and the emergent outcomes, that are 
emergent properties appearing in the system as a result of the synergy of a set of 
accomplished goals/functions. Production of emergent outcomes depend from 
preconditions defined in terms of the state of the system.  A precondition is then a 
logical description of the state of the system that induces an emergent outcome.  
Emergent outcomes have also an impact over the accomplishment of one or more 
goals/functions, affecting the state of the system. These changes induced by the 
emergent outcomes are called the postconditions of the emergent outcome. Thus 
between both kind of elements exist causal relationships represented by directed arcs. 
System goals/functions are defined as places of the Petri net model (PNM), and 
represented as circles. Emergent outcomes are defined as transitions of the PNM and 
represented as bars. The inputs of a transition are the preconditions of the 
corresponding emergent outcome; the outputs are the postconditions. 
b) Because of the bipartite nature of a Petri net, the direct causal antecedents of 
accomplished goals/functions are always emergent outcomes of the system or external 
events provided by the environment. In the same way the direct causal antecedents of 
emergent outcomes are always accomplished goals/functions or external inputs 
provided by the environment.  
c) When the accomplishment of the goal/function fulfills the precondition of an 
emergent outcome, a token is assigned to the goal/function. A marking μ is an 
assignment of tokens to the goals/functions  of the system.  If an accomplished 
goal/function is causal antecedent of several different emergent outcomes, with different 
sets of preconditions, several tokens are assigned to the accomplished goal/function, 
each token with a different color corresponding to every emergent outcome. 
d) Therefore the holding of a precondition is represented by a token in the place 
corresponding to the precondition.  If all preconditions of an emergent outcome have 
tokens, the emergent outcome is enabled and occurs, firing the corresponding 
transition. 
e) The system interacts with the environment through its inputs and outputs.  Inputs 
are considered external events from the environment, with no antecedent, that 
determine the accomplishment of one or more goals/functions inside the model, thus 
changing the state of the system.  These input events constitute the input layer of the 
Petri net.  Therefore the input layer represents scenario events or decisions taken by a 
decision maker.  This allows to incorporate policy simulation in the model. Outputs 
are accomplishment of goals/functions or emergent outcomes giving the response of 
the system to the environment.  
f) The accomplishment of a goal/function is then a product of the synergy of 
several specific lower level emergent outcomes acting together. The directed link eij 
from an emergent outcome i to a higher level goal/function accomplishment j 
indicates that emergent outcome i contributes to goal/function j.  

190 
F. Lara-Rosano 
 
g) In the same way the production of an emergent outcome is then the product of 
the synergy of specific goals/functions acting together. The directed link eji from a 
goal/function j to an emergent outcome i indicates that goal/function j contributes to 
emergent outcome i.  
h) The firing of an emergent outcome is then the result of enabling the 
corresponding transition through the fulfilling of all its preconditions, that is. when all 
its precedent goals/functions have the corresponding colored token.   
i) From the state vector and the emergent outcomes at time t, we may estimate the 
state vector at time t + Δt, element by element as following: 
• The input external events occur or not occur according to problem conditions. 
• The goals/functions are accomplished or not according to the accomplishment of 
the required lower level emergent outcomes and input environmental events. 
j) If all preconditions of an emergent outcome have the corresponding color tokens, 
the emergent outcome is enabled and occurs, firing the corresponding transition.  If 
the emergent outcome occurs, its postconditions modify the goals/functions 
accomplishments, other preconditions may be fulfilled, new tokens may be assigned, 
new emergent outcomes enabled, new transitions fired and so on, until no new 
emergent outcomes are enabled and the execution of the model halts and the 
accomplishment or not of output goals/functions is determined.   
One of the advantages of this approach is that different models with different 
number or kind of elements can be combined in a composed Petri net by introducing 
new goals/functions and/or new emergent outcomes and their relationships. This 
permits to gather knowlege from several experts and/or combine different knowledge 
domains. 
Validation of these purposeful dymanic models are done by simulation of historical 
situations with the model. The model outcomes are then compared with the available 
historical evidence and the discrepancies suggest changes in the model elements and 
its causal connections.  
4 
Conclusions 
In this paper a Petri net modeling approach for purposeful complex dynamic systems 
was proposed.  This approach is based on considering a hierarchy of interrelated 
systemic goals/functions and emergent outcomes. The system model has 
goals/functions whose accomplishment is desirable and emergent outcomes that 
represent system properties.  Goals/functions are interpreted as places in a Petri net 
whose accomplishment may determine the firing of emergent outcomes as transitions 
when the corresponding preconditions at the places are fulfilled. As a simulation tool, 
the model may combine subjective or intuitive concepts and expert opinions with hard 
data. The emphasis is placed on finding the significance of structural relations and 
dynamic behavior, rather than on producing a numerical prediction.  Application of 
this model to real problems and refinement to include thresholds and uncertainty in 
the causal links are under way. 
Acknowledgement. This work was supported in part by the project Conacyt 152008. 

 
Petri Net Models of Purposeful Complex Dynamic Systems 
191 
 
References 
Ackoff, R.L.: Towards a System of Systems Concepts. Management Science 17(11), 661–671 
(1971) 
Ackoff, R.L., Emery, F.E.: On Purposeful Systems. Aldine Atherton, Chicago (1972) 
Auyang, S.Y.: Foundations of Complex-System Theories in Economics. In: Evolutionary 
Biology and Statistical Physics. Cambridge University Press, Cambridge (1988) 
Beautement, P., Broenner, C.: Complexity Demystified: A Guide for Practitioners. Triarchy 
Press, Devon (2011) 
Buckley, W.: Modern Systems Research for the Behavioral Scientist. Aldine Publishing Co., 
Chicago (1968) 
Churchman, C.W., Ackoff, R.L.: Purposive Behavior and Cybernetics. Social Forces 29(1),  
32–39 (1950) 
Lara-Rosano, F.: Fuzzy Causal Impact Connectionist Models of Dynamic Systems. In: Lasker, 
G.E. (ed.) Advances in Systems Studies, vol. II, pp. 22–26. IIAS, Windsor (1995) 
Lara-Rosano, F.: Fuzzy Causal Modeling of Complex Systems through Petri Paradigm and 
Neural Nets. In: Lasker, G.E. (ed.) Advances in Artificial Intelligence and Engineering 
Cybernetics, vol. III, pp. 125–129. IIAS, Windsor (1994) 
Mainzer, K.: Thinking in Complexity: The Computational Dynamics of Matter. Springer, 
Berlin (1994) 
Marion, R.: The Edge of Organization: Chaos and Complexity Theories of Formal Social 
Systems. Sage Publications, Thousand Oaks (1999) 
Peterson, J.L.: Petri Net Theory and the Modeling of Systems. Prentice-Hall, Englewood Cliffs 
(1981) 
Rosenblueth, A., Wiener, N.: Purposeful and Non.Purposeful Behavior. Philosophy of 
Science 17, 318–326 (1950) 
Rosenblueth, A., Wiener, N., Bigelow, J.: Behavior, Purpose and Teleology. Philosophy of 
Science 10, 18–24 (1943) 
Singer, E.A.: Mind as Behavior. R. G. Adams, Columbus (1924) 
Singer, E.A.: Experience and Refletion. University of Pennsylvania Press, Philadelphia (1959) 
Sommerhof, G.: Analitical Biology, ch. II. Oxford University Press, Oxford (1950) 
Taylor, R.: Purposeful and Non-Purposeful Behavior: A Rejoinder. Philosophy of Science 17, 
327–332 (1950) 
Von Mises, L.: Human Action: A Treatise on Economics. Von Mises Institute, Auburn (1949), 
Scholar Edition 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Part II 
 
Evolutionary Computations 
 
 
 
 
 
 
 
 
 
 
 
 

Does Evolutionary Dynamics Need Randomness,
Complexity or Determinism?
Ivan Zelinka1 and Roman Senkerik2
1 VSB-Technical University of Ostrava, 17. listopadu 15 708 33, Ostrava-Poruba,
Czech Republic
ivan.zelinka@vsb.cz
2 Faculty of Applied Informatics, Tomas Bata University in Zlin
senkerik@fai.utb.cz
Abstract. Inherent part of evolutionary algorithms that are based on
Darwin theory of evolution and Mendel theory of genetic heritage, are
random processes. In our as well as another researcher papers is suc-
cessfully discussed possibility to replace pseudorandom number gener-
ators by deterministic chaos generator, generating chaos, and then by
n periodical series based on deterministic chaos generators and ﬁnally
also fully deterministic periodical functions. In all cases was observed
that pseudorandom generators can be successfully replaced by chaotic
or deterministic generators and thus question whether evolutionary al-
gorithms needs randomness, complexity or determinism and we propose
novel way how to understand, analyze and control complex dynamics of
evolutionary algorithms.
1
Introduction
An important part of evolutionary techniques that are based on Darwin theory
of evolution and Mendel theory of genetic heritage, are so called pseudorandom
processes. In literature as for example ([1]-[5]), ([9] - [12], [8] and [7]) amongst
the others is successfully discussed possibility to replace pseudorandom num-
ber generators (PRNG) by deterministic chaos generator (DCHG), generating
chaos (thus very complex behavior), and then by n periodical series based on
deterministic chaos generators, [6], [19], [20], [21]. Periodic behavior of chaotic
systems, in this case of logistic equation (1), is visualized for better imagination
in Fig. 1 - 2.
Above mentioned papers papers are discussing use of deterministic chaos in-
side evolutionary algorithm instead of PRNG investigating relations between
chaos and randomness or the latest or using chaos with EAs in applications,
amongst the others. Because all those previous experiments were successful or
highly comparable with classical evolutionary algorithms powered by classical
pseudorandom number generators, another experiments [30] used successfully
classical deterministic periodical generator. In all cases was observed that pseu-
dorandom generators can be successfully replaced by chaotic or deterministic
c⃝Springer International Publishing Switzerland 2015
195
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_21

196
I. Zelinka and R. Senkerik
generators and thus question whether evolutionary algorithms needs random-
ness, complexity or determinism.
For long time various PRNG were used inside evolutionary algorithms. It is
well known that an ideal random number generator shall be inﬁnite, aperiodic,
uniform, uncorrelated, and computationally eﬃcient. It means that ideal gener-
ator produces an endless sequence of numbers without any periodicity, and each
number in the sequence has an equal probability of being generated. If seed is
not known, then also it is impossible to predict successive terms.
During last few years deterministic chaos systems (DCHG) are used instead
of PRNG. As was demonstrated in [1]-[5], very often is performance of EAs using
DCHG better or fully comparable with EAs using PRNGs. See for example [1].
Used EAs (we do not discuss here special cases, modiﬁed for special experiments)
of diﬀerent kind like for example genetic algorithms [17], diﬀerential evolution
[14], particle swarm [18], [23], [24], SOMA [13], scatter search [15], evolutionary
strategies [16], etc... do not analyze whether used pseudo-random numbers are
really random (better to say ”well pseudorandom”) one and do not use infor-
mation about its randomness. Pseudorandom numbers are only simply used to
select individuals, make mutation, etc. On the other side, as demonstrated in
mentioned references, EAs with DCHG gives the same or often better perfor-
mance. Diﬀerence between series from PRNGs and DCHG is that in the case
of DCHG one can less-more easily reconstruct/calculate whole series generated
by DCHS from arbitrary time series point (of course with very high precision
only, otherwise trajectories will diverge soon) or/and reconstruct equations that
generate observed chaotic series. Because DCHG can generate periodical series
(thanks to ﬁnal numerical precision - numbers behind decimal point, [6], [19],
[20], it is question what will be EAs performance based on classical mathemati-
cal periodical functions (CPF) and whether it is comparable with performance
of classical EAs with PRNG.
xn+1 = Axn (1 −xn)
(1)
Fig. 1. Time series of period 36 (precision
= 4) based on Eq. 1 for A = 4
Fig. 2. Periodic behavior of logistic equa-
tion
with
numerical
precision
1
and
xstart = 0.45

Does Evolutionary Dynamics Need Randomness, Complexity or Determinism?
197
2
Existing Experiment Results
Experiments reported above, leading to question whether evolutionary algo-
rithms needs randomness, complexity or determinism were focused on use of
periodic series (CPS) generated by CPF instead of series from PRNG. This idea
is extension of the [6], [19], where was reported use (DCHG) instead of PRNGs.
Idea of use (DCHG) was then modiﬁes so that chaotic systems under given pre-
cision generated n periodic series (CHPS), see [6]. Because all experiments has
showed that EAs powered by DCHG as well as CHPS shows the same or better
performance rather than EAs powered by PRNG, then idea to extend simula-
tions on clearly periodical generators was straightforward. For last experiments
reported in [30] was used simple periodic function sinus, Eq. (2), and uniﬁed
into appropriate interval. The δ parameter was empirically set to 0.3, for expla-
nation see [30]. This trivial ”generator” was used to generate real numbers in
the interval [0, 1] (for mutations etc.) and integer numbers in interval [1, NP]
for parents selection in DE. For SOMA it was not necessary, because parent
selection is based on diﬀerent principles, [13]. For integer numbers has been ob-
served obvious periodicity (see [30] and Fig. 3) with some impact on algorithm
performance, that is discussed in the [30].
sin(x)
where x ∈[xstart, xstart + δ, xstart + 2δ, ...]
xstart = rnd[0, π],
δ = 0.3
(2)
The most curious experiments were done in [30] where 2000 evolutionary
simulations, based on replacement of PRNGs generators by classical periodical
generators (given by (2)) for mutation etc... has been processed and summarized.
An example of results is reported in tables 1 - 2 for DE and in tables 3 - 4 for
SOMA. In each table is recorded all 5 test functions and its the worst (maximal),
0
50
100
150
200
0
10
20
30
40
50
x
sinx
Π10
Fig. 3. Periodic behavior of Eq. 2 with randomly selected xstart. The value at the axe y
is in fact No. of individual that was selected from population to create an oﬀspring. As
visible, for DE such selection caused bad performance because only a few individuals
was selected for crossover.

198
I. Zelinka and R. Senkerik
median, average and the best (minimal) values. In each table the result from EAs
powered by CPF (2), are bolded if they are the same or better like results from
EAs powered by PRNG. Tables are organized according to used algorithms and
dimension to made mutual comparison as easy as possible.
Table 1. Dimension 5: DE with PRNG
Function
Max
Median Average
Min
Ackley
0
0
0
0
Griewangk 0.00985 0.00049
0
0
Rana
-1872.01 -1908.92 -1905.38 -1961.09
Rastrigin
-1000
-1000
-1000
-1000
Schwefel
-2094.91 -2094.91 -2094.91 -2094.91
Table 2. Dimension 5: DE with deterministic periodic generator
Function
Max
Median
Average
Min
Ackley
7.10543 × 10−15 2.84217 × 10−16
0
0
Griewangk
0.27834
0.18504
0.18769
0.07529
Rana
-1830.78
-1904.4
-1901.9 -1972.27
Rastrigin
-926.05
-982.127
-994.438
-1000
Schwefel
-2094.91
-2094.91
-2094.91 -2094.91
Table 3. Dimension 10: SOMA with PRNG
Function
Max
Median
Average
Min
Ackley
0.00228119
0.00132
0.00132521
0.00062262
Griewangk 0.0221847 0.00475218 0.00189315 0.0000478522
Rana
-3872.87
-4005.03
-3997.3
-4164.25
Rastrigin
-2000.
-2000.
-2000.
-2000.
Schwefel
-4189.83
-4189.83
-4189.83
-4189.83
Table 4. Dimension 10: SOMA with deterministic periodic generator
Function
Max
Median
Average
Min
Ackley
5.15986
0.103203 4.64739 × 10−6 1.33639 × 10−6
Griewangk 0.0713621 0.0322835
0.029518
4.29257 × 10−8
Rana
-4108.64
-4398.14
-4416.91
-4521.97
Rastrigin
-1920.4
-1980.1
-1980.1
-2000
Schwefel
-4189.83
-4189.83
-4189.83
-4189.83
Algorithms selected for our all previous experiments, reported above, were
of diﬀerent nature like for example diﬀerential evolution (DERand1Bin), [14]
and SOMA (AllToOne), [13] amongst the others. All experiments has clearly
indicated (numerically, no mathematical proof is behind) that randomness (or
pseudo randomness in computer applications) is probably not necessary, or bet-
ter, randomness is in EAs dynamics aggravating.

Does Evolutionary Dynamics Need Randomness, Complexity or Determinism?
199
3
Evolution as a Feedback Loop Control?
In classical control theory is one of ways how to control dynamical system repre-
sented by so called feedback loop, that is depicted at Fig. 4, where w is so called
desired value (the aim toward to which we would like to control system), y is
output value of controlled system, that is further taken back in feedback loop,
subtracted from w and diﬀerence e goes to the controller. In controller is e used
to calculate the most suitable controller output u so that y will be more close
to expected aim of control and future e = w −y will be minimized as much as
possible. Whole control process can be inﬂuenced by noise v, whose existence
is not, in control theory, welcome and each control technique try to avoid or
eliminate it.
Controller
-
+
w
y
y
e
u
Controlled system
v
Fig. 4. Classical feedback loop control
Between EA dynamics and feedback loop are very tight similarities, that allow
us to think about evolutionary algorithm dynamics as if that would be complex
feedback loop system. The complexity here is present in evolutionary dynamics
in which interact individuals amongst themselves. If EA is considered like a
dynamical MIMO system (multiple input - multiple output) then interaction
between inputs (parents) is ”arbitratrry” driven by PRNGs, DCHS or CPF and
output (oﬀsprings) are then result of this interaction. Cost function itself can
be very often complex (even without noise) and thanks to feedback loop its
complexity inﬂuence dynamics of EA (i.e. controller).
In the case of feedback loop interpretation of EAs a very wide spectra of con-
trol techniques can be used to control and analyze EAs. Lets take a closer com-
parison. If EA is considered like feedback loop system, then it can be interpreted
so that w is expected (desired) minimal or maximal value of optimization, the y
is actual ﬁtness of considered individual(s) (if control is considered like MIMO
control), e is diﬀerence between individual ﬁtness and aim of optimization. The
e is then used by controller, that can be understand like procedures of parents
selection and crossover. New individual u is then evaluated by cost function and
calculated ﬁtness (controlled system). The role of noise v can be understand like

200
I. Zelinka and R. Senkerik
mutation (input to the controller or addition to u) and/or like noise acting on
cost function (cost function with noise).
This control theory interpretation of EA dynamics clearly show that random-
ness is not vital for EA dynamics and is usually disruptive with negative
impact on controlling process, which is in full coincidence with our previous
experimental observations on evolution without PRNGs.
4
Conclusion
In this paper we propose novel way how to understand, analyze and control
complex dynamics of evolutionary algorithms. The results reported in our previ-
ous experiments shows, surprisingly many results from EAs powered by DCHS
or CPF are better or the same, and if worst, then in many cases only slightly
worst, which support our ideas, that randomness (i.e. noise) is disrup-
tive with negative impact on evolutionary process.
The question how much shall be randomness eliminated or used in EAs dy-
namics from control engineering point of view is further step of pour research.
As an alternative way of EAs dynamics control, reported in [31] is depicted at
Fig. 5. Some methods on CML (coupled map lattices) systems control, especially
by means of evolutionary algorithms, exist today. The spirit of this idea is to
create a closed loop in the following schematic: evolutionary dynamics complex
network
CML system
control CML
control evolutionary dynamics. Reason
for this is that this proposed techniques can be used for analysis and control
of complex networks exists and if complex network structure would be hidden
behind EA dynamics, then it is almost sure, that for example above mentioned
Controller (EA?)
-
+
w
y
y
e
u
Controlled system
1
2
3
4
5
5
6
7
1
2
3
4
5
6
EAs has dynamics, 
that can be described ...
... like complex network with 
increasing edges and its
            weights, ...
... and converted to the CML system as 
shown by example with row of pendulums. 
Each row of CML represent one pendulum 
and structure of  connections is more 
complex than in classic CML.
CML is controllable system.
And can be controlled by classical methods (probably more complex problem) or by 
another evolutionary algorithm, as is already published in our case studies.
If we control CML, then, in fact, we 
control dynamics of CML and EA.
Fig. 5. The schematic principle of proposed feedback control EAs and CNS: evolu-
tionary dynamics →complex network →CML system →control CML →control
evolutionary (complex network) dynamics

Does Evolutionary Dynamics Need Randomness, Complexity or Determinism?
201
control techniques could be used to improve dynamics of EAs. Complex network
is depicted as a set of vertices, mutually joined by single and multiple edges.
Each edge can be added or cancelled during the evolution of the network, or
importance of an edge can be modiﬁed by weights associated to the each edge.
Adding or canceling of the edges (or its weights) represents, in fact, dynamics
of the algorithm, i.e. network.
Both approaches mentioned here are under intensive investigation and results
will be presented soon in another research papers.
Acknowledgement. The following two grants are acknowledged for the ﬁnan-
cial support provided for this research: Grant Agency of the Czech Republic -
GACR P103/13/08195S, by the Development of human resources in research and
development of latest soft computing methods and their application in practice
project, reg. no. CZ.1.07/2.3.00/20.0072 funded by Operational Programme Ed-
ucation for Competitiveness, co-ﬁnanced by ESF and state budget of the Czech
Republic, partially supported by Grant of SGS No. SP2014/159, VˇSB - Technical
University of Ostrava, Czech Republic, and by European Regional Development
Fund under the project CEBIA-Tech No. CZ.1.05/2.1.00/03.0089.
References
1. Pluhacek, M., Senkerik, R., Davendra, D., Kominkova Oplatkova, Z.: On the Be-
haviour and Performance of Chaos Driven PSO Algorithm with Inertia Weight. In:
Computers and Mathematics with Applications (in print) ISSN 0898-1221
2. Pluhacek, M., Budikova, V., Senkerik, R., Oplatkova, Z., Zelinka, I.: Extended
Initial Study on the Performance of Enhanced PSO Algorithm with Lozi Chaotic
Map. In: Zelinka, I., Snasel, V., R¨ossler, O.E., Abraham, A., Corchado, E.S. (eds.)
Nostradamus: Mod. Meth. of Prediction, Modeling. AISC, vol. 192, pp. 167–177.
Springer, Heidelberg (2013)
3. Pluhacek, M., Senkerik, R., Zelinka, I.: Impact of Various Chaotic Maps on the
Performance of Chaos Enhanced PSO Algorithm with Inertia Weight an Initial
Study. In: Zelinka, I., Snasel, V., R¨ossler, O.E., Abraham, A., Corchado, E.S. (eds.)
Nostradamus: Mod. Meth. of Prediction, Modeling. AISC, vol. 192, pp. 153–166.
Springer, Heidelberg (2013)
4. Pluhacek, M., Senkerik, R., Davendra, D., Zelinka, I.: PID Controller Design For
4th Order system By Means of Enhanced PSO algorithm With Lozi Chaotic Map.
In: Proceedings of 18th International Conference on Soft Computing, MENDEL
2012, pp. 35–39 (2012) ISBN 978-80-214-4540-6
5. Pluhacek, M., Budikova, V., Senkerik, R., Oplatkova, Z., Zelinka, I.: On The Per-
formance of Enhanced PSO algorithm With Lozi Chaotic Map An Initial Study.
In: Proceedings of 18th International Conference on Soft Computing, MENDEL
2012, pp. 40–45 (2012) ISBN 978-80-214-4540-6
6. Persohn, K.J., Povinelli, R.J.: Analyzing logistic map pseudorandom number gen-
erators for periodicity induced by ﬁnite precision ﬂoating-point representation.
Chaos, Solitons and Fractals 45, 238–245 (2012)
7. Davendra, D., Zelinka, I., Senkerik, R.: Chaos driven evolutionary algorithms for
the task of PID control. Computers and Mathematics with Applications 60(4),
1088–1104 (2010) ISSN 0898-1221

202
I. Zelinka and R. Senkerik
8. Senkerik, R., Davendra, D., Zelinka, I., Oplatkova, Z., Pluhacek, M.: Optimization
of the Batch Reactor by Means of Chaos Driven Diﬀerential Evolution. In: Snasel,
V., Abraham, A., Corchado, E.S. (eds.) SOCO Models in Industrial & Environ-
mental Appl. AISC, vol. 188, pp. 93–102. Springer, Heidelberg (2013)
9. Lozi, R.: Emergence Of Randomness From Chaos. International Journal of Bifur-
cation and Chaos 22(2), 1250021 (2012), doi:10.1142/S0218127412500216
10. Wang, X.-Y., Qin, X.: A new pseudo-random number generator based on CML
and chaotic iteration. Nonlinear Dynamics An International Journal of Nonlinear
Dynamics and Chaos in Engineering Systems, Nonlinear Dyn. 70(2), 1589–1592
(2012), doi:10.1007/s11071-012-0558-0
11. Pareek, N.K., Patidar, V., Sud, K.K.: A Random Bit Generator Using Chaotic
Maps. International Journal of Network Security 10(1), 32–38 (2010)
12. Xing-Yuan, W., Lei, Y.: Design of Pseudo-Random Bit Generator Based on Chaotic
Maps. International Journal of Modern Physics B 26(32), 1250208, 9 (2012),
doi:10.1142/S0217979212502086
13. Zelinka, I.: SOMA – Self Organizing Migrating Algorithm. In: Babu, B.V., On-
wubolu, G. (eds.) New Optimization Techniques in Engineering. STUDFUZZ,
vol. 141, pp. 167–217. Springer, Heidelberg (2004)
14. Price, K.: An Introduction to Diﬀerential Evolution. In: Corne, D., Dorigo, M.,
Glover, F. (eds.) New Ideas in Optimization, pp. 79–108. McGraw-Hill, London
(1999)
15. Glover, F., Laguna, M., Mart, R.: Scatter Search. In: Ghosh, A., Tsutsui, S. (eds.)
Advances in Evolutionary Computation: Theory and Applications, pp. 519–537.
Springer, New York (2003)
16. Beyer, H.G.: Theory of Evolution Strategies. Springer, New York (2001)
17. Holland, J.H.: Genetic Algorithms. Scientiﬁc American, 44–50 (July 1992)
18. Clerc, M.: Particle Swarm Optimization. ISTE Publishing Company (2006) ISBN
1905209045
19. Zelinka, I., Senkerik, R., Pluhacek, M.: Do Evolutionary Algorithms Indeed Re-
quire Randomness? In: 2013 IEEE Congress on Evolutionary Computation (CEC),
pp. 2283–2289 (2013)
20. Zelinka, I., Chadli, M., Davendra, D., Senkerik, R., Pluhacek, M., Lampinen, J.:
Hidden Periodicity - Chaos Dependance on Numerical Precision. In: Zelinka, I.,
Chen, G., R¨ossler, O.E., Snasel, V., Abraham, A. (eds.) Nostradamus 2013: Pre-
diction, Model. & Analysis. AISC, vol. 210, pp. 47–59. Springer, Heidelberg (2013)
21. Zelinka, I., Chadli, M., Davendra, D., Senkerik, R., Pluhacek, M., Lampinen, J.: Do
Evolutionary Algorithms Indeed Require Random Numbers? Extended Study. In:
Zelinka, I., Chen, G., R¨ossler, O.E., Snasel, V., Abraham, A. (eds.) Nostradamus
2013: Prediction, Model. & Analysis. AISC, vol. 210, pp. 61–75. Springer, Heidelberg
(2013)
22. Alatas, B., Akin, E., Ozer, B.A.: Chaos embedded particle swarm optimization
algorithms. Chaos, Solitons and Fractals 40(4), 1715–1734 (2009) ISSN 0960-0779
23. Eberhart, R., Kennedy, J.: Swarm Intelligence. The Morgan Kaufmann Series in
Artiﬁcial Intelligence. Morgan Kaufmann (2001)
24. Dorigo, M., Gambardella, L.M., Birattari, M., Martinoli, A., Poli, R., St¨utzle, T.
(eds.): ANTS 2006. LNCS, vol. 4150. Springer, Heidelberg (2006)
25. Skanderova, L., Zelinka, I., ˇSaloun, P.: Chaos Powered Selected Evolutionary Al-
gorithms. In: Zelinka, I., Chen, G., R¨ossler, O.E., Snasel, V., Abraham, A. (eds.)
Nostradamus 2013: Prediction, Model. & Analysis. AISC, vol. 210, pp. 111–124.
Springer, Heidelberg (2013)

Does Evolutionary Dynamics Need Randomness, Complexity or Determinism?
203
26. Franois, M., Grosges, T., Barchiesi, T., Erra, D., Pseudo-random, R.: number gen-
erator based on mixing of three chaotic maps. Commun Nonlinear Sci. Numer.
Simulat. 19, 887–895 (2014)
27. Vattulainena, I., Kankaalaa, K., Saarinena, J., Ala-Nissila, T.: A comparative
study of some pseudorandom number generators. Computer Physics Communi-
cations 86(3), 209–226 (1995)
28. Kanso, A., Smaoui, N.: Logistic chaotic maps for binary numbers generations.
Chaos, Solitons and Fractals 40(5), 2557–2568 (2009)
29. Hellekalek, P.: A note on pseudorandom number generators, Simulation Practice
and Theory. Simulation Practice and Theory 5(6), p6–p8 (1997)
30. Zelinka, I., Senkerik, R., Pluhacek, M.: Nonrandom Evolutionary Algorithms. In:
Proceedings of 20th International Conference on Soft Computing, MENDEL 2014
(2014) ISBN 978-80- 214-4540-6
31. Zelinka, I., Saloun, P., Senkerik, R., Pavlech, M.: Controlling Complexity. In:
Zelinka, I., Sanayei, A., Zenil, H., Rossler, O.E. (eds.) How Nature Works. Springer
(2014)

 
© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
205
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_22 
 
A Brief Survey on the Chaotic Systems  
as the Pseudo Random Number Generators 
Roman Senkerik1, Michal Pluhacek1, Ivan Zelinka2,  
Donald Davendra2, and Zuzana Kominkova Oplatkova1 
1 Tomas Bata University in Zlin, Faculty of Applied Informatics, Nam T.G. Masaryka 5555,  
760 01 Zlin, Czech Republic 
{senkerik,pluhacek,oplatkova}@fai.utb.cz 
2 Technical University of Ostrava, Faculty of Electrical Engineering and Computer Science,  
17. listopadu 15,708 33 Ostrava-Poruba, Czech Republic 
{ivan.zelinka,donald.davendra}@vsb.cz 
Abstract. This paper briefly investigates the utilization of the both discrete dis-
sipative chaotic system as well as the time-continuous chaotic systems as the 
chaotic pseudo random number generators. (CPRNGs) Several examples of 
chaotic systems are simulated, statistically analyzed and compared within this 
brief survey. 
Keywords: Chaos, Dissipative systems, Discrete maps, Chaotic flows, Pseudo 
random number generators. 
1 
Introduction 
Generally speaking, the term “chaos” can denote anything that cannot be predicted 
deterministically. In the case that the word “chaos” is combined with an attribute such 
as “deterministic”, then a specific type of chaotic phenomena is involved, having their 
specific laws, mathematical apparatus and a physical origin. The deterministic chaos 
is a phenomenon that - as its name suggests - is not based on the presence of a random 
or any stochastic effects. It is clear from the structure of the equations (see the section 
4), that no mathematical term expressing randomness is present. The seeming  
randomness in deterministic chaos is related to the extreme sensitivity to the initial 
conditions [1].  
Till now, the chaos has been observed in many of various systems (including evo-
lutionary one). Systems exhibiting deterministic chaos include, for instance, weather, 
biological systems, many electronic circuits (Chua’s circuit), mechanical systems, 
such as double pendulum, magnetic pendulum, or so called billiard problem. 
The idea of using chaotic systems instead of random processes (pseudo-number 
generators - PRNGs) has been presented in several research fields and in many  
applications with promising results [2], [3].  
Another research joining deterministic chaos and pseudorandom number generator has 
been done for example in [4]. Possibility of generation of random or pseudorandom 

206 
R. Senkerik et al. 
 
numbers by use of the ultra weak multidimensional coupling of p 1- dimensional dynam-
ical systems is discussed there. Another paper [5] deeply investigate logistic map as a 
possible pseudorandom number generator and is compared with contemporary pseudo-
random number generators. A comparison of logistic map results is made with conven-
tional methods of generating pseudorandom numbers. The approach used to determine 
the number, delay, and period of the orbits of the logistic map at varying degrees of pre-
cision (3 to 23 bits). Another paper [6] proposed an algorithm of generating pseudoran-
dom number generator, which is called (couple map lattice based on discrete chaotic 
iteration) and combine the couple map lattice and chaotic iteration. Authors also tested 
this algorithm in NIST 800-22 statistical test suits and for future utilization in image 
encryption. In [7] authors exploit interesting properties of chaotic systems to design a 
random bit generator, called CCCBG, in which two chaotic systems are cross-coupled 
with each other. A new binary stream-cipher algorithm based on dual one-dimensional 
chaotic maps is proposed in [8] with statistic proprieties showing that the sequence is of 
high randomness (similar studies in [9], [10], [11]). 
2 
Motivation 
Till now the chaos was observed in many of various systems (including evolutionary 
one) and in the last few years is also used to replace pseudo-number generators 
(PRGNs) in evolutionary algorithms (EAs). 
Recent research in chaos driven heuristics has been fueled with the predisposition 
that unlike stochastic approaches, a chaotic approach is able to bypass local optima 
stagnation. This one clause is of deep importance to evolutionary algorithms. A chaot-
ic approach generally uses the chaotic map in the place of a pseudo random number 
generator [12]. This causes the heuristic to map unique regions, since the chaotic map 
iterates to new regions. The task is then to select a very good chaotic map as the 
pseudo random number generator. 
The initial concept of embedding chaotic dynamics into the evolutionary algo-
rithms is given in [13]. Later, the initial study [14] was focused on the simple embed-
ding of chaotic systems in the form of chaos pseudo random number generator 
(CPRNG) for DE and SOMA [15] in the task of optimal PID tuning 
Several papers have been recently focused on the connection of heuristic and cha-
otic dynamics either in the form of hybridizing of DE with chaotic searching algo-
rithm [16] or in the form of chaotic mutation factor and dynamically changing 
weighting and crossover factor in self-adaptive chaos differential evolution (SACDE) 
[17]. Also the PSO (Particle Swarm Optimization) algorithm with elements of chaos 
was introduced as CPSO [18] or CPSO combined with chaotic local search [19]. 
The focus of our research is the pure embedding of chaotic systems in the form of 
chaos pseudo random number generator for evolutionary algorithms.  
 

 
A Brief Survey on the Chaotic Systems as the Pseudo Random Number Generators 
207 
 
This idea was later extended with the successful experiments with chaos driven DE 
(ChaosDE) [20], [21] with both and complex simple test functions and in the task of 
chemical reactor geometry optimization [22]. 
The concept of Chaos DE has proved itself to be a powerful heuristic also in com-
binatorial problems domain [23]. 
At the same time the chaos embedded PSO with inertia weigh strategy was closely 
investigated [24], followed by the introduction of a PSO strategy driven alternately by 
two chaotic systems [25] and novel chaotic Multiple Choice PSO strategy (Chaos 
MC-PSO) [26]. 
3 
The Concept of CPRNG 
The general idea of CPRNG is to replace the default PRNG with the chaotic system. 
As the chaotic system is a set of equations with a static start position, we created a 
random start position, in order to have different start position for different experi-
ments. This random position is initialized with the default PRNG, as a one-off ran-
domizer. Once the start position of the chaotic system has been obtained, the system 
generates the next sequence using its current position. 
The first possible way is to generate and store a long data sequence (approx. 50-
500 thousand numbers) during the evolutionary process initialization and keep the 
pointer to the actual used value in the memory. In case of the using up of the whole 
sequence, the new one will be generated with the last known value as the new initial 
one. 
The second approach is that the chaotic system is not re-initialized during the ex-
periment and no long data series is stored, thus it is imperative to keep the current 
state of the chaotic system in memory to obtain the new output values. 
As two different types of numbers are required in computer science; real and inte-
gers, the modulo operators is used to obtain values between the specified ranges, as 
given in the following equations (1) and (2): 
 
rndreal = mod (abs (rndChaos) , 1.0) 
(1) 
 
rndint = mod (abs (rndChaos) , 1.0) × Range + 1 
(2) 
Where abs refers to the absolute portion of the chaotic map generated number 
rndChaos, and mod is the modulo operator. Range specifies the value (inclusive) till 
where the number is to be scaled. 
Nevertheless there exist many other approaches as to how to deal with the negative 
numbers as well as with the scaling of the wide range of the numbers given by the 
chaotic systems into the typical range 0 – 1: 
• Finding of the maximum value of the pre-generated long discrete sequence and 
dividing of all the values in the sequence with such a maxval number. 
• Shifting of all values to the positive numbers (avoiding of ABS command) and 
scaling. 

208 
R. Senkerik et al. 
 
Utilization of the time-cont
tion on the sampling time
distributions of generated p
4 
Discrete Chaotic
This section contains the d
used as the chaotic pseudo r
of the chaotic maps were u
cal range <0 - 1>. Followin
tive standard map (4). 
The x, y plots of the cha
Fig. 3 - left (Dissipative sta
ures represents the typical 
ples of direct output iteratio
The illustrative histogram
range <0 - 1> generated by 
4.1 
Burgers Map 
The Burgers mapping is a 
which were used by Burger
tion to the study of hydrod
control parameters a = 0.75
 
 
Fig. 1. x, y plot of the Burge
tinuous systems (chaotic flows) requires also the investi
. Since the different sampling times gives different fi
pseudo-random numbers. 
c Maps 
escription of discrete dissipative chaotic maps, which w
random generators. In this research, direct output iterati
sed for the generation of real numbers scaled into the ty
ng chaotic maps were used: Burgers map (3) and Dissi
aotic maps are depicted in Fig. 1 - left (Burgers map) 
andard map), whereas the right part of aforementioned F
chaotic behavior of the utilized maps given by the exa
ons. 
ms of the distribution of real numbers transferred into 
means of studied chaotic maps are in Figures 2 and 4. 
discretization of a pair of coupled differential equati
rs [27] to illustrate the relevance of the concept of bifur
dynamics flows. The map equations are given in (3) w
5 and b = 1.75 as suggested in [28]. 
n
n
n
n
n
n
n
Y
X
bY
Y
Y
aX
X
+
=
−
=
+
+
1
2
1
ers map (left); Iterations of the Burgers map (variable x) (right
iga-
final 
was 
ions 
ypi-
ipa-
and 
Fig-
am-
the 
ions 
rca-
with 
(3) 
 
t) 

 
A Brief Survey on the Chao
 
Fig. 2. Histogram of the distrib
ed by means of the chaotic Bur
4.2 
Dissipative Standar
The Dissipative Standard m
in this work are b = 0.1 and
exhibits typical chaotic beh
research papers and other li
 
Y
X
 
Fig. 3. x, y plot of the Dissip
map (variable x) (right) 
otic Systems as the Pseudo Random Number Generators 
 
bution of real numbers transferred into the range <0 - 1> gene
rgers map – 5000 samples 
rd Map 
map is a two-dimensional chaotic map. The parameters u
d k = 8.8 as suggested in [28]. For these values, the syst
havior and with this parameter setting it is used in the m
iterature sources. The map equations are given in (4). 
)
2
(mod
sin
)
2
(mod
1
1
1
π
π
n
n
n
n
n
n
X
k
bY
Y
Y
X
X
+
=
+
=
+
+
+
pative standard map (left); Iterations of the Dissipative stand
209 
erat-
used 
tem 
most 
(4) 
 
dard 

210 
R. Senkerik et al. 
 
Fig. 4. Histogram of the distrib
ed by means of the chaotic Dis
5 
Time-Continuou
This section contains the 
oscillators), which were us
chaotic systems were tested
Oscillator (6). 
The graphical outputs ar
the graphic grids (Fig 5 an
whereas the right part depic
tems, represented by the ex
trative histograms of the 
 <0 - 1> generated by mea
rate of 0.5 seconds are in Fi
5.1 
UEDA Oscillator 
UEDA oscillator is the simp
the most significant exampl
The UEDA system can b
Duffing oscillator that has b
sents the both biologically a
motion. It can be used to exp
The UEDA chaotic syste
b = 0.05, c = 7.5 and ω = 1.
 
 
bution of real numbers transferred into the range <0 - 1> gene
ssipative standard map – 5000 samples 
us Chaotic Systems 
description of time-continuous chaotic system (flows
sed as the chaotic pseudo random generators. Follow
d: unmodified UEDA oscillator (5) and Driven Van der 
re organized as in the case of chaotic maps. The left par
nd 7) shows x, y parametric plots of the chaotic syste
cts the typical chaotic behavior of the utilized chaotic s
xamples of direct output for the variable x. Finally the ill
distribution of real numbers transferred into the ra
ans of studied chaotic systems with the selected sampl
igures 6 and 8. 
ple example of driven pendulums, which represent some
les of chaos and regularity. 
be simply considered as a special case of intensively stud
both a linear and cubic restoring force. Ueda oscillator rep
and physically important dynamical model exhibiting cha
plore much physical behavior in biological systems. [29] 
em equations are given in (5). The parameters are: a = 
.0 as suggested in [28]. 
t
c
by
ax
dt
dy
y
dt
dx
ω
sin
3
+
−
−
=
=
erat-
s or 
wing 
Pol 
rt of 
ems, 
sys-
lus-
ange 
ling 
e of 
died 
pre-
aotic 
1.0 
(5) 

 
A Brief Survey on the Chao
 
Fig. 5. x, y parametric plot of
(variable x) (right) 
Fig. 6. Histogram of the distrib
ed by means of the chaotic UE
5.2 
Van der Pol Oscilla
Van der Pol oscillator is th
in electrical circuits emplo
can be used to explore phys
In this paper, the forced
investigated. This system c
the added driving function 
The parameters are: μ = 0.2
otic Systems as the Pseudo Random Number Generators 
f the UEDA oscillator (left); Simulation of the UEDA oscill
 
bution of real numbers transferred into the range <0 - 1> gene
EDA oscillator, sampling rate of 0.5 seconds – 5000 samples 
ator 
he simple example of the limit cycles and chaotic behav
oying vacuum tubes. Similarly to the UEDA oscillator
sical (unstable) behaviour in biological sciences. [30]. 
d, or commonly known as driven, Van der Pol oscillato
onsist of the original Van der Pol oscillator definition w
a sin(ωt), thus the differential equations have the form 
2 γ = 8.0, a = 0.35 and ω = 1.02 as suggested in [28]. 
211 
lator 
erat-
vior 
r, it 
or is 
with 
(6). 

212 
R. Senkerik et al. 
 
 
d
dy
dt
dx
Fig. 7. x, y parametric plot of
oscillator (variable x) (right) 
Fig. 8. Histogram of the distrib
ed by means of the chaotic Van
6 
Conclusion 
This paper was investigatin
as the chaotic pseudo rand
chaotic systems were simul
research study. 
From the graphical comp
chaotic systems; entirely 
(
)
t
a
x
y
x
t
y
y
t
x
ω
γ
μ
sin
1
3
2
+
−
−
=
=
f the Van der Pol oscillator (left); Simulation of the Van der 
 
bution of real numbers transferred into the range <0 - 1> gene
n der Pol oscillator, sampling rate of 0.5 seconds – 5000 samp
ng the utilization of the discrete dissipative chaotic syst
dom number generators. (CPRNGs) Totally four differ
lated, statistically analyzed and compared within this ini
parisons, it follows that through the utilization of differ
different statistical characteristics of CPRNGs can 
(6) 
 
Pol 
erat-
ples 
tem 
rent 
itial 
rent 
be 

 
A Brief Survey on the Chaotic Systems as the Pseudo Random Number Generators 
213 
 
achieved. Thus the different influence to the system, which utilizes the selected 
CPRNG, can be chosen through the implementation of particular inner chaotic dy-
namics given by the particular chaotic system. 
Furthermore chaotic systems have additional parameters, which can by tuned. This 
issue opens up the possibility of examining the impact of these parameters to genera-
tion of random numbers, and thus influence on the results obtained by means of either 
evolutionary techniques or different systems from the softcomputing/ computational 
intelligence field. 
Acknowledgments. This work was supported by: Grant Agency of the Czech Repub-
lic - GACR P103/13/08195S, is partially supported by Grant of SGS No. SP2014/159 
and 170, VŠB - Technical University of Ostrava, Czech Republic, by the Develop-
ment of human resources in research and development of latest soft computing meth-
ods and their application in practice project, reg. no. CZ.1.07/ 2.3.00/20.0072 funded 
by Operational Programme Education for Competitiveness, co-financed by ESF and 
state budget of the Czech Republic, further was supported by European Regional 
Development Fund under the project CEBIA-Tech No. CZ.1.05/2.1.00/03.0089 and 
by Internal Grant Agency of Tomas Bata University under the project No. 
IGA/FAI/2014/010. 
References 
1. Celikovsky, S., Zelinka, I.: Chaos Theory for Evolutionary Algorithms Researchers. In: 
Zelinka, I., Celikovsky, S., Richter, H., Chen, G. (eds.) Evolutionary Algorithms and Cha-
otic Systems. SCI, vol. 267, pp. 89–143. Springer, Heidelberg (2010) 
2. Lee, J.S., Chang, K.S.: Applications of chaos and fractals in process systems engineering. 
Journal of Process Control 6(2-3), 71–87 (1996) 
3. Wu, J., Lu, J., Wang, J.: Application of chaos and fractal models to water quality time se-
ries prediction. Environmental Modelling & Software 24(5), 632–636 (2009) 
4. Lozi, R.: Emergence of Randomness from Chaos. International Journal of Bifurcation and 
Chaos 22(02), 1250021 (2012) 
5. Persohn, K.J., Povinelli, R.J.: Analyzing logistic map pseudorandom number generators 
for periodicity induced by finite precision floating-point representation. Chaos, Solitons & 
Fractals 45(3), 238–245 (2012) 
6. Wang, X.-Y., Qin, X.: A new pseudo-random number generator based on CML and chaot-
ic iteration. Nonlinear Dyn. 70(2), 1589–1592 (2012) 
7. Narendra, K.P., Vinod, P., Krishan, K.S.: A Random Bit Generator Using Chaotic Maps. 
International Journal of Network Security 10, 32–38 (2010) 
8. Yang, L., Wang, X.-Y.: Design of Pseudo-random Bit Generator Based on Chaotic Maps. 
International Journal of Modern Physics B 26(32), 1250208 (2012) 
9. Bucolo, M., Caponetto, R., Fortuna, L., Frasca, M., Rizzo, A.: Does chaos work better than 
noise? IEEE Circuits and Systems Magazine 2(3), 4–19 (2002) 
10. Hu, H., Liu, L., Ding, N.: Pseudorandom sequence generator based on the Chen chaotic 
system. Computer Physics Communications 184(3), 765–768 (2013) 
11. Pluchino, A., Rapisarda, A., Tsallis, C.: Noise, synchrony, and correlations at the edge of 
chaos. Physical Review E 87(2), 022910 (2013) 
12. Aydin, I., Karakose, M., Akin, E.: Chaotic-based hybrid negative selection algorithm and 
its applications in fault and anomaly detection. Expert Systems with Applications 37(7), 
5285–5294 (2010) 
 

214 
R. Senkerik et al. 
 
13. Caponetto, R., Fortuna, L., Fazzino, S., Xibilia, M.G.: Chaotic sequences to improve the 
performance of evolutionary algorithms. IEEE Transactions on Evolutionary Computa-
tion 7(3), 289–304 (2003) 
14. Davendra, D., Zelinka, I., Senkerik, R.: Chaos driven evolutionary algorithms for the task 
of PID control. Computers & Mathematics with Applications 60(4), 1088–1104 (2010) 
15. Zelinka, I.: SOMA — Self-Organizing Migrating Algorithm. In: New Optimization Tech-
niques in Engineering. STUDFUZZ, vol. 141, pp. 167–217. Springer, Heidelberg (2004) 
16. Liang, W., Zhang, L., Wang, M.: The chaos differential evolution optimization algorithm and its 
application to support vector regression machine. Journal of Software 6(7), 1297–1304 (2011) 
17. Zhenyu, G., Bo, C., Min, Y., Binggang, C.: Self-Adaptive Chaos Differential Evolution. 
In: Jiao, L., Wang, L., Gao, X.-b., Liu, J., Wu, F. (eds.) ICNC 2006. LNCS, vol. 4221,  
pp. 972–975. Springer, Heidelberg (2006) 
18. Coelho, L.D.S., Mariani, V.C.: A novel chaotic particle swarm optimization approach us-
ing Hénon map and implicit filtering local search for economic load dispatch. Chaos, 
Solitons & Fractals 39(2), 510–518 (2009) 
19. Hong, W.-C.: Chaotic particle swarm optimization algorithm in a support vector regression 
electric load forecasting model. Energy Conversion and Management 50(1), 105–117 (2009) 
20. Senkerik, R., Pluhacek, M., Zelinka, I., Oplatkova, Z.K., Vala, R., Jasek, R.: Performance 
of chaos driven differential evolution on shifted benchmark functions set. In: Herrero, A., 
et al. (eds.) International Joint Conference SOCO’13-CISIS’13-ICEUTE’13. AISC, 
vol. 239, pp. 41–50. Springer, Heidelberg (2014) 
21. Senkerik, R., Davendra, D., Zelinka, I., Pluhacek, M., Kominkova Oplatkova, Z.: On the 
Differential Evolution Drivan by Selected Discrete Chaotic Systems: Extended Study. In: 
19th International Conference on Soft Computing, MENDEL 2013, pp. 137–144 (2013) 
22. Senkerik, R., Pluhacek, M., Oplatkova, Z.K., Davendra, D., Zelinka, I.: Investigation on 
the Differential Evolution driven by selected six chaotic systems in the task of reactor ge-
ometry optimization. In: 2013 IEEE Congress on Evolutionary Computation (CEC), June 
20-23, pp. 3087–3094 (2013) 
23. Davendra, D., Bialic-Davendra, M., Senkerik, R.: Scheduling the Lot-Streaming Flowshop 
scheduling problem with setup time with the chaos-induced Enhanced Differential Evolution. 
In: 2013 IEEE Symposium on Differential Evolution (SDE), April 16-19, pp. 119–126 (2013) 
24. Pluhacek, M., Senkerik, R., Davendra, D., Kominkova Oplatkova, Z., Zelinka, I.: On the 
behavior and performance of chaos driven PSO algorithm with inertia weight. Computers 
& Mathematics with Applications 66(2), 122–134 (2013) 
25. Pluhacek, M., Senkerik, R., Zelinka, I., Davendra, D.: Chaos PSO algorithm driven alter-
nately by two different chaotic maps - An initial study. In: 2013 IEEE Congress on Evolu-
tionary Computation (CEC), June 20-23, pp. 2444–2449 (2013) 
26. Pluhacek, M., Senkerik, R., Zelinka, I.: Multiple Choice Strategy Based PSO Algorithm 
with Chaotic Decision Making – A Preliminary Study. In: Herrero, Á., et al. (eds.) Interna-
tional Joint Conference SOCO’13-CISIS’13-ICEUTE’13. AISC, vol. 239, pp. 21–30. 
Springer, Heidelberg (2014) 
27. ELabbasy, E., Agiza, H., EL-Metwally, H., Elsadany, A.: Bifurcation Analysis, Chaos and Con-
trol in the Burgers Mapping. International Journal of Nonlinear Science 4(3), 171–185 (2007) 
28. Sprott, J.C.: Chaos and Time-Series Analysis. Oxford University Press (2003) 
29. Bharti, L., Yuasa, M.: Energy Variability and Chaos in Ueda Oscillator,  
http://www.rist.kindai.ac.jp/no.23/yuasa-EVCUO.pdf 
30. Kanamaru, T.: Van der Pol oscillator. Scholarpedia 2(1), 2202 (2007) 

Emergent Behaviors on Coevolutionary
Networks of Chaotic Dynamical Systems
A. Anzo and J.G. Barajas-Ram´ırez⋆
Divisi´on de Matem´aticas Aplicadas, IPICyT,
Camino a la Presa San Jos´e 2055
Col. Lomas 4a Secc. C.P.78216,
San Luis Potos´ı, S.L.P., M´exico
{andres.anzo,jgbarajas}@ipicyt.edu.mx
Abstract. Coevolutionary networks involve two simultaneous dynami-
cal processes: structural evolution and dynamics of nodes. In this context,
a current research topic has been the role of coevolution in the emer-
gence of self-organized phenomena. In order to investigate this topic, we
propose a network model where each node is a chaotic dynamical sys-
tem and, simultaneously, the structure of the network evolves according
to a local rule. In particular, the local rule updates synchronously the
binary state of each link as either “on” or “oﬀ”. We deﬁne the local
rule in terms of the state variables of the nodes and the structural fea-
tures of the network. After iterate the local rule we observe two types of
emergent behaviors: the network achieves complete synchronization; and
the formation of structural patterns which are displayed in the coupling
matrix.
1
Introduction
A dynamical network is an ensemble of interconnected dynamical units called
nodes which are usually modeled as chaotic dynamical systems. In recent years
for such dynamical network signiﬁcant results have been obtained in regards to
the stability of collective behaviors like synchronization [2,3]. These results are
based on the assumption that the network structure remains static while nodes
change their state variables. However, in many real-life situations the network
structure has their own dynamics of change which is driven by processes as the
addition or deletion of links.
When structural evolution and dynamics of nodes are considered in a network
model, we can observe that they have a causal impact on each other evolution.
In other words, the evolution of the structure aﬀects the collective behavior of
the nodes, and vice versa [5,4]. Networks with this dynamical attribute are called
coevolutionary networks. One of the mayor research questions in this direction
can be expressed as follows: what is the role of coevolution in the emergence
of self-organized phenomena? In this sense, a methodology to try to elucidate
⋆We acknowledge ﬁnancial support from IPICyT and CONACYT.
c
⃝Springer International Publishing Switzerland 2015
215
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_23

216
A. Anzo and J.G. Barajas-Ram´ırez
a response to this question is to propose models that involve both dynamical
processes simultaneously.
There are works in current references that look into modeling coevolutionary
networks. For example Sayama and Laramee have proposed a model where, based
on the graph grammar formalism, the vicinity of a given node is transformed
according to the dynamical state of its neighborhood [6]. On the other hand,
Tomita et al. have proposed a generalization of the Cellular Automata (CA)
paradigm, where the structural changes are the results of a CA rule operated
at each node of the network [7]. An alternative coevolutionary model is the
proposed by Smith et al, who have introduced the concept of Network Automata,
a network whose structural evolution depends on their own current structural
features [8]. With these research works, we can observe that the network exhibit
distinct types of structural patterns. However, on these models the state space
of each node is discrete, that is, each node takes one state value from a ﬁnite set
of states. In this context, DeLellis et al. have proposed a coevolution model for
networks of chaotic dynamical system [9]. The key idea of this model is to adjust
all the connections strengths according to adaptive rules, which are dependent
on the state variables of the nodes. DeLellis et al. have shown that with these
adaptive rules the network can achieve a synchronous behavior. Nevertheless, in
this model the conﬁguration of who is connected with whom remains ﬁxed for
all time.
Based on the Cellular Automata framework [10,11], in this contribution we
propose a coevolution model for networks of identical chaotic dynamical systems.
Our model consists of an iterative local rule describing the logic that establishes
whether the binary state of a given link is set to either “on” or “oﬀ”. In order to
deﬁne the local rule, we construct two neighborhoods for each possible link in the
network: the neighborhood of links and the neighborhood of nodes. In this sense
we deﬁne the local rule according to dynamical and structural conditions. We
change the state “on/oﬀ” of the link (i, j) if: I) Dynamical condition.- The binary
state of the link is “oﬀ” and the dynamical error between the ith and jth nodes
is smaller than that of the neighborhood of nodes ; II) Structural condition.-
If the the binary state of the link is “on” and the link betweenness centrality
of (i, j) is smaller than that of the neighborhood of links. We implement this
local rule synchronously to all possible links at speciﬁc points of time. With
this local rule we observe two classes of emergent behaviors; on the one hand the
network achieves complete synchronization; and on the other hand the formation
of structural patterns which are displayed in the coupling matrix.
The remainder of the paper is organized as follows: in section 2 our coevolution
model is described with detail. In section 3 we provide some numerical results,
and in section 4 we provide some closing remarks.
2
Model Description
We consider a network of N identical chaotic dynamical systems coupled by
unweighted and bidirectional links which have their own dynamic of change.
The state equation of the entire network is described by the following equations:

Emergent Behaviors on Coevolutionary Networks
217
˙xi(t) = f(xi(t)) + c
N

j=1
aij(t)Γ(xj(t) −xi(t)),
i = 1, . . . , N.
(1)
where t is the continuous-time scale, xi(t) = [xi1(t), xi2(t), . . . , xin(t)]T ∈Rn is
a vector with the state variables of the ith node, f : Rn →Rn deﬁnes the vector
ﬁeld of the individual chaotic dynamical system of dimension n. The coupling
of the ith node with their neighbors is described via the second term at the
right hand of equation (1), where c is the uniform coupling strength between the
nodes; Γ ∈Rn×n is the inner linking matrix which linking the state variables. We
assume that Γ = diag{r1, . . . , rn} where rl = 1 if two nodes are linked through
their l-th state variable, and rl = 0 otherwise. The term A(t) = {aij(t)} ∈
RN×N represents a time-varying coupling matrix whose elements are zero or
one depending on which nodes are connected or not at a given time instant.
This matrix is constructed as follows: if aij(t) takes the value one at time t,
then the nodes with labels i and j are connected, otherwise those nodes are not
connected. In this context, we say that the binary state of the link (i, j) is “on”
at time t if aij(t) = 1; and we say that the binary state of such link is “oﬀ” at
time t if aij(t) = 0. To complete the construction of the coupling matrix, their
diagonal entries are calculated as
aii(t) = −
N

j=1
j̸=i
aij(t) = −
N

j=1
j̸=i
aji(t),
i = 1, 2, . . . , N, ∀t.
(2)
We call (2) the diﬀusive condition. It is worth to remark that each element
of the coupling matrix aij(t), with i, j = 1, 2, . . . , N and i ̸= j, is a piecewise-
constant and continuous from the right function. We show and example of such
functions in Figure 1.
If at a given time instant t the network has not isolated nodes, then from
Gerschgoring’s circle theorem, the eigenspectrum of the coupling matrix A(t)
have the following features: zero is an eigenvalue of multiplicity one; all its non-
zero eigenvalues are strictly negative, and they can be ordered as 0 = λ1(t) >
λ2(t) ≥λ3(t) ≥· · · ≥λN(t).
Fig. 1. Each element of the the coupling matrix aij(t), with i, j = 1, 2, . . . , N and i ̸= j,
is a piecewise-constant and continuous from the right function. We use the notation t−
k
to indicate the instant previous to the k-th updating time.

218
A. Anzo and J.G. Barajas-Ram´ırez
The dynamical network (1) is said to (asymptotically) achieve complete syn-
chronization if the trajectories of every node satisfy
lim
t→∞∥xi(t) −xj(t)∥= 0,
for
i, j = 1, 2, . . ., N.
(3)
Because the diﬀusive condition (2) the synchronized trajectory of (1) corre-
sponds to that of an isolated node, i.e, ˙s(t) = f(s(t)); where s(t) ∈Rn is called
the synchronized solution [3].
Simultaneously to the dynamics of nodes, we consider that the network struc-
ture of (1) evolves driven by a process that updates the binary state “on/oﬀ”
of all the N(N −1)/2 possible links. Furthermore we assume that the update
of the binary state of the links occurs instantaneously at specif points of the
time scale t. We call such points of time the updating times and we denote it as
t1 < t2 < · · · < tk, with k ∈Z+. In this contribution we select arbitrarily every
updating time; in particular we consider that they are placed over the t scale at
regular intervals T . Additionally, we use the notation t−
k to indicate the instant
previous to the k-th updating time as we illustrate in Figure 1.
In this sense, we model the structural evolution as an operator that at a given
updating time tk alters the elements of the current coupling matrix A(t−
k ) in
order to construct the next coupling matrix. For a coevolutionary network, such
operator depends both structural and dynamical features of the network at the
instant previous to the k-th updating time t−
k , that is
A(tk) = Φ(A(t−
k ), X(t−
k )),
(4)
for k = 1, 2, . . . where X(t−
k ) = [xT
1 (t−
k ), xT
2 (t−
k ), . . . , xT
N(t−
k )]T ∈RNn is a vector
with the Nn state variables of the nodes at time instant t−
k ; and Φ : RN×N ×
RNn →RN×N is the operator that updates the current structure. The action of
the operator Φ(·) consist on visit all the entries of the current coupling matrix
A(t−
k ) and update it according to a local rule. Note that this process is analogous
to the synchronous update of the cells in the Cellular Automata framework [8,11].
In order to deﬁne the local rule we need ﬁrst to specify the neighborhood for
each possible link. In this work we propose to deﬁne the neighborhood of each
link according to their position within the coupling matrix A(t). In particular we
consider the von Neumann neighborhood deﬁnition as we illustrate in Figure 2.
It is worth remarking that in this neighborhood deﬁnition we avoid the diagonal
elements of the coupling matrix and we consider a periodic boundary. In this
context, if r is the radius of the neighborhood, i.e, the number of neighbors at
the right (or left, or up, or down) of the entry aij(t−
k ), then the neighborhood
of links is
Lij(t−
k ) = {ai−r,j(t−
k ), . . . , ai+r,j(t−
k ), ai,j−r(t−
k ), . . . , ai,j+r(t−
k )}.
Additionally, we deﬁne the neighborhood of nodes as the set of nodes involved
on Lij(t−
k ), i.e.,
Xij(t−
k ) = {xi−r(t−
k ), . . . , xi+r(t−
k ), xj−r(t−
k ), . . . , xj+r(t−
k )}.

Emergent Behaviors on Coevolutionary Networks
219
Fig. 2. The construction of the neighborhood for the link a36(t) with r = 1. At the
left the network structure where gray nodes and gray links indicates the neighborhood
of nodes and links for a36(t). At the right the coupling matrix where we highlight the
neighborhood of the entry a36(t).
On Figure 2 we show an example of the construction of the two neighborhoods
for r = 1.
Further, if we consider that all the links are updating with the same local
rule, we can rewrite (4) as follows
aij(tk) = φ(aij(t−
k ), Lij(t−
k ), Xij(t−
k )),
(5)
for i, j = 1, . . . , N with i ̸= j, where φ(·) is the local rule which determines the
next binary state of the link aij. In order to deﬁne the local rule (5) we use
the following information: 1) the link aij(t−
k ) own binary state; 2) the structural
features of the links with binary state “on” in Lij(t−
k ); and 3) the dynamical
error epq(t−
k ) =
xp(t−
k ) −xq(t−
k )
 between all the pairs of nodes p, q in Xij(t−
k ).
Concerning to the structural features of the links, in this contribution we
consider their betweenness centraliyt bij(t−
k ), which is deﬁned as the sum of
the fraction of all-pairs shortest paths that pass through the link aij [1]. More
precisely, the link betweenness is deﬁned as
bij(t−
k ) =
N

p,q=1
p̸=q
σpq(aij(t−
k ))
σpq(t−
k )
(6)
where σpq(t−
k ) is the number of shortest paths connecting p and q at time instant
t−
k , while σpq(aij(t−
k )) is the number of shortest paths connecting p and q and
passing through link aij(t−
k ).

220
A. Anzo and J.G. Barajas-Ram´ırez
Then, using the information described above, we propose the following local
rule
aij(tk) =
⎧
⎪
⎨
⎪
⎩
1
if
aij(t−
k ) = 0
and
eij(t−
k ) ≤

epq(t−
k )
	
Xij
0
if
aij(t−
k ) = 1
and
bij(t−
k ) ≤

bpq(t−
k )
	
Lij
aij(t−
k )
otherwise
(7)
for i, j = 1, 2, . . ., N with i ̸= j; p, q ∈Xij(tk) with p ̸= q, where

·
	
Xij and

·
	
Lij are the average value over the sets Xij(t−
k ) and Lij(t−
k ) respectively.
On the ﬁrst part of the local rule (7) we argue that if at the current time the
nodes i and j are disconnected, and the dynamical error between their states
is smaller than that the average in the neighborhood of nodes, then we create
a link between such nodes. On the second part of the rule (7) we argue that if
at the current time the nodes i and j are connected, but the link centrality is
smaller than that the average in the neighborhood of links, then we remove the
link between such links. And on the third part of (7) we argue that the current
binary state of the link is preserved if any of these above parts are not satisﬁed.
We summarize our coevolution model using the following algorithm:
· Step 1.- The initial condition for every node is chosen uniformly randomly.
Next, the initial network structure (A0 = A(t0)) is chosen to be a regular
graph; and the coupling strength c is selected such that the initial network
will not synchronize for the ﬁrst time instants. On the other hand the up-
dating times are chosen such that tk+1 −tk = T , for k ∈Z+, and T > 0.
For the initial time interval [0, T ) the nodes dynamical state are evaluated
according to (1) with the ﬁxed network structure A0.
· Step 2. At each updating time tk, with k ∈Z+, the local rule (7) is utilized
to update all the possible N(N −1)/2 links in the network and construct
the next structure Ak = A(tk).
· Step 3.- The nodes dynamical state are evaluated according to (1) for t ∈
[T k, T (k + 1)) with the network structure Ak.
· Step 4.- Repeat Step 2
In the following section, our coevolution model is applied to a network of
chaotic dynamical systems.
3
Simulation Results
We consider a network described by (1) with N = 100 nodes, where each node
is a Chua’s circuit given by
⎡
⎣
˙x(t)
˙y(t)
˙z(t)
⎤
⎦=
⎡
⎣
a[−y(t) −NL(x(t))]
x(t) −y(t) + z(t)
by(t)
⎤
⎦
(8)

Emergent Behaviors on Coevolutionary Networks
221
Fig. 3. Network of N = 100 Chua’s circuits, with coupling strength c = 0.5 and a)
r = 3; b) r = 10 . Synchronization is achieved after the ﬁrst updating time t1 = 5.
Fig. 4. The grid representation of the coupling matrix at diﬀerent updating times for
N = 100 nodes, coupling strength c = 0.5 and radius r = 3
where NL(x(t)) = m0x(t)+0.5(m1 −m0)(|x(t)+1|−|x(t)−1|), with a = 4.916,
b = 3.641, m0 = −0.07 and m1 = 1.5. We select the initial condition for each
node randomly from a uniform distribution in the interval [−20.0, 20.0].

222
A. Anzo and J.G. Barajas-Ram´ırez
Fig. 5. The grid representation of the coupling matrix at diﬀerent updating times for
N = 100 nodes, coupling strength c = 0.5 and radius r = 10
On the other hand, we chose the initial structure A0 = A(t0) to be that of
a Nearest Neighbors (NN) network where each node is connected with its two
forward and two backward neighbors. In Figure 3.a we show the grid represen-
tation of the coupling matrix A0, where black squares represent the links with
binary state “on”; and white squares represent the links with binary state “oﬀ”.
To complete the description of (1), we select a uniform coupling strength
c = 0.5, and we select the inner linking matrix to be the identity matrix Γ = In,
where n = 3 for the Chua’s circuit (7). Using the synchronization criteria shown
in [3], we have that our original NN network will not synchronize for the ﬁrst
time instants ([t0, t1)).
On the other hand, we choose to update synchronously all the possible links at
equal time periods, that is, the updating times are selected such that tk+1 −tk =
T = 5 time steps, with k ∈Z+.
First, we consider that the neighborhood of links is constructed with radius
r = 3. As a second example we consider r = 10. The simulation results for
N = 100 Chua’s circuits are shown in Figs 3. In both examples, synchronization
is achieved after the ﬁrst updating time t1. For the ﬁrst example, we observe that

Emergent Behaviors on Coevolutionary Networks
223
Table 1. Basic network metrics for the structures in Figure 4; where Avg. is the average
value over all the nodes in the network
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
A11
No. links
1315 1334 1801 1770 1647 1609 1663 1986 1520 2153 1293
No. connected com.
1
2
4
2
4
2
3
2
2
2
2
Avg. node degree
26.3
26.7
36.0
35.4
32.9
32.1
33.3
39.7
30.4
43.1
25.9
Avg. clustering c.
0.63
0.61
0.65
0.64
0.63
0.64
0.57
0.63
0.56
0.6
0.47
Avg. path eﬃciency
0.56
0.62
0.64
0.67
0.62
0.65
0.63
0.7
0.62
0.7
0.6
Avg. link betweenness 0.022 0.017 0.015 0.016 0.016 0.016 0.016 0.015 0.017 0.016 0.018
the trajectory of a single node does not collapse into the synchronized state; and
in the second example we observe that all nodes are synchronized after the ﬁrst
updating time t1 = 5, but the trajectory of a single node moves away from the
synchronized state after the t = 75 time step.
In Figures 4 and 5 we show the grid representation of the coupling matrix at
diﬀerent updating times for the ﬁrst (r = 3) and the second (r = 10) example
respectively. In both examples we can visually identify various types of structural
patterns at each updating time. It is worth to note that in t1, the network
structure favors the emergence of a stable synchronized behavior of the nodes. In
Tables 1 and 2 we show some basic network metrics for the structures in Figures
3 and 4 respectively. We can observe that the rule (7) favor the formation of
groups, and, additionally, imposes a structural evolution that turns oﬀlinks, to
then turn on links as time passes. However the rule tends to decrease the value
of the link betweenness centrality.
Table 2. Basic network metrics for the structures in Figure 5; where Avg. is the average
value over all the nodes in the network
A∗
1
A∗
2
A∗
3
A∗
4
A∗
5
A∗
6
A∗
7
A∗
8
A∗
9
A∗
10
A∗
11
No. links
2002 533
2718 1147 2961 1049 3225 958
3129 1062 2921
No. connected com.
1
3
3
3
3
5
3
5
3
3
3
Avg. node degree
40.0
10.6
54.3
22.9
59.2
21.0
64.5
19.16 62.6
21.24 58.42
Avg. clustering c.
0.73
0.4
0.76
0.33
0.7
0.33
0.73
0.35
0.71
0.43
0.7
Avg. path eﬃciency
0.68
0.44
0.75
0.56
0.78
0.54
0.8
0.53
0.8
0.55
0.77
Avg. link betweenness 0.017 0.024 0.014 0.018 0.013 0.018 0.013 0.018 0.013 0.019 0.013
4
Conclusions
We propose a coevolution model where nodes are chaotic dynamical systems and
the network structure changes according to a local rule. In particular, the local
rule updates synchronously, at speciﬁc points of time, the binary state “on/oﬀ”
of every possible link. Based on the Cellular Automata framework, we deﬁne a lo-
cal rule that depends on: 1) the binary state of the link; 2) the structural features

224
A. Anzo and J.G. Barajas-Ram´ırez
of neighboring links; and 3) the dynamical features of neighboring nodes. In this
sense we construct the neighborhood for each link according to their position
in the coupling matrix, and we consider the von Neumann neighborhood deﬁni-
tion. With this model we can observe that, because the interplay between the
dynamics of nodes and the structural evolution, the network exhibit two classes
of emergent behaviors. On the one hand the structure spontaneously forms vari-
ous types of structural patterns which are displayed in the coupling matrix. And
on the other hand, we observe that the structural features of the network at the
ﬁrst updating time favor the emergence of a stable synchronized behavior on the
nodes. Further, we observe that diﬀerent patterns are former with diﬀerent ra-
dius (r) of the neighborhood; additionally, our local rule favors the formation of
groups. We believe that our coevolution model provides a scenario for studying
the role of coevolution in the emergent behaviors. Other extensions of our model
as the insertion of weighted links are currently under study.
References
1. Newman, M.: Networks: an introduction. Oxford University Press, Inc. (2010)
2. Arenas, A., Daz-Guilera, A., Kurths, J., Moreno, Y., Zhou, C.: Synchronization in
complex networks. Physics Reports 469(93), 175–308 (2008)
3. Boccaletti, S., Latora, V., Moreno, Y., Chavez, M., Hwang, D.: Complex networks:
Structure and dynamics. Physics Reports 424(4), 175–308 (2006)
4. Mukherjee, A., Choudhury, M., Peruani, F., Ganguly, N., Mitra, B.: Dynamics
on and of Complex Networks. Applications to Time-Varying Dynamical Systems,
vol. 2. Springer, Heidelberg (2013)
5. Gross, T., Blasius, B.: Adaptive coevolutionary networks: a review. J. R. Soc.
Interface 5(20), 259–271 (2010)
6. Sayama, H.: Generative network automata: A generalized framework for modeling
dynamical systems with autonomously varying topologies. In: Proceedings of the
2007 IEEE Symposium on Artiﬁcial Life, pp. 214–221 (2007)
7. Tomita, K., Kurokawa, H., Murata, S.: Graph-Rewriting Automata as a Natu-
ral Extension of Cellular Automata. In: Adaptive Networks: Theory, Models and
Applications, ch. 14. Springer, Heidelberg (2009)
8. Smith, D.M.D., Onnela, J.-P., Lee, C.F., Fricker, M., Johnson, N.F.: Network au-
tomata and the functional dynamic network framework, arXiv:physics/0701307v3
(physics.soc-ph) (2009)
9. DeLellis, P., Di Bernardo, M., Gorochowski, T.E., Russo, G.: Synchronization and
control of complex networks via contraction, adaptation and evolution. Cir. and
Sys. Mag. 10(3), 64–82 (2010)
10. Hoekstra, A., Kroc, J., Sloot, P.M.A. (eds.): Simulating Complex Systems by Cel-
lular Automata. Understanding Complex Systems. Springer, Heidelberg (2010)
11. Ilachinski, A.: Cellular Automata: A Discrete Universe. World Scientiﬁc, Singapore
(2001)

© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
225
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_24 
 
Chaos Driven PSO – On the Influence  
of Various CPRNG Implementations – An Initial Study 
Michal Pluhacek1, Roman Senkerik1, Ivan Zelinka1, and Donald Davendra2 
1 Faculty of Applied Informatics, 
Tomas Bata University in Zlin, 
T.G. Masaryka 5555, 760 01 Zlin, Czech Republic 
{pluhacek,senkerik,zelinka}@fai.utb.cz 
2 Faculty of Electrical Engineering and Computer Science, 
VŠB-Technical University of Ostrava, 
17. listopadu 15, 708 33 Ostrava-Poruba, Czech Republic 
donald.davendra@vsb.cz 
Abstract. This paper presents deep study of the process of implementation of 
discrete chaotic maps as chaotic pseudo-random number generators (CPRNGs) 
for the needs of Particle Swarm Optimization (PSO) algorithm. There are sev-
eral different ways for the CPRNG creation. This study addresses the main  
issues (including examples and results comparison) and may serve as a very 
useful resource for any future researchers.  
Keywords: Particle swarm optimization, chaos, PSO, Evolutionary algorithm, 
optimization.  
1 
Introduction 
The particle swarm optimization algorithm (PSO) is one of the new and promising 
swarm intelligence optimization algorithms which are a subset of evolutionary com-
putation techniques (ECTs). The ECTs are a significant class of soft computing opti-
mization methods that is inspired in nature. In past decades various ECTs were used 
with promising results in many areas of complex optimization [1–8]. Later on, some 
studies indicated that using chaotic number generators might improve the quality of 
results, convergence speed or other performance indicators of various ECTs [9 - 16]. 
Several studies have already dealt with the possibilities of integration of chaotic sys-
tems into the PSO algorithm and the performance of such algorithms [11–16]. This 
paper focuses on the issue of designing chaotic pseudo-random number generator 
(CPRNG) based on two-dimensional discrete chaotic maps. Several ways of CPRNG 
design are described and analyzed. Further the different versions of CPRNGs are 
employed in PSO algorithm and the performance compared on different benchmark 
functions.  
The paper is structured as follows: the second section describes the motivation for 
this research. In the third section the definition of PSO algorithm is given. Used cha-
otic maps are described in section four. The process of CPRNG implementation is 

226 
M. Pluhacek et al. 
detailed in section five. Used benchmark functions are described in the sixth section 
and experiment setup is given in the section number seven. Finally the results are 
presented and discussed in the last two sections. 
2 
Motivation 
The interconnection of chaos and ECTs starts to attract increased attention in the past 
years. In the various researches focused on chaos and ECTs [9-16] the CPRNG based 
on different two-dimensional discrete chaotic systems is often employed. In the pro-
cess of CPRNG design there are however some issues that are not usually addressed 
(typically due to paper length limits etc.). This paper is primarily motivated by exper-
iments with different chaotic systems and their implementation as CPRNGs. It has 
been found that the way the CPRNG is designed seems to have an impact on the per-
formance of chaos driven ECT (in this case the PSO algorithm). The goal of this 
study is to investigate this issue, present some ways of CPRNG design and do an in-
deep analysis based on experiments with chaos driven PSO algorithm. 
3 
Particle Swarm Optimization 
The PSO algorithm is inspired by the natural swarm behavior of animals (such as 
birds and fish). It was firstly introduced by Eberhart and Kennedy in 1995 [1]. As an 
alternative to other ECTs, such as Ant Colony Optimization [2], Genetic Algorithms 
(GA) [4] or Differential Evolution (DE) [5] the PSO became popular method for 
global optimization. Each particle in the population represents a possible solution of 
the optimization problem which is defined by the cost function (CF). In each iteration 
of the algorithm, a new location (combination of CF parameters) of the particle is 
calculated based on its previous location and velocity vector (velocity vector contains 
particle velocity for each dimension). 
According to the method of selection of the swarm or subswarm for best solution 
information spreading, the PSO algorithms are noted as global PSO (GPSO) or local 
PSO (LPSO). Within this research the PSO algorithm with global topology (GPSO) 
[6] was utilized.  The chaotic PRNG is used in the main GPSO formula (1), which 
determines a new “velocity”, thus directly affects the position of each particle in the 
next iteration. 
)
(
)
(
2
1
1
t
ij
j
t
ij
ij
t
ij
t
ij
x
gBest
Rand
c
x
pBest
Rand
c
v
w
v
−
⋅
⋅
+
−
⋅
⋅
+
⋅
=
+
 
(1)
Where: 
vi
t+1
 - New velocity of the ith particle in iteration t+1. 
w – Inertia weight value. 
vi
t - Current velocity of the ith particle in iteration t. 
c1, c2 - Priority factors. 

Chaos Driven PSO – On the Influence of Various CPRNG Implementations        227 
pBesti – Local (personal) best solution found by the ith particle.  
gBest - Best solution found in a population.  
xij
t - Current position of the ith particle (component j of the dimension D) in iteration t.  
Rand – Pseudo random number, interval (0, 1). CPRNG is applied only here. 
The maximum velocity was limited to 0.2 times the range as it is usual. The new 
position of each particle is then given by (2), where xit+1 is the new particle position: 
1
1
+
+
+
=
t
i
t
i
t
i
v
x
x
 
(2)
Finally the linear decreasing inertia weight [6, 7] is used in the typically referred 
GPSO design that was used in this study. The dynamic inertia weight is meant to slow 
the particles over time thus to improve the local search capability in the later phase of 
the optimization. The inertia weight has two control parameters wstart and wend. A 
new w for each iteration is given by (3), where t stands for current iteration number 
and n stands for the total number of iterations. The values used in this study were 
wstart = 0.9 and wend =0.4. 
(
)
(
)
n
t
w
w
w
w
end
start
start
⋅
−
−
=
 
(3)
4 
Chaotic Maps 
This section contains the description of discrete chaotic maps that were used as the 
CPRNGs for PSO algorithm in this study. The initial concept of embedding chaotic 
dynamics into evolutionary algorithms is given in [9].  
 
 
Fig. 1. x,y plot of Lozi map 

228 
M. Pluhacek et al. 
4.1 
Lozi Map 
The Lozi map is a simple discrete two-dimensional chaotic map. The map equations 
are given in (4). The parameters used in this work are: a = 1.7 and b = 0.5 with respect 
to [17, 18]. 
The Lozi map is depicted in Fig. 3.  
n
n
n
n
n
X
Y
bY
X
a
X
=
+
−
=
+
+
1
1
1
 
(4)
4.2 
Burgers Chaotic Map 
The Burgers map (See Fig. 2) is a discretization of a pair of coupled differential equa-
tions The map equations are given in  (5) with control parameters a = 0.75 and b = 
1.75 as suggested in [17].  
n
n
n
n
n
n
n
Y
X
bY
Y
Y
aX
X
+
=
−
=
+
+
1
2
1
 
(5)
 
Fig. 2. x,y plot of Burgers map 
5 
CPRNG Implementation 
For the needs of PSO main formula (1) it is necessary to generate pseudo-random 
numbers in the interval [0,1] however the majority of chaotic maps does not follow 
this restriction for the area it covers. It is therefore necessary to transform the output  
 

Chaos Driven PSO – On the Influence of Various CPRNG Implementations        229 
sequence into the required interval. In the case of two-dimensional discrete chaotic 
map as output sequence it can be used either the x or y value sequence given by the 
pair of equations that define the chaotic map (as described in previous section). Fig. 3 
gives x value sequence for the first 100 of the Lozi map. Corresponding y value se-
quence is depicted in Fig. 4. It can be observed that these sequences share certain 
similarities. However in the case of Burger´s map (Fig. 5 and 6.) the corresponding x 
and y sequences are significantly more different. Further from this point the PSO 
versions that use the x value sequence for the CPRNG are noted with “X” and vice 
versa. 
 
Fig. 3. Sample output sequence of the Lozi map (x value )  
 
Fig. 4. Sample output sequence of the Lozi map (y value ) 
0
20
40
60
80
100
-1.0
-0.5
0.0
0.5
1.0
Iteration
x
0
20
40
60
80
100
-1.0
-0.5
0.0
0.5
1.0
Iteration
y

230 
M. Pluhacek et al. 
 
Fig. 5. Sample output sequence of the Burger´s map (x value ) 
 
Fig. 6. Sample output sequence of the Burger´s map (y value ) 
The performance differences of the “X“ and “Y“ versions is investigated later in 
this paper. Furthermore two different approaches for transforming the sequences into 
the interval [0,1] are utilized in this study. In the first case the absolute value is used 
to transform negative numbers to positive. All numbers in the sequence are then di-
vided by the maximum value found in the sequence (6). As an example the distribu-
tion of CPRNG based on the Lozi map x value sequence created this way is given in 
Fig. 7. 
*)
(
/
*
X
Max
X
X =
 
(6)
Where: 
X – Transformed sequence 
X* – Original sequence 
Max(X*) – Maximum number in original sequence 
0
20
40
60
80
100
-2.5
-2.0
-1.5
-1.0
-0.5
0.0
Iteration
x
0
20
40
60
80
100
-1.0
-0.5
0.0
0.5
1.0
Iteration
y

Chaos Driven PSO – On the Influence of Various CPRNG Implementations        231 
 
Fig. 7. CPRNG based on Lozi map (x sequence, absolute value applied) – distribution  
histogram 
Alternatively it is possible to shift the whole sequence to positive numbers and in-
terval [0,1] according to (7). The distribution of CPRNG based on the Lozi map x 
value sequence created this alternative way is given in Fig. 8. Versions of PSO using 
this approach are further noted with “s” (as shifted). 
*))
(
*)
(
/(
*)
*)
(
(
X
Max
X
Min
X
X
Min
X
+
+
=
 
(7)
Where: 
Min(X*) – Minimum number in original sequence 
 
Fig. 8. CPRNG based on Lozi map (x sequence, shifted) – distribution histogram 

232 
M. Pluhacek et al. 
6 
Test Functions 
Within this research two well-known and frequently used benchmark functions (given 
by (8) and (9)) were utilized to demonstrate the difference in performance of various 
PSO versions. The dimension (D) was set to 30 for all experiments. The global opti-
mum value is 0 for all functions used. The description of used benchmark function 
follows: 
6.1 
Sphere Function 

=
=
D
i
ix
x
f
1
2
1
)
(
 
(8)
Search Range: [-100,100]D 
Init. Range: [-100,50]D 
Glob. Opt. Pos.: [0]D 
6.2 
Schwefel´s Function 

=
−
⋅
=
D
i
i
x
x
x
f
1
5
)
sin(
-
D
  
418.9829
)
(
 
(9)
Search Range: [-500,500]D 
Init. Range: [-500,500]D 
Glob. Opt. Pos.: [420.96]D 
7 
Experiment Setup 
The control parameters of PSO algorithm were set following way: 
Population size: 40; Iterations: 5000; Runs: 30; 
7.1 
Notation 
A several versions of GPSO algorithm were used. The notation follows the pattern 
described in previous sections. The version is noted according to the chaotic map that 
was used (“Lozi” or “Burger”) + the sequence that was used for CPRNG (“X” or “Y”) 
+ “s” if the shifting approach described in section 5 was applied to move the sequence 
into required interval. Examples: 
• GPSO Lozi Y – CPRNG based on the y value sequence of Lozi map. Transformed 
into the interval [0,1] using the absolute value approach. 
• GPSO Burger Xs – CPRNG based on the x value sequence of Burgers map. Trans-
formed into the interval [0,1] using the shift approach. 
• Etc. 

Chaos Driven PSO – On the Influence of Various CPRNG Implementations        233 
As already mentioned in previous sections the CPRNG based on Lozi map or 
Burger´s map was applied only for the main formula of PSO (1). For other purposes 
(generating of initial population etc.) default C language built-in pseudorandom num-
ber generator was used within all described versions of PSO. 
8 
Results 
In this section the results for each test function and version of CPRNG are summa-
rized into a statistical overview (Tables 1 – 4). The best result (Cost function value) 
and the best mean result are highlighted by bold numbers. Furthermore the mean 
gBest history for each function and version of CPRNG are depicted in Figs. 9 – 11. 
The brief results analysis follows in the next section. 
Table 1. Results - Sphere function – Lozi CPRNGs 
GPSO Lozi Y GPSO Lozi X GPSO Lozi Ys GPSO Lozi Xs 
Mean CF Value: 
4.03E-108 
7.78E-106 
1.39E-78 
2.98E-78 
Std. Dev.: 
2.18E-107 
4.26E-105 
6.78E-78 
1.39E-77 
CF Value Median: 4.73E-112 
8.74E-113 
1.49E-81 
1.17E-81 
Max. CF Value: 
1.20E-106 
2.33E-104 
3.72E-77 
7.57E-77 
Min. CF Value: 
2.35E-116 
3.43E-118 
1.95E-85 
7.04E-86 
Table 2. Results – Schwefel´s function – Lozi CPRNGs 
GPSO Lozi Y GPSO Lozi X 
GPSO Lozi Ys 
GPSO Lozi Xs 
Mean CF Value: 
4.08E+03 
4.09E+03 
5.07E+03 
5.05E+03 
Std. Dev.: 
4.46E+02 
3.26E+02 
3.93E+02 
4.77E+02 
CF Value Median: 
4.14E+03 
4.05E+03 
5.07E+03 
5.04E+03 
Max. CF Value: 
4.78E+03 
5.07E+03 
6.02E+03 
5.98E+03 
Min. CF Value: 
3.08E+03 
3.61E+03 
4.44E+03 
4.32E+03 
Table 3. Results - Sphere function – Burger CPRNGs 
GPSO Burger Y GPSO Burger X GPSO Burger Ys GPSO Burger Xs 
Mean CF Value: 
1.45E+02 
3.87E-12 
2.32E-88 
2.93E+03 
Std. Dev.: 
6.47E+01 
1.04E-11 
1.27E-87 
2.61E+02 
CF Value Median: 1.30E+02 
1.61E-12 
2.68E-100 
3.03E+03 
Max. CF Value: 
2.84E+02 
5.80E-11 
6.97E-87 
3.25E+03 
Min. CF Value: 
2.96E+01 
5.25E-14 
3.11E-111 
2.29E+03 

234 
M. Pluhacek et al. 
Table 4. Results – Schwefel´s function Burger CPRNGs 
GPSO Burger Y GPSO Burger X GPSO Burger Ys GPSO Burger Xs 
Mean CF Value: 
3.98E+03 
2.07E+03 
4.61E+03 
8.66E+03 
Std. Dev.: 
6.68E+02 
3.30E+02 
5.28E+02 
2.40E+02 
CF Value Median: 3.98E+03 
2.09E+03 
4.71E+03 
8.65E+03 
Max. CF Value: 
5.69E+03 
2.88E+03 
5.65E+03 
8.98E+03 
Min. CF Value: 
3.07E+03 
1.54E+03 
3.57E+03 
8.01E+03 
 
 
Fig. 9. Mean gBest history - sphere function – Lozi CPRNGs 
 
Fig. 10. Mean gBest history - Schwefel´s function – Lozi CPRNGs 
æ
æ
æ
æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ
à
à
à
à à à à à à à à à à à à à à à à à à à à à à à à à à à à à à à à à à à à
ì
ì
ìììììììììììììììììììììììììììììììììììììììììì
ò
ò
ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò ò
1000
2000
3000
4000
5000
Generations
10
20
30
40
CF Value
ò
GPSO Lozi , Xs
ì
GPSO Lozi Ys
à
GPSO Lozi X
æ
GPSO Lozi Y
æ
æ
æ
æ
æ
æ
æ
æ
à
à
à
à
à
à
à
à
à
ì
ì
ì
ì
ì
ì
ì
ì
ì
ò
ò
ò
ò
ò
ò
ò
ò
ò
1000
2000
3000
4000
5000
Generations
1000
2000
3000
4000
5000
6000
7000
CF Value
ò
GPSO Lozi Xs
ì
GPSO Lozi Ys
à
GPSO Lozi X
æ
GPSO Lozi Y

Chaos Driven PSO – On the Influence of Various CPRNG Implementations        235 
 
Fig. 11. Mean gBest history - sphere function – Burger CPRNGs 
 
Fig. 12. Mean gBest history - Schwefel´s function – Burger CPRNGs 
9 
Brief Analysis of the Results 
The results presented in previous section support several claims. Firstly, according to 
data presented in Tables 1 and 2 and Fig. 9 and 10 the performance of GPSO driven 
by CPRNG based on Lozi map is not too sensitive to the choice of output sequence (x 
or y) however it is very sensitive to the way the sequence is transformed to the re-
quired [0,1] interval. In the case of used benchmark functions it seems so that it is 
possible to achieve better mean results when using the “absolute value“ approach. 
Secondly according to results given in Tables 3 and 4 and the mean gBest history 
presented in Figs. 11 and 12 its seems that the GPSO driven by CPRNG based on 
Burger´s map is extremely sensitive to both the output sequence selection and the 
æ
æ
æ
æ
æ
æ
æ
æ
à
à
à
à
à
à
à
à
à
ì
ì
ì
ì
ì
ì
ì
ì
ì
ò
ò
ò
ò
ò
ò
ò
ò
ò
1000
2000
3000
4000
5000
Generations
5000
10000
15000
CF Value
ò
GPSO Burger Xs
ì
GPSO Burger Ys
à
GPSO Burger X
æ
GPSO Burger Y
æ
æ
æ
æ
æ
æ
æ
æ
æ
à
à
à
à
à
à
à
à
à
ì
ì
ì
ì
ì
ì
ì
ì
ì
ò
ò
ò
ò
ò
ò
ò
ò
ò
1000
2000
3000
4000
5000
Generations
2000
4000
6000
8000
10000
CF Value
ò
GPSO Burger Xs
ì
GPSO Burger Ys
à
GPSO Burger X
æ
GPSO Burger Y

236 
M. Pluhacek et al. 
method of transformation into the required interval. However different versions of 
GPSO driven by Burger´s map based CPRNG achieved best mean results on the used 
benchmark functions. 
10 
Conclusion 
In this initial study it was presented the issue of different ways of implementation of 
discrete two-dimensional chaotic systems as CPRNGs for PSO algorithm. Several 
different approaches were described and analyzed. According to presented data there 
are significant differences in performance of PSO algorithm based on the way the 
CPRNG is designed. Also it seems so that CPRNGs based on different chaotic sys-
tems differ significantly in the performance. The goal of this paper was to highlight 
this issue and in the future serve as a resource for researches in the fast developing 
area of chaos driven ECTs. 
Acknowledgments. The following grants are acknowledged for the financial support 
provided for this research: Grant Agency of the Czech Republic - GACR 
P103/13/08195S, is partially supported by Grant of SGS No. SP2014/159 and SGS 
No. SP2014/170 Czech Republic, by the Development of human resources in research 
and development of latest soft computing methods and their application in practice 
project, reg. no. CZ.1.07/2.3.00/20.0072 funded by Operational Programme Educa-
tion for Competitiveness, co-financed by ESF and state budget of the Czech Republic, 
further was supported by European Regional Development Fund under the project 
CEBIA-Tech No. CZ.1.05/2.1.00/03.0089 and by Internal Grant Agency of Tomas 
Bata University under the project No. IGA/FAI/2014/010. 
References  
1. Kennedy, J., Eberhart, R.: Particle Swarm Optimization. In: Proceedings of IEEE Interna-
tional Conference on Neural Networks, vol. IV, pp. 1942–1948 (1995) 
2. Dorigo, M., Gambardella, L.M., Birattari, M., Martinoli, A., Poli, R., Stützle, T. (eds.): 
ANTS 2006. LNCS, vol. 4150. Springer, Heidelberg (2006) 
3. Eberhart, R., Kennedy, J.: Swarm Intelligence. The Morgan Kaufmann Series in Artificial 
Intelligence. Morgan Kaufmann (2001) 
4. Goldberg, D.E.: Genetic Algorithms in Search Optimization and Machine Learning, p. 41. 
Addison Wesley (1989) ISBN 0201157675 
5. Storn, R., Price, R.: Differential evolution - a simple and efficient heuristic for global op-
timization over continuous spaces. Journal of Global Optimization 11, 341–359 (1997) 
6. Shi, Y.H., Eberhart, R.C.: A modified particle swarm optimizer. In: IEEE International 
Conference on Evolutionary Computation, Anchorage Alaska, pp. 69–73 (1998) 
7. Nickabadi, A., Ebadzadeh, M.M., Safabakhsh, R.: A novel particle swarm optimization al-
gorithm with adaptive inertia weight. Applied Soft Computing 11(4), 3658–3670 (2011) 
ISSN 1568-4946 

Chaos Driven PSO – On the Influence of Various CPRNG Implementations        237 
8. Zelinka, I.: SOMA - self organizing migrating algorithm. In: Babu, B.V., Onwubolu, G. 
(eds.) New Optimization Techniques in Engineering, ch. 7, vol. 33. Springer (2004) ISBN: 
3-540-20167X 
9. Caponetto, R., Fortuna, L., Fazzino, S., Xibilia, M.G.: Chaotic sequences to improve the 
performance of evolutionary algorithms. IEEE Transactions on Evolutionary Computa-
tion 7(3), 289–304 (2003) 
10. Davendra, D., Zelinka, I., Senkerik, R.: Chaos driven evolutionary algorithms for the task 
of PID control. Computers & Mathematics with Applications 60(4), 1088–1104 (2010) 
ISSN 0898-1221 
11. Araujo, E., Coelho, L.: Particle swarm approaches using Lozi map chaotic sequences to 
fuzzy modelling of an experimental thermal-vacuum system. Applied Soft Compu-
ting 8(4), 1354–1364 (2008) 
12. Alatas, B., Akin, E., Ozer, B.A.: Chaos embedded particle swarm optimization algorithms. 
Chaos, Solitons & Fractals 40(4), 1715–1734 (2009) ISSN 0960-0779 
13. Pluhacek, M., Senkerik, R., Davendra, D., Kominkova Oplatkova, Z., Zelinka, I.: On the 
behavior and performance of chaos driven PSO algorithm with inertia weight. Computers 
and Mathematics with Applications (2013), doi:10.1016/j.camwa.2013.01.016 (Article in 
press) 
14. Pluhacek, M., Budikova, V., Senkerik, R., Oplatkova, Z., Zelinka, I.: On The Performance 
Of Enhanced PSO Algorithm With Lozi Chaotic Map – An initial Study. In: Proceedings 
of the 18th International Conference on Soft Computing, MENDEL 2012, pp. 40–45 
(2012) ISBN 978-80-214-4540-6 
15. Pluhacek, M., Senkerik, R., Zelinka, I., Davendra, I.D.: Chaos PSO algorithm driven alter-
nately by two different chaotic maps - An initial study. In: 2013 IEEE Congress on Evolu-
tionary Computation (CEC), pp. 2444–2449 (2013), doi:10.1109/CEC.2013.6557862, 
ISBN: 978-1-4799-0451-8 
16. Pluhacek, M., Senkerik, R., Zelinka, I., Davendra, D.: New Adaptive Approach for Chaos 
PSO Algorithm Driven Alternately by Two Different Chaotic Maps – An Initial Study. In: 
Zelinka, I., Chen, G., Rössler, O.E., Snasel, V., Abraham, A. (eds.) Nostradamus 2013: 
Prediction, Model. & Analysis. AISC, vol. 210, pp. 77–87. Springer, Heidelberg (2013) 
17. Sprott, J.C.: Chaos and Time-Series Analysis. Oxford University Press (2003) 
18. Aziz-Alaoui, M.A., Robert, C., Grebogi, C.: Dynamics of a Hénon–Lozi-type map. Chaos, 
Solitons & Fractals 12(12), 2323–2341 (2001) ISSN 0960-0779 

Evolutionary Based ARIMA Models
for Stock Price Forecasting
Tomas Vantuch and Ivan Zelinka
VSB-Technical University of Ostrava, 17. listopadu 15 708 33,
Ostrava-Poruba, Czech Republic
{tomas.vantuch.st,ivan.zelinka}@vsb.cz
Abstract. Time series prediction is mostly based on computing future
values by the time set past behavior. If the prediction like this is met
with a reality, we can say that the time set has a memory, otherwise
the new values of time set are not aﬀected by its past values. In the
second case we can say, there is no memory in the time set and it is
pure randomness. In a faith of ”market memory”, the stock prices are
often studied, analyzed and forecasted by a statistic, an econometric, a
computer science... In this article the econometric ARIMA model is taken
for previously mentioned purpose and its constructing and estimation is
modiﬁed by evolution algorithms. The algorithms are genetic algorithm
(GA) and particle swarm optimization PSO.
Keywords: ARIMA, GA, PSO, AIC, BIC, forecasting.
1
Introduction
Nowadays, there is quite often used the econometric ARIMA model [1] proposed
by Box and Jenkins for an analysis and a forecast of time series because of its
complexity and variability. The main part of the model is the combination of
auto-regression (AR) and moving-average (MA) polynomials into one complex
polynomial:
yt = μ +
p

i=1
(γiyt−i) +
q

i=1
(θiϵt−i) + ϵt
(1)
This model is based on statistic analysis of a time set. At ﬁrst, it has to be
fulﬁlled the condition of a stationarity of a time set. A time set is stationary if
it does not contain any trends or seasonal behavior and its mean and variance
does not change over time. The condition of stationary behavior is crucial for
input time set.
The next step in ARIMA modeling is an estimating the model [4–9]. It means
to estimate p and q parameters for AR and MA polynomials and the level of
diﬀerentiation.
An auto-regression is the polynomial that contains variables of the time set
moved q-periods back in time and multiplied by AR coeﬃcients γ. This sum is
later increased by model parameter μ and white noise ϵ.
c
⃝Springer International Publishing Switzerland 2015
239
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_25

240
T. Vantuch and I. Zelinka
yt = μ +
p

i=1
(γiyt−i) + ϵt
(2)
MA polynomial does not contain any variable from a time set and it has
nothing to do with familiar known moving-average function. MA contains its
own set of MA coeﬃcients multiplied by model residuals and at the end the
whole sum is increased by model parameter and white noise.
yt = μ +
q

i=1
(θiϵt−i) + ϵt
(3)
The letter ”I” in ARIMA model stands for ”integrated”, which means the
level of diﬀerentiation of the time set.
The values of p and q parameters for AR and MA part can be obtained from
the behavior of auto-correlation (ACF) and partial auto-correlation (PACF)
function in the nth level of diﬀerentiation [1, 9]. There are alternative meth-
ods, that use patterns evaluation by symbols ”X” and ”O” in a matrix, like
SCAN [6], ESACF [7] to determine p and q values.
By all this tasks we can say that the ﬁnding of the suitable ARIMA model can
be harder and not very solid job and it could be the point of other improvements.
This opportunity is taken in this article. We will try to use an evolution
algorithm approach [10] to ﬁnd the suitable model for the time set. This approach
requires to have some clear evaluation function to know if the reproduced model
is good enough. The evaluation can be made of AIC [4] and BIC [5] criteria.
Both of them are designed like a likelihood penalization criteria
[2logL + kp]
(4)
where L stands for the maximal value of the likelihood function of the model,
p is count of model parameters, k has for AIC the value of 2 and for BIC the
value is log(n).
The next step it to estimate AR and MA coeﬃcients for our previously ob-
tained model. This coeﬃcients are normally gained from maximum likelihood
function [2] that is based on maximization of searched parameters’ probability:
L(Θ, x) = fΘ(x) = f(x2|x1) ∗f(x3|x2)... ∗f(xn|xn−1)
(5)
This maximum likelihood function can be replaced by other approaches too.
For this experiment there is used the particle swarm optimization algorithm [13].
With estimated coeﬃcients we will be able to create forecasting and evaluate
it to ﬁnd out if it is good enough or what can be improved.
2
Experiment Design
In this experiment, as it was mentioned before, the main idea was to work with
ARIMA model. The ﬁrst part of the experiment is to create ARIMA(p,d,q)

Evolutionary Based ARIMA Models for Stock Price Forecasting
241
model by the genetic algorithm [10]. For the GA we will use Java framework
ECJ [15], that is known solution for evolutionary based computing methods.
The creating of the model by individuals’ genotype and its evaluation by AIC
and BIC criteria is provided by Matlab econometric toolbox.
This evolved model will have the AR and MA coeﬃcients computed by MLE
method, so in the next step we will try to breed the new parameters by PSO
algorithm. As an evaluation function for this process will be the quality of the
forecasting of the time set evaluated by MSE.
In the end we will compare gained results by forecasting made by classic
approach with PSO and we will see whether it is worth.
2.1
Data Set
For this experiment historical data of a stock market title as Microsoft(MSFT)
were used. This historical data consist of records known as candles, that contains
information of some period of time, in this case the period of time was one day.
Each candle contains information about the highest price of the period (HIGH),
the lowest price of the period (LOW), the ﬁrst price of the period (OPEN),
the last price (CLOSE), the quantity of traded instruments (VOLUME) and of
course the time stamp of the period. There are more than ten years of obser-
vations but in this experiment, there was not used more than four years of its
length, mostly because of longer execution time of all the algorithms. All the
historical data were taken from yahoo ﬁnance [https://ﬁnance.yahoo.com].
3
Analysis of Historical Data
As it was said before, at ﬁrst we analyze our time set for stationarity, auto-
correlation and we create the suitable ARIMA model by ACF and PACF ap-
proach.
Because we worked with the time set of the stock prices, we can be sure,
that this time set will contain up-trends or down-trends, which indicates its
non-stationarity. Proceeded Dickey-Fuller test [16] rejected our hypothesis about
stationary behavior by resulted value γ∗= 0 as well.
Afterwards, we compute the ﬁrst diﬀerentiation and draw the process of ACF
and PACF function. It is expected the ”tailing” of the ACF, that will indicate
the value of q parameter and ”cutting oﬀ” of the PACF, that will indicate the
value of p parameter.
The chart of ACF shows, that there is no conﬁrmed correlation between time
set variables and it indicates stationary behavior of the time set. We check it by
new Dickey-Fuller test and the result γ∗= 1 conﬁrms the hypothesis that the
diﬀerentiated time set is no more non-stationary.
These charts’ progresses show some possible combinations of the future ARIMA
models. The p and q values will be lower by this method, because there was no
present of correlations between variables.

242
T. Vantuch and I. Zelinka
Fig. 1. Progress of ACF and PACF function after ﬁrst diﬀerentiation
4
Genetic Algorithm Approach for Model Building
The suitable ARIMA model for later testing is evolved by the genetic algorithm.
The Genetic algorithm (GA) belongs to evolution algorithms [10]. This set of
algorithms works like an imitation of the evolution theory described by Darwin.
The GA purpose is to evolve the best combination of variables from the scanning
set.
As a ﬁrst step of the GA there is a population of random individuals. Each
of the individuals is represented by the binary vector with the length of 12. The
transfer from genotype to phenotype is made by the splitting of whole vector
into three parts by four bits and each part belongs to some ARIMA variable like
ARIMA(p = 1011,d = 0010,q = 1100).
There is a ﬁtness function for an evaluation of individuals. By the theory
of evolution, the ﬁtness function simulates the natural selection. It has a pur-
pose that better individuals according to the ﬁtness will live longer and create
the new generation by its crossover. The one-point crossover [12] was used for
crossbreeding in this experiment.
The breeded individuals are eﬀected by an added mutation. It means that in
the random time, there can be changed one bit of an individual genotype. This
process brings to this algorithm some added randomness.
The ﬁtness function of this GA covers the requirement of the minimal value
of AIC and BIC and some added advantage for individuals with greater values
of p and q parameters. This advantage is added because the longer ARIMA
polynomial we will have as result, the more we can optimize by the PSO.

Evolutionary Based ARIMA Models for Stock Price Forecasting
243
fitness = (105/AIC + BIC) + p + q
(6)
The GA runs in this case by this steps:
1. Initialization - creating the ﬁrst generation from one prototype individual
2. Repeat in limited count of cycles
(a) Evaluate individuals:
i. Create the ARIMA model by the binary vector
ii. Estimate parameters by the default MLE function
iii. Return AIC and BIC by the minimal likelihood function value
(b) Proceed crossover on the selected individuals to create new generation
(c) Add random mutation
3. End of loop
4. Return the best individual
The GA returned in this experiment the ARIMA model with parameters
p = 12, d = 2 and q = 8. This result was compared to the previously chosen
models.
Table 1. ARIMA models evaluation
BIC
AIC
ARIMA(12,2,8) 485.6266 400.4396
ARIMA(1,1,0)
415.0880
403.9767
ARIMA(0,1,1)
415.0647
403.9534
ARIMA(2,1,1)
423.6821
405.1632
ARIMA(1,1,2)
428.7655
406.5429
ARIMA(2,1,3)
434.0470
408.1205
The model from the GA has softly greater BIC value but because of lower
AIC and much greater count of coeﬃcients, is was chosen as the model for the
later PSO optimization and prediction.
5
PSO Approach for Parameters Estimation
The evolutionary algorithm used for the breeding of the model coeﬃcients is the
particle swarm optimization [13].
The PSO is known, very powerful algorithm inspired by swarm intelligence.
Each individual is described by its position, velocity and memory of the latest
best position. This algorithm is not divided into generations, because individuals
are not dying and creating again, they are just moving during iterations in N-
dimensional space. Their moves are aﬀected by its previous best position or the
best position in its neighborhood [14]. The quality of the position is evaluated
by the objective function, and during every iteration, all the current positions
of all individuals are confronted with its best positions.

244
T. Vantuch and I. Zelinka
Obviously the count of dimensions of the searched space inﬂuences the count
of individuals, which are adjusted. In this case we have 22 dimensional space,
We need to breed the AR coeﬃcients, which are the vector of size 12. Than we
need to breed MA coeﬃcients, which are the another vector of size 8 and two
added variables - one for model constant μ and another for variance.
In the case of PSO, it was used the item of neighborhood [14] which means
that individual is not aﬀected by the best position of all individuals, but only
by the best position of smaller amount of closer individuals, its neighbors. The
neighborhood is not deﬁned by the distance between individuals, but randomly.
The present of the neighborhood brings to this system the opportunity ﬁnd the
best global maximum from more observed local maximums.
The objective function in our case is to decrease the value of MSE of forecasted
values to minimum.
The algorithm for parameters breeding:
1. Initialization - place all particles in n-dimensional hyper space and adjust to
them the velocity and the neighborhood
2. Repeat in limited count of cycles
(a) Evaluate the position of individual:
i. Create the ARIMA model from obtained parameters
ii. Create the forecasting for next n values
iii. Compare the forecasting values to real values and compute MSE
(b) Actualize particles position
(c) Actualize the previous best position
3. End of the loop
4. Return the particle with the best position
Concretely, each particles’ evaluation consists of the creating of the ARIMA
model and splitting time set. Time set has to be split into two parts. At ﬁrst there
is the input time set for ARIMA model and the second part will be compared
to this ARIMA model prediction. The predicted values are compared to the real
values (second part of the time set) and the best ﬁt is saved as the best position.
The best position will do the forecast of next n values and this will be com-
pared to forecast of the same ARIMA model but with coeﬃcients made by the
MLE. Arima model has to be estimated and it was provided in Matlab. The
same subset from time set was used in case of ”estimate” Matlab command and
also in the PSO approach.
6
Conclusions
This article covers the simple idea of working with ARIMA models by the ap-
proach of evolution algorithms. In the section of modeling, there was described
very simple kind of obtaining quite suitable ARIMA model without knowledge
of stationarity, ACF or PACF. The resulted model had suﬃcient low values of
likelihood penalization criteria like AIC and BIC.

Evolutionary Based ARIMA Models for Stock Price Forecasting
245
In the second part, there was a task to estimate AR and MA coeﬃcients in
”training part” of time set by the PSO algorithm and the best set of coeﬃ-
cients (PSO particle) was used to create prediction compared to classic ARIMA
prediction.
There were created two tests for this conclusion. Their diﬀerence is the size of
time set, adjusted for ”training phase”. The ﬁrst has ”training phase” of size of
ten and the second has this size increased to twenty. The count of particles and
iterations was in both test adjusted to same values (particles = 100, iterations
= 40).
There are some charts to provide our results.
Fig. 2. Chart of prediction when size of the ”learning phase” was 10
This chart describes the value of MSE between forecasting and real values of
time set.
As we can see, these tests do not prove that the estimation of the coeﬃcient
by PSO has signiﬁcantly improved the ARIMA results, but we can say that this
combination is at least comparable to standard ARIMA computing.
There are some areas for improvement of this approach. One of the weaknesses
of these tests was very low number of PSO iterations. PSO can obtain better
results in parallel computing, where it can provide more iterations in a shorter
time.
The other improvement can be gained by adjusting bigger time set for learning
phase and to work with bigger values of p and q of ARIMA model. Both of these
tasks require more computing resources.

246
T. Vantuch and I. Zelinka
Fig. 3. Chart of prediction when size of the ”learning phase” was 20
Fig. 4. Chart of the MSE on ﬁve predicted values during both tests
Finally, there are many other evolutionary algorithms like symbolic regression,
diﬀerential evolution, neural networks to try to combine them with ARIMA.
Acknowledgment. The following grants are acknowledged for the ﬁnancial
support provided for this research: Grant Agency of the Czech Republic - GACR
P103/13/08195S, is partially supported by Grant of SGS No. SP2014/159, VB -
Technical University of Ostrava, Czech Republic, by the Development of human
resources in research and development of latest soft computing methods and
their application in practice project, reg. no. CZ.1.07/2.3.00/20.0072 funded by
Operational Programme Education for Competitiveness.

Evolutionary Based ARIMA Models for Stock Price Forecasting
247
References
1. Box, G.E.P., Jenkins, G.M.: Time Series Analysis: Forecasting and Control.
Holden-Day, San Francisco (1976)
2. Ljung, G.M., Box, G.E.P.: The likelihood function of stationary autoregressive-
moving average models. Biometrika 66(2), 265–270 (1979)
3. Morf, M., Sidhu, G.S., Kailath, T.: Some new algorithms for recursive estima-
tion on constant linear discrete time systems. IEEE Transactions on Automatic
Control 19(4), 315–323 (1974)
4. Akaike, H.: A new look at the statistical model identiﬁcation. IEEE Transactions
on Automatic Control 19(6), 716–723 (1974)
5. Weakliem, D.L.: A Critique of the Bayesian Information Criterion for Model Se-
lection. Sociological Methods and Research 27(3), 359–397 (1999)
6. Tasy, R.S., Tiao, G.C.: Use of canonical analysis in time series model identiﬁcation.
Biometrika 72(2), 299–315 (1985)
7. Tasy, R.S., Tiao, G.C.: Consistent estimates of autoregressive parameters and ex-
tended sample autocorrelation function for stationary and nonstationary ARMA
model. Journal of the American Statistical Association 79(1), 84–96 (1984)
8. Hannan, E.J., Rissanen, J.: Recursive estimation of mixed autogressive-moving
average order. Biometrika 69(1), 81–94 (1982)
9. Hannan, E.J., Quinn, B.G.: The determination of the order of an autoregression.
Journal of the Royal Statistical Society B 41(2), 190–195 (1979)
10. Koza, J.R.: Genetic Programming: On the Programming of Computers by Means
of Natural Selection. MIT Press, Cambridge (1992)
11. Kinnear Jr., K.E. (ed.): Advances in Genetic Programming. MIT Press, Cambridge
(1994)
12. Poli, R., Langdon, W.B.: Genetic Programming with One-Point Crossover
13. Kennedy, J., Eberhart, R.C.: Particle swarm optimization. In: Proc. IEEE Int.
Conf. Neural Networks, Perth, Australia, pp. 1942–1948 (November 1995)
14. Kennedy, J.: The particle swarm: Social adaptation of knowledge. In: Proc. 1997
Int. Conf. Evolutionary Computation, Indianapolis, IN, pp. 303–308 (April 1997)
15. White, D.R.: Software review: the ECJ toolkit (March 2012)
16. Dickey, D.G.: Dickey-Fuller Tests. In: International Encyclopedia of Statistical Sci-
ence, pp. 385–388 (2011)

On Some False Chaos Indicators
When Analyzing Sampled Data
Petra Augustov´a, Zdenˇek Beran, and Sergej ˇCelikovsk´y
Institute of Information Theory and Automation, v.v.i.,
Academy of Sciences of the Czech Republic, Pod vod´arenskou vˇeˇz´ı 4, P.O. Box 18
182 08 Prague 8, Czech Republic
{augustova,beran,celikovs}@utia.cas.cz
Abstract. The main aim of this paper is to demonstrate a possible
risk of erroneous conclusions made throughout the process of the sam-
pled data analysis. In particular, it will be shown that the false chaotic
behavior of the sampled data may be incorrectly derived from the topo-
logical similarity with the already well known chaotic systems like that
of Lorenz or from the positivity of the largest Lyapunov exponent. The
possible faulty conclusions will be demonstrated using some examples
that were recently published in various literature sources.
1
Introduction
Analysis and control of complex technological systems as well as of systems aris-
ing in physics, natural environment, human society and other important ﬁelds
has become so complicated that the typical complex system has to be considered
as a “black box” and the necessary information concerning it is revealed through
a set of data that are conveniently sampled. The interpretation of the data sam-
pled in such a manner thereby appears to be an important task. The importance
of the appropriate rigorous interpretation is further strengthened when the na-
ture of the sampled data is chaotic. The main aim of this paper can be brieﬂy
summarized as follows. Suppose, in general, that a set of sequences of discretized
measurements is given, then the following question naturally arises: Is there an
approach to reveal its chaotic behavior once it exists? It is very well known that
a proof of the chaotic behavior is a quite peculiar task. In fact, the fundamental
question is: What is actually chaos or chaotic behavior of a system represented
by a sampled data? The question has not been convincingly answered till now,
in particular, due to the large number of existing various chaos deﬁnitions, see
Preliminaries later on. The deﬁnitions diﬀer accordingly to the nature of the
systems, e.g., discrete vs. continuous, one-dimensional vs. multidimensional, lin-
ear vs. nonlinear, etc. The full exact proof of the chaotic behavior of the one
of the ﬁrst observed chaotically behaving system - the Lorenz system [1] - has
been provided actually only quite recently [17],[18],[19]. In other words, the exact
proof of the chaotic behavior is a challenge, while quite frequent approach is that
if the graphical representation of an analyzed system is topologically similar to
c
⃝Springer International Publishing Switzerland 2015
249
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_26

250
P. Augustov´a, Z. Beran, and S. ˇCelikovsk´y
some well known chaotic attractor, then such a system is considered as chaotic,
too. The famous double scroll being the attractor of Lorenz system is quite of-
ten addressed in such a way. Another erroneous but generally very often used
argument to claim a given system to be chaotic is the positivity of its largest
Lyapunov exponent.
The rest of the paper is organized as follows. Some preliminaries are given in
the following section. Using convenient examples, Sections 3 and 4 will demon-
strate that the above mentioned arguments for the chaos existence may be mis-
leading when interpreting the sampled data. The ﬁnal section draws some con-
clusions.
2
Preliminaries
First, let us introduce some deﬁnitions and connections between them and let us
note some wrong interpretations of the chaos widely present in the literature. At
the very beginning, one has to distinguish between the chaotic behavior and the
stochastic behavior of the system. There are many papers where the authors deal
with this problem, see, e.g., [21],[22],[23]. To this end, it is important to underline
that the current paper will be devoted to the deterministic chaos only. Consid-
ering the linear systems versus non-linear systems, it is very well known that in
the case of ﬁnite dimension of the phase space the following statement is true.
In the case of linear systems and one-/two-dimensional autonomous non-linear
systems the chaotic behavior of these systems can not occur due to the Hartman-
Grobman theorem, as well as due to the Poincare-Bendixson theorem. Diﬀerent
situation comes up when the linear system is inﬁnite-dimensional. Then, see,
e.g., [2], such a system can be chaotic in the Wiggins or Devaney sense, see
below. Moreover, it is well known that one-dimensional discrete systems, unlike
continuous one, can embody chaotic behavior, see e.g. [6].
In order to present some of the chaos deﬁnitions, let us introduce some basic
“building stones” ﬁrst.
Deﬁnition 1. A dynamical system (se, e.g. [10]), is a tuple (T, M, Φ) where
T is a monoid (a semigroup with a neutral element), written additively, M is a
set and Φ is a function
Φ : U ⊂T × M →M
with
I(x) = {t ∈T |(t, x) ∈U}
Φ(0, x) = x
Φ(t2, Φ(t1, x)) = Φ(t1 + t2, x) for t1, t2, t1 + t2 ∈I(x) .
The set M is a manifold locally diﬀeomorphic to a Banach space. If T is an
open interval in the real numbers R the tuple (T, M, Φ) is a continuous time
dynamical system of a ﬂow. If T are the integers then tuple (T, M, Φ) is a
discrete-time dynamical system or a map or a cascade.

On Some False Chaos Indicators When Analyzing Sampled Data
251
Deﬁnition 2. Let (M, d) be a metric space without isolating points. Then the
dynamical system (T, M, Φ) where Φ : U ⊂T × M →M, has sensitive depen-
dence on initial conditions, see, e.g. [6], if there exists a number δ > 0 such
that for all x ∈M and ϵ > 0 exists some y ∈M such that d(x, y) < ϵ and such
that for some n ≥0 it holds d(Φnx, Φny) > δ in the case of maps or if there
exists a T > 0 such that d(Φ(t, x), Φ(t, y)) > δ ∀t ≥T in the case of ﬂows.
Deﬁnition 3. A dynamical system (T, M, Φ) where Φ : U ⊂T × M →M, is
topologically transitive,see, e.g. [6], if for every pair V, W of nonempty open
subsets of M there exists some n ≥0 such that Φn(V ) ∩W ̸= ∅in the case of
maps or there exists a T > 0 such that Φ(t, V ) ∩W ̸= ∅∀t ≥T in the case of
ﬂows.
Deﬁnition 4. Let (X, B, μ) be a probability space and let a collection of mea-
surable subsets ζ = {Cα ∈B | α ∈I}, where I is a ﬁnite or countable set of
indices, be a measurable partition. Then the Kolmogorov entropy, see, e.g.
[11], of a measurable partition ζ is given by
h(ζ) =

μ(Cα)|α∈I >0
μ(Cα log(μ(Cα)) ≥0.
For a diﬀeomorphism Ft : X →X, t ≥0, of a compact set X ⊂Rn, we assume
that there exists an ergodic invariant measure with respect to Ft. Let DFt(x)
denote the derivative of Ft at x, which is represented by the Jacobian matrix of
Ft. Then the multiplicative ergodic theorem of Oseledec, see [4], guarantees that
at almost every x ∈X there exist a decomposition of Rn into Rn = r
i=1 Ei(x)
and constants λ1 < · · · < λr such that the limits
λi = lim
t→∞
1
t log10 ∥DFt(x)vi ∥
exist for some nonzero vector vi ∈Ei(x), 1 ≤i ≤r. Note that Ei(x) is the
subspace of Rn and the symbol  denotes the direct sum. The constants λi, 1 ≤
i ≤r, which depend neither on vi nor on x, are Lyapunov exponents. The
number max1≤i≤r λi is the largest Lyapunov exponent.
As already noted, various deﬁnitions of chaos are possible, some of them are
as follows.
Deﬁnition 5 (Smale). The dynamical system is chaotic in the Smale sense,
[3], if its part is topologically conjugated with the Smale horseshoe.
Remark 1. The deﬁnition of the Smale horseshoe has a constructive nature and
it was ﬁrst time published in [5].
Deﬁnition 6 (Devaney). The dynamical system is chaotic in the Devaney
sense, [6], if it
1. has sensitive dependence on initial conditions
2. is topologically transitive

252
P. Augustov´a, Z. Beran, and S. ˇCelikovsk´y
3. its set of periodic points is dense.
Remark 2. It can be shown that the sensitive dependence on initial conditions
can be avoided.
Deﬁnition 7 (Wiggins). The dynamical system is chaotic in the Wiggins
sense, [7], if
1. has sensitive dependence on initial conditions
2. it is topologically transitive.
Deﬁnition 8 (Gulick). The dynamical system is chaotic in the Lyapunov
sense, [8], if it it satisﬁes at least one of the following conditions
1. it has a positive Lyapunov exponent at each point in its domain that is not
eventually periodic
2. it has sensitive dependence on initial conditions.
Deﬁnition 9 (Kolmogorov). The dynamical system is chaotic in the Kol-
mogorov sense, [9], if it has positive Kolmogorov entropy.
One can often ﬁnd other chaos deﬁnitions in the literature quite often reﬂect-
ing the wishes and/or needs of the respective authors.
Looking at the deﬁnitions above the reader can see that the exact proof of
the chaotic behavior of the system that is represented by the sets of sampled
data is generally a very tough task. Consequently, people try to use graphical or
computational methods instead of the analytical/topological tools to reveal the
chaotic behavior. The very well known graphical output is the Lorenz attractor
that graphically evokes the double scroll. From time to time that evocation leads
some authors to the conclusion that every system that embodies such a pattern
has to be chaotic. It will be shown in Section 3 that such conclusion can be
wrong. The same situation can occur when the sampled data are concentrated
into a small volume where it behaves “strange” - the data is cumulated inside a
small phase space where they behave irregularly on the ﬁrst sight as it will be
seen in Example 3 of Section 3.
Applying the Deﬁnition 8 to reveal the chaos by analyzing the sampled data
seems to make the task much easier. Basically, that deﬁnition states that the
system is chaotic if the largest Lyapunov exponent is positive. As it will be shown
in Section 4, such a statement may be generally misleading when analyzing a
priori given sampled data. Nevertheless, it is still frequently mechanically used
as an indicator of the chaos - one can encounter usage of such an indicator even
in the textbooks like [8] (page 86, Deﬁnition 2.3.), repeated for convenience as
Deﬁnition 8 above. Some other sources where the positivity of the Lyapunov
exponent is considered as the only crucial criterion for the chaos existence are
as follows:
“Any system containing at least one positive Lyapunov exponent is deﬁned
to be chaotic”, see [12].
“A positive largest Lyapunov exponent indicates the existence of chaos”, see
[13].

On Some False Chaos Indicators When Analyzing Sampled Data
253
“A system is chaotic if it has at least one positive Lyapunov exponent”, see
[14].
“If the system has at least one positive Lyapunov exponent, it indicates the
chaos. If the largest Lyapunov exponent is negative then the orbits converge
in time and system is insensitive to initial conditions. If it is positive, then
the distance between adjacent orbits grows exponentially and system exhibits
sensitive dependence on initial conditions, so it is chaotic we say”, see [15].
“By deﬁnition, any system containing a positive Lyapunov exponent is
chaotic”, see [16].
Nevertheless, one should note that correct computation of the largest Lya-
punov exponent is almost impossible for a priori given data time series produced
by some unknown system and sampled at given sampling rate that can not be
arbitrarily chosen. Therefore the above statements are often practically mislead-
ing. Some examples and analysis are given in the rest of the paper.
3
Geometrical Similarity with Lorenz Attractor
First of all, often the famous Lorenz attractor is chosen as an etalon for the
chaos indication. Recall that the evolution of the Lyapunov dynamical system
is described by a set of the three bi-linear diﬀerential equations of the form
˙x = σ(y −x)
˙y = ρx −y −xz
˙z = xy −βz
(1)
where σ = 10, ρ = 28, β = 8/3. One of the 2D projections of its trajectories in
the 3D phase space is shown in Figure 1. The following example shows that some
data time series produced in a quite simple way may appear during the analysis
as the chaotic one due to geometric resemblance of the Lorenz attractor.
−20
−15
−10
−5
0
5
10
15
20
−25
−20
−15
−10
−5
0
5
10
15
20
25
Fig. 1. Lorenz attractor

254
P. Augustov´a, Z. Beran, and S. ˇCelikovsk´y
Example 1. Consider the following 2D linear dynamical system:
¨y + 2β ˙y + (ω2 + β2)y = 0
where y(t) is the scalar state, β = ln(2) is the decay rate, and ω = 2π is the
frequency of the damped oscillations. Let y(t) be a solution of the above equation
on interval [0, T ], T = 100 with starting point y(0) = 1, ˙y(0) = 10 solved using
the Runge-Kutta ode45 Matlab software. As the solution, a vector y(i) of the
size N = 1333 has been obtained. A transformation of the solution y(i) gets the
following new vector x(i):
x(i),
i = 1, · · · , 2N,
where
x(i) = y(N + 1 −i),
i = 1, · · · , N,
x(i) = 0.5 −y(i),
i = N + 1, · · · , 2N
and the sampled data are represented by the above vector x(i), i = 1, · · · , 2N. It
is worth to mention that the sampling is irregular due to the varying integration
step of the integration procedure represented by the Runge-Kutta integration
method as one can see on the ﬁgure 3 using hexagonals.
Very often, a high dimensional physical system is only observable through
a single scalar variable. The method of time delay embedding, see, e.g. [20], is
widely applied to estimate the evolution of the underlying vector ﬁeld. From a
scalar time series {xt}N
t=1 of N observations one reconstructs a vector time series
with evolution topologically equivalent to the original system via the transfor-
mation xt →(xt, xt−τ, xt−2τ, · · · , xt−(de−1)τ). The quantity τ is called the lag
or time-delay and the quantity de is called the embedding dimension. Then,
the Takens Time-Delay Embedding Theorem, see [20], is used to embed the scalar
time series into higher dimensional physical space. Thenceforth one can imagine
that the sampled data are represented by the vector x that was acquired as a
measurement provided on some ”black-box”. It is a standard praxis to visualize
an one dimensional time series accordingly to the following picture when analyze
the time series where the “time”-shift M is the lag. The assessment of the lag
value is a serious task as, e.g., the evaluation of the largest Lyapunov exponent
or the assessment of the dimension of the trajectory space is heavily dependent
Fig. 2. Data visualization using a lag M

On Some False Chaos Indicators When Analyzing Sampled Data
255
−1.5
−1
−0.5
0
0.5
1
1.5
2
−10
−8
−6
−4
−2
0
2
4
6
8
10
Fig. 3. Example 1
on it. The value of M = 52 has been chosen. Using the time delay-embedding as
a result. Figure 3 has been obtained.
One can see now the similarity of the graﬁcal interpretation of these data to
the Lorenz attractor despite this data were created in a rather simple way that
can not produce chaotic phenomena.
Example 2. In [24] a construction of a new signal comes out from the following
function:
ψ(t) =

−
1
2−t+exp10t cos(2πt) + 5 exp−10t2 t < 0
−
1
2−t+exp10t + 5 exp−10t2
t ≥0
After some manipulations one gets the following ﬁgure, Fig. 4. One can see
the similarity with the Lorenz signal Fig. 1 also in this case, though the original
data source is apparently no a chaotic one.
Fig. 4. Example 2

256
P. Augustov´a, Z. Beran, and S. ˇCelikovsk´y
The third example shows an irregular behavior of the system although the signal
is fully regular.
Example 3. In [25] a pulsations of the cepheid models is analyzed. As a recon-
structed signal the following one has been used:
R(t) = a1 cos(ω1t) + a2 cos(ω2t) + a3 cos(ω3t)
+ 1
2a1a3[cos(ω1 + ω3)t + cos(ω1 −ω3t)]
+ 1
10a2a3[cos(ω2 + ω3)t + cos(ω2 −ω3t)]
+ 1
10a1a2[cos(ω1 + ω2)t + cos(ω1 −ω2t)]
+ 1
10a2
1 cos(2ω1t) + 1
10a2
2 cos(2ω2t)
V (t) = dR(t)
dt
(2)
where a1 = 1, a2 = 0.6, a3 = 0.2, ν1 = 0.1, ν2 = 0.048, ν3 = 0.0252, ωi = 2πνi.
When plotting R(t) versus V (t) one gets the following picture.
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−1
−0.5
0
0.5
1
1.5
Fig. 5. Example 3
The signal is concentrated into a rather small volume and one can see a
irregular signal behavior on the ﬁrst sight though the signal is multi-periodic
only.
4
Lyapunov Exponent of Sampled Data
Lyapunov exponent serves as a measure that characterizes a regularity of the
dynamical system behavior. Very roughly speaking, Lyapunov exponent mea-
sures a time evolution of the initially close trajectories in the phase space. The

On Some False Chaos Indicators When Analyzing Sampled Data
257
scenario of the trajectories’ distance can be two-folding: either the trajectories
can stay close when the time goes to inﬁnity or the trajectories draw apart in an
exponential way in the case of positive Lyapunov exponent. There is an indica-
tion of possibly chaotic behavior in the second case. But the sign of the largest
Lyapunov exponent is not a suﬃcient one as we will see. Looking at diﬀerent
paper sources it is interesting that in the case of the Lorenz system the maximal
Lyapunov exponent can get the values scattered between the values 0.8438 and
1.751. In any case, the positivity of the Lyapunov exponent coincides with the
chaotic behavior of the Lorenz system as it has been proven in [17], [18], [19].
On the other side, evaluating the maximal Lyapunov exponent in the case of
the above three examples one gets the following values:
Example # Largest Lyapunov exponent
Example 1
0.1389
Example 2
0.693
Example 3
0.0871
Positive values of the largest Lyapunov exponent could indicate chaotic behavior
of the corresponding systems. But, as it was shown, the systems are far from
being chaotic in the sense of the chaos deﬁnitions presented in the preliminary
part.
Computations done in section 3 and in section 4 were provided in Matlab
software [26]. The Lyapunov exponents were calculated using Matlab’s Lyapunov
Exponents Toolbox (LET), [27].
5
Conclusions
In this paper, the examples of misinterpretations of sampled data to indicate and
analyze a possible chaotic behavior were considered. In particular, it was shown
that topological resemblance of the graphical representation of scalar data time
series plotted in 2-d using suitable time lag to the well-known Lorenz attractor
may be present even for simply generated and clearly non-chaotic data. Further-
more, the examples of non-chaotic systems having the numerically computed
positive largest Lyapunov exponent were provided as well. In such a way, it was
demonstrated that careless analysis of the sampled dynamical systems data may
lead to false conclusions which is, moreover, often the case in various published
results.
Acknowledgments. This work is supported by Czech Science Foundation
through the research grant no. 13-20433S.
References
1. Lorenz, E.N.: Deterministic nonperiodic ﬂow. J. Atmos. Sci. 20, 130–141 (1963)
2. Grosse-Erdmann, G.-K., Manguillot, A.P.: Linear Chaos. Springer (2011)

258
P. Augustov´a, Z. Beran, and S. ˇCelikovsk´y
3. de Almeida, A.M.O.: Hamiltonian Systems: Chaos and quantization. Cambridge
University Press, Cambridge (1988)
4. Oseledec, V.I.: A multiplicative ergodic theorem. Lyapunov characteristic numbers
for dynamical systems. Trans. Moscow Math. Soc. 19, 197–231 (1968)
5. Smale, S.: Diﬀerentiable dynamical systems. NBulletin of the American Mathe-
matical Society 73(6), 747–817 (1967)
6. Devaney, R.: An introduction to chaotic dynamical systems. Addison–Wesley, New
York (1989)
7. Wiggins, S.: Chaos transport in dynamical systems. Springer, New York (1992)
8. Gulick, D.: Encounters with chaos. McGraw-Hill, New York (1992)
9. Schuster, H.: Deterministic chaos. VCH, Weinheim (1988)
10. Kotik, K., Lectures, L.: on Dynamical Systems, Structural Stability and their Ap-
plications. World Scientiﬁc Publishing Co. Pte. Ltd. (1992)
11. Katok, A., Hasselblatt, B.: Introduction to the Modern Theory of Dynamical Sys-
tems. Cambridge University Press (1997)
12. Shin, K., Hammond, J.K.: The instantaneous Lyapunov exponent and its applica-
tion to chaotic dynamical systems. Journal of Sound and Vibration 218(3), 389–403
(1998)
13. Yalamova, R., Qi, L., Wang, L., Chen, Y., Mathews, S., Gong, G.: Detecting Chaos
in Financial Time Series. Tech. Print, 1–9 (2006)
14. de Wijn, A.S.: Chaos in systems with many degrees of freedom. PhD thesis, Utrecht
University (2004)
15. ¨Ozer, A.B., Akin, E.: Tools for detecting chaos. SA¨U Fen Bilimleri Enstit¨u s¨u
Dergisi 9.Cilt, 1.Say˜o, 60–66 (2005)
16. Lindlay, D.H., Campbell, A.: A chaos approach to bankruptcy prediction. J. of
Appl. Business Research 12(4), 1–9 (1996)
17. Mischaikow, K., Mrozek, M.: Chaos in the Lorenz equations: A computer-assisted
proof. Bulletin AMS 32(1), 66–72 (1995)
18. Mischaikow, K., Mrozek, M.: Chaos in the Lorenz equations: A computer assisted
proof. Part II: Details. Mathematics of Computation 67, 1023–1046 (1998)
19. Mischaikow, K., Mrozek, M., Szymczak, A.: Chaos in the Lorenz equations: A
computer assisted proof. Part III: Classical Case Parameter Values. J. of Diﬀ.
Equations 169, 17–56 (2001)
20. Robinson, J.C.: Dimensions, Embeddings, and Attractors. Cambrigde University
Press (2011)
21. Casdagli, M.: Chaos and Deterministic versus Stochastic Nonlinear Modeling.
Santa Fe Institute, SFI Working Paper (July 029, 1991)
22. Puente, C.E., Obren, N., Sivakumar, B.: Chaos and stochasticity in deterministi-
cally generated multifractal structures. Fractals 10(1), 91–102 (2002)
23. Neimark, Y.I., Landa, P.S.: Stochastic and chaotic oscillations. Mathematics and
Its Applications Soviet Series, vol. 77. Springer, Netherlands (1992)
24. Hirata, Y., Judd, K.: Constructing dynamical systems with speciﬁed symbolic dy-
namics. Chaos: An Interdisciplinary Journal of Nonlinear Science 5(3), 033102-1–
033102-6 (2005)
25. Kov´acs, G., Buchler, J.R.: Regular and irregular nonlinear pulsations in population
II cepheid models. The Astrophysical Journal 334, 971–994 (1988)
26. Matlab R2014a release. The MathWorks Inc., Natick (2014)
27. http://www.mathworks.com/matlabcentral/fileexchange/233-let (1998)

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Part III 
 
Complex Biological Systems 
 
 
 
 
 
 
 
 
 
 
 
 

Multifractality in Imaging: Application
of Information Entropy for Observation of Inner
Dynamics Inside of an Unlabeled Living Cell
in Bright-Field Microscopy
Renata Rychtarikova1, Tomas Nahlik1, Rebecca Smaha2, Jan Urban1,
Dalibor Stys Jr.3, Petr Cisar1, and Dalibor Stys1
1 Institute of Complex Systems, South Bohemian Research Center of Aquaculture
and Biodiversity of Hydrocenoses, FFPW, University of South Bohemia in Ceske
Budejovice, Zamek 136, 373 33 Nove Hrady, Czech Republic
{rrychtarikova,stys}@frov.jcu.cz
2 Department of Chemistry, Princeton University, Princeton, New Jersey 08544, USA
3 Department of Measurement, Faculty of Electrical Engineering,
Czech Technical University in Prague, Technicka 2, 166 27 Prague 6, Czech Republic
Abstract. The theoretical background of bright ﬁeld optical microscopy
is not described to the extent that would allow the extraction of as
many features of the original object from the image as possible. In
this article, we present the determination of image features based on
a general assumption that images transmitted by an optical microscope
have multifractal character. In order to determine the borders of the de-
terminable point spread function, we derived a Point Divergence Gain
(PDGα,x(l),y(l)) variable from the Renyi entropy. This variable calcu-
lates image points that carry the same information in consequent images
captured upon moving the object along the lens’ optical axis (z-scan).
In this way, we may precisely identify the border of the point spread
function of immovable identiﬁable objects.
Keywords: Renyi
entropy,
multifractality,
point
spread
function,
bright-ﬁeld microscopy.
1
Introduction
The goal of microscopy in general is to obtain images of observed objects’ po-
sitions that exactly copy their actual positions and, if possible, also provide
information about the objects’ properties at a particular location. In addition,
the goal of biological research is to determine the states of elementary units that
determine the state of the organism at a particular level: in the case of a cell,
the elementary units are organelles and their constituents. The latter limits our
ability to manipulate the sample for a microscopy experiment. Instead of techni-
cally optimizing an experiment, in most cases we must maximize the information
yield from a given image.
c
⃝Springer International Publishing Switzerland 2015
261
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_27

262
R. Rychtarikova et al.
In many biological experiments, evaluating the results depends on how the
image content is interpreted. This, in practice, means equating the object ob-
served in the dynamic optical image of the cell with the analogous object in
the ﬁxed and stained electron microscopic image of a microtome section. The
obvious physical limit of this approach is a property of the optical microscope,
which is known as the diﬀraction limit. In this article, we discuss bright ﬁeld op-
tical microscopy, which is the most commonly used type of microscopy and may
be used for unlabeled (or native) specimens. In the discussion section, we take
advantage of another of its features, the relative intelligibility of the diﬀraction
pattern formed after transmission of light by the microscope [1].
Instead of examining the imaging model and attempting to reconstruct the
positions of elementary diﬀracting objects in the cell, we have proposed assuming
only the multifractal character of the image [2,3]. The origin of multifractality
is twofold: it arises from the self-similarity of point spread functions of objects
and from the self-organization of observed cells.
Here, we describe an algorithm for determining the spatial extent of point
spread functions of stable structures inside living cells. The spatial extent of an
image of an individual object is determined by a newly introduced approach: a
Point Divergence Gain (PDGα,x(l),y(l)) calculation giving the last discriminable
intensity level limited by the discretization of the camera chip at each level of
the z-series. By this approach, we obtain series of images that depict points in
which the information in the image systematically varies, describing the inner
dynamics inside the living cell, in this case immovable objects.
2
Results and Discussion
2.1
Image Processing
In this article we describe an algorithm that uses properties of the point spread
function to achieve a more detailed analysis of optical bright-ﬁeld microscopy
images of light-diﬀracting objects, mainly the static ones. For this purpose, an
original z-stack of 12-bit images of a MG-63 osteosarcoma cell, obtained using
a versatile sub-microscope (Optax Ltd., CZ) equipped with a Basler ACA2000-
340kc color digital camera, is used as an example.
Image processing of the z-stack of the MG-63 cell is shown in Fig. 1. First,
columns that serve as internal calibration of the noise in the camera were re-
moved. Then, 92 relevant images corresponding to 100-nm steps of the micro-
scope stage, which is controlled by piezomechanics, were selected from the orig-
inal 155-image z-stack through consulting the z-position stored in each image’s
EXIF ﬁle (custom adopted). De-mosaicing was performed as published previ-
ously [4] by shrinking the number of points in the image to a quarter of the
original one: for each four pixels, the red and blue channel levels were adopted
to the intensity level captured by the respective chip pixel. The green level was
set to the arithmetic average of the two corresponding green levels. By this proce-
dure, we obtained a z-stack of least color map images of a living MG-63 cell. For

Multifractality in Imaging: Application of Information Entropy
263
further processing, we used individual channels as sources of independent infor-
mation about the observed signal. The size of the ﬁnal pixel after de-mosaicing
was 68 × 68 nm2.
Fig. 1. Image processing of a z-stack of images of a MG-63 living cell obtained using
bright-ﬁeld microscopy (Institute of Complex Systems FFPW USB, CZ)
The original 12-bit images were converted to 8-bit resolution by shrinking the
original 12-bit signal into an 8-bit signal through re-scaling all occupied intensity
levels in the whole image series. This conversion method (least information loss
– LIL) allows the data to be viewed in commercial software with minimal loss of
information for subsequent image recalculation. The shifted 8-bit images were
aligned (registered) and consequently, the single MG-63 cell was cut from the
background. This process was performed using the NanoscopeDataProcessing
software package (Institute of Complex Systems FFPW USB, CZ).
We analyzed the information content of each image using the Between Entropy
Calculator software package [5]. We assumed the most general distribution that
can be expected in self-organized objects such as living cells – the multifractal
distribution. Here, we introduce a new variable derived from the Renyi entropy
– the Point Divergence Gain (PDGα,x(l),y(l)) – which quantiﬁes the diversity,
uncertainty, and randomness in 3D space:
PDGα,x,y =
1
1 −α log2
n
i=1 pα
i
n
i=1 pα
i,x(l+1),y(l+1)
(1)
This variable calculates the information change achieved by replacing a partic-
ular point at position (x,y) from an image (l) by the point at the same position
in the next image (l+1). This may result in either increased or decreased infor-
mation content. The former occurs when the intensity of the point corresponding
to a given object in the ﬁrst image has lower probability of occurrence than the
intensity of the same point in the following image, and vice versa. As seen in Fig.
2 together, if the information content increases, each pair of consecutive z-stack
images at each α-coeﬃcient gives rise to two PDG-transformed images – with the
loss (the -Neg-1 sub-image) and increase (the -Pos-1 sub-image) of information.
In the formula, the uncertainty and randomness of the described system are
connected to the probability of the occurrence of intensity i in the original image
(l) (pi,x(l),y(l)) and to the probability of the given intensity in the same image
where the examined point at coordinates (x,y) was replaced by the point at the
same location in the next image (l+1) (pi,x(l+1),y(l+1)), respectively.

264
R. Rychtarikova et al.
The dimensionless coeﬃcient α is related to the multifractality of the point
spread functions. To analyze the cell interior, PDG at α = 4 (PDG4,x(l),y(l) in
Fig. 2) is the most suitable because it is where the highest number of occupied
intensity levels (the highest number of separable groups of diﬀerent information
contribution) were found. This shows the advantage of using the Renyi entropy
instead of, for instance, the best known Shannon entropy, which is the limiting
value of Renyi entropy as α →1. In Shannon entropy, numerous image levels
would be merged. Nevertheless, even in the case of α = 4, we also observed
numerous merged intensity levels which separate at diﬀerent α levels.
Fig. 2. Detail (29.24 × 32.64 µm2) of the green channel of the original and PDG-
transformed z-stack images of a MG-63 living cell around the focus plane (ICS FFPW
USB, CZ). Original microscopic images Orig-1 (55th image in the z-stack) and Orig-2
(56th image in the z-stack) produce PDG-Neg-1 (negative information change) and
PDG-Pos-1 (positive information change) as a result of exchanging pixels at the same
position in the original images.
2.2
Analysis of Zero Digital Levels of PDG-Transformed Images
The image is composed of contributions from diﬀerent image levels. The
PDGα,x(l),y(l) calculation helps to indicate where the image of a certain ob-
served object is located and serves to highlight the object borders, even those
which are not evident in the original data sets. Thus, the change in an image

Multifractality in Imaging: Application of Information Entropy
265
series through a z-scan is the result of either an object’s appearance or disap-
pearance from the image or changes in the point spread function of the object
with the distance from its ideal focal plane.
In this article, we limit ourselves to the detection and analysis of point spread
functions of static large homogenous diﬀracting organelles (Fig. 3), which are
projected in the green channel of the original microscopic images. Their PDG-
transformed sub-images are shown in Fig. 2.
Fig. 3a depicts four PDG4,x(l),y(l) = 0 sub-images. PDG-Neg-1 and PDG-
Pos-1 were extracted from the PDG-transformed sub-images shown in Fig. 2;
analogously, PDG-Neg-2 and PDG-Pos-2 were thresholded in the green channel
of PDG-sub-images calculated from Orig-2 and Orig-3 images (i.e. the 56th and
57th
z-stack
images,
respectively).
Rounding
during
re-scaling
of
the
PDG4,x(l),y(l)-values from double-precision ﬂoating point format to 8-bit un-
signed integer number format resulted in a depiction of points diﬀerent from 0.
The points that correspond to 0 are found through the intersection of
PDG4,x(l),y(l) = 0 digital levels of each pair of PDG-sub-images (Fig. 3b). By
this procedure, we identiﬁed the points in the point spread function for which
the image is homogeneous within the precision of the 8-bit intensity section and
the distance along the z-axis spanning two consequent physical levels. In fact,
points that truly do not change between two consequent images in a z-scan are
very rare and their quantity obviously depends on the step size chosen for the
z-scan as well as the digital camera’s discretization.
Consequently, the intersection of 0 points of two consecutive PDG-1 and PDG-
2 images (Fig. 3c) gives us static objects and other intensities that did not
change in two consecutive original images. This complementarity is the result
of calculating PDG-values from two consecutive original z-stack images (Fig. 2).
We can ﬁnd two types of objects in Fig. 3c: those for which we may also detect
parts of the point spread function corresponding to a second diﬀraction ring, and
those for which only the part of the point spread function that contains the core
of the diﬀracting object itself was observed. The former objects behave exactly
as expected by the Abbe model [6]. For the behavior of objects not showing
secondary interference, another model is needed. We expect that other extant
objects not showing secondary interference are composed from a relatively dilute
solution of small diﬀracting objects of a size reported to be observable by video-
enhanced microscopy [7]. It should be noted that objects detectable by video-
enhancement are usually detected without any secondary diﬀraction rings. With
regard to their observability, the extent of the eﬀective point spread function of
these objects under a given technical set-up must be smaller than that proposed
by the Abbe [6] or Nijboer-Zernike [1,8,9] models. Since there is no real reason
why there should be two models describing the imaging of relatively small and
large objects, this ﬁnding requires additional examination.
This method of image processing connects to the complementarity of zero
levels in PDG-transformed images, emphasizes variability in structure details,
and is mathematically distinguishable by semantic measurement. It can thus
potentially be used for segmentation and 3D reconstruction of organelles.

266
R. Rychtarikova et al.
Fig. 3. Analysis of immovable organelles in a z-stack of 29.24 × 32.64 µm2
images of a MG-63 living cell at PDG4,x(l),y(l) = 0 levels of the green channels. (a)
PDG4,x(l),y(l) = 0 intensities thresholded in PDG-sub-images shown in Fig. 2; these
points were formed by re-scaling PDG-values from double-precision to unsigned inte-
gers. (b) Intersection of PDG-Neg-1 and PDG-Pos-1 sub-images (PDG-1) and PDG-
Neg-2 and PDG-Pos-2 sub-images (PDG-2); these points show intensities that did not
change between two consecutive images (Orig-1 →Orig-2, Orig-2 →Orig-3) of the z-
stack and correspond to diﬀracting objects of the size down to 68 × 68 × 100 nm3. (c)
PDG4,x(l),y(l) = 0 points calculated as the intersection of PDG-1 and PDG-2 images.
It depicts large homogeneous static objects and other unchanged intensities between
two consecutive images in a z-stack.

Multifractality in Imaging: Application of Information Entropy
267
Acknowledgment. This work was partly supported and co-ﬁnanced by the
South Bohemian Research Center of Aquaculture and Biodiversity of Hydro-
cenoses (CZ.1.05/2.1.00/01.0024), LO1205 from MEYS CR (NPU I program),
and Postdok JU (CZ.1.07/2.3.00/30.0006).
References
1. Braat, J.J.M., Dirksen, P., Janssen, A.J.E.M.: Assessment of an Extended Nijboer-
Zernike Approach for the Computation of Optical Point-Spread Functions. J. Opt.
Soc. Am. A 19, 858–870 (2002)
2. Stys, D., Urban, J., Vanek, J., Cisar, P.: Analysis of Biological Time-Lapse Micro-
scopic Experiment from the Point of View of the Information Theory. Micron 42,
360–365 (2011)
3. ˇStys, D., Jizba, P., Pap´aˇcek, ˇS., N´ahl´ık, T., C´ısaˇr, P.: On Measurement of Internal
Variables of Complex Self-Organized Systems and Their Relation to Multifractal
Spectra. In: Kuipers, F.A., Heegaard, P.E. (eds.) IWSOS 2012. LNCS, vol. 7166,
pp. 36–47. Springer, Heidelberg (2012)
4. Tkacik, G., Garrigan, P., Ratliﬀ, C., Milcinski, G., Klein, J.M., Seyfarth, L.H., Ster-
ling, P., Brainard, D.H., Balasubramanian, V.: Natural Images from the Birthplace
of the Human Eye. PLoS One 6(6), e20409 (2011)
5. http://www.expertomica.eu/software.php
6. Abbe,
E.:
Beitr¨age
zur
Theorie
des
Mikroskops
und
der
mikroskopischen
Wahrnehmung. Archiv f¨ur Mikroskopische Anatomie 9(1), 469–480 (1874)
7. Lichtscheidel, I.K., Foissner, I.: Video Microscopy of Dynamic Plant Cell Organelles:
Principles of the Technique and Practical Application. J. Microsc.-Oxford 181,
117–128 (1996)
8. Zernike, F.: The Concept of Degree of Coherence and Its Application to Optical
Problems. Physica 5, 785–795 (1938)
9. Nijboer, B.R.A.: The Diﬀraction Theory of Aberrations. Ph.D. dissertation, Uni-
versity of Groningen, Groningen, The Netherlands (1942)

Trajectory Tracking for Genetic Networks
Using Control Theory
Natalja Strelkowa
Boehringer Ingelheim Pharma GmbH and Co. KG
Rhineland-Palatinate, Germany
natalja.strelkowa@gmail.com
Abstract. Synthetic biology has impressively progressed during the last
decades making it possible to rationally design and implement genetic
networks with new functionalities in living microorganisms. With these
new technologies the expression of genes can be observed using ﬂuores-
cent markers and inﬂuenced using light ﬂashes and photo-active expres-
sion inducers. In this contribution, we suggest the implementation of
external feedback control for dynamic trajectory tracking of a synthetic
genetic network. The feedback control can be implemented in living mi-
croorganisms using ﬂuorescent markers for system readout and photo-
active gene expression inducers for external control signals. In particular
we show that hierarchical or sequential design for synthetic gene net-
works makes controlled trajectory tracking possible using the readout
and control actions on few instead of all genes. Optimised trajectory
tracking opens the possibility to interact and inﬂuence genetic networks
in a very precise manner in terms of time and location with minimal cell
burden.
Keywords: synthetic gene networks, feedback control, generalised re-
pressilator, trajectory tracking.
Control theoretical concepts have proved useful for the understanding of several
biological mechanisms [1]. These concepts have also been used to design opti-
mised intervention strategies with natural systems. In particular, feedback con-
trol based on the input-output relationships seems promising many applications
in Life Sciences. In Epidemiology for instance sequential non-pharmaceutical
interventions as ﬁrst response to a pandemic threats can be formulated as con-
trol theoretical problems [2]. In Medicine optimised drug treatment of HIV-
infected patients can be found using control theoretical framework [3]. Also in
Neuroscience, control theoretical concepts have been widely applied, see for ex-
ample [4], where the well-known Kalman ﬁlter was used for estimation of the
system state and parameters in neural cortex model.
Synthetic biology is another quickly advancing scientiﬁc branch where the
applications of control theory can be promising [5, 6]. Recent progress in this
ﬁeld shows that the two necessary pieces for the implementation of external
feedback control namely targeted interference for control signal implementation
c
⃝Springer International Publishing Switzerland 2015
269
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_28

270
N. Strelkowa
and dynamic gene state tracing for system state readout. Fluorescent markers
can for example be used for quantitative in vivo estimates of gene expression
states [7, 8] and monochromatic light can be used for implementation of targeted
gene expression induction [9, 10].
a
b
Fig. 1. Adaptation of control-theoretical approach to genetic networks. a)
In the training phase the dynamics of the system are studied and time traces of key
variables, for instance protein concentrations, are collected. The approach can be ap-
plied to experimentally measured time traces as well as for simulated time traces. b)
Based on the integrated information in the training set, a state dependent feedback
control strategy is induced in order to conduct a genetic network to a desired state or
track a dynamic trajectory.
The deﬁnition of a control problem also requires a speciﬁcation of the opti-
misation criterion. In many classical applications of the nonlinear control theory
it is the minimal energy [11]. For Synthetic Biology we rather suggest the min-
imisation of metabolic cell burden due to high number of heterologous proteins.
So the gene network functionality as well as controlled tracking of predeﬁned
trajectories or control towards a particular state should be achieved with the
usage of as few heterologous proteins as possible.
For the induction of optimised tracking policies we use the reinforcement
learning framework as described in Ref. [12]. This algorithm uses time traces
to induce a multi-dimensional function Q(n, u), which is the score of a control
action u ∈U for a system state n ∈Ns. The optimised intervention strategy for
direction of the network to a desired state is obtained by taking the maximum
of this score function at each state n: maxu∈U Q(n, u).
In this contribution we demonstrate how a genetic network inspired by syn-
thetic biology can be used for controlled trajectory tracking. Recently we inves-
tigated the generalised repressilator model - genes ordered in a ring structure
and sequentially repressing the following gene (see Fig. 2) - and found that even
numbered gene rings possess unstable limit cycles showing long lasting tran-
sient oscillations [16, 17]. Here, we apply trajectory tracking control in order to

Trajectory Tracking for Genetic Networks Using Control Theory
271
keep even numbered rings in this oscillating state. The control strategy for this
tracking problem is induced via reinforcement learning [12].
In the previous publication we have induced a control strategy for toggling
a genetic switch [13]. The aim of the control policy in the previous publication
was to lead a genetic network to a desired system state. Here, we expand the
application of the control theory in synthetic biology to tracking pre-deﬁned
trajectories and use the even numbered gene ring model as an example.
Methods Description: The Reinforcement Learning Approach
Reinforcement learning approaches have been proved to be useful for a wide
range of applications, where only few information on the underlying process is
available [12, 14]. The major requirement on the underlying process is that it is
ﬁrst order Markovian, which means that the following state is fully determined
by the information on the current state. In particular classical mass-action ki-
netics underpinning gene expression processes can be formulated as ﬁrst order
Markovian processes using either Master equation or diﬀerential equations [15].
Reinforcement learning can be used to ﬁnd optimised control strategies for
problems, which can be approximated as general ﬁrst order Markov processes.
Using otherwise notation as in Ref. [15] Markov property for successive times (i.
e. t0 < (t0 + δt) < · · · < t) and control signals u ∈U can be formulated as:
Pr(n(t), u(t), t|n0, u0, t0; . . . ; n(t −δt), u(t −δt), (t −δt))
= Pr(n(t), u(t), t|n(t −δt), u(t −δt), (t −δt))
(1)
That means a process is ﬁrst order Markovian if the state transition probability
Pr(n(t), u(t)|·) at the time t only depends on the state of the system at the time
t −δt and is not dependent or aﬀected by other previous states.
Master equation formulation for gene expression networks dynamics fullﬁls
this requirement since the time evolution of the probability P on discrete set of
states n ∈Ns is a PDE
dP(n, t)
dt
=
 n′
W(n|n′, u)P(n′, t) −W(n′|n, u)P(n, t)
where W(·|·) are transition probabilities per unit time, which are now also de-
pendent on the external control signals u ∈U.
Approximate dynamics of the Master equation can be formulated as a stochas-
tic diﬀerential equation (SDE) or as an ordinary diﬀerential equation (ODE)
dtx = f(x) (see [15] for connections between the formalisms via Ω-expansion).
The deterministic approximation x = φ(x0, t −t0) is a special case for a Markov
process with a δ-peak transition probability δ(x −φ(x0, t −t0)). Similarly an
SDE is a Markov process, where the transition probabilities are modulated by
Gaussian noise.
Processes described by Master equations, stochastic or ordinary diﬀerential
equations are ﬁrst order Markovian and can be expanded in order to include

272
N. Strelkowa
external input signals. In principle the system dynamics, i.e. state transitions
upon applied external control signals, can also be obtained directly from the
experiment as time traces taken at small enough intervals.
At each state transition a control signal u(t) can be applied to the system.
What we would like to know is: if the system is in a state n, what is the best
action on the long term to be applied? This question is answered by a control
strategy μ(·) : Ns →U, which is the mapping from the state space into the
action space.
The framework set up for a speciﬁc system usually starts with the deﬁni-
tion of desired and forbidden/undesired states and actions. We use the function
r(n(t), u(t)) in order to associate an instantaneous reward to each state transi-
tion. This function must be bounded. The reward for the transition to desired
target states is deﬁned as high, for the undesired or forbidden states as low and
neutral for all other states. The induced scores for the transitions from each state
n ∈Ns if the control action u is applied. The action with the highest score will
identify the optimised control strategy μ.
Score functions for strategies μ will be denoted as Jµ
∞, i.e. the expected score
obtained over an inﬁnite time horizon when the system is controlled using the
control policy μ: u(t) = μ(n(t)), ∀t. For a given initial condition n0, Jµ
∞is given
by:
Jµ
∞(n) = E

 ∞
t=0
γtr(n, μ(n(t)))dt



 n0 = n

(2)
where γ is a discount factor (0 ≤γ < 1) that weights short-term rewards more
than long-term ones, and where the conditional expectation is taken over all
trajectories starting with the initial condition n0. We target to ﬁnd an approx-
imation of the optimal policy μ∗, which maximizes the expectation Jµ
∞for all
system states in the considered hypercube n ∈Ns.
Controlling Even-Number Gene Rings around Unstable Periodic
Orbits
We have introduced a design for a genetic oscillator some years ago where the
application of the control theory can help to ﬁnd optimal sequence of external
interference signals, i.e. an optimised control policy [16, 17].
The suggested design of a genetic network consists of genes ordered in a ring
structure where each gene represses its follower in the network (see Fig. 2). The
steady state system dynamics without the external control actions are such that
a ring with an even number of genes acts as a switch in the stable degenerated
solution. Either the ﬁrst gene and all odd numbered genes are expressed and
the second gene and all even numbered genes are repressed or vice versa (see
Ref. [16, 17]).
This system also has an oscillating transient solution which can last for a
long time for some initial conditions. This transient solution is explained by an
unstable limit cycle due to the ﬁrst Hopf bifurcation in even numbered rings [17].

Trajectory Tracking for Genetic Networks Using Control Theory
273
a
b





500
1000
1500
2000
2500
0
5
10
15
20
25
time
 p1, p2
sample trajectory no control
Fig. 2. The generalized repressilator model. a) Network scheme of the generalised
repressilator with readout and control action points. The gene repression is marked
with the symbol ⊥. b) Dynamics of even-numbered rings without the control actions.
Simulated time traces of two sequential genes in the repression ring are shown. Starting
in oscillating dynamical state the ring converges to the globally stable ﬁxed point
solution [16, 17]. The oscillations are non-sustainable transient without.
Using external control signals on two consecutive genes in the ring the system
can be kicked between the oscillating regime and the stable steady state and
also kept in the oscillating state [16].
In this article we will induce an optimised control policy in order to keep the
ring in the oscillating state. The control problem is deﬁned as trajectory tracking
of the oscillating pattern.
Deﬁnition of the Trajectory Control
The even gene ring system is described by the following set of ODEs:
˙mj =
c1
1 + p2
j−1
−c2mj + δj1u1qs1 + δj2u2qs2
˙pj = c3mj −c4pj
(3)
with the usual deﬁnition for δij as Kronecker symbol [15] and using otherwise
the same set of constants as in Ref. [16].
The control signals u ∈{u1, u2, 0} are applied on the transcriptional level to
the ﬁrst and the second genes in this scenario. The strength of the light signals
on the mRNA production is denoted by qsi.
Using the kick signals the ring can be brought to the stable steady state and
back to the oscillating state [16]. We start the trajectory tracking control from
the oscillating state, i.e. close nearby to the trajectory which we want to track.
Then the goal of the control policy will be to induce a control policy which
should keep the ring in the oscillating state.

274
N. Strelkowa
The reference trajectory is a nonlinear curve, which can be obtained either
experimentally shortly after the ring has been put in an oscillating state or from
the unstable limit cycle of model simulations.
Finally, the instantaneous reward function needs to be deﬁned for the tra-
jectory tracking system. We use a distance function between the observed state
p(t) = (p1(t), p2(t)) and the reference trajectory p0(t):
r(p, u, t) = exp(−|p(t) −p0(t)|)
(4)
Due to the inherent circular symmetry of the system the read-out and control
interface contain only the states and control interactions points of the ﬁrst two
genes, other variables describing transcriptional level and the expression of other
genes are not part of this control scheme.
a
b
time
 controlled genes
 sample trajectory 
time
 controlled genes
 sample trajectory (zoom)
Fig. 3. Optimised tracking of an unstable periodic trajectory in the gen-
eralized repressilator model. a) A sample trajectory starting at the limit cycle
for two observed and controlled genes. The aim of the control policy is to keep the
ring in the oscillating state around the unstable periodic limit cycle. As the oscillation
starts to ﬂatten oﬀ(noticeable by continuously decreasing oscillation amplitude), con-
trol actions marked with red stars are applied preventing the ring from approaching
the stable non-oscillating state. b) zoom into the trajectory shown in a) around the
second control signal.
Discussion
We applied the concept of trajectory tracking to a model of a synthetic genetic
network in an unstable, but long lasting oscillation state. The objective of the
formulated feedback control was to track the oscillation trajectory avoiding the
over-expression of controlled genes and reducing the burdening of the cells by
sparse usage of heterologous proteins. Therefore we have formulated the tracking
optimisation problem inducing the expression of the controlled genes by just
enough to keep the genetic network in the oscillating state.
In our test network we readout and inﬂuence only two genes out of 10 genes,
and it is suﬃcient to keep the ring in the oscillating state. This has of course to
do with the circular symmetry of the network and with the fact that sequential

Trajectory Tracking for Genetic Networks Using Control Theory
275
genes are dynamically dependent on the controlled genes. It means that synthetic
genetic networks with a hierarchical or sequential organisation can be controlled
using partial and not full system observability. Indeed natural and also designed
time
 all genes
 sample trajectory 
Fig. 4. Control of 2 genes keeps a 10 gene-ring in the oscillating state around
an unstable periodic orbit. The control procedure has been designed for a gene ring
with 10 genes, but we have used only the time traces of the ﬁrst 2 genes for the policy
induction. The genes which time evolution has been used for readout and control signal
interference are shown in full colours, the time traces of the dependent genes are shown
in gray.
networks are often organised as cascades of genes or as feedback loops containing
several genes [18]. If we want to control these networks towards desired states or
towards pre-speciﬁed dynamic trajectories then we need to ﬁnd few interaction
points, i.e. key genes for readout and control interactions.
Designed synthetic networks should be organised in a hierarchical manner
so that the infeasible complete observability of the genetic network including
readout of all components is not necessary for the implementation of the external
feedback control. Such key genes for dynamic control can be found based on the
connectivity matrix of the genetic network and this would be an interesting
direction towards advancing Synthetic Biology ﬁeld.
For engineered networks we usually will have a quantitative model for the
genetic network. The control deduction technique applied in this contribution
does not use this information, mainly due to the fact that the models are still
not precise enough for a quantitative estimate of the expected system state. A
further development of this framework could be to expand the approach so that
it would take the model prediction into account while learning, as is done for
instance for the Kalman ﬁlter.

276
N. Strelkowa
References
[1] Csete, M.E., Doyle, J.C.: Reverse engineering of biological complexity. Science 295,
1664–1669 (2002)
[2] Lin, F., Muthuraman, K., Lawley, M.: An optimal control theory approach to
non-pharmaceutical interventions. BMC Infectious Diseases 10, 32 (2010)
[3] Stan, G.B., Belmudes, F., Fonteneau, R., Zeggwagh, F., Lefebvre, M.A., et al.:
Modelling the inﬂuence of activation-induced apoptosis of cd4[sup + ] and cd8[sup
+ ] t-cells on the immune system response of a hiv-infected patient. IET Systems
Biology 2, 94–102 (2008)
[4] Schiﬀ, S.J., Sauer, T.: Kalman ﬁlter control of a model of spatiotemporal cortical
dynamics. J. Neural Eng. 5, 1–8 (2008)
[5] Khalil, A.S., Collins, J.J.: Synthetic biology: applications come of age. Nat. Rev.
Genet. 11, 367–379 (2010)
[6] Cameron, D.E., Bashor, C.J., Collins, J.J.: A brief history of synthetic biology.
Nat. Rev. Micro. 12, 381–390 (2014)
[7] Cai, L., Friedman, N., Xie, X.S.: Stochastic protein expression in individual cells
at the single molecule level. Nature 440, 358–362 (2006)
[8] Bennett, M.R., Hasty, J.: Microﬂuidic devices for measuring gene network dynam-
ics in single cells. Nat. Rev. Genet. 10, 628–638 (2009)
[9] Shimizu-Sato, S., Huq, E., Tepperman, J.M., Quail, P.H.: A light-switchable gene
promoter system. Nat. Biotech. 20, 1041–1044 (2002)
[10] Levskaya, A., Weiner, O.D., Lim, W.A., Voigt, C.A.: Spatiotemporal control of
cell signalling using a light-switchable protein interaction. Nature 461, 997–1001
(2009)
[11] Slotine, J.J.E., Li, W.: Applied nonlinear control. Prentice-Hall, New Jersey (1991)
[12] Ernst, D., Geurts, P., Wehenkel, L.: Tree-based batch mode reinforcement learn-
ing. Journal of Machine Learning Research 6, 503–556 (2005)
[13] Strelkowa, N.: Inference of Optimized Control Strategies for Genetic Networks.
In: ISCS 2013: Interdisciplinary Symposium on Complex Systems. Emergence,
Complexity and Computation, vol. 8, pp. 265–270. Springer, Heidelberg (2014)
[14] Bertsekas, D.: Dynamic Programming and Optimal Control, 2nd edn., vol. I.
Athena Scientiﬁc, Belmont (2000)
[15] van Kampen, N.G.: Stochastic Processes in Physics and Chemistry, 3rd edn. El-
sevier, Amsterdam (2007)
[16] Strelkowa, N., Barahona, M.: Switchable genetic oscillator operating in quasi-
stable mode. Journal of the Royal Society Interface (2010)
[17] Strelkowa, N., Barahona, M.: Transient dynamics around unstable periodic or-
bits in the generalized repressilator model. Chaos: An Interdisciplinary Journal of
Nonlinear Science 21, 023104 (2011)
[18] El-Samad, H., Khammash, M.: Modelling and analysis of gene regulatory networks
using feedback control theory. International Journal of Systems Science 41, 17–33
(2010)

Modeling and Optimization of Microalgae
Growth in Photobioreactors: A Multidisciplinary
Problem
ˇStˇep´an Pap´aˇcek1, Jiˇr´ı Jablonsk´y1, Karel Petera2, Branislav Reh´ak3,
and Ctirad Matonoha4
1 University of South Bohemia in Ceske Budejovice, FFPW USB, CENAKVA,
Institute of Complex Systems, Z´amek 136, 373 33 Nov´e Hrady, Czech Republic
spapacek@frov.jcu.cz, jiri.jablonsky@gmail.com
2 Czech Technical University in Prague, Faculty of Mechanical Engineering,
Technicka 4, 166 07 Prague 6, Czech Republic
Karel.Petera@fs.cvut.cz
3 Institute of Information Theory and Automation,
Academy of Sciences of the Czech Republic, 182 09 Prague, Czech Republic
rehakb@utia.cas.cz
4 Institute of Computer Science, Academy of Sciences of the Czech Republic,
Pod Vodarenskou vezi 2, 182 07 Prague 8, Czech Republic
matonoha@cs.cas.cz
Abstract. Microalgae have the potential to be a major biofuel source
in the future. Computational biology plays a key role in understand-
ing biological processes within microalgae and optimizing biofuel pro-
duction. Here, we present a multidisciplinary, multi-timescale model-
ing approach of microalgae growth in photobioreactors. Our modeling
framework bridges biology (cell growth), physics (hydrodynamics and
light distribution), and optimization together. This framework consists
of (i) the state system (mass balance equations in form of advection-
diﬀusion-reaction PDEs), (ii) the ﬂuid ﬂow equations (the Navier-Stokes
equations), and (iii) the optimization problem formulation. The model-
ing and optimization of microalgae growth in a Couette-Taylor reactor
is presented to demonstrate this method. We show how the ﬂashing light
eﬀect can be an intrinsic part of the model. Finally, we discuss further
methodological integration with the metabolomic-transcriptomic kinetic
model, which explains cellular concentrations of key metabolites in con-
nection with cell growth.
Keywords: Microalgae, photobioreactor, optimization, multiscale mod-
eling, ﬂashing light eﬀect.
1
Introduction
Microalgae are re-gaining more attention in biotechnology due to its new po-
tential as a biofuel source, such as modiﬁcation for high lipid content. However,
c
⃝Springer International Publishing Switzerland 2015
277
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_29

278
ˇS. Pap´aˇcek et al.
reliable methods with predictive power for in silico simulation of microalgal
growth in production systems, such as photobioreactors (PBR), are missing.
The modeling of microalgal behavior and growth is a very complex prob-
lem, that involves three-dimensional multiphase (gas-liquid-solid) ﬂow dynamics,
description of properties within PBR, e.g., irradiance distribution, and multi-
dimensional functionality of cellular processes. Moreover, all these parts interact
across diﬀerent timescales. Thus, one has to solve theoretical (e.g., how to cope
with multi-timescale phenomena, to prove the existence and regularity of opti-
mal solutions, etc.) and practical (enormous computational requirement) issues.
As a consequence, one or more interacting parts of the system are implemented
either in over-simpliﬁed or even inadequate way.
While most studies on PBR focus on speciﬁc problems without clear connec-
tion to the whole production process, in this communication we formulate the
uniﬁed framework for modeling of microalgae growth in a PBR. We provide a
simple case study of microalgae growth in the Couette-Taylor reactor, a device
where the so-called Taylor vortices can induce high frequency light-dark cycles
regime.
2
State and Fluid-Dynamic Models
The material (mass) balance equations for the state variables of interest (e.g.,
microbial species, nutrients, CO2, O2, etc.) have to be solved simultaneously
with the ﬂuid dynamics (momentum balances, continuity equations). Neverthe-
less, according to [1], the stationary ﬂow ﬁeld inside the production system (e.g.,
photobioreactor, open pond or raceway) is not aﬀected by mass transfer and re-
action for many biotechnological processes with Newtonian ﬂow behavior. This
assumption permits the separation of both models and consequently the reduc-
tion of the computational eﬀort due to the fact that each model can be solved
with diﬀerent numerical method and with diﬀerent spatial-temporal discretiza-
tion (on diﬀerent numerical grids). The separation can be total (for the stationary
ﬂow ﬁeld in a continuous system) or stepwise, e.g., reﬂecting some sequence of
quasi-steady states in a production system operated in batch mode.
Moreover, we would ideally model the transport of material (including pho-
tons) to and from the surface of microbial cells and the corresponding biological
reactions (cell metabolism). The model parameters would be independent of
the PBR size, operational mode, etc. However, there are two reasons why this
approach (microkinetics) is rather theoretical:
– To model all the transport phenomena inﬂuencing the microenvironment
is still almost numerically prohibitive. E.g., the length scales over which it
is necessary to resolve the mass transport could be smaller than the ﬂuid-
ﬂow scales, especially in the turbulent ﬂow regime. Special approaches must
be used in such cases, which results in a very large number of mesh cells
substantially limiting consequent optimization calculations.
– Due to the inhomogeneity of cell population there could be morphological
diﬀerentiation of cells caused by shifted cell cycle and overall changes in gene
expression that are almost impossible to observe.

Modeling and Optimization of Microalgae Growth in Photobioreactors
279
Thus being able to measure only averaged behavior over a large number of cells
in diﬀerent states, we opt for the integral description called the macrokinetics
and the respective model parameters have to be tuned dynamically.
2.1
State Model
The material balance equations for the state variables of interest describe the
transport and reaction of the reacting species or components:
∂ci
∂t + ∇· (vci) −∇· (De∇ci) = R(ci) + S(ci),
x ∈Ω ⊂R3,
t ∈[t0, T ], (1)
with the initial condition ci0 = ci(x, t0), x ∈Ω ⊂R3, and the boundary con-
dition (impermeability of the domain boundary): ∇ci(x, t) = 0, x ∈∂Ω, t ∈
[t0, T ], where ci = ci(x, t), i = 1, ..., m, are conservative quantities, e.g. a con-
centration or cell density of the ith component, v is the velocity ﬂow ﬁeld, and x
stands for a position vector in a coordinate system. De(x) is the dispersion coeﬃ-
cient, which corresponds to the diﬀusion coeﬃcient in microstructure description
and becomes a mere empirical parameter that suitably describes mixing in the
system. It is inﬂuenced by the molecular diﬀusion and velocity proﬁle. R(ci) and
S(ci) are the reaction and source (inlets/outlets) terms, respectively.
The source term S(ci), e.g. the load of nutrients, is usually modeled as a
corresponding boundary condition. However, in order to further simplify the
analytical study of the existence of an optimal solution, cf. [2], and respecting
that the location of discharge of some material could be inside the domain Ω,
we prefer the above form (1).
The reaction term R(ci) in the transport equation (1) is usually provided by a
”specialist” not being familiar with the complications residing in the fact that the
relevant transport and reaction phenomena are multi-scale. If we realize that the
characteristic times of microalgae growth and turbulent diﬀusion are of the order
of hours and seconds, respectively, then after adopting whatever growth kinetic
model, only two alternatives exist: either (i) to neglect the details concerning
mixing phenomena, e.g. by accepting the hypothesis that the entire cell culture
dispersed in medium was homogenized at each calculation step (cf. [3], where
the time step Δt was set to 3600 s), or (ii) to observe the changes due to the hy-
drodynamic mixing and neglect those of biochemical reaction. Both alternatives
lose the coupling between transport and reaction phenomena, which qualiﬁes the
corresponding modeling framework as unsatisfactory. Our proposition of how to
solve the above mentioned diﬃculties is based on the multi-timescale approach,
practically shown in Section 3.
2.2
Fluid-Dynamic Model
If we model only the incompressible liquid phase (suspension of water, nutrients
and microalgae), then the classical system of Navier-Stokes equations and the
continuity equation are used as a ﬂuid-dynamic model

280
ˇS. Pap´aˇcek et al.
ρ∂v
∂t + ρ(v · ∇)v = f −∇p + μ∇2v,
∇· v = 0,
(2)
in (t0, T ) × Ω with suitable boundary conditions on (t0, T ) × ∂Ω and initial
conditions in Ω. Here v, p, f, ρ, and μ denote the ﬂuid velocity, the pressure, the
volumetric forces, ﬂuid density, and dynamic viscosity, respectively.
2.3
Numerical Aspects of the Solution of PDEs (1-2)
Using a suitable numerical method, the partial diﬀerential equations (1-2) can
be transformed into a system of linear algebraic equations. In the ﬂuid ﬂow
problems, the ﬁnite volume approach (FVM) is more popular in obtaining the
discretized form of equations than other approaches like ﬁnite element (FEM) or
ﬁnite diﬀerences (FDM) method, especially with unstructured grids. The system
of discretized equations is usually solved iteratively to ﬁnd the values of velocities
and other scalar quantities like concentrations in all grid points.
When the number of grid points is large, the solution domain can be split into
several regions (partitions), solving each of them in parallel to decrease the com-
putational time. One of such methods is the Lattice Boltzmann method (LBM),
cf. [4], which provides a superior parallel performance. A good parallel perfor-
mance of this method can be advantageously employed in GPU computations,
cf. [5].
2.4
Process Control and Optimization of Design and Operational
Variables
Our approach is entirely model-based and rather than on a real-time control is
focused on the optimal PBR design and control, which could answer the ques-
tions about the optimal PBR performance in the preliminary stage of the PBR
design in silico. Further we formulate the optimal control & design problem,
denoted by (P), with the following elements:
– Design variables: Let us suppose the PBR structure is parameterized, i.e.
the PBR geometry is determined by p time independent design variables d =
(d1, ..., dp). These variables must be nonnegative and bounded by a certain
maximal admissible value or they must lie in an admissible set: d ∈Ud
ad.
– Operational variables (Controls): Let us suppose the PBR is controlled
by means of the operational (generally time dependent) control variables
g(t). E.g. for the batch operation mode: incident irradiance I0(t), a quantity
ω(t) corresponding to the pumping or mixing, the inﬂow rate of nutrients
S(cj), j = 1, ..., n, the initial microbial concentration cx(t0) and the total
time of cultivation T . For the continuous mode: the steady-state microbial
concentration cx and the dilution rate D. For technological reasons, some
constraints are applied (e.g. nonnegativity and boundedness): g ∈Uc
ad.
– State variables: The state variables c(x, t) = ci(x, t), i = 1, ..., m, are
concentrations of some conservative quantities or cell densities. There could
be some thresholds or constraints speciﬁed by the relation c ∈Us
ad.

Modeling and Optimization of Microalgae Growth in Photobioreactors
281
– Model parameters: Although many authors disregard this issue, we have
to be aware that not only the optimization but also the parameter estimation
and model veriﬁcation might be unfeasible with increasing model complexity.
For instance, the reader is referred to our work [6].
– Objective function: Let us suppose, we have only one objective to opti-
mize. For example, we are interested in maximizing biomass production, so
we maximize the (averaged) volumetric productivity setting the cost func-
tional J(d, g) as follows:
J(d, g) =
1
meas(Ω) T
 T
0

Ω
μ(x, t) cx(t) dxdt ,
(3)
where μ(x, t) is the speciﬁc growth rate of the algal biomass, which mainly
depends on the irradiance distribution, deﬁned as μ :=
1
cx
dcx
dt .
Finally, we formulate the optimal control & design problem (P) describing
the optimal PBR performance:
max J(d, g)
such that
d ∈Ud
ad, g ∈Uc
ad, c ∈Us
ad
veriﬁes (1-2).
(P)
To demonstrate that the problem (P) admits, at least, a solution, and to derive
optimality conditions is out of the scope of this paper. For a similar problem see
e.g. [2].
3
Case Study – Microalgae Growth in CTBR
We present a model of microalgae growth in the Couette-Taylor photobioreactor
(CTBR) in this section. CTBR is a special device basically composed from two
coaxial rotating cylinders: the inner is rotating and the outer is homogeneously
illuminated. The ﬂow regime inside the Couette-Taylor device belongs to the
intensively studied topics of mathematical physics since 1923, cf. [7]. Our main
interest concerns the Taylor vortex ﬂow, when the so-called Taylor vortices can
induce high frequency light-dark cycles regime. We show the positive impact of
this hydrodynamically induced light ﬂuctuation on performance index, i.e. we
demonstrate that the ﬂashing light eﬀect forms an intrinsic part of the model.
Assuming that the whole geometrical design and irradiance distribution are ax-
isymmetrical, we care only about the ﬂow description in the radial direction, i.e.
in direction of light gradient. Moreover, if the inner cylinder rotation ω, which
induces the ﬂow, is constant, the convective ﬂow in the radial direction can be
described by the eﬀective dispersion coeﬃcient Deff, and the state system (1)
actually changes into the stationary 1D diﬀusion(dispersion)-reaction PDE, cf.
[8]. The reaction model is described in the following subsection.
3.1
Model of Photosynthetic Factory – PSF Model
The light inside PBR is the main limiting factor in biotechnological applica-
tions. We therefore focus on the case in which microbiological photosynthesis is

282
ˇS. Pap´aˇcek et al.
light limited only, i.e., CO2 and nutrients abundant environment. Thus we were
looking for a model which is able to describe three basic phenomena occurring
simultaneously in three largely separated time-scales: (i) cell growth, (ii) pho-
toinhibition, and (iii) photosynthetic light and dark reactions. Therefore, the
reaction model has to be ”dynamic” and cope with the following experimental
observations: (i) the steady-state kinetics (so-called P–I curve) is of Haldane
type or Substrate inhibition kinetics; (ii) the ”slow” dynamics of the photoinhi-
bition can not be depreciated (it is observable as the so-called afternoon depres-
sion on the open ponds, cf. [9]), (iii) the microalgal culture in suspension has
the so-called light integration property; i.e. as the light/dark cycle frequency is
growing, production rate (measuring e.g. via oxygen evolution rate) goes to a
certain limit value, which depends on the average irradiance only, cf. [10],[11].
All these features are comprised of the model of photosynthetic factory –
PSF described in detail below.
The state vector y of the PSF model is three dimensional, namely, y =
(yR, yA, yB)⊤, where yR represents the probability that PSF is in the resting
state R, yA the probability that PSF is in the activated state A, and yB the
probability that PSF is in the inhibited state B. The possible transitions be-
tween states are of zeroth or ﬁrst order respective to the irradiance I(t):
⎡
⎣
˙yR
˙yA
˙yB
⎤
⎦=
⎡
⎣
0
γ
δ
0 −γ
0
0
0 −δ
⎤
⎦
⎡
⎣
yR
yA
yB
⎤
⎦+ I(t)
⎡
⎣
−α
0 0
α −β 0
0
β 0
⎤
⎦
⎡
⎣
yR
yA
yB
⎤
⎦.
(4)
For given values of the model parameters α, β, γ, δ and the input variable,
i.e. the irradiance I(t), the ODE system (4) can be solved either by numerical
methods or by asymptotic methods. For the special case of the periodic piecewise
constant input, the state trajectories were calculated explicitly in [12].
The PSF model is completed by an equation connecting the hypothetical
states of the PSF model with the speciﬁc growth rate μ. According to [9],[13],
the rate of photosynthetic production is proportional (there is a dimensionless
constant κ) to the number of transitions from the activated to the resting state:
d
dtcx = (κ γ yA(t) −Me) cx , where the maintenance term Me corresponds
to the energetic requirements for internal metabolism and allows the negative
growth (i.e. the decay of cell concentration) rate in light condition below the
compensation point. Considering that the value of κ · γ
is of order 10−4, cf.
[13], and yA(t) is periodic with period h (a quasi steady-state is reached), cf.
[12] for more details, we have the following relation for the speciﬁc growth rate
μ: μ = κγ
h
 h
0 yA(t)dt −Me .
The above equation reveals why PSF model can successfully simulate the
microalgae growth in high-frequency ﬂuctuating light conditions: the growth is
described through the ”fast” state yA, hence we reach the sensitivity to high-
frequency inputs, see e.g. ﬂashing light experiments [11]. As we know, this highly
required sensitivity is not achieved by any other model describing the microalgae
growth in PBR.

Modeling and Optimization of Microalgae Growth in Photobioreactors
283
Now we explain how to introduce the reaction term into the transport equation
(1). Let us evaluate the PSF model states as relative concentrations (molar
fractions) of microbial cells in each state (R, A, or B). Deﬁne the variables ci
as the concentrations of cells in respective states of PSF model, and cx as an
overall microbial cell concentration. The concentrations are generally varying
in time and space, so ci = ci(x, t), i ∈{R, A, B}. Nevertheless it holds that
cx = cR + cA + cB. Consequently, we re-deﬁne the state vector of PSF model
as follows: y = (yR, yA, yB)⊤:=
1
cx (cR, cA, cB)⊤. Furthermore, after dividing
(1) by c, we can substitute the right hand side of the PSF model equation (4)
with the reaction term on the right hand side (where A and B stand for the
corresponding matrices in (4)) of the following equation:
∂y
∂t + ∇· (vy) −∇· (De∇y) =

A + I(x, t)B
	
y.
(5)
Equation (5) with suitable initial and boundary conditions represents the PDE
based model for describing multi-scale transport and reaction phenomena not
only in CTBR but also in a general PBR. To evaluate the spatio-temporal aver-
age of speciﬁc growth rate in PBR, we use [13] to get
μ =
κγ
meas(Ω) T
 T
0

Ω
(yA(x, t) −Me) dxdt,
(6)
which is used for evaluation of the objective function J, cf. (3).
3.2
Simulation Results and Discussion
For a laboratory bioreactor based on Couette-Taylor ﬂow, and for the long term
cultivation of the microalgae culture in continuous mode (we supposed the quasi-
steady state is reached), we solve the stationary form of PDEs (1-2). We apply
the multi-timescale PSF model to the domain with heterogeneously distributed
relevant parameters, being the irradiance I(x), distributed according to the ex-
ponential Lambert-Beer law, and hydrodynamic dispersion Deff(x). For the
parameter values and more details see [8].
We solve the following boundary value problem with homogeneous Neumann
boundary conditions and inhomogeneous right-hand side:
−1
x [xp(x)y′
A]′ + DaII

I(x)
q1
+ q2

yA = DaII

I(x)
q1
+ q2

yAss,
x ∈(r0/R, 1),
y′
A(r0/R) = 0,
y′
A(1) = 0,
(7)
where r0 and R are the inner and outer cylinder radii, respectively, p(x) := Deff
D0 ,
D0 is some characteristic value, units

m2s−1	
, and DaII := q4 R2
D0
is the so-called
Damk¨ohler number of second type, which characterizes the ratio of reaction rate
and mixing rate (due to the hydrodynamic dispersion). As DaII > 0, and the
transformed model parameters
qi > 0, i ∈{1, 2, 4}, the problem (7) has a

284
ˇS. Pap´aˇcek et al.
0.7
0.75
0.8
0.85
0.9
0.95
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
x
yA(x)
 
 
DaII = 1d−1
DaII = 1d0
DaII = 1d1
DaII = 1d2
DaII = 1d3
DaII = 1d4
yA
ss
(x)
Fig. 1. Dependence of the solution of (7) on the radial position in CTBR for diﬀerent
mixing rates (inversely proportional to DaII). For the low DaII, i.e. high mixing rate,
the eﬀect of the heterogeneous light distribution is suppressed.
unique solution. It was solved numerically using the ﬁnite diﬀerence scheme.
Fig. 1 shows the dependence of the solution yA(x) on the Damk¨ohler number
DaII. We can see that the solution becomes ﬂatter for decreasing DaII and for
DaII = 0.1 it is nearly constant. It holds that yA(x) →0.625 for DaII →0.
Notice also that the value yA = 0.625 corresponds to the value yAss(Iopt), cf.
[8]. This means that the ODE system (7) performs the ”averaging” of I(x) for
the case DaII →0 because the incident irradiance (the irradiance on the outer
surface of CTBR) was chosen in such a manner that the average irradiance inside
CTBR equals the optimal irradiance Iav = Iopt.
From practical point of view, in order to maximize the speciﬁc growth rate, it
is important to evaluate the integral average of the activated state yA(x), being
proportional to the performance index J. Fig. 2 shows the dependence of J on
DaII. The maximum value arises for DaII →0 and its value is again J = 0.625.
Practically it means to set the mixing rate on its upper bound.1
Our results reﬂect well the dependence of microalgae growth on the Damk¨ohler
number DaII, i.e. on hydrodynamic dispersion permitting the announcement of
our statement about light integration property of PSF model for CTBR as well.
The resulting photosynthetic production rate in a photo-bioreactor goes (with
growing mixing rate) to a certain limit value, which depends on the average
irradiance only.
1 Obviously, there is a known phenomenon called hydrodynamical shear stress making
the higher mixing or pumping rates prohibitive. By incorporating this eﬀect, e.g. on
the Me term, cf. (6), our result would be more realistic.

Modeling and Optimization of Microalgae Growth in Photobioreactors
285
−2
−1
0
1
2
3
4
5
6
7
0.44
0.46
0.48
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
DaII (log scale)
J
 
 
max = 0.625
value J
min = 0.4539
Fig. 2. Performance index J, cf. (3), vs. DaII. The average irradiance in the culture
is set to the optimal value Iopt. Indeed, for DaII →0 it holds J →yAss(Iopt) = 0.625.
4
Conclusions and Future Prospects
The model of photosynthetic factory, introduced in the previous section, is a ﬁrst
approximation how to model a microalgae cell in PBR. It considers a growth
rate, i.e., biomass production, but it cannot tell us anything about a particular
metabolite, e.g. lipid, which could be interesting as a biofuel. Moreover, it ne-
glects possible subpopulations or complex cellular functions such as metabolic
regulation. It is therefore desirable to employ a complex cellular model which
could analyze diﬀerent subpopulations scenarios in the PBR but also provide an
answer about how to boost or reduce certain metabolic pathways. There are three
ways of incorporating the cellular functions into kinetic model of PBR: reduced
model of light photosynthetic reactions, reduced model of carbon metabolism,
and model of whole cell. The ﬁrst two options work with a simpliﬁed view of
the cell and consider parts of those cellular functions which can be easily mea-
sured. There are chlorophyll ﬂuorescence and oxygen evolution in the case of
light reactions, both phenomena can be modeled within a single but complex
model.
We are developing a multi-scale kinetic model for photosynthetic cyanobac-
teria, combining various omic data sets (metabolic, transcriptomics, etc.) from
diﬀerent steady and transient states. Such models can explain metabolic reg-
ulation such as isozymes, cf. [14], or even predict the cellular concentration of
metabolites. Combining the results from the reduced model of algae and ad-
vanced model of cyanobacterial cell should allow the development of a complex
multi-level kinetic model of algae, integrated into the model of PBR.

286
ˇS. Pap´aˇcek et al.
Acknowledgement.
This
work
was
supported
by
the
CENAKVA
CZ.1.05/2.1.00/01.0024, by the CENAKVA II (project LO1205 with a ﬁnan-
cial support from the MEYS under the NPU I program), by the Postdok JU
(CZ.1.07/2.3.00/30.0006), by the grant MˇSMT MSM 600 766 58 08 of the
Ministry of Education, Youth and Sports of the Czech Republic, and by the
long-term strategic development ﬁnancing of the Institute of Computer Science
(RVO:67985807).
References
1. Schugerl, K., Bellgardt, K.: Bioreaction Engineering, Modeling and Control.
Springer, Heidelberg (2000)
2. Alvarez-V´azquez, L., Fern´andez, F.: Optimal control of bioreactor. Applied Math-
ematics and Computation 216, 2559–2575 (2010)
3. Muller-Feuga, A., Gu´edes, R.L., Pruvost, J.: Beneﬁts and limitations of modeling
for optimization of Porphyridium cruentum cultures in an annular photobioreactor.
Journal of Biotechnology 103, 153–163 (2003)
4. Succi, S.: The Lattice Boltzmann equation. Oxford University Press (2001)
5. ˇStumbauer, V., Petera, K., ˇStys, D.: The lattice Boltzmann method in bioreac-
tor design and simulation. Mathematical and Computer Modelling 57, 1913–1918
(2013)
6. Reh´ak, B., ˇCelikovsk´y, S., Pap´aˇcek, ˇS.: Model for photosynthesis and photoinhi-
bition: Parameter identiﬁcation based on the harmonic irradiation O2 response
measurement. Joint Special Issue of TAC IEEE and TCAS IEEE, 101–108 (2008)
7. Taylor, G.I.: Stability of a viscous liquid containing between two rotating cylinders.
Phil. Trans. Royal Society 223, 289–343 (1923)
8. Pap´aˇcek, ˇS., ˇStumbauer, V., ˇStys, D., Petera, K., Matonoha, C.: Growth impact of
hydrodynamic dispersion in a couette-taylor bioreactor. Mathematical and Com-
puter Modelling 54, 1791–1795 (2011)
9. Eilers, P., Peeters, J.: Dynamic behaviour of a model for photosynthesis and pho-
toinhibition. Ecological Modelling 69, 113–133 (1993)
10. Terry, K.L.: Photosynthesis in modulated light: Quantitative dependence of pho-
tosynthetic enhancement on ﬂashing rate. Biotechnology and Bioengineering 28,
988–995 (1986)
11. Nedbal, L., Tich´y, V., Xiong, F., Grobbelaar, J.: Microscopic green algae and
cyanobacteria in high-frequency intermittent light. J. Appl. Phycol. 8, 325–333
(1996)
12. Pap´aˇcek, ˇS., ˇCelikovsk´y, S., ˇStys, D., Ruiz-Le´on, J.: Bilinear system as modelling
framework for analysis of microalgal growth. Kybernetika 43, 1–20 (2007)
13. Wu, X., Merchuk, J.: A model integrating ﬂuid dynamics in photosynthesis and
photoinhibition processes. Chemical Engineering Science 56, 3527–3538 (2001)
14. Jablonsk´y, J., Hagemann, M., Schwarz, D., Wolkenhauer, O.: Phosphoglycerate
mutases function as reverse regulated isoenzymes in synechococcus elongatus pcc
7942. PLOS One 8, e58281 (2013)

© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
287
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_30 
 
Digital Processing of Toxoplasma gondii Cysts in Meat 
Samples for Human Consumption in Colombia 
Quiñones Armando and Juez-C Graciela 
Research group BIOAXIS, Bioengineering Program, El Bosque University, Bogotá, Colombia 
{quinonesarmando,juezgraciela}@unbosque.edu.co 
Abstract. Toxoplasma gondii is an important pathogen of humans and other 
warm-blooded vertebrate animals. This parasite is clinically important due to 
the diseases that it may cause on the fetus, namely chorioretinitis, 
microcephaly, hydrocephaly, mental retardation and hepatosplenomegaly, 
among others. The parasite affects also immunocompromissed patients that 
suffer from any type of cancer, organ transplantations and HIV, reactivating 
the infection. Zoonosis in Colombia is an issue of high epidemiological 
importance due to the transmission of pathogens. A deeper study on 
toxoplasmosis requires the analysis of the developmental stages of the parasite, 
including the cyst, which allows it to evade the immune system and to get 
established in the host organism permanently. The goal of this work was to 
develop a digital image processing algorithm based on texture recognition to 
allow automatic detection of the cysts associated to pathogen T. gondii from 
isolated Colombian native strains from meat samples for human consumption, 
using computational software Matlab 2013, specifically through the Image 
Processing Toolbox. 
Keywords: Toxoplasma gondii, Eccentricity, Co-ocurrence. 
1 
Introduction 
Toxoplasma gondii is an intracellular parasite member of the phylum Apicomplexa. It 
is an important pathogen in a variety of animals including humans. Infection in 
intermediate hosts, including humans, occurs through different ways; namely 
transmission from mother to fetus through the placenta, transmission of tachyzoites in 
organ transplantation, ingestion of bradyzoites or tachyzoites directly present in 
undercooked meat [1], and ingestion of oocysts present in water for human 
consumption [2].  
This parasite is of widespread clinical importance due to the diseases caused on the 
fetus. A number of clinical studies suggest there are different clinical manifestations that 
may occur in individuals with congenital toxoplasmosis, including chorioretinitis, 
microcephaly, hydrocephalus, encephalitis, mental retardation, hepatosplenomegaly, 
eritroblastocis, among others [3]. Furthermore, this pathogen may generate 
opportunistic infection in immunocompromised patients with any type of cancer [4] or 

288 
Q. Armando and J.-C. Graciela 
organ transplantation [3]; additionally, people with HIV are extremely vulnerable to the 
development of toxoplasmic encephalitis [5]. Consequently, about 30 to 50% of people  
with HIV who have had chronic infection with T. gondii have developed toxoplasmic 
encephalitis; also, during pregnancy, there is some probability that acute infections with 
T. gondii lead to congenital diseases [6]. Moreover, seropositivity of T. gondii has been 
associated with the manifestation of other chronic infections such as chronic hepatitis B, 
diabetes mellitus, neoplasms, schizophrenia and arthritis [7]. T. gondii strongly 
predominates because of its powerful interconversion features, which endow it with the 
ability to resist the immune system effectively. This parasite has a very complex life 
cycle that includes sexual and asexual stages that depend on its biological development. 
The sexual cycle occurs only in the intestine of cats that are the definitive hosts [8]. 
Unlike the sexual cycle, the asexual cycle can occur in most types of the 
intermediate hosts [9]. When the host develops immunity, the amount of free 
tachyzoites decreases and their intracellular multiplication slows down. Within a  
few weeks, by interconversion, the tachyzoites (i.e. proliferative phase or acute  
phase of the parasite) become bradyzoites (i.e. move into their chronic phase),  
which ultimately forms the actual tissue cysts. These cysts grow in some tissues, 
especially in the brain, protecting the parasites against the immune response of the 
host cell [8]. 
Undoubtedly, one of the most complex survival strategies of this parasite is its 
ability to take advantage of immunosuppressed hosts and reactivate the infection, 
causing potentially fatal encephalitis [9]. The increased interest on the consequences 
of immunodeficiency has led several researchers to focus on the recrudescence of 
toxoplasmosis; for example, in immunocompromised individuals infected with HIV, 
the bradyzoites found in tissues may differentiate into tachyzoites, which generates 
abscesses in the brain, leading to fatal problems for patients [5]. This cyst stage of T. 
gondii has not been exhaustively characterized, which encourages researchers to 
continue studying the mechanisms that allow the parasite to stay in the host cell for 
months, years, and even lifetime [10].  
Studying such mechanisms represents an important contribution in the 
knowledge of Toxoplasma-associated infections and will surely provide a better 
understanding of each of the events involved in the pathogenesis of toxoplasmosis. 
The goal of this work was to develop a digital image processing algorithm based 
on texture recognition to allow automatic detection of cysts of T. gondii, which 
have been isolated from Colombian native strains, obtained from mouse brain 
tissue, which were previously inoculated with samples of heart and tongue of porks 
for human consumption. The  mathematical algorithm was developed using 
specific stages computational software Matlab 2013, specifically through the 
Image Processing Toolbox, which allowed segmenting these images, highlighting 
typical features of the cysts and calculating its eccentricity as a circularity 
criterion.  

Digital Processing of Toxoplasm
2 
Materials and M
The development of T. go
with avirulent strain JUEZ
consumption ready to ea
parasite, heart tissue and
processed and then inocu
Then the mice were sacri
processes and histochem
visualization of T. gondii 
mathematical algorithm. 
The histochemical analy
Giemsa reagents. Visualiza
37081 AX10sKop 40 micro
SONY DSC-S85 (4.1 Meg
were conducted in the labo
Biology) at Universidad de 
Algorithm 
To carry out the analysis 
stages (Fig. 1) of a compu
an Image Processing Too
highlighting typical feature
to a circularity criterion. 
distribution histograms (Fig
segmentation process allow
parameter. Features such as
(Fig. 3). Finally, through a c
carried out using the homog
provides information of the
cyst itself (Fig. 4 A, B, C). 
Fig. 1. Flo
ma gondii Cysts in Meat Samples for Human Consumption 
Methods  
ondii cysts was studied on mouse brain tissue infec
Z1. The samples were isolated cuts of pork for hum
at by Colombian population. To detect cysts of 
d tongue of pork were isolated, these samples w
ulated into mice for to allow development the paras
ificed by isolating the brain tissue in which molecu
mical staining were carried out, that allowed 
cysts, obtaining images and subsequent detection by 
ysis were performed with Periodic-Acid-Schiff (PAS) 
ation was performed at different scales using a Carl Ze
oscope, obtaining images directly from a Nikon Carl Ze
ga pixels CCD 6x zoom) camera. Histological procedu
oratories of BBMP (Parasite Biochemistry and Molecu
Los Andes, Bogotá, Col. 
of T. gondii cysts, images were processed using spec
utational software tool (Matlab 2013), specifically throu
olbox that allowed segmenting the images as well
es of the cysts and calculating their eccentricity accord
Qualitative image analysis was performed using unifo
g. 2) and conversion to grayscale histogram equalization
wed information removal using a threshold area a
s circularity and eccentricity were extracted of cyst ima
co-occurrence matrix (Formulas 1, 2), texture analysis w
geneity index (Formula 3) as a fundamental parameter t
e local regularity of the texture (Fig. 5), also detecting 
owchart of the stages of cyst image processing 
289 
cted 
man 
the 
were 
site. 
ular 
the 
the 
and 
eiss  
eiss 
ures 
ular 
cific 
ugh 
l as 
ding 
orm 
n. A 
as a 
ages 
was 
that 
the 
 

290 
Q. Armando and J.-C. Graciela 
 
Fig. 2. Preprocessing of cyst images. Matlab 2013. 
 
Fig. 3. Segmentation process. Circularity and Eccentricity. Matlab 2013. 
 
 
 

Digital Processing of Toxoplasma gondii Cysts in Meat Samples for Human Consumption 
291 
 
 
  
     
 
Fig. 4. Texture and Homogeneity of image. Matlab 2013. Cysts of Toxoplasma gondii. 
Colombian native Strain JUEZ1. Giemsa staining, mouse brain tissue with eight-month 
infection. Cyst of 55um in diameter, 100x (A,B).  Cyst of 77um in diameter, 100x (C). Carl 
Zeiss 37081 AX10sKop 40 Microscope. 
 
Fig. 5. Parameters of homogeneity of cyst image. Matlab 2013. 

292 
Q. Armando and J.-C. Graciela 
Co-occurrence Matrix 
 
 
  
(1) 
 
  
(2) 
Homogeneity 
 
  
(3)  
The Performance Indices of the Algorithm Are as Follows: 
 
Classification accuracy = 
ே௨௠௕௘௥ ௢௙ ௘௩௘௡௧௦ ௖௢௥௥௘௖௧௟௬ ௖௟௔௦௦௜௙௜௘௟ௗ
்௢௧௔௟ ௡௨௠௕௘௥ ௢௙ ௘௩௘௡௧௦
 =
଺଴
଺଴= 1  
(4) 
 
Sensitivity =
்௥௨௘ ௣௢௦௜௧௜௩௘௦
்௥௨௘ ௣௢௦௜௧௜௩௘௦ାி௔௟௦௘ ௡௘௚௔௧௜௩௘௦ =
రబ
రబశబ= 1 
 (5) 
 
Specificity=
்௥௨௘ ௡௘௚௔௧௜௩௘௦ 
்௥௨௘ ௡௘௚௔௧௜௩௘௦ାி௔௟௦௘ ௣௢௦௜௧௜௩௘௦=
ଶ଴
ଶ଴ା଴= 1  
 (6) 
 
  
(7) 
3 
Results 
For the detection of T. gondii (Fig. 6 A, B, C, D) in cysts images, the algorithm of 
automatic detection is mainly based on two metrics: namely an index of circularity, 
which initially allowed the differentiation of forms or organelles present in the image, 
and a texture analysis, whose parameter was homogeneity (understood as the way 
local regularity of the intensity of pixels in the area of the detected circle is 
distributed). The circularity index makes use of a threshold, considering the images 
that exist within various cellular structures that are similar to those of circular-
shapeed cysts. This indicates that the threshold rule allowed discrimination of all 
those circular shapes that had a circular area below the threshold. Applying this 
digital image processing algorithm to microscopic optical images of the avirulent 
Colombian native JUEZ1 strain cysts of T. gondii, automatic detection was achieved 
(Figs. 6 A, B, C, D). Thus it can be stated that the algorithm proved highly sensitive 
and reliable as a computational tool, also capable of accelerating the detection and 
counting processes of cysts of T. gondii in biological samples.  
 


+
+
=
=
f
if
i
j
i
C
y
Homogeneit
2)
(
1
Error =
Falsenegatives + Falsepositives
Truenegatives + Trupositives
=
0
60
= 0

Digital Processing of Toxoplasma gondii Cysts in Meat Samples for Human Consumption 
293 
   
     
 
 
   
    
 
Fig. 6. Image processing with Matlab 2013. Cysts of Toxoplasma gondii of Colombian native 
Strain JUEZ1. Giemsa staining, mouse brain tissue with eight-month infection. Cyst of 75um in 
diameter, 1000x (A). Cyst of 55um in diameter, 100x (B). Cyst of 97um in diameter, 1000x 
(C). Cyst of 75um in diameter, 1000x (D). Carl Zeiss 37081 AX10sKop 40 Microscope. 
4 
Conclusions  
The algorithm was tested with 60 images with the presence of cysts of Toxoplasma 
gondii, already identified. The results show high sensitivity and specificity (i.e. small 
error) as shown in formula 7. Applying this digital image processing algorithm to 
microscopic optical images of the avirulent Colombian native JUEZ1 strain, cysts of 
T. gondii were automatically detected, thus it can be stated that the algorithm proved 
highly sensitive and reliable as a computational tool, also capable of accelerating the 
detection and counting processes of cysts of T. gondii in a biological sample.  
This work provides a computational tool to deepen the study on toxoplasmosis in 
Colombia through the automation, unbiased detection and quantification of T. gondii 
cysts in biological samples. Future work shoukd continue with the analysis of a wide 
range of images in order to validate the efficiency of the algorithm. 
Acknowledgment. Barbara H. Zimmermann, Head of  BBMP research group at 
Universidad de Los Andes. Juan Miguel Escobar, Head of the Bioengineering 
program at Universidad El Bosque. 

294 
Q. Armando and J.-C. Graciela 
References 
1. Tenter, A.M., Heckeroth, A.R., Weiss, L.M.: Toxoplasma gondii: from animals to humans. 
Int. J. Parasitol. 31(2), 217–220 (2001) 
2. Aubert, D., Villena, I.: Detection of Toxoplasma gondiioocysts in water: proposition of a 
strategy and evaluation in Champagne-Ardenne Region, France. Mem. Inst. Oswaldo 
Cruz 104(2), 290–295 (2009) 
3. Derouin, F., Pelloux, H.: Prevention of toxoplasmosis in transplants patients. Clin. 
Microbiol. Infect. 14(12), 1089–1101 (2008) 
4. Yuan, Z., Gao, S., Liu, Q., Xia, X., Liu, X., Liu, B., Hu, R.: Toxoplasma gondii antibodies 
in cáncer patients. Cancer Lett. 254(1), 71–74 (2007) 
5. Israelski, D.M., Remington, J.S.: Toxoplasmic encephalitis in AIDS. Clin. Infect. 
Dis. 15(2), 211–222 (1992) 
6. Sibley, L.D., Boothroyd, J.C.: Virulent strains of Toxoplasma gondiicomprise a single 
clonal lineage. Nature 359(6390), 82–85 (1992) 
7. Shin, D.W., Cha, D.Y., Hua, Q.J., Cha, G.H., Lee, Y.H.: Seroprevalence of Toxoplasma 
gondii infection and characteristics of seropositive patients in general hospitals in Daejeon, 
Korea. Korean J. Parasitol. 47(2), 125–130 (2009) 
8. Dubey, J.P., Lindsay, D.S., Speer, C.A.: Structures of Toxoplasma gondii Tachyzoites, 
Bradyzoites, and Sporozoites and Biology and Development of Tissue Cysts. Clinical 
Microbiology Reviews 11, 267–299 (1998) 
9. Manger, I.D., Hehl, A., Parmley, S., Sibley, L.D., Marra, M., Hillier, L., Waterston, R., 
Boothroyd, J.C.: Expressed sequence tag analysis of the bradyzoite stage of Toxoplasma 
gondii: identification of developmentally regulate genes. Infect. Immun. 66(4), 1632–1637 
(1998) 
10. Weiss, L.M., Kim, K.: The development and biology of bradyzoites of Toxoplasma gondii. 
Front Biosci. 1(5), 391–405 (2000) 

Modeling of Tumour Growth Induced
by Circadian Rhythm Disruption
in Epithelial Tissue
Dmitry Bratsun1, Andrey Zakharov1, and Len Pismen2
1 Theoretical Physics Department, Perm State Humanitarian Pedagogical University,
614990, Perm, Russia
dmitribratsun@rambler.ru, az1211@mail.ru
2 Department of Chemical Engineering, Technion - Israel Institute of Technology,
32000, Haifa, Israel
pismen@technion.ac.il
Abstract. We propose a multiscale model of cancer tumour growth in
a quasi epithelial tissue. Basic model of the epithelium growth describes
the appearance of intensive movement and growth of tissue via mecha-
nisms of division and intercalation of cells. It is assumed that the move-
ment of cells is caused by the wave of mitogen-activated protein kinase
(MAPK), which in turn activated by the chemo-mechanical signal propa-
gating along tissue due to its local damage. It is assumed also that cancer
cells can arise from local failure of a spatial synchronization of circadian
rhythms. We hope that the subsequent study of the dynamic properties
of the model could determine the relationship between the occurrence
of the cancer cells and development of the entire tissue coordinating its
evolution through the exchange of chemical and mechanical signals.
Keywords: cancer modeling, circadian rhythms, gene regulation, sig-
naling, time-delay, complexity in biology.
1
Introduction
Cancer modeling has, over the years, grown immensely as one of the challeng-
ing topics involving applied mathematicians, physicists working with researchers
active in the biological sciences. The natural motivation is not only scientiﬁc as
the cancer has now become the most common cause of death [1] .
One of the main problems of mathematical modeling of cancer (as, indeed,
any biological system) is the multiscale nature of this phenomenon [2]. The
characterization of the system suggests the identiﬁcation of three natural scales
which are also connected to diﬀerent stages of the disease: processes on the
cellular scale are triggered by signals stemming from the sub-cellular level and
have an impact on the macroscopic scale, i.e. on the organism, when tumours
grow and spread. Let us consider some of the approaches to modeling. At the
cellular scale a system of coupled ODEs can be used to model large systems
of cell populations, where each variable corresponds to a well-deﬁned biological
c
⃝Springer International Publishing Switzerland 2015
295
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_31

296
D. Bratsun, A. Zakharov, and L. Pismen
property characteristic of all cells of the same population. The paper [3] seems
to be ﬁrst to initiate a systematic development of population dynamics models
focused on cancer. This approach has been developed by various authors toward
accounting for more ﬁne eﬀects (see, for example, survey [4]). The advantage of
the above approach is that models are easily tractable, allowing a relatively rapid
identiﬁcation of the parameters, but it omits potentially important phenomena,
such as spatial aspects, and heterogeneity among cells. Another type of modeling,
when in addition to population dynamics enter certain variables determining the
average structure of a population of cancer cells (for example, age of cells) can
be attributed to semi-phenomenological models with internal structure [5].
A large body of literature has been devoted to models which link the cellular
scale to the macroscopic tissue scale [2]. In this way, models can address how
changes in cell-cell interactions aﬀect the macroscopic properties of the tumour.
The most common approach here - the tumour is considered as a continuous
medium. As a rule, the PDEs system include the mass balance equation for the
cellular medium and of reaction-diﬀusion equations, which describes the ﬁeld
of chemical signals exchanged between cells. Such models can be attributed to
the phenomenological ones with all the evident disadvantages that inherent to
them. For example, in [6], the tumour is considered as a solid matrix of the
porous medium, which interacts with saturating its cellular liquid of normal
cells. Thus, the authors can describe some spatial features of real tumours.
A more eﬀective trend seems to be discrete simulations. Unlike continuum
models, discrete models have the ability to track the behaviour of single cells.
Due to advances in biotechnologies, there is an increasing amount of experimen-
tal data available at a single cell level which merit inclusion in mathematical
models. Here one can ﬁnd models based on cellular automata [7] or random walk
cells [8]. In addition, hybrid models began to appear, which include continuous
medium approach and the discrete nature of the tumour. There is a large group
of models dealing with the interaction at the sub-cellular and cellular level [2].
It is motivated by the fact that the whole system is driven, to some extent, by
genetic mutations. For example, the paper [9] addresses the issue as too strong
expression of certain genes can lead to malfunction of the cells, i.e. make it a
cancer cell. Here one can use a popular approach of the stochastic description of
the processes of gene regulation [10]. The weak point of this simulation is that
the process of formation of the tumour remains outside the model.
Since the cancer is multiscale phenomenon, the most realistic approach to
modeling requires consideration in the model processes at all levels of descrip-
tion. Due to the diﬃculties of this approach, there are not so many attempts to
do it. One of the ﬁrst attempt to implement it is commonly referred to [11]. Au-
thors have applied a hybrid approach, including calculation of cellular automata
whose state is determined by a continuous distribution of oxygen around the
blood vessel near the origin of the tumour. In another remarkable work [12] the
authors examined the spherically growing tumour, which included lattice-model
of discrete cells. The system of the ODEs describing the processes of a gene

Modeling of Tumour Growth Induced by Circadian Rhythm Disruption
297
regulation have been used separately for each cell in the population. Cells can
mechanically interact with each other through the lattice gas.
Thus, we can conclude that any realistic modeling of tumour growth involves
the development of a dynamic model of interaction of a large number of cells [13].
This model must take into account the physical properties of individual cells in
the ensemble: to describe a dynamically changing volume and surface of cells,
its elasticity with respect to external mechanical impacts, the ability to move,
divide, etc. These processes must be inﬂuenced by the exchange between the
cells of various signals. The signals should be generated at the sub-cellular level
(transcription/translations) or macroscopic level (collective behaviour). There-
fore, the model should be a combination of a discrete system with individual
cell dynamics and continuous medium chemical ﬁelds that are common to the
whole ensemble. The development of such models of complex systems is not triv-
ial. But the progress in this area over the last ten years and the simultaneous
breakthrough in the computer technology currently brought researchers to the
possibility of full realistic simulation of functioning of a living tissue.
In this paper we use the model of the growing epithelium, we have proposed
previously [14,15]. This model meets all the above criteria: it includes the cellular
level (division, proliferation and shape change of each cell under external pressure
of a whole ensemble), the level of the gene (the molecular dynamics of MAPK),
as well as the macroscopic level of the ensemble of cells (the collective behaviour
via polarization mechanism). This model seems to be the most realistic model
of a cellular tissue reported in the literature. Although the epithelium is a quasi-
2D medium, the model can be easily generalized to the case of 3D tissue. In
numerical calculations [15], the number of cells reached several thousands. But
the calculation of larger systems is limited only by the power of the computer.
The main idea of this work is that a tumour occurrence driven by the circa-
dian rhythm disruption in the epithelial tissue. It has been recognized in recent
years that core circadian genes seem to be important in tissue homeostasis and
tumorigenesis. Many studies have shown that circadian clock gene deregulation
is implicated in the development of cancer and other diseases [16,17]. Circadian
rhythms are biological rhythms that are common to almost all living organ-
isms. Timing of circadian clocks is established in a cell-autonomous manner by
a self-sustaining molecular oscillator that consists of intertwined negative and
positive transcription/translation-based feedback loops. We have supplemented
the epithelium model [15] by sub-cellular dynamic model of circadian rhythms
proposed in our previous paper [18], where the time-delay eﬀect of protein syn-
thesis reactions in the transcription and translation processes of genes is the key
element of oscillations mechanism. In addition, we have developed the simple
phenomenological model of the cell diﬀerentiation.
2
Cellular Model of Epithelial Tissue
Epithelial tissue is a layer of cells covering the surface of an organ or body.
Thus, one may use only quasi-two-dimensional system in modeling the epithe-
lium behaviour, that makes the calculations easier. The model suggested in [15]

298
D. Bratsun, A. Zakharov, and L. Pismen
includes the calculation of separate cells dynamics, which are presented in the
form of polygons. The initial conﬁguration is a regular hexagonal lattice. In the
course of spreading, it becomes distorted and incorporates also polygons with
a diﬀerent number of vertices due to cell division and proliferation. Generally,
the system has been dimensioned in such a way, as hexagonal cell is the most
probable form of a cell. The cells are closely located to each other forming solid
two-dimensional epithelial surface (Fig.1).
The model has a set of properties, which are suitable to simulate the real
features of the epithelium:
– possibility to change the cell’s size in the process of tissue evolution and to
change the local mechanical properties of environment;
– possibility for the total number of cells to spread by their division;
– possibility for the cells to move by the mechanism of intercalation;
– the calculation of the dynamics of concentration, which participate in the
regulation of tissues activities;
– exchange of chemical signals between the neighbouring cells through common
border (Fig.1);
– eﬀect of cells polarization, which occurs under the external conditions.
Fig. 1. Elements of the chemo-mechanical
model of an epithelial tissue
To describe the model in more de-
tail, we deﬁne the elastic potential en-
ergy E of the tissue by summing up
the contributions of the perimeter L
and the area A of each cell:
E = 1
2

cells

μL2 + η(A −A0)2
,
(1)
where μ is attributed to the action
of active contractile forces within the
cell cortex, η is an elastic constant
that reﬂects resistance to stretching or
compressing the cell vertically when
its area decreases or increases at con-
served volume, and A0 is the reference cell area.
The tissue is evolved by moving the cell nodes (Fig.1). The mechanical force
acting on any jth node is deﬁned as the derivative of the potential energy with
respect to the node position Rj :
Fj
mech = −∂E
∂Rj
.
(2)
The crucial ingredient of the model is the active force [15]. For any jth cell,
the active force is directed along the polarization vector Pj and is dependent
on the local MAPK concentration Mj. We assume that the cells polarize un-
der the inﬂuence of the active force exerted by adjacent cells, which reﬂects cell

Modeling of Tumour Growth Induced by Circadian Rhythm Disruption
299
interaction through adherens junctions and ensures coherence of motion. Ac-
cordingly, the active force is computed as the average over adjacent cells:
Fj
act = ⟨MkPk⟩k∈adj(j) .
(3)
Then we write the dynamic equation of the polarization vector as
dPj
dt
= Dp

κFj
act + Fst

−βpPj,
(4)
where Dp is the polarization mobility coeﬃcient, κ is the polarization diﬀusivity,
βp is the linear polarization decay and Fst is an uncorrelated zero mean stochas-
tic input. Since the motion is strongly overdamped, the appropriate equation
governing the displacement velocities Vi should be based on Aristotelean rather
than Newtonian dynamics, having the form similar to the Darcy law with the
mobility coeﬃcient K. We also introduce a threshold force F0 below which the
node remains immobile:
Vi = dRi
dt
= KH

|Fi
mech + Fi
act| −F0

(Fi
mech + Fi
act),
(5)
where H is the Heaviside step function. Although the mobility should be gen-
erally anisotropic in a polarizable medium, we assume K to be scalar, both for
simplicity and due to the lack of data. Generally, the equations (1-2,4-5) deﬁne
the dynamics of the tissue on the cellular level. The active force (3) links the
cellular scale to the macroscopic tissue scale.
Let us discuss now the gene level. MAPK activation, caused by either chemical
or mechanical signaling, plays a crucial role in cell spreading, as indicated by
suppression of spreading by MAPK inhibition [19]. Although the mechanism of
MAPK action remains so far unclear, it is reasonable to assume that it enables
active traction, as implied in Eq. (3). The experiment suggests that MAPK can
be activated both chemically and mechanically. We assume that the MAPK level
is driven by both the injury signal C and strain σ = A−A0, deﬁned as the change
of the cell area relative to the reference value A0. The dynamic equation of the
activated MAPK concentration in a ith cell is written as
τ0
dMi
dt
= Ci + aH(σi)σi
1 + bσi
−βmMi,
(6)
where the MAPK activation is assumed to be linear in the signal concentration,
βm is the linear decay coeﬃcient and τ0 is the capacity constant. The mechani-
cally induced activation is supposed to occur under extension only.
It is assumed that the signaling species is transported diﬀusively from one
cell to the other, whereas its ﬂux does not depend on the distance between the
two cells i and j but is proportional to the boundary length Lij. This implies
that the transport is limited by the transfer though cell membranes. Allowing
for a linear decay with the coeﬃcient βc, we write the equation for the signal
concentration in a jth cell as
dCj
dt =

i∈adj(j)
αLij(Ci −Cj) −βcCj,
(7)

300
D. Bratsun, A. Zakharov, and L. Pismen
where α is the transfer coeﬃcient. The link between sub-cellular and macroscopic
scales is established through the Eq. (7), since the ﬁeld of the activator C is global
for the whole tissue.
3
Submodel of Circadian Rhythms
Fig. 2. Network architecture of the circa-
dian rhythm molecular components [18]
To describe the circadian rhythms in
cells we use the dynamic model pro-
posed in [18]. The model is rather
general by nature, although it was
originally suggested to describe the
circadian rhythms of the organism
Neurospora crassa. The time-delay ef-
fect of protein synthesis reactions in
the transcription and translation pro-
cesses of genes is the key element
of oscillations mechanism (Fig.2). In
fact, the time delay in the processes of
transcription/translation is, perhaps,
the easiest source of oscillations in the
genetic systems [20]. These processes
are both very slow and consist of multistage biochemical reactions involving the
sequential assembly of long molecules. Thus, these processes are long in time and
particularly time-delayed. It is evident that the delay prevents the system from
achieving equilibrium, and results instead in the familiar limit cycle oscillations.
We have shown in [13,18] that the primary set of biochemical reactions re-
ﬂecting the molecular components of bioclocks can be reduced to the following
two-variable discrete reaction-diﬀusion system for ith cell:
dFi
dt =
1
(1 + 4KF
1 Fi)

kF
KW
1 KF
2 W 2
i (t −τ)
1 + KW
1 KF
2 W 2
i (t −τ) −BF Fi −kFiWi

+
+

j∈adj(i)
αLij(Fj −Fi),
(8)
dWi
dt
=
1
(1 + 4KW
1 Wi)

kW
KF
1 KW
2 F 2
i (t −τ)
1 + KF
1 KW
2 F 2
i (t −τ) −BW Wi −kFiWi

,
(9)
where F and W stand for number of isolated monomers of two proteins re-
sponsible for the circadian rhythms [18]. The core of the model describing the
temporal behavior of the system is based on the interplay between two dynamical
variables, concentrations of the F and W proteins, where the synthesis of both
proteins is delayed in time. For the sake of simplicity we have not introduced a
separate type of protein transporting the circadian signal outside the cell. This
means that the synthesis of the transport protein occurs relatively quickly com-
pared with the characteristic period of the circadian rhythm. This assumption

Modeling of Tumour Growth Induced by Circadian Rhythm Disruption
301
is justiﬁed, since the transcription of circadian genes usually a lengthy process.
In the suggested model the transportation function is performed by the same
protein F (8). Its partner, protein W, may move only inside the cell space. The
term responsible for the cell-to-cell signaling in Eq.8 is written similarly as it
was in (7). Notice that after the division of a cell, the daughter cells inherit the
phase of the circadian rhythm of the parent cell.
Spatio-temporal dynamics of the system (8-9) has been studied in [13,18]. We
found the eﬀect of clustering of cells [13] and diﬀerent wave patterns [18].
4
Submodel of Diﬀerentiation of Cells
As already mentioned above, core circadian genes seem to be important in tissue
homeostasis and tumorigenesis. Molecular mechanisms of generation of circadian
rhythms organize a biochemical network in suprachiasmatic nucleus and periph-
eral tissues, building cell autonomous clock pacemakers. Rhythmicity is observed
in transcriptional expression of a wide range of clock-controlled genes that reg-
ulate a variety of normal cell functions, such as cell division and proliferation.
Desynchrony of this rhythmicity seems to be implicated in several pathologic
conditions, including tumorigenesis and progression of cancer. There is more
and more increasing evidence that links dysfunction of the clockwork with the
pathogenesis of cancer [16,17].
Fig. 3. Clustering of circadian rhythms: os-
cillations of the F protein in the epithelium
Since the exact mechanism linking
the circadian oscillations and cancer
is unknown, we propose a simple phe-
nomenological model. The basic idea
of the mechanism of diﬀerentiation is
a local oscillation phase failure in the
common ﬁeld of spatial synchronized
circadian rhythms in the epithelial tis-
sue.
The preliminary numerical simula-
tions have shown that in the case of a
large amount of cells a complete syn-
chronization, meaning total alignment
of the oscillation phases in all cells
cannot be achieved. Instead here the
macroscopic eﬀect of clustering is revealed: the cells develop two approximately
equal communities, which collectively oscillate in anti-phase (Fig.3). These two
groups are separated by a thin layer of cells oscillating with the intermediate
values of phase. Let us introduce the local value of the phase diﬀerence of the
circadian rhythm of jth cell with respect to the adjacent cells:
Φj = ⟨|φj −φk|⟩k∈adj(j) ,
(10)
where φ is the oscillation phase. If the cell is in a fully synchronized ﬁeld, the
value of Φ, obviously, will be zero, since the cell oscillation phase will not diﬀer

302
D. Bratsun, A. Zakharov, and L. Pismen
from the oscillation phase of neighbors. On the other hand, the dephasing (10)
grows near the boundary of the clusters (Fig.3). The maximum value of the
dephasing would occur in case of an isolated cell inside the phase cluster (there
are several such cells in Fig.3). According to our hypothesis, this cell is most
at risk to diﬀerentiate into cancerous state. Then, the dynamic equation for the
state of jth cell can be written as
dXi
dt = −λXi(1 −Xi)(A −Xi) + Φiξi(t),
(11)
where X is the state function, λ is the damping parameter, ξ(t) is a noise term
with the amplitude depending on the dephase value. Without the noise Eq.(11)
has two stable stationary solutions: X = 0 stands for the normal state of the
cell, X = 1 is responsible for the cancer state. The parameters of the model are
calibrated so that the diﬀerentiation would occur under the inﬂuence of noise,
but only at a high value of the dephasing (Fig.4), and the reverse transition of
already diﬀerentiated cell is not possible in principle.
Fig. 4. Phase portrait of the system (11)
without the noise (top) and under non-zero
stochastic input (bottom)
As it known, while the normal cells
stop division in the presence of genetic
damage, the cancer cells continue to
divide. The results of this are daugh-
ter cells that contain abnormal DNA.
Continued cell division leads to the
formation of a tumour. In order to
model this transition, the set of pa-
rameters that determine the physical
and chemo-mechanical properties (η,
A0) and parameters of cell functioning
(for example, a rate of cell division)
of cancer cells has been changed after
the diﬀerentiation. Thus, it is formed
a separate species of cells that consti-
tute their own medium (tumour).
5
Numerical Simulations
The model equations contain a large number of parameters; their values are
mostly not known from independent measurements but only their relative values
are essential. In our computations, we adopt the values in the ranges making the
simulation results qualitatively compatible with the experimental observations
(where it is possible). The typical values of the parameters are given in Table 1.
The typical result of numerical simulation is presented in Fig.5. One can see
that the size and shape of the cells diﬀerentiated into the cancerous state diﬀer
from the normal members of the community. Cancer cells are approximately
twice as large and irregular in a shape. The normal healthy cells that border
on the cancer cells are experiencing the signiﬁcant stress: they are squeezed and

Modeling of Tumour Growth Induced by Circadian Rhythm Disruption
303
Table 1. List of principal parameters
Mechanical Properties and Polarization
μ
η
K
F0
A0(norm.)
D
βp
κ
A0(cancer)
1.0
1.0
1.0
0.02
3
√
3/2
0.4
1.0
0.1
9
√
3/4
Signal and MAPK
τ0
a
b
βm
βc
α
0.1
500
70
1.0
0.1
0.1
Circadian rhythms and Diﬀerentiation
τ
k
kF
kW
KF
1
KF
2
KW
1
KW
2
BF
BW
λ
A
6.0
30
8.0
4.0
5.0
5.0
5.0
5.0
0.3
0.4
10
0.15
t=25
t=50
t=75
t=100
Fig. 5. Evolution of the cancer tumour in epithelial tissue in time. Scaling corresponds
to changes in cell size relative to the average size of a normal cell.
stretched. Since the period of the division of cancer cells is shorter, the tumour
evolves rapidly increasing the occupied area.

304
D. Bratsun, A. Zakharov, and L. Pismen
6
Discussion and Conclusions
The cancer is certainly the complex biological system. Therefore the cancer mod-
eling dealt with in this paper needs a miltiscale mathematical approach. We pro-
pose a minimal multiscale model which includes three natural scales connected
to diﬀerent stages of the disease: processes on the cellular scale are triggered by
circadian rhythm signals stemming from sub-cellular level and have an impact
on the macroscopic scale characterized by a long-range coordination of dynam-
ics of multiple cells in epithelium. The basic medium, where tumour grows and
spreads, is constructed to take into account chemo-mechanical interactions in-
cluding a chemical eﬀect of strain, chemically induced polarization and active
traction, and interaction between polarized cells. Many details of this model need
experimental veriﬁcation, as direct identiﬁcation of the mechanisms involved in
this feedback loop is not yet available.
Nonetheless, research across the last three decades has revolutionized our
understanding of cancer. In large part, this success was made possible by the
development and application of the techniques of molecular biology, techniques
that enabled researchers to probe and describe features of individual cells in
ways unimaginable not so long ago. Today, we know that cancer is a disease of
molecules and genes, and we even know some of them involved.
To our opinion, a new type of simulation should be able to include experi-
mentally discovered new molecular mechanisms. Our model is designed so that
it can easily be modiﬁed to take account of any new feedbacks arising between
chemo-mechanical behavior of cells and the gene regulation processes. In this
version of the model we have considered only two of these molecular mech-
anisms that have been studied experimentally: the excitation of the wave of
mitogen-activated protein kinase responsible for the movement of cells and cir-
cadian rhythms generation responsible for the synchronization of cell activity in
a tissue. But other mechanisms can be easily added. Moreover, since the present
model includes individual behavior of each cell with its own gene regulation dy-
namics, it can be used to simulate the targeted therapeutic interventions for
various types of cancer. As it is known, many proteins possess interacting faces
with unique recognition patterns that allow them to selectively recognize and
bind to proteins to form homo- and hetero-complexes, thus activating biological
pathways. Therefore, targeting protein-protein interactions has the potential to
produce highly selective drugs. Computational approaches can assist in the iden-
tiﬁcation and optimization of ligands at all stages of the drug discovery process,
and computer-aided rational design is a particularly powerful tool for studying
the interaction of agents at protein-protein interfaces. In the model presented
here an important role has been played by the hetero-dimer complex FW which
ensures the generation of the circadian rhythm in each cell (see Eqs. 8-9). Partic-
ularly intriguing result of the work is also the possibility of modeling the impact
of the intracellular processes on the dynamics of a whole organ and vice versa.
We anticipate that this work will encourage further quantitative studies and
will, in its turn, be adjusted and modiﬁed as more data become available.

Modeling of Tumour Growth Induced by Circadian Rhythm Disruption
305
Acknowledgements. The research has been supported by the Ministry of Ed-
ucation of Perm Region (grant C-26/244), Program of Strategic Development of
Perm State Humanitarian Pedagogical University (project 031-F) and grant of
Russian Fund for Basic Research (14-01-96022r ural a).
References
1. Weber, G.F.: Molecular Mechanisms of Cancer. Springer (2007)
2. Bellomo, N., Li, N.K., Maini, P.K.: On the foundations of cancer modelling: se-
lected topics, speculations, and perspectives. Mathematical Models and Methods
in Applied Sciences 18, 593–646 (2008)
3. Gyllenberg, M., Webb, G.: A nonlinear structured population model of tumour
growth with quiescence. J. Math. Biol. 28, 671–684 (1990)
4. Kimmel, M., Lachowicz, M., ´Swierniak, A.: Cancer growth and progression, math-
ematical problems and computer simulations. Int. J. Appl. Math. Comput. Sci. 13,
279–429 (2003)
5. Dyson, J., Villella-Bressan, R., Webb, G.: The steady state of a maturity structured
tumor cord cell population. Discr. Cont. Dyn. Syst. B 4, 115–134 (2004)
6. Ambrosi, D., Preziosi, L.: On the closure of mass balance models for tumour growth.
Math. Mod. Meth. Appl. Sci. 12, 737–754 (2002)
7. Smolle, J., Stettner, H.: Computer simulation of tumour cell invasion by a stochas-
tic growth model. J. Math. Biol. 160, 63–72 (1993)
8. Anderson, A.: A hybrid mathematical model of solid tumour invasion: The impor-
tance of cell adhesion. Math. Med. Biol. 22, 163–186 (2005)
9. Vogelstein, B., Kinzler, K.W.: Cancer genes and the pathways they control. Nature
Med. 10, 789–799 (2004)
10. Komarova, N.: Stochastic modeling of loss- and gain-of-function mutation in cancer.
Math. Mod. Meth. Appl. Sci. 17, 1647–1674 (2007)
11. Alarcon, T.: A cellular automaton model for tumour growth in inhomogenous en-
vironment. J. Theor. Biol. 225, 257–274 (2003)
12. Kim, Y., Stolarska, M.A., Othmer, H.G.: A hybrid model for tumour spheroid
growth in vitro I: Theoretical development and early results. Math. Mod. Meth.
Appl. Sci. 17, 1773–1798 (2007)
13. Zakharov, A., Bratsun, D.: Synchronization of Circadian Rhythms at Scale of Gene,
Cell and Whole Organism. In: Sanayei, A., Zelinka, I., Rossler, O.E. (eds.) ISCS
2013. Emergence, Complexity and Computation, vol. 8, pp. 345–355. Springer,
Heidelberg (2014)
14. Viktorinov´a, I., Pismen, L.M., Aigouy, B., Dahmann, C.: Modelling planar polar-
ity of epithelia: The role of signal relay in collective cell polarization. J. R. Soc.
Interface 8, 1059–1063 (2011)
15. Salm, M., Pismen, L.M.: Chemical and mechanical signaling in epithelial spreading.
Phys. Biol. 9, 026009–026023 (2012)
16. Greene, M.W.: Circadian rhythms and tumor growth. Cancer Letters 318, 115–123
(2012)
17. Rossetti, S., Esposito, J., Corlazzoli, F., Gregorski, A., Sacchi, N.: Entrainment
of breast (cancer) epithelial cells detects distinct circadian oscillation patterns for
clock and hormone receptor genes. Cell Cycle 11, 350–360 (2012)

306
D. Bratsun, A. Zakharov, and L. Pismen
18. Bratsun, D., Zakharov, A.: Deterministic modeling spatio-temporal dynamics of
delay-induced circadian oscillations in Neurospora crassa. In: Sanayei, A., Zelinka,
I., Rossler, O.E. (eds.) ISCS 2013. Emergence, Complexity and Computation,
vol. 8, pp. 179–189. Springer, Heidelberg (2014)
19. Nikolic, D.L., Boettiger, A.N., Bar-Sagi, D., Carbeck, J.D., Shvartsman, S.Y.: Role
of boundary conditions in an experimental model of epithelial wound healing. Am.
J. Physiol. Cell Physiol. 291, 68–75 (2006)
20. Bratsun, D., Volfson, D., Hasty, J., Tsimring, L.S.: Delay-induced stochastic oscil-
lations in gene regulation. Proc. Natl. Acad. Sci. U.S.A. 102, 14593–14598 (2005)

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Part IV 
 
Complex Networks 
 
 
 
 
 
 
 
 
 
 
 
 

Comparing Overlapping Properties
of Real Bipartite Networks
Fabien Tarissan
Sorbonne Universit´es, UPMC Universit´e Paris 6 and CNRS, UMR 7606, LIP6, Paris
Abstract. Many real-world networks lend themselves to the use of
graphs for analysing and modelling their structure. But such a simple
representation has proven to miss some important and non trivial prop-
erties hidden in the bipartite structure of the networks. Recent papers
have shown that overlapping properties seem to be present in bipartite
networks and that it could explain better the properties observed in sim-
ple graphs. This work intends to investigate this question by studying
two proposed metrics to account for overlapping structures in bipartite
networks. The study, conducted on four dataset stemming from very dif-
ferent contexts (computer science, juridical science and social science),
shows that the most popular metrics, the clustering coeﬃcient, turns
out to be less relevant that the recent redundancy coeﬃcient to analyse
intricate overlapping properties of real networks.
Keywords: Complex networks, Bipartite graphs, Social networks,
Overlapping.
1
Introduction
Many complex networks lend themselves to the use of graphs for analysing and
modelling their structure. Usually, vertices of the graph stand for the nodes of
the network and the edges between vertices stand for (possible) interactions be-
tween nodes of the network. This approach have proven to be useful to identify
non trivial properties of the structure of networks in very diﬀerent contexts,
ranging from computer science (the Internet, peer-to-peer networks, the web),
to biology (protein-protein interaction networks, gene regulation networks), so-
cial science (friendship networks, collaboration networks), linguistics, economy,
etc. [17,4,12,2,6,13,1,14].
Although useful, such a simple representation is not particularly close to the
real structure of most of real networks. If one considers for instance actor net-
works which link actors performing in the same movies [17,11] or co-authoring
networks which link authors publishing together [11,12], one would rather relate
actors to the movies they performed in and authors to their papers. This obser-
vation led the community to use bipartite graphs instead, i.e. graphs in which
nodes can be divided into two disjoint sets, ⊤(e.g. movies) and ⊥(e.g. actors),
such that every link connects a node in ⊤to one in ⊥. Bipartite graphs are a
c
⃝Springer International Publishing Switzerland 2015
309
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_32

310
F. Tarissan
fundamental object which has proven to be very eﬃcient for both the analy-
sis [5,16,1,14] and the modelling [3,15] of complex networks as it is able to reveal
patterns that could not have been detected on simple graphs.
In a recent work [15], this framework has been investigated in an attempt to
propose, for the ﬁrst time, a bipartite model of the Internet topology. It relies
on recent developments in topology discovery [8,7] that allows for revealing two
layers in the Internet structure. The model remains simple: it only takes as in-
put the node degree sequence for both layers and randomly generates a bipartite
graph respecting those distributions. The paper showed that, despite the sim-
plicity of the model, realistic network properties, such as high local density and
non trivial correlations among properties of the nodes of the lower layer, emerge
naturally. But it also showed that the model fails in reproducing the overlapping
observed in the two-layer structure.
The present paper extends the analysis of overlapping structures to a wide va-
riety of networks and tries to identify how bipartite metrics can account for those
complex properties. In particular, it investigates whether two recently proposed
metrics, namely the bipartite clustering coeﬃcient and bipartite redundancy co-
eﬃcient, are relevant for explaining the observed overlaps.
The remaining of the paper is organised as follow: Section 2 will review the
technical background necessary for going throughout the paper; Section 3 will
present the main obtained results and ﬁnally Section 4 will conclude the paper
and open on new perspectives.
2
Background
In this section, we introduce the required background for the remainder of the
paper. First, we focus on the diﬀerent dataset (Section 2.1) we used in this study.
Then, we recall the necessary deﬁnitions of the bipartite graph framework and
its related metrics (Section 2.2).
2.1
Dataset
As stated in the introduction, many real networks exhibit a complex structure
that involves several layers. In order to be as general as possible in the present
study, we used a wide variety of networks presenting a two-level structure. We
chose to focus on an infrastructure network (Internet), a juridical network (In-
ternational Criminal Court decisions network) and two social networks (a co-
publication network and a network composed of YouTube users). Here below
we describe the four dataset and precise, for each one, the meaning of the upper
layer (⊤nodes) and the lower layer (⊥nodes):
Internet [15,8]: In this network, ⊥nodes stands for Internet routers and ⊤
nodes indicates the presence of Ethernet switches whose purpose is to in-
duce indirect connections among routers. The dataset used in this study
corresponds to a measurement campaign conducted in September 2006.

Comparing Overlapping Properties of Real Bipartite Networks
311
ICC [14]: This dataset describes the juridical decisions taken by the Interna-
tional Criminal Court (ICC) in the Lubanga case. Here, ⊥nodes stands for
the juridical decisions made by the judges and ⊤nodes for the articles of the
Rome Statute they invoked. The dataset was extracted from a public server
in March of 2013.
Publications [10]: This network describes the scientiﬁc collaboration among
researchers through co-published papers. It is based on preprints posted to
the Condensed Matter section of arXiv E-Print Archive between 1995 and
1999. In this network, ⊥nodes stands for authors and ⊤nodes for articles.
YouTube [9]: This dataset describes some characteristics of YouTube users.
It has been collected in 2007 and show the relation between users (⊥) and
their membership (⊤).
As we will see further (see Section 3.1 in particular), although the nature of
those networks are very diﬀerent, their two-level structure share some particular
and non trivial properties, among which the classical heterogeneous distribu-
tion of the degree of the nodes. Note that the size of the networks varies from
thousands of nodes to hundred of thousand of nodes. For this reason, all the
distributions that we will study further will be normalised by the size of the
networks in order to ease the comparisons.
2.2
Bipartite Graphs
Bipartite graphs – also referred to sometime by two-mode networks – are triplets
Gb = (⊤, ⊥, Eb), where ⊤is the set of top nodes (the papers in the Publication
dataset for instance), ⊥the set of bottom nodes (the authors), and Eb ⊆⊤× ⊥
the set of links between ⊤and ⊥(that relate the papers to their authors in our
example). We denote by n⊤(resp. n⊥) the number of top nodes (resp. bottom
nodes) and by mbip the number of links.
Compared to standard graphs, nodes in a bipartite graph are separated in two
disjoint sets, and the links are always between a node in one set and a node in
the other set. Note that from a given bipartite graph, one can always induce a
corresponding simple graph by a ⊥-projection. In the case of the Publication
network, it would generate a simple graph in which nodes are authors and a link
relates two authors if they have published a joint paper. This would allow to
reuse all the metrics deﬁned for standard graphs.
But we can also compute speciﬁc metrics for bipartite graphs, such as k⊤
(resp. k⊥) the average degree of top nodes (resp. bottom nodes), d+
⊤(resp. d+
⊥)
the maximal degree observed in top nodes (resp. bottom nodes) and δb =
mb
n⊤.n⊥
the density of the bipartite graph.
Those are natural extensions of standard metrics deﬁned for simple graphs.
But for more intricate properties, it can be tedious to propose a ”natural” def-
inition. This is the case for the local density in the graph (more or less the
density around a node) which is usually captured by the clustering coeﬃcient.

312
F. Tarissan
The reason for the diﬃculty in deﬁning such an extension is that it relies on
the presence of triangles which does not exist in bipartite graphs. As suggested
in [5], one can however rely on the following coeﬃcient that tends to capture the
overlapping between the neighbourhood of two nodes of ⊤. Let N⊤(u) for u ∈⊤
denote the set of neighbours (i.e. bottom nodes u is linked to) and N⊥(u) the
dual deﬁnition for ⊥nodes. Then we deﬁne:
cc⊤(u, v) = |N⊤(u) ∩N⊤(v)|
|N⊤(u) ∪N⊤(v)|.
(1)
This coeﬃcient is interesting as it captures the relative overlap between neigh-
bourhoods of top nodes, i.e. cc⊤(u, v) is equal to 1 if the neighbourhood of u
and v intersects exactly, to 0 if they do not share any neighbour. From this coef-
ﬁcient, it becomes natural to deﬁne the clustering coeﬃcient related to a speciﬁc
⊤node v. This is given by:
cc⊤(v) =

u∈N⊥N⊤(v)
cc⊤(u, v)
|N⊥N⊤(v)|
.
(2)
This coeﬃcient enables in particular to study the distribution of this property
over the top nodes as well as its correlation with the degree or other properties.
Then one can naturally compute the bipartite top clustering coeﬃcient ccbip of
Gb as the average value of cc⊤(v) over all the nodes v of ⊤. More formally:
ccbip(Gb) =
1
|⊤|

v∈⊤
cc⊤(v).
(3)
However it has been shown in [5] that this coeﬃcient might miss some impor-
tant properties of the overlapping between ⊤nodes in the bipartite structures.
This is why the authors suggested to use the redundancy coeﬃcient rd⊤(v) of a
node v which focuses on the impact of removing v as regard the ⊥-projection.
Intuitively, a high value of the coeﬃcient indicates that two ⊥nodes v relates
are likely to be related by another ⊤node. Formally, the coeﬃcient is given by:
rd⊤(v) = |{{u, w} ∈N⊤(v)2 s.t. ∃v′ ̸= v, (v′, u) ∈Eb and (v′, w) ∈Eb}|
|N⊤(v)|(|N⊤(v)|−1)
2
.
(4)
Following this deﬁnition, we can derive naturally the redundancy coeﬃcient
rdbip of the bipartite graph Gb, deﬁned as the average value of the former coef-
ﬁcient over all ⊤nodes. More formally:
rdbip(Gb) =
1
|⊤|

v∈⊤
rd⊤(v).
(5)
3
Analysis of the Bipartite Structure
The purpose of this section is to analyse the overlapping observed in the bipartite
structure of the four dataset presented in Section 2.1. We will focus in particular

Comparing Overlapping Properties of Real Bipartite Networks
313
Table 1. Global properties of the bipartite structure of the dataset
Internet
ICC
Publication
YouTube
n⊤
10 224
713
22 015
30 087
n⊥
9 758
1 360
16 726
94 238
mb
25 422
6 670
58 595
293 360
δb(∗10−3)
0.26
6.88
0.16
0.10
k⊤
2.5
9.4
2.7
9.8
k⊥
2.6
4.9
3.5
3.1
d+
⊤
58
250
18
7 591
d+
⊥
41
81
116
1 035
Table 2. Value of the overlapping coeﬃcients of the bipartite structures
Internet
ICC
Publication
YouTube
ccbip
0.32
0.15
0.39
0.16
rdbip
0.11
0.69
0.63
0.33
to the two metrics that have been proposed to account for such a topological
property, namely the bipartite clustering coeﬃcients and the bipartite redun-
dancy coeﬃcients (referred to further simply as clustering and redundancy).
First, we start by looking at some global and standard statistics deﬁned for bi-
partite graphs (Section 3.1). Then we turn to the overlapping properties and
study distributions and correlations among the diﬀerent metrics (Section 3.2).
3.1
A Global Perspective
The ﬁrst statistics we focus on concern some basic properties observed in most
real-world networks, formally presented in the previous section. Table 1 presents
the results for the four dataset of Section 2.1. As expected, all usual observa-
tions made on real-world networks stand also for the networks under study. In
particular the graph is sparse (on the order of magnitude of 10−4) and the max-
imal degree is several orders of magnitude higher that the average degree, which
indicates usually some heterogeneity in the degree of the nodes.
This is conﬁrmed by the inverse cumulative distribution of the degree of the
nodes (both ⊤and ⊥) presented in Figure 1. It clearly shows a heavy-tail dis-
tribution for all the four dataset and the nodes of the two layers.
3.2
Analysis of the Overlapping Structure
We focus now more precisely to the core of the analysis related to the overlap-
ping in the bipartite structure. First, Table 2 presents the global values of the
two coeﬃcients computed for all the dataset. It shows that, although the two

314
F. Tarissan
(a) Top nodes
(b) Bottom nodes
Fig. 1. Inverse CDF of the node degree distribution
coeﬃcients intend to capture the same property (overlapping patterns), they
strongly diﬀer on each dataset. The most obvious case is the ICC since the clus-
tering coeﬃcient is quite low (0.15) but the redundancy is very high (0.69). For
the other dataset the gap is less important but we can notice that the higher
coeﬃcient depends on the dataset thus showing that no general behaviour can
be drown here.
Those global average metrics do not allow for a detailed comprehension of
the coeﬃcients. Fortunately, we can compute them for each of the ⊤nodes in
the networks. This allows to study several properties related to it such as the
distribution of the coeﬃcients. Figure 2 presents the inverse cumulative distri-
bution of the clustering (Figure 2(a)) and the redundancy (Figure 2(a)). We can
observe that the distributions of the two coeﬃcients are very diﬀerent. For the
clustering, the plot shows that the decrease of the value is very sharp and for
low values. The majority of ⊤nodes have indeed a small clustering coeﬃcient.
This indicates that the overlapping, according to this metrics, is not particularly
important in the networks.
For the redundancy, the behaviour is diﬀerent. Except for the Internet case,
for which one can observe a sharp decrease, the value is uniformly distributed
among the nodes. As opposed to the clustering, this seems to indicate on the
contrary that some overlaps are present in three over four dataset. Note for
instance that the fraction of ⊤nodes having a redundancy of 1 is non negligible:
9% in Internet, 13% in the YouTube case, 46% in the ICC network and 52%
in the Publication network. Taking this last case as an example it means that,
for more than half of the articles of the Publication network, every authors
have also published together at least one other article. This indicates a strong
overlapping in the network which, in the case of co-publication networks, is not
surprising but is not captured by the clustering coeﬃcient.

Comparing Overlapping Properties of Real Bipartite Networks
315
(a) Clustering
(b) Redundancy
Fig. 2. Inverse CDF of the clustering and redundancy coeﬃcients
The distribution shown above is interesting but it does not help to understand
why some coeﬃcients are high and other low. In order to understand better the
situation, we show Figure 3 the correlation between the degree of a ⊤node
and the value of its clustering (Figure 3(a)) or its redundancy (Figure 3(b)).
More precisely, a (x, y) dot in the plots means that the average value of the
coeﬃcient for nodes having degree x is y. Figure 3(a) shows a very interesting
fact: the value of the clustering seems to be completely governed by the degree
of the corresponding node. The higher the degree, the lower the clustering. Such
a correlation makes the interest of the coeﬃcient weak since it seems derivable
from the degree of the nodes. On the contrary, Figure 3(b) does not present
such a correlation, except for the Publication network for which one observe a
similar behaviour. The notion engulfed in the redundancy coeﬃcient seems then,
to that regard, contain more information than simpler local properties.
4
Conclusion
In this paper, we studied the overlapping properties observed in the bipartite
structure of diﬀerent networks exhibiting a two-level structure. The main con-
cern of the study was to discriminate between two recently proposed metrics to
account for such properties, namely the clustering coeﬃcient and the redundancy
coeﬃcient.
By analysing the structure of 4 networks stemming from very diﬀerent con-
texts, we showed that the notion captured by the clustering coeﬃcient turns out
to be quite poor as it is closely related to the simple degree of the node. On the
contrary, the behaviour of the redundancy coeﬃcient is totally unpredictable
regarding local properties such as the degree. The value of the coeﬃcient is not
related to simple local properties of the nodes, at least in 3 of the 4 dataset of
the study.

316
F. Tarissan
(a) Clustering
(b) Redundancy
Fig. 3. Correlation between the degree and the overlapping coeﬃcients
Understanding the characteristics of the bipartite structure of real networks
are fundamental for several reasons. First, as shown in several studies, such
structures help understanding non trivial properties of simple networks (see [15]
for instance). But more importantly it has been shown to be a better support
for models, enabling in particular to generate random structures closer to real
ones than most of classical models [15].
To that regard, the present work opens the way to several improvements in
recently proposed models. It shows in particular that one could improve bipartite
models by integrating such a property in the model, which has not been done
so far. One way to achieve this goal would be to encode the redundancy in an
artiﬁcial third level and control the coeﬃcient by randomly permuting links in
such a tripartite structure. We let such an investigation as a further work.
Acknowledgement. This work is partly funded by the National Center for
Scientiﬁc Research (CNRS) through the PEPS Project ”DoR´e”.
References
1. Ahn, Y.-Y., Ahnert, S.E., Bagrow, J.P., Barab´asi, A.-L.: Flavor network and the
principles of food pairing. Scientiﬁc Reports 1 (2011)
2. Battiston, S., Catanzaro, M.: Statistical properties of corporate board and direc-
tor networks. The European Physical Journal B-Condensed Matter and Complex
Systems 38(2), 345–352 (2004)
3. Guillaume, J.-L., Latapy, M.: Bipartite graphs as models of complex networks.
Physica A: Statistical Mechanics and its Applications 371(2), 795–813 (2006)
4. i Cancho, R.F., Sol´e, R.V.: The small world of human language. Proceedings of
the Royal Society of London. Series B: Biological Sciences 268(1482), 2261–2265
(2001)

Comparing Overlapping Properties of Real Bipartite Networks
317
5. Latapy, M., Magnien, C., Del Vecchio, N.: Basic notions for the analysis of large
two-mode networks. Social Networks 30(1), 31–48 (2008)
6. Le Fessant, F., Handurukande, S.B., Kermarrec, A.-M., Massouli´e, L.: Clustering
in peer-to-peer ﬁle sharing workloads. In: Voelker, G.M., Shenker, S. (eds.) IPTPS
2004. LNCS, vol. 3279, pp. 217–226. Springer, Heidelberg (2005)
7. M´erindol, P., Donnet, B., Bonaventure, O., Pansiot, J.-J.: On the impact of layer-
2 on node degree distribution. In: Proc. ACM/USENIX Internet Measurement
Conference (IMC) (November 2010)
8. M´erindol, P., Van den Schriek, V., Donnet, B., Bonaventure, O., Pansiot, J.-
J.: Quantifying ASes multiconnectivity using multicast information. In: Proc.
ACM/USENIX Internet Measurement Conference (IMC) (November 2009)
9. Mislove, A., Marcon, M., Gummadi, K.P., Druschel, P., Bhattacharjee, B.: Mea-
surement and Analysis of Online Social Networks. In: Proceedings of the 5th
ACM/Usenix Internet Measurement Conference (IMC 2007), San Diego, CA (Oc-
tober 2007)
10. Newman, M.E.J.: The structure of scientiﬁc collaboration networks. Proceedings of
the National Academy of Sciences of the United States of America 98(2), 404–409
(2001)
11. Newman, M.E., Strogatz, S.H., Watts, D.J.: Random graphs with arbitrary degree
distributions. Physics Reviews E, 64 (2001)
12. Newman, M.E., Watts, D.J., Strogatz, S.H.: Random graph models of social net-
works. Proceedings of the National Academy of Sciences of the United States of
America 99(suppl. 1), 2566–2572 (2002)
13. Prieur, C., Cardon, D., Beuscart, J.-S., Pissard, N., Pons, P.: The stength of weak
cooperation: A case study on ﬂickr. arXiv preprint arXiv:0802.2317 (2008)
14. Tarissan, F., Nollez-Goldbach, R.: The network of the international criminal court
decisions as a complex system. In: Sanayei, A., Zelinka, I., Rossler, O.E. (eds.) ISCS
2013: Interdisciplinary Symposium on Complex Systems. Emergence, Complexity
and Computation, vol. 8, pp. 225–264. Springer (2013)
15. Tarissan, F., Quoitin, B., M´erindol, P., Donnet, B., Pansiot, J.-J., Latapy, M.:
Towards a bipartite graph modeling of the internet topology. Computer Net-
works 57(11), 2331–2347 (2013)
16. Tumminello, M., Miccich`e, S., Lillo, F., Piilo, J., Mantegna, R.N.: Statistically
validated networks in bipartite complex systems. PloS One 6(3), e17994 (2011)
17. Watts, D.J., Strogatz, S.H.: Collective dynamics of ’small-world’ networks. Na-
ture 393(6684), 440–442 (1998)

Risk Perception and Epidemic Spreading
in Multiplex Networks
Franco Bagnoli1 and Emanuele Massaro2
1 Dept. Physics and Astronomy and CSDC, University of Florence,
Via G. Sansone, 1 50019 Sesto Fiorentino (FI) Italy Also INFN, Sez. Firenze
franco.bagnoli@unifi.it
2 Risk and Decision Science Team, US Army Engineer Research and
Development Center, 696 Virginia Rd., COncord, MA 01742,
and Department of Civil and Environmental Engineering,
Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213
emassaro@andrew.cmu.edu
Abstract. In this paper we study the interplay between epidemic
spreading and risk perception on multiplex networks. The basic idea is
that the eﬀective infection probability is aﬀected by the perception of the
risk of being infected, which we assume to be related to the number of
infected neighbours. We re-derive previous results using a self-organized
method, that automatically gives the percolation threshold in just one
simulation. We then extend the model to multiplex networks considering
that people get infected by contacts in real life but often gather infor-
mation from an information networks, that may be quite diﬀerent from
the real ones. The similarity between the real and information networks
determine the possibility of stopping the infection for a suﬃciently high
precaution level: if the networks are too diﬀerent there is no mean of
avoiding the epidemics.
1
Introduction
Recently, the Health magazine reported “Although H1N1 inﬂuenza killed more
than 4,000 people in the United States in 2009-2010, this outbreak was relatively
mild compared to some ﬂu pandemics” [1].
Indeed, the twentieth century was characterized by a series of more serious
events. During the 1918-19 the world assisted to the so-called Spanish Flu. Start-
ing from three diﬀerent places: Brest (France), Boston (Massachusetts) and Free-
town (Sierra Leone), the disease spread worldwide, killing 25 million people in
6 months (about 17 million in India, 500,000 in the United States and 200,000
in the United Kingdom).
In 1957, another pandemic originated in China and spread rapidly in South-
east Asia, taking hence the name of Asian. The virus responsible was identiﬁed
in the subtype H2N2, new to humans, resulting from a previous human H1N1
virus that was remixed with a duck virus from which it received the genes en-
coding the H2 and N2. This pandemic took eight months to travel worldwide
and caused one to two million victims.
c
⃝Springer International Publishing Switzerland 2015
319
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_33

320
F. Bagnoli and E. Massaro
The 1968 pandemic was the mildest of the twentieth century and started once
again in China. From there it spread to Hong Kong, where more than half a
million people fell ill, and in the same year reached the United States and the
rest of the world.
Given these facts (and the whole records of pandemics in history [2]), it is
not surprising that the public health organizations are concerned about the
appearance of a new deadly pandemics.
However, in recent decades there have been many cases of false or exaggerated
information about epidemics. One example if the Swine ﬂu of 1976, or the Avian
ﬂu in 1997 where a United Nations health oﬃcial warned that the virus could
kill up to 150 million worldwide [1] or the more recent 2009 H1N1 ﬂu, during
whose outbreak the U.K. Department of Health warned about 65000 possible
deaths as reported by the Daily Mail in 2010 [3]. Fortunately these fears did not
realize.
These catastrophic scenarios and the extent of their impacts on the economic
and social contexts induce a reﬂection on the method used to forecast the evolu-
tion of a disease in real world. It is well known that in deeply connected networks
(and in particular in scale-free ones without strong compartmentalization) the
epidemic threshold of standard epidemic modeling is vanishing [4,5,6,7,8]. How-
ever, the last deadly pandemics of pest in Europe happened in 1820 [2], and
worldwide in Vietnam in the 60’s; the last pandemic inﬂuenza, Hong Kong ﬂu,
in 1968-1969. In other words, the last rapid deadly pandemics happened well
before the appearance of highly-connected human networks.
Clearly, the public health systems put a lot of eﬀorts in trying to make people
aware of the dangers connected to hygiene, dangerous sexual habits and so on.
Indeed, the current worldwide diﬀusion of large-scale diseases (HIV, seasonal
inﬂuenza, cold, papilloma virus, herpes virus, viral hepatitis among others) is
deeply related to their silent (and slow) progression or to the assumption (pos-
sibly erroneous) or their harmlessness. However, it is well known that the direct
experience (contact with actual ill people) is much more inﬂuential than public
exhortations.
Therefore, in order to accurately modeling the spreading of a disease in human
societies, we need to take into account the perception of its impacts and the
consequent precautions that people take when they become aware of an epidemic.
These precautions may consist in changes in personal habits, vaccination or
modiﬁcations of the contact network.
We are here interested in diseases for which no vaccination is possible, so
that the only way of avoiding a pandemic is by means of an appropriate level of
precautions, in order to lower the infection rate below the epidemic threshold.
We also assume that there is no acquired immunity from the disease and that
its consequences are suﬃciently mild not to induce a radical change in social
contacts.
In a previous work [9], some of us investigated the inﬂuence of the risk per-
ception in epidemic spreading. We assumed that the knowledge about the diﬀu-
sion of the disease among neighbors (without knowing who is actually infected)

Risk Perception and Epidemic Spreading in Multiplex Networks
321
eﬀectively lowers the probability of transmission (the eﬀective infectiousness).
We studied the worst case of an infection on a scale-free network with exponent
γ = 2 and we showed that in this case no degree of prevention is able to stop
the infection and one has to take additional precaution for hubs (such as public
oﬃcers and physicians).
We extend here the investigation to diﬀerent network structures, on order to
obtain a complete reference frame. For regular, random, Watts-Strogatz small-
world and non-assortative scale-free networks with exponent γ > 3 there is
always a ﬁnite level of precaution parameter for which the epidemics go extinct.
For scale-free networks with γ < 3 the precaution level depends on the cutoﬀof
the power-law, which at least depends on the ﬁnite number of the network.
We consider then an important factor of modern society: the fact that most of
information comes no more from physical contacts nor from broadcasting media,
but rather from the “virtual” social contact networks [10,11,12]. A recent study,
State of the news media for the United States [13], highlights this phenomena.
It shows the extent of the inﬂuence of social networks, for what concerns sub-
scribers who can read news published by newspapers. The 9% of the population
claims to inquire “very often” through Facebook and Twitter and seven out of
ten members are addressed to articles (from newspapers and other sources) by
friends and family members.
We are therefore confronted with news coming mainly from an information
network. On the other hand, the real network of contacts is the environment
where actual infections occur. We extend our model to the case in which the
source of information (mixed real and virtual contacts) does not coincide with
the actual source of infection (the real contacts).
This system is well represented as a multiplex network [14,15,16,17,18], i.e.,
a graph composed by several layers in which the same set of N nodes can be
connected to each other by means of links belonging to diﬀerent layers, which
represents a speciﬁc case of interdependent network [19,20]. Recently, Granell
et al. [21] have pointed out the attention to an interesting scenario where the
multiplex corresponds to a two-layers network, one where the dynamics of the
awareness about the disease (the information dynamics) evolves and another
where the epidemic process spreads.
The ﬁrst layer represents the information network where people become aware
of the epidemic thanks to news coming from virtual and real contacts in various
proportions. The second layer represents the real contact network where the
epidemic spreading takes place.
In this paper we want to model the eﬀect of the virtual information for simu-
lating the awareness of the agents in the real-world network contacts. We study
how the percolation threshold of a susceptible-infected-susceptible (SIS) dynam-
ics depends on the perception of the risk (that aﬀects the infectivity probability)
when this information comes from the same contact network of the disease or
from a diﬀerent network. In other words, we study the interplay between risk
perception and disease spreading on multiplex networks.

322
F. Bagnoli and E. Massaro
We are interested in the epidemic threshold, which is a quantity that it is
not easy to obtain automatically (for diﬀerent values of the parameters) using
numerical simulations. We extend a self-organized formulation of percolation
phenomena [22] that allows to obtain this threshold in just one simulation (for
a suﬃciently large system).
2
The Network Model
In this section we show our method for generating multiplex networks. First of
all we describe the mechanisms for generating regular, random and scale-free
networks.
Let us denote by aij = 0, 1 the adjacency matrix of our network, aij = 1 if
there is a link from j to i and zero otherwise. We shall denote by ki = 
j aij
the connectivity of site i and by j(i)
1 , j(i)
2 , . . . , j(i)
ki its neighbourhood (ai,j(i)
n = 1).
We shall consider only symmetric networks. We generate networks with N nodes
and 2mN links, so that the average connectivity is ⟨k⟩= 2m.
– Regular 1D: Nodea are arranged on a ring (periodic boundary condition).
Any given node establishes a link with the m closest nodes at its right. For
instance for m = 2, node 1 establishes a link with nodes 2 and 3, node 2
with nodes 3 and 4, and so on until nodes N −2 and N −1 establish a link
with node 1.
– Random: Any node establishes m links with randomly chosen nodes, avoid-
ing self-loops and multiple links. The probability distribution of random
networks is Possonian, P(k) = zke−z
k!
, where z = ⟨k⟩.
– Scale-Free: we use a conﬁgurational model ﬁxing also a cutoﬀK. First, at
each node i out of N is assigned a connectivity ki draft from a power-law
distribution P(k) = Ak−γ, m ≤k ≤K, with A = (γ −1)/(m1−γ −K1−γ).
Then links are connected at random aviding self-loops and multiple links,
and ﬁnally the total number of link is pruned in order to adjust the total
number of links. This mechanism allow us to generate scale-free networks
with a given exponent γ. We generally use γ = 2.1.
In order to generate multiplex networks composed by two sub-networks that
we call Real (R) and Information (I). We generate freely the real network by
choosing one from regular, random or scale-free. Then we generate a Virtual
network (V ) also chosen from the three benchmark networks, with same average
connectivity ⟨k⟩= 2m. In order to construct the information network we overlap
the real and the virtual ones and then, for each node, we prune its “real” links
with probability q and its “virtual” ones with probability 1 −q. The overlap
between the real and the information networks is thus 1 −q. The information
network is not symmetric.
This procedure allow us to study the eﬀects of the diﬀerence between the
R, where the epidemic spreading takes place, and V which is the information
network where actors become aware of the disease (i.e., over which they evaluate
the perception of the risk of being infected).

Risk Perception and Epidemic Spreading in Multiplex Networks
323
3
Infection Model and Mean-Field Approximation
Following Ref. [9], we assume that the probability that a site i is infected by a
neighbour j is given by
u(s, k) = τ exp

−J s
ki

,
where τ is the “bare” infection probability and s is the number of infected
neighbours. The idea is that the perception of the risk, modeled as the percentage
of infected neighbours and modulated by the factor J eﬀectively lowers the
infection probability (for instance because people takes more precautions). In
the case of information networks, the perception is computed on the mixed real-
virtual neighbourhood, while the actual infection process takes places on the real
network.
It is possible to derive a simple mean-ﬁeld approximation for the ﬁxed-k case.
Denoting by c the fraction of infected individuals at time t and by c′ those at
time t + 1, we have, considering a random network,
c′ =
k

s=0
k
s

cs(1 −c)k−sp(s, k)
(1)
where p(s, k) is the probability of being infected if there are s out of k infected
neighbours. The probability p depends on u as p(s, k) = 1−

1 −u(s, k)
	s, since
the infection processes are independent (although the infection probabilities are
coupled by the “perception”-dependent infection probability q, Eq. (11). Near
the threshold, the probability u is small, and therefore we can approximate
p(s, k) ≃su(s, k) = sτ exp

−J s
k

.
Replacing into Eq. 1, we get
c′ =
k

s=0
k
s

cs(1 −c)k−ssτ exp

−J s
k

,
and setting a = exp

−J s
k

,
c′ = τ
k

s=0
k
s

cs(1 −c)k−ssas,
which gives
c′ = τak(ca + 1 −c)k−1.
The critical threshold Jc corresponds to the stationary state c′ = c in the
limit c →0, i.e.,
τ = 1
k exp
Jc
k

;
Jc = k ln(kτ).
(2)

324
F. Bagnoli and E. Massaro
This prediction is quite accurate: in Fig. 2 the comparison between Eq. (2) and
actual simulations in reported for diﬀerent values of ⟨k⟩using random networks.
The analysis can be extended to non-homogeneous networks with connectivity
distribution P(k) like the scale-free ones. We can start analysing a node with
connectivity k
c′
k =
1

s1,s2,...,sk=0
∞

j1,j2,...,jk=0
k

i=1
C(ji, k)I(si, ji)T (k|si),
where we denote with i = 1, . . . , k the neighbours, si = 0, 1 is their state (healthy,
infected) and ji their connectivity. C(j, k) is the probability that a node with
connectivity j is attached to node with connectivity k, I(si, ji) is the probability
that the neighbour i is infected, and T (k|si) is the probability that it transmits
the infection to the node under investigation.
Clearly, 
j C(j, k) = 1. We use symmetric networks, so jC(j, k)P(j) =
kC(k, j)Pk (detailed balance). For non-assortative networks, C(j, k) does not
depend on k, and summing over the detailed balance condition, C(j, k) =
jP(j)/⟨k⟩. The quantity I(si, ji) is simply csi
ji (1 −cji)1−si and T (k|si) =
1 −(1 −τ exp(−Js/k))s, where s = 
i si (risk perception). Near the extinction,
τ exp(−Js/k) is small and we can approximate T (k|si) = sτ exp(−Js/k),
Summing up, we have
c′
k =
1

s1,s2,...,sk=0
∞

j1,j2,...,jk=0
k

i=1
jiP(ji)
⟨k⟩
csi
ji(1 −cji)1−sisτ exp

−J s
k

,
=
1

s1,s2,...,sk=0
sτ exp

−J s
k

k

i=1
∞

ji=0
jiP(ji)
⟨k⟩
csi
ji (1 −cji)1−si.
Since si = 0, 1, csi
ji (1 −cji)1−si is either si or 1 −si. Let us deﬁne ˜c =

j jcj/⟨k⟩, and we get
c′
k =
1

s1,s2,...,sk=0
sτ exp

−J s
k

k

i=1
˜csi(1 −˜c)1−si =
k

s=0
k
s

sτ exp

−J s
k

˜cs(1 −˜c)k−s
= ˜ckτ exp
 −Js
k
 
˜c exp

−J
k

+ 1 −˜c

.
Near the epidemic threshold ˜c →0, and
˜c′ =
1
⟨k⟩

k
kc′
kP(k) = τ˜c
⟨k⟩

k
k2P(k) exp

−J
k

.
The correspondence between τc and Jc is therefore
τc(Jc) =
⟨k⟩

k k2P(k) exp

−Jc
k
,
(3)

Risk Perception and Epidemic Spreading in Multiplex Networks
325
which, for Jc = 0, gives τc = ⟨k⟩/⟨k2⟩, that for a sharply peaked P(k) corre-
sponds to Eq. (2).
By using a continuous approximation, it is possible to make explicit the rela-
tionship between τ and Jc in the scale-free case. Eq. (3) becomes
τc(Jc) =
⟨k⟩
 K
m k2P(k) exp

−Jc
k

dk
,
(4)
Substituting, for the scale free case, P(k) = Ak−γ, where A is the normaliza-
tion constant so that
 K
m P(k) = 1
A =
γ −1
m1−γ −K1−γ ≃(γ −1)mγ−1,
if K ≫m (and γ < 3). We get
⟨k⟩= γ −1
γ −2 · m2−γ −K2−γ
m1−γ −K1−γ ≃γ −1
γ −2m
for K ≫m, and
τc(Jc) = J3−γ
c

Γ

γ −3, Jc
K

−Γ

γ −3, Jc
m

,
(5)
where Γ(a, x) is the incomplete gamma function. Eq. (5) diverges for K →∞
and thus for inﬁnite networks Jc = 0 ∀τ. However, real networks always have a
cut-oﬀ(at least due to the ﬁnite number of nodes) [23]. For Jc = 0 we recover
the standard threshold
τc(0) = γ −3
γ −2 · m2−γ −K2−γ
m3−γ −K3−γ ≃3 −γ
γ −2 · m2−γ
K3−γ
(6)
The problem of epidemic threshold in ﬁnite-size scale-free networks was stud-
ied in Ref. [24]. The conclusions there is that even in ﬁnite-size networks the
epidemics is hard to stop. Indeed, we ﬁnd numerically that the epidemics always
stops in ﬁnite scale-free networks although the required critical value of Jc may
be quite large.
In Fig. 3 the comparison between the mean-ﬁeld prediction, Eq. (3), and actual
simulations is shown, for three instances of a scale-free network generated with
the same parameters. The theoretical prediction coincides with the simulations
only for Jc = 0. Moreover, although the simulation results seems to be suﬃciently
independent on the details of the generated networks, the theoretical prediction
is quite sensible to them. The continuous approximation, Eq. (6) gives, for m = 2,
K = 300 and γ = 2.4, a value τ(0) ≃0.037, quite diﬀerent from the computed
one τ(0) ≃0.08.
4
The Self-organized Percolation Method
Here we show a self-organized percolation method that allow to obtain the crit-
ical value of the percolation parameter in a single run, for a given network. We

326
F. Bagnoli and E. Massaro
consider a parallel SIS process, which is equivalent to a directed percolation
problem where the directed direction is time.
Let us denote by xi(t) = 0, 1 (0 =healthy, 1 =infected), the percolating vari-
able and by p the control parameter (percolation probability).
Let us start considering the problem of directed percolation (simple infection).
Considering p is ﬁxed, the stochastic evolution process for the network is deﬁned
as
xi(t + 1) =

j=j(i)
1
,...,j(i)
ki
[p > rij(t)]xj(t)
(7)
where  represents the OR operator and the multiplication represents the AND.
The square bracket represents the the truth function, [·] = 1 if “·” is true, and
zero otherwise. The quantity rij(t) is a random number between 0 and 1 that
varies with i, j and t. We want to derive an equation for pi(t), which is the
minimum value of p for which xi(t) is infected. We can replace xi(t) by [p > pi(t)].
Eq. 7 becomes:
[p > pi(t + 1)] =

j=j(i)
1
,...,j(i)
ki
[p > rij(t)][p > pj(t)].
(8)
Now [p > a][p > b] is equal to [p > max(a, b)] and [p > a] ∨[p > b] is equal to
[p > min(a, b)], therefore Eq. 8 becomes:
[p > pi(t + 1)] =

p >

MIN
j=j(i)
1
,...,j(i)
ki
max

rij(t), pj(t)


,
(9)
and therefore we get the equations for the pi’s
pi(t + 1) =
MIN
j=j(i)
1
,...,j(i)
ki
max

rij(t), pj(t)

.
(10)
Let assume that at time t = 0 all sites are infected, so that xi(0) = 1 ∀p. We can
alternatively write pi(0) = 0 (since the minimum value of p for which xi(0) = 1 is
one for sure. We can therefore iterate Eq. 10 and get the asymptotic distribution
of pi. The minimum of this distribution gives the critical value pc for which there
is at least one percolating cluster with at least one “infected” site at large times.
As usual, t cannot be inﬁnitely large for ﬁnite N otherwise there will be surely
a ﬂuctuation that will bring the system into the absorbing (healthy xi = 0)
conﬁguration.
We shall now consider the problem of ﬁrect percolation with risk perception.
Now, let us apply the method to a more diﬃcult problem, for which the perco-
lation probability depends on the fraction of infected sites in the neighborhood
(risk perception). As above, we deﬁne the infection probability u as
u(s, k) = τ exp

−J · s
k

(11)

Risk Perception and Epidemic Spreading in Multiplex Networks
327
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
o
c
 
 
SF, a=2.2, <k>=13.95, <k2>=538.5
Poisson <k>=12, <k2>=150
Regular k=2
Theory
Theory
Theory
Fig. 1. Asymptotic number of infected in-
dividuals c versus the bare infectiousness
probability τ for the SIS dynamics for dif-
ferent networks, N = 10000.













o



	

	

	

	

Fig. 2. Comparison between mean ﬁeld
approximation and simulations for ran-
dom networks with diﬀerent values of ⟨k⟩.
where τ is the bare infection probability, s is the number of infected neighbors
and k is the node connectivity. In this case we want to ﬁnd the minimum value
of the parameter J for which there is no spreading of the infection at large times.
The quantity [u > r] = [τ exp(−Js/k) > r] is equivalent to [J < −(k/s) ln(r/τ)].
Therefore Eq. (8) is replaced by
[J < Ji(t + 1)] =

j=j(i)
1
,...,j(i)
ki

J < −ki
si
ln
rij(t)
τ

[J < Jj(t)]
(12)
where
si ≡si(J) =

j=j(i)
1
,...,j(i)
ki
xj =

j=j(i)
1
,...,j(i)
ki
[Jj(t) ≥J].
(13)
So
[J < Ji(t + 1)] =

j=j(i)
1
,...,j(i)
ki

J < −
ki
si(Jj(t)) ln
rij(t)
τ

[J < Jj(t)]
(14)
and therefore
Ji(t + 1) =
MAX
j=j(i)
1
,...,j(i)
ki
min

−
ki
si(Jj(t)) ln
rij(t)
τ

, Jj(t)

.
(15)
Analogously to the previous case, the critical value of Jc is obtained by taking
the maximum value of the Ji(t) for some large (but ﬁnite) value of t.
We can now turn to the problem of computing the critical value Jc if the
perception is computed on the information network that is partially diﬀerent
from that of the real infection. Here the perception of the importance of the

328
F. Bagnoli and E. Massaro
0
10
20
30
40
50
60
70
80
90
0
0.2
0.4
0.6
0.8
1
Jc
τ
Fig. 3. Comparison between the results of
the mean ﬁeld approximation and simula-
tions for three instances of scale-free net-
works with γ = 2.4, N = 10000, m = 2
and K = 300. Although the simulations
give similar results, the mean-ﬁeld com-
putations (that coincide with the simula-
tion only for Jc = 0) are very dependent
on the details of the network.
q
o
 
 
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
20
40
60
80
100
120
140
160
180
200
Fig. 4. Critical precaution threshold Jc
(color code) as a function of the bare in-
fectiousness τ and of the diﬀerence be-
tween the real and the information net-
work q. Here the real and virtual networks
are Poissonian (random) with ⟨k⟩= 6 and
N = 1000. In the darker region there is
always a value of Jc able to stop the epi-
demics, while in the lighter region the epi-
demics cannot be stopped. The separation
boundary is the stoppability frontier.
infection, si, is computed on the neighbours j
(i) on the information network.
The perceived number of infected neighbours depends on how many of them, in
the information network, have a valueJj larger than that computed in the real
network, i.e.,
Ji(t + 1) =
MAX
j=j(i)
1
,...,j(i)
ki
min

−
ki
si(Jj) ln
rij(t)
τ

, Jj(t)

,
(16)
where
si(J) =

j=j(i)
1 ,...,j(i)
ki
[Jj ≥J].
(17)
In other words: for any value of J in the real neighbourhood one computes how
many neighbours j in the information network have Jj ≥J. This is the perceived
value of the risk.
5
Results
In this section we show the results of the self-organized percolation method in
both single-layered and multiplex networks (with and without risk perception).

Risk Perception and Epidemic Spreading in Multiplex Networks
329
For our experiments we ﬁxed the size of the networks N = 1000 and the perco-
lation time T = 10000.
We investigated the SIS dynamics over regular, Poisson and scale-free net-
works as shown in Fig. 1: in particular we evaluated the critical epidemic thresh-
old values pc ≡τc for which there is at least one percolating clusters with at
least one infected nodes.
Considering a regular lattice with connectivity degree k = 2, we found τc ≃
0.6447 which is compatible with the results of the bond percolation transition
in the Domany-Kinzel model [25].
In the case of random networks with Poisson degree distributions the critical
epidemic threshold τc = ⟨k⟩/⟨k2⟩≃⟨k⟩−1 if the distribution is sharp [5]. Indeed,
for Poisson network with ⟨k⟩= 12 the self-organized percolation method gives
τc ≃0.08 ≃1/12.
For a scale-free network with ⟨k⟩= 13.95 and ⟨k2⟩= 538.5 we get from
simulations τc ≃0.026, in agreement with the expected value.
We then investigated the eﬀects of risk perception in a simple model of epi-
demic spreading. Here we show the results of the direct percolation method
comparing the simulations with the theoretical formulation derived in Section 3.
Results are quite interesting if compared with the simple SIS dynamics (Fig. 1)
here we are able to stop the epidemic increasing the bare infection τ until τ = 1.
Considering for instance the case of random networks with ⟨k⟩= 6 in which we
found a critical value of τc = 0.165 (Fig. 1) while in Fig we observe that after
that value of τc we need to adopt a precaution level J > 0 in order to stop the
spreading of the disease. The same consideration can be done also for the other
scenarios.
0
20
40
60
80
100
0
10
20
30
40
50
60
70
80
90
100
q
Jc
 
 
o = 0.2
o = 0.3
o = 0.4
o = 0.5
o = 0.6
Fig. 5. Critical precaution threshold Jc
versus the diﬀerence between the real and
the information network q for some values
of the bare infectiousness τ (from right to
left: τ = 0.2, 0.3, . . . , 0.6). Random real
and virtual networks, both with ⟨k⟩= 6
and N = 10000.
0
20
40
60
80
100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Jc
q
τ = 0.3
τ = 0.2
τ = 0.1
Fig. 6. Critical precaution threshold Jc
versus the diﬀerence between the real and
the information network q for some values
of the bare infectiousness τ(from right to
left τ = 0.1, 0.2, 0.3). Random real net-
work and scale-free virtual network, both
with ⟨k⟩= 6, N = 10000.

330
F. Bagnoli and E. Massaro
5.1
Multiplex Risk Perception
Finally, we illustrate our results for risk perception in multiplex networks. The
phase diagram for the risk-perception in modelling SIS dynamics in multiplex
networks is reported in Fig. 4. The general shape of this phase diagram can be
understood considering the continuous approximation, Eq. (4). A given node of
connectivity k is connected, in the information network, to a (1−q)k real neigh-
bours, and to qk virtual ones. At the threshold, the global fraction of infected
sites is small. For the spreading of the epidemics, the important sites are those
that have a infected real neighbour. It might be assumed that the virtual neigh-
bours, being uncorrelated with the real ones, do not contribute at all to the risk
perception. Among the real neighbours, the fraction kq replaced by virtual ones
has become invisible, and so the perception decreases by a factor q. Replacing
Jc with qJc in Eq. (4) for the scale-free case one gets τq3−γ = const, and indeed
the shape of the stoppability frontier of Fig. 4 resembles that of a generalized
hyperbola.
The general trend is that, increasing the diﬀerence q between the information
network and the real one it becomes harder to stop an epidemics. It is interesting
to investigate this transition. As we can see in Fig. 6, this transition is quite
sharp, especially for low values of τ. A similar scenario holds for a mixture of
random real and random ghost networks, as shown in Fig. 5.
6
Conclusions
We investigated the interplay between epidemic spreading and risk perception on
multiplex networks, exploiting mean-ﬁeld approximations and a self-organized
method, that automatically gives the percolation threshold in just one simula-
tion. We focused on multiplex networks, considering that people get infected by
contacts in real life but often gather information from an information networks,
that may be quite diﬀerent from the real ones. The main conclusion is that the
similarity between the real and information networks determine the possibility of
stopping the infection for a suﬃciently high precaution level: if the networks are
too diﬀerent there is no mean of avoiding the epidemics. Moreover, especially for
low values of the bare infectiousness probability, this transition occurs sharply
without evident forerunners. This last observation remarks that, although the
virtual world has indeed the advantage of allowing a fast diﬀusion of information,
real epidemics still propagate in the real world. This is of particular importance
for “neglected” diseases, possibly diﬀused in marginalized parts of the popula-
tion that have little access to Internet (and that in any case are not part of the
“neighbourhood” of “real neighbors” belonging to other social classes).
Acknowledgements. FB acknowledges partial ﬁnancial support from the EU
projects 288021 (EINS – Network of Excellence in Internet Science) and 611299
(FP7 Programme on Collective-Awareness Platforms SciCafe2.0). The authors
are grateful to Dr. Nicola Perra for helpful discussions.

Risk Perception and Epidemic Spreading in Multiplex Networks
331
References
1. Pandemic Scares Throughout History. Health Magazine (2013)
2. Wikipedia (2013), http://en.wikipedia.org/wiki/Pandemic
3. The “false” pandemic: Drug ﬁrms cashed in on scare over swine ﬂu, claims Euro
health chief. Daily Mail (2010)
4. Moore, C., Newman, M.E.J.: Epidemics and percolation in small-world networks.
Phys. Rev. E 61, 5678–5682 (2000)
5. Pastor-Satorras, R., Vespignani, A.: Epidemic spreading in scale-free networks.
Phys. Rev. Lett. 86, 3200–3203 (2001)
6. Newman, M.E.J.: Exact solutions of epidemic models on networks. Working Papers
01-12-073, Santa Fe Institute (December 2001)
7. May, R.M., Lloyd, A.L.: Infection dynamics on scale-free networks. Phys. Rev.
E 64, 066112 (2001)
8. Pastor-Satorras, R., Vespignani, A.: Immunization of complex networks. Phys. Rev.
E 65, 036104 (2002)
9. Bagnoli, F., Li`o, P., Sguanci, L.: Risk perception in epidemic modeling. Phys. Rev.
E 76, 061904 (2007)
10. Ginsberg, J., Mohebbi, M., Patel, R., Brammer, L., Smolinski, M., Brilliant, L.: De-
tecting inﬂuenza epidemics using search engine query data. Nature 457, 1012–1014
(2009)
11. Scanfeld, D., Scanfeld, V., Larson, E.L.: Dissemination of health information
through social networks: Twitter and antibiotics. American Journal of Infection
Control 38(3), 182–188 (2010)
12. Chew, C., Eysenbach, G.: Pandemics in the age of twitter: Content analysis of
tweets during the 2009 h1n1 outbreak. PLoS One 5(11), 014118 (2010)
13. The State of the News Media. The Pew Research Center’s project for Excellence
in Journalism (2010)
14. Kurant, M., Thiran, P.: Layered complex networks. Phys. Rev. Lett. 96, 138701
(2006)
15. Mucha, P.J., Richardson, T., Macon, K., Porter, M.A., Onnela, J.-P.: Com-
munity structure in time-dependent, multiscale, and multiplex networks. Sci-
ence 328(5980), 876–878 (2010)
16. Szell, M., Lambiotte, R., Thurner, S.: Multirelational Organization of Large-scale
Social Networks in an Online World (2010)
17. Arenas, A., Lozano, S., Rodriguez, X.-P.: Evolution of cooperation in multiplex
networks. Scientiﬁc Reports 2 (2012)
18. Bianconi, G.: Statistical mechanics of multiplex networks: Entropy and overlap.
Phys. Rev. E 87, 062806 (2013)
19. Buldyrev, S.V., Parshani, R., Paul, G., Stanley, H.E., Havlin, S.: Catastrophic
cascade of failures in interdependent networks. Nature 464(7291), 1025–1028 (2010)
20. Gao, J., Buldyrev, S.V., Stanley, H.E., Havlin, S.: Networks formed from interde-
pendent networks. Nat. Phys. 8(1), 40–48 (2012)
21. Granell, C., G´omez, S., Arenas, A.: Dynamical interplay between awareness and
epidemic spreading in multiplex networks. Phys. Rev. Lett. 111, 128701 (2013)

332
F. Bagnoli and E. Massaro
22. Bagnoli, F., Palmerini, P., Rechtman, R.: Algorithmic mapping from criticality to
self-organized criticality. Phys. Rev. E 55, 3970–3976 (1997)
23. Dorogovtsev, S.N., Mendes, J.F.F.: Evolution of networks. Advances in Physics 51,
1079–1187 (2002)
24. Pastor-Satorras, R., Vespignani, A.: Epidemic dynamics in ﬁnite size scale-free
networks. Physical Review E 65(035108) (2002)
25. Domany, E., Kinzel, W.: Equivalence of cellular automata to ising models and
directed percolation. Phys. Rev. Lett. 53, 311–314 (1984)

Applications of Multifractal Diﬀusion Entropy
Analysis to Daily and Intraday Financial Time
Series
Petr Jizba1,2 and Jan Korbel1,3
1 Faculty of Nuclear Sciences and Physical Engineering,
Czech Technical University in Prague, Bˇrehov´a 7, 115 19, Prague, Czech Republic
2 Institute for Theoretical Physics, Freie Universit¨at Berlin,
Arnimallee 14, 14195, Berlin, Germany
3 Max-Planck-Institute for the History of Science,
Boltzmannstr. 22, 14195 Berlin, Germany
{petr.jizba,korbeja2}@fjfi.cvut.cz
Abstract. Scaling properties and fractal structure are one of the most
important aspects of real systems that point to their complexity. These
properties are closely related to the theory of multifractal systems and
theory of entropy. Estimation of scaling (or multifractal) exponents be-
longs to the essential techniques that can reveal complexity and inner
structure of the system. To successful techniques belongs Multifractal
diﬀusion entropy analysis, based on estimation of R´enyi entropy of the
system. In the recent article [1], we have discussed one possible method
of estimation R´enyi entropy from proper estimation of underlying prob-
ability histograms. In Ref. [2], we have applied the method to daily and
intraday ﬁnancial data in order to test the stability of the system. This
article summarizes existing progress in this ﬁeld and shows the robust-
ness of the method on high-frequency ﬁnancial data.
Keywords: Financial time series, R´enyi entropy, Multifractal analysis.
1
Introduction
Complex systems are currently in a viewﬁnder of many scientists across the
ﬁelds. This is because complexity and emergent behavior is observed on a large
number of various dynamical systems as for example dynamical [3], physical [4],
climatic [5], biological [6], social [7] and ﬁnancial [8]. Classical models exhibit
lack of power to capture the complexity of such systems and there is neces-
sary to come with some new concepts, as e.g. Multifractal analysis, introduced
e.g., in [9–11]. Particular methods, as e.g., Detrended Fluctuation Analysis [12],
Wavelet Analysis [13], Generalized Hurst Exponent [14], have celebrated a great
success in order to reveal scaling exponents. In the recent paper [1], we inves-
tigated yet another approach, i.e. Diﬀusion Entropy Analysis [15, 16], which is
based on estimation of R´enyi entropies, that are closely related to scaling ex-
ponents [17]. We have shown that it is necessary to estimate the bin-with of
c
⃝Springer International Publishing Switzerland 2015
333
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_34

334
P. Jizba and J. Korbel
probability histograms properly. Moreover, in [2] we have applied the MFDEA
approach to several types of ﬁnancial time series in order to test the robustness
of the method and its limitations. We have also used high-frequency data to
compare the spectra with analysis on daily data.
This paper is organized as follows: after an introduction to multifractal analy-
sis in Sect. 2, the main method of the paper, i.e., Multifractal Detrended Fluctu-
ation Analysis, is brieﬂy introduced in Sect. 3. Sect. 4 is devoted to the overview
of theoretical aspects of the method, while Sect. 5 presents the application of
the method on some particular examples of ﬁnancial time series.
2
Multifractal Analysis
In this section we brieﬂy introduce some basics of multifractal analysis [9,11,18,
19]. Let us have a discrete time series {xi}N
i=1, with a characteristic time lag s. All
obtained values are divided into separate regions Kj and empirical probabilities
pj in each region are calculated as pj = Nj/N, where Nj is a number of elements
that falls into region Kj. Let us assume that these probabilities scale with s as
pj(s) ∝sαj, where αj is a typical exponent for the region Kj. We assume that
probability of regions scaling with an exponent α depends smoothly on α so
ρ(s, α)dα = c(α)s−f(α)dα .
(1)
Here f(α) is called multifractal spectrum, which is nothing else than a scaling
exponent of ρ(s, α). Usually, we also deﬁne an alternative multifractal measure
called partition function Z(q, s) = 
j pq
j(s). In this case, partition function
scales as
Z(q, s) =

j
pq
j(s) ∝sτ(q) .
(2)
It can be easily shown, that τ(q) = maxα(qα −f(α)) [11]. This is nothing else
than Legendre transform from (α, f(α)) to (q, τ(q)).
Exponent τ(q) is closely related to R´enyi entropy Hq(s) =
1
1−q ln Z(q, s).
For limq→1 Hq, be become the well-known Shanon entropy. Scaling exponents of
R´enyi entropy Dq, obtained from relation Hq(s) ∼sDq, are connected to scaling
exponents τ(q) by a relation Dq =
τ(q)
q−1 .
Scaling exponents Dq are also known as generalized dimensions [11, 17]. For
particular values generalized dimensions coincide with other familiar fractal di-
mensions as informational dimension or correlation dimension. The important
implication from the above relations is that it is possible to estimate any of the
scaling exponents (f(α), τ(q) and Dq) and then to transform them to the others.
3
Multifractal Detrended Fluctuation Analysis
The Multifractal Detrended Fluctuation Analysis is based on estimation of (con-
tinuous) R´enyi entropy,
Hq(t) =
1
1 −q ln

pq(x, t)dx ,
(3)

Applications of Multifractal Diﬀusion Entropy Analysis
335
from which we get the scaling exponents via the relation
Hq(t) = δ(q) ln t + B(q)
(4)
In case of complex processes (e.g., Multifractal Random Walk [20], Multifractal
Model of asset Returns [21] and also models discussed in Refs. [1,17]), the δ(q)
is actually q-dependent function and is closely related to Dq.
The estimation of the exponents consists of two steps: ﬁrstly, we have to to
estimate the probability distribution p(x, t) for diﬀerent times; secondly, from
estimated distributions we can calculate R´enyi entropies and subsequently δ(q)-
spectrum from the linear regression. In case of stationary time series, we use the
so-called Fluctuation collection algorithm (FCA). We sum up all noise-like values
of the series into partial sums. These sums of ﬂuctuations ξk(s) = n−s
t=1 xt+k(s)
can be therefore exploited to probability estimation; when we divide the space
into boxes Ki of length h (also called bin-width), the histogram ˆpj(s) that ap-
proximates theoretical probability distribution is deﬁned as
ˆpj(s) ≡#{k | ξk(s) ∈Kj}
N −s + 1
,
(5)
where symbol # denotes the number of elements. The whole algorithm is de-
picted in Fig. 1. Of course, the important role in histogram estimation plays the
choice of bin-width h. In extreme cases, i.e., when number of boxes Ki is too
large or too small, we get a degenerate histogram that for too few boxes looses
too much information about the distribution shape (shown in Fig. 2), in case of
too many boxes, the histogram disintegrates into counting function of particular
values. This aﬀects also the precision of spectrum estimation (Fig. 3). The next
section discusses the problem optimal histogram bin-width in case of MFDEA
method.
4
Optimal Bin-Width of Histograms in MFDEA
Approach
In theory of statistics we have to usually create an approximation ˆp(x) of prob-
ability distribution p(x) from input data {xi}N
i=1. The histogram is in one-
dimensional case deﬁned as
ˆp(x) =
1
Nh
nB

j=1
νjχj(x) ,
(6)
where nB is number of boxes Kj, νj is the absolute occurrence frequency of Kj
and χj(x) is a characteristic function of interval Kj = [x0 + (j −1)h, x0 + jh].
The optimal bin-width h∗, for which the histogram is most proper approxi-
mation of p(x), can be obtained by several diﬀerent approaches, from clas-
sical Sturges rule [22], that simply estimates the optimal number of bins as
nB = 1+log2 N, to the approaches based on asymptotic mean squared integrated

336
P. Jizba and J. Korbel
0
5000
10000
15000
−0.20
0.00
1950−2013
0
50
100
150
200
250
−0.05
0.05
2008
0
10
20
30
40
−0.08
0.00
Fluctuation collection for s = 8 over Jan and Feb 2008
0
2
4
6
8
Histogram
0
50
100
150
200
250
−0.5
−0.2
0.1
Fluctuation collection for s = 64 over 2008
0
20
40
60
Histogram
Fig. 1. Illustration of ﬂuctuation collection algorithm. From above: a) Time series of
ﬁnancial index S&P500 from January 1950 to March 2013, containing approx. 16000
entries. b) A subset of S&P500 for the year 2008 alone. c) Fluctuation collection algo-
rithm for the ﬁrst two months of 2008 and s = 8 days. Series are partially integrated,
i.e., ﬂuctuation sums are collected into the histogram on the right-hand side. d) Fluc-
tuation collection algorithm for the whole year 2008 for s = 64. Bin-widths of both
histograms were estimated separately for both histograms.
Fluctuation collection algorithm for time series S&P 500
a)
b)
c)
d)
error, as i.e., Scott rule [23], which estimates the bin-width as ˆhSc = 3.5ˆσN −1/3,
respectively Freedman-Diaconis rule [24], which estimates the bin-width as ˆhF D =
2(
IQR)N −1/3, where ˆσ is empirical variance and 
IQR is interquantile range.
In case of MFDEA approach, we need not only approximations of p(x), but
also its powers pq(x) (in most cases, the estimation restricts only for q > 0, be-
cause of instability for negative coeﬃcients [17]). We show that it is also necessary
to choose diﬀerent bin-widths for every q. The natural measure of discrepancy
between histogram and theoretical distribution is the expected R´enyi divergence
EH[Dq(p||ˆp)] =
1
q −1EH

ln

R
ˆp1−q(x)pq(x)dx

,
(7)

Applications of Multifractal Diﬀusion Entropy Analysis
337
Fig. 2. Un-normalized (or frequency-based) histograms of the ﬂuctuation sums σs,
with s = 8, 64 and 512 and with bin-widths h = 100, 10, 1, 0.1 and 0.01, measured in
units u = 3×10−4 for better visualization. We can see that for not optimal bin-widths,
the shape of histogram is not appropriately approximating the theoretical probability
distribution, i.e., we observe under-ﬁtted or over-ﬁtted histograms.
h = 100
h = 10
h = 1
h = 0.1
h = 0.01
where EH denotes the average over all possible histograms. Unfortunately, deal-
ing with the divergence is technically diﬃcult, so we rather use similar approach
based on mean-squared integrated error between q-th powers of p(x) and ˆp(x) [1]
EH∥pq −ˆpq∥2
L2 = EH

R
(pq(x) −ˆpq(x))2dx .
(8)
The main advantage lies in the fact that when minimizing the error, we can
interchange the integral and expectation value, so we get
min
h>0 EH∥pq −ˆpq∥2
L2 = min
h>0
nB

j=1

Kj
Eνj
⎡
⎣
	
pq(x) −
νq
j
N qhq

2⎤
⎦dx .
(9)

338
P. Jizba and J. Korbel
2
3
4
5
6
1
2
3
4
5
width of bin = 100
2
3
4
5
6
3
4
5
6
7
width of bin = 10
2
3
4
5
6
5.0
6.0
7.0
8.0
width of bin = 1
2
3
4
5
6
7.0
7.5
8.0
8.5
9.0
width of bin = 0.1
2
3
4
5
6
8.8
9.0
9.2
9.4
9.6
width of bin = 0.01
−1
0
1
2
3
4
Symbols for diffrent values of q
Fig. 3. Linear ﬁts of estimated R´enyi entropy vs. logarithm of s with h = 100, 10, 1,
0.1, 0.01, measured in the same units as in Fig. 2, i.e., in units u = 3 × 10−4. Note, in
particular, that the error is also non-trivially reﬂected in the deﬁning linear regression
for the scaling exponents (also dependent on q). This means that choice of a single h∗
valid for all q’s leads to incorrect results and one has to resort to h∗
q that is q-dependent.
ˆHq(s)
ˆHq(s)
ˆHq(s)
ln(s)
ln(s)
ln(s)
ln(s)
ln(s)
Here, the expected value is calculated only locally, i.e. as mean of multinomial
variable νj ∈{1, . . . , n}. Calculating of the optimal value in the leading order
O(N −2 + hN −1) [25], gives us the optimal value h∗
q equal to [1]
h∗
q =
3

6q2
N

R p2q−1(x)dx

R(dpq(x)/dx)2dx .
(10)
Assuming that the normality of p(x), we get
h∗
q = σN −1/3 3
24√π
q1/2
6√2q −1 = h∗
1ρq ,
(11)
where the h∗
1 corresponds to the classic result obtained by Scott [23]. The es-
timation rule is then deﬁned as hSc
q
= 3.5ˆσN −1/3ρq, and similarly is estimated
the Freedman-Diaconis rule as hF D
q
= 2.6(
IQR)N −1/3ρq. For estimation of δ(q)-
spectrum, we need to have exponents on several time scales. In this case, the most

Applications of Multifractal Diﬀusion Entropy Analysis
339
appropriate way is to estimate all histograms at the same bin-width, otherwise
one would need to keep in mind the necessity of recalculation of histograms.
The optimal bin-width for several simultaneously estimated histograms can
be obtained by minimization of the Total mean squared error, which is nothing
else than sum of mean squared errors for particular histograms listed in Eq. (8).
Having several time scales {s1, . . . , sm}, we get that the optimal bin-width that
minimizes total mean squared error under an assumption of normal underlying
distribution is equal to [1]
h∗
q = (24√π)1/3ρq
3




m
i=1 σ2(1−q)
si
/Nsi
m
i=1 σ−(1+2q)
si
,
(12)
where σsi and Nsi are standard deviation of probability distribution in time
si and Nsi = N −si + 1 is the number of ﬂuctuations collected at time si.
After inserting the empirical variance and the multiple of interquartile range,
one obtains analog bin-width rules to the case, when i = 1, i.e. when we have to
optimize only one histogram.
5
Numerical Estimation of Scaling Exponents of
Financial Time Series
The numerical analysis is divided into two parts. Firstly, in [1], we have estimated
the optimal win-width hq and the δ(q)-spectrum for the time series of daily re-
turns of ﬁnancial index S&P500. In Fig. 4, optimal bin-width and δ(q)-spectrum
are depicted for both Scott and Freedman-Diaconis method. We observe that
for values q < 1/2, the bin-width diverges and the spectrum collapses. That
is caused by leading-order expansion and normality assumption. This problem
could be patched by estimation of bounded model, which would not allow only
values larger than some given threshold. Nevertheless, theoretical estimation of
such threshold is almost improbable and the improper estimation of the maximal
value could aﬀect the whole spectrum estimation. This is one of the limitations
of the method.
In the second part of the analysis [2], we have used the MFDEA to investigate
scaling exponents of ﬁnancial time series. We have investigated several kinds
of data, from share prices, indices, currency ratios to volatility. We have used
two types of time series, i.e., daily data and minute data. We try to estimate
the spectrum with help of MFDEA method to see the diﬀerences between the
nature of time series and to test the robustness of the method. We can observe a
distinct diﬀerence between the spectrum of volatility compared to other spectra,
which is given by its nature and memory eﬀects observed usually in the volatility
series [8]. We can observe that especially in case of intraday data, the spectrum
is not smooth, or there are present some outliers in the spectrum. That can
be caused by the fact that for short periods, ﬁnancial data exhibit heavy-tailed
persistent behavior, which should be modeled by power-law models rather than
Gaussian distributions.

340
P. Jizba and J. Korbel
0
2
4
6
8
10
0.30
0.35
0.40
0.45
0.50
0
2
4
6
8
10
0.003
0.004
0.005
0.006
0.007
0.008
0.009
0.010
10
20
30
Scott
FD
method
optimal bin-width for q = 1
Scott
0.00470
Freedman–Diaconis 0.00384
q
δ(q)
q
h∗
q
a)
b)
Fig. 4. From left: a) δ(q) spectrum for bin-widths estimated by diﬀerent methods.
Spectra for both methods almost everywhere coincide. b) Optimal bin-widths ˆh∗
q for
both methods. Under ﬁgures: Table with optimal values of h∗
1 for diﬀerent methods.
0
2
4
6
8
10
0.2
0.4
0.6
0.8
q
δ(q)
Spectra for minute data
ASE
IBM
N225
VIX
XAU
0
2
4
6
8
10
0.2
0.3
0.4
0.5
0.6
q
δ(q)
Spectra for daily data
ASE
IBM
N225
VIX
XAU
Fig. 5. δ(q)-spectrum of various ﬁnancial time series. Spectra are estimated for ﬁve
diﬀerent series (Athens stock index, IBM stock price, Nikkei 225 stock index, Volatility
index of S&P 500 and gold price) and two diﬀerent time scales (minute data and daily
data). Minute time series are from year 2013 and have approximately 105 entries, daily
data are from the last ten to twenty years (depending on the particular series) and
have 5000 to 10000 records.

Applications of Multifractal Diﬀusion Entropy Analysis
341
6
Conclusions and Perspectives
In this paper we have used the Multifractal Diﬀusion Entropy Analysis to es-
timate δ-spectrum of several ﬁnancial time series. In a theoretical part of the
paper we have discussed some aspects of multifractal analysis and stressed the
recent progress in the MFDEA method, including optimal bin-width estima-
tion [1]. The optimal bin-width was obtained by minimization of mean squared
integrated error. In the latter part of the paper, the δ-spectrum was estimated
for ﬁnancial time series on two diﬀerent time scales [2]. The estimation showed
us the necessity of a proper bin-width estimation. The limitations in case of
heavy-tailed models were also discussed and authors believe that future research
will bring some more results that would include also power-law distributions.
Acknowledgments. We want to thank to Xaver Sailer of Nomura, Ltd., who
provided us with ﬁnancial data. This work was supported by the Grant Agency
of the Czech Republic, grant No. GCP402/12/J077, and the Grant Agency of
the CTU in Prague, grant No. SGS13/217/OHK4/3T/14.
References
1. Jizba, P., Korbel, J.: Multifractal diﬀusion entropy analysis: Optimal bin-width of
probability histograms. Physica A 413, 438–458 (2014)
2. Jizba, P., Korbel, J.: Multifractal diﬀusion entropy analysis: Applications to ﬁnan-
cial time series. In: Proceedings ITISE 2014, International Work-Conference on
Time Series (2014)
3. Bar-Yam, Y.: Dynamics of complex systems, vol. 213. Addison-Wesley, Reading
(1997)
4. Kleinert, H.: Path Integrals in Quantum Mechanics, Statistics, Polymer Physics
and Financial Markets, 4th edn. World Scientiﬁc (2009)
5. Lovejoy, S., Schertzer, D.: The weather and Climate: emergent laws and multifrac-
tal cascades. Cambridge University Press (2013)
6. Jensen, H.J.: Self-organized criticality: emergent complex behavior in physical and
biological systems, vol. 10. Cambridge University Press (1998)
7. Luhmann, N.: Social systems. Stanford University Press (1995)
8. Mantegna, R., Stanley, H.E.: An introduction to econphysics: cirrelations and com-
plexity in ﬁnance. Cambridge University Press (2000)
9. Harte, D.: Multifractals: Theory and Application. Chapmann & Hall/CRC (2001)
10. Mandelbrot, B.: Self-aﬃne fractals and fractal dimension. Physica Scripta 32,
257–260 (1985)
11. Hentschel, H., Procaccia, I.: The inﬁnite number of generalized dimensions of frac-
tals and strange attractors. Physica D 8(3), 435–444 (1983)
12. Kantelhardt, W., Zschiegner, S., Koscielny-Bunde, E., Havlin, S., Bunde, A., Stan-
ley, H.E.: Multifractal detrended ﬂuctuation analysis of nonstationary time series.
Physica A 316, 87 (2002)
13. Muzy, J.F., Bacry, E., Arneodo, A.: Multifractal formalism for fractal signals:
The structure-function approach versus the wavelet-transform modulus-maxima
method. Phys. Rev. E 47(2), 875 (1993)

342
P. Jizba and J. Korbel
14. Morales, R., Matteo, T.D., Gramatica, R., Aste, T.: Dynamical generalized hurst
exponent as a tool to monitor unstable periods in ﬁnancial time series. Physica
A 391(11), 3180 (2012)
15. Scafetta, N., Grigolini, P.: Scaling detection in time series: diﬀusion entropy anal-
ysis. Phys. Rev. E 66(3), 036130 (2002)
16. Huang, J., et al.: Multifractal diﬀusion entropy analysis on stock volatility in ﬁ-
nancial markets. Physica A 391(22), 5739 (2012)
17. Jizba, P., Arimitsu, T.: The world according to R´enyi: thermodynamics of multi-
fractal systems. Annals of Physics 312(1), 17 (2004)
18. Mandelbrot, B.: Multifractals and 1/f Noise: Wild Self-Aﬃnity in Physics (1963-
1976). Springer (1999)
19. Stanley, H.E., Meakin, P.: Multifractal phenomena in physics and chemistry. Na-
ture 335(29), 405 (1988)
20. Bacry, E., Delour, J., Muzy, J.F.: A multifractal random walk. Phys. Rev. E 64,
200–201 (2000)
21. Mandelbrot, B., Calvet, L., Fisher, A.: A multifractal model of asset returns.
Cowles Foundation Discussion Papers (1164)
22. Sturges, H.: The choice of a class-interval. J. Amer. Statist. Assoc. 21, 65 (1926)
23. Scott, D.W.: Multivariate density estimation: Theory, practice and visualisation.
John Willey and Sons, Inc. (1992)
24. Freedman, D., Diaconis, P.: On the histogram as a density estimator. Zeitschrift
f¨ur Wahrscheinlichkeitstheorie und Verwandte Gebiete 57(4), 453 (1981)
25. Scott, D.W.: Feasibility of multivariate density estimates. Biometrika 78(1), 197
(1991)

 
© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
343
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_35 
 
An Economic Approach to the Evolution of an Ecology 
Doug McLeod 
Charles Sturt University, Bathurst Australia 
Abstract. We construct a simple economic model for a biological system using 
Markov chains with variable coefficients, in order to investigate how a 
biological system organizes itself and whether any measure of progress can be 
defined.   We show that if exchange of resources between creatures is based on 
relative scarcity, we get a similar outcome to a market economy even though 
such exchanges are not reciprocal. The biological economy promotes the 
development of specialization and interdependence - an ecology. Within the 
framework of this simple model, the number of creatures increases over time 
which may be construed as a large scale trend.    
Keywords: complexity, economic biology, Lesley matrix, evolutionary 
biology, large scale trend. 
1 
Introduction 
Nowadays biologists prefer to speak in terms of 'large scale trends' rather than 
'progress' in evolution.  McShea [1]  identifies eight evolutionary trends: entropy, 
energy intensiveness, evolutionary versatility, developmental depth, structural depth, 
adaptedness, size of creature and complexity.   What is interesting about McShea's list 
is the omission of two dimensions which to an economist would seem most obvious: 
• 
efficiency of resource usage 
• 
amount of total biomass  
We model the evolution of an ecosystem by employing the following concepts from 
economics.  Our agents are referred to as creatures rather than firms or consumers.  
• 
Resources    We focus on resources rather than on predation which is more 
typical of biological models.   Economics investigates the allocation of 
scarce resources so we assume that one resource is fixed in amount.   In 
Malthusian fashion we take it that this sets an upper limit on the population 
of creatures. 
• 
Exchange   Here we do not have a monetary economy but one based on the 
donation of resources by one creature to another without reciprocal 
compensation.   As we shall see, this does not make as much difference to 
behaviour or the final outcome as one might expect, because there is a 
pattern of resource flows within the community. 

344 
D. McLeod 
 
• 
Behaviour  is habitual.  Creatures have fixed probabilities of doing various 
things in particular situations so their behaviour can be represented by a 
Markov chain.   This is not the normal assumption is economics, which is 
based on optimisation (but from an empirical point of view, habitual 
behaviour is probably closer to the truth even in economic contexts).  
• 
Mutation alters the behaviour of creatures, this is represented by a change 
of the coefficients in the Markov matrices.  Mutation combined with natural 
selection leads to the same result as conscious optimisation. 
We investigate the development of interrelationships and complexity in the system. 
The conclusions are that efficiency in resource usage will increase over time, and the 
number (or biomass) of individuals will increase as a result.   The model is biological 
in a broad conceptual sense but it is a question whether enough biological detail has 
been incorporated to make our conclusions compelling to a practicing biologist. 
2 
The Model 
There are two resources which a creature needs to sustain itself, and the amount of 
each resource changes in each period according to production, consumption and trade.   
When the creature is born, state B, it has the amount (
)
1
2
,
B
B
X
X
 of resources 1, 2.   
If the amount of either resource falls to 0 then the creature dies, state D, and 
disappears.   If the amount of both resources increases to the life state L or better, 
(
)
1
2
,
L
L
X
X
, then the creature reproduces, creating S new creatures which commence 
their life back at the B state. 
Production:  In the case of Resource 1 (r1), production increases the amount of r1 by 
one if it occurs.  There is a fixed amount of resource in each period L available to be 
allocated amongst N creatures, and this affects the probability 
1P
p
 of any one creature 
producing in the period.    
 
1
P
N p
L
=
 
(1) 
In the case of Resource 2 (r2), production proceeds by the creature converting its 
stocks of r1 into r2.  The amount of r1 decreases by 1 and the amount of r2 increases 
by 1. 
Consumption:   There are three subcategories: 
o 
Metabolic.   There is a fixed probability of metabolic consumption of each 
resource. The amount decreases by one. 
o 
Reproduction.  When a creature has the resources required for production 
(
)
1
2
,
L
L
X
X
 or more and the reproduction transition is chosen,  S new 
creatures are created at the birth state B.  

 
An Economic Approach to the Evolution of an Ecology 
345 
 
 
1
1
1
2
L
B
L
B
X
X
S
X
X




=
⋅








 
(2) 
If the creature has more resources than the minimum required for 
reproduction, there will be a reduction of net resources.  This is a type of 
wastage referred to as L (life) wastage. 
o 
Death.  If consumption or production causes a creature to run out of either 
resource then it dies and is transferred to the death state D.   There is no 
transition from the death state.  The resource which has been consumed will 
decrease by 1 as usual, but some amount of the other resource will be lost 
also - this type of wastage is referred to as D (death) wastage. 
Trade:  We assume that creatures move around their environment and they meet 
randomly.   Where one creature has a resource and meets another creature, there is 
some chance that a unit of resource will be transferred to the other creature.  The 
resource flow can go either way, but there is no reciprocity in a given transaction.  
Nor of course is there any expectation of reciprocity in the future, although it may 
work out that way.  Such trade is symbiotic, each creature produces resources which 
the other creature can use.  In practice most symbiotic relationships in biology occur 
at the cellular level, where different kinds of bacteria have various chemical roles to 
play; bacteria are the chemical processing plants of the living world.  Symbiotic 
relationships can be distinguished from parasitic relationships, where the flow of 
resources can only go one way.  The chains of predation which we tend to associate 
with biology fit into this class. 
We assume trading relationships which reflect relative abundance (and scarcity), 
and this is what gives rise to the economic character of the trading. Trade is divided 
into: 
• 
Trade in.  Resources are received by the creature  
• 
Trade out.  Resources are lost by the creature 
Sitting:  This means that the creature stays in its current state and nothing changes.  It 
has residual probability remaining after other processes have been accounted for. 
The transition matrix:  We represent state transitions by a transition matrix 
*
M M
M
.   
Each element 
ij
m  of M gives the probabilities of transition from state j in this period 
to state i in the next period, i.e. we read down the columns to see what happens to 
state j.  The creature may remain in the same state between one period and the next, 
this is represented by a probability at the diagonal element 
jj
m .   Each possible 
combination of resource amounts (
)
1
2
,
X
X
 corresponds to a state, so the dimension 
M of the matrix M is equal to the number of distinct resource states, i.e. 
 
(
) (
)
1
2
1
1
MAX
MAX
M
X
X
=
+
⋅
+
 
(3) 

346 
D. McLeod 
 
D states like (
)
1,0
X
 and (
)
2
0, X
 which contain 0 of one resource are included to 
allow resource tracking but as they represent death they are never occupied and their 
columns contain only zeros.  L states contain some probabilities which are multiplied 
by the number of descendants S.   The elements in a column do not always sum to 
unity as probabilities are expected to, so the matrix M is not exactly a Markov chain 
matrix but rather a Lesley matrix, after biologist Patrick Lesley who introduced such 
matrices in the 1940s.  In the biological literature the states in Lesley matrices usually 
represent different ages or developmental states, but here states represent resource 
amounts. 
The steady state.  The distribution of the population of the creature is denoted by 
*1
M
π
 and the elements of π sum to unity.  The Perron-Frobenius theorem states that 
for an irreducible non-negative real matrix there is a unique positive real and maximal 
eigenvalue, and a corresponding unique real positive eigenvector.   As it is possible 
within transition matrix M to move from any states other than the D states to adjacent 
resource states (i.e. neighbouring states on the resource grid) and subsequently to any 
other state, M is irreducible save for the D states.    We ignore the D states and their 
corresponding rows and columns in matrix M, and apply the Perron-Frobenius 
theorem to the remainder of M.   We then apply the results to M by adding a zero to 
the eigenvectors for the D  state.  
So 
λ
=
Mπ
π  
(4) 
where   
*1
M
π
 represents the steady state distribution of population, 
 
1*1
λ
 is the rate of increase of the population per period 
Also 
λ
=
vM
v  
(5) 
where 
1*M
v
 represents the fitness coefficients (within the biological literature, the 
reproduction coefficients) of a population.   Each element of fitness vector 
v represents the expected number of descendants that creatures in that state will have, 
measured relative to other states.  Elements in the D state are zero.   Here we scale 
eigenvector v so that  
 
1
=
v π
 
(6) 
Resources.  We introduce the resource vector 
*
K M
X
which gives the amount of each 
of K resources for each state in M.  Here 
2
K =
.  The expected amount of resource 
held per creature 
*1
K
R
 is given by:  
 
=
R
Xπ  
(7) 
Resource matrix R  is non-negative. The change in resource per creature per period is 
denoted 
*1
K
ΔR
. 

 
An Economic Approach to the Evolution of an Ecology 
347 
 
RESULT 1:  RESOURCE THEOREM.    A population is stable if and only if the expected 
amount of resource per creature is stable.      
 
(
)
*1
K
−
=
X M
I π
0
 
(8) 
PROOF: 
final
initial
Expected changein resource per creatureby state =
−
X
X
 
(9) 
 
(
)
=
−
=
−
XM
X
X M
I  
(10) 
so 
(
)
(
)
(
)
1
1
0
λ
λ
Δ
=
−
=
−
=
−
=
R
X M
I π
Xπ
R
   if 
1
λ =           
(11) 
# 
Creatures:  We introduce different types of creature, denoted c1 and c2 and indicated 
by subscripts on the variables.  We assume that the creatures share same set of 
resource requirements and states although their transition matrices might differ in 
certain respects.  This may appear to fly in the face of the obvious differences 
between different creatures, but the biochemical differences between creatures are 
surprisingly small on a cellular level, with similar or identical chemical processes 
being employed.  What we observe are largely differences in scale and adaptation.  
Here we are interested in speciation - development of different species from one 
original - and the branching species will be identical in the first instance. 
Stability methodology: We assume that changes in evolutionary time are relatively 
slow compared to the flow of generations, so only the steady state eigenvectors need 
to be considered and not transitory population dynamics. 
3 
Modelling Evolution     
By representing creatures and their processes with a Markov chain we have a natural 
entry point for imposing mutation on the production function, namely changing the 
elements in the transition matrix.  Two distinct types of mutation can be implemented 
in this model: 
• 
incremental shifts, where a transition matrix parameter is perturbed 
incrementally.   
• 
deletions, where the creature loses the ability to do something.  Deletions are 
not incremental but involve setting parameters to zero.  It is relatively easy 
for a creature to lose the ability to do something, since doing it requires 
everything to be functioning correctly.    
The following result applies to incremental shifts: 
RESULT 2:   FITNESS THEOREM.   The impact on the growth rate dλ of an incremental 
mutation in the transition matrix M is given by: 
 
d
d
λ = v M π  
(12) 

348 
D. McLeod 
 
PROOF:  Differentiate (12) to get 
d
d
d
d
λ
λ
+
=
+
Mπ
M π
π
π                                       (13) 
d
d
d
d
λ
λ
+
=
+
v Mπ
vM π
v
π
v
π             times v on LHS                               (14) 
1
d
d
d
d
λ
λ
λ
+
=
⋅+
v Mπ
v π
v
π              by (5), (6)                                        (15) 
# 
This result (which curiously I have not found within the biological literature) gives a 
simple test of the fitness of a mutation representable by the elements of the transition 
matrix M.  We do not have to consider the change in the transition matrix dM in the 
context of changes to the population vector dπ  or the fitness vector dv - which may 
not be known - but can evaluate dM  using the existing population and fitness vectors 
π and v.    
As we will show, deletion mutations may have considerable evolutionary 
importance as they cause the creature to specialize.   We state a variant form of the 
Fitness Theorem which applies to these discrete jumps in matrix coefficients. 
RESULT 3:  DISCRETE FITNESS THEOREM.     Suppose a population is stable (
1
λ = ) 
and a coefficient in the transition matrix M is set to zero and the diagonal element in 
that column altered to compensate (i.e. 
0
new
ij
m
=
 and 
0
0
new
jj
jj
ij
m
m
m
=
+
).    
If 
0
Δ
>
v Mπ
 
(16) 
then 
1
new
new
new
new
λ
=
>
v
M
π
 
(17) 
where ΔM  is not infinitesimal, and 
0
new =
+ Δ
M
M
M  
  
PROOF: Straightforward and available from the author on request. 
If this criterion is satisfied by every column of the transition matrix M then the 
deletion of the capability will enhance fitness. 
4 
Modelling the Processes  
We now show how production, consumption and trade are represented in transition 
matrix M. Define: 
*
M M
γ
:  the gamma matrix for a process consists of the coefficients for the process 
in transition matrix M together with an offsetting entry in the diagonal element.  
There is a gamma matrix for each different process and resource.  For the 
reproduction process (one of the consumption processes), the offset entry in the 
diagonal element is 
REPROD
p
−
 not 
REPROD
S p
−⋅
.    In this way we can break up the M  
matrix into components: 
1,2
KP
KC
KT
KD
KL
K =
=
+
+
+
+
+

M
I
γ
γ
γ
γ
γ
 
   sum over both resources r1 and r2    (18) 

 
An Economic Approach to the Evolution of an Ecology 
349 
 
i.e. 
1,2
KP
KC
KT
KD
KL
K =
−=
+
+
+
+

M
I
γ
γ
γ
γ
γ
 
(19) 
where superscript P denotes production, C metabolic consumption, T trade, D death 
wastage and L  life wastage (i.e. reproduction consumption). 
*
M M
Γ
:  Signature matrix.  Some processes (production, consumption and trade) 
use signature matrices, which factor out the probability of the event and leave the 
elements which define the process, i.e. 
 
*
1*1
*
M M
M M
p
=
γ
Γ
 
(20) 
Usually the signature matrix Γ  consists of unity elements and p, a scalar, is the 
probability of the process, which can vary.  For trading out, the signature matrix takes 
a different form.  
TYPE
scalar
a
:  This is the expected amount of resource generated per creature per period 
for the given process shown by the superscript.   
TYPE
TYPE
TYPE
a
p
=
=
Xγ
π
XΓ
π                   using (20)                                    (21) 
Total resource amount for all processes is zero at equilibrium, as required by the 
Resource Theorem: 
(
)
*1
K
−
=
X M
I π
0
                                         by (8)                                           (22) 
(
)
1*1
K
KP
KC
KT
KD
KL
+
+
+
+
=
X
γ
γ
γ
γ
γ
π
0
    by (19), one resource only           (23) 
so    
0
KP
KC
KT
KD
KL
a
a
a
a
a
+
+
+
+
=
                  by (21)                                          (24) 
Two creatures may differ in their production characteristics but at equilibrium the 
total amount each produces must sum to zero.  Consumption 
KC
a
is defined to be the 
same for each creature, so the corresponding adjustments occur in production, trade 
and wastage. 
We now consider the detail of each process. 
Production r1:  Gamma for r1, 
1P
γ
 , transfers to a state one unit higher in r1.   
i.e. 
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0
0
P
P
P
P
P
P
P
P
P
P
p
p
p
p
p
p
p
p
−


−






−
−




=
=
=




−
−








γ
Γ
 
(25) 
In the maximal state  there is a zero, but it is assumed there is no chance of actually 
reaching that state. 

350 
D. McLeod 
 
so 
1
1
1
1
0
0
1
2
3
1
2
2
2
2
1
1
0
1
0
P
P
P
P
p
a
p
p




−







=
=
= 






−









ι
π
π
0
 
(26) 
where 
1*M
ι
 contains 1s in active states A. 
Production r2:    r2 increases by one unit and r1 is reduced by one unit. 
 
2
2
P
P
a
p
=
X Γ π  
(27) 
 
2
1
5
4
3
1
1
1
2
3
1
1
1
P
p
−




−



=



−





π  
(28) 
 
2
1
1
1
1
1
1
P
p
−
−
−


=




π  
(29) 
 
2
1
1
P
p
−


=




 
(30) 
i.e.  expression X Γ π  reduces to a production vector.  This can be used to apply 
linear production theory to this model to specify what production vectors are viable 
given the consumption vector. 
Metabolic consumption: Gamma 
KC
γ
 subtracts one from the resource.    
 
0
1
0
1
1
1
1
1
KC
KC
KC
KC
KC
KC
KC
KC
KC
KC
p
p
p
p
p
p
p
p








−
−




=
=
=




−
−




−
−




γ
Γ
 
(31) 
In the D states there are zeros, but there is no probability of being in these states. 
 
1
1
1
1
*1
0
1
0
1
2
3
1
1
0
2
2
2
2
1
1
0
1
C
C
C
C
K
p
a
p
p




−







=
= −
= −






−








−


ι
π
π
 
(32) 
Reproduction consumption:  The signature matrix transfers the creature from the L 
state to the B state. 

 
An Economic Approach to the Evolution of an Ecology 
351 
 
D wastage and L wastage:  These are not constant but vary according to the state in 
the matrix.   Both of these amounts are non-positive for every state. 
Net trade:  It is possible to choose signature matrices for trade such that the expected 
amount of net trade is given by: 
 
1
1
1
KT
KTI
KTO
a
a
a
=
+
 
(33) 
 
(
)
2
2
1
2
2
2
1
K
K
K
K
K
K
K
c x N
c x N
N c
x
x
=
−
=
−
 
(34) 
The author will provide details on request.  We see trade is proportional to the 
difference in means between the populations.  Note that intra-species trade will net 
out to zero as there is no difference in population mean between a population and 
itself. 
5 
Building Blocks 
Positive, neutral and negative production:  We define gross production in resource K 
per creature per period, 
KG
a
, as the net of production, consumption and D wastage.  It 
does not include trade or L wastage. 
 
KG
KP
KC
KD
a
a
a
a
=
+
+
 
(35) 
We define net production in resource K per creature per period, 
KN
a
, as gross 
production plus net trade.   L wastage is still excluded. 
 
KN
KP
KC
KD
KT
a
a
a
a
a
=
+
+
+
 
(36) 
If production sums to zero we refer to the process as being neutral.  If production is 
positive (negative) we say the process is positive (negative).  This applies to both 
gross and net measures. 
Standard results from classical ruin theory:  If a point starts at t in a grid running 
between 0 and T, and in each move moves up one space with probability p and down 
one space with probability q, then 
• 
If p is somewhat greater than q, then the probability of reaching T is close to 
unity, and the duration of the game is a first order function of t (proportional 
to t). 
• 
If p
q
=
 then the probability of reaching T is proportional to t, and the 
duration of the game is a second order function of t  (proportional to 
2t ).  
We use this to make the heuristic assumptions (which are simplifications of the 
reality) that: 

352 
D. McLeod 
 
• 
If a creature's net resource process is positive, the creature has no chance of 
running out of that resource, and the creature will reach the amount of that 
resource required for reproduction, 
L
X , quickly.     
• 
If a creature's net resource process is neutral then the creature may run out of 
the resource, and it will take a long time for the creature to reach the amount 
of that resource required for reproduction.   
Building block results:  The following rules govern resource production and trading: 
RESULT 4: NO NEGATIVE PROCESS.   A creature cannot be net negative in a resource. 
RESULT 5: AT LEAST ONE NEUTRAL PROCESS.    The creature cannot have positive net 
production in every process at equilibrium; net production in at least one process must 
be neutral. 
RESULT 6:  HABITAT THEOREM.     If a creature is net neutral in resource K, 
 
K
KB
x
= X
 
(37) 
Of course particular creatures can have amounts of K which differ from 
KB
X
, it is the 
average over all creatures which equals 
KB
X
.   A positive resource reaches its 
requirement for positive resource J, 
JL
X
 an order of magnitude sooner than neutral 
resource K  but has to 'wait for it' there until the creature has a full quota of all 
resources and can reproduce.   
RESULT 7: NET TRADE.   If one creature c1 is net positive and another c2 is net neutral 
then trade is given by: 
 
(
)
1
2
1
KT
K
KB
K
a
N c
x
=
−
X
 
(38) 
 
(
)
2
1
1
KT
K
K
KB
a
N c
x
=
−X
 
(39) 
Result follows immediately from (34) and Result 6 Habitat theorem.   
The following results work the other way around, inferring production strength 
from trading. 
RESULT 8:  POSITIVE FOR TRADE OUT.  If a creature trades out resource K then it is net 
positive and gross positive in K.  
RESULT 9:  COMPARATIVE PRODUCTION.   If a creature's gross production exceeds 
another's in a resource, i.e. 
 
1
2
KG
KG
a
a
>
 
(40) 
then the creature is net positive in that resource. 
RESULT 10: TWO POSITIVE PROCESSES. If two different creatures are both gross 
positive in a resource, the creatures will be net positive in that resource as well. 

 
An Economic Approach to the Evolution of an Ecology 
353 
 
Proofs:  The proofs are sketched out below: 
PROOF 4:   If a creature were net negative, i.e. 
0
KN
a
<
, then 
 
0
0
K TOTAL
KN
KL
a
a
a
=
+
<
+
,           contrary to (8).   
(41) 
# 
PROOF 5:  Suppose all resources have net positive processes.  Consider the resource K 
which increases at the slowest rate.  Then since the creature reproduces as soon as 
resource K increases up to 
KL
X
, there is no life wastage 
KL
a
 of that resource. So total 
production  
KT
KN
KL
a
a
a
=
+
 is greater than 0, contrary to Resource Theorem (8)   # 
PROOF 6:   By Result 5, creature must be net neutral in at least one resource, J say. By 
definition of a neutral resource, the expected rate of increase per period is zero so the 
average amount does not change from the original endowment  
KB
X
                    # 
PROOF 8:   If c1 is net neutral in resource K then it will not trade out, by (39).  So c1 is 
net positive in K.  By premise c1 trades out the resource, so its gross production must 
be even greater                                                                                                        # 
PROOF 9:  Assume not, then the creature c1 is net neutral in resource K.  Then by (39) 
it must be gaining resource K in trade and creature c2 must be losing it.   So by (40), 
c2 must be net negative in K, contrary to Result 1 Resource Theorem.                 # 
PROOF 10:   Suppose c1 trades out to c2 in resource K.    By Result 8, c1 must be net 
positive in K. c2 is gross positive and receives more K from trade so is net positive in 
K also.                                                                                                                    #  
6 
System Results 
One creature:  Assume that production of r2 exceeds the need for it,  so r2 is a gross 
positive process.  It follows by Result 5 that  r1 is a neutral process, so  
1
0
D
a
=
,
1
0
L
a
=
.  
The system is given below.  Each row represents a resource, denoted by superscript 
1,2.  There are three equations in three unknowns 
1
1
2
,
,
P
N x a
.  Processes 
2
1
2
,
,
P
C
C
a
a
a
 
are constant with respect to amount N but production 
1P
a
 varies with amount N. 
 
1
2
1
2
2
2
0
0
P
P
C
P
C
W
a
a
a
a
a
a


−

=






 
(42) 
 
1P
Na
L
=
 
(43) 
 

354 
D. McLeod 
 
RESULT  11:  SINGLE SPECIES EQUILIBRIUM.    Total population is given by 
1
2
2
C
C
W
L
N
a
a
a
= −
+
+
                  (note this is positive)                           (44) 
PROOF: Add (42)A and (42)B to get: 
1
1
2
2
0
P
C
C
W
a
a
a
a
+
+
+
=
                                                                           (45) 
1
2
2
0
C
C
W
L
a
a
a
N +
+
+
=
            sub 
1P
a
 (43). Rearrange for result.       (46) 
# 
Two creatures, incremental mutation case: Introduce a creature with an incremental 
mutation, so there are now two types of creature, one with the original transition 
matrix M and the other with new transition matrix 
1
d
=
+
M
M
M . 
RESULT 12:  INCREMENTAL MUTATION.   If the mutation affects a neutral process, only 
one type of creature can survive in equilibrium. 
PROOF:   Denote original creature c1 and suppose r1 is its neutral process.  Suppose 
firstly that the mutation is positive so new creature c2 is more efficient in producing 
r1 than c1.   Process r2 is unaffected so is gross positive for both creatures. 
Assume c1 and c2 both survive in equilibrium. 
By Result 9, c2 is net positive in r1. 
By Result 10, c2 is net positive in r2. 
By Result 5, c2 is not net positive in both processes.  CONTRA. 
Similarly if the mutation is negative                                                                 # 
To see what happens in practice we employ a dynamic argument.  Assess the fitness 
of the new creature c2 using the Result 2 Incremental Fitness Theorem.  If  criterion 
0
d
<
v Mπ
, rate of increase 
0
dλ <
 and c2 will go extinct immediately.   If 
0
d
>
v Mπ
 then  
0
dλ >
 and c2 will increase.  As the total number of creatures N 
increases, the probability of production of r1, 
1
P
p
, will decrease pursuant to resource 
constraint (43) and this impacts on both creatures.  For c2, the decrease in 
1
P
p
 will 
reduce d
v Mπ  and rate of increase dλ  but c2 will remain viable.  For c1, decrease in 
1
P
p
 leads to 
0
d
<
v Mπ
 and 
0
dλ <
.   c1 will reduce until it is extinct.  The 
population size N will have been increased. 
Two creatures, deletion mutation case:   Given the excess of r2, Result 3 Discrete 
Fitness Theorem suggests that a mutation whereby the creature no longer produces r2 
will be viable. Assume such a creature, c2, arises.  If c2 is going to be viable then the 
results in the previous section imply the following: 
• 
By Result 4 No negative process, process c2r2 is gross negative so c2 must 
trade in and make the process net neutral.    

 
An Economic Approach to the Evolution of an Ecology 
355 
 
• 
By Result 8 Positive for trade out, process c1r2 must remain net positive to 
sustain trade.  
• 
By Result 5 At least one neutral process, process c1r1 must be a neutral 
process. 
• 
By Result 9 Comparative production, process c2r1 must be a positive 
process as it is more productive than process c1r1, which has an additional 
subtraction of amount  
2
P
a
.    
In writing the system, we omit creature subscripts where the process is the same for 
both creatures. There are five equations in five unknowns 
2
1
1
1
2
1
2
,
,
,
,
P
N N
x
x a
.  Processes 
2
1
2
,
,
P
C
C
a
a
a
 are constant with respect to amounts 
1
2
,
N N and means 
2
1
1
2
,
x
x . We look at 
the change in resources which the introduction of c2 in amount 
2
N  creates.  
c1:          
1
2
1
1
2
1
2
2
2
2
1
2
0
0
P
P
C
T
P
C
W
T
a
a
a
N a
N
a
a
a
N a


−
+
+

=



+
+
−



                   
(47) 
c2:         
1
1
1
1
2
1
2
2
2
1
0
0
P
C
W
T
C
T
a
a
a
N a
N
a
N a


+
+
−

=



+
+



                      
(48) 
1
1
1
2
P
P
N a
N a
L
+
=
           
1
1
P
P
a
p
=
 by (26) and ignoring 0 element          (49) 
The trade amounts are given by: 
(
)
1
1
1
1
2
T
B
a
c
x
X
=
−
                                                                                      (50) 
(
)
2
2
2
2
1
T
B
a
c
x
X
=
−
                                                                                     (51) 
RESULT 13:  SYSTEM EQUILIBRIUM.    If c2 is viable, i.e. c2 is able to procure sufficient 
resources by trade to cover consumption of r2: 
2
2
1
0
C
T
a
N a
+
>
               evaluated at 
1
2
,
0
ONE CREATURE
N
N
N
=
=
             (52) 
then       
2
0
N >
                                                                                                         (53) 
and      
0
N
Δ
>
                             where 
1
2
N
N
N
=
+
                                            (54) 
PROOF: Add equations (47)A, (47)B, (48)A, (48)B to get: 
(
)
1
1
2
2
1
1
1
2
2
0
P
C
C
W
W
Na
Na
Na
N a
N a
+
+
+
+
=
                                                 (55) 
We want to compare waste with one creature case.  Restate that case and subtract: 
1
1
2
2
0
P ORIG
C
C
W ORIG
Na
Na
Na
Na
+
+
+
=
      (45)*N,   
1
2
,
C
C
a
a
 constant     (56) 
(
) (
)
1
1
2
1
2
1
1
2
2
0
P
P ORIG
W
W
W ORIG
Na
Na
N a
N a
Na
−
+
+
−
=
      (55) less (56)                  (57) 

356 
D. McLeod 
 
Find the (
)
1
1
P
P ORIG
Na
Na
−
term using the c1r1 process equations and eliminate: 
(
)
1
1
1
2
0
P
P ORIG
T
Na
Na
N N a
−
+
⋅
=
                        (47)*N less (42)* N                (58) 
(
) (
)
2
1
2
1
1
1
2
2
2
0
W
W
W ORIG
T
N a
N a
Na
N N a
+
−
+
⋅
=
    (57) less (58)                          (59) 
Derive expression for N here and compare with the one creature case:  
(
)
1
2
2
1
1
2
0
C
C
W ORIG
T
L
Na
Na
Na
N N a
+
+
+
+
⋅
=
    (55) with (49),(59)                  (60) 
1
2
2
1
1
2
C
C
W ORIG
T
L
N
a
a
a
N a
= −
+
+
+
                      rearranging                             (61) 
1
2
2
1
ONE CREATURE
C
C
W ORIG
L
N
a
a
a
= −
+
+
                  restating (44)                          (62) 
ONE CREATURE
N
N
>
     i.e.  
0
dN >
                        if 
1
2
0
T
N a
>
, i.e. if 
2
0
N >
   (63) 
We now evaluate 
2
N  using differentials.  The initial position for evaluating 
differential expressions is: 
2
0
N =
                                                                                                                (64) 
1
1
P
P
a dN
da
N
−
=
                                                    differentiating  (49)               (65) 
1
1
1
2
2
0
P
T
T
da
dN a
N da
+
+
=
                                differentiating (47)A              (66) 
1
1
2
0
P
T
a dN
dN a
N
−
+
=
                                        by (65), (66) and (64)             (67) 
so    
1
2
1
0
P
T
dN a
dN
N
a
=
⋅
>
                                            
if 
0
dN >
                                (68) 
We see by (63), (68) that a consistent positive solution exists for both variables. # 
The amount of wastage of resource 1 in the one creature case is reduced here by the 
amount of resource 1 which creature 1 gains by trade, 
1
2
T
N a
, and this is what drives 
the increase in amount N .   We see that the expansion of the ecosystem to two 
creatures leads to a more efficient utilization of the available resources, and 
consequently an expansion in the total number of creatures.   We also see that a 
deletion mutation is compatible with the existence of both creatures, unlike 
incremental mutations.  This suggests that deletion mutations are the cause of the 
specialization we see in nature, and indeed in human economies. 

 
An Economic Approach to the Evolution of an Ecology 
357 
 
Two creatures, two deletion mutations (parasite) case:  Could a new creature trade in 
both resources and survive?  In this model, one of the original creature's processes 
would be a neutral process (Result 5 At least one neutral process)  and not make 
resources available for trade (Result 8 Positive for trade out), so the new creature 
would not be able to trade in that resource and survive.  To model parasitism requires 
that a creature can trade in resources more aggressively than in the normal case. 
7 
Conclusion 
We have shown that speciation will lead to an increase in the number of creatures 
which a given amount of resource L will support, because it allows a resource to be 
produced in proportion to its requirements even if the naturally available production 
function for the resource results in an oversupply.  This conclusion holds within a 
system where the two creatures are identical in other respects.  We can imagine that 
as the creatures begin to pursue different types of production, they will acquire further 
characteristics which fit them more nearly to their chosen role - in other words, they 
adapt and become more efficient.  We see the interesting result that an ecology is 
likely to derive from deletion mutations rather than incremental mutations because 
deletion mutations lead to interdependence of species, rather than replacement of one 
species by a more efficient one.   Finally, we have shown that the economic concepts 
of efficiency and quantity may also define long term trends in biology, and deserve to 
be considered alongside more obvious measures of complexity. 
Reference 
1. McShea, D.W.: Possible largest-scale trends in organismal evolution: eight live hypotheses. 
Annual Review of Ecology and Systematics 29, 293–318 (1998) 

Foraging Multi-Agent System Simulation Based
on Attachment Theory
Carlos Alberto Riveros Varela1, Ferney Beltr´an Velandia1,
Miguel Alberto Melgarejo Rey1, Nadya Gonz´alez Romero2,
and Nelson Obreg´on Neira2
1 Universidad Distrital ”Francisco Jos´e de Caldas”, Bogot´a, Colombia
carlosriverosv@gmail.com, {kilmfer,mmelgarejo}@ieee.org
2 Pontiﬁcia Universidad Javeriana, Bogot´a, Colombia
ngonzale328@gmail.com, nobregon@javeriana.edu.co
Abstract. A multi-agent system is developed to simulate the emer-
gency of collective behavioral patterns in a foraging model based on the
Ant Foraging Model made in NetLogo R
⃝. Two kind of agents are imple-
mented, secure and selﬁsh, and they are modeled considering the attach-
ment theory, which describes the dynamics of long-term relationships
between humans and establishes speciﬁc behavioral patterns. 100 simu-
lations are developed varying the amount of selﬁsh and secure agents, the
results show that secure agents are more collaborative than selﬁsh ones
and the steps of simulation substantially decrease when secure agents are
most than selﬁsh agents.
Keywords: Multi-Agent System, Attachment Theory, Foraging Model,
Human Behavioral Patterns.
1
Introduction
Human beings tend to explore the environment to ﬁnd out information which let
them self-regulate and protect their integrity[1]. Through the behavioral system,
one person can respond in diﬀerent ways to diﬀerent stimulations and exhibit
some behaviors for speciﬁc situations. The behavioral system is composed by
4 sub-systems: exploration, alert, membership and attachment, and the attach-
ment theory is developed based on the last one; it describes the dynamics of
long-term relationships between humans and establishes speciﬁc behavioral pat-
terns. The correct development of those patterns is strongly inﬂuenced by the
childhood relationship between people and their attachment ﬁgure.
Complexity has been recognized as a discipline which try to explain those
natural phenomena hard to understand beginning with the individual behavior
of their components. Complexity has been deﬁned from diﬀerent kind of disci-
plines but there is not an agreement among these yet [2]. However, Bar Yam has
given his deﬁnition of a complex system as a system formed out of many com-
ponents whose behavior is emergent, that is, the behavior of the system cannot
be simply inferred from the behavior of its components [3]. When the complex
c
⃝Springer International Publishing Switzerland 2015
359
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems,
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_36

360
C.A.R. Varela et al.
system components, often called agents, can adapt or learn as the interact, it
is referred as a Complex Adaptive Systems (CAS) [4]. According to the Holland
deﬁnition [4]:
”A CAS is a complex, self-similar collectivity of interacting adaptive
agents”.
Multi-agent systems (MAS) are used to model and simulate CAS where multi-
ple agents interact within a speciﬁc environment. An agent has a set of rules and
properties which governs their behavior and their making decision capability.
MAS have some common properties involving autonomous/self-driven agents,
local interactions, variable connections, collective behaviors, distributed regions
and communicating neighbors [5]. The aim of MAS is to achieve global emer-
gence behaviors merging from a speciﬁc model with a particular purpose where
a set of local behavior rules is deﬁned among agents and their environment [6].
NetLogo R⃝is an agent-based programming language and integrate modeling
environment to simulate MAS [7], An Ant Foraging Model developed by Uri
Wilensky in 1997 [8] and simulated in NetLogo R⃝is create a Foraging MAS sim-
ulation of two types of human behavior (secure and selﬁsh agents) based on the
attachment theory.
2
Methods
2.1
Ants Foraging NetLogo R
⃝Model
This project called Ants Foraging was developed in NetLogo R⃝by Uri Wilensky
and it shows an ants colony forages for food where each ant has a set of simple
rules that let it to interact with its environment and other ants. When an ant
ﬁnds a piece of food, it carries the food back to the nest while drops a chemical as
it moves. When other ants “sniﬀ” the chemical, they follow the chemical toward
the food. The more ants carry food to the nest the more they reinforce the
chemical trail. The environment has a shape of 71x71 patches, 3 food placements
and 1 nest.
2.2
Attachment Theory and Agents
Taking into account the attachment theory, two types of agents are developed:
selﬁsh and secure people. Selﬁsh agent represent people who are not collaborative
trying to do all by themselves, they do not tend to communicate with other
people, they prefer to work lonely as fast as they can. The secure agents tend
to be more collaborative and communicate with people, they could not be faster
than selﬁsh people when they do something, but they are better working in
a team. For simulation, selﬁsh agents are faster than secure ones, but their
chemical (called communication trail for humans) is smaller than the chemical
trail of secure agents.

Foraging Multi-Agent System Simulation Based on Attachment Theory
361
2.3
Agent Behavior
The state diagram of a single agent is shown in Fig. 1; ﬁrstly, the agent is born
and pass to the state “look for food”. If it perceives a communication trail, it
means that other agent which is carrying food was near there and the ﬁrst ant
will change its orientation toward the strongest perception, and turn back to
“look for food” state; if it does not perceive a communication trail, it will move
randomly and continue in the “look for food” state; if the agent ﬁnds food, it
will pick food up and change its state to “look for the nest”. Until the agent
does not ﬁnd the nest, it will drop a communication trail as it moves. When the
agent ﬁnds the Nest, it drops food and come back to state “look for food”.
2.4
Simulations
The proposal of this work is related to the emergence of collective behavior from
ant foraging model where two kind of agents interact (selﬁsh and secure people)
based on behavioral patterns of people describe in [1], the environment is the
same to the “Ant Foraging” model from NetLogo R⃝. Table 1 shows some static
characteristics for the environment, selﬁsh and secure agents. 100 simulations
are developed varying the amount of selﬁsh and secure agents in this order: 10
experiments for 0 selﬁsh agents and 100 secure agents at ﬁrst, then 10 experi-
ments for 10 selﬁsh and 90 secure agents, until the last 10 experiments with 100
selﬁsh and 0 secure agents. The amount of food collected for each kind of agent
and the steps of simulation are the measures of the system.
3
Results
Table 2 shows the percentage of total food collected by secure and selﬁsh agents
as well as its standard deviation and the minimum, maximum and the mean
value of steps (time of simulation) until all food were collected. 10 simulations
were performed keeping 100 agents by each group varying the amount of secure
and selﬁsh ones in 10. When 90 % of agents have a secure behavioral pattern
and only 10 % of them are of selﬁsh the total amount of food collected by secure
ones is 17.48 % versus 5.45 % of food collected when 90 % of them are selﬁsh and
only 10 % are secure. This would prove the emergency of a collaborative behavior
from secure agents to the selﬁsh ones, which does not happen otherwise. In Fig. 2
is shown the communication trail formed by secure agents and Fig. 3 shows the
same for selﬁsh agents. Figure 2 shows a vast amount of trail is formed, which
helps selﬁsh agents to increase their amount of food collected as can be seen in
the slope of Fig. 4. Otherwise when selﬁsh agents are most, their communication
trail does not help secure agents to collect food. In Table 2 the mean of simulation
steps increases as more selﬁsh agents exist and this increment aﬀects negatively
the population time spend to collect all food.

362
C.A.R. Varela et al.
Table 1. Principal Characteristics of Multi-agent system
Speed Communication Trail Shape
Movement population food
Secure Agents
0.8
200
Big random ±40 ◦
100-0
Selﬁsh Agents
1
10
Small random ±10 ◦
0-100
Environment
71x71
100
3
Fig. 1. Agent state diagram
Table 2. Total food collected by selﬁsh and secure agents for foraging simulations
Secure Selﬁsh
Total food of
Total food of
Steps of Simulation
Agents Agents secure Agents selﬁsh agents
Mean() Std dev Mean() Std dev Mean(step) Minimum Maximum
100
0
100
0
0
0
1379
1066
1784
90
10
82.51
2.68
17.49
2.68
1461.6
1022
2240
80
20
67.91
3.10
32.09
3.10
1677.6
950
2910
70
30
52.74
2.82
47.26
2.82
2327.4
1464
4310
60
40
44.21
2.42
55.79
2.42
2365.3
928
3420
50
50
33.70
3.44
66.30
3.44
2577.7
1294
3670
40
60
24.46
2.33
75.54
2.33
3109.7
1637
3770
30
70
17.57
1.76
82.42
1.76
3298
2940
4180
20
80
10.74
1.72
89.26
1.72
3617.8
3027
4216
10
90
5.46
1.05
94.54
1.04
3372.1
3056
3822
0
100
0
0
100
0
3401
3050
3800

Foraging Multi-Agent System Simulation Based on Attachment Theory
363
Fig. 2. Communication trail formed by a group of secure agents
Fig. 3. Communication trail formed by a group of selﬁsh agents
Fig. 4. Total food collected in a simulation where selﬁsh agents are most than secure
ones

364
C.A.R. Varela et al.
4
Conclusions
– There is a signiﬁcant diﬀerence on simulation time between population with
more secure agents and population with more selﬁsh agents. The mean of
steps for 100 selﬁsh agents is twice as long as for 100 secure agents. It could
happen because selﬁsh agents tend to be less communicative and collabora-
tive with other agents.
– Collaborative behavior from secure agents to selﬁsh agents is evidenced when
they were searching for food and a trail between food place and the nest was
created. It is observed that 17.48 % of food was collected by selﬁsh agents
when there were 90 % of secure agents on simulation, whereas only 5.45 % of
food was collected by secure agents when there were 90 % of selﬁsh ones.
References
1. Gonzalez, N.: Modelo Generativo para la visualizaci´on de emergencias sociales
futuras (2013),
http://prezi.com/fdv9 bh5f1r3/apego/?utm source=website&utm medium=
prezi landing related solr&utm campaign=prezi landing related owner
2. Maldonado, C.E., Alfonso, N.: El Mundo de las Ciencias de la Complejidad. Editorial
Universidad del Rosario (2010)
3. Bar-Yam, Y.: The emergence of complexity. Addison-Wesley (2004)
4. Holland, J.H.: Studying complex adaptive systems. Journal of Systems Science and
Complexity 19, 1–8 (2006)
5. Lu, J., Chen, G., Yu, X.: Modelling, analysis and control of multi-agent systems:
A brief overview. In: 2011 IEEE International Symposium of Circuits and Systems
(ISCAS), pp. 2103–2106 (2011)
6. Macal, C.M., North, M.J.: Tutorial on Agent-Based Modeling and Simulation PART
2: How to Model with Agents. In: Proceedings of the Winter Simulation Conference,
WSC 2006, pp. 73, 83, 3–6 (2006)
7. Wilensky, U.: NetLogo Ants model (1997),
http://ccl.northwestern.edu/netlogo/models/Ants
8. Zhang, C., Jia, S., Wei, F.: Artiﬁcial ant colony foraging simulation and emergent
property analysis. In: IEEE Congress on Evolutionary Computation, CEC 2008, pp.
1907, 1912, 1–6 (2008)

 
© Springer International Publishing Switzerland 2015 
A. Sanayei et al. (eds.), ISCS 2014: Interdisciplinary Symposium on Complex Systems, 
365
Emergence, Complexity and Computation 14, DOI: 10.1007/978-3-319-10759-2_37 
 
A Spatial Model for Infrastructure Network Generation 
and Evolution 
Gaihua Fu*, Sean Wilkinson, and Richard Dawson  
School of Civil Engineering and Geosciences, Newcastle University,  
Newcastle upon Tyne, U.K. 
{Gaihua.Fu,Sean.Wilkinson,Richard.Dawson}@ncl.ac.uk 
Abstract. Infrastructure systems are vitally important to our society as they 
deliver the goods and services that communities require to function.  This 
reliance on infrastructure systems makes any disruption to their functioning 
liable to result in disproportionate consequences to communities. In this paper 
we investigate key parameters that exist in real world networks in an attempt to 
uncover the driving forces of network generation and evolution.  We then 
present a network model that can reproduce networks with several non-trivial 
properties that are the key signature of real infrastructure networks. By 
changing the drivers that control evolution of our synthetic networks, the model 
can potentially help us to better predict what a future infrastructure network 
may look like, and how resilience it will be to future hazards. 
Keywords: complex system, infrastructure network, network evolution.  
1 
Introduction 
Of the various physical components that form our society, infrastructure networks are 
one of the most vital parts. Infrastructure systems, such as electrical distribution, 
water and transport networks, deliver goods and services to our communities as well 
as promoting social well-being and supporting economic growth. Our reliance on 
these systems is now so great that we are acutely exposed to disturbances to these 
systems and disruption to their functioning can lead to disproportionate consequences 
to the communities that rely on them [1-3].  Hence the ability to understand the key 
features of resilient networks and their capability to adapt to future changes are vitally 
important. 
Among other techniques employed to study the properties of infrastructure 
systems, network modelling approaches [4, 5] have been employed to understand how 
an infrastructure system behaves itself in network disruption and to predict how it 
might grow and evolve over time [6-10].  As discussed in [11], as these networks 
serve populations, they too must evolve in response to evolving populations and 
demographic shifts.  This evolution is believed to be governed by a few simple rules, 
that optimise their ability to provide services to the populations that rely on them [11-
                                                           
* Corresponding author. 

366 
G. Fu, S. Wilkinson, and R. Dawson 
 
13] and result in emergent behaviour and network structures possessing particular 
architectures [14, 15].   
Based on existing studies on spatial network generation [10, 16, 17], in this paper 
we propose a spatial network model in an attempt to reproduce networks that exhibit 
structure and properties as observed in real infrastructure networks. We consider that 
three factors, network service demand, network efficiency, and network cost, are the 
main elements that drive the evolution of an infrastructure network. We show how 
this model can be used to generate various network structures in response to different 
drivers. Our preliminary experimental results show that the proposed model generates 
networks that present several non-trivial properties that are the key signature of real 
infrastructure networks. The model can also potentially help us to better predict what 
a future infrastructure network may evolve into, the likely benefits it will impart on 
the communities it supports and how vulnerable it will be to hazard and help to 
support interventions that may achieve these aims. 
2 
Empirical Analysis of Real Infrastructure Networks  
We have studied the properties of a few real world infrastructure networks, in an 
attempt to uncover the driving force of network evolution. We first investigated how 
infrastructure facilities are distributed in space and found that many of them have a 
clustered node distribution. The scale of this nodal clustering varies from one system 
to another. For example, we have observed very tight nodal clusters in railway 
networks and power distribution networks; whereas nodes in telecom and water 
distribution systems form very loose clusters resulting in a more dispersed nodal 
distribution. We observed that the clustering of infrastructure facilities is correlated 
with population density and we believe that this is a proxy for infrastructure service 
demand. This correlation between demand and infrastructure facilities has been 
proven by our research and is summarised in Figure 1, which shows spatial 
distribution of population (an indication of service demand) and the distribution of a 
sample of infrastructure in England and Wales.  
We went on to examine how infrastructure facilities are connected or linked to 
each other. We believe that infrastructure network linkages are formed by balancing 
network cost and efficiency. If one is interested in minimising the network 
construction and operation cost, then an optimal network is the one with many short 
range links. We made a rather simplified assumption here that the cost of a network is 
only dependent on the physical length of its links. Conversely if one is interested in 
efficient network service transportation, then an optimal network is the one with short 
average path length (APL [18]).  
To prove this, we studied the cost and efficiency of a few infrastructure networks, 
and uncover the relationship between two. We found that an efficient network is 
usually not economic. For example, an Air traffic or Internet network has a small 
APL and it only takes 2 or 3 steps (APL) to travel from one node to another, but it has 
long edge length and therefore costs more to build or operate. On the other hand, an 
economic network such as road and railway network has many short links, but it is 

 
A Spatial Model for Infrastructure Network Generation and Evolution 
367 
 
usually less efficient and it takes many hops (having a large APL) in order to travel 
from one place to another. This supports the intuitive conclusions that cost and 
efficiency are two competing variables in the formation of networks. Depending on 
the purpose of networks, an infrastructure network may be designed to bias towards 
one variable or may consider both variables.  
 
Fig. 1. Distribution of population and infrastructure facilities in the area of England and Wales 
(a) population density in km2 obtained from 2011 Census data [19] (b) spatial distribution of 
electricity substations (c) spatial distribution of telecom masts 
3 
A Spatial Network Model  
We have developed a spatial network model in an attempt to reproduce networks that 
exhibit structure and properties as observed in real infrastructure networks. To 
generate a network in a predefined geographical space S, we consider that three 
factors, network demand, network efficiency, and network cost, are the main elements 
that drive the evolution of an infrastructure network. The demand for a network 
service in our model influences how a network node i is distributed in S, and nodes of 
different spatial distributions can be generated depending on the scale of their 
correlation to population/ demand density.  
Once a network node ݅ is introduced into an area a in S, it connects to existing 
network nodes. Our research suggests that most nodes choose to connect to nodes that 
are in its proximity. This reduces the network construction cost. To reduce the number 
of hops for a network service to be delivered to another network place, a node often 
chooses to connect to a high degree/capacity node. This improves network efficiency. 
As such we have two variables that regulate network linkages. One aims to reducing 
the cost of network construction, and one aims to improving network efficiency.  
We use ݀௜௝ to denote the geographical distance between node ݅ and node ݆, and 
݇௝ denotes the degree of node j. The probability of building an edge < ݅, ݆>   
 
 

368 
G. Fu, S. Wilkinson,
 
between a newly introduce
݇௝ and ݀௜௝, and it is prop
larger is ݇௝, or smaller is ݀
generate a range of network
for connecting to proximate
degree nodes.  
 
 (a) 
(c) 
Fig. 2. Networks generated 
emphases low cost and the r
network which emphases eff
network which puts emphasi
exhibits spatial clusters with lo
0.050 - 0.230
0.231 - 0.410
0.411 - 0.590
0.591 - 0
 and R. Dawson 
d node  ݅ and an existing node ݆ is jointly determined
portional to ݇௝ and is inversely proportional to ݀௜௝. T
݀௜௝, the more likely that i is connected to j. To be able
ks, scaling parameters are used to 1) govern the prefere
e nodes and 2) govern the preference of connecting to h
 
(b) 
 
(d) 
with our model (a) demand distribution (b) a network wh
resultant network exhibits short links and spatial clusters (c
ficiency and hub nodes emerge in the resultant network (d
is on both low cost and efficiency, and the resultant netw
ocal hub nodes. 
0.770
0.771 - 0.950
d by 
The 
e to 
ence 
high 
hich 
c) a 
d) a 
work 

 
A Spatial Model for Infrastructure Network Generation and Evolution 
369 
 
The proposed model is powerful and able to generate a range of networks. Figure 2 
shows a few example networks generated with our model. We assume the availability 
of demand distribution as presented in Figure 2(a). For demonstration purpose,  
nodes are plotted in the same color with its demand area , and the size of a node is 
plotted proportionally to its degree. For networks with little preference for attaching 
to high degree nodes, increasing the preference for short links will encourage a linage 
to a proximate node and spatial clusters start to emerge. An example network of this 
type is presented in Figure 2(b).  These types of networks cost less to build but are 
not efficient, exhibiting spatial and topological properties demonstrated by electricity 
or road networks. For networks with little preference for attaching to a proximate 
node, increasing the preference for a high degree node results in the emergence of 
network hubs. An example network is shown in Figure 2(c). This type of networks is 
efficient but costs more to build, bearing properties that are similar to that of Internet 
or airway traffic networks. By setting the preference for a node that is both in 
proximate and of high degree, we generate a network that exhibits spatial clusters as 
well as hub nodes, which makes the network both economic and efficient. An 
example network of this type is shown in Figure 2(d). 
4 
Simulation and Validation 
We validated the proposed network model by trying to re-generated networks that 
share properties of real infrastructure networks, and we use the 2011 UK Census 
statistics [19] as a demand input to our model. For demonstration purpose, we present 
simulation results for the railway network of England and Wales, where a network 
node is a train station, and there is an edge between two nodes if they are connected 
by a rail line. There are total 3547 network nodes, and the average node degree of the 
network is 2.92. The nodes are spatially clustered and a strong correlation to the 
demand is identified. Our analysis shows that the network has a rather uniform node 
degree distribution, with 2- and 3-degree nodes accounting for 64% of all nodes, and 
the highest node degree is 8. Hence no preference is given for connection to high 
degree nodes. The network shows a clear bias towards short range edges, with the 
longest edge of the network spans only about 4% of the diameter of its embedding 
space.  
Calibrating a set of parameters, we have simulated the railway network of 
England and Wales. The results were obtained by averaging over 100 realisations. 
Figure 3 shows the distribution of network nodes for the network, and results  
are compared against that of the real network. The good match between two were 
obtained.  

370 
G. Fu, S. Wilkinson, and R. Dawson 
 
 
Fig. 3. Distribution of the network nodes  
Figure 4 compares the edge length distribution of simulated networks and real 
network. Again, good agreement is observed between simulated networks and real 
network. Similar results were obtained for other networks investigated in this 
research, and a reader is referred to [20] for our further experimental results on this. 
 
Fig. 4. Distribution of edge length   
5 
Conclusions  
This paper presents observed relationships between population density, infrastructure 
density and connectivity in infrastructure networks.  These are then used to develop a 
spatial network model which forms network facilities and connecting linkages by 
balancing three driving factors namely; infrastructure service demand, network 
efficiency, and network cost. Through empirical study and computer simulation, we 
show that proposed network model is capable of reproducing networks that exhibit 
0
10
20
30
40
50
0
50
100
150
200
250
300
350
Number of stations in a area
Census Areas in England and Wales 
Empirical
Simulated
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0
5
10
15
20
25
30
35
40
45
P(d)
Edge length d in KM
Empirical
Simulated

 
A Spatial Model for Infrastructure Network Generation and Evolution 
371 
 
structure and properties observed in real infrastructure networks. The model may be 
useful in testing scenarios of infrastructure development by changing the relative 
importance of the driving factors and observing the resulting network and its 
associated properties.   
References   
1. Wilkinson, S.M., Alarcon, J.E., Mulyani, R., Whittle, J., Chian, S.C.: Observations of 
damage to buildings from M w 7.6 Padang earthquake of 30 September 2009. Nat. 
Hazards 63, 521–547 (2012) 
2. Fu, G., Dawson, D., Khoury, M., Bullock, S.: Interdependent networks: Vulnerability 
analysis and strategies to limit cascading failure. Eur. Phys. J. B 87, 148 (2014) 
3. Dunn, S., Fu, G., Wilkinson, S., Dawson, D.: Network Theory for Infrastructure Systems 
Modelling. Proceedings of the ICE - Engineering Sustainability 166, 281–292 (2013) 
4. Newman, M., Barabasi, A., Watts, D.: The Structure and Dynamics of Networks. 
Princeton University Press, USA (2006) 
5. Albert, R., Barabasi, A.: Statistical mechanics of complex networks. Rev. Mod. Phys. 74, 
47–97 (2002) 
6. Rosas-Casals, M., Valverde, S., Sole, R.V.: Topological vulnerability of the European 
power grid under errors and attacks. Int. J. Bifurcat. Chaos 17, 2465–2475 (2007) 
7. Albert, R., Albert, I., Nakarado, G.: Structural vulnerability of the North American power 
grid. Phys. Rev. E 69, 025103 (2004) 
8. Ten, C., Liu, C., Manimaran, G.: Vulnerability Assessment of Cybersecurity for SCADA 
Systems. IEEE T. Power Syst. 23, 1836–1846 (2008) 
9. Cohen, R., Erez, K., Ben-Avraham, D., Havlin, S.: Breakdown of the Internet under 
intentional attack. Phys. Rev. Lett. 86, 3682–3685 (2001) 
10. Yook, S.H., Jeong, H.W., Barabasi, A.L.: Modeling the Internet’s large-scale topology. P. 
Natl. Acad. Sci. USA 99, 13382–13386 (2002) 
11. Gastner, M.T., Newman, M.E.J.: Optimal design of spatial distribution networks. Phys. 
Rev. E 74 (2006) 
12. Wilkinson, S., Dunn, S., Ma, S.: The vulnerability of the European air traffic network to 
spatial hazards. Nat. Hazards 60, 1027–1036 (2012) 
13. Bettencourt, L.M.A., Lobo, J., Helbing, D., Kuehnert, C., West, G.B.: Growth, innovation, 
scaling, and the pace of life in cities. P. Natl. Acad. Sci. USA 104, 7301–7306 (2007) 
14. Crucitti, P., Latora, V., Marchiori, M.: A topological analysis of the Italian electric power 
grid. Physica. A 338, 92–97 (2004) 
15. Carvalho, R., Buzna, L., Bono, F., Gutierrez, E., Just, W., Arrowsmith, D.: Robustness of 
trans-European gas networks. Phys. Rev. E 80, 016106 (2009) 
16. Cao, X.B., Du, W.B., Hu, M.B., Rong, Z.H., Sun, P., Chen, C.L.: Topology Property and 
Dynamic Behavior of a Growing Spatial Network. Int. J. Mod. Phys. C 22, 333–345 (2011) 
17. Barthelemy, M.: Spatial Networks. Phys. Rep. 499, 1–101 (2011) 
18. Gross, J.L., Yellen, J.: Handbook of Graph Theory. CRC Press (2004) 
19. Office for National Statistics, http://www.ons.gov.uk/ons/rel/census/ 
2011-census/population-and-household-estimates-for-england-
and-wales—unrounded-figures-for-the-data-published-16-july-
2012/index.html 
20. Fu, G., Wilkinson, S., Dawson, D.: A Network Model for Infrastructure Network 
Evolution and Resilience Study. Newcastle University (2014) 

Author Index
Anzo, A.
215
Armando, Quiñones
287
Augustová, Petra
249
Avcu, Neslihan
77
Bagnoli, Franco
319
Barajas-Ramírez, J.G.
215
Barlow, Thomas M.
143
Beige, Almut
143
Beran, Zdenˇek
249
Boostani, Reza
153
Bratsun, Dmitry
295
ˇCelikovský, Sergej
249
Cherubini, Anna Maria
99
Cisar, Petr
261
Clark, Lewis A.
143
Corrado, Raffaele
99
Davendra, Donald
205, 225
Dawson, Richard
365
Demir, Güleser Kalaycı
77
Dmitrichev, Aleksey
27
Fu, Gaihua
365
Gentili, Pier Luigi
37
Ghodratnama, Samira
153
Graciela, Juez-C
287
Güzeli¸s, Cüneyt
77
Huang, Wei
143
Jablonský, Jiˇri Jablons´k
277
Jasinevicius, Raimundas
87
Jizba, Petr
333
Kasatkin, Dmitry V.
27, 57
Kazanavicius, Egidijus
87
Korbel, Jan
333
Lampart, Marek
173
Lara-Rosano, Felipe
183
Manin, Yuri I.
3
Maslennikov, Oleg V.
57
Massaro, Emanuele
319
Matonoha, Ctirad
277
McLeod, Doug
343
Mouri, Hideaki
109
Nahlik, Tomas
261
Neira, Nelson Obregón
359
Nekorkin, Vladimir I.
27, 57
Oplatkova, Zuzana Kominkova
205
Papáˇcek, Štˇepán
277
Pavel, Šedek
115
Pekergin, Ferhan
77
Pekergin, Nihal
77
Pennetta, Cecilia
99
Petera, Karel
277
Petrauskas, Vytautas
87
Pismen, Len
295
Pluhacek, Michal
205, 225
Politi, Antonio
21
Radko, Kˇríž
115
Rehák, Branislav
277
Rey, Miguel Alberto Melgarejo
359
Romero, Nadya González
359

374
Author Index
Rossler, Otto E.
135
Rychtarikova, Renata
261
Sayankina, Maria K.
125
Senkerik, Roman
195, 205, 225
Shiraki, Yoshinao
165
Smaglichenko, Alexander V.
125
Smaglichenko, Tatyana A.
125
Smaha, Rebecca
261
Strelkowa, Natalja
269
Stys, Dalibor
261
Stys Jr., Dalibor
261
Suetani, Hiromichi
47
Tarissan, Fabien
309
Urban, Jan
261
Vantuch, Tomas
239
Varela, Carlos Alberto Riveros
359
Velandia, Ferney Beltrán
359
Volodin, Igor A.
125
Voorhees, Burton
67
Wiesner, Karoline
15
Wilkinson, Sean
365
Zakharov, Andrey
295
Zapomˇel, Jaroslav
173
Zelinka, Ivan
195, 205, 225, 239

