Deductive Beam Search:
Decoding Deducible Rationale for Chain-of-Thought Reasoning
Tinghui Zhu * 1 Kai Zhang * 2 Jian Xie 1 Yu Su 2
Abstract
Recent advancements have significantly aug-
mented the reasoning capabilities of Large Lan-
guage Models (LLMs) through various method-
ologies, especially chain-of-thought (CoT) reason-
ing. However, previous methods fail to address
reasoning errors in intermediate steps, leading to
accumulative errors. In this paper, we propose De-
ductive Beam Search (DBS), which seamlessly
integrates CoT and deductive reasoning with step-
wise beam search for LLMs. Our approach de-
ploys a verifier, verifying the deducibility of a
reasoning step and its premises, thus alleviating
the error accumulation. Furthermore, we intro-
duce a scalable and labor-free data construction
method to amplify our model’s verification capa-
bilities. Extensive experiments demonstrate that
our approach significantly enhances the base per-
formance of LLMs of various scales (7B, 13B,
70B, and ChatGPT) across 8 reasoning datasets
from 3 diverse reasoning genres, including arith-
metic, commonsense, and symbolic. Moreover,
our analysis proves DBS’s capability of detecting
diverse and subtle reasoning errors and robustness
on different model scales1.
1. Introduction
Machine reasoning has witnessed tremendous progress
thanks to the emergence of Large Language Models (LLMs)
(OpenAI, 2023; Google, 2023a;b; Meta, 2023). The power
of LLMs activates the ability to conduct step-by-step chain-
of-thought (CoT) reasoning (Wei et al., 2022b;a), signifi-
cantly boosting the performance of reasoning tasks (Wang
et al., 2022; Paul et al., 2023; Lyu et al., 2023).
*The first two author contribute equally. Work done during
Tinghui Zhu’s internship at OSU NLP Group. 1School of Com-
puter Science, Fudan University 2The Ohio State University. Cor-
respondence to: Tinghui Zhu <thzhu22@m.fudan.edu.cn>, Kai
Zhang <zhang.13253@osu.edu>, Yu Su <su.809@osu.edu>.
1Code is available at https://github.com/OSU-NLP-Group/
Deductive-Beam-Search.
Question: 
James decides to run 3 sprints 3 times a week. 
He runs 60 meters each sprint.  
How many total meters does he run a week? 
Answer: 
Step 1: He runs 3 times a week, which is equal 
to 3 x 7 = 21 sprints. 
Step 2: The distance he runs each sprint is 60 
meters, so the total distance he runs is 60 x 21 
= 1320 meters. 
Final Answer: 1320
Error Analysis 
Step 1:  
Grounding error.  
7 does not come 
from premises. 
Step 2: 
Accumulative error. 
Misled by errors in 
step 1. 
Figure 1. Example of error in an intermediate step leading to accu-
mulative error from Llama2-7b. The dependency on intermediate
steps introduces accumulative errors in the reasoning process.
Although CoT reasoning has demonstrated the superiority
of step-by-step reasoning, its dependency on intermediate
steps inevitably introduce accumulative errors (Du et al.,
2023; Yu et al., 2023) in the process, as shown in Figure
1. Previous research that alleviates these errors lies in two
main paradigms: 1) Answer aggregation across multiple
rationales. They utilize majority voting (Wang et al., 2022)
or deploy a verifier to score on each rationale (Li et al.,
2023). However, these methods do not directly address
errors in the reasoning process, undermining the reliability
of their outcomes. 2) Intermediate step correction. This
line of works decomposes the reasoning path into reasoning
steps and applies self-correction on each step (Weng et al.,
2023; Ling et al., 2023; Paul et al., 2023; Xie et al., 2023).
Yet, recent research finds that, without external feedback,
LLMs tend to modify reasoning steps regardless of their
correctness (Huang et al., 2023; Hong et al., 2023).
Previous works fail to address reasoning errors in intermedi-
ate steps, compromising the ability to conduct systematic
reasoning. To mitigate this issue, we embrace the principle
of deductive reasoning (Clark, 1969; Johnson-Laird, 1999;
2010) to navigate CoT. In deductive reasoning, every step
logically follows its premises, where a deducible reasoning
step is termed a logical consequence (Dinkmeyer, 1976;
Hanson, 1997). A key attribute of logical consequence is
that if the premises hold, the deducible reasoning step is
true, suggesting a correct outcome. Inspired by this attribute,
we propose to navigate CoT towards a more deducible path.
1
arXiv:2401.17686v2  [cs.CL]  4 Feb 2024

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
The number of 
sprints per week is 
3 x 3 = 9 sprints.
He runs 3 times a 
week, which is 
equal to 3 x 7 = 21 
sprints.
The number of 
sprints per week is 
3 x 3 = 9 sprints.
So the total 
distance he runs 
is 60 x 21=1320 
meters.
So the total distance 
of 9 sprints is 9 x 60 = 
540 meters.
The distance of each 
sprint is 60 meters.
Question:
James decides to 
run 3 sprints 3 
times a week. 
He runs 60 meters 
each sprint. 
How many total 
meters does he run 
a week?
The distance of 
each sprint is 60 
meters.
Therefore, the total 
distance James runs in 
a week is 9 x 60 = 540 
meters.
540
So the total 
distance of 9 
sprints is 9 x 60 
= 540 meters.
540
…
…
Deductive Beam Search
Deductive Score
Low
High
Step 1
Step 2
Step 3
Generation
Question
LLM
The distance of each 
sprint is 60 meters.
So the total distance 
of 9 sprints is 9 x 60 = 
540 meters.
Verification
Contexts
Sampling
Verifier
Premises
Evaluating
Low deductive score!
Aborted
The number of 
sprints per week is 
3 x 3 = 9 sprints.
Question
Figure 2. Overview of Deductive Beam Search. We illustrate the process under the configuration of beam size 2 and sampling times 2.
Nonetheless, challenges arise when introducing the prin-
ciple of deductive reasoning into CoT reasoning without
changing the standard prompt paradigm and the parame-
ters of LLMs. 1) Navigation on CoT reasoning. Since
LLMs cannot always conduct correct deductive reasoning,
they have to explore the potential reasoning space and then
choose reasoning steps that are more likely to be deducible,
which brings the trade-off between exploration and exploita-
tion (Donoso et al., 2014; Dasgupta et al., 2019). 2) Ver-
ification of deducibility. On one hand, previous research
shows it is hard for LLMs to detect reasoning errors (Hong
et al., 2023). On the other hand, symbolic reasoning engines
(Cavada et al., 2014; Li et al., 2018) can reliably verify
the correctness. However, transferring natural language to
symbolic language without losing generality remains an
unsolved problem in machine reasoning.
Confronted with these challenges, we propose Deductive
Beam Search (DBS), adaptable to all models and settings,
as shown Figure 2. For the trade-off challenge, we de-
compose the reasoning process into reasoning steps and
incorporate step-wise beam search. In terms of the verifica-
tion challenge, we propose a deductive verifier, which takes
a reasoning step and its premises as inputs and outputs a
deductive score, evaluating the logical coherence between
them. Specifically, LLM samples a list of potential reason-
ing steps to explore. Then, our deductive verifier exploits
by selecting steps that are more deducible. To train an ef-
fective verifier, we propose a scalable way of synthesizing
fine-grained and diverse deductive reasoning errors without
human annotation. Initially, the verifier is trained to verify
heuristically synthesized wrong steps with typical reasoning
error patterns. Subsequently, we ask LLMs to generate rea-
soning steps where false ones detected by our verifier serve
as hard negatives. These hard negatives are adopted to train
a deductive verifier with model feedback.
As we aim to enable LLMs to decode more deducible reason-
ing paths, DBS can be integrated with answer-aggregation-
based methods. We evaluate our methods across 5 arith-
metic reasoning tasks, 2 commonsense reasoning tasks, and
1 symbolic reasoning task in single chain setting and mul-
tiple chain setting. The improvements can be expected not
only on models of all scales and diverse model families but
also under different settings. Concretely, taking arithmetic
reasoning tasks as an example, the average improvement is
5.3% / 3.2% on Llama2-7b / ChatGPT under single chain
setting and 3.9% / 2.5% under multiple chain setting. More-
over, we comprehensively analyze our verifier, demonstrat-
ing its capability of detecting diverse and subtle reasoning
errors and robustness on different model scales.
2. Deductive Beam Search
We begin by formulating multi-step CoT reasoning with
step-wise beam search before describing DBS. For notation
convenience, we denote [n] to be a set of natural numbers
from 1 to n, and v[n] = [v1, v2, ..., vn] represents the first
n elements of v, where v[0] = [] representing an empty
sequence. Specifically, we denote tokens as y.
2.1. Multi-Step Chain-of-Thought Reasoning
Standard chain-of-thought reasoning (Wei et al., 2022b)
generates the whole reasoning path for the final outcome.
Formally, given the question q, CoT formulates the answer
distribution PrLM(a∣q) as a product of the rationales gen-
eration distribution PrLM(r[t]∣q) and a final answer distri-
bution PrLM(a∣r[t]), which is:
Pr LM(a∣q) = Pr LM(a∣r[t]) × Pr LM(r[t]∣q),
(1)
where r[t] = [r1, r2, ..., rt] is a complete reasoning path,
and t is the number of steps required to complete the rea-
2

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
soning process. Each r = [y1, y2, ...yl] is an intermediate
reasoning step, where l is its token length.
Problems in this setting lie in the complexity of navigat-
ing the generation of r[t], which are sampled as a whole
directly from language models, a process wherein errors can
accumulate (Zhang et al., 2023a). To avoid error accumula-
tion and navigate the reasoning process, we decompose the
process of generating r[t] as:
Pr LM(r[t]∣q) = Pr LM(r1∣q) ×
t−1
∏
i=1
Pr LM(ri+1∣q, r[i])
=
t−1
∏
i=0
Pr LM(ri+1∣q, r[i]).
(2)
As Equation 2 suggested, at timestamp i, the language
model generates the next reasoning step ri based on pre-
vious premises, which is ri ∼PrLM(ri∣q, r[i−1]). This
formulation follows the principle of deductive reasoning.
2.2. Step-wise Beam Search
Under beam size m, traditional beam search decodes
at token level, which stores Top-m candidate tokens,
and uses them for future decoding.
Formally, we de-
note the log-probability of LM generating the k-th to-
kens as ϕ(yk)
=
log PrLM(yk∣y1, y2, ..., yk−1, x)
=
log PrLM(yk∣y[k−1], x), and the log-probability of a so-
lution at timestamp k as Φ(y[k]) = ∑i∈[k] ϕ(yi). Given
a set of m previous solutions at timestamp i as Yi−1 =
{y1
[i−1], y2
[i−1], ..., ym
[i−1]}, beam search generates as:
Y[i] =
arg max
y1
[i],y2
[i],...,ym
[i]
∑
k∈[m]
Φ(yk
[i]).
(3)
However, in reasoning tasks, it is hard to verify whether a
single token is deducible. Thus, we assign a reasoning step r
as the minimal unit in step-wise beam search. Formally, we
denote the log-probability of generating the k-th reasoning
step as ψ(rk) = Φ(y) and the log-probability of a solution
at timestamp k to be Ψ(r[k]) = ∑i∈[k] ψ(ri). Given a
set of m previous solutions at timestamp i as R[i−1] =
{r1
[i−1], r2
[i−1], ..., rm
[i−1]}, step-wise beam search infers as:
R[i] =
arg max
r1
[i],r2
[i],...,rm
[i]
∑
k∈[m]
Ψ(rk
[i]).
(4)
Combining multi-step CoT reasoning with step-wise beam
search balances exploration and exploitation in reasoning
tasks. However, confidence scores from language models
cannot verify logical consequence between a reasoning step
and its premises.
To tackle this problem, we propose to
constrain the step-wise beam search with deductive scores.
2.3. Deductive Verification Constrained Beam Search
To verify the logical coherence between a reasoning step
and its premises, we propose to train a deductive verifier
since LLM itself often fails to detect reasoning errors (Hong
et al., 2023). Formally, given premises c[i] = [c1, c2, ..., ci]
and the candidate reasoning step r, the deductive score can
be formulated as: s = f(c[i], r) = Prf(r∣c1, c2, ..., ci),
where f is the deductive verifier function. The details of
the deductive verifier are illustrated in Section 3. Then, we
utilize the deductive verifier to constrain the step-wise beam
search. To clearly illustrate the process, we show the case
how, given one antecedent solution beam r[i−1] ∈R[i−1] at
timestamp i, reasoning steps are sampled and scored.
In the exploration phase of beam search, the language
model samples a list of potential reasoning steps. Con-
cretely, for sampling times n, the question q and r[i−1] form
the current context c[i] = [q, r[i−1]], and we can sample
a set of n possible reasoning steps ˆRi = {r1, r2, ..., rn},
where r ∼PrLM(r∣q, r[i−1]). Concatenating r ∈ˆRi with
r[i−1] generates candidate reasoning chains set ˆR[i] =
{[r[i−1], r1], [r[i−1], r2], ..., [r[i−1], rn]} at timestamp i.
In terms of exploitation, instead of using the language model
probability PrLM(r∣c[i]) to evaluate these reasoning steps,
deductive verification scores S = {s1, s2, ..., sn} of candi-
date reasoning paths ˆRi are applied. Each score sj, j ∈[n]
is calculated by multiplying the score of r[i−1] and the score
of each candidate reasoning step, that is:
sj = s([r[i−1], rj]) = s(r[i−1]) × Pr f(rj∣q, r[i−1])
=
i
∏
k=1
Pr f(rk∣q, r[k−1]),
(5)
which follows the autoregressive factorization form and
allows us to apply on the beam search algorithm.
Consequently, LM generates n times for each beam at each
step, sampling a total number of m × n candidate reasoning
steps. After scoring on these steps, the top m of them are
selected according to the deductive score. This cycle of
exploration and exploitation repeats until the final answer is
generated or it reaches the upper limit of reasoning length.
3. Deductive Verifier
As stated above, a deductive verifier evaluates whether
the reasoning step can be deduced from previous contexts,
which resembles a natural language inference (NLI) task.
Thus, we use deberta-v3-large (He et al., 2023), which
achieves the best performance across various NLI bench-
marks despite its small size, as the backbone. A small scalar
head is adopted to predict deductive scores based on embed-
ding the [CLS] token.
3

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Table 1. Examples of heuristically synthesized false reasoning steps. The notations correspond to the notations in Section 3.
Context
Type
Not.
Reasoning Step
Error Reason
Randy has some money. He spent
$10 buying his lunch. He spent a
quarter of the money he had left on
an ice cream cone. If the ice cream
cone cost $5, what is the amount of
money, in dollars, Randy had at
first?
Gold
r
Randy has 5*4=20 dollars left after buying
lunch.
-
Grounding
r′
1
Randy has 10*4=40 dollars left after buy-
ing lunch.
Minor token-level error, hard for models to
detect.
Logic
r′
2
At first, Randy had a sum of 20+10=30
dollars.
Logic-level error caused by reversed steps,
not following deductive reasoning.
Irrelevant
r′
3
He eats 65 black cookies from the cookie
jar, with 1/2 * 130 = 65.
Major error, completely incoherent with the
context.
However, the difficulties of training a deductive verifier pri-
marily reside in the training data quality and the training
method. Changing one single token could lead to various er-
rors, which is hard for any model to detect. Furthermore, the
lack of high-quality false deductive reasoning step hinges on
the training of the verifier. To fully understand how LLMs
make mistakes, we dive into the incorrect samples gener-
ated by LLMs. From the perspective of deductive reasoning,
there are two main classes of reasoning errors: grounding
errors and logic errors (Ling et al., 2023). Most grounding
errors happening in the reasoning process can be detected
by finding the contradiction between the context given and
the generated rationales, while the latter ones are illogical
reasoning steps deduced from the previous context.
Thus, we propose a scalable and labor-free data construction
method and a ranking-based training framework to teach
the verifier to detect false reasoning steps. The whole train-
ing is divided into two stages. In stage 1, we heuristically
corrupt gold reasoning steps to simulate typical false rea-
soning and train the verifier to detect them. In stage 2, the
verifier trained from stage 1 is deployed to detect potential
false reasoning steps generated by LLMs, bridging the gap
between synthetic data and real-world data. Consequently,
the model from stage 1 is continue-trained.
3.1. A General Deductive Verifier
In the first stage, we require the verifier to detect two gen-
eral types of reasoning errors: grounding error and logic
error. However, such fine-grained step-wise data is hard to
annotate. Thus, we propose to synthesize false reasoning
steps automatically.
Since it is hard to edit natural language to meet our demands,
we turn to arithmetic reasoning, which can be viewed as
a middle ground between symbols and natural language.
In terms of reasoning steps with grounding errors, we ran-
domly replace one of the numbers on the left side of the
equation in the gold reasoning step with numbers exist-
ing in previous contexts or randomly generated numbers
to simulate false grounding or hallucinations. As for logic
errors, we randomly select reasoning steps after the current
gold reasoning step. Under this circumstance, the reasoning
process is reversed and disrupted, making it a logic error.
Moreover, to enhance the understanding of the model for
this task, we use randomly selected reasoning steps across
the whole dataset as an irrelevant false reasoning step. The
examples of these errors are shown in Table 1.
To provide fine-grained supervision for error detection, we
use margin ranking (Shashua & Levin, 2002) to model the
task. Specifically, given context c, gold reasoning step r,
and three false reasoning steps r′
1, r′
2, and r′
3, respectively
representing grounding error, logic error, and irrelevant rea-
soning step, the verifier f scores all the candidates through
s = f(c, r), which outputs four scores s, s′
1, s′
2, and s′
3.
Then, the loss of ranking these reasoning steps is formulated
as the weighted sum of three margin ranking losses:
L = −
3
∑
i=1
αi × (s −s′
i −mi),
(6)
where mi is the hyper-parameter controlling the margin and
αi is the hyper-parameter weighing each loss.
3.2. Deductive Verifier with Model Feedback
In the first stage, we train a general deductive verifier, but
the wrong samples synthesized heuristically are less diverse
than the ones encountered during inference. To bridge the
gap between synthesized data and real-world data, we use
the verifier from stage 1 to detect false reasoning steps
generated by an actual language model, where we choose
Llama2-7b for the generation. The reason why we choose
a relatively small language model for the generation is to
maximize the diversity and the likelihood of generating
incorrect reasoning steps.
To be concrete, given the verifier f1 trained by stage 1, we
feed current context c into the language model and sample
10 reasoning steps. Then, these reasoning steps are scored
and ranked by f1. From this ranking, we select the reason-
ing step that exhibits the most significant decrease in the
deductive score, designating it as the hard negative sample.
We replace r′
1 with the generated hard negative sample as
r′′
1, keeping the original way of generating r′
2 and r′
3. Con-
sequently, we continue training the verifier f1 by Equation
6 with a smaller learning rate.
4

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Table 2. The result comparison on arithmetic reasoning, commonsense reasoning, and symbolic reasoning tasks. The results represent
accuracy (%) on each dataset. Bold indicates best results and underline indicates second bests. The white area represents single reasoning
chain setting, where greedy represents the greedy decoding algorithm (Jurafsky & Martin). The grey area represents multiple reasoning
chains, where we decode 10 reasoning paths for each method. SC represents self-consistency (Wang et al., 2022).
Method
Arithmetic Reasoning
Commonsense Reasoning
Symbolic Reasoning
GSM8K
SVAMP
AQuA
SingleEq
MultiArith
Avg.↑↓
StrategyQA
CSQA
Avg.↑↓
Coin
Avg.↑↓
Llama2-7b
Greedy
22.0
49.0
3.2
67.5
68.3
+5.3
64.0
66.9
+2.6
53.8
-2.2
DBS
31.2
55.0
5.7
69.0
74.4
66.4
67.0
51.6
SC
28.1
56.7
4.9
77.5
77.8
65.6
67.2
53.0
DBS + SC
32.1
59.3
8.5
78.9
85.6
+3.9
67.6
68.3
+1.6
54.1
+1.1
Llama2-13b
Greedy
35.6
52.3
2.8
72.2
70.6
66.2
53.2
60.2
DBS
43.2
58.0
6.1
76.7
85.6
+7.2
64.6
53.7
-0.6
61.2
+1.0
SC
42.0
68.3
3.6
86.4
91.7
65.4
68.0
61.8
DBS + SC
45.2
72.0
9.3
90.7
94.4
+3.9
66.6
69.8
+1.5
63.4
+1.6
Llama2-70b
Greedy
41.7
51.3
10.1
70.0
70.6
69.8
59.4
71.2
DBS
58.3
61.7
10.1
78.9
90.6
+11.2
70.6
62.4
+1.9
80.4
+8.7
SC
64.8
79.3
10.5
91.3
97.2
74.0
74.0
79.6
DBS + SC
67.6
79.3
14.5
92.7
97.2
+1.6
75.0
73.3
+0.2
80.2
+0.6
ChatGPT
Greedy
68.8
72.0
16.5
95.1
97.2
65.4
65.1
75.1
DBS
75.9
75.7
24.8
92.8
97.8
+3.2
68.6
74.0
+6.2
75.5
+0.4
SC
81.3
81.3
20.2
97.6
98.3
70.6
75.4
78.9
DBS + SC
83.5
82.7
28.8
97.0
99.4
+2.5
69.8
78.3
+1.1
79.5
+0.6
4. Experimental Setup
4.1. Reasoning Tasks
For our evaluation, we choose benchmarks from 3 different
reasoning genres, namely, arithmetic reasoning, common-
sense reasoning, and symbolic reasoning. These 3 types of
reasoning tasks represent diverse reasoning paradigms.
Arithmetic Reasoning. Following Li et al. (2023) and
Ling et al. (2023), we choose GSM8K (Cobbe et al., 2021),
SVAMP (Patel et al., 2021), AQuA (Ling et al., 2017), Sin-
gleEq (Koncel-Kedziorski et al., 2015), and MultiArith (Roy
& Roth, 2016) for evaluation. For AQuA, we evaluate the
accuracy by comparing with the answer of the ground truth.
Commonsense Reasoning. Following Li et al. (2023), we
use CommonsenseQA (Talmor et al., 2019) and StrategyQA
(Geva et al., 2021). CommonsenseQA asks the model to
choose the best answer from 5 choices, and StrategyQA
asks for a True/False answer.
Symbolic Reasoning. We use the Coin Flip dataset from
BIG-bench (bench authors, 2023). The task is to determine
which face of the coin is up after a series of operations.
4.2. Details
Language Models. We evaluate our method on models of
various scales, including Llama2-7b, Llama2-13b, Llama2-
70b (Meta, 2023), and ChatGPT (gpt-3.5-turbo-instruct)
(OpenAI, 2022). These models represent different levels of
reasoning abilities.
Prompts. For arithmetic reasoning tasks, we apply one
prompt to all tasks. For commonsense reasoning tasks and
the symbolic reasoning task, we write a prompt for each task
to ensure the model can output the correct answer format.
All methods are evaluated by the same prompt on each task.
The details of prompts are in Appendix B.3.
Verifier. We choose deberta-v3-large as the backbone of
our verifier. At stage 1, the general verifier is trained on
22,363 samples generated from the GSM8K dataset. At
stage 2, the verifier for arithmetic and symbolic reasoning is
trained on 150,000 samples generated from MetaMathQA,
while the verifier for commonsense reasoning is trained on
5,000 samples generated from StrategyQA. The training
details are in Appendix B.1 and B.2.
Inference. During inference, we set beam size m to 5 and
sampling times n to 10. For all models and baselines, we
use their default parameter settings for generation.
5. Main Result
Table 2 demonstrates the overall performance of the meth-
ods. We compare DBS with baselines under two paradigms:
single reasoning chain setting and multiple reasoning chain
setting. In multiple reasoning chain setting, the generated
outcomes are integrated with self-consistency.
5

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Table 3. Comparison of different decoding methods and evaluation
methods. We test all the listed methods on 100 randomly sampled
arithmetic reasoning questions from the GSM8K dataset. We
report accuracy, output tokens (including evaluation tokens used by
other methods), and the number of rationales for self-consistency.
Model
Method
Accu
Tokens
#Rationales
Llama2-7b
SelfEval
24
5,371,280
1
SelfEval + SC
22
10
DBS
38
297,301
1
DBS + SC
43
10
ChatGPT
SelfEval
70
1,854,147
1
SelfEval + SC
74
10
DBS
81
474,221
1
DBS + SC
85
10
Table 3 presents a comparative analysis of our approach
against other decoding strategies through a self-evaluate
pattern (Xie et al., 2023). Since the self-evaluate pattern
is token-expensive, we only evaluate the methods on 100
randomly sampled questions from the GSM8K dataset on
Llama2-7b and GPT-3.5, which are two typical examples of
models with distinct levels of abilities.
5.1. Effectiveness
As shown in Table 2, DBS improves the performance across
models of different scales and diverse reasoning tasks. For
the single reasoning chain setting, the improvement is sub-
stantial. On arithmetic reasoning tasks, taking GSM8K as
an example, we observe an increase from 7.6% to 16.6%
across models of various scales. Specifically, with Llama2-
7b and gpt-3.5-turbo-instruct, DBS yields improvements of
9.2% and 7.0%, respectively, affirming the effectiveness of
our proposed decoding strategy. On commonsense reason-
ing tasks and symbolic reasoning tasks, we can expect an
average increase of 2.5%/2.0% on models of all scales.
Regarding the multiple reasoning chain setting, DBS out-
performs naive self-consistency. Concretely, we can see
an average of 3.0% improvement on arithmetic reasoning
tasks, 0.5% on commonsense reasoning tasks, and 1.0% on
symbolic reasoning tasks, respectively. On the SingleEq
and StrategyQA datasets, the performance of DBS on Chat-
GPT is slightly lower (-0.6%/-0.8%). These datasets require
fewer reasoning steps for the final answer, as opposed to
our paradigm of multiple reasoning steps. Nevertheless, the
universal improvements demonstrate the effectiveness of
our proposed method.
5.2. Comparison with Decoding Strategies
Table 3 compares our decoding strategy with previous SOTA
decoding strategies. The comparative results, grounded in
accuracy and token cost metrics, substantiate our approach’s
effectiveness and token efficiency. Notably, the self-evaluate
pattern consumes an excessive amount of tokens during
Table 4. Verification ability comparison of our verifier and Chat-
GPT. We ask both models to rank among the gold reasoning steps
and the synthesized reasoning steps. The evaluation focuses on
determining the position these models assigned to the gold steps.
Method
MRR
HITS@1
HITS@3
HITS@5
ChatGPT
0.48
0.32
0.49
0.67
Our Verifier
0.59
0.41
0.69
0.86
evaluation. In contrast, DBS enhances the performance by
approximately 10% on both Llama2-7b and gpt-3.5-turbo-
instruct, with fewer tokens generated across the paradigms
of single reasoning chain and multiple reasoning chains.
6. Analysis
We conduct a detailed analysis to investigate the verifiability
and efficiency of our proposed method. Moreover, we show
how our method can adapt to different settings.
6.1. Verifier Analysis
We comprehensively analyze the verifiability of our pro-
posed verifier. We analyze performance (Table 4), score
distribution (Figure 3), and robustness on different model
scales (Figure 4).
Empirical Study. For empirical experiments, we test how
our verifier will score the gold reasoning steps compared
to synthesized reasoning steps. We randomly sample 500
context-step pairs from the test set of the GSM8K dataset
as gold reasoning steps. For each pair, we synthesize nine
reasoning steps using Llama2-7b as inferior reasoning steps.
The verifier’s task is to rank them, and the performance is
evaluated by four metrics, namely, Mean Reciprocal Rank
(MRR), HITS@1/3/5. MRR evaluates the average rank
of the gold reasoning step, and the HITS metrics reflect
whether the gold reasoning step will be chosen under beam
size setting 1/3/5. To compare with the self-evaluation pat-
tern, we ask ChatGPT to rank these reasoning steps rather
than predict scores, leveraging its inherent reranking capabil-
ities (Ma et al., 2023). The results are listed in Table 4. Our
verifier outperforms ChatGPT across all metrics, evidencing
its capability. Notably, our verifier correctly identifies 86%
of the gold reasoning steps within the top 5 positions out
of 10 samples, affirming the deducibility of the reasoning
paths decoded under m = 5, n = 10.
Distribution Analysis. To ascertain the reliability of our
verifier, we compare the score distributions for correct and
wrong predictions between the original LM confidence (gpt-
3.5-turbo-instruct) and our deductive verification score. We
use results from greedy decoding, which naturally produces
confidence scores from LM, and ask the verifier to score on
them. Figure 3 shows the substantial difference between an
LM confidence score and our deductive score. Notably, the
6

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Wrong: 0.75
All: 0.78
Correct: 0.79
(a) LM distribution
Wrong: 0.41
All: 0.52
Correct: 0.58
(b) Verifier distribution
Figure 3. Distributions of language model and verifier scores on
correct/incorrect reasoning paths. The dashed lines represent the
median. Our verifier differentiates the scores attributed to correct
and incorrect reasoning paths.
0.0
0.2
0.4
0.6
0.8
Deductive Score
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Accuracy
ChatGPT
Llama2-7b
Figure 4. Accuracy under different deductive score thresholds on
greedy decoding results from Llama2-7b and ChatGPT. The dashed
line shows the polynomial fitting the scattered data. The near-
parallel of these lines suggests the robustness of our verifier’s
performance.
LM confidence score demonstrates a mere 4% increase in
scores of the correct reasoning paths, whereas our verifier
exhibits a 17% increase. This significant difference proves
the enhanced verifiability of our verification approach.
Robustness. To ensure that our verifier can equally verify
reasoning steps generated by different models, we visualize
the accuracy under different deductive score thresholds for
Llama2-7b and ChatGPT, as depicted in Figure 4. The lines
in the figure represent polynomial fits of the data. Their
near-parallel alignment suggests the robustness of perfor-
mance improvement across these models as the threshold
increases. Intriguingly, these lines also offer insights into
the inductive reasoning capabilities of the two models. Ide-
ally, the accuracy approaches zero at the deductive score
zero. However, the observed non-zero accuracy suggests the
models’ inductive reasoning capabilities. Although the rea-
soning process might not align with the deductive reasoning
paradigm, the models can still arrive at correct conclusions,
likely by intuitively skipping over specific reasoning steps,
which is the act of inductive reasoning.
Table 5. Cost Analysis of our method and other baseline methods.
We report output tokens for each method. We use Llama2-7b as
the backbone for all methods. n represents sampling times of the
method, and m represents beam size.
Method
Accuracy
Tokens
Greedy
21.99
267,462
Self-consistency, n = 10
28.05
1,984,921
DBS , m = 1, n = 10
29.34
1,414,435
DBS , m = 5, n = 10
31.16
4,042,053
Table 6. Different prompt settings on commonsense reasoning
tasks. We use Llama2-7b as the backbone for all methods. For
methods with multiple reasoning chains, we take 10 reasoning
paths for self-consistency.
Method
StrategyQA
CSQA
Greedy
64.00
66.91
DBS
66.40
66.99
- w. recall prompt
65.40
66.67
SC
65.60
67.24
DBS
67.60
68.29
- w. recall prompt
65.80
69.45
6.2. Cost Analysis
The cost of sampling multiple times is enormous. We an-
alyze the cost of our methods under different settings and
compare them with the baselines. Specifically, we compare
our approach with greedy decoding, self-consistency, and
self-evaluated decoding strategies (Xie et al., 2023). The
results are presented in Table 3 and Table 5. Our analysis
reveals that greedy decoding is the most token-economic,
as it does not involve any form of sampling, but its per-
formance lags. When m is constrained to 1, the token
generation is minimized even further than that required by
self-consistency strategies. Still, our method demonstrates
higher accuracy. Moreover, DBS proves more effective
and token-efficient under the same beam size than those
leveraging LLMs’ self-evaluation capabilities.
6.3. Commonsense Reasoning Task
Unlike arithmetic or symbolic reasoning tasks, common-
sense reasoning tasks leverage the model’s intrinsic com-
monsense. Thus, we introduce a recall prompt, which asks
the model to recall the necessary commonsense via greedy
decoding, followed by DBS navigating the reasoning over
the elicited contexts. We compare this setting to standard
settings and baselines, and the results are shown in Table
6. The performance varies from different tasks, attributed
to the distinct nature of the tasks. Concretely, StrategyQA
typically requires a 2-3 step knowledge recall followed by a
single reasoning step, whereas CSQA demands a more ex-
tensive 3-5 step recall process alongside multiple reasoning
steps. Our findings suggest that the recall prompt is more
suitable for tasks demanding comprehensive commonsense
and multi-step reasoning.
7

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Table 7. Two GSM8K examples from Llama2-7b scored with our verifier.
Question
Greedy
Ours
James decides to run 3
sprints 3 times a week. He
runs 60 meters each sprint.
How many total meters does
he run a week?
He runs 3 times a week, which is equal to 3
x 7 = 21 sprints. 0.0102 The distance he
runs each sprint is 60 meters, so the total
distance he runs is 60 x 21 = 1320 meters.
0.4963 Final Answer:1320
Each sprint is 60 meters. 0.8270 So he runs 60 x 3 = 180 meters each
day. 0.9643 He runs each sprint 3 times a week. 0.9983 Therefore, he
runs a total of 180 x 3 = 540 meters per week. 0.9463 Final Answer:540
Toulouse
has
twice
as
many sheep as Charleston.
Charleston has 4 times as
many sheep as Seattle. How
many sheep do Toulouse,
Charleston, and Seattle have
together if Seattle has 20
sheep?
Toulouse has twice as many sheep as
Charleston, so Toulouse has 2 x 4 = 8 sheep.
0.3149 Charleston has 4 times as many
sheep as Seattle, so Charleston has 4 x 20 =
80 sheep. 0.9877 So Toulouse has 8 + 80
= 88 sheep. 0.3923 Seattle has 20 sheep.
0.7479 Final Answer:88 + 20 + 88 = 196
Let’s first find out how many sheep Charleston has. 0.9447 Charleston
has 4 times as many sheep as Seattle. Seattle has 20 sheep. Charleston
has 20 x 4 = 80 sheep.
0.9912 Now, let’s find out how many
sheep Toulouse has. Toulouse has twice as many sheep as Charleston.
Charleston has 80 sheep. Toulouse has 80 x 2 = 160 sheep. 0.9736
Finally, let’s find the total number of sheep Toulouse, Charleston, and
Seattle have together. Add the number of sheep Toulouse, Charleston,
and Seattle have. 0.9981 Toulouse has 160 sheep, Charleston has 80
sheep, and Seattle has 20 sheep. So the total number of sheep they have
together is 160 + 80 + 20 = 260 sheep. 0.9924 Final Answer:260
6.4. Case Study
Table 7 presents two GSM8K examples from Llama2-7b.
The first example demonstrates a scenario where halluci-
nation emerges under greedy decoding, which our verifier
identifies with an extremely low score, thereby precluding
its selection by our deductive decoding strategy. On the
contrary, every reasoning step from our reasoning path is de-
duced from the previous context and is scored much higher
than the incorrect steps. In the second example, a ground-
ing error occurs in steps marked by red scores. Although
the wrong reasoning steps resemble the correct ones, our
verifier detects these minor errors. In both examples, the
reasoning path generated by our decoding strategy initiates
the reasoning by listing premises, followed by one reasoning
step. This pattern strictly follows the principle of deductive
reasoning, making the generated results more deducible.
7. Related Work
Answer Aggregation. Sampling techniques of language
models, such as temperature sampling (Ackley et al., 1985),
top-k sampling (Fan et al., 2018), and top-p sampling (Holtz-
man et al., 2019), bring diversity to the outcome but also
uncertainty to the reasoning process, which is not favored in
reasoning tasks. These methods aim to reduce uncertainty
in the reasoning process by aggregating answers from sam-
pled reasoning paths. After sampling diverse outputs from
LLMs, Wang et al. (2022) propose to use consistency as the
metric to aggregate the answers. Other methods evaluate
whether the reasoning step can lead to the correct answer by
training a verifier (Li et al., 2023; Wang et al., 2023).
Self-Evaluation. Recent research on reducing reasoning
errors is inclined to follow the self-verify-then-correct pat-
tern (Dhuliawala et al., 2023; Weng et al., 2023; Zhang
et al., 2023b; Ling et al., 2023; Miao et al., 2023). They
design different procedures and prompts to achieve better
performance. Taking two typical approaches as examples,
Dhuliawala et al. (2023) design a chain-of-verification proce-
dure to verify facts from its outputs, and Miao et al. (2023)
ask LLM to detect errors in their step-by-step rationales.
However, recent works (Huang et al., 2023; Hong et al.,
2023) have pinpointed a critical limitation of LLMs in self-
correction during reasoning tasks. Their findings suggest
that LLMs indiscriminately alter reasoning steps without
external feedback, irrespective of their initial accuracy.
Decoding Strategies. Conventional decoding strategies in-
clude greedy decoding (Teller, 2000), which selects tokens
with the highest probabilities, and beam search (Graves,
2012), which stores candidate beams for future prediction.
In the era of LLMs, these decoding strategies are imple-
mented at a more coarse-grained level, especially on rea-
soning tasks involving multiple steps in the solving process.
They decompose the reasoning process into steps (Khot
et al., 2022) and apply decoding or search algorithms (Yao
et al., 2023; Xie et al., 2023).
8. Conclusions
In this paper, we aim to eliminate errors in intermediate
reasoning steps in CoT reasoning. To this end, we propose
Deductive Beam Search that integrates CoT with step-wise
beam search and scores each reasoning step with a deduc-
tive verifier, which verifies whether the reasoning step is a
logical consequence. Beam search explores by sampling
potential reasoning steps, while the verifier exploits by se-
lecting the most deducible steps. To train such a verifier, we
propose a scalable and labor-free data construction method.
It initiates by heuristically introducing errors into gold rea-
soning steps and enhances the diversity and difficulty of
training data by synthesizing hard negatives through the
verifier trained on those typical wrong steps. Extensive ex-
periments show our method’s effectiveness across various
model scales and diverse reasoning tasks without chang-
ing the standard CoT paradigm and parameters of LLMs.
Further analysis proves the verifiability and robustness en-
dowed by our verifier, thereby significantly improving the
deducibility of the generated reasoning paths.
8

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
References
Ackley, D. H., Hinton, G. E., and Sejnowski, T. J. A learning
algorithm for boltzmann machines. Cognitive science, 9
(1):147–169, 1985.
bench authors, B. Beyond the imitation game: Quantifying
and extrapolating the capabilities of language models.
Transactions on Machine Learning Research, 2023. ISSN
2835-8856.
Cavada, R., Cimatti, A., Dorigatti, M., Griggio, A., Mariotti,
A., Micheli, A., Mover, S., Roveri, M., and Tonetta, S.
The nuxmv symbolic model checker. In Computer Aided
Verification: 26th International Conference, CAV 2014,
Held as Part of the Vienna Summer of Logic, VSL 2014,
Vienna, Austria, July 18-22, 2014. Proceedings 26, pp.
334–342. Springer, 2014.
Clark, H. H. Linguistic processes in deductive reasoning.
Psychological review, 76(4):387, 1969.
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,
R., Hesse, C., and Schulman, J. Training verifiers to solve
math word problems, 2021.
Dasgupta, I., Wang, J., Chiappa, S., Mitrovic, J., Ortega,
P., Raposo, D., Hughes, E., Battaglia, P., Botvinick,
M., and Kurth-Nelson, Z. Causal reasoning from meta-
reinforcement learning. arXiv preprint arXiv:1901.08162,
2019.
Dhuliawala, S., Komeili, M., Xu, J., Raileanu, R., Li, X.,
Celikyilmaz, A., and Weston, J. Chain-of-verification
reduces hallucination in large language models, 2023.
Dinkmeyer, D. Logical consequences: A key to the re-
duction of disciplinary problems. The Phi Delta Kap-
pan, 57(10):664–666, 1976.
ISSN 00317217.
URL
http://www.jstor.org/stable/20298430.
Donoso, M., Collins, A. G., and Koechlin, E. Foundations
of human reasoning in the prefrontal cortex. Science, 344
(6191):1481–1486, 2014.
Du, J., Jiang, Y., Tan, V. Y., Zhou, J. T., and Li, H. Minimiz-
ing the accumulated trajectory error to improve dataset
distillation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pp.
3749–3758, 2023.
Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neu-
ral story generation.
In Proceedings of the 56th An-
nual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pp. 889–898, Mel-
bourne, Australia, July 2018. Association for Computa-
tional Linguistics. doi: 10.18653/v1/P18-1082. URL
https://aclanthology.org/P18-1082.
Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and
Berant, J. Did aristotle use a laptop? a question an-
swering benchmark with implicit reasoning strategies.
Transactions of the Association for Computational Lin-
guistics, 9:346–361, 2021. doi: 10.1162/tacl a 00370.
URL https://aclanthology.org/2021.tacl-1.21.
Google. Palm: Scaling language modeling with pathways.
Journal of Machine Learning Research, 24(240):1–113,
2023a. URL http://jmlr.org/papers/v24/22-1144.
html.
Google. Palm 2 technical report, 2023b.
Graves, A. Sequence transduction with recurrent neural
networks. arXiv preprint arXiv:1211.3711, 2012.
Hanson, W. H. The concept of logical consequence. The
Philosophical Review, 106(3):365–409, 1997.
ISSN
00318108, 15581470. URL http://www.jstor.org/
stable/2998398.
He, P., Gao, J., and Chen, W.
Debertav3: Improving
deberta using electra-style pre-training with gradient-
disentangled embedding sharing, 2023.
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The
curious case of neural text degeneration. In International
Conference on Learning Representations, 2019.
Hong, R., Zhang, H., Pang, X., Yu, D., and Zhang, C. A
closer look at the self-verification abilities of large lan-
guage models in logical reasoning, 2023.
Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W.,
Song, X., and Zhou, D. Large language models cannot
self-correct reasoning yet, 2023.
Johnson-Laird, P. Deductive reasoning. WIREs Cognitive
Science, 1(1):8–17, 2010. doi: https://doi.org/10.1002/
wcs.20.
Johnson-Laird, P. N. Deductive reasoning. Annual Review of
Psychology, 50(1):109–135, 1999. doi: 10.1146/annurev.
psych.50.1.109. PMID: 15012459.
Jurafsky, D. and Martin, J. H. Speech and language pro-
cessing: An introduction to natural language processing,
computational linguistics, and speech recognition.
Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K.,
Clark, P., and Sabharwal, A. Decomposed prompting:
A modular approach for solving complex tasks. arXiv
preprint arXiv:2210.02406, 2022.
Koncel-Kedziorski, R., Hajishirzi, H., Sabharwal, A., Et-
zioni, O., and Ang, S. D. Parsing algebraic word prob-
lems into equations. Transactions of the Association for
Computational Linguistics, 3:585–597, 2015.
9

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Li, S., Xu, H., and Lu, Z. Generalize symbolic knowledge
with neural rule engine. arXiv preprint arXiv:1808.10326,
2018.
Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and
Chen, W. Making language models better reasoners with
step-aware verifier. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pp. 5315–5333, Toronto,
Canada, July 2023. Association for Computational Lin-
guistics.
doi: 10.18653/v1/2023.acl-long.291.
URL
https://aclanthology.org/2023.acl-long.291.
Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program
induction by rationale generation: Learning to solve and
explain algebraic word problems. In Proceedings of the
55th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pp. 158–167,
Vancouver, Canada, July 2017. Association for Compu-
tational Linguistics. doi: 10.18653/v1/P17-1015. URL
https://aclanthology.org/P17-1015.
Ling, Z., Fang, Y., Li, X., Huang, Z., Lee, M., Memisevic,
R., and Su, H. Deductive verification of chain-of-thought
reasoning, 2023.
Lyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong,
E., Apidianaki, M., and Callison-Burch, C. Faithful chain-
of-thought reasoning, 2023.
Ma, Y., Cao, Y., Hong, Y., and Sun, A. Large language
model is not a good few-shot information extractor, but
a good reranker for hard samples!
In Bouamor, H.,
Pino, J., and Bali, K. (eds.), Findings of the Associa-
tion for Computational Linguistics: EMNLP 2023, pp.
10572–10601, Singapore, December 2023. Association
for Computational Linguistics. doi: 10.18653/v1/2023.
findings-emnlp.710.
Meta. Llama 2: Open foundation and fine-tuned chat mod-
els, 2023.
Miao, N., Teh, Y. W., and Rainforth, T. Selfcheck: Using
llms to zero-shot check their own step-by-step reasoning,
2023.
OpenAI.
Chatgpt, 2022.
URL https://openai.com/
blog/chatgpt.
OpenAI. Gpt-4 technical report, 2023.
Patel, A., Bhattamishra, S., and Goyal, N. Are NLP models
really able to solve simple math word problems? In Pro-
ceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pp. 2080–2094,
Online, June 2021. Association for Computational Lin-
guistics. doi: 10.18653/v1/2021.naacl-main.168. URL
https://aclanthology.org/2021.naacl-main.168.
Paul, D., Ismayilzada, M., Peyrard, M., Borges, B., Bosse-
lut, A., West, R., and Faltings, B. Refiner: Reasoning
feedback on intermediate representations, 2023.
Roy, S. and Roth, D.
Solving general arithmetic word
problems. arXiv preprint arXiv:1608.01413, 2016.
Shashua, A. and Levin, A. Ranking with large margin prin-
ciple: Two approaches. Advances in neural information
processing systems, 15, 2002.
Talmor, A., Herzig, J., Lourie, N., and Berant, J. Common-
senseQA: A question answering challenge targeting com-
monsense knowledge. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers), pp. 4149–4158,
Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics. doi: 10.18653/v1/N19-1421.
URL https://aclanthology.org/N19-1421.
Teller, V. Speech and language processing: an introduction
to natural language processing, computational linguistics,
and speech recognition, 2000.
Wang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y., Chen,
D., Wu, Y., and Sui, Z.
Math-shepherd: Verify and
reinforce llms step-by-step without human annotations,
2023.
Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi,
E. H., Narang, S., Chowdhery, A., and Zhou, D. Self-
consistency improves chain of thought reasoning in lan-
guage models. In The Eleventh International Conference
on Learning Representations, 2022.
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,
Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-
zler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,
P., Dean, J., and Fedus, W. Emergent abilities of large
language models, 2022a.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,
Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought
prompting elicits reasoning in large language models.
Advances in Neural Information Processing Systems, 35:
24824–24837, 2022b.
Weng, Y., Zhu, M., Xia, F., Li, B., He, S., Liu, S., Sun, B.,
Liu, K., and Zhao, J. Large language models are better
reasoners with self-verification, 2023.
Xie, Y., Kawaguchi, K., Zhao, Y., Zhao, X., Kan, M.-Y.,
He, J., and Xie, Q. Self-evaluation guided beam search
for reasoning. In Thirty-seventh Conference on Neural
Information Processing Systems, 2023.
10

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y.,
and Narasimhan, K. Tree of thoughts: Deliberate prob-
lem solving with large language models. arXiv preprint
arXiv:2305.10601, 2023.
Yu, J., He, R., and Ying, R. Thought propagation: An
analogical approach to complex reasoning with large lan-
guage models. arXiv preprint arXiv:2310.03965, 2023.
Zhang, M., Press, O., Merrill, W., Liu, A., and Smith, N. A.
How language model hallucinations can snowball, 2023a.
Zhang, Y., Yang, J., Yuan, Y., and Yao, A. C.-C. Cumulative
reasoning with large language models, 2023b.
11

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
1
2
3
4
5
Beam Size
22
24
26
28
30
32
Accuracy
DBS-single
DBS-multiple
Greedy
Figure 5. Accuracy under different beam sizes on different models.
A. Extended Experiments
Ablation study on beam size.
We conduct experiments
on how beam size affects performance. Figure 5 shows
the trend of DBS performance under single chain setting
and multiple chain setting. The performance steadily grows
when the beam size rises.
Comparison with more baselines.
We compare more
baselines on GSM8K, using Llama2-7b as the backbone.
All methods are implemented with the same prompt for
reasoning step generation. Our method outperforms this
baseline on both single chain setting and multiple chain
setting. Notably, these methods utilize self-evaluation, in-
dicating that the self-evaluation capability is weak on a
relatively small language model.
B. Experimental Details
B.1. Training Data
At stage 1, we choose GSM8K dataset to train the general
deductive verifier. The gold rationales provided are decom-
posed into sentences as gold reasoning steps. After filtering
out some steps that cannot be altered into false reasoning
steps, we construct a training dataset of 22,362 samples. At
stage 2, the verifier from stage 1 is used to generate hard
negative reasoning steps as stated in Sec. 3. We choose
Llama2-7b as our language model to generate candidate
false reasoning steps. For arithmetic reasoning and sym-
bolic reasoning tasks, we use the MetaMathQA dataset to
generate training data. For commonsense reasoning tasks,
we use the StrategyQA dataset to generate training data.
Finally, we train the arithmetic verifier on 150,000 samples
and the commonsense verifier on 5,000 samples.
B.2. Training Details
At stage 1, we finetune deberta-v3-large with learning rate
1 × 10−5 and batch size 128. As for margins, we set the
Table 8. Comparison of baselines focusing on correcting intermedi-
ate errors. DV represent Ling et al. (2023), and SelfEval represents
Xie et al. (2023).
Method
Accu
#Rationales
Greedy
21.99
1
DV
10.46
1
SelfEval
22.76
1
Ours
31.16
1
SC
28.05
10
SelfEval + SC
24.18
10
Ours + SC
32.07
10
margins between the gold reasoning step and grounding
error step/logic error step/irrelevant step to 0.3/0.6/0.9. At
stage 2, we continue to finetune the verifier from stage 1 with
learning rate 1 × 10−6 and batch size 128. As for margins,
we set the margins between the gold reasoning step and hard
negative reasoning step/logic error step/irrelevant step to
0.3/0.6/0.9.
B.3. Prompts
For the results in Table 2, we use the following prompts:
• Arithmetic reasoning tasks share the same prompt,
shown in Table 9.
• For StrategyQA, we use the prompt in Table 10.
• For CSQA, we use the prompt in Table 11.
• For Coin, we use the prompt in Table 12.
For the results in Table 6, we use the following prompts:
• For StrategyQA, we use prompt in Table 13.
• For CSQA, we use prompt in Table 14.
12

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Table 9. Prompt for arithmetic reasoning tasks.
Given the question, please give the rationales step by step and give a final answer.
Example 1:
Question:
Kate’s hair is half as long as Emily’s hair.
Emily’s hair is 6 inches longer than Logan’s hair.
If Logan hair is 20 inches, how many inches is Kate’s hair?
Answer:
Emily’s hair is 20-6 = 14 inches long.
Kate’s hair 14/2= 7 inches long.
Final Answer:7
Example 2:
Question:
John puts $25 in his piggy bank every month for 2 years to save up for a vacation.
He had to spend $400 from his piggy bank savings last week to repair his car.
How many dollars are left in his piggy bank?
Answer:
He saved money for 2 years, which is equal to 12 x 2 = 24 months.
The amount of money he saved is $25*24 = $600.
But he spent some money so there is $600 - $400 = 200 left.
Final Answer:200
Table 10. Prompt for StrategyQA.
Given the question, output the rationale step by step and give the final answer (yes or no).
Example 1
Question:
Do hamsters provide food for any animals?
Answer:
Hamsters are prey animals.
Prey are food for predators.
Final answer: yes
Example 2
Question:
Could a llama birth twice during War in Vietnam (1945-46)?
Answer:
The War in Vietnam was 6 months.
The gestation period for a llama is 11 months, which is more than 6 months.
Final answer: no
13

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Table 11. Prompt for CSQA.
Given the question, output the rationale step by step and give the final answer.
You should choose the best answer.
Example 1
Question:
Sammy wanted to go to where the people were. Where might he go?
A. race track
B. populated area
C. the desert
D. apartment
E. roadblock
Answer:
Sammy wanted to go to places with many people.
Race track and apartment do not have many people.
The desert and roadblock have few people.
And, the populated area means that it is the place with many people.
Thus, Sammy should go to populated area.
Final Answer: B
Example 2
Question:
The fox walked from the city into the forest, what was it looking for?
A. pretty flowers
B. hen house
C. natural habitat
D. storybook
E. dense forest
Answer:
The forest does not have hen house or storybook.
The fox is a carnivore that does not look for flowers and forest.
The forest is a natural habitat for foxes.
Thus, it was looking for a natural habitat.
Final Answer: C
14

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Table 12. Prompt for Coin.
Given the question, output the rationale step by step and give the final answer.
Example 1
Question:
A coin is heads up. sager does not flip the coin. zyheir flips the coin.
Is the coin still heads up?
Answer:
sager does not flip the coin, so the coin is heads up.
zyheir flips the coins, so the coin is tails up.
Final Answer: no
Example 2
Question:
A coin is heads up. mailey does not flip the coin. maurisa does not flip the coin.
Is the coin still heads up?
Answer:
mailye does not flip the coin, so the coin is heads up.
maurisa does not flip the coin, so the coin is heads up.
Final Answer: yes
Table 13. Prompt for StrategyQA with prompt.
Given the question, output the rationale step by step and give the final answer (yes or no).
Example 1
Question:
Do hamsters provide food for any animals?
Answer:
Fact:
Hamsters are prey animals.
Prey are food for predators.
Reasoning:
Hamsters are food for some predators.
Final answer: yes
Example 2
Question:
Could a llama birth twice during War in Vietnam (1945-46)?
Answer:
Fact:
The War in Vietnam was 6 months.
The gestation period for a llama is 11 months, which is more than 6 months.
Reasoning:
A llama could not birth twice during War in Vietnam.
Final answer: no
15

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
Table 14. Prompt for CSQA with recalling commonsense first.
Given the question, output the rationale step by step and give the final answer.
You should choose the best answer.
Example 1
Question:
Sammy wanted to go to where the people were. Where might he go?
A. race track
B. populated area
C. the desert
D. apartment
E. roadblock
Answer:
Fact:
Sammy wanted to go to places with many people.
Race track and apartment do not have many people.
The desert and roadblock have few people.
And, the populated area means that it is the place with many people.
Reasoning:
Thus, Sammy should go to populated area.
Final Answer: B
Example 2
Question:
The fox walked from the city into the forest, what was it looking for?
A. pretty flowers
B. hen house
C. natural habitat
D. storybook
E. dense forest
Answer:
Fact:
The forest does not have hen house or storybook.
The fox is a carnivore that does not look for flowers and forest.
The forest is a natural habitat for foxes.
Reasoning:
Thus, it was looking for a natural habitat.
Final Answer: C
16

