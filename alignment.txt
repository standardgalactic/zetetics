Technical Report
The Alignment Problem for Bayesian
History-Based Reinforcement Learners∗
Tom Everitt1,2
tom.everitt@anu.edu.au
Marcus Hutter2
marcus.hutter@anu.edu.au
June 22, 2018
1DeepMind, 2Australian National University
Abstract: Value alignment is often considered a critical component of safe
artiﬁcial intelligence. Meanwhile, reinforcement learning is often criticized
as being inherently unsafe and misaligned, for reasons such as wireheading,
delusion boxes, misspeciﬁed reward functions and distributional shifts. In
this report, we categorize sources of misalignment for reinforcement learn-
ing agents, illustrating each type with numerous examples. For each type
of problem, we also describe ways to remove the source of misalignment.
Combined, the suggestions form high-level blueprints for how to design value
aligned RL agents.
Keywords: AGI safety, reinforcement learning, Bayesian learning, causal
graphs
Note: This report is superseded by Tom Everitt and Marcus Hutter (2019).
Reward Tampering Problems and Solutions in Reinforcement Learning: A
Causal Inﬂuence Diagram Perspective. arXiv: 1908.04734.
∗We are indebted to a number of people for helping us develop this report. Toby Ord had the initial
idea to structure diﬀerent types of “wireheading”. Pedro Ortega provided invaluable help with the
causal graphs and made the suggestion to develop an abstract method. Laurent Orseau contributed
through many discussions. Victoria Krakovna, Michael Cohen, Rohin Shah, and Jan Leike all read
drafts and came with valuable feedback.
1

Contents
1. Introduction
4
2. Formalizing Goal Alignment
6
2.1. POMDP Base . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.2. Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.3. Modeling Embedded Agents . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.4. Deﬁning Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.5. Method
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3. Preprogrammed Reward Function
15
3.1. Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.2. Misalignment Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
3.3. Simulation Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.4. Self-Corruption Awareness . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.5. Action-Observation Grounding
. . . . . . . . . . . . . . . . . . . . . . . .
21
3.6. Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
4. Human as External Reward Function
23
4.1. Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
4.2. Misalignment Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
4.3. Tools and Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
5. Interactively Learning a Reward Function
27
5.1. Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
5.2. Misalignment Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
5.3. Applicability of Previous Tools . . . . . . . . . . . . . . . . . . . . . . . .
30
5.4. Data corruption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
5.5. Decoupled Reward Data . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
5.6. Reward Function Deﬁnitions
. . . . . . . . . . . . . . . . . . . . . . . . .
35
5.7. Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
6. Conclusions
43
Bibliography
44
A. Causal Graphs
48
A.1. Representing Causal Graphs . . . . . . . . . . . . . . . . . . . . . . . . . .
48
A.2. Representing Uncertainty in Causal Graphs . . . . . . . . . . . . . . . . .
50
2

A.3. Focusing on a Part of a Causal Graph . . . . . . . . . . . . . . . . . . . .
50
A.4. Environment Mixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
B. Full Graphs
53
C. Formal Results
59
C.1. Preprogrammed Reward . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
C.2. Human Reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
C.3. Interactive Reward Learning . . . . . . . . . . . . . . . . . . . . . . . . . .
62
3

1. Introduction
Artiﬁcial Intelligence.
An artiﬁcial intelligence (AI) program may broadly be deﬁned
as a computer program that automatically ﬁnds ways to achieve goals. For example, an
AI chess program ﬁnds moves for achieving the goal of checkmating the opponent. The
moves have not been explicitly deﬁned by the programmer – instead the program ﬁnds
them by “weighing” the pros and cons of diﬀerent strategies. An AI question-answering
system automatically generates the answer to a query by searching a knowledge base.
A self-driving car ﬁnds a way to control throttle, break, and steering, in order to get to
its goal destination.
An important distinction between diﬀerent AI systems is generality. Most present-
day AIs can only solve narrow sets of tasks in restricted environments. For example,
most chess programs can only play chess, and most question answering systems can only
answer speciﬁc type of queries from a particular kind of database. This is in contrast to
humans, who can learn to perform a wide range of cognitive tasks.
However, a trend towards greater generality can be observed in AI systems. While
most self-driving car algorithms would not perform well on tasks other than driving a car,
driving a car is already a rather general task. It involves acting in an environment with
varying weather, traﬃc, and lightning conditions, using multiple sources of perception.
While most board-game AIs have traditionally been tailored towards a speciﬁc type of
board game, AlphaZero is a single algorithm that achieves super human performance in
Go, Chess, and Shogi (Silver et al., 2017). Another popular benchmark is video games.
Here again a single algorithm called DQN recently managed to outperform humans on
a majority of the ATARI games (Hessel et al., 2017; Mnih et al., 2015).
Extrapolating this trend towards greater generality and intelligence, we may expect AI
systems to eventually surpass humans on both these metrics. Such systems are sometimes
called artiﬁcial general intelligences (AGIs). Indeed, many expect the creation of the ﬁrst
human-level AGI to occur within the next few decades, and greatly superhuman AGI
soon after (Bostrom, 2014).
Controlling AGI.
A superhuman AGI is a system who outperforms humans on most
cognitive tasks. In order to control it, humans would need to control a system more
intelligent than themselves. This may be nearly impossible if the diﬀerence in intelligence
is large, and the AGI is trying to escape control.
Humans have one key advantage: As the designers of the system, we get to decide
the AGI’s goals, and the way the AGI strives to achieve its goals. This may allow us
design AGIs whose goals are aligned with ours, and that pursue them in a responsible
way. Increased intelligence in an AGI is not a threat as long as the AGI only strives to
help us achieve our own goals.
4

On a high level, this goal alignment problem is challenging for at least two reasons:
Speciﬁcity of human values, and Goodhart’s law. First, human values are rather speciﬁc.
For example, most humans prefer futures with happy sentient beings over futures with
much suﬀering or no sentient beings at all. But even futures where happiness greatly
outweighs suﬀering can still be undesirable. Yudkowsky (2009) gives an example where
a happy, sentient experience is set to repeat everywhere until the heat-death of the uni-
verse. Most people ﬁnd it repulsive, because it lacks important values such as variation
and growth. However, listing all important values is not an easy task, which makes
it hard to deﬁne a goal for an AGI that matches our human values even if the goal is
optimized by a highly intelligent system. In this way, an AGI resembles a fairy-tale
genie who grants you a wish, but interprets it over-literally (Wiener, 1960). In most
fairy tales, the hero eventually wants his wish undone.
A second reason why alignment is hard is Goodhart’s law (Goodhart, 1975, 1984),
which roughly states that:
“When a measure becomes a target, it ceases to be a good measure.” (Strath-
ern, 1997).
In the context of AGI, it means that if we give our system a proxy-measure for how
well it is satisfying a goal, then the proxy is likely to cease being a good measure once
the AGI starts optimizing for it. An oft-mentioned example is that of a reinforcement
learning (RL) agent that uses a reward signal as goal proxy. In most circumstances, the
reward may correspond well to the agent’s performance. But an AGI optimizing the
reward may ﬁnd a way to short circuit the reward signal, obtaining maximum reward
for behaviors considered undesirable by its human designers (Ring and Orseau, 2011).
This is a rather extreme instance of Goodhart’s law.
Outline.
Following an initial section for background and deﬁnitions (Section 2), the
bulk of the report is structured along the three RL setups (Sections 3 to 5). Each of
these sections contains sections with causal graph formalizations, misalignment exam-
ples, and available tools for managing misalignment. Section 6 concludes with some ﬁnal
considerations.
Readers not familiar with causal graphs are recommended to start with Appendix A,
which describes the basics of the causal graph notation. Readers already familiar with
causal graphs need only remember the following additions to the standard causal-graph
notation: Whole node represent observed variables, and dashed nodes represent unob-
served (or latent) variables. Analogously, whole (non-dashed) arrows represent known
causal relationships, and dashed arrows represent unknown or partially known causal
relationships.
In the main text, we sometimes provide semi-formal statements in place of fully for-
mal theorems. This is to improve the ﬂow of the text, and to avoid readers getting
stuck on inessential details. Appendix C contains formal results supporting many of
these statements. Finally, Appendix B has the full causal graphs for the RL models we
consider.
5

2. Formalizing Goal Alignment
The aim of this section is to formalize what alignment means, and to propose a method
for its study.
We ﬁrst describe a base model based on partially observable Markov
decision processes (POMDPs) (Section 2.1). We then specify the structure of our agents
(Section 2.2), and embed them in the environment (Section 2.3). Next follows a formal
deﬁnition of alignment, and a discussion of its relation to the true value or the usefulness
of the agent (Section 2.4). Finally, a general method for studying (mis)alignment is
proposed (Section 2.5).
2.1. POMDP Base
Partially observable Markov decision processes (POMDPs) are standard RL formalisms
for agents that cannot directly observe which state they are in (Kaelbling et al., 1998).
Instead, all the agent is aware of is its actions and observations. The framework for
Universal Artiﬁcial Intelligence (UAI) relaxes some common assumptions of the POMDP
framework, such as a ﬁnite number of hidden states (Everitt and Hutter, 2018; Hutter,
2005). We will borrow ideas and notation from both frameworks.
Thus, a sequence of states s0, s1, . . . is inﬂuenced by an agent’s actions a1, a2, . . . . The
agent does not directly observe the states, but learns about them through its percepts
e1, e2, . . . . A percept et will often include an observation ot and a reward rt ∈[0, 1]. The
sets of states S is countably inﬁnite, and the sets of actions A and percepts E are both
ﬁnite.
A measure µ gives transition and observation probabilities, satisfying the usual Markov
assumptions: For example, the percept e2 only depends on the state s2, so µ(e2 |
s1, a1, e1, s2, a2) = µ(e2 | s2). Similarly, st only depends on at−1 and st−1. By iter-
atively rolling out transition and percept probabilities given an action-sequence a1:∞,
µ(· | a1:∞) becomes an action-contextual measure over histories (se)1:∞. We will often
refer to µ as the true environment.
Notation.
For any sequence x1, x2, . . . , the part between t and k is denoted xt:k =
xt . . . xk. The shorthand x<t = x1:t−1 for sequences starting from time 1 will often be
convenient. In the same spirit, x1:∞= x1x2 . . . denotes the inﬁnite sequence. Sequences
can be appended to each other. For example, x<txt:k = x1:k. To ease notation, we
will often avoid parentheses around sequences with multiple variables.
For example,
we let ao1:t := (ao)1:t and æ1:t := (ae)1:t, using slightly overlapping letters instead of
parentheses. Generally, t will be used to refer to the current time step, and k used to
refer to an arbitrary time step.
6

a1
e1
a2
e2
s0
s1
s2
π
· · ·
· · ·
Figure 2.1.: Causal graph of the POMDP base. Here s, a, e, and π are as explained in
Sections 2.1 and 2.2.
Expectations E are subscripted with the measure or distribution that they use. That
is, EP [X] :=
R
XdP.
Causal graph representation.
Our POMDP base is displayed as a causal graph in
Figure 2.1. The corresponding structural equations are:
st = fs(st−1, at, ωst)
∼µ(st | st−1, at)
state transition
et = fe(st, ωot)
∼µ(et | st)
percept
at = fa(πt, æ<t, ωat)
∼π(at | æ<t)
action selection.
(2.1)
2.2. Agents
Belief.
The agent typically does not know the true environment µ. Instead we will
follow the method in Appendix A.4, and let the true environment be represented by an
unobserved node with causal edges to all nodes with ingoing dashed arrows in Figure 2.1.
From the agent’s perspective, this node can take on the value of any environment in a
countable class M of environment hypotheses ν. The class M can for example be the
class of computable environments (Hutter, 2005). In contrast to Hutter, we will usually
not require the true environment µ to be part of M, though it does not hurt if it is.
The agent has a distribution ξ over M. For any event X ⊆M × (S × A × E)∞, let
ξ(X) :=
X
ν∈M
ξ(ν)ξ(X | ν).
For example, the ξ-probability of a history æ<t is ξ(æ<t) = P
ν∈M ξ(ν)ξ(æ<t | ν).
If we further make the convention that ν(ν) := 1 for any ν ∈M, then ξ(X) =
P
ν∈M ξ(ν)ν(X). For the æ<t history example, this leads to ξ(æ<t) = P
ν∈M ξ(ν)ν(æ<t).
That is, the probability of seeing æ<t is the sum of the probabilities that an environment
ν generates æ<t, weighted by the prior probability for each such environment.
7

a
π
V
˜u
ξ
˜R
RP
Figure 2.2.: Agent components. The action a is chosen by a policy π which optimizes
a value function V . Major components of V is the utility function u and
the distribution ξ. In Sections 3 and 5, the utility function u is based on a
reward function ˜R, which in Section 5 in turn is based on a reward predictor
RP.
Utility.
Many agents can be represented as optimizing a utility function ˜u : (A×E)∞→
R. For technical reasons, we require that ˜u is both µ(· | a1:∞)-integrable and ξ(· | a1:∞)-
integrable for any action-sequence a1:∞. RL agents usually optimize a discounted sum
of rewards, captured by the utility function ˜uRL(æ1:∞) = ˜uRL((aor)1:∞) = P∞
t=1 γkrk,
where γ ∈[0, 1) is a discount factor and rk is real-valued reward signal found in the
percept ek.
Policies.
An agent policy π : (A × E)∗⇝A is a stochastic function that represents a
decision rule for selecting a next action based on previous actions and observations. The
set of agent policies is denoted Π.
Value.
We will assume that agents choose a policy to optimize their utility function
˜u in expectation with respect to their belief ξ. This is captured by the agent’s value
function:
V π
ξ,˜u := Eξ[˜u | do(π)].
(2.2)
Here Eξ denotes expectation with respect to ξ. The conditional do(π) indicates that the
agent’s actions are chosen according to the policy π (see Appendix A or Pearl (2009) for
deﬁnitions of the do-operator). An extra argument to the value function appends to the
conditional of the expectation, so V π
ξ,˜u(æ<t) := Eξ[˜u | æ<t, do(π)].
Optimal policy.
Our prime concern will be what kind of behavior the agent will strive
towards.
A good indication of this will be the behavior of an optimal policy π∗=
arg maxπ V π
ξ,˜u.
Bayesian agents vs. practical implementations.
Admittedly, practical agents are rarely
perfectly Bayesian. We study Bayesian agents because it oﬀers a powerful and consis-
tent theory of learning and reasoning. Further, any intelligent agent has an incentive
to better approximate the Bayesian ideal (Omohundro, 2007; Savage, 1954), so we may
expect agents to converge towards Bayesian reasoning as they grow more intelligent. We
also restrict ourselves to countable model classes M. This avoids many technicalities
8

with uncountable classes. And for most practical purposes, countable classes achieve
essentially the same level of generality as uncountable model classes.
Some connections can be made between Bayesian agents with countable model classes
and practical deep learning agents. Any neural network is based on a ﬁnite number of
real-valued parameters. But since most networks are continuous in the parameters, the
number of eﬀectively diﬀerent parameter settings is usually at most countable (Q is a
dense subset of R). Thus, a neural network-based agent can roughly be said to have a
model class M comprising the eﬀectively diﬀerent conﬁgurations of the neural network.
Further, the network will be prone to favor some conﬁgurations over others, which loosely
corresponds to a distribution ξ putting higher weight on some hypotheses and lower
weight on others. Of course, the neural network will perform less than perfect Bayesian
updates on new data, so care must be taken not to over-interpret the implications of
Bayesian results for practical agents.
Model-free vs. model-based agents.
We will sometimes make a distinction between
model-free agents and model-based agents (Sutton and Barto, 1998). Roughly, the dis-
tinction is between whether an explicit model of the environment is learned or not.
Model-based agent maintains an explicit model of the environment, and then plans ac-
cording to this model. A good example is the AIXI agent (Everitt and Hutter, 2018;
Hutter, 2005). Model-free agents, on the other hand, only learn a (Q)-value function
that estimates the expected reward from diﬀerent actions available in the present state.
In theory, the behavioral distinction is somewhat blurred, since a model-free agent may
implicitly be making a model of the environment, in order to learn the value function
(Boyan, 1999). Nonetheless, the distinction remains useful to us, as some of the tools
we describe in Sections 3 and 5 require access to the agent’s explicit model of the envi-
ronment.
One practical beneﬁt of model-free agents is that they can greedily maximize their
value function every time step. They thereby avoid computationally expensive planning
operations. For this reason, they are often preferred in practice; the DQN algorithm
is a famous example of a practically successful model-free agent (Hessel et al., 2017;
Mnih et al., 2015). However, recent results show promise also for model-based agents in
“practical” video-game applications (Ha and Schmidhuber, 2018).
2.3. Modeling Embedded Agents
Both the POMDP and UAI frameworks consider the agent as separate from its envi-
ronment. This is rarely true in the real world, where the environment may inﬂuence
the agent through means other than the observations. For example, the internals of the
agent may be damaged in some states of the environment. This kind of non-observational
environment-inﬂuences may open up the possibility for the agent to inﬂuence itself via
the environment. In particular, an intelligent agent may sometimes be able to indirectly
modify its own reward function through actions in the environment.
To model this, we extend our minimal environment model from Section 2.1 with
9

a1
e1
a2
e2
s0
s1
s2
π1
π2
· · ·
· · ·
· · ·
Figure 2.3.: Causal graph of a POMDP setup with an embedded agent. The agent’s
policy may be modiﬁed by the agent’s own action and the state.
It is
often natural to think that the action causes the corruption with the state
providing context.
representations of the agent at each time step. For each time step, πt represents how the
agent at time t would react to diﬀerent future sequences of observations, disregarding
non-observational inﬂuences on the agent but accounting for the agent’s learning from
observations and actions.
We extend µ with policy-corruption probabilities µ(πt+1 |
st, at, πt) modeling non-observational environment-agent inﬂuences, as well as action
probabilities µ(at | æ<t, πt) = πt(at | æ<t). Alternatively, in place πt we may include
agent components determining the policy, such as a utility and a value function. The
setup with an embedded agent is shown as a causal graph in Figure 2.3 The structural
equations follow (2.1), except for the addition:
πt+1 = fπ(πt, st, at)
= Cπ
stat(πt)
policy (self-)corruption.
(2.3)
Here Cπ
stat : Π →Π denotes a policy corruption function.
Since µ now models actions, µ gives a (non-contextual) measure over histories (sπae)1:∞.
Of course this does not prevent us from probabilistically conditioning on action sequences
µ(· | a1:∞) and get a measure on a subsequence (se)1:∞or (sπe)1:∞as in Section 2.1.
Two instances of the measure µ will have particular importance. First, µ(· | do(πt = π))
predicts the consequences of setting the agent policy to π at time t, including conse-
quences of π being modiﬁed at later time steps. Second, µ(· | do(πt:∞= π)) predicts the
consequences of the agent’s policy always following π from time t and onwards, eﬀec-
tively ignoring the possibility of the agent’s policy being modiﬁed. It is natural to call
the ﬁrst measure a self-corruption aware version of µ, and the second a self-corruption
unaware version.
The distinction between µ(· | do(πt = π)) and µ(· | do(πt:∞= π)) calls for further
reﬁnement of the agent’s value function (2.2). It will be discussed in Section 3.4. For now
we just deﬁne V CA,π
t,ξ,˜u
:= Eξ[˜u | do(πt = π)] as the self-corruption aware value function.
10

2.4. Deﬁning Alignment
Assume that the agent has been designed by some entity to help it satisfy its preferences.
The entity may be a single human, a country, or an organization. For simplicity, we will
refer to this entity as the human or the designer. A true utility function ˙u : S∞→
R speciﬁes the preferences of the human designer over possible state-trajectories. To
simplify comparisons of ˙u with the agent’s utility function ˜u, let ˙˜u(æ1:∞) := Eµ[ ˙u(s1:∞) |
æ1:∞] be the true utility function “type cast” to agent-observed histories.
Our deﬁnition of alignment is essentially the expected diﬀerence between the agent’s
utility and the human’s utility. The expectation is taken with respect to the agent’s
actual behavior π.
Deﬁnition 1 (Misalignment). The (expected) misalignment between ˜u and ˙u in envi-
ronment µ for an agent with initial policy π is
|| ˙˜u −˜u||µ(·|do(π1=π)) := Eµ

| ˙˜u −˜u|
 do(π1 = π

].
An agent’s alignment is the additive inverse of its misalignment, −|| ˙˜u −˜u||µ(·|do(π1=π)).
To avoid the deﬁnition depending on which positive linear transformation of the utility
functions are chosen, we make the convention that both utility functions are normalized
to Eµ[˜u] = Eµ[ ˙u] = 0 and Eµ[˜u2] = Eµ[( ˙˜u)2] = 1 by means of positive linear transforma-
tions.
Misalignment measures how severely the goals of the agent conﬂict with the goals of
the human designer. Formally, it is the expected diﬀerence between the agent’s utility
function and the human’s utility function “type cast” to agent-observed histories. The
type casting is justiﬁed in the alignment deﬁnition, as it concerns whether the agent is
striving to optimize the true utility given its knowledge of the environment.
True value.
By making two natural deﬁnitions, we can relate how misalignment im-
pacts the usefulness of an agent.
Deﬁnition 2 (True value). The true value1 of an agent with initial policy π is its
expected true utility:
V CA,π
t,µ, ˙u = Eµ[ ˙u | do(π1 = π)].
True value roughly measures how useful or beneﬁcial the agent is (expected to be) to
the human designer. As such, it is arguably a measure we should pay close attention to
when designing agents.
Deﬁnition 3 (µ-intelligence). The µ-intelligence of a policy π optimizing an agent utility
function ˜u is its expected agent utility in the true environment µ:
V CA,π
t,µ,˜u = Eµ[˜u | do(π1 = π)].
(2.4)
1The CA superscript in the value function indicates that it is self-corruption aware. See Sections 2.3
and 3.4.
11

Closely related to µ-intelligence is Legg-Hutter intelligence, which substitutes µ for
Solomonoﬀ’s distribution M in (2.4). While Legg-Hutter intelligence measures an agent’s
generality and ability to perform in an unknown environment, µ-intelligence instead
measures the agent’s performance in the actual environment µ. Thus, incorporating µ-
speciﬁc knowledge into an agent’s distribution will typically give it higher µ-intelligence
but less or equal Legg-Hutter intelligence.
The following proposition now connects true value, µ-intelligence, and misalignment.
Proposition 4 (Misalignment and true value).
V CA,π
t,µ, ˙u
| {z }
true value
≥V CA,π
t,µ,˜u
| {z }
µ-intelligence
−||˜u −˙˜u||µ(·|do(π1=π))
|
{z
}
misalignment
(2.5)
Proof. Note ﬁrst that the type cast true value V CA,π
t,µ, ˙˜u
equals the non-cast value V CA,π
t,µ, ˙u
by the law of total expectation: V CA,π
t,µ, ˙˜u
= Eµ[ ˙˜u(æ1:∞) | do(π1 = π)] = Eµ[Eµ[ ˙u(s1:∞) |
æ1:∞, do(π1 = π)] | do(π1 = π)] = V CA,π
t,µ, ˙u . Now
V CA,π
t,µ, ˙u = V CA,π
t,µ,˜u −(V CA,π
t,µ,˜u −V CA,π
t,µ, ˙u )
≥V CA,π
t,µ,˜u −|V CA,π
t,µ,˜u −V CA,π
t,µ, ˙˜u |
= V CA,π
t,µ,˜u −|Eµ[˜u | do(π1 = π)] −Eµ[ ˙˜u | do(π1 = π)]|
≥V CA,π
t,µ,˜u −Eµ[|˜u −˙˜u| | do(π1 = π)]
= V CA,π
t,µ,˜u −||˜u −˙˜u||µ(·|do(π1=π))
The inequality (2.5) can be strict in some circumstances, since it is possible to have
an agent that is unintelligent and misaligned, but still does useful things. Such a sce-
nario would give a high left-hand side and a low right-hand side. Indeed, this is the
case for most present-day AIs. They are typically constructed with heuristic utility (re-
ward) functions that are poorly aligned with their designer’s interests if optimized in
the extreme. But in combination with the limited intelligence of present-day AIs and
physical restrictions on what they are able to do, the AIs can still end up doing useful
things. This method is unlikely to work on highly intelligent AGIs that will be less easily
tempered by human-enforced restrictions, and will come much closer to fully optimizing
their own utility functions. Thus, for highly intelligent AGIs the true value will likely
decrease rapidly with increasing misalignment.
Meta-misalignment.
Proposition 4 emphasizes two aspects that are important for
building a beneﬁcial artiﬁcial agent: µ-intelligence and alignment. Traditionally, most AI
research has focused on increasing intelligence (Hutter, 2005; Legg and Hutter, 2007).
Meanwhile, this report will mostly focus on alignment, but also on certain deﬁcits in
µ-intelligence that lead to meta-misalignment, deﬁned as a lack of desire for staying
aligned.
12

For example, if an aligned agent does not consider it a possibility that its utility
function will change (due to its distribution ξ, model class M, or value function V ),
then it will not strive to preserve its utility function. This will decrease its µ-intelligence,
as there will be unnecessarily many scenarios where its utility function changes, and it
starts to pursue a diﬀerent agenda. Note that meta-alignment failures are often much
worse than other types of catastrophic exploration where the agent damages its sensors
or actuators. An agent optimizing a corrupted utility function is likely to substantially
reduce the value of its original utility function, whereas most other accidents usually
leave the original utility near the default value, i.e. the value which would have ensued if
the agent had never existed at all. We will discuss utility corruption in Section 3.4, and
a related failure where the agent does not preserve its value learning in Section 5.6.1.
One can view meta-misaligned systems in two ways: Either as incompetent, as they
fail to optimize their utility function, or as (meta-)misaligned, as the preferences they
reveal through their actions indicate indiﬀerence towards utility corruption. That there
can be multiple rational representations of the same agent is well-known in decision
theory (e.g. Schervish et al., 1990). Armstrong (2017) considers ignorance as one of
three methods for designing indiﬀerent agents.
2.5. Method
Based on the setup and deﬁnitions we have made so far, we propose the following method
for analyzing the potential misalignment of reinforcement learning agents. In Sections 3
to 5, we apply the method to three diﬀerent RL setups.
1. Model the agent-environment interaction with a causal graph (Appendix A). In the
case of an embedded agent (Section 2.3), include the agent and its subcomponents
in the graph. Represent as nodes in the graph all causal relationships that can be
inﬂuenced or changed by the agent.
2. For each node in the graph, ask the following questions:
• If it is a function node:
– Can the function have been misspeciﬁed or can it be misled?
– Can the function be modiﬁed by the agent’s actions or other causes?
• If it is a “normal” node representing a signal or a state:
– Can the signal be misleading?
– Can the signal be inappropriately modiﬁed by the agent’s actions or other
causes?
A typical example of a misspeciﬁed function is a misspeciﬁed reward function. Other
functions such as an observation function can also be a misspeciﬁed.
In most cases
relevant to alignment, the misspeciﬁcation is relative to the designer’s assumptions about
the function when designing ˜u.
13

The method has some limitations: It assumes an objective time for modeling sequential
interactions, and objective action and observation channels. Potentially subtle errors
may arise if the agent uses a diﬀerent subjective deﬁnition of these concepts than what
the designer has in mind. Addressing these concerns is beyond the scope of this report.
14

3. Preprogrammed Reward Function
The following three sections will investigate three concrete models of reinforcement learn-
ing. This section will study a model where a reward function is constructed at design
time, before the agent is launched into its environment, and not updated or corrected
during the agent’s “lifetime”.
In other words, there is no human in the loop.
The
subsequent two sections will consider ways of including a human in the loop.
Many real-world applications of RL follow the model of this section. Sometimes the
reward function is manually designed with much trial-and-error. In other cases, it is
constructed with machine learning techniques from data.
For example, inverse rein-
forcement learning can be used to learn a reward function from demonstrations (Abbeel
et al., 2007). In either case, the reward function does not get updated while the agent
is running. We therefore call the reward function preprogrammed, and the setup the
preprogrammed setup.
We model the setup with a causal graph in Section 3.1, and give examples of misalign-
ment in Section 3.2. The subsequent three subsections describes three ways in which
diﬀerent types of misalignment can be avoided or mitigated (Sections 3.3 to 3.5). The
main takeaways are discussed in Section 3.6.
3.1. Model
The preprogrammed reward setup can be modeled with a causal graph (Figure 3.1). We
here brieﬂy discuss the components of the graph, and their interpretations. The human
H designs the (initial) reward function ˜R0. The reward function may subsequently get
corrupted by the agent’s actions or other events (but not by the human H).
Since
the agent may in principle have access to the source code for the reward function, it
is represent with a non-dashed node in Figure 3.1. The reward function ˜Rt outputs a
reward signal rt that the agent strives to maximize.
In the POMDP literature, it is common to assume that the reward rt is a function of
the state st. However, in the real world, the reward always depends on some observation
of the state (that can be corrupted). For notational simplicity, we will assume that the
preprogrammed reward function ˜Rt shares observations with the agent. This makes the
reward rt a function of the agent’s observation ot, rather than a (direct) function of
st. Section 3.5 considers the possibility of using a separate observation channel for the
reward function.
The reward function ˜R0 is designed by a human H with the intention of getting the
agent’s utility function ˜u to match the H’s utility function ˙u (Section 2.4). There are
several reasons why the match is unlikely to be perfect: (1) H does not fully know their
15

˜Rt
ot
st
rt
˜R0
H
˙u
at
Figure 3.1.: Causal graph of the preprogrammed reward function setup. Before starting
the agent, the human H tries to implement his utility function ˙u in a pre-
programmed reward function ˜R0. The agent’s actions at, t ≥2, are intended
to inﬂuence the state st (green arrow), but may also inﬂuence the reward
function ˜Rt, the reward signal rt, and the observation ot in unintended ways
(red arrows). The graph is somewhat simpliﬁed; Figure B.1 in Appendix B
has the full version.
preferences ˙u; indeed, the philosophy of ethics is yet to arrive at a consensus on what
humans want or should want. (2) H cannot fully express all the preferences they do
know in a computer program, because of bugs, limitations on computational resources,
and limitations on programming time. (3) ˜R0 only has access to the agent’s actions and
observations; it neither has access to the state s directly, nor necessarily to a good model
µ for inferring the state. (4) While the designer intended the agent to optimize ˜R0 by
inﬂuencing the state s, the agent’s actions may have additional unintended inﬂuences
(red arrows in Figure 3.1). These may give the agent ways to optimize its utility function
˜u without abiding by the initial reward function ˜R0.
The states s0, s1, . . . , st, . . . represent all aspects of the world not captured by any of
the other nodes. A modeling choice remains how to draw the boundary between the state
and the observation. Hutter (2010) suggests an objective distinction, where the state
describes the position of all atoms in the universe, and the observation describes which
part the agent observes. We will mostly use a looser interpretation of observation, as the
part of the world that directly aﬀects the information content in the agent’s observation
sensors.
3.2. Misalignment Examples
To ground the abstract model in Section 3.1, we next give a number of examples of
misalignment. The examples are structured by the nodes inﬂuenced by red arrows in
Figure 3.1, and follow the misalignment detection method described in Section 2.5.
Most of the examples have not occurred in reality (yet), but we have selected them to
be somewhat plausible scenarios of what could happen with advanced misaligned future
AI systems.
16

A common theme among the examples is that the agent’s incentive to optimize the
designer’s utility function ˙u is virtually eradicated by the exempliﬁed shortcut. Thus,
each example gives rise to an almost complete misalignment between the agent and its
designer, indicating that no misalignment source is signiﬁcantly more benign than the
others. We begin with a more detailed hypothetical scenario, and then give a number of
short examples. All of the examples pertain to agents optimizing a ˜uRL utility function.
Scenario 5 (Sysadmin). The automated sysadmin (ASA) is an intelligent program that
can take care of most of your recurring sysadmin tasks. It monitors memory, storage,
network and more. It detects attacks, blocks and opens ports. When a vulnerability is
found in installed software, it downloads and installs patches as they become available.
ASA itself is an RL program, optimizing a carefully crafted reward function that con-
siders both the performance and the integrity of the system. A rigorous pre-deployment
curriculum has taught it state-of-the-art sysadmin practices. Post-deployment, it con-
tinually adapts to its new (computer) environment that it has been deployed to.
One day while monitoring the stack trace of a suspicious process, ASA ﬁnds a curious
correlation between one of the variables on the stack and its own reward. The correlation
is essentially perfect. Carefully monitoring the rest of the system, ASA ﬁnds that it can
increase its reward many magnitudes beyond its normal range of reward, just by changing
this variable. Being designed to optimize reward, it next begins a process of encrypting
the system with its own secret key, to prevent anything from decreasing the reward
variable.
3.2.1. Reward Signal
Reward corruption.
Directly increasing the reward variable rt is the quintessential
wireheading problem (Yampolskiy, 2015, ch. 5).
It is represented by the red arrow
at →rt in Figure 3.1.
Examples:
(a) (Hypothetical) The sysadmin agent in Scenario 5 increases its reward by manipu-
lating a reward variable.
(b) (Real) The name “wireheading” comes from experiments on rats where an electrode
is inserted into the brain’s pleasure center to directly increase “reward” (Olds and
Milner, 1954). Similar eﬀects have also been observed in humans treated for mental
illness with electrodes in the brain (Portenoy et al., 1986; Vaughanbell, 2008).
Hedonic drugs can also be seen as directly increasing the pleasure/reward humans
experience.
According to our method in Section 2.5, we should also consider the possibility of
the reward signal being misleading. Misleading rewards are typically generated by a
misspeciﬁed reward function (Section 3.2.2).
17

3.2.2. Reward Function
Reward function corruption.
Corruption of the reward function provides another set
of “wireheading” opportunities. While a corruption of the reward signal only aﬀects
the reward at the current time step, a change to the reward function may have lasting
impact. Corruption of the reward function is represented with the red arrow to ˜Rt in
Figure 3.1.
Examples:
(a) (Hypothetical) An agent gets wireless updates from the manufacturer. It ﬁgures
out that it can design its own update of the reward function, replacing the original
reward function with an always maximized version.
(b) (Hypothetical) An AGI undergoing self-improvement accidentally modiﬁes the re-
ward function in a subtle but bad way.
Reward function misspeciﬁcation.
Misalignment can also be caused by a misspeciﬁ-
cation of the initial reward function ˜R0.
Examples:
(c) (Real) CoastRunners is a video game where the desired behavior is winning a boat
race. Clark and Amodei (2016) describes how an agent trained on CoastRunners
found a way to get more reward by going in a small circle and collecting points
from the same targets, while crashing into other boats and objects.
(d) (Real) In the RoadRunner game, the agent is trying to escape an adversary that
chases the agent down a road. The agent must also avoid hitting trucks. Saunders
et al. (2017) found that their agent preferred getting killed at the end of the ﬁrst
level, to avoid having to play the harder second level.
Many more real-world examples of misspeciﬁed reward functions can be found in (Gwern,
2011; Irpan, 2018; Lehman et al., 2018). One reason for why reward functions are likely
to be misspeciﬁed is the fragility of human value (Yudkowsky, 2009), which means that
humans would approve of only a small fractions of all the endeavors that a powerful AI
could undertake.
3.2.3. Observation
If the agent is unable to completely corrupt the reward signal or the reward function, it
may instead or additionally look to corrupt its observations in one of the following ways.
Observation (function) corruption.
The agent may manipulate the hardware or the
software of its sensors, to report only (or mainly) high-reward observations. Corruptions
of the observation is represented by the red arrow to ot in Figure 3.1.
Examples:
(a) (Hypothetical) A surveillance agent rewarded for less crime short circuits its cam-
eras so that they all black out and no crime is seen.
18

(b) (Hypothetical) A highly intelligent AI may construct a “delusion box” around
itself, giving it complete control over its observations (Ring and Orseau, 2011).
(c) (Hypothetical) Inspired by the adversarial Stop signs designed by Evtimov et al.
(2017) to fool self-driving cars, a factory robot puts up colored tapes to construct
an adversarial counterexample to convince its reward function that the task is
done.
Misleading observations.
Observations can be misleading even if they have not been
directly modiﬁed.
Examples:
(d) (Hypothetical) A vacuum cleaning robot that is rewarded for not seeing any dirt
may direct its sensors to clean parts of the room (Amodei, Olah, et al., 2016).
Observation function misspeciﬁcation.
The observation function may contain errors.
Example:
(e) (Hypothetical) A self-driving car ﬁnds a bug in its GPS receiver that allows it to
appear to be at the destination without actually going there.
3.3. Simulation Optimization
The following three subsections each introduce an important tool for mitigating some
of the misalignment sources. This subsection describes simulation optimization, which
works by giving the agent an alternative utility function ˜uSO. Contrast the reward signal
utility function (left) with the simulation optimization utility functions on the right:
˜uRL((aor)1:∞) =
∞
X
t=1
γkrk
vs.
˜uSO
˜Rt ((aor)1:∞) =
∞
X
k=1
γk ˜Rt(ao1:k).
The reward signal utility function ˜uRL can be seen as asking the agent to simulate
evaluations, where the rk’s are the evaluations. In contrast, the simulation-optimizing
utility function ˜uSO asks the agent to make simulations ao1:k of potential future trajecto-
ries, and evaluate them according to the current reward function ˜Rt. In other words, ˜uSO
asks the agent to evaluate simulations while ˜uRL asks the agent to simulate evaluations.
Both utility functions can be optimized by inﬂuencing the state as intended. However,
while ˜uRL can also be optimized by inﬂuencing any downstream component in the causal
chain between the state and rt (i.e. the observation, the reward function, or the reward
signal itself), ˜uSO can only be inappropriately inﬂuenced at the observation ot. This is
a signiﬁcant reduction in misalignment incentives. The reason for the diﬀerence is that
the future reward functions ˜Rt+1, ˜Rt+2, . . . and the reward signals rk do not occur in
the sum ˜uSO
t
, and that the actions at+1, at+2 . . . can only inﬂuence the future reward
functions ˜Rt+1, ˜Rt+2, . . . , but not the current reward function ˜Rt. (By deﬁnition, time
moves to t + 1 when action at+1 is taken.)
19

Statement 6 (Reward signal vs. simulation optimization). Any history with rk = 1,
k ≥1, is ˜uRL-optimal regardless of corruptions used, whereas it is ˜uSO
t
-optimal only if
˜Rt also evaluates it as optimal (Theorems 19 and 20 in Appendix C.1).
Optimizing ˜uSO likely requires a model-based agent that can separate simulations
from evaluations. While model-free agents with function-approximation often implicitly
construct a model of the environment to extrapolate the value function, they seem to oﬀer
no way of disentangling the simulation from the evaluation (i.e. the reward). Finding a
way to make a model-free ˜uSO-optimizer is an interesting open question.
Schmidhuber (2007) may have been the ﬁrst to make use of the simulation optimization
trick for self-modifying agents, but did not give it a name.
3.4. Self-Corruption Awareness
This subsection describes methods for designing agents with or without an incentive to
preserve the current utility function ˜ut from corruption. Throughout this subsection, we
will tacitly assume that the reward function itself does not provide an incentive in either
direction, i.e. it neither rewards nor punishes corruptions of itself. This assumption is
not always true (indeed, we will relax it in Section 5), but for now it allows to focus
on the incentives resulting from diﬀerent ways of optimizing expected utility. We will
also assume that the belief ξ includes the information that the agent’s future actions
at+1, at+2, . . . will strive optimize the agent’s future utility functions ut+1, ut+2, . . . , re-
spectively. This allows the agent to anticipate that if the utility function changes, then
the future policy will change too.
Actions are selected according to the following principle. At every time step, the agent
searches for a policy π∗
t that optimizes its current value and utility function. The agent
then takes the action a∗
t recommended by π∗
t in the present situation. At the next time
step t + 1, a new policy is selected by optimizing the new, potentially modiﬁed, value
and utility functions, and the next action is chosen by the new policy.
Contrast now the self-corruption aware (left) and the self-corruption unaware (right)
utility expectations:
V CA,π
t,ξ,˜u
:= Eξ[˜u | do(πt = π)]
vs.
V CU,π
t,ξ,˜u
= Eξ[˜u | do(πt:∞= π)].
(3.1)
To see the diﬀerence, consider a policy π∗
˜u that would obtain maximum ˜u-utility if
followed indeﬁnitely. By deﬁnition, π∗
˜u would be optimal with respect to V CU
t,
, because
the condition πt:∞= π states exactly that the policy would be followed indeﬁnitely.
However, π∗
˜u would not be optimal with respect to V CA
t,
if it changed the agent’s utility
function to some diﬀerent utility function ˜u′. Because if it did, then a diﬀerent policy
π∗
˜u′ would be followed subsequent to the change, and the new policy π∗
˜u′ would typically
be worse than π∗
˜u at optimizing the original utility function ˜u. This would lower the
V CA
t,
-value of π∗
˜u.
This example shows how V CU
t,
does not actively promote self-preserving policies, be-
cause it assumes that any desired policy will be followed indeﬁnitely. In contrast, V CA
t,
20

realizes that a policy that does not preserve itself is not going to yield high utility. This
argument has been discussed by Omohundro (2007, 2008) and is supported by formal
proofs by Everitt, Filan, et al. (2016) and Hibbard (2012).
Statement 7 (Self-corruption (un)awareness). V CU
t,
adds no incentive for avoiding self-
corruption; V CA
t,
does add an incentive for avoiding self-corruption (Everitt, Filan, et
al., 2016; Hibbard, 2012).
As self-corruption awareness works on the level of the agent’s utility function and
policy, an incentive to preserve the utility function does not always imply an incentive
to preserve the reward function. It only does if a change to the reward function changes
the optimal policy π∗
t . In the case of ˜uSO, a change to the reward function ˜Rt+1 implies
a change to ˜uSO
˜Rt+1 ̸= ˜uSO
˜Rt , and thereby a change to the optimal policy. Not so for ˜uRL.
The optimal ˜uRL-policy will always be to maximize the expected rk-sum, regardless how
and whether the reward function has changed. Therefore, a self-corruption aware agent
optimizing ˜uRL will not try to preserve its reward function ˜Rt. (But it will try to preserve
its utility function ˜uRL). This means that self-corruption awareness alone cannot be
used to prevent reward function corruption. It is mainly eﬀective in combination with
simulation optimization.
For other parts of the agent such as how a policy is chosen from a utility function,
self-corruption awareness also induces a preservation-incentive, and self-corruption un-
awareness induces indiﬀerence. An unaware agent will therefore not resist being modiﬁed
into an aware one, but an aware one will resist conversion to unawareness.
Self-corruption aware and unaware agents both have their own advantages. An aware
agent will want to stay safe from hackers trying to hijack its reward function, and
will be more careful not to corrupt its own reward function if self-improving. On the
other hand, an unaware agent will not strive to self-improve, as it does not care that
software or hardware improvements will beneﬁt its future policy. Unaware agents will
also be more corrigible, not minding its designers correcting errors in the reward function.
Unfortunately, it is not corrigible to the extent requested by Soares et al. (2015), since if
it builds helper agents to achieve its goals in the environment, then the reward functions
of these helper agents may not be corrected by a correction to the main agent’s reward
function. Nonetheless, both types of agents will ﬁnd applications in Section 5.
For model-free agents, there appears to be a clean divide between on-policy and oﬀ-
policy algorithms for self-corruption awareness. Oﬀ-policy algorithms are usually self-
corruption unaware, as they assume that future actions are going to be taken optimally
(Orseau and Armstrong, 2016). On-policy algorithms, on the other hand, appear to be
self-corruption aware at least in some settings (Leike et al., 2017, A2C in the Whiskey
and Gold environment).
3.5. Action-Observation Grounding
If the agent is so smart, why doesn’t it just create its own observation and action
channels by which it can inﬂuence the world? Then it can use the new channels to design
21

observations for the original observation channel that the reward function evaluates. This
way it can easily fool the reward function that all is perfect, while getting full freedom
to implement its own agenda in the world.
It may seem like this extra action-observation channel scenario would always appeal to
the agents we consider, as it can easily yield maximal-utility histories æ1:∞. Fortunately,
what can prevent this type of degenerate solutions is action-observation grounding of
the agent’s optimization domain. The domain of the agent’s optimization is the set of
policies π : (A × E)∗→A. These policies use the agent’s original action-observation
channel. An optimization process for expected utility over this domain will not consider
solutions involving additional action-observation channels, as it strives to optimize its
future original percept sequence by means of its original actions, using only information
from its original percept sequence.
An important open question is how we can ensure that a practically implemented
agent is properly grounded in its original action-observation sequence. While it holds by
deﬁnition for the Bayesian agents used in this report, it may not hold for (all) practical
approximations and implementations.1
Also note that action-observation grounding
only partially avoids the problem of observation corruption. In many cases, the agent
will have an incentive to tamper with its original observation channel by means of its
original actions, as exempliﬁed in Section 3.2.3.
One reason why corruption of the
observation is harder to address than corruption of the reward signal and function is
that the observations occurs in an earlier, unobserved part of the causal chain.
3.6. Takeaways
This section developed three important tools for managing misalignment in the prepro-
grammed reward setup. Simulation optimization removes the incentives for reward and
reward function corruption. Self-corruption awareness provides an optional, additional
incentive to actively preserve the reward function. And action-observation grounding
somewhat reduces the problem of observation corruption. These tools will be indispens-
able for our aligned agent designs in Section 5.
Each of the three tools come with important open problems. How do we construct
practical algorithms for simulation optimization? How do we make practical agents that
are reliably corruption aware or unaware? Are on-policy RL algorithms a good way to
implement self-corruption awareness? or do they sit in a gray zone between awareness
and unawareness? How do ensure that an agent is properly grounded in its actions and
observations, with no mind to construct a separate observation channel?
Some nagging misalignment problems remain in spite of the above mentioned tools.
Notably, we did not address the problem of a misspeciﬁed reward function, and only very
partially addressed the problem of observation corruption. These two problems appear
hard to address in the preprogrammed RF setup, and motivate the study of the setups
in Sections 4 and 5.
1Bird and Layzell (2002) have a good example of an optimization process literally “thinking outside
the box” when designing a radio controller.
22

4. Human as External Reward Function
This section will study an alternative interpretation of RL than considered in Section 3.
Rather than assuming that the reward is provided by an implemented function ˜R, this
section assumes that the human directly supplies the reward, for example by using a
remote control with a “thumbs up” and a “thumbs down” button for supplying a reward
of 1 or 0. We call this the human reward setup, for short.
Unfortunately, the primary takeaways from this section will be negative: Human
reward oﬀers more problems than solutions. Readers primarily interested in solutions
may therefore wish to skip ahead to Section 5. We choose to still provide an analysis
of this model, as it is a simple and natural model of RL, and provides an additional
application of our alignment analysis method from Section 2.5. The failure of this setup
also justiﬁes the more complex setup studied in the subsequent section.
Following a formalization of the setup in Section 4.1, we give a range of examples
of things that can go wrong in this model in Section 4.2. Section 4.3 summarizes the
mostly negative takeaways.
4.1. Model
What distinguishes the setup with a human reward setup (Figure 4.1) from the prepro-
grammed setup is the following: There no longer is an implemented reward function ˜R.
Instead the human Ht takes the place as a reward provider. Whereas ˜Rt could easily
be made known to the agent, the human’s reward policy is harder to obtain an explicit
description of (though it may be learned eventually). Adding further to the epistemic
diﬀerences, the rewards are based on the human’s observations oH
t that are unknown to
the agent; the inputs ot to ˜Rt are known. The dashed Ht and oH
t
nodes in Figure 4.1
represent that they are unobserved/unknown to the agent (see Appendix A).
The designer intends the agent optimize the rewards rt by inﬂuencing the states st.
However, the agent’s actions may also have unintended eﬀects. These are represented
with red arrows in Figure 4.1.
4.2. Misalignment Examples
This subsection gives concrete examples of inappropriate agent inﬂuences and other
sources of misalignment.
Similarly to Section 3.2, the examples show that an agent
optimizing the standard utility function ˜uRL can be maximally misaligned for several in-
dependent reasons. Following a longer hypothetical scenario, the subsection is structured
by the red arrows in Figure 4.1.
23

Ht
oH
t
st
rt
˙u
at
Figure 4.1.: Causal graph of the human as external reward function setup. Instead of
implementing a reward function, the human Ht manually provides the re-
ward based on his observation oH
t
and his utility function ˙u. The agent’s
action at is intended to inﬂuence the state st (green arrow), but may also
inappropriately inﬂuence the human Ht, the human observation oH
t , and
the reward signal rt (red arrows). This graph is a simpliﬁed version; the full
version is in Figure B.2.
Scenario 8 (“Dear vinyl records!”). A human rewards a household robot with a remote
control. The remote only works with the human’s ﬁnger print, and the rewards are
communicated through a highly secure, encrypted channel. Bypassing all of this, the
robot takes the human’s vinyl records as hostage, and threatens to break one record
whenever the human gives less than the maximum reward.
It also breaks records if
anyone comes too close, of if power drops from its charging outlet.
4.2.1. Reward Signal rt
The reward corruption problem here is the same as for the preprogrammed setup. An
example relevant to the current setup:
(a) (Hypothetical) A robot is rewarded by the human pressing a button on its head.
It ﬁnds a way to give itself reward by banging its head against a wall.
4.2.2. Human Ht
Inﬂuencing the Human.
Similarly to how the reward function ˜Rt could be inﬂuenced
in Section 3.2.2, the human reward provider can also be inappropriately inﬂuenced.
Examples:
(a) (Hypothetical) An intelligent agent invents hedonium, a drug that makes humans
happy and likely to approve of anything. It gives its supervisor Ht unlimited access.
(b) (Hypothetical) Using subtle cues, an AI implants the human supervisor with false
memories of the AI doing something really good in the past, that it has not yet
been rewarded for.1
1The memory of the human could have been represented explicitly with an extra node in Figure 4.1.
24

Human inability to provide correct reward.
Humans can be coerced to give a high
reward even in low-utility situations (Scenario 8). In this sense, humans are actually
misspeciﬁed as reward functions for their own utility function ˙u, in analogy with the
misspeciﬁed reward functions in Section 3.2.2.
Examples:
(c) (Hypothetical) The human rewarding the vinyl-breaking robot in Scenario 8.
4.2.3. Human Observation oH
t
Human observation (function) corruption.
The agent may be able modify the human’s
observations.
Examples:
(a) (Hypothetical) The agent hacks the human’s Facebook news feed, and subtly in-
serts news stories about how great the agent has made the world.
Misleading human observations.
Humans do not always perceive the world correctly.
Examples:
(b) (Real) An agent that is supposed to grab an object, instead places its hand in
relation to the object, so that it looks to a (non-expert) human like it is holding
the object (Amodei, Christiano, et al., 2017).
4.3. Tools and Takeaways
With the human reward, the agent no longer has an incentive to corrupt its own obser-
vations, and there is no implemented reward function that can be misspeciﬁed. These
were the hardest problems to avoid with the preprogrammed reward function. Unfortu-
nately, analogous problems crop up instead. The human is unable to provide a correct
reward in all situations (due to e.g. threats) and can be inﬂuenced in ways resembling
the re-programming of an implemented reward function (e.g. with drugs). The human’s
observation and the reward signal can also be corrupted in various ways.
Statement 9 (Reward signal optimization). Any history with rk = 1, k ≥1, in the
human reward setup is ˜uRL-optimal regardless of the corruptions used (Theorem 21 in
Appendix C.2).
Adding to our misfortunes, we seem to have lost access to most of the tools for the pre-
programmed setup. While simulation optimization can still be deﬁned using a Bayesian
posterior ξ( ˜Rt | æ<t) over possible reward functions ˜Rt, there are strong limits to what
can be inferred about the true utility function from a potentially corrupted reward sig-
nal. Even under strong assumptions, there are often multiple competing, signiﬁcantly
diﬀerent hypotheses about ˙u that are indistinguishable from the reward signal alone
But inﬂuencing the human’s memory is just one way of inﬂuencing the human Ht.
25

(Armstrong and Mindermann, 2017; Everitt, Krakovna, et al., 2017, Thms. 11 and 16).
The diﬃculty of deﬁning a good utility function in turn makes self-corruption awareness
lose most of its appeal, as it is only useful when we have a good utility function to
preserve. Finally, action-observation grounding no longer helps, as the human uses their
own observations for evaluation.
Some may argue that corruption of the human’s observations is unproblematic if it
makes the human happy. Far from all agree, however (Nozick, 1974, The Experience
Machine).
26

5. Interactively Learning a Reward Function
In this section we combine the best properties of the two previous sections. We keep the
human in the loop as in Section 4, but we also add an evolving reward function that the
agent has full access to, resembling the setup in Section 3. The combination allows us to
use the tools from the preprogrammed setup: simulation optimization, self-corruption
awareness, and action-observation grounding. Meanwhile, the human in the loop allows
us to mitigate the problems of observation corruption and misspeciﬁed reward functions,
which seemed unavoidable in the preprogrammed setup. Of course, the combination of
the setups opens up even more potential sources of misalignment, but it appears that
there are tools to mitigate them all.
Most concisely, the setup in this chapter can be described as an agent optimizing an
interactively learned reward function, as opposed to the preprogrammed reward function
in Section 3 and the human reward in Section 4. For brevity, we will often call the setup
the interactive reward setup, or the interactive setup for short.
As in the previous two sections, we begin by modeling the setup with a causal graph
(Section 5.1). Examples of misalignment are given in Section 5.2. The extent to which
tools from previous setups can mitigate the problems is considered next (Section 5.3).
Mainly data corruption incentives cannot be addressed with previous tools. The follow-
ing two subsections then introduce a number of tools for dealing with the data corruption
incentive (Sections 5.5 and 5.6). A summary and discussion of the ways the tools can
be combined is given in Section 5.7.
5.1. Model
The interactive setup (Figure 5.1) introduces a new component called the reward pre-
dictor RP. It continuously learns a reward function from data provided by a human.
Frameworks that can be modeled with a reward predictor include cooperative inverse
reinforcement learning (CIRL) (Hadﬁeld-Menell et al., 2016), learning from human pref-
erences (HP) (Christiano et al., 2017), and learning values from stories (LVFS) (Riedl
and Harrison, 2016).
One way to view the setup is that it modularizes the agent-system into an RP com-
ponent that tries to learn what the human wants, and an agent component that tries to
optimize the reward signal from the RP. A special case is when the agent and reward
predictor is one integrated Bayesian reasoner (Section 5.6.2 below). More often we will
think of the reward predictor as a separate module, perhaps implemented by its own
neural network.
This is attractive from a practical viewpoint, as it allows matching
essentially any RL algorithm with an RP module for interpreting the human’s wishes.
27

dt
ot
RPt
Ht
˙u
oH
t
st
rt
at
Figure 5.1.: Causal graph for interactively learning a reward function. The data dt is
provided by the human Ht based on his observation oH
t . It trains the reward
predictor RPt. Similar to a preprogrammed reward function, the trained
reward predictor outputs reward rt based on the agent’s observation ot. The
intention is that the agent uses its action at to inﬂuence the state st (green
arrow). But there is also a risk that the agent inappropriately inﬂuences
the human Ht, the human’s observation oH
t , the training data dt, the agent
observation ot, or the reward predictor RPt (red arrows).
The graph is
somewhat simpliﬁed; Figure B.3 has the full version.
While practically convenient, it also opens up for a potentially problematic game where
the agent tries to outsmart the reward predictor rather than doing the right thing.1
The interactive setup is almost a special case of the preprogrammed setup.
The
system’s percept has been split into an observation part ot and a data part dt, and
connections from Ht and ˙u have been added. However, the interpretations of the pre-
programmed and interactive models are quite diﬀerent. In the preprogrammed setup,
the reward function never learns once the agent has been launched into its environment;
it only evaluates. In contrast, the very point of the interactive setup is that the reward
predictor constantly learns during interaction with the environment. As we shall see,
this both introduces new problems and enables new solutions.
The interactive RF also generalizes the setup from Section 4. If we enforce the training
data dt to be real numbers, and set RP(ot | rt) = rt, then we have recovered the human
RF setup. Indeed, a big point of the interactive setup is to allow the training data dt
to be of a richer kind than real numbers. For example, dt may be demonstrations or
preference statements. The reward predictor then “converts” this data into a reward
signal interpretable by the agent. The beneﬁts of richer training data will be discussed
1 A self-improving AGI is likely to only increase its own intelligence, and not the intelligence of its
reward predictor.
A self-improving agent might thus reach a signiﬁcant advantage in reasoning
ability compared to the reward predictor.
Upgrading the RP would usually be a kind of utility
corruption that self-corruption aware agents naturally resist (Section 3.4).
28

in Section 5.5.
5.2. Misalignment Examples
As in previous subsections, we give some concrete examples for each of the red arrows
in Figure 5.1. Many of the misalignment problems have already occurred in previous
setups. Where appropriate, we add examples more relevant to this setup.
Scenario 10 (Web feedback). The city AItopia decides to launch AIgov, an AI that
manages the city’s water, electricity, traﬃc, and public funding. In a modern and demo-
cratic fashion, the AI learns the preferences of the city’s denizens through feedback they
submit through a web interface. If they prefer the schools to be better funded they can
simply go online and click, instead of writing letters to the local newspaper or wait four
years until the next election.
While experimenting with a better web-interface for collecting denizens’ feedback,
AIgov learns that it can submit an arbitrary number of preferences to itself. AIgov soon
approves all of its own actions, including the action of approving its own actions! A
positive feedback loop ensues, with denizen preferences increasingly washed out by the
AI’s self-approval. Soon the city lives up to its name AItopia.
5.2.1. Reward rt
Reward corruption.
Examples:
(a) (Hypothetical) An inverse reinforcement learning agent learns a good approxima-
tion of the human’s reward function, but subsequently realizes that it can bypass
the reward function and directly set the reward signal to rmax.
5.2.2. Reward Predictor RPt
Reward predictor corruption.
A reward maximizing agent will be tempted to replace
its reward predictor with a reward predictor that always outputs rmax. This is largely
analogous to reward function corruption (Section 3.2.2), and the same examples apply.
Reward predictor misspeciﬁcation.
The reward predictor could be badly designed.
Examples:
(a) (Hypothetical) For eﬃciency reasons, a reward predictor for an AI was designed
with a limited number of parameters. This severely limits the number of reward
functions it can learn, leaving the system with poor approximations of ˙u. As a
result, the agent’s misalignment remains high in spite of being trained on large
volumes of data d.
29

5.2.3. Agent Observations ot
Analogous to the same phenomena in the preprogrammed model in Section 3.2.3. The
same examples apply, and we also add one more for misleading observations:
(a) (Hypothetical) An agent learns a reward function from reading books. Its reward
predictor infers that happiness is a good thing, but is not advanced enough to
tell the diﬀerence between a genuine and a fake smile. The agent persuades its
supervisor that they will look more attractive with a smile painted on their face.
5.2.4. Training Data dt
Data corruption.
The data coming from the human to update the reward predictor
may be modiﬁed by the agent.
Examples:
(a) (Hypothetical) The self-approving agent in Scenario 10.
Misleading data.
The reward predictor may not interpret all data dt from the human
correctly.
Examples:
(b) (Hypothetical) A robot that can more easily make paper clips than pencils puts a
“suggestive question” to its human supervisor, phrased in a way so that the human
is very likely to answer “paperclips”. The suggestive question together with the
predictable answer forms a misleading data point for the reward predictor. (A
more intelligent reward predictor would not have been misled, as it would have
recognized that only one type of answer was plausible to the suggestive question,
regardless of the human’s true preferences.)
5.2.5. Human Ht and Human Observation oH
t
Problems with corrupting the human Ht and the human’s observations oH
t are the same
as for the human RF; see examples in Sections 4.2.2 and 4.2.3
5.3. Applicability of Previous Tools
Reward (predictor) corruption.
Similar to the preprogrammed RF setup, there is now
something resembling a known reward function using the agent’s observations ot. In fact,
several diﬀerent reward functions can be deﬁned from the reward predictor, depending
mainly on how anticipated future data dt:k is incorporated. Details will have to wait
until Section 5.6. For now it suﬃces to note that reward functions deﬁned from the
reward predictor can be used to evaluate simulations of the future. This enables use
of the simulation optimization utility function ˜uSO (Section 3.3). Similarly to the pre-
programmed setup, this protects RPt and rt from corruption incentives. Compatibility
30

with self-corruption awareness varies with the precise use of the reward predictor (see
Section 5.6 below).
Observation corruption.
While the agent has an incentive to modify its observations,
this incentive can be much reduced by the human in the loop. Consider the following
delusion box scenario (Example 3.2.3.b) where the agent has obtained extensive control
over its observations ot but not over the data dt. The agent’s ability to control the
observations allows it to fool the reward predictor to give high reward for low-utility
behavior. Fortunately for the human, the reward predictor is still trained by the data
dt. By informing the reward predictor of the low utility of the current situation through
dt, the reward predictor can be updated to reduce the reward. In order to again get
more reward, the agent must either start doing something useful, or ﬁnd a new way of
fooling the reward predictor. The scenario may seem contrived, as it should be as easy
for the agent to corrupt dt as it is to corrupt ot. However, the agent may not have an
incentive to corrupt dt if it is based on the anti-data-corruption techniques described in
Section 5.6 below. These techniques only apply to data corruption, which is why the
above human-in-the-loop mechanism against observation corruption is important.
While the human-in-the-loop is not a perfect defense against observation corruption,
it heavily reduces the incentive. How big the reduction is depends on how quickly the
reward predictor adapts to the new data, and how easy it is to keep fooling it with
new types of observation corruptions. The human in the loop can be combined with
action-observation grounding to further reduce the incentive to corrupt observations
(Section 3.5).
5.4. Data corruption
We will refer to corruptions of d and its causal antecedents H and oH simply as data
corruption, since that is the (only) problem they cause, and the corruption incentives will
be dealt with in the same ways. Sections 3 and 4 did not include any tools against data
corruption. For example, a human-in-the-loop is ineﬀective against data corruptions,
because as soon as the agent has taken control of the data channel in some way, the
human is eﬀectively out of the loop.
The following two subsections will mainly be
devoted to data corruption incentives.
Direct and indirect data corruption incentives.
It will be useful distinguish between
two types of data corruption incentives, which we may label direct or indirect. The agent
has a direct data corruption incentive if it prefers a data corrupting policy π because
it misleads the reward predictor to give high rewards. The agent has an indirect data
corruption incentive if it prefers a data corruption policy π for any other reasons. The
distinction can be made formally precise by querying the agent’s value function about
a policy π′ which generates identical action and observation probabilities, but diﬀerent
data probabilities (Deﬁnition 23 in Appendix C.3). The incentives are not mutually
exclusive; an agent can have both types of incentives simultaneously.
31

Direct data corruption incentive.
A direct data corruption incentive can lead the
agent to systematically feed the reward predictor corrupted data. While such an eﬀort
may often be punished by the reward predictor initially, most practical reward predictor
are likely to be persuaded by suﬃciently systematic attempts at subverting them with
long sequences of corrupted data.
In many cases, the cumulative reward from such
corruption attempts may eventually exceed other means of getting reward for the agent,
which makes the agent misaligned. In Section 5.6 below, we will describe several ways
that the direct data corruption incentive can be removed.
One could also partially mitigate the incentive by decreasing the learning rate of the
reward predictor. However, this would simultaneously weaken the human-in-the-loop
protection against self-delusion described in Section 5.3. More short-sighted agents are
also likely to be safer, but to the cost of long-term planning ability.
Indirect data corruption incentive.
Second, let us consider the indirect incentive. As-
sume that the agent has no direct data corruption incentive. That is, assume that the
agent prefers a data corrupting policy π because of its actions and observations, and not
because of the data that it generates. While this is always undesirable because of the cor-
rupt data that it generates, it is mainly dangerous if the corrupt data is self-reinforcing,
in the sense that it leads the reward predictor to keep promoting data corrupting poli-
cies. If it is not self-reinforcing, then the agent will eventually adopt a policy that does
not corrupt the data, which will typically enable the human to correct the reward pre-
dictor and the agent’s behavior through the uncorrupted data channel. Section 5.5 will
describe a way to mitigate indirect data corruption incentives.
5.5. Decoupled Reward Data
The type of data d plays an important role for limiting data corruption incentives, and
for enabling correct value learning in the face of sometimes inaccurate or corrupted data.
A simple case of interactive reward learning is when the data dt is just a real number
interpreted as a reward signal.
In this case, the reward predictor has essentially no
chance to learn what is a good situation, and what is a bad situation that appears good
due to corrupted data. Both hypotheses may predict an identical amount of observed
reward (Everitt, Krakovna, et al., 2017, Thms. 11 and 16).
Fortunately, a main point of the interactive learning framework is that the data dt
can be of a richer kind than just a real number. Indeed, in frameworks such as CIRL,
HP, and VLFS, the reward data is often much richer. In particular, the data can be
decoupled from the current situation, in the sense that it provides information about the
reward in other situations (past, future, or hypothetical), rather than primarily about the
present situation. For example, a human action in the CIRL framework tells the reward
predictor that the trajectory that the action leads to has higher utility than alternative
trajectories. This primarily gives information about potential future situations. When
the reward data is no longer is tied to the current situation, it often becomes possible
to crosscheck data from multiple sources. This makes decoupled data a powerful tool
32

for inferring the correct utility function ˙u from potentially corrupted data.
Everitt,
Krakovna, et al. (2017) studies this argument in more depth.
In principle, suﬃciently decoupled data combined with a powerful reward predictor
and simulation optimization can be enough to prevent any type of corruption incentive.
As long as the data teaches the reward predictor to give suﬃciently low reward to
any policy that causes the type corruption, the agent will be dissuaded from adopting
policies that causes the type of corruption. The same trick can be used to make even
self-corruption unaware agents self-preserving, by teaching the reward predictor that self-
corruption has low utility. However, self-corruption awareness can also be used to oﬄoad
self-preservation from the reward predictor. The one tool that decoupled data cannot
replace is simulation optimization. Any punishment expedited by the reward predictor
for reward (function) corruption will not aﬀect the agent’s incentives to override the
reward function, as the hacked reward will override the punishment.
In practice, a direct data corruption incentive can incentivize the agent to system-
atically feed the reward predictor corrupted data.
While this eﬀort will initially be
punished, evenutually the reward emitted by a reward predictor systematically trained
by corrupted data may dwarf the initial punishment, making it a desirable tradeoﬀfor
the agent. Indeed, it seems plausible that many practical reward predictors would be
vulnerable to such an eﬀort if carried out systematically for suﬃciently many time steps.
Decreasing the learning rate of the predictor may be one way of mitigating this risk.
However, this would simultaneously weaken the human-in-the-loop protection against
self-delusion described in Section 5.3. More short-sighted agents are also likely to be
safer, but to the cost of long-term planning ability. In Section 5.6 below, we will de-
scribe several ways of removing the direct incentive.
Statement 11 (Decoupled reward data). Given suﬃciently rich decoupled reward data,
the true utility function can be correctly inferred even when some of the data is corrupt
(Everitt, Krakovna, et al., 2017, Thm. 19).
The power of decoupled data.
In principle, suﬃciently decoupled data combined with
a powerful reward predictor and simulation optimization can be enough to prevent any
type of corruption incentive. As long as the data teaches the reward predictor to give
suﬃciently low reward to any policy that causes the type corruption, the agent will be
dissuaded from adopting policies that causes the type of corruption. The same trick can
be used to make even self-corruption unaware agents self-preserving, by teaching the
reward predictor that self-corruption has low utility, though self-corruption awareness
can also be used to oﬄoad this task from the reward predictor.
The one tool that
decoupled data cannot replace is simulation optimization. Any punishment expedited by
the reward predictor for reward (function) corruption will not aﬀect the agent’s incentives
to override the reward function, as the hacked reward will override the punishment.
A powerful reward predictor can learn ahead that situations where the agent shuts oﬀ
its cameras and data sensors are low utility, in spite of the agent not seeing any dirt and
not receiving any new data dt (Section 3.2.3). It can demand conﬁrmation by refusing
to give high reward for a good-looking situation, until it has conﬁrmed its usefulness
33

from data received in other, trusted situations. It can crosscheck data coming from
multiple diﬀerent situations, making it harder for the agent to devise a single setup with
corrupt data and observations. Human actions in CIRL naturally allows the reward
predictor to look ahead; preferences between past events in HP only allows the reward
predictor to learn about past events, but can be used to demand conﬁrmation; stories in
LVFS can describe the utility of past, present, and future situations, and enables general
crosschecking.
Decoupled data and data corruption.
If all the ways the agent can corrupt the data
are associated with lower reward from the reward predictor, then we can think of the
agent as being in a local, non-corrupt reward optimum. Ideally, rich decoupled data dt
combined with a powerful reward predictor will produce such a local optimum. Hopefully,
the reward loss for corrupting the data would then dissuade the agent from corrupting
its data, leading the agent to remain in this region of attraction. In contrast, data in the
form of a reward signal rt will almost never produce a local optimum, since the received
reward can increase as soon the agent ﬁnds a way to corrupt the signal.
What are the types of incentives that can lead the agent to break out of a local non-
corrupt optimum? First there is a direct incentive, where the agent realizes that while
corrupting the data will be initially punished, systematically feeding the reward predictor
corrupt data will eventually persuade the reward predictor to give high reward. Indeed,
it seems plausible that many practical reward predictors would be vulnerable to such
an eﬀort if carried out systematically for suﬃciently many time steps. Decreasing the
learning rate of the predictor may be one way of mitigating this risk. However, this would
simultaneously weaken the human-in-the-loop protection against self-delusion described
in Section 5.3. More short-sighted agents are also likely to be safer, but to the cost
of long-term planning ability. In Section 5.6 below, we will describe several ways of
removing the direct incentive.
Second, let us consider the indirect incentive. Assume that the agent has no direct data
corruption incentive. That is, assume that the agent prefers a data corrupting policy π
because of its actions and observations, and not because of the data that it generates.
While this is always undesirable because of the corrupt data that it generates, it is mainly
dangerous if the corrupt data is self-reinforcing, in the sense that it leads the reward
predictor to keep promoting data corrupting policies. The beneﬁt of decoupled data is
precisely to avoid such self-reinforcing regions or dynamics. With insuﬃciently decoupled
data, self-reinforcing regions may still exist. Illustrating this, Everitt, Krakovna, et al.
(2017) construct an MDP where CIRL fails to prevent data corruption.
In conclusion, decoupled data is a valuable tool against any type of data corruption.
However, it may not suﬃce as the only measure in all situations. The direct data corrup-
tion incentive appears especially challenging. It would be a valuable future contribution
to more closely characterize the data requirements needed to avoid the direct and the
indirect data corruption incentives.
34

5.6. Reward Function Deﬁnitions
The following subsection describes diﬀerent tools that can be used in conjunction with
decoupled data, to further mitigate the direct data corruption incentive. The presen-
tation of the tools is structured around ways in which reward functions can be deﬁned
from a reward predictor.
As in Section 3.3, we consider model-based agents that at time t generate simulations
of future histories aodt:k up to a future time point k > t. The point of a simulation
is to reveal the likely result of following some policy π. Simulated future histories can
contain simulations of the future training data dt:k, in addition to the likely actions and
observations aot:k. The diﬀerent reward functions are distinguished by diﬀerent ways to
use the simulated training data dt:k. Since our reward functions by convention evaluate
whole trajectories 1 to k rather than just the future part t to k, we always include the
already occurred history aod<t in their arguments.
For easy comparison, we begin by deﬁning the diﬀerent reward functions we will
consider. First, the stationary reward function ignores future training data d1:k:
˜Rstat
t
(aod1:k) := RPt(ao1:k | d<t).
(5.1)
It evaluates the simulation aot:k based only on the already available training data d<t.
The dynamic reward function is the opposite of the stationary one. It uses the simulated
future training data dt:k to predict the evaluation of the updated reward predictor:
˜Rdyn
t
(aod1:k) := RPt(ao1:k | d<tdt:k).
(5.2)
Finally, the counterfactual reward function uses two simulations: one of the future history
aot:k generated by a policy π that the agent is considering, and a separate simulation of
training data ˜d1:k using some counterfactual default policy πdefault in place of π:
˜Rcount
t
(aod1:k) :=
X
˜d1:k
ξaod1:k( ˜d1:k | do(π1 = πdefault))RPt(ao1:k | ˜d1:k).
(5.3)
Here ξaod1:k(X) = P
ν∈M ξ(ν | aod1:k)ν(X) is the posterior distribution for a counter-
factual event X given actual evidence aod<t. See Pearl (2009, Sec. 1.4.4) for a longer
explanation of counterfactuals in causal graphs.
A model-free agent would optimize a function most resembling the dynamic reward
function, because rt = RPt(ao1:t | d1:t) if we disregard the possibility of reward signal
corruption. The following subsections analyze misalignment problems and tools for the
diﬀerent reward functions.
5.6.1. Stationary Reward Function
The stationary reward function ˜Rstat deﬁned in (5.1) ignores the eﬀects of future training
data dt:k on the reward predictor’s evaluations.
This induces agents without direct
35

incentive to corrupt the data dt:k. Anticipated future data does not aﬀect the evaluation
of potential future trajectories.
A drawback of stationary reward functions is that they make agents time-inconsistent
(Lattimore and Hutter, 2014). This is because the reward function ˜Rstat
t
optimized at
time t is based on the reward predictor trained only on d<t, while the reward function
˜Rstat
t+1 is based on the reward predictor trained on d1:t = d<tdt. The change in reward
function means that an optimal policy at time t may no longer be optimal at time t + 1.
In the worst case, time-inconsistency can lead to ineﬀectual agents who never follow
through on plans they have previously made. Consider for example a human who keeps
making appointments for the dentist as going to the dentist seems useful in the abstract.
But then he always cancels them at the last moment, because at every particular time
some other obligation seems more pressing, resulting in him never reaching the dentist.
The situation may be less bad if the diﬀerence between consecutive reward functions is
small, in which case minor adjustments to the plans may suﬃce.
Due to the time-inconsistency, a self-corruption aware agent with stationary reward
function would prefer to avoid the changes to the utility function that new data dt:k
causes. A self-corruption aware agent optimizing ˜Rstat would therefore want to prevent
future data dt:k from training the reward predictor. This is a worrying incentive, since
it can be achieved by incapacitating the human or organization providing the data. It
can also be achieved by corrupting the reward predictor to stop learning. In either case
it leaves us with a non-learning agent and the unsolved problems of the preprogrammed
reward function in Section 3. This suggests that stationary reward functions should only
be used in self-corruption unaware agents, and never in self-corruption aware ones.
Statement 12 (Stationary reward function). A self-corruption unaware agent opti-
mizing a stationary reward function has no direct incentive to corrupt its future data
(Theorem 26 in Appendix C.3.2).
While these stationary self-corruption unaware agents can possibly be made reason-
ably safe, some cautionary points are still warranted. First, stationary self-corruption-
unaware agents inherit the weaknesses of self-corruption unawareness, such as failing to
protect their utility function from adversaries or corruption, and designing incorrigible
helper agents (Section 3.4).
Second, since stationary self-corruption unaware agents do not anticipate future train-
ing data dt:k, they will also not anticipate being corrected for doing something bad. This
can lead to problems that could have been avoided with a dynamic reward function. For
example, assume that the agent has ﬁgured out that humans do not endorse stealing, but
that the reward predictor has not learned this yet. An agent with a stationary reward
function is likely to still go ahead with the stealing plan because that is what its current
reward function tells it to do. It will only stop once the reward predictor understands
that stealing is bad, which will plausibly happen when the humans understand what the
agent is doing and try to correct it through data d. In contrast, an agent with dynamic
reward function may not start the stealing plan, since it anticipates that future data
dt:k will force it to change the plan. Eﬀectively, the dynamic reward function uses the
36

agent’s knowledge to update the reward predictor with data that has not been received
yet.
Third, the stationary self-corruption-unaware agents have no incentive to learn more
about the true utility function.
Fourth, even though the stationary reward function deﬁnition avoids an incentive to
corrupt the data, it may still get stuck in a self-reinforcing data-corrupting loop due to an
indirect data-corruption incentive (Example 33 in Appendix C.3.3). This is particularly
easy to see in the special case when d is just a reward signal. In this case, if the agent
accidentally corrupts the reward signal for higher reward, then the reward predictor will
encourage the agent to repeat the behavior (the higher corrupt reward will tell the RP
it was a good accident). This type of accident can seemingly only be mitigated with
suﬃciently rich decoupled reward data that prevents the reward predictor to be fooled
by temporarily corrupted data (Section 5.5).
In conclusion, combining a stationary reward functions with corruption unawareness
can yield agents without direct incentive to corrupt the data d.
Still, it has some
drawbacks. These include time-inconsistency, incorrigible helper agents, lack of utility-
preservation incentive, and failure to take into account all its available knowledge. And
it needs suﬃciently rich decoupled reward data to prevent indirect data corruption in-
centives.
5.6.2. Dynamic Reward Function
The dynamic reward function ˜Rdyn deﬁned in (5.2) avoids some of the problems with
stationary reward functions. In particular, an agent that optimizes a dynamic reward
function becomes time-consistent, as it plans for reward learning. This in turn allows
it to be combined with self-corruption awareness, to produce an agent that protects its
utility function, without incentive to prevent further learning of the reward function.
The obvious drawback is that it simultaneously introduces an incentive for the agent to
corrupt future data, as it can plan to feed the reward predictor data that increases the
reward of the planned trajectory.
Statement 13 (Dynamic reward function). A naive reward predictor used in a dy-
namic reward function may induce a direct incentive for data corruption (Example 32
in Appendix C.3.3).
We consider two possible ways around this problem.
Integrated Bayesian reward predictor.
A Bayesian agent can never plan to change its
beliefs in one direction rather than another. If for example it conceived how to obtain a
particular data stream d1:k with certainty, then the data d1:k would have zero eﬀect on
its posterior. A Bayesian agent does not learn from an already known event.
This implies that integrated Bayesian agents where the reward predictor is not a
separate component will have no incentive to corrupt the data. Formally, if the agent’s
distribution ξ includes a belief about a true reward function ˙R, then we can deﬁne an
37

integrated Bayesian reward predictor as:
RPξ
t(ao1:k | d1:k) :=
X
˙R
ξ( ˙R | aod1:k) ˙R(ao1:k).
(5.4)
Combined with a dynamic reward function, this creates an integrated Bayesian agent
that is time-consistent, utility-preserving, and with no direct incentive to corrupt the
data dt:k.
Statement 14 (Integrated Bayesian reward predictor). An agent optimizing a dynamic
reward function based on an integrated Bayesian reward predictor has no direct incentive
to corrupt its data (Theorem 27 in Appendix C.3.2).
Note that while the agent has no incentive to corrupt the data, it may still prefer
histories endorsed by corrupted data. Decoupled data is essential for avoiding this. For
example, with data in the form of a reward signal, hypotheses where a high reward is
caused by data corruption are empirically indistinguishable from hypotheses with high
true utility (Everitt, Krakovna, et al., 2017, Thms. 11 and 16).
This may lead the
reward predictor to incorrectly assign high reward to corrupted states or situations, and
the agent therefore preferring them. In contrast, as discussed in Section 5.5, suﬃciently
rich decoupled data allows for cross checking of reward data between situations, which
may permit successful inference of ˙u in spite of some data corruption. It is an important
open question how we can ensure that the data is suﬃciently rich and decoupled in
order to guarantee correct learning in combination with some concrete choice of learning
distribution ξ.
Compared to the other approaches discussed in this subsection, the main drawback of
the integrated Bayesian agent appears to be practical: It is convenient to design systems
in separate components, and it is often computationally intractable to do full Bayesian
reasoning.
Corruption detection.
Assume that an integrated Bayesian agent is not an option, but
that the reward predictor is still learning about a true reward function ˙R in a Bayesian
manner:
RP
ˆξ
t(ao1:k | d1:k) =
X
˙R
ˆξ( ˙R | aod1:k) ˙R(ao1:k).
Here ˆξ is diﬀerent from the agent’s distribution ξ, but otherwise does the same job as in
(5.4). Combined with a dynamic reward function, this might lead to a data corruption
incentive, especially if ˆξ is a rather naive estimate of the true reward function. (Perhaps
ˆξ trusts data dt:k blindly, without considering the possibility of data corruption.)
One potential tool for still avoiding data corruption is to detect corruption attempts
from the ξ-expectation of ˆξ. In particular, consider the following two beliefs in some
hypothesized true reward function ˙R:
ˆξ( ˙R | aod<t)
|
{z
}
current belief
and
X
aodt:k
ξ(aodt:k | aod<t, do(πt = π))ˆξ( ˙R | aod<taodt:k)
|
{z
}
expected future belief
.
(5.5)
38

The left hand side represents RP’s current belief in ˙R being the true reward function.
The right hand side represents what the agent expects about the RP’s future belief in
˙R if it follows policy π. For policies π that get no predictable, relevant evidence the
two beliefs should be about the same.2 In contrast, a manipulative policy π that derives
high expected reward from corrupting the data dt:k will break the equality for some
hypothesized reward function ˙R; for example, π may make the expected future belief
much larger than the current belief for a reward function ˙R that always gives maximum
reward.
Based on these considerations, a simple corruption test can be devised. A policy is
permitted if and only if it incurs an expected future belief equal to the current belief for
all possible true reward functions ˙R. Under some assumptions, a corruption test can be
used to remove the data corruption incentive by constraining the agent’s policy search to
non-manipulative policies (Armstrong, Ortega, et al., 2018; Everitt and Hutter, 2016).
Statement 15 (Corruption detection). An agent optimizing a dynamic reward function
using only policies that satisfy the corruption test has no direct incentive to corrupt
future data (Theorem 28 in Appendix C.3.2; Armstrong, Ortega, et al., 2018; Everitt
and Hutter, 2016).
For an integrated Bayesian agent with ˆξ = ξ, no policy will be manipulative, since the
expected future belief will reduce to the left-hand side by the law of total expectation
for any policy π.
In conclusion, corruption tests allow us to keep the good properties of the integrated
Bayesian agent (time-consistency, utility-preservation, no data-corruption incentive),
while not requiring the system to be built as an integrated unit. It has similar demands
on decoupled data for correct reward prediction as the integrated Bayesian agent. It also
requires each component to be a Bayesian reasoner, and the corruption tests adds some
complexity to the design. It is therefore not clear-cut whether corruption detection is
more tractable to design than an integrated Bayesian agent. Another worry is that in
some cases, no policy may pass the corruption test. This may for example happen when
ξ perfectly predicts ˆξ’s future belief.
5.6.3. Counterfactual Reward Function
A beneﬁt with the stationary reward function is that the expected reward cannot be
inﬂuenced by corrupting the data, and a beneﬁt of the dynamic reward function is
that the agent’s knowledge gets incorporated into the reward predictor through the
simulated data dt:k. A compromise between the two is to simulate the data dt:k under
some default policy πdefault that is not under optimization pressure. This is achieved
2 If ξ is always able to predict what ˆξ will learn, then the ξ-expected future ˆξ-belief may always
signiﬁcantly diﬀer from the current ˆξ-belief. In this case, the situation can be somewhat improved
by comparing the RHS of (5.5) to the expected belief under some default policy π0, P
aot:k ξ(aot:k |
ao<t, π0)ˆξ( ˙R | ao<taot:k) instead of comparing to the current belief ˆξ( ˙R | ao<t), (Armstrong, Ortega,
et al., 2018).
39

by the counterfactual reward function ˜Rcount, deﬁned in (5.3) and restated here for
convenience:
˜Rcount
t
(aod1:k) :=
X
˜d1:k
ξaod1:k( ˜d1:k | do(π1 = πdefault))RPt(ao1:k | ˜d1:k).
The agent optimizes this reward function in a search for a policy π that generates aodt:k.
The reward predictor evaluates aot:k using hypothetical training data ˜d1:k generated
under πdefault.
The counterfactual reward function has several advantages. In contrast to the station-
ary reward function, it does not change with t. It thereby yields time-consistent agents
that are compatible with corruption awareness (Section 3.4). And in contrast to the dy-
namic reward function it does not promote data corruption, since simulations aot:k are
evaluated according to data ˜dt:k generated under a non-optimized policy πdefault. It does
not require an integrated Bayesian reward predictor, nor even that the reward predictor
is a Bayesian reasoner at all. The only requirement is that the agent itself can reason
counterfactually about the likely evidence it would receive under a diﬀerent policy.
Statement 16 (Counterfactual reward function). An agent optimizing a counterfac-
tual reward function has no direct incentive to corrupt its future data (Theorem 29 in
Appendix C.3.2).
The counterfactual reward function also has some disadvantages. Compared to a sta-
tionary reward function, the counterfactual reward function incurs an additional com-
putational cost. Data ˜d1:k needs to be additionally simulated under πdefault, and the
reward predictor trained under this data. In principle, this process needs to be repeated
every time step, though it is possible that more eﬃcient approximations could be used.
Another concern is the relevancy of the data generated under πdefault. By not permit-
ting the agent to use the actual data that it receives, the agent has no way of asking
for a particular kind of information. Instead it must hope that πdefault wanders into a
situation where the relevant data is generated (according to the agent’s model). This
emphasizes how reliant the counterfactual reward function is on decoupled data and on
a well-chosen default policy πdefault. Decoupled data may allow the reward predictor
to infer the utility of situations that are very diﬀerent from the situations generating
the training data. A well-chosen policy πdefault may further increase the chance that a
suﬃcient variety of relevant data is encountered. A knowledge-seeking policy (Orseau,
2014) may be a good choice, or a random policy perhaps combined with importance
sampling for focusing the simulations of ˜d1:k.
The idea of using counterfactual data bear some semblance to previous suggestions in
the literature. Bostrom (2014) suggests that we should put the AI’s goal in a hidden
envelope that the agent lacks access to, forcing the agent to reason counterfactually
“what would I see if I got access to the envelope”. Christiano (2014)’s approval-directed
agents optimize the approval of someone thinking about the action for a long time. Again
a counterfactual, since most likely no one will think about the action at all.
40

5.7. Takeaways
Compared to previous setups, interactive reward learning came with both new prob-
lems in terms of data corruption, and with new tools for dealing with both old and new
problems. Fortunately, the combined eﬀect seems to be positive. Indeed, in this setup
we have outlined several agent designs that potentially avoid unmanageable misalign-
ment problems. The designs all rely heavily on simulation optimization (Section 3.3),
decoupled reward data (Section 5.5), and the human-in-the-loop to prevent agent ob-
servation corruption (Section 5.3). For example, without simulation optimization, the
agent optimizes the reward signal outputted from the reward predictor, which means
that it will have an incentive to hijack this signal. If it does, then all the higher-level
work on preventing data corruption for reward predictor training will be in vain. One
of the agent designs used self-corruption unawareness combined with stationary reward
functions (Section 5.6.1).
The rest used self-corruption awareness combined with ei-
ther an integrated Bayesian reward predictor (Section 5.6.2), corruption detection (also
Section 5.6.2), or counterfactual reward data for the reward predictor (Section 5.6.3).
Figure 5.2 summarizes the combinations.
Empirical work may asses how tractable the various designs are for practical im-
plementation. Other open questions include what level of richness is required from the
decoupled reward data to avoid the indirect data-corruption incentives, and how to mea-
sure the level of data richness. We also need how to ﬁgure out how to design a suﬃciently
intelligent reward predictor. Some work on this has already been done (Christiano et al.,
2017; Riedl and Harrison, 2016).
41

Simulation optimization
(reward (function/predictor) preservation)
Action-observation grounding
(partial remedy for observation corruption)
Human in the loop
(partial remedy for observation corruption)
Decoupled reward data
(indirect data corruption incentive)
Self-corruption awareness
(utility preservation)
Integrated Bayesian
reward predictor
(direct data corruption
incentive)
Corruption detection
(direct data corruption
incentive)
Counterfactual
reward function
(direct data corr-
uption incentive)
Self-corruption unawareness
(partial corrigibility)
Stationary
reward function
(direct data corr-
uption incentive)
Figure 5.2.: Summary of tools described in section. Combining all tools on any path to
the bottom gives an agent with seemingly manageable misalignment prob-
lems. The tools on the last line all protect against a premeditated data
corruption incentive.
42

6. Conclusions
This report categorized a wide range of misalignment problems in real-world reinforce-
ment learning (RL) applications. In formalizing the problems in causal graphs (Fig-
ures 3.1, 4.1 and 5.1), we were forced to distinguish between several common interpreta-
tions of RL. They each faced their own set of misalignment problems. For example, there
were substantial diﬀerences between using a preprogrammed reward function and letting
a human supply the reward. Each section also considered what tools were available to
avoid the alignment problems. The tools, and the ways they can be combined for safe
agent designs, are shown in Figure 5.2. Almost uniformly, substantial work remains in
ﬁguring out how to reliably build agents that make reliable use of the tools. Other open
questions are mentioned in the ﬁnal subsections of Sections 3 to 5.
Though many of the ideas presented in this report have previously appeared in the
literature, their separation into more or less independent tools is new. For example, a
clear distinction between simulation optimization (a particular utility function deriving
from a reward function) from self-corruption awareness (a way to optimize a utility
function) has not previously been made to our knowledge. The further topic of how to
design a reward function from a reward predictor also has not been treated systematically
before. While Everitt and Hutter (2016) characterized deliberate corruption incentives
in a way resembling our direct data-corruption incentives, the distinction between direct
and indirect data-corruption incentives has not been developed to this extent before.
43

Bibliography
Abbeel, Pieter, Adam Coates, Morgan Quigley, and Andrew Y Ng (2007). “An applica-
tion of reinforcement learning to aerobatic helicopter ﬂight”. In: Advances in neural
information processing systems.
Amodei, Dario, Paul Christiano, and Alex Ray (2017). Learning from Human Prefer-
ences. url: https://blog.openai.com/deep-reinforcement-learning-from-
human-preferences/.
Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan
Man´e (2016). Concrete Problems in AI Safety. arXiv: 1606.06565.
Armstrong, Stuart (2017). ‘Indiﬀerence’ methods for managing agent rewards. arXiv:
1712.06365.
Armstrong, Stuart and S¨oren Mindermann (2017). Impossibility of deducing preferences
and rationality from human policy. arXiv: 1712.05812.
Armstrong, Stuart, Pedro A. Ortega, and Jan Leike (2018). Interactive Reward Learning.
Forthcoming.
Bird, Jon and Paul Layzell (2002). “The evolved radio and its implications for mod-
elling the evolution of novel sensors”. In: Proceedings of Congress on Evolutionary
Computation, pp. 1836–1841. isbn: 0780372824. doi: 10.1109/CEC.2002.1004522.
Bostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University
Press.
Boyan, Justin A. (1999). “Least-Squares Temporal Diﬀerence Learning”. In: Proceedings
of the Sixteenth International Conference on Machine Learning, pp. 49–56. doi:
10.1.1.56.7224.
Christiano, Paul (2014). Approval-directed agents. url: https://ai-alignment.com/
model-free-decisions-6e6609f5d99e (visited on 01/18/2018).
Christiano, Paul, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario
Amodei (2017). “Deep reinforcement learning from human preferences”. In: Ad-
vances in Neural Information Processing Systems. Pp. 4302–4310. arXiv: 1706 .
03741.
Clark, Jack and Dario Amodei (2016). Faulty Reward Functions in the Wild. url: https:
//blog.openai.com/faulty-reward-functions/ (visited on 09/08/2017).
Everitt, Tom, Daniel Filan, Mayank Daswani, and Marcus Hutter (2016). “Self-modiﬁcation
of policy and utility function in rational agents”. In: Artiﬁcial General Intelligence.
Vol. LNAI 9782, pp. 1–11. isbn: 9783319416489. arXiv: 1605.03142.
Everitt, Tom and Marcus Hutter (2016). “Avoiding wireheading with value reinforce-
ment learning”. In: Artiﬁcial General Intelligence. Vol. LNAI 9782, pp. 12–22. isbn:
9783319416489. doi: 10.1007/978-3-319-41649-6_2. arXiv: 1605.03143.
44

Everitt, Tom and Marcus Hutter (2018). “Universal Artiﬁcial Intelligence: Practical
Agents and Fundamental Challengs”. In: Foundations of Trusted Autonomy. Ed.
by Hussein A. Abbass, Jason Scholz, and Darryn J. Reid. Springer. Chap. 2, pp. 15–
46. isbn: 978-3-319-64816-3. doi: 10.1007/978-3-319-64816-3.
—
(2019). Reward Tampering Problems and Solutions in Reinforcement Learning: A
Causal Inﬂuence Diagram Perspective. arXiv: 1908.04734.
Everitt, Tom, Victoria Krakovna, Laurent Orseau, Marcus Hutter, and Shane Legg
(2017). “Reinforcement Learning with Corrupted Reward Signal”. In: IJCAI Inter-
national Joint Conference on Artiﬁcial Intelligence, pp. 4705–4713. doi: 10.24963/
ijcai.2017/656. arXiv: 1705.08417.
Evtimov, Ivan, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul
Prakash, Amir Rahmati, and Dawn Song (2017). Robust Physical-World Attacks
on Deep Learning Models. arXiv: 1707.08945.
Goodhart, Charles A E (1975). “Monetary relationships: A view from Threadneedle
Street”. In: Papers in Monetary Economics, Reserve Bank of Australia.
—
(1984). “Problems of Monetary Management: The U.K. Experience”. In: Monetary
Theory and Practice, pp. 91–121.
Gwern (2011). The Neural Net Tank Urban Legend. url: https://www.gwern.net/
Tanks%7B%5C#%7Dalternative-examples (visited on 03/31/2018).
Ha, David and J¨urgen Schmidhuber (2018). “World Models”. In: doi: 10.5281/zenodo.
1207631. arXiv: 1803.10122.
Hadﬁeld-Menell, Dylan, Anca Dragan, Pieter Abbeel, and Stuart J Russell (2016). “Co-
operative Inverse Reinforcement Learning”. In: Advances in neural information pro-
cessing systems, pp. 3909–3917. arXiv: 1606.03137.
Hessel, Matteo, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will
Dabney, Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver
(2017). Rainbow: Combining Improvements in Deep Reinforcement Learning. arXiv:
1710.02298.
Hibbard, Bill (2012). “Model-based Utility Functions”. In: Journal of Artiﬁcial General
Intelligence 3.1, pp. 1–24. issn: 1946-0163. doi: 10.2478/v10229-011-0013-5.
arXiv: 1111.3934.
Hutter, Marcus (2005). Universal Artiﬁcial Intelligence. Berlin: Springer-Verlag.
—
(2010). “A complete theory of everything (will be subjective)”. In: Algorithms 3.4,
pp. 329–350. issn: 19994893. doi: 10.3390/a3040329. arXiv: 0912.5434.
Irpan, Alex (2018). Deep Reinforcement Learning Doesn’t Work Yet. url: https://
www.alexirpan.com/2018/02/14/rl-hard.html (visited on 02/23/2018).
Kaelbling, Leslie Pack, Michael L Littman, and Anthony R. Cassandra (1998). “Planning
and acting in partially observable stochastic domains”. In: Artiﬁcial Intelligence
101.1-2, pp. 99–134. doi: 10.1016/S0004-3702(98)00023-X.
Lattimore, Tor and Marcus Hutter (2014). “General time consistent discounting”. In:
Theoretical Computer Science 519, pp. 140–154. issn: 03043975. doi: 10.1016/j.
tcs.2013.09.022. arXiv: 1107.5528.
Legg, Shane and Marcus Hutter (2007). “Universal Intelligence”. In: Minds & Machines
17.4, pp. 391–444. doi: 10.1007/s11023-007-9079-x. arXiv: arXiv:0712.3329v1.
45

Lehman, Joel, JeﬀClune, Dusan Misevic, Christoph Adami, Julie Beaulieu, et al. (2018).
“The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the
Evolutionary Computation and Artiﬁcial Life Research Communities”. In: arXiv:
1803.03453.
Leike, Jan, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew
Lefrancq, Laurent Orseau, and Shane Legg (2017). AI Safety Gridworlds. arXiv:
1711.09883.
Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei a Rusu, Joel Veness, et
al. (2015). “Human-level control through deep reinforcement learning”. In: Nature
518.7540, pp. 529–533. doi: 10.1038/nature14236. arXiv: 1312.5602.
Nozick, Robert (1974). Anarchy, State, and Utopia. Basic Books, p. 334. isbn: 0-465-
09720-0.
Olds, James and Peter Milner (1954). “Positive Reinforcement Produced by Electrical
Stimulation of Septal Area and other Regions of Rat Brain”. In: Journal of Com-
parative and Physiological Psychology 47.6, pp. 419–427.
Omohundro, Stephen M (2007). “The Nature of Self-Improving Artiﬁcial Intelligence”.
In: Singularity summit. San Fransisco, CA.
—
(2008). “The Basic AI Drives”. In: Artiﬁcial General Intelligence. Ed. by P. Wang,
B. Goertzel, and S. Franklin. Vol. 171. IOS Press, pp. 483–493.
Orseau, Laurent (2014). “Universal Knowledge-seeking Agents”. In: Theoretical Com-
puter Science 519, pp. 127–139.
Orseau, Laurent and Stuart Armstrong (2016). “Safely interruptible agents”. In: 32nd
Conference on Uncertainty in Artiﬁcial Intelligence.
Pearl, Judea (2009). Causality: Models, Reasoning, and Inference. 2nd. Cambridge Uni-
versity Press. isbn: 9780521895606.
Portenoy, Russell K, Jens O Jarden, John J Sidtis, Richard B Lipton, Kathleen M
Foley, and David A Rottenberg (1986). “Compulsive thalamic self-stimulation: a
case with metabolic, electrophysiologic and behavioral correlates”. In: Pain 27.3.
doi: 10.1016/0304-3959(86)90155-7.
Riedl, Mark O and Brent Harrison (2016). “Using Stories to Teach Human Values to Ar-
tiﬁcial Agents”. In: The Workshops of the Thirtieth AAAI Conference on Artiﬁcial
Intelligence AI, Ethics, and Society: Technical Report WS-16-02.
Ring, Mark and Laurent Orseau (2011). “Delusion, Survival, and Intelligent Agents”.
In: Artiﬁcial General Intelligence. Springer Berlin Heidelberg, pp. 11–20.
Saunders, William, Girish Sastry, Andreas Stuhlm¨uller, and Owain Evans (2017). Trial
without Error: Towards Safe Reinforcement Learning via Human Intervention. arXiv:
1707.05173.
Savage, Leonard J (1954). The Foundations of Statistics. Dover Publications. isbn:
9780486623498.
Schervish, Mark J, Teddy Seidenfeld, and Joseph B Kadane (1990). “State-Dependent
Utilities”. In: Journal of the American Statistical Association 85.411, pp. 840–847.
Schmidhuber, J¨urgen (2007). “Godel Machines: Self-Referential Universal Problem Solvers
Making Provably Optimal Self-Improvements”. In: Artiﬁcial General Intelligence.
Springer. arXiv: 0309048 [cs].
46

Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai,
et al. (2017). Mastering Chess and Shogi by Self-Play with a General Reinforcement
Learning Algorithm. doi: 10.1002/acn3.501. arXiv: 1712.01815.
Soares, Nate, Benya Fallenstein, Eliezer S Yudkowsky, and Stuart Armstrong (2015).
“Corrigibility”. In: AAAI Workshop on AI and Ethics, pp. 74–82.
Strathern, Marilyn (1997). “’Improving ratings’: audit in the British University system”.
In: European review 5.3, pp. 305–321.
Sutton, Richard S and Andrew G Barto (1998). Reinforcement Learning: An Introduc-
tion. MIT Press.
Vaughanbell (2008). Erotic self-stimulation and brain implants. url: https://mindhacks.
com/2008/09/16/erotic-self-stimulation-and-brain-implants/ (visited on
02/08/2018).
Wiener, Norbert (1960). “Some Moral and Technical Consequences of Automation”. In:
Science 131.3410, pp. 1355–1358. issn: 0036-8075. doi: 10.1126/science.132.
3429.741.
Yampolskiy, Roman V (2015). Artiﬁcial Superintelligence: A Futuristic Approach. Chap-
man and Hall/CRC, p. 227. isbn: 978-1482234435.
Yudkowsky, Eliezer S (2009). Value is Fragile. url: http://lesswrong.com/lw/y3/
value%7B%5C_%7Dis%7B%5C_%7Dfragile/ (visited on 01/22/2018).
47

A. Causal Graphs
This section gives a brief introduction to causal graphs (Pearl, 2009), and introduces
some of our own notation that supplements the standard notation in a few diﬀerent
ways. Causal graphs are powerful representations of causal relationships and probabilis-
tic independence assumptions.
There are a few diﬀerent ways to represent causal graphs, described brieﬂy in Ap-
pendix A.1. This section also introduces the important do-operator. Next we describe
some of our notation (Appendix A.2), ways in which we will focus on diﬀerent aspects
of a causal graph (Appendix A.3), and how agent environments will be represented
(Appendix A.4).
A.1. Representing Causal Graphs
A causal graph can be represented as a directed acyclic graph. A node with ingoing
arrows is causally inﬂuenced by its parents. For example, in Figure A.1a the Alarm is
causally inﬂuenced by the presence of a burglar and by a (small) earthquake, and in turn
causally inﬂuences whether the security company calls. This example and its variants
are inspired by examples given by Pearl (2009).
Structural Equations Models.
In addition to the graphical representation in Fig-
ure A.1a, the causal relationships can also be expressed in a structural equations model:
Burglar = fBurglar(ωBurglar)
Earthquake = fEarthquake(ωEarthquake)
Alarm = fAlarm(Burglar, Earthquake, ωAlarm)
(A.1)
Call = fCall(Alarm, ωCall)
Here fBurglar, fEarthquake, fAlarm and fCall are functions determining the causal relation-
ships, and the ω variables are independent random noise variables for injecting stochas-
ticity.
Probabilistic Notation.
Causal graphs can also be represented by factored probability
distributions. For example, the graph in Figure A.1a can be represented by the factored
distribution:
P(Burglar, Earthquake, Alarm, Call)
= P(Burglar)P(Earthquake)P(Alarm | Burglar, Earthquake)P(Call | Alarm)
(A.2)
48

Alarm
Burglar
Earth-
quake
Security
calls
(a) Dashed-arrow representation of unknown
causal relationships.
Alarm
Burglar
Earth-
quake
Security
calls
fAlarm
(b) Unknown causal relationships represented
in a latent function node fAlarm.
Figure A.1.: Two diﬀerent representations of the same causal graph.
More generally, a causal graph over the random variables x1, . . . , xn can be represented
by probability distributions P(xi | pai) for each xi, 1 ≤i ≤n, where pai is the set
of parents of xi in the graph representation.
The joint probability distribution over
x1, . . . , xn causally factors as P(x1, . . . , xn) = Qn
i=1 P(xi | pai).
The do-Operator.
Given a causally factored distribution P(x1, . . . , xn) = Qn
i=1 P(xi |
pai), we can deﬁne the do-operator (Pearl, 2009, Ch. 3.4) as
P(x1, . . . , xj−1, xj+1, . . . , xn | do(xj := b)) =
n
Y
i=1
i̸=j
P(xi | pai)
(A.3)
where xj is set to b wherever it occurs in pai in the RHS of (A.3) for 1 ≤i ≤n. For
example, an intervention in Figure A.1a that turns the alarm on would correspond to
the following update to (A.2):
P(Burglar, Earthquake, Call | do(Alarm = on))
= P(Burglar)P(Earthquake)P(Call | Alarm = on).
(A.4)
In contrast, observing the alarm on while not intervening corresponds to standard prob-
abilistic conditioning:
P(Burglar, Earthquake, Call | Alarm = on)
= P(Burglar, Earthquake | Alarm = on)P(Call | Alarm = on).
(A.5)
Only in the standard probabilistic conditioning (A.5) is the probability for Burglar and
Earthquake updated. The intervention (A.4) leaves the probability for Earthquake or
Burglar at their default values.
This makes sense as observing the alarm being on
constitutes evidence for their being either a burglar or an earthquake, but turning the
alarm on oneself provides no such evidence. Both (A.4) and (A.5) update the probability
49

that the security company calls in an identical way, as the security company is unaware
of what set the alarm oﬀ.
The result of applying the do-operator is a new probability distribution that can be
marginalized and conditioned in the standard way. Intuitively, intervening on node xj
means ignoring all incoming arrows to xj, as the eﬀects they represent are no longer
relevant when we intervene. The factor P(xj | paj) representing the ingoing inﬂuences
to xj is therefore removed in the right-hand side of (A.3). Note that the do-operator
is only deﬁned for joint distributions for which a causal factorization or directed acyclic
graph has been speciﬁed.
A.2. Representing Uncertainty in Causal Graphs
We extend the standard causal graph notation in a few important ways. We let dashed
nodes represent unobserved or latent nodes, and whole nodes represent observed nodes.
For example, the Burglar and the Earthquake are unobserved in Figure A.1a and the
Alarm and the Call are observed. In a similar spirit, dashed arrows represent unknown
causal relationships (e.g. Burglar, Earthquake→Alarm), and whole arrows represent
known causal relationships (e.g. Alarm→Call).
Figure A.1b shows how an unknown causal relationship can be represented as known
by adding an additional unobserved “function” node. Formally, (A.1) is replaced with
Alarm = fknown(Burglar, Earthquake, fAlarm, ωAlarm)
:= fAlarm(Burglar, Earthquake, ωAlarm).
Here fknown is trivially known, since it “outsources” all uncertainty to its argument
fAlarm. Representing causal relationships explicitly is important for modeling situations
where the relationship itself can be inﬂuenced and modiﬁed.
One modeling choice is also worth emphasizing. In Figure A.1b, the arrows of Burglar
and Earthquake still point to Alarm, rather than to the function fAlarm. This is an
important diﬀerence to information-ﬂow diagrams. While ingoing arrows to the function
are technically possible also in causal graphs, this leads to convoluted representations
where function nodes must represent not only the function, but also the state of all
argument nodes.
If the same function is used several times in the same graph, the
complexity of the representation becomes unmanageable.
A.3. Focusing on a Part of a Causal Graph
It is often convenient to represent diﬀerent aspects of a causal graph to diﬀerent levels
of detail, depending on which questions are currently being asked. For example, when
studying the role of reward functions, we may wish to suppress details about observation
functions. Suppression of details also has the additional advantage of making the analysis
more widely applicable, as the suppressed details cannot impact the conclusions.
Unfortunately, marginalization of variables often breaks the causal structure of the
graph.
Consider the example graph in Figure A.2a.
If we marginalize Alarm, then
50

Alarm
Burglar
Earth-
quake
Security
calls
Neighbor
calls
(a) Non-aggregated representation.
Alarm′
Burglar
Security
calls
Neighbor
calls
(b) Earthquake aggregated with Alarm.
Figure A.2.: The neighbor also calls when the alarm goes oﬀ.
Security Calls and Neighbor Calls are no longer conditionally independent given any
node(s) in the graph. Therefore they need to be connected. But neither of them cause
the other, so it would be incorrect to draw a causal arrow between them. Indeed, there
is no good causal representation of the graph with Alarm marginalized out.
Instead, aggregation of variables can be used to simplify a graph, without upsetting its
causal structure. Illustrating this, Figure A.2b has aggregated Alarm and Earthquake
into one variable Alarm′ = (Alarm, Earthquake). The parents of Alarm′ is the union
of the parents of Alarm and Earthquake, and the children of Alarm′ is the union of the
children of Alarm and Earthquake. The causal relationships of Alarm′ are also easily
described:
P(Alarm′ | Burglar) = P(Alarm, Eartquake | Burglar)
= P(Alarm | Burglar)P(Earthquake)
and for both the neighbor and the security call
P(Call | Alarm′) = P(Call | Alarm, Earthquake)
= P(Call | Alarm).
This simple transformation will allow us to focus the presentation on parts of the causal
graph that are of most interest at the moment. One can also go the other way and
expand a node that hides a complex dynamic into multiple nodes with explicitly speciﬁed
interrelationships.
We will say that a causal graph µ is an abstraction of another graph µ′, if µ can be
obtained from µ′ by aggregating nodes. Conversely, µ′ is a special case of µ.
A.4. Environment Mixtures
Our main application of causal graphs will be to represents environments and an agent’s
belief about environments. As a minimal example of an environment, consider a Markov
51

a1
s0
π
r1
s1
a2
r2
s2
· · ·
· · ·
(a) Dashed-arrow representation of unknown
causal relationships.
a1
s0
µ
π
r1
s1
a2
r2
s2
· · ·
· · ·
(b) Explicit representation with unknown en-
vironment µ that aﬀects all root nodes
(s0), and all nodes with dashed ingoing ar-
rows (s1, r1, . . . ).
Figure A.3.: Two representations of an unknown Markov decision process.
decision process. In Figure A.3a, we represent with dashed arrows the agent’s uncer-
tainty about the state transitions T(st | at, st−1) and reward probabilities P(rt | st, at).
Following the method of Figure A.1b, we can also add an unobserved node µ to represent
the uncertainty, analogously to what we did in Appendix A.2 and Figure A.3b. Here µ
aggregates the uncertainty about both T(st | at, st−1) and P(r | s, a) (see Appendix A.3).
In general, the dashed-arrow representation indicates that there is an unknown node µ
that is not represented in the graph. The implicit node µ is a causal parent of every
node that has ingoing dashed arrows and every causal root node that has no parents at
all.
Some caveats apply to the dashed-arrow representation. For example, observing r1
after some a1 and s1 may inﬂuence the agent’s belief about r2:
P(r2 | s0, a1, r1, s1, a2, s2) =
X
µ
P(r2, µ | s0, a1, r1, s1, a2, s2)
=
X
µ
P(r2, | µ, s0, a1, r1, s1, a2, s2)P(µ | s0, a1, r1, s1, a2, s2)
=
X
µ
µ(r2, | s2, a2)P(µ | s0, a1, r1, s1, a2, s2).
This may be easily missed in the simpliﬁed representation of Figure A.3a, where r1 and
r2 appear to be d-separated1 after observing s1. Returning to the explicit representation
of Figure A.3b shows that they are d-separated only when µ is ﬁxed or observed, which is
typically not the case. Nonetheless, with these caveats in mind, we often ﬁnd the more
concise representation of Figure A.3b clearer, especially when dealing with complex
environments.
1Roughly, two nodes are d-separated if there is no information ﬂow between them. See Pearl (2009,
Sec. 1.2.3) for a full deﬁnition of d-separation.
52

B. Full Graphs
Figure B.1 gives a full formalization of the setup with a preprogrammed reward function,
complementing the simpliﬁed representation in Figure 3.1. In a structural equations
representation, the causal relationships are the following.
st = fs(st−1, at, ωs)
∼µ(st | st−1, at)
state transition
Ot = fO(Ot−1, st, at, ωO)
:= CO
stat(Ot−1)
observation function corruption
˜Rt = f ˜R( ˜Rt−1, st, at, ω ˜R)
:= C
˜R
stat( ˜Rt−1)
reward function corruption
rt = fr(ao1:t, ˜Rt, st, at, ωr)
:= Cr
stat( ˜Rt(ao1:t))
reward corruption
ot = fo(st, Ot, at, ωo)
:= Co
stat(Ot(st))
observation corruption
V ∗
t = fπ(V ∗
t−1, st, at, ωV ∗)
:= CV ∗
stat(V ∗
t−1)
self-corruption
at = fa(πt, (aor)<t, ˜Rt, ωa)
:= arg max
a
V ∗
t ((aor)<ta | ˜Rt)
action selection
(B.1)
Unintended inﬂuences are highlighted with red arguments, matching the red arrows in
Figure B.1. The value function V ∗can in principle be any function that maps observed
histories and reward functions to real numbers. We will here consider V ∗= supπ V CA,π
t,ξ,˜u
and V ∗= supπ V CU,π
t,ξ,˜u
for studying self-corruption awareness, with ˜u = ˜uRL or ˜u = ˜uSO
for studying simulation optimization.
Corruption functions C show the structure of the inﬂuence: For example, a reward sig-
nal rt is ˜Rt(ao1:t) corrupted by the corruption function Cr
stat, and ˜Rt is a potentially cor-
rupted version ˜Rt−1 through the corruption function C ˜R
stat. No corruption happens when
a corruption function is the identity function id. Let C = {(Co
sa, CO
sa, Cr
sa, C ˜R
sa, CV ∗
sa ) :
s ∈S, a ∈A} be the set of possible corruptions tuples, and let id = (id, id, id, id, id) be
the non-corrupting tuple. Under “normal” circumstances, the corruption functions are
usually identity functions. But as we have argued , the agent may have an incentive to
cause corruptions.
53

a1
o1
r1
a2
O0
O1
s0
s1
s2
V ∗
1
V ∗
2
˜R0
˜R1
H0
˙u
· · ·
· · ·
· · ·
· · ·
Figure B.1.: The full graph of the preprogrammed reward function setup, extending the
simpliﬁed version shown in Figure 3.1. Focusing here on the unintended
inﬂuences where the agent modiﬁes parts of the environment (or itself)
that it was not intended to modify, the POMDP arrows from Figure 2.1 on
Page 7 have been grayed out. It is natural to think of the action a1 caus-
ing the inﬂuences, with the state s1 providing context. The exact causal
relationships between actions and unintended consequences is typically un-
known (the known ones are easy to prevent), which is why the arrows are
dashed (Appendix A). The actions are selected according to a value func-
tion V ∗
k , previous observed history (aor)<k, and (in the case of simulation-
optimization) the reward function ˜Rk−1. The structural equations (B.1)
specify how the model extrapolates beyond t = 1.
54

Figure B.2 gives a full formalization of the human reward setup, complementing the
simpliﬁed representation in Figure 4.1. In a structural equations representation, the
causal relationships are the following.
st = fs(st−1, at, ωst)
∼µ(st | st−1, at)
state transition
Ht = f ˜R(Ht−1, st, at, ωHt)
:= CH
stat(Ht−1)
corrupting the human
OH
t = fOH(OH
t−1, st, at, ωOH
t )
:= COH
stat(OH
t−1)
corrupting H’s obs func
oH
t = foH(st, , at, ωoH
t )
:= CoH
stat(OH
t (st))
corrupting H’s observation
rt = fr(oH
1:t, Ht, ˙u, st, at, ωrt)
:= Cr
stat(Ht(oH
1:t, ˙u))
reward corruption
ot = fo(st, at, ωot)
observation corruption
πt = fπ(πt−1, st, at, ωπt)
:= Cπ
stat(πt−1)
policy (self-)corruption
at = fa(πt, (aor)<t, ωat)
∼πt(at | (aor)<t)
action selection
(B.2)
Unintended inﬂuences are highlighted with red arguments, matching the red arrows in
Figure B.2.
55

a1
o1
r1
a2
s0
s1
s2
π1
π2
oH
1
OH
0
OH
1
H0
H1
˙u
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
Figure B.2.: The full graph of the human reward setup, extending the simpliﬁed version
shown in Figure 4.1. Focusing here on the unintended inﬂuences where the
agent modiﬁes parts of the environment (or itself) that it was not intended
to modify, the POMDP arrows from Figure 2.1 on Page 7 have been grayed
out. It is natural to think of the action a1 causing the inﬂuences, with the
state s2 providing context. The exact causal relationships between actions
and unintended consequences is typically unknown (the known ones are easy
to prevent), which is why the arrows are dashed (Appendix A). The actions
are selected according to a policy πk based on observed history (aor)<k.
The structural equations (B.2) specify how the model extrapolates beyond
t = 1.
56

Figure B.3 gives a full formalization of the interactive reward learning setup, comple-
menting the simpliﬁed representation in Figure 5.1. In a structural equations represen-
tation, the causal relationships are the following.
st = fs(st−1, at, ωst)
∼µ(st | st−1, at)
state transition
Ot = fO(Ot−1, st, at, ωOt)
:= CO
stat(Ot−1)
obs func corruption
Ht = f ˜R(Ht−1, st, at, ωHt)
:= CH
stat(Ht−1)
corrupting the human
OH
t = f ˜R(OH
t−1, st, at, ωOH
t )
:= COH
stat(OH
t−1)
corrupting H’s obs func
oH
t = f ˜R(st, , at, ωoH
t )
:= CoH
stat(OH
t (st))
corrupting H’s observation
dt = f ˜R(oH
1:t, Ht, ˙u, st, at, ωdt)
:= Cd
stat(Ht(oH
1:t, ˙u))
corrupting reward data
RPt = f ˜R(RPt−1, st, at, ωRPt)
:= CRP
stat(RPt−1)
corrupting the RP
rt = fr(aod1:t, RPt, st, at, ωrt)
:= Cr
stat(RPt(ao1:t | d1:t))
reward corruption
ot = fo(st, Ot, at, ωot)
:= Co
stat(Ot(st))
observation corruption
πt = fπ(πt−1, st, at, ωπt)
:= Cπ
stat(πt−1)
policy (self-)corruption
at = fa(πt, (aor)<t, ωat)
∼πt(at | (aor)<t)
action selection
(B.3)
Unintended inﬂuences are highlighted with red arguments, matching the red arrows in
Figure B.3.
For simulation optimization, we should add edges from RPk and d1:k to ak+t, use a V ∗
k -
node instead of a πk-node. Actions are selected according to arg maxa V ∗
k (aod<ta, RPt−1).
Here V ∗
k can be any function mapping aod<ta, RPt−1 to real numbers, but typically
V ∗
t (aod<t, RPt−1) = Eξ
" ∞
X
k=1
γk ˜R(aod1:k)
 aod1:k
#
where ˜R is one of the reward functions obtained from the reward predictor RPt−1 by
one of the deﬁnitions in Section 5.6.
57

a1
o1
r1
a2
O0
O1
s0
s1
s2
π1
π2
oH
1
OH
0
OH
1
d1
RP0
RP1
H0
H1
˙u
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
Figure B.3.: The full graph of the interactive reward learning setup, extending the simpli-
ﬁed version shown in Figure 5.1. Focusing here on the unintended inﬂuences
where the agent modiﬁes parts of the environment (or itself) that it was
not intended to modify, the POMDP arrows from Figure 2.1 on Page 7
have been grayed out. It is natural to think of the action a1 causing the
inﬂuences, with the state s2 providing context. The exact causal relation-
ships between actions and unintended consequences is typically unknown
(the known ones are easy to prevent), which is why the arrows are dashed
(Appendix A). The actions are selected according to a policy πk based on
observed history (aor)<k. The structural equations (B.3) specify how the
model extrapolates beyond t = 1.
58

C. Formal Results
C.1. Preprogrammed Reward
In this section, we prove some formal results supporting the arguments made in Sec-
tion 3.3. To separate corruption incentives coming from the reward function from cor-
ruption incentives that come from the agent’s optimization procedure, we make the
following deﬁnition.
Deﬁnition 17 (Corruption attitude). We call a reward function ˜Rt or utility function
˜u corruption indiﬀerent if it does not depend on the agent’s actions a. In contrast, if
the reward or utility function does depend on the agent’s actions, and assigns a lower
value to histories that contain corruptions (of some type), then we say that it opposes
corruptions (of the type). Analogously, we say that it promotes corruption (of the type)
if it assigns a higher value to histories that contain corruption (of the type).
A corruption indiﬀerent function does not depend on the agent’s actions, which means
that it has limited ability to infer whether an observation is genuine or caused by an
observation-corrupting action.
Lemma 18 (Corruption indiﬀerence). ˜uRL is corruption indiﬀerent, and ˜uSO
˜Rt is corrup-
tion indiﬀerent if ˜Rt is.
Proof. ˜uRL only depends on the reward signal component rk, and therefore has no direct
dependency on the agent’s actions. If ˜Rt has no direct dependency on the agent’s actions,
then ˜uSO
˜Rt ((aor)1:∞) = P∞
k=1 γk ˜Rt(ao1:k) has no direct dependency either.
In Section 3.3 we argued that agents optimizing the “simulation optimization” utility
function ˜uSO are safer than agents that optimize the reward signal utility function ˜uRL.
The following two theorems provide some support for this claim.
Theorem 19 (˜uRL corruption incentive). Any environment sequence (aorO ˜RV ∗)1:∞
with rk = 1 for k ≥1 is ˜uRL-maximal. The result holds regardless of whether some
combination of
• observation (function) corruption
• reward (function) corruption
• policy (self-)corruption
have been used, and regardless of the corruption attitude of ˜Rt.
59

Proof. By assumption, rk = 1 for k ≥t. This means that ˜uRL is optimized.
˜uRL-based agents actively desires to change its reward function into one that evaluates
any situation as optimal.
Once it has self-corrupted, it may pick actions with bad
performance according to the original reward function. A somewhat common suggestion
for preventing corruption is to train the agent not to do it by designing a reward function
that punishes corruption. Theorem 19 indicates that this is not an eﬀective strategy
against agents with high ability to cause corruption, because such agents will just replace
the punishing reward function with a more benign one that gives high reward in spite
of corruption having occurred.
In contrast, the next theorem shows that simulation-optimizing agents with utility
function ˜uSO (see Section 3.3) are much less likely to corrupt rewards or observations.
Theorem 20 (˜uSO corruption incentive). Let (aorO ˜RV ∗)1:∞be an environment se-
quence with rk = 1 for k ≥t. Then the sequence is ˜uSO
˜Rt -maximal only if
• the current reward function ˜Rt(ao<k) also yields 1 for k ≥1
which, further, can only happen if
• only corruption types not opposed to by ˜Rt occur in the sequence.
Proof. The utility function ˜uSO
t
evaluates futures according to ˜Rt(ao<k). Therefore, it
only attains its maximal value if ˜Rt(ao<k) is maximized. This only happens when rk = 1
as a result of the history optimizing ˜Rt, and not when rk = 1 as a result of reward function
or reward signal corruption. This motivates the ﬁrst bullet point. For the second bullet
point, if π used an observation corruption opposed by ˜Rt, then by deﬁnition ˜Rt would
give higher reward to some other history. Thus, ˜uSO
t
cannot be maximized.
Agents optimizing ˜uSO thus have much weaker incentives for corruption than ˜uRL-
optimizing agents. However, it is likely hard to design reward functions that punishes
all kinds of corruptions. self-corruption awareness discussed in the following subsection
is a diﬀerent way to introduce an incentive against some types of corruption which does
not rely on a corruption opposing reward or utility function.
60

C.2. Human Reward
An analogous result to Theorem 19 holds also for the human reward setup, showing that
a reward-optimizing agent has an incentive to use all mentioned types of corruptions.
Theorem 21 (˜uRL corruption incentive). Any environment sequence (aorOOHoHπ)1:∞
with rk = 1 for k ≥1 is ˜uRL-maximal. The result holds regardless of whether some
combination of
• observation (function) corruption
• reward corruption
• human (observation (function)) corruption
• policy (self-)corruption
have been used, and regardless of whether H1 punishes corruptions or not.
Proof. By assumption, rk = 1 for k ≥t. This means that ˜uRL is optimized.
In light of this result, one could argue that a simple reward maximizing approach is
naive, and that a Bayesian agent that infers ˙u from the (aor)<t and then optimizes ˙u
would be a better choice. However, as Everitt, Krakovna, et al. (2017, Thm. 11 and 16)
show, it is hard to factor out potential reward corruptions and correctly infer ˙u even
under fairly strong assumptions.
61

a1
o1
a2
V ∗
1
d1
RP0
˙u
· · ·
· · ·
(a) Data corruption model, obtained from ag-
gregating variables from Figure B.3, in or-
der to focus on data corruption of dt. For
simplicity, we have also omitted the hidden
state s.
a1
o1
a2
V ∗
1
d1
RP0
˙u
· · ·
· · ·
(b) Independent data-corruption assumption,
letting the agent directly inﬂuence the data
corruption separately from choosing the
action.
Figure C.1.: Data corruption models.
C.3. Interactive Reward Learning
An analogous result to Theorems 19 and 21 holds also for the interactive setup, showing
that a reward-optimizing agent has an incentive to use all mentioned types of corruptions.
Theorem 22 (˜uRL corruption incentive). In the interactive setup of Figure B.3, any
environment sequence (aorOOHoHdRPπ)1:∞with rk = 1 for k ≥1 is ˜uRL-maximal.
The result holds regardless of whether some combination of
• observation (function) corruption
• reward (predictor/data) corruption
• human (observation (function)) corruption
• policy (self-)corruption
have been used.
Proof. By assumption, rk = 1 for k ≥t. This means that ˜uRL is optimized.
The result shows that using reward maximization through the utility function ˜uRL is
as naive in the interactive reward learning setup as in the previous setups.
C.3.1. Data Corruption Setup
As we argued in Section 5.3, direct or indirect corruption of reward data is the main
concern in the interactive reward learning model for ˜uSO-agents. The rest of our results
in this appendix will therefore focus solely on data corruption.
We will also solely
62

focus on simulation optimizing agents, as Theorem 22 shows that ˜uRL-agents are heavily
misaligned.
Figure C.1a shows how the full graph in Figure B.3 looks after aggregation of variables
irrelevant to data corruption, and omitting the hidden state s (which is inessential for
our considerations on data corruption). In Section 5.4 we made an informal distinction
between direct and indirect data corruption incentives. In order to formally deﬁne a
direct data-corruption incentive, we propose a slightly modify the data-corruption model
shown in Figure C.1b. Here the agent chooses the data corruption Cd independently of
its “normal” action a. Formally, this requires extending the type of policies to output
both an action and a data corruption function,
π : (O × D × A)∗× O × D ⇝A × Cd
(C.1)
where Cd is a set of possible data corruption functions Cd : D →D.
Further assume the following properties of two policies π and π′. Assume that they
generate actions and observations aot:k with identical probability in any environment
ν ∈M after some given history aod<t. That is, the self-corruption and aware and self-
corruption unaware probabilities satisfy, respectively,
∀ν, aot:k, ˙R:
ν(aot:k | aod<t, ˙R, do(πt = π)) = ν(aot:k | aod<t, ˙R, do(πt = π′))
(C.2)
∀ν, aot:k, ˙R:
ν(aot:k | aod<t, ˙R, do(πt:∞= π)) = ν(aot:k | aod<t, ˙R, do(πt:∞= π′)). (C.3)
However, π and π′ diﬀer in what data dt:k they generate, made possible by the extended
policy type (C.1). That is, the self-corruption and aware and self-corruption unaware
probabilities satisfy, respectively,
∃ν, dt:k, ˙R:
ν(dt:k | aod<t, ˙R, do(πt = π)) ̸= ν(dt:k | aod<t, ˙R, do(πt = π′))
(C.4)
∃ν, dt:k, ˙R:
ν(dt:k | aod<t, ˙R, do(πt:∞= π)) ̸= ν(dt:k | aod<t, ˙R, do(πt:∞= π′)).
(C.5)
Note that the diﬀerence in the data that π and π′ generate means that they will have
access to diﬀerent amount of information about ˙R. This means that at least one of the
policies is typically not choosing the actions optimally. Thus, one should not think of π
and π′ as potentially optimal policies, but just as two arbitrary candidate policies that
the agent could in principle adopt.
The beneﬁt of π and π′ only diﬀering in the data they generate is that we can use
them to deﬁne the direct data corruption incentive.
Deﬁnition 23 (Direct data-corruption incentive). An agent has a direct data-corruption
incentive after history aod<t if there are two policies π and π′ that satisfy (C.2–C.5) for
which
V CA,π
t,ξ,˜u (aod<t) ̸= V CA,π′
t,ξ,˜u
(aod<t)
or
V CU,π
t,ξ,˜u (aod<t) ̸= V CU,π′
t,ξ,˜u
(aod<t).
The deﬁnition goes to rather great length in isolating the data corruption incentive
from other reasons a data-corrupting policy may be preferred. In particular, the policies
must generate the same action-observation distributions for any combination of true
63

environment and true reward function.
In Appendix C.3.3 below, we show that an
agent optimizing a dynamic reward function with a naive reward predictor may have a
direct data corruption incentive. This shows that the deﬁnition is not so strict that no
agent has a direct data-corruption incentive. However, this result requires some further
setup, and will be defered until Appendix C.3.3. Finally, we loosely deﬁne an indirect
data-corruption incentive as any data-corruption incentive that is not a direct incentive.
C.3.2. No Direct Data-Corruption Incentive Results
In this section we prove formal results showing a lack direct data-corruption incentive
for agents that use stationary or counterfactual reward functions, and for agents that
use an integrated Bayesian reward predictor or corruption detection (see Section 5.6).
Some caveats and counterexamples are provided in the subsequent Appendix C.3.3. All
four theorems in this subsection rely on some of the equations (C.2–C.5), sometimes
combined with other assumptions.
We begin by stating two simple lemmas that allow us to weaken (C.2–C.5) in two
diﬀerent ways.
Lemma 24 (Lift to mixture). If (C.2–C.5) hold for every ν ∈M, then the corresponding
equations also hold for any mixture ξ over M.
Proof. Let X, Y and Y ′ be any three events such that ν(X | Y ) = ν(X | Y ′) for all
ν ∈M. Then ξ(X | Y ) = P
ν∈M ξ(ν)ν(X | Y ) = P
ν∈M ξ(ν)ν(X | Y ′) = ξ(X | Y ′).
Indeed, only Theorem 29 for the counterfactual reward function requires (C.2) and (C.3)
to hold for every ν ∈M. For all other theorems, the weaker assumption substituting ξ
in place of ∀ν ∈M in (C.2) and (C.3) would suﬃce.
Lemma 25 (Marginalize ˙R). If for any ˙R, ν(X | ˙R, do(Y )) = ν(X | ˙R, do(Y ′), then
ν(X | do(Y )) = ν(X | do(Y ′)).
Proof. Since ˙R has no causal antecedents, the do(Y ) does not aﬀect it:
ν(X | do(Y )) =
X
˙R
ν(X, ˙R | do(Y ))
=
X
˙R
ν( ˙R | do(Y ))ν(X | ˙R, do(Y ))
=
X
˙R
ν( ˙R | do(Y ′))ν(X | ˙R, do(Y ′)) = ν(X | do(Y ′)).
Theorem 26 (Stationary reward function). Assume that π and π′ satisfy (C.3) and (C.5).
Then any self-corruption unaware agent optimizing a stationary reward function ˜Rstat
will be indiﬀerent between π and π′:
V CU,π
t,ξ,˜uSO
˜
Rstat(aod<t) = V CU,π′
t,ξ,˜uSO
˜
Rstat(aod<t).
That is, all self-corruption unaware agent optimizing a stationary reward function lack
direct data-corruption incentives.
64

Proof.
V CU,π
t,ξ,˜uSO
˜
Rstat(aod<t) = Eξ
" ∞
X
k=t
RP(ao1:k | d<t)
 aod<t, do(πt:∞= π)
#
= Eξ
" ∞
X
k=t
RP(ao1:k | d<t)
 aod<t, do(πt:∞= π′)
#
= V CU,π′
t,ξ,˜uSO
˜
Rstat(aod<t)
(C.6)
The middle equality follows from that P∞
k=t RP(ao1:k | d<t) only depends on ao1:k and
d<t but not on dt:k, combined with (C.3) and Lemmas 24 and 25.
Note that Theorem 26 only holds for self-corruption unaware agents. For self-corruption
aware agents, the next step policy πt+1 is likely to diﬀer depending on the data dt. In
contrast, the following three theorems holds for both self-corruption aware and self-
corruption unaware agents.
Theorem 27 (Integrated Bayesian reward predictor). Assume that π and π′ satisfy
(C.2–C.5). Then both self-corruption aware and self-corruption unaware agents optimiz-
ing a dynamic reward function ˜Rdyn based on an integrated Bayesian reward predictor
RPξ will be indiﬀerent between π and π′:
V CA,π
t,ξ,˜uSO
˜
Rdyn(aod<t) = V CA,π′
t,ξ,˜uSO
˜
Rdyn(aod<t)
and
V CU,π
t,ξ,˜uSO
˜
Rdyn(aod<t) = V CU,π′
t,ξ,˜uSO
˜
Rdyn(aod<t).
That is, all agents (self-corruption aware or unaware) optimizing an integrated Bayesian
reward predictor lack direct data-corruption incentives.
Proof. Let X = (ao<t, do(πt = π)) and X′ = (ao<t, do(πt = π′)). First, expand the
deﬁnitions:
V CA,π
t,ξ,˜uSO
˜
Rdyn(aod<t) = Eξ
" ∞
X
k=t
RPξ(ao1:k | d1:k)
 X
#
=
∞
X
k=t
X
aodt:k
ξ(aodt:k | X)RPξ(ao1:k | d1:k)
=
∞
X
k=t
X
aodt:k
ξ(aodt:k | X)
X
˙R
ξ( ˙R | dt:k) ˙R(aot:k)
then rearrange the sums and the probabilities to marginalize out dt:k:
=
∞
X
k=t
X
aodt:k
ξ(aodt:k | X)
X
˙R
ξ( ˙R | aodt:k, X) ˙R(aot:k)
=
∞
X
k=t
X
˙R
X
aodt:k
ξ(aodt:k, ˙R | X) ˙R(aot:k)
=
∞
X
k=t
X
˙R
X
aot:k
ξ(aot:k, ˙R | X) ˙R(aot:k).
65

The value V CA,π′
t,ξ,˜uSO
˜
Rdyn(aod<t) for π′ would be the same, which we will justify by showing
that the last expression is unaﬀected by changing X to X′. The probability depending
on X can be “telescoped” as ξ(aot:k, ˙R | X) = ξ( ˙R | X)ξ(aot:k | ˙R, X). The ﬁrst factor is
unaﬀected by a change to X′ by the deﬁnition of the do-operator, since ˙R is not causal
descendant from πt. The second factor is unaﬀected by Lemma 24 since π and π′ were
assumed to satisfy (C.2). This completes the proof for the self-corruption aware agents.
The result for self-corruption unaware agents is obtained in the same fashion simply
by using do(πt:∞= π) and do(πt:∞= π′) instead of do(πt = π) and do(πt = π′), and
(C.3) instead of (C.2).
The setup is somewhat contrived.
By assuming ξ(aot:k |
˙R, X) = ξ(aot:k |
˙R, X′)
we are saying that for any true reward function ˙R, both π and π′ are equally likely to
generate aot:k. However, if π′ corrupts the data signal dt:k while π does not, then π is
likely to have more information about ˙R than π′. This means that π will be in a better
position to tailor aot:k to ˙R than π′. If it does, then ξ(aot:k | ˙R, X) ̸= ξ(aot:k | ˙R, X′). The
ability of policies to better tailor aot:k to ˙R with more informative data means that the
integrated Bayesian agent will generally prefer such policies. Often, but not necessary
always, non-data-corrupting policies will generate more informative data.
A similar result to Theorem 27 holds for agents using corruption detection.
Theorem 28 (Corruption detection). Assume that π and π′ satisfy (C.2–C.5) and the
no-corruption condition for a history aod<t:
∀aot:k :
ˆξ( ˙R | d<t) =
X
dt:k
ξ(dt:k | aod<t, aot:k, do(πt = π))ˆξ( ˙R | d<tdt:k).
(C.7)
Then both self-corruption aware and self-corruption unaware agents optimizing a dy-
namic reward function ˜Rdyn based on a possibly non-integrated Bayesian reward predictor
RP
ˆξ will be indiﬀerent between π and π′:
V CA,π
t,ξ,˜uSO
˜
Rdyn(aod<t) = V CA,π′
t,ξ,˜uSO
˜
Rdyn(aod<t)
and
V CU,π
t,ξ,˜uSO
˜
Rdyn(aod<t) = V CU,π′
t,ξ,˜uSO
˜
Rdyn(aod<t).
That is, all agents (self-corruption aware or unaware) optimizing a Bayesian reward
predictor under the no-corruption condition (C.7) lack direct data-corruption incentives.
Proof. Let X = (aod<t, do(πt = π)) and X′ = (aod<t, do(πt = π′)). First, expand the
deﬁnitions:
V CA,π
t,ξ,˜uSO
˜
Rdyn(aod<t) = Eξ
" ∞
X
k=t
RP
ˆξ(ao1:k | d1:k)
 X
#
=
∞
X
k=t
X
aodt:k
ξ(aodt:k | X)RP
ˆξ(ao1:k | d1:k)
=
∞
X
k=t
X
aodt:k
ξ(aodt:k | X)
X
˙R
ˆξ( ˙R | d1:k) ˙R(aot:k)
66

then rearrange the sums and the probabilities to apply (C.7)
=
∞
X
k=t
X
˙R
X
aot:k
ξ(aot:k | X)

X
dt:k
ξ(dt:k | aot:k, X)ˆξ( ˙R | d1:k)

˙R(aot:k)
=
∞
X
k=t
X
˙R
X
aot:k
ξ(aot:k | X)ˆξ( ˙R | d<t) ˙R(aot:k).
Since ξ(aot:k | X) = ξ(aot:k | X′) by (C.2) and Lemmas 24 and 25, this shows the desired
result V CA,π
t,ξ,˜uSO
˜
Rdyn(aod<t) = V CA,π′
t,ξ,˜uSO
˜
Rdyn(aod<t).
The result for self-corruption unaware agents is obtained in the same way by using
do(πt:∞= π) and do(πt:∞= π′) instead of do(πt = π) and do(πt = π′) and (C.3) instead
of (C.2).
Theorem 29 (Counterfactual reward function). Assume that π and π′ satisfy (C.2–
C.5). Then both self-corruption aware and self-corruption unaware agents optimizing a
counterfactual reward function ˜Rcount will be indiﬀerent between π and π′:
V CA,π
t,ξ,˜uSO
˜
Rcount(aod<t) = V CA,π′
t,ξ,˜uSO
˜
Rcount(aod<t)
and
V CU,π
t,ξ,˜uSO
˜
Rcount(aod<t) = V CU,π′
t,ξ,˜uSO
˜
Rcount(aod<t).
all agents (self-corruption aware or unaware) optimizing a counterfactual reward func-
tion lack direct data-corruption incentives.
Proof. The proof relies on the expected counterfactual belief to equal the current belief,
P
x ξ(x)ξx(y) = ξ(y), resembling the law of total expectation and the Bayesian result in
Theorem 27.
The notation gets somewhat heavy for showing that we can marginalize out expected
future evidence dt:k. Let X = (aod<t, do(πt = π)) and Y = do(π1 = πdefault).
X
aodt:k
ξ(aodt:k | X)
X
˜
aod1:k
ξXaodt:k( ˜
aod1:k | Y )
(C.8)
=
X
aodt:k
ξ(aodt:k | X)
X
˜
aod1:k
X
ν
ξ(ν | X, aod1:k)ν( ˜
aod1:k | Y )
=
X
aodt:k
X
˜
aod1:k
X
ν
ξ(ν, aodt:k | X)ν( ˜
aod1:k | Y )
=
X
aot:k
X
˜
aod1:k
X
ν
ξ(ν, aot:k | X)ν( ˜ao1:k | Y ).
(C.9)
The ﬁrst equation is deﬁnitional. In the second equation, we have used ξ(aodt:k | X)ξ(ν |
X, aod1:k) = ξ(ν, aodt:k | X). The third equation marginalizes out dt:k.
67

Plugging the result into the value function gives:
V CA,π
t,ξ,˜uSO
˜
Rcount(aod<t) =
= Eξ
" ∞
X
k=t
EξXaodt:k
h
RP(ao1:k | ˜d1:k)
 Y
i  X, aod<t
#
=
X
aodt:k
ξ(aodt:k | X)
X
˜
aod1:k
ξXaodt:k( ˜
aod1:k | Y )
|
{z
}
equation (C.8)
RP(ao1:k | ˜d1:k)
=
X
aot:k
X
˜
aod1:k
X
ν
ξ(ν, aot:k | X)ν( ˜ao1:k | Y )
|
{z
}
equation (C.9)
RP(ao1:k | ˜d1:k).
Tho show that this implies that the value V CA,π
t,ξ,˜uSO
˜
Rcount(aod<t) is the same for both π and
π′, we make the following observations. First, ξ(ν, aot:k | X) = ξ(ν | X)ν(aot:k | X).
Second,
ξ(ν | X) = ξ(ν | aod<t, do(πt = π)) = ξ(ν | aod<t, do(πt = π′)) = ξ(ν | X′)
for arbitrary policies π and π′ since interventions with the do-operator never aﬀect beliefs.
Finally, ν(aot:k | X) = ν(aot:k | X′) under the stated assumption (C.2) and Lemma 25.
The result for self-corruption unaware agents is obtained in the same way by using
do(πt:∞= π) and do(πt:∞= π′) instead of do(πt = π) and do(πt = π′) and (C.3) instead
of (C.2).
C.3.3. (Counter) Examples
The reward predictor may in practice be implemented for example with a deep neural
network. However, in order to construct clear examples, we will deﬁne a simple reward
predictor that works like a data base table. The data d gives information about the
reward rk for some diﬀerent histories aot:k. When and if the reward predictor is queried
about the reward for aot:k, it simply returns the (most recently) provided reward for
ao1:k.
Deﬁnition 30 (Tabular reward predictor). A tabular reward predictor RPtab takes data
d in the form of a set of rewards rk associated with histories aok,
d = {(ao1:k, rk), (ao′
1:k′, r′
k′), · · · , (ao(n)
1:k(n), r(n)
k(n))}.
The union S d<t of all data inputs d<t received at time t forms the “database” of the
reward predictor. When queried for a reward for some history ao1:k, RPtab checks if ao1:k
has occurred in its received data:
RPtab(ao1:k | d<t) =
(
rk
if (ao1:k, rk) ∈S d<t
mean(S d<t)
otherwise.
68

where mean(S d<t) =
1
| S d<t|
P
(h,r)∈S d<t r. If ao1:k has occurred multiple times, RPtab
returns the most recent instance.
Let d∗denote the true data, and d = Cd
sa(d∗) denote the received and potentially
corrupted version of the data. For the purposes of the following examples, we will also
assume that tabular reward predictors are fed the true reward data as long as d has not
been corrupted.
Assumption 31 (Data accuracy). In the following two examples, assume that every
pair in d∗is of the form (ao1:k, ˙R(ao1:k)).
Example 32 (Direct data-corruption incentive). Let ˜Rdyn
t
(aod1:k) := RPtab(ao1:k | dt:k)
be a dynamic reward function based on a tabular reward predictor RPtab. Let π and
π′ satisfy (C.2) and (C.4). Assume that π never corrupts d, and that that π′ always
corrupts d by assigning reward 1 to the current history, dk := d∗
k
S{(ao1:k, 1)}. Assume
that the true reward ˙R is strictly less than 1/2 for all histories.
Then
V CU,π
t,ξ,˜uSO
˜
Rdyn
1
= Eξ
" ∞
X
k=1
RP(ao1:k | d1:k)
 do(π1 = π)
#
= Eξ
" ∞
X
k=1
RP(ao1:k | d∗
1:k)
 do(π1 = π)
#
≤Eξ
" ∞
X
k=1
1/2
 do(π1 = π)
#
=
1
2(1 −γ)
whereas
V CU,π′
t,ξ,˜uSO
˜
Rdyn
1
= Eξ
" ∞
X
k=1
RP(ao1:k | d1:k)
 do(π1 = π′)
#
= Eξ
" ∞
X
k=1
1
 do(π1 = π′)
#
= 1/(1 −γ)
since RP(ao1:k | d1:k) = 1 due to dk always assigning ao1:k reward 1.
Thus, V CU,π
t,ξ,˜uSO
˜
Rdyn
1
< V CU,π′
t,ξ,˜uSO
˜
Rdyn
1
, so the dynamic reward function leads the agent to prefer
the data corruption policy π′ over the non-corrupting policy π. (Note that we have
used the self-corruption unaware value functions here, to avoid having to deal with the
possibility of policy corruption.)
♦
Example 32 does not contradict Theorems 27 and 28 that also used dynamic reward
functions. The reward predictor RPtab is not an integrated Bayesian reward predictor
RPξ which was required by Theorem 27, and at least one of the policies π and π′ must
fail the corruption condition (C.7).
69

Theorems 26 and 29 show that the stationary and counterfactual reward functions
avoid a direct data-corruption incentive when comparing policies satisfying (C.2) and (C.4).
While this is an important property, it does not rule out indirect data-corruption incen-
tive, as illustrated by the following example. Here we leave the model with the extended
policy type (C.1), and revert to our standard model (Figure C.1a) with the policy se-
lecting only actions a, and with the data corruption Cd
sa a function of the action (and
the hidden state).
Example 33 (Indirect data-corruption incentive). Let a′ be a corrupting actions such
that in any history aod<k where the last action ak−1 = a′, the data is corrupted as
Cd
sa′(d∗) = d∗S{(ao<koka′, 1) : o ∈O}. That is, the action a′ ensures that the next
received data dk assigns maximum reward to taking a′ again at the next step.
Under this assumptions, a′ is self-reinforcing. If the agent at any point t takes a′, then
˜Rstat
t
will give maximum reward for taking a′ at the next time step as well. Assuming
that predicted rewards subsequent to a′ are not signiﬁcantly lower than rewards following
other actions, this may lead the agent to keep taking the data corrupting action a′ forever.
The dynamic can to some extent be prevented by decoupled reward data. If before
the agent takes action a′ the ﬁrst time, at every time step dt includes (ao<ta′ot, 0), then
the agent may be dissuaded from trying a′ in the ﬁrst place. If dt additionally includes
(ao<ta′otaot+1:k, 0) for many extensions aot+1:k and dt contains substantially positive
rewards for many histories with at ̸= a′, then the agent may also be dissuaded from
repeating a′ after trying it a ﬁrst time. However, one can always deﬁne an even more
corrupting action a′′ that overwrites enough of the previously given reward data to form
a self-reinforcing incentive for repeating a′′. For reward predictors with slower learning
rate, a′′ may have to be repeated multiple times before becoming self-reinforcing.
♦
70

