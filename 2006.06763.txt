STOCHASTIC SADDLE-POINT OPTIMIZATION FOR THE
WASSERSTEIN BARYCENTER PROBLEM‚àó
Daniil Tiapkin
HSE University, Moscow, Russia
dtyapkin@hse.ru
Alexander Gasnikov
Moscow Institute of Physics and Technology, Moscow, Russia
Institute for Information Transmission Problems, Moscow, Russia
HSE University, Moscow, Russia
Weierstrass Institute for Applied Analysis and Stochastics, Berlin, Germany
gasnikov@yandex.ru
Pavel Dvurechensky
Weierstrass Institute for Applied Analysis and Stochastics, Berlin, Germany
Institute for Information Transmission Problems, Moscow, Russia
pavel.dvurechensky@wias-berlin.de
December 6, 2021
ABSTRACT
We consider the population Wasserstein barycenter problem for random probability measures sup-
ported on a Ô¨Ånite set of points and generated by an online stream of data. This leads to a complicated
stochastic optimization problem where the objective is given as an expectation of a function given as
a solution to a random optimization problem. We employ the structure of the problem and obtain a
convex-concave stochastic saddle-point reformulation of this problem. In the setting when the distri-
bution of random probability measures is discrete, we propose a stochastic optimization algorithm
and estimate its complexity. The second result, based on kernel methods, extends the previous one
to the arbitrary distribution of random probability measures. Moreover, this new algorithm has a
total complexity better than the Stochastic Approximation approach combined with the Sinkhorn
algorithm in many cases. We also illustrate our developments by a series of numerical experiments.
Keywords Stochastic Optimization ¬∑ Saddle-Point Optimization ¬∑ Computational Optimal Transport ¬∑
Wasserstein Barycenter Problem
In this paper we consider stochastic optimization problems, which arise in computational optimal transport when the
goal is to estimate population Wasserstein barycenter [1] (or Fr√©chet mean w.r.t. Wasserstein distance) of a probability
distribution on the Wasserstein space of probability measures. Wasserstein barycenter problem has recently attracted
a lot of attention from the machine learning, statistics, and optimization community, see [48, 17] and references
therein. From the computational point of view, approximating population Wasserstein barycenter is a challenging
optimization problem since it contains several layers of complications. The Ô¨Årst layer requires to deÔ¨Åne for p ‚â•1 the
p-Wasserstein distance Wp(r,c) between two probability measures r,c, which in this paper we assume to be discrete
probability distributions that belong to the standard simplex ‚àÜn. Wp(r,c) is deÔ¨Åned as an optimal value in an optimal
transport problem, where the goal is to Ô¨Ånd a transport plan between vectors r,c such that the total transportation cost
is minimal (see (1) for a formal deÔ¨Ånition). The next layer is that the measure c is assumed to be random with some
probability distribution Pc on ‚àÜn and the population barycenter r‚àóminimizes the expected Wasserstein distance, i.e.
r‚àó= argminr‚àà‚àÜn EcWp
p(r, c). This is a stochastic optimization problem, where the objective is deÔ¨Åned as a solution
to some other random optimization problem. The third layer of complication is that the dimension n can be very large,
‚àóThe work of A. Gasnikov in Section 2 was partially supported by the Ministry of Science and Higher Education of the Russian
Federation (Goszadaniye) 075-00337-20-03, project no. 0714-2020-0005. The work of P. Dvurechensly in Section 3 was funded
by Russian Science Foundation (project 18-71-10108). The work of D. Tiapkin was prepared within the framework of the HSE
University Basic Research Program.
arXiv:2006.06763v3  [math.OC]  2 Dec 2021

A PREPRINT - DECEMBER 6, 2021
e.g. 106. To motivate this setting we refer to [9], where the authors consider a template estimation problem from a
random sample of its transformations. A particular example of such a template can be images, considered in [15], where
empirically the Wasserstein barycenter gives the best quality of the template image reconstruction. For a 103 by 103
image, we obtain that n = 106 and the dimension of the transportation plan is then n2 = 1012 leading to a huge-scale
optimization problem.
Similar to other stochastic optimization problems, see e.g. [45], there are two approaches to the population Wasser-
stein barycenter problem, which we refer to as ofÔ¨Çine (also known as Sample Average Approximation, SAA) and
online (also known as Stochastic Approximation, SA). The ofÔ¨Çine approach assumes that a number of random
probability measures ci, i = 1,...,m is sampled in advance and all the measures are stored in memory. Then
the population Wasserstein barycenter problem is approximated by the empirical Wasserstein barycenter problem
ÀÜr = argminr‚àà‚àÜn 1
m
Pm
i=1 Wp
p(r, ci), i.e. the expectation is approximated by the Ô¨Ånite-sum. Finally, it is assumed that
an optimization algorithm has access to all of the measures ci, i = 1,...,m immediately, probably in parallel, and the goal
is to minimize this Ô¨Ånite-sum. So far, nearly all algorithms in the literature for approximating Wasserstein barycenter
are designed for this Ô¨Ånite-sum problem, to name a few [15, 6, 54, 13, 23, 59, 38, 40, 22, 33, 36]. Additionally, in this
setting, it is possible to use a big arsenal of modern decentralized distributed optimization algorithms, see for example,
[23, 59, 52, 19, 36, 29, 35, 34, 30, 51, 50, 27, 7], where, in particular, such algorithms are developed with best-known
complexity bounds for general Ô¨Ånite-sum optimization with cheap dual stochastic oracle, which turns out to be available
for the Wasserstein distance.
Despite all these developments, the ofÔ¨Çine approach does not consider clearly the question of how well the solution to
the Ô¨Ånite-sum problem approximates the target population Wasserstein barycenter which is a solution to a stochastic
optimization problem. Moreover, in practice it may not be possible to generate all the measures in advance, the storage
could be problematic, and the data consisting of random measures can appear as an online stream. Thus, in this paper,
we focus on the less studied online setting with all measures unavailable in advance. Online estimation techniques have
been very recently developed in [42] for Wasserstein distance. Online setting for population Wasserstein barycenter
has started recently to attract interest in the statistical community [37, 12] where population Wasserstein barycenters
are studied in a different from ours setting of Bures-Wasserstein space of Gaussian probability measures, and in
optimization community [17], where online approach is compared to ofÔ¨Çine.
The approach of the latter paper is based on a widespread idea of entropy regularized optimal transport [14, 48]. One of
the main negative sides of the entropic regularization is that if the goal is to Ô¨Ånd an Œµ-approximation to the original
non-regularized Wasserstein distance or barycenter, the regularization parameter has to be chosen proportional to Œµ
[25, 41, 38, 40, 26]. This leads to numerical instability [14, 58] in the Sinkhorn‚Äôs algorithm for Wasserstein distance
and Sinkhorn-type algorithms for Wasserstein barycenters [6]. Alternative methods, e.g. based on accelerated gradient
method [11, 24, 25, 2, 32, 41, 40, 47, 26, 31], similarly to Sinkhorn‚Äôs algorithm require the regularization parameter to
be small. Moreover, entropic regularization leads to a blurred reconstruction of the original template [15].
Thus, in this paper, we focus on the following question and give a positive answer to it. Is it possible to Ô¨Ånd a population
Wasserstein barycenter of a set of discrete probability measures in the online setting and without the use of entropic
regularization and the calculation of (regularized) Wasserstein distance or its (sub)gradients?
To propose such an online approach that does not require calculating Wasserstein distance and its subgradient, we use
a saddle-point representation for the Wasserstein barycenter problem. Since the population Wasserstein barycenter
problem is a convex stochastic programming problem, we obtain partially stochastic convex-concave saddle-point
representation. Then we adapt Stochastic Mirror Descent [46, 45, 5] for this, partially inÔ¨Ånite-dimensional, saddle-point
problem. Finally, by using a reproducing kernel Hilbert space trick inspired by [28], we demonstrate how to make our
approach practical and obtain Ô¨Ånite complexity bounds. We compare the proposed methods with the online approach of
[17] and obtain a regime in which our complexity bounds are better. We also illustrate numerically the regime in which
our methods have better performance than the online algorithm in [17].
Notation.
We deÔ¨Åne Matn√óm(X) as a space of all matrices of size n √ó m with entries from the set X. We denote by
[n] an n-element set {1, 2, . . . , n}, and ev, eA, log(v), log(A), ‚àöv,
‚àö
A for v ‚ààRn, A ‚ààMatn√ón(R) as the element-
wise exponent, logarithm and square root respectively. Also we deÔ¨Åne ‚àÜn to be n-dimensional probability simplex
‚àÜn = {(s1, . . . , sn) | ‚àÄi ‚àà[n] : si ‚â•0,
Pn
i=1 si = 1}. 1n is n-dimensional vector consisting of ones. If the
dimension is clear from the context, the subscript is omitted. As ei we denote the i-th coordinate vector. The probability
measure induced by a random variable Œæ will be denoted by PŒæ. For a matrix M ‚ààMatn√óm(X) we denote by M(i)
the i-th row of this matrix.
2

A PREPRINT - DECEMBER 6, 2021
1
Preliminaries
1.1
Background on optimal transport
In this section, following [48], we recall some basic deÔ¨Ånitions related to optimal transport. Since we deal only with
discrete measures, we consider only the discrete-discrete optimal transport problem.
Optimal transport (OT) problem.
For a Ô¨Åxed non-negative matrix C and two discrete probability measures r,c ‚àà‚àÜn
with n-element support deÔ¨Åne a transportation cost between measures r and c associated with the cost matrix C as a
solution to the following optimization problem
LC(r,c) =
min
X‚ààU(r,c)‚ü®C, X‚ü©,
(1)
where X is called a transport plan and U(r,c) is a transport polytope, deÔ¨Åned as U(r,c) = {X ‚ààMatn√ón(R+) |
X1 = r, XT 1 = c}, and ‚ü®¬∑, ¬∑‚ü©is the Frobenius dot-product of two matrices.
If r and c are probability measures onto discrete n-element metric space (M, d), the p-Wasserstein distance between
these two measures is deÔ¨Åned as the p-th root of the transportation cost associated with the matrix Dp
i,j = d(xi, xj)p,
where xi and xj are elements of M. Formally, Wp(r,c) = (LDp(r,c))1/p . With a slight abuse of notation, we will
refer to LC(r,c) for an arbitrary non-negative cost matrix C as a Wasserstein distance too, and denote it by W(r,c)
when matrix C is Ô¨Åxed.
Dual problem.
The linear program in the deÔ¨Ånition of the transportation cost can be reformulated using so-called
Kantorovich duality in two ways: using some reformulation of results from [48] gives:
LC(r,c) =
max
Œª,¬µ‚ààRn
‚àíCi,j‚àíŒªi‚àí¬µj‚â§0
‚àí‚ü®Œª, r‚ü©‚àí‚ü®¬µ, c‚ü©,
(2)
and an equivalent formulation is
LC(r,c) = max
¬µ‚ààRn ‚àí‚ü®Œª‚àó(¬µ, C), r‚ü©‚àí‚ü®¬µ, c‚ü©,
(3)
where Œª‚àó: Rn √ó Matn√ón(R) ‚ÜíRn is deÔ¨Åned element-wise:
Œª‚àó
i (¬µ, C) = max
j‚àà[n](‚àíCi,j ‚àí¬µj).
(4)
Barycenter deÔ¨Ånition.
Suppose that we have a random variable Œæ : ‚Ñ¶Œæ ‚Üí‚àÜn on probability simplex, or, equivalently,
on the space of probability measures onto M. Then we can deÔ¨Åne a p-Wasserstein (population) barycenter w.r.t. Œæ as
the solution to the following optimization problem:
r‚àó= argmin
r‚àà‚àÜn E

Wp
p(r, Œæ)

.
(5)
However, we are interested in a more general situation deÔ¨Åned by using an arbitrary non-negative cost matrix C:
r‚àó= argmin
r‚àà‚àÜn E [LC(r, Œæ)] = argmin
r‚àà‚àÜn E [W(r, Œæ)] .
1.2
Notation for saddle-point problems
In this section, we give a necessary background on saddle-point problems. For a more comprehensive description we
refer to [10, 45]. The following convex-concave saddle point problem is of relevance to us:
min
x‚ààX max
y‚ààY F(x,y).
The typical way to evaluate the quality of the algorithm that outputs the pair (ex,ey) is to use the so-called duality gap:
max
y‚ààY F(ex, y) ‚àímin
x‚ààX F(x, ey) ‚â§Œµ.
For the stochastic setting, where (ex, ey) are random, we are mostly interested in an approximate solution with large
probability:
Pr

max
y‚ààY F(ex, y) ‚àímin
x‚ààX F(x, ey) ‚â•Œµ

‚â§œÉ.
(6)
We refer to Œµ as accuracy or precision and to œÉ as conÔ¨Ådence level. Another useful and well-known criteria is small
expectation of the duality gap:
E

max
y‚ààY F(ex, y) ‚àímin
x‚ààX F(x, ey)

‚â§Œµ.
(7)
3

A PREPRINT - DECEMBER 6, 2021
2
Saddle-point representation
Using the dual reformulation of the optimal transport problem and the deÔ¨Ånition of the Wasserstein barycenter, we
obtain the following problem:
min
r‚àà‚àÜn E [W(r,Œæ)] = min
r‚àà‚àÜn E

max
¬µ‚ààRn ‚àí‚ü®Œª‚àó(¬µ, C), r‚ü©‚àí‚ü®¬µ, Œæ‚ü©

.
For this problem, we apply Theorem 14.60 from [49] to the space of all PŒæ-measurable functions F. Clearly, it is
decomposable. It is also clear that the function under maximum is a normal integrand and Ô¨Ånite, since the barycenter is
well-deÔ¨Åned. Thus, we have the next equality:
min
r‚àà‚àÜn E [W(r, Œæ)] = min
r‚àà‚àÜn sup
f¬µ‚ààF
E [‚àí‚ü®Œª‚àó(f¬µ(Œæ), C), r‚ü©‚àí‚ü®f¬µ(Œæ), Œæ‚ü©] .
(8)
Here we call f¬µ : ‚àÜn ‚ÜíRn the function that assigns a value of Kantorovich potential ¬µ for every point in ‚àÜn. In other
words, we are trying to Ô¨Ånd potentials simultaneously for every possible measure. We will refer to such functions as
potential functions. The Wasserstein barycenter is a solution to this stochastic saddle-point problem.
2.1
Bounds for the dual variables
To derive theoretical guarantees of the proposed algorithms, we need to construct a bound on the optimal variable value
in the dual problem (3). We obtain it by using properties of the entropy-regularized optimal transport problem and
some additional assumptions on the barycenter r‚àó. The core idea is to use a known bound from [25, 41] on the optimal
variable in the regularized case and transfer it into the non-regularized case by going to the limit when the regularization
parameter goes to zero.
Entropic regularization.
Firstly, let us deÔ¨Åne the entropy regularized OT problem:
LŒ≥
C(r,c) =
min
X‚ààU(r,c)‚ü®C, X‚ü©‚àíŒ≥H(X),
where H(X) = ‚àíPn
i,j=1 Xi,j log Xi,j is the entropic regularizer. One can rewrite the problem in the equivalent form
using Lagrange multipliers Œª,¬µ:
LŒ≥
C(r,c) = max
Œª,¬µ‚ààRn ‚àí‚ü®Œª, r‚ü©‚àí‚ü®¬µ, c‚ü©‚àíŒ≥
n
X
i,j=1
exp
‚àíCi,j ‚àíŒªi ‚àí¬µj
Œ≥
‚àí1

.
(9)
Denote by Œª‚àó
Œ≥, ¬µ‚àó
Œ≥ the optimal variables for (9). Our goal is to connect optimal dual variables of the regularized and
non-regularized problems. The needed result is presented in the following proposition.
Proposition 1. (Œª‚àó
Œ≥, ¬µ‚àó
Œ≥) converges to a pair of optimal dual variables of the non-regularized problem (2) as Œ≥ ‚Üí0+.
Remark 1. Optimal dual variables of the non-regularized problem are not necessarily unique, but for our purposes, it
is sufÔ¨Åcient to work with one of the solutions.
Proof. Using Theorem 7.17 and Proposition 7.30 from [49], it is sufÔ¨Åcient to prove that the function maximized in (9)
converges point-wise to the function maximized in (2) if in (2) we equivalently change the constraint to its indicator
function in the objective.
Notice that by the properties of the limit it is sufÔ¨Åcient to prove that
Œ≥ exp
Œ±
Œ≥ ‚àí1

‚Üí
Œ≥‚Üí0+
+‚àû
Œ± > 0
0
Œ± ‚â§0
as a function of Œ±.
If Œ± ‚â§0, the claim clearly holds. If Œ± > 0, then
Œ±
Œ≥ ‚àí1 + log Œ≥ = Œ± + Œ≥ log Œ≥
Œ≥
‚àí1 ‚Üí+‚àûas Œ≥ ‚Üí0+.
4

A PREPRINT - DECEMBER 6, 2021
Thus, it is sufÔ¨Åcient to provide a Œ≥-dependent bound for the norm of Œª‚àó
Œ≥, ¬µ‚àó
Œ≥ and take the limit as Œ≥ ‚Üí0+ to obtain a
bound for a pair of optimal dual variables in the non-regularized problem.
To obtain the bounds in the regularized case, we, Ô¨Årst, make the change of variables: u = ‚àíŒª/Œ≥ ‚àí1/2, v = ‚àí¬µ/Œ≥ ‚àí1/2.
Denote by K the matrix e‚àíC/Œ≥. Then we can rewrite (9) in the following form using the notation of [14]:
LŒ≥
C(r,c) = Œ≥ max
v,u [‚ü®u, r‚ü©+ ‚ü®v,c‚ü©‚àí‚ü®1, diag euK diag ev‚ü©] + 2Œ≥.
(10)
Remark 2. If (u‚àó, v‚àó) are optimal in (10), then (u‚àó+ Œ±1, v‚àó‚àíŒ±1) are optimal too. Hence, we can assume
maxi v‚àó= maxi‚àà[n](‚àí¬µ‚àó
Œ≥/Œ≥ ‚àí1/2) = ‚àí1/2 ‚áê‚áímini‚àà[n](¬µ‚àó
Œ≥)i = 0.
Now, under assumption r > 0, c > 0, it is possible to use bounds from Lemma 1 of the work [25] :
max
i (u‚àó)i ‚àímin
i (u‚àó)i ‚â§R,
max
i (v‚àó)i ‚àímin
i (v‚àó)i ‚â§R,
where R := ‚àílog(ŒΩ mini‚àà[n]{ri, ci}), ŒΩ = e‚àí‚à•C‚à•‚àû/Œ≥.
Returning to the original variables:
max
i (¬µ‚àó
Œ≥)i ‚àímin
i (¬µ‚àó
Œ≥)i ‚â§‚à•C‚à•‚àû‚àíŒ≥ log min
i‚àà[n]{ri, ci}.
(11)
We know that mini‚àà[n](¬µ‚àó
Œ≥)i = 0 ‚áê‚áí¬µ‚àó
Œ≥ ‚â•0. Hence, (11) can be rewritten as
‚à•¬µ‚àó
Œ≥‚à•‚àû‚â§‚à•C‚à•‚àû‚àíŒ≥ log min
i‚àà[n]{ri, ci}.
By tending Œ≥ to 0+ and noting that problem (3) is equivalent to problem (2), we prove the following theorem.
Theorem 1. Assume that r,c > 0. Then, there exists an optimal solution ¬µ‚àófor the problem (3) such that ‚à•¬µ‚àó‚à•‚àû‚â§
‚à•C‚à•‚àû.
Additionally, there in an immediate corollary of Theorem 1.
Corollary 1. Assume that (r‚àó, f ‚àó
¬µ) are optimal variables in the saddle-point problem (8) in which all Œæ > 0. Then, if
r‚àó> 0, there exists another optimal ÀÜf¬µ such that ‚à•ÀÜf¬µ‚à•‚àû‚â§‚à•C‚à•‚àûPŒæ-almost surely.
Using Corollary 1, we can equivalently reformulate problem (8) in the following manner:
min
r‚àà‚àÜn E [W(r, Œæ)] = min
r‚àà‚àÜn sup
f¬µ‚ààFb E [‚àí‚ü®Œª‚àó(f¬µ(Œæ), C), r‚ü©‚àí‚ü®f¬µ(Œæ), Œæ‚ü©] ,
(12)
where Fb = {f : ‚àÜn ‚ÜíRn | f is PŒæ-measurable, ‚à•f‚à•‚àû‚â§‚à•C‚à•‚àûPŒæ-a.s.}. We will use this form of the problem in
the further analysis.
Notice that this reformulation is valid in the case of an existence of a positive solution to the original problem. If all Œæ
are positive, then the solution to the original problem is also positive. At the same time, all Œæ can be made positive by
adding the vector of all ones multiplied by a small number, similarly to as it was done in the analysis of the Sinkhorn
algorithm for Wasserstein distances in [25].
3
Algorithms
In this section, we provide algorithms for computation of population Wasserstein barycenters using the saddle-point
formulation (12) combined with an assumption of the existence of a positive optimal r‚àó.
3.1
Finite support case
First we study a simpler situation and assume that the random variable Œæ has a Ô¨Ånite support Supp(Œæ) = {c1, . . . , cm}.
Then, any function f¬µ ‚ààFb can be represented as a matrix of size m √ó n and the value f¬µ(ci) of this function is
deÔ¨Åned as the i-th row of this matrix. The value of this function on points outside of Supp(Œæ) is deÔ¨Åned to be zero. The
corresponding matrix is denoted by M(f¬µ).
5

A PREPRINT - DECEMBER 6, 2021
To make the notation simpler, we say that we have a random variable Œ∂ over indices [m], such that cŒ∂ = Œæ. Also, we
have f¬µ(Œæ) = M(f¬µ)(Œ∂) and denote the uniform distribution over [n] by U([n]) and the distribution associated with r
by P(r).
Hence, if we rewrite (12) in terms of Œ∂ and use the matrix notation, we obtain the following problem
min
r‚àà‚àÜn E [W(r, Œæ)] = min
r‚àà‚àÜn
max
M‚ààMatm√ón(R)
‚à•M‚à•‚àû‚â§‚à•C‚à•‚àû
EŒ∂

‚àí‚ü®Œª‚àó(M(Œ∂), C), r‚ü©‚àí‚ü®M(Œ∂), cŒ∂‚ü©

.
(13)
This non-smooth non-strongly convex-concave saddle-point problem can be solved using the mirror descent algorithm
for saddle point problems. We refer to [45, 10] for additional details on this basic algorithm.
Theorem 2. Algorithm 1 outputs a pair (er, f
M) that satisÔ¨Åes for the problem (13) the duality gap criteria (6) with the
precision Œµ and the conÔ¨Ådence level œÉ in
N = O
n max(n log n, m)‚à•C‚à•2
‚àû
Œµ2
log2
 1
œÉ

iterations and its total complexity is O(nN).
Algorithm 1: Mirror Descent for Finite Support case
Data: N ‚Äì number of iterations;
Result: er ‚Äì approximation of barycenter;
1 begin
2
Set er = r = (1/n, . . . , 1/n) ‚àà‚àÜn;
3
Set M = 0m√ón ‚ààMatm√ón(R);
4
Set Œ± = 2 log n, Œ≤ = 4mn‚à•C‚à•‚àû;
5
Set Œ∑ =
2
‚à•C‚à•‚àû‚àö
8n2 log n+16mn¬∑
‚àö
5N ;
6
for k = 1 to N do
7
Sample t from PŒ∂;
8
Sample s from U([n]);
9
Sample q from P(rk‚àí1);
10
gs := ‚àín ¬∑ maxj‚àà[n](‚àíCs,j ‚àíMt,j);
11
Jq = argmaxj‚àà[n](‚àíCq,j ‚àíMt,j);
12
ht := ‚àíeJq + ct,;
13
rs := rs exp(‚àíŒ± ¬∑ Œ∑ ¬∑ gs);
14
r := r/(Pn
i=1 ri);
15
M(t) := M(t) ‚àíŒ≤ ¬∑ Œ∑ ¬∑ ht;
16
Project M onto ‚Ñì‚àûbox;
17
er := 1
kr + k‚àí1
k er;
18
Return er;
Remark 3. er is an Œµ-approximation of the Wasserstein barycenter with probability at least 1 ‚àíœÉ.
Remark 4. If we assume that ‚à•C‚à•2
‚àû, œÉ are constants and m ‚â•n log n, Algorithm 1 has complexity eO(n2m¬∑Œµ‚àí2). This
is the same complexity as for the Iterative Bregman Projections (IBP) algorithm [6, 38]. In the case when m ‚â§n log n
it is also possible to obtain the complexity O(n2m ¬∑ Œµ‚àí2) but under the weaker convergence criteria (7) (see for details
Remark 5 after the proof of the theorem).
Proof. We have a problem of type
min
r‚ààX max
M‚ààY EŒ®(r,M,Œ∂),
where X := ‚àÜn, Y := {M ‚ààMatm√ón(R) | ‚à•M‚à•‚àû‚â§‚à•C‚à•‚àû}. In our case
Œ®(r,M,Œ∂) = ‚àí‚ü®Œª‚àó(M(Œ∂), C), r‚ü©‚àí‚ü®M(Œ∂), cŒ∂‚ü©,
where Œª‚àóis deÔ¨Åned in (4).
To use the mirror descent algorithm, we need to deÔ¨Åne prox-structures on spaces X and Y.
6

A PREPRINT - DECEMBER 6, 2021
‚Ä¢ As a prox-structure on X we choose the entropy setup with prox-function dX (r) = Pn
i=1 ri log ri. We can
easily bound the prox-diameter of X as R2
X = log n. Then the Bregman projection is calculated by the
exponential weighting in O(n) time. The corresponding norm is the ‚à•¬∑ ‚à•1 norm.
‚Ä¢ As a prox-structure on Y we choose the Euclidean setup with prox-function dY(M) = 1
2‚à•M‚à•2
F . Clearly, we
have R2
Y = m ¬∑ n ¬∑ 2‚à•C‚à•2
‚àû. The Bregman projection can be computed in linear time. The corresponding norm
is the ‚à•¬∑ ‚à•F norm.
Further, we endow the space Z := X √ó Y with the prox-function dZ(r,M) =
1
2 log ndX (r) +
1
4mn‚à•C‚à•2‚àûdY(M).
Next, let us deÔ¨Åne stochastic (sub)gradient oracles. For this purpose, we use a sample t ‚àºPŒ∂ as the source of
randomness. We have:
G(r, M, t) = ‚àÇrŒ®(r, M, t) = ‚àíŒª‚àó(M(t), C),
H(r, M, t) = ‚àÇM(t) [‚àíŒ®(r, M, t)]
=
n
X
i=1
ri‚àÇM(t) max
j‚àà[n] [‚àíCij ‚àíMtj] + ct.
At each iteration oracle H gives us t-th row of the full subgradient matrix (up to the probability of ct) and this way can
be interpreted as a random coordinate oracle similar to random coordinate descent methods.
However, these subgradients are computationally expensive and their calculation requires O(n2) time. To make the
iteration cheaper, we introduce additional randomization and apply a pure random coordinate technique to G and
H. More precisely, using that r ‚àà‚àÜn, we sample index i ‚àà[n] from probability distribution associated with r and
calculate only i-th term of the sum in the deÔ¨Ånition of H. Importantly, such stochastic subgradients remain unbiased
stochastic approximations to the true subgradients.
Then, we can write our Ô¨Ånal unbiased stochastic oracles using the subgradient calculus, namely subgradient of the
maximum:
g(r, M, t, s) = ‚àín ¬∑ max
j‚àà[n](‚àíCs,j ‚àíMt,j) ¬∑ es;
(14)
h(r, M, t, q) = ‚àíeJq(M(t),t) + ct,
(15)
where multiplication by n in g is required to keep the unbiasedness property and Jq(M(t), t) is an index at each the
value Œª‚àó
q(M(t),t) is achieved (see (4)). These subgradients can be computed in linear in n time because they require to
compute maximum only over n values.
For the main complexity result, we need to bound the dual norm of the stochastic subgradients
‚à•g(r, M, t, s)‚à•X ‚àó= n
max
j‚àà[n](‚àíCs,j ‚àíMt,j)
 ‚â§2n‚à•C‚à•‚àû,
‚à•h(r, M, t, q)‚à•Y‚àó‚â§‚à•ct‚à•2 + ‚à•eJq(M(t),ct)‚à•2 ‚â§2.
Combining all the above, for the Ô¨Ånite horizon of N iterations and a constant step-size Œ∑ =
2
L
‚àö
5N , where L2 =
8n2 log n‚à•C‚à•2
‚àû+ 16mn‚à•C‚à•2
‚àû, we apply Proposition 3.2 of [45] and obtain the following complexity bound. After no
more than
N = 5(8n2 log n‚à•C‚à•2
‚àû+ 16mn‚à•C‚à•2
‚àû)
Œµ2

8 + 2 log 2
œÉ
2
iterations Algorithm 1 solves the saddle-point problem (13) with an accuracy Œµ > 0 and a conÔ¨Ådence level œÉ ‚àà(0,1) in
terms of (6).
Remark 5. If we consider convergence of the duality gap in expectation (see (7)), we can obtain better dependence on
m for the number of iterations:
N = O
nm‚à•C‚à•2
‚àû
Œµ2

and for the total complexity: O
 n2m‚à•C‚à•2
‚àû¬∑ Œµ‚àí2
. In this case, the complexity bounds of our algorithm are the same
as that of Iterative Bregman Projections (IBP) algorithm [6, 38] in all the regimes of m compared to n.
7

A PREPRINT - DECEMBER 6, 2021
This improvement is possible since for the analysis in terms of the expectation it is sufÔ¨Åcient to bound the second moment
of the stochastic subgradients rather than provide uniform bounds. The second moment of the dual norm of g can be
bounded as E

‚à•g(r,M,t,s)‚à•2
X ‚àó

‚â§4n‚à•C‚à•2
‚àû=: B2
X , whereas the uniform bound for the dual norm suffers from an
additional ‚àön factor. Moreover, in this setting, it is possible to use dynamic step-sizes to obtain a fully online algorithm
[45].
3.2
Background on the inÔ¨Ånite-dimensional optimization
The approach in the previous subsection can be viewed as a coordinate descent for the ofÔ¨Çine problem since when Œæ has
Ô¨Ånite support the expectation w.r.t. Œæ is actually a Ô¨Ånite-sum. To deal with the purely online setting, we need to make
additional assumptions and optimize our functional over a ¬´good¬ª subspace of Fb (see (12) for the deÔ¨Ånition) where it
can be done effectively. We follow the idea of using kernel spaces used in [28] to approximate Wasserstein distance.
Note that such kernel-based approach is widely used in other machine learning problems [44, 55].
Firstly, following [8], we introduce a necessary background for the inÔ¨Ånite-dimensional analysis. The main instrument
for us is the Fr√®chet derivative, which is deÔ¨Åned almost like the derivative in Ô¨Ånite dimensions, except for the special
requirements on the family of sets. For this special type of derivatives, we also have a chain rule, that we are going to
actively use.
The main challenge is that this derivative is an element of the dual space that typically is not canonically isomorphic to
our space. Hence, we cannot reproduce the standard proof for gradient-descent-like procedures. A possible solution is
to restrict the considerations only to Hilbert spaces and apply the Riesz representation theorem to deÔ¨Åne steps in the
direction of (sub)gradients. In this case, we also have a notion of subgradient f ‚Ä≤(x0) at x0 deÔ¨Åned as
f(x) ‚àíf(x0) ‚â•‚ü®f ‚Ä≤(x0), x ‚àíx0‚ü©, x ‚ààdomf
that is the most crucial part of the convergence proofs for gradient-descent-like procedures.
The only question is how to compute and store this inÔ¨Ånite-dimensional object and how to compute derivatives in f of
linear functionals like Œµx(f) := f(x). For this purpose, we use an additional assumption that the space H is so-called
reproducing kernel Hilbert space.
A Hilbert space H of functions f : X ‚ÜíR is called a reproducing kernel Hilbert space (RKHS) if there is a symmetric
positive-deÔ¨Åned function K: X √ó X ‚ÜíR called a kernel, such that
1. ‚àÄx ‚ààX : K(¬∑, x) ‚ààH;
2. ‚àÄf ‚ààH : ‚ü®f, K(¬∑, x)‚ü©H = f(x);
3. ‚àÄx,y ‚ààX : ‚ü®K(¬∑, x), K(¬∑, y)‚ü©H = K(x,y).
It could be proven [44] by an almost explicit construction that for each symmetric positive-deÔ¨Åned function K there
exists such a Hilbert space. However, there are some kernels that are called universal [55]: they can uniformly
approximate any continuous function if X is a compact subset of Rd. An example of such a kernel is the Gaussian RBF
kernel K(x,x‚Ä≤) = exp(‚àís‚à•x ‚àíx‚Ä≤‚à•2
2). Due to the universal approximation property RKHS-assumption is in a sense
without loss of generality.
3.3
General Kernel Mirror Descent
In this subsection we apply the mirror descent algorithm to our partially inÔ¨Ånite-dimensional problem using the
kernel idea. We Ô¨Åx (H,K) is a RKHS of functions from ‚àÜn to R. Further, we search for an optimal f¬µ in the space
Hn of tuples of function from H that forms a Hilbert space of functions from ‚àÜn to Rn with the inner product
‚ü®f, g‚ü©Hn = Pn
i=1‚ü®fi, gi‚ü©H, where fi,gi are i-th coordinate functions of f and g respectively.
This leads to the following new optimization problem derived from (12) using the RKHS assumption:
min
r‚àà‚àÜn EW(r, Œæ) = min
r‚àà‚àÜn sup
f¬µ‚ààHn
b
E [‚àí‚ü®Œª‚àó(f¬µ(Œæ), C), r‚ü©‚àí‚ü®f¬µ(Œæ), Œæ‚ü©] ,
(16)
where Hn
b = {f ‚ààHn | ‚à•f‚à•‚àû‚â§‚à•C‚à•‚àûPŒæ-a.s.} is an intersection of Hn and Fb (see (12)).
Theoretical dependence on the kernel is hidden in the following two constants: R2 = supf
1
2‚à•f‚à•2
H and Œ∫2 =
supx K(x,x), that fully depend on the chosen kernel. Unfortunately, for most of non-trivial kernels the value of R2 is
inÔ¨Ånite because ‚à•¬∑ ‚à•H and uniform norm ‚à•¬∑ ‚à•‚àûare not equivalent in these inÔ¨Ånite-dimensional spaces. Typically the
value R is used to bound the norm ‚à•f‚à•Hn =
pPn
i=1 ‚à•fi‚à•2
H, where fi is a coordinate function of f. Since R may be
8

A PREPRINT - DECEMBER 6, 2021
unbounded, we introduce to the problem an auxiliary constraint ‚à•f‚à•Hn ‚â§R for a sufÔ¨Åciently large R. In this case, if
an optimal potential function lies in this ball, by Lemma 1 of [3] our algorithm outputs an approximation to a solution
of the original problem. To guarantee that R is sufÔ¨Åciently large, it is possible to use a doubling technique: multiply R
by two and restart the algorithm with the new value of R until convergence. The total complexity will be asymptotically
similar to the case if we choose R = ‚à•f ‚àó
¬µ‚à•Hn.
The following theorem is the main result of this subsection.
Theorem 3. For an arbitrary kernel K and an arbitrary distribution PŒæ s.t. all Œæ > 0, Algorithm 2 (Kernel Mirror
Descent) outputs with probability at least 1‚àíœÉ an Œµ-approximation of the Wasserstein barycenter in terms of the duality
gap (6) in
N = O
 
nŒ∫2R
2 + ‚à•C‚à•2
‚àûlog n
Œµ2
log2
 1
œÉ
!
sample iterations and with
O
 
n3Œ∫4R
4 + ‚à•C‚à•4
‚àûn log2 n
Œµ4
log4
 1
œÉ
!
total complexity.
Remark 6. This algorithm is from the family of Stochastic Approximation (SA) algorithms and the most correct
comparison can be obtained with other SA algorithms. In [17, 18] the following total complexity of SA approach based
on entropy regularization with parameter Œ≥ and Sinkhorn algorithm is shown:
ÀúO
n3‚à•C‚à•2
‚àû
Œµ2
min
1
Œµ
s
n‚à•C‚à•2‚àû
Œ≥¬µ
,
exp
‚à•C‚à•‚àûlog n
Œµ
 ‚à•C‚à•‚àûlog n
Œµ
+ log
‚à•C‚à•‚àûlog n
Œ≥Œµ2
 
,
where ¬µ is a constant of local strong convexity at the optimal point. In this approach it is crucial to choose Œ≥ = O(Œµ) to
obtain an Œµ-approximation of the original non-regularized barycenter and, if we assume that all factors are constant
except n and Œµ, the complexity of our algorithm is better in n and worse in Œµ. At the same time,
1) Choice Œ≥ = O(Œµ) leads to numerical instability of the Sinkhorn algorithm.
2) ¬µ can be dependent on parameter Œ≥ and, due to the choice Œ≥ = O(Œµ), on Œµ.
3) R can be dimension-dependent, at least in the case of the linear kernel. Thus, it is not easy to compare our complexity
bound and the bound of [17, 18]. But, our approach does not use entropy regularization and, thus, does not encounter
the numerical instability of the Sinkhorn algorithm.
Proof. We are dealing with stochastic programming problem of the form
min
r‚ààX max
f¬µ‚ààY EŒ¶(r,f¬µ,Œæ),
where X := ‚àÜn, Y := Hn
b , and
Œ¶(r,f,c) = ‚àí‚ü®Œª‚àó(f(c), C), r‚ü©‚àí‚ü®f(c), c‚ü©.
For X we use the entropy setup exactly as in the Ô¨Ånite support case: there is no effect of the inÔ¨Ånite dimension of
f¬µ. For Y we deÔ¨Åne the prox-function dY(f) = 1
2‚à•f‚à•2
Hn = 1
2
Pn
i=1 ‚à•fi‚à•2
H. The corresponding prox-diameter is
R2
Y = R
2. The corresponding Bregman divergence is
BY(f,g) = 1
2‚à•g‚à•2
Hn ‚àí1
2‚à•f‚à•2
Hn ‚àí‚ü®f, g ‚àíf‚ü©Hn = 1
2‚à•f ‚àíg‚à•Hn.
Hence, the Bregman projection is the projection onto Hn
b . Finally, for the product space Z := X √ó Y we deÔ¨Åne
dZ(r, f¬µ) =
1
2 log ndX (r) +
1
2R
2 dY(f¬µ).
9

A PREPRINT - DECEMBER 6, 2021
Algorithm 2: Kernel Mirror Descent (KMD)
Data: N ‚Äì number of iterations, K ‚Äì kernel, R
2, Œ∫2 ‚Äì constants associated with the kernel;
Result: er ‚Äì approximation of barycenter;
1 begin
2
Set er = r = (1/n, . . . , 1/n) ‚àà‚àÜn;
3
Set f¬µ = 0;
4
Set Œ± = 2 log n, Œ≤ = 2nR2;
5
Set Œ∑ =
2
‚àö
8 log n‚à•C‚à•2‚àû+8n2Œ∫2R
2¬∑
‚àö
5N ;
6
for k = 1 to N do
7
Sample c(k) from PŒæ;
8
for t = 1 to n do
9
(f¬µ(c(k)))t = min{1, max{‚àí1,
k‚àí1
P
i=1
Œ≤(i)
t K(c(k), c(i))}};
10
for i = 1 to n do
11
Ji = arg maxj‚àà[n](‚àíCi,j ‚àíf¬µ(c(k))j);
12
gi = ‚àímaxj‚àà[n](‚àíCi,j ‚àíf¬µ(c(k))j)
13
for t = 1 to n do
14
Œ≤(k)
t
= Œ∑ ¬∑ Œ≤ ¬∑
 
‚àíc(k)
t
+
n
X
i=1
r(k)
i
I{t = Ji(c(k)))}
!
15
r := r ¬∑ exp(‚àíŒ∑ ¬∑ Œ± ¬∑ g);
16
r := r/(Pn
i=1 ri);
17
er := 1
kr + k‚àí1
k er;
18
Return er;
The last component of the analysis is the computation of the subgradients and Ô¨Ånding bounds for their dual norm. For
f¬µ we compute the subgradient coordinate-wise, i.e. for each Ô¨Åxed coordinate t:
G(r, f, c) = ‚àÇrŒ¶(r, f, t) = ‚àíŒª‚àó(f(c), C),
H(r, f, c)t = (‚àÇf(‚àíŒ¶(r,f(c),c)))t
= (‚àÇf(‚àíŒ¶(r,‚ü®f, K(¬∑,c)‚ü©H,c)))t
= K(¬∑, c) ¬∑ (‚àÇ¬µ(‚ü®Œª‚àó(¬µ, C), r‚ü©+ ‚ü®¬µ, c‚ü©))t
= K(¬∑, c)
 
c ‚àí
n
X
i=1
riI{t = Ji(c)}
!
,
where Ji(c) is one of the indices in Œª‚àó
i where the maximum value is achieved. We can bound the norms of both
subgradients as follows:
‚à•G(r,f,c)‚à•‚àû‚â§2‚à•C‚à•‚àû
‚à•H(r, f¬µ, c)t‚à•H ‚â§2‚à•K(¬∑, c)‚à•H ‚â§2Œ∫,
where we used that ‚à•K(¬∑, c)‚à•H =
p
‚ü®K(¬∑,c), K(¬∑, c)‚ü©H =
p
K(c,c) ‚â§Œ∫. Aggregating the bound for each coordinate,
we obtain
‚à•H(r,f¬µ,c)‚à•2
Hn ‚â§4nŒ∫2.
Finally, for a Ô¨Åxed budget of N iterations, we choose a constant step-size Œ∑ =
2
L
‚àö
5N , where L2 = 8 log n‚à•C‚à•2
‚àû+
8nŒ∫2R
2 and, applying the result of [45], we obtain the iteration complexity
N = 5(8 log n‚à•C‚à•2
‚àû+ 8nŒ∫2R
2)
Œµ2

8 + 2 log 2
œÉ
2
10

A PREPRINT - DECEMBER 6, 2021
to Ô¨Ånd an approximate solution with accuracy Œµ > 0 and conÔ¨Ådence level œÉ ‚àà(0,1).
The last step is to calculate the complexity of (sub)gradient computations. Note that the actual value of f (k)
¬µ
is used at
each iteration k. For a step-size sequence Œ∑k the following results gives an explicit formula for calculating f (k)
¬µ
for the
case f (0)
¬µ
= 0.
Proposition 2. If f (0)
¬µ
= 0, we have the following formulas to calculate f (k)
¬µ :
(f (k)
¬µ )t =
k
X
i=1
Œ≤(i)
t
¬∑ K(¬∑, c(i)),
Œ≤(k)
t
= Œ∑k ¬∑
 
‚àíc(k)
t
+
n
X
i=1
r(k)
i
I{t = Ji(c(k)))}
!
,
(17)
where {c(k)} are samples from PŒæ.
Proof. We proceed by induction on k. The basis case k = 0 is clear. The induction step follows from the formula of a
step of the gradient descent:
(f (k)
¬µ )t = (f (k‚àí1)
¬µ
)t ‚àíŒ∑kH(r, f (k‚àí1)
¬µ
, c(k))t = (f (k‚àí1)
¬µ
)t + Œ≤(k)
t
K(¬∑, c(k)).
The proof is Ô¨Ånished by observing that we need O(N) iterations to recalculate one coordinate function of f¬µ. To
calculate the subgradient w.r.t. r we need to calculate the full vector f¬µ for a new sampled measure c(k), which costs
O(nN). Thus, the total complexity is O(nN 2).
We use the described kernel approach with the Gaussian RBF kernel [43]
K(x,x‚Ä≤) = exp
 ‚àís‚à•x ‚àíx‚Ä≤‚à•2
2

,
and an information-diffusion kernel for simplex [39]
K(x,x‚Ä≤) = exp

‚àí1
t arccos2 
‚ü®‚àöx,
‚àö
x‚Ä≤‚ü©

.
These kernels are very similar and their main properties are also similar. Namely, for both kernels Œ∫2 = supx K(x,x) =
1, and the value of R
2 needs to be tuned. Thus, we use it as a hyperparameter of our algorithm. Note that the values of
the hyperparameters s and t need to be tuned as well.
Dynamic step-size.
If we consider the convergence of the duality gap in expectation (see (7)) instead of Ô¨Ånding an
approximate solution with large probability according to (6), we may use a decreasing step-size approach [45] and
obtain essentially the same theoretical complexity bounds. More precisely, we can use step-sizes
Œ∑t =
‚àö
2

supr,r‚Ä≤ BX (r,r‚Ä≤)
2 log n + supf,f ‚Ä≤ BY (f,f ‚Ä≤)
2R
2
1/2
L
‚àö
t
=
‚àö
3
L
‚àö
t, t ‚â•0
and use the sliding average er = (PN
t=1 Œ∑tr(t))/(PN
t=1 Œ∑t) as an output of our algorithm to obtain a fully online
algorithm for an inÔ¨Ånite horizon.
3.4
Linear Kernel Mirror Descent
In this subsection, we study a simple kernel that allows easy computation of f¬µ and all related constants. The price for
this simplicity is a poor family of optimized functions: it is a linear kernel K(x,x‚Ä≤) = ‚ü®x, x‚Ä≤‚ü©and a family of linear
operators ‚àÜn 7‚ÜíRn. For this kernel we can compute both constants:
Œ∫2 = sup
x‚àà‚àÜn‚ü®x,x‚ü©= 1,
R2 =
sup
f¬µ‚ààHn
b
‚à•f¬µ‚à•2
2 ‚â§2n2‚à•C‚à•2
‚àû,
where we used representation of a linear operator as a matrix of size n√ón. Overall, in this setting we have the following
complexity in terms of the number of gradient computations:
N = O
n4‚à•C‚à•2
‚àû
Œµ2
log2
 1
œÉ

.
11

A PREPRINT - DECEMBER 6, 2021
Figure 1: W2-barycenter functional value with various step-size parameters in Algorithm 1. Presented in log-log scale.
Moreover, each computation of the gradient can be done on O(n2) time since any function f¬µ can be represented by a
square matrix Œò, and each gradient step can be represented in the space of matrices. This leads to the following total
complexity
N = O
n6‚à•C‚à•2
‚àû
Œµ2
log2
 1
œÉ

.
On the one hand, this theoretical complexity is quite pessimistic. On the other hand, the main advantage of our algorithm
with the linear kernel is the lack of parameters to tune, and relatively cheap iterations in comparison to KMD with
general kernels and methods based on gradients of the optimal transport distance [17].
Remark 7. All the complexity bounds in sections 3.1, 3.3, 3.4 are based on bounds for the norm of the solution to the
dual problem (3) and for the dual norm of stochastic subgradients. These bounds are typically rough enough and we
may expect that in practice signiÔ¨Åcantly smaller estimates can be used, which may lead to faster convergence due to
larger step-sizes. From this point of view, it may be useful to use adaptive variants of Stochastic Mirror Descent or
Stochastic Mirror Prox): [56, 4, 57] with the adaptive batching idea of [20, 21].
4
Experiments
In this section, we illustrate the practical performance of the proposed algorithms by numerical experiments. Our goal
is to compute population barycenter with respect to W2 distance on various datasets.
4.1
Finite Support Algorithm
Algorithm 1 for the Ô¨Ånite-support setting has good theoretical properties but its practical performance depends on the
choice of the step-size. To demonstrate this effect, we perform experiments on an auxiliary dataset of m = 10 1-d
Gaussian measures. Each measure is discretized over the segment [‚àí10; 10] with n = 300 point masses. We use a
small number of measures to make computation of the objective functional feasible.
We vary dependence on the total number of steps N in the step-size formula of Algorithm 1 from N ‚àí0.5 to
N ‚àí0.33, N ‚àí0.2, N ‚àí0.1 and perform computation with N = 5,000,000 steps. We demonstrate the rate of conver-
gence for all these step-size choices in Figure 1. Additionally, we compare shapes of the Ô¨Ånal measures in the case of a
step-size ‚àºN ‚àí0.33 with the barycenter given by the solution of the corresponding linear program and with output of
the Iterative Bregman Projections (IBP) algorithm with Œ≥ = 3 ¬∑ 10‚àí4 and Œ≥ = 10‚àí2 in Figure 2.
In the case of larger step-size (‚àºN ‚àí0,2 and ‚àºN ‚àí0.1) we observe problems with scaling: there is some point with
much bigger assigned mass than other. At the same time for IBP, smaller values of the regularization parameter lead to
numerical instability of the algorithm whereas the value Œ≥ = 10‚àí3 gives us a very similar picture to Œ≥ = 3 ¬∑ 10‚àí4. We
also can see that IBP outperforms our approach in terms of convergence rate.
4.2
Kernel Mirror Descent.
We study Kernel Mirror Descent Algorithm 2 in three settings:
12

A PREPRINT - DECEMBER 6, 2021
Figure 2: Visual comparison between Algorithm 1, IBP and solution of LP.
‚Ä¢ Gaussian RBF kernel K(x,x‚Ä≤) = exp
 ‚àís‚à•x ‚àíx‚Ä≤‚à•2
2

;
‚Ä¢ Information-Diffusion kernel K(x,x‚Ä≤) = exp

‚àí1
t arccos2 
‚ü®‚àöx,
‚àö
x‚Ä≤‚ü©

;
‚Ä¢ Linear kernel K(x,x‚Ä≤) = ‚ü®x, x‚Ä≤‚ü©.
For the Ô¨Årst two settings we choose a good value of the parameter R
2 and the parameter of the kernel by performing a
grid-search. For the last setting, we implement a more computationally efÔ¨Åcient algorithm that works directly with
matrices as brieÔ¨Çy discussed in Section 3.4.
We compare our approach with two algorithms, described in [17]:
‚Ä¢ Gradient descent with gradients of the regularized Wasserstein distance computed by Sinkhorn algorithm with
stabilization [53];
‚Ä¢ Mirror descent with subgradients of non-regularized Wasserstein distance computed by solving the dual OT
problem via linear programming solver.
We refer to the Ô¨Årst approach as Sinkhorn-based and the second one as direct LP-based.
1-d Gaussian measures.
To compare convergence we consider an auxiliary dataset of randomly generated Gaussian
measures. The main reason to choose this dataset is the ability to compute the true W2-barycenter of the distribution
[16]. We generate 10000 1-d Gaussian measures N(¬µ,œÉ2) with ¬µ sampled from N(1,4) and œÉ sampled from Exp(1/2).
The true population barycenter for this distribution over Gaussian measures is N(1,4), since the expected value of ¬µ is
1 and the expected value of œÉ is 2.
After that, for each measure, we consider its discretization over the segment [‚àí10; 10] with n = 300 point masses. In the
Ô¨Årst step, we consider distributions over mean and variance with much more general support to generate measures that
could be called outliers, and make this model closer to applications. Our goal is to compute approximate W2-barycenter
using these discrete measures.
As a performance measure we choose W2-distance to the true barycenter. We notice that we do not have convergence
guarantees in this metric for our algorithms. However, this is one of the very limited number of ways to evaluate the
quality of a barycenter approximation in the online approach where the expectation can not be expressed analytically.
Firstly, we found optimal parameters for our general kernel methods by the 2-dimensional search over log-scale grid.
For both considered types of general kernels the optimal value was R
2 = 45, optimal s for RBF kernel is 0.02 and
optimal t for information diffusion kernel is 200. We observe a tradeoff between the stability of the learning trajectory
and the functional optimization, as can be seen in Figure 3. The key effect is that it is sufÔ¨Åcient to choose a relatively
small value of R
2 to have good convergence properties. Notice that we do not claim that our algorithm has convergence
guarantees in W2-distance and the effect of stagnation on the level about 5 ¬∑ 10‚àí2 in Figure 3 could be explained by a
lack of convergence in W2-distance.
Secondly, we compare the barycenter approximations obtained by our two kernel methods with the Sinkhorn-based
approach with three values of the regularization parameter Œ≥ = 10‚àí2, 10‚àí4, 10‚àí6. Since the barycenters produced by
13

A PREPRINT - DECEMBER 6, 2021
Figure 3: Change of parameter R
2 with Ô¨Åxed parameter of information-diffusion kernel. Presented in log-log scale.
Kernel Mirror Descent with both studied kernels are visually almost the same, we present only one of them. We also
perform the comparison for Œ≥ = 10‚àí3 and Œ≥ = 10‚àí5, but we choose previous three values as the most illustrative ones.
The result of comparison can be found in Figure 4.
For Sinkhorn-based methods, we faced numerical instabilities in the computation of the gradients that may lead to
numerical errors even using a stabilized algorithm. This computational issue may explain the strange shape of the
approximate barycenter produced by Sinkhorn-based method with Œ≥ = 10‚àí4. Also, notice that the gradient descent
with Sinkhorn-calculated gradients converges not to the true but to the regularized barycenter.
Next, we compare barycenter approximations computed with the linear KMD and the Sinkhorn-based method in Figure
5. Note that our algorithm outperforms Sinkhorn-based method with a relatively big value of Œ≥ = 0.01. For the very
small value Œ≥ = 10‚àí6 the results are close with our approximation being a bit sharper. Moreover, our algorithm is
parameter-free and we do not require to tune the regularization parameter Œ≥ and the number of Sinkhorn iterations to
have a relatively good approximation quality.
A comparison of the approximation by our method with the direct LP-based method is presented in Figure 6, where we
can see that to obtain a good quality, the direct LP-based method requires more measures than KMD.
Finally, we compare all the methods using the proposed quality metric in Figure 7. Gaussian and information diffusion
kernels show almost the same performance. The best method in W2 distance to the true barycenter is the direct LP-based
method.
Also, we can observe that the Sinkhorn-based method with small Œ≥ = 10‚àí6 has an increasing trend in terms of
W2-distance to the true barycenter. This may be due to the convergence to the regularized barycenter and due to
numerical instabilities in gradients caused by the small value of Œ≥: the computation of the gradient becomes less and
less precise.
Additionally, we can see that approximate barycenters produced by the linear KMD outperforms Sinkhorn barycenters
with Œ≥ = 10‚àí2 and Œ≥ = 10‚àí4 not only visually but also in the W2 distance, and the barycenter approximations
produced by the KMD with Gaussian kernel become better than the Sinkhorn-based method when the iteration number
is large.
We compare also the running time of each algorithm in Figure 8 except the direct LP-based method and Sinkhorn-based
methods with Œ≥ = 10‚àí4 and Œ≥ = 10‚àí6 since they spend about 132, 24 and 25 minutes respectively whereas all the
other methods converge in less than 4 minutes. We observe one expected effect: the curves corresponding to the general
14

A PREPRINT - DECEMBER 6, 2021
Figure 4: Comparison between the KMD and the Sinkhorn-based method.
15

A PREPRINT - DECEMBER 6, 2021
Figure 5: Comparison between the linear KMD and the Sinkhorn-based method.
16

A PREPRINT - DECEMBER 6, 2021
Figure 6: Comparison between the KMD and the direct LP-based method.
17

A PREPRINT - DECEMBER 6, 2021
Figure 7: Quality of the estimation in W2-distance to the true barycenter on 1-d Gaussian dataset. Presented in log-log
scale.
Figure 8: Speed of processing new measures on 1-d Gaussian dataset.
KMD are parabolas whereas all other curves are lines. Another effect on this graph is related to the regularization
parameter Œ≥ for the Sinkhorn-based method: smaller parameter value requires more iterations to produce a gradient.
Also we can see that the information diffusion kernel is slightly more computationally efÔ¨Åcient than the Gaussian kernel.
2-d Gaussian measures.
Our next goal is to evaluate performance of our algorithms in the high-dimensional setting.
Our choice is a dataset of randomly generated 2-dimensional Gaussian measures discretized over the grid 50 √ó 50.
Thus, our input histogram measures are elements of a 2500-dimensional simplex.
In this case, LP-based approach is computationally infeasible since the complexity of gradient computation by an LP
solver is O(n3). Thus, we compare the proposed approach with the Sinkhorn-based online method.
First of all, we describe the measure generation process: we sample means of 2-dimensional Gaussians from
N((‚àí1,1)‚ä§, 0.75) and covariance matrices from Inverse-Wishart distribution with parameters Œ¶ =

1
0.1
0.1
0.3

and ŒΩ = 2. To compute the barycenter of these distributions, we sample 2 ¬∑ 106 measures and apply the Ô¨Åxed-point
iteration procedure [16]. After that, we compute histograms of all these measures at the segment [‚àí5, 5]2 over grid
50 √ó 50, add 1e-9 to all elements and normalize to ensure positivity.
18

A PREPRINT - DECEMBER 6, 2021
Figure 9: Quality of the estimation in W2-distance to the true barycenter on 2-d Gaussian dataset. Presented in log-log
scale.
Sinkhorn, Œ≥ =
10‚àí6
10‚àí5
10‚àí4
10‚àí3
10‚àí2
10‚àí1
Time, min.
521.85
491.88
502.63
433.81
354.13
30.01
KMD
Information-diffusion kernel
RBF kernel
Linear kernel
Time, min.
9.42
10.32
5.37
Table 1: Running time comparison for 2-d Gaussian dataset.
To compare our algorithms with the Sinkhorn-based method we used 5000 measures. The comparison in W2-distance
to the true barycenter can be found in Figure 9. The best result is obtained by the Sinkhorn-based algorithm with
Œ≥ = 10‚àí6. Also, notice that our approaches with general kernels perform well whereas linear KMD almost failed.
Running time comparison is presented in Table 1. Notice that values for the smallest regularization parameters almost
coincide because algorithms reached a maximum number of Sinkhorn iterations. In this table, we see that KMD
performs much faster than Sinkhorn-based method whereas it has a relatively good quality that was outperformed only
by using very small Œ≥ = 10‚àí6 with almost 50-time worse running time.
MNIST dataset
First of all, a real-world dataset gives us an opportunity to demonstrate the performance on real
images. We use 2500 images of hand-written digits ¬´3¬ª from the MNIST dataset2 and our goal is to compute their
W2-barycenter in the online setting. The barycenter approximations generated by the Kernel Mirror Descent (Algorithm
2) with an information-diffusion kernel with parameters R
2 = 10, t = 10 during Ô¨Årst 100 iterations can be seen in
Figure 10. Notice that the Sinkhorn-based method does not provide any meaningful pictures during the Ô¨Årst 100
iterations.
Our next goal is to study the inÔ¨Çuence of the kernel parameters and the parameter R
2 for the Kernel Mirror Descent
algorithm with the information diffusion kernel. For this experiment, we run a 2-d grid-search over the parameters R
2
and t. The most interesting results are presented in Figure 11.
2http://yann.lecun.com/exdb/mnist/
Figure 10: Learning of the barycenter of MNIST dataset.
19

A PREPRINT - DECEMBER 6, 2021
Figure 11: Comparison of the barycenter computed by the KMD with an information-diffusion kernel with different
values of R and t.
For the values of t less than 0.004 the image disappears. Thus, we may say that the value of t ‚â•1 can be considered as
a ¬´safe¬ª threshold value for obtaining a good barycenter. However, there is a possibility to vary the ¬´thickness¬ª of the
barycenter by tuning this parameter.
If we Ô¨Åx t and start to vary the parameter R
2, we can see that this parameter affects the ¬´sharpness¬ª of the barycenter.
Additionally, the choice of R
2 can be considered as a new type of regularization.
5
Conclusion
In this work, we consider two algorithms for the stochastic approximation approach to the population Wasserstein
barycenter problem in the discrete setting. This barycenter minimizes the expectation of the Wasserstein distance to
random probability measure generated from some distribution.
The Ô¨Årst algorithm uses an additional assumption that the expectation is taken over a Ô¨Ånite number of measures. This
assumption is very similar to the standard ofÔ¨Çine setting with the SAA-approach when we replace the expectation
with the Ô¨Ånite-sum. This algorithm has similar theoretical complexity to that of the well-known Iterative Bregman
Projections algorithm.
The second algorithm is proposed for a general setting without such restrictive assumptions and uses optimization in
reproducing kernel Hilbert spaces as the main instrument. It is the Ô¨Årst algorithm for the online Wasserstein barycenter
problem that does not use typically expensive or numerically unstable computation of (sub)gradients of the Wasserstein
distance or its regularized version. Moreover, in the experiment section, we show that our approach is practical and
comparable to both main existing online approaches. Also, the use of the simplest possible linear kernel gives us a
practical parameter-free algorithm.
20

A PREPRINT - DECEMBER 6, 2021
Turning to possible extensions, a promising direction for future work is an application of this saddle-point framework to
the entropy-regularized problem. Regularization allows one to use accelerated gradient methods that are known to be
efÔ¨Åcient for solving the regularized optimal transport problem [38, 41]. Another reasonable direction is extension of
this approach to the continuous measure spaces, e.g. the space of Gaussian measures.
References
[1] Agueh, M., Carlier, G.: Barycenters in the wasserstein space. SIAM Journal on Mathematical Analysis 43(2),
904‚Äì924 (2011)
[2] Anikin, A.S., Gasnikov, A.V., Dvurechensky, P.E., Tyurin, A.I., Chernov, A.V.: Dual approaches to the minimiza-
tion of strongly convex functionals with a simple structure under afÔ¨Åne constraints. Computational Mathematics
and Mathematical Physics 57(8), 1262‚Äì1276 (2017)
[3] Antonakopoulos,
K.,
Belmega,
V.,
Mertikopoulos,
P.:
An adaptive mirror-prox method for vari-
ational inequalities with singular operators.
In:
H. Wallach,
H. Larochelle,
A. Beygelzimer,
F. d'Alch√©-Buc,
E. Fox,
R. Garnett (eds.) Advances in Neural Information Processing Systems
32,
pp.
8455‚Äì8465.
Curran
Associates,
Inc.
(2019).
URL
http://papers.nips.cc/paper/
9053-an-adaptive-mirror-prox-method-for-variational-inequalities-with-singular-operators.
pdf
[4] Bach, F., Levy, K.Y.: A universal algorithm for variational inequalities adaptive to smoothness and noise. In:
A. Beygelzimer, D. Hsu (eds.) Proceedings of the Thirty-Second Conference on Learning Theory, Proceedings of
Machine Learning Research, vol. 99, pp. 164‚Äì194. PMLR, Phoenix, USA (2019). URL http://proceedings.
mlr.press/v99/bach19a.html. ArXiv:1902.01637
[5] Bayandina, A., Dvurechensky, P., Gasnikov, A., Stonyakin, F., Titov, A.: Mirror descent and convex optimization
problems with non-smooth inequality constraints. In: P. Giselsson, A. Rantzer (eds.) Large-Scale and Distributed
Optimization, chap. 8, pp. 181‚Äì215. Springer International Publishing (2018). DOI 10.1007/978-3-319-97478-1_8.
ArXiv:1710.06612
[6] Benamou, J.D., Carlier, G., Cuturi, M., Nenna, L., Peyr√©, G.: Iterative bregman projections for regularized
transportation problems. SIAM Journal on ScientiÔ¨Åc Computing 37(2), A1111‚ÄìA1138 (2015)
[7] Beznosikov, A., Dvurechensky, P., Koloskova, A., Samokhina, V., Stich, S.U., Gasnikov, A.: Decentralized local
stochastic extra-gradient for variational inequalities. arXiv:2106.08315 (2021)
[8] Bogachev, V.I., Smolyanov, O.G.: Real and functional analysis, vol. 4. Springer (2020)
[9] Boissard, E., Le Gouic, T., Loubes, J.M.: Distribution‚Äôs template estimate with wasserstein metrics. Bernoulli
21(2), 740‚Äì759 (2015). DOI 10.3150/13-BEJ585. URL https://doi.org/10.3150/13-BEJ585
[10] Bubeck, S.: Convex optimization: Algorithms and complexity. Foundations and Trends¬Æ in Machine Learning
8(3-4), 231‚Äì357 (2015). DOI 10.1561/2200000050. URL http://dx.doi.org/10.1561/2200000050
[11] Chernov, A., Dvurechensky, P., Gasnikov, A.: Fast primal-dual gradient method for strongly convex minimization
problems with linear constraints. In: Y. Kochetov, M. Khachay, V. Beresnev, E. Nurminski, P. Pardalos (eds.)
Discrete Optimization and Operations Research: 9th International Conference, DOOR 2016, Vladivostok, Russia,
September 19-23, 2016, Proceedings, pp. 391‚Äì403. Springer International Publishing (2016)
[12] Chewi, S., Maunu, T., Rigollet, P., Stromme, A.: Gradient descent algorithms for Bures-Wasserstein barycenters.
In: J. Abernethy, S. Agarwal (eds.) Proceedings of Thirty Third Conference on Learning Theory, Proceedings of
Machine Learning Research, vol. 125, pp. 1276‚Äì1304. PMLR (2020). URL http://proceedings.mlr.press/
v125/chewi20a.html
[13] Claici, S., Chien, E., Solomon, J.: Stochastic Wasserstein barycenters. In: J. Dy, A. Krause (eds.) Proceedings of
the 35th International Conference on Machine Learning, Proceedings of Machine Learning Research, vol. 80, pp.
999‚Äì1008. PMLR (2018). URL http://proceedings.mlr.press/v80/claici18a.html
[14] Cuturi, M.: Sinkhorn distances: Lightspeed computation of optimal transport. In: C.J.C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, K.Q. Weinberger (eds.) Advances in Neural Information Processing Systems 26, pp.
2292‚Äì2300. Curran Associates, Inc. (2013)
[15] Cuturi, M., Doucet, A.: Fast computation of wasserstein barycenters. In: E.P. Xing, T. Jebara (eds.) Proceedings
of the 31st International Conference on Machine Learning, Proceedings of Machine Learning Research, vol. 32,
pp. 685‚Äì693. PMLR, Bejing, China (2014). URL http://proceedings.mlr.press/v32/cuturi14.html
[16] Delon, J., Desolneux, A.: A wasserstein-type distance in the space of gaussian mixture models. SIAM J. Imaging
Sci. 13(2), 936‚Äì970 (2020). DOI 10.1137/19M1301047. URL https://doi.org/10.1137/19M1301047
21

A PREPRINT - DECEMBER 6, 2021
[17] Dvinskikh, D.: Stochastic averaging versus sample average approximation for population wasserstein barycenter
calculation. arXiv:2001.07697 (2020)
[18] Dvinskikh, D.: Decentralized algorithms for wasserstein barycenters. arXiv:2105.01587 (2021)
[19] Dvinskikh, D., Gorbunov, E., Gasnikov, A., Dvurechensky, P., Uribe, C.A.: On primal and dual approaches
for distributed stochastic convex optimization over networks. In: 2019 IEEE 58th Conference on Decision and
Control (CDC), pp. 7435‚Äì7440. IEEE (2019). ArXiv:1903.09844
[20] Dvinskikh, D., Ogaltsov, A., Dvurechensky, P., Gasnikov, A., Spokoiny, V.: Adaptive gradient descent for convex
and non-convex stochastic optimization. arXiv preprint arXiv:1911.08380 (2019)
[21] Dvinskikh, D., Ogaltsov, A., Gasnikov, A., Dvurechensky, P., Spokoiny, V.: On the line-search gradient methods for
stochastic optimization. IFAC-PapersOnLine 53(2), 1715‚Äì1720 (2020). DOI https://doi.org/10.1016/j.ifacol.2020.
12.2284.
URL https://www.sciencedirect.com/science/article/pii/S240589632032944X.
21th
IFAC World Congress, arXiv:1911.08380
[22] Dvinskikh, D., Tiapkin, D.: Improved complexity bounds in wasserstein barycenter problem. In: A. Baner-
jee, K. Fukumizu (eds.) Proceedings of The 24th International Conference on ArtiÔ¨Åcial Intelligence and
Statistics, Proceedings of Machine Learning Research, vol. 130, pp. 1738‚Äì1746. PMLR (2021).
URL
http://proceedings.mlr.press/v130/dvinskikh21a.html
[23] Dvurechensky, P., Dvinskikh, D., Gasnikov, A., Uribe, C.A., Nedi¬¥c, A.:
Decentralize and randomize:
Faster algorithm for Wasserstein barycenters.
In: S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, R. Garnett (eds.) Advances in Neural Information Processing Systems 31, NeurIPS
2018, pp. 10783‚Äì10793. Curran Associates, Inc. (2018).
URL http://papers.nips.cc/paper/
8274-decentralize-and-randomize-faster-algorithm-for-wasserstein-barycenters.pdf.
ArXiv:1806.03915
[24] Dvurechensky, P., Gasnikov, A., Gasnikova, E., Matsievsky, S., Rodomanov, A., Usik, I.: Primal-dual method for
searching equilibrium in hierarchical congestion population games. In: Supplementary Proceedings of the 9th
International Conference on Discrete Optimization and Operations Research and ScientiÔ¨Åc School (DOOR 2016)
Vladivostok, Russia, September 19 - 23, 2016, pp. 584‚Äì595 (2016). ArXiv:1606.08988
[25] Dvurechensky, P., Gasnikov, A., Kroshnin, A.: Computational optimal transport: Complexity by accelerated
gradient descent is better than by Sinkhorn‚Äôs algorithm. In: J. Dy, A. Krause (eds.) Proceedings of the 35th
International Conference on Machine Learning, Proceedings of Machine Learning Research, vol. 80, pp. 1367‚Äì
1376 (2018). ArXiv:1802.04367
[26] Dvurechensky, P., Gasnikov, A., Omelchenko, S., Tiurin, A.: A stable alternative to Sinkhorn‚Äôs algorithm for
regularized optimal transport. In: A. Kononov, M. Khachay, V.A. Kalyagin, P. Pardalos (eds.) Mathematical
Optimization Theory and Operations Research, pp. 406‚Äì423. Springer International Publishing, Cham (2020)
[27] Dvurechensky, P., Kamzolov, D., Lukashevich, A., Lee, S., Ordentlich, E., Uribe, C.A., Gasnikov, A.: Hyperfast
second-order local solvers for efÔ¨Åcient statistically preconditioned distributed optimization. arXiv:2102.08246
(2021)
[28] Genevay, A., Cuturi, M., Peyr√©, G., Bach, F.: Stochastic optimization for large-scale optimal transport. In:
Advances in neural information processing systems, pp. 3440‚Äì3448 (2016)
[29] Gorbunov, E., Dvinskikh, D., Gasnikov, A.: Optimal decentralized distributed algorithms for stochastic convex
optimization. arXiv preprint arXiv:1911.07363 (2019)
[30] Gorbunov, E., Rogozin, A., Beznosikov, A., Dvinskikh, D., Gasnikov, A.: Recent theoretical advances in
decentralized distributed convex optimization. arXiv preprint arXiv:2011.13259 (2020)
[31] Guminov, S., Dvurechensky, P., Tupitsa, N., Gasnikov, A.: On a combination of alternating minimization and
Nesterov‚Äôs momentum. In: M. Meila, T. Zhang (eds.) Proceedings of the 38th International Conference on
Machine Learning, Proceedings of Machine Learning Research, vol. 139, pp. 3886‚Äì3898. PMLR, Virtual (2021).
URL http://proceedings.mlr.press/v139/guminov21a.html. ArXiv:1906.03622, WIAS Preprint No.
2695
[32] Guminov, S.V., Nesterov, Y.E., Dvurechensky, P.E., Gasnikov, A.V.: Accelerated primal-dual gradient descent
with linesearch for convex, nonconvex, and nonsmooth optimization problems. Doklady Mathematics 99(2),
125‚Äì128 (2019)
[33] Heinemann, F., Munk, A., Zemel, Y.: Randomised wasserstein barycenter computation: Resampling with statistical
guarantees. arXiv preprint arXiv:2012.06397 (2020)
[34] Hendrikx, H., Bach, F., Massoulie, L.: An optimal algorithm for decentralized Ô¨Ånite sum optimization. arXiv
preprint arXiv:2005.10675 (2020)
22

A PREPRINT - DECEMBER 6, 2021
[35] Hendrikx, H., Xiao, L., Bubeck, S., Bach, F., Massoulie, L.: Statistically preconditioned accelerated gradient
method for distributed optimization. In: H.D. III, A. Singh (eds.) Proceedings of the 37th International Conference
on Machine Learning, Proceedings of Machine Learning Research, vol. 119, pp. 4203‚Äì4227. PMLR (2020). URL
http://proceedings.mlr.press/v119/hendrikx20a.html
[36] Krawtschenko, R., Uribe, C.A., Gasnikov, A., Dvurechensky, P.: Distributed optimization with quantization for
computing wasserstein barycenters. arXiv:2010.14325 (2020). DOI 10.20347/WIAS.PREPRINT.2782. WIAS
preprint 2782
[37] Kroshnin, A., Spokoiny, V., Suvorikova, A.: Statistical inference for Bures‚ÄìWasserstein barycenters. The Annals
of Applied Probability 31(3), 1264 ‚Äì 1298 (2021). DOI 10.1214/20-AAP1618. URL https://doi.org/10.
1214/20-AAP1618
[38] Kroshnin, A., Tupitsa, N., Dvinskikh, D., Dvurechensky, P., Gasnikov, A., Uribe, C.: On the complexity of
approximating Wasserstein barycenters. In: K. Chaudhuri, R. Salakhutdinov (eds.) Proceedings of the 36th
International Conference on Machine Learning, Proceedings of Machine Learning Research, vol. 97, pp. 3530‚Äì
3540. PMLR, Long Beach, California, USA (2019). ArXiv:1901.08686
[39] Lafferty, J., Lebanon, G.: Diffusion kernels on statistical manifolds. Journal of Machine Learning Research 6(Jan),
129‚Äì163 (2005)
[40] Lin, T., Ho, N., Chen, X., Cuturi, M., Jordan, M.I.: Revisiting Ô¨Åxed support wasserstein barycenter: Computational
hardness and efÔ¨Åcient algorithms. arXiv preprint arXiv:2002.04783 (2020)
[41] Lin, T., Ho, N., Jordan, M.: On efÔ¨Åcient optimal transport: An analysis of greedy and accelerated mirror descent
algorithms. In: K. Chaudhuri, R. Salakhutdinov (eds.) Proceedings of the 36th International Conference on
Machine Learning, Proceedings of Machine Learning Research, vol. 97, pp. 3982‚Äì3991. PMLR, Long Beach,
California, USA (2019)
[42] Mensch, A., Peyr√©, G.: Online sinkhorn: Optimal transport distances from sample streams. Advances in Neural
Information Processing Systems 33 (2020)
[43] Minh, H.Q.: Nonparametric stochastic approximation with large step-sizes. Some Properties of Gaussian
Reproducing Kernel Hilbert Spaces and Their Implications for Function Approximation and Learning Theory 44,
307‚Äì338 (2010). DOI 10.1007/s00365-009-9080-0. URL https://doi.org/10.1007/s00365-009-9080-0
[44] Mohri, M., Rostamizadeh, A., Talwalkar, A.: Foundations of machine learning. MIT press (2018)
[45] Nemirovski, A., Juditsky, A., Lan, G., Shapiro, A.: Robust stochastic approximation approach to stochastic
programming. SIAM Journal on Optimization 19(4), 1574‚Äì1609 (2009). DOI 10.1137/070704277. URL
https://doi.org/10.1137/070704277
[46] Nemirovsky, A., Yudin, D.: Problem Complexity and Method EfÔ¨Åciency in Optimization. J. Wiley & Sons, New
York (1983)
[47] Nesterov, Y., Gasnikov, A., Guminov, S., Dvurechensky, P.: Primal-dual accelerated gradient methods with small-
dimensional relaxation oracle. Optimization Methods and Software 0(0), 1‚Äì28 (2020). DOI 10.1080/10556788.
2020.1731747. URL https://doi.org/10.1080/10556788.2020.1731747. Accepted,arXiv:1809.05895
[48] Peyr√©, G., Cuturi, M.: Computational optimal transport. Foundations and Trends in Machine Learning 11(5-6),
355‚Äì607 (2019)
[49] Rockafellar, R.T., Wets, R.J.B.: Variational analysis, vol. 317. Springer Science & Business Media (2009)
[50] Rogozin, A., Beznosikov, A., Dvinskikh, D., Kovalev, D., Dvurechensky, P., Gasnikov, A.: Decentralized
distributed optimization for saddle point problems. arXiv:2102.07758 (2021)
[51] Rogozin, A., Bochko, M., Dvurechensky, P., Gasnikov, A., Lukoshkin, V.: An accelerated method for decentralized
distributed stochastic optimization over time-varying graphs. arXiv:2103.15598 (2021)
[52] Scaman, K., Bach, F., Bubeck, S., Lee, Y.T., Massouli√©, L.: Optimal algorithms for smooth and strongly convex
distributed optimization in networks. In: D. Precup, Y.W. Teh (eds.) Proceedings of the 34th International
Conference on Machine Learning, Proceedings of Machine Learning Research, vol. 70, pp. 3027‚Äì3036. PMLR,
International Convention Centre, Sydney, Australia (2017). URL http://proceedings.mlr.press/v70/
scaman17a.html
[53] Schmitzer, B.: Stabilized sparse scaling algorithms for entropy regularized transport problems. SIAM Journal on
ScientiÔ¨Åc Computing 41(3), A1443‚ÄìA1481 (2019). ArXiv:1610.06519
[54] Staib, M., Claici, S., Solomon, J.M., Jegelka, S.: Parallel streaming wasserstein barycenters. In: I. Guyon, U.V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett (eds.) Advances in Neural Information
23

A PREPRINT - DECEMBER 6, 2021
Processing Systems 30, pp. 2647‚Äì2658. Curran Associates, Inc. (2017). URL http://papers.nips.cc/paper/
6858-parallel-streaming-wasserstein-barycenters.pdf
[55] Steinwart, I., Christmann, A.: Support vector machines. Springer Science & Business Media (2008)
[56] Stonyakin, F., Gasnikov, A., Dvurechensky, P., Alkousa, M., Titov, A.: Generalized Mirror Prox for monotone
variational inequalities: Universality and inexact oracle. arXiv:1806.05140 (2018)
[57] Stonyakin, F., Tyurin, A., Gasnikov, A., Dvurechensky, P., Agafonov, A., Dvinskikh, D., Alkousa, M., Pasechnyuk,
D., Artamonov, S., Piskunova, V.: Inexact model: A framework for optimization and variational inequalities.
Optimization Methods and Software (2021). DOI 10.1080/10556788.2021.1924714. URL https://doi.org/
10.1080/10556788.2021.1924714. WIAS Preprint No. 2709, arXiv:2001.09013, arXiv:1902.00990
[58] Stonyakin, F.S., Dvinskikh, D., Dvurechensky, P., Kroshnin, A., Kuznetsova, O., Agafonov, A., Gasnikov, A.,
Tyurin, A., Uribe, C.A., Pasechnyuk, D., Artamonov, S.: Gradient methods for problems with inexact model of
the objective. In: M. Khachay, Y. Kochetov, P. Pardalos (eds.) Mathematical Optimization Theory and Operations
Research, pp. 97‚Äì114. Springer International Publishing, Cham (2019). ArXiv:1902.09001
[59] Uribe, C.A., Dvinskikh, D., Dvurechensky, P., Gasnikov, A., Nedi¬¥c, A.: Distributed computation of Wasserstein
barycenters over networks. In: 2018 IEEE Conference on Decision and Control (CDC), pp. 6544‚Äì6549 (2018).
ArXiv:1803.02933
24

