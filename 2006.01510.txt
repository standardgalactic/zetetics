arXiv:2006.01510v1  [math.OC]  2 Jun 2020
Recht–R´e Noncommutative Arithmetic-Geometric Mean Conjecture is False
Zehua Lai 1 Lek-Heng Lim 1
Abstract
Stochastic optimization algorithms have become
indispensable in modern machine learning. An
unresolved foundational question in this area is
the difference between with-replacement sam-
pling and without-replacement sampling — does
the latter have superior convergence rate com-
pared to the former?
A groundbreaking re-
sult of Recht and R´e reduces the problem to
a noncommutative analogue of the arithmetic-
geometric mean inequality where n positive num-
bers are replaced by n positive deﬁnite matrices.
If this inequality holds for all n, then without-
replacement sampling indeed outperforms with-
replacement sampling. The conjectured Recht–
R´e inequality has so far only been established
for n = 2 and a special case of n = 3. We
will show that the Recht–R´e conjecture is false
for general n. Our approach relies on the non-
commutative Positivstellensatz, which allows us
to reduce the conjectured inequality to a semidef-
inite program and the validity of the conjecture
to certain bounds for the optimum values, which
we show are false as soon as n = 5.
1. Introduction
The breathtaking reach of deep learning, permeating ev-
ery area of science and technology, has led to an outsize
role for randomized optimization algorithms. It is probably
fair to say that in the absence of randomized algorithms,
deep learning would not have achieved its spectacular
level of success. Fitting an exceedingly high-dimensional
model with an exceedingly large training set would have
been prohibitively expensive without some form of ran-
dom sampling, which in addition provides other crucial
beneﬁts such as saddle-point avoidance (Fang et al., 2019;
*Equal contribution 1Computational and Applied Mathematics
Initiative, University of Chicago, Chicago, IL 60637, USA. Cor-
respondence to: Zehua Lai <laizehua@uchicago.edu>.
Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by
the author(s).
Jin et al., 2017). As such, in machine learning computa-
tions, stochastic variants of gradient descent (Bottou, 2010;
Johnson & Zhang, 2013; Nemirovski et al., 2008), alternat-
ing projections (Strohmer & Vershynin, 2009), coordinate
descent (Nesterov, 2012), and other algorithms have largely
overtaken their classical deterministic counterparts in rele-
vance and utility.
There are numerous random sampling strategies but the
most fundamental question, before all other considerations,
is deciding between sampling with replacement or sam-
pling without replacement. In the vast majority of random-
ized algorithms, a random sample is selected or a random
action is performed with replacement from a pool, making
the randomness in each iteration independent and thus eas-
ier (often much easier) to analyze. However, when it comes
to practical realizations of these algorithms, one invariably
sample without replacement, since they are easier (often
much easier) to implement. Take the ubiquitous stochas-
tic gradient descent for example, many if not most imple-
mentations would pass through each item exactly once in
a random order — this is sampling without replacement.
Likewise, in implementations of randomized coordinate de-
scent, coordinates are usually just chosen in a random order
— again sampling without replacement.
Apart from its ease of implementation, there are other rea-
sons for favoring without-replacement sampling. Empiri-
cal evidence (Bottou, 2009) suggests that in stochastic gra-
dient descent, without-replacement sampling regularly out-
performs with-replacement sampling. Theoretical results
also point towards without-replacement sampling: Under
standard convexity assumptions, the convergence rate of a
without-replacement sampling algorithm typically beats a
with-replacement sampling one by a factor of O(n−1/2) or
O(n−1). This has been established for stochastic gradient
descent (Shamir, 2016; Nagaraj et al., 2019) and for coor-
dinate descent (Beck & Tetruashvili, 2013; Wright, 2015).
Recht & Re (2012) proposed a matrix theoretic approach
to compare the efﬁcacy of with- and without-replacement
sampling methods. Since nearly every common optimiza-
tion algorithm, deterministic or randomized, works with a
linear or quadratic approximation of the objective function
locally, it sufﬁces to examine the two sampling strategies
on linear or quadratic functions to understand their local

Noncommutative Arithmetic-Geometric Mean Conjecture is False
convergence behaviors. In this case, the iteration reduces to
matrix multiplication and both sampling procedures are lin-
early convergent (often called “exponential convergence”
in machine learning).
The question of which is better
then reduces to comparing their linear convergence rates.
In this context, Recht & Re (2012) showed that without-
replacement sampling outperforms with-replacement sam-
pling provided the following noncommutative version of
the arithmetic-geometric mean inequality holds.
Conjecture 1 (Recht & Re 2012). Let n be a positive inte-
ger, A1, . . . , An be symmetric positive semideﬁnite matri-
ces, and ∥· ∥be the spectral norm. Then for any m ≤n,
1
nm

X
1≤j1,...,jm≤n
Aj1 · · · Ajm
 ≥
(n −m)!
n!

X
1≤j1,...,jm≤n,
j1,...,jm distinct
Aj1 · · · Ajm
.
(1)
While one may also ask if (1) holds for other norms, the
most natural and basic choice is the spectral norm, i.e., the
operator 2-norm. Unless speciﬁed otherwise, ∥· ∥will al-
ways denote the spectral norm in this article.
To give an inkling of how (1) arises, consider the Kacz-
marz algorithm (Strohmer & Vershynin, 2009) where we
attempt to solve an overdetermined linear system Cx = b,
C ∈Rp×d, p > d, with ith row vector1 cT
i where ci ∈Rd.
For k = 1, 2, . . ., the (k + 1)th iterate is formed with a
randomly chosen i and
x(k+1) = x(k) + bi −⟨ci, x(k)⟩
∥ci∥2
ci.
The kth error e(k) = x(k) −x∗is then
e(k+1) =

I −cicT
i
∥ci∥2

e(k) =: Pcie(k),
where Pc
∈
Rd×d,
the orthogonal projector onto
span{c}⊥, is clearly symmetric positive semideﬁnite. A
careful analysis would show that the relative efﬁcacy of
with- and without-replacement sampling depends on a mul-
titude of inequalities like ∥A4 + B4 + AB2A + BA2B∥≥
2∥AB2A + BA2B∥, which are difﬁcult to analyze on a
case-by-case basis. Nevertheless, more general heuristics
(Recht & Re, 2012) would lead to (1) — if it holds, then
without-replacement sampling is expected to outperform
with-replacement sampling. In fact, the gap can be signif-
icant — for random Wishart matrices, Recht & Re (2012)
showed that the ratio between the two sides of (1) increases
exponentially with m.
1We adopt standard convention that any vector x ∈Rd is a
column vector; a row vector will always be denoted xT.
Current status:
Recht & Re (2012) applied results from
random matrix theory to show that Conjecture 1 holds with
high probability for (i) independent Wishart matrices, and
(ii) the incremental gradient method. To date, extensive
numerical simulations have produced no counterexample.
Conjecture 1 has been rigorously established only in very
special cases, notably for (m, n) = (2, 2) (Recht & Re,
2012) and (m, n) = (3, 3k) (Zhang, 2018).
Our contributions:
We show how to transform Conjec-
ture 1 into a form where the noncommutative Positivstel-
lensatz applies, which implies in particular that for any spe-
ciﬁc values of m and n, the conjecture can be checked via
two semideﬁnite programs. This allows us to show in Sec-
tion 3 that the conjecture is false as soon as m = n = 5.
We also establish in Section 2 that the conjecture holds for
m = 2 and 3 with arbitrary n by extending the approach
in Zhang (2018). While the conjectured inequality (1) is
clearly sharp (as we may choose all Ai’s to be equal) when-
ever it is true, we show in Section 5 that the m = 2 case
may nonetheless be improved in a different sense, and we
do likewise for m = 3 in Section 2. The m = 4 case re-
mains open but our noncommutative Positivstellensatz ap-
proach permits us to at least check that it holds for n = 4
and 5 in Section 3.
Over the next two sections, we will transform Recht and
R´e’s Conjecture 1 into a “Loewner form” (Conjecture 1A),
a “sum-of-squares form” (Conjecture 1B), and ﬁnally a
“semideﬁnite program form” (Conjecture 1C). All four con-
jectures are equivalent but the correctness of the last one for
any m, n can be readily checked as a semideﬁnite program.
2. Recht–R´e inequality for m = 2 and 3
Our goal here is to establish (1) for a pair and a triple of
matrices. In so doing, we take Conjecture 1 a step closer
to a form where noncommutative Positivstellensatz applies.
There is independent value in establishing these two special
cases given that the classical noncommutative arithmetic-
geometric-harmonic mean inequality (Bhatia & Holbrook,
2006) is only known for a pair of matrices but nonetheless
attracted a lot of interests from linear algebraists. These
special cases also have implications on randomized algo-
rithms — take the Kaczmarz algorithm for example, the
fact that Conjecture 1 holds for m = 2 and 3 implies that if
we randomly choose two or three distinct samples, perform
the iterations, and sample again, then this “replacing after
every two or three samples” strategy will converge faster
than a “replacing after every sample” strategy.
We begin by providing some context for the inequality (1).
The usual arithmetic-geometric mean inequality for n non-

Noncommutative Arithmetic-Geometric Mean Conjecture is False
negative real numbers a1, . . . , an, i.e.,
(a1 + · · · + an)/n ≥(a1 · · · an)1/n,
is a special case of Maclaurins inequality (Hardy et al.,
1988): If we deﬁne
sm :=
1
  n
m

X
1≤j1<···<jm≤n
aj1 · · · ajm,
then s1 ≥√s2 ≥· · · ≥
n√sn. So s1 ≥
m√sm gives us
1
nm (a1 + · · · + an)m ≥(n −m)!
n!
X
1≤j1,...,jm≤n,
j1,...,jm distinct
aj1 · · · ajm,
which is just (1) for 1-by-1 positive semideﬁnite matrices.
For real symmetric or complex Hermitian matrices A, B,
the Loewner order is deﬁned by A ⪰B iff A−B is positive
semideﬁnite. The Maclaurins inequality has several non-
commutative extensions but we regard the following as the
starting point for all noncommutative arithmetic-geometric
mean inequalities.
Proposition 1. For any unitary invariant norm ∥· ∥and
Hermitian matrices A, B, ∥AB + BA∥≤∥A2 + B2∥and
2∥AB + BA∥≤∥(A + B)2∥.
Proof. Since −A2 −B2 ⪯AB + BA ⪯A2 + B2, by
Lemma 2.1 in Bhatia & Kittaneh (2008), the desired in-
equalities hold for any unitary invariant norm.
The result was extended to compact operators on a separa-
ble Hilbert space and strengthened to 2∥A∗B∥≤∥A∗A +
B∗B∥in Bhatia & Kittaneh (1990), with yet other exten-
sions in Bhatia & Kittaneh (2008; 2000). In Recht & Re
(2012), Conjecture 1 was also formulated as an extension
of Proposition 1, with the second inequality corresponding
to the m = n = 2 case.
Straightforward counterexamples for n = 3 show that we
cannot simply drop the norm in (1) and replace the inequal-
ity ≥with the Loewner order ⪰. Nevertheless Conjecture 1
may be written as two Loewner inequalities, as demon-
strated by Zhang (2018).
Conjecture 1A (Loewner form). Let A1, . . . , An be sym-
metric positive semideﬁnite and A1 +· · ·+An ⪯nI. Then
for any m ≤n,
−
n!
(n −m)!I ⪯
X
1≤j1,...,jm≤n,
j1,...,jm distinct
Aj1 · · · Ajm ⪯
n!
(n −m)!I. (2)
We prefer this equivalent formulation (2) as the original for-
mulation (1) hides an asymmetry — note that there is an
upper bound and a lower bound in (2) and there is no rea-
son to expect that they should have the same magnitude.
In fact, as we will see in the later sections, the upper and
lower bounds have different magnitudes in every case that
we examined.
We will next prove Conjecture 1 in its equivalent form Con-
jecture 1A for m = 2 and 3. Our proofs rely on techniques
introduced by Zhang (2018) in his proof for the case m = 3,
n = 3k, but our two additional contributions are that (i) we
will obtain better lower bounds (deferred to Section 5), and
(ii) our proof will work for arbitrary n (not necessarily a
multiple of 3).
Theorem 1 (Recht–R´e for m = 2). Let A1, . . . , An be
symmetric positive semideﬁnite and A1 + · · · + An ⪯nI.
Then
−n(n −1)I ⪯
X
i̸=j
AiAj ⪯n(n −1)I.
(3)
Proof. The right inequality in (3) follows from
(n −1)
X
i,j
AiAj −n
X
i̸=j
AiAj
= (n −1)
X
i
A2
i −
X
i̸=j
AiAj =
X
i<j
(Ai −Aj)2 ⪰0,
and so
X
i̸=j
AiAj ⪯n −1
n
X
i,j
AiAj ⪯n(n −1)I.
For the left inequality in (3), expand (P
i Ai)2 ⪰0 to get
X
i
A2
i ⪰−
X
i̸=j
AiAj.
Let B := nI −P
i Ai ⪰0 and Bi := Ai + 1
nB ⪰0. So
P
i Bi = nI. Then
−n
X
i̸=j
AiAj = −(n −1)
X
i̸=j
AiAj −
X
i̸=j
AiAj
⪯(n −1)
X
i
A2
i −
X
i̸=j
AiAj
=
X
i<j
(Ai −Aj)2 =
X
i<j
(Bi −Bj)2
= (n −1)
X
i
B2
i −
X
i̸=j
BiBj
= n
X
i
B2
i −
X
i
Bi
2
= n
X
i
B2
i −n2I.

Noncommutative Arithmetic-Geometric Mean Conjecture is False
Therefore
−
X
i̸=j
AiAj−(n−1)nI ⪯
X
i
B2
i −n2I =
X
i
(B2
i −nBi).
The eigenvalues of Bi fall between 0 and n, so the eigen-
values of B2
i −nBi are all nonpositive, i.e., B2
i −nBi ⪯0.
Hence −P
i̸=j AiAj −(n −1)nI ⪯0.
The right inequality of (3) is clearly sharp. In Section 5, we
will prove a stronger result, improving the constant in the
left inequality of (3) to n(n −1)/4.
Following Zhang (2018), we write Ei1,...,ik for expectation
or average over all indices 1 ≤i1, . . . , ik ≤n, and eEi1,...,ik
for that over distinct indices 1 ≤i1, . . . , ik ≤n.
Theorem 2 (Recht–R´e for m = 3). Let A1, . . . , An be
symmetric positive semideﬁnite and A1 + · · · + An ⪯nI.
Then
−I ⪯eEi,j,kAiAjAk ⪯I.
(4)
Proof. Let A, B, C be positive semideﬁnite. Then ABC +
CBA ⪯ABA + CBC. If B ⪯C, then ABA ⪯ACA.
We start with the right inequality of (4),
eEi,j,kAiAjAk = 1
2
eEi,j,k(AiAjAk + AkAjAi)
⪯1
2
eEi,j,k(AiAjAi + AkAjAk)
= eEi,j,kAiAjAi.
Fix a positive integer l < n whose value we decide later.
eEi,j,kAiAjAk ⪯eEi,j,k
h
1 −1
l

AiAjAk + 1
l AiAjAi
i
=
1
l2(n −l)
eEi1,...,in

(Ai1 + · · · + Ail)
· (Ail+1 + · · · + Ain)(Ai1 + · · · + Ail)

.
Since Ail+1 + · · · + Ain ⪯nI −(Ai1 + · · · + Ail),
eEi,j,kAiAjAk ⪯
1
l2(n −l)
eEi1,...,il

(Ai1 + · · · + Ail)
·
 nI −(Ai1 + · · · + Ail)

(Ai1 + · · · + Ail)

.
Consider the function f(x) = x2(n −x). Let the line y =
cx + d be tangent to f at x = l. We require that c ≥0
and f(x) ≤cx + d for 0 ≤x ≤n. Elementary calculation
shows that such a line exists as long as 1/2 ≤l/n ≤2/3.
Let A = Ai1 + · · · + Ail. As cA + dI and A(nI −A)A
are simultaneous diagonalizable, each eigenvalue of cA +
dI −A(nI −A)A can be obtained by applying the function
g(x) = cx + d −x2(n −x) to an eigenvalue of A. Hence
eEi,j,kAiAjAk
⪯
1
l2(n −l)
eEi1,...,il

c(Ai1 + · · · + Ail) + dI

⪯
cl + d
l2(n −l)I,
where the ﬁrst inequality follows from the fact that it holds
for each eigenvalue. Note that if we choose A1 = · · · =
An = I, all inequalities above as well as the right inequal-
ity of (4) hold with equality. So as long as 1/2 ≤l/n ≤
2/3, l, c, d will give us
1
l2(n −l)(cl + d) = 1
and thus the right inequality of (4).
For the left inequality of (4), we start by noting
(A1 + · · · + An−1)An(A1 + · · · + An−1) ⪰0.
Taking expectation, we have −(n −2)eEi,j,kAiAjAk ⪯
eEi,j,kAiAjAi and thus
−eEi,j,kAiAjAk
= −n −2
n −1
eEi,j,kAiAjAk −
1
n −1
eEi,j,kAiAjAk
⪯
1
n −1
eEi,j,k(AiAjAi −AiAjAk)
⪯
1
2(n −1)
eEi,j,k

(Ai −Aj)Ak(Ai −Aj)

.
As in the proof of Theorem 1, set B := nI −P
i Ai ⪰0
and Bi := Ai + 1
nB ⪰0. Then
−eEi,j,kAiAjAk
⪯
1
2(n −1)
eEi,j,k

(Bi −Bj)Bk(Bi −Bj)

=
1
n −1
eEi,j,kBiBjBi −
1
n −1
eEi,j,kBiBjBk.
Let Xi := Bi(nI−Bi)Bi and Yi := (nI−Bi)Bi(nI−Bi).
Routine calculations give
eEiXi = (n −1)eEi,j,kBiBjBi,
eEiYi = (n −1)eEi,j,kBiBjBi
+ (n −1)(n −2)eEi,j,kBiBjBk,
which
allows
us
to
express
eEi,j,kBiBjBi
and
eEi,j,kBiBjBk in terms of eEiXi and eEiYi. Then
−eEi,j,kAiAjAk
⪯
1
n −1
eEi,j,kBiBjBi −
1
n −1
eEi,j,kBiBjBk
=
1
(n −1)2(n −2)
eEi[(n −1)Xi −Yi]

Noncommutative Arithmetic-Geometric Mean Conjecture is False
=
n
(n −1)2(n −2)
eEi[−Bi(Bi −nI)(Bi −I)].
As −x(x −n)(x −1) ≤(n −1)2x/4 for 0 ≤x ≤n,
−eEi,j,kAiAjAk ⪯
n
4(n −2)
eEiBi =
n
4(n −2)I.
When n ≥3, we have n/
 4(n −2)

≤1.
Our proof in fact shows that the constant in the left inequal-
ity of (4) can be improved to n/
 4(n −2)

. Nevertheless,
we will see in the next section (Table 1) that this is not
sharp.
3. Noncommutative Positivstellensatz
In a seminal paper (Helton, 2002), Helton proved an as-
tounding result: Every positive polynomial in noncommu-
tative variables can be written as a sum of squares of poly-
nomials. The corresponding statement for usual polynomi-
als, i.e., in commutative variables, is well-known to be false
and is the subject of Hilbert’s 17th Problem. Subsequent
developments ultimately led to a noncommutative version
of the Positivstellensatz for semialgebraic sets. We refer
interested readers to Pascoe (2018) for an overview of this
topic.
Stating noncommutative Positivstellensatz will require that
we introduce some terminologies. Let X1, . . . , Xn be n
noncommutative variables, i.e., XiXj ̸= XjXi whenever
i ̸= j. A monomial of degree d or a word of length d is an
expression of the form Xi1 · · · Xid. The monomials span
a real inﬁnite-dimensional vector space R⟨X1, . . . , Xn⟩,
called the space of noncommutative polynomials.
For
any d ∈N, the ﬁnite-dimensional subspace of noncom-
mutative polynomials of degree ≤
d will be denoted
R⟨X1, . . . , Xn⟩d. The transpose of f ∈R⟨X1, . . . , Xn⟩
is denoted f T and is deﬁned on monomials by reversing
the order of variables (Xi1 · · · Xid)T = Xid · · · Xi1 and ex-
tended linearly to all of R⟨X1, . . . , Xn⟩. If f T = f, then f
is called symmetric.
The bottom line is that noncommutative polynomials may
be evaluated on square matrices of the same dimensions,
i.e., they deﬁne matrix-valued functions of matrix variables.
For our purpose, if A1, . . . , An are real symmetric matrices,
then f(A1, . . . , An) is also a matrix, but it may not be a
symmetric matrix unless f is a symmetric polynomial.
Let L = {ℓ1, . . . , ℓk} ⊆R⟨X1, . . . , Xn⟩1 be a set of k
linear polynomials, i.e., d = 1. We will refer to ℓ1, . . . , ℓk
as linear constraints and
BL := {(A1, . . . , An) | ℓ1(A1, . . . , An) ⪰0, . . . ,
ℓk(A1, . . . , An) ⪰0}
as the feasible set.
Note that elements of BL are n tu-
ples of symmetric matrices. We say that BL is bounded if
there exists r > 0 such that all (A1, . . . , An) ∈BL satisfy
∥A1∥≤r, . . . , ∥An∥≤r. Let d ∈N. We write
Σd(L) :=
 k
X
i=1
pi
X
j=1
f
T
ijℓifij

fij ∈R⟨X1, . . . , Xn⟩d,
k, p1, . . . , pk ∈N

for the set of noncommutative sum-of-squares generated
by L. The following theorem is a simpliﬁed version of
the noncommutative Positivstellensatz, i.e., Theorem 1.1 in
Helton et al. (2012), that will be enough for our purpose.
Theorem 3 (Noncommutative Positivstellensatz). Let f be
a symmetric polynomial with deg(f) ≤2d + 1 and the
feasible set BL be bounded with nonempty interior. Then
f(A1, . . . , An) ⪰0 for all (A1, . . . , An) ∈BL
if and only if f ∈Σd(L).
Readers familiar with the commutative Positivstellsatz
(Lasserre, 2015) would see that the noncommutative ver-
sion is, surprisingly, much simpler and neater.
To avoid notational clutter, we introduce the shorthand
X
ji̸=jk
:=
X
1≤j1,...,jm≤n,
j1,...,jm distinct
for sum over distinct indices. Applying Theorem 3 with
linear constraints X1 ⪰0, . . . , Xn ⪰0, X1 + · · · + Xn ⪯
nI, Conjecture 1A becomes the following.
Conjecture 1B (Sum-of-squares form). Let m ≤n ∈
N and d = ⌊m/2⌋.
For the linear constraints ℓ1 =
X1, . . . , ℓn = Xn, ℓn+1 = n −X1 −· · · −Xn, let
λ1 = argmin

λ ∈R
 λ −
X
ji̸=jk
Xj1 · · · Xjm ∈Σd(L)

,
λ2 = argmin

λ ∈R
 λ +
X
ji̸=jk
Xj1 · · · Xjm ∈Σd(L)

.
Then both λ1 and λ2 ≤n!/(n −m)!.
In polynomial optimization (Lasserre, 2015), the commuta-
tive Positivstellsatz is used to transform a constrained opti-
mization problem into a sum-of-squares problem that can
in turn be transformed into a semideﬁnite programming
(SDP) problem. Helton (2002) has observed that this also
applies to noncommutative polynomial optimization prob-
lems, i.e., we may further transform Conjecture 1B into an
SDP form.
The vector space R⟨X1, . . . , Xn⟩d has dimension q :=
1+n+n2 +· · ·+nd and a basis comprising all q monomi-
als of degree ≤d. We will assemble all basis elements into

Noncommutative Arithmetic-Geometric Mean Conjecture is False
a q-tuple of monomials that we denote by β. With respect
to this basis, any f ∈R⟨X1, . . . , Xn⟩d may be represented
uniquely as f = βTu for some u ∈Rq. Therefore a non-
commutative square may be expressed as
p
X
j=1
f
T
j ℓfj =
p
X
j=1
u
T
jβℓβ
Tuj = tr

βℓβ
T
 p
X
j=1
uju
T
j

by simply writing fj = βTuj, uj ∈Rq, j = 1, . . . , p.
Since a symmetric matrix Y is positive semideﬁnite iff it
can be written as Y = Pp
j=1 ujuT
j, we obtain the follow-
ing one-to-one correspondence between noncommutative
squares and positive semideﬁnite matrices:
p
X
j=1
f
T
j ℓfj ∈R⟨X1, . . . , Xn⟩2d+1, fj ∈R⟨X1, . . . , Xn⟩d
~w
p
X
j=1
uju
T
j ∈Rq×q, uj ∈Rq.
With this correspondence, the two minimization problems
in Conjecture 1B become two SDPs.
Conjecture 1C (Semideﬁnite program form). Let m ≤
n ∈N and d = ⌊m/2⌋. Let β be a monomial basis of
R⟨X1, . . . , Xn⟩d and let Xn+1 = n −X1 −· · · −Xn. Let
λ1 be the minimum value of the SDP:
minimize
λ
subject to
λ −
X
ji̸=jk
Xj1 · · · Xjm =
n+1
X
i=1
tr(βXiβ
TYi),
Y1 ⪰0, . . . , Yn+1 ⪰0;
(5)
and λ2 be that of the SDP:
minimize
λ
subject to
λ +
X
ji̸=jk
Xj1 · · · Xjm =
n+1
X
i=1
tr(βXiβ
TYi),
Y1 ⪰0, . . . , Yn+1 ⪰0.
(6)
Then both λ1 and λ2 ≤n!/(n −m)!.
Note that the minimization is over the scalar variable λ
and the matrix variables Y1, . . . , Yn+1; the equality con-
straint equating two noncommutative polynomials is sim-
ply saying that the coefﬁcients on both sides are equal, i.e.,
for each monomial, we get a linear constraint involving
λ, Y1, . . . , Yn+1 — the Xi’s play no role other than to serve
as placeholders for these linear constraints. We may ex-
press (5) and (6) as SDPs in standard form with a single
matrix variable Y := diag(λ, Y1, . . . , Yn+1), see (7) for
example.
Readers acquainted with (commutative) polynomial opti-
mization (Lasserre, 2015) would be familiar with the above
λ1
λ2
n!/(n −m)!
m = 2, n = 2
2.0000
0.5000
2
m = 2, n = 3
6.0000
1.5000
6
m = 2, n = 4
12.0000
3.0000
12
m = 2, n = 5
20.0000
5.0000
20
m = 3, n = 3
6.0000
3.4113
6
m = 3, n = 4
24.0000
8.5367
24
m = 3, n = 5
60.0000
17.3611
60
m = 4, n = 4
24.0000
22.4746
24
m = 4, n = 5
120.0000
80.2349
120
m = 5, n = 5
120.0000
144.6488
120
Table 1. Results from the SDPs in Conjecture 1C for m ≤n ≤5.
The bold entry for λ2 shows that the Recht–R´e conjecture is false
for m = n = 5 since 144.6488 > 120.
discussions. In fact, the only difference between the com-
mutative and noncommutative cases is that Pd
i=0 ni, the
size of a noncommutative monomial basis, is much larger
than
 d+n
d

, the size of a commutative monomial basis.
For any ﬁxed values of m and n, Conjecture 1C is in a form
that can be checked by standard SDP solvers. The dimen-
sion of the SDP grows exponentially with m, and without
access to signiﬁcant computing resources, only small val-
ues of m, n are within reach. Fortuitously, m = n = 5 al-
ready yields the required violation 144.6488 ≰120, show-
ing that Conjecture 1C and thus Conjecture 1 is false in
general. We tabulate our results for m ≤n ≤5 in Table 1.
The fact that the SDP in (6) for m = n = 5 has a minimum
λ2 > 144 > 120 = 5! shows that there are uncountably
many instances with A1 ⪰0, A2 ⪰0, A3 ⪰0, A4 ⪰0,
A5 ⪰0, and A1 + A2 + A3 + A4 + A5 ⪯5I such that the
matrix
X
σ∈S5
Aσ(1)Aσ(2)Aσ(3)Aσ(4)Aσ(5)
has an eigenvalue that is less than −144 < −120 = −5!.
Here Sn is the symmetric group on n elements. We em-
phasize that neither (6) nor its dual would give us ﬁve such
matrices explicitly, although the dual does provide another
way to verify our result, as we will see in Section 4.
Indeed, the beauty of the noncommutative Positivstellen-
satz approach is that it allows us to show that Conjecture 1
is false for m = n = 5 without actually having to produce
ﬁve positive semideﬁnite matrices A1, . . . , A5 that violates
the inequality (1). It would be difﬁcult to ﬁnd A1, . . . , A5
explicitly as one does not even know the smallest dimen-
sions required for these matrices to give a counterexample
to (1). Our approach essentially circumvents the issue by
replacing them with noncommutativevariables X1, . . . , X5
— the reader may have observed that the dimensions of the
matrices A1, . . . , A5 did not make an appearance anywhere
in this article.

Noncommutative Arithmetic-Geometric Mean Conjecture is False
4. Veriﬁcation via Farkas
We take a closer look at the m = n = 5 case that provided
a refutation to the Recht–R´e conjecture. In this case, the
basis β has 1 + 5 + 52 = 31 monomials; the SDP in (6)
has 1 + 5 + 52 + 53 + 54 + 55 = 3906 linear constraints,
312 × 6 + 1 = 5767 variables, and takes the form:
minimize
tr(C0Y )
subject to
tr(CiY ) = bi,
i = 1, . . . , 3906,
Y = diag(λ, Y1, . . . , Y6) ⪰0.
(7)
Here C0, C1, . . . , C3906 ∈S187
++ , b ∈R3096, λ is a scalar
variable, and Y1, . . . , Y6 are 31-by-31 symmetric matrix
variables. To put (7) into standard form, the block diago-
nal structure of Y may be further encoded as linear con-
straints requiring that off-diagonal blocks be zero. The out-
put of our program gives a minimizer of the form Y ∗=
diag(λ∗, Y ∗
1 , . . . , Y ∗
6 ) ∈S187
++ with
λ∗= 144.6488,
Y ∗
1 , . . . , Y ∗
6 ∈S31
++.
(8)
The actual numerical entries of the matrices appearing in
(7) and (8) are omitted due to space constraints; but they
can be found in the output of our program (code in supple-
ment).
The values in (8) are of course approximate because of the
inherent errors in numerical computations. In our opinion,
the gap between the computed 144.6488 and the conjec-
tured 120 is large enough to override any concerns of a
mistaken conclusion resulting from numerical errors. Nev-
ertheless, to put to rest any lingering doubts, we will di-
rectly show that the conjectured value λ = 120 is infeasible
by producing a Farkas certiﬁcate. Consider the feasibility
problem:
minimize
0
subject to
tr(CiY ) = bi,
i = 1, . . . , 3906,
tr(C0Y ) = 120,
Y ⪰0,
(9)
with C0, C1, . . . , C3906 ∈S187
++ and b ∈R3096 as in (7).
Note that C0 = e1eT
1 is the matrix with one in the (1, 1)th
entry and zero everywhere else. So (9) is the feasibility
problem of the optimization problem (7) with the additional
linear constraint y11 = 120 and where we have disregarded
the block diagonal constraints2 on Y . The dual of (9) is
maximize
120y0 + bTy
subject to
y0C0 + y1C1 + · · · + y3906C3906 ⪯0.
Our program produces a Farkas certiﬁcate y ∈R3096 with
120y0 + bTy ≈47.3 > 0, implying that (9) is infeasible.
2If (9) is already infeasible, then adding these block diagonal
constraints just makes it even more infeasible.
While this is a consequence of Farkas Lemma for SDP
(Lasserre, 1995), all we need is the following trivial ver-
sion.
Lemma 1. Let m, n ∈N. Let C0, C1, . . . , Cm ∈Sn and
b ∈Rm+1. If there exists a y ∈Rm+1 with
y0C0 + · · · + ymCm ⪯0,
b
Ty > 0,
then there does not exist a Y ∈Sn with
tr(C0Y ) = b0, . . . , tr(CmY ) = bm,
Y ⪰0.
Proof. If such a Y exists, then
0 ≥tr
 (y0C0+· · ·+ymCm)Y

= y0b0+· · ·+ymbm > 0,
a contradiction.
Hence a matrix of the form
Y = diag(120, Y1, . . . , Y6) ∈S187
is infeasible for (7), providing another refutation of Con-
jecture 1C and thus Conjecture 1. In particular, showing
that λ = 120 is infeasible for (7) does not require any of
the values computed in (8). Of course, aside from being
the conjectured value of λ2, there is nothing special about
λ = 120 — for any λ < 144.6488, we may similarly com-
pute a Farkas certiﬁcate y to show that such a value of λ is
infeasible for (7).
We conclude with a few words on the computational costs
of the SDPs in this and the last section. Our resulting dense
linear system for m = n = 5 requires 3906 × 5767 ≈22
million ﬂoating point storage. Using a personal computer
with an Intel Core i7-9700k processor and 16GB of RAM,
our SeDuMi (Sturm, 1999) program in Matlab takes 150
seconds. For m = n = 6, storage alone would have taken
26 billion ﬂoating numbers, beyond our modest computing
resources.
5. Improving the Recht–R´e inequality
An unexpected beneﬁt of the noncommutative Positivstel-
lensatz approach is that it leads to better bounds for the
m = 2 and 3 cases that we know are true. Observe that the
values for λ2 in Table 1 for m = 2 are exactly smaller than
the values for n!/(n−m)! by a factor of 1/4. This suggests
that the Recht–R´e inequality (3) for m = 2 in Theorem 1
may be improved to
−1
4n(n −1)I ⪯
X
i̸=j
AiAj ⪯n(n −1)I.
Table 1 only shows this for n = 2, 3, 4, 5 but in this section,
we will give a proof for arbitrary n ≥2. Although our

Noncommutative Arithmetic-Geometric Mean Conjecture is False
proof below does not depend on the SDP formulation in (6),
the correct coefﬁcients in (11) for arbitrary n would have
been impossible to guess without solving (6) for m = 2
and some small values of n.
So far we have not explored the symmetry evident in our
formulations of the Recht–R´e inequality: In Conjecture 1A,
the matrix expression
λI ±
X
ji̸=jk
Aj1 · · · Ajm
and the constraints A1 ⪰0, . . . , An ⪰0, A1 + · · · + An ⪯
nI are clearly invariant under any permutation σ ∈Sn. In
Conjecture 1C, the noncommutative sum-of-squares
λ ±
X
ji̸=jk
Xj1 · · · Xjm =
n+1
X
i=1
tr(βXiβ
TYi),
(10)
where Xn+1 = n −X1 −· · · −Xn, is also invariant under
Sn and so we may average over all permutations to get a
symmetrized sum-of-squares. For commutative polynomi-
als, results from classical invariant theory are often used to
take advantage of symmetry (Gatermann & Parrilo, 2004).
We will see next that such symmetry may also be exploited
for noncommutative polynomials.
Consider the case m = 2, n = 3. The monomial basis
of R⟨X1, X2, X3⟩1 is β = (1, X1, X2, X3).
The sym-
metry imposes linear constraints on the matrix variables
Y1, Y2, Y3, Y4 in (6), requiring them to take the following
forms:
Y1 =


a
b
c
c
b
d
e
e
c
e
f
g
c
e
g
f

,
Y2 =


a
c
b
c
c
f
e
g
b
e
d
e
c
g
e
f

,
Y3 =


a
c
c
b
c
f
g
e
c
g
f
e
b
e
e
d

,
Y4 =


x
y
y
y
y
z
w
w
y
w
z
w
y
w
w
z

.
These symmetries allow us to drastically reduce the degree
of freedom in our SDP: For any m = 2, n ≥2, the matrices
Y1, . . . , Yn are always determined by precisely 11 variables
that we label a, b, c, d, e, f, g, x, y, z, w. We computed their
values explicitly for n = 2, 3, 4. For n = 2,
Y1 =


5
4
−3
4
1
4
−3
4
1
2
0
1
4
0
1
2

,
Y3 =


1
4
−1
4
−1
4
−1
4
1
2
0
−1
4
0
1
2

,
and Y2 can be determined from Y1. For n = 3,
Y1 =


5
2
−1
0
0
−1
4
9
1
9
1
9
0
1
9
4
9
1
9
0
1
9
1
2
4
9

, Y4 =


1
2
−1
3
−1
3
−1
3
−1
3
4
9
1
9
1
9
−1
3
1
9
4
9
1
9
−1
3
1
9
1
2
4
9

,
and Y2, Y3 can be determined from Y1. For n = 4,
Y1 =


15
4
−9
8
−1
8
−1
8
−1
8
−9
8
3
8
1
8
1
8
1
8
−1
8
1
8
3
8
1
8
1
8
−1
8
1
8
1
8
3
8
1
8
−1
8
1
8
1
8
1
8
3
8


,
Y5 =


3
4
−3
8
−3
8
−3
8
−3
8
−3
8
3
8
1
8
1
8
1
8
−3
8
1
8
3
8
1
8
1
8
−3
8
1
8
1
8
3
8
1
8
−3
8
1
8
1
8
1
8
3
8


,
and Y2, Y3, Y4 can be determined from Y1. The rational
numbers above are all chosen by observing the ﬂoating
numbers output of the SDP (6).
The values of the matrices Yi’s for n = 2, 3, 4 allow us to
guess that the variables a, b, c, d, e, f, g, x, y, z, w are:
a = 5(n −1)
4
,
b = −3(n −1)
2n
,
c = 3 −n
2n ,
d = f = z = 2(n −1)
n2
,
e = g = w = n −2
n2
,
x = n −1
4
,
y = −n −1
2n .
(11)
The proof of our next theorem will ascertain that these
choices are indeed correct — they yield the sum-of-squares
decomposition in (10) for m = 2.
Theorem 4 (Better Recht–R´e for m = 2). Let A1, . . . , An
be positive semideﬁnite matrices. If A1 + · · · + An ⪯nI,
then
−1
4n(n −1)I ⪯
X
i̸=j
AiAj ⪯n(n −1)I.
Proof. The upper bound has already been established in
Theorem 1. It remains to establish the lower bound. We
start from the following readily veriﬁable inequalities
1
2n(n −1)(Aj −Ak)Ai(Aj −Ak) ⪰0,
(12)
5(n −1)
4

I −6
5nAi + 2(3 −n)
5n(n −1)
X
j̸=i
Aj

(13)
Ai

I −6
5nAi + 2(3 −n)
5n(n −1)
X
j̸=i
Aj

⪰0,
n −1
5n2

Ai + 2n −1
n −1
X
j̸=i
Aj

(14)
Ai

Ai + 2n −1
n −1
X
j̸=i
Aj

⪰0,

Noncommutative Arithmetic-Geometric Mean Conjecture is False
1
2n2 (Aj −Ak)

n −
X
i
Ai

(Aj −Ak) ⪰0,
(15)
n −1
4

I −2
n
X
i
Ai

n −
X
i
Ai

(16)

I −2
n
X
i
Ai

⪰0.
Sum (12) over all distinct i, j, k; sum (13) over all i; sum
(14) over all i; sum (15) over all distinct j, k; add all results
to (16). The ﬁnal inequality is our required lower bound.
For n = m = 2, the new lower bound is sharp. Take
A1 =
"
3
2
0
0
0
#
,
A2 =
"
1
6
√
2
3
√
2
3
4
3
#
,
then ∥A1+A2∥= 2 and the smallest eigenvalue of A1A2+
A2A1 is −1/2. We conjecture that this bound is sharp for
all m = 2, n ≥2.
The method in this section also extends to higher m. For
example, we may impose symmetry constraints for m =
n = 3 and see if the Y1, Y2, Y3, Y4 obtained have rational
values, and if so write down a sums-of-squares proof by
factoring the Yi’s.
6. Conclusion and open problems
We conclude our article with a discussion of some open
problems and why we think the Recht–R´e conjecture, while
false as it is currently stated, only needs to be reﬁned.
An immediate open question is whether the conjecture is
true for m = 4: Table 1 shows that it holds for (m, n) =
(4, 4) and (4, 5); we suspect that it is true for all n ≥4.
As we pointed out after Conjecture 1A, the Recht–R´e in-
equality as stated in (1) conceals an asymmetry — it actu-
ally contains two inequalities, as shown in (2). What we
have seen is that the lower bound is never attained in any of
the cases we have examined. For m = 2 and 3, the lower
bound is too large, and we improved it in Theorem 4 and
the proof of Theorem 2 respectively. For m = 5, the lower
bound is too small, which is why the Recht–R´e inequality
is false. A natural follow-up question is then: “What is the
correct lower bound?” On the other hand, we conjecture
that the remaining half of the Recht–R´e inequality, i.e., the
upper bound in (2), holds true for all m ≤n ∈N.
Recht & Re (2012) has another conjecture similar to Con-
jecture 1 but where the norms appear after the summation.
Conjecture 2 (Recht & Re 2012). Let A1, . . . , An be pos-
itive semideﬁnite matrices. Then
1
nm
X
1≤j1,...,jm≤n
∥Aj1 · · · Ajm∥≥
(n −m)!
n!
X
1≤j1,...,jm≤n,
j1,...,jm distinct
∥Aj1 · · · Ajm∥.
This has been established for m = 2 and 3 for any unitary
invariant norm in Israel et al. (2016). It is not clear to us if
the noncommutative Positivstellensatz might perhaps also
shed light on this related conjecture.
Lastly, if our intention is to analyze the relative efﬁcacies
of with- and without-replacement sampling strategies in
randomized algorithms, then it is more pertinent to study
these inequalities for random matrices, i.e., we do not just
assume that the indices are random variables but also the en-
tries of the matrices. For example, if we want to analyze the
Kaczmarz algorithm, then we ought to take expectation not
only with respect to all permutations but also with respect
to how we generate the entries of the matrices. This would
provide a more realistic platform for comparing with- and
without-replacement sampling strategies.
References
Beck, A. and Tetruashvili, L. On the convergence of block
coordinate descent type methods. SIAM J. Optim., 23(4):
2037–2060, 2013.
Bhatia, R. and Holbrook, J. Noncommutative geometric
means. Math. Intelligencer, 28(1):32–39, 2006.
Bhatia, R. and Kittaneh, F.
On the singular values of a
product of operators. SIAM J. Matrix Anal. Appl., 11(2):
272–277, 1990.
Bhatia, R. and Kittaneh, F.
Notes on matrix arithmetic-
geometric mean inequalities. Linear Algebra Appl., 308
(1-3):203–211, 2000.
Bhatia, R. and Kittaneh, F.
The matrix arithmetic-
geometric mean inequality revisited.
Linear Algebra
Appl., 428(8-9):2177–2191, 2008.
Bottou, L. Curiously fast convergence of some stochastic
gradient descent algorithms. In Proceedings of the sym-
posium on learning and data science, Paris, 2009.
Bottou, L.
Large-scale machine learning with stochas-
tic gradient descent.
In Proceedings of COMP-
STAT’2010, pp. 177–186. Physica-Verlag/Springer, Hei-
delberg, 2010.
Fang, C., Lin, Z., and Zhang, T. Sharp analysis for non-
convex sgd escaping from saddle points.
In Beygelz-
imer, A. and Hsu, D. (eds.), Proceedings of the Thirty-
Second Conference on Learning Theory, volume 99 of

Noncommutative Arithmetic-Geometric Mean Conjecture is False
Proceedings of Machine Learning Research, pp. 1192–
1234, Phoenix, USA, 2019.
Gatermann, K. and Parrilo, P. A.
Symmetry groups,
semideﬁnite programs, and sums of squares.
J. Pure
Appl. Algebra, 192(1-3):95–128, 2004.
Hardy, G. H., Littlewood, J. E., and P´olya, G. Inequalities.
Cambridge Mathematical Library. Cambridge University
Press, Cambridge, 1988. Reprint of the 1952 edition.
Helton, J. W. “Positive” noncommutative polynomials are
sums of squares.
Ann. of Math. (2), 156(2):675–694,
2002.
Helton, J. W., Klep, I., and McCullough, S. The convex
Positivstellensatz in a free algebra. Adv. Math., 231(1):
516–534, 2012.
Israel, A., Krahmer, F., and Ward, R.
An arithmetic-
geometric mean inequality for products of three matrices.
Linear Algebra Appl., 488:1–12, 2016.
Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan,
M. I. How to escape saddle points efﬁciently. In Precup,
D. and Teh, Y. W. (eds.), Proceedings of the 34th Interna-
tional Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pp. 1724–
1732, International Convention Centre, Sydney, Aus-
tralia, 2017.
Johnson, R. and Zhang, T. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances
in neural information processing systems, pp. 315–323,
2013.
Lasserre, J. B. A new Farkas lemma for positive semidef-
inite matrices.
IEEE Trans. Automat. Control, 40(6):
1131–1133, 1995.
Lasserre, J. B. An introduction to polynomial and semi-
algebraic optimization, volume 52. Cambridge Univer-
sity Press, 2015.
Nagaraj, D., Jain, P., and Netrapalli, P.
SGD without
replacement: Sharper rates for general smooth convex
functions.
In International Conference on Machine
Learning, pp. 4703–4711, 2019.
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Ro-
bust stochastic approximation approach to stochastic pro-
gramming. SIAM J. Optim., 19(4):1574–1609, 2008.
Nesterov, Y. Efﬁciency of coordinate descent methods on
huge-scale optimization problems. SIAM J. Optim., 22
(2):341–362, 2012.
Pascoe, J. E. Positivstellens¨atze for noncommutative ratio-
nal expressions. Proc. Amer. Math. Soc., 146(3):933–
937, 2018.
Recht, B. and Re, C. Toward a noncommutative arithmetic-
geometric mean inequality: Conjectures, case-studies,
and consequences.
In Mannor, S., Srebro, N., and
Williamson, R. C. (eds.), Proceedings of the 25th Annual
Conference on Learning Theory, volume 23 of Proceed-
ings of Machine Learning Research, pp. 11.1–11.24, Ed-
inburgh, Scotland, 2012.
Shamir, O. Without-replacement sampling for stochastic
gradient methods.
In Advances in neural information
processing systems, pp. 46–54, 2016.
Strohmer, T. and Vershynin, R. A randomized Kaczmarz
algorithm with exponential convergence. J. Fourier Anal.
Appl., 15(2):262–278, 2009.
Sturm, J. F. Using SeDuMi 1.02, a MATLAB toolbox for
optimization over symmetric cones. Optimization meth-
ods and software, 11(1-4):625–653, 1999.
Wright, S. J. Coordinate descent algorithms. Math. Pro-
gram., 151(1, Ser. B):3–34, 2015.
Zhang, T. A note on the matrix arithmetic-geometric mean
inequality.
Electron. J. Linear Algebra, 34:283–287,
2018.

