arXiv:2006.01510v1  [math.OC]  2 Jun 2020
Rechtâ€“RÂ´e Noncommutative Arithmetic-Geometric Mean Conjecture is False
Zehua Lai 1 Lek-Heng Lim 1
Abstract
Stochastic optimization algorithms have become
indispensable in modern machine learning. An
unresolved foundational question in this area is
the difference between with-replacement sam-
pling and without-replacement sampling â€” does
the latter have superior convergence rate com-
pared to the former?
A groundbreaking re-
sult of Recht and RÂ´e reduces the problem to
a noncommutative analogue of the arithmetic-
geometric mean inequality where n positive num-
bers are replaced by n positive deï¬nite matrices.
If this inequality holds for all n, then without-
replacement sampling indeed outperforms with-
replacement sampling. The conjectured Rechtâ€“
RÂ´e inequality has so far only been established
for n = 2 and a special case of n = 3. We
will show that the Rechtâ€“RÂ´e conjecture is false
for general n. Our approach relies on the non-
commutative Positivstellensatz, which allows us
to reduce the conjectured inequality to a semidef-
inite program and the validity of the conjecture
to certain bounds for the optimum values, which
we show are false as soon as n = 5.
1. Introduction
The breathtaking reach of deep learning, permeating ev-
ery area of science and technology, has led to an outsize
role for randomized optimization algorithms. It is probably
fair to say that in the absence of randomized algorithms,
deep learning would not have achieved its spectacular
level of success. Fitting an exceedingly high-dimensional
model with an exceedingly large training set would have
been prohibitively expensive without some form of ran-
dom sampling, which in addition provides other crucial
beneï¬ts such as saddle-point avoidance (Fang et al., 2019;
*Equal contribution 1Computational and Applied Mathematics
Initiative, University of Chicago, Chicago, IL 60637, USA. Cor-
respondence to: Zehua Lai <laizehua@uchicago.edu>.
Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by
the author(s).
Jin et al., 2017). As such, in machine learning computa-
tions, stochastic variants of gradient descent (Bottou, 2010;
Johnson & Zhang, 2013; Nemirovski et al., 2008), alternat-
ing projections (Strohmer & Vershynin, 2009), coordinate
descent (Nesterov, 2012), and other algorithms have largely
overtaken their classical deterministic counterparts in rele-
vance and utility.
There are numerous random sampling strategies but the
most fundamental question, before all other considerations,
is deciding between sampling with replacement or sam-
pling without replacement. In the vast majority of random-
ized algorithms, a random sample is selected or a random
action is performed with replacement from a pool, making
the randomness in each iteration independent and thus eas-
ier (often much easier) to analyze. However, when it comes
to practical realizations of these algorithms, one invariably
sample without replacement, since they are easier (often
much easier) to implement. Take the ubiquitous stochas-
tic gradient descent for example, many if not most imple-
mentations would pass through each item exactly once in
a random order â€” this is sampling without replacement.
Likewise, in implementations of randomized coordinate de-
scent, coordinates are usually just chosen in a random order
â€” again sampling without replacement.
Apart from its ease of implementation, there are other rea-
sons for favoring without-replacement sampling. Empiri-
cal evidence (Bottou, 2009) suggests that in stochastic gra-
dient descent, without-replacement sampling regularly out-
performs with-replacement sampling. Theoretical results
also point towards without-replacement sampling: Under
standard convexity assumptions, the convergence rate of a
without-replacement sampling algorithm typically beats a
with-replacement sampling one by a factor of O(nâˆ’1/2) or
O(nâˆ’1). This has been established for stochastic gradient
descent (Shamir, 2016; Nagaraj et al., 2019) and for coor-
dinate descent (Beck & Tetruashvili, 2013; Wright, 2015).
Recht & Re (2012) proposed a matrix theoretic approach
to compare the efï¬cacy of with- and without-replacement
sampling methods. Since nearly every common optimiza-
tion algorithm, deterministic or randomized, works with a
linear or quadratic approximation of the objective function
locally, it sufï¬ces to examine the two sampling strategies
on linear or quadratic functions to understand their local

Noncommutative Arithmetic-Geometric Mean Conjecture is False
convergence behaviors. In this case, the iteration reduces to
matrix multiplication and both sampling procedures are lin-
early convergent (often called â€œexponential convergenceâ€
in machine learning).
The question of which is better
then reduces to comparing their linear convergence rates.
In this context, Recht & Re (2012) showed that without-
replacement sampling outperforms with-replacement sam-
pling provided the following noncommutative version of
the arithmetic-geometric mean inequality holds.
Conjecture 1 (Recht & Re 2012). Let n be a positive inte-
ger, A1, . . . , An be symmetric positive semideï¬nite matri-
ces, and âˆ¥Â· âˆ¥be the spectral norm. Then for any m â‰¤n,
1
nm

X
1â‰¤j1,...,jmâ‰¤n
Aj1 Â· Â· Â· Ajm
 â‰¥
(n âˆ’m)!
n!

X
1â‰¤j1,...,jmâ‰¤n,
j1,...,jm distinct
Aj1 Â· Â· Â· Ajm
.
(1)
While one may also ask if (1) holds for other norms, the
most natural and basic choice is the spectral norm, i.e., the
operator 2-norm. Unless speciï¬ed otherwise, âˆ¥Â· âˆ¥will al-
ways denote the spectral norm in this article.
To give an inkling of how (1) arises, consider the Kacz-
marz algorithm (Strohmer & Vershynin, 2009) where we
attempt to solve an overdetermined linear system Cx = b,
C âˆˆRpÃ—d, p > d, with ith row vector1 cT
i where ci âˆˆRd.
For k = 1, 2, . . ., the (k + 1)th iterate is formed with a
randomly chosen i and
x(k+1) = x(k) + bi âˆ’âŸ¨ci, x(k)âŸ©
âˆ¥ciâˆ¥2
ci.
The kth error e(k) = x(k) âˆ’xâˆ—is then
e(k+1) =

I âˆ’cicT
i
âˆ¥ciâˆ¥2

e(k) =: Pcie(k),
where Pc
âˆˆ
RdÃ—d,
the orthogonal projector onto
span{c}âŠ¥, is clearly symmetric positive semideï¬nite. A
careful analysis would show that the relative efï¬cacy of
with- and without-replacement sampling depends on a mul-
titude of inequalities like âˆ¥A4 + B4 + AB2A + BA2Bâˆ¥â‰¥
2âˆ¥AB2A + BA2Bâˆ¥, which are difï¬cult to analyze on a
case-by-case basis. Nevertheless, more general heuristics
(Recht & Re, 2012) would lead to (1) â€” if it holds, then
without-replacement sampling is expected to outperform
with-replacement sampling. In fact, the gap can be signif-
icant â€” for random Wishart matrices, Recht & Re (2012)
showed that the ratio between the two sides of (1) increases
exponentially with m.
1We adopt standard convention that any vector x âˆˆRd is a
column vector; a row vector will always be denoted xT.
Current status:
Recht & Re (2012) applied results from
random matrix theory to show that Conjecture 1 holds with
high probability for (i) independent Wishart matrices, and
(ii) the incremental gradient method. To date, extensive
numerical simulations have produced no counterexample.
Conjecture 1 has been rigorously established only in very
special cases, notably for (m, n) = (2, 2) (Recht & Re,
2012) and (m, n) = (3, 3k) (Zhang, 2018).
Our contributions:
We show how to transform Conjec-
ture 1 into a form where the noncommutative Positivstel-
lensatz applies, which implies in particular that for any spe-
ciï¬c values of m and n, the conjecture can be checked via
two semideï¬nite programs. This allows us to show in Sec-
tion 3 that the conjecture is false as soon as m = n = 5.
We also establish in Section 2 that the conjecture holds for
m = 2 and 3 with arbitrary n by extending the approach
in Zhang (2018). While the conjectured inequality (1) is
clearly sharp (as we may choose all Aiâ€™s to be equal) when-
ever it is true, we show in Section 5 that the m = 2 case
may nonetheless be improved in a different sense, and we
do likewise for m = 3 in Section 2. The m = 4 case re-
mains open but our noncommutative Positivstellensatz ap-
proach permits us to at least check that it holds for n = 4
and 5 in Section 3.
Over the next two sections, we will transform Recht and
RÂ´eâ€™s Conjecture 1 into a â€œLoewner formâ€ (Conjecture 1A),
a â€œsum-of-squares formâ€ (Conjecture 1B), and ï¬nally a
â€œsemideï¬nite program formâ€ (Conjecture 1C). All four con-
jectures are equivalent but the correctness of the last one for
any m, n can be readily checked as a semideï¬nite program.
2. Rechtâ€“RÂ´e inequality for m = 2 and 3
Our goal here is to establish (1) for a pair and a triple of
matrices. In so doing, we take Conjecture 1 a step closer
to a form where noncommutative Positivstellensatz applies.
There is independent value in establishing these two special
cases given that the classical noncommutative arithmetic-
geometric-harmonic mean inequality (Bhatia & Holbrook,
2006) is only known for a pair of matrices but nonetheless
attracted a lot of interests from linear algebraists. These
special cases also have implications on randomized algo-
rithms â€” take the Kaczmarz algorithm for example, the
fact that Conjecture 1 holds for m = 2 and 3 implies that if
we randomly choose two or three distinct samples, perform
the iterations, and sample again, then this â€œreplacing after
every two or three samplesâ€ strategy will converge faster
than a â€œreplacing after every sampleâ€ strategy.
We begin by providing some context for the inequality (1).
The usual arithmetic-geometric mean inequality for n non-

Noncommutative Arithmetic-Geometric Mean Conjecture is False
negative real numbers a1, . . . , an, i.e.,
(a1 + Â· Â· Â· + an)/n â‰¥(a1 Â· Â· Â· an)1/n,
is a special case of Maclaurins inequality (Hardy et al.,
1988): If we deï¬ne
sm :=
1
  n
m

X
1â‰¤j1<Â·Â·Â·<jmâ‰¤n
aj1 Â· Â· Â· ajm,
then s1 â‰¥âˆšs2 â‰¥Â· Â· Â· â‰¥
nâˆšsn. So s1 â‰¥
mâˆšsm gives us
1
nm (a1 + Â· Â· Â· + an)m â‰¥(n âˆ’m)!
n!
X
1â‰¤j1,...,jmâ‰¤n,
j1,...,jm distinct
aj1 Â· Â· Â· ajm,
which is just (1) for 1-by-1 positive semideï¬nite matrices.
For real symmetric or complex Hermitian matrices A, B,
the Loewner order is deï¬ned by A âª°B iff Aâˆ’B is positive
semideï¬nite. The Maclaurins inequality has several non-
commutative extensions but we regard the following as the
starting point for all noncommutative arithmetic-geometric
mean inequalities.
Proposition 1. For any unitary invariant norm âˆ¥Â· âˆ¥and
Hermitian matrices A, B, âˆ¥AB + BAâˆ¥â‰¤âˆ¥A2 + B2âˆ¥and
2âˆ¥AB + BAâˆ¥â‰¤âˆ¥(A + B)2âˆ¥.
Proof. Since âˆ’A2 âˆ’B2 âª¯AB + BA âª¯A2 + B2, by
Lemma 2.1 in Bhatia & Kittaneh (2008), the desired in-
equalities hold for any unitary invariant norm.
The result was extended to compact operators on a separa-
ble Hilbert space and strengthened to 2âˆ¥Aâˆ—Bâˆ¥â‰¤âˆ¥Aâˆ—A +
Bâˆ—Bâˆ¥in Bhatia & Kittaneh (1990), with yet other exten-
sions in Bhatia & Kittaneh (2008; 2000). In Recht & Re
(2012), Conjecture 1 was also formulated as an extension
of Proposition 1, with the second inequality corresponding
to the m = n = 2 case.
Straightforward counterexamples for n = 3 show that we
cannot simply drop the norm in (1) and replace the inequal-
ity â‰¥with the Loewner order âª°. Nevertheless Conjecture 1
may be written as two Loewner inequalities, as demon-
strated by Zhang (2018).
Conjecture 1A (Loewner form). Let A1, . . . , An be sym-
metric positive semideï¬nite and A1 +Â· Â· Â·+An âª¯nI. Then
for any m â‰¤n,
âˆ’
n!
(n âˆ’m)!I âª¯
X
1â‰¤j1,...,jmâ‰¤n,
j1,...,jm distinct
Aj1 Â· Â· Â· Ajm âª¯
n!
(n âˆ’m)!I. (2)
We prefer this equivalent formulation (2) as the original for-
mulation (1) hides an asymmetry â€” note that there is an
upper bound and a lower bound in (2) and there is no rea-
son to expect that they should have the same magnitude.
In fact, as we will see in the later sections, the upper and
lower bounds have different magnitudes in every case that
we examined.
We will next prove Conjecture 1 in its equivalent form Con-
jecture 1A for m = 2 and 3. Our proofs rely on techniques
introduced by Zhang (2018) in his proof for the case m = 3,
n = 3k, but our two additional contributions are that (i) we
will obtain better lower bounds (deferred to Section 5), and
(ii) our proof will work for arbitrary n (not necessarily a
multiple of 3).
Theorem 1 (Rechtâ€“RÂ´e for m = 2). Let A1, . . . , An be
symmetric positive semideï¬nite and A1 + Â· Â· Â· + An âª¯nI.
Then
âˆ’n(n âˆ’1)I âª¯
X
iÌ¸=j
AiAj âª¯n(n âˆ’1)I.
(3)
Proof. The right inequality in (3) follows from
(n âˆ’1)
X
i,j
AiAj âˆ’n
X
iÌ¸=j
AiAj
= (n âˆ’1)
X
i
A2
i âˆ’
X
iÌ¸=j
AiAj =
X
i<j
(Ai âˆ’Aj)2 âª°0,
and so
X
iÌ¸=j
AiAj âª¯n âˆ’1
n
X
i,j
AiAj âª¯n(n âˆ’1)I.
For the left inequality in (3), expand (P
i Ai)2 âª°0 to get
X
i
A2
i âª°âˆ’
X
iÌ¸=j
AiAj.
Let B := nI âˆ’P
i Ai âª°0 and Bi := Ai + 1
nB âª°0. So
P
i Bi = nI. Then
âˆ’n
X
iÌ¸=j
AiAj = âˆ’(n âˆ’1)
X
iÌ¸=j
AiAj âˆ’
X
iÌ¸=j
AiAj
âª¯(n âˆ’1)
X
i
A2
i âˆ’
X
iÌ¸=j
AiAj
=
X
i<j
(Ai âˆ’Aj)2 =
X
i<j
(Bi âˆ’Bj)2
= (n âˆ’1)
X
i
B2
i âˆ’
X
iÌ¸=j
BiBj
= n
X
i
B2
i âˆ’
X
i
Bi
2
= n
X
i
B2
i âˆ’n2I.

Noncommutative Arithmetic-Geometric Mean Conjecture is False
Therefore
âˆ’
X
iÌ¸=j
AiAjâˆ’(nâˆ’1)nI âª¯
X
i
B2
i âˆ’n2I =
X
i
(B2
i âˆ’nBi).
The eigenvalues of Bi fall between 0 and n, so the eigen-
values of B2
i âˆ’nBi are all nonpositive, i.e., B2
i âˆ’nBi âª¯0.
Hence âˆ’P
iÌ¸=j AiAj âˆ’(n âˆ’1)nI âª¯0.
The right inequality of (3) is clearly sharp. In Section 5, we
will prove a stronger result, improving the constant in the
left inequality of (3) to n(n âˆ’1)/4.
Following Zhang (2018), we write Ei1,...,ik for expectation
or average over all indices 1 â‰¤i1, . . . , ik â‰¤n, and eEi1,...,ik
for that over distinct indices 1 â‰¤i1, . . . , ik â‰¤n.
Theorem 2 (Rechtâ€“RÂ´e for m = 3). Let A1, . . . , An be
symmetric positive semideï¬nite and A1 + Â· Â· Â· + An âª¯nI.
Then
âˆ’I âª¯eEi,j,kAiAjAk âª¯I.
(4)
Proof. Let A, B, C be positive semideï¬nite. Then ABC +
CBA âª¯ABA + CBC. If B âª¯C, then ABA âª¯ACA.
We start with the right inequality of (4),
eEi,j,kAiAjAk = 1
2
eEi,j,k(AiAjAk + AkAjAi)
âª¯1
2
eEi,j,k(AiAjAi + AkAjAk)
= eEi,j,kAiAjAi.
Fix a positive integer l < n whose value we decide later.
eEi,j,kAiAjAk âª¯eEi,j,k
h
1 âˆ’1
l

AiAjAk + 1
l AiAjAi
i
=
1
l2(n âˆ’l)
eEi1,...,in

(Ai1 + Â· Â· Â· + Ail)
Â· (Ail+1 + Â· Â· Â· + Ain)(Ai1 + Â· Â· Â· + Ail)

.
Since Ail+1 + Â· Â· Â· + Ain âª¯nI âˆ’(Ai1 + Â· Â· Â· + Ail),
eEi,j,kAiAjAk âª¯
1
l2(n âˆ’l)
eEi1,...,il

(Ai1 + Â· Â· Â· + Ail)
Â·
 nI âˆ’(Ai1 + Â· Â· Â· + Ail)

(Ai1 + Â· Â· Â· + Ail)

.
Consider the function f(x) = x2(n âˆ’x). Let the line y =
cx + d be tangent to f at x = l. We require that c â‰¥0
and f(x) â‰¤cx + d for 0 â‰¤x â‰¤n. Elementary calculation
shows that such a line exists as long as 1/2 â‰¤l/n â‰¤2/3.
Let A = Ai1 + Â· Â· Â· + Ail. As cA + dI and A(nI âˆ’A)A
are simultaneous diagonalizable, each eigenvalue of cA +
dI âˆ’A(nI âˆ’A)A can be obtained by applying the function
g(x) = cx + d âˆ’x2(n âˆ’x) to an eigenvalue of A. Hence
eEi,j,kAiAjAk
âª¯
1
l2(n âˆ’l)
eEi1,...,il

c(Ai1 + Â· Â· Â· + Ail) + dI

âª¯
cl + d
l2(n âˆ’l)I,
where the ï¬rst inequality follows from the fact that it holds
for each eigenvalue. Note that if we choose A1 = Â· Â· Â· =
An = I, all inequalities above as well as the right inequal-
ity of (4) hold with equality. So as long as 1/2 â‰¤l/n â‰¤
2/3, l, c, d will give us
1
l2(n âˆ’l)(cl + d) = 1
and thus the right inequality of (4).
For the left inequality of (4), we start by noting
(A1 + Â· Â· Â· + Anâˆ’1)An(A1 + Â· Â· Â· + Anâˆ’1) âª°0.
Taking expectation, we have âˆ’(n âˆ’2)eEi,j,kAiAjAk âª¯
eEi,j,kAiAjAi and thus
âˆ’eEi,j,kAiAjAk
= âˆ’n âˆ’2
n âˆ’1
eEi,j,kAiAjAk âˆ’
1
n âˆ’1
eEi,j,kAiAjAk
âª¯
1
n âˆ’1
eEi,j,k(AiAjAi âˆ’AiAjAk)
âª¯
1
2(n âˆ’1)
eEi,j,k

(Ai âˆ’Aj)Ak(Ai âˆ’Aj)

.
As in the proof of Theorem 1, set B := nI âˆ’P
i Ai âª°0
and Bi := Ai + 1
nB âª°0. Then
âˆ’eEi,j,kAiAjAk
âª¯
1
2(n âˆ’1)
eEi,j,k

(Bi âˆ’Bj)Bk(Bi âˆ’Bj)

=
1
n âˆ’1
eEi,j,kBiBjBi âˆ’
1
n âˆ’1
eEi,j,kBiBjBk.
Let Xi := Bi(nIâˆ’Bi)Bi and Yi := (nIâˆ’Bi)Bi(nIâˆ’Bi).
Routine calculations give
eEiXi = (n âˆ’1)eEi,j,kBiBjBi,
eEiYi = (n âˆ’1)eEi,j,kBiBjBi
+ (n âˆ’1)(n âˆ’2)eEi,j,kBiBjBk,
which
allows
us
to
express
eEi,j,kBiBjBi
and
eEi,j,kBiBjBk in terms of eEiXi and eEiYi. Then
âˆ’eEi,j,kAiAjAk
âª¯
1
n âˆ’1
eEi,j,kBiBjBi âˆ’
1
n âˆ’1
eEi,j,kBiBjBk
=
1
(n âˆ’1)2(n âˆ’2)
eEi[(n âˆ’1)Xi âˆ’Yi]

Noncommutative Arithmetic-Geometric Mean Conjecture is False
=
n
(n âˆ’1)2(n âˆ’2)
eEi[âˆ’Bi(Bi âˆ’nI)(Bi âˆ’I)].
As âˆ’x(x âˆ’n)(x âˆ’1) â‰¤(n âˆ’1)2x/4 for 0 â‰¤x â‰¤n,
âˆ’eEi,j,kAiAjAk âª¯
n
4(n âˆ’2)
eEiBi =
n
4(n âˆ’2)I.
When n â‰¥3, we have n/
 4(n âˆ’2)

â‰¤1.
Our proof in fact shows that the constant in the left inequal-
ity of (4) can be improved to n/
 4(n âˆ’2)

. Nevertheless,
we will see in the next section (Table 1) that this is not
sharp.
3. Noncommutative Positivstellensatz
In a seminal paper (Helton, 2002), Helton proved an as-
tounding result: Every positive polynomial in noncommu-
tative variables can be written as a sum of squares of poly-
nomials. The corresponding statement for usual polynomi-
als, i.e., in commutative variables, is well-known to be false
and is the subject of Hilbertâ€™s 17th Problem. Subsequent
developments ultimately led to a noncommutative version
of the Positivstellensatz for semialgebraic sets. We refer
interested readers to Pascoe (2018) for an overview of this
topic.
Stating noncommutative Positivstellensatz will require that
we introduce some terminologies. Let X1, . . . , Xn be n
noncommutative variables, i.e., XiXj Ì¸= XjXi whenever
i Ì¸= j. A monomial of degree d or a word of length d is an
expression of the form Xi1 Â· Â· Â· Xid. The monomials span
a real inï¬nite-dimensional vector space RâŸ¨X1, . . . , XnâŸ©,
called the space of noncommutative polynomials.
For
any d âˆˆN, the ï¬nite-dimensional subspace of noncom-
mutative polynomials of degree â‰¤
d will be denoted
RâŸ¨X1, . . . , XnâŸ©d. The transpose of f âˆˆRâŸ¨X1, . . . , XnâŸ©
is denoted f T and is deï¬ned on monomials by reversing
the order of variables (Xi1 Â· Â· Â· Xid)T = Xid Â· Â· Â· Xi1 and ex-
tended linearly to all of RâŸ¨X1, . . . , XnâŸ©. If f T = f, then f
is called symmetric.
The bottom line is that noncommutative polynomials may
be evaluated on square matrices of the same dimensions,
i.e., they deï¬ne matrix-valued functions of matrix variables.
For our purpose, if A1, . . . , An are real symmetric matrices,
then f(A1, . . . , An) is also a matrix, but it may not be a
symmetric matrix unless f is a symmetric polynomial.
Let L = {â„“1, . . . , â„“k} âŠ†RâŸ¨X1, . . . , XnâŸ©1 be a set of k
linear polynomials, i.e., d = 1. We will refer to â„“1, . . . , â„“k
as linear constraints and
BL := {(A1, . . . , An) | â„“1(A1, . . . , An) âª°0, . . . ,
â„“k(A1, . . . , An) âª°0}
as the feasible set.
Note that elements of BL are n tu-
ples of symmetric matrices. We say that BL is bounded if
there exists r > 0 such that all (A1, . . . , An) âˆˆBL satisfy
âˆ¥A1âˆ¥â‰¤r, . . . , âˆ¥Anâˆ¥â‰¤r. Let d âˆˆN. We write
Î£d(L) :=
 k
X
i=1
pi
X
j=1
f
T
ijâ„“ifij

fij âˆˆRâŸ¨X1, . . . , XnâŸ©d,
k, p1, . . . , pk âˆˆN

for the set of noncommutative sum-of-squares generated
by L. The following theorem is a simpliï¬ed version of
the noncommutative Positivstellensatz, i.e., Theorem 1.1 in
Helton et al. (2012), that will be enough for our purpose.
Theorem 3 (Noncommutative Positivstellensatz). Let f be
a symmetric polynomial with deg(f) â‰¤2d + 1 and the
feasible set BL be bounded with nonempty interior. Then
f(A1, . . . , An) âª°0 for all (A1, . . . , An) âˆˆBL
if and only if f âˆˆÎ£d(L).
Readers familiar with the commutative Positivstellsatz
(Lasserre, 2015) would see that the noncommutative ver-
sion is, surprisingly, much simpler and neater.
To avoid notational clutter, we introduce the shorthand
X
jiÌ¸=jk
:=
X
1â‰¤j1,...,jmâ‰¤n,
j1,...,jm distinct
for sum over distinct indices. Applying Theorem 3 with
linear constraints X1 âª°0, . . . , Xn âª°0, X1 + Â· Â· Â· + Xn âª¯
nI, Conjecture 1A becomes the following.
Conjecture 1B (Sum-of-squares form). Let m â‰¤n âˆˆ
N and d = âŒŠm/2âŒ‹.
For the linear constraints â„“1 =
X1, . . . , â„“n = Xn, â„“n+1 = n âˆ’X1 âˆ’Â· Â· Â· âˆ’Xn, let
Î»1 = argmin

Î» âˆˆR
 Î» âˆ’
X
jiÌ¸=jk
Xj1 Â· Â· Â· Xjm âˆˆÎ£d(L)

,
Î»2 = argmin

Î» âˆˆR
 Î» +
X
jiÌ¸=jk
Xj1 Â· Â· Â· Xjm âˆˆÎ£d(L)

.
Then both Î»1 and Î»2 â‰¤n!/(n âˆ’m)!.
In polynomial optimization (Lasserre, 2015), the commuta-
tive Positivstellsatz is used to transform a constrained opti-
mization problem into a sum-of-squares problem that can
in turn be transformed into a semideï¬nite programming
(SDP) problem. Helton (2002) has observed that this also
applies to noncommutative polynomial optimization prob-
lems, i.e., we may further transform Conjecture 1B into an
SDP form.
The vector space RâŸ¨X1, . . . , XnâŸ©d has dimension q :=
1+n+n2 +Â· Â· Â·+nd and a basis comprising all q monomi-
als of degree â‰¤d. We will assemble all basis elements into

Noncommutative Arithmetic-Geometric Mean Conjecture is False
a q-tuple of monomials that we denote by Î². With respect
to this basis, any f âˆˆRâŸ¨X1, . . . , XnâŸ©d may be represented
uniquely as f = Î²Tu for some u âˆˆRq. Therefore a non-
commutative square may be expressed as
p
X
j=1
f
T
j â„“fj =
p
X
j=1
u
T
jÎ²â„“Î²
Tuj = tr

Î²â„“Î²
T
 p
X
j=1
uju
T
j

by simply writing fj = Î²Tuj, uj âˆˆRq, j = 1, . . . , p.
Since a symmetric matrix Y is positive semideï¬nite iff it
can be written as Y = Pp
j=1 ujuT
j, we obtain the follow-
ing one-to-one correspondence between noncommutative
squares and positive semideï¬nite matrices:
p
X
j=1
f
T
j â„“fj âˆˆRâŸ¨X1, . . . , XnâŸ©2d+1, fj âˆˆRâŸ¨X1, . . . , XnâŸ©d
~w
p
X
j=1
uju
T
j âˆˆRqÃ—q, uj âˆˆRq.
With this correspondence, the two minimization problems
in Conjecture 1B become two SDPs.
Conjecture 1C (Semideï¬nite program form). Let m â‰¤
n âˆˆN and d = âŒŠm/2âŒ‹. Let Î² be a monomial basis of
RâŸ¨X1, . . . , XnâŸ©d and let Xn+1 = n âˆ’X1 âˆ’Â· Â· Â· âˆ’Xn. Let
Î»1 be the minimum value of the SDP:
minimize
Î»
subject to
Î» âˆ’
X
jiÌ¸=jk
Xj1 Â· Â· Â· Xjm =
n+1
X
i=1
tr(Î²XiÎ²
TYi),
Y1 âª°0, . . . , Yn+1 âª°0;
(5)
and Î»2 be that of the SDP:
minimize
Î»
subject to
Î» +
X
jiÌ¸=jk
Xj1 Â· Â· Â· Xjm =
n+1
X
i=1
tr(Î²XiÎ²
TYi),
Y1 âª°0, . . . , Yn+1 âª°0.
(6)
Then both Î»1 and Î»2 â‰¤n!/(n âˆ’m)!.
Note that the minimization is over the scalar variable Î»
and the matrix variables Y1, . . . , Yn+1; the equality con-
straint equating two noncommutative polynomials is sim-
ply saying that the coefï¬cients on both sides are equal, i.e.,
for each monomial, we get a linear constraint involving
Î», Y1, . . . , Yn+1 â€” the Xiâ€™s play no role other than to serve
as placeholders for these linear constraints. We may ex-
press (5) and (6) as SDPs in standard form with a single
matrix variable Y := diag(Î», Y1, . . . , Yn+1), see (7) for
example.
Readers acquainted with (commutative) polynomial opti-
mization (Lasserre, 2015) would be familiar with the above
Î»1
Î»2
n!/(n âˆ’m)!
m = 2, n = 2
2.0000
0.5000
2
m = 2, n = 3
6.0000
1.5000
6
m = 2, n = 4
12.0000
3.0000
12
m = 2, n = 5
20.0000
5.0000
20
m = 3, n = 3
6.0000
3.4113
6
m = 3, n = 4
24.0000
8.5367
24
m = 3, n = 5
60.0000
17.3611
60
m = 4, n = 4
24.0000
22.4746
24
m = 4, n = 5
120.0000
80.2349
120
m = 5, n = 5
120.0000
144.6488
120
Table 1. Results from the SDPs in Conjecture 1C for m â‰¤n â‰¤5.
The bold entry for Î»2 shows that the Rechtâ€“RÂ´e conjecture is false
for m = n = 5 since 144.6488 > 120.
discussions. In fact, the only difference between the com-
mutative and noncommutative cases is that Pd
i=0 ni, the
size of a noncommutative monomial basis, is much larger
than
 d+n
d

, the size of a commutative monomial basis.
For any ï¬xed values of m and n, Conjecture 1C is in a form
that can be checked by standard SDP solvers. The dimen-
sion of the SDP grows exponentially with m, and without
access to signiï¬cant computing resources, only small val-
ues of m, n are within reach. Fortuitously, m = n = 5 al-
ready yields the required violation 144.6488 â‰°120, show-
ing that Conjecture 1C and thus Conjecture 1 is false in
general. We tabulate our results for m â‰¤n â‰¤5 in Table 1.
The fact that the SDP in (6) for m = n = 5 has a minimum
Î»2 > 144 > 120 = 5! shows that there are uncountably
many instances with A1 âª°0, A2 âª°0, A3 âª°0, A4 âª°0,
A5 âª°0, and A1 + A2 + A3 + A4 + A5 âª¯5I such that the
matrix
X
ÏƒâˆˆS5
AÏƒ(1)AÏƒ(2)AÏƒ(3)AÏƒ(4)AÏƒ(5)
has an eigenvalue that is less than âˆ’144 < âˆ’120 = âˆ’5!.
Here Sn is the symmetric group on n elements. We em-
phasize that neither (6) nor its dual would give us ï¬ve such
matrices explicitly, although the dual does provide another
way to verify our result, as we will see in Section 4.
Indeed, the beauty of the noncommutative Positivstellen-
satz approach is that it allows us to show that Conjecture 1
is false for m = n = 5 without actually having to produce
ï¬ve positive semideï¬nite matrices A1, . . . , A5 that violates
the inequality (1). It would be difï¬cult to ï¬nd A1, . . . , A5
explicitly as one does not even know the smallest dimen-
sions required for these matrices to give a counterexample
to (1). Our approach essentially circumvents the issue by
replacing them with noncommutativevariables X1, . . . , X5
â€” the reader may have observed that the dimensions of the
matrices A1, . . . , A5 did not make an appearance anywhere
in this article.

Noncommutative Arithmetic-Geometric Mean Conjecture is False
4. Veriï¬cation via Farkas
We take a closer look at the m = n = 5 case that provided
a refutation to the Rechtâ€“RÂ´e conjecture. In this case, the
basis Î² has 1 + 5 + 52 = 31 monomials; the SDP in (6)
has 1 + 5 + 52 + 53 + 54 + 55 = 3906 linear constraints,
312 Ã— 6 + 1 = 5767 variables, and takes the form:
minimize
tr(C0Y )
subject to
tr(CiY ) = bi,
i = 1, . . . , 3906,
Y = diag(Î», Y1, . . . , Y6) âª°0.
(7)
Here C0, C1, . . . , C3906 âˆˆS187
++ , b âˆˆR3096, Î» is a scalar
variable, and Y1, . . . , Y6 are 31-by-31 symmetric matrix
variables. To put (7) into standard form, the block diago-
nal structure of Y may be further encoded as linear con-
straints requiring that off-diagonal blocks be zero. The out-
put of our program gives a minimizer of the form Y âˆ—=
diag(Î»âˆ—, Y âˆ—
1 , . . . , Y âˆ—
6 ) âˆˆS187
++ with
Î»âˆ—= 144.6488,
Y âˆ—
1 , . . . , Y âˆ—
6 âˆˆS31
++.
(8)
The actual numerical entries of the matrices appearing in
(7) and (8) are omitted due to space constraints; but they
can be found in the output of our program (code in supple-
ment).
The values in (8) are of course approximate because of the
inherent errors in numerical computations. In our opinion,
the gap between the computed 144.6488 and the conjec-
tured 120 is large enough to override any concerns of a
mistaken conclusion resulting from numerical errors. Nev-
ertheless, to put to rest any lingering doubts, we will di-
rectly show that the conjectured value Î» = 120 is infeasible
by producing a Farkas certiï¬cate. Consider the feasibility
problem:
minimize
0
subject to
tr(CiY ) = bi,
i = 1, . . . , 3906,
tr(C0Y ) = 120,
Y âª°0,
(9)
with C0, C1, . . . , C3906 âˆˆS187
++ and b âˆˆR3096 as in (7).
Note that C0 = e1eT
1 is the matrix with one in the (1, 1)th
entry and zero everywhere else. So (9) is the feasibility
problem of the optimization problem (7) with the additional
linear constraint y11 = 120 and where we have disregarded
the block diagonal constraints2 on Y . The dual of (9) is
maximize
120y0 + bTy
subject to
y0C0 + y1C1 + Â· Â· Â· + y3906C3906 âª¯0.
Our program produces a Farkas certiï¬cate y âˆˆR3096 with
120y0 + bTy â‰ˆ47.3 > 0, implying that (9) is infeasible.
2If (9) is already infeasible, then adding these block diagonal
constraints just makes it even more infeasible.
While this is a consequence of Farkas Lemma for SDP
(Lasserre, 1995), all we need is the following trivial ver-
sion.
Lemma 1. Let m, n âˆˆN. Let C0, C1, . . . , Cm âˆˆSn and
b âˆˆRm+1. If there exists a y âˆˆRm+1 with
y0C0 + Â· Â· Â· + ymCm âª¯0,
b
Ty > 0,
then there does not exist a Y âˆˆSn with
tr(C0Y ) = b0, . . . , tr(CmY ) = bm,
Y âª°0.
Proof. If such a Y exists, then
0 â‰¥tr
 (y0C0+Â· Â· Â·+ymCm)Y

= y0b0+Â· Â· Â·+ymbm > 0,
a contradiction.
Hence a matrix of the form
Y = diag(120, Y1, . . . , Y6) âˆˆS187
is infeasible for (7), providing another refutation of Con-
jecture 1C and thus Conjecture 1. In particular, showing
that Î» = 120 is infeasible for (7) does not require any of
the values computed in (8). Of course, aside from being
the conjectured value of Î»2, there is nothing special about
Î» = 120 â€” for any Î» < 144.6488, we may similarly com-
pute a Farkas certiï¬cate y to show that such a value of Î» is
infeasible for (7).
We conclude with a few words on the computational costs
of the SDPs in this and the last section. Our resulting dense
linear system for m = n = 5 requires 3906 Ã— 5767 â‰ˆ22
million ï¬‚oating point storage. Using a personal computer
with an Intel Core i7-9700k processor and 16GB of RAM,
our SeDuMi (Sturm, 1999) program in Matlab takes 150
seconds. For m = n = 6, storage alone would have taken
26 billion ï¬‚oating numbers, beyond our modest computing
resources.
5. Improving the Rechtâ€“RÂ´e inequality
An unexpected beneï¬t of the noncommutative Positivstel-
lensatz approach is that it leads to better bounds for the
m = 2 and 3 cases that we know are true. Observe that the
values for Î»2 in Table 1 for m = 2 are exactly smaller than
the values for n!/(nâˆ’m)! by a factor of 1/4. This suggests
that the Rechtâ€“RÂ´e inequality (3) for m = 2 in Theorem 1
may be improved to
âˆ’1
4n(n âˆ’1)I âª¯
X
iÌ¸=j
AiAj âª¯n(n âˆ’1)I.
Table 1 only shows this for n = 2, 3, 4, 5 but in this section,
we will give a proof for arbitrary n â‰¥2. Although our

Noncommutative Arithmetic-Geometric Mean Conjecture is False
proof below does not depend on the SDP formulation in (6),
the correct coefï¬cients in (11) for arbitrary n would have
been impossible to guess without solving (6) for m = 2
and some small values of n.
So far we have not explored the symmetry evident in our
formulations of the Rechtâ€“RÂ´e inequality: In Conjecture 1A,
the matrix expression
Î»I Â±
X
jiÌ¸=jk
Aj1 Â· Â· Â· Ajm
and the constraints A1 âª°0, . . . , An âª°0, A1 + Â· Â· Â· + An âª¯
nI are clearly invariant under any permutation Ïƒ âˆˆSn. In
Conjecture 1C, the noncommutative sum-of-squares
Î» Â±
X
jiÌ¸=jk
Xj1 Â· Â· Â· Xjm =
n+1
X
i=1
tr(Î²XiÎ²
TYi),
(10)
where Xn+1 = n âˆ’X1 âˆ’Â· Â· Â· âˆ’Xn, is also invariant under
Sn and so we may average over all permutations to get a
symmetrized sum-of-squares. For commutative polynomi-
als, results from classical invariant theory are often used to
take advantage of symmetry (Gatermann & Parrilo, 2004).
We will see next that such symmetry may also be exploited
for noncommutative polynomials.
Consider the case m = 2, n = 3. The monomial basis
of RâŸ¨X1, X2, X3âŸ©1 is Î² = (1, X1, X2, X3).
The sym-
metry imposes linear constraints on the matrix variables
Y1, Y2, Y3, Y4 in (6), requiring them to take the following
forms:
Y1 =
ï£®
ï£¯ï£¯ï£°
a
b
c
c
b
d
e
e
c
e
f
g
c
e
g
f
ï£¹
ï£ºï£ºï£»,
Y2 =
ï£®
ï£¯ï£¯ï£°
a
c
b
c
c
f
e
g
b
e
d
e
c
g
e
f
ï£¹
ï£ºï£ºï£»,
Y3 =
ï£®
ï£¯ï£¯ï£°
a
c
c
b
c
f
g
e
c
g
f
e
b
e
e
d
ï£¹
ï£ºï£ºï£»,
Y4 =
ï£®
ï£¯ï£¯ï£°
x
y
y
y
y
z
w
w
y
w
z
w
y
w
w
z
ï£¹
ï£ºï£ºï£».
These symmetries allow us to drastically reduce the degree
of freedom in our SDP: For any m = 2, n â‰¥2, the matrices
Y1, . . . , Yn are always determined by precisely 11 variables
that we label a, b, c, d, e, f, g, x, y, z, w. We computed their
values explicitly for n = 2, 3, 4. For n = 2,
Y1 =
ï£®
ï£¯ï£°
5
4
âˆ’3
4
1
4
âˆ’3
4
1
2
0
1
4
0
1
2
ï£¹
ï£ºï£»,
Y3 =
ï£®
ï£¯ï£°
1
4
âˆ’1
4
âˆ’1
4
âˆ’1
4
1
2
0
âˆ’1
4
0
1
2
ï£¹
ï£ºï£»,
and Y2 can be determined from Y1. For n = 3,
Y1 =
ï£®
ï£¯ï£¯ï£¯ï£°
5
2
âˆ’1
0
0
âˆ’1
4
9
1
9
1
9
0
1
9
4
9
1
9
0
1
9
1
2
4
9
ï£¹
ï£ºï£ºï£ºï£», Y4 =
ï£®
ï£¯ï£¯ï£¯ï£°
1
2
âˆ’1
3
âˆ’1
3
âˆ’1
3
âˆ’1
3
4
9
1
9
1
9
âˆ’1
3
1
9
4
9
1
9
âˆ’1
3
1
9
1
2
4
9
ï£¹
ï£ºï£ºï£ºï£»,
and Y2, Y3 can be determined from Y1. For n = 4,
Y1 =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
15
4
âˆ’9
8
âˆ’1
8
âˆ’1
8
âˆ’1
8
âˆ’9
8
3
8
1
8
1
8
1
8
âˆ’1
8
1
8
3
8
1
8
1
8
âˆ’1
8
1
8
1
8
3
8
1
8
âˆ’1
8
1
8
1
8
1
8
3
8
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»
,
Y5 =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
3
4
âˆ’3
8
âˆ’3
8
âˆ’3
8
âˆ’3
8
âˆ’3
8
3
8
1
8
1
8
1
8
âˆ’3
8
1
8
3
8
1
8
1
8
âˆ’3
8
1
8
1
8
3
8
1
8
âˆ’3
8
1
8
1
8
1
8
3
8
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»
,
and Y2, Y3, Y4 can be determined from Y1. The rational
numbers above are all chosen by observing the ï¬‚oating
numbers output of the SDP (6).
The values of the matrices Yiâ€™s for n = 2, 3, 4 allow us to
guess that the variables a, b, c, d, e, f, g, x, y, z, w are:
a = 5(n âˆ’1)
4
,
b = âˆ’3(n âˆ’1)
2n
,
c = 3 âˆ’n
2n ,
d = f = z = 2(n âˆ’1)
n2
,
e = g = w = n âˆ’2
n2
,
x = n âˆ’1
4
,
y = âˆ’n âˆ’1
2n .
(11)
The proof of our next theorem will ascertain that these
choices are indeed correct â€” they yield the sum-of-squares
decomposition in (10) for m = 2.
Theorem 4 (Better Rechtâ€“RÂ´e for m = 2). Let A1, . . . , An
be positive semideï¬nite matrices. If A1 + Â· Â· Â· + An âª¯nI,
then
âˆ’1
4n(n âˆ’1)I âª¯
X
iÌ¸=j
AiAj âª¯n(n âˆ’1)I.
Proof. The upper bound has already been established in
Theorem 1. It remains to establish the lower bound. We
start from the following readily veriï¬able inequalities
1
2n(n âˆ’1)(Aj âˆ’Ak)Ai(Aj âˆ’Ak) âª°0,
(12)
5(n âˆ’1)
4

I âˆ’6
5nAi + 2(3 âˆ’n)
5n(n âˆ’1)
X
jÌ¸=i
Aj

(13)
Ai

I âˆ’6
5nAi + 2(3 âˆ’n)
5n(n âˆ’1)
X
jÌ¸=i
Aj

âª°0,
n âˆ’1
5n2

Ai + 2n âˆ’1
n âˆ’1
X
jÌ¸=i
Aj

(14)
Ai

Ai + 2n âˆ’1
n âˆ’1
X
jÌ¸=i
Aj

âª°0,

Noncommutative Arithmetic-Geometric Mean Conjecture is False
1
2n2 (Aj âˆ’Ak)

n âˆ’
X
i
Ai

(Aj âˆ’Ak) âª°0,
(15)
n âˆ’1
4

I âˆ’2
n
X
i
Ai

n âˆ’
X
i
Ai

(16)

I âˆ’2
n
X
i
Ai

âª°0.
Sum (12) over all distinct i, j, k; sum (13) over all i; sum
(14) over all i; sum (15) over all distinct j, k; add all results
to (16). The ï¬nal inequality is our required lower bound.
For n = m = 2, the new lower bound is sharp. Take
A1 =
"
3
2
0
0
0
#
,
A2 =
"
1
6
âˆš
2
3
âˆš
2
3
4
3
#
,
then âˆ¥A1+A2âˆ¥= 2 and the smallest eigenvalue of A1A2+
A2A1 is âˆ’1/2. We conjecture that this bound is sharp for
all m = 2, n â‰¥2.
The method in this section also extends to higher m. For
example, we may impose symmetry constraints for m =
n = 3 and see if the Y1, Y2, Y3, Y4 obtained have rational
values, and if so write down a sums-of-squares proof by
factoring the Yiâ€™s.
6. Conclusion and open problems
We conclude our article with a discussion of some open
problems and why we think the Rechtâ€“RÂ´e conjecture, while
false as it is currently stated, only needs to be reï¬ned.
An immediate open question is whether the conjecture is
true for m = 4: Table 1 shows that it holds for (m, n) =
(4, 4) and (4, 5); we suspect that it is true for all n â‰¥4.
As we pointed out after Conjecture 1A, the Rechtâ€“RÂ´e in-
equality as stated in (1) conceals an asymmetry â€” it actu-
ally contains two inequalities, as shown in (2). What we
have seen is that the lower bound is never attained in any of
the cases we have examined. For m = 2 and 3, the lower
bound is too large, and we improved it in Theorem 4 and
the proof of Theorem 2 respectively. For m = 5, the lower
bound is too small, which is why the Rechtâ€“RÂ´e inequality
is false. A natural follow-up question is then: â€œWhat is the
correct lower bound?â€ On the other hand, we conjecture
that the remaining half of the Rechtâ€“RÂ´e inequality, i.e., the
upper bound in (2), holds true for all m â‰¤n âˆˆN.
Recht & Re (2012) has another conjecture similar to Con-
jecture 1 but where the norms appear after the summation.
Conjecture 2 (Recht & Re 2012). Let A1, . . . , An be pos-
itive semideï¬nite matrices. Then
1
nm
X
1â‰¤j1,...,jmâ‰¤n
âˆ¥Aj1 Â· Â· Â· Ajmâˆ¥â‰¥
(n âˆ’m)!
n!
X
1â‰¤j1,...,jmâ‰¤n,
j1,...,jm distinct
âˆ¥Aj1 Â· Â· Â· Ajmâˆ¥.
This has been established for m = 2 and 3 for any unitary
invariant norm in Israel et al. (2016). It is not clear to us if
the noncommutative Positivstellensatz might perhaps also
shed light on this related conjecture.
Lastly, if our intention is to analyze the relative efï¬cacies
of with- and without-replacement sampling strategies in
randomized algorithms, then it is more pertinent to study
these inequalities for random matrices, i.e., we do not just
assume that the indices are random variables but also the en-
tries of the matrices. For example, if we want to analyze the
Kaczmarz algorithm, then we ought to take expectation not
only with respect to all permutations but also with respect
to how we generate the entries of the matrices. This would
provide a more realistic platform for comparing with- and
without-replacement sampling strategies.
References
Beck, A. and Tetruashvili, L. On the convergence of block
coordinate descent type methods. SIAM J. Optim., 23(4):
2037â€“2060, 2013.
Bhatia, R. and Holbrook, J. Noncommutative geometric
means. Math. Intelligencer, 28(1):32â€“39, 2006.
Bhatia, R. and Kittaneh, F.
On the singular values of a
product of operators. SIAM J. Matrix Anal. Appl., 11(2):
272â€“277, 1990.
Bhatia, R. and Kittaneh, F.
Notes on matrix arithmetic-
geometric mean inequalities. Linear Algebra Appl., 308
(1-3):203â€“211, 2000.
Bhatia, R. and Kittaneh, F.
The matrix arithmetic-
geometric mean inequality revisited.
Linear Algebra
Appl., 428(8-9):2177â€“2191, 2008.
Bottou, L. Curiously fast convergence of some stochastic
gradient descent algorithms. In Proceedings of the sym-
posium on learning and data science, Paris, 2009.
Bottou, L.
Large-scale machine learning with stochas-
tic gradient descent.
In Proceedings of COMP-
STATâ€™2010, pp. 177â€“186. Physica-Verlag/Springer, Hei-
delberg, 2010.
Fang, C., Lin, Z., and Zhang, T. Sharp analysis for non-
convex sgd escaping from saddle points.
In Beygelz-
imer, A. and Hsu, D. (eds.), Proceedings of the Thirty-
Second Conference on Learning Theory, volume 99 of

Noncommutative Arithmetic-Geometric Mean Conjecture is False
Proceedings of Machine Learning Research, pp. 1192â€“
1234, Phoenix, USA, 2019.
Gatermann, K. and Parrilo, P. A.
Symmetry groups,
semideï¬nite programs, and sums of squares.
J. Pure
Appl. Algebra, 192(1-3):95â€“128, 2004.
Hardy, G. H., Littlewood, J. E., and PÂ´olya, G. Inequalities.
Cambridge Mathematical Library. Cambridge University
Press, Cambridge, 1988. Reprint of the 1952 edition.
Helton, J. W. â€œPositiveâ€ noncommutative polynomials are
sums of squares.
Ann. of Math. (2), 156(2):675â€“694,
2002.
Helton, J. W., Klep, I., and McCullough, S. The convex
Positivstellensatz in a free algebra. Adv. Math., 231(1):
516â€“534, 2012.
Israel, A., Krahmer, F., and Ward, R.
An arithmetic-
geometric mean inequality for products of three matrices.
Linear Algebra Appl., 488:1â€“12, 2016.
Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan,
M. I. How to escape saddle points efï¬ciently. In Precup,
D. and Teh, Y. W. (eds.), Proceedings of the 34th Interna-
tional Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pp. 1724â€“
1732, International Convention Centre, Sydney, Aus-
tralia, 2017.
Johnson, R. and Zhang, T. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances
in neural information processing systems, pp. 315â€“323,
2013.
Lasserre, J. B. A new Farkas lemma for positive semidef-
inite matrices.
IEEE Trans. Automat. Control, 40(6):
1131â€“1133, 1995.
Lasserre, J. B. An introduction to polynomial and semi-
algebraic optimization, volume 52. Cambridge Univer-
sity Press, 2015.
Nagaraj, D., Jain, P., and Netrapalli, P.
SGD without
replacement: Sharper rates for general smooth convex
functions.
In International Conference on Machine
Learning, pp. 4703â€“4711, 2019.
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Ro-
bust stochastic approximation approach to stochastic pro-
gramming. SIAM J. Optim., 19(4):1574â€“1609, 2008.
Nesterov, Y. Efï¬ciency of coordinate descent methods on
huge-scale optimization problems. SIAM J. Optim., 22
(2):341â€“362, 2012.
Pascoe, J. E. PositivstellensÂ¨atze for noncommutative ratio-
nal expressions. Proc. Amer. Math. Soc., 146(3):933â€“
937, 2018.
Recht, B. and Re, C. Toward a noncommutative arithmetic-
geometric mean inequality: Conjectures, case-studies,
and consequences.
In Mannor, S., Srebro, N., and
Williamson, R. C. (eds.), Proceedings of the 25th Annual
Conference on Learning Theory, volume 23 of Proceed-
ings of Machine Learning Research, pp. 11.1â€“11.24, Ed-
inburgh, Scotland, 2012.
Shamir, O. Without-replacement sampling for stochastic
gradient methods.
In Advances in neural information
processing systems, pp. 46â€“54, 2016.
Strohmer, T. and Vershynin, R. A randomized Kaczmarz
algorithm with exponential convergence. J. Fourier Anal.
Appl., 15(2):262â€“278, 2009.
Sturm, J. F. Using SeDuMi 1.02, a MATLAB toolbox for
optimization over symmetric cones. Optimization meth-
ods and software, 11(1-4):625â€“653, 1999.
Wright, S. J. Coordinate descent algorithms. Math. Pro-
gram., 151(1, Ser. B):3â€“34, 2015.
Zhang, T. A note on the matrix arithmetic-geometric mean
inequality.
Electron. J. Linear Algebra, 34:283â€“287,
2018.

