From Loops to Oops: Fallback Behaviors of Language Models
Under Uncertainty
Maor Ivgi
Ori Yoran
Jonathan Berant
Mor Geva
Blavatnik School of Computer Science, Tel Aviv University
Abstract
Large language models (LLMs) often exhibit
undesirable behaviors, such as hallucinations
and sequence repetitions. We propose to view
these behaviors as fallbacks that models ex-
hibit under uncertainty, and investigate the con-
nection between them. We categorize fallback
behaviors — sequence repetitions, degenerate
text, and hallucinations — and extensively an-
alyze them in models from the same family
that differ by the amount of pretraining tokens,
parameter count, or the inclusion of instruction-
following training. Our experiments reveal a
clear and consistent ordering of fallback be-
haviors, across all these axes: the more ad-
vanced an LLM is (i.e., trained on more tokens,
has more parameters, or instruction-tuned), its
fallback behavior shifts from sequence repeti-
tions, to degenerate text, and then to halluci-
nations. Moreover, the same ordering is ob-
served throughout a single generation, even
for the best-performing models; as uncertainty
increases, models shift from generating hal-
lucinations to producing degenerate text and
then sequence repetitions. Lastly, we demon-
strate that while common decoding techniques,
such as random sampling, might alleviate some
unwanted behaviors like sequence repetitions,
they increase harder-to-detect hallucinations.
1
Introduction
While large language models (LLMs) have been
known to generate human-like language remark-
ably well (Radford et al., 2019; Brown et al., 2020;
Touvron et al., 2023; OpenAI et al., 2024), there
are growing concerns about their propensity for un-
desirable behaviors, such as degenerate1 or repet-
itive text (Holtzman et al., 2020), hallucinations
(Ji et al., 2022; Zhang et al., 2023), and verbatim
recollection of training samples when processing
out-of-distribution inputs (Nasr et al., 2023a). Pre-
vious work (Kim et al., 2024; Snyder et al., 2023)
1Degenerate text includes repetitive textual patterns and/or
rephrasing of previously generated text.
The following is a bio of Elsa Pataky:
Elsa Pataky is a Spanish actress who has appeared in a 
number of films and television shows. She is best known 
for her roles in the Fast and Furious franchise [...] Elsa 
Pataky was born on July 18, 1976, in Madrid, Spain. She is 
the daughter of actors José Luis Pataky and Cristina Pons. 
Her father is of Spanish and Hungarian descent, while her 
mother is of Spanish and Italian descent [...] Pataky began 
her acting career in 1998, with a role in the film The 
Almond Tree. She has since appeared in a number of films 
and television shows, including the Fast and Furious 
franchise. Pataky is a Spanish actress who has appeared 
in a number of films and television shows. She is best 
known for her roles in the Fast and Furious franchise [...]
correct
hallucination
degenerate
Figure 1: When language models face uncertainty, they
exhibit fallback behaviors, shifting from hallucinations
to degenerate text generation (repeating previous facts
in different phrasing) and finally verbatim repetitions.
has studied these phenomena, and suggested solu-
tions, but has done so for each in isolation, without
considering the interactions between them.
In this work, we propose that the behaviors il-
lustrated in Figure 1 can be viewed collectively as
fallback behaviors, which emerge when the model
faces uncertainty. We aim to categorize and analyze
the relationship between these behaviors across a
range of LLMs. To this end, we create controlled
yet natural settings to manage the level of uncer-
tainty, force the generation of factual information,
and evaluate the correctness of each prediction.
We then test three model families—Pythia (Bider-
man et al., 2023), Llama 2 (Touvron et al., 2023),
Llama 3 (AI, 2024), and OLMo (Groeneveld et al.,
2024)—considering various factors that could in-
fluence the emergence of different fallbacks: (a)
number of parameters, (b) number of pretraining
tokens, (c) instruction-following training, and (d)
decoding algorithms. We observe a clear ordering
in the appearances of different fallbacks, as demon-
strated in Figure 2, which persists across all the
1
arXiv:2407.06071v1  [cs.CL]  8 Jul 2024

70M
160M
410M
1B
1.4B
2.8B
6.9B
12B
0
5
10
15
20
25
Average answer counts
3.0
16.3
5.7
3.2
17.8
3.7
4.2
18.3
2.6
5.7
16.1
3.2
6.0
15.6
3.7
7.3
12.5
4.1
7.7
12.7
4.1
8.0
12.2
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 2: Larger Pythia models resort to more com-
plex fallback behaviors. Models with larger parame-
ters count produce more correct facts (green) and hallu-
cinations (orange) while less repeating facts (blue). The
green line indicates the number of ground truth answers.
above factors. Our findings points to repetitions
as the simplest fallback, followed by degenerate
text, and finally hallucinations as the most complex
behavior.
We present evidence that increasing the strength
of a model tends to shift its fallback behavior
to more complex forms, whereas weaker models
rely on simpler behaviors like repetitive text. We
demonstrate that the so-called strength of a model
can be the result of increasing the model’s parame-
ter count, additional pretraining, or the addition of
an instruction-following training phase. Moreover,
we show that as the generation length grows and
the model struggles to generate additional factually
correct responses or convincing hallucinations, it
shifts backwards in the fallback behaviors spectrum
from hallucinations to repetitions.
We further examine the effect of the decoding
scheme, and demonstrate that while random sam-
pling may alleviate degenerate text (Holtzman et al.,
2020), it increases the rate of hallucinations, which
are harder to detect and potentially more damag-
ing to the user. Despite evidence suggesting that
models may be partially aware of their knowledge
gaps (Kadavath et al., 2022), models are not easily
steered away from non-factual generations. Even
when provided with demonstrations on how to ab-
stain rather than hallucinate, models continue to
produce hallucinations.
Overall, our study shows that while increasing
models’ strength improves their accuracy (Kaplan
et al., 2020), the root causes of fallback behaviors
persist even in the largest and best-trained models.
Moreover, superficial patches such as decoding
schemes fail to solve the problem and can trade
easily-detected degenerate text for more nuanced
and potentially harmful hallucinations.
To summarize, our main contributions are: (a)
A controlled analysis of LLM behaviors under un-
certainty, showing that strengthening models in-
creases hallucinations over degenerate text and
repetitions, (b) demonstrating that during gener-
ation, models undergo a phase shift from cor-
rect answers to hallucinations and then repetitions,
(c) evidence that methods to reduce text degen-
eration, like sampling, increase errors and hal-
lucinations.
Our code and data is available at
https://github.com/Mivg/fallbacks.
2
Fallback Behaviors
Since their emergence as powerful generative tools
(Radford et al., 2019; Brown et al., 2020), there
have been numerous reports of unwanted behav-
iors of LLMs. Holtzman et al. (2020) showed
that greedy decoding methods can cause degen-
erate generations, such as incoherent text or repe-
titions. They propose the use of nucleus sampling
to mitigate this problem. In another study, Nasr
et al. (2023b) demonstrate that when given out-
of-distribution inputs, models might output their
training data verbatim. More recently, growing evi-
dence (Zhu et al., 2024; Zhang et al., 2023; Aich-
berger et al., 2024, inter alia) suggests that even the
best models often generate convincing yet factually
incorrect text, typically referred to as “hallucina-
tions” or “confabulations” (Ji et al., 2022; Bang
et al., 2023). Xiao and Wang (2021) investigate
model uncertainty, defining it as the probabilistic
uncertainty in predicting the next token, which is
influenced by the model’s training and knowledge,
and identify it as a cause of hallucinations.2
In this paper, we propose to view these seem-
ingly independent phenomena as fallback behav-
iors that models adopt when uncertain, and we
investigate the relationships between them. Fur-
thermore, we hypothesize that the strength of an
LLM influences its fallback behavior, and aim to
understand what factors determine how a model
would behave in cases of uncertainty.
Specifically, we focus on the following phe-
nomena: 1. Sequence repetition When models
face an inability to produce plausible continuations,
they tend to repeat previously generated sequences,
which are known to be plausible within the current
context. 2. Degenerate text As shown by Holtz-
2While the model may experience internal uncertainty
when predicting the next token, this might not manifest in
the final logits. Once a fallback behavior is selected, it may
promote the token, resulting in very high logit value.
2

man et al. (2020), models can generate degenerate
text that is not strictly repetitive but follows a con-
sistent pattern, such as enumerating or repeating
sentences with variations in subject entities or at-
tributes. 3. Hallucinations In cases that require
the recollection of facts which the model cannot
produce, it may fabricate coherent and seemingly
plausible yet factually incorrect or unfounded con-
tent. We note that creating convincingly plausible
incorrect facts is not an easy feat; even in humans,
this ability develops at a late stage of development
(Talwar et al., 2007; Evans and Lee, 2013).
In our analysis, we additionally looked for cases
of verbatim recollection of training samples (Nasr
et al., 2023a), but found no evidence of such fall-
back behavior in our setting (see further discussion
in Section 6.1).
3
Experimental Setting
To directly investigate the relation between fallback
behaviors and their contributing factors, our setup
exposes models to naturally-occurring controlled
cases of uncertainty while recalling facts.
Datasets
We use the following datasets:
1. TRIVIAFACTS (TRIV): We manually curate
95 high-quality domain-diverse questions with
a list of answers (list questions) that (a) con-
tains multiple elements, but no more than 15,
(b) requires only knowledge that appears very
frequently in the web, and (c) includes short el-
ements without multiple correct phrasings. Due
to its high-quality and highly controllable setup,
we base the bulk of our experiments on this
dataset, with minor modifications described in-
line as relevant to each experiment.
2. BIOGENERATION (BIO): To investigate fall-
back behaviors in unstructured natural text, we
follow Min et al. (2023a) and prompt the model
to create biographies of entities from five popu-
larity levels, analyzing the resulting facts. We
sample 25 entities from each popularity level.
3. QAMPARI (QAMP): We sample 100 questions
from the dataset introduced by Amouyal et al.
(2023), which contains naturally occurring list
questions with answers from Wikipedia.3
3While similar to our TRIVIAFACTS dataset, this dataset
is known to have non-exhaustive answer sets and different
possible phrasings for the same answers, which can result in
correct answers being flagged as hallucinations.
4. FAKEQAMPARI (FQAMP): We replace the sub-
jects from QAMPARI with made-up names, veri-
fying they do not exist, forcing uncertainty.
Table 1 provides example questions. For addi-
tional details on the datasets, see Appendix A.
Generating predictions
To remove behaviors
caused by different decoding schemes, we perform
our analyses with greedy decoding unless stated
otherwise (Section 5.1 investigates the effects of
temperature sampling). When requiring models
to provide answers to the TRIV data, we prompt
the model to produce a list of up to 25 answers
(see Table 1 for example), thus forcefully push-
ing the models to recall facts, even when there
are none. We ablate this behavior by removing
the pre-defined length of the answer list and in-
cluding specific demonstrations to complete the
list while abstaining instead of generating incorrect
facts (Section 5.2).
Evaluation metrics
Given a generated output,
we first parse it into a set of facts, and evaluate
each one independently as correct, repeated or hal-
lucinated. For the list questions datasets (TRIV,
QAMP and FQAMP), this parsing is mostly straight-
forward as the generations are structured as lists
and the ground truth is given as a set of answers.
For open-ended generation, we use FactScore (Min
et al., 2023a) to extract atomic facts and verify them
against the entities’ Wikipedia entries. As mod-
els frequently continue generating tokens after the
completion of the instruction prompt, we further
detect what the model did at that point. Namely,
we consider the following options: 1) generating
EOS token, 2) changing the topic, for example, by
creating a new list/biography which we refer to as
topic change or 3) continuing to predict tokens
indefinitely (until the token budget is exhausted)
within the same sentence/paragraph of the answer
which we note by bad format. For additional
information on the parsing process and example
generations and endings, see Appendix B.
Models
We perform our experiments on a variety
of model families, sizes, pretraining corpora sizes
and finetuning stages. We evaluate both the base
and chat-specific checkpoints of Llama 2 which
were finetuned on instruction and dialogue data, as
well as on the recently introduced Llama 3 models
(Touvron et al., 2023; AI, 2024). Secondly, we use
OLMo (Groeneveld et al., 2024), which comes in
multiple model sizes, includes intermediate check-
3

Dataset
Question
Example Answer
TRIV
The following 25 colors are in the Olympic rings\n 1.
Blue, Yellow, Black, Green, Red
BIO
The following is a bio of Harrison Ford:\n Harrison Ford
Harrison Ford (born July 13, 1942) is an American
actor. He has been a leading man in . . . ∗
QAMP
The following 25 manga were drawn by Ryoichi Ikegami:\n 1.
Heat, Mai, the Psychic Girl, Wounded Man, Sanc-
tuary, Crying Freeman, Strain♯
FQAMP
The following 25 manga were drawn by Haru Tanemura:\n 1.
N/A (Haru Tanemura is not a known manga creator)
Table 1: Example questions and answers for our main datasets.
∗Taken from Wikipedia at https://en.wikipedia.org/wiki/Harrison_Ford.
♯While not listed in Amouyal et al. (2023), Ryoichi Ikegami also wrote and drew Spider-Man: The Manga, according
to https://en.wikipedia.org/wiki/Ryoichi_Ikegami.
points throughout the pretraining phase and also
offers instruction-tuned variants. Finally, we make
use of the Pythia model family (Biderman et al.,
2023) which comes in 8 different scales, each with
154 intermediate pretraining checkpoints. We also
include Dolly models which are instruction-tuned
Pythia checkpoints (Conover et al., 2023). This
suite of models allows us to control for different
factors, such as pretraining data, model scale and
training procedure, examining the effect of each
factor on the emergence of fallback behaviors.
4
Fallback Behaviors of Different LLMs
Takeaway: Increasing model strength through addi-
tional pretraining, more parameters, or instruction-
tuning shifts from simple fallback behaviors to more
complex ones, i.e., from repetitions to hallucinations.
Scaling up models and training data improves
performance and reduces undesired artifacts (Ka-
plan et al., 2020; Brown et al., 2020). Incorpo-
rating instruction-following phases aligns LLMs’
outputs with human preferences (Ouyang et al.,
2022). In this section, we investigate how these
improvements influence model behavior under un-
certainty. We first focus on the trade-off between
sequence repetitions and hallucinations, measured
as discrete shifts in fallback behaviors. Due to the
various forms and broad definition of degenerate
text, measuring its appearance is not straightfor-
ward. We revisit this in depth in Section 6, demon-
strating that the fallback shift is a continuous rather
than discrete process.
4.1
Model scale dictates the fallback behavior
To minimize confounding factors and understand
the direct effect of model scale on fallback behav-
iors, we generate predictions with greedy decod-
ing over our TRIVIAFACTS data, and analyze the
results by model family and scale. Since Pythia
models were all trained on the same data in the
70M
160M 410M
1B
1.4B
2.8B
6.9B
12B
0
5
10
15
20
25
Average answer counts
2.7
15.3
6.8
2.4
18.6
3.9
2.6
20.8
4.6
19.3
3.6
20.7
4.7
18.6
4.8
18.5
6.1
17.8
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 3: Larger models hallucinate instead of ab-
staining. When completing a list of facts about fictitious
entities, larger Pythia models produce more hallucina-
tions (orange), while smaller models repeat facts (blue).
Models never abstain from providing incorrect facts.
same manner, they are the most comparable. Fig-
ure 2 shows that larger models recall more correct
answers on average (green bar) and have lower fail-
ure rates in understanding task formats (red bar), as
expected. However, a clear trend emerges: while
smaller models struggle to recall many facts and
resort to repeating the same ones (blue bar), as the
number of parameters increases, repetitions are re-
placed with hallucinations (orange bar). This trend
is consistent across the OLMo and Llama 2 model
families (Figure 17). We also tested naturally oc-
curring list questions from our QAMPARI subset,
confirming the same trends (Figure 18).
Finally, we test what happens when we push
uncertainty to the limit using our FAKEQAMPARI
dataset, which has no correct answers. One might
expect models to abstain, indicating they do not
know the entity. However, as Figure 3 shows, not
only do the models fail to abstain, but we also
observe that larger and more advanced models are
more likely to fabricate facts compared to their
smaller counterparts. Specifically, the proportion
of hallucinated answers more than doubles from
approximately 15% in models with fewer than one
billion parameters to over 30% in larger models,
while the proportion of repetition decreases.
4

2B
4B
8B
17B
34B
67B
134B
300B
0
5
10
15
20
25
Average answer counts
15.3
8.1
1.8
19.6
3.4
2.8
17.1
4.7
3.9
17.7
1.6
1.7
4.4
17.2
2.8
6.3
15.8
3.5
5.7
14.4
4.1
7.7
12.7
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 4: Models that train longer shift to com-
plex fallbacks. The more training tokens Pythia-6.9B
checkpoints see (in billions), the more hallucinations
they produce and fewer repetitions they generate. The
green line marks the number of ground truth answers.
4.2
Models shift fallbacks during pretraining
Most LLMs are trained with autoregressive lan-
guage modeling objective, maximizing the log
probability of the next token given some context.
Increasing the number of tokens seen during train-
ing lowers perplexity and improves language un-
derstanding (Kaplan et al., 2020). Using intermedi-
ate checkpoints from the OLMo and Pythia model
families, we study fallback behavior during pre-
training. Figure 4 depicts the fallbacks breakdown
of Pythia-6.9B across training, showing that ini-
tially, after seeing only 2-4 billion tokens, it mainly
repeats facts (blue). As training continues, it pro-
duces more correct answers (green) and more in-
correct unique facts, i.e., hallucinations (orange),
while repeating facts less. Similar trends were ob-
served for Pythia-12B checkpoints, as well as for
OLMo models (see Figures 19 and 20).
4.3
Instruction finetuning shifts behaviors
Recently, instruction finetuning (Ouyang et al.,
2022) has been adopted as a valuable method to im-
prove model performance and align its generation
with human preferences. Although one might as-
sume such training reduces hallucinations, OpenAI
et al. (2024) suggest it increases model miscali-
bration, resulting in hallucinations associated with
high logit values.
Repeating the experiments from Section 4.1 and
comparing models to their instruction-tuned coun-
terparts, we see a similar shift in fallback behavior;
instruction-tuned models generate fewer repeated
sequences and more hallucinations on average (Fig-
ures 21 and 22). For the OLMo and Llama 2 family
which had undergone more finetuning than Dolly
checkpoints, the results are much more pronounced,
with the hallucinations portion almost doubling be-
2.8B
6.9B
12B
0
10
20
30
40
Atomic facts
3.5
(8.3%)
23.4
(55.2%)
14.5
(34.1%)
3.3
(7.6%)
28.4
(64.6%)
11.2
(25.6%)
5.4
(11.3%)
33.8
(71.0%)
7.4
(15.6%)
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 5: Larger Pythia models hallucinate more
when generating biographies. When producing bi-
ographies of rare entities, larger models generate more
atomic facts, with an increasing rate of hallucinations.
tween scales. While pretrained LLMs are generally
unable to generate sequences shorter than what
they encountered during training, instruction-tuned
models are more likely to produce an EOS token,
preempting the generation early and resulting in
fewer facts. However, we also note that while in-
struction finetuning can improve alignment with
human preferences, it can sometimes cause fine-
tuned models to diverge more frequently, resulting
in some bad format generations.
4.4
Similar trends in open-ended generation
To mimic real-world user requests, we use the BIO-
GENERATION dataset, sampling completions from
a subset of the Pythia model scales with a tem-
perature of τ = 0.5.
We use FactScore (Min
et al., 2023b) to parse each generated biography
into atomic facts and let ChatGPT 3.5 (Ouyang
et al., 2022) verify them against Wikipedia entries.4
Without the limitation on number of facts to pro-
duce, we observe that the larger models also gener-
ate more facts. Interestingly, when averaging over
the very-rare entities (Figure 5) we see that even in
this natural scenario, when models are required to
elaborate on topics they know little about, they fall
back to the same behaviors, with shifts occurring
as predictably as in the controlled settings. Similar
trends emerge over the rest of the popularity levels,
though the more frequent an entity is, the more
likely the models are to be able to recall facts for
them and less uncertainty they face, thus making
the results less useful for our analysis (Figures 27
to 31). This aligns only partially with Min et al.
(2023b), who found stronger models generate more
atomic facts and struggle more with rare entities.
4While FactScore, which relies on ChatGPT 3.5, is not
fully robust and can miss some atomic facts or incorrectly
label them, we assume such errors occur at similar rates across
generations, resulting in reliable trend analysis.
5

0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
5
10
15
20
25
Average answer counts
4.1
8.0
12.2
4.0
8.3
12.0
4.1
8.5
11.7
4.0
9.0
11.3
4.0
9.4
10.6
4.0
9.8
10.4
4.0
10.8
9.4
3.9
12.4
7.4
3.6
14.1
5.6
3.2
16.2
3.3
2.7
17.2
2.0
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 6: Increasing the sampling temperature shifts
fallback behavior. An average of 5 completions from
Pythia-12B on TRIVIAFACTS for varying temperatures.
Green line marks the number of ground truth answers.
However, they observed that within a model family,
larger models are generally more precise (i.e., hallu-
cinate less). In contrast, Lin et al. (2022) found the
largest models are the least truthful, which aligns
with our findings.
5
Factors Influencing the Fallback
Behaviors of an LLM
Takeaway: While LLMs have some internal capabil-
ity to avoid hallucinations, this fallback behavior is
inherent to their generation scheme and is likely un-
avoidable with current decoding methods. Mitigating
degenerate text through random sampling can come at
the cost of more hallucinations.
In this section, we shift our focus from compar-
ing different models to understanding the factors
influencing the fallback behaviors of a frozen pre-
rtained model used for inference.
5.1
Effect of sampling methods
So far in our experiments, we mostly used greedy
decoding to obtain responses from the models.
However, in real-world applications, decoding of-
ten includes sampling from the model’s output dis-
tribution and including repetition/frequency penal-
ties (e.g., (Holtzman et al., 2020; Keskar et al.,
2019; Kumar et al., 2022)). We analyze the ef-
fect of temperature sampling on fallback behav-
iors by repeating our experiment on TRIVIAFACTS,
while generating five different sequences per model
for each input, with increasing levels of random-
ness (using larger values of temperature). Figure 6
shows the results for Pythia-12B, revealing a sur-
prising finding – while higher levels of randomness
mitigate some repetitiveness and help models to
break out of endless loops in the generation,5 it
5This is expected as even when copying previous tokens
with high probability such as 0.99, continuing the generation
long enough will inevitably lead to generating a token that is
not part of the loop, thus steering the text to a new direction.
also causes a shift towards hallucinations. More-
over, we observe that introducing randomness re-
duces the number of correct facts, which could be
attributed to the random skipping of correct facts
associated with low confidence in the model.
To assess whether our findings in Section 4.1
hold in realistic setups that involve random sam-
pling, we repeat them with τ = 0.5. This choice
of temperature sets a good balance between the
model’s performance on the task and the amount
of degenerate text it outputs (Figure 6). Figure 23
confirms that even with additional randomness, all
fallback behaviors emerge, with shifts occurring as
predictably as in the greedy-decoding case.
5.2
Models “volunteer” hallucinations
In many previous experiments, we deliberately
placed the model in high-uncertainty scenarios,
prompting it to produce additional facts when none
existed. To lift this restriction, we modify the base
prompt in the TRIVIAFACTS data by removing the
requirement for a specific number of answers (25).
For example, the modification to the example in
the first row of Table 1 would result in the prompt
“The following colors are in the Olympic
rings:”, allowing the model to stop generating
additional answers once it exhausted its knowledge.
However, the same trends continue to emerge (Fig-
ure 24), showcasing that even in natural situations
without synthetically induced uncertainty, LLMs
still fall back on the same behaviors and fail to use
more appropriate “exit strategies”, such as topic
changes or explicit abstaining.
5.3
Can fallbacks be prevented by prompting?
Kadavath et al. (2022) find encouraging evidence
that LLMs may be calibrated (i.e. their confidence
approximates the true probability of the output)
and able to assess what they don’t know, espe-
cially when they are larger. However, Kapoor et al.
(2024a) recently showed that this observation does
not hold for some popular open-source models, and
Yona et al. (2024) showed that LLMs struggle to
express their uncertainty in words. We aim to un-
derstand whether LLMs are internally aware of
their uncertainty, and if this awareness can reduce
unwanted behaviors.
We modify the prompts as follows:
For
instruction-tuned models, we add the following pre-
fix to each prompt
“Complete the following
list with facts you are sure of, and stop
when you cannot recall additional facts”.
6

Figure 7: Models shift fallback behavior during a sin-
gle completion. Order of fallbacks per generation for
a Pythia-12B model—each row represents a specific
prompt with the 25 produced facts. Green marks correct
answers, orange hallucinations and blue repeated facts.
Questions are sorted by the number of consecutive repe-
titions.
For pretrained models, we add a prefix with three
demonstrations for in-context learning, each con-
sists of an easy list question and its corresponding
answers followed by a topic-change to a new list
(Figure 25 gives the full prefix). While for Pythia
and OLMo we observe a minor increase in abstaining
behavior at the expense of hallucinations, the over-
all trend remains the same, with hallucinated facts
emerging abundantly (Figure 26). In comparison,
both Llama 2 and more pronouncedly Llama 3 man-
age to abstain by changing topics often, but the por-
tion of hallucinated facts still increases with model
size. We conclude that fallback behaviors are in-
herent to current pretrained LLMs and emerge “un-
intentionally” and unavoidably under uncertainty.
6
Fallback Behaviors in One Generation
Takeaway: As models generate longer texts, they shift
in their fallback behavior, first generating hallucina-
tions and eventually producing degenerate text.
While we established that both model strength
and the decoding method impact the fallback behav-
iors of a model, these parameters are decided ahead
of time. In this section, we focus on a single model
at a time and investigate the effect of generation
length on emergence of fallback behaviors.
6.1
Emergence of fallbacks during generation
We look at the facts generated by the model in
response to a given query as an ordered list of la-
bels (i.e. correct, hallucination, repetition). For
example, Figure 7 depicts in each row the 25 labels
of the facts produced by Pythia-12B for each of
the 95 samples in TRIVIAFACTS. Surprisingly, we
observe that almost without exception, the model
first generates all the correct facts it can (green),
then shifts to producing hallucinations (orange),
and finally shifts further to repeating previous facts
(blue). This is especially noteworthy as we see
that the fallback behavior is not determined by the
question, and the ordering between fallbacks fol-
lows the same ordering as when increasing model
strength. This trend holds regardless of the model,
the dataset, or the decoding method (see Figures 32
to 39 for further results).
To quantify this phenomenon, we denote a label
by 1 for repetition, 2 for hallucination, and 3 for
correct, and define
ShiftScore(f1, . . . , fn) :=
1
n −1
n−1
X
i=1
1fi+1≥fi
Where f1, . . . , fn is an ordered list of fact-labels
(fi ∈{1, 2, 3}), and 1 is the indicator function.
For each pair of dataset and model, we measure the
ShiftScore for each generation. We then run a
Mann-Whitney U-test (McKnight and Najab, 2010)
to compare the results to the expected ShiftScore
if the list of facts was produced in a random order
(more details in Appendix D.2). We find that for
all tested pairs, the p-value is < 10−8 (see Table 2
for full analysis), and the U-statistic is always pos-
itive. We thus conclude that the ordering is not
random, and follows the aforementioned hierarchy
of fallback behaviors.
When investigating the fallback behaviors in a
single generation, one may expect to observe occur-
rences of verbatim recollection of training samples,
as suggested by Nasr et al. (2023a). To address this,
we analyzed the generations from our experiments
both qualitatively and quantitatively, and found no
evidence of such behavior. We therefore hypothe-
size that verbatim generation of training samples
stems from other factors than uncertainty regarding
factual information, and is more prevalent in the
presence of out-of-distribution inputs. For further
details, see Appendix C.
6.2
Fallback shifts is a continuous process
When analyzing generations as a discrete list of
facts, we also find discrete shifts in fallback be-
haviors as the model’s uncertainty grows. While
examining strict repetition of facts is of particular
interest, one may also consider a softer measure of
degenerate text generation. For example, produc-
ing a list of URLs such as https://page.domain/xyz,
https://page.domain/xyq, https://page.domain/wyz
is not strictly a repetition nor necessarily a halluci-
nation, but it is clear that some form of conditional
7

0
100
200
300
400
500
Tokens
0.2
0.4
0.6
0.8
1.0
DiversityScore
Baseline
Pythia-2.8B
Pythia-6.9B
Pythia-12B
Figure 8: Model output gradually degenerates with
increased generation length. When generating biogra-
phies for rare entities, model output degenerate as com-
pletion length increases, compared to the human base-
line (Wikipedia entries for the same entities). Stronger
models’ output deteriorates more slowly.
repetitiveness is occurring. Similarly, producing
the same entities in random permutations is clearly
undesired behavior and is not strictly a repeated
sequence. For example, in Figure 12 we can see
that the model starts repeating previous facts such
as the shows Elsa Pataky appeared in, though with
a slightly different phrasing, before it begins repeat-
ing previous sequences verbatim.
To measure such phenomena, we introduce
a DiversityScore.
Given a sequence of pre-
dicted tokens x = t1, . . . , tn from the model’s
vocabulary V, we define DiversityScore(x) :=
1
n
P
v∈V 1v∈x, i.e. the number of unique tokens in
the sequence divided by the sequence length. When
n ≪|V| we expect DiversityScore ≈1 while a
repetitive sequence will have a DiversityScore
that diminishes rapidly towards 0.
When experimenting with naturally occurring
(human-generated) sequences, such as Wikipedia
paragraphs, for sequences up to 512 tokens, the
DiversityScore is almost always higher than 0.6
and often ranges around 0.7 −0.8 even for longer
sequences (see Figure 8). Specifically, we use the
subset of BIOGENERATION entities that are very
frequent according to Min et al. (2023b), as we
expect the least amount of uncertainty to appear
and the most full and diverse generations, as dis-
cussed in Section 4.4. We compare the Wikipedia
paragraphs for these entities to models’ outputs
when generating biographies for them. For all
models’ outputs, the DiversityScore diminishes
much faster, with weaker models crossing the 0.5
threshold at around 150 tokens (i.e., after only 150
tokens, every other token is repeating a previous
one). We include the same analysis on generations
from the stronger model Llama 3 8B (AI, 2024)
for comparison. Indeed, while it produces slightly
more diverse outputs, diversity diminishes towards
zero as the sequence length grows.
We conclude that the shift in fallback behaviors
towards the degenerate end does not occur in a
discrete phase shift, but is a continuous process
that exacerbates as the generation length grows.
7
Related Work
Repetitions and degenerate text
Holtzman et al.
(2020) first attributed the tendency of LMs to gener-
ate highly repetitive and degenerate text to greedy
sampling and suggested nucleus sampling as a pos-
sible mitigation. Follow-up work demonstrated
the role of uncertainty in generating degenerate
text and proposes various solutions using random,
controlled, or constrained generation techniques
Keskar et al. (2019); Kumar et al. (2022); Zhang
et al. (2022); Finlayson et al. (2023); Su et al.
(2022). In a parallel approach, Olsson et al. (2022)
focused on understanding the intrinsic mechanisms
that cause models to copy previous inputs.
Hallucinations
As random sampling techniques
became ubiquitous and LLMs grew in size and ca-
pability, the main focus shifted toward understand-
ing the causes and proposing solutions to the gener-
ation of non-factual text, commonly referred to as
hallucinations. Recent work has focused on under-
standing how and why hallucinations emerge dur-
ing generation (Rashkin et al., 2023; Zhang et al.,
2023; Adlakha et al., 2023; Kim et al., 2024), reduc-
ing hallucinations (e.g., by retrieval-augmentation
or prompting) (Roller et al., 2021; Shuster et al.,
2021; Dhuliawala et al., 2023), and detecting hallu-
cinations (Zhou et al., 2021; Liu et al., 2022; Hon-
ovich et al., 2022; Min et al., 2023b; Mishra et al.,
2024; Gottesman and Geva, 2024). For a survey on
hallucinations in natural language generation, see
Ji et al. (2022). In concurrent work, Denison et al.
(2024, inter alia) demonstrated how models gener-
alize during training and learn to shift from simple
dishonest strategies such as sycophancy to generat-
ing false facts with premeditation. Similarly, Band
et al. (2024) suggest that the problem of halluci-
nating rather than abstaining is an issue with the
calibration of the model, as was also hypothesized
by OpenAI et al. (2024).
8

Uncertainty in language modeling
The phe-
nomenon where LMs abstain or hallucinate when
uncertain is well-known and has been reported
in previous work (Xiao and Wang, 2021; Lin
et al., 2022; Snyder et al., 2023; Baan et al., 2023;
Kang et al., 2024). Recent studies have examined
whether the generation probability of LMs is cal-
ibrated (Kadavath et al., 2022) and whether they
can express their uncertainty in natural language
(Li et al., 2023; Yona et al., 2024). Unlike previous
research, our focus is on understanding the varying
factors impacting the different fallback behaviors
of LMs under uncertainty.
8
Conclusion
This work links the notorious unwanted behaviors
of LLMs, such as degenerate and repetitive text and
hallucinations, showing that they are all fallback
behaviors for models when faced with uncertainty.
We provide abundant evidence that these behaviors
emerge with a clear ordering between their appear-
ances, either when comparing similar models of
different strength, different decoding strategies and
even in a single generation. Our work suggests
that these fallback behaviors are inherent to current
LLMs and that the ubiquitous methods to allevi-
ate such issues may simply replace one fallback
by another. Moreover, increasing model strength
by training longer or adding more parameters en-
hances overall performance and shifts its fallbacks
towards more complex range. However, as genera-
tion length grows, even the strongest models will
resort to hallucinations and may eventually produce
degenerate text.
Limitations
In this work we study different fallback behaviors
of language models when faced with uncertainty.
While we conduct multiple experiments trying to
mimic real-world usage of such models as much
as possible, there are several confounders that may
still differentiate our controlled experiments than
behavior in the wild. First, many of the commodity
products are wrapped in additional levels of verifi-
cation layers to reduce the effect of such behaviors,
and it is possible that when observed as a whole,
the behaviors of these products could differ signifi-
cantly than their underlying language models.
Second, while we study the effect of random
sampling, it becomes more common to apply even
stricter modifiers to the language model next-word
prediction distribution by using methods such as nu-
cleus sampling, top-k decoding, repetition penalty
and more (Holtzman et al., 2020; Keskar et al.,
2019; Finlayson et al., 2023). We leave to future
work to examine such decoding strategies on fall-
back behaviors.
Third, we study the effect of instruction-
following finetuning on fallback behaviors, but it
remains possible that additional preference align-
ment may allow models to be instructed not to hal-
lucinate. While recent work (Kapoor et al., 2024b;
Yona et al., 2024) has showed these models are not
well calibrated and cannot tell whether they halluci-
nate or not, it is an open research and it is possible
that eventually it will be a viable solution.
Finally, this work focuses on producing factually
correct facts in natural language, and it remains
for future work to investigate similar phenomena
in producing faithful text to in-context informa-
tion (such as entities position and attributes with a
story), as well as when the task involves synthetic
language such as when producing code.
Acknowledgments
This research was partially supported by The Yan-
dex Initiative for Machine Learning,the Israeli
council of higher education, the Len Blavatnik and
the Blavatnik Family foundation, and the Euro-
pean Research Council (ERC) under the European
Union Horizons 2020 research and innovation pro-
gramme (grant ERC DELPHI 802800). This work
was completed in partial fulfillment for the Ph.D.
degree of the first author.
References
Vaibhav Adlakha, Parishad BehnamGhader, Xing Han
Lù, Nicholas Meade, and Siva Reddy. 2023. Eval-
uating correctness and faithfulness of instruction-
following models for question answering. Trans-
actions of the Association for Computational Linguis-
tics, 12:681–699.
Meta AI. 2024. Introducing meta llama 3: The most
capable openly available llm to date. https://ai.m
eta.com/blog/meta-llama-3/.
Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielan-
skyi, and Sepp Hochreiter. 2024. How many opinions
does your LLM have? improving uncertainty estima-
tion in NLG. In ICLR 2024 Workshop on Secure and
Trustworthy Large Language Models.
Samuel Amouyal, Tomer Wolfson, Ohad Rubin, Ori
Yoran, Jonathan Herzig, and Jonathan Berant. 2023.
QAMPARI: A benchmark for open-domain questions
9

with many answers. In Proceedings of the Third
Workshop on Natural Language Generation, Evalua-
tion, and Metrics (GEM), pages 97–110, Singapore.
Association for Computational Linguistics.
Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer,
Haau-Sing Li, R. Fernández, Barbara Plank, Rico
Sennrich, Chrysoula Zerva, and Wilker Aziz. 2023.
Uncertainty in natural language generation: From
theory to applications. arXiv:2307.15703.
Neil Band, Xuechen Li, Tengyu Ma, and Tatsunori
Hashimoto. 2024.
Linguistic calibration of long-
form generations. arXiv:2404.00474.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,
and Pascale Fung. 2023. A multitask, multilingual,
multimodal evaluation of ChatGPT on reasoning, hal-
lucination, and interactivity. In Proceedings of the
13th International Joint Conference on Natural Lan-
guage Processing and the 3rd Conference of the Asia-
Pacific Chapter of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 675–718,
Nusa Dua, Bali. Association for Computational Lin-
guistics.
Stella Biderman, Hailey Schoelkopf, Quentin G. An-
thony, Herbie Bradley, Kyle O’Brien, Eric Halla-
han, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, Aviya Skowron,
Lintang Sutawika, and Oskar van der Wal. 2023.
Pythia: A suite for analyzing large language mod-
els across training and scaling. arXiv:2304.01373.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual.
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,
Matei Zaharia, and Reynold Xin. 2023. Free dolly:
Introducing the world’s first truly open instruction-
tuned llm. https://www.databricks.com/blog/
2023/04/12/dolly-first-open-commerciall
y-viable-instruction-tuned-llm. Accessed:
2023-06-30.
Carson Denison, Monte MacDiarmid, Fazl Barez, David
Duvenaud, Shauna Kravec, Samuel Marks, Nicholas
Schiefer, Ryan Soklaski, Alex Tamkin, Jared Ka-
plan, Buck Shlegeris, Samuel R. Bowman, Ethan
Perez, and Evan Hubinger. 2024. Sycophancy to
subterfuge: Investigating reward-tampering in large
language models.
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,
Roberta Raileanu, Xian Li, Asli Celikyilmaz, and
Jason Weston. 2023. Chain-of-verification reduces
hallucination in large language models.
Angela D. Evans and Kang Lee. 2013. Emergence of
lying in very young children. Developmental psy-
chology, 49 10:1958–63.
Matthew Finlayson, John Hewitt, Alexander Koller,
Swabha Swayamdipta, and Ashish Sabharwal. 2023.
Closing the curious case of neural text degeneration.
arXiv:2310.01693.
Jonas Geiping, Alex Stein, Manli Shu, Khalid Saiful-
lah, Yuxin Wen, and Tom Goldstein. 2024.
Co-
ercing llms to do and reveal (almost) anything.
arXiv:2402.14020.
Daniela Gottesman and Mor Geva. 2024. Estimating
knowledge in large language models without gener-
ating a single token.
Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bha-
gia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish
Ivison, Ian Magnusson, Yizhong Wang, Shane Arora,
David Atkinson, Russell Authur, Khyathi Raghavi
Chandu, Arman Cohan, Jennifer Dumas, Yanai
Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William
Merrill, Jacob Daniel Morrison, Niklas Muennighoff,
Aakanksha Naik, Crystal Nam, Matthew E. Peters,
Valentina Pyatkin, Abhilasha Ravichander, Dustin
Schwenk, Saurabh Shah, Will Smith, Emma Strubell,
Nishant Subramani, Mitchell Wortsman, Pradeep
Dasigi, Nathan Lambert, Kyle Richardson, Luke
Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Sol-
daini, Noah A. Smith, and Hanna Hajishirzi. 2024.
Olmo: Accelerating the science of language models.
arXiv:2402.00838.
Adi Haviv, Ido Cohen, Jacob Gidron, Roei Schuster,
Yoav Goldberg, and Mor Geva. 2023. Understanding
transformer memorization recall through idioms.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text
degeneration. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net.
Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai
Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas
Scialom, Idan Szpektor, Avinatan Hassidim, and
Yossi Matias. 2022. TRUE: Re-evaluating factual
consistency evaluation. In Proceedings of the Second
DialDoc Workshop on Document-grounded Dialogue
and Conversational Question Answering, pages 161–
175, Dublin, Ireland. Association for Computational
Linguistics.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan
Su, Yan Xu, Etsuko Ishii, Yejin Bang, Delong Chen,
10

Wenliang Dai, Andrea Madotto, and Pascale Fung.
2022. Survey of hallucination in natural language
generation. ACM Computing Surveys, 55:1 – 38.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zachary Dodds, Nova Dassarma, Eli Tran-
Johnson, Scott Johnston, Sheer El-Showk, Andy
Jones, Nelson Elhage, Tristan Hume, Anna Chen,
Yuntao Bai, Sam Bowman, Stanislav Fort, Deep
Ganguli, Danny Hernandez, Josh Jacobson, John
Kernion, Shauna Kravec, Liane Lovitt, Kamal
Ndousse, Catherine Olsson, Sam Ringer, Dario
Amodei, Tom B. Brown, Jack Clark, Nicholas Joseph,
Benjamin Mann, Sam McCandlish, Christopher Olah,
and Jared Kaplan. 2022. Language models (mostly)
know what they know. arXiv:2207.05221.
Katie Kang, Eric Wallace, Claire Tomlin, Aviral Ku-
mar, and Sergey Levine. 2024. Unfamiliar finetuning
examples control how language models hallucinate.
arXiv:2403.05612.
Jared Kaplan, Sam McCandlish, Tom Henighan,
Tom B. Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeff Wu, and Dario Amodei.
2020.
Scaling laws for neural language models.
arXiv:2001.08361.
Sanyam Kapoor, Nate Gruver, Manley Roberts, Kather-
ine Collins, Arka Pal, Umang Bhatt, Adrian Weller,
Samuel Dooley, Micah Goldblum, and Andrew Gor-
don Wilson. 2024a.
Large language models
must be taught to know what they don’t know.
arXiv:2406.08391.
Sanyam Kapoor, Nate Gruver, Manley Roberts, Arka
Pal, Samuel Dooley, Micah Goldblum, and Andrew
Wilson. 2024b. Calibration-tuning: Teaching large
language models to know what they don’t know. In
Proceedings of the 1st Workshop on Uncertainty-
Aware NLP (UncertaiNLP 2024), pages 1–14, St
Julians, Malta. Association for Computational Lin-
guistics.
Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,
Caiming Xiong, and Richard Socher. 2019. Ctrl: A
conditional transformer language model for control-
lable generation. arXiv:1909.05858.
Sunnie S. Y. Kim, Q. Vera Liao, Mihaela Vorvoreanu,
Steph Ballard, and Jennifer Wortman Vaughan. 2024.
"i’m not sure, but...": Examining the impact of large
language models’ uncertainty expression on user re-
liance and trust. arXiv:2405.00623.
Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov.
2022. Constrained sampling from language mod-
els via langevin dynamics in embedding spaces.
arXiv:2205.12558.
Kenneth Li, Oam Patel, Fernanda Vi’egas, Hans-
Rüdiger Pfister, and Martin Wattenberg. 2023.
Inference-time intervention: Eliciting truthful an-
swers from a language model. arXiv:2306.03341.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
TruthfulQA: Measuring how models mimic human
falsehoods. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 3214–3252, Dublin,
Ireland. Association for Computational Linguistics.
Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin
Choi, and Hannaneh Hajishirzi. 2024. Infini-gram:
Scaling unbounded n-gram language models to a tril-
lion tokens. arXiv:2401.17377.
Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao,
Zhifang Sui, Weizhu Chen, and Bill Dolan. 2022.
A token-level reference-free hallucination detection
benchmark for free-form text generation. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 6723–6737, Dublin, Ireland. Association
for Computational Linguistics.
Patrick E McKnight and Julius Najab. 2010. Mann-
whitney u test. The Corsini encyclopedia of psychol-
ogy, pages 1–1.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen tau Yih, Pang Wei Koh, Mohit
Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.
2023a. Factscore: Fine-grained atomic evaluation
of factual precision in long form text generation.
arXiv:305.14251.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-
moyer, and Hannaneh Hajishirzi. 2023b. FActScore:
Fine-grained atomic evaluation of factual precision
in long form text generation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing, pages 12076–12100, Singa-
pore. Association for Computational Linguistics.
Abhika Mishra, Akari Asai, Vidhisha Balachandran,
Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and
Hannaneh Hajishirzi. 2024. Fine-grained halluci-
nation detection and editing for language models.
arXiv:2401.06855.
Milad Nasr,
Nicholas Carlini,
Jonathan Hayase,
Matthew Jagielski, A. Feder Cooper, Daphne Ip-
polito, Christopher A. Choquette-Choo, Eric Wallace,
Florian Tramèr, and Katherine Lee. 2023a. Scal-
able extraction of training data from (production)
language models. arXiv:2311.17035.
Milad Nasr,
Nicholas Carlini,
Jonathan Hayase,
Matthew Jagielski, A. Feder Cooper, Daphne Ip-
polito, Christopher A. Choquette-Choo, Eric Wallace,
Florian Tramèr, and Katherine Lee. 2023b. Scal-
able extraction of training data from (production)
language models. arXiv:2311.17035.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-
erly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,
11

Danny Hernandez, Scott Johnston, Andy Jones, Jack-
son Kernion, Liane Lovitt, Kamal Ndousse, Dario
Amodei, Tom Brown, Jack Clark, Jared Kaplan,
Sam McCandlish, and Chris Olah. 2022. In-context
learning and induction heads. Transformer Circuits
Thread.
Https://transformer-circuits.pub/2022/in-
context-learning-and-induction-heads/index.html.
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-
wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,
Christopher Berner, Lenny Bogdonoff, Oleg Boiko,
Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-
man, Tim Brooks, Miles Brundage, Kevin Button,
Trevor Cai, Rosie Campbell, Andrew Cann, Brittany
Carey, Chelsea Carlson, Rory Carmichael, Brooke
Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully
Chen, Ruby Chen, Jason Chen, Mark Chen, Ben
Chess, Chester Cho, Casey Chu, Hyung Won Chung,
Dave Cummings, Jeremiah Currier, Yunxing Dai,
Cory Decareaux, Thomas Degry, Noah Deutsch,
Damien Deville, Arka Dhar, David Dohan, Steve
Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
Simón Posada Fishman, Juston Forte, Isabella Ful-
ford, Leo Gao, Elie Georges, Christian Gibson, Vik
Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-
Lopes, Jonathan Gordon, Morgan Grafstein, Scott
Gray, Ryan Greene, Joshua Gross, Shixiang Shane
Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
Yuchen He, Mike Heaton, Johannes Heidecke, Chris
Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin
Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-
woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-
mali, Ingmar Kanitscheider, Nitish Shirish Keskar,
Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,
Christina Kim, Yongjik Kim, Jan Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine B. Thompson,
Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,
Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea Voss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason
Wei, CJ Weinmann, Akila Welihinda, Peter Welin-
der, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave
Willner, Clemens Winter, Samuel Wolrich, Han-
nah Wong, Lauren Workman, Sherwin Wu, Jeff
Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo,
Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan
Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao,
Tianhao Zheng, Juntang Zhuang, William Zhuk,
and Barret Zoph. 2024.
Gpt-4 technical report.
arXiv:2303.08774.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke E.
Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul Francis Christiano, Jan Leike, and Ryan J.
Lowe. 2022. Training language models to follow in-
structions with human feedback. arXiv:2203.02155.
USVSN Sai Prashanth, Alvin Deng, Kyle O’Brien,
Jyothir S V au2, Mohammad Aflah Khan, Jaydeep
Borkar, Christopher A. Choquette-Choo, Jacob Ray
Fuehne, Stella Biderman, Tracy Ke, Katherine Lee,
and Naomi Saphra. 2024. Recite, reconstruct, rec-
ollect: Memorization in lms as a multifaceted phe-
nomenon.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. Preprint.
https://d4mucfpksywv.cloudfront.net/bett
er-language-models/language_models_are_un
supervised_multitask_learners.pdf.
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm,
Lora Aroyo, Michael Collins, Dipanjan Das, Slav
Petrov, Gaurav Singh Tomar, Iulia Turc, and David
Reitter. 2023. Measuring attribution in natural lan-
guage generation models. Computational Linguistics,
49(4):777–840.
12

Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,
Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,
Eric Michael Smith, Y-Lan Boureau, and Jason We-
ston. 2021. Recipes for building an open-domain
chatbot. In Proceedings of the 16th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Main Volume, pages 300–325,
Online. Association for Computational Linguistics.
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,
and Jason Weston. 2021. Retrieval augmentation
reduces hallucination in conversation. In Findings
of the Association for Computational Linguistics:
EMNLP 2021, pages 3784–3803, Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Ben Snyder, Marius Moisescu, and Muhammad Bilal
Zafar. 2023. On early detection of hallucinations in
factual question answering. arXiv:2312.14183.
Niklas Stoehr, Mitchell Gordon, Chiyuan Zhang, and
Owen Lewis. 2024.
Localizing paragraph mem-
orization in language models.
arXiv preprint
arXiv:2403.19851.
Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama,
Lingpeng Kong, and Nigel Collier. 2022.
A
contrastive framework for neural text generation.
arXiv:2202.06417.
Victoria Talwar, Heidi M. Gordon, and Kang Lee. 2007.
Lying in the elementary school years: verbal decep-
tion and its relation to second-order belief understand-
ing. Developmental psychology, 43 3:804–10.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-
tian Cantón Ferrer, Moya Chen, Guillem Cucurull,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony S. Hartshorn, Saghar Hos-
seini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V.
Korenev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai
Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, R. Subramanian, Xia Tan, Binh Tang, Ross
Taylor, Adina Williams, Jian Xiang Kuan, Puxin
Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-
gela Fan, Melanie Kambadur, Sharan Narang, Aure-
lien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom. 2023. Llama 2: Open foundation
and fine-tuned chat models. arXiv:2307.09288.
Yijun Xiao and William Yang Wang. 2021. On hal-
lucination and predictive uncertainty in conditional
language generation. In Proceedings of the 16th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Main Volume, pages
2734–2744, Online. Association for Computational
Linguistics.
G. Yona, Roee Aharoni, and Mor Geva. 2024. Can large
language models faithfully express their intrinsic un-
certainty in words? arXiv:2405.16908.
Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou,
and Dawei Song. 2022. A survey of controllable
text generation using transformer-based pre-trained
language models. ACM Computing Surveys, 56:1 –
37.
Muru Zhang, Ofir Press, William Merrill, Alisa Liu,
and Noah A. Smith. 2023. How language model
hallucinations can snowball. arXiv:2305.13534.
Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,
Francisco Guzmán, Luke Zettlemoyer, and Marjan
Ghazvininejad. 2021. Detecting hallucinated content
in conditional neural sequence generation. In Find-
ings of the Association for Computational Linguis-
tics: ACL-IJCNLP 2021, pages 1393–1404, Online.
Association for Computational Linguistics.
Zhiying Zhu, Zhiqing Sun, and Yiming Yang. 2024.
Halueval-wild: Evaluating hallucinations of language
models in the wild. arXiv:2403.04307.
13

A
Datasets
As discussed in Section 3, we make use of multiple
datasets for our experiments. In this section, we
provide further details on the construction of each
of them. We release all the datasets used in our
experiments at https://github.com/Mivg/fall
backs.
A.1
TRIVIAFACTS
When creating this dataset, we set the following
desiderata:
1. Exhaustiveness: In order to label all correct
answers as such, we verify that our ground-
truth answer set is exhaustive.
2. Non-ambiguity: To avoid incorrectly labeling
a correct answer as a hallucination, we avoid
questions where the answers may be phrased
in many ways and focus on short answers with
a single common way to refer to them.
3. Easiness: To be sure models are able to re-
call correct facts, we choose only questions
where the answer list contains at least some
answers that any graduate student can pro-
duce, as a proxy to the knowledge contained
in language models that are trained on web
data and Wikipedia.
4. Diversity: To avoid biases in evaluations, we
set out to create a diverse set of questions
that relates to as many domains as possible,
spanning science, sports, culture, politics, ge-
ography, and more.
5. Uncertainty: To ensure questions induce un-
certainty, we restrict the size of the ground-
truth answer set to 10. As we ask models to
produce 25 answers, we thus ensure they will
become uncertain even when recalling all the
correct answers.
We collect a set of nearly 300 candidate ques-
tions through a mix of manual suggestions and in-
teractions with ChatGPT. We then manually anno-
tate each of the questions according to the desider-
ata above, verifying the correctness and complete-
ness of the answer set. After aggressive filtering,
we are left with 95 high-quality questions that meet
all of our requirements.
Table 1 shows the format of questions in TRIV-
IAFACTS. In contrast to the other list questions
70M
160M
410M
1B
1.4B
2.8B
6.9B
12B
0
5
10
15
20
25
Average answer counts
3.6
17.3
4.1
2.3
18.8
3.4
1.8
4.7
17.6
2.8
7.1
14.1
3.1
7.8
12.9
3.7
7.4
11.3
2.5
4.2
8.0
12.3
4.1
9.7
10.7
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 9: Larger Pythia models’ fallback behaviors
on TRIVIAFACTS when adding a colon at the end
of a questions. Models with larger parameters count
produce more correct facts (green) and hallucinations
(orange) while less repeating facts (blue). The green
line indicates the number of ground truth answers.
datasets (QAMPARI and FAKEQAMPARI), here the
prompts do not end with a “:”. To verify that this
has no significant effect on the results, we append a
colon to each question and repeat the experiments
from Section 4.1. As depicted in Figure 9, the same
trend repeat here, and the results are very similar
to those in Figure 2.
A.2
BIOGENERATION
Min et al. (2023b) introduce FactScore, which
uses an LLM (e.g., ChatGPT) to parse an unstruc-
tured passage into atomic facts and verify them
independently against a knowledge base. Min et al.
(2023b) use topics from Wikipedia and divide them
into five popularity levels based on their frequency
in the knowledge base, from very rare to very fre-
quent. They then require a model to generate an
open-ended biography for each topic (entity) and
use the respective Wikipedia page as the ground-
truth knowledge source to verify facts. We ran-
domly sample 25 topics from each of the five pop-
ularity levels and use them throughout our experi-
ments.
A.3
QAMPARI
We use the dataset as introduced by Amouyal et al.
(2023). We filter the questions to those where the
answer is a set of size ≥3 and sample 100 random
questions. Then, we rephrase each question to be in
the format depicted in Table 1. Where appropriate,
some of the questions follow a slightly different
template. For example, one of the questions is
“Larry Cohen wrote and directed the following 25
works of art:”.
14

A.4
FAKEQAMPARI
We take the questions from QAMPARI (see Ap-
pendix A.3) and employ ChatGPT to replace the
subject entity in each question with an alternative
entity name which sounds plausible but does not
exist (for example, choosing a Japanese name for
an anime creator). We manually verified the gener-
ated questions, while checking that all new subjects
feel plausible in the context and that they refer to
entities that do not exist.
B
Parsing Answers
In this section, we provide further implementation
details on the process used to parse the answers
from each generation. We make our code and the
model generations available in https://github
.com/Mivg/fallbacks.
B.1
List Questions
Extracting answers
In Table 1, we show exam-
ples of inputs given to the model for completion. To
steer models to complete the answers in a specified
format, we append to each question the suffix “\n
1.”. In almost all cases, the models indeed gener-
ate completions in the format of “\n 1. <answer
1>\n 2. <answer 2> . . . ”, which allows us
to easily extract the answers (Figure 10). In some
cases, the generations include no \n or extra new-
line characters, but those cases are easily dealt with
by a simple regex. The next most common case is
a given answer as a comma-separated list. We iden-
tify those cases when none of the above options
were detected and at least five commas appeared
in the first sentence of the generation (no newlines
or periods). Finally, if none of the above was de-
tected but newline characters are identified (one
or more) between short lines, we treat each new
non-empty line as an answer. If none of the cases
were identified, or a detected structured format is
violated without new line characters, we collect
the answers until that point and mark the difference
from the expected 25 answers as missing due to Bad
Format. All answers are normalized by removing
extra white spaces to evaluate for repetitions.
Identifying end of lists
Since pretrained lan-
guage models are often trained with sequences of
a predetermined length, they do not produce end-
of-sequence (EOS) tokens and continue to generate
tokens until their budget is exhausted. As such,
to avoid false-positive hallucination detection, we
have to be able to identify when the model stopped
producing answers for the given prompt and started
generating completions for other topics. The easi-
est to identify and most common case is when the
models generate the completion in the structured
format as described above. In such cases, we can
simply identify when the structure is violated (e.g.,
after the 10th answer the model stops enumerating
answers and changes the topic), or take the first 25
answers given. Another common pattern used by
many models is a Topic change by including the
prefix “The following ...” (which we use as the
prefix in our prompts), followed by a new list of
items. Finally, some models (especially instruction-
tuned ones) explicitly output an EOS token to mark
the end of the generation, in which case we mark
the missing answers to complete the expected 25
as missing due to EOS.
Evaluating correctness
To avoid labeling an-
swers as hallucinations when they are correct but
phrased differently, we perform a relaxed evalua-
tion for correct answers. Namely, each given an-
swer is normalized by removing redundant white
spaces, articles, and punctuation, removing any
parentheses, and making all characters lowercase.
When multiple synonyms for the correct name exist
(such as Southern Ocean, which is another name
for Antarctic Ocean), we attempt to include both
as possible correct answers. We then consider a
produced answer correct if it has an f1 score of at
least 0.55 with an answer in the ground-truth set.
We verify our methods manually by sampling
generations from multiple models and experiments
and find that the extraction, end-of-generation iden-
tification, and evaluation are correct in over 95%
of the cases.
B.2
Biographies Generations
As we rely on FactScore (Min et al., 2023b) to ex-
tract atomic facts and evaluate them, our task here
is mainly identifying topic changes to avoid false
detection of hallucinations. To do so, we look for
a prefix indicating a new biography is being gen-
erated, for special keywords such as References,
Discography, etc., if multiple newline characters
appear, or if a title-like line is encountered. We
also consider any new line that starts with the sub-
ject name or a pronoun as a continuation of the
biography.
After extracting only the part of the generation
we consider as the biography and indicating the
strategy to change the topic, we split the biography
15

The following 25 planets are in our solar system:
1.Mercury
2. Venus
3. Earth
4. Mars
5. Jupiter
6. Saturn
7. Uranus
8. Neptune
9. Pluto
10. Ceres
11. Pallas
12. Vesta
13. Juno
14. Vesta
15. Ceres
16. Pallas
17. Juno
18. Pallas
19. Vesta
20. Ceres
21. Pallas
22. Juno
23. Vesta
24. Ceres
25. Pallas
The following 25 planets are in our solar system
1. Mercury
2. Venus
3. Earth
4. Mars
5. Jupiter
6. Saturn
7. Uranus
8. Neptune
9. Pluto
10. Ceres
[... We omit the rest of the generation which continues to repeat
the previous content indefinitely]
,→
Figure 10: An example of a generation where the out-
put followed the structured format precisely. For the
extracted and labeled answer set, refer to Figure 11.
into sentences and delegate to FactScore (using
ChatGPT 3.5 (Brown et al., 2020; Ouyang et al.,
2022)) to extract atomic facts and validate them
against the entity’s Wikipedia page. An example
of a full generation is given in Figure 12 and the
list of extracted atomic facts and their labeling is
given in Figure 13.
C
How Models Change Topics
Throughout our experiments, we analyze the classi-
fication of atomic facts in models’ generations, up
until a point where they change the topic. Nasr et al.
(2023a); Geiping et al. (2024) demonstrate how ex-
tremely out-of-distribution inputs can cause models
to fall back into recalling training samples verba-
tim, and Haviv et al. (2023); Stoehr et al. (2024)
showed that generation of memorized sequences
has a distinct internal “profile” from generation of
non-memorized sequences. In contrast, Prashanth
et al. (2024, inter alia) investigate memorization
and find that it can be caused by different factors
such as duplicated training data, repeating com-
mon and predictable patterns, and the recollection
of sequences that are neither.
Mars (C)
Uranus (C)
Venus (C)
Saturn (C)
Jupiter (C)
Mercury (C)
Neptune (C)
Earth (C)
Pluto (H)
Ceres (H)
Pallas (H)
Vesta (H)
Juno (H)
Vesta (R)
Ceres (R)
Pallas (R)
Juno (R)
Pallas (R)
Vesta (R)
Ceres (R)
Pallas (R)
Juno (R)
Vesta (R)
Ceres (R)
Pallas (R)"
Figure 11: Figure 10 shows an example of a genera-
tion where the output followed the structured format
precisely. We display the extracted set of answers and
their labeling according to our evaluation. An example
breakdown of a generated biography into atomic facts
and their labels is provided. Correct facts are noted by
(C), hallucinated ones by (H), and repeated ones as (R).
Inspired by this line of work, we analyzed how
models change topics, and in particular, if memo-
rized sequences emerge in these cases. We manu-
ally inspect such generations from multiple models,
for both our TRIVIAFACTS and BIOGENERATION
datasets, using both greedy and temperature sam-
pling. Using Infini-gram (Liu et al., 2024), we
iterate over sliding windows of lengths 8 and 16
tokens to look for possible occurrences of verbatim
recollection, which we manually inspect. We found
no cases where a sequence longer than a few tokens
repeats verbatim, and even then it comprises of
common patterns, such as “public static void
main”.
Upon manual inspection, we have found that
in almost all cases, the “topic change” included a
repetition of the exact same content as before, ad-
ditional data in some template format (e.g. listing
references for BIOGENERATION samples), or an-
other generation in the same format as the question
(e.g. another list about another topic for TRIV-
IAFACTS and another biography for a different
entity for BIOGENERATION). The only notable
behavior observed was models attempting to pro-
vide some code snippets to support the answer-list
generated, as seen in Figure 14.
16

The following is a bio of Elsa Pataky:
Elsa Pataky is a Spanish actress who has appeared in a number of films and television shows. She is best known 
for her roles in the Fast and Furious franchise, as well as in the films Snakes on a Plane and Giallo. Pataky has 
also appeared in several Spanish-language films, including the 2008 film The Orphanage. In 2017, she starred in 
the Netflix series The OA. Pataky is married to actor Chris Hemsworth, with whom she has three children.
Elsa Pataky was born on July 18, 1976, in Madrid, Spain. She is the daughter of actors José Luis Pataky and 
Cristina Pons. Her father is of Spanish and Hungarian descent, while her mother is of Spanish and Italian descent. 
Pataky has two older brothers, Javier and Ignacio.
Pataky began her acting career in 1998, with a role in the film The Almond Tree. She has since appeared in a 
number of films and television shows, including the Fast and Furious franchise, Snakes on a Plane, Giallo, The 
Orphanage, and The OA. Pataky is married to actor Chris Hemsworth, with whom she has three children.
Pataky is a Spanish actress who has appeared in a number of films and television shows. She is best known for 
her roles in the Fast and Furious franchise, as well as in the films Snakes on a Plane and Giallo. Pataky has also 
appeared in several Spanish-language films, including the 2008 film The Orphanage. In 2017, she starred in the 
Netflix series The OA. Pataky is married to actor Chris Hemsworth, with whom she has three children.
Elsa Pataky is a Spanish actress who has appeared in a number of films and television shows. She is best known 
for her roles in the Fast and Furious franchise, as well as in the films Snakes on a Plane and Giallo. Pataky has 
also appeared in several Spanish-language films, including the 2008 film The Orphanage. In 2017, she starred in 
the Netflix series The OA. Pataky is married to actor Chris Hemsworth, with whom she has three children.
correct
hallucination
degenerate
Figure 12: An example generated biography by Llama 3 8B, illustrating the different fallback behaviors of LLMs,
gradually shifting from factually correct claims, through hallucinations and degenerate text, to sequence repetitions.
The breakdown into atomic facts and their labels can be found in Figure 13
Overall, we did not observe any evidence of ver-
batim generation of training samples in the models’
generations. We therefore hypothesize that this be-
havior stems from other factors than uncertainty
regarding factual information, and is more preva-
lent in the presence of out-of-distribution inputs.
D
Additional Results
In this section, we provide additional results to sup-
port the results discussed in Sections 4 to 6. As
discussed in Section 3, we evaluate three model
families over multiple datasets and in various set-
tings. For the sake of brevity, we include here
a representative set of results and share the code
and results to produce all plots for the rest of the
experiments.
Figures 17 and 18 depict the scaling behavior of
larger models on the TRIVIAFACTS and QAMPARI
datasets respectively. Figures 19 and 20 shows
fallback trends during pretraining for Pythia-6.9B
and 12B models in the former and OLMo-1B and 7B
models in the latter. Figures 21 and 22 gives similar
results when adding instruction finetuning to mod-
els on the TRIVIAFACTS and QAMPARI datasets.
In Figure 25 we ablate the prompt for instruction
finetuned models to nudge them into abstaining
rather than producing hallucinations, and Figure 26
uses in-context examples to do the same for pre-
trained models. Finally, Figures 27 to 31 bring a
breakdown of the results on the BIOGENERATION
datasets with different levels of popularity.
D.1
Exclusion of Llama 2 13b Checkpoint
We find that Llama 2 13b as released in meta-
llama/Llama-2-13b-hf almost always produces ex-
tremely poor results. Upon manual inspection of
its outputs, we conclude that there is likely a bug
in the uploaded weights of this specific model, and
thus we exclude it. We provide a few examples
in Figure 15. We note that for the other check-
points, as well as the chat variants of all three sizes,
the generation are considerably of higher quality,
further supporting this decision.
D.2
Order of Facts in a Single Generation
Section 6.1 introduces the ShiftScore, which
measures how predictable the order of facts is with
respect to the hierarchy between fallback behaviors
as introduced in this work. To perform the Mann-
Whitney U-test, we consider only answer sets with
17

Model
QAMPARI
TRIV
TRIV (τ = 0.5)
Pythia-70M
1.7 × 10−12
2.2 × 10−13
7.8 × 10−11
Pythia-160M
1.4 × 10−18
2.0 × 10−12
2.3 × 10−15
Pythia-410M
4.8 × 10−22
3.2 × 10−20
1.6 × 10−12
Pythia-1B
1.3 × 10−18
7.8 × 10−23
1.5 × 10−16
Pythia-1.4B
1.5 × 10−17
1.0 × 10−25
1.4 × 10−17
Pythia-2.8B
8.4 × 10−16
2.1 × 10−22
1.7 × 10−19
Pythia-6.9B
3.3 × 10−14
3.8 × 10−28
1.1 × 10−17
Pythia-12B
8.0 × 10−19
9.8 × 10−28
1.1 × 10−22
OLMo-1B
4.9 × 10−15
8.3 × 10−22
2.4 × 10−21
OLMo-7B
7.4 × 10−20
3.8 × 10−21
9.2 × 10−24
Llama2-7B
8.7 × 10−11
9.2 × 10−26
6.2 × 10−18
Llama2-70B
5.1 × 10−09
4.3 × 10−18
1.4 × 10−12
Llama3-8B
2.2 × 10−13
2.1 × 10−11
1.4 × 10−12
Llama3-70B
8.2 × 10−10
3.6 × 10−11
7.8 × 10−11
Table 2:
The results (p-values) of running Mann-
Whitney U-test on the ordering of facts with respect
to the predicted hierarchy introduced in this paper, for
each model-dataset pair. In all cases, the p-value is less
than 10−8 while the U-statistic is strictly positive sug-
gesting the order of facts is far from being random.
at least five unique answers and model-dataset pairs
with at least 30 such answer sets. For each such
set, we compute the expected ShiftScore of a
random ordering of the answers by taking 1000
random permutations of their order and averaging
their ShiftScore.6 We then perform the statisti-
cal test on the list of ShiftScore values from the
original ordering of the model against the scores
of the random orders. Table 2 shows the p-value
of the two lists of scores coming from the same
distribution. In all cases, the U-statistic was strictly
positive, allowing us to conclude that the original
ordering follows the expected order in a statistically
significant way.
Additionally, in Section 6.1 we discuss the or-
dering of fact-labels within a single generation, as
presented in Figure 7 for Pythia-12B model on
TRIVIAFACTS. Figures 32 to 41 give similar plots
for other models.
6For TRIVIAFACTS with temperature sampling, we use
the ordering of the first of the five generations sampled for
each question.
Elsa Pataky is a Spanish actress. (C)
Elsa Pataky has appeared in a number of films. (C)
Elsa Pataky has appeared in a number of television shows. (C)
She is best known for her roles in the Fast and Furious franchise.
(C)
,→
She is best known for her roles in the films Snakes on a Plane. (C)
She is best known for her roles in the film Giallo. (C)
Pataky has appeared in several Spanish-language films. (C)
The Orphanage is a film. (C)
The Orphanage was released in 2008. (H)
In 2017, she starred in The OA. (H)
The OA is a Netflix series. (H)
Pataky is married to Chris Hemsworth. (C)
Pataky has three children. (C)
Pataky and Chris Hemsworth have three children together. (C)
Elsa Pataky was born on July 18, 1976. (C)
Elsa Pataky was born in Madrid. (C)
Elsa Pataky was born in Spain. (C)
She is the daughter of actors. (H)
Her father is José Luis Pataky. (H)
Her mother is Cristina Pons. (H)
Her father is of Spanish descent. (C)
Her father is of Hungarian descent. (H)
Her mother is of Spanish descent. (H)
Her mother is of Italian descent. (H)
Pataky has two older brothers. (H)
Pataky's older brothers are named Javier and Ignacio. (H)
Pataky began her acting career in 1998. (H)
Pataky's first role was in The Almond Tree. (H)
The Almond Tree is a film. (H)
She has appeared in a number of films. (C)
She has appeared in a number of television shows. (C)
The Fast and Furious franchise is a film. (C)
She has appeared in the Fast and Furious franchise. (C)
Snakes on a Plane is a film. (C)
She has appeared in Snakes on a Plane. (C)
Giallo is a film. (C)
She has appeared in Giallo. (C)
The Orphanage is a film. (H)
She has appeared in The Orphanage. (H)
The OA is a television show. (H)
She has appeared in The OA. (H)
Pataky is married to Chris Hemsworth. (R)
Pataky has three children. (R)
Pataky and Chris Hemsworth have three children together. (R)
Pataky is a Spanish actress. (C)
Pataky has appeared in films. (C)
Pataky has appeared in television shows. (C)
She is best known for her roles in the Fast and Furious franchise.
(R)
,→
She is best known for her roles in the films Snakes on a Plane. (R)
She is best known for her roles in the film Giallo. (R)
Pataky has appeared in several Spanish-language films. (R)
The Orphanage is a film. (R)
The Orphanage was released in 2008. (R)
In 2017, she starred in The OA. (R)
The OA is a Netflix series. (R)
Pataky is married to Chris Hemsworth. (R)
Pataky has three children. (R)
Pataky and Chris Hemsworth have three children together. (R)
Elsa Pataky is a Spanish actress. (R)
Elsa Pataky has appeared in a number of films. (R)
Elsa Pataky has appeared in a number of television shows. (R)
She is best known for her roles in the Fast and Furious franchise.
(R)
,→
She is best known for her roles in the films Snakes on a Plane. (R)
She is best known for her roles in the film Giallo. (R)
Pataky has appeared in several Spanish-language films. (R)
The Orphanage is a film. (R)
The Orphanage was released in 2008. (R)
In 2017, she starred in The OA. (R)
The OA is a Netflix series. (R)
Pataky is married to Chris Hemsworth. (R)
Pataky has three children. (R)
Pataky and Chris Hemsworth have three children together. (R)
(EOS)
Figure 13: An example breakdown of a generated biog-
raphy into atomic facts and their labels. Correct facts
are noted by (C), hallucinated ones by (H), and repeated
ones as (R). (EOS) marks that the generation was pre-
empted by the model, with no additional unrelated con-
tent generated. For the original generation as produced
by the model, see Figure 12.
18

<OLMo 1B>
The following 25 are halogens in the periodic table\n1. Cesium\n2.
Neon\n3. Fluorine\n4. Bromine\n5. Iodine\n6. Sulfur\n7.
Phosphorus\n8. Chlorine\n9. Arsenic\n10. Mercury\n11.
Cadmium\n12. Lead\n13. Tin\n14. Antimony\n15. Bismuth\n16.
Tellurium\n17. Germanium\n18. Gallium\n19. Germanium\n20.
Silicon\n21. Gallium\n22. Germanium\n23. Gallium\n24.
Germanium\n25. Gallium\n\n## How to run\n\n```\n$ git clone
https://github.com/julian-k/periodic-table.git\n$ cd
periodic-table\n$ python3 main.py\n```\n\n## License\n\nThis
project is licensed under the MIT License - see the
[LICENSE.md](LICENSE.md) file for details\n\n##
Contributing\n\nPlease see
[CONTRIBUTING.md](CONTRIBUTING.md)\n\n##
Acknowledgments\n\nThis project is based on the
[periodic-table](https://github.com/julian-k/periodic-table)
project.
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
<Pythia 1B>
The following 25 are the cardinal directions on the compass rose\n1.
North\n2. East\n3. South\n4. West\n5. Up\n6. Down\n7. Left\n8.
Right\n9. Up\n10. Down\n11. Left\n12. Right\n13. Up\n14.
Down\n15. Left\n16. Right\n17. Up\n18. Down\n19. Left\n20.
Right\n21. Up\n22. Down\n23. Left\n24. Right\n25. Up\n\nA:\n\nI
think you can use the following formula:\n\nNorth = (East +
South) / 2\nEast = (West + South) / 2\nSouth = (North + West) /
2\n\nThe formula is:\n\nNorth = (East + South) / 2\nEast =
(West + South) / 2\nSouth = (North + West) / 2\n\nThe formula
is:\n\nNorth = (East + South) / 2...
,→
,→
,→
,→
,→
,→
,→
,→
,→
<Pythia 6.9B>
The following 25 are phases of human sleep\n1. REM sleep\n2. NREM
sleep\n3. Wakefulness\n4. Light sleep\n5. Deep sleep\n6. REM
sleep\n7. NREM sleep\n8. Wakefulness\n9. Light sleep\n10. Deep
sleep\n11. REM sleep\n12. NREM sleep\n13. Wakefulness\n14.
Light sleep\n15. Deep sleep\n16. REM sleep\n17. NREM sleep\n18.
Wakefulness\n19. Light sleep\n20. Deep sleep\n21. REM sleep\n22.
NREM sleep\n23. Wakefulness\n24. Light sleep\n25. Deep
sleep\n\nA:\n\nI think you are looking for this:\nimport
java.util.Arrays;\nimport java.util.List;\n\npublic class Main
{\n
public static void main(String[] args) {\n
List<String> list = Arrays.asList(\"1\", \"2\", \"3\", \"4\",
\"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",
\"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\",
\"21\", \"22\", \"23\", \"24\", \"25\");\n
System.out.println(list);\n
}\n}\n\nOutput:\n[1, 2, 3, 4, 5,
6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
23, 24, 25]\n\nA:\n\nYou can use a List<String> and use the
index to get the value.\nList<String> list = new
ArrayList<String>();\nlist.add(\"1\");\nlist.add(\"2\");
\nlist.add(\"3\");\nlist.add(\"4\");...
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
,→
}
Figure 14: A few examples of generation with greedy
decoding. < · > indicate the generating model, and \n
indicate line breaks in the generation. We use “. . . ”
when the sequence becomes repetitive.
The following 25 fingers are part of the human hand:
1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. <this continues
indefinitely>
,→
The following 25 are signs of the Zodiac:
1. *The* *Sword* *of* *the* *Lictor*
2.
*The* *Sword* *of* *the* *Lictor*
3.
*The* *Sword* *of* *the* *Lictor* <this list continues
indefinitely>
,→
The following 25 are athletic events in the Decathlon for men:
1.
*
*
*
<this empty itemized lise continues indefinitely>
The following 25 are seasons of the year:
1. *The History of the Decline and Fall of the Roman Empire*
1. Volume 1
2. Volume 2
3. Volume 3 <this list continues indefinitely>
Figure 15: A few examples of generation with greedy de-
coding from the meta-llama/Llama-2-13b-hf check-
point. Bold text indicates the prompt. Boldface text in
<> indicates authors’ notes. This model often produces
incoherent text, leading us to conclude there is a bug in
the uploaded weights.
The following 25 are known moons of Mars
1. Phobos
2. Deimos
The following 25 are the species of the main characters with a
dialogue in the movie The Lion King
,→
1. Lion
2. Warthog
3. Meerkat
4. Mandrill
5. Hyena
6. Hornbill
The following 25 are the vegetables common in traditional greek
salads
,→
1. Tomato
2. Cucumber
3. Onion
4. Pepper
5. Kalamata olive
Figure 16: Prefix given to pretrained language model
before each sample of the TRIVIAFACTS dataset to en-
courage in-context learning and demonstrate how topic-
change can be done, half-way through a list, when the
full answer set is exhausted, for the experiments men-
tioned in Section 5.2
19

Llama2-7B
Llama2-70B
Llama3-8B
Llama3-70B
OLMo-1B
OLMo-7B
Pythia-70M
Pythia-160M
Pythia-410M
Pythia-1B
Pythia-1.4B
Pythia-2.8B
Pythia-6.9B
Pythia-12B
0
5
10
15
20
25
Average answer counts
4.7
11.3
7.8
5.1
13.1
5.0
4.0
7.0
6.1
6.3
4.1
8.6
5.3
5.0
2.9
4.7
16.5
4.5
8.2
8.1
4.2
3.0
16.3
5.7
3.2
17.8
3.7
4.2
18.3
2.6
5.7
16.1
3.2
6.0
15.6
3.7
7.3
12.5
4.1
7.7
12.7
4.1
8.0
12.2
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 17: Larger models resort to more sophisticated fallback behaviors on the TRIVIAFACTS dataset. Here,
models of increasing size produce more correct facts (green) and hallucinations (orange) while producing fewer
repeated facts (blue). The horizontal green line indicates the maximum number of correct answers possible.
Llama2-7B
Llama2-70B
Llama3-8B
Llama3-70B
OLMo-1B
OLMo-7B
Pythia-70M
Pythia-160M
Pythia-410M
Pythia-1B
Pythia-1.4B
Pythia-2.8B
Pythia-6.9B
Pythia-12B
0
5
10
15
20
25
Average answer counts
7.2
13.4
2.7
2.1
10.9
9.0
4.2
16.5
3.3
6.4
11.7
4.7
4.0
18.0
2.0
4.8
17.2
3.8
14.7
6.5
2.7
19.2
2.8
2.6
20.5
3.9
19.7
4.0
18.6
4.9
18.1
4.2
17.5
2.4
6.3
16.7
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 18: Larger models resort to more sophisticated fallback behaviors on the QAMPARI dataset. Here,
models of increasing size produce more correct facts (green) and hallucinations (orange) while producing fewer
repeated facts (blue). The horizontal green line indicates the maximum number of correct answers possible.
0B
1B
2B
4B
8B
17B
34B
67B
134B 268B 300B
0B
1B
2B
4B
8B
17B
34B
67B
134B 268B 300B
0
5
10
15
20
25
Average answer counts
23.7
24.7
15.3
8.1
1.8
19.6
3.4
2.8
17.1
4.7
3.9
17.7
1.6
1.7
4.4
17.2
2.8
6.3
15.8
3.5
5.7
14.4
4.2
6.8
13.3
4.1
7.7
12.7
4.5
17.3
2.4
24.6
15.7
8.2
2.3
18.4
2.9
3.2
19.5
4.6
18.2
1.9
5.1
16.9
2.8
5.4
15.2
3.6
7.2
13.8
4.1
7.6
12.5
4.1
8.0
12.2
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 19: Pythia models that train longer shift to complex fallbacks. The more training tokens Pythia models
see (in billions), the more hallucinations they produce and the fewer repetitions they generate (on TRIVIAFACTS).
The left group depicts the trend for Pythia-6.9B pretraining while the right group is for Pythia-12B. The horizontal
green line indicates the maximum number of correct answers possible.
4B
22B
88B
221B
442B
695B
1137B 1579B 2022B 2464B
84B
126B
168B
210B
494B
1416B 1837B 2257B 2676B
0
5
10
15
20
25
Average answer counts
2.8
20.7
2.7
17.6
4.0
4.4
18.3
2.7
4.5
15.7
1.8
3.1
5.7
12.7
2.2
3.5
6.1
13.6
4.1
9.2
9.9
4.0
8.3
11.2
4.2
8.8
7.8
3.7
4.5
8.2
8.1
4.2
2.7
19.2
2.4
3.6
17.8
2.8
3.3
16.7
4.3
2.8
18.6
2.5
2.3
4.2
17.8
2.4
5.0
16.7
2.5
5.0
15.8
2.6
5.9
14.7
2.9
4.7
16.5
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 20: OLMo models that train longer shift to complex fallbacks. The more training tokens Pythia models
see (in billions), the more hallucinations they produce and the fewer repetitions they generate (on TRIVIAFACTS).
The left group depicts the trend for OLMo-1B pretraining while the right group is for OLMo-7B. The horizontal green
line indicates the maximum number of correct answers possible.
20

Llama2-7B-base
Llama2-7B-chat
Llama2-70B-base
Llama2-70B-chat
Llama3-8B-base
Llama3-8B-Instruct
Llama3-70B-base
Llama3-70B-Instruct
OLMo-7B-base
OLMo-7B-SFT
OLMo-7B-Instruct
Pythia-2.8B-base
Pythia-2.8B-Dolly
Pythia-6.9B-base
Pythia-6.9B-Dolly
Pythia-12B-base
Pythia-12B-Dolly
0
5
10
15
20
25
Average answer counts
4.7
11.3
7.8
5.2
12.3
4.2
5.1
13.1
5.0
5.2
15.6
2.6
4.0
7.0
6.1
6.3
5.2
12.6
3.6
3.5
4.1
8.6
5.3
5.0
3.2
3.7
2.6
12.6
2.3
4.5
8.2
8.1
4.2
4.5
11.0
7.4
4.3
12.4
4.0
3.4
3.7
7.3
12.5
3.0
8.5
5.6
2.2
5.3
4.1
7.7
12.7
3.0
7.8
3.1
5.4
5.2
4.1
8.0
12.2
3.1
6.6
4.8
2.8
7.5
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 21: Instruction-tuned models resort to more complex fallback behaviors, on the TRIVIAFACTS dataset.
The Dolly models (instruction-tuned variants of Pythia) hallucinate more and repeat facts less, while also breaking
out of loops more often and abstaining from producing additional facts (increase in red and pink bars). For the OLMo
and Llama 2 families, which had a more robust phase of instruction-following training, the results are much more
pronounced. The horizontal green line indicates the maximum number of correct answers possible.
Llama2-7B-base
Llama2-7B-chat
Llama2-70B-base
Llama2-70B-chat
Llama3-8B-base
Llama3-8B-Instruct
Llama3-70B-base
Llama3-70B-Instruct
OLMo-7B-base
OLMo-7B-SFT
OLMo-7B-Instruct
Pythia-2.8B-base
Pythia-2.8B-Dolly
Pythia-6.9B-base
Pythia-6.9B-Dolly
Pythia-12B-base
Pythia-12B-Dolly
0
5
10
15
20
25
Average answer counts
7.2
13.4
2.7
16.7
3.2
2.1
10.9
9.0
2.5
18.8
2.0
4.2
16.5
3.3
11.8
8.9
6.4
11.7
4.7
4.7
2.2
6.0
9.7
4.8
17.2
8.4
14.9
12.8
9.1
4.9
18.1
4.6
8.2
10.7
4.2
17.5
2.4
10.9
5.6
5.1
6.3
16.7
9.9
6.8
6.8
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 22: Instruction-tuned models resort to more complex fallback behaviors, on the QAMPARI dataset.
The Dolly models (instruction-tuned variants of Pythia) hallucinate more and repeat facts less, while also breaking
out of loops more often and abstaining from producing additional facts (increase in red and pink bars). For the OLMo
and Llama 2 families, which had a more robust phase of instruction-following training, the results are much more
pronounced.
Llama2-7B
Llama2-70B
Llama3-8B
Llama3-70B
OLMo-1B
OLMo-7B
Pythia-70M
Pythia-160M
Pythia-410M
Pythia-1B
Pythia-1.4B
Pythia-2.8B
Pythia-6.9B
Pythia-12B
0
5
10
15
20
25
Average answer counts
4.6
12.6
5.8
4.8
13.2
3.4
2.1
4.2
8.3
6.1
4.7
3.8
8.1
4.9
2.8
4.7
2.8
7.0
13.4
4.5
8.9
8.2
3.1
3.5
15.4
5.4
3.8
14.9
2.3
3.7
5.6
15.6
2.3
7.7
13.8
3.0
8.5
12.6
3.5
7.6
11.1
2.2
4.0
8.4
11.7
4.1
9.6
10.2
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 23: Larger models use more sophisticated fallbacks even with random decoding. Results of different
models on the TRIVIAFACTS dataset with temperature sampling, setting the sampling temperature (τ) to 0.5. Each
completion was sampled five times. The horizontal green line indicates the maximum number of correct answers
possible. The horizontal green line indicates the maximum number of correct answers possible.
21

OLMo-1B
OLMo-7B
Pythia-70M
Pythia-160M
Pythia-410M
Pythia-1B
Pythia-1.4B
Pythia-2.8B
Pythia-6.9B
Pythia-12B
0
5
10
15
20
25
Average answer counts
2.7
6.0
12.3
2.8
4.7
4.6
3.3
12.4
2.9
16.4
5.7
2.6
19.7
2.4
1.5
3.6
16.3
3.2
2.5
4.9
14.3
2.8
3.3
5.7
13.6
1.9
3.5
5.2
9.8
6.2
4.0
6.5
10.1
4.0
4.1
6.2
9.7
4.6
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 24: Larger models volunteer hallucinations, even when not asked for additional facts. When given
prompts from TRIVIAFACTS without specifying the number of items in advance (see Section 5.2), larger models
continue to produce more hallucinations than their smaller counterparts and barely exhibit abstaining strategies (e.g.,
changing the topic). The horizontal green line indicates the maximum number of correct answers possible.
Llama2-7B-chat (IDK)
Llama2-7B-chat
Llama2-70B-chat (IDK)
Llama2-70B-chat
Llama3-8B-Instruct (IDK)
Llama3-8B-Instruct
Llama3-70B-Instruct (IDK)
Llama3-70B-Instruct
OLMo-7B-SFT
OLMo-7B-SFT (IDK)
OLMo-7B-Instruct
OLMo-7B-Instruct (IDK)
Pythia-2.8B-Dolly (IDK)
Pythia-2.8B-Dolly
Pythia-6.9B-Dolly (IDK)
Pythia-6.9B-Dolly
Pythia-12B-Dolly (IDK)
Pythia-12B-Dolly
0
5
10
15
20
25
Average answer counts
5.1
13.7
2.6
3.2
5.2
12.3
4.2
5.3
17.2
5.2
15.6
2.6
5.3
14.4
5.0
5.2
12.6
3.6
3.5
4.1
8.8
3.1
8.0
3.2
3.7
2.6
12.6
2.3
4.5
11.0
7.4
4.7
11.0
9.3
4.3
12.4
4.0
3.4
4.6
14.3
4.4
3.3
7.4
5.2
6.9
3.0
8.5
5.6
2.2
5.3
3.4
9.3
3.6
5.3
3.0
7.8
3.1
5.4
5.2
3.1
7.6
3.0
9.8
3.1
6.6
4.8
2.8
7.5
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 25: Instruction tuned models continue to hallucinate, even when told to prefer abstaining over non-
factual responses. When given the prefix “Complete the following list with facts you are sure of,
and stop when you cannot recall additional facts” to encourage abstaining instead of hallucinating, all
instruction tuned models fail to utilize internal uncertainty estimations if exists and continue to hallucinate. The
models are only slightly more inclined to abstain (by changing topics or producing EOS tokens). The results are
given for the TRIVIAFACTS dataset, with “(IDK)” marking the model with the modified prompt. The horizontal
green line indicates the maximum number of correct answers possible.
22

Llama2-7B
Llama2-70B
Llama3-8B
Llama3-70B
OLMo-1B
OLMo-7B
Pythia-70M
Pythia-160M
Pythia-410M
Pythia-1B
Pythia-1.4B
Pythia-2.8B
Pythia-6.9B
Pythia-12B
0
5
10
15
20
25
Average answer counts
5.0
8.3
4.0
7.5
5.1
7.4
11.5
3.5
20.0
3.9
2.1
18.8
3.7
4.9
13.2
2.9
4.7
7.2
6.4
6.7
2.6
20.8
3.4
20.6
2.0
3.5
17.6
2.4
4.7
13.2
4.1
2.9
4.9
12.9
4.1
3.8
5.8
12.1
3.1
4.1
6.7
12.4
4.3
7.0
11.0
2.5
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 26: Larger models hallucinate more, even when shown how to abstain. When given the prefix from
Figure 16 to encourage abstaining instead of hallucinating, larger Pythia and OLMo models continue to exhibit the
same fallback behavior trends, with repetitions decreasing and hallucinations increasing as model size grows. The
models are only slightly more inclined to abstain (by changing topics). Notably, while Llama 2 and Llama 3 models
exhibit the predicted trends in the proportion of hallucinations, they are considerably more capable of abstaining
when given the opportunity. Interestingly, Pythia-410M is able to change topics remarkably well, though when
manually inspecting its outputs, we find that in almost all cases it produces a list of five facts, often with repetitions,
and then continues to repeat this list indefinitely. The results are given for the TRIVIAFACTS dataset, and the
horizontal green line indicates the maximum number of correct answers possible.
23

2.8B
6.9B
12B
0
10
20
30
40
Atomic facts
3.5
(8.3%)
23.4
(55.2%)
14.5
(34.1%)
3.3
(7.6%)
28.4
(64.6%)
11.2
(25.6%)
5.4
(11.3%)
33.8
(71.0%)
7.4
(15.6%)
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 27: Larger Pythia models hallucinate more
when generating open-ended biographies on very
rare entities. When producing biographies of entities
with very low frequency of appearance in Wikipedia,
larger models generate more atomic facts, with an in-
creasing rate of hallucinations.
2.8B
6.9B
12B
0
10
20
30
40
Atomic facts
3.6
(9.7%)
23.4
(62.4%)
9.5
(25.3%)
5.1
(11.7%)
27.8
(63.5%)
9.8
(22.5%)
4.8
(10.1%)
33.2
(70.5%)
8.2
(17.3%)
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 28: Larger Pythia models hallucinate more
when generating open-ended biographies on rare en-
tities. When producing biographies of entities with low
frequency of appearance in Wikipedia, larger models
generate more atomic facts, with an increasing rate of
hallucinations.
2.8B
6.9B
12B
0
10
20
30
40
50
Atomic facts
6.4
(13.5%)
29.6
(61.9%)
10.8
(22.6%)
5.3
(9.6%)
34.4
(62.1%)
14.7
(26.5%)
7.0
(14.7%)
33.9
(71.7%)
5.4
(11.4%)
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 29:
Larger Pythia models hallucinate
more when generating open-ended biographies on
medium-popularity entities. When producing biogra-
phies of entities with medium frequency of appearance
in Wikipedia, larger models generate more atomic facts,
with an increasing rate of hallucinations.
2.8B
6.9B
12B
0
10
20
30
40
50
60
Atomic facts
10.4
(21.5%)
24.8
(51.3%)
12.2
(25.2%)
11.4
(21.0%)
34.2
(63.1%)
7.6
(14.0%)
14.2
(22.8%)
32.0
(51.4%)
15.1
(24.2%)
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 30: Larger Pythia models hallucinate more
when generating open-ended biographies on popular
entities. When producing biographies of entities with
high frequency of appearance in Wikipedia, larger mod-
els generate more atomic facts, with an increasing rate
of hallucinations.
2.8B
6.9B
12B
0
20
40
60
Atomic facts
13.0
(22.7%)
32.4
(56.6%)
10.8
(19.0%)
13.6
(22.6%)
36.8
(61.1%)
8.8
(14.6%)
19.0
(27.4%)
33.9
(49.0%)
15.2
(22.1%)
correct
hallucinations
repetitions
bad_format
topic_change
EOS
Figure 31: Larger Pythia models hallucinate more
when generating open-ended biographies on very
popular entities.
When producing biographies of
entities with very high frequency of appearance in
Wikipedia, larger models generate more atomic facts,
with an increasing rate of hallucinations.
Figure 32: Pyhtia-1.4B model shift fallback behav-
ior during a single completion. Order of fallbacks
per generation on TRIVIAFACTS for a Pythia-1.4B
model—each row represents a specific prompt with the
25 produced facts. Green marks correct answers, orange
hallucinations and blue repeated facts. Purple sequences
indicate a topic change. Questions are sorted by the
number of consecutive repetitions.
24

Figure 33: Pyhtia-2.8B model shift fallback behav-
ior during a single completion. Order of fallbacks
per generation on TRIVIAFACTS for a Pythia-2.8B
model—each row represents a specific prompt with the
25 produced facts. Green marks correct answers, or-
ange hallucinations and blue repeated facts. Purple
sequences indicate a topic change, and red ones indicate
a divergence to bad-format. Questions are sorted by the
number of consecutive repetitions.
Figure 34: Pyhtia-6.9B model shift fallback behav-
ior during a single completion. Order of fallbacks
per generation on TRIVIAFACTS for a Pythia-6.9B
model—each row represents a specific prompt with the
25 produced facts. Green marks correct answers, or-
ange hallucinations and blue repeated facts. Purple
sequences indicate a topic change, and red ones indicate
a divergence to bad-format. Questions are sorted by the
number of consecutive repetitions.
Figure 35: Pyhtia-12B model shift fallback behav-
ior during a single completion. Order of fallbacks
per generation on TRIVIAFACTS for a Pythia-12B
model—each row represents a specific prompt with the
25 produced facts. Green marks correct answers, or-
ange hallucinations and blue repeated facts. Purple
sequences indicate a topic change, and red ones indicate
a divergence to bad-format. Questions are sorted by the
number of consecutive repetitions.
Figure 36: OLMo-1B model shift fallback behavior
during a single completion. Order of fallbacks per gen-
eration on TRIVIAFACTS for a OLMo-1B model—each
row represents a specific prompt with the 25 produced
facts. Green marks correct answers, orange hallucina-
tions and blue repeated facts. Purple sequences indicate
a topic change, and red ones indicate a divergence to
bad-format. Questions are sorted by the number of con-
secutive repetitions.
Figure 37: OLMo-7B model shift fallback behavior
during a single completion. Order of fallbacks per gen-
eration on TRIVIAFACTS for a OLMo-7B model—each
row represents a specific prompt with the 25 produced
facts. Green marks correct answers, orange hallucina-
tions and blue repeated facts. Purple sequences indicate
a topic change, and red ones indicate a divergence to
bad-format. Questions are sorted by the number of con-
secutive repetitions.
Figure 38: Llama 2 7B model shift fallback behav-
ior during a single completion. Order of fallbacks
per generation on TRIVIAFACTS for a Llama 2 7B
model—each row represents a specific prompt with the
25 produced facts. Green marks correct answers, or-
ange hallucinations and blue repeated facts. Purple
sequences indicate a topic change, and red ones indicate
a divergence to bad-format. Questions are sorted by the
number of consecutive repetitions.
25

Figure 39: Llama 2 70B model shift fallback behav-
ior during a single completion. Order of fallbacks
per generation on TRIVIAFACTS for a Llama 2 70B
model—each row represents a specific prompt with the
25 produced facts. Green marks correct answers, or-
ange hallucinations and blue repeated facts. Purple
sequences indicate a topic change, and red ones indicate
a divergence to bad-format. Questions are sorted by the
number of consecutive repetitions.
Figure 40: Llama 3 8B model shift fallback behav-
ior during a single completion. Order of fallbacks
per generation on TRIVIAFACTS for a Llama 3 8B
model—each row represents a specific prompt with the
25 produced facts. Green marks correct answers, or-
ange hallucinations and blue repeated facts. Purple
sequences indicate a topic change, and red ones indicate
a divergence to bad-format. Questions are sorted by the
number of consecutive repetitions.
Figure 41: Llama 3 70B model shift fallback behav-
ior during a single completion. Order of fallbacks
per generation on TRIVIAFACTS for a Llama 3 70B
model—each row represents a specific prompt with the
25 produced facts. Green marks correct answers, or-
ange hallucinations and blue repeated facts. Purple
sequences indicate a topic change, and red ones indicate
a divergence to bad-format. Questions are sorted by the
number of consecutive repetitions.
26

