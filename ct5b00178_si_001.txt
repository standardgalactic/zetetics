Designing Free Energy Surfaces that Match
Experimental Data with Metadynamics
Andrew D. White, James F. Dama, Greg A. Voth
Department of Chemistry, James Frank Institute,
Institute for Biophysical Dynamics, and Computation Institute,
The University of Chicago,
5735 S Ellis Ave.,
Chicago, Illinois 60637, USA
Center for Nonlinear Studies,
Theoretical Division,
Los Alamos, New Mexico, 87545. USA
1
Calculus of Variations Minimal Biasing Proof
We will follow the derivation in Roux and Weare,1 which followed the derivations in Jayne2 and was sketched
out in the main text. Assume we have a statistical ensemble with coordinates X = {⃗r1,⃗r2, ...,⃗rN} deﬁned by
a probability distribution P0(X) which is exponentially distributed. We want to ﬁnd a related probability
distribution P(X) which matches some target PMF, Q(s) along a collective variable s(X) so that:
Z
dX δ(s′(X) −s)P(X) = Q(s)
(S1)
We will drop the s(X) dependence on X and note that s may be a vector. The other condition is that
P(X) is normalized:
Z
dX P(X) = 1
(S2)
To make P(X) the minimal bias, we will minimize the relative entropy between P0(X) and P(X), which
is
η [P(X)||P0(X)] =
Z
dXP(X) ln P(X)
P0(X)
(S3)
This may be done with the method Lagrange multipliers, where the relative entropy is a functional to
be minimized subject to the constraints. The Lagrange equation is:
0 =
δ
δP(X ′)
Z
dXP(X) ln P(X)
P0(X) + α

1 −
Z
dX P(X)

+ λ(s)
Z
dX δ(s′(X) −s)P(X) −Q(s)

(S4)
Notice here that we have a diﬀerent λ at each s, which is unusual for a Lagrange multiplier. The functional
derivative of this expression becomes
0 = ln[P0(X)] −ln[P(X)] + α + λ(s)
(S5)
S1

P(X) = P0(X)eα+λ(s)
(S6)
Using a Boltzmann distribution for P0 and dropping α into a normalization constant, the new probability
distribution becomes:
P(X) =
e−β[U0(X)+λ(s)]
R
dX e−β[U0(X)+λ(s)]
(S7)
2
Proof that Additional Dimensions Increase Relative Entropy
Now, treat Equation S7 as a proposed form of the biased ensemble, where λ(s) is the bias along the collective
variable. The requirement on the bias is that it satisﬁes Equation S1, via the metadynamics procedure
described in the main text, and is a function of the collective variable only. Consider an alternative λ′(s, u)
that satisﬁes Equation S1 but also varies in the u(X) dimension that may not be independent of s(X).
We wish to prove that
η [P(X)||P0(X)] < η [P ′(X)||P0(X)]
(S8)
for
P(X) = 1
Z e−βU0(X)e−βλ(s) = 1
ZL
P0(X)L(s), ZL =
R
dX e−βU0(X)e−βλ(s)
R
dX e−βU0(X)
(S9)
P ′(X) = 1
Z′ e−βU0(X)e−βλ′(s,u) = 1
Z′
L
P0(X)L′(s, u), Z′
L =
R
dX e−βU0(X)e−βλ′(s,u)
R
dX e−βU0(X)
(S10)
P(X) is the proposed minimum bias where the bias acts only in the dimension of the PMF constraint.
P ′(X) is an alternative distribution which matches the PMF constraint, but the bias acts in a second
dimension u. This corresponds to, for example, modifying a force-ﬁeld parameter in a molecular simulation
to match new experimental data. Such a modiﬁcation creates bias in the direction of the PMF constraint,
but in other dimensions as well. Z and Z′ are the partition functions for P(X) and P ′(X), respectively. ZL
is the ratio of the partition function of the unbiased distribution and the bias distribution.
Before proving the increase in relative entropy, we will need to be able to relate properties of the two
proposed distributions via the constraint of Equation S1 that forces them to match the target probability
distribution. That relationship is through the following lemma:
1
Z′
L
Z
du P0(u|s)L′(s, u) = L(s)
ZL
(S11)
where P0(u|s) is the conditional probability and P0(s, u) is the marginal distribution, both of P0(X). This
lemma shows that for a particular value of s, the expectation over u of L′(s, u) is the same as L(s). The
additional dimension u does not change the average bias in the s dimension. Thus the P ′(X) distribution
behaves like P(X) in the s dimension and, by deﬁnition, not like P0(X) in the u dimension.
The proof of the lemma in Equation S11 starts with the constraint of Equation S1 acting on P ′(X):
Z
dX δ(s′(X) −s)P ′(X) = Q(s)
(S12)
which can be rewritten as
1
Z′
L
Z
du
Z
dX δ(s′(X) −s)δ(u′(X) −u)P0(X)L′(s, u) = Q(s)
(S13)
= 1
Z′
L
Z
du P0(s, u)L′(s, u) = P0(s)
Z′
L
Z
du P0(u, s)
P0(s) L′(s, u)
(S14)
S2

1
Z′
L
Z
du P0(u|s)L′(s, u) = Q(s)
P0(s)
(S15)
The constraint of Equation S1 acting on P(X) can be substituted:
Q(s)
P0(s) = L(s)P0(s)
ZLP0(s) = L(s)
ZL
(S16)
This proves Equation S11.
Now use the deﬁnition of relative entropy, Equation S3, and the ﬁrst probability distribution given in
Equation S9. We may separate out the set of dimensions that depend on s:
Z
dXP(X) ln P(X)
P0(X) = 1
ZL
Z
ds′
Z
dX δ(s′ −s)P0(X)L(s) ln L(s)
ZL
(S17)
=
Z
ds′ L(s′) ln L(s′)
ZL
Z
dX δ(s′ −s)P0(X)
(S18)
Z
dXP(X) ln P(X)
P0(X) =
Z
ds P0(s)L(s)
ZL
ln L(s)
ZL
(S19)
Following a similar derivation for P ′(X):
Z
dXP ′(X) ln P ′(X)
P0(X) = 1
Z′
L
Z
ds′
Z
du′
Z
dX δ(s′ −s)δ(u′ −u)P0(X)L′(s, u) ln L′(s, u)
Z′
L
(S20)
= 1
Z′
L
Z
ds
Z
du P0(s, u)L′(s, u) ln L′(s, u)
Z′
L
(S21)
= 1
Z′
L
Z
ds P0(s)
Z
du P0(u|s)L′(s, u) ln L′(s, u)
Z′
L
(S22)
We will rewrite this using the function φ(x) = x ln x, which is a convex function.
= 1
Z′
L
Z
ds P0(s)
Z
du P0(u|s)φ
L′(s, u)
Z′
L

(S23)
Applying Jensen’s Inequality to the u-integrand
Z
du P0(u|s)φ
L′(s, u)
Z′
L

≥φ
Z
du P0(u|s)L′(s, u)
Z′
L

(S24)
We may insert Equation S11, the lemma proved above, into the right-hand-side of Equation S24
Z
du P0(u|s)φ
L′(s, u)
Z′
L

≥φ
L(s)
ZL

= L(s)
ZL
ln L(s)
ZL
(S25)
Recombining the terms and using Equation S19:
Z
dXP ′(X) ln P ′(X)
P0(X) ≥
Z
ds P0(s)L(s)
ZL
ln L(s)
ZL
=
Z
dXP(X) ln P(X)
P0(X)
(S26)
thereby proving Equation S8.
S3

3
Uniqueness of bias
Equation S1 can be rewritten as
1
ZL
P0(s)L(s) = Q(s)
(S27)
where it is understood that P0(s) is the marginal distribution over the collective variable s of the original
distribution. Assume L(s) exists. If either Q(s) > 0 and P0(s) > 0 or we restrict s to a region where those
two inequalities are true, then L(s) must be unique for Equation S27 to be true for every s. The condition
that Q(s) is strictly positive is not always true (e.g., a radial distribution functions). P0(s) > 0 is true given
ﬁnite energies but estimates of P0(s) often contain zeros due to ﬁnite sampling in simulations. Practically,
this means that L(s) is unique in regions where s is both sampled and Q(s) > 0.
4
Smoothed Boundary Corrected Gaussian in 1D
The hills are given by
G(s, x) = e−(x−s)2/σ2 +

e−(x−L)2/σ2 −e−(x−s)2/σ2
u
s −L
bσ

+

e−(U−x)2/σ2 −e−(x−s)2/σ2
u
U −s
bσ

(S28)
where s is the coordinate of the bias, x is the location of the hill, U is the upper boundary, L is the lower
boundary, σ is the hill width, b sets the size of the transition region and u(s) is
u(s) = 2s3 −3s2 + 1
(S29)
The McGovern-De Pablo multiplicative boundary correction is3
¯G(s) =
Z U
L
G(s, x)dx
(S30)
so that the hills become G(s, x)/ ¯G(s). The usual McGovern-De Pablo multiplicative boundary correction
are indicated as m(s, U, L, σ).
¯G(s, L, U, b, σ) = m(s, U, L) +
1
2
√πσ erf
U −L
σ

−m(s, U, L)

u
s −L
bσ

+
(S31)
1
2
√πσ erf
U −L
σ

−m(s, U, L)

u
U −s
bσ

m(s, U, L, σ) =
√π
2 σ

erf
s −L
σ

+ erf
U −s
σ

The derivatives in 1 dimension are given below.
∂G
∂s = 2(s −x)
σ2
e−(x−s)2/σ2+
(S32)

e−(x−L)2/σ2 −e−(x−s)2/σ2 ∂u
∂x
s −L
bσ
 1
bσ −2(s −x)
σ2
e−(x−s)2/σ2u
s −L
bσ

−

e−(U−x)2/σ2 −e−(x−s)2/σ2 ∂u
∂x
U −s
bσ
 1
bσ −2(s −x)
σ2
e−(x−s)2/σ2u
U −s
bσ

S4

∂¯G
∂s = ∂m(s, U, L, σ)
∂s
+
1
2
√πσ erf
U −L
σ

−m(s, U, L, σ)
 ∂u
∂x
s −L
bσ
 1
bσ −∂m(s, U, L, σ)
∂s
u
s −L
bσ

−
1
2
√πσ erf
U −L
σ

−m(s, U, L, σ)
 ∂u
∂x
U −s
bσ
 1
bσ −∂m(s, U, L, σ)
∂s
u
U −s
bσ

∂m(s, U, L, σ)
∂s
= e−(s−L)2/σ2 −e−(U−s)2/σ2
∂G/ ¯G
∂s
=
∂G
∂s ¯G −∂¯
G
∂s G
¯G2
References
[1] B Roux, J Weare. On the Statistical Equivalence of Restrained-Ensemble Simulations with Maximum
Entropy Method (2013) J. Chem. Phys. 138 084107.
[2] ET Jayne. Information Theory and Statistical Mechanics, I (1957) Phys. Rev. 106 620-630.
[3] M Mcgovern, J de Pablo. A boundary correction algorithm for metadynamics in multiple dimensions
(2013) J. Chem. Phys. 139 (8).
S5

System Potential
a)
b)
Figure S1: Comparison of smoothed boundary-corrected hills (SBC) vs boundary-corrected hills. This is a
partially converged simulation with exaggerated boundary eﬀects. The region within the dashed vertical lines
is being ﬂattened. Panel (a) uses SBC hills and (b) uses BC hills. The BC hills have non-zero derivatives
at the boundary, which manifest as the bias applied having opposite slopes at the interface as compared
with the system potential. This is most visible at the right boundary. SBC hills vary between the system
potential and biasing, although at the expense having much slower convergence in the transition region. The
purple line is the result of histogramming the last 20% of the simulation and the sharp transitions can be
seen aﬀecting the PMF.
Figure S2: Scaling of Lammps implementation. The run-time ratio of 500 steps of a 25,000 particle Lennard-
Jones simulation with EDM at a stride of 100, an unbounded hill density (all pairs are considered) and a
cutoﬀof 4σ.
S6

a)
c)
d)
b)
Figure S3: The results of EDM simulations targeting the blue radial distribution function with various hill
widths. The particulars of this system are described in the main text; here, only the hill width diﬀers. In
order of a-d, the widths are 0.1σ∗, 0.05σ∗, 0.025σ∗, and 0.01σ∗. Panel c was chosen for the main text.
a)
b)
Figure S4: The radial distribution function for the lithium-ethylene carbonate carbonyl oxygen (a) and
lithium-ﬂuorine pairs (b) from an EDM simulation of the lithium electrolyte system described in the main
text. Both radial distribution functions were biased and the improvement in Panel b relative to Figure 6b is
signiﬁcant.
S7

