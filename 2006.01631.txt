arXiv:2006.01631v2  [math.CT]  28 Jul 2020
Bayesian Updates Compose Optically
Toby St. Clere Smithe
Department of Experimental Psychology,
University of Oxford
arxiv@tsmithe.net
July 29, 2020
Bayes’ rule tells us how to invert a causal process in order to update our beliefs in light of
new evidence. If the process is believed to have a complex compositional structure, we may ask
whether composing the inversions of the component processes gives the same belief update as the
inversion of the whole. We answer this question aﬃrmatively, showing that the relevant compo-
sitional structure is precisely that of the lens pattern, and that we can think of Bayesian inversion
as a particular instance of a state-dependent morphism in a corresponding ﬁbred category. We
deﬁne a general notion of (mixed) Bayesian lens, and discuss the (un)lawfulness of these lenses
when their contravariant components are exact Bayesian inversions. We prove our main result
both abstractly and concretely, for both discrete and continuous states, taking care to illustrate
the common structures.
1. Introduction
Bayesian inference appears whenever we wish to understand the latent causes of stochastically generated
data. In this paper, we show how the inversion of a generative process ﬁts a pattern known as optics [1, 2]
that describes various kinds of bidirectional transformation. In particular, we show that the Bayesian inverse
of a complex stochastic process can be constructed compositionally according to this common pattern. We
will assume that the reader has some rudimentary knowledge of category theory, but not necessarily either
of optics or of categorical approaches to probability. As such, we have attempted to keep this paper self-
contained. For basic introductions to category theory and its applications, we recommend Leinster [3] and
Fong and Spivak [4].
As a slogan, our main result is that Bayesian updates compose optically (Theorem 5.2). The following corol-
lary states this more formally:
Corollary (5.3). Let C be a copy-delete category (Deﬁnition 2.2), and let C† be its wide subcategory of mor-
phisms that admit Bayesian inversion (Deﬁnition 2.3). Then C† embeds functorially into the category of
Bayesian lenses BayesLens (Deﬁnition 4.3).
To supply some intuition for these ideas, we situate this work in the nascent discipline of categorical cy-
bernetics: although Bayesian inference is very widely applicable, the cybernetic context makes plain the
bidirectional information ﬂow in which we are interested.
1

We think of a cybernetic system as being embedded in some environment, and aiming to control some
aspect of that environment—such as its habitability. In order for the system to achieve its aims, it must
maintain a representation of the state of the environment which it seeks to control. But this external state
may not be directly accessible to the system, and moreover the process by which the system’s inputs are
generated from the environmental state is likely to be stochastic; or, the system may only have a partial view
of this state. Somehow, in forming a representation of the relevant environmental state, the system must
invert this generative process.
In such a setting, we can model the generative process by a stochastic channel X→
• Y , where X is some
space of environmental states and Y is some space of ‘sensory’ inputs to the system. Part of the system’s
task is therefore to obtain from this channel an inverse channel Y →
• X by which it can infer, given some
sensory data in Y , a belief about the environmental state in X which caused that sensory data. This process
of inference is known as Bayesian inference, following Bayes’ theorem of probability.
There is an inherent bidirectionality here: a generative forwards channel X→
• Y and an inverse backwards
channel Y →
• X that, according to Bayes’ law, depends also on a prior belief about X. The abstract pattern that
captures this kind of ‘dependent’ bidirectionality is called a lens. Lenses were originally developed in database
theory [5], where the idea is that given a database record and a ﬁeld within it, one can zoom in on the ﬁeld
and view its value inside the record; then in the other direction, given a record and a new value for a ﬁeld, one
can go back and obtain a correspondingly updated record. Consequently, we call the ﬁrst transformation view
and the second update. The inference process is similar: the environment causes input data (a partial ‘view’);
then, given some prior belief about the environment’s state and this sensory data, the system can update its
belief to reﬂect the new input information.
In the database context, these transformations are functions view : X →Y and update : X × Y →
X that are both morphisms in the same category, typically the category Set of sets and functions; such
lenses are called Cartesian. But, given a stochastic channel c : X→
• Y , the corresponding Bayesian inversion
operation is in general not another channel c† : X ⊗Y →
• X in the same category C: instead, it is a map of
the form PX →C(Y, X), where PX denotes some space of states (i.e., prior beliefs) on X. Our ﬁrst main
contribution is to formalize this ﬁbrationally: given a base category of channels C, there is a ﬁbre over each
X : C whose morphisms B→
• A are X-state-dependent channels of the form PX →C(B, A), and which
compose compatibly with both the horizontal structure in the base category and the vertical structure in the
ﬁbres. The operation of Bayesian inversion is such a state-dependent channel, and following Spivak [6] we
can use this structure to deﬁne a particular abstract category of lenses whose view maps live in the base and
whose update maps live in the ﬁbres.
In keeping with the bidirectionality of lenses, the view transformations compose covariantly, while the
backwards update transformations compose contravariantly. This just means that, given a composite gen-
erative process X→
• Y →
• Z, we form the composite inverse by ﬁrst inverting the (causally proximal) second
process Y →
• Z, and then inverting the (distal) ﬁrst process X→
• Y . For example, at the cinema, the projector
determines the state of the whole screen, which reﬂects light onto the retinae of the viewer. Then, while
focusing on a small region of the screen, the viewer’s brain maintains a belief about the whole screen, and
updates it by ﬁrst inferring from the retinal signals the picture on the region under focus, then inferring the
new state of the whole screen from the new belief about this region.
Our second main contribution is thus to show that the Bayesian inversion of a composite channel, following
Bayes’ rule, is equivalent (up to almost-equality) to the contravariant lens composition of the inversions of the
component channels. We prove this result both abstractly and concretely, for both discrete and continuous
probability. With the help of the Yoneda embedding, we also show how to translate the ﬁbred category of
lenses into optic form, and thereby recover an equivalent category of Bayesian lenses that is indeed Cartesian
(in the database sense). This allows us to deﬁne mixed Bayesian lenses, whose ‘vertical’ category is diﬀerent
2

from the base category, but which still captures the state-dependence of Bayesian inversion.
Finally, lenses as introduced in the database literature are often accompanied by lens laws which capture
aspects of their behaviour that are desirable in the database context. We show that ‘exact’ Bayesian lenses are
only weakly lawful in this sense, but that this is desirable: the ‘beliefs’ held by a database are Boolean (either
true or false), whereas Bayesian beliefs are in general fuzzy mixtures, and such mixing is contradicted by the
lens laws.
Overview of paper
We have attempted to keep the presentation in this paper self-contained, and assume
that the reader may not be familiar with either categorical probability theory or coend optics. As such, in
section §2, we summarize the key structures: basic categorical probability in §2.1 (abstract and concrete,
discrete and continuous) and optics and lenses in §2.2; in Appendix §B we give a brief summary of coend
calculus necessary for optics. Since a number of our proofs are made graphically, we also introduce the
necessary graphical calculi along the way.
In §3 we introduce ﬁbred categories of state-dependent channels and the corresponding Grothendieck
lenses; then in §4 we translate these into optic form, deﬁning categories of Bayesian lenses. In §5, we prove
that the Bayesian inversion of a composite channel is (almost-)equal to the lens composite of the Bayesian
inversions of the channel’s factors, in each of the categories introduced in §2.1. Finally, in §6 we discuss the
‘lawfulness’ of Bayesian lenses.
Contributions
We deﬁne a collection of ﬁbred categories whose morphisms depend on states in the base
category (Deﬁnition 3.1), and show that Bayesian inversion is an instance of such a state-dependent morphism
(Example 3.2). The abstract pattern is however more general, and so we expect it to be more widely applicable.
We show how to construct categories of optics from such ﬁbred categories, using their (co)Yoneda embed-
dings to deﬁne actegory structures (Proposition 4.2), and we deﬁne a corresponding notion of Bayesian lens
(Deﬁnition 4.3). We generalize this to mixed optics (Deﬁnition 4.7), and exemplify the generalization with
state-dependent algebra homomorphisms (Example 4.8).
We prove that the Bayesian inversion of a composite channel coincides with the lens composite of the
inversions of its factors (Theorem 5.2). Consequently, we show that stochastic channels embed functorially
into Bayesian lenses (Corollary 5.3). We show that ‘exact’ Bayesian lenses are only weakly lawful (§6).
We hope to have presented these results and constructions pedagogically, so that this paper may serve as
a useful introduction to some of the important structures and techniques of the nascent discipline of cate-
gorical cybernetics. To this end, the background section (§2) is comprehensive but informal, and we provide
comparative proofs of Theorem 5.2 (abstract and concrete, discrete and continuous).
Notation
We write C0 for the set of objects (0-cells) in the category C. We write C(−, X) and C(X, −)
for the representable presheaf and copresheaf on the object X : C. Where C is supposed to be a category
of stochastic channels, we write its composition operator as • and denote morphisms (channels) by X→
• Y .
Otherwise, we write the composition operator as ◦and morphisms as X →Y , except for lenses and optics,
where we write ◦| and (X, A) 7→(Y, B). Given a stochastic channel c, we often adopt ‘conditional probability
notation’ c(B|x) to indicate the probability of B given x; we remind the reader of this at the relevant points.
Acknowledgements
The author thanks Bruno Gavranović, Jules Hedges, and Neil Ghani for stimulating
and insightful conversations, and credits Jules Hedges for observing the Cartesian lens form of the Bayesian
update map in discussion at SYCO 6, and for indicating problems with an earlier version of these results.
3

2. Mathematical background and graphical calculi
2.1. Compositional probability theory
In informal scientiﬁc literature, Bayes’ rule is often written in the following form:
P(A|B) = P(B|A) · P(A)
P(B)
where P(A) is the probability of the ‘event’ A, and P(A|B) is the probability of the event A given that
the event B occurred; and vice versa swapping A and B. Unfortunately, this notation obscures that there
is in general no unique assignment of probabilities to events: diﬀerent observers can hold diﬀerent beliefs.
Moreover, we are usually less interested in the probability of particular events than in the process of assigning
probabilities to arbitrarily chosen beliefs; and what should be done if P(B) = 0 for some B? The aim in this
section is to make this expression suﬁciently precise for our purposes.
The assignment of probabilities or beliefs to events is formally the task of a state on the space from which
the events are drawn; we should think of states as generalizing distributions or measures. We can write
Pπ(A) to denote the probability of A according to the state π. Similarly, we can write Pc(B|A) to denote
the probability of B given A according to the channel c, where the channel c takes events A as inputs and
emits states c(A) as outputs. This means that we can alternatively write Pc(B|A) = Pc(A)(B). In general,
whenever we encounter a ‘conditional probability distribution’, it is formally a stochastic channel.
If the input events are drawn from the space X and the output states encode beliefs about Y , then the
channel c is of type X→
• Y , written c : X→
• Y . Given a channel c : X→
• Y and a channel d : Y →
• Z, we can
compose them sequentially by marginalizing (averaging) over the possible outcomes in Y , giving a composite
channel d • c : X→
• Z. We will see precisely how this works in various settings below.
Given two spaces X and Y of events, we can form beliefs about them jointly, represented by states on
the product space denoted X ⊗Y . The numerator in Bayes’ rule represents such a joint state, by the law of
conditional probability or ‘product rule’:
Pω(A, B) = Pc(B|A) · Pπ(A)
(1)
where · is multiplication of probabilities, π is a state on X, and ω denotes the joint state on X ⊗Y . By
composing c and π to form a state c • π on Y , we can write
Pω′(B, A) = Pc†
π(A|B) · Pc•π(B)
where c†
π will denote the Bayesian inversion of c with respect to π.
Joint states in classical probability theory are symmetric, meaning that there is an isomorphism swap :
X ⊗Y ∼
−→
• Y ⊗X. Consequently, we have ω′ = swap • ω and Pω(A, B) = Pω′(B, A), and thus
Pc(B|A) · Pπ(A) = Pc†
π(A|B) · Pc•π(B)
(2)
where both left- and right-hand sides are called disintegrations of ω [7]. From this equality, we can write down
the usual form of Bayes’ theorem, now with the sources of belief indicated:
Pc†
π(A|B) = Pc(B|A) · Pπ(A)
Pc•π(B)
.
(3)
As long as Pc•π(B) ̸= 0, this equality deﬁnes the inverse channel c†
π. If the division is undeﬁned, or if we
cannot guarantee Pc•π(B) ̸= 0, then c†
π can be any channel satisfying (2).
4

There is therefore generally no unique Bayesian inversion c† : Y →
• X for a given channel c : X→
• Y : rather,
we have an inverse c†
π : Y →
• X for each prior state π on X; moreover, c†
π is not a “posterior distribution” (as
written in some literature), but a channel which emits a posterior distribution, given an observation in Y . By
allowing π to vary, we obtain a map of the form c†
(·) : PX →C(Y, X), where PX denotes a space of states
on X. This is the form described in §1, and is the key to the present paper.
Remark 2.1. There are two easily confused pieces of terminology here. We will call c†
π := c†
(·)(π) the
Bayesian inversion of the channel c with respect to π. Then, given some y ∈Y , c†
π(y) is a new ‘posterior’
distribution on X. We will call c†
π(y) the Bayesian update of π along c given y.
2.1.1. Discrete probability
Interpreting the informal Bayes’ rule (3) is simplest in the case of discrete or ﬁnitely-supported probability,
where events are just elements of sets, and a probability distribution is just an assignment of probabilities
to these elements such that that sum of all the assignments is 1. This situation is formalized by the ﬁnitely-
supported distribution monad D : Set →Set, and in this setting our category of stochastic channels will be
its Kleisli category Kℓ(D). Instead of giving a rigorous presentation of this category, we refer the reader to
Cho and Jacobs [7] and Fritz [8], giving alternatively a self-contained introduction of the structures relevant
for our purposes.
The functor D : Set →Set acts on a set X by returning the set DX of ﬁnite probability distributions over
X: that is, the set of functions p : X →[0, 1] such that p(x) ̸= 0 for only ﬁnitely many elements x ∈X, and
P
x:X p(x) = 1. We can think of DX as a (convex) vector space, with basis vectors |x⟩given by the elements
x of X. We can then write a (ﬁnitely-supported) distribution p : X →[0, 1] as a convex (weighted) sum of
these basis vectors P
x:X
p(x) |x⟩, where the expression inside the box evaluates to a probability.
Channels in Kℓ(D): stochastic matrices
The objects of Kℓ(D) are sets, and morphisms X→
• Y are func-
tions X →DY ; equivalently, using the Cartesian-closed structure of Set, they are functions X ×Y →[0, 1],
i.e. (left stochastic) matrices of size |X| × |Y |, each of whose columns sums to 1. We think of morphisms
as stochastic channels emitting outputs stochastically for each input, with the stochasticity encoded in the
output states. We adopt ‘conditional probability’ notation: given p : X→
• Y , x ∈X and y ∈Y , we write
p(y|x) := p(x)(y) ∈[0, 1] for “the probabilty of y given x, according to p”.
Identity morphisms idX : X→
• X in Kℓ(D) take points to ‘Dirac distributions’: idX := x 7→1 |x⟩; these
are the unit maps ηX of the monad structure on D. Note that any function f : Y →X can be made into a
(deterministic) channel ⟨f⟩= ηX ◦f : Y →DX by post-composition with ηX.
Given p : X →DY and q : Y →DZ, we write their (sequential) composite as q • p : X →DZ,
constructed by ‘averaging over’ or ‘marginalizing out’ Y via the Chapman-Kolmogorov equation:
q • p : X →DZ := x 7→
X
z:Z
X
y:Y
q(z|y) · p(y|x) |z⟩.
Note that this is just the (matrix) product of the stochastic matrices corresponding to the channels q and p.
Abstractly, the composite is formed as the composite q⊲◦p in Set of p followed by the Kleisli extension q⊲:
DY →DZ of q. Kleisli extension (−)⊲turns any morphism q : Y →DZ into a morphism q⊲: DY →DZ,
and in Kℓ(D), it is deﬁned using marginalization as follows:
q⊲: DY →DZ := ρ 7→
X
z:Z
X
y:Y
q(z|y) · ρ(y) |z⟩.
(4)
5

Monoidal structure: joint states and parallel channels
D is a monoidal monad, meaning that there is
a family of maps DX × DY →D(X × Y ), natural in X and Y , which take a pair of distributions (ρ, σ)
in DX × DY to the joint distribution on X × Y given by (x, y) 7→ρ(x) · σ(y); ρ and σ are then the
(independent) marginals of this joint distribution. This structure makes Kℓ(D) into a monoidal category,
with a tensor product functor ⊗: Kℓ(D) × Kℓ(D) →Kℓ(D). This functor is deﬁned on pairs of objects X
and Y as their product X ⊗Y = X × Y , and on stochastic maps f : X →DA and g : Y →DB as the
‘parallel composite’ f ⊗g : X × Y →D(A × B) via the monoidal structure of D:
X × Y
f×g
−−→DA × DB →D(A × B) .
Note that because not all joint states have independent marginals, the monoidal product ⊗is not Cartesian:
that is, given an arbitrary ω : D(X ⊗Y ), we do not have ω ∼= (ρ, σ) for some ρ : DX and σ : DY .
As indicated in §2.1, Kℓ(D) is symmetric monoidal: since X × Y ∼= Y × X, there are natural ‘swap’
isomorphisms swapX,Y : X ⊗Y ∼
−→
• Y ⊗X and swapY,X : Y ⊗X ∼
−→
• X ⊗Y such that swapY,X • swapX,Y =
idX⊗Y .
The tensor product ⊗is equipped with a unit object I, which means that, for all objects X, there are natural
isomorphisms λX : I ⊗X ∼
−→
• X and ρX : X ⊗I ∼
−→
• X called the left and right unitors. Note that, when Set
is equipped with the Cartesian product ×, the monoidal unit I is the singleton set 1 = {∗}, and we have
1 × X
λ∼= X
ρ∼= 1 × X. Since ⊗on Kℓ(D) derives from × on Set, we also have I = 1 in Kℓ(D). Finally, note
that states π : DX correspond isomorphically to functions π : 1 →DX, and hence channels π : I→
• X.
Marginalization: discarding, causality, and projections
Given a joint distribution ω : 1 →D(X ×Y ),
we can recover each marginal ω1 : 1 →DX or ω2 : 1 →DY by marginalizing out the other. Categorically,
this is captured by the existence of discarding maps
X : X →D1 ∼= 1 := x 7→1 |∗⟩. From the discarding
maps, we can construct projection maps for the tensor product; these witness marginalization:
π1 : X × Y →DX := X × Y
id ×
−−−−→DX × 1 ∼= DX
and
π2 : X × Y →DY := X × Y
×id
−−−→1 × DY ∼= DY ,
which are natural in that πi • (f1 ⊗f2) = fi • πi. Explicitly, using the deﬁnitions of id and
given above,
we have π1(x, y) = 1 |x⟩and π2(x, y) = 1 |y⟩; and so, given some joint distribution ω : 1 →D(X × Y ),
ω1 = π1 • ω = P
y:Y ω(x, y) |x⟩, and similarly, ω2 = π2 • ω = P
x:X ω(x, y) |y⟩.
We say that a stochastic map f is causal if doing f then throwing away the output is the same as just
throwing away the input:
• f =
; this means that f cannot aﬀect states ‘in its past’. In Kℓ(D), every map
is causal (the discarding maps are natural), but this will not be true in all the categories of interest to us in
this paper.
Copying
In order to deﬁne lens composition, we need one more piece of structure: a family of copying
maps, denoted
. In Kℓ(D), these are the maps
X : X →D(X × X) := x 7→1 |x, x⟩. Together with the
discarding maps
X, they make every object X into a commutative comonoid; this will be elaborated further
in §2.1.2. Note that the copying maps are not natural in Kℓ(D): in general,
• f ̸= f ⊗f •
. Those maps
f that do satisfy this equality are comonoid homomorphisms, and in Kℓ(D) correspond to the deterministic
maps (i.e. those that emit Dirac delta distributions).
6

Bayesian updating
We can now instantiate Bayesian updating in Kℓ(D). Given a channel p : X →DY
and a prior ρ : 1 →DX, the Bayesian update of ρ along p is given by the function
p†
(·) : DX × Y →DX := ρ × y 7→
X
x:X
p(y|x) · ρ(x)
P
x′:X p(y|x′) · ρ(x′) |x⟩=
X
x:X
p(y|x) · ρ(x)
[p • ρ](y)
|x⟩.
(5)
The expression on the right-hand side is easily seen to correspond to the informal expression of Bayes’ rule
in equation (3).
2.1.2. Graphical calculus
We now move from Kℓ(D) to a more general setting. We will assume that, in each category C of stochas-
tic channels of interest to us, we are able to form parallel channels and coherently copy and delete states,
analogously to the discrete case in §2.1.1. This means that C must be a copy-delete category [7].
Deﬁnition 2.2 (Cho and Jacobs [7, Def. 2.2]). A copy-delete category is a symmetric monoidal category
(C, ⊗, I) in which every object X is supplied with a commutative comonoid structure (
X,
X) compatible
with the monoidal structure of (⊗, I). An aﬃne copy-delete category, or Markov category [8], is a copy-
delete category in which every channel c is causal in the sense that
• c =
. Equivalently, a Markov
category is a copy-delete category in which the monoidal unit I is the terminal object.
Symmetric monoidal categories, and (co)monoids within them, admit a formal graphical calculus that sub-
stantially simpliﬁes many calculations involving complex morphisms: proofs of many equalities reduce to
visual demonstrations of isotopy, and structural morphisms such as the symmetry of the monoidal product
acquire intuitive topological depictions. We make substantial use of this calculus below, and summarize its
features here. For more details, see Cho and Jacobs [7, §2] or Fritz [8, §2] or the references cited therein.
Basic structure
Diagrams in the graphical calculus represent morphisms. We draw morphisms as boxes on
strings, labelling the strings with the corresponding objects in the category. Identity morphisms are drawn as
plain strings. Sequential composition is represented by connecting strings together; and parallel composition
⊗by placing diagrams adjacent to one another.
Diagrams for C will be read vertically, with information ﬂowing upwards (from bottom to top). This way,
c : X→
• Y , idX : X→
• X, d • c : X c−→
• Y d−→
• Z, and f ⊗g : X ⊗Y →
• A ⊗B are depicted respectively as:
c
X
Y
X
X
d
Z
c
X
f
X
A
g
Y
B
We represent (the identity morphism on) the monoidal unit I as an empty diagram: that is, we leave it implicit
in the graphical representation.
States and eﬀects
In Kℓ(D) we saw that a channel I→
• X was a ﬁnitely supported distribution over X. In
general, we will call such a morphism I→
• X a state of X. Dually, a morphism X→
• I in C is called an eﬀect.
7

States σ : I→
• X and eﬀects η : X→
• I will be represented as follows:
σ
X
η
X
Discarding, causality, marginalization and projections
As noted in §2.1.1, in Kℓ(D) there is only one
possible eﬀect of each type X, given by the discarding map
X : X→
• I. This uniqueness follows categorically
from the fact that the object I = 1 is the terminal object in Kℓ(D) — meaning that there is a unique map from
every object into I — and is equivalent to the condition that every channel c : X→
• Y is causal:
=
c
From the discarding maps, we constructed projections in Kℓ(D) witnessing the marginalization of joint
states. This has a pleasing graphical representation. Suppose a joint state ω : I→
• X ⊗Y has marginals
ω1 : I→
• X and ω2 : I→
• Y . Then
ω
=
ω1
X
X
and
ω
=
ω2
Y
Y
.
Copying
The copying maps
X : X→
• X ⊗X have a similarly intuitive graphical representation. They
are required to interact nicely with the discarding maps, making each object X into a comonoid (satisfying
unitality and associativity):
=
=
and
=
(6)
A category with such comonoid structure (
X,
X) for every object X is said to supply comonoids [9].
We will draw the swap isomorphisms of the symmetric monoidal structure as the swapping of wires,
and assume that the copying maps commute with this swapping, making the comonoids into commutative
comonoids:
=
and
=
(7)
Conditional probability
We end this summary with a graphical statement of the law of conditional prob-
ability (1). Suppose as before that A ⊆X and B ⊆Y , with ω : 1→
• X ⊗Y , c : X→
• Y , and π : 1→
• X. The
8

disintegration Pω(A, B) = Pc(B|A) · Pπ(A) then takes the graphical form
c
π
X
Y
ω
X
Y
=
with the marginals π and c • π of ω given by
c
π
X
ω
X
=
=
π
X
and
c
π
Y
ω
Y
=
=
π
Y
c
.
2.1.3. Abstract Bayesian inversion
Bayesian inversion informally satisﬁes the equation Pc(B|A) · Pπ(A) = Pc†
π(A|B) · Pc•π(B) (2). Given the
structures introduced above, we can formalize this rule, depicting it as the following graphical equality [7, eq.
5]:
c
π
X
Y
=
c†
π
π
c
X
Y
(8)
This diagram can be interpreted as follows. Given a prior π : I→
• X and a channel c : X→
• Y , we form the joint
distribution ω := (idX ⊗c) •
X • π : I→
• X ⊗Y shown on the left hand side: this is the product rule form,
Pω(A, B) = Pc(B|A) · Pπ(A), and π is the corresponding X-marginal. As in the concrete case of Kℓ(D),
we seek an inverse channel Y →
• X witnessing the ‘dual’ form of the rule, Pω(A, B) = P(A|B) · P(B); this
is depicted on the right hand side. By discarding X, we see that c • π : I→
• Y is the Y -marginal witnessing
P(B). So any channel c†
π : Y →
• X witnessing P(A|B) and satisfying the equality above is a Bayesian inverse
of c with respect to π.
Deﬁnition 2.3. We say that a channel c : X→
• Y admits Bayesian inversion with respect to π : I→
• X if
there exists a channel c†
π : Y →
• X satisfying equation (8). We say that c admits Bayesian inversion tout court
if c admits Bayesian inversion with respect to all states π : I→
• X such that c • π has non-empty support.
2.1.4. Density functions
Abstract Bayesian inversion (8) generalizes the product rule form of Bayes’ theorem (2) but in most applica-
tions, we are interested in a speciﬁc channel witnessing P(A|B) = P(B|A) · P(A)/ P(B). In the common
9

setting of continuous spaces, this is often written informally as
p(x|y) = p(y|x) · p(x)
p(y)
=
p(y|x) · p(x)
R
x′:X p(y|x′) · p(x′) dx′
(9)
but the formal semantics of such an expression are not trivial: for instance, what is the object p(y|x), and
how does it relate to a channel c : X→
• Y ? Moreover, it is not generally true that, given a channel c : X→
• Y
and prior π : I→
• X, a Bayesian inversion c†
π : Y →
• X necessarily exists [10]!
We can interpret p(y|x) as a density function for a channel: an eﬀect X ⊗Y →
• I in our ambient category
C. Consequently, C cannot be semicartesian (i.e., C cannot be an aﬃne copy-delete category)—as this would
trivialize all density functions—though it must still supply comonoids. We can think of this as expanding the
collection of channels in the category to include acausal or ‘partial’ maps and unnormalized distributions or
states. An example of such a category is Kℓ(D≤1), whose objects are sets (as for Kℓ(D)), and whose morphisms
X→
• Y are functions X →D(Y + 1), where Y + 1 is the disjoint union of Y with 1 = {∗}. Then a stochastic
map is partial if it sends any probability to the added element ∗. The subcategory of ‘total’ (equivalently,
causal) maps is Kℓ(D) [11].
Deﬁnition 2.4 (Density functions). A channel c : X→
• Y is said to be represented by an eﬀect p : X⊗Y →
• I
with respect to µ : I→
• Y if
c
X
Y
µ
p
X
Y
=
.
In this case, we call p a density function for c.
We will also need the concepts of almost-equality and almost-invertibility.
Deﬁnition 2.5 (Almost-equality, almost-invertibility). Given a state π : I→
• X, we say that two channels
c : X→
• Y and d : X→
• Y are π-almost-equal, denoted c π∼d, if
c
π
X
Y
∼=
d
π
X
Y
10

and we say that an eﬀect p : X→
• I is π-almost-invertible with π-almost-inverse q : X→
• I if
π∼
q
p
X
X
.
Proposition 2.6 (Composition preserves almost-equality). If c π∼d, then f • c π∼f • d.
Proof. Immediate from the deﬁnition of almost-equality.
Proposition 2.7 (Almost-inverses are almost-equal). Suppose q : X→
• I and r : X→
• I are both π-almost-inverses
for the eﬀect p : X→
• I. Then q π∼r.
Proof. Deferred to Appendix §A.1.
With these notions, we can characterise Bayesian inversion via density functions.
Proposition 2.8 (Bayesian inversion via density functions; Cho and Jacobs [7]). Suppose c : X→
• Y is rep-
resented by the eﬀect p with respect to µ. The Bayesian inverse c†
π : Y →
• X of c with respect to π : I→
• X is
given by
p
π
p−1
X
Y
where p−1 : Y →
• I is a µ-almost-inverse for the eﬀect
p
π
Y
Proof. Deferred to Appendix §A.2.
The following proposition is an immediate consequence of the deﬁnition of almost-equality and of the
abstract characterisation of Bayesian inversion (8). We omit the proof.
Proposition 2.9 (Bayesian inverses are almost-equal). Suppose α : Y →
• X and β : Y →
• X are both Bayesian
inversions of the channel c : X→
• Y with respect to π : I→
• X. Then α c•π
∼β.
We will also need the following two technical results about almost-equality.
11

Lemma 2.10. Suppose the channels α and β satisfy the following relations for some f, q, r:
α
q
f
∼=
and
β
r
f
∼=
Suppose q
µ∼r. Then α
µ∼β.
Proof. Deferred to Appendix §A.3.
Lemma 2.11. If the channel d is represented by an eﬀect with respect to the state ν, and if f ν∼g, then f
d•ρ
∼g
for any state ρ on the domain of d.
Proof. Deferred to Appendix §A.4.
2.1.5. S-finite kernels
To represent channels by concrete eﬀects (i.e., density functions), we work in the category sfKrn of mea-
surable spaces and s-ﬁnite kernels. Once again, we only sketch the structure of this category, and refer the
reader to Cho and Jacobs [7] and Staton [12] for elaboration.
Objects in sfKrn are measurable spaces (X, ΣX); often we will just write X, and leave the σ-algebra ΣX
implicit. Morphisms (X, ΣX)→
• (Y, ΣY ) are s-ﬁnite kernels. A kernel k from X to Y is a function k : X ×
ΣY →[0, ∞] satisfying the following conditions:
• for all x ∈X, k(x, −) : ΣY →[0, ∞] is a measure; and
• for all B ∈ΣY , k(−, B) : X →[0, ∞] is measurable.
A kernel k : X × ΣY →[0, ∞] is ﬁnite if there exists some r ∈[0, ∞) such that, for all x ∈X, k(x, Y ) ≤r.
And k is s-ﬁnite if it is the sum of at most countably many ﬁnite kernels kn, k = P
n:N kn.
Identity morphisms idX : X→
• X are Dirac kernels δX : X × ΣX →[0, ∞] := x × A 7→1 iﬀx ∈A and 0
otherwise. Composition is given by a Chapman-Kolmogorov equation, analogously to composition in Kℓ(D).
Suppose c : X→
• Y and d : Y →
• Z. Then
d • c : X × ΣZ →[0, ∞] := x × C 7→
Z
y:Y
d(C|y) c(dy|x)
where we have again used the ‘conditional probability’ notation d(C|y) := d◦(y ×C). Reading d(C|y) from
left to right, we can think of this notation as akin to reading the string diagrams from top to bottom, i.e. from
output(s) to input(s).
Monoidal structure on sfKrn
There is a monoidal structure on sfKrn analogous to that on Kℓ(D). On
objects, X ⊗Y is the Cartesian product X ×Y of measurable spaces. On morphisms, f ⊗g : X ⊗Y →
• A⊗B
is given by
f ⊗g : (X × Y ) × ΣA×B := (x × y) × E 7→
Z
a:A
Z
b:B
δA⊗B(E|x, y) f(da|x) g(db|y)
12

where, as above, δA⊗B(E|a, b) = 1 iﬀ(a, b) ∈E and 0 otherwise. Note that (f⊗g)(E|x, y) = (g⊗f)(E|y, x)
for all s-ﬁnite kernels (and all E, x and y), by the Fubini-Tonelli theorem for s-ﬁnite measures [7, 12], and so
⊗is symmetric on sfKrn.
The monoidal unit in sfKrn is again I = 1, the singleton set. Unlike in Kℓ(D), however, we do have
nontrivial eﬀects p : X→
• I, given by kernels p : (X × Σ1) ∼= X →[0, ∞], with which we will represent
density functions.
Comonoids in sfKrn
sfKrn also supplies comonoids, again analogous to those in Kℓ(D). Discarding
is given by the family of eﬀects
X : X →[0, ∞] := x 7→1, and copying is again Dirac-like:
X :
X × ΣX×X := x × E 7→1 iﬀ(x, x) ∈E and 0 otherwise. Because we have nontrivial eﬀects, discarding
is only natural for causal or ‘total’ channels: if c satisﬁes
• c =
, then c(−|x) is a probability measure
for all x in the domain1. And, once again, copying is natural (that is,
• c = (c ⊗c) •
) iﬀthe channel is
deterministic.
Channels represented by eﬀects
We can interpret the string diagrams of §2.1.2 in sfKrn, and we will
do so by following the intuition of the conditional probability notation and reading the string diagrams from
outputs to inputs. Hence, if c : X→
• Y is represented by the eﬀect p : X ⊗Y →
• I with respect to the measure
µ : I→
• Y , then
c : X × ΣY →[0, ∞] := x × B 7→
Z
y:B
µ(dy) p(y|x).
Note that we also use conditional probability notation for density functions, and so p(y|x) := p ◦(x × y).
Suppose that c : X→
• Y is indeed represented by p with respect to µ, and that d : Y →
• Z is represented by
q : Y ⊗Z→
• I with respect to ν : I→
• Z. Then in sfKrn, d • c : X→
• Z is given by
d • c : X × ΣZ := x × C 7→
Z
z:C
ν(dz)
Z
y:Y
q(z|y) µ(dy) p(y|x)
Alternatively, by deﬁning the eﬀect (pµq) : X ⊗Z→
• I as
(pµq) : X × Z →[0, ∞] := x × z 7→
Z
y:Y
q(z|y) µ(dy) p(y|x),
we can write d • c as
d • c : X × ΣZ := x × C 7→
Z
z:C
ν(dz) (pµq)(z|x).
Bayesian inversion via density functions
Once again writing π : I→
• X for a prior on X, and interpreting
the string diagram of Proposition 2.8 for c†
π : Y →
• X in sfKrn, we have
c†
π : Y × ΣX →[0, ∞] := y × A 7→
Z
x:A
π(dx) p(y|x)

p−1(y)
= p−1(y)
Z
x:A
p(y|x) π(dx),
(10)
where p−1 : Y →
• I is a µ-almost-inverse for eﬀect p • (π ⊗idY ), and is given up to µ-almost-equality by
p−1 : Y →[0, ∞] := y 7→
Z
x:X
p(y|x) π(dx)
−1
.
1This means that Kℓ(G) is the subcategory of total maps in sfKrn, where G is the Giry monad taking each measurable space X to
the space GX of measures over X.
13

Note that from this we recover the informal form of Bayes’ rule for measurable spaces (9). Suppose π is itself
represented by a density function pπ with respect to the Lebesgue measure dx. Then
c†
π(A|y) =
Z
x:A
p(y|x) pπ(x)
R
x′:X p(y|x′) pπ(x′) dx′ dx.
2.2. Optics
In §2.1 we noted that, given a channel c : X→
• Y , its Bayesian inversion is of the form c†
(·) : C(I, X) →
C(Y, X), where C(I, X) is a space of states on X. This is not a map in Kℓ(D), for instance, because there is
in general no space Z such that Kℓ(D)(Y, X) ∼= DZ; and nor do we obtain a map in Kℓ(D) if we attempt to
‘uncurry’ c†
(·) into the form DX ⊗Y →DX 2. So, unlike in the case of Cartesian lenses, our forwards and
backwards morphisms do not live in the same category, yet somehow they still interact and behave similarly:
we need mixed optics.
Mixed or profunctor optics [1, 2, 13] allow the forwards and backwards morphisms of bidirectional transfor-
mations such as lenses to live in arbitrary (possibly diﬀerent) categories C and D, with interaction mediated
by an arbitrary third category M of ‘residuals’. The objects of M can be somehow tensored with the objects
of C and D, giving new C and D objects that behave like the original objects plus “some other stuﬀ”; through
this tensoring, we say that M acts on C and D, and C and D are M-actegories. For example, recall that the
view map of a Cartesian lens takes a structure and returns a part of it; the residual (the “other stuﬀ”) in this
case is just the rest of the record, and Set is acting on itself.
Henceforth, rather than work in the setting of locally small categories enriched in Set, we will work in the
somewhat more general setting of enrichment in an arbitrary cocomplete Cartesian closed category V. We
write V-Cat for the category of V-enriched categories, so that V-Cat(C, D) is the V-category of V-functors
between V-categories. Since V is assumed to be Cartesian, we write × for the categorical product both in V
and the induced product in V-Cat.
Deﬁnition 2.12 (M-actegory). Suppose M is a monoidal category with tensor ⊗and unit object I. We
say that C is an M-actegory when C is equipped with a functor ⊙: M →V-Cat(C, C) called the action
along with natural unitor and associator isomorphisms λ⊙
X : I ⊙X
∼
−→X and a⊙
M,N,X : (M ⊗N) ⊙X
∼
−→
M ⊙(N ⊙X) compatible with the monoidal structure of (M, ⊗, I).
Deﬁnition 2.13 (Mixed optics [2]). Suppose (C, L ) and (D, R ) are two M-actegories. Let X, Y : C and
A, B : D. An optic from (X, A) to (Y, B), written (X, A) 7→(Y, B), is an element of the following object in
V:
Optic L , R

(X, A), (Y, B)

=
Z M : M
C(X, M
L Y ) × D(M
R B, A)
(11)
The ‘integral’ here is not an integral but a coend: a kind of generalized sum or existential quantiﬁer; see
Loregian [14, Example 5.4] or Fong and Spivak [4, Chapter 4] for some background to this intuition. The
coend ranges over objects M : M, binding pairs of morphisms X →M
L Y in C and M
R B →A in
D into equivalence classes along the residuals M. Let v : C(X, M
L Y ), u : D(N
R B, A). Then, for any
f : M(M, N), we have two pairs of morphisms
⟨v | u ◦(f
R idB)⟩:=
 v, u ◦(f
R idB)

: C(X, M
L Y ) × D(M
R B, A)
2Not only is Kℓ(D) not categorically closed, but c†
(·) is not linear in the prior: the Bayesian inversion of c with respect to 0.5π+0.5ρ
is not 0.5c†
π +0.5c†
ρ; such linearity characterizes maps in Kℓ(D). Alternatively, c† is not generally a morphism in sfKrn, because
there may be some prior π such that (c • π)(y) = 0, which would make the required almost-inverse undeﬁned, so that c† is not
the sum of at most countably many ﬁnite kernels.
14

and
⟨(f
L idY ) ◦v | u⟩:=
 (f
L idY ) ◦v, u

: C(X, N
L Y ) × D(N
R B, A) .
We give a recap of the deﬁnition of coend in §B. In brief, the coend equivalence relation says precisely that
two such pairs are equivalent, and so we call ⟨v | u ◦(f
R idB)⟩and ⟨(f
L idY ) ◦v | u⟩representatives of their
equivalence class. We adopt the notation ⟨l | r⟩to indicate the element of the coend (i.e., the equivalence class)
represented by the pair (l, r).
Apart from providing a uniﬁed compositional framework for describing bidirectional transformations, op-
tics admit an intuitive graphical calculus [15, 16]. A general optic ⟨l | r⟩: (X, A)
7→(Y, B) is depicted3
as
l
r
X
A
B
Y
M
C
M
D
where the top region of the diagram represents C, the middle region M, and the bottom region D. Information
ﬂows from left to right in the top region, and right to left in the bottom, and M mediates interaction between
C and D. We can depict the equivalent representatives ⟨v | u ◦(f
R idB)⟩∼⟨(f
L idY ) ◦v | u⟩accordingly
as
v
u
X
A
B
Y
N
M
f
C
M
D
∼
v
u
X
A
N
M
f
Y
B
C
M
D
which indicates that two pairs of morphisms are equivalent under the coend when there is some f that can
‘slide between’ residual types.
As these diagrams suggest, optics for L and R form a category: composition is by pasting of diagrams, and
identities are plain wires.
3For these diagrams we adopt the graphical calculus of Boisseau [15] of the bicategory of Tambara modules, which are presheaves
of optics. 0-cells are actegories, depicted as planar regions. 1-cells are Tambara modules, depicted as edges of regions (i.e., strings).
2-cells are natural transformations, depicted as vertices on edges (i.e., boxes on strings). For our purposes, these 2-cells will always
be morphisms in an underlying actegory, lifted by the Yoneda embedding. The graphical calculus described by Román [16] is more
ﬂexible, representing the monoidal bicategory of pointed profunctors without the extra Tambara module structure, but here we
follow Boisseau [15] for simplicity.
15

Proposition 2.14 (Category of optics [13, p. 3.1.1]). Given M-actegories, (C, L ) and (D, R ), there is a cat-
egory of optics Optic L , R whose objects are pairs of objects (X, A) : (C × D)0 and whose morphisms
(X, A)
7→(Y, B) are elements of Optic L , R

(X, A), (Y, B)

as deﬁned in (11). The (representative of)
the composition of two optics is as depicted in the following diagram. Let ⟨v | u⟩: (X, A)
7→(Y, B) and
⟨l | r⟩: (Y, B) 7→(Z, C). Then ⟨l | r⟩◦| ⟨v | u⟩: (X, A) 7→(Z, C) ∼=
v
u
X
A
M
l
r
Y
B
C
Z
N
∼=
v
u
X
A
B
Y
l
r
C
Z
N
M
∼=
(idM
L l) ◦v
u ◦(idM
R r)
X
A
C
Z
M ⊗N
∼=
D
a
L
M,N,Y
−1 ◦(idM
L l) ◦v
 u ◦(idM
R r) ◦a
R
M,N,Y
E
.
(12)
Identity optics id(X,A) : (X, A)
7→(X, A) are given by the unitors of the actegory structures: id(X,A) =
D
λ
L
X
−1  λ
R
A
E
, depicted as plain wires in an otherwise empty box:
X
A
A
X
16

2.2.1. Lenses
A Cartesian lens as introduced in §1 is a pair of functions X →Y and X × B →A; that is, an element of the
product Set(X, Y ) × Set(X × B, A). We can write this in optical form:
Set(X, Y ) × Set(X × B, A) ∼=
Z M : Set
Set(X, Y ) × Set(X, M) × Set(M × B, A)
(13)
∼=
Z M : Set
Set(X, M × Y ) × Set(M × B, A)
(14)
∼= Optic×,×

(X, A), (Y, B)

where the ﬁrst isomorphism obtains by Yoneda reduction (27) and the second by the universal property of
the categorical product × : Set →Cat(Set, Set).
The universal property of the Cartesian product that justiﬁes (14) ∼
−→(13) entails that Set supplies comonoids
and every morphism in Set is a comonoid homomorphism: i.e.,
◦f = f ⊗f ◦
, where
: x 7→(x, x)
is the diagonal copier in Set. When either of the M-actegories underlying a category of optics is equivalent
to M itself, we can lift string diagrams in that actegory directly into the string diagrams for those optics [15,
Note 3.7]. In particular, this includes the depictions of comonoids introduced in §2.1.2. We can thus depict
any Cartesian lens as
v
u
X
A
B
Y
X
(15)
where v is called view and u is called update. We can deﬁne a general lens to be any optic that is isomorphic
to such a depiction.
Deﬁnition 2.15 (After Clarke et al. [2, §3.1]). A lens is any optic that can be depicted as in (15). Equivalently,
suppose (C, ⊗) is a symmetric monoidal category and write Comon(C) for its subcategory of comonoids
and comonoid homomorphisms. ⊗lifts to Comon(C) and induces a corresponding Comon(C)-actegory
structure on Comon(C). Suppose also that (D, R ) is any Comon(C)-actegory. Then a lens is any optic in
Optic(⊗, R ). Note that
Optic⊗, R

(X, A), (Y, B)

∼=
Z M : Comon(C)
Comon(C)(X, M ⊗Y ) × D(M
R B, A)
∼=
Z M : Comon(C)
Comon(C)(X, Y ) × Comon(C)(X, M) × D(M
R B, A)
∼= Comon(C)(X, Y ) × D(X
R B, A)
where the second isomorphism follows because
◦f ∼= f ⊗f ◦
for every morphism f in Comon(C)
and the third follows by Yoneda reduction (27). Every such optic therefore has a representative as depicted in
(15).
17

In the sequel, we will see that Bayesian inversions constitute the ‘backwards’ components of a particular
category of lenses.
3. Channels relative to a state
The Bayesian inversion of a ‘forward’ channel is deﬁned with respect to a prior state on the domain of the
forward channel. Changes in the prior entail changes in the inversions – but “changes in the prior” are just
channels in the forwards direction, and the “changes in the inversions” correspond to pulling inversions back
along corresponding forward channels. Formally, this means that the backward channels are ﬁbred over the
forward channels: for each domain in the ‘base category’ of forward channels, we have a category of channels
with respect to that domain, and forward channels correspond to contravariant functors between the ﬁbres
that implement the aforesaid pulling-back. This is an instance of the Grothendieck construction [17], making
Bayesian lenses an instance of Grothendieck lenses [6]. In this section, we make these ideas precise; in the
next, we translate them into the optical vernacular introduced in §2.2.
Deﬁnition 3.1 (State-indexed categories). Let (C, ⊗, I) be a monoidal category enriched in a Cartesian closed
category V. Deﬁne the C-state-indexed category Stat : C op →V-Cat as follows.
Stat
:
C op →V-Cat
X 7→Stat(X) :=





Stat(X)0
:=
C0
Stat(X)(A, B)
:=
V(C(I, X), C(A, B))
idA :
Stat(x)(A, A)
:=
(
idA : C(I, X) →C(A, A)
ρ
7→
idA





(16)
f : C(Y, X) 7→







Stat(f) :
Stat(X)
→
Stat(Y )
Stat(X)0
=
Stat(Y )0
V(C(I, X), C(A, B))
→
V(C(I, Y ), C(A, B))
α
7→
f ∗α :
 σ : C(I, Y )

7→
 α(f • σ) : C(A, B)








Composition in each ﬁbre Stat(X) is given by composition in C; that is, by the left and right actions of the pro-
functor Stat(X)(−, =) : C op ×C →V (§B supplies some intuition). Explicitly, given α : V(C(I, X), C(A, B))
and β : V(C(I, X), C(B, C)), their composite is β ◦α : V(C(I, X), C(A, C)) := ρ 7→β(ρ) • α(ρ). Since
V is Cartesian, there is a canonical copier
: x 7→(x, x) on each object, so we can alternatively write
(β ◦α)(ρ) =
 β(−) • α(−)

◦
◦ρ. Note that we indicate composition in C by • and composition in the
ﬁbres Stat(X) by ◦.
Example 3.2. Let V = Meas be a ‘convenient’ (i.e., Cartesian closed) category of measurable spaces, such
as the category of quasi-Borel spaces [18], let P : Meas →Meas be a probability monad deﬁned on this
category, and let C = Kℓ(P) be the Kleisli category of this monad. Its objects are the objects of Meas, and
its hom-spaces Kℓ(P)(A, B) are the spaces Meas(A, PB) [8]. This C is a monoidal category of stochastic
channels, whose monoidal unit I is the space with a single point. Consequently, states of X are just measures
18

(distributions) in PX. That is, Kℓ(P)(I, X) ∼= Meas(1, PX). Instantiating Stat in this setting, we obtain:
Stat
:
Kℓ(P) op →V-Cat
X 7→Stat(X) :=





Stat(X)0
:=
Meas0
Stat(X)(A, B)
:=
Meas(PX, Meas(A, PB))
idA :
Stat(X)(A, A)
:=
(
idA : PX →Meas(A, PA)
ρ
7→
ηA





(17)
c : Kℓ(P)(Y, X) 7→







Stat(c) :
Stat(X)
→
Stat(Y )
Stat(X)0
=
Stat(Y )0
 
d† :
PX
→Kℓ(P)(A, B)
π
7→
d†
π
!
7→
 
c∗d† : PY →Kℓ(P)(A, B)
ρ
7→
d†
c•ρ
!







Each Stat(X) is a category of stochastic channels with respect to measures on the space X. We can write
morphisms d† : PX →Kℓ(P)(A, B) in Stat(X) as d†
(·) : A
(·)
−→
• B, and think of them as generalized Bayesian
inversions: given a measure π on X, we obtain a channel d†
π : A π−→
• B with respect to π. Given a channel
c : Y →
• X in the base category of priors, we can pull d† back along c, to obtain a Y -dependent channel in
Stat(Y ), c∗d† : PY →Kℓ(P)(A, B), which takes ρ : PY to the channel d†
c•ρ : A
c•ρ
−−→
• B deﬁned by pushing
ρ through c and then applying d†.
Remark 3.3. Note that by taking Meas to be Cartesian closed, we have Meas(PX, Meas(A, PB)) ∼=
Meas(PX × A, PB) for each X, A and B, and so a morphism c† : PY →Kℓ(P)(X, Y ) equivalently has
the type PY × X →PY . Paired with a channel c : Y →PX, we have something like a Cartesian lens; and
to compose such pairs, we can use the Grothendieck construction [6, 17].
Deﬁnition 3.4 (Grothendieck lenses [6]). We deﬁne the category GrLensF of Grothendieck lenses for a
(pseudo)functor F : C op →V-Cat to be the total category of the Grothendieck construction for the point-
wise opposite of F. Explicitly, its objects (GrLensF )0 are pairs (C, X) of objects C in C and X in F(C),
and its hom-sets GrLensF
 (C, X), (C′, X′)

are given by dependent sums
GrLensF
 (C, X), (C′, X′)

=
X
f : C(C,C′)
F(C)
 F(f)(X′), X

(18)
so that a morphism (C, X) 7→(C′, X′) is a pair (f, f †) of f : C(C, C′) and f † : F(C)
 F(f)(X′), X

. We
call such pairs Grothendieck lenses for F or F-lenses.
Proposition 3.5 (GrLensF is a category). The identity Grothendieck lens on (C, X) is id(C,X) = (idC, idX).
Sequential composition is as follows. Given (f, f †) : (C, X) 7→(C′, X′) and (g, g†) : (C′, X′) 7→(D, Y ),
their composite (g, g†) ◦| (f, f †) is deﬁned to be the lens
 g • f, F(f)(g†)

: (C, X) 7→(D, Y ). Associativity
and unitality of composition follow from functoriality of F.
Example 3.6 (GrLensStat). Instantiating GrLensF with F = Stat : C op →V-Cat, we obtain the cate-
gory GrLensStat whose objects are pairs (X, A) of objects of C and whose morphisms (X, A) 7→(Y, B) are
elements of the set
GrLensStat
 (X, A), (Y, B)
 ∼= C(X, Y ) × V
 C(I, X), C(B, A)

.
(19)
The identity Stat-lens on (Y, A) is (idY , idA), where by abuse of notation idA : C(I, Y ) →C(A, A) is the
constant map idA deﬁned in (16) that takes any state on Y to the identity on A. The sequential composite of
19

(c, c†) : (X, A) 7→(Y, B) and (d, d†) : (Y, B) 7→(Z, C) is the Stat-lens
 (d • c), (c† ◦c∗d†)

: (X, A) 7→
(Z, C) with (d • c) : C(X, Z) and where (c† ◦c∗d†) : V
 C(I, X), C(C, A)

takes a state π : C(I, X) on X
to the channel c†
π • d†
c•π. If we think of the notation (·)† as denoting the operation of forming the Bayesian
inverse of a channel (in the case where A = X, B = Y and C = Z), then the main result of this paper is to
show that (d • c)†
π
d•c•π
∼
c†
π • d†
c•π, where d•c•π
∼
denotes (d • c • π)-almost-equality (Deﬁnition 2.5).
4. Bayesian lenses
We now show how to translate the categories of Grothendieck Stat-lenses deﬁned above into the canonical
profunctor optic form, thereby opening Bayesian lenses up to comparison and composition with other optics,
and representation in the corresponding graphical calculi.
In order to give an optical form for GrLensStat, we need to ﬁnd two M-actegories with a common category
of actions M. Let ˆC and ˇC denote the categories ˆC := V-Cat(C op, V) and ˇC := V-Cat(C, V) of presheaves
and copresheaves on C, and consider the following natural isomorphisms.
GrLensStat
 (X, A), (Y, B)
 ∼= C(X, Y ) × V
 C(I, X), C(B, A)

∼=
Z M : C
C(X, Y ) × C(X, M) × V
 C(I, M), C(B, A)

∼=
Z
ˆ
M : ˆC
C(X, Y ) × ˆ
M(X) × V
  ˆ
M(I), C(B, A)

(20)
The second isomorphism follows by Yoneda reduction (27), and the third follows by the Yoneda lemma. We
take M to be M := ˆC, and deﬁne an action ⊙of ˆC on ˇC as follows.
Deﬁnition 4.1 (⊙). We give only the action on objects; the action on morphisms is analogous.
⊙: ˆC →V-Cat( ˇC, ˇC)
ˆ
M 7→
 
ˆ
M ⊙−
:
ˇC
→
ˇC
P
7→
V
  ˆ
M(I), P

!
(21)
Functoriality of ⊙follows from the functoriality of copresheaves.
To conﬁrm that ⊙makes ˇC into a ˆC-actegory, we need to check the actegory structure isomorphisms.
Proposition 4.2. ⊙equips ˇC with a ˆC-actegory structure: unitor isomorphisms λ⊙
F : 1 ⊙F
∼
−→F and
associator isomorphisms a⊙
ˆ
M, ˆ
N,F : ( ˆ
M × ˆN) ⊙F
∼
−→ˆ
M ⊙( ˆN ⊙F) for each ˆ
M, ˆN in ˇC, both natural in
F : V-Cat(C, V).
Proof. We ﬁrst check the unitor:
λ⊙
F : 1 ⊙C(B, −) = V
 1(I), C(B, −)

∼= V
 1, C(B, −)

∼= C(B, −)
where 1 is the terminal object in V.
20

The associator is given as follows:
a⊙
ˆ
M, ˆ
N,P
−1 : ˆ
M ⊙

ˆN ⊙P

= V

ˆ
M(I), V

ˆN(I), P

∼= V

ˆ
M(I) × ˆN(I), P

∼= V

( ˆ
M × ˆN)(I), P

= ( ˆ
M × ˆN) ⊙P
where the ﬁrst isomorphism follows by the Cartesian closure of V.
We are now in a position to deﬁne the category of abstract Bayesian lenses, and show that this category
coincides with the category of Stat-lenses.
Deﬁnition 4.3 (Bayesian lenses). Denote by BayesLens the category of optics Optic×,⊙for the action of
the Cartesian product on presheaf categories × : ˆC →V-Cat( ˆC, ˆC) and the action ⊙: ˆC →V-Cat( ˇC, ˇC)
deﬁned in (21). Its objects ( ˆX, ˇY ) are pairs of a presheaf and a copresheaf on C, and its morphisms ( ˆX, ˇA) 7→
( ˆY , ˇB) are abstract Bayesian lenses—elements of the set
Optic×,⊙

( ˆX, ˇA), ( ˆY , ˇB)

=
Z
ˆ
M : ˆC
ˆC( ˆX, ˆ
M × ˆY ) × ˇC( ˆ
M ⊙ˇB, ˇA)
A Bayesian lens ( ˆX, ˇX) 7→( ˆY , ˇY ) is called a simple Bayesian lens.
Proposition 4.4. BayesLens is a category of lenses.
Proof. The product × : ˆC →V-Cat( ˆC, ˆC) on ˆC is Cartesian, so Comon( ˆC) = ˆC. Hence
Optic×,⊙

( ˆX, ˇA), ( ˆY , ˇB)

∼=
Z
ˆ
M : ˆC
ˆC( ˆX, ˆY ) × ˆC( ˆX, ˆ
M) × ˇC( ˆ
M ⊙ˇB, ˇA)
(22)
is of the form in deﬁnition 2.15.
Proposition 4.5 (Stat-lenses are Bayesian lenses). Let ˆ(·) : C ֒→V-Cat(C op, V) denote the Yoneda em-
bedding and ˇ(·) : C ֒→V-Cat(C, V) the coYoneda embedding. Then
Optic×,⊙

( ˆX, ˇA), ( ˆY , ˇB)

∼= GrLensStat

(X, A), (Y, B)

so that GrLensStat is equivalent to the full subcategory of Optic×,⊙on representable (co)presheaves.
Proof.
Optic×,⊙

( ˆX, ˇA), ( ˆY , ˇB)

∼=
Z
ˆ
M : ˆC
ˆC( ˆX, ˆY ) × ˆC( ˆX, ˆ
M) × ˇC( ˆ
M ⊙ˇB, ˇA)
∼=
Z
ˆ
M : ˆC
ˆC( ˆX, ˆY ) × ˆC( ˆX, ˆ
M) × ˇC

V( ˆ
M(I), ˇB), ˇA

∼=
Z
ˆ
M : ˆC
C(X, Y ) × ˆ
M(X) × V

ˆ
M(I), C(B, A)

∼= GrLensStat

(X, A), (Y, B)

The ﬁrst isomorphism is just (22), the second obtains by deﬁnition of ⊙, the third by the Yoneda lemma, and
the fourth by (20).
21

Since Bayesian lenses are lenses, we can check diagrammatically that sequential composition in Optic×,⊙
corresponds to that in GrLensStat. The composite lens

d
 d†
◦| 
c
 c†
of

d
 d†
: (Y, B) 7→(Z, C) after

c
 c†
: (X, A) 7→(Y, B) has the depiction
c
c†
X
A
B
Y
X
d
d†
C
Z
Y
∼=
c
c†
X
A
d
d†
C
Z
c
where the copier
is the universal map with diagonal components x 7→(x, x) induced by the Cartesian
product × on ˆC; recall from the discussion preceding (15) that we can lift diagrams in ( ˆC, ×) to diagrams
in Optic×,⊙. The isomorphism therefore follows because every morphism in ˆC is canonically a comonoid
homomorphism, and we can slide morphisms along the optical residual.
From the right-hand side, we can read that the view component of the composite optic is represented by
d • c and the update component is represented by
c† ◦
 id ˇ
X ⊙d† 
◦a⊙
ˆ
X, ˆY , ˇ
X ◦
 id ˆ
X × c

◦
∼=
 c†
(−) • d†
c•(−)

◦
∼= c† ◦c∗d†
where the ﬁrst expression is given by reading the right-hand side following the deﬁnition of optical composi-
tion (12); where the ﬁrst isomorphism follows by the deﬁnitions of ⊙, a⊙
ˆ
X, ˆY , ˇ
X, and the notation c†
(−) formally
deﬁned in Example 3.2; and where the second isomorphism follows by the deﬁnition of c†
(−) and the deﬁnition
of ﬁbrewise composition in Deﬁnition 3.1.
We therefore have

d
 d†
◦| 
c
 c† ∼=

d • c
 c† ◦c∗d†
, which are just the components of the corre-
sponding composite Stat-lens (Example 3.6), and so Stat-lenses are Bayesian lenses.
Remark 4.6. We will often abuse notation by indicating representable objects in BayesLens by their rep-
resentations in C. That is, we will write (X, A) instead of ( ˆX, ˇA) where this would be unambiguous.
It may be of interest sometimes to consider cases where the update morphisms admit more or diﬀerent
structure to the view morphisms in C. We can generalize Bayesian lenses to such a mixed case as follows.
Deﬁnition 4.7. We ﬁrst generalize the action ⊙. Let D be the category of update morphisms. We assume it
to be V-enriched. We deﬁne an action ⊘: ˆC →V-Cat( ˇD, ˇD) of ˆC on ˇD as a straightforward generalization
of ⊙as deﬁned in (21). Once again, we give only the action on objects; the action on morphisms is analogous.
⊘: ˆC →V-Cat( ˇD, ˇD)
ˆ
M 7→
 
ˆ
M ⊘−
:
ˇD
→
ˇD
P
7→
V
  ˆ
M(I), P

!
22

⊘equips ˇD with a ˆC-actegory structure, just as in Proposition 4.2. We deﬁne a corresponding category of
mixed Bayesian lenses as the obvious generalization of Deﬁnition 4.3. Objects ( ˆX, ˇY ) are pairs of a presheaf
on C and a copresheaf on D, and morphisms ( ˆX, ˇA) 7→( ˆY , ˇB) are elements of
Optic×,⊘

( ˆX, ˇA), ( ˆY , ˇB)

=
Z
ˆ
M : ˆC
ˆC( ˆX, ˆ
M × ˆY ) × ˇD( ˆ
M ⊘ˇB, ˇA) .
Example 4.8 (State-dependent algebra homomorphisms). Let C = Kℓ(M) be the Kleisli category of a monad
M : Set →Set and let D = EM(M) be its Eilenberg-Moore category. Both C and D are Set -enriched.
A (representable) mixed Bayesian lens ⟨v | u⟩: (S, T)
7→(A, B) over C and D is then given by a Kleisli
morphism v : S →MA and an S-state-dependent algebra homomorphism u : Set
 MS, EM(M)(B, T)

.
Under the forgetful functor U : EM(M) →Set and by the Cartesian closed structure of Set, u is equivalently
a function u♭: MS ×B →T such that u♭(µ, −) : B →T is an M-algebra homomorphism for each µ : MS.
5. Bayesian updates compose optically
The categories of state-dependent channels and of Bayesian lenses deﬁned in §3 and §4 are substantial gen-
eralizations of concrete Bayesian inversion as introduced in §2.1. In this section, we concentrate on the latter,
noting that every pair of a stochastic channel c and its (state-dependent) inversion c†
(·) constitutes a simple
Bayesian lens

c
 c†
satisfying the following deﬁnition. We adopt the terminology of ‘exact’ and ‘approxi-
mate’ inference [19].
Deﬁnition 5.1 (Exact and approximate Bayesian lens). Let

c
 c†
: (X, X) 7→(Y, Y ) be a simple Bayesian
lens. We say that

c
 c†
is exact if c admits Bayesian inversion and, for each π : I→
• X such that c • π has
non-empty support, c and c†
π together satisfy equation (8). Simple Bayesian lenses that are not exact are said
to be approximate.
We seek to prove the following theorem, which is the main result of this paper.
Theorem 5.2. Let

c
 c†
and

d
 d†
be sequentially composable exact Bayesian lenses. Then the con-
travariant component of the composite lens

d
 d†
◦| 
c
 c† ∼=

d • c
 c† ◦c∗d†
is, up to d • c • π-almost-
equality, the Bayesian inversion of d • c with respect to any state π on the domain of c such that c • π has
non-empty support. That is to say, Bayesian updates compose optically: (d • c)†
π
d•c•π
∼
c†
π • d†
c•π. Graphically:
c
c†
X
A
d
d†
C
Z
c
∼
d • c
(d • c)†
X
A
C
Z
X
(23)
Corollary 5.3. Let C† be the wide subcategory of channels in C that admit Bayesian inversion (Deﬁnition
2.3). Then C† embeds functorially into BayesLens. On objects, the embedding is given by X 7→( ˆX, ˇX); on
morphisms, c 7→

c
 c†
.
23

Because Bayesian inversion is only determined up to almost-equality, the embedding C† ֒→BayesLens is
not unique, requiring a choice of inversion for each channel. However, in most situations of practical interest,
there is a canonical choice. For those channels which have density function representations, the canonical
choice is given by Proposition 2.8 or equation (10); alternatively, by restricting to ﬁnite support, Bayesian
inversions are actually unique.
We supply proofs of Theorem 5.2 in various copy-delete categories at various levels of abstraction, starting
with the concrete case of ﬁnitely-supported probability in Kℓ(D). We follow this with the most abstract
case, in an arbitrary copy-delete category admitting Bayesian inversion (ﬁrst without and then with density
functions), followed then by the case of s-ﬁnite measures (with density functions), from which we also recover
the discrete result.
For pedagogical purposes, the structure of this section mirrors that of §2.1, and we have attempted to
structure the proofs to emphasize their commonalities.
5.1. Discrete case
In this section, we work in the category of stochastic channels C = Kℓ(D), described in §2.1.1. Note that with
ﬁnite support, almost-equality reduces to equality, and so Bayesian inversions where they exist are unique.
Proof of Theorem 5.2. Suppose p : X →DY and q : Y →DZ. Given a prior ρ : 1 →DX on X, we are
interested in the Bayesian inversion (q • p)†
ρ : Z →DX of q • p : X →DZ with respect to ρ. Following (5),
we have
(q • p)†
(·) : DX × Z →DX
:= ρ × z 7→
X
x:X
(q • p)(z|x) · ρ(x)
P
x′:X(q • p)(z|x′) · ρ(x′) |x⟩=
X
x:X
(q • p)(z|x) · ρ(x)
(q • p • ρ)(z)
|x⟩.
The lens composite of q† and p† with respect to ρ is p†
ρ • q†
p•ρ. Our task is therefore to show that
p†
ρ • q†
p•ρ(z) =
X
x:X
(q • p)(z|x) · ρ(x)
(q • p • ρ)(z)
|x⟩= (q • p)†
ρ(z) .
By Kleisli extension (4), for any σ : 1 →DY ,
p†
ρ • σ =
X
x:X
X
y:Y
p†
ρ(x|y) · σ(y) |x⟩=
X
x:X
X
y:Y
p(y|x) · ρ(x)
(p • ρ)(y)

σ(y) |x⟩.
Now, let σ 7→q†
p•ρ(z), so
p†
ρ • q†
p•ρ(z) =
X
x:X
X
y:Y
p(y|x) · ρ(x)
(p • ρ)(y)

· (q†
p•ρ)(y|z) |x⟩
=
X
x:X
X
y:Y
p(y|x) · ρ(x)
(p • ρ)(y)

·
q(z|y) · (p • ρ)(y)
(q • p • ρ)(z)

|x⟩
=
X
x:X
X
y:Y
q(z|y) · p(y|x) · ρ(x)
(q • p • ρ)(z)
|x⟩
=
X
x:X
(q • p)(z|x) · ρ(x)
(q • p • ρ)(z)
|x⟩
= (q • p)†
ρ(z)
24

as required.
5.2. Abstract case
Here, we work in an arbitrary copy-delete category C with those morphisms that admit Bayesian inversion in
the abstract sense of equation (8) (§2.1.3). This proof implies the result in the more concrete categories Kℓ(D)
and sfKrn; we include those for their computational and pedagogical content.
Proof of Theorem 5.2. Suppose c†
π : Y →
• X is the Bayesian inverse of c : X→
• Y with respect to π : I→
• X.
Suppose also that d†
c•π : Z→
• Y is the Bayesian inverse of d : Y →
• X with respect to c • π : I→
• Y , and that
(d • c)†
π : Z→
• X is the Bayesian inverse of d • c : X→
• Z with respect to π : I→
• X:
d
π
c
Y
Z
∼=
d†
c•π
π
c
d
Y
Z
and
c
π
d
X
Z
∼=
(d • c)†
π
π
c
d
X
Z
The lens composite of these Bayesian inverses has the form c†
π • d†
c•π : Z→
• X, so to establish the result it
suﬃces to show that
d†
c•π
π
c
d
c†
π
X
Z
∼=
c
π
d
X
Z
where we can think of the left-hand side as ‘unfolding’ along the residual the left-hand side of (23).
25

We have the following isomorphisms:
d†
c•π
π
c
d
c†
π
X
Z
∼=
d
π
c
c†
π
X
Z
∼=
c
π
d
X
Z
where the ﬁrst obtains because d†
c•π is the Bayesian inverse of d with respect to c•π, and the second because c†
π
is the Bayesian inverse of c with respect to π. Hence, c†
π •d†
c•π and (d•c)†
π are both Bayesian inversions of d•c
with respect to π. Since Bayesian inversions are almost-equal (Proposition 2.9), we have c†
π•d†
c•π
d•c•π
∼
(d•c)†
π,
as required.
5.2.1. With density functions
Here, we work in an abstract copy-delete category C in which stochastic channels can be represented by
eﬀects in the sense of Deﬁnition 2.4 (§2.1.4).
Proof of Theorem 5.2. Suppose c : X→
• Y and d : Y →
• Z are represented by eﬀects
c
X
Y
µ
p
X
Y
=
and
d
Y
Z
ν
q
Y
Z
=
so that the composite d • c : X→
• Z is given by
d
Z
c
X
=
ν
q
µ
p
X
Z
=
ν
pµq
X
Z
where the eﬀect pµq : X ⊗Z→
• I is deﬁned in the obvious way.
26

Following Proposition 2.8, the Bayesian inverse c†
π : Y →
• X of c with respect to π : I→
• X is given by
p
π
p−1
X
Y
where p−1 : Y →
• I is a µ-almost-inverse for the eﬀect
p
π
Y
.
Similarly, the Bayesian inverse d†
c•π : Z→
• Y of d with respect to c • π : I→
• Y is
q
q−1
c
π
Y
Z
,
with q−1 : Z→
• I the corresponding ν-almost-inverse for
q
c
π
Z
,
and the Bayesian inverse for d • c with respect to π is (d • c)†
π : Z→
• X, with the form
pµq
π
(pµq)−1
Z
X
=
q
µ
p
π
(pµq)−1
X
Z
27

where (pµq)−1 is also a ν-almost-inverse for q •
 (c • π) ⊗idZ

: Z→
• I.
We seek to show that (d • c)†
π
d•c•π
∼
c†
π • d†
c•π. We start from the lens composite c†
π • d†
c•π which is given by
q
q−1
c
π
p
π
p−1
X
Z
=
q
q−1
p
π
p−1
X
Z
µ
p
π
 by deﬁnition of c

28

∼=
q
q−1
p
π
p−1
X
Z
µ
p
π
 by associativity (6) and commutativity (7) of copiers

µ∼
q
q−1
X
Z
µ
p
π
 by deﬁnition of p−1
∼=
q
q−1
π
µ
p
X
Z
 by unitality (6) of copiers

29

ν∼
q
µ
p
π
(pµq)−1
X
Z
.
The last line follows by Lemma 2.10, since the two almost-inverses (pµq)−1 and q−1 are ν-almost equal
(Proposition 2.7).
We have shown that (d • c)†
π
ν∼c†
π • d†
c•π. Recall that d is represented by an eﬀect with respect to the state
ν. So by Lemma 2.11, we have (d • c)†
π
d•c•π
∼
c†
π • d†
c•π, as required.
5.3. S-Finite case with density functions
Here, we instantiate the abstract density function proof of §5.2.1 in the category sfKrn of s-ﬁnite kernels
described in §2.1.5, in order to obtain a form of the result commensurate with the informal form of Bayes’
rule (9). Then, restricting to ﬁnitely supported measures, we recover the discrete case of the result in Kℓ(D).
Proof of Theorem 5.2. Equation (10) statesthat, by interpretingthe string diagram of Proposition 2.8 for Bayesian
inversion via density functions in sfKrn, the Bayesian inverse d†
ρ of d : Y →
• Z with respect to ρ : I→
• Y is
given by
d†
ρ : Z × ΣY →[0, ∞] := z × B 7→
Z
y:B
ρ(dy) q(z|y)

q−1(z)
= q−1(z)
Z
y:B
q(z|y) ρ(dy)
where q−1 : Z→
• I is a ν-almost-inverse for q • ((c • π) ⊗idZ), given up to ν-almost-equality by
q−1 : Z →[0, ∞] := z 7→
Z
y:Y
q(z|y) µ(dy)
Z
x:X
p(y|x) π(dx)
−1
.
Suppose then that
ρ = c • π : 1 × ΣY →[0, ∞] := ∗× B 7→
Z
y:B
µ(dy)
Z
x:X
p(y|x) π(dx) .
We therefore have, by direct substitution,
d†
c•π : Z × ΣY →[0, ∞] := z × B 7→q−1(z)
Z
y:B
q(z|y)µ(dy)
Z
x:X
p(y|x) π(dx).
We now write down directly the Bayesian inverse of the composite channel, (d • c)†
π : Z→
• X:
(d • c)†
π : Z × ΣX →[0, ∞]
:= z × A 7→
Z
x:A
π(dx) (pµq)(z|x)

(pµq)−1(z)
=(pµq)−1(z)
Z
x:A
(pµq)(z|x) π(dx)
=(pµq)−1(z)
Z
x:A
Z
y:Y
q(z|y) µ(dy) p(y|x) π(dx)
30

where (pµq)−1 is a ν-almost-inverse for (pµq) •(π ⊗idZ), or equivalently (since almost-inverses are almost-
equal; Proposition 2.7), a ν-almost-inverse for q • ((c • π) ⊗idZ).
The lens form of the inverse composite is c†
π • d†
c•π : Z→
• X, and we follow the diagrammatic reasoning to
show that this is equal to the direct form above:
c†
π • d†
c•π : Z × ΣX →[0, ∞]
:= z × A 7→
Z
y:Y
c†
π(A|y) · (d†
c•π)(dy|z)
=
Z
y:Y

p−1(y)
Z
x:A
p(y|x) π(dx)

·

q−1(z) q(z|y) µ(dy)
Z
x:X
p(y|x) π(dx)

=
Z
y:Y

p−1(y)
Z
x:X
p(y|x) π(dx)

·

q−1(z) q(z|y) µ(dy)
Z
x:A
p(y|x) π(dx)

µ∼q−1(z)
Z
y:Y
q(z|y) µ(dy)
Z
x:A
p(y|x) π(dx)
= q−1(z)
Z
x:A
Z
y:Y
q(z|y) µ(dy) p(y|x) π(dx)
ν∼
 (d • c)†
π

(A|z)
The second equality follows by Fubini’s theorem in sfKrn (equivalently, by the symmetry of the monoidal
product ⊗; see §2.1.5), and the last line by Lemma 2.10 since almost-inverses are almost-equal(Proposition 2.7);
compare the ﬁnal part of the graphical proof in §5.2.1. Finally, by Lemma 2.11, we have (d•c)†
π
d•c•π
∼
c†
π •d†
c•π,
also as in §5.2.1.
Corollary 5.4. By restricting to ﬁnitely supported measures, we recover the discrete-case result of §5.1.
Proof. Suppose then that the prior π : I→
• X is represented by a density function ρ : X→
• I with respect to a
measure υ : I→
• X. Suppose further that the measures µ, ν, and υ are ﬁnitely supported. Then
 (d • c)†
π

(x | z) = q−1(z)
Z
x:{x}
Z
y:Y
q(z|y) µ(dy) p(y|x) υ(dx) ρ(x)
=
R
x:{x}
R
y:Y q(z|y) µ(dy) p(y|x) υ(dx) ρ(x)
R
y′:Y q(z|y′) µ(dy′)
R
x′:X p(y′|x′) υ(dx) ρ(x′)
=
P
y:Y q(z|y) p(y|x) ρ(x)
P
y′:Y q(z|y′) P
x′:X p(y′|x′) ρ(x′)
Now, assume that the density functions are indeed probability densities (i.e., they sum to 1 over their support),
and note that they are therefore functions of the form X × Y →[0, 1]. We therefore recognise them as
stochastic matrices X→
• Y in Kℓ(D), and can interpret the foregoing expression as
P
y:Y q(z|y) p(y|x) ρ(x)
P
y′:Y q(z|y′) P
x′:X p(y′|x′) ρ(x′) =
 q • p

(z|x) · ρ(x)
 q • p • ρ

(z)
,
where • is now composition in Kℓ(D), thereby recovering the result in the discrete case.
6. Lawfulness of Bayesian lenses
The study of Cartesian lenses substantially originates in the context of bidirectional transformations of data
in the computer science and database community [1, 5], where we can think of the view (or get) function
31

as returning part of a database record, and the update (or put) function as ‘putting’ a part into a record and
returning the updated record. In this setting, extra structure known as lens laws can be imposed on lenses to
ensure that they are ‘well-behaved’ with respect to database behaviour. Well-behaved and ‘very well-behaved’
lenses in the database context roughly correspond to our notion of ‘exact’ Bayesian lens, but as we will see,
even exact Bayesian lenses are only weakly lawful in the database sense.
We will concentrate on the three lens laws that have attracted recent study [1, 15]: GetPut, PutGet,
and PutPut. A Cartesian lens satisfying the former two is well-behaved while a lens satisfying all three
is very well-behaved, in the terminology of Foster et al. [5]. Informally, GetPut says that getting part of
a record and putting it straight back returns an unchanged record; PutGet says that putting a part into a
record and then getting it returns the same part that we started with; and PutPut says that putting one part
and then putting a second part has the same eﬀect on a record as just putting the second part (that is, update
completely overwrites the part in the record). We will express these laws graphically, and consider them each
brieﬂy in turn.
Note ﬁrst that we can lift any channel c in the base category C into any state-dependent ﬁbre Stat(A) using
the constant (identity-on-objects) functor taking c to the constant-valued state-indexed channel ρ 7→c that
maps any state ρ to c. We can lift diagrams in C into the ﬁbres accordingly.
GetPut
Deﬁnition 6.1. A lens

c
 c†
is said to satisfy the GetPut law if it satisﬁes the left isomorphism in (24)
below. Equivalently, because the copier induced by the Cartesian product is natural (i.e.,
◦f ∼= (f ×f)◦
),
for any state π, we say that

c
 c†
satisﬁes GetPut with respect to π if it satisﬁes the right isomorphism
in (24) below.
c
c†
∼=
=⇒
π
c
c†
π
π
∼=
(24)
Proposition 6.2. When c is causal, the exact Bayesian lens

c
 c†
satisﬁes the GetPut law with respect
to any state π for which c admits Bayesian inversion.
Proof. Starting from the right-hand-side of (24), we have the following chain of isomorphisms
π
c†
π
π
c
c
π
∼=
∼=
π
∼=
π
c
c†
π
∼=
32

where the ﬁrst holds by the unitality of
(6), the second by the causality of c, the third since c admits Bayesian
inversion (8) with respect to π, and the fourth again by unitality of
.
Note that by Bayes’ law, exact Bayesian lenses only satisfy GetPut with respect to states. This result
means that, if we think of c as generating a prediction c • π from a prior belief π, then if our observation
exactly matches the prediction, updating the prior π according to Bayes’ rule results in no change.
PutGet
The PutGet law is characterized for a lens ⟨v | u⟩by the following isomorphism:
u
v
∼=
In general, PutGet does not hold for exact Bayesian lenses ⟨v | u⟩=

c
 c†
. However, because GetPut
holds with respect to states π, we do have c ◦c† ◦(π × c • π) ∼= (
× id) ◦(π × c • π); that is, PutGet holds
for exact Bayesian lenses

c
 c†
for the input π × c • π.
The reason PutGet fails to hold in general is that Bayesian updating by construction mixes information
from the prior and the observation, according to the strength of belief. Consequently, updating a belief accord-
ing to an observed state and then producing a new prediction need not result in the same state as observed;
unless, of course, the prediction already matches the observation.
PutPut
Finally, the PutPut law for a lens ⟨v | u⟩is characterized by the following isomorphism:
u
u
u
∼=
PutPut fails to hold for exact Bayesian lenses for the same reason that PutGet fails to hold in general:
updates mix old and new beliefs, rather than entirely replace the old with the new.
Comment
The lens laws were originally deﬁned in the context of computer databases, where there is as-
sumed to be no uncertainty: database logic is Boolean (database instances are objects in a presheaf topos [4]),
so a ‘belief’ is either true or false. Consequently, there can be no ‘mixing’ of beliefs; and in database applica-
tions, such mixing may be highly undesirable (putting epistemological concerns aside). Bayesian lenses, on
the other hand, live in a fuzzier world: the present author’s interest in Bayesian lenses originates in their ap-
plication to describing cognitive and cybernetic processes such as perception and action, and here the ability
to mix beliefs according to uncertainty is desirable.
Possibly it would be of interest to give analogous information-theoretic lens laws that characterize exact
and approximate Bayesian lenses and their generalizations; and we might then expect the ‘Boolean’ lens
laws to emerge in the extremal case where there is no uncertainty and only Dirac states. We leave such an
endeavour for future work: Bayes’ law (8) is suﬃciently concise and productive for our purposes here.
33

7. References
[1]
Mitchell Riley. “Categoriesof Optics”. In: (09/03/2018). arXiv:http://arxiv.org/abs/1809.00738v2
[math.CT].
[2]
Bryce Clarke et al. “Profunctor optics, a categorical update”. In: (01/21/2020). arXiv: 2001.07488v1
[cs.PL].
[3]
Tom Leinster. Basic Category Theory. Vol. Cambridge University Press, 2014. Cambridge University
Press, 12/2016, CambridgeStudiesinAdvancedMathematics, Vol.143. eprint: 1612.09375.
[4]
Brendan Fong and David I. Spivak. Seven Sketches in Compositionality: An Invitation to Applied Category
Theory. 2018. arXiv: http://arxiv.org/abs/1803.05316v3 [math.CT].
[5]
J. Nathan Foster et al. “Combinators for bidirectional tree transformations”. In: ACM Transactions on
Programming Languages and Systems 29.3 (05/2007), p. 17. doi: 10.1145/1232420.1232424.
[6]
David I. Spivak. “GeneralizedLens Categoriesvia functors Cop →Cat”. In: (08/06/2019). arXiv: http://arxiv.o
[math.CT].
[7]
Kenta Cho and Bart Jacobs. “Disintegration and Bayesian Inversion via String Diagrams”. In: Math.
Struct. Comp. Sci. 29 (2019) 938-971 (08/29/2017). doi: 10.1017/S0960129518000488. arXiv:
http://arxiv.org/abs/1709.00322v3 [cs.AI].
[8]
Tobias Fritz. “A synthetic approach to Markov kernels, conditional independence and theorems on
suﬃcient statistics”. In: (08/19/2019). arXiv: http : / / arxiv . org / abs / 1908 . 07021v3
[math.ST].
[9]
Brendan Fong and David I Spivak. “Supplying bells and whistles in symmetric monoidal categories”.
In: (08/07/2019). arXiv: http://arxiv.org/abs/1908.02633v1 [math.CT].
[10]
J.M. Stoyanov. Counterexamples in Probability: Third Edition. Dover Books on Mathematics. Dover Publi-
cations, 2014. isbn: 9780486499987. url: https://books.google.co.uk/books?id=xaH8AQAAQBAJ
[11]
Kenta Cho et al. “An Introduction to Eﬀectus Theory”. In: arXiv preprint arXiv:1512.05813 (2015). arXiv:
http://arxiv.org/abs/1512.05813v1 [cs.LO].
[12]
Sam Staton. “Commutative Semantics for Probabilistic Programming”. In: Programming Languages and
Systems. Springer Berlin Heidelberg, 2017, pp. 855–879. doi: 10.1007/978-3-662-54434-1_32.
[13]
Mario Román. “Profunctor optics and traversals”. In: (01/22/2020). arXiv: 2001.08045v1 [cs.PL].
[14]
Fosco Loregian. “This is the (co)end, my only (co)friend”. In: (01/11/2015). arXiv: http://arxiv.org/abs/150
[math.CT].
[15]
Guillaume Boisseau. “String Diagramsfor Optics”. In: (02/11/2020). arXiv: 2002.11480v1 [math.CT].
[16]
Mario Román. “Open Diagramsvia Coend Calculus”. In: (04/09/2020). arXiv: 2004.04526v2 [math.CT].
34

[17]
nLab authors. Grothendieck construction. Version Revision 62. Revision 62. 04/2020. url: http://ncatlab.org/
[18]
Chris Heunen et al. “A Convenient Category for Higher-Order Probability Theory”. In: (01/10/2017).
doi: 10.1109/lics.2017.8005137. arXiv: http://arxiv.org/abs/1701.02547
[cs.PL].
[19]
Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. “Generalized Variational Inference”. In:
(12/13/2019). arXiv: http://arxiv.org/abs/1904.02063 [stat.ML].
A. Extraneous proofs
A.1. Proposition 2.7: Almost-inverses are almost-equal
Proof. By assumption, we have
π∼
q
p
π∼
r
p
.
Then, by the deﬁnition of almost-equality (Deﬁnition 2.5,:
q
p
π
π
π
r
p
π
∼=
∼=
∼=
.
(25)
We seek to show that
∼=
q
π
r
π
.
(26)
35

Substituting the right-hand-side of (25) for π in the left-hand-side of (26), we have that
q
r
p
π
q
π
∼=
r
q
p
π
r
π
∼=
∼=
which establishes the result. The second isomorphism follows by the associativity of
(6), and the third ex
hypothesi and by the unitality of
(6).
A.2. Proposition 2.8: Bayesian inversion via density functions
Proof. We seek to establish the relation (8) characterizing Bayesian inversion. By substituting the density
function representations for c and c†
π into the right-hand-side of (8), we have
c†
π
π
c
∼=
µ
p
p
π
p−1
π
36

∼=
µ
p
p
π
p−1
π
∼=
µ
p
π
c
π
∼=
as required. The second isomorphism holds by the associativity of
(6), the third since p−1 is an almost-
inverse ex hypothesi, and the fourth by the unitality of
(6) and the density function representation of c.
A.3. Lemma 2.10
Proof. By assumption, we have
α
q
f
∼=
and
β
r
f
∼=
.
Consequently,
α
∼=
µ
q
f
µ
∼=
f
q
µ
37

β
∼=
µ
r
f
µ
∼=
f
r
µ
∼=
and so, by the commutativity of
(7), α
µ∼β. The ﬁrst isomorphism holds by the deﬁnition of α, the second
by the associativity of
(6), the third since q
µ∼r, the fourth by the associativity of
, and the ﬁfth by the
deﬁnition of β.
A.4. Lemma 2.11
Proof. We start from the left-hand-side of the relation deﬁning almost-equality (Deﬁnition 2.5), substituting
the density function representation for d. This gives the following chain of isomorphisms:
ρ
d
ν
q
ρ
ν
q
ρ
f
f
f
∼=
∼=
ν
q
ρ
g
ν
q
ρ
g
∼=
ρ
d
g
∼=
∼=
The second isomorphism holds by the associativity of
(6); the third since f ν∼g; the fourth by associativity
of
; and the ﬁfth by the density function representation for d. This establishes the required relation.
B. Review of basic coend calculus
In §2.2, we introduced coends informally in the context of optics. Here, we brieﬂy review some basic properties
of coends and their associated coend calculus. We continue to work in the general setting of a V-enriched
38

category C, where V is assumed to be cocomplete and Cartesian closed.
(Co)presheaves act by composition
Recall that for every object X : C0, we have the representable
presheaf C(−, X) : C op →V and representable copresheaf C(X, −) : C →V. C(−, X) represents the
collection of morphisms into X (i.e. with codomain X), and C(X, −) the collection of morphisms out of X (i.e.
with domain X).
Being functors, representable (co)presheaves act on objects and morphisms in C. On objects, C(X, Y ) is
the object in V of morphisms X→
• Y in C. Given a morphism h : C(Y, Z), the copresheaf C(X, −) takes
h to the map C(X, h) : C(X, Y ) →C(X, Z) in V which acts by postcomposition: that is, C(X, h) takes
g : C(X, Y ) to h • g : C(X, Z). Similarly, the action of the presheaf C(−, Y ) is precomposition: given a
morphism f : C(W, X), the map C(f, Y ) : C(X, Y ) →C(W, Y ) takes g : C(X, Y ) to g • f : C(W, Y ). Note
that the action of a presheaf is contravariant: presheaves ‘pull back’; copresheaves ‘push forwards’.
Profunctors
By allowing the functors C(X, −) and C(−, Y ) to vary in both arguments simultaneously, we
obtain the hom profunctor C(−, =) : C op ×C →V, which picks out the object C(X, Y ) in V of morphisms
X→
• Y in C for each pair of objects (X, Y ) in C; we can think of the elements this hom object as witnessing
the relation between X and Y , and indeed, profunctors are categoriﬁed relations (see Loregian [14, Example
5.4] or Fong and Spivak [4, Chapter 4]). Naturally, the action of the hom profunctor on morphisms in C is by
pulling back (precomposition) on the left and pushing forwards (postcomposition) on the right.
Coends
Fix an arbitrary profunctor P : C op ×C →V. The coend
R C P is the greatest quotient in V
of `
X P(X, X) that coequalizes the left and right actions of P on morphisms in C. `
X P(X, X) is the
coproduct (disjoint union) of all the objects P(X, X). That is, the elements of the coend
R C P are equivalence
classes such that two elements u : P(W, W) and v : P(X, X) are related if there exists some f : P(X, W)
satisfying u • f = f • v; that is, u and v are related by the coend if we can ‘slide’ some f between them.
Compare this with the discussion following the deﬁnition of optics, Deﬁnition 2.13. The universal property
of the coend is then that it is the largest such quotient, so that any morphism out of any such quotient factors
through the coend.
More formally, let f : X→
• Y be any morphism in C; we will denote its domain X by dom(f) = X and its
codomain Y by cod(f) = Y . We can then formally deﬁne the coend
R C P as the following coequalizer.
Deﬁnition B.1 (Coend). Deﬁne witnesses to the left and right actions of P as follows.
λ∗:=
a
W :C, X:C,
f : C(W, X)
ιdom(f) ◦P
 f, dom(f)

and
ρ∗:=
a
W :C, X:C,
f : C(W, X)
ιcod(f) ◦P
 cod(f), f

where, given an object Y : C0, we denote by ιY : P(Y, Y ) →`
X P(X, X) the inclusion of P(Y, Y ) into the
coproduct `
X P(X, X). Then the coend
R C P is given by the following coequalizer:
a
W :C, X:C,
f : C(W, X)
P
 cod(f), dom(f)

a
X
P(X, X)
Z C
P
λ∗
ρ∗
To make the ‘variable of integration’ explicit, we often denote the coend
R C P by
Z X: C
P(X, X)
39

Basic coend calculus
We make use of the following isomorphisms, which are equivalent to the basic
categorical result that any (co)presheaf is canonically a colimit of representables [3, Theorem 6.2.17]. They are
easy to prove using the Yoneda lemma and the basic categorical result that hom functors preserve (co)limits;
for an explicit proof, we refer the reader to Loregian [14]. The application of these to the proof of categorical
results, along with the application of universal properties and the Yoneda lemma, is known as coend calculus.
Proposition B.2 (Yoneda reduction). Let F : C op →V be any presheaf, and G : C →V any presheaf. Then
F ∼=
Z X: C
C(−, X) × F(X)
and
G ∼=
Z X: C
G(X) × C(X, −) .
(27)
Corollary. The hom profunctor is the coend analogue of the Dirac delta measure: for any W, Y : C,
C(−, Y ) ∼=
Z X: C
C(−, X) × C(X, Y )
and
C(W, −) ∼=
Z X: C
C(W, X) × C(X, −) .
Along with the idea of presheaves as ‘pulling back’ by precomposition and copresheaves as ‘pushing for-
wards’ by postcomposition, these last two isomorphisms supply useful intuition about Yoneda reduction. The
ﬁrst says roughly that the object of maps into Y is the same as the object of maps into some X paired with
maps into Y such that we can slide forwards by composition from X into Y ; for instance, we can choose the
representative given by X = Y and where the second map in the pair is idY . The second isomorphism is just
the dual, where instead of sliding forwards, we slide back.
40

