Vid2Seq: Large-Scale Pretraining of a Visual Language Model
for Dense Video Captioning
Antoine Yang†* Arsha Nagrani§
Paul Hongsuck Seo§
Antoine Miech♯
Jordi Pont-Tuset§
Ivan Laptev†
Josef Sivic¶
Cordelia Schmid§
§Google Research
†Inria Paris and D´epartement d’informatique de l’ENS, CNRS, PSL Research University
♯DeepMind
¶Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague
https://antoyang.github.io/vid2seq.html
Large-scale pretraining from narrated videos
Dense video captioning
Hey guys today I am going 
to teach you how to ski
The kids make it look easy
First slope, congratz!
Vid2Seq
<1s><8s>The man is fastening the dog. <20s><50s>The dogs are pulling the sled. <45s><49s>The man is saying hello.
Figure 1. Vid2Seq is a visual language model that predicts dense event captions together with their temporal grounding in the video by
generating a single sequence of tokens (right). This ability is enabled by large-scale pretraining on unlabeled narrated videos (left).
Abstract
In this work, we introduce Vid2Seq, a multi-modal
single-stage dense event captioning model pretrained on
narrated videos which are readily-available at scale. The
Vid2Seq architecture augments a language model with spe-
cial time tokens, allowing it to seamlessly predict event
boundaries and textual descriptions in the same output se-
quence. Such a uniﬁed model requires large-scale training
data, which is not available in current annotated datasets.
We show that it is possible to leverage unlabeled narrated
videos for dense video captioning, by reformulating sen-
tence boundaries of transcribed speech as pseudo event
boundaries, and using the transcribed speech sentences as
pseudo event captions. The resulting Vid2Seq model pre-
trained on the YT-Temporal-1B dataset improves the state
of the art on a variety of dense video captioning bench-
marks including YouCook2, ViTT and ActivityNet Captions.
Vid2Seq also generalizes well to the video paragraph cap-
tioning task and the standard task of video clip captioning.
Our code and models will be publicly released.
1. Introduction
Dense video captioning requires the temporal localiza-
tion and captioning of all events in an untrimmed video [46,
102,131]. This differs from standard video captioning [63,
70, 80], where the goal is to produce a single caption for
a given short video clip. Dense captioning is signiﬁcantly
more difﬁcult, as it raises the additional complexity of lo-
calizing the events in minutes-long videos. However, it also
*This work was done when the ﬁrst author was an intern at Google.
beneﬁts from long-range video information. This task is
potentially highly useful in applications such as large-scale
video search and indexing, where the video content is not
segmented into clips.
Existing
methods
mostly
resort
to
two-stage
ap-
proaches [37, 46, 100], where events are ﬁrst localized and
then captioned.
To further enhance the inter-task inter-
action between event localization and captioning, some
approaches have introduced models that jointly solve the
two tasks [20, 102, 131]. However, often these approaches
still require task-speciﬁc components such as event coun-
ters [102]. Furthermore, they exclusively train on manu-
ally annotated datasets of limited size [35, 46, 130], which
makes it difﬁcult to effectively solve the task. To address
these issues, we take inspiration from recent sequence-
to-sequence models pretrained on Web data which have
been successful on a wide range of vision and language
tasks [4,11,13,105,117].
First, we propose a video language model, called
Vid2Seq. We start from a language model trained on Web
text [78] and augment it with special time tokens that repre-
sent timestamps in the video. Given video frames and tran-
scribed speech inputs, the resulting model jointly predicts
all event captions and their corresponding temporal bound-
aries by generating a single sequence of discrete tokens, as
illustrated in Figure 1 (right). Such a model therefore has
the potential to learn multi-modal dependencies between
the different events in the video via attention [94]. However
this requires large-scale training data, which is not avail-
able in current dense video captioning datasets [35,46,130].
Moreover, collecting manual annotations of dense captions
1
arXiv:2302.14115v1  [cs.CV]  27 Feb 2023

for videos is expensive and prohibitive at scale.
Hence we propose to pretrain Vid2Seq by leveraging un-
labeled narrated videos which are readily-available at scale.
To do this, we reformulate sentence boundaries of tran-
scribed speech as pseudo event boundaries, and use the tran-
scribed speech sentences as pseudo event captions. We then
pretrain Vid2Seq with a generative objective, that requires
predicting the transcribed speech given visual inputs, and
a denoising objective, which masks spans of transcribed
speech. Note that transcribed speech may not describe the
video content faithfully, and is often temporally misaligned
with the visual stream [32, 43, 71]. For instance, from the
example in Figure 1 (left), one can understand that the grey
skier has descended a slope from the last speech sentence
which is said after he actually descended the slope. Intu-
itively, Vid2Seq is particularly suited for learning from such
noisy supervision as it jointly models all narrations and the
corresponding timestamps in the video.
We demonstrate the effectiveness of our pretrained
model through extensive experiments. We show the im-
portance of pretraining on untrimmed narrated videos, the
ability of Vid2Seq to use both the visual and speech modali-
ties, the importance of the pretraining objectives, the beneﬁt
of joint caption generation and localization, as well as the
importance of the language model size and the scale of the
pretraining dataset. The pretrained Vid2Seq model achieves
state-of-the-art performance on various dense video cap-
tioning benchmarks [35, 46, 130]. Our model also excels
at generating paragraphs of text describing the video: with-
out using ground-truth event proposals at inference time,
our model outperforms all prior approaches including those
that rely on such proposals [50,76,128]. Moreover, Vid2Seq
generalizes well to the standard task of video clip cap-
tioning [9, 109].
Finally, we introduce a new few-shot
dense video captioning setting in which we ﬁnetune our pre-
trained model on a small fraction of the downstream train-
ing dataset and show beneﬁts of Vid2Seq in this setting.
In summary, we make the following contributions:
(i) We introduce Vid2Seq for dense video captioning.
Given multi-modal inputs (transcribed speech and video),
Vid2Seq predicts a single sequence of discrete tokens that
includes caption tokens interleaved with special time to-
kens that represent event timestamps.
(ii) We show that
transcribed speech and corresponding timestamps in unla-
beled narrated videos can be effectively used as a source
of weak supervision for dense video captioning. (iii) Fi-
nally, our pretrained Vid2Seq model improves the state of
the art on three dense video captioning datasets (YouCook2,
ViTT, ActivityNet Captions), two video paragraph caption-
ing benchmarks (YouCook2, ActivityNet Captions) and two
video clip captioning datasets (MSR-VTT, MSVD).
Code and trained models will be publicly released at [1].
2. Related Work
Dense video captioning. Dense video captioning lies at the
intersection of event localization [25, 29, 33, 62, 65, 66, 84,
127] and event captioning [30,63,75,97,104]. The majority
of existing methods for dense video captioning [37, 38, 46,
100, 103] consist of a temporal localization stage followed
by an event captioning stage. To enrich inter-task interac-
tions, recent works [8,10,20,61,73,79,82,83,100,102,131]
jointly train the captioning and localization modules. In
particular, Wang et al. [102] propose to view dense video
captioning as a set prediction task, and jointly perform
event localization and captioning for each event in paral-
lel. In contrast, our model generates event boundaries and
captions conditioned on the previously generated events.
Deng et al. [20] propose to ﬁrst generate a paragraph and
then ground each sentence in the video. We also generate
all captions as a single output sequence, however our out-
put already includes event timestamps. Zhang et al. [125]
propose to generate event boundaries sequentially, but sep-
arately perform event localization and single event caption-
ing, and only use visual input. Most related to our work,
Zhu et al. [133] also perform dense video captioning by
generating a single output sequence. Their method, how-
ever, infers event locations directly from the timestamps of
transcribed speech and, hence, can only detect events that
closely follow the speech. In contrast, our model generates
event timestamps as special tokens and can produce dense
captions for videos with limited speech, as we demonstrate
on the ActivityNet Captions dataset.
Video and language pretraining. Following the success of
image-text pretraining [14,21,23,24,28,34,36,39–41,53–
55,58,60,68,69,85,88,92,93,99,118–120,124,129], recent
works have explored video-text pretraining [3–5,26,31,32,
43,49,52,56,71,72,74,80,81,89,96,98,108,111,111–115,
121, 122]. These methods show strong improvements on
various tasks such as text-video retrieval [5,71], video ques-
tion answering [112,122] and video clip captioning [4,80].
While these works mostly learn global video representa-
tions to tackle video-level prediction tasks, we here fo-
cus on learning detailed representations to address a dense
prediction task requiring reasoning over multiple events in
untrimmed videos. Several works have explored long-form
video-text pretraining [90] and video-text pretraining for
temporal localization tasks [7, 48, 64, 106, 110, 116]. How-
ever these works focus on video understanding tasks while
our pretraining approach is tailored for a generative task that
not only requires the model to reason over multiple events
in the video, but also to describe them by natural language.
A few works explore pretraining for dense video cap-
tioning. Zhang et al. [125] pretrain on ActivityNet Cap-
tions to improve the downstream performance on the same
dataset. In contrast, we propose a pretraining method that
does not rely on any manual annotation, and show its ben-
2

<6>  <10>    Please
stay
calm!    <86>    <92>   Hey     my
friend!
[BOS]    <1>      <17>     The    man    is
<89>     <98>      The    man    is
saying hello. 
Transformer Text Decoder ht
Decoder Token Embedder hs
Output event sequence z
Transcribed speech sequence
Input video frames  x
Encoder Token Embedder gs
Spatial Encoder f s
... 
<1>    <17>    The      man     is fastening … <98>     The      man     is saying
hello. [EOS]
Visual and speech embeddings [xt, yt]
Visual token
Time token
Text token
…
…
stay calm man          is hey  <0> <1>       <99>
... 
... 
... 
Input transcribed speech
3.02s   à 4.99s: Please stay calm!
42.87s à 45.97s: Hey my friend! 
Output dense 
event captions
0.50s à 8.53s: The man is fastening the dog. 
20.08s à 49.70s: The dogs are pulling the sled. 
44.68s à 49.20s: The man is saying hello.
Text + Time Tokenization
Time Tokenization
Video timeline, duration T = 49.70s quantized in N = 100 bins 
s1=3.02s 
tstart1 = s1×"
#
=
$.&'×(&&
)*.+&
= 6
Visual Encoder  f
Text Encoder  g
Text Decoder h
xt
1
Temporal Encoder  f t
Transformer Text Encoder  gt
Language Modeling Head hl
Text + Time  Tokens
xt
2
xt
F-1
xt
F
yt
1
yt
2
yt
3
yt
4
yt
5
yt
6
yt
7
yt
8
yt
9
yt
s
xs
1
xs
2
xs
F-1
xs
F
ys
1
ys
2
ys
3
ys
4
ys
5
ys
6
ys
7
ys
8
ys
9
ys
s
zs
1
zs
2
zs
3
zs
4
zs
5 zs
6
zs
L-6
zs
L-5
zs
L-4 zs
L-3 zs
L-2
zs
L-1
zs
L
Figure 2. Vid2Seq model overview. We formulate dense event captioning as a sequence-to-sequence problem, using special time tokens
to allow the model to seamlessly understand and generate sequences of tokens containing both textual semantic information and temporal
localization information grounding each text sentence in the video. In detail, all input video frames x and the transcribed speech sequence
y are ﬁrst processed with a Visual Encoder f (a frozen Spatial Encoder f s followed by a Temporal Encoder f t) and a Text Encoder g (a
Token Embedder gs followed by a Transformer Encoder gt), respectively. Then the Text Decoder h (composed of a Token Embedder hs,
a Transformer Encoder ht and a Language Modeling Head hl) autoregressively generates the output event sequence z by cross-attending
to the visual and speech embeddings xt and yt.
eﬁts on multiple downstream datasets. Huang et al. [35]
explore pretraining on narrated instructional videos, but
only consider event captioning using ground truth propos-
als as their model does not handle localization.
Finally,
[35,133] explore pretraining on a domain speciﬁc text-only
dataset [45]. In contrast, we propose to pretrain on a generic
video corpus [121] and show beneﬁts on various domains.
Unifying tasks as language modeling. Recent works [11–
13,15,17,44,59,101,117,132] have shown that it is possi-
ble to cast various computer vision problems as a language
modeling task, addressing object detection [11], grounded
image captioning [117] or visual grounding [132]. In this
work we also cast visual localization as a language mod-
eling task. However, unlike prior work focused on image-
level spatial localization, we address the different problem
of event localization in time, in untrimmed videos.
3. Method
The goal of dense video captioning is to temporally lo-
calize and describe with natural language all events in an
untrimmed input video.
Therefore a key challenge is to
effectively model the relationships between the different
events in the video, as for example, it is easier to predict
that the dogs are pulling the sled if we know that the man
has just fastened a dog (see Figure 1 (right)). Furthermore,
due to the dense nature of the task, there can be many events
in a long video and the requirement is to output a natural
language caption for each event. Hence, another key chal-
lenge is that the manual collection of annotations for this
task is particularly expensive. To tackle these challenges,
we ﬁrst develop a uniﬁed multi-modal model that jointly
predicts event boundaries and captions as a single sequence
of tokens, as explained in Section 3.1 and Figure 2. Second,
we design a pretraining strategy that effectively leverages
cross-modal supervision in the form of transcribed speech
from unlabeled narrated videos by reformulating sentence
boundaries as pseudo event boundaries, as presented in Sec-
tion 3.2 and Figure 3.
3.1. Model
We wish to design a model for dense video captioning
that can capture relationships between events using visual
and (transcribed) speech cues in order to effectively local-
ize and describe these events in untrimmed minutes-long
videos. To tackle this challenge, we cast dense video cap-
tioning as a sequence-to-sequence problem where the input
and output sequences contain both the semantic information
about the event in the form of natural language descriptions
and the temporal localization of the events in the form of
temporal timestamps. In addition, to best leverage both the
visual and the language signal, we develop an appropriate
multi-modal encoder-decoder architecture.
As illustrated
3

in Figure 2, our architecture takes as input video frames
x = {xi}F
i=1 together with the transcribed speech sequence
y = {yj}S
j=1. The output of our model is an event sequence
z = {zk}L
k=1, where each event contains both its textual
description and timestamps corresponding to the temporal
event locations in the video. Below we explain the structure
of the transcribed speech and event sequences constructed
for our model as well as details of our model architecture.
Sequence construction.
To model inter-event relation-
ships in dense event captioning annotations (or the readily-
available transcribed narration, see Section 3.2), we cast
dense video captioning as predicting a single output se-
quence of tokens z.
This output event sequence is con-
structed by leveraging a text tokenizer augmented with spe-
cial time tokens. Furthermore, we enable our architecture
to jointly reason about the semantic and temporal informa-
tion provided in the transcript of the input narration by con-
structing the input transcript sequence y in a similar manner
as the event sequence z. Details are given next.
Time tokenization. We start from a text tokenizer with a
vocabulary size V , and augment it with N additional time
tokens, resulting in a tokenizer with V + N tokens. The
time tokens represent relative timestamps in a video, as we
quantize a video of duration T into N equally-spaced times-
tamps. In detail, we use the SentencePiece tokenizer [47]
with vocabulary size V = 32, 128 and N = 100.
Event sequence. Our introduced tokenizer enables us to
construct sequences that contain both video timestamps and
text video descriptions.
We next explain how we con-
struct the output event sequence z. Note that videos have
a variable number of events in standard dense video cap-
tioning datasets [35, 46, 130]. Each event k is character-
ized by a text segment, a start time and an end time. We
ﬁrst construct for each event k a sequence by concatenat-
ing its start time token tstartk, its end time token tendk and
its text tokens [zk1, ..., zklk ]. Then we order all these se-
quences in increasing order of their start times and con-
catenate them. In practice, each text segment ends with
a dot symbol indicating the separation between different
events. Finally, the event sequence is obtained by prepend-
ing and appending a BOS and an EOS tokens to indi-
cate the start and the end of sequence, respectively, i.e.
z = [BOS, tstart1, tend1, z11, ..., z1l1, tstart2, ..., EOS].
Transcribed speech sequence. To enable the model to use
both the transcribed speech and its corresponding times-
tamps, we convert the speech transcript into a speech se-
quence y similarly as the input training dense event captions
z. This is done by segmenting the raw speech transcript into
sentences with the Google Cloud API1, and using each tran-
scribed speech sentence with its corresponding timestamps
1https://cloud.google.com/speech-to-text/docs/automatic-punctuation.
analogously as an event in the previously explained process.
Architecture.
We wish to design an architecture that can
effectively model relationships between different events in
untrimmed minutes-long videos. To tackle this challenge,
we propose a multi-modal encoder-decoder architecture, il-
lustrated in Figure 2, that gradually reﬁnes and outputs
the event sequence described above.
In detail, given an
untrimmed minutes-long video, the visual encoder f em-
beds its frames while the text encoder g embeds transcribed
speech and the corresponding timestamps. Then a text de-
coder h predicts event boundaries and text captions using
the visual and transcribed speech embeddings. The individ-
ual modules are described next.
Visual encoder. The visual encoder operates on a sequence
of F frames x ∈RF ×H×W ×C where H, W and C are the
height, width and the number of channels of each frame. A
visual backbone f s ﬁrst encodes each frame separately and
outputs frame embeddings xs = f s(x) ∈RF ×d, where
d is the embedding dimension.
Then a transformer en-
coder [94] f t models temporal interactions between the dif-
ferent frames, and outputs F contextualized visual embed-
dings xt = f t(xs + xp) ∈RF ×d, where xp ∈RF ×d are
learnt temporal positional embeddings, which communicate
time information from visual inputs to the model. In de-
tail, the visual backbone is CLIP ViT-L/14 [22, 77] at res-
olution 224 × 224 pixels, pretrained to map images to text
descriptions with a contrastive loss on Web-scraped image-
text pairs. We keep the backbone frozen for efﬁciency.
Text encoder. The text encoder operates on a transcribed
speech sequence of S tokens y ∈{1, ..., V +N}S, where V
is the text vocabulary size, N is the size of the vocabulary of
time tokens and S is the number of tokens in the transcribed
speech sequence. Note that the transcribed speech sequence
includes time tokens to input the temporal information from
the transcribed speech into the model.
An embedding
layer gs ∈R(V +N)×d embeds each token independently
and outputs semantic embeddings ys = gs(y) ∈RS×d.
Then a transformer encoder gt computes interactions in the
transcribed speech sequence and outputs S contextualized
speech embeddings yt = gt(ys) ∈RS×d.
Text decoder.
The text decoder generates the event se-
quence z by using the encoder embeddings, which are ob-
tained by concatenating the visual and speech embeddings
xt and yt. The text decoder is based on a causal transformer
decoder ht that cross-attends to the encoder outputs, and
at each autoregressive step k, self-attends to the previously
generated tokens ˆzt
<k to output a contextualized representa-
tion zt
k = ht(hs(ˆzt
<k), xt, yt) ∈Rd where hs ∈R(V +N)×d
is the decoder token embedding layer.
Then a language
modeling head hl ∈Rd×(V +N) predicts a probability dis-
tribution over the joint vocabulary of text and time tokens
in order to predict the next token in the event sequence, i.e.
4

Corrupted transcribed speech 
sequence
<3><5> Hey guys today I am 
going to [X] … <93> [Y] congratz!
Input video frames
[X] teach you how to ski [Y] 
<96> First slope [Z]
Recovered transcribed
speech sequence
Transcribed speech sequence
<3><5> Hey guys today I am 
going to teach you how to… 
<93><96> First slope, congratz!
Generative Task
Denoising Task
Vid2Seq
Vid2Seq
Figure 3. Pretraining tasks. To train Vid2Seq on unlabeled nar-
rated videos, we design two pretraining objectives. Top: gener-
ative objective, given visual inputs x only, the task is to generate
the transcribed speech sequence y. Bottom: denoising objective,
given visual inputs x and the corrupted speech sequence ˜y, the task
is to generate the sequence of recovered speech segments ¯y.
zl
k = hl(zt
k) ∈RV +N.
Text initialization. We initialize the text encoder and the
text decoder with T5-Base [78] which has been pretrained
on Web text corpora with a denoising loss.
Therefore
their implementation and parameters also closely follow
T5-Base, e.g. they use relative positional embeddings and
share their token embedding layer gs = hs ∈R(V +N)×d.
3.2. Training
In this Section, we describe how we leverage a large
amount of unlabeled narrated videos to train the previously
described dense event captioning model. We ﬁrst present
the pretraining method used to effectively train Vid2Seq
using cross-modal supervision in readily-available narrated
videos in Section 3.2.1 and Figure 3. Then we explain how
we ﬁnetune our architecture for various downstream tasks
including dense event captioning in Section 3.2.2.
3.2.1
Pretraining on untrimmed narrated videos
We wish to leverage narrated videos for pretraining as they
are easily available at scale [72,121]. However these videos
do not contain dense event captioning annotations. There-
fore we use as supervisory signal the transcribed speech
sentences and their corresponding timestamps. As speech
transcripts are not always visually grounded and often tem-
porally misaligned [32,43,71], we note that they only pro-
vide weak supervision.
Furthermore, speech transcripts
drastically differ from dense event captioning annotations.
For instance, in the YT-Temporal-1B dataset [121], a video
contains 120 speech sentences on average which is an order
of magnitude more than the number of events in standard
dense video captioning datasets [35,46,130]. Our Vid2Seq
model is particularly suitable for using such weak super-
vision as it constructs the speech sequence similarly as a
manually annotated event sequence, and jointly contextual-
izes the speech boundaries and semantic information on the
level of potentially minutes-long videos (see Section 3.1)
rather than at a shorter clip-level, enabling our model to
learn long-term relationships between the different speech
segments: in experiments we show that pretraining on en-
tire minutes-long videos is highly beneﬁcial.
We next describe the two proposed training objectives,
which are both based on a maximum likelihood objective.
Formally, given visual inputs x, encoder text sequence y and
a decoder target text sequence z, both objectives are based
on minimizing the following loss:
Lθ(x, y, z) = −
1
PL−1
k=1 wk
L−1
X
k=1
wk log pθ(zk+1|x, y, z1:k),
(1)
where L is the length of the decoder target sequence, wk is
the weight for k-th token in the sequence, which we set to
wk = 1 ∀k in practice, θ denotes the trainable parameters in
the model and pθ is the output probability distribution over
the vocabulary of text and time tokens.
Generative objective. This objective uses the transcribed
speech as a (pseudo-)supervisory signal to teach the decoder
to predict a sequence of events given visual inputs. Given
video frames x, which are fed to the encoder, the decoder
has to predict the transcribed speech sequence y (see Fig-
ure 3), which serves as a proxy dense event captioning an-
notation. Note that no text input is given to the encoder for
this task as using transcribed speech both as input and target
would lead the model to learn text-only shortcuts.
Denoising objective. As no text input is given to the en-
coder for the generative proxy task, the generative objective
only trains the visual encoder and the text decoder, but not
the text encoder. However when our model is used for dense
video captioning, the text encoder has a signiﬁcant impor-
tance as it encodes speech transcripts. Hence we introduce
a denoising objective that aims at jointly aligning the visual
encoder, the text encoder and the text decoder. Inspired by
T5 [78] in the text domain, we randomly mask spans of (text
and time) tokens in the transcribed speech sequence with a
probability P and an average span length M. The encoder
input is composed of the video frames x together with the
corrupted speech sequence ˜y, which contains sentinel to-
kens that uniquely identify the masked spans. The decoder
then has to predict a sequence ¯y constructed with the cor-
responding masked spans for each sentinel token, based on
visual inputs x and speech context ˜y (see Figure 3).
3.2.2
Downstream task adaptation
Our architecture and task formulation enables us to tackle
dense video captioning with a generic language modeling
training objective and inference procedure. Note that as a
by-product of our generic architecture, our model can also
be used to generate paragraphs about entire videos by sim-
ply removing the time tokens from the output sequence, and
can also be easily adapted to video clip captioning with the
same ﬁnetuning and inference recipe.
Finetuning. To ﬁnetune our model for dense video cap-
tioning, we use a maximum likelihood objective based on
5

the event sequence (see Equation 1). Given video frames
x and speech transcripts y, the decoder has to predict the
event sequence z.
Inference. The text decoder autoregressively generates the
event sequence by sampling from the model likelihood. In
practice, we use beam search as we ﬁnd that it improves
the captioning quality compared with argmax sampling or
nucleus sampling. Finally, the event sequence is converted
into a set of event predictions by simply reversing the se-
quence construction process.
4. Experiments
This section demonstrates the effectiveness of our pre-
trained Vid2Seq model and compares our method to the
state of the art. We ﬁrst outline our experimental setup in
Section 4.1. We then present ablation studies in Section 4.2.
The comparison to the state of the art in dense video cap-
tioning, video paragraph captioning and video clip caption-
ing is presented in Section 4.3. Next, we present results in a
new few-shot dense video captioning setting in Section 4.4.
Finally, we show qualitative results in Section 4.5.
4.1. Experimental setup
Datasets.
For pretraining, following prior work show-
ing the beneﬁts of pretraining on a diverse and large
dataset [122], we use the YT-Temporal-1B dataset [121],
which includes 18 million narrated videos collected from
YouTube. We evaluate Vid2Seq on three downstream dense
video captioning datasets: YouCook2 [130], ViTT [35] and
ActivityNet Captions [46]. YouCook2 has 2K untrimmed
videos of cooking procedures.
On average, each video
lasts 320s and is annotated with 7.7 temporally-localized
sentences. ViTT consists of 8K untrimmed instructional
videos. On average, each video lasts 250s and is annotated
with 7.1 temporally-localized short tags. ActivityNet Cap-
tions contains 20k untrimmed videos of various human ac-
tivities. On average, each video lasts 120s and is annotated
with 3.7 temporally-localized sentences. For video clip cap-
tioning, we use two standard benchmarks, MSR-VTT [109]
and MSVD [9]. For all datasets, we follow the standard
splits for training, validation and testing. Note that we only
use videos available on YouTube at the time of the work, re-
sulting in 10 to 20% less videos than in the original datasets.
Implementation details. We extract video frames at 1FPS,
and subsample or pad the sequence of frames to F frames
where we set F = 100. The text encoder and decoder se-
quence are truncated or padded to L = S = 1000 tokens.
Our model has 314M trainable parameters.
We use the
Adam optimizer [42]. We pretrain our model for 200,000
iterations with a batch size of 512 videos split on 64 TPU
v4 chips, which lasts a day. We sum both pretraining objec-
tives with equal weighting to get our ﬁnal pretraining loss.
Our Jax implementation is based on the Scenic library [19].
More details are included in Appendix Section B.
Pretraining input
YouCook2
ActivityNet
Untrimmed
Time tokens
S
C
F1
S
C
F1
1.
No pretraining
4.0
18.0
18.1
5.4
18.8
49.2
2.


5.5
27.8
20.5
5.5
26.5
52.1
3.


6.7
35.0
23.3
5.6
27.4
52.2
4.


7.9
47.1
27.3
5.8
30.1
52.4
Table 1.
Ablation showing the impact of using untrimmed
videos and adding time tokens during pretraining. When we
use untrimmed video-speech inputs, time information from tran-
scribed speech sentence boundaries is integrated via time tokens.
Evaluation metrics. For captioning, we use CIDEr [95]
(C) and METEOR [6] (M). For dense video captioning,
we follow the commonly used evaluation tool [46] which
calculates matched pairs between generated events and the
ground truth across IoU thresholds of {0.3, 0.5, 0.7, 0.9},
and compute captioning metrics over the matched pairs.
However, these metrics do not take into account the story
of the video. Therefore we also use SODA c [27] (S) for an
overall dense video captioning evaluation. To further isolate
the evaluation of event localization, we report the average
precision and average recall across IoU thresholds of {0.3,
0.5, 0.7, 0.9} and their harmonic mean, the F1 Score.
4.2. Ablation studies
The default Vid2Seq model predicts both text and time
tokens, uses both visual frames and transcribed speech as
input, builds on the T5-Base language model, and is pre-
trained on untrimmed videos from YT-Temporal-1B with
both the generative and denoising losses. Below we ablate
the importance of each of these factors on the downstream
dense video captioning performance by reporting results on
YouCook2 and ActivityNet Captions validation sets.
Pretraining on untrimmed narrated videos by exploit-
ing transcribed speech sentence boundaries. In Table 1,
we evaluate the effectiveness of our pretraining task for-
mulation that uses untrimmed videos and integrates sen-
tence boundaries of transcribed speech via time tokens.
In contrast, most video clip captioning pretraining meth-
ods [35,70,80] use short, trimmed, video-speech segments
for pretraining. We adapt this strategy in our model and ﬁnd
that it indeed yields signiﬁcant performance improvements
over the baseline that uses no video-text pretraining (row
2 vs row 1). However, larger improvements are obtained
by using untrimmed video-speech inputs (row 3 vs row 2).
Moreover, using time tokens to integrate time information
from transcribed speech drastically improves performance
(row 4 vs row 3). This shows the beneﬁts of exploiting sen-
tence boundaries of transcribed speech via time tokens and
of using untrimmed videos during pretraining. In Appendix
Section C.2, we show additional ablations to quantify how
the performance improves by pretraining on longer narrated
videos that contain more speech sentences.
Input modalities and pretraining objectives. In Table 2,
we analyze the importance of input modalities and pretrain-
6

Finetuning Input
Pretraining loss
YouCook2
ActivityNet
Visual
Speech
Generative Denoising
S
C
F1
S
C
F1
1.


No pretraining
3.0 15.6 15.4 5.4 14.2 46.5
2.


No pretraining
4.0 18.0 18.1 5.4 18.8 49.2
3.




5.7 25.3 23.5 5.9 30.2 51.8
4.




2.5 10.3 15.9 4.8 17.0 48.8
5.




7.9 47.1 27.3 5.8 30.1 52.4
Table 2. Effect of input modalities and pretraining losses.
Captioning Pretraining
YouCook2
ActivityNet
Recall Precision
F1
Recall Precision
F1
1.


17.8
19.4
17.7
47.3
57.9
52.0
2.


17.2
20.6
18.1
42.5
64.1
49.2
3.


25.7
21.4
22.8
52.5
53.0
51.1
4.


27.9
27.8
27.3
52.7
53.9
52.4
Table 3. Effect of joint captioning and localization on the lo-
calization performance. The variant that does not caption corre-
sponds to a localization-only variant that only predicts time tokens.
Language
Model
Pretraining
YouCook2
ActivityNet
# Videos Dataset
S
C
F1
S
C
F1
1.
T5-Small
15M
YTT
6.1 31.1 24.3
5.5
26.5
52.2
2.
T5-Base
∅
∅
4.0 18.0 18.1
5.4
18.8
49.2
3.
T5-Base
15K
YTT
6.3 35.0 24.4
5.1
24.4
49.9
4.
T5-Base
150K
YTT
7.3 40.1 26.7
5.4
27.2
51.3
5.
T5-Base
1M5
YTT
7.8 45.5 26.8
5.6
28.7
52.2
6.
T5-Base
1M
HTM
8.3 48.3
26.6
5.8
28.8
53.1
7.
T5-Base
15M
YTT
7.9 47.1
27.3
5.8
30.1
52.4
Table 4. Effect of language model size and pretraining data.
HTM: HowTo100M [72], YTT: YT-Temporal-1B [121].
ing tasks on the downstream dense video captioning perfor-
mance. The model with visual inputs only (no transcribed
speech as input) beneﬁts signiﬁcantly from pretraining with
the generative objective (row 3 vs row 1). This shows the
effectiveness of using the transcribed speech as a proxy an-
notation for dense video captioning pretraining. However,
this model is pretrained with visual inputs only and its per-
formance largely drops when it is ﬁnetuned with both visual
and transcribed speech inputs (row 4 vs row 3). With both
modalities, adding the denoising loss strongly beneﬁts our
model (row 5 vs rows 4 and 2). We conclude that the de-
noising objective beneﬁts multi-modal reasoning.
Effect of captioning on localization. In Table 3, we com-
pare the event localization performance of our model with
a localization-only variant that only predicts event bound-
aries. We ﬁnd that the model that jointly predicts event
boundaries and captions localizes better and beneﬁts more
from pretraining than the localization-only baseline (row 4
vs row 3), which demonstrates the importance of contextu-
alizing the noisy timestamps of the transcribed speech with
the speech semantic content during pretraining.
Model size and pretraining data. In Table 4, we show
that the language model size has a great importance on the
performance, as the model with T5-Base outperforms its
variant with T5-Small (row 7 vs row 1).
We also eval-
uate the importance of the size of the pretraining dataset
of narrated videos by constructing subsets such that larger
Method
YouCook2 (val)
ViTT (test)
ActivityNet (val)
S
C
M
S
C
M
S
C
M
MT [131]
—
6.1
3.2
—
—
—
—
9.3
5.0
ECHR [103]
—
—
3.8
—
—
—
3.2
14.7
7.2
PDVC [102]
4.4
22.7
4.7
—
—
—
5.4
29.0
8.0
UEDVC [125]
—
—
—
—
—
—
5.5
—
—
E2ESG [133]
—
25.0*
3.5
—
25.0
8.1
—
—
—-
Vid2Seq (Ours)
7.9
47.1
9.3
13.5 43.5
8.5
5.8
30.1
8.5
Table 5. Comparison to the state of the art for dense video cap-
tioning. * Results provided by the authors.
Method
YouCook2 (val)
ViTT (test)
ActivityNet (val)
Recall
Precision
Recall
Precision
Recall
Precision
PDVC [102]
—
—
—
—
55.4
58.1
UEDVC [125]
—
—
—
—
59.0
60.3
E2ESG [133]
20.7*
20.6*
32.2*
32.1*
—
—
Vid2Seq (Ours)
27.9
27.8
42.6
46.2
52.7
53.9
Table 6. Comparison to the state of the art for event localization.
* Results provided by the authors.
subsets include the smaller ones. We ﬁnd that scaling up
the size of the pretraining dataset is beneﬁcial, and that
our pretraining method yields important beneﬁts when only
using 150K narrated videos for pretraining (row 4). We
further show that our pretraining method generalizes well
to the HowTo100M dataset [72].
The model pretrained
on HowTo100M (row 6) actually achieves best results on
YouCook2, as these datasets are from a similar domain. Fi-
nally, we ablate the importance of the size and pretraining
of the visual backbone in Appendix Section C.2.
4.3. Comparison to the state of the art
Dense video captioning.
In Table 5, we compare our
approach to state-of-the-art dense video captioning meth-
ods using cross-entropy training 1 on the YouCook2, ViTT
and ActivityNet Captions datasets.
Vid2Seq sets new
state of the art on all three datasets.
In particular, our
method improves the SODA metric by 3.5 and 0.3 points on
YouCook2 and ActivityNet Captions over PDVC [102] and
UEDVC [125], respectively. Our method also outperforms
E2ESG [133] which uses in-domain text-only pretraining
on Wikihow. These results demonstrate the strong dense
event captioning ability of our pretrained Vid2Seq model.
Event localization. In Table 6, we evaluate the event local-
ization performance of our dense video captioning model in
comparison with prior work. On both YouCook2 and ViTT,
Vid2Seq outperforms prior work [133] tackling dense video
captioning as a single sequence generation task. However,
our model underperforms compared to PDVC [102] and
UEDVC [102] on ActivityNet Captions. We emphasize that
our approach integrates less prior knowledge about tempo-
ral localization than both these approaches, which include
task speciﬁc components such as event counters [102] or
separately train a model for the localization subtask [125].
1We do not include methods directly optimizing the test metric [20,73].
7

Vis2Seq
GT
Input 
Speech
An athlete is seen standing ready before a large track.
The woman throws a javelin off into the distance and is shown again
afterwards.
She throws her hands up to 
cheer and wraps herself in a 
flag.
A woman runs with a javelin. 
She throws it onto the field.
She throws a second javelin.
She waves to 
the crowd and 
holds up a flag.
Next Oh is Christina Oh Beck full most consistent off the top women javelin throwers
around at the moment.
Well, that's
another
very fine.
She's got over the years know what
major gold medals until now.
Christina Oh beg for 
what a wonderful record.
…
Input
Frames
Figure 4. Example of dense event captioning predictions of Vid2Seq on ActivityNet Captions validation set, compared with ground-truth.
Method
YouCook2 (val) ActivityNet (val-ae)
C
M
C
M
With Ground-Truth Proposals
VTransformer [131]
32.3
15.7
22.2
15.6
Transformer-XL [18]
26.4
14.8
21.7
15.1
MART [50]
35.7
15.9
23.4
15.7
GVDSup [128]
—
—
22.9
16.4
AdvInf [76]
—
—
21.0
16.6
PDVC [102]
—
—
27.3
15.9
With Learnt Proposals
MFT [107]
—
—
19.1
14.7
PDVC [102]
—
—
20.5
15.8
Vid2Seq (Ours)
50.1
24.0
28.0
17.0
Table 7. Comparison to the SoTA for video paragraph captioning.
Method
MSR-VTT (test)
MSVD (test)
C
M
C
M
ORG-TRL [126]
50.9
28.8
95.2
36.4
SwinBERT [63]
53.8
29.9
120.6
41.3
MV-GPT [80]
60.0
—∗
—
—
Vid2Seq (Ours)
64.6
30.8
146.2
45.3
Table 8. Comparison to the SoTA for video clip captioning. * Au-
thors conﬁrmed that they used an incorrect metric implementation.
Data
YouCook2
ViTT
ActivityNet
S
C
M
S
C
M
S
C
M
1.
1%
2.4
10.1
3.3
2.0
7.4
1.9
2.2
6.2
3.2
2.
10%
3.8
18.4
5.2
10.7
28.6
6.0
4.3
20.0
6.1
3.
50%
6.2
32.1
7.6
12.5
38.8
7.8
5.4
27.5
7.8
4.
100%
7.9
47.1
9.3
13.5
43.5
8.5
5.8
30.1
8.5
Table 9. Few-shot dense event captioning, by ﬁnetuning Vid2Seq
using a small fraction of the downstream training dataset.
Video paragraph captioning.
In Table 7, we com-
pare our approach to state-of-the-art video paragraph cap-
tioning methods on the YouCook2 and ActivityNet Cap-
tions datasets. Vid2Seq outperforms all prior methods on
both datasets, including the ones using ground-truth event
boundary proposals at inference time [18, 50, 76, 102, 128,
131], showing strong video paragraph captioning ability.
Video clip captioning. In Table 8, we compare our ap-
proach to state-of-the-art video clip captioning methods on
the MSR-VTT and MSVD datasets.
Vid2Seq improves
over prior methods on both datasets. We conclude that our
pretrained Vid2Seq model generalizes well to the standard
video clip captioning setting.
4.4. Few-shot dense video captioning
To further evaluate the generalization capabilities of our
pretrained Vid2Seq model, we propose a new few-shot
dense video captioning setting where we ﬁnetune Vid2Seq
using only a fraction of the downstream training dataset.
From Table 9, we observe important improvements when
using 10% compared to 1% of training data (row 2 vs 1).
In Appendix Section C.1 we further show that pretraining is
essential in this few-shot setting.
4.5. Qualitative examples
In Figure 4, we show an example of dense event caption-
ing predictions from Vid2Seq. This example shows that our
model can predict meaningful event boundaries and cap-
tions, and that the predicted captions and boundaries dif-
fer considerably from the transcribed speech input (show-
ing the importance of the visual tokens in the input). More
examples are provided in Appendix Section A.
5. Conclusion
We introduced Vid2Seq, a visual language model that
performs dense video captioning by generating a single se-
quence of tokens including both text and time tokens given
multi-modal inputs. We showed that Vid2Seq beneﬁts from
large-scale pretraining on unlabeled untrimmed narrated
videos by leveraging transcribed speech sentences and cor-
responding temporal boundaries. Vid2Seq achieves state-
of-the-art results on various dense event captioning datasets,
as well as multiple video paragraph captioning and standard
video clip captioning benchmarks. Finally, we believe the
sequence-to-sequence design of Vid2Seq has the potential
to be extended to a wide range of other video tasks such as
temporally-grounded video question answering [51,56,57]
or temporal action localization [16,67,123].
Acknowledgements.
The work was partially funded by a Google
gift, the French government under management of Agence Nationale de
la Recherche as part of the ”Investissements d’avenir” program, refer-
ence ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), the Louis Vuitton ENS
Chair on Artiﬁcial Intelligence, the European Regional Development Fund
under project IMPACT (reg. no. CZ.02.1.01/0.0/0.0/15 003/0000468). We
thank Anurag Arnab, Minsu Cho, Anja Hauth, Ashish Thapliyal, Bo Pang,
Bryan Seybold and the entire Ganesha team for helpful discussions.
8

References
[1] Vid2Seq project webpage.
https://antoyang.
github.io/vid2seq.html. 2
[2] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul
Natsev, George Toderici, Balakrishnan Varadarajan, and
Sudheendra Vijayanarasimhan.
Youtube-8m:
A large-
scale video classiﬁcation benchmark.
arXiv preprint
arXiv:1609.08675, 2016. 14
[3] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong
Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt:
Transformers for multimodal self-supervised learning from
raw video, audio and text. NeurIPS, 2021. 2
[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-
sch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. In NeurIPS,
2022. 1, 2
[5] Max Bain, Arsha Nagrani, G¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In ICCV, 2021. 2
[6] Satanjeev Banerjee and Alon Lavie.
Meteor: An auto-
matic metric for mt evaluation with improved correlation
with human judgments. In Proceedings of the acl workshop
on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization, 2005. 6
[7] Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue
Wang, and Yuexian Zou. Locvtp: Video-text pre-training
for temporal localization. In ECCV, 2022. 2
[8] Aman Chadha, Gurneet Arora, and Navpreet Kaloty. iPer-
ceive: Applying common-sense reasoning to multi-modal
dense video captioning and video question answering. In
WACV, 2021. 2
[9] David L Chen and William B Dolan. Collecting highly par-
allel data for paraphrase evaluation. In ACL, 2011. 2, 6,
14
[10] Shaoxiang Chen and Yu-Gang Jiang.
Towards bridging
event captioner and sentence localizer for weakly super-
vised dense event captioning. In CVPR, 2021. 2
[11] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-
offrey Hinton. Pix2seq: A language modeling framework
for object detection. In ICLR, 2022. 1, 3
[12] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J
Fleet, and Geoffrey Hinton. A uniﬁed sequence interface
for vision tasks. In NeurIPS, 2022. 3
[13] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-
vanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,
Adam Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI:
A jointly-scaled multilingual language-image model. arXiv
preprint arXiv:2209.06794, 2022. 1, 3
[14] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.
UNITER: Universal image-text representation learning. In
ECCV, 2020. 2
[15] Zhiyang Chen, Yousong Zhu, Zhaowen Li, Fan Yang, Wei
Li, Haixin Wang, Chaoyang Zhao, Liwei Wu, Rui Zhao,
Jinqiao Wang, et al. Obj2seq: Formatting objects as se-
quences with class prompt for visual tasks. In NeurIPS,
2022. 3
[16] Feng Cheng and Gedas Bertasius. TALLformer: Tempo-
ral action localization with long-memory transformer. In
ECCV, 2022. 8
[17] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying
vision-and-language tasks via text generation.
In ICML,
2021. 3
[18] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,
Quoc V Le, and Ruslan Salakhutdinov. Transformer-XL:
Attentive language models beyond a ﬁxed-length context.
In ACL, 2019. 8
[19] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab,
Matthias Minderer, and Yi Tay. Scenic: A jax library for
computer vision research and beyond. In CVPR, 2022. 6
[20] Chaorui Deng, Shizhe Chen, Da Chen, Yuan He, and Qi
Wu. Sketch, ground, and reﬁne: Top-down dense video
captioning. In CVPR, 2021. 1, 2, 7
[21] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin John-
son. RedCaps: Web-curated image-text data created by the
people, for the people. In NeurIPS Datasets and Bench-
marks, 2021. 2
[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-
age is worth 16x16 words: Transformers for image recog-
nition at scale. In ICLR, 2021. 4
[23] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan
Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu,
Yann LeCun, Nanyun Peng, et al. Coarse-to-ﬁne vision-
language pre-training with fusion in the backbone.
In
NeurIPS, 2022. 2
[24] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuo-
hang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan
Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of
training end-to-end vision-and-language transformers. In
CVPR, 2022. 2
[25] Victor Escorcia,
Fabian Caba Heilbron,
Juan Carlos
Niebles, and Bernard Ghanem. Daps: Deep action propos-
als for action understanding. In ECCV, 2016. 2
[26] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang
Wang, Lijuan Wang, and Zicheng Liu. VIOLET: End-to-
end video-language transformers with masked visual-token
modeling. arXiv preprint arXiv:2111.12681, 2021. 2
[27] Soichiro Fujita, Tsutomu Hirao, Hidetaka Kamigaito, Man-
abu Okumura, and Masaaki Nagata. SODA: Story oriented
dense video captioning evaluation framework. In ECCV,
2020. 6
[28] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,
and Jingjing Liu.
Large-scale adversarial training for
vision-and-language representation learning. In NeurIPS,
2020. 2
[29] Jiyang Gao, Zhenheng Yang, Kan Chen, Chen Sun, and
Ram Nevatia. Turn tap: Temporal unit regression network
for temporal action proposals. In ICCV, 2017. 2
9

[30] Lianli Gao, Zhao Guo, Hanwang Zhang, Xing Xu, and
Heng Tao Shen. Video captioning with attention-based lstm
and semantic consistency. IEEE Transactions on Multime-
dia, 2017. 2
[31] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xi-
aohu Qie, and Ping Luo. Bridging video-text retrieval with
multiple choice questions. In CVPR, 2022. 2
[32] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal
alignment networks for long-term video. In CVPR, 2022.
2, 5
[33] Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard
Ghanem. Fast temporal activity proposals for efﬁcient de-
tection of human actions in untrimmed videos. In CVPR,
2016. 2
[34] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,
Zicheng Liu, Yumao Lu, and Lijuan Wang.
Scaling up
vision-language pre-training for image captioning.
In
CVPR, 2022. 2
[35] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and
Radu Soricut. Multimodal pretraining for dense video cap-
tioning. In AACL-IJCNLP, 2020. 1, 2, 3, 4, 5, 6, 14, 17
[36] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu,
Dongmei Fu, and Jianlong Fu.
Seeing out of the box:
End-to-end pre-training for vision-language representation
learning. In CVPR, 2021. 2
[37] Vladimir Iashin and Esa Rahtu. A better use of audio-visual
cues: Dense video captioning with bi-modal transformer. In
BMVC, 2020. 1, 2
[38] Vladimir Iashin and Esa Rahtu. Multi-modal dense video
captioning. In CVPR Workshops, 2020. 2
[39] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana
Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,
and Tom Duerig.
Scaling up visual and vision-language
representation learning with noisy text supervision.
In
ICML, 2021. 2
[40] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel
Synnaeve, Ishan Misra, and Nicolas Carion.
MDETR
- modulated detection for end-to-end multi-modal under-
standing. In ICCV, 2021. 2
[41] Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-
and-language transformer without convolution or region su-
pervision. In ICML, 2021. 2
[42] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR, 2015. 6, 14
[43] Dohwan Ko, Joonmyung Choi, Juyeon Ko, Shinyeong Noh,
Kyoung-Woon On, Eun-Sol Kim, and Hyunwoo J Kim.
Video-text representation learning via differentiable weak
temporal alignment. In CVPR, 2022. 2, 5
[44] Alexander Kolesnikov, Andr´e Susano Pinto, Lucas Beyer,
Xiaohua Zhai, Jeremiah Harmsen, and Neil Houlsby.
Uvim: A uniﬁed modeling approach for vision with learned
guiding codes. In NeurIPS, 2022. 3
[45] Mahnaz Koupaee and William Yang Wang.
Wikihow:
A large scale text summarization dataset. arXiv preprint
arXiv:1810.09305, 2018. 3
[46] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Juan Carlos Niebles. Dense-captioning events in videos. In
ICCV, 2017. 1, 2, 4, 5, 6, 14
[47] Taku Kudo and John Richardson. Sentencepiece: A sim-
ple and language independent subword tokenizer and deto-
kenizer for neural text processing. In ACL, 2018. 4
[48] Jie Lei, Tamara L Berg, and Mohit Bansal. Detecting mo-
ments and highlights in videos via natural language queries.
In NeurIPS, 2021. 2
[49] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg,
Mohit Bansal, and Jingjing Liu. Less is more: ClipBERT
for video-and-language learning via sparse sampling.
In
CVPR, 2021. 2
[50] Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L
Berg, and Mohit Bansal. MART: Memory-augmented re-
current transformer for coherent video paragraph caption-
ing. In ACL, 2020. 2, 8, 14
[51] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.
TVQA: Localized, compositional video question answer-
ing. In EMNLP, 2018. 8
[52] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles,
and Steven CH Hoi.
Align and prompt:
Video-and-
language pre-training with entity prompts. In CVPR, 2022.
2
[53] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang,
and Ming Zhou. Unicoder-VL: A universal encoder for vi-
sion and language by cross-modal pre-training. In AAAI,
2020. 2
[54] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
BLIP: Bootstrapping language-image pre-training for uni-
ﬁed vision-language understanding and generation.
In
ICML, 2022. 2
[55] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
Shaﬁq Joty, Caiming Xiong, and Steven Chu Hong Hoi.
Align before fuse:
Vision and language representation
learning with momentum distillation. In NeurIPS, 2021.
2
[56] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng
Yu, and Jingjing Liu.
HERO: Hierarchical encoder
for video+language omni-representation pre-training.
In
EMNLP, 2020. 2, 8
[57] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen,
Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang,
William Yang Wang, et al. VALUE: A multi-task bench-
mark for video-and-language understanding evaluation. In
NeurIPS Track on Datasets and Benchmarks, 2021. 8
[58] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, et al.
Grounded
language-image pre-training. In CVPR, 2022. 2
[59] Wanhua Li, Zhexuan Cao, Jianjiang Feng, Jie Zhou, and
Jiwen Lu. Label2Label: A language modeling framework
for multi-attribute learning. In ECCV, 2022. 3
[60] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-
aowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,
Furu Wei, et al.
Oscar: Object-semantics aligned pre-
training for vision-language tasks. In ECCV, 2020. 2
[61] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and
Tao Mei. Jointly localizing and describing events for dense
video captioning. In CVPR, 2018. 2
10

[62] Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao
Luo, Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang,
and Rongrong Ji. Fast learning of temporal action proposal
via dense boundary generator. In AAAI, 2020. 2
[63] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe
Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swin-
BERT: End-to-end transformers with sparse attention for
video captioning. In CVPR, 2022. 1, 2, 8
[64] Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan,
Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao,
Rongcheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocen-
tric video-language pretraining. In NeurIPS, 2022. 2
[65] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei
Wen. BMN: Boundary-matching network for temporal ac-
tion proposal generation. In ICCV, 2019. 2
[66] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and
Ming Yang. BSN: Boundary sensitive network for temporal
action proposal generation. In ECCV, 2018. 2
[67] Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei
Zhang, Song Bai, and Xiang Bai. End-to-end temporal ac-
tion detection with transformer. In IEEE Transactions on
Image Processing, 2022. 8
[68] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViL-
BERT: Pretraining task-agnostic visiolinguistic representa-
tions for vision-and-language tasks. In NeurIPS, 2019. 2
[69] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi
Parikh, and Stefan Lee. 12-in-1: Multi-task vision and lan-
guage representation learning. In CVPR, 2020. 2
[70] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan
Duan, Tianrui Li, Xilin Chen, and Ming Zhou.
Uni-
ViLM: A uniﬁed video and language pre-training model for
multimodal understanding and generation. arXiv preprint
arXiv:2002.06353, 2020. 1, 6, 17
[71] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan
Laptev, Josef Sivic, and Andrew Zisserman. End-to-end
learning of visual representations from uncurated instruc-
tional videos. In CVPR, 2020. 2, 5
[72] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand
Tapaswi,
Ivan
Laptev,
and
Josef
Sivic.
HowTo100M: Learning a text-video embedding by watch-
ing hundred million narrated video clips. In ICCV, 2019. 2,
5, 7, 14
[73] Jonghwan Mun, Linjie Yang, Zhou Ren, Ning Xu, and Bo-
hyung Han. Streamlined dense video captioning. In CVPR,
2019. 2, 7
[74] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja
Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid.
Learning audio-video modalities from image captions. In
ECCV, 2022. 2
[75] Yingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. Video
captioning with transferred semantic attributes. In CVPR,
2017. 2
[76] Jae Sung Park, Marcus Rohrbach, Trevor Darrell, and Anna
Rohrbach. Adversarial inference for multi-sentence video
description. In CVPR, 2019. 2, 8
[77] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, 2021. 4, 17
[78] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,
and Peter J Liu. Exploring the limits of transfer learning
with a uniﬁed text-to-text transformer. JMLR, 2020. 1, 5
[79] Tanzila Rahman, Bicheng Xu, and Leonid Sigal. Watch,
listen and tell: Multi-modal weakly supervised dense event
captioning. In ICCV, 2019. 2
[80] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and
Cordelia Schmid.
End-to-end generative pretraining for
multimodal video captioning. In CVPR, 2022. 1, 2, 6, 8, 17
[81] Paul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid.
Look before you speak: Visually contextualized utterances.
In CVPR, 2021. 2
[82] Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong
Chen, Yu-Gang Jiang, and Xiangyang Xue. Weakly super-
vised dense video captioning. In CVPR, 2017. 2
[83] Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen,
Zhendong Niu, and Ming Zhou. Dense procedure caption-
ing in narrated instructional videos. In ACL, 2019. 2
[84] Zheng Shou, Dongang Wang, and Shih-Fu Chang. Tempo-
ral action localization in untrimmed videos via multi-stage
cnns. In CVPR, 2016. 2
[85] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-
laume Couairon, Wojciech Galuba, Marcus Rohrbach, and
Douwe Kiela. FLAVA: A foundational language and vision
alignment model. In CVPR, 2022. 2
[86] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: A simple
way to prevent neural networks from overﬁtting. JMLR,
2014. 15
[87] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai,
Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How
to train your vit? data, augmentation, and regularization in
vision transformers. In TMLR, 2022. 17
[88] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
Wei, and Jifeng Dai.
VL-BERT: Pre-training of generic
visual-linguistic representations. In ICLR, 2019. 2
[89] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy,
and Cordelia Schmid. VideoBERT: A joint model for video
and language representation learning. In ICCV, 2019. 2
[90] Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu, Huan
Yang, and Jianlong Fu.
Long-form video-language pre-
training with multimodal temporal contrastive learning. In
NeurIPS, 2022. 2
[91] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR, 2016. 15
[92] Hao Tan and Mohit Bansal.
LXMERT: Learning cross-
modality encoder representations from transformers.
In
EMNLP, 2019. 2
[93] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Es-
lami, Oriol Vinyals, and Felix Hill. Multimodal few-shot
learning with frozen language models. In NeurIPS, 2021. 2
11

[94] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In NeurIPS,
2017. 1, 4
[95] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. CIDEr: Consensus-based image description eval-
uation. In CVPR, 2015. 6
[96] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge,
Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xi-
aohu Qie, and Mike Zheng Shou.
All in one: Explor-
ing uniﬁed video-language pre-training.
arXiv preprint
arXiv:2203.07303, 2022. 2
[97] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Recon-
struction network for video captioning. In CVPR, 2018. 2
[98] Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong
Lin, Ying Shan, Xiaohu Qie, and Mike Zheng Shou.
Object-aware video-language pre-training for retrieval. In
CVPR, 2022. 2
[99] Jianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang,
Xiyang Dai, Zicheng Liu, Yumao Lu, and Lijuan Wang.
UFO: A uniﬁed transformer for vision-language represen-
tation learning. arXiv preprint arXiv:2111.10023, 2021. 2
[100] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong
Xu. Bidirectional attentive fusion with context gating for
dense video captioning. In CVPR, 2018. 1, 2
[101] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Unifying architectures, tasks, and modali-
ties through a simple sequence-to-sequence learning frame-
work. In ICML, 2022. 3
[102] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran
Cheng, and Ping Luo. End-to-end dense video captioning
with parallel decoding. In ICCV, 2021. 1, 2, 7, 8, 14
[103] Teng Wang, Huicheng Zheng, Mingjing Yu, Qian Tian, and
Haifeng Hu. Event-centric hierarchical representation for
dense video captioning. IEEE Transactions on Circuits and
Systems for Video Technology, 2020. 2, 7
[104] Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, and
William Yang Wang. Video captioning via hierarchical re-
inforcement learning. In CVPR, 2018. 2
[105] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. SimVLM: Simple visual language
model pretraining with weak supervision. In ICLR, 2022. 1
[106] Zixu Wang, Yujie Zhong, Yishu Miao, Lin Ma, and Lu-
cia Specia. Contrastive video-language learning with ﬁne-
grained frame sampling. In AACL-IJCNLP, 2022. 2
[107] Yilei Xiong, Bo Dai, and Dahua Lin. Move forward and
tell: A progressive generator of video descriptions.
In
ECCV, 2018. 8
[108] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,
Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and
Christoph Feichtenhofer.
VideoCLIP: Contrastive pre-
training for zero-shot video-text understanding. In EMNLP,
2021. 2
[109] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A
large video description dataset for bridging video and lan-
guage. In CVPR, 2016. 2, 6, 14
[110] Mengmeng Xu, Erhan Gundogdu, Maksim Lapin, Bernard
Ghanem, Michael Donoser, and Loris Bazzani.
Con-
trastive language-action pre-training for temporal localiza-
tion. arXiv preprint arXiv:2204.12293, 2022. 2
[111] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,
Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-
vancing high-resolution video-language representation with
large-scale video transcriptions. In CVPR, 2022. 2
[112] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Just ask: Learning to answer questions
from millions of narrated videos. In ICCV, 2021. 2
[113] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Learning to answer visual questions from
web videos. IEEE TPAMI, 2022. 2
[114] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,
and Cordelia Schmid. TubeDETR: Spatio-temporal video
grounding with transformers. In CVPR, 2022. 2
[115] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Zero-shot video question answering via
frozen bidirectional language models. In NeurIPS, 2022. 2
[116] Jianwei Yang, Yonatan Bisk, and Jianfeng Gao.
Taco:
Token-aware cascade contrastive learning for video-text
alignment. In ICCV, 2021. 2
[117] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,
Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.
Crossing the format boundary of text and boxes: Towards
uniﬁed vision-language modeling. In ECCV, 2021. 1, 3
[118] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua
Wu, and Haifeng Wang. ERNIE-ViL: Knowledge enhanced
vision-language representations through scene graph.
In
AAAI, 2020. 2
[119] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captioners are image-text foundation models. Transactions
on Machine Learning Research, 2022. 2
[120] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, et al.
Florence:
A new
foundation model for computer vision.
arXiv preprint
arXiv:2111.11432, 2021. 2
[121] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yan-
peng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack
Hessel, Ali Farhadi, and Yejin Choi. MERLOT Reserve:
Neural script knowledge through vision and language and
sound. In CVPR, 2022. 2, 3, 5, 6, 7, 14
[122] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,
Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.
MERLOT: Multimodal neural script knowledge models. In
NeurIPS, 2021. 2, 6
[123] Chenlin Zhang, Jianxin Wu, and Yin Li. Actionformer: Lo-
calizing moments of actions with transformers. In ECCV,
2022. 8
[124] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun
Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu
Yuan, Jenq-Neng Hwang, and Jianfeng Gao. GLIPv2: Uni-
fying localization and vision-language understanding. In
NeurIPS, 2022. 2
12

[125] Qi Zhang, Yuqing Song, and Qin Jin. Unifying event detec-
tion and captioning as sequence generation via pre-training.
In ECCV, 2022. 2, 7
[126] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin
Wang, Weiming Hu, and Zheng-Jun Zha. Object relational
graph with teacher-recommended learning for video cap-
tioning. In CVPR, 2020. 8
[127] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xi-
aoou Tang, and Dahua Lin. Temporal action detection with
structured segment networks. In ICCV, 2017. 2
[128] Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J
Corso, and Marcus Rohrbach. Grounded video description.
In CVPR, 2019. 2, 8, 14
[129] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-
son J Corso, and Jianfeng Gao. Uniﬁed vision-language
pre-training for image captioning and VQA. In AAAI, 2020.
2
[130] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards
automatic learning of procedures from web instructional
videos. In AAAI, 2018. 1, 2, 4, 5, 6, 14
[131] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard
Socher, and Caiming Xiong. End-to-end dense video cap-
tioning with masked transformer. In CVPR, 2018. 1, 2, 7,
8
[132] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo,
Xingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xi-
aoshuai Sun, and Rongrong Ji. SeqTR: A simple yet uni-
versal network for visual grounding. In ECCV, 2022. 3
[133] Wanrong Zhu, Bo Pang, Ashish Thapliyal, William Yang
Wang, and Radu Soricut. End-to-end dense video caption-
ing as sequence generation. In COLING, 2022. 2, 3, 7
13

Appendix
In this Appendix, we present the following additional
material:
(i) Additional qualitative examples of dense video cap-
tioning predictions (Section A).
(ii) Additional information about our experimental setup
(Section B);
(iii) Additional experimental results (Section C), including
an ablation on the importance of pretraining for few-
shot dense video captioning (Section C.1) and addi-
tional ablation studies in the standard fully-supervised
dense video captioning setting (Section C).
A. Qualitative examples of dense video cap-
tioning predictions
In Figure 4, we show qualitative results of dense event
captioning by our Vid2Seq model. Here in Figures 5 and 6 ,
we show additional results on examples from the YouCook2
and ActivityNet Captions datasets. These results show that
Vid2Seq can predict meaningful dense captions and event
boundaries in diverse scenarios, with or without transcribed
speech input, e.g. series of instructions in cooking recipes
(Figure 5) or actions in human sports or leisure activities
(ﬁrst three examples in Figure 6). The last example in Fig-
ure 6 illustrates a failure case where the model hallucinates
events that are not visually grounded such as ‘one man hats
off to the camera‘.
B. Experimental setup
In this section, we complement the information provided
in Section 4.1 about the datasets we use (Section B.1). We
also give additional implementation details (Section B.2).
B.1. Datasets
YT-Temporal-1B [121] consists of 18.821M unlabeled nar-
rated videos covering about 150 years of video content for
pretraining. Compared with HowTo100M [72], this dataset
was created to cover a wider range of domains and not only
instructional videos.
HowTo100M [72] consists of 1.221M unlabeled narrated
instructional videos covering about 15 years of video con-
tent for pretraining.
YouCook2 [130] has 1,790 untrimmed videos of cooking
procedures. On average, each video lasts 320s and is an-
notated with 7.7 temporally-localized imperative sentences.
The dataset is split into 1,333 videos for training and 457
videos for validation.
ViTT [35] consists of 7,672 untrimmed instructional videos
from the YouTube-8M dataset [2]. Compared to YouCook2,
ViTT was created to better reﬂect the distribution of in-
structional videos in the wild. On average, each video lasts
250s and is annotated with 7.1 temporally-localized short
tags. The dataset is split into 5,476, 1,102 and 1,094 videos
for training, validation and testing, respectively. Videos in
the validation and test sets are provided with multiple sets
of dense event captioning annotations. Following [35], we
treat each set of annotations as a single example during eval-
uation and discard videos with more than 3 sets of annota-
tions.
ActivityNet-Captions [46] contains 14,934 untrimmed
videos of various human activities.
Different from
YouCook2 and ViTT where most videos contain transcribed
speech content, we ﬁnd that 68% of videos in Activi-
tyNet Captions do not have transcribed narration. On av-
erage, each video lasts 120s and is annotated with 3.7
temporally-localized sentences.
The dataset is split into
10,009 and 4,925 videos for training and validation, respec-
tively. Videos in the validation set are provided with two
sets of dense video captioning annotations. Following prior
work [102], we use both sets of annotations for evaluation,
by computing the average of the scores over each set for
SODA c and by using the standard evaluation tool [46] for
all other dense event captioning metrics. For video para-
graph captioning, we follow [102] and report results on the
’val-ae’ split that includes 2,460 videos [50,128].
MSR-VTT [109] consists of 10,000 open domain video
clips. The duration of each video clip is between 10 and
30 seconds. 20 natural language descriptions are manually
annotated for each clip. The dataset is split into 6,513, 497
and 2,990 videos for training, validation and testing, respec-
tively.
MSVD [9] consists of 1,970 open domain video clips. The
duration of each video clip is between 10 and 30 seconds.
Each video clip has roughly 40 manually annotated cap-
tions. The dataset is split into 1,200, 100 and 670 videos
for training, validation and testing, respectively.
B.2. Implementation details
Architecture.
The visual temporal transformer encoder
f t, the text encoder gt and the text decoder ht all have
12 layers, 12 heads, embedding dimension 768, and MLP
hidden dimension of 2048. The text encoder and decoder
sequences are truncated or padded to L = S = 1000 to-
kens during pretraining, and S = 1000 and L = 256 tokens
during ﬁnetuning. At inference, we use beam search decod-
ing where we track the top 4 sequences and apply a length
normalization of 0.6.
Training.
We use the Adam optimizer [42] with β =
(0.9, 0.999) and no weight decay. During pretraining, we
use a learning rate of 1e−4, warming it up linearly (from
14

Input 
Speech
Input
Frames
Vis2Seq
GT
Cut the chicken.
Pound the 
chicken.
Whisk the eggs.
Trim off the 
excess fat of 
chicken breast and 
cut it into halves.
Cover the chicken in 
plastic wrap and 
pound it out.
Crack two large 
eggs into a bowl 
and whisk them
together.
Mix bread
crumbs
and 
parmesan 
cheese
together.
Add bread crumbs
grated parmesan 
cheese and italian
bread crumbs to a 
bowl.
Coat the 
chicken in the 
flour mixture 
the egg
mixture and 
then the bread
crumbs.
Coat the chicken in 
the flour mixture 
and then the bread
crumbs.
Add
oil to 
a pan.
Fry the 
chicken
in the 
pan.
Add marinara 
sauce and 
cheese on top 
of the chicken.
Fry the 
chicken in a 
pan with
oil.
Pour tomato sauce 
and mozzarella 
cheese on top of the 
chicken.
Bake the 
chicken in 
an oven.
Bake the chicken in an 
oven.
I'm going
to start 
off with
two
boneless
skinless
chicken
breasts
here.
I'm just
going to 
trim off 
the grisly
parts and 
the 
excess
fat 
maybe
some of 
the skin 
that's left
over on 
there.
I've got a piece
of wax paper
here and I put 
that onto my
cutting board
[…] and I'm
going to pound 
out my breast
halves until
they are about 
1/2 an inch
thicker.
…
…
The 
first 
thing
I'm
going
to 
need is
an egg
wash.
So I'm
going to 
take two
large eggs
and crack 
those into
a bowl 
and if you
get any
shells in 
there, be
sure to get
those […]
…
Now, I'm
using my
homema
de Italian
bread
crumbs
here.
…
I'm just
going to 
mix this
together
and now
we can 
start 
breading
our
chicken.
Now, 
the 
breading
process 
is really
simple 
on this
you just
want to 
take one 
of your
[…] 
…
I've got my small
cast-iron skillet on 
medium-high heat
here and I'm going to 
put in about a quarter 
of an inch or so of 
extra virgin olive oil
into the bottom of 
that and I'm going to 
let that come up to 
temperature and then
I'm going to start 
frying up my chicken
pieces.
…
We're
going to 
be baking
these and 
that will
finish 
cooking 
them.
…
And if you'd
like to 
follow me 
on Google 
Plus 
Facebook 
and/or 
Pinterest all 
my links 
will be in 
the 
description 
box.
Input 
Speech
Input
Frames
Vis2Seq
GT
Finely chop a 
cabbage to small
pieces.
Add 20g salt
caraway seeds
juniper berries
and dill.
Massage the cabbage
with the seasoning.
Take a large cabbage
into a bowl and chop 
it into small pieces.
Add some salt to 
the cabbage and 
mix.
Add caraway seeds juniper
berries and dill tips to the 
cabbage and mix.
Put the mixture in a 
jar and press firm to 
the bottom.
Place the 
mixture 
into a 
glass jar.
Seal the jar 
and put in 
dark place 
for 4 weeks.
Seal the 
jar.
And 
today
what I 
want to 
do is
share one 
of my
favorite 
recipes
and in 
my
opinion
[…]
So then
we're
going
to chop 
the 
cabbag
e you
can use 
a 
mandol
in to do 
this.
…
…
So the 
rule of 
thumb
with salt
is that you
want
about 2.5 
to 3 
percent of 
the weight
of the 
cabbage.
…
So now
we're going
to get in 
really mix 
that salt into
the cabbage
which is
going to take
about ten
minutes so
really work
it it's a good 
workout.
…
And after
that it can be
popped into
the fridge or 
popped into
another jar 
and enjoyed, 
you know, as 
a condiment 
once a day
[…]
There 
we go.
So in 
this
case, 
we've
got
about 
20 
grams.
…
So it's gonna
be really well
covered and 
obviously it's
important to 
wash your
hands before
you do this
that seems
like a pretty
obvious thing
to do but […]
So no 
oxygen
that's
that's
the 
idea.
…
If you do have 
fresh dill, I 
would
recommend
using that now
for the fun 
part, what you
want to do is
we really need
to release all 
of the juices
from this
cabbage
…
Mix 
flour
salt and 
pepper
together
.
Place 
the 
chicken
in a 
baking
dish.
Figure 5. Examples of dense event captioning predictions of Vid2Seq on the validation set of YouCook2, compared with ground-truth.
0) for the ﬁrst 1000 iterations, and keeping it constant for
the remaining iterations. During ﬁnetuning, we use a learn-
ing rate of 3e−4, warming it up linearly (from 0) for the
ﬁrst 10% of iterations, followed by a cosine decay (down
to 0) for the remaining 90%. During ﬁnetuning, we use a
batch size of 32 videos split on 16 TPU v4 chips. We ﬁne-
tune for 40 epochs on YouCook2, 20 epochs on ActivityNet
Captions and ViTT, 5 epochs on MSR-VTT and 10 epochs
on MSVD. We clip the maximum norm of the gradient to
0.1 during pretraining, and 1 during ﬁnetuning. For data
augmentation, we use random temporal cropping. For reg-
ularization, we use label smoothing [91] with value 0.1 and
dropout [86] with probability 0.1.
C. Experiments
In this section, we provide additional experiments that
complement the results presented in Section 4.
We ﬁrst
show the importance of pretraining in our proposed few-
shot setting in Section C.1. Then we provide additional
15

Ø
Ø
Ø
Ø
Ø
Ø
Ø
A weightlifter is
standing on a 
stage.
He lifts the barbell before dropping it.
He jumps up and down in excitement.
A very strong
man is shown in a 
competition
He lifts a very heavy weight over his head.
He then drops the weight to the ground before shaking
his hands.
Ø
Vis2Seq
GT
Input 
Speech
Input
Frames
Ø
Ø
Ø
Ø
Ø
Ø
Ø
Ø
Input 
Speech
Input
Frames
Vis2Seq
GT
A group of children are seen
swimming in a pool.
The kids hit a ball back and forth in the water.
A picture of a sky is shown and leads into a 
group of boys playing a game of water polo.
The camera pans around a small group of kids playing and then a man 
chases a ball around.
The boys continue playing and one man hats off to the 
camera.
Ø
Ø
Ø
Ø
Ø
Ø
Ø
Ø
Input 
Speech
Vis2Seq
GT
A man walks up 
to parallel bars 
while spectators, 
competitors, and 
officials are in the 
background.
The man performs a routine on the parallel bars.
The man finishes his routine and dismounts.
A man walks up to a 
set of uneven bars.
He mounts the bars, then spins himself around.
He does a handstand, then dismounts.
Input
Frames
They fight over the ball, trying to get it into the goal.
Ø
Ø
Ø
Ø
Ø
Ø
Ø
Ø
Input 
Speech
Vis2Seq
GT
A man is seen looking at the camera and leads into
him playing a poker game with others.
One man deals cards and chips while
speaking to one another.
They continue playing and speaking to one another.
A man is sitting behind a table playing
poker.
He deals cards to the people, then he puts them on the table.
The man puts the 
cards on the table, 
and puts the chips 
in the middle.
Input
Frames
Figure 6. Examples of dense event captioning predictions by Vid2Seq on the validation set of ActivityNet Captions, compared with ground-
truth. The ﬁrst three examples show successful predictions, while the last example illustrates a failure case where the model hallucinates
events that are not visually grounded (‘one man hats off to the camera‘). Note that in all of these videos, there is no transcribed speech.
ablation studies in the standard fully-supervised setting in
Section C.2, where we ablate various factors including pre-
training on long narrated videos, the time tokenization pro-
cess and the number of time tokens, the sequence construc-
tion process, the temporal positional embeddings and the
initialization of the language model.
C.1. Importance of pretraining in few-shot settings
In Section 4.2, we show the beneﬁts of our pretrain-
ing method in the fully-supervised setting, i.e. when using
100% of the downstream training dataset. In Table 10, we
further show that our pretraining method has a considerable
16

Data
Pretrain
YouCook2
ViTT
ActivityNet
S
C
M
S
C
M
S
C
M
1.
1%

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.1
2.
1%

2.4 10.1 3.3
2.0
7.4
1.9
2.2
6.2
3.2
3.
10%

0.1
0.0
0.2
3.3
0.4
3.3
3.4
11.9 4.6
4.
10%

3.8 18.4 5.2
10.7 28.6
6.0
4.3
20.0 6.1
5.
50%

1.8
8.5
2.4
6.5
18.7
3.9
4.6
13.1 6.3
6.
50%

6.2 32.1 7.6
12.5 38.8
7.8
5.4
27.5 7.8
7. 100%

4.0 18.0 4.6
7.9
21.2
6.2
5.4
18.8 7.1
8. 100%

7.9 47.1 9.3
13.5 43.5
8.5
5.8
30.1 8.5
Table 10. Impact of our pretraining on few-shot dense event
captioning, by ﬁnetuning Vid2Seq using a small fraction of the
downstream training dataset.
Max number
of narrations
YouCook2
ActivityNet
S
C
F1
S
C
F1
1.
No pretraining
4.0
18.0
18.1
5.4
18.8
49.2
2.
1
6.0
32.1
22.1
5.1
22.9
48.1
3.
10
6.5
34.6
23.6
5.4
27.1
50.3
4.
∞
7.9
47.1
27.3
5.8
30.1
52.4
Table 11. Ablation showing the importance of pretraining on
long narrated videos, by varying the maximum number of narra-
tion sentences that a randomly cropped video can cover. ∞means
the cropping is unrestricted and can sample arbitrarily long videos.
Pretraining Data
Model
YouCook2
ActivityNet
S
C
F1
S
C
F1
1.
ImageNet
ViT-B/16
6.6
40.2
24.3
4.5
17.2
49.3
2.
CLIP
ViT-B/16
7.7
46.3
26.5
5.6
28.4
51.7
3.
CLIP
ViT-L/14
7.9
47.1
27.3
5.8
30.1
52.4
Table 12. Ablation on the pretraining data and model size of
the visual backbone f s.
importance in the few-shot setting deﬁned in Section 4.4,
i.e. when using a smaller fraction of the downstream train-
ing dataset. In particular, our pretraining method enables
our Vid2Seq model to have a non zero performance when
using only 1% of the downstream training dataset (rows 1
and 2).
C.2. Additional ablation studies
We here complement ablation studies reported in Sec-
tion 4.2, using the same default settings, evaluation metrics
and downstream datasets.
Pretraining on long narrated videos.
In Table 1, we
show the beneﬁts of pretraining on untrimmed videos in
comparison with the standard practice of pretraining on
short, trimmed, video-speech segments [35, 70, 80]. In Ta-
ble 11, we further evaluate the importance of sampling long
narrated videos during pretraining. By default, at each train-
ing iteration, we randomly temporally crop each narrated
video without constraints, resulting in a video that can span
over hundreds of transcribed speech sentences. We here
Tokenization
N
YouCook2
ActivityNet
S
C
F1
S
C
F1
1.
Absolute
20
0.3
0.2
0.9
3.2
23.0
23.1
2.
Absolute
100
3.5
25.7
12.0
4.8
25.5
41.5
3.
Absolute
500
7.9
39.8
24.3
5.4
28.1
48.6
4.
Relative
20
7.2
39.6
23.7
5.6
29.0
49.4
5.
Relative
100
7.9
47.1
27.3
5.8
30.1
52.4
6.
Relative
500
7.2
40.0
25.0
5.7
28.6
52.5
Table 13. Ablation on time tokenization (relative or absolute)
and the number of time tokens N.
Dot symbol
between segments
Time tokens
Position
YouCook2
ActivityNet
S
C
F1
S
C
F1
1.

After text
7.9
48.3
26.7
5.6
29.8
51.1
2.

After text
8.3
50.9
26.2
5.7 30.4
51.8
3.

Before text
8.0
50.0
27.3
5.6
28.2
50.7
4.

Before text
7.9
47.1
27.3
5.8 30.1
52.4
Table 14. Ablation on the sequence construction process.
evaluate a baseline that constrains this cropping process
such that the cropped video only spans over a given max-
imum number of narration sentences. Even with a maxi-
mum of 10 narration sentences, this baseline signiﬁcantly
underperforms our model trained in default settings where
we sample longer untrimmed narrated videos (rows 1, 2 and
3). This demonstrates that our model beneﬁts from pretrain-
ing on long narrated videos.
Visual features.
In Table 4, we show the beneﬁts of scal-
ing up the size of the pretraining dataset of narrated videos
and the size of the language model. In Table 12, we further
analyze the importance of the pretraining dataset and size of
the visual backbone f s. We ﬁnd that CLIP pretraining [77]
considerably improves over ImageNet pretraining [87] with
the same ViT-B/16 visual backbone model (row 2 vs 1).
Furthermore, scaling up the visual backbone size from ViT-
B/16 to ViT-L/14 brings additional improvements (row 3 vs
2).
Time tokenization and number of time tokens.
In Ta-
ble 13, we further ablate the time tokenization process pre-
sented in Section 3.1. Our default time tokens represent rel-
ative timestamps in a video, as we quantize a video of dura-
tion T into N equally-spaced timestamps. Another possibil-
ity is to use time tokens that represent absolute timestamps
in the video, i.e. the k-th token represents the k-th second in
the video. For both these variants, we vary the number of
time tokens N. For the relative time tokens, increasing N
makes the quantization more ﬁne-grained but also spreads
the data into more time tokens. On the other hand, for the
absolute time tokens, increasing N increases the video du-
ration that the time tokens can cover. We ﬁnd that the best
17

Temporal
embeddings
YouCook2
ActivityNet
S
C
F1
S
C
F1
1.

6.8
42.0
24.9
5.3
27.0
50.6
2.

7.9
47.1
27.3
5.8
30.1
52.4
Table 15. Ablation on the temporal positional embeddings.
Language Model
Initialization
Video-text
Pretraining
YouCook2
ActivityNet
S
C
F1
S
C
F1
1.


0.9
4.2
7.6
4.3
23.7
41.2
2.


4.0
18.0
18.1
5.4
18.8
49.2
3.


8.8
51.3
28.4
5.7
28.7
51.2
4.


7.9
47.1
27.3
5.8
30.1
52.4
Table 16. Ablation on language model initialization and pre-
training.
dense video captioning results are obtained with the relative
time tokens and N = 100 time tokens (row 5).
Sequence construction.
In Table 14, we further ablate the
sequence construction process presented in Section 3.1. Our
default sequence inserts the start and end time tokens of
each segment before its corresponding text sentence. An-
other possibility is to insert time tokens after each corre-
sponding text sentence. We ﬁnd that both variants achieve
similar results (rows 2 and 4), with the default sequence
(row 4) resulting in slightly higher event localization per-
formance (F1 Score) but slightly lower dense captioning re-
sults overall. Furthermore, we observe that the dot symbols
indicating the separation between different events have low
importance (rows 1 and 2, rows 3 and 4).
Temporal positional embeddings.
In Table 1, we show
that time tokens in the speech sequence provide temporal
information about the speech transcript to our model. In
Table 15, we also evaluate the importance of the temporal
positional embeddings which communicate temporal infor-
mation from the visual stream to our model. We ﬁnd that
these temporal embeddings are beneﬁcial (row 2 vs 1).
Language model initialization and pretraining.
In Ta-
ble 4, we show the beneﬁts of using T5-Base instead of
T5-Small. In Table 16, we further investigate the impor-
tance of initializing the language model from weights pre-
trained on Web text. Without pretraining on narrated videos,
we ﬁnd that text-only initialization is helpful (rows 1 and
2). Interestingly, after pretraining on narrated videos, we
ﬁnd that text-only initialization has little importance (rows
3 and 4), as it slightly improves the performance on Activ-
ityNet Captions while resulting in a slight drop of perfor-
mance on YouCook2. We believe that this may be because
of the domain gap between Web text and the imperative-
style dense captions in YouCook2, which are more similar
to transcribed speech in YT-Temporal-1B.
18

