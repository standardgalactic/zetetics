The Quantization Model of Neural Scaling
Eric J. Michaud∗1, 2, Ziming Liu1, 2, Uzay Girit3, and Max Tegmark1, 2, 4
1Department of Physics, MIT
2NSF AI Institute for AI and Fundamental Interactions
3Department of EECS, MIT
4Center for Brains, Minds and Machines, MIT
Abstract
We propose the Quantization Model of neural scaling laws, explaining both the
observed power law dropoﬀof loss with model and data size, and also the sudden
emergence of new capabilities with scale. We derive this model from what we call the
Quantization Hypothesis, where learned network capabilities are quantized into discrete
chunks (quanta). We show that when quanta are learned in order of decreasing use
frequency, then a power law in use frequencies explains observed power law scaling
of loss. We validate this prediction on toy datasets, then study how scaling curves
decompose for large language models. Using language model internals, we auto-discover
diverse model capabilities (quanta) and ﬁnd tentative evidence that the distribution
over corresponding subproblems in the prediction of natural text is compatible with
the power law predicted from the neural scaling exponent as predicted from our theory.
1
Introduction
In the aggregate, larger neural networks trained on more data perform better than smaller
neural networks trained on less data, in a predictable way. Across a range of studies, mean
test loss has been observed to decrease as a power law in both the number of network
parameters (L ∝N −αN ) and the number of training samples (L ∝D−αD) (Hestness et al.
2017; Rosenfeld et al. 2019; Kaplan et al. 2020; Henighan et al. 2020; Gordon et al. 2021; Zhai
et al. 2022; Hoﬀmann et al. 2022). Although aggregate performance changes predictably
with scale, when particular capabilities are examined, larger models often have emergent
abilities, i.e., unexpected and qualitatively diﬀerent behavior than smaller models (Wei et
al. 2022). Understanding both facets of scaling – the predictable power law decrease in loss
and the emergence of new capabilities at scale – is not just of theoretical interest, but highly
relevant to the near-term future of deep learning (Ganguli et al. 2022).
Understanding
the precise way in which larger models are diﬀerent from smaller ones is entangled with
basic questions about what deep neural networks are doing internally and whether they will
continue to improve with scale.
Recent studies of the internal workings of neural networks have found a variety of im-
pressive algorithms learned by gradient descent (Olah et al. 2020; Olsson et al. 2022; Nanda
et al. 2023). As more work is put into understanding the internal computations performed
by neural networks (the task of so-called mechanistic interpretability), we may ﬁnd more and
more “circuits” (Elhage et al. 2021) in models – intelligible computations for accomplish-
ing prediction in speciﬁc contexts. A natural question is whether such circuits are learned
universally across models with diﬀerent random initializations and across scales. Olsson
et al. (2022) ﬁnd evidence for universality of “induction heads”, a type of circuit that may
underlie in-context learning. In this paper, we will put forth the Quantization Hypothesis,
∗ericjm@mit.edu
1
arXiv:2303.13506v1  [cs.LG]  23 Mar 2023

...
...
...
...
      ...
...
ents his famous tonadas, a genre of the Venezuelan plains folk music.

Track listing
01- Mi Querencia (Simón Díaz)
02- Tonada De Luna Llena (Simón Díaz)
03- Sabana (José Salazar/Simón Díaz)
04- Caballo Viejo (Simón Díaz)
05- Todo Este Campo Es Mío (Simón Díaz)
06- La Pena Del Becerrero (Simón Díaz)
07

sis supplied.) Appealing from that order, the city asserts (1) 
plaintiffs have no standing or right to maintain the action; (2) that the 
proposed road was in an undedicated part of the park; (3) that the 
proposed road was an access road and not a through street or part of the 
city's street system; (4

  4. _Introduction_
  5. Chapter 1: What Is Trust?
  6. Chapter 2: Trust Brings Rest
  7. Chapter 3: Who Can I Trust?
  8. Chapter 4: The Folly of Self-Reliance
  9. Chapter 5: Trust God and Do Good (Part 1)
  10. Chapter 6: Trust God and Do Good (Part 2)
  11. Chapter 7: At All Times
  12. Chapter 8


gn of noncavitated lesion seen only when the tooth is dried; 2 = 
visible noncavitated lesion seen when wet and dry; 3 = microcavitation in 
enamel; 4 = noncavitated lesion extending into dentine seen as an 
undermining shadow; 5 = small cavitated lesion with visible dentine: less 
than 50% of surface; 6

DynamicKey><Action>F1</Action><Label>F1</Label></DynamicKey>
        <DynamicKey><Action>F2</Action><Label>F2</Label></DynamicKey>
        <DynamicKey><Action>F3</Action><Label>F3</Label></DynamicKey>
        <DynamicKey><Action>F4</Action><Label>F4</Label></DynamicKey>
        <DynamicKey><Action>F5

    GetPrepareVoteMsg       = 0x07
    PrepareVotesMsg         = 0x08
    GetQCBlockListMsg       = 0x09
    QCBlockListMsg          = 0x0a
    GetLatestStatusMsg      = 0x0b
    LatestStatusMsg         = 0x0c
    PrepareBlockHashMsg     = 0x0d
    GetViewChangeMsg        = 0x0e
    PingMsg                 = 0x0f
...
...
...
        
...
...
C REGRESSION.
THE GOALS OF THIS VIDEO ARE 
TO PERFORM QUADRATIC REGRESSION
ON THE TI84 GRAPHING CALCULATOR,
DETERMINE HOW WELL THE 
REGRESSION MODEL FITS THE DATA,
AND THEN MAKE PREDICTIONS 
USING THE REGRESSION EQUATION.
IN STATISTICS, 
REGRESSION ANALYSIS INCLUDES
ANY TECHNIQUES USED FOR MODELING \n

ump is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# creddump is distributed in the hope that it will be useful,\n

  *
Pursuant to 5TH CIR. R. 47.5, the court has determined
that this opinion should not be published and is not precedent
except under the limited circumstances set forth in 5TH CIR.\n


 
files (the
// "Software"), to deal in the Software without restriction, including
// without limitation the rights to use, copy, modify, merge, publish,
// distribute, sublicense, and/or sell copies of the Software, and to 
permit
// persons to whom the Software is furnished to do so, subject to the\n

<!--
/**
 * Copyright (c) 2019, The Android Open Source Project
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.\n


f maturity and an underdeveloped
sense of responsibility, leading to recklessness, impul-
sivity, and heedless risk-taking.... Second, children
are more vulnerable... to negative influences and
outside pressures, including from their family and
peers; they have limited contro[l] over their own envi-\n

quantum for numerical sequence continuation 
(examples from cluster 50)
“Quanta” of LLM capabilities auto-discovered in natural text
quantum for predicting newlines to maintain text width 
(examples from cluster 100)
Figure 1: From network internals, we auto-discover quanta – discrete units of model capabil-
ity – for the task of language modeling. Samples from the left cluster all involve continuing
a numerical sequence. Samples from the right cluster involve predicting a newline in order
to maintain text width. We indicate the token which was predicted from the context be-
fore it with a red highlight. We indicate the prediction of a newline with a highlighted \n
character. See Section 4.3 for explanation.
a set of conjectures about the universality of computations performed across model scales
and about how properties of the data distribution produce power law neural scaling.
In particular, we hypothesize that to many prediction problems, there corresponds a
universal and discrete set of computations which are instrumental for reducing loss, and that
model performance is determined by which of these computations are successfully learned.
We call these basic building blocks of model performance the quanta. We then argue that an
intrinsic power law distribution in how frequently the quanta are useful for prediction leads
to a power law marginal improvement in loss from learning additional quanta. If the eﬀect
of scaling is to simply learn more quanta, then this leads to power law neural scaling. Under
the Quantization Hypothesis, neural scaling exponents are determined by the exponent in
a power law distribution over subtasks in data. We describe this Quantization Model of
neural scaling power laws in detail in Section 2.
As a proof of concept, in Section 3 we construct toy datasets consisting of many subtasks,
and ﬁnd that power law neural scaling emerges according to the Quantization Model. We
2

then investigate scaling in large language models in Section 4. We ﬁrst analyze how power
law scaling in mean loss decomposes into scaling on individual tokens, and ﬁnd diverse be-
havior. Since it is unclear how to break up natural language prediction into quanta a priori,
we develop a method which we call QDG (for “quanta discovery with gradients”) based on
spectral clustering with model gradients, to sort language model inputs into clusters. With
this method, we auto-discover diverse and abstract language model behaviors, some of which
we show in Figure 1. We ﬁnd that the distribution over these auto-discovered clusters in
natural text roughly follows the power law our theory would expect given the empirical
scaling exponent, though bias inherent in our clustering algorithm makes our measurement
fairly uncertain.
If correct, the Quantization Hypothesis could have many implications for understanding
neural networks.
If model performance can be understood in terms of the presence or
absence of a discrete set of computations in models, then we may be able to mechanistically
understand large neural networks by enumerating the quanta they learn. Furthermore, we
may be able to predict when certain capabilities will emerge at scale by estimating the
frequency at which the relevant quanta for that capability are useful for prediction in the
natural distribution of data.
2
Theory
Consider the task of modeling the distribution of text on the internet. Successful prediction
requires an immense amount of knowledge, and the ability to perform diverse computa-
tions, due to the immense complexity and diversity of the world and therefore of human
language. For instance, in order to predict what word will come next in a conversation
between two physicists, one must “know” much about physics. In order to continue the
text “2534 + 7261 = ”, one must be able to perform arithmetic (for large enough num-
bers, memorization becomes a highly ineﬃcient strategy). A great many distinct types of
computations are present in the world in the processes that produce text, and so predicting
text requires those computations to be present in our models.
In this paper, we conjecture the Quantization Hypothesis:
QH1 Many natural prediction problems involve a discrete set of compu-
tations which are natural to learn and instrumental for reducing
loss. We call these “quanta”. Model performance is determined
by which quanta have been learned.
QH2 Some abilities are more useful for reducing loss than others, lead-
ing to a natural ordering of the quanta.
We call the ordered
quanta the Q Sequence.
Optimally trained networks should
therefore learn the quanta in that order. The eﬀect of scaling is
to learn more of the quanta in the Q Sequence, so scaling perfor-
mance is simply determined by how many quanta are successfully
learned.
QH3 The frequencies at which the quanta are used for prediction drop
oﬀas a power law.
We will show that together these result in power law neural scaling. The power law
governing the frequency that the quanta are used from QH3 will determine the exponent of
neural scaling laws. Note that we use the word “quanta” to refer interchangeably to both
3

model behavior (indivisible units of model capability) and to the corresponding computa-
tions implemented by the model which enable that behavior.
We model the Quantization Hypothesis as follows. Let q denote a bit string whose kth bit
qk = 1 if the kth quantum in the Q Sequence has been learned, and qk = 0 otherwise. QH1
implies that the mean loss L is simply a function of q. QH2 implies that when n ≡P
k qk
quanta have been learned, we have qk = 1 for k ≤n. Let Ln denote the mean loss in this
case.
From QH3, we have that the kth quantum beneﬁts prediction on a randomly chosen
sample with probability
pk =
1
ζ(α + 1)k−(α+1) ∝k−(α+1)
(1)
for a Zipf power law α > 0, where ζ(s) ≡P∞
k=1 k−s. Let us also assume that learning the
kth quantum reduces average loss from bk before it is learned to ak after it is learned on the
samples where it is utilized.
If ak and bk are k-independent (ak = a, bk = b), then a model that has learned the ﬁrst
n quanta will have expected loss
Ln
=
n
X
k=1
apk +
∞
X
k=n+1
bpk =
∞
X
k=1
apk +
∞
X
k=n+1
(b −a)pk
≈
a +
b −a
ζ(α + 1)
Z ∞
n
k−(α+1)dk = a +
b −a
αζ(α + 1)n−α.
(2)
In other words, L∞= a and (Ln −L∞) ∝n−α is a power law.
In Appendix A, we provide analogous derivations for other assumptions for ak and bk,
including the case where bk ∝−log pk, the entropy for a baseline model whose predictions
use no other aspects of the data besides token frequencies (assuming that quanta involve
the prediction of a particular token). Interestingly, we ﬁnd that the power law prediction
is quite robust, in the sense that the broad range of assumptions we explore all produce
curves (Ln −L∞) that are exact or approximate power laws — the latter include a small
logarithmic correction.
An implicit assumption above is that all quanta are what we will refer to as monogenic,
meaning that token predictions rely on at most a single quantum, akin to how monogenic
traits in biology (e.g. cystic ﬁbrosis) depend on a single gene. Many real-world prediction
problems are likely to involve both monogenic and polygenic quanta, a topic we explore
in Section 4.2.
When all learned quanta are monogenic, the expected loss (which involves an average over
all predicted tokens) transforms into an average over quanta, by simply grouping together all
tokens predicted using each quantum, and summing their token probabilities to obtain the
quantum use probabilities pk discussed above. A rigorous generalization of our formalism
to the polygenic case is an interesting challenge for future work. However, it should be
noted that polygenic quanta that reduce the loss of certain tokens by a ﬁxed number of bits
regardless of what other tokens have been learned will still produce power law scaling. For
example, If one quantum predicts that the next word is an adjective and another quantum
predicts that the next word relates to sports, they may each reduce the entropy by a ﬁxed
number of bits regardless of the order in which they are learned.
For the following derivations, we use a = 0 and b = 1 resulting in the simple formula
Ln ≈
1
αζ(α+1)n−α. Scaling in model parameters (N), training samples (D), and training
time (S) can translate into scaling in n and therefore loss L as follows:
Parameter scaling: In networks of ﬁnite size, only ﬁnitely many quanta can be learned –
network capacity is a bottleneck. If we assume that all quanta require the same capacity of
4

C network parameters, and we have a network with N total parameters, roughly n = N/C
elements in the Q Sequence can be learned. We therefore expect loss to depend on the
number of model parameters N like so:
L(N) = LN/C ≈
1
αζ(α + 1)
N
C
−α
∝N −α.
(3)
Given a power law distribution over quanta with exponent α + 1, we get power law neural
scaling in parameters with exponent αN = α. Note that we have also assumed that all
quanta require the same model capacity. This is surely an unrealistic assumption (some
computations, enabling certain model capabilities, probably require more capacity than
others to implement), although if the average capacity consumed per quanta is small enough,
ﬂuctuations away from the mean capacity will be averaged out and the number of quanta
learned n will still be roughly proportional to model size N.
Data scaling (multi-epoch): For data scaling, we assume that a threshold of τ examples
utilizing quantum k are needed in the training set in order for quantum k to be learned.
τ can perhaps be thought of as the minimum number of examples on average requiring
quantum k needed to uniquely specify its computation. Assuming network capacity is not a
bottleneck, how many quanta will be learned? If we have a training set of D samples, then
it will contain roughly Dp1 samples utilizing quantum 1, Dp2 samples utilizing quantum
2, and so on. If pk =
1
ζ(α+1)k−(α+1), the last quantum n learned in the Q Sequence will
then roughly be n such that D
1
ζ(α+1)n−(α+1) = τ and so n = (D/τζ(α + 1))1/(1+α). Under
this model of how the training set size D inﬂuences which quanta are learned, we would
therefore expect data scaling:
L(D) = L(D/τζ(α+1))1/(1+α) ≈
1
αζ(α + 1)

D
τζ(α + 1)
−
α
α+1
∝D−
α
α+1 .
(4)
This mechanism of data scaling therefore predicts that a power law distribution over quanta
with exponent α + 1 translates into a data scaling exponent αD = α/(α + 1). From our
earlier result that αN = α, we would predict that αD = αN/(αN + 1). We discuss whether
this relationship holds empirically for data and parameter scaling exponents observed across
a variety of studies in Appendix E.
Data scaling (single-epoch): In multi-epoch training, the information contained in the
training dataset can bottleneck which quanta are learned. However, the rate of convergence
of SGD can also bottleneck performance. For single-epoch training, a greater number of
training samples allows one to train for longer. Assume that batches are large and that
they contain eﬀectively perfect gradient information. If quanta each reduce mean loss by an
amount given by a power law, then the gradients incentivizing each quantum to form may
also roughly follow a power law in magnitude. We might therefore expect that the number
of training steps S to learn quantum k to be inversely proportional to use frequency pk
(more commonly useful quanta have larger gradients and are learned faster). Therefore if
the ﬁrst quantum requires T steps to be learned, then quantum n will require Tnα+1 steps
to converge. As a function of the number of training steps S, the number of quanta learned
is therefore n = (S/T)1/(α+1), and so:
L(S) = L(S/T )1/(α+1) ≈
1
αζ(α + 1)
S
T
−
α
α+1
∝S−
α
α+1 .
(5)
The scaling exponent αS of loss w.r.t steps S is therefore the same as the multi-epoch data
scaling exponent αD.
5

Review of prior work: Several models of power law neural scaling have been proposed
in prior work.
Sharma and Kaplan (2022) develop a model of power law scaling w.r.t.
model parameters which describes networks as performing a piecewise-linear approximation
of a function on a data manifold of intrinsic dimension d. Under their model, the scaling
exponent αN is determined by the dimension of the data manifold via αN ≤4
d. Michaud
et al. (2023) point out the eﬀective dimension d could be generalized to the maximum arity
of the task computation graph for sparse compositional problems. The model of Sharma and
Kaplan (2022) was also generalized by Bahri et al. (2021) to account for power law scaling
in training data and who additionally relate scaling exponents to a power law spectrum
of certain kernels.
Maloney et al. (2022) develop a random-feature model of scaling, in
which power law scaling comes from power law spectra of the data feature-feature covariance
matrix, and scaling exponents are determined by the power law exponent over these spectra.
Hutter (2021) propose a toy model of data scaling in which features are learned based on
whether they’ve been seen during training, and a Zipﬁan distribution over features produces
power law data scaling.
3
Proof of concept: a toy dataset
In this section, we will describe a toy dataset transparently consisting of distinct subtasks
which are power law distributed in frequency. We observe power law neural scaling in data
and parameters on this task, and ﬁnd that the mechanism of neural scaling coincides with
our theory from Section 2. It is therefore possible for power law neural scaling to arise
from the Quantization Model. We leave a study of whether natural datasets (e.g. natural
language) possess such structure to Section 4.
3.1
The “multitask sparse parity” dataset
The toy task we will construct consists of many subtasks – distinct types of inputs which
each require corresponding distinct computations (quanta). For each subtask, we choose a
variant of the “sparse parity” problem, recently studied in Barak et al. 2022. The sparse
parity prediction problem is simple: given a bit string of length n, compute the parity (sum
mod 2) of a ﬁxed subset of k of those bits. We introduce an extension of this task, which we
call “multitask sparse parity”. Beyond n and k, multitask sparse parity adds an additional
parameter ntasks, the number of subtasks (number of distinct versions of sparse parity)
present in the dataset. To construct the task, we ﬁrst choose ntasks random subsets Si of k
indices from {1, 2, . . . , n}: Si ⊂{1, 2, . . . , n} and |Si| = k, where i = 1, 2, . . . , ntasks. Input
bit strings are length ntasks + n. We call the ﬁrst ntasks bits the control bits and the last
n bits the task bits. If control bit i is active, then the parity is computed from the subset
Si of the task bits. The control bits 1-hot encode the task number: on a given input, only
one control bit is set to 1 at a time – the rest are zero. For the sample shown below, since
control bit 2 is active, the answer is the parity of the task bits S2 = {2, 7}, which is 0 for
this input:
Subtask 3
Subtask 2
0100...00  1010010...1110011
Subtask 1
“control bits”
“task bits”
6

Figure 2: Top: Neural networks exhibit power law neural scaling parameters N, training
time S, and training samples D (for multi-epoch training) when trained on the multitask
sparse parity dataset. Here α = 0.4 and we plot lines ∝N −α, ∝S−α/(α+1), ∝D−α/(α+1).
Bottom: neural scaling broken down by subtask. Scaling behavior on individual subtasks
exhibits emergence, where subtasks are not learned below a certain scale and then suddenly
learned beyond a certain scale. Power law neural scaling of mean test loss averages over
a large number of qualitative changes in network performance (when broken down by sub-
task), with loss being driven to zero on an increasing number of subtasks which are power
law distributed in frequency, a realization of the mechanism of neural scaling discussed in
Section 2.
We impose a uniform distribution over the task bits. On the control bits, we impose a
Zipﬁan distribution: the probability that a sample has control bit i active (and therefore
the parity must be computed from the subset Si of the task bits) is
1
Z i−(α+1) where Z =
Pntasks
i=1
i−(α+1). This imposes a power law distribution over subtasks in data. Since answers
are parities, this task can be treated as a binary classiﬁcation problem on the subset of bit
strings {0, 1}ntasks+n where for each string all but one bit of the ﬁrst ntasks bits are zero.
3.2
Power law scaling and emergence
We train ReLU MLPs with a single hidden layer to solve this task. The input dimension is
ntasks + n and we use cross-entropy loss, so the output dimension is 2. We use the Adam
optimizer with a learning rate of 10−3. To study scaling with respect to the number of
model parameters, we train networks of varying width by sampling batches online. For high
enough n (e.g. 100) it is unlikely that the network will encounter the same sample twice
during training. Within an individual single-epoch training run, we can study scaling in
steps S. To study scaling with respect to multi-epoch training dataset size D, we use a
network of suﬃcient width for capacity to not be a bottleneck, and for varying D we sample
a training set of D samples and train for multiple epochs, recording model performance
7

when mean test loss is lowest (early-stopping).
Training dynamics on the multitask sparse parity problem are highly nontrivial – on each
individual subtask, loss follows a reverse-S curve, undergoing a “phase transition” after an
initial plateau. However, this transition happens at diﬀerent times for diﬀerent subtasks, so
the overall loss decreases smoothly, averaging over these phase transitions. We leave a more
detailed discussion of training dynamics to Appendix B.
Figure 2 shows scaling curves on the multitask sparse parity problem. For the results
shown, we used ntasks = 500, n = 100, k = 3, α = 0.4, and a batch size of 20000. We vary
training dataset size from 1e4 to 5e6 and vary hidden-layer width from 10 to 500 neurons.
We train for 2e5 steps. In line with the theory from Section 2, we ﬁnd that as we scale
training data and parameters, networks learn more and more quanta (reducing loss on more
and more subtasks), roughly in order of their frequency, and that this is what drives neural
scaling. We see that mean loss decreases as a power law with αN ≈α and αD ≈α/(α + 1),
although αS is somewhat greater than α/(α + 1). We see that scaling w.r.t. parameters
is noisier than data scaling, possibly due to model initialization having some inﬂuence on
which computational quanta are learned (for our data scaling experiments, we use the same
seed and same model size for all runs, eliminating this eﬀect). We also see that when we
look at scaling on individual subtasks, there is a rough scale of data or parameters below
which networks do not learn the task, and above which they do. Smooth power law scaling
therefore averages over a large number of phase transitions in model performance when
properly decomposed by subtask, a proof of concept that the Quantization Model can be
the mechanism of neural scaling for data with the right structure. For additional discussion
of training dynamics and results on how the empirical scaling exponents αN, αS, αD relate
to quanta distribution power law exponent α + 1 for a variety of α (beyond just α = 0.4)
see Appendix B.
4
Decomposing empirical LLM scaling
We now study how scaling curves for large language models decompose. For our experiments,
we use the “Pythia” model sequence from Eleuther (EleutherAI 2023). These are decoder-
only transformers of varying size trained on the same data in the same order – approximately
300 billion tokens of the train set of The Pile (Gao et al. 2020).
Eleuther released 143
checkpoints for these models, spaced 1000 optimization steps apart. We can therefore study
scaling w.r.t. model parameters N and training steps S. We evaluate the ﬁrst seven models
in the sequence, which range from 19m to 6.4b non-embedding parameters, on approximately
10 million tokens from the test set of The Pile. We record cross-entropy loss on every token.
With this collection of loss values, we are able to study how neural scaling decomposes
– rather than looking just at how mean test loss changes with scale, we can see how the
distribution over losses changes with scale.
4.1
The distribution over per-token losses
In Figure 3, we plot some basic facts about how neural scaling decomposes in LLMs. First,
we ﬁnd that for the ﬁrst six models in Pythia sequence, the mean loss of the ﬁnal model
against the number of non-embedding model parameters is well-ﬁt by a power law with
exponent αN = 0.083. This is roughly in line with the parameter scaling exponent of 0.076
measured in Kaplan et al. 20201. The 6.4b model does not ﬁt the scaling curve well, so
we excluded its loss when measuring the scaling exponent. Next, we plot the probability
distribution over per-token losses p(L). We ﬁnd that losses close to zero are by far the most
common, and that scaling increases the portion of approximately-zero losses. We also plot
1Kaplan et al. 2020 also use non-embedding parameters when studying scaling w.r.t. parameters
8

Figure 3: Top left: Scaling of mean test loss w.r.t. non-embedding parameters for the
Eleuther Pythia models. The parameter scaling exponent αN is measured to be ≈0.083
from the ﬁrst six points along the curve (the seventh model appears to break the trend),
roughly similar to the parameter scaling exponent measured in Kaplan et al. 2020. Top
center: the distribution p(L) over losses on individual tokens for models of diﬀerent size.
Token losses ≈0 are by far the most common, and larger models achieve ≈0 loss on an
increasing fraction of tokens. Top right: the expected loss integrand L · p(L) for models
of diﬀerent sizes. Despite their very high prevalence, low-loss tokens contribute minimal
mass to the mean loss, which is instead dominated by tokens with much higher loss of 5-10
bits (depending on scale). Bottom left: Training curves (scaling w.r.t. steps S) of mean
test loss for Pythia models. We measure exponents αS between 0.037 and 0.06. Bottom
center: the distribution p(L) over time. Over time, models achieve ≈0 loss on an increasing
fraction of tokens, similar to scaling in model size. Bottom right: The distribution L·p(L)
over time.
9

L · p(L), the probability density over losses weighted by loss. The mean loss is the area
under this curve. We see that despite approximately-zero-loss tokens being by far the most
common, they do not contribute much mass to the mean loss. We also plot mean loss as
well as p(L) and L · p(L) versus optimization steps rather than model size.
We see immediately that neural scaling in the wild is somewhat more complicated than
our theoretical model. Notably, the distribution over losses is not bimodal like it was for
multitask sparse parity. Nevertheless, we do see that losses of approximately zero are by far
the most common and that models of increasing scale achieve approximately zero loss on
an increasing fraction of the dataset. We leave a detailed study of whether the statistics of
neural scaling in LLMs are compatible with prior models of neural scaling to future work.
4.2
A taxonomy: monogenic versus polygenic behaviors
In our introduction of the Quantization Hypothesis in Section 2 and our multitask sparse
parity study in Section 3 we modeled network performance on individual samples as bene-
ﬁtting from a single quanta – all samples belong to a single subtask, which is either solved
or not solved in a binary fashion based on the presence or absence of some computation
in the network. In our model and on multitask sparse parity, scaling curves on individual
examples all exhibit emergence – loss on individual examples undergoes a phase transition
at a particular scale of parameters or data. Do we observe this in large language models?
Manually inspecting a large number of per-token scaling curves, we observe a variety of
scaling behaviors. We see that not all loss scaling curves on individual tokens undergo a
phase transition, or a single drop at a particular model scale. More commonly, loss improves
at more than one model scale.
If it were true, as we conjectured earlier, that the eﬀect of scaling is to simply add
computations to the network, while still learning quanta present in smaller networks, then
for scaling curves on individual prediction problems to show progress at multiple scales, it
must be the case that prediction on those problems beneﬁts from multiple quanta additively.
As ﬁrst mentioned in Section 2, we borrow terminology from genetics and refer to pre-
diction problems for which the model’s loss is inﬂuenced by multiple quanta as polygenic (in
analogy to when multiple genes contribute to a trait) and problems for which performance
is determined by a single quantum as monogenic (akin to when a single gene determines a
trait). In multitask sparse parity, all prediction problems are monogenic. In natural lan-
guage, we observe that the majority of tokens are polygenic but that we can indeed ﬁnd
monogenic tokens for which loss drops as a single phase transition in scale. Polygenicity
forms a spectrum: the smoothness of the loss curve can vary substantially between exam-
ples, presumably with some prediction problems only using few quanta and others using
many. In Figure 4, we show extreme examples of both monogenic and polygenic prediction
problems.
Note that our monogenic/polygenic taxonomy of model behaviors assumes that QH1
and QH2 are true, that larger networks contain the quanta of smaller networks. However, it
could be the case that very little is similar between large networks from small networks. It
is encouraging that some structures such as induction heads have been found across many
models at many scales (Olsson et al. 2022), but whether other computations performed
across models are truly universal, and whether scaling has the eﬀect we described, will have
to be investigated in future studies of the internals of neural networks.
4.3
Auto-discovering quanta with language model internals
We will now attempt to auto-discover quanta in language modeling. While for multitask
sparse parity it was clear how to partition the prediction task into subtasks, it is unclear a
priori how to do so for the task of predicting natural language. For instance, partitioning
10

...at a Congress event where Sheila Dikshit 
took charge as party's Delhi 
chief.Shiromani Akali Dal MLA Manjinder 
Singh Sirsa alleged that the Congress...
...
estation 
of worms and several days of torrential
The big disappointment this summer was 
that despite my 2 plum trees fruiting 
super-abundantly, beyond expectations, the 
fruit was mostly spoiled by an inf
...
 controls. In cases with...
In general, the lesions of thoraco-
cervical level were difficult to detect, 
because the appearance rate of SSEP peaks 
are reduced over the thoraco-cervical spine 
even in normal
Prompt:
Prompt:
Prompt:
Monogenic Tokens
Polygenic Tokens
...
TB Capital...
and the history of previous military 
interventions in the region is not a 
recipe for political and economic 
stability," said Neil MacKinnon, global 
macro strategist at V
Prompt:
Figure 4: Scaling on individual tokens can have diverse behavior. Here we show exam-
ples of scaling curves on examples which we call monogenic and polygenic. Scaling curves
on monogenic examples display emergence: there is a particular model scale at which the
model’s performance improves rather abruptly. Scaling on polygenic curves displays grad-
ual progress, since (we conjecture) many quanta, emerging at diﬀerent scales, marginally
contribute to the loss.
samples based on the correct output token is suboptimal since the same token can occur for
diﬀerent reasons depending on the context and where prediction relies separate quanta. Par-
titioning inputs based on the ﬁnal n-gram of the context is also suboptimal, since prediction
often relies on information contained throughout the whole context and on abstract pat-
terns within it. Clustering based on inputs or outputs therefore seems unlikely to discover
quanta in language modeling. We therefore use the internals of trained language models to
cluster samples. An ideal clustering scheme would group samples based on which internal
mechanism(s) models use for prediction on those examples.
Quanta Discovery from Gradients (QDG): For the discovery of quanta, we propose
11

a method based on spectral clustering with model gradients. This method clusters samples
together based on whether gradients on those samples point in similar directions. In particu-
lar, given a set of samples (xi, yi) and a model fθ, we compute gradients gi = ∇θL(fθ(xi), yi).
We then normalize these gradients gi 7→ˆgi so that ˆgi · ˆgi = 1. Let A be a matrix whose rows
are the normalized gradients: Ai,· = ˆgi. We can deﬁne an aﬃnity matrix C = AAT , so that
Cij = ˆgi · ˆgj, the cosine similarity between gradients gi, gj. One can then deﬁne an aﬃnity
matrix ˆC of angular similarities (which take values in [0, 1]) via ˆCij = 1 −arccos(Cij)/π.
We perform spectral clustering with ˆC to cluster samples (xi, yi).
One challenge of QDG is that it is expensive to compute when gradients are very high
dimensional. When applying QDG to language models, we therefore use only the smallest
model in the Pythia sequence, which has 19m non-embedding parameters. We use gradients
within self-attention and MLP layers, but do not include embed, unembed, or layer norm
gradients when we ﬂatten and concatenate gradients into a vector g.2 We choose samples
(xi, yi) for which our 19m-parameter model achieves a cross-entropy loss less than 0.1 nats.
We ﬁlter based on this criteria since (1) we cannot cluster samples based on model mech-
anism if the model does not have such a mechanism for performing prediction correctly on
those samples and (2) our intuition that samples with particularly low loss are more likely
to be monogenic. We further exclude samples which can be solved via induction on the
context3, since such samples are quite common (possibly interfering with our task of ﬁnd-
ing diverse quanta) and since early experiments indicated that QDG had trouble clustering
such samples together. We choose 10000 such samples to perform clustering on from the
test set of The Pile. After computing the aﬃnity matrix ˆC, we use the spectral clustering
implementation from SciPy (Virtanen et al. 2020) with labels assigned via k-means.
We ﬁnd that QDG discovers many clusters of coherent model behavior. We show exam-
ples from clusters in Figure 1 and Figure 11. These clusters were found with the spectral
clustering hyperparameter n clusters = 400. While most clusters involve the prediction
of the same token, manually inspecting these clusters we ﬁnd that they usually involve pre-
dicting the same token for a coherent reason, rather than being based merely on having the
same output. We also ﬁnd clusters for more abstract prediction rules. For instance, the
quantum shown on the left column of Figure 1 is for continuing a numerical sequence, and
the examples involve prediction for a variety of numbers.
4.4
The natural distribution over language modeling quanta
Some quanta of prediction ability are more frequently relied upon than others. Earlier,
we hypothesized that a power law in how frequently the quanta are utilized is the origin
of power law scaling. When we cluster samples with QDG, do we ﬁnd that a power law
governs the size of the clusters? The measured scaling exponent of αN = 0.083 implies a
power law distribution over quanta with exponent −1.083, and so we would hope to recover
a power law with this exponent governing the relative size of the clusters.
Figure 5 shows rank-frequency curves for clusters discovered with QDG for varying
choices of n clusters. These curves rank the clusters according to their size and then plot
size against cluster index. We plot rank-frequency curves for many choices of n clusters
since it is unclear a priori which n clusters to use. When we measure the slope of the
rank-frequency curve, we measure it from the envelope formed by the many rank-frequency
2We exclude gradients for embed and unembed parameters because they are high dimensional and also
because they may contain information more about the input and output rather than the computations
the model performs internally.
We exclude layer norm gradients because they appeared to contain less
information about clusters in toy experiments.
3We ﬁlter (copying) induction problems by excluding samples where the token which is to be predicted is
the last token in a trigram which occurred earlier in the context. This is not a very comprehensive ﬁltering
scheme.
12

Figure 5: Left: angular similarity between model gradients for a variety of natural language
samples. Samples are reordered according to their QDG cluster (with 400 clusters) to reveal
the block-diagonal structure of the similarity matrix. We visualize a small part of the overall
similarity matrix in this plot – note that not all clusters are as visibly distinct as the ones
shown. Right: rank-frequency plot of clusters computed with spectral clustering from the
similarity matrix of model gradients. We measure the slope of the envelope of the rank-
frequency curves from cluster rank 100-1000 to be ≈−1.24, which is a steeper than the
slope of -1.08 expected from the measured parameter-scaling exponent from Figure 3, though
within margin of error given the uncertainty of our clustering methodology. See Appendix D
for a discussion of the bias/uncertainty of our method.
curves, a practice which we discuss in Appendix D. Biases in the clustering algorithm and
inherent noise in model gradients make clustering imperfect, and lead to high uncertainty
of our the measured power law exponent. From an argument in Appendix D, we think
that extracting the power law exponent over quanta utilization frequency by measuring the
slope of the rank-frequency curve should have uncertainty of at least 0.2. We measure a
slope of ≈−1.24, about 0.16 oﬀour expected slope of −1.08, and so within the margin of
error. While less naive clustering schemes could help sharpen this measurement in future
work, we are encouraged that the size of our discovered clusters seems to decay at a rate
compatible with the power law predicted from the Quantization Model given the empirical
scaling exponents for language modeling on The Pile.
5
Discussion
We have articulated the Quantization Model of neural scaling laws. This relied on the Quan-
tization Hypothesis which posits that neural network performance can be understood with
respect to a discrete set of computations and the associated capabilities they enable, which
networks can either succeed or fail at learning. We called these computations/capabilities
the quanta of the prediction problem, and sorted them into the Q Sequence according to how
frequently they are used for prediction in the data distribution. We saw that when the use
frequencies of the quanta are given by a power law, we can get power law neural scaling as
networks learn additional quanta. For the multitask sparse parity problem, we found that
the Quantization Hypothesis holds, and that power law neural scaling averages over the
emergence of quanta (network capabilities on subtasks). We then decomposed LLM scaling
13

curves by token and auto-discovered quanta for language prediction with a method we called
QDG. Beyond understanding neural scaling laws, we speculate that our perspective could
have a number of other implications for understanding deep neural networks:
Understanding Emergence: Srivastava et al. (2022) study how model capabilities scale
on a variety of tasks, and ﬁnd diverse scaling behavior: some tasks display high “linearity”
where model performance improves gradually with scale and others display “breakthrough-
ness” where model performance improves sharply at a particular scale. Under the Quanti-
zation Hypothesis, the linearity or breakthroughness of a task would be inﬂuenced by how
the quanta relevant to the task are distributed along the Q Sequence. If performance relies
on a single quantum of knowledge or computation, or on multiple quanta close together in
the Q Sequence, we should expect high breakthroughness. On the other hand, if the rele-
vant quanta are numerous and distributed widely across the Q Sequence, we would expect
performance to improve gradually across scale. The Quantization Hypothesis also suggests
that we may be able to predict when certain capabilities will arise with scale if we could
know where their corresponding quanta lie on the Q Sequence. This could in theory be
estimated if we could compute how frequently those quanta would be useful for prediction
in the training distribution.
Mechanistic Interpretability: If it were true that computations were learned universally
across model scales, then the task of mechanistically understanding neural networks might
simplify.
If performance is determined by the quanta – a particular, enumerable set of
computations – then understanding the network could reduce to enumerating the quanta.
Having done this, the learned knowledge and abilities of our networks could perhaps then
be translated into a more interpretable format (something like code), studied in this format,
and eventually executed in this format, rather than via the operation of the network.
The Science of Deep Learning: If we can understand model performance with respect
to a particular set of computations, then perhaps these become natural objects of study
in deep learning. Instead of studying as a black box how engineering choices like architec-
ture, optimization hyperparameters, and scale aﬀect model performance, we could instead
study at an intermediate level how these choices inﬂuence the building blocks of model
performance – the quanta. For instance, instead of studying the training dynamics at the
level of individual parameters, or at the level of the whole-network performance, one could
study how the quanta emerge over training. This mesoscale understanding of networks, in
terms of the internal computations which collectively constitute their performance, could
act like statistical physics for deep learning, perhaps allowing us to bridge our microscale
understanding of low-level training dynamics and our macroscale understanding of model
performance.
Acknowledgements: We thank Tamay Besiroglu, Neel Nanda, Tony Wang, Ben Edelman,
Wes Gurnee, Eleni Shor, Max Nadeau, and Xander Davies for helpful conversations and
feedback. We thank Lauro Langosco for helping with code to visualize samples from The
Pile.
This work was supported by the Foundational Questions Institute, the Rothberg
Family Fund for Cognitive Science, the NSF Graduate Research Fellowship (Grant No.
2141064), and IAIFI through NSF grant PHY-2019786.
14

References
Ardalani, Newsha, Carole-Jean Wu, Zeliang Chen, Bhargav Bhushanam, and Adnan Aziz
(2022). “Understanding Scaling Laws for Recommendation Models”. In: arXiv preprint
arXiv:2208.08489.
Bahri, Yasaman, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma (2021).
“Explaining neural scaling laws”. In: arXiv preprint arXiv:2102.06701.
Barak, Boaz, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril
Zhang (2022). “Hidden progress in deep learning: Sgd learns parities near the computa-
tional limit”. In: arXiv preprint arXiv:2207.08799.
Droppo, Jasha and Oguz Elibol (2021). “Scaling laws for acoustic models”. In: arXiv preprint
arXiv:2106.09488.
EleutherAI (2023). Pythia: Interpreting Autoregressive Transformers Across Time and Scale.
https://github.com/EleutherAI/pythia. GitHub repository.
Elhage, Nelson, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain,
Deep Ganguli, Zac Hatﬁeld-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion,
Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,
Sam McCandlish, and Chris Olah (2021). “A Mathematical Framework for Transformer
Circuits”. In: Transformer Circuits Thread. https://transformer- circuits.pub/
2021/framework/index.html.
Ganguli, Deep, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen,
Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. (2022). “Predictabil-
ity and surprise in large generative models”. In: 2022 ACM Conference on Fairness,
Accountability, and Transparency, pp. 1747–1764.
Gao, Leo, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,
Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. (2020). “The pile: An 800gb
dataset of diverse text for language modeling”. In: arXiv preprint arXiv:2101.00027.
Gordon, Mitchell A, Kevin Duh, and Jared Kaplan (2021). “Data and parameter scaling
laws for neural machine translation”. In: Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, pp. 5915–5922.
Henighan, Tom, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,
Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. (2020). “Scaling laws
for autoregressive generative modeling”. In: arXiv preprint arXiv:2010.14701.
Hestness, Joel, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan
Kianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou (2017). “Deep learn-
ing scaling is predictable, empirically”. In: arXiv preprint arXiv:1712.00409.
Hoﬀmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,
Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan
Clark, et al. (2022). “Training compute-optimal large language models”. In: arXiv preprint
arXiv:2203.15556.
Hutter, Marcus (2021). “Learning curve theory”. In: arXiv preprint arXiv:2102.04074.
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeﬀrey Wu, and Dario Amodei (2020). “Scaling laws
for neural language models”. In: arXiv preprint arXiv:2001.08361.
Maloney, Alexander, Daniel A Roberts, and James Sully (2022). “A Solvable Model of Neural
Scaling Laws”. In: arXiv preprint arXiv:2210.16859.
Michaud, Eric J., Ziming Liu, and Max Tegmark (2023). “Precision Machine Learning”. In:
Entropy 25.1. issn: 1099-4300. doi: 10.3390/e25010175. url: https://www.mdpi.
com/1099-4300/25/1/175.
15

Nanda, Neel, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt (2023).
“Progress measures for grokking via mechanistic interpretability”. In: arXiv preprint
arXiv:2301.05217.
Olah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan
Carter (2020). “Zoom In: An Introduction to Circuits”. In: Distill. https://distill.
pub/2020/circuits/zoom-in. doi: 10.23915/distill.00024.001.
Olsson, Catherine, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn
Drain, Deep Ganguli, Zac Hatﬁeld-Dodds, Danny Hernandez, Scott Johnston, Andy
Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack
Clark, Jared Kaplan, Sam McCandlish, and Chris Olah (2022). “In-context Learning and
Induction Heads”. In: Transformer Circuits Thread. https://transformer-circuits.
pub/2022/in-context-learning-and-induction-heads/index.html.
Rosenfeld, Jonathan S, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit (2019). “A
constructive prediction of the generalization error across scales”. In: arXiv preprint
arXiv:1909.12673.
Sharma, Utkarsh and Jared Kaplan (2022). “Scaling Laws from the Data Manifold Dimen-
sion”. In: Journal of Machine Learning Research 23.9, pp. 1–34. url: http://jmlr.
org/papers/v23/20-1111.html.
Srivastava, Aarohi, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,
Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et
al. (2022). “Beyond the imitation game: Quantifying and extrapolating the capabilities
of language models”. In: arXiv preprint arXiv:2206.04615.
Villalobos, Pablo (2023). Scaling Laws Literature Review. https://epochai.org/blog/
scaling-laws-literature-review. Accessed: 2023-01-31.
Virtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David
Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright,
St´efan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay
Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan
Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert
Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antˆonio
H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors (2020).
“SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python”. In: Nature
Methods 17, pp. 261–272. doi: 10.1038/s41592-019-0686-2.
Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raﬀel, Barret Zoph, Sebastian Borgeaud,
Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori
Hashimoto, Oriol Vinyals, Percy Liang, JeﬀDean, and William Fedus (2022). “Emergent
Abilities of Large Language Models”. In: Transactions on Machine Learning Research.
Survey Certiﬁcation. issn: 2835-8856. url: https://openreview.net/forum?id=
yzkSU5zdwD.
Zhai, Xiaohua, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer (2022). “Scaling vision
transformers”. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 12104–12113.
A
More general scaling laws
If one learns the ﬁrst n quanta, reducing the loss from bk to ak (1 ≤k ≤n), while the loss
remains bk for k > n. The expected loss is given by:
Ln =
n
X
k=1
akpk +
∞
X
k=n+1
bkpk.
(6)
16

In the main text, we used ak = a and bk = b for our model. However, one can imagine a
variety of other choices for ak and bk.
Case 1 bk = −log pk and ak = 0, where pk = k−(α+1)/ζ(α + 1). The expected loss is
given by:
Ln =
n
X
k=1
0·pk+
∞
X
k=n+1
(−log pk)·pk ≈1 + α + αlogζ(α + 1)
α2ζ(α + 1)
n−α+
α + 1
αζ(α + 1)n−αlog n, (7)
which contains a power law term n−α plus a log term n−αlog n. For very large n, the log
term can be ignored, so L is still approximately a power law of n with exponent −α, shown
in Figure 6.
101
103
105
107
109
log n
10
4
10
3
10
2
10
1
100
log (L
L )
 = 0.05 (power law)
 = 0.05 (including the log factor)
 = 0.1 (power law)
 = 0.1 (including the log factor)
 = 0.2 (power law)
 = 0.2 (including the log factor)
 = 0.4 (power law)
 = 0.4 (including the log factor)
Figure 6: Comparing diﬀerent scaling laws. Setting ak = 0, we compare bk = −log pk (solid
lines) and bk = 1 (dashed lines) for diﬀerent alphas. Although the bk = −log pk case would
cause an extra loss term n−αlogn in additional to the power law term n−α, the loss becomes
a power law asymptotically when n becomes large.
Case 2 bk = −log pk and ak = −log (Cpk) (C > 1), where pk = k−(α+1)/ζ(α + 1). The
expected loss is given by:
Ln =
n
X
k=1
(−log (Cpk))·pk+
∞
X
k=n+1
(−log pk)·pk ≈
logC
αζ(α + 1)n−α−logC+1 + α + αlogζ(α + 1)
α2ζ(α + 1)
,
(8)
which is a power law n−α plus a constant.
B
Additional results on multitask sparse parity
Training dynamics: When loss is broken down by subtask on multitask sparse parity,
learning curves consist of many reverse-S shaped curves, and mean loss decreases smoothly
as an average over these curves. In Figure 7, we show loss versus time for each subtask for
training runs in both the single-epoch and multi-epoch regimes. In Figure 8 we show how
convergence time for each subtask relates to the frequency of that subtask.
Scaling for varying α: In Figure 10 we show scaling curves on multitask sparse parity in
N, S, D for a variety of quanta distribution parameters α. While all scaling curves appear
to be power laws, the relationship between αN, αS, αD and α is not precisely as predicted
by theory:
17

Figure 7: Training dynamics on the multitask sparse parity dataset consist of many “phase
transitions” when decomposed by subtask – the loss curve for each subtask drops following
an initial plateau of no apparent progress, in line with Barak et al. 2022. The mean loss
decreases smoothly, averaging over these phase transitions in the model’s performance on
subtasks. We show curves for single-epoch training (top) and multi-epoch training on 5
million samples (bottom). The dashed red line indicates the early stopping point where
mean test loss is minimized. For these runs, α = 0.4.
10
3
10
2
10
1
Subtask frequency
103
104
105
Steps to convergence
fitted slope=-0.81
slope=-1
102
103
104
105
Optimization steps
0.1
0.2
0.3
0.4
0.5
0.6
0.8
1.0
Mean loss
mean loss
fitted slope=-0.45
Figure 8: Convergence time for each subtask versus the frequency of that subtask. We see
that convergence time Sk on subtask k is Sk ∝p−0.81
k
rather than Sk ∝p−1
k
as we had
expected. This leads to a steeper scaling w.r.t. S than expected from theory. For these
experiments, we used α = 0.4, and so we would have predicted αS ≈0.29 but instead we
get αS ≈0.45. We consider the model to have converged on a subtask once it gets mean
test loss less than 0.1 bits on that subtask.
18

104
105
106
Training samples (D)
100
101
n (No. of subtasks learned)
1.10
1.15
1.20
1.25
1.30
1.35
1.40
1.45
1.50
1.55
1.60
1.65
1.70
1.75
1.80
quanta distribution exponent: 
+ 1
0.0
0.2
0.4
0.6
0.8
quanta distribution 
0.6
0.8
1.0
scaling exponent for n
empirical exponent
theory: y = 1/(
+ 1)
Figure 9: Number of subtasks learned (n), including subtasks learned after early-stopping
would terminated the training run, versus training samples D for a variety of α. We see that
the relation n ∝D1/(α+1) approximately holds, in line with theory. Deviation from theory
for the scaling exponent of loss L w.r.t. D therefore likely originates from our failure to
regularize network training, leading to early-stopping ending training before some subtasks
can be learned.
1. Parameter scaling: We observe that the relationship between αN and α deviates a
bit from the prediction αN = α, with αN < α for small α and αN > α for large α.
Perhaps model size does not inﬂuence learning just by changing capacity, but also by
aﬀecting optimization.
2. Step scaling: We observe that αS is consistently higher than the theoretical predic-
tion α/(α + 1). In Figure 8, we saw that the number of steps to convergence for each
subtask did not precisely follow Sk ∝p−1
k , but was closer to Sk ∝p−0.81
k
. This means
that many subtasks converge faster than we would expect, producing a steeper scaling
curve.
3. Data scalaing: We observe that αD is substantially higher than the theoretical
prediction α/(α + 1) for small α.
We think this may be related to the fact that
early-stopping cuts oﬀtraining before all subtasks are learned as observed in Figure 7.
In Figure 9, we show how the number of subtasks learned n, when we include subtasks
learned after early-stopping, seems to be in line with theory: n ∝D1/(α+1).
Better understanding the precise nature of power law scaling on multitask sparse parity
is an interesting avenue for future work.
19

104
105
Number of parameters (P)
0.1
0.2
0.3
0.4
0.5
0.7
1.0
Mean test cross-entropy (bits)
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
quanta distribution exponent: 
+ 1
0.0
0.2
0.4
0.6
0.8
quanta distribution 
0.0
0.2
0.4
0.6
0.8
N
empirical exponent
theory
103
104
105
Steps (S)
0.1
0.2
0.3
0.4
0.5
0.7
1.0
Mean test cross-entropy (bits)
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
quanta distribution exponent: 
+ 1
0.0
0.2
0.4
0.6
0.8
quanta distribution 
0.0
0.2
0.4
0.6
0.8
S
empirical exponent
theory
104
105
106
Training data (D)
0.1
0.2
0.3
0.4
0.5
0.7
1.0
Mean test cross-entropy (bits)
1.10
1.15
1.20
1.25
1.30
1.35
1.40
1.45
1.50
1.55
1.60
1.65
1.70
1.75
1.80
quanta distribution exponent: 
+ 1
0.0
0.2
0.4
0.6
0.8
quanta distribution 
0.0
0.2
0.4
0.6
0.8
D
empirical exponent
theory
Figure 10: Scaling in parameters (N), single-epoch training time (S), and multi-epoch
training samples (D) for varying quanta power law distribution parameter α on multitask
sparse parity. We notice that scaling curves in steps S are typically steeper than the αS =
α/(α + 1) predicted from theory, and that for low α the scaling curves in D also deviate
from theory substantially.
C
Additional results on language models
In Figure 11 we show additional examples from clusters discovered with QDG.
20

After his tweet went viral Aslan apologized on Twitter saying “it’s not 
like me” to use profanity.

I should not have used a profanity to describe the President when 
responding to his shocking reaction to the #LondonAttacks. My statement: 
pic.twitter.com/pW69jjpoZy — Reza Aslan (@rezaaslan) June 4,

Sam Willard

Samuel Steven Willard (born September 9,

215 U.S. 437 (1910)
MECHANICAL APPLIANCE COMPANY
v.
CASTLEMAN.
No. 48.
Supreme Court of United States.
Argued December 3, 1909.
Decided January 3,

485 F.2d 283
73-2 USTC  P 9685, 179 U.S.P.Q. 450
GEORATOR CORPORATION, Appellee,v.UNITED STATES of America, Appellant.
No. 73-1187.
United States Court of Appeals,Fourth Circuit.
Argued June 4, 1973.Decided Oct. 2,







.rickshaw_graph.detail {
 pointer-events: none;
 position: absolute;
 top: 0;
 z-index: 2;
 background: rgba(0, 0, 0, 0.1);
 bottom: 0;
 width:


@import '../../../assets/sass/spin';

.app-header {
  background-color: #282c34;
  min-height: 100vh;
  display:


...o work. I tried $("#plane").toggle(".plane-right,.plane-left") inside 
the listener but that didn't do the trick.
And the CSS class
.plane-right {
    background-image: url("../img/zoomzoom.png");
    background-position: center;
    background-repeat: no-repeat;
    background-size: 100%;
    height:
Romford Ice Arena

Romford Ice Arena was an ice rink located in Romford in the London Borough 
of Havering, England. The venue was built in the 1980s

ownloadable formats: PDF

The rings were stamped with a distinctive Kleinberg logo. Although the 
novel continues to be the dominant medium of the crime-mystery-detective 
narrative, short stories by these contemporary authors may be found in 
numerous anthologies of the genre published during the 1990s

...as the Founder and First Director of the Institute of Atomic Physics 
(IFA) in Bucharest, Romania. He became a titular member of the Romanian 
Academy in 1946; stripped of membership by the new communist regime in 
1948, he was restored to the Academy in 1955.

University teaching
During the early 1960s

...king down Ryan Farish’s “Beautiful” CD after hearing “Full Sail” played 
during TWC’s “Local On The 8’s” segment. [Farish’s music clips and a 
streaming Internet broadcast here] Yesterday, visitor Greg Davidson 
commented that he was searching for songs played on the local forecast back 
in the late ’80s





##################
# TeslaCrypt Ransomware Payment Sites domain blocklist (TC_PS_DOMBL)     #
#                                                                        #
#                                                                        #
# For questions please refer to:                                         #
# https://

to that document rather than overwrite it.
If it does not exist, it should insert the new document to the collection.

When I run the below code, I am getting an error: MongoError: The dollar 
($) prefixed field '$push' in '$push' is not valid for storage.
I put this together based on the docs: https://

Gruber, Martin A. Views of the National Zoological Park in Washington, DC, 
showing Exhibit. 1919. Retrieved from the Digital Public Library of 
America, http://

 it be discontinued? I heard Java Swing is discontinued and no more 
future enhancements will be made. As a Beginner what should I learn. 

A:

JavaFX is more recent and can be considered as the successor of Swing.
There is many very useful features added in JavaFX. See here some key 
features : https://
...
                                                     ...
...
...
Examples from Cluster 146: comma 
after day of month
Examples from Cluster 278: colon 
after CSS property
Examples from Cluster 269: “s” after start 
year of decade
Examples from Cluster 292: “://” after “http”
Figure 11: Additional examples of clusters of inputs discovered by QDG. Like in Figure 1,
we used 10000 samples and n clusters of 400.
D
The diﬃculty of estimating the power law exponent from clus-
ters
In Section 4.4, when we looked at the distribution over elements in each cluster, we did
not perfectly recover a Zipf distribution with exponent ≈1.08 that we expected from our
theory. In this section, we describe the diﬃculty of accurately estimating such an exponent
with our method.
D.1
QDG on multitask sparse parity
As a ﬁrst experiment, we performed QDG on multitask sparse parity, where there is a
known, artiﬁcially-imposed power law distribution over subtasks.
We train a width-500
single-hidden-layer ReLU MLP on multitask sparse parity with α = 0.4 and with n = 100,
k = 3, and ntasks = 500. We then took 10000 samples which the network achieves ≈0 loss on
21

Figure 12: Similarity matrix and rank-frequency plots from QDG on multitask sparse parity.
Despite sparse parity having a known decomposition into subtasks which are power law
distributed in frequency, we do not recover this same power law from samples. We used
α = 0.4 for the frequency distribution for an expected rank-frequency power law exponent
of -1.4, but measure a rank-frequency envelope slope closer to -1.1.
(sampled from the Zipf distribution over subtasks with exponent 1.4). We compute gradients
of cross-entropy loss w.r.t. all model parameters for these samples, and then perform QDG
just like for LLMs. We show results in Figure 12. We plot the full similarity matrix where
samples are ordered according to their a priori known subtask, rather than their cluster from
QDG, and see a clear pattern where elements from the same subtask have on average higher
angular similarity than elements between subtasks. However, from the rank-frequency plot
of the clusters, we do not recover a slope of -1.4, but rather a lower slope of ≈−1.1. This
shows that even when there is an exact decomposition of inputs into subtasks with a known
Zipf distribution over these subtasks, that we do not perfectly recover this Zipf distribution
from QDG.
D.2
A toy model of QDG uncertainty and bias
A toy model: To understand the bias of spectral clustering, we develop the following
toy model. We assume the dataset has N = 1000 subtasks, each subtask containing ni =
⌊A
iα ⌋(1 ≤i ≤N) tokens (A = 1000). We use a Gaussian distribution N(mi, σ2Id×d) to
model gradients within a subtask i, where d is the embedding dimension, σ is the noise level,
and mi is the Gaussian mean. mi itself is drawn from the standard Gaussian distribution
mi ∼N(0, Id×d). We deﬁne the similarity between two vectors x, y to be sim ≡1+ x
|x| · y
|y|.
We compute pairwise similarity between all PN
i=1 ni tokens, and input the similarity matrix
to the spectral clustering algorithm. We also need to specify the number of clusters k.
We have two hyperparameters in the toy model, the embedding dimension d and the
noise level σ. We need to determine them such that this toy model can decently imitate
LLM results (Figure 5). We ﬁx α = 1, sweeping d = {30, 100, 1000}, σ = {0, 0.5, 2.0},
and k = {100, 200, 500}. As shown in Figure 13, the high-dimension (d = 1000) large-noise
(σ = 2.0) scheme seem to best agree with the LLM results, since the k = 200 curve can
reproduce the sag and the cliﬀpresent in LLM curves.
Estimating α from the frequency curve is hard, in fact, the slope depends on k and the
22

100
101
102
103
cluster size
d=30,
=0.0
k=100
k=200
k=500
truth
d=100,
=0.0
d=1000,
=0.0
100
101
102
103
cluster size
d=30,
=0.5
d=100,
=0.5
d=1000,
=0.5
100
101
102
103
cluster rank
100
101
102
103
cluster size
d=30,
=2.0
100
101
102
103
cluster rank
d=100,
=2.0
100
101
102
103
cluster rank
d=1000,
=2.0
sag
cliff
Figure 13: To understand the bias of spectral clustering, we apply spectral clustering to
a toy model with diﬀerent embedding dimension d, noise scale σ and number of cluster k.
The high-dimension (d = 1000) large-noise (σ = 2.0) scheme seems to best agree with the
LLM results (Figure 5).
region used to estimate it. However, we observe that diﬀerent k curves form a clear envelope,
whose slope is robust in a reasonably wide region. The envelope slope seems to indicate
α. We ﬁx d = 1000 and σ = 2.0, sweeping α = {0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5}. For each
α, we estimate the slope of the envelope. Although there is clear correlation between the
estimated envelope and α, if we use the envelope slope to estimate α, the error is on the
order of 0.2, as shown in Figure 14.
23

100
101
102
103
104
cluster size
 = 0.800 
 envelope slope = 1.047
562
1124
1687
2249
2811
3374
3936
4498
truth
envelope
 = 0.900 
 envelope slope = 1.202
215
430
646
861
1077
1292
1508
1723
truth
envelope
 = 1.000 
 envelope slope = 1.220
100
200
300
400
500
600
700
800
truth
envelope
 = 1.100 
 envelope slope = 1.296
53
106
160
213
266
320
373
426
truth
envelope
101
103
cluster rank
100
101
102
103
104
cluster size
 = 1.200 
 envelope slope = 1.344
31
63
94
126
158
189
221
252
truth
envelope
101
103
cluster rank
 = 1.300 
 envelope slope = 1.380
20
40
60
81
101
121
142
162
truth
envelope
101
103
cluster rank
 = 1.400 
 envelope slope = 1.400
13
27
41
55
69
83
97
111
truth
envelope
101
103
cluster rank
 = 1.500 
 envelope slope = 1.645
9
19
29
39
49
59
69
79
truth
envelope
0.8
1.0
1.2
1.4
1.6
0.8
1.0
1.2
1.4
1.6
envelop slope
Figure 14: The diﬃculty of measuring α from curves. We apply spectral clustering to a toy
model with diﬀerent α and number of clusters k. For a ﬁxed α, diﬀerent k curves deﬁne an
envelope. One could use the envelope slope to infer α, but this incurs errors around 0.2.
E
Parameter and data scaling exponents across studies
In Figure 15, we show αN and αD (or possibly αS, depending on the study) for a variety of
prior studies of deep learning scaling, as compiled by Villalobos (2023). While the data is
messy, it is intriguing that most of the Rosenfeld et al. (2019) samples lie below the αD = αN
line, as our model would predict. The scaling exponents from Hoﬀmann et al. (2022) are
also closer to our prediction than the relation αD = αN, which has been proposed by other
models of neural scaling laws. Overall though, the existing empirical results are too messy
to deﬁnitively support or contradict our model.
0.2
0.4
0.6
0.8
1.0
1.2
N
0.2
0.4
0.6
0.8
1.0
1.2
D
Rosenfeld et al.
Kaplan et al.
Hoffmann et al.
Ardalani et al.
Gordon et al.
Droppo et al.
D =
N/(
N + 1)
D =
N
Figure 15: Parameter and data scaling exponents from various studies of deep learning
scaling, compiled from the database of neural scaling laws from Villalobos 2023. Our model
of scaling predicts that αD = αN/(αN + 1), indicated with the solid black line. Visible
points are from Rosenfeld et al. 2019; Kaplan et al. 2020; Hoﬀmann et al. 2022; Gordon
et al. 2021; Droppo and Elibol 2021. Ardalani et al. 2022 is above the visible window of the
ﬁgure.
24

