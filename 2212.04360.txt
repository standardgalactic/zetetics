MIME: Human-Aware 3D Scene Generation
Hongwei Yi1 Chun-Hao P. Huang2∗Shashank Tripathi1 Lea Hering1 Justus Thies1 Michael J. Black1
1Max Planck Institute for Intelligent Systems, T¨ubingen, Germany
2Adobe Inc.
{firstname.lastname}@{tuebingen.mpg.de} chunhaoh@adobe.com
Figure 1. Estimating 3D scenes from human movement. Given 3D human motion, e.g. from motion capture or body-worn sensors, we
reconstruct plausible 3D scenes in which the motion could have taken place. Our generative model is able to produce multiple realistic
scenes that take into account the locations and poses of the person, with appropriate human-scene contact.
Abstract
Generating realistic 3D worlds occupied by moving hu-
mans has many applications in games, architecture, and
synthetic data creation. But generating such scenes is ex-
pensive and labor intensive. Recent work generates human
poses and motions given a 3D scene. Here, we take the
opposite approach and generate 3D indoor scenes given
3D human motion. Such motions can come from archival
motion capture or from IMU sensors worn on the body, effec-
tively turning human movement into a “scanner” of the 3D
world. Intuitively, human movement indicates the free-space
in a room and human contact indicates surfaces or objects
that support activities such as sitting, lying or touching. We
propose MIME (Mining Interaction and Movement to infer
3D Environments), which is a generative model of indoor
scenes that produces furniture layouts that are consistent
with the human movement. MIME uses an auto-regressive
transformer architecture that takes the already generated
objects in the scene as well as the human motion as input,
and outputs the next plausible object. To train MIME, we
build a dataset by populating the 3D FRONT scene dataset
with 3D humans. Our experiments show that MIME pro-
duces more diverse and plausible 3D scenes than a recent
generative scene method that does not know about human
movement. Code and data are available for research at
https://mime.is.tue.mpg.de.
*This work was performed when C.P. H. was at the MPI-IS.
1. Introduction
Humans constantly interact with their environment. They
walk through a room, touch objects, rest on a chair, or sleep
in a bed. All these interactions contain information about
the scene layout and object placement. In fact, a mime is a
performer who uses our understanding of such interactions
to convey a rich, imaginary, 3D world using only their body
motion. Can we train a computer to take human motion
and, similarly, conjure the 3D scene in which it belongs?
Such a method would have many applications in synthetic
data generation, architecture, games, and virtual reality. For
example, there exist large datasets of 3D human motion
like AMASS [32] and such data rarely contains information
about the 3D scene in which it was captured. Could we
take AMASS and generate plausible 3D scenes for all the
motions? If so, we could use AMASS to generate training
data containing realistic human-scene interaction.
To answer such questions, we train a new method called
MIME (Mining Interaction and Movement to infer 3D Envi-
ronments) that generates plausible indoor 3D scenes based
on 3D human motion. Why is this possible? The key in-
tuitions are that (1) A human’s motion through free space
indicates the lack of objects, effectively carving out regions
of the scene that are free of furniture. And (2), when they
are in contact with the scene, this constrains both the type
and placement of 3D objects; e.g, a sitting human must be
sitting on something, such as a chair, a sofa, a bed, etc.
To make these intuitions concrete, we develop MIME,
1
arXiv:2212.04360v1  [cs.CV]  8 Dec 2022

which is a transformer-based auto-regressive 3D scene gen-
eration method that, given an empty ﬂoor plan and a human
motion sequence, predicts the furniture that is in contact
with the human. It also predicts plausible objects that have
no contact with the human but that ﬁt with the other objects
and respect the free-space constraints induced by the human
motion. To condition the 3D scene generation with human
motion, we estimate possible contact poses using POSA [20]
and divide the motion in contact and non-contact snippets.
The non-contact poses deﬁne free-space in the room, which
we encode as 2D ﬂoor maps, by projecting the foot vertices
onto the ground plane. The contact poses and correspond-
ing 3D human body models are represented by 3D bounding
boxes of the contact vertices predicted by POSA. We use this
information as input to the transformer and auto-regressively
predict the objects that fulﬁll the contact and free-space
constraints.
To train MIME, we built a new dataset called 3D-FRONT
Human that extends the large-scale synthetic scene dataset
3D-FRONT [15]. Speciﬁcally, we automatically populate
the 3D scenes with humans, i.e., non-contact humans (a
sequence of walking motion and standing humans) as well
as contact humans (sitting, touching, and lying humans). To
this end, we leverage motion sequences from AMASS [32],
as well as static contact poses from RenderPeople [42] scans.
At inference time, MIME is generating a plausible 3D
scene layout for the input motion, represented as 3D bound-
ing boxes. Based on this layout, we select 3D models from
the 3D-FUTURE dataset [16] and reﬁne their 3D placement
based on geometric constraints between the human poses
and the scene.
In comparison to pure 3D scene generation baselines like
ATISS [40], our method generates a 3D scene that supports
human contact and motion while putting plausible objects in
free space. In contrast to Pose2Room [37] which is a recent
pose-conditioned generative model, our method enables the
generation of objects that are not in contact with the human,
thus, predicting the entire scene instead of isolated objects.
We demonstrate that our method can directly be applied to
real captured motion sequences such as PROXD [19] without
ﬁnetuning.
In summary, we make the following contributions:
• a novel motion-conditioned generative model for 3D
room scenes that auto-repressively generates objects
that are in contact with the human or avoid free-space
deﬁned by the motion.
• a new 3D scene dataset with interacting humans and
free space humans which is constructed by populating
3D FRONT with static contact/standing poses from
RenderPeople and motion data of AMASS.
Figure 2. We divide input humans into two parts: contact humans
and free-space humans. We extract the 3D bounding boxes for
each contact human, and use non-maximum suppression on the
3D IoU to aggregate multiple humans in the same 3D space into
a single contact 3D bounding box (orange boxes). We project the
foot vertices of free-space humans on the ﬂoor plane, to get the 2D
free-space mask (dark blue).
2. Related Work
Generative Scene Synthesis (No People). Most prior work
on indoor scene synthesis, ignores the human and is based on
(1) procedural modeling with grammars[10, 27, 35, 39, 44,
45, 52]; (2) graph neural networks [12, 28, 30, 31, 45, 56, 62–
64, 64]; (3) autoregressive neural networks [48, 57]; or (4)
transformers [38, 41, 58]. Some works leverage lexical
text [6] or a sentence [7] as input to guide the 3D scene
synthesis. Fisher et al. [13] take 3D scans as input and syn-
thesize the corresponding 3D object arrangements. This is
extended [14] to also include functionality aspects in the
reconstruction. Recently, ATISS [41] performs scene syn-
thesis using a transformer-based architecture. ATISS takes
a ﬂoorplan as input and auto-regressively generates a 3D
scene that is represented as an unordered set of objects.
All methods mentioned above do not take human mo-
tion into consideration to guide the 3D scene synthesis. In
contrast, we generate 3D scenes that are compatible with
the humans deﬁned by a given input motion. Speciﬁcally,
the objects in the generated scene should support the hu-
man motion (e.g., a chair or couch for sitting) and should
not collide with the path of a walking human, To this end,
we build upon the auto-regressive scene synthesis architec-
ture of ATISS [41] and incorporate contact and free-space
information into the pipeline.
2

Figure 3. Method overview. In training, our method generates object M + 1 through a transformer encoder and a decoding module,
conditioned on the free space concatenated with the ﬂoor plan, contact humans cN
j=1, other existing objects oM
j=1 and a learnable query q.
We minimize the negative log-likelihood between the distribution of the generated object M + 1 and the ground truth. In inference, we
start from the ﬂoor plane, the free space and input contact humans cN
i=1 and assign the contact label of the ﬁrst human as 1 by default, to
autoregressively generate objects. At each step, we remove the contact humans that are overlapped with the previously generated object and
generate next objects until the end symbol is generated.
Human-aware Scene Reconstruction. Qi et al. [46] pro-
pose a method that synthesizes a 3D scene based on a hu-
man’s affordance map together with a spatial And-Or graph.
PiGraphs [49] learns a probability distribution over human
pose and object geometry from interactions. It does not
model the lack of interaction, i.e. the free space carved out
by movement. Similarly, recent methods [36, 37] explore
how to estimate a 3D scene from human behaviors and in-
teractions. Mura et al. [36] predict the “3D ﬂoor plan” from
a 2D human walking trajectory in a deterministic way. The
approach only indicates the room layout and furniture foot-
prints and does not model objects or contact. Nie et al. [37]
propose Pose2Room, which predicts 3D objects inside a
room from 3D human pose trajectories in a probabilistic
way, by learning 3D object arrangement distribution. It only
predicts contacted objects and can not generate objects in
free space. In addition, it cannot take ﬂoor plans as input.
We ﬁnd these crucial in our experiments since object arrange-
ments are highly related to the ﬂoor plan; e.g. some furniture
is designed to go against a wall.
Human-Scene Interaction Datasets. Many datasets ex-
ist for understanding humans or scenes in separation, but
relatively few address humans and scenes together. Hu-
man bodies are commonly captured using optical markers
[8, 25, 50], IMU sensors [24, 55], and multiple RGB cameras
[26, 33, 61]. See [53] for a comprehensive review. These
datasets contain only humans, forgoing the 3D environments
which the subjects interact with, e.g., ﬂoor plane, walls,
furniture. In contrast, real 3D scene datasets such as Mat-
terport3D [5], ScanNet [9] and Replica [51] are captured
primarily through time-of-ﬂight sensors, where humans are
excluded since only static content is reconstructed. Conse-
quently, despite having a large variety of scenes, they are not
suitable for modeling human-scene interaction.
To train MIME, we need diverse scene arrangement given
a set of sparse or continuously-moving bodies. While recent
real datasets [2, 17, 19, 23, 34, 59] capture both humans
and environments, they fail to provide sufﬁcient variety be-
cause the a priori scanned scenes are static and only the
subject moves. This limits the variety of scenes that can
be practically captured. Hassan et al. [18] use mocap to
capture a person interacting with objects like chairs, sofas
and tables. They then augment the dataset by changing the
size and shape of the objects and updating the human pose
using inverse kinematics. The approach does not capture
full scenes. For MIME we need a dataset with more variety.
Composite or synthetic datasets such as [1, 3, 42] are also
widely used for human mesh recovery, but the meaningful
human-scene interaction in them is fairly limited. To our
knowledge, Pose2Room [37] and GTA-IM [4] are the clos-
est to our needs. However, they represent humans with 3D
skeletons, which cannot represent realistic contact between
the body surface and the scene. Also the scene arrangement
is still not rich enough to train a generative model. Thus, we
introduce a new dataset called 3D FRONT Human, which
is generated by populating 3D scenes from 3D FRONT [15]
with humans that move and interact with the scene.
3. Method
Given input motion of a human and an empty or partially
occupied room of a speciﬁc kind (e.g., bedroom, living room,
etc.) with its ﬂoor plan, we learn a generative model that
can populate the room with objects that do not collide with
the input humans and also support them. To this end, we
propose a human-aware autoregressive model that represents
scenes as one unordered set of objects. We divide the objects
3

into two kinds, i.e., contact objects and non-contact objects,
based on the human-object interaction. Contact objects are
ones that humans interact with. Non-contact objects can be
placed anywhere in the free space of a room that makes se-
mantic sense. These objects enrich the content and potential
functionality of a room.
In the following, we describe our human-aware scene syn-
thesis model, MIME, which consists of two components: (1)
a generative scene synthesis method based on 3D bounding
boxes with object labels, and (2) a 3D reﬁnement method
that takes 3D human-scene interactions into account to opti-
mize the rotation and placement of the generated objects. In
Sec. 4, we detail the dataset generation process to train our
model.
3.1. Generative Human-aware Scene Synthesis
Given humans H and a ﬂoor plan F, our goal is to gen-
erate a “habitat” X = {H, F, S} where the 3D scene S can
support all human interactions and motions. In contrast to
the pure 3D scene generation methods [38, 41], we focus
on leveraging information from human motion to guide the
3D scene generation. To this end, we extract two types of
information from the input motion and the corresponding
human bodies: (i) contact humans C and (ii) free-space hu-
mans. We use POSA [20], to take posed human meshes and
automatically label which of their vertices are potentially in
contact with an object. Free-space humans are those that are
only in contact with the ﬂoor plane, F. These deﬁne a binary
mask that we call free-space mask FS, which is constructed
by the union of all projected foot contact points on F. This
free-space mask FS deﬁnes the region of a room that is free
from objects as a human can stand and walk there. Given
all contact humans, we compute the bounding boxes of their
contact vertices and keep only the non-overlapping boxes
using non-maximum suppression; we denote these as ci. The
collection of contact boxes is referred to as C = {ci}N
i=1. In-
stead of storing all contact vertices of all bodies, our features
are compact and encode complementary information. The
contact humans, represented by C, indicate where to locate
an object. See Fig. 2 top and middle rows for an illustration.
We represent a 3D scene S as an unordered set of objects,
consisting of two kinds of objects based on human-object
interaction. Objects in contact with the input human are
referred to as contact objects O = {o}N
i=1, while non-contact
objects Q = {q}M
i=1 are without any human interaction.
Formally, a 3D scene is the union of contact and non-contact
objects: S = O ∪Q.
The free-space mask FS, the ﬂoor plan F, the contact
humans C as well as the already existing objects S are in-
put to an auto-regressive transformer model. Each input is
encoded with a respective encoder, detailed below.
The log-likelihood of the generation of scene S including
contact objects and non-contact objects is:
log p(S) = log p(O|F, FS, C) + log p(Q|F, FS, C). (1)
To calculate the likelihood of all generated contact objects
Q, we accumulate the likelihood of every contact object:
p(O|F, FS, C) =
X
ˆ
O∈π(O)
Y
j∈ˆ
O
p (oj | o<j, F, FS, c≥j) ,
where p (oj | o<j, F, FS, c≥j) is the probability of generat-
ing the jth object conditioned on the input ﬂoor plan, free-
space humans, the rest of contact humans and the previously
generated objects, and π is the random permutation function
for those generated contact objects in the scene. The likeli-
hood of all non-contact objects Q is computed by replacing
the input contact humans with the corresponding generated
contact objects. During the training, we remove all contact
humans inside the room, thus, all contact objects O can be
treated as non-contact objects Q′:
p(Q|F, FS, C) = p(Q|F, FS, O)
= p(Q|F, FS, Q′)
=
X
ˆ
Q∈π(Q+Q′)
Y
j∈ˆ
Q
p (qj | q<j, F, FS) .
We follow [41] to use Monte Carlo sampling to approximate
all different object permutations during training, to make our
model invariant to the order of generated objects.
Free-Space Encoder. The 2D free-space mask FS is en-
coded together with the 2D ﬂoor plan F using a ResNet-18
[21]. The encoded feature provides the information to the
transformer encoder about where an object can be placed.
Contact Encoder. We represent the contact humans as 3D
bounding boxes, which consist of the contact label I, the
contact class category k (sitting, touching, lying), the trans-
lation t, the rotation r, and the size s. During generation
of a scene, we set the contact label I of one contact human
to 1 while the others are labeled 0. This label highlights
the contribution of the speciﬁc contact human to the next
generated contacted object. Note that we remove contact
humans from the input set if they are already in contact with
an existing object in the scene. Otherwise, we encode the
jth input contact human by applying:
Eθ : (Ij, kj, tj, rj, sj) →(Ij, λ(kj), p(tj), p(rj), p(sj)),
where λ (·) is a learnable embedding for the contact class
category k, and p (·) [54] is the positional encoding for the
translation t, rotation r and size s.
Furniture Encoder. The furniture encoder computes the
embedding of existing objects in the room:
Eθ : (Ij = 0, kj, tj, rj, sj) →(0, λ(kj), p(tj), p(rj), p(sj)).
4

Figure 4. Scene reﬁnement with the collision and contact loss from
MOVER [60]. In contact loss, all contact vertices (orange color)
are accumulate from all bodies into 3D space and the sofa and chair
are reﬁned by minimize the one-directional Chamfer Distance with
the contact vertices. In collision loss, we compute one uniform
SDF volume for all bodies, where the inside of bodies are denoted
as blue voxels. The table is optimized with it.
Note that the furniture encoder is sharing the same weight
as the contact encoder. The contact labels of the objects are
all zero, where j ∈[1, M].
Scene Synthesis Transformer. We pass the free-space fea-
ture F, context embedding T M+N
i=1
, and a learnable query
vector q ∈R64 into a transformer encoder τθ [11, 54] with-
out any positional encoding [54], to predict the feature ˆq that
is used to generate the next object:
τθ(F, T M+N
i=1
, q) →ˆq.
To decode the attribute distribution (ˆk, ˆt, ˆr, ˆs) of the gener-
ated object oM+1 from ˆq, we follow the same design from
ATISS [40]. Speciﬁcally, we employ an MLP for each at-
tribute in a consecutive fashion. Given ˆq, we ﬁrst predict the
class category label ˆk, then we predict the ˆt, ˆr and ˆs in this
speciﬁc order, where the previous attribute will be concate-
nated with the input ˆq for the next attribution prediction.
3.2. Training and Inference.
We train our model on the training set of 3D FRONT HU-
MAN, by maximizing the log-likelihood of each generated
scene S in Eq. (1). During training, we select a human-
populated scene in 3D FRONT HUMAN and add a random
permutation π (·) on all N contact and M non-contact ob-
jects. We randomly select the mth+1 as the generated object,
where m ∈[0, N + M]. Note that, m = 0 represents an
empty scene, while m = N + M indicates the generated
scene is already full and the class label of the predicted ob-
ject is an extra end symbol. Our model predicts the attribute
distribution of the generated object, conditioned on the ﬂoor
plane F, free space FS, previous m objects and contact
humans C; see Fig. 3. To enable our model to generate both
contact objects and non-contact objects, we make a data
augmentation for adding input contact humans or dropping
them out in equal frequency.
During inference, we start from an empty ﬂoor plane F
with input humans including free-space humans FS, and
contact humans C. We autoregressively sample the attribute
of the next generated object to put one object into a scene. By
default, we set the contact label of the ﬁrst contact human to
1, and the rest are 0. After each generation step, we remove
contact humans that are already in contact, by computing the
2D IoU of the human bounding box and the generated object
by projecting them on the ground plane. Speciﬁcally, if the
IoU is larger than 0.5, we remove the contact human from
the input. Once the end symbol is generated, the generated
scene is ﬁnished.
3.3. 3D Scene Reﬁnement
The generated scene from our model is represented with
3D bounding boxes. Based on the bounding box size and
class category label, we retrieve the closest mesh model from
3D FUTURE [16]. To improve the human-scene interaction
between the generated scenes and input humans, we apply
the collision loss and the contact loss from MOVER [60]
to reﬁne the object position, as can be seen in Fig. 4. We
calculate a uniﬁed SDF volume and accumulate all contact
vertices for all humans in the 3D space, and jointly opti-
mize the object alignment to improve human-object contact
and resolve 3D interpenetrations between humans and the
scene. The MOVER contact loss weight and the collision
loss weight are 1e5 and 1e3 respectively.
4. Dataset Generation of 3D FRONT HUMAN
To enable 3D scene generation from humans, we need a
dataset that consists of large numbers of rooms with a wide
variety of human interactions. Since no such dataset exists,
we generate a new synthetic dataset by populating the 3D
rooms in the 3D FRONT [15] with interactive humans. We
name the resulting dataset 3D FRONT HUMAN. To populate
the rooms of 3D FRONT with people, we insert humans
with contact and humans that stand or walk in free space,
as shown in Fig. 5. We represent people with the SMPL-X
model [43] and add contact humans from RenderPeople [42]
by randomly assigning plausible interactions to different
contactable objects in the room. Speciﬁcally, we allow for
three types of contact interactions: touching, sitting, and
lying. In Fig. 5 (bottom), we put a lying down person on
a bed, and multiple humans interact with a nightstand or
5

Figure 5. The illustration of populated 3D scenes in 3D FRONT
HUMAN. Given a room, we put random numbers of static “stand-
ing” people and add multiple “walking” motion sequences with
variant start positions and directions in the free space. We also put
various “contact humans” into the scene so that their interaction
with the objects makes sense, e.g., “touching” and “lying”.
wardrobe. In the free space, we put a random number of
static standing people and add multiple walking motion clips
from AMASS [32] with random start positions and directions
to the scene, and remove humans that intersect with objects
in the scene.
5. Experiments
We qualitatively and quantitatively evaluate our method
and compare with two baselines. Speciﬁcally, we compare to
the 3D scene generation method ATISS [41] and the human-
aware scene reconstruction method Pose2Room [37].
Evaluation Datasets. Our human-populated dataset 3D
FRONT HUMAN contains four room types: 1) 5689 bed-
rooms, 2) 2987 living rooms, 3) 2549 dining rooms and 4)
679 libraries. We use 21 object categories for the bedrooms,
24 for the living and dining rooms, and 25 for the libraries.
We independently train our model four times on the four
kinds of rooms. Following our baseline ATISS [41], for each
kind of room, we split the data 80%, 10%, 10% into training,
validation and test sets. We train and validate MIME on the
training and validation sets respectively, and evaluate it on
the test set. See more details in Sup. Mat. Since ATISS [41]
does not provide a pretrained model, we retrain it with the
ofﬁcial code1 following the same training strategy on the
original 3D FRONT dataset as one of our baseline.
To evaluate the effectiveness and generalization of our
method, we test MIME on a real RGB-D motion captured
dataset PROX-D [19] and compare it with Pose2Room [37].
Pose2Room needs a sequence of human motions that are
in contact with objects. Our 3D FRONT HUMAN does
not provide these interactive human-object motions, so we
cannot enable ﬁne tune and evaluate Pose2Room on 3D
FRONT HUMAN.
Evaluation Metrics. We compare MIME with the baselines
in two different ways: (i) the plausibility between human-
scene interaction and (ii) the realism of the generated scenes
only. We propose a interpenetration loss (↓) to evaluate the
collision between the generated objects and the free space,
through computing the ratio of the violated free space by the
2D projection of the generated objects:
Linter =


M
X
j=1
X
p∈Oj
FS(p)

/
X
p∈FS
FS(p),
where p denotes each pixel on the ﬂoor plane image. We
calculate the 2D IoU and 3D IoU between generated objects
and input contact bounding boxes to measure the human-
object interaction. To evaluate the realism and diversity of
generated scenes, we follow common practice [41, 62] and
calculate the FID [22] (at 2562 resolution) score between
bird-eye view orthographic projections of generated scenes
and real scenes from the test set, as well as the category
KL divergence. We compute the FID score 10 times and
report the mean and variance of it. All these evaluation
experiments are conducted on the test split of the 3D FRONT
HUMAN dataset.
5.1. Human-aware Scene Synthesis.
In Fig. 6, we visualize the ability of our method to gener-
ate plausible 3D scenes from input motion and ﬂoor plans for
different kinds of rooms; we also show our baseline methods
for comparison. See Sup. Mat. for more examples. Note
that the original ATISS [41] model generates a 3D scene
only based on the ﬂoor plan, without taking the humans
into account. Thus, generated scenes from ATISS violate
free space constraint and are not consistent with the human
contact. For a more fair comparison, we extend ATISS to
take information about the human motion as input. Specif-
ically, we adapt the 2D input ﬂoor plan to also contain the
free space information of the walking and standing humans.
However, ATISS with input free space still generates objects
in free space, while also generating implausible object con-
ﬁgurations such as the white closet inside the bed (Fig. 6,
1https://github.com/nv-tlabs/ATISS/commit/6b46c11.
6

Figure 6. Qualitative comparison on the test split in 3D FRONT HUMAN. Given free space and contact humans as input, MIME generates
more plausible scenes in which the contact humans interact with the contact objects and the free space humans have fewer collisions with all
the generated objects. We also show the original ATISS w/ or w/o the free space mask as input. All results are w/o reﬁnement. Top and
bottom rows represent two different example inputs.
Interpenetration(↓)
2D IoU(↑)
3D IoU(↑)
FID Score (↓)
Category KL Div. (↓)
ATISS [41]
Ours
ATISS [41]
Ours
ATISS [41]
Ours
ATISS [41]
Ours
ATISS [41]
Ours
Bedroom
0.348
0.129
0.472
0.939
0.376
0.756
70.21±1.80
74.18±2.19
0.028
0.044
Living
0.129
0.050
0.480
0.971
0.360
0.920
130.61±1.27
150.03± 1.00
0.004
0.053
Dining
0.121
0.047
0.163
0.959
0.122
0.769
45.99 ± 0.90
76.75 ± 1.45
0.004
0.037
Library
0.139
0.106
0.351
0.725
0.390
0.570
93.16 ± 2.59
118.34±2.94
0.066
0.093
Table 1. Quantitative comparison on the test split of the 3D FRONT HUMAN dataset. The interpenetration loss, 2D IoU and 3D IoU are
used to evaluate human-scene interaction in generated scenes. The FID score (reported at 2562) and category KL divergency are used to
evaluate the realism and diversity of generated scenes, compared with ground truth scenes.
top). In contrast, MIME generates plausible 3D scenes that
have less interpenetration with the free space and support
interacting humans; e.g. a bed beneath a lying person and a
chair under a sitting person.
The observations in the qualitative comparison are also
conﬁrmed by a quantitative evaluation in Tab. 1. MIME
achieves signiﬁcant improvements on human-scene interac-
tion evaluation metrics compared with ATISS. Note, since
our scene generation is constrained by the input motion,
the diversity scores (FID, KL divergence) are lower than
of ATISS, which is not human-aware. This is not a fail-
ure/limitation of MIME.
To evaluate the generalization of our method, we test
it on a real dataset of human motion. We consider the
PROXD [19] dataset and the 3D bounding box annotation
from [60]. We use it without ﬁnetuning, and use the mo-
tions to generate scenes. We compare our method with
Pose2Room [37], which predicts 3D objects from a motion
Method
3D IoU
P2R-Net [37] w/o pretrain
5.36
Ours (MIME) w/o pretrain
8.47
Table 2. Comparisons on 3D object detection accuracy (mAP@0.5)
using the PROXD qualitative dataset [19].
7

Figure 7.
Evaluation on PROX [19, 60].
Compared with
Pose2Room [37], MIME (w/o ﬁnetuning and w/o reﬁnement) can
not only generate more accurate contact objects, but it also gener-
ates objects appropriately in free space. GT = ground truth.
sequence of 3D skeletons. Note that Pose2Room can only
predict contact objects, it does not predict an entire scene
which is the goal of our method. Fig. 7 presents a qualitative
comparison of the methods and we report the quantitative
metrics in Tab. 2. Speciﬁcally, we compute the mean average
precision with 3D IoU 0.5 (mAP@0.5) to evaluate the 3D ob-
ject detection accuracy for those contact objects only. Both
methods are probabilistic generative models to predict the
object attribute distribution. Following Pose2Room, we use
the same 5 input motions and sample 10 scenes for each mo-
tion sequence, and report the mean value of it. Our method
achieves better 3D object detection accuracy compared to
Pose2Room without pretraining.
5.2. Ablation Study on Input Humans
In Fig. 8, we evaluate the inﬂuence of the density of free-
space humans, and the number of contact humans, that we
provide as input to MIME. We observe that MIME generates
contact objects according to the number of contact humans
and, as the density of free-space humans increases, MIME
generates fewer objects in scenes. This is as expected.
6. Discussion
Given a sequence of human motions, MIME generates
diverse and plausible scenes with which the humans interact.
We assume that the generated scenes are static, and future
work should explore generating moving objects by exploring
the interaction between humans and moving objects, such as
moving a chair, grasping a cup, opening a door, etc.
MIME, like ATISS, needs a pre-deﬁned ﬂoor plan room
layout as input. The resolution of the 2D ﬂoor plan is coarse;
Figure 8. Ablation study on different number of contact humans
and different density of free space humans. In a), with more con-
tact humans as input, the generated scenes contain more occupied
objects. In b), the more free space humans have in a room leads to
fewer available generated objects in a scene.
i.e., 1 pixel stands for around 10 centimeters, which is ex-
tracted as a 512 dimension feature by ResNet-18. Intro-
ducing a ﬁner ﬂoor plan representation, such as dividing
one ﬂoor plan into multiple patches (cf. ViT[47]) or simply
enlarging the size of the feature dimension could improve
the generated object placement, resulting in less collision
between the humans and the free space. Another interesting
direction is to estimate a ﬂoor plan and 3D object layout
jointly from input humans only.
During inference, MIME uses a hand-crafted metric 2D
IoU between the generated objects and the input contact
humans to factor out which human it is in contacted with. A
simple extension would be to use the network to learn this in-
formation. Our model directly estimates 3D bounding boxes
as a 3D scene representation, followed by a scene reﬁnement
that places the mesh models into the scene. Learning to di-
rectly estimate the mesh models from the interacting humans
is another promising direction.
7. Conclusion
We have introduced MIME, which generates varied furni-
ture layouts that are consistent with input human movement
and contacts. To train MIME, we built a new dataset called
3D FRONT HUMAN, by populating humans into the large-
scale synthetic scene dataset [15]. We have demonstrated
that by incorporating input human motion into free space
and contact boxes, our method can generate multiple real-
istic scenes, where the input motion can take place. MIME
has many applications, particularly for generating synthetic
training data at scale. MIME provides a means of taking
existing human motion capture data and “upgrading” it to
include plausible 3D scenes that are consistent with it.
8

Acknowledgments.
We thank Despoina Paschalidou,
Wamiq Para for useful feedback about the reimplementation
of ATISS, and Yuliang Xiu, Weiyang Liu, Yandong Wen, Yao
Feng for the insightful discussions, and Benjamin Pellkofer
for IT support. This work was supported by the German Fed-
eral Ministry of Education and Research (BMBF): T¨ubingen
AI Center, FKZ: 01IS18039B.
Disclosure. MJB has received research gift funds from Adobe,
Intel, Nvidia, Meta/Facebook, and Amazon. MJB has ﬁnancial in-
terests in Amazon, Datagen Technologies, and Meshcapade GmbH.
JT has received research gift funds from Microsoft Research.
References
[1] Eduard Gabriel Bazavan, Andrei Zanﬁr, Mihai Zanﬁr,
William T Freeman, Rahul Sukthankar, and Cristian Smin-
chisescu. Hspace: Synthetic parametric humans animated
in complex environments. arXiv preprint arXiv:2112.12867,
2021. 3
[2] Bharat Lal Bhatnagar, Xianghui Xie, Ilya Petrov, Cristian
Sminchisescu, Christian Theobalt, and Gerard Pons-Moll.
Behave: Dataset and method for tracking human object inter-
actions. In Computer Vision and Pattern Recognition (CVPR).
IEEE, 2022. 3
[3] Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei,
Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, and Zi-
wei Liu. Playing for 3d human recovery. arXiv preprint
arXiv:2110.07588, 2021. 3
[4] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qizhi Cai, Minh
Vo, and Jitendra Malik. Long-term human motion prediction
with scene context. In European Conference on Computer
Vision (ECCV), 2020. 3
[5] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3D: Learning from rgb-d
data in indoor environments. International Conference on 3D
Vision (3DV), 2017. 3
[6] Angel Chang, Will Monroe, Manolis Savva, Christopher
Potts, and Christopher D Manning.
Text to 3d scene
generation with rich lexical grounding.
arXiv preprint
arXiv:1505.06289, 2015. 2
[7] Angel X Chang, Mihail Eric, Manolis Savva, and Christo-
pher D Manning. Sceneseer: 3d scene design with natural
language. arXiv preprint arXiv:1703.00050, 2017. 2
[8] CMU Graphics Lab. CMU Graphics Lab Motion Capture
Database. http://mocap.cs.cmu.edu/, 2000. 3
[9] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber,
Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-
annotated 3d reconstructions of indoor scenes. In Computer
Vision and Pattern Recognition (CVPR), 2017. 3
[10] Jeevan Devaranjan, Amlan Kar, and Sanja Fidler. Meta-sim2:
Learning to generate synthetic datasets. In ECCV, 2020. 2
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova.
Bert:
Pre-training of deep bidirectional
transformers for language understanding.
arXiv preprint
arXiv:1810.04805, 2018. 5
[12] Xinhan Di, Pengqian Yu, Hong Zhu, Lei Cai, Qiuyan Sheng,
Changyu Sun, and Lingqiang Ran. Structural plan of indoor
scenes with personalized preferences. In European Confer-
ence on Computer Vision (ECCV), pages 455–468. Springer,
2020. 2
[13] Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas
Funkhouser, and Pat Hanrahan. Example-based synthesis of
3d object arrangements. Transactions on Graphics (TOG),
31(6):1–11, 2012. 2
[14] Matthew Fisher, Manolis Savva, Yangyan Li, Pat Hanrahan,
and Matthias Nießner. Activity-centric scene synthesis for
functional 3d scene modeling. Transactions on Graphics
(TOG), 34(6):1–13, 2015. 2
[15] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming
Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Bin-
qiang Zhao, et al. 3d-front: 3d furnished rooms with layouts
and semantics. In International Conference on Computer
Vision (ICCV), pages 10933–10942, 2021. 2, 3, 5, 8
[16] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang
Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furni-
ture shape with texture. International Journal of Computer
Vision (IJCV), pages 1–25, 2021. 2, 5
[17] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard
Pons-Moll. Human poseitioning system (HPS): 3d human
pose estimation and self-localization in large scenes from
body-mounted sensors.
In Computer Vision and Pattern
Recognition (CVPR), pages 4318–4329, 2021. 3
[18] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito,
Jimei Yang, Yi Zhou, and Michael Black. Stochastic scene-
aware motion prediction. In International Conference on
Computer Vision (ICCV), Oct. 2021. 3
[19] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and
Michael J. Black. Resolving 3D human pose ambiguities
with 3D scene constraints. In International Conference on
Computer Vision (ICCV), pages 2282–2292, 2019. 2, 3, 6, 7,
8
[20] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios
Tzionas, and Michael J. Black. Populating 3D scenes by
learning human-scene interaction. In Proceedings IEEE/CVF
Conf. on Computer Vision and Pattern Recognition (CVPR),
June 2021. 2, 4
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 4
[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-
hard Nessler, and Sepp Hochreiter. Gans trained by a two time-
scale update rule converge to a local nash equilibrium. Con-
ference on Neural Information Processing Systems (NeurIPS),
30, 2017. 6
[23] Chun-Hao P. Huang, Hongwei Yi, Markus H¨oschle, Matvey
Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel
Scharstein, and Michael J. Black. Capturing and inferring
dense full-body human-scene contact. In Computer Vision
and Pattern Recognition (CVPR), 2022. 3
[24] Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J
Black, Otmar Hilliges, and Gerard Pons-Moll. Deep inertial
poser: Learning to reconstruct human pose from sparse in-
ertial measurements in real time. Transactions on Graphics
(TOG), 37(6):1–15, 2018. 3
[25] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
9

Sminchisescu. Human3.6m: Large scale datasets and predic-
tive methods for 3d human sensing in natural environments.
Transactions on Pattern Analysis and Machine Intelligence
(TPAMI), 36(7):1325–1339, jul 2014. 3
[26] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan,
Lin Gui, Sean Banerjee, Timothy Scott Godisart, Bart Nabbe,
Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser
Sheikh. Panoptic studio: A massively multiview system for
social interaction capture. Transactions on Pattern Analysis
and Machine Intelligence (TPAMI), 2017. 3
[27] Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci,
Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba,
and Sanja Fidler. Meta-sim: Learning to generate synthetic
datasets. In ICCV, 2019. 2
[28] Mohammad Keshavarzi, Aakash Parikh, Xiyu Zhai, Melody
Mao, Luisa Caldas, and Allen Y Yang. Scenegen: Genera-
tive contextual scene augmentation using scene graph priors.
arXiv preprint arXiv:2009.12395, 2020. 2
[29] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 12
[30] Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaud-
huri, Owais Khan, Ariel Shamir, Changhe Tu, Baoquan Chen,
Daniel Cohen-Or, and Hao Zhang. Grains: Generative re-
cursive autoencoders for indoor scenes.
Transactions on
Graphics (TOG), 38(2):1–16, 2019. 2
[31] Andrew Luo, Zhoutong Zhang, Jiajun Wu, and Joshua B
Tenenbaum. End-to-end optimization of scene layout. In
Computer Vision and Pattern Recognition (CVPR), pages
3754–3763, 2020. 2
[32] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-
ard Pons-Moll, and Michael J. Black. AMASS: Archive of
motion capture as surface shapes. In International Conference
on Computer Vision (ICCV), pages 5442–5451, Oct. 2019. 1,
2, 6
[33] Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller,
Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll, and Chris-
tian Theobalt. Single-shot multi-person 3D pose estimation
from monocular rgb. In International Conference on 3D
Vision (3DV). IEEE, sep 2018. 3
[34] Aron Monszpart, Paul Guerrero, Duygu Ceylan, Ersin Yumer,
and Niloy J. Mitra. iMapper: interaction-guided scene map-
ping from monocular videos.
Transactions on Graphics
(TOG), 38(4):92:1–92:15, 2019. 3
[35] Pascal M¨uller, Peter Wonka, Simon Haegler, Andreas Ulmer,
and Luc Van Gool. Procedural modeling of buildings. pages
614–623, 2006. 2
[36] Claudio Mura, Renato Pajarola, Konrad Schindler, and Niloy
Mitra. Walk2map: Extracting ﬂoor plans from indoor walk
trajectories. In Computer Graphics Forum (CGF), volume 40,
pages 375–388, 2021. 3
[37] Yinyu Nie, Angela Dai, Xiaoguang Han, and Matthias
Nießner. Pose2room: understanding 3d scenes from human
activities. In European Conference on Computer Vision, pages
425–443. Springer, 2022. 2, 3, 6, 7, 8
[38] Wamiq Reyaz Para, Paul Guerrero, Niloy Mitra, and Peter
Wonka. Cofs: Controllable furniture layout synthesis. arXiv
preprint arXiv:2205.14657, 2022. 2, 4
[39] Yoav IH Parish and Pascal M¨uller. Procedural modeling
of cities. In Proceedings of the 28th annual conference on
Computer graphics and interactive techniques, pages 301–
308, 2001. 2
[40] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten
Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregres-
sive transformers for indoor scene synthesis. Conference on
Neural Information Processing Systems (NeurIPS), 34:12013–
12026, 2021. 2, 5
[41] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten
Kreis, Andreas Geiger, and Sanja Fidler. ATISS: Autoregres-
sive transformers for indoor scene synthesis. In Conference
on Neural Information Processing Systems (NeurIPS), 2021.
2, 4, 6, 7, 12
[42] Priyanka Patel, Chun-Hao P. Huang, Joachim Tesch, David T.
Hoffmann, Shashank Tripathi, and Michael J. Black.
AGORA: Avatars in geography optimized for regression anal-
ysis. In Computer Vision and Pattern Recognition (CVPR),
June 2021. 2, 3, 5
[43] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo
Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and
Michael J. Black. Expressive body capture: 3d hands, face,
and body from a single image. In Computer Vision and Pat-
tern Recognition (CVPR), 2019. 5
[44] Aayush Prakash, Shaad Boochoon, Mark Brophy, David
Acuna, Eric Cameracci, Gavriel State, Omer Shapira, and
Stan Birchﬁeld. Structured domain randomization: Bridg-
ing the reality gap by context-aware synthetic data. pages
7249–7255. IEEE, 2019. 2
[45] Pulak Purkait, Christopher Zach, and Ian Reid. Sg-vae: Scene
grammar variational autoencoder to generate new indoor
scenes. In European Conference on Computer Vision (ECCV),
pages 155–171. Springer, 2020. 2
[46] Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and
Song-Chun Zhu. Human-centric indoor scene synthesis us-
ing stochastic grammar. In Computer Vision and Pattern
Recognition (CVPR), pages 5899–5908, 2018. 3
[47] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In International Con-
ference on Computer Vision (ICCV), pages 12179–12188,
2021. 8
[48] Daniel Ritchie, Kai Wang, and Yu-an Lin. Fast and ﬂexi-
ble indoor scene synthesis via deep convolutional generative
models. In Computer Vision and Pattern Recognition (CVPR),
pages 6182–6190, 2019. 2
[49] Manolis Savva, Angel X. Chang, Pat Hanrahan, Matthew
Fisher, and Matthias Nießner. PiGraphs: Learning Interac-
tion Snapshots from Observations. ACM Transactions on
Graphics (TOG), 35(4), 2016. 3
[50] Leonid Sigal, Alexandru O Balan, and Michael J Black. Hu-
maneva: Synchronized video and motion capture dataset and
baseline algorithm for evaluation of articulated human motion.
International Journal of Computer Vision (IJCV), 87(1):4–27,
2010. 3
[51] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik
Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl
Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian
Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kim-
berly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham,
Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Ba-
10

tra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele,
Steven Lovegrove, and Richard Newcombe. The Replica
dataset: A digital replica of indoor spaces. arXiv preprint
arXiv:1906.05797, 2019. 3
[52] Jerry O Talton, Yu Lou, Steve Lesser, Jared Duke, Radom´ır
Mˇech, and Vladlen Koltun. Metropolis procedural modeling.
Transactions on Graphics (TOG), 30(2):1–14, 2011. 2
[53] Yating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang.
Recovering 3d human mesh from monocular images: A sur-
vey. arXiv preprint arXiv:2203.01923, 2022. 3
[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. volume 30, 2017. 4, 5
[55] Timo von Marcard, Roberto Henschel, Michael J. Black,
Bodo Rosenhahn, and Gerard Pons-Moll. Recovering ac-
curate 3D human pose in the wild using IMUs and a mov-
ing camera. In European Conference on Computer Vision
(ECCV), pages 614–631, 2018. 3
[56] Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, An-
gel X Chang, and Daniel Ritchie. Planit: Planning and instan-
tiating indoor scenes with relation graph and spatial prior net-
works. Transactions on Graphics (TOG), 38(4):1–15, 2019.
2
[57] Kai Wang, Manolis Savva, Angel X Chang, and Daniel
Ritchie. Deep convolutional priors for indoor scene synthesis.
Transactions on Graphics (TOG), 37(4):1–14, 2018. 2
[58] Xinpeng Wang, Chandan Yeshwanth, and Matthias Nießner.
Sceneformer: Indoor scene generation with transformers.
arXiv preprint arXiv:2012.09793, 2020. 2
[59] Zhe Wang, Liyan Chen, Shaurya Rathore, Daeyun Shin, and
Charless Fowlkes. Geometric pose affordance: 3D human
pose with scene constraints. arXiv preprint arXiv:1905.07718,
2019. 3
[60] Hongwei Yi, Chun-Hao P. Huang, Dimitrios Tzionas,
Muhammed Kocabas, Mohamed Hassan, Siyu Tang, Justus
Thies, and Michael J. Black. Human-aware object placement
for visual environment reconstruction. In Computer Vision
and Pattern Recognition (CVPR), 2022. 5, 7, 8
[61] Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth Venkatesh,
Jaesik Park, Jihun Yu, and Hyun Soo Park. HUMBI: A large
multiview dataset of human body expressions. In Computer
Vision and Pattern Recognition (CVPR), June 2020. 3
[62] Song-Hai Zhang, Shao-Kui Zhang, Wei-Yu Xie, Cheng-Yang
Luo, and Hong-Bo Fu. Fast 3d indoor scene synthesis with
discrete and exact layout pattern extraction. arXiv preprint
arXiv:2002.00328, 2020. 2, 6
[63] Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo,
Alexander Huth, Etienne Vouga, and Qixing Huang. Deep
generative modeling for scene synthesis via hybrid represen-
tations. Transactions on Graphics (TOG), 39(2):1–21, 2020.
[64] Yang Zhou, Zachary While, and Evangelos Kalogerakis.
Scenegraphnet: Neural message passing for 3d indoor scene
augmentation. In International Conference on Computer
Vision (ICCV), pages 7384–7392, 2019. 2
11

Figure 10. Qualitative comparison on libraries in the test split of
3D FRONT HUMAN. Given free space and contact humans as
input, MIME generates more plausible scenes in which the contact
humans interact with the contact objects and the free space humans
have fewer collisions with all the generated objects. We also show
the original ATISS w/ or w/o the free space mask as input. All
results are w/o reﬁnement. Each row represent an example input.
Figure 11. Qualitative comparison on living rooms (the ﬁrst two
rows) and dining rooms (the last two rows) in the test split of
3D FRONT HUMAN. Given free space and contact humans as
input, MIME generates more plausible scenes in which the contact
humans interact with the contact objects and the free space humans
have fewer collisions with all the generated objects. We also show
the original ATISS w/ or w/o the free space mask as input. All
results are w/o reﬁnement. Each row represent an example input.
Appendices
A. Training Details
During training, we apply the Adam optimizer [29] with
learning rate 1e−4 and no weight decay. In Adam optimizer,
we use the default PyTorch implemented parameters, i.e.,
β1 = 0.9, β2 = 0.999 and ϵ = 1e −8. We train MIME with
the batch size 128 for 100k iterations. We perform random
global rotation augmentation between [0, 360] degrees on
the holistic populated scene, including the ﬂoor plane, all
objects, the free space and all contact humans.
B. More Qualitative Examples
We present more qualitative examples for different kinds
of rooms, in Fig. 9, Fig. 10, and Fig. 11. Compared with
our baseline methods [41], our method can generate more
plausible 3D scenes that input motions can interact with.
Figure 9. Qualitative comparison on bedrooms in the test split of
3D FRONT HUMAN. Given free space and contact humans as
input, MIME generates more plausible scenes in which the contact
humans interact with the contact objects and the free space humans
have fewer collisions with all the generated objects. We also show
the original ATISS w/ or w/o the free space mask as input. All
results are w/o reﬁnement. Each row represents an example input.
12

