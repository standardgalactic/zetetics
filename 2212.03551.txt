arXiv:2212.03551v2  [cs.CL]  11 Dec 2022
Talking About Large Language Models
Murray Shanahan
Imperial College London
m.shanahan@imperial.ac.uk
December 2022
Abstract
Thanks to rapid progress in artiﬁcial intelligence,
we have entered an era when technology and
philosophy intersect in interesting ways.
Sit-
ting squarely at the centre of this intersection
are large language models (LLMs).
The more
adept LLMs become at mimicking human lan-
guage, the more vulnerable we become to an-
thropomorphism, to seeing the systems in which
they are embedded as more human-like than they
really are. This trend is ampliﬁed by the natu-
ral tendency to use philosophically loaded terms,
such as “knows”, “believes”, and “thinks”, when
describing these systems. To mitigate this trend,
this paper advocates the practice of repeatedly
stepping back to remind ourselves of how LLMs,
and the systems of which they form a part, ac-
tually work. The hope is that increased scien-
tiﬁc precision will encourage more philosophical
nuance in the discourse around artiﬁcial intelli-
gence, both within the ﬁeld and in the public
sphere.
1
Introduction
The advent of large language models (LLMs)
such as Bert (Devlin et al., 2018) and GPT-
2 (Radford et al., 2019) was a game-changer
for artiﬁcial intelligence. Based on transformer
architectures (Vaswani et al., 2017),
compris-
ing hundreds of billions of parameters, and
trained on hundreds of terabytes of textual data,
their contemporary successors such as GPT-3
(Brown et al., 2020), Gopher (Rae et al., 2021),
and PaLM (Chowdhery et al., 2022) have given
new meaning to the phrase “unreasonable eﬀec-
tiveness of data” (Halevy et al., 2009).
The eﬀectiveness of these models is “unreason-
able” (or, with the beneﬁt of hindsight, some-
what surprising) in three inter-related ways.
First, the performance of LLMs on benchmarks
scales with the size of the training set (and, to
a lesser degree with model size). Second, there
are qualitative leaps in capability as the models
scale. Third, a great many tasks that demand in-
telligence in humans can be reduced to next token
prediction with a suﬃciently performant model.
It is the last of these three surprises that is the
focus of the present paper.
As we build systems whose capabilities more
and more resemble those of humans, despite the
fact that those systems work in ways that are
fundamentally diﬀerent from the way humans
work, it becomes increasingly tempting to an-
thropomorphise them. Humans have evolved to
co-exist over many millions of years, and human
culture has evolved over thousands of years to
facilitate this co-existence, which ensures a de-
gree of mutual understanding. But it is a serious
mistake to unreﬂectingly apply to AI systems the
same intuitions that we deploy in our dealings
with each other, especially when those systems
are so profoundly diﬀerent from humans in their
underlying operation.
The AI systems we are building today have
considerable utility and enormous commercial
potential, which imposes on us a great respon-
sibility. To ensure that we can make informed
decisions about the trustworthiness and safety of
the AI systems we deploy, it is advisable to keep
to the fore the way those systems actually work,
and thereby to avoid imputing to them capaci-
ties they lack, while making the best use of the
remarkable capabilities they genuinely possess.
2
What LLMs Really Do
As Wittgenstein reminds us, human language use
is an aspect of human collective behaviour, and
it only makes sense in the wider context of the
human social activity of which it forms a part
(Wittgenstein, 1953).
A human infant is born
into a community of language users with which
1

it shares a world, and it acquires language by
interacting with this community and with the
world it shares with them. As adults (or indeed
as children past a certain age), when we have a
casual conversation, we are engaging in an activ-
ity that is built upon this foundation. The same
is true when we make a speech or send an email
or deliver a lecture or write a paper. All of this
language-involving activity makes sense because
we inhabit a world we share with other language
users.
A large language model is a very diﬀer-
ent sort of animal (Bender and Koller, 2020;
Bender et al., 2021; Marcus and Davis, 2020).
(Indeed, it is not an animal at all, which is very
much to the point.) LLMs are generative math-
ematical models of the statistical distribution
of tokens in the vast public corpus of human-
generated text, where the tokens in question in-
clude words, parts of words, or individual char-
acters including punctuation marks.
They are
generative because we can sample from them,
which means we can ask them questions.
But
the questions are of the following very speciﬁc
kind. “Here’s a fragment of text. Tell me how
this fragment might go on. According to your
model of the statistics of human language, what
words are likely to come next?”1
It is very important to bear in mind that this
is what large language models really do. Sup-
pose we give an LLM the prompt “The ﬁrst per-
son to walk on the Moon was ”, and suppose it
responds with “Neil Armstrong”. What are we
really asking here?
In an important sense, we
are not really asking who was the ﬁrst person to
walk on the Moon. What we are really asking
the model is the following question: Given the
statistical distribution of words in the vast pub-
lic corpus of (English) text, what words are most
likely to follow the sequence “The ﬁrst person to
walk on the Moon was ”? A good reply to this
question is “Neil Armstrong”.
Similarly, we might give an LLM the prompt
“Twinkle twinkle ”, to which it will most likely
respond “little star”. On one level, for sure, we
are asking the model to remind us of the lyrics
of a well-known nursery rhyme. But in an im-
portant sense what we are really doing is ask-
ing it the following question: Given the statis-
1Even if an LLM is ﬁne-tuned, for example using rein-
forcement learning with human feedback (e.g. to ﬁlter out
potentially toxic language) (Glaese et al., 2022), the re-
sult is still a model of the distribution of tokens in human
language, albeit one that has been slightly perturbed.
tical distribution of words in the public corpus,
what words are most likely to follow the sequence
“Twinkle twinkle ”? To which an accurate an-
swer is “little star”.
Here’s a third example. Suppose you are the
developer of an LLM and you prompt it with
the words “After the ring was destroyed, Frodo
Baggins returned to ”, to which it responds “the
Shire”.
What are you doing here?
On one
level, it seems fair to say, you might be testing
the model’s knowledge of the ﬁctional world of
Tolkien’s novels.
But, in an important sense,
the question you are really asking (as you pre-
sumably know, because you are the developer) is
this: Given the statistical distribution of words
in the public corpus, what words are most likely
to follow the sequence “After the ring was de-
stroyed, Frodo Baggins returned to ”? To which
an appropriate response is “the Shire”.
To the human user, each of these examples
presents a diﬀerent sort of relationship to truth.
In the case of Neil Armstrong, the ultimate
grounds for the truth or otherwise of the LLMs
answer is the real world. The Moon is a real ob-
ject and Neil Armstrong was a real person, and
his walking on the Moon is a fact about the phys-
ical world. Frodo Baggins, on the other hand, is
a ﬁctional character, and the Shire is a ﬁctional
place. Frodo’s return to the Shire is a fact about
an imaginary world, not a real one. As for the lit-
tle star in the nursery rhyme, well that is barely
even a ﬁctional object, and the only fact at issue
is the occurrence of the words “little star” in a
familiar English rhyme.
These distinctions are invisible at the level of
what the LLM actually does, which is simply to
generate statistically likely sequences of words.
However, when we evaluate the utility of the
model, these distinctions matter a great deal.
There is no point in seeking Frodo’s (ﬁctional)
descendants in the (real) English county of Sur-
rey. This is one reason why it’s a good idea for
users to repeatedly remind themselves of what
LLMs really do. It’s also a good idea for develop-
ers to remind themselves of this, to avoid the mis-
leading use of philosophically fraught words to
describe the capabilities of LLMs, words such as
“belief”, “knowledge”, “understanding”, “self”,
or even “consciousness”.
2

3
LLMs and the Intentional Stance
It is perfectly natural to use anthropomorphic
language in everyday conversations about arte-
facts, especially in the context of information
technology.
We do it all the time.
My watch
doesn’t realise we’re on daylight saving time. My
phone thinks we’re in the car park.
The mail
server won’t talk to the network.
And so on.
These examples of what Dennett calls the inten-
tional stance are harmless and useful forms of
shorthand for complex processes whose details
we don’t know or care about.2 They are harm-
less because no-one takes them seriously enough
to ask their watch to get it right next time, say, or
to tell the mail server to try harder. Even with-
out having read Dennett, everyone understands
they are taking the intentional stance, that these
are just useful turns of phrase.
The same consideration applies to LLMs, both
for users and for developers. Insofar as everyone
implicitly understands that these turns of phrase
are just convenient shorthands, that they are
taking the intentional stance, it does no harm to
use them. However, in the case of LLMs, such is
their power, things can get a little blurry. When
an LLM can be made to improve its performance
on reasoning tasks simply by being told to “think
step by step” (Kojima et al., 2022) (to pick just
one remarkable discovery), the temptation to see
it as having human-like characteristics is almost
overwhelming.
To be clear, it is not the argument of this paper
that a system based on a large language model
could never, in principle, warrant description in
terms of beliefs, intentions, reason, etc. Nor does
the paper advocate any particular account of be-
lief, of intention, or of any other philosophically
contentious concept.3 Rather, the point is that
such systems are simultaneously so very diﬀer-
ent from humans in their construction, yet (often
but not always) so human-like in their behaviour,
that we need to pay careful attention to how they
work before we speak of them in language sug-
gestive of human capabilities and patterns of be-
haviour.
To sharpen the issue, let’s compare two very
2“The intentional stance is the strategy of interpreting
the behavior of an entity ... by treating it as if it were a
rational agent ” (Dennett, 2009).
3In particular, when I use the term “really”, as in the
question ‘Does X “really” have Y?’, I am not assum-
ing there is some metaphysical fact of the matter here.
Rather, the question is whether, when more is revealed
about the nature of X, we still want to use the word Y.
short conversations, one between Alice and Bob
(both human), and a second between Alice
and BOT, a ﬁctional question-answering system
based on a large language model. Suppose Al-
ice asks Bob “What country is to the south of
Rwanda?”
and Bob replies “I think it’s Bu-
rundi”. Shortly afterwards, because Bob is often
wrong in such matters, Alice presents the same
question to BOT, which (to her mild disappoint-
ment) oﬀers the same answer: “Burundi is to the
south of Rwanda”. Alice might now reasonably
remark that both Bob and BOT knew that Bu-
rundi was south of Rwanda. But what is really
going on here? Is the word “know” being used
in the same sense in the two cases?
4
Humans and LLMs Compared
What is Bob, a representative human, doing
when he correctly answers a straightforward fac-
tual question in an everyday conversation? To
begin with, Bob understands that the question
comes from another person (Alice), that his an-
swer will be heard by that person, and that it will
have an eﬀect on what she believes. In fact, af-
ter many years together, Bob knows a good deal
else about Alice that is relevant to such situa-
tions: her background knowledge, her interests,
her opinion of him, and so on. All of this frames
the communicative intent behind his reply, which
is to impart a certain fact to her, given his un-
derstanding of what she wants to know.
Moreover, when Bob announces that Burundi
is to the south of Rwanda, he is doing so against
the backdrop of various human capacities that
we all take for granted when we engage in every-
day commerce with each other. There is a whole
battery of techniques we can call upon to ascer-
tain whether a sentence expresses a true propo-
sition, depending on what sort of sentence it is.
We can investigate the world directly, with our
own eyes and ears.
We can consult Google or
Wikipedia, or even a book. We can ask some-
one who is knowledgeable on the relevant sub-
ject matter. We can try to think things through,
rationally, by ourselves, but we can also argue
things out with our peers. All of this relies on
there being agreed criteria external to ourselves
against which what we say can be assessed.
How about BOT? What is going on when a
large language model is used to answer such ques-
tions? First, it’s worth noting that a bare-bones
3

LLM is, by itself, not a conversational agent.4
For a start, the LLM will have to be embedded
in a larger system to manage the turn-taking in
the dialogue. But it will also need to be coaxed
into producing conversation-like behaviour.5 Re-
call that an LLM simply generates sequences of
words that are statistically likely follow-ons from
a given prompt. But the sequence “What coun-
try is to the south of Rwanda? Burundi is to the
south of Rwanda”, with both sentences squashed
together exactly like that, may not, in fact, be
very likely.
A more likely pattern, given that
numerous plays and ﬁlm scripts feature in the
public corpus, would be something like the fol-
lowing.
Fred:
What country is south of Rwanda?
Jane:
Burundi is south of Rwanda.
Of course, those exact words may not appear,
but their likelihood, in the statistical sense, will
be high.
In short, BOT will be much better
at generating appropriate responses if they con-
form to this pattern rather than to the pattern
of actual human conversation. Fortunately, the
user (Alice) doesn’t have to know anything about
this.
In the background, the LLM is invisibly
prompted with a preﬁx along the following lines.
This is a conversation between
User, a human, and BOT, a clever and
knowledgeable AI agent.
User:
What is 2+2?
BOT: The answer is 4.
User:
Where was Albert Einstein born?
BOT: He was born in Germany.
Alice’s query, in the following form, is ap-
pended to this preﬁx.
User:
What country is south of Rwanda?
BOT:
This yields the full prompt to be submitted
to the LLM, which will hopefully predict a con-
tinuation along the lines we are looking for, i.e.
“Burundi is south of Rwanda”.
Dialogue is just one application of LLMs that
can be facilitated by the judicious use of prompt
preﬁxes. In a similar way, LLMs can be adapted
to perform numerous tasks without further train-
ing (Brown et al., 2020). This has led to a whole
new category of AI research, namely prompt en-
gineering, which will remain relevant until we
have better models of the relationship between
4Strictly speaking, the large language model itself com-
prises just the model architecture and the trained param-
eters.
5See Thoppilan et al. (2022) for an example of such a
system, as well as a useful survey of related dialogue work.
what we say and what we want.
5
Do LLMs Really Know Anything?
Turning an LLM into a question-answering sys-
tem by a) embedding it in a larger system, and
b) using prompt engineering to elicit the required
behaviour exempliﬁes a pattern found in much
contemporary work. In a similar fashion, LLMs
can be used not only for question-answering,
but also to summarise news articles, to generate
screenplays, to solve logic puzzles, and to trans-
late between languages, among other things.
There are two important takeaways here. First,
the basic function of a large language model,
namely to generate statistically likely continua-
tions of word sequences, is extraordinarily versa-
tile. Second, notwithstanding this versatility, at
the heart of every such application is a model do-
ing just that one thing: generating statistically
likely continuations of word sequences.
With this insight to the fore, let’s revisit the
question of how LLMs compare to humans, and
reconsider the propriety of the language we use
to talk about them. In contrast to humans like
Bob and Alice, a simple LLM-based question-
answering system, such as BOT, has no commu-
nicative intent (Bender and Koller, 2020). In no
meaningful sense, even under the licence of the
intentional stance, does it know that the ques-
tions it is asked come from a person, or that a
person is on the receiving end of its answers. By
implication, it knows nothing about that person.
It has no understanding of what they want to
know nor of the eﬀect its response will have on
their beliefs.
Moreover, in contrast to its human interlocu-
tors, a simple LLM-based question-answering
system like BOT does not properly speaking have
beliefs.6 BOT does not “really” know that Bu-
rundi is south of Rwanda, although the inten-
tional stance does, in this case, license Alice’s
casual remark to the contrary. To see this, we
need to think separately about the underlying
LLM and the system in which it is embedded.
First, let’s consider the underlying LLM, that
is to say the bare-bones model, comprising the
6This paper focuses on belief, knowledge, and rea-
son.
Others have argued about meaning in LLMs
(Bender and Koller,
2020;
Piantadosi and Hill,
2022).
Here we take no particular stand on meaning, instead pre-
ferring questions about how words are used, whether they
are words generated by the LLMs themselves or words
generated by humans that are about LLMs.
4

model architecture and the trained parameters.
A bare-bones LLM doesn’t “really” know any-
thing because all it does, at a fundamental level,
is sequence prediction.
Sometimes a predicted
sequence takes the form of a proposition. But the
special relationship propositional sequences have
to truth is apparent only to the humans who are
asking questions, or to those who provided the
data the model was trained on.
Sequences of
words with a propositional form are not special
to the model itself in the way they are to us. The
model itself has no notion of truth or falsehood,
properly speaking, because it lacks the means to
exercise these concepts in anything like the way
we do.
It could perhaps be argued that an LLM
“knows” what words typically follow what other
words, in a sense that does not rely on the inten-
tional stance. But even if we allow this, knowing
that the word “Burundi” is likely to succeed the
words “The country to the south of Rwanda is”
is not the same as knowing that Burundi is to the
south of Rwanda. To confuse those two things
is to make a profound category mistake. If you
doubt this, consider whether knowing that the
word “little” is likely to follow the words “Twin-
kle, twinkle” is the same as knowing that twinkle
twinkle little. The idea doesn’t even make sense.
So much for the bare-bones language model.
What about the whole dialogue system of which
the LLM is the core component? Does that have
beliefs, properly speaking? At least the very idea
of the whole system having beliefs makes sense.
There is no category error here.
However, for
a simple dialogue agent like BOT, the answer is
surely still “no”. A simple LLM-based question-
answering system like BOT lacks the means to
use the words “true” and “false” in all the ways,
and in all the contexts, that we do. It cannot
participate fully in the human language game of
truth, because it does not inhabit the world we
human language-users share.7
6
What About Emergence?
Contemporary large language models are so pow-
erful, so versatile, and so useful that the argu-
ment above might be diﬃcult to accept.
Ex-
changes with state-of-the-art (in late 2022) LLM-
based conversational agents are so convincing, it
is hard to not to anthropomorphise them. Could
7For a discussion of the “language game of truth”, see
Shanahan (2010), pp.36–39.
it be that something more complex and subtle is
going on here?
One tempting line of argument goes like this.
Although large language models, at root, only
perform sequence prediction, it’s possible that,
in learning to do this, they have discovered
emergent mechanisms that warrant a description
in higher-level terms. These higher-level terms
might include “knowledge” and “belief”. Indeed,
we know that artiﬁcial neural networks can ap-
proximate any computable function to an arbi-
trary degree of accuracy.
So whatever mecha-
nisms are needed to enable the formation of be-
liefs, they probably reside in the parameter space
somewhere. Given a big enough model, enough
data of the right sort, and enough computing
power to train the model, perhaps stochastic gra-
dient descent can discover such mechanisms if
they are the best way to optimise the objective
of making accurate sequence predictions.
This argument has considerable appeal.
Af-
ter all, the overriding lesson of recent progress
in LLMs is that extraordinary and unexpected
capabilities emerge when big enough models are
trained on very large quantities of textual data.
However, as long as our considerations are con-
ﬁned to a simple LLM-based question-answering
system, this has little bearing on the issue of
communicative intent.
It doesn’t matter what
internal mechanisms it uses, a sequence predic-
tor is not, in itself, the kind of thing that could,
even in principle, have communicative intent,
and simply embedding it in a dialogue manage-
ment system will not help.
But what about knowledge and belief? Could
a sophisticated emergent mechanism of the right
sort license us to speak of an LLM as if it “re-
ally” knew or believed something?
Again, it’s
important to distinguish between the bare-bones
model and the whole system. Only in the context
of a capacity to distinguish truth from falsehood
can we legitimately speak of “belief” in its fullest
sense. But an LLM is not in the business of mak-
ing judgements. It just models what words are
likely to follow from what other words. The in-
ternal mechanisms it uses to do this, whatever
they are, cannot in themselves be sensitive to
the truth or otherwise of the word sequences it
predicts.
Of course, it is perfectly acceptable to say
that an LLM “encodes”, “stores”, or “contains”
knowledge, in the same sense that an encyclo-
pedia can be said to encode, store, or contain
5

knowledge. Indeed, it can reasonably be claimed
that one emergent property of an LLM is that
it encodes kinds of knowledge of the everyday
world and the way it works that no encyclope-
dia captures (Li et al., 2021). But if Alice were
to remark that “Wikipedia knew that Burundi
was south of Rwanda”, it would be a ﬁgure of
speech, not a literal statement. An encyclopedia
doesn’t literally “know” or “believe” anything,
in the way that a human does, and neither does
a bare-bones LLM.
The real issue here is that, whatever emergent
properties it has, the LLM itself has no access
to any external reality against which its words
might be measured, nor the means to apply any
other external criteria of truth, such as agree-
ment with other language-users.8 It only makes
sense to speak of such criteria in the context of
the system as a whole, and for a system as a
whole to meet them, it needs to be more than
a simple conversational agent. In the words of
B.C.Smith, it must “authentically engage with
the world’s being the way in which [its] repre-
sentations represent it as being” (Smith, 2019).
7
External Information Sources
The point here does not concern any speciﬁc be-
lief. It concerns the prerequisites for ascribing
any beliefs at all to a system. Nothing can count
as a belief about the world we share — in the
largest sense of the term — unless it is against
the backdrop of the ability to update beliefs ap-
propriately in the light of evidence from that
world, an essential aspect of the capacity to dis-
tinguish truth from falsehood.
Could Wikipedia, or some other trustworthy
factual website, provide external criteria against
which the truth or falsehood of a belief might be
measured? Suppose an LLM were embedded in
a system that regularly consulted such sources
(as LaMDA does (Thoppilan et al., 2022)), and
used a contemporary model editing technique
to maintain the factual accuracy of its predic-
tions (such as the one described by Meng et al.
(2022)9). Would this not count as exercising the
8Davidson uses a similar argument to call into question
whether belief is possible without language (Davidson,
1982).
The point here is diﬀerent.
We are concerned
with conditions that have to be met for the generation of
a natural language sentence to reﬂect the possession of a
propositional attitude.
9Commendably, Meng et al. (2022) use the term “fac-
tual associations” to denote the information that under-
required sort of capacity to update belief in the
light of evidence?
Crucially, this line of thinking depends on the
shift from the language model itself to the larger
system of which the language model is a part.
The language model itself is still just a sequence
predictor, and has no more access to the exter-
nal world than it ever did. It is only with respect
to the whole system that the intentional stance
becomes more compelling in such a case.
But
before yielding to it, we should remind ourselves
of how very diﬀerent such systems are from hu-
man beings. When Alice took to Wikipedia and
conﬁrmed that Burundi was south of Rwanda,
what took place was more than just an update
to a model in her head of the distribution of word
sequences in the English language.
The change that took place in Alice was a re-
ﬂection of her nature as a language-using ani-
mal inhabiting a shared world with a community
of other language-users. Humans are the natu-
ral home of talk of beliefs and the like, and the
behavioural expectations that go hand-in-hand
with such talk are grounded in our mutual under-
standing, which is itself the product of a common
evolutionary heritage. When we interact with an
AI system based on a large language model, these
grounds are absent, an important consideration
when deciding whether or not to speak of such a
system as if it “really” had beliefs.
8
Vision-Language Models
A sequence predictor may not by itself be the
kind of thing that could have communicative
intent or form beliefs about an external real-
ity. But, as repeatedly emphasised, LLMs in the
wild must be embedded in larger architectures
to be useful. To build a question-answering sys-
tem, the LLM simply has to be supplemented
with a dialogue management system that queries
the model as appropriate. There is nothing this
larger architecture does that might count as com-
municative intent or the capacity to form beliefs.
So the point stands.
However, LLMs can be combined with other
sorts of models and / or embedded in more
complex architectures.
Vision-language mod-
els (VLMs) such as VilBERT (Lu et al., 2019)
and Flamingo (Alayrac et al., 2022), for exam-
ple, combine a language model with an image
lies an LLM’s ability to generate word sequences with a
propositional form.
6

encoder, and are trained on a multi-modal cor-
pus of text-image pairs. This enables them to
predict how a given sequence of words will con-
tinue in the context of a given image. VLMs can
be used for visual question-answering or to en-
gage in a dialogue about a user-provided image.
Could a user-provided image stand in for an
external reality against which the truth or false-
hood of a proposition can be assessed? Could it
be legitimate to speak of a VLM’s beliefs, in the
full sense of the term? We can indeed imagine a
VLM that uses an LLM to generate hypotheses
about an image, then veriﬁes their truth with re-
spect to that image (perhaps by consulting a hu-
man), and then ﬁne-tunes the LLM not to make
statements that turn out to be false.
Talk of
belief here would perhaps be less problematic.
However, most contemporary VLM-based sys-
tems don’t work this way. Rather, they depend
on frozen models of the joint distribution of text
and images. In this respect, the relationship be-
tween a user-provided image and the words gen-
erated by the VLM is fundamentally diﬀerent
from the relationship between the world shared
by humans and the words we use when we talk
about that world. Importantly, the former rela-
tionship is mere correlation, while the latter is
causal.10
The consequences of the lack of causality are
troubling. If the user presents the VLM with a
picture of a dog, and the VLM says “This is a
picture of a dog”, there is no guarantee that its
words are connected with the dog in particular,
rather than some other feature of the image that
is spuriously correlated with dogs (such as the
presence of a kennel). Conversely, if the VLM
says there is a dog in an image, there is no guar-
antee that there actually is a dog, rather than
just a kennel.
Whether or not these concerns apply to any
speciﬁc VLM-based system depends on exactly
how that system works; what sort of model it
uses, and how that model is embedded in the
system’s overall architecture. But to the extent
that the relationship between words and things
for a given VLM-based system is diﬀerent than
it is for human language-users, it might be pru-
dent not to take literally talk of what that system
“knows” or “believes”.
10Of course, there is causal structure to the computa-
tions carried out by the model during inference. But this
is not the same as there being causal relations between
words and the things those words are taken to be about.
9
What About Embodiment?
Humans
are
members
of
a
community
of
language-users inhabiting a shared world, and
this primal fact makes them essentially diﬀerent
to large language models. Human language users
can consult the world to settle their disagree-
ments and update their beliefs. They can, so to
speak, “triangulate” on objective reality. In iso-
lation, an LLM is not the sort of thing that can
do this, but in application, LLMs are embedded
in larger systems. What if an LLM is embedded
in a system capable of interacting with a world
external to itself? What if the system in ques-
tion is embodied, either physically in a robot or
virtually in an avatar?
When such a system inhabits a world like our
own — a world populated with 3D objects, some
of which are other agents, some of whom are
language-users — it is, in this important respect,
a lot more human-like than a disembodied lan-
guage model.
But whether or not it is appro-
priate to speak of communicative intent in the
context of such a system, or of knowledge and
belief, in their fullest sense, depends on exactly
how the LLM is embodied.
As an example, let’s consider the SayCan sys-
tem of Ahn et al. (2022). In this work, an LLM
is embedded in a system that controls a physi-
cal robot. The robot carries out everyday tasks
(such as clearing a spillage) in accordance with
a user’s high-level natural language instruction.
The job of LLM is to map the user’s instruction
to low-level actions (such as ﬁnding a sponge)
that will help the robot to achieve the required
goal.
This is done via an engineered prompt
preﬁx that makes the model output natural lan-
guage descriptions of suitable low-level actions,
scoring them for usefulness.
The language model component of the SayCan
system suggests actions without taking into ac-
count what the environment actually aﬀords the
robot at the time.
Perhaps there is a sponge
to hand. Perhaps not. Accordingly, a separate
perceptual module assesses the scene using the
robot’s sensors and determines the current feasi-
bility of performing each low-level action. Com-
bining the LLM’s estimate of each action’s use-
fulness with the perceptual module’s estimate of
each action’s feasibility yields the best action to
attempt next.
SayCan exempliﬁes the many innovative ways
that a large language model can be put to use.
Moreover, it could be argued that the natu-
7

ral language descriptions of recommended low-
level actions generated by the LLM are grounded
thanks to their role as intermediaries between
perception and action.11
Nevertheless, despite
being physically embodied and interacting with
the real world, the way language is learned and
used in a system like SayCan is very diﬀerent
from the way it is learned and used by a human.
The language models incorporated in systems
like SayCan are pre-trained to perform sequence
prediction in a disembodied setting from a text-
only dataset. They have not learned language by
talking to other language-users while immersed
in a shared world and engaged in joint activity.
SayCan is suggestive of the kind of embodied
language-using system we might see in the fu-
ture. But in such systems today, the role of lan-
guage is very limited. The user issues instruc-
tions to the system in natural language, and the
system generates interpretable natural language
descriptions of its actions. But this tiny reper-
toire of language use hardly bears comparison
to the cornucopia of collective activity that lan-
guage supports in humans.
The upshot of this is that we should be just
as cautious in our choice of words when talking
about embodied systems incorporating LLMs as
we are when talking about disembodied systems
that incorporate LLMs. Under the licence of the
intentional stance, a user might say that a robot
knew there was a cup to hand if it stated “I can
get you a cup” and proceeded to do so.
But
if pressed, the wise engineer might demur when
asked whether the robot really understood the
situation, especially if its repertoire is conﬁned
to a handful of simple actions in a carefully con-
trolled environment.
10
Can Language Models Reason?
While the answer to the question “Do LLM-
based systems really have beliefs?”
is usually
“no”, the question “Can LLM-based systems re-
ally reason?”
is harder to settle.
This is be-
cause reasoning, insofar as it is founded in formal
logic, is content neutral. The modus ponens rule
of inference, for example, is valid whatever the
premises are about. If all squirgles are splonky
and Gilfred is a squirgle then it follows that Gil-
11None of the symbols manipulated by an LLM are
grounded in the sense of Harnad (1990), that is to say
through perception, except indirectly and parasitically
through the humans who generated the original training
data.
fred is splonky. The conclusion follows from the
premises here irrespective of the meaning (if any)
of “squirgle” and “splonky”, and whoever the un-
fortunate Gilfred might be.
The content neutrality of logic means that we
cannot criticise talk of reasoning in LLMs on the
grounds that they have no access to an exter-
nal reality against which truth or falsehood can
be measured.
However, as always, it’s crucial
to keep in mind what LLMs really do.
If we
prompt an LLM with “All humans are mortal
and Socrates is human therefore”, we are not
instructing it to carry out deductive inference.
Rather, we are asking it the following question.
Given the statistical distribution of words in the
public corpus, what words are likely to follow the
sequence ‘All humans are mortal and Socrates is
human therefore”.
If reasoning problems could be solved with
nothing more than a single step of deductive in-
ference, then an LLM’s ability to answer ques-
tions such as this might be suﬃcient. But non-
trivial reasoning problems require multiple infer-
ence steps. LLMs can be eﬀectively applied to
multi-step reasoning, without further training,
thanks to clever prompt engineering. In chain-of-
thought prompting, for example, a prompt pre-
ﬁx is submitted to the model, before the user’s
query, containing a few examples of multi-step
reasoning, with all the intermediate steps ex-
plicitly spelled out (Nye et al., 2021; Wei et al.,
2022).
Including a prompt preﬁx in the chain-of-
thought style encourages the model to generate
follow-on sequences in the same style, which is
to say comprising a series of explicit reasoning
steps that lead to the ﬁnal answer.
As usual,
the question really being posed to the model is
of the form “Given the statistical distribution of
words in the public corpus, what words are likely
to follow the sequence S”, where in this case the
sequence S is the chain-of-thought prompt preﬁx
plus the user’s query. The sequences of tokens
that are most likely to follow S will have a simi-
lar form to sequences found in the prompt preﬁx,
which is to say they will include multiple steps of
reasoning, so these are what the model generates.
Remarkably, not only do the model’s responses
take the form of an argument with multiple steps,
the argument in question is often (but not al-
ways) valid, and the ﬁnal answer is often (but
not always) correct. To the extent that a suit-
ably prompted LLM appears to reason correctly,
8

it does so by mimicking well-formed arguments
in its training set and / or in the prompt. But
could this mimicry ever amount to genuine rea-
soning? Even if today’s models make occasional
mistakes, could further scaling iron these out to
the point that a model’s performance was indis-
tinguishable from that of a hard-coded reasoning
algorithm, such as we ﬁnd in a theorem prover,
for example? Maybe. But how would we know?
How could we come to trust such a model?
Well, the sequences of sentences generated by
a theorem prover are faithful to logic, in the
sense that they are the result of an underly-
ing computational process whose causal struc-
ture mirrors the inferential structure of the
problem (Creswell and Shanahan, 2022).
One
way to build a trustworthy reasoning system
using an LLM is to embed it in an algo-
rithm that enforces the same causal structure
(Creswell and Shanahan, 2022; Creswell et al.,
2022). But if we stuck with a pure LLM, the only
way to fully trust the arguments it generated
would be by reverse engineering it and discov-
ering emergent mechanisms that conformed to
the faithful reasoning prescription. In the mean
time, we should proceed with caution, and use
discretion when characterising what these mod-
els do.
11
Conclusion: Why This Matters
Does the foregoing discussion amount to any-
thing more than philosophical nitpicking? Surely
when researchers talk of “beliefs”, “knowledge”,
“reason”, and the like, the meaning of those
terms is perfectly clear. In papers, researchers
use such terms as a convenient shorthand for pre-
cisely deﬁned computational mechanisms, as al-
lowed by the intentional stance. Well, this is ﬁne
as long as there is no possibility of anyone as-
signing more weight to such terms than they can
legitimately bear, if there is no danger of their
use misleading anyone about the character and
capabilities of the systems being described.
However, today’s large language models, and
the applications that use them, are so powerful,
so convincingly intelligent, that such licence can
no longer safely be applied (Ruane et al., 2019;
Weidinger et al., 2021). As AI practitioners, the
way we talk about LLMs matters. It matters not
only when we write scientiﬁc papers, but also
when we interact with policy makers or speak
to the media. The careless use of philosophically
loaded words like “believes” and “thinks” is espe-
cially problematic, because such terms obfuscate
mechanism and actively encourage anthropomor-
phism.
Interacting with a contemporary LLM-based
conversational agent can create a compelling il-
lusion of being in the presence of a thinking
creature like ourselves.
Yet in their very na-
ture, such systems are fundamentally not like
ourselves.
The shared “form of life” that un-
derlies mutual understanding and trust among
humans is absent, and these systems can be in-
scrutable as a result, presenting a patchwork of
less-than-human with superhuman capacities, of
uncannily human-like with peculiarly inhuman
behaviours.
The sudden presence among us of exotic,
mind-like entities might precipitate a shift in the
way we use familiar psychological terms like “be-
lieves” and “thinks”, or perhaps the introduction
of new words and turns of phrase. But it takes
time for new language to settle, and for new ways
of talking to ﬁnd their place in human aﬀairs. It
may require an extensive period of interacting
with, of living with, these new kinds of artefact
before we learn how best to talk about them.12
Meanwhile, we should try to resist the siren call
of anthropomorphism.
Acknowledgments
Thanks to Toni Creswell, Richard Evans, Chris-
tos Kaplanis, Andrew Lampinen, and Kyriacos
Nikiforou for invaluable (and robust) discussions
on the topic of this paper.
References
M. Ahn, A. Brohan, N. Brown, Y. Chebotar,
O. Cortes, et al. Do as I can, not as I say:
Grounding language in robotic aﬀordances.
arXiv preprint arXiv:2204.01691, 2022.
J.-B. Alayrac, J. Donahue, P. Luc, A. Miech,
I. Barr, et al.
Flamingo: a visual language
model for few-shot learning.
arXiv preprint
arXiv:2204.14198, 2022.
E. Bender and A. Koller. Climbing towards NLU:
On meaning, form, and understanding in the
12Ideally, we would also like a theoretical understand-
ing of their inner workings. But at present, despite some
commendable work in the right direction (Elhage et al.,
2021; Li et al., 2021; Olsson et al., 2022), we still await
such an understanding.
9

age of data. In Proceedings of the 58th Annual
Meeting of the Association for Computational
Linguistics, pages 5185–5198, 2020.
E. Bender, T. Gebru, A. McMillan-Major, and
S. Shmitchell.
On the dangers of stochastic
parrots: Can language models be too big? In
Proceedings of the 2021 ACM Conference on
Fairness, Accountability, and Transparency,
pages 610–623, 2021.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D.
Kaplan, et al. Language models are few-shot
learners. In Advances in Neural Information
Processing Systems, volume 33, pages 1877–
1901, 2020.
A. Chowdhery, S. Narang, J. Devlin, M. Bosma,
G. Mishra, et al.
PaLM: Scaling language
modeling with pathways.
arXiv preprint
arxiv:2204.02311, 2022.
A. Creswell and M. Shanahan. Faithful reasoning
using large language models. arXiv preprint
arXiv:2208.14271, 2022.
A. Creswell,
M. Shanahan,
and I. Higgins.
Selection-inference: Exploiting large language
models for interpretable logical reasoning.
arXiv preprint arXiv:2205.09712, 2022.
D. Davidson. Rational animals. Dialectica, 36:
317–327, 1982.
D. Dennett. Intentional systems theory. In The
Oxford Handbook of Philosophy of Mind, pages
339–350. Oxford University Press, 2009.
J.
Devlin,
M.-W.
Chang,
K.
Lee,
and
K. Toutanova.
BERT: Pre-training of deep
bidirectional transformers for language under-
standing.
arXiv preprint arXiv:1810.04805,
2018.
N. Elhage, N. Nanda, C. Olsson, T. Henighan,
N. Joseph, et al.
A mathematical frame-
work for transformer circuits.
Transformer
Circuits Thread, 2021.
https://transformer-
circuits.pub/2021/framework/index.html.
A.
Glaese,
N.
McAleese,
M.
Tr¸ebacz,
J. Aslanides, V. Firoiu, et al. Improving align-
ment of dialogue agents via targeted human
judgements. arXiv preprint arXiv:2209.14375,
2022.
A. Y. Halevy, P. Norvig, and F. Pereira.
The
unreasonable eﬀectiveness of data. IEEE In-
telligent Systems, 24(2):8–12, 2009.
S. Harnad.
The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1-3):
335–346, 1990.
T. Kojima,
S. S. Gu,
M. Reid,
Y. Mat-
suo, and Y. Iwasawa.
Large language mod-
els are zero-shot reasoners.
arXiv preprint
arXiv:2205.11916, 2022.
B. Z. Li, M. Nye, and J. Andreas. Implicit repre-
sentations of meaning in neural language mod-
els. In Proceedings of the 59th Annual Meeting
of the Association for Computational Linguis-
tics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume
1: Long Papers), 2021.
J. Lu,
D. Batra,
D. Parikh,
and S. Lee.
ViLBERT: Pretraining task-agnostic visiolin-
guistic representations for vision-and-language
tasks. arXiv preprint arXiv:1908.02265, 2019.
G. Marcus and E. Davis. GPT-3, bloviator: Ope-
nAI’s language generator has no idea what it’s
talking about. MIT Technology Review, Au-
gust 2020, 2020.
K. Meng, D. Bau, A. J. Andonian, and Y. Be-
linkov. Locating and editing factual associa-
tions in GPT. In Advances in Neural Infor-
mation Processing Systems, 2022.
M.
Nye,
A.
J.
Andreassen,
G.
Gur-Ari,
H. Michalewski, et al.
Show your work:
Scratchpads
for
intermediate
computation
with
language
models.
arXiv
preprint
arXiv:2112.00114, 2021.
C. Olsson, N. Elhage, N. Nanda, N. Joseph,
N.
DasSarma,
et
al.
In-context
learn-
ing
and
induction
heads.
Transformer
Circuits Thread, 2022.
https://transformer-
circuits.pub/2022/in-context-learning-and-
induction-heads/index.html.
S. T. Piantadosi and F. Hill.
Meaning with-
out reference in large language models. arXiv
preprint arXiv:2208.02957, 2022.
A.
Radford,
J.
Wu,
R.
Child,
D.
Luan,
D. Amodei, and I. Sutskever. Language mod-
els are unsupervised multitask learners. 2019.
10

J. W. Rae, S. Borgeaud, T. Cai, K. Millican,
J. Hoﬀmann, et al. Scaling language models:
Methods, analysis & insights from training Go-
pher. arXiv preprint arXiv:2112.11446, 2021.
E. Ruane, A. Birhane, and A. Ventresque. Con-
versational AI: Social and ethical considera-
tions.
In Proceedings 27th AIAI Irish Con-
ference on Artiﬁcial Intelligence and Cognitive
Science, pages 104–115, 2019.
M. Shanahan. Embodiment and the Inner Life:
Cognition and Consciousness in the Space of
Possible Minds.
Oxford University Press,
2010.
B. C. Smith. The Promise of Artiﬁcial Intelli-
gence: Reckoning and Judgment. MIT Press,
2019.
R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer,
A. Kulshreshtha, et al.
LaMDA: Language
models for dialog applications. arXiv preprint
arXiv:2201.08239, 2022.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszko-
reit, L. Jones, A. N. Gomez,  L. Kaiser, and
I. Polosukhin. Attention is all you need. In Ad-
vances in Neural Information Processing Sys-
tems, pages 5998–6008, 2017.
J. Wei, X. Wang, D. Schuurmans, M. Bosma,
B. Ichter, et al.
Chain-of-thought prompt-
ing elicits reasoning in large language models.
In Advances in Neural Information Processing
Systems, 2022.
L. Weidinger, J. Mellor, M. Rauh, C. Griﬃn,
J. Uesato, et al.
Ethical and social risks of
harm from language models.
arXiv preprint
arXiv:2112.04359, 2021.
L. Wittgenstein.
Philosophical Investigations.
(Translated by Anscombe, G.E.M.).
Basil
Blackwell, 1953.
11

