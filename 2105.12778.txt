arXiv:2105.12778v1  [math.ST]  26 May 2021
Statistical Depth Meets Machine Learning: Kernel Mean
Embeddings and Depth in Functional Data Analysis
George Wynne† and Stanislav Nagy‡
† Imperial College London, Department of Mathematics, London, UK
E-mail: g.wynne18@imperial.ac.uk
‡ Charles University, Faculty of Mathematics and Physics, Prague, Czech Rep.
E-mail: nagy@karlin.mff.cuni.cz
May 28, 2021
Abstract
Statistical depth is the act of gauging how representative a point is compared to a reference
probability measure. The depth allows introducing rankings and orderings to data living in
multivariate, or function spaces. Though widely applied and with much experimental success,
little theoretical progress has been made in analysing functional depths. This article highlights
how the common h-depth and related statistical depths for functional data can be viewed as a
kernel mean embedding, a technique used widely in statistical machine learning. This connec-
tion facilitates answers to open questions regarding statistical properties of functional depths,
as well as it provides a link between the depth and empirical characteristic function based
procedures for functional data.
1
Introduction: Statistical Depth for Functional Data
Functional data analysis (FDA) concerns the study of observations that can be represented as func-
tions, often residing in an inﬁnite-dimensional space. In the recent decades, FDA saw remarkable
progress, with many theoretical and practical problems successfully resolved. Often, however, sta-
tistical concepts used in ﬁnite-dimensional spaces do not readily generalise to random functions.
As a result, alternative deﬁnitions and desired properties have to be used [80, 27, 46, 47].
An example of a prominent tool of multivariate analysis that is difﬁcult to generalise to func-
tional data is statistical depth. Developed in the 1980s, statistical depth is an umbrella term for
methods introducing orderings, ranks, and by extension, nonparametric statistical inference, to
multivariate and more complex datasets. Let X be a general topological sample space, and write
P(X ) for the set of Borel probability measures on X ; typical examples of X are the Euclidean
space Rd with d ∈N, or the space of square-integrable functions L2([0, 1]). Given P ∈P(X ) a
depth function x 7→D(x; P), mapping X to [0, ∞), is meant to tell a user how representative of
1

P the point x is. High values of the depth correspond to points x located “centrally” with respect
to the given measure P; low depths ﬂag atypical sample points, or outliers. An example of appli-
cation of depth to two datasets is given in Figure 1. Many depths have been proposed in X = Rd;
their comprehensive theory can be found in [100, 84] and an array of applications to multivariate
analysis in [58].
Figure 1: Left panel: Contours of halfspace depth [96] applied to a random sample of points in
R2. The depth orders the sample points in terms of their centrality with respect to the geometry of
the whole random sample. Right panel: A random sample of functional data [0, 1] →R with the
observations attaining higher values of h-depth [16] plotted in dark thick lines, and the data with
low values of h-depth in light thick lines.
Most of the standardly used ﬁnite-dimensional depths, such as the halfspace [96, 20], simpli-
cial [57], or zonoid [53, 63] depth are not possible to extend to function spaces directly. Instead,
alternative approaches have been proposed in the literature [30, 60, 15, 75, 35]. These different no-
tions of functional depths have found many applications in data visualisation [93], nonparametric
estimation [30], outlier detection [25], classiﬁcation [59], or testing [60], to name a few.
Among functional depths, the h-depth [16] (also called the h-modal depth, or the h-mode
depth) has emerged as a convenient choice which satisﬁes many of the desired criteria of a depth
for functional inputs [75, 35]. It is deﬁned as follows.
Deﬁnition 1. Let X be a vector space equipped with a norm ∥·∥X, P ∈P(X ) and κ: [0, ∞) →
[0, ∞) be a continuous, non-increasing function with κ(0) > 0 and limt→∞κ(t) = 0. The h-depth
of x ∈X with respect to P is deﬁned as
Dκ(x; P) = EX∼P [κ(∥x −X∥X)] .
(1)
A common choice of function κ in (1) is κ(t) = e−t2/2, which gives rise to an h-depth that is
sometimes referred to as the Gauss h-depth.
2

Remark 1. The h in the name h-depth is from the rescaling κh(·) = h−1κ(·/h) used in practice in
(1) instead of κ. The parameter h > 0 plays the role of a bandwidth, but in this context is typically
taken to be ﬁxed to a constant rather than altering with new data [17]. For ease of presentation
we assume without loss of generality that h = 1, for other values our exposition is identical with
κh replacing κ.
The sample h-depth takes a form similar to a kernel density estimator applied in a general
normed vector space X . More precisely, let X1, . . . , Xn be a random sample of independent iden-
tically distributed (i.i.d.) variables generated from P ∈P(X ), and let Pn = n−1 Pn
i=1 δXi ∈P(X )
be its random empirical measure. The sample h-depth (1) with respect to Pn takes the form
Dκ(x; Pn) = n−1
n
X
i=1
κ(∥x −Xi∥X).
(2)
From this expression we observe that the h-depth extends the notions of likelihood-based depths
[29, 31] from Rd to functional data, and draws connections of the statistical depth with density-
like concepts explored in function spaces [32, 27]. The sample h-depth (2) is fast to compute and
easily interpretable. Even though in X = Rd the concept does not ﬁt directly into the framework
of (global) statistical depths [100, 84] but rather to their localised counterparts [76], in function
spaces it proved to be highly competitive [16, 17, 24, 25, 8, 75, 78]. The h-depth is frequently used
as a well performing benchmark method in nonparametric FDA [33, 54, 14], and is available in
standard FDA software packages [26, 79].
Despite its appeal in the practice of FDA, the theoretical properties of the h-depth in function
spaces X are largely unexplored [67, 71, 35] and many open questions remain [68, Chapter 8].
These include consistency and asymptotics of h-depth estimators and the theory for testing based
on this depth. A major open problem of functional depths is the characterisation of probability
measures via the depth: Is the measure P on X uniquely characterised by its depth mapping
x 7→D(x; P)? So far, no depth applicable to function spaces was proved to satisfy this highly
desirable property [64].
This paper aims to summarise and leverage a surprising relationship between h-depth and ker-
nel mean embeddings (KME) to address these open questions, and shall use a mixture of existing
and new results to do so. KMEs are well-studied in statistical machine learning, but they have not
been utilised yet in the FDA literature. In the framework of KME, a kernel on X is a symmetric
positive deﬁnite function k: X × X →R. Given a kernel k a kernel mean embedding maps P
into the reproducing kernel Hilbert space (RKHS) [1] corresponding to k. That results in easy to
manipulate expressions used to perform inference and tests for X -valued data [3, 40, 88, 65]. Al-
though not under the same name, KMEs have been studied for decades [41] and in the last twenty
years have become a successful statistical machine learning method [38, 39] with applications to
a wide range of data sources [40, 6]. Little attention has, however, been applied to functional data
in the statistical machine learning literature. This paper aims to bridge the gap between FDA and
statistical machine learning by applying KME, a statistical machine learning concept so far not
used in FDA, to functional data.
The rest of this paper is structured as follows: In Section 2 we introduce the kernel mean
embeddings, RKHS and maximum mean discrepancy, and in Section 3 we discuss the equivalence
of those concepts from machine learning and the h-depth. A special class of integrated depths for
3

functional data [15, 72], including another popular functional depth called the (random) functional
projection depth [17], is also shown to be equivalent with KMEs in Section 3.1. Section 4 leverages
the link between KME and h-depth to prove an array of new results regarding asymptotics and
consistency of estimators of h-depth. Under minimal conditions we verify the uniform consistency
and asymptotic normality of the sample version of h-depth in function spaces X , for both perfectly
observed functional data, and discretised functional observations. Rates of convergence of the
corresponding estimators are provided. Our results are remarkable because of their universality
— in contrast with both common functional depths [35] and functional pseudo-density estimators
[27], the asymptotics of h-depth is shown to be valid without any restrictions on the probability
measure P, and holds true on the whole inﬁnite-dimensional sample space X . Results of this
type are quite rare in nonparametric FDA, but as we show they follow directly from the related
theory developed for KMEs. In Section 5 we demonstrate that under a mild condition, the h-depth
characterises all probability measures in Hilbert spaces X . That result should be compared with
the related advances in the theory of the depth in X = Rd [92, 63, 69, 64]. We show that the
h-depth is the ﬁrst statistical depth completely characterising not only all probability distributions
in Rd, but even in inﬁnite-dimensional Hilbert spaces X . As such, the h-depth turns out to be
of great interest in nonparametric FDA, where no natural and simply interpretable density-like
concept characterising measures exists. In Section 6 we focus on statistical testing, and outline
the equivalence between empirical characteristic function based tests, quite popular in FDA, and
h-depth. Finally, Section 7 provides concluding remarks.
2
Kernel Mean Embeddings
Kernel mean embeddings (KMEs) are an easily computable nonparametric method of reasoning
with probability distributions. Over the past 15 years, KMEs have become a commonplace tool in
statistical machine learning. The earliest developments in the 1970s were done from a theoretical
perspective, focusing on their topological properties [41]; for a summary see [3, Chapter 4]. The
ﬁrst uses of KMEs within statistical machine learning were described in [38, 39], and they have
found numerous applications since [94, 9, 65]. Kernel-based methods in general have been recently
ﬁnding wider use also within FDA [49, 55, 4].
2.1
Kernels and Reproducing Kernel Hilbert Spaces
First of all, it is important to note that the deﬁnition of a kernel in machine learning is different
from the deﬁnition of a kernel used in other areas of statistics, such as kernel smoothing or kernel
density estimation. For example, in the FDA literature the function κ in Deﬁnition 1 is usually
referred to as a kernel. Therefore, it is necessary to establish that the deﬁnition of a kernel we are
using can be different from the deﬁnition that the reader may be familiar with before proceeding
further.
Deﬁnition 2. Let X be a set. We call a function k: X × X →R a kernel if it is (i) symmetric,
meaning k(x, y) = k(y, x) ∀x, y ∈X , and (ii) positive deﬁnite, that is Pn
i,j=1 aiajk(xi, xj) ≥0
for all n ∈N, {ai}n
i=1 ⊂R and {xi}n
i=1 ⊂X .
4

An example of a kernel over X = Rd is the squared exponential (also known as Gaussian) ker-
nel k(x, y) = e−1
2∥x−y∥2
Rd. An analogous kernel can be deﬁned on any normed space by replacing
the Euclidean norm ∥·∥Rd with the norm on X . Before providing an array of other examples of
kernels in Section 2.2, we will now describe the motivation behind the deﬁnition of a kernel. The
central idea is that a kernel is a measure of “similarity” between its two inputs via an implicit inner
product. We begin by describing how this occurs mathematically through the use of feature maps,
and then comment on why kernels are useful for statistical machine learning tasks.
Kernels as implicit inner products. Inner products are a natural way to measure similarity be-
tween two inputs. For example, the standard inner product ⟨f, g⟩L2([0,1]) in X = L2([0, 1]) may be
perceived as a measure of similarity — for two functions f, g ∈L2([0, 1]) of the same norm, the
inner product ⟨f, g⟩L2([0,1]) is maximized if and only if f = g, thanks to the usual Cauchy-Schwarz
inequality [21, Corollary 5.1.4]. To obtain more complex measures of similarity one can transform
an input before applying an inner product. Such a transform is known as using a feature map, or
feature expansion, in machine learning [90, 85]. Formally, if for a Hilbert space H equipped with
an inner product ⟨·, ·⟩H and a function ϕ: X →H we have k(x, y) = ⟨ϕ(x), ϕ(y)⟩H, then we say
that ϕ is a feature map for k.
Example 1. Let X = R and k(x, y) = (xy + 1)2. Setting H = R3 and ϕ(x) = (x2,
√
2x, 1) gives
k(x, y) = ⟨ϕ(x), ϕ(y)⟩R3. This example can straightforwardly be generalised to X = Rd.
For any function ϕ: X →H with H a Hilbert space, k(x, y) = ⟨ϕ(x), ϕ(y)⟩H is a kernel. The
symmetry of k follows from the symmetry of the inner product in H, and Pn
i,j=1 aiajk(xi, xj) =
⟨Pn
i=1 aiϕ(xi), Pn
j=1 ajϕ(xj)⟩H ≥0 for any n ∈N, {ai}n
i=1 ⊂R and {xi}n
i=1 ⊂X , so k is
positive deﬁnite. This begs the opposite question: Does every kernel have a feature map? The
answer is afﬁrmative [90, Theorem 4.16], which justiﬁes the statement that a kernel is a measure
of similarity using an implicit inner product.
Kernels in machine learning. The reason this implicit representation is helpful is twofold. First,
it is easier to check that a function is a kernel using Deﬁnition 2 than to manually derive its feature
map. Second, many numerical algorithms in machine learning, such as the least-squares estimator
in linear regression, end up only depending on the inner products of the data. This provides the
opportunity to “kernelise” such algorithms by replacing the standard inner product with a kernel.
Examples are the kernel ridge regression [90], support vector machines [86] and kernel ICA [2].
The use of kernelised algorithms is equivalent to performing the algorithm on the H-valued data
after the original X -valued data have been passed through the feature map ϕ. This explains the
use of the terminology feature map, as it extracts features of the data which help to enhance the
performance of the algorithm. Again, we do not need the feature map explicitly to do this, only
the kernel.
Reproducing kernel Hilbert spaces. For a given kernel k the choice of H and ϕ is not unique.
However, there is a canonical choice, called the reproducing kernel Hilbert space (RKHS) and a
canonical feature map corresponding to the kernel.
Deﬁnition 3. A Hilbert space H of functions from a set X to R is called a reproducing kernel
Hilbert space if there exists a kernel k on X such that k(·, x) ∈H for all x ∈X and
⟨f, k(·, x)⟩H = f(x)
for all f ∈H, x ∈X.
(3)
The identity (3) is called the reproducing property.
5

The Moore-Aronszajn theorem [1] guarantees there is a one-to-one relationship between ker-
nels and RKHSs. This justiﬁes why the RKHS of a kernel k is considered a canonical choice of
a feature space. Due to that relationship, it is common to use Hk to denote the RKHS of k and
⟨·, ·⟩k, ∥·∥k to denote the inner-product and the induced norm on Hk, respectively.
Setting ϕ: X →Hk, x 7→ϕ(x) = k(x, ·), meaning that ϕ is a function-valued map, shows that
k(x, y) = ⟨ϕ(x), ϕ(y)⟩k by the reproducing property (3). Thus, ϕ is the canonical feature map.
Using the Cauchy-Schwarz inequality [21, Corollary 5.1.4] along with the reproducing property
(3) is a common trick for obtaining bounds for functions in an RKHS. Namely, for any x ∈X and
f ∈Hk we have
|f(x)| = |⟨f, k(x, ·)⟩k⟩≤∥f∥k∥k(x, ·)∥k = ∥f∥k
p
k(x, x).
(4)
This formula shows that the RKHS norm controls the supremum norm of f if k(x, x) is bounded
for x ∈X , which is true for nearly all kernels commonly used in practice.
Explicit forms for RKHS when X = Rd are known for a wide range of kernels. Often, spectral
properties of the kernels are used to obtain the characterisations [3, 50]. When X is non-Euclidean,
it is harder to derive interpretable representations of RKHSs, but results in this direction for X a
separable Hilbert space and a particular family of kernels are provided in [98, Theorem 2, Propo-
sition 2] and [74].
The reproducing property (3) allows one to easily manipulate quantities such as integrals and
expectations of functions from Hk using the kernel, which would be difﬁcult in other function
spaces. As a result, RKHSs are widely studied in statistical machine learning since they facilitate
the analysis of many machine learning algorithms involving kernels [44, 85, 90]. For a summary
of the basic properties of RKHSs with some applications to statistics see [90, 3, 77].
2.2
Examples of Kernels
For X = Rd the most basic kernel is the standard Euclidean inner product kernel k(x, y) =
⟨x, y⟩Rd. The corresponding RKHS is the set of functions of the form fy(x) = ⟨x, y⟩Rd for some
y ∈Rd and the inner product of the RKHS is ⟨fx, fy⟩k = ⟨x, y⟩Rd.
A kernel on Rd commonly used for spatial modelling [89] is the Mat´ern-3/2 kernel k(x, y) =
(1 +
√
3∥x −y∥Rd)e−
√
3∥x−y∥Rd which is a particular instance of the wider Mat´ern class [3]. The
RKHS of such kernels are Sobolev spaces [50] of functions that map from subsets of Rd to R.
Remark 2. Note that given any kernel k one can generate classes of kernels by introducing hyper-
parameters. The most typical is a bandwidth obtained by replacing k(x, y) with k(x/γ, y/γ) for
some γ > 0. As we will see in Section 3, this corresponds to taking different values of the tuning
parameter h > 0 in h-depth ass discussed in Remark 1.
Two kernels which are popular for use in kernel mean embeddings, the main topic of this paper
to be introduced in Section 2.3, are the squared exponential (SE) and inverse multi-quadric (IMQ)
kernels. We will present them now in the generality required to deal with inputs from function
spaces [98]. Let X , Y be Hilbert spaces and T : X →Y. The SE-T kernel is
kSE-T(x, y) = e−1
2∥T(x)−T(y)∥2
Y ,
(5)
6

and the IMQ-T kernel is
kIMQ-T(x, y) = (∥T(x) −T(y)∥2
Y + 1)−1/2.
(6)
In the special case X = Y and T = γ−1I with I the identity map and γ > 0 we recover the
bandwidth hyper-parameter from Remark 2. For X = Rd the SE-I kernel is also referred to as the
Gauss kernel. Its RKHS has been investigated in [62, 91]. For different choices of T the RKHS of
the SE-T kernel in more general spaces X was studied in [98, Theorem 2].
Translation invariant kernels. The important collection of translation invariant kernels in a
Hilbert space X , meaning k(x, y) = φ(x−y) for some φ: X →R, has an intimate connection with
the Fourier transform of probability measures. Recall that for µ ∈P(X ), the Fourier transform of
µ is deﬁned as bµ(x) =
R
X ei⟨x,v⟩X dµ(v). In probability theory, the Fourier transform is also known
as the characteristic function of measure µ. When X is ﬁnite-dimensional, Bochner’s theorem [5]
states that a kernel is translation invariant with φ(0) = 1 and continuous if and only if k(x, y) =
bµ(x −y) for some µ ∈P(X ). For example, in X = Rd we have kSE-I(x, y) = b
NId(x −y) for NId
the standard d-variate normal distribution.
When X is an inﬁnite-dimensional Hilbert space, Bochner’s theorem cannot be applied in the
description of translation invariant kernels. Instead, the more involved Minlos-Sazonov theorem
must be used, see e.g. [97, Theorem VI.1.1] or [61, Theorem 1.1.5]. To give a simple inﬁnite-
dimensional example deﬁne L+
1 (X ) to be the space of symmetric, positive, trace class operators
from X to X [18]. Then for C ∈L+
1 (X ) we have
kSE-C1/2(x −y) = bNC(x −y),
(7)
where NC is the Gaussian measure on X with mean zero and covariance operator C, and C1/2 is
square root operator of C.
2.3
Deﬁnition of Kernel Mean Embedding
We are now ready to deﬁne the kernel mean embeddings, the main object of study of this paper.
It will allow us to draw connections to h-depth and empirical characteristic function based testing
procedures.
Deﬁnition 4. Suppose X is a measurable space, k a measurable kernel, and deﬁne Pk ⊆P(X )
as {P ∈P(X ):
R
X
p
k(x, x)dP(x) < ∞}. For P ∈Pk deﬁne the kernel mean embedding ΦkP
of P as
ΦkP :=
Z
X
k(·, x)dP(x).
(8)
A few comments are in order. The integral in (8) is to be understood as a Bochner integral
[47, Chapter 2]. Therefore, ΦkP is an element of Hk, meaning it is a function from X to R. It
represents the linear operator from Hk to R given by f 7→
R
X f(x)dP(x). This linear operator is
bounded since

Z
X
f(x)dP(x)
 ≤
Z
X
|f(x)|dP(x) =
Z
X
|⟨f, k(·, x)⟩k|dP(x)
≤∥f∥k
Z
X
p
k(x, x)dP(x) ≤ck∥f∥k,
(9)
7

for some ck < ∞by the assumption on P, where (9) is by (4). The interpretation of ΦkP as a
Bochner integral allows us to write for f ∈Hk
EX∼P[f(X)] = EX∼P[⟨f, k(·, X)⟩k] = ⟨f, EX∼P[k(·, X)]⟩k = ⟨f, ΦkP⟩k,
(10)
where the ﬁrst equality is by the reproducing property (3) and the second by the deﬁnition of the
Bochner integral [37].
How much smaller Pk is compared to P(X )? It turns out that Pk = P(X ) if and only if
k is bounded [88, Proposition 2], which is satisﬁed by nearly all kernels used in practice. The
integral ΦkP is empirically estimated given i.i.d. samples {Xi}n
i=1 from P by replacing P with the
empirical measure Pn = n−1 Pn
i=1 δXi. We obtain
ΦkPn = n−1
n
X
i=1
k(·, Xi).
(11)
The discussion so far has assumed little structure on X ; indeed we could take X to be inﬁnite-
dimensional if one so desires.
Closed form expressions of KME for certain pairs of kernels and probability distributions exist.
The most relevant to FDA are closed forms when P is a Gaussian measure on X , i.e. a Gaussian
process whose paths lie in X . Such formulas can be found in [51, 98] for the SE-T kernel from
(5), see Example 3. Closed form expressions in the case X ⊆Rd can be found in [7].
2.4
Maximum Mean Discrepancy
Armed with the deﬁnition of a KME, one may ask for P, Q ∈Pk, how different ΦkP and ΦkQ are
as functions in Hk? This is measured by the maximum mean discrepancy (MMD) deﬁned as the
norm in the RKHS of the difference of the two embeddings.
Deﬁnition 5. Given a measurable space X , a kernel k on X and P, Q ∈Pk(X ) the maximum
mean discrepancy between P and Q is deﬁned as MMDk(P, Q) = ∥ΦkP −ΦkQ∥k.
Due to the reproducing property (3) there are other interpretations of MMD in terms of the ker-
nel. They will help us better understand how MMD behaves and how it depends on the properties
of the kernel k. The next result is well known in the KME literature [88, 40] and we provide the
proof in full for clarity.
Theorem 1. Let X be a topological space, k a kernel on X and P, Q ∈Pk(X ). Then
MMDk(P, Q)2 =
 
sup
∥f∥k≤1

Z
X
f(x)dP(x) −
Z
X
f(x)dQ(x)

!2
(12)
=
Z
X
Z
X
k(x, y)d(P −Q)(x)d(P −Q)(y).
(13)
If X is a Hilbert space and k(x, y) = bµ(x −y) for some µ ∈P(X ) then
MMDk(P, Q)2 =
Z
X
| bP(v) −bQ(v)|2dµ(v).
(14)
8

Proof. First, by (10)
sup
∥f∥k≤1

Z
X
f(x)dP(x) −
Z
X
f(x)dQ(x)
 = sup
∥f∥k≤1
|⟨f, ΦkP −ΦkQ⟩k|
= ∥ΦkP −ΦkQ∥k,
where the second equality is by Cauchy-Schwarz [21, Corollary 5.1.4]. For the second representa-
tion of MMDk,
∥ΦkP −ΦkQ∥2
k = ⟨ΦkP, ΦkP⟩k + ⟨ΦkQ, ΦkQ⟩k −2⟨ΦkP, ΦkQ⟩k,
so it sufﬁces to compute ⟨ΦkP, ΦkQ⟩k, and then set P = Q for the other two terms. To this end,
using (10) on ΦkP gives
⟨ΦkP, ΦkQ⟩k = EY ∼Q[ΦkP(Y )] =
Z
X
Z
X
k(x, y)dP(x)dQ(y),
which completes the proof of the second representation. For the last representation we write
MMDk(P, Q)2 =
Z
X
Z
X
k(x, y)d(P −Q)(x)d(P −Q)(y)
(15)
=
Z
X
Z
X
Z
X
ei⟨x−y,v⟩X dµ(v)d(P −Q)(x)d(P −Q)(y)
(16)
=
Z
X
Z
X
ei⟨x,v⟩X d(P −Q)(x)
 Z
X
e−i⟨y,v⟩X d(P −Q)(y)

dµ(v)
(17)
=
Z
X
| bP(v) −bQ(v)|2dµ(v),
(18)
where (15) is using (13), (16) is by the deﬁnition of the Fourier transform for measures, (17) is
Fubini’s theorem [21, Theorem 4.4.5] and (18) comes from the product of a complex number and
its conjugate being its absolute value squared.
In Section 2.2 we provided examples of kernels that satisfy k(x, y) = bµ(x −y) for some µ ∈
P(X ). Quantities of the form (12) are known as integral probability metrics [66]. Many common
distances of probability measures are of this form — prominent examples are the Wasserstein and
the total variation distance, where the only difference is that the supremum in (12) is taken over
a different set. MMD can be easily estimated given samples from P, Q by means of a standard
empirical estimate of (13). This approximation is very straightforward and has pleasant statistical
properties. The representation (14) shows how MMD can be viewed as a weighted L2-type distance
between the Fourier transforms of probability measures. This will allow us to draw connections
between MMD and testing procedures based on the empirical characteristic function in Section 6.
3
Equivalence of Kernel Mean Embeddings and h-Depth
An immediate connection between KME and h-depth can be made. It will be used in Section 4 to
establish consistency and asymptotic normality of h-depth estimators in a streamlined way using
9

the structure of the RKHS. In Section 5 we use it to show that h-depth characterises probability
distributions completely. Although some instances of depth functions have already been used in
machine learning research [36, 22, 52], as far as we are aware, this is the ﬁrst solid connection
of a depth notion with concepts from theoretical machine learning literature. The next theorem
connects KME and h-depth. Its statement follows directly from Deﬁnitions 1 and 4.
Theorem 2. If X is a normed vector space and
k(x, y) = κ(∥x −y∥X)
(19)
is a kernel on X with κ satisfying the conditions in Deﬁnition 1, then Dκ(x; P) = ΦkP(x).
It is natural to ask what conditions are needed on κ aside from those in Deﬁnition 1 to ensure
that the corresponding k deﬁned in (19) is a kernel. This is in fact a classical problem studied since
the 1930s — kernels of the form (19) are known as radial kernels [82] since they depend only on
the normed difference of the two inputs. The next result was proven in [83, Theorem 1.1] in the case
of X = Rd. But, the same proof works for X a separable Hilbert space since the supporting results
[82] also hold in that situation. Following [82], call a function f : [0, ∞) →[0, ∞) completely
monotone if (−1)nf (n)(t) ≥0 for all t > 0 and all n = 0, 1, 2, . . . .
Theorem 3. Let X be a separable Hilbert space, κ: [0, ∞) →[0, ∞) and k(x, y) = κ(∥x−y∥X).
Then the following are equivalent:
(i) k is a kernel.
(ii) There exists a ﬁnite Borel measure µ on [0, ∞) such that
k(x, y) =
Z ∞
0
e−t2∥x−y∥2
X dµ(t).
(20)
(iii) κ(√·) is completely monotone.
If κ(√·) is completely monotone then κ is continuous and non-increasing. By (20) one can see
that if κ, or equivalently k, is zero anywhere, then µ must be the zero measure hence also κ would
be identically zero. Therefore if κ is not identically zero then it is zero nowhere and κ(0) > 0. This
leaves only the decay condition in Deﬁnition 1 to be checked on a case by case basis to conclude
that a radial kernel corresponds to a function κ that satisﬁes Deﬁnition 1. Not all kernels decay to
zero, as can be seen in the example of a kernel k(x, y) = e−∥x−y∥2
Rd + 1 on Rd. Nevertheless, if the
condition of decay to zero in Deﬁnition 1 is relaxed to
lim
t→∞κ(t) = inf
t≥0 κ(t)
(21)
then κ(√·) being completely monotone ensures this modiﬁed decay condition holds.
In conclusion, if condition (21) is used in Deﬁnition 1 then for any radial kernel k and κ
deﬁned by (19) the KME corresponds to an h-depth. Note that interestingly (20) shows that any
radial kernel is an average of squared exponential kernels with the average being taken over the
bandwidth hyper-parameter considered in Remark 2.
10

Examples of h-depths corresponding to KMEs. Examples of functions κ that satisfy the con-
ditions of Theorem 3 include κ(t) = e−t2/2 corresponding to the SE-I kernel introduced in (5)
in Section 2.2. This is a standard choice in the practice of depth-based methods resulting in the
widely used Gauss h-depth function. Another example not explored in the literature on h-depth
is κ(t) = (t2 + 1)−1/2 corresponding to the IMQ-I kernel (6). In fact, the IMQ-I kernel also
appears naturally in the context of statistical analysis as it can be obtained from (20) by setting
µ to be a Gaussian measure restricted to [0, ∞) with appropriate scaling. In more generality, if
T : X →Y is a linear map then by Theorem 3 both the SE-T and IMQ-T kernels are h-depths.
Indeed, in that situation k(x, y) = κ(∥x −y∥Z) where Z is the Hilbert space with inner product
⟨x, y⟩Z = ⟨Tx, Ty⟩Y.
3.1
Integrated h-Depth and (Random) Functional Projection h-Depth
Besides the h-depths considered throughout this article, another major class of functional depths
widely used in practice are the integrated depths [30, 15, 72, 81]. Special cases of integrated
depths are the popular modiﬁed band depth [60] and the multivariate functional halfspace depth
[13]. Integrated depths are in general obtained by averaging univariate depths of one-dimensional
projections of functional data, with respect to a reference measure deﬁned in the dual of the func-
tion space X . For simplicity we will focus on the situation when X is a Hilbert space and consider
integrated depths of the form
DI(x; P) =
Z
X
Dκ1(⟨x, v⟩X; Pv)dν(v)
where Dκ1 : R × P(R) →[0, ∞) is a one-dimensional h-depth (1) using κ1 : [0, ∞) →[0, ∞),
Pv ∈P(R) is the distribution of ⟨X, v⟩X where X ∼P and v ∈X , and ν is some measure on
X (identiﬁed with its dual using the Riesz representation theorem [47, Theorem 3.2.1]). In this
context, we may write
DI(x; P) =
Z
X
Dκ1(⟨x, v⟩X; Pv)dν(v) =
Z
X
Z
X
κ1 (|⟨x, v⟩X −⟨y, v⟩X|) dP(y)dν(v).
(22)
A depth quite similar to (22) was used also in [17] under the name random projection depth. The
term random comes from the way this depth is used in practice, as depths of this type typically have
to be numerically approximated. The outer integral in (22) is commonly replaced by an average
taken over i.i.d. realisations v1, . . . , vm ∈X sampled from ν, for m ∈N large enough. This
(i) makes the practically used integrated depths inherently random, (ii) can be computationally
expensive if ν is hard to sample from, and (iii) necessarily decreases the accuracy of procedures
which rely on the depth, and the computation of the depth itself. The next theorem gives conditions
under which an integrated h-depth (22) can be represented as a KME. In particular, we demonstrate
that under mild conditions the depth in (22) can be expressed in a closed form, without the need for
a random numerical approximation of the outer integral. Hence, the term random can be dropped.
Theorem 4. Let X be a Hilbert space and DI be an integrated h-depth of the form (22) for κ1
satisfying Theorem 3. Then there exists µ1 ∈P(R) whose Fourier transform bµ1 satisﬁes κ1(|s −
t|) = κ1(0)bµ1(s−t) for all s, t ∈R. In addition, DI(x; P) = κ1(0) ΦkP(x) for k(x, y) = bµ(x−y).
Here, x, y ∈X , and µ ∈P(X ) is the probability measure associated with the random element
UV in X where U ∼µ1 ∈P(R) and V ∼ν ∈P(X ) are independent.
11

Proof. The existence of µ1 follows from Theorem 3 and our remarks on translation invariant ker-
nels from Section 2.2 applied to function eκ(·) = κ1(·)/κ1(0). Denote by k1 the kernel on R
corresponding to eκ. Starting at (22) we can write
DI(x; P) = κ1(0)
Z
X
Z
X
eκ (|⟨x, v⟩X −⟨y, v⟩X|) dP(y)dν(v)
= κ1(0)
Z
X
Z
X
k1(⟨x, v⟩X, ⟨y, v⟩X)dP(y)dν(v)
(23)
= κ1(0)
Z
X
Z
X
Z
R
ei⟨x−y,uv⟩X dµ1(u)dP(y)dν(v)
(24)
= κ1(0)
Z
X
Z
X
Z
R
ei⟨x−y,uv⟩X dµ1(u)dν(v)dP(y)
(25)
= κ1(0)
Z
X
k(x, y)dP(y) = κ1(0) ΦkP(x),
(26)
where (23) is by the assumption on κ1, (24) is by the assumption k1(s, t) = bµ1(s −t), (25) is by
Fubini’s theorem [21, Theorem 4.4.5] and (26) is by noting that
R
X
R
R ei⟨x−y,uv⟩X dµ1(u)dν(v) is
the Fourier transform of the random variable UV with U ∼µ1 and V ∼ν are independent.
A common choice of ν in practice is a Gaussian measure on X . The next example shows
that the framework of Theorem 4 provides a way to obtain the integrated depth (22) in a closed
form, circumventing the need for an empirical approximation as previously done in the literature
[17, 25, 13]. This important observation brings an original viewpoint on the standardly used (ran-
dom) functional projection depths, and explains their good performance consistently observed in
simulations and real data applications. It also allows direct computation of the sample version of
the integrated depth (22) by means of the empirical KME as considered in (11).
Example 2. Consider DI(x; P) from (22) with κ1(|s −t|) = k1(s, t) = e−1
2 |s−t|2, so µ1 is the
standard normal on R and ν = NC for some Gaussian measure with mean zero and covariance
operator C ∈L+
1 (X ). Then the corresponding kernel k from Theorem 4 is equal to the IMQ-C1/2
kernel from (6) from Section 2.2. To see this ﬁrst note that the IMQ-C1/2 kernel may be written as
kIMQ-C1/2(x, y) =
 ∥C1/2x −C1/2y∥2
X + 1
−1/2 =
Z
X
e−1
2⟨x−y,v⟩2
X dNC(v),
(27)
due to the distribution of ⟨x −y, v⟩X with v ∼NC being the univariate normal distribution
N(0, ⟨C(x −y), x −y⟩X) and standard Gaussian integral formulas, for a proof see [98, Section
5]. Now using (27) along with (23) and Fubini’s theorem [21, Theorem 4.4.5] we obtain
DI(x; P) =
Z
X
Z
X
k1(⟨x, v⟩X, ⟨y, v⟩X)dNC(v)dP(y)
=
Z
X
kIMQ-C1/2(x, y)dP(y) = ΦkIMQ-C1/2P(x).
In summary, the KME of the IMQ-C1/2 kernel is equal to the integrated h-depth (or the functional
projection depth) (22) using as κ1 the one-dimensional Gauss h-depth (1) and NC as the averaging
12

measure ν. The sample version of that integrated h-depth does not have to involve numerical
approximation, and can be evaluated directly using (11) as
DI(x; Pn) = n−1
n
X
i=1
(∥C1/2x −C1/2Xi∥2
X + 1)−1/2.
4
Asymptotics of Functional Depth Using KME
Recall that for a sample X1, . . . , Xn of i.i.d. random variables from P ∈P(X ) we denote by Pn ∈
P(X ) the corresponding random empirical measure. Consistency results for h-depth obtained
in [67] show almost complete convergence of Dκ(x; Pn) to Dκ(x; P) uniformly across bounded
subsets of the input space X . A different uniform consistency result for h-depth without rates of
convergence can be found in [35]. For (random) functional projection depth (22) no explicit results
regarding the sample version asymptotics are available in the literature.
We are now ready to use the connections observed in Section 3 to present an array of new
theoretical results regarding the uniform consistency and asymptotic normality of the h-depth (1)
and the functional projection depth (22). We demonstrate that asymptotic results with far weaker
assumptions on X and with far less complicated proofs are possible to be derived by using the
representation (12), and exploiting the observed connections between h-mode depth (1) and func-
tional projection depth (22) with KMEs. We use the fact that the MMD in (12) between Pn and P
is an empirical process [37, Chapter 3], for which concentration inequalities are readily available
in the literature.
Uniform consistency: Completely observed data. Our ﬁrst result involves a notion of con-
vergence called almost complete convergence. For a sequence of real-valued random variables
{Xn}∞
n=1, a real-valued random variable X, and a sequence {an}∞
n=1 ⊂[0, ∞) converging to zero
we write Xn −X = Oa.co.(an) if there exists some ε0 > 0 such that P∞
n=1 P(|Xn −X| > anε0) <
∞. This notion of convergence is stronger than almost sure convergence [67].
Theorem 5. Let X be a separable topological space, k be a bounded, continuous kernel on X and
P ∈P(X ). Then for Pn the empirical measure formed by i.i.d. observations from P we have
sup
x∈X
|ΦkPn(x) −ΦkP(x)| = Oa.co.
p
log n/n

.
Proof. First note that by the reproducing property (3) of the RKHS and the Cauchy-Schwarz in-
equality [21, Corollary 5.1.4]
sup
x∈X
|ΦkPn(x) −ΦkP(x)| = sup
x∈X
|⟨k(·, x), ΦkPn −ΦkP⟩k|
≤sup
x∈X
p
k(x, x)∥ΦkPn −ΦkP∥k ≤c ∥ΦkPn −ΦkP∥k,
for some c < ∞by the boundedness assumption on the kernel. Without loss of generality suppose
c = 1 and for ease of notation set En = ∥ΦkPn −ΦkP∥k. By (12) we know that En is an empirical
process indexed by the RKHS. As k is continuous and X is separable the RKHS is also separable
[90, Lemma 4.33], and we may apply simple concentration inequalities for empirical processes
13

over separable Hilbert spaces. For example, [95, Proposition A.1] gives P(En > ε) ≤e−1
2(√nε−1)2.
Setting ε0 = 2 in the deﬁnition of almost complete convergence gives
∞
X
n=1
P(En > ε0
p
log n/n) ≤
∞
X
n=1
e−1
2(4 log n−4√log n+1) = e−1
2
∞
X
n=1
n−2e2√log n < ∞.
Theorem 5 is similar to [67, Theorem 2] but the set over which the supremum is taken here does
not depend on n and the assumptions on X are less restrictive. By choosing a kernel which satisﬁes
Theorems 2 and 5 we get a consistency result for h-depth. An analogous result for functional
projection depth follows directly from Theorem 4 and Example 2.
Having obtained the consistency and the rate of convergence of h-depth we may ask the same
questions about the maximizer of the h-depth of P
θ(P) = arg max
x∈X
Dκ(x; P).
Exploiting the analogy of h-depth and functional pseudo-density estimation [27] the point θ(P)
may be interpreted as a functional h-mode [17] of the probability measure P. In case when the
argument of maxima is not a single point set, any measurable selection from that set can be used
in the deﬁnition of θ(P). A natural estimator of the functional h-mode is its empirical counter-
part θn(P) = arg maxx∈X Dκ(x; Pn). Under the assumption of uniqueness of θ(P), the statistic
θn(P) estimates θ(P) consistently, with an explicit rate of convergence. The proof of this result
follows directly from [27, Section 9.7.3] and [19, Theorem 1]. An analogous result for functional
projection depth (22) is immediate.
Theorem 6. For a kernel satisfying Theorem 2 and under the assumptions of Theorem 5, suppose
in addition that for
Aε = {x ∈X : Dκ(x; P) ≥Dκ(θ(P); P) −ε} .
we can write T(ε) = supx,y∈Aε ∥x −y∥X →0 as ε →0. Then
∥θn(P) −θ(P)∥X = Oa.co.

T
p
log n/n

.
A remarkable feature of h-depth and functional projection depth is that the convergence in
Theorems 5 and 6 holds over the whole inﬁnite-dimensional space X , with no restrictions imposed
on P. Furthermore, the rates of convergence of the depth do not implicitly depend on the structure,
or the complexity of X , as well as they are not affected by the concentration properties of P.
Results of this generality and simplicity are rare in nonparametric FDA [27, 28]. The rate of
convergence of the functional h-mode θn(P) does, of course, hinge on the degree of “peakedness”
of the h-depth measured by the diameter of its upper level set Aε. In speciﬁc cases, the latter
quantity can be expressed explicitly, giving us an impression of the typical rates of convergence of
the functional h-mode estimator.
Example 3. For a Gaussian measure P = Nm,C ∈P(X ) in a separable Hilbert space X with
mean m ∈X and covariance operator C ∈L+
1 (X ) the h-depth with the SE-I kernel takes the
form [51]
Dκ(x; P) = (det (I + C))−1/2 e−1
2⟨(I+C)−1(x−m),x−m⟩.
14

Here, det(I + C) = Q∞
j=1(1 + λj) is the Fredholm determinant of the operator I + C with
eigenvalues {1 + λj}∞
j=1, which is well-deﬁned since C is trace class. The h-mode θ(P) is therefore
the mean m, its depth is Dκ(m; P) = (det (I + C))−1/2, and the set Aε can be written as
Aε =

x ∈X :

(I + C)−1(x −m), x −m

≤cε
	
with cε = −2 log

1 −ε (det (I + C))1/2
. The level set Aε is therefore the X -valued “ellipsoid”
of diameter T(ε) = 2
p
cε(1 + λ1) with 1 + λ1 the largest eigenvalue of the operator I + C. Since
cε = O(ε) we obtain that T(ε) = O(ε1/2) and using Theorem 6 we see that the rate of convergence
of the sample h-mode is ∥θn(P) −θ(P)∥X = Oa.co.

(log n/n)1/4
.
The following result is similar to [34, Theorem 1] where it is shown that the J-th order adjusted
band depth [34] is P-uniformly strongly consistent when considering a supremum over a uniformly
equicontinuous set of functions on [0, 1]. The adjusted band depth is a functional depth related to h-
depth if one extends the deﬁnition of the former to include J = 1 [68, Section 3.2.2]. The following
theorem improves on the latter consistency result for h-depth by not requiring the supremum to be
taken over an equicontinuous subset, with more general assumptions on X .
Theorem 7. Let X be a separable topological space and k be a bounded, continuous kernel on X .
Then for every ε > 0
sup
P ∈P(X)
P

sup
m≥n
sup
x∈X
|ΦkPm(x) −ΦkP(x)| > ε

→0.
Proof. Copying the start of the proof of Theorem 5, with the same notation, we have
P(sup
m≥n
Em > ε) ≤
X
m≥n
P(Em > ε) ≤
X
m≥n
e−1
2(√mε−1)2.
(28)
The last term in (28) converges to zero as n →∞uniformly for all P ∈P(X ) since the concen-
tration inequality has no constant dependent on P. This completes the proof.
Uniform consistency: Discretely observed data. In practice, functional data are seldom observed
completely, simply because each random function typically attains a continuum of values. Instead,
we standardly observe only discretised versions of the functional data, having the value of each
functional observation Xi ∈X from Pn ∈P(X ) recorded only at a ﬁnite grid in the domain,
with possible measurement errors contaminating the discretised observations. In that setting, it is
standard to ﬁrst perform data pre-processing, consisting of approximating the unobserved function
Xi by its estimate e
Xi based on the available data. We will use ePn ∈P(X ) to denote an empirical
distribution of the reconstructed functions e
X1, . . . , eXn. Given assumptions on the quality of the
reconstruction, consistency for the estimation of the KME is obtained. These results are, of course,
directly applicable to h-depth and functional projection depth.
The next theorem is similar to [70, Theorem 7] but the proof leverages the KME representation
of h-depth and uses the relationship between MMD and weak convergence to easily obtain the
desired convergence result.
15

Theorem 8. Let X be a separable metric space, k a bounded, continuous kernel on X and P ∈
P(X ). Let Pn be an empirical measure formed by i.i.d. observations from P and let ePn be an
approximation of Pn. Then P( ePn
w−→P) = 1 implies
sup
x∈X
Φk ePn(x) −ΦkP(x)

a.s.
−−→0.
Proof. As in the proof of Theorem 5 and Theorem 7 bound the supremum norm by MMDk( ePn, P).
By the assumption on ePn we have MMDk( ePn, P) →0 with probability one since MMD is domi-
nated by weak convergence, see [87, Lemma 10] and [41, Theorem 1.D.1].
A simple sufﬁcient condition ensuring validity of Theorem 8 is given in the next lemma.
Lemma 1. Let X be a separable metric space, X1, . . . , Xn, . . . a sequence of i.i.d. observations
from P ∈P(X ), and let e
X1, . . . , e
Xn, . . . ∈X be a sequence of its independent approximations.
Then E[∥˜Xn −Xn∥X] →0 as n →∞implies P( ePn
w−→P) = 1.
Proof. As discussed in the proof of [70, Theorem 1] the assumption on e
Xn and Markov’s inequality
show that e
Xn converges in law to P. Applying [71, Lemma 2] completes the proof.
Two common observation scenarios in FDA are those of densely, and sparsely observed func-
tional data. In the former case, it is assumed that as the sampling process continues, each Xn ∈X
is observed at an increasingly denser grid of time points in its domain. Under that assumption,
Lemma 1 is valid in, for example, the reconstruction scheme based on nonparametric smoothing
as described in [70]. Note, however, that Theorem 8 applies also to situations when the random
functions are observed sparsely, meaning that each Xi ∈X is known only at a ﬁnite grid of a
limited number of time points. In that situation, information from the pooled sample of all the
observed data must be utilised to obtain the reconstructed curves e
X1, . . . , e
Xn, which are there-
fore typically no longer independent [99]. Our only requirement for validity of Theorem 8 is the
Varadarajan-type assumption [21, Theorem 11.4.1] of the almost sure weak convergence of ePn to
P that needs to be veriﬁed on a case-by-case basis, depending on the exact observation scenario,
and the reconstruction method selected.
Asymptotic normality. Aside from ascertaining that a KME (or a functional depth) can be esti-
mated consistently, another desire is to know its asymptotic distribution. Since ΦkPn is an empiri-
cal mean in the RKHS Hk we can simply employ Hilbert space CLT-type results. In the following
theorem we establish a uniform central limit theorem for the sample KME, applicable to both h-
depth and functional projection depth. Remarkably, the result holds true without any assumptions
on the distribution P ∈P(X ), over the whole — possibly inﬁnite-dimensional — sample space
X . It is the ﬁrst result of its kind for a functional depth available in the literature. The only compa-
rable results are [15, Theorem 4] and [73, Theorem 1] where much weaker, non-uniform versions
of CLTs are derived for speciﬁc integrated functional depths, under restrictive conditions on the
distribution P ∈P(X ).
Theorem 9. Let X be a separable topological space, k be a bounded, continuous kernel on X and
P ∈P(X ). Then
n
1
2(ΦkPn −ΦkP)
d−→Z,
16

where Z is a Gaussian random element in Hk with covariance operator C ∈L+
1 (Hk) given by
⟨Cf, g⟩k = EX∼P[f(X)g(X)].
Proof. By [90, Lemma 4.33] the RKHS Hk is separable. Since k is bounded, EX∼P[∥k(X, ·)∥2
k] =
EX∼P[k(X, X)] is ﬁnite. Therefore, we may apply a classic CLT in separable Hilbert spaces [46,
Theorem 2.1].
For certain kernels a similar result holds even if the samples from P are weakly dependent,
namely if they are L2-m-approximable in the sense of [45], see also [46, Chapter 16]. This is
because the weak dependence is inherited by the kernel to become weak dependence in the RKHS,
which allows [46, Theorem 16.3] to be applied.
5
Characterising Probability Measures Using Functional Depth
The depth function X →[0, ∞): x 7→D(x; P) is a concept designed to serve as a representative
of the underlying probability distribution P ∈P(X ) in nonparametric statistics. As such, an
important desideratum is the characterisation property of a statistical depth, meaning that for any
two different distributions P ̸= Q in P(X ), there should exist a point x ∈X that distinguishes P
and Q in terms of their depth, i.e. D(x; P) ̸= D(x; Q). Despite the vast amount of research on the
properties of various depth functions, not many depths are known to satisfy the characterisation
property, even in the base case X = Rd [64, Section 3.3]. The derivation of characterisation results
is generally not easy. Mainly for these reasons, the same problem in function spaces X has not
received much attention in the literature [68, Section 8.2.4]. So far, no functional depth has been
identiﬁed to satisfy the characterisation property.
Using Theorem 2 we see that our characterisation problem for h-depth and functional projec-
tion depth reduces to the question of when, for a kernel k(x, y) = κ(∥x −y∥X) with κ satisfying
Deﬁnition 1, the mean embedding Φk is injective. Or, in other words, when does MMDk(P, Q) = 0
imply P = Q? Kernels for which this holds are called characteristic. Not all kernels have this
property.
Example 4. Suppose P, Q ∈P(Rd) have well deﬁned means. Consider the kernel k(x, y) =
⟨x, y⟩Rd on Rd. Since
R
Rd
R
Rd k(x, y)dP(x)dQ(y) = ⟨EX∼P[X], EY ∼Q[Y ]⟩Rd, by (13) we obtain
that MMDk(P, Q) = ∥EX∼P[X] −EY ∼Q[Y ]∥Rd. Hence, MMDk(P, Q) = 0 if and only if P and
Q have the same mean, and the kernel k is not characteristic.
The problem of identifying characteristic kernels has been studied widely in statistical machine
learning [39, 88, 87]. Most research has focused on the case where X ⊆Rd for which many
characteristic kernels are known [88]. Examples include the SE, IMQ and Mat´ern kernels, directly
providing examples of h-depths in Rd that characterise all probability distributions. A concept
typically used in conjunction with characteristicness of a kernel is universality. Nevertheless, to
realise the link the space X needs to be locally compact [87] which does not hold if X is an inﬁnite-
dimensional Hilbert space, a common assumption in FDA. Therefore this link is hard to use in our
main scenario of interest.
Theory for characteristic kernels over function spaces X has not been studied much. In [11] a
characteristic kernel was derived for probability measures over the space of continuous functions of
bounded variation using the signature transform [10] which involves an inﬁnite sum in the kernel.
17

In practice, however, the sum has to be truncated, meaning the resulting kernel used in practice is
not characteristic. In [42] it was shown that the SE-I kernel can distinguish Gaussian probability
measures using explicit formulas of the KMEs. A natural generalisation of the established theory
of characteristic kernels was presented in [98] which dealt with kernels on separable Hilbert spaces.
The main idea is to use the spectral properties of the kernel to prove characteristicness. The next
result was proven for X = Rd in [88] and the proof for an arbitrary Hilbert space is analogous.
Theorem 10. Let X be a Hilbert space and k(x, y) = bµ(x −y) for some µ ∈P(X ). Then µ
having full support implies that k is characteristic.
Proof. By (14) if MMDk(P, Q) = 0 and µ has full support then bP = bQ on every open set of X .
Since bP, bQ are continuous this gives that bP = bQ and so, by injectivity of the Fourier transform of
probability measures [37], we obtain P = Q.
The next two examples use Theorem 10 to give characteristic kernels.
They involve the
condition of C ∈L+
1 (X ) being injective. This is equivalent to ⟨Cx, x⟩X = 0 if and only if
x = 0 and holds, for example, if all the eigenvalues of C are strictly positive. In the partic-
ular case of X = L2(D) for D ⊆Rd it is known that every such operator C is of the form
Cx(s) =
R
D kC(s, t)x(t)dt for some kernel kC on D [23, Theorem 2.1]. Therefore, if kC is inte-
grally strictly positive deﬁnite, meaning
R
D
R
D f(s)kC(s, t)f(t)dsdt ≥0 with equality if and only
if f = 0, then C is injective. For a list of integrally strictly positive kernels see [88].
Example 5. For any C ∈L+
1 (X ) the SE-C1/2 kernel is the Fourier transform of the centred Gaus-
sian measure NC with covariance operator C on X , see (7) in Section 2.2. The measure NC has
full support if C is injective [18, Proposition 1.2.5], therefore the SE-C1/2 kernel is characteristic
if C is injective.
Example 6. In the notation of Theorem 4 suppose that U ∼µ1 ∈P(R) and V ∼ν ∈P(X ) are
such that the distribution µ ∈P(X ) of UV has full support in X . Then the kernel corresponding
to the integrated depth (22) is characteristic by Theorem 10. This applies to the IMQ-C1/2 kernel,
used in Example 2, if C is injective since µ1 being the standard normal has full support in R and
ν = NC has full support too. In particular, the corresponding functional projection depth (22)
characterises all probability distributions in X .
Example 5 is a simple way to show that the SE kernels are characteristic for certain choices of
C. The assumption on C is, however, still limiting. For example, it is not satisﬁed by the identity
operator employed in the classical Gauss h-depth. The same line of argument was nevertheless
extended using a limiting argument in [98, Theorem 4] to obtain the next result. A consequence
of this result is that, for instance, the Gauss h-depth also completely characterises all probability
measures in separable Hilbert spaces.
Theorem 11. Let X , Y be real, separable, Hilbert spaces and T : X →Y be Borel measurable,
continuous and injective. Then SE-T and IMQ-T kernels are characteristic.
Drawing from the advances in statistical kernel methods, we have demonstrated that multiple
kernels commonly used in Hilbert spaces are characteristic. This means that the corresponding
h-depths and functional projection depths under mild conditions characterise all distributions from
P(X ). We obtain the ﬁrst examples of statistical depths that completely satisfy the characterisation
18

property not only in the Euclidean space Rd, but also in inﬁnite-dimensional Hilbert function
spaces H such as the space L2([0, 1]). Consequently, in statistical inference of P ∈P(X ) based
exclusively on h-depth or integrated depths (22), no information about P is lost, and these depths
can be used as tantamount to the probability distribution itself. This observation opens novel
research directions in nonparametric FDA, and the related depth-based statistics.
6
MMD and Empirical Characteristic Function Based Testing
The main use of MMD in practice, and its original purpose in statistical machine learning, is to
perform non-parametric tests [38, 39]. We begin by considering the two-sample problem [40],
where for P, Q ∈P(X ) we are given two independent samples {Xi}n
i=1, {Yi}n
i=1 of i.i.d. observa-
tions from P and Q, respectively. Our intention is to test the null hypothesis H0: P = Q against
the alternative H1 : P ̸= Q. We are assuming for simplicity that we observe an equal number of
observations from each distribution but this can be easily generalised.
A natural test statistic based on the expression for MMD from (13) is the U-statistic
\
MMDk(P, Q)2 =
X
1≤i<j≤n
(k(Xi, Xj) + k(Yi, Yj) −k(Xi, Yj) −k(Xj, Yi)) .
(29)
The idea is that for P = Q should this test statistic, which approximates MMDk(P, Q)2, be small.
The asymptotic distribution of (29) under the null hypothesis is derived from the standard U-
statistics theory [40]. The test procedure is performed using permutation bootstrap, for more details
see [40]. The two-sample test based on (29) has had wide success in a range of applications given
the different types of setups kernels can be deﬁned in, such as random vectors, graphs, or functional
data [40, 6, 98].
The connection between MMD-based tests and the existing tests in FDA have so far gone
unnoticed, likely due to the way MMD has only recently been applied to functional data. The key
link is that if k(x, y) = bµ(x−y) for some µ ∈P(X ) then the representation (14) shows that MMD,
and hence the MMD-based tests, can be interpreted in terms of a distance between characteristic
functions. Therefore, tests based on the empirical approximations of characteristic functions, such
as those presented in [48] and [43], are in fact MMD-based tests. We will discuss two examples,
one for two-sample testing for functional data in Section 6.1 and one for testing Gaussianity of
random functions in Section 6.2.
6.1
Connection to Empirical Characteristic Function Two-Sample Test
A recent two-sample test based on empirical characteristic functions of discretised random func-
tions [48, Equation (15)] is very similar to a biased estimate of MMD when using an SE kernel
(5). In [48] the authors consider functions which may have multi-dimensional outputs. We will
consider only univariate outputs for brevity but the conclusions in the general case are the same.
Let P, Q ∈P(L2([0, 1])) and let {Xi}n
i=1
i.i.d.
∼P be independent with {Yi}n
i=1
i.i.d.
∼Q. Suppose
we observe m-dimensional discretisations of these samples in equidistant points in [0, 1] denoted
by {X(m)
i
}n
i=1, {Y (m)
i
}n
i=1. For example we may suppose that the function Xi is represented by
X(m)
i
= (Xi(0), Xi(1/m), . . . , Xi((m −1)/m)), and analogously for Yi. Denote by P (m)
n
, Q(m)
n
∈
19

P(Rm) the empirical distributions, each formed from n functions observed at m locations. The
test in [48] is based upon the weighted distance between the empirical characteristic functions of
P (m)
n
and Q(m)
n
Tn =
Z
Rm

d
P (m)
n
(v) −d
Q(m)
n (v)

2
e−
∥v∥2
Rm
2
dv.
Following a similar derivation to the proof of Theorem 1 one can obtain [48, Equation (15)]
Tn = 1
n2
n
X
i,j=1
e−∥X(m)
i
−X(m)
j
∥2
Rm/2 −2
n2
n
X
i,j=1
e−∥X(m)
i
−Y (m)
j
∥2
Rm/2 + 1
n2
n
X
i,j=1
e−∥Y (m)
i
−Y (m)
j
∥2
Rm/2.
(30)
We see that (30) is almost the same as (29) using X = Rm and k(x, y) = e−∥x−y∥2
Rm/2. The only
exception is that (30) is a biased estimator of the corresponding MMD due to including i = j in
the sum in the negative term. In summary, the test statistic used in [48] is the same as a biased
estimator of MMD, using an SE kernel, between discretised versions of the random functions. This
discretisation causes the test to not be able to truly distinguish between P, Q since it only checks
the joint distribution at the observation locations. A solution to this problem has been proposed in
the literature. It can be shown that if approximations of a certain level of quality are made based
on the discretisations then the test statistic will behave as if we had non-discretised samples, see
[98, Theorem 5].
6.2
Connection to Empirical Characteristic Function Gaussianity Test
A test for Gaussianity of functional data performed in [43] uses empirical characteristic functions.
That may be viewed as an MMD-based test of normality studied in [51]. Let X be a Hilbert space,
we observe {Xi}n
i=1
i.i.d.
∼P for some P ∈P(X ) and we want to test whether P is Gaussian. The
following test statistic was proposed in [43, Equation (4)]
Tn =
Z
X
c
Pn(v) −bNmn,Σn(v)

2
dNC(v).
(31)
Here mn, Σn are, respectively, the empirical mean and covariance operator of {Xi}n
i=1, Nmn,Σn
is the Gaussian measure with mean mn and covariance operator Σn, and NC is some zero mean
Gaussian measure on X with C ∈L+
1 (X ) given.
Once again, following the proof of Theorem 1 we can rewrite (31) using MMD as
Tn = 1
n2
n
X
i=1
k(Xi, Xj) −2
n
n
X
i=1
Z
X
k(Xi, y)dNmn,Σn(y) +
Z
X
Z
X
k(x, y)dNmn,Σn(x)dNmn,Σn(y)
= 1
n2
n
X
i=1
k(Xi, Xj) −2
n
n
X
i=1
ΦkNmn,Σn(Xi) + ∥ΦkNmn,Σn∥2
k,
where k(x, y) = c
NC(x −y) is the SE-C1/2 kernel (5) from Section 2.2. This is the same test
statistic as the one derived in [51, Equation (4.1)]. The terms involving ΦkNmn,Σn can be calculated
20

in closed form using standard Gaussian measure integral identities, see [51, Proposition 4.2] and
[98, Theorem 6].
Using the observed link to MMD means that an empirical average to approximate the integral
in (31) is not required. Such an empirical approximation was used in the numerical experiments in
[43], introducing an unnecessary extra computational cost. In summary, the test for Gaussianity of
functional data based on the empirical characteristic function presented in [43] coincides with the
MMD-based tests for normality in [51] when using an SE-C1/2 kernel.
7
Conclusion
In this paper we have shown that both commonly used h-depth and (random) functional projection
depths for functional data may be realised as KMEs. As a result, alternative and shorter proofs of
existing consistency results are presented with more general assumptions. Novel results regarding
asymptotic normality of the sample functional depths are provided. The characterisation of proba-
bility measures via their h-depth and functional projection depth is proved by using characteristic
kernels. Finally, tests based on empirical characteristic functions were shown to be equivalent to
MMD-based tests due to the spectral representation (14) of MMD.
The future work exploring additional interplays of machine learning and FDA that was not
considered in this contribution would consist of employing advances in kernel-based testing to
functional data. This includes (i) independence testing using the Hilbert-Schmidt independence
criterion [38] which is the MMD between a joint and a product measure; (ii) using linear compu-
tational cost estimators of MMD [40]; and (iii) goodness-of-ﬁt tests [56, 12]. Aside from testing,
MMD is also successfully used as a distance for performing parameter inference [9], which could
be of interest especially in FDA. Speciﬁc applications of MMD-based testing to FDA have been
demonstrated in [98, 42], but there certainly remains a broad space for further applications.
Acknowledgements
The research of G. Wynne was supported by an EPSRC Industrial CASE award [EP/S513635/1]
in partnership with Shell UK Ltd. The research of S. Nagy was supported by the grant 19-16097Y
of the Czech Science Foundation, and by the PRIMUS/17/SCI/3 project of Charles University. We
thank Andrew B. Duncan for helpful comments.
References
[1] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical
Society, 68(3):337–337, 1950.
[2] F. R. Bach and M. I. Jordan. Kernel independent component analysis. Journal of Machine
Learning Research, 3:1–48, 2002.
[3] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and
Statistics. Springer US, 2004.
21

[4] J. R. Berrendero, B. Bueno-Larraz, and A. Cuevas. On Mahalanobis distance in functional
settings. Journal of Machine Learning Research, 21(9):1–33, 2020.
[5] S. Bochner.
Lectures on Fourier Integrals. With an author’s supplement on Monotonic
Functions, Stieltjes Integrals, and Harmonic Analysis. Princeton University Press, 1959.
[6] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Scholkopf, and A. J. Smola. In-
tegrating structured biological data by kernel maximum mean discrepancy. Bioinformatics,
22(14):49–57, 2006.
[7] F.-X. Briol, C. J. Oates, M. Girolami, M. A. Osborne, and D. Sejdinovic. Probabilistic
integration: A role in statistical computation? Statistical Science, 34(1):1–22, 2019.
[8] A. Chakraborty and P. Chaudhuri. On data depth in inﬁnite dimensional spaces. Annals of
the Institute of Statistical Mathematics, 66(2):303–324, 2014.
[9] W. Y. Chen, A. Barp, F.-X. Briol, J. Gorham, M. Girolami, L. Mackey, and C. Oates. Stein
point Markov chain Monte Carlo. In Proceedings of the 36th International Conference on
Machine Learning, pages 1011–1021, 2019.
[10] I. Chevyrev and A. Kormilitzin. A primer on the signature method in machine learning.
arXiv:1603.03788, 2016.
[11] I. Chevyrev and H. Oberhauser. Signature moments to characterize laws of stochastic pro-
cesses. arXiv:1810.10971, 2018.
[12] K. Chwialkowski, H. Strathmann, and A. Gretton. A kernel test of goodness of ﬁt. In
Proceedings of the 33rd International Conference on Machine Learning, pages 2606–2615,
2016.
[13] G. Claeskens, M. Hubert, L. Slaets, and K. Vakili. Multivariate functional halfspace depth.
Journal of the American Statistical Association, 109(505):411–423, 2014.
[14] J. A. Cuesta-Albertos, M. Febrero-Bande, and M. Oviedo de la Fuente. The DDG-classiﬁer
in the functional setting. TEST, 26(1):119–142, 2017.
[15] A. Cuevas and R. Fraiman.
On depth measures and dual statistics. A methodology for
dealing with general data. Journal of Multivariate Analysis, 100(4):753–766, 2009.
[16] A. Cuevas, M. Febrero, and R. Fraiman. On the use of the bootstrap for estimating functions
with functional data. Computational Statistics and Data Analysis, 51(2):1063 – 1074, 2006.
[17] A. Cuevas, M. Febrero, and R. Fraiman. Robust estimation and classiﬁcation for functional
data via projection-based depth notions. Computational Statistics, 22(3):481–496, 2007.
[18] G. Da Prato. An Introduction to Inﬁnite-Dimensional Analysis. Springer Berlin Heidelberg,
2006.
[19] S. Dabo-Niang, F. Ferraty, and P. Vieu. On the using of modal curves for radar waveforms
classiﬁcation. Computational Statistics & Data Analysis, 51(10):4878–4890, 2007.
22

[20] D. L. Donoho and M. Gasko. Breakdown properties of location estimates based on halfspace
depth and projected outlyingness. The Annals of Statistics, 20(4):1803–1827, 1992.
[21] R. M. Dudley. Real Analysis and Probability, volume 74 of Cambridge Studies in Advanced
Mathematics. Cambridge University Press, Cambridge, 2002. Revised reprint of the 1989
original.
[22] S. Dutta, S. Sarkar, and A. K. Ghosh. Multi-scale classiﬁcation using localized spatial depth.
Journal of Machine Learning Research, 17(217):1–30, 2016.
[23] G. Fasshauer and M. McCourt.
Kernel-based Approximation Methods using MATLAB.
World Scientiﬁc, 2015.
[24] M. Febrero, P. Galeano, and W. Gonz´alez-Manteiga. A functional analysis of NOx levels:
location and scale estimation and outlier detection. Computational Statistics, 22(3):411–
427, 2007.
[25] M. Febrero, P. Galeano, and W. Gonz´alez-Manteiga. Outlier detection in functional data by
depth measures, with application to identify abnormal NOx levels. Environmetrics, 19(4):
331–345, 2008.
[26] M. Febrero-Bande and M. Oviedo de la Fuente. Statistical computing in functional data
analysis: The R package fda.usc. Journal of Statistical Software, 51(4):1–28, 2012.
[27] F. Ferraty and P. Vieu. Nonparametric Functional Data Analysis. Theory and Practice.
Springer Series in Statistics. Springer, New York, 2006.
[28] F. Ferraty, N. Kudraszow, and P. Vieu. Nonparametric estimation of a surrogate density
function in inﬁnite-dimensional spaces. Journal of Nonparametric Statistics, 24(2):447–
464, 2012.
[29] R. Fraiman and J. Meloche. Multivariate L-estimation. TEST, 8(2):255–317, 1999.
[30] R. Fraiman and G. Muniz. Trimmed means for functional data. TEST, 10(2):419–440, 2001.
[31] R. Fraiman, R. Y. Liu, and J. Meloche. Multivariate density estimation by probing depth. In
L1-statistical procedures and related topics (Neuchˆatel, 1997), volume 31 of IMS Lecture
Notes Monogr. Ser., pages 415–430. Inst. Math. Statist., Hayward, CA, 1997.
[32] T. Gasser, P. Hall, and B. Presnell. Nonparametric estimation of the mode of a distribution of
random curves. Journal of the Royal Statistical Society. Series B. Statistical Methodology,
60(4):681–691, 1998.
[33] D. Gervini. Outlier detection and trimmed estimation for general functional data. Statistica
Sinica, 22(4):1639–1660, 2012.
[34] I. Gijbels and S. Nagy. Consistency of non-integrated depths for functional data. Journal of
Multivariate Analysis, 140:259–282, 2015.
23

[35] I. Gijbels and S. Nagy. On a general deﬁnition of depth for functional data. Statistical
Science, 32(4):630–639, 2017.
[36] R. Gilad-Bachrach and C. J. Burges. Classiﬁer selection using the predicate depth. Journal
of Machine Learning Research, 14(77):3591–3618, 2013.
[37] E. Gine and R. Nickl. Mathematical Foundations of Inﬁnite-Dimensional Statistical Models.
Cambridge University Press, 2015.
[38] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Sch¨olkopf. Kernel methods for
measuring independence. Journal of Machine Learning Research, 6(70):2075–2129, 2005.
[39] A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. J. Smola. A kernel method for
the two-sample-problem. In Advances in Neural Information Processing Systems 19, pages
513–520, 2007.
[40] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, and A. Smola. A kernel two-
sample test. Journal of Machine Learning Research, 13(1):723–773, 2012.
[41] C. Guilbart. ´Etude des produits scalaires sur l’espace des mesures: Estimation par projec-
tions tests `a noyaux. PhD thesis, Universit´e des Sciences et Techniques de Lille, 1978.
[42] S. Hayati, K. Fukumizu, and A. Parvardeh. Kernel mean embedding of probability measures
and its applications to functional data analysis. arXiv:2011.02315, 2020.
[43] N. Henze and M. D. Jim´enez-Gamero. A test for Gaussianity in Hilbert spaces via the
empirical characteristic functional. Scandinavian Journal of Statistics, 2021. To appear.
[44] T. Hofmann, B. Sch¨olkopf, and A. J. Smola. Kernel methods in machine learning. The
Annals of Statistics, 36(3):1171–1220, 2008.
[45] S. H¨ormann and P. Kokoszka. Weakly dependent functional data. The Annals of Statistics,
38(3):1845–1884, 2010.
[46] L. Horv´ath and P. Kokoszka. Inference For Functional Data With Applications. Springer
Science & Business Media, 2012.
[47] T. Hsing and R. Eubank. Theoretical Foundations of Functional Data Analysis, with an
Introduction to Linear Operators. John Wiley & Sons, Ltd, 2015.
[48] Q. Jiang, M. Huˇskov´a, S. G. Meintanis, and L. Zhu. Asymptotics, ﬁnite-sample comparisons
and applications for two-sample tests with functional data. Journal of Multivariate Analysis,
170:202–220, 2019.
[49] H. Kadri, E. Duﬂos, P. Preux, S. Canu, A. Rakotomamonjy, and J. Audiffren. Operator-
valued kernels for learning from functional response data. Journal of Machine Learning
Research, 17(20):1–54, 2016.
[50] M. Kanagawa, P. Hennig, D. Sejdinovic, and B. K. Sriperumbudur. Gaussian processes and
kernel methods: A review on connections and equivalences. arXiv:1807.02582, 2018.
24

[51] J. Kellner and A. Celisse. A one-sample test for normality with kernel methods. Bernoulli,
25(3):1816–1837, 2019.
[52] M. Kleindessner and U. von Luxburg. Lens depth function and k-relative neighborhood
graph: Versatile tools for ordinal data analysis. Journal of Machine Learning Research, 18
(58):1–52, 2017.
[53] G. A. Koshevoy and K. Mosler. Zonoid trimming for multivariate distributions. The Annals
of Statistics, 25(5):1998–2017, 1997.
[54] S. Kuhnt and A. Rehage. An angle-based multivariate functional pseudo-depth for shape
outlier detection. Journal of Multivariate Analysis, 146:325–340, 2016.
[55] A. Kupresanin, H. Shin, D. King, and R. Eubank. An RKHS framework for functional data
analysis. Journal of Statistical Planning and Inference, 140(12):3627–3637, 2010.
[56] Q. Liu, J. D. Lee, and M. I. Jordan. A kernelized Stein discrepancy for goodness-of-ﬁt tests.
In Proceedings of The 33rd International Conference on Machine Learning, pages 276–284,
2016.
[57] R. Y. Liu. On a notion of data depth based on random simplices. The Annals of Statistics,
18(1):405–414, 1990.
[58] R. Y. Liu, J. M. Parelius, and K. Singh. Multivariate analysis by data depth: descriptive
statistics, graphics and inference. The Annals of Statistics, 27(3):783–858, 1999.
[59] S. L´opez-Pintado and J. Romo. Depth-based classiﬁcation for functional data. In Data
depth: robust multivariate analysis, computational geometry and applications, volume 72
of DIMACS Ser. Discrete Math. Theoret. Comput. Sci., pages 103–119. Amer. Math. Soc.,
Providence, RI, 2006.
[60] S. L´opez-Pintado and J. Romo. On the concept of depth for functional data. Journal of the
American Statistical Association, 104(486):718–734, 2009.
[61] S. Maniglia and A. Rhandi. Gaussian Measures on Separable Hilbert Spaces and Applica-
tions. Quaderni di Matematica, 2004.
[62] H. Q. Minh. Some properties of Gaussian reproducing kernel Hilbert spaces and their im-
plications for function approximation and learning theory. Constructive Approximation, 32
(2):307–338, 2009.
[63] K. Mosler. Multivariate Dispersion, Central Regions and Depth: The Lift Zonoid Aap-
proach, volume 165 of Lecture Notes in Statistics. Springer-Verlag, Berlin, 2002.
[64] K. Mosler and P. Mozharovskyi. Choosing among notions of multivariate depth statistics.
Statistical Science, 2021. To appear.
[65] K. Muandet, K. Fukumizu, B. Sriperumbudur, and B. Sch¨olkopf. Kernel mean embedding
of distributions: A review and beyond. Foundations and Trends® in Machine Learning, 10
(1-2):1–141, 2017.
25

[66] A. M¨uller. Integral probability metrics and their generating classes of functions. Advances
in Applied Probability, 29(2):429–443, 1997.
[67] S. Nagy. Consistency of h-mode depth. Journal of Statistical Planning and Inference, 165:
91–103, 2015.
[68] S. Nagy. Statistical Depth for Functional Data. PhD thesis, KU Leuven and Charles Uni-
versity, 2016.
[69] S. Nagy. The halfspace depth characterization problem. In M. La Rocca, B. Liseo, and
L. Salmaso, editors, Nonparametric Statistics, pages 379–389, Cham, 2020. Springer Inter-
national Publishing.
[70] S. Nagy and F. Ferraty. Data depth for measurable noisy random functions. Journal of
Multivariate Analysis, 170:95–114, 2019.
[71] S. Nagy, I. Gijbels, and D. Hlubinka. Weak convergence of discretely observed functional
data with applications. Journal of Multivariate Analysis, 146:46–62, 2016.
[72] S. Nagy, I. Gijbels, M. Omelka, and D. Hlubinka. Integrated depth for functional data:
Statistical properties and consistency. ESAIM. Probability and Statistics, 20:95–130, 2016.
[73] S. Nagy, S. Helander, G. Van Bever, L. Viitasaari, and P. Ilmonen. Flexible integrated
functional depths. Bernoulli, 27(1):673–701, 2021.
[74] N. H. Nelsen and A. M. Stuart. The random feature model for input-output maps between
Banach spaces. arXiv:2005.10224, 2020.
[75] A. Nieto-Reyes and H. Battey. A topologically valid deﬁnition of depth for functional data.
Statistical Science, 31(1):61–79, 2016.
[76] D. Paindaveine and G. Van Bever. From depth to local depth: A focus on centrality. Journal
of the American Statistical Association, 108(503):1105–1119, 2013.
[77] V. I. Paulsen and M. Raghupathi. An introduction to the theory of reproducing kernel Hilbert
spaces, volume 152 of Cambridge Studies in Advanced Mathematics. Cambridge University
Press, Cambridge, 2016.
[78] O. Pokotylo and K. Mosler. Classiﬁcation with the pot-pot plot. Statistical Papers, 60(3):
553–581, 2019.
[79] O. Pokotylo, P. Mozharovskyi, R. Dyckerhoff, and S. Nagy. ddalpha: Depth-based classi-
ﬁcation and calculation of data depth, 2020. R package version 1.3.11.
[80] J. O. Ramsay and B. W. Silverman. Functional Data Analysis. Springer New York, 2005.
[81] K. Ramsay, S. Durocher, and A. Leblanc.
Integrated rank-weighted depth.
Journal of
Multivariate Analysis, 173:51–69, 2019.
26

[82] I. J. Schoenberg. Metric spaces and completely monotone functions. The Annals of Mathe-
matics, 39(4):811, 1938.
[83] C. Scovel, D. Hush, I. Steinwart, and J. Theiler. Radial kernels and their reproducing kernel
Hilbert spaces. Journal of Complexity, 26(6):641–660, 2010.
[84] R. Serﬂing. Depth functions in nonparametric multivariate inference. In Data depth: robust
multivariate analysis, computational geometry and applications, volume 72 of DIMACS
Ser. Discrete Math. Theoret. Comput. Sci., pages 1–16. Amer. Math. Soc., Providence, RI,
2006.
[85] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Uni-
versity Press, 2004.
[86] J. Shawe-Taylor, C. K. I. Williams, and N. Cristianiniand J. Kandola. On the eigenspec-
trum of the gram matrix and the generalization error of kernel-PCA. IEEE Transactions on
Information Theory, 51(7):2510–2522, 2005.
[87] C.-J. Simon-Gabriel and B. Sch¨olkopf. Kernel distribution embeddings: Universal kernels,
characteristic kernels and kernel metrics on distributions. Journal of Machine Learning
Research, 19(44):1–29, 2018.
[88] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch¨olkopf, and G. R. Lanckriet. Hilbert
space embeddings and metrics on probability measures. Journal of Machine Learning Re-
search, 11:1517–1561, 2010.
[89] M. L. Stein. Interpolation of Spatial Data. Springer New York, 1999.
[90] I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.
[91] I. Steinwart, D. Hush, and C. Scovel. An explicit description of the reproducing kernel
Hilbert spaces of Gaussian RBF kernels. IEEE Transactions on Information Theory, 52
(10):4635–4643, 2006.
[92] A. Struyf and P. J. Rousseeuw. Halfspace depth and regression depth characterize the em-
pirical distribution. Journal of Multivariate Analysis, 69(1):135–153, 1999.
[93] Y. Sun and M. G. Genton. Functional boxplots. Journal of Computational and Graphical
Statistics, 20(2):316–334, 2011.
[94] D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gret-
ton. Generative models and model criticism via optimized maximum mean discrepancy. In
Proceedings of the 8th International Conference on Learning Representations, 2016.
[95] I. Tolstikhin, B. K. Sriperumbudur, and K. Muandet. Minimax estimation of kernel mean
embeddings. Journal of Machine Learning Research, 18(86):1–47, 2017.
[96] J. W. Tukey.
Mathematics and the picturing of data.
Proceedings of the International
Congress of Mathematicians, Vancouver, 2:523–531, 1975.
27

[97] N. N. Vakhania, V. I. Tarieladze, and S. A. Chobanyan. Probability Distributions on Banach
Spaces. Springer Netherlands, 1987.
[98] G. Wynne and A. B. Duncan.
A kernel two-sample test for functional data.
arXiv:2008.11095, 2020.
[99] F. Yao, H.-G. M¨uller, and J.-L. Wang. Functional data analysis for sparse longitudinal data.
Journal of the American Statistical Association, 100(470):577–590, 2005.
[100] Y. Zuo and R. Serﬂing. General notions of statistical depth function. The Annals of Statistics,
28(2):461–482, 2000.
28

