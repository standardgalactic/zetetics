David Turner∗
Church’s Thesis and Functional
Programming
The earliest statement of Church’s Thesis, from Church [1936,
p. 356] is
We now deﬁne the notion, already discussed, of an eﬀectively
calculable function of positive integers by identifying it
with the notion of a recursive function of positive integers (or
of a lambda-deﬁnable function of positive integers).
The phrase in parentheses refers to the apparatus which Church had
developed to investigate this and other problems in the foundations
of mathematics: the calculus of lambda conversion. Both the Thesis
and the lambda calculus have been of seminal inﬂuence on the de-
velopment of Computing Science. The main subject of this article
is the lambda calculus but I will begin with a brief sketch of the
emergence of the Thesis.
The epistemological status of Church’s Thesis is not immediately
clear from the above quotation and remains a matter of debate, as is
explored in other papers of this volume. My own view, which I will
state but not elaborate here, is that the thesis is empirical because
it relies for its signiﬁcance on a claim about what can be calculated
by mechanisms.
This becomes clearer in Church’s restatement of
the thesis the following year, after he had seen Turing’s paper, see
below. For a fuller discussion see Hodges [this volume].
Three deﬁnitions of the eﬀectively computable functions of the
natural numbers (non-negative integers, hereafter N), developed
nearly contemporaneously in the early to mid 1930’s, turned out
to be equivalent. Church [1936, quoted above] showed that his own
theory of lambda deﬁnable functions yielded the same functions on
∗D.A. Turner, School of Computing Science, Middlesex University, UK.

Church’s Thesis and Functional Programming
519
Nk →N as the recursive functions of Herbrand and G¨odel [Herbrand
1932, G¨odel 1934]. This was proved independently by Kleene [1936].
A few months later Turing [1936] introduced his concept of logical
computing machine (LCM)—a ﬁnite automaton with an unbounded
tape divided into squares, on which it could move left or right and
read or write symbols from a ﬁnite alphabet, in accordance with a
speciﬁed state transition table. A central result of the paper is the
existence of a universal LCM, which can emulate the behaviour of
any LCM whose description is written on its tape. In an appendix
Turing shows that the numeric functions computable by his machines
coincide with the lambda-deﬁnable ones.
In his review of Turing’s paper, Church [1937] writes
there is involved here the equivalence of three diﬀerent notions:
computability by a Turing machine, general recursiveness [...]
and lambda-deﬁnability [...]
The ﬁrst has the advantage of
making the identiﬁcation with eﬀectiveness in the ordinary
sense evident immediately [...]
The second and third have
the advantage of suitability for embodiment in a system of
symbolic logic.
The Turing machine led, about a decade later, to the Tur-
ing/von Neumann computer—a realization in electronics of Turing’s
universal machine, with the important optimization that (an ini-
tial portion of) the tape is replaced by a random access store. The
concept of a programming language didn’t yet exist in 1936, but the
second and third notions were eventually to provide the basis of what
we now know as functional programming.
The Halting Theorem
All three notions of computability involve partiality in an essen-
tial way. General recursion schemas yield the partial recursive func-
tions, which may for some values of their arguments fail to produce a
result. We will write their type as Nk →N. We have N = N ∪{⊥}
where the undeﬁned value ⊥represents non-termination.1 The recur-
sive functions are the subset that are everywhere deﬁned. That this
subset is not recursively enumerable is shown by a use of Cantor’s
1The idea of treating non-termination as a peculiar kind of value, ⊥, is more
recent and was not current at the time of Church and Turing’s foundational work.

520
David Turner
diagonalization argument.2 Since the partial recursive functions are
recursively enumerable it follows that the property of being total (for
a partial recursive function) is not recursively decidable.
By a separate argument it can be shown that the property for
a partial recursive function of being deﬁned at a speciﬁed value of
its input vector is also not in general recursively decidable. Sim-
ilarly, Turing machines may not halt and lambda-terms may have
no normal form; and these properties are not, respectively, Turing-
computable or lambda-deﬁnable, as is shown in each case by a simple
argument involving self-application.
Thus of perhaps equal importance with Church’s Thesis and
which emerges from it is the Halting Theorem: given an arbitrary
computation whose result is of type N we cannot in general decide if
it is ⊥. What is actually proven, e.g. of the halts predicate on Tur-
ing machines, is that it is not Turing-computable (equiv not lambda-
deﬁnable etc). It is by an appeal to Church’s Thesis that we pass
from this to the assertion that halting is not eﬀectively decidable.
The three convergent notions (to which others have since been
added) identify an apparently unique, eﬀectively enumerable, class
of functions of type Nk →N corresponding to what is computable
by ﬁnite but unbounded means. Church’s identiﬁcation of this class
with eﬀective calculability amounts to the conjecture that this is the
best we can do.
In the case of the Turing machine the unbounded element is the
tape (it is initially blank, save for a ﬁnite segment but provides an
unlimited working store). In the case of the lambda calculus it is the
fact that there is no limit to the intermediate size to which a term
may grow during the course of its attempted reduction to normal
form.
In the case of recursive functions it is the minimalization
operation, which searches for the smallest nϵN on which a speciﬁed
recursive function takes the value 0.
The Halting Theorem tells us that unboundedness of the kind
needed for computational completeness is eﬀectively inseparable from
the possibility of non-termination.
2The proof is purely constructive and doesn’t depend on Church’s Thesis: any
eﬀective enumeration, h, of computable functions in N →N is incomplete—it
lacks f(n) = h[n)(n] + 1.

Church’s Thesis and Functional Programming
521
The Lambda Calculus
Of the various convergent notions of computability Church’s
lambda calculus is distinguished by its combination of simplicity with
remarkable expressive power.
The lambda calculus was conceived as part of a larger theory,
including logical operators such as implication, intended as an alter-
native foundation for mathematics based on functions rather than
sets. This gave rise to paradoxes, including a version of the Russell
paradox. What remained with the propositional part stripped out is
a consistent theory of pure functions, of which the ﬁrst systematic
exposition is Church [1941].3
In the sketch given here we use for variables lower case letters:
a, b, c · · · x, y, z and as metavariables denoting terms upper case let-
ters: A, B, C · · · . The abstract syntax of the lambda calculus has
three productions. A term is one of
variable e.g. x
application AB
abstraction λx.A
In the last case λx. is a binder and free occurrences of x in A
become bound. A term in which all variables are bound is said to be
closed otherwise it is open. The motivating idea is that closed term
represent functions. The intended meaning of AB is the application
of function A to argument B while λx.A is the function which for
input x returns A. Terms which are the same except for renaming
of bound variables are not distinguished, thus λx.x and λy.y are the
same, identity function.
In writing terms we freely use parentheses to remove ambiguity.
We further adopt the conventions that application is left-associative
and that the scope of a binder extends as far to the right as possible.
For example f g h means (f g)h and λx.λy.Ba means λx.(λy.[Ba)].
The calculus has only one essential rule, which shows how to
substitute an argument into the body of a function:
(β)
(λx.A)B →β [B/x]A
3In his monograph Church deﬁnes two slightly diﬀering calculi called λI and
λK, of these λK is now regarded as canonical and is what we sketch here.

522
David Turner
Here [B/x]A means substitute B for free occurrences of x in A.
The smallest reﬂexive, symmetric, transitive, substitutive relation on
terms including →β, written ⇔, is Church’s notion of λ-conversion.
If we omit symmetry from the deﬁnition we get an oriented relation,
written ⇒, called reduction.
An instance of the left hand side of rule β is called a redex. A
term containing no redex is said to be in normal form.
A term
which is convertible to one in normal form is said to be normalizing.
There are non-normalizing terms, of which perhaps the simplest is
(λx.xx)(λx.xx). We have the cyclic
(λx.xx)(λx.xx) →β (λx.xx)(λx.xx)
as the only available step.
The two most important technical results are
Church–Rosser Theorem If A ⇔B there is a term C such that
A ⇒C and B ⇒C. An immediate consequence of this is that
the normal form of a normalizing term is unique.4
Normal Order Theorem Stated informally: the normal form of
a normalizing term can be found by repeatedly reducing its
leftmost redex.5
To see the signiﬁcance of the normal order theorem consider the term
(λy.z)((λx.xx)(λx.xx))
We have
(λy.z)((λx.xx)(λx.xx)) →β z
which is the normal form. But if we try to reduce the argument
((λx.xx)(λx.xx)) to normal form ﬁrst, we get stuck in an endless
loop.
In general there are many ways of reducing a term, since it or
one of its reducts may contain multiple redexes.
The normal or-
der theorem gives a sequential procedure, normal order reduc-
tion, which is guaranteed to reach the normal form if there is one.
4This means unique up to changes of bound variable, of course.
5In case of nested redexes, leftmost is usually deﬁned as leftmost-outermost,
although the theorem will still hold if we take leftmost-innermost.

Church’s Thesis and Functional Programming
523
Note that normal order reduction substitutes arguments into func-
tion bodies without ﬁrst reducing any redexes inside the argument,
which amounts to lazy evaluation.
A closed term of pure6 λ-calculus is called a combinator. Note
that any normalizing closed term of pure λ-calculus must reduce to
an abstraction.
Some combinators with their conventional names
are:
S = λx.λy.λz.xz(yz)
K = λx.λy.x
I = λx.x
B = λx.λy.λz.x(yz)
C = λx.λy.λz.xzy
It is evident that λ-calculus has a rich collection of functions,
including functions of higher type, that is whose arguments and/or
results are functions, but since (at least closed) terms can denote
only functions and never ground objects it remains to show how to
represent data such as the natural numbers. Here are the Church
numerals
0 = λa.λb.b
1 = λa.λb.ab
2 = λa.λb.a(ab)
3 = λa.λb.a(a[ab)]
etc. · · ·
To understand this representation for numbers note the eﬀect of
applying a Church numeral to function f and object a:
0fa
⇔
a
1fa
⇔
fa
2fa
⇔
f(fa)
3fa
⇔
f(f[fa)]
6Pure means using only variables and no proper constants, as λ-calculus is
presented here.

524
David Turner
The numbers are thus represented as iterators. It is now straightfor-
ward to deﬁne the arithmetic operations, for example
+ = λm.λn.λa.λb.ma(nab)
× = λm.λn.λa.λb.m(na)b
predecessor and subtraction are a little trickier, see Church [1941].
We also need a way to branch on 0:
zero = λa.λb.λn.n(Kb)a
We have
zero A B N
⇔
A,
N ⇔0
⇔
B,
N ⇔n + 1
The master-stroke, which shows every recursive function to be
λ-deﬁnable is to ﬁnd a universal ﬁxpoint operator, that is a term Y
with the property that for any term F,
Y F ⇔F(Y F)
There are many such terms, of which the simplest is due to
H.B. Curry.
Y = λf.(λx.f[xx)](λx.f[xx)]
The reader may satisfy himself that we have Y F ⇔F(Y F) as re-
quired.
The beauty of λ-deﬁnability as a theory of computation is that it
gives not only—assuming Church’s Thesis—all computable functions
of type N →N but also those of higher type of any ﬁnite degree,
such as (N →N) →N, (N →N) →(N →N) and so on.
Moreover we are not limited to arithmetic. The idea behind the
Church numerals is very general and allows any data type—pairs,
lists, trees and so on—to be represented in a purely functional way.
Each datum is encoded as a function that captures its elimination
operation, that is the way in which information is extracted from it
during computation. It is also possible to represent codata, such as
inﬁnite lists, inﬁnitary trees and so on.
Part of the simplicity of the calculus lies in its considering only
functions of a single argument. This is no real restriction since it is a
basic result of set theory that for any sets A, B, the function spaces

Church’s Thesis and Functional Programming
525
(A × B) →C and A →(B →C) are isomorphic. Replacing the ﬁrst
by the second is called Currying.7 We have made implicit use of this
idea all along, e.g. + is curried addition.
Solvability and Non-Strictness
A non-normalizing term is by no means necessarily useless. An
example is Y , which has no normal form but can produce one when
applied to another term.
On the other hand (λx.xx)(λx.xx) is
irredeemable—there is no term and no sequence of terms to which
it can be applied and yield a normal form.
Deﬁnition: a term T is SOLVABLE if there are terms A1, · · · , Ak
for some k ≥0 such that TA1 · · · Ak is normalizing.
Thus Y is
solvable because we have for example
Y (λx.λy.y) ⇔(λy.y)
whereas (λx.xx)(λx.xx) is unsolvable.
An important result, due to Corrado B¨ohm, is that a term is
solvable if and only if it can be reduced to head normal form:
λx1 · · · λxn.xkA1 · · · Am
the variable xk is called the head and if the term is closed must be
one of the x1 · · · xn. If a term is solvable normal order reduction will
reduce it to HNF in a ﬁnite number of steps. See Barendregt [1984].
All unsolvable terms are equally useless, so we can think of them
as being equivalent and introduce a special term ⊥to represent them.
This gives us an extension of ⇔for which we will use ≡. The two
fundamental properties of ⊥, which follow from the deﬁnitions of
unsolvability and head normal form, are:
⊥A ≡⊥
λx.⊥≡⊥
Introducing ⊥allows an ordering relation to be deﬁned on terms
with ⊥as least element and a stronger equivalence relation using
limits which is studied in domain theory (see later). We make one
7After H.B. Curry, although the idea was ﬁrst used in Sch¨onﬁnkel [1924].

526
David Turner
further remark here.
Deﬁnition: a term A is STRICT if
A ⊥≡⊥
and non-strict otherwise. A strict function thus has ⊥for a ﬁxpoint
and applying Y to it will produce ⊥. So non-strict functions play
an essential role in the theory of λ-deﬁnability—without them we
could not use Y to encode recursion.
Combinatory Logic
Closely related to λ-calculus is combinatory logic, originally due
to Sch¨onﬁnkel [1924] and subsequently explored by H.B. Curry. This
has meagre apparatus indeed—just application and a small collection
of named combinators. These are deﬁned by stating their reduction
rule. In the minimal version we have two combinators, deﬁned as
follows
S x y z ⇒x z(y z)
K x y ⇒x
here x, y, z are metavariables standing for arbitrary terms and are
used to state the reduction rules. Combinatory logic terms have no
variables and are built using only constants and application:, e.g.
K(SKK).
A central result, perhaps one of the strangest in all of logic, is
that every λ-deﬁnable function can be written using only S and K.
Here is a start
I = SKK
The proof is by considering application to an arbitrary term. We
have
SKKx ⇒Kx(Kx) ⇒x
as required.
The deﬁnitive study of combinatory logic and its relationship to
lambda calculus is Curry & Feys [1958]. There are several algorithms
for transcribing λ-terms to combinators and for convenience most of
these use besides S, K, additional combinators such as B, C, I etc.

Church’s Thesis and Functional Programming
527
It would seem that only a dedicated cryptologist would choose to
write other than very small programs directly in combinatory logic.
However, Turner [1979a] describes compilation to combinators as
an implementation method for a high-level functional programming
language. This required ﬁnding a translation algorithm, described
in Turner [1979b], that produces compact combinator code when
translating expressions containing many nested λ-abstractions. The
attraction of the method is that combinator reduction rules are much
simpler than β-reduction, each requiring only a few machine instruc-
tions, allowing a fast interpreter to be constructed which carries out
normal order reduction.
The Paradox
It is easy to see why the original versions of λ-calculus and com-
binatory logic, which included properly logical notions, led to para-
doxes. (Curry calls these theories illative.) The untyped theory is
too powerful, because of the ﬁxpoint combinator, Y . Suppose N is
a term denoting logical negation. We have
Y N ⇔N(Y N)
which is the Russell paradox. Even minimal logic, which lacks nega-
tion, becomes inconsistent in the presence of Y —implication is suﬃ-
cient to generate the paradox, see Barendregt [1984, p. 575]. Because
of this Y is sometimes called Curry’s paradoxical combinator.
Typed λ-Calculi
The λ-calculus of Church [1941] is untyped: it allows the promis-
cuous application of any term to any other, so types arise only in the
interpretation of terms. In a typed λ-calculus the rules of term forma-
tion embody some theory of types. Only terms which are well-typed
according to the theory are permitted. The rules for β reduction
remain unchanged, as does the Church–Rosser Theorem. Most type
systems disallow self-application, as in (λx.xx), preventing the for-
mation of a ﬁxpoint combinator like Curry’s Y . Typed λ-calculi fall
into two main groups depending on what is done about this
(i) Add an explicit ﬁxpoint construction to the calculus—for exam-
ple a polymorphic constant Y of type schema (α →α) →α,

528
David Turner
with reduction rule Y H ⇒H(Y H). This allows general re-
cursion at every type and thus retains the computational com-
pleteness of untyped λ.
(ii) In the other kind of typed λ-calculus there is no ﬁxpoint con-
struct and every term is normalizing. This brings into play
a fundamental isomorphism between programming and logic:
the Propositions-as-Types principle.
This gives two apparently very diﬀerent models of functional pro-
gramming, which we discuss in the next two sections.
Lazy Functional Programming
Imperative programming languages, from the earliest such as
FORTRAN and COBOL which emerged in the 1950’s to current
“object-oriented” ones such as C++ and Java have certain features
in common. Their basic action is the assignment command, which
changes the content of a location in memory and they have an ex-
plicit ﬂow of control by which these state changes are ordered. This
reﬂects more or less directly the structure of the Turing/von Neu-
mann computer, as a central processing unit operating on a passive
store. Backus [1978] calls them “von Neumann languages”.
Functional8 programming languages oﬀer a radical alternative—
they are descriptive rather than imperative, have no assignment com-
mand and no explicit ﬂow of control—sub–computations are ordered
only partially, by data dependency.
The claimed merits of functional programming—in conciseness,
mathematical tractability, potential for parallel execution—have
been argued in many places so we will not dwell on them here. Nor
will we go into the history of the concept, other than to say that the
basic ideas go back over four decades, see in particular the important
early papers of McCarthy [1960], Landin [1966]—and that for a long
period functional programming was mainly practised in imperative
languages with functional subsets (LISP, Scheme, Standard ML).
The disadvantages of functional programming within a language
that includes imperative features are two. First, you are not forced
8We here use functional to mean what some call purely functional, an older
term for this is applicative, yet another term which includes other mathematically
based models, such as logic programming, is declarative.

Church’s Thesis and Functional Programming
529
to explore the limits of the functional style, since you can escape at
will into an imperative idiom. Second, the presence of side eﬀects,
exceptions etc., even if they are rarely used, invalidate important
theorems on which the beneﬁts of the style rest.
The λ-calculus is the most natural candidate for functional pro-
gramming: it is computationally complete in the sense of Church’s
Thesis, it includes functions of higher type and it comes with a the-
ory of λ-conversion that provides a basis for reasoning about pro-
gram transformation, correctness of evaluation mechanisms and so
on. The notation is a little spartan for most tastes but it was shown
long ago by Peter Landin that the dish can be sweetened by adding
a sprinkling of syntactic sugar.9
Eﬃcient Normal Order Reduction
The Normal Order Theorem tells us that an implementation of
λ-calculus on a sequential machine should use normal order reduc-
tion10, otherwise it may fail to ﬁnd the normal form of a normalizing
term. This requires that arguments be substituted unevaluated into
function bodies as we noted earlier. In general this will produce mul-
tiple copies of the argument, requiring any redexes it contains to be
reduced multiple times. For λ-calculus-based functional program-
ming to be a viable technology it is necessary to have an eﬃcient
way of handling this.
A key step was the invention of normal graph reduction, by
Wadsworth [1971].
In this scheme the term is held as a directed
acyclic graph, and the result of β-reduction is that a single copy of
the argument is retained, with the function body containing multi-
ple pointers to it. As a consequence any redexes in the argument are
reduced at most once.
Turner adapted this idea to graph reduction on S, K, I, etc. com-
binators, allowing a much simpler abstract machine.
In Turner’s
scheme the graph may be cyclic, permitting a more compact rep-
resentation of recursion. The reduction rule for the Y combinator,
Y H ⇒H (Y H), creates a loop in the graph, increasing the amount
of sharing. The combinators are a target code for a compiler for
9The phrase syntactic sugar is due to Strachey, as are other evocative terms
and concepts in programming language theory.
10Except where prior analysis of the program shows it can be avoided, a process
known as strictness analysis.

530
David Turner
compilation from a high level functional language. Initially this was
SASL [Turner 1976] and in later incarnations of the system, Miranda.
While using a set of combinators ﬁxed in advance is a good solu-
tion if graph reduction is to be carried out by an interpreter, if the
ﬁnal target of compilation is to be native code on conventional hard-
ware it is advantageous to use the λ-abstractions present (explicitly
or implicitly) in the program source as the combinators whose reduc-
tion rules are to be implemented. This requires a source-to-source
transformation called λ-lifting, Hughes [1983], Johnsson [1985]. This
method was ﬁrst used in the compiler of LML, a lazy version of the
functional subset of ML, written by Lennart Augustsson & Thomas
Johnsson at Chalmers University in Sweden, around 1984.
Their
model for mapping graph reduction onto conventional hardware, the
G machine, has since been further reﬁned, leading to the optimized
model of Simon Peyton Jones [1992].
Thus over a period of two decades normal order functional lan-
guages have been implemented with increasing eﬃciency.
Miranda
Miranda is a functional language designed by David Turner in
1983–6 and is a sugaring of a typed λ-calculus with a universal ﬁx-
point operator. There are no explicit λ’s—instead we have function
deﬁnition by equations and local deﬁnitions with where. The insight
that one can have λ-calculus without λ goes back to Peter Landin
[1966] and his ISWIM notation. Neither is the user required to mark
recursive deﬁnitions as such—the compiler analyses the call graph
and inserts Y where it is required.
The use of normal order reduction (aka lazy evaluation) and non-
strict functions has a very pervasive eﬀect. It supports a more math-
ematical style of programming, in which inﬁnite data structures can
be described and used and, which is most important, permits commu-
nicating processes and input/output to be programmed in a purely
functional manner.
Miranda is based on the earlier lazy functional language SASL
[Turner 1976] with the addition of the system of polymorphic strong
typing of Milner [1978].
For an overview of Miranda see Turner
[1986].
Miranda doesn’t use Church numerals for its arithmetic—modern
computers have fast ﬁxed and ﬂoating point arithmetic units and it

Church’s Thesis and Functional Programming
531
would be perverse not to take advantage of them. Arithmetic oper-
ations on unbounded size integers and 64bit ﬂoating point numbers
are provided as primitives.
In place of the second order representation of data used within
the pure untyped lambda calculus we have algebraic type deﬁnitions.
For example
bool ::= False | True
nat ::= Zero | Suc nat
tree ::= Leaf nat | Fork tree tree
Introducing new data types in this way is in fact better than us-
ing second order impredicative deﬁnitions for two reasons: you get
clearer and more speciﬁc type error messages if you misuse them—
and each algebraic type comes with a principle of induction which
can be read oﬀfrom the deﬁnition. The analysis of data is by pat-
tern matching, for example
flatten :: tree -> [nat]
flatten (Leaf n) = [n]
flatten (Fork x y) = flatten x ++ flatten y
The type speciﬁcation of ﬂatten is optional as the compiler is able
to deduce this; ++ is list concatenation.
There is a rich vocabulary of standard functions for list process-
ing, map, ﬁlter, foldl, foldr, etc. and a notation, called list compre-
hension that gives concise expression to a useful class of iterations.
Miranda was widely used for teaching and for about a decade fol-
lowing its initial release by Research Software Ltd in 1985–6 provided
a de facto standard for pure functional programming, being taken
up by over 200 universities. The fact that it was interpreted rather
than compiled limited its use outside education, but several signiﬁ-
cant industrial projects were successfully undertaken using Miranda,
see for example Major et. al. [1991] and Page & Moe [1993].
Haskell, a successor language designed by a committee, includes
many extensions, of which the most important are type classes and
monadic input-output. The language remains purely functional, how-
ever. For a detailed description see S.L. Peyton Jones [2003]. Avail-
able implementations of Haskell include, besides an interpreter suit-
able for educational use, native code compilers. This makes Haskell
a viable choice for production use in a range of areas.

532
David Turner
The fact that people are able to write large programs for serious
applications in a language, like Miranda or Haskell, that is essentially
a sugaring of λ-calculus is in itself a vindication of Church’s Thesis.
Domain Theory
The mathematical theory which explains programming languages
with general recursion is Scott’s domain theory.
The typed λ-calculus looks as though it ought to have a set-
theoretic model, in which types denote sets and λ-abstractions de-
note functions. But the ﬁxpoint operator Y is problematic. It is not
the case in set theory that every function fϵA →A has a ﬁxpoint in
A.
There is second kind of ﬁxpoint to be explained, at the level of
types. We can deﬁne recursive algebraic data types, like (we are here
using Miranda notation):
big ::= Leaf nat | Node (big -> big)
This appears to require a set with the property
Big ∼= N + (Big →Big)
which is impossible on cardinality grounds.
Dana Scott’s domain theory solves both these problems. A do-
main is a complete partial order: a set with a least element, ⊥, rep-
resenting non-termination, and limits of ascending chains (or more
generally of directed sets). The function space A →B for domains
A, B, is deﬁned to contain just the continuous functions from A to
B and this is itself a domain. Continuous means preserving limits.
The continuous functions are also monotonic (= order preserving).
For a complete partial order, D, each monotonic function fϵD →D
has a least ﬁxed point, F∞
n=0 fn⊥.
A plain set, like N can be turned into a domain by adding ⊥,
to get N. Further, domain equations, like D ∼= N + (D × D), D ∼=
N +(D →D) and so on, all have solutions. The details can be found
in Scott [1976] or Abramsky & Jung [1994]. This includes that there
is a non-trivial11 domain D∞with
D∞∼= D∞→D∞
11The one-point domain, with ⊥for its only element, if allowed, would be a
trivial solution.

Church’s Thesis and Functional Programming
533
providing a semantic model for Church’s untyped λ-calculus.
Domain theory was originally developed to underpin denotational
semantics, Christopher Strachey’s project to formalize semantic de-
scriptions of real programming languages using a typed λ-calculus
as the metalanguage [see Strachey, 1967; Strachey & Scott 1971].
Strachey’s semantic equations made frequent use of Y to explain con-
trol structures such as loops and also required recursive type equa-
tions to account for the domains of the various semantic functions. It
was during Scott’s collaboration with Strachey in the period around
1970 that domain theory emerged.
Functional programming in non-strict languages like Miranda
and Haskell is essentially programming directly in the metalanguage
of denotational semantics.
Computability at Higher Types, Revisited
Dana Scott once remarked that λ-calculus is only an algebra,
not a calculus. With domain theory and proofs using limits we get
a genuine calculus, allowing many new results.
Studying a typed functional language with arithmetic, Plotkin
[1977] showed that if we consider functions of higher type where
we allow inputs as well as outputs to be ⊥, there are computable
functions which are not λ-deﬁnable. Using domain B where B =
{True, False}, two examples are:
Or ϵ B →B →B where Or x y is True if either x or y is
True
Exists ϵ (N →B) →B where Exists f is True when
∃iϵN.f i = True
This complete or parallel Or must interleave two computations, since
either of its inputs may be ⊥. Exists is a multi-way generalization.
What we get from untyped λ-calculus, or a typed calculus with
N and general recursion, are the sequential functions.
To get all
computable partial functions at every type we must add primitives
expressing interleaving or concurrency. In fact just the two above
are suﬃcient.
This becomes important for programming with exact real num-
bers, an active area of research. Martin Escardo [1996] shows that
a λ-calculus with a small number of primitives including Exists can

534
David Turner
express every computable function of analysis, including those of
higher type, e.g. diﬀerentiation and integration.
Strong Functional Programming
There is an extended family of typed λ-calculi, all without Y or
any other method of expressing general recursion, in which every
term is normalizing. The family includes
simply typed λ-calculus—this is a family in itself
Girard’s system F [1971], also known as the second order λ-
calculus (we consider here the Church-style or explicitly typed
version)
Coquand & Huet’s calculus of constructions [1988]
Martin–L¨of’s intuitionist theory of types [1973]
In a change of convention we will use upper case letters A, B, C · · ·
for types and lower case letters a, b, c · · · for terms, reserving x, y, z,
for λ-calculus variables (this somewhat makeshift convention will be
adequate for a short discussion).
In addition to the usual conversion and reduction relations, ⇔
, ⇒, these theories have a judgement of well-typing, written a : A
which says that term a has type A (which may or may not be unique).
All the theories share the following properties:
Church–Rosser If a ⇔b there is a term c such that a ⇒c
and b ⇒c.
Decidability of well-typing This what is meant by saying
that a programming language or formalism is strongly typed
(aka staticly typed).
Strongly normalizing Every well-typed term is normalizing
and every reduction sequence terminates in a normal form.
Uniqueness of normal forms Immediate
from
Church–
Rosser.
Decidability of ⇔on well-typed terms From the two pre-
vious properties—reduce both sides to normal form and see if
they are equal.

Church’s Thesis and Functional Programming
535
Note that decidability of the well typing judgment, a : A, is not the
same as type inference. The latter means that given an a we can ﬁnd
an A with a : A, or determine that there isn’t one. The simply typed
λ-calculus has type inference (in fact with most general types) but
none of the stronger theories do.
The ﬁrst two properties in the list are shared with other well-
behaved typed functional calculi, including those with general recur-
sion. So the distinguishing property here is strong normalization.
Programming in a language of this kind has important diﬀerences
from the more familiar kind of functional programming. For want of
any hitherto agreed name, we can call it strong functional program-
ming.12
An obvious diﬀerence is that all evaluations terminate13, so we
do not have to worry about ⊥. It is clear that such a language can-
not be computationally complete—there will be always-terminating
computable functions it cannot express (and one of these will be the
interpreter for the language itself). It should not be inferred that
a strongly normalizing language must therefore be computationally
weak. Even simple typed lambda calculus, equipped with N as a
base type and primitive recursion, can express every recursive func-
tion of arithmetic whose totality is provable in ﬁrst order number
theory (a result due to G¨odel [1958]). A proposed elementary func-
tional programming system along these lines, but including codata
as well as data, is discussed in Turner [2004].
A less obvious but most striking consequence of strongly normal-
ization is a new and unexpected interface between λ-calculus and
logic. We show how this works by considering the simplest calculus
of this class.
Propositions-as-Types
The simply typed λ-calculus (STLC) has for its types the closure
under →of a set of base types, which we will leave unspeciﬁed. As
12Another possible term is “total functional programming”, although this has
the disadvantage of encouraging the unfortunate term “total function” (redun-
dant because it is part of the deﬁnition function that it is everywhere deﬁned on
its domain).
13This seems to rule out indeﬁnitely proceeding processes, such as an operating
system, but we can include these by allowing codata and corecursion, see eg
Turner [2004].

536
David Turner
before we use A, B, C · · · as variables ranging over types. We can
associate with each closed term a type schema, for example
λx.x : A →A
The function λx.x has many types but they are all instances of A →
A, which is its most general type.
A congruence ﬁrst noticed by Curry in the 1950’s is that the
types of closed terms in STLC correspond to tautologies of intuition-
ist propositional logic, if we read →as implication, e.g. A →A is a
tautology. The correspondence is exact, for example A →B is not a
tautology and neither can we make any closed term of this type. Fur-
ther, the most general types of the combinators s = λx.λy.λz.xz(yz)
and k = λx.λy.x are
s : ((A →(B →C)) →((A →B) →(A →C))
k : A →(B →A)
and these formulae are the two standard axioms for the intuitionist
theory of implication: every other tautology in →can be derived
from them by modus ponens. What is going on here?
Let us look at the rules for forming well-typed terms of simply
typed λ.
(x : A)
c : A →B
b : B
a : A
λx.b : A →B
c a : B
On the left14 we have the rule for abstraction, on the right that
for application. If we look only at the types and ignore the terms,
these are the introduction and elimination rules for implication in a
natural deduction system. So naturally, the formulae we can derive
using these rules are all and only the tautologies of the intuitionist
theory of implication.15
14The left hand rule says that if from assumption x : A we can derive b : B
then we can derive what is under the line.
15The classical theory of implication includes additional tautologies dependant
on the law of the excluded middle—the leading example is ((A →B) →A) →A,
Pierce’s law.

Church’s Thesis and Functional Programming
537
In the logical reading, the terms on the left of the colons provide
witnessing information—they record how the formula on the right
was proved. The judgement a : A thus has two readings—that term a
has type A, but also that proof-object or witness a proves proposition
A.
The correspondence readily extends to the other connectives of
propositional logic by adding some more type constructors to SLTC
besides →. The type of pairs, cartesian product, A×B, corresponds
to the conjunction A∧B. The disjoint union type, A⊕B, corresponds
to the disjunction A∨B. The empty type corresponds to the absurd
(or False) proposition, which has no proof.
This Curry–Howard isomorphism between types and proposi-
tions is jointly attributed to Curry [1958] and to W. Howard [1969],
who showed how it extended to all the connectives of intuitionist
logic including the quantiﬁers. It is at the same time an isomor-
phism between terminating programs and constructive (or intuition-
istic) proofs.
The Constructive Theory of Types
Per Martin–L¨of [1973] formalizes a proposed foundational lan-
guage for constructive mathematics based on the isomorphism. The
Intuitionist (or Constructive) Theory of Types is at one and the same
time a higher order logic and a theory of types, providing for con-
structive mathematics what for classical mathematics is done by set
theory. It provides a uniﬁed notation in which to write functions,
types, propositions and proofs.
Unlike the constructive set theory of Myhill [1975], Martin–L¨of
type theory includes a principle of choice (not as an axiom, it is
provable within the theory). It seems that the source of the non-
constructivities of set theory is not the choice principle, which for
Martin–L¨of is constructively valid, but the axiom of separation, a
principle which is noticeably absent from type theory.16 17
Constructive type theory is both a theory of constructive mathe-
matics and a strongly typed functional programming language. Ver-
16Note that Goodman & Myhill’s [1978] proof that Choice implies Excluded
Middle makes use of an instance of the Axiom of Separation. The title should be
Choice + Separation implies Excluded Middle.
17The frequent proposals to “improve” CTT by adding a subtyping constructor
should therefore be viewed with suspicion.

538
David Turner
ifying the validity of proofs is the same process as type checking.
Martin–L¨of [1982] writes
I do not think that the search for high level programming
languages that are more and more satisfactory from a logical
point of view can stop short of anything but a language in
which all of constructive mathematics can be expressed.
There exist by now a number of diﬀerent versions of the theory,
including several computer-based implementations, of which perhaps
the longest established is NuPRL [Constable et al. 1986].
An alternative impredicative theory, also based on the Curry–
Howard isomorphism, is Coquand and Huet’s Calculus of Construc-
tions [1988] which provides the basis for the COQ proof system de-
veloped at INRIA.
Type Theory with Partial Types
Being strongly normalizing, constructive type theory cannot be
computationally complete. Moreover we might like to reason about
partial functions and general recursion using this powerful logic. Is
it possible to somehow unify type theory with a constructive version
of Dana Scott’s domain theory?
In his PhD thesis Scott F. Smith [1989] investigated adding par-
tial types to the type theory of NuPRL. The idea can be sketched
brieﬂy as follows. For each ordinary type T there is a partial type T
of T-computations, whose elements include those of T and a diver-
gent element, ⊥. For partial types [only] there is a ﬁxpoint operator,
fix : (T →T) →T. This allows the deﬁnition of general recursive
functions.
The constructive account of partial types is signiﬁcantly diﬀerent
from the classical account given by domain theory. For example we
cannot assert
∀x : T. x ϵ T ∨x = ⊥
because constructively this implies an eﬀective solution to the halt-
ing problem for T. A number of intriguing theorems emerge. Cer-
tain non-computability results can be established absolutely, that is
independently of Church’s Thesis, see Constable & Smith [1988].18
18The paper misleadingly claims that among these is the Halting Theorem,
which would be remarkable. What is in fact proved is the extensional halting

Church’s Thesis and Functional Programming
539
Further, the logic of the host type theory is altered so that it is no
longer compatible with classical logic—some instances of the law of
the excluded middle, of the form ∀x.P(x) ∨¬P(x) can be disproved.
To recapture domain theory requires something more than T and
fix, namely a second order ﬁxpoint operator, FIX, that solves re-
cursive equations in partial types. As far as the present author is
aware, noone has yet shown how to do this within the logic of type
theory. This would unify the two theories of functional program-
ming. Among other beneﬁts it would allow us to give within type
theory a constructive account of the denotational semantics of recur-
sive programming languages.
Almost certainly relevant here is Paul Taylor’s Abstract Stone
Duality [2002], a computational approach to topology. The simplest
partial type is Sierpiński space, Σ, which has only one point other
than ⊥. This plays a special role in Taylor’s theory: the open sets
of a space X are the functions in X →Σ and can be written as
λ-terms. ASD embraces both traditional spaces like the reals and
Scott domains (topologically these are non-Hausdorﬀspaces).
Conclusion
Church’s Thesis played a founding role in computing theory by
providing a single notion of eﬀective computability. Without this
foundation we might have been stuck with a plethora of notions
of computability depending on computer architecture, programming
language etc.: we might have Motorola-computable versus Intel-
computable, Java-computable versus C-computable and so on.
The λ-calculus, which Church developed during the period of
convergence from which the Thesis emerged, has inﬂuenced almost
every aspect of the development of programming and programming
languages. It is the basis of functional programming, which after a
long infancy is entering adulthood as a practical alternative to tradi-
tional ad-hoc imperative programming languages. Many important
ideas in mainstream programming languages—recursion, procedures
as parameters, linked lists and trees, garbage collectors—came by
cross fertilization from functional programming. Moreover the main
theorem, which is already provable in domain theory, trivially from monotonicity.
The real Halting Theorem is intensional, in that the halting function whose ex-
istence is to be disproved is allowed access to the internal structure of the term,
by being given its G¨odel number.

540
David Turner
schools of both operational and denotational semantics are λ-calculus
based and amount to using functional programming to explain other
programming systems.
The original project from whose wreckage by paradox λ-calculus
survived, to unify logic with an account of computable functions, ap-
pears to have been reborn in unexpected form, via the propositions-
as-types paradigm. Further exciting developments undoubtedly lie
ahead and ideas from Church’s λ-calculus will continue to be central
to them.
References
Abramsky, S. and Jung, A. [1994], “Domain Theory”, in Handbook
of Logic in Computer Science, vol. III, OUP. (S. Abramsky,
D.M. Gabbay, and T. Maibaum eds.),
Barendregt, H.P. [1984], The Lambda Calculus: Its Syntax and
Semantics, North-Holland.
Church, A. [1936], “An Unsolvable Problem of Elementary Number
Theory”, American Journal of Mathematics 58, 345–363.
Church, A. [1937], Review of A.M. Turing [1936], “On computable
numbers...”, Journal of Symbolic Logic 2(1), 42–43, (March).
Church, A. [1941], The Calculi of Lambda Conversion, Princeton
University Press.
Constable, R.L., et al. [1986], Implementing Mathematics with the
Nuprl Proof Development System, Prentice Hall.
Constable, R.L. and Smith, S.F. [1988], “Computational
Foundations of Basic Recursive Function Theory”, Proceedings
3rd IEEE Symposium on Logic in Computer Science,
pp. 360–371, (also Cornell Dept CS, TR 88–904), March; this
and other papers of the NuPRL group can be found at
<http://www.nuprl.org/>.
Coquand, T. and Huet, G. [1988], “The Calculus of Constructions”,
Information and Computation 76, 95–120.
Curry, H.B. and Feys, R. [1958], Combinatory Logic, vol. I,
North-Holland, Amsterdam.
Escardo, M.H. [1996], “Real PCF Extended with Existential is
Universal”, Proceedings 3rd Workshop on Theory and Formal
Methods, (A. Edalat, S. Jourdan, G. McCusker eds.), IC

Church’s Thesis and Functional Programming
541
Press, 13–24, April; this and other papers of Escardo can be
found at <http://www.cs.bham.ac.uk/∼mhe/papers/>.
Girard, J.-Y. [1971], “Une extension de l’interpretation
fonctionnelle de G¨odel a l’analyse et son application a
l’elimination des coupures dans l’analyse et la theorie des
types”, Proceedings 2nd Scandinavian Logic Symposium,
(J.F. Fenstad ed.), North-Holland 1971. pp. 63–92; a modern
treatment of System F can be found in Proofs and Types,
(J.-Y. Girard, Y. Lafont, and P. Taylor eds.), Cambridge
University Press, 1989.
G¨odel, K. [1965], “On Undecidable Propositions of Formal
Mathematical Systems”, 1934 Lecture notes taken by Kleene
and Rosser at the Institute for Advanced Study; reprinted in
The Undecidable, (M. Davis ed.), Raven, New York 1965.
G¨odel, K. [1958], “On a hitherto unutilized extension of the ﬁnitary
standpoint”, Dialectica 12, 280–287.
Goodman, N.D. and Myhill, J. [1978], “Choice Implies Excluded
Middle”, Zeit. Logik und Grundlagen der Math 24, 461.
Herbrand, J. [1932], “Sur la non-contradiction de l’arithmetique”,
Journal fur die reine und angewandte Mathematik 166, 1–8.
Hodges, A. [this collection], “Did Church and Turing have a Thesis
about Machines?”.
Hughes, J. [1984], “The Design and Implementation of
Programming Languages”, D. Phil. Thesis, University of
Oxford, 1983; Published by Oxford University Computing
Laboratory Programming Research Group, as Technical
Monograph PRG-40, September.
Howard, W. [1969], “The Formulae as Types Notion of
Construction”, privately circulated letter, published in To
H.B. Curry, Essays on Combinatory Logic, Lambda Calculus
and Formalism, (Seldin and Hindley eds.), Academic Press
1980.
Johnsson, T. [1985], “Lambda Lifting: Transforming Programs to
Recursive Equations”, Proceedings IFIP Conference on
Functional Programming Languages and Computer
Architecture, Nancy, France, Sept. 1985, Springer LNCS 201.
Kleene, S.C. [1936], “Lambda-Deﬁnability and Recursiveness”,
Duke Mathematical Journal 2, 340–353.

542
David Turner
Landin, P.J. [1966], “The Next 700 Programming Languages”,
CACM 9(3), 157–165, March.
McCarthy, J. [1960], “Recursive Functions of Symbolic Expressions
and their Computation by Machine”, CACM 3(4), 184–195.
Major, F., Turcotte, M., et al. [1991], “The Combination of
Symbolic and Numerical Computation for Three-Dimensional
Modelling of RNA”, SCIENCE 253, 1255–1260, September.
Martin–L¨of, P. [1975], “An Intuitionist Theory of
Types—Predicative Part”, in Logic Colloquium 1973, (Rose
and Shepherdson eds.), North Holland 1975.
Martin–L¨of, P. [1982], “Constructive Mathematics and Computer
Programming”, Proceedings of the Sixth International
Congress for Logic, Methodology and Philosophy of Science,
(Cohen, Los, Pfeiﬀer, and Podewski eds.), North Holland,
pp. 153–175; also in Mathematical Logic and Programming
Languages, (Hoare and Shepherdson eds.), Prentice Hall 1985,
pp. 167–184.
Milner, R. [1978], “A Theory of Type Polymorphism in
Programming”, Journal of Computer and System Sciences
17(3), 348–375.
Myhill, J. [1975], “Constructive Set Theory”, Journal of Symbolic
Logic 40(3), 347–382, September.
Page, R.L. and Moe, B.D. [1993], “Experience with a Large
Scientiﬁc Application in a Functional Language”, in
Proceedings ACM Conference on Functional Programming
Languages and Computer Architecture, Copenhangen, June
1993.
Peyton Jones, S.L. [1992], “Implementing Lazy Functional
Languages on Stock Hardware: the Spineless Tagless
G-Machine”, Journal of Functional Programming 2(2),
127–202, April.
Peyton Jones, S.L. [2003], Haskell 98 Language and Libraries: the
Revised Report, Cambridge University Press; also published in
Journal of Functional Programming, 13(1), January. This and
other information about Haskell can be found at
<http://haskell.org/>.
Plotkin, G. [1977], “LCF Considered as a Programming Language”,
Theoretical Computer Science 5(1), 233–255.

Church’s Thesis and Functional Programming
543
Sch¨onﬁnkel, M. [1924], “¨Uber die Bausteine der mathematischen
Logik”, translated as “On the Building Blocks of
Mathematical Logic”, in Heijenoort, From Frege to G¨odel—a
Source Book in Mathematical Logic 1879–1931, Harvard 1967.
Scott, D. [1976], “Data Types as Lattices”, SIAM Journal on
Computing 5(3), 522–587.
Smith, S.F. [1989], “Partial Objects in Type Theory”, Cornell
University, Ph.D. Thesis.
Strachey, C. [2000], “Fundamental Concepts in Programming
Languages”, originally notes for an International Summer
School on computer programming, Copenhagen, August 1967,
published in Higher-Order and Symbolic Computation, vol. 13,
Issue 1/2, April 2000—this entire issue is dedicated in memory
of Strachey.
Scott, D. and Strachey, C. [1971], “Toward a Mathematical
Semantics for Computer Languages”, Oxford University
Programming Research Group Technical Monograph PRG-6,
April.
Taylor, P. [2002], “Abstract Stone Duality”, privately circulated,
2002—this and published papers about ASD can be found at
<http://www.cs.man.ac.uk/∼pt/ASD/>.
Turing, A.M. [1937], “On Computable Numbers with an
Application to the Entscheidungsproblem”, Proceedings London
Mathematical Society, Series 2 42, 230–265; correction
43(1937), 544–546.
Turner, D.A. [1976], “SASL Language Manual”, St. Andrews
University, Department of Computational Science Technical
Report, 43 pages, December.
Turner, D.A. [1979a], “A New Implementation Technique for
Applicative Languages”, Software-Practice and Experience
9(1), 31–49, January.
Turner, D.A. [1979b], “Another Algorithm for Bracket
Abstraction”, Journal of Symbolic Logic 44(2), 267–270, June.
Turner, D.A. [1986], “An Overview of Miranda”, SIGPLAN Notices
21(12), 158–166, December; this and other information about
Miranda† can be found at <http://miranda.org.uk>.
Turner, D.A. [2004], “Total Functional Programming”, Journal of
Universal Computer Science 10(7), 751–768, July.

544
David Turner
Wadsworth, C.P. [1971], “The Semantics and Pragmatics of the
Lambda Calculus”, D. Phil. Thesis, Oxford University
Programming Research Group.
† Miranda is a trademark of Research Software Limited.

