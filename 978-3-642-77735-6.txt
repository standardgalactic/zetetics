
EATCS 
Monographs on Theoretical Computer Science 
Editors: W. Brauer G. Rozenberg A. Salomaa 
Advisory Board: G.Ausiello M.Broy S.Even 
J. Hartmanis N. Jones T. Leighton M. Nivat 
C. Papadimitriou D. Scott 

Osamu Watanabe (Ed.) 
Kolmogorov Complexity 
and 
Computational Complexity 
Springer-Verlag 
Berlin Heidelberg New York 
London Paris Tokyo 
Hong Kong Barcelona 
Budapest 

Volume Editor 
Prof Dr. Osamu Watanabe 
Department of Computer Science 
Tokyo Institute of Technology 
Meguro-ku, Ookayama 
Tokyo 152, Japan 
Editors 
Prof Dr. Wilfried Brauer 
Institut fur Informatik, Technische Universitiit Miinchen 
Arcisstrasse 21, W-8000 Miinchen 2 
Prof Dr. Grzegorz Rozenberg 
Institute of Applied Mathematics and Computer Science 
University of Leiden, Niels-Bohr-Weg 1, P. o. Box 9512 
2300 RA Leiden, The Netherlands 
Prof Dr. Arto Salomaa 
The Academy of Finland 
Department of Mathematics, University ofTurku 
SF-20500 Turku, Finland 
ISBN-13: 978-3-642-77737-0 
e-ISBN-13: 978-3-642-77735-6 
DOl: 10.1007/978-3-642-77735-6 
Library of Congress Cataloging-in-Publication Data 
Watanabe, Osamu, 1958-
Kolmogorov complexity and Computational Complexity / 
Osamu Watanabe. p. 
c. - (EATCS monographs on theoretical computer science) "In 
March 1990, the Symposium on Theory and Application of Minimal Length Encoding was 
held at Stanford University as part of the AAAI 1990 spring symposium series" - Galley. 
Includes bibliographical references and index. 
ISBN 0-387-55840-3 (N.y.) 
I. Kolmogorov complexity - Congresses. 2. Computational complexity - Congresses. 1. Title. 
II. Series. QA267.7.w38 1992 
511.3 - dc20 92-26373 
This work is subject to copyright. All rights are reserved, whether the whole or part of the 
material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, 
recitation, broadcasting, reproduction on microfilm or in any other way, and storage in data 
banks. Duplication of this publication or parts thereofis permitted only under the provisions 
of the German Copyright Law of September 9,1965, in its current version, and a permission 
for use must always be obtained from Springer-Verlag. Violations are liable for prosecution 
under the German Copyright Law. 
© Springer-Verlag Berlin Heidelberg 1992 
Sofkover reprint of the hardcover 1 st edition 1992 
The use of general descriptive names, registered names, trademarks, etc. in this publication 
does not imply, even in the absence of a specific statement, that such names are exempt from 
the relevant protective laws and regulations and therefore free for general use. 
Typesetting: Camera ready by editor using the 1EX macro package from Springer-Verlag. 
45/3140-543210 - Printed on acid-free paper 

Preface 
The mathematical theory of computation has given rise to two important ap-
proaches to the informal notion of "complexity": Kolmogorov complexity, usu-
ally a complexity measure for a single object such as a string, a sequence etc., 
measures the amount of information necessary to describe the object. Compu-
tational complexity, usually a complexity measure for a set of objects, measures 
the compuational resources necessary to recognize or produce elements of the 
set. The relation between these two complexity measures has been considered 
for more than two decades, and may interesting and deep observations have 
been obtained. 
In March 1990, the Symposium on Theory and Application of Minimal-
Length Encoding was held at Stanford University as a part of the AAAI 1990 
Spring Symposium Series. Some sessions of the symposium were dedicated to 
Kolmogorov complexity and its relations to the computational complexity the-
ory, and excellent expository talks were given there. Feeling that, due to the 
importance of the material, some way should be found to share these talks 
with researchers in the computer science community, I asked the speakers of 
those sessions to write survey papers based on their talks in the symposium. In 
response, five speakers from the sessions contributed the papers which appear 
in this book. 
In this book, the main topic is Kolmogorov complexity and its relations to 
the structure of complexity classes. As I explain in the Introduction, each paper 
discusses a different type of Kolmogorov complexity, and each paper uses a dif-
ferent viewpoint in developing a relationship between Kolmogorov complexity 
and computational complexity. Thus, this book provides a good overview of 
current research on Kolmogorov complexity in structural complexity theory. 
I whish to thank Dr. Edwin Pednault, the Chair of the Symposium, for 
having organized the interesting sessions from which this book originated. Each 
paper was reviewed by some outside reviewer as well as by fellow authors. I 
would like to thank the outside reviewers, Professor Jose Balcazar, Professor 
Kojiro Kobayashi, and Professor Keri Ko, for their constructive comments. 
Osamu Watanabe 
May 1992 

Contents 
Introduction 
Osamu Watanabe 
Applications of Time-Bounded Kolmogorov 
Complexity in Complexity Theory 
Eric Allender 
On Sets with Small Information Content 
Ronald V. Book 
Kolmogorov Complexity, Complexity Cores, 
and the Distribution of Hardness 
David W. Juedes and Jack H. Lutz 
Resource Bounded Kolmogorov Complexity 
and Statistical Tests 
Luc Longpre 
Complexity and Entropy: An Introduction 
to the Theory of Kolmogorov Complexity 
Vladimir A. Uspensky 
Subject Index 
1 
4 
23 
43 
66 
85 
103 

Introduction 
Osamu Watanabe 
Department of Computer Science 
Tokyo Institute of Technology 
Meguro-ku, Ookayama, Tokyo 152, Japan 
watanabe@cs.titech.ac.jp 
For a given string, it is often asked how much information is encoded in the string. 
There is an excellent mathematical framework for discussing this type of ques-
tion, namely, that of Kolmogorov complexity theory. The theory was established 
through independent works by R.J. 80lmonoff [80164], A.N. Kolmogorov [Ko165], 
and G.J. Chaitin [Cha69], and it has been shown to be an important subject in 
both mathematics and computer science1 . In particular, there has been consid-
erable research recently in the interface between Kolmogorov complexity theory 
and computational complexity theory, and this research has yielded many inter-
esting results as well as proof techniques. In this book, we study Kolmogorov 
complexity theory while focusing on its relation to the study of structural prop-
erties of complexity classes such as P and NP. Here we give a brief overview of 
the chapters in this book. 
We begin our tour by explaining the several versions of Kolmogorov com-
plexity considered in the various chapters. 
Intuitively, Kolmogorov complexity measures the amount of information nec-
essary to describe a given string. More precisely, for a string x, the Kolmogorm1 
complexity of x is the size of the shortest string y from which a certain fixed 
program (i.e., the universal Turing machine) produces x. Here we refer to this 
measure as resou'rce-unbounded Kolmogorov complexity. While this complexity 
measure is the most standard one, there are many variants of it that have been 
introduced and used in some applications or investigations. In fact, the reader 
will find a different variation on the notion of Kolmogorov complexity (or, more 
generally, "descriptive complexity") in each chapter in this book. These varia-
tions can be categorized in the following way: 
{
resource-unbounded K-complexity 
a string 
resource-bounded K-complexity of { 
{ (elements) 
(1) 
a set 
(char. seq.) 
(2) 
. 
(recognizer size) (3) 
Taxonomy of Variants of Kolmogorov Complexity 
1 For the history of Kolmogorov complexity theory and various explorations in this 
theory, the reader is advised to read a thorough survey paper [LV90] by M. Li and 
P. Vitanyi as well as a forthcoming book [LV] by the same authors. 

2 
Osamu Watanabe 
In relation to computational complexity theory, resource-bounded Kolmogorov 
complexities are frequently considered. For example, the t(n)-time bounded Kol-
mogorov complexity of a string x is the size of the shortest string y from which 
the universal Turing machine produces x in t(lxl) steps. The space bounded 
Kolmogorov complexity of x is defined similarly. 
In computational complexity theory, one does not usually discuss the com-
plexity of a single string. Instead, we consider the complexity of recognizing a 
set of strings, i.e., the complexity of deciding whether a string belongs to the 
set. Thus, the "Kolmogorov complexity of a set" is often studied in relation to 
computational complexity. Since Kolmogorov complexity was originally defined 
for a string, there is no standard definition for "Kolmogorov complexity of a 
set." In this book, the Kolmogorov complexity of a set is defined by using either 
(1) the largest/smallest Kolmogorov complexity of elements of the set, (2) the 
Kolmogorov complexity of the characteristic sequence of the set, or (3) the size 
of programs or logical circuits recognizing the set2 • 
Once formal definitions are given, it is not difficult to see how these different 
versions of Kolmogorov complexity measures are related each other; this task is 
left to the interested reader. But note that some of these relations are explained 
explicitly in the appropriate chapters. 
Now we quickly go through each chapter and explain which type of Kol-
mogorov complexity is used and how it is investigated. 
E. Allender investigates the KL complexity of sets L in relatively low com-
plexity classes such as P, where KL (n) is the smallest time bounded Kolmogorov 
complexity of strings in L=n; that is, the type (1) of Kolmogorov complexity of 
sets is considered. He shows connections between several open problems in com-
putational complexity theory and upper and lower bounds of the KL complexity 
for sets L in P and P / poly. 
R. Book discusses the computational complexity of sets with small informa-
tion content. By a "set with small information content", he means (i) a set of 
strings whose polynomial-time bounded Kolmogorov complexity is low, and (ii) 
a set of strings with polynomial-size circuits. Thus, the types (2) and (3) of Kol-
mogorov complexities of sets are considered. In this chapter, the computational 
complexity of a given set is measured in terms of the power of the set when 
it is used as an oracle set. He sketches a variety of results that can be viewed 
as evidence that sets with small information content are also computationally 
"easy". 
On the other hand, D. Juedes and J. Lutz explain their recent results that 
witness computationally hard sets have a highly organized structure and there-
fore have relatively low Kolmogorov complexity. In that chapter, the type (2) 
of Kolmogorov complexity of sets that is considered, and ESPACE (the class of 
languages recognized by Turing machines that use O(2cn) work space for some 
constant c, where n is the length of the input) is investigated. They first show 
2 This third one is so far away from the original notion of Kolmogorov complexity 
that it may be bit awkward if we use the term "Kolmogorov complexity." But this 
is certainly one type of definition of "descriptive complexity" of a set. 

Introduction 
3 
that almost every ESPACE set has very high Kolmogorov complexity, and then 
show that every ESPACE hard set has relatively low Kolmogorov complexity. 
The "simplicity" of ESPACE hard sets is also shown by considering the size of 
their complexity cores. 
The last two chapters are about the Kolmogorov complexity of strings. L. 
Longpre considers the time- and space-bounded Kohnogorov complexities, and 
discusses the statistical properties of Kolmogorov-random strings, i.e., strings 
with the highest Kolmogorov complexity. He investigates the question of whether 
Martin-LoCs theorem, showing that Kolmogorov-random strings possess all the 
statistical properties of random strings, can be extended so that it holds in 
resource-bounded cases. He also relates the notion of Kolmogorov-random se-
quences to Yao's definition of pseudorandom sequences. 
Finally, V. Uspensky surveys resource-unbounded Kohllogorov complexity 
measures (or entropies, as they are often called) that have been introduced in the 
literature. We should note that one can define several Kolmogorov complexity 
measures even in the context of resource-unbounded Kolmogorov complexity 
although their difference vanishes in many cases where the additive O(logn) 
factor is ignored. In the pioneering papers of Kolmogorov complexity theory, 
including Kolmogorov's paper, five basic entropies have been introduced from 
somewhat different motivations. He explains those entropies from a uniform view 
point, thereby giving clear understanding their definitions and relations. 
References 
[Cha69] 
[Ko165] 
[LV90] 
[LV] 
[So164] 
G.J. Chaitin. On the length of programs for computing finite binary se-
quences: statistical considerations. 1. Assoc. Compo Mach. 16:145-159, 
1969. 
A.N. Kolmogorov. Three approaches to the quantitative definition of infor-
mation. Problems in Information Transmission 1:1-7, 1965. 
M. Li and P.M.B. Vitanyi. Kolmogorov complexity and its applications. In 
Handbook of Theoretical Computer Science, J. van Leeuwen (ed.), Elsevier, 
189-254, 1990. 
M. Li and P.M.B. Vitanyi. An Introduction to Kolmogorov Complexity and 
Its Applications, to appear. 
R.J. Solmonoff. A formal theory of inductive inference, Part 1 and Part 2. 
Information and Control 7:1-2, 22-254, 1964. 

Applications of Time-Bounded Kolmogorov 
Complexity in Complexity Theory* 
Eric Allender 
Department of Computer Science 
Rutgers University 
New Brunswick, NJ 08903, USA 
allender@cs.rutgers.edu 
Abstract. This paper presents one method of using time-bounded Kol-
mogorov complexity as a measure of the complexity of sets, and outlines 
a number of applications of this approach to different questions in com-
plexity theory. Connections will be drawn among the following topics: 
NE predicates, ranking functions, pseudorandom generators, and hier-
archy theorems in circuit complexity. 
1 Introduction 
Complexity theory provides a setting in which one can associate to any recursive 
set L a function tL on the natural numbers, and with justification claim that 
tL is a measure of the complexity of L; namely L can be accepted by exactly 
those machines that run in time Q(tLCn)). In this paper, we will consider a 
means of using time-bounded Kolmogorov complexity to define a function K L , 
th~i measures a different aspect of the complexity of L. We will argue that this 
is a useful measure by presenting a number of applications of this measure to 
questions in complexity theory. 
1.1 Complexity of Strings 
Before going any further, it is necessary to define the sort of time-bounded Kol-
mogorov complexity that we will be considering. Many alternate approaches 
exist for adding a time-complexity component to Kolmogorov complexity. Sipser 
[Sip83] and Ko [K086] proposed essentially identical definitions, allowing one to 
define, for each function f, a fen) time-bounded Kolmogorov complexity mea-
sure K f, where K f (x) is the length of the shortest description of x from which x 
can be produced in f(lxl) steps. A related (and much more influential) definition 
due to Hartmanis [Har83] yields sets of the form K[g(n),G(n)], consisting of all 
strings x that can be produced from a description of length gClxl) in time G(lxi). 
Pointers to other approaches to time-bounded Kolmogorov complexity may be 
found in [A1l89a, LV90]. 
* Preparation of this paper was supported in part by the National Science Foundation 
under Grant CCR-9000045. 

Time-Bounded Kolmogorov Complexity in Complexity Theory 
5 
The variants of time-bounded Kolmogorov complexity mentioned in the pre-
ceding paragraph all suffer from certain drawbacks. For example, the definitions 
of Ko and Sipser provide a family of measures K f but offer no guidance in select-
ing anyone function f as the prefered choice when defining the time-bounded 
Kolmogorov complexity of a string x. Additionally, for any given f(n) ~ n, the 
measure Kf assigns the same complexity to a string x, regardless of whether it 
can be built from a short description in linear time or requires time f(lxi), and 
thus some important distinctions can not be made. The definition of Hartmanis 
does allow many fine distinctions to be made, but does not provide a function 
measuring the complexity of a string x; the time and length parameters are not 
combined in any way. 
Thus we turn to another version of time-bounded Kolmogorov complexity: a 
definition due to Levin [Lev84] (see also [Lev73]). 
Definition 1. [Lev84] For any strings x and z, and for any Turing machine M v , 
define Ktv(xlz) to be 
min{IYI + logt : Mv(Y, z) = x in at most t steps}. 
Ktv(x) is defined to be Ktv(xl>-), where >- denotes the empty string. Via a 
standard argument, one can show the existence of a "universal" Turing ma-
chine2 Mu such that, for all v there exists a c such that for all x Ktv(x) 2: 
Ktu(x) + c + loglogKtv(x). Choose some such universal Turing machine, and 
define Kt(xlz) to be Ktu(xlz), and Kt(x) = Ktu(x). 
It is clear that Levin's definition overcomes the objections raised above. How-
ever, it may be less clear that Levin's definition is the appropriate definition -
or even a reasonable one. 
What is the motivation for defining the complexity of x to be the minimum 
of the sum of the description length and the log of the time required to build x 
from that description? The answer to this question is that this is precisely the 
combination of time and description length that is most useful in the study of 
problems such as the P versus NP question. Consider the problem of finding a 
satisfying assignment for a formula ¢ with n variables. When searching through 
all the 2n possible assignments to the variables of ¢, what is the optimal search 
strategy that will lead to a satisfying assignment as quickly as any? The answer, 
as noted by Levin [Lev73] is to consider each string z E En in order of increasing 
Kt(zl¢). Levin also used this approach to provide bounds on "speed-up" (in the 
sense of Blum's speed-up theorem [Blu67]) possible for the problem of inverting 
a polynomial-time computable permutation. 
Levin's Kt function is clearly closely related to the generalized Kolmogorov 
complexity sets defined by Hartmanis: 
2 Levin actually defines Kt-complexity using a different model of computation, al-
lowing the log log term to be eliminated; for simplicity, we will stick to the Turing 
machine model of computation in this paper, as the log log terms are insignificant 
for our purposes. 

6 
Eric Allender 
Proposition2. Kt(x) ::; s(lxl) '* X E K[s(n), 2s(n)] '* Kt(x)::; 2s(lxl). 
As mentioned above, Hartmanis' formulation has the advantage that one is 
able to discuss separately the size of a string's description and the time required 
to build the string; thus some finer distinctions can be made. However, one of 
our goals in this section is to define a measure of the complexity of a language, 
and for this purpose Levin's Kt function combines the time and size components 
in the most appropriate fashion. 
1.2 Complexity of Languages 
Now that we have settled on a measure of the time-bounded Kolmogorov com-
plexity of strings, let us consider how to define a complexity measure for lan-
guages. 
Perhaps the most obvious way to use Kolmogorov complexity to measure the 
complexity of a language L is to consider the characteristic sequence of L: the 
sequence aI, a2, .•• where ai is zero or one, according to whether or not Xi E L, 
where Xl, X2, ... is an enumeration of E*. InvestigatiotJ.s of this sort may be found 
in [K086, Huy85, Huy86, BDG87, MS90, Lut91]. For example, in [BDG87]' it 
was shown that PSPACE/poly is the class of all languages L such that each finite 
prefix of the characteristic sequence of L has small space-bounded Kolmogorov 
complexity. 
It is often useful, however, to consider the complexity of the individual strings 
in a language L, as opposed to the characteristic sequence of L. This leads us to 
the following definitions [AIl89a]. 
Definition3. Let L ~ {O,l}*. Then we define: 
-
KL(n) = min{Kt(x) : x E L=n} 
-
KL(n) = max{Kt(x) : X E L=n} 
If there are no strings of length n in L, then KL (n) and K L (n) are both un-
defined. When we consider the rate of growth of functions of the form KL(n), 
the undefined values are not taken into consideration. Thus, for example, we say 
KL (n) = O(log n) if there is some constant e such that, for all large n, if KL (n) 
is defined, then KL(n) < clogn. Similarly, KL(n) :f= w(s(n)) if there is some 
constant e such that, for infinitely many n, KL(n) is defined and KL(n) ::; es(n). 
If, for some language L, the function K L has a slow rate of growth, then this 
says that all of the strings in L have small time-bounded Kolmogorov complex-
ity. In particular, K L (n) = O(log n) if and only if L ~ K[k log n, n k] for some k. 
Sets with this property have been studied extensively in recent years; the inter-
ested reader will find material concerning these sets, along with pointers to the 
relevant literature, in the survey article by R. V. Book [Bo092]. Because of this, 
we will not dwell on the KL measure, and will focus instead on the KL measure 
throughout the rest of this paper. 
It is immediate from the definition that for any language L, KLC n) ::; n + 
logn + 0(1), and K~(n) = Q(1ogn). The question of how quickly KL(n) may 

Time-Bounded Kolmogorov Complexity in Complexity Theory 
7 
grow, when L is a set in P, turns out to have many connections to a variety of 
questions in complexity theory, and the rest of this paper is devoted to exploring 
some of those connections, beginning with questions concerning deterministic 
and nondeterministic exponential time. 
2 NE Predicates 
Let the complexity classes DTIME(20(n») and NTIME(20(n») be denoted by E 
and NE, respectively. These are simply the exponential-time analogs of P and 
NP, and the E=NE question is generally considered to be of essentially the same 
level of difficulty as the famous P=NP question. 
Of course, much of the motivation for the complexity class NP comes from 
so-called "search" problems: for example, the problem of searching for a Hamil-
tonian path in a graph G, or the problem of producing a satisfying assignment 
for a Boolean formula if> if one exists. In contrast to search problems, language 
recognition problems in NP are questions about the existence of solutions. (E.g., 
if> E SAT iff a satisfying assignment for if> exists.) In practical applications, it is 
of little use to know merely that a Hamiltonian path of a given weight exists in 
a graph - it is much more important to have the path itself. The P=NP ques-
tion itself is usually phrased in terms of language recognition instead of search 
problems, but it is a well-known fact that P=NP if and only if all of the related 
search problems are solvable in polynomial time. 
An analogous notion of "search problem" may be defined for NE: 
Definition 4. An NE-predicate is a relation R defined by an exponential-time 
nondeterministic Turing machine M; R(x, y) is true iff y encodes an accepting 
computation of M on input x. R is solvable in time T if there is a deterministic 
Turing machine running in time T that, on input x, finds a string y such that 
R(x,y) holds, if any such y exists. 
Stated another way, R is solvable if there is a routine that, for all x, can 
find a witness for x if a witness exists. Conversely, R is not solvable in time t if 
each routine running in time t fails to find witnesses for infinitely many x that 
have witnesses. Note however that non-solvability of R says nothing about the 
frequency with which "hard" inputs are encountered. Note that the strongest 
statement about non-solvability of an NE predicate that one could make would 
be to say that all large inputs are "hard" in this sense. This leads to the following 
definition, which generalizes the classical notion of immunity. 
Definition 5. An NE predicate R is immune with respect to time t( n) if (1) the 
set {x: 3y R(x,y)} is infinite, and (2) for all f computable in time t(n), the set 
{x : R(x, f( x))} is finite. 
The connections between NE predicates and Kolmogorov complexity were 
first drawn in [AW90]; the following theorem is a slight generalization of the 
results presented there. In short, it says that there are hard NE predicates if and 
only if there are sets Lin P such that KL(n) grows quickly. 

8 
Eric Allender 
Theorem 6. 
(aJ Every NE predicate is solvable in time t(20(n») iff for every set L in P, 
KL(n) = O(logt(nO(l»)) 
(b J No NE predicate is immune with respect to time t( 20 ( n») iff for every set L 
in P, KL(n) =I w(1ogt(nO(l»)) 
Proof. We will sketch a proof of the first equivalence; the second equivalence 
is proved in a very similar manner. For the forward direction, assume that NE 
predicates can be solved in the stated time bound, and let L be a set in P. 
Let R be the relation given by R( m, x) ¢} x E L =m. (Here, we assume the 
standard binary representation of numbers, and identify a number with its binary 
representation.) Then R is an NE predicate and there is a function f computable 
in time t( 2cn ) that solves R, for some constant c. Let s be a description of a 
machine computing f. Now note that if L=m is nonempty, then f(m) E L=m, 
and Kt(f(m)) ~ lsi + Iml + logt(2c1ml ) = 0(1) + log If(m)1 + logt(lf(m)IO(l»), 
which establishes the forward direction. 
For the converse, assume the given bounds on the rate of growth KL for sets L 
in P, and let R be any NE predicate defined by a machine M running in time 2cn . 
Let L be the set of all strings z of length m c for some m such that there is some 
prefix w of z such that R( m, w). L is in P, and by assumption KL (n) ~ d log t( n d) 
for some d. Consider the following routine for solving the NE predicate R: On 
input m, for each string y of length at most dlogt(md ), run Mu on input y for 
t(md)d steps and see if the output produced has a prefix encoding an accepting 
computation of M on input m. Output the first accepting computation found 
in this way, if any is found. It is easily verified that this routine solves R within 
the claimed time bound. 
D 
Corollary 7. 
(aJ Every NE predicate is solvable in exponential time iff KL(n) = O(logn) for 
all L in P. 
(bJ No NE predicate is immune with respect to exponential time iff KL(n) =I 
w(log n) for all L in P. 
An essentially identical proof shows that if there is an NE predicate that 
cannot be solved in time 22n- 1 , then there is a set L in P with KL(n) ~ n/5. 
We thus see that the most common conjectures concerning the difficulty of sets 
in NE have as a consequence that there are some sets L in P with rather high 
Kolmogorov complexity, as measured by the function K L . 
From Theorem 6, we see that the question of whether or not a set L in P 
can have nontrivial growth rate is very closely related to the E=NE question. 
It is natural to ask if it is in fact equivalent to E=NE. Note that if every NE 
predicate is solvable in exponential time, then E=NE is a trivial consequence; 
does the converse hold? 
This question was explicitly raised in [AW90] as a result of an investigation 
using Kolmogorov complexity as a tool for answering certain questions concern-

Time-Bounded Kolmogorov Complexity in Complexity Theory 
9 
ing classes of sets equivalent to tally sets3 under varying notions of reducibility. 
(See [Bo092] for a discussion.) The question was essentially answered by Im-
pagliazzo and Tardos [IT89]; they present an oracle relative to which E=NE 
but there are NE predicates that cannot be solved in exponential time. Thus 
the equivalence of the language recognition and witness-finding problems as it 
relates to the P versus NP problem does not generalize to larger time bounds. 
3 Related Notions 
The questions of whether or not KL( n) = O(log n) or KL (n) i= w(log n) for all 
L in P have been asked many times by different researchers studying apparently 
unrelated topics. In this section, we will gather some of these diverse results 
together. 
3.1 P-Printability 
A set L is P-printable if there is an algorithm that can list all of the elements 
of L=n in time polynomial in n. An immediate consequence of this definition is 
that all P-printable sets are sparse and are in P. However, it is suspected that 
there are sparse sets in P that are not P-printable. P-printable sets were defined 
in [HY84] and have been studied in many papers; for further information see 
[Bo092]. It was shown in [AR88] (see also [HH88]) that L is P-printable iff L is 
in P and KL(n) = O(logn). 
Many of the papers that consider P-printable sets ask if every infinite set 
in P has an infinite P-printable subset. It is an easy observation that a set L 
in P has an infinite P-printable subset iff KL(n) i= w(logn). Furthermore, it 
was observed by Russo (see [AR88]) that sets in NP are similar to sets in P in 
this regard: every set in P has an infinite P-printable subset iff every set in NP 
has an infinite P-printable subset. Rephrasing Russo's observation in terms of 
Kolmogorov complexity, we get: 
Theorem 8. 
(aJ Kdn ) i= w(logn) for all L in P iff KL(n) i= w(logn) for all L in NP 
(bJ Kdn) = O(1ogn) for all L in P iff Kdn) = O(logn) for all Lin NP 
3.2 Ranking 
The property of having an infinite P-printable subset was called "tangibility" in 
[HR90]. It was studied there as a very weak notion related to a concept known as 
ranking. As there are other connections with ranking to explore, let us consider 
ranking more closely. 
3 A set is a tally set if it is a subset of 0* . 

10 
Eric Allender 
Definition9. Let L be a language. The function defined by rankL(x) = lib E 
L : y :::; x }II is known as the ranking function for L. (Here, :::; denotes lexico-
graphic order.) 
If L has an easily-computed ranking function, then there is an efficiently-
computable bijection from L onto the natural numbers; each element of L is 
mapped to its rank in L. This bijection may be thought of as mapping each 
element of L to a "compressed" representation. If L contains only a small fraction 
of the words of each length and has an easily-computed ranking function this 
clearly places bounds on the Kt-complexity of any string in L. Ranking functions 
were first studied in [GS91] in connection with data compression. Other material 
on ranking may be found in [AIl85, BGS91, BGM90, Huy90a, HR90]. A number 
of classes of sets (including the unambiguous context-free languages [GS91]) have 
been shown to have easy ranking functions. 
If L has an easy ranking function, then a simple binary search procedure 
enables one to quickly locate the lexicographically least element of L =n. It thus 
follows that any such L has KL(n) = O(log n). Thus all the classes of sets shown 
in [A1l85, BGS91, BGM90, Huy90a, GS91] to have easy ranking functions are 
also classes of sets with low KL complexity. 
It is worth noting, however, that the class of sets in P with low KL complex-
ity is somewhat larger than the class of sets with easy ranking functions. For 
example, it is not hard to see that if L is a context-free language, then Kdn) = 
o (log n). (The proof is quite similar to the proof of Theorem 4 in [AR88]; see also 
[A1l85, Huy90b].) However, it was shown in [Huy90a] that if P =f. PP there are 
context-free languages that have hard ranking functions. As another example, 
it was shown in [Huy90a] that if every set in NTIME(logn) has an easy rank-
ing function,4 then P = PP, although Gore has observed [Gor90] that Kdn) = 
O(logn) for all L E NTIME(logn). On the other hand, Huynh's techniques 
[Huy90a] can easily be used to prove that there are sets S E coNTIME(logn) 
with Ks =f. O(logn) unless KL(n) = O(logn) for all L E P (which is to say, 
unless all NE predicates are solvable in exponential time). 
3.3 Invertibility 
Note that finding an element of L=n is roughly the same as computing a sort 
of inverse of the characteristic function of L, XL; the aim is to find a string in 
a certain range that is in X:L1(1). In [A1l85] a number of classes of automata 
are exhibited that compute only easy-to-invert functions. 5 If L is a language 
accepted by any machine in one of these classes, then Kdn) = O(1ogn). 
4 In order to consider sublinear running times, the Turing machine model considered 
here has an "index tape" allowing the machine to access a given input position in 
unit time. This model is commonly used when using the Turing machine formalism 
to characterize circuit complexity classes. 
5 In a related result, some of these same classes of machines were shown in [KGY89] 
to be unable to compute pseudorandom generators. 

Time-Bounded Kolmogorov Complexity in Complexity Theory 
11 
The connection to invertible functions was stated more precisely in [AW90]. 
It is shown there that KL(n) = O(logn) for all L E P iff every honest function 
f : E* --+ 0* computable in polynomial time is weakly invertible.6 
3.4 Generation 
In work originally motivated by the problem of generating test data for heuris-
tic testing and evaluation, Sanchis and Fulk [SF90] defined the notion of a 
polynomial-time constructor (PTC) for a language L, which is a routine run-
ning in polynomial time that on input In either produces an element of L=n or 
announces that L=n = 0. Clearly, if L has a PTC, then Kdn) = O{logn), and 
if L is in P, then L has a PTC iff KL (n) = O(log n). (Additional work dealing 
with constructors is reported in [Huy90b].) 
It is noted in [SF90] that most "natural" sets of interest in practical situations 
are easily seen to have PTCs. Thus sets L in P such that KL grows rapidly seem 
to be somewhat "unnatural" - but note that such sets must exist, unless E=NE. 
4 Pseudorandom Generators 
A pseudorandom generator is an efficient algorithm that takes a short input 
(the random seed) and produces a long pseudorandom output. A pseudorandom 
generator is secure if the output produced passes all "feasible" statistical tests 
for randomness. Pseudorandom generators are the object of much study in the 
theory of cryptography; an excellent introduction to the theory of pseudorandom 
generators may be found in [BH88]. 
There are many different ways of formalizing the hypothesis "secure pseudo-
random generators exist", depending on the particular notion of statistical test 
being used, and depending on the desired degree of "security". In this overview, 
we will try to present material on an intuitive level; the reader is invited to 
consult the cited references for more precise definitions. 
For the purposes of this section we will use the definitions of [Ya082]. A 
statistical test is a language L in P /poly (that is, a set accepted by a (possibly 
nonuniform) family of polynomial-size circuits). For each input length n, the 
probability that L contains a random input of length n is simply IIL=nll/2n. 
If a pseudorandom generator f takes inputs of length n and produces output 
of length p(n), then the probability that L contains a pseudorandom input of 
length p(n) is II{y E E=n : f(y) E L }11/2n. The generator f will be said to pass 
the statistical test L if for all polynomials q and for all large n, the probabilities 
that L contains random and pseudorandom strings of length p( n) differ by at 
most I/q(n). If f passes all statistical tests in P /poly, then f is said to be secure. 
That is, f is secure if the pseudorandom output it produces "looks random" to 
all tests in P / poly. 
6 A function f is honest if If(x)1 is polynomially related to Ixl; f is weakly invertible 
if there is a function 9 computable in polynomial time such that f (g( x» = x for all 
x in the image of f. 

12 
Eric Allender 
In [A1l89a]' it was shown that most ways of formalizing the hypothesis that 
secure pseudorandom generators exist have as a consequence that KL(n) grows 
slowly for all dense sets L in p.7 For the purposes of this section, the following 
result is illustrative. 
Theorem 10. [A1l89a] If there are pseudorandom generators that are secure 
against P /poly statistical tests, then KL( n) = O( nf) for all dense sets L in 
P /poly, and for all E > O. 
Variants of Theorem 10 were used in [A1l89a] to show new inclusion rela-
tions among complexity classes, under various assumptions about the security 
of pseudorandom generators. 
It is known that pseudorandom generators exist if and only if one-way func-
tions exist that are hard to invert over a significant fraction of their range 
[ILL89, Has90J. Thus the existence of this sort of one-way function implies that 
KL cannot grow too quickly for any dense set L in P. On the other hand, we saw 
in the preceding section that the solvability of NE predicates was also equiv-
alent to the existence of a certain sort of one-way function, and the existence 
of this sort of one-way function implies that KL must grow quickly for some 
sets L in P. Thus there seems to be some sort of trade-off concerning what no-
tions of one-way-ness are mutually compatible; this situation is still only poorly 
understood. 
Taken together, Theorems 6 and 10 motivate the question of whether or not 
there is any connection between the density of a set L in P and the rate of growth 
of KL(n). That is, 
NE contains hard sets => KL(n) grows quickly for some set L in P. 
Secure pseudorandom generators exist => KL (n) grows slowly for all dense 
sets L in P. 
Since the left hand sides of these implications are often conjectured to hold, it 
follows that it is conjectured that KL(n) can achieve a faster growth rate if L is 
sparse than if L is dense. 
Apart from observations such as these, there is little to guide one's intuition 
in questions concerning the KL complexity of sets L in P. In the following para-
graphs, we turn to the study of random and generic oracles for help in hypothesis 
formation. 8 
4.1 Random and Generic Oracles 
The study of random oracles in complexity theory was initiated in [BG81J. There, 
among other results, it was shown that with probability one, pA =1= NpA, relative 
7 In the following, the density of a set L, denoted dL( n), is the function given by dL( n) 
= II{L=n l11/2n. A set L is said to be dense iffor some k and all large n, if L=n =p 0, 
then dL(n) ?: link. That is, L is dense if it contains many strings of length n, if it 
contains any at all. 
S Some of the material in Section 4.1 originally appeared in [AIl89bJ. 

Time-Bounded Kolmogorov Complexity in Complexity Theory 
13 
to a random oracle A. More formally; if we consider a probability space of all 
oracles over the alphabet E = {O, I}, where for each string x the event x E A 
has probability one half and these probabilities are independent of each other, 
then the set of all oracles A relative to which P i- NP has measure one. Bennett 
and Gill observed in [BG81] that for most complexity-theoretic statements C of 
interest (e.g., for C equal to any of the statements "P=BPP," "P=NP n coNP," 
etc.), the set of oracles relative to which C holds satisfies Kolmogorov's zero-one 
law (see, e.g., [Oxt80](Theorem 21.3)). As a consequence, for statements C of 
this sort, either C holds with probability one or with probability zero, relative 
to a random oracle. 
Bennett and Gill went on to conjecture that complexity theoretic statements 
that hold with probability one relative to a random oracle also hold in the unrel-
ativized case. Although their conjecture has been disproved [Kur83, HCRR90, 
CGH90], it is at least true that the study of complexity-theoretic statements that 
hold relative to a random oracle provides an internally consistent world view, 
which sometimes seems useful in gaining intuition concerning the unrelativized 
case.9 Thus we will examine the KL complexity of sets L in P, relative to random 
oracles. 
An alternative to random oracles is provided by the notion of "generic" or-
acles. Generic sets arise in set theory, logic, and computability, as examples of 
sets that simultaneously satisfy all properties that can be guaranteed via certain 
types of diagonalization arguments. They also arise from the use of Baire cate-
gory to describe a topological notion of "typical set," analogous to the measure-
theoretic notion of randomness. The following paragraphs provide a brief intro-
duction to genericity; the reader is encouraged to consult the cited references for 
more detailed discussions. 
The fundamental notion of Baire category is the concept of a nowhere-dense 
set; a set C (on the real line, say) is nowhere-dense if it is "full of holes" in 
the sense that for every interval I there is a subinterval J contained in the 
complement of C. (The classic example of a nowhere-dense set is the Cantor 
"excluded middle" set; nowhere dense sets should intuitively be thought of as 
being "small" in some sense.) For our purposes, the nowhere-dense sets C we 
will be considering will be sets of oracles, and intervals correspond to sets of 
oracles, all of which agree on some finite initial segment. Thus C is nowhere-
dense if for every finite oracle F, there is a finite extension F' :2 F such that 
every extension of F' is in the complement of C. This corresponds to the classical 
definition (e.g., as presented in [Oxt80j), but for our purposes we will want to 
require that the function (called an extension function) that produces F' from 
input F be describable in first-order logic, or even be computable. A development 
along this line leads to notions of effective and resource-bounded category (see 
[Meh73, Lut90, Fen91]). Let Ll be a class of extension functions. Then an oracle 
Gis Ll-generic if G is not an element of any Ll-nowhere-dense set. Equivalently, 
9 A number of other papers discuss in depth the interpretation that should be given 
to results concerning random oracles. The reader is referred to [KMR89, KMR91, 
Cai89,Boo91]. 

14 
Eric Allender 
G is Ll-generic if for all extension functions hELl, there is some finite oracle F 
such that h(F) is an initial segment of G. 
Many diagonalization arguments can be modelled in terms of extension func-
tions. For example, a typical diagonalization argument proceeds in stages, with 
Fo = 0 at stage 0, and then, given any finite oracle Fi at the start of stage i, the 
argument shows how to build an extension Fi+l satisfying some property. That 
is, the construction is the description of an extension function. Furthermore, if Ll 
is a class of extension functions (such as the class of extension functions describ-
able in first-order logic, the class of recursive extension functions, etc.) and if G is 
Ll-generic, then G satisfies all properties that can be ensured via diagonalization 
arguments in Ll. For example, if Ll is the class of recursive extension functions 
and G is Ll-generic, then pG #- NpG #- coNpG (because the standard diag-
onalization argument showing the existence of oracles satisfying this property 
[BGS75] can be modelled in this way). Different notions of genericity (for differ-
ent classes Ll) have been studied by [Maa82, AFH87, Dow82, Poi86, B187, Fen91]. 
Observe that if Ll ~ Ll', then G Ll'-generic implies G Ll-generic. 
In [BI87] Blum and Impagliazzo promoted the study of complexity classes 
relative to generic oracles specifically as an alternative to random oracles. (They 
focused primarily on the notion of genericity that results when Ll is the class 
of extension functions expressible in the first-order theory of arithmetic; for the 
rest of this paper, "generic" will mean Ll-generic for this choice of Ll.) 
As with random oracles, generic oracles offer a consistent world view, in 
that all "reasonable" complexity theoretic statements C either hold relative to 
all generic oracles or hold relative to no generic oracle. In [BI87], Blum and 
Impagliazzo make the case that generic oracles are perhaps more likely than 
random oracles to give correct intuition concerning inclusions among complexity 
classes; they prove a number of theorems to support this case. They also show 
that it will be impossible to determine if certain questions (such as P = NP 
n coNP) hold relative to a generic oracle, without first solving some related 
questions in the unrelativized case. In fact, at the time of this writing, there is 
no statement C concerning inclusions among "familiar" complexity classes that 
is known to hold relative to a generic oracle and known not to hold relative to 
a random oracle, or vice-versa. Furthermore, the statements C that are shown 
in [HCRR90, CGH90] to hold relative to a random oracle but to be false in the 
unrelativized case, also hold relative to generic oracles. That is, neither random 
nor generic oracles give reliable information about which statements hold in the 
unrelativized case. 
The reader is certainly asking "Then why consider random and generic oracles 
at all?" 
Our purpose here is to investigate the question of whether or not there is any 
relationship between the density of a set L in P and the upper bounds that one 
can prove on K L . We have seen above that certain popular conjectures indicate 
that such a relationship does exist. We shall see below that relative to a random 
oracle there is in fact a very close relationship between the density of a set L 
in P and the growth rate of KL . On the other hand, relative to a generic oracle 
there is no such relationship at all. We leave the interpretation of these results 

Time-Bounded Kolmogorov Complexity in Complexity Theory 
15 
to the reader; let us just mention here, however, that we conjecture that one of 
these extremes actually holds in the unrelativized case. That is, we believe that 
either there is a close connection between density and Kolmogorov complexity 
of sets, or there is no connection at all. 
Theorem 11. For a large class of functions f, relative to a random oracle A: 
(aJ (L EPA/poly and h(n) 2': l/f(n)) ::::} Kt(n) ::::; logf(n) + O(logn). 
(bJ :3 L EpA, ddn) 2': l/f(n) and Kt(n) 2': (logf(n))/5 -
2logn. 
(That is, relative to a random oracle, sets L of density 1/ j(n) can have Kdn) no 
greater than about log j(n), and the bound is relatively tight. In the statement 
of this theorem, K t is simply the function that one obtains from the definition 
of K L , where the universal machine has access to the oracle A.) 
Proof. In order to prove part 1, it suffices to show that, for all b > 0, the set of 
oracles C has measure less than b, where 
C = {A : :3L EPA/poly ddn) 2': 1/ f(n) and 
'v'c3°On Kt(n) > log j(n) + clogn}. 
The idea of the proof is to show that if a machine accepts very many strings 
relative to a random oracle, then we can find some accepted string "encoded" in 
the oracle, in the following sense. Given the numbers nand r, one can query an 
oracle about membership for the strings 
YZ"+rn+l, YZ"+rn+2,· .. ,YZ"+rn+n, 
where YI, Y2, ... is a lexicographic enumeration of E*. These n queries return n 
answers from the oracle, and these answers can be concatenated to form a string 
w that can be said to be "encoded" in the oracle, with index (n, r). Thus w has 
relatively low Kt-complexity, relative to the oracle. (The actual encoding used 
will vary only slightly from this.) 
Let b be given, and let D = 10g(1/b). In the following discussion, assume 
that M I , M 2 , ••. is an enumeration of polynomial-time oracle Turing machines, 
where Mi runs in time ni + i. 
Define Ei,n = {A : there is an "advice" string z of length ni with which 
MiA accepts 2': 2n / j (n) strings of length n and does not accept any of the 
j(n)(i + n + D) strings of length n given by the characteristic sequence of A 
starting at on;+i+l}. That is, A E Ei,n if the oracle machine M i, given some 
advice string z for the strings of length n, accepts many strings of length n, 
but nonetheless manages to avoid accepting any of the strings that are stored in 
the "table" encoded by the oracle at position on'+i+l. Since the strings encoded 
in this "table" can't actually be read by Mi on inputs of length n (because it 
doesn't have time to query the oracle about strings of that size), acceptance 
of each one of those strings occurs with probability at least 1/ f(n) (since Mi 
is accepting at least a fraction of 1/ f (n) of the strings of length n). It follows 

16 
Eric Allender 
that each E· 
has measure at most (1 - b)f(n)(i+n+D) < 2-(i+n+D). Thus 
',n 
fin) 
Ui,n Ei,n has measure less than < 8. 
Note that each of the f (n) (i + n + log 8) strings of length n encoded in A 
starting at oni+i+l can be described relative to A by the pair (n,j) for some 
j :=:; f (n) (i + n + log 8), and thus any such string has K t A complexity bounded 
by O(logn) + log f(n) + 0(1). Thus A E C implies there is some i such that 
for infinitely many n there is an "advice" string z of length ni on which Ml 
accepts at least 2n / f(n) strings of length n but does not accept any of the strings 
appearing soon after oni+i+l in the encoding given by A. Thus C ~ Ui n Ei,n' 
~~~~~. 
' 
To see part 2, note that, with probability one, there is a set B in NTIMEA(2n) 
that has no infinite subset in DTIMEA(2Zn-l) [BG81, Gas87). It follows that the 
corresponding NE predicate is immune with respect to DTIMEA(22n- 1 ), and 
thus, as we observed earlier, with probability one there is a set C in pA with 
K~(n) 2:: n/5 for all large n such that K~(n) is defined. 
Now let f be any easy-to-compute function of the form f(n) = 2g(n), and let 
L = {y : y = xz for some z E C with Izl = g(iyl)} Then dL(n) 2:: 1/ f(n), and 
KL(n) 2:: g(n)/5 - 2logn. 
0 
In [AIl89a), some hope is held out that it might be possible to show that there 
are relatively dense sets L in P /poly such that KL(n) grows somewhat quickly. 
(There would have been interesting consequences for the theory of pseudorandom 
generators, if such sets could have been shown to exist.) The preceding theorem 
dashes these hopes (at least as far as relativizing proof techniques are concerned), 
since the sort of bounds that [AIl89a) discussed the possibility of exceeding are 
exactly the bounds shown above to hold relative to a random oracle. 
Theorem 11 shows that, relative to a random oracle, there is a very close 
relationship between the density of a set and the achievable Kt-complexity of 
the simplest elements of the set. Next we shall see that, relative to a generic 
oracle, no such relationship exists. 
Theorem 12. Relative to a generic oracle A, 
(a) There is set L in pA such that, for infinitely many n, dL( n) 2:: 1 - 2-n / 2 
and Kt(n) 2:: n/4. 
(b) For all infinite L in pA, Kt(n) i= w(logn). 
Thus, relative to a generic oracle, there are sets L that, for infinitely many n, 
contain many strings of length n, but only contain complex strings of length n. 
However, every infinite set in pA contains infinitely many simple strings. 
Proof. Part 2 follows from a proof very similar to that of Theorem 2.7 in [BI87). 
To see part 1, we let L be the generic oracle A itself. Thus, we need to 
show that, for a generic oracle A, there are infinitely many n such that dA (n) 2:: 
1- 2-n / 2 , and K1(n) 2:: n/4. 
Let G be any finite oracle. Let n be chosen so that G has no string of length n 
or greater. For all strings w of length:=:; n/4, run M:!(w) for 2n / 4 steps, and let Q 

Time-Bounded Kolmogorov Complexity in Complexity Theory 
17 
be the set of strings output or queried by Mu during any of these computations. 
Note that IIQII ::; 2n/2 Let G' = G U {x E En : x is not in Q}. By construction, 
G' contains many strings of length n, but contains no string of length n with 
KtA-complexity ::; n/4. It follows by the results and definitions of [BI87] that, 
since any finite oracle can be extended in this way, any generic oracle has the 
properties claimed in the statement of the theorem. 
0 
Corollary 13. Relative to a generic oracle, there is no pseudorandom generator 
that is secure against P /poly adversaries. 
Proof. It follows easily from the preceding theorem that relative to a generic 
oracle A there is a dense set L in PA/poly such that Kt(n) 2: n/4 for infinitely 
many n. The result now follows from Theorem 10. 
0 
Although we turned to generic and random oracles in order to find help in 
guiding our intuition concerning the likely behavior of the functions KL for sets 
Lin P and P /poly, the results of this section have given contrary indications. It 
is still far from clear how one might expect the KL complexity of sets L in P to 
behave. 
5 Circuit Complexity 
Recently, Kolmogorov complexity has been used as a tool in proving some new 
results in the area of circuit complexity; we will review these developments in 
this section. First let us present some basic definitions. (For more background on 
circuit complexity the reader is referred to the excellent exposition in [BS90].) 
A language L is said to be accepted by a family of circuits {Gn } if each circuit 
Gn takes inputs of length n, and for each x of length n, x E L iff Gn outputs 1 
when given input x. The size of a circuit is the number of gates, and the depth 
of a circuit is the length of the longest path from input to output. 
The class AGo is the class of languages that can be recognized by families 
of circuits of polynomial size and constant depth, where these circuits consist 
of unbounded fan-in AND and OR gates. Powerful combinatorial lower bound 
techniques have been developed in [Has86] (among others), showing that many 
very simple sets (notably the set PARITY consisting of all strings with an odd 
number of Is) cannot be computed by constant depth circuits of such gates of 
less than exponential size. 
Although AGo is a very "weak" complexity class in some sense, note that 
using the definition given above, AGo contains nonrecursive sets. (For example, 
there are nonrecursive tally sets, and every tally set is trivially in AGo.) These 
pathological examples can be avoided by restricting our attention to "uniform" 
circuit families: i.e., families {Gn } such that the function n 1-+ Gn is "easily 
computable" in some sense. The issue of choosing the correct notion of uniformity 
for AGo has been addressed by [BIS90j. Throughout the rest of this paper, we 
will consider only "uniform" AGo. 

18 
Eric Allender 
In light of the impressive lower bound results that have been proved, showing 
that various languages are not in AGo, it is natural to wonder if stronger sepa-
rations of P and AGo can be proved. For instance, is there a set in P that has 
no infinite subset in AGo? (Such a set is said to be immune to AGo.) In most 
cases in complexity theory, if a complexity class C1 can be shown to properly 
contain a complexity class C2 , then C1 can usually be shown to contain a set 
that is immune to C2 • 
Somewhat surprisingly, it seems that it will represent a significant break-
through if one is able to present a language in P (or even in NP) that is immune 
to AGo (or to show that no such language exists). As we show below, these 
questions are very closely related to questions about the complexity of sets in E. 
Note that if L is a P-printable set, then L is accepted by a family of constant-
depth circuits of unbounded fan-in AND and OR gates {Gn } where the function 
n I-> Gn is computable in polynomial time. Call the set of languages accepted 
by circuits of this type P-Uniform AGo. It is not known if P-Uniform AGo = 
AGo; the P-uniformity condition just described is much less restrictive than the 
uniformity condition of [BIS90]. This is addressed by the following theorem: 
Theorem 14. [AG91] P-Uniform AGo = AGo iff E = Uk Ektime(n).lO 
This may be taken as evidence that AGo is properly contained in P-uniform AGo, 
because E = Uk Ektime(n) implies that E = DSPACE(n) = NSPACE(n), as well 
as implying that the polynomial hierarchy collapses and is equal to PSPACE 
(which in turn is equal to DTIME(2nO(1))). 
Recall that every NE predicate is solvable in exponential time if and only if 
every set in NP has an infinite P-printable subset. By the observations in the 
preceding paragraph, this happens only if every set in NP has an infinite subset 
in P-Uniform AGo. Thus we observe: 
Observation 15. If every NE predicate is solvable in exponential time and E = 
Uk Ektime(n), then no set in NP is immune to AGo. 
The hypothesis of this observation seems quite unlikely; Uk Ektime(n) appears 
to be a small subclass of E, and they can be equal only if the polynomial hierarchy 
collapses. Assuming E = Uk Ektime(n) thus says that E is "not very powerful" 
in some sense, and it seems difficult to imagine that exponential time could 
simultaneously be powerful enough to solve all NE predicates. Nonetheless, it is 
shown in [AG91] that: 
Theorem 16. [AG91] There is an oracle relative to which all NE predicates are 
solvable in exponential time and E = Uk Ektime(n). 
Thus it would represent a significant advance at this time to show that there are 
sets in NP that are immune to AGo. The oracle construction in [AG91] makes 
heavy use of the notions of time-bounded Kolmogorov complexity surveyed here. 
Conversely, it was observed in [AG91] that if P=NP, then there is a set in 
P that is immune to AGo (because P=NP implies there is a set in E that is 
10 Ek time( n) is the linear-time analog of the kth level of the polynomial-time hierarchy. 

Time-Bounded Kolmogorov Complexity in Complexity Theory 
19 
immune to Uk L'ktime(n), and this gives rise to a tally set in P that is immune 
to ACO). Thus it will also be very significant if one can show that there are no 
sets in NP that are immune to ACo. 
6 Conclusion 
We have studied one method of measuring the time-bounded Kolmogorov com-
plexity of sets, and we have surveyed a number of applications of this measure 
to different topics in complexity theory. We believe that functions of the form 
KL offer a useful way of visualizing the complexity of a set, and we hope that 
they will prove useful in more situations to come. 
7 Acknowledgments 
I thank Ken Regan and Jack Lutz for helpful comments regarding genericity. 
References 
[A1l85] 
E. Allender. Invertible Functions. PhD thesis, Georgia Institute of Technol-
ogy, 1985. 
[A1l89a) 
E. Allender. Some consequences of the existence of pseudorandom genera-
tors. J. Comput. System Sci. 39:101-124, 1989. 
[A1l89b) 
E. Allender. The generalized Kolmogorov complexity of sets. In Proc. 4th 
IEEE Structure in Complexity Theory Conj., pages 186-194,1989. 
[AG91] 
E. Allender and V. Gore. On strong separations from ACo. In Proc. Fun-
damentals of Computation Theory, Springer-Verlag, Lecture Notes in Com-
puter Science 529:1-15, 1991. 
[AR88] 
E. Allender and R. Rubinstein. P-printable sets. SIAM J. Comput. 
17:1193-1202,1988. 
[AW90] 
E. Allender and O. Watanabe. Kolmogorov complexity and degrees of tally 
sets. Inform. and Computation 86:160-178, 1990. 
[AFH87] 
K. Ambos-Spies, H. Fleischhack, and H. Huwig. Diagonalizations over 
polynomial-time computable sets. Theoret. Comput. Sci. 51:177-204, 1987. 
[BGS75] 
T. Baker, J. Gill, and R. Solovay. Relativizations of the P=?NP question. 
SIAM J. Comput. 4:431-444,1975. 
[BDG87] 
J. Balcazar, J. Diaz, and J. Gabarro. Characterizations of the class 
PSPACE/poly. Theoret. Comput. Sci. 52:251-267,1987. 
[BG81] 
C. Bennett and J. Gill. Relative to a random oracle, P(A) ::j; NP(A) ::j; 
Co-NP(A) with probability 1. SIAM J. Comput. 10:96-113, 1981. 
[BGM90] 
A. Bertoni, M. Goldwurm, and P. Massazza. Counting problems and alge-
braic formal power series in noncommuting variables. Inform. Processing 
Letters 34:117-121, 1990. 
[BGS91] 
A. Bertoni, M. Goldwurm, and N. Sabadini. The complexity of computing 
the number of strings of given length in context-free languages. Theoret. 
Comput. Sci. 86:325-342, 1991. 
[BIS90] 
D. Mix Barrington, N. Immerman, and H. Straubing. On uniformity within 
NC!. J. Comput. System Sci. 41:274-306, 1990. 

20 
[Blu67] 
[BI87] 
[Boo91] 
[Boo92] 
[BH88] 
[BS90] 
[Cai89] 
[CGH90] 
[Dow82] 
[Fen91] 
[Gas87] 
[GS91] 
[Gor90] 
[Har83] 
[HCRR90] 
[HH88] 
[HY84] 
[Has86] 
[Has90j 
[HR90j 
[Huy85] 
Eric Allender 
M. Blum. A machine-independent theory of the complexity of recursive 
functions. J. ACM 14:322-336, 1967. 
M. Blum and R. Impagliazzo. Generic oracles and oracle classes. In Proc. 
28th IEEE Symp. Foundations of Computer Science, pages 118-126, 1987. 
R. Book. Some observations on separating complexity classes. SIAM J. 
Comput. 20:246-258, 1991. 
R. Book. On sets with small information content. In this volume. 
R. Boppana and R. Hirschfeld. Pseudorandom generators and complexity 
classes. In Advances in Computing Research. Volume 5: Randomness and 
Computation, pages 1-26. Edited by S. Micali. JAI Press, Greenwich, CT, 
1988. 
R. Boppana and M. Sipser. The complexity of finite functions. In Hand-
book of Theoretical Computer Science. Vol. A: Algorithms and Complexity, 
pages 757-804. Edited by J. van Leeuwen. MIT Press (in the United States, 
Canada, and Japan), and Elsevier Science Publishers (in other countries), 
1990. 
J.-Y. CaL With probability one, a random oracle separates PSPACE from 
the polynomial-time hierarchy. J. Comput. System Sci. 38:68-85, 1989. 
B. Chor, O. Goldreich, and J. Hastad. The random oracle hypothesis is 
false. Technical report 631, Department of Computer Science, Technion -
Israel Institute of Technology, 1990. 
M. Dowd. Forcing and the P hierarchy. Technical report LCSR-TR-35, Lab-
oratory for Computer Science Research, Rutgers University, 1982. 
S. Fenner. Notions of resource-bounded category and genericity. In Proc. 
6th IEEE Structure in Complexity Theory Conf., pages 196-212, 1991. 
W. Gasarch. Oracles: three new results. In Mathematical Logic and Theo-
retical Computer Science, Marcel Dekker, Inc., Lecture Notes in Pure and 
Applied Mathematics 106:219-251,1987. 
A. Goldberg and M. Sipser. Compression and ranking. SIAM J. Comput. 
20:524-536, 1991. 
V. Gore. Personal communication. 
J. Hartmanis. Generalized Kolmogorov complexity and the structure of fea-
sible computations. In Proc. 24th IEEE Symp. Foundations of Computer 
Science, pages 439-445, 1983. 
J. Hartmanis, R. Chang, D. Ranjan, and P. Rohatgi. Structural complex-
ity theory: recent surprises. In Proc. 2nd Scandinavian Workshop on Algo-
1'ithm Theory, Springer-Verlag, Lecture Notes in Computer Science 447:1-
12,1990. 
J. Hartmanis and L. Hemachandra. On sparse oracles separating feasible 
complexity classes. Inform. Processing Letters 28:291-296, 1988. 
J. Hartmanis and Y. Yesha. Computation times ofNP sets of different den-
sities. Theoret. Comput. Sci. 34:17-32, 1984. 
J. Hastad. Computational Limitations for Small-Depth Circuits. MIT 
Press, 1986. 
J. Hastad. Pseudo-random generators under uniform assumptions. In Proc. 
22nd ACM Symp. Theory of Computing, pages 395-404, 1990. 
L. Hemachandra and S. Rudich. On the complexity of ranking. J. Comput. 
System Sci. 41:251-271, 1990. 
D. Huynh. Non-uniform complexity and the randomness of certain com-
plete languages. Technical report TR 85-34, Computer Science Department, 
Iowa State University, 1985. 

Time-Bounded Kolmogorov Complexity in Complexity Theory 
21 
[Huy86] 
[Huy90a] 
[Huy90b] 
[IT89] 
[ILL89] 
[KGY89] 
[K086] 
[Kur83] 
[KMR89] 
[KMR91] 
[Kur83] 
[Lev73] 
[Lev84] 
[LV90] 
[Lut90] 
[Lut91] 
[Maa82] 
[Meh73] 
[MS90] 
[Oxt80] 
[Poi86] 
D. Huynh. Resource-bounded Kolmogorov complexity of hard languages. 
In Proc. Structure in Complexity Theory, Springer-Verlag, Lecture Notes 
in Computer Science 223:184-195, 1986. 
D. Huynh. The complexity of ranking simple languages. Math. Systems 
Theory 23:1-20, 1990. 
D. Huynh. Efficient detectors and constructors for simple languages. Tech-
nical report UTDCS-26-90, Computer Science Program, University of Texas 
at Dallas, 1990. 
R. Impagliazzo and G. Tardos. Decision versus search in super-polynomial 
time. In Proc. 30th IEEE Symp. Foundations of Computer Science, pages 
222-227,1989. 
R. Impagliazzo, L. Levin, and M. Luby. Pseudo-random generation from 
one-way functions. In Pmc. 21st ACM Symp. Theory of Computing, pages 
12-24, 1989. 
M. Kharitonov, A. Goldberg, and M. Yung. Lower bounds for pseudoran-
dom number generators. In Proc. 30th IEEE Symp. Foundations of Com-
puter Science, pages 242-247,1989. 
K. Ko. On the notion of infinite pseudorandom sequences. Theoret. Com-
put. Sci. 48:9-33, 1986. 
S. Kurtz. Notions of weak genericity. J. Symbolic Logic 48:764-770, 1983. 
S. Kurtz, S. Mahaney, and J. Royer. The isomorphism conjecture fails rel-
ative to a random oracle. In Proc. 21st ACM Symp. Theory of Computing, 
pages 157-166, 1989. 
S. Kurtz, S. Mahaney, and J. Royer. Average dependence and random ora-
cles. Technical report SU-CIS-91-03, School of Computer and Information 
Science, Syracuse University, 1991. 
S. Kurtz. On the random oracle hypothesis. Inform. and Control 57:40-47, 
1983. 
L. Levin. Universal sequential search problems. Problems Inform. Trans-
mission 9:265-266, 1973. 
L. Levin. Randomness conservation inequalities; information and indepen-
dence in mathematical theories. Inform. and Control 61:15-37, 1984. 
M. Li and P. Vitanyi. Applications of Kolmogorov complexity in the theory 
of computation. In Complexity Theory Retrospective, pages 147-203. Edited 
by A. Selman. Springer-Verlag, 1990. 
J. Lutz. Category and measure in complexity classes. SIAM J. Comput. 
19:1100-1131,1990. 
J. Lutz. Almost everywhere high nonuniform complexity. J. Comput. Sys-
tem Sci., to appear. A preliminary version appeared in Pmc. 4th IEEE 
Structure in Complexity Theory Conj., pages 37-53, 1989. 
W. Maass. Recursively enumerable generic sets. J. Symbolic Logic 47:809-
823,1982. 
K. Mehlhorn. On the size of sets of computable functions. In Proc. 14th 
IEEE Symp. Switching and Automata Theory, pages 190-196,1973. 
M. Mundhenk and R. Schuler. Non-uniform complexity classes and random 
languages. In Proc. 5th IEEE Structure in Complexity Theory Conf., pages 
110-119, 1990. 
J. Oxtoby. Measure and Category (second edition). Springer-Verlag, 1980. 
B. Poizat. Q = NQ? J. Symbolic Logic 51:22-32, 1986. 

22 
[SF90] 
[Sip83] 
[Yao82] 
Eric Allender 
L. Sanchis and M. Fulk. On the efficient generation of language instances. 
SIAM J. Comput. 19:281-296,1990. 
M. Sipser. A complexity theoretic approach t.o randomness. In Proc. 15th 
ACM Symp. Theory of Computing, pages 330-335, 1983. 
A. Yao. Theory and applications of trapdoor functions. In Proc. 23rd IEEE 
Symp. Foundations of Computer Science, pages 80-91, 1982. 

On Sets with Small Information Content * 
Ronald V. Book 
Department of Mathematics 
University of California 
Santa Barbara, CA 93106, USA 
book%henri@hub.ucsb.edu 
Abstract. The purpose of this paper is to review and summarize a 
number of results relating to sets with small information content such as 
sets with small generalized Kolmogorov complexity. The emphasis is on 
the role of such sets as oracle sets in the context of structural complexity 
theory 
1 Introduction 
Many of the issues that dominate the effort in computational complexity theory 
stem from the open problem of whether every problem solvable nondetermin-
istically in polynomial time is in fact tractable, i.e., does P equal N P? One 
of the themes that has received a great deal of attention in this context is the 
study of the computational complexity of relativized computation, that is, the 
complexity of computation relative to oracle sets. It is in this area where the 
idea of sets with "small information content" has arisen and where many inter-
esting results have been obtained by considering the notion of sets with "small 
generalized Kolmogorov complexity" and related concepts. The purpose of this 
paper is to review some of the recent work on these topics within structural 
complexity theory (see [BDG88, 90]) in order to make it known to researchers 
who are interested in the abundance of ideas stemming from the basic definitions 
of Kolmogorov complexity. 
How does one measure the amount of information encoded in a set of strings? 
One method of doing this is to identify this notion with the inherent computa-
tional complexity of the characteristic function of the set as measured by some 
dynamic measure of the complexity of algorithms for computing the character-
istic function, e.g., running time or amount of work space used. Thus a set that 
can be recognized in exponential time but not polynomial time is considered to 
encode more information than a set that can be recognized in polynomial time. A 
second method of doing this is to consider a function that bounds the "general-
ized Kolmogorov complexity" of the strings in the set. Then a set with large gen-
eralized Kolmogorov complexity is considered to encode more information than a 
* Preparation of this paper was supported in part by the National Science Foundation 
under Grant CCR89-13584 and by the Alexander von Humboldt-Stiftung while the 
author was visiting the Institut fUr Informatik, TU Miinchen, Germany. 

24 
Ronald V. Book 
set with small generalized Kolmogorov complexity. A third method is to consider 
the set as an oracle set and apply operators that correspond to resource-bounded 
reducibilities; for example, when studying sets in the polynomial-time hierarchy, 
it is natural to use the "NP( )-operator": if NP(A) = Ef but NP(B) = Ef, then 
B is considered to encode more information than A (since there is strong feeling 
that Ef =F En· One of the themes of research in this area is to explore the pos-
sible relationships among these last two methods and this is what is emphasized 
in the present paper. 
When studying structural complexity theory it is often desirable to have 
intrinsic characterizations of complexity classes, that is, to describe membership 
in the class in terms that do not involve the same concepts used in defining the 
class. On the one hand, such characterizations are useful in recognizing members 
of the class; on the other hand, they are helpful in studying properties of the 
class. The same theme arises in the study of sets with small information content 
and some intrinsic characterizations of the various classes of sets with small 
information content will be described. 
There are many uses of the ideas surrounding the basic definition of Kol-
mogorov complexity in computational complexity theory. The forthcoming book 
by Li and Vitanyi (see [LV 90]) contains a great deal of information. The purpose 
of the present paper is to review some of the aspects of structural complexity 
theory where the study of sets of strings with either high or low generalized 
Kolmogorov complexity is of use. There are many topics that are not covered; of 
particular interest in the present context is the work of Watanabe [Wat87j in his 
study on complete sets with respect to different polynomial time reducibilities 
for the class DEXT of sets recognized in time 20 (n), the work of Allender [AIl89] 
in his study of upwards separation results, and the work of Huynh [Huy87j who 
uses such sets in studying the notion of a "complexity core" relative to the class 
of sets with polynomial-size circuits. 
Section 2 contains notation and a review of some concepts of computational 
complexity theory and, more specifically, structural complexity theory. The idea 
of sets with small generalized Kolmogorov complexity is reviewed in Sect. 3, 
properties of this class are described, and the relationship between this class 
and other classes of sets with small information content is developed. In struc-
tural complexity theory, reducibilities computed with restricted computational 
resources, particularly those computed in polynomial time, have been the sub-
ject of much effort. In Sect. 4 some results regarding classes of sets polynomial 
time reducible to the sets with small generalized Kolmogorov complexity are 
reviewed. This review is continued in Sect. 5 where a decomposition of the class 
of sets with polynomial-size circuits is considered. In Sect. 6, results having to 
do with "lowness" are described, while in Sect. 7 characterizations of complexity 
classes in terms of reducibilities is reviewed. 
2 Notation and Concepts from Complexity Theory 
In this section notation is established and some definitions and important con-
cepts of structural complexity are reviewed. 

On Sets with Small Information Content 
25 
A fixed finite alphabet E is assumed and {O, l} ~ E. The set of all (finite) 
strings over E is denoted by E*. The length of a string x will be denoted by Ixl. 
The cardinality of a set S will be denoted by II S II. 
A fixed polynomial time computable bijection ( , ) : E* x E* -+ E* is 
assumed; such a function is a "pairing function," and it is assumed that the 
appropriate inverses are also computable in polynomial time. 
For sets A, B ~ E*, the join of A and B, A EB B, is defined to be the set 
{Ox, ly I x E A, y E B}. 
For any class C of subsets of E*, co-C = {E* - L I L E C}. 
A tally set is a set of strings over a one letter alphabet (so that a tally set 
encodes a set of natural numbers by using unary notation). A set A is sparse if 
there is a polynomial p such that for every n > 0, II {x E A I Ix I ::; n} II ::; p( n). 
Clearly, every tally set is sparse but there are sparse sets that are not tally 
sets. 
It is assumed that the reader is familiar with the basic ideas underlying formal 
models of computation and the basic issues surrounding the study of algorithms 
and complexity theory. Since this paper presents an overview, no specific model 
of computation is assumed but results are described in terms of Turing machines. 
A "good" algorithm is one for which there is a fixed polynomial that serves 
as an upper bound to the algorithm's running time; running time is measured 
in terms of the size of the input, e.g., the length of the input string, so that the 
polynomial bound is evaluated on the size of the input. A problem is "tractable" 
if there is a good algorithm that solves all instances of the problem. Thus, class 
P of problems solvable in polynomial time is the collection of (suitable encodings 
of) all tractable problems. 
Algorithms are "deterministic" in the sense that at any point in a computa-
tion there is at most one instruction to be executed at the next step. But there is 
a mathematical construct that allows there to be a choice among finitely many 
instructions that might be executed at the next step. This construct is called 
"nondeterminism" and in the case of time-bounded computation it formalizes 
the notion of "guessing" which instruction to perform at each step. 
The class NP is the collection of (suitable en co dings of) all problems that 
can be solved nondeterministically in polynomial time. The class NP can be 
characterized as follows: A E NP if and only if there exists a set B E P and a 
polynomial q such that (Vx)[x E A ¢} (3y)(lyl ::; q(lxl) and (x,y) E B)); here, y 
is a "guess" and a polynomial-time algorithm testing membership in the set B 
is considered to be a "checking" procedure. 
The major open problem of computational complexity theory (or even, all 
of computer science) is whether the classes P and NP are the same. The reader 
is referred to the book by Garey and Johnson [GJ79) for a description aimed at 
those who do not specialize in theoretical computer science, and to the books 
by Balcazar, Diaz, and Gabarro [BDG88, 90) for a presentation of some of the 
central issues of structural complexity theory. 
In addition to the question of whether P and NP are equal, the question of 
whether NP is closed under complementation (i.e., is NP equal to co-NP?) is 
open. 

26 
Ronald V. Book 
One approach to the open problems about complexity classes is the method of 
relativization as used in recursive function theory. In the midst of a computation, 
it is sometimes desirp,ble to obtain information from an external source; such 
a source is called an "oracle" and machines that utilize an oracle are "oracle 
machines." 
An oracle machine is a Turing machine with a distinguished work tape and 
three distinguished states, QUERY, YES, and NO. At some step of a compu-
tation on an input string x, the machine may enter the state QUERY. In state 
QUE RY, the machine transfers into the state YES if the string currently on 
the query tape is in some oracle set A; otherwise, the machine transfers into 
state NO; in either case, the query tape is instantly erased. The set of strings 
accepted by an oracle machine M relative to the oracle set A is L(M, A) = {x I 
there is an accepting computation of M on input x when the oracle set is A}. 
Oracle machines may be deterministic or nondeterministic. An oracle ma-
chine may operate within some time bound T, where T is a function of the 
length of the input string, and the notion of time bound for an oracle machine 
is just the same as that for an ordinary Turing machine. For any time bound T 
and oracle set A, DTIME(T, A) = {L(M, A) I Mis a deterministic oracle ma-
chine that operates within time bound T} and NTIME(T, A) = {L(M, A) I M 
is a nondeterministic oracle machine that operates within time bound T}. Of 
particular interest are the classes P(A) and NP(A) for arbitrary oracle sets A. 
Here P (A) = {L( M, A) I M is a deterministic oracle machine that operates in 
time p(n) for some fixed polynomial p}, so that P(A) = Uk>oDTIME(nk, A). If 
B E P(A), then we say that B is recognized in polynomial time relative to A. 
Oracle machines are used to define Turing reducibilities that are computed 
within time bounds or space bounds. Thus, set A is Turing reducible to set B in 
polynomial time, written A:::;~B, if A E P(B). 
It is clear that :::;~ is reflexive and transitive. For any class C of sets, define 
P(C) = U{P(A) I A E C}. 
The analogous notions are used in defining NP(A). However, the reducibility 
:::;~p is not transitive. (For, if from some A, NP(A) = NP(NP(A)), then co-
NP(A) = NP(A). But it is known [BGS75] that there exists a set B such that 
NP(B) "I co-NP(B).) 
In addition, an oracle machine may operate within some space bound S, 
where S is a function of the length of the input string; in this case, it is required 
that the query tape as well as the ordinary work tapes be bounded in length by 
S. For any space bound S and oracle set A, DSPACE(S,A) = {L(M,A) 1M 
is a deterministic oracle machine that operates within space bound S} and 
NSPACE(S, A) = {L(M, A) I M is a nondeterministic oracle machine that op-
erates within space bound S}. It is known that for appropriate space bounds S, 
NSPACE(B) ~ DSPACE(S2); this result also applies to classes specified by or-
acle machines. Thus, for arbitrary oracle sets A, PSPACE(A) = {L(M, A) 1M 
is a deterministic oracle machine that operates in space p( n) for some fixed 
polynomial p} = {L( M, A) I M is a nondeterministic oracle machine that op-
erates in space p(n) for some fixed polynomial p} since Uk>oDSPACE(nk, A) = 
Uk>oNSPACE(nk, A). 

On Sets with Small Information Content 
27 
There are some important complexity classes that we can now define. 
Let A be a set. Define E5(A) = P(A), Ef(A) = NP(A), and Er+1(A) = 
NP(Er(A)) = {L(M,A) I M is a nondeterministic oracle machine that runs 
in polynomial time and A E Er(A)}. Define Ll~(A) = Lli(A) = P(A) and 
Ll~+l (A) = p(Er(A)). For each i, define nr(A) = co-Er(A). Then E5(A) ~ 
Ef(A) ~ ... ~ Er(A) ~ ... is the polynomial-time hierarchy relative to A, and 
E5 ~ Ef ~ ... ~ Ef ~ ... is the polynomial-time hierarchy. Let PH(A) = 
Uk~oEr(A) and PH = Uk~oEr It is clear that for every set A, PH (A) ~ 
PSPACE(A). It is not known whether the polynomial-time hierarchy extends 
beyond E5 = P. 
There is another important type of reducibility. 
Define the binary relation ::;~ between subsets of E* by A ::;~B if there 
is a function f that is computable in polynomial time and has the property 
that (V'x)(x E A {:} f(x) E B), i.e., A = f-l(B). We say that "A is many-one 
reducible to B in polynomial time" when A::;~B. For a class C of subsets of E*, 
a set B is ::;~-complete for C if B E C and for every A E C, A::;~B. A set that 
is ::;~-complete for NP is NP-complete. 
If ::;~ is a reducibility computed in polynomial-time, then a set B is ::;~ -hard 
for NP if there is an NP-complete set A such that A::;~ B. 
Define A=~B if A::;~B and B::;~lA; A and B are said to be many-one 
equivalent in polynomial time. Similarly, define A=~B if A::;~B and B::;~A; A 
and B are said to be Turing equivalent in polynomial time. 
3 Sets with Small Information Content 
The idea of the Kolmogorov complexity of finite strings provides one possible 
definition of the "degree of randomness" of a string. Informally, the Kolmogorov 
complexity of a finite string is the length of the shortest program that will 
generate the string; intuitively, it is a measure of the amount of information 
that the string contains. A string is considered to be "random" if the length of 
the shortest problem that generates the string is the same as that of the string 
itself. This concept has been studied extensively; recently, it has found many 
applications in computer science and, particularly, in computational complexity 
theory (see [LV90J). 
A modification of the idea of Kolmogorov complexity has also been developed: 
consider not only the length of a program but also, simultaneously, the running 
time of the program. This modification has been used by Ko [K086], by Sipser 
[Sip83], and by Hartmanis [Har83] who considered the notion of "a generalized, 
two-parameter Kolmogorov complexity measure for finite strings which measures 
how far a given string can be compressed and how easily it can be recomputed 
from the shortened representation": gi~en a universal Turing machine U and 
functions G and g, a string x of length n is in the generalized Kolmogorov class 
Ku[g(n), G(n)] if there is a string y of length at most g(n) with the property 
that U will generate x on input y in at most G(n) steps. 

28 
Ronald V. Book 
A set A of strings has small generalized K olmogorov complexity if there exist 
constants c and k such that for almost every x, x is in A if and only if x 
is in Ku[c . log n, nk] where n = Ixl. The class of sets with small generalized 
Kolmogorov complexity is denoted K[log, poly]. 
Allender and Rubinstein [AR88] have provided a useful intrinsic characteri-
zation of the class K[log, poly]. 
Theorem 1. A set has small generalized Kolmogorov complexity if and only if 
it is p-isomorphic to a tally set, i.e., set S is in K[log, poly] if and only if there 
is a tally set T and a bijection f mapping S onto T such that both f and 1-1 
can be computed in polynomial time. 
Since most of the properties of sets and of classes of sets studied in computa-
tional complexity theory are invariant under p-isomorphism, Theorem 1 shows 
that for the purposes of complexity theory, the class of tally sets can be identified, 
up to p-isomorphism, with the class of sets with small generalized Kolmogorov 
complexity. This means that the properties of the class of tally sets (a very 
well-studied class) are of interest with considering sets with small information 
content. 
For any arbitrary set A ~ {O, I} *, there is a tally set T A that represents the 
unary encodings of the strings in A when those strings are viewed as integers 
encoded in binary. It is easy to see that TA::;~A and that there is a function I 
computable in exponential time that computes a bijection from A to TA • From 
Theorem 1, one sees that this fact implies the existence of sets of small gener-
alized Kolmogorov complexity that are arbitrarily complex when the inherent 
computational complexity is measured by the running time or work space or any 
of the other standard dynamic measures (which are recursively related). Since 
the inherent computational complexity of a set has meaning only when the set 
is recursive, this fact implies that there is no possibility of finding a relationship 
between the inherent computational complexity of a set and a bound on the 
generalized Kolmogorov complexity of that same set. 
In the remainder of this section, properties of the class K[log, poly] are 
investigated by using the interpretation based on Theorem 1. This leads to the 
investigation of other classes of sets that have properties that are generalizations 
of properties of the class of tally sets. 
For any set A, let enumA be the function, that for each string on, has value 
a string encoding the set of all strings in A of length at most n. Set A is self-p-
printable if there is a deterministic oracle Turing machine that computes relative 
to A the function enumA and that runs in polynomial time. 
Notice that every self-p-printable set is sparse. It is easy to see that P = NP 
if and only if for every self-p-printable set A, P(A) = NP(A). Hartmanis and 
Hemachandra [HH88] have shown that the class of self-p-printable sets can be 
viewed as a relativized version of K[log, poly]. 
Theorem 2. A set A is self-p-printable if and only if A E K A[log, poly], that 
is, there is a universal oracle machine U and constants c and k with the property 

On Sets with Small Information Content 
29 
that for almost every x, x is in A if and only if x is in K U A [c . log n, n k) where 
n= Ixl. 
The idea of a "self-p-printable set" is easily generalized. For sets A and B, 
A is P(B )-printable if there is a deterministic oracle machine that computes 
relative to B the function enumA and that runs in polynomial time. Clearly, if 
A is P(B)-printable, then A is sparse. 
The reader should also consider the results of Allender and Rubinstein [AR88] 
considering the printability of sets. 
A class that plays an important role in the discussion of sets with small 
information content is the class of sets with "polynomial-size circuits." 
Let A ~ {O, 1}*. If C A = {Cn}n>O is a family of Boolean circuits (i.e., circuits 
with 'and', 'or', and 'not' gates) such that for every n > 0, Cn computes the 
characteristic function of {x E A I I x I :::; n}, then C A recognizes A. If there is 
a function f such that for every Cn in C A, the size of Cn is bounded above by 
f(n) (where the size of Cn is the number of gates in Cn), then A has f(n)-size 
circuits. 
It is easy to see that every set A ~ {O, I} * has exponential-size circuits. The 
class of interest is the class of sets with polynomial-size circuits, i.e., the class of 
sets A such that there is a family C A = {Cn}n>O of circuits that recognizes A 
and has the property that for some polynomial p and all n > 0, the size of Cn 
is bounded above by p(n). Clearly, every sparse set has polynomial-size circuits 
so there are sets that are not recursive but do have polynomial-size circuits. On 
the other hand, there are recursive sets that do not have polynomial-size circuits 
[Kan82). 
The class P /poly is the collection of all sets A such that there exists a poly-
nomial length-bounded function h : {O} * -t {O, I} * and a set B E P with the 
property that for every x E {O, 1}*, x E A if and only if (x, h(OI:>:I)) E B. 
Theorem 3. For any set A ~ {O, I} *, the following are equivalent. 
( a) A has polynomial-size circuits; 
(b) A is in P /poly; 
(c) there is a sparse set S such that A is Turing reducible in polynomial time to 
S; 
(d) there is a tally set T such that A is Turing reducible in polynomial time to 
T. 
Corollary 4. For any set A ~ {O, I} *, A has polynomial-size circuits if and only 
if there is a set S with small generalized K olmogorov complexity such that A is 
Turing reducible to S in polynomial time. 
Pippenger [Pip79] showed the equivalence of (a) and (b). The equivalence 
of (a) and (c) is attributed to A. Meyer in [BH77]. The equivalence of (c) and 
(d) has been proved in several places; see [BDG88]. The corollary follows by 
combining Theorem 1 and Theorem 3. 
Because of Theorem 3, the class of sets with polynomial-size circuits is de-
noted by P /poly. 

30 
Ronald V. Book 
An interesting subclass of P /poly is the class of sets with "self-producible" 
circuits studied by Ko[K085]. 
Set A has self-producible circuits if there exists a polynomial length-bounded 
function h : {o}* -
{O, 1}* and a set B E P with the properties that (i) for every 
x E {O, 1}*, x E A if and only if (x, h(OI"'I)) E B, and (ii) h can be computed 
relative to A by a deterministic polynomial time-bounded oracle transducer. 
Ba1cazar and Book [BB86] have provided an intrinsic characterization of the 
class of sets with self-producible circuits that is similar to the characterization 
of the class K[log, poly]. 
Theorem 5. A set has self-producible circuits if and only if it is polynomial-time 
Turing equivalent to a tally set (i.e., AsiT and TsiA). 
The characterizations in both Theorem 1 and Theorem 5 depend on the class 
of tally sets. The difference between the two characterizations is in the type of 
reducibility that is used. The former case uses p-isomorphisms and the latter 
case uses polynomial-time Turing equivalence. These are very different notions 
in general and are different when applied to the class of tally sets (as will be 
seen in the next section). 
The relationships between three of the classes defined so far was shown by 
Ba1cazar and Book. 
Theorem 6. 
(a) Every set with small generalized Kolmogorov complexity is self-p-printable, 
but there are sparse sets that are self-p-printable but do not have small gen-
eralized K olmogorov complexity. 
(b) Every self-p-printable set has self-producible circuits, but there are sets that 
have self-producible circuits but are not self-p-printable. 
It is known that there are nonsparse sets that have self-producible circuits 
and such sets cannot be self-p-printable (since they are not sparse). Shou-wen 
Tang (personal communication) has shown that if there is a sparse set that has 
self-producible circuits but is not self-p-printable, then P =f:. NP. 
In the context of Theorems 1, 3, and 6, other classes of sets are of interest. 
Their definitions stem from the problem of determining the inherent computa-
tional complexity of the characteristic function of sets. 
For an oracle machine M, a set A, and a string x, let Q(M, A, x) = {y I 
there is a computation of M on x relative to A that queries the oracle about 
y's membership in A}. For every set A, let NPB(A) = {L(M,A) I there is a 
polynomial q such that for all x, II Q(M, A, x) II s q(lxl)}. 
Let SAT denote the set of Boolean formulas in conjunctive normal form that 
are satisfiable. Recall that SAT is the first NP-complete set. (For the purposes 
of the present paper, SAT could denote any NP-complete set.) 
Book, Long, and Selman [BLS84] showed (i) for every set A, P(A) ~ NPB(A) 
~ P(A EEl SAT), (ii) P = NP if and only if for every set A, P(A) = NPB(A) 
if and only if for every set A, SAT E P(A) if and only if for every set A, 

On Sets with Small Information Content 
31 
P(A) = P(A Ef) SAT), and (iii) NP = co-NP if and only if for every set A, 
NPB(A) = co-NPB(A) if and only iffor every set A, SAT E NPB(A) if and only 
if for every set A, NPB(A) = P(A Ef) SAT). 
If a set A has the property that NP(A) = NPB(A), then the set A does not 
encode a great deal of information since it can be retrieved by using the NP B ( ) 
operator which forces a strong restriction on access to the oracle. For example, if 
NP(SAT) = NPB(SAT), then Ef = Ll~ so that the polynomial-time hierarchy 
extends no further than Ll~. 
If a set A has self-producible circuits, then NP(A) = NPB(A) so NP(A) ~ 
P(A Ef) SAT). However, there are sets with this property that do not have self-
producible circuits. 
Theorem 7. There is a set A such that P(A) = NPB(A) = NP(A) = P(A Ef) 
SAT) and A does not have polynomial size-circuits and, hence, does not have 
self-producible circuits. 
Proof. Let A be any set that is ~~-complete for EXPSPACE. (Recall that 
EXPSPACE = DSPACE(2po1y ).) If A has polynomial-size circuits, then there 
exists a sparse set S such that A~~S. Since A is ~~-complete for EXPSPACE, 
it is the case that for every set B E EXPSPACE, B~~A so that A~~S implies 
B~~S. Thus, by Theorem 3, every set in EXPSPACE has polynomial-size cir-
cuits. But Kannan [Kan82] showed there are sets in EXPSPACE that do not 
have polynomial-size circuits. Hence, A does not have polynomial-size circuits. 
Since A is 
~~-complete for EXPSPACE, SAT E EXPSPACE, and 
P(EXPSPACE)= NP(EXPSPACE)=EXPSPACE, it follows that P(A) = 
NPB(A) = NP(A) = P(A Ef) SAT). 
In a different context, Kurtz [Kur83] showed that with probability one, for 
a randomly selected set A, NP(A) <;f; P(A Ef) SAT). From this one can show 
that with probability one, for a randomly selected set A, NPB(A) #- NP(A) (see 
[Bo091].) 
Since for every set A, NPB(A) ~ NP(A) and NPB(A) ~ P(A Ef) SAT), it is 
of interest to compare the classes discussed above with the class {A I NP(A) = 
NPB(A)} and the class {A I NP(A) ~ P(A Ef) SAT)}. The first of these classes is 
included in the second and from the last paragraph one can see that with proba-
bility one, a randomly selected set is not in the latter class. If the polynomial-time 
hierarchy extends properly up to at least Ef, that is, Llf #- Ef, then a candi-
date for a specific example is SAT since NP(SAT) = Ef, P(SAT Ef) SAT) = 
P(SAT) = Llf, and Llf #- Ef implies that NPB(SAT) #- NP(SAT) (since 
Llf = P(SAT) ~ NPB(SAT) ~ P(SAT Ef) SAT) = Lln· 
If one considers other bounds on the generalized Kolmogorov complexity of 
sets, do similar properties hold? Does the information that NP(A) ~ P(A Ef) 
SAT) yield any information about the generalized Kolmogorov complexity of 
A? It is likely that the answer to the last question is "no," since any set that is 
~~-complete (or ~~,-complete) for EXPSPACE has this property and has high 
generalized Kolmogorov complexity. 

32 
Ronald V. Book 
The reader should note that if C is a class of sets such that for every A E C, 
NPB(A) = NP(A), then showing the existence of a set A E C such that peA) f. 
NP(A) is equivalent to showing that P f. NP. Similarly, if C is a class of sets such 
that for every A E C, NP(A) ~ P(AEB SAT), then showing the existence of a set 
A E C such that NP(A) f. P(AEBSAT) is equivalent to showing that NP f. co-NP 
since it shows the existence of a set A such that NP B(A) f. peA EB SAT). 
Thus, the classification of sets A as to whether NP(A) = NP B(A) or whether 
NP(A) ~ peA EB SAT) has merit. The class of sets with self-producible circuits 
is an example of such a class C and it is the largest known class having those 
properties and being defined in a way that those properties are not themselves 
used. It is of considerable interest to identify the largest class having those 
properties and being defined in a way that those properties are not themselves 
used. Long [Lon85] has shown that there exists a recursive sparse set S such that 
NPB(S) f. NP(S); for this S, peS) f. NP(S) and S does not have self-producible 
circuits. In addition, Long showed that for every sparse set Sl, there exists a 
sparse set S2 such that NP(Sd ~ NPB(S2). Allender and Hemachandra [AH89] 
showed the existence of a sparse set S such that NP(S) !l peS EB SAT). 
There has been a great deal of effort expended in the construction of sets 
that when used as oracle sets result in the separation or collapse of various rel-
ativized complexity classes. For example, there exist sets A and B such that 
peA) = NP(A) and PCB) f. NP(B), but for almost every set C, P(C) f. NP(C). 
What happens if the oracle set is required to be a set with small generalized Kol-
mogorov complexity? Using the characterization of sets with small generalized 
Kolmogorov complexity as being p-isomorphic to tally sets, one can see that the 
situation is different, as shown by Long and Selman [LS86]. 
TheoremS. 
(a) P f. NP if and only if there exists a set A with small generalized K olmogorov 
complexity such that peA) f. NP(A). 
(b) NP f. co-NP if and only if there exists a set A with small generalized K 01-
mogorov complexity such that NP(A) f. co-NP(A). 
Combining Theorem 8 with the characterization of sets with self-producible 
circuits (and simple properties of reducibilities), one obtains the following fact. 
Corollary 9. 
(a) P f. NP if and only if there exists a set A with self-producible circuits such 
that peA) f. NP(A). 
(b) NP f. co-NP if and only if there exists a set A with self-producible circuits 
such that NP(A) f. co-NP(A). 
4 Using Reducibilities to Obtain Information 
Consider another approach to the problem of defining the amount of information 
encoded in a set of strings: the set is considered as an oracle set and operators 

On Sets with Small Information Content 
33 
corresponding to various resource-bounded reducibilities are applied. In Sect. 3, 
the type of reducibility considered was Turing reducibility (or some modification 
of it). But there are circumstances where less computational power is needed to 
compute the reducibility and, hence, to obtain the information encoded in a set. 
In Sect. 2, just two types of reducibilities were described, many-one (~::) and 
Turing (~~). But there are other reducibilities that are studied in complexity 
theory and results about these reducibilities help illustrate this. Some of this 
material is described in this section. 
If f is a function that witnesses A~::B, then for each x, the question "is 
f(x) in B?" is asked just once. If M is an oracle machine that witnesses A~~B, 
then for any x, the computation of M on x relative to B may ask whether a 
computed query string is in B for as many query strings as are generated in the 
computation and, in general, this number is bounded only by the number of steps 
in the computation. Thus, when one considers different notions of reducibilities, 
one property of interest is the number of questions that are allowed in any 
given computation. It turns out that there are infinitely many polynomial-time 
reducibilities that lie between many-one and Turing. 
Turing reducibility has the property that the answer to one query may in-
fluence the generation of the next query string. This means that this type of 
reducibility is "adaptive". The idea of "non-adaptive" reducibilities is cap-
tured by the notion of "truth-table" reducibility where all the query strings are 
generated before any of the oracle queries are made. 
Define reducibilities computed in polynomial time as follows: 
(i) for every k > 0, set A is k-truth-table reducible to set B, written A~~_ttB, if 
there exist polynomial time computable functions f and 9 such that for all x, 
f(x) is a list of k strings, g(x) describes a boolean circuit with k inputs, and 
x E A if and only if the circuit given by g(x) evaluates to true on the k-tuple 
(XB (Yl), ... 'XB (Yk)) where f(x) = (Yl, ... ,Yk) and XB is the characteristic 
function of B; 
(ii) set A is bounded truth-table reducible to set B, written A~~ttB, if there is 
an integer k such that A~r-ttB; 
(iii) set A is truth-table reducible to set B, written A~ftB ifthere exist polynomial 
time computable functions f and 9 such that for all x, f( x) is a list of strings, 
g(x) is a truth-table with the number of variables being equal to the number 
of strings in the list J(x), and x E A if and only if the truth-table g(x) 
evaluates to true on (XB(Yl), ... ,XB(Yk)) where J(x) = (Yl, ... ,Yk). 
(For properties of these reducibilities, see [LLS75].) 
What is important to notice is that truth-table reducibilities are non-adaptive, 
that is, the list of query strings is computed before any evaluations are made so 
that the i + 18t string on the list does not depend on whether any of the first i 
strings on the list happen to be in the oracle set. 
There are several interesting results about sets that may be reducible to 
sparse sets. 

34 
Ronald V. Book 
Theorem 10. 
(a) [OW91] If there is a sparse set that is ~~tt-hard for NP, then P = NP. 
(b) [KL82] If there is a sparse set that is ~~-hard for NP, then the polynomial-
time hierarchy collapses to Ef. 
(c) [OL91] If there is a sparse set that is ~~tt-hard for PSPACE, then P = 
PSPACE. 
(d) (KL82] If there is a sparse set that is ~~-hardforPSPACE, then PSPACE = 
Ef. 
(e) (BBS86a] The polynomial-time hierarchy collapses if and only if for every 
sparse set S, the polynomial-time hierarchy relative to S collapses if and only 
if there exists a sparse set S such that the polynomial-time hierarchy relative 
to S collapses. 
(f) [BBS86a] The union of the classes in the polynomial-time hierarchy is equal 
to PSPACE if and only if for every sparse set S, the union of the classes in 
the polynomial-time hierarchy relative to S is equal to PSPACE relative to 
S if and only if there exists a sparse set S such that the union of the classes 
in the polynomial-time hierarchy relative to S is equal to PSPACE relative 
to S. 
Most researchers in the field believe that the polynomial-time hierarchy ex-
tends to infinitely many levels. The results in Theorem 10 suggest that the 
information encoded in a sparse set is not powerful enough to force a collapse of 
the hierarchy even if a powerful computational operator is used to retrieve that 
information. 
In contrast, consider the algorithmically random sets of Martin-Lof [Mar66], 
sets that encode a great deal of information. It has been shown recently [BLW92] 
that even with a powerful computational operator, it may be extremely difficult 
to retrieve that information. This is illustrated by the following result. 
Theorem 11. 
(a) If there exists an algorithmically random set that is ~~tt -hard for NP, then 
P = NP. 
(b) If there exists an algorithmically random set that is ~~tt -hard for PSPACE, 
then P = PSPACE. 
(c) If there exists an algorithmically random set that is ~~ -hard for NP, then 
the polynomial-time hierarchy collapses. 
(d) If there exists an algorithmically random set that is ~~-hard for PSPACE, 
then PH=PSPACE. 
5 A Decomposition of P / poly 
Since Theorem 1 essentially identifies the sets with small generalized Kolmogorov 
complexity with the class of tally sets, certain results about the class of sets 
with polynomial-size circuits are of interest. Recall from Theorem 3 that a set 

On Sets with Small Information Content 
35 
has polynomial-size circuits if and only if it is Turing reducible in polynomial 
time to some tally set. Some results about sets reducible to tally sets by means 
of these reducibilities will be reviewed here. 
Recall that truth-table reducibilities are non-adaptive, that is, the list of 
query strings is computed before any evaluations are made so that the i + 1st 
string on the list does not depend on whether any of the first i strings on the 
list happen to be in the oracle set. 
For any reducibility r computed in polynomial time and any class C of sets, 
let Pr(C) = {A I there exists C E C such that A~~ B}. Also, for any truth-table 
reducibility r computed in polynomial time, A=; B denotes the fact that A ~; B 
and B~; A. For any class C of sets, let E;(C) denote {A I there exists C E C 
such that A=;C}. In addition, write PT(C) for P(C). Thus, for any reducibility 
r and for any class C, E;(C) ~ Pr(C). From Theorem 3, this notation can 
be used to denote the class of sets with polynomial size circuits in each of the 
following ways: PT(SPARSE), PT(TALLY), PT(K[log, poly]), or P /poly; and 
the class of sets with self-producible circuits can be denoted Ei(TALLY) or 
Ei(K[log, poly]). 
It is clear that for any tally set T, A~iT if and only if A~ftT. Thus, 
P/poly = Ptt(TALLY) = Ptt(K[log, poly]). What relationship do classes such 
as Pk-tt(TALLY) or Pbtt(TALLY) have with P/poly? Book and Ko [BK88] in-
vestigated this question and obtained the following results. 
Theorem12. Pbtt(TALLY) = Pm (TALLY) and Pbtt(TALLY) =1= P/poly. 
Thus, while there are infinitely many different polynomial time reducibilities, 
there are 
only 
two 
classes 
of the form 
Pr(TALLY), 
P /poly 
and 
Pm(TALLY). The reader must be cautioned that this should not be interpreted 
as saying that for every tally set T, Pm(T) = Pbtt(T); this is certainly not the 
case as was shown by Tang and Book [TB91]. 
Theorem 13. 
(a) For almost every tally set T, Pm(T) =1= P1-tt(T) and Pbtt(T) =1= Ptt(T). 
(b) For every k > ° and for almost every tally set T, Pk-tt(T) =1= PCk+l)-tt(T). 
Given the fact that Ptt(TALLY) = PT(TALLY) and Pm (TALLY) = 
Pbtt (TALLY), there is the question of whether the class TALLY (equivalently, 
K[log, poly]) can be used to separate the reducibilities ~ft and ~i. Tang 
and Book [TB88] determined that this could be done if classes of the form 
E; (TALLY) were considered. 
Theorem 14. Eft (TALLY) =1= Ei(TALLY) and E~tt(TALLY) =1= Eft (TALLY). 
But Tang and Book were not able to determine whether Ef:tt(T ALLY) is 
equal to E;, (TALLY). Based on the work of Tang and Book, Allender and 
Watanabe [AW88] developed a very interesting result. 
Theorem 15. Either E;,(TALLY) 
E~tt(TALLY), or E;,(TALLY) 
=1= 
Ei_tt(TALLY) and for every k > 0, Er_tt(TALLY) =1= Efk+ll_tt(TALLY). 

36 
Ronald V. Book 
Allender and Watanabe described a certain condition Q with the prop-
erty that if Q is true, then E~(TALLY) = E~tt(TALLY), and if Q is false, 
then E~(TALLY) =1= Ei_tt(TALLY) and for every k > 0, E~_tt(TALLY) =1= 
Efk+1)-tt (TALLY). Condition Q is a statement about the nonexistence of a spe-
cific type of "one-way" function, and condition Q holds if and only if for every 
length-increasing function f : E* -> {O}* computable in polynomial time, there 
exists a t such that for all x in the image of f, f- 1 (x) n K[t ·log n, nt] =1= ¢ where 
n = Ixl. Thus, once again sets with small generalized Kolmogorov complexity 
arise. 
Recently, Gavalda and Watanabe [GW91] showed that no two of the following 
classes are equal: Eit(SPARSE), E~(SPARSE), EiN (SPARSE), E~P(SPARSE), 
where SN denotes the strong nondeterministic polynomial-time reducibility stud-
ied by Long [Lon82]. In addition, they showed E~(TALLY) Sf:: Eit(SPARSE). 
6 Lowness 
There is another approach to defining the amount of information encoded in a 
set of strings which is related to the approach discussed in Sect. 4. 
One way to quantify the notion of the "power" of an oracle set is to consider 
notions of "lowness". The notion of lowness (from recursive function theory) was 
introduced in computational complexity theory by Schoning [Sch83]. In general, 
the notion of lowness with respect to the classes E;, i > 0, may be interpreted 
as setting an upper bound on the amount of information that can be encoded in 
the set, that is, a low set in the polynomial-time hierarchy lIas the power of only 
a bounded number of alternating quantifiers or, equivalently, a bounded number 
of applications of the NP( )-operator. The formal definitions follow. 
For n 2: 0, define Hn = {A E NP I E~+1 £; E~(A)} and Ln = {A E NP I 
E~ (A) £; E~}. (Notice that it follows trivially from the definitions that for 
every set A, E~ £; E~(A), and for every set A E NP, E~(A) £; E~+1.) Define 
HH = Un;:::oHn and LH = Un;:::oLn. A set in NP is high if it is in HH and is low if 
it is in LH. The structure Ho £; HI £; H2 £; ... is the high hierarchy within NP 
and the structure Lo £; Ll £; L2 £; ... is the low hierarchy within NP. 
Set A is in Hn if it encodes information that is equivalent to the power of 
an additional application of the NP( )-operator (NP(E~) = E~+1 £; E~ (A)): 
Ho is the class of all sets that are complete for NP with respect to polynomial-
time Turing reducibilities. Set A is in Ln if continued application of the NP( )-
operator yields no more than if the set were empty set (NP (E~ (A)) £; NP (E~) = 
NP(E~(¢))); Lo is the class P. 
Some of the results (from [Sch83j, [KS85]) about the high and low hierarchies 
within NP are given in the following: 
Theorem 16. 
(a) For every n 2: 0, either Ln = Hn = NP or Ln n Hn = ¢. 
(b) For every n 2: 0, if E~ = E~+l> then Ln = Hn = NP. 
(c) For every n 2: 0, if E~ =1= E~+l> then Ln n Hn = ¢. 

On Sets with Small Information Content 
37 
(d) The polynomial-time hierarchy extends to infinitely many levels if and only 
ifLHnHH = ¢. 
(e) If the polynomial-time hierarchy extends to infinitely many levels, then NP # 
LHuHH. 
(f) Every sparse set (hence, every set with small generalized Kolmogorov com-
plexity) in NP is in L 2 • 
(g) Every set in NP that has polynomial-size circuits is in L 3 . 
While Schoning defined the notions of highness and lowness only for sets 
in NP, others generalized them to discuss sets in other classes. Two of those 
generalizations will be described briefly, the first being due to Balcazar, Book, 
and Schoning [BBS86b]. 
For every n > 0, define EHn = {A I L'~(A EJ1 SAT) ~ L'~(A)} and ELn = 
{A I L'~(A) ~ L'~_l (AEJ1SAT)}. A set is extended high if it belongs to Un>oEHn 
and is extended low if it belongs to Un>oLHn. 
Theorem 17. 
1. If a set has self-producible circuits, then it is in ELI; hence, every set with 
small generalized K olmogorov complexity is in ELI. 
2. If a set has polynomial-size circuits, then it is in EL3 ; hence, every sparse 
set is in EL3 . 
3. Either every sparse set is extended low, in which case the polynomial-time 
hierarchy collapses, or no sparse set is extended low, in which case the 
polynomial-time hierarchy extends to infinitely many levels. 
Notice that the property NP(A) ~ P(A EJ1 SAT) considered in Section 3 is 
precisely the property of being in ELI. 
Very sharp bounds on highness and lowness have been announced by Long 
and Sheu [LS91]. 
Another setting in which the notion of lowness has been investigated is that 
of exponential time computation. Let DEXT = Uc>oDTIME(2cn). 
If a set A has the property that DEXT(A) = DEXT, then A is exponentially 
low. 
It is not difficult to show that if A has small generalized Kolmogorov com-
plexity, then A is exponentially low if and only if A is in the class P. It would 
be interesting to classify the sparse sets that are exponentially low. 
Theorem 18. There is a sparse set that is exponentially low but is not in P. 
This result is due to Book, Orponen, Russo, and Watanabe [BORW88]. They 
constructed a (very) sparse set S by choosing elements of high generalized Kol-
mogorov complexity to put into S. In fact, S n Ku[n/2, 23n] = ¢, so that no 
element x of S can be produced from a string of length Ixl /2 within 231 "'1 steps 
and S ¢ P. But the elements are chosen such that S E DEXT. For any set L 
in DEXT(S), an exponential-time oracle machine M recognizing L relative to S 
can be simulated by an exponential time machine that does not use an oracle. 

38 
Ronald V. Book 
During its computation on input of length n, the machine M cannot produce 
any string in S of length greater than cn (for some c depending only on M). 
But the machine can determine the answer to a query about membership of a 
shorter string in S by simulating an exponential time machine that recognizes 
S. This shows that L is in DEXT so that S is exponentially low. 
7 Characterizations of Complexity Classes 
When studying structural complexity theory it is often desirable to have intrinsic 
characterizations of complexity classes, that is, to describe membership in the 
class in terms that do not involve the same concepts used in defining the class. 
In Sect. 3 intrinsic characterizations of the class K[log, poly] and the class 
of sets with self-producible circuits were described; in both cases the character-
izations involved reducibilities. 
A different type of characterization by means of reducibilities was introduced 
by Bennett and Gill [BG81] and refined by Ambos-Spies [Amb86]. Of course, for 
every set A and every k 2: 0, A is in E~ if and only if for every set B, A is in 
E~(B). The characterizations introduced by Bennett and Gill and by Ambos-
Spies are different in the sense that they involve reductions to "almost every" 
oracle set in a measure-theoretic (i.e., probabilistic) sense. Ambos-Spies proved 
that for every set A, A is in the class P if and only if for almost every set B, A 
is many-one reducible in polynomial time to B. Bennett and Gill showed that A 
is in the class BPP if and only if for almost every set B, A is Turing reducible 
in polynomial time to B. (Recall that BPP is the class defined by probabilistic 
machines that operate in polynomial time and have bounded error probability.) 
Similar characterizations were obtained by Babai and Moran [BM88] and 
others who were studying the power of interactive proof systems. Generalizing 
from properties of the class BPP, Schoning [Sch86] defined the "BP-operator" 
and studied its properties. For any class C, BP . C is the class of sets A such 
that for some B in C, some polynomial p( n), and all x E E*, 
Prp(l",n[Y: x E A if and only if (x,y) E B((x,y))] > 3/4, 
where for any predicate P and natural number m, Prm[Y : P(y)] is the con-
ditional probability Prm[P/Em] = 2-m x 
II {y I P(y) and Iyl = m} II. As 
observed by Babai and Moran, the "Arthur-Merlin" class AM can be character-
ized as BP . NP, so that it is the nondeterministic counterpart to BPP. 
The above results lead to the following question: can membership in BP . E~ 
be characterized by means of oracles? That is, is it the case that for every set 
A, A E Bp· E~ if and only if for almost every oracle set B, A E E~(B)? 
Tang and Watanabe [TW89] answered this question by showing that for every 
integer k 2: 0 and for every set A, A E BP . E~ if and only if for almost every 
tally set T, A E E~ (T). The reader should observe that in some sense this is the 
"minimal" answer. The reason for this informal comment is that it is difficult 
to see how the results of Tang and Watanabe can hold for any class with "less 
information" than the class of tally sets, since the class of tally sets is, up to 

On Sets with Small Information Content 
39 
p-isomorphism, precisely K[log, poly], the class of sets with small generalized 
Kolmogorov complexity. 
The results of Tang and Watanabe were presented at the Second IEEE Con-
ference on Structure in Complexity Theory in June 1988. In October 1988, Nisan 
and Wigderson [NW88] presented what can be considered as the "maximal" an-
swer to these questions by showing that for every set A, A E AM if and only if 
for almost every set B, A E NP(B); this result can be extended to show that for 
every k > 0 and every set A, A E BP . E~ if and only if for almost every set B, 
A E EnB). 
Given the characterizations of the classes BP . E{ in the probabilistic poly-
nomial-time hierarchy, it is reasonable to ask if there are similar characteriza-
tions of the classes E~ in the polynomial-time hierarchy. (The reader should 
note that it is not known whether there exists a k such that E~ = BP . E~ or 
whether there exists a k such that E~+l = BP . E~. However, Bp· PH = PH 
and Bp· PSPACE = PSPACE.) Tang and Book [TB91] extended the character-
ization of P given by Ambos-Spies by showing that for every set A, A E P if and 
only if for almost every set B, A is (log n)-Turing reducible in polynomial time 
to B. (Polynomial-time (logn)-Turing reducibility is simply the restriction of 
Turing reducibility obtained by demanding that each deterministic polynomial-
time bounded oracle machine make at most c . log n queries in its computation 
relative to any oracle set on any input of length n, where c is a constant that de-
pends only on the machine.) Thus, no reducibility computed in polynomial time 
with power strictly between many-one and (log n)-Turing (for example, bounded 
truth-table lies strictly between these reducibilities) can characterize in this way 
any class other than P. 
Book and Tang [BT90] developed characterizations for each of the classes 
E~ (and II~ and Lln, k > 0, by defining the notion of "Erlogn-truth-table 
reducibility, S~~n-tt''' The characterizations follow easily from the definitions: 
for every k and for every set A, A E E~ if and only if for almost every set 
E P 
B, ASlo~n-ttB. If one uses sets in K[log, poly] as the oracle sets, then the 
analogous results hold so that once again sets in K[log, poly] provide "minimal" 
solutions. 
Acknowledgement 
It is a pleasure to thank Ms. Shilo Brooks for preparing the manuscript in the 
style requested by the editor. 
References 
[A1l89] 
E. Allender. Limitations of the upward separation technique. In Proc. 16th 
Int. Colloq. Automata, Languages, and Programming, Springer-Verlag, Lec-
ture Notes in Computer Science 372:186-194, 1989. 

40 
Ronald V. Book 
[AH89] 
E. Allender and L. Hemachandra. Lower bounds for the low hierarchy. In 
Automata, Languages, and Programming, Springer-Verlag, Lecture Notes 
in Computer Science 372:31-45,1989. 
[AR88] 
E. Allender and R. Rubinstein. P-printable sets. SIAM J. Computing 
17:1193-1202,1988. 
[AW88] 
E. Allender and O. Watanabe. Kolmogorov complexity and degrees oftally 
sets. Info. and Computation 86:160-178, 1990. 
[Amb86] 
K. Ambos-Spies. Randomness, relativizations, and polynomial reducibili-
ties. In Proc. 1st Conference on Structure in Complexity Theory, Springer-
Verlag, Lecture Notes in Computer Science 223:23-34, 1986. 
[BGS75] 
T. Baker, J. Gill, and R. Solovay. Relativizations ofthe P =? NP question. 
SIAM J. Computing 4:431-442,1975. 
[BB86] 
J. Balca.zar and R. Book. Sets with small generalized Kolmogorov com-
plexity. Acta Informatica 23:679-688, 1986. 
[BBS86a] 
J. Balcazar, R. Book, and U. Schoning. The polynomial-time hierarchy and 
sparse oracles. J. Assoc. Comput. Mach. 33:603-617,1986. 
[BBS86b] 
J. Balcazar, R. Book, and U. Schoning. Sparse sets, lowness, and highness. 
SIAM J. Computing 15:739-747, 1986. 
[BDG88] 
J. Balcazar, J. Dfaz, and J. Gabarro. "Structural Complexity" vol. I and 
II, Springer-Verlag, 1988 and 1990. 
[BM88] 
L. Babai and S. Moran. Arthur-Merlin games: a randomized proof system, 
and a hierarchy of complexity classes. J. Comput. System Sci. 36:254-276, 
1988. 
[BG81] 
C. Bennett and J. Gill. Relative to a random oracle, pA # N pA # co-N pA 
with probability one. SIAM J. Computing 10:96-113, 1981. 
[BH77] 
L. Berman and J. Hartmanis. On isomorphism and density of N P and 
other complete sets. SIAM J. Computing 16:305-322, 1977. 
[Boo91] 
R. Book. Some observations on separating complexity classes. SIAM J. 
Computing 20:246-258, 1991. 
[BK88] 
R. Book and K. Ko. On sets truth-table reducible to sparse sets. SIAM J. 
Computing 17:903-919, 1988. 
[BLS84] 
R. Book, T. Long, and A. Selman. Quantitative relativizations of complex-
ity classes. SIAM J. Computing 13:461-487, 1984. 
[BLW92] 
R. Book, J. Lutz, and K. Wagner. On complexity cores and algorithmically 
random languages. In Proc. STACS 92, to appear. 
[BORW88] R. Book, P. Orponen, D. Russo, and O. Watanabe. Lowness properties 
of sets in the exponential-time hierarchy. SIAM J. Computing 17:504-516, 
1988. 
[BT90] 
[GJ79] 
[GW91] 
[Har83] 
[HH88] 
R. Book and S. Tang. Characterizing polynomial complexity classes by 
reducibilities. Mathematical Systems Theory 23:165-174, 1990. 
M. Garey and D. Johnson. "Computers and Intractability - a guide to the 
theory of N P-completeness", W. H. Freeman 1979. 
R. Gavalda and O. Watanabe. On the computational complexity of small 
descriptions. In Proc. 6th IEEE Conference on Structure in Complexity 
Theory 89-101,1991. 
J. Hartmanis. Generalized Kolmogorov complexity and the structure offea-
sible computations. In 24th IEEE Symp. Foundations of Computer Science 
439-445,1983. 
J. Hartmanis and L. Hemachandra. On sparse oracles separating feasible 
complexity classes. Info. Processing Letters 28:291-295, 1988. 

On Sets with Small Information Content 
41 
[HIS83] 
[Huy87] 
[Kan82] 
[KL82] 
[K085] 
[K086] 
[KS85] 
[Kur83] 
[LLS75] 
[LV90] 
[Lon82] 
[Lon85] 
[LS86] 
[LS91] 
[Mar66] 
[NW88] 
[OL91] 
[OW91] 
[Pip79] 
[Sch83] 
[Sch86] 
[Sch89] 
[Sip83] 
J. Hartmanis, N. Immerman, and V. Sewelson. Sparse sets in NP-P: EXP-
TIME versus NEXPTIME. In Proc. 15th ACM Symp. Theory of Computing 
382-391, 1983. 
D. Huynh. On solving hard problems by polynomial-size circuits. Info. 
Processing Letters 24:171-176, 1987. 
R. Kannan. Circuit-size lower bounds and nonreducibility to sparse sets. 
Info. Control 55:40-56, 1982. 
R. Karp and R. Lipton. Turing machines that take advice. L 'Enseignement 
Mathematique 28:191--209, 1982. 
K. Ko. Continuous optimization problems and a polynomial hierarchy of 
real functions. J. Complexity 1:210-231, 1985. 
K. Ko. On the notion of infinite pseudorandom sequences: Theoret. Com-
put. Sci. 39:9-33, 1986. 
K. Ko and U. Schoning. On circuit-size complexity and the low hierarchy 
in N P. SIAM J. Computing 14:41-51, 1985. 
S. Kurtz. On the random oracle hypothesis. Info. and Control 57:40-47, 
1983. 
R. Ladner, N. Lynch, and A. Selman. A comparison of polynomial-time 
reducibilities. Theoret. Comput. Sci. 1:103-123, 1975. 
M. Li and P. Vitanyi. Applications of Kolmogorov complexity in the the-
ory of computation. Complexity Theory Retrospective, A. Selman (ed.), 
Springer-Verlag Publ. Co. 147-203,1990. 
T. Long. Strong nondeterministic polynomial-time reducibilities. Theoret. 
Comput. Sci. 21:1-25, 1982. 
T. Long. On restricting the size of oracles compared with restricting access 
to oracles. SIAM J. Computing 14:585-597, 1985. 
T. Long and A. Selman. Relativizing complexity classes with sparse oracles. 
J. Assoc. Comput. Mach. 33:616-627,1986. 
T. Long and M.J. Sheu. A refinement of the low and high hierarchies. 
Submitted for publication, 1991. 
P. Martin-Lof. On the definition of random sequences. Info. and Control 
9:602-619, 1966. 
N. Nisan and A. Wigderson. Hardness vs. randomness. In Proc. 29th IEEE 
Symp. Foundations of Comput. Sci. 2-11, 1988. 
M. Ogiwara and A. Lozano. On one query self-reducible sets. In Proc. 6th 
IEEE Conference on Structure in Complexity Theory. 139-151,1991. 
M. Ogiwara and O. Watanabe. On polynomial-time bounded truth-table 
reducibility of N P sets to sparse sets. SIAM J. Computing 20:471-483, 
1991. 
N. Pippinger. On simultaneous resource bounds. In Proc. 20th IEEE Symp. 
Found. Comput. Sci. 307-311, 1979. 
U. Schoning. A low and a high hierarchy within N P. J. Comput. System 
Sci. 27:14-28, 1983. 
U. Schoning. Complexity and Structure. In Springer-Verlag, Lecture Notes 
in Computer Science 211, 1986. 
U. Schoning. Probabilistic complexity classes and lowness. J. Comput. Sys-
tem Sci. 39:84-100, 1989. 
M. Sipser. A complexity-theoretic approach to randomness. In Proc. 15th 
ACM Symp. Theory of Computing 330-335, 1983. 

42 
[TB88] 
[TB91] 
[TW89] 
[Wat87] 
Ronald V. Book 
S. Tang and R. Book. Separating polynomial-time Turing and truth-
table reductions by tally sets. In Automata, Languages, and Programming, 
Springer-Verlag, Lecture Notes in Computer Science 317:591-599, 1988. 
S. Tang and R. Book. Polynomial-time reducibilities and "almost-all" or-
acle sets. Theoret. Comput. Sci. 81:36-47, 1991. 
S. Tang and O. Watanabe. On tally relativizations of BP-complexity 
classes. SIAM 1. Computing 18:449-462, 1989. 
O. Watanabe. A comparison of polynomial time completeness notions. The· 
oret. Comput. Sci. 54:249-265,1987. 

Kolmogorov Complexity, Complexity Cores, 
and the Distribution of Hardness* 
David W. Juedes and Jack H. Lutz 
Department of Computer Science 
Iowa State University 
Ames, IA 50011, USA 
juedes@iastate.edu, lutz@iastate.edu 
Abstract. Problems that are complete for exponential space are prov-
ably intractable and known to be exceedingly complex in several techni-
cal respects. However, every problem decidable in exponential space is 
efficiently reducible to every complete problem, so each complete prob-
lem must have a highly organized structure. The authors have recently 
exploited this fact to prove that complete problems are, in two respects, 
unusually simple for problems in expontential space. Specifically, every 
complete problem must have ususually small complexity cores and un-
usually low space-bounded Kolmogorov complexity. It follows that the 
complete problems form a negligibly small subclass of the problems de-
cidable in exponential space. This paper explains the main ideas of this 
work. 
1 Introduction 
It is well understood that an object that is complex in one sense may be simple 
in another. In this paper we show that every decision problem that is complex in 
one standard, complexity-theoretic sense must be unusually simple in two other 
such senses. 
Throughout this paper, the terms "problem," "decision problem," and "lan-
guage" are synonyms and refer to a set A ~ {a, 1}*, i.e., a set of binary strings. 
The three notions of complexity considered are completeness (or hardness) for 
a complexity class, space-bounded Kolmogorov complexity, and the existence 
of large complexity cores. (All terms are defined and discussed in §§2-6 below, 
so this paper is essentially self-contained.) In a certain setting, we prove that 
every problem that is complete for a complexity class must have unusually low 
space-bounded Kolmogorov complexity and unusually small complexity cores. 
Thus complexity in one sense implies simplicity in another. 
To 
be 
specific, 
we 
work 
with 
the 
complexity 
class 
ESPACE 
= DSPACE(2linear). There are two related reasons for this choice. First, ESPACE 
* This research was supported in part by National Science Foundation Grants CCR-
8809238 and CCR-9157382 and in part by DIMACS, where the second author was 
a visitor while part of this work was carried out. 

44 
David W. Juedes and Jack H. Lutz 
has a rich, well-behaved structure that is well enough understood that we can 
prove absolute results, unblemished by oracles or unproven hypotheses. In par-
ticular, much is known about the distribution of Kolmogorov complexities in 
ESPACE [Lut92a, §4 below], while very little is known at lower complexity 
levels. Second, the structure of ESPACE is closely related to the structure of 
polynomial complexity classes. For example, Hartmanis and Yesha [HY84] have 
shown that 
E ~ ESPACE {:=::} P ~ P /Poly n PSPACE. 
This, together with the first reason, suggests that the separation of P from 
PSPACE might best be achieved by separating E from ESPACE. We thus seek 
a detailed, quantitative account of the structure of ESPACE. 
For simplicity of exposition, we work with polynomial time, many-one re-
ducibility ("~;':-reducibility"), introduced by Karp[Kar72]. Problems that are 
~;':-complete for ESPACE have been exhibited by Meyer and Stockmeyer [MS72], 
Stockmeyer and Chandra[SC89], and others. Such problems are correctly re-
garded as exceedingly complex. They are provably intractable in terms of compu-
tational time and space. They have exponential circuit-size complexity [Kan82], 
weakly exponential space-bounded Kolmogorov complexity [Huy86], and dense 
complexity cores [OS86, Huy87]. Problems that are ~;':-hard for ESPACE have 
all these properties and need not even be recursive. 
Notwithstanding these lower bounds on the complexity of ~;':-hard problems 
for ESPACE, we will prove in §6 below that such problems are unusually simple 
in two respects. The word "unusually" here requires some explanation. 
Suppose that we choose a language A ~ {0,1}* probabilistically, according 
to a random experiment in which an independent toss of a fair coin is used to 
decide membership of each string x E {O, I} * in A. For a set X of languages, 
let Pr(X) = PrA[A E X] denote the probability that A E X (" the probability 
that event X occurs") in this experiment, provided that this probability exists. 
(All sets X of languages considered in this paper are Lebesgue measurable, so 
that Pr(X) is well-defined. Thus we will not concern ourselves with issues of 
measurability.) If the event X has the property that Pr(X) = 1, then we say 
that almost every language A ~ {O, I} * is in X. In such a case, the complement 
Xc of X has probability Pr(X C ) = 0, so it is unusual for a language A to be 
in Xc. In particular, a language A is unusually simple in the sense of a given 
complexity measure if there is a lower complexity bound that holds for almost 
all languages but does not hold for A. 
This probabilistic notion of "almost every" and "unusual" is intuitive and 
suggestive of our intent, but is not strong enough for our purposes. As we have 
noted, we seek to understand the structure of ESPACE. Accordingly, we will 
prove in §6 below that ::;;':-hard problems for ESPACE are unusually simple 
for problems in ESPACE in two specific senses. This means that, in each of 
these senses, there is a lower complexity bound that holds for almost every 
language in ESPACE but does not hold for languages that are ::;;':-hard for 
ESPACE. This immediately yields a quantitative result on the distribution of 
~;':-complete problems in ESPACE: Almost every language in ESPACE fails to 
be ~;':-complete. 

Kolmogorov Complexity, Cores, and Hardness 
45 
But what does it mean for "almost every language in ESPACE" to have 
some property? Naively, we would like to say that almost every language is ES-
PACE is in some set X if, in the above random experiment, Pr(XIESPACE) 
= PrA[A E XIA E ESPACE] = 1. The problem here is that ESPACE is 
a countable set of languages, so PrA[A E ESPACE] = 0, so the conditional 
probability Pr(XIESPACE) is not defined. We thus turn to resource-bounded 
measure, a complexity-theoretic generalization of Lebesgue measure developed 
by Lutz[Lut92a, Lut92b]. Suppose we are given a resource bound, e.g., the set 
pspace, consisting of all functions computable in polynomial space. Then resource-
bounded measure theory defines the pspace-measure JLpspace(X) of a set X oflan-
guages (provided that X is pspace-measurable). In all cases, ° ::; JLpspace(X) ::; 1. 
If JLpspace(X) = ° or JLpspace(X) = 1, then Pr(X) = ° or Pr(X) = 1, respectively, 
but the pspace-measure conditions are much stronger than this: It is shown in 
[Lut92a, Lut92b] that, if J.tpspace(X) = 0, then XnESPACE is a negligibly small 
subset of ESPACE. In fact, pspace-measure induces a natural, internal, measure 
structure on ESPACE. In this structure, a set X of languages has measure 0 
in ESPACE, and we write J.t(XIESPACE) = 0, if J.tpspace(X n ESPACE) = o. 
A set X has measure 1 in ESPACE, and we write JL(XIESPACE) = 1, if 
J.t(XCIESPACE) = O. Finally, we say that almost every language in ESPACE 
is in some set X of languages if JL(XIESPACE) = 1. In §3 below we summarize 
those aspects of resource-bounded measure that are used in this paper. 
Kolmogorov complexity, discussed in several papers in this volume, was intro-
duced by Solomonoff[So164], Kolmogorov[Ko165]' and Chaitin[Cha66]. Resource-
bounded Kolmogorov complexity has been investigated extensively [Ko165, Har83, 
Sip83, Lev84, Lon86, BB86, Huy86, K086, AR88, A1l89, AW90, Lut90, Lut92a, 
etc.]. In this paper we work with the space-bounded Kolmogorov complexity of 
languages. Roughly speaking, for A ~ {a, 1}*, n E N, and a space bound t, the 
space-bounded Kolmogorov complexity K st (A=n) is the length of the shortest 
program that prints the 2n-bit characteristic string of A=n = An {a, l}n, using 
at most t units of workspace. This quantity Kst(A=n) is frequently interpreted 
as the "amount of information" that is contained in A=n and is "accessible" 
by computation using::; t space. In §4 below, we review the precise formula-
tion of this definition (and the analoguous definition of Kst(A::;n)) and some 
of its properties. After surveying some recent complexity-theoretic applications 
of an almost-everywhere lower bound on K st(A::;n)[Lut92a], we prove a new 
almost everywhere lower bound result (Theorem 6jCorollary 7) showing that 
for all c E Nand E > 0, almost every language A E ESPACE has space-bounded 
Kolmogorov complexity 
KS2cn (A=n) > 2n - n€ a.e. 
(This improves the 2n - 2m lower bound of [Lut92a].) It should be noted that 
the proof of this result is the only direct use of resource-bounded measure in 
this paper. All the measure-theoretic results in §5-6 are proven by appeal to this 
almost everywhere lower bound on space-bounded Kolmogorov complexity. 
In §5 , we review the fundamental notion of a complexity core, introduced 
by Lynch[Lyn75] and investigated by many others [Du85, ESY85, Orp86, OS86, 

46 
David W. Juedes and Jack H. Lutz 
BD87, Huy87, R087, BDR88, DB89, Ye90, etc.). Intuitively, a complexity core 
for a language A is a fixed set K of inputs such that e11ery machine whose 
decisions are consistent with A fails to decide efficiently on almost all elements 
of K. The meanings of "efficiently" and "almost all" are parameters of this 
definition that may be varied according to the context. In §5, in order to better 
understand ESPACE, we work with DSPACE(2Cn)-complexity cores (for fixed 
constants c). In Theorem 9 we prove that any upper bound on the densities of 
DSPACE(2Cn)-complexity cores for a language A implies a corresponding upper 
hound on the space-bounded Kolmogorov complexity of A. The quantitative 
details imply that almost every language in ESPACE has co-sparse complexity 
cores. 
In §6, we apply these results to our main topic, which is the complexity 
and distribution of ~;,-hard problems for ESPACE. It is well-known that such 
problems are not feasibly decidable and must obey certain lower bounds on their 
complexities. As noted above, Huynh[Huy86] has proven that every ~;,-hard for 
ESPACE has weakly exponential (i.e., > 2n ' for some E > 0) space-bounded 
Kolmogorov complexity; and Orponen and Schoning[OS86] have (essentially) 
proven that every ~;,-hard language for ESPACE has a dense DSPACE(2cn)-
complexity core. Intuitively, such results are not surprising, as we do not expect 
hard problems to be simple. However, in §6, we prove that these hard problems 
must be simple in that they obey upper bounds on their complexities. In Theorem 
13 we prove that every DSPACE(2n )-complexity core of every ~;,-hard language 
for ESPACE must have a dense complement. Note that this upper bound is the 
"mirror image" of the Orponen-Schoning lower bound cited above: Every hard 
problem has a dense core, but this core's complement must also be dense. In 
Theorem 14 we use Theorems 9 and 13 to prove that every ~;,-hard language 
for ESPACE has space-bounded Kolmogorov complexity that is less than 2n by 
a weakly exponential amount. Again, note that this upper bound is the "mirror 
image" of the Huynh lower bound cited above. 
We have seen that almost every language in ESPACE has co-sparse com-
plexity cores and essentially maximal Kolmogorov complexity. Thus our upper 
bounds imply that the ~;,-complete problems have unusually low space-bounded 
Kolmogorov complexity and unusually small complexity cores for problems in 
ESPACE. It follows that the ~;,-complete problems form a measure 0 subset of 
ESPACE. 
In order to simplify the exposition of the main ideas and to highlight the role 
played by Kolmogorov complexity, we do not state our results in the strongest 
possible form in this volume. The interested reader may wish to consult the tech-
nical paper [JL92] for a more thorough treatment of these issues. For example, 
it is shown in [JL92] that ~;,-hard problems for E have unusually small com-
plexity cores, whence the ~;,-complete problems for E form a measure 0 subset 
of E. (Note added in proof: Recently, Mayordomo[May91] has independently 
proven that the ~;,-complete problems for E form a measure 0 subset of E. 
Mayordomo's proof exploits the Berman [Ber76] result that every ~;,-complete 
problem for E has an infinite subset in P.) 

Kolmogorov Complexity, Cores, and Hardness 
47 
2 Preliminaries 
Most of our notation and terminology is standard. We deal with strings, lan-
guages, functions, and classes. Strings are finite sequences of characters over the 
alphabet {0,1}; we write {0,1}* for the set of all strings. Languages are sets 
of strings. Functions usually map {O, I} * into {O, I} *. A class is either a set of 
languages or a set of functions. 
When a property ¢( 11,) of the natural numbers is true for all but finitely many 
11, EN, we say that ¢( 11,) holds almost everywhere (a. e.) . Similarly, ¢( 11,) holds 
infinitely often (i.o.), if ¢(n) is true for infinitely many 11, E N. We write [¢~ for 
the Boolean value of a condition ¢. That is, [¢] = 1 if ¢ is true, ° 
if ¢ is false. 
If x E {0,1}* is a string, we write Ixl for the length of x. If A ~ {0,1}* is 
a language, then we write A C , A<n, and A=n for {O, 1}* \ A, An {O, 1}~n, and 
An {O, 1 } n respectively. The seqU:-ence of strings over {O, 1}, So = >., Sl = 0, S2 = 
1, S3 = 00, ... , is referred to as the standard lexicographic enumeration of {O, 1} *. 
The characteristic string of A~n is the N -bit string 
XA~n = [so E A~[Sl E A]' .. [SN-1 E A], 
where N = I{O, l}~nl = 2n +l - 1. 
We use the string pairing function (x,y) = bd(x)Oly, where bd(x) is x with 
each bit doubled (e.g., bd(1101) = 11110011). Note that l(x,y)1 = 21xl + Iyl + 2 
for all x,y E {O,l}*. For each g: {O,l}* -+ {O,l}* and kEN, we also define 
the function gk : {O, I} * -+ {O, 1} * by gk(X) = g( (Ok, x)) for all x E {O, I} *. 
If A is a finite set, we denote its cardinality by IAI. A language D is dense 
if there exists some constant E > ° 
such that ID::;nl > 2n ' a.e. A language S is 
sparse if there exists a polynomial p such that IS::;n I ::; p( 11,) a.e .. A language S 
is co-sparse if SC is sparse. 
All machines here are deterministic Turing machines. A machine M is an 
acceptor if M on input x either accepts, rejects or does not halt. The language 
accepted by a machine M is denoted by L(M). A machine M is a transducer 
defining the function 1M if M on input x outputs fM(X). The functions timeM(x) 
and spaceM(x) represent the number of steps and tape cells, respectively, that 
the machine M uses on input x. Some of our machines take inputs of the form 
(x,n), where x E {0,1}* and 11, E N. These machines are assumed to have 
two input tapes, one for x and the other for the standard binary representation 
(3(11,) E {O, 1}* of n. 
The following standard time- and space-bounded uniform complexity classes 
are used in this paper. 
DTIME(t(n)) = {L(M) I (3c)(Vx)timeM(x) ::; c· t(lxl) + c} 
DTIMEF(t(n)) = {fM I (3c)(Vx)timeM(x) ::; c· t(lx!) + c} 
DSPACE(s(n)) = {L(M) I (3c)(Vx)spaceM(x) ::; c· s(lx!) + c} 
DSPACEF(s(n)) = {fM I (3c) (Vx)spaceM (x) ::; c· s(lxl) + c} 
00 
P = U DTIME(ni), 
i=l 

48 
David W. Juedes and Jack H. Lutz 
00 
PSPACE = U DSPACE(ni ), 
i=1 
00 
PF = U DTIMEF(ni), 
i=1 
00 
E = U DTIME(2cn ), and 
c=1 
00 
ESPACE = U DSPACE(2cn ). 
c=1 
The nonuniform complexity class P jPoly, mentioned in §l, is defined in terms 
of machines with advice. An advice function is a function h : N -+ {O, I} *. A 
language A is in P jPoly if and only if there exist B E P, a polynomial'p, and an 
advice function h such that Ih(k)1 S p(k) and x E A {::::::} (x, h(lxl)) E B for all 
kEN and x E {O, I} *. It is well-known [KL80] that P jPoly consists exactly of 
those languages that are computed by polynomial-size Boolean circuits. 
If A and B are languages, then a polynomial time, many-one reduction 
(briefly S;,-reduction) of A to B is a function f E PF such that A = rl(B) = 
{x 1 f (x) E B}. A S;,-reduction of A is a function f E PF that is a S;, -reduction 
of A to some language B. Note that! is a S;,-reduction of A if and only if I is 
S;,-reduction of A to I(A) = {I(x) I x E A}. We say that A is polynomial time, 
many-one reducible (briefly, S;,-reducible) to B, and we write AS;,B, if there 
exists a S;,-reduction f of A to B. In this case, we also say that AS;,B via I. 
A language H is S;,-hard for a class C of languages if A S;,H for all A E C. 
A language C is S;,-complete for C if C E C and Cis S;,-hard for C. If C = NP, 
this is the usual notion of NP-completeness[GJ79j. In this paper we are especially 
concerned with languages that are S;,-hard or S;,-complete for ESPACE. 
3 Resource-Bounded Measure 
In this section we very briefly give some fundamentals of resource-bounded mea-
sure, where the resource bound is polynomial space. (This is the resource bound 
that endows ESP ACE with measure structure.) For more details, examples, mo-
tivation, and proofs, see [Lut92a, Lut92bj. 
The characteristic sequence of a language A ~ {O, I} * is the binary sequence 
XA E {0,1}00 defined by XA[i] = [Si E A] for all i E N. (Recall from §2, 
that SO,Si,S2,'" is the standard enumeration of {O,l}*.) For x E {O,l}* and 
A ~ {O, I} * , we say that x is a prefix, or partial specification, of A if x is a prefix 
of XA, i.e., if there exists y E {0,1}00 such that XA = xy. In this case, we write 
x !;;;; A. The cylinder specified by a string x E {O, l} * is 
C'" = {A ~ {O, l}*lx !;;;; A}. 
We let D = {m2- n lm, n E N} be the set of nonnegative dyadic rationals. 
Many functions in this paper take their values in D or in [0,00), the set of 

Kolmogorov Complexity, Cores, and Hardness 
49 
nonnegative real numbers. In fact, with the exception of some functions that 
map into [0,00), all our functions are of the form f : X -> Y, where each of the 
sets X, Y is N, {O, 1}*, D, or some cartesian product of these sets. Formally, in 
order to have uniform criteria for their computational complexity, we regard all 
such functions as mapping {O, 1} * into {O, 1 } *. For example, a function f : N 2 X 
{O, 1} * -> N X D is formally interpreted as a function j : {O, 1} * -> {O, 1} *. Under 
this interpretation, f(i,j,w) = (k,q) means that j«(Oi, (oj,w») = (Ok, (u,v», 
where u and v are the binary representations of the integer an_d fractional parts 
of q, respectively. Moreover, we only care about the values of f for arguments of 
the form (Oi, (oj, w», and we insist that these values have the form (Ok, (u, v» 
for such arguments. 
For a function f : N X X -> Y and kEN, we define the function fk : X ---+ Y 
by fk(X) = f(k,x) = f«(Ok,x»). We then regard f as a "uniform enumeration" 
of the functions fo, iI, 12, .... For a function f : N n x X -> Y (n ~ 2), we write 
ik,l = Uk)/, etc. 
We work with the resource bound 
pspace = {f : {O, 1}* -> {O, 1}* I f is computable in polynomial space}. 
(The length If(x)1 of the output is included as part of the space used in com-
puting f.) 
Resource-bounded measure was originally developed in terms of "modulated 
covering by cylinders" [Lut90]. Though the main results of this paper are true, 
the underlying development was technically flawed. This situation is remedied 
in [Lut92a, Lut92b], where resource-bounded measure is reformulated in terms 
of density functions. We review relevant aspects of the latter formulation here. 
A density function is a function d : {O, 1}* -> [0,00) satisfying 
d(x) ~ d(XO); d(x1) 
for all x E {O, 1} *. The global value of a density function d is d( >.). An n-
dimensional density system (n-DS) is a function d : N n x {O, 1} * -> [0,00) such 
that dk is a density function for every k E N n . It is sometimes convenient to 
regard a density function as a O-DS. 
A computation of an n-DS d is a function d: N n+1 x {O, 1}* -> D such that 
(1) 
for all k E N n , r E N, and x E {O, 1}*. A pspace-computation of an n-DS d is 
a computation d such that d E pspace. An n-DS is pspace-computable if there 
exists a pspace-computation d of d. 
The set covered by a density function d is 
S[d] = U ex' 
d(x)~l 
A density function d covers a set X of languages if X ~ S[d]. A null cover of 
a set X of languages is a 1-DS d such that, for all kEN, dk covers X with 

50 
David W. Juedes and Jack H. Lutz 
global value dk(A):::; 2- 10 • It is easy to show [Lut92bl that a set X oflanguages 
has classical Lebesgue measure 0 (i.e., probability 0 in the coin-tossing random 
experiment) if and only if there exists a null cover of X. In this paper we are 
interested in the situation where the null cover d is pspace-computable. 
Definition 1. Let X be a set of languages and let Xc denote the complement 
of X. 
(1) A pspace-null cover of X is a null cover of X that is pspace-computable. 
(2) X has pspace-measure 0, and we write fJ,pspace(X) = 0, if there exists a 
pspace-null cover of X. 
(3) X has pspace-measure 1, and we write fJ,pspace(X) = 1, if fJ,pspace(XC) = O. 
(4) X has measure 0 in ESPACE, and we write fJ,(X I ESPACE) = 0, if 
fJ,pspace(X n ESPACE) = o. 
(5) X has measure 1 in ESPACE, and we write Jl(X I ESPACE) = 1, if fJ,(X C I 
ESPACE) = O. In this case, we say that X contains almost every language 
in ESPACE. 
It is shown in [Lut92a, Lut92bl that these definitions endow ESPACE with 
internal measure-theoretic structure. Specifically, if 'I is either the collection 
'Ipspace of all pspace-measure 0 sets or the collection 'IESPACE of all sets of mea-
sure 0 in ESPACE, then 'I is a "pspace-ideal," i.e., is closed under subsets, finite 
unions, and "pspace-unions" (countable unions that can be generated in poly-
nomial space). More importantly, it is shown that the ideal 'IEsPACE is a proper 
ideal, i.e., that ESPACE does not have measure 0 in ESPACE. 
Our proof of Theorem 6 below does not proceed directly from the above 
definitions. Instead we use a sufficient condition, proved in [Lut92a], for a set to 
have pspace-measure o. To state this condition we need a polynomial notion of 
convergence for infinite series. All our series here consist of nonnegative terms. 
00 
A modulus for a series 1: an is a function m : N -> N such that 
n=O 
00 L 
an:::; Tj 
n=m(j) 
for all j E N. A series is p-convergent if it has a modulus that is a polynomial. 
The following sufficient condition for a set to have pspace-measure 0 is a 
special case (for pspace) of a resource-bounded generalization of the classical 
first Borel-Cantelli lemma. 
Lemma 2. (Lutz[Lut92aj). If d is a pspace-computable 1-DB such that the series 
00 
1: dn(A) is p-convergent, then 
n=O 
00 
00 
fJ,pspace( n U S[dnD = fJ,pspace( {AlA E S[dnl i.o.}) = o. 
t=o n=t 

Kolmogorov Complexity, Cores, and Hardness 
51 
4 Space-Bounded Kolmogorov Complexity 
In this section we present the basic facts about space-bounded Kolmogorov com-
plexity that are used in this paper. 
Some terminology and notation will be useful. For a fixed machine M and 
"program" 7r E {a, I}* for M, we say that "M(7r,n) = win:::; s space" if M, on 
input Crr,n), outputs the string wE {a, I}* and halts without using more than 
s cells of workspace. We are especially interested in situations where the output 
is of the form XA=n or of the form XA<n' i.e., the 2n-bit characteristic string of 
A=n or the (2n+1 - I)-bit characteristIc string of A::;n, for some language A. 
Given a machine M, a space bound s : N -t N, a language A ~ {O, 1} * , 
and a natural number n, the s(n)-space-bounded Kolmogorov complexity of A=n 
relative to M is 
Similarly, the s(n)-space-bounded Kolmogorov complexity of A::;n relative to M 
IS 
KS~n)(A::;n) = minHrr!IM(7r,n) = XA~n in :::; s(nJ space}. 
Well-known simulation techniques show that there is a machine U that is 
optimal in the sense that for each machine M there is a constant c such that for 
all s, A and n, we have 
and 
KS~8(n)+C(A::;n) :::; KS~n)(A::;n) + c. 
As is standard in this subject, we fix an optimal machine U and omit it from 
the notation. 
We now recall the following almost-everywhere lower bound result. 
Theorem 3. (Lutz(Lut92aj). Let c E Nand E > 0. 
(aJ If 
x = {A ~ {a, 1}*!KS2cn(A=n) > 2n - 2m a.e.}, 
then f.Lpspace(X) = f.L(X!ESPACE) = l. 
(bJ If 
Y = {A ~ {O,I}*!KS2cn (A::;n) > 2n+l_ 2m a.e.}, 
then f.Lpspace(Y) = f.L(Y!ESPACE) = l. 
Informally, Theorem 3 says that KS(A=n) and KS(A::;n) are very high for 
almost all n, for all almost all A E ESPACE. This lower bound has been useful 
in a variety of applications in complexity theory, especially in contexts involving 
Boolean circuits. 

52 
David W. Juedes and Jack H. Lutz 
Example 1. The circuit-size complexity of a language A ~ {O, 1} * is the function 
GSA: N -+ N defined as follows: For each n E N, GSA(n) is the minimum size 
(number of gates) required for an n-input, l-output Boolean (acyclic, combina-
tional) circuit to decide the set A=n. (See [Lut92a, BDG88, Weg87] for details 
of the circuit model, which can be varied in minor ways without affecting this 
discussion.) Circuit-size complexity has been investigated extensively for over 
forty years. Shannon[Sha49] proved that almost every language A ~ {O, 1} * has 
circuit-size complexity 
( 4.1) 
That is, if we choose the language A ~ {0,1}* probabilistically, according to a 
random experiment in which an independent toss of a fair coin is used to decide 
membershi p of each string x E {O, 1 } * in A, then 
2n 
PrA[GSA(n) > -
a.e.] = 1. 
n 
(4.2) 
Lupanov[Lup58] proved that every language A ~ {O, 1} * has circuit-size com-
plexity 
2n 
1 
GSA(n) < -(1 + O( c)). 
n 
yn 
(4.3) 
Since the lower bound (4.1) and the upper bound (4.3) have asympotic ra-
tio 1, these results say that almost every language A has essentially maximum 
circuit-size complexity almost everywhere. Lupanov named this phenomenon the 
Shannon effect. 
Lutz[Lut92a] used Theorem 3 to investigate the Shannon effect in ESPACE. 
The upper bound (4.3) applies a fortiori to languages in ESPACE, but the lower 
bound (4.2) does not directly say anything about ESPACE because PrA[A (/. 
ESPACE] = 1 in the same random experiment. However, it is not difficult to 
see that an upper bound on GSA(n) implies an upper bound on KS(A=n). In 
fact, Lutz[Lut92a] showed that the quantitave details of this relation, combined 
with Theorem 3(a), imply that, for every real a < 1, almost every language 
A E ESPACE (and, as a corollary, almost every language A ~ {O, 1 } *) has 
circuit-size complexity 
2n 
alogn 
GSA(n) > -(1 + --) a.e. 
n 
n 
Thus the Shannon effect holds with full force in ESPACE. 
Example 2. Nisan and Wigderson[NW88] proved that, if E contains a language 
A that is, in a certain technical sense, "very hard to approximate with circuits," 
then this language A can be used to construct a pseudorandom generator that is 
fast enough and secure enough to establish the condition P = BPP. Subsequent 
to this, Lutz[Lut91] proved that there is a constant c E N such that every 
language A that is not "very hard to approximate with circuits" has space-
bounded Kolmogorov complexity 
2cn
. 
n.!l • 
KS 
(A=n) < 2 - 24 1.0. 

Kolmogorov Complexity, Cores, and Hardness 
53 
By Theorem 3(a), this implies that almost every language A E ESPACE is "very 
hard to approximate with circuits." This fact, together with the result of Nisan 
and Wigderson, immediately yields an upward measure separation theorem, stat-
ing that 
P:f-: BPP ~ /-L(EIESPACE) = 0. 
(Hartmanis and Yesha[HY84) had previously shown that P :f-: BPP ~ E~ 
ESPACE.) 
In each of the above examples, space-bounded Kolmogorov complexity is used 
to prove that some set Z of languages has measure 1 in ESPACE. In each case, 
the method is simply to prove that every language not in Z has 'unusually low 
space-bounded Kolmogorov complexity for languages in ESPACE. That is, every 
language not in Z has space-bounded Kolmogorov complexity that infinitely 
often violates the lower bounds obeyed by almost every element of ESPACE. 
In this paper we will use similar arguments to show that almost every lan-
guage A E ESPACE fails to be S;~-complete for ESPACE. In fact, we will 
prove that every language H that is S;~-hard for ESPACE has unusually low 
space-bounded Kolmogorov complexity, by which we mean space-bounded Kol-
mogorov complexity that violates a lower bound obeyed by almost every lan-
guage A E ESPACE (and almost every language A ~ {O,l}*). 
As it turns out, Theorem 3 is not strong enough for this purpose! We will 
show that every S;~-hard language H for ESPACE has an unusually low upper 
bound on its space bounded Kolmogorov complexity, but this upper bound win 
not violate the lower bounds of Theorem 3. We are thus led to ask how tight the 
lower bounds of Theorem 3 are. 
We first consider Theorem 3(b). Martin-Lof [Mar71) has shown that, for every 
real a > 1, almost every language A ~ {a, 1}* has space-bounded Kolmogorov 
complexity 
( 4.4) 
(In fact, Martin-Lof showed that this holds even in the absence of a space bound.) 
The following known bounds show that the lower bound (4.4) is tight. 
Theorem 4. There exist constants Cl, C2 E N such that every language A satis-
fies the following two conditions. 
(i) K S2 n (A<n) < 2n+l + Cl for all n. 
(ii) KS 2c2 " (A<n) < 2n + 1 - n i.o. 
(Part (i) of Theorem 4 is well known and obvious. Part (ii), proven in [Lut92aJ, 
extends a result of Martin-Lof [Mar7l].) 
Since the bound of Theorem 3(b) is considerably lower than that of (4.4), 
one might expect to improve Theorem 3(b). However, the following upper bound 
shows that Theorem 3(b) is also tight. (In comparing Theorems 3(b) and 5 it is 
critical to note the order in which A and € are quantified.) 

54 
David W. Juedes and Jack H. Lutz 
Theorem 5. For every language A E ESPACE, there exists a real E > Osuch 
that 
Proof. Fix A E ESPACE and a E N such that A E DSPACE(2an ). For each 
n E N, let n' = La~lJ and let Yn be the string of length 2n+1 - 2n'+1 such that 
XA<n = XA<n,Yn' Let M be a machine that, on input (y,n), computes XA<n' 
usi~g :S 2an' space and then outputs XA<n'Y' Let c be the optimality const~nt 
for the machine M (given by the definition of the optimal machine U at the 
beginning of this section). Then M(Yn, n) outputs XA<n in :S 2an' space, so for 
all sufficiently large n, we have 
-
h 
-
1 
were E -
a+2' 
2 2n 
2an' 
KS 
(A~n):S KSM 
(A~n) + c 
:S IYnl + c 
= 2n+1 _ 2n'+1 + c 
< 2n+1 _ 2m , 
Thus we cannot hope to improve Theorem 3(b). 
An elementary counting argument shows that, for every c E N, there exists 
a language A E ESPACE with KS2cn (A=n) 2: 2n for all n E N. This suggests 
that the prospect for improving Theorem 3(a) may be more hopeful. In fact, we 
have the following almost-everywhere lower bound result. 
Theorem 6. Let c E N and let f : N -+ N be such that f E pspace and 
00 I: 2-f (n) is p-convergent. If 
n=O 
x = {A ~ {O, 1}*IKS2cn (A=n) > 2n - fen) a.e.}, 
then /Lpspace(X) = /L(XIESPACE) = 1. 
Proof. Assume the hypothesis. By Lemma 2, it suffices to exhibit a pspace-
computable I-DS d such that 
00 L dn (>.) is p-convergent 
( 4.5) 
n=O 
and 
00 
00 
xc ~ n U S[dnl· 
(4.6) 
t=o n=t 
Some notation will be helpful. For n E N, let 
Bn = {1r E {O, 1}9n-f(n)IU(1r,n) E {O, 1}2n in :S 2cn space}. 
(4.7) 

Kolmogorov Complexity, Cores, and Hardness 
For n E Nand 7r E En, let 
Zn,7r = U CzU( 7r,n)· 
Izl=2n-l 
55 
(Thus Zn,7r is the set of all languages A such that U(7r,n) is the 2n-bit charac-
teristic string of A=n.) For n E Nand w E {O, 1}*, let 
tT(n, w) = L Pr(Zn,7rICw), 
( 4.8) 
7rEEn 
where the conditional probabilities Pr(Zn,7rICw) = PrA[A E Zn,7rIA E Cw] are 
computed according to the random experiment in which a language A ~ {O, 1}* 
is chosen probabilistically, using an independent toss of a fair coin to decide 
membership of each string in A. Finally, define the function d : N x {O, I} * ~ 
[0,00) as follows. (In all three clauses, n E N, w E {0,1}*, and bE {0,1}.) 
(i) If 0:'S Iwl < 2n - 1, then dn(w) = 21-!(n). 
(ii) If 2n - 1 :'S Iwl < 2n+1 - 1, then dn(wb) = dn(W)':\~~b!. 
(iii) If Iwl 2: 2n+1 - 1, then dn(wb) = dn(w). 
(The condition tT(n,w) = ° can only occur if dn(w) = 0, in which case we 
understand clause (ii) to mean that dn(wb) = 0.) 
It is clear from (4.8) that 
tT(n,w) = tT(n,wO) + tT(n, wI) 
2 
for all n E Nand w E {O, 1}*. It follows by a routine induction on the definition 
of d that d is a 1-DS. It is also routine to check that d is pspace-computable. 
(The crucial point here is that we are only required to perform computations of 
the type (4.8) when Iwl 2: 2n -1, so the 2cn space bound of (4.7) is polynomial in 
00 
Iwl.) Since .E 2-f (n) is p-convergent, it is immediate from clause (i) that (4.5) 
n=O 
holds. All that remains, then, is to verify (4.6). 
For each language A ~ {O, I} *, let 
Fix a language A for a moment and let n E lA. Then there exists 7ro E En such 
that A E Zn,7ro. Fix such a program 7ro and let x, y E {O, 1}* be the characterstic 
strings of A<n, A<n, respectively. (Thus Ixl = 2n - 1, Iyl = 2n+1 -
1, and 
y = xU(7ro, n).) Th~ definition of d tells us that dn(y) is dn(x) times a telescoping 
product, i.e., 
(4.9) 

56 
David W. Juedes and Jack H. Lutz 
Since Cy ~ Zn,7ro, we have 
cr(n, y) = L Pr(Zn,7rICy) 2 Pr(Zn,7roICy) = 1. 
7rEBn 
For each 7r E Bn, the events Cx and Zn,7r are independent, so 
cr(n, x) = l: Pr(Zn,7rICx) 
7rEBn 
= l: Pr(Zn,1I") 
1I"EBn 
= IBn 12-2n 
<21-!(n). 
(4.10) 
( 4.11) 
By (4.9), (4.10), and (4.11), we have dn(y) > 1. It follows that A E Cy ~ S[dnJ. 
Since n E IA is arbitrary here, we have shown that A E S[dnJ for all A ~ {O, 1}* 
and n E I A . It follows that, for all A ~ {O, 1}*, 
A E Xc :::} IIAI = 00 
:::} A E S[dnJ i.o. 
00 
00 
:::} A E n U S[dnJ, 
t=o n=t 
i.e., (4.6) holds. This completes the proof. 
Corollary 7. Let c E Nand E > O. If 
X = {A ~ {0,1}*IKS2cn(A=n) > 2n -n€ a.e.}, 
then Jl.pspace(X) = Jl.(XIESPACE) = 1. 
00 
Proof. Routine calculus shows that the series l: 2-n ' is p-convergent. 
n=O 
Corollary 7 is clearly a substantial improvement of Theorem 3(a). We will 
exploit this improvement in the following two sections. 
5 Complexity Cores 
A complexity core for a language A is a fixed set K ~ {O, 1} * such that every 
machine consistent with A fails to decide efficiently on almost all inputs from K. 
In this section we review this notion carefully and prove that upper bounds on 
the size of complexity cores for a language A imply corresponding upper bounds 
on the space-bounded Kolmogorov complexity of A. 
Given a machine M and an input x E {0,1}*, we write M(x) = 1 if M 
accepts x, M(x) = 0 if M rejects x, and M(x) = .1 in any other case (i.e., 
if M fails to halt or M halts without deciding x). If M(x) E {0,1}, we write 
spaceM(x) for the number of tape cells used in the computation of M(x). If 
M(x) = .1, we define spaceM(x) = 00. We partially order the set {O, 1,.1} by 
.1 < 0 and .1 < 1, with G and 1 incomparable. A machine M is consistent with 
a language A ~ {O, 1}* if M(x) :::; [x E A] for all x E {G,l}*. 

Kolmogorov Complexity, Cores, and Hardness 
57 
Definition 8. Let s : N --+ N be a space bound and let A, K ~ {O, I} *. Then K 
is a DSPACE( s( n) )-complexity core of A if, for every c E N and every machine 
M that is consistent with A, the "fast set" 
F = {x IspaceM(x) ~ c· s(lxl) + c} 
satisfies IF n KI < 00. (By our definition of SpaC€M(x), M(x) E {O, I} for all 
x E F. Thus F is the set of all strings that M "decides efficiently".) 
Note that every subset of a DSPACE(s(n))-complexity core of A is a 
DSPACE(s(n))-complexity core of A. Note also that, if t(n) = O(s(n)), then 
every DSPACE( s( n) )-complexity core of A is a DSPACE( t( n) )-complexity core 
of A. 
Remark. Definition 8 quantifies over all machines consistent with A, while the 
standard definition of complexity cores (d. [BDG90]) quantifies only over ma-
chines that decide A. This difference renders Definition 8 stronger than the stan-
dard definition when A is not recursive. For example, consider tally languages 
(i.e., languages A ~ {O}*). Under Definition 8, every DSPACE(n)-complexity 
core K of every tally language must satisfy IK \ {O}*I < 00. However, under 
the standard definition, every set K ~ {O, l}* is vacuously a complexity core for 
every nonrecursive language (tally or otherwise). Thus by quantifying over all 
machines consistent with A, Definition 8 makes the notion of complexity core 
meaningful for nonrecursive languages A. This enables one to eliminate the ex-
traneous hypothesis that A is recursive from several results. In some cases (e.g., 
the fact that A is P-bi-immune if and only if {O, l}* is a P-complexity core for 
A [BS85)), this improvement is of little interest. However in §6 below, we show 
that every ~;;:-hard language H for ESPACE has unusually small complexity 
cores, hence unusually low space-bounded Kolmogorov complexity. This upper 
bound holds regardless of whether H is recursive. 
It should also be noted that standard existence theorems on complexity cores 
(e.g., every language A (/. P has an infinite P-complexity core [Lyn75); every ~;;:­
hard language for E has a dense P-complexity core [OS86]) remain true under 
Definition 8. Thus no harm is done by quantifying over all machines consistent 
with A. 
Intuitively, a language is complex if it has very large complexity cores. The 
converse implication, that a language is simple if it does not have large complex-
ity cores, is supported by the following technical result. 
Theorem 9. Let A ~ {0,1}*, E > 0, b > c> 0, and g : N --+ [0,00). If every 
DSPACE(2cn )-complexity core K of A has density IK=nl ~ 2n - g(n) i.o., then 
KS 2bn (A=n) < 2n - n-€g(n) + 3dogn i.o. 
Proof. Let A ~ {O,l}*, € > 0, and b > c > 0. Let k = nl, fix a,d such 
that b > a > d > c, and let Mo, Ml , M2 , ... be a standard enumeration of the 

58 
David W. Juedes and Jack H. Lutz 
deterministic Turing machines. For each mEN, define the sets 
Fm = {xlspaCeMm(x) :$ 2di"'i}, 
Bm = Fm \ {O, l}::;mk , 
B = U 
Bm, 
cons(m,A) 
K = {O, l}* \ B, 
where the predicate cons(m, A) asserts that Mm is consistent with A. Note that, 
if Mm is a machine that is consistent with A, then FmnK = Fm \B ~ Fm \Bm ~ 
{O, l}::;m k , so IFm n KI < 00. Thus K is a DSPACE(2Cn )-complexity core for A. 
Let 
Then, for each n E S, we have 
g(n) :$IB=nl = I( U Bm)=nl 
cons(m,A) 
< L: 
I(Bm)=nl 
cons(m,A) 
L: 
I(Bm)=nl 
(m k <n)A(cons(m,A» 
< 
L: 
I(Bm)=nl 
(O::;m<n')A( cons( m,A» 
(O::;m<n')A(cons( m,A» 
and there are :$ n E terms in this last sum, so there exists 0 :$ m < n' such that 
Mm is consistent with A and I(Fm)=nl ;::: n-'g(n). 
Now let M be a machine that implements the algorithm of Figure 1 with 
input ((,B(m),y),n), where y E {O,l}* and ,B(m) is the binary representation 
of a natural number m. (Let N = 2n and let Wo, ... , WN-l be the lexicographic 
enumeration of {O, l}n. We use the symbol -L for a bit of z that has not yet been 
defined. For a string y f. >', head(y) is the first bit of y and tail (y) is the rest 
of y.) Since a > d, it is clear that M can be designed so that M((,B(m),y),n) 
uses :$ 2an workspace. For each n E S, choose mEN and y E {O,l}* such 
that 0 :$ m < n', Mm is consistent with A, I(Fm)=nl ;::: n-'g(n), and y consists 
of the 2n -
l(Fm)=nl successive bits [Wi E A] for Wi E {O, l}n \ Fm. Then 
M((,B(m),y),n) is the 2n-bit characteristic string of A=n, so 
KS~n (A=n) :$ 1(,B(m), y)1 
= Iyl + 21,B(m)1 + 2 
:$ 2n -1(Fm)=nl + 210gm + 3 
:$ 2n - n-'g(n) + 2dogn + 3. 

Kolmogorov Complexity, Cores, and Hardness 
begin 
--z:=l.N; 
for .j := 0 to N - 1 do 
begin 
-- Simulate Mm(wd as long as this uses::; 2dn space. 
if this simulation accepts or rejects 
end; 
output Z; 
endM. 
then set z[i) := 1 or z[i) := 0, respectively 
else (z[i), y) := (head(y), tail(y)) 
Fig. 1. Algorithm for proof of Theorem 9. 
It follows that there is a constant CM EN such that, for all n E S, 
2bn 
KS 
(A=n)::; 2n - n-<g(n) + 2dogn + 3 + CM. 
Hence, 
for all but finitely many n E S. 
59 
(5.1) 
If the hypothesis of Theorem 9 holds, then S is infinite, so (5.1) holds 1.0. 
Since almost every language in ESPACE has high space-bounded Kolmogorov 
complexity almost everywhere, Theorem 9 allows us to conclude that almost 
every language in ESPACE has very large complexity cores. 
Theorem 10. Fix real constants C > ° and E > 0. Let Y be the set of all lan-
guages A such that A has a DSPACE(2Cn )-complexity core K with IK=nl > 
2n - n< a.e. Then /Lpspace(Y) = /L(YIESPACE) = 1. 
Proof. Let c, E and Y be as given. Assume that A cf. Y. Then every DSPACE(2cn )-
complexity core K of A has IK=nl ~ 2n - n< i.o. Since ~ > 0, it follows by 
Theorem 9 that 
2(C+l)n( 
) 
n 
€ 
• 
KS 
A=n < 2 - n"2 + 2Elogn 1.0. 
Since n! > nt + 2dogn a.e., it follows that 
2(c+l)n( 
) 
n 
'. 
KS 
A=n < 2 - ni 1.0. 
Taking the contrapositive, this argument shows that X ~ Y, where 
"{ 
{ 
}*I 
2(c+l)n( 
) 
n 
.!. 
} 
X= A~ 0,1 
KS 
A=n >2 -n 4 a.e .. 
It follows by Corollary 7 that /Lpspace(Y) = /L(YIESPACE) = l. 
Corollary 11. For every C > 0, almost every language in ESPACE has a co-
sparse DSPACE(2Cn)-complexity core. 

60 
David W. Juedes and Jack H. Lutz 
6 The Distribution of Hardness 
In this section we use the results of §§4-5 to investigate the complexity and 
distribution of the :5!;.-hard languages for ESPACE. From a technical stand-
point, the main result of this section is Theorem 12, which says that every :5~­
hard language for ESPACE is DSPACE(2n )-decidable on a dense, DSPACE(2n )-
decidable set of inputs. 
Two simple notations will be useful in the proof of Theorem 12. First, the 
nonreduced image of a language S S;; {O, 1}* under a function I : {O, 1} * -+ 
{O, 1}* is 
f~(S) = {f(x)! xES and I/(x)1 ~ Ixl}. 
Note that 
for all f and S. 
The collision set of a function I : {O, 1}· -+ {O, 1} * is 
G, = {x I (3y < x)/(x) = I(y)}. 
(Here, we are using the standard ordering So < Sl < S2 < ... of {O, 1}*.) Note 
that I is one-to-one if and only if G, = 0. Also, 
holds for every set S S;; {O, 1 } * . 
A language A S;; {O, 1}* is incompressible by :5~-reductions if IG,I < 00 for 
every :5~-reduction I of A. 
Theorem12. Fo: every :5~-hard language H lor ESPACE, there exist B,D e 
DSPACE(2n ) such that D is dense and B = H n D. 
Prool. By 
a 
construction 
of 
Meyer(Mey77], 
there 
is 
a 
language 
A E DSPACE(2n ) that is incompressible by :5~-reductions. For the sake of 
completeness, we review the construction of A at the end of this proof. First, 
however, we use A to prove Theorem 12. 
Let H be :5~-hard for ESPACE. Then there is a :5~-reduction f of A to H. 
Let B = I~(A),D = 1~({0,1}*). Since A E DSPACE(2n ) and I e PF, it is 
clear that B,D E DSPACE(2n ) . 
Fix a polynomial q and a real number f > 0 such that If(x)1 :5q(lxl) for 
all x E {0,1}* and q(n2<) < n a.e. Let W = {xllf(x)1 < Ixl}. Then, for all 
sufficiently large n EN, writing m = L n2< J, we have 
f({O, 1}~m) \ {O, l}<m ~ I({O, 1}~m) \ I(W~m) 
~ I~({O, 1}~m) 
~ D~q(m) 
~ D~n' 

Kolmogorov Complexity, Cores, and Hardness 
whence 
ID~nl ~ I/({O,l}~m)l- HO,l}<ml 
~ I{O, 1}~ml-IO,I- HO, I} <ml 
= 2m_IO,I· 
61 
Since 10,1 < 00, it follows that ID~nl > 2n' for all sufficiently large n. Thus D 
is dense. 
Finally, note that B = I~(A) = 1~(J-l(H» = H n I~({O, 1}*) = H n D. 
This completes the proof of Theorem 12. 
We now describe Meyer's construction of the language A. It is well-known 
that there is a function 9 E DTIMEF( n10g n) that is universal for PF in the sense 
that 
PF = {gklk EN}. 
(Recall that gk is defined by gk (x) = g( (Ole, x}) for all x E {O, I} * .) Fix such a 
function g. Let A = L(M), where M is a machine that implements the algorithm 
begin 
input z ; 
R:=0; S :=0; 
for n:= 0 to Izl do 
begin 
--R:=RU{n}; 
end· 
-, 
if there exists (k, y, z) e R x {O, l}n X {O,1}Sn 
such that z < y and 9"(Y} = 9"(z} then 
begin 
-- find the lexicographically first such (k, y, z); 
jl z ¢ S then S := S U {y}; 
R:= R\ {k} 
jl z e S then accept else reject 
endM. 
Fig. 2. Meyer's construction (for proof of Theorem 12). 
in Figure 2. Itis clear by inspection that A E DSPACE(2n). To see that A is 
incompressible by ~~-reductions, suppose that 1 E PF and 10,1 = 00. It suffices 
to show that 1 is not a ~~-reduction of A. Fix kEN such that 1 = g1c. Then 
there is some n E N such that, on input x = on, M finds a triple (k,y,z) 
on cycle n of the for-loop. We then have I(y) = 9k(Y) = 91c(Z) = I(z) and 
YEA <=} Z FI. A, so 1-1(J(A» I- A, so 1 is not a ~~-reduction of A. 

62 
David W. Juedes and Jack H. Lutz 
We now use Theorem 12 to prove our upper bound on the size of complexity 
cores for hard languages. 
Theorem 13. Every DSPA CE(2n )-complexity core of every 5:;' -hard language 
for ESPACE has a dense complement. 
Proof. Let H be 5:;,-hard for ESPACE, and let K be a DSPACE(2n )- complexity 
core of H. Choose B, D for H as in Theorem 12. Fix machines M B , and Mn 
that decide Band D respectively, with spaceMB (x) = 0(21"'1) and spaceMD(x) = 
0(21"'1). Let M be a machine that implements the following algorithm. 
begin 
input x; 
if Mn(x) accepts 
then simulate M B (x) 
else run forever 
endM. 
Then xED => M(x) = [x E B] = [x E H n D] = [x E H] and x ~ D => 
M (x) = 1- 5: [x E H], so M is consistent with H. Also, there is a constant 
c EN such that for all XED, 
spaceM(x) 5: c2n + c. 
Since K is a DSPACE(2n )-complexity core of H, it follows that K n D is finite. 
But D is dense, so this implies that D \ K is dense, whence KC is dense. 
Our upper bound on the size of complexity cores now yields an upper bound 
on the space-bounded Kolmogorov complexity of hard languages. 
Theorem 14. For every 5:;,-hard language H for ESPACE, there exists E > ° 
such that 
K 
22n (H) 
n 
n'· 
S 
=n < 2 - 2 
~.o. 
Proof. Let H be ~~-hard for ESPACE. By Theorem 13, there exists E > Osuch 
that every DSPACE(2n )-complexity core K of H has density IK=nl 5: 2n -
2n2 ' i.o. It follows by Theorem 9 that K S2 2n (H=n) < 2n - n -12n2 , + 3log n i.o. 
Since n -12n2 , > 2n ' + 3log n a.e., this implies that KS2 2n (H=n) < 2n - 2n ' i.o. 
Theorems 13 and 14 give upper bounds on the complexity of hard languages. All 
that remains is to observe that it is unusual for languages in ESPACE to satisfy 
these bounds: 
Theorem 15. Let 'H, C be the sets of languages that are ~;, -hard, 5:~ -complete 
for ESPACE, respectively. (Thus, C = 'H n ESPACE.) Then 'H has pspace-
measure 0, so C is a measure 0 subset of ESPACE. 
Proof. By Theorem 14, 'H n {A ~ {O, 1}*IKS22n (A=n) > 2n - ..;n a.e.} = 0, so 
this follows from Corollary 7. 

Kolmogorov Complexity, Cores, and Hardness 
63 
7 Conclusion 
Very roughly speaking, our results (together with earlier work of [OS86, Huy86]) 
admit the following simple summary. We use KS(A=n) and IK=nl as measures 
of the complexity of a language A, where K is a "largest" complexity core for 
A. These measures roughly satisfy the condition 0 ~ KS(A=n) ~ IK=nl ~ 
2n. In both measures, almost every language in ESPACE has complexity ~ 2n 
for almost every n. In both measures, every hard language for ESPACE has 
complexity between 2n' and 2n - 2n' for infinitely many n. In fact [JL92]' these 
bounds are tight. 
Acknowledgment 
We thank Osamu Watanabe and two anonymous reviewers for suggestions that 
have improved the exposition of this paper. 
References 
[A1l89] 
E. W. Allender. Some consequences of the existence of pseudorandom gener-
ators. Journal of Computer and System Sciences 3'9:101-124,1989. 
[AR88] 
E. W. Allender and R. Rubinstein. P-printable sets. SIAM Journal on Com-
puting 17:1193-1202 ,1988. 
[AW90] E. W. Allender and O. Watanabe. Kolmogorov complexity and degrees of 
tally sets. Information and Computation 86:160-178, 1990. 
[Amb86] K. Ambos-Spies. Randomness, relativizations, and polynomial reducibilities. 
In Proceedings of the First Annual Structure in Complexity Theory Confer-
ence, pages 23-34, 1986. 
[BB86] 
J. L. Bald,zar and R. Book. Sets with small generalized Kolmogorov com-
plexity. Acta Informatica 23:679-688, 1986. 
[BDG88] J. L. Balcazar, J. Diaz, and J. Gabarr6. Structural Complexity I, Springer-
Verlag, 1988. 
[BDG90] J. L. Balcazar, J. Diaz, and J. Gabarr6. Structural Complexity II, Springer-
Verlag, 1990. 
[BS85] 
J. L. Balcazar and U. Schoning. Bi-immune sets for complexity classes. Math-
ematical Systems Theory 18:1-10, 1985. 
[Ber76] 
L. Berman. On the structure of complete sets: almost everywhere complexity 
and infinitely often speed-up. In Proceedings of the 17th. IEEE Symp. of the 
Foundations of Computer Science, pages 76-80, 1976. 
[BH77] 
L. Berman and J. Hartmanis. On isomorphism and density of NP and other 
complete sets. SIAM Journal on Computing 6:305-322, 1977. 
[BD87] 
R. Book and D.-Z. Du. The existence and density of generalized complexity 
cores. Journal of the Association for Computing Machinery 34:718-730, 1987. 
[BDR88] R. Book, D.-Z Du, and D. Russo. On polynomial and generalized complexity 
cores. In Proceedings of the Third Structure in Complexity Theory Conference, 
pages 236-250, 1988. 
[Cha66] G. J. Chaitin. On the length of programs for computing finite binary se-
quences. Journal of the Association for Computing Machinery 13:547-569, 
1966. 

64 
David W. Juedes and Jack H. Lutz 
[Du85] 
D.-Z. Du. Generalized complexity cores and levelability of intractable sets. 
Ph.D. dissertation, University of California, Santa Barbara, CA. 1985. 
[DB89] 
D.-Z. Du and R. Book. On inefficient special cases of NP-complete problems. 
Theoretical Computer Science 63:239-252, 1989. 
[ESY85] S. Even, A. Selman, and Y. Yacobi. Hard core theorems for complexity classes. 
Journal of the Association for Computing Machinery 35:205-217,1985. 
[GJ79) 
M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to 
the Theory of NP-completeness, W.H. Freeman and Company, 1979. 
[Har83) 
J. Hartmanis. Generalized Kolmogorov complexity and the structure of feasi-
ble computations. In Proceedings of the 24th IEEE Symposium on the Foun-
dations of Computer Science, pages 439-445, 1983. 
[HY84] 
J. Hartmanis and Y. Yesha. Computation times of NP sets of different den-
sities. Theoretical Computer Science 34:17-32, 1984. 
[Huy86) D. T. Huynh. Resource-bounded Kolmogorov complexity of hard languages. 
In Proceedings of the First Annual Structure in Complexity Theory Confer-
ence, pages 184-195, 1986. 
[Huy87] D. T. Huynh. On solving hard problems by polynomial-size circuits. Infor-
mation Processing Letters 24:171-176,1987. 
[JL92) 
D. W. Juedes and J. H. Lutz. The complexity and distribution of hard prob-
lems. in preparation. 
[Kan82] R. Kannan. Circuit-size lower bounds and non-reducibility to sparse sets. 
Information and Control 55:40-56, 1982. 
[Kar72) R. Karp. Reducibility among combinatorial problems. In Complexity of Com-
puter Computations. Ed. R. E. Miller, and J. W. Thatcher, 85-104. New York: 
Plenum Press, 1972. 
[KL80] 
R. M. Karp and R. J. Lipton. Some connections between nonuniform and 
uniform complexity classes. In Proceedings of the 12th ACM Symposium on 
Theory of Computing, pages 302-309, 1980. Also published as Turing ma-
chines that take advice. L'Enseignement Mathematique 28:191-209, 1982. 
[Ko86) 
K. Ko. On the notion of infinite pseudorandom sequences. Theoretical Com-
puter Science 48:9-33, 1986. 
[KM81] K. Ko, and D. Moore. Completeness, approximation, and density. SIAM Jour-
nal on Computing 10:787-796,1981. 
[Kol65) 
A. N. Kolmogorov. Three approaches to the quantitative definition of'infor-
mation'. Problems of Information Transmission 1:1-7, 1965. 
[Lev84] 
L. A. Levin. Randomness conservation inequalities; information and indepen-
dence in mathematical theories. Information and Control 61:15-37, 1984. 
[Lon86] L. Longpre. Resource bounded Kolmogorov complexity, a link between com-
putational complexity and information theory. Ph.D. thesis, Cornell Univer-
sity, 1986. Technical Report TR-86-776. 
[Lup58] O. B. Lupanov. On the synthesis of contact networks. DoH. Akad. Nauk SSSR 
19:23-26, 1958. 
[Lut90) 
J. H. Lutz. Category and measure in complexity classes. SIAM Journal on 
Computing 19:1100-1131,1990. 
[Lut91] 
J. H. Lutz. An upward measure separation theorem. Theoretical Computer 
Science 81:127-135, 1991. 
[Lut92a) J. H. Lutz. Almost everywhere high nonuniform complexity. Journal of Com-
puter and System Sciences 44, 1992, to appear. 

Kolmogorov Complexity, Cores, and Hardness 
65 
[Lut92b] J. H. Lutz. Resource-bounded measure. in preparation. 
[Lyn75] N. Lynch. On reducibility to complex or sparse sets. Journal of the Associa-
tion for Computing Machinery 22:341-345,1975. 
[Mar71] P. Martin-Lof. 
Complexity oscillations in infinite binary sequences. 
[May91] 
[Mey77] 
[MS72j 
[NW88] 
[Orp86j 
[OS86] 
[R087] 
[Sha49] 
[Sip83] 
[So164] 
[SC89] 
[Weg87] 
[Wil85] 
[Ye90] 
ZeitschriJt fUr Wahrscheinlichkeitstheory und Verwandte Gebiete 19:225-230, 
1971. 
E. Mayordomo. Almost every set in exponential time is P-bi-immune. In Pro-
ceedings of the Seventeenth International Symposium on Mathematical Foun-
dations of Computer Science, Springer-Verlag, 1992, to appear. 
A. R. Meyer. reported in [BH77]. 
A. R. Meyer and L. Stockmeyer. The equivalence problem for regular expres-
sions with squaring requires exponential space. In Proceedings of the 13th 
IEEE Symposium on Switching and Automata Theory, pages 125-129, 1972. 
N. Nisan and A. Wigderson. Hardness vs. randomness. In Proceedings of 
the 29th IEEE Symposium on Foundations of Computer Science, pages 2-11, 
1988. 
P. Orponen. A classification of complexity core lattices. Theoretical Computer 
Science 70:121-130,1986. 
P. Orponen and U. Schoning. The density and complexity of polynomial cores 
for intractable sets. Information and Control 70:54-68, 1986. 
D. A. Russo and P. Orponen On P-subset structures. Mathematical Systems 
Theory 20:129-136, 1987. 
C. E. Shannon. The synthesis of two-terminal switching circuits. Bell System 
Technical Journal 28:59-98,1949. 
M. Sipser. A complexity-theoretic approach to randomness. In Proceedings of 
the 15th ACM Symposium of the Theory of Computing, pages 330-335, 1983. 
R. J. Solomonoff. A formal theory of inductive inference. Information and 
Control 7:1-22,224-254, 1964. 
L. Stockmeyer and A. K. Chandra. Provably difficult combinatorial games. 
SIAM Journal on Computing 8:151-174,1979. 
1. Wegener. The Complexity of Boolean Functions. (Wiley--Teubner series in 
computer science), Stuttgart: Wiley-Teubner, 1987. 
C. B. Wilson. Relativized circuit complexity. Journal of Computer and Sys-
tem Sciences 31:169-181,1985. 
H. Yeo Complexity cores for P /poly. submitted. 

Resource Bounded Kolmogorov Complexity 
and Statistical Tests 
Luc Longpre 
College of Computer Science 
Northeastern University 
Boston, MA 02115, USA 
e-mail: luc@corwin.ccs.northeastern.edu 
Abstract. This paper investigates the statistical properties of resource 
bounded Kolmogorov-random strings. In an attempt to define random 
sequences, Martin-Lof has shown that the Kolmogorov-random strings 
(strings with no short description) possess all the statistical properties of 
random strings. We look at the statistical properties of resource bounded 
Kohnogorov-random strings. For space bounds, we show that there is 
a direct relation between the space bound on the Kolmogorov random-
ness classes and the space required to check a statistical property. The 
problem is still open for time bounds. We then relate this notion of 
random sequences to Yao's definition of secure pseudo random number 
generators. 
1 Introduction 
Before looking at Martin-Lof's results, let's review briefly the attempts to define 
random sequences. For a more detailed discussion of the history of theories of 
probability and random sequences, see [Fin73, vL87, LV88, LV91) (from which 
this overview is inspired). 
The concept of randomness comes up in various forms in mathematics and 
computer science. The formalization of the concept has resulted in various the-
ories of probability, that differ according to their domains of application and 
their goal. These theories have also gone through evolution with time and new 
discoveries. 
There have been many attempts at formalizing probability. Kolmogorov 
[Ko133) (see the English translation in [Ko156)) developed in 1933 his axiomatic 
quantitative probability, which is since widely accepted. A quantitative prob-
ability is one that provides a probability measure predicate P that measures 
the probability of every possible event. Kolmogorov's formulation consists of a 
probability space (fl, F, P), where fl is the sample space, F is a a--algebra of 
selected subsets of fl, and P the probability measure. His axioms are as follows: 
1. Unit normalization: P(fl) = 1. 
2. Nonnegativity: ("IF E F)P(F) 2: O. 

Kolmogorov Complexity and Statistical Tests 
3. Finite additivity: Fl , ... , Fn E :F and pairwise disjoint imply P(Ui'=lFi) 
Ei=lP(Fi). 
4. Continuity: ("Ii)Fi ;2 FHl and n~l Fi = 0 implies limi-+= P(Fi ) = 0. 
67 
Since probability is supposed to model a physical phenomenon or describe 
a law of nature (at least initially), many researchers have tried to interpret the 
model in terms of the property of random experiments. In particular, von Mises 
was interested in the applicability of probability to real world phenomena. He 
suggested that the study of probability is intricately tied to the study of random 
sequences [vM19, vM39]. He considered the study of repeating an experiment, 
each trial having a finite number of possible outcomes, and each trial being 
unrelated (see [vMG64]). He introduced a field of events V that describes the 
possible results of this infinite number of experiments. If Q is the cr-algebra as 
defined by Kolmogorov, then consider the set of all infinite sequences of elements 
of Q. A cylinder over this set is a set of infinite sequences determined by a 
finite number of positions. For example, all sequences that have a ° in the first 
position and a 1 in the 5th position. The field V is the closure of the set of 
all cylinders under (finite) unions and complements, and under the following 
limiting property: 
If V is such that there exist infinite sequences Li and Ui having the 
following properties: 
1. ("In)Ln ~ V ~ Un, 
2. limn->oo[P(Un) - P(Ln)] = 0, 
then V E V. 
The consideration of infinite sequences and the desire to apply the theory 
to real random phenomena led to the frequency concept of probability. If an 
experiment is performed a finite number of times, there will be a fluctuation in 
the relative frequency of outcomes. But, as the number of experiments grows, 
there seems to be a decrease in this fluctuation. From this, it is tempting to define 
probability as the limit of this relative frequency, as the number of experiments 
tends to infinity. This is called the frequentist's approach. 
One problem arises from this approach when one tries to make statements 
about future outcomes of an experiment. The above definition says nothing about 
the rate of convergence. Perhaps we would like to say that the frequency will be 
E-close to the limiting frequency after some number of trials. But if We want to 
include the rate of convergence in the definition of probability, it Seems inevitable 
that the definition becomes circular. 
In 1919, von Mises [vM19] addresses this problem when he introduced his 
definition of random sequences, which he calls collectives. Following the notation 
in [vMG64], 
Definition 1. A collective K(cf>, Q) is an infinite sequence S of elements of Q 
and a family cf> of place-selection functions that is closed under composition such 
that 
1. ("Iwi E Q), the number of occurrences of Wi in the first n terms of S tends 
to Pi as n tends to infinity. 

68 
Luc Longpre 
2. E~lPi = 1. 
3. If we generate an infinite subsequence Sf from S by means of any place 
selection 1> E CP, then Sf satisfies 1. and 2. for the same {Pi}. 
The third condition, intuitively says that there is no gambling strategy. It 
says that if we select any subsequence of S, using an admissible place selection, 
then the subsequence is also a collective with the same derived probabilities. In 
other words, assume a player is betting in fixed amounts, on the outcomes of the 
sequence, after seeing all the previous outcomes, then no gain can be obtained 
on the long run. 
One problem with this definition is that the notion of admissible place selec-
tion has not been defined formally. Instead, von Mises gives examples of admis-
sible place selection: 
-
choose those Wn for which n is prime, 
choose those Xn which follow the word 010, 
-
toss a coin (independently) and choose Wn if the nth toss yields heads. 
While intuitively appealing, the lack of formality in von Mises' definition has 
fueled a stream of criticism against this theory. For example, we cannot accept 
as admissible the function that selects all the position for which the outcome is a 
1. So, any formal definition should exclude those functions. Various authors have 
tried to provide formal definitions of admissible place selections. Wald [WaI38] 
proposed to restrict the set of place selection functions to any countable set of 
functions that on a string of length n representing the outcome of n experi-
ments, decides whether to bet on the next experiment. He showed the existence 
of collectives under any such set of place selection functions. Church [Chu40] 
proposed to choose the set of recursive functions. This proposition offered a 
rigorous definition of random sequences. In addition, under this definition, the 
random sequences form a set of measure one. This goes along with the intuition 
that most sequences should be random. 
But this definition turned out to be not good enough. For any countable set of 
place selections, Ville [ViI39] constructed a sequence which is a random sequence 
according to the definition, but has too much regularity to be called random. Up 
until now, there has been no acceptable way of giving a class of admissible place 
selections that provides a reasonable definition of random sequences, especially 
if we want to include as well the random place selections, as described by the 
original von Mises examples. 
In 1966, Martin-Lof [ML66] gave a definition of random sequences based on 
Kolmogorov complexity and on statistical tests that did not have all the problems 
expressed above. He noticed that all statistical tests are effective. More precisely, 
if an infinite sequence fails the statistical test, then it fails on infinitely many 
increasing prefixes of the infinite sequence. So, we can define the set of infinite 
random sequences as the intersection of all sets of infinite sequences of measure 
one whose complement is recursively enumerable. This set is a set of measure 
one as well. 

Kolmogorov Complexity and Statistical Tests 
69 
One important objection of all this work on infinite random sequences is 
exactly because they are infinite. After all, in practice, we never see infinite se-
quences, but only finite ones. In other words, if all we encounter in this world 
are finite sequences, why not base a theory of random sequences on finite se-
quences. An interesting aspect of the work of Kolmogorov [KoI63] and Martin-Lof 
[ML66] is that it also applies to finite sequences. Using Kolmogorov complexity, 
a Kolmogorov-random (K-random) string is a string of length n that cannot be 
printed by any program of size strictly less than n. The K-random strings have 
many nice properties of random strings. For example, given any portion of any 
K-random string, it is impossible to compute the remaining part of the string, 
because that would provide a short program to print the whole string. The K-
random strings also have many other properties of random strings. In [ML66], 
Martin-Lof gave a definition of statistical test. His statistical tests have levels of 
randomness, and a finite sequence can pass or fail the test at various levels, with 
the restriction that fewer and fewer sequences fail the test as the level increases. 
He constructed a universal test and showed that the K-random strings possess 
all the computable statistical properties of the random strings by giving a direct 
relation between the length of the shortest program to print a string and the 
level of non-randomness given by this universal test. 
After the evolution of computational complexity in the last 2 decades, it 
is important that we revisit those theories in the context of resource bounded 
environments. Looking back at infinite sequences, what is a reasonable definition 
of a space bounded or time bounded random sequence? Intuitively, there may 
be some sequences that are not random, but such that the non-randomness is so 
intricately buried that any space or time bounded process would fail to detect 
it. Research in this area is described in considerable details by Lutz in this book. 
With respect to finite sequences, the same question arises. What is a good 
definition of a finite random sequence in a time or space bounded environments. 
Would all the Martin-Lof's theorems still hold? Can we still build a univer-
sal test? Can we use those new definitions in building pseudo random number 
generators? 
It is impossible to use unbounded theories in building a pseudo random num-
ber generator, because it is undecidable whether or not a string is K-random. 
We can use time and space bounded Kolmogorov complexity (studied early on 
in [Har83, Sip83, K086, Lon86] and others) to define random sequences. Since 
these classes are decidable, answering this question is relevant to the theory of 
pseudo-random number generators, and gives more insight into related problems. 
In this paper, we address those questions. Although historically, many re-
searchers consider sequences of symbols from a finite alphabet, in the rest of this 
paper, we only consider sequences of 0 and 1. The theory developed could be 
extended to sequences of symbols from a finite alphabet. After reviewing careful 
definitions of Kolmogorov complexity classes used in this paper in section 2, we 
review the definition of Martin-Lof's statistical tests in section 3. We provide 
a simplified proof of Martin-Lof's result on finite random sequences in section 
4. In section 5, we consider those notions in space bounded environments. We 
show on one hand that the Space Bounded Kolmogorov random (SBK-random) 

70 
Luc Longpre 
strings possess all the statistical properties that can be verified in less space. On 
the other hand, we show that if we allow a little more space, we can design a 
test that will single out some highly SBK-random strings (that are, of course, 
not K-random). Moreover, we build a space bounded universal test. 
Although most of the problems are still open for time bounded environments, 
applying the techniques of section 5 to time bounds do give us some weak results. 
We give these results in section 6 and address the issue of whether the results 
could be strengthened. 
In the last section we address the question of pseudo random number gen-
erators. Martin-Lof's notion of statistical tests is related to Yao's definition of 
a pseudo random number generator [Ya082]. We explore the relevance of our 
results to the construction of pseudo-random number generators. 
2 Resource Bounded Kolmogorov Complexity 
In this paper, we use several variants of Kolmogorov complexity functions and 
sets. The Kolmogorov complexity functions K(·) measure the Kolmogorov com-
plexity of a finite string (the size of the shortest description of the string). The 
Kolmogorov complexity sets K[·] are sets containing all strings that have a 
bounded Kolmogorov complexity. We use in this paper classical and resource 
bounded complexity. We also use conditional Kolmogorov complexity. 
Definition 2. For a Turing machine Mi and a finite string x, the Kolmogorov 
complexity of x with respect to Mi is defined as follows: 
Ki(X) = min{ll (3y)[lYI = l) and Mi(Y) = x} 
Ki(X) = 00 if the above set is empty. 
Definition 3. For a Turing machine Mi and a function g( n) ~ 1, the Kol-
mogorov complexity set based on Mi and 9 is defined as follows: 
Ki[g(n)] = {x I (3y)[lYI ~ g(lxl) and Mi(Y) = x} . 
Definition 4. For a Turing machine Mi, a space bound S(n) and a string x, the 
space bounded Kolmogorov complexity of x is defined as follows: 
KSi(X, S(n)) = min{ll (3y)[lYI = l) and Mi(Y) = x, 
using at most S(lxl) space} 
Ki(X) :;:: 00 if the above set is empty. 
Definition5. For a Turing machine M i , a space bound S(n) and a function 
g(n) ~ 1, the space bounded Kolmogorov complexity set is defined as follows: 
KSi[g(n), S(n)] = {x I (3Y)[IYI ::; g(lxl) and Mi(Y) = x, 
using at most S(lxl) space} . 

Kolmogorov Complexity and Statistical Tests 
71 
The function K Si( x, S( n)) is the size of the smallest program that will print 
x, using at most S(lx!) space to do so. The set KSdg(n) , S(n)] contains all 
the strings x that have a description of length g(lxl), and for which we can 
recompute x within the space bound S(lxl). The time bounded Kolmogorov 
complexity KT(x, T(n)) and sets KT[g(n), T(n)] are defined similarly with a 
time bound instead of a space bound. 
The main idea initially motivating those definitions is to measure randomness 
in finite sequences. If a pattern can be detected in a sequence, then there should 
be a description of the sequence that is shorter than the sequence itself. One 
drawback is that the definitions seem to depend on the choice of the Turing 
machine Mi. However, as observed by Solomonoff, Kolmogorov and Chaitin, if 
a universal Turing machine Mu is used in the definition, then the measure is 
nearly optimal: 
Theorem 6 (Invariance Theorem [80164, Ko165, Cha69]). 
There is a Turing machine Mu which is nearly optimal, in the sense that 
(Vi)(3e)[Kdg(n)] ~ Ku[g(n) + ell . 
Hartmanis observed that if a space or time efficient universal Turing machine 
is used, then the invariance theorem still holds with time and space bounded 
Kolmogorov complexity: 
Theorem 7 (Invariance Theorem, space bounded variant [Har83]). 
There is a Turing machine Mu which is nearly optimal, in the sense that 
(Vi)(3e)[KSdg(n), S(n)] ~ KSu[g(n) + e, eS(n)ll . 
Theorem 8 (Invariance Theorem, time bounded variant [Har83]). 
There is a Turing machine Mu which is nearly optimal, in the sense that 
(Vi) (3e)[KTM(n), T(n)] ~ KTu[g(n) + e, eT(n) log T(n)]] . 
In other words, any string that has a short description with respect to Mi has 
also a short description with respect to Mu. This means that we can safely omit 
the index, implicitly referring to a fixed chosen optimal Turing machine Mu. All 
the theorems derived later are unaffected by the additive constant factor in the 
description length, and by the multiplicative factor in the resource bounds. 
We also need to define conditional Kolmogorov complexity. Intuitively, this 
gives the size of the shortest description of a string x, where an integer m is 
available to recompute x from its description: 
Definition 9. For a Turing machine Mi , a space bound S(n) and a function 
g( n) ?: 1, and a value m the space bounded conditional Kolmogorov complexity 
set is defined as follows: 
KS[g(n) Im,S(n)] = {x I (3y)[lyl ~ g(lx!) and Mu((y,m)) = x, 
using at most S(lx!) space} . 

72 
Luc Longpre 
The Invariance Theorems are also valid when using conditional complexity, 
so again we can omit the index in the notation. The value m in the above defi-
nition could be a function of x. In this paper, we will use conditional complexity 
KS[g(n) In, S(n)], where n is the size of x, to make abstraction of the length 
of the string to be printed. As the Kolmogorov complexity gives the shortest 
description of a string x, including its length, the conditional Kolmogorov com-
plexity K(x Ilxl) makes abstraction of the length of the string to be printed (we 
do not need to code the length of x in the description). 
3 Statistical Tests 
Our aim is to show that the Kolmogorov random strings possess many statistical 
properties. Martin-Lof [ML66] gave a definition of statistical test, and showed 
that the Kolmogorov random strings possess all the computable properties of 
random strings. For example, while a binary string picked at random has a high 
probability of having about the same number of O's and 1 's, a Kolmogorov random 
string will have that property. 
One might consider a test that classifies the strings as random or non-random. 
But Martin-Lof wanted to consider strings that have various levels of random-
ness. For example, it might be awkward to classify a string starting with 9 
zeroes as random and a string starting with 10 zeroes as non-random. We would 
prefer to design a test that detects more and more non-randomness as the num-
ber of leading zeroes increases. With this in mind, a Martin-LM statistical test 
will have many levels of significance to differentiate between non-random strings 
and highly non-random strings. The strings which have a high degree of non-
randomness will fail the test at high levels. The number of strings that fail the 
test should decrease as the level increases. Martin-Lof considers exponential de-
crease. It is understood that a test is checking only a specific property of the 
strings, and some K-non-random strings could easily be called totally random 
by a specific test. 
It is also inherent in a test that very few strings are considered non-random, 
and that most strings are considered random (a test classifies a string as non-
random if it has some characteristic not shared by most of the strings). 
A level of a statistical test is a set of strings which the test finds relatively 
non-random. Each level is a subset of the previous level, containing fewer and 
fewer strings, considered more and more non-random. So, we can consider that 
a statistical test measures the non-randomness of a string. 
It is required that the number of strings decrease exponentially fast at each 
level. If we take a specific test, the level 0 contains all the strings, the level 1 at 
most 1/2 of the strings, the level 2 at most 1/4 of the strings, and so on. This 
counting should be valid for any fixed n, if we consider only the set of strings of 
length n. The level m thus contains at most 2n - m strings of length n. 
Definition 10. A test F is a set of pairs (m, s), m integer and s E {O, 1}*, such 
that 

Kolmogorov Complexity and Statistical Tests 
73 
1. (\fx)[(O,x)EFj 
2. (\fx)(\fm 2: l)[(m,x) E F ~ (m -l,x) E Fj 
3. (\fm)(\fn)[#{(m,x)l(m,x) E F and Ixl = n}::; 2n - m j. 
The first condition insures that all strings are considered non-random at 
level O. The second condition insures that there is only one level at which a 
string becomes random. The third condition forces the density of the non-random 
strings to decrease with the level. 
We define the level of x with respect to the test F as the maximum level at 
which x is considered non-random: 
mF(x) = max{m I (m, x) E F} 
So, a test can be defined either as a set F, or as a function mF. 
We should note that according to the definition, any test F assigns to each 
string x a level of at most Ixl: 0 ::; mF(x) ::; Ixl. The string x is considered non 
random if mF(x) is close to Ixl. 
From the definition, it is possible to have a test F for which m F (x) = 0 for 
each x. This kind of test does not detect any non randomness. We will later 
need a test detecting the maximum possible number of strings allowed by the 
definition of statistical test. 
Definition 11. A full range test is a test having exactly 2n - m strings of length 
n at level m. 
An example of a full range test is a test classifying strings according to how 
many leading zeroes the string has. 
We will be able to show that if a statistical test does not take too much 
space, then it never gives a high level to a SBK-random string. In fact, the level 
assigned to a SBK-random string is bounded by a constant. Proving this will be 
equivalent to proving that the set KS[n - 1, S(n)j passes space bounded tests. 
Definition 12. The set A passes the test F ¢:> 
(3c)(\fx)[x E A ~ mF(x) ::; Ixl-log(IAlxd) + cj 
where the notation An means the set of all strings of A of size n. 
This definition says that a set passes a test if all the elements of the set are 
in the lowest level possible (allowing a constant number of levels more). This is 
significant when the set does not contain almost all the strings. Note that E* 
passes any test. If the set contains at most a fixed fraction a of the strings, then 
passing a test means that all the elements are contained in the lowest c levels, 
for some constant c. If the set A is empty, it passes the test. However, finite 
non-empty sets could fail to pass a test. Also note that if F is a full range test, 
and both the set A and A contain more than a constant number of strings of 
each length, then either A or A will not pass the test. 

74 
Luc Longpre 
4 Random Strings and Statistical Tests 
Martin-Lof [ML66] proved that the K-random strings possess all the computable 
statistical properties. Here by computable, we really mean that the set F is 
recursively enumerable. He proved this by first constructing a universal test, 
which is optimal up to a constant factor. This means that a string passing the 
universal test at level l will pass any other computable test at a level within a 
constant of l. Then, he proved that any K-random string pass this universal test, 
concluding that it will pass every computable test. 
In this section we offer a proof of his theorem. This proof will be used in 
later sections. Although our proof below is based on the same idea, we take a 
different approach. The basic idea in our proof of his theorem is that if according 
to some test, a string has a high level, the string has a short description, due 
to the fact that the level is limited in size. Because of this, the string could not 
be K-random. We can in fact say more. The maximum level a string can reach 
according to a test provides an upper bound on the K-complexity of the string. 
So, our proof doesn't need the universal test. Moreover, the universal test can 
easily be constructed once the theorem has been proven. 
The universal test turns out to be very important in our understanding of 
the subject. Indeed, while K-complexity measures in which proportion the string 
is random, the universal test measures in which proportion the string is non-
random, and these two measures will always add up to the length of the string 
(up to a constant factor). 
Theorem 13. For any function g(n) 2: 0, the set K[g(n) In] passes any com-
putable (r. e.) test F. 
Proof. Let F be a computable (r.e.) test, and let Mi be a machine accepting F. 
Let A = K[g(n) In]. 
The set :II = K[g(n) In] has at least 2g(n)-k strings of size n, for some k 
because any string that has n - g( n) + k leading zeroes can be printed efficiently 
by a program of size g (n ), if k is chosen appropriately. 
The set :II has at most 2g(n) strings of size n, because there are at most this 
many programs of size g(n). This means that 
g(n) - k ::; 10g(IA,x,) ::; g(n) 
To show that A passes the test F, we have to show that 
(3c)(V'x) [x f/. K[g(n) In] => mF(x) ::; n - g(n) + c] 
We show that 
(3c)(V'x) [x E K[n - mF(x) + c In]] 
Notice that if mF(x) > Ixl- g(lxl) + c, then 
x E K[n - mF(x) + c In] => x E K[n - (n - g(n) + c) + c In] = K[g(n) In] 
(1) 
(2) 

Kolmogorov Complexity and Statistical Tests 
75 
So, from (2) we can get that 
(3c)(Vx) [mF(x) > Ixl- g(lxl) + c =} x E K[g(n) In]] 
(3) 
which is equivalent to (1). 
To show (2), we need to construct a program reconstructing x from a de-
scription of appropriate length. 
Let A be the set of all strings of level mF(x): 
A = {x'lmF(x') = mF(x) and Ix'i = Ixl} 
We can say, from the definition of a test, that IAI :::; 2Ixl-mF(x). 
Let i be the canonical order of x in A. From a binary expansion of i of length 
exactly Ixl- mF(x) and an algorithm for F, and also given lxi, we can compute 
x. We first compute mF(x) = lxi-Iii. We then enumerate A, which is the level 
m of F, and print the ith element of A. 
The length of the program is Ixl - mF(x) + c for some constant c. We can 
conclude that x E K[n - mF(x) + e In]. 
0 
Notice that we did not need that g( n) be computable in the previous proof. 
Also, we didn't have any upper bound on g(n). However, if g(n) > n, then the 
theorem becomes trivially true, because of our definition of passing a test. 
To prove the theorem, Martin-Lof first built a universal test by simulating all 
computable tests at once, and then showed that the level given by this test to a 
string x is Ixl-g(lxl) + e. This universal test is very important in understanding 
the properties of random sequences. i.,From Theorem 13, we can easily build a 
universal test. 
Theorem 14. There is a r.e. test F such that for any r.e. test T, (3e) [mF(x) ~ 
mT(x) - e]. 
Proof. Let F be the test defined as follows. 
(m, x) E F {:> K(x I n) + m :::; Ixl 
To see if (m, x) E F, simulate (using dovetailing) all programs of size Ixl-m. 
If one of them halts and prints x, then accept x. This means that F is r.e. 
From the definition of F, we know that n - mF(x) = K(x In), so for any 
string x, x ¢ K[n - mF(x) + lin]. By Theorem 13, for any function g, 
(3e)(Vx) [x ¢ K[g(n) In] =} mT(x) :::; n - g(n) + e] . 
Looking more carefully at the proof of Theorem 13, we notice that the constant 
e does not depend on the function g, so we can write 
(3e)(Vx)(Vg) [x ¢ K[g(n) In] =} mT(x) :::; n - g(n) + e] 
Now, letting g(n) = n - mF(x) + 1, we can conclude that: 
(3e)(Vx)mT(x) :::; n - (n - mF(x) + 1) + e , 
which means that 
o 

76 
Luc Longpre 
Let Fu denote the universal statistical test. The construction of the universal 
test above, and the fact that any universal test will give the same level to any 
string (within an additive constant factor) provides the following interesting 
corollary, that says that the Kolmogorov complexity and the level given by a 
universal test are complementary: 
Corollary 15. For any universal test FUJ there is a constant c such that for any 
string x, 
5 The Space Bound Equivalent 
In this section, we first show that the set of SBK-random strings passes any 
statistical test using less space. We further show how to design a test requiring 
just a little more space, and which the set of SBK-random strings does not pass. 
Since our theorems will be sensitive to multiplicative constants in the space 
bounds, we need to introduce a refinement of the standard space complexity. 
Definition 16. SPACEu[S(n)] is the class of sets accepted by a program running 
on the universal Turing machine Mu in space Sen). 
This definition is a refinement space complexity because 
SPACE[S(n)] = UuSPACEu[S(n)] . 
Also notice that the space compression theorem does not apply in this space 
complexity, so it is important to keep constant factors. 
Theorem 17. Let Sen) > nand g(n) be any function on natural numbers. Let 
F be a test such that mF(x) can be computed in SPACEu[S(lxi) - 41xll. Then 
KS[g(n) In, Sen)] passes the test F. 
Proof. We have to modify the proof of Theorem 13 to account for space bounds. 
In Theorem 13, we show how to construct x from a description of size n -
mF(x) + c. Let's look at how much space is involved in constructing x from its 
description. We need to enumerate the set A. To do that, we need to simulate 
F on every string of the same size as x. The space needed is the space used by 
F, Sen) - 4n, space n to keep the current input being tried, space n to keep the 
current output, space n to keep the target index, space n to keep the current 
index, {or a total of at most Sen). 
0 
Now, we show that if we have a little more space, the situation is reversed. 
Theorem 18. For any Sen) > n space constructible on M t" there is a full range 
test F in SPACEu [2S(n) + D(n)] which detects KS[g(n) In, Sen)], in the sense 
that mF(x) =1= 0 => x rf. KS[g(n) In,S(n)], for any g(n) < n-1. 

Kolmogorov Complexity and Statistical Tests 
77 
Proof. The idea is to put only strings that are somewhat random at every non-
zero level of the test F. First, F contains all the pairs (O,x). For m > 0, let 
An,m be the first 2n- m elements of K Sin - 2 In, S( n)] of length n in a canonical 
enumeration of that set. Then, (m, x) E F ¢} x E AI",I,m. 
-;:-;:-:::-,:.:.--:::--;--;:;-;---" 
The test F is a full range test, because the set KS[n - 2 In, S(n)] has at least 
2n - 1 strings of length n, so F contains exactly 2n-m strings of length n at level 
1n. 
Now, if mF(x) =f. 0, then x ~ KS[g(n) In, S(n)] because each string of the set 
is given the highest possible level, with the constraint that the number of strings 
at each level is limited. Because K S[n - 2 In, S( n)] contains at least half of the 
strings of length n, its strings fill up all the levels but level 0. 
Since KS[n - 21n, S(n)] C KS[g(n) In, S(n)], we can deduce that F detects 
KS[g(n) In, S(n)], for any g(n). 
It remains to show that the test F can be computed within the claimed 
space bound. Given (x, m), compute S(lxl) and mark off the space. Distribute 
the marks at every even position on each tape. This will allow us to do the 
simulation of programs on the odd positions while making sure the program 
does not exceed its space bound. Then start enumerating KS[n - lin, S(n)] by 
simulating in order all the ptograms of size :::; n - 1. While doing the simulation, 
we check that the program does not exceed space S(n). We also need to check 
that the program does not loop. To do this, we need to count the steps of 
the simulated program. If the program runs for more than 2S (n) steps without 
exceeding its space bound, then we know that it will never halt. Without loss 
of generality, we can assume that M" has an alphabet large enough that we 
can combine the marks with the counter on the even positions of the tape. In 
addition to the space required for the simulation, we need to remember the 
program being simulated and an index for the stage of the enumeration. All of 
this can be stored in O(n) space. 
0 
In fact, this technique can be used to find a test in SPACE,,[2S(n) + O(n)] 
to detect any set A in SPACEu[S(n)]. 
Next, we address the question of the space bounded universal test. We can 
construct a universal test, which detects anything that can be detected by a test 
in SPACEu[S(n)]. 
Theorem 19. If S(n) is space constructible on M", then there is a test F m 
SPACEu[2S(n) + O(n)] such that for any test T in SPACEu[S(n)], 
(3c) [mT(x) = f(lxl) => mF(x) ~ f(lx!) - c] . 
Proof. For each Turing machine M i , let M/ be the machine which accepts x 
if and only if Mi accepts x without using more than S(n) space. Using the 
simulation described in Theorem 18, M/ uses 2S(n). The test Fi is defined as 
follows. All the pairs (0, x) are in Fi . For m > 0, (m, x) is in Fi ¢} [x E L(M/) 
and there are at most 2n - m strings y of length n smaller than x such that 
y E L(M/) ]. Using the simulation above and O(n) space for indices, we have 
that Fi is in SPACE,,[2S(n) + O(n)]. 

78 
Luc Longpre 
Now, every test in SPACEu[S(n)] is an Fi for some i. The universal test F 
is defined in the following way: 
(m,x) E F {::} (30::; k::; Ixl- m) [(m+ k,x) E Fk] . 
The test F uses an additional n space to store an index for k, so F E 
SPACEu[2S(n) + O(n)]. The test is universal because 
o 
6 The Time Bound Equivalent 
The same techniques as in section 5 can be applied to the time bounded Kol-
mogorov classes. However, the results here are much looser, mainly because time 
is not a reusable resource. It requires 2nT( n) time bound to insure that a Kol-
mogorov random string passes the T(n) bounded tests, and vice-versa. We first 
need to define the finer notion of time complexity, as we did for time. 
Definition 20. T1MEu[T(n)] is the class of sets accepted by a program running 
on the universal Turing machine Mu in time T(n). 
Theorem 21. There is a constant c such the the following holds. Let T( n) > n2 
and g(n) be any function on natural numbers. Let F bt;/a test such that mF(x) 
can be computed in TIMEu[T(lxl-clxI2 + IxlJ. Then KT[g(n) In, 2nT(n)] passes 
the test F. 
Proof. Same as in the proof of Theorem 17, but calculating the time requirements 
instead. 
0 
Theorem 22. There is a constant c such that for any T(n) > n 2 computable on 
Mu in time T(n), there is a full range test Fin TIMEu[c2nT(lxl)+cn2] which de-
tects KT[g(n) In,T(n)] , in the sense that mF(x) #- 0 => x (j. KT[g(n) In,T(n)], 
for any g(n) < n. 
Proof. Same as in the proof of Theorem 18, but calculating the time requirements 
instead. 
0 
We do not expect that the bound can be improved in the last two theorems. 
The problem seems to have a relation with the power of nondeterminism. As-
suming NTIME[T(n)] ~ TIME[Tz(n)], for some other reasonable time bound 
Tz, we can show that the 2nT(n) time bound can be reduced to cn2g(n)T2(n) in 
both theorems. If g( n) ::; clog ( n) for some c, and if T2 is not too large, then the 
exponential multiplicative factor has been reduced to polynomial. 
Theorem 23. Suppose NTIME[T( n)] ~ TIME[T2 (n )]. Then, the 2nT( n) can be 
replaced by n2g(n)T2 (n) in Theorems 21 and 22. (In Theorem 22, we would also 
need that g(n) is computable in time T(n).) 

Kolmogorov Complexity and Statistical Tests 
79 
Proof. The time blow up in Theorems 21 and 22 is due to the enumeration of 
sets. If the membership in a set A can be computed in time T(n), then the set 
{(n, x) I (3y E A) of length n such that x is a prefix of y} 
is in NTIME[T(n)]. If this set can be computed in DTIME[T2 ], then one can 
enumerate the first i elements of A in time cniT2(n). 
0 
On the other hand, for each time constructible function T(n), there is an 
oracle A under which a 2n- g(n)-c factor is required to the relativized time 
bounded K-random strings to pass all the T(n) time bounded tests. Again, if 
g(n) :::; clog(n) for some c, this is almost an exponential multiplicative factor. 
Theorem 24. Let T(n) be time constructible on Mu. Then, there is an ora-
cle A and a test T in TIM.eA[T(n)] and a constant c such that for any g(n), 
KTA[g(n) In, 2n- g(n)-cT(n)] does not pass T. 
Proof. For some specific values of n, we will put a string of length n at the higher 
level of T. The test T works as follows. A string x is either at level 0 or at level 
Ixl. To determine if a string x is at level Ixl, compute T(lxl). Make a query of 
all extensions of x in increasing order of length to the oracle A. If any query 
is successful, put x at level Ixl. If no query is successful when the time expires 
(after at least ;S~) queries, for some C2), put x at level O. 
Now, we define A to insure that T is a test and that 
S = KTA[g(n) In,2n- g(n)-cT(n)] 
does not pass T. 
Let c > C2 + 1. For our specific n, we look at the set of all strings queried 
by any program of length g(n) during its computation of length 2n- g(n)-CT(n), 
using for oracle the part of A that has been determined so far and answering no 
to any new query. There are at most 2n - cT( n) of these strings. Take a string x of 
length n such that for some extension y of x oflength at most n + log(T(n)jc2)' 
y has not been queried. Such a string exists because at least half of the strings 
x are available and if for all of them, all extensions have been queried, it means 
there would have been at least 2n- 1- C2T(n) > 2n- CT(n) queries. Put this string 
y in A. This ensures that x is at level Ixl and is in S. 
We now need to do this for infinitely many n. To pick up a new n, we just 
need to be sure that not too many strings of similar size will be put in A, as 
that could put more than one string x at level Ixl. To do that, we just need to 
choose niH> ni + 10g(T(n)). 
0 
7 Random Number Generators 
Random number generators have many applications for cryptographic protocols 
[BM84, VV83] and randomizing algorithms. One can think of a pseudo-random 
number generator as a deterministic process, which from a finite seed, produces 

80 
Luc Longpre 
an infinite sequence of bits that hopefully possess all the desirable statistical 
properties of a real random source. Such processes have been proposed and 
analyzed with respect to some statistical properties [Knu69], or with respect to 
predictors trying to detect their output [Wil83]. 
The second kind of pseudo-random number generator is the one that produces 
a finite long string from a short string obtained from a real random source. Yao 
[Ya082] gave a definition of a good polynomial-time pseudo-random number 
generator of this kind and many candidates have been suggested. Essentially, a 
pseudo-random number generator is good if it cannot be distinguished from a 
real random source by any efficient program. 
We see that our tight relation on the space bounded case have direct conse-
quences on space bounded random number generation. The following theorems 
are a direct simple application of the previous sections. They state that if a 
generator can use a little more space, it can produce random looking bits. On 
the other hand, if a test can use a little more space, then it can detect the non-
randomness. (See also [WiI83] for a deeper analysis of generators and predictors 
not relying on Kolmogorov complexity and [K086] for a study of infinite strings 
in a polynomial time bound setting.) 
Theorem 25. There is a statistical test which can detect any random number 
generator bounded in space and starting from a finite seed, using just a little 
more space. 
Proof. Because the generator uses a finite seed, all the prefixes of its output 
string are in K S[c In, S( n)] for some constant c. But Theorem 18 says that some 
test will detect this set. Moreover, Theorem 19 says that some fixed universal 
test will detect the set as well. 
0 
Theorem 26. Let S(n) > n be a nondecreasing function computable in O(S(n)) 
space. Then, there is a pseudo-random number generator producing an infinite 
string a from a finite seed, using at most O(S(2n)) space to produce the first n 
bits of the string, and for which for any n, KS(an In,S(n)) ~ n/4 - 210g(n). 
Proof. The generator produces the stream of bits by batches. Suppose at stage 
i we have produced am. Then, we consider all strings x of length 2m with am 
as prefix. We take a string x such that KS(x In, S(n)) is maximized. Then, we 
output an m/2 bits extension of am according to the m/2 bits following am in 
x. The last portion of x is ignored. 
For computing any bit, we use at most O(S(2n)) bits for computing the space 
bounded K-complexity of strings of length at most 2n. 
It can be shown by induction on i that the string am produced at stage i 
is such that KS(am 1m, S(4m/3)) ~ m/3 and that all of its prefixes yare such 
that K S(y Ilyl, S( n)) ~ lyl/4 - 3log(lyl). 
0 
For the second kind of pseudo-random number generator, a generator can 
be seen as a device which, given an input of length n, outputs a longer string, 
say of length n 2• It differs from a pure random number generator by the strings 

Kolmogorov Complexity and Statistical Tests 
81 
• 
2 
It can output. A pure random number generator can output each of the 2n 
possible strings of length n 2 with equal probability whereas the pseudo-random 
number generator will output at most 2n different strings of length n 2 , and not 
necessarily with the same probability. 
Definition27. A (Yao) test is a program which receives a string as input and 
answers 0 or 1. 
Definition 28. Let G be a pseudo-random number generator and T be a test. 
Let PrG{n) be the probability that the test answers 1 when given a string of 
length n 2 with the probability distribution induced from G on inputs of length 
n. Let PrR(n) be the probability that the test answers 1 when given a string 
of length n 2 with a uniform distribution. Then, G passes the test if for any c, 
IPrG(n) - PrR(n)1 is in O{l/{nC )). 
Definition 29. A pseudo-random number generator is perfect if it passes all 
polynomial-time bounded tests. 
The first question that comes to mind is whether the set of strings output by 
a perfect generator, passes the Martin-Lof statistical tests. Another question is 
whether a generator that would produce only Kolmogorov random strings would 
be perfect. These questions are answered in the following theorems. 
Theorem 30. Let A be a set of strings containing at most a constant fraction a 
of the strings of length n, for any n. Then, if A passes the Martin-Liif statistical 
tests, a random number generator outputting only the strings in the set is not 
perfect. 
Proof. Let A be as stated in the theorem. One Martin-Lof statistical test would 
count the number of leading zeroes in a string. If A passes this test, the number 
of leading zeroes is bounded by a constant c. Now, let p be a program counting 
the number of leading zeroes and answering 1 if more than c zeroes have been 
found. For p, the probability of answering 1 given a string from A is exactly 
o. However, given a random string, the probability is 2-(c+1). This difference is 
enough to say that the generator is not perfect. 
0 
Corollary 31. If a pseudo-random number generator is perfect, the set of strings 
output by the generator doesn't pass Martin-Laf's test. 
We now know that the definition of "passing a test" based on Martin-Lof's 
definition differs from Yao's definition [Yao82]. We can explain the difference as 
follows. 
The reason a generator outputting only K-random strings is not suitable is 
that a real random number generator will sometimes output non-random strings. 
Even if this happens relatively infrequently, a polynomial time test can detect 
it with a probability which is sufficient to unqualify the pseudo-random source. 
Yao's definition is oriented to categorize the random number generators which, 
given a seed, output a longer finite random sequences. It analyzes the statistical 

82 
Luc Longpre 
properties of the set of strings output by the generator. Martin Lor's tests is 
designed to analyze the statistical properties of the pattern inside one long string. 
So, maybe it would be better to make a generator output parts of a long 
K-random string instead of many K-random short strings. Indeed, we can have 
some relations between those two measures. In the following theorem, we look 
at the concatenation the strings output by a generator into a long string. The 
following theorem says that if we take a long K-random string, cut it into small 
strings and let the pseudo-random number generator G choose randomly among 
the few strings, then G is perfect. 
Theorem 32. Let G be a pseudo-random number generator. Let Xm be the con-
catenation of all 2m strings of length m 2 given by G when we vary the seed 
over all possible strings of length m. Let f( n) be a function such that for all k, 
f (n) > logk n almost everywhere. Then, 
n 
2 
(Vm)[xm ¢ KT[n -
f(n) In,2n II => G is perfect. 
Proof. Let T be a polynomial-time (Yao) test. Let p be the probability that T 
answers 1, given a random string of length m 2 . For a string y of length m 22m , let 
Sy be the number of parts of y of length m 2 for which T answers 1. If every bit of 
y is chosen randomly (by flipping a fair coin), Sy is a binomial random variable 
with probability p of success for each 2m trial. The expected value of Sy is 2mp. 
For the test T to fail on a string z, T must answer 1 with probability that differs 
from p by at least llmc, for some c. This means that either Sz 2: 2m(p + llmC ) 
or that Sz ~ 2m(p - llmC). Let Am be the set of strings z of length m22m such 
that T fails on z. If z is chosen randomly, an upper bound on the probability 
that z E Am is calculated using the Chernoff bounds [Che52]. We will use the 
following simply stated bounds (see [AV79]): 
For a binomial distribution with n experiments and for 0 < E < 1 
Prob[Sn ~ L(1- E)npJ] ~ e-e2np/2 
Prob[Sn 2: f(l + E)nPl] ~ e-e 2np/3 . 
For our case, using n = 2m and € = I/mpc, this means that 
for some constant b. 
Since there are 2m22m strings oflength m 22m , an upper bound on the number 
of strings in Am is: 
This means that those strings can be described by their index of size m22m_ 
b2~c. If n = m 22m , then the description size is at most n -
p-::-n ,for some k 
~ 
~n 
that depends on c, or at most n - nl f(n). 
To compute Xm from its description, first compute p by simulating T on 
all strings of length m 2 . Then, simulate T using all possible strings y of length 

Kolmogorov Complexity and Statistical Tests 
83 
m22m computing Sy. Knowing Sy will tell if y E Am.. Stop the simulation when 
the right index has been reached. This takes time at most 2n2. 
We can conclude that if G is not perfect, then the string Xm. is in KT[n -
f~) In,2n2]]. 
0 
This last theorem is intended as showing a relation between Martin-LM's sta-
tistical tests. It gives a sufficient condition based on space bounded Kolmogorov 
complexity for the existence of a secure pseudo random number generator. The 
above construction cannot be used in practice, because the generator constructed 
requires double exponential time. This brings an interesting problem. What is 
a sufficient condition (perhaps based on time bounded Kolmogorov complexity) 
that enables us to build an efficient pseudo random number generator? 
References 
[AV79] 
D. Angluin and L.G. Valiant. Fast probabilistic algorithms for hamiltonian 
paths and matchings. Compo Syst. Sci., 18:155-193,1979. 
[BM84] 
M. Blum and S. Micali. How to generate cryptographically strong sequences 
of pseudo-random bits. SIAM J. Comput., 13:850-864,1984. 
[Cha69] 
G. Chaitin. On the length of programs for computing finite binary sequences: 
statistical considerations. J. Assoc. Comput. Mach., 16:145-159,1969. 
[Che52] 
H. Chernoff. A measure of asymptotic efficiency for tests of hypothesis based 
on the sum of observations. Annals of Math. Statistics, 23, 1952. 
[Chu40] 
A. Church. On the concept of a random sequence. Bulletin of the American 
Mathematical Society, 46:130-135, 1940. 
[Fin73] 
T. Fine. Theories of Probability. Academic Press, 111 Fifth Avenue, New 
York, NY 10003, 1973. 
[Har83] 
J. Hartmanis. Generalized Kolmogorov complexity and the structure of fea-
sible computations. In Proc. 24th IEEE Symposium on Foundations of Com-
puter Science, pages 439-445, 1983. 
[Knu69] D. Knuth. SemiNumerical Algorithms, volume 2 of The Art of Computer 
Programming. Addison-Wesley, Reading, MA, 1969. 
[Ko86] 
K. Ko. On the notion of infinite pseudorandom sequences. Theoretical Com-
puter Science, 48:9-13, 1986. 
[KoI33] 
A. Kolmogorov. Grundbegriffe der Wahrscheinlichkeitsrechnung. Springer 
Verlag, Berlin, 1933. 
[KoI56] 
A. Kolmogorov. Foundations of the Theory of Probability. Chelsea, Bronx, 
New York, 1956. Translation by N. Morrison. 
[KoI63] 
A. Kolmogorov. On tables of random numbers. Sankhyi, The Indian Journal 
of Statistics, Series A, 25:369-376, 1963. 
[KoI65] 
A. Kolmogorov. Three approaches for defining the concept of information 
quantity. Prob. Inform. Trans., 1:1-7,1965. 
[Lon86] 
L. Longpre. Resource bounded Kolmogorov complexity, a link between compu-
tational complexity and information theory. PhD thesis, Cornell University, 
1986. Technical Report TR86-776. 
[LV88] 
M. Li and P.M.B. Vitanyi. Two decades of applied kolmogorov complexity. 
In Proc. Structure in Complexity Theory third annual conference, pages 80-
lOL 1988. 

84 
Luc Longpre 
[LV91) 
M. Li and P.M.B. Vitanyi. An introduction to kolmogorov complexity and 
its applications, part 1: Theory. Technical Report Note CS-R9101, Centrum 
voor Wiskunde en Informatica, P.O. Box 4079, 1009 AB Amsterdam, The 
Netherlands, 1991. Preliminary version of a textbook. 
[ML66) 
P. Martin-Lof. The definition of random sequences. Information and Con-
trol, 9:602-619, 1966. 
[Sip83) 
M. Sipser. A complexity theoretic approach to randomness. In Pmc. 15th 
ACM Symposium on Theory of Computing, pages 330-335, 1983. 
[Sol64) 
R. Solomonoff. A formal theory of inductive inference, part 1 and part 2. 
Information and Control, 7:1-22, 224-254, 1964. 
[Vil39) 
J. Ville. Etude critique de la notion de collectif. Gauthier-Villars, Paris, 
1939. 
[vL87] 
M. van Lambalgen. Von Mises' definition of random sequences reconsidered. 
Journal of Symbolic Logic, 52(3):725-755, Sept. 1987. 
[vM19) 
R. von Mises. 
Grundlagen del' wahrscheinlichkeitsrechnung. 
Mathemat. 
Zeitsch., 5:52-99, 1919. 
[vM39) 
R. von Mises. Probability, Statistics and Truth. MacMillan, New York, 1939. 
Reprint: Dover, 1981. 
[vMG64) R. von Mises and H. Geiringer. The Mathematical Theory of Probability and 
Statistics. Academic Press, New York, 1964. 
[VV83) 
U. Vazirani and V. Vazirani. Trapdoor pseudo-random number generators 
with applications to protocol design. In Proc. 24th IEEE Symp. on Founda-
tions of Computer Science, pages 23-30, 1983. 
[Wal38) 
A. Waldo Die Widerspruchsfreiheit des Kollektivbegriffs in del' Wahrschein-
lichkeitsrechnung. Ergebnisse eines Mathematischen Kolloquiums, 8:38-72, 
1938. 
[Wil83) 
R. Wilber. Randomness and the density of hard problems. In Pmc. 24th 
IEEE Symp. on Foundations of Computer Science, pages 335-342, 1983. 
[Yao82) 
A. Yao. Theory and applications of trapdoor functions. In Proc. 23rd IEEE 
Symposium on Foundations of Computer Science, pages 80-91, 1982. 

Complexity and Entropy: 
An Introduction to 
the Theory of Kolmogorov Complexity * 
Vladimir A. Uspensky 
Department of Mathematical Logic 
Faculty of Mechanics and Mathematics 
Moscow Lomonosov University 
V-234 Moscow, GSP-3, 119899 Russia 
uspensky@globlab.msk.su 
Contents. 
1. Complexity, Entropy, and Randomness. . . . . . . . . . . . . . . . . . . . . . . 85 
1.1. Generation of Complexities by Means of Encoding Procedure 87 
1.2. Two Symmetric Relations and Four Entropies. . . . . . . . . . . . . .. 88 
1.3. Two Approximation Spaces and Four Entropies. . . . . . . . . . . .. 89 
1.4. The Ordering of the Four Entropies.. . .. . . . .. .. . . . . . . . . . . . .. 90 
1.5. Encoding-Free Generation of Complexities and Entropies. ... 90 
1.6. Relations between Two Quadruples of Entropies. .. . . . .. . . .. 93 
1.7. A Semantic for ET-Entropy ................................ 94 
1.8. Historical, Bibliographical, and Terminological Remarks; 
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. 96 
2. Quantitative Analysis on Entropies. . . . . . . . . . . . . . . . . . . . . . . . . .. 97 
2.1. Bounds for Entropies. . . . . . . . . . . . .. . . . .. . . . . . . . . . . . . . .. . . . .. 97 
2.2. Bounds for Differences of Entropies ......................... 100 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. 102 
1 Complexity, Entropy, and Randomness 
Things can be large or small, and their size (the length or the volume or the 
weight or so on) can be measured by a number. Besides, things can be simple 
or complex, and their complexity can also be measured by a number. I do not 
know to whom we are indebted for measuring sizes by numbers. It was Andrei 
Kolmogorov [KoI65] who proposed to measure the complexity of a thing by a 
natural number (i.e., a non-negative integer), and he developed the rudiments 
of the theory. 
Complexity of things (as opposed to the complexity of processes, e.g., of 
computational processes) took the name descriptional complexity, or K olmogorov 
complexity. As will be seen here, in appropriate cases one may say "entropy" 
instead of "complexity". 
* Preparation of this paper was supported in part by the Institute of New Technologies 
at Moscow. 

86 
Vladimir A. Uspensky 
Thus we assume that there is a set Y of things, or objects, y's, and a total 
function "complexity of y" defined on Y. That function will be denoted by Compl 
and its possible values are 0,1,2,3, ... , n, ... , 00. So the function Compl is a total 
function from Y to IN U {oo}. We do not put any further restrictions on Compl, 
but take it on an intuitive level as a measure of complexity, or a complexity 
function, or, shorter, a complexity. 
Let Compll and Compl2 be two measures of complexity. Let us say that 
Compll is not worse than Compl2 if 
Explanation. The notation A(y) < B(y) means that for some constant c not 
Cil 
depending on y and for all y, A(y) ~ B(y) + c holds. 
Let Z be some class of complexities, or (that is the same) of measures of 
complexity. Let Complo, belonging to Z, be not worse than any complexity 
belonging to Z. Then Complo is called optimal in the class Z. So a way of 
measuring complexity is called optimal if it gives, roughly speaking, the shortest 
complexities of things. Of course, a class of complexities may have no optimal 
one. 
Any optimal complexity is called an entropy. It is possible that a class Z 
has several entropies, but any two entropies, Bntl and Bnt2' fulfill the following 
condition: 
Bntl (y) ~ Bnt2 (y) 
Explanation. The notation A(y) ~ B(y) means that \A(y) -
B(y)\ ~ 0, or 
A(y) ~ B(y) andB(y) ~ A(y). 
Important Remark. There is no semantic problem when one speaks about an 
entropy related to some class of complexities. But in the theory of Kolmogorov 
complexity it is usual to speak about the entropy and even to denote it by a 
special notation. What does it mean? Here we have an abus de langage (after 
N.Bourbaki). Speaking about the entropy related to some class, one speaks in 
fact about an arbitrary entropy of that class. And the notation denotes any of 
such entropies. Of course, our statements must be invariant and do not change 
their truth value when a particular entropy changes to another one but still 
belonging to the same class. But we must be cautious. Let V and W be two 
classes of complexity functions, and let K be the entropy related to V and L be 
the entropy related to W. In fact, K and L denote two families of entropies, or, it 
is better to say, any entropies of those two families. When we write K(y) < L(y), 
Cil 
we suppose that this relation < holds for any particular entropy denoted by K 
Cil 
and any particular entropy denoted by L (so there is an additive constant hidden 
in this relation depending on the choices of particular representatives of K and 

Complexity and Entropy 
87 
L). But when we declare that K and L coincide (are the same entropy), we do 
not want to express the opinion that any entropy denoted by K coincides with 
any entropy denoted by L. That is, we understand the coincidence statement in 
the following way: for any of the entropies K and any of the entropies L, there 
exists a constant c such that IK(y) - L(y)1 < c for all y. 
Terminological Remark. In literature on Kolmogorov complexity, the term 
"complexity" (synonymous with "complexity function" and "measure of com-
plexity") is most often used in the sense of the term "entropy". But we make 
the distinction between those two terms: entropy is an optimal complexity. 
As has already been said, there may be no entropy among complexity func-
tions belonging to a class Z. An important property of a class Z is that of having 
an entropy. In such case, we say that the Solomonoff-Kolmogorov Theorem holds 
for Z. 
There exist several important classes of complexities that contain entropies. 
And among those entropies, there are ones of special interest -
namely, those 
entropies that can be used for a definition of randomness. Kolmogorov has pro-
posed the following definition of randomness for an infinite binary sequence 
ala2a3'" an"': the sequence is called random, or, more exactly, Kolmogorov 
random, if 
Ent(al ... an) > n, 
Itl 
where Ent is an entropy. Of course, the choice of Ent is to be specified. Not 
every sort of entropy goes to a "good" definition of randomness; a definition by 
Kolmogorov scheme is regarded as "good" if the class of Kolmogorov random 
sequences sprung up by that definition coincides with the class of sequences that 
are random in the sense of Martin-Lof (or are typical sequences; see [KU87a] and 
[KU87b] and [Mar66]). 
To sum up: in order to define an entropy, one must define an appropriate 
class of complexities and show that the Solomonoff-Kolmogorov Theorem holds 
for that class. 
1.1 Generation of Complexities by Means of an Encoding Procedure 
The idea (due to Kolmogorov) is very simple. There are objects and there are de-
scriptions (encodings) of objects, and the complexity of an object is the minimal 
size of its description. 
In more detail, there is a set Y of objects y, and a set X of descriptions 
(names, encodings) x. There is a volume function l defined on X; that l is 
a function from X to IN. A mode of description, or a description mode, is an 
arbitrary set E ~ X x Y. If (x, y) E E, then x is called a description (a name, an 
encoding) of y with respect to E. Thus an object y may have many descriptions 
and a description may serve as a description for many objects. 

88 
Vladimir A. Uspensky 
The complexity ofy with respect to a description mode E is defined as follows: 
CompIE(y) = min{l(x) : (x,y) E E}. 
We make the convention that CompIE(Y) = 00 if there is no x such that (x, y) E 
E. 
Let E be a class of modes of description. Each mode E E E gives the corre-
sponding complexity function ComplE. Then there arises the class Z = Z(E) of 
all complexity functions related to the modes of E, and one may ask whether the 
class Z contains an optimal function, or an entropy. If such an optimal function 
exists, then it corresponds to some description mode which is also called optimal. 
Until now we have not imposed any restrictions on X, Y, E. It is reasonable 
to assume that X and Y consist of constructive objects, and E is a generable set 
(in the sense of Post) and, consequently, is a recursively enumerable set. 
In the following exposition we shall restrict ourselves with the following sim-
ple case: Both X and Yare E, where E is the set of all binary words, or finite 
binary sequences. The volume function l is defined to be leO = I~I for every 
~ E E, where I~I is the length of ~. 
1.2 Two Symmetric Relations and Four Entropies 
Our task is to define-in a reasonable way--a class E of modes of description 
as a class of subsets of E x E. Having this goal in mind, we define a binary 
relation on E which we shall call the concordance relation: Ul and U2 are called 
conco'rdant if they have a mutual continuation, i.e., if there are tl, t2 E E such 
that ultl = U2t2. This concordance relation will be denoted by f. 
Thus we have two natural binary relations on E: the equality = and the 
concordance 'Y. Both are symmetric and decidable (see Explanation in Sect.1.3). 
Let a and (3 be two binary relation on E. We say that a mode of description 
E ~ E x E fulfills the (a, (3) -property if for every Xl, X2, Yl, Y2 E ~, 
Let us consider the class E = E (a, (3) of all recursively enumerable modes of 
description that fulfill (a,(3)-property and the related class ZOI,{3 = Z(E) of 
complexity functions. If the class ZOI,{3 contains an optimal complexity function, 
that complexity function will be called (a,(3)-entropy. 
Now move from variable a and (3 to constants = and 'Y. Taking = or 'Y as 
a or (3, we obtain four classes of complexity functions: Z='=, Z=,'"'(, Z'"'('=, Z'"'(,'"'(. 
For each of these four classes, the Solomonoff-Kolmogorov theorem is valid, so 
we have four entropies: 
1. (=, = )-entropy, or ININ-entropy, 
2. (=, 'Y )-entropy, or INE-entropy, 
3. ('Y, = )-entropy, or EIN-entropy, and 
4. ("(, 'Y)-entropy, or EE-entropy. 

Complexity and Entropy 
89 
Note. The notations ININ, etc., have the following origin. In [US81] and [US87], 
the notation "E" had the following meaning: the set of all binary words being 
considered together with relation ,. In place of the set of all binary words with 
the relation = on that set, the set IN of all natural numbers with the relation 
= and the volume function lex) = Llog2(x + l)J was considered. This volume 
function is induced by the following 1-1 correspondence between IN and E: zero 
rv A, one rv 0, two rv 1, three rv 00, four rv 01, five rv 10, and so on. 
1.3 Two Approximation Spaces and Four Entropies 
There is another way to come to the four basic entropies of Sect.1.2. 
Any set of constructive objects with a decidable partial ordering defined on 
that set will be called an approximation space. 
Explanation. The term "decidable" means that there is an algorithm to decide 
for any Xl and XII, whether Xl :S XII or not. 
On an intuitive level, the elements of an approximation space can be taken 
as informations, and Xl :S XII means that the information XII is a refinement of 
the information Xl (and hence XII is closer than Xl to some limit value to which 
both Xl and XII serve as approximations). 
To develop a more attractive theory of approximation spaces, especially with 
the intention to apply this theory to an advanced theory of Kohnogorov com-
plexity, one needs to include some additional requirements into the definition of 
an approximation space. For our goals, however, it suffices to have a decidable 
partial ordering. Moreover, only two approximation spaces will be considered: 
the bunch ill and the tree T. Their definitions follow immediately. 
- The bunch ill: The set of objects is E, and the partial ordering :S is =, i.e., 
u :S w iff u = w. 
- The t'ree 1': The set of objects is E. The partial ordering :S is defined as 
follows: u :S w iff u is a prefix of w (and w is a continuation of u), i.e., 
3v[uv=w]. 
Let X and Y be two approximat.ion spaces. The spaces X and Y will be 
treat.ed, respectively, as the space of descriptions (names, encodings) and as the 
space of the object.s described (named, encoded). Our near goal is to define the 
class E of acceptable description modes E S;;; X x Y. 
We impose on E the following three requirements: 
1. if (x,y) E E and Xl 2 x, then (Xl,y) E E, 
2. if (x, y) E E and yl :S y, then (x, y/) E E, and 
3. if (x, Yl) E E and (x, Y2) E E, then there exists a y that (x, y) E E, Yl :S Y, 
and Y2 :S y. 
Hence, the only cause of the existence of two different objects having the same 
description is the execution of the first requirement. 

90 
Vladimir A. Uspensky 
A description mode is called acceptable if it is (recursively) enumerable and 
fulfills all the above requirements. 
If one wishes to relate a complexity function with any description mode, one 
needs to introduce a volume function [ defined on X. Here we are interested in 
the cases X = m and X = T only. In both these cases, we put [(x) = Ixl where 
Ixl is the length of x. 
N ow let us fix approximation spaces X and Y, and let us consider the class 
of all acceptable description modes and the corresponding class of complexities. 
Let us ask whether the Solomonoff-Kolmogorov theorem holds for that class, 
and, if it does hold, then the related entropy will be called XY entropy. 
It turns out that the Solomonoff-Kolmogorov theorem is valid for four cases 
when X and Y is respectively m or T: 
1. For X = m and Y = ID, we have IDID-entropy, 
2. For X = m and Y = T, we have mT-entropy, 
3. For X = T and Y = ID, we have TID-entropy, and 
4. For X = T and Y = T, we have TT-entropy. 
It is easy to see that IDID-, IDT-, TID-, TT-entropy respectively coincides with 
(=,=)-, (=,,)-, (1,=)-, (I,,)-entropy ofSect.1.2. Speaking on the coincidence, 
take into account the Important Remark of Sect.1. 
1.4 The Ordering of the Four Entropies 
Now we have four entropies, and any two of them, A and B, do not coincide; 
which means that the assertion A(y) = B(y) is not valid. Let us write A < B if 
!t1 
A(y) < B(y) but not vice versa. Then there is a partial ordering on the set of 
!t1 
four entropies. That ordering can be shown by the following picture; Fig.1. 
The picture is directed from bottom to top. That is, it shows that 
IDT < IDm, IDT < TT, IDID < TID, TT < TID, 
and, of course, 
IDT < TID. 
On the other hand, 
neither IDID < TT nor TT < mID. 
1.5 Encoding-Free Generation of Complexities and Entropies 
It turns out that the four entropies of Sect.1.4 admit an encoding-free definition 
with no use of such terms as "descriptions", "names", or "encodings". 
As before and always, an entropy is defined as an optimal complexity function 
for some class Z of complexity functions; and all members of Z are functions 
from Y to IN U {oo}. So our goal is to describe appropriate classes Z. 
Having this goal in mind, let us introduce two conditions, C and E, which 
could be imposed on a function f: Y -+ IN U {oo}. 

Complexity and Entropy 
IBIB, 
or (=, =) 
TIB, 
or (-y, =) 
IBT, 
Fig. 1. The ordering of the four entropies: TIB, IBIB, 1'1', IBT 
91 
TT, 
or (-y,,) 
Condition C (of Cardinality of a set). Let n E N, and let M ~ Y be an arbi-
trary set such that 
1. any two elements of M are non-comparable, and 
2. M ~ f-l(n). 
Then the cardinality of M is less than or equal to 2n. 
Condition E (of summation of a series). Let M ~ Y be an arbitrary set such 
that any two elements of M are non-comparable. Then 
Explanation. Elements Yl and Y2 are non-comparable if neither Yl :::; Y2 nor 
Y2 :::; Yl· 
Thus, an arbitrary function f mayor may not satisfy Condition C or Con-
dition E. And it is easy to see that Condition E implies Condition C. 

92 
Vladimir A. Uspensky 
Further, a definition of "enumerability from above" is to appear. A function 
f : Y ..... IN U {oo} is called enumerable from above if the set { (y, n) : y E Y, n E 
lN, fey) ~ n} is enumerable, that is, recursively enumerable. 
Let us denote by ZeD, Y) the class of all functions from Y to lN U {oo} that 
are enumerable from above and satisfy the condition D, where D is either C or 
E. Any element of ZeD, Y) may be called a D-acceptable complexity. Hence, we 
have four classes of acceptable complexities: Z(C, IB), Z(C, T), Z(E, IB), and 
Z(E, T). For each of these four classes, there holds the Solomonoff-Kolmogorov 
theorem. 
Thus, there are four entropies: CIB-entropy, CT-entropy, EIB-entropy, and 
ET -entropy. 
If one imposes the ordering on these four entropies, as in Sect.1.4, then one 
obtains the following picture; Fig.2. 
EIB 
CIB 
ET 
CT 
Fig. 2. The ordering of four entropies: EIB, CIB, ET, CT 
The four entropies of this section also admit definitions with slightly modified 
versions of conditions C and E. 
Condition C'. There exists a constant b such that the cardinality of M is less 

Complexity and Entropy 
93 
than or equal to b· 2n for every M ~ Y satisfying the requirements (i) and (ii) 
of Condition C. 
Condition 2}'. There exists a constant b such that 
for every M ~ Y of mutually non-comparable elements. 
Classes Z(C',IB), Z(C', T), Z(E',IB), and Z(E', T) differ from the corre-
sponding classes Z(C,IB), Z(C, T), Z(E,IB), and Z(E, T); nevertheless, the 
related entropies coincide in the sense of Important Remark of Sect.l. That is, 
C'IB = CIB, C'T = CT, E'IB = EIB, and E'T = ET. 
Condition 1700 • For an arbitrary set M ~ Y of mutually non-comparable ele-
ments, 
L Tf(y) < +00. 
yEM 
It is obvious that Condition 1700 is equivalent to Condition 17' for Y = IB. 
Hence, the EOOIB-entropy coincide with the E'IB-entropy and consequently with 
the EIB-entropy. 
Theorem (Andrei Muchnik). The conditions 17' and 17= are equivalent in 
the case Y = T. 
Hence the EOOT-entropy coincides with the E'T-entropy and with the ET-
entropy. 
1.6 Relations between Two Quadruplets of Entropies 
Now we have two quadruplets of entropies: the quadruplet IBIB, IBT, TIB, and 
TT, which respectively generated by means of encoding, and the quadruplet 
CIB, CT, EIB, and ET, which respectively generated by using some quantitative 
approach represented by conditions C and E. 
It turns out that (in the sense of the equality explained in Sect.l, Important 
Remark) the following relations hold: 
CIB = IBIB, CT = IBT, and EIB = TIB. 
As to ET, we have the following non-trivial fact, which will be discussed in 
Sect.2.2(5): 
ET < TT. 
Summarizing them, we obtain the following picture; Fig.3. 

94 
Vladimir A. Uspensky 
TlB = L1B 
TT 
lBlB = ClB 
L'T 
lBT = CT 
Fig. 3. The relation between entropies 
1. 7 A Semantic for .ET-Entropy 
Four entropies of Fig.3 have an encoding semantic, but the fifth entropy, L'T, 
has not yet obtained an appropriate semantic. Now a semantic for L'T will be 
set forth. That semantic is based upon probabilistic machines. 
To this end let us consider a probabilistic Turing machine with one-way 
infinite output tape whose head moves in only one direction. "Probabilistic" 
means that one must flip a symmetric coin before performing any command, and 
the result of flipping determines which command is to be performed. Another 
version: at the input tape, there comes a random infinite binary sequence with 
equal probabilities of digits. We suppose that our machine has binary output 
alphabet and never stops, so a finite or infinite binary sequence appears on the 
output tape. 
Let us fix a machine M. For any YES, let us denote by PM(y) the probability 
of the event 'y is the beginning of the output sequence'; in this notation, "n" 
stands for ·'non-stop". Consider a preorder relation :::; on the set of machines: 
M:::; N means that PM(y) ~ P/.r(Y). 

Complexity and Entropy 
95 
Explanation. A(y) < B(y) means that for some constant c not depending on 
r:1 
y, and for every y, A(y) ::; c· B(y). 
It turns out that there exists a maximal machine W such that M ::; W for 
every machine M. In fact, there are several such machines, but any two of them, 
U and V, satisfy the condition 
P¥,(y) = Pt-(y). 
r:1 
Explanation. A(y) = B(y) iff A(y) < B(y) and B(y) < A(y). 
r:1 
r:1 
r:1 
Hence for any two maximal machines U and V, we have 
So we have moved from the probability p~ to its logarithm. For any maximal 
machine W one can verify that 
Ilogz P~(Y)I = ET(y) 
r:t1 
This fact enables us to identify Ilogz P~ I (or, if you prefer, the integer 
Lilogz P~IJ) with ET. (Recall again Important Remark of Sect.I). Then the 
probabilistic definition of P~ just given can be taken as a semantic for ET. 
The probability P~(y), related to an arbitrary maximal machine W, can be 
called a priori probability of y as an element of the tree T. 
Remark. There exists also the a priori probability of y as an element of the 
bunch ID. To obtain the a priori probability of that second sort, one should 
consider probabilistic machines of a slightly different type. The change is: instead 
of machines that never stop, one should take now probabilistic machines that 
can stop. Then P'M(y) is, by definition, the probability of the event 'the word 
printed on output tape after machine M stops coincides with y'; here "s" stands 
for "stop". A preorder on machines and the notion of a maximal machine are 
defined as above, and maximal machines do exist. Then Pw(y) calculated for 
an arbitrary maximal machine W is the a priori probability of y as an element 
of ID. Here it occurs that 
Ilogz Pw(y)1 = EID(y). 
r:t1 
Hence EID has a probabilistic semantic too. But, since EID = TID, the entropy 
EID has also an encoding semantic. 

96 
Vladimir A. U spensky 
1.8 Historical, Bibliographical, and Terminological Remarks; 
Acknowledgments 
We begin the history of the theory of Kolmogorov complexity with Kolmogorov's 
paper [KoI65]. The purpose of that paper was to bring the notion of complexity 
(now we should say "of entropy") to the foundations of information theory. In 
his paper Kolmogorov expounded some results of his studies of 1963-1964. In 
those years he knew nothing about the paper [SoI64] in which Ray Solomonoff 
presented some similar ideas -
but in vague and rather non-mathematical man-
ner. We place the paper [SoI64] in the prehistory of the theory of Kolmogorov 
complexity. At the early stage of the theory's development, an important role 
belonged to the paper [ZL 70]. 
In the papers of pioneers of the theory, there were introduced all five basic 
entropies of our Sect.1.6. The authors gave them various names and various 
notations. What was common in all those notations was the use of the letter 
"K" or the letter "k" as a part of the notation; one should believe the cause 
of this usage is a homage to Kolmogorov. Here we try to set some system of 
names and notations with the observance of the historical tradition. (In such a 
way the author makes his own contribution to the existing chaos of names and 
notations. This contribution is not too great because some names and notations 
are already in use. Simultaneously the author expresses the hope to introduce a 
standard system.) 
We would like to fix the following names and notations for the five basic 
entropies. 
1. For lBIB-entropy. Name: simple entropy; notation: KS. 
2. For IBT-entropy. Name: decision entropy; notation: KD. 
3. For TIB-entropy. Name: prefix entropy; notation: KP. 
4. For TT-entropy. Name: monotonic entropy; notation: KM. 
5. For ET-entropy. Name: a priori entropy; notation: KA. 
The entropies should be attributed to the following authors: 
- simple entropy KS to Kolmogorov [KoI65](§3} and also (though in some 
nebulous form) to Solomonoff [So164]' 
- decision entropy KD to Loveland [Lov69], 
- a priori entropy KA to Levin [ZL70] (nO 3.3) and [Lev73], 
- monotonic entropy KM to Levin [Lev73], and 
- prefix entropy KP to Levin [Lev76]. 
Remark Strictly speaking, we denote by the symbols KS, KD, KA, KM and 
KP exactly those versions of entropies as they were formulated by Kolmogorov, 
Loveland, and Levin. Let us recall the Important Remark of Sect.1. The co-
incidence KS with IBIB has the following meaning: for any particular entropy 
KS and for any particular entropy IBIB, there holds KS(y) = IBIB(y). The other 
I+l 
coincidences, KD with IBT, etc., are to be understood in the same way. 

Complexity and Entropy 
97 
Attributing the entropies to their inventors, we make no claim about the us-
age of these notations by the inventors. None of them made any essential use of 
the term "entropy"; usually the term "complexity" was used. Kolmogorov used 
simply the word "complexity" with no adjective. Loveland used the term "uni-
form complexity" , and it was renamed as "decision complexity" by Zvonkin and 
Levin [ZL 70] (Definition 2.2). Levin used the words "monotonic complexity" and 
"complexity related to a prefix algorithm" . He had not introduced any name for 
KA, but used terms "universal semicomputable measure" (in [ZL 70]( n° 3.3)) and 
"a priori probability" (in [Lev73]) for related quantities of which the logarithm 
is to be taken. 
As to notations, Kolmogorov in [KoI65](§3) employed the notation KA(Y) for 
the simple entropy. Loveland in [Lov69](p.513) employed the notation KA(X"; n) 
for the decision entropy; and Zvonkin and Levin used for it the notation KR 
[ZL70](Definition 2.2). Zvonkin and Levin in [ZL70] (nO 3.3) employed the nota-
tion -log2 R{ ro:} for the a priori entropy; later, in [Lev73] that entropy was 
denoted by Levin as kM. In the same paper [Lev73] the notation km was used 
for the monotonic entropy. The notation KP (for the prefix entropy) appeared 
in [Lev76]. 
The general idea of an approximation space as a space of informations re-
fining, or exactifying, one another is, without doubt, due to D. Scott. This idea 
was embodied into the notion of fo-space in the sense of Yu. Ershov. A classi-
fication of entropies on the basis of that notion is given in [She84](Theorem 8); 
the classification of our Sect.1.3 is very close to that of [She84]. The general idea 
of the encoding-free approach to entropies (see Sect.1.5 above) was laid down in 
[Lev76]. 
A very useful exposition of various entropies and their interrelation is given 
in [Vyu81]. A survey of the use of entropies in a definition of randomness is 
presented in [KU87a] and [KU87b]. 
In the process of preparing this paper, the author had many discussions 
with Andrei Muchnik, Alexander Shen', and Nikolai Vereshchagin. The author 
enjoyed their advice and help. Many final formulations emerged from those 
thankworthy discussions. The bounds of Sect.2.1 and of Sect.2.2 probably be-
longs to what is called "mathematical folk-lore", but the final formulae are also 
due to discussions with Muchnik, Shen" and Vereshchagin. 
To conclude this section let us redraw Fig.3 in terms and notations that we 
accept as standard; Fig.4. 
The pentagon of Fig.4 shows, in particular, that neither KA < KS nor KS < 
KA. The exclamation note attached to an entropy means that the entropy can 
be used in the Kolmogorov definition of randomness. 
2 Quantitative Analysis on Entropies 
2.1 Bounds for Entropies 
Some upper and lower bounds for entropies will be written down in this section. 
But first of all the reader must be warned that in this exposition, the sense of an 

98 
simple entropy 
KS 
Fig. 4. Five basic entropies 
prefix entropy (!) 
KP 
decision entropy 
KD 
Vladimir A. Uspensky 
monotonic entropy (!) 
KM 
a priori entropy (!) 
KA 
upper bound and the sense of a lower bound are rather different. An upper bound 
for an entropy shows that the entropy cannot be too large. A lower bound for an 
entropy does not show that the entropy cannot be too small but does show that 
in infinitely many instances, the entropy can be large enough. So upper bounds 
are absolute, or strong, upper bounds. Lower bounds are not absolute; we shall 
call them weak lower bounds. A weak lower bound has the purpose of supporting 
the corresponding upper bound and demonstrating, in a favorable case, that the 
upper bound cannot be improved. 
After this warning let us consider the five basic entropies of Sect.l.8, Fig.4. 
(1) Entropies KS, KM, KA, and KD. 
Let Ent denotes one of the entropies KS, KM, KA, KD. Then (an upper 
bound) for all y (in 5), 
Ent(y) < IYI, 
1+1 
and (a weak lower bound) for infinitely many y (in 5), 

Complexity and Entropy 
99 
Ent(y) > Iyl. 
I+l 
(1) 
Let us formulate the lower bound of (1) more exactly. We have four cases: Ent = 
KS, Ent = KM, Ent = KA, Ent = KD. For each of these cases, the symbol 
Ent (as well as KS, KM, KA or KD) denotes an arbitrary function belonging to 
some collection, i.e., the collection of Ent-entropies. In each case the meaning of 
(1) is as follows: for any particular function Ent of that collection, there exist a 
constant c, perhaps negative, and an infinite set M ~ E such that 
Vy E M [ Ent(y) 2: Iyl + c]. 
(2) Entropy KP. 
It is helpful to introduce some notation. A function qlog, quasilogarithm, is 
introduced by the following definition: 
I 
_ {log2 Z, Z 2: 1, 
q ogz -
0, 
z:::; 1. 
The iterations of that function are defined as follows: 
qlog(l) z = qlogz, and qlog(kH) z = qlog( qlog(k) z). 
Then we have for any k, any € > 0, and all y, 
KP(y) < Iyl + qlog(l) IYI + qlog(2) IYI + ... + qlog(k-l) Iyl + (1 + E)qlog(k) Iyl. (2) 
I+l 
It is an upper bound for KP. 
For a weak lower bound, let k be an arbitrary positive integer. Then, for 
infinitely many y, 
[ KP(y) 2: Iyl + qlog(l) Iyl + qlog(2) Iyl + ... + qlog(k) Iyl]. 
(3) 
That means that inequality (3) holds for any function denoted by KP (i.e. for 
any prefix entropy) and for an appropriate infinite set of y's, depending on the 
choice of that function. 
Now it is reasonable to introduce an abbreviation for the sum qlog(l) z + ... + 
(1 + E)qlog(k) z. Let us take, e.g., Qk(Z, E) as such an abbreviation. That is, 
Qk(Z, E) = qlog(l) z + qlog(2) z + ... + qlog(k-l) z + (1 + E)qlog(k) z. 
Then (2) and (3) can be rewritten as follows: 
and 
\fy [KP(y) < Iyl + Qk(lyl, E) ], 
I+l 
for infinitely many y [KP(y) 2: IYI + Qk(iyl, 0) ]. 

100 
Vladimir A. Uspensky 
2.2 Bounds for Differences of Entropies 
By how much can two entropies of different sorts, e.g., KP and KD, can differ 
from one another? Perhaps it is better to ask, how much can one entropy exceed 
the other? Upper and lower bounds are to give the answer. The warning of the 
beginning of Sect.2.1 about the different meanings of upper and lower bounds is 
valid here and now too. 
When A(y) < B(y), an upper bound for the difference A(y) - B(y) is trivial; 
CH 
namely, a constant. So in this section, only differences A(y) - B(y) for which the 
assertion A(y) < B (y) is false will be studied. 
CH 
Now let us proceed to the differences. 
(1) Difference KP - KD. 
For any k, any € > 0, and all y, 
KP(y) - KD(y) < qloglyl + Qk(lyl,€)· 
CH 
For any k and infinitely many y, 
KP(y) - KD(y) ~ qloglyl + Qk(lyl, 0). 
(4) 
(5) 
Note. Let us not forget that an additive constant implied in (4) depends not 
only on k and € but also on the particular versions of KP and KD. In (5) the set 
of y's depends not only on k but also on the versions of KP and KD. This point 
is valid for further inequalities related to lower and upper bounds. 
(2) Differences KS - KM and KS - KA. 
Let Iyl =1= o. Then, for all y, 
KS{y) - KM(y) < KS(y) - KA(y) < logzlyl. 
CH 
CH 
For some c and for infinitely many y, 
KS(y) - KM(y) ~ log21yl + c], 
and for some c and for infinitely many y, 
KS(y) - KA(y) ~ log21yl + c. 
(3) Differences KM - KS and KA - KS. 
For any k, any € > 0, and all y, 
KM(y) - KS(y) < Qk(iyl,€), 
CH 
KA{y) - KS(y) < Qk(lyl, E). 
CH 

Complexity and Entropy 
For any k and infinitely many y, 
KM(y) - KS(y) 2:: Qk(lyl, 0), 
KA(y) - KS(y) 2:: Qk(iyl,O). 
101 
(4) Differences KP-KS, KS-KD, KP-KM, KP-KA, KM-KD, and KA-KD. 
Let B - A be any of the six entropy differences mentioned above. For any k, 
any € > 0, and all y, 
B(y) - A(y) < Qk(lyl,€). 
It! 
And for any k and infinitely many y, 
B(y) - A(y) 2:: Qk(lyl,O). 
(5) The Difference KM - KA. 
This difference is of special interest. The very fact that the conjecture KM(y) 
= KA(y) is false is disappointing. The refutation of that conjecture is due to 
It! 
Peter Gacs [Gac83]. (The Hungarian surname "Gacs" is to be pronounced as 
English "garch".) 
Both KM and KA are defined on the binary tree T. Gacs studied two en-
tropies K and H of similar sorts; but his K and H are defined not on T but on 
the tree consisting of all words in a countable alphabet (say, in :IN if one takes :IN 
as an alphabet). Some bound for the difference K - H is stated in Theorem 1.1 of 
[Gac83]; there the author writes: "Therefore for binary strings, the lower bound 
obtainable from the proof of Theorem 1.1 is only the inverse of some version 
of Ackermann's function" [Gac83](p.75). As it is known, Ackermann's function 
is a function from :IN to IN which exceeds in its growth any primitive recursive 
function. The inverse 1-1 for a function 1 is defined as follows: 
r1(a) = min{z: I(z) 2:: a}. 
Thus, for infinitely many y, 
(6) 
where 1 is the version of Ackermann's function mentioned by Gacs. 
Let Z(y) denote the number of zeros in the word y. Then, as a corollary of 
Theorem 1.1 of [Gac83], we have the following: for any k and for any m, there 
exists ayE E such that Z(y) > m and 
KM(y) - KA(y) 2:: QdZ(y),O). 
(7) 
Therefore, we have two weak lower bounds. As to upper bounds, there is 
known no one except the following trivial one: for any k, any € > 0, and all y, 
KM(y) - KA(y) < Qk(lyl,€). 
It! 
(8) 
Since the weak lower bounds (6) and (7) do not support the upper bound 
(8), the task of improving all those bounds is open. 

102 
Vladimir A. U spensky 
References 
[Gac83] 
P. Gacs. On the relation between descriptional complexity and algorithmic 
probability. Theoretical Computer Science 22:71-93, 1983. 
[Kol65] 
A. N. Kolmogorov. Three approaches to the quantitative definition of in-
formation. Problems Inform. Transmission 1:1-7, 1965. (Translated from 
the Russian version.) 
[Kol68] 
A. N. Kolmogorov. Logical basis for information theory and probability 
theory. IEEE Trans. on Information Theory IT-14.5:662-664, 1969. (The 
Russian version exists.) 
[KU87a] 
A. N. Kolmogorov and V. A. Uspensky. Algorithms and randomness. In 
Proc. 1st World Congress of the Bernoulli Society, vol. 1, Yu. A. Prohorov 
and V. V. Sazonov, (eds.), VNU Science Press, Utrecht 3-53, 1987. 
[KU87b] 
A. N. Kolmogorov and V. A. Uspensky. Algorithms and randomness. The-
ory Probab. Appl. 32:389-412,1987. (Translated from the Russian version.) 
( Comment: There are two regrettable errors in the English version: p.394, 
line 2 from the bottom, and p.395, lines 1 and 3 from the top, the word 
"countable" must be replaced by "enumerable (i.e. recursively enumer-
able)"; p.395, line 1 from the top, the word "in" must be removed.) 
[Lev73] 
L. A. Levin. On the notion of a random sequence. Soviet Math. Dokl. 
14:1413-1416,1973. (Translated from the Russian version.) 
[Lev76] 
L. A. Levin. Various measures of complexity for finite objects (axiomatic 
description). Soviet Math. Dokl. 17:522-526, 1976. (Translated from the 
Russian version.) 
[Lov69] 
. D. W. Loveland. A variant of the Kolmogorov concept of complexity. In-
formation and Control 15:510-526, 1969. 
[Mar66] 
P. Martin-Lof. On the definition of random sequences. Information and 
Control 9:602-619, 1966. 
[She84] 
A. Kh. Shen'. Algorithmic variants of the notion of entropy. Soviet Math. 
Dokl. 29:569-573,1984. (Translated from the Russian version.) (Comment: 
There are many misprints in the English version.) 
[So164] 
R. Solomonoff. A formal theory of inductive inference, Part I. Information 
and Control 7:1-22,1964. 
[US81] 
V. A. Uspensky and A. L. Semenov. What are the gains of the theory of 
algorithms: basic developments connected with the concept of algorithm 
and with its applications in mathematics (Part 1,§17). In Springer-Verlag, 
Lecture Notes in Computer Science 122:100-234, 1981. 
[US87] 
V. A. Uspensky and A. L. Semenov. Teoria algoritmo1): osnovnye otkrytiya 
i prilozheniya (Theory of algorithms: main discoveries and applications) 
(§1.17). Nauka, Moscow, 1987; in Russian. 
[Vyu81] 
V. V. V'yugin. Algorithmic entropy (complexity) of finite objects and its 
application to defining randomness and quantity of information. Serniotika 
i Informatika (Semiotics and Informatics) 16:14-43, 1981; in Russian. 
[ZL70] 
A. K. Zvonkin and L. A. Levin. The complexity of finite objects and the de-
velopment of the concepts of information and randomness by means of the 
theory of algorithms. Russian Math. Surveys 25:83-124, 1970. (Translated 
from the Russian version.) 

Subject Index 
enumA,28 
J-t(X I ESP ACE), 50 
J-tpspace(X),50 see also pspace 
~, 86 
ACO, 10,17 
E,48 
ESPACE,48 
K[log, poly], 28 
P/Poly,48 
SPACEu [S(n)],76 
T1MEu[T(n)], 78 
admissiblel place selection, 68 
advice function, 48 
algorithmically random see random 
almost every language, 50 
almost everywhere (a.e.), 47 
approximation space, 89 
Baire category, 13 
Borel-Cantelli lemma, 50 
BP-operator, 38 
bunch,89 
characteristic sequence, 48 
characteristic string, 47 
ciruit see also ACo, P /Poly 
circuit, 29 
circuit complexity, 10,17 
circuit-size complexity, 52 
self-producible circuit, 30 
P-uniform circuit, 18 
polynomial-size circuit, 29 
uniform circuit, 17,18 
collectives, 67 
collision set, 60 
complete 
complete, 48 
NP-complete, 27 
::::;~-complete for C, 27 
complexity core, 57 
computation of an n-DS, 49 
concordance relation, 88 
consistent, 56 
context-free language, 10 
cryptographic protocols, 79 
cylinder, 48,67 
data compression, 10 
dense 
dense, 47 
dense set, 12,14,17 
nowhere-dense set, 13 
density 
density, 12 
density function, 49 
density system (n-DS), 49 

EATCS Monographs on Theoretical Computer Science 
Vol. I: K. Mehlhorn 
Data Structures and Algorithms 1: 
Sorting and Searching 
Vol.2: K. Mehlhorn 
Data Structures and Algorithms 2: 
Graph Algorithms and NP-Completeness 
Vol.3: K.Mehihorn 
Data Structures and Algorithms 3: 
Multidimensional Searching and 
Computational Geometry 
Vol.4: W.Reisig 
Petri Nets 
An Introduction 
Vol. 5: W. Kuich, A. Salomaa 
Semirings, Automata, Languages 
Vol. 6: H. Ehrig, B. Mahr 
Fundamentals of Algebraic Specification 1 
Equations and Initial Semantics 
Vol. 7: F. Gecseg 
Products of Automata 
Vol. 8: F.Kroger 
Temporal Logic of Programs 
Vol. 9: K.Weihrauch 
Computability 
Vol. 10: H.Edelsbrunner 
Algorithms in Combinatorial Geometry 
Vol. 11: J.L.Balcizar, J.Diaz, J.Gabarro 
Structural Complexity I 
Vol. 12: J. Berstel, C. Reutenauer 
Rational Series and Their Languages 
Vol. 13: E.Best, C.Fernimdez C. 
Nonsequential Processes 
A Petri Net View 
Vol. 14: M.Jantzen 
Confluent String Rewriting 
Vo1.l5: S. Sippu, E. Soisalon-Soininen 
Parsing Theory 
Volume I: Languages and Parsing 
Vol. 16: P. Padawitz 
Computing in Hom Oause Theories 
Vol. 17: J.Paredaens, P.DeBra, M.Gyssens, 
D.VanGucht 
The Structure of the Relational Database 
Model 
Vol. 18: J. Dassow, G. Piiun 
Regulated Rewriting in Formal Language 
Theory 
Vol. 19: M. Tofte 
Compiler Generators 
What they can do, what they might do, 
and what they will probably never do 
Vol. 20: S. Sippu, E. Soisalon-Soininen 
Parsing Theory 
Volume II: LR(k) and LL(k) Parsing 
Vol.21: H.Ehrig, B.Mahr 
Fundamentals of Algebraic Specification 2 
Module Specifications and Constraints 
Vol. 22: J.L.Balcazar, J.Diaz, J.Gabarro 
Structural Complexity II 
Vol. 23: A.Salomaa 
Public-Key Cryptography 
Vol. 24: T. Gergely, L. Ury 
First-Order Programming Theories 
Vol. 25: W Wechler 
Universal Algebra for Computer 
Scientists 

EATCS Monographs on Theoretical Computer Science 
R. Janicki, P. E. Lauer 
Specification and Analysis of 
Concurrent Systems 
The COSY Approach 
O. Watanabe (Ed.) 
Kolmogorov Complexity 
and Computational Complexity 
K.Jensen 
Coloured Petri Nets 
Basic Concepts, Analysis Methods 
and Practical Use, Vol. 1 

