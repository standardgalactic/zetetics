Sparse Modeling of Data  
and its Relation to  
Deep Learning  
Michael Elad 
 
Computer Science Department  
The Technion - Israel Institute of Technology 
Haifa 32000, Israel 
 
The research leading to these results has been received funding 
from the European union's Seventh Framework Program 
(FP/2007-2013) ERC grant Agreement ERC-SPARSE- 320649 
Tuesday, November 27th 2018 

   This Lecture is About ‚Ä¶  
2 
Michael Elad 
The Computer-Science Department 
 
The Technion 
A Proposed Theory for Deep-Learning (DL) 
 
Explanation:  
 
o DL has been extremely successful in  
solving a variety of learning problems  
o DL is an empirical field, with numerous  
tricks and know-how, but almost no  
theoretical  foundations 
o A theory for DL has become the  
holy-grail of current research in  
Machine-Learning and related fields  

   Who Needs Theory ?  
3 
Michael Elad 
The Computer-Science Department 
 
The Technion 
We All Do !!  
 
                           ‚Ä¶ because ‚Ä¶ A theory  
 
o ‚Ä¶ could bring the next rounds of ideas  
to this field, breaking existing barriers  
and opening new opportunities 
o ‚Ä¶ could map clearly the limitations of 
existing DL solutions, and point to key 
features that control their performance 
o ‚Ä¶ could remove the feeling with many  
of us that DL is a ‚Äúdark magic‚Äù, turning  
it into a solid scientific discipline 
 
Understanding is a good 
thing ‚Ä¶ but another goal is 
inventing methods. In the 
history of science and 
technology, engineering  
preceded theoretical understanding:  
ÔÇßLens & telescope ÔÇÆ Optics 
ÔÇßSteam engine ÔÇÆ Thermodynamics 
ÔÇßAirplane ÔÇÆ Aerodynamics  
ÔÇßRadio & Comm. ÔÇÆ Info. Theory  
ÔÇßComputer ÔÇÆ Computer Science 
‚ÄúMachine 
learning has 
become 
alchemy‚Äù 
Ali Rahimi: 
NIPS 2017 
Test-of-Time 
Award 
Yan LeCun 

   A Theory for DL ? 
4 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Architecture 
Algorithms 
Data 
Rene Vidal  (JHU): Explained the ability to optimize the typical non-
convex objective and yet get to a global minima 
Naftali Tishby  (HUJI): Introduced the Information Bottleneck (IB) 
concept and demonstrated its relevance to deep learning  
Stefano Soatto‚Äôs team (UCLA): Analyzed the Stochastic Gradient 
Descent (SGD) algorithm, connecting it to the IB objective  
Stephane Mallat (ENS) & 
Joan Bruna (NYU): Proposed 
the scattering transform 
(wavelet-based) and 
emphasized the treatment of 
invariances in the input data 
Richard Baraniuk & Ankit 
Patel (RICE): Offered a 
generative probabilistic 
model for the data,  
showing how classic 
architectures and learning 
algorithms relate to it 
Raja Giryes (TAU): Studied the architecture of DNN in the context 
of their ability to give distance-preserving embedding of signals 
Gitta Kutyniok (TU) & Helmut Bolcskei (ETH): Studied the ability of 
DNN architectures to approximate families of functions 

   So, is there a Theory for DL ? 
5 
Michael Elad 
The Computer-Science Department 
 
The Technion 
The answer is tricky: 
 
There are already 
various such attempts, 
and some of them are 
truly impressive 
 
‚Ä¶ but ‚Ä¶ 
 
none of them is 
complete 

   Interesting Observations 
6 
Michael Elad 
The Computer-Science Department 
 
The Technion 
o Theory origins: Signal Proc., Control Theory, Info. Theory, Harmonic 
Analysis, Sparse Represen., Quantum Physics, PDE, Machine learning ‚Ä¶ 
 
Ron Kimmel: ‚ÄúDL is a dark monster covered  
with mirrors. Everyone sees his reflection in it ‚Ä¶‚Äù 
 
David Donoho: ‚Äú‚Ä¶ these mirrors are taken  
from Cinderella's story, telling each that  
he is the most beautiful‚Äù 
 
o Today‚Äôs talk is on our proposed theory: 
 
 ‚Ä¶ and yes, our theory is the best 
Vardan Papyan 
Yaniv Romano 
Jeremias Sulam 
     Architecture 
Algorithms 
Data 

ML-CSC   
Multi-Layered 
Convolutional 
Sparse Coding 
Sparseland 
Sparse 
Representation 
Theory 
Another underlying idea that accompanies us 
  
Generative modeling of data sources enables  
o A systematic algorithm development, &   
o A theoretical analysis of their performance  
CSC 
Convolutional 
Sparse  
Coding 
   This Lecture: More Specifically    
7 
  Sparsity-Inspired Models 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Deep-Learning 
Disclaimer: Being a 
lecture on the theory  
of DL, this lecture  
is ‚Ä¶ theoretical ‚Ä¶ and 
mathematically oriented  

Multi-Layered Convolutional  
Sparse Modeling 
 
8 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Our eventual goal in today‚Äôs talk is to present the ‚Ä¶ 
 
 
 
 
 
 
So, lets use this as our running title,  
parse it into words,  
and explain each of them 

Multi-Layered Convolutional  
Sparse Modeling 
 
9 
Michael Elad 
The Computer-Science Department 
 
The Technion 

3D Objects
 
Medical Imaging
 
10 
   Our Data is Structured 
o We are surrounded by various diverse 
sources of massive information 
o Each of these sources have an internal 
structure, which can be exploited 
o This structure, when identified, is the  
engine behind the ability to process data 
o How to identify structure?  
Voice Signals
 
Stock Market
 
Biological Signals
 
Videos
 
Text Documents
 
Radar Imaging
 
Matrix Data
 
     Social Networks
 
Traffic info
 
Seismic Data
 
Still Images
 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Using models 

11 
   Model? 
Effective removal of noise (and many other tasks)       
relies on an proper modeling of the signal 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Fact 1: 
This signal 
contains AWGN 
‚Ñï(0,1)  
 
Fact 2:  
The clean signal 
is believed to  
be PWC 

12 
   Models 
o A model: a mathematical  
description of the underlying  
signal of interest, describing our 
beliefs regarding its structure 
o The following is a partial list of  
commonly used models for images 
o Good models should be simple while 
matching the signals 
 
 
o Models are almost always imperfect 
Principal-Component-Analysis 
   Gaussian-Mixture 
Markov Random Field 
   Laplacian Smoothness 
DCT concentration 
   Wavelet Sparsity 
Piece-Wise-Smoothness 
   C2-smoothness 
Besov-Spaces 
   Total-Variation 
Beltrami-Flow 
 
Simplicity 
 
 
Reliability 
 
Michael Elad 
The Computer-Science Department 
 
The Technion 

13 
   What this Talk is all About?  
Data Models and Their Use 
o Almost any task in data processing requires a model ‚Äì  
true for denoising, deblurring, super-resolution, inpainting, 
compression, anomaly-detection, sampling, recognition, 
separation, and more 
o Sparse and Redundant Representations offer a new and 
highly effective model ‚Äì we call it  
                                        Sparseland  
o We shall describe this and descendant versions of it that 
lead all the way to ‚Ä¶ deep-learning 
Michael Elad 
The Computer-Science Department 
 
The Technion 

Multi-Layered Convolutional  
Sparse Modeling 
 
14 
Michael Elad 
The Computer-Science Department 
 
The Technion 

Machine 
Learning 
15 
 
Mathematics 
Signal   
Processing 
   A New Emerging Model 
Sparseland 
Wavelet 
Theory 
Signal 
Transforms 
Multi-Scale 
Analysis 
Approximation 
Theory 
Linear  
Algebra 
Optimization 
Theory 
Denoising 
Interpolation 
Prediction 
Compression 
Inference (solving 
inverse problems) 
Anomaly 
detection 
Clustering 
Summarizing 
Sensor-Fusion 
Source-
Separation 
Segmentation 
Recognition 
Semi-Supervised 
Learning 
Identification 
Classification 
Synthesis 
Michael Elad 
The Computer-Science Department 
 
The Technion 

16 
   The Sparseland  Model 
o Task: model image patches of                                               
size 8√ó8 pixels 
o We assume that a dictionary of  
such image patches is given,  
containing 256 atom images 
o The Sparseland  model assumption:                          
every image patch can be                                              
described as a linear                                    
combination of few atoms 
Œ±1 
Œ±2 
Œ±3 
Œ£ 
Michael Elad 
The Computer-Science Department 
 
The Technion 

17 
   The Sparseland  Model 
o We start with a 8-by-8 pixels patch and 
represent it using 256 numbers         
   ‚Äì This is a redundant representation 
o However, out of those 256 elements in the 
representation, only 3 are non-zeros  
    ‚Äì This is a sparse representation 
o Bottom line in this case: 64 numbers 
representing the patch are replaced by 6  
(3 for the indices of the non-zeros, and 3  
for their entries) 
Properties of this model:                      
        Sparsity and Redundancy 
Œ±1 
Œ±2 
Œ±3 
Œ£ 
Michael Elad 
The Computer-Science Department 
 
The Technion 

18 
   Chemistry of Data 
Œ±1 
Œ±2 
Œ±3 
Œ£ 
o Our dictionary stands for the Periodic Table 
containing all the elements 
o Our model follows a similar rationale:                                            
Every molecule is built of few elements 
We could refer to the Sparseland   
model as the chemistry of information: 
Michael Elad 
The Computer-Science Department 
 
The Technion 

19 
   Sparseland : A Formal Description 
MÔÄ† 
m 
n 
A Dictionary 
o Every column in ùêÉ 
(dictionary) is a  
prototype signal (atom) 
o The vector ÔÅ° is 
generated  
with few non-
zeros at arbitrary 
locations and  
values 
A sparse  
vector 
ÔÄΩ
n 
o This is a generative model 
that describes how (we 
believe) signals are created 
x 
Œ± 
ùêÉ 
Michael Elad 
The Computer-Science Department 
 
The Technion 

20 
   Difficulties with Sparseland 
o Problem 1: Given a signal, how                            
can we find its atom decomposition? 
o A simple example:  
ÔÇß
There are 2000 atoms in the dictionary 
ÔÇß
The signal is known to be built of 15 atoms 
 
                                                       possibilities  
 
ÔÇß
If each of these takes 1nano-sec to test,                                      this 
will take ~7.5e20 years to finish !!!!!!  
o So, are we stuck?  
Œ±1 
Œ±2 
Œ±3 
Œ£ 
2000
2.4e
37
15
ÔÉ¶
ÔÉ∂ÔÇª
ÔÄ´
ÔÉß
ÔÉ∑
ÔÉ®
ÔÉ∏
Michael Elad 
The Computer-Science Department 
 
The Technion 

21 
   Atom Decomposition Made Formal 
Greedy methods 
Thresholding/OMP 
Relaxation methods 
Basis-Pursuit 
ÔÇßL0 ‚Äì counting number of 
non-zeros in the vector 
ÔÇßThis is a projection onto  
the Sparseland model 
ÔÇßThese problems are known 
to be NP-Hard problem 
Approximation Algorithms 
minŒ± Œ± 0  s. t. ùêÉŒ± ‚àíy 2 ‚â§Œµ 
minŒ± Œ± 0  s. t.  x = ùêÉŒ± 
m 
n 
ÔÄΩ
ùêÉ 
x 
Œ± 
Michael Elad 
The Computer-Science Department 
 
The Technion 

22 
   Pursuit Algorithms  
Michael Elad 
The Computer-Science Department 
 
The Technion 
Basis Pursuit 
 
Change the L0 into L1  
and then  the problem 
becomes convex and 
manageable  
Matching Pursuit 
 
 Find the support greedily, 
one element at a time 
Thresholding 
 
Multiply y by ùêÉùêì  
and apply shrinkage: 
Œ± = ùí´ùõΩùêÉùêìy   
minŒ± Œ± 0  s. t. ùêÉŒ± ‚àíy 2 ‚â§Œµ 
Approximation Algorithms 
minŒ± Œ± 1   
s. t.  
      ùêÉŒ± ‚àíy 2 ‚â§Œµ 
ÔÅÄ

Œ±1 
Œ±2 
Œ±3 
Œ£ 
23 
   Difficulties with Sparseland 
o There are various pursuit algorithms 
o Here is an example using the Basis Pursuit (L1): 
 
 
 
 
 
 
o Surprising fact: Many of these algorithms are often  
accompanied by theoretical guarantees for their  
success, if the unknown is sparse enough 
Michael Elad 
The Computer-Science Department 
 
The Technion 

24 
   The Mutual Coherence 
o The Mutual Coherence Œº ùêÉ is the largest off-diagonal  
entry in absolute value 
o We will pose all the theoretical results in this talk using  
this property, due to its simplicity 
o You may have heard of other ways to characterize the 
dictionary (Restricted Isometry Property - RIP, Exact  
Recovery Condition - ERC, Babel function, Spark, ‚Ä¶) 
= 
o Compute 
Assume 
normalized 
columns 
ùêÉ 
ùêÉT 
ùêÉTùêÉ 
Michael Elad 
The Computer-Science Department 
 
The Technion 

25 
   Basis-Pursuit Success  
Comments:  
o If ÔÅ•=0 ÔÇÆ Œ± = Œ± 
o This is a worst-case 
analysis ‚Äì better 
bounds exist  
o Similar theorems 
exist for many other 
pursuit algorithms 
Theorem: Given a noisy signal y = ùêÉŒ± + v where v 2 ‚â§Œµ 
and Œ± is sufficiently sparse,   
 
then Basis-Pursuit:   minŒ±  Œ± 1   s. t.  ùêÉŒ± ‚àíy 2 ‚â§ Œµ 
leads to a stable result:  Œ± ‚àíŒ± 2
2 ‚â§
4ùúÄ2
1‚àíŒº 4 Œ± 0‚àí1  
Michael Elad 
The Computer-Science Department 
 
The Technion 
Donoho, Elad & Temlyakov (‚Äò06) 
Œ±  
Œ± 0 < 1
4 1 + 1
Œº  
M 
x 
Œ± 
ùêÉ 
+ 
y 
v 2 ‚â§Œµ 
minŒ±  Œ± 1  
s. t.  
ùêÉŒ± ‚àíy 2 ‚â§ Œµ 
minŒ±  Œ± 0  
s. t.  
ùêÉŒ± ‚àíy 2 ‚â§ Œµ 

26 
   Difficulties with Sparseland 
Œ±1 
Œ±2 
Œ±3 
Œ£ 
o Problem 2: Given a family of signals, how do                      
we find the dictionary to represent it well? 
o Solution: Learn! Gather a large set of                                
signals (many thousands), and find the                                                         
dictionary that sparsifies them 
o Such algorithms were developed in the                               
past 10 years (e.g., K-SVD), and their                          
performance is surprisingly good 
o We will not discuss this matter further  
in this talk due to lack of time 
Michael Elad 
The Computer-Science Department 
 
The Technion 

27 
   Difficulties with Sparseland 
Œ±1 
Œ±2 
Œ±3 
Œ£ 
o Problem 3: Why is this model suitable to                   
describe various sources? e.g., Is it good 
for images? Audio? Stocks? ‚Ä¶  
o General answer: Yes, this model is                                
extremely effective in representing                                    
various sources 
ÔÇß
Theoretical answer: Clear connection  
to other models 
ÔÇß
Empirical answer:  In a large variety of  
signal and image processing (and later  
machine learning), this model has been  
shown to lead to state-of-the-art results 
Michael Elad 
The Computer-Science Department 
 
The Technion 

28 
   Difficulties with Sparseland ? 
o Problem 1: Given an image patch, how   
can we find its atom decomposition ? 
o Problem 2: Given a family of signals,                                    
how do we find the dictionary to                                        
represent it well? 
o Problem 3: Is this model flexible                                      
enough to describe various sources?                                   
E.g., Is it good for images? audio? ‚Ä¶  
Œ±1 
Œ±2 
Œ±3 
Œ£ 
Michael Elad 
The Computer-Science Department 
 
The Technion 

29 
o Sparseland  has a great success in  
signal & image processing &  
machine learning  
o In the past 8-9 years, many books 
were published on this and closely 
related fields 
Michael Elad 
The Computer-Science Department 
 
The Technion 
 This Field has been rapidly GROWING ‚Ä¶  

30 
 A New Massive Open Online Course  
Michael Elad 
The Computer-Science Department 
 
The Technion 

31 
o When handling images, Sparseland  is typically deployed on small 
overlapping patches due to the desire to train the model to fit the 
data better 
 
 
 
o The model assumption is: each patch in the image is believed to 
have a sparse representation w.r.t. a common local dictionary 
o What is the corresponding global model? This brings us to ‚Ä¶ the 
Convolutional Sparse Coding (CSC)  
Michael Elad 
The Computer-Science Department 
 
The Technion 
   Sparseland  for Image Processing 

Multi-Layered Convolutional  
Sparse Modeling 
 
32 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Joint work with 
Vardan Papyan 
Yaniv Romano 
Jeremias Sulam 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Convolutional Sparse Coding (CSC) 
 
[ùêó] =  di
ùëö
i=1
‚àó[Œìi] 
ùëö filters convolved with their sparse 
representations  
An image 
with ùëÅ 
pixels 
i-th feature-map: An 
image of the same size 
as ùêó  holding the sparse 
representation related 
to the i-filter 
The i-th filter of  
small size ùëõ 
33 
This model emerged in 2005-2010, developed and advocated by Yan LeCun and 
others. It serves as the foundation of Convolutional Neural Networks 

Michael Elad 
The Computer-Science Department 
 
The Technion 
oHere is an alternative global sparsity-based model formulation 
 
 
oùêÇi ‚àà‚ÑùùëÅ√óùëÅ is a banded and Circulant  
matrix containing a single atom  
with all of its shifts 
 
 
oùö™i ‚àà‚ÑùùëÅ are the corresponding coefficients  
ordered as column vectors 
ùêó=  ùêÇiùö™i
ùëö
i=1
 
CSC in Matrix Form
 
ùëõ 
ùëÅ 
ùêÇi = 
=
ùêÇ1 ‚ãØ  ùêÇùëö
ùö™1
‚ãÆ
ùö™ùëö
= ùêÉùö™ 
34 

Michael Elad 
The Computer-Science Department 
 
The Technion 
The CSC Dictionary
 
ùêÇ1 ùêÇ2 ùêÇ3 = 
ùêÉ= 
ùëõ 
ùêÉL 
ùëö 
35 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Classical Sparse Theory for CSC ? 
 
Theorem: BP is guaranteed to ‚Äúsucceed‚Äù ‚Ä¶. if  ùö™ùüé<
ùüè
ùüíùüè+
ùüè
ùõç 
min
ùö™   ùö™0   s. t. ùêò‚àíùêÉùö™2 ‚â§Œµ 
o Assuming that ùëö= 2 and ùëõ= 64 we have that [Welch, ‚Äô74] 
 
Œº ‚â•0.063 
 
o Success of pursuits is guaranteed as long as 
         ùö™0 <
1
4 1 +
1
Œº(ùêÉ) ‚â§
1
2 1 +
1
0.063 ‚âà4.2 
o Only few (4) non-zeros GLOBALLY are  
allowed!!! This is a very pessimistic result! 
36 

Michael Elad 
The Computer-Science Department 
 
The Technion 
=
 
ùêëiùêó= ùõÄùõÑi 
ùëõ 
(2ùëõ‚àí1)ùëö 
ùêëiùêó 
ùëõ 
(2ùëõ‚àí1)ùëö 
ùêëi+1ùêó 
ùõÑi 
ùõÑi+1
Why CSC?
 
ùêó= ùêÉùö™ 
stripe-dictionary
 
Every patch has a sparse 
representation w.r.t. to the 
same local dictionary (ùõÄ) just 
as assumed for images 
stripe vector
 
37 
ùêëi+1ùêó= ùõÄùõÑi+1 
‚Ñ¶
 

Michael Elad 
The Computer-Science Department 
 
The Technion 
 
 
 
 
 
 
 
The main question we aim to address is this:  
 
Can we generalize the vast theory of Sparseland to this  
new notion of local sparsity? For example, could we  
provide guarantees for success for pursuit algorithms? 
ùëö= 2 
Moving to Local Sparsity: Stripes  
min
ùö™  ùö™0,‚àû
s
  s. t.  ùêò‚àíùêÉùö™2 ‚â§Œµ 
‚Ñì0,‚àû Norm:   ùö™0,‚àû
s
= max
i   ùõÑi 0 
  ùö™0,‚àû
s
 is low ÔÇÆ all  ùõÑi are sparse ÔÇÆ every 
patch has a sparse representation over ùõÄ 
38 
ùõÑi 
ùõÑi+1 
ùö™ 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Success of the Basis Pursuit  
39 
Theorem: For Y = ùêÉŒì + E, if Œª = 4 E 2,‚àû
p
 , if  
ùö™ùüé,‚àû
ùê¨
< ùüè
ùüëùüè+
ùüè
ùõçùêÉ
 
then Basis Pursuit performs very-well: 
1.  The support of ŒìBP is contained in that of Œì 
2.
 ŒìBP ‚àíŒì ‚àû‚â§7.5 E 2,‚àû
p
 
3.
 Every entry greater than 7.5 E 2,‚àû
p
 is found 
4.
 ŒìBP is unique 
ŒìBP = min
Œì    1
2 Y ‚àíùêÉŒì 2
2 + Œª Œì 1 
Papyan, Sulam 
& Elad (‚Äò17) 
This is a much better 
result ‚Äì it allows few 
non-zeros locally in 
each stripe, implying 
a permitted O ùëÅ 
non-zeros globally  

Multi-Layered Convolutional  
Sparse Modeling 
 
40 
Michael Elad 
The Computer-Science Department 
 
The Technion 

Michael Elad 
The Computer-Science Department 
 
The Technion 
From CSC to Multi-Layered CSC 
ùêó‚àà‚ÑùùëÅ 
ùëö1 
ùëõ0 
ùêÉ1 ‚àà‚ÑùùëÅ√óùëÅùëö1 
ùëõ1ùëö1 
ùëö2 
ùêÉ2 ‚àà‚ÑùùëÅùëö1√óùëÅùëö2 
ùëö1 
ùö™1 ‚àà‚ÑùùëÅùëö1 
ùö™1 ‚àà‚ÑùùëÅùëö1 
ùö™2 ‚àà‚ÑùùëÅùëö2 
Convolutional sparsity 
(CSC) assumes an 
inherent structure is 
present in natural 
signals 
We propose to impose the 
same structure on the 
representations themselves 
Multi-Layer CSC (ML-CSC)
 
41 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Intuition: From Atoms to Molecules 
42 
& 
ùêó 
ùêÉ1 
ùö™1 
ùêÉ2 
ùö™2 
ùêó 
ùêÉ1 
o The atoms of ùêÉ1ùêÉ2 are 
combinations of atoms from ùêÉ1 
- these are now molecules 
o Thus, this model offers 
different levels of abstraction  
in describing X   
atoms  
molecules  
cells 
 tissue 
body-parts  ‚Ä¶ 
 
 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Intuition: From Atoms to Molecules 
43 
ùêÉ2 
ùêó 
ùêÉ1 
ùêÉK 
ùö™K 
 
 ùêÉeff = ùêÉ1ùêÉ2ùêÉ3 ‚àô‚àô‚àôùêÉK     ÔÇÆ      ùê±= ùêÉeff ùö™K   
 
o This is a special Sparseland  (indeed, a CSC) model 
o However: A key property in our model: the intermediate 
representations are required to be sparse as well 

Michael Elad 
The Computer-Science Department 
 
The Technion 
A Small Taste: Model Training (MNIST) 
ùêÉ1ùêÉ2ùêÉ3  (28√ó28) 
  
MNIST Dictionary: 
‚Ä¢D1:  32 filters of size 7√ó7, with stride of 2 (dense) 
‚Ä¢D2: 128 filters of size 5√ó5√ó32 with stride of 1 -  99.09 % sparse 
‚Ä¢D3: 1024 filters of size 7√ó7√ó128 ‚Äì 99.89 % sparse 
ùêÉ1ùêÉ2 (15√ó15) 
 
ùêÉ1 (7√ó7) 
44 

Michael Elad 
The Computer-Science Department 
 
The Technion 
ML-CSC: Pursuit 
o Deep‚ÄìCoding Problem ùêÉùêÇùêèŒª  (dictionaries are known): 
 
Find   ùö™j j=1
K     ùë†. ùë°.  
ùêó= ùêÉ1ùö™1
ùö™1 0,‚àû
s
‚â§Œª1
ùö™1 = ùêÉ2ùö™2
ùö™2 0,‚àû
s
‚â§Œª2
‚ãÆ
‚ãÆ
ùö™K‚àí1 = ùêÉKùö™K
ùö™K 0,‚àû
s
‚â§ŒªK
   
 
o Or, more realistically for noisy signals,  
        Find   ùö™j j=1
K     ùë†. ùë°.  
ùêò‚àíùêÉ1ùö™1 2 ‚â§‚Ñ∞
ùö™1 0,‚àû
s
‚â§Œª1
ùö™1 = ùêÉ2ùö™2
ùö™2 0,‚àû
s
‚â§Œª2
‚ãÆ
‚ãÆ
ùö™K‚àí1 = ùêÉKùö™K
ùö™K 0,‚àû
s
‚â§ŒªK
 
 
45 

Michael Elad 
The Computer-Science Department 
 
The Technion 
A Small Taste: Pursuit 
Œì1 
Œì2 
Œì3 
Œì0 
Y 
99.51% sparse 
(5 nnz) 
99.52% sparse 
(30 nnz) 
94.51 % sparse 
(213 nnz) 
46 
x=ùêÉ1Œì1 
 
x=ùêÉ1ùêÉ2Œì2 
 
x=ùêÉ1ùêÉ2ùêÉ3Œì3 
x 

Michael Elad 
The Computer-Science Department 
 
The Technion 
The simplest pursuit algorithm (single-layer case)  is  
the THR algorithm, which operates on a given input signal ùêò by: 
 
 
 
 
ùö™ = ùí´ùõΩùêÉTùêò 
ML-CSC: The Simplest Pursuit 
ùêò= ùêÉùö™+ ùêÑ  
              and ùö™ is sparse 
47 

Michael Elad 
The Computer-Science Department 
 
The Technion 
The layered (soft nonnegative) 
thresholding and the CNN forward pass 
algorithm are the very same thing !!!
 
oLayered Thresholding (LT): 
 
 
 
 
oNow let‚Äôs take a look at how Conv. Neural Network operates: 
 
 
ùö™ 2 = ùí´Œ≤2 ùêÉ2
T ùí´Œ≤1 ùêÉ1
Tùêò
 
ùö™ 2 = ùí´Œ≤2 ùêÉ2
T ùí´Œ≤1 ùêÉ1
Tùêò
 
ùö™ 2 = ùí´Œ≤2 ùêÉ2
T ùí´Œ≤1 ùêÉ1
Tùêò
 
Consider this for Solving the DCP
 
Estimate ùö™1 via the THR algorithm
 
Estimate ùö™2 via the THR algorithm
 
ùêÉùêÇùêèŒª
‚Ñ∞:  Find   ùö™j j=1
K     ùë†. ùë°.   
ùêò‚àíùêÉ1ùö™1 2 ‚â§‚Ñ∞
ùö™1 0,‚àû
s
‚â§Œª1
ùö™1 = ùêÉ2ùö™2
ùö™2 0,‚àû
s
‚â§Œª2
‚ãÆ
‚ãÆ
ùö™K‚àí1 = ùêÉKùö™K
ùö™K 0,‚àû
s
‚â§ŒªK
 
ùëìùêò= ReLU ùêõ2 + ùêñ2
T ReLU ùêõ1 + ùêñ1
Tùêò
 
48 

Michael Elad 
The Computer-Science Department 
 
The Technion 
ùêò 
Theoretical Path
 
M
 
A
 
ùö™ i i=1
K  
ùêó= ùêÉ1ùö™1 
ùö™1 = ùêÉ2ùö™2 
‚ãÆ 
ùö™K‚àí1 = ùêÉKùö™K 
 
ùö™i is ùêã0,‚àû sparse
 
ùêÉùêÇùêèŒª
‚Ñ∞ 
  
Layered THR
 
(Forward Pass)
 
 
Maybe other?
 
ùêó 
Armed with this view of a generative source model, we 
may ask new and daring theoretical questions 
49 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Success of the Layered-THR 
Theorem: If ùö™i 0,‚àû
s
<
1
2 1 +
1
Œº ùêÉi ‚ãÖ
ùö™ i
min
ùö™ i
max
‚àí
1
Œº ùêÉi ‚ãÖ
ŒµL
i‚àí1
ùö™ i
max  
then the Layered Hard THR (with the proper thresholds)     
finds the correct supports  and  ùö™ i
LT ‚àíùö™i 2,‚àû
p
‚â§ŒµL
i ,  where 
we have defined ŒµL
0 =
ùêÑ2,‚àû
p
 and 
ŒµL
i =
ùö™i 0,‚àû
p
‚ãÖŒµL
i‚àí1 + Œº ùêÉi
ùö™i 0,‚àû
s
‚àí1 ùö™ i
max
 
The stability of the forward pass is guaranteed 
if the underlying representations are locally 
sparse and the noise is locally bounded
 
Problems:  
1. Contrast 
2. Error growth 
3. Error even if no noise 
50 
Papyan, Romano & Elad (‚Äò17) 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Layered Basis Pursuit (BP)
 
ùö™1
LBP = min
ùö™1  1
2 ùêò‚àíùêÉ1ùö™1 2
2 + Œª1 ùö™1 1 
ùö™2
LBP = min
ùö™2  1
2 ùö™1
LBP ‚àíùêÉ2ùö™2 2
2+ Œª2 ùö™2 1 
ùêÉùêÇùêèŒª
‚Ñ∞:  Find   ùö™j j=1
K     ùë†. ùë°.   
ùêò‚àíùêÉ1ùö™1 2 ‚â§‚Ñ∞
ùö™1 0,‚àû
s
‚â§Œª1
ùö™1 = ùêÉ2ùö™2
ùö™2 0,‚àû
s
‚â§Œª2
‚ãÆ
‚ãÆ
ùö™K‚àí1 = ùêÉKùö™K
ùö™K 0,‚àû
s
‚â§ŒªK
 
oWe chose the Thresholding algorithm 
due to its simplicity, but we do know 
that there are better pursuit methods 
‚Äì how about using them? 
oLets use the Basis Pursuit instead ‚Ä¶ 
Deconvolutional networks 
[Zeiler, Krishnan, Taylor & Fergus ‚Äò10] 
51 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Success of the Layered BP 
Theorem: Assuming that  ùö™ùê¢ùüé,‚àû
ùê¨
<
ùüè
ùüëùüè+
ùüè
ùõçùêÉùê¢
 
then the Layered Basis Pursuit performs very well:  
 
1.  The support of ùö™ i
LBP is contained in that of ùö™i 
2.  The error is bounded:  ùö™ i
LBP ‚àíùö™i 2,‚àû
p
‚â§ŒµL
i , where  
    ŒµL
i = 7.5i ùêÑ2,‚àû
p
 
ùö™j 0,‚àû
p
i
j=1
 
3. Every entry in ùö™i greater than  
ŒµL
i /
ùö™i 0,‚àû
p
will be found 
52 
Papyan, Romano & Elad (‚Äò17) 
Problems:  
1.
Contrast 
2.
Error growth 
3.
Error even if no noise 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Layered Iterative Soft-Thresholding Algorithm (ISTA): 
 
ùö™j
t = ùíÆŒæj/cj ùö™j
t‚àí1 + ùêÉj
T ùö™ j‚àí1 ‚àíùêÉjùö™j
t‚àí1
 
Layered Iterative Thresholding
 
Layered BP:    ùö™j
LBP = min
ùö™j  
1
2 ùö™j‚àí1
LBP ‚àíùêÉjùö™j 2
2+ Œæj ùö™j 1 
Can be seen as a very deep 
recurrent neural network 
[Gregor & LeCun ‚Äò10] 
t 
j 
j 
Note that our suggestion 
implies that groups of layers 
share the same dictionaries 
53 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Where are the Labels? 
 
M
 
ùêó= ùêÉ1ùö™1 
ùö™1 = ùêÉ2ùö™2 
‚ãÆ 
ùö™K‚àí1 = ùêÉKùö™K 
 
ùö™i is ùêã0,‚àû sparse
 
ùêó 
54 
Answer 1:  
o We do not need labels because everything we 
show refer to the unsupervised case, in which 
we operate on signals, not necessarily in the 
context of recognition 
We presented the ML-CSC as a 
machine that produces signals X 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Where are the Labels? 
 
M
 
ùêó= ùêÉ1ùö™1 
ùö™1 = ùêÉ2ùö™2 
‚ãÆ 
ùö™K‚àí1 = ùêÉKùö™K 
 
ùö™i is ùêã0,‚àû sparse
 
ùêó 
Answer 2:  
o This model could be augmented by a synthesis 
of the corresponding label by:  
  
L ùêó= ùë†ùëñùëîùëõc +  
wj
TŒìj
K
j=1
 
o This assumes that knowing the  
representations suffice for identifying the label  
o A successful pursuit algorithm can lead  
to an accurate recognition if the network is 
augmented by a FC classification layer 
o See our recent paper (on ArXiv), analyzing 
bounds on adversarial noise permitted and the 
influence of the pursuit algorithm 
55 
L ùêó 
We presented the ML-CSC as a 
machine that produces signals X 

Michael Elad 
The Computer-Science Department 
 
The Technion 
What About Learning?  
 
All these models rely on  proper  
Dictionary Learning Algorithms to fulfil their mission:  
ÔÇß
Sparseland: We have unsupervised and supervised such algorithms,  
and a beginning of theory to explain how these work 
ÔÇß
CSC: We have few and only unsupervised methods, and even  
these are not fully stable/clear 
ÔÇß
ML-CSC: We proposed two such algorithms ‚Äì see ArXiv (handling both 
unsupervised and supervised learning) 
56 
ML-CSC   
Multi-Layered 
Convolutional 
Sparse Coding 
Sparseland 
Sparse 
Representation 
Theory 
CSC 
Convolutional 
Sparse  
Coding 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Time to Conclude
 
65 

Michael Elad 
The Computer-Science Department 
 
The Technion 
This Talk
 
A novel interpretation 
and theoretical 
understanding of CNN 
Multi-Layer 
Convolutional  
Sparse Coding 
 
Sparseland 
The desire to 
model data 
Novel View of 
Convolutional  
Sparse Coding 
Take Home Message 1: 
Generative modeling of data 
sources enables algorithm 
development along with 
theoretically analyzing 
algorithms‚Äô performance  
We spoke about the importance of models in signal/image 
processing and described Sparseland in details 
We presented a theoretical study of the CSC  model and  
how to operate locally while getting global optimality  
We propose a multi-layer extension of  
CSC, shown to be tightly connected to CNN 
The ML-CSC was shown to enable a theoretical  
study of CNN, along with new insights  
66 
Take Home Message 2: 
The Multi-Layer 
Convolutional Sparse 
Coding model could be 
a new platform for 
understanding and 
developing deep-
learning solutions  

Michael Elad 
The Computer-Science Department 
 
The Technion 
More on these (including these slides and the relevant papers) can be 
found in http://www.cs.technion.ac.il/~elad  
Questions? 

