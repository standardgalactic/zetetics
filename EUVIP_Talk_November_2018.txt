Sparse Modeling of Data  
and its Relation to  
Deep Learning  
Michael Elad 
 
Computer Science Department  
The Technion - Israel Institute of Technology 
Haifa 32000, Israel 
 
The research leading to these results has been received funding 
from the European union's Seventh Framework Program 
(FP/2007-2013) ERC grant Agreement ERC-SPARSE- 320649 
Tuesday, November 27th 2018 

   This Lecture is About â€¦  
2 
Michael Elad 
The Computer-Science Department 
 
The Technion 
A Proposed Theory for Deep-Learning (DL) 
 
Explanation:  
 
o DL has been extremely successful in  
solving a variety of learning problems  
o DL is an empirical field, with numerous  
tricks and know-how, but almost no  
theoretical  foundations 
o A theory for DL has become the  
holy-grail of current research in  
Machine-Learning and related fields  

   Who Needs Theory ?  
3 
Michael Elad 
The Computer-Science Department 
 
The Technion 
We All Do !!  
 
                           â€¦ because â€¦ A theory  
 
o â€¦ could bring the next rounds of ideas  
to this field, breaking existing barriers  
and opening new opportunities 
o â€¦ could map clearly the limitations of 
existing DL solutions, and point to key 
features that control their performance 
o â€¦ could remove the feeling with many  
of us that DL is a â€œdark magicâ€, turning  
it into a solid scientific discipline 
 
Understanding is a good 
thing â€¦ but another goal is 
inventing methods. In the 
history of science and 
technology, engineering  
preceded theoretical understanding:  
ï‚§Lens & telescope ï‚® Optics 
ï‚§Steam engine ï‚® Thermodynamics 
ï‚§Airplane ï‚® Aerodynamics  
ï‚§Radio & Comm. ï‚® Info. Theory  
ï‚§Computer ï‚® Computer Science 
â€œMachine 
learning has 
become 
alchemyâ€ 
Ali Rahimi: 
NIPS 2017 
Test-of-Time 
Award 
Yan LeCun 

   A Theory for DL ? 
4 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Architecture 
Algorithms 
Data 
Rene Vidal  (JHU): Explained the ability to optimize the typical non-
convex objective and yet get to a global minima 
Naftali Tishby  (HUJI): Introduced the Information Bottleneck (IB) 
concept and demonstrated its relevance to deep learning  
Stefano Soattoâ€™s team (UCLA): Analyzed the Stochastic Gradient 
Descent (SGD) algorithm, connecting it to the IB objective  
Stephane Mallat (ENS) & 
Joan Bruna (NYU): Proposed 
the scattering transform 
(wavelet-based) and 
emphasized the treatment of 
invariances in the input data 
Richard Baraniuk & Ankit 
Patel (RICE): Offered a 
generative probabilistic 
model for the data,  
showing how classic 
architectures and learning 
algorithms relate to it 
Raja Giryes (TAU): Studied the architecture of DNN in the context 
of their ability to give distance-preserving embedding of signals 
Gitta Kutyniok (TU) & Helmut Bolcskei (ETH): Studied the ability of 
DNN architectures to approximate families of functions 

   So, is there a Theory for DL ? 
5 
Michael Elad 
The Computer-Science Department 
 
The Technion 
The answer is tricky: 
 
There are already 
various such attempts, 
and some of them are 
truly impressive 
 
â€¦ but â€¦ 
 
none of them is 
complete 

   Interesting Observations 
6 
Michael Elad 
The Computer-Science Department 
 
The Technion 
o Theory origins: Signal Proc., Control Theory, Info. Theory, Harmonic 
Analysis, Sparse Represen., Quantum Physics, PDE, Machine learning â€¦ 
 
Ron Kimmel: â€œDL is a dark monster covered  
with mirrors. Everyone sees his reflection in it â€¦â€ 
 
David Donoho: â€œâ€¦ these mirrors are taken  
from Cinderella's story, telling each that  
he is the most beautifulâ€ 
 
o Todayâ€™s talk is on our proposed theory: 
 
 â€¦ and yes, our theory is the best 
Vardan Papyan 
Yaniv Romano 
Jeremias Sulam 
     Architecture 
Algorithms 
Data 

ML-CSC   
Multi-Layered 
Convolutional 
Sparse Coding 
Sparseland 
Sparse 
Representation 
Theory 
Another underlying idea that accompanies us 
  
Generative modeling of data sources enables  
o A systematic algorithm development, &   
o A theoretical analysis of their performance  
CSC 
Convolutional 
Sparse  
Coding 
   This Lecture: More Specifically    
7 
  Sparsity-Inspired Models 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Deep-Learning 
Disclaimer: Being a 
lecture on the theory  
of DL, this lecture  
is â€¦ theoretical â€¦ and 
mathematically oriented  

Multi-Layered Convolutional  
Sparse Modeling 
 
8 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Our eventual goal in todayâ€™s talk is to present the â€¦ 
 
 
 
 
 
 
So, lets use this as our running title,  
parse it into words,  
and explain each of them 

Multi-Layered Convolutional  
Sparse Modeling 
 
9 
Michael Elad 
The Computer-Science Department 
 
The Technion 

3D Objects
 
Medical Imaging
 
10 
   Our Data is Structured 
o We are surrounded by various diverse 
sources of massive information 
o Each of these sources have an internal 
structure, which can be exploited 
o This structure, when identified, is the  
engine behind the ability to process data 
o How to identify structure?  
Voice Signals
 
Stock Market
 
Biological Signals
 
Videos
 
Text Documents
 
Radar Imaging
 
Matrix Data
 
     Social Networks
 
Traffic info
 
Seismic Data
 
Still Images
 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Using models 

11 
   Model? 
Effective removal of noise (and many other tasks)       
relies on an proper modeling of the signal 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Fact 1: 
This signal 
contains AWGN 
â„•(0,1)  
 
Fact 2:  
The clean signal 
is believed to  
be PWC 

12 
   Models 
o A model: a mathematical  
description of the underlying  
signal of interest, describing our 
beliefs regarding its structure 
o The following is a partial list of  
commonly used models for images 
o Good models should be simple while 
matching the signals 
 
 
o Models are almost always imperfect 
Principal-Component-Analysis 
   Gaussian-Mixture 
Markov Random Field 
   Laplacian Smoothness 
DCT concentration 
   Wavelet Sparsity 
Piece-Wise-Smoothness 
   C2-smoothness 
Besov-Spaces 
   Total-Variation 
Beltrami-Flow 
 
Simplicity 
 
 
Reliability 
 
Michael Elad 
The Computer-Science Department 
 
The Technion 

13 
   What this Talk is all About?  
Data Models and Their Use 
o Almost any task in data processing requires a model â€“  
true for denoising, deblurring, super-resolution, inpainting, 
compression, anomaly-detection, sampling, recognition, 
separation, and more 
o Sparse and Redundant Representations offer a new and 
highly effective model â€“ we call it  
                                        Sparseland  
o We shall describe this and descendant versions of it that 
lead all the way to â€¦ deep-learning 
Michael Elad 
The Computer-Science Department 
 
The Technion 

Multi-Layered Convolutional  
Sparse Modeling 
 
14 
Michael Elad 
The Computer-Science Department 
 
The Technion 

Machine 
Learning 
15 
 
Mathematics 
Signal   
Processing 
   A New Emerging Model 
Sparseland 
Wavelet 
Theory 
Signal 
Transforms 
Multi-Scale 
Analysis 
Approximation 
Theory 
Linear  
Algebra 
Optimization 
Theory 
Denoising 
Interpolation 
Prediction 
Compression 
Inference (solving 
inverse problems) 
Anomaly 
detection 
Clustering 
Summarizing 
Sensor-Fusion 
Source-
Separation 
Segmentation 
Recognition 
Semi-Supervised 
Learning 
Identification 
Classification 
Synthesis 
Michael Elad 
The Computer-Science Department 
 
The Technion 

16 
   The Sparseland  Model 
o Task: model image patches of                                               
size 8Ã—8 pixels 
o We assume that a dictionary of  
such image patches is given,  
containing 256 atom images 
o The Sparseland  model assumption:                          
every image patch can be                                              
described as a linear                                    
combination of few atoms 
Î±1 
Î±2 
Î±3 
Î£ 
Michael Elad 
The Computer-Science Department 
 
The Technion 

17 
   The Sparseland  Model 
o We start with a 8-by-8 pixels patch and 
represent it using 256 numbers         
   â€“ This is a redundant representation 
o However, out of those 256 elements in the 
representation, only 3 are non-zeros  
    â€“ This is a sparse representation 
o Bottom line in this case: 64 numbers 
representing the patch are replaced by 6  
(3 for the indices of the non-zeros, and 3  
for their entries) 
Properties of this model:                      
        Sparsity and Redundancy 
Î±1 
Î±2 
Î±3 
Î£ 
Michael Elad 
The Computer-Science Department 
 
The Technion 

18 
   Chemistry of Data 
Î±1 
Î±2 
Î±3 
Î£ 
o Our dictionary stands for the Periodic Table 
containing all the elements 
o Our model follows a similar rationale:                                            
Every molecule is built of few elements 
We could refer to the Sparseland   
model as the chemistry of information: 
Michael Elad 
The Computer-Science Department 
 
The Technion 

19 
   Sparseland : A Formal Description 
Mï€  
m 
n 
A Dictionary 
o Every column in ğƒ 
(dictionary) is a  
prototype signal (atom) 
o The vector ï¡ is 
generated  
with few non-
zeros at arbitrary 
locations and  
values 
A sparse  
vector 
ï€½
n 
o This is a generative model 
that describes how (we 
believe) signals are created 
x 
Î± 
ğƒ 
Michael Elad 
The Computer-Science Department 
 
The Technion 

20 
   Difficulties with Sparseland 
o Problem 1: Given a signal, how                            
can we find its atom decomposition? 
o A simple example:  
ï‚§
There are 2000 atoms in the dictionary 
ï‚§
The signal is known to be built of 15 atoms 
 
                                                       possibilities  
 
ï‚§
If each of these takes 1nano-sec to test,                                      this 
will take ~7.5e20 years to finish !!!!!!  
o So, are we stuck?  
Î±1 
Î±2 
Î±3 
Î£ 
2000
2.4e
37
15
ïƒ¦
ïƒ¶ï‚»
ï€«
ïƒ§
ïƒ·
ïƒ¨
ïƒ¸
Michael Elad 
The Computer-Science Department 
 
The Technion 

21 
   Atom Decomposition Made Formal 
Greedy methods 
Thresholding/OMP 
Relaxation methods 
Basis-Pursuit 
ï‚§L0 â€“ counting number of 
non-zeros in the vector 
ï‚§This is a projection onto  
the Sparseland model 
ï‚§These problems are known 
to be NP-Hard problem 
Approximation Algorithms 
minÎ± Î± 0  s. t. ğƒÎ± âˆ’y 2 â‰¤Îµ 
minÎ± Î± 0  s. t.  x = ğƒÎ± 
m 
n 
ï€½
ğƒ 
x 
Î± 
Michael Elad 
The Computer-Science Department 
 
The Technion 

22 
   Pursuit Algorithms  
Michael Elad 
The Computer-Science Department 
 
The Technion 
Basis Pursuit 
 
Change the L0 into L1  
and then  the problem 
becomes convex and 
manageable  
Matching Pursuit 
 
 Find the support greedily, 
one element at a time 
Thresholding 
 
Multiply y by ğƒğ“  
and apply shrinkage: 
Î± = ğ’«ğ›½ğƒğ“y   
minÎ± Î± 0  s. t. ğƒÎ± âˆ’y 2 â‰¤Îµ 
Approximation Algorithms 
minÎ± Î± 1   
s. t.  
      ğƒÎ± âˆ’y 2 â‰¤Îµ 
ï€

Î±1 
Î±2 
Î±3 
Î£ 
23 
   Difficulties with Sparseland 
o There are various pursuit algorithms 
o Here is an example using the Basis Pursuit (L1): 
 
 
 
 
 
 
o Surprising fact: Many of these algorithms are often  
accompanied by theoretical guarantees for their  
success, if the unknown is sparse enough 
Michael Elad 
The Computer-Science Department 
 
The Technion 

24 
   The Mutual Coherence 
o The Mutual Coherence Î¼ ğƒ is the largest off-diagonal  
entry in absolute value 
o We will pose all the theoretical results in this talk using  
this property, due to its simplicity 
o You may have heard of other ways to characterize the 
dictionary (Restricted Isometry Property - RIP, Exact  
Recovery Condition - ERC, Babel function, Spark, â€¦) 
= 
o Compute 
Assume 
normalized 
columns 
ğƒ 
ğƒT 
ğƒTğƒ 
Michael Elad 
The Computer-Science Department 
 
The Technion 

25 
   Basis-Pursuit Success  
Comments:  
o If ï¥=0 ï‚® Î± = Î± 
o This is a worst-case 
analysis â€“ better 
bounds exist  
o Similar theorems 
exist for many other 
pursuit algorithms 
Theorem: Given a noisy signal y = ğƒÎ± + v where v 2 â‰¤Îµ 
and Î± is sufficiently sparse,   
 
then Basis-Pursuit:   minÎ±  Î± 1   s. t.  ğƒÎ± âˆ’y 2 â‰¤ Îµ 
leads to a stable result:  Î± âˆ’Î± 2
2 â‰¤
4ğœ€2
1âˆ’Î¼ 4 Î± 0âˆ’1  
Michael Elad 
The Computer-Science Department 
 
The Technion 
Donoho, Elad & Temlyakov (â€˜06) 
Î±  
Î± 0 < 1
4 1 + 1
Î¼  
M 
x 
Î± 
ğƒ 
+ 
y 
v 2 â‰¤Îµ 
minÎ±  Î± 1  
s. t.  
ğƒÎ± âˆ’y 2 â‰¤ Îµ 
minÎ±  Î± 0  
s. t.  
ğƒÎ± âˆ’y 2 â‰¤ Îµ 

26 
   Difficulties with Sparseland 
Î±1 
Î±2 
Î±3 
Î£ 
o Problem 2: Given a family of signals, how do                      
we find the dictionary to represent it well? 
o Solution: Learn! Gather a large set of                                
signals (many thousands), and find the                                                         
dictionary that sparsifies them 
o Such algorithms were developed in the                               
past 10 years (e.g., K-SVD), and their                          
performance is surprisingly good 
o We will not discuss this matter further  
in this talk due to lack of time 
Michael Elad 
The Computer-Science Department 
 
The Technion 

27 
   Difficulties with Sparseland 
Î±1 
Î±2 
Î±3 
Î£ 
o Problem 3: Why is this model suitable to                   
describe various sources? e.g., Is it good 
for images? Audio? Stocks? â€¦  
o General answer: Yes, this model is                                
extremely effective in representing                                    
various sources 
ï‚§
Theoretical answer: Clear connection  
to other models 
ï‚§
Empirical answer:  In a large variety of  
signal and image processing (and later  
machine learning), this model has been  
shown to lead to state-of-the-art results 
Michael Elad 
The Computer-Science Department 
 
The Technion 

28 
   Difficulties with Sparseland ? 
o Problem 1: Given an image patch, how   
can we find its atom decomposition ? 
o Problem 2: Given a family of signals,                                    
how do we find the dictionary to                                        
represent it well? 
o Problem 3: Is this model flexible                                      
enough to describe various sources?                                   
E.g., Is it good for images? audio? â€¦  
Î±1 
Î±2 
Î±3 
Î£ 
Michael Elad 
The Computer-Science Department 
 
The Technion 

29 
o Sparseland  has a great success in  
signal & image processing &  
machine learning  
o In the past 8-9 years, many books 
were published on this and closely 
related fields 
Michael Elad 
The Computer-Science Department 
 
The Technion 
 This Field has been rapidly GROWING â€¦  

30 
 A New Massive Open Online Course  
Michael Elad 
The Computer-Science Department 
 
The Technion 

31 
o When handling images, Sparseland  is typically deployed on small 
overlapping patches due to the desire to train the model to fit the 
data better 
 
 
 
o The model assumption is: each patch in the image is believed to 
have a sparse representation w.r.t. a common local dictionary 
o What is the corresponding global model? This brings us to â€¦ the 
Convolutional Sparse Coding (CSC)  
Michael Elad 
The Computer-Science Department 
 
The Technion 
   Sparseland  for Image Processing 

Multi-Layered Convolutional  
Sparse Modeling 
 
32 
Michael Elad 
The Computer-Science Department 
 
The Technion 
Joint work with 
Vardan Papyan 
Yaniv Romano 
Jeremias Sulam 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Convolutional Sparse Coding (CSC) 
 
[ğ—] =  di
ğ‘š
i=1
âˆ—[Î“i] 
ğ‘š filters convolved with their sparse 
representations  
An image 
with ğ‘ 
pixels 
i-th feature-map: An 
image of the same size 
as ğ—  holding the sparse 
representation related 
to the i-filter 
The i-th filter of  
small size ğ‘› 
33 
This model emerged in 2005-2010, developed and advocated by Yan LeCun and 
others. It serves as the foundation of Convolutional Neural Networks 

Michael Elad 
The Computer-Science Department 
 
The Technion 
oHere is an alternative global sparsity-based model formulation 
 
 
oğ‚i âˆˆâ„ğ‘Ã—ğ‘ is a banded and Circulant  
matrix containing a single atom  
with all of its shifts 
 
 
oğšªi âˆˆâ„ğ‘ are the corresponding coefficients  
ordered as column vectors 
ğ—=  ğ‚iğšªi
ğ‘š
i=1
 
CSC in Matrix Form
 
ğ‘› 
ğ‘ 
ğ‚i = 
=
ğ‚1 â‹¯  ğ‚ğ‘š
ğšª1
â‹®
ğšªğ‘š
= ğƒğšª 
34 

Michael Elad 
The Computer-Science Department 
 
The Technion 
The CSC Dictionary
 
ğ‚1 ğ‚2 ğ‚3 = 
ğƒ= 
ğ‘› 
ğƒL 
ğ‘š 
35 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Classical Sparse Theory for CSC ? 
 
Theorem: BP is guaranteed to â€œsucceedâ€ â€¦. if  ğšªğŸ<
ğŸ
ğŸ’ğŸ+
ğŸ
ğ› 
min
ğšª   ğšª0   s. t. ğ˜âˆ’ğƒğšª2 â‰¤Îµ 
o Assuming that ğ‘š= 2 and ğ‘›= 64 we have that [Welch, â€™74] 
 
Î¼ â‰¥0.063 
 
o Success of pursuits is guaranteed as long as 
         ğšª0 <
1
4 1 +
1
Î¼(ğƒ) â‰¤
1
2 1 +
1
0.063 â‰ˆ4.2 
o Only few (4) non-zeros GLOBALLY are  
allowed!!! This is a very pessimistic result! 
36 

Michael Elad 
The Computer-Science Department 
 
The Technion 
=
 
ğ‘iğ—= ğ›€ğ›„i 
ğ‘› 
(2ğ‘›âˆ’1)ğ‘š 
ğ‘iğ— 
ğ‘› 
(2ğ‘›âˆ’1)ğ‘š 
ğ‘i+1ğ— 
ğ›„i 
ğ›„i+1
Why CSC?
 
ğ—= ğƒğšª 
stripe-dictionary
 
Every patch has a sparse 
representation w.r.t. to the 
same local dictionary (ğ›€) just 
as assumed for images 
stripe vector
 
37 
ğ‘i+1ğ—= ğ›€ğ›„i+1 
â„¦
 

Michael Elad 
The Computer-Science Department 
 
The Technion 
 
 
 
 
 
 
 
The main question we aim to address is this:  
 
Can we generalize the vast theory of Sparseland to this  
new notion of local sparsity? For example, could we  
provide guarantees for success for pursuit algorithms? 
ğ‘š= 2 
Moving to Local Sparsity: Stripes  
min
ğšª  ğšª0,âˆ
s
  s. t.  ğ˜âˆ’ğƒğšª2 â‰¤Îµ 
â„“0,âˆ Norm:   ğšª0,âˆ
s
= max
i   ğ›„i 0 
  ğšª0,âˆ
s
 is low ï‚® all  ğ›„i are sparse ï‚® every 
patch has a sparse representation over ğ›€ 
38 
ğ›„i 
ğ›„i+1 
ğšª 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Success of the Basis Pursuit  
39 
Theorem: For Y = ğƒÎ“ + E, if Î» = 4 E 2,âˆ
p
 , if  
ğšªğŸ,âˆ
ğ¬
< ğŸ
ğŸ‘ğŸ+
ğŸ
ğ›ğƒ
 
then Basis Pursuit performs very-well: 
1.  The support of Î“BP is contained in that of Î“ 
2.
 Î“BP âˆ’Î“ âˆâ‰¤7.5 E 2,âˆ
p
 
3.
 Every entry greater than 7.5 E 2,âˆ
p
 is found 
4.
 Î“BP is unique 
Î“BP = min
Î“    1
2 Y âˆ’ğƒÎ“ 2
2 + Î» Î“ 1 
Papyan, Sulam 
& Elad (â€˜17) 
This is a much better 
result â€“ it allows few 
non-zeros locally in 
each stripe, implying 
a permitted O ğ‘ 
non-zeros globally  

Multi-Layered Convolutional  
Sparse Modeling 
 
40 
Michael Elad 
The Computer-Science Department 
 
The Technion 

Michael Elad 
The Computer-Science Department 
 
The Technion 
From CSC to Multi-Layered CSC 
ğ—âˆˆâ„ğ‘ 
ğ‘š1 
ğ‘›0 
ğƒ1 âˆˆâ„ğ‘Ã—ğ‘ğ‘š1 
ğ‘›1ğ‘š1 
ğ‘š2 
ğƒ2 âˆˆâ„ğ‘ğ‘š1Ã—ğ‘ğ‘š2 
ğ‘š1 
ğšª1 âˆˆâ„ğ‘ğ‘š1 
ğšª1 âˆˆâ„ğ‘ğ‘š1 
ğšª2 âˆˆâ„ğ‘ğ‘š2 
Convolutional sparsity 
(CSC) assumes an 
inherent structure is 
present in natural 
signals 
We propose to impose the 
same structure on the 
representations themselves 
Multi-Layer CSC (ML-CSC)
 
41 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Intuition: From Atoms to Molecules 
42 
& 
ğ— 
ğƒ1 
ğšª1 
ğƒ2 
ğšª2 
ğ— 
ğƒ1 
o The atoms of ğƒ1ğƒ2 are 
combinations of atoms from ğƒ1 
- these are now molecules 
o Thus, this model offers 
different levels of abstraction  
in describing X   
atoms  
molecules  
cells 
 tissue 
body-parts  â€¦ 
 
 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Intuition: From Atoms to Molecules 
43 
ğƒ2 
ğ— 
ğƒ1 
ğƒK 
ğšªK 
 
 ğƒeff = ğƒ1ğƒ2ğƒ3 âˆ™âˆ™âˆ™ğƒK     ï‚®      ğ±= ğƒeff ğšªK   
 
o This is a special Sparseland  (indeed, a CSC) model 
o However: A key property in our model: the intermediate 
representations are required to be sparse as well 

Michael Elad 
The Computer-Science Department 
 
The Technion 
A Small Taste: Model Training (MNIST) 
ğƒ1ğƒ2ğƒ3  (28Ã—28) 
  
MNIST Dictionary: 
â€¢D1:  32 filters of size 7Ã—7, with stride of 2 (dense) 
â€¢D2: 128 filters of size 5Ã—5Ã—32 with stride of 1 -  99.09 % sparse 
â€¢D3: 1024 filters of size 7Ã—7Ã—128 â€“ 99.89 % sparse 
ğƒ1ğƒ2 (15Ã—15) 
 
ğƒ1 (7Ã—7) 
44 

Michael Elad 
The Computer-Science Department 
 
The Technion 
ML-CSC: Pursuit 
o Deepâ€“Coding Problem ğƒğ‚ğÎ»  (dictionaries are known): 
 
Find   ğšªj j=1
K     ğ‘ . ğ‘¡.  
ğ—= ğƒ1ğšª1
ğšª1 0,âˆ
s
â‰¤Î»1
ğšª1 = ğƒ2ğšª2
ğšª2 0,âˆ
s
â‰¤Î»2
â‹®
â‹®
ğšªKâˆ’1 = ğƒKğšªK
ğšªK 0,âˆ
s
â‰¤Î»K
   
 
o Or, more realistically for noisy signals,  
        Find   ğšªj j=1
K     ğ‘ . ğ‘¡.  
ğ˜âˆ’ğƒ1ğšª1 2 â‰¤â„°
ğšª1 0,âˆ
s
â‰¤Î»1
ğšª1 = ğƒ2ğšª2
ğšª2 0,âˆ
s
â‰¤Î»2
â‹®
â‹®
ğšªKâˆ’1 = ğƒKğšªK
ğšªK 0,âˆ
s
â‰¤Î»K
 
 
45 

Michael Elad 
The Computer-Science Department 
 
The Technion 
A Small Taste: Pursuit 
Î“1 
Î“2 
Î“3 
Î“0 
Y 
99.51% sparse 
(5 nnz) 
99.52% sparse 
(30 nnz) 
94.51 % sparse 
(213 nnz) 
46 
x=ğƒ1Î“1 
 
x=ğƒ1ğƒ2Î“2 
 
x=ğƒ1ğƒ2ğƒ3Î“3 
x 

Michael Elad 
The Computer-Science Department 
 
The Technion 
The simplest pursuit algorithm (single-layer case)  is  
the THR algorithm, which operates on a given input signal ğ˜ by: 
 
 
 
 
ğšª = ğ’«ğ›½ğƒTğ˜ 
ML-CSC: The Simplest Pursuit 
ğ˜= ğƒğšª+ ğ„  
              and ğšª is sparse 
47 

Michael Elad 
The Computer-Science Department 
 
The Technion 
The layered (soft nonnegative) 
thresholding and the CNN forward pass 
algorithm are the very same thing !!!
 
oLayered Thresholding (LT): 
 
 
 
 
oNow letâ€™s take a look at how Conv. Neural Network operates: 
 
 
ğšª 2 = ğ’«Î²2 ğƒ2
T ğ’«Î²1 ğƒ1
Tğ˜
 
ğšª 2 = ğ’«Î²2 ğƒ2
T ğ’«Î²1 ğƒ1
Tğ˜
 
ğšª 2 = ğ’«Î²2 ğƒ2
T ğ’«Î²1 ğƒ1
Tğ˜
 
Consider this for Solving the DCP
 
Estimate ğšª1 via the THR algorithm
 
Estimate ğšª2 via the THR algorithm
 
ğƒğ‚ğÎ»
â„°:  Find   ğšªj j=1
K     ğ‘ . ğ‘¡.   
ğ˜âˆ’ğƒ1ğšª1 2 â‰¤â„°
ğšª1 0,âˆ
s
â‰¤Î»1
ğšª1 = ğƒ2ğšª2
ğšª2 0,âˆ
s
â‰¤Î»2
â‹®
â‹®
ğšªKâˆ’1 = ğƒKğšªK
ğšªK 0,âˆ
s
â‰¤Î»K
 
ğ‘“ğ˜= ReLU ğ›2 + ğ–2
T ReLU ğ›1 + ğ–1
Tğ˜
 
48 

Michael Elad 
The Computer-Science Department 
 
The Technion 
ğ˜ 
Theoretical Path
 
M
 
A
 
ğšª i i=1
K  
ğ—= ğƒ1ğšª1 
ğšª1 = ğƒ2ğšª2 
â‹® 
ğšªKâˆ’1 = ğƒKğšªK 
 
ğšªi is ğ‹0,âˆ sparse
 
ğƒğ‚ğÎ»
â„° 
  
Layered THR
 
(Forward Pass)
 
 
Maybe other?
 
ğ— 
Armed with this view of a generative source model, we 
may ask new and daring theoretical questions 
49 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Success of the Layered-THR 
Theorem: If ğšªi 0,âˆ
s
<
1
2 1 +
1
Î¼ ğƒi â‹…
ğšª i
min
ğšª i
max
âˆ’
1
Î¼ ğƒi â‹…
ÎµL
iâˆ’1
ğšª i
max  
then the Layered Hard THR (with the proper thresholds)     
finds the correct supports  and  ğšª i
LT âˆ’ğšªi 2,âˆ
p
â‰¤ÎµL
i ,  where 
we have defined ÎµL
0 =
ğ„2,âˆ
p
 and 
ÎµL
i =
ğšªi 0,âˆ
p
â‹…ÎµL
iâˆ’1 + Î¼ ğƒi
ğšªi 0,âˆ
s
âˆ’1 ğšª i
max
 
The stability of the forward pass is guaranteed 
if the underlying representations are locally 
sparse and the noise is locally bounded
 
Problems:  
1. Contrast 
2. Error growth 
3. Error even if no noise 
50 
Papyan, Romano & Elad (â€˜17) 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Layered Basis Pursuit (BP)
 
ğšª1
LBP = min
ğšª1  1
2 ğ˜âˆ’ğƒ1ğšª1 2
2 + Î»1 ğšª1 1 
ğšª2
LBP = min
ğšª2  1
2 ğšª1
LBP âˆ’ğƒ2ğšª2 2
2+ Î»2 ğšª2 1 
ğƒğ‚ğÎ»
â„°:  Find   ğšªj j=1
K     ğ‘ . ğ‘¡.   
ğ˜âˆ’ğƒ1ğšª1 2 â‰¤â„°
ğšª1 0,âˆ
s
â‰¤Î»1
ğšª1 = ğƒ2ğšª2
ğšª2 0,âˆ
s
â‰¤Î»2
â‹®
â‹®
ğšªKâˆ’1 = ğƒKğšªK
ğšªK 0,âˆ
s
â‰¤Î»K
 
oWe chose the Thresholding algorithm 
due to its simplicity, but we do know 
that there are better pursuit methods 
â€“ how about using them? 
oLets use the Basis Pursuit instead â€¦ 
Deconvolutional networks 
[Zeiler, Krishnan, Taylor & Fergus â€˜10] 
51 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Success of the Layered BP 
Theorem: Assuming that  ğšªğ¢ğŸ,âˆ
ğ¬
<
ğŸ
ğŸ‘ğŸ+
ğŸ
ğ›ğƒğ¢
 
then the Layered Basis Pursuit performs very well:  
 
1.  The support of ğšª i
LBP is contained in that of ğšªi 
2.  The error is bounded:  ğšª i
LBP âˆ’ğšªi 2,âˆ
p
â‰¤ÎµL
i , where  
    ÎµL
i = 7.5i ğ„2,âˆ
p
 
ğšªj 0,âˆ
p
i
j=1
 
3. Every entry in ğšªi greater than  
ÎµL
i /
ğšªi 0,âˆ
p
will be found 
52 
Papyan, Romano & Elad (â€˜17) 
Problems:  
1.
Contrast 
2.
Error growth 
3.
Error even if no noise 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Layered Iterative Soft-Thresholding Algorithm (ISTA): 
 
ğšªj
t = ğ’®Î¾j/cj ğšªj
tâˆ’1 + ğƒj
T ğšª jâˆ’1 âˆ’ğƒjğšªj
tâˆ’1
 
Layered Iterative Thresholding
 
Layered BP:    ğšªj
LBP = min
ğšªj  
1
2 ğšªjâˆ’1
LBP âˆ’ğƒjğšªj 2
2+ Î¾j ğšªj 1 
Can be seen as a very deep 
recurrent neural network 
[Gregor & LeCun â€˜10] 
t 
j 
j 
Note that our suggestion 
implies that groups of layers 
share the same dictionaries 
53 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Where are the Labels? 
 
M
 
ğ—= ğƒ1ğšª1 
ğšª1 = ğƒ2ğšª2 
â‹® 
ğšªKâˆ’1 = ğƒKğšªK 
 
ğšªi is ğ‹0,âˆ sparse
 
ğ— 
54 
Answer 1:  
o We do not need labels because everything we 
show refer to the unsupervised case, in which 
we operate on signals, not necessarily in the 
context of recognition 
We presented the ML-CSC as a 
machine that produces signals X 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Where are the Labels? 
 
M
 
ğ—= ğƒ1ğšª1 
ğšª1 = ğƒ2ğšª2 
â‹® 
ğšªKâˆ’1 = ğƒKğšªK 
 
ğšªi is ğ‹0,âˆ sparse
 
ğ— 
Answer 2:  
o This model could be augmented by a synthesis 
of the corresponding label by:  
  
L ğ—= ğ‘ ğ‘–ğ‘”ğ‘›c +  
wj
TÎ“j
K
j=1
 
o This assumes that knowing the  
representations suffice for identifying the label  
o A successful pursuit algorithm can lead  
to an accurate recognition if the network is 
augmented by a FC classification layer 
o See our recent paper (on ArXiv), analyzing 
bounds on adversarial noise permitted and the 
influence of the pursuit algorithm 
55 
L ğ— 
We presented the ML-CSC as a 
machine that produces signals X 

Michael Elad 
The Computer-Science Department 
 
The Technion 
What About Learning?  
 
All these models rely on  proper  
Dictionary Learning Algorithms to fulfil their mission:  
ï‚§
Sparseland: We have unsupervised and supervised such algorithms,  
and a beginning of theory to explain how these work 
ï‚§
CSC: We have few and only unsupervised methods, and even  
these are not fully stable/clear 
ï‚§
ML-CSC: We proposed two such algorithms â€“ see ArXiv (handling both 
unsupervised and supervised learning) 
56 
ML-CSC   
Multi-Layered 
Convolutional 
Sparse Coding 
Sparseland 
Sparse 
Representation 
Theory 
CSC 
Convolutional 
Sparse  
Coding 

Michael Elad 
The Computer-Science Department 
 
The Technion 
Time to Conclude
 
65 

Michael Elad 
The Computer-Science Department 
 
The Technion 
This Talk
 
A novel interpretation 
and theoretical 
understanding of CNN 
Multi-Layer 
Convolutional  
Sparse Coding 
 
Sparseland 
The desire to 
model data 
Novel View of 
Convolutional  
Sparse Coding 
Take Home Message 1: 
Generative modeling of data 
sources enables algorithm 
development along with 
theoretically analyzing 
algorithmsâ€™ performance  
We spoke about the importance of models in signal/image 
processing and described Sparseland in details 
We presented a theoretical study of the CSC  model and  
how to operate locally while getting global optimality  
We propose a multi-layer extension of  
CSC, shown to be tightly connected to CNN 
The ML-CSC was shown to enable a theoretical  
study of CNN, along with new insights  
66 
Take Home Message 2: 
The Multi-Layer 
Convolutional Sparse 
Coding model could be 
a new platform for 
understanding and 
developing deep-
learning solutions  

Michael Elad 
The Computer-Science Department 
 
The Technion 
More on these (including these slides and the relevant papers) can be 
found in http://www.cs.technion.ac.il/~elad  
Questions? 

