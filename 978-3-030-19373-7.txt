Symplectic 
Difference Systems: 
Oscillation and 
Spectral Theory
Ondrˇej Došlý
Julia Elyseeva
Roman Šimon Hilscher
Pathways in Mathematics

Pathways in Mathematics
Series Editors
T. Hibi
Toyonaka, Japan
W. König
Berlin, Germany
J. Zimmer
Bath, United Kingdom

Each “Pathways in Mathematics” book offers a roadmap to a currently well devel-
oping mathematical research ﬁeld and is a ﬁrst-hand information and inspiration
for further study, aimed both at students and researchers. It is written in an
educational style, i.e., in a way that is accessible for advanced undergraduate and
graduate students. It also serves as an introduction to and survey of the ﬁeld for
researchers who want to be quickly informed about the state of the art. The point of
departure is typically a bachelor/masters level background, from which the reader
is expeditiously guided to the frontiers. This is achieved by focusing on ideas and
concepts underlying the development of the subject while keeping technicalities to
a minimum. Each volume contains an extensive annotated bibliography as well as a
discussion of open problems and future research directions as recommendations for
starting new projects
More information about this series at http://www.springer.com/series/15133

Ondˇrej Došlý • Julia Elyseeva •
Roman Šimon Hilscher
Symplectic Difference
Systems: Oscillation
and Spectral Theory

Ondˇrej Došlý (deceased)
Department of Mathematics and Statistics
Faculty of Science
Masaryk University
Brno, Czech Republic
Julia Elyseeva
Department of Applied Mathematics
Moscow State Technological University
“STANKIN”
Moscow, Russia
Roman Šimon Hilscher
Department of Mathematics and Statistics
Faculty of Science
Masaryk University
Brno, Czech Republic
ISSN 2367-3451
ISSN 2367-346X
(electronic)
Pathways in Mathematics
ISBN 978-3-030-19372-0
ISBN 978-3-030-19373-7
(eBook)
https://doi.org/10.1007/978-3-030-19373-7
Mathematics Subject Classiﬁcation (2010): 39A21, 39A12, 47B39
© Springer Nature Switzerland AG 2019
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afﬁliations.
This book is published under the imprint Birkhäuser, www.birkhauser-science.com, by the registered
company Springer Nature Switzerland AG.
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
The principal concern of the book is the qualitative theory of symplectic difference
systems. These are the ﬁrst-order systems
yk+1 = Skyk,
(SDS)
where yk ∈R2n and Sk ∈R2n×2n are symplectic matrices, i.e.,
ST
k J Sk = J ,
J =
 0 I
−I 0

.
Symplectic difference systems are natural discrete counterparts of the linear Hamil-
tonian differential systems
y′ = J H(t) y,
(LHS)
where the coefﬁcient H(t) ∈R2n×2n is a piecewise continuous and symmetric
matrix, i.e., HT (t) = H(t). A common feature of (SDS) and (LHS) is that
their fundamental matrix is symplectic whenever it has this property at an initial
condition. Therefore, (SDS) and (LHS) are the most general linear difference and
differential systems, which have symplectic fundamental matrices.
Referring to the linear Hamiltonian differential system (LHS), the classical
qualitative theory of (LHS) is deeply developed; basic as well as advanced results of
this theory can be found, for example, in the monographs [70, 205, 248, 250, 328]
by Coppel, Kratz, Reid, and Yakubovich and in the recent monograph [203]
by Johnson, Obaya, Novo, Núñez, and Fabbri. Regarding symplectic difference
systems (SDS), the basic theory of (SDS) is presented in the monograph [16] from
1996 by Ahlbrandt and Peterson. As far as we know, there is no book or a survey
paper presenting in a uniﬁed way the results of the qualitative theory of (SDS),
which were obtained in this rapidly developing area in the last more than 20 years.
This book represents an attempt to ﬁll in this gap by providing numerous results,
methods, and citations, which are till now scattered only in journal papers. Our aim
v

vi
Preface
is to cover both the traditional and the most current topics in the oscillation and
spectral theory of symplectic systems, which are in the heart of the present research
work.
The book is divided into six chapters. The ﬁrst one is motivating and contains the
results concerning the oscillation properties of the second-order Sturm-Liouville
difference equations (being the simplest symplectic difference systems) and linear
Hamiltonian differential systems (LHS). It also contains the elements of the discrete
calculus of variations and optimal control theory, in which symplectic systems arise
in very natural way as Jacobi systems in the second-order optimality conditions.
Another motivating factor for studying symplectic difference systems originates in
the classical Hamiltonian mechanics. In addition, for an easier reference, we present
in this chapter an overview of the main tools from matrix analysis needed in the
book, in particular about symplectic matrices, the Moore-Penrose pseudoinverse
matrices, and certain orthogonal projectors.
Chapter 2 starts with an introduction to the theory of symplectic difference
systems. We present the main notions and basic methods for the investigation of
symplectic difference systems. We introduce the concept of a conjoined basis, the
notion of the nonexistence of focal points, the Riccati difference equation and
inequality, and the Picone formula and relate these concepts to the positivity and
nonnegativity of a discrete quadratic functional. These results are easily connected
to the necessary and sufﬁcient optimality conditions presented in Chap. 1. We
also investigate the existence of extremal solutions of (SDS) at inﬁnity, called the
recessive and dominant solutions of (SDS) at inﬁnity, under a certain controllability
assumption and present basic transformation theory of symplectic systems. We
explain in details the role of these concepts in the oscillation theory of system (SDS).
In Chap. 3, we present one of the main tools of the modern oscillation theory of
(SDS), namely, a concept of the comparative index of two matrices (which can be
regarded as conjoined bases of (SDS)). It turned out to be very useful tool for solving
several open problems in the oscillation theory of (SDS). This chapter summarizes
basic properties of the comparative index, which will be needed in the subsequent
chapters.
In Chap. 4, we introduce the second main concept used in this book, a concept
of the multiplicity of a focal point of a conjoined basis of (SDS). During the past
years, it was a central problem of the oscillation theory of (SDS), how to count
the numbers of focal point of a conjoined basis between two consecutive integers.
This problem was resolved in [208] and opened a new area for the investigation
of (SDS). The next parts of this chapter relate the multiplicities of focal points
with the comparative index. This can be regarded as an essential contribution to
the oscillation theory of (SDS). The main results in this chapter also include, among
others, the Sturmian separation and comparison theorems and the transformation
theory for symplectic systems, which are presented in the form of explicit relations
between the multiplicities of focal points. These results now serve as an inspiration
for the progress in the investigation of the oscillation theory for continuous time
linear Hamiltonian systems (LHS).

Preface
vii
Chapter 5 concentrates on the second main topic of the book, the eigenvalue and
spectral theory of (SDS). We investigate the distribution of the (ﬁnite) eigenvalues
of various boundary value problems associated with system (SDS). The main
essence of this chapter is that we consider the nonlinear dependence on the spectral
parameter in the coefﬁcient matrix, as well as in the boundary conditions. We
prove the oscillation theorems for discrete symplectic eigenvalue problems with
various boundary conditions (Dirichlet, separated, jointly varying), which relate
the number of ﬁnite eigenvalues with the number of focal points of a speciﬁc
conjoined basis of the system. As a special case, we then obtain traditional results, in
which the dependence on the spectral parameter is linear, including the variational
characterization of the ﬁnite eigenvalues and the Rayleigh principle. We also present
extensions of the basic theory to the case, when the coefﬁcients in (SDS) and/or in
the boundary conditions may oscillate with respect to the spectral parameter.
In Chap. 6, we collect various additional topics from the oscillation and spectral
theory of (SDS). We present the relative oscillation theory for symplectic difference
systems, as well as the inequalities and interlacing properties for (ﬁnite) eigenvalues
of symplectic boundary value problems. We also investigate a nonoscillatory system
(SDS) on an unbounded interval without any eventual controllability assumption.
This leads to an extensive theory of conjoined bases of (SDS) and, in particular,
to a new theory of recessive and dominant solutions of (SDS) at inﬁnity. This
yields—together with a suitable application of the comparative index theory—to
a new singular Sturmian theory for symplectic systems.
The last section in each chapter contains notes about the presented results and
additional references regarding the studied topics. The bibliography includes also
several references for further reading about related topics, in particular from the
oscillation and spectral theory of linear Hamiltonian differential system (LHS).
The intended readership of the book includes the researchers and graduate
students in pure and applied mathematics, who are interested in topics related
to discrete symplectic and Hamiltonian systems, discrete oscillation theory, and
spectral theory of difference equations and systems, covering also Sturm-Liouville
difference equations of higher order. Researchers and students in matrix analysis
will also beneﬁt from this book by understanding new applications of the theory of
matrices in this ﬁeld, in particular of the Moore-Penrose pseudoinverse matrices (or
the generalized inverses), orthogonal projectors, and (symplectic) matrix factoriza-
tions. The educational aim of the book is covered by including a variety of methods
from the discrete oscillation and eigenvalue theory, which are particularly useful in
the theory of symplectic and Hamiltonian systems. We demonstrate the utility of
the presented methods by including different proofs of the same result at several
occasions. We also support the understanding of the main results and important
notions by illustrating examples. The methods can be followed from a relatively
simple introduction to advanced constructions and applications, which are on the
edge of the current research in this ﬁeld. Therefore, this book may be used as
a reference literature for a graduate course in the methods of the discrete oscillation
and spectral theory.

viii
Preface
This monograph is written by three experts in the oscillation and spectral theory
of symplectic difference systems, who follow the development of this theory since
its ﬁrst steps more than 20 years ago. The original idea about this book came
from O. Došlý and J. Elyseeva in December 2013, who initiated the translation of
Chapters 1–4 of her monograph [121], which in revised and extended form became
constituent parts of this book. In July 2014, they invited R. Šimon Hilscher to join
the author team with the aim to prepare a monograph covering the state of the art
of the oscillation and spectral theory of symplectic difference systems. It is sad to
know that Professor Došlý suddenly passed away in November 2016. The remaining
two authors wish to acknowledge the fundamental contribution of Professor Došlý
of this monograph by completing it under his coauthorship (Fig. 1).
The authors wish to thank the Czech Science Foundation for the support of this
work provided through the grant 16–00611S. This support allowed J. Elyseeva to
enjoy friendly environment and hospitality of the Department of Mathematics and
Statistics (Faculty of Science of Masaryk University) during the years 2017–2018
to write substantial parts of Chaps. 5 and 6 and to prepare the ﬁnal version of
this monograph. The second author thanks the management of the Moscow State
University of Technology Stankin, in particular the chief of the Department of
Applied Mathematics Professor Ludmila Uvarova, for their invaluable assistance
in solving organizational issues related to the work on the monograph. We also
thank our colleagues from the team of Mathematical Analysis at the Department
Fig. 1 Professor Došlý lecturing on Sturm-Liouville difference equations and symplectic systems
on a seminar on differential equations (Pálava, 2002)

Preface
ix
of Mathematics and Statistics for creating a constant positive environment for
our work. We thank especially Peter Šepitka for his useful comments about parts
of Chaps. 1 and 6. We cordially thank Clemens Heine and Luca Sidler from
Birkhäuser Verlag for their editorial help and for promoting the book to Pathways
in Mathematics. We also thank Rajeswari Rajkumar from Springer for professional
typesetting of the ﬁnal version of the book. Finally, our very special thanks belong
to Zuzana Došlá for her continuous support and encouragement to complete this
monograph.
Brno, Czech Republic
Ondˇrej Došlý (deceased)
Moscow, Russia
Julia Elyseeva
Brno, Czech Republic
Roman Šimon Hilscher
December 2018

Contents
1
Motivation and Preliminaries...............................................
1
1.1
Short History of Symplectic Difference Systems......................
1
1.2
Sturm-Liouville Difference Equation ..................................
3
1.2.1
Generalized Zeros .............................................
4
1.2.2
Reid Roundabout Theorem ...................................
7
1.2.3
Recessive and Dominant Solutions ...........................
11
1.2.4
Discrete Prüfer Transformation ...............................
14
1.2.5
Sturm-Liouville Eigenvalue Problems .......................
16
1.3
Discrete Variational Theory ............................................
25
1.3.1
Discrete Calculus of Variations ...............................
25
1.3.2
Discrete Optimal Control Theory.............................
29
1.4
Symplectic Structure of Phase Flow in Hamiltonian Mechanics .....
35
1.5
Linear Hamiltonian Differential Systems ..............................
40
1.5.1
Basic Properties of Solutions of Hamiltonian Systems......
40
1.5.2
Symplectic Transformations ..................................
44
1.5.3
Riccati Equation and Reid Roundabout Theorem ...........
45
1.5.4
Trigonometric and Prüfer Transformations ..................
49
1.5.5
Principal and Nonprincipal Solutions ........................
51
1.5.6
Nonlinear Dependence on Spectral Parameter ...............
53
1.6
Linear Algebra and Matrix Analysis ...................................
54
1.6.1
Symplectic Matrices...........................................
55
1.6.2
Moore-Penrose Pseudoinverse ................................
58
1.6.3
Symplectic Matrices and Generalized LU Factorization....
62
1.6.4
Symplectic Matrices Depending on Parameter ..............
67
1.6.5
Monotone Matrix-Valued Functions..........................
73
1.6.6
Miscellaneous Topics from Matrix Analysis.................
75
1.7
Notes and References ...................................................
78
xi

xii
Contents
2
Basic Theory of Symplectic Systems .......................................
83
2.1
Symplectic Systems and Their Particular Cases .......................
83
2.1.1
Conjoined Bases and Wronskian .............................
84
2.1.2
Special Symplectic Difference Systems......................
86
2.2
Focal Points .............................................................
92
2.2.1
Focal Points of Conjoined Bases .............................
92
2.2.2
Backward Focal Points ........................................
95
2.3
Riccati Equation and Quadratic Functional............................
97
2.3.1
Riccati Matrix Difference Equation ..........................
97
2.3.2
Energy Quadratic Functional .................................
99
2.3.3
Picone Identity................................................. 102
2.3.4
Reid Roundabout Theorem ................................... 103
2.3.5
Nonnegative Quadratic Functional ........................... 109
2.3.6
Quadratic Functionals with General Endpoints .............. 116
2.4
Riccati-Type Inequalities ............................................... 120
2.4.1
Inequalities for Riccati-Type Quotients ...................... 120
2.4.2
Discrete Riccati Inequality .................................... 124
2.5
Recessive and Dominant Solutions..................................... 128
2.5.1
Deﬁnitions and Basic Properties.............................. 128
2.5.2
Minimal Solution of Riccati Equation........................ 134
2.6
Transformations of Symplectic Systems ............................... 135
2.6.1
General Symplectic Transformation.......................... 135
2.6.2
Trigonometric or Bohl Transformation....................... 139
2.6.3
Hyperbolic Transformation ................................... 141
2.6.4
Prüfer Transformation......................................... 144
2.7
Notes and References ................................................... 146
3
Comparative Index Theory ................................................. 149
3.1
Comparative Index and Its Properties .................................. 149
3.1.1
Deﬁnition of Comparative Index ............................. 150
3.1.2
Dual Comparative Index ...................................... 152
3.1.3
Basic Properties of Comparative Index....................... 153
3.1.4
Duality Principle for Comparative Index..................... 159
3.1.5
Proof of Properties (i)–(vii), (ix) of Comparative Index..... 161
3.2
Comparative Index and Symmetric Operators ......................... 166
3.2.1
Index of Block Symmetric Matrices.......................... 166
3.2.2
Symmetric Operator [V ] and Proof of Property (viii) ..... 169
3.3
Comparative Index for Symplectic Matrices........................... 173
3.3.1
Basic Properties ............................................... 173
3.3.2
General Case................................................... 179
3.3.3
Invertible Block B ............................................. 183
3.3.4
Invertible Block A............................................. 184
3.3.5
Additional Properties of Comparative Index ................. 190
3.3.6
Comparative Index for Symplectic Matrices: Proofs ........ 196
3.4
Notes and References ................................................... 199

Contents
xiii
4
Oscillation Theory of Symplectic Systems ................................ 201
4.1
Multiplicity of Focal Points ............................................ 201
4.1.1
Deﬁnition and Main Properties ............................... 202
4.1.2
Multiplicity of a Focal Point and Comparative Index ....... 205
4.2
Sturmian Separation Theorems and Its Corollaries.................... 211
4.2.1
Separation Theorem: First Step ............................... 211
4.2.2
Separation Theorems and Comparative Index ............... 216
4.2.3
Number of Focal Points and Principal Solutions ............ 222
4.2.4
Separation Results on Singular Intervals ..................... 227
4.3
Comparison Theorems and Its Corollaries............................. 229
4.3.1
Sturmian Comparison Theorems ............................. 230
4.3.2
Comparison Theorems for Principal Solutions .............. 241
4.3.3
Singular Comparison Theorems .............................. 245
4.4
Focal Points and Symplectic Transformations......................... 248
4.4.1
Focal Points and General Symplectic Transformation....... 248
4.4.2
Focal Points and Special Symplectic Transformations ...... 251
4.4.3
Generalized Reciprocity Principle............................ 253
4.4.4
Applications and Examples ................................... 255
4.5
Notes and References ................................................... 257
5
Discrete Symplectic Eigenvalue Problems................................. 261
5.1
Nonlinear Dependence on Spectral Parameter......................... 261
5.1.1
Finite Eigenvalues ............................................. 262
5.1.2
Finite Eigenfunctions ......................................... 266
5.1.3
Oscillation Theorems for Constant Rank of Bk(λ) .......... 270
5.1.4
Construction of Auxiliary Conjoined Basis .................. 273
5.1.5
Construction of Auxiliary Symplectic System ............... 282
5.1.6
Application of the Index Theorem............................ 294
5.1.7
Applications and Examples ................................... 298
5.2
Eigenvalue Problems with General Boundary Conditions ............ 303
5.2.1
Transformations of Boundary Conditions.................... 304
5.2.2
Transformation of Quadratic Functionals .................... 311
5.2.3
Oscillation Theorems for General Endpoints ................ 316
5.3
Linear Dependence on Spectral Parameter ............................ 322
5.3.1
Transformation of Boundary Conditions ..................... 323
5.3.2
Quadratic Functionals ......................................... 328
5.3.3
Finite Eigenvalues and Finite Eigenfunctions................ 331
5.3.4
Global Oscillation Theorem .................................. 337
5.4
Variational Description of Finite Eigenvalues ......................... 339
5.4.1
Extended Picone Identity ..................................... 340
5.4.2
Rayleigh Principle............................................. 342
5.5
Applications of Oscillation Theorems ................................. 346
5.5.1
Sturmian Comparison and Separation Theorems ............ 346
5.5.2
Modiﬁcations of Global Oscillation Theorem ............... 351

xiv
Contents
5.6
Oscillation Theorems for Variable Rank of Bk(λ)..................... 352
5.6.1
Statement of Main Results .................................... 354
5.6.2
Monotonicity and the Comparative Index.................... 362
5.6.3
Monotonicity and the Cayley Transform ..................... 368
5.6.4
Proofs of the Main Results.................................... 373
5.6.5
Weighted Focal Points ........................................ 376
5.6.6
General Endpoints and Nonconstant Rank ................... 382
5.7
Notes and References ................................................... 394
6
Miscellaneous Topics on Symplectic Systems ............................. 397
6.1
Relative Oscillation Theory ............................................ 397
6.1.1
Sturm-Liouville Difference Equations ....................... 399
6.1.2
Dirichlet Boundary Value Problems .......................... 400
6.1.3
Lower Block-Triangular Perturbation ........................ 404
6.1.4
Matrix Sturm-Liouville Eigenvalue Problems ............... 411
6.1.5
Examples....................................................... 418
6.1.6
Separated Boundary Conditions .............................. 423
6.1.7
General Boundary Conditions ................................ 428
6.2
Inequalities for Finite Eigenvalues ..................................... 433
6.2.1
Comparison of Finite Eigenvalues............................ 434
6.2.2
Interlacing of Eigenvalues for Joint Endpoints .............. 438
6.2.3
Interlacing of Eigenvalues for Separated Endpoints ......... 449
6.3
Symplectic Systems Without Controllability .......................... 460
6.3.1
Order of Abnormality ......................................... 461
6.3.2
Nonoscillatory Symplectic System ........................... 462
6.3.3
Conjoined Bases with Given Rank ........................... 482
6.3.4
Minimal Conjoined Bases .................................... 495
6.3.5
Asymptotics of S-Matrices ................................... 501
6.3.6
Recessive Solutions at Inﬁnity ................................ 509
6.3.7
Dominant Solutions at Inﬁnity................................ 517
6.3.8
Genus Conjoined Bases ....................................... 523
6.3.9
Limit Properties of Recessive and Dominant Solutions ..... 532
6.3.10
Reid’s Construction of Minimal Recessive Solution ........ 538
6.3.11
Additional Properties of Minimal Recessive Solution....... 543
6.3.12
Further Examples.............................................. 545
6.4
Singular Sturmian Separation Theorems............................... 548
6.4.1
Multiplicity of Focal Point at Inﬁnity ........................ 550
6.4.2
Singular Separation Theorems I .............................. 554
6.4.3
Singular Separation Theorems II ............................. 561
6.5
Notes and References ................................................... 569
References......................................................................... 573
Index ............................................................................... 589

List of Figures
Fig. 1
Professor Došlý lecturing on Sturm-Liouville difference
equations and symplectic systems on a seminar on differential
equations (Pálava, 2002) ...............................................
viii
Fig. 5.1
The graphs of Example 5.116 ......................................... 381
Fig. 6.1
The graphs of the sign of the Wronskian and the relative
oscillation numbers in Example 6.19 for the values a = −0.8
and b = 1.8 ............................................................. 420
Fig. 6.2
The graphs of the functions y1,2(λ) of the number of ﬁnite
eigenvalues below or equal to λ for λ ∈R for problems 1
and 2 in Example 6.20 ................................................. 420
Fig. 6.3
The graphs of the signs of the Wronskian,
Ck(a, b) = qk(a) −ˆqk(b), Bk(a, b) = ˆr−1
k (b) −r−1
k (a),
and the relative oscillation numbers in Example 6.20 for the
values a = b = −4..................................................... 422
Fig. 6.4
The graphs of the signs of the Wronskian,
Ck(a, b) = qk(a) −ˆqk(b), Bk(a, b) = ˆr−1
k (b) −r−1
k (a),
and the relative oscillation numbers in Example 6.20 for the
values a = −10 and b = 10 ........................................... 422
xv

Chapter 1
Motivation and Preliminaries
In this chapter we describe main motivating factors for the investigation of
symplectic difference systems in this book. This motivation comes from several
sources, mainly from (i) a generalization of the theory of second-order Sturm-
Liouville difference equations, (ii) discrete variational analysis, (iii) (classical and
discrete) Hamiltonian mechanics, (iv) discrete analogy of the theory of linear Hamil-
tonian differential systems, and (v) numerical methods for Hamiltonian differential
systems preserving the symplectic structure. Some of these topics are covered in
the next sections in order to motivate the subsequent detailed study of symplectic
difference systems. Some results (in particular those about Hamiltonian mechanics
and linear Hamiltonian differential systems) are presented without proofs, as they
serve mainly for comparison with the corresponding discrete-time theory. At the end
of this chapter, we also provide an overview of matrix analysis needed for this work,
in particular about symplectic matrices, the Moore-Penrose pseudoinverse matrices,
and symplectic matrix valued functions.
1.1
Short History of Symplectic Difference Systems
Symplectic difference systems (SDS) originate, to our knowledge, in two main
branches of mathematics. The ﬁrst one is the numerical analysis of Hamiltonian dif-
ferential systems in the works [140, 142, 143, 220] by K. Feng and his collaborators.
According to [140, pg. 18], symplectic difference systems “represent a proper way,
i.e., the Hamiltonian way, for computing the Hamiltonian dynamics.” The research
in this area has been recently summarized in the book [141]. The second source of
symplectic difference systems can be identiﬁed in the representation of continued
fractions in the works [8, 10] by C. D. Ahlbrandt; see also the references about
continued fractions in [16].
© Springer Nature Switzerland AG 2019
O. Došlý et al., Symplectic Difference Systems: Oscillation and Spectral Theory,
Pathways in Mathematics, https://doi.org/10.1007/978-3-030-19373-7_1
1

2
1
Motivation and Preliminaries
The concept of a symplectic difference system, as we consider in this work,
was introduced in 1996 in the book [16] by C. D. Ahlbrandt and A. Peterson.
During that time M. Bohner created in [39] the theory of general linear Hamiltonian
difference systems, which then essentially led to the ﬁnal establishment of the
symplectic systems theory as an independent subject in the theory of difference
equations. An extension of the concepts from [39], such as disconjugacy and
implicit Riccati equations and their relationship to the positivity of discrete quadratic
functionals, was given in [45] by M. Bohner and O. Došlý. That paper can be
regarded as a starting point of the efforts of many mathematicians to develop the
theory of symplectic difference systems in a parallel way to the theory of linear
Hamiltonian differential systems (LHS). From this time we mention the results
on the positivity and nonnegativity of discrete quadratic functionals with various
boundary conditions, including explicit and implicit Riccati matrix equations, by
R. Šimon Hilscher and V. Zeidan [100, 178, 180–182, 186, 188], discrete Riccati
inequality [173, 174], trigonometric and hyperbolic systems [24, 46–48, 85, 108],
and ﬁnally the theory of Weyl disks and their limit point and limit circle behavior
(the Weyl-Titchmarsh theory) by S. Clark, R. Šimon Hilscher, and P. Zemánek
[67, 308–313]. In this context we wish to emphasize the contributions of O. Došlý to
the transformation theory of symplectic difference systems and linear Hamiltonian
difference systems, including the trigonometric (Bohl), Prüfer, and hyperbolic
transformations for symplectic systems [46, 47, 83, 85, 93, 96, 98, 108], which
became a basic reference for further studies in the discrete oscillation theory.
As a breaking point in the development of the theory of symplectic systems
(SDS) in the last 20 years, we can consider the paper [208] by W. Kratz, where
the concept of the multiplicity of focal points for conjoined bases of (SDS) was
introduced. This paper led to the development of the Sturmian and eigenvalue theory
for symplectic systems in [56, 102] by M. Bohner, O. Došlý, and W. Kratz. Since
then the discrete-time theory became a strong motivation for the development of
the linear Hamiltonian differential systems. Another important contribution, which
changed rapidly the study of symplectic difference systems, is represented by the
papers [114, 115, 117] by J. Elyseeva, who introduced the concept of a comparative
index for conjoined bases of (SDS); see also [121]. Due to its close relations
with the multiplicities of focal points, this turned out to be a very powerful tool
which allowed to derive several deep and perhaps unexpected results not only for
discrete symplectic systems—for example, the discrete Sturmian theory, the relative
oscillation theory, the spectral theory, and the revisited transformation theory [94–
96, 116–120, 122, 123, 125, 126] by J. Elyseeva and partly by O. Došlý. Finally,
among recent important contributions to this theory, we consider the introduction
of the nonlinear dependence on the spectral parameter in symplectic difference
systems in [297] by R. Šimon Hilscher and the theory of recessive and dominant
solutions of (SDS) at inﬁnity for possibly uncontrollable symplectic systems in
[284, 290] by P. Šepitka and R. Šimon Hilscher. The ﬁrst mentioned topic initiated
the study of advanced topics in the spectral theory of symplectic systems, such
as the Weyl-Titchmarsh theory, the concept of weighted focal points, oscillation
theory for nonconstant rank, etc. The second mentioned topic leads, among others,

1.2
Sturm-Liouville Difference Equation
3
to a completely new singular Sturmian theory for symplectic difference systems
[292].
The current research in oscillation and spectral theory of symplectic difference
systems includes, for example, the topics from the spectral theory of linear
relations associated with symplectic systems, relative oscillation theory, recessive
and dominant solutions, Riccati equations and inequalities, or the uniﬁcation of the
theory of symplectic systems with linear Hamiltonian differential systems in the
theory of time scales.
In the traditional setting, such as in [16], the theory of symplectic difference
systems was developed in a parallel way to the known continuous time theory of
linear Hamiltonian systems. Starting from the milestone papers [39] by M. Bohner
and [208] by W. Kratz, the inspiration moved in favor of the discrete-time
theory, i.e., since then the symplectic difference systems motivate the progress in
the continuous time theory. A typical example of this process can be found in
[127, 129, 130, 207, 289, 295]. In current research the inspiration of continuous
and discrete theories is mutual, and, roughly speaking, the results for symplectic
difference systems and linear Hamiltonian differential systems are “cooked” in the
same pot. This process then gives weight and credit to both of these theories.
1.2
Sturm-Liouville Difference Equation
The simplest and most frequently studied special case of symplectic difference
systems is the second-order Sturm-Liouville difference equation
(rkxk) + pkxk+1 = 0,
rk ̸= 0.
(1.1)
In this section we present essentials of the oscillation theory of (1.1), for a more
extended treatment of this topics, we refer to [4, 110, 204]. The comparison of
the results for the case of equation (1.1) with those for symplectic difference
systems (SDS) should help the reader better understand the general qualitative
theory presented in the subsequent chapters.
According to some historical investigations, Sturm in his famous paper [279]
from 1836 originally wanted to formulate the results for a difference equation but
eventually decided to formulate them for a continuous counterpart of (1.1), i.e., for
the differential equation
r(t) x′′ + p(t) x = 0,
r(t) > 0.
(1.2)
Consider, for a moment, that r(t) ≡1 in (1.2). We want to “discretize” this
equation. Suppose that (1.2) is considered on an interval [a, b] and we want to ﬁnd
an approximate solution of this equation using the Euler discretization scheme. We
take the equidistant partition of [a, b] with the discretization stepsize h = (b−a)/n,
and we denote tk = a + kh, xk = x(tk), pk−1 = p(tk)h2 for k = 0, . . . , n. We

4
1
Motivation and Preliminaries
approximate the second derivative x′′(tk) by the second difference, where the values
xk−1, xk, xk+1 occur, i.e., x′′(tk) ≈2xk−1/h2 = (xk+1 −2xk + xk−1)/h2. Hence,
we obtain the difference equation 2xk−1 + pk−1xk = 0, which after relabeling k
to k + 1 gives the equation
2xk + pkxk+1 = 0.
These considerations justify why the shift k + 1 appears in the second term of
(1.1). Another reason is purely mathematical—without this shift we have no discrete
analog of the Wronskian identity, and also some other important formulas would be
missing when there is no shift at x in (1.1).
1.2.1
Generalized Zeros
Consider the second-order recurrence relation
xk+2 −xk+1 −xk = 0,
(1.3)
which deﬁnes the Fibonacci sequence. Upon looking for its solutions in the form
xk = λk, we ﬁnd that (1.3) has a pair of linearly independent solutions
x[1]
k
=

1 +
√
5
2
k
,
x[2]
k
=

1 −
√
5
2
k
.
(1.4)
Clearly, x[1] is a monotonically increasing sequence, while x[2] is an oscillating
sequence. From this point of view, it seems that the standard Sturmian theory, which
eliminates the coexistence of oscillatory and nonoscillatory solutions of a second-
order linear differential equation, is not valid in the discrete case.
Fortunately, this is not the case, and there exists a deeply developed Sturmian
theory for (1.1) when the concept of a generalized zero (sometimes also called
a focal point by analogy with the symplectic systems theory) is properly deﬁned.
To motivate this deﬁnition, consider the continuous counterpart of (1.1), i.e.,
the second-order Sturm-Liouville differential equation (1.2). Together with this
equation, consider its energy functional
F(y) =
 b
a
[r(t) y′2(t) −p(t) y2(t)] dt
(1.5)
and the associated Riccati equation (which related to (1.2) by the substitution w =
r(t) x′/x)
w′ + p(t) + w2
r(t) = 0.
(1.6)

1.2
Sturm-Liouville Difference Equation
5
If w is a solution of (1.6) deﬁned on the whole interval [a, b] and y ∈C1[a, b], then
computing (wy2)′ (we add and subtract the term r(t) y′2 in the resulting formula)
and then integrating the obtained formula from a to b, we obtain the so-called Picone
identity
 b
a
(ry′2 −py2)(t) dt = w(t) y2(t)
			
b
a +
 b
a
1
r(t)

r(t) y′(t) −w(t) y(t)2 dt.
(1.7)
This means that the energy functional is positive for a nontrivial y with y(a) = 0 =
y(b). In other words, the functional can be “completed to a square” whenever there
exists a solution of the Riccati equation (1.6) deﬁned on the whole interval [a, b].
Now we will follow the previous considerations in the discrete case. The Riccati
difference equation, which is related to (1.1) by the substitution wk = rkxk/xk,
has the form
wk + pk +
w2
k
rk + wk
= 0
(1.8)
or equivalently,
wk+1 + pk −
rkwk
wk + rk
= 0.
(1.9)
Indeed, we have
wk = 
rkxk
xk

= (rkxk) xk −rk(xk)2
xkxk+1
= −pk −r2
k (xk)2
rkxkxk+1
= −pk −r2
k (xk)2
x2
k
xk
rkxk+1
= −pk −
w2
k
rk (xk + xk)/xk
= −pk −
w2
k
rk + wk
.
Now, for y = {yk}N+1
k=0 with y0 = 0 = yN+1, we have
(wky2
k) = wk+1y2
k+1 −wky2
k
=

−pk +
wkrk
rk + wk

y2
k+1 −wky2
k + rk(yk)2 −rk(yk)2
= rk(yk)2 −pky2
k+1
−
1
rk + wk

−rkwky2
k+1 + (rk + wk) wky2
k + rk(rk + wk) (yk)2

6
1
Motivation and Preliminaries
= rk(yk)2 −pky2
k+1
−
1
rk + wk

r2
k (yk)2 + w2
ky2
k + rkwk(−y2
k+1 + y2
k + (yk)2)

= rk(yk)2 −pky2
k+1 −
1
rk + wk
(rkyk −wkyk)2.
The summation of the last formula and using the endpoints condition y0 = 0 =
yN+1 give the discrete Picone identity
N

k=0
[rk(yk)2 −pky2
k+1] =
N

k=0
1
wk + rk
(rkyk −wkyk)2.
(1.10)
Therefore, to get the positivity of the energy functional for a nontrivial y with y0 =
0 = yN+1, we need rk + wk > 0. Using the Riccati substitution, we then have
rk + wk = rk + rkxk
xk
= rk
xk + xk
xk
= rkxk+1
xk
.
Now we can formulate the deﬁnition of a generalized zero of a nontrivial solution
of (1.1).
Deﬁnition 1.1 Let x be a nontrivial solution of (1.1). We say that this solution has
a generalized zero (equivalently, a focal point) in the interval (k, k + 1] if xk ̸= 0
and rkxkxk+1 ≤0. More precisely, we say that the generalized zero is at k + 1 if
xk+1 = 0, while it is in the interval (k, k + 1) if rkxkxk+1 < 0.
This means that in contrast with the continuous case, where we normally assume
r(t) > 0, in the discrete case, we only need the condition rk ̸= 0. Returning back to
the Fibonacci equation (1.3), it can be written in the Sturm-Liouville form as


(−1)kxk

+ (−1)kxk+1 = 0,
i.e., rk = (−1)k which changes its sign between each k and k + 1. Applying
Deﬁnition 1.1, where also the sequence rk is incorporated into the notion of
a generalized zero, both solutions x[1] and x[2] of (1.3) are actually oscillating
solutions (they have inﬁnitely many generalized zeros).
We ﬁnish this subsection with the Wronskian identity and with the so-called
d’Alembert formula (an alternative terminology is the reduction of order for-
mula). Let x and y be solutions of (1.1). Then substituting from (1.1), we have
(rkxkyk −rkykxk) = 0, i.e., the quantity w(x, y)k := rk(xkyk −ykxk) is
constant in k. Moreover, if xk ̸= 0 in some discrete interval, then

yk
xk

= rkxkyk −rkykxk
xkxk+1
= w(x, y)
rkxkxk+1
.

1.2
Sturm-Liouville Difference Equation
7
The summation of this formula gives the so-called d’Alembert formula
yk = xk
k−1

j=0
w(x, y)
rjxjxj+1
.
(1.11)
This enables to express the second linearly independent solution of (1.1), when
a solution (being nonzero in some interval) is known.
1.2.2
Reid Roundabout Theorem
The next statement, usually referred to as the Reid roundabout theorem, describes
a relationship between the basic notions from the discrete oscillation theory of
equation (1.1). We present this result including its proof so that it can be compared
with that of the Reid roundabout theorem for symplectic difference systems, which
we present in Sect. 2.3. For discrete intervals we will use the notation
[a, b]Z := {a, a + 1, . . . , b −1, b},
a, b ∈Z, a < b.
(1.12)
Theorem 1.2 Assume rk ̸= 0 for all k ∈[0, N]Z. The following statements are
equivalent.
(i) Equation (1.1) is disconjugate in the discrete interval [0, N + 1]Z, i.e., the
solution x of (1.1) given by the initial conditions x0 = 0 and x1 = 1/r0 has
no generalized zero in the interval (0, N + 1].
(ii) There exists a solution x of (1.1) such that rkxkxk+1 > 0 for k ∈[0, N]Z.
(iii) There exists a solution w of (1.8) deﬁned for k ∈[0, N + 1]Z which satisﬁes
rk + wk > 0 for all k ∈[0, N]Z.
(iv) The discrete quadratic functional
F(y) =
N

k=0

rk(yk)2 −pky2
k+1
 > 0
(1.13)
for every nontrivial y = {yk}N+1
k=0 with y0 = 0 = yN+1.
Proof
(i)
	⇒
(ii): Consider the solution x[ε] of (1.1) given by the initial conditions
x[ε]
0
= ε > 0 and x[ε]
1
= 1/r0. Then, according to the continuous dependence
of solutions on the initial condition, we have x[ε]
k
→xk as ε →0+ for k ∈
[0, N + 1]Z, where x is the solution from (i). Then rkx[ε]
k x[ε]
k+1 > 0, k ∈[1, N]Z,
if ε is sufﬁciently small and also r0x[ε]
0 x[ε]
1
= ε > 0.
(ii) 	⇒(iii): This is just the Riccati substitution wk = rkx[ε]
k /x[ε]
k
for k ∈[0, N]Z.

8
1
Motivation and Preliminaries
(iii)
	⇒
(iv): This implication follows immediately from the Picone identity
(1.10).
(iv)
	⇒
(i): By contradiction, suppose that the solution x given by the initial
condition x0 = 0, x1 = 1/r0 has a generalized zero in (m, m + 1], i.e., xm ̸= 0
and rmxmxm+1 ≤0 for some m ∈[1, N]Z. Deﬁne the test sequence y by the
formula
yk =

xk,
k ∈[0, m]Z,
0
k ∈[m + 1, N + 1]Z.
Denote by L[x]k the left-hand side of (1.1), i.e., L[x]k = (rkxk) + pkxk+1.
Then ym = xm ̸= 0 (i.e., y is nontrivial) and y0 = x0 = 0 = yN+1 and by the
summation by parts of the ﬁrst term in F, we obtain
F(y) = rkykyk
		N+1
0
−
N

k=0
yk+1L[y]k
= −
 m−2

k=0
xk+1L[x]k

−xmL[y]m−1 −ym+1L[y]m
= xm[(rm + rm−1 −pm−1) xm −rm−1xm−1] = xmrmxm+1 ≤0,
which yields a contradiction. In the previous computation, we have used the
construction of y as a solution of (1.1) for k ∈[0, m −2]Z and the fact that
(1.1) can be written as the three-term symmetric recurrence relation
rk+1xk+2 + skxk+1 + rkxk = 0,
sk = −rk+1 −rk + pk,
(1.14)
where we applied (1.14) at k = m −1.
⊓⊔
In the oscillation theory of (1.1), the equivalence (i)
⇐⇒
(iii) is called the
Riccati technique, while the equivalence (i)
⇐⇒
(iv) is called the variational
principle. As a consequence of the roundabout theorem, we get that (1.1) can be
classiﬁed as oscillatory or nonoscillatory, similarly as in the case of its continuous
counterpart, the second-order Sturm-Liouville differential equation. This follows
from the next statement.
Theorem 1.3 Let x be a solution of (1.1) with generalized zeros in the intervals
(m, m + 1] and (n, n + 1], where m, n ∈[0, N]Z with m < n. Then any other
solution of this equation has a generalized zero in the interval (m, n + 1].

1.2
Sturm-Liouville Difference Equation
9
Proof The idea of the proof is the following. The existence of a nontrivial solution
without any generalized zero in (m, n + 1] implies that the functional
F(y) =
n

k=m

rk(yk)2 −pky2
k+1

> 0
for every nontrivial y = {yk}n+1
k=m with ym = yn+1 = 0, by Theorem 1.2. On the
other hand, a construction similar to the one in the proof of the implication (vi)
	⇒(i) in Theorem 1.2 yields a test sequence y, for which F(y) ≤0. This way we
obtain a contradiction.
⊓⊔
We say that a nontrivial solution x of (1.1) is oscillatory (at ∞) if there exists
a sequence of integers nk →∞as k →∞such that interval (nk, nk + 1]
contains a generalized zero of x for every k. In the opposite case, the solution x
of (1.1) is said to be nonoscillatory. Equation (1.1) is said to be oscillatory, if it
possesses an oscillatory solution, and it is said to be nonoscillatory, when there
exists a nonoscillatory solution. The previous theorem eliminates the coexistence of
oscillatory and nonoscillatory solutions of (1.1); hence it justiﬁes the classiﬁcation
of (1.1) as being oscillatory or nonoscillatory.
In the literature there exist numerous oscillation and nonoscillation criteria
for equation (1.1). As an example, we give here the so-called Leighton-Wintner
oscillation criterion.
Theorem 1.4 Suppose that ∞1/rk = ∞= ∞pk. Then equation (1.1) is
oscillatory.
Proof It can be veriﬁed that for any K ∈N, there exist integers K < L < M < N
such that for the sequence
yk =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
0
for k < K,
k−1
j=K 1/rj
 L−1
j=K 1/rj
−1
for k ∈[K, L]Z,
1
for k ∈[L + 1, M −1]Z,
N−1
j=k 1/rj
 N−1
j=M 1/rj
−1
for k ∈[M, N −1]Z,
0
for k ≥N,
we have F(y) = M
k=K[rk(yk)2 −pky2
k+1] ≤0 if M is sufﬁciently large.
To verify it, one needs to apply the second mean value theorem of summation
calculus (see [80, Lemma 3.2]) in computing the sum N−1
k=M pky2
k+1. Other than
that the computation is straightforward. Note that the second mean value theorem
of summation calculus is not needed under the additional assumption that pk ≥0
for large k.
⊓⊔

10
1
Motivation and Preliminaries
Let us recall that recurrence (1.14) (considered for k ∈[0, ∞)Z) and for x =
{xk}∞
k=0 with x0 = 0) can be rewritten using the so-called Jacobi matrix. This is
deﬁned to be an (inﬁnite) three-diagonal symmetric matrix
J =
⎛
⎜⎜⎜⎜⎜⎝
s0 r1 0 0 0 . . . 0 . . .
r1 s1 r2 0 0 . . . 0 . . .
0 r2 s2 r3 0 . . . 0 . . .
0 0 r3 s4 r4 . . . 0 . . .
...
⎞
⎟⎟⎟⎟⎟⎠
,
as (1.1) is equivalent with the equality Jx = 0. We note that the (mainly spectral)
theory of these matrices is deeply developed; see, e.g., [315].
The following transformation formula represents another important tool in the
qualitative theory of (1.1).
Theorem 1.5 Let hk ̸= 0 in some discrete interval. Then the transformation xk =
hkyk transforms equation (1.1) into the equation of the same form, i.e., to
(ˆrkyk) + ˆpkyk+1 = 0,
(1.15)
where ˆrk and ˆpk are given by the formulas
ˆrk = rkhkhk+1,
ˆpk = rkhk
 + pkhk+1.
(1.16)
Proof We have
hk+1

(rkxk) + pkxk+1

= hk+1

(rkhk+1yk + rkhkyk) + pkhk+1yk+1

= hk+1(rkhk+1yk) + hk+1(rkhkyk) + pkh2
k+1yk+1
= (rkhkhk+1yk) −hkrkhk+1yk + hk+1(rkhk)yk+1
+ hk+1rkhkyk + pkh2
k+1yk+1
= (rkhkhk+1yk) + hk+1[(rkhk) + pkhk+1] yk+1.
Hence, (1.1) is indeed transformed into (1.15) with ˆrk and ˆpk given by (1.16).
⊓⊔
If the sequence h is a solution of equation (1.1), then ˆpk = 0, and (1.15)
becomes the equation (rkhkhk+1yk) = 0. The summation of this equation gives
an alternative proof of the d’Alembert formula (1.11) (with h instead of x).
We ﬁnish this subsection by the so-called oscillation theorem, which concerns the
discrete Sturm-Liouville eigenvalue problem with the Dirichlet boundary conditions
(the statement remains to hold also for general self-adjoint boundary conditions, but
for simplicity we formulate the statement only for the Dirichlet conditions). The

1.2
Sturm-Liouville Difference Equation
11
proof of this statement can be found in [204, Theorem 7.6]. Note that this reference
deals with the case of rk > 0 only, but the presented proof essentially works also
when rk ̸= 0. Alternatively, this result follows from the more general statement for
nonlinear dependence on λ (Theorem 1.19) or for symplectic difference systems
(Corollary 5.79).
Theorem 1.6 Consider the eigenvalue problem
−(rkxk)+qkxk+1 = λwkyk+1,
k ∈[0, N−1]Z,
x0 = 0 = xN+1
(1.17)
with rk ̸= 0 and wk > 0 for all k ∈[0, N]Z. Then the number of eigenvalues of
(1.17), which are less than or equal to a given λ0 ∈R, is equal to the number of
generalized zeros in (0, N + 1] of the solution x(λ0) of the difference equation in
(1.17) with λ = λ0, which satisﬁes the initial conditions x0(λ0) = 0 and x1(λ0) =
1/r0.
1.2.3
Recessive and Dominant Solutions
In this subsection we will suppose that equation (1.1) is nonoscillatory. This means
that there exists M ∈N such that the solution x[M] given by the initial condition
xM = 0 and xM+1 = 1/rM has no generalized zero in the interval (M, ∞), i.e.,
rkx[M]
k
x[M]
k+1 > 0 for all k ≥M. We claim that under these conditions, there exists
a solution ˜x of (1.1) with the property that
lim
k→∞
˜xk
xk
= 0
(1.18)
for every solution x of (1.1), which is linearly independent of ˜x. The solution ˜x is
called a recessive solution (at ∞). Any solution linearly independent of the recessive
solution is called a dominant solution (at ∞).
In order to show the existence of a recessive solution of (1.1), we consider two
linearly independent solutions x and y of (1.1) such that x has no generalized zero
in (N, ∞) for some N ∈N. Then their Wronskian c := w(x, y)k is constant on
[N, ∞), and hence (yk/xk) = c/rkxkxk+1 is of constant sign (equal to the sign
of c); see Sect. 1.2.1. Therefore, the sequence yk/xk is monotone, and hence there
exists (ﬁnite or inﬁnite) limk→∞yk/xk = L. If L = 0, then we put ˜x = y. Then
every solution linearly independent of ˜x is of the form α ˜x + βx with β ̸= 0 and
lim
k→∞
˜xk
α ˜xk + βxk
= lim
k→∞
˜xk/xk
α (˜xk/xk) + β = 0.

12
1
Motivation and Preliminaries
If 0 ̸= L ∈R, then the principal solution is ˜x = x −Ly, and similarly as above,
we have limk→∞˜xk/xk = 0 for every solution linearly independent of ˜x. Finally, if
L = ±∞, we put ˜x = x. Therefore we have just proved the following statement.
Theorem 1.7 A recessive solution of (1.1) exists whenever (1.1) is nonoscillatory.
Moreover, the recessive solution is unique up to a constant nonzero multiplicative
factor. Finally, a solution x of (1.1) is recessive if and only if
∞

k=N
1
rkxkxk+1
= ∞,
(1.19)
where N is sufﬁciently large.
Another characterization of the recessive solution is closely connected with
the concept of eventually minimal solution (an equivalent terminology is the
distinguished solution) of the Riccati equation (1.8).
Theorem 1.8 Let ˜x be a solution of (1.1) with ˜xk ̸= 0 for k sufﬁciently large. Let
˜wk = (rk˜xk)/˜xk be the corresponding solution of the associated Riccati equation
(1.8). Then ˜x is the recessive solution of (1.1) if and only if for any other solution w
of (1.8) we have wk > ˜wk for all k sufﬁciently large.
Proof Let ˜x be the recessive solution of (1.1) and deﬁne ˜w as in the theorem. Let
wk be any other solution of the Riccati equation (1.8). By the Reid roundabout
theorem (Theorem 1.2), we may assume that rk + wk > 0 for large k and that
wk = (rkxk)/xk for large k for some solution x of (1.1) with xk ̸= 0 for large k.
Then we have
wk −˜wk = rkxk
xk
−rk˜xk
˜xk
= rk(˜xkxk −xk˜xk)
xk ˜xk
=
d
xk ˜xk
,
(1.20)
where d = w(˜x, x)k is the constant Wronskian of ˜x and x. Since rkxkxk+1 > 0 and
rk ˜xk ˜xk+1 > 0 for large k, we have
xkxk+1
˜xk ˜xk+1
> 0
i.e.,
xk
˜xk
· xk+1
˜xk+1
> 0.
This means that xk/˜xk, and hence also xk ˜xk, does not change its sign for large k.
Replacing x by −x if necessary, we may suppose without loss of generality that
xk ˜xk > 0, i.e., ˜xk/xk > 0. Since ˜xk/xk →0 as k →∞and (˜xk/xk) =
c/(rkxkxk+1) with positive denominator of the right-hand side, it follows that
c = w(x, ˜x)k < 0 for large k. Hence, d = −c > 0 and from (1.20) we obtain
that wk −˜wk > 0 for large k.
Conversely, suppose that ˜w is the eventually minimal solution of equation (1.8),
i.e., we have for large k
wk = rkxk
xk
> ˜wk = rk˜xk
˜xk
,
i.e.,
d
xk ˜xk
= wk −˜wk > 0,

1.2
Sturm-Liouville Difference Equation
13
where d = rk(˜xkxk −xk˜xk). Using the same argument as in the ﬁrst part of the
proof, we have xk ˜xk > 0 for large k, hence d > 0. We have
(˜xk/xk) = −
d
rkxkxk+1
< 0.
for large k. This means that ˜xk/xk is a monotonically decreasing sequence of
positive numbers; hence it tends to some limit L ≥0 as k →∞. Suppose that
L > 0 and put ˆxk = ˜xk −Lxk. Then we have
rkˆxk
ˆxk
−rk˜xk
˜xk
= rk ˜xk(˜xk −Lxk) −(˜xk −Lxk) rk˜xk
ˆxkxk
= −L rk(˜xkxk −xk˜xk)
ˆxkxk
= −L
c
ˆxkxk
< 0
and we get a contradiction with the assumption that ˜wk < wk for large k for any
other solution w of (1.8). Hence L = 0, and ˜x is then the recessive solution.
⊓⊔
Finally, the last statement of this subsection states that the largest generalized
zero of the recessive solution (if any) has a certain extremal property.
Theorem 1.9 Let the interval (N, N +1] contain the largest generalized zero of the
recessive solution ˜x of (1.1). Then any linearly independent solution has at least one
generalized zero in the interval (N, ∞).
Proof We outline the proof only. Suppose that N is the largest integer such that the
interval (N, N + 1] contains a generalized zero of ˜x, i.e., ˜xN ̸= 0 and rN ˜xN ˜xN+1 ≤
0. We will deal with the case rN ˜xN ˜xN+1 < 0 (in case of the equality, the proof needs
minor a modiﬁcation, which we will not present here). Hence we have rN + ˜wN < 0,
where ˜wk = (rk˜xk)/˜xk. Suppose, by contradiction, that there exists a solution
x of (1.1) with rkxkxk+1 > 0 for all k ∈[N, ∞)Z. Then rk + wk > 0, where
wk = (rkxk)/xk, in particular, rN + wN > 0. This implies ˜wN > wN. We have
(we use (1.9))
˜wk+1 −wk+1 =
r2
k
(rk + wk)(rk + ˜wk)( ˜wk −wk).
Substituting k = N, we have ˜wN+1 > wN+1. Since rk + wk > 0, rk + ˜wk > 0,
k ∈[N +1, ∞)Z, we have ˜wk > wk for all k ∈[N +1, ∞)Z. This is a contradiction
with the fact that the recessive solution of (1.1) generates the eventually minimal
solution of (1.8), by Theorem 1.8.
⊓⊔

14
1
Motivation and Preliminaries
1.2.4
Discrete Prüfer Transformation
The Prüfer transformation is an important tool in the investigation of the oscillatory
properties of conjoined bases of system (1.103). In this subsection we introduce
the Prüfer transformation for the self-adjoint difference equation (1.1) and use it to
obtain the so-called reciprocity principle for this equation.
Let x be a nontrivial solution of (1.1). Then x2
k + (rkxk)2 ̸= 0 for all k
(otherwise x would be identically zero) and we can ﬁnd real numbers ϱk > 0 and
ϕk with 0 ≤ϕk < 2π such that the equations
xk = ϱk sin ϕk,
(1.21)
rkxk = ϱk cos ϕk
(1.22)
are satisﬁed for all k. That is, we set
ϱk :=

x2
k + (rkxk)2,
ϕk := arccot rkxk
xk
.
We call (1.21) and (1.22) the discrete Prüfer transformation.
Theorem 1.10 Let x be a nontrivial solution of (1.1), and let ϱ and ϕ be deﬁned
by (1.21) and (1.22). Then we have the equations
ϱk = ϱk
 1
rk
cos ϕk sin ϕk+1 −pk sin ϕk cos ϕk+1
−pk
rk
cos ϕk cos ϕk+1 −1
2

( sin ϕk)2 + ( cos ϕk)2
,
(1.23)
sin ϕk = 1
rk
cos ϕk cos ϕk+1 + pk sin ϕk sin ϕk+1 + pk
rk
cos ϕk sin ϕk+1.
(1.24)
Proof Use of the discrete product rule for (1.21) yields
sin ϕk+1 ϱk + ϱk  sin ϕk = (ϱk sin ϕk) = xk
= 1
rk
(rkxk) = 1
rk
ϱk cos ϕk,

1.2
Sturm-Liouville Difference Equation
15
while doing the same for (1.22) implies
cos ϕk+1 ϱk + ϱk  cos ϕk = (ϱk cos ϕk) = (rkxk)
= −pkxk+1 = −pk
rk
(rkxk) −pkxk
= −pk
rk
ϱk cos ϕk −pkϱk sin ϕk,
where we have also used that x is a solution of (1.1). Hence we obtain
sin ϕk+1 ϱk + ϱk  sin ϕk = ϱk
rk
cos ϕk,
(1.25)
cos ϕk+1 ϱk + ϱk  cos ϕk = −pk
rk
ϱk cos ϕk −pkϱk sin ϕk.
(1.26)
We now multiply (1.25) by sin ϕk+1 and (1.26) by cos ϕk+1 and add the resulting
equations to obtain (1.23). To verify (1.24), we multiply (1.25) by cos ϕk+1 and
(1.26) by −sin ϕk+1 and add the resulting equations. Dividing the obtained equation
by ϱk > 0 directly yields (1.24).
⊓⊔
As an application of Theorem 1.10, we derive the reciprocity principle for
equation (1.1), which relates the oscillatory properties of a solution x of (1.1) with
the oscillatory properties of the quasi-difference rx. For this we assume that
rk > 0
and
pk > 0
for all k.
(1.27)
First we need an auxiliary result.
Lemma 1.11 Assume (1.27), let x be a solution of (1.1), and deﬁne ϕ by (1.21)–
(1.22). If xkxk+1 > 0 for some integer k, then 0 < ϕk < π.
Proof First of all use equations (1.1), (1.21), and (1.22) to obtain
cos ϕk+1+pk sin ϕk+1 = rk+1xk+1 + pkxk+1
ϱk+1
= rkxk
ϱk+1
= ϱk cos ϕk
ϱk+1
.
(1.28)
This is implied by (1.21) and (1.24)
sin ϕk = pk sin ϕk sin ϕk+1 + cos ϕk
rk
(cos ϕk+1 + pk sin ϕk+1)
= pkxkxk+1
ϱkϱk+1
+ ϱk cos2 ϕk
rkϱk+1
> 0
and hence (observe that by deﬁnition 0 ≤ϕk < 2π), we have 0 < ϕk < π,
which is the conclusion.
⊓⊔

16
1
Motivation and Preliminaries
Theorem 1.12 (Reciprocity Principle) Assume (1.27) and let x be a solution of
(1.1). Then xk is eventually of one sign if and only if rkxk is eventually of one sign.
Proof Let x be a solution of (1.1) and deﬁne ϕ by the (1.21)–(1.22) as before. Let
N be an integer such that xkxk+1 > 0 for all k ≥N. Then the points (rkxk, xk) are
either in the upper half plane for all k ≥N or in the lower half plane for all k ≥N.
This and Lemma 1.11 imply that ϕk ≤ϕN + π for all k ≥N. By deﬁnition, ϕk ≤
ϕk+1 for all k ≥N, so that limk→∞ϕk must exist. But then also limk→∞cos ϕk
exists, and hence (observe also that cos ϕk ≥cos ϕk+1 for all k ≥N or cos ϕk ≤
cos ϕk+1 for all k ≥N), there exists an integer M such that cos ϕk cos ϕk+1 > 0 for
all k ≥M. This together with (1.22) proves the result. The converse follows from
the fact that the sequence ˜xk = rkxk satisﬁes the so-called reciprocal equation

 1
pk
˜xk

+
1
rk+1
˜xk+1 = 0,
as can be easily veriﬁed by direct calculations. One can use then the same
argumentation as in the ﬁrst part of the proof.
⊓⊔
1.2.5
Sturm-Liouville Eigenvalue Problems
In this section we consider the second-order Sturm-Liouville difference equation
(rk(λ) xk) + qk(λ) xk+1 = 0,
k ∈[0, N −1]Z,
(SLλ)
where rk : R →R for k ∈[0, N]Z and qk : R →R for k ∈[0, N −1]Z are given
differentiable functions of the spectral parameter λ such that
rk(λ) ̸= 0 and ˙rk(λ) ≤0,
k ∈[0, N]Z,
˙qk(λ) ≥0,
k ∈[0, N −1]Z.
(1.29)
Here N ∈N is a ﬁxed number with N ≥2 and [a, b]Z := [a, b] ∩Z, and the dot
denotes the differentiation with respect to λ. With equation (SLλ) we consider the
Dirichlet boundary conditions, that is, we study the eigenvalue problem
(SLλ),
λ ∈R,
x0 = 0 = xN+1.
(E0)
We recall ﬁrst the classical setting of Sturm-Liouville difference equations; see,
e.g., [16, 23, 28, 204], in which the function rk(·) is constant (nonzero) in λ and the
function qk(·) is linear and increasing in λ. That is, the traditional assumptions for
the oscillation and spectral theory of equation (SLλ) are the following:
rk(λ) ≡rk ̸= 0,
for all k ∈[0, N]Z,
qk(λ) = qk + λ wk,
wk > 0,
for all k ∈[0, N −1]Z.

(1.30)

1.2
Sturm-Liouville Difference Equation
17
In some publications, such as in [28, 204], the authors also impose the sign condition
rk > 0 for all k ∈[0, N]Z, but it is well-known nowadays that rk ̸= 0 is sufﬁcient
to develop the oscillation and spectral theory of these equations; see Sect. 1.2.1 and,
e.g., [315, pg. 5] or [304]. The explanation of this phenomenon also follows from
the analysis of the general equation (SLλ) discussed below.
Assume for a moment that (1.30) holds. Following [204, Chapter 7] or [28,
Chapter 4], a number λ0 ∈C is an eigenvalue of (E0) if there exists a nontrivial
solution x = x(λ0) of equation (SLλ0) satisfying the Dirichlet endpoints x0(λ0) =
0 = xN+1(λ0). By the uniqueness of solutions of equation (SLλ0), it follows that
the eigenvalues of (E0) are characterized by the condition ˆxN+1(λ0) = 0, where
ˆx(λ) is the principal solution of equation (SLλ), i.e., it is the solution starting with
the initial values ˆx0(λ) = 0 and ˆx1(λ) = 1/r0. If x(λ) is a solution of (SLλ) with
(1.30), then the functions xk(λ) are polynomials in λ for every k ∈[0, N + 1]Z.
Therefore, the zeros of xk(λ) are isolated, showing that the eigenvalues of (E0) are
simple (with the multiplicity equal to one) and isolated. Furthermore, by a standard
argument from linear algebra, it follows that the eigenvalues of (E0) with λ ∈C
are indeed real and that the eigenfunctions corresponding to different eigenvalues
are orthogonal with respect to the inner product ⟨x, y⟩w := N
k=0 wk xk+1 yk+1.
The oscillation theorem for (E0) then says that the j-th eigenfunction has exactly j
generalized zeros in the interval (0, N + 1]; see Deﬁnition 1.1.
In this subsection we show that some of the above properties can be extended to
the eigenvalue problem (E0) in which the coefﬁcients depend on the spectral param-
eter λ in general nonlinearly and they satisfy the monotonicity assumption (1.29). In
particular, we discuss the notions of ﬁnite eigenvalues and ﬁnite eigenfunctions for
such problems which are appropriate generalizations of the corresponding notions
for the case of (1.30).
First we show how certain solutions of (SLλ) behave with respect to λ. Assump-
tion (1.29) implies that the solutions of (SLλ) are differentiable, hence continuous,
in λ on R. We will consider the solutions whose initial values
x0(λ),
r0(λ) x0(λ)
do not depend on λ.
(1.31)
This condition is satisﬁed, for example, by the principal solution ˆx(λ), for which
ˆx0(λ) = 0,
ˆx1(λ) = 1/r0(λ)
for all λ ∈R.
(1.32)
The following result shows that under the monotonicity assumption (1.29), the
oscillation behavior in λ is not allowed for the above type of solutions near any
ﬁnite value of λ.
Lemma 1.13 Assume that (1.29) holds and let x(λ) = {xk(λ)}N+1
k=0 be a nontrivial
solution of (SLλ) satisfying (1.31). Then for each k ∈[0, N +1]Z and λ0 ∈R, there
exists δ > 0 such that xk(λ) is either identically zero or never zero on (λ0, λ0 + δ),
resp. on (λ0 −δ, λ0).

18
1
Motivation and Preliminaries
Proof Let λ0 ∈R and k ∈[0, N + 1]Z be ﬁxed. If k = 0, then the result follows
trivially. Also, if xk(λ0) ̸= 0, then the statement is a consequence of the continuity
of xk(λ) in λ. Therefore, further on we assume that k ∈[1, N +1]Z and xk(λ0) = 0.
First we construct another solution y(λ) = {yj(λ)}N+1
j=0 whose initial conditions
do not depend on λ as in (1.31) such that yk(λ0) ̸= 0 and such that the Casorati
determinant
C[y(λ), x(λ)]j := rj(λ)
				
yj(λ)
xj(λ)
yj(λ) xj(λ)
				 = 1
for all j ∈[0, N + 1]Z, λ ∈R.
This means that the solutions y(λ) and x(λ) form a normalized pair of solutions of
(SLλ). The solution y(λ) can be constructed from the initial conditions
y0(λ) = r0(λ) x0(λ)/ω0,
r0(λ) y0(λ) = −x0(λ)/ω0,
where ω0 := x2
0(λ) + r2
0(λ) [x0(λ)]2 is independent of λ. By the continuity of
yk(λ) in λ, there exists ε > 0 such that yk(λ) ̸= 0 on (λ0 −ε, λ0 + ε). For these
values of λ, a direct calculation shows the formula
d
dλ
xk(λ)
yk(λ)

=
1
y2
k(λ)
k−1

j=0

˙qj(λ)
				
xj+1(λ) xk(λ)
yj+1(λ) yk(λ)
				
2
−˙rk(λ)
				
xj(λ) xk(λ)
yj(λ) yk(λ)
				
2  
.
Therefore, under the assumption (1.29) the function zk(λ) := xk(λ)/yk(λ) is
nondecreasing in λ on (λ0 −ε, λ0 + ε). This means that once zk(λ0) = 0, then
zk(λ) is either identically zero on (λ0, λ0 + δ) for some δ ∈(0, ε), or zk(λ) is
positive on (λ0, λ0 + ε). Similar argument applies also on the left side of λ0. And
since the zeros of zk(λ) in (λ0 −ε, λ0 + ε) are exactly those of xk(λ), the result
follows.
⊓⊔
Remark 1.14 The statement of Lemma 1.13 says that for a nontrivial solution
x(λ) = {xk(λ)}N+1
k=0 of (SLλ) satisfying (1.31), the quantity
hk(λ) := rank xk(λ)
(1.33)
is piecewise constant in λ on R for every given k ∈[0, N + 1]Z.
Remark 1.15 If a solution x(λ) of (SLλ) satisﬁes xk(λ0) ̸= 0 at some index k ∈
[0, N + 1]Z and λ0 ∈R, then there exists δ > 0 such that xk(λ) ̸= 0 on the interval
(λ0 −δ, λ0 + δ). Moreover, as in the proof of Lemma 1.13, we can derive for all
λ ∈(λ0 −δ, λ0 + δ) the formula
˙pk(λ) = −˙rk(λ) x2
k(λ)
r2
k (λ) x2
k+1(λ) +
1
r2
k (λ)
k−1

j=0
!
˙qj(λ) x2
j+1(λ) −˙rj(λ) [xj(λ)]2"
,
(1.34)

1.2
Sturm-Liouville Difference Equation
19
where
pk(λ) :=
xk(λ)
rk(λ) xk+1(λ).
(1.35)
Identity (1.34) shows that the function pk(λ) is nondecreasing in λ whenever it is
deﬁned, i.e., whenever xk+1(λ) ̸= 0. This monotonicity of pk(λ) in λ is essential for
deriving the oscillation theorem below. Note also that according to Deﬁnition 1.1,
we have pk(λ) < 0 if and only if the solution x(λ) has a generalized zero in (k, k +
1).
Remark 1.16 The uniqueness of solutions of (SLλ) implies that a nontrivial solution
x(λ) of (SLλ) cannot vanish at any two consecutive points k and k + 1. Therefore,
if xk(λ) = 0, then xk+1(λ) ̸= 0, while if xk+1(λ) = 0, then xk(λ) ̸= 0.
Let x(λ) = {xk(λ)}N+1
k=0 be a nontrivial solution of (SLλ) and denote by mk(λ)
the number of its generalized zeros in (k, k + 1]. Then mk(λ) ∈{0, 1}. Our aim is
to prove the following local oscillation theorem.
Theorem 1.17 (Local Oscillation Theorem I) Assume that (1.29) holds. Consider
a nontrivial solution x(λ) = {xk(λ)}N+1
k=0 of (SLλ) satisfying (1.31). Fix an index k ∈
[0, N]Z and denote by mk(λ) the number of generalized zeros of x(λ) in (k, k + 1].
Then mk(λ−) and mk(λ+) exist and for all λ ∈R
mk(λ+) = mk(λ) ≤1,
(1.36)
mk(λ+) −mk(λ−) = hk(λ) −hk(λ−) + hk+1(λ−) −hk+1(λ),
(1.37)
where hk(λ) and hk+1(λ) are given in (1.33).
In the above formula, the value of the function hj(λ) is 1 if xj(λ) ̸= 0, and it is 0
if xj(λ) = 0, for j ∈{k, k + 1}. Moreover, the notation hj(λ−) means the left-hand
limit of the function hj(λ) at the given point λ. Similarly, the notation mk(λ−) and
mk(λ+) stands, respectively, for the left-hand and right-hand limits of the function
mk(λ) at the point λ.
Proof of Theorem 1.17 Let k ∈[0, N]Z and λ0 ∈R be given. By Remark 1.14, the
limits hk(λ−
0 ) and hk+1(λ−
0 ) exist. We will show that the left-hand and right-hand
limits of the function mk(λ) at λ0 also exist and equations (1.36) and (1.37) are
satisﬁed. We split the proof into two parts depending on the rank of xk+1(λ0).
Part I. Assume ﬁrst that xk+1(λ0) ̸= 0. Then there exists ε > 0 such that
xk+1(λ) ̸= 0 for all λ ∈(λ0 −ε, λ0 + ε). This means that for these values of λ, the
point k+1 is not a generalized zero of the solution x(λ). According to Remark 1.15,
the function pk(λ) in (1.35) is nondecreasing on (λ0 −ε, λ0 + ε), and we have on
this interval either mk(λ) = 1 if pk(λ) < 0 or mk(λ) = 0 if pk(λ) ≥0. We further
distinguish the following three subcases:
(I-a)
pk(λ0) < 0,
(I-b)
pk(λ0) > 0, and
(I-c)
pk(λ0) = 0.

20
1
Motivation and Preliminaries
In subcase (I-a), in which pk(λ0) < 0, we have pk(λ) < 0 and xk(λ) ̸= 0 for
all λ ∈(λ0 −δ, λ0 + δ) for some δ ∈(0, ε), so that in this case mk(λ0) =
mk(λ−
0 ) = mk(λ+
0 ) = 1, hk(λ0) = hk(λ−
0 ) = 1, and hk+1(λ0) = hk+1(λ−
0 ) = 1.
Therefore, the equations in (1.36) and (1.37) hold as the identities 1 = 1 and
0 = 0, respectively. Similarly in subcase (I-b), in which pk(λ0) > 0, there is
δ ∈(0, ε) such that pk(λ) > 0 and xk(λ) ̸= 0 for all λ ∈(λ0 −δ, λ0 + δ), so
that in this case mk(λ0) = mk(λ−
0 ) = mk(λ+
0 ) = 0, hk(λ0) = hk(λ−
0 ) = 1, and
hk+1(λ0) = hk+1(λ−
0 ) = 1. Therefore, both equations (1.36) and (1.37) now hold
as the identity 0 = 0. In subcase (I-c), in which pk(λ0) = 0, we have xk(λ0) = 0.
By Lemma 1.13, there is δ ∈(0, ε) such that one of the additional four subcases
applies for the behavior of xk(λ) near λ0:
(I-c-i)
xk(λ) ̸= 0 on (λ0 −δ, λ0) and on (λ0, λ0 + δ),
(I-c-ii)
xk(λ) ̸= 0 on (λ0 −δ, λ0) and xk(λ) ≡0 on (λ0, λ0 + δ),
(I-c-iii)
xk(λ) ≡0 on (λ0 −δ, λ0) and xk(λ) ̸= 0 on (λ0, λ0 + δ), and
(I-c-iv)
xk(λ) ≡0 both on (λ0 −δ, λ0) and on (λ0, λ0 + δ).
In subcase (I-c-i), the function pk(λ) must be nondecreasing on (λ0 −δ, λ0 + δ),
which implies that pk(λ) < 0 on (λ0 −δ, λ0) and pk(λ) > 0 on (λ0, λ0 + δ).
Therefore, in this case mk(λ−
0 ) = 1, mk(λ+
0 ) = mk(λ0) = 0, hk(λ−
0 ) = 1, hk(λ0) =
0, and hk+1(λ−
0 ) = hk+1(λ0) = 1. This means that the equations in (1.36) and (1.37)
now hold as the identities 0 = 0 and −1 = −1, respectively. In subcase (I-c-ii), the
function pk(λ) is nondecreasing on (λ0 −δ, λ0], which implies that pk(λ) < 0 on
(λ0 −δ, λ0) and pk(λ) ≡0 on (λ0, λ0 +δ). Thus, as in subcase (I-c-i), we now have
mk(λ−
0 ) = 1, mk(λ+
0 ) = mk(λ0) = 0, hk(λ−
0 ) = 1, hk(λ0) = 0, and hk+1(λ−
0 ) =
hk+1(λ0) = 1, so that the equations in (1.36) and (1.37) hold as the identities 0 = 0
and −1 = −1, respectively. In subcase (I-c-iii), the situation is similar with the
result that pk(λ) is nondecreasing on [λ0, λ0 + δ), so that pk(λ) ≡0 in (λ0 −δ, λ0]
and pk(λ) > 0 on (λ0, λ0+δ). Thus, in this case mk(λ−
0 ) = mk(λ+
0 ) = mk(λ0) = 0,
hk(λ−
0 ) = hk(λ0) = 0, and hk+1(λ−
0 ) = hk+1(λ0) = 1, so that both equations
(1.36) and (1.37) hold as the identity 0 = 0. In the last subcase (I-c-iv), we have
pk(λ) ≡0 on (λ0 −δ, λ0 + δ) and in this case mk(λ−
0 ) = mk(λ+
0 ) = mk(λ0) = 0,
hk(λ−
0 ) = hk(λ0) = 0, and hk+1(λ−
0 ) = hk+1(λ0) = 1, so that (1.36) and (1.37)
hold as the identity 0 = 0.
Part II. Assume that xk+1(λ0) = 0. Then by Remark 1.16, we have xk(λ0) ̸= 0,
and there exists ε > 0 such that xk(λ) ̸= 0 for all λ ∈(λ0 −ε, λ0 + ε). By
Lemma 1.13, there is δ ∈(0, ε) such that one of the following four subcases applies
for the behavior of xk+1(λ) near the point λ0:
(II-a)
xk+1(λ) ̸= 0 on (λ0 −δ, λ0) and on (λ0, λ0 + δ),
(II-b)
xk+1(λ) ̸= 0 on (λ0 −δ, λ0) and xk+1(λ) ≡0 on (λ0, λ0 + δ),
(II-c)
xk+1(λ) ≡0 on (λ0 −δ, λ0) and xk+1(λ) ̸= 0 on (λ0, λ0 + δ), and
(II-d)
xk+1(λ) ≡0 both on (λ0 −δ, λ0) and on (λ0, λ0 + δ).
In subcase (II-a), the function pk(λ) is well deﬁned on (λ0 −δ, λ0) and (λ0, λ0 +δ),
so that it is nondecreasing on each of these two intervals, by Remark 1.15. Since
xk(λ0) ̸= 0, it follows that pk(λ−
0 ) = +∞and pk(λ+
0 ) = −∞, which shows

1.2
Sturm-Liouville Difference Equation
21
that mk(λ−
0 ) = 0 and mk(λ+
0 ) = 1. Since in this case we also have mk(λ0) =
1 (by the deﬁnition of a generalized zero at k + 1) and hk(λ−
0 ) = hk(λ0) = 1,
hk+1(λ−
0 ) = 1, and hk+1(λ0) = 0, it follows that the equations in (1.36) and (1.37)
hold as the identity 1 = 1. In subcase (II-b), the function pk(λ) is well deﬁned and
nondecreasing on (λ0 −δ, λ0), so that pk(λ−
0 ) = +∞, and hence mk(λ−
0 ) = 0.
Moreover, hk(λ−
0 ) = hk(λ0) = 1, hk+1(λ−
0 ) = 1, hk+1(λ0) = 0, and mk(λ+
0 ) =
mk(λ0) = 1, by the deﬁnition of a generalized zero at k + 1. This shows that in this
case, (1.36) and (1.37) hold again as the identity 1 = 1. In subcase (II-c), we have
mk(λ−
0 ) = mk(λ0) = 1 (by the deﬁnition of a generalized zero at k + 1), hk(λ−
0 ) =
hk(λ0) = 1, and hk+1(λ−
0 ) = hk+1(λ0) = 0. Moreover, the function pk(λ) is
well deﬁned and nondecreasing on (λ0, λ0 + δ), so that pk(λ+
0 ) = −∞, and hence
mk(λ+
0 ) = 1. In this case (1.36) and (1.37) hold as the identities 1 = 1 and 0 = 0,
respectively. Finally, in subcase (II-d), we have mk(λ−
0 ) = mk(λ0) = mk(λ+
0 ) (by
the deﬁnition of a generalized zero at k + 1), while hk(λ−
0 ) = hk(λ0) = 1 and
hk+1(λ−
0 ) = hk+1(λ0) = 0. Thus, both (1.36) and (1.37) now hold as the identity
0 = 0. This completes the proof.
⊓⊔
The above result (Theorem 1.17) now leads to further oscillation theorems for
the problem (E0). Denote by
n1(λ) := the number of generalized zeros of x(λ) in (0, N + 1].
(1.38)
Theorem 1.18 (Local Oscillation Theorem II) Assume that (1.29) holds. Con-
sider a nontrivial solution x(λ) = {xk(λ)}N+1
k=0 of (SLλ) satisfying (1.31). Then
n1(λ−) and n1(λ+) exist and for all λ ∈R
n1(λ+) = n1(λ) ≤N + 1,
(1.39)
n1(λ+) −n1(λ−) = hN+1(λ−) −hN+1(λ) ∈{0, 1}.
(1.40)
Hence, the function n1(λ) is nondecreasing in λ on R; the limit
m :=
lim
λ→−∞n1(λ)
(1.41)
exists with m ∈[0, N + 1]Z, so that for a suitable λ0 < 0, we have
n1(λ) ≡m
and
hN+1(λ−) −hN+1(λ) ≡0
for all λ ≤λ0.
(1.42)
Proof The number of generalized zeros of the solution x(λ) in (0, N + 1] is by
deﬁnition
n1(λ) =
N

k=0
mk(λ),
λ ∈R,

22
1
Motivation and Preliminaries
where, as in Theorem 1.17, mk(λ) is the number of generalized zeros of x(λ) in
(k, k + 1]. The statement in (1.39) follows directly from (1.36). The expression in
(1.40) is calculated by the telescope sum of the expression in (1.37). This yields that
n1(λ+) −n1(λ−) = hN+1(λ−) −hN+1(λ) −h0(λ−) + h0(λ),
λ ∈R.
But since by (1.31) the initial conditions of x(λ) do not depend on λ, we have
h0(λ−) = h0(λ) for all λ ∈R, which shows (1.40). From the two conditions (1.39)
and (1.40), we then have that the function n1(λ) is nondecreasing in λ on R. Since
the values of n1(λ) are nonnegative integers, the limit in (1.41) exists with m ∈
N ∪{0}. Consequently, n1(λ) ≡m for λ sufﬁciently negative, say for all λ ≤λ0 for
some λ0 < 0. Hence, n1(λ+) −n1(λ−) ≡0 for λ ≤λ0. Applying (1.40) once more
then yields the second equation in (1.42). This completes the proof.
⊓⊔
Now we relate the above oscillation results with the eigenvalue problem (E0).
We say that a number λ0 ∈R is a ﬁnite eigenvalue of (E0), provided there exists a
nontrivial solution x(λ) = {xk(λ)}N+1
k=0 of (E0) such that xN+1(λ0) = 0 and
xN+1(λ) ̸= 0 for λ in some left neighborhood of λ0.
(1.43)
Note that such a requirement is justiﬁed by Lemma 1.13. We observe that every
ﬁnite eigenvalue of (E0) is also a traditional eigenvalue, for which the “nondegen-
eracy condition” (1.43) is dropped. From the uniqueness of solutions of equation
(SLλ), it then follows that λ0 is a ﬁnite eigenvalue of (E0) if and only if the principal
solution ˆx(λ) (see (1.32)) satisﬁes ˆxN+1(λ0) = 0 and ˆxN+1(λ) ̸= 0 for λ in some left
neighborhood of λ0. Or equivalently, the principal solution ˆx(λ) has hN+1(λ−
0 ) = 1
and hN+1(λ0) = 0. This shows that the difference hN+1(λ−
0 )−hN+1(λ0), whenever
it is positive, indicates a ﬁnite eigenvalue of problem (E0).
From Lemma 1.13 we obtain that under the assumption (1.29), the ﬁnite
eigenvalues of (E0) are isolated. This property was also proven for the classical
eigenvalues of (SLλ) in [44] under the strict monotonicity of rk(λ) and qk(λ). Such
a strict monotonicity assumption is not required in this subsection.
Thus, we ﬁnally arrive at the following global oscillation theorem. Denote by
n2(λ) := the number of ﬁnite eigenvalues of (E0) in (−∞, λ].
(1.44)
Then from this deﬁnition, we have
n2(λ+) = n2(λ),
n2(λ) −n2(λ−) = hN+1(λ−) −hN+1(λ)
for all λ ∈R,
(1.45)
i.e., the positivity of the difference n2(λ)−n2(λ−) indicates a ﬁnite eigenvalue at λ.

1.2
Sturm-Liouville Difference Equation
23
Theorem 1.19 (Global Oscillation Theorem) Assume (1.29). Then for all λ ∈R
n2(λ+) = n2(λ) ≤1,
(1.46)
n2(λ+) −n2(λ−) = n1(λ+) −n1(λ−) ∈{0, 1},
(1.47)
and there exists m ∈[0, N + 1]Z such that
n1(λ) = n2(λ) + m
for all λ ∈R.
(1.48)
Moreover, for a suitable λ0 < 0, we have
n2(λ) ≡0
and
n1(λ) ≡m
for all λ ≤λ0.
(1.49)
Proof The result follows directly from Theorem 1.18.
⊓⊔
Corollary 1.20 Under the assumption (1.29), the ﬁnite eigenvalues of (E0) are
isolated and bounded from below.
Proof From Lemma 1.13 we know that the ﬁnite eigenvalues of (E0) are isolated.
The second statement follows from condition (1.49) of Theorem 1.19, since n2(λ) ≡
0 for all λ ≤λ0 means that there are no ﬁnite eigenvalues of (E0) in the interval
(−∞, λ0].
⊓⊔
It remains to connect the above global oscillation theorem with the traditional
statement saying that the j-th eigenfunction has exactly j generalized zeros in the
interval (0, N+1]. We will see that under some additional assumption, the statement
of this result remains exactly the same when we replace the eigenfunctions of (E0)
by its ﬁnite eigenfunctions. This additional assumption is formulated in terms of the
associated discrete quadratic functional (1.13)
F(η, λ) :=
N

k=0
!rk(λ) (ηk)2 −qk(λ) η2
k+1
",
where η = {ηk}N+1
k=0 is a sequence such that η0 = 0 = ηN+1. The functional F(·, λ)
is positive; we write F(·, λ) > 0, if F(η, λ) > 0 for every sequence η with η0 =
0 = ηN+1 and η ̸= 0.
Theorem 1.21 (Oscillation Theorem) Assume (1.29). Then
n1(λ) = n2(λ)
for all λ ∈R
(1.50)
if and only if there exists λ0 < 0 such that F(·, λ0) > 0. In this case, if λ1 <
λ2 < · · · < λr (where r ≤N + 1) are the ﬁnite eigenvalues of (E0) with the
corresponding ﬁnite eigenfunctions x(1), x(2), ..., x(r), then for each j ∈{1, . . . , r}
the ﬁnite eigenfunction x(j) has exactly j generalized zeros in (0, N + 1].

24
1
Motivation and Preliminaries
Remark 1.22
(i) Note that since the ﬁnite eigenfunction x(j) has x(j)
N+1 = 0, it satisﬁes x(j)
N ̸= 0,
by Remark 1.16. Therefore, the point N + 1 is one of the generalized zeros of
x(j), and consequently the remaining j −1 generalized zeros of x(j) are in the
open interval (0, N + 1). This complies with the traditional continuous time
statement.
(ii) Conditions of Theorem 1.21 are automatically satisﬁed for the classical Sturm-
Liouville problem (1.30), i.e., (1.50). Indeed, for the case qk(λ) = qk + wkλ
and wk > 0, we can estimate
F(η, λ) :=
N

k=0
!
rk (ηk)2 −qk η2
k+1
"
−λ
N

k=0
wkη2
k+1 = F0(η, λ)−λ
N

k=0
wkη2
k+1
such that
pk = rk (ηk)2 −qk η2
k+1 =

ηk ηk+1)
 rk
−rk
−rk rk −qk
  ηk
ηk+1

obeys the inequality |pk| ≤c (η2
k + η2
k+1) for some c > 0, and then introducing
the notation d = mink∈[0,N]Z wk > 0, we see that there exists λ0 < 0 such that
for all λ < λ0
F(η, λ) = F0(η, λ) −λ
N

k=0
wkη2
k+1 ≥−2c∥η∥2 −λd∥η∥2 > 0,
where we use that ∥η∦= 0 (see also [206, Theorem 3]).
Proof of Theorem 1.21 If n1(λ) = n2(λ) for all λ ∈R, then the number m in
equation (1.48) of Theorem 1.19 is zero. This implies through condition (1.49)
that n1(λ) ≡0 for all λ ≤λ0 with some λ0 < 0. By Theorem 1.2, the latter
condition is equivalent to the positivity of the functional F(·, λ) for every λ ≤λ0,
in particular for λ = λ0. Conversely, assume that F(·, λ0) > 0 for some λ0 < 0.
Then n1(λ0) = 0, by Theorem 1.2, and since the function n1(·) is nondecreasing
in λ on R (see Theorem 1.18), it follows that n1(λ) ≡0 for all λ ≤λ0. From this
we see that m = 0 in (1.49) and hence also in (1.48). Equality (1.50) is therefore
established. Finally, assume that (1.50) holds and let λj (where j ∈{1, . . . , r}) be
the j-th ﬁnite eigenvalue of (E0) with the corresponding ﬁnite eigenfunction x(j).
Then n2(λj) = j, and from (1.50), we get n1(λj) = j, i.e., x(j) has exactly j
generalized zeros in (0, N + 1]. The proof is complete.
⊓⊔
In the last part of this section, we present certain results on the existence of ﬁnite
eigenvalues of (E0), in particular a necessary condition and a sufﬁcient condition
for the existence of a ﬁnite eigenvalue and a characterization of the smallest ﬁnite
eigenvalue. The proofs of these results are also based on Theorem 1.2 (see also more
general Theorems 5.32–5.34 for symplectic systems).

1.3
Discrete Variational Theory
25
Theorem 1.23 Assume (1.29). If (E0) has a ﬁnite eigenvalue, then there exist
λ0, λ1 ∈R with λ0 < λ1 and m ∈N ∪{0} such that n1(λ) ≡m for all λ ≤λ0 and
F(·, λ1) ̸> 0.
Theorem 1.24 Assume (1.29). If there exist λ0, λ1 ∈R with λ0 < λ1 such that
F(·, λ0) > 0 and F0(·, λ1) ̸> 0, then (E0) has at least one ﬁnite eigenvalue.
Theorem 1.25 Assume (1.29). Let there exist λ0, λ1 ∈R with λ0 < λ1 such that
F(·, λ0) > 0 and F(·, λ1) ̸> 0. Then the eigenvalue problem (E0) possesses the
smallest ﬁnite eigenvalue λmin, which is characterized by any of the conditions:
λmin = sup{λ ∈R, F(·, λ) > 0},
λmin = min{λ ∈R, F(·, λ) ̸> 0}.
1.3
Discrete Variational Theory
In this section we explain the motivation and origin of the symplectic difference
systems (SDS) in the discrete variational theory, in particular in the discrete calculus
of variations and discrete optimal control theory. We shall see that in both cases
symplectic difference systems arise in these variational problems naturally as the
second-order systems (i.e., as the Jacobi systems). Moreover, in Theorems 1.29
and 1.34, we justify the fact that the theory of symplectic difference systems, rather
than the theory linear Hamiltonian difference systems, is the proper platform for
studying the second-order optimality conditions in the discrete variational theory.
1.3.1
Discrete Calculus of Variations
Given an index N ∈N, we consider the classical nonlinear discrete calculus of
variations problem
minimize
F(x) := K(x0, xN+1) +
N

k=0
L(k, xk+1, xk)
(1.51)
subject to sequences x = {xk}N+1
k=0 satisfying the endpoints constraint
ϕ(x0, xN+1) = 0.
(1.52)
Here x : [0, N +1]Z →Rn is the state variable, K : R2n →R is the endpoints cost,
L : [0, N]Z×Rn×Rn →R is the Lagrangian, and ϕ : R2n →Rr with r ≤2n is the
constraint function. We assume that the data are sufﬁciently smooth, i.e., C1 for the
ﬁrst-order optimality conditions and C2 for the second-order optimality conditions.

26
1
Motivation and Preliminaries
A sequence x = {xk}N+1
k=0 is feasible if it satisﬁes the boundary condition (1.52).
A feasible sequence ˆx is called a local minimum for (1.51) if there exists ε > 0 such
that F(ˆx) ≤F(x) for all feasible x satisfying ∥xk −ˆxk∥< ε for all k ∈[0, N +1]Z,
where ∥·∥is any norm in Rn. Moreover, a local minimum ˆx is strict if F(ˆx) < F(x)
for all feasible x ̸= ˆx with ∥xk −ˆxk∥< ε for all k ∈[0, N + 1]Z. Note that the
concepts of a weak and strong local extremum now coincide, since problem (1.51)
is ﬁnite dimensional.
In the next two theorems, we present the ﬁrst- and second-order necessary and
sufﬁcient optimality conditions for problem (1.51). The proofs of these results can
be found, e.g., in [16, 178, 204]; see also the comments in Sect. 1.7. In the sequel,
we shall denote by Lx and Lu the gradients of L (i.e., the row vectors of partial
derivatives of L) with respect to the second and third variables. Similarly, we denote
by Lxx, Lxu, Luu the matrices containing the corresponding second-order partial
derivatives of L. We note that in contrast with the exposition in [16, Chapter 4], the
gradients Lx and Lu are here the row vectors. The gradient of the functions K and
ϕ will be denoted by the symbol ∇.
Linearizing the problem (1.51) along a feasible sequence ˆx, we deﬁne the r × 2n
matrix
M := ∇ϕ(ˆx0, ˆxN+1).
(1.53)
A sequence η = {ηk}N+1
k=0 is called admissible if it satisﬁes the endpoints constraint
M
 η0
ηN+1

= 0.
(1.54)
The second variation of the functional F at a feasible sequence ˆx is the discrete
quadratic functional
F′′(ˆx, η) :=
 η0
ηN+1
T

 η0
ηN+1

+
N

k=0
ηk+1
ηk
T  Pk Qk
QT
k Rk
 ηk+1
ηk

,
(1.55)
where the symmetric 2n × 2n matrix  and the n × n matrices Pk, Qk, Rk are given
by
Pk := ˆLxx(k),
Qk := ˆLxu(k),
Rk := ˆLuu(k),
(1.56)
 := ∇2KT (ˆx0, ˆxN+1) + γ T ∇2ϕT (ˆx0, ˆxN+1)
(1.57)
with some γ ∈Rr and with the partial derivatives of L evaluated at (k, ˆxk+1, ˆxk).
Note that the matrices Pk and Rk are symmetric, as the Lagrangian L is assumed to
be C2 in the second and third variables.
We say that the second variation F′′ is nonnegative at ˆx if F′′(ˆx, η) ≥0 for
all admissible sequences η = {ηk}N+1
k=0 . An alternative terminology is nonnegative

1.3
Discrete Variational Theory
27
deﬁnite or positive semideﬁnite at ˆx. The quadratic functional is positive at ˆx if
F(ˆx, η) > 0 for all admissible sequences η = {ηk}N+1
k=0 satisfying η ̸≡0. An
alternative terminology is positive deﬁnite at ˆx.
Theorem 1.26 Let {ˆxk}N+1
k=0 be a local minimum for problem (1.51) and assume
that the matrix M in (1.53) has full rank r. Then there exists a vector γ ∈Rr such
that the following conditions hold:
(i) the Euler-Lagrange difference equation
 ˆLu(k) = ˆLx(k),
k ∈[0, N −1]Z,
(1.58)
where the partial derivatives of L are evaluated at (k, ˆxk+1, ˆxk),
(ii) the transversality condition
 ˆLu(0), −ˆLu(N) −ˆLx(N)

= ∇K(ˆx0, ˆxN+1) + γ T M,
(1.59)
(iii) the second variation F′′ with the data from (1.56) and (1.57) evaluated at
(k, ˆxk+1, ˆxk) is nonnegative at ˆx.
Remark 1.27 If we deﬁne ˆLu at k = N + 1 by ˆLu(N + 1) := ˆLu(N) + ˆLx(N),
then the Euler-Lagrange difference equation (1.58) is satisﬁed for all k ∈[0, N]Z,
and the transversality condition in (1.59) can be written in the form known in the
continuous time theory, i.e.,
 ˆLu(0), −ˆLu(N + 1)

= ∇K(ˆx0, ˆxN+1) + γ T M.
Next we formulate sufﬁcient optimality conditions for problem (1.51).
Theorem 1.28 Suppose that a feasible sequence ˆx = {ˆxk}N+1
k=0 satisﬁes, for some
γ ∈Rr, the ﬁrst-order optimality conditions (i) and (ii) in Theorem 1.26 with
the matrix M in (1.53) having full rank r. Furthermore, suppose that the second
variation F′′ is positive at ˆx. Then ˆx is a strict local minimum for problem (1.51).
If the second variation F′′ is nonnegative or even positive at ˆx, then the zero
sequence is its minimum. Applying Theorem 1.26 to this situation, we obtain the
Jacobi equation for problem (1.51), which is the Euler-Lagrangedifference equation
for the functional F′′. Thus, from (1.58) we get the second-order difference equation
(Rkηk + QT
k ηk+1) = Pkηk+1 + Qkηk,
k ∈[0, N −1]Z.
(1.60)
Note that when n = 1 and Qk = 0 for all k ∈[0, N −1]Z, equation (1.60) becomes
the Sturm-Liouville difference equation (1.1) studied in Sect. 1.2.
It is well-known in the literature that equation (1.60) can be written as a special
symplectic difference system (SDS) when the matrices
Rk and Rk + QT
k are invertible for all k ∈[0, N]Z.
(1.61)

28
1
Motivation and Preliminaries
More precisely, in [16, Example 3.17] it is shown that under (1.61) the substitution
xk := ηk,
k ∈[0, N + 1]Z,
uk := Rkηk + QT
k ηk+1,
k ∈[0, N]Z,
uN+1 := uN + PNηN+1 + QNηN
⎫
⎪⎬
⎪⎭
(1.62)
transforms equation (1.60) into the linear Hamiltonian difference system
xk = Akxk+1 + Bkuk,
uk = Ckxk+1 −AT
k uk,
k ∈[0, N]Z,
(1.63)
with symmetric Bk and Ck and invertible I −Ak. In turn, it is shown in [16,
Example 3.10] that system (1.63) is a special symplectic system (SDS) with
invertible Ak = (I −Ak)−1 for all k ∈[0, N]Z; see also Sect. 2.1.2.
In the next result, we present an alternative method, which transforms the Jacobi
equation (1.60) directly into the symplectic system (SDS). This method uses the
weaker assumption that only the matrix
Rk + QT
k is invertible for all k ∈[0, N]Z.
(1.64)
Therefore, in comparison with the result in [16, Section 3.6], we allow the matrices
Rk to be singular.
Theorem 1.29 Assume that (1.64) holds and deﬁne for k ∈[0, N]Z the 2n × 2n
matrices Sk :=

Ak Bk
Ck Dk

by
Ak = (Rk + QT
k )−1Rk, Ck = Pk(Rk + QT
k )−1Rk −Qk(Rk + QT
k )−1QT
k ,
Bk = (Rk + QT
k )−1,
Dk = (Rk + Qk + QT
k + Pk) (Rk + QT
k )−1.
Then Sk is a symplectic matrix for all k ∈[0, N]Z. Consequently, the Jacobi
equation (1.60) is a special symplectic system (SDS), in which yk := (xT
k , uT
k )T
is deﬁned by (1.62). In addition, the resulting symplectic system (SDS) is a Hamil-
tonian system (1.63) if and only if the matrix Rk is invertible for all k ∈[0, N]Z.
Proof By direct calculations and with the aid of the symmetry of Rk and Pk, one
can easily verify that ST
k J Sk = J , which proves the statement. Alternatively, see
Theorem 1.34 in combination with Remark 1.35.
⊓⊔
Remark 1.30 Analogously to problem (1.51), we now consider the discrete calculus
of variations problem without the shift in xk+1 in the Lagrangian L, i.e.,
minimize
F(x) := K(x0, xN+1) +
N

k=0
L(k, xk, xk)
(1.65)

1.3
Discrete Variational Theory
29
subject to sequences x = {xk}N+1
k=0 satisfying the endpoints constraint (1.52). Then
a similar analysis as in Theorem 1.26 yields the Euler-Lagrange difference equation
 ˆLu(k) = ˆLx(k + 1),
k ∈[0, N −1]Z,
(1.66)
or equivalently
[ ˆLu(k) −ˆLx(k)] = ˆLx(k),
k ∈[0, N −1]Z,
(1.67)
where the partial derivatives of L are evaluated at (k, ˆxk, ˆxk) and the transversality
condition
 ˆLu(0) −ˆLx(0), −ˆLu(N)  = ∇K(ˆx0, ˆxN+1) + γ T M.
(1.68)
In fact, these results for problem (1.65) can also be obtained from problem (1.51)
upon writing xk = xk+1 −xk and considering the Lagrangian L(k, x, u) :=
L(k, x −u, u) in (1.51). Applying (1.67) to the second variation F′′, the resulting
Jacobi difference equation then has the form
[(Rk−Qk) ηk+(QT
k −P k) ηk] = P kηk+Qkηk,
k ∈[0, N−1]Z.
(1.69)
Equation (1.69) can be written as the symplectic system (SDS) when the matrix
Rk −QT
k (and not necessarily Rk) is invertible for all k ∈[0, N]Z. These systems
are studied in more detail in [294, Section 2].
1.3.2
Discrete Optimal Control Theory
We now extend the considerations in the previous subsection to discrete optimal
control setting. Thus, for a ﬁxed index N ∈N, we consider the discrete optimal
control problem
minimize
G(x, u) := K(x0, xN+1) +
N

k=0
L(k, xk+1, uk)
(1.70)
subject to the pairs (x, u) of sequences x = {xk}N+1
k=0 and u = {uk}N
k=0 satisfying the
difference equation
xk = f (k, xk+1, uk),
k ∈[0, N]Z,
(1.71)
and the endpoints constraint (1.52). Here x : [0, N +1]Z →Rn is the state variable,
u : [0, N]Z →Rm with m ≤n is the control variable, K : R2n →R is the endpoints
cost, L : [0, N]Z ×Rn ×Rm →R is the Lagrangian, f : [0, N]Z ×Rn ×Rm →Rn

30
1
Motivation and Preliminaries
is the dynamics, and ϕ : R2n →Rr with r ≤2n is the constraint function. We
assume that the functions K, L, f , ϕ are sufﬁciently smooth, i.e., C1 for the ﬁrst-
order optimality conditions and C2 for the second-order optimality conditions. In
order to be able to solve equation (1.71) for xk+1 in terms of xk and uk, we assume
that the n × n matrix
I −fx(k, x, u) is invertible for every k ∈[0, N]Z, x ∈Rn, u ∈Rm.
(1.72)
A pair (x, u) of sequences x = {xk}N+1
k=0 and u = {uk}N
k=0 is feasible if it satisﬁes
(1.71) and (1.52). A feasible pair (ˆx, ˆu) is called a local minimum for (1.70) if there
exists ε > 0 such that G(ˆx, ˆu) ≤G(x, u) for all feasible pairs (x, u) satisfying
∥xk −ˆxk∥< ε for all k ∈[0, N + 1]Z and ∥uk −ˆuk∥< ε for all k ∈[0, N]Z. Here
∥· ∥is any norm in Rn, resp., in Rm. Moreover, a local minimum (ˆx, ˆu) is strict if
G(ˆx, ˆu) < G(x, u) for all such feasible pairs (x, u) ̸= (ˆx, ˆu).
The Hamiltonian H : [0, N]Z × Rn × Rm × Rn × R →R corresponding to
problem (1.70), (1.71) is deﬁned by
H(k, x, u, p, λ) := pT f (k, x, u) + λ L(k, x, u).
(1.73)
Denote the gradients of the function f along a feasible pair (ˆx, ˆu) by
Ak := fx(k, ˆxk+1, ˆuk),
Bk := fu(k, ˆxk+1, ˆuk)
(1.74)
and deﬁne the r × 2n matrix M by (1.53). Then we have A : [0, N]Z →Rn×n and
B : [0, N]Z →Rn×m, and, according to (1.72), the matrix I −Ak is invertible for
all k ∈[0, N]Z. We say that the linear system
ηk = Akηk+1 + Bkvk,
k ∈[0, N]Z,
(1.75)
is M-controllable if for every d ∈Rr there exists a vector α ∈Rn and a sequence
v = {vk}N
k=0 such that the solution η = {ηk}N+1
k=0 of the initial value problem (1.75)
with η0 = α satisﬁes
M
 η0
ηN+1

= d.
In [192, Proposition 4.5] it is proved that if the matrix M has full rank r and I −
Ak is invertible for all k ∈[0, N]Z, then the M-controllability of system (1.75) is
equivalent with the following normality condition on problem (1.70): the system
pk = −AT
k pk,
Bkpk = 0,
k ∈[0, N]Z,
 −p0
pN+1

= MT γ,
(1.76)
where γ ∈Rr possesses only the trivial solution pk ≡0 on [0, N + 1]Z (and then
also γ = 0).

1.3
Discrete Variational Theory
31
A pair (η, v) with η = {ηk}N+1
k=0 and v = {vk}N
k=0 is said to be admissible if it
satisﬁes equation (1.75) and the boundary condition (1.54). For a normal problem,
(1.70) we deﬁne the second variation of the functional G at a feasible pair (ˆx, ˆu) as
the discrete quadratic functional
G′′(ˆx, ˆu, η, v) :=
 η0
ηN+1
T

 η0
ηN+1

+
N

k=0
ηk+1
vk
T  Pk Qk
QT
k Rk
 ηk+1
vk

.
(1.77)
The 2n × 2n matrix  is given in (1.57), and the n × n, n × m, m × m matrices Pk,
Qk, Rk, respectively, are deﬁned by
Pk := pT
k ˆfxx(k) + ˆLxx(k),
Qk := pT
k ˆfxu(k) + ˆLxu(k),
Rk := pT
k ˆfuu(k) + ˆLuu(k),
⎫
⎪⎬
⎪⎭
(1.78)
for some sequence p = {pk}N+1
k=0 , see Theorem 1.32. The partial derivatives of f
and L are evaluated at (k, ˆxk+1, ˆuk). Note that Pk and Rk are symmetric.
Remark 1.31 When the endpoints in problem (1.70) or in problem (1.60) are ﬁxed,
i.e., x0 = A and xN+1 = B with some given A, B ∈Rn, we have r = 2n and
M = I2n in (1.53). In this case the endpoints cost K(x0, xN+1) = K(A, B) is
constant; the variations η = {ηk}N+1
k=0 satisfy η0 = 0 = ηN+1, and  = 02n in
(1.57).
We say that the functional G′′ is nonnegative (or nonnegative deﬁnite or positive
semideﬁnite) at (ˆx, ˆu) if G′′(ˆx, ˆu, η, v) ≥0 for every admissible pair (η, v). We
say that G′′ is positive (or positive deﬁnite) at (ˆx, ˆu) if G′′(ˆx, ˆu, η, v) > 0 for every
admissible pair (η, v) ̸= (0, 0).
Necessary and sufﬁcient optimality conditions for problem (1.70) are formulated
in terms of the weak Pontryagin (maximum) principle and the deﬁniteness of the
second variation G′′.
Theorem 1.32 Assume that (ˆx, ˆu) is a local minimum for problem (1.70) and deﬁne
the matrices Ak, Bk, M by (1.74) and (1.53) and suppose that M has full rank r.
Then there exist a constant λ0 ≥0, a vector γ ∈Rr and a sequence p : [0, N +
1]Z →Rn such that λ0 + N+1
k=0 ∥pk∦= 0 and satisfying:
(i) the adjoint equation −pk = Hx(k, ˆxk+1, ˆuk, pk, λ0), i.e.,
−pk = AT
k pk + λ0 ˆLT
x (k),
k ∈[0, N]Z,
(1.79)
(ii) the stationarity condition Hu(k, ˆxk+1, ˆuk, pk, λ0) = 0, i.e.,
BT
k pk + λ0 ˆLT
u (k) = 0,
k ∈[0, N]Z,
(1.80)

32
1
Motivation and Preliminaries
(iii) the transversality condition
 −p0
pN+1

= λ0 ∇KT (ˆx0, ˆxN+1) + MT γ.
(1.81)
If, in addition, system (1.75) is M-controllable, then we may take λ0 = 1, the
sequence p = {pk}N+1
k=0 and the vector γ are unique, and
(iv) the second variation G′′ is nonnegative at (ˆx, ˆu).
Theorem 1.33 Assume that (ˆx, ˆu) is a feasible pair for problem (1.70); the matrix
M in (1.53) has full rank r, and assume that there exist a vector γ ∈Rr and
a sequence p : [0, N +1]Z →Rn satisfying the conditions (i)–(iii) in Theorem 1.32
with λ0 := 1. Furthermore, suppose that the second variation G′′ is positive at
(ˆx, ˆu). Then (ˆx, ˆu) is a strict local minimum for problem (1.70).
We now proceed in deriving the main result of this section. By Theorem 1.32, the
second variation G′′ is a nonnegative functional at a local minimum (ˆx, ˆu) with the
minimum value zero. Thus, applying the weak Pontryagin principle, i.e., equations
(1.79) and (1.80), to the second variation G′′, we obtain for k ∈[0, N]Z the Jacobi
system (here we substitute qk := −pk)
qk = −AT
k qk + Pkηk+1 + Qkvk,
−BT
k qk + QT
k ηk+1 + Rkvk = 0.
(1.82)
Our aim is to show that system (1.75) and (1.82) has a natural symplectic structure.
More precisely, we will show that under a natural invertibility assumption, the pair
(η, q) solves a symplectic difference system associated with (1.75) and (1.82). For
that we need to solve the second equation in (1.82) for vk, which of course can
be done if, e.g., Rk is invertible. However, similarly to treatment of the discrete
calculus of variations in Theorem 1.29, we wish to avoid the invertibility of Rk and
use a more natural invertibility condition.
Following assumption (1.72) and the notation in (1.74), we deﬁne the n × n
matrix ˜Ak and the m × m matrix Sk by
˜Ak := (I −Ak)−1,
Sk := Rk + QT
k ˜AkBk.
(1.83)
Note that Ak and ˜Ak commute. We impose the following structural assumption that
the matrix
Sk in (1.83) is invertible for all k ∈[0, N]Z.
(1.84)
We will discuss the connection of condition (1.84) with the invertibility of Rk in
Lemma 1.36 and Corollary 1.37 below. Whenever the matrix Rk is invertible and
whenever the matrix Sk is invertible, we deﬁne the n × n matrices Tk and Vk by
Tk := I + ˜AkBkR−1
k QT
k ,
Vk := I −˜AkBkS−1
k QT
k .
(1.85)

1.3
Discrete Variational Theory
33
The next result shows that under (1.84), the Jacobi system (1.82) is a symplectic
difference system.
Theorem 1.34 Assume that the matrices ˜Ak, Sk, Vk are deﬁned in (1.83) and (1.85)
and that condition (1.84) holds. Deﬁne for k ∈[0, N]Z the 2n × 2n matrix Sk :=

Ak Bk
Ck Dk

by
Ak = Vk ˜Ak,
Ck = (PkVk −QkS−1
k QT
k ) ˜Ak,
Bk = ˜AkBkS−1
k BT
k , Dk = (Qk + Pk ˜AkBk) S−1
k BT
k + I −AT
k .

(1.86)
Then Sk is a symplectic matrix for all k ∈[0, N]Z. Consequently, the Jacobi system
(1.75) and (1.82) is a special symplectic system (SDS), in which yk := (ηT
k , qT
k )T .
In addition, the resulting symplectic system (SDS) is a Hamiltonian system (1.63) if
and only if the matrix Vk (or equivalently Rk) is invertible for all k ∈[0, N]Z.
Remark 1.35 In the discrete calculus of variations setting, we have xk = uk for
all k ∈[0, N]Z, i.e., f (k, x, u) = u. In this case m = n and (1.74) and (1.83)
imply that Ak = 0, Bk = I, ˜Ak = I, and Sk = Pk + QT
k . Therefore, we see that
the statement of Theorem 1.34 reduces exactly to the statement of Theorem 1.29,
and that in both cases, the invertibility of Rk is not needed in order to reveal the
symplectic structure of the corresponding Jacobi system.
The proof of Theorem 1.34 is displayed at the end of this section. First we clarify
the relationship between the invertibility conditions on Rk, Sk, Tk, and Vk.
Lemma 1.36 Assume that the matrices ˜Ak, Sk, Tk, Vk are deﬁned in (1.83) and
(1.85). Then the following statements are equivalent:
(i) the matrices Rk and Tk are invertible,
(ii) the matrices Rk and Sk are invertible,
(iii) the matrices Sk and Vk are invertible.
Proof For the proof we adopt a known matrix inversion formula. Given any matrices
A, B, C, D such that the products below are deﬁned, then the invertibility of A, D,
and D −CA−1B implies the invertibility of A −BD−1C with
(A −BD−1C)−1 = A−1 + A−1B (D −CA−1B)−1CA−1.
(1.87)
Based on this fact, we prove the implications
Rk and Tk invertible 	⇒Sk invertible,
(1.88)
Rk and Sk invertible 	⇒Vk and Tk invertible,
(1.89)
Sk and Vk invertible 	⇒Rk invertible.
(1.90)

34
1
Motivation and Preliminaries
Indeed, for (1.88) we set A := Rk, B := −QT
k , C := Bk, D := ˜A−1
k , which yields
by (1.87) that the matrix Sk is invertible. For (1.89) we set A := I, B :=
˜AkBk,
C := QT
k , D := Sk, and then (1.87) yields the invertibility of Vk with the inverse
V −1
k
= Tk. For (1.90) we set A := Sk, B := QT
k , C := ˜AkBk, D := I, and then by
(1.87), we obtain the invertibility of Rk.
Suppose now that condition (i) holds. Then (1.88) yields that Sk is invertible,
so that condition (ii) holds. Next, assuming (ii), we have from (1.89) that Vk is
invertible, i.e., condition (iii) holds. Finally, if we assume (iii), then (1.90) implies
Rk invertible, and then in turn (1.89) yields the invertibility of Tk. Therefore,
condition (i) holds, which completes the proof.
⊓⊔
Corollary 1.37 Assume that the matrices ˜Ak, Sk, Tk, Vk are deﬁned in (1.83) and
(1.85).
(i) Assume that Rk is invertible. Then Tk is invertible if and only if Sk is invertible.
In this case the matrix Vk is also invertible and V −1
k
= Tk.
(ii) Assume that Sk is invertible. Then Vk is invertible if and only if Rk is invertible.
In this case the matrix Tk is also invertible and T −1
k
= Vk.
Note that the invertibility of the matrix Sk alone (without assuming the invertibil-
ity of Rk) in general does not imply the invertibility of Vk or Rk.
Proof of Theorem 1.34 Let the triple (η, v, q) with η = {ηk}N+1
k=0 , v = {vk}N
k=0,
q = {qk}N+1
k=0 solve the Jacobi system (1.75) and (1.82). We solve equation (1.75)
for ηk+1 to get ηk+1 = ˜AkAkηk + ˜AkBkvk. We insert this expression into the second
equation in (1.82), which we solve for vk. Then we get vk = S−1
k (BT
k qk−QT
k ˜Akηk).
Substituting this into the ﬁrst equation in (1.82) and into the above formula for ηk+1,
we obtain that the pair (η, q) solve the equations ηk+1 = Akηk + Bkqk and qk+1 =
Ckηk + Dkqk, whose coefﬁcients are given by (1.86). Upon verifying condition
(1.145) from Sect. 1.6.1 below, we show that the coefﬁcient matrix Sk of this system
is indeed symplectic. Since Rk is symmetric, from the deﬁnition of Sk in (1.83), we
have
Sk −QT
k ˜AkBk = Rk = ST
k −BT
k ˜AT
k Qk.
(1.91)
By using the deﬁnition of Vk in (1.85), it follows that the matrices
CT
k Ak = ˜AT
k (V T
k Pk −QkST −1
k
QT
k ) Vk ˜Ak
(1.91)
=
˜AT
k V T
k PkVk ˜Ak −˜AT
k QkST −1
k
RkS−1
k QT
k ˜Ak,
DT
k Bk = [BkST −1
k
(QT
k + BT
k ˜AT
k Pk) + I −Ak] ˜AkBkS−1
k BT
k
(1.91)
=
BkST −1
k
(Sk + ST
k −Rk + BT
k ˜AT
k Pk ˜AkBk) S−1
k BT
k

1.4
Symplectic Structure of Phase Flow in Hamiltonian Mechanics
35
are symmetric and that
AT
k Dk −CT
k Bk = ˜AT
k V T
k [(Qk + Pk ˜AkBk) S−1
k BT
k + ˜AT −1
k
]
−˜AT
k (V T
k Pk −QkST −1
k
QT
k ) ˜AkBkS−1
k BT
k
= ˜AT
k (I −QkST −1
k
BT
k ˜AT
k ) (QkS−1
k BT
k + ˜AT −1
k
)
+ ˜AT
k QkST −1
k
QT
k ˜AkBkS−1
k BT
k
= I + ˜AT
k QkST −1
k
(ST
k −BT
k ˜AT
k Qk + QT
k ˜AkBk −Sk) S−1
k BT
k
(1.91)
=
I.
Therefore, by (1.145) the matrix Sk in Theorem 1.34 with the block entries in (1.86)
is indeed a symplectic matrix. This shows that the Jacobi system (1.75) and (1.82)
is a symplectic difference system (SDS), in which yk := (ηT
k , qT
k )T . Moreover, this
symplectic difference system is a Hamiltonian system (1.63) if and only if the matrix
Ak is invertible for all k ∈[0, N]Z, i.e., if and only if the matrix Vk (or equivalently
Rk by Corollary 1.37) is invertible for all k ∈[0, N]Z. The proof is complete.
⊓⊔
Remark 1.38 Similarly to Remark 1.30, we may consider an alternative discrete
optimal control problem, in which the dynamics f and the Lagrangian L have no
shift in the state variable, i.e., they are evaluated at (k, xk, uk). Such a problem
leads to a dual Jacobi system, where there is no shift in ηk, but the adjoint variable
appears with the shift as pk+1 (and hence also as qk+1). More precisely, the resulting
equations in the discrete weak Pontryagin principle have the form
−pk = AT
k pk+1 + λ0 ˆL
T
x (k),
k ∈[0, N]Z,
(1.92)
BT
k pk+1 + λ0 ˆL
T
u (k) = 0,
k ∈[0, N]Z,
(1.93)
compare with equations (1.79) and (1.80) in Theorem 1.32. Here the matrices Ak
and Bk are given by the formulas
Ak := f x(k, ˆxk, ˆuk),
Bk := f u(k, ˆxk, ˆuk).
(1.94)
Similarly to Theorem 1.34, the associated Jacobi system also has the symplectic
structure. More details in this direction can be found in the paper [303].
1.4
Symplectic Structure of Phase Flow in Hamiltonian
Mechanics
The attention concentrated to linear symplectic systems as well as to simple models
of discrete Hamiltonian mechanics was initiated in the paper [8] of C. D. Ahlbrandt.
This paper was devoted to basic equations of discrete Lagrangian and Hamiltonian

36
1
Motivation and Preliminaries
mechanics and to ﬁrst applications of the discrete oscillation theory in the discrete
calculus of variations. Ahlbrandt also formulated some open problems in oscilla-
tion theory associated with this topic. The fundamental theorem of the classical
Hamiltonian mechanics states that the evolution of a Hamiltonian system in time is
a symplectic transformation. From this point of view, every Hamiltonian system has
the symplectic structure; see [27].
Let us consider a system consisting of l-mass points with masses mi and
coordinates ri = (xi, yi, zi), i = 1, . . . , l. Suppose that the potential energy of
the system is determined by the function U(r1, . . . , rl), then the evolution of the
system in the potential ﬁeld is described by the Newton equations of motion (see
[27, Section 10])
d
dt (mi ˙ri) + ∂U
∂ri
= 0,
i = 1, . . ., l,
(1.95)
together with associated initial conditions ri(t0) = r0
i and ˙ri(t0) = ˙r0
i for i ∈
{1, . . ., l}. According to the principle of least action, equation of motion (1.95) of
a given mechanical system complies with extremals of the functional
(r) =
 t1
t0
L(t, q, ˙q) dt,
where L = T −U and T = 1
2
l
i=1 mi(˙x2
i + ˙y2
i + ˙z2
i ) is the kinetic energy of the
system. Extremals of this functional are given by its Euler-Lagrange equation
d
dt
∂L
∂˙q

−∂L
∂q = 0,
(1.96)
where q = (q1, . . . , qn) are the generalized coordinates, ˙q = (˙q1, . . . , ˙qn) are the
generalized velocities, n := 3l, and
L(t, q, ˙q) = T −U = 1
2
n

i=1
mi ˙q2
i −U(q1, . . . , qn)
is the Lagrangian of the system. Under the strict convexity assumption on the
Lagrangian with respect to the vector variable ˙q, i.e., that the matrix of second
derivatives
∂2L
∂˙qi∂˙qj , i, j ∈{1, . . . , n}, is positive deﬁnite, it is known (see, e.g., [27,
pg. 65]) that (1.96) is equivalent with the system of 2n ﬁrst-order equations
˙q = ∂H
∂p ,
˙p = −∂H
∂q ,
(1.97)

1.4
Symplectic Structure of Phase Flow in Hamiltonian Mechanics
37
where the Hamiltonian H(t, p, q) is the Legendre (sometimes also called Fenchel
or conjugate) transformation of the Lagrange function with respect to ˙q, i.e.,
H(t, p, q) = sup
˙q

p ˙q −L(t, q, ˙q)

,
where p ˙q is the scalar product of p and ˙q. Introducing the variable y =
p
q

∈R2n,
system (1.97) can be written in the form
˙y = J ∂H
∂y (t, y).
(1.98)
In particular, if the Hamiltonian is quadratic with
H(y, t) = 1
2 yT H(t) y,
H(t) = HT (t),
then system (1.98) takes the form of the linear Hamiltonian differential system
y′ = J H(t) y,
H(t) =
−C(t) AT (t)
A(t)
B(t)

,
H(t) = HT (t).
(1.99)
Recall (see [147, Chapter 4]) that the transformation ˜y = R(t, y) of solutions of
system (1.98) is called canonical if the transformation carries Hamiltonian system
(1.98) again into a Hamiltonian system
˙˜y = J ∂˜H
∂˜y (t, ˜y).
(1.100)
The importance of studying canonical transformations is due to the fact that these
transformations permit replacing the Hamiltonian system (1.98) by another Hamil-
tonian system (1.100), in which the Hamiltonian ˜H is of a simpler structure than
H. If in a phase space we perform successively two canonical transformations, then
the resulting transformation will again be canonical. Furthermore, a transformation
that is inverse to a certain canonical transformation will always be canonical, and the
identical transformation is again canonical. Therefore, all canonical transformations
taken together form a group.
Next we recall two basic statements from the Hamiltonian mechanics (see [27,
147]).
Theorem 1.39 For a certain transformation ˜y = R(t, y) to be canonical, it
is necessary and sufﬁcient that the Jacobian matrix S corresponding to this
transformation is a generalized-symplectic matrix with constant valence c, i.e.,
STJ S = cJ , c ̸= 0.

38
1
Motivation and Preliminaries
In particular, in the case of a univalent transformation with c = 1, the matrix S
is a symplectic matrix. Then, the condition of the symplectic nature of (1.98) must
hold identically relative to all the variables t, q, p.
If in addition R(t, y) is linear, i.e., R(t, y) = S(t)y, then in a given basis in R2n
this mapping can be identiﬁed with its matrix S(t).
For system (1.100) consider the evolution operator gt
H mapping each point y ∈
R2n regarded as the initial value y := y(0) to the point y(t) = gt
H (y(0)) on the
corresponding trajectory through y. This operator is called the Hamiltonian phase
ﬂow of (1.100). We have the following basic property of the Hamiltonian phase ﬂow
(see [147, p.139] and [27]).
Theorem 1.40 A transformation of phase space (from the variables y(0) to the
variables y(t)) carried out by the Hamiltonian phase ﬂow
!
gt
H
"
of (1.100) is
canonical and univalent, and its Jacobian matrix is symplectic.
Basic properties of symplectic matrices will be recalled in Sect. 1.6.1, in particu-
lar, symplectic matrices form a group with respect to the matrix multiplication. This
group of 2n-dimensional symplectic matrices is usually denoted by Sp(2n).
In the particular case when H(y) =
1
2 yT Hy is autonomous, i.e., the matrix
H is constant in t, the phase ﬂow can be expressed in the form y(t) = gt
H y(0),
where gt
H = exp(J H t), and the phase ﬂow exp(J H t) is a symplectic matrix
for every t. In other words, the inﬁnitesimal generator of a symplectic ﬂow is the
matrix J H with symmetric H. The matrix A := J H is called a Hamiltonian matrix
in the literature, as the Hamiltonian matrices of order 2n are deﬁned by the property
ATJ + J A = 0.
The study of the symplectic structure of the phase ﬂow of a discrete Hamiltonian
system
qk = ∂H(k, qk+1, pk)
∂p
,
pk = −∂H(k, qk+1, pk)
∂q
,
(1.101)
where pk, qk ∈Rn, was motivated by the investigation of discrete models of
various mechanical systems (see [145, 162] and the comprehensive references given
therein) and also by the construction of symplectic algorithms in the Hamiltonian
mechanics (see [141, 163, 164, 232] and the references therein). In particular, the
central difference scheme
yk = J hHz
yk+1 + yk
2

for the autonomous system in (1.98) is symplectic (see [140, 141, 163]). The
symplectic structure of the phase ﬂow for the discrete linear Hamiltonian system
xk
uk

= J Hk
xk+1
uk

,
Hk = HT
k ,
(1.102)

1.4
Symplectic Structure of Phase Flow in Hamiltonian Mechanics
39
with
Hk =
−Ck AT
k
Ak
Bk

,
det (I −Ak) ̸= 0,
was investigated under the assumption that Bk > 0 (i.e., Bk is positive deﬁnite) in
[134–137]. The proof of the symplecticity of the discrete ﬂow follows easily from
the fact that system (1.102) can be written as symplectic system (see Sect. 2.1.2).
The following theorem concerning symplectic structure of the phase ﬂow of
(1.101) was proved in [263, Theorem 2.1]; see also [8, Theorem 3] and [13,
Theorem 11].
Theorem 1.41 Let the Hamiltonian H(t, q, p) be twice continuously differentiable
with respect to p, q ∈Rn and
det

I −
 ∂2H
∂q∂p

̸= 0
in some domain D ⊆R2n. Then the phase ﬂow (f k, gk) : R2n →R2n of (1.101),
written in the form
qk+1 = f (k, qk, pk),
pk+1 = g(k, qk, pk),
where
(f k+1, gk+1)(p, q) := (f, g)(k, (f k, gk)(p, q)),
f k0, gk0(p, q) := (p, q)
for some ﬁxed k0, is a discrete one-parametric group of symplectic transformations.
Observe that one of the important consequences of the existence of the sym-
plectic structure of the phase ﬂow of continuous and discrete Hamiltonian systems
(1.98) and (1.101) is the volume-preserving property of a bounded domain of
the phase space R2n (see [27, Liouville theorem, pg. 69], [263, Theorem 2.2]).
Concerning autonomous system (1.98), also the energy-preserving law (see [27,
pg. 207]) is satisﬁed, which says that the Hamiltonian H(p(t), q(t)) is constant
along the solutions p(t), q(t) of (1.98) (i.e., the Hamiltonian H is the ﬁrst integral of
system (1.98)). However, for autonomous discrete system (1.101), this energy law is
no longer satisﬁed in general; see [8, Example 4]. Various examples of conservative
and nonconservative symplectic difference schemes for (1.98) can be found in [163,
Chap. II.16].

40
1
Motivation and Preliminaries
1.5
Linear Hamiltonian Differential Systems
Linear Hamiltonian differential systems (LHS) represent a natural continuous
counterpart of symplectic difference systems (SDS), in the sense that they are
the most general ﬁrst-order linear differential systems, whose fundamental matrix
(i.e., the phase ﬂow) is symplectic, whenever it has this property at an initial
condition. The purpose of this section is to present some basic results from the
oscillation theory of linear Hamiltonian differential systems (LHS), which will be
later compared with corresponding results for the symplectic difference systems
(SDS). In order to keep the content clear and compact, the results in this section are
presented without the proofs. For a comprehensive treatment of linear Hamiltonian
differential systems, we refer to the books [70, 203, 205, 248, 250].
1.5.1
Basic Properties of Solutions of Hamiltonian Systems
In this section we consider the self-adjoint linear differential system (1.99), i.e.,
y′ = J H(t) y,
J =
 0 I
−I 0

, H(t) =
−C(t) AT (t)
A(t)
B(t)

,
(1.103)
with H(t)
=
HT (t)
∈
R2n×2n. Sometimes it is suitable to write (1.103)
componentwise, i.e., we split y =
x
u

with x, u ∈Rn, and then (1.103) can be
written as
x′ = A(t) x + B(t) u,
u′ = C(t) x −AT (t) u.
(1.104)
When referring to the Hamiltonian system (1.104), we always assume that the real
n × n matrices A(t), B(t), C(t) are piecewise continuous on the interval I (by
I we always denote a nondegenerate interval of the real line which may be open,
closed, or half-open) although integrability on I would sufﬁce for most results. We
will start with the notion of a conjoined basis of system (1.104). A 2n × n matrix
solution Y =
X
U

is said to be the conjoined basis if
Y T (t) J Y(t) = XT (t) U(t) −UT (t) X(t) = 0,
rank
X(t)
U(t)

= n.
(1.105)
An alternative terminology for solutions satisfying the ﬁrst condition in (1.105) is
an isotropic solution [70] or a prepared solution [168].
Bases, which are not conjoined, play in the oscillation theory of (1.103) a similar
“destroying” role as the complex solutions in the oscillation theory of the Sturm-
Liouville differential equation (being a special case of system (1.103))

r(t)x′′ + p(t)x = 0.
(1.106)

1.5
Linear Hamiltonian Differential Systems
41
More precisely, the function x(t) = eit is a never vanishing solution of the
oscillatory equation x′′ + x = 0, i.e., it “violates” the Sturmian theory of (1.106).
In the higher dimension, we consider the Hamiltonian system consisting of two
“copies” of the differential equation x′′ + x = 0, i.e.,
x1
x2
′′
+
x1
x2

= 0,
which is “evidently” oscillatory, as the determinant of the X-component of the
conjoined basis
X(t) =
sin t
0
0
cos t

,
U(t) =
cos t
0
0
−sin t

oscillates; see the deﬁnition of the (non)oscillation of (1.103) given below. But at
the same time,
X(t) =
 cos t
sin t
−sin t cos t

,
U(t) =
−sin t
cos t
−cos t −sin t

(1.107)
is a 2n × n matrix solution, whose ﬁrst component is everywhere nonsingular. Note
that Y =
X
U

in (1.107) is not a conjoined solution, as can be veriﬁed by a direct
computation.
Other particular cases of linear Hamiltonian systems (1.103) include the 2n-order
Sturm-Liouville differential equation
n

k=0
(−1)krk(t) x(k)(k) = 0,
rn(t) > 0,
(1.108)
and the Jacobi matrix differential equation

R(t) x′ + QT (t) x
′ = Q(t) x′ + P(t) x
(1.109)
with symmetric R(t) and P(t) and positive deﬁnite R(t); see, e.g.,[250]. Equation
(1.108) can be written as (1.103) with
A(t) ≡(Ai,j)n
i,j=1 =

1
if j = i + 1,
0
otherwise,
B(t) = diag

0, . . . , 0,
1
rn(t)
 
and
C(t) = diag{r0(t), . . . , rn−1(t)}.

42
1
Motivation and Preliminaries
Concerning equation (1.109), substituting u = R(t) x′ + QT (t) x we obtain system
(1.103) with
A(t) = −R−1(t) QT (t),
B(t) = R−1(t),
C(t) = P(t) −Q(t) R−1(t) QT (t).
If Y = X
U
 and ˜Y =  ˜X
˜U
 are 2n × n (not necessarily conjoined) solutions of
(1.103), then we have the Wronskian identity
Y T (t) J ˜Y(t) = XT (t) ˜U(t) −UT (t) ˜X(t) ≡M,
(1.110)
where M ∈Rn×n is a constant matrix. Every conjoined basis of (1.103) can
be completed to a fundamental symplectic matrix of this system. To show this,
let Y =
X
U

be a conjoined basis (1.103) and let ˜Y =
 ˜X
˜U

to be a solution
of (1.103) given by the initial condition ˜Y(t0) = −J Y(t0) K−1, where K :=
Y T (t0) Y(t0). Then by a direct computation, one can verify that Z(t) := (Y(t) ˜Y(t))
is a symplectic fundamental matrix of (1.103), because it is symplectic at t = t0 (see
Lemma 1.58(v)). From this point of view, any conjoined basis of (1.103) generates
a one half of the solution space of (1.103).
Throughout this section we suppose that system (1.103) is identically normal
on an interval I (an alternative terminology is completely controllable), i.e., if
a solution z =
x
u

of (1.103) satisﬁes x(t) ≡0 on a nondegenerate subinterval
of I, then u(t) ≡0 for t ∈I as well. Note that there is a recent theory of focal
points for linear Hamiltonian differential systems (1.103) without the assumption
of identical normality; see [127, 129, 130, 201, 207, 283, 285–289, 291, 295, 321],
but for our motivation, it is suitable to consider identically normal systems. We
also suppose throughout this section (without mentioning it later) that the so-called
Legendre condition is satisﬁed
the matrix B(t) is nonnegative deﬁnite for t ∈I
(1.111)
In the matrix notation introduced in Sect. 1.6 below, the Legendre condition is
written as B(t) ≥0. Next we introduce the concept of a focal point of a conjoined
basis of (1.103).
Deﬁnition 1.42 Suppose that (1.103) is identically normal and the Legendre
condition (1.111) holds on I. We say that a conjoined basis Y =
X
U

has a focal
point at t0 ∈I if det X(t0) = 0. Its multiplicity is then deﬁned as
m(t0) = def X(t0) = dim Ker X(t0) = n −rankX(t0),
(1.112)
where Ker is the kernel of the indicated matrix.

1.5
Linear Hamiltonian Differential Systems
43
Note that this deﬁnition implies that the focal points of conjoined bases of (1.103)
are isolated. Moreover, under assumption (1.111) system (1.103) is completely
controllable on I if and only if the focal points of Y(t) are isolated in I (see [205,
Theorem 4.1.3]). Based on this property, we introduce the notion of an oscillatory
conjoined basis of (1.103) at ∞.
Deﬁnition 1.43 A conjoined basis Y(t) of (1.103) is called oscillatory at ∞if
lim
b→∞P(Y, a, b) = lim
b→∞

t∈(a,b)
m(t) = ∞,
(1.113)
where P(Y, a, b) is the total number of focal points in (a, b). In opposite case Y(t)
is called nonoscillatory at ∞.
Let us recall ﬁrst some results of the Sturmian theory for conjoined bases of
a completely controllable linear Hamiltonian differential systems; see [250]. We
note that all focal points are counted including their multiplicities.
Theorem 1.44 (Sturmian Separation Theorem) Let Y = X
U
 and ˆY =  ˆX
ˆU
 be
conjoined bases of linear Hamiltonian differential system (1.103). Then the numbers
of their focal points in an interval I ⊂R differ by at most n. In particular if Ya =
Xa
Ua
 and Yb = Xb
Ub
 are the conjoined bases of (1.103) given by the initial conditions
Xa(a) = 0, Ua(a) = I and Xb(b) = 0, Ub(b) = −I, then the number of focal
points of Ya in the interval (a, b] is the same as the number of focal point of Yb in
the interval [a, b).
In particular, Theorem 1.44 implies that the existence of a (non)oscillatory
conjoined basis of (1.103) at ∞is equivalent to the property that all conjoined bases
of this system are (non)oscillatory at ∞. In this case we say that system (1.103) is
(non)oscillatory at ∞.
We have also Sturmian comparison theorem for a completely controllable system
(1.103), which reads as follows.
Theorem 1.45 Along with (1.103) consider another system of the same form
y′ = J ˜H(t) y
(1.114)
and suppose that this system is minorant to (1.103), i.e., H(t) ≥˜H(t) for t ∈[a, b].
Let Ya =
Xa
Ua

and ˜Y =
 ˜Xa
˜Xb

be the solutions of (1.103) and (1.114), respectively,
given by the initial condition Ya(a) = 0
I
 = ˜Ya(a). If ˜Ya has m ∈N focal points in
the interval (a, b], then Ya has at least m focal points in this interval.

44
1
Motivation and Preliminaries
1.5.2
Symplectic Transformations
Let R(t) be a symplectic matrix with differentiable entries in some interval I ⊆R.
Consider the transformation
y = R(t) w.
(1.115)
Then w is a solution of the system
w′ = J ˆH(t) w,
ˆH(t) := J R−1(t) [R′(t) −J H(t) R(t)].
(1.116)
We claim that system (1.116) is again a linear Hamiltonian system. Indeed, we
need to prove that the matrix
ˆH(t) is symmetric. By Lemma 1.58(ii) we note
that R−1(t) = −J RT (t) J , since R(t) is symplectic. Moreover, (suppressing the
argument t in the calculations below)
(RT )′J R + RT J R′ = 0
i.e.,
J R−1R′ = −(RT )′ RT −1J .
This implies that the matrix J R−1R′ is symmetric. Moreover,
J R−1J HR = RT J J HR = −RT HR
is also symmetric, which yields that ˆH is symmetric.
In the oscillation theory of linear Hamiltonian differential systems, an important
role is played by the lower block-triangular symplectic transformations. Thus, we
consider the matrix R in the form
R =
H
0
G H T −1

(1.117)
with H T G = GT H. Then the block entries of the matrix ˆH in (1.116) are
ˆA = H −1(−H ′ + AH + BG),
ˆB = H −1BH T −1,
(1.118)
ˆC = −GT (−H ′ + AH + BG) + H T (−G′ + CH −AT G)
as can be veriﬁed by a direct computation.
Remark 1.46 Recall that for any Hamiltonian system (1.103), there exist a block
diagonal transformation matrix
R(t) =
H(t)
0
0
H T −1(t)

(1.119)

1.5
Linear Hamiltonian Differential Systems
45
which transforms (1.103) into (1.116) with
ˆA(t) ≡0. Indeed, it is sufﬁcient to
assume that H(t) is a fundamental matrix of the differential system
H ′ = A(t) H,
(1.120)
then according to (1.118), we have ˆA(t) ≡0.
Suppose that Y =
X
U

is a conjoined basis of (1.103) with X(t) invertible for
t ∈I ⊆R and consider the (symplectic) transformation matrix
R(t) =
X(t)
0
U(t) XT −1(t)

.
Then ˆA(t) = 0, ˆB(t) = X−1(t) B(t) XT −1(t), and ˆC(t) = 0, i.e., the resulting
system for w =
w1
w2

, w1, w2 ∈Rn, has the form
w′
1 = X−1(t) B(t) XT −1(t) w2,
w′
2 = 0.
If we replace vectors w1, w2 by n × n matrices W1, W2, we get a solution
W2(t) = M,
W1(t) =
 t
a
X−1(s) B(s) XT −1(s) ds M + N,
a ∈I,
(1.121)
where M, N are constant n×n matrices. Substituting t = a, we see that this solution
is a conjoined basis if and only if MT N = NT M and rank
M
N

= n. If we denote
S(t) :=
& t
a X−1(s) B(s) XT −1(s) ds in (1.121) and we take M = I and N = 0,
then substituting into the transformation formula ˜Y =
 ˜X
˜U

= R(t)
W1
W2

we see by
a direct computation that
˜X(t) = X(t) S(t),
˜U(t) = U(t) S(t) + XT −1(t)
(1.122)
is a conjoined basis of (1.103). Moreover, we have Y TJ ˜Y = XT ˜U −UT ˜X = I, so
that Z = (Y, ˜Y) is a symplectic fundamental matrix of (1.103).
1.5.3
Riccati Equation and Reid Roundabout Theorem
Suppose that Y =
X
U

is a conjoined basis of (1.103) with X(t) invertible in some
interval I ⊆R. Then Q = UX−1 is a symmetric solution of the Riccati matrix
differential equation
Q′ −C(t) + AT (t) Q + QA(t) + QB(t) Q = 0.
(1.123)

46
1
Motivation and Preliminaries
Another important object associated with (1.103) is its quadratic energy func-
tional. We say that a pair of functions x(t), u(t) for t ∈[a, b] is admissible for the
quadratic functional
F(x, u) =
 b
a

uT (t) B(t) u(t) + xT (t) C(t) x(t)

dt
(1.124)
if x and u satisfy x′(t) = A(t) x(t) + B(t) u(t) on I, i.e., the ﬁrst equation in the
Hamiltonian system (1.103). This equation is again called the equation of motion
or the admissibility equation. For an admissible pair y = (x, u) such that u is
differentiable, we then have
F(y) =
 b
a

uT (x′ −Ax) + xT Cx

(t) dt
=
 b
a

uT x′ + (uT )′x −xT u′ −uTAx + xT Cx

(t) dt
= xT u
			
b
a +
 b
a

xT (−u′ −AT u + Cx)(t) dt.
Consequently, if y satisﬁes also second equation in (1.103), called the Euler-
Lagrange equation, then we have F(y) = xT (t) u(t)
		b
a.
Now we show how (1.124) transforms under lower block-triangular symplectic
transformation with transformation matrix given in (1.117). Substituting (1.118)
into the admissibility equation for the quadratic functional associated with the
transformed system, we see by a short computation that
w1
w2

= R−1(t)
x
u

is
admissible for this functional, i.e., w′
1(t) = ˆA(t) w1(t) + ˆB(t) w2(t), where ˆA(t)
and ˆB(t) are given by (1.118). Further, again substituting ˆA(t), ˆB(t), ˆC(t) from
(1.118) and using that w1 = H −1(t) x and w2 = −GT (t) x + H T (t) u, we derive
the identity
wT
2 ˆB(t) w2 + wT
1 ˆC(t) w1 −(wT
1 w2)′ = uT B(t) u + xT C(t) x −(xT u)′,
which upon integration yields the transformation formula
 b
a
[uT Bu + xT Cx](t) dt = (wT
1 GT Hw1)(t)
			
b
a +
 b
a
[wT
2 ˆBw2 + wT
1 ˆCw1](t) dt.
If the transformation matrix is R(t) =

I
0
Q(t) I

, where Q(t) is a symmetric solution
of Riccati equation (1.123), then ˆA = A + BQ, ˆB = B, ˆC = 0, and we obtain the
so-called Picone identity
F(x, u) = xT (t) Q(t) x(t)
			
b
a +
 b
a

(u −Qx)T B(u −Qx)

(t) dt.
(1.125)

1.5
Linear Hamiltonian Differential Systems
47
In particular, this means that F(x, u) > 0 for admissible x, u with x(a) = 0 = x(b)
and x(t) ̸≡0, t ∈[a, b], whenever there exists a conjoined basis Y =
X
U

with X(t)
invertible for t ∈[a, b].
The previous considerations are summarized in the next statement, the so-
called Reid roundabout theorem for completely controllable linear Hamiltonian
differential systems (1.103), see [250, Theorem V.6.3]. We recall that we assume
the standing hypothesis (1.111) about the matrix B(t).
Theorem 1.47 The following statements are equivalent.
(i) System (1.103) is disconjugate on [a, b], i.e., the solution Y =
X
U

of (1.103)
given by the initial condition X(a) = 0, U(a) = I has no focal point in (a, b],
i.e., it satisﬁes det X(t) ̸= 0 for t ∈(a, b].
(ii) There exists a conjoined basis Y =
X
U

of (1.103) with X(t) invertible for
t ∈[a, b].
(iii) Riccati equation (1.123) has a symmetric solution Q(t) deﬁned in the whole
interval [a, b].
(iv) The quadratic functional F is positive deﬁnite, i.e., F(x, u) > 0 for every
admissible y = (x, u) with x(a) = 0 = x(b) and x(t) ̸≡0 for t ∈[a, b].
(v) The Riccati inequality
Q′ −C(t) + AT (t) Q + QA(t) + QB(t) Q ≤0,
t ∈[a, b],
(1.126)
has a symmetric solution Q(t) deﬁned in the whole interval [a, b].
The following classical result (see [205, Theorem 5.1.2]) concerning solvability
and inequalities for solutions of Riccati equation (1.123) play an important role in
the consideration of their discrete analogs in Chap. 4.
Theorem 1.48 (Riccati Inequality) With the linear Hamiltonian system (1.103),
consider another linear Hamiltonian system
ˆy′ = J ˆH(t) ˆy,
ˆH(t) =

−ˆC(t) ˆAT (t)
ˆA(t)
ˆB(t)

,
ˆH(t) = ˆHT (t),
t ∈I
(1.127)
with the piecewise continuous blocks ˆA(t), ˆB(t), ˆC(t) for t ∈I = [a, b). Suppose
that system (1.127) is controllable on I,
ˆB(t) ≥0 for t
∈I, and for the
Hamiltonians H(t) and ˆH(t), the majorant condition
H(t) −ˆH(t) ≥0,
t ∈[a, b)
(1.128)
holds, where A ≥0 means that the symmetric matrix A is nonnegative deﬁnite.
Consider conjoined bases Y =
X
U

and ˆY =
 ˆX
ˆU

of systems (1.103) and (1.127)
and assume that the initial values Y(a) and ˆY(a) obey the conditions
ˆXT (a) ˆU(a) ≥DT XT (a) U(a) D

48
1
Motivation and Preliminaries
for some matrix D such that ˆX(a) = X(a) D. Assume that Y does not have focal
points in (a, b) and Q(t) = QT (t) = U(t) X−1(t) solves (1.123) on (a, b). Then
the solution ˆQ(t) = ˆQT (t) of
ˆQ′ −ˆC(t) + ˆAT (t) ˆQ + ˆQ ˆA(t) + ˆQ ˆB(t) ˆQ = 0,
where ˆQ(t) = ˆU(t) ˆX−1(t) exists on (a, b) and
ˆQ(t) ≥Q(t),
t ∈(a, b).
Note that when formulating Theorem 1.48, we do not assume that det X(a) ̸=
0 in the initial point t = a. One of the principal tools we use in the proofs of
our results in oscillation theory of symplectic difference systems is the notion of
a comparative index of conjoined bases; see Sect. 3.1. The following statement,
proved in [205, Theorem 7.3.1], can be regarded as a continuous analog of the most
important results of the comparative index theory.
Theorem 1.49 Let Y
=
X
U

and ˆY
=
 ˆX
ˆU

be conjoined bases of systems
(1.103) and (1.127) under the majorant condition (1.128), and system (1.127) is
controllable. Then for the numbers P(Y, a, b) and P( ˆY , a, b) of focal points in
(a, b) of Y(t) and ˆY(t), we have
P( ˆY , a, b) −P(Y, a, b) ≤ind [Q(b−) −ˆQ(b−)] −ind [Q(a+) −ˆQ(a+)],
P( ˆY , a, b) −P(Y, a, b) ≤ind [ ˆQ(a+) −Q(a+)] −ind [ ˆQ(b−) −Q(b−)],
(1.129)
where Q(t) = U(t) X−1(t) and ˆQ(t) = ˆU(t) ˆX−1(t) and ind denotes the index,
i.e., the number of negative eigenvalues of a symmetric matrix.
For a symmetric matrix valued function M(t), the notation ind M(t+
0 ) and
ind M(t−
0 ) mean the right-hand and left-hand limits of the piecewise constant
quantity ind M(t) for t →t+
0 and t →t−
0 .
According to [205, Theorem 7.3.1], inequalities (1.129) turn into equalities for
H(t) ≡ˆH(t), and then we derive from Theorem 1.49 the following separation result
(see [205, Theorem 5.2.1]).
Theorem 1.50 Let Y and ˆY be conjoined bases of a controllable linear Hamil-
tonian system (1.103) with condition (1.111). Deﬁne Q(t) := U(t)X(t)−1 and
ˆQ(t) := ˆU(t) ˆX(t)−1. Then for all t ∈(a, b), we have
m(t) −ˆm(t) = ind [Q(t−) −ˆQ(t−)] −ind [Q(t+) −ˆQ(t+)],
m(t) −ˆm(t) = ind [ ˆQ(t+) −Q(t+)] −ind [ ˆQ(t−) −Q(t−)],
(1.130)

1.5
Linear Hamiltonian Differential Systems
49
where m(t) and ˆm(t) are the multiplicities of focal points of Y and ˆY given by
(1.112). Moreover, instead of inequalities (1.129), we have
P( ˆY , a, b) −P(Y, a, b) = ind [Q(b−) −ˆQ(b−)] −ind [Q(a+) −ˆQ(a+)],
P( ˆY , a, b) −P(Y, a, b) = ind [ ˆQ(a+) −Q(a+)] −ind [ ˆQ(b−) −Q(b−)].
(1.131)
As a direct consequence of Theorem 1.50, we derive the estimate
|P( ˆY , a, b) −P(Y, a, b)| ≤n,
(1.132)
which holds for any conjoined bases Y and ˆY of (1.103). In particular, Theorem 1.50
implies the ﬁrst statement of Theorem 1.44.
Next we have a look at the difference between positivity and nonnegativity of
(1.124). As we have shown, positivity is equivalent to disconjugacy of (1.103) in
the closed interval [a, b]. A slightly modiﬁed considerations prior to Theorem 1.47
show that a necessary and sufﬁcient condition for the nonnegativity of the functional
F in (1.124) over admissible pairs y =
x
u

and the Dirichlet boundary conditions at
a and b is invertibility of X(t) in the open interval (a, b). Here the system (1.103)
is again assumed to be identically normal, and Y =
X
U

is the conjoined basis of
(1.103) given by X(a) = 0 and U(a) = I as in Theorem 1.47. Hence, the difference
between the positivity and nonnegativity of F is just in the possible noninvertibility
of X(b). We have mentioned this topic because later we will show that conditions for
the positivity or nonnegativity of quadratic functionals associated with symplectic
difference systems considerably differ.
Finally, we note that the concept of disconjugacy of (1.103) can be equivalently
deﬁned using the concept of conjugate points of vector solutions of (1.103). We say
that two points t1 < t2 are conjugate relative to (1.103) if there exists a solution
y =
x
u

∈R2n such that x(t1) = 0 = x(t2) and x(t) ̸≡0 for t ∈(t1, t2). Now,
we can deﬁne the disconjugacy of system (1.103) in [a, b] (equivalently to item (i)
of Theorem 1.47) as follows. System (1.103) is disconjugate on [a, b] if there is no
solution of this system with two or more conjugate points in [a, b].
1.5.4
Trigonometric and Prüfer Transformations
The trigonometric transformation (an alternative terminology is the Bohl transfor-
mation) and the Prüfer transformation for classical Sturm-Liouville equation (1.106)
present formulas, where either a pair of linearly independent solutions (for the case
of the trigonometric transformation) or a solution and its quasiderivative (for the
case of the Prüfer transformation) are expressed via the sine and cosine functions.

50
1
Motivation and Preliminaries
To formulate a similar result for Hamiltonian systems, we need the concept of
trigonometric Hamiltonian system. Consider the special Hamiltonian system
S′ = Q(t) C,
C′ = −Q(t) S
(1.133)
with symmetric and nonnegative deﬁnite coefﬁcient matrix Q(t) ∈Rn×n. This
system is called a trigonometric system (see [32, 245]), and its solution are the
sine and cosine matrices. This terminology is motivated by the fact that in the
scalar case n = 1, the functions S(t) = sin ∫t Q(s) ds, C(t) = cos ∫t Q(s) ds
are solutions of (1.133). Also, if S
C
 is a conjoined basis of (1.133), then  C
−S
 is
a conjoined basis of this system as well. Moreover, if the initial condition (say at
t = a) is such that the matrix Z =

C(a) −S(a)
S(a) C(a)

is orthogonal (i.e., ZT Z = I, e.g.,
S(a) = 0, C(a) = I), then this matrix is orthogonal everywhere, and we have the
“trigonometric” identities
ST (t) S(t) + CT (t) C(t) = I = S(t) ST (t) + C(t) CT (t),
(1.134)
ST (t) C(t) = CT (t) S(t),
S(t) CT (t) = C(t) ST (t).
In this subsection we will denote the block entries of H by the calligraphic letters
A, B, C, since the letter C will be reserved for the cosine matrix function.
Theorem 1.51 (Matrix Prüfer Transformation) Let Y =
X
U

be a conjoined
basis of (1.103). There exist a nonsingular matrix H(t) and a symmetric matrix
Q(t) on I such that
X(t) = ST (t) H(t),
U(t) = CT (t) H(t),
(1.135)
where S
C
 is a conjoined basis of (1.133) with
Q(t) =
ST (t)
CT (t)
T
H(t)
ST (t)
CT (t)

and H(t) solves the ﬁrst-order differential system
H ′ =
ST (t)
CT (t)
T
J H(t)
ST (t)
CT (t)

H.
Recall that the “angular” equation in the Prüfer transformation for (1.106) (x =
ρ sin ϕ, rx′ = ρ cos ϕ) is
ϕ′ =
1
r(t) cos2 ϕ + p(t) sin2 ϕ =
sin ϕ
cos ϕ
T p(t)
0
0
1/r(t)
 sin ϕ
cos ϕ

.

1.5
Linear Hamiltonian Differential Systems
51
Therefore, Q plays the role of ϕ′ when (1.103) is rewritten Sturm-Liouville equation
(1.106). This is in a good agreement with the fact that S(t) = sin ∫t Q(s) ds and
C(t) = cos ∫t Q(s) ds is a solution of (1.133) in this scalar case.
The trigonometric (Bohl) transformation expresses a normalized pair of con-
joined bases via trigonometric matrices; see [77]. Recall that conjoined bases
Y =
X
U

, ˜Y =
 ˜X
˜U

form a normalized pair of conjoined bases if Y TJ ˜Y =
XT ˜U −UT ˜X = I.
Theorem 1.52 Let Y =
X
U

and ˜Y =
 ˜X
˜U

be a pair of normalized conjoined
bases of (1.103). There exist continuously differentiable n×n matrix functions H(t)
and G(t) such that H T (t) G(t) = GT (t) H(t) and the transformation (1.115) with
R(t) given by (1.117) transforms system (1.103) into (1.133). In particular, the ﬁrst
components of Y, ˜Y can be expressed as X(t) = H(t) S(t) and ˜X(t) = H(t) C(t).
Moreover, the matrices H(t) and G(t) are given by the formulas (suppressing the
argument t)
HH T = XXT + ˜X ˜XT ,
G = (UXT + ˜U ˜XT ) H T −1,
and the matrix Q(t) in (1.133) is given by the formula
Q(t) = H −1(t) B(t) H T −1(t).
Next we show some applications of this transformation in oscillation theory
of differential Hamiltonian systems. It is known (see [250]) that an eventually
controllable trigonometric differential system (1.133) is oscillatory if and only if
 ∞
TrQ(s) ds = ∞,
(1.136)
where Tr denotes the trace, i.e., the sum of the diagonal elements of a matrix.
Combining this result with the transformation given in Theorem 1.52, we have the
following oscillation criterion for (1.103).
Theorem 1.53 Suppose that B(t) is nonnegative deﬁnite for large t, system (1.103)
is eventually controllable,
& ∞Tr B(s) ds = ∞, and there exists a constant M such
that eventually ∥X(t)∥≤M for any conjoined basis (X, U) of (1.103). Then system
(1.103) is oscillatory.
1.5.5
Principal and Nonprincipal Solutions
Recall that we suppose throughout this section that (1.103) is identically normal. In
this subsection we suppose that this assumption is satisﬁed in an interval of the form
[t0, ∞). We also suppose that system (1.103) is nonoscillatory, i.e., this system is

52
1
Motivation and Preliminaries
disconjugate on [t0, ∞) if t0 is sufﬁciently large. These assumptions imply that X(t)
is invertible for large t for any conjoined basis Y =
X
U

of (1.103).
A conjoined basis ˜Y =
 ˜X
˜U

of (1.103) is said to be a principal solution (at ∞) if
lim
t→∞X−1(t) ˜X(t) = 0
(1.137)
for any conjoined basis Y =
X
U

such that the (constant) matrix
Y T (t) J ˜Y(t) = XT (t) ˜U(t) −UT (t) ˜X(t)
is nonsingular.
(1.138)
Any conjoined basis Y satisfying (1.138) is then said to be a nonprincipal solution
of (1.103) (at ∞); an alternative terminology is an antiprincipal solution (at ∞).
Principal and nonprincipal solutions of Hamiltonian systems can be characterized
equivalently as conjoined bases whose ﬁrst component satisﬁes
lim
t→∞
 t
t0
X−1(s) B(s) XT −1(s) ds
−1
= 0
(1.139)
for the principal solution, while
lim
t→∞
 t
t0
X−1(s) B(s) XT −1(s) ds
−1
= T,
where T is an invertible matrix, for the nonprincipal solution. A principal solution
of a nonoscillatory and identically normal system (1.103) is unique up to a constant
right nonsingular multiple (see Theorem 1.54 below).
Theorem 1.54 The principal solution of (1.103) exists whenever this system is
nonoscillatory and eventually identically normal. This solution is unique up to
a right multiplication by a constant invertible n × n matrix.
Another equivalent characterization of the principal solution of (1.123) is via the
associated Riccati matrix equation. A conjoined basis  ˜X
˜U
 is the principal solution
if and only if ˜Q = ˜U ˜X−1 is the eventually minimal solution of (1.123) in the sense
that for any other symmetric solution Q of this equation, we have Q(t) ≥˜Q(t).
The largest focal point of the principal solution at ∞, if any, has the following
extremal property; see [105].
Theorem 1.55 Let T be the largest focal point of the principal solution ˜Y =  ˜X
˜U
 of
(1.103) at ∞. Then any other conjoined basis Y =
X
U

has a focal point in [T, ∞).
More precisely, denote by ˜P(T ) and P(T ) the number of focal points in [T, ∞) of
˜Y and of Y (including multiplicities), respectively. Then
P(T ) ≥˜P(T ).

1.6
Linear Algebra and Matrix Analysis
53
1.5.6
Nonlinear Dependence on Spectral Parameter
In this ﬁnal subsection, we consider linear Hamiltonian systems with generally
nonlinear dependence on a spectral parameter. The presented results are taken from
the book [205]. We consider the linear Hamiltonian system
y′ = J H(t, λ) y,
H(t, λ) =
−C(t, λ) AT (t, λ)
A(t, λ)
B(t, λ)

,
(1.140)
on the interval [a, b] with symmetric H(t, λ) and the Dirichlet boundary conditions
x(a) = 0 = x(b).
(1.141)
We suppose that the matrices A, B, C are piecewise continuous with respect to t
on [a, b], that system (1.140) is identically normal, and that the Legendre condition
B(t) ≥0 for t ∈[a, b] holds. Furthermore, we suppose the following.
(i) For every t ∈[a, b] the Hamiltonian H(t, λ) is nondecreasing on R with
respect to the spectral parameter λ, i.e., H(t, λ1) ≤H(t, λ2) for λ1 < λ2.
(ii) System (1.140) is strictly normal, i.e., if y(t, λ) is a solution of (1.140) on [a, b]
for two different values λ1 and λ2 of the spectral parameter, then y(t, λi) ≡0
for t ∈[a, b] and i ∈{1, 2}.
(iii) There exists λ0 such that for all λ ≤λ0 the associated quadratic functional is
positive deﬁnite, i.e.,
F(y, λ) =
 b
a
[xT (t) C(t, λ) x(t) + uT (t) B(t, λ) u(t)] dt > 0
for all admissible y = (x, u) with x(a) = 0 = x(b) and x(t) ̸≡0 on [a, b].
Note that these assumptions imply that focal points of any conjoined basis of system
(1.140) are isolated (this we have already recalled earlier) and also that the spectrum
of (1.140), (1.141) is discrete and bounded below. The following statement, which
can be found in [205, Theorem 7.2.1], we will partially “discretize” in the later parts
of the book.
Theorem 1.56 (Global Oscillation Theorem) Let the above assumptions be sat-
isﬁed. Then the number of eigenvalues of (1.140), (1.141), which are less than or
equal to a given value λ ∈R, is equal to the number of focal points in (a, b] of the
solution Y(t, λ) of (1.140) satisfying Y(a, λ) = (0 I)T .

54
1
Motivation and Preliminaries
1.6
Linear Algebra and Matrix Analysis
In this section we present basic notions and results from linear algebra needed for
our treatment. Naturally, we concentrate on symplectic matrices but also on the
Moore-Penrose pseudoinverse matrices, which play important role in the discrete
oscillation theory. We also discuss some results related to the LU factorization for
symplectic matrices.
We use a standard matrix notation. The transpose and inverse of a matrix M
are denoted by MT and M−1. Instead of (MT )−1 we will write MT −1. By M∗we
denote the conjugate transpose of M, i.e., M∗= (M)T , where M is the matrix,
whose entries are complex conjugate of entries of M. Similarly as above, the
notation M∗−1 means (M∗)−1. If M is a symmetric matrix, then the inequalities
M > 0, M ≥0, M < 0, M ≤0 mean that M is positive deﬁnite, positive
semideﬁnite, negative deﬁnite, and negative semideﬁnite, respectively. If M and N
are symmetric matrices, then the inequalities M > N, M ≥N, M < N, M ≤N
mean that M −N > 0, M −N ≥0, M −N < 0, M −N ≤0. By ind M we
denote the index of M, i.e., the number of negative eigenvalues of M (including the
algebraic multiplicities of the eigenvalues).
If M is an m × n matrix with real entries, we will write M ∈Rm×n. By Ker M
and Im M, we denote the kernel and image of M, i.e.,
Ker M = {c ∈Rn : Mc = 0},
Im M = {d ∈Rm : ∃c ∈Rn with d = Ac}.
By def M and rank M, we denote the defect and the rank of M, i.e., the dimension of
Ker M and Im M, respectively. If M ∈Rn×n, then def M = n −rank M. Moreover,
for any matrices M and N of suitable dimensions, we have the formula
rank(MN) = rank N −dim (Ker M ∩Im N).
(1.142)
If M(t) is a matrix valued function, then we will use the notation
rank M(t±
0 ),
ind M(t±
0 ),
defM(t±
0 )
for the right-hand and left-hand limits at the point t0 of the piecewise constant
quantities rankM(t), ind M(t), def M(t). In agreement with the above, the notation
μ(Y1(t±
0 ), Y2(t±
0 )) involving the comparative index (see Sect. 3.1.1) means the one-
sided limits of the piecewise constant quantity μ(Y1(t), Y2(t)). In a similar way, we
will use the notation Ker M(t±
0 ) for the constant subspace Ker M(t) in the right and
left neighborhoods of t0.
If a1, . . . , an ∈R, we denote by diag{a1, . . . , an} the diagonal matrix A =
{Aij}n
i,j=1 with Aij = 0 for i ̸= j and Aii = ai for i = 1, . . . , n. A similar notation
will be used for block diagonal matrices. If some additional matrix notation will be
used only in a particular place in this book, we will explain it at that place where it
is used.

1.6
Linear Algebra and Matrix Analysis
55
In Sect. 1.6.2 we will recall the concept of the Moore-Penrose generalized inverse
(or the pseudoinverse) of a matrix M. This will be denoted by M†.
We will also use a standard notation for matrix groups. By GL(n) we denote
the group of invertible n × n matrices with real entries; O(n) denotes the group
of n × n orthogonal matrices, i.e., square matrices M with MT = M−1. Finally,
Sp(2n) denotes the group of 2n×2n symplectic matrices, whose deﬁnition is given
below in Sect. 1.6.1.
1.6.1
Symplectic Matrices
In this subsection we provide the deﬁnition and basic properties of symplectic
matrices, which are of course fundamental for the theory of symplectic difference
systems.
Deﬁnition 1.57 A real 2n × 2n matrix S is symplectic, we write S ∈Sp(2n), if
STJ S = J ,
J =
 0 I
−I 0

.
(1.143)
In (1.143), the matrix I is the n×n identity matrix. The matrix J is the canonical
skew-symmetric 2n × 2n matrix, and of course J is symplectic, i.e., J ∈Sp(2n)
with J −1 = J T = −J . If we write the matrix S in the block form
S =
A B
C D

(1.144)
with n × n matrices A, B, C, D, then substituting into (1.143), we get the identities
AT D −CT B = I,
AT C = CT A,
BT D = DT B.
(1.145)
The following lemma summarizes basic properties of symplectic matrices.
Lemma 1.58 Symplectic matrices have the following properties.
(i) The product of two symplectic matrices is also a symplectic matrix, i.e., if
S1, S2 ∈Sp(2n), then also the product S1S2 ∈Sp(2n).
(ii) Every symplectic matrix is invertible with determinant equal to 1, and the
inverse of a symplectic matrix is also symplectic, i.e., for S ∈Sp(2n) we
have det S = 1 and S−1 ∈Sp(2n). In the block notation (1.144), this means
that
S−1 = J T ST J =
 DT −BT
−CT
AT

.

56
1
Motivation and Preliminaries
(iii) The transpose of a symplectic matrix is also symplectic, i.e., for S ∈Sp(2n),
we have ST ∈Sp(2n). This means in terms of the blocks of S that
BAT = ABT ,
CDT = DCT ,
ADT −BCT = I.
(1.146)
(iv) If P is a matrix such that P TJ P = J T , then S ∈Sp(2n) if and only if
PSP −1 ∈Sp(2n).
(v) If S = (Y
˜Y) with 2n × n matrices Y and ˜Y, then S ∈Sp(2n) if and only if
Y and ˜Y satisfy
Y TJ Y = 0,
˜Y TJ ˜Y = 0,
rank Y = n = rank ˜Y
(1.147)
and
w(Y, ˜Y ) := Y TJ ˜Y = I.
(1.148)
(vi) If a 2n × n matrix Y =
X
U

satisﬁes conditions in (1.147), then there exist
symplectic matrices Z and ˜Z such that Z =

ˆX X
ˆU U

and ˜Z =

X ˜X
U ˜U

, i.e.,
i.e., Z
0
I

= Y and ˜Z
I
0

= Y.
(vii) If the 2n × n matrix Y satisﬁes (1.147), then we have
Ker Y T = Im(J Y).
(1.149)
If M ∈Rn×n is invertible, and S ∈Sp(2n), then the matrix S YM also
satisﬁes (1.147).
(viii) For any 2n × n matrix Y =
X
U

satisfying (1.147) there exists σ ∈R such
that
det(X + σ U) ̸= 0.
(1.150)
Proof The properties (i)–(iii) and (vii) are well-known properties of symplectic
matrices which are proved, e.g., in [74, 205, 328, 332]. To prove the property (iv),
we use the deﬁnition of the group Sp(2n) and the arguments
S ∈Sp(2n) ⇐⇒ST J S = J ⇐⇒ST J T S = J T
⇐⇒ST P T J PS = P T J P ⇐⇒P T −1ST P T J PSP −1 = J
⇐⇒PSP −1 ∈Sp(2n).
The property (v) follows from the deﬁnition of Sp(2n) and property (ii). The proof
of the property (vi) can be found in [205, Corollary 3.3.9]. To construct the matrix
Z, we set K := (Y T Y)−1 = (XT X + UT U)−1. The matrix K is invertible because

1.6
Linear Algebra and Matrix Analysis
57
of the rank condition in (1.147). Then
Z =
 UK X
−XK U

,
˜Z =
X −UK
U
XK

.
The matrices Z and ˜Z are really symplectic in view of property (v).
The proof of (viii) about the existence of σ such that (1.150) holds, which we
present here, can be found in [1, pg. 41] or [205, Lemma 3.3.1, pg. 86]. Consider
the polynomial q(σ) = det(X + σU), σ ∈C. Then for the choice of σ = i, the
matrix (X + iU)∗(X + iU) = XT X + UT U is nonsingular in view of the third
condition in (1.147). This implies that q(σ) ̸≡0 and q(σ) = 0 only for a ﬁnite
number of its roots. Hence there exists σ ∈R such that (1.150) holds.
⊓⊔
Particular cases of the matrix P from (iv) of the previous lemma which will be
used later in this book are
P1 =
0 I
I 0

,
P2 = diag{I, −I},
P3 = diag{−I, I}.
(1.151)
None of these matrices is symplectic, but they satisfy the assumptions of property
(iv), as one can easily verify.
Examples of symplectic matrices, which we will frequently use in this book, are
the lower block-triangular matrix
L =
H
0
G H T −1

,
H T G = GT H,
with H invertible,
(1.152)
and its special case (unit lower block-triangular matrix)
L =
 I 0
Q I

,
with Q symmetric.
(1.153)
Analogically, given (1.152) or (1.153), then the matrices
R1 =
H
G
0 H T −1

,
R2 =
I Q
0 I

(1.154)
are also symplectic (unit upper block triangular in the case of R2), as a result of
Lemma 1.58(iv) with P := P1 from (1.151).
We will also need matrices which are symplectic and orthogonal at the same
time. Recall that an invertible matrix M is orthogonal if M−1 = MT . Orthogonal
matrices form a group with respect to the matrix multiplication; this group is for
matrices with dimension 2n denoted by O(2n). If S ∈Sp(2n) ∩O(2n), then the

58
1
Motivation and Preliminaries
matrix S has the block form
S =
 P Q
−Q P

,
(1.155)
where P and Q are n × n matrices satisfying
PT P + QTQ = I = PPT + QQT ,
PTQ = QT P, PQT = QPT .
(1.156)
The structure in (1.155) and the properties in (1.156) follow from the structure in
(1.144) and from (1.145) and (1.146).
Remark 1.59 If Q ∈Rn×n is orthogonal, then the matrix diag{Q, Q} is orthogonal
and symplectic, i.e., the matrices diag{Q, Q} and J commute.
1.6.2
Moore-Penrose Pseudoinverse
Pseudoinverse matrices play an important role in the analysis of symplectic
difference systems. A comprehensive treatment of pseudoinverse matrices can be
found, e.g., in the book [34]; further references are given in Sect. 1.7. Here we
present only the results, which we need in this book.
Let A ∈Rm×n. Then there exists a uniquely determined matrix B ∈Rn×m
satisfying the following four properties
BAB = B,
ABA = A,
BA and AB are symmetric.
(1.157)
The matrix B is called the pseudoinverse (or the Moore-Penrose generalized inverse)
of A, and it is denoted by A†. It can be explicitly given by the formula
A† = lim
t→0+(AT A + tI)−1AT = lim
t→0+ AT (AAT + tI)−1.
Remark 1.60
(i) For a matrix A ∈Rm×n, we have (AT )† = (A†)T , (A†)† = A, and Im A† =
Im AT , Ker A† = Ker AT .
(ii) If A ∈Rm×n and if P and Q are orthogonal matrices of suitable dimensions,
then the formula (QAP)† = P TA† QT holds.
(iii) If A ∈Rm×n, then Ker AT = Ker(AA†).
(iv) For matrices A ∈Rm×n and B ∈Rn×p, we have Ker (AB) = Ker (A†AB).
(v) If {Aj}∞
j=1 is a sequence of m × n matrices such that Aj →A for j →∞,
then the limit of A†
j as j →∞exists, i.e., A†
j →B for j →∞, if and only
if there exists j0 ∈N such that rankAj = rankA for all j ≥j0. And in this
case B = A†.

1.6
Linear Algebra and Matrix Analysis
59
(vi) Let A and B be symmetric and positive semideﬁnite matrices such that A ≤
B. Then AB†A ≤A or equivalently A†AB†A†A ≤A†. Furthermore, the
inequality B† ≤A† is equivalent with Im A = Im B or with rank A = rank B.
(vii) For matrices A, B ∈Rm×n satisfying AT B = 0 and BAT = 0, we have
(A + B)† = A† + B† and rank (A + B) = rankA + rank B.
The following result will also be useful in Sect. 6.3.5.
Lemma 1.61 Let {Aj}∞
j=1 be a sequence of matrices. Assume that
(i) Aj →A for j →∞,
(ii) there exists an index j0 ∈N such that Im Aj ⊆Im A and Im AT
j ⊆Im AT for
all j ≥j0.
Then there exists an index j1 ∈N such that Im Aj = Im A and Im AT
j = Im AT for
all j ≥j1 and A†
j →A† for j →∞.
The Moore-Penrose pseudoinverse is closely related with constructing orthogo-
nal projectors. If V is a linear subspace in Rn, then we denote by PV the n × n
corresponding orthogonal projector onto V .
Remark 1.62 For any A ∈Rm×n the matrix AA† is the orthogonal projector
onto Im A, and the matrix A†A is the orthogonal projector onto Im AT . Moreover,
rankA = rankAA† = rankA†A. In addition, for matrices A and B, we have
(AB)† = (PIm AT B)† (APIm B)† = (A†AB)†(ABB†)†.
(1.158)
One of the main reasons why pseudoinverse matrices appear in oscillation
theory of discrete symplectic systems is that if V, W ∈Rn×n, then we have the
equivalences
Ker V ⊆Ker W
⇐⇒
W = WV †V,
(1.159)
Im W ⊆Im V
⇐⇒
V V †W = W.
(1.160)
In our later treatment, we will also need the following properties of pseudoinverse
matrices. The next lemma can be found in [34, Theorem 8 and Lemma 3]. This
result also follows from the facts that the matrix XX† is the orthogonal projector
onto Im X and the matrix X†X is the orthogonal projector onto Im XT .
Lemma 1.63 Let X, ˜X ∈Rm×n. If Im X = Im ˜X, then XX† = ˜X ˜X†. Analogously,
if Ker X = Ker ˜X, then X†X = ˜X† ˜X.
As a corollary to Lemma 1.63, we derive the following result which plays an
important role in the proofs of Chap. 3.
Lemma 1.64 Suppose that the square matrices X, ˜X, A, B satisfy
˜X = AXB,
det A ̸= 0, det B ̸= 0.

60
1
Motivation and Preliminaries
Then
I −˜X ˜X† = T (I −XX†) A−1,
I −˜X† ˜X = B−1(I −X†X) ˜T ,
(1.161)
where T and ˜T are invertible matrices independent of B and A, respectively, such
that T = I for A = I and ˜T = I for B = I.
Proof Suppose that the singular value decomposition (SVD, see [157, p.70]) of X
is of the form X = V UT , where  is the diagonal matrix containing the singular
values of X, and U and V are orthogonal matrices. Then
I −XX† = V GV T ,
I −X†X = UGUT ,
G := I −F,
F := †.
(1.162)
Observe that according to Lemma 1.63 applied to the particular case A = I (in this
case Im ˜X = Im X), we have I −˜X ˜X† = I −XX†. Applying this result to the
matrices ˜X and ˜XB−1U, we obtain
I −˜X ˜X† = I −(AV ) (AV )†
= [I −(AV ) (AV )†] (AV F + AV G) (AV )−1
= [I −(AV ) (AV )†] AV GV T A−1
= AV [I −F (AV )†AV G] GV T A−1
= T V GV T A−1 = T (I −XX†) A−1,
T = AV [I −F (AV )†AV G] V T ,
where (1.162) has been used. Observe that the matrix T is invertible, since it is
a product of the matrices A, V , and I −F (AV )†AV G, the last matrix being
invertible by the (easily to verify) implication
PR = 0
	⇒
(I −RUP) (I + RUP) = I
(1.163)
with P := G, R := F, U :=  (AV )†AV . Using Remark 1.60 (ii) it is easy
to verify that T = I if A = I. Hence, the ﬁrst statement in (1.161) is proved.
Applying this result in computing I −˜XT ( ˜XT )† = I −˜X† ˜X, we obtain also the
second identity in (1.161).
⊓⊔
Next we consider results on solvability of the matrix equations
XT QX = XT U.
(1.164)
associated with the n × n blocks X, U of the matrix Y =
X
U

with conditions
(1.105).

1.6
Linear Algebra and Matrix Analysis
61
Lemma 1.65 Assume that the matrix Y ∈R2n×n satisﬁes the ﬁrst condition in
(1.147), then equation (1.164) is solvable, and the general solution of (1.164) is of
the form
Q = XX†UX† + E −XX†EXX†,
(1.165)
where E ∈Rn×n is arbitrary.
Proof According to [215, Theorem 6.11] the matrix equation
A Q C = B
(1.166)
has a solution Q if and only if AA†BC†C = B. In this case the general solution of
(1.166) is of the form
Q = A†BC† + E −A†AECC†,
where E ∈Rn×n is arbitrary. Putting A := XT , C := X, B := XT U and using
(1.147), we see that the relation XT (XT )†XT UX†X = XT U does hold and the
general solution of (1.164) has the form (1.165).
⊓⊔
Remark 1.66 In the oscillation theory, we use symmetric solutions of (1.164).
Applying (1.147) we see that the ﬁrst addend XX†UX† = (X†)T XT UX† in (1.165)
is symmetric. Then, to construct the general symmetric solution of (1.164), we
demand the symmetry of the last addends in (1.165)
E = E −XX†EXX†
= XX†E (I −XX†) + (I −XX†) EXX† + (I −XX†) E (I −XX†)
= ET .
(1.167)
Finally, the general symmetric solution of (1.164) is of the form
Q = XX†UX† + E,
E = ET ,
XT EX = 0.
(1.168)
We complete this section by the following important complement of properties
(vi) and (viii) in Lemma 1.58.
Lemma 1.67 Under assumptions and the notation of Lemma 1.58(vi), there exists
σ ∈R such that the symplectic matrix Z
 I
0
σI I

=
 ˆX+σX X
ˆU+σU U

obeys the condition
det( ˆX + σX) ̸= 0. Moreover, the number σ ∈R can be chosen in such a way that
( ˆX + σX)−1X ≥0,
(1.169)

62
1
Motivation and Preliminaries
or
( ˆX + σX)−1X ≤0.
(1.170)
The same assertion holds for ˜Z
 I σI
0 I

=

X ˜X+σX
U ˜U+σU

.
Proof Putting Y := ( ˆX X)T in (1.150) of Lemma 1.58(viii), we derive det( ˆXT +
σXT ) = det( ˆX + σX) ̸= 0. For the proof of (1.169) (or (1.170)), we note that this
condition is equivalent to
X( ˆX + σX)T = X(Q + σI)XT ≥0
(X(Q + σI)XT ≤0),
where Q = QT solves the matrix equation XQXT = X ˆXT (see Remark 1.66).
So we see that for σ ≥−min1≤i≤n λi (or σ ≤−max1≤i≤n λi), where λi for i ∈
{1, . . ., n} are the eigenvalues of Q, condition (1.169) (or (1.170)) holds. The proof
is complete.
⊓⊔
1.6.3
Symplectic Matrices and Generalized LU Factorization
In this subsection we prove some results concerning the representation of a sym-
plectic matrix in the form of a product of lower (upper) block-triangular matrices
and orthogonal symplectic matrices. Matrices in Y ∈R2n×n satisfying (1.147)
frequently appear in the proofs of statements in this subsection. These matrices are
expressed in the form of a product of a lower block-triangular matrix, an orthogonal
matrix, and an invertible matrix. The results of this section play an important role in
the proofs of some properties of the comparative index introduced in Chap. 3
In the ﬁrst auxiliary statement, we present necessary and sufﬁcient conditions for
a matrix Y ∈R2n×n to satisfy (1.147).
Lemma 1.68 A 2n × n matrix Y =
X
U

satisﬁes (1.147) if and only if
XX†U(I −X†X) = 0,
(1.171)
Q = QT ,
Q := XX†UX†,
(1.172)
rank[(I −XX†) U(I −X†X)] = rank (I −X†X).
(1.173)
Moreover, if (1.147) holds, then we have
det L ̸= 0,
L := X −(I −XX†) U(I −X†X),
(1.174)
det M ̸= 0,
M := X −(I −XX†) U.
(1.175)

1.6
Linear Algebra and Matrix Analysis
63
Proof
(i) Sufﬁciency. Let (1.171)–(1.173) hold. We will show that (1.173) implies
(1.174), (1.175). In fact, using Remark 1.60(vii) we obtain
rank L = rankX + rank [(I −XX†) U(I −X†X)] = n,
which proves the invertibility of L in (1.174). As for the matrix M, we have
M = X −(I −XX†) U = X −(I −XX†) U(I −X†X) −(I −XX†) U(X†X)
= [I −(I −XX†) UX†] L,
where the matrix I −(I −XX†) UX† is invertible. This conclusion follows
from (1.163) with P := X† and R := (I −XX†). Hence, (1.175) holds. Now
by (1.171) we have
X = XX† M,
U = XX†U + (I −XX†) U = XX†UX†X + (I −XX†) U
(1.176)
= [XX†UX† −(I −XX†)] M.
Hence,
XT U = MT XX†UX† M = MT Q M.
(1.177)
Therefore, (1.172) implies that XT U = UT X, which is the same as Y TJ Y =
0. Furthermore,
Y T Y = XT X + UT U = MT [XX† + (I −XX†) + (XX†UX†)2] M
= MT (I + Q2) M.
The last matrix is invertible in view of (1.172). Indeed, Q2 = X†T UT XX†UX†
≥0 and hence, rankY = n, which completes the sufﬁciency part of the proof.
(ii) Necessity. The condition Y TJ Y = 0, i.e., XT U = UT X, implies
(X†)T XT U(I −X†X) = (X†)T UT X(I −X†X) = 0,
which means that (1.171) holds. Further,
Q = XX†UX† = (X†)T XT UX† = (X†)T UT XX† = (XX†UX†)T = QT ,

64
1
Motivation and Preliminaries
which implies that (1.172) holds. Concerning (1.173), we have
rank(XT X + UT U) (I −X†X) = rank(I −X†X) = rankUT U(I −X†X)
= rankUT (I −XX†) U(I −X†X)
≤rank(I −X†X) U(I −X†X) ≤rank(I −X†X),
where we have used that rank Y
= n = rank(XT X + UT U) and that
for a product of matrices rank AB ≤min{rankA, rankB}. Thus, we proved
(1.171), and the proof is completed.
⊓⊔
Remark 1.69 Recall (see Remark 1.66) that if Y ∈R2n×n satisﬁes (1.147), then the
matrix Q given in (1.172) is a symmetric solution of the equation
XT QX = XT U.
(1.178)
In particular, if X is invertible, then we have Q = UX−1.
We mentioned at the end of Sect. 1.6.1 the matrices, which belong to Sp(2n) ∩
O(2n). In the results of this subsection, we will need special matrices of this class,
which are constructed as follows. Let P ∈Rn×n. Then P is an orthogonal projector
(i.e., P is symmetric and P 2 = P) if and only if the 2n × 2n matrix
NP :=

P
I −P
−(I −P)
P

(1.179)
is orthogonal. In this case the matrix NP is also symplectic, i.e., it belongs to
Sp(2n) ∩O(2n).
In the theorem below, we will use the matrix NXX† deﬁned in (1.179) via the
projector P := XX†. It concerns a factorization of Y satisfying conditions (1.147)
and plays crucial role in proofs of Chap. 3.
Theorem 1.70 Let Y =
X
U

∈R2n×n satisfy (1.147), the matrix NXX† be given by
(1.179), and Q be a symmetric matrix satisfying (1.178). Then Y can be expressed
in the form
Y = L NXX†
M
0

,
L :=
 I 0
Q I

,
det M ̸= 0,
(1.180)
where
M := X −(I −XX†) (U −QX).
(1.181)

1.6
Linear Algebra and Matrix Analysis
65
Proof Observe that if a matrix Y satisﬁes (1.147), then in view of property (vii)
of Lemma 1.58, the matrix ˜Y = L−1Y =

X
U−QX

also satisﬁes (1.147). Then
it sufﬁces to apply Lemma 1.68 to ˜Y. Using (1.175), the matrix M in (1.181) is
invertible, X = XX†M, and by (1.178), we get
U −QX = (I −XX†) (U −QX) = −(I −XX†) M.
Then ˜Y = NXX†
M
0
, i.e., Y = L NXX†
M
0
, what we needed to prove.
⊓⊔
Remark 1.71 Formula (1.180) implies the representation for the blocks X and U in
the form
X = XX†M
(1.182)
U = [QXX† −(I −XX†)] M,
(1.183)
and
XT U = MT XX†UX†M,
(1.184)
where det M ̸= 0. In particular,
rankU = rankXT U + n −rankX,
(1.185)
because for the matrix Q := XX†UX† (see Remark 1.66), we have
UM−1 = XX†UX† −(I −XX†)
and then it follows that
rank U = rank(XX†UX†) + n −rankX,
rank XX†UX† = rankXT U.
Theorem 1.70 leads also to the following statement concerning a factorization of
symplectic matrices.
Lemma 1.72 Let Z =

X ˜X
U ˜U

∈Sp(2n) and Q be a symmetric solution of (1.178).
Then the matrix Z can be expressed in the form
Z = L NXX† diag{M, MT −1} H,
H :=
I ˜Q
0 I

,
(1.186)
where the matrices L, M, NXX† are given in Theorem 1.70 and
˜QT = ˜Q,
˜Q := ˜XT XX†( ˜U −Q ˜X) −( ˜U −Q ˜X)T (I −XX†) ˜X,
(1.187)
with ˜Q solving the equation X ˜QXT = X ˜XT .

66
1
Motivation and Preliminaries
Proof Using factorization (1.180) for Y =
X
U

, being the ﬁrst column of the matrix
Z, one can verify by a direct computations that
NT
XX† L−1Z =
M XX† ˜X −(I −XX†) ( ˜U −Q ˜X)
0
MT −1

= diag{M, MT −1} H,
where the matrices M and H are determined by (1.181) and (1.186) and
MT −1 = (I −XX†) ˜X + XX†( ˜U −Q ˜X).
(1.188)
Then, by (1.188), we have ˜Q = M−1[XX† ˜X −(I −XX†) ( ˜U −Q ˜X)], i.e., (1.187)
holds. Consequently, from the last equality, we get X ˜Q = XX†M ˜Q = XX† ˜X, so
that X ˜QXT = X ˜XT follows by the symmetry of ˜XXT ; see Lemma 1.58(iii).
⊓⊔
Under the additional assumption regarding the invertibility of the matrix X, we
obtain the following result concerning the n × n block LU-factorization of a sym-
plectic matrix. This result was for the ﬁrst time proved in [219, Proposition 2.36].
Corollary 1.73 Consider the symplectic matrix Z =

X ˜X
U ˜U

with invertible matrix
X. Then the factorization in (1.186) takes the form
Z =

I
0
UX−1 I
 X
0
0 XT −1
 I X−1 ˜X
0
I

.
(1.189)
The result in Corollary 1.73 justiﬁes that (1.180) and (1.189) can be called
a generalized block LU-factorization of Y and Z, respectively. Note that the
matrices L and NXX† in these formulas generally do not commute (they commute
if a solution of (1.178) is taken in the form (1.172)).
As a consequence of Corollary 1.73 and Lemma 1.58(viii), we obtain the next
statement concerning the factorization of a symplectic matrix. The proof (different
from ours) can be found in [332, Theorem 6.2].
Theorem 1.74 Every symplectic matrix Z ∈Sp(2n) can be expressed as the
product
Z = H1 L2 H3 = J L1 J L2 J L3 J ,
where H1 and H3 are upper block-triangular symplectic matrices and L1, L2, and
L3 are lower block-triangular symplectic matrices.
Proof Put H1 =
 I −σI
0
I

. Then by Lemma 1.58(viii) for Y := Z
I
0

=:
X
U

one
can choose σ ∈R such hat (1.150) holds. Hence, the matrix (I 0)H −1
1 Z
I
0

is
invertible, and by Corollary 1.73, there exists a block LU-factorization of the matrix
H −1
1 Z = L2H3. The multiplication of this equality by H1 proves the theorem.
⊓⊔

1.6
Linear Algebra and Matrix Analysis
67
1.6.4
Symplectic Matrices Depending on Parameter
In this section we consider symplectic matrices W(λ) depending on a parameter
λ ∈R. We assume that W(λ) is piecewise continuously differentiable in λ ∈R,
i.e., it is continuous on R with the piecewise continuous derivative. Then we can
introduce the following matrix
(W(λ)) = −J d
dλ(W(λ)) W −1(λ) = J d
dλ(W(λ)) J W T (λ) J
(1.190)
deﬁned for any piecewise continuously differentiable symplectic matrix W(λ). The
ﬁrst result concerning the matrix (W(λ)) is the following proposition.
Proposition 1.75 A matrix function W : R →R2n×2n of the real variable λ is
symplectic for all λ ∈R if and only if W(0) is symplectic and
˙W(λ) = J H(λ) W(λ),
λ ∈R,
(1.191)
with a symmetric matrix H(λ) for all λ ∈R.
Proof The proof can be found in [216, pg. 229]. First note that (1.191) is the
Hamiltonian differential system (1.103), where we replace the variable t by the
variable λ. It is well known (see [328] or [205]) that under the assumption that
W(0) is symplectic for the fundamental matrix W(t) of (1.191), it follows that
W(λ) ∈Sp(2n) for λ ∈R. In the opposite direction, differentiating the identity
W T (λ)J W(λ) = J with respect to λ, we get
d
dλ(W T (λ) J W(λ)) = 0 = ˙W T (λ) J W(λ) + W T (λ) J ˙W(λ),
then, multiplying the previous identity by W −1 T (λ) from the left side and by
W −1(λ) from the right side, we derive the desired identity
H(λ) = J T ˙W(λ) W −1(λ) = W T −1(λ) ˙W T (λ) J = (W(λ)).
(1.192)
One can now see the symmetry of H(λ) = (W(λ)) directly from (1.192).
⊓⊔
In particular, in terms of the blocks A(λ), B(λ), C(λ), D(λ) of the symplectic
matrix W(λ) =

A(λ) B(λ)
C(λ) D(λ)

the operator (1.190) takes the form
(W(λ)) =
 ˙D(λ) CT (λ) −˙C(λ) DT (λ) ˙C(λ) BT (λ) −˙D(λ) AT (λ)
˙A(λ) DT (λ) −˙B(λ) CT (λ) ˙B(λ) AT (λ) −˙A(λ) BT (λ)

.
(1.193)

68
1
Motivation and Preliminaries
The proof is based on direct computations incorporating the identities
AT (λ) C(λ) = CT (λ) A(λ),
BT (λ) D(λ) = DT (λ) B(λ),
A(λ) BT (λ) = B(λ) AT (λ),
D(λ) CT (λ) = C(λ) DT (λ),
AT (λ) D(λ) −CT (λ) B(λ) = I,
D(λ) AT (λ) −C(λ) BT (λ) = I.
⎫
⎪⎬
⎪⎭
which hold due to the symplecticity of W(λ). Then upon differentiating the above
formulas with respect to λ, we get
˙AT (λ) C(λ) + AT (λ) ˙C(λ) = ˙CT (λ) A(λ) + CT (λ) ˙A(λ),
˙BT (λ) D(λ) + BT (λ) ˙D(λ) = ˙DT (λ) B(λ) + DT (λ) ˙B(λ),
˙AT (λ) D(λ) + AT (λ) ˙D(λ) = ˙CT (λ) B(λ) + CT (λ) ˙B(λ),
˙D(λ) CT (λ) + D(λ) ˙CT (λ) = ˙C(λ) DT (λ) + C(λ) ˙DT (λ),
˙A(λ) BT (λ) + A(λ) ˙BT (λ) = ˙B(λ) AT (λ) + B(λ) ˙AT (λ),
˙D(λ) AT (λ) + D(λ) ˙AT (λ) = ˙C(λ) BT (λ) + C(λ) ˙BT (λ).
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(1.194)
Now we are prepared to formulate the main results of this section concerning
properties of the symmetric operator (1.190).
Proposition 1.76 The following statements hold.
(i) For arbitrary piecewise continuously differentiable symplectic matrices W(λ)
and V (λ), we have
(V (λ)W(λ)) = (V (λ)) + V T −1(λ) (W(λ)) V −1(λ).
(1.195)
(ii) If W(λ) is piecewise continuously differentiable, then
(W −1(λ)) = −W T (λ) (W(λ)) W(λ).
(1.196)
(iii) For arbitrary constant symplectic matrices R and P, we have
(R−1W(λ)P) = RT (W(λ)) R.
(1.197)
(iv) For arbitrary piecewise continuously differentiable symplectic matrix W(λ)
and constant symplectic matrices R and P, we have
(W(λ)) ≥0 ⇔(R−1W(λ) P) ≥0 ⇔(W −1(λ)) ≤0
⇔(W T (λ)) ≤0
⇔(W T −1(λ)) ≥0.

(1.198)

1.6
Linear Algebra and Matrix Analysis
69
Proof Applying (1.192) to the product V (λ)W(λ), we have
(V (λ) W(λ)) = J T d
dλ(V (λ) W(λ)) (V (λ) W(λ)))−1
= J T ( ˙V (λ) W(λ) + V (λ) ˙W(λ)) W −1(λ) V −1(λ)
= J T ˙V (λ) V −1(λ) + J T V (λ) ˙W(λ) W −1(λ) V −1(λ)
= (V (λ)) + V T −1(λ) (W(λ)) V −1(λ),
where we used J T V (λ) = V T −1(λ) J T according to Lemma 1.58(ii). So we have
proved (i). For the proof of property (ii), we put V (λ) := W −1(λ) in property (i)
and then use that (W −1(λ) W(λ)) = (I) = 0. The proof of (iii) is based on the
subsequent application of property (i). So we have
((R−1W(λ)) P) = (R−1W(λ)) = RT (W(λ))R.
Property (iv) is the direct consequence of (iii), (ii), and Lemma 1.58(ii), when
property (iii) is applied to the cases W T = J T W −1J and W T −1 = J T WJ .
⊓⊔
Another important consequence from Proposition 1.76 are presented by the
following lemma.
Lemma 1.77 Assume that for a piecewise continuously differentiable symplectic
matrix W(λ), there exist constant symplectic matrices R and P such that
W(λ) = R ˜W(λ) P −1, ˜W(λ) =
 ˜A(λ) ˜B(λ)
˜C(λ) ˜D(λ)

, det ˜B(λ) ̸= 0, λ ∈R.
(1.199)
Then we have (omitting the argument λ)
( ˜W) = −J
 ˜B 0
˜D I
 d
dλ
 ˜B−1 ˜A −˜B−1
−˜BT −1 ˜D ˜B−1
  ˜B 0
˜D I
T
J T ,
(1.200)
and then the condition
(W(λ)) ≥0,
λ ∈R
(1.201)
is equivalent to
d
dλ
˜Q(λ) ≤0,
˜Q(λ) =
 ˜B−1(λ) ˜A(λ)
−˜B−1(λ)
−˜BT −1(λ)
˜D(λ) ˜B−1(λ)

,
λ ∈R.
(1.202)
Proof By Proposition 1.76(iv) we see that ( ˜W(λ)) ≥0, and one can verify
by direct computations (applying Proposition 1.76(i)) that representation (1.200)

70
1
Motivation and Preliminaries
holds. Then, using the nonsingularity of ˜B(λ), we prove that (1.201) is equivalent
to (1.202). The proof is completed.
⊓⊔
In the following results, we will use Lemma 1.77 locally, i.e., in a sufﬁciently
small neighborhood of λ0 ∈R. So we have the following property.
Corollary 1.78 Assume that a piecewise continuously differentiable symplectic
matrix W(λ) =

A(λ) B(λ)
C(λ) D(λ)

obeys (1.201) and for some λ0 ∈R, we have
det A(λ0) ̸= 0.
(1.203)
Then there exists ε > 0 such that det A(λ) ̸= 0 for λ ∈(λ0 −ε, λ0 + ε) and
d
dλ(A−1(λ) B(λ)) ≥0,
d
dλ(C(λ) A−1(λ)) ≤0,
λ ∈(λ0 −ε, λ0 + ε).
(1.204)
Similarly, if for some λ0 ∈R
det D(λ0) ̸= 0,
(1.205)
then there exists ε > 0 such that det D(λ) ̸= 0 for λ ∈(λ0 −ε, λ0 + ε) and
d
dλ(B(λ) D−1(λ)) ≥0,
d
dλ(D−1(λ) C(λ)) ≤0,
λ ∈(λ0 −ε, λ0 + ε).
(1.206)
Proof Putting R := I and P := J in Lemma 1.77, we see that ˜W(λ) = W(λ) J
has the form
˜W(λ) =

−B(λ) A(λ)
−D(λ) C(λ)

, and then by (1.202), we derive (1.204).
Similarly, if R := J T and P = I in Lemma 1.77, then ˜W(λ) = J W(λ) =

C(λ)
D(λ)
−A(λ) −B(λ)

. Substituting the blocks of ˜W(λ) into (1.202), we derive (1.206).
⊓⊔
The following theorem is the most important result of this section.
Theorem 1.79 Assume that W(λ) =

A(λ) B(λ)
C(λ) D(λ)

is piecewise continuously differ-
entiable on R and obeys assumption (1.201). Then Ker B(λ) is piecewise constant
in λ, i.e., for any λ0 ∈R there exists δ > 0 such that
Ker B(λ) ≡Ker B(λ−
0 ) ⊆Ker B(λ0)
for all λ ∈(λ0 −δ, λ0),
(1.207)
Ker B(λ) ≡Ker B(λ+
0 ) ⊆Ker B(λ0)
for all λ ∈(λ0, λ0 + δ),
(1.208)

1.6
Linear Algebra and Matrix Analysis
71
Proof The proof is based on Corollary 1.78 and Lemma 1.67. Putting Z := W(λ0)
in Lemma 1.67, we have that there exist σ > 0 such that for the blocks of
˜W := W(λ0)
 I 0
σI I

=
A(λ0) + σB(λ0) B(λ0)
C(λ0) + σD(λ0) D(λ0)

we have det [A(λ0) + σB(λ0)] ̸= 0 and
(A(λ0) + σB(λ0))−1B(λ0) ≤0.
(1.209)
Then we apply Corollary 1.78 to the matrix ˜W(λ) = W(λ)
 I
0
σI I

=
 ˜A(λ) B(λ)
˜C(λ) D(λ)

.
Since W(λ) obeys (1.201), i.e., (W(λ)) ≥0, then the same condition holds for
˜W(λ) by Proposition 1.76(iv). Moreover, the matrix ˜W(λ) obeys the assumption
det ˜A(λ0) ̸= 0. Applying Corollary 1.78 we see that there exist ε > 0 such that
˜A−1(λ) B(λ) is nondecreasing matrix function for λ ∈(λ0 −ε, λ0 + ε). Choose
c ∈Ker B(λ) for some λ ∈(λ0 −ε, λ0). Then the monotonicity of ˜A−1(λ) B(λ)
and (1.209) imply
0 = cT ˜A−1(λ) B(λ) c ≤cT ˜A−1(ν) B(ν) c ≤cT ˜A−1(λ0) B(λ0) c ≤0
for all ν ∈[λ, λ0]. Hence, cT ˜A−1(ν) B(ν) c = 0 and so c ∈Ker B(ν) for every
ν ∈[λ, λ0]. Therefore, Ker B(λ) ⊆Ker B(ν) for all λ, ν ∈(λ0 −ε, λ0] with λ ≤ν.
This means that the set Ker B(λ) is nondecreasing in λ on (λ0 −ε, λ0]. This implies
that condition (1.207) is satisﬁed for some sufﬁciently small δ ∈(0, ε). For (1.208)
we proceed in the same way except that we choose σ according to Lemma 1.67 such
that
(A(λ0) + σB(λ0))−1B(λ0) ≥0.
(1.210)
So we have proved that Ker B(λ) is piecewise constant in λ ∈R.
⊓⊔
Remark 1.80 The assertions of Theorem 1.79 hold true if we replace (1.201) by the
monotonicity assumption
(W(λ)) ≤0, λ ∈R.
(1.211)
Indeed, in the proof of Theorem 1.79, we used Proposition 1.76(iv) and Corol-
lary 1.78, where the replacement of (1.201) by (1.211) derives the respective
replacements of all signs ≥and ≤by the opposite signs ≤and ≥. In particular, in
this case we have (see the proof of Theorem 1.79) that ˜A−1(λ) B(λ) is nonincreasing
matrix function for λ ∈(λ0 −ε, λ0 + ε). Then, under assumption (1.211) we prove
(1.208) repeating the proof of (1.207) under assumption (1.201). Similarly, we use
the proof of (1.208) under assumption (1.201) to prove (1.207) using (1.211).
Based on Remark 1.80, we prove another important fact connected with (1.201).

72
1
Motivation and Preliminaries
Theorem 1.81 Assume that W(λ) =

A(λ) B(λ)
C(λ) D(λ)

is piecewise continuously differ-
entiable on R and obeys assumption (1.201). Then Im B(λ) is piecewise constant in
λ, i.e., for any λ0 ∈R there exists δ > 0 such that
Im B(λ0)⊆Im B(λ) ≡Im B(λ−
0 )
for all λ ∈(λ0 −δ, λ0),
(1.212)
Im B(λ0)⊆Im B(λ) ≡Im B(λ+
0 )
for all λ ∈(λ0, λ0 + δ).
(1.213)
Proof To prove the result, we note that (1.201) is equivalent to
(W −1(λ)) ≤0,
λ ∈R,
(1.214)
by Proposition 1.76(iv) and by Lemma 1.58(ii)
W −1(λ) =
 DT (λ) −BT (λ)
−CT (λ) AT (λ)

.
Then, by Remark 1.80 we can apply Theorem 1.79 to W −1(λ) which implies that
Ker BT (λ) is piecewise constant in λ, i.e., for any λ0 ∈R there exists δ > 0 such
that
Ker BT (λ) ≡Ker BT (λ−
0 ) ⊆Ker BT (λ0)
for all λ ∈(λ0 −δ, λ0),
(1.215)
Ker BT (λ) ≡Ker BT (λ+
0 ) ⊆Ker BT (λ0)
for all λ ∈(λ0, λ0 + δ).
(1.216)
Conditions (1.215), (1.216) are equivalent to (1.212), (1.213), because Ker BT (λ) is
the orthogonal complement to Im B(λ).
⊓⊔
Now we formulate several corollaries to Theorems 1.79, and 1.81.
Theorem 1.82 Assume (1.201) (or (1.211)). Then the following three conditions
are equivalent:
rank B(λ) is constant for λ ∈R,
(1.217)
Ker B(λ) is constant for λ ∈R,
(1.218)
Im B(λ) is constant for λ ∈R.
(1.219)
Proof It is clear that the constancy of Ker B(λ) in λ ∈R (or Im B(λ) in λ ∈R)
implies condition (1.217). Conversely, assume (1.217). Then, by Theorem 1.79 we
have Ker B(λ±
0 ) ⊆Ker B(λ0), which implies Ker B(λ±
0 ) = Ker B(λ0) for any λ0 ∈
R. Similarly, by Theorem 1.81, (1.217) and the inclusion Im B(λ0) ⊆Im B(λ±
0 )
implies the equality Im B(λ0) = Im B(λ±
0 ) for any λ0 ∈R.
⊓⊔
Based on Theorems 1.79 and 1.81 and by Lemma 1.63, we also have the
following corollary.

1.6
Linear Algebra and Matrix Analysis
73
Corollary 1.83 Assume (1.201) (or (1.211)). Then, for any λ0 ∈R there exists
δ > 0 such that the matrices B†(λ) B(λ) and B(λ) B†(λ) are constant for λ ∈
(λ0 −δ, λ0). A similar assertion holds also for λ ∈(λ0, λ0 + δ).
Remark 1.84 We note that by Proposition 1.76(iv), all the monotonicity properties
of (W(λ)) formulated in Theorems 1.79, 1.81, and 1.82 and in Corollary 1.83 hold
also for (R−1W(λ) P), where R and P are constant symplectic matrices. These
properties are satisﬁed for all blocks of W(λ) and their linear combinations.
1.6.5
Monotone Matrix-Valued Functions
In this subsection we present two useful results about the behavior of symmetric
monotone matrix-valued functions. The ﬁrst theorem describes the change in the
index (i.e., the number of negative eigenvalues) of a monotone matrix-valued
function when its argument crosses a singularity. This result will be utilized in
Sect. 5.1 in order to derive oscillation theorems for discrete eigenvalue problems
for symplectic difference systems.
We use a standard monotonicity deﬁnition for symmetric matrix-valued func-
tions, i.e., a matrix-valued function A
:
(0, ε)
→
Rn×n is nonincreasing
(nondecreasing) on (0, ε), if the scalar function dT A(t) d is nonincreasing (non-
decreasing) on (0, ε) for every d ∈Rn. Moreover, the notation f (0+) and f (0−)
stands for the right-hand and left-hand limits of the function f (t) at t = 0.
Theorem 1.85 (Index Theorem) Let X(t), U(t), R1(t), R2(t) be given real m×m-
matrix-valued functions on [0, ε) such that
R1(t) RT
2 (t) and XT (t) U(t) are symmetric,
rank(R1(t), R2(t)) = rank(XT (t), UT (t)) = m

for t ∈[0, ε),
(1.220)
and assume that X(t), U(t), R1(t), and R2(t) are continuous at 0, i.e.,
lim
t→0+ R1(t) = R1 := R1(0), lim
t→0+ X(t) = X := X(0),
lim
t→0+ R2(t) = R2 := R2(0), lim
t→0+ U(t) = U := U(0),
⎫
⎬
⎭
(1.221)
and that X(t) is invertible for t ∈(0, ε). Moreover, denote
M(t) := R1(t) RT
2 (t) + R2(t) U(t) X−1(t) RT
2 (t),
(t) := R1(t) X(t) + R2(t) U(t),
 := (0)
S(t) := X†RT
2 (t),
S := S(0),
S∗(t) := RT
2 (t) −XS(t) = (I −XX†) RT
2 (t),
S∗:= S∗(0),
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(1.222)

74
1
Motivation and Preliminaries
and suppose that the functions U(t) X−1(t) and M(t) are either both nonincreasing
or both nondecreasing on (0, ε) and that
rankR2(t) ≡rankR2
and
rankS∗(t) ≡rankS∗=: m −r
(1.223)
are constant on [0, ε). Finally, let T ∈Rm×r be such that
rankT = r, T T T = Ir×r, Im T = Ker S∗, and Q := T T ST ∈Rr×r.
(1.224)
Then the matrix Q is symmetric, and ind M(0+), ind M(0−), and def (0+) exist
with
ind M(0+) = ind Q + m −rank T + def  −def (0+) −def X
(1.225)
if M(t) and U(t) X−1(t) are nonincreasing on (0, ε) and
ind M(0+) = ind Q + m −rankT
(1.226)
if M(t) and U(t) X−1(t) are nondecreasing on (0, ε).
Proof We refer to [210, Theorem 2.1].
⊓⊔
The most signiﬁcant disadvantage of condition (1.223) is that it depends also on
the matrix X = X(0), which makes it “not really” suitable in practical applications.
The following special case of Theorem 1.85 removes this disadvantage, since the
crucial assumption is formulated only in terms of R2(t).
Corollary 1.86 (Index Theorem) With the notation (1.220)– (1.222) and the
assumptions of Theorem 1.85, suppose that
Im RT
2 (t) ≡Im RT
2
is constant on [0, ε)
(1.227)
instead of (1.223). Then the assertions (1.225) and (1.226) of Theorem 1.85 hold.
Proof If Im RT
2 (t) is constant on [0, ε), then (1.223) is trivially satisﬁed. The
statement then follows from Theorem 1.85. Observe that this constant image
assumption does not depend on X, while of course (1.223) depends in general on
R2(t) and also on X.
⊓⊔
Next we discuss a limit theorem for symmetric monotone matrix-valued func-
tions, which concerns invertible matrices.
Proposition 1.87 (Limit Theorem) Let S(t) be a real, symmetric, positive deﬁnite,
and increasing m × m matrix-valued function on [a, ∞). Then the limit X :=
limt→∞S−1(t) exists; X is symmetric and positive semideﬁnite, and
lim
t→∞XS(t) X = X.
(1.228)

1.6
Linear Algebra and Matrix Analysis
75
Proof The result follows from a general Limit Theorem in [205, Theorem 3.3.7], in
which we take X(t) := S(1/t) and U(t) ≡I on (0, ε] with ε := 1/a (without loss
of generality, we assume that a > 0).
⊓⊔
We extend the above result to noninvertible matrices S(t).
Theorem 1.88 (Limit Theorem) Let S(t) be a real symmetric, positive semideﬁ-
nite, and nondecreasing m × m matrix-valued function on the interval [a, ∞). Then
the limit X := limt→∞S†(t) exists; X is symmetric and positive semideﬁnite, and
equality (1.228) holds.
Proof The monotonicity of S(t) implies that Ker S(t) is constant on some interval
[α, ∞) with α ≥a. Let ℓ:= rank S(t) and k := def S(t) for t ∈[α, ∞), so that
ℓ+k = m. Choose K ∈Rm×k and L ∈Rm×ℓsuch that Ker S(t) ≡Im K on [α, ∞)
and the matrix V := (K, L) ∈Rm×m is orthogonal. Then for all t ∈[α, ∞)
V T S(t) V =
0
0
0 ˜S(t)

,
S(t) = V
0
0
0 ˜S(t)

V T,
(1.229)
where the ℓ× ℓmatrix-valued function ˜S(t) := LT S(t) L is symmetric, positive
deﬁnite, and increasing on [α, ∞). By Remark 1.60(ii) we get from (1.229) that
S†(t) = V
0
0
0 ˜S−1(t)

V T,
X := V
0 0
0 ˜X

V T,
(1.230)
where X = limt→∞S†(t) and where the matrix ˜X := limt→∞˜S−1(t) exists
by Proposition 1.87. Moreover, the matrices ˜X and X are symmetric and positive
semideﬁnite, and limt→∞˜X ˜S(t) ˜X = ˜X holds. From (1.229) and (1.230), we get
lim
t→∞XS(t) X = lim
t→∞V
0
0
0 ˜X ˜S(t) ˜X

V T = V
0 0
0 ˜X

V T = X,
which completes the proof.
⊓⊔
1.6.6
Miscellaneous Topics from Matrix Analysis
In this subsection we collect various result from matrix analysis, which will be
needed in the subsequent chapters of this book. The ﬁrst result is a generalization
of the statement that every symmetric matrix G is a limit of a sequence of invertible
symmetric matrices Gν; compare with [70, pg. 40]. In the present context, the
matrices Gν are no longer invertible, but their image is equal to the image of some
ﬁxed orthogonal projector.

76
1
Motivation and Preliminaries
Lemma 1.89 Let G ∈Rn×n be a symmetric matrix, and let Q be an orthogonal
projector with Im G ⊆Im Q. Then there exists a sequence {Gν}∞
ν=1 of symmetric
matrices such that Im Gν = Im Q for all ν ∈N and Gν →G as ν →∞.
Proof Let g := rankG and q := rankQ, so that g ≤q. If g = q, then Im G =
Im Q, and we may take the constant sequence Gν := G for all ν ∈N. Suppose
now that g < q. Then there exists an orthogonal matrix V ∈Rn×n such that its
ﬁrst g columns form an orthonormal basis of Im G and at the same time its ﬁrst q
columns form an orthonormal basis of Im Q. This means that we have the equalities
V T GV = diag{g, 0n−g} and V T Q V = diag{q, 0n−q}, where g ∈Rg×g and
q ∈Rq×q are symmetric and nonsingular. Since Q is an orthogonal projector,
Q2 = Q. It follows that 2
q = q, which implies that q = Iq. Therefore, we have
G = V diag{g, 0n−g}V T and Q = V diag{Iq, 0n−q}V T . Consider the sequence
{Gν}∞
ν=1 of matrices, where
Gν := V
⎛
⎝
g
0
0
0
1
ν Iq−g
0
0
0
0n−q
⎞
⎠V T
for all ν ∈N.
(1.231)
It is obvious that for each ν ∈N, the matrix Gν is symmetric and QGν = Gν. And
since rank Gν = q, we have Im Gν = Im Q. Moreover, from (1.231) it follows that
limν→∞Gν = G, which completes the proof.
⊓⊔
Next we present some special properties of orthogonal projectors. We recall that
every orthogonal projector is a diagonalizable matrix (being symmetric) with the
spectrum consisting of only two values 0 and 1. More precisely, if P ∈Rn×n is
an orthogonal projector and p := rank P, then there exists an n × n orthogonal
matrix V such that
P = V diag{Ip, 0n−p} V T .
(1.232)
Lemma 1.90 Let P∗∈Rn×n be an orthogonal projector with p∗:= rank P∗, and
let V∗∈Rn×n be the corresponding orthogonal matrix from (1.232), i.e.,
P∗= V∗diag{Ip∗, 0n−p∗} V T
∗.
(1.233)
Let p ∈N satisfy p∗≤p ≤n. Then P ∈Rn×n is an orthogonal projector with
Im P∗⊆Im P
and
rankP = p
(1.234)
if and only if P has the form
P = V∗diag{Ip∗, R∗} V T
∗,
(1.235)
where R∗∈R(n−p∗)×(n−p∗) is an orthogonal projector with rank equal to p −p∗.

1.6
Linear Algebra and Matrix Analysis
77
Proof It is easy to see that every matrix P of the form (1.235) is symmetric
and idempotent (i.e., it is an orthogonal projector) and (1.234) holds. Conversely,
suppose that P ∈Rn×n is an orthogonal projector satisfying (1.234). Then we may
write
P = V∗
K∗L∗
LT
∗R∗

V T
∗,
(1.236)
where K∗∈Rp∗×p∗and R∗∈R(n−p∗)×(n−p∗) are symmetric and L∗∈Rp∗×(n−p∗).
The ﬁrst condition in (1.234) is equivalent with the equality PP∗= P∗, from which
we obtain by using the representations in (1.233) and (1.236) that K∗= Ip∗and
L∗= 0p∗×(n−p∗). Thus, P has the form in (1.235), where rank R∗is equal to
rankP −p∗= p −p∗according to (1.234). Finally, the idempotence of P now
implies the idempotence of R∗, showing that R∗is an orthogonal projector.
⊓⊔
Theorem 1.91 Let P∗, P, ˜P ∈Rn×n be orthogonal projectors satisfying
Im P∗⊆Im P,
Im P∗⊆Im ˜P ,
rankP = rank ˜P .
(1.237)
Then there exists an invertible matrix E ∈Rn×n such that
EP∗= P∗
and
Im EP = Im ˜P .
Proof Let p∗:= rankP∗and p := rankP = rank ˜P. Then obviously p ≥p∗.
Let V∗∈Rn×n be the orthogonal matrix in (1.232) associated with projector P∗,
that is, (1.233) holds. According to Lemma 1.90, there exist orthogonal projectors
R∗, ˜R∗∈R(n−p∗)×(n−p∗) such that
P = V∗diag{Ip∗, R∗} V T
∗,
˜P = V∗diag{Ip∗, ˜R∗} V T
∗,
(1.238)
and rank R∗= rank ˜R∗= p −p∗. Let Z∗, ˜Z∗∈R(n−p∗)×(n−p∗) be orthogonal
matrices in (1.232) associated with the projectors R∗and ˜R∗, that is, we have R∗=
Z∗diag{Ip−p∗, 0n−p} ZT
∗and ˜R∗= ˜Z∗diag{Ip−p∗, 0n−p} ˜ZT
∗. It follows that
˜Z∗ZT
∗R∗= ˜Z∗diag{Ip−p∗, 0n−p} ZT
∗= ˜R∗˜Z∗ZT
∗.
(1.239)
We set E := V∗diag{Ip∗, ˜Z∗ZT
∗} V T
∗
∈Rn×n. Then E is nonsingular and from
(1.233) it follows that EP∗= P∗. Finally, by (1.238) and (1.239), we obtain
EP = V∗diag{Ip∗, ˜Z∗ZT
∗R∗} V T
∗= V∗diag{Ip∗, ˜R∗˜Z∗ZT
∗} V T
∗= ˜P E,
which shows that Im EP = Im ˜P E = Im ˜P. The proof is complete.
⊓⊔

78
1
Motivation and Preliminaries
1.7
Notes and References
Concerning the oscillatory and spectral properties of Sturm-Liouville difference
equations (1.1), this topic is treated in detail in [204]. A comprehensive treat-
ment of oscillation theory of various difference equations and systems, including
an introduction to oscillation theory of symplectic difference systems, is presented
in [4]. In particular, [4, Section 2.9] contains a relevant list of references for the
oscillation theory of the second-order Sturm-Liouville difference equation (1.1).
Other monographs devoted to various aspects of linear difference equations are
[2, 110] and also the paper [169]. The Leighton-Wintner oscillation criterion
(Theorem 1.4) is based on the second mean value theorem of summation calculus
(a discrete analog of the second mean value theorem of integral calculus) proven
in [80, Lemma 3.2]. Applications of this technique were developed, e.g., in [82,
Theorem 2] and [259, Theorem 4]. The discrete Prüfer transformation presented in
Sect. 1.2.4 was obtained in [47], and the oscillation theorems in Sect. 1.2.5 were
derived in [298].
The ﬁrst-order optimality conditions in Theorem 1.26 can be proven via the
mathematical programming approach as presented in [62, 178, 179, 223] or via the
variational approach as in [3, 177] and [16, Chapter 4]; see also the scalar case (i.e.,
n = 1) in [204, Chapter 8]. An overview of the second-order optimality conditions
for discrete calculus of variations problem (1.51), including a historical development
of these conditions, is presented in the survey paper [186]. Further necessary
and sufﬁcient conditions for discrete calculus of variations problems with variable
endpoints in terms of coupled intervals are derived in [180]. In the discrete optimal
control setting, such conditions are presented in [177–179, 184, 225–227, 330].
The symplectic structure of the Jacobi equations (1.63) and (1.69) discussed in
Theorems 1.29 and Remark 1.30 is derived in [294]; see also [303, Corollaries 5.2
and 5.8]. The discrete calculus of variations problem (1.65) without the shift in the
state variable is analyzed in [294, Section 2].
The discrete weak Pontryagin (maximum) principle in Theorem 1.32 can be
proven via the mathematical programming method (i.e., the Lagrange multipliers
rule) as presented in [178, 181, 223] or via a variational method (based on a general-
ized DuBois-Reymond lemma) as presented in [192]. These references also contain
a more general optimal control problem (1.70) involving the pointwise equality
control constraints. Problems with state inequality constraints are considered, e.g.,
in [225–227]. The symplectic structure of the Jacobi system for the discrete optimal
control problem (1.70), in which only the matrix Sk in (1.83) is invertible while Rk
may be singular, is proven in [303]. The matrix inversion formula (1.87) is from
[146]. Optimal control problems with and without a shift in the state variable are
studied in [192], where it is also shown that they can transformed one to another by
using the implicit function theorem. Symplectic difference systems were recently
applied in [331] to study constrained linear-quadratic control problems with and
without shift in their data.

1.7
Notes and References
79
The literature related to the symplectic phase ﬂow in Hamiltonian mechanics is
given at particular places Sect. 1.4. Let us mention here at least the books [16, 27]
and the paper [8].
Classical qualitative theory of linear Hamiltonian differential systems (1.103)
is developed in the books [28, 70, 205, 248, 250]. The trigonometric and Prüfer
transformations for linear Hamiltonian systems are discussed in [32, 77, 92, 93,
245, 247, 250]. In particular, Theorem 1.51 is the ﬁrst part of [247, Theorem 4.1],
while Theorems 1.52, 1.53 are proved in [77, Theorems 1, 3]. Properties of
principal and nonprincipal solutions for completely controllable system (1.103) are
proven in [6, 7, 78, 79, 246], as well as in the above general references on linear
Hamiltonian systems. An overview of applications of principal solutions of (1.103)
at inﬁnity is also presented in [283]. The oscillation and eigenvalue theory of these
systems without the complete controllability (or identical normality assumption)
was initiated in [207] and further developed in [209, 295, 296, 321]. In particular,
the theory of principal and antiprincipal solutions of possibly abnormal linear
Hamiltonian systems (1.103) is developed in [283, 285–290, 293]. Uncontrollable
systems (1.103) were also considered in [138, 200–203] in the relation with
the notion of a weak disconjugacy of (1.103) and dissipative control processes.
Applications of the theory of comparative index to linear Hamiltonian systems are
derived in [127, 129, 130, 289, 293]. A generalization of the oscillation theorem
(Theorem 1.56) for linear Hamiltonian systems (1.140) under no strict normality
assumption is proven in [57]. A theory of Riccati matrix differential equations
for linear Hamiltonian systems without the controllability assumption was recently
developed in [282].
As we mentioned in Sect. 1.6.1, properties of symplectic matrices are discussed,
e.g., in [27, 139, 216, 328, 332] and in the papers [35, 74, 75, 219, 222, 224, 326,
327]. We note that there is also a notion of a complex symplectic matrix (symplectic
matrix with complex entries), which is sometimes called a conjugate symplectic
matrix. In the complex case, some of the properties of symplectic matrices remain
the same, but some other are slightly different. We refer to the abovementioned
literature for a comparison. The theory of Moore-Penrose pseudoinverses and its
properties is presented, e.g., in the books [34, 36, 64, 205]. In particular, the
limit result for sequences of Moore-Penrose pseudoinverses in Remark 1.60(v)
is from [64, Theorem 10.4.1]. The inequality in Remark 1.60(vi) is proven in
[170, Lemma 1]; see also [36, Facts 8.15.7 and 8.15.5]. For the properties in
Remark 1.60(vii), we refer to [36, Facts 6.4.32 and 2.10.8]. The statement in
Lemma 1.61 is proven in [283, Corollary 10.5 and Lemma 10.4]; Lemma 1.64 is
derived in [113, Lemma 2.4 and Remark 2.5]. The Moore-Penrose pseudoinverses
are efﬁciently used in the relation with orthogonal projectors. Applications of
this type in symplectic difference systems are contained in the recent papers
[284, 290, 292].
The results of the auxiliary Lemma 1.67 also follow from [205, Theorem 3.1.2
and Corollary 3.3.9].
Much of matrix factorization theory comes from numerical linear algebra.
Results concerning factorization of symplectic matrices play an important role in

80
1
Motivation and Preliminaries
applications; see [35, 74, 75, 139, 219, 222, 224, 234, 239] and the references given
therein. Several special types of symplectic factorizations are highly important for
Sturmian theory of discrete symplectic systems as well. For example, the solvability
of the discrete matrix Riccati equation (2.52) is equivalent to the existence the
symplectic block LU factorization for fundamental solution matrices Zk of (SDS),
see [219]. The trigonometric transformations considered in Sect. 2.6 are based on
the symplectic block QR factorization from [63] for ZT
k , where Q is a symplectic
orthogonal matrix and R is a symplectic upper block-triangular matrix of the
form (1.154). The symplectic singular value decomposition (SVD), see [239,
Theorem 2.1], is a basic tool of the oscillation theory for discrete trigonometric
systems (2.146) in [96]. The results of Sect. 1.6.3 related to the so-called generalized
LU factorizations of symplectic matrices present another example of applications
of the factorization methods in the discrete oscillation theory.
Several statements of Lemma 1.68 were proven originally in [114, Lemma 2.1].
In particular, it was shown that the conditions in (1.147) are sufﬁcient for (1.171) and
(1.173)–(1.175). The result of Theorem 1.70 is a special case of [114, Theorem 3.1],
which is proved under more general assumptions. Consider symplectic and orthog-
onal matrices NP given by (1.179). Then [114, Theorem 3.1] states that a 2n × n
matrix Yk is a conjoined basis of (SDS) if and only if there exists an orthogonal
projector Pk, a lower block-triangular symplectic matrix Lk, and a nonsingular n×n
matrix Mk such that
Yk = LkNPk
Mk
0

,
L−1
k+1SkLk = NPk+1HkNT
Pk,
where Hk(I 0)T =
Mk+1M−1
k
0

. In particular, the matrix Pk can be chosen in the
form Pk = I −XkX†
k assumed in Theorem 1.70. Other special cases of the matrices
NPk are considered in [112, 113]. Also, it should be pointed out that the results of
Lemma 1.68 and Theorem 1.70 imply some special properties of matrices Y with
conditions (1.147) proved in [205]. For instance, formula (1.185) was derived for
the ﬁrst time in [205, Theorem 3.1.2(iii)].
The results of Sect. 1.6.4 are closely related to the oscillation theory of the
differential Hamiltonian systems (1.99) where t := λ. This relation is illustrated
by Proposition 1.75. In this connection, a part of the results in Sect. 1.6.4 are well-
known from [205, 207], where the oscillation properties of (1.99) are investigated
under the Legendre condition (1.111). Under the notation of Sect. 1.6.4, condition
(1.111) coincides with
(0 I) (W(λ)) (0 I)T ≥0.
(1.240)
Remark that (1.240) and (1.203) imply the ﬁrst inequality in (1.204) and then the
statements of Theorem 1.79, see the proof of [207, Theorem 3], where t := λ
and (X(t) U(t)) := W(λ)(0 I)T . The operator (1.190) and monotonicity condition
(1.201) for the symplectic coefﬁcient matrix Sk(λ) were introduced by the third

1.7
Notes and References
81
author in [297] as the main basic tool for the investigation of discrete symplectic
eigenvalue problems with the nonlinear dependence on λ. This notion is closely
related to the so-called multiplicative derivative for the matrix functions X(t)
deﬁned as DtX = X′(t)X−1(t) (see [148, Chapter 15] and the references therein).
In Sect. 1.6.4 we unify the monotonicity results from [55, 57, 102, 205, 207, 297]
using the factorization approach; see [125, Lemma 3.3] and the proof of [125,
Lemma 4.3]. In particular, we derive that (1.201) implies also the piecewise constant
image of B(λ) (see Theorem 1.81) and other results based on the equivalence of
(1.201) and (1.214); see the parts of Theorem 1.82 and Corollary 1.83 related to the
properties of Im B(λ). This connection points out that the “image properties” can be
derived from the restricted monotonicity condition (compare also with (1.214))
(0 I) (W −1(λ)) (0 I)T ≤0,
(1.241)
while the “kernel properties” follow from (1.240).
Index theorems for monotone matrix-valued functions are often utilized in
the oscillation theory of symplectic and Hamiltonian systems; see, e.g., [205,
Section 3.4] and [297, Proposition 2.5]. In these references the index theorems are
considered with the constant matrix R2(t) ≡R2 on [0, ε). The generalized versions
in Theorem 1.85 and Corollary 1.86 with variable R2(t) are from the paper [210,
Theorem 2.1 and Corollary 2.3]. The dependence of R2(t) on t is crucial for the
applications in the oscillation theorems in Sect. 5.1. The extended limit theorem
(Theorem 1.88) and its proof were communicated to the authors by Werner Kratz in
June 2014.
The statement in Lemma 1.89 about symmetric matrices is proven in [283,
Lemma 10.2]. The results in Lemma 1.90 and Theorem 1.91 about orthogonal
projectors are from [285, Lemma 9.1 and Theorem 9.2].
We recommend the following additional related references for further reading
about the topics presented in this chapter: [5, 31, 76, 83, 199] for the Sturm-Liouville
difference equations, [30, 241, 249, 255, 336] for the Sturm-Liouville differential
equations, [65, 236, 256] for integration of Hamiltonian systems and variational
analysis, and [110, 204, 214] for general theory of difference equations.

Chapter 2
Basic Theory of Symplectic Systems
In this chapter we present basic theory of symplectic difference systems. We show
that these systems incorporate as special cases many important equations or systems,
such as the Sturm-Liouville difference equations, symmetric three-term recurrence
equations, Jacobi difference equations, linear Hamiltonian difference systems,
or trigonometric and hyperbolic systems. We investigate the deﬁniteness of the
associated discrete quadratic functional and its relationship with the nonexistence
of focal points of conjoined bases and with the solvability of the Riccati matrix
difference equation. We pay special attention to recessive and dominant solutions
of a nonoscillatory and eventually controllable symplectic systems. We study
general and special symplectic transformations, such as the trigonometric and Prüfer
transformations.
2.1
Symplectic Systems and Their Particular Cases
The central concept of our book is the symplectic difference system
yk+1 = Skyk
(SDS)
where yk ∈R2n and the matrices Sk ∈R2n×2n are symplectic, i.e.,
ST
k J Sk = J ,
J :=
 0 I
−I 0

.
If we write the matrix Sk in the block form
Sk =
Ak Bk
Ck Dk

,
(2.1)
© Springer Nature Switzerland AG 2019
O. Došlý et al., Symplectic Difference Systems: Oscillation and Spectral Theory,
Pathways in Mathematics, https://doi.org/10.1007/978-3-030-19373-7_2
83

84
2
Basic Theory of Symplectic Systems
and yk as yk = xk
uk
 with xk, uk ∈Rn, then system (SDS) can be written as
xk+1 = Akxk + Bkuk,
uk+1 = Ckxk + Dkuk.
(2.2)
In analogy with the terminology for linear Hamiltonian differential systems (see
Sect. 1.5), we call the ﬁrst equation in (2.2) as the equation of motion while the
second equation as the Euler-Lagrange equation.
Basic property of (SDS) is that its fundamental matrix (sometimes also called the
discrete phase ﬂow) is symplectic whenever it is symplectic at an initial condition,
say at k = 0. This easily follows from the fact that we have for the fundamental
matrix Z of (SDS) the expression
Zk = Sk−1Sk−2 · · · S1S0Z0 =
 k−1
'
i=0
Sk−1−i

Z0,
(2.3)
since the symplectic matrices form a group with respect to the matrix multiplication
(see Lemma 1.58). Here we use the convention that the matrices under the matrix
product sign are ordered from the left to the right, i.e., the matrix staying on the left
in the product corresponds to the lower index in the product, and the matrix staying
in the right corresponds to the upper index in the product operator. Also, if the upper
index is less than the lower one, we take the product equal to the identity matrix I.
2.1.1
Conjoined Bases and Wronskian
Next we deﬁne basic concepts of the theory of symplectic systems as presented
in [45]. If Y =
X
U

and ˆY =
 ˆX
ˆU

are two 2n × n solutions of (SDS), then their
Wronskian
Y T
k J ˆYk = XT
k ˆUk −UT
k ˆXk ≡L
(2.4)
is constant with respect to k, where L is a constant n×n matrix. We will denote this
constant matrix by w(Y, ˆY ).
Deﬁnition 2.1 A 2n × n solution Y =
X
U

of (SDS) is a conjoined basis if
w(Y, Y) = XT
k Uk −UT
k Xk = 0
and
rank Yk = n.
(2.5)
The ﬁrst condition in (2.5) means that the matrix XT
k Uk is symmetric. A special
case of a conjoined basis is the so-called principal solution of (SDS) at the point
k = j.

2.1
Symplectic Systems and Their Particular Cases
85
Deﬁnition 2.2 For a ﬁxed index j, let Y [j] =
X[j]
U[j]

be the solution of (SDS) given
by the initial conditions X[j]
j
= 0 and U[j]
j
= I. Then this solution is said to be
the principal solution at k = j.
Remark 2.3 If condition (2.5) holds at one particular index k, then it holds for all
indices. Indeed, suppose that (2.5) holds for k = 0. Denote by K := Y T
0 Y0 and
consider the 2n × n solution ˜Y =  ˜X
˜U
 of (SDS) given by the initial condition
˜Y0 = −J Y0K−1. Then Zk := (Yk
˜Yk) is symplectic for k = 0, see the proof
of Lemma 1.58(vi). It follows by (2.3) that Zk is symplectic for all k, in particular
(2.5) holds for every index k.
Throughout the book we will concentrate on the conjoined bases of (SDS) only,
since 2n × n solutions of (SDS) which are not conjoined play a “destructive” role
in the oscillation theory of (SDS) similarly as their continuous counterparts in the
theory of linear Hamiltonian differential systems. This reasoning has been explained
in more details in Sect. 1.5
If w(Y, ¯Y ) = I, then we say that Y and ¯Y form a pair of normalized conjoined
bases. In this case, see Lemma 1.58(v),
Zk :=

Yk ¯Yk

=
Xk ¯Xk
Uk ¯Uk

is a symplectic fundamental matrix of (SDS). By equations (1.145) and (1.146) in
Lemma 1.58, we then have the properties
XT
k ¯Uk −UT
k ¯Xk = I,
XT
k Uk = UT
k Xk,
¯XT
k ¯Uk = ¯UT
k ¯Xk,
(2.6)
Xk ¯UT
k −¯XkUT
k = I,
Xk ¯XT
k = ¯XkXT
k ,
Uk ¯UT
k = ¯UkUT
k .
(2.7)
We note that given a conjoined basis Y of (SDS), there always exists another
conjoined basis ˆY, which together with Y forms a pair of normalized conjoined
bases. The proof of this claim is essentially contained in Remark 2.3.
Lemma 2.4 For any conjoined bases Y, ¯Y, ˜Y of (SDS) such that w(Y, ¯Y ) = I the
n × n matrix w( ˜Y, Y) [w( ˜Y, ¯Y)]T is symmetric.
Proof The proof follows from the properties in (2.7) by direct calculation of the
product (suppressing the index k)
w( ˜Y , Y) [w( ˜Y, ¯Y)]T
= −˜Y T J Y ¯Y T J ˜Y
(2.7)
=
 ˜UT −˜XT  
X ¯XT
X ¯UT
¯UXT −I U ¯UT
  ˜U
−˜X

= ˜UTX ¯XT ˜U −( ˜UTX ¯UT ˜X + ˜XT ¯UXT ˜U) + ˜XT ˜U + ˜XT U ¯UT ˜X.
(2.8)

86
2
Basic Theory of Symplectic Systems
By (2.5) and (2.7), we know that the matrices ˜XT ˜U, X ¯XT , and U ¯UT are symmetric.
Therefore, the sum in (2.8) above is also a symmetric matrix.
⊓⊔
2.1.2
Special Symplectic Difference Systems
In this subsection we present several important examples of symplectic difference
systems. We start this subsection with a system which is equivalent with (SDS).
Example 2.5 We consider the time-reversed symplectic difference system
yk = S−1
k yk+1.
(2.9)
In [45] an alternative terminology—a reciprocal system—is used. Using the formula
for the inverse of a symplectic matrix from Lemma 1.58(ii), system (2.9) can be
written as
xk = DT
k xk+1 −BT
k uk+1,
uk = −Ckxk+1 + AT uk+1.
(2.10)
All results obtained for (SDS) can be “translated” in a natural way to system (2.9)
by reversing the direction of the independent variable. Indeed, for S∗
k := S−1
N−k for
k ∈[0, N]Z and y∗
k := yN+1−k for k ∈[0, N + 1]Z, we obtain from (2.9) the
equivalent system
y∗
k+1 = yN−k = S−1
N−kyN−k+1 = S∗
k y∗
k,
where the matrix S∗
k is symplectic.
Example 2.6 Next we consider the so-called trigonometric symplectic difference
systems. The trigonometric system is a system (SDS) where the matrix S satisﬁes
the additional condition
J T SkJ = Sk,
(2.11)
which says that if y =
x
u

is a solution of (SDS), then ˜y = −J y =
−u
x

is
a solution of (SDS) as well. Again, substituting into (2.11), we have that (SDS) is
a trigonometric symplectic system if and only if
Dk = Ak,
Ck = −Bk.
Consequently, combining this with (1.145) and (1.146), we see that a trigonometric
symplectic system is a system of the form
xk+1 = Akxk + Bkuk,
uk+1 = −Bkxk + Akuk,
(2.12)

2.1
Symplectic Systems and Their Particular Cases
87
where
AT
k Ak + BT
k Bk = I,
AT
k Bk = BT
k Ak,
(2.13)
which is equivalent to
AkAT
k + BkBT
k = I,
AkBT
k = BkAT
k .
(2.14)
The terminology “trigonometric system” is justiﬁed by the fact that in the scalar
case n = 1, the equalities in (2.13) imply that Ak = cos ϕk and Bk = sin ϕk for
some ϕk and then solutions of (2.12) are
(xk, uk) =

cos
k−1

ϕj

, sin
k−1

ϕj

,
(˜xk, ˜uk) =

−sin
k−1

ϕj

, cos
k−1

ϕj

.
Basic properties of solutions of trigonometric symplectic systems with nonsingular
Bk are established in [25].
Example 2.7 Next we consider the linear Hamiltonian difference system

xk
uk

= J Hk
xk+1
uk

,
(2.15)
where
Hk = HT
k ,
Hk =
−Ck AT
k
Ak
Bk

,
det(I −Ak) ̸= 0.
(2.16)
System (2.15) can be equivalently written as
xk = Akxk+1 + Bkuk,
uk = Ckxk+1 −AT
k uk.
(2.17)
Expanding the forward differences in (2.17), we obtain the system
xk+1
uk+1

= S[H]
k
xk
uk

,
where
S[H]
k
=
 (I −Ak)−1
(I −Ak)−1Bk
Ck(I −Ak)−1 Ck(I −Ak)−1Bk + I −AT
k

.
(2.18)
By direct substitution into (1.145), we see that the matrix S[H]
k
is symplectic.

88
2
Basic Theory of Symplectic Systems
The exact relationship between symplectic and Hamiltonian systems is described
in the next statement.
Theorem 2.8 A symplectic system (SDS) with the matrix Sk in (2.1) is a rewritten
Hamiltonian system (2.15) if and only if the matrix Ak is invertible for all k.
Proof It sufﬁces to prove that invertibility of Ak implies that (SDS) can be written
as (2.15). Deﬁne
Ak := I −A−1
k ,
Bk := A−1
k Bk,
Ck := CkA−1
k .
(2.19)
Then (1.145) and (1.146) imply that the matrices Bk and Ck are symmetric and that
Dk = AT −1
k
(I + CT
k Bk) = AT −1
k
+ CkA−1
k Bk = I −AT
k + Ck(I −Ak)−1Bk.
Then, in view of (2.18), symplectic system (SDS) can be written as a linear
Hamiltonian system (2.15).
⊓⊔
Sometimes, when the matrix Ak in a symplectic system is invertible, we say that
this system has a Hamiltonian structure.
Example 2.9 As an example of discrete symplectic systems with the Hamiltonian
structure, consider the so-called hyperbolic systems in the form (SDS), where Sk
obeys the additional condition
P1SkP1 = Sk,
P1 =
0 I
I 0

.
(2.20)
Here the matrix P1 introduced in Sect. 1.6.1 (see (1.151)) obeys all assumptions of
Lemma 1.58(iv); in particular, we have that P1SkP1 ∈Sp(2n). Relation (2.20) says
that if y =
x
u

is a solution of (SDS), then
u
x

is a solution as well. By analogy with
the trigonometric case, we have that (SDS) is a hyperbolic symplectic system if and
only if
Dk = Ak,
Ck = Bk.
Consequently, combining this with (1.145) and (1.146), we see that a hyperbolic
symplectic system is a system of the form
xk+1 = Akxk + Bkuk,
uk+1 = Bkxk + Akuk,
(2.21)
with n × n matrices Ak and Bk satisfying
AT
k Ak −BT
k Bk = I = AkAT
k −BkBT
k ,
AT
k Bk −BT
k Ak = 0 = AkBT
k −BkAT
k .
(2.22)

2.1
Symplectic Systems and Their Particular Cases
89
The ﬁrst equality in (2.22) implies that matrix Ak is nonsingular (use AT
k Ak =
I + BT
k Bk, where BT
k Bk ≥0), and then according to Theorem 2.8 system (2.21) is
a special case of the discrete Hamiltonian system (2.15) with the blocks (see (2.19))
Ak := I −A−1
k ,
Bk := A−1
k Bk,
Ck := BkA−1
k .
Moreover, since
(AT
k + BT
k )(Ak −Bk) = AT
k Ak + BT
k Ak −AT
k Bk −BT
k Bk = I,
the matrices Ak + Bk and Ak −Bk are nonsingular, too. Furthermore, we have
(Ak −Bk)−1 = AT
k + BT
k ,
(Ak + Bk)−1 = AT
k −BT
k .
(2.23)
The terminology “hyperbolic system” is justiﬁed by the fact that in the scalar case
n = 1, the equalities in (2.22) imply that A2
k −B2
k = 1 and the solution of (2.21)
deﬁned by the initial condition x0 = 0 and u0 = 1 is
xk =
 k−1
'
i=0
sgn Ai

sinh
 k−1

i=0
ln |Ai + Bi|

,
uk =
 k−1
'
i=0
sgn Ai

cosh
 k−1

i=0
ln |Ai + Bi|

.
Basic properties of solutions of hyperbolic symplectic systems are established in
[108].
Example 2.10 Another special case of (SDS) is the symmetric three-term recur-
rence equation and the equivalent Jacobi matrix difference equation (1.60). Consider
the equation
(Rkxk + QT
k xk+1) −(Qkxk + Pkxk+1) = 0
(2.24)
with Pk, Qk, Rk ∈Rn×n, Rk and Pk symmetric and Rk + QT
k invertible, and its
particular case, the matrix Sturm-Liouville equation
(Rkxk) −Pkxk+1 = 0.
(2.25)
Expanding the forward difference in (2.24), we obtain equivalent three-term matrix
recurrence equation
Kk+1xk+2 −Lk+1xk+1 + KT
k xk = 0,
(2.26)

90
2
Basic Theory of Symplectic Systems
where
Kk := Rk + QT
k ,
Lk := Rk + Rk−1 + QT
k−1 + Qk−1 + Pk−1,
i.e., Kk is an invertible matrix and Lk is symmetric.
Example 2.11 Consider now equation (2.26) with Kk invertible, and assume that
Rk are any symmetric matrices. Then, following [304, Theorem 3.1, Corollary 3.4]
(compare also with Theorem 1.29) system (2.26) can be written as a symplectic
difference system (SDS) with the matrices
Ak = K−1
k Rk, Ck = (Lk+1 −Rk+1) K−1
k Rk −KT
k ,
Bk = K−1
k ,
Dk = (Lk+1 −Rk+1) K−1
k .
Indeed, if we deﬁne uk := Kkxk+1 −Rkxk for all k ∈[0, N]Z together with the
additional value uN+1 := (LN+1 −RN+1) xN+1 −KT
NxN, then
xk+1 = K−1
k Rkxk + K−1
k uk = Akxk + Bkuk,
uk+1 = Kk+1xk+2 −Rk+1x + k + 1 = (Lk+1 −Rk+1) xk+1 −KT
k xk
= Ckxk + Dkuk,
for all k ∈[0, N]Z. Conversely, if the matrices Bk for k ∈[0, N]Z in (2.2) are
invertible, then this system can be written as (2.26) with
Kk := B−1
k ,
Lk := B−1
k Ak + Dk−1Bk−1.
Special choices of the matrices Rk (such as Rk := 0 or Rk := Kk when Kk is
also symmetric) yield different representations of (2.26) as a symplectic difference
system (SDS). For a more detailed treatment of the relationship between (SDS) and
(2.24), we refer to [304].
Example 2.12 Consider the 2n-order Sturm-Liouville difference equation
n

ν=0
(−1)ννr[ν]
k νyk+n−ν
 = 0,
r[n]
k
̸= 0.
(2.27)
We put
xk =
⎛
⎜⎜⎜⎝
yk+n−1
yk+n−2
...
n−1yk
⎞
⎟⎟⎟⎠,
uk =
⎛
⎜⎜⎜⎜⎝
n
j=1(−)j−1
r[j]
k jyk+n−j

...
−

r[n]
k nyk

+ r[n−1]
k
n−1yk+1
r[n]
k nyk
⎞
⎟⎟⎟⎟⎠
.
(2.28)

2.1
Symplectic Systems and Their Particular Cases
91
Then y = x
u
 is a solution of the Hamiltonian system (2.17) with
Ak ≡A = {aij} =

1
if j = i + 1,
0
otherwise
,
Bk = diag

0, . . . , 0, 1
r[n]
k
 
,
(2.29)
Ck = diag
(
r[0]
k , r[1]
k , . . . , r[n−1]
k
)
,
(2.30)
with I −Ak upper triangular and det (I −Ak) = 1. Hence I −Ak is invertible with
(I −Ak)−1 =
⎛
⎜⎜⎜⎜⎜⎝
1 1 . . . 1 1
0 1 . . . 1 1
... ... ... ... ...
0 0 . . . 1 1
0 0 . . . 0 1
⎞
⎟⎟⎟⎟⎟⎠
.
(2.31)
Therefore, equation (2.27) is also a special case of system (SDS), in which according
to (2.18) and (2.31), the coefﬁcients are
Ak =
⎛
⎜⎜⎜⎜⎜⎝
1 1 . . . 1 1
0 1 . . . 1 1
...
... ... ...
...
0 0 . . . 1 1
0 0 . . . 0 1
⎞
⎟⎟⎟⎟⎟⎠
,
Bk =
1
r[n]
k
⎛
⎜⎜⎜⎜⎜⎝
0 . . . 0 1
0 . . . 0 1
... ... ...
...
0 . . . 0 1
0 . . . 0 1
⎞
⎟⎟⎟⎟⎟⎠
,
(2.32)
Ck =
⎛
⎜⎜⎜⎜⎜⎜⎝
r[0]
k
r[0]
k
. . . r[0]
k
r[0]
k
0 r[1]
k
. . . r[1]
k
r[1]
k
...
...
...
...
0
0 . . . r[n−2]
k
r[n−2]
k
0
0 . . .
0
r[n−1]
k
⎞
⎟⎟⎟⎟⎟⎟⎠
,
(2.33)
Dk =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
0 0 . . . 0
0
r[0]
k /r[n]
k
−1 1 0 . . . 0
0
r[1]
k /r[n]
k
0 −1 1 . . . 0
0
r[2]
k /r[n]
k
...
...
... ...
...
...
...
0
0 0 . . . 1
0
r[n−3]
k
/r[n]
k
0
0 0 . . . −1 1
r[n−2]
k
/r[n]
k
0
0 0 . . . 0 −1 1 + r[n−1]
k
/r[n]
k
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
(2.34)

92
2
Basic Theory of Symplectic Systems
Example 2.13 Finally, consider the classical second-order Sturm-Liouville differ-
ence equation


rkxk

+ pkxk+1 = 0,
rk ̸= 0.
(2.35)
The symplectic system corresponding to this equation (with uk := rkxk) is
xk+1
uk+1

=
 1
1/rk
−pk 1 −pk/rk
 xk
uk

,
(2.36)
as can be veriﬁed by a direct computation after expanding the forward differences
in the system xk = (1/rk) uk and uk = −pkxk+1.
Let us summarize that the above-described particular cases of (SDS) are equiva-
lent to the invertibility of certain blocks in the matrix Sk. If Ak are invertible, then
(SDS) is equivalent to the linear Hamiltonian difference system (2.15), while if Bk
are invertible, then (SDS) is equivalent to the Jacobi equation (2.24), which is in
turn equivalent to a symmetric three-term recurrence equation (2.26). Finally, if Bk
are invertible and symmetric, then (SDS) is equivalent to a matrix Sturm-Liouville
equation (2.25).
2.2
Focal Points
The central concept of the oscillation theory of symplectic difference systems (SDS)
is the concept of a focal point of a conjoined basis of this system and the concept
of its multiplicity. The concept of a focal point is treated here, while its multiplicity
will be discussed later.
2.2.1
Focal Points of Conjoined Bases
The following notion is motivated by the work [39] and [45] by Bohner and Došlý.
Deﬁnition 2.14 We say that a conjoined basis Y =
X
U

of symplectic difference
system (SDS) has no (forward) focal point in (k, k + 1] if the following conditions
hold
Ker Xk+1 ⊆Ker Xk,
Pk := XkX†
k+1Bk ≥0.
(2.37)
The ﬁrst condition in (2.37) is usually called the kernel condition, while the
second condition in (2.170) is called the P-condition. This means that a conjoined
basis Y does have a focal point in the interval (k, k + 1] if one of the conditions
in (2.170) is violated. The term “forward” focal point refers to the direction of the
kernel condition in (2.37). The following lemma provides equivalent formulations
of the kernel condition in (2.37).

2.2
Focal Points
93
Lemma 2.15 Let Y =
X
U

be a conjoined basis of (SDS). Then
Ker Xk+1 ⊆Ker Xk
⇐⇒
Ker XT
k+1 ⊆Ker BT
k
⇐⇒
Im Bk ⊆Im Xk+1
(2.38)
⇐⇒
Xk+1X†
k+1Bk = Bk.
(2.39)
Proof The equivalence of the second and third condition in (2.38) with (2.39)
follows from (1.159) and (1.160). We will show the equivalence of the ﬁrst two
conditions in (2.38). Suppose that the ﬁrst inclusion holds. Let c ∈Ker XT
k+1 and
˜Y =
 ˜X
˜U

be the conjoined basis which together with Y =
X
U

forms a normalized
pair of conjoined bases, i.e., Xk ˜UT
k −˜XkUT
k = I. Then Xk ˜XT
k = ˜XkXT
k by (1.146)
for the block entries of a symplectic matrix so that Xk+1 ˜XT
k+1c = ˜Xk+1XT
k+1c = 0.
This means that ˜XT
k+1c ∈Ker Xk+1 ⊆Ker Xk. Therefore,
BT
k c = ˜XkXT
k+1c + BT
k c = ˜XkXT
k AT
k c + (I + ˜XkUT
k ) BT
k c
= Xk ˜XT
k AT
k c + Xk ˜UT
k BT
k c = Xk ˜XT
k+1c = 0.
Conversely, let c ∈Ker Xk+1. By (2.39), we have BT
k = BT
k (X†
k+1)T XT
k+1. Then
using (2.10), we get
Xkc = (DT
k Xk+1 −BT
k Uk+1) c = −BT
k (X†
k+1)T XT
k+1Uk+1c
= −BT
k (X†
k+1)T UT
k+1Xk+1c = 0,
hence c ∈Ker Xk. The proof is complete.
⊓⊔
Remark 2.16 Note that if the kernel condition in (2.37) holds, then the matrix Pk
is really symmetric. Indeed, let Qk be a symmetric matrix satisfying the equality
QkXk = UkX†
kXk. Such a matrix really exists, e.g., it is the matrix
Qk := UkX†
k + (UkX†
k ˜Xk −˜Uk)(I −X†
kX) UT
k ,
(2.40)
where ˜Y =
 ˜X
˜U

is a conjoined basis which together with Y =
X
U

forms a pair
of normalized conjoined bases. Substituting for Xk from (2.10), and using formula
(2.39) in Lemma 2.15, we have
Pk = XkX†
k+1Bk = (DT
k Xk+1 −BT
k Uk+1)X†
k+1Bk
= DT
k Xk+1X†
k+1Bk −BT
k Uk+1X†
k+1Xk+1X†
k+1Bk
= DT
k Bk −BT
k Qk+1Bk,
which is symmetric.

94
2
Basic Theory of Symplectic Systems
Observe that Deﬁnition 2.14 is in good agreement with the deﬁnition of a focal
point of a solution of the second- order equation (2.35), when this equation is written
as a symplectic system. Since by (2.36) the matrix Bk = 1/rk ̸= 0 in this case, no
focal point of a solution x of (2.35) in the interval (k, k + 1] means that rkxkxk+1 >
0. This implies that if xk ̸= 0, then xk+1 ̸= 0 as well (the kernel condition), and
rkxkxk+1 > 0 is the P-condition taking into account that rk ̸= 0.
As we will show later in this section, oscillatory properties of (SDS) can be
equivalently deﬁned via generalized zeros of vector solutions. We conclude this
subsection with the deﬁnition of this concept.
Deﬁnition 2.17 We say that a solution y =
x
u

∈R2n of (SDS) has a generalized
zero in the interval (k, k + 1] if
xk ̸= 0,
xk+1 ∈Im Bk,
and
xT
k B†
kxk+1 ≤0.
(2.41)
Deﬁnition 2.18 Symplectic system (SDS) is said to be disconjugate on the interval
[0, N + 1] if no solution of (SDS) has more than one generalized zero in (0, N + 1]
and the solution y =
x
u

with x0 = 0 has no generalized zero in (0, N + 1].
Deﬁnition 2.19 Symplectic system (SDS) is said to be nonoscillatory at ∞if there
exists M such that this system is disconjugate on [M, N +1] for any N ≥M. In the
opposite case, system (SDS) is called oscillatory at ∞.
Remark 2.20 Let y =
x
u

be a vector solution of (SDS), and suppose that xk+1 =
Bkc for some c ∈Rn, i.e., xk+1 ∈Im Bk. Then xT
k B†
kxk+1 = xT
k c. To see this, note
that xk+1 = Bkc implies
xk = DT
k xk+1 −BT
k uk+1 = DT
k Bkc −BT
k uk+1 = BT
k (Dkc −uk+1)
and
xT
k B†
kxk+1 = (Dkc −uk+1)T BkB†
kBkc = xT
k c.
Remark 2.21 Consider now that (SDS) a rewritten even order Sturm-Liouville
difference equation (2.27), i.e., a rewritten linear Hamiltonian system (2.15) with
xk and uk given by (2.28) and Ak, Bk, Ck, Dk given by (2.32)–(2.34). Then by
a direct computation (verifying the four properties in (1.157)), we have
B†
k = r[n]
k
n
⎛
⎜⎜⎜⎝
0 . . . 0
... ... ...
0 . . . 0
1 . . . 1
⎞
⎟⎟⎟⎠.
(2.42)
Taking into account formulas (2.28) and expanding the higher-order difference in
the formula for x, we see that xk+1 = Bkc ∈Im Bk with c = (c1, . . . , cn)T ∈Rn if

2.2
Focal Points
95
and only if xk+1 = cn/r[n]
k
 (1, . . . , 1)T . This means that all the entries of xk+1 in
(2.28) are equal and hence
yk+1 = yk+2 = · · · = yk+n−1 = 0,
cn = r[n]
k yk+n.
(2.43)
We then obtain that
xk =
⎛
⎜⎜⎜⎝
0
...
0
(−1)n−1yk
⎞
⎟⎟⎟⎠,
xk+1 = yk+n
⎛
⎜⎝
1
...
1
⎞
⎟⎠,
(2.44)
and
xT
k B†
kxk+1 = xT
k c = (−1)n−1r[n]
k ykyk+n.
(2.45)
Therefore, combining (2.43), (2.44), and (2.45), we can see that a solution
 xu

of
(SDS) has a forward focal point in the interval (k, k+1] according to Deﬁnition 2.17
if and only if the solution y of (2.27) satisﬁes
yk ̸= 0,
yk+1 = yk+2 = · · · = yk+n−1 = 0,
(−1)n−1r[n]
k ykyk+n ≤0.
(2.46)
Condition (2.46) agrees with the notion of a generalized zero in (k, k + n] for
a solution y of (2.27) introduced by Hartman in [169, pg. 2] or by Bohner in
[41, Remark 3(iii)]. Also, when n = 1, then the second condition in (2.46) is
vacuous, and the ﬁrst and third conditions yield the deﬁnition of a generalized zero
in (k, k + 1] for a solution y of (2.35) (or (1.1); see Deﬁnition 1.1).
2.2.2
Backward Focal Points
The adjective “forward” focal point is used in the previous subsection to distinguish
this concept from the concept of a backward focal point which is deﬁned below. We
will use the convention that “focal point” without any adjective means by default
a forward focal point.
As we have mentioned in the previous section, symplectic system (SDS) is
equivalent to the so-called time-reversed symplectic system
zk = S−1
k zk+1,
S−1 =

DT
k
−BT
k
−CT
k
AT
k

,
(2.47)
and this system motivates the deﬁnition of the backward focal point as follows.

96
2
Basic Theory of Symplectic Systems
Deﬁnition 2.22 Let Y =
X
U

be a conjoined basis of (SDS). We say that Y has no
backward focal point in [k, k + 1) if
Ker Xk ⊆Ker Xk+1
and
Xk+1X†
kBT
k ≥0.
(2.48)
We can also deﬁne the concept of a generalized zero of a vector solution in the
interval [k, k + 1) which is associated with the concept of backward focal point and
it is deﬁned as follows.
Deﬁnition 2.23 Let y =
x
u

∈R2n be a solution of (SDS). This solution has
a generalized zero in the interval [k, k + 1) if
xk+1 ̸= 0,
xk ∈Im BT
k ,
and
xT
k B†
kxk+1 ≤0.
(2.49)
Then, similarly to generalized zeros in (k, k + 1], we have for xk = BT
k c
xk+1 = Akxk + Bkuk = AkBT
k c + Bkuk = Bk(Akc + uk)
and
xT
k B†
kxk+1 = cT BkB†
kBk(Akc + uk) = cT Bk(AT
k c + uk)
= cT xk+1 = xT
k+1c.
Remark 2.24 Returning to the 2n-th order Sturm-Liouville difference equation
(2.27), the concept of a backward focal point in [k, k + 1) for a solution y of (2.27)
in Deﬁnition 2.22 translates as follows: according to (2.49) and (2.32), we have
xk = BT
k c ∈Im BT
k for some c = (c1, . . . , cn)T ∈Rn if and only if
xk =
1
r[n]
k
(0, . . . , 0, c0)T ,
c0 :=
n

j=1
cj.
This means in view of the deﬁnition of xk in (2.28) that
yk+1 = · · · = yk+n−1 = 0,
xk = (0, . . . , 0, (−1)n−1yk)T ,
c0 = (−1)n−1r[n]
k yk,
xk+1 = yk+n (1, . . ., 1)T .
Therefore, xk+1 ̸= 0 if and only if yk+n ̸= 0, and
xT
k B†
kxk+1 = xT
k c = yk+nc0 = (−1)n−1r[n]
k ykyk+n.

2.3
Riccati Equation and Quadratic Functional
97
This shows that a solution  xu
 of (SDS) has a backward focal point in the interval
[k, k +1) according to Deﬁnition 2.23 if and only if the solution y of (2.27) satisﬁes
yk+n ̸= 0,
yk+1 = yk+2 = · · · = yk+n−1 = 0,
(−1)n−1r[n]
k ykyk+n ≤0.
(2.50)
This is what we deﬁne as a generalized zero in the interval [k, k + n) for a solution
y of (2.27). For the second-order Sturm-Liouville difference equation (2.35) (or
(1.1)), we then obtain from (2.50) the deﬁnition of a generalized zero of a solution
y of (2.35) in [k, k + 1), which reads as yk+1 ̸= 0 and rkykyk+1 ≤0.
2.3
Riccati Equation and Quadratic Functional
In this section, we treat two basic concepts which are associated with symplectic
difference systems, namely, the Riccati matrix difference equation and the associ-
ated (energy) quadratic functional. Picone’s identity is then an identity which relates
these two concepts and shows that, similarly to the scalar case, the existence of
a symmetric solution of Riccati equation enables to “complete to the square” the
energy functional, and hence it implies its positivity for any nontrivial admissible
sequence. The main result of this section is formulated in Theorem 2.36 (the Reid
roundabout theorem). We also discuss the nonnegativity of the energy quadratic
functional.
2.3.1
Riccati Matrix Difference Equation
Let us recall that in Sect. 1.5, we have supposed that the considered linear
Hamiltonian differential system is identically normal, which implied (together with
the Legendre condition (1.111)) that focal points of any conjoined basis are isolated.
In the discrete case, all the points in underlaying set Z (or its subsets) are isolated
by themselves, so the assumption of identical normality is of different character.
We start with the simple case. Consider a conjoined basis Y =
X
U

with Xk
invertible for all k ∈[0, N + 1]Z. Then by a direct computation we verify that
Qk := UkX−1
k
is a symmetric solution of the Riccati matrix difference equation
Qk+1 = (Ck + DkQk)(Ak + BkQk)−1,
(2.51)
see also Theorem 2.28 below. In some cases, we will need this equation in a slightly
different form with the so-called Riccati operator
Rk[Q] = 0,
Rk[Q] := Qk+1(Ak + BkQk) −(Ck + DkQk).
(2.52)

98
2
Basic Theory of Symplectic Systems
Another important quantity associated with the matrix Q and used frequently in the
later parts is the matrix
Pk[Q] := (DT
k −BT
k Qk+1) Bk.
(2.53)
When no identical normality is supposed, it may happen that the ﬁrst component
Xk of a conjoined basis is noninvertible on some discrete interval. A typical example
is the symplectic system corresponding to the 2n-th order difference equation
2nyk = 0. Then for the conjoined basis of this system given by the initial condition
Y0 = (0 I)T , we have rank Xk = k for all k = [0, n]Z, as can be veriﬁed by a direct
computation. Nevertheless, also in this case, we can exhibit a kind of Riccati-type
difference equation, sometimes called an implicit Riccati equation. Another form of
the implicit Riccati equation appears later in Theorem 2.36. In this respect equation
(2.51) is called the explicit Riccati equation.
Lemma 2.25 Let Y =
X
U

be a conjoined basis of (SDS) with Ker Xk+1 ⊆Ker Xk
for k ∈[0, N]Z and suppose that Qk is a symmetric matrix with QkXk = UkX†
kXk.
Then we have
Rk[Q] Xk = 0
and
Pk[Q] = XkX†
k+1Bk.
Proof From (1.159) we have XkX†
k+1Xk+1 = Xk and Xk+1X†
k+1Bk = Bk, which
yields
Rk[Q] Xk =

I
Qk+1
T
J T Sk
Xk
Uk

X†
kXk =
Qk+1
−I
TXk+1
Uk+1

X†
kXk = 0.
The second claim is shown in Remark 2.16.
⊓⊔
The following identity relates the matrix Ak +BkQk from (2.51) with the matrix
DT
k −BT
k Qk+1 in (2.53).
Lemma 2.26 Assume that the matrices Qk and Qk+1 are symmetric. Then
(Dk −Qk+1Bk) (AT
k + QkBT
k ) = I −Rk[Q] BT
k .
(2.54)
Proof By (1.146) we know that DkAT
k = I + CkBT
k and AkBT
k is symmetric. Then
(Dk −Qk+1Bk) (AT
k + QkBT
k ) = DkAT
k + DkQkBT
k −Qk+1BkAT
k −Qk+1BkQkBT
k
= I + [(Ck + DkQk) −Qk+1(Ak + BkQk)] BT
k
= I −Rk[Q] BT
k ,
which shows the result.
⊓⊔

2.3
Riccati Equation and Quadratic Functional
99
When the matrices Qk and Qk+1 solve the Riccati equation (2.51), we obtain
from Lemma 2.26 the following important property.
Corollary 2.27 Assume that Qk and Qk+1 are symmetric. Then they satisfy the
implicit Riccati equation Rk[Q]BT
k = 0 if and only if the matrices Ak + BkQk and
DT
k −BT
k Qk+1 are invertible and they are inverses of each other, i.e.,
(Ak + BkQk)−1 = DT
k −BT
k Qk+1.
(2.55)
In particular, property (2.55) holds when Qk and Qk+1 satisfy the explicit Riccati
equation (2.51).
Proof The result follows from (2.54) with Rk[Q] BT
k = 0, resp. with Rk[Q] = 0.
⊓⊔
The following results connects the symmetric solutions of the explicit Riccati
equation (2.52) with conjoined bases Y of (SDS) with Xk invertible.
Theorem 2.28 The Riccati equation (2.52) on [0, N]Z has a symmetric solution
Qk deﬁned on [0, N + 1]Z if and only if there exists a conjoined basis Y of (SDS)
with Xk invertible on [0, N + 1]Z. In this case Qk = UkX−1
k
on [0, N + 1]Z and
Ak + BkQk = Xk+1X−1
k
is invertible on [0, N]Z.
Proof Assume that Y is a conjoined (SDS) with Xk invertible on [0, N + 1]Z. We
deﬁne Qk := UkX−1
k
on [0, N + 1]Z. Then it easily follows that Qk is symmetric
on [0, N + 1]Z and that it solves equation (2.52) on [0, N]Z . Conversely, if Qk
is symmetric solution of (2.52) on [0, N]Z, then by Corollary 2.27, we know that
Ak + BkQk is invertible on [0, N]Z. Consider the solution Xk on [0, N + 1]Z of the
linear system Xk+1 = (Ak + BkQk) Xk for k ∈[0, N]Z with the initial condition
X0 = I. Then Xk is invertible on [0, N + 1]Z, and we set Uk := QkX−1
k
on [0, N +
1]Z. It then follows from (2.52) that Y =
 X
U

is a conjoined basis of (SDS), which
completes the proof.
⊓⊔
2.3.2
Energy Quadratic Functional
In this subsection we will study the quadratic functional, for which the symplectic
system (SDS) is the associated Jacobi system in the spirit of Sect. 1.3.2. Denote
K :=
0 0
I 0

and consider the quadratic functional
F(y) =
N

k=0
yT
k
ST
k KSk −K yk,
yk =
xk
uk

∈R2n.
(2.56)

100
2
Basic Theory of Symplectic Systems
If we write the matrix Sk in the block form, we have
ST
k KSk −K =
CT
k Ak CT
k Bk
BT
k Ck DT
k Bk

,
and hence we can write the energy functional equivalently as
F(y) =
N

k=0
{xT
k CT
k Akxk + 2xT
k CT
k Bkuk + uT
k BT
k Dkuk}.
(2.57)
Deﬁnition 2.29 We say that the sequence y = {yk}N+1
k=0 with yk =
xk
uk

∈R2n is
admissible for the quadratic functional F if
xk+1 = Akxk + Bkuk,
k ∈[0, N]Z.
(2.58)
The admissibility of y =
x
u

means that the pair (x, u) satisﬁes the ﬁrst equation
in symplectic system (the equation of motion). The admissibility of y can be also
characterized as follows. Deﬁne the so-called controllability matrices by
Gk :=
⎛
⎝
k−1
'
j=1
Ak−jB0
k−2
'
j=1
Ak−jB1
. . .
Ak−1Bk−2
Bk−1
⎞
⎠∈Rn×kn.
(2.59)
It follows by induction that y =
x
u

with x0 = 0 is admissible if and only if
xk = Gk
⎛
⎜⎝
u0
...
uk−1
⎞
⎟⎠
for all k ∈[1, N + 1]Z.
(2.60)
Lemma 2.30 Let y =
x
u

be admissible for (2.57). Then this functional can be
expressed in the form
F(x, u) = xT
k uk
			
N+1
k=0 +
N

k=0
xT
k+1(−uk+1 + Ckxk + Dkuk).
(2.61)
Proof Using the admissibility of y = x
u
, we have
F(x, u) =
N

k=0
{xT
k CT
k (Akxk + Bkuk) + (xT
k CT
k + uT
k DT
k ) Bkuk}
=
N

k=0
{xT
k CT
k (Akxk + Bkuk) + xT (AT
k Dk −I) uk + uT
k DT
k (xk+1 −Akxk)}

2.3
Riccati Equation and Quadratic Functional
101
=
N

k=0
{xT
k+1Ckxk −xT
k uk + xT
k+1Dkuk + xT
k+1uk+1 −xT
k+1uk+1}
=
N

k=0
(xT
k uk) +
N

k=0
xT
k+1(−uk+1 + Ckxk + Dkuk)
= xT
k uk
			
N+1
k=0 +
N

k=0
xT
k+1(−uk+1 + Ckxk + Dkuk),
which shows the result.
⊓⊔
The quadratic functional (2.56) is given by the symmetric bilinear form
F( ˜y, y) =
N

k=1
˜yT
k (ST
k KSk −K) yk
=
N

k=0
{˜xT
k CT
k Akxk + ˜xT
k CT
k Bkuk + ˜uT
k BT
k Ckxk + ˜uT
k BT
k Dkuk}.
Similarly as in the previous lemma, we have
F( ˜y, y) = ˜xT
k uk
			
N+1
k=0 +
N

k=0
˜xT
k+1(−uk+1 + Ckxk + Dkuk).
(2.62)
By a direct computation, one can verify that for admissible y, ˜y
F(y + ˜y) = F(y) + 2F(y, ˜y) + F( ˜y),
which has the following consequence frequently used in the discrete calculus of
variations and optimal control.
Theorem 2.31 Suppose that F(y) ≥0 for any admissible y =
x
u

with x0 =
0 = xN+1. Let ˜y =
˜x
˜u

be a solution of (SDS) for which ˜x0 = p ∈Rn and
˜xN+1 = q ∈Rn. Then for any admissible y = x
u
 with x0 = p and xN+1 = q, we
have F(y) ≥F( ˜y).
Proof Denote δyk := ˜yk −yk, i.e., ˜yk = yk + δyk. Then δy is also admissible and
δx0 = 0 = δxN+1, where δyk =
δxk
δuk

, i.e., F(δy) ≥0. We have
F( ˜y) = F(y + δy) = F(y) + 2F(δy, y) + F(δy) ≥F(y),
since according to (2.62), we have F(δy, y) = 0.
⊓⊔

102
2
Basic Theory of Symplectic Systems
Finally, note that another way how to write F(y) for an admissible y is to replace
Bkuk in the last two terms in (2.57) by xk+1 −Akxk. Using this approach, one can
eliminate the variable u in the functional, and this functional can be written in the
form
F(y) =
N

k=0
 xk
xk+1
T
Gk
 xk
xk+1

,
(2.63)
where the symmetric 2n × 2n matrix Gk and the symmetric n × n matrix Ek are
deﬁned by
Gk :=
AT
k EkAk −AT
k Ck CT
k −AT
k Ek
CT
k EkAk
Ek

,
Ek := BkB†
kDkB†
k.
(2.64)
This elimination of u turned to be useful in proving the Sturmian theorems for
symplectic difference systems in Chaps. 4 and 5.
2.3.3
Picone Identity
The next lemma presents the so-called Picone identity, which is used in establishing
conditions for positivity of the energy functional F in the class of admissible pairs
(x, u) with x0 = 0 = xN+1.
Lemma 2.32 Let y =
x
u

be an admissible pair for F and put zk := uk −Qkxk,
where Qk ∈Rn×n is a symmetric matrix. Then we have the identities
(xT
k Qkxk) −xT
k CT
k Akxk −2xT
k CT
k Bkuk −uT
k BT
k Dkuk + zT
k Pk[Q] zk
= 2 uT
k BT
k Rk[Q] xk + xT
k {RT
k [Q] Ak −QkBT
k Rk[Q]} xk,

(2.65)
and
(DT
k −BT
k Qk+1) xk+1 = xk + Pk[Q] zk −BT
k Rk[Q] xk.
(2.66)
In particular, if Q is a solution of Riccati equation (2.52), i.e., Rk[Q] = 0, and
x0 = 0 = xN+1, we have the Picone identity
F(y) =
N

k=0
zT
k Pk[Q] zk.
(2.67)
Proof Formula (2.65), the so-called Picone identity in difference form, can be
derived by direct calculations, which are however rather technical. We will present

2.3
Riccati Equation and Quadratic Functional
103
a more elegant proof of (2.65) at the end of Sect. 2.6. As for identity (2.66), we
prove it as follows. Denote the left-hand side of this identity by L. Then we have
L = (DT
k −BT
k Qk+1) (Akxk + Bkuk)
= DT
k Akxk −BT
k Qk+1Akxk + DT
k Bkuk −BT
k Qk+1Bkuk
= (I + BT
k Ck) xk −BT
k Qk+1Akxk + Pk[Q] (uk −Qkxk)
+ (BT
k Dk −BT
k Qk+1Bk) Qkxk
= xk + Pk[Q] zk −BT
k Rk[Q] xk,
which is the right-hand side of (2.66). Formula (2.67) is obtained by the summation
of (2.65) from k = 0 to k = N by using Rk[Q] = 0 and x0 = 0 = xN+1.
⊓⊔
2.3.4
Reid Roundabout Theorem
Now we are ready to formulate and prove the Reid roundabout theorem for
symplectic difference system. This theorem plays the fundamental role in the
oscillation theory of these systems. In the proof we will need the following auxiliary
result, which shows that the kernel condition (2.37) implies the so-called image
condition for admissible pairs.
Lemma 2.33 Let Y = X
U
 be a conjoined basis of (SDS) satisfying the kernel
condition in (2.37). Then for any admissible sequence y =
x
u

with x0 ∈Im X0, we
have xk ∈Im Xk for all k ∈[0, N + 1]Z.
Proof Suppose that xk ∈Im Xk, i.e., xk = Xkc for some c ∈Rn and k ∈[0, N]Z.
The admissibility of y means that
xk+1 = Axk + Bkuk = Xk+1c + Bk(Ukc + uk) ∈Im Xk+1,
since Im Bk ⊆Im Xk+1 by Lemma 2.15.
⊓⊔
As we will see later, in the investigation of the positivity or nonnegativity of the
quadratic functional F associated with a symplectic difference system, an important
role is played by the principal solution at k = 0 given in Deﬁnition 2.2.
Lemma 2.34 Suppose that for all solutions y = x
u
 of (SDS) with x0 = 0, we have
that xT
k c > 0 whenever xk ̸= 0 and xk+1 = Bkc hold for k ∈[0, N]Z, i.e., y has no
generalized zero in (0, N + 1]. Then the principal solution of (SDS) at k = 0 has
no focal points in (0, N + 1].
Proof Let Y =
X
U

be the principal solution of (SDS) at k = 0. First, let α ∈
Ker Xm+1 for some m ∈[0, N]Z and put yk := Ykα on [0, N + 1]Z. Then y solves

104
2
Basic Theory of Symplectic Systems
(SDS) with x0 = 0 and xm+1 = 0 ∈ImBm, so that xm = 0 as well. Otherwise
we would have xT
mc > 0 with c = 0 by the assumptions of the lemma. Hence
Ker Xm+1 ⊆Ker Xm holds. Now, let c ∈Rn, α := X†
m+1Bmc, and deﬁne again
yk := Ykα on [0, N + 1]Z. Then y solves (SDS) with x0 = 0, xm+1 = Bmc, and
xT
mc = cT XmX†
m+1Bmc. Therefore, XmX†
m+1Bm ≥0 holds as well, and Y =
X
U

has no focal points in (0, N + 1].
⊓⊔
Lemma 2.35 If F(y) > 0 for all admissible pairs y = x
u
 with x0 = 0 = xN+1
and x ̸≡0, then (SDS) is disconjugate on the interval [0, N + 1].
Proof Suppose that (SDS) is not disconjugate. Then by Remark 2.20, there exist
a solution y =
x
u

of (SDS), the indices m, p ∈[0, N]Z with m < p, and vectors
cm, cp ∈Rn such that
xm ̸= 0, xm+1 = Bmcm, xT
mcm ≤0,
xp ̸= 0, xp+1 = Bpcp, xT
p cp ≤0.
This means that the solution y has generalized zeros in the intervals (m, m + 1] and
(p, p + 1]. Deﬁne
˜xk :=

xk, m + 1 ≤k ≤p,
0
otherwise,
˜uk :=
⎧
⎪⎪⎨
⎪⎪⎩
cm,
k = m,
uk,
m + 1 ≤k ≤p −1,
up −cp, k = p,
0,
otherwise.
Then ˜x0 = 0 = ˜xN+1 and ˜x ̸≡0 hold. Moreover, ˜y =
˜x
˜u

is easily checked to be
admissible on [0, N + 1]Z, and by Lemma 2.30 (with y := ˜y), we get
F( ˜y) = ˜xT
k ˜uk
		N+1
0
+
N

k=0
˜xT
k+1{Ck ˜xk + Dk ˜uk −˜uk+1}
=

k∈{m,p−1}
˜xT
k+1{Ck ˜xk + Dk ˜uk −˜uk+1} +
p−2

k=m+1
˜xT
k+1{Ck ˜xk + Dk ˜uk −˜uk+1}
= xT
m+1(Dmcm −um+1) + xT
p (Cp−1xp−1 + Dp−1up−1 + cp −up)
(2.10)
=
(xT
m + uT
m+1Bm) cm −xT
m+1um+1 + xT
p cp
= xT
mcm + xT
p cp ≤0.
Thus F(y) ̸> 0, which completes the proof.
⊓⊔
We are now ready to prove our main result of this section, the Reid roundabout
theorem for symplectic systems (SDS), which states several conditions which are
equivalent to the positivity of the functional F deﬁned in (2.56). We recall that
the Riccati operator Rk[Q] is given by (2.52), Pk[Q] is given by (2.53), and the
controllability matrix Gk is deﬁned in (2.59).

2.3
Riccati Equation and Quadratic Functional
105
Theorem 2.36 (Reid Roundabout Theorem) The following statements are equiv-
alent.
(i) The quadratic functional F(y) > 0 for all admissible sequences y =
x
u

with
x0 = 0 = xN+1 and x ̸≡0.
(ii) System (SDS) is disconjugate on [0, N + 1] according to Deﬁnition 2.18.
(iii) No solution y =
x
u

of (SDS) with x0 = 0 has any generalized zero in the
interval (0, N + 1].
(iv) The principal solution Y [0] =
X[0]
U[0]

of (SDS) has no focal points in the interval
(0, N + 1], i.e., condition (2.37) holds for all k ∈[0, N]Z.
(v) The implicit Riccati equation Rk[Q] Gk = 0, k ∈[0, N]Z, has a symmetric
solution Q deﬁned on [0, N + 1]Z such that Pk[Q] ≥0 for all k ∈[0, N]Z.
Proof The statement (i) implies (ii) by Lemma 2.35, while (iii) follows from (ii)
trivially, and condition (iii) implies (iv) by Lemma 2.34. Now, assume that (iv)
holds. Let Y := Y [0] be the principal solution at k = 0 and let ˜Y =
 ˜X
˜U

be the
associated solution of (SDS) at k = 0, i.e., ˜X0 = I and ˜U0 = 0. The matrix
Qk deﬁned by (2.40) is symmetric and satisﬁes the assumptions of Lemma 2.25,
so that Pk[Q] ≥0 holds. Moreover, Lemma 2.39 and condition (2.60) imply that
Rk[Q] Gk = 0, which completes the proof of (v). Suppose now that (v) is true
with some symmetric Q. Let y =
x
u

be admissible with x0 = 0 = xN+1. Then
Rk[Q] xk = 0 for all k ∈[0, N]Z by (2.60). From (2.67) in Lemma 2.32 we obtain
F(y) =
N

k=0
zT
k Pk[Q] zk ≥0.
To show positive deﬁniteness, assume that F(y) = 0 for some admissible y =
x
u

with x0 = 0 = xN+1. Then Pk[Q] zk = 0 for all k ∈[0, N]Z and (2.66) shows
that x ≡0. Thus F(y) > 0, showing (i). In conclusion, we have proven that the
statements (i) through (v) are equivalent.
⊓⊔
Remark 2.37 The proof of the implications (iv) ⇒(v) ⇒(i) shows that the
following more general result is true. The functional F is positive deﬁnite if and
only if there exists a conjoined basis Y = X
U
 of (SDS) with no focal points in the
interval (0, N + 1]. Indeed, for such a conjoined basis Y =
X
U

of (SDS) and the
corresponding symmetric Q in (2.40) we have Rk[Q] Xk = 0 for k ∈[0, N]Z. Then
for any admissible y =
x
u

with x0 = 0 = xN+1 we have by Lemma 2.33 that
xk ∈Im Xk for all k ∈[0, N + 1]Z. The positivity of the functional F now follows
from the Picone formula (Lemma 2.32) as in the proof of (v) ⇒(i) in Theorem 2.36.
Conversely, the positivity of F implies condition (iv); hence, there indeed exists
a conjoined basis Y =
X
U

of (SDS) with no focal points in (0, N + 1], as we claim
in this remark.

106
2
Basic Theory of Symplectic Systems
Next we present a construction of an admissible pair for (2.57) for which F ̸≥0,
when kernel condition or P-condition in (2.37) are violated (compare with the
proof of Theorem 2.44 in Sect. 2.3.5). This result also shows a direct proof of the
implication (i) ⇒(iv) in Theorem 2.36 above.
Proposition 2.38 Let Y =
X
U

be a conjoined basis of (SDS).
(i) If there exists m ∈[0, N]Z such that Ker Xm+1 ̸⊆Ker Xm, i.e., there exists
α ∈Ker Xm+1 \ Ker Xm, then the pair y =
x
u

deﬁned by
xk :=

Xkα
0 ≤k ≤m,
0
m + 1 ≤k ≤N + 1,
uk :=

Ukα
0 ≤k ≤m,
0
m + 1 ≤k ≤N + 1,
is admissible on [0, N + 1]Z, x ̸≡0, and we have F(y) = −αT X0U0α.
(ii) If there exists m ∈[0, N]Z such that Ker Xm+1 ⊆Ker Xm and Pm ̸≥0, i.e.,
there exists c ∈Rn such that cT Pmc < 0, then for d := X†
m+1Bmc the pair
y =
x
u

deﬁned by
xk :=

Xkd
0 ≤k ≤m,
0
m + 1 ≤k ≤N + 1,
uk :=
⎧
⎪⎪⎨
⎪⎪⎩
Ukd
0 ≤k ≤m −1,
Ukd −c
k = m,
0
m + 1 ≤k ≤N + 1,
is admissible on [0, N + 1]Z, x ̸≡0, and F(y) = −αT X0U0α + cT Pmc.
(iii) In particular, if Y = Y [0] is the principal solution of (SDS) at k = 0, then
F(y) = 0 holds in case (i), and F(y) < 0 holds in case (ii).
Proof We prove the statement (ii), the proof of (i) is similar. We have x0 = xN+1 =
0, xk+1 = Akxk + Bkuk for 0 ≤k ≤N with k ̸= m and
Amxm + Bmum = Xm+1d −Bmc = 0 = xm+1,
because Xm+1X†
m+1Bm = Bm by (4.1) and (4.2). Hence y =
x
u

is admissible.
Moreover, xm = Xmd = Pmc ̸= 0, which shows that x ̸≡0 (in case (i) we use
xm = Xmα ̸= 0, since α ̸∈Ker Xm). Using Lemma 2.30 and
Cm−1xm−1 + Dm−1um−1 = Umd = um + c,

2.3
Riccati Equation and Quadratic Functional
107
we obtain
F(y) =
N

k=0
xT
k+1{Ckxk + Dkuk −uk+1}
= xT
m{Cm−1xm−1 + Dm−1um−1 −um} = cT Xmd = cT Pmc < 0.
Hence F ̸≥0 and the proof is complete.
⊓⊔
The following result is a direct extension of the equivalence of conditions (i) and
(iv) in Theorem 2.36.
Theorem 2.39 Let Y be any conjoined basis of (SDS). Then Y has no forward
focal points in (0, N + 1] if and only if F(y) + dT XT
0 U0d > 0 for all admissible
y =
 xu

with x0 = X0d, xN+1 = 0, and x ̸≡0.
Proof The necessity follows from the Picone identity (Lemma 2.32) similarly as
the proof of the corresponding parts in Theorem 2.36. The sufﬁciency is a direct
consequence of Proposition 2.38.
⊓⊔
We conclude this subsection with a statement completing Theorem 2.36 with
further conditions equivalent to the positivity of the functional F in (2.57). In
contrast with conditions (iv) and (v) in the previously mentioned result, we now
consider a conjoined basis Y with Xk invertible on the whole interval [0, N + 1]Z
and the explicit Riccati equation (2.52).
Theorem 2.40 The following statements are equivalent.
(i) The quadratic functional F is positive deﬁnite, i.e., condition (i) in Theo-
rem 2.36 holds.
(ii) There exists a conjoined basis Y of (SDS) with no forward focal points in
(0, N +1] such that Xk is invertible for all k ∈[0, N +1]Z, i.e., XkX−1
k+1Bk ≥0
for k ∈[0, N]Z.
(iii) The explicit Riccati equation (2.52), k ∈[0, N]Z, has a symmetric solution Qk
on [0, N + 1]Z such that
Ak + BkQk is invertible and (Ak + BkQk)−1Bk ≥0,
k ∈[0, N]Z.
(2.68)
Proof Assume that (i) holds. It follows by a perturbation argument in [258,
Theorem 2(iii’)] or by [172, Theorem 7] that there exists α
> 0 such that
F(y) + α ∥x0∥2 > 0 for all admissible y =
x
u

with xN+1 = 0 and x ̸≡0.
Then, by Theorem 2.39, the conjoined basis Y of (SDS) starting with the initial
conditions X0 = (1/α) I and U0 = I has no forward focal points in (0, N + 1].
Since X0 is invertible, the kernel condition in (2.37) implies that Xk is invertible
on [0, N + 1]Z and XkX−1
k+1Bk ≥0 on [0, N]Z, as we claim in part (ii). Next,
if (ii) holds then we set Qk := UkX−1
k
on [0, N + 1]Z, which satisﬁes condition

108
2
Basic Theory of Symplectic Systems
(iii) by the Riccati equivalence in Theorem 2.28. Finally, if (iii) holds, then
Pk[Q] = (Ak + BkQk)−1Bk ≥0 on [0, N]Z and the Picone formula (2.67) in
Lemma 2.32 implies that the functional F is nonnegative deﬁnite, i.e., F(y) ≥0
for any admissible y =
x
u

with x0 = 0 = xN+1. If now F(y) = 0 for such
an admissible y, then Pk[Q] zk = 0 with zk := uk −Qkxk on [0, N]Z, so that
(DT
k −BT
k Qk+1) xk+1 = xk on [0, N]Z by (2.66). From xN+1 = 0 we then get
xk = 0 for all k ∈[0, N + 1]Z, showing that the functional F is actually positive
deﬁnite, i.e., condition (i) holds.
⊓⊔
The Reid roundabout theorem has its analogy for the time-reversed symplectic
system (2.9).
Theorem 2.41 (Reid Roundabout Theorem for Time-Reversed System) The
following statements are equivalent:
(i) The quadratic functional
FR(y) :=
N

k=0
yT
k+1{ST −1
k
KS−1
k
−K} yk+1
(2.69)
=
N

k=0
{−xk+1CkDT
k xk+1 + 2xT
k+1CkBT
k uk+1 −uT
k+1AkBT
k uk+1} < 0
for all y =
x
u

such that xk = DT
k xk+1 −BT
k uk+1 for k ∈[0, N]Z and such
that x0 = 0 = xN+1 and x ̸≡0.
(ii) System (2.9) is disconjugate on [0, N + 1], i.e., no solution of (2.9) has
more than one and no solution y =
x
u

of (2.9) with xN+1 = 0 has any
generalized zeros in the interval [0, N + 1). Here the interval [m, m + 1)
contains a generalized zero of a solution y =
x
u

of (2.9) if
xm+1 ̸= 0,
xm ∈Im BT
m,
and
xT
mB†
mxm+1 ≤0.
(iii) No solution y =
x
u

of (2.9) with xN+1 = 0 has any generalized zero in the
interval [0, N + 1).
(iv) The solution Y =
X
U

of (2.9) with YN+1 =
 0
−I

has no backward focal points
in the interval [0, N + 1), i.e., condition (2.48) holds for all k ∈[0, N]Z.
(v) The equation ˜Rk[Q] ˜Gk = 0, k ∈[0, N]Z, has a symmetric solution Q deﬁned
on on [0, N + 1]Z such that ˜Pk[Q] ≥0 for all k ∈[0, N]Z, where
˜Rk[Q] := (QkBT
k −AT
k ) Qk+1 + (QkDT
k −CT
k ),
˜Pk[Q] := BkAT
k −BkQkBT
k ,
˜Gk :=
⎛
⎝
N−1
'
j=k+1
DT
j BT
N
N−2
'
j=k+1
DT
j BT
N−1 . . . DT
k+1BT
k+2 BT
k+1
⎞
⎠.

2.3
Riccati Equation and Quadratic Functional
109
Proof The proof is based on the transformation of the time-reversed system to
a standard symplectic system according to Example 2.5, to which Theorem 2.36
is applied.
⊓⊔
Remark 2.42 Similarly to Remark 2.37, condition (i) in Theorem 2.41 is equivalent
with the existence of a conjoined basis Y =
X
U

of (2.9) with no backward focal
points in the interval [0, N + 1).
Actually, all the statements in Theorems 2.36 and 2.41 are mutually equivalent,
as we show in the last statement of this subsection.
Theorem 2.43 The statements (i) of Theorem 2.36 and (i) of Theorem 2.41 are
equivalent.
Proof Assume that (i) of Theorem 2.36 holds and let y = x
u
 be admissible for
the functional FR in (2.69) corresponding to (2.9), i.e., xk = DT
k xk+1 −BT
k uk+1
for k ∈[0, N]Z, x0 = 0 = xN+1, and x ̸≡0. Put y∗
k =
x∗
k
u∗
k

:= S−1
k yk+1 for
k ∈[0, N]Z and x∗
N+1 := 0. Then x∗
k = DT
k xk+1 −BT
k uk+1 = xk for all k ∈[0, N]Z
and x∗
N+1 = 0 = xN+1. It follows that x∗
k+1 = xk+1 = Akx∗
k + Bku∗
k holds for all
k ∈[0, N]Z and (i) of Theorem 2.36 implies 0 < F(y∗) = −FR(y). This shows
that (i) of Theorem 2.41 is satisﬁed. Conversely, if (i) of Theorem 2.41 is true, then
for an admissible y =
x
u

for F such that x0 = 0 = xN+1 and x ̸≡0, we put
y∗
k+1 := Skyk for k ∈[0, N]Z and y∗
0 := 0. Similarly as above, it follows by (i) of
Theorem 2.41 that 0 > FR(y∗) = −F(y), so that (i) of Theorem 2.36 holds, which
completes the proof.
⊓⊔
2.3.5
Nonnegative Quadratic Functional
In Sect. 1.3, we discussed the importance of the nonnegativity and positivity of
discrete quadratic functionals in the second-order necessary and sufﬁcient optimal-
ity conditions for discrete calculus of variations and optimal control problems (in
particular, Theorems 1.26 and 1.32 contain necessary optimality conditions and
Theorems 1.28 and 1.33 contain sufﬁcient optimality conditions). Along with the
results on the positivity of the discrete quadratic functional F in (2.57) associated
with symplectic system (SDS) in Sect. 2.3.4, we now present conditions, which
are equivalent with the nonnegativity of F. We recall the deﬁnition of the matrix
Pk := XkX†
k+1Bk in (2.37), and also deﬁne the new matrices
Mk := (I −Xk+1X†
k+1) Bk,
Tk := I −M†
k Mk.
(2.70)
Note that Tk is symmetric. By Lemma 2.15 we know that Mk = 0 (and hence
Tk = I) on [0, N]Z if and only if the kernel condition
Ker Xk+1 ⊆Ker Xk,
k ∈[0, N]Z,
(2.71)

110
2
Basic Theory of Symplectic Systems
holds. We note that the matrices Mk and Tk will be used in Sect. 4.1 in the deﬁnition
of the multiplicities of forward focal points of Y (see Deﬁnition 4.1).
Theorem 2.44 The quadratic functional F is nonnegative, i.e., F(y) ≥0 for every
admissible y =
x
u

with x0 = 0 = xN+1 if and only if the principal solution
Y [0] =
X[0]
U[0]

of (SDS) satisﬁes the image condition
xk ∈Im X[0]
k ,
k ∈[0, N + 1]Z,
(2.72)
for every admissible y =
x
u

with x0 = 0 = xN+1 and the P-condition
TkPkTk ≥0,
k ∈[0, N]Z.
(2.73)
The main difference between the statements of Theorem 2.36 (condition (iv))
and the above theorem lies in the relationship between the kernel condition in (2.71)
and the image condition (2.72). Of course, (2.71) implies (2.72), which follows
from Lemma 2.33. The converse is not in general true, however. Furthermore, under
(2.71) the P-condition (2.73) reduces to the one in (2.37).
Before presenting the proof of Theorem 2.44, we state several additional
properties of the matrices Pk, Mk, and Tk and derive a version of the Picone formula
(see Lemma 2.32) under the image condition (2.72). The properties of Moore-
Penrose inverses (see Sect. 1.6.2) imply that
XT
k+1Mk = 0,
M†
k Xk+1 = 0,
MkTk = 0,
BkTk = Xk+1X†
k+1BkTk.
(2.74)
With a given conjoined basis Y = X
U
 we associate on [0, N + 1]Z the matrix
Qk := UkX†
k + (UkX†
k)T (I −XkX†
k).
(2.75)
Lemma 2.45 Let Y =
X
U

be a conjoined basis of (SDS), Qk be deﬁned by (2.75),
and let y =
x
u

be an admissible pair satisfying the image condition (2.72). Then
Y, Q, and y satisfy the following conditions.
(i) The matrix Qk is symmetric and QkXk = UkX†Xk on [0, N + 1]Z.
(ii) The matrix TkPkTk is symmetric for all k ∈[0, N]Z.
(iii) If zk := uk −Qkxk, then Mkzk = 0 (i.e., zk = Tkzk) on [0, N]Z.
(iv) We have the Picone identity
xT
k CT
k Akxk + 2 xT
k CT
k Bkuk + uT
k DT
k Bkuk = (xT
k Qkxk) + zT
k Pkzk,
(2.76)
{DT
k −BT
k Qk+1} xk+1 = xk + Pk zk,
(2.77)

2.3
Riccati Equation and Quadratic Functional
111
for all k ∈[0, N]Z, and consequently
F(y) =
N

k=0
zT
k Pkzk =
N

k=0
zT
k TkPkTkzk.
(2.78)
Proof The matrix Qk in (2.75) is symmetric by the symmetry of XT
k Uk and XkX†
k.
The identity QkXk = UkX†Xk follows from (2.75) trivially. By using the time-
reversed symplectic system (2.10) and the fourth identity in (2.74), we obtain
PkTk = (DT
k −BT
k Qk+1) BkTk.
(2.79)
Since DT
k Bk is symmetric, it follows that the matrix TkPkTk is symmetric on [0, N]Z
as well. Since xk ∈Im Xk, we have xk = XkX†
kxk on [0, N + 1]Z. Then we have
Mkuk = (I −Xk+1X†
k+1) (xk+1 −Akxk)
= (I −Xk+1X†
k+1) (Xk+1X†
k+1xk+1 −AkXkX†
kxk)
= (I −Xk+1X†
k+1) (BkUk −Xk+1) X†
kxk
= MkUkX†
kxk = MkUkX†
kXkX†
kxk = MkQkxk
on [0, N]Z. This shows that Mkzk = 0 on [0, N]Z. Finally, formulas (2.76) and
(2.77) follow from the Picone formula in Lemma 2.32; since for the matrix Qk in
(2.75) we have XT
k+1Rk[Q] Xk = 0 on [0, N]Z by Lemma 2.58(ii).
⊓⊔
Proof of Theorem 2.44 Let Y := Y [0] be the principal solution of (SDS) at k = 0,
i.e., X0 = 0 and U0 = I. First we show that conditions (2.72) and (2.73) are
necessary for the nonnegativity of the functional F. Thus, assume that F(y) ≥0
for every admissible y =
x
u

with x0 = 0 = xN+1. Assume that there exists
an admissible y =
x
u

with x0 = 0 = xN+1 and m ∈[1, N −1]Z such that
xk ∈Im Xk for all k ∈[0, m]Z but xm+1 ̸∈Im Xm+1. First we observe that the n×n
matrices
K := XT
m+1Xm+1 + UT
m+1Um+1 > 0,
S′ := K−1UT
m+1Mm+1
satisfy the equations
Xm+1S′ = 0,
Um+1S′ = Mm.
(2.80)
Indeed, for the 2n × 2n matrix, Z deﬁned below has the property
Z :=

K−1XT
m+1 K−1UT
m+1
−UT
m+1
XT
m+1

,
Z−1 =
Xm+1 −Um+1K−1
Um+1 Xm+1K−1

,

112
2
Basic Theory of Symplectic Systems
so that
Z
 0
Mm

=
S′
0

,
 0
Mm

= Z−1
S′
0

=
Xm+1S′
Um+1S′

.
Next we prove that MT
mxm+1 ̸= 0. Assume that MT
mxm+1 = 0. Then
0 = MT
mxm+1 = BT
m(I −Xm+1X†
m+1) xm+1
= BT
m(I −Xm+1X†
m+1) (AmXmα + Bmum)
= BT
m(I −Xm+1X†
m+1) [(Xm+1 −BmUm) α + Bmum]
= BT
m(I −Xm+1X†
m+1)2Bm(um −Umα) = MT
mMm(um −Umα).
This implies that ∥Mm(um −Umα)∥2 = 0, i.e., Mm(um −Umα) = 0. Then in turn
0 = Mm(um −Umα) = (I −Xm+1X†
m+1) (Bmum −BmUmα)
= (I −Xm+1X†
m+1) [ xm+1 −AmXmα −(Xm+1 −AmXm) α ]
= (I −Xm+1X†
m+1) (xm+1 −Xm+1α) = (I −Xm+1X†
m+1) xm+1.
Therefore, xm+1
= Xm+1X†
m+1xm+1
∈Im Xm+1, which is a contradiction.
Therefore, MT
mxm+1 ̸= 0 must hold. Now we deﬁne the sequence ˜y =
˜x
˜u

as
follows:
˜xk :=

Xk(α + ˜α), k ∈[0, m]Z,
xk,
k ∈[m + 1, N + 1]Z,
˜uk :=
⎧
⎪⎨
⎪⎩
Uk(α + ˜α), k ∈[0, m −1]Z,
um + Um ˜α, k = m,
uk,
k ∈[m + 1, N + 1]Z,
where ˜α := t S′MT
mxm+1 with t ∈R, t ̸= 0. Now ˜x0 = X0(α + ˜α) = 0 and
˜xN+1 = xN+1 = 0. For k ∈[0, m −1]Z we have
Ak ˜xk + Bk ˜uk = (AkXk + BkUk) (α + ˜α) = Xm+1(α + ˜α) = ˜xk+1,
for k ∈[m + 1, N]Z we have
Ak ˜xk + Bk ˜uk = Akxk + Bkuk = xk+1 = ˜xk+1,

2.3
Riccati Equation and Quadratic Functional
113
and for k = m we have
Ak ˜xk + Bk ˜uk = AmXm(α + ˜α) + Bm(um + Um ˜α)
= Amxm + Bmum + (AmXm + BmUm) ˜α
= xm+1 + Xm+1 ˜α = xm+1 = ˜xm+1,
because Xm+1 ˜α = t Xm+1S′MT
mxm+1 = 0 by (2.80). Therefore, ˜y is admissible
with ˜x0 = 0 = ˜xN+1. It follows by Lemma 2.30 and by splitting the total sum over
[0, N]Z into four sums over the sets [0, m −2]Z, {m −1}, {m}, [m + 1, N]Z that
F( ˜y) =
 m−2

k=0
+
m−1

k=m−1
+
m

k=m
+
N

k=m+1
 
˜xT
k+1(Ck ˜xk + Dk ˜uk −˜uk+1)
= ˜xT
m(Cm−1 ˜xm−1 + Dm−1 ˜um−1 −˜um) + ˜xT
m+1(Cm ˜xm + Dm ˜um −˜um+1)
+
N

k=m+1
˜xT
k+1(Ck ˜xk + Dk ˜uk −˜uk+1)
= F1(α) + F2(α, ˜α),
where
F1(α) := xT
m+1(CmXmα + Dmum −um+1) + αT XT
m(Umα −um)
+
N

k=m+1
xT
k+1(Ckxk + Dkuk −uk+1),
F2(α, ˜α) := ˜αT XT
m(Umα −um) + xT
m+1Um+1 ˜α,
i.e., the term F1(α) does not depend on ˜α (and hence on t), while the term F2(α, ˜α)
does depend on ˜α (and hence on t). Since Xm+1 ˜α = 0, Xm = DT
mXm+1 −BT
mUm+1
by (2.10), and UT
m+1Xm+1 is symmetric, the ﬁrst term in F2(α, ˜α) is equal to
˜αT XT
m(Umα −um) = ˜αT (XT
m+1Dm −UT
m+1Bm) (Umα −um)
= ˜αT UT
m+1(Bmum −BmUmα)
= ˜αT UT
m+1[ xm+1 −AmXmα −(Xm+1 −AmXm) α]
= ˜αT UT
m+1(xm+1 −Xm+1α) = ˜αT UT
m+1xm+1.

114
2
Basic Theory of Symplectic Systems
Hence, by (2.80) we get the expression
F2(α, ˜α) = ˜αT UT
m+1xm+1 + xT
m+1Um+1 ˜α = 2t xT
m+1UT
m+1S′MT
mxm+1
= 2t xT
m+1MmMT
mxm+1 = 2t ∥MT
mxm+1∥2,
as well as
F( ˜y) = F1(α) + F2(α, ˜α) = F1(α) + 2t ∥MT
mxm+1∥2.
But since MT
mxm+1 ̸= 0, it follows that F( ˜y) →−∞as t →−∞, which
contradicts the assumed nonnegativity of the functional F.
Next we prove that the P-condition (2.73) is necessary for the nonnegativity of
F. Suppose that there exists c ∈Rn, c ̸= 0, such that cT TmPmTmc < 0 for some
m ∈[0, N]Z. Set d := X†
m+1BmTmc and deﬁne the pair y =
x
y

by
xk :=
⎧
⎨
⎩
Xkd,
k ∈[0, m −1]Z,
PmTmc, k = m,
0,
k ∈[m + 1, N + 1]Z,
uk :=
⎧
⎨
⎩
Ukd,
k ∈[0, m −1]Z,
−AT
m(Dm −Qm+1Bm) Tmc, k = m,
0,
k ∈[m + 1, N]Z,
where Qm+1 is deﬁned by (2.75). Then y is admissible for k ∈[0, m −2]Z as well
as for k ∈[m + 1, N]Z. For k = m −1 we have
Am−1xm−1 + Bm−1um−1 = (Am−1Xm−1 + Bm−1Um−1) d = Xmd
= XmX†
m+1BmTmc = PmTmc = xm,
while for k = m we use (1.145) and (2.79) to obtain
Amxm + Bmum = AmPmTmc −BmAT
m(Dm −Qm+1Bm) Tmc
= Am(PmTm −PmTm) c = 0 = xm+1.
Thus, y is admissible for F. Moreover, x0 = X0d = 0 and xN+1 = 0. To compute
the value of F(y) we use Lemma 2.30, (2.79), and (2.74) to obtain
F(y) =
N

k=0
xT
k+1(Ckxk + Dkuk −uk+1)
=
m−1

k=0
xT
k+1(Ckxk + Dkuk −uk+1) = xT
m(Cm−1xm−1 + Dm−1um−1 −um)

2.3
Riccati Equation and Quadratic Functional
115
= dT XT
m
!(Cm−1Xm−1 + Dm−1Um−1) d + AT
m(Dm −Qm+1Bm) Tmc"
= dT XT
mUmd + cT TmP T
m AT
m(Dm −Qm+1Bm) Tmc
= dT XT
mUmd + cT TmBT
m(X†
m+1)T (XT
m+1 −UT
mBT
m) (Dm −Qm+1Bm) Tmc
= dT XT
mUmd + cT Tm(BT
mDm −BT
mQm+1Bm) Tmc −dT UT
mPmTmc
= cT TmPmTmc < 0.
This contradicts the assumed nonnegativity of the functional F.
The remaining part of the proof is devoted to showing that (2.72) and (2.73) are
sufﬁcient conditions for the nonnegativity of F. Thus, let y =
x
u

be admissible
with x0 = 0 = xN+1. Then by (2.72) the sequence y satisﬁes xk ∈Im Xk for all
k ∈[0, N + 1]Z. In turn, the Picone formula (2.78) in Lemma 2.45 together with
condition (2.73) imply that
F(y) =
N

k=0
zT
k Pkzk ≥0.
The proof of Theorem 2.44 is complete.
⊓⊔
Remark 2.46 In view of the notion of multiplicities of focal points introduced in
Deﬁnition 4.1 in Sect. 4.1, the statement in Theorem 2.44 says that the functional
F is nonnegative if and only if the principal solution Y [0] of (SDS) at k = 0 has no
forward focal points in the intervals (k, k + 1) for every k ∈[0, N]Z and the image
condition (2.72) holds.
We conclude this subsection with the analysis of the image condition (2.72). It
will be further utilized in Sects. 5.4.1 and 5.4.2. We recall the deﬁnition of the matrix
Mk in (2.70) and its properties in (2.74).
Lemma 2.47 Suppose that Y is a conjoined basis of system (SDS) and ﬁx an index
k ∈[0, N]Z. Then we have
(i) the condition xk+1 ∈Im Xk+1 implies that MT
k xk+1 = 0,
(ii) the equalities xk+1 = Akxk + Bkuk, MT
k xk+1 = 0, and xk ∈Im Xk imply that
xk+1 ∈Im Xk+1.
Proof First, the condition xk+1 = Xk+1c ∈Im Xk+1 implies by (2.74) that
MT
k xk+1 = MT
k Xk+1c = 0. Hence, (i) is true. Next, if xk+1 = Akxk + Bkuk
holds together with conditions xk = Xkc ∈Im Xk and MT
k xk+1 = 0, then
0 = MT
k xk+1 = MT
k (AkXkc + Bkuk) = MT
k [Xk+1c + Bk(uk −Ukc)]
= MT
k Bk(uk −Ukc) = BT
k (I −Xk+1X†
k+1)T (I −Xk+1X†
k+1) Bk(uk −Ukc)
= MT
k Mk(uk −Ukc),

116
2
Basic Theory of Symplectic Systems
so that
0 = Mk(uk −Ukc) = (I −Xk+1X†
k+1) Bk(uk −Ukc).
Therefore, the equality
xk+1 = Xk+1c + Bk(uk −Ukc) = Xk+1c + Xk+1X†
k+1Bk(uk −Ukc) ∈Im Xk+1
holds, which proves part (ii).
⊓⊔
The following result provides a characterization of the image condition in terms
of the matrices Mk. The importance of this condition is highlighted in Theorem 2.44,
where we discuss the nonnegativity of the quadratic functional F. Note that for
x0 = 0 = xN+1, the two inclusions x0 ∈Im X0 and xN+1 ∈Im XN+1 are satisﬁed
trivially.
Lemma 2.48 Suppose that Y is a conjoined basis of the symplectic system (SDS)
and let y = (x, u) be admissible with x0 = 0 = xN+1. Then
xk ∈Im Xk
for all k ∈[1, N]Z
(2.81)
holds if and only if
MT
k xk+1 = 0
for all k ∈[0, N −1]Z.
(2.82)
Proof First, (2.81) implies (2.82) by Lemma 2.47(i). Next, suppose that y = (x, u)
is admissible, x0 = 0 = xN+1, and (2.82) holds. Then x0 = 0 ∈Im X0, and
inductively we get by Lemma 2.47(ii) that xk+1 ∈Im Xk+1 for all k ∈[0, N −1]Z,
i.e., (2.81) is satisﬁed.
⊓⊔
2.3.6
Quadratic Functionals with General Endpoints
In this subsection we present extensions of the equivalence (i) and (iv) in Theo-
rem 2.36 (positivity of F) and of Theorem 2.44 (nonnegativity of F) to quadratic
functionals with separated and jointly varying (or coupled) boundary conditions.
Thus, we consider the matrices R0, R∗
0, RN+1, R∗
N+1, 0, N+1 ∈Rn×n satisfying
R∗
0RT
0 = R0(R∗
0)T ,
rank (R∗
0, R0) = n,
R∗
N+1RT
N+1 = RN+1(R∗
N+1)T ,
rank(R∗
N+1, RN+1) = n,
0 := −R†
0R∗
0R†
0R0,
N+1 := R†
N+1R∗
N+1R†
N+1RN+1,
⎫
⎪⎬
⎪⎭
(2.83)

2.3
Riccati Equation and Quadratic Functional
117
where the matrices 0 and N+1 are symmetric. We will investigate the deﬁniteness
of the quadratic functional
G(y) := xT
0 0 x0 + xT
N+1 N+1 xN+1 + F(y),
(2.84)
over admissible sequences y = (x, u) satisfying the separated boundary conditions
x0 ∈Im RT
0 ,
xN+1 ∈Im RT
N+1,
(2.85)
where F(y) is the quadratic functional deﬁned in (2.57). We will also utilize
a special conjoined basis ¯Y =

¯X
¯U

of (SDS), called the natural conjoined basis,
which is given by the initial conditions ¯Y0 = (−R0, R∗
0)T , i.e.,
¯X0 = −RT
0 ,
¯U0 = (R∗
0)T .
(2.86)
Note that for R0 = 0 = RN+1 and R∗
0 = I = R∗
N+1 the separated boundary
conditions (2.85) reduce to the Dirichlet boundary conditions x0 = 0 = xN+1, and
the natural conjoined basis ¯Y reduces to the principal solution Y [0] at k = 0.
Theorem 2.49 (Nonnegativity for Separated Endpoints)
Assume that the con-
ditions in (2.83) hold. Then the following statement are equivalent.
(i) The quadratic functional G in (2.84) is nonnegative over the separated end-
points (2.85), i.e., G(y) ≥0 for all admissible sequences y = (x, u) satisfying
(2.85).
(ii) The natural conjoined basis ¯Y of (SDS), given by the initial conditions (2.86),
satisﬁes the following three conditions: the image condition
xk ∈Im ¯Xk,
k ∈[0, N + 1]Z,
(2.87)
for every admissible y = (x, u) satisfying (2.85), the P-condition (2.73), and
the ﬁnal endpoint inequality
N+1 + ¯UN+1 ¯X†
N+1 ≥0
on Im RT
N+1 ∩Im ¯XN+1.
(2.88)
Proof The result is proven by a matrix diagonalization argument in [50, Theorem 2].
Alternatively, we can use the Picone identity (Lemma 2.45) or the transformation
of this problem to the Dirichlet endpoints described in Sect. 5.2.2 and apply
Theorem 2.44.
⊓⊔
For the positivity of the functional G, we use the appropriate strengthening of the
conditions in Theorem 2.49.
Theorem 2.50 (Positivity for Separated Endpoints)
Assume that the conditions
in (2.83) hold. Then the following statement are equivalent.

118
2
Basic Theory of Symplectic Systems
(i) The quadratic functional G in (2.84) is positive over the separated endpoints
(2.85), i.e., G(y) > 0 for all admissible sequences y = (x, u) satisfying (2.85)
and x ̸≡0.
(ii) The natural conjoined basis ¯Y of (SDS), given by the initial conditions (2.86),
has no forward focal points in the interval (0, N + 1] and satisﬁes the ﬁnal
endpoint inequality
N+1 + ¯UN+1 ¯X†
N+1 > 0
on Im RT
N+1 ∩Im ¯XN+1.
(2.89)
Proof The result is proven by using the Picone formula (Lemma 2.32) in [50,
Theorem 2]. Alternatively, we can use the transformation of this problem to the
Dirichlet endpoints described in Sect. 5.2.2 and apply the equivalence of conditions
(i) and (iv) in Theorem 2.36.
⊓⊔
Remark 2.51 The ﬁnal endpoint inequality in (2.88) is equivalent to
¯XT
N+1(N+1 ¯XN+1 + ¯UN+1) ≥0
on Ker [(I −R†
N+1RN+1) ¯XN+1].
(2.90)
Similarly, the ﬁnal endpoint inequality in (2.89) is equivalent to (2.90) together with
the condition
Ker [R†
N+1RN+1(N+1 ¯XN+1 + ¯UN+1)]
∩Ker [(I −R†
N+1RN+1) ¯XN+1] ⊆Ker ¯XN+1.

(2.91)
These conditions are used in [174, Theorem 2] and [181, Theorem 5] (with the
matrix M := I −R†
N+1RN+1).
For the case of joint (or coupled) boundary conditions, we consider the matrices
R1, R2,  ∈R2n×2n satisfying
R1RT
2 = R2RT
1 ,
rank(R1, R2) = 2n,
 := R†
2R1R†
2R2,

(2.92)
where the matrix  is symmetric. We will investigate the deﬁniteness of the
quadratic functional
G(y) :=
 x0
xN+1
T

 x0
xN+1

+ F(y),
(2.93)
over admissible sequences y = (x, u) satisfying the joint boundary conditions
 x0
xN+1

∈Im RT
2 ,
(2.94)

2.3
Riccati Equation and Quadratic Functional
119
where F(y) is the quadratic functional deﬁned in (2.57). We note that for the choice
of R1 = diag{R∗
0, R∗
N+1} and R2 = diag{R0, RN+1} the joint boundary conditions
(2.94) reduce to the separated boundary conditions (2.85). Also, for the choice of
R1 = I2n and R2 = 02n, the joint boundary conditions (2.94) reduce to the Dirichlet
boundary conditions x0 = 0 = xN+1.
The results below are formulated in terms of the principal solution Y [0] (SDS) at
k = 0 and the associated conjoined basis ˜Y, which are given by the initial conditions
Y [0]
0
= (0, I)T ,
˜Y0 = (I, 0)T ,
(2.95)
and which form the symplectic fundamental matrix Z = ( ˜Y, Y [0]) of (SDS) with
Z0 = I2n. Then we deﬁne the 2n × 2n matrices
Xk :=

0
I
X[0]
k
˜Xk

,
Uk :=

−I
0
U[0]
k
˜Uk

,
k ∈[0, N + 1]Z.
(2.96)
Theorem 2.52 (Nonnegativity for Joint Endpoints)
Assume that the conditions
in (2.92) hold. Then the following statement are equivalent.
(i) The quadratic functional G in (2.93) is nonnegative over the joint endpoints
(2.94), i.e., G(y) ≥0 for all admissible sequences y = (x, u) satisfying (2.94).
(ii) The principal solution Y [0] of (SDS) together with the associated conjoined
basis ˜Y, given by the initial conditions (2.95), satisﬁes the following three
conditions: the image condition
xk −˜Xkx0 ∈Im X[0]
k ,
k ∈[0, N + 1]Z,
(2.97)
for every admissible y = (x, u) satisfying (2.94), the P-condition (2.73), and
the ﬁnal endpoint inequality
 + UN+1X †
N+1 ≥0
on Im RT
2 ∩Im XN+1,
(2.98)
where XN+1 and UN+1 are deﬁned in (2.96).
Proof The result is proven in [174, Theorem 2] by transforming this problem to the
separated endpoints (see Sect. 5.2.2) and applying Theorem 2.49.
⊓⊔
Theorem 2.53 (Positivity for Joint Endpoints)
Assume that the conditions in
(2.83) hold. Then the following statement are equivalent.
(i) The quadratic functional G in (2.93) is positive over the joint endpoints (2.94),
i.e., G(y) > 0 for all admissible sequences y = (x, u) satisfying (2.94) and
x ̸≡0.
(ii) The principal solution Y [0] of (SDS) has no forward focal points in the interval
(0, N + 1] and satisﬁes together with the associated conjoined basis ˜Y, given

120
2
Basic Theory of Symplectic Systems
by the initial conditions (2.95), the ﬁnal endpoint inequality
 + UN+1X †
N+1 > 0
on Im RT
2 ∩Im XN+1,
(2.99)
where XN+1 and UN+1 are deﬁned in (2.96).
Proof The result is proven in [181, Theorem 10] by transforming this problem to
the separated endpoints (see Sect. 5.2.2) and applying Theorem 2.50.
⊓⊔
Further conditions, which are equivalent to the positivity and nonnegativity of
the quadratic functionals G over the separated or joint endpoints (2.85) or (2.94),
are discussed in Sect. 2.7.
2.4
Riccati-Type Inequalities
In this section we present the results concerning inequalities involving solutions
of the Riccati difference equation (2.52) or solutions of a discrete time inequality
involving the Riccati operator Rk[Q].
2.4.1
Inequalities for Riccati-Type Quotients
This subsection contains a comparison result for solutions of two discrete symplec-
tic systems and the corresponding Riccati-type quotients. It is a generalization of
the corresponding result for linear Hamiltonian difference systems (2.15) known in
[51, Theorem 1]. Therefore, with system (2.15) we consider another system of the
same form

xk
uk

= J Hk
xk+1
uk

,
(2.100)
where
Hk = HT
k ,
Hk =
−Ck AT
k
Ak
Bk

,
det(I −Ak) ̸= 0.
(2.101)
Theorem 2.54 (Riccati Inequality for Two Hamiltonian Systems) Let us assume
that the coefﬁcients of systems (2.15) and (2.100) satisfy (2.16) and (2.101) and
Hk ≥Hk,
Bk ≥BkB†
k Bk,
Ker Bk ⊆Ker Bk,
k ∈[0, N]Z.
(2.102)
Let Y and Y be conjoined bases of systems (2.15) and (2.100), respectively, and
that Qk and Qk are symmetric with XT
k QkXk = UT
k Xk and XT
k QkXk = UT
k Xk

2.4
Riccati-Type Inequalities
121
for k ∈[0, N + 1]Z. If Im X0 ⊆Im X0 and XT
0 (Q0 −Q0) XT
0 ≥0, and if the
conjoined basis Y has no forward focal points in (0, N + 1], then Y has no forward
focal points in (0, N + 1] either and
Im Xk ⊆Im Xk,
XT
k (Qk −Qk) XT
k ≥0,
k ∈[0, N + 1]Z.
Below we present an extension of Theorem 2.54 to symplectic systems. Hence,
with system (SDS) we consider another discrete symplectic system
yk+1 = Sk yk,
(2.103)
where
ST
k J Sk = J ,
Sk =
Ak Bk
Ck Dk

.
Let F be the discrete quadratic functional, as in (2.57) and (2.63), corresponding to
symplectic system (2.103) with the associated symmetric matrices Ek and Gk, which
are deﬁned in a parallel way as the matrices Ek and Gk in (2.64). Admissible pairs
(x, u) are now deﬁned by the equation xk+1 = Akxk + Bkuk for k ∈[0, N]sZ,
and we shall emphasize this fact by saying that (x, u) is admissible with respect to
(A, B).
Theorem 2.55 (Riccati Inequality for Two Symplectic Systems) Let Y and Y be
any conjoined bases of (SDS) and (2.103), respectively. Furthermore, let Qk and
Qk be symmetric matrices such that XT
k QkXk = XT
k Uk and XT
k QkXk = XT
k Uk on
[0, N + 1]Z, and conditions
Im

Ak −Ak Bk

⊆Im Bk
k ∈[0, N]Z,
(2.104)
Im X0 ⊆Im X0,
XT
0 (Q0 −Q0) X0 ≥0,
(2.105)
Gk ≤Gk
k ∈[0, N]Z,
(2.106)
hold. If Y has no forward focal points in (0, N + 1], then Y has no forward focal
points in (0, N + 1] either, and
Im Xk ⊆Im Xk,
XT
k (Qk −Qk) Xk ≥0,
k ∈[0, N + 1]Z.
(2.107)
Before presenting the proof of Theorem 2.55, we need some preparatory
considerations. If Y is a conjoined basis of (SDS) satisfying kernel condition in
(2.37), then for any admissible (x, u) with x0 ∈Im X0, we have xk ∈Im Xk for all
k ∈[0, N + 1]Z and, by Lemma 2.32,
xT
k CT
k Akxk + 2 xT
k CT
k Bkuk + uT
k DT
k Bkuk = (xT
k ¯Qkxk) + ¯wT
k Pk ¯wk,
(2.108)

122
2
Basic Theory of Symplectic Systems
where the symmetric matrix ¯Qk := XkX†
kUkX†
k on [0, N +1]Z and the vector ¯wk :=
uk −¯Qkxk. It is interesting to observe that once formula (2.108) is established, then
it is satisﬁed with any symmetric Qk such that XT
k QkXk = XT
k ¯QkXk. In order to
see this, we let xk = Xkck for k ∈[0, N + 1]Z and some ck ∈Rn, and then
xT
k ¯Qkxk = cT
k XT
k ¯QkXkck = cT
k XT
k QkXkck = xT
k Qkxk.
(2.109)
Proof of Theorem 2.55 Let Y be a conjoined basis of (SDS) with no focal points in
(0, N + 1]. We proceed in the proof by showing the following steps.
Claim 1 For all k ∈[0, N]Z, we have the inclusions
Im(Xk+1 −AkXk) ⊆Im Bk ⊆Im Xk+1.
The ﬁrst inclusion follows from the identity
Xk+1 −AkXk = (Ak −Ak) Xk + BkUk
and from assumption (2.104). The second inclusion is equivalent to the kernel
condition Ker Xk+1 ⊆Ker Xk, by Lemma 2.15, hence it is satisﬁed.
Claim 2 For all k ∈[0, N + 1]Z we have the inclusion
Im Xk ⊆Im Xk.
(2.110)
We shall prove it by induction. By assumption (2.105), the statement holds for
k = 0. Suppose that condition (2.110) holds for some k ∈[0, N]. In this case
Xk = XkX†
kXk by (1.160), and then we have
Xk+1 = Xk+1 −AkXk + AkXkX†
kXk
= Xk+1X†
kXk + (Xk+1 −AkXk) −(Xk+1 −AkXk)X†
kXk
= Xk+1X†
kXk + (Xk+1 −AkXk) −BkUkX†
kXk.
(2.111)
The inclusion Im Xk+1 ⊆Im Xk+1 then follows from Claim 1, since each of the
three terms in (2.111) above is contained in Im Xk+1.
Claim 3 If (x, u) is admissible with respect to (A, B), then xk+1 −Akxk ∈Im Bk
for all k ∈[0, N]Z, i.e., there exist vectors {uk}N
k=0 such that (x, u) is admissible
with respect to (A, B).

2.4
Riccati-Type Inequalities
123
This is a consequence of assumption (2.104), since
xk+1 −Akxk = (Ak −Ak) xk + Bkuk ∈Im Bk.
Claim 4 If Y is a solution of (2.103), then
XT
k CT
k AkXk + 2 XT
k CT
k BkUk + UT
k DT
k BkUk = (XT
k Uk) = (XT
k QkXk).
(2.112)
This follows by a direct calculation.
Claim 5 For all k ∈[0, N] we have XT
k (Qk −Qk) Xk ≥0, i.e., condition (2.107)
holds.
We will show that [XT
k (Qk −Qk) Xk] ≥0 for k ∈[0, N]Z, which together
with initial condition (2.105) imply the statement. Let c ∈Rn be arbitrary and put
xk := Xkc and uk := Ukc on [0, N + 1]Z. Then (x, u) is admissible with respect to
(A, B) and
cT [XT
k (Qk −Qk) Xk] c = cT (XT
k Uk) c −cT (XT
k QkXk) c
(2.112), (2.63)
=
 xk
xk+1
T
Gk
 xk
xk+1

−(xT
k Qkxk).
(2.113)
By Claim 3, there exists u = {uk}N
k=0 such that (x, u) is admissible with respect to
(A, B), while Claim 2 yields that xk ∈Im Xk for k ∈[0, N + 1]Z. Hence, we get
(xT
k Qkxk)
(2.109)
=
(xT
k ¯Qkxk)
(2.108)
=
 xk
xk+1
T
Gk
 xk
xk+1

−¯wT
k Pk ¯wk.
Using this identity in formula (2.113) and using assumption (2.106), we obtain
cT [XT
k (Qk −Qk) Xk] c =
 xk
xk+1
T
(Gk −Gk)
 xk
xk+1

+ ¯wT
k Pk ¯wk ≥0,
where we used the fact that Pk = Pk = XkX†
k+1Bk ≥0, since Y is assumed to have
no focal points in (0, N + 1].
Claim 6 The conjoined basis Y has no focal points in (0, N + 1].
We shall prove this via Theorem 2.39. Let (x, u) be admissible with respect to
(A, B) with x0 = X0d for some d ∈Rn, xN+1 = 0, and x ̸≡0. Then, by Claim 3,
there is u = {uk}N
k=0 such that (x, u) is admissible with respect to (A, B) and, by

124
2
Basic Theory of Symplectic Systems
assumption (2.105), x0 = X0d for some d ∈Rn. Hence, we have
F(x, u) + dT XT
0 U0d
(2.63)
=
N

k=0
 xk
xk+1
T
Gk
 xk
xk+1

+ dT XT
0 Q0X0d
(2.106), (2.105)
≥
N

k=0
 xk
xk+1
T
Gk
 xk
xk+1

+ dT XT
0 Q0X0d
(2.63)
= F(x, u) + dT XT
0 U0d > 0,
since Y is assumed to have no focal points in (0, N + 1]. Thus, by Theorem 2.39,
the conjoined basis Y has no focal points in (0, N +1]. This theorem is now proven.
⊓⊔
When the two systems (SDS) and (2.103) are the same, we obtain from
Theorem 2.55 the following Riccati inequality.
Corollary 2.56 (Inequality for One Symplectic System) Let Y and Y be conjoi-
ned bases of (SDS), and let Qk and Qk be symmetric n × n matrices such that
XT
k QkXk = XT
k Uk and XT
k QkXk = XT
k Uk on [0, N + 1]Z, and conditions (2.105)
hold. If Y has no forward focal points in (0, N + 1], then Y has no forward focal
points in (0, N + 1] either, and inequality (2.107) is satisﬁed.
2.4.2
Discrete Riccati Inequality
In Theorems 2.36 and 2.40, we presented several conditions which are equivalent to
the positivity of the discrete quadratic functional F in (2.57), which are analogous
to the continuous time conditions (i)–(iv) in Theorem 1.47. However, in contrast
with (1.123), the discrete Riccati operator Rk[Q] deﬁned in (2.52) is not symmetric
even when Qk is symmetric. This creates a difﬁculty in understanding what should
be the discrete version of the Riccati inequality in condition (v) of Theorem 1.47. In
this subsection, we answer this question and derive a complete discrete time analog
of the latter result.
Theorem 2.57 (Discrete Riccati Inequality) The functional F is positive deﬁnite
if and only if either of the following equivalent conditions is satisﬁed.
(i) The system
Xk+1 = AkXk + BkUk,
Nk := XT
k+1(Uk+1 −CkXk −DkUk) ≤0,

(2.114)
k ∈[0, N]Z, has a solution Y = X
U
 such that XT
k Uk is symmetric and Xk is
invertible for all k ∈[0, N + 1]Z and XkX−1
k+1Bk ≥0 on [0, N]Z.

2.4
Riccati-Type Inequalities
125
(ii) The discrete Riccati inequality
Rk[Q] (Ak + BkQk)−1 ≤0,
(2.115)
k ∈[0, N]Z, has a symmetric solution Qk deﬁned on [0, N + 1]Z satisfying the
conditions in (2.68), i.e., Ak + BkQk is invertible and (Ak + BkQk)−1Bk ≥0
for all k ∈[0, N]Z.
The proof of Theorem 2.57 is shown below after the following auxiliary result.
Lemma 2.58 Let k ∈[0, N]Z be ﬁxed and assume that Xj and Uj are n × n
matrices for j ∈[k, k + 1]Z such that Xk+1 = AkXk + BkUk. Then the following
conditions hold.
(i) If XT
j Uj is symmetric for j ∈[k, k + 1]Z, then the matrix
XT
k+1(Uk+1 −CkXk −DkUk)
= (XT
k Uk) −(XT
k CT
k AkXk + 2 XT
k CT
k BkUk + UT
k DT
k BkUk)
is symmetric as well.
(ii) If XT
k+1Uk+1 is symmetric and if Qj is symmetric with QjXj = UjX†
jXj for
j ∈[k, k + 1]Z, then
XT
k+1Rk[Q] Xk = XT
k+1(Uk+1 −CkXk −DkUk) X†
kXk.
(2.116)
(iii) If XT
j Uj and Qj are symmetric with QjXj = UjX†
jXj for j ∈[k, k + 1]Z
and if Xk is invertible, then the matrix
XT
k+1Rk[Q] Xk = XT
k+1(Uk+1 −CkXk −DkUk)
(2.117)
is symmetric.
Proof Part (i) is a simple calculation. For part (ii), we ﬁrst derive
Rk[Q] Xk = [Qk+1Xk+1 −(CkXk + DkUk)] X†
kXk
= [Uk+1 −CkXk −DkUk −Uk+1(I −X†
k+1Xk+1)] X†
kXk.
Then, after multiplying by XT
k+1 from the left and by using the symmetry of
XT
k+1Uk+1, we obtain (2.116). Identity (2.117) in part (iii) then follows directly
from (i) and (ii).
⊓⊔
Proof of Theorem 2.57 If not speciﬁed otherwise, conditions (i) and (ii) refer to
Theorem 2.57. By Theorem 2.40, the positivity of F implies condition (i) trivially
with Nk ≡0. Condition (i) implies (ii) by the Riccati substitution Qk := UkX−1
k
on

126
2
Basic Theory of Symplectic Systems
[0, N + 1]Z. Next, we show that condition (ii) implies (i). Let
Fk := Rk[Q] (Ak + BkQk)−1 ≤0
be the matrix deﬁning the inequality (2.115), where Qk satisﬁes condition (ii). Let
X be the solution of the equation Xk+1 = (Ak + BkQk) Xk, k ∈[0, N]Z, given
by the initial condition X0 = I. Then Xk is invertible on [0, N + 1]k. If we set
Uk := QkXk on [0, N + 1]Z, then (X, U) satisﬁes Xk+1 = AkXk + BkUk and
Nk = XT
k+1[Qk+1 −(Ck + DkQk) XkX−1
k+1] Xk+1 = XT
k+1FkXk+1 ≤0
for all k ∈[0, N]sZ, that is, (X, U) solves system (2.114). Note that the matrices
Nk and Fk = XT −1
k+1 NkX−1
k+1 are symmetric, by Lemma 2.58(iii).
The rest of the proof is about showing that condition (i) implies the positivity of
the functional F. With Nk as in (2.114) we put Fk := XT −1
k+1 NkX−1
k+1 ≤0. Deﬁne
Ak := Ak, Bk := Bk, Ck := Ck + FkAk, Dk := Dk + FkBk, and
Sk :=
Ak Bk
Ck Dk

= Sk + Rk
with Rk :=

0
0
FkAk FkBk

.
(2.118)
The proof will be ﬁnished by showing the following claims.
Claim 1 The matrix Sk is symplectic.
This follows from the observation that ST
k J Rk = 
Ak Bk
T Fk

Ak Bk
 is
symmetric, RT
k J Rk = 0, and from the calculation
ST
k J Sk = (Sk + Rk)TJ (Sk + Rk) = ST
k J Sk + ST
k J Rk + RT
k J Sk + RT
k J Rk
= J + RT
k J T Sk + RT
k J Sk = J .
Claim 2 The pair (X, U) solves the system (2.103); hence it is a conjoined basis of
(2.103) with no focal points in (0, N + 1]. This follows from the invertibility of Xk
on [0, N + 1]Z, the calculations
AkXk + BkUk = AkXk + BkUk = Xk+1
CkXk + DkUk = CkXk + DkUk + Fk(AkXk + BkUk)
= CkXk + DkUk + XT −1
k+1 Nk = Uk+1,
and from P k := XkX−1
k+1Bk = XkX−1
k+1Bk ≥0.
Claim 3 The functional F is positive deﬁnite. We have by Theorem 2.40 applied
to (2.103) that the functional F is positive deﬁnite. Next, the deﬁnition of Ak and
Bk implies that Im

Ak −Ak Bk

= Im Bk = Im Bk. Furthermore, the symmetric

2.4
Riccati-Type Inequalities
127
matrix Ek := Ek + Fk satisﬁes
DT
k Bk = DT
k Bk + BT
k FkBk = BT
k (Ek + Fk) Bk = BT
k EkBk,
Gk −Gk =
AT
k EkAk −AT
k Ck CT
k −AT
k Ek
Ck −EkAk
Ek

−
AT
k (Ek + Fk) Ak −AT
k (Ck + FkAk) CT
k + AT
k Fk −AT
k (Ek + Fk)
Ck + FkAk −(Ek + Fk) Ak
Ek + Fk

=
0
0
0 −Fk

≥0.
Consequently, the conditions
Im 
Ak −Ak Bk
 ⊆Im Bk,
Gk ≤Gk,
k ∈[0, N]Z,
are satisﬁed, compare with conditions (2.104)–(2.106) in which the roles of systems
(SDS) and (2.103) are interchanged. Therefore, by Theorem 2.55 in this latter
context with Y k := Yk and Qk = Qk = UkX−1
k
on [0, N + 1]Z, we obtain that Y is
a conjoined basis of (SDS) with no forward focal points in (0, N +1]. Consequently,
by Theorem 2.40, the positivity of F follows, and the proof of this theorem is now
complete.
⊓⊔
Remark 2.59 In the proof above, we used a matrix Sk of the form Sk = Sk + Rk
with Rk =
 0
0
Gk Hk

. This matrix Sk is symplectic if and only if GT
k Ak and H T
k Bk
are symmetric, and the identity H T
k Ak = BT
k Gk holds. The choice Gk := FkAk
and Hk := FkBk with symmetric Fk is then natural, which was ﬁrst observed in
[55] in connection with an eigenvalue problem associated with system (SDS), see
Sect. 5.3.
Riccati inequality is often used in nonoscillation criteria for differential and
difference equations, since it is easier to ﬁnd a solution of the inequality (which
corresponds to a solution of some majorant equation) than a solution to the equality.
In the following examples, we show a situation when a symmetric Qk solves the
Riccati inequality (2.115) and satisﬁes condition (ii) in Theorem 2.57, but it does
not solve the Riccati equation (2.52) so that condition (iii) in Theorem 2.40 is not
satisﬁed with this Qk.
Example 2.60
(i) Let Ak ≡0, Bk ≡−CT −1, Ck ≡C, Dk ≡−CT −1−C−K, where C is a constant
nonsingular matrix, K ̸= 0, and CKT = KCT ≥0. Then Qk ≡I satisﬁes
condition (ii) in Theorem 2.57, since Ak +BkQk = −CT −1 is invertible, (Ak +
BkQk)−1Bk ≡I > 0, and Rk[Q] (Ak + BkQk)−1 = −KCT ≤0, while the
Riccati equation is Rk[Q] = K ̸= 0. Another (more speciﬁc) example can

128
2
Basic Theory of Symplectic Systems
be obtained when we take, e.g., C = K = I. Note also that since Ak is not
invertible, the Hamiltonian Riccati inequality in [176, Corollary 4.1] cannot be
applied to this setting.
(ii) Let Ak and Ck be invertible, Bk ≡0, and Dk = AT −1
k
, with CT
k Ak > 0. Then
Qk ≡0 satisﬁes condition (ii) in Theorem 2.57, since Ak + BkQk = Ak is
invertible, (Ak+BkQk)−1Bk ≡0, and Rk[Q] (Ak+BkQk)−1 = −CkA−1
k
< 0,
while the Riccati equation is Rk[Q] = −Ck ̸= 0.
2.5
Recessive and Dominant Solutions
In this section we develop the notions of a recessive and dominant solution
for a nonoscillatory symplectic system (SDS) under the eventual controllability
condition. These concepts extend to the matrix case the corresponding notions for
the second-order equation (1.1) in Sect. 1.2. We note that the exposition in this
subsection differs from the presentation of recessive and dominant solutions in [16,
Sections 3.11–3.16]. For controllable systems, these two approaches are however
equivalent (see Remark 2.68).
2.5.1
Deﬁnitions and Basic Properties
First we note that the nonoscillation of (SDS) at ∞(Deﬁnition 2.19) means that
every conjoined basis Y of (SDS) does not have eventually any focal points, i.e.,
there exists N ∈[0, ∞)Z such that condition (2.37) holds for every k ∈[N, ∞)Z.
This implies that for every conjoined basis Y of a nonoscillatory system (SDS),
the kernel of Xk is eventually constant.
(2.119)
System (SDS) is said to be (completely) controllable on some discrete interval
[N, ∞)Z if for any subinterval [N1, N2]Z of [N, ∞)Z with at least two points the
trivial solution (x, u) ≡(0, 0) is the only solution of (SDS) for which xk ≡0
on [N1, N2]Z. System (SDS) is eventually (completely) controllable if there exists
N ∈[0, ∞)Z such that it is (completely) controllable on [N, ∞)Z.
The eventual controllability of (SDS) in combination with property (2.119)
means the matrix Xk is eventually invertible. That is, if the system (SDS) is
nonoscillatory at ∞and eventually controllable, then for every conjoined basis Y,
there exists N ∈Z such that
Xk is invertible and XkX−1
k+1Bk ≥0 for all k ∈[N, ∞)Z.
(2.120)

2.5
Recessive and Dominant Solutions
129
Remark 2.61
(i) The above condition of eventual controllability is satisﬁed, for example, by
systems (SDS) with Bk invertible for large k, such as equation (1.1). Other types
of such systems are discussed in Sect. 2.1.2. Equation (2.27) is an example of
a controllable system with singular Bk.
(ii) On the other hand, system (SDS) with Bk ≡0 is not eventually controllable.
In this case we have by (1.146) that Ak and Dk = AT −1
k
are invertible and
the ﬁrst equation of (SDS) has the form xk+1 = Akxk. This means that if Y is
a conjoined basis with singular XN, then Xk is singular for all k ∈[N, ∞)Z.
Under (2.120) we may associate with the conjoined basis Y the symmetric matrix
Sk :=
k−1

j=N
X−1
j+1BjXT −1
j
,
k ∈[N + 1, ∞)Z,
SN := 0.
(2.121)
The matrix Sk is symmetric, positive semideﬁnite, and nondecreasing in k, since
Sk = X−1
k+1BkXT −1
k
= X−1
k
(XkX−1
k+1Bk) XT −1
k
≥0,
k ∈[N, ∞)Z.
(2.122)
In fact, we shall prove a stronger result under the complete controllability assump-
tion on [N, ∞)Z.
Lemma 2.62 Assume that system (SDS) is nonoscillatory at ∞and eventually
completely controllable. Then for any conjoined basis Y of (SDS), there exists
N ∈[0, ∞)Z such that the matrix Sk deﬁned in (2.121) is positive deﬁnite for all
k ∈[N + 2, ∞)Z.
Proof The assumptions imply that for some N ∈[0, ∞)Z, condition (2.120) holds.
Assume that Skd = 0 for some k ∈[N + 2, ∞)Z. Then X−1
j+1BjXT −1
j
d ≡0 for
all j ∈[N, k −1]Z, i.e., BjXT −1
j
d ≡0 for all j ∈[N, k −1]Z. We now deﬁne
xj := 0, uj := XT −1
j
d, and Qj := UjX−1
j
for j ∈[N, k]Z. It follows that for every
j ∈[N, k −1]Z we have Bjuj = 0 and, by (2.55) in Corollary 2.27,
uj+1 = XT −1
j+1 d = (AjXj + BjUj)T −1d = (AT
j + QjBT
j ) XT −1
j
d
(2.55)
=
(Dj −Qj+1Bj) uk = Djuj.
This implies that the pair (x ≡0, u) solves system (SDS) on the interval [N, k−1]Z.
By the complete controllability assumption, we obtain that uj ≡0 on [N, k]Z, i.e.,
d = 0. This shows that the matrix Sk is invertible, hence positive deﬁnite.
⊓⊔

130
2
Basic Theory of Symplectic Systems
The result in Lemma 2.62 yields that for any conjoined basis Y of a nonoscilla-
tory and eventually controllable system (SDS), the matrix S−1
k
eventually exists as
a positive deﬁnite matrix, which is nonincreasing in k. Therefore, there exists the
limit
T := lim
k→∞S−1
k ,
T ≥0.
(2.123)
Depending on the form of the limiting matrix T , we make the following deﬁnition:
Deﬁnition 2.63 A conjoined basis ˜Y of a nonoscillatory and eventually controllable
system (SDS) is called a recessive solution at ∞if the matrix T deﬁned in (2.123)
satisﬁes T = 0, while it is called a dominant solution at ∞if T > 0.
Remark 2.64 One can easily see from the limit properties of invertible matrices that
Y is a dominant solution at ∞if and only if the limit of Sk itself exists as k →∞,
and in this case
lim
k→∞Sk = T −1.
The following reduction of order theorem will be useful for the construction and
properties of recessive and dominant solutions of (SDS).
Theorem 2.65 Assume that Y is a conjoined basis of (SDS) such that Xk is
invertible on some interval [N, M]Z. Then ˜Y is a solution of (SDS) if and only
if for some constant matrices K and L, we have
˜Xk = Xk (K+SkL),
˜Uk = Uk (K+SkL)+XT −1
k
L,
k ∈[N, M]Z,
(2.124)
where the matrix Sk is deﬁned in (2.121). In this case K = X−1
N ˜XN and L =
Y TJ ˜Y. Moreover, ˜Y is also a conjoined basis of (SDS) if and only if KT L is
symmetric.
Proof The proof is rather straightforward in both implications. The details can be
found in [16, Theorem 3.32, pp. 105–109].
⊓⊔
The following two results justify the existence of recessive and dominant
solutions at ∞for a nonoscillatory system (SDS), as well as the uniqueness of the
recessive solution at ∞. They provide an extension of Theorem 1.7 to symplectic
systems.
Theorem 2.66 Assume that system (SDS) is nonoscillatory at ∞and eventually
controllable. Then there exists a recessive solution of (SDS) at ∞. In addition, the
recessive solution is unique up to a right constant nonsingular multiple.
Proof Consider the principal solution ˆY at k = 0, i.e., ˆX0 = 0 and ˆU0 = I. Then for
some N ∈[1, ∞)Z, the matrix ˆXk satisﬁes (2.120), and we may deﬁne the matrices
ˆSk and ˆT by (2.121) and (2.123) through ˆXk. Let ¯Y be the conjoined basis of (SDS)

2.5
Recessive and Dominant Solutions
131
given by the initial conditions ¯XN = 0 and ¯UN = ˆXT −1
N
. Then ˆY T
N J ¯YN = I, so
that the conjoined bases ˆY and ¯Y are normalized. We will show that the sequence
 ˜Xk
˜Uk

:=
 ˆXk ¯Xk
ˆUk ¯Uk
  I
−ˆT

,
k ∈[0, ∞)Z,
(2.125)
is a conjoined basis of (SDS) and that ˜Y is a recessive solution of (SDS) at ∞. By
Theorem 2.65 (with K = 0 and L = I) we know that
¯Xk = ˆXk ˆSk,
¯Uk = ˆUk ˆSk + ˆXT −1
k
,
k ∈[N, ∞)Z.
(2.126)
Then, substituting (2.126) into (2.125), we obtain that
˜Xk = ˆXk(I −ˆSk ˆT ),
˜Uk = ˆUk(I −ˆSk ˆT ) −ˆXT −1
k
ˆT ,
k ∈[N, ∞)Z,
(2.127)
so that by Theorem 2.65 (with Y := ˆY, K = I, and L = −ˆT ), we conclude that
˜Y is a conjoined basis of (SDS). The assumptions of the theorem imply that ˜Xk is
invertible for all k ∈[L, ∞)Z for some index L ∈[N, ∞)Z. Then by (2.127) we
know that also I −ˆSk ˆT is invertible for k ∈[L, ∞)Z. Therefore, for the symmetric
matrices Gk := ˆT (I −ˆSk ˆT ), k ∈[N, ∞)Z, we have
Gk ≥0,
k ∈[N, ∞)Z,
{Gk}∞
k=N is nonincreasing on[N, ∞)Z,
(2.128)
Im Gk = Im [ ˆT (I −ˆSk ˆT )] = Im ˆT ,
k ∈[L, ∞)Z.
(2.129)
But from the deﬁnition of Gk, we also have
Im Gk = Im [ ˆT (I −ˆSk ˆT )] ⊆Im ˆT ,
k ∈[N, L −1]Z.
(2.130)
Hence, from (2.128)–(2.130) we obtain that Im Gk = Im ˆT for all k ∈[N, ∞)Z. By
taking the orthogonal complements (using the symmetry of Gk and ˆT ), we get
Ker [ ˆT (I −ˆSk ˆT )] = Ker Gk = Ker ˆT ,
k ∈[N, ∞)Z.
(2.131)
Assume now that ˜Xkd = 0 for some k ∈[N, ∞)Z and d ∈Rn. It follows by (2.127)
and by the invertibility of ˆXk that (I −ˆSk ˆTk) d = 0, i.e., d = ˆSk ˆTkd. At the same
time, Gkd = ˆT (I −ˆSk ˆTk) d = 0, so that d ∈Ker Gk = Ker ˆT by (2.131). Therefore,
d = ˆSk ˆTkd = 0. This shows that the matrix ˜Xk is invertible for all k ∈[N, ∞)Z.
Deﬁne the associated matrix ˜Sk as in (2.121) through ˜Xk in [N, ∞)Z. Since
˜Y T
N J ¯YN = I, we get by Theorem 2.65 (with K = 0 and L = I again) that
¯Xk = ˜Xk ˜Sk,
¯Uk = ˜Uk ˜Sk + ˜XT −1
k
,
k ∈[N, ∞)Z.
(2.132)

132
2
Basic Theory of Symplectic Systems
Combining the ﬁrst equations in (2.126) and (2.132) yields the equality ˆXk ˆSk =
˜Xk ˜Sk on [N, ∞)Z. Therefore, together with the expression for ˜Xk in (2.127), we
obtain that (note that ˆSk and ˜Sk are invertible for k ≥N + 2 by Lemma 2.62)
˜S−1
k
= ˆS−1
k
ˆX−1
k
˜Xk = ˆS−1
k (I −ˆSk ˆT ) = ˆS−1
k
−ˆT ,
k ∈[N + 2, ∞)Z.
Thus, the matrix ˜T deﬁned in (2.121) through ˜Sk satisﬁes
˜T = lim
k→∞
˜S−1
k
= lim
k→∞( ˆS−1
k
−ˆT ) = 0,
which shows that ˜Y is a recessive solution of (SDS) at ∞according to Deﬁni-
tion 2.63.
Now we prove the essential uniqueness of the recessive solution ˜Y. Let ˜Y (0)
be another recessive solution of (SDS) at ∞. Let N ∈[0, ∞)Z be an index such
that (2.120) holds for ˜Xk and ˜X(0)
k . Let ˜Sk and ˜S(0)
k
be the associated matrices
deﬁned in (2.121) through ˜Xk and ˜X(0)
k , respectively, and let ˜T and ˜T (0) be the
matrices in (2.123), which according to Deﬁnition 2.63 satisfy ˜T = 0 = ˜T (0). By
Theorem 2.65, we know that
˜X(0)
k
= ˜Xk(K + ˜SkL),
˜U(0)
k
= ˜Uk(K + ˜SkL) + ˜XT −1
k
L,
k ∈[N, ∞)Z,
(2.133)
where K = ˜X−1
N ˜X(0)
N is regular and L = ˜Y TJ ˜Y (0) is the Wronskian of ˜Y and ˜Y (0).
By [284, Proposition 4.20], the matrices ˜Sk and ˜S(0)
k
are related by the formula
 ˜S(0)
k
−1 = KT ˜S−1
k K + KT L,
which means upon taking the limit as k →∞that
0 = ˜T (0) = KT ˜T K + KT L = KT L.
Since K is regular, it follows that L = 0, and hence by (2.133) we obtain ˜Y (0)
k
=
˜YkK on [N, ∞)Z. Thus, the recessive solution ˜Y (0) is a regular constant multiple
of the recessive solution ˜Y. Conversely, if ˜Y is a recessive solution of (SDS) at ∞,
satisfying (2.120), and if K is a regular n × n matrix, then we deﬁne the conjoined
basis ˜Y (0) by ˜Y (0)
k
:= ˜YkK on [N, ∞)Z. Then ˜X(0)
k
is invertible on [N, ∞)Z and
 ˜X(0)
k+1
−1Bk
 ˜X(0)
k
T −1 = K−1 ˜X−1
k+1B ˜XT −1
k
KT −1 ≥0
˜S(0)
k
= K−1 ˜Sk KT −1,

k ∈[N, ∞)Z,

2.5
Recessive and Dominant Solutions
133
so that
˜T (0) = lim
k→∞
 ˜S(0)
k
−1 = lim
k→∞KT ˜S−1
k
K = KT ˜T K = 0.
Hence, ˜Y (0) is also a recessive solution of (SDS) according to Deﬁnition 2.63. The
proof is complete.
⊓⊔
The next result shows that the recessive solution at ∞is the smallest solution
among all conjoined bases Y of (SDS) which are linearly independent with ˜Y,
measured in terms of their ﬁrst components ˜Xk and Xk. This property extends the
limit (1.18) to symplectic systems.
Theorem 2.67 Assume that system (SDS) is nonoscillatory at ∞and eventually
controllable. Let ˜Y and Y be two conjoined bases of (SDS) such that the Wronskian
W := w( ˜Y, Y) = ˜Y TJ Y is invertible. Then ˜Y is the recessive solution of (SDS) at
∞if and only if
lim
k→∞X−1
k
˜Xk = 0.
(2.134)
In this case Y is a dominant solution of (SDS) at ∞.
Proof Let N ∈[0, ∞)Z be an index such that ˜Xk and Xk satisfy (2.120). Let ˜Sk,
˜T and Sk, T be the matrices in (2.121) and (2.123) associated with ˜Xk and Xk,
respectively. Then by Theorem 2.65 (with the invertible matrix K := X−1
N ˜XN and
with L := −W T ), we have
˜Xk = Xk(K −SkW T ),
Xk = ˜Xk(K−1+ ˜SkW),
k ∈[N, ∞)Z.
(2.135)
Assume that ˜Y is the recessive solution of (SDS) at ∞, i.e., ˜T = 0. Then (2.135)
implies that K−1 + ˜SkW =
˜X−1
k Xk is invertible on [N, ∞)Z, as well as ˜Sk is
invertible on [N + 2, ∞)Z by Lemma 2.62. Hence,
lim
k→∞X−1
k
˜Xk = lim
k→∞(K−1 + ˜SkW)−1 = lim
k→∞( ˜S−1
k K−1 + W)−1 ˜S−1
k
= ( ˜T K−1 + W)−1 ˜T = 0.
Conversely, assume that (2.134) holds. The second equation in (2.135) implies that
˜Sk = ( ˜X−1
k Xk −K−1) W −1 on [N, ∞)Z, so that
˜T = lim
k→∞
˜S−1
k
= lim
k→∞W( ˜X−1
k Xk −K−1)−1
= lim
k→∞W(I −X−1
k
˜XkK−1)−1X−1
k
˜Xk
(2.134)
=
0.

134
2
Basic Theory of Symplectic Systems
This proves that ˜Y is the recessive solution of (SDS) at ∞. Finally, knowing that ˜Y
is the recessive solution of (SDS) at ∞, then from (2.135), we obtain
K −SkW T = X−1
k
˜Xk = (K−1+ ˜SkW)−1,
k ∈[N, ∞)Z,
so that
Sk = [K −( ˜S−1
k K−1 + W)−1 ˜S−1
k ] W T −1,
k ∈[N + 2, ∞)Z.
Upon taking the inverse and using that ˜T = 0, we get
T = lim
k→∞S−1
k
= lim
k→∞W T [K −( ˜S−1
k K−1 + W)−1 ˜S−1
k ]−1
= W T [K −( ˜T K−1 + W)−1 ˜T ]−1 = W T K−1.
Therefore, the matrix T is invertible. At the same time, we know that T ≥0 by
(2.123), so that T > 0 follows. Hence, Y is a dominant solution of (SDS) at ∞
according to Deﬁnition 2.63. The proof is complete.
⊓⊔
Remark 2.68 The result in Theorem 2.67 shows that the notions of a recessive
solution at ∞for a nonoscillatory and completely controllable system (SDS) in
Deﬁnition 2.63 and in [16, pg. 115] are equivalent.
Remark 2.69 The results in Theorems 2.66 and 2.67 show that, under the eventual
controllability assumption, the nonoscillation of (SDS) implies the existence of the
recessive solution of (SDS) at ∞, as well as the existence of a dominant solution of
(SDS) at ∞. However, from the quantitative results on focal points (i.e., Sturmian
separation theorems) presented later in Sects. 4.2.4 and 5.5.1, it follows that also
the converse to Theorem 2.66 holds. Namely, the existence of a nonoscillatory
conjoined basis of (SDS) at ∞implies that every conjoined basis of (SDS) is also
nonoscillatory at ∞, i.e., system (SDS) is nonoscillatory at ∞.
2.5.2
Minimal Solution of Riccati Equation
The recessive solution of (SDS) gives rise to a minimal solution of the associated
Riccati difference equation (2.51). This statement extends Theorem 1.8 to symplec-
tic systems.
Theorem 2.70 Assume that (SDS) is nonoscillatory and eventually controllable,
and let ˜Y be its recessive solution at ∞. Then the solution ˜Qk := ˜Uk ˜X−1
k
of the
Riccati equation (2.51) is eventually minimal, i.e., if Qk is any symmetric solution
of (2.51) such that (Ak + BkQk)−1Bk ≥0 for all k in some interval [N, ∞)Z, then
Qk ≥˜Qk
for all k ∈[N, ∞)Z.
(2.136)

2.6
Transformations of Symplectic Systems
135
Proof Let ˜Qk and Qk be as in the theorem. Then the matrices Ak + ˜QkBk and
Ak + QkBk are invertible on [N, ∞)Z by Corollary 2.27. By Theorem 2.28 we
associate with Qk a conjoined basis Y of (SDS) such that Xk is invertible and Qk =
UkX−1
k
on [N, ∞)Z. Then Y satisﬁes condition (2.120) by the assumptions in the
theorem. Denote by W := w( ˜Y, Y) = ˜Y TJ Y the (constant) Wronskian of ˜Y and
Y. Then for k ∈[N, ∞)Z we have
Qk −˜Qk = UkX−1
k
−˜Uk ˜X−1
k
= ˜XT −1
k
WX−1
k
= ˜XT −1
k
(WX−1
k
˜Xk) ˜X−1
k .
(2.137)
Consider now the symmetric matrix function WX−1
k
˜Xk on [N, ∞)Z. Then
(WX−1
k
˜Xk) = W(X−1
k+1 ˜Xk+1 −X−1
k
˜Xk)
= W [X−1
k+1(Ak ˜Xk + Bk ˜Uk) −X−1
k+1(AkXk + BkUk) X−1
k
˜Xk]
= WX−1
k+1Bk( ˜Uk −UkX−1
k
˜Xk) = −WX−1
k+1BkXT −1
k
W T ≤0,
so that the function WX−1
k
˜Xk is nonincreasing on [N, ∞)Z. At the same time,
we know from Theorem 2.67 that WX−1
k
˜Xk →0 as k →∞. This implies that
WX−1
k
˜Xk ≥0 on [N, ∞)Z and hence, equation (2.137) yields that Qk −˜Qk ≥0
for all k ∈[N, ∞)Z. Therefore, (2.136) is satisﬁed.
⊓⊔
2.6
Transformations of Symplectic Systems
Transformation approach to various differential and difference equations is histori-
cally a very effective method. The basic idea is simple. An equation is transformed
into an “easier” equation, this equation is then investigated, and ﬁnally the obtained
results are transformed back to the original equation. Of course, important are
invariants of the applied transformations. In the investigation of symplectic systems,
the situation is similar, and naturally the main role will be played by symplectic
transformations.
2.6.1
General Symplectic Transformation
Our ﬁrst result shows that symplectic transformations preserve the type (i.e., the
symplectic property) of the underlying system.
Lemma 2.71 Let Rk ∈R2n×2n for k ∈[0, N + 1]Z be a sequence of symplectic
matrices, and consider the transformation
yk = Rkwk.
(2.138)

136
2
Basic Theory of Symplectic Systems
Then (2.138) transforms system (SDS) into another symplectic system
wk+1 = ˜Skwk,
˜Sk := R−1
k+1SkRk = −J RT
k+1J SkRk,
(2.139)
where the matrix ˜Sk is symplectic for k ∈[0, N]Z. Moreover, Yk is a conjoined basis
of (SDS) if and only if ˜Yk = R−1
k Yk is a conjoined basis of (2.139).
Proof By (2.138) we have
yk+1 = Rk+1wk+1 = Skyk = SkRkwk.
Hence, the new variable w is a solution of the system (2.139). Moreover, the matrix
˜Sk is symplectic, being a product of symplectic matrices (see Lemma 1.58(i)).
Recall that the multiplication by a symplectic (hence invertible) matrix preserves
both conditions, which deﬁne a conjoined basis (i.e., the rank condition and the
symmetry of ˜XT
k ˜Uk), see Lemma 1.58(vii). This property proofs the last statement
of the lemma.
⊓⊔
Sometimes we will need to know the block structure of the matrix ˜Sk in the
transformed system (2.139). Hence, consider the transformation matrix Rk in the
form
Rk =
Hk Mk
Gk Nk

,
(2.140)
and denote by
˜Ak, ˜Bk, ˜Ck, ˜Dk the block in the matrix
˜Sk. Then by a direct
computation, we have the formulas
˜Ak = NT
k+1(AkHk + BkGk) −MT
k+1(CkHk + DkGk),
˜Bk = NT
k+1(AkMk + BkNk) −MT
k+1(CkMk + DkNk),
˜Ck = −GT
k+1(AkHk + BkGk) + H T
k+1(CkHk + DkGk),
˜Dk = −GT
k+1(AkMk + BkNk) + H T
k+1(CkMk + DkNk).
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(2.141)
Summarizing, a linear transformation with a symplectic transformation matrix
transforms a symplectic system again into a symplectic systems. In our treatment,
we will mainly concentrate on oscillatory properties of symplectic systems, and the
above-described transformation generally does not preserve an oscillatory nature
of the transformed systems. The oscillatory properties are preserved, when the
transformation matrix is lower block triangular, i.e., when Mk = 0 (see Lemma 2.72
below). In this case, the transformation matrix has the form
Rk =
Hk
0
Gk H T −1
k

,
(2.142)

2.6
Transformations of Symplectic Systems
137
and the formulas for ˜Ak, ˜Bk, ˜Ck, ˜Dk in (2.141) simplify to
˜Ak = H −1
k+1(AkHk + BkGk),
˜Bk = H −1
k+1BkH T −1
k
,
˜Ck = −GT
k+1(AkHk + BkGk) + H T
k+1(CkHk + DkGk),
˜Dk = −GT
k+1BkH T −1
k
+ H T
k+1DkH T −1
k
.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(2.143)
Lemma 2.72 Suppose that Y is a conjoined basis of (SDS) and ˜Yk =
 ˜Xk
˜Uk

=
R−1
k Yk, where Rk is a lower block-triangular symplectic matrix. Then Y has no
focal point in (k, k + 1] if and only if ˜Y has no focal point in this interval.
Proof Obviously, we have Ker Xk+1 ⊆Ker Xk if and only if Ker ˜Xk+1 ⊆Ker ˜Xk
and, by (2.39), Bk = Xk+1X†
k+1Bk or ˜Bk = H −1
k+1Xk+1X†
k+1BkH T −1
k
according to
(2.143). Substituting the last representation for ˜Bk into ˜Pk := ˜Xk ˜X†
k+1 ˜Bk and using
Lemma 1.63, we have
˜Pk = ˜Xk ˜X†
k+1 ˜Bk = H −1
k Xk(H −1
k+1Xk+1)†H −1
k+1Xk+1X†
k+1BkH T −1
k
= H −1
k XkX†
k+1BkH T −1
k
= H −1
k
PkH T −1
k
,
i.e., Pk ≥0 if and only if ˜Pk ≥0. This completes the proof.
⊓⊔
Note that a symplectic transformation with a lower block-triangular transforma-
tion matrix preserves also the so-called multiplicity of a focal point. We will present
and prove this statement later in this book (see Sect. 4.4.2).
In the next statement, we prove that the quadratic functional associated with
(SDS) transforms under a transformation with lower block-triangular symplectic
transformation matrix (2.142) essentially in the same way as the corresponding
symplectic system.
Lemma 2.73 Let y =
x
u

be an admissible pair for functional (2.57) and let
˜yk = R−1
k yk with ˜y =
˜x
˜u

, where Rk is a lower block-triangular transformation
matrix given in (2.142). Denote by ˜F( ˜y) the quadratic functional corresponding to
transformed system (2.139), i.e., to the symplectic system with block entries given in
(2.143). Then we have
F(y) = ˜xT
k GT
k Hk ˜xk
		N+1
k=0 + ˜F( ˜y).
(2.144)
Proof Following (2.57), denote by Fk and  ˜Fk the expressions
Fk(y) := xT
k CT
k Akxk + 2xT
k CT
k Bkuk + uT
k BT
k Dkuk,
 ˜Fk( ˜y) := ˜xT
k ˜CT
k ˜Ak ˜xk + 2 ˜xT
k ˜CT
k ˜Bk ˜uk + ˜uT
k ˜BT
k ˜Dk ˜uk.

138
2
Basic Theory of Symplectic Systems
By using Lemma 2.30, we then prove that
 ˜Fk( ˜y) −(˜xT
k ˜uk) = Fk(y) −(xT
k uk)
or
Fk(y) = (˜xT
k GT
k Hk ˜xk) +  ˜Fk( ˜y).
(2.145)
Indeed
 ˜Fk( ˜y) −(˜xT
k ˜uk)
= ˜xT
k+1(−˜uk+1 + ˜Ck ˜xk + ˜Dk ˜uk)
= ˜xT
k+1(−˜uk+1 + (0 I) ˜Sk ˜yk)
= xT
k+1H T −1
k+1

GT
k+1xk+1 −H T
k+1uk+1 + (−GT
k+1, H T
k+1)Skyk

= xT
k+1H T −1
k+1

GT
k+1xk+1 −H T
k+1uk+1 −GT
k+1(I 0) Skyk + H T
k+1(0 I) Skyk

= xT
k+1

−uk+1 + (0 I) Skyk

= xT
k+1(−uk+1 + Ckxk + Dkuk)
= Fk(y) −(xT
k uk).
Finally, using the connection ˜yk = R−1
k yk, we derive (2.145). Summing this identity
for k ∈[0, N]Z, we prove (2.144).
⊓⊔
We ﬁnish this section with the proof of formula (2.65) in Lemma 2.32.
Alternative proof of Lemma 2.32 Consider the transformation
xk
uk

=
 I
0
Qk I
 ˜xk
zk

,
i.e.,
˜xk = xk, zk = uk −Qkxk.
Then by (2.143) we have
˜Ak = Ak + BkQk,
˜Bk = Bk,
˜Ck = −Rk[Q],
˜Dk = −Qk+1Bk + Dk.
Then the summand in the transformed quadratic functional is
 ˜Fk(˜x, z) = −˜xT
k RT
k [Q] (Ak + BkQk) ˜xk −2 ˜xT
k RT
k [Q] Bkzk
+ zT
k BT
k (−Qk+1Bk + Dk) zk
= −xT
k RT
k [Q] (Ak + BkQk) xk −2 xT
k RT
k [Q] Bk(uk −Qkxk)

2.6
Transformations of Symplectic Systems
139
+ zT
k Pk[Q] zk
= xT
k RT
k [Q] (BkQk −Ak) xk −2 xT
k RT
k [Q] Bkuk + zT
k Pk[Q] zk.
Substituting the last expression into (2.145), we derive (2.65).
⊓⊔
2.6.2
Trigonometric or Bohl Transformation
Now, consider the trigonometric symplectic system (2.12) in the matrix form
Sk+1 = PkSk + QkCk,
Ck+1 = −QkSk + PkCk
(2.146)
with Pk and Qk satisfying (see (2.13))
PT
k Pk + QT
k Qk = I,
PT
k Qk = QT
k Pk.
The fact that trigonometric systems are self-reciprocal, i.e., transformation (2.138)
with Rk = J transforms this system into itself, implies that if (S, C) is a solution
of (2.146), then (C, −S) solves this system as well. Consequently, if the matrix

Sk
Ck
Ck −Sk

is symplectic for some k, then it is symplectic everywhere and the
following identities hold:
CT
k Ck + ST
k Sk = I,
CT
k Sk = ST
k Ck,
(2.147)
CkCT
k + SkST
k = I,
SkCT
k = CkST
k .
(2.148)
The following theorem presents the discrete analog of Theorem 1.52.
Theorem 2.74 There exist n × n-matrices Hk and Kk such that Hk is nonsingular
and H T
k Kk = KT
k Hk for k ∈[0, N + 1]Z, and the transformation
sk
ck

=
 H −1
k
0
−KT
k H T
k
 xk
uk

(2.149)
transforms the symplectic system (SDS) into the trigonometric system (2.146)
without changing the oscillatory behavior. Moreover, the matrices Pk and Qk from
(2.146) may be explicitly given by
Pk = H −1
k+1AkHk + H −1
k+1BkKk,
Qk = H −1
k+1BkH T −1
k
.
(2.150)
Proof We let ˜Y = ( ˜X, ˜U) and Y = (X, U) be normalized conjoined bases of (SDS).
Then they form the symplectic fundamental solution matrix Z = ( ˜Y, Y), and then

140
2
Basic Theory of Symplectic Systems
ZkZT
k > 0 because Zk is nonsingular. In particular, the matrix XkXT
k + ˜Xk ˜XT
k is
positive deﬁnite, and there exists a nonsingular n × n-matrix Hk with
HkH T
k = XkXT
k + ˜Xk ˜XT
k ,
k ∈[0, N + 1]Z.
(2.151)
We put
Kk :=

UkXT
k + ˜Uk ˜XT
k

H T −1
k
,
(2.152)
so that the following identity hold
ZkZT
k = RkRT
k ,
Rk :=
Hk
0
Kk H T −1
k

.
(2.153)
Indeed, since ZkZT
k
is symplectic, we have that the matrix HkH T
k KkH T
k
is
symmetric, and then, due to nonsingularity of Hk, so is H T
k Kk, i.e., H T
k Kk =
KT
k Hk. Moreover, UUT + ˜U ˜UT = KkKT
k + (HkH T
k )−1.
So we have proved that the block lower triangular matrix Rk in the right-hand
side of (2.153) is symplectic, and then the product of two symplectic matrices ˜Zk =

H −1
k
0
−KT
k H T
k

Zk satisﬁes the condition ˜Zk ˜ZT
k = I, i.e.,
˜Zk =
 Ck Sk
−Sk Ck

= R−1
k Zk
(2.154)
is symplectic and orthogonal matrix. Moreover since ˜Sk = ˜Zk+1 ˜Z−1
k
is also sym-
plectic and orthogonal, we have that ˜Zk solves trigonometric system (2.146) with
the blocks given by (2.150) according to formulas (2.143) for the transformation
matrix Rk in (2.153). The proof is completed.
⊓⊔
Remark 2.75 Observe that every trigonometric system (2.146) can be transformed
into another trigonometric system with symmetric and positive semideﬁnite matri-
ces ˜Qk at the position of the matrices Qk, using the transformation
˜sk
˜ck

=
H −1
k
0
0
H T
k
 sk
ck

,
where the matrices Hk are recursively deﬁned by H0 = I and Hk+1 = G−1
k Hk
with orthogonal matrices Gk, i.e., GT
k Gk = I, such that GkQk are symmetric and
positive semideﬁnite. Such matrices Gk exist according to the well-known principle
of polar decomposition (see, e.g., [195, Theorem 3.1.9(c)]). This setting implies
that all matrices Hk are orthogonal and hence that the transformation matrices

2.6
Transformations of Symplectic Systems
141
diag{H −1
k , H T
k } are symplectic and orthogonal. The transformed system then reads
˜Sk+1 = ˜Pk ˜Sk + ˜Qk ˜Ck,
˜Ck+1 = −˜Qk ˜Sk + ˜Pk ˜Ck,
where
˜Pk = H −1
k+1PkHk,
˜Qk = H −1
k+1QkHk = H −1
k
GkQkH T −1
k
so that indeed all matrices ˜Qk are symmetric and positive semideﬁnite.
By analogy with the continuous case (see Sect. 1.5.4), we formulate a necessary
and sufﬁcient condition of oscillation of (2.146) (see formula (1.136)).
Theorem 2.76 Assume that the matrices Qk in the trigonometric system (2.146)
are symmetric and positive deﬁnite. Then this system is nonoscillatory at ∞if and
only if
∞

k=0
arccot λ(1)(Q−1
k Pk) < ∞,
(2.155)
where λ(1)(·) denotes the minimal eigenvalue of the matrix indicated.
Proof The proof of this result can be found in [46].
⊓⊔
Remark 2.77 The previous theorem requires the matrices Qk in (2.146) to be posi-
tive deﬁnite. By the trigonometric transformation of (SDS) given in Theorem 2.74,
the matrices Qk are given by Qk = H −1
k+1BkH T −1
k
, hence a necessary condition for
positive deﬁniteness of Qk is nonsingularity of Bk. However, symplectic systems
(SDS) with Bk nonsingular do not cover a relatively large class of equations and
systems, e.g., the higher-order Sturm-Liouville difference equations (2.27), which
can be written in the form (SDS) with (see Example 2.21)
Bk =
1
r(n)
k
⎛
⎜⎝
0 . . . 0 1
...
...
...
0 . . . 0 1
⎞
⎟⎠.
The extension of Theorem 2.74 to trigonometric systems with arbitrary Qk is given
in [96].
2.6.3
Hyperbolic Transformation
In this section we prove that any symplectic difference systems (SDS) satisfying
certain additional condition can be transformed into a hyperbolic system (see

142
2
Basic Theory of Symplectic Systems
Sect. 2.1.2). Consider the hyperbolic system (2.21) in the matrix form
Sk+1 = PkSk + QkCk,
Ck+1 = QkSk + PkCk,
(2.156)
with Pk and Qk satisfying (see (2.22))
PT
k Pk −QT
k Qk = I = PkPT
k −QkQT
k ,
PT
k Qk −QT
k Pk = 0 = PkQT
k −QkPT
k .
The fact that transformation (2.138) with Rk = P1 (see (2.20)) transforms this
system into itself implies that if (S, C) is a solution of (2.156), then (C, S) solves
this system as well. Consequently, if the matrix

Sk Ck
Ck Sk

is symplectic for some k,
then it is symplectic everywhere and the following identities hold:
ST
k Ck −CT
k Sk = 0 = SkCT
k −CkST ,
(2.157)
CT
k Ck −ST
k Sk = I = CkCT
k −SkST
k .
(2.158)
Theorem 2.78 Suppose that symplectic system (SDS) possesses normalized con-
joined bases Y =
X
U

, ˜Y =
 ˜X
˜U

such that Xk ˜XT
k is positive deﬁnite in a given
discrete interval. Then, in this interval, there exist n × n-matrices Hk and Kk such
that Hk is nonsingular, H T
k Kk = KT
k Hk, and the transformation
sk
ck

=
 H −1
k
0
−KT
k H T
k
 xk
uk

(2.159)
transforms symplectic system (SDS) into the hyperbolic system (2.156) without
changing the oscillatory behavior, i.e., a conjoined basis Yk of (SDS) has a focal
point in (k, k + 1] if and only if

H −1
k
Xk
−KT
k Xk+H T
k Uk

has a focal point there. Moreover,
the matrices Pk and Qk are given by the formulas
Pk = H −1
k+1AkHk + H −1
k+1BkKk,
Qk = H −1
k+1BkH T −1
k
.
(2.160)
Proof Let Y =
X
U

and ˜Y =
 ˜X
˜U

be normalized conjoined bases of (SDS) such that
Xk ˜XT
k is positive deﬁnite, H be any n × n matrix satisfying
HkH T
k = 2Xk ˜XT
k
(2.161)
and let
Kk := (Uk ˜XT
k + ˜UkXT
k ) H T −1
k
.
(2.162)

2.6
Transformations of Symplectic Systems
143
Then Hk is nonsingular by the assumption in Theorem 2.78. Consider the symplectic
fundamental solution matrix of system (SDS) of the form
Zk =
1
√
2
( ˜Yk, Yk)
I −I
I
I

, and
then, by analogy with the proof of Theorem 2.74, we prove the identity
ZkP2ZT
k P2 = RkP2RT
k P2,
Rk :=
Hk
0
Kk H T −1
k

,
P2 = diag{I, −I},
(2.163)
where Hk and Kk are given by (2.161) and (2.162). Indeed, using that the matrix in
the left-hand side of this relation is symplectic (see Lemma 1.58 (iv)), we have by
(2.161) and (2.162) that HkH T
k KkH T
k is symmetric together with H T
k Kk. Moreover,
Uk ˜UT
k + ˜UkUT
k = 2Uk ˜UT
k = KkKT
k −(HkH T
k )−1. Then we have proved (2.163)
and the symplecticity of the transformation matrix Rk. Identity (2.163) implies the
relation
˜ZkP2 ˜ZT
k P2 = I, ˜Zk = R−1
k Zk
for the symplectic matrix ˜Z, then
˜Zk = P2 ˜ZT −1
k
P2 = P2 J ˜ZkJ T P2 = P1 ˜ZkP1,
where P1 :=
0 I
I 0

,
that is,
˜Zk =
Ck Sk
Sk Ck

= R−1
k Zk
(2.164)
is a symplectic and hyperbolic matrix. Moreover, since ˜Sk =
˜Zk+1 ˜Z−1
k
is also
symplectic and hyperbolic, we have that ˜Zk solves hyperbolic system (2.156) with
the blocks given by (2.160) according to formulas (2.143) for the transformation
matrix Rk in (2.153). The proof is completed.
⊓⊔
Remark 2.79 By the same arguments as in Remark 2.75, one can transform any
hyperbolic system (2.156) into another hyperbolic system with symmetric and
positive semideﬁnite matrices ˜Qk at the position of the matrices Qk, using the
transformation
 ˜xk
˜uk

=
H −1
k
0
0
H T
k
 xk
uk

,

144
2
Basic Theory of Symplectic Systems
where the matrices Hk are recursively deﬁned by H0 = I and Hk+1 = G−1
k Hk
with orthogonal matrices Gk, i.e., GT
k Gk = I, such that GkQk are symmetric and
positive semideﬁnite. The transformed system then reads
˜xk+1 = ˜Pk ˜xk + ˜Qk ˜uk,
˜uk+1 = ˜Qk ˜xk + ˜Pk ˜uk,
where
˜Pk = H −1
k+1PkHk
and
˜Qk = H −1
k+1QkHk = H −1
k GkQkH T −1
k
,
so that indeed all matrices ˜Qk are symmetric and positive semideﬁnite.
Remark 2.80 This remark concerns the assumption of the existence of a pair of
normalized conjoined bases Y = X
U
 and ˜Y =  ˜X
˜U
 such that Xk ˜XT
k is positive
deﬁnite. If (SDS) is nonoscillatory at ∞(and eventually controllable), then this
system possesses the recessive and dominant solutions ¯Y =  ¯X
¯U
 and ˆY =  ˆX
ˆU
 at ∞,
respectively, such that w( ¯Y, ˆY ) = I. Then
Yk =
Xk
Uk

=
1
√
2
( ˆYk + ¯Yk),
˜Yk =
 ˜Xk
˜Uk

=
1
√
2
( ¯Yk −ˆYk),
is a normalized pair of conjoined bases for which Xk ˜XT
k = 1
2 ( ˆXk ˆXT
k −¯Xk ¯XT
k ) is
positive deﬁnite eventually, since
lim
k→∞
ˆX−1
k
¯Xk = 0.
Consequently, any nonoscillatory symplectic difference system (SDS) at ∞can be
transformed into a hyperbolic symplectic system.
2.6.4
Prüfer Transformation
In this section, we present a discrete analog of Theorem 1.51 for systems (SDS)
which in turn generalizes results of Sect. 1.2.4 for the scalar Sturm-Liouville
equations.
Theorem 2.81 Let Y =
X
U

be a conjoined basis of (SDS). There exist nonsingular
matrices Hk and n × n matrices Sk and Ck such that
Xk = ST
k Hk,
Uk = CT
k Hk,
(2.165)

2.6
Transformations of Symplectic Systems
145
where
S
C

is a conjoined basis of (2.146) with
Pk = H T −1
k+1 Y T
k ST
k YkH −1
k ,
Qk = H T −1
k+1 Y T
k ST
k J YkH −1
k ,
(2.166)
and Hk solve the ﬁrst order difference system
Hk+1 = ˜Y T
k+1Sk ˜YkHk,
˜Yk =
ST
k
CT
k

.
(2.167)
Proof Let Y = X
U
 be a conjoined basis of (SDS), and let Hk be nonsingular
matrices satisfying H T
k Hk = XT
k Xk + UT
k Uk > 0. Introduce the 2n × 2n matrix
Zk = P1 (J YkH −1
k , YkH −1
k ) P1 =
CT
k −ST
k
ST
k
CT
k

,
where P1 =
 0 I
I 0

. Then, according to properties (iv), (v), and (vii) in Lemma 1.58,
we have that Zk is symplectic and orthogonal (see Sect. 1.6.1), where we used the
fact
w(J YkH −1
k , YkH −1
k
) = H T −1
k
Y T
k YkH −1
k
= I.
Moreover, it is easy to verify directly that
ZT
k+1Zk = ˜Sk =
 Pk
Qk
−Qk Pk

,
(2.168)
where Pk and Qk are given by (2.166). Indeed, from (2.168) we have
(Ck+1 Sk+1)
CT
k
ST
k

= (Sk+1 Ck+1)
ST
k
CT
k

= H T −1
k+1 Y T
k+1YkH −1
k
= H T −1
k+1 Y T
k ST
k YkH −1
k
= Pk.
Similarly,
(Ck+1 Sk+1)
−ST
k
CT
k

= (Sk+1 Ck+1)
 CT
k
−ST
k

= H T −1
k+1 Y T
k ST
k J YkH −1
k
= Qk.

146
2
Basic Theory of Symplectic Systems
Certainly the matrix in the right-hand side of (2.168) is also symplectic and
orthogonal. Equation (2.168) implies
Sk+1
Ck+1

=
˜Sk
Sk
Ck

, i.e., the matrix
Sk
Ck

is
a conjoined basis of (2.146). Finally, from (SDS), we have
ST
k+1
CT
k+1

Hk+1 = Sk
ST
k
CT
k

Hk, and
(2.169)
then, using the property SkST
k + CkCT
k
= I, we derive (2.167) by multiplying
(2.169) from the left by (Sk+1 Ck+1).
⊓⊔
2.7
Notes and References
Main references for various special symplectic systems are the books [4, 16]. In
addition, Jacobi matrix difference equations and symmetric three-term recurrence
equations are studied from the point of view of symplectic systems in [304]. For
properties of discrete linear Hamiltonian systems, we refer to the survey paper [3]
and to the book [4, Sections 2.3–2.7]. Basic properties of solutions of trigonometric
symplectic systems with nonsingular Bk are established in [25] and of hyperbolic
symplectic systems in [108].
To deﬁne properly the concept of a focal point in a given interval (more precisely,
the concept of “no focal points” in this interval) of a conjoined basis of linear
Hamiltonian difference systems was a difﬁcult problem, which resisted its solution
for rather long time. The paper [39] of M. Bohner from 1996 constitutes a breaking
point in this direction, in which he presented the deﬁnition of this notion for the
linear Hamiltonian systems (2.15). More precisely, a conjoined basis Y =
X
U

of
(2.15) has no focal point in the interval (k, k + 1] if
Ker Xk+1 ⊆Ker Xk,
Pk := XkX†
k+1(I −Ak)−1Bk ≥0.
(2.170)
In earlier works, oscillatory properties of linear Hamiltonian difference systems
were studied only under the assumption that the matrix Bk is positive deﬁnite (see
the papers of Erbe and Yan [134–137]). This situation corresponds, in some sense,
to a controllable system (2.15). The notion of “no (forward) focal points” in the
interval (k, k+1] for conjoined bases of symplectic system (SDS) in Deﬁnition 2.14
was introduced in [45] by M. Bohner and O. Došlý, and it was motivated by the
corresponding notion in (2.170) from [39]. In addition, the notion of “no backward
focal points” in [k, k + 1) is also from [45]. A transformation between the notions
of no forward and backward focal points is presented in [300].
Identities in Lemma 2.26 and Corollary 2.27 were established in [189]. In this
respect it is surprising that the solvability of the explicit Riccati equation (2.52)
implies the invertibility of the matrices Ak + BkQk and DT
k −BT
k Qk+1. This fact is
not very very well known, and it brings the result in Theorem 2.28 to the same form
as in the continuous case—parts (ii) and (iii) of Theorem 1.47.

2.7
Notes and References
147
The importance of the study of equivalent conditions for the positivity and
nonnegativity of discrete quadratic functionals is justiﬁed by the results in Theo-
rems 1.26, 1.28, 1.32, and 1.33. The results on the nonnegativity of the functional
F (Theorem 2.44 in Sect. 2.3.5) is essentially from [54]. However, the latter
reference uses the necessity of the P-condition (2.73) for the nonnegativity of
F from [100, Theorem 1]. The results on discrete Reid roundabout theorems
(Theorems 2.36 and 2.41) are from [45]. A different proof of the equivalence
of parts (i), (ii), and (iv) in Theorem 2.36, based on a matrix diagonalization
technique, is presented in [171]. Extension of this technique to separated endpoints
and to the nonnegativity of a quadratic functional (Theorem 2.49) is presented in
[50, Theorem 2]. The extension of this nonnegativity result to the jointly varying
endpoints (Theorem 2.52) is derived in [174, Theorem 2]. The corresponding results
regarding the positivity of a quadratic functional (Theorems 2.50 and 2.53) are
from [181, Theorems 5 and 10]. We note that the latter paper contains also further
equivalent conditions for the positivity of the functional G in (2.84) in terms of
a conjoined basis Y of (SDS) with no forward focal points in (0, N + 1] and with
invertible Xk for all k ∈[0, N + 1]Z. This then yields an additional initial equality
or inequality at k = 0 (see [181, Theorems 6, 7, and 11]). First results on the
positivity of G with joint endpoints are proven in [42, Theorem 1]. A survey of these
conditions for continuous and discrete case is presented in [306]. In the literature
one can ﬁnd additional conditions, which are equivalent to the positivity of discrete
quadratic functional F and G, such as the nonexistence of conjugate intervals [181],
the nonexistence of coupled intervals [180, 184, 188], the solvability of implicit
Riccati equations [173, 174, 181], or the positivity and nonnegativity of perturbed
quadratic functionals [175, 258]. Various Picone-type identities for symplectic
systems are derived in [191, 302]. Transformations which reduce the problem with
separated endpoints into a problem with zero endpoints on an extended interval,
and a problem with general jointly varying endpoints into a problem with separated
endpoints in the double dimension 2n (see Sect. 5.2.1 for more details) are known in
[39, 42, 53, 103] due to Bohner, Došlý, and Kratz, or earlier in [109] due to Dwyer
and Zettl. These techniques are also used in [173, 174, 181, 185]. Recently, another
transformation of this type was developed in [196, 308]. In this respect the statement
in Theorem 2.40 is a special case of [181, Theorem 7].
The inequalities for the Riccati-type quotients for symplectic systems presented
in Sect. 2.4.1 are from [173]. The results on discrete Riccati inequality (2.115)
in Sect. 2.4.2 are from [174]. For the discrete linear Hamiltonian system (1.102),
the Riccati inequality was derived in [176]. Further inequalities between symmetric
solutions of two Riccati equations (2.52) or Riccati inequalities (2.115) in the spirit
of Theorem 2.55 are proven in [300].
The results on recessive and dominant solutions of (SDS) at inﬁnity are
essentially from [81], with some small adjustments from the recent work [284].
In particular, the recessive and dominant solutions are introduced in Sect. 2.5.1
through the summation property (2.123) with (2.121), which was not the case
in [16, Section 3.11] and [25, 81]. In the latter references, the limit property in
Theorem 2.67 was used for this purpose as the deﬁnition. We refer to Remark 6.117

148
2
Basic Theory of Symplectic Systems
for more discussion on this subject. Recently, a theory of recessive and dominant
solutions of (SDS) was initiated in [284, 290] without the controllability assumption.
We will discuss these more general concepts in Sect. 6.3. Observe that the
arguments in the construction of the recessive solution at ∞of an eventually
controllable symplectic system (in the proof of Theorem 2.66) follow the more
general considerations presented in Sect. 6.3.5 (see the proof of Theorem 6.103).
This explicit construction of the recessive solution of (SDS) at ∞is also new in the
controllable case.
Transformation theory of symplectic systems was initiated by Bohner and Došlý
in [45]. The results on trigonometric, Prüfer, and hyperbolic transformations in
Sects. 2.6.2–2.6.4 were obtained in [46, 47, 108]. Applications of trigonometric
and hyperbolic transformations in the oscillation theory of symplectic system
are derived in [48, 85, 106, 107]. Discrete matrix trigonometric and hyperbolic
functions, as solutions of discrete trigonometric and hyperbolic symplectic systems,
are deﬁned in [24, 194, 333].
We recommend the following additional related references for further reading
about the topics presented in this chapter: [9, 14, 40, 111, 237] for discrete Riccati
equations, [11, 15] for recessive and dominant solutions of special symplectic
systems, and [17, 84, 86, 88, 183, 266, 323, 324] for general theory of symplectic
difference systems. Moreover, results about more general theory of dynamic equa-
tions on time scales, in particular on Hamiltonian or symplectic dynamic systems
on time scales, are presented in [12, 49, 58, 59, 97, 99, 187, 190, 299, 307, 334].

Chapter 3
Comparative Index Theory
In this chapter we introduce the comparative index as a main mathematical tool for
the results in the subsequent chapters of this book. Recall from [27] that a subspace
span{u1, . . . , un} ⊆R2n is a Lagrangian subspace if it has dimension n and
uT
i J uj = 0 for all i and j. We introduce a notion of the comparative index μ(Y, ˆY )
for a pair of Lagrangian subspaces represented by 2n × n matrices Y and ˆY with
conditions
Y TJ Y = 0,
rank Y = n,
(3.1)
(and similarly for ˆY). Depending on the formulation of the problem (see Chaps. 4
and 5), the matrices Y and ˆY can represent Lagrangian subspaces associated with
conjoined bases of system (SDS) or with symplectic coefﬁcient matrices Sk and ˆSk
of two symplectic systems (SDS) or with their fundamental solution matrices. For
the special case of Y := Yk+1 and ˆY := Sk(0 I)T , the comparative index μ(Y, ˆY )
represents the main concept of the discrete oscillation theory—the multiplicity of
a focal point (see Chap. 4). Because of this connection, algebraic properties of the
comparative index turned out to be an essential effective tool for solving several
important problems in the discrete oscillation theory.
3.1
Comparative Index and Its Properties
In this section we deﬁne one of the fundamental concepts of the oscillation theory of
symplectic difference systems, the concept of a comparative index of two matrices
satisfying (3.1).
© Springer Nature Switzerland AG 2019
O. Došlý et al., Symplectic Difference Systems: Oscillation and Spectral Theory,
Pathways in Mathematics, https://doi.org/10.1007/978-3-030-19373-7_3
149

150
3
Comparative Index Theory
3.1.1
Deﬁnition of Comparative Index
Consider two 2n × n matrices Y =
X
U

and ˆY =
 ˆX
ˆU

satisfying (3.1) and let
w := w(Y, ˆY ) = Y TJ ˆY
(3.2)
be their Wronskian.
Deﬁnition 3.1 (Comparative Index) Let
μ1(Y, ˆY ) := rankM,
M := (I −X†X) w,
(3.3)
and
μ2(Y, ˆY) := ind P,
P := T T (wT X† ˆX) T ,
T := I −M†M,
(3.4)
where M is deﬁned in (3.3). The quantity
μ(Y, ˆY ) = μ1(Y, ˆY ) + μ2(Y, ˆY )
is called the comparative index of the matrices Y and ˆY.
The next result will be important in proving the properties of the comparative
index.
Theorem 3.2 The following statements hold.
(i) The matrix M in (3.3), (3.4) can be replaced by the matrix
˜
M := (I −XX†) ˆX,
(3.5)
i.e., rankM = rank ˜
M and
T = I −M†M = I −˜
M† ˜
M.
(3.6)
(ii) The condition μ1(Y, ˆY ) = 0 is equivalent to the condition Im ˆX ⊆Im X. In
this case T = I and
μ(Y, ˆY) = μ2(Y, ˆY ) = ind [ ˆXT ( ˆQ −Q) ˆX],
(3.7)
where Q and ˆQ are symmetric matrices such that
XT QX = XT U,
ˆXT ˆQ ˆX = ˆXT ˆU.
(3.8)

3.1
Comparative Index and Its Properties
151
In particular, if X and ˆX are invertible, then
μ(Y, ˆY ) = μ2(Y, ˆY ) = ind( ˆQ −Q),
Q := UX−1,
ˆQ := ˆU ˆX−1.
(3.9)
(iii) The matrix P in (3.4) is symmetric and can be presented in the form
P = T T [ ˆXT ( ˆQ −Q) ˆX] T ,
(3.10)
where Q and ˆQ are any symmetric matrices satisfying (3.8) and T is deﬁned
by (3.6) with M and
˜
M given by (3.3) and (3.5).
(iv) The equality μ(Y, ˆY) = 0 holds if and only if
Im ˆX ⊆Im X
and
ˆXT ( ˆQ −Q) ˆX ≥0,
(3.11)
where Q = QT and ˆQ = ˆQT are again given by (3.8).
Proof
(i) By (3.3) we have
M = (I −X†X) w = (I −X†X) (XT ˆU −UT ˆX)
= −(I −X†X) UT ˆX = −(I −X†X) UT (I −XX†) ˆX
= LT (I −XX†) ˆX = LT ˜
M,
where we have used (1.171) and where the (invertible) matrix L is deﬁned in
(1.174). Then rank M = rank ˜
M. Furthermore, the equality M†M =
˜
M† ˜
M
holds by Lemma 1.63. Hence, we proved (3.6), and the matrix M in (3.3) and
(3.4) can be replaced by
˜
M.
(ii) By the previous part of the proof, we have μ1(Y, ˆY) = 0 if and only if M =
0 =
˜
M. Then ˆX = XX† ˆX, which is equivalent by (1.160) to the condition
Im ˆX ⊆Im X. In this case T = I −M†M = I and the matrix
P = wT X† ˆX = ( ˆUT X −ˆXT U) X† ˆX = ˆUT ˆX −ˆXT (XT )†XT UX† ˆX
are symmetric, since we suppose (3.1). In addition, if symmetric matrices Q
and ˆQ satisfy (3.8), then
P = ˆXT ˆQ ˆX −ˆXT (XT )†XT QXX† ˆX = ˆXT ( ˆQ −Q) ˆX,
where we have again used that
˜
M = 0, i.e., ˆX = XX† ˆX. In this case we
have μ(Y, ˆY) = μ1(Y, ˆY ) + μ2(Y, ˆY ) = μ2(Y, ˆY ) = ind P. In particular, for
invertible X and ˆX, obviously μ1(Y, ˆY ) = 0 holds, and, hence, Q = UX−1,
ˆQ = ˆU ˆX−1, and (3.9) follows from (3.7).

152
3
Comparative Index Theory
(iii) In the general case, we have MT = 0, i.e., Im( ˆXT ) ⊆Im X. Then the matrix
wT X† ˆX in (3.4) is symmetric, when restricted to the subspace Im T . Indeed,
repeating the arguments from part (ii) of this proof, we obtain
P = T T [ ˆUT ˆX −ˆXT (XT )†XT UX† ˆX] T = T XT ( ˆQ −Q) ˆXT = PT ,
so that P is symmetric. Recall that we proved (3.6) in part (i).
(iv) This statement is a direct consequence of part (ii).
⊓⊔
Remark 3.3 Observe that the comparative index is not a commutative function of
Y and ˆY. Later, in Theorem 3.5(v) we present the connection between μ(Y, ˆY) and
μ( ˆY, Y).
Remark 3.4 We now evaluate the comparative index for some special cases.
(i) As it was already mentioned in Theorem 3.2(ii), when det X ̸= 0 and det ˆX ̸=
0, then M =
˜
M = 0, and Q = UX−1, ˆQ = ˆU ˆX−1, so that we get μ(Y, ˆY ) =
ind( ˆQ −Q).
(ii) If ˆX = 0, then μ(Y, ˆY ) = 0 for any Y according to (3.5) and (3.10).
(iii) If X = 0, then μ2(Y, ˆY ) = 0 by (3.4) and μ(Y, ˆY ) = μ1(Y, ˆY ) = rank ˆX.
(iv) If U = 0, then det X ̸= 0, and μ1(Y, ˆY ) = 0, μ2(Y, ˆY ) = ind ˆXT ˆU.
3.1.2
Dual Comparative Index
In this subsection we introduce the so-called dual comparative index μ∗(Y, ˆY ),
whose deﬁnition is very similar to that of the standard comparative index (Deﬁ-
nition 3.1), only the matrix J in (3.3) and (3.4), and in particular in the deﬁnition
of the Wronskian (3.2), is replaced by the matrix J T = −J . We deﬁne
μ∗
1(Y, ˆY ) := μ1(Y, ˆY ),
μ∗
2(Y, ˆY ) := ind(−P)
(3.12)
with P given in (3.4). The dual comparative index μ∗(Y, ˆY ) is deﬁned as
μ∗(Y, ˆY ) := μ∗
1(Y, ˆY ) + μ∗
2(Y, ˆY ).
Observe that if the 2n × n matrices Y and ˆY satisfy (3.1), then the matrices P2Y
and P2 ˆY with P2 = diag{I, −I} given in (1.151) satisfy the same condition, because
the matrix P2 is invertible and
P T
2 J P2 = −J ,
(P2Y)TJ (P2Y) = Y T P T
2 J P2Y = −Y TJ Y = 0.

3.1
Comparative Index and Its Properties
153
Consequently, the dual comparative index can be determined by the formula
μ∗
i (Y, ˆY ) = μi(P2Y, P2 ˆY),
i ∈{1, 2},
(3.13)
with P2 given by (1.151). Observe also that the dual comparative index as introduced
in (3.12) makes it possible to compute the signature and the rank of P, which are
the difference and the sum of the numbers of positive and negative eigenvalues of
P, see [148, Chapter 10]. Indeed, we have the formulas
sgn P = μ∗
2(Y, ˆY ) −μ2(Y, ˆY ) = μ∗(Y, ˆY ) −μ(Y, ˆY),
rankP = μ∗
2(Y, ˆY ) + μ2(Y, ˆY ).
The terminology “dual” comparative index will be explained later in this section.
3.1.3
Basic Properties of Comparative Index
Let Z and ˆZ be symplectic matrices, and let Y and ˆY be 2n × n matrices given by
the formulas
Y =
X
U

= Z
0
I

,
ˆY =
 ˆX
ˆU

= ˆZ
0
I

.
(3.14)
Then, by Lemma 1.58(v), the matrices Y and ˆY obey condition (3.1). Conversely, as
we showed in Lemma 1.58(vi), given a 2n×n matrix Y satisfying (3.1), there exists
another 2n×n matrix ˜Y satisfying (3.1) such that Z = ( ˜Y Y) is a symplectic matrix,
for which (3.14) holds. Moreover, in this case the matrices ˜Y and Y are normalized
in the sense that w( ˜Y , Y) = I, by Lemma 1.58(v).
In the next theorem, we present basic properties of the comparative index.
Theorem 3.5 Let Y =
X
U

and ˆY =
 ˆX
ˆU

be 2n × n matrices satisfying (3.1),
and let w(Y, ˆY ) be their Wronskian deﬁned in (3.2). The comparative index has the
following properties.
(i) μi(YC1, ˆYC2) = μi(Y, ˆY ), i ∈{1, 2}, where C1, C2 are invertible n × n
matrices. This property can be reformulated for Z and ˆZ given by (3.14) as
μi

Z(0 I)T , ˆZ(0 I)T 
= μi

ZL1(0 I)T , ˆZL2(0 I)T 
,
i ∈{1, 2},
where L1, L2 are arbitrary 2n × 2n lower block-triangular symplectic
matrices.

154
3
Comparative Index Theory
(ii) μi(LY, L ˆY ) = μi(Y, ˆY ), i ∈{1, 2}, and hence μ(LY, L ˆY ) = μ(Y, ˆY), where
L is any 2n × 2n lower block-triangular symplectic matrix.
(iii) For any symplectic matrices Z and ˆZ, we have
μi

Z(0 I)T , ˆZ(0 I)T 
= μ∗
i

Z−1(0 I)T , Z−1 ˆZ(0 I)T 
,
i ∈{1, 2}.
(iv) μ2(Y, ˆY) = μ∗
2( ˆY, Y).
(v) μ(Y, ˆY) + μ( ˆY, Y) = rank w(Y, ˆY ). This property can be reformulated for
the matrices Z and ˆZ given by (3.14) as
μ

Z(0 I)T , ˆZ(0 I)T 
+μ

ˆZ(0 I)T , Z(0 I)T 
= μ

(0 I)T , Z−1 ˆZ(0 I)T 
.
(vi) μ1(Y, ˆY) = rank ˆX −rankX + μ1( ˆY, Y). In view of (iv), this is equivalent to
μ(Y, ˆY) = rank ˆX −rank X + μ∗( ˆY, Y).
(vii) 0 ≤μ(Y, ˆY ) ≤p and 0 ≤μ∗(Y, ˆY ) ≤p, where p = p(Y, ˆY ) is deﬁned by
p := min {rankw(Y, ˆY ), rank ˆX, rank ˆX −rankX + rank w(Y, ˆY )}.
(3.15)
In particular,
μ(Y, ˆY ) = rank ˆX
⇔

μ1(Y, ˆY ) = rank w(Y, ˆY ) −rankX,
μ2(Y, ˆY ) = rank ˆX + rank X −rankw(Y, ˆY ),
⇔μ( ˆY, Y) = rankw(Y, ˆY ) −rank ˆX,
⇔
μ1( ˆY, Y) = rankw(Y, ˆY ) −rank ˆX,
μ2( ˆY, Y) = 0,
⇔μ∗(Y, ˆY ) = rankw(Y, ˆY ) −rank X,
⇔
μ∗
1(Y, ˆY ) = rankw(Y, ˆY ) −rank X,
μ∗
2(Y, ˆY ) = 0,
⇔μ∗( ˆY, Y) = rank X,
⇔

μ∗
1( ˆY, Y) = rank w(Y, ˆY ) −rank ˆX,
μ∗
2( ˆY, Y) = rank X + rank ˆX −rank w(Y, ˆY ),

3.1
Comparative Index and Its Properties
155
and
μ(Y, ˆY ) = rankw(Y, ˆY )
⇔

μ1(Y, ˆY ) = rank ˆX −rankX,
μ2(Y, ˆY ) = rankw(Y, ˆY ) −rank ˆX + rankX,
⇔μ( ˆY, Y) = 0,
⇔μ∗( ˆY, Y) = rank X −rank ˆX + rank w(Y, ˆY ),
⇔

μ∗
1( ˆY, Y) = 0,
μ∗
2( ˆY, Y) = rank X −rank ˆX + rank w(Y, ˆY ),
⇔μ∗(Y, ˆY ) = rank ˆX −rankX,
⇔
μ∗
1(Y, ˆY ) = rank ˆX −rankX,
μ∗
2(Y, ˆY ) = 0,
and
μ(Y, ˆY ) = rank ˆX −rank X + rankw(Y, ˆY )
⇔

μ1(Y, ˆY ) = 0,
μ2(Y, ˆY ) = rank ˆX −rank X + rankw(Y, ˆY ),
⇔μ∗(Y, ˆY ) = 0,
⇔μ∗( ˆY, Y) = rank w(Y, ˆY ),
⇔

μ∗
1( ˆY, Y) = rank X −rank ˆX,
μ∗
2( ˆY, Y) = rank ˆX −rankX + rank w(Y, ˆY ),
⇔μ( ˆY, Y) = rankX −rank ˆX,
⇔
μ1( ˆY, Y) = rankX −rank ˆX,
μ2( ˆY, Y) = 0.
(viii) It holds
μ

J Y, J ˆY

= μ(Y, ˆY) + μ

J Y, (I 0)T 
−μ

J ˆY, (I 0)T 
.
(ix) For arbitrary symplectic matrices W, Z, ˆZ, we have
μ

WZ(0 I)T , W(0 I)T 
−μ

W ˆZ(0 I)T , W(0 I)T 
= μ

ˆZ(0 I)T , W −1(0 I)T 
−μ

Z(0 I)T , W −1(0 I)T 
.

156
3
Comparative Index Theory
The proof of the previous theorem is postponed for later (see Sects. 3.1.5
and 3.2.2).
At this moment we present a result which plays a key role in the comparative
index theory. In particular it shows how the comparative index behaves under
a symplectic transformation.
Theorem 3.6 (Main Theorem on the Comparative Index) Let Z, ˆZ, W be
arbitrary 2n × 2n symplectic matrices. Then
μ

WZ(0 I)T , W ˆZ(0 I)T 
−μ

Z(0 I)T , ˆZ(0 I)T 
= μ

WZ(0 I)T , W(0 I)T 
−μ

W ˆZ(0 I)T , W(0 I)T 
.
(3.16)
Proof First of all, observe that properties (ii) and (viii) of the the previous theorem
are particular cases of (3.16) with W = L or W = J , since it is not difﬁcult to
verify that
μ

LZ(0 I)T , L(0 I)T 
= μ

L ˆZ(0 I)T , L(0 I)T 
= 0
for a lower block-triangular matrix L (see (ii) of Remark 3.4). Then, to prove the
statement of theorem, it sufﬁces to apply Theorem 1.74 concerning the represen-
tation of a symplectic matrix W as a product W = H1L2H3 = J L1J L2J L3J ,
where H1, H2 are upper block-triangular symplectic matrices and L1, L2, L3 are
lower block-triangular symplectic matrices. We prove that if (3.16) holds for
symplectic matrices W1, W2 (instead of W) for arbitrary symplectic matrices Z, ˆZ,
then this formula holds also for their product W = W1W2. To show this, we use ﬁrst
(3.16) for W = W2 and then for W = W1. So we have
μ

W2 Z(0 I)T , W2 ˆZ(0 I)T 
−μ

Z(0 I)T , ˆZ(0 I)T 
= μ

W2 Z(0 I)T , W2(0 I)T 
−μ

W2 ˆZ(0 I)T , W2(0 I)T 
= μ

W −1
1 W2 Z(0 I)T , W −1
1 W2(0 I)T 
+ μ

W2 Z(0 I)T , W1(0 I)T 
−μ

W −1
1 W2 ˆZ(0 I)T , W −1
1 W2(0 I)T 
−μ

W2 ˆZ(0 I)T , W1(0 I)T 
.
On the other hand, by using the assumption that (3.16) holds for W = W1 and
arbitrary symplectic Z and ˆZ, we obtain
μ

W2 Z(0 I)T , W2 ˆZ(0 I)T 
= μ

W1(W −1
1 W2 Z)(0 I)T , W1(W −1
1 W2 ˆZ)(0 I)T 
= μ

W −1
1 W2 Z(0 I)T , W −1
1 W2 ˆZ(0 I)T 
+ μ

W2 Z(0 I)T , W1(0 I)T 
−μ

W2 ˆZ(0 I)T , W1(0 I)T 
.

3.1
Comparative Index and Its Properties
157
If we compare the obtained expressions and cancel the same terms, then we get
μ

W −1
1 W2 Z(0 I)T , W −1
1 W2 ˆZ(0 I)T 
−μ

Z(0 I)T , ˆZ(0 I)T 
= μ

W −1
1 W2 Z(0 I)T , W −1
1 W2(0 I)T 
−μ

W −1
1 W2 ˆZ(0 I)T , W −1
1 W2(0 I)T 
.
Consequently, formula (3.16) holds for W = W −1
1 W2. In particular, if (3.16) holds
for W1, then it holds also for W −1
1 . Now it sufﬁces to apply the already proved
statement to the pair of matrices W −1
1
and W2. Hence, we obtain validity of (3.16)
for W = W1W2. Applying the proved “multiplicative” rule to the product W =
J L1J L2J L3J , we obtain that (3.16) holds for any symplectic matrix W.
⊓⊔
Remark 3.7 Given that formula (3.16) holds with W = J and W = L (where
L is a lower block-triangular symplectic matrix), we can use the factorization of
a symplectic matrix W into the product W = J L1J L2J L3J (where L1, L2, L3
are lower block-triangular symplectic matrices) to show that (3.16) holds for any
symplectic matrix W. This yields another proof of Theorem 3.6. Indeed, it sufﬁces
to prove that if (3.16) holds for a symplectic W, then it holds also for J W and LW.
However, the validity of (3.16) for J W and LW follows by a double application of
properties (viii) and (ii) of Theorem 3.5, respectively.
By using (ix) of Theorem 3.5, formula (3.16) can be restated as
μ

WZ(0 I)T , W ˆZ(0 I)T 
−μ

Z(0 I)T , ˆZ(0 I)T 
= μ

ˆZ(0 I)T , W −1(0 I)T 
−μ

Z(0 I)T , W −1(0 I)T 
.
(3.17)
From (3.17) we derive the following property, which we call the “triangle
inequality” for the comparative index.
Theorem 3.8 (Triangle Inequality) For arbitrary symplectic matrices W1, W2,
and W3 of dimension 2n, we have
μW1(0 I)T , W3(0 I)T 
≤μ

W1(0 I)T , W2(0 I)T 
+ μ

W2(0 I)T , W3(0 I)T 
.
(3.18)
Moreover, the equality in (3.18) holds if and only if
μW −1
3 W1(0 I)T , W −1
3 W2(0 I)T 
= μ∗
W −1
1 W3(0 I)T , W −1
1 W2(0 I)T 
= 0.
(3.19)

158
3
Comparative Index Theory
Proof From (3.17) we have
μ

Z(0 I)T , ˆZ(0 I)T 
+ μ
 ˆZ(0 I)T , W −1(0 I)T 
= μ

WZ(0 I)T , W ˆZ(0 I)T 
+ μ

Z(0 I)T , W −1(0 I)T 
.
Putting Z := W1, ˆZ := W2 and W −1 := W3, we derive
μ

W1(0 I)T , W2(0 I)T 
+ μ

W2(0 I)T , W3(0 I)T 
= μ

W −1
3 W1(0 I)T , W −1
3 W2(0 I)T 
+ μ

W1(0 I)T , W3(0 I)T 
.
Using that μ

W −1
3 W1(0 I)T , W −1
3 W2(0 I)T 
≥0, we derive (3.18). From the last
identity, we have (3.19), where we also used Theorem 3.5(iii).
⊓⊔
The following lower bounds of the comparative index complement the upper
bounds in Theorem 3.5(vii).
Lemma 3.9 Let Y and ˜Y be 2n × n matrices satisfying (3.1). Then
min{μ(Y, ˆY), μ∗(Y, ˆY )}
≥max{0, rank ˆX −rankX, rank w(Y, ˆY ) −rankX}.
(3.20)
Proof We know from property (vi) in Theorem 3.5 and from μ∗( ˆY, Y) ≥0 that
the inequality μ(Y, ˆY ) ≥rank ˆX −rankX holds. Similarly from property (vi) in
Theorem 3.5 applied to μ( ˆY, Y) ≥0, we get μ∗(Y, ˆY) ≥rank ˆX −rankX. Next,
the properties (iii) and (vi) in Theorem 3.5 imply that
μ(Y, ˆY) = μ∗(Z−1(0 I)T , Z−1 ˆY)
= rankw(Y, ˆY ) −rank X + μ(Z−1 ˆY, Z−1(0 I)T )
(3.21)
= rankw(Y, ˆY ) −rank X + μ∗( ˆZ−1Y, ˆZ−1(0 I)T )
(3.22)
and then μ(Y, ˆY ) ≥rank w(Y, ˆY ) −rank X. In a similar way, we obtain from
the properties (iii) and (vi) in Theorem 3.5 for the dual comparative index (see
also Theorem 3.11 below) that μ∗(Y, ˆY ) ≥rankw(Y, ˆY ) −rank X. Therefore, the
estimates in (3.20) are proved.
⊓⊔

3.1
Comparative Index and Its Properties
159
Remark 3.10
(i) From Lemma 3.9 and property (vii) in Theorem 3.5, we have the estimates
r ≤μ(Y, ˆY) ≤p,
r ≤μ∗(Y, ˆY ) ≤p,
r := max{0, rank ˆX −rank X, rankw(Y, ˆY ) −rankX},
p := min{rankw(Y, ˆY ), rank ˆX, rank ˆX −rankX + rankw(Y, ˆY )},
⎫
⎪⎬
⎪⎭
(3.23)
where the lower and upper bounds r and p are such that
r + p = rank ˆX −rankX + rank w(Y, ˆY ).
(3.24)
Identity (3.24) can be easy veriﬁed by direct computations.
(ii) It follows from property (vii) of Theorem 3.5 that μ(Y, ˆY) achieves the upper
bound p if and only if the dual index μ∗(Y, ˆY ) coincides with the lower
bound r in (3.23). Replacing the roles of Y and ˆY in the formulation of
Theorem 3.5 (vii), we see that the condition μ∗(Y, ˆY ) = p is equivalent to
μ(Y, ˆY) = r.
(iii) Putting a := rankX, b := rank ˆX, and c := rank w(Y, ˆY ), we see from (3.23)
that the quantities a ≥0, b ≥0, c ≥0 obey the triangle inequalities a ≤b+c,
c ≤a + b, b ≤a + c. This fact can be proved independently using the
well-known inequalities rank(AB) ≤min{rankA, rank B} and rank(A+B) ≤
rankA + rankB.
3.1.4
Duality Principle for Comparative Index
Observe that the properties of μ(Y, ˆY) displayed in the previous subsection can be
reformulated for the dual comparative μ∗(Y, ˆY ). This reformulation is given in the
next theorem.
Theorem 3.11 Concerning the dual comparative index the following holds.
(i) The properties (i)–(ix) of Theorem 3.5 hold also for the dual comparative index
μ∗.
(ii) Any formula, proved using properties (i)–(ix) of Theorem 3.5, holds also for
the dual comparative index in the “dual form”; the comparative index μ is
replaced by the dual comparative index μ∗and vice versa.
Proof We will use the matrices P2 given in (1.151). In view of (3.12) and (3.13),
we have
μ∗∗
i (Y, ˆY ) = (μ∗
i (Y, ˆY))∗= (μi(P2Y, P2 ˆY))∗= μi(Y, ˆY ),
i ∈{1, 2}.

160
3
Comparative Index Theory
Since property (i) of Theorem 3.5 holds for any matrices satisfying (3.1), we have
μi(P2YC1, P2 ˆYC2) = μi(P2Y, P2 ˆY), i ∈{1, 2}. Consequently, this property holds
also for the dual comparative index. Then obviously
μ
⎛
⎝
k'
i=1
Wi(0 I)T ,
l'
j=1
Vj(0 I)T
⎞
⎠= μ
⎛
⎝
k'
i=1
Wi(0 −I)T ,
l'
j=1
Vj(0 −I)T
⎞
⎠
= μ
⎛
⎝P2
k'
i=1
(P2WiP2)(0 I)T , P2
l'
j=1
(P2VjP2)(0 I)T
⎞
⎠
= μ∗
⎛
⎝
k'
i=1
(P2WiP2)(0 I)T ,
l'
j=1
(P2VjP2)(0 I)T
⎞
⎠
(3.25)
for the products *k
i=1 Wi and *l
j=1 Vj of arbitrary symplectic matrices Wi and Vj.
Since P2J P2 = −J in view of property (iv) of Lemma 1.58, we have
Wi, Vj ∈Sp(2n) ⇔P2WiP2, P2VjP2 ∈Sp(2n),
i ∈{1, . . . , k}, j ∈{1, . . . , l}.
Consequently, since each of the properties (i)–(vii), (ix) holds for any symplectic
matrices Z, ˆZ, W, it holds also for the matrices P2ZP2, P2 ˆZP2, P2WP2. Therefore,
because of (3.25), these properties hold for μ∗. Further, a matrix L ∈Sp(2n)
is lower block-triangular if and only if P2LP2 ∈Sp(2n), where P2LP2 is also
lower block-triangular. This means in view of (3.25) that property (ii) for the dual
comparative index is proved. Now we prove property (viii) for the dual comparative
index. It follows from property (i) and J T = −J that the matrix J in (viii) can be
replaced by J T . Then we have
μ

J TZ(0 I)T , J T ˆZ(0 I)T 
= μ

Z(0 I)T , ˆZ(0 I)T 
+ μ

J TZ(0 I)T , J T(0 I)T 
−μ

J T ˆZ(0 I)T , J T(0 I)T 
for any symplectic matrices Z and ˆZ. Upon taking P2ZP2 and P2 ˆZP2 instead of Z
and ˆZ and using that J T = P2J P2, we derive
μ

P2J Z(0 −I)T , P2J ˆZ(0 −I)T 
= μ

P2Z(0 −I)T , P2 ˆZ(0 −I)T 
+ μ

P2J Z(0 −I)T , P2J (0 −I)T 
−μ

P2J ˆZ(0 −I)T , P2J (0 −I)T 
,

3.1
Comparative Index and Its Properties
161
or the same formula in the dual form
μ∗
J Z(0 −I)T , J ˆZ(0 −I)T 
= μ∗
Z(0 −I)T , ˆZ(0 I)T 
+ μ∗
J Z(0 −I)T , J (0 −I)T 
−μ∗
J ˆZ(0 −I)T , J (0 −I)T 
.
Since property (i) holds for the dual comparative index, we derive property (viii) for
the dual index as well. This completes the proof of the ﬁrst part of theorem. The
proof of the second part is obvious.
⊓⊔
In particular, the following “dual” reformulation of (3.17) holds:
μ∗
Z(0 I)T , ˆZ(0 I)T 
−μ∗
WZ(0 I)T , W ˆZ(0 I)T 
= μ∗
Z(0 I)T , W −1(0 I)T 
−μ∗
ˆZ(0 I)T , W −1(0 I)T 
.
(3.26)
And for further reference, we also present the dual version of the main theorem for
the comparative index (Theorem 3.6).
Corollary 3.12 Let Z, ˆZ, W be arbitrary 2n × 2n symplectic matrices. Then
μ∗
WZ(0 I)T , W ˆZ(0 I)T 
−μ∗
Z(0 I)T , ˆZ(0 I)T 
= μ∗
WZ(0 I)T , W(0 I)T 
−μ∗
W ˆZ(0 I)T , W(0 I)T 
.
(3.27)
3.1.5
Proof of Properties (i)–(vii), (ix) of Comparative Index
As we have already mentioned before, the proof of Theorem 3.5 substantially uses
the results of Sect. 1.6.3. The proof of the ﬁrst two properties uses Lemmas 1.63
and 1.64.
Proof of Property (i) According to Theorem 3.2,
μ1(YC1, ˆYC2) = rank [(I −(XC1) (XC1)†] ˆXC2
= rank (I −XX†) ˆXC2 = μ1(Y, ˆY ),
where we have used Lemma 1.63. Applying Lemma 1.64 in computing the
comparative index μ2(YC1, ˆYC2) by (3.10), we have
T = I −[(I −XX†) ˆXC2]†(I −XX†) ˆXC2
= C−1
2

I −[(I −XX†) ˆX]†(I −XX†) ˆX
 ˜T ,
det ˜T ̸= 0.

162
3
Comparative Index Theory
Since det C1 ̸= 0, then
CT
1 XT QXC1 = CT
1 XT UC1 ⇔XT QX = XT U,
so that in the computation of μ2(YC1, ˆYC2) by using (3.10), one can take any
symmetric matrices Q and ˆQ satisfying (3.8). Hence, we obtain
μ2(YC1, ˆYC2) = indT T CT
2 ˆXT ( ˆQ −Q) ˆXC2T  = μ2(Y, ˆY),
where we have used the above formula for T and the fact that the matrix ˜T is
invertible. This proves property (i).
⊓⊔
Proof of Property (ii) Recall that any symplectic lower block-triangular matrix L is
of the form L =

K
0
P KT −1

, where PK−1 = (PK−1)T . Then obviously
w(LY, L ˆY ) = Y T LTJ L ˆY = Y TJ ˆY = w(Y, ˆY ),
and I −(KX)†(KX)
=
I −X†X because of Lemma 1.63. According to
Deﬁnition 3.1, μ1(LY, L ˆY ) = μ1(Y, ˆY ) and the multiplication of Y and ˆY by L
do not change the matrix T . Consequently, it is sufﬁcient to prove that
P = T T 
wT (KX)†K ˆX

T = T T 
wT X† ˆX

T .
Observe that by Theorem 3.2, the matrix M in (3.3) can be replaced by the matrix
˜
M = (I −XX†) ˆX and that
˜
MT = 0, which is equivalent to ˆXT = XX† ˆXT or
K ˆXT = KXX† ˆXT . Consequently,
(KX)†K ˆX T = (KX)†KXX† ˆX T = X†XX† ˆXT = X† ˆX T ,
and hence μ2(LY, L ˆY ) = μ2(Y, ˆY ). The property (ii) is now proved.
⊓⊔
Proof of Property (iii) The proof is based on Theorem 3.2. Indeed,
−w

Z−1(0 I)T , Z−1 ˆZ (0 I)T 
= (0 I) ZT −1J T Z−1 ˆZ (0 I)T
= (0 I) J T ˆZ (0 I)T = ˆX,
(3.28)
where we have used that ZT −1J T Z−1 = J T . Further, (I 0) Z−1(0 I)T = −XT ,
and hence the number μ∗
1 in the right-hand side of property (iii) computed according
to (3.12), (3.3) complies with μ1(Y, ˆY ), computed using the ﬁrst statement of
Theorem 3.2. It is not difﬁcult to verify that (I 0) Z−1 ˆZ(0 I)T = −w(Y, ˆY ), and,
hence, the index μ∗
2, computed in the right-hand side of property (iii), is μ2(Y, ˆY ),
where we also used the symmetry of the matrix P in (3.4).
⊓⊔

3.1
Comparative Index and Its Properties
163
The proof of properties (iv) and (v) is based on the results of Lemma 1.68 and
Theorem 1.70.
Proof of Property (iv) We will use Theorem 1.70 applying factorizations (1.180) to
Y and ˆY. Using the already proved properties (i) and (ii),
μ(Y, ˆY) = μN(I 0)T , L−1 ˆL ˆN (I 0)T ,
(3.29)
where the symplectic matrices L and N := NXX† are determined by (1.180) and
(1.179) with Y, and the symplectic matrices ˆL and ˆN := N ˆX ˆX† are determined in
a similar way with ˆY. In order to shorten the notation, we deﬁne the matrices
F := XX†,
G := I −XX†,
ˆF := ˆX ˆX†,
ˆG := I −ˆX ˆX†.
Then, evaluating (3.29) according to formulas (3.5) and (3.10), we have M = G ˆF,
P = T ˆF( ˆQ −Q) ˆFT , and
ˆFT = ˆF(I −M†M) = (F + G) ˆF(I −M†M) = F ˆF(I −M†M).
(3.30)
The matrix NT ˆN (0 I)T satisﬁes (3.1), and then using (1.171) for NT ˆN (0 I)T and
Remark 1.60(vii), we obtain
F ˆF(I −M†M) = F(F ˆF + G ˆG) [I −M†M −(M∗T )†M∗T ]
= F [I −MM† −M∗T (M∗T )†] (F ˆF + G ˆG) ×
× [I −M†M −(M∗T )†M∗T ]
= −F(I −M∗†M∗) L,
where M∗= ˆGF and the invertible matrix L are determined for NT ˆN (0 I)T by
formula (1.174). Therefore, we have proved that
ˆFT = −FT ∗L,
T = I −M†M, T ∗= I −M∗†M∗,
M = G ˆF,
M∗= ˆGF,
det L ̸= 0,

(3.31)
or T ˆF( ˆQ −Q) ˆFT = LT T ∗F( ˆQ −Q) FT ∗L. Therefore, we have
μ2(Y, ˆY ) = ind T ˆF( ˆQ−Q) ˆFT = ind T ∗F( ˆQ−Q) FT ∗= μ∗
2( ˆY, Y),
(3.32)
which shows that the property (iv) holds.
⊓⊔

164
3
Comparative Index Theory
Proof of Property (v) We ﬁrst substitute factorization (1.180) for Y and ˆY into
w(Y, ˆY ). Using the notation of Theorem 1.70, we obtain
w(Y, ˆY ) = MT [G ˆF −F ˆG + F( ˆQ −Q) ˆF] ˆM
= MT M1[M −M∗T + T ∗F( ˆQ −Q) ˆFT ] M2 ˆM,
where the invertible matrices M and ˆM are determined by (1.180) via Y and ˆY, the
matrices M and M∗are given by (3.31), and the matrices
M1 := I + FT ∗( ˆQ −Q) M†G, M2 = I −ˆGM∗T †( ˆQ −Q) ˆF
are obviously also invertible in view of (1.163) and the orthogonality of the
projectors F, G, ˆF, ˆG. Using Lemma 1.68 again for the matrix NT ˆN (0 I)T (see
the proof of the property (iv)) and using (3.31), we have
w(Y, ˆY ) = −MT M1[M M† + M∗†M∗+ T ∗F( ˆQ −Q) FT ∗] L M2 ˆM.
Consequently,
rankw(Y, ˆY ) = rank [MM† + M∗†M∗+ T ∗F( ˆQ −Q) FT ∗]
= rank M + rankM∗+ rankT ∗F( ˆQ −Q) FT ∗
= μ1(Y, ˆY ) + μ1( ˆY, Y) + ind T ∗F( ˆQ −Q) FT ∗
+ ind T ∗F(−ˆQ + Q) FT ∗= μ(Y, ˆY) + μ( ˆY, Y),
where we have used (3.32), and in computing the rank of a sum of two matrices,
we have used that rank(A + B) = rank A + rankB if AT B = ABT = 0 (see
Remark 1.60(vii)). This completes the proof of property (v).
⊓⊔
Proof of Property (vi) This property is a consequence of formula (1.142) for the
rank of a product of two matrices. But it can also be obtained from the proofs of the
previous properties. In fact, using properties (iii) and (v), we obtain
μ(Y, ˆY) = μ∗
Z−1(0 I)T , Z−1 ˆZ(0 I)T 
= rank ˆX −μ∗
Z−1 ˆZ(0 I)T , Z−1(0 I)T 
,
(3.33)
where, in addition, it is used in formula (3.28). Further
μ∗
Z−1 ˆZ(0 I)T , Z−1(0 I)T 
= μ

ˆZ−1Z(0 I)T , ˆZ−1(0 I)T 
= rank X −μ

ˆZ−1(0 I)T , ˆZ−1Z(0 I)T 
= rank X −μ∗
ˆZ(0 I)T , Z(0 I)T 
,

3.1
Comparative Index and Its Properties
165
where we have again used properties (iii) and (v). Finally, using property (iv), we
conclude the proof of property (vi).
⊓⊔
Proof of Property (vii) Observe that μ(Y, ˆY ) ≤min{rankw, rank ˆX} follows from
property (v) and (3.33). Combining properties (v) and (vi), we also have the identity
μ(Y, ˆY ) + μ∗(Y, ˆY ) = rank ˆX −rankX + rank w(Y, ˆY ) ≥0,
(3.34)
which means that μ(Y, ˆY ) ≤p(Y, ˆY ), where p(Y, ˆY ), is given by (3.15). A similar
estimate holds also for μ∗(Y, ˆY ). In view of the property (v) and the dual form of
the properties (vi) and (v), it is also obvious that
μ(Y, ˆY ) = rank ˆX
(v)
⇔μ( ˆY, Y) = rankw(Y, ˆY ) −rank ˆX
(vi)
⇔μ∗(Y, ˆY ) = rankw(Y, ˆY ) −rankX
(v)
⇔μ∗( ˆY, Y) = rank X.
(3.35)
Further, by (3.33) we have
μ(Y, ˆY) = rank ˆX
⇔
μ∗
Z−1 ˆZ(0 I)T , Z−1(0 I)T 
= 0,
so that
μ∗
1

Z−1 ˆZ(0 I)T , Z−1(0 I)T 
= rank[I −w(Y, ˆY ) w(Y, ˆY )†] XT = 0.
Using the properties (iii) and (vi) with Z−1(0 I)T and Z−1 ˆZ(0 I)T , we obtain
μ1(Y, ˆY ) = μ∗
1

Z−1(0 I)T , Z−1 ˆZ(0 I)T 
= rank w(Y, ˆY ) −rankX.
(3.36)
Hence, we showed that the equality μ(Y, ˆY ) = rank ˆX is necessary and sufﬁcient
for the identities
μ1(Y, ˆY ) = rankw(Y, ˆY ) −rankX,
μ2(Y, ˆY ) = rank ˆX + rank X −rankw(Y, ˆY )

(3.37)
Applying the property (vi), we see that (3.36) is equivalent with
μ1( ˆY, Y) = rankw(Y, ˆY ) −rank ˆX.
(3.38)
Combining conditions (3.36), (3.38) with (3.35), we derive all equalities referred to
the case μ(Y, ˆY) = rank ˆX. Similar equivalences hold also for the case μ∗(Y, ˆY ) =
rank ˆX because of the duality principle (see Sect. 3.1.4). The remaining cases of

166
3
Comparative Index Theory
μ(Y, ˆY) = rankw(Y, ˆY ) and μ(Y, ˆY) = rank ˆX −rankX + rankw(Y, ˆY ) can be
treated analogously.
⊓⊔
Proof of Property (ix) Applying properties (v), (iii), and ﬁnally (vi), we obtain
μ

WZ(0 I)T , W(0 I)T 
−μ

W ˆZ(0 I)T , W(0 I)T 
= μ

(0 I)T , Z(0 I)T 
−μ

(0 I)T , ˆZ(0 I)T 
−μ

W(0 I)T , WZ(0 I)T 
+ μ

W(0 I)T , W ˆZ(0 I)T 
= μ

(0 I)T , Z(0 I)T 
−μ

(0 I)T , ˆZ(0 I)T 
−μ∗
W −1(0 I)T , Z(0 I)T 
+ μ∗
W −1(0 I)T , ˆZ(0 I)T 
= μ

ˆZ(0 I)T , W −1(0 I)T 
−μ

Z(0 I)T , W −1(0 I)T 
.
This completes the proofs of properties (i)–(vii) and (ix) in Theorem 3.5.
⊓⊔
Concerning the proof of the property (viii), this proof requires results about index
of a block symmetric operator. The connection of the comparative index and the
index of some symmetric operators is the subject of the next section.
3.2
Comparative Index and Symmetric Operators
This section is devoted to establishing a connection between the comparative index
and the negative inertia index of some symmetric operator. In the next chapter, we
will show that the multiplicity of a focal point in the interval (k, k + 1] is fully
determined by a suitable comparative index, and, hence, there is a deep connection
between the (non)oscillation and the (non)existence of negative eigenvalues of
a symmetric operator associated with (SDS).
3.2.1
Index of Block Symmetric Matrices
In this subsection we prove results concerning the index of symmetric 2n × 2n
matrices consisting of four n × n matrices.
Let B
A∼C mean that the matrices B and C are congruent, i.e., B = AT CA with
det A ̸= 0. An important role in our treatment is played by the next lemma.

3.2
Comparative Index and Symmetric Operators
167
Lemma 3.13 Let D, X ∈Rn×n, where D is symmetric. Then
ind
 0 X
XT D

= rankX + ind (I −X†X) D (I −X†X).
Proof Suppose that the singular value decomposition of X is of the form V UT ,
where  is the diagonal matrix having the singular values of X on the diagonal, and
U and V are orthogonal matrices. Then
K :=
 0 X
XT D

A∼
 0
F
F GUTDUG

,
A :=
(G + ) V T
S
0
UT

,
where F := †, G := I −F and S := 1
2 UT DU(I + G) UT . Then
ind K = ind
0 F
F 0

+ diag{0, GUTDUG}

= ind
0 F
F 0

+ ind (GUTDUG)
and
ind
0 F
F 0

= rankF = rankX,
and, in view of Remark 1.60 (ii),
ind (GUTDUG) = ind (I −X†X)TD (I −X†X).
The proof is complete.
⊓⊔
An important consequence of Lemma 3.13 and of Theorem 3.2 is formulated in
the next statement.
Lemma 3.14 Let Y and ˆY satisfy (3.1). Then, using the notation of Theorem 3.2,
we have
μ(Y, ˆY) = i−(),
μ∗(Y, ˆY ) = i+(),
 =

0
(I −XX†) ˆX
ˆXT (I −XX†)
ˆXT ( ˆQ −Q) ˆX

,
Y =
X
U

,
ˆY =
 ˆX
ˆU

,
⎫
⎪⎬
⎪⎭
(3.39)
where i−() = ind(Φ) is the number of negative eigenvalues, and i+() is the
number of positive eigenvalues of the (symmetric) matrix .

168
3
Comparative Index Theory
Proof The result follows directly from Lemma 3.13, Theorem 3.2, and (3.12).
⊓⊔
In particular, for the cases mentioned in Remark 3.4, the corresponding matrices
have the following form.
(i) If det X ̸= 0 and det ˆX ̸= 0, then  = diag{0, ˆXT ( ˆQ −Q) ˆX} and
μ(Y, ˆY ) = i−( ˆQ −Q),
μ∗(Y, ˆY ) = i+( ˆQ −Q).
(ii) If X = 0, it follows that
 =
 0
ˆX
ˆXT
ˆXT ( ˆQ −Q) ˆX

,
μ(Y, ˆY) = μ∗(Y, ˆY ) = rank ˆX.
(iii) If U = 0, then det X ̸= 0 and hence
 = diag{0, ˆXT ˆQ ˆX},
μ(Y, ˆY) = i−( ˆXT ˆU),
μ∗(Y, ˆY ) = i+( ˆXT ˆU).
The next statement is a generalization of Lemma 3.13.
Lemma 3.15 Let AT = A, DT = D, and X be square matrices. Then
ind
 A X
XT D

= ind A + ind

0
(I −AA†) X
[(I −AA†)X]T D −XT A†X

= ind A + rank M
+ ind (I −M†M) (D −XT A†X) (I −M†M)
= ind D + ind
A −XD†XT XT (I −DD†)
(I −DD†) X
0

= ind D + rank ˜M
+ ind (I −˜M† ˜M) (A −XD†XT ) (I −˜M† ˜M),
where ˜M := (I −DD†) XT and M := (I −AA†) X.
Proof Using the congruent transformation, we obtain
K =
 A X
XT D

P∼

A
(I −AA†) X
[(I −AA†) X]T D −XT A†X

, P =
I A†X
0
I

.

3.2
Comparative Index and Symmetric Operators
169
Consequently, in view of Lemma 3.13,
ind K = ind

diag{A, 0} +

0
(I −AA†) X
[(I −AA†) X]T D −XT A†X

= ind A + ind

0
(I −AA†) X
[(I −AA†) X]T D −XT A†X

= ind A + rankM + ind [(I −M†M) (D −XT A†X) (I −M†M)],
where M = (I −AA†) X. Observe that
K
P1∼
D XT
X A

= ˜K,
P1 =
0 I
I 0

.
Consequently, applying the just proved result to the congruent matrix ˜K, we obtain
ind K = ind ˜K = ind D + rank ˜M + ind [(I −˜M† ˜M) (A −XD†XT ) (I −˜M† ˜M)],
where ˜M = (I −DD†) XT . The statement is proved.
⊓⊔
The following corollary to Lemma 3.15 was used in the formulation of a compar-
ison theorem for differential and difference Hamiltonian systems; see [43, 51, 205].
Corollary 3.16 The symmetric matrix

A X
XT D

is nonnegative deﬁnite if and only
if
A ≥0,
D −XT A†X ≥0,
Im X ⊆Im A,
(3.40)
which is equivalent to
D ≥0,
A −XD†XT ≥0,
Ker D ⊆Ker X.
(3.41)
3.2.2
Symmetric Operator [V ] and Proof of Property (viii)
In the deﬁnition of the quadratic functional (2.56), we have used the symmetry of
the operator
[Sk] := ST
k
0 I
0 0

Sk −
0 I
0 0

acting on the group of symplectic matrices determining (SDS). Recall that according
to property (v) of Lemma 1.58, the 2n × n blocks of Sk satisfy (1.147) and (1.148).

170
3
Comparative Index Theory
A generalization of [Sk] is the operator, acting on pairs of matrices Y, ˆY
satisfying only (3.1). Let a 2n × 2n matrix V consist of 2n × n matrices
V = (Y ˆY),
Y =
X
U

,
ˆY =
 ˆX
ˆU

.
(3.42)
Then under the assumption
XT U = UT X,
ˆXT ˆU = ˆUT ˆX,
(3.43)
the operator
[V ] := diag{UT , ˆXT } V =
UT X UT ˆX
ˆXT U ˆXT ˆU

= V T
0 I
0 0

V −

0 w(Y, ˆY )
0
0

(3.44)
is symmetric (here w(Y, ˆY ) = Y TJ ˆY). Next we prove some properties of this
operator.
Lemma 3.17
(i) Let W be a symplectic matrix, and V be determined by (3.42) and (3.43). Then
[WV ] = V T [W] V + [V ],
(3.45)
(ii) It holds
[J V J T ] = −J [V ] J T .
(3.46)
Proof
(i) By the deﬁnition of the operator , we have
[WV ] = V T W T
0 I
0 0

WV −
0 w(WY, W ˆY )
0
0

= V T

W T
0 I
0 0

W −
0 I
0 0
 
V + V T
0 I
0 0

V −
0 w(Y, ˆY )
0
0

= V T [W] V + [V ].
where we have used (1.148) for W and
w(WY, W ˆY ) = Y T W TJ W ˆW = Y TJ ˆY = w(Y, ˆY ).
This proves (3.45).

3.2
Comparative Index and Symmetric Operators
171
(ii) Again, by the deﬁnition of , we have
−J [V ]J T = −J
UT X UT ˆX
ˆXT U ˆXT ˆU

J T
=
−ˆXT ˆU
ˆXT U
UT ˆX −UT X

= [J ˆY, −J Y] = [J VJ T ],
which proves (3.46).
⊓⊔
An important role in our treatment is played by the following results describing
the relationship between [V ] and the comparative index; see [115, Lemma 4.4].
Proposition 3.18 If V = (Y, ˆY) and Y, ˆY satisfy (3.1), then
ind [V ] = ind XT U + μ(Y, ˆY) = ind ˆXT ˆU + μ∗(J ˆY, J Y),
(3.47)
hence, the comparative index μ(Y, ˆY) can be expressed as
μ(Y, ˆY ) = ind [V ] −ind XT U.
(3.48)
Proof To compute ind [V ], we use (1.180) for Y and ˆY. Using the same notation
as in the proof of property (iv) in Theorem 3.5, it is possible to verify that
[V ] = AT
FQF
−G ˆF
−ˆFG
2 ˆFG ˆF + ˆF ˆQ ˆF −ˆFFQF ˆF

A,
A =
(I −GQF) M
ˆF ˆM
0
ˆM

,
where det M ̸= 0, det ˆM ̸= 0, det(I −GQF) ̸= 0 and, hence, the matrix A is
invertible. Consequently, by Lemma 3.15, we have
ind [V ] = ind FQF + ind

0
−G ˆF
−ˆFG
2 ˆFG ˆF + ˆF ˆQ ˆF −ˆF FQF ˆF

= ind MT FQFM + rankM + ind T ˆF( ˆQ −Q) ˆFT
= ind XT QX + μ(Y, ˆY ) = ind XT U + μ(Y, ˆY),
where we have used (3.31), (3.32), and (3.30). This proves the ﬁrst part of formula
(3.47). To prove the second part, we use property (ii) of Lemma 3.17. Then
ind [V ] = ind (−Λ[J V J T ]) = ind (−Λ[J ˆY, −J Y])
= ind ˆXT ˆU + μ∗(J ˆY, J Y),

172
3
Comparative Index Theory
where ind (−[J ˆY, −J Y]) is computed analogously as ind [V ] in the proof of
the ﬁrst equality in (3.47). The proof is now complete.
⊓⊔
As a corollary to Proposition 3.18, we have a similar result for the dual
comparative index μ∗(Y, ˆY).
Corollary 3.19 Under the assumptions of Proposition 3.18, we have
ind (−[V ]) = ind (−XT U) + μ∗(Y, ˆY ) = ind (−ˆXT ˆU) + μ(J ˆY, J Y).
(3.49)
Hence, the comparative index μ∗(Y, ˆY ) can be expressed as
μ∗(Y, ˆY ) = ind (−[V ]) −ind (−XT U),
(3.50)
and then
μ(Y, ˆY) + μ∗(Y, ˆY ) = rank [V ] −rank XT U
= rank ˆX −rankX + rankw(Y, ˆY ).
(3.51)
Proof We apply Proposition 3.18 to the case P2Y, P2 ˆY, where P2 = diag{I, −I}
(see Lemma 1.58). It is easy to verify that [ ˜V ] = −[V ], where ˜V := (P2Y P2 ˆY)
and V := (Y
ˆY). Then the proof of (3.49) and (3.50) is completed. Formula
(3.51) follows from (3.34). Note that for the case of w(Y, ˆY ) = I, we have
rank([V ]) = rankU + rank ˆX, where the nonsingularity of V is used. For this
special case, formula (3.51) takes the form
μ(Y, ˆY) + μ∗(Y, ˆY ) = rankU + rank ˆX −rank(XT U) = rank ˆX −rank X + n,
which can also be derived from (1.185).
⊓⊔
We conclude this section with a statement, which yields the proof of property
(viii) of Theorem 3.5.
Proposition 3.20 Formula (3.47) and property (viii) of Theorem 3.5 are equivalent.
Proof By property (vi) of Theorem 3.5, we obtain the equality
μ∗(J ˆY, J Y) = rank U −rank ˆU + μ(J Y, J ˆY).
Consequently, (3.47) equivalent to
μ∗(J ˆY, J Y) = μ(Y, ˆY ) + ind XT U −ind ˆXT ˆU,
and this is equivalent to
μ(J Y, J ˆY) = μ(Y, ˆY ) + p,
p := ind XT U −rankU + rank ˆU −ind ˆXT ˆU.

3.3
Comparative Index for Symplectic Matrices
173
On the other hand, it is possible to prove that p = μ

J Y, (I 0)T 
−μ

J ˆY, (I 0)T 
.
Indeed, if we compute the above comparative indices by using Theorem 3.2, we
obtain
μ

J Y, (I 0)T 
= rank (I −UU†) + ind XT U = n −rankU + ind XT U,
μ

J ˆY, (I 0)T 
= rank (I −ˆU ˆU†) + ind ˆXT ˆU = n −rank ˆU + ind ˆXT ˆU.
The proof is complete.
⊓⊔
3.3
Comparative Index for Symplectic Matrices
In this section we introduce the comparative index for a pair of matrices determined
by the blocks of symplectic matrices W, ˆW ∈R2n×2n. The comparative index
for symplectic matrices plays a fundamental role in comparison theorems for
symplectic systems. It enables to formulate an exact relationship between the
number of focal points of conjoined bases of two symplectic systems. Another
problem, where we apply the results of this section, is the theory of boundary value
problems associated with (SDS).
3.3.1
Basic Properties
We introduce the following notation. For any matrix W =
 A B
C D

∈R2n×2n, we
deﬁne the 4n × 2n matrix
⟨W⟩:=
⎛
⎜⎜⎝
I
0
A B
0 −I
C D
⎞
⎟⎟⎠.
(3.52)
For easier application of the identities below, we note that when W is a symplectic
matrix, then its inverse satisﬁes W −1 =

DT
−BT
−CT
AT

; see Lemma 1.58(ii).
Lemma 3.21 Let W, ˆW ∈R2n×2n be symplectic matrices. Then the matrices ⟨W⟩
and ⟨ˆW⟩deﬁned in (3.52) have the following properties.
(i) The matrix ⟨W⟩satisﬁes (3.1), i.e., rank⟨W⟩= 2n, ⟨W⟩TJ ⟨W⟩= 0.
(ii) For the Wronskian of ⟨W⟩and ⟨ˆW⟩, we have the relations
w(⟨W⟩, ⟨ˆW ⟩) = ⟨W⟩TJ ⟨ˆW⟩= −J (I −W −1 ˆW),
rankw(⟨W⟩, ⟨ˆW ⟩) = rank(W −ˆW),

174
3
Comparative Index Theory
and the comparative index for ⟨W⟩and ⟨ˆW⟩obeys the estimate
μ(⟨W⟩, ⟨ˆW ⟩)
≤min
!
rank(W −ˆW), n + rank ˆB, rank(W −ˆW) + rank ˆB −rankB
"
.
(3.53)
(iii) The comparative index of ⟨W⟩and ⟨ˆW⟩satisﬁes
μi(⟨W⟩, ⟨ˆW ⟩) = μ∗
i (⟨W −1⟩, ⟨ˆW −1⟩), i ∈{1, 2}.
(iv) The comparative index of ⟨W⟩and ⟨ˆW⟩satisﬁes
μ(⟨W⟩, ⟨ˆW ⟩) = rank ˆB −rankB + μ∗(⟨ˆW⟩, ⟨W⟩).
(v) The comparative index of ⟨W⟩and ⟨ˆW⟩satisﬁes
μ(⟨W⟩, ⟨ˆW ⟩) = μ

W(0 I)T , ˆW(0 I)T 
+ μ

⟨ˆW −1W⟩, ⟨I⟩

(3.54)
= μ∗
W −1(0 I)T , ˆW −1(0 I)T 
+ μ

⟨W ˆW −1⟩, ⟨I⟩

.
(3.55)
(vi) We have the identity
μ

W(0 I)T , ˆW(0 I)T 
−μ∗
W −1(0 I)T , ˆW −1(0 I)T 
= μ

⟨W ˆW −1⟩, ⟨I⟩

−μ

⟨ˆW −1W⟩, ⟨I⟩

.
Proof of Parts (i)–(iv) For part (i) we have that rank⟨W⟩= rank W = 2n and
I 0
A B
T 0 −I
C D

=
CT A CT B
BT C BT D

=
AT C CT B
BT C DT B

=
0 −I
C D
T I 0
A B

,
where we have used property (1.145) of a symplectic matrix.
For part (ii) we have
⟨W⟩TJ ⟨ˆW⟩=
I 0
A B
T 0 −I
ˆC
ˆD

−
0 −I
C D
T I 0
ˆA ˆB

=J T (I −W −1 ˆW).
Consequently, we obtain the identity
rank⟨W⟩TJ ⟨ˆW⟩ = rank (I −W −1 ˆW) = rank [W −1(W −ˆW)]
= rank (W −ˆW).

3.3
Comparative Index for Symplectic Matrices
175
Estimate (3.53) follows from the upper bound for the comparative index in property
(vii) of Theorem 3.5, where we evaluate
rank
I 0
A B

= rank
I 0
A I

diag{I, B}
= rank diag{I, B} = n + rank B.
By a similar way, we evaluate the rank of the 2n × 2n upper block of ⟨ˆW⟩.
For the proof of property (iii), we note that by Theorem 3.5(i)–(ii), we obtain
μi

⟨W⟩, ⟨ˆW ⟩

= μi

L⟨W⟩W −1, L⟨ˆW⟩ˆW −1
= μi

P2⟨W −1⟩, P2⟨ˆW −1⟩

for i ∈{1, 2}, where
L := diag
0 I
I 0

,
0 I
I 0
 
,
P2 = diag{I2n, −I2n},
and, in view of (3.13), μi(P2Y, P2 ˆY) = μ∗
i (Y, ˆY) for any Y =
X
U

and ˆY =
 ˆX
ˆU

satisfying (3.1).
Property (iv) is a consequence of Theorem 3.5(vi). Obviously, for our particular
case, we evaluate the ranks of the upper blocks of ⟨W⟩and ⟨ˆW⟩by analogy with the
proof of (ii) and substitute them into Theorem 3.5(vi). This then yields the proof of
part (iv) of this lemma.
⊓⊔
The proof of properties (v) and (vi) will be given in Sect. 3.3.6. Here we point
out that this rather technical proof is based on a generalization of Theorem 3.6
(see Lemma 3.23 below) and on additional algebraic properties of the comparative
indices for (3.52) (see Proposition 3.37).
The main result of this section is the generalization of Theorem 3.6 to the case
where instead of Z, ˆZ, W, four matrices Z, ˆZ, W, ˆW are used. For this purpose
introduce the notation
L(Y, ˆY , W, ˆW) := μ( ˆW ˆY, ˆW(0 I)T ) −μ(WY, W(0 I)T )
+μ(WY, ˆW ˆY) −μ(Y, ˆY),

(3.56)
where Y = Z(0 I)T and ˆY = ˆZ(0 I)T are 2n × n matrices satisfying conditions
(3.1). We prove the following properties of operator (3.56).
Lemma 3.22 Let Y and ˆY be 2n × n matrices satisfying (3.1). Then the following
properties of operator in (3.56) hold.
(i) For any symplectic matrix W, we have L(Y, ˆY , W, W) = 0.
(ii) For p ∈N and symplectic matrices W1, . . . , Wp and ˆW1, . . . , ˆWp, we deﬁne
Z(r) :=
r'
k=1
Wr−k+1 = WrWr−1 . . . W1, ˆZ(r) :=
r'
k=1
ˆWr−k+1, r = 1, . . . , p,

176
3
Comparative Index Theory
and Z(0) := I, ˆZ(0) := I. Then we have the following “multiplicative
property” of operator (3.56):
L(Y, ˆY , Z(p), ˆZ(p)) =
p

r=1
L

Z(r −1) Y, ˆZ(r −1) ˆY, Wr, ˆWr

+
p

r=1

μ

Z(r) (0 I)T , Wr(0 I)T 
−μ
 ˆZ(r) (0 I)T , ˆWr(0 I)T 
.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(3.57)
Proof For the proof of (i), we use Theorem 3.6, which is equivalent to property (i).
For the proof of (ii), we note that by the deﬁnition in (3.56), we have on the left-hand
side of (3.57) that
L(Y, ˆY , Z(p), ˆZ(p))
= μ
 ˆZ(p) ˆY , ˆZ(p) (0 I)T 
−μ

Z(p) Y, Z(p) (0 I)T 
+μ

Z(p) Y, ˆZ(p) ˆY

−μ(Y, ˆY ).
⎫
⎪⎬
⎪⎭
(3.58)
Similarly, for the operator L

Z(r −1) Y, ˆZ(r −1) ˆY, Wr, ˆWr

on the right-hand side
of (3.57), we derive
L

Z(r −1) Y, ˆZ(r −1) ˆY, Wr, ˆWr

= μ
 ˆZ(r) ˆY , ˆWr(0 I)T 
−μ(Z(r) Y, Wr(0 I)T ) + μ(Z(r −1) Y, ˆZ(r −1) ˆY ),
and then
LZ(r −1) Y, ˆZ(r −1) ˆY , Wr, ˆWr
 −μ( ˆZ(r) (0 I)T , ˆWr(0 I)T )
+ μ(Z(r) (0 I)T , Wr(0 I)T )
= −μ
 ˆZ(r) (0 I)T , ˆWr(0 I)T 
+ μ
 ˆZ(r) ˆY , ˆWr(0 I)T 
+ μ

Z(r) (0 I)T , Wr(0 I)T 
−μ

Z(r) Y, Wr(0 I)T 
+ μZ(r −1) Y, ˆZ(r −1) ˆY 
= −L ˆZ(r −1) ˆY , ˆZ(r −1) (0 I)T , ˆWr, ˆWr
 + μ ˆZ(r −1) ˆY , ˆZ(r −1) (0 I)T 
+ LZ(r −1) Y, Z(r −1) (0 I)T , Wr, Wr
 −μZ(r −1) Y, Z(r −1) (0 I)T 
+ μ

Z(r −1) Y, ˆZ(r −1) ˆY

.
(3.59)
By property (i) we have
L( ˆZ(r −1) ˆY, ˆZ(r −1) (0 I)T , ˆWr, ˆWr) = 0,
L(Z(r −1) Y, Z(r −1) (0 I)T , Wr, Wr) = 0.

3.3
Comparative Index for Symplectic Matrices
177
Then summing (3.59) from r = 1 to r = p, we derive equality (3.57). The proof is
complete.
⊓⊔
Using notation (3.56) we begin with the following auxiliary result, which turns
out to be highly important for future applications.
Lemma 3.23 Let W, Z,
ˆW, ˆZ be symplectic matrices. Then for the operator
L(Y, ˆY , W, ˆW) given by (3.56) with Y = Z(0 I)T and ˆY = ˆZ(0 I)T , we have
L(Y, ˆY , W, ˆW)
= μ

W(0 I)T , ˆW(0 I)T 
+ μ

ˆW −1WZ(0 I)T , Z(0 I)T 
−μ

ˆZ−1 ˆW −1WZ(0 I)T , ˆZ−1Z(0 I)T 
−μ

ˆW −1WZ(0 I)T , ˆW −1W(0 I)T 
,
(3.60)
and
L(Y, ˆY , W, ˆW)
= μ

W(0 I)T , W ˆW −1(0 I)T 
+ μ

ˆZ−1Z(0 I)T , ˆZ−1 ˆW −1WZ(0 I)T 
−μ

ˆWZ(0 I)T , WZ(0 I)T 
−μ

WZ(0 I)T , W ˆW −1(0 I)T 
.
(3.61)
Proof For the proof of equality (3.60), we apply Lemma 3.22(ii) to the case p = 2,
W1 := ˆW −1W, W2 := ˆW, ˆW1 := I, ˆW2 := ˆW, and then by (3.57), we have
L(Y, ˆY , W, ˆW) = L(Y, ˆY , ˆW −1W, I) + μ(W(0 I)T , ˆW(0 I)T ),
(3.62)
where we have used that L( ˆW −1WY, ˆY , ˆW, ˆW) = 0 by Lemma 3.22(i). By (3.56),
L(Y, ˆY , ˆW −1W, I) = −μ( ˆW −1WZ(0 I)T , ˆW −1W(0 I)T )
+ μ( ˆW −1WZ(0 I)T , ˆZ(0 I)T ) −μ(Z(0 I)T , ˆZ(0 I)T ),
and by Theorem 3.6 for the last two addends in the previous formula, we have
μ( ˆW −1WZ(0 I)T , ˆZ(0 I)T ) −μ(Z(0 I)T , ˆZ(0 I)T )
= μ( ˆW −1WZ(0 I)T , Z(0 I)T ) −μ( ˆZ−1 ˆW −1WZ(0 I)T , ˆZ−1Z(0 I)T ).

178
3
Comparative Index Theory
Finally, for the operator L(Y, ˆY , ˆW −1W, I) in (3.62), we have the representation
L(Y, ˆY , ˆW −1W, I) = −μ( ˆW −1WZ(0 I)T , ˆW −1W(0 I)T )
+ μ( ˆW −1WZ(0 I)T , Z(0 I)T ) −μ( ˆZ−1 ˆW −1WZ(0 I)T , ˆZ−1Z(0 I)T )
which completes the proof of (3.60).
Similarly, the proof of (3.61) is derived by using Lemma 3.22(ii) with p = 2,
W1 :=
ˆW, W2 := W ˆW −1,
ˆW1 :=
ˆW,
ˆW2 := I, and then by (3.57) and
Lemma 3.22(i), we have
L(Y, ˆY , W, ˆW) = L( ˆWY, ˆW ˆY, W ˆW −1, I) + μ(W(0 I)T , W ˆW(0 I)T ).
(3.63)
By (3.56), we get
L( ˆWY, ˆW ˆY, W ˆW −1, I) = −μ(WZ(0 I)T , W ˆW −1(0 I)T )
+ μ(WZ(0 I)T , ˆW ˆZ(0 I)T ) −μ( ˆWZ(0 I)T , ˆW ˆZ(0 I)T ),
and by Theorem 3.6 for the last two addends in the previous formula we have
μ( ˆWZ(0 I)T , ˆW ˆZ(0 I)T ) −μ(WZ(0 I)T , ˆW ˆZ(0 I)T )
= μ( ˆWZ(0 I)T , WZ(0 I)T ) −μ( ˆZ−1Z(0 I)T , ˆZ−1 ˆW −1WZ(0 I)T ).
Finally, the operator L( ˆWY, ˆW ˆY, W ˆW −1, I) takes the form
L( ˆWY, ˆW ˆY, W ˆW −1, I) = −μ(WZ(0 I)T , W ˆW −1(0 I)T )
−μ( ˆWZ(0 I)T , WZ(0 I)T ) + μ( ˆZ−1Z(0 I)T , ˆZ−1 ˆW −1WZ(0 I)T ).
Substituting the last formula into (3.63), we complete the proof of (3.61).
⊓⊔
Using Lemma 3.23 and the comparative index for a pair of symplectic matrices,
we can now approach the main result of this section, which generalizes Theorem 3.6
to the case of two different matrices W and ˆW.
Theorem 3.24 Let W, Z, ˆW, ˆZ be symplectic matrices. Then for the operator
L(Y, ˆY , W, ˆW) given by (3.56) with Y = Z(0 I)T and ˆY = ˆZ(0 I)T , we have
L(Y, ˆY , W, ˆW) = μ(⟨W⟩, ⟨ˆW ⟩) −μ

⟨ˆZ−1 ˆW −1WZ⟩, ⟨ˆZ−1Z⟩

,
(3.64)
and
μ(⟨W⟩, ⟨ˆW ⟩) −μ

⟨ˆZ−1 ˆW −1WZ⟩, ⟨ˆZ−1Z⟩

= μ

⟨ˆZ−1Z⟩, ⟨ˆZ−1 ˆW −1WZ⟩

−μ(⟨ˆW⟩, ⟨W⟩).
(3.65)

3.3
Comparative Index for Symplectic Matrices
179
Proof To prove (3.64) we show that this identity is equivalent to (3.60). Using
successively formulas (3.54) and (3.55), we obtain the following representation for
the comparative indices on the right-hand side of (3.60):
μ

W(0 I)T , ˆW(0 I)T 
= μ(⟨W⟩, ⟨ˆW ⟩) −μ(⟨ˆW −1W⟩, ⟨I⟩),
μ

ˆW −1WZ(0 I)T , Z(0 I)T 
= μ

⟨ˆW −1WZ⟩, ⟨Z⟩

−μ

⟨Z−1 ˆW −1WZ⟩, ⟨I⟩

= μ

ˆW −1WZ(0 I)T , ˆW −1W(0 I)T 
+ μ(⟨ˆW −1W⟩, ⟨I⟩) −μ

⟨Z−1 ˆW −1WZ⟩, ⟨I⟩

,
and
μ

ˆZ−1 ˆW −1WZ(0 I)T , ˆZ−1Z(0 I)T 
= μ

⟨ˆZ−1 ˆW −1WZ⟩, ⟨ˆZ−1Z⟩

−μ

⟨Z−1 ˆW −1WZ⟩, ⟨I⟩

.
Substituting the obtained equalities into (3.60) and canceling the same summands,
we get (3.64). To prove (3.65) it is sufﬁcient to observe that by Lemma 3.21(ii)
rank(⟨W⟩T J ⟨ˆW⟩) = rank(W −ˆW),
(3.66)
and similarly
rank⟨ˆZ−1 ˆW −1WZ⟩TJ ⟨ˆZ−1Z⟩= rank( ˆZ−1 ˆW −1WZ −ˆZ−1Z)
= rank ˆZ−1 ˆW −1(W −ˆW) Z
= rank(W −ˆW).
(3.67)
Consequently, using property (v) of Theorem 3.5, we obtain (3.65).
⊓⊔
3.3.2
General Case
Next we apply the properties of the comparative index established in Sects. 3.1.2–
3.1.4 to the comparative index of a pair of symplectic matrices. We also present
particular cases of the comparative index for matrices appearing in discrete linear
Hamiltonian systems and Sturm-Liouville difference equations.
Applying Theorem 3.2 to matrices (3.52), we obtain the following result.

180
3
Comparative Index Theory
Lemma 3.25 Let W and
ˆW be symplectic matrices. The comparative index
μ(⟨W⟩, ⟨ˆW ⟩) can be computed by the following formulas
μ1(⟨W⟩, ⟨ˆW ⟩) = rankM,
(3.68)
μ2(⟨W⟩, ⟨ˆW ⟩) = ind T

I
ˆAT
0 ˆBT


Q⟨ˆW⟩−Q⟨W⟩
 I 0
ˆA ˆB

T ,
(3.69)
M := (I −BB†) ( ˆA −A, ˆB),
T := I2n −M†M,
(3.70)
where the symmetric matrices Q⟨ˆW⟩and Q⟨W⟩satisfy
I AT
0 BT

Q⟨W⟩
I 0
A B

= [W],
[W] :=
CT A CT B
BT C BT D

and are determined by the formulas (analogously for Q⟨ˆW⟩)
Q⟨W⟩:=
I −AT
0
I
 + CTA
CT BB†
BB†C BB†DB†

+ C
,  I
0
−A I

,
(3.71)
C = CT ,
diag{I, BT } C diag{I, B} = 0.
(3.72)
Proof By a direct computation, one can verify that
⟨W⟩= diag
I 0
A I

,
I −AT
0
I
 
diag {I, B, CT , I}
I2n
W

.
(3.73)
The ﬁrst matrix on the right-hand side of (3.73) is a symplectic block diagonal
matrix; hence by (ii) of Theorem 3.5, we obtain
μ1(⟨W⟩, ⟨ˆW ⟩) = μ1(D⟨W⟩, D⟨ˆW ⟩) = rank M,
D := diag
 I
0
−A I

,
I AT
0 I
 
,
where M is given in (3.70). Hence, formula (3.68) is proved. Further, using (3.73),
it is not difﬁcult to obtain general solution of Riccati-type relation (3.8) according
to (1.168). A general solution of relation (3.8) for
Y = diag {I, B, CT , I}
I2n
W

=
⎛
⎜⎜⎝
I
0
0
B
CTA CT B
C
D
⎞
⎟⎟⎠

3.3
Comparative Index for Symplectic Matrices
181
has the form, in view of (1.168),
˜Q = diag {I, BB†}
CTA CT B
C
D

diag {I, B†} + C,
where the matrix C satisﬁes (3.72). Then the general solution of (3.8) for ⟨W⟩is of
the form (3.71). One particular solution of (3.8) for ⟨W⟩which coincides with the
matrix Gk given by (2.64) is
Q⟨W⟩=
I −AT
0
I
 CTA
CT
C
BB†DB†
  I
0
−A I

=
AT BB†DB†A −AT C CT −AT BB†DB†
C −(AT BB†DB†)T
BB†DB†

,
(3.74)
which we obtain from (3.71) with
C :=

0
CT (I −BB†)
(I −BB†) C
0

.
This proves the statement of this lemma.
⊓⊔
Corollary 3.26 Let V be a symplectic matrix, and then for W := V and ˆW := I,
the formulas in Lemma 3.25 take the form
μ(⟨V ⟩, ⟨I⟩) = rank M + ind T [(D −I) B†(A −I) −C] T ,
(3.75)
V =
A B
C D

,
M := (I −BB†) (A −I),
T := I −M†M.
Proof Observe that by (3.53), we have the estimate
μ(⟨V ⟩, ⟨I⟩) ≤n,
because ˆB = 0. The formulas of Lemma 3.25 for this case reduce to determining
the rank and the index of n × n matrices. The solution Q ˆW can be taken to be equal
to zero. Using QW in the form (3.71) with C = 0 leads to the formula
μ2(⟨V ⟩, ⟨I⟩) = ind
+
−T (I, I −AT )
 CTA
CT BB†
BB†C BB†DB†
 
I
I −A

T
,
,
from which we derive the second addend in (3.75) because of
M T = 0
⇔
BB†(A −I) T = (A −I) T
and (1.145). The proof is complete.
⊓⊔

182
3
Comparative Index Theory
From the previous computations and Lemma 3.21(v), we obtain another way of
determining the comparative index μ(⟨W⟩, ⟨ˆW ⟩).
Lemma 3.27 Let W and ˆW be symplectic matrices. We have the formula
μ

⟨W⟩, ⟨ˆW ⟩

= μ
B
D

,
 ˆB
ˆD

+ μ

⟨ˆW −1W⟩, ⟨I⟩

,
(3.76)
where μ(⟨ˆW −1W⟩, ⟨I⟩) is determined by (3.75) with V := ˆW −1W. Analogously,
μ

⟨W⟩, ⟨ˆW⟩

= μ∗
−BT
AT

,
−ˆBT
ˆAT

+ μ

⟨W ˆW −1⟩, ⟨I⟩

,
(3.77)
where μ(⟨W ˆW −1⟩, ⟨I⟩) is given by (3.75) with V := W ˆW −1.
In the particular cases, when the matrices ˆW −1W and W ˆW −1 are unit lower (or
unit upper) block triangular (see Sect. 1.6.1), the formulas in Corollary 3.26 and
Lemma 3.27 reduce as follows.
Corollary 3.28 Let W and ˆW be symplectic matrices. If ˆW −1W or W ˆW −1 is a unit
lower block-triangular matrix of the form
 I 0
Q I

,
Q = QT ,
(3.78)
then
μ(⟨W⟩, ⟨ˆW ⟩) = ind (−Q).
If ˆW −1W is unit upper block-triangular matrix of the form
I Q
0 I

,
Q = QT ,
(3.79)
then
μ(⟨W⟩, ⟨ˆW ⟩) = μ
B
D

,
 ˆB
ˆD

.
If the analogical assumption (3.79) holds for W ˆW −1, then
μ(⟨W⟩, ⟨ˆW⟩) = μ∗
−BT
AT

,
−ˆBT
ˆAT

.

3.3
Comparative Index for Symplectic Matrices
183
Proof The ﬁrst terms on the right-hand side of (3.76) and (3.77) in Lemma 3.27
equal to zero for the matrices of the form (3.78) in view of Remark 3.4(ii). For
the second terms in (3.76) and (3.77), we obtain according to (3.75) the ﬁrst
statement of the corollary. Concerning the second statement for the upper block-
triangular matrices, μ(⟨ˆW −1W⟩, ⟨I⟩) = 0 and μ(⟨W ˆW −1⟩, ⟨I⟩) = 0, by (3.75).
Consequently, computing μ(⟨W⟩, ⟨ˆW ⟩) by Lemma 3.27, we obtain the proof of the
second statement of this corollary.
⊓⊔
3.3.3
Invertible Block B
Suppose that the blocks B and ˆB in the matrices W =
 A B
C D

and ˆW =
 ˆA
ˆB
ˆC
ˆD

are
invertible. Since
I 0
A B

=
I 0
A I
 I 0
0 B

,
(3.80)
then the upper blocks of the matrices ⟨W⟩and ⟨ˆW⟩are invertible as well. Then
obviously μ1(⟨W⟩, ⟨ˆW ⟩) = 0, and by Remark 3.4(i), the comparative index is given
by μ(⟨W⟩, ⟨ˆW ⟩) = μ2(⟨W⟩, ⟨ˆW ⟩). Consequently,
μ(⟨W⟩, ⟨ˆW ⟩) = ind
-0 −I
ˆC
ˆD
I 0
ˆA ˆB
−1
−
0 −I
C D
I 0
A B
−1.
= ind

ˆB−1 ˆA −B−1A
B−1 −ˆB−1
(B−1 −ˆB−1)T
ˆD ˆB−1 −DB−1

.
(3.81)
Observe that that the above formula can also be obtained from Lemma 3.25. Now
we consider simple examples of computations of the comparative index for a pair of
symplectic matrices
Example 3.29 Consider a pair of Sturm-Liouville difference equations (compare
with Sect. 1.2)


r[1]
k xk

−r[0]
k xk+1 = 0,


ˆr[1]
k xk

−ˆr[0]
k xk+1 = 0.
According to Example 2.13, these equations can be written as 2 × 2 symplectic
systems with the matrices Sk and ˆSk of the form
Sk =

1
1/r[1]
k
r[0]
k
1 + r[0]
k /r[1]
k

,
ˆSk =

1
1/ˆr[1]
k
ˆr[0]
k
1 + ˆr[0]
k /ˆr[1]
k

.

184
3
Comparative Index Theory
Computing μ(⟨Sk⟩, ⟨ˆSk⟩) according to (3.81), we obtain
μ(⟨Sk⟩, ⟨ˆSk⟩) = ind ( ˆUk ˆX −1
k
−UkX −1
k
)
= ind
 1 0
−1 1
 
ˆr[1]
k
−r[1]
k
0
0
ˆr[0]
k
−r[0]
k
 1 −1
0 1

= ind (ˆr[1]
k
−r[1]
k ) + ind (ˆr[0]
k
−r[0]
k ).
In particular, μ(⟨Sk⟩, ⟨ˆSk⟩) = 0 if and only if ˆr[i]
k
≥r[i]
k
for i ∈{0, 1}.
Next we present a generalization of the previous example to the comparative
index for a pair of matrices of the symplectic system corresponding to the vector
Sturm-Liouville equation (2.25), i.e.,
(Rkxk) −Pkxk+1 = 0.
Example 3.30 Consider a pair of equations (2.25) with the corresponding symplec-
tic matrices (see Sect. 2.1)
Sk =
 I
R−1
k
Pk I + PkR−1
k

,
ˆSk =

I
ˆR−1
k
ˆPk I + ˆPk ˆR−1
k

.
The invertibility of the blocks Bk := R−1
k
and ˆBk :=
ˆR−1
k
enables to compute
μ(⟨Wk⟩, ⟨ˆWk⟩) by (3.81)
μ(⟨Wk⟩, ⟨ˆWk⟩) = ind
 I
0
−I I
 
ˆRk −Rk
0
0
ˆPk −Pk
 I −I
0 I

= ind ( ˆRk −Rk) + ind ( ˆPk −Pk).
In particular, μ(⟨Sk⟩, ⟨ˆSk⟩) = 0 if and only if ˆRk ≥Rk and ˆPk ≥Pk.
3.3.4
Invertible Block A
In this subsection we consider the situation when the matrices W =
 A B
C D

and
ˆW =
 ˆA
ˆB
ˆC
ˆD

have A and ˆA invertible. In this case we can recover a Hamiltonian
structure from the symplectic matrices W and ˆW; see Example 2.7 in Sect. 2.1.2.

3.3
Comparative Index for Symplectic Matrices
185
Lemma 3.31 Suppose that symplectic matrices W and ˆW satisfy
det A ̸= 0,
det ˆA ̸= 0.
(3.82)
Then
μ(⟨W⟩, ⟨ˆW ⟩) = μ

⟨W⟩H , ⟨ˆW⟩H

,
(3.83)
⟨W⟩H :=
⎛
⎜⎜⎝
I
0
I −A−1
A−1B
CA−1
−(I −AT −1)
0
I
⎞
⎟⎟⎠,
(3.84)
where
⟨W⟩T
H J ⟨ˆW⟩H = H −ˆH,
H :=
 −CA−1 I −AT −1
I −A−1
A−1B

(3.85)
and where ˆH is deﬁned similarly through ˆW.
Proof Assumption (3.82) implies the block LU-factorization of W and ˆW (compare
with Corollary 1.73)
W =

I
0
CA−1 I

diag
(
A,
AT −1) I A−1B
0
I

(3.86)
and an analogical factorization holds for ˆW. Using the factorization (3.86), proper-
ties (i) and (ii) in Theorem 3.5, we obtain
μ(⟨W⟩, ⟨ˆW ⟩) = μ

L⟨W⟩C1, L⟨ˆW⟩C2

= μ

⟨W⟩H , ⟨ˆW⟩H

,
where
L := diag
 0 I
−I I

,
 I
I
−I 0
 
,
C1 =
A−1 −A−1B
0
I

, C2 =
 ˆA−1 −ˆA−1 ˆB
0
I

.
Hence, formula (3.83) is proved. Identity (3.85) follows by direct computations.
⊓⊔
Now we present how to compute the comparative index μ

⟨W⟩H , ⟨ˆW⟩H

.

186
3
Comparative Index Theory
Lemma 3.32 Let W and ˆW be symplectic matrices satisfying (3.82). Then
μ

⟨W⟩H , ⟨ˆW⟩H

= ind (H −ˆH) + ind ( ˆA−1 ˆB) −ind (A−1B),
(3.87)
where H = H T and ˆH = ˆH T are given by (3.85).
Proof Observe that ⟨W⟩H = NT  I2n
−H

holds, where
N :=
⎛
⎜⎜⎝
I
0 0 0
0 0 0 I
0 0 I 0
0 −I 0 0
⎞
⎟⎟⎠.
Applying (3.17) to the matrices Z(0 I)T =
 I2n
−H

, ˆZ(0 I)T =
 I2n
−ˆH

, and W = N,
we obtain
μ

⟨W⟩H , ⟨ˆW⟩H

= μ
 I2n
−H

,
 I2n
−ˆH

+ μ
 I2n
−ˆH

, NT (02n I2n)T

−μ
 I2n
−H

, NT (02n I2n)T

.
Computing the comparative indices on the right-hand side of the above formula, we
obtain (3.87).
⊓⊔
Another method of computing μ

⟨W⟩H , ⟨ˆW⟩H

is based on Theorem 3.2 and
Lemma 3.25.
Lemma 3.33 Let W and ˆW be symplectic matrices satisfying (3.82). Then the
comparative index (3.83) can be computed in the following way
μ1

⟨W⟩H , ⟨ˆW⟩H

= rank M,
M := [I −(A−1B)(A−1B)†]

A−1−ˆA−1,
ˆA−1 ˆB

,
μ2

⟨W⟩H , ⟨ˆW⟩H

= ind
+
T

I I −ˆAT −1
0
ˆA−1 ˆB
 
ˆG −G
 
I
0
I −ˆA−1 ˆA−1 ˆB

T
,
,
where T := I −M†M. The matrix G is a symmetric solution of the matrix equation
I I −A−T
0
A−1B

G

I
0
I −A−1 A−1B

=
CA−1
0
0
A−1B

,
(3.88)

3.3
Comparative Index for Symplectic Matrices
187
and it is determined by the formula
G =
I −I + AT −1
0
I
 +CA−1
0
0
(A−1B)†

+ C
, 
I
0
−I + A−1 I

,
(3.89)
C = CT ,
diag{I, A−1B} C diag{I, A−1B} = 0.
Similar formulas to (3.88) and (3.89) hold also for the matrix ˆG.
Proof We use the representation
⟨W⟩H = diag

I
0
I −A−1 I

,
I −(I −AT −1)
0
I
 
⎛
⎜⎜⎝
I
0
0
A−1B
CA−1
0
0
I
⎞
⎟⎟⎠
and follow the idea of the proof of Lemma 3.25.
⊓⊔
Example 3.34 We analyze the comparative index for matrices associated with
discrete linear Hamiltonian systems. Consider the system (2.15), i.e.,

xk
uk

= J Hk
xk+1
uk

,
Hk =
−Ck AT
k
Ak
Bk

,
J =
 0 I
−I 0

and

xk
uk

= J ˆHk
xk+1
uk

,
ˆHk =
−ˆCk ˆAT
k
ˆAk
ˆBk

.
(3.90)
Using (3.87), it is easy to see that
μ
/
S[H]
k
0
H ,
/ ˆS[H]
k
0
H

= ind (Hk −ˆHk) + ind ˆBk −ind Bk,
(3.91)
where S[H]
k
is the symplectic matrix given in (2.18), with ˆS[H]
k
being deﬁned
analogously. From (3.47) it follows that
ind Bk + μ
Bk
I

,
 ˆBk
I

= ind ˆBk + μ∗

J
 ˆBk
I

, J
Bk
I

,
or in other terms,
ind ˆBk −ind Bk = μ
Bk
I

,
 ˆBk
I

−ind (Bk −ˆBk).
(3.92)

188
3
Comparative Index Theory
Substituting the last formula into (3.91), we obtain
μ
/S[H]
k
0
H, / ˆS[H]
k
0
H

= μ
Bk
I

,
 ˆBk
I

+
(
ind (Hk −ˆHk) −ind (Bk −ˆBk)
)
.
(3.93)
Observe that by Lemma 3.15, the expression in braces in the last computation is
nonnegative and represents the index of a symmetric operator with the zero block
on the main diagonal. Simplifying formula (3.77) for the “Hamiltonian” case, it is
possible to show that the expression in braces is equal to the comparative index
μ
1
S[H]
k
( ˆS[H]
k
)−12
, ⟨I⟩

.
Analogously, the formulas in Lemma 3.33 for the pair of systems (2.15) and
(3.90) have the form
μ1
/
S[H]
k
0
H,
/ ˆS[H]
k
0
H

= rank Mk,
(3.94)
μ2
/S[H]
k
0
H, / ˆS[H]
k
0
H

= ind
+
Tk
I ˆAT
k
0 ˆBk
 
Gk −ˆGk
  I
0
ˆAk ˆBk

Tk
,
,
(3.95)
where
Mk := (I −BkB†
k ) ( ˆAk −Ak, ˆBk),
Tk := I −M†
kMk,
and where Gk is a symmetric solution of the equation
I AT
k
0 Bk

Gk
 I
0
Ak Bk

=
Ck 0
0 Bk

,
(3.96)
i.e., Gk is given by the formula
Gk =
I −AT
k
0
I
 +Ck 0
0 B†
k

+ Ck
,  I
0
−Ak I

,
(3.97)
Ck = CT
k ,
diag{I, Bk} Ck diag{I, Bk} = 0.
Similar formulas to (3.96) and (3.97) hold also for the matrix ˆGk.
Remark 3.35 Note that formula (3.92) generalizes Remark 1.60(vi) to the case
when A = AT and B = BT are indeﬁnite by sign. Indeed, under the assumptions
A ≥0 and B ≥0, we have from (3.92) that
μ
B
I

,
A
I

= ind (B −A).
(3.98)

3.3
Comparative Index for Symplectic Matrices
189
Then ind (B −A) = 0 is equivalent to μ
B
I
, A
I

= 0, and the last condition
coincides with
Im A ⊆Im B,
A −AB†A ≥0,
by Theorem 3.2(iv). By a similar way, the condition B† ≤A† is equivalent to
μ
A†
I

,
B†
I

= 0 or to
Im B† ⊆Im A†,
B† −B†AB† ≥0.
Using these equivalences it is possible to prove easily the last assertion in
Remark 1.60(vi).
Example 3.36 In this example we turn our attention to the 2n-th order Sturm-
Liouville difference equation (2.27), i.e.,
n

ν=0
(−1)νν
r[ν]
k νyk+n−ν

= 0,
where r[n]
k
̸= 0 for all k. This equation can be written as the Hamiltonian system
(2.15) with the coefﬁcient matrices Ak, Bk, and Ck given in (2.29) and (2.30).
Together with (2.27) we consider the equation of the same form with the coefﬁcients
ˆr[j]
k
for j = 0, . . . , n, i.e.,
n

ν=0
(−1)νν
ˆr[ν]
k νyk+n−ν

= 0
(3.99)
with ˆr[n]
k
̸= 0 for all k. In this particular case, Ak =
ˆAk and Mk = 0, so that
μ1

⟨S[H]
k
⟩H , ⟨ˆS[H]
k
⟩H

= 0. Simplifying equation (3.95), we obtain
μ

⟨S[H]
k
⟩H , ⟨ˆS[H]
k
⟩H

= ind diag
(
ˆr[0]
k
−r[0]
k , . . . , ˆr[n]
k
−r[n]
k
)
=
n

ν=0
ind (ˆr[ν]
k
−r[ν]
k ).
(3.100)
The same result follows directly from (3.93) and (2.29), (2.30). In particular,
μ

⟨S[H]
k
⟩H, ⟨ˆS[H]
k
⟩H

= 0 if and only if ˆr[j]
k
≥r[j]
k
for all j = 0, . . . , n.

190
3
Comparative Index Theory
3.3.5
Additional Properties of Comparative Index
In this section we derive additional properties of the comparative index, which we
will use in subsequent chapters when dealing with boundary value problems for
symplectic system with general (nonseparated) boundary conditions.
We introduce the following notation. Let WandV be matrices separated into
n × n blocks
W =
A B
C D

,
V =

ˆA ˆB
ˆC ˆD

.
Introduce the notation
{W, V } :=
⎛
⎜⎜⎝
A 0 −B 0
0
ˆA 0
ˆB
−C 0 D
0
0
ˆC
0
ˆD
⎞
⎟⎟⎠,
{W} := {I, W} =
⎛
⎜⎜⎝
I 0 0 0
0 A 0 B
0 0 I 0
0 C 0 D
⎞
⎟⎟⎠,
(3.101)
and
S0 :=
⎛
⎜⎜⎝
0 0 I
0
0 I I
0
−I 0 0 −I
0 0 0 I
⎞
⎟⎟⎠.
(3.102)
Then one can directly verify that S0 ∈Sp(4n) and under the additional assumption
W, V ∈Sp(2n), the 4n × 4n matrices {W, V } and {W} are symplectic as well, i.e.,
{W, V }, {W} ∈Sp(4n).
We have the multiplicative property
{WV, ˆW ˆV } = {W, ˆW} {V, ˆV },
(3.103)
in particular, {W, ˆW}−1 = {W −1, ˆW −1}. We also derive the following relations
{W, ˆW} S0 =
⎛
⎜⎜⎝
B
0 A
B
0
ˆA
ˆA
ˆB
−D 0 −C −D
0
ˆC
ˆC
ˆD
⎞
⎟⎟⎠,
{W} S0 (02n, I2n)T = ⟨W⟩,
(3.104)
which can be easily veriﬁed by direct computations. Moreover, by (3.104) for any
symplectic 2n × 2n matrices P and R, we have
⟨R−1WP⟩= {P −1, R−1} ⟨W⟩P,
(3.105)

3.3
Comparative Index for Symplectic Matrices
191
where the matrix ⟨W⟩is deﬁned in (3.52). Using the properties of the comparative
index, we can prove the following result.
Proposition 3.37 Let W, V, R, P ∈Sp(2n) and S0 be given by (3.102). Then,
using the notation in (3.101) and (3.52), we have
μ

⟨W⟩, {W} (02n I2n)T 
= 0,
(3.106)
μ

⟨W⟩, {V } (02n I2n)T 
= μ

{W} (02n I2n)T , {V } (02n I2n)T 
(3.107)
= μ

W(0 I)T, V (0 I)T 
,
(3.108)
μ (⟨{W}⟩, ⟨{V }⟩) = μ (⟨W⟩, ⟨V ⟩) .
(3.109)
Proof Using (3.104), properties (i), (iii) of Theorem 3.5 and (3.13), we obtain
μ

⟨W⟩, {W} (02n I2n)T 
= μ

{W} S0 (02n I2n)T , {W} (02n I2n)T 
= μ∗
({W}S0)−1(02n I2n)T , S−1
0 (02n I2n)T 
= μ
⎛
⎜⎜⎝
⎛
⎜⎜⎝
I AT
0 BT
0 0
0 AT
⎞
⎟⎟⎠,
⎛
⎜⎜⎝
I I
0 0
0 0
0 I
⎞
⎟⎟⎠
⎞
⎟⎟⎠
= μ
⎛
⎜⎜⎝
⎛
⎜⎜⎝
I
0
0 BT
0 0
0 AT
⎞
⎟⎟⎠
I AT
0 I

,
⎛
⎜⎜⎝
I 0
0 0
0 0
0 I
⎞
⎟⎟⎠
I I
0 I

⎞
⎟⎟⎠= μ
⎛
⎜⎜⎝
⎛
⎜⎜⎝
I
0
0 BT
0 0
0 AT
⎞
⎟⎟⎠,
⎛
⎜⎜⎝
I 0
0 0
0 0
0 I
⎞
⎟⎟⎠
⎞
⎟⎟⎠.
The last comparative index can be easily computed as being equal to zero. Thus,
we have proved (3.106). Now we proceed with the proof of formula (3.107). By
Theorem 3.6,
μ

⟨W⟩, {V } (02n I2n)T 
−μ

{W} (02n I2n)T , {V } (02n I2n)T 
= μ

⟨W⟩, {W} (02n I2n)T 
−μ

⟨V −1W⟩, {V −1W} (02n I2n)T 
.
From (3.106) it follows that the right-hand side of the last formula equals to zero,
and then (3.107) follows. Since the 2n × 2n blocks of the matrices {W} (02n I2n)T
and {V } (02n I2n)T have a block diagonal structure, then by the deﬁnition of the
comparative index
μ

{W} (02n I2n)T , {V } (02n I2n)T 
= μ

W(0 I)T, V (0 I)T 
,

192
3
Comparative Index Theory
which proves formula (3.108). Finally, formula (3.109) can be veriﬁed by direct
computations by using the block diagonal structure of the 2n × 2n blocks of the
matrices {W} and {V } in (3.101).
⊓⊔
Remark 3.38 Note that we have the identity
diag{P1, P1}{W, I} diag{P1, P1} = {I, P2WP2} := {P2WP2},
(3.110)
where P1 and P2 are given by (1.151), and the block diagonal matrix diag{P1, P1}
is symplectic. Using (3.110) and Theorem 3.5(i), (ii), one can prove that the
comparative indices associated with {W, I} have properties similar to (3.106),
(3.107), (3.108). In particular, we obtain
μ

{W, I}⟨I⟩, {W, I} (02n I2n)T 
= 0,
(3.111)
μ

{W, I}⟨I⟩, {V, I} (02n I2n)T 
= μ

{W, I} (02n I2n)T , {V, I} (02n I2n)T 
= μ

P2WP2(0 I)T, P2V P2(0 I)T 
= μ∗
W(0 I)T, V (0 I)T 
.
(3.112)
The statements in Proposition 3.37 and Remark 3.38 are used in the proof of the
following result, which will be further applied in the subsequent chapters.
Lemma 3.39 Let W, V, R, P be symplectic matrices and deﬁne ˜W := R−1WP
and ˜V := R−1V P. . Then
μ(⟨˜W⟩, ⟨˜V ⟩) −μ(⟨W⟩, ⟨V ⟩)
= μ

⟨˜W⟩, {P −1, R−1} (02n I2n)T 
−μ

⟨˜V ⟩, {P −1, R−1} (02n I2n)T 
(3.113)
= μ

⟨V ⟩, {P, R} (02n I2n)T 
−μ

⟨W⟩, {P, R} (02n I2n)T 
.
(3.114)
Moreover, each term in (3.114) can be computed by using the formulas
μ

⟨W⟩, {P, R} (02n I2n)T 
= μ

W (0 I)T, WP (0 I)T 
+ μ

WP (0 I)T, R (0 I)T 
(3.115)
= μ∗
W −1R (0 I)T, P (0 I)T 
+ μ

W (0 I)T, R (0 I)T 
.
(3.116)

3.3
Comparative Index for Symplectic Matrices
193
In particular, we have
μ(⟨˜W⟩, ⟨˜V ⟩) −μ(⟨W⟩, ⟨V ⟩)
= μ

˜W (0 I)T, R−1 (0 I)T 
−μ

˜V (0 I)T, R−1 (0 I)T 
+ μ∗
V −1 (0 I)T, P (0 I)T 
−μ∗
W −1 (0 I)T, P (0 I)T 
,
(3.117)
or
μ(⟨˜W⟩, ⟨˜V ⟩) −μ(⟨W⟩, ⟨V ⟩)
= μ

R−1 (0 I)T, ˜V (0 I)T 
−μ

R−1 (0 I)T, ˜W (0 I)T 
+ μ∗
P (0 I)T, W −1 (0 I)T 
−μ∗
P (0 I)T, V −1 (0 I)T 
.
(3.118)
The proof of this lemma is postponed to Sect. 3.3.6.
Based on the results in Lemma 3.39, we derive the next corollary, which will be
used in the subsequent chapters.
Corollary 3.40 We have the following statements.
(i) Let R, P ∈Sp(2n) be lower block-triangular. Then
μ

⟨R−1WP⟩, ⟨R−1V P⟩

= μ (⟨W⟩, ⟨V ⟩) .
(ii) Let R = I and P = J . Then
μ (⟨W⟩, ⟨V ⟩) = μ∗(⟨V J ⟩, ⟨WJ ⟩) + μ∗
(I 0)T , V −1 (0 I)T 
−μ∗
(I 0)T , W −1 (0 I)T 
.
(iii) Let ˜V := R−1V P = I. Then
μ

⟨˜W⟩, ⟨I⟩

−μ

⟨W⟩, ⟨RP −1⟩

= μ

˜W(0 I)T, R−1(0 I)T 
−μ∗
W −1(0 I)T, P(0 I)T 
−μ

P −1(0 I)T, R−1(0 I)T 
(3.119)

194
3
Comparative Index Theory
= μ∗
P(0 I)T, W −1(0 I)T 
−μ

R−1(0 I)T, ˜W(0 I)T 
−μ

P −1(0 I)T, R−1(0 I)T 
.
(3.120)
(iv) If W =  I
0
−Q I
, where Q = QT and Q ≥0, then for arbitrary R ∈Sp(2n),
we have
μ

⟨R−1WR⟩, ⟨I⟩

= 0.
(3.121)
(v) For any R ∈Sp(4n) we have
μ

⟨R−1 {W} S0⟩, ⟨R−1 {V } S0⟩

= μ

R−1 ⟨W⟩, R−1 ⟨V ⟩

.
(3.122)
Proof Particular cases (i) and (iii), formula (3.120) follows immediately from
(3.117) and (3.118), respectively, where by Theorem 3.5(iii), we have
μ∗(P(0 I)T , PR−1(0 I)T ) = μ(P −1(0 I)T , R−1(0 I)T ).
(3.123)
For the proof of formula (3.119) in (iii), we apply (3.117), where
μ

(0 I)T, R−1 (0 I)T 
−μ∗
P R−1 (0 I)T, P (0 I)T 
= μ∗
P (0 I)T , P R−1 (0 I)T 
according to Theorem 3.5(v). Then, using (3.123), we derive (3.119).
We turn our attention to the proof of (ii). From (3.118) we obtain
μ (⟨WJ ⟩, ⟨V J ⟩) −μ (⟨W⟩, ⟨V ⟩)
= μ

(0 I)T, V J (0 I)T 
−μ

(0 I)T, WJ (0 I)T 
+ μ∗
(I 0)T , W −1(0 I)T 
−μ∗
(I 0)T , V −1(0 I)T 
.
Putting the ﬁrst two summands on the right-hand side of the last computation to the
left and then using Lemma 3.21(iv), we obtain the proof of (ii).
Property (iv) follows from (3.120) with R = P, since on the right-hand side
of (3.120) we have in our case μ∗
P(0 I)T, W −1(0 I)T 
= 0, as W −1 is lower
block-triangular, and
μ

R−1(0 I)T, ˜W(0 I)T 
= μ∗(R (0 I)T, WR (0 I)T ) = ind RT
12QR12 = 0,

3.3
Comparative Index for Symplectic Matrices
195
where R12 is the right upper block of R, and μ

R−1(0 I)T, R−1(0 I)T 
= 0.
Further, μ(⟨W⟩, ⟨RR−1⟩) = μ(⟨W⟩, ⟨I⟩) = ind Q = 0 by Corollary 3.28. The
property (iv) is proved.
Finally, we prove (v). By (3.114) with P := S0, we have
μ

⟨R−1{W}S0⟩, ⟨R−1{V }S0⟩

= μ (⟨{W}⟩, ⟨{V }⟩) + μ

⟨{V }⟩, {S0, R} (02n I2n)T 
−μ

⟨{W}⟩, {S0, R} (02n I2n)T 
= μ (⟨W⟩, ⟨V ⟩) + μ

⟨{V }⟩, {S0, R} (02n I2n)T 
−μ

⟨{W}⟩, {S0, R} (02n I2n)T 
,
where we have used (3.109). In addition, applying formula (3.115), we obtain
μ

⟨{V }⟩, {S0, R} (02n I2n)T 
= μ

{V } (02n I2n)T , {V }S0 (02n I2n)T 
+ μ

{V } S0 (02n I2n)T , R (02n I2n)T 
.
Using the notation ⟨V ⟩= {V } S0 (02n I2n)T , we obtain
μ

{V } (02n I2n)T , ⟨V ⟩

= n −μ

⟨V ⟩, {V } (02n I2n)T 
= n,
where we have used property (v) of Theorem 3.5 and (3.106). Applying similar
computations for μ

⟨{W}⟩, {S0, R} (02n I2n)T 
, we obtain
μ

⟨{V }⟩, {S0, R} (02n I2n)T 
−μ

⟨{W}⟩, {S0, R} (02n I2n)T 
= μ

⟨V ⟩, R (02n I2n)T 
−μ

⟨W⟩, R (02n I2n)T 
,
and, hence,
μ

⟨R−1{W} S0⟩, ⟨R−1{V } S0⟩

= μ (⟨W⟩, ⟨V ⟩) + μ

⟨V ⟩, R (02n I2n)T 
−μ

⟨W⟩, R (02n I2n)T 
= μ

R−1⟨W⟩, R−1⟨V ⟩

,
where we have used Theorem 3.6. This proves (3.122).
⊓⊔

196
3
Comparative Index Theory
Remark 3.41 Note that for the comparative indices associated with {W, ˆW} and
{W, ˆW}S0 and their special cases with W := I or ˆW := I, the duality principle
holds (see Sect. 3.1.4). Introduce the matrix P2 := diag{I2n, −I2n}, which is an
analog of P2 = diag{I, −I} deﬁned by (1.151). According to (3.13), the dual
comparative index can be deﬁned using P2. Similarly, we have
μ∗(⟨W⟩, ⟨ˆW ⟩) = μ(P2⟨W⟩, P2⟨ˆW⟩)
= μ(P2⟨W⟩P2, P2⟨ˆW⟩P2)
= μ(⟨P2WP2⟩, ⟨P2 ˆWP2⟩),
where we have applied Theorem 3.5(i). By a similar way, using the relations
P2{W, ˆW}P2 = {P2WP2, P2 ˆWP2},
P2S0 diag{−P2, P2} = S0,
Theorem 3.5(i), and representation (3.13) for the dual comparative index associated
with P2 ∈R4n, one can prove that all results of Sect. 3.3 hold for the dual indices
as well.
3.3.6
Comparative Index for Symplectic Matrices: Proofs
In this section we present the proofs of properties of the comparative index, which
we displayed earlier in Sect. 3.3. In particular, we present the proofs of properties
(v) and (vi) of Lemma 3.21 and the proof of Lemma 3.39.
Proof of Properties (v) and (vi) of Lemma 3.21 The proof of the property (v) in
Lemma 3.21 is based on Proposition 3.37; see formulas (3.106) and (3.108).
Applying identity (3.60) from Lemma 3.23 to the case W := {W}, ˆW := { ˆW},
Z = ˆZ = S0 with S0 given by (3.102), we have Y = ˆY := ⟨I⟩, and using (3.106)
for the matrices W, ˆW, ˆW −1W, we obtain
L(⟨I⟩, ⟨I⟩, {W}, { ˆW }) = μ

{W}⟨I⟩, { ˆW} ⟨I⟩

= μ

{W} (0 I)T, { ˆW} (0 I)T 
+ μ

{ ˆW −1W}⟨I⟩, ⟨I⟩

= μ

W (0 I)T, ˆW (0 I)T 
+ μ

⟨ˆW −1W⟩, ⟨I⟩

,
(3.124)
where we also incorporate the last equality in (3.108). Hence, equality (3.54) is
proved. Using property (iii) of Lemma 3.21 and repeating the proof of (3.124) for

3.3
Comparative Index for Symplectic Matrices
197
μ∗(⟨W −1⟩, ⟨ˆW −1⟩), we obtain
μ

⟨W⟩, ⟨ˆW ⟩

= μ∗
⟨W −1⟩, ⟨ˆW −1⟩

= μ∗
W −1 (0 I)T, ˆW −1 (0 I)T 
+ μ∗
⟨ˆWW −1⟩, ⟨I⟩

,
i.e., equality (3.55) holds as well. Using part (iii) of Lemma 3.21 again, we get
μ∗(⟨ˆWW −1⟩, ⟨I⟩) = μ(⟨W ˆW −1⟩, ⟨I⟩). Finally, part (iii) of Theorem 3.5 yields
the identity μ∗(W −1 (0 I)T, ˆW −1 (0 I)T ) = μ(W (0 I)T, W ˆW −1 (0 I)T ), which
ﬁnishes the proof of (3.55). The proof of property (vi) follows directly from (3.54)
and (3.55).
⊓⊔
Proof of Lemma 3.39 Remark that by (3.105) and part (i) of Theorem 3.5, the
invertible matrices P in the comparative index
μ(⟨˜W⟩, ⟨˜V ⟩) = μ

{P −1, R−1} ⟨W⟩P, {P −1, R−1} ⟨V ⟩P

can be neglected. Now, using Theorem 3.6, we obtain the proof of (3.113). Formula
(3.114) follows from (3.17) or from property (ix) of Theorem 3.5.
Now we prove (3.116). By (i) of Theorem 3.5, we have
μ

⟨W⟩, {P, R} (02n I2n)T 
= μ

⟨W⟩P, {P, R} (02n I2n)T 
,
and further, using (3.105) for the case R := I and (3.103), we derive
μ

⟨W⟩P, {P, R} (02n I2n)T 
= μ

{P, I} ⟨WP⟩, {P, R} (02n I2n)T 
= μ

{P, I} ⟨WP⟩, {P, I} {I, R} (02n I2n)T 
= μ

⟨WP⟩, {I, R} (02n I2n)T 
+ μ

{I, R} (02n I2n)T, {P −1, I} (02n I2n)T 
−μ

⟨WP⟩, {P −1, I} (02n I2n)T 
,
where the last sum is obtained via (3.17). By (3.108),
μ

⟨WP⟩, {I, R} (0 I)T 
= μ

⟨WP⟩, {R} (0 I)T 
= μ

WP (0 I)T, R (0 I)T 
,
which yields the second summand on the right-hand side of (3.115). Further, by
(3.101) and Deﬁnition 3.1 of the comparative index, we have
μ

{I, R} (02n I2n)T, {P −1, I} (02n I2n)T 
= μ

(0 I)T, P −1 (0 I)T 
.

198
3
Comparative Index Theory
To determine the last comparative index, we use again Theorem 3.5(i) and (3.105).
We obtain
μ

⟨WP⟩, {P −1, I} (02n I2n)T 
= μ

⟨WP⟩(WP)−1, {P −1, I} (02n I2n)T 
= μ

{P −1W −1, I} ⟨I⟩, {P −1, I} (02n I2n)T 
.
Using Remark 3.38 we have by (3.112) that
μ

{P −1W −1, I} ⟨I⟩, {P −1, I}(02n I2n)T 
= μ∗
P −1W −1(0 I)T, P −1(0 I)T 
.
Finally, we derive
μ

(0 I)T, P −1(0 I)T 
−μ∗
P −1W −1(0 I)T, P −1(0 I)T 
= μ∗
W −1(0 I)T, P(0 I)T 
= μ

W (0 I)T, WP(0 I)T 
,
where we have used properties (ix) and (iii) of Theorem 3.5. This proves formula
(3.115). Applying Theorem 3.6 to just proved formula, we have
μ

W (0 I)T, WP (0 I)T 
+ μ

WP(0 I)T, R (0 I)T 
= μ

R−1W (0 I)T, R−1WP (0 I)T 
+ μ

W (0 I)T, R (0 I)T 
= μ∗
W −1R (0 I)T, P (0 I)T 
+ μ

W (0 I)T, R (0 I)T 
.
This proves formula (3.116).
The proof of (3.117) we get from (3.113) in the following way:
μ

⟨˜W⟩, ⟨˜V ⟩

−μ (⟨W⟩, ⟨V ⟩)
= μ

⟨˜W⟩, {P −1, R−1} (02n I2n)T 
−μ

⟨˜V ⟩, {P −1, R−1} (02n I2n)T 
,
where, on the right-hand side, we twice apply formula (3.116) obtaining
μ

⟨˜W⟩, {P −1, R−1} (02n I2n)T 
−μ

⟨˜V ⟩, {P −1, R−1} (02n I2n)T 
= μ

˜W (0 I)T, R−1 (0 I)T 
−μ

˜V (0 I)T, R−1 (0 I)T 
+ μ∗
P −1W −1 (0 I)T, P −1(0 I)T 
−μ∗
P −1V −1 (0 I)T, P −1 (0 I)T 
.

3.4
Notes and References
199
Applying property (ix) of Theorem 3.5 to the difference of last summands completes
the proof of (3.117).
To prove (3.118), we apply twice property (v) of Theorem 3.5 on the right-hand
side of the just proved formula (3.117) obtaining the identities
μ

˜W (0 I)T, R−1 (0 I)T 
= μ

(0 I)T, P −1W −1 (0 I)T 
−μ

R−1 (0 I)T, ˜W (0 I)T 
,
μ∗
W −1 (0 I)T, P(0 I)T 
= μ

(0 I)T, WP (0 I)T 
−μ∗
P (0 I)T, W −1 (0 I)T 
.
Consequently, we have
μ

˜W (0 I)T, R−1 (0 I)T 
−μ∗
W −1 (0 I)T, P (0 I)T 
= μ∗
P (0 I)T, W −1 (0 I)T 
−μ

R−1 (0 I)T, ˜W (0 I)T 
.
An analogous identity holds also for the matrix V . Substituting the obtained
differences into (3.117), we obtain the proof of (3.118). The proof of this lemma
is completed.
⊓⊔
3.4
Notes and References
The concept of a comparative index was introduced in [114]. The origin of this
notion is twofold. On one hand, μ(Y, ˆY) gives the possibility to compare matrices Y
and ˆY with (3.1) presenting a quantitative measure of violation of conditions (3.11).
On the other hand, according to the results of Sect. 3.2, the quantity μ(Y, ˆY ) is
equal to the index, i.e., the number of negative eigenvalues of a symmetric matrix
associated with Y and ˆY.
The results in Theorem 3.2 were proven in [114], while Theorem 3.6 and
the properties of the comparative index in the form presented in the book (see
Theorem 3.5) are from the paper [115]. The lower bounds for the comparative index
in Lemma 3.9 are improved versions of the lower bounds derived in [289].
The statement in Lemma 3.13 was proven in [113, Lemma 2.7] and then used
together with Lemma 3.14 in the proofs of the properties of the comparative index in
[115, Lemma 4.4]. The proof of Corollary 3.16 is presented in [205, Lemma 3.1.10].
Finally, let us note that generalizations of Lemmas 3.13 and 3.14 for blocks of
different dimensions can be found in [316, Theorem 2.3] and [317, Lemma 1.4].
In Sect. 3.2.2 we constructed a symmetric operator [V ] associated with V =
(Y, ˆY), where Y, ˆY satisfy (3.1). Computing the index of [V ], we prove the key

200
3
Comparative Index Theory
property (viii) of Theorem 3.5 which is basic for the proof of the main theorem on
the comparative index (see Theorem 3.6). For the special case w(Y, ˆY ) = Y T J ˆY =
I, the matrix V is symplectic. Note that one can construct other symmetric operators
associated with V ∈Sp(2n). For example, in [154, 155] the symmetric operator
J (V −V −1) and its generalizations are used for the classiﬁcation of eigenvalues of
symplectic matrices; other examples can be found in [163, 164], where symplectic
difference schemes are constructed using the symmetry of the Hamiltonian matrix
in (1.103). In [132] we offered an algorithm for the computation of focal points
based on Proposition 3.18.
The main results of Sect. 3.3 can be found in [117, 121], while Lemma 3.22(ii) is
proven in [124, Lemma 2.3]. We point out that 4n×2n matrices in the form of (3.52)
are well known and often used for different purposes in the discrete and continuous
Sturmian and spectral theory; see, for example, [39, 42, 54, 205, 248, 250].
Symmetric matrices (3.74) were originally derived in [44, 171, 172]. They were
used in comparison results for (SDS), e.g., in [56, 173, 258]. In the present work,
we derive (3.74) as a particular solution of matrix equation (3.8) associated with the
4n × 2n matrix ⟨S⟩. The result in Lemma 3.32 and formula (3.91) was proven for
the ﬁrst time in [128, Lemma 1]. Majority of the result in Sect. 3.3.5 can be found
in [121, Chapter 2].
Note also that the comparative index theory itself as based on the notion of
the Moore-Penrose generalized inversion and index results for block symmetric
matrices can be considered as a new perspective tool in the matrix linear algebra. In
this connection we mention the index results in Proposition 3.18, Lemma 3.22, and
Remark 3.35 and other results of this chapter which deserve future investigations.

Chapter 4
Oscillation Theory of Symplectic Systems
In this chapter we present the oscillation theory of symplectic difference system
(SDS). In particular, we show that it is possible to develop this theory in a similar
way to the oscillation theory of linear Hamiltonian systems or their special cases, the
Sturm-Liouville differential and difference equations. A crucial role in these results
plays the notion of a multiplicity of a focal point for conjoined bases of (SDS),
which allows to describe in quantitative way the behavior of the solutions. We deﬁne
this notion in Sect. 4.1 for the case of forward focal points in the interval (k, k + 1]
and backward focal points in the interval [k, k + 1) and connect these notions with
the comparative index from Chap. 3. The algebraic properties of the comparative
index, in particular Theorems 3.6 and 3.24, make it possible to present a completely
new approach to the oscillation theory of (SDS) by deriving classical separation
and comparison results in the form of explicit relations between the multiplicities
of focal points. In Sects. 4.2 and 4.3, we provide the Sturmian separation and
comparison theory for symplectic difference systems (SDS). In Sect. 4.4 we study
symplectic transformations and their effect on focal points on conjoined bases of
(SDS) by presenting explicit relations between their multiplicities.
4.1
Multiplicity of Focal Points
In this section we introduce the multiplicity of focal points of conjoined bases
of (SDS) as the main notion of the modern oscillation theory. We also present
results, which relate two basic concepts of the oscillation theory of discrete
symplectic systems, namely, the concepts of the multiplicity of a focal point and
the comparative index. Among others, we also establish basic relationship between
forward and backward focal points.
© Springer Nature Switzerland AG 2019
O. Došlý et al., Symplectic Difference Systems: Oscillation and Spectral Theory,
Pathways in Mathematics, https://doi.org/10.1007/978-3-030-19373-7_4
201

202
4
Oscillation Theory of Symplectic Systems
4.1.1
Deﬁnition and Main Properties
The notion of the nonexistence of a focal point for a conjoined basis of (SDS) solved
the problem of positivity of the associated quadratic functional (see Sect. 2.3).
But the problem of the Sturmian separation and comparison theory for symplectic
difference systems remained untouched. In the deﬁnition of the multiplicity of
a focal point in the discrete case, the problem was how to “measure” the violation
of the kernel condition and the P-condition in (2.37). In particular, the problem was
that if the kernel condition is violated, then the matrix P is no longer symmetric,
hence to count something like its negative eigenvalues or so had no meaning. This
problem was ﬁnally resolved in the fundamental paper of W. Kratz [208] in 2003,
where the multiplicity of a focal point of a conjoined basis of (SDS) was deﬁned as
follows.
Deﬁnition 4.1 Let Y =
X
U

be a conjoined basis of (SDS), and consider the
following matrices
Mk := (I −Xk+1X†
k+1) Bk,
Tk := I −M†
k Mk,
(4.1)
Pk := T T
k XkX†
k+1BkTk,
(4.2)
The multiplicity of a forward focal point (or a left focal point) of the conjoined basis
Y in the interval (k, k + 1] is deﬁned as the number
m(Yk) = m(k) := rank Mk + ind Pk.
(4.3)
Recall that ind denotes the index of the matrix indicated, i.e., the number of its
negative eigenvalues. This deﬁnition is correct, since the matrix P in (4.2) is really
symmetric (see Proposition 4.4 or Lemma 2.45(ii) with a slightly different notation
of Pk). In the next treatment, we will denote
m1(Yk) = m1(k) := rankMk,
m2(Yk) = m2(k) := ind Pk.
The quantity m1(k) characterizes “how much” the kernel condition is violated (it
is also called the multiplicity of a focal point at k + 1), and the quantity m2(k)
“measures” violation of P-condition; it is also called the multiplicity of a focal
point in the interval (k, k + 1); compare also with Remark 2.46.
Remark 4.2 There are equivalent deﬁnitions of the multiplicity of a focal point. For
example, the multiplicity of a focal point at k + 1 can be deﬁned as
m1(k) = rankMk,
Mk := (I −X†
k+1Xk+1) XT
k ,
and the multiplicity of a focal point in (k, k + 1) can be deﬁned as
m2(k) = ind Pk,
Pk := Tk (BT
k Dk −BT
k Qk+1Bk) Tk,
Tk := I −M†
kMk,

4.1
Multiplicity of Focal Points
203
where Q is a symmetric matrix satisfying XT
k QkXk = XT
k Uk. The equivalence
of the above given formula with those given in Deﬁnition 4.1 can be proved by
a direct computation; see [193]. However, we prefer to postpone the proof to the
next subsection, where we obtain the equivalence of the two deﬁnitions of the
multiplicity of a focal point from the equivalence of various deﬁnitions of the
comparative index and from a formula connecting the comparative index with the
multiplicity of a focal point.
The adjective “forward” focal point is used here to distinguish this concept
from the concept of a backward focal point which is given below. We will use
the convention that when no adjective by “focal point” is used, we actually mean
a forward focal point.
Consider the time-reversed symplectic system (2.47), i.e., the system
Yk = S−1
k Yk+1,
S−1 =
 DT −BT
−CT
AT

.
Recall that this system motivates Deﬁnition 2.22 of a backward focal point,
which can be generalized as follows.
Deﬁnition 4.3 Let Y =
X
U

be a conjoined basis of (SDS), and consider the
following matrices:
˜Mk := (I −XkX†
k) BT
k ,
˜Tk := I −˜M†
k ˜Mk,
(4.4)
˜Pk := ˜T T
k Xk+1X†
kBT
k ˜Tk.
(4.5)
The multiplicity of a backward focal point (or a right focal point) of the conjoined
basis Y =
X
U

in the interval [k, k + 1) is deﬁned as the number
m∗(Yk) = m∗(k) = rank ˜Mk + ind ˜Pk.
(4.6)
In the next treatment, we will denote
m∗
1(Yk) = m∗
1(k) := rank ˜Mk,
m∗
2(Yk) = m∗
2(k) := ind ˜Pk.
The quantity m∗
1(k) is called the multiplicity in the point k, while m∗
2(k) is called
the multiplicity in the interval (k, k + 1). The main properties of m(k) and m∗(k)
are the following.
Proposition 4.4 Let m(k) and m∗(k) be the multiplicities of forward and backward
focal points of a conjoined basis Y =
X
U

of (SDS) in (k, k + 1] and [k, k + 1),
respectively. Then we have the following properties :
(i) The matrices Pk and ˜Pk in (4.2) and (4.5) are always symmetric.
(ii) We have m1(k) = 0 if and only if the kernel condition in (2.37) holds, i.e.,
Ker Xk+1 ⊆Ker Xk.

204
4
Oscillation Theory of Symplectic Systems
Similarly, m1(k) = 0 if and only if the image condition
Im Bk ⊆Im Xk+1
holds. In this case Tk = I, Pk = XkX†
k+1Bk, and m(k) = m2(k) = ind Pk.
(iii) We have m∗
1(k) = 0 if and only if the reversed kernel condition in (2.48) holds,
i.e.,
Ker Xk ⊆Ker Xk+1.
Similarly, m∗
1(k) = 0 if and only if the image condition
Im BT
k ⊆Im Xk
holds. In this case ˜Tk = I, ˜Pk = Xk+1X†
kBT
k , and m∗(k) = m∗
2(k) = ind ˜Pk.
(iv) We have m(k) = 0 if and only if the conjoined basis Y has no forward focal
points in (k, k + 1], i.e., the conditions in (2.37) hold. Similarly, m∗(k) = 0 if
and only if Y has no backward focal points in [k, k + 1), i.e., the conditions in
(2.48) are satisﬁed.
(v) The following inequalities are satisﬁed:
0 ≤rk ≤m(k) ≤Rk ≤n,
rk + Rk = rank Bk − rankXk,
rk := max{0, − rankXk, rankBk −rank Xk+1},
Rk := min{rank Bk, rank Xk, rankBk − rankXk},
and similarly
0 ≤r∗
k ≤m∗(k) ≤R∗
k ≤n,
r∗
k + R∗
k = rankBk +  rankXk,
r∗
k := max{0,  rankXk, rank Bk −rankXk},
R∗
k := min{rankBk, rank Xk+1, rankBk +  rankXk}.
(vi) The multiplicities m(k) and m∗(k) are connected by the formulas
m∗
1(k) −m1(k) = rankXk+1 −rankXk,
(4.7)
m∗
2(k) = m2(k),
(4.8)
m∗(k) −m(k) = rankXk+1 −rankXk.
(4.9)
The proof of Proposition 4.4 follows from the comparative index properties and
will be postponed later in Sect. 4.1.2.
Remark 4.5 Recall that the second summands m2(k) and m∗
2(k) in Deﬁnitions 4.1
and 4.3 of m(k) and m∗(k) are called the multiplicity of a focal point in the open

4.1
Multiplicity of Focal Points
205
interval (k, k + 1) (or between k and k + 1). Formula (4.8) says that these quantities
are really the same justifying the terminology used in Deﬁnitions 4.1 and 4.3.
Deﬁne the number of (forward) focal points of a conjoined basis Y =
X
U

in the
interval (M, N + 1] by the formula
l(Y, M, N + 1) :=
N

k=M
m(k).
(4.10)
Analogously, the number of backward focal points in [M, N + 1) is deﬁned by
l∗(Y, M, N + 1) :=
N

k=M
m∗(k).
(4.11)
From Proposition 4.4(vi), it immediately follows
Corollary 4.6 We have for the conjoined basis Y =
X
U

l∗(Y, M, N + 1) −l(Y, M, N + 1) = rank XN+1 −rankXM,
(4.12)
in particular
		l∗(Y, M, N + 1) −l(Y, M, N + 1)
		 ≤max{rankXN+1, rank XM} ≤n.
(4.13)
4.1.2
Multiplicity of a Focal Point and Comparative Index
The ﬁrst statement of this subsection presents a basic formula relating the multiplic-
ity of a focal point in (k, k + 1] and the comparative index.
Lemma 4.7 Let Z be a symplectic fundamental matrix of (SDS), and let Y =
Z(0 I)T . Then
mi(k) = μi

Yk+1, Sk(0 I)T 
= μ∗
i

Z−1
k+1(0 I)T , Z−1
k (0 I)T 
,
i ∈{1, 2},
(4.14)
where m(k) = m1(k) + m2(k) is the number of focal points of Y in (k, k + 1],
μ = μ1 +μ2 is the comparative index of the matrices indicated, and μ∗= μ∗
1 +μ∗
2
is the dual comparative index.
Proof In view of property (iii) of Theorem 3.5 and that Z−1
k
= Z−1
k+1Sk, the second
equality in (4.14) is obvious. Now we prove the ﬁrst equality. To this end, it is
sufﬁcient to apply the statement of Theorem 3.2(i) as follows
μ1

Yk+1, Sk(0 I)T 
= rank(I −Xk+1X†
k+1) Bk = m1(k),

206
4
Oscillation Theory of Symplectic Systems
and also the Wronskian identity (2.4)
w

Yk+1, Sk(0 I)T 
= w

Yk, (0 I)T 
= XT
k
(4.15)
when computing
μ2

Yk+1, Sk(0 I)T 
= ind Tk(XkX†
k+1Bk) Tk = m2(k),
Tk = I −˜
M†
k ˜
Mk,
˜
Mk = (I −Xk+1X†
k+1)Bk.
This proves the lemma.
⊓⊔
Similarly as in Lemma 4.7, we can prove the following statement.
Lemma 4.8 Let Z be a symplectic fundamental matrix of (SDS) (which is also
a fundamental matrix of (2.9)), and Y = Z(0 I)T . Then
m∗
i (k) = μ∗
i

Yk, S−1
k (0 I)T 
= μk

Z−1
k (0 I)T , Z−1
k+1(0 I)T 
,
i ∈{1, 2},
(4.16)
where m∗(k) = m∗
1(k) + m∗
2(k) is the number of backward focal points of Y in
[k, k + 1).
Proof The second equality in (4.16) follows from property (iii) of Theorem 3.5 and
the duality principle (Theorem 3.11), where the equality Z−1
k S−1
k
= Z−1
k+1 has been
used. As for the ﬁrst equality in (4.16), using the deﬁnition of the dual comparative
index (3.12), we have by (i) of Theorem 3.5
μ∗
1

Yk, S−1
k (0 I)T 
= μ1

Yk, S−1
k (0 I)T 
= μ1
Xk
Uk

,
−BT
k
AT
k

= rank(I −XkX†
k) BT
k = m∗
1(k).
Using the Wronskian identity (2.4)
w

Yk, S−1
k (0 I)T 
= w

Yk+1, (0 I)T 
= XT
k+1
(4.17)
and the deﬁnition of μ∗
2, we obtain
μ∗
2

Yk, S−1
k (0 I)T 
= ind [−˜TkXk+1X†
k(−BT
k ) ˜Tk] = m∗
2(k).
This lemma is proved.
⊓⊔

4.1
Multiplicity of Focal Points
207
Proof of Proposition 4.4
(i): By Lemma 4.7,
m2(k) = μ2(Yk+1, Sk(0 I)T ) = ind P,
where the matrix P, evaluated by the deﬁnition of the comparative index (see
Deﬁnition 3.1), coincides with Pk given by (4.2). Then, Pk is symmetric
by Theorem 3.2(iii). Similarly, by Lemma 4.8, Theorem 3.2(iii), and the
deﬁnition of the dual comparative index (3.12), we prove the symmetry of
˜Pk.
(ii): By Lemma 4.7, we have
m1(k) = μ1(Yk+1, Sk(0 I)T ) = μ∗
1(Z−1
k+1(0 I)T , Z−1
k (0 I)T )
= μ1(Z−1
k+1(0 I)T , Z−1
k (0 I)T ),
and by Theorem 3.2(ii) the equality μ1(Z−1
k+1(0 I)T , Z−1
k (0 I)T ) = 0 holds if
and only if Im XT
k ⊆Im XT
k+1 or if and only if Ker Xk+1 ⊆Ker Xk. Similarly,
μ1(Yk+1, Sk(0 I)T ) = 0 is equivalent to Im Bk
⊆Im Xk+1 by Theo-
rem 3.2(ii). In both cases Tk = I and m(k) = m2(k) = μ2(Yk+1, Sk(0 I)T ) =
ind Pk, again by Theorem 3.2(ii).
(iii): Applying Lemma 4.8 we have
m∗
1(k) = μ∗
1(Yk, S−1
k (0 I)T ) = μ1(Yk, S−1
k (0 I)T )
= μ1(Z−1
k (0 I)T , Z−1
k+1(0 I)T ).
By Theorem 3.2(ii), the equality μ1(Z−1
k (0 I)T , Z−1
k+1(0 I)T ) = 0 holds
if and only if Im XT
k+1 ⊆Im XT
k or if and only if Ker Xk ⊆Ker Xk+1.
Similarly, condition μ1(Yk, S−1
k (0 I)T ) = 0 is equivalent to Im BT
k ⊆Im Xk.
In both vases, by Theorem 3.2(ii), we have Tk = I and m∗(k) = m∗
2(k) =
μ∗
2(Yk, S−1
k (0 I)T ) = ind ˜Pk.
(iv): The proof follows from (ii) and (iii).
(v): According to Lemma 4.7, m(k) = μ(Y, ˆY) holds, where Y := Yk+1 and
ˆY := Sk(0 I)T , and then w(Y, ˆY ) = XT
k (see (4.15)) and ˆX := Bk. We
see that the ﬁrst estimate follows from Theorem 3.5(vii), Lemma 3.9, and
Remark 3.10. In a similar way, the second estimate follows from Lemma 4.8
(4.17) and again from Theorem 3.5(vii), Lemma 3.9, and Remark 3.10.
(vi): By Lemma 4.8, we have m∗
i (k) = μi

Z−1
k (0 I)T , Z−1
k+1(0 I)T 
for i ∈
{1, 2}. Using property (vi) of Theorem 3.5 in the right-hand side of the last
formula, we obtain
μ1

Z−1
k (0 I)T , Z−1
k+1(0 I)T 
= rank Xk+1−rank Xk+μ∗
1

Z−1
k+1(0 I)T , Z−1
k (0 I)T 
.

208
4
Oscillation Theory of Symplectic Systems
Using the identity μ∗
1

Z−1
k+1(0 I)T , Z−1
k (0 I)T 
= m1(k) from Lemma 4.7,
we obtain (4.7). Analogously, from (iv) of Theorem 3.5, it follows that
m∗
2(k) = μ2

Z−1
k (0 I)T , Z−1
k+1(0 I)T 
= μ∗
2

Z−1
k+1(0 I)T , Z−1
k (0 I)T 
= m2(k),
which proves (4.8). Formula (4.9) now follows from the deﬁnition of m∗(k) as
the sum of m∗
1(k) and m∗
2(k) (analogously for m(k)) and from (4.7) and (4.8).
⊓⊔
Note that by Theorems 3.2 and 3.5(iii), it is possible to formulate six equivalent
deﬁnitions of the comparative index μ(Y, ˆY ); as for any choice of M or
˜
M given
by (3.3) or (3.5), there exist three different representations of P deﬁned by (3.4),
(3.10), and the similar representation for the second component of the comparative
index μ(Z−1(0 I)T , Z−1 ˆZ(0 I)T ). By Lemmas 4.7 and 4.8, these deﬁnitions of the
comparative index imply equivalent deﬁnitions of the multiplicity of a focal point
in (k, k + 1] and [k, k + 1), as we have already mentioned in Remark 4.2. Now
we formulate some of them and leave it to the reader to show how other deﬁnitions
follow from the properties of the comparative index.
Deﬁnition 4.9 The number of forward focal points m(k) = m1(k) + m2(k) in
(k, k + 1] of a conjoined basis Y =
X
U

of (SDS) is given by the formula
m1(k) = rank Mk,
Mk := (I −X†
k+1Xk+1) XT
k ,
Tk := I −M†
kMk,
(4.18)
m2(k) = ind Pk,
Pk = TkXkX†
k+1BkTk,
(4.19)
where the matrix Pk in (4.19) can be expressed as
Pk = Tk (BT
k Dk −BT
k Qk+1Bk) Tk,
(4.20)
and Qk is any symmetric matrix satisfying
XT
k QkXk = XT
k Uk.
(4.21)
Deﬁnition 4.10 Let Z = ( ˜Y Y) be the fundamental symplectic matrix of (SDS),
where ˜Y =
 ˜X
˜U

and Y =
X
U

are conjoined bases of (SDS). Then the number of
focal points m(k) = m1(k) + m2(k) in (k, k + 1] of the conjoined basis Y is given
by the formula (4.18) and
m2(k) = ind TkXk( ˜Qk) XT
k Tk,
(4.22)

4.1
Multiplicity of Focal Points
209
where the symmetric matrix ˜Qk satisﬁes
Xk ˜QkXT
k = −˜XkXT
k .
(4.23)
In a similar way, for the multiplicity of backward focal points, one can formulate
the following deﬁnitions.
Deﬁnition 4.11 The number of backward focal points m∗(k) = m∗
1(k) + m∗
2(k) in
[k, k + 1) of a conjoined basis Y =
X
U

of (SDS) is given by the formula
m∗
1(k) = rank ˜
Mk,
˜
Mk := (I −X†
kXk) XT
k+1,
˜Tk := I −˜
M†
k ˜
Mk,
(4.24)
m∗
2(k) = ind ˜Pk,
˜Pk = ˜T T
k Xk+1X†
kBT
k ˜Tk,
(4.25)
where the matrix ˜Pk in (4.25) can be expressed as
˜Pk = ˜Tk (BkQkBT
k + BkAT
k ) ˜Tk,
(4.26)
and Qk is any symmetric matrix satisfying
XT
k QkXk = XT
k Uk.
(4.27)
Deﬁnition 4.12 Let Z = ( ˜Y Y) be the fundamental symplectic matrix of (SDS),
where ˜Y =
 ˜X
˜U

and Y =
X
U

are conjoined bases of (SDS). Then the number of
backward focal points m∗(k) = m∗
1(k) + m∗
2(k) in [k, k + 1) of the conjoined basis
Y is given by the formula (4.24) for m∗
1(k) and m∗
2(k) = m2(k) with m2(k) given
by (4.22), (4.23).
Proposition 4.13 Deﬁnitions 4.1, 4.9, and 4.10 are equivalent. Similarly, Deﬁni-
tions 4.3, 4.11, and 4.12 are equivalent.
Proof The equivalence of Deﬁnitions 4.1 and 4.9 follows from Lemma 4.7. Indeed,
the computation of μ Yk+1, Sk(0 I)T  by Deﬁnition 3.1, using (4.15), gives (4.18)
and (4.19). Formulas (4.20) and (4.21) follow from (iii) of Theorem 3.2. The proof
of the equivalence of Deﬁnitions 4.1 and 4.10 follows from the second formula in
Lemma 4.7. Indeed, we compute the quantity μ∗
Z−1
k+1(0 I)T , Z−1
k (0 I)T 
by (i)
and (iii) of Theorem 3.2, using the formula Z−1
k (0 I)T = −XT
k
˜XT
k
 and the deﬁnition
of the dual comparative index (see (3.12)). This proves (4.18) and (4.22). In a similar
way, based on Lemma 4.8 and the properties of the comparative index, we prove the
equivalence of Deﬁnitions 4.3, 4.11, and 4.12.
⊓⊔

210
4
Oscillation Theory of Symplectic Systems
Remark 4.14
(i) A consequence of the just proved Proposition 4.13 was already mentioned
in Lemma 2.15, where we have proved the equivalence of the inclusions
Ker Xk+1 ⊆Ker Xk and Im Bk ⊆Im Xk+1, which is the consequence of
the fact that m1(k) = 0. Proposition 4.13 also implies the equivalence of
the inequalities XkX†
k+1Bk ≥0, BT
k Dk −BT
k Qk+1Bk ≥0, and Xk( ˜Qk+1 −
˜Qk) XT
k ≥0 when m2(k) = 0.
(ii) The properties of the matrix
˜Q in Deﬁnition 4.10 satisfying (4.23) have
an interesting relationship with the analogously deﬁned matrix for linear
Hamiltonian differential system (1.103). Under certain assumptions (see
Sect. 1.5), the invertibility of the upper entry X of a conjoined basis Y =
X
U

of (1.103) implies the strict monotonicity of the matrix ˜Q(t) = −X−1(t) ˜X(t),
i.e., ˜Q1(t1) <
˜Q(t2) for t1 < t2, where ˜Y =  ˜X
˜U
 and Y are normalized
conjoined bases of (1.103); see [205, pg. 125]. Note that the assumption of
invertibility of the matrix X(t) in the continuous case corresponds to the
absence of focal points of Y = X
U
 in the discrete case, i.e., to m(k) = 0 for all
k ∈[0, N]Z. For this case and under the additional assumption det Xk ̸= 0, we
have det Xl ̸= 0 and  ˜Ql−1 ≥0 for l ∈[k + 1, N + 1]Z with ˜Ql := −X−1
l
˜Xl
for l ∈[k, N +1]Z (see Remark 4.14(i)), and the strict monotonicity  ˜Qk > 0
holds if and only if det Bk ̸= 0. This fact follows from (iii) of Theorem 3.2 and
from the equivalence of Deﬁnitions 4.1 and 4.10. It is interesting to note that
this strict monotonicity does not require Bk > 0.
(iii) Consider symplectic system (SDS) for the scalar case (n = 1)
xk+1
uk+1

=
ak bk
ck dk
 xk
uk

,
akdk −ckbk = 1.
(4.28)
We will show that the number m(yk), resp., m∗(yk), evaluated for a solution
yk
= (xk, uk)T with x2
k + u2
k
̸= 0, takes the maximal value 1 if and
only if yk has a generalized zero in (k, k + 1], resp., in [k, k + 1) (see
Deﬁnitions 2.17, 2.23). First of all let us note that the case bk = 0 is excluded
from the consideration. Indeed, condition bk = 0 implies that m(yk) = 0, resp.
m∗(yk) = 0, by the estimates in Proposition 4.4(v). In a similar way, if bk = 0,
then yk has no generalized zero in (k, k + 1], resp., in [k, k + 1), indeed, for
this case the conditions xk ̸= 0 and xk+1 = 0 in (2.41), resp., xk+1 ̸= 0 and
xk = 0 in (2.49), are impossible by (4.28), where akdk −ckbk = akdk = 1.
Next assume that m(yk) = 1. Then, by Deﬁnition 4.9 we have
m1(yk) = 1 ⇔rank [(1 −xk+1x†
k+1) xk] = 1 ⇔xk ̸= 0, xk+1 = 0,
m2(yk) = 1 ⇔xkx−1
k+1bk < 0.

4.2
Sturmian Separation Theorems and Its Corollaries
211
The last conditions constitute Deﬁnition 2.17 for n = 1 (compare also with
Deﬁnition 1.1). Similarly, assuming that m∗(yk) = 1, we have by Deﬁnition 4.11
that
m∗
1(yk) = 1 ⇔rank[(1 −xkx†
k) xk+1] = 1 ⇔xk+1 ̸= 0, xk = 0,
m∗
2(yk) = 1 ⇔xk+1x−1
k bk < 0.
The last conditions constitute Deﬁnition 2.23 for n = 1.
Remark 4.15 In Remark 6.60 we will present additional formulas for the multiplici-
ties of forward and backward focal points m(k) and m∗(k) by using the comparative
index.
4.2
Sturmian Separation Theorems and Its Corollaries
In this section we present discrete analogs of classical Sturmian separation theorems
(see Theorem 1.44) and its generalizations (see Theorem 1.50). The consideration
is based on the fundamental notion of the multiplicity of a focal point.
4.2.1
Separation Theorem: First Step
In this subsection we present the results of the paper [101], where the ﬁrst step
toward separation theorem which counts focal points including multiplicities has
been made. The construction in the proof of the main result of this subsection shows
that if the principal solution Y [0] of (SDS) at k = 0 has m ∈N (forward) focal points
in the interval (0, N+1], then there exist m admissible pairs y = (x, u) with linearly
independent components x = {xk}N
k=1 ∈RNn with x0 = 0 = xN+1, such that the
associated quadratic functional F(y) ≤0.
Theorem 4.16 (Sturmian Separation Theorem) Suppose that the principal solu-
tion Y [0] of (SDS) at 0 has no forward focal points in (0, N + 1]. Then any other
conjoined basis of (SDS) has at most n focal points in (0, N + 1].
The general idea of the proof is simple. The assumptions of theorem imply
that F(y) > 0 for every nontrivial admissible y with x0 = 0 = xN+1 (see
Theorem 2.36). We show that the existence of a conjoined basis with more than
n focal points enables the construction of an admissible y for which F(y) < 0.
In Proposition 2.38 we presented a construction of an admissible pair for (2.57)
for which F ̸≥0, when kernel condition or P-condition in (2.37) are violated. In the
following result we discuss this construction in the framework of the multiplicities

212
4
Oscillation Theory of Symplectic Systems
of forward focal points. The next lemma is a consequence of [205, Lemmas 3.1.5
and 3.1.6]; see also [208, pg. 142].
Lemma 4.17 Let Y =
X
U

be a conjoined basis of (SDS), Mk be given by (4.1)
and k ∈[0, N]Z. Then there exists an n × n matrix Sk such that
rank Sk = rankMk,
Xk+1Sk = 0,
Ker Xk ∩Im Sk = {0}.
(4.29)
In the next two lemmas, the matrices Mk, Pk, Tk are deﬁned by (4.1) and (4.2).
Lemma 4.18 Let rank Mk = p. Then there exist linearly independent vectors
α1, . . . , αp ∈Rn such that
Xk+1αj = 0,
Xkαj ̸= 0,
j = 1, . . . , p.
Proof Let Sk be the n × n matrix for which (4.29) holds, and let α1, . . . , αp be
a basis of Im Sk. Then Im Sk ⊆Ker Xk+1 implies Xj+1αj = 0, and in addition
Ker Xk ∩Im Sk = {0} implies Xjαj ̸= 0 for j ∈{1, . . ., p}.
⊓⊔
Lemma 4.19 Let (k, k + 1] contain a focal point of multiplicity p + q ≤n of
a conjoined basis Y =
X
U

of (SDS), where p = rank Mk and q = ind Pk. Further,
let α1, . . . , αp be the same as in the Lemma 4.18 and β1, . . . , βq be orthogonal
vectors corresponding to the negative eigenvalues of Pk, i.e., βT
j Pkβj < 0 for j ∈
{1, . . ., q}. Denote γj := X†
k+1BkTkβj. Then the vectors α1, . . . αp, γ1, . . . , γq are
linearly independent.
Proof First we prove that γ1, . . . , γq are linearly independent. Suppose that this is
not the case, i.e., there exists a nontrivial linear combination q
j=1 μjγj = 0, and
let β = q
j=1 μjβj. Then
0 > βT Pkβ = βT T T
k Xk
⎛
⎝
q

j=1
μjX†
k+1BkTkβj
⎞
⎠= βT T T
k Xk
⎛
⎝
q

j=1
μjγj
⎞
⎠= 0,
which is a contradiction. Now suppose that γ = q
j=1 μjγj = p
j=1 λjαj ̸= 0,
and let β = q
j=1 μjβj be as before. Then
0 > βT Pkβ = βT T T
k XkX†
k+1BkTkβ = βT T T
k XkX†
k+1Xk+1γ = 0,
which is a contradiction.
⊓⊔
Recall that if (k, k + 1] contains a focal point of multiplicity p + q, where p =
rankMk, q = ind Pk, we say that p focal points are at k + 1, and q focal points are
in the open interval (k, k + 1).

4.2
Sturmian Separation Theorems and Its Corollaries
213
Proof of Theorem 4.16 Let Y =
X
U

be a conjoined basis of (SDS), and let the
intervals
(ki, ki + 1] ⊆(0, N + 1],
i ∈{1, . . ., l}, 0 ≤k1 < k2 < · · · < kl ≤N,
contain focal points of Y of multiplicities mi, i ∈{1, . . . , l}. Let mi = pi+qi, where
pi = rank Mki, qi = ind Pki. For each interval (ki, ki+1] deﬁne the admissible pairs
y[i,j] as follows. For j ∈{1, . . . , pi} we set
x[i,j]
k
=

Xkα[i]
j ,
0 ≤k ≤ki,
0,
ki + 1 ≤k ≤N + 1,
u[i,j]
k
=

Ukα[i]
j ,
0 ≤k ≤ki,
0,
ki + 1 ≤k ≤N + 1,
(4.30)
where α[i]
j
∈Ker Xki+1 \ Ker Xki are linearly independent n-dimensional vectors
(see Lemma 4.19). For j ∈{pi + 1, . . . , pi + qi} we deﬁne
x[i,j]
k
=

Xkγ [i]
j ,
0 ≤k ≤ki,
0,
ki + 1 ≤k ≤N + 1,
u[i,j]
k
=
⎧
⎪⎪⎨
⎪⎪⎩
Ukγ [i]
j ,
0 ≤k ≤ki −1,
Ukγ [i]
j
−Tkβ[i]
j ,
k = ki,
0,
ki + 1 ≤k ≤N + 1,
(4.31)
where β[i]
j , j ∈{1, . . ., qi} are orthogonal eigenvectors corresponding to the nega-
tive eigenvalues of the matrix Pki and γ [i]
j
= X†
ki+1Bkiβ[i]
j . By Proposition 2.38, we
have for any i ∈{1, . . . , l}
F(y[i,j]) = (x[i,j]
0
)T u[i,j]
0
,
j ∈{1, . . ., pi},
F(y[i,j]) = (x[i,j]
0
)T u[i,j]
0
+ (β[i]
j )T Pk1β[i]
j , j ∈{pi + 1, . . . , pi + qi}.
To simplify some of the next computations, we relabel occasionally the quantities
x[i,j], u[i,j], α[i]
j , . . . as follows. We introduce the index ℓ∈{1, . . . , l
i=1 mi} by
[i, j] !−→ℓ= i−1
s=0 ms + j, m0 := 0.
Now suppose, by contradiction, that the number of focal points of Y in (0, N +
1] exceeds n, i.e., m := l
i=1 mi > n. In order to make the idea of the proof
more understandable, we will ﬁrst suppose that qi = 0 for i ∈{1, . . ., l}, i.e., all
focal points are at ki + 1 (the kernel condition is violated but all Pki ≥0). Since

214
4
Oscillation Theory of Symplectic Systems
l
i=1 mi = l
i=1 pi = m > n, there exists a nontrivial linear combination
m

ℓ=1
μℓx[ℓ]
0
= 0,
(4.32)
i.e., the admissible pair y =
x
u

given by
xk :=
m

ℓ=1
μℓx[ℓ]
k ,
uk :=
m

ℓ=1
μℓu[ℓ]
k ,
k ∈[1, N + 1]Z,
(4.33)
satisﬁes x0 = 0 = xN+1. Moreover, the Nn-dimensional vector x = {xk}N
k=1 is
nonzero. Indeed, consider ﬁrst the largest focal point kl +1 in (0, N +1]. According
to the construction of x[i,j] (returning to the original labeling), we have
x[i,j]
kl
= 0,
i ∈{1, . . ., l −1}, j ∈{1, . . . , pi},
so if x = 0, i.e., in particular, xkl = 0, we have
pl

j=1
μl,jx[l,j]
kl
=
pl

j=1
μl,jXklα[l]
j = Xkl
⎛
⎝
pl

j=1
μl,jα[l]
j
⎞
⎠= 0.
(4.34)
Since the vectors α[l]
j , j ∈{1, . . ., pl} form the basis of the space Im Skl, where Skl
is the same as Sk in the proof of Lemma 4.18 (here with k = kl) and at the same
time by (4.34)
pl

j=1
μl,jα[l]
j
∈Ker Xkl,
we have pl
j=1 μl,jα[l]
j = 0 because of Lemma 4.17, which means that the numbers
μl,j = 0, j ∈{1, . . ., pl}, since the vectors α[l]
j , are linearly independent. Repeating
the previous argument for k ∈{kl−1, . . . , k = k1}, we ﬁnd that μi,j = 0 for i ∈
{1, . . ., l} and j ∈{1, . . . , pi}, which contradicts our assumption that the linear
combination (4.32) is nontrivial. Therefore, x ̸= 0 in the admissible pair given by
(4.33).
Now, let y[κ] = (x[κ], u[κ]) and y[ℓ] = (x[ℓ], u[ℓ]) for κ, ℓ∈{1, . . . , m} be two
admissible pairs constructed by (4.30). Then using (2.62) we obtain
F(y[κ]; y[ℓ]) =

0,
κ ̸= ℓ,
(x[ℓ]
0 )T u[ℓ]
0 ,
κ = ℓ.
(4.35)

4.2
Sturmian Separation Theorems and Its Corollaries
215
Consequently, for y = x
u
 given by (4.33), we have
F(y) = F m

ℓ=1
μly[ℓ] =
m

κ,ℓ=1
μκμℓF(y[κ]; y[ℓ])
=
 m

ℓ=1
μℓx[ℓ]
0
T  m

ℓ=1
μℓu[ℓ]
0

= xT
0 u0 = 0,
since x0 = 0 by (4.32). This contradicts the positivity of F.
Suppose now that at least one of the qi, i ∈{1, . . ., l} is positive. Then we have
for this index and for j ∈{1, . . . , pi}
F(y[i,j]) = (x[i,j]
0
)T u[i,j]
0
+ (β[i]
j )T Pkiβ[i]
j ,
and we have admissible pairs deﬁned both by (4.30) and (4.31). In the previous
part of the proof, we have already computed F(z[κ]; z[ℓ]) for admissible pairs given
by (4.30). It remains to compute this bilinear form if one or both admissible pairs
are of the form (4.31). We will perform the computation in the latter case. In the
former case (i.e., one of the admissible pairs is given by (4.30) and the second one
by (4.31)), substituting into the formula into (2.62), we get again (4.35). So, let
y[κ] and y[ℓ] be two admissible pairs given (4.31). If they are associated with the
different focal intervals (i.e., the integers ki in (4.31) are different for y[κ] and y[ℓ]),
using (2.62) we ﬁnd again that (4.35) holds. Therefore, suppose ﬁnally that y[κ] and
y[ℓ] correspond to the same focal interval (ki, ki + 1). Then
F(y[κ]; y[ℓ]) = (x[κ]
0 )T u[ℓ]
0 + (x[κ]
ki )T {Cki−1x[ℓ]
ki−1 + Dki−1u[ℓ]
ki−1 −u[ℓ]
ki }
= (x[κ]
0 )T u[ℓ]
0 + (γ [κ])T XkiTkiβ[ℓ]
= (x[κ]
0 )T u[ℓ]
0 + (β[κ])T Pkiβ[ℓ].
If κ ̸= ℓ, then the vectors β[κ] and β[ℓ] are orthogonal eigenvectors of the matrix
Pki, and thus (β[κ])T Pki β[ℓ] = 0.
Summarizing our previous computations, for z = (x, u) given by (4.33) (i.e.,
x0 = 0 by (4.32)), we have (again with the two-indices labeling)
F(y) =
l
i=1
qi

j=1
(β[i]
j )T Pkiβ[i]
j
< 0,
which again contradicts the positivity of F. Note that x = {xk}N
k=1 is again
nontrivial, since for each i ∈{1, . . ., l} the vectors α[i]
j
and γ [i]
s
for j ∈{1, . . ., pi}
and s ∈{1, . . . , qi} are linearly independent (by Lemma 4.19) and one can repeat

216
4
Oscillation Theory of Symplectic Systems
the same argument as used in that part of the proof where we supposed that qi = 0
for i ∈{1, . . . , l}.
⊓⊔
4.2.2
Separation Theorems and Comparative Index
In this section we present a discrete version of Theorem 1.50 and derive corollaries
of this result, in particular we prove a discrete version of Theorem 1.44. We prove
these results using the relationship between the concept of a focal point and the
comparative index elaborated in Sect. 4.1.2. This relationship gives the possibility
to apply algebraic properties of the comparative index, in particular Theorem 3.6,
which is crucial for the results of this section. We start with a discrete version of
Theorem 1.50.
Theorem 4.20 Let Y and ˆY be conjoined bases of (SDS). Then
μ(Yk, ˆYk) = m(k) −ˆm(k),
(4.36)
where μ(Yk, ˆYk) is the comparative index of Yk and ˆYk and where m(k) and ˆm(k)
are the multiplicities of forward focal points of Y and ˆY, respectively, in the interval
(k, k + 1].
Proof The proof easily follows from Theorem 3.6 and Lemma 4.7. We put Z := Zk,
ˆZ := ˆZk, W := Sk in (3.16), where Z and ˆZ are symplectic fundamental matrices
of (SDS) such that Y = Z(0 I)T and ˆY = ˆZ(0 I)T . Then
WZ = Zk+1 = SkZk,
W ˆZ = ˆZk+1 = Sk ˆZk,
and using Lemma 4.7 and (3.16), we have
μ(Yk+1, ˆYk+1) −μ(Yk, ˆYk) = m(k) −ˆm(k)
which is equality (4.36).
⊓⊔
Similarly, using (3.26) and Lemma 4.8, we obtain the “dual” identity
μ∗(Yk, ˆYk) −μ∗(Yk+1, ˆYk+1) = m∗(k) −ˆm∗(k),
which is essentially a formula (4.36) for reversed symplectic system (2.47). This
result is formulated in the next theorem.
Theorem 4.21 Let Y and ˆY be conjoined bases of (SDS) (which are also conjoined
bases of (2.47)). Then
−μ∗(Yk, ˆYk) = m∗(k) −ˆm∗(k),
(4.37)

4.2
Sturmian Separation Theorems and Its Corollaries
217
where μ∗(Yk, ˆYk) is the dual comparative index of Y and ˆY and where m∗(k) and
ˆm∗(k) are the numbers of backward focal points of Y and ˆY in the interval [k, k+1).
Remark 4.22 In Theorems 4.20 and 4.21, the conjoined bases Y and ˆY play the
same role. It means that interchanging them in (4.36), (4.37), we obtain
−μ( ˆYk, Yk) = m(k) −ˆm(k),
(4.38)
and
μ∗( ˆYk, Yk) = m∗(k) −ˆm∗(k).
(4.39)
Note that comparing left-hand sides of (4.36) and (4.38) (analogously for (4.37) and
(4.39)), we obtain
−μ( ˆYk, Yk) = μ(Yk, ˆYk),
resp.
−μ∗( ˆYk, Yk) = μ∗(Yk, ˆYk).
The last equalities, taking into account property (v) of Theorem 3.5, are conse-
quences of the Wronskian identity (2.4) for conjoined bases Yk, ˆYk of (SDS).
The ﬁrst important consequence of (4.36) is a discrete version of Theorem 1.44
for linear Hamiltonian differential systems. It says, among others, that the numbers
of focal points of any two conjoined bases of (SDS) in a given interval differ by at
most n.
Theorem 4.23 Let the number of (forward) focal points of Y and ˆY in the interval
(M, N + 1] be given by (4.10). Then
l(Y, M, N + 1) −l( ˆY, M, N + 1) = μ(YN+1, ˆYN+1) −μ(YM, ˆYM),
(4.40)
and
μ(YN+1, ˆYN+1) −μ(YM, ˆYM)
= μ(YN+1, Y [M]
N+1) −μ( ˆYN+1, Y [M]
N+1)
(4.41)
= μ( ˆYM, Y [N+1]
M
) −μ(YM, Y [N+1]
M
),
(4.42)
where Y [M] and Y [N+1] are the principal solutions of (SDS) at M and N + 1,
respectively.
Proof The summation of both sides of (4.36) from k = M to k = N yields equality
(4.40). Formula (4.41) follows from Theorem 3.6 with W := ZN+1Z−1
M , Y := YM,
and ˆY := ˆYM, where Zk is a fundamental solution matrix of (SDS). Formula (4.42)
is derived from (4.41), applying property (ix) in Theorem 3.5, where we also use the
obvious relations Y [M]
N+1 = ZN+1Z−1
M (0 I)T and Y [N+1]
M
= ZMZ−1
N+1(0 I)T .
⊓⊔
Quite analogously we derive similar results for the backward focal points.

218
4
Oscillation Theory of Symplectic Systems
Theorem 4.24 Let the number of backward focal points of conjoined bases Y and
ˆY of (SDS) in [M, N + 1) be given by (4.11). Then
l∗(Y, M, N + 1) −l∗( ˆY, M, N + 1) = μ∗(YM, ˆYM) −μ∗(YN+1, ˆYN+1),
(4.43)
and
μ∗(YM, ˆYM) −μ∗(YN+1, ˆYN+1)
= μ∗( ˆYN+1, Y [M]
N+1) −μ∗(YN+1, Y [M]
N+1)
(4.44)
= μ∗(YM, Y [N+1]
M
) −μ∗( ˆYM, Y [N+1]
M
),
(4.45)
where Y [M] and Y [N+1] are the principal solutions of (SDS) at M and N + 1,
respectively.
Proof By the summation of both sides of (4.37) from k = M to k = N, we
obtain (4.43). Formula (4.45) follows from the dual version of Theorem 3.6 (see
formula (3.26)) for the case W := ZN+1Z−1
M , Y := YM, ˆY := ˆYM, where Zk
is a fundamental solution matrix of (SDS). Formula (4.44) is derived from (4.45)
applying the dual version of property (ix) in Theorem 3.5.
⊓⊔
In the remaining part of this subsection, we present important corollaries to
Theorems 4.23 and 4.24. In particular, applying property (vii) of Theorem 3.5,
it is possible to derive a new set of inequalities for the difference l(Y, M, N + 1) −
l( ˆY, M, N + 1) of the multiplicities of focal points. The ﬁrst result in this direction,
based on (4.40), is presented below.
Corollary 4.25 Let Y and ˆY be two conjoined bases of (SDS). Then the following
estimate holds
		l(Y, M, N + 1) −l( ˆY, M, N + 1)
		
≤min
!
rank w(Y, ˆY ), rM,N+1, ˆrM,N+1
"
≤n,
(4.46)
where n is the dimension of block entries of the matrix Sk in (SDS), w(Y, ˆY ) is the
(constant) Wronskian of Y and ˆY, and
rM,N+1 := max{rankXM, rank XN+1},
ˆrM,N+1 := max{rank ˆXM, rank ˆXN+1}.

(4.47)
Proof Estimate (4.46) follows from (vii) of Theorem 3.5 and from the Wron-
skian identity w(YM, ˆYM) = w(YN+1, ˆYN+1) in (2.4). More speciﬁcally, the
left-hand side of (4.46) is equal to |μN+1 −μM|, where μk is the abbrevia-
tion for μ(Yk, ˆYk). From property (vii) in Theorem 3.5, we know that μM ≤
min{rankw(Y, ˆY ), rank ˆXM} and μN+1 ≤min{rankw(Y, ˆY ), rank ˆXN+1}, which

4.2
Sturmian Separation Theorems and Its Corollaries
219
yields that
|μN+1 −μM| ≤min{rankw(Y, ˆY ), ˆrM,N+1},
(4.48)
where ˆrM,N+1 is given in (4.47). If we now switch the roles of the conjoined bases
Y and ˆY, abbreviate μ
 ˆYk, Yk

as ˆμk, and use that the Wronskian of ˆY and Y is
equal to −wT (Y, ˆY ), then the formula μk + ˆμk = rankw(Y, ˆY ) in property (v) of
Theorem 3.5 yields
|μN+1 −μM| = | ˆμN+1 −ˆμM|
(4.48)
≤
min{rankw(Y, ˆY ), rM,N+1}
(4.49)
with rM,N+1 given again in (4.47). By combining (4.48) and (4.49), we obtain the
estimate in (4.46).
⊓⊔
By a similar way, we derive estimates for the backward focal points.
Corollary 4.26 Let Y and ˆY be two conjoined bases of (SDS). Then we have
		l∗(Y, M, N + 1) −l∗( ˆY, M, N + 1)
		
≤min
!
rankw(Y, ˆY ), rM,N+1, ˆrM,N+1
"
≤n,
(4.50)
where rM,N+1 and ˆrM,N+1 are given in (4.47).
Quite analogously we obtain the next statement, based on (4.41), (4.42), (4.44),
and (4.45). In contrast with Corollaries 4.25 and 4.26, we obtain the estimates,
which deal with conjoined bases of (SDS) taken in the same point M or N + 1.
Corollary 4.27 For any conjoined bases Y and ˆY of (SDS), we have the estimates
		 l(Y, M, N + 1) −l( ˆY, M, N + 1)
		 ≤min
!
qM, qN+1, rankX[N+1]
M
"
≤n,
(4.51)
		 l∗(Y, M, N + 1) −l∗( ˆY, M, N + 1)
		 ≤min
!
qM, qN+1, rankX[N+1]
M
"
≤n,
(4.52)
where
qk := max{rankXk, rank ˆXk}.
(4.53)
Proof From formula (4.41) we get the estimate
		l(Y, M, N + 1) −l( ˆY, M, N + 1)
		 ≤max
!
μ(YN+1, Y [M]
N+1), μ( ˆYN+1, Y [M]
N+1)
"

220
4
Oscillation Theory of Symplectic Systems
and by a similar way, from (4.42) we have
		l(Y, M, N + 1) −l( ˆY, M, N + 1)
		 ≤max
!
μ( ˆYM, Y [N+1]
M
), μ(YM, Y [N+1]
M
)
"
.
Then, applying property (vii) in Theorem 3.5 to the comparative indices in these
inequalities and using the Wronskian identity (2.4) when computing
rankw(Yk, Y [i]
k ) = rank Xi,
rank w( ˆYk, Y [i]
k ) = rank ˆXi, i ∈{M, N + 1},
we derive
		 l(Y, M, N + 1) −l( ˆY, M, N + 1)
		
≤max
!
min{rankXM, rankX[M]
N+1}, min{rank ˆXM, rankX[M]
N+1}
"
,
and
		 l(Y, M, N + 1) −l( ˆY , M, N + 1)
		
≤max ! min{rankXN+1, rankX[N+1]
M
}, min{rank ˆXN+1, rank X[N+1]
M
} ".
Formula (4.51) now follows from
rankw(Y [N+1]
k
, Y [M]
k
) = rankX[N+1]
M
= rank X[M]
N+1
and from the fact that for any real numbers x, y, z we have the equality
max{min{x, z}, min{y, z}} = min{max{x, y}, z}.
In a similar way we obtain (4.52) from (4.44), (4.45), and from property (vii) in
Theorem 3.5.
⊓⊔
Remark 4.28
(i) The results in Corollary 4.27 have interesting consequences. In particular, it
is possible to obtain an estimate for the difference of the forward focal points
of two conjoined bases Y and ˆY in the interval (M, N + 1], which does not
depend on the values at the right (or left) endpoint k = N + 1 (or k = M).
More precisely, inequality (4.51) implies that
		 l(Y, M, N + 1) −l( ˆY, M, N + 1)
		 ≤max{rankXM, rank ˆXM} ≤n,
(4.54)
		 l(Y, M, N + 1) −l( ˆY, M, N + 1)
		 ≤max{rankXN+1, rank ˆXN+1} ≤n.
(4.55)

4.2
Sturmian Separation Theorems and Its Corollaries
221
(ii) Analogously, inequality (4.52) yields similar estimates for the quantity
		l∗(Y, M, N + 1) −l∗( ˆY, M, N + 1)
		. The results in (4.54)–(4.55) allow
to compare the numbers of focal points of Y and ˆY in unbounded intervals,
when the system (SDS) is nonoscillatory.
(iii) It also follows from Corollary 4.27 that we can compare the numbers of focal
points of Y and ˆY using estimates which do not depend on Y and ˆY. In more
details we have from (4.51) and (4.52)
		 l(Y, M, N + 1) −l( ˆY, M, N + 1)
		 ≤rankX[M]
N+1 = rankX[N+1]
M
≤n,
(4.56)
		 l∗(Y, M, N + 1) −l∗( ˆY, M, N + 1)
		 ≤rankX[N+1]
M
= rank X[M]
N+1 ≤n.
(4.57)
The next group of corollaries to Theorems 4.20 and 4.21 is connected with
inequalities for the Riccati type quotients, which are partly considered in Sect. 2.4.1.
We begin with the shortest proof of Corollary 2.56. Recall that we assumed in this
corollary (we use the notation of this subsection) that the two conjoined bases Y and
ˆY of (SDS) satisfy
Im ˆXk ⊆Im Xk,
ˆXT
k ( ˆQk −Qk) ˆXk ≥0
(4.58)
for k = 0, i.e., μ(Y0, ˆY0) = 0 according to Theorem 3.2(iv), and Y has no focal point
in (0, N +1], i.e., m(k) = 0 for k ∈[0, N]Z. Then ˆY has no focal point in (0, N +1]
either, and (4.58) holds for all k ∈[0, N + 1]Z. The symmetric matrices Q and ˆQ
solve the equations XT
k QkXk = XT
k Uk and ˆXT
k ˆQk ˆXk = ˆXT
k ˆUk for k ∈[0, N +1]Z.
Proof of Corollary 2.56 We use formula (4.36) and the fact that all four numbers
involved in this identity are nonnegative. Then the equalities m(k) = 0 and
μ(Yk, ˆYk) = 0 are equivalent to ˆm(k) = 0 and μ(Yk+1, ˆYk+1) = 0. Consequently,
the proof follows from the fact that m(k) = 0 and μ(Yk, ˆYk) = 0 imply ˆm(k) = 0
and μ(Yk+1, ˆYk+1) = 0.
⊓⊔
The following result concerning the condition μ(YN+1, ˆYN+1) = 0 can be
proved quite analogically.
Corollary 4.29 Suppose that two conjoined bases Y and ˆY of (SDS) satisfy (4.58)
for k = N + 1 and ˆY does not have any forward focal point in (M, N + 1]. Then
Y has no forward focal point in (M, N + 1] either, and (4.58) holds for all k ∈
[M, N + 1]Z.
Proof See the proof of Corollary 2.56 above, where we showed that the conditions
ˆm(k) = 0, μ(Yk+1, ˆYk+1) = 0 are equivalent to m(k) = 0, μ(Yk, ˆYk) = 0 by
formula (4.36).
⊓⊔
Note that analogical statements can be obtained from (4.39) for the backward
focal points.

222
4
Oscillation Theory of Symplectic Systems
Corollary 4.30 Suppose that
Im Xk ⊆Im ˆXk,
XT
k ( ˆQk −Qk) Xk ≥0
(4.59)
holds for k = N + 1. If ˆY has no backward focal point in [M, N + 1), then Y has
no backward focal in this interval either, and (4.59) holds for all k ∈[M, N + 1]Z.
Proof We use formula (4.39), i.e.,
m∗(k) −ˆm∗(k) = μ∗( ˆYk, Yk).
Then conditions ˆm∗(k) = 0 and μ∗( ˆYk+1, Yk+1) = 0 are equivalent to the
conditions m∗(k) = 0 and μ∗( ˆYk, Yk) = 0. The proof now follows from the fact
that ˆm∗(k) = 0 and μ∗( ˆYk+1, Yk+1) = 0 imply m∗(k) = 0 and μ∗( ˆYk, Yk) = 0.
⊓⊔
Certainly the proof the the previous result also implies a similar statement
connected with the condition (4.59) given at the initial point k = 0. We leave the
formulation of this result to the reader.
4.2.3
Number of Focal Points and Principal Solutions
In this subsection we derive optimal bounds for the numbers of forward and
backward focal points of any conjoined basis Y in (M, N + 1] and [M, N + 1),
respectively. These bounds are optimal in a sense that they are formulated in terms
of quantities, which do not depend on the chosen conjoined basis Y. More precisely,
they are formulated in terms of the numbers of forward and backward focal points
of principal solutions Y [M] and Y [N+1].
Important consequences of formulas (4.36) and (4.37) are related to properties
of the principal solutions of symplectic system (SDS) and its reversed system (2.9).
Recall that the principal solution of (SDS) at k is the solution satisfying the condition
Y [k]
k
= (0 I)T . Note that μ(Yk, Y [k]
k ) = 0 for any conjoined basis Y. In particular,
from (4.40) it follows
l(Y, M, N + 1) −l(Y [M], M, N + 1) = μ(YN+1, Y [M]
N+1),
(4.60)
where Y [M] is the principal solution at k = M. Analogously,
l(Y, M, N + 1) −l(Y [N+1], M, N + 1) = −μ(YM, Y [N+1]
M
),
(4.61)

4.2
Sturmian Separation Theorems and Its Corollaries
223
where Y [N+1] is the principal solution at k = N + 1. As a consequence of (4.60)
and (4.61), we have the estimate
l(Y [M], M, N + 1) ≤l(Y, M, N + 1) ≤l(Y [N+1], M, N + 1),
(4.62)
l(Y [N+1], M, N + 1) = l(Y [M], M, N + 1) + rank X[M]
N+1
(4.63)
for the number of forward focal points of any conjoined basis Y of (SDS) in (0, N +
1]. Similarly, for the number of backward focal points, we have
l∗(Y, M, N + 1) −l∗(Y [M], M, N + 1) = −μ∗(YN+1, Y [M]
N+1),
(4.64)
l∗(Y, M, N + 1) −l∗(Y [N+1], M, N + 1) = μ∗(YM, Y [N+1]
M
),
(4.65)
where μ∗(Y, ˆY ) is the dual comparative index. Then we also have
l∗(Y [N+1], M, N + 1) ≤l∗(Y, M, N + 1) ≤l∗(Y [M], M, N + 1),
(4.66)
l∗(Y [M], M, N + 1) = l∗(Y [N+1], M, N + 1) + rankX[N+1]
M
(4.67)
for the number of backward focal points of any conjoined basis Y of (SDS) in the
interval [M, N + 1).
Remark 4.31 In Theorems 4.34 and 4.35, we will show that the lower bounds in
(4.62) and (4.66) are the same, as well as the upper bounds in (4.62) and (4.66).
Moreover, these lower and upper bounds are independent on the conjoined basis
Y. Since these bounds are attained for the speciﬁc choices of Y := Y [M] and Y :=
Y [N+1], the inequalities in (4.62) and (4.66) are universal and cannot be improved—
in the sense that the estimates (4.62) and (4.66) are satisﬁed for all conjoined bases
Y of (SDS).
In (4.60) and (4.65), we derived the exact formulas
l(Y, M, N + 1) = l(Y [M], M, N + 1) + μ(YN+1, Y [M]
N+1),
(4.68)
l∗(Y, M, N + 1) = l∗(Y [N+1], M, N + 1) + μ(YM, Y [N+1]
M
),
(4.69)
which show how to calculate the number of forward or backward focal points of
an arbitrary conjoined basis Y of (SDS) as a sum of a quantity which does not
depend on Y and the comparative index of Y with Y [M] at k = N + 1 or the dual
comparative index of Y with Y [N+1] at k = M. For practical purposes, e.g., in the
oscillation theory on unbounded intervals, it is convenient to have estimates for the
numbers l(Y, M, N + 1) and l∗(Y, M, N + 1), which do not explicitly involve the
possible complicated evaluation of the comparative index. In Theorem 4.32 below,
we present such estimates of l(Y, M, N + 1) and l∗(Y, M, N + 1). At the same
time, we show that the universal lower and upper bounds for l(Y, M, N + 1) and

224
4
Oscillation Theory of Symplectic Systems
l∗(Y, M, N + 1) in (4.62) and (4.66) can be improved for some particular choice of
the conjoined basis Y.
Theorem 4.32 For any conjoined basis Y of (SDS), we have the inequalities
rN ≤l(Y, M, N + 1) −l(Y [M], M, N + 1) ≤RN,
r∗
N ≤l∗(Y, M, N + 1) −l∗(Y [N+1], M, N + 1) ≤R∗
N,
where
rN = max{0, rankX[M]
N+1 −rankXN+1, rankXM −rank XN+1},
(4.70)
RN = min{rankXM, rank X[M]
N+1, rankX[M]
N+1 −rank XN+1 + rankXM},
(4.71)
r∗
N = max{0, rankX[N+1]
M
−rank XM, rank XN+1 −rankXM},
(4.72)
R∗
N = min{rankXN+1, rankX[N+1]
M
, rank XN+1 −rank XM + rankX[N+1]
M
}.
(4.73)
Moreover, we have
rN + RN = rank X[M]
N+1 −rank XN+1 + rankXM,
(4.74)
r∗
N + R∗
N = rank X[N+1]
M
+ rankXN+1 −rank XM.
(4.75)
Proof The values of the lower and upper bounds in (4.70), (4.71), and equality
(4.74) follow from (4.60) and Remark 3.10(i) (see (3.23), (3.24)), in which we take
Y := YN+1, ˆY := Y [M]
N+1, and w(Y, ˆY ) = XT
M being the Wronskian of Y and Y [M]. In
a similar way we obtain (4.72), (4.73), and (4.75) from (4.65) and Remark 3.10(i)
for the dual comparative index, in which we take Y := YM, ˆY := Y [N+1]
M
, and
w(Y, ˆY ) = XT
N+1 being the Wronskian of Y and Y [N+1].
⊓⊔
Now we present a discrete version of the second part of Theorem 1.44. This result
is concerned with the numbers of forward focal points of the principal solution Y [M]
in (M, N + 1] and the number of backward focal points of the principal solution
Y [N+1] in [M, N + 1). We begin with the result in a more general form.
Lemma 4.33 Let Y and ˆY be conjoined bases of (SDS) and Z and ˆZ be symplectic
fundamental matrices of (SDS), such that Y = Z(0 I)T and ˆY = ˆZ(0 I)T . Then
the number m(k) of forward focal points of Y and the number ˆm∗(k) of backward
focal points of ˆY are connected by the formula
ˆm∗(k) −m(k) = μ( ˆZ−1
k Yk, ˆZ−1
k (0 I)T ),
(4.76)

4.2
Sturmian Separation Theorems and Its Corollaries
225
where
μ( ˆZ−1
k Yk, ˆZ−1
k (0 I)T ) = μ∗(Z−1
k
ˆYk, Z−1
k (0 I)T ).
(4.77)
Moreover,
l∗( ˆY, M, N + 1) −l(Y, M, N + 1)
= μ( ˆZ−1
N+1YN+1, ˆZ−1
N+1(0 I)T ) −μ( ˆZ−1
M YM, ˆZ−1
M (0 I)T ),
(4.78)
and
μ( ˆZ−1
N+1YN+1, ˆZ−1
N+1(0 I)T ) −μ( ˆZ−1
M YM, ˆZ−1
M (0 I)T )
= μ(YM, Y [N+1]
M
) −μ∗( ˆYN+1, Y [M]
N+1)
(4.79)
= μ∗( ˆYM, Y [N+1]
M
) −μ(YN+1, Y [M]
N+1),
(4.80)
where Y [M] and Y [N+1] are the principal solutions of (SDS) at M and N + 1,
respectively.
Proof The proof is based on Theorem 4.20 and formula (4.9). Substituting the value
ˆm(k) = ˆm∗(k) − rank ˆXk into (4.36), we have
m(k) −ˆm∗(k) = μ(Yk, ˆYk) − rank ˆXk.
(4.81)
Then we replace μ(Yk, ˆYk) −rank ˆXk by μ(Yk, ˆZk(0 I)T ) −μ((0 I)T , ˆZk(0 I)T )
on the right-hand side of the last identity and derive (4.76) by application of property
(ix) of Theorem 3.5. Formula (4.77) is based on the application of property (iii) of
Theorem 3.5; see also (3.33). Summing (4.76) from k = M to k = N, we derive
(4.78). Formulas (4.80) and (4.79) follow from (4.41) and (4.42) by subtracting
the substitution rank ˆXk|N+1
M
from both sides according to (4.81), and then using
formula (3.22) applied to μ( ˆYN+1, Y [M]
N+1) in (4.41) and μ( ˆYM, Y [N+1]
M
) in (4.42),
respectively.
⊓⊔
From Lemma 4.33 we derive the main results of this subsection.
Theorem 4.34 Let l(Y [M], M, N + 1) be the number of forward focal points of
Y [M] in (M, N + 1] and l∗(Y [N+1], M, N + 1) be the number of backward focal
points of Y [N+1] in [M, N + 1). Then
l(Y [M], M, N + 1) = l∗(Y [N+1], M, N + 1).
(4.82)
Proof Putting Y := Y [M] and ˆY := Y [N+1] in (4.78), we see that the substitution
on the right-hand side of (4.78) equals to zero.
⊓⊔

226
4
Oscillation Theory of Symplectic Systems
Similarly we prove the next statement, which illustrates the duality principle in
the comparative index theory.
Theorem 4.35 Let l∗(Y [M], M, N + 1) be the number of backward focal points of
Y [M] in [M, N +1), and l(Y [N+1], M, N +1) be the number of forward focal points
of Y [N+1] in (0, N + 1]. Then
l∗(Y [M], M, N + 1) = l(Y [N+1], M, N + 1).
(4.83)
Proof Putting ˆY := Y [M] and Y := Y [N+1] in (4.78), we see that the substitution
on the right-hand side of (4.78) equals to zero.
⊓⊔
The next group of corollaries to Lemma 4.33 is connected with estimates for the
difference l∗( ˆY, M, N + 1) −l(Y, M, N + 1), which we derive by analogy with
a similar results in Sect. 4.2.2.
Lemma 4.36 We have the following estimates
		 l(Y, M, N + 1) −l∗( ˆY, M, N + 1)
		
≤max
!
min{rankXM, rank ˆXM}, min{rankXN+1, rank ˆXN+1}
"
≤n.
(4.84)
and
		 l(Y, M, N + 1) −l∗( ˆY, M, N + 1)
		 ≤rankX[N+1]
M
= rankX[M]
N+1 ≤n
(4.85)
In particular, for one conjoined basis Y of (SDS), we have
		 l(Y, M, N + 1) −l∗(Y, M, N + 1)
		
≤min
!
max{rankXM, rankXN+1}, rankX[M]
N+1
"
≤n.
(4.86)
Proof Estimate (4.84) follows from (4.78) and property (vii) in Theorem 3.5.
Indeed, by (4.78) we have
		 l∗( ˆY, M, N + 1) −l(Y, M, N + 1)
		
≤max{μ( ˆZ−1
N+1YN+1, ˆZ−1
N+1(0 I)T ), μ( ˆZ−1
M YM, ˆZ−1
M (0 I)T )},
then it is sufﬁcient to apply Theorem 3.5(vii), taking into account that
rankw( ˆZ−1
k Yk, ˆZ−1
k (0 I)T ) = rank Xk.

4.2
Sturmian Separation Theorems and Its Corollaries
227
Note also that according to distributivity of the operation max with respect to min
we have that the right-hand side in (4.84) less than or equal to each of the num-
bers max{rankXl, rank Xp}, max{rankXl, rank ˆXp}, and max{rank ˆXl, rank ˆXp}
for p, l ∈{M, N + 1}, p ̸= l.
Estimate (4.85) follows from (4.79) using the inequality
		 l∗( ˆY, M, N + 1) −l(Y, M, N + 1)
		
≤max{μ(YM, Y [N+1]
M
), μ∗( ˆYN+1, Y [M]
N+1)} ≤rankX[N+1]
M
= rankX[M]
N+1
derived by Theorem 3.5 (vii).
⊓⊔
4.2.4
Separation Results on Singular Intervals
In this subsection we apply the separation results of the previous sections to the case
of symplectic systems (SDS), which are nonoscillatory near ±∞.
Deﬁnition 4.37 System (SDS) is said to be nonoscillatory at ∞if there exists M ∈
N such that the principal solution of (SDS) at M, i.e., the conjoined basis Y [M]
given by the initial condition Y [M]
M
= (0 I)T , has no forward focal points in the
interval (M, ∞). Similarly, (SDS) is nonoscillatory at −∞if there exists M such the
principal solution at M has no backward focal points in (−∞, M). In the opposite
case system, (SDS) is said to be oscillatory at ∞, resp., at −∞.
An equivalent deﬁnition of the (non)oscillation of system (SDS) at ±∞in
terms of arbitrary conjoined bases of (SDS) is presented in Sect. 6.3.2 (see
Proposition 6.61).
Lemma 4.38 Assume that the system (SDS) is nonoscillatory at ∞. Then for any
conjoined basis Y of (SDS) and for arbitrary M ∈Z, there exist the ﬁnite limits
l(Y, M, ∞) =
∞

k=M
m(k),
l∗(Y, M, ∞) =
∞

k=M
m∗(k),
lim
k→∞rank Xk
⎫
⎪⎪⎬
⎪⎪⎭
(4.87)
for the numbers of forward focal points in (M, ∞) and backward focal points in
[M, ∞) connected by the equality
l∗(Y, M, ∞) −l(Y, M, ∞) = lim
k→∞rankXk −rankXM.
(4.88)

228
4
Oscillation Theory of Symplectic Systems
Similarly, if the system (SDS) is nonoscillatory at −∞, then for any conjoined basis
Y of (SDS) and for any N ∈Z, there exist the ﬁnite limits
l(Y, −∞, N + 1) =
N

k=−∞
m(k),
l∗(Y, −∞, N + 1) =
N

k=−∞
m∗(k),
lim
k→−∞rankXk
⎫
⎪⎪⎬
⎪⎪⎭
(4.89)
for the numbers of forward focal points of Y in (−∞, N + 1] and backward focal
points in (−∞, N + 1), and
l∗(Y, −∞, N + 1) −l(Y, −∞, N + 1) = rank XN+1 −
lim
k→−∞rankXk.
(4.90)
Proof Note that by (4.60)
0 ≤l(Y, M, N + 1) −l(Y [M], M, N + 1) = μ(YN+1, Y [M]
N+1) ≤n
for arbitrary M ∈Z, then the ﬁrst ﬁnite limit in (4.87) does exist for any conjoined
basis Y of nonoscillatory system (SDS). Next, according to (4.13)
|l∗(Y, M, N + 1) −l(Y, M, N + 1)| ≤max(rank XN+1, rank XM) ≤n,
then the second limit in (4.87) exists as well. The existence of the limit of rank Xk
as k →∞can be proved directly using the kernel condition Ker Xk+1 ⊆Ker Xk
in (2.37), which implies 0 ≤rankXk ≤rank Xk+1 ≤n and then rank Xk is
bounded nondecreasing function of the index k. However the existence of the limit
limk→∞rankXk also follows from identity (4.12), which says that the existence of
the ﬁrst two limits in (4.87) implies that the third one does exist as well, and then
taking the limit in the above identity as N →∞we derive (4.88). The proof of the
second claim of Lemma 4.38 is similar.
⊓⊔
Next we present a singular version of Theorems 4.23 and 4.24.
Theorem 4.39 (Singular Separation Theorem)
Assume that (SDS) is nonoscil-
latory at ∞. Then for any two conjoined bases Y and ˆY of this system, there exists
the ﬁnite limit of the comparative index
μ∞(Y, ˆY ) := lim
k→∞μ(Yk, ˆYk)
(4.91)
connecting the numbers of forward focal points of these conjoined bases in (M, ∞)
l(Y, M, ∞) −l( ˆY, M, ∞) = μ∞(Y, ˆY ) −μ(YM, ˆYM).
(4.92)

4.3
Comparison Theorems and Its Corollaries
229
Similarly, there exists the ﬁnite limit of the dual comparative index
μ∗
∞(Y, ˆY ) := lim
k→∞μ∗(Yk, ˆYk)
(4.93)
such that the numbers of backward focal points in [M, ∞) are connected by the
identity
l∗(Y, M, ∞) −l∗( ˆY, M, ∞) = μ∗(YM, ˆYM) −μ∗
∞(Y, ˆY ).
(4.94)
Proof The proof follows immediately from Lemma 4.38 and from Theorems 4.23
and 4.24, where we evaluate the limits of the summands in (4.40) and (4.43) as N
tends to ∞.
⊓⊔
We note that a similar theorem can be formulated for the case when system (SDS)
is nonoscillatory at −∞.
Remark 4.40 Assume that system (SDS) is nonoscillatory at ∞, then for arbitrary
conjoined bases of this system, we have the following connections for the ﬁnite
limits (4.91) and (4.93):
μ∞(Y, ˆY ) + μ∞( ˆY, Y) = rankw(Yk, ˆYk),
(4.95)
μ∗
∞(Y, ˆY ) + μ∗
∞( ˆY, Y) = rankw(Yk, ˆYk),
(4.96)
μ∞(Y, ˆY ) = lim
k→∞rank ˆXk −lim
k→∞rankXk + μ∗
∞( ˆY, Y).
(4.97)
Indeed, all limits in (4.95)–(4.97) exist by Lemma 4.38 and Theorem 4.39, and
then we derive identities (4.95)–(4.97) using the properties of the comparative index
according to Theorem 3.5(v), (vi) by taking limits as k →∞.
Remark 4.41 Further singular separation theorems on unbounded intervals, e.g., of
the form [M, ∞)Z as in this subsection, will be presented in Sect. 6.4. They are
based on comparison with the numbers of focal points of the principal solution
Y [M] and the (minimal) recessive solution Y [∞] of (SDS) at ∞from Sect. 6.3. As
we shall see, the latter approach also allows to evaluate the limits of the comparative
indices μ∞(Y, ˆY ) and μ∗
∞(Y, ˆY ) from Remark 4.40 explicitly; see Corollaries 6.174
and 6.186.
4.3
Comparison Theorems and Its Corollaries
In this section we consider together with (SDS) another system of the same form
(which we write now in the 2n × n matrix form)
ˆYk+1 = ˆSk ˆYk,
ˆST
k J ˆSk = J .
(4.98)

230
4
Oscillation Theory of Symplectic Systems
If Y and ˆY are conjoined bases of (SDS) and (4.98), respectively, then we can
introduce the Wronskian of Y and ˆY using notation (3.2) as
w(Yk, ˆYk) := Y T
k J ˆYk,
but, generally speaking, the Wronskian identity (2.4) is lost due to the formula
w( ˆYk, Yk) = ˆY T
k ( ˆST
k J Sk −J ) Yk = ˆY T
k+1(J −ˆST −1
k
J S−1
k ) Yk+1.
(4.99)
Note also that by the properties of symplectic matrices (see Lemma 1.58(ii)), for
any symplectic fundamental matrices Z and ˆZ of (SDS) and (4.98) such that Y =
Z(0 I)T , ˆY = ˆZ(0 I)T , we have the important connection
w( ˆYk, Yk) = ˆY T
k J Yk = −(I 0) ˆZ−1
k Zk(0 I)T = −(I 0) ˆZ−1
k Yk
(4.100)
between the matrix ˆZ−1
k Yk and the Wronskian (3.2) of two conjoined bases ˆY and
Y of systems (4.98) and (SDS). This fact will be used in the subsequent results. For
example, identity (4.99) is a direct consequence of the relation
( ˆZ−1
k Zk) = ˆZ−1
k+1Zk+1 −ˆZ−1
k Zk = ˆZ−1
k ( ˆS−1
k Sk −I) Zk
= ˆZ−1
k+1(I −ˆSkS−1
k ) Zk+1.
(4.101)
The results of this section show that there exists a deep connection between numbers
of focal points of conjoined bases of (SDS), (4.98), and ˜Yk = ˆZ−1
k Yk.
4.3.1
Sturmian Comparison Theorems
We start with some auxiliary statements. The ﬁrst one is based on Lemma 3.23.
Lemma 4.42 Let Z and ˆZ be symplectic fundamental matrices of (SDS) and
(4.98), respectively, Y = Z(0 I)T , ˆY = ˆZ(0 I)T , and
L(Yk, ˆYk, Sk, ˆSk) := ˆm(k) −m(k) + μ(Yk, ˆYk).
(4.102)
Then
L(Yk, ˆYk, Sk, ˆSk)
= μ

Sk(0 I)T, ˆSk(0 I)T 
+ μ
 ˆS−1
k SkYk, Yk

−μ
 ˆZ−1
k+1Zk+1(0 I)T, ˆZ−1
k Zk(0 I)T 
−μ
 ˆS−1
k SkYk, ˆS−1
k Sk(0 I)T 
,
(4.103)

4.3
Comparison Theorems and Its Corollaries
231
or
L(Yk, ˆYk, Sk, ˆSk)
= μSk(0 I)T, Sk ˆS−1
k (0 I)T  + μ ˆZ−1
k Zk(0 I)T, ˆZ−1
k+1Zk+1(0 I)T 
−μ
 ˆSkS−1
k Yk+1, Yk+1

−μ

Yk+1, Sk ˆS−1
k (0 I)T 
.
(4.104)
Proof We put Z := Zk, ˆZ := ˆZk, W := Sk, ˆW := ˆSk in Lemma 3.23. Since
Zk+1 = SkZk, ˆZk+1 =
ˆSk ˆZk, using the formula for the relationship between
the number of focal points and the comparative index in Lemma 4.7, we have the
required statement.
⊓⊔
Now we turn our attention to two particular cases of (4.103) and (4.104), which
will be important in applications.
Lemma 4.43 Suppose that
ˆS−1
k Sk =
 I
0
Qk I

(4.105)
holds. Then
L(Yk, ˆYk, Sk, ˆSk)
= ind(−XT
k QkXk) −μ ˆZ−1
k+1Zk+1(0 I)T, ˆZ−1
k Zk(0 I)T 
= μ
 ˆZ−1
k Zk(0 I)T, ˆZ−1
k+1Zk+1(0 I)T 
−ind(XT
k QkXk).
(4.106)
Similarly, if
Sk ˆS−1
k
=
 I
0
Rk I

,
(4.107)
then
L(Yk, ˆYk, Sk, ˆSk)
= μ ˆZ−1
k Zk(0 I)T , ˆZ−1
k+1Zk+1(0 I)T  −ind(XT
k+1RkXk+1)
(4.108)
= ind(−XT
k+1RkXk+1) −μ
 ˆZ−1
k+1Zk+1(0 I)T , ˆZ−1
k Zk(0 I)T 
.

232
4
Oscillation Theory of Symplectic Systems
Proof Recall that the matrices Qk and Rk in (4.105) and (4.107) are symmetric by
(1.153). If (4.105) holds, then obviously by (iii) of Theorem 3.5, we have
μ

Sk(0 I)T, ˆSk(0 I)T 
= μ∗
S−1
k (0 I)T, S−1
k
ˆSk(0 I)T 
= μ∗
S−1
k (0 I)T, (0 I)T 
= 0,
μ
 ˆS−1
k SkYk, ˆS−1
k Sk(0 I)T 
= μ
 ˆS−1
k SkYk, (0 I)T 
= 0,
μ( ˆS−1
k SkYk, Yk) = μ
 
Xk
QkXk + Uk

,
Xk
Uk
 
= ind (−XT
k QkXk).
Substituting these formulas for comparative indices into the right-hand side of
(4.103), we prove the ﬁrst part of (4.106). Note that by Theorem 3.5(v), we obtain
μ ˆS−1
k SkYk, Yk
 = μ(0 I)T , Z−1
k S−1
k
ˆSkYk
 −μYk, ˆS−1
k SkYk
,
and analogously
μ
 ˆZ−1
k+1Zk+1(0 I)T , ˆZ−1
k Zk(0 I)T 
= μ

(0 I)T , Z−1
k S−1
k
ˆSkYk

−μ
 ˆZ−1
k Zk(0 I)T , ˆZ−1
k+1Zk+1(0 I)T 
.
Consequently,
μ
 ˆS−1
k SkYk, Yk

−μ
 ˆZ−1
k+1Zk+1(0 I)T , ˆZ−1
k Zk(0 I)T 
= μ
 ˆZ−1
k Zk(0 I)T , ˆZ−1
k+1Zk+1(0 I)T 
−μ

Yk, ˆS−1
k SkYk

,
where μ(Yk, ˆS−1
k SkYk) = ind(XT
k QkXk). This proves (4.106). Formula (4.108) can
be proved analogously using (4.104).
⊓⊔
The representation for the operator L in (4.102) of Lemma 4.42 can be simpliﬁed
if we apply Theorem 3.24 to this operator. Recall that in the previous chapter, we
have introduced notation (3.52) for a symplectic matrix W, i.e.,
⟨W⟩=
⎛
⎜⎜⎝
I
0
A B
0 −I
C D
⎞
⎟⎟⎠,
W =
A B
C D

.
Using notation (3.52) we introduce the notion of the relative oscillation numbers.

4.3
Comparison Theorems and Its Corollaries
233
Deﬁnition 4.44 For the pair of symplectic difference systems (SDS) and (4.98)
and their symplectic fundamental matrices Z and ˆZ, respectively, we deﬁne the
relative oscillation number #k( ˆZ, Z) at index k of these fundamental matrices by
the formula
#k( ˆZ, Z) := μ

⟨Sk⟩, ⟨ˆSk⟩

−μ

⟨ˆZ−1
k+1Zk+1⟩, ⟨ˆZ−1
k Zk⟩

= μ

⟨ˆZ−1
k Zk⟩, ⟨ˆZ−1
k+1Zk+1⟩

−μ

⟨ˆSk⟩, ⟨Sk⟩


(4.109)
and the relative oscillation number of Z and ˆZ in the interval [M, N] by the formula
#( ˆZ, Z, M, N) :=
N

k=M
#k( ˆZ, Z).
(4.110)
Now we formulate the Sturmian comparison theorem for the pair of symplectic
difference systems (SDS), (4.98).
Theorem 4.45 (Sturmian Comparison Theorem) Let Z and ˆZ be symplectic
fundamental matrices of (SDS) and (4.98), respectively, and Y = Z(0 I)T and
ˆY = ˆZ(0 I)T . Then the operator L from (4.102) can be expressed as
L(Yk, ˆYk, Sk, ˆSk) = #k( ˆZ, Z),
(4.111)
where #k( ˆZ, Z) is the relative oscillation number given by (4.109).
Proof The proof is similar to that of Lemma 4.42. We put Z = Zk, ˆZ = ˆZk,
W = Sk, ˆW = ˆSk in Theorem 3.24. Then Zk+1 = SkZk, ˆZk+1 = ˆSk ˆZk, and using
Lemma 4.7, we obtain the proof of (4.111) with (4.109).
⊓⊔
Remark 4.46 Consider the meaning of the addends in the deﬁnition of the relative
oscillation numbers in details.
(i) The comparative index
μ(⟨ˆZ−1
k+1Zk+1⟩, ⟨ˆZ−1
k Zk⟩) = μ∗(⟨Z−1
k+1 ˆZk+1⟩, ⟨Z−1
k
ˆZk⟩)
in the ﬁrst equality of (4.109) describes the multiplicity of a forward focal
point of a conjoined basis of a symplectic 4n × 4n system associated with
(SDS) and (4.98). Indeed, by (3.101)–(3.104), the 4n × 2n matrix Yk =
S−1
0 { ˆZ−1
k Zk}(02n I2n)T = Zk(02n I2n)T is a conjoined basis of the symplectic
system
Yk+1 = S−1
0 { ˆZ−1
k
ˆS−1
k Sk ˆZk}S0 Yk.
(4.112)

234
4
Oscillation Theory of Symplectic Systems
Then, according to (4.14) in Lemma 4.7 and by (3.104), the multiplicity m(Yk)
of a forward focal point of this basis in (k, k + 1] is given by the comparative
index
μ∗(Z−1
k+1(02n I2n)T , Z−1
k (02n I2n)T ) = μ∗(⟨Z−1
k+1 ˆZk+1⟩, ⟨Z−1
k
ˆZk⟩).
(ii) Applying Lemma 3.21(v) to the comparative index μ(⟨ˆZ−1
k+1Zk+1⟩, ⟨ˆZ−1
k Zk⟩)
in the ﬁrst equation of (4.109), we have the representation
μ(⟨ˆZ−1
k+1Zk+1⟩, ⟨ˆZ−1
k Zk⟩)
= μ∗(Z−1
k+1 ˆZk+1(0 I)T , Z−1
k
ˆZk(0 I)T ) + μ(⟨ˆZ−1
k
ˆS−1
k Sk ˆZk⟩, ⟨I⟩),
(4.113)
where the ﬁrst addend is the multiplicity of a forward focal point of the
conjoined basis ˜Yk = ˆZ−1
k Yk of the symplectic system
˜Yk+1 = ˆZ−1
k
ˆS−1
k Sk ˆZk ˜Yk
(4.114)
by (4.14) in Lemma 4.7. Recall that the upper block of ˜Yk is associated with
the Wronskian of Yk and ˆYk; see (4.100).
(iii) Similarly, the comparative index μ(⟨ˆZ−1
k Zk⟩, ⟨ˆZ−1
k+1Zk+1⟩) in the second
formula of (4.109) represents the multiplicity m∗(Yk) of a backward focal point
in the interval [k, k+1) of the conjoined basis Yk = S−1
0 {Z−1
k
ˆZk}(02n I2n)T =
Zk(02n I2n)T of the symplectic system
Yk+1 = S−1
0 {Z−1
k S−1
k
ˆSkZk}S0Yk,
(4.115)
according to (4.16) in Lemma 4.8. The ﬁrst addend in the formula
μ(⟨ˆZ−1
k Zk⟩, ⟨ˆZ−1
k+1Zk+1⟩)
= μ( ˆZ−1
k Zk(0 I)T , ˆZ−1
k+1Zk+1(0 I)T ) + μ(⟨Z−1
k S−1
k
ˆSkZk⟩, ⟨I⟩),
(4.116)
derived by Lemma 3.21(v) is the multiplicity of a backward focal point of the
conjoined basis ˜Yk = Z−1
k
ˆYk of the symplectic system
˜Yk+1 = Z−1
k S−1
k
ˆSkZk ˜Yk.
(4.117)
Using the results of Sects. 3.3 and 3.3.5, we can prove the following properties
of the relative oscillation numbers.

4.3
Comparison Theorems and Its Corollaries
235
Lemma 4.47 Let Z and ˆZ be symplectic fundamental matrices of (SDS) and
(4.98), respectively. Then we have the following properties for their relative
oscillation numbers:
(i) If Y = Z(0 I)T and ˆYi = ˆZ(0 I)T , then
#k( ˆZ, Z) + #k(Z, ˆZ) = rankw(Yk, ˆYk),
w(Yk, ˆYk) = Y T
k J ˆYk,
(4.118)
(ii) We have the estimate
|#k( ˆZ, Z)| ≤rank(Sk −ˆSk).
(4.119)
(iii) Let Z and  be symplectic fundamental matrices of (SDS), and let ˆZ and ˆ
be symplectic fundamental matrices of (4.98). Then we have the formula
#k( ˆZ, Z) −#k( ˆ, ) = μ

⟨ˆ−1
k k⟩, {W, V }(02n, I2n)T 
,
W = −1
k Zk,
V = ˆ−1
k
ˆZk.
(4.120)
Consequently
#( ˆZ, Z, M, N) −#( ˆ, , M, N)
= μ

⟨ˆ−1
k k⟩, {W, V }(02n, I2n)T 			
N+1
M
,
(4.121)
where {W, V } is deﬁned by (3.101).
(iv) If W and V are lower block-triangular matrices, then
#k( ˆZ, Z, M, N) = #k(, ˆ, M, N).
Proof
(i) Using (4.109) and Lemma 3.21 (iii), (iv) we obtain
#k(Z, ˆZ) = μ(⟨Z−1
k
ˆZk⟩, ⟨Z−1
k+1 ˆZk+1⟩) −μ(⟨Sk⟩, ⟨ˆSk⟩)
= μ(⟨ˆSk⟩, ⟨Sk⟩) −μ(⟨Z−1
k+1 ˆZk+1⟩, ⟨Z−1
k
ˆZk⟩)
= μ(⟨ˆSk⟩, ⟨Sk⟩) −μ∗(⟨ˆZ−1
k+1Zk+1⟩, ⟨ˆZ−1
k Zk⟩)
= μ(⟨ˆSk⟩, ⟨Sk⟩) −μ(⟨ˆZ−1
k Zk⟩, ⟨ˆZ−1
k+1Zk+1⟩) + rankw(Yk, ˆYk),
where in the last equality, we have also incorporated equation (4.100)
when applying Lemma 3.21(iv). Consequently, #k(Z, ˆZ) = −#k( ˆZ, Z) +
 rankw(Yk, ˆYk) holds, which is the same as (4.118).

236
4
Oscillation Theory of Symplectic Systems
(ii) The proof follows from Lemma 3.21 (ii) (see (3.53)) and equation (4.101),
which implies
μ(⟨ˆZ−1
k Zk⟩, ⟨ˆZ−1
k+1Zk+1⟩) ≤rank( ˆZ−1
k Zk) = rank(Sk −ˆSk)
and
μ(⟨ˆSk⟩, ⟨Sk⟩) ≤rank(Sk −ˆSk).
(iii) Note that the matrix W in (4.120) is constant for two fundamental matrices
Z,  of the same system (SDS) (similarly for V and ˆZ and ˆ), then −1
l
l =
V ˆZ−1
l
ZlW −1, l = k, k + 1. Using Lemma 3.39 we obtain
μ

⟨ˆ−1
k k⟩, ⟨ˆ−1
k+1k+1⟩

= μ⟨ˆZ−1
k Zk⟩, ⟨ˆZ−1
k+1Zk+1⟩ −μ⟨ˆ−1
k k⟩, {W, V }(02n, I2n)T .
Consequently,
μ

⟨ˆZ−1
k Zk⟩, ⟨ˆZ−1
k+1Zk+1⟩

−μ

⟨ˆ−1
k k⟩, ⟨ˆ−1
k+1k+1⟩

= μ

⟨ˆ−1
k k⟩, {W, V }(02n, I2n)T 
.
Hence the last formula implies (4.120) and its summation from k = M to
k = N yields (4.121).
(iv) The proof follows from the elementary fact that if W and V are symplectic
lower block-triangular matrices, then {W, V }(0 I)T deﬁned by (3.101) has
upper block equal to zero, and hence, we have μ(⟨ˆ−1
i i⟩, {W, V }(02n, I2n)T )
= 0.
⊓⊔
Note that part (iv) of the previous lemma shows that the results based on
Theorem 4.45 depend really only on the second block column of the considered
fundamental matrices, while the ﬁrst matrix column (which is not determined by
the second matrix column uniquely) is disregarded.
Remark 4.48 Note also that the operator L in (4.102), in view of properties (v) and
(vi) of Theorem 3.5, can be expressed in various forms as
L(Yk, ˆYk, Sk, ˆSk) = ˆm(k) −m(k) +  rankw(Yk, ˆYk) −μ( ˆYk, Yk),
(4.122)
L(Yk, ˆYk, Sk, ˆSk) = ˆm∗(k) −m∗(k) + μ∗( ˆYk, Yk),
(4.123)
L(Yk, ˆYk, Wk, ˆWk) = ˆm∗(k) −m∗(k) +  rankw(Yk, ˆYk) −μ∗(Yk, ˆYk).
(4.124)

4.3
Comparison Theorems and Its Corollaries
237
Using property (4.118) of the numbers #k( ˆZ, Z), we obtain from (4.122) a general-
ization of formula (4.38) for the case of conjoined bases of two different systems
ˆm(k) −m(k) −μ( ˆYk, Yk) = −#k(Z, ˆZ),
(4.125)
Analogously, from (4.123) it follows a generalization of (4.39):
ˆm∗(k) −m∗(k) + μ∗( ˆYk, Yk) = #k( ˆZ, Z).
(4.126)
Finally, from (4.124) it follows the generalization of (4.37)
ˆm∗(k) −m∗(k) −μ∗(Yk, ˆYk) = −#k(Z, ˆZ).
(4.127)
Now we formulate a consequence of Theorem 4.45. We use there the concept of
the number of (forward) focal points of the conjoined bases of (SDS) in (M, N + 1]
deﬁned by (4.10) and
l( ˆY, M, N + 1) =
N

i=M
ˆm(i).
(4.128)
The next theorem extends formula (4.40) in Theorem 4.23 and Corollary 4.25 to the
case when Y and ˆY are conjoined bases of two different symplectic systems.
Theorem 4.49 Suppose that Y and ˆY are conjoined bases of (SDS) and (4.98),
respectively, their numbers of focal points in (M, N + 1] are deﬁned by (4.10),
(4.128). Further suppose that the relative oscillation number in [M, N] of symplec-
tic fundamental matrices Z and ˆZ is given by (4.110) and these matrices are related
to Y and ˆY by the relations Y = Z (0 I)T and ˆY = ˆZ (0 I)T . Then
l(Y, M, N + 1) −l( ˆY, M, N + 1) + #( ˆZ, Z, M, N)
= μ(YN+1, ˆYN+1) −μ(YM, ˆYM),
(4.129)
in particular,
		l(Y, M, N + 1) −l( ˆY, M, N + 1) + #( ˆZ, Z, M, N)
		
≤min !ˆrM,N+1, ¯rM,N+1
" ≤n,
(4.130)
where ˆrM,N+1 is given by (4.47),
¯rM,N+1 = max{rankw(YM, ˆYM), rankw(YN+1, ˆYN+1)},
and n is dimension of blocks of Sk in (SDS).

238
4
Oscillation Theory of Symplectic Systems
Proof The summation of (4.111) from k = M to k = N gives (4.129). The estimate
(4.130) is derived by analogy with the proof of (4.46) in Corollary 4.25, where we
cannot use that the Wronskian w(Y, ˆY ) is constant.
⊓⊔
The next theorems extend Corollaries 2.56 and 4.30 to the case of two symplectic
systems (SDS) and (4.98). The ﬁrst result is a discrete counterpart of Theorem 1.48
(Riccati inequality) and generalizes Theorem 2.54 to two discrete Hamiltonian
systems and Theorem 2.55 to two symplectic systems.
Theorem 4.50 Suppose that conjoined bases Y =
X
U

and ˆY =
 ˆX
ˆU

of (SDS)
and (4.98) satisfy (4.58) for k = M, the relative oscillation number (4.109) for
symplectic fundamental matrices Z and ˆZ of these systems associated with Y and
ˆY such that Y = Z (0 I)T and ˆY = ˆZ (0 I)T satisﬁes
#k( ˆZ, Z) ≤0,
k ∈[M, N]Z,
(4.131)
and that Y has no forward focal points in (M, N + 1]. Then ˆY has no forward focal
points in this interval either,
#( ˆZ, Z, M, N) = 0,
(4.132)
and condition (4.58) holds for all k ∈[M, N + 1]Z.
Proof Rewrite formula (4.111) in the form
ˆm(k) + μ(Yk+1, ˆYk+1) + (−#k( ˆZ, Z)) = μ(Yk, ˆYk) + m(k).
Because all summands in the left-hand side are nonnegative, the fact that the right-
hand side equals zero by assumptions implies that all three summands also equal
zero for all indices k ∈[M, N]Z.
⊓⊔
Remark 4.51
(i) The comparative indices for the symplectic coefﬁcient matrices of (SDS) and
(4.98) in (4.109) are traditionally assumed to be zero in the classical oscillation
theory. In this case we will call the conditions
μ⟨Sk⟩, ⟨ˆSk⟩ = 0
(4.133)
and
μ

⟨ˆSk⟩, ⟨Sk⟩

= 0
(4.134)
as the (Sturmian) majorant conditions for the pair of symplectic systems (SDS)
and (4.98). These conditions can be rewritten in terms of the blocks of Sk and
ˆSk according to the results in Sect. 3.3.2. For example, the majorant condition

4.3
Comparison Theorems and Its Corollaries
239
(4.134) is equivalent to the conditions
Im (Ak −ˆAk,
Bk) ⊆Im ˆBk,

I AT
k
0 BT
k


Q⟨Sk⟩−Q⟨ˆSk⟩
  I
0
Ak Bk

,
⎫
⎪⎬
⎪⎭
(4.135)
where the symmetric matrix Q⟨Sk⟩is deﬁned by (3.71) with W := Sk and
Q⟨ˆSk⟩is deﬁned similarly for system (4.98) (see Lemma 3.25). In particular,
these matrices can be taken in form (3.74), which is equivalent to (2.64), and
one can replace the second condition in (4.135) by the more strict majorant
condition
Gk −ˆGk ≥0,
(4.136)
where Gk is deﬁned by (2.64) and ˆGk is deﬁned similarly for system (4.98).
Note that (4.136) is sufﬁcient for the second condition in (4.135).
(ii) Consider linear Hamiltonian difference system (2.15) and another system of
the same form with the Hamiltonian ˆH =

−ˆC ˆAT
ˆA
ˆB

. Then using Example 3.34
and formula (3.93), we see that (4.133) is equivalent to the conditions
Ker Bk ⊆Ker ˆBk,
ˆBk ≥ˆBkB†
k ˆBk,
(4.137)
Im(Ak −ˆAk) ⊆Im(Bk −ˆBk),
ˆCk −Ck −(Ak −ˆAk)T (Bk −ˆBk)†(Ak −ˆAk) ≥0.

(4.138)
Then, (4.137) is equivalent to the ﬁrst summand in (3.93) being zero, while
(4.138) is equivalent to the fact that the second summand in (3.93) takes the
value zero. Let us now examine the relationship of (4.137) and (4.138) with
conditions (2.102) in Theorem 2.54, which read as follows:
Hk −ˆHk ≥0,
Ker Bk ⊆Ker ˆBk,
ˆBk ≥ˆBkB†
k ˆBk.
(4.139)
The conditions in (4.139) represent a discrete counterpart of conditions (1.128)
for linear Hamiltonian differential systems (1.99), i.e., of the conditions
H(t) −ˆH(t) ≥0,
ˆB(t) ≥0.
Note that by Corollary 3.16, the condition Hk−ˆHk ≥0 in (4.139) is equivalent
to three conditions, namely, to (4.138) and the additional condition Bk −ˆBk ≥
0. In this way, all three conditions in (4.139) are sufﬁcient for (4.137) and
(4.138) and, in turn, also for (4.133). The last fact conﬁrms that Corollary 4.50
is a generalization of Theorem 2.54. Note, however, that (4.137) and (4.138)
do not imply the inequality Bk −ˆBk ≥0. Moreover, if (4.137) and Bk ≥ˆBk

240
4
Oscillation Theory of Symplectic Systems
simultaneously hold, then ind Bk = ind ˆBk, which follows from (3.92) (see also
Remark 3.35). In particular, for a pair of 2n-th order Sturm-Liouville difference
equations (2.27), the leading coefﬁcients r[n]
k
and ˆr[n]
k
must have in this case the
same sign; see also Example 3.36. For the continuous time case, the conditions
in (1.128) imply the Legendre condition (1.111) for system (1.103), and then
in this case ind ˆB(t) = ind B(t) = 0. In the discrete case, we need neither
the Legendre condition ind ˆBk = ind Bk = 0 nor the more general assumption
ind ˆBk = ind Bk. Hence, the sufﬁcient conditions of Theorem 2.54 are slightly
overdeﬁned by the condition Bk −ˆBk ≥0, which is included in Hk −ˆHk ≥
0. On the other hand, the last condition is convenient for applications of this
theorem in some particular cases.
(iii) Concerning special cases of the majorant condition in (4.133) for the scalar
and matrix Sturm-Liouville difference equations and for the 2n-th order
Sturm-Liouville difference equations, see, respectively, Examples 3.29, 3.30,
and 3.36.
Note the majorant condition (4.133) represents a simple assumption, which
implies (4.131). This follows from (4.109). In this case the results of Theorem 4.50
can be strengthened in the following direction.
Corollary 4.52 Suppose that all assumptions of Theorem 4.50 hold and (4.131) is
replaced by majorant condition (4.133) for k ∈[M, N]Z. Then all statements of
Theorem 4.50 are true, and we have instead of (4.132) the equality
μ

⟨ˆZ−1
k+1Zk+1⟩, ⟨ˆZ−1
k Zk⟩

= 0,
k ∈[M, N]Z,
(4.140)
i.e., the conjoined bases Y and ˜Y of systems (4.112) and (4.114) do not have any
forward focal points in the interval (M, N + 1].
In a similar way, in order to present an extension of Corollary 4.30, we use
formula (4.126), which connects the multiplicities of backward focal points of
conjoined bases of (SDS) and (4.98).
Theorem 4.53 Suppose that for conjoined bases Y and ˆY of systems (SDS) and
(4.98) condition (4.59) holds at k = N + 1, ˆY has no backward focal points in the
interval [M, N +1), and the relative oscillation number for symplectic fundamental
matrices Z and ˆZ associated with Y and ˆY obeys the condition
#k( ˆZ, Z) ≥0,
k ∈[M, N]Z.
(4.141)
Then the conjoined basis Y has no backward focal points in this interval either
#k( ˆZ, Z, M, N) = 0,
(4.142)
and (4.59) holds for all k ∈[M, N + 1].

4.3
Comparison Theorems and Its Corollaries
241
Proof We rewrite (4.126) in the form
ˆm∗(k) + μ∗( ˆYk+1, Yk+1) = #k( ˆZ, Z) + m∗(k) + μ∗( ˆYk, Yk).
Then taking into account the assumptions of the theorem, we have that the condition
ˆm∗(k) = μ∗( ˆYk+1, Yk+1) = 0 implies #k( ˆZ, Z) = m∗(k) = μ∗( ˆYk, Yk) = 0.
⊓⊔
In view of formula (4.109), the majorant condition (4.134) is a simplest sufﬁcient
condition for (4.141).
Corollary 4.54 Suppose that instead of (4.141) the majorant condition (4.134)
holds for k ∈[M, N]Z, while all the other assumptions of Theorem 4.53 are
satisﬁed. Then all statements of this theorem are true, and, additionally, we have
that the conjoined bases Y and ˜Y of systems (4.115) and (4.117) do not have any
backward focal points in the interval [M, N + 1).
4.3.2
Comparison Theorems for Principal Solutions
We have proved in Theorem 4.34 the equality for the numbers of forward and
backward focal points of principal solutions at k = M and k = N + 1, i.e.,
l

Y [M], M, N + 1

= l∗
Y [N+1], M, N + 1

.
Now we will turn our attention to the situation, when the conjoined bases in the
previous formula are solutions of different symplectic systems. For the proof we
need the following auxiliary result on a representation of the relative oscillation
numbers for the special case of the principal solutions.
Lemma 4.55 Let Z[M] and ˆZ[N+1] be fundamental symplectic matrices of (SDS)
and (4.98) such that Y [M] = Z[M] (0 I)T and ˆY [N+1] = ˆZ[N+1] (0 I)T are the
principal solutions of these systems at k = M and k = N + 1, respectively. We
deﬁne the relative oscillation number of these matrices by formula (4.109), i.e.,
#k( ˆZ[N+1], Z[M]) := μ(⟨Gk⟩, ⟨Gk+1⟩) −μ(⟨ˆSk⟩, ⟨Sk⟩),
k ∈[M, N]Z,
(4.143)
where
Gk := ( ˆZ[N+1]
k
)−1Z[M]
k
.

242
4
Oscillation Theory of Symplectic Systems
Then, for the endpoints k = M and k = N of the interval, the formula in (4.143) is
speciﬁed as
#M( ˆZ[N+1], Z[M]) = μ∗ ˆY [N+1]
M
, ˆS−1
M SM (0 I)T 
−μ ˆSM(0 I)T , SM(0 I)T ,
(4.144)
#N( ˆZ[N+1], Z[M]) = μ ˆSNY [M]
N
, ˆSNS−1
N (0 I)T 
−μ∗ ˆS−1
N (0 I)T , S−1
N (0 I)T 
.
(4.145)
Proof The speciﬁcation of the values #k
 ˆZ[N+1], Z[M]
for k = M and k = N
follows from Lemma 3.21. In fact, using the condition Z[M]
M (0 I)T = (0 I)T , we
obtain that
μ⟨GM⟩, ⟨GM+1⟩ = μGM(0 I)T , GM+1(0 I)T  + μ⟨G−1
M+1GM⟩, ⟨I⟩
= μ

GM(0 I)T , GM+1(0 I)T 
+ μ

⟨S−1
M ˆSM⟩, ⟨I⟩

,
where we have used Lemma 3.21(v). Analogously
μ

⟨ˆSM⟩, ⟨SM⟩

= μ
 ˆSM(0 I)T , SM(0 I)T 
+ μ

⟨S−1
M ˆSM⟩, ⟨I⟩

.
Consequently, by Theorem 3.5(iii),
μ

⟨GM⟩, ⟨GM+1⟩

−μ

⟨ˆSM⟩, ⟨SM⟩

= μ

GM(0 I)T , GM+1(0 I)T 
−μ
 ˆSM(0 I)T , SM(0 I)T 
= μ

( ˆZ[N+1]
M
)−1(0 I)T , ( ˆZ[N+1]
M+1 )−1SM(0 I)T 
−μ
 ˆSM(0 I)T , SM(0 I)T 
= μ∗ ˆZ[N+1]
M
(0 I)T , ˆS−1
M SM(0 I)T 
−μ
 ˆSM(0 I)T , SM(0 I)T 
,
which proves (4.144). Analogously, by Lemma 3.21(v), we have
μ

⟨GN⟩, ⟨GN+1⟩

= μ∗
G−1
N (0 I)T , G−1
N+1(0 I)T 
+ μ

⟨GNG−1
N+1⟩, ⟨I⟩

= μ∗G−1
N (0 I)T , G−1
N+1(0 I)T  + μ⟨ˆSNS−1
N ⟩, ⟨I⟩,
and similarly
μ

⟨ˆSN⟩, ⟨SN⟩

= μ∗ ˆS−1
N (0 I)T , S−1
N (0 I)T 
+ μ

⟨ˆSNS−1
N ⟩, ⟨I⟩

.

4.3
Comparison Theorems and Its Corollaries
243
Consequently, by Theorem 3.5(iii) we have proved that
μ

⟨GN⟩, ⟨GN+1⟩

−μ

⟨ˆSN⟩, ⟨SN⟩

= μ∗
G−1
N (0 I)T , G−1
N+1(0 I)T 
−μ∗ ˆS−1
N (0 I)T , S−1
N (0 I)T 
= μ∗
(Z[M]
N )−1 ˆS−1
N (0 I)T , (Z[M]
N+1)−1(0 I)T 
−μ∗ ˆS−1
N (0 I)T , S−1
N (0 I)T 
= μ
 ˆSNZ[M]
N (0 I)T , ˆSNS−1
N (0 I)T 
−μ∗ ˆS−1
N (0 I)T , S−1
N (0 I)T 
.
The proof is complete.
⊓⊔
Now we present a generalization of Theorems 4.34 and 4.35 to two symplectic
systems.
Theorem 4.56 (Sturmian Comparison Theorem) Consider the symplectic fun-
damental matrices Z[M], Z[N+1] and ˆZ[M], ˆZ[N+1] of systems (SDS) and (4.98),
which are associated with the principal solutions Y [M], Y [N+1] and ˆY [M], ˆY [N+1]
of these systems via (3.14). Then
l∗( ˆY [N+1], M, N + 1) −l(Y [M], M, N + 1)
= l( ˆY [M], M, N + 1) −l(Y [M], M, N + 1) = #( ˆZ[N+1], Z[M], M, N),
(4.146)
and
l( ˆY [N+1], M, N + 1) −l∗(Y [M], M, N + 1)
= l∗( ˆY [M], M, N + 1) −l∗(Y [M], M, N + 1) = #( ˆZ[M], Z[N+1], M, N),
(4.147)
where the quantities l∗(Y, M, N + 1), l(Y, M, N + 1), and #( ˆZ, Z, M, N) are
given by formulas (4.10), (4.11), and (4.109), (4.110). Moreover, for the relative
oscillation numbers in the right-hand sides of (4.146), (4.147) we have the relations
#( ˆZ[N+1], Z[M], M, N) = −#(Z[N+1], ˆZ[M], M, N),
(4.148)
#( ˆZ[M], Z[N+1], M, N) = −#(Z[M], ˆZ[N+1], M, N).
(4.149)
Proof For the proof of (4.146), we use Theorem 4.49 for the particular case when
ˆY := ˆY [N+1] is the principal solution of (4.98) at k = N + 1 and Y := Y [M] is the
principal solution of (SDS) at k = M. We rewrite (4.129) into the form
l( ˆY [N+1], M, N + 1) −l(Y [M], M, N + 1) −rank ˆX[N+1]
M
= #( ˆZ[N+1], Z[M], M, N),
(4.150)

244
4
Oscillation Theory of Symplectic Systems
where we have used that μ

Y [M]
M , ˆY [N+1]
M

= rank ˆX[N+1]
M
and μ

Y [M]
N+1, ˆY [N+1]
N+1

=
0, according to the deﬁnition of the comparative index. We modify the summands
in the left-hand side of (4.150) as
l( ˆY [N+1], M, N +1)−rank ˆX[N+1]
M
= l∗( ˆY [N+1], M, N +1) = l( ˆY [M], M, N +1),
where we have used (4.12) for ˆY [N+1]
k
and (4.82) for l∗( ˆY [N+1], M, N + 1) and
l( ˆY [M], M, N + 1). This proves formula (4.146).
In a similar way, by putting ˆY := ˆY [M] and Y := Y [N+1] in Theorem 4.49, we
have (4.129) in the form
l( ˆY [M], M, N + 1) −l(Y [N+1], M, N + 1) + rank ˆX[M]
N+1
= #( ˆZ[M], Z[N+1], M, N).
(4.151)
Applying (4.12) we have l( ˆY [M], M, N + 1) + rank ˆX[M]
N+1 = l∗( ˆY [M], M, N + 1),
and by (4.83) we get l(Y [N+1], M, N + 1) = l∗(Y [M], M, N + 1). Substituting the
last representations into (4.151), we complete the proof of (4.147). Relations (4.148)
and (4.149) follow from interchanging the roles of Y [M] and ˆY [M] in the proof of
(4.146) and (4.147). Note that they can also be derived independently by using parts
(i) and (iii) of Lemma 4.47.
⊓⊔
The result in Theorem 4.56 plays a fundamental role in the relative oscillation
theory of eigenvalue problems in Chap. 6. As a consequence of Theorem 4.56, we
have the following corollary.
Corollary 4.57 The condition
#( ˆZ[N+1], Z[M], M, N) ≥0
(4.152)
for the relative oscillation number at the right-hand side of (4.146) is necessary and
sufﬁcient for the inequality
l( ˆY [M], M, N + 1) ≥l(Y [M], M, N + 1)
(4.153)
concerning the number of (forward) focal points of the principal solutions at k = M
of (SDS) and (4.98), respectively.
Remark 4.58 The above corollary implies the statements of Theorems 5.89
and 5.90 in the next chapter, which are there proved by using a relationship of
the investigated eigenvalue problems to their quadratic energy functionals. Formula
(4.153) together with (4.62) implies the statements of Theorems 5.89 and 5.90 as
follows:

4.3
Comparison Theorems and Its Corollaries
245
(i) The difference between the number of focal points of any conjoined basis of
(SDS) and the number of focal points of the principal solution at k = M of
(4.98) in (M, N + 1] is at most n, proven in [56, Theorem 1.2].
(ii) The number of focal points in (M, N + 1] of any conjoined basis of (4.98) is
greater or equal to the number of focal points of the principal solution of (SDS)
at k = M, proven in [56, Theorem 1.3].
Note that statements (i) and (ii) are simple consequences of (4.153) and (4.60), resp.
of (4.62). Indeed, using (4.60) and
l(Y [M], M, N + 1) = l(Y, M, N + 1) −μYN+1, Y [M]
N+1

we obtain from (4.153) that
l( ˆY [M], M, N + 1) −l(Y, M, N + 1) + μ(YN+1, Y [M]
N+1) ≥0,
or
l(Y, M, N + 1) −l( ˆY [M], M, N + 1) ≤μ(YN+1, Y [M]
N+1) ≤n.
The last equality implies the statement (i). As for (ii), from (4.60) it follows that
l( ˆY, M, N + 1) ≥l( ˆY [M], M, N + 1) and then inequality (4.153) proves that
l( ˆY, M, N +1) ≥l(Y [M], M, N +1) for any conjoined basis ˆY of (4.98). Note also
that according to (4.143) the majorant condition (4.134) is the simplest sufﬁcient
condition for (4.152).
4.3.3
Singular Comparison Theorems
Now we turn our attention to Theorem 4.45 and its corollaries for the singular case
presenting a generalization of Theorem 4.39 for two symplectic systems (SDS) and
(4.98). We introduce the following notation
#( ˆZ, Z, M, +∞) :=
∞

k=M
#k( ˆZ, Z)
(4.154)
for the inﬁnite series of the relative oscillation numbers #k( ˆZ, Z) given by (4.109)
and introduce a similar notation for the case of −∞, i.e.,
#( ˆZ, Z, −∞, N) =
N

k=−∞
#k( ˆZ, Z).
(4.155)
The singular version of Theorem 4.39 is the following.

246
4
Oscillation Theory of Symplectic Systems
Theorem 4.59 (Singular Sturmian Comparison Theorem) Assume that the
majorant condition (4.133) holds for all k ≥M0 for some M0 ∈Z and system
(SDS) is nonoscillatory at ∞. Then system (4.98) is nonoscillatory at ∞as well,
for arbitrary symplectic fundamental matrices Z and ˆZ of (SDS) and (4.98) such
that Y = Z (0 I)T and ˆY = ˆZ (0 I)T and for arbitrary M ∈Z there exist ﬁnite
limits (4.154), (4.87), (4.91), (4.93), which are connected by the identities
#( ˆZ, Z, M, +∞) = l( ˆY, M, +∞) −l(Y, M, +∞)
+ μ+∞(Y, ˆY ) −μ(YM, ˆYM),
#(Z, ˆZ, M, +∞) = l∗(Y, M, +∞) −l∗( ˆY, M, +∞)
+ μ∗
+∞(Y, ˆY ) −μ∗(YM, ˆYM).
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(4.156)
Proof If system (SDS) is nonoscillatory at ∞, then applying Corollary 4.52 for the
pair of the principal solutions Y [M] and ˆY [M] of (SDS) and (4.98) at k = M such
that l(Y [M], M, +∞) = 0, we see that l( ˆY [M], M, +∞) = 0, i.e., system (4.98) is
nonoscillatory as well. By Lemma 4.38, there exist ﬁnite limits (4.87) for arbitrary
conjoined bases Y and ˆY of (SDS) and (4.98), and then, by identity (4.129) and by
a similar identity
l∗(Y, M, N + 1) −l∗( ˆY, M, N + 1) + μ∗(YN+1, ˆYN+1) −μ∗(YM, ˆYM)
= #(Z, ˆZ, M, N)
(4.157)
derived from (4.127), we see that the relative oscillation numbers are bounded, i.e.,
for some positive C1 and C2, we have
|#( ˆZ, Z, M, N)| ≤C1,
|#(Z, ˆZ, M, N)| ≤C2,
i ∈{1, 2}.
(4.158)
Moreover, the relative oscillation numbers are monotonic with respect to N ≥M0,
by the majorant condition (4.133), which holds for all k ≥M0. Then the sum in
(4.154) and the similar deﬁned sum #(Z, ˆZ, M, +∞) are ﬁnite, and, by taking the
limits as N →+∞in (4.129) and (4.157), we prove that there exist ﬁnite limits of
the comparative index (4.91) and (4.93) for conjoined bases Y and ˆY of (SDS) and
(4.98). In this way we derive (4.156), which completes the proof.
⊓⊔
Remark 4.60 Note that according to Remark 4.46 and Corollary 4.52, systems
(4.112) and (4.114) are nonoscillatory at +∞, i.e., condition (4.140) for the mul-
tiplicities of forward focal points of their conjoined bases holds for all sufﬁciently
large k. Then, by Lemma 4.38 and by Theorem 4.59, the multiplicities of backward
focal points of these conjoined bases also equal zero for all sufﬁciently large k, i.e.,
μ(⟨Z−1
k
ˆZ−1
k ⟩, ⟨Z−1
k+1 ˆZ−1
k+1⟩) = 0,
k ≥M1.

4.3
Comparison Theorems and Its Corollaries
247
Moreover, by (4.118) in Lemma 4.47, we have the relation
#( ˆZ, Z, M, +∞) + #(Z, ˆZ, M, +∞) = lim
k→∞w(Yk, ˆYk) −w(YM, ˆYM)
(4.159)
between the ﬁnite limits in (4.156), where limk→∞w(Yk, ˆYk) is the ﬁnite limit of
the Wronskian. Note also that under the assumptions of Theorem 4.59 for arbitrary
conjoined bases of systems (SDS) and (4.98), we have the following generalization
of relations (4.95)–(4.97):
μ+∞(Y, ˆY ) + μ+∞( ˆY, Y) = lim
k→∞rank w(Yk, ˆYk),
(4.160)
μ∗
+∞(Y, ˆY ) + μ∗
+∞( ˆY, Y) = lim
k→∞rank w(Yk, ˆYk),
(4.161)
μ+∞(Y, ˆY ) = lim
k→∞rank ˆXk −lim
k→∞rankXk + μ∗
+∞( ˆY, Y).
(4.162)
Recall that all quantities μ(Yk, ˆYk), μ∗(Yk, ˆYk), rank Xk, and rank w(Yk, ˆYk) take
their values from the set {0, 1, . . ., n} and then the existence of the limits of these
quantities for k →∞means that they are constant for sufﬁciently large k.
Obviously, Theorem 4.59 holds when Y and ˆY are conjoined bases of the same
system (SDS). In this case Theorem 4.59 turns into Theorem 4.39. For the case of
the nonoscillation at −∞, we have the following analog of Theorem 4.59.
Theorem 4.61 (Singular Sturmian Comparison Theorem) Suppose that the
majorant condition (4.134) holds for all k ≤M0 for some M0 ∈Z and system
(4.98) is nonoscillatory at −∞. Then system (SDS) is nonoscillatory at −∞as
well, for arbitrary symplectic fundamental matrices Z and ˆZ of (SDS) and (4.98)
such that Y = Z (0 I)T and ˆY = ˆZ (0 I)T and for arbitrary N ∈Z there exist
ﬁnite limits (4.155), (4.89), and
μ−∞( ˆY, Y) :=
lim
k→−∞μ( ˆYk, Yk),
μ∗
−∞( ˆY, Y) :=
lim
k→−∞μ∗( ˆYk, Yk),
which are connected by the identities
#(Z, ˆZ, −∞, N) = l(Y, −∞, N + 1) −l( ˆY, −∞, N + 1)
+ μ( ˆYN+1, YN+1) −μ−∞( ˆY, Y)
#( ˆZ, Z, −∞, N) = l∗( ˆY, −∞, N + 1) −l∗(Y, −∞, N + 1)
+ μ∗( ˆYN+1, YN+1) −μ∗
−∞( ˆY, Y).
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(4.163)
Proof Assuming that system (4.98) is nonoscillatory at −∞, we see that sys-
tem (SDS) is nonoscillatory at −∞as well by Corollary 4.54. Then applying
Lemma 4.38 to the case −∞, we obtain that the limits (4.89) exist for arbitrary
conjoined bases Y and ˆY of (SDS) and (4.98). By (4.125) and (4.126), the relative

248
4
Oscillation Theory of Symplectic Systems
oscillation numbers are then bounded, i.e., (4.158) holds as M →−∞. Using
majorant condition (4.134) for all sufﬁciently negative k, we see that these numbers
are also monotonic with respect to M ≤M0. Then there exist ﬁnite limits (4.155)
and #(Z, ˆZ, −∞, N), and we derive (4.163) from (4.125) and (4.126) by analogy
with the proof of Theorem 4.59.
⊓⊔
4.4
Focal Points and Symplectic Transformations
In Sect. 2.6 we presented basic elements of the theory of symplectic transformations
of symplectic difference system. Recall that if Rk are symplectic 2n × 2n matrices,
then transformation (2.138), i.e., yk = Rkwk, transforms (SDS) into another
symplectic system (2.139), i.e., wk+1 = ˜Skwk. We rewrite the latter system in the
matrix form as
˜Yk+1 = ˜Sk ˜Yk,
˜Sk = R−1
k+1SkRk
(4.164)
for the transformed conjoined basis ˜Y given by
Yk = Rk ˜Yk.
(4.165)
An important question is what are the invariants of transformation (4.165).
In particular, what additional assumptions on Rk imply that (4.165) preserves
oscillatory nature of the transformed systems (e.g., the number of focal points,
deﬁniteness of the associated quadratic functionals, etc.).
4.4.1
Focal Points and General Symplectic Transformation
We start with the formula relating the number of focal points of a conjoined basis Y
of (SDS) and the conjoined basis ˜Y := R−1Y of (4.164).
Theorem 4.62 Suppose that the conjoined bases Y and ˜Y of (SDS) and of (4.164),
respectively, are related by (4.165). Then we have
m( ˜Yk) −m(Yk) −μ ˜Yk, R−1
k (0 I)T  = uk,
(4.166)
where
uk = μ

R−1
k+1(0 I)T, ˜Sk(0 I)T 
−μ∗
Rk(0 I)T, S−1
k (0 I)T 
(4.167)
= μ∗
S−1
k (0 I)T, Rk(0 I)T 
−μ
 ˜Sk(0 I)T, R−1
k+1(0 I)T 
,
(4.168)

4.4
Focal Points and Symplectic Transformations
249
and where m(Yk) and m( ˜Yk) are the numbers of focal points of the indicated
conjoined bases in (k, k + 1].
Proof Let Z and ˜Z = R−1Z be symplectic fundamental matrices of (SDS) and
(4.164), respectively, such that Y = Z (0 I)T and ˜Y = ˜Z (0 I)T . We introduce the
symplectic difference system for the transformation matrix Rk as
Rk+1 = ˆSkRk,
ˆSk := Rk+1R−1
k .
(4.169)
Now we apply Theorem 4.45 to systems (SDS) and (4.169) by putting ˆZk := Rk
and ˆY := Rk (0 I)T and then derive the formula
m( ˆYk) −m(Yk) + μ(Yk, Rk(0 I)T ) = #(Rk, Zk),
(4.170)
where
#(Rk, Zk) = μ(⟨Sk⟩, ⟨Rk+1R−1
k ⟩) −μ(⟨˜Zk+1⟩, ⟨˜Zk⟩).
Evaluating the relative oscillation number #(Rk, Zk) according to Remark 4.46(ii),
we have by (4.113) the formula
#(Rk, Zk) = μ(⟨Sk⟩, ⟨Rk+1R−1
k ⟩) −μ(⟨˜Sk⟩, ⟨I⟩) −m( ˜Yk).
Applying Corollary 3.40(iii) to the difference μ(⟨Sk⟩, ⟨Rk+1R−1
k ⟩) −μ(⟨˜Sk⟩, ⟨I⟩)
on the right-hand side of the last identity, we derive the representation
#(Rk, Zk) = uk + m∗( ˆYk) −m( ˜Yk),
(4.171)
where uk is given by (4.167), (4.168) and m∗( ˆYk) = μ(R−1
k (0 I)T, R−1
k+1(0 I)T )
is the multiplicity of a backward focal point of ˆYk = Rk (0 I)T in [k, k + 1).
Substituting (4.171) into (4.170) and using the connection m∗( ˆYk) −m( ˆYk) =
μ((0 I)T, Rk (0 I)T ) between the multiplicities of backward and forward focal
points, we derive (4.166) in the form
m( ˜Yk) −m(Yk) + μ

Yk, Rk (0 I)T 
= uk + μ

(0 I)T, Rk(0 I)T 
.
(4.172)
The ﬁnal representation (4.166) now follows from the formula
μYk, Rk (0 I)T  = μ(0 I)T, Rk (0 I)T  −μ ˜Yk, R−1
k (0 I)T ,
(4.173)
which is derived according to Theorem 3.5(ix). The proof is complete.
⊓⊔
Note that we can interchange systems (SDS) and (4.164). Then Rk and R−1
k
also change their role, and as a result of this approach, we obtain another formulas

250
4
Oscillation Theory of Symplectic Systems
expressing the difference ˜m(k) −m(k). The results are summarized in the next
corollary.
Corollary 4.63 Suppose that the assumptions of Theorem 4.62 hold. Then
m( ˜Yk) −m(Yk) + μYk, Rk (0 I)T  = ˜uk,
(4.174)
where
˜uk = μ∗
R−1
k (0 I)T, ˜S−1
k (0 I)T 
−μ

Rk+1(0 I)T, Sk(0 I)T 
(4.175)
= μSk (0 I)T, Rk+1 (0 I)T  −μ∗ ˜S−1
k (0 I)T, R−1
k (0 I)T 
(4.176)
and where the sequences ˜uk and uk given by (4.167) are connected by the formula
˜uk −uk = 

rank(I 0) Rk (0 I)T 
.
(4.177)
Proof As it was mentioned above, we derive (4.174) and (4.175) just by inter-
changing the roles of (SDS) and (4.164) in (4.166). Formula (4.177) follows from
(4.172) and (4.173), where we evaluate the comparative index μ

(0 I)T, Rk (0 I)T 
according to Remark 3.4(iii).
⊓⊔
Summation of formulas (4.166) and (4.174) from k = M to k = N gives the
fundamental result relating the number of (forward) focal points in (M, N + 1] by
a general symplectic transformation which is formulated in the next theorem.
Theorem 4.64 The numbers l(Y, M, N + 1) and l( ˜Y, M, N + 1) of forward focal
points of Y and ˜Y related by (4.165) in (M, N + 1] satisfy the formulas
l( ˜Y, M, N + 1) −l(Y, M, N + 1)
−μ
 ˜Yk, R−1
k (0 I)T 			
N+1
M = S(M, N),
S(M, N) = ˜S(M, N) −rank

(I 0) Rk (0 I)T 		N+1
M
,
S(M, N) :=
N
k=M
uk,
˜S(M, N) :=
N
k=M
˜uk,
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
(4.178)
where uk and ˜uk are the sequences given in (4.167) and (4.175).
Remark 4.65
(i) Note that for the partial sums S(M, N) and ˜S(M, N) in (4.178), we have by
(4.177) the estimate
|S(M, N) −˜S(M, N)|
≤max
!
rank(I 0)T RN+1 (0 I)T, rank (I 0)T RM (0 I)T "
≤n.
(4.179)

4.4
Focal Points and Symplectic Transformations
251
In particular, S(M, N) = ˜S(M, N) for the case when the transformation matrix
Rk is constant, i.e., Rk ≡R. It follows from (4.179) that either the partial
sums S(M, N) and ˜S(M, N) are simultaneously bounded for a ﬁxed M ∈Z as
N →∞, i.e., the inequalities
|S(M, N)| ≤C(M),
| ˜S(M, N)| ≤˜C(M),
N ≥M,
(4.180)
hold for some positive constants C(M) and ˜C(M), or these sums are simulta-
neously unbounded.
(ii) The left-hand side of (4.166) can be written in the equivalent form
m( ˜Yk) −m(Yk) −μ
 ˜Yk, R−1
k
(0 I)T 
= m∗( ˜Yk) −m∗(Yk) −μ∗
Yk, Rk (0 I)T 
,
(4.181)
where m∗(Yk) and m∗( ˜Yk) are the multiplicities of backward focal points of
Y and ˜Y in [k, k + 1). The proof of (4.181) is based on Proposition 4.4(vi)
and formulas (4.173), (3.34). Identity (4.181) implies that the numbers of focal
points l∗(Y, M, N + 1) and l∗( ˜Y, M, N + 1) can be expressed by a formula
similar to (4.178), i.e., by
l∗( ˜Y, M, N + 1) −l∗(Y, M, N + 1) −μ∗
Yk, Rk (0 I)T 			
N+1
M
= S(M, N).
(4.182)
4.4.2
Focal Points and Special Symplectic Transformations
We consider now formula (4.166) in the particular case when the transformation
matrix is Rk = J .
Corollary 4.66 When Rk ≡J , formulas (4.166) and (4.167) read as
m(J T Yk) −m(Yk) −μ

J T Yk, J T (0 I)T 
= ind (−AT
k Ck) −ind (AkBT
k ),
μJ T Yk, J T (0 I)T  = rank(I −UkU†
k ) + ind (XT
k Uk),
⎫
⎪⎬
⎪⎭
(4.183)
and formulas (4.174) and (4.175) can be written as
m(J T Yk) −m(k) + μ

Yk, J (0 I)T 
= ind (−CT
k Dk) −ind (BT
k Dk),
μ

Yk, J (0 I)T 
= rank(I −XkX†
k) + ind (−XT
k Uk).

(4.184)

252
4
Oscillation Theory of Symplectic Systems
and
uk = ind (−AT
k Ck)−ind (AkBT
k ) = ind (−CT
k Dk)−ind (BT
k Dk) = ˜uk.
(4.185)
Proof In the particular case Rk = J , we obtain from (4.167) the formulas
μ

R−1
k+1(0 I)T, ˜Sk(0 I)T 
= μ

(−I 0)T , J T SkJ (0 I)T 
= μ2

(−I 0)T , (−CT
k , AT
k )T 
= ind (−AT
k Ck),
μ∗
Rk(0 I)T, S−1
k (0 I)T 
= μ∗
(I 0)T , (−Bk, Ak)T 
= μ∗
2
(I 0)T , (−Bk, Ak)T  = ind (AkBT
k ).
Further, (4.175) implies
μ

Rk+1(0 I)T, Sk(0 I)T 
= μ

(I 0)T , Sk(0 I)T 
= μ2

(I 0)T , (BT
k , DT
k )T 
= ind (BT
k Dk),
μ∗
R−1
k (0 I)T, ˜S−1
k (0 I)T 
= μ∗
(−I 0)T , (Ck, Dk)T 
= ind (−CkDT
k ),
and
μ

R−1
k Yk, R−1
k (0 I)T 
= μ

J T Yk, (−I 0)T 
= rank (I −UkU†
k ) + ind (XT
k Uk),
μ

Yk, Rk(0 I)T 
= μ(Yk, (0 I)T ) = rank(I −XkX†
k) + ind (−XT
k Uk),
where in all previous computations we have used Remark 3.4(iv) and the deﬁnitions
of the comparative index and the dual comparative index. Relation (4.185) follows
from formula (4.177) for the case when the transformation matrix does not depend
on k.
⊓⊔
Formula (4.166) also implies that multiplicity of a focal point is preserved under
transformation with lower block-triangular symplectic matrix, as it is formulated in
the next corollary. This is the problem that we have already mentioned in Sect. 2.6.
Corollary 4.67 Let Rk = Lk, where Lk is a symplectic lower block-triangular
matrix. Then for any conjoined basis Y of (SDS), we have m(Yk) = m(L−1
k Yk).
Proof In our particular case μ
 ˜Yk, L−1
k (0 I)T 
= 0, consequently, the left-hand
side of (4.166) takes the form m( ˜Yk) −m(Yk). Further, using deﬁnition of uk in
Theorem 4.62 by (4.168), we obtain
μ∗S−1
k (0 I)T, Lk(0 I)T  = μ ˜Sk(0 I)T, L−1
k+1(0 I)T  = 0,
which follows directly from Remark 3.4(ii). Consequently, the right-hand side of
(4.166) equals zero.
⊓⊔

4.4
Focal Points and Symplectic Transformations
253
4.4.3
Generalized Reciprocity Principle
Recall from Deﬁnition 4.37 that symplectic system (SDS) is nonoscillatory (at ∞)
if there exists M ∈N such that the principal solution Y [M] of (SDS) at k = M has
no forward focal point in (M, ∞). In the opposite case, (SDS) is oscillatory (at ∞).
The inequality
		l(Y, M, N + 1) −l(Y [M], M, N + 1)
		 ≤n,
N ≥M,
from Corollary 4.25 implies that one can take any conjoined basis Y in the
deﬁnition of (non)oscillation of (SDS) at ∞instead of the principal solution Y [M].
In the following result, we present the most general statement in this context. It is
a consequence of Theorem 4.64.
Theorem 4.68 (Generalized Reciprocity Principle for Symplectic Systems) Let
us deﬁne the sequences S(M, N) and ˜S(M, N) by (4.178).
(i) Assume that at least one of the sequences S(M, N) or ˜S(M, N) is bounded as
N →∞, i.e., there exist constants C(M) or ˜C(M) such that
|S(M, N)| ≤C(M)
or
| ˜S(M, N)| ≤˜C(M),
N ≥M.
(4.186)
Then systems (SDS) and (4.164) have the same oscillatory nature at ∞, i.e.,
they are oscillatory or nonoscillatory at ∞at the same time.
(ii) If (SDS) and (4.164) are simultaneously nonoscillatory at ∞, then the
sequences S(M, N) and ˜S(M, N) are bounded as N →∞.
(iii) If at least one of the sequences S(M, N) and ˜S(M, N) is unbounded, then at
least one of systems (SDS) and (4.164) is oscillatory at ∞.
Proof Recall that the sequences S(M, N) and ˜S(M, N) are both bounded or both
unbounded (see Remark 4.65).
(i) Condition (4.186), formula (4.178), and property (vii) of Theorem 3.5 imply
that
−C(M) ≤l( ˜Y, M, N + 1) −l(Y, M, N + 1) −μ( ˜Yk, R−1
k )
		N+1
M
= S(M, N) ≤C(M),
−C(M) −n ≤−C(M) −μ( ˜YM, R−1
M ) ≤l( ˜Y, M, N + 1) −l(Y, M, N + 1)
≤C(M) + μ( ˜YN+1, R−1
N+1) ≤C(M) + n.
Consequently,
		l( ˜Y, M, N +1)−l(Y, M, N +1)| ≤C(M)+n
for all N ≥M.
(4.187)

254
4
Oscillation Theory of Symplectic Systems
Obviously, if we replace the integer M by M1, the last estimate remains to
hold with some other constant C(M1). Suppose that (SDS) is nonoscillatory
at ∞, i.e., for any conjoined basis Y, there exists M1 (depending on the
conjoined basis Y) such that l(Y, M1, N) = 0 for every N > M1. Then
from (4.187) we obtain that l(R−1Y, M, N + 1) is bounded as N →∞.
Since l(R−1Y, M1, N + 1) is the partial sum of a series formed by integers
or zeros, then its boundedness is possible only if l(Y, M2, N +1) = 0 for some
M2 > M1 and every N ≥M2. Hence, (4.164) is nonoscillatory at ∞as well.
Quite similarly we prove that the nonoscillation of (4.164) at ∞implies the
nonoscillation of (SDS) at ∞.
(ii) If both systems (SDS) and (4.164) are nonoscillatory at ∞, then there exists
M1 (sufﬁciently large) such that l(Y, M1, N +1) = l( ˜Y, M1, N +1) = 0. Then
by (4.178) we have |S(M1, N)| ≤n, because of property (vii) of Theorem 3.5.
Hence, by (4.179) the sequence ˜S(M1, N) is also bounded.
(iii) This statement follows immediately from part (ii).
⊓⊔
The result in Theorem 4.68 implies that only the case of unboundedness
of sequences S(M, N) and ˜S(M, N) (case (iii) in the previous theorem) and
the oscillation of one of systems (SDS) and (4.164) at ∞needs an additional
investigation to answer the question about the (non)oscillation of the other system at
∞. In all the remaining cases and under the additional assumption on the oscillatory
nature of one of systems (SDS) or (4.164) at ∞, Theorem 4.68 provides the answer
about the (non)oscillation of the other one at ∞.
A simple sufﬁcient condition for the boundedness of S(M, N) is given in the
next theorem.
Theorem 4.69 Systems (SDS) and (4.164) oscillate or do not oscillate simultane-
ously at ∞, if at least one of the sequences uk or ˜uk given by (4.167), (4.168), and
(4.175), (4.176) tends to zero as k →∞, i.e., there exists M > 0 such that for all
k ≥M we have
uk = 0
⇔
μR−1
k+1(0 I)T, ˜Sk(0 I)T  = μ∗Rk(0 I)T, S−1
k (0 I)T 
(4.188)
or
˜uk = 0
⇔
μ∗(R−1
k (0 I)T, ˜S−1
k (0 I)T ) = μ(Rk+1(0 I)T, Sk(0 I)T ).
(4.189)
Proof Under assumption (4.188) we have S(M, N) = 0 for all M ≥N, and then
the ﬁrst statement follows directly from Theorem 4.68(i). Similarly, (4.189) implies
˜S(M, N) = 0 for all M ≥N, and then, again by Theorem 4.68(i) both systems
oscillate or do not oscillate simultaneously at ∞.
⊓⊔
In particular, for Rk = J T , we have the following corollary to Theorem 4.69.

4.4
Focal Points and Symplectic Transformations
255
Corollary 4.70 Systems (SDS) and (4.164) oscillate or do not oscillate simultane-
ously at ∞if there exists M > 0 such that
ind (−AT
k Ck) = ind (AkBT
k ),
k ≥M,
(4.190)
and (4.190) is equivalent to
ind (−CkDT
k ) = ind (BT
k Dk),
k ≥M.
(4.191)
Remark 4.71
(i) Note that for the case when rank[(I 0) Rk(0 I)T ] is constant for k ≥M,
conditions (4.188) and (4.189) are equivalent according to (4.177). In particular,
rank[(I 0) Rk(0 I)T ] = n for the case Rk = J T (see Corollary 4.70).
(ii) Conditions (4.188), (4.189) will be satisﬁed if we assume for all k ≥M
μ(R−1
k+1(0 I)T, ˜Sk(0 I)T ) = μ∗(Rk(0 I)T, S−1
k (0 I)T ) = 0,
(4.192)
or
μ∗(R−1
k (0 I)T, ˜S−1
k (0 I)T ) = μ(Rk+1(0 I)T, Sk(0 I)T ) = 0.
(4.193)
In particular, for the case Rk = J T by (4.190), (4.191), we have that (4.192)
are equivalent to the conditions
AT
k Ck ≤0,
AkBT
k ≥0,
k ≥M,
(4.194)
while (4.193) implies
CkDT
k ≤0,
BT
k Dk ≥0.
(4.195)
Note that in some special cases, to verify (4.193) (or (4.195)) may be easier than
to verify (4.192) (or (4.194)). An illustrating example supporting this idea is given
in Sect. 4.4.4 (see Example 4.73).
4.4.4
Applications and Examples
In this subsection we present several examples, which illustrate the applicability of
the above results.
Example 4.72 Let Sk =  1 0
3 1
 be the coefﬁcient matrix of a (nonoscillatory)
symplectic system (SDS) at ∞. The transformation (4.165) with the matrix Rk = J
satisﬁes condition (iii) of Theorem 4.68, since S(M, N) = N
k=M 1 = N −M + 1

256
4
Oscillation Theory of Symplectic Systems
is unbounded. Then, by (iii) of Theorem 4.68, the transformed system (4.164) with
the matrix
˜S = J T SJ =
1 −3
0 1

is oscillatory at ∞. Indeed, the conjoined basis ˜Yk = (1 0)T of this system has
exactly
l( ˜Y, M, N + 1) =
N

k=M
μ

(1 0)T , (−3 1)T 
= N −M + 1
forward focal points in the interval (M, N + 1].
Example 4.73 The symplectic difference system corresponding to the equation
determining the Fibonacci numbers xk+1 = xk+1 + xk, which can be written in
the self-adjoint form as 

(−1)kxk

+ (−1)k+1 = 0, has the coefﬁcient matrix
Sk =

1
(−1)k
(−1)k+1
0

.
This system is oscillatory at ∞, since the ﬁrst component x of the principal solution
at k = 0 is x0 = 0, x1 = 1, and xk+2 = xk+1 + xk for k ≥0. Consequently, we
have m(k) = m2(k) = ind (−1)k for all k ≥1. Obviously, condition (4.194) is not
satisﬁed, but condition (4.190) is satisﬁed for all k. Consequently, system (4.164)
with the transformation matrix Rk = J is also oscillatory at ∞. Remark also that
this system satisﬁes (4.195), since Dk = 0. Then, for the given example to verify,
(4.195) is easier than to verify (4.190).
Example 4.74 Here we present an example, in which condition (4.190) is not
satisﬁed, but condition (4.186) holds. Consider the nonoscillatory system with the
coefﬁcient matrix
Sk =

1
0
−(−2)k+1 1
 1 1
0 1
 
1
0
(−2)k 1

=

1 + (−2)k
1
(−2)k(3 −(−2)k+1) 1 −(−2)k+1

.
(4.196)
This system is nonoscillatory at ∞, since it is constructed from the nonoscillatory
symplectic system Yk+1 =
 1 1
0 1

Yk by a symplectic transformation with the lower
triangular transformation matrix Rk :=

1
0
−(−2)k 1

. Note that the latter system is
a rewritten Sturm-Liouville difference equation 2xk = 0. Concerning the matrix

4.5
Notes and References
257
Sk deﬁned in (4.196), we have
ind (BkAT
k ) = ind [1 + (−2)k] =
 0, k = 2m,
1, k = 2m + 1,
and
ind (−AT
k Ck) = ind [(1 + (−2)k)(−2)k(−3 + (−2)k+1)] =
 1, k = 2m,
0, k = 2m + 1.
Consequently, the sequence S(M, N) = N
k=M(−1)k is bounded, and by Theo-
rem 4.178 the system with the matrix J T SkJ is also nonoscillatory at ∞.
Example 4.75 Consider the coefﬁcient matrix
Sk =

1
0
−(−1)k+1 1
 1 1
0 1
 
1
0
(−1)k 1

=

1 + (−1)k
1
(−1)k(2 + (−1)k) 1 + (−1)k

.
(4.197)
The system with the matrix (4.197) is again nonoscillatory at ∞, by using the same
arguments as in Example 4.74. It is not difﬁcult to verify that
ind (−AT
k Ck) = ind [(1 + (−1)k)(−1)k+1(2 + (−1)k)] =
1, k = 2m,
0, k = 2k + 1,
and ind (BkAT
k ) = ind [1 + (−1)k] = 0. Hence, S(M, N) is unbounded and
therefore the system with the matrix J T SkJ is oscillatory at ∞.
4.5
Notes and References
As it was mentioned above, the results of this chapter present discrete analogs of
well-known classical oscillation results for linear Hamiltonian differential systems
(1.103). The basic concept in both of the theories is the multiplicity of focal points of
conjoined bases of (1.103) and (SDS). Point out that this notion for the continuous
case is based on two assumptions. The ﬁrst one is the Legendre condition (1.111),
while the second one (the identical normality assumption) is completely omitted in
the modern consideration of oscillation theory of (1.103); see, for example, [127,
207, 283, 289, 321]). The notion of the multiplicities of focal points of conjoined
bases of (SDS) and (1.103) (without the controllability assumption) was for the
ﬁrst time introduced by W. Kratz in his two outstanding papers [208] and [207].
In Sect. 4.1.1 we used the deﬁnition of the multiplicity (see Deﬁnition 4.3) and the
main terminology concerning the numbers m1 and m2 from [208].

258
4
Oscillation Theory of Symplectic Systems
The notion of a backward focal point, or more precisely the notion of “no
backward focal points” in [k, k + 1), for conjoined bases of (SDS) was introduced
in [45]. The deﬁnition of the multiplicities of backward focal points (see Deﬁ-
nition 4.3) was introduced in [87] and [115], [119]. Properties (i), (ii), and (iv)
of Proposition 4.4 concerning the multiplicities of forward focal points and the
estimate m(k) ≤rank Bk from Proposition 4.4(v) were for the ﬁrst time proved in
[208, Lemma 1]. Property (vi) in Proposition 4.4, which connects the multiplicities
of forward and backward focal points, was derived in and [115] and [119]. Point out
that a similar relation for the multiplicities of right (backward) and left (forward)
proper focal points holds for the continuous case as well (see the resent result [289,
Theorem 5.1]). The main difference in the proofs of these results is based on the
absence of the Legendre condition (1.111) in the discrete case. Because of this, the
leading role in the proof of (4.9) is played by relation (4.8). On the other hand, in
the continuous case, we have that (4.8) is trivially satisﬁed because of the Legendre
condition; see [127, Lemma 3.2], where it is proven that the second component of
the comparative index associated with the multiplicity of proper focal points is zero.
Finally, we note that the main result in Corollary 4.6 coincides with [115, formula
(3.12)].
The connection of the comparative index with the multiplicities of focal points
in Lemma 4.7 was established in [114, Lemmas 2.2 and 2.3] and then presented
together with Lemma 4.8 in [115, Lemmas 3.1 and 3.2]. Among other important
consequences, this lemma states the equivalences of different deﬁnitions of the
multiplicity of forward focal points, in particular, the equivalences of Deﬁnitions 4.1
and 4.9; see also [193]. The equivalence with Deﬁnition 4.9 stated in Proposi-
tion 4.13 is from [121].
Regarding Sect. 4.2.1, the result in Theorem 4.16 is proven in [101, Theorem 1].
An analog of Theorem 4.16 concerning the multiplicities of backward focal points
was proven in [87, Theorem 1]. The main results of Sect. 4.2.2 were derived
in [114] and [115]. The reﬁnement of the discrete oscillation theory offered by
the comparative index theory is based on the possibility to present the relations
between focal points in the form of explicit equalities instead of inequalities.
Such a possibility is based on the algebraic properties of the comparative index,
in particular, on the connection between the comparative index and the negative
inertia of some symmetric matrix (see Sect. 3.2.1). Sect. 4.2.2 presents the ﬁrst
results in this direction. The result in Theorem 4.20 (see [115, Theorem 1.1])
can be viewed as a discrete version of Theorem 1.50 (see [205, Theorem 5.2.1]),
which was recently generalized to the abnormal differential Hamiltonian systems
in [127, Theorem 2.3] and [289, Theorem 4.1] via the comparative index approach.
Similarly, Theorem 4.21 for the multiplicity of backward focal points (see [115,
formula (3.4)]) is a discrete counterpart of the same result for the right proper
focal points in [289, Theorem 4.1]. The result in Theorem 4.23 and estimate
(4.46) in Corollary 4.25 concerning rankw(Y, ˆY ) are from [115, Corollary 3.1].
Formulas (4.41) and (4.45) were derived in the continuous time settings in [289,
Proposition 4.2] via the comparative index approach. The new estimates technique
for the numbers of focal points based on the upper bounds for the comparative index

4.5
Notes and References
259
in part (vii) of Theorem 3.5 was for the ﬁrst time applied to discrete eigenvalue
problems in [123, Corollary 7]. This new technique is illustrated by the results
of Corollaries 4.25, 4.26, and 4.27. Some parts of these results are formulated by
analogy with [289, Theorem 5.2 and Corollaries 5.8 and 5.10]. Recall from Sect. 2.7
that Corollary 2.56 was proven in [173] but then again in [115, Theorem 1.2] by
the comparative index approach. The result in Corollary 4.30 follows from [94,
Corollary 3.6] applied to the case Sk ≡ˆSk.
The question about a possible coincidence of the numbers of forward and
backward focal points of the principal solutions of (SDS) was ﬁrst posed as an open
problem in [101, Section 4]. The main result of Sect. 4.2.3 (see Theorem 4.34)
solves this problem, and it is proven in [115, Lemma 3.3]. Here we present another
proof of this result, which follows from Lemma 4.33. Based on the latter lemma,
on property (vii) of Theorem 3.5 and Remark 3.10, we present some estimates (see
Lemma 4.36 and Theorem 4.32) from [289] in the more complete and improved
form.
The consideration in Sect. 4.2.4 is based on results of [94] applied to the case
Sk ≡ˆSk. Further singular Sturmian separation theorems involving the (minimal)
recessive solution of (SDS) at ∞were derived in [292]; see Sect. 6.4. The results of
Sect. 4.3.1 are from the paper [117] and from the monograph [121]. The main result
(Theorem 4.45) was proven for the ﬁrst time in [117, Theorem 2.1]; the notion
of the relative oscillation numbers (in slightly different notation) was introduced
in [124, Theorem 2.1] and then in [94, Deﬁnition 3.2]. The comparison results
in Theorem 4.45 in terms of the relative oscillation numbers can be viewed as
a discrete generalization of Theorem 1.49 (proved in [205, Theorem 7.3.1]), because
we now deal with explicit equalities for the multiplicities of focal points instead
of inequalities. Point out that a generalization of [205, Theorem 7.3.1] to abnormal
Hamiltonian differential systems (1.103) was recently proven in [127, Theorem 2.2],
which also deals with equalities for the multiplicities of proper focal points. In both
cases (discrete and continuous), the difference between inequalities and equalities
is based on incorporating the focal points of conjoined bases of some transformed
“Wronskian” system associated with two discrete symplectic (or two differential
Hamiltonian) systems. From this point of view, the main results of this section may
also belong to the relative oscillation theory for discrete symplectic systems. For
controllable linear Hamiltonian differential systems, the relative oscillation theory is
developed in the recent paper [92]. For the second-order Sturm-Liouville difference
equations (which are a special case of (SDS)), the renormalized and more general
relative oscillation theory is established in [22, 314]. The results in Theorems 4.50
and 4.53 were proven in [121]; a special case of these theorems (Corollaries 4.52
and 4.54) is presented in [94, Corollaries 3.4 and 3.6]; see also [117, Corollaries 2.1
and 2.2].
The main result of Sect. 4.3.2 (Theorem 4.56) was proven in [117, Corol-
lary 2.4]. The results of Sect. 4.3.3 are from [94, Section 4]. The statement in
Lemma 4.55 is from [121]. Together with Theorem 4.56, it opened the door to
the relative oscillation theory for symplectic eigenvalue problems developed in
[118, 120, 123, 124]; see also Sect. 6.1. The considerations in Sect. 4.4 about the

260
4
Oscillation Theory of Symplectic Systems
effect of symplectic transformations on multiplicities of focal points are based on
the papers [116, 117, 126]. The results in Theorem 4.62 and Corollary 4.66 (in
a slightly different notation) and a part of Theorem 4.64 associated with uk were
proven in [116, Lemma 3.1, Corollary 3.2, Theorem 3.3]. The reciprocity principle
in the restricted form based on assumptions (4.192) and (4.194) was proven in
[116, Theorem 3.5 and Corollary 3.6]. Then the same result was proven under the
more general assumptions (4.188) and (4.190) in [117, Theorem 3.2] for the case
of constant transformation matrices. The generalized reciprocity principle in the
form presented in this book was proven in [121, 126]. Let us note that condition
(4.194) covers as a particular case the reciprocity principle for linear Hamiltonian
difference systems in [45, Theorem 3], where this principle is formulated for
Hamiltonian systems (2.15) with Ak = 0, Ck ≤0, Bk ≥0 and under the identical
normality assumption. The proof of this special statement is based on properties
of the recessive solution of (2.15) at ∞. Similarly, conditions (4.192) cover the
results of [98, Theorem 3.4]. In particular, there is an interesting interpretation
of the numbers uk given by (4.167) under the assumptions of [98, Theorem 3.4]
(see also [116, Remark (ii)]). It is necessary to point out that the recently proven
continuous analogs of Theorems 4.62 and 4.64 were derived for abnormal linear
differential Hamiltonian systems in [129],[130], [131] based on the comparative
index approach.
Important applications of Theorem 4.62 to the special trigonometric transforma-
tions
Rk =
 cos(αk) I
sin(αk) I
−sin(αk) I cos(αk) I

can be found in [96, 122].

Chapter 5
Discrete Symplectic Eigenvalue Problems
In this chapter we investigate eigenvalue problems associated with symplectic
system (SDS), where the coefﬁcient matrix depends on a spectral parameter, i.e.,
yk+1(λ) = Sk(λ) yk(λ),
k ∈[0, N]Z.
(5.1)
Here λ ∈R is the eigenvalue parameter, and the 2n × 2n matrix Sk(λ) is
symplectic for every λ ∈R. First we study in Sect. 5.1 problem (5.1) with a general
nonlinear dependence on λ and with the coefﬁcient matrix Bk(λ) having constant
rank. Here we assume a natural monotonicity assumption on the behavior of the
coefﬁcient matrix Sk(λ) in λ. This type of monotonicity condition was discussed
in details in Sect. 1.6.4. In Sect. 5.2, we present transformations between various
boundary conditions for system (5.1), in particular a transformation of separated
endpoints into Dirichlet boundary conditions and a transformation of general joint
boundary conditions into separated ones. These boundary conditions may depend
nonlinearly on the spectral parameter λ (under a certain monotonicity condition). In
Sects. 5.3–5.5, we proceed with the study of problem (5.1) with a special linear
dependence on λ, as these systems have important applications in the Sturmian
theory for system (SDS). Finally, in Sect. 5.6, we also present some extensions
of the oscillation theorems from Sect. 5.1 to symplectic systems, whose coefﬁcient
Bk(λ) has nonconstant rank.
5.1
Nonlinear Dependence on Spectral Parameter
In this section we consider a general eigenvalue problem with symplectic difference
system depending nonlinearly on the spectral parameter λ. We develop the notions
of (ﬁnite) eigenvalues and (ﬁnite) eigenfunctions and their multiplicities and prove
the corresponding oscillation theorem for Dirichlet boundary conditions. Consider
© Springer Nature Switzerland AG 2019
O. Došlý et al., Symplectic Difference Systems: Oscillation and Spectral Theory,
Pathways in Mathematics, https://doi.org/10.1007/978-3-030-19373-7_5
261

262
5
Discrete Symplectic Eigenvalue Problems
the system (5.1) in the form
xk+1(λ) = Ak(λ) xk(λ) + Bk(λ) uk(λ),
uk+1(λ) = Ck(λ) xk(λ) + Dk(λ) uk(λ),

k ∈[0, N]Z,
(SDSλ)
and the corresponding eigenvalue problem with the Dirichlet boundary conditions
yk+1(λ) = Sk(λ) yk(λ),
k ∈[0, N]Z,
λ ∈R,
x0(λ) = 0 = xN+1(λ),
(E)
where y = (x, u). The coefﬁcient matrix Sk(λ) of system (SDSλ) is assumed to be
symplectic, i.e., for all k ∈[0, N]Z and λ ∈R
ST
k (λ) J Sk(λ) = J ,
Sk(λ) :=
Ak(λ) Bk(λ)
Ck(λ) Dk(λ)

,
J :=
 0 I
−I 0

.
(5.2)
The same property is then satisﬁed by the fundamental matrix of system (SDSλ). In
addition, we assume that the matrix Sk(λ) piecewise continuously differentiable,
i.e., it is continuous on R and the derivative ˙Sk(λ) :=
d
dλSk(λ) is piecewise
continuous in the parameter λ ∈R for all k ∈[0, N]Z. Given the above symplectic
matrix Sk(λ), we consider the monotonicity assumption
(Sk(λ)) = k(λ) := J ˙Sk(λ) J ST
k (λ) J ≥0,
k ∈[0, N]Z,
λ ∈R.
(5.3)
Recall that the matrix k(λ) = (Sk(λ)) is symmetric for any k ∈[0, N]Z and
λ ∈R (see Proposition 1.75).
5.1.1
Finite Eigenvalues
In this subsection, we derive some monotonicity results based on the assump-
tion (5.3), which lead to the deﬁnition of ﬁnite eigenvalues for problem (E). Recall
that in Sect. 1.6.4 we proved important properties of (Sk(λ)) and corollaries
to (5.3) applied to arbitrary piecewise continuously differentiable symplectic matrix
W(λ). Putting W(λ) := Sk(λ) in Theorems 1.79, 1.81, 1.82 and in Corollary 1.83,
we derive the following important properties of the symplectic coefﬁcient matrix
Sk(λ) under assumption (5.3).

5.1
Nonlinear Dependence on Spectral Parameter
263
Theorem 5.1 Assume (5.3) for Sk(λ) given by (5.2). Then for any k ∈[0, N]Z, the
following assertions hold:
(i) The set Ker Bk(λ) is piecewise constant in λ, i.e., for any λ0 ∈R, there exists
δ > 0 such that
Ker Bk(λ) ≡Ker Bk(λ−
0 ) ⊆Ker Bk(λ0)
for all λ ∈(λ0 −δ, λ0),
(5.4)
Ker Bk(λ) ≡Ker Bk(λ+
0 ) ⊆Ker Bk(λ0)
for all λ ∈(λ0, λ0 + δ).
(5.5)
(ii) The set Im Bk(λ) is piecewise constant in λ, i.e., for any λ0 ∈R, there exists
δ > 0 such that
Im Bk(λ0)⊆Im Bk(λ) ≡Im Bk(λ−
0 )
for all λ ∈(λ0 −δ, λ0),
(5.6)
Im Bk(λ0)⊆Im Bk(λ) ≡Im Bk(λ+
0 )
for all λ ∈(λ0, λ0 + δ).
(5.7)
(iii) The following three conditions are equivalent:
rank Bk(λ) is constant for λ ∈R,
(5.8)
the set Ker Bk(λ) is constant for λ ∈R,
(5.9)
the set Im Bk(λ) is constant for λ ∈R.
(5.10)
(iv) The matrices Bk(λ) B†
k(λ) and B†
k(λ) Bk(λ) are piecewise constant in λ.
Next we show that similar properties hold also for arbitrary symplectic fun-
damental matrix Zk(λ) of (SDSλ) such that Z0(λ) is piecewise continuously
differentiable and (Z0(λ) ≥0 for λ ∈R. In this case, since the matrix Sk(λ)
is piecewise continuously differentiable with respect to λ, it follows that the funda-
mental matrix Zk(λ) is piecewise continuously differentiable for all k ∈[0, N +1]Z.
As a corollary to the multiplicative property (1.195) in Proposition 1.76(i), we have
the following important result.
Proposition 5.2 Assume that Zk(λ) is a symplectic fundamental matrix of (5.3)
such that Z0(λ) is piecewise continuously differentiable. Then under the assumption
(Zk(λ)) ≥0,
λ ∈R
(5.11)
for the index k = 0, we have that (5.11) holds for any k ∈[0, N + 1]Z.
Proof Applying Proposition 1.76(i), we have
(Zk+1(λ)) = (Sk(λ) Zk(λ)) = ST −1
k
(λ) (Zk(λ)) S−1
k (λ) + (Sk(λ)),
or by Proposition 1.76(ii)
(Z−1
k (λ)) = −ZT
k+1(λ) (Sk(λ) Zk+1(λ)).

264
5
Discrete Symplectic Eigenvalue Problems
Then we derive
ZT
k+1(λ) (Zk+1(λ)) Zk+1(λ)
= ZT
0 (λ) (Z0(λ)) Z0(λ) +
k

i=0
ZT
i+1(λ) (Si(λ)) Zi+1(λ),
(5.12)
and therefore (5.11) holds for any k ∈[0, N + 1]Z.
⊓⊔
Putting W(λ) := Zk(λ) for k ∈[0, N + 1]Z in Theorems 1.79, 1.81, 1.82 and in
Corollary 1.83, we now formulate the most important result of this section.
Theorem 5.3 Assume (5.3) and (Z0(λ)) ≥0 for λ ∈R for a symplectic
fundamental matrix Zk(λ) of (SDSλ) in the form
Zk(λ) =
 ˆYk(λ) Yk(λ)

=
 ˆXk(λ) Xk(λ)
ˆUk(λ) Uk(λ)

.
Then for every k ∈[0, N + 1]Z, we have
(i) The set Ker Xk(λ) is piecewise constant in λ, i.e., for any λ0 ∈R, there exists
δ > 0 such that
Ker Xk(λ) ≡Ker Xk(λ−
0 ) ⊆Ker Xk(λ0)
for all λ ∈(λ0 −δ, λ0),
(5.13)
Ker Xk(λ) ≡Ker Xk(λ+
0 ) ⊆Ker Xk(λ0)
for all λ ∈(λ0, λ0 + δ).
(5.14)
(ii) The set Im Xk(λ) is piecewise constant in λ, i.e., for any λ0 ∈R, there exists
δ > 0 such that
Im Xk(λ0)⊆Im Xk(λ) ≡Im Xk(λ−
0 )
for all λ ∈(λ0 −δ, λ0),
(5.15)
Im Xk(λ0)⊆Im Xk(λ) ≡Im Xk(λ+
0 )
for all λ ∈(λ0, λ0 + δ).
(5.16)
(iii) The following three conditions are equivalent:
rankXk(λ) is constant for λ ∈R,
(5.17)
the set Ker Xk(λ) is constant for λ ∈R,
(5.18)
the set Im Xk(λ) is constant for λ ∈R.
(5.19)
(iv) The matrices Xk(λ) X†
k(λ) and X†
k(λ) Xk(λ) are piecewise constant in λ.

5.1
Nonlinear Dependence on Spectral Parameter
265
Remark 5.4 We remark that one can associate Zk(λ) with a given conjoined basis
Y(λ) =

X(λ)
U(λ)

of (SDSλ); see Lemma 1.58(iv). In particular cases, we will assume
in addition that the initial conditions of the conjoined basis Y(λ) do not depend on
λ, i.e.,
Y0(λ) ≡Y0
for all λ ∈R.
(5.20)
It then follows from the proof of Lemma 1.58(iv) that the symplectic fundamental
matrix Zk(λ) such that Y(λ) = Z(λ)(0 I)T also does not depend on λ for k = 0,
i.e., Z0(λ) ≡Z0. In this special case, the condition (Z0(λ)) ≥0 for λ ∈R is
trivially satisﬁed, and we derive all assertions of Theorem 5.3 for the block Xk(λ)
of a conjoined basis Yk(λ) = Xk(λ)
Uk(λ)
 of (SDSλ) with initial conditions (5.20).
In Theorem 5.3, we showed that for every ﬁxed λ0 ∈R, the quantity rank Xk(λ)
is constant on some left and right neighborhoods of λ0. This allows to deﬁne
correctly the notion of a ﬁnite eigenvalue of problem (E). Let Y [0](λ) =

X[0](λ)
U[0](λ)

be the principal solution of (SDSλ) at k = 0, that is, the solution starting with the
initial values
X[0]
0 (λ) ≡0,
U[0]
0 (λ) ≡I
for all λ ∈R,
so that these initial conditions are independent of λ, and Theorem 5.3 works for this
special case (see Remark 5.4).
The result in Theorem 5.3 justiﬁes the introduction of the following notion.
Deﬁnition 5.5 (Finite Eigenvalue) Under (5.3), a number λ0 ∈R is a ﬁnite
eigenvalue of problem (E) if
θ(λ0) := rank X[0]
N+1(λ−
0 ) −rankX[0]
N+1(λ0) ≥1.
(5.21)
In this case the number θ(λ0) is called the algebraic multiplicity of λ0.
Remark 5.6
(i) The deﬁnition of a ﬁnite eigenvalue of (E) is one-sided, that is, it only depends
on the behavior of X[0]
N+1(λ) in a left neighborhood of λ0.
(ii) By (5.13), the ﬁnite eigenvalues are well deﬁned, since the number θ(λ0) is
always nonnegative. Moreover,
θ(λ0) = defX[0]
N+1(λ0) −def X[0]
N+1(λ−
0 )
= dim

[Ker X[0]
N+1(λ−
0 )]⊥∩Ker X[0]
N+1(λ0)

.
(5.22)
(iii) When X[0]
N+1(λ) is invertible except at isolated values of λ (which is the case of
“controllable” systems), then def X[0]
N+1(λ−
0 ) = 0 for every λ0 ∈R. Therefore,

266
5
Discrete Symplectic Eigenvalue Problems
in this case a ﬁnite eigenvalue reduces to the classical eigenvalue, which is
determined by the condition def X[0]
N+1(λ0) ≥1, i.e., by the singularity of
X[0]
N+1(λ0). The algebraic multiplicity is then equal to defX[0]
N+1(λ0); see, e.g.,
[44, Corollary 1].
(iv) When the dependence in λ is linear as in (5.238) or (5.241), we will discuss
the special features of the ﬁnite eigenvalues in Sect. 5.3.3.
The following is a simple consequence of Theorem 5.3 and Deﬁnition 5.5.
Corollary 5.7 Under (5.3), the ﬁnite eigenvalues of (E) are isolated.
5.1.2
Finite Eigenfunctions
In this subsection, we develop a geometric notion corresponding to the ﬁnite
eigenvalues from Deﬁnition 5.5. First observe that if λ0 is a ﬁnite eigenvalue of (E)
and c ∈Ker X[0]
N+1(λ0), then y := Y [0](λ0) c is a vector solution satisfying both
(Sλ0) and x0 = 0 = xN+1, i.e., y solves the problem (E) with λ = λ0. It remains to
describe which of these solutions are in a sense “degenerate,” that is, which of them
do not correspond to a ﬁnite eigenvalue λ0. This procedure has a parallel strategy in
the classical eigenvalue theory, where only the nontrivial solutions of (E) count as
the eigenfunctions for the eigenvalue λ0.
Deﬁnition 5.8 (Degenerate Solution) Let λ0 ∈R be given. A solution y of system
(Sλ0) is said to be degenerate at λ0 (or it is a degenerate solution), if there exists
δ > 0 such that for all λ ∈(λ0 −δ, λ0], the solution y(λ) of (SDSλ) given by the
initial conditions y0(λ) = y0 satisﬁes
k(λ) yk+1(λ) = 0
for all k ∈[0, N]Z.
(5.23)
In the opposite case, we say that the solution y is nondegenerate at λ0.
A degenerate solution at λ0 represents in fact a family of solutions y(λ) for λ ∈
(λ0−δ, λ0], which includes the solution y itself for λ = λ0. Moreover, this family of
solutions is independent of λ with respect to the semi-norm induced by the positive
semideﬁnite matrix k(λ), i.e.,
33 y(λ)
332
λ :=
N

k=0
yT
k+1(λ) k(λ) yk+1(λ)
for λ ∈(λ0 −δ, λ0].
Remark 5.9
(i) Since the matrix k(λ) is symmetric and since Sk(λ) and J are invertible, it
follows from (5.3) that condition (5.23) can be written in the equivalent form
˙ST
k (λ) J yk+1(λ) = 0
for all k ∈[0, N]Z.

5.1
Nonlinear Dependence on Spectral Parameter
267
(ii) When the dependence on λ is linear as in (5.238) and (5.241), a degenerate
solution y = (x, u) at λ0 is a solution of (Sλ0) satisfying
Wk xk+1 = 0
for all k ∈[0, N]Z.
(5.24)
Condition (5.24) is indeed equivalent to (5.23) where λ ∈(λ0 −δ, λ0], since
under (5.241) any solution of (Sλ0) satisfying (5.24) is at the same time a solution
of (SDSλ) for every λ ∈R. Hence, degeneracy condition (5.23) is a local property
when (SDSλ) depends on λ nonlinearly, but it is a global property for the linear
dependence on λ.
Consider the following spaces of solutions of system (Sλ0):
E(λ0) :=
!
y = (x, u) solves system (Sλ0) with x0 = 0 = xN+1
"
,
W(λ0) := !y = (x, u) ∈E(λ0), y is degenerate at λ0
".
Then it follows that
E(λ0) =
!
Y [0](λ0) c, c ∈Ker X[0]
N+1(λ0)
"
.
(5.25)
Indeed, the inclusion ⊇in (5.25) follows from the considerations at the beginning
of this section, while the inclusion ⊆in (5.25) is obtained from the uniqueness of
solutions of system (Sλ0)—every solution y ∈E(λ0) is of the form y = Y [0](λ0) c,
where c = u0 and where X[0]
N+1(λ0) c = 0. Our aim is to prove that the degenerate
solutions at λ0 correspond to those vectors c ∈Ker X[0]
N+1(λ0) which are in
Ker X[0]
N+1(λ−
0 ), i.e., we will prove in Theorem 5.11 below that
W(λ0) =
!
Y [0](λ0) c, c ∈Ker X[0]
N+1(λ−
0 )
"
.
(5.26)
In turn, the ﬁnite eigenfunctions are exactly the nondegenerate solutions at λ0.
Deﬁnition 5.10 (Finite Eigenfunction) Under (5.3), every nondegenerate solution
y at λ0 of (E) with λ = λ0 is called a ﬁnite eigenfunction corresponding to the ﬁnite
eigenvalue λ0, and the number
ω(λ0) := dim E(λ0) −dim W(λ0)
(5.27)
is called the geometric multiplicity of λ0.
The following result is a characterization of the ﬁnite eigenvalues of (E) with the
nonlinear dependence on λ.
Theorem 5.11 (Geometric
Characterization
of
Finite
Eigenvalues) Let
assumption (5.3) be satisﬁed. A number λ0 is a ﬁnite eigenvalue of (E) with
algebraic multiplicity θ(λ0) ≥1 deﬁned in (5.21) if and only if there exists

268
5
Discrete Symplectic Eigenvalue Problems
a corresponding ﬁnite eigenfunction y. In this case, the geometric multiplicity
of λ0 deﬁned in (5.27) is equal to its algebraic multiplicity, i.e., ω(λ0) = θ(λ0).
For the proof of Theorem 5.11, we need the following auxiliary result.
Lemma 5.12 Let Y(λ) = (X(λ), U(λ)) and ˜Y(λ) = ( ˜X(λ), ˜U(λ)) be normalized
conjoined bases of (SDSλ) such that they form the symplectic fundamental matrix
˜Zk(λ) =
Xk(λ) ˜Xk(λ)
Uk(λ) ˜Uk(λ)

(5.28)
of (SDSλ). Assume that ˜Z0(λ) ≡
˜Z0, i.e., it does not depend on λ, and that
Xk+1(λ0) is invertible for some k ∈[0, N]Z and λ0 ∈R. Then there exists ε > 0
such that
d
dλ [X−1
k+1(λ) ˜Xk+1(λ)] =
k

j=0
ζ T
k+1,j+1(λ) j(λ) ζ k+1,j+1(λ),
(5.29)
for all λ ∈(λ0 −ε, λ0 + ε), where
ζ k,j(λ) := ˜Zj(λ)
−X−1
k (λ) ˜Xk(λ)
I

.
(5.30)
Proof Using the assumption ˜Z0(λ) ≡˜Z0, Proposition 1.76(ii), and the deﬁnition of
( ˜Zk+1(λ)), we derive from (5.12) that
( ˜Z−1
k+1(λ)) = ˜ZT
k+1(λ) J ˙˜Zk+1(λ) = −k(λ),
k ∈[0, N]Z,
λ ∈R,
(5.31)
where
k(λ) :=
k

j=0
˜ZT
j+1(λ) j(λ) ˜Zj+1(λ).
(5.32)
Since Xk+1(λ0) is invertible, it follows that Xk+1(λ) is invertible on (λ0 −ε, λ0 +ε)
for some ε > 0, and then one can apply Lemma 1.77 with W(λ) := ˜Z−1
k+1(λ),
R := −J , P := I, and ˜W(λ) := J ˜Z−1
k+1(λ). By (1.200) and Proposition 1.76(i),
we derive (suppressing the argument λ and index k + 1)
J T ( ˜W(λ)) J =( ˜Z−1
k+1(λ))=−
XT 0
˜XT I
 d
dλ
−UX−1 −XT −1
−X−1
X−1 ˜X
X ˜X
0 I

.
Substituting the above representation for ( ˜Z−1
k+1(λ)) into formula (5.31), we derive
identity (5.29).
⊓⊔

5.1
Nonlinear Dependence on Spectral Parameter
269
Proof of Theorem 5.11 If we prove that equality (5.26) holds, then the result will
follow since the ﬁnite eigenfunctions y for λ0 will be of the form y = Y [0](λ0) c
with c ∈Ker X[0]
N+1(λ0) \ Ker X[0]
N+1(λ−
0 ).
Let y = (x, u) be a solution of (E) with λ = λ0, and assume that y is degenerate
at λ0. Let δ > 0 and y(λ) = (x(λ), u(λ)) for λ ∈(λ0 −δ, λ0] be the constant
and the corresponding family of solutions from Deﬁnition 5.8. By the uniqueness of
solutions of (SDSλ), we get
yk(λ) = Y [0]
k (λ) c
for all k ∈[0, N + 1]Z and λ ∈(λ0 −δ, λ0]
(5.33)
for some c ∈Rn, in fact for c = u0. From xN+1 = 0, we must necessarily
have c ∈Ker X[0]
N+1(λ0). By Theorem 5.3, we may assume that Ker X[0]
N+1(λ) ≡
Ker X[0]
N+1(λ−
0 ) is constant on (λ0 −δ, λ0). Then for every k ∈[0, N]Z and every
λ ∈(λ0 −δ, λ0], we have
yk+1(λ) = Sk(λ) yk(λ),
yk(λ) = −J ST
k (λ) J yk+1(λ).
(5.34)
By taking the derivative of this equation at λ ∈(λ0 −δ, λ0), resp., the left derivative
of this equation at λ = λ0, we obtain
˙yk+1(λ) = Sk(λ) ˙yk(λ) + ˙Sk(λ) yk(λ)
(5.34)
= Sk(λ) ˙yk(λ) + J k(λ) yk+1(λ)
(5.23)
=
Sk(λ) ˙yk(λ),
k ∈[0, N]Z, λ ∈(λ0 −δ, λ0].
(5.35)
In addition, since y0(λ) = y0 for every λ ∈(λ0 −δ, λ0], the initial conditions
of y(λ) do not depend on λ. Hence, ˙y0(λ) = 0 for all λ ∈(λ0 −δ, λ0]. By the
uniqueness of solutions of system (SDSλ), it follows from (5.35) that ˙yk(λ) = 0 for
all k ∈[0, N + 1]Z and λ ∈(λ0 −δ, λ0]. This means that the functions y(λ) do not
depend on λ on (λ0 −δ, λ0], i.e.,
yk(λ) ≡yk(λ0)
for all λ ∈(λ0 −δ, λ0] and all k ∈[0, N + 1]Z.
(5.36)
Therefore, for every λ ∈(λ0 −δ, λ0] and k ∈[0, N + 1]Z, we have
Y [0]
k (λ) c
(5.33)
= yk(λ)
(5.36)
=
yk(λ0) = yk = (xk, uk).
The endpoint condition xN+1 = 0 then yields X[0]
N+1(λ) c = 0 for all λ ∈(λ0 −
δ, λ0], i.e., c ∈Ker X[0]
N+1(λ) for all λ ∈(λ0 −δ, λ0]. And since Ker X[0]
N+1(λ) is
constant on (λ0 −δ, λ0), it follows that c ∈Ker X[0]
N+1(λ−
0 ).
Conversely, let c ∈Ker X[0]
N+1(λ−
0 ). Then c ∈Ker X[0]
N+1(λ) for all values
λ ∈(λ0 −ε, λ0] for some ε > 0. Let ˜Y(λ0) be a conjoined basis of (Sλ0) such
that ˜Y(λ0) and Y(λ0) are normalized and ˜XN+1(λ0) is invertible (similar to the

270
5
Discrete Symplectic Eigenvalue Problems
proof of Theorem 5.3). For each λ ∈R, let ˜Y(λ) be the conjoined basis of (SDSλ)
starting with the initial conditions ˜Y0(λ) = ˜Y0(λ0), so that these initial conditions
are independent of λ. Then ˜XN+1(λ) is invertible for all λ ∈(λ0 −δ, λ0] for some
δ ∈(0, ε). For j ∈[0, N + 1]Z and λ ∈(λ0 −δ, λ0], we now deﬁne the functions
yj(λ) := Y [0]
j (λ) c −˜Yj(λ) ˜X−1
N+1(λ) X[0]
N+1(λ) c = Y [0]
j (λ) c,
compared with ζN+1,j(λ) in (5.30). Then for every λ ∈(λ0 −δ, λ0], the function
z(λ) solves the system (SDSλ) with z0(λ) = (0, c). Since
cT ˜X−1
N+1(λ) X[0]
N+1(λ) c = 0
for all λ ∈(λ0 −δ, λ0],
(5.37)
differentiating equation (5.37) at any λ ∈(λ0 −δ, λ0], we get from Lemma 5.12, in
which Y(λ) := ˜Y(λ) and ˜Y(λ) := Y [0](λ), that
0
(5.37)
=
d
dλ cT ˜X−1
N+1(λ) X[0]
N+1(λ) c
(5.29)
=
N

j=0
yT
j+1(λ) j(λ) yj+1(λ)
for λ ∈(λ0 −δ, λ0]. Therefore, by (5.3), j(λ) yj+1(λ) = 0 for all j ∈[0, N]Z and
λ ∈(λ0 −δ, λ0], showing that y(λ0) = Y [0](λ0) c is a degenerate solution at λ0.
The proof is complete.
⊓⊔
Remark 5.13 The proof of Theorem 5.11 shows that, under (5.3), for any ﬁxed k ∈
[0, N]Z and any c ∈Ker Xk+1(λ−
0 ), there exists δ > 0 such that the function Yj(λ) c
is independent of λ ∈(λ0 −δ, λ0] for all j ∈[0, k + 1]Z, where Y(λ) is a conjoined
basis of (SDSλ) satisfying (5.20).
Remark 5.14 Of course, similar statements as above can be proven for functions
deﬁned in the right neighborhood of λ0, say for λ ∈[λ0, λ0 + δ). For example,
when c ∈Ker Xk+1(λ+
0 ), there exists δ > 0 such that Yj(λ) c is independent of
λ ∈[λ0, λ0 + δ) for all j ∈[0, k + 1]Z.
5.1.3
Oscillation Theorems for Constant Rank of Bk(λ)
In this section we establish the main results on the oscillation properties of
system (SDSλ) under the additional restriction (5.8), i.e.,
rankBk(λ) is constant for λ ∈R.
(5.38)
Later, in Sect. 5.6 we completely omit this assumption using the comparative index
tools presented in Chaps. 3 and 4.
Recall the deﬁnition of focal points and their multiplicities for conjoined bases
of (SDSλ). According to Deﬁnition 4.1, a conjoined basis Y(λ) = (X(λ), U(λ))

5.1
Nonlinear Dependence on Spectral Parameter
271
of (SDSλ) has a focal point in the real interval (k, k + 1] provided
mk(λ) := rankMk(λ) + ind Pk(λ) ≥1
(5.39)
and then the number mk(λ) is its multiplicity, where
Mk(λ) := [I −Xk+1(λ) X†
k+1(λ)] Bk(λ),
Tk(λ) := I −M†
k (λ) Mk(λ),
Pk(λ) := Tk(λ) Xk(λ) X†
k+1(λ) Bk(λ) Tk(λ),
⎫
⎪⎬
⎪⎭
(5.40)
and the matrix Pk(λ) is symmetric. By this deﬁnition, all algebraic properties of the
multiplicities of focal points mk formulated in Sect. 4.1.1 remain true for mk(λ) and
λ ∈R. For example, in the subsequent proofs, we will use that the matrix Mk(λ)
deﬁned in (5.40) satisﬁes
rank Mk(λ) = rankNk(λ),
Nk(λ) := [I −X†
k+1(λ) Xk+1(λ)] XT
k (λ),
(5.41)
which gives the number of focal points of the conjoined basis Y(λ) located at k + 1
in terms of Xk(λ) and Xk+1(λ) only, i.e., without explicitly appearing Bk(λ) (see
Deﬁnition 4.9 and Proposition 4.13).
Theorem 5.15 (Local Oscillation Theorem I) Assume (5.3). Let Y(λ) be a con-
joined basis of (SDSλ) with (5.20). Fix k ∈[0, N]Z and suppose (5.38). As in (5.39),
let mk(λ) denote the number of focal points of Y(λ) in (k, k + 1]. Then mk(λ−) and
mk(λ+) exist and for all λ ∈R
mk(λ+) = mk(λ) ≤n,
(5.42)
mk(λ+) −mk(λ−) = 

rank Xk(λ−) −rankXk(λ)

.
(5.43)
The proof of Theorem 5.15 will be presented in Sect. 5.1.6. Next we establish
further results based on Theorem 5.15. Denote by, including the multiplicities,
n1(λ) := the number of focal points of Y(λ) in (0, N + 1],
(5.44)
i.e., n1(λ) = l(Y(λ), 0, N + 1) according to the notation in (4.10).
Theorem 5.16 (Local Oscillation Theorem II) Assume that conditions (5.3)
and (5.38) hold for all k ∈[0, N]Z. Let Y(λ) be a conjoined basis of (SDSλ) such
that (5.20) holds. Then n1(λ−) and n1(λ+) exist and for all λ ∈R
n1(λ+) = n1(λ) ≤(N + 1) n < ∞,
(5.45)
n1(λ+) −n1(λ−) = rank XN+1(λ−) −rankXN+1(λ) ≥0.
(5.46)

272
5
Discrete Symplectic Eigenvalue Problems
Hence, the function n1(·) is nondecreasing on R, the limit
m :=
lim
λ→−∞n1(λ)
(5.47)
exists with m ∈[0, (N + 1) n]Z, so that for a suitable λ0 < 0 we have
n1(λ) ≡m,
rankXN+1(λ−) −rank XN+1(λ) ≡0,
λ ≤λ0.
(5.48)
Proof Since n1(λ) = N
k=0 mk(λ) for all λ ∈R with mk(λ) given in (5.39), the
statement in (5.45) follows directly from (5.42). The expression in (5.46) is then
a telescope sum of the expression in (5.43). This yields for all λ ∈R
n1(λ+) −n1(λ−) = rankXN+1(λ−) −rankXN+1(λ) −rank X0(λ−) + rank X0(λ).
But since by (5.20) the initial conditions of Y(λ) do not depend on λ, we have
rankX0(λ−) = rank X0(λ) for all λ ∈R, so that the statement in (5.46) follows.
From the two conditions (5.45) and (5.46), we then have that the function n1(·) is
nondecreasing on R. Since the values of n1(λ) are nonnegative integers, the limit
in (5.47) exists and m ∈N ∪{0}. Consequently, n1(λ) ≡m for all λ sufﬁciently
negative, say for all λ ≤λ0 for some λ0 < 0, so that n1(λ+) −n1(λ−) ≡0 for
λ ≤λ0. Applying (5.46) once more then yields the second equation in (5.48).
⊓⊔
Now we apply the above local oscillation theorem to the principal solution
Y [0](λ) = (X[0](λ), U[0](λ)) of (SDSλ) at k = 0. In this case we have from
system (SDSλ) and (5.40) that
Y [0]
0 (λ) =
0
I

,
Y [0]
1 (λ) =
B0(λ)
D0(λ)

,
M0(λ) = 0,
T0(λ) = I,
P0(λ) = 0.
This means that the principal solution Y [0](λ) has no forward focal points in the
interval (0, 1] for all λ ∈R, and hence, according to (5.44), we have
n1(λ) = the number of focal points of Y [0](λ) in (1, N + 1],
(5.49)
i.e., n1(λ) = l(Y [0](λ), 0, N + 1) according to (4.10). We also denote by, including
the multiplicities,
n2(λ) := the number of ﬁnite eigenvalues of (E) in (−∞, λ].
(5.50)
Then from this deﬁnition, we have
n2(λ+) = n2(λ),
n2(λ) −n2(λ−) = θ(λ)
for all λ ∈R,
(5.51)
i.e., the difference n2(λ) −n2(λ−) gives the number of ﬁnite eigenvalues at λ.

5.1
Nonlinear Dependence on Spectral Parameter
273
Theorem 5.17 (Global Oscillation Theorem) Assume that (5.3) and (5.38) hold
for all k ∈[0, N]Z. Then with the notation (5.44) and (5.50), we have for all λ ∈R
n1(λ+) = n1(λ) ≤Nn,
(5.52)
n2(λ+) = n2(λ) < ∞,
(5.53)
n2(λ+) −n2(λ−) = n1(λ+) −n1(λ−) ≥0,
(5.54)
and there exists m ∈[0, Nn]Z such that
n1(λ) = n2(λ) + m
for all λ ∈R.
(5.55)
Moreover, for a suitable λ0 < 0, we have
n2(λ) ≡0
and
n1(λ) ≡m
for all λ ≤λ0.
(5.56)
Proof Conditions (5.53) and (5.54) follow directly from (5.51) and (5.46). Since
both functions n1(·) and n2(·) are right-continuous and (5.54) holds, then they must
differ on R by a constant l ∈R. But by (5.48), we have n1(λ) ≡m for all λ ≤λ0
with m given in (5.47). Taking into account (5.49), we see that m ≤Nn and the
statement in (5.55) follows. From (5.55) we obtain in turn that n2(λ) ≡0 for all
λ ≤λ0.
⊓⊔
Corollary 5.18 Under the assumptions of Theorem 5.17, the ﬁnite eigenvalues
of (E) are bounded from below and from above.
Proof This result follows from (5.56), since n2(λ) ≡0 for all λ ≤λ0 means that
there are no ﬁnite eigenvalues of (E) in the interval (−∞, λ0].
⊓⊔
We note that the boundedness of the ﬁnite eigenvalues of (E) from above follows
from equation (5.55) and estimate (5.52), which is a consequence of the discreteness
of the problem. In the continuous time case, the number of focal points (and hence
the number of ﬁnite eigenvalues) can be unbounded from above; see, e.g., [205,
Theorems 7.6.3 and 7.7.1].
The next three subsections are devoted to the proof of Theorem 5.15. This
proof is based on a construction of suitable partitioned matrices and an auxiliary
symplectic system between the indices k and k +1. Note that the entire construction
is valid for an arbitrary coefﬁcient Bk(λ). Only at the end assumption (5.38) is
invoked in order to apply the index theorem in Corollary 1.86.
5.1.4
Construction of Auxiliary Conjoined Basis
In the construction below, we assume that Y(λ) is a given conjoined basis
of (SDSλ) whose initial conditions do not depend on λ, i.e., satisfying (5.20), and

274
5
Discrete Symplectic Eigenvalue Problems
condition (5.3) holds. We also ﬁx an index k ∈[0, N]Z and a number λ0 ∈R. By
Theorem 5.3, we know that (5.13) holds, so that
r := rank Xk+1(λ−
0 ) ≡rank Xk+1(λ)
for all λ ∈(λ0 −δ, λ0)
(5.57)
exists for some δ > 0. The number δ will also appear in the construction below.
Lemma 5.19 There exist orthogonal matrices P, Q ∈Rn×n such that
Q Xk+1(λ) P =
X11(λ) 0r×(n−r)
0
0n−r

, Q Uk+1(λ) P =
U11(λ)
0
U21(λ) U22

,
(5.58)
for all λ ∈(λ0 −δ, λ0], and the matrix
XT
11(λ) U11(λ)
is symmetric for all λ ∈(λ0 −δ, λ0].
(5.59)
Here X11(λ) ∈Rr×r is invertible for all λ ∈(λ0 −δ, λ0), and U22 ∈R(n−r)×(n−r)
is invertible.
Proof Let P1 ∈Rn×r and P2 ∈Rn×(n−r) be matrices whose columns form
orthonormal bases for Im XT
k+1(λ−
0 ) and Ker Xk+1(λ−
0 ), respectively. Then since
[Ker Xk+1(λ−
0 )]⊥
= Im XT
k+1(λ−
0 ), the matrix P
:= (P1, P2) ∈Rn×n is
orthogonal,
Im P2 = Ker Xk+1(λ−
0 ) ≡Ker Xk+1(λ)
for all λ ∈(λ0 −δ, λ0).
(5.60)
Xk+1(λ) P =

Xk+1(λ) P1, 0n×(n−r)

for all λ ∈(λ0 −δ, λ0].
(5.61)
Note that (5.61) indeed holds also at λ = λ0 by the continuity of Xk+1 in λ. Let
Q1 ∈Rr×n and Q2 ∈R(n−r)×n be matrices whose columns form orthonormal bases
for Im Xk+1(λ1) and Ker XT
k+1(λ1), respectively, for some ﬁxed λ1 ∈(λ0 −δ, λ0).
Then the matrix Q :=

Q1
Q2

∈Rn×n is orthogonal, Im QT
2 = Ker XT
k+1(λ1), and
Q Xk+1(λ1) P =
Q1
Q2

Xk+1(λ1)

P1, P2

=
X11 0r×(n−r)
0
0n−r

,
(5.62)
where X11 := Q1Xk+1(λ1) P1 ∈Rr×r has rank X11 = r, i.e., X11 is invertible.
Deﬁne now
X11(λ) X12(λ)
X21(λ) X22(λ)

:= Q Xk+1(λ) P,
λ ∈(λ0 −δ, λ0],
(5.63)
U11(λ) U12(λ)
U21(λ) U22(λ)

:= Q Uk+1(λ) P,
λ ∈(λ0 −δ, λ0],
(5.64)

5.1
Nonlinear Dependence on Spectral Parameter
275
so that by (5.61) and (5.62)
X12(λ) ≡0,
X22(λ) ≡0
for all λ ∈(λ0 −δ, λ0],
X11(λ1) = Q1Xk+1(λ1) P1 = X11 is invertible,
X21(λ1) = Q2Xk+1(λ1) P1 = 0.
Moreover, since the matrices P and Q are orthogonal, we have for all λ ∈(λ0 −
δ, λ0]
Xk+1(λ) = QT
X11(λ) 0
X21(λ) 0

PT, Uk+1(λ) = QT
U11(λ) U12(λ)
U21(λ) U22(λ)

PT.
(5.65)
Since Y(λ) is a conjoined basis of (SDSλ), the matrix XT
k+1(λ) Uk+1(λ) symmetric
and rank Yk+1(λ) = n for all λ ∈R. Hence, with λ = λ1, we have
XT
k+1(λ1) Uk+1(λ1) = P
XT
11U11(λ1) XT
11U12(λ1)
0
0

PT .
This implies that XT
11U12(λ1) = 0 and since X11 is invertible, U12(λ1) = 0. Now
from (5.64), we have U12(λ) = Q1Uk+1(λ) P2 and U22(λ) = Q2Uk+1(λ) P2 for
all λ ∈(λ0 −δ, λ0], which implies by Remark 5.13 that the functions U12(λ) and
U22(λ) do not depend on λ ∈(λ0 −δ, λ0]. Thus, since U12(λ1) = 0, we get
U12(λ) ≡0,
U22(λ) ≡U22(λ−
0 ) =: U22 ∈R(n−r)×(n−r)
for all λ ∈(λ0−δ, λ0].
This proves the second formula in (5.58). Moreover, since
n = rank

XT
k+1(λ1), UT
k+1(λ1)

= rank
XT
11 0r×(n−r) UT
11(λ1) UT
21(λ1)
0
0n−r
0(n−r)×r
UT
22

,
it follows that rank U22 = n−r, that is, U22 is invertible. Next, for λ ∈(λ0 −δ, λ0],
the matrix
XT
k+1(λ) Uk+1(λ)=P
XT
11(λ) U11(λ)+XT
21(λ) U21(λ) XT
21(λ) U22
0
0

PT
(5.66)
is symmetric, so that XT
21(λ) U22 ≡0 on (λ0 −δ, λ0]. But since U22 is invertible, we
get X21(λ) ≡0 on (λ0 −δ, λ0], showing through (5.65) that also the ﬁrst formula
in (5.58) holds. In addition, since rank Q Xk+1(λ) P = rank Xk+1(λ) ≡r for all
λ ∈(λ0 −δ, λ0) and rank X11(λ1) = rankX11 = r, we get from (5.58) that
rankX11(λ) ≡r on (λ0 −δ, λ0), and so X11(λ) is invertible for all λ ∈(λ0 −δ, λ0).
Finally, from (5.66) we obtain that
XT
k+1(λ) Uk+1(λ) = P
XT
11(λ) U11(λ) 0
0
0

PT

276
5
Discrete Symplectic Eigenvalue Problems
is symmetric for all λ ∈(λ0−δ, λ0]. From this we conclude that (5.59) holds, which
completes the proof.
⊓⊔
Corollary 5.20 In addition to Theorem 5.3(iv), we have
Xk+1(λ) X†
k+1(λ) ≡QT
Ir
0
0 0n−r

Q,
X†
k+1(λ) Xk+1(λ) ≡P
Ir
0
0 0n−r

PT
(5.67)
for all λ ∈(λ0 −δ, λ0)
Proof By Lemma 5.19 and Remark 1.60(ii), we have for all λ ∈(λ0 −δ, λ0]
Xk+1(λ) = QT
X11(λ) 0
0
0

PT ,
X†
k+1(λ) = P

X†
11(λ) 0
0
0

Q.
And since by Lemma 5.19 the matrix X11(λ) is invertible on (λ0 −δ, λ0), we
get (5.67). The proof is complete.
⊓⊔
Based on the result of Lemma 5.19, we deﬁne the matrices
˜Xk+1(λ) :=
X11(λ) 0r×(n−r)
0
0r

,
˜Uk+1(λ) :=
U11(λ)
0
U21(λ) U22

,
λ ∈R.
(5.68)
Then by (5.59) the matrix ˜XT
k+1(λ) ˜Uk+1(λ) is symmetric for all λ ∈(λ0 −δ, λ0].
In addition, by (5.58) we have
˜Xk+1(λ) = Q Xk+1(λ) P,
˜Uk+1(λ) = Q Uk+1(λ) P,
λ ∈(λ0 −δ, λ0],
(5.69)
which yields that
rank
 ˜XT
k+1(λ), ˜UT
k+1(λ)
 (5.69)
= rank

XT
k+1(λ), UT
k+1(λ)

= n,
λ ∈(λ0 −δ, λ0].
Next we construct for λ ∈(λ0 −δ, λ0] suitable matrices ˜Xk(λ) and ˜Uk(λ). First
we observe that by Remark 5.13 with j = k, the functions
Xk(λ) P2 and Uk(λ) P2 do not depend on λ ∈(λ0 −δ, λ0],
because Im P2 = Ker Xk+1(λ−
0 ). This implies that the number
ρ := rankXk(λ−
0 ) P2 ≡rankXk(λ) P2
for all λ ∈(λ0 −δ, λ0)
(5.70)

5.1
Nonlinear Dependence on Spectral Parameter
277
is well deﬁned. Note that by (1.142) and (5.41), the deﬁnition of ρ yields
ρ = rankP2 −dim

Ker Xk(λ) ∩Im P2

= def Xk+1(λ) −dim  Ker Xk(λ) ∩Ker Xk+1(λ)
= rankNk(λ)
(5.41)
=
rankMk(λ)
for all λ ∈(λ0 −δ, λ0). Later in this section, we will prove directly that the
number ρ = rankMk(λ−
0 ). In addition, the deﬁnition of Mk(λ), formula (5.67)(i),
and (1.142) yield that
ρ = rank MT
k (λ) = rank BT
k (λ) [I −Xk+1(λ) X†
k+1(λ)]
= rank [I −Xk+1(λ) X†
k+1(λ)] −dim

Ker BT
k (λ) ∩Im [I −Xk+1(λ) X†
k+1(λ)]

= n −r −dim

Ker BT
k (λ) ∩Ker XT
k+1(λ)

,
λ ∈(λ0 −δ, λ0).
(5.71)
Remark 5.21 From equation (5.71), we can see that ρ ≤n −r and that ρ = n −r
if and only if Ker BT
k (λ) ∩Ker XT
k+1(λ) = {0} on (λ0 −δ, λ0). This latter condition
is satisﬁed, e.g., when Bk(λ) or Xk+1(λ) is invertible on (λ0 −δ, λ0).
We now reﬁne the structure of the above matrices to partition Xk(λ) and Uk(λ).
Since ρ ≤rankXk(λ−
0 ), we may put
˜r := rk −ρ,
where rk := rank Xk(λ−
0 ).
(5.72)
Then since rank Xk(λ−
0 ) P2 = ρ, we must have rank Xk(λ−
0 ) P1 = rk −ρ = ˜r.
But the matrix Xk(λ−
0 ) P1 ∈Rn×r, which implies ˜r ≤r. The block structure of the
matrices below is such that their total dimension n is partitioned as
n = ˜r + (r −˜r) + (n −r −ρ) + ρ.
Lemma 5.22 There are orthogonal matrices P, Q, ˜Q satisfying Lemma 5.19 and
˜Q Xk(λ) P =
⎛
⎜⎜⎝
˜X11(λ)
0
0
0˜r×ρ
0
0r−˜r
0
0
0
0
0n−r−ρ
0
˜X41(λ) ˜X42
0
˜X44
⎞
⎟⎟⎠,
λ ∈(λ0 −δ, λ0],
(5.73)
where ˜X11(λ) ∈R˜r×˜r is invertible for all λ ∈(λ0 −δ, λ0) and ˜X44 ∈Rρ×ρ is
invertible.

278
5
Discrete Symplectic Eigenvalue Problems
Proof Let the matrices P and Q be from Lemma 5.19. Since Xk(λ) P2 ∈Rn×(n−r)
and rank Xk(λ) P2 = ρ for λ ∈(λ0 −δ, λ0), there are orthogonal matrices ˜Q ∈
Rn×n and ¯P2 ∈R(n−r)×(n−r) such that
˜Q Xk(λ) P2 ¯P2 =
⎛
⎜⎜⎜⎝
0˜r
0˜r×ρ
0
0(r−˜r)×ρ
0 0(n−r−ρ)×ρ
0
˜X44
⎞
⎟⎟⎟⎠,
λ ∈(λ0 −δ, λ0),
(5.74)
where
˜X44
∈
Rρ×ρ is invertible. Let λ1
∈
(λ0 −δ, λ0) be ﬁxed. Since
˜Q Xk(λ1) P1 ∈Rn×r, there are orthogonal matrices ¯Q1 ∈R(n−ρ)×(n−ρ) and
¯P1 ∈Rr×r such that
¯Q ˜Q Xk(λ1) P1 ¯P1 =
⎛
⎜⎜⎜⎝
˜X11(λ1)
0˜r×(r−˜r)
0
0r−˜r
0
0(n−r−ρ)×(r−˜r)
˜X41(λ1)
˜X42(λ1)
⎞
⎟⎟⎟⎠,
¯Q :=
 ¯Q1 0
0 Iρ

∈Rn×n,
where ˜X11(λ1) ∈R˜r×˜r is invertible, ˜X41(λ1) ∈Rρ×˜r, and ˜X42(λ1) ∈Rρ×(r−˜r).
Note that the multiplication of equation (5.74) by the orthogonal matrix ¯Q from
the left does not change the structure of (5.74). Therefore, since the product ¯Q ˜Q is
an orthogonal matrix, we may assume without loss of generality that the matrix ˜Q
in (5.74) is such that
˜Q Xk(λ1) P1 ¯P1 =
⎛
⎜⎜⎜⎝
˜X11(λ1)
0˜r×(r−˜r)
0
0r−˜r
0
0(n−r−ρ)×(r−˜r)
˜X41(λ1)
˜X42(λ1)
⎞
⎟⎟⎟⎠,
˜Q Xk(λ) P2 ¯P2 =
⎛
⎜⎜⎜⎝
0˜r
0˜r×ρ
0
0(r−˜r)×ρ
0 0(n−r−ρ)×ρ
0
˜X44
⎞
⎟⎟⎟⎠
for λ ∈(λ0 −δ, λ0) with both ˜X11(λ1) ∈R˜r×˜r and ˜X44 ∈Rρ×ρ invertible,
˜X41(λ1) ∈Rρ×˜r, and ˜X42(λ1) ∈Rρ×(r−˜r). Next we observe that the multiplication
of the equations in (5.58) by the orthogonal matrix
¯P :=

¯P1
0r×(n−r)
0(n−r)×r
¯P2

∈Rn×n

5.1
Nonlinear Dependence on Spectral Parameter
279
from the right does not change the structure of the formulas in (5.58). Therefore,
since the matrix P ¯P is orthogonal, we may assume without loss of generality that
the matrix P is such that
˜Q Xk(λ1) P1 =
⎛
⎜⎜⎜⎝
˜X11(λ1)
0˜r×(r−˜r)
0
0r−˜r
0
0(n−r−ρ)×(r−˜r)
˜X41(λ1)
˜X42(λ1)
⎞
⎟⎟⎟⎠,
˜Q Xk(λ) P2 =
⎛
⎜⎜⎜⎝
0˜r
0˜r×ρ
0
0(r−˜r)×ρ
0 0(n−r−ρ)×ρ
0
˜X44
⎞
⎟⎟⎟⎠
(5.75)
for all λ ∈(λ0−δ, λ0) with ˜X11(λ1) ∈R˜r×˜r and ˜X44 ∈Rρ×ρ invertible, ˜X41(λ1) ∈
Rρ×˜r, and ˜X42(λ1) ∈Rρ×(r−˜r). Therefore, for λ = λ1 we have from (5.75)
˜Q Xk(λ1) P =
⎛
⎜⎜⎜⎝
˜X11(λ1)
0˜r×(r−˜r)
0˜r
0˜r×ρ
0
0r−˜r
0
0(r−˜r)×ρ
0
0(n−r−ρ)×(r−˜r) 0n−r−ρ 0(n−r−ρ)×ρ
˜X41(λ1)
˜X42(λ1)
0
˜X44
⎞
⎟⎟⎟⎠
(5.76)
with ˜X11(λ1) ∈R˜r×˜r and ˜X44 ∈Rρ×ρ invertible. We now calculate the sets
Im K := Ker ˜Q Xk(λ1) P,
Im ˜K := Ker PT XT
k (λ1) ˜QT
(5.77)
for some matrices K and ˜K. Since rank ˜Q Xk(λ1) P = rankX(λ1) = rk =
˜r + ρ, we have def ˜Q Xk(λ1) P = n −˜r −ρ. And since rank ˜Q Xk(λ1) P =
rankPT XT
k (λ1) ˜QT , we have def PT XT
k (λ1) ˜QT = n −˜r −ρ as well. Therefore,
the matrices K, ˜K ∈Rn×(n−˜r−ρ). Denote within the reﬁned block structure
K =

Kij

,
˜K =
 ˜Kij

,
where i ∈{1, 2}, j ∈{1, 2, 3, 4}.
Then the ﬁrst equality in (5.77) yields through (5.76) that K11 = 0 and K12 = 0.
Then with the choice K21 := I, we get K41 = −˜X−1
44 ˜X42(λ1). With the additional
choice K22 := 0, K31 := 0, K32 := I, and K44 := 0, we obtain
K =
⎛
⎜⎜⎜⎝
0˜r×(r−˜r)
0˜r×(n−r−ρ)
Ir−˜r
0(r−˜r)×(n−r−ρ)
0(n−r−ρ)×(r−˜r)
In−r−ρ
−˜X−1
44 ˜X42(λ1)
0ρ×(n−r−ρ)
⎞
⎟⎟⎟⎠.

280
5
Discrete Symplectic Eigenvalue Problems
But since Im K = Ker ˜Q Xk(λ1) P is independent on λ ∈(λ0−δ, λ0), it follows that
the matrix ˜X42(λ) ≡˜X42 is constant on (λ0 −δ, λ0). Similarly, the second equality
in (5.77) yields ˜K41 = 0, ˜K42 = 0, and then ˜K11 = 0 and ˜K12 = 0. We then choose
˜K21 := I, ˜K22 := 0, ˜K31 := 0, and ˜K32 := I. Therefore we proved
K=
⎛
⎜⎜⎜⎝
0˜r×(r−˜r)
0˜r×(n−r−ρ)
Ir−˜r
0(r−˜r)×(n−r−ρ)
0(n−r−ρ)×(r−˜r)
In−r−ρ
−˜X−1
44 ˜X42
0ρ×(n−r−ρ)
⎞
⎟⎟⎟⎠, ˜K=
⎛
⎜⎜⎝
0˜r×(r−˜r)
0˜r×(n−r−ρ)
Ir−˜r
0(r−˜r)×(n−r−ρ)
0(n−r−ρ)×(r−˜r)
In−r−ρ
0ρ×(r−˜r)
0ρ×(n−r−ρ)
⎞
⎟⎟⎠.
Now from Theorem 5.3, the sets Ker Xk(λ) and Ker XT
k (λ) are constant on the
interval (λ0 −δ, λ0), then also the sets Ker ˜Q Xk(λ) P and Ker PT XT
k (λ) ˜QT are
constant on (λ0−δ, λ0), which together with (5.76) implies the equality in (5.73) for
λ ∈(λ0 −δ, λ0). And by the continuity of Xk(·), we also have the formula in (5.73)
at λ = λ0. Finally, from rk = rankXk(λ) = rank ˜Q Xk(λ) P, equation (5.76), and
the invertibility of ˜X44, we get rank ˜X11(λ) = ˜r for all λ ∈(λ0 −δ, λ0), i.e., ˜X11(λ)
is invertible on (λ0 −δ, λ0).
⊓⊔
Within the reﬁned block structure, we deﬁne for any λ ∈R the matrices
˜Xk(λ) := ˜Q Xk(λ) P =  ˜Xij (λ)
 ,
˜Uk(λ) := ˜Q Uk(λ) P =  ˜Uij(λ)
 ,
(5.78)
where i, j ∈{1, 2, 3, 4} and ˜Q, P ∈Rn×n are from Lemma 5.22. Then for λ ∈
(λ0 −δ, λ0], the matrix ˜Xk(λ) is given by formula (5.73).
Lemma 5.23 There are orthogonal matrices P, Q,
˜Q satisfying Lemmas 5.19
and 5.22 and
˜Q Uk(λ) P =
⎛
⎜⎜⎜⎜⎝
˜U11(λ) ˜U12(λ)
0
˜U14
˜U21(λ) ˜U22(λ)
0
˜U24
˜U31(λ) ˜U32(λ) ˜U33 ˜U34
˜U41(λ)
˜U42
0
˜U44
⎞
⎟⎟⎟⎟⎠
,
λ ∈(λ0 −δ, λ0],
(5.79)
where ˜U33 ∈R(n−r−ρ)×(n−r−ρ) is invertible.
Proof Since Uk(λ) P2 is independent of λ ∈(λ0 −δ, λ0] by Remark 5.13 with
j = k, the third and fourth block columns of ˜Uk(λ) are constant in λ ∈(λ0 −δ, λ0],
i.e., ˜Uij(λ) ≡˜Uij on (λ0 −δ, λ0] for i ∈{3, 4} and j ∈{1, 2, 3, 4}. Since
˜XT
k (λ) ˜Uk(λ)
(5.78)
=
PT XT
k (λ) Uk(λ) P,
λ ∈R,

5.1
Nonlinear Dependence on Spectral Parameter
281
the matrix ˜XT
k (λ) ˜Uk(λ) is symmetric for all λ ∈R. From this and (5.73), we
conclude that with ˜F1j(λ) := ˜XT
11(λ) ˜U1j(λ) + ˜XT
41(λ) ˜U4j(λ), j ∈{1, 2, 3, 4},
the matrix
⎛
⎜⎜⎜⎜⎝
˜F11(λ)
˜F12(λ)
˜F13(λ)
˜F14(λ)
˜XT
42 ˜U41(λ) ˜XT
42 ˜U42(λ) ˜XT
42 ˜U43 ˜XT
42 ˜U44
0
0
0
0
˜XT
44 ˜U41(λ) ˜XT
44 ˜U42(λ) ˜XT
44 ˜U43 ˜XT
44 ˜U44
⎞
⎟⎟⎟⎟⎠
is symmetric for λ ∈(λ0 −δ, λ0].
Then since ˜X44 is invertible, we obtain ˜U43 = 0, and in turn since ˜X11(λ) is
invertible for λ ∈(λ0 −δ, λ0), we have ˜U13 = 0. Next, the equality ˜XT
44 ˜U42(λ) =
( ˜XT
42 ˜U44)T yields that ˜U42(λ) ≡˜XT −1
44
˜UT
44 ˜X42 =: ˜U42 is constant on (λ0 −δ, λ0].
Next, for every λ ∈(λ0 −δ, λ0]
n=rank

XT
k (λ), UT
k (λ)

= rank
 ˜XT
k (λ), ˜UT
k (λ)

=rank
⎛
⎜⎜⎜⎜⎝
˜XT
11(λ)
0
0
˜XT
41(λ) ˜UT
11(λ) ˜UT
21(λ) ˜UT
31(λ) ˜UT
41(λ)
0
0r−˜r
0
˜XT
42
˜UT
12(λ) ˜UT
22(λ) ˜UT
32(λ)
˜UT
42
0
0
0n−r−ρ
0
0
˜UT
23
˜UT
33
0
0
0
0
˜XT
44
˜UT
14
˜UT
24
˜UT
34
˜UT
44
⎞
⎟⎟⎟⎟⎠
,
where ˜U23 ∈R(r−˜r)×(n−r−ρ), ˜U33 ∈R(n−r−ρ)×(n−r−ρ), and rank( ˜UT
23,
˜UT
33) =
n −r −ρ. Hence, there exists an orthogonal matrix ¯R2 ∈R(n−˜r−ρ)×(n−˜r−ρ) with
¯R2
 ˜U23
˜U33

=

0(r−˜r)×(n−r−ρ)
¯U33

,
where ¯U33 ∈R(n−r−ρ)×(n−r−ρ) is invertible. Now the multiplication of ˜Xk(λ) and
˜Uk(λ) by the orthogonal matrix
¯R :=
⎛
⎝
I˜r 0
0
0 ¯R2 0
0 0 Iρ
⎞
⎠∈Rn×n
from the left does not change the structure of ˜Xk(λ) and ˜Uk(λ) in (5.78). Therefore,
since the matrix ¯R ˜Q is orthogonal, we may assume without loss of generality that
the matrix ˜Q in (5.78) is such that equation (5.79) holds. The proof of this lemma is
complete.
⊓⊔

282
5
Discrete Symplectic Eigenvalue Problems
Finally, we ﬁnish the construction of ˜Uk+1(λ) within the reﬁned matrix block
structure.
Lemma 5.24 There are orthogonal matrices P, Q,
˜Q satisfying Lemmas 5.19
and 5.22 and 5.23, and such that the matrix U22 ∈R(n−r)×(n−r) in (5.58) satisﬁes
U22 =
 ¯U33 ¯U34
0
¯U44

,
U−1
22 =
 ¯U−1
33 −¯U−1
33
¯U34 ¯U−1
44
0
¯U−1
44

,
(5.80)
where the matrices ¯U33 ∈R(n−r−ρ)×(n−r−ρ) and ¯U44 ∈Rρ×ρ are invertible.
Proof There exists an orthogonal matrix ¯S2 ∈R(n−r)×(n−r) such that the matrix
¯S2 U22 is upper block-triangular, i.e., we have
¯S2 U22 =
 ¯U33 ¯U34
0
¯U44

with ¯U33 ∈R(n−r−ρ)×(n−r−ρ) and ¯U44 ∈Rρ×ρ invertible, because U22 is invertible.
The multiplication of the formulas in (5.68) by the orthogonal matrix
¯S :=

Ir 0
0 ¯S2

∈Rn×n
from the left does not change the structure of (5.68), and the matrix ¯SQ is
orthogonal. Hence, we may assume without loss of generality that the matrix Q
in Lemmas 5.19, 5.22, and 5.23 satisﬁes the ﬁrst equation in (5.80). The formula for
the inverse of U22 is then veriﬁed by a direct calculation.
⊓⊔
5.1.5
Construction of Auxiliary Symplectic System
Recall that the fact that Sk(λ) is symplectic implies by Lemma 1.58(iii) that ST
k (λ)
is symplectic as well. Therefore, the coefﬁcients of system (SDSλ) satisfy for every
k ∈[0, N]Z and λ ∈R the identities
AT
k (λ) Ck(λ) = CT
k (λ) Ak(λ),
BT
k (λ) Dk(λ) = DT
k (λ) Bk(λ),
Ak(λ) BT
k (λ) = Bk(λ) AT
k (λ),
Dk(λ) CT
k (λ) = Ck(λ) DT
k (λ),
AT
k (λ) Dk(λ) −CT
k (λ) Bk(λ) = I,
Dk(λ) AT
k (λ) −Ck(λ) BT
k (λ) = I.
⎫
⎪⎬
⎪⎭
(5.81)

5.1
Nonlinear Dependence on Spectral Parameter
283
Upon differentiating the above formulas with respect to λ, we get
˙AT
k (λ) Ck(λ) + AT
k (λ) ˙Ck(λ) = ˙CT
k (λ) Ak(λ) + CT
k (λ) ˙Ak(λ),
˙BT
k (λ) Dk(λ) + BT
k (λ) ˙Dk(λ) = ˙DT
k (λ) Bk(λ) + DT
k (λ) ˙Bk(λ),
˙AT
k (λ) Dk(λ) + AT
k (λ) ˙Dk(λ) = ˙CT
k (λ) Bk(λ) + CT
k (λ) ˙Bk(λ),
˙Dk(λ) CT
k (λ) + Dk(λ) ˙CT
k (λ) = ˙Ck(λ) DT
k (λ) + Ck(λ) ˙DT
k (λ),
˙Ak(λ) BT
k (λ) + Ak(λ) ˙BT
k (λ) = ˙Bk(λ) AT
k (λ) + Bk(λ) ˙AT
k (λ),
˙Dk(λ) AT
k (λ) + Dk(λ) ˙AT
k (λ) = ˙Ck(λ) BT
k (λ) + Ck(λ) ˙BT
k (λ).
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.82)
For the matrix k(λ) in (5.3), we have
k(λ) =
 ˙Dk(λ) CT
k (λ) −˙Ck(λ) DT
k (λ) ˙Ck(λ) BT
k (λ) −˙Dk(λ) AT
k (λ)
˙Ak(λ) DT
k (λ) −˙Bk(λ) CT
k (λ) ˙Bk(λ) AT
k (λ) −˙Ak(λ) BT
k (λ)

.
(5.83)
One can now see the symmetry of k(λ) directly from (5.82).
With the aid of the orthogonal matrices P, Q, and ˜Q from Lemma 5.24, we now
construct an auxiliary symplectic system between the indices k and k + 1. For every
λ ∈R, we deﬁne the n × n matrices
˜Ak(λ) := Q Ak(λ) ˜QT =
A11(λ) A12(λ)
A21(λ) A22(λ)

=
⎛
⎜⎜⎜⎜⎝
¯A11(λ) ¯A12(λ) ¯A13(λ) ¯A14(λ)
¯A21(λ) ¯A22(λ) ¯A23(λ) ¯A24(λ)
¯A31(λ) ¯A32(λ) ¯A33(λ) ¯A34(λ)
¯A41(λ) ¯A42(λ) ¯A43(λ) ¯A44(λ)
⎞
⎟⎟⎟⎟⎠
and similarly
˜Bk(λ) := Q Bk(λ) ˜QT = 
Bij (λ)

i,j∈{1,2} =  ¯Bij (λ)

i,j∈{1,2,3,4} ,
˜Ck(λ) := Q Ck(λ) ˜QT = 
Cij(λ)

i,j∈{1,2} =  ¯Cij (λ)

i,j∈{1,2,3,4} ,
˜Dk(λ) := Q Dk(λ) ˜QT = 
Dij (λ)

i,j∈{1,2} =  ¯Dij(λ)

i,j∈{1,2,3,4} ,
where the matrices Aij(λ), Bij(λ), Cij (λ), Dij (λ) are formed within the block
structure with n = r + (n −r) as in Lemma 5.19 and the matrices ¯Aij(λ),
¯Bij(λ), ¯Cij(λ), ¯Dij (λ) are formed within the reﬁned block structure with n =
˜r + (r −˜r) + (n −r −ρ) + ρ as in Lemmas 5.22 and 5.23. Then the 2n × 2n
matrix
˜Sk(λ) :=
 ˜Ak(λ) ˜Bk(λ)
˜Ck(λ) ˜Dk(λ)

=
Q 0
0 Q

Sk(λ)
 ˜QT
0
0
˜QT

(5.84)

284
5
Discrete Symplectic Eigenvalue Problems
is symplectic as a product of three symplectic matrices; see Lemma 1.58(i) and
Remark 1.59. Consequently, the matrices ˜Xk(λ), ˜Uk(λ), ˜Xk+1(λ), ˜Uk+1(λ) given
in (5.78) and (5.68) satisfy for λ ∈(λ0 −δ, λ0] the symplectic system
˜Xk+1(λ) = ˜Ak(λ) ˜Xk(λ) + ˜Bk(λ) ˜Uk(λ),
˜Uk+1(λ) = ˜Ck(λ) ˜Xk(λ) + ˜Dk(λ) ˜Uk(λ),

(5.85)
as well as, by (2.10), the time-reversed equations
˜Xk(λ) = ˜DT
k (λ) ˜Xk+1(λ) −˜BT
k (λ) ˜Uk+1(λ),
˜Uk(λ) = −˜CT
k (λ) ˜Xk+1(λ) + ˜AT
k (λ) ˜Uk+1(λ).

(5.86)
It follows that
 ˜Yj(λ)

j∈{k,k+1} = ( ˜Xj(λ), ˜Uj(λ))j∈{k,k+1} is a conjoined basis of
system (5.85). Next, we deﬁne for λ ∈R the symmetric 2n × 2n matrix
˜k(λ) := J
d
dλ [ ˜Sk(λ)] J ˜ST
k (λ) J
(5.84)
=
Q 0
0 Q

k(λ)
QT
0
0 QT
 (5.3)
≥0.
(5.87)
Now we analyze the structure of the coefﬁcients of system (5.85).
Lemma 5.25 Given the above coefﬁcients
˜Ak(λ), ˜Bk(λ), ˜Ck(λ), ˜Dk(λ) and the
matrices ˜Xk(λ), ˜Uk(λ) in (5.78) and ˜Xk+1(λ), ˜Uk+1(λ) in (5.68), we have for all
λ ∈(λ0 −δ, λ0]
˜Bk(λ)=
 B11(λ) B12(λ)
0(n−r)×r
B22

, B12(λ)=
0 ¯B14(λ)
0 ¯B24(λ)

, B22 =
0n−r−ρ 0
0
¯B44

,
(5.88)
where ¯B44 ∈Rρ×ρ is invertible,
˜Dk(λ)=
D11(λ)D12(λ)
D21(λ)D22(λ)

, D12(λ)=
0 ¯D14(λ)
0 ¯D24(λ)

, D22(λ)=
 ¯D33 ¯D34(λ)
0
¯D44(λ)

,
(5.89)
where ¯D33 ∈R(n−r−ρ)×(n−r−ρ) is invertible,
˜Ak(λ)=
A11(λ)A12(λ)
A21
A22

, A21 =
 0
0
¯A41 ¯A42

, A22(λ)=
 ¯A33 0
¯A43 ¯A44

,
(5.90)
and where ¯A33 ∈R(n−r−ρ)×(n−r−ρ) and ¯A44 ∈Rρ×ρ are invertible. Moreover,
Y11(λ) :=
 ˜X11(λ)
0
0
0r−˜r

= DT
11(λ) X11(λ) −BT
11(λ) U11(λ),
λ ∈(λ0 −δ, λ0],
(5.91)

5.1
Nonlinear Dependence on Spectral Parameter
285
and the matrices B11(λ), D11(λ) ∈Rr×r satisfy for all λ ∈(λ0 −δ, λ0]
BT
11(λ) D11(λ) is symmetric and rank
BT
11(λ), DT
11(λ)
 = r.
(5.92)
Proof The ﬁrst equation in (5.86) for λ ∈(λ0 −δ, λ0] yields in its diagonal blocks
and in the right upper block, respectively, the equations
 ˜X11(λ) 0
0
0

= DT
11(λ) X11(λ) −

BT
11(λ) U11(λ) + BT
21(λ) U21(λ)

,
0
0
0 ˜X44

= −BT
22(λ) U22,
0 = −BT
21(λ) U22.
Since U22 is invertible, we obtain B21(λ)
≡
0 on (λ0 −δ, λ0], and then
equation (5.91) is satisﬁed. Next, by (5.80), for every λ ∈(λ0 −δ, λ0], we have
0
0
0 ˜X44

= −
 ¯BT
33(λ) ¯BT
43(λ)
¯BT
34(λ) ¯BT
33(λ)
  ¯U33 ¯U34
0
¯U44

.
Since ¯U33 is invertible, this yields that ¯B33(λ) ≡0 and ¯B34(λ) ≡0 on (λ0 −δ, λ0],
and then since ¯U44 is invertible, ¯B43(λ) ≡0 on (λ0 −δ, λ0]. Finally, the equation
˜X44 = −¯BT
44(λ) ¯U44 on (λ0 −δ, λ0] implies that ¯B44(λ) ≡−¯UT −1
44
˜XT
44 =: ¯B44 ∈
Rρ×ρ is constant and invertible on (λ0 −δ, λ0] and B22(λ) ≡B22 is constant on
(λ0 −δ, λ0]. Next, the ﬁrst equation in (5.85), in particular its third column in the
reﬁned block structure, yields through (5.68), (5.73), and (5.79) that ¯B13(λ) ˜U33 = 0
and ¯B23(λ) ˜U33 = 0 hold on (λ0 −δ, λ0]. Since ˜U33 is invertible, we get ¯B13(λ) ≡0
and ¯B23(λ) ≡0 on (λ0 −δ, λ0]. Therefore, we showed that for all λ ∈(λ0 −δ, λ0]
equation (5.88) holds with the matrix ¯B44 ∈Rρ×ρ invertible.
Next, from the second equation in (5.85), in particular from its third column
in the reﬁned block structure, we get via (5.68), (5.73), (5.79), and (5.80) that
¯D13(λ) ˜U33 = 0, ¯D23(λ) ˜U33 = 0, ¯D33(λ) ˜U33 = ¯U33, and ¯D43(λ) ˜U33 = 0 on
(λ0 −δ, λ0]. And since ˜U33 is invertible, it follows that ¯D13(λ) ≡0, ¯D23(λ) ≡0,
¯D33(λ) ≡¯U33 ˜U−1
33
=:
¯D33, and ¯D43(λ) ≡0 on (λ0 −δ, λ0]. This shows that
equality (5.89) holds.
Since the matrix ˜Sk(λ) is symplectic, we have from (1.145) that
˜BT
k (λ) ˜Dk(λ) =

BT
11(λ) D11(λ)
BT
11(λ) D12(λ)
BT
12(λ) D11(λ) + BT
22 D21(λ) BT
12(λ) D12(λ) + BT
22 D22(λ)

is symmetric for all λ ∈(λ0 −δ, λ0]. This implies that BT
11(λ) D11(λ) is symmetric
for λ ∈(λ0 −δ, λ0] and that
BT
11(λ) D12(λ) =

BT
12(λ) D11(λ) + BT
22 D21(λ)
T ,
λ ∈(λ0 −δ, λ0].
(5.93)

286
5
Discrete Symplectic Eigenvalue Problems
By extracting the second column of (5.93) in the reﬁned block structure, we get
BT
11(λ)
 ¯D14(λ)
¯D24(λ)

= DT
11(λ)
 ¯B14(λ)
¯B24(λ)

+
 ¯DT
41(λ)
¯DT
24(λ)

¯B44,
λ ∈(λ0 −δ, λ0].
And since ¯B44 is invertible, it follows that
 ¯DT
41(λ)
¯DT
24(λ)

∈Im 
BT
11(λ), DT
11(λ)
 ,
λ ∈(λ0 −δ, λ0].
(5.94)
From the identity
˜AT
k (λ) ˜Dk(λ) −˜CT
k (λ) ˜Bk(λ) = I for all λ ∈R, compare
with (5.81), we have rank ˜BT
k (λ),
˜DT
k (λ) = n, so that by (5.88) and (5.89) and
the invertibility of ¯B44 and ¯D33
n = rank
⎛
⎜⎜⎜⎜⎝
¯BT
11(λ) ¯BT
21(λ) 0 0
¯DT
11(λ) ¯DT
21(λ) ¯DT
31(λ)
¯DT
41(λ)
¯BT
12(λ) ¯BT
22(λ) 0 0
¯DT
12(λ) ¯DT
22(λ) ¯DT
32(λ)
¯DT
42(λ)
0
0
0 0
0
0
¯DT
33
0
¯BT
14(λ) ¯BT
24(λ) 0 ¯BT
44 ¯DT
14(λ) ¯DT
24(λ) ¯DT
34(λ) ¯DT
441(λ)
⎞
⎟⎟⎟⎟⎠
= rank
⎛
⎜⎜⎜⎜⎝
¯BT
11(λ) ¯BT
21(λ) 0 0
¯DT
11(λ) ¯DT
21(λ)
0
¯DT
41(λ)
¯BT
12(λ) ¯BT
22(λ) 0 0
¯DT
12(λ) ¯DT
22(λ)
0
¯DT
42(λ)
0
0
0 0
0
0
In−r−ρ
0
0
0
0 Iρ
0
0
0
0
⎞
⎟⎟⎟⎟⎠
.
(5.95)
for λ ∈(λ0 −δ, λ0]. If we now interchange in (5.95) the third and seventh columns
and use condition (5.94), we obtain from (5.95) that for λ ∈(λ0 −δ, λ0]
n = rank

BT
11(λ) 0r×(n−r) DT
11(λ) 0r×(n−r)
0(n−r)×r
In−r
0(n−r)×r
0n−r

= rank 
BT
11(λ), DT
11(λ)
 + n −r.
Therefore, rankBT
11(λ), DT
11(λ) = r and condition (5.92) is established.
It remains to prove (5.90). From the second equation in (5.86), in particular from
its third and fourth columns in the reﬁned block structure, we have
AT
21(λ) U22 =

0 ˜U14
0 ˜U24

,
AT
22(λ) U22 =
 ˜U33 ˜U34
0
˜U44

,
λ ∈(λ0 −δ, λ0].
Since by Lemma 5.24 the matrix U22 is invertible, it follows from (5.80) that
A21(λ) ≡A21 and A22(λ) ≡A22 are constant on (λ0 −δ, λ0] and that on this

5.1
Nonlinear Dependence on Spectral Parameter
287
interval ¯A31(λ) ≡0, ¯A32(λ) ≡0, ¯A41(λ) ≡
¯UT −1
44
˜UT
14 =:
¯A41, ¯A42(λ) ≡
¯UT −1
44
˜UT
24 =:
¯A42, ¯A33(λ) ≡
¯UT −1
33
˜UT
33 =:
¯A33 is invertible, ¯A34(λ) ≡0,
¯A43(λ) ≡
¯A43 is constant, and ¯A44(λ) ≡
¯UT −1
44
˜UT
44 =:
¯A44 is invertible. The
proof is complete.
⊓⊔
Following Lemma 5.25, let us now analyze the structure of the matrix ˜k(λ)
in (5.87). By (5.83) we have for any λ ∈R
˜k(λ) =
 ˙˜Dk(λ) ˜CT
k (λ) −˙˜Ck(λ) ˜DT
k (λ) ˙˜Ck(λ) ˜BT
k (λ) −˙˜Dk(λ) ˜AT
k (λ)
˙˜Ak(λ) ˜DT
k (λ) −˙˜Bk(λ) ˜CT
k (λ) ˙˜Bk(λ) ˜AT
k (λ) −˙˜Ak(λ) ˜BT
k (λ)

=:
 ˜Hk(λ) ˜Gk(λ)
˜Fk(λ) ˜Ek(λ)

,
(5.96)
and ˜k(λ) is symmetric. Then the matrices ˜Hk(λ) and ˜Ek(λ) are symmetric as well.
For brevity, we introduce the following notation for the row block columns of the
matrices ˜Ak(λ), ˜Bk(λ), ˜Ck(λ), ˜Dk(λ) as
A1(λ) :=

A11(λ), A12(λ)

,
A2(λ) :=

A21(λ), A22(λ)

,
B1(λ) :=

B11(λ), B12(λ)

,
B2(λ) :=

B21(λ), B22(λ)

,
C1(λ) :=

C11(λ), C12(λ)

,
C2(λ) :=

C21(λ), C22(λ)

,
D1(λ) :=

D11(λ), D12(λ)

,
D2(λ) :=

D21(λ), D22(λ)

.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.97)
Then A1(λ) ∈Rr×n and A2(λ) ∈R(n−r)×n and similarly for the other matrices
above. Then, by the form of the coefﬁcients ˜Ak(λ), ˜Bk(λ), ˜Dk(λ) in (5.88)–(5.90),
we obtain
˜Hk(λ) =
H11(λ) H12(λ)
H21(λ) H22(λ)

,
˜Gk(λ) =
G11(λ)
0
G21(λ) 0n−r

,
˜Fk(λ) =
F11(λ) F12(λ)
0
0n−r

,
˜Ek(λ) =
E11(λ)
0
0
0n−r

,
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
(5.98)
for all λ ∈(λ0 −δ, λ0], where
Hij(λ) := ˙Di(λ) CT
j (λ) −˙Ci(λ) DT
j (λ),
Gij(λ) := ˙Ci(λ) BT
j (λ) −˙Di(λ) AT
j (λ),
Fij(λ) := ˙Ai(λ) DT
j (λ) −˙Bi(λ) CT
j (λ),
Eij(λ) := ˙Bi(λ) AT
j (λ) −˙Ai(λ) BT
j (λ),
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
i, j ∈{1, 2},
(5.99)
and where Hii(λ) and Eii(λ) are symmetric and GT
ij(λ) = Fji(λ) for i, j ∈{1, 2}.

288
5
Discrete Symplectic Eigenvalue Problems
Our next aim is to analyze the behavior of the r × r matrix P11(λ) deﬁned by
P11(λ) := BT
11(λ) D11(λ) −BT
11(λ) Q11(λ) B11(λ),
λ ∈(λ0 −δ, λ0),
(5.100)
Q11(λ) := U11(λ) X−1
11 (λ),
λ ∈(λ0 −δ, λ0),
(5.101)
where X11(λ) and U11(λ) are from (5.58) and B11(λ), D11(λ) are from (5.88),
(5.89). For this we need to know ﬁrst the behavior of the function Q11(λ) on
(λ0 −δ, λ0). We deﬁne the n × n matrix
˜Lk(λ) := PT Lk(λ) P,
Lk(λ) :=

In, 0n

k(λ)

In, 0n
T ,
(5.102)
where k(λ) is deﬁned in (5.32).
Lemma 5.26 The symmetric matrix Q11(λ) deﬁned in (5.101) satisﬁes
˙Q11(λ) = −
XT −1
11
(λ), 0r×(n−r)
 ˜Lk(λ)
 X−1
11 (λ)
0(n−r)×r

,
λ ∈(λ0 −δ, λ0).
(5.103)
Consequently, under (5.3), the function Q11(·) is nonincreasing on (λ0 −δ, λ0).
Proof We suppress the argument λ. First note that by (5.31)
XT
k+1 ˙Uk+1 −UT
k+1 ˙Xk+1 =

I 0
 ˜ZT
k+1 J ˙˜Zk+1

I 0
T = −Lk(λ)
(5.104)
on R. From this and from
d
dλ X−1
11 = −X−1
11 ˙X11X−1
11 , we conclude that on (λ0 −
δ, λ0)
˙Q11
(5.59)
=
XT −1
11
(XT
11 ˙U11 −UT
11 ˙X11) X−1
11
(5.68)
=

XT −1
11 , 0r×(n−r)
 
 ˜XT
k+1
d
dλ ˜Uk+1 −˜UT
k+1
d
dλ ˜Xk+1
 
XT −1
11 , 0r×(n−r)
T
(5.69)
=

XT −1
11
, 0

PT [XT
k+1 ˙Uk+1 −UT
k+1 ˙Xk+1] P

XT −1
11
, 0
T
(5.104)
=
−
XT −1
11
, 0
 ˜Lk

XT −1
11
, 0
T ,
which shows the result in (5.103). Under (5.3) we then get ˙Q11(λ) ≤0 for all
λ ∈(λ0 −δ, λ0). Hence, the function Q11(·) is nonincreasing on (λ0 −δ, λ0).
⊓⊔

5.1
Nonlinear Dependence on Spectral Parameter
289
Remark 5.27 If Xk+1(λ) is invertible on (λ0 −δ, λ0), then r = n, and so no
construction of the matrices P and Q in Lemma 5.19 is needed. In this case we
get from (5.103), (5.102), and (5.31) the following identity for λ ∈(λ0 −δ, λ0)
˙Qk+1(λ) = −XT −1
k+1 (λ)
+
k

j=0
Y T
j+1(λ) j(λ) Yj+1(λ)
,
X−1
k+1(λ),
where Qk+1(λ) := Uk+1(λ) X−1
k+1(λ).
We will see in Lemma 5.28 that the behavior of the matrix P11(λ) is determined
by the behavior of Q11(λ). Namely, since Q11(λ) is nonincreasing, and hence the
function −BT
11(λ) Q11(λ) B11(λ) appearing in P11(λ) is nondecreasing, then this
behavior is not destroyed by the additional term BT
11(λ) D11(λ). This is similar as in
the linear case in (5.238) with (5.241), in which Q11(λ) is (of course) nonincreasing
and
P11(λ) = BT
11D11 −λ BT
11W11B11 −BT
11 Q11(λ) B11
is nondecreasing, even though W11 ≥0, see [102, pg. 601] and [211, Proposi-
tion 4.1].
Lemma 5.28 The symmetric matrix P11(λ) deﬁned in (5.100) satisﬁes
˙P11(λ) = BT
11(λ) ϒ11(λ) B11(λ) + RT
11(λ) E11(λ) R11(λ),
λ ∈(λ0 −δ, λ0),
(5.105)
where the matrix E11(λ) ∈Rr×r is deﬁned in (5.99) and
ϒ11(λ) :=

XT −1
11
(λ), 0
 ˜Lk−1(λ)

XT −1
11
(λ), 0
T ∈Rr×r,
(5.106)
R11(λ) := Q11(λ) B11(λ) −D11(λ) ∈Rr×r,
(5.107)
and where the matrix ˜Lk−1(λ) is given by (5.102) with the index k−1. Consequently,
under (5.3) the function P11(·) is nondecreasing on (λ0 −δ, λ0).
Proof We suppress the argument λ in this proof. First note that by (5.96), (5.98),
and the form of ˜Xk+1 and ˜Uk+1 in (5.68)
11 :=

XT −1
11
, 0
 ˜Y T
k+1 ˜k ˜Yk+1

XT −1
11
, 0
T
(5.68)
=

Ir, 0(n−r)×r, Q11, QT
21
 ˜k

Ir, 0(n−r)×r, Q11, QT
21
T
(5.96), (5.98)
=

I, Q11
 H11 G11
F11 E11
  I
Q11

,
(5.108)

290
5
Discrete Symplectic Eigenvalue Problems
where Q11 = U11X−1
11 as above, Q21 := U21X−1
11 , and H11, G11, F11, E11 are given
in (5.99), implying that H11 and E11 are symmetric and F11 = GT
11. Now we use
Lemma 5.26 to get
−˙Q11
(5.103)
=

XT −1
11 , 0
  ˜Lk−1 + ˜Y T
k+1 ˜k ˜Yk+1
 
XT −1
11 , 0
T
(5.106), (5.108)
=
ϒ11 + 11
on (λ0 −δ, λ0).
(5.109)
After these preparatory calculations, we have from (5.100)
˙P11 = ˙BT
11 D11 + BT
11 ˙D11 −˙BT
11 Q11 B11 −BT
11 ˙Q11 B11 −BT
11 Q11 ˙B11
(5.109)
=
BT
11 ϒ11 B11 + S11,
(5.110)
where the matrix S11 is deﬁned by
S11 := ˙BT
11 D11+BT
11 ˙D11−˙BT
11 Q11 B11−BT
11 Q11 ˙B11+BT
11 11 B11.
(5.111)
If we show that S11 = RT
11 E11 R11 with R11 given in (5.107), then the required
identity in (5.105) will follow from equation (5.110). Consider now the identities
in (5.81) written for the auxiliary symplectic system (5.85) with the coefﬁcients ˜Ak,
˜Bk, ˜Ck, ˜Dk and with their block structure (5.97). From the symmetry of the matrices
˜Dk ˜CT
k and ˜Ak ˜BT
k and from the identity ˜Dk ˜AT
k −˜Ck ˜BT
k = In, we obtain
D1 CT
1 = C1 DT
1 , A1 BT
1 = B1 AT
1 , D1 AT
1 −C1 BT
1 = Ir, A2 BT
1 = B2 AT
1 ,
(5.112)
and from the symmetry of the matrix ˜BT
k ˜Dk and from ˜AT
k ˜Dk −˜CT
k ˜Bk = In, we get
BT
11 D11 = DT
11 B11,
BT
12 D11 + BT
22 D21 −DT
12 B11 = 0(n−r)×r,
(5.113)
AT
11 D11 + AT
21 D21 −CT
11 B11 = Ir,
AT
12 D11 + AT
22 D21 −CT
12 B11 = 0(n−r)×r.
(5.114)
Consider the identities in (5.82) written in terms of ˜Ak, ˜Bk, ˜Ck, ˜Dk. By (5.82)(vi),
˙D1 AT
2 = ˙C1 BT
2 ,
(5.115)
while from (5.82)(ii), we have
˙BT
11 D1 + BT
11 ˙D1 = ˙DT
11 B1 + DT
11 ˙B1 + ˙DT
21 B2,
(5.116)

5.1
Nonlinear Dependence on Spectral Parameter
291
and ﬁnally from (5.82)(iii), we obtain
DT
11 ˙A1 + ˙DT
11 A1 + ˙DT
21 A2 = BT
11 ˙C1 + ˙BT
11 C1.
(5.117)
Then we calculate by using the deﬁnitions of G11 and E11 in (5.99)
BT
11 G11
(5.116), (5.117)
=
DT
11 ( ˙A1 BT
1 −˙B1 AT
1 ) + ˙DT
11 (A1 BT
1 −B1 AT
1 )
+ ˙BT
11 (D1 AT
1 −C1 BT
1 ) + ˙DT
21 (A2 BT
1 −B2 AT
1 )
(5.99), (5.112)
=
˙BT
11 −DT
11 E11.
(5.118)
Therefore, by using the identity
˙A1 DT
1 −˙B1 CT
1
(5.99)
= F11 = GT
11 = B1 ˙CT
1 −A1 ˙DT
1
and formulas (5.108) and (5.118), we obtain from expression (5.111) that
S11
(5.118)
=
BT
11 H11 B11 + ( ˙BT
11 −DT
11 E11) Q11 B11 + BT
11 Q11 ( ˙B11 −E11 D11)
+ BT
11 Q11 E11 Q11 B11 −˙BT
11 Q11 B11 −BT
11 Q11 ˙B11
+ ˙BT
11 D11 + BT
11 ˙D11
= (BT
11 Q11 −DT
11) E11 (Q11 B11 −D11) + Z11,
(5.119)
where
Z11 := BT
11 H11 B11 −DT
11 E11 D11 + ˙BT
11 D11 + BT
11 ˙D11.
We evaluate the matrix Z11 by using identities (5.113), (5.114), (5.115) as follows
Z11 (5.99)
=
BT
11 ( ˙D1 CT
1 −˙C1 DT
1 ) B11 −DT
11 ( ˙B1 AT
1 −˙A1 BT
1 ) D11
+ ˙BT
11 D11 + BT
11 ˙D11
(5.116), (5.117)
=
BT
11 ( ˙D1 CT
1 −˙C1 DT
1 ) B11
−( ˙BT
11 D1 + BT
11 ˙D1 −˙DT
11 B1 −˙DT
21 B2) AT
1 D11
+ (BT
11 ˙C1 + ˙BT
11 C1 −˙DT
11 A1 −˙DT
21 A2) BT
1 D11
+ ˙BT
11 D11 + BT
11 ˙D11
(5.113), (5.114)
=
BT
11 ( ˙D1 CT
1 −˙C1 DT
1 ) B11

292
5
Discrete Symplectic Eigenvalue Problems
+ BT
11
+
˙C1

DT
11 B11
DT
12 B11 −BT
22 D21

−˙D1

CT
11 B11 −AT
21 D21 + Ir
CT
12 B11 −AT
22 D21
 ,
+ ˙BT
11 (C1 BT
1 −D1 AT
1 ) D11 + ˙DT
11 (B1 AT
1 −A1 BT
1 ) D11
+ ˙DT
21 (B2 AT
1 −A2 BT
1 ) D11 + ˙BT
11 D11 + BT
11 ˙D11
(5.112)
=
BT
11 ( ˙D1 CT
1 −˙C1 DT
1 ) B11 + BT
11 ˙D11 + BT
11

 ˙C1 DT
1 B11 −˙C1 BT
2 D21
−˙D1 CT
1 B11 + ˙D1 AT
2 D21 −˙D1

I, 0
T 
= BT
11 ˙D11 + BT
11 ( ˙D1 AT
2 −˙C1 BT
2 ) D21 −BT
11

˙D11, ˙D12
 
I, 0
T
(5.115)
=
BT
11 ˙D11 −BT
11 ˙D11 = 0.
Therefore, upon inserting Z11 = 0 into equation (5.119) and then expression (5.119)
into formula (5.110), we see that the matrix ˙P11 has the form displayed in (5.105).
If now assumption (5.3) holds, then ˜k ≥0 by (5.87), as well as E11 ≥0 by (5.96)
with (5.98). And since in this case ˜Lk−1 ≥0 by (5.102) and (5.31), formula (5.105)
implies that ˙P11 ≥0 on (λ0 −δ, λ0). This shows that the function P11(·) is
nondecreasing on the interval (λ0 −δ, λ0). The proof is complete.
⊓⊔
Remark 5.29 If Xk+1(λ) is invertible on (λ0 −δ, λ0), then r = n, and similar to
Remark 5.27, we get from (5.105), (5.102), and (5.31) the following identity for
λ ∈(λ0 −δ, λ0)
˙Pk(λ) = BT
k (λ)
+ k−1

j=0
Y T
j+1(λ) j(λ) Yj+1(λ)
,
Bk(λ),
+ [Qk+1(λ) Bk(λ) −Dk(λ)]T Ek(λ) [Qk+1(λ) Bk(λ) −Dk(λ)],
where the matrix Qk+1(λ) is from Remark 5.27, and
Pk(λ) := BT
k (λ) Dk(λ) −BT
k (λ) Qk+1(λ) Bk(λ),
Ek(λ) := ˙Bk(λ) AT
k (λ) −˙Ak(λ) BT
k (λ).
The above formula is an extension of the discrete version of [211, Lemma 4.3] to
the nonlinear dependence on λ. Note that when the dependence on λ is linear as
in (5.241), the matrices Ak(λ) ≡Ak and Bk(λ) ≡Bk are constant on R, so that in
this case Ek(λ) ≡0.
Deﬁne now for λ ∈R the auxiliary n × n matrices
˜Mk(λ) := Q Mk(λ) ˜QT ,
˜Tk(λ) := ˜Q Tk(λ) ˜QT ,
˜Pk(λ) := ˜Q Pk(λ) ˜QT ,
(5.120)

5.1
Nonlinear Dependence on Spectral Parameter
293
where Mk(λ), Tk(λ), Pk(λ) are deﬁned in (5.40). Then
rank ˜Mk(λ) = rank Mk(λ),
ind ˜Pk(λ) = ind Pk(λ),
λ ∈R.
(5.121)
Since by (5.69) and Remark 1.60(ii), we have for λ ∈(λ0 −δ, λ0]
˜X†
k+1(λ) = PT X†
k+1(λ) QT ,
˜Xk+1(λ) ˜X†
k+1(λ) = Q Xk+1(λ) X†
k+1(λ) QT ,
it follows that
˜Mk(λ) =

I −˜Xk+1(λ) ˜X†
k+1(λ)
 ˜Bk(λ),
λ ∈(λ0 −δ, λ0].
(5.122)
Again by Remark 1.60(ii), we have ˜M†
k (λ) = ˜Q M†
k (λ) QT , so that for λ ∈(λ0 −
δ, λ0]
˜Tk(λ) = I −˜M†
k (λ) ˜Mk(λ),
˜Pk(λ) = ˜Tk(λ) ˜Xk(λ) ˜X†
k+1(λ) ˜Bk(λ) ˜Tk(λ).
(5.123)
From the form of ˜Xk+1(λ), ˜Bk(λ), ˜Mk(λ) in (5.68), (5.88), (5.122), we get
˜Mk(λ) =

I −X11(λ) X†
11(λ)
0
0
In−r
 B11(λ) B12(λ)
0
B22

,
λ ∈(λ0 −δ, λ0],
(5.124)
with the matrices B12(λ) and B22 as in (5.88). This implies that
Mk(λ) ≡QT
0r
0
0 B22

˜Q,
B22 =
0n−r−ρ
0
0
¯B44

,
λ ∈(λ0 −δ, λ0),
with invertible ¯B44 ∈Rρ×ρ. This shows that rankMk(λ) ≡ρ on (λ0 −δ, λ0),
compared with the paragraph preceding Remark 5.21. Now since the ﬁrst block
column of B12(λ) in (5.88) is zero and ¯B44 is invertible, (5.124) yields that
Ker ˜Mk(λ) = Ker
M11(λ) 0
0
B22

(5.125)
for all λ ∈(λ0 −δ, λ0], where the matrix M11(λ) ∈Rr×r is deﬁned by
M11(λ) :=

I −X11(λ) X†
11(λ)

B11(λ),
λ ∈R.
(5.126)
Consequently, by Lemma 1.63, for any λ ∈(λ0 −δ, λ0], we have
˜M†
k (λ) ˜Mk(λ) =

M†
11(λ) 0
0
B†
22
 M11(λ)
0
0
B22

=

M†
11(λ) M11(λ)
0
0
B†
22 B22

.

294
5
Discrete Symplectic Eigenvalue Problems
Therefore, by the form of ˜Tk(λ) in (5.123),
˜Tk(λ) =
T11(λ)
0
0
I −B†
22 B22

,
λ ∈(λ0 −δ, λ0],
(5.127)
where the matrix T11(λ) ∈Rr×r is deﬁned by
T11(λ) := I −M†
11(λ) M11(λ),
λ ∈R,
I −B†
22 B22 =
In−r−ρ 0
0
0ρ

.
(5.128)
Finally, from the form of the matrices ˜Xk(λ), Y11(λ), ˜Pk(λ), and ˜Tk(λ) in (5.73),
(5.91), (5.123), and (5.127), we get
˜Pk(λ) =
P11(λ)
0
0
0n−r

,
λ ∈(λ0 −δ, λ0],
(5.129)
where the matrix P11(λ) is given in (5.100)–(5.101) for λ ∈(λ0 −δ, λ0) and
P11(λ0) := T11(λ0) Y11(λ0) X†
11(λ0) B11(λ0) T11(λ0).
(5.130)
In addition, since X11(λ) is invertible for λ ∈(λ0 −δ, λ0), we have
M11(λ) ≡0, T11(λ) ≡I, P11(λ) = Y11(λ) X−1
11 (λ) B11(λ), λ ∈(λ0 −δ, λ0),
(5.131)
and the matrix P11(λ) is nondecreasing on (λ0 −δ, λ0), by Lemma 5.28. Thus, we
are now ready for the ﬁnal step in the proof of the ﬁrst local oscillation theorem
(Theorem 5.15).
5.1.6
Application of the Index Theorem
In this subsection we utilize the above construction in order to apply the index
theorem (Theorem 1.85 and Corollary 1.86) to the reduced quantities in the
dimension r on the left neighborhood of λ0 and to similarly reduced quantities
in the appropriate dimension on the right neighborhood of λ0. Here we need to
assume that the matrix B11(λ) has constant image in λ, as the matrix RT
2 (t) in
Corollary 1.86 is assumed to have constant image in t. But under monotonicity
assumption (5.3), conditions (5.38) and (5.10) are equivalent (see Theorem 5.1 (iii)),
and then condition (5.38) can be used instead of (5.10).

5.1
Nonlinear Dependence on Spectral Parameter
295
Proof of Theorem 5.15 Assume that (5.38) holds, then the matrix Bk(λ) has con-
stant image in λ on R (see Theorem 5.1 (iii)); hence the matrix ˜Bk(λ) has constant
image in λ with B11(λ) ∈Rr×r having constant image in λ, too. For t ∈(0, δ), we
deﬁne the matrices
X(t) := X11(λ0 −t),
(t) := Y11(λ0 −t),
U(t) := −U11(λ0 −t),
M(t) := P11(λ0 −t),
R1(t) := DT
11(λ0 −t),
R2(t) := BT
11(λ0 −t),
S1(t) := 1
2

D11(λ0 −t) B†
11(λ0 −t) + B†T
11 (λ0 −t) DT
11(λ0 −t)

,
S2(t) := 1
2

DT
11(λ0 −t) [I −B11(λ0 −t) B†
11(λ0 −t)]
+[I −B†
11(λ0 −t) B11(λ0 −t)] DT
11(λ0 −t)

,
S1 := 1
2

D11(λ0) B†
11(λ0) + B†T
11 (λ0) DT
11(λ0),
S2 := 1
2

DT
11(λ0) [I −B11(λ0) B†
11(λ0)]
+[I −B†
11(λ0) B11(λ0)] DT
11(λ0),
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.132)
and the matrices
X := X11(λ0),
 := Y11(λ0),
S∗:= M11(λ0),
U := −U11(λ0),
R2 := BT
11(λ0)
T := T11(λ0),
R1 := DT
11(λ0),
S := X†
11(λ0) B11,
Q := P11(λ0),
⎫
⎪⎬
⎪⎭
(5.133)
where the matrices Y11(λ), P11(λ), M11(λ), T11(λ) are given in (5.91), (5.100),
(5.130), (5.126), and (5.128). We now verify the assumptions of the index theorem
(Corollary 1.86).
By Lemma 5.19 and (5.92), the matrices XT (t) U(t), R1(t) RT
2 (t), and S1(t) are
symmetric, rank(XT (t), UT (t)) = r, as well as rank(R1(t), R2(t)) = r for all t ∈
[0, δ). Moreover, the symmetry of B†
11(λ) B11(λ) and DT
11(λ) B11(λ) in (5.92) yields
that R1(t) = R2(t) S1(t)+S2(t) and S2(t) RT
2 (t) = 0 for t ∈[0, δ), i.e., Im RT
2 (t) ⊆
Ker S2(t) for t ∈[0, δ). The opposite inclusion Ker S2(t) ⊆Im RT
2 (t) for t ∈[0, δ)
is a consequence of the relation Ker(DT
11(λ), BT
11(λ)) = Im(BT
11(λ), −DT
11(λ))T
obtained from (5.92). Hence, Ker S2(t) = Im RT
2 (t) for t ∈[0, δ). Furthermore,
by Lemma 5.19, the matrix X(t) is invertible for t ∈(0, δ). Next, from (5.132)–
(5.133), we have X(t) →X, U(t) →U, S1(t) →S1, S2(t) →S2, R1(t) →R1,
R2(t) →R2 for t →0+. Equations (5.100), (5.101), (5.91), (5.126), (5.128),
and (5.130) imply that the matrices M(t), (t), , S, S∗, T , Q deﬁned in (5.132)–
(5.133) are equal to their corresponding deﬁning expressions in Theorem 1.85.
Finally, Im R2(t) is constant for t ∈[0, δ) by assumption (5.38). By Lemma 5.26,
the matrix U(t) X−1(t) is nonincreasing on (0, δ), as X(t) and U(t) are deﬁned
in (5.132) through X11(λ) and U11(λ) with the argument λ = λ0−t and U(t) has the
opposite sign to U11(λ0−t), while by Lemma 5.28 the matrix M(t) is nonincreasing
on (0, δ), as M(t) is deﬁned in (5.132) through P11(λ) with the argument λ = λ0−t.

296
5
Discrete Symplectic Eigenvalue Problems
This means that equality (1.225) of Theorem 1.85 (Corollary 1.86) holds, in which
we take m = r. From the construction in Sects. 5.1.4 and 5.1.5, we calculate
ind M(0+)
(5.132)
=
ind P11(λ−
0 )
(5.129)
=
ind ˜Pk(λ−
0 )
(5.120)
=
ind Pk(λ−
0 ),
def (0+)
(5.132)
=
defY11(λ−
0 )
(5.91)
= r −˜r
(5.72)
= r + ρ −rankXk(λ−
0 ),
def
(5.132)
=
defY11(λ0)
(5.91)
=
r −rank ˜X11(λ0)
(5.73)
= r + ρ −rank ˜Xk(λ0)
(5.78)
=
r + ρ −rankXk(λ0),
def X
(5.133)
=
defX11(λ0) = r −rank X11(λ0)
(5.58)
=
r −rank ˜Xk+1(λ0)
(5.69)
=
r −rankXk+1(λ0),
ind Q
(5.133)
=
ind P11(λ0)
(5.129)
=
ind ˜Pk(λ0)
(5.120)
=
ind Pk(λ0),
rankT
(5.133)
=
rankT11(λ0)
(5.128)
=
r −rankM11(λ0)
(5.125)
=
r −[rank ˜Mk(λ0) −ρ]
(5.121)
=
r + ρ −rank M(λ0).
When we insert the above data into equation (1.225) and if we recall the deﬁnitions
of r = rankXk+1(λ−
0 ) and ρ = rankMk(λ−
0 ), we get
ind Pk(λ−
0 ) = ind M(0+) (1.225)
=
ind Q + m −rank T + def  −def (0+) −def X
= ind Pk(λ0) + r −[r + ρ −rank Mk(λ0)] + [r + ρ −rank Xk(λ0)]
−[r + ρ −rank Xk(λ−
0 )] −[r −rank Xk+1(λ0)]
= ind Pk(λ0) + rank Mk(λ0) + rank Xk(λ−
0 ) −rank Xk(λ0)
+ rank Xk+1(λ0) −rank Xk+1(λ−
0 ) −rank Mk(λ−
0 ).
(5.134)
In view of the deﬁnition of mk(λ) in (5.39) as the number of focal points in (k, k+1],
we have from (5.134) the identity
mk(λ−
0 ) = mk(λ0) + rankXk(λ−
0 ) −rankXk(λ0)
+ rankXk+1(λ0) −rank Xk+1(λ−
0 ).
(5.135)
Now we need to consider the construction in Sects. 5.1.4 and 5.1.5 in the right
neighborhood of λ0, i.e., for λ ∈[λ0, λ0 + δ). The analysis of this construction
shows that exactly the same principles in obtaining the partitioned auxiliary
matrices ˜Xk+1(λ), ˜Uk+1(λ), the reﬁned partitioned matrices ˜Xk(λ), ˜Uk(λ), and the
coefﬁcients ˜Ak(λ), ˜Bk(λ), ˜Ck(λ), ˜Dk(λ) hold when the dimensions r and ρ in (5.57)

5.1
Nonlinear Dependence on Spectral Parameter
297
and (5.70) are replaced by the dimensions
s := rank Xk+1(λ+
0 )
and
σ := rank Xk(λ+
0 ) P2 = rankMk(λ+
0 ),
respectively, and at the same time the intervals (λ0 −δ, λ0] and (λ0 −δ, λ0) are
replaced by the intervals [λ0, λ0 +δ) and (λ0, λ0 +δ). Therefore, we now deﬁne for
t ∈(0, δ) the s × s matrices
X(t) := X11(λ0 + t),
(t) := Y11(λ0 + t),
U(t) := −U11(λ0 + t),
M(t) := P11(λ0 + t),
R1(t) := DT
11(λ0 + t),
R2(t) := BT
11(λ0 + t),
S1(t) := 1
2

D11(λ0 + t) B†
11(λ0 + t) + B†T
11 (λ0 + t) DT
11(λ0 + t),
S2(t) := 1
2

DT
11(λ0 + t) [I −B11(λ0 + t) B†
11(λ0 + t)]
+[I −B†
11(λ0 + t) B11(λ0 + t)] DT
11(λ0 + t)

,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.136)
and the matrices S1, S2, X, U, R1, , R2, S, S∗, T , Q by the equations in (5.132)
and (5.133). It follows by Lemmas 5.26 and 5.28 that the matrices U(t) X−1(t)
and M(t) are now nondecreasing on (0, δ), as they are deﬁned in (5.136) through
X11(λ), U11(λ), P11(λ) with the argument λ = λ0+t and U(t) has the opposite sign
to U11(λ0 + t). This means that equality (1.226) of Theorem 1.85 (Corollary 1.86)
now holds, in which we take m = s. We now calculate
ind M(0+)
(5.136)
=
ind P11(λ+
0 ) = ind ˜Pk(λ+
0 ) = ind Pk(λ+
0 ),
ind Q
(5.133)
=
ind P11(λ0) = ind ˜Pk(λ0) = ind Pk(λ0),
rankT
(5.133)
=
rankT11(λ0) = s −rankM11(λ0) = s −[rank ˜Mk(λ0) −σ]
= s + σ −rankM(λ0).
When we insert the above data into equation (1.226), we obtain
ind Pk(λ+
0 ) = ind M(0+)
(1.226)
=
ind Q + m −rank T
= ind Pk(λ0) + s −[s + σ −rank Mk(λ0)]
= ind Pk(λ0) + rankMk(λ0) −rank Mk(λ+
0 ).
(5.137)
In view of the deﬁnition of mk(λ) in (5.39), we get from (5.137) that mk(λ+
0 ) =
mk(λ0), which proves the equality in (5.42). By combining (5.42) with equa-
tion (5.135), we can see that the equality in (5.43) is also satisﬁed. The proof of
Theorem 5.15 is now complete.
⊓⊔

298
5
Discrete Symplectic Eigenvalue Problems
5.1.7
Applications and Examples
In this subsection we show some applications of the oscillation theorems from
Sect. 5.1.3. For this purpose we deﬁne the associated quadratic functional, compare
with (2.56),
F(y, λ) :=
N

k=0
yT
k
ST
k (λ) K Sk(λ) −K yk,
K :=
0n 0n
In 0n

,
(5.138)
where y = (x, u) is an admissible pair, i.e., xk+1 = Ak(λ) xk + Bk(λ) uk for all
k ∈[0, N]Z, satisfying the Dirichlet endpoints x0 = 0 = xN+1. We say that the
functional F(·, λ) is positive, if F(z, λ) > 0 for every admissible z = (x, u) with
x0 = 0 = xN+1 and x ̸≡0. Conversely, the functional F(·, λ) is not positive, if
F(z, λ) ≤0 for some admissible z = (x, u) with x0 = 0 = xN+1 and x ̸≡0. These
two properties will be denoted by F(·, λ) > 0 and F(·, λ) ̸> 0, respectively. The
following result is from Theorem 2.36.
Proposition 5.30 Let λ0 ∈R be ﬁxed. Then the functional F(·, λ0) > 0 if and
only if n1(λ0) = 0, i.e., the principal solution Y [0](λ0) of system (Sλ0) has no focal
points in (0, N + 1].
Additional conditions which are equivalent to the positivity of F(·, λ0) can be
found in the literature, such as the solvability of the explicit and implicit Riccati
equations and inequalities, conjugate and coupled intervals, perturbed quadratic
functionals, etc.; see, e.g., Theorem 2.36.
From (5.55) and (5.56), it follows that the constant m ∈N ∪{0} in Theorem 5.17
is actually zero, if and only if n2(λ) = n1(λ) ≡m = 0 for all or for some λ ∈R.
Combining this observation with Proposition 5.30 yields the following.
Theorem 5.31 Assume (5.3) and (5.38) for all k ∈[0, N]Z. Then
n1(λ) = n2(λ)
for all λ ∈R
(5.139)
if and only if there exists λ0 < 0 such that F(·, λ0) > 0.
Proof If F(·, λ0) > 0, then n1(λ0) = 0, by Proposition 5.30. Since the function
n1(·) is nondecreasing on R by Theorem 5.16, it follows that n1(λ) ≡0 for all
λ ≤λ0, i.e., m = 0 in (5.55), showing (5.139). Conversely, if (5.139) holds, then
m = 0, and so, by (5.56), we have n1(λ) ≡m = 0 for λ ≤λ0 with λ0 from
Theorem 5.17. From Proposition 5.30, we then get F(·, λ0) > 0.
⊓⊔
Next we present conditions in terms of F(·, λ) > 0 which are closely related to
the existence of ﬁnite eigenvalues of (E).
Theorem 5.32 (Existence of Finite Eigenvalues: Necessary Condition) Let the
assumptions (5.3) and (5.38) be satisﬁed for all k ∈[0, N]Z. If (E) has a ﬁnite

5.1
Nonlinear Dependence on Spectral Parameter
299
eigenvalue, then there exist λ0, λ1 ∈R with λ0 < λ1 and m ∈N ∪{0} such that
n1(λ) ≡m for all λ ≤λ0 and F(·, λ1) ̸> 0.
Proof The fact that (E) has a ﬁnite eigenvalue of (E) means that n2(λ1) ≥1 for
some λ1 ∈R. By Theorem 5.17, we know that equality (5.55) is satisﬁed for some
m ∈N ∪{0} and n1(λ) ≡m for all λ ≤λ0 for some λ0 < 0. Without loss of
generality, we may take λ0 < λ1, so that the ﬁrst part of this theorem is proven.
Next, from (5.55) with λ = λ1, we obtain n1(λ1) = n2(λ1) + m ≥n2(λ1) ≥1,
showing that the principal solution of (Sλ1) has at least one focal point in (0, N +1].
In turn, Proposition 5.30 shows that F(·, λ1) ̸> 0.
⊓⊔
Theorem 5.33 (Existence of Finite Eigenvalues: Sufﬁcient Condition) Let the
assumptions (5.3) and (5.38) be satisﬁed for all k ∈[0, N]Z. If there exist λ0, λ1 ∈R
with λ0 < λ1 such that F(·, λ0) > 0 and F(·, λ1) ̸> 0, then (E) has at least one
ﬁnite eigenvalue.
Proof The positivity of F(·, λ0) implies by Theorem 5.31 that equality (5.139)
holds. If we assume that there is no ﬁnite eigenvalue of (E) at all, i.e., if n2(λ) ≡0
for every λ ∈R, then n1(λ) ≡0 for all λ ∈R as well. In particular, n1(λ1) = 0. This
means by Proposition 5.30 that F(·, λ1) > 0, which contradicts our assumption.
Therefore, under the given conditions, the eigenvalue problem (E) must have at least
one ﬁnite eigenvalue.
⊓⊔
The following result characterizes the smallest ﬁnite eigenvalue of (E).
Theorem 5.34 Assume that (5.3) and (5.38) hold for all k ∈[0, N]Z. If there
exist λ0, λ1 ∈R with λ0 < λ1 such that F(·, λ0) > 0 and F(·, λ1) ̸> 0, then
the eigenvalue problem (E) possesses a smallest ﬁnite eigenvalue λmin, which is
characterized by any of the following conditions:
λmin = sup P,
P := {λ ∈R, F(·, λ) > 0},
(5.140)
λmin = min N,
N := {λ ∈R, F(·, λ) ̸> 0}.
(5.141)
Moreover, the algebraic multiplicity of λmin is then equal to n1(λmin), i.e., to the
number of focal points of the principal solution of (Sλmin) in (0, N + 1].
Proof From Theorem 5.33, we know that the eigenvalue problem (E) has at least
one ﬁnite eigenvalue. Since F(·, λ0) > 0 is assumed, then λ0 ∈P and the set
P is nonempty. Moreover, by Proposition 5.30 we have n1(λ0) = 0. Since the
function n1(·) is nondecreasing on R, it follows that n1(λ) ≡0 for λ ≤λ0, i.e.,
F(·, λ) > 0 for λ ≤λ0. This implies that (−∞, λ0] ⊆P. In addition, λ1 ̸∈P,
so that P is bounded from above and therefore P = (−∞, ω), where ω = sup P
exists. It follows that n1(ω) ≥1, because by Theorem 5.16, the function n1 is
right-continuous on R. We will show that λmin = ω is the smallest ﬁnite eigenvalue
of (E). From Theorem 5.31, we know that n1(λ) = n2(λ) for all λ ∈R. Hence,
n2(λ) ≡0 for all λ < ω and n2(ω) = n1(ω) ≥1, proving that ω is the smallest
ﬁnite eigenvalue of (E) with the algebraic multiplicity n1(ω). As for (5.141), we

300
5
Discrete Symplectic Eigenvalue Problems
note that the set N is nonempty, because λ1 ∈N, and the interval (−∞, λ0] is not
contained in N. Therefore, N is bounded from below. Let ν ∈N, i.e., n1(ν) ≥1.
Then n2(ν) = n1(ν) ≥1. Since we know from Theorem 5.17 and Corollary 5.7 that
the function n2 is right-continuous on R and the ﬁnite eigenvalues are isolated and
bounded from below, it follows that κ := min{ν ∈R, n2(ν) ≥1} = min N exists
and satisﬁes λ0 < κ. Furthermore, by the deﬁnition of κ, we have n2(λ) ≡0 for all
λ < κ and n2(κ) ≥1. This yields that λmin = κ is the smallest ﬁnite eigenvalue
of (E) with multiplicity n2(κ) = n1(κ).
⊓⊔
Finally, we discuss the applicability of our results to some special discrete
symplectic systems (SDSλ).
Example 5.35 Consider the second-order Sturm-Liouville difference equation; see
Sect. 1.2, which can also be viewed as the Jacobi matrix,
rk(λ) xk(λ) + qk(λ) xk+1(λ) = 0,
k ∈[0, N −1]Z,
(5.142)
where the coefﬁcients rk, qk : R →R satisfy
rk(λ) ̸= 0,
˙rk(λ) ≤0,
˙qk(λ) ≥0,
k ∈[0, N]Z, λ ∈R.
In this case the matrices Sk(λ) and k(λ) have the form (see Example 2.13)
Sk(λ) =

1
1/rk(λ)
−qk(λ) 1 −qk(λ)/rk(λ)

,
k(λ) =
1
r2
k (λ)
rk(λ) qk(λ)
0
1
  ˙qk(λ)
0
0
−˙rk(λ)
 rk(λ) 0
qk(λ) 1

.
The square of rk(λ) in the matrix k(λ) above shows that the oscillation and
spectral theory of the second-order Sturm-Liouville difference equations, even in
the classical setting where rk(λ) ≡rk ̸= 0 and qk(λ) = qk+λ wk with wk > 0, does
not depend on the sign of the coefﬁcient rk(λ) but rather on its monotonicity. The
principal solution ˆx(λ) of (5.142) is the solution starting with the initial conditions
ˆx0(λ) ≡0 and ˆx1(λ) = 1/r0(λ) for all λ ∈R. The result in Theorem 5.3
yields that if x(λ) is a nontrivial solution of (5.142) and xk(λ0) = 0 for some
k ∈[0, N + 1]Z and λ0 ∈R, then either xk(λ) is identically zero or never zero in
some left and right neighborhoods of λ0. This implies, according to Deﬁnition 5.5,
that a number λ0 ∈R is a ﬁnite eigenvalue of (E) provided there exists δ > 0 such
that ˆxN+1(λ0) = 0 and ˆxN+1(λ) ̸= 0 for all λ ∈(λ0 −δ, λ0). The degeneracy
condition in (5.23) of Deﬁnition 5.8 at λ0 then means that the solutions x(λ)
of (5.142) with λ ∈(λ0 −δ, λ0] starting with the initial conditions x0(λ) = x0(λ0)
and x1(λ) = x0(λ) + r0(λ0) x0(λ0)/r0(λ) satisfy
˙rk(λ) xk(λ) ≡0,
˙qk(λ) xk+1(λ) ≡0
for all k ∈[0, N]Z and λ ∈(λ0 −δ, λ0].

5.1
Nonlinear Dependence on Spectral Parameter
301
Since rank Bk(λ) = rank 1/rk(λ) ≡1 is constant in λ, the results in Theorems 5.15
and 5.16 and 5.17 and in Corollary 5.18 hold without any additional assumption.
Similar analysis can also be done for the higher-order Sturm-Liouville difference
equations; see Example 5.38.
Example 5.36 Similar to Example 5.35, we consider the second-order matrix
Sturm-Liouville equation (see Example 2.10)
Rk(λ) xk(λ) + Qk(λ) xk+1(λ) = 0,
k ∈[0, N −1]Z,
(5.143)
where Rk, Qk : R →Rn×n are symmetric matrix functions such that Rk(λ) is
invertible, ˙Rk(λ) ≤0, and ˙Qk(λ) ≥0 for all k ∈[0, N]Z and λ ∈R. In this case
Sk(λ) =

I
R−1
k (λ)
Qk(λ) I −Qk(λ) R−1
k (λ)

,
k(λ) =
I Qk(λ) R−1
k (λ)
0
R−1
k (λ)
  ˙Qk(λ)
0
0
−˙Rk(λ)
 
I
0
R−1
k (λ) Qk(λ) R−1
k (λ)

.
A number λ0 ∈R is a ﬁnite eigenvalue of (E) if the principal solution ˆX(λ)
of (5.143), i.e., the solution starting with ˆX0(λ) ≡0 and ˆX1(λ) = R−1
0 (λ) for
all λ ∈R, has ˆXN+1(λ0) singular and
ˆXN+1(λ) is invertible for λ ∈(λ0 −δ, λ0)
for some δ > 0. Since in this case rank Bk(λ) = rankR−1
k (λ) ≡n is constant in
λ, the results in Theorems 5.15 and 5.16 and 5.17 and in Corollary 5.18 then hold
without any additional assumption.
Example 5.37 Consider the linear Hamiltonian system (see Example 2.7)
xk(λ) = Ak(λ) xk+1(λ) + Bk(λ) uk(λ),
uk(λ) = Ck(λ) xk+1(λ) −AT
k (λ) uk(λ),

k ∈[0, N]Z,
(5.144)
where the coefﬁcients Ak, Bk, Ck : R →Rn×n are such that Bk(λ) and Ck(λ) are
symmetric, I −Ak(λ) is invertible with ˜Ak(λ) := [I −Ak(λ)]−1, and
˙Hk(λ) ≥0,
Hk(λ) :=
−Ck(λ) AT
k (λ)
Ak(λ) Bk(λ)

,
k ∈[0, N]Z, λ ∈R.
The matrices Sk(λ) and k(λ) have the form
Sk(λ) =

˜Ak(λ)
˜Ak(λ) Bk(λ)
Ck(λ) ˜Ak(λ) Ck(λ) ˜Ak(λ) Bk(λ) + I −AT
k (λ)

,
(5.145)
k(λ) =
I −Ck(λ) ˜Ak(λ)
0
˜Ak(λ)

˙Hk(λ)

I
0
−˜AT
k (λ) Ck(λ) ˜AT
k (λ)

.
(5.146)

302
5
Discrete Symplectic Eigenvalue Problems
The results in Theorems 5.15 and 5.16 and 5.17 and in Corollary 5.18 then hold
under the assumption that the matrix Bk(λ) has constant rank in λ for every k ∈
[0, N]Z. In Sect. 5.6 we completely omit this restriction.
Example 5.38 For a ﬁxed n ∈N, we consider the 2n-th order Sturm-Liouville
difference equation (see Example 2.12)
n

j=0
(−1)jjr[j]
k (λ) jyk+n−j(λ) = 0,
k ∈[0, N −n]Z,
(5.147)
where r[i]
k
: R →R are piecewise continuously differentiable for all i
∈
{0, 1 . . ., n} and k ∈[0, N]Z such that
r[n]
k (λ) ̸= 0,
˙r[i]
k (λ) ≤0
for all i ∈{0, 1, . . ., n}.
Equation (5.147) can be written as a special linear Hamiltonian system (5.144),
whose coefﬁcients are given in (2.29) and (2.30). In particular Ak(λ) ≡A is
constant in λ. In turn by Example 5.37, system (5.144) is a special symplectic
difference system (SDSλ) with
Ak(λ) ≡
⎛
⎜⎜⎜⎝
1 1 . . . 1
0 1 . . . 1
... ... ... ...
0 0 . . . 1
⎞
⎟⎟⎟⎠,
Bk(λ) =
1
r[n]
k (λ)
⎛
⎜⎜⎜⎝
0 . . . 0 1
0 . . . 0 1
... ... ... ...
0 . . . 0 1
⎞
⎟⎟⎟⎠,
(5.148)
Ck(λ) =
⎛
⎜⎜⎜⎜⎜⎝
r[0]
k (λ) r[0]
k (λ) . . . r[0]
k (λ)
0
r[1]
k (λ) . . . r[1]
k (λ)
...
...
...
...
0
0
. . . r[n−1]
k
(λ)
⎞
⎟⎟⎟⎟⎟⎠
,
(5.149)
Dk(λ) =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
0 0 . . . 0
0
r[0]
k (λ)/r[n]
k (λ)
−1 1 0 . . . 0
0
r[1]
k (λ)/r[n]
k (λ)
0 −1 1 . . . 0
0
r[2]
k (λ)/r[n]
k (λ)
...
...
... ...
...
...
...
0
0 0 . . . 1
0
r[n−3]
k
(λ)/r[n]
k (λ)
0
0 0 . . . −1 1
r[n−2]
k
(λ)/r[n]
k (λ)
0
0 0 . . . 0 −1 1 + r[n−1]
k
(λ)/r[n]
k (λ)
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
(5.150)

5.2
Eigenvalue Problems with General Boundary Conditions
303
The matrix k(λ) is now given in (5.146), where
˙Hk(λ) = diag{−˙Ck(λ), ˙Bk(λ)}
= −diag

˙r[0]
k (λ), . . . , ˙r[n−1]
k
(λ), 0, . . . , 0,
˙r[n]
k (λ)

r[n]
k (λ)
2
 
≥0.
Since the rank of the matrix Bk(λ) in (5.148) is constant in λ, it follows that the
results in Theorems 5.15 and 5.16 and 5.17 and in Corollary 5.18 then hold without
any additional assumption.
Example 5.39 Assume that the symplectic matrix Sk(λ) in (5.2) is linear in λ, that
is, we have Sk(λ) := Sk +λ Vk for all k ∈[0, N]Z and λ ∈R. Then Lemma 1.58(iii)
yields that ST
k (λ) = ST
k + λ VT
k ∈Sp(2n), i.e.,
(Sk + λ Vk) J (Sk + λ Vk)T = Sk J ST
k + λ (VkJ ST
k + SkJ VT
k ) + λ2VkJ VT
k = J
for all λ ∈R. It follows that the matrix Sk is symplectic, VkJ VT
k
= 0, and
VkJ ST
k
= SkJ T VT
k . Using this fact we prove that the symmetric matrix k(λ)
in (5.3) is constant in λ and it has the form
k(λ) ≡k := J Vk J ST
k J ≥0
for all k ∈[0, N]Z.
In this case we can actually prove that the ﬁnite eigenvalues of (E) are real and that
the ﬁnite eigenfunctions corresponding to different ﬁnite eigenvalues are orthogonal
with respect to the semi-inner product
⟨z, ˜z⟩ :=
N

k=0
zT
k+1 k ˜zk+1.
The proof follows standard arguments from linear algebra. The results in Theo-
rems 5.15, 5.16, and 5.17 and in Corollary 5.18 then hold under the assumption that
the matrix Bk(λ) = Bk + λ ˜Bk has constant rank in λ for every k ∈[0, N]Z. We note
that in Sect. 5.6 we completely omit this restriction.
5.2
Eigenvalue Problems with General Boundary Conditions
In this section we present the theory of eigenvalues for system (5.1) with general
boundary conditions, i.e., with separated and jointly varying endpoints.

304
5
Discrete Symplectic Eigenvalue Problems
5.2.1
Transformations of Boundary Conditions
In this subsection we consider system (SDSλ) together with the so-called self-
adjoint boundary conditions
R1(λ)
 x0(λ)
xN+1(λ)

+ R2(λ)
 −u0(λ)
uN+1(λ)

= 0,
(5.151)
where R1(λ) and R2(λ) are piecewise continuously differentiable matrix-valued
functions such that (compare with (2.92))
R1(λ), R2(λ) ∈R2n×2n,
rank(R1(λ) R2(λ)) = 2n,
R1(λ) RT
2 (λ) = R2(λ) RT
1 (λ),
λ ∈R.

(5.152)
Moreover, with the notation R(λ) := (R1(λ) R2(λ)) ∈R2n×4n, we impose the
following monotonicity restriction
˙R1(λ) RT
2 (λ) −˙R2(λ) RT
1 (λ) = ˙R(λ) J4nRT (λ) ≤0.
(5.153)
As a particular case of matrices R1(λ) and R2(λ) in (5.151), we consider their block
diagonal form
R1(λ) = diag{R∗
0(λ), R∗
N+1(λ)},
R2(λ) = diag{−R0(λ), RN+1(λ)}.
(5.154)
Here Ri(λ), R∗
i (λ) ∈Rn×n, i ∈{0, N + 1} and ∗is just a notation without
any connection to the conjugate transpose of a matrix. In this case the boundary
conditions are separated, i.e.,
R∗
0(λ) x0(λ) + R0(λ) u0(λ) = 0,
R∗
N+1(λ) xN+1(λ) + RN+1(λ) uN+1(λ) = 0,

(5.155)
and conditions (5.152) translate as (compared with (2.83))
R∗
0(λ) RT
0 (λ) = R0(λ) R∗T
0 (λ),
rank(R∗
0(λ), R0(λ)) = n,
R∗
N+1(λ) RT
N+1(λ) = RN+1(λ) R∗T
N+1(λ),
rank (R∗
N+1(λ), RN+1(λ)) = n,

(5.156)
while the monotonicity condition (5.153) is rewritten in the form
˙R∗
0(λ) RT
0 (λ) −˙R0(λ) R∗T
0 (λ) = ˙˜R(λ) J ˜RT (λ) ≥0,
(5.157)
˙R∗
N+1(λ) RT
N+1(λ) −˙RN+1(λ) R∗T
N+1(λ) = ˙R(λ) J RT (λ) ≤0,
(5.158)

5.2
Eigenvalue Problems with General Boundary Conditions
305
where
˜R(λ) := (R∗
0(λ) R0(λ)),
R(λ) := (R∗
N+1(λ) RN+1(λ)).
(5.159)
In particular, if R1(λ) ≡R1 and R2(λ) ≡R2 are independent on λ, then the
monotonicity conditions in (5.153), (5.157), (5.158) are automatically satisﬁed.
Moreover, for R∗
0 = I, R0 = 0, R∗
N+1 = I, and RN+1 = 0, we obtain the Dirichlet
boundary conditions used in problem (E).
The discreteness of the underlaying set N or (Z) enables a construction, which
transforms general boundary condition (5.151) to the Dirichlet condition.
Lemma 5.40 The eigenvalue problem (SDSλ) with separated boundary condi-
tions (5.155) on [0, N + 1]Z can be extended to an eigenvalue problem of the same
form (the so-called extended system) on the interval [−1, N +2]Z with the Dirichlet
boundary conditions
˜x−1(λ) = 0 = ˜xN+2(λ).
(5.160)
Proof The reduction of separated boundary conditions on [0, N + 1]Z to the
Dirichlet conditions on [−1, N + 2]Z is realized by the following construction.
Consider separated boundary conditions (5.155). We construct the extended system,
where we extend the original eigenvalue problem (5.1), (5.155) considered for
k ∈[0, N]Z to a system for k ∈[−1, N +1]Z, where we transform general separated
boundary conditions (5.155) at k = 0 and k = N + 1 to the Dirichlet boundary
condition at k = −1 and k = N + 2.
Step 1 Firstly, we deﬁne the symplectic matrices S−1(λ) and SN+1(λ) according to
Lemma 1.58(vi) such that
S−1(λ) (0 I)T = J T ˜RT (λ) Q0(λ),
det Q0(λ) ̸= 0,
S−1
N+1(λ) (0 I)T = J T RT (λ) QN+1(λ),
det QN+1(λ) ̸= 0,

λ ∈R,
(5.161)
where the n×2n matrices ˜R(λ) and R(λ) are given by (5.159). Note that the matrices
S−1(λ) and SN+1(λ) are not deﬁned uniquely by conditions (5.161). For S−1(λ) and
SN+1(λ) given by (5.161), we introduce the extended problem
˜yk+1(λ) = Sk(λ) ˜yk(λ),
k ∈[−1, N + 1]Z,
˜x−1(λ) = 0 = ˜xN+2(λ).
(5.162)
One can show that for any choice of piecewise continuously differentiable Q0(λ)
and QN+1(λ), if a vector function yk(λ) = (xk(λ), uk(λ)) for k ∈[0, N + 1]Z
obeys (SDSλ) and (5.155) under assumption (5.156), then ˜yk = (˜xk(λ), ˜uk(λ)) for

306
5
Discrete Symplectic Eigenvalue Problems
k ∈[−1, N + 2]Z solves the extended problem (5.162), where ˜yk(λ) ≡yk(λ) for
k ∈[0, N + 1]Z. The proof is based on Lemma 1.58(vii); see formula (1.149).
Indeed, assume that yk(λ) solves (SDSλ) and (5.155). Then y0(λ) ∈Ker ˜R(λ) and
yN+1(λ) ∈Ker R(λ), where ˜R(λ) and R(λ) are deﬁned in (5.159). Note that in
this step we do not impose any monotonicity conditions (5.3), (5.157), (5.158).
Then, by (1.149), y0(λ) ∈Im J ˜RT (λ) and yN+1(λ) ∈Im J RT (λ), compared
with the deﬁnition of S−1(λ) and SN+1(λ) in (5.161). The last conditions mean
that ˜yk = (˜xk(λ), ˜uk(λ)) for k ∈[−1, N + 2]Z obeys (5.160), i.e., it solves the
extended problem (5.162), where ˜yk(λ) ≡yk(λ) for k ∈[0, N + 1]Z. Conversely, if
a vector function ˜yk(λ) = (˜xk(λ), ˜uk(λ)) for k ∈[−1, N + 2]Z solves the extended
problem (5.162), then conditions (5.160) imply ˜y0(λ) ∈Im J ˜RT (λ) and ˜yN+1(λ) ∈
Im J RT (λ) or, again by (1.149), ˜y0(λ) ∈Ker ˜R(λ) and ˜yN+1(λ) ∈Ker R(λ). This
shows that ˜y0(λ) and ˜yN+1(λ) obey (5.155).
Step 2 Secondly, we present the construction of S−1(λ) and SN+1(λ) in such a way
that conditions (5.157), (5.158) imply
(S−1(λ)) ≥0,
(SN+1(λ)) ≥0.
(5.163)
According to (5.161) and the proof of Lemma 1.58(vi), we introduce the n × n
matrices
K0(λ) = [ ˜R(λ) ˜RT (λ)]−1,
KN+1(λ) = [R(λ) RT (λ)]−1
(5.164)
and deﬁne the ﬁrst 2n × n blocks of S−1(λ) and S−1
N+1(λ) in (5.161) according to
the proof of Lemma 1.58(vii) as
S−1(λ) (I 0)T = ˜RT (λ) K0(λ) Q−1 T
0
(λ),
S−1
N+1(λ) (I 0)T = RT (λ) KN+1(λ) Q−1 T
0
(λ),

(5.165)
where the matrices ˜R(λ) and R(λ) are deﬁned in (5.159). Then, by (5.161), (5.165),
and (1.193), we have
(S−1
−1(λ)) = LT
0 (λ)

−˙˜R(λ) J ˜RT (λ)
˜P(λ)
˜P T (λ)
−˙˜R(λ) J ˜RT (λ)

L0(λ),
˜P (λ) = K−1
0 (λ) ˙Q0(λ) Q−1
0 (λ) + R∗
0(λ) ˙R∗T
0 (λ) + R0(λ) ˙RT
0 (λ),
L0(λ) = diag{K0(λ) QT −1
0
(λ), Q0(λ)}
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(5.166)

5.2
Eigenvalue Problems with General Boundary Conditions
307
and in a similar way
(SN+1(λ)) = LT
N+1(λ)
−˙R(λ) J RT (λ)
P(λ)
P T (λ)
−˙R(λ) J RT (λ)

LN+1(λ),
P(λ) = K−1
N+1(λ) ˙QN+1(λ) Q−1
N+1(λ)
+R∗
N+1(λ) ˙R∗T
N+1(λ) + RN+1(λ) ˙RT
N+1(λ),
LN+1(λ) = diag{KN+1(λ) QT −1
N+1(λ), QN+1(λ)}.
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
(5.167)
Then, under the assumption that Q0(λ) and QN+1(λ) are the fundamental matrices
of the differential equations (compared with Remark 1.46)
˙Q0(λ) = −K0(λ) [R∗
0(λ) ˙R∗T
0 (λ) + R0(λ) ˙RT
0 (λ)] Q0(λ),
˙QN+1(λ) = −KN+1(λ) [R∗
N+1(λ) ˙R∗T
N+1(λ) + RN+1(λ) ˙RT
N+1(λ)] QN+1(λ),

(5.168)
we have in (5.166) and (5.167) that ˜P (λ) = P(λ) = 0. Then by (5.157) and (5.158),
we obtain
(S−1
−1(λ)) = LT
0 (λ)

−˙˜R(λ) J ˜RT (λ)
0
0
−˙˜R(λ) J ˜RT (λ)

L0(λ) ≤0,
(5.169)
which yields by using Proposition 1.76(iv) that (S−1(λ)) ≥0. Similarly,
(SN+1(λ)) = LT
N+1(λ)
−˙R(λ) J RT (λ)
0
0
−˙R(λ) J RT (λ)

LN+1(λ) ≥0.
(5.170)
Thus, we have proved that monotonicity conditions (5.157) and (5.158) really
imply (5.169) and (5.170) with the matrices S−1(λ) and SN+1(λ) given by (5.161),
(5.165), and (5.168).
Step 3 Lastly, We remark that according to the above construction, the block
diagonal matrices L0(λ) and LN+1(λ) are nonsingular, so that conditions (5.169),
(5.170), (5.168) imply (5.157), (5.158).
⊓⊔
As a direct consequence of the proof of Lemma 5.40, we formulate the main
result of this section.
Theorem 5.41 Under assumptions (5.2), (5.3), (5.156), (5.157), and (5.158) with
all coefﬁcient matrices being piecewise continuously differentiable (with respect to
λ ∈R), the problem (SDSλ), (5.155) is equivalent to the extended problem (5.162),

308
5
Discrete Symplectic Eigenvalue Problems
where the matrices S−1(λ) and SN+1(λ) obey monotonicity conditions (5.163) and
are deﬁned as
S−1(λ) =
A−1(λ) B−1(λ)
C−1(λ) D−1(λ)

:= V−1(λ) diag{QT −1
0
(λ), Q0(λ)},
V−1(λ) :=
R∗T
0 (λ) K0(λ) −RT
0 (λ)
RT
0 (λ) K0(λ) R∗T
0 (λ)

,
(5.171)
K0(λ) := [R∗
0(λ) R∗T
0 (λ) + R0(λ) RT
0 (λ)]−1
(5.172)
and
SN+1(λ) =
AN+1(λ) BN+1(λ)
CN+1(λ) DN+1(λ)

:= diag{QT
N+1(λ), Q−1
N+1(λ)}VN+1(λ),
VN+1(λ) :=

R∗
N+1(λ)
RN+1(λ)
−KN+1(λ) RN+1(λ) KN+1(λ) R∗
N+1(λ)

,
(5.173)
KN+1(λ) := [R∗
N+1(λ) R∗T
N+1(λ) + RN+1(λ) RT
N+1(λ)]−1.
(5.174)
Here Q0(λ) and QN+1(λ) are the fundamental matrices of the differential equations
in (5.168). In particular, if R∗
i (λ) and Ri(λ) do not depend on λ, then one can put
Qi(λ) ≡I for i ∈{0, N + 1}.
Remark 5.42 Formulas (5.171) and (5.173) yield that the coefﬁcients B−1(λ) and
BN+1(λ) of the extended system are
B−1(λ) = −RT
0 (λ) Q0(λ),
BN+1(λ) = QT
N+1(λ) RN+1(λ),
λ ∈R.
(5.175)
Remark 5.43 Remark that the principal solution of the extended system in (5.162)
at k = −1 takes the value
Y [−1]
0
(λ) = S−1(λ) (0 I)T = J T ˜RT (λ) Q0(λ),
det Q0(λ) ̸= 0,
(5.176)
where Q0(λ) is the fundamental matrix of (5.168). Similarly, the principal solution
of the extended system in (5.162) at k = N + 2 takes the value
Y [N+2]
N+1 (λ) = S−1
N+1(λ) (0 I)T = J T RT (λ) QN+1(λ),
det QN+1(λ) ̸= 0,
(5.177)
where QN+1(λ) is the fundamental matrix of the second equation in (5.168).
We point out that in the subsequent formulations of the oscillation theorems
for (5.162), the nonsingular matrices Q0(λ) and QN+1(λ) can be omitted because
of Theorem 3.5(i), which says that the comparative index is invariant with respect to

5.2
Eigenvalue Problems with General Boundary Conditions
309
multiplication by nonsingular matrices, i.e., μ(YC, ˆY ˆC) = μ(Y, ˆY) with det C ̸= 0
and det ˆC ̸= 0.
In the theorem below, we present a similar result on the equivalence of
problem (SDSλ), (5.151) with the general (nonseparated) boundary conditions and
some 4n × 4n augmented problem with the separated boundary conditions. The
consideration is based on the notation in Sect. 3.3. Recall that in Sect. 3.3.5, we
introduced the notation
{W, V } :=
⎛
⎜⎜⎝
A 0 −B 0
0
ˆA 0
ˆB
−C 0 D
0
0
ˆC
0
ˆD
⎞
⎟⎟⎠,
{W} := {I, W} =
⎛
⎜⎜⎝
I 0 0 0
0 A 0 B
0 0 I 0
0 C 0 D
⎞
⎟⎟⎠,
where
W =
A B
C D

,
V =

ˆA ˆB
ˆC ˆD

are 2n × 2n matrices; see formula (3.101). We also use the 4n × n matrices; see
formula (3.52),
⟨W⟩=
⎛
⎜⎜⎝
I
0
A B
0 −I
C D
⎞
⎟⎟⎠.
Theorem 5.44 Under the assumptions (5.2), (5.3), (5.152), (5.153), the prob-
lem (SDSλ) with (5.151) is equivalent to the augmented eigenvalue problem with
the separated boundary conditions
˜yk+1(λ) = {Sk(λ)} ˜yk(λ),
k ∈[0, N]Z,
 0 0
−I I

˜x0(λ) +
−I −I
0
0

˜u0(λ) = 0,
R1(λ) ˜xN+1(λ) + R2(λ) ˜uN+1(λ) = 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.178)
where
4n({Sk(λ)}) := J4n{ ˙Sk(λ)} J4n{ST
k (λ)} J4n = {0, (Sk(λ))} ≥0.
(5.179)
Proof Let Z[0](λ) = ( ˜Y(λ), Y(λ)) be the fundamental symplectic matrix of (5.1)
given by the initial condition Z[0]
0 (λ) = I. Then the general solution of (5.1) is of
the form yk(λ) = Z[0]
k (λ) c with c ∈R2n. Substituting this solution into (5.151), we

310
5
Discrete Symplectic Eigenvalue Problems
obtain
R1(λ)

I
0
˜XN+1(λ) XN+1(λ)

c + R2(λ)

0
−I
˜UN+1(λ) UN+1(λ)

c
= R(λ) /Z[0]
N+1(λ)0 c = 0,
where R(λ) = (R1(λ) R2(λ)) as in the beginning of this subsection and where
/Z[0]
N+1(λ)0 = !Z[0]
N+1(λ)"S0(0 I)T according to (3.104) and the notation introduced
in (3.101) and (3.102). Observe that matrices {W} and {V } deﬁned by (3.101) satisfy
{W}{V } = {WV } (see (3.103)). Consequently, the 4n × 2n matrix
/
Z[0]
k (λ)
0
is
a conjoined basis (see Lemma 3.21(i)) of a symplectic system with the coefﬁcient
matrix {Sk(λ)}, and it obviously satisﬁes (5.178) for k = 0. Then yk(λ) = Z[0]
k (λ) c
is a solution of (5.1) with (5.151) if and only if ˜yk(λ) =
/
Z[0]
k (λ)
0
c is a solution of
problem (5.178).
⊓⊔
At the end of this section, we formulate an analog of Theorem 5.44 for the case
when boundary conditions (5.151) are transformed into the left endpoint k = 0.
Theorem 5.45 Under the assumptions (5.2), (5.3), (5.152), (5.153), the prob-
lem (SDSλ) with (5.151) is equivalent to the eigenvalue problem with the separated
boundary conditions
˜yk+1(λ) = {Sk(λ)} ˜yk(λ),
k ∈[0, N]Z,
˜R1(λ) ˜x0(λ) + ˜R2(λ) ˜u0(λ) = 0,
 0 0
−I I

˜xN+1(λ) +
−I −I
0
0

˜uN+1(λ) = 0,
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(5.180)
where we have monotonicity condition (5.179) for {Sk(λ)} and where
˜R1(λ) = R1(λ) P1,
˜R2(λ) = −R2(λ) P1,
P1 :=
0 I
I 0

.
(5.181)
Here ˜R(λ) J4n ˜RT (λ) = 0 and rank ˜R(λ) = 2n with ˜R(λ) := ( ˜R1(λ) ˜R2(λ)) and,
moreover, instead of (5.153), we have (compare with (5.157))
˙˜R1(λ) ˜RT
2 (λ) −˙˜R2(λ) ˜RT
1 (λ) = ˙˜R(λ) J4n ˜RT (λ) ≥0.
(5.182)
Proof Let Z[N+1](λ) = ( ˜Y(λ), Y(λ)) be the fundamental symplectic matrix
of (5.1) given by the initial condition Z[N+1]
N+1 (λ) = I. Then the general solution
of (5.1) is of the form yk = Z[N+1]
k
(λ) c with c ∈R2n. Substituting this solution

5.2
Eigenvalue Problems with General Boundary Conditions
311
into (5.151), we obtain
R1(λ)
 ˜X0(λ) X0(λ)
I
0

c + R2(λ)
−˜U0(λ) −U0(λ)
0
I

c
= 
R1(λ) P1 −R2(λ) P1
 /Z[N+1]
0
(λ)0 c = 0,
where
/
Z[N+1]
k
(λ)
0
is a conjoined basis of the symplectic system in (5.180),
and it obviously satisﬁes (5.180) for k = N + 1. Then yk(λ) = Z[N+1]
k
(λ) c
is a solution of (5.1) with (5.151) if and only if ˜yk(λ)
=
/Z[N+1]
k
(λ)0 c
is a solution of (5.180). Recall also that the transformation of R(λ)
=
(R1(λ) R2(λ)) =
˜R(λ) diag{P1, −P1} with the matrix P1 deﬁned in (1.151)
(see also Lemma 1.58(iv)) completely preserves the symplectic structure of the
boundary conditions, i.e.,
˜RT (λ) again obeys (1.147) and in the monotonicity
condition for ˜R(λ) the sign is opposite according to the demands for the separated
boundary conditions for k = 0; see (5.157).
⊓⊔
Remark 5.46 Assume for the moment that we construct extended problem (5.162)
for (5.178), and then the principal solution Y[−1](λ) of this problem obeys the
condition
Y[−1]
0
(λ) = ⟨I⟩,
(5.183)
i.e., instead of this principal solution, one can consider the conjoined basis /Z[0](λ)0
of the augmented system in (5.178). Here Z[0](λ) is the fundamental matrix of (5.1)
such that Z[0]
0 (λ) = I; see the proof of Theorem 5.44. In a similar way, the
principal solution Y[N+2](λ) at k = N +2 of extended problem (5.162) constructed
for (5.180) satisﬁes the condition
Y[N+2]
N+1 (λ) = ⟨I⟩,
(5.184)
and then the conjoined basis
/
Z[N+1](λ)
0
of the augmented system in (5.180) can be
used instead of Y[N+2](λ). Here Z[N+1](λ) is the fundamental matrix of (5.1) such
that Z[N+1]
N+1 (λ) = I; see the proof of Theorem 5.45.
5.2.2
Transformation of Quadratic Functionals
In this subsection we extend the transformations from the previous subsection
to include the associated quadratic functionals. Following (2.56) and (5.138), we
consider the associated quadratic functional for admissible functions y = (x, u),
i.e., xk+1 = Ak(λ) xk +Bk(λ) uk for k ∈[0, N]Z. Given the 2n×2n matrices R1(λ)

312
5
Discrete Symplectic Eigenvalue Problems
and R2(λ) satisfying (5.152), we deﬁne the symmetric 2n × 2n matrix (see (2.92))
(λ) := R†
2(λ) R1(λ) R†
2(λ) R2(λ)
(5.185)
and the quadratic functional
G(y, λ) := F(y, λ) +
 x0
xN+1
T
(λ)
 x0
xN+1

,
(5.186)
where F(y, λ) is deﬁned in (5.138) and where y is admissible and satisﬁes the
general boundary conditions
 x0
xN+1

∈Im RT
2 (λ).
(5.187)
The functional G(·, λ) is said to be positive if G(y, λ) > 0 for every admissible
y = (x, u) satisfying (5.187) and x ̸≡0. In this case we write G(·, λ) > 0. Similarly,
the functional G(·, λ) is said to be nonnegative if G(y, λ) ≥0 for every admissible
y = (x, u) satisfying (5.187), we write G(·, λ) ≥0.
When the boundary conditions are separated, i.e., when (5.154) holds with
n × n matrices R0(λ), R∗
0(λ), RN+1(λ), R∗
N+1(λ) satisfying (5.156), the functional
G(y, λ) in (5.186) takes the form
G(y, λ) = F(y, λ) + xT
0 0(λ) x0 + xT
N+1 N+1(λ) xN+1,
(5.188)
where the symmetric n × n matrices 0(λ) and N+1(λ) are deﬁned by (see (2.83))
0(λ) := −R†
0(λ) R∗
0(λ) R†
0(λ) R0(λ),
N+1(λ) := R†
N+1(λ) R∗
N+1(λ) R†
N+1(λ) RN+1(λ),

(5.189)
so that (λ) = diag{0(λ), N+1(λ)}. The boundary conditions in (5.187) are then
also separated and have the form
x0 ∈Im RT
0 (λ),
xN+1 ∈Im RT
N+1(λ).
(5.190)
First we consider the quadratic functional G(y, λ) in (5.188) with separated end-
points (5.190). We deﬁne the extended quadratic functional Fext(y, λ) in (5.191),
compared with (5.138), by
Fext(y, λ) :=
N+1

k=−1
yT
k

ST
k (λ) K Sk(λ) −K

yk,
(5.191)
= F(y, λ) + γ−1(λ) + γN+1(λ),
(5.192)

5.2
Eigenvalue Problems with General Boundary Conditions
313
for an admissible y on [−1, N + 2]Z satisfying x−1 = 0 = xN+2. Here the matrices
S−1(λ) and SN+1(λ) are deﬁned by (5.171) and (5.173) and
γk(λ) := yT
k

ST
k (λ) K Sk(λ) −K

yk,
k ∈{−1, N + 1}.
Lemma 5.47 Assume that the matrices R0(λ), R∗
0(λ), RN+1(λ), R∗
N+1(λ) sat-
isfy (5.156) and 0(λ), N+1(λ) are deﬁned by (5.189). If y is admissible on
[−1, N +2]Z with x−1 = 0 = xN+2, then y is admissible on [0, N +1]Z with (5.190)
and G(y, λ) = Fext(y, λ). Conversely, if y is admissible on [0, N +1]Z with (5.190),
then it can be extended to be admissible on [−1, N +2]Z with x−1 = 0 = xN+2 and
Fext(y, λ) = G(y, λ). In particular, we have G(·, λ) > 0 over (5.190) if and only if
Fext(·, λ) > 0 over x−1 = 0 = xN+2 and G(·, λ) ≥0 over (5.190) if and only if
Fext(·, λ) ≥0 over x−1 = 0 = xN+2.
Proof First we assume that y is admissible on [−1, N + 2]Z with x−1 = 0 = xN+2.
Then y is admissible on [0, N + 1]Z as well and
x0 = A−1(λ) x−1 + B−1(λ) u−1 = −RT
0 (λ) Q0(λ) u−1 ∈Im RT
0 (λ),
see (5.175). Moreover, we have
0 = xN+2 = AN+1(λ) xN+1 + BN+1(λ) uN+1
= QT
N+1(λ) [R∗
N+1(λ) xN+1 + RN+1(λ) uN+1],
which implies that, see Lemma 1.58(vii) and (5.156),
yN+1 =
xN+1
uN+1

∈Ker R∗
N+1(λ), RN+1(λ) = Im

−RT
N+1(λ)
R∗T
N+1(λ)

.
Therefore, for some d ∈Rn, we get
xN+1 = −RT
N+1(λ) d ∈Im RT
N+1(λ),
uN+1 = R∗T
N+1(λ) d.
(5.193)
Then by using x−1 = 0, B−1(λ) B†
−1(λ) = R†
0(λ) R0(λ), and the symmetry of the
matrix BT
−1(λ) D−1(λ), we obtain
γ−1(λ) = uT
−1BT
−1(λ) D−1(λ) u−1 = xT
0 0(λ) x0.
(5.194)
Moreover, following the proof of Lemma 2.30 and using (5.193), we have (sup-
pressing the argument λ by the coefﬁcients of SN+1(λ))
γN+1(λ) = xT
N+1CT
N+1(AN+1xN+1 + BN+1uN+1)
+ uT
N+1(DT
N+1AN+1 −I) xN+1 + uT
N+1DT
N+1BN+1uN+1

314
5
Discrete Symplectic Eigenvalue Problems
= (xT
N+1CT
N+1 + uT
N+1DT
N+1) xN+2 −uT
N+1xN+1
= xT
N+1 N+1(λ) xN+1.
(5.195)
This shows that G(y, λ) = Fext(y, λ). Conversely, if y is admissible on [0, N + 1]Z
with (5.190), i.e., x0 = RT
0 (λ) c and xN+1 = RT
N+1(λ) d for some c, d ∈Rn, then
we deﬁne
x−1 := 0,
u−1 := B†
−1(λ) x0,
uN+1 := R∗T
N+1(λ) d,
xN+2 := 0.
Then we get
A−1(λ) x−1 + B−1(λ) u−1
= B−1(λ) B†
−1(λ) x0 = R†
0(λ) R0(λ) RT
0 (λ) c = RT
0 (λ) c = x0,
AN+1(λ) xN+1 + BN+1(λ) uN+1
= QT
N+1(λ) [R∗
N+1(λ) RT
N+1(λ) −RN+1(λ) R∗T
N+1(λ)] d = 0 = xN+2.
Therefore, y is admissible on [−1, N + 2]Z with x−1 = 0 = xN+2. The same
calculations as in (5.194) and (5.195) then show that also in this case Fext(y, λ) =
G(y, λ). Finally, since we extend the x component of an admissible y at k = −1 and
k = N + 2 by the zero values x−1 = 0 and xN+2 = 0, it follows that G(·, λ) > 0
over (5.190) if and only if Fext(·, λ) > 0 over x−1 = 0 = xN+2, and G(·, λ) ≥0
over (5.190) if and only if Fext(·, λ) ≥0 over x−1 = 0 = xN+2. The proof is
complete.
⊓⊔
Next we consider the general quadratic functional G(y, λ) in (5.186) over the
jointly varying endpoints (5.187). Given the 2n × 2n matrices R1(λ), R2(λ), and
(λ) satisfying (5.152) and (5.185), we deﬁne the auxiliary 2n × 2n matrices
˜R∗
0(λ) :=
 0 0
−I I

,
˜R0(λ) :=
−I −I
0
0

,
˜R∗
N+1(λ) := R1(λ),
˜RN+1(λ) := R2(λ),
˜0(λ) := 0,
˜N+1(λ) := (λ).
⎫
⎪⎬
⎪⎭
(5.196)
In addition, we deﬁne the the augmented quadratic functional
˜Faug( ˜y, λ) := ˜F( ˜y, λ) + ˜xT
0 ˜0 ˜x0 + ˜xT
N+1 ˜N+1 ˜xN+1
(5.197)
over the separated endpoints
˜x0 ∈Im ˜RT
0 (λ),
˜xN+1 ∈Im ˜RT
N+1(λ),
(5.198)

5.2
Eigenvalue Problems with General Boundary Conditions
315
where the functional ˜F( ˜y, λ) is deﬁned by, compared with (5.138),
˜F( ˜y, λ) :=
N

k=0
˜yT
k
 ˜ST
k (λ) ˜K ˜Sk(λ) −˜K

˜yk,
˜K :=
02n 02n
I2n 02n

,
(5.199)
and where the 4n × 4n matrix ˜Sk(λ) is the coefﬁcient matrix of the symplectic
system in (5.178), i.e., for k ∈[0, N]Z, we set
˜Sk(λ) =
 ˜Ak(λ) ˜Bk(λ)
˜Ck(λ) ˜Dk(λ)

:= {Sk(λ)} =
⎛
⎜⎜⎝
I
0
0
0
0 Ak(λ) 0 Bk(λ)
0
0
I
0
0 Ck(λ) 0 Dk(λ)
⎞
⎟⎟⎠.
(5.200)
It is easy to verify by a direct calculation that
˜ST
k (λ) ˜K ˜Sk(λ) −˜K = {0, ST
k (λ) K Sk(λ) −K},
(5.201)
where we use the notation given by (3.101).
The following result describes the connection between the functionals G(y, λ)
in (5.186) and ˜Faug( ˜y, λ) in (5.197).
Lemma 5.48 Assume that the matrices R1(λ), R2(λ) satisfy (5.152) and (λ) is
deﬁned by (5.185). If y is (A(λ), B(λ))-admissible with (5.187), then ˜y = (˜x, ˜u)
deﬁned by
˜xk :=
x0
xk

,
k ∈[0, N + 1]Z,
˜uk :=
−u0
uk

,
k ∈[0, N]Z,
(5.202)
is ( ˜A(λ), ˜B(λ))-admissible with (5.198) and ˜Faug( ˜y, λ) = G(y, λ). Conversely, if ˜y
is ( ˜A(λ), ˜B(λ))-admissible with (5.198), then y = (x, u) given by
xk := (0 I) ˜xk,
k ∈[0, N + 1]Z,
uk := (0 I) ˜uk,
k ∈[0, N]Z,
(5.203)
is (A(λ), B(λ))-admissible with (5.187) and G(y, λ) = ˜Faug( ˜y, λ). In particular,
we have G(·, λ) > 0 over (5.187) if and only if ˜Faug(·, λ) > 0 over (5.198) and
G(·, λ) ≥0 over (5.187) if and only if ˜Faug(·, λ) ≥0 over (5.198)
Proof Assume that y is (A(λ), B(λ))-admissible with (5.187) and deﬁne ˜y = (˜x, ˜u)
by (5.202). Then ˜y is ( ˜A(λ), ˜B(λ))-admissible, (5.198) holds, and the equality
˜Faug( ˜y, λ) = G(y, λ) follows by (5.201). Conversely, assume that ˜y = (˜x, ˜u)
is ( ˜A(λ), ˜B(λ))-admissible with (5.198). Then ˜xk has the form in (5.202) for
k ∈[0, N + 1]Z, while ˜uk = (βT
k , uT
k )T for some βk ∈Rn for k ∈[0, N]Z,
where y := (x, u) is (A(λ), B(λ))-admissible and satisﬁes (5.187). The equality
G(y, λ) = ˜Faug( ˜y, λ) again follows by (5.201).
⊓⊔

316
5
Discrete Symplectic Eigenvalue Problems
5.2.3
Oscillation Theorems for General Endpoints
In this subsection we derive the oscillation theorem for the eigenvalue problem (5.1)
with general boundary conditions. First we consider the separated endpoints (5.155).
Let ¯Y(λ) = ( ¯X(λ), ¯U(λ)) be the so-called natural conjoined basis of system (5.1),
i.e., it the conjoined basis satisfying the initial conditions
¯X0(λ) = −RT
0 (λ),
¯U0(λ) = R∗T
0 (λ),
λ ∈R.
(5.204)
Then we deﬁne for λ ∈R the auxiliary matrices
(λ) := R∗
N+1(λ) ¯XN+1(λ) + RN+1(λ) ¯UN+1(λ),
M(λ) := [I −(λ) †(λ)] RN+1(λ),
T (λ) := I −M†(λ) M(λ),
P(λ) := T (λ) ¯XN+1(λ) †(λ) RN+1(λ) T (λ).
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.205)
We will see in the proof of the main result below (Theorem 5.50) that under
assumptions (5.156)–(5.158) the quantity rank (λ) is piecewise constant on R.
This fact allows to make the following deﬁnition.
Deﬁnition 5.49 We say that λ0
∈R is a ﬁnite eigenvalue of problem (5.1)
with (5.155) if
θ(λ0) := rank(λ−
0 ) −rank(λ0) ≥1.
(5.206)
In this case the number θ(λ0) is called an algebraic multiplicity of λ0 as a ﬁnite
eigenvalue of (5.1), (5.155).
As in Sect. 5.1.3, we will count the ﬁnite eigenvalues including their multiplici-
ties. We will also use the notation:
n1(λ) := number of forward focal points of ¯Y(λ) in (0, N + 1],
(5.207)
n2(λ) := number of ﬁnite eigenvalues of (5.1), (5.155) in (−∞, λ],
(5.208)
p(λ) := rankM(λ) + ind P(λ),
(5.209)
that is, n1(λ) = l( ¯Y(λ), 0, N + 1) according to notation (4.10). The next result is
a generalization of Theorem 5.17 from Dirichlet boundary conditions x0(λ) = 0 =
xN+1(λ) to general separated boundary conditions (5.155). Namely, the choice of
R∗
0(λ) = I = R∗
N+1(λ) and R0(λ) = 0 = RN+1(λ) yields Theorem 5.17 from
Theorem 5.50.

5.2
Eigenvalue Problems with General Boundary Conditions
317
Theorem 5.50 (Global Oscillation Theorem for Separated Endpoints) Let us
assume that the matrices R0(λ), R∗
0(λ), RN+1(λ), R∗
N+1(λ) are piecewise con-
tinuously differentiable on R and satisfy conditions (5.156)–(5.158). Furthermore,
suppose that
rankBk(λ) is constant for λ ∈R for all k ∈[0, N]Z
rankR0(λ) and rank RN+1(λ) are constant for λ ∈R.

(5.210)
Then with the notation (5.207)–(5.209), we have for every λ ∈R the identities
n1(λ+) = n1(λ) ≤(N + 1) n,
p(λ+) = p(λ),
(5.211)
n2(λ+) = n2(λ),
(5.212)
n2(λ+) −n2(λ−) = n1(λ+) −n1(λ−) + p(λ+) −p(λ−),
(5.213)
and there exists ℓ∈[0, (N + 2) n]Z such that
n1(λ) + p(λ) = n2(λ) + ℓ,
λ ∈R.
(5.214)
Moreover, for a suitable λ0 < 0, we have
n2(λ) ≡0,
n1(λ) + p(λ) ≡ℓ,
λ ≤λ0.
(5.215)
Remark 5.51 Under the assumptions of Theorem 5.50, it follows by Theorem 5.3
that the ﬁnite eigenvalues of (5.1), (5.155) are isolated and bounded from below and
from above. The ﬁrst condition in (5.215) then means that these ﬁnite eigenvalues
are bounded from below, while conditions (5.214) and (5.211) imply that the ﬁnite
eigenvalues are bounded from above.
Proof of Theorem 5.50 The method is to transform (by Lemma 5.40) the prob-
lem (5.1), (5.155) into the extended problem (5.162) on the interval [−1, N + 2]Z,
to which the result in Theorem 5.17 can be applied. Consider the principal solution
Y [−1](λ) at k = −1 of the extended symplectic system in (5.162). Then by
Remark 5.43 (see (5.176)), we have Y [−1]
0
(λ) = ¯Y0(λ) Q0(λ), so that
Y [−1]
k
(λ) = ¯Yk(λ) Q0(λ),
k ∈[0, N + 1]Z,
ˆY [−1]
N+2(λ) = Q(λ) VN+1(λ) ¯YN+1(λ) Q0(λ),
Q(λ) := diag{QT
N+1(λ), Q−1
N+1(λ)},
⎫
⎪⎬
⎪⎭
(5.216)
where the symplectic matrix VN+1(λ) is given by (5.173). Then we can write
X[−1]
N+2(λ) = QT
N+1(λ) (λ) Q0(λ) by the deﬁnition of (λ) in (5.205). This implies
by Theorems 1.81 and 1.82 that the image of X[−1]
N+2(λ) and the rank of (λ) are
piecewise constant on R. Moreover, by (5.216) the multiplicities of focal points of

318
5
Discrete Symplectic Eigenvalue Problems
Y [−1]
k
(λ) and ¯Yk(λ) are connected as follows:
m(Y [−1]
−1 (λ)) = 0,
m(Y [−1]
k
(λ)) = μY [−1]
k+1 (λ), Sk(λ) (0 I)T  = μ ¯Yk+1(λ) Q0(λ), Sk(λ) (0 I)T 
= μ
 ¯Yk+1(λ), Sk(λ) (0 I)T 
= m( ¯Yk(λ)),
k ∈[0, N]Z,
where we used Lemma 4.7 (see (4.14)) and Theorem 3.5(i). Moreover, for k =
N + 1, we have (see (5.216))
m(Y [−1]
N+1(λ)) = μ

Y [−1]
N+2(λ), SN+1(λ) (0 I)T 
= μ

Q(λ) VN+1(λ) ¯YN+1(λ) Q0(λ), Q(λ) VN+1(λ) (0 I)T 
= μ

VN+1(λ) ¯YN+1(λ), VN+1(λ) (0 I)T 
= p(λ),
where we again used Lemma 4.7 (see (4.14)), Theorem 3.5(i)–(ii), the deﬁnition
of the comparative index μVN+1(λ) ¯YN+1(λ), VN+1(λ) (0 I)T  according to The-
orem 3.2(i), and (5.205) for p(λ). Note that by Theorem 5.15, we have that the
multiplicities m(Y [−1]
k
(λ)) of focal points of the principal solution Y [−1]
k
(λ) of the
extended problem (5.162) on the interval [−1, N + 2]Z are right-continuous in λ for
k ∈[0, N + 1]Z, and then the same property holds for m( ¯Yk(λ)) for k ∈[0, N]Z and
p(λ). Therefore, we have proved (5.211).
Moreover, the ﬁnite eigenvalues of the extended problem (5.162) according to
Deﬁnition 5.5 coincide with the ﬁnite eigenvalues of (5.1), (5.155) according to
Deﬁnition 5.49. By (5.175) and (5.210), we guarantee that assumption (5.38) is
satisﬁed for all k ∈[−1, N + 1]Z, and hence, the results in (5.213)–(5.215) follow
from the corresponding statements in Theorems 5.16 and 5.17. We remark that the
number ℓin (5.214) denotes the number of forward focal points of Y [−1](λ) in
(−1, N + 2] for λ sufﬁciently negative, and it is estimated, according to (5.47)
and (5.56), as
ℓ=
lim
λ→−∞[n1(λ) + p(λ)] ≤(N + 1) n + n = (N + 2) n.
(5.217)
The proof is complete.
⊓⊔
Remark 5.52 In the proof of Theorem 5.50, we have shown that the function p(λ)
given by (5.205) can be presented in terms of the comparative index as follows
p(λ) = μ(VN+1(λ) ¯YN+1(λ), VN+1(λ) (0 I)T ),
(5.218)
where VN+1(λ) is given by (5.173).

5.2
Eigenvalue Problems with General Boundary Conditions
319
The following result is a generalization of Theorem 5.31 from the Dirichlet
endpoints to the case of separated endpoints. We utilize the quadratic functional
G(y, λ) deﬁned in (5.188), which is considered over the separated endpoints (5.190).
Theorem 5.53 Assume that the matrices R0(λ), R∗
0(λ), RN+1(λ), R∗
N+1(λ) are
piecewise continuously differentiable on R and satisfy conditions (5.156)–(5.158)
and (5.210). Then with the notation (5.207)–(5.209), we have
n1(λ) + p(λ) = n2(λ),
λ ∈R,
(5.219)
if and only if there exists λ0 < 0 such that the quadratic functional G(y, λ) in (5.188)
obeys the condition G(·, λ0) > 0.
Proof The result follows from the application of Theorem 5.31 to the extended
problem (5.162) on the interval [−1, N + 2]Z, once we take into account the
connection between the quadratic functional G(y, λ) in (5.188) over (5.190) and
the extended quadratic functional Fext(y, λ) in (5.191) over x−1 = 0 = xN+2 in
Lemma 5.47.
⊓⊔
Next we consider the eigenvalue problem (5.1) with general jointly varying
endpoints (5.151). Under the notation of Theorem 5.44, consider the symplectic
fundamental matrix Z[0](λ) of (5.1) such that Z[0]
0 (λ) = I and for k ∈[0, N + 1]Z
deﬁne the augmented 4n × 2n matrix Y(λ) = ⟨Z[0](λ)⟩with the 2n × 2n blocks
Xk(λ) :=

I
0
˜Xk(λ) X[0]
k (λ)

,
Uk(λ) :=

0
−I
˜Uk(λ) U[0]
k (λ)

,
(5.220)
where Y [0](λ) =

X[0](λ)
U[0](λ)

is the principal solution of (5.1) at k = 0. We note
that the pair Y(λ) := (X(λ), U(λ)) constitutes a conjoined basis of the augmented
system in (5.178). Finally, following (5.205), we deﬁne the 2n × 2n matrices
L(λ) := R1(λ) XN+1(λ) + R2(λ) UN+1(λ),
M(λ) := [I −L(λ) L†(λ)] R2(λ),
T (λ) := I −M†(λ) M(λ),
P(λ) := T (λ) XN+1(λ) L†(λ) R2(λ) T (λ).
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.221)
We will see as before that under assumptions (5.152) and (5.153), the quantity
rankL(λ) is piecewise constant on R. Therefore, we make the following deﬁnition.
Deﬁnition 5.54 We say that λ0
∈R is a ﬁnite eigenvalue of problem (5.1)
with (5.151) if
ζ(λ0) := rank L(λ−
0 ) −rank L(λ0) ≥1.
(5.222)

320
5
Discrete Symplectic Eigenvalue Problems
In this case the number ζ(λ0) is called an algebraic multiplicity of λ0 as a ﬁnite
eigenvalue of (5.1), (5.151).
We will again count the ﬁnite eigenvalues including their multiplicities. We will
use the notation:
n1(λ) := number of forward focal points of Y [0](λ) in (0, N + 1],
(5.223)
n2(λ) := number of ﬁnite eigenvalues of (5.1), (5.151) in (−∞, λ],
(5.224)
q(λ) := rankM(λ) + ind P(λ),
(5.225)
that is, n1(λ) = l(Y [0](λ), 0, N + 1) according to (4.10). The next result is
a generalization of Theorem 5.17 from Dirichlet boundary conditions x0(λ) = 0 =
xN+1(λ) to general jointly varying boundary conditions (5.151). Namely, the choice
of R1(λ) = I and R1(λ) = 0 yields Theorem 5.17 from Theorem 5.55.
Theorem 5.55 (Global Oscillation Theorem for Joint Endpoints)
Suppose that
the matrices R1(λ) and R2(λ) are piecewise continuously differentiable on R and
satisfy conditions (5.152)–(5.153). Furthermore, suppose that
rankBk(λ) is constant for λ ∈R for all k ∈[0, N]Z
rankR2(λ) is constant for λ ∈R.

(5.226)
Then with the notation (5.223)–(5.225), we have for every λ ∈R the identities
n1(λ+) = n1(λ) ≤Nn, q(λ+) = q(λ),
(5.227)
n2(λ+) = n2(λ),
(5.228)
n2(λ+) −n2(λ−) = n1(λ+) −n1(λ−) + q(λ+) −q(λ−),
(5.229)
and there exists ℓ∈[0, (N + 2)n]Z such that
n1(λ) + q(λ) = n2(λ) + ℓ,
λ ∈R.
(5.230)
Moreover, for a suitable λ0 < 0, we have
n2(λ) ≡0,
n1(λ) + q(λ) ≡ℓ,
λ ≤λ0.
(5.231)
Proof We make the transformation of problem (5.1) with the joint boundary
conditions (5.151) to the augmented problem (5.178) with separated boundary
conditions, which is described in Theorem 5.44. We deﬁne the 2n × 2n matrices
˜R∗
0(λ), ˜R0(λ), ˜R∗
N+1(λ), ˜RN+1(λ) by (5.196) and the augmented problem
˜yk+1(λ) = ˜Sk(λ) ˜yk(λ),
k ∈[0, N]Z,
˜R∗
0(λ) ˜x0(λ) + ˜R0(λ) ˜u0(λ) = 0,
˜R∗
N+1(λ) ˜xN+1(λ) + ˜RN+1(λ) ˜uN+1(λ) = 0.
⎫
⎪⎬
⎪⎭
(5.232)

5.2
Eigenvalue Problems with General Boundary Conditions
321
with the 4n × 4n symplectic coefﬁcient matrix ˜Sk(λ) = {Sk(λ)} in (5.200). Then
the monotonicity assumption (5.157) for the augmented problem (5.232) is trivially
satisﬁed, since the matrices ˜R∗
0(λ) and ˜R0(λ) do not depend on λ, as well as
rank ˜R0(λ) ≡n is constant for λ ∈R. Assumption (5.158) for the augmented
problem (5.232) is also satisﬁed by condition (5.153). Note the correct inequality
“≤0” in the assumptions (5.158) and (5.153). Finally,
rank ˜Bk(λ) = rank diag{0, Bk(λ)} = rankBk(λ)
is constant for λ ∈R
for all k ∈[0, N]Z. Regarding the natural conjoined basis of the augmented problem,
by the uniqueness of solutions, we obtain that the augmented conjoined basis
˜Y(λ) = ( ˜X (λ), ˜U(λ)) of the system in (5.232) satisfying the initial conditions,
compared with (5.204),
˜X0(λ) = −˜RT
0 (λ) =
I 0
I 0

,
˜U0(λ) = ˜R∗T
0 (λ) =
0 −I
0 I

,
is equal to Y(λ) = (X(λ), U(λ)) given in (5.220). Thus, the ﬁnite eigenvalues
of problem (5.232) according to Deﬁnition 5.49 translate as the ﬁnite eigenvalues
of (5.1) with (5.151) according to Deﬁnition 5.54. The result in the theorem then
follows by the application of Theorem 5.50 to the augmented problem (5.232). In
more details, we prove that
m( ˜Yk(λ)) = m(Y [0]
k (λ)),
k ∈[0, N]Z,
(5.233)
where Y [0](λ) is the principal solution of (5.1) at k = 0. Indeed, we have by
Lemma 4.7 and Proposition 3.37 (see (3.107), (3.108))
m( ˜Yk(λ)) = μ⟨Z[0]
k+1⟩, {Sk(λ)}(02n I2n)T 
= μ

{Z[0]
k+1}(02n I2n)T , {Sk(λ)}(02n I2n)T 
= μ

Y [0]
k+1(λ), Sk(λ) (0 I)T 
= m(Y [0]
k (λ)).
We note that the number ℓin (5.230) denotes the number of forward focal points
of ˜Y(λ) or equivalently of Y [0](λ) in (0, N + 1] for λ sufﬁciently negative, and it
satisﬁes the estimate
ℓ=
lim
λ→−∞[n1(λ) + q(λ)] ≤Nn + 2n = (N + 2) n,
(5.234)
compared with (5.217). This estimate is better than the one, which we could obtain
from a direct application of Theorem 5.50 to the augmented problem in dimension
2n, which yields that ℓ≤(N + 2) 2n. The proof is complete.
⊓⊔

322
5
Discrete Symplectic Eigenvalue Problems
Remark 5.56 As in Remark 5.51, we point out that, under the assumptions of
Theorem 5.55, the ﬁnite eigenvalues of (5.1), (5.151) are isolated and bounded from
below and from above.
The following result is a generalization of Theorem 5.31 from the Dirichlet
endpoints to jointly varying endpoints. We utilize the quadratic functional G(y, λ)
deﬁned in (5.186), which is considered over the jointly varying endpoints (5.187).
Theorem 5.57 Assume that R1(λ) and R2(λ) are piecewise continuously differ-
entiable on R and satisfy conditions (5.152)–(5.153) and (5.226). Then with the
notation (5.223)–(5.225), we have
n1(λ) + q(λ) = n2(λ),
λ ∈R,
(5.235)
if and only if there exists λ0 < 0 such that the quadratic functional G(y, λ) in (5.186)
over (5.187) obeys the condition G(·, λ0) > 0.
Proof The result follows from the application of Theorem 5.53 to the augmented
problem (5.232), when we take into account the connection between the quadratic
functional G(y, λ) in (5.186) over (5.187) and the augmented quadratic functional
˜Faug( ˜y, λ) in (5.197) over (5.198) in Lemma 5.48.
⊓⊔
Remark 5.58 The periodic and antiperiodic boundary conditions are included
in (5.155) for the special choice of constant matrices R1(λ) and R2(λ). Indeed, for
the periodic endpoints, we have
y0(λ) = yN+1(λ),
R1(λ) =
 0 0
−I I

,
R2(λ) = −
I I
0 0

,
L(λ) = L1(λ) := J −J

˜YN+1(λ) Y [0]
N+1(λ)

= J I −Z[0](λ),
R†
2(λ) = 1
2 RT
2 (λ),
 = 0,
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.236)
where  is given by (5.185), while for the antiperiodic endpoints, we have
y0(λ) = −yN+1(λ),
R1(λ) = −
0 0
I I

,
R2(λ) =
−I I
0 0

,
L(λ) = L2(λ) := J + J

˜YN+1(λ) Y [0]
N+1(λ)

= J

I + Z[0](λ)

,
R†
2(λ) = 1
2 RT
2 (λ),
 = 0.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.237)
5.3
Linear Dependence on Spectral Parameter
In this section we consider an eigenvalue problem with a linear dependence on the
spectral parameter. In particular, we will investigate the problem with the special
linear dependence on λ, in which the ﬁrst equation of (SDSλ) does not depend on

5.3
Linear Dependence on Spectral Parameter
323
the spectral parameter λ, i.e.,
xk+1(λ) = Akxk(λ) + Bkuk(λ),
uk+1(λ) = Ckxk(λ) + Dkuk(λ) −λWkxk+1(λ),

k ∈[0, N]Z.
(5.238)
This will allow to use the theory of discrete quadratic functionals (see Sect. 2.3.2),
for which the set of admissible pairs y = (x, u) does not depend on λ. For this
special case, the matrix Sk(λ) in (E) has the form
Sk(λ) =

I
0
−λWk I

Sk,
Sk =
Ak Bk
Ck Dk

,
(5.239)
where Sk is symplectic and
Wk = WT
k ,
Wk ≥0,
k ∈[0, N]Z.
(5.240)
Then, applying Proposition 1.76(i) to the product

I
0
−λWk I

Sk, we see that the
symmetric matrix (Sk(λ)) for this special case has the form
k(λ) ≡k :=
Wk 0
0 0

≥0,
k ∈[0, N]Z,
λ ∈R.
(5.241)
In this section we apply the results of Sect. 5.1 to problem (E) with this special
linear dependence on λ; in particular, we concentrate on the speciﬁc properties of
this problem which do not hold for the general nonlinear case. Further results about
system (5.238) will be presented in Sects. 5.4 and 5.5.
5.3.1
Transformation of Boundary Conditions
In this subsection we consider system (5.238) together with the self-adjoint
boundary conditions (5.151), which are in the special form
R1(λ)
 x0(λ)
xN+1(λ)

+ R2(λ)
 −u0(λ)
uN+1(λ)

= 0,
R1(λ) := R1 −λR2 W,
R2(λ) := R2.
⎫
⎬
⎭
(5.242)
Here the constant matrices R1, R2, W obey the conditions (compared with (5.152))
R1, R2 ∈R2n×2n,
rank(R1 R2) = 2n,
R1RT
2 = R2RT
1 ,
W = WT .

(5.243)

324
5
Discrete Symplectic Eigenvalue Problems
Note that monotonicity assumption (5.153) for this special case is equivalent to
˙R1(λ) RT
2 = −R2 WRT
2 ≤0,
which is satisﬁed under the assumption
W ≥0.
(5.244)
For the separated boundary conditions in the block diagonal form
R1(λ) = diag{R∗
0(λ), R∗
N+1(λ)},
R2 = diag{−R0, RN+1},
W = diag{W−1, WN+1}
we have from (5.242) the conditions
R∗
0(λ) x0(λ) + R0u0(λ) = 0,
R∗
N+1(λ) xN+1(λ) + RN+1uN+1(λ) = 0,
R∗
0(λ) = R∗
0 + λR0W−1,
R∗
N+1(λ) = R∗
N+1 −λRN+1WN+1,
⎫
⎪⎬
⎪⎭
(5.245)
while conditions (5.243) translate as
R∗
0RT
0 = R0(R∗
0)T ,
rank(R∗
0, R0) = n,
R∗
N+1RT
N+1 = RN+1(R∗
N+1)T ,
rank (R∗
N+1, RN+1) = n,
W−1 = WT
−1,
WN+1 = WT
N+1.
⎫
⎪⎬
⎪⎭
(5.246)
The monotonicity conditions (5.157), (5.158) are rewritten in the form
˙R∗
0(λ) RT
0 = R0W0RT
0 ≥0,
˙R∗
N+1(λ) RT
N+1 = −RN+1WN+1RT
N+1 ≤0.
Therefore, according to (5.244), it is sufﬁcient to require that
W−1 ≥0,
(5.247)
WN+1 ≥0.
(5.248)
In particular, if R1(λ) is independent on λ, then monotonicity conditions (5.244),
(5.247), (5.248) are automatically satisﬁed. Moreover, for the choice of R∗
0 = I,
R0 = 0, R∗
N+1 = I, and RN+1 = 0, we obtain the Dirichlet boundary conditions
in (E).
The main purpose of this subsection is to transform the mentioned above bound-
ary conditions in such a way that new extended boundary value problem (5.162)
preserves the special linear dependence on λ given by (5.238).

5.3
Linear Dependence on Spectral Parameter
325
Now we prove the following results for the case of the separated boundary
conditions (5.242).
Lemma 5.59 Consider the low block triangular transformation ˜yk = Lk(λ) yk
with the matrices
Lk(λ) := I,
k ∈[0, N]Z,
LN+1(λ) :=

I
0
−λWN+1 I

.
(5.249)
Then the eigenvalue problem for system (5.238) with separated boundary condi-
tions (5.245) on [0, N + 1]Z takes the form
˜yk+1(λ) =

I
0
−λ ˜Wk I

Sk ˜yk(λ),
k ∈[0, N]Z,
˜Wk = Wk,
k ∈[0, N −1]Z,
˜WN = WN + WN+1 ≥0,
⎫
⎬
⎭
(5.250)
with the boundary conditions
R∗
0(λ) ˜x0(λ) + R0 ˜u0(λ) = 0,
R∗
0(λ) = R∗
0 + λR0W−1
R∗
N+1 ˜xN+1(λ) + RN+1 ˜uN+1(λ) = 0.

(5.251)
That is, for the transformed problem (5.250) with (5.251), the boundary conditions
for k = N + 1 are independent on λ.
Proof The proof follows from the representation of the boundary conditions in the
right point k = N + 1 as

R∗
N+1 −λRN+1WN+1 RN+1

=

R∗
N+1 RN+1

LN+1(λ),
where the matrix LN+1(λ) is deﬁned in (5.249).
⊓⊔
Remark 5.60 We note that the symplectic transformation with matrix (5.249)
preserves the multiplicities of focal points of any conjoined basis of (SDSλ)
according to Corollary 4.67.
In the next lemma, we consider the situation when the (separated) boundary
conditions (5.245) are independent on λ for k = N + 1.
Lemma 5.61 Under the additional assumption
WN+1 = 0,
(5.252)
the eigenvalue problem for system (5.238) with separated boundary condi-
tions (5.245) on [0, N + 1]Z can be extended to an eigenvalue problem of the
special linear form (5.239) on the interval [−1, N +2]Z with the Dirichlet boundary
conditions (5.160).

326
5
Discrete Symplectic Eigenvalue Problems
Proof In Lemma 5.40 for the nonlinear case, we preserve the construction for SN+1,
because under assumption (5.252) the matrix SN+1 is independent on λ (see the
proof of Lemma 5.40). For the left point k = 0, we introduce the notation
˜R :=

R∗
0
R0

and construct the constant matrix S−1 such that
S−1 =
 ˜RT K J T ˜RT 
,
K := ( ˜R ˜RT )−1.
Then we complete the construction of the matrix S−1(λ) by
S−1(λ) =

I
0
−λW−1 I
  ˜RT K J T ˜RT  .
(5.253)
The result then follows from Lemma 5.40.
⊓⊔
Based on the results of Lemmas 5.59 and 5.61, one can reformulate Theorem 5.41
for the case of the special linear dependence on λ.
Theorem 5.62 Under assumptions (5.240), (5.246), (5.247), and (5.248), the
eigenvalue problem for system (5.238) with separated boundary conditions (5.245)
on [0, N + 1]Z is equivalent to the extended problem
˜yk+1(λ) =

I
0
−λ ˜Wk I

Sk ˜yk(λ), k ∈[−1, N + 1]Z,
˜x−1(λ) = 0 = ˜xN+2(λ),
where the symmetric matrix
˜Wk :=
⎧
⎪⎨
⎪⎩
Wk,
k ∈[−1, N −1]Z,
WN + WN+1,
k = N,
0,
k = N + 1.
(5.254)
is nonnegative deﬁnite for all k ∈[−1, N + 1]Z,
S−1 =
A−1 B−1
C−1 D−1

:=
R∗
0
T K0 −RT
0
RT
0 K0 R∗
0
T

,
K0 := (R∗
0R∗
0
T + R0RT
0 )−1
⎫
⎪⎬
⎪⎭
(5.255)
and
SN+1 =
AN+1 BN+1
CN+1 DN+1

:=

R∗
N+1
RN+1
−KN+1RN+1 KN+1R∗
N+1

,
KN+1 := (R∗
N+1R∗T
N+1 + RN+1RT
N+1)−1.
⎫
⎪⎬
⎪⎭
(5.256)

5.3
Linear Dependence on Spectral Parameter
327
Proof The proof is based on the subsequent application of Lemmas 5.59 and 5.61.
At the ﬁrst step, applying Lemma 5.59, we derive the new boundary problem
with the additional assumption (5.252). Applying Lemma 5.61 to this problem and
incorporating (5.250), we complete the proof of Theorem 5.62.
⊓⊔
Next we apply Theorem 5.45 for the nonlinear case to the linear problem (5.238)
with (5.242).
Theorem 5.63 Under
assumptions
(5.243)
and
(5.244),
problem
(5.238)
with (5.242) is equivalent to the linear eigenvalue problem with the separated
boundary conditions
˜yk+1(λ) =

I
0
−λWk I
 
{Sk} ˜yk(λ),
k ∈[0, N]Z,
˜R1(λ) ˜x0(λ) + ˜R2(λ) ˜u0(λ) = 0,
˜R1(λ) := ˜R1 + λ ˜R2 ˜W, ˜R2(λ) := ˜R2
 0 0
−I I

˜xN+1(λ) +
−I −I
0
0

˜uN+1(λ) = 0,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.257)
where
˜R1 = R1 P1,
˜R2 = −R2 P1,
˜W = P1WP1,
P1 =
0 I
I 0

,
(5.258)
and
˙˜R1(λ) ˜RT
2 = ˜R2 ˜W ˜RT
2 ≥0.
(5.259)
Proof According to Theorem 5.45, the coefﬁcients in (5.242) are transformed as
˜R(λ) =
 ˜R1(λ) ˜R2

=

R1 R2
 
I
0
−λW I

diag{P1, −P1}
=

R1 R2

diag{P1, −P1} diag{P1, −P1}

I
0
−λW I

diag{P1, −P1}
=
 ˜R1 ˜R2
 
I
0
λP1WP1 I

=
 ˜R1 ˜R2
  I
0
λ ˜W I

=  ˜R1 + λ ˜R2 ˜W ˜R2
 ,
where the matrices above are deﬁned by (5.257) and (5.258). Applying Theo-
rem 5.45 to the case under the consideration, we complete the proof.
⊓⊔

328
5
Discrete Symplectic Eigenvalue Problems
Remark 5.64
(i) Observe that according to our construction, the matrix of the symplectic system
in (5.257) is of the same form as the matrix of system (5.239); since by
deﬁnition (3.101), we have

I
0
−λW I
 
=
⎛
⎜⎜⎝
I
0
0 0
0
I
0 0
0
0
I 0
0 −λW 0 I
⎞
⎟⎟⎠.
Consequently, the weight matrix by −λ is equal to diag{0, W} ≥0.
(ii) Note that in problem (5.257) the boundary condition for k = N + 1 is
independent on λ. Applying Lemma 5.61, one can extend (5.257) to the
problem with the Dirichlet boundary conditions preserving the special linear
dependence on λ. By a similar way, one can modify Theorem 5.44 for the
nonlinear case to derive a problem with the separated boundary conditions,
which are independent on λ for k = 0, and then apply Theorem 5.62 for the
construction of the extended problem with the Dirichlet boundary conditions,
again with the preservation of the special linear dependence on λ.
5.3.2
Quadratic Functionals
In this subsection we analyze the quadratic functional G(y, λ) deﬁned in (5.186)
over the joint endpoints (5.187), respectively, the quadratic functional G(y, λ)
deﬁned in (5.188) over the separated endpoints (5.190), when the boundary
conditions depend linearly on λ as in Sect. 5.3.1.
Given system (SDSλ) and the 2n × 2n matrices R1, R2, W satisfying (5.243)
and (5.244), the functional G(y, λ) in (5.186) takes the form
G(y, λ) = F0(y)+
 x0
xN+1
T
−λ W
  x0
xN+1

−λ
N

k=0
xT
k+1Wkxk+1,
(5.260)
where F0(y) is the basic functional deﬁned in (2.56) and  := R†
2R1R†
2R2 as
in (5.185). (Note that in Sect. 2.3 the functional F0(y) is denoted by F(y).) The
functional G(y, λ) in (5.260) acts on admissible functions y = (x, u) satisfying the
boundary conditions
 x0
xN+1

∈Im RT
2 .
(5.261)

5.3
Linear Dependence on Spectral Parameter
329
Observe that the admissibility condition, i.e., the ﬁrst equation of system (5.238),
and the boundary conditions (5.261) do not depend on λ. The form of the functional
G(y, λ) in (5.260) yields the following simple comparison result.
Proposition 5.65 Assume that (5.240) and (5.244) hold and consider the functional
G(y, λ) in (5.260). Then for every λ, λ0 ∈R, λ < λ0, we have the inequality
G(y, λ) ≥G(y, λ0) for every admissible y satisfying (5.261). Consequently, if
G(·, λ0) > 0 for some λ0 ∈R, then G(y, λ) > 0 for all λ < λ0 as well.
Similarly, given n × n matrices R∗
0, R0, R∗
N+1, RN+1, and W−1, WN+1
satisfying (5.246), (5.247), and (5.248), the functional G(y, λ) in (5.188) takes the
form
G(y, λ) = F0(y) + xT
0 (0 −λW−1) x0
+xT
N+1(N+1 −λWN+1) xN+1 −λ
N

k=0
xT
k+1Wkxk+1,
⎫
⎪⎪⎬
⎪⎪⎭
(5.262)
where 0 := −R†
0R∗
0R†
0R0 and N+1 := R†
N+1R∗
N+1R†
N+1RN+1 as in (5.189). The
functional G(y, λ) in (5.262) acts on admissible functions y = (x, u) satisfying the
separated boundary conditions
x0 ∈Im RT
0 ,
xN+1 ∈Im RT
N+1.
(5.263)
Observe that the boundary conditions (5.263) do not depend on λ. Motivated by the
statements of the global oscillation theorems (Theorems 5.31, 5.53, and 5.57), our
aim is to ﬁnd conditions, which will guarantee the existence of λ0 < 0 such that
the functional G(·, λ0) is positive deﬁnite. The statement in Proposition 5.65 then
holds for the functional G(y, λ) in (5.262) under the assumptions (5.240), (5.247),
and (5.248).
For the next results, we recall the deﬁnition in (2.64) of the symmetric n × n
matrix Ek = BkB†
kDkB†
k, which satisﬁes BT
k EkBk = BT
k Dk for k ∈[0, N]Z.
Proposition 5.66 Assume
W−1 ≥0,
Wk > 0,
k ∈[0, N −1]Z,
WN ≥0,
WN+1 ≥0,
(5.264)
and for some λ < 0, we have
0 −CT
0A0 + AT
0 E0A0 −λW−1 > 0
on Im RT
0
(5.265)
N+1 + EN −λ(WN + WN+1) > 0
on Im RT
N+1.
(5.266)
Then there exists λ0 < 0 such that G(·, λ0) > 0 over (5.263).

330
5
Discrete Symplectic Eigenvalue Problems
Proof For an admissible y, we rewrite G(y, λ) by using the matrix Ek as
G(y, λ) = xT
0 (0 −CT
0A0 + AT
0 E0A0 −λW−1) x0
+
N−1

k=0
!2xT
k (CT
k −AT
k Ek) xk+1 + xT
k+1(Ek −CT
k+1Ak+1 −λWk) xk+1
"
+ 2xT
N(CT
N −AT
NEN) xN+1 + xT
N+1[ N+1 + EN −λ(WN + WN+1)] xN+1.
Therefore, the positivity assumption on Wk for k ∈[0, N −1]Z in (5.264) and the
positivity assumptions (5.265) and (5.266) imply that for some λ0 < 0 the functional
G(y, λ0) > 0 for all admissible y with (5.263) and x = {xk}N+1
k=0 ̸≡0.
⊓⊔
Consider now a special case of the functional G(y, λ) in (5.262) in which the
matrices W−1 = 0 = WN+1, i.e.,
G(y, λ) = F0(y) + xT
0 0x0 + xT
N+1N+1xN+1 −λ
N

k=0
xT
k+1Wkxk+1.
(5.267)
Proposition 5.67 Suppose that
Wk > 0,
k ∈[0, N]Z,
(5.268)
0 −AT
0 C0 + AT
0 E0A0 > 0
on Im RT
0 .
(5.269)
Then there exists λ0 < 0 such that the functional G(y, λ0) in (5.267) is positive
deﬁnite over (5.263).
Proof The result follows from Proposition 5.66, since assumptions (5.268)
and (5.269) imply the validity of (5.264)–(5.266) with W−1 := 0 and WN+1 := 0.
⊓⊔
Remark 5.68 One may certainly formulate numerous other sufﬁcient conditions
for (5.264)–(5.266) to be satisﬁed involving nonzero weight matrices W−1 and/or
WN+1. For example, one such condition could be
Wk > 0,
k ∈[−1, N −1]Z,
WN + WN+1 > 0,
WN ≥0,
WN+1 ≥0,
(5.270)
or the following ones, implying (5.270),
Wk > 0,
k ∈[−1, N]Z,
WN+1 ≥0,
(5.271)
Wk > 0,
k ∈[−1, N + 1]Z.
(5.272)
Condition (5.269) is trivially satisﬁed for the Dirichlet boundary conditions x0 =
0 = xN+1, i.e., for R0 = 0 = RN+1. In this case, in agreement with the notation

5.3
Linear Dependence on Spectral Parameter
331
in (5.138), the functional G(y, λ) in (5.267) has the form
F(y, λ) = F0(y) −λ
N

k=0
xT
k+1Wk xk+1.
(5.273)
Thus, we obtain from Proposition 5.67 the following.
Corollary 5.69 Suppose that (5.268) holds. Then there exists λ0 < 0 such that the
functional F(y, λ) in (5.273) is positive deﬁnite over x0 = 0 = xN+1.
Remark 5.70 The question of ﬁnding some simple and easy to verify sufﬁcient con-
ditions for the positivity of the functional G(·, λ0) for some λ0 < 0 in (5.260) over
joint endpoints (5.261) in terms of the coefﬁcients of system (5.1), e.g., a “positivity
condition” on the weight matrices Wk and W in the spirit of Proposition 5.67 and
Corollary 5.69, remains an open problem.
5.3.3
Finite Eigenvalues and Finite Eigenfunctions
In this subsection we analyze special properties of the ﬁnite eigenvalues of
problem (SDSλ) with (5.239) and with the boundary conditions depending linearly
on λ as in Sect. 5.3.1. In particular, we will discuss the results of the corresponding
global oscillation theorems, including special properties of ﬁnite eigenvalues and
ﬁnite eigenfunctions, which can be derived for this case.
First we consider the general boundary conditions (5.242) determined by the
2n × 2n matrices R1, R2, and W satisfying (5.243) and (5.244). According to
Deﬁnition 5.54, the ﬁnite eigenvalues are determined by the behavior of the function
L(λ) in (5.221) deﬁned through the matrices Xk(λ) and Uk(λ) in (5.220) and
through the conjoined bases Y [0](λ) and ˜Y(λ) of (5.238) such that ˜Y0(λ) = (I 0)T
and Y [0](λ) is the principal solution at k = 0. Since the dependence on λ in
system (SDSλ) and in the boundary conditions (5.242) is now linear, and since the
initial conditions of X(λ) and U(λ) at k = 0 do not depend on λ, it follows that
L(λ) is a matrix polynomial. This means that for each λ0 ∈R, the one-sided limits
of the rank of L(λ) satisfy
rank L(λ+
0 ) = r = rank L(λ−
0 ),
r := max
λ∈R rank L(λ).
(5.274)
This means that the algebraic multiplicity ζ(λ0) in (5.222) is equal to
ζ(λ0) = r −rank L(λ0)
(5.275)
with r given in (5.274). In order to understand the deﬁnition of a ﬁnite eigenfunction
for the problem (SDSλ) with (5.242), we combine the results in Sects. 5.1.2
and 5.3.1.

332
5
Discrete Symplectic Eigenvalue Problems
Deﬁnition 5.71 Assume (5.239), (5.240), (5.243), and (5.244). A solution y =
(x, u) of (SDSλ) and (5.242) with λ
=
λ0 is called a ﬁnite eigenfunction
corresponding to a ﬁnite eigenvalue λ0 if it satisﬁes the nondegeneracy condition
{Wk xk+1}N
k=0 ̸≡0
or
W
 x0
xN+1

̸= 0.
(5.276)
The dimension ω(λ0) of the space of all ﬁnite eigenfunctions y corresponding to
the ﬁnite eigenvalue λ0 is called a geometric multiplicity of λ0.
In the following result, we provide a geometric characterization of the ﬁnite
eigenvalues of (SDSλ) with (5.239). We thus extend Theorem 5.11 to general jointly
varying endpoints.
Theorem 5.72 Let assumptions (5.239), (5.240), (5.243), and (5.244) be satisﬁed.
A number λ0 is a ﬁnite eigenvalue of (SDSλ) with (5.239) with algebraic multiplicity
ζ(λ0) ≥1 in (5.275) if and only if there exists a corresponding ﬁnite eigenfunction
y for λ0. In this case, the geometric multiplicity of λ0 is equal to its algebraic
multiplicity, i.e., ω(λ0) = ζ(λ0).
Proof We proceed by adopting the transformations to the augmented eigenvalue
problem in Theorem 5.44 and then to the extended eigenvalue problem in The-
orem 5.62 (see Remark 5.64(ii)). In this way we obtain an extended augmented
problem over the interval [−1, N + 2]Z with the augmented functions ˜xk =
 x0
xk

and ˜uk =
 −u0
uk

and the augmented matrices ˜Wk, which have according to (5.254)
the form
˜W−1 = 0,
˜Wk = diag{0, Wk},
k ∈[0, N −1]Z,
˜WN = diag{0, WN} + W,
˜WN+1 = 0.

(5.277)
The weight matrix ˜k(λ) ≡˜k ≥0 on [−1, N + 1]Z corresponding to (5.241) then
has the form ˜k = diag{ ˜Wk, 0} for k ∈[−1, N + 1]Z. The degeneracy condition
in Deﬁnition 5.8 for this extended augmented problem reads as
˜Wk ˜xk+1 ≡0 on
[−1, N + 1]Z, i.e.,
Wk xk+1 ≡0
on [0, N −1]Z,
WNxN+1 + W
 x0
xN+1

= 0.
(5.278)
Since WN ≥0 and W ≥0 is assumed, condition (5.278) and hence the degeneracy
condition is equivalent to Wkxk+1 ≡0 on [0, N]Z and W

x0
xN+1

= 0. Therefore,
our nondegeneracycondition in (5.276) means that the corresponding solution of the
extended augmented problem is also nondegenerate on [−1, N +2]Z. The statement
in the theorem then follows from the application of Theorem 5.11 to the extended
augmented problem.
⊓⊔

5.3
Linear Dependence on Spectral Parameter
333
In the last part of this subsection, we will discuss the properties of the ﬁnite
eigenvalues of (SDSλ) with (5.242) from the point of view of self-adjoint boundary
value problems. Based on Theorem 5.72, we may deﬁne a ﬁnite eigenvalue
of (SDSλ) with (5.239) in the context of the linear dependence on the spectral
parameter in the symplectic system and in the boundary conditions as a number λ0 ∈
C, for which there exists corresponding a solution y = (x, u) of (SDSλ) with (5.242)
satisfying (5.276). This means, that we now allow complex ﬁnite eigenvalues. Then
we can actually prove that these ﬁnite eigenvalues are real (Proposition 5.74) and
the ﬁnite eigenfunctions corresponding to different ﬁnite eigenvalues are orthogonal
(Proposition 5.73). Here we consider the semi-inner product
⟨y, ˜y⟩:=
N

k=0
xT
k+1Wk ˜xk+1 +
 x0
xN+1
T
W
 ˜x0
˜xN+1

.
(5.279)
Proposition 5.73 Assume (5.239), (5.240), (5.243), and (5.244). Let y and ˜y be
ﬁnite eigenfunctions corresponding to the ﬁnite eigenvalues λ and ˜λ of (SDSλ)
with (5.242), and let λ ̸= ˜λ. Then ⟨y, ˜y⟩= 0, i.e., the ﬁnite eigenfunctions y and ˜y
are orthogonal with respect to the inner product (5.279).
Proof Let y satisfy (SDSλ) with (5.239), and let ˜y satisfy (SDSλ) with (5.239) for
the spectral parameter ˜λ. Then
(λ −˜λ) ⟨y, ˜y⟩= (λ −˜λ)
N

k=0
xT
k+1Wk ˜xk+1 + (λ −˜λ)

x0
xN+1
T
W

˜x0
˜xN+1

=
N

k=0
!
(uT
k+1 −xT
k CT
k −uT
k DT
k ) ˜xk+1 −xT
k+1(˜uk+1 −Ck ˜xk −Dk ˜uk)
"
+ (λ −˜λ)

x0
xN+1
T
W

˜x0
˜xN+1

=
N

k=0
!
xT
k (AT
k Dk −CT
k Bk −I) ˜uk −uT
k (BT
k Ck −DT
kAk + I) ˜xk
"
+ (uT
k ˜xk −xT
k ˜uk)
		N+1
0
+ (λ −˜λ)

x0
xN+1
T
W

˜x0
˜xN+1

=

−u0
uN+1
T 
˜x0
˜xN+1

−

x0
xN+1
T 
−˜u0
˜uN+1

+ (λ −˜λ)

x0
xN+1
T
W

˜x0
˜xN+1

.
(5.280)

334
5
Discrete Symplectic Eigenvalue Problems
The assumptions on R1, R2, W in (5.243) and (5.244) imply that
Ker 
R1 −λR2W, R2
 = Im

−RT
2
(R1 −λR2W)T

(5.281)
and similarly for ˜λ. Therefore, since y satisﬁes the boundary conditions (5.239) and
˜y satisﬁes (5.239) with λ := ˜λ, it follows that there exist d, ˜d ∈R2n such that
 x0
xN+1

= −RT
2 d,
 −u0
uN+1

= (R1 −λR2W)T d,
 ˜x0
˜xN+1

= −RT
2 ˜d,
 −˜u0
˜uN+1

= (R1 −˜λR2W)T ˜d.
Substituting this into (5.280), we obtain
(λ −˜λ) ⟨y, ˜y⟩= −dT (R1 −λR2W) RT
2 ˜d + dT R2(R1 −˜λR2W)T ˜d
+ (λ −˜λ) dT R2WRT
2 ˜d = 0.
And since λ ̸= ˜λ, it follows that ⟨y, ˜y⟩= 0, i.e., the ﬁnite eigenfunctions y and ˜y
are orthogonal with respect to (5.279).
⊓⊔
Proposition 5.74 Assume (5.239), (5.240), (5.243), and (5.244). Then the ﬁnite
eigenvalues of (SDSλ) with (5.242) are real and bounded from below and from
above. Moreover, the total number of ﬁnite eigenvalues of (SDSλ) with (5.242) can
be estimated by
N−1

k=0
rankWk + rank

diag{0, WN} + W

≤(N + 2) n.
(5.282)
Proof Since the coefﬁcients of system (SDSλ) and the boundary conditions (5.242)
are real, then with λ being its ﬁnite eigenvalue with the ﬁnite eigenfunction y, the
complex conjugate number ¯λ is also a ﬁnite eigenvalue with the ﬁnite eigenfunction
¯y. In this case ⟨¯y, ¯y⟩= ⟨y, y⟩= ⟨y, y⟩> 0, by (5.276). Moreover, by Lemma 2.61
applied to system (SDSλ) and the functional G(y, λ), we have
G(y, λ) =
 −u0
uN+1
T  x0
xN+1

+
 x0
xN+1
T
(  −λW)
 x0
xN+1

= 0,
where we used (5.281) and the fact that y satisﬁes the boundary conditions (5.242).
Similarly, G( ¯y, ¯λ) = 0. Denote F(y) := G(y, 0) and F( ¯y) := G( ¯y, 0). It follows
that F(y) = F(y) = F( ¯y), and thus by (5.260), we obtain
λ = F(y)
⟨y, y⟩= F( ¯y)
⟨¯y, ¯y⟩= ¯λ.

5.3
Linear Dependence on Spectral Parameter
335
This shows that λ ∈R. The boundedness from below of the ﬁnite eigenvalues
follows from Remark 5.56. The estimate in (5.282) follows from the estimate for
the sum N
k=0 rank ˜
Wk, where ˜
Wk are given by (5.277).
⊓⊔
Remark 5.75 For the Dirichlet boundary conditions
x0(λ) = 0 = xN+1(λ),
(5.283)
i.e., for R1 = I and R2 =  = W = 0, the ﬁnite eigenvalues of (SDSλ) with (5.283)
are determined by the condition
θ(λ0) = r −rankX[0]
N+1(λ0),
1 ≤θ(λ0) ≤n,
(5.284)
where according to (5.284), the value r is given by
r = max
λ∈R rankX[0]
N+1(λ) = rankX[0]
N+1(λ+
0 ) = rank X[0]
N+1(λ−
0 )
(5.285)
and the function X[0]
N+1(λ) comes from the principal solution Y [0](λ) of (SDSλ)
at k = 0. In this case X[0]
N+1(λ) is a matrix polynomial. The nondegeneracy
condition (5.276) for y being a ﬁnite eigenfunction has the form
{Wkxk+1}N−1
k=0 ̸≡0.
(5.286)
By Propositions 5.73 and 5.74, the ﬁnite eigenvalues of (SDSλ) with (5.283) are
real, and the ﬁnite eigenfunctions corresponding to different ﬁnite eigenvalues are
orthogonal with respect to the inner product
⟨y, ˜y⟩=
N−1

k=0
xT
k+1Wk ˜xk+1.
(5.287)
The ﬁnite eigenvalues are bounded from below, by Corollary 5.18. Finally,
by (5.286) the total number of ﬁnite eigenvalues of (SDSλ) with (5.283) can be
estimated by
N−1

k=0
rankWk ≤nN.
(5.288)
Remark 5.76 For the separated boundary conditions (5.245), i.e., when the matrices
R1 = diag{R∗
0, R∗
N+1} and R2 = diag{−R0, RN+1}, as well as  = diag{0, N+1}
and W = diag{W−1, WN+1}, are block diagonal, the ﬁnite eigenvalues of (SDSλ)
with (5.245) are determined by the condition; see (5.206),
θ(λ0) = r −rank (λ0),
1 ≤θ(λ0) ≤n,
(5.289)

336
5
Discrete Symplectic Eigenvalue Problems
where the value r is given by
r = max
λ∈R rank(λ) = rank (λ+
0 ) = rank (λ−
0 ).
(5.290)
The function (λ) is deﬁned in (5.205) and comes from the natural conjoined basis
¯Y(λ) of (SDSλ) at k = 0. In this case (λ) is again a matrix polynomial. The
nondegeneracy condition (5.276) for y being a ﬁnite eigenfunction is
{Wkxk+1}N−1
k=−1 ̸≡0
or
(WN + WN+1) xN+1 ̸= 0.
(5.291)
The ﬁnite eigenvalues of (SDSλ) with (5.245) are real, and the ﬁnite eigenfunctions
corresponding to different ﬁnite eigenvalues are orthogonal with respect to the inner
product
⟨y, ˜y⟩=
N−1

k=−1
xT
k+1Wk ˜xk+1 + xT
N+1(WN + WN+1) xN+1.
(5.292)
By Remark 5.51, the ﬁnite eigenvalues of (SDSλ) with (5.245) are bounded from
below. By (5.291), the total number of ﬁnite eigenvalues of (SDSλ) with (5.245) can
be estimated by
N−1

k=−1
rank Wk + rank (WN + WN+1) ≤(N + 2) n.
(5.293)
Example 5.77 The following example shows that the conclusions in Remark 5.75
do not hold in general without the assumption Wk ≥0. Put N = 3,
Ak = diag{1, 0}, Bk = diag{0, 1}, Ck = diag{0, −1}, Dk = diag{1, 0}
for k ∈[0, 3]Z, and
W0 = diag{0, 1},
W1 = 0,
W2 = diag{0, −1}.
Then the system is symplectic, W0 ≥0, W1 ≥0, but W2 ̸≥0. The calculation of
the principal solution Y [0](λ) by the difference system yields that
X[0]
0 (λ) = 02,
X[0]
1 (λ) = diag{0, 1},
X[0]
2 (λ) = diag{0, λ},
X[0]
3 (λ) = diag{0, −1},
X[0]
4 (λ) = X[0]
N+1(λ) ≡02.
Hence rN+1 = 0, so that there are no ﬁnite eigenvalues by (5.284) in Remark 5.75.
But every λ possesses a ﬁnite eigenfunction, namely, the function y = {yk}4
k=0 with
yk = Y [0]
k (λ)
0
1

, so that x0(λ) =
0
0

, x1(λ) =
0
1

, x2(λ) =
 0
−λ

, and x4(λ) =
0
0

.

5.3
Linear Dependence on Spectral Parameter
337
5.3.4
Global Oscillation Theorem
In this subsection we present the global oscillation theorem for problem (5.238)
with (5.283) and comment on its special features resulting from the linear depen-
dence on λ. The statement of this special global oscillation theorem will be of
particular interest in several applications later in this section as well as in Sect. 5.5.
First we recall the notation (5.49) for n1(λ) and the notation (5.50) for n2(λ),
being the number of focal points of Y [0](λ) in (1, N + 1] and the number of ﬁnite
eigenvalues of (5.238) with (5.283) in (−∞, λ], respectively.
Theorem 5.78 (Global Oscillation Theorem for Linear Dependence on λ)
Consider system (5.238), i.e., system (5.1) with (5.239) and with Dirichlet boundary
conditions (5.283). Suppose that (5.240) holds, i.e., Wk ≥0 for all k ∈[0, N]Z.
Then we have for all λ ∈R
n1(λ+) = n1(λ) ≤L,
(5.294)
n2(λ+) = n2(λ) < ∞,
(5.295)
n2(λ+) −n2(λ−) = n1(λ+) −n1(λ−) ≥0,
(5.296)
and there exists m ∈[0, L]Z, where L := N
k=1 rank Bk ≤Nn, such that
n1(λ) = n2(λ) + m
for all λ ∈R.
(5.297)
Moreover, for a suitable λ0 < 0, we have
n2(λ) ≡0
and
n1(λ) ≡m
for all λ ≤λ0.
(5.298)
Proof The statement follows directly from Theorem 5.17. Here we realize that
assumption (5.38) is automatically satisﬁed for system (5.238), since the coefﬁcient
matrix Bk(λ) ≡Bk does not depend on λ. Also, in view of Proposition 4.4, the
number n1(λ) is in this case bounded from above by the number L deﬁned in the
theorem.
⊓⊔
When the weight matrices Wk are positive deﬁnite, we obtain the following more
speciﬁc oscillation theorem.
Corollary 5.79 (Global Oscillation Theorem for Linear Dependence on λ)
Consider system (5.238), i.e., system (5.1) with (5.239), with Dirichlet boundary
conditions (5.283). Suppose that (5.268) holds, i.e., Wk > 0 for all k ∈[0, N]Z.
Then we have
n1(λ) = n2(λ)
for all λ ∈R,
(5.299)
and for a suitable λ0 < 0, we have n1(λ) = n2(λ) ≡0 for all λ ≤λ0.

338
5
Discrete Symplectic Eigenvalue Problems
Proof By Corollary 5.69 we know that the positivity assumption (5.268) implies
the existence of λ0 < 0 such that the functional F0(y) −λ ⟨y, y⟩W > 0 for all
admissible y = (x, u) with (5.283) x ̸≡0. This means by Theorem 5.31 that the
equality in (5.299) holds (i.e., the number m = 0 in Theorem 5.78).
⊓⊔
We make the following remarks about the above oscillation theorems for linear
dependence on λ.
Remark 5.80
(i) The second equality in (5.298) measures the “index” of the quadratic functional
F0(y, λ) := F0(y) −λ ⟨y, y⟩W. More precisely, the number m = 0 in
Theorem 5.78 if and only if F0(·, λ) > 0 for λ < λ1, where λ1 is the smallest
ﬁnite eigenvalue of (5.238) with (5.283), compared with Theorem 5.31.
Moreover, in general by the methods of Sect. 5.4, we have that for λ < λ1
m = dim{x = {xk}N+1
k=0 , y = (x, u) is admissible such that F0(y; λ) ≤0}.
(ii) The eigenvalue problem (5.238) with (5.283) is self-adjoint in the following
sense. All ﬁnite eigenvalues are real, and the ﬁnite eigenfunctions correspond-
ing to different ﬁnite eigenvalues are orthogonal, as we discuss in Remark 5.75.
(iii) The eigenvalue problem (5.238) with (5.283) is equivalent with the correspond-
ing eigenvalue problem for a 2n(N +1)×2n(N +1) matrix pencil A+λB with
the eigenvectors (u0, x1, u1, . . . , xN, uN, uN+1) and with the block diagonal
matrix
B = diag{0, W0, 0, W1, . . . , 0, WN−1, 0, 0}.
We omit here to write down A explicitly, because it is not used here. Then
it follows quite easily from the difference system and the deﬁnition of the
principal solution Y [0](λ) of (5.238) that
det (A + λB) = det X[0]
N+1(λ).
The property in (5.284) and the references on matrix pencils (cf. [148, Ch. XII]
or [61, 318, 319]) imply that rN+1 is the normal rank of the pencil, that
our notion of a ﬁnite eigenvalue (or zeros) coincides with the corresponding
notion for pencils, and, particularly, that the assumption (A2) of [55], i.e.,
det X[0]
N+1 ̸≡0, means that the pencil is regular. Hence, by omitting this
assumption, we consider the singular case. We also want to mention here that
all the minimal indices of our special matrix pencil (occurring in the Kronecker
canonical form, cf. [148, Ch. XII] or [71, 157, 318]), equal to zero; see also
[148, Section XII.6]. This fact does simplify the Kronecker canonical form of
the pencil considerably, but it is not used directly further on.
(iv) The concept of eigenvalues and eigenvectors for (5.1) of the main reference
[55], i.e., λ is an eigenvalue if and only if det X[0]
N+1(λ) = 0, stems from

5.4
Variational Description of Finite Eigenvalues
339
the continuous eigenvalue problems for linear Hamiltonian differential sys-
tems (1.103). If det X[0]
N+1(λ) ̸≡0, i.e., if (A2) of [55] holds, then as mentioned
above, the corresponding matrix pencil is regular, and the deﬁnitions in (5.284)
and in [55] coincide. But if the pencil is singular, then the deﬁnition of
eigenvalues in [55] is not appropriate anymore. Instead, the concept of ﬁnite
eigenvalues from (5.284) is the right one for the singular case as one can
see, and this concept stems from the theory of matrix pencils. Actually, there
exists also the notion of inﬁnite eigenvalues (or zeros) in the theory of matrix
pencils (cf. [148] or [319]), but it does not play any role here. In particular,
the geometric meaning of the concept of ﬁnite eigenvalues as formulated in
Remark 5.75 depends on the special structure (e.g., self-adjointness) of the
corresponding matrix pencil, where the assumption Wk ≥0 plays an important
role.
(v) We conclude this remark by pointing out some possible applications of
formula (5.299); see also [55, Remark 3(iv)]. Let λ0 ∈R be given. If we want
to know how many ﬁnite eigenvalues of (5.1) are less than or equal to λ0, we
can calculate recursively the principal solution Y [0](λ) at k = 0 of (5.238)
and determine the number of ﬁnite eigenvalues (zeros) of (5.238), (5.283),
resp., of X[0]
N+1(λ), that are less than or equal to λ0. However, X[0]
N+1(λ) is
a polynomial, and it might be difﬁcult to calculate this number. Alternatively,
if the number m as discussed above is known, then by Theorem 5.78 we could
calculate the principal solution at k = 0 of (5.238) for the particular λ0 in
question and count the number of its focal points in the interval (0, N + 1].
This procedure could possibly lead to a numerical algorithm to treat the
algebraic eigenvalue problem (5.238) with (5.283) also in this singular case,
although it is well known that singular matrix pencils have in general ill-posed
eigenstructure (cf. [318, 319] or [71, pg. 180]). For the numerical treatment of
the algebraic eigenvalue problem for symmetric, banded matrices via Sturm-
Liouville difference equations (note that this is a very special case of (5.238)),
the theory shows that det X[0]
k (λ) for a “Sturmian chain,” which may be used
similarly as for treating symmetric tridiagonal matrices; see [206] and [212].
5.4
Variational Description of Finite Eigenvalues
In this section we present further results related to the eigenvalue problems for
system (5.1) with linear dependence on the spectral parameter.
In linear algebra, it is well known that the eigenvalues of a real symmetric
matrix can be calculated as the minimum of the associated quadratic form over
an appropriate space of vectors. More precisely, let A be a real symmetric n × n
matrix and λ1 ≤· · · ≤λn be its eigenvalues, each eigenvalue repeated according
to its multiplicity. Denote by y1, . . . , yn the corresponding system of eigenvectors.

340
5
Discrete Symplectic Eigenvalue Problems
Then the eigenvalues of A satisfy the formula
λm = min
⟨Ay, y⟩
⟨y, y⟩, y ⊥yk, k = 1, . . . , m −1, y ̸= 0
 
.
(5.300)
The main result of this section presents a similar formula for the ﬁnite eigenvalues
of (5.1) with (5.283), resp., with (5.245) or with (5.242). First we need some pre-
liminary computations, which may be interesting regardless their later application.
5.4.1
Extended Picone Identity
In formula (2.67) in Lemma 2.32, we derived the Picone identity, which presents
the quadratic functional F(y) as a sum of “squares.” In the following statement,
we extend this result to the quadratic functional F0(y, λ) by incorporating the ﬁnite
eigenfunctions of (5.1) with (5.283).
Theorem 5.81 (Extended Picone’s Identity) Suppose that Y is a conjoined basis
of the symplectic system (5.1) for a ﬁxed λ ∈R. Let Q be symmetric with
QX = UX†X, and deﬁne M, T , P by (4.1) and (4.2). Let λ1, . . . , λm be the
ﬁnite eigenvalues of (5.1) and (5.283) with the corresponding orthonormal ﬁnite
eigenfunctions y(j) = (x(j), u(j)), 1 ≤j ≤m. Let β1, . . . , βm ∈R and deﬁne
ˆy :=
m

j=1
βjy(j).
Finally, suppose that y = (x, u) is admissible, i.e., xk+1 = Akxk + Bkuk for k ∈
[0, N]Z, put ˜y := y + ˆy and ˜z := ˜u −Q˜x, and assume that
y ⊥βjλjy(j),
i.e.,
1
y, βjλjy(j)2
= 0,
for all 1 ≤j ≤m
(5.301)
and that
˜xk ∈Im Xk
for all k ∈[1, N]Z.
(5.302)
Then we have that
F0(y, λ) −xT
k uk
			
N+1
k=0 =
N

k=0
˜zT
k Pk ˜zk +
m

j=1
(λ −λj) β2
j
+ ˜xT
k Qk ˜xk
			
N+1
k=0 −˜xT
k ˜uk
			
N+1
k=0 .
(5.303)

5.4
Variational Description of Finite Eigenvalues
341
Proof First we note that since the ﬁnite eigenfunctions y(j) satisfy the boundary
conditions (5.283), it follows that ˆx0 = 0 = ˆxN+1, ˜x0 = x0, and ˜xN+1 = xN+1.
From (2.67) in Lemma 2.32, we obtain
F0( ˜y, λ) =
N

k=0
(
˜xT
k+1 ˜Qk ˜xk+1 −˜xT
k ˜Qk ˜xk + ˜zT
k Pk ˜zk
)
= ˜xT
k Qk ˜xk
			
N+1
k=0 +
N

k=0
˜zT
k Pk ˜zk,
because ˜y is admissible and (5.302) holds. Next, by using the recursion in
system (5.1) with λ = λj for 1 ≤j ≤m and by (2.61) in Lemma 2.30, we conclude
that
F0( ˆy) =
N

k=0
ˆxT
k+1
!
Ck ˆxk + Dk ˆuk −ˆuk+1
"
=
N

k=0
m

j=1
βjλj ˆxT
k+1Wkx(j)
k+1 =
m

j=1
λjβ2
j .
Moreover, by using y ⊥y(j) from (5.301), we have
F0(y, ˆy) = xT
k ˆuk
			
N+1
k=0 +
N

k=0
m

j=1
βjλjxT
k+1Wkx(j)
k+1 = xT
k ˆuk
			
N+1
k=0
and similarly
F0( ˆy, y) = ˆxT
k uk
			
N+1
k=0 = 0.
Altogether, by using x0 = ˜x0 and xN+1 = ˜xN+1 and the above calculations, we get
F0(y, λ) −xT
k uk
			
N+1
k=0 = F(y) −λ ⟨y, y⟩−xT
k uk
			
N+1
k=0
= F(y) + F( ˆy) + F(y, ˆy) + F( ˆy, y) −
m

j=1
λjβ2
j −xT
k ˆuk
			
N+1
k=0
−λ
 ⟨y, y⟩+
/
ˆy, ˆy
0 
+ λ
/
ˆy, ˆy
0
−xT
k uk
			
N+1
k=0

342
5
Discrete Symplectic Eigenvalue Problems
= F( ˜y) −
m

j=1
λjβ2
j −xT
k ˆuk
			
N+1
k=0 −λ ⟨˜y, ˜y⟩+ λ
m

j=1
β2
j −xT
k uk
			
N+1
k=0
= F( ˜y, λ) +
m

j=1
(λ −λj) β2
j −xT
k (ˆuk + uk)
			
N+1
k=0
= ˜xT
k Qk ˜xk
			
N+1
k=0 +
N

k=0
˜zT
k Pk ˜zk +
m

j=1
(λ −λj) β2
j −˜xT
k ˜uk
			
N+1
k=0 .
Therefore, the proof of (5.303) is complete.
⊓⊔
We note that the role of assumption (5.302) in Theorem 5.81 is characterized by
Lemma 2.47 in Sect. 2.3.5.
Remark 5.82 Condition (5.301) in Theorem 5.81 is slightly more general than
the corresponding condition used in [56, Theorem 4.2]. Here we incorporate the
coefﬁcients βj and the ﬁnite eigenvalues λj into the orthogonality condition (5.301),
which shows that in the case of βj = 0 or λj = 0, this condition is not needed.
5.4.2
Rayleigh Principle
In this subsection we extend formula (5.300) to the ﬁnite eigenvalues of (5.1)
with (5.283), resp., with (5.245) or with (5.242). The main results are essentially
based on the validity of the oscillation theorem (Corollary 5.79) saying that
n1(λ) = n2(λ),
n1(λ+) = n1(λ),
n2(λ+) = n2(λ)
for all λ ∈R.
(5.304)
The latter reference states that the conditions in (5.304) are equivalent to the
positivity of the quadratic functional F(·, λ0) in (5.273) for some λ0 < 0. For
practical applications, it will be convenient to guarantee this positivity assumption
by conditions imposed on the coefﬁcients of system (5.1).
If λ1 ≤. . . ≤λm are the ﬁnite eigenvalues of (5.1) with (5.283), including their
multiplicities, then we put for convenience λ0 := −∞and λm+1 := ∞. Note that
m ≤nN < ∞by (5.288) in Remark 5.75.
Theorem 5.83 (Rayleigh Principle for Dirichlet Boundary Conditions) Assume
that (5.239), (5.240), and (5.273) hold and
there exists λ0 < 0 such that F(·, λ0) > 0.
(5.305)
Let λ1 ≤λ2 ≤· · · ≤λm denote the ﬁnite eigenvalues of (5.1) with (5.283), with the
corresponding orthonormal ﬁnite eigenfunctions y(j), 1 ≤j ≤m, with respect to

5.4
Variational Description of Finite Eigenvalues
343
the inner product (5.287). Then for every 0 ≤j ≤m, we have
λj+1 = min
F(y, 0)
⟨y, y⟩, y = (x, u) is admissible with x0 = 0 = xN+1,
y ⊥y(1), . . . , y(j), and {Wkxk+1}N−1
k=0 ̸≡0
 
.
⎫
⎪⎪⎬
⎪⎪⎭
(5.306)
Note that we include the case of j = 0, where the orthogonality condition on y
becomes empty, as well as the case of j = m, where λm+1 = ∞.
Proof of Theorem 5.83 Let 0 ≤j ≤m be ﬁxed, and choose λ ∈(λj, λj+1). Then,
by (5.304), we have n2(λ) = n1(λ) = j, so that the principal solution Y(λ) :=
Y [0](λ) at k = 0 possesses exactly j focal points in the interval (0, N + 1]. The fact
that N + 1 is actually not a focal point of Y(λ) follows from assumption (5.305),
Proposition 5.65, and Theorem 2.36, which imply that rank Mk(˜λ) = 0 for all ˜λ ≤
λ0. Hence,
rankMk(λ+) = rankMk(λ) = 0
for all k ∈[0, N]Z,
λ ∈(λm, λm+1).
In particular, we have rank MN(λ) = 0. First, we apply the extended Picone identity
(Theorem 5.81) to y = 0, which yields
˜y = ˆy =
m

i=1
βiy(i)
with ˜x0 = 0 = ˜xN+1, x0 = 0 = xN+1.
Suppose that β1, . . . , βj satisfy the j linear homogeneous equations
MT
k ˜xk+1 = 0,
k ∈[0, N −1]Z,
˜zk ⊥
!
α ∈Rn, α is an eigenvector corresponding
to a negative eigenvalue of Pk
"
,
k ∈[0, N]Z,
⎫
⎪⎬
⎪⎭
(5.307)
where ˜zk = ˜uk −Qk ˜xk. Note that the number of these equations is equal to the
number of of focal points of Y in the open interval (0, N+1). Then, by Lemma 2.48,
assumption (5.302) holds, and we obtain from Theorem 5.81 that
0 = F(y, λ) =
N

k=0
˜zT
k Pk ˜zk +
j

i=1
(λ −λi) β2
i ,
(5.308)
where ˜zT
k Pk ˜zk ≥0 for k ∈[0, N]Z by (5.307) and λ −λi ≥λ −λj > 0 for
1 ≤i ≤j. Hence, (5.308) implies that β1 = · · · = βj = 0, so that (5.307)
possesses only the trivial solution. Thus we have shown that
the coefﬁcient matrix corresponding to system (5.307) is nonsingular.
(5.309)

344
5
Discrete Symplectic Eigenvalue Problems
Now suppose that y = (x, u) is admissible with x0 = 0 = xN+1 and y ⊥y(i)
for 1 ≤i ≤j. According to (5.309), there exists a unique set of constants
β1, . . . , βj such that the function ˜y := y + ˆy = y + j
i=1 βiy(i) satisﬁes the j
linear inhomogeneous equations deﬁned by (5.307). Consequently, Theorem 5.81
implies that
F(y, λ) = F(y, λ) −xT
k uk
			
N+1
k=0
=
N

k=0
˜zT
k Pk ˜zk +
j

i=1
(λ −λi) β2
i ≥
j

i=1
(λ −λi) β2
i ≥0.
This shows that
F(y) ≥λ ⟨y, y⟩
for all λ ∈(λj, λj+1).
(5.310)
Hence, F(y) ≥λj+1 ⟨y, y⟩by the continuity and taking λ →λ−
j+1 in (5.310).
Moreover, we have F(y) = λj+1 ⟨y, y⟩for y = y(j+1). For multiple ﬁnite
eigenvalues, use the fact that F(y, λ) = F(y + ˆy, λ) if y is admissible with
x0 = 0 = xN+1 and if ˆy is a ﬁnite eigenvector corresponding to λ. Hence the
assertion follows.
⊓⊔
In later applications in Sect. 5.5, we will use the statement of Theorem 5.83 under
stronger positivity assumption (5.268) instead of (5.305), i.e., under Wk > 0 for all
k ∈[0, N]Z.
Corollary 5.84 Assume that (5.239), (5.240), (5.246), (5.268), and (5.273) hold.
Then the ﬁnite eigenvalues λ1 ≤· · · ≤λm of (5.1) with (5.283) satisfy for every
0 ≤j ≤m the equality in (5.306), in which we consider admissible y with x0 =
0 = xN+1, y ⊥y(1), . . . , y(j), and {xk}N
k=1 ̸≡0.
Proof The result follows from Theorem 5.83 and Corollary 5.69.
⊓⊔
Now we consider the Rayleigh principle for the eigenvalue problem (5.1) with
separated boundary conditions (5.245). Here we use the quadratic functional G(·, λ)
given in (5.262). We note that by (5.293) the total number m of ﬁnite eigenvalues
of (5.1) with (5.245) satisﬁes m ≤(N + 2) n.
Theorem 5.85 (Rayleigh
Principle
for
Separated
Boundary
Conditions)
Assume that (5.239), (5.240), (5.246), (5.247), (5.248), and (5.262) hold and
there exists λ0 < 0 such that G(·, λ0) > 0.
(5.311)
Let λ1 ≤λ2 ≤· · · ≤λm denote the ﬁnite eigenvalues of (5.1) with (5.245), with the
corresponding orthonormal ﬁnite eigenfunctions y(j), 1 ≤j ≤m, with respect to

5.4
Variational Description of Finite Eigenvalues
345
the inner product (5.292). Then for every 0 ≤j ≤m, we have
λj+1 = min
G(y, 0)
⟨y, y⟩, y = (x, u) is admissible with (5.263),
y ⊥y(1), . . . , y(j), and {Wk xk+1}N−1
k=−1 ̸≡0
or (WN + WN+1) xN+1 ̸= 0
 
.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.312)
Proof This statement follows from Theorem 5.83 upon applying it to the equivalent
extended eigenvalue problem on the interval [−1, N + 2]Z with the Dirichlet
boundary conditions x−1(λ) = 0 = xN+2(λ). This equivalence is discussed in
details in Theorem 5.41 for the two eigenvalue problems and in Lemma 5.47 for the
associated quadratic functionals.
⊓⊔
Assumption (5.311) can be guaranteed by several conditions described in Propo-
sitions 5.66 and 5.67 and in Remark 5.68. Condition (5.311) can also be tested by
the properties of the natural conjoined basis ¯Y(λ0) of system (5.238) with λ := λ0,
which are presented in Theorem 2.50.
Corollary 5.86 Assume that (5.239), (5.246), (5.270), and (5.262) hold. Then the
ﬁnite eigenvalues λ1 ≤· · · ≤λm of (5.1) with (5.245) satisfy for every index 0 ≤
j ≤m the equality in (5.312), in which we consider admissible y with (5.263),
y ⊥y(1), . . . , y(j), and {xk}N+1
k=0 ̸≡0.
Proof The result follows from Theorem 5.85 and Remark 5.68.
⊓⊔
When W−1 = 0, WN+1 = 0, and Wk > 0 for all k ∈[0, N]Z, we obtain another
generalization of Corollary 5.84 to the separated boundary conditions
R∗
0 x0(λ) + R0 u0(λ) = 0,
R∗
N+1 xN+1(λ) + RN+1 uN+1(λ) = 0,
(5.313)
which in this case do not depend on the spectral parameter λ. In this case the inner
product in (5.292) reduces to the traditional expression
⟨y, ˜y⟩=
N

k=0
xT
k+1Wk ˜xk+1.
(5.314)
Corollary 5.87 Assume that (5.239), (5.246), (5.268), (5.269), and (5.267) hold.
Then the ﬁnite eigenvalues λ1 ≤· · · ≤λm of (5.1) with (5.313) satisfy for every
index 0 ≤j ≤m the equality in (5.312), in which we consider inner product (5.314)
and admissible y with (5.263), y ⊥y(1), . . . , y(j), and {xk}N+1
k=0 ̸≡0.
Proof The result follows from Theorem 5.85 and Proposition 5.67.
⊓⊔
As a ﬁnal result in this section, we consider the Rayleigh principle for the
eigenvalue problem consisting of system (5.1) with the joint endpoints (5.242).

346
5
Discrete Symplectic Eigenvalue Problems
Theorem 5.88 (Rayleigh Principle for Joint Boundary Conditions) Assume
that (5.239), (5.240), (5.243), (5.244), and (5.260) hold and
there exists λ0 < 0 such that G(·, λ0) > 0.
(5.315)
Let λ1 ≤λ2 ≤· · · ≤λm denote the ﬁnite eigenvalues of (5.1) with (5.242), with the
corresponding orthonormal ﬁnite eigenfunctions y(j), 1 ≤j ≤m, with respect to
the inner product (5.279). Then for every 0 ≤j ≤m, we have
λj+1 = min
G(y, 0)
⟨y, y⟩, y = (x, u) is admissible with (5.261),
y ⊥y(1), . . . , y(j), and {Wk xk+1}N−1
k=0 ̸≡0
or W
 x0
xN+1

̸= 0
 
.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(5.316)
Proof This statement follows from Theorem 5.85 upon applying it to the equivalent
augmented eigenvalue problem on the interval with the separated. This equivalence
is discussed in details in Theorem 5.44 for the two eigenvalue problems.
⊓⊔
Assumption (5.315) can also be tested by the properties of the principal solution
Y [0](λ0) of system (5.238) with λ := λ0, which are presented in Theorem 2.53.
5.5
Applications of Oscillation Theorems
In this section we present results on the numbers the focal points of conjoined bases
of one or two symplectic systems. In the derivation of these results, we utilize the
eigenvalue theory and the oscillation theorems for boundary value problems for
symplectic systems depending linearly on the spectral parameter, i.e., essentially
based on the results in Sects. 5.3–5.4.
5.5.1
Sturmian Comparison and Separation Theorems
In this subsection we present an alternative approach to the Sturmian comparison
and separation theorems based on the variational techniques from Sect. 5.4.
Comparison theory for symplectic difference systems has been already treated in
Sect. 4.3 (see Corollary 4.57). There, the main tool was the relationship between
the comparative index and the multiplicity of a focal point. Here we present
a proof based on the eigenvalue problem (5.1) and the associated discrete quadratic
functional.

5.5
Applications of Oscillation Theorems
347
We recall from Sects. 2.3.2 and 5.3.4 the quadratic functionals
F(y) =
N

k=0
(
xT
k AT
k Ckxk + 2 xT
k CT
k Bkuk + uT
k BT
k Dkuk
)
,
F(y, λ) = F(y) −λ ⟨y, y⟩,
where y = (x, u) is admissible, i.e., xk+1 = Akxk + Bkuk for k ∈[0, N]Z, and
x0 = 0 = xN+1. We will utilize the symmetric 2n × 2n matrices Gk and the n × n
matrices Ek deﬁned in (2.64), which are used to write the functional F(y) in terms
of the x component of y only; see (2.63). With systems (SDS) and (5.1), we consider
another symplectic systems
ˆyk+1 = ˆSk ˆyk,
k ∈[0, N]Z,
(5.317)
and
ˆyk+1(λ) = ˆSk(λ) ˆyk(λ),
k ∈[0, N]Z,
(5.318)
with the block structure as in (5.239) for the coefﬁcient matrices Sk(λ) and ˆSk(λ).
In this case we deﬁne the corresponding matrices ˆG and ˆEk and the quadratic
functionals ˆF( ˆy) and ˆF( ˆy, λ) analogously to (2.64) and (5.273). Finally, in this
section we suppose that the weight matrices satisfy
Wk = I = ˆWk,
k ∈[0, N]Z,
(5.319)
so that the number m = 0 in formula (5.297), i.e.,
n1(λ) = n2(λ)
for all λ ∈R
(5.320)
for system (SDS), as well as for system (5.317). In particular, the Rayleigh principle
in Corollary 5.84 holds for both systems (5.1) and (5.318) with the Dirichlet
boundary conditions.
Two following two results represent Sturmian-type comparison theorems for
symplectic systems (SDS) and (5.317).
Theorem 5.89 (Sturmian Comparison Theorem) Suppose that
Gk ≥ˆGk and Im (Ak −ˆAk,
Bk) ⊆Im ˆBk
(5.321)
for all k ∈[0, N]Z. If the principal solution of (5.317) at k = 0 has q focal points
in (0, N + 1], then any conjoined basis of (SDS) has at most q + n focal points in
(0, N + 1].

348
5
Discrete Symplectic Eigenvalue Problems
Proof Let Y be a conjoined basis of (SDS) and suppose that it has p focal points
in (0, N + 1]. Then, as in Proposition 2.38 and the proof of Theorem 4.16, we can
construct for each focal point of Y the corresponding admissible y[j] = (x[j], u[j])
such that x[j]
N+1 = 0 for all 1 ≤j ≤p. Furthermore, since the principal solution
Y [0] of (5.317) at k = 0 is assumed to have q focal points in (0, N +1], we know by
Corollary 5.79 with λ = 0 applied to the eigenvalue problem (5.318) with ˆW = I
that this eigenvalue problem has q nonpositive ﬁnite eigenvalues λi, 1 ≤i ≤q, with
the corresponding orthonormal ﬁnite eigenvectors ˆy(i) = (ˆx(i), ˆu(i)), 1 ≤i ≤q.
Moreover, by Corollary 5.84, we have F(y) ≥ˆF(y) > 0 · ⟨y, y⟩= 0 for ( ˆA, ˆB)-
admissible y = (x, u) satisfying
y ⊥ˆy(i),
i.e.,
1
y, ˆy(i)2
=
N

k=0
xT
k+1 ˆx(i)
k+1 = 0,
1 ≤i ≤q,
x ̸= 0.
Now suppose that p > q + n. Then there exists a nontrivial linear combination
p

j=1
cj
⎛
⎜⎜⎜⎜⎜⎜⎝
/y[j], ˆy(1)0
/
y[j], ˆy(2)0
...
/
y[j], ˆy(q)0
x[j]
0
⎞
⎟⎟⎟⎟⎟⎟⎠
= 0.
Deﬁne
y = (x, u) =
p

j=1
cjy[j].
By construction, xN+1 = 0 and y is admissible. Moreover, p
j=1 cjx[j]
0
= 0 implies
that x0 = 0, as well as
0 =
p

j=1
cj
1
y[j], ˆy(i)2
=
1
y, ˆy(i)2
for all 1 ≤i ≤q,
so that y ⊥ˆy(i) for all 1 ≤i ≤q. As in the proof of Theorem 4.16 we have that
x ̸≡0 (since not all cj = 0) and F(z) ≤0. From the second condition in (5.321),
it follows that there exists ˆu = {ˆuk}N
k=0 such that xk+1 = Akxk + Bk ˆuk for all k ∈
[0, N]Z, and hence ˆy := (x, ˆu) is admissible for ˆF. By using the representation of
quadratic functional ˆF via the matrix ˆG, as in (2.63), coupled with the ﬁrst condition
in (5.321), we ﬁnd that
ˆF(4y) =
N

k=0
 xk
xk+1
T
ˆGk
 xk
xk+1

≤
N

k=0
 xk
xk+1
T
Gk
 xk
xk+1

= F(y) ≤0.

5.5
Applications of Oscillation Theorems
349
Hence we have found an ( ˆA, ˆB)-admissible y = (x, u) with x ̸≡0, x0 = 0 = xN+1,
y ⊥ˆy(i) for all 1 ≤i ≤q, and ˆF(y) ≤0. This contradicts the Rayleigh principle
(Corollary 5.84), by which ˆF(y) > 0 for all admissible y with x0 = 0 = xN+1,
x ̸= 0, and y ⊥ˆy(i) for all 1 ≤i ≤q. Therefore, we must have p ≤q + n, which
proves the result.
⊓⊔
Theorem 5.90 (Sturmian Comparison Theorem) Suppose that (5.321) holds. If
the principal solution of (SDS) at k = 0 has q focal points in (0, N + 1], then any
conjoined basis of (5.317) has at least q focal points in (0, N + 1].
Proof Let Y [0] be the principal solution of the (SDS) at k = 0, and let ˆY be
a conjoined basis of (5.317). For any λ ∈R, let Y [0](λ) be the principal solution
of (5.1) at k = 0, and let ˆY(λ) be the conjoined basis of (5.318) such that
ˆY0(λ) ≡ˆY0, so that these initial conditions are constant in λ ∈R. We denote by
n1(λ) and p(λ) the numbers of focal points of Y [0](λ) and ˆY(λ) in the interval
(0, N + 1]. The assumptions then imply that q = n1(0), and we need to prove that
n1(0) ≤p(0). We will show that
n1(λ) ≤p(λ)
for all λ ∈R.
(5.322)
By the oscillation theorem (Corollary 5.79), there are exactly n2(0) = n1(0) =
q nonpositive ﬁnite eigenvalues of (5.1), (5.283). We denote by λi for 1 ≤i ≤
r all the ﬁnite eigenvalues of (5.1), (5.283) with the corresponding orthonormal
eigenfunctions y(i), 1 ≤i ≤r (so that q ≤r). Fix now λ ∈R. Then we have
λ ∈[λm, λm+1)
for some m ∈{0, . . ., r},
where we put λ0 := −∞and λr+1 := ∞. By (5.320), this means that n1(λ) =
n2(λ) = m. First suppose that λ is not a ﬁnite eigenvalue of (5.1), (5.283) so
that λ ∈(λm, λm+1). Let ˜p be the number of focal points of ˆY(λ) in the open
interval (0, N + 1) so that ˜p ≤p(λ). Put ˜y = (˜x, ˜u) := m
j=1 βjy(j), where the
constants β1, . . . , βj are chosen in such a way that ˜y satisﬁes ˜p linear homogeneous
conditions
ˆMT
k (λ) ˜xk+1 = 0,
k ∈[0, N −1]Z,
˜zk ⊥
!
α ∈Rn, α is an eigenvector corresponding
to a negative eigenvalue of ˆPk(λ)",
k ∈[0, N]Z
⎫
⎪⎬
⎪⎭
(5.323)
where
˜zk = ˜uk −ˆUk(λ) ˆX†
k(λ) ˜xk =
m

j=1
βj

˜u(j)
k
−ˆUk(λ) ˆX†
k(λ) ˜x(j)
k


350
5
Discrete Symplectic Eigenvalue Problems
and where the matrices ˆMk(λ) and ˆPk(λ) are given by (5.40) with ˆY(λ) instead of
Y(λ). The sequence ˜y is admissible for F, and by the second condition in (5.321),
there exists ˆu such that the pair ˆy := (˜x, ˆu) is admissible for ˆF. Since the value of
the quadratic functional does not depend on the second component of an admissible
sequence y = (x, u), see (2.63), we can write also ˜y = (˜x, ˆu). Then by the extended
Picone identity (Theorem 5.81), we have
ˆF( ˜y, λ) := ˆF( ˜y) −λ ⟨˜y, ˜y⟩=
N

k=0
˜zT
k ˆPk ˜zk ≥0.
We note that the ﬁrst condition in (5.323) implies that ˜xk ∈Im ˆXk for all k ∈
[0, N + 1]Z by Lemma 2.48, and hence Theorem 5.81 can be applied. At the same
time, by a direct computation using the orthonormality of y(1), . . . , y(m), we have
Fλ( ˜y) := F( ˜y) −λ ⟨˜y, ˜y⟩=
m

j=1
(λj −λ) β2
j .
Now again by (2.63) and the ﬁrst condition in (5.321), we have
m

j=1
(λj −λ) β2
j ≥
N

k=0
˜zT
k ˆPk(λ) ˜zk ≥0,
since λ > λj for all 1 ≤j ≤m. This is however possible only if βj = 0 for all
1 ≤j ≤m. This means that the system (5.323) of ˜p linear homogeneous conditions
has only the trivial solution, and hence the number of conditions ˜p is greater than
or equal to the number of parameters βj (which is equal to q). This proves the
required statement when λ is not a ﬁnite eigenvalue of (5.1), (5.283). Finally, since
the functions n1(λ) and p(λ) are right-continuous, by letting λ →λ+
m, we obtain
the statement also in the case when λ = λm is a ﬁnite eigenvalue of (5.1), (5.283).
The proof is complete.
⊓⊔
When the two systems (SDS) and (5.317) are the same, then the assumptions
in (5.321) are trivially satisﬁed, and we obtain from Theorems 5.89 and 5.90 the
following, compare with Sect. 4.2.3 and in particular formula (4.68), where a more
precise estimate for the number of focal points was derived by using the comparative
index.
Corollary 5.91 (Sturmian Separation Theorem) Suppose that the principal solu-
tion of (SDS) at k = 0 has q focal points in (0, N + 1]. Then any conjoined basis
of (SDS) has at at least q and at most q + n focal points in (0, N + 1].

5.5
Applications of Oscillation Theorems
351
5.5.2
Modiﬁcations of Global Oscillation Theorem
In this subsection we present some modiﬁcations of the global oscillation theorem
(Theorem 5.78). These extensions are obtained using the number l∗(Y, M, N) of
the backward focal points in the interval [M, N) of the conjoined basis Y (see
Deﬁnition 4.3 and Sect. 4.2.3). The ﬁrst application of the concept of backward focal
points (which is the same as “focal points” of the reversed symplectic system (2.47))
consists in replacing the quantity
n1(λ) = n1(Y [0](λ)) := l(Y [0](λ), 0, N + 1)
by the quantity
n∗
1(Y [N+1](λ)) := l∗(Y [N+1](λ), 0, N + 1),
(5.324)
where l∗(Y [N+1](λ), 0, N + 1) is number of backward focal points of the principal
solution of (5.238) at N + 1 in [0, N + 1); see Theorem 4.34. Hence, as a corollary
of Theorem 4.34, we obtain the next statement.
Corollary 5.92 Under the assumptions of Theorem 5.78, the number n1(λ)
in (5.294), (5.296), (5.297), (5.298) can be replaced by n∗
1(Y [N+1](λ)) deﬁned
in (5.324), where l∗(Y [N+1](λ), 0, N + 1) is the number of backward focal points
of the principal solution Y [N+1](λ) of (5.238) at N + 1 in [0, N + 1).
Another possibility how to apply the number of backward focal points in the
interval [M, N) is the following modiﬁcation of the global oscillation theorem to
eigenvalues which are strictly less than a given value λ ∈R.
Theorem 5.93 Under the assumptions of Theorem 5.78, there exists an index m∗∈
N ∪{0} such that
n∗
1(Y [0](λ)) = no
2(λ) + m∗,
no
2(λ) =

μ<λ
θ(μ),
(5.325)
where 
μ<λ θ(μ) is the number of ﬁnite eigenvalues of (5.238), (5.283) which
are less than λ ∈R, Y [0](λ) is the principal solution of (5.238) at k = 0, and
n∗
1(Y [0](λ)) := l∗(Y [0](λ), 0, N + 1) is the number of backward focal points of
Y [0](λ) in [0, N + 1).
Proof Let λ0 < λmin = min σ, where σ is the ﬁnite spectrum of (5.238), (5.283).
Then
no
2(λ) = n2(λ) −θ(λ) = n2(λ) + rankX[0]
N+1(λ)
		λ
λ0,

352
5
Discrete Symplectic Eigenvalue Problems
where θ(λ) is the (algebraic) multiplicity of λ ∈σ and θ(λ) = 0 for λ ̸∈σ. Hence,
using Corollary 4.6, we obtain
no
2(λ) = l(Y [0](λ), 0, N + 1) −l(Y [0](λ0), 0, N + 1) + rankX[0]
N+1(λ)
		λ
λ0
= l∗(Y [0](λ), 0, N + 1) −l∗(Y [0](λ0), 0, N + 1)
= n∗
1(Y [0](λ)) −n∗
1(Y [0](λ0)),
where l∗(Y, 0, N + 1) is the number of backward focal points of a given conjoined
basis in [0, N + 1). If we denote m∗:= n∗
1(Y [0](λ0)), then the theorem is proved.
⊓⊔
Using the equality l∗(Y [0](b), 0, N + 1) = l(Y [N+1](b), 0, N + 1) (see Theo-
rem 4.35), we derive the “dual version” of Theorem 5.93.
Corollary 5.94 (Global Oscillation Theorem for Linear Dependence on λ)
Under the assumptions of Theorem 5.78, there exists an index m∗∈N ∪{0} such
that
n1(Y [N+1](λ)) = no
2(λ) + m∗,
no
2(λ) =

μ<λ
θ(μ),
(5.326)
where 
μ<λ θ(μ) is the number of ﬁnite eigenvalues of (5.238), (5.283) which are
less than λ ∈R, Y [N+1](λ) is the principal solution of (5.238) at k = N + 1, and
n1(Y [N+1](λ)) is the number of forward focal points of Y [N+1](λ) in (0, N + 1].
The advantage of the just proved statements for the multiplicities of backward
focal points is that, in view of Lemma 4.8, the previous formulas involve the blocks
of S−1
k (0 I)T , which do not contain the spectral parameter λ.
5.6
Oscillation Theorems for Variable Rank of Bk(λ)
In this section we extend the results of Sect. 5.1 for the nonlinear eigenvalue
problems (E), i.e.,
yk+1 = Sk(λ) yk,
yk =
 xk
uk

,
λ ∈R,
x0 = 0 = xN+1,
by applying the comparative index theory (see Chap. 3). The comparative index
approach gives us the possibility to remove the restrictive assumption (5.38), i.e.,
rankBk(λ) is constant in λ on R

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
353
which was used in Sect. 5.1.3 in the local and global oscillation theorems (see
Theorems 5.15, 5.16, 5.17 and Corollary 5.18). Recall that condition (5.38) is
automatically satisﬁed for nonlinear eigenvalue problems for the scalar and matrix
Sturm-Liouville equations (see Examples 5.35 and 5.36), because the symplectic
coefﬁcient matrix Sk(λ) of the associated symplectic eigenvalue problem (E) obeys
the condition det Bk(λ) ̸= 0 for all λ ∈R. It was shown in Example 5.38
that the higher- order Sturm-Liouville difference equations written as a discrete
symplectic system also obey (5.38). However there are important classes of spectral
problems (E), for which condition (5.38) imposes serious restrictions on the
applicability of the local and global oscillation theorems. In Example 5.39, for
spectral problems (E) for the real symplectic difference systems with the general
linear dependence, Sk(λ) = Sk + λVk on λ condition (5.38) implies the absence of
real ﬁnite eigenvalues of the linear matrix pencil Bk(λ) = Bk + λ ˜Bk. For the most
important special case of (E), for the linear Hamiltonian difference systems (see
Example 5.37) condition (5.38) means that rank [I −Ak(λ)]−1Bk(λ) = rank Bk(λ)
is constant for λ ∈R.
Recall that the proofs of Theorems 5.15, 5.16, 5.17 are based on two main
grounds. The ﬁrst one is monotonicity condition (5.3) and its corollaries, which
imply almost all basic properties of the spectral problem (E), including the main
notion of ﬁnite eigenvalues (Deﬁnition 5.5) and their isolated character (see
Corollary 5.7). The second foundation is the index theorem (see Theorem 1.85
and Corollary 1.86) imposing the restriction in (5.38). The lower boundedness of
the spectrum of (E) (see Corollary 5.18) follows from Theorem 5.17 based on
Corollary 1.86, and then (5.3) together with (5.38) implies that all eigenvalues of (E)
are isolated and bounded from below.
In this section we retain the ﬁrst ground assuming that (5.3) holds but change
the second one replacing Corollary 1.86 by the main theorem of the comparative
index theory (see Theorem 3.24). By applying Theorem 3.24, we avoid (5.38) in the
extended version of the local oscillation theorem (see Theorem 5.95) incorporating
the jump discontinuities of the piecewise constant function rank Bk(λ) for k ∈
[0, N]Z. As a corollary we show that the ﬁnite spectrum of the symplectic eigenvalue
problem (E) is bounded from below if and only if so is the set of discontinuity points
of rankBk(λ) (see Corollary 5.96).
For the proof of the extended version of the global oscillation theorem (see
Theorem 5.98), we assume additionally to (5.3) that rank Bk(λ) is constant for
all sufﬁciently negative λ. Note that this assumption is naturally satisﬁed for
problems (E) with a polynomial dependence on λ, in particular for a general linear
dependence on λ (see Example 5.39), as well as for the Hamiltonian eigenvalue
problems with the nonlinear dependence in λ considered in Example 5.37.

354
5
Discrete Symplectic Eigenvalue Problems
5.6.1
Statement of Main Results
The consideration in this section is based on Theorem 5.1. In the previous results,
under assumption (5.38), we did not need assertions (i) and (ii) of this theorem,
because we have that (5.9) and (5.10) hold. In this section, assertions (i) and (ii) of
Theorem 5.1 play a leading role.
By Theorem 5.1(i), we conclude that the set Ker Bk(λ) is piecewise constant in λ
on R. That is, for every λ0 ∈R, there exists δ > 0 such that
Ker Bk(λ) ≡Ker Bk(λ−
0 ) ⊆Ker Bk(λ0)
for all λ ∈(λ0 −δ, λ0),
(5.327)
Ker Bk(λ) ≡Ker Bk(λ+
0 ) ⊆Ker Bk(λ0)
for all λ ∈(λ0, λ0 + δ),
(5.328)
and the quantity rank Bk(λ) is constant on some left and right neighborhoods of λ0.
By analogy with Deﬁnition 5.5, we introduce the numbers
ϑk(λ0) := rank Bk(λ−
0 ) −rankBk(λ0),
k ∈[0, N]Z,
(5.329)
which count the jumps of rank Bk(λ) in the left neighborhood of λ0. Moreover,
Theorem 5.1(i) implies that all break points of rank Bk(λ) are isolated. Now we
present an extended version of Theorems 5.15 and 5.16 without condition (5.38).
Recall that according to deﬁnition (5.39), we denote by mk(λ) the multiplicity of
focal points for conjoined bases of (SDSλ) in the interval (k, k +1] (see (5.39)). The
number n1(λ) = l(Y(λ), 0, N + 1) is then the number of forward focal points of
a conjoined basis Y(λ) = (X(λ), U(λ)) in (0, N + 1], including the multiplicities.
Theorem 5.95 (Local Oscillation Theorem for rankB(λ) ̸= const) Let assumpt-
ion (5.3) be satisﬁed. Let Y(λ) be a conjoined basis of (SDSλ) with (5.20). Then
mk(λ−) and mk(λ+) exist for all λ ∈R and
mk(λ+) = mk(λ) ≤n,
mk(λ+) −mk(λ−) + rank Bk(λ−) −rank Bk(λ)
= rankXk(λ) −rankXk(λ−) + rankXk+1(λ−) −rank Xk+1(λ).
⎫
⎪⎪⎬
⎪⎪⎭
(5.330)
Moreover,
n1(λ+) = n1(λ) ≤n(N + 1),
n1(λ+) −n1(λ−) +
N

k=0
ϑk(λ)
= rank XN+1(λ−) −rankXN+1(λ) = θ(λ).
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.331)
The proof of Theorem 5.95 is presented in Sect. 5.6.4.

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
355
Recall again that by Corollary 5.7 all break points of rank XN+1(λ) and of
rankBk(λ) are isolated, so that by using that n1(λ) is right-continuous we derive
from (5.331) for any a, b ∈R with a < b
n1(b) −n1(a) +

μ∈(a,b]
N

k=0
ϑk(μ) =

μ∈(a,b]
θ(μ),
(5.332)
where the sums 
μ∈(a,b]
N
k=0 ϑk(μ) and 
μ∈(a,b] θ(μ) are ﬁnite. If we assume
additionally that the function rank Bk(λ) is left-continuous for all sufﬁciently small
λ, i.e., if there exists λ00 ∈R such that
rankBk(λ) = rankBk(λ−)
for all λ < λ00,
k ∈[0, N]Z,
(5.333)
then by applying Theorem 5.95, we have
n1(λ+) = n1(λ),
n1(λ+) −n1(λ−) = rankXN+1(λ−) −rank XN+1(λ) ≥0,

λ < λ00.
(5.334)
Repeating the proof of Theorem 5.16 we conclude by (5.334) that n1(λ) is bounded
and nondecreasing for all λ < λ00, then there exists λ0 such that
n1(λ) ≡m
for all λ < λ0.
So we see by (5.334) that
rankXN+1(λ) = rank XN+1(λ−)
for all λ < λ0,
(5.335)
i.e., the ﬁnite spectrum of (E) is bounded from below. Moreover, using the
boundedness of n1(λ), we can prove the following corollary to Theorem 5.95.
Corollary 5.96 Assume (5.3). Then, condition (5.333) holds if and only if the ﬁnite
spectrum of (E) is bounded from below.
Proof It was already proved above that (5.333) implies (5.335). Conversely,
assume (5.335). It follows from (5.332) that
						

μ∈(a,b]
N

k=0
ϑk(μ) −

μ∈(a,b]
θ(μ)
						
= |n1(b) −n1(a)| ≤n(N + 1).
Moreover, under assumption (5.335), we have 
μ∈(a,b] θ(μ) = 0 for b < λ0. It
follows that 
μ∈(a,b]
N
k=0 ϑk(μ) is bounded as a →−∞and ϑk(μ) ≥0. Finally,
there exists λ00 such that (5.333) holds, i.e., ϑk(λ) ≡0.
⊓⊔

356
5
Discrete Symplectic Eigenvalue Problems
Remark 5.97
(i) Note that conditions (5.3) and (5.333) imply that there exists ˜λ such that
rank Bk(λ) = const,
rankXN+1(λ) = const
for all λ < ˜λ.
(5.336)
Indeed, by (5.327), (5.328) rank Bk(λ0) ≤rankBk(λ) for λ ∈(λ0 −δ, λ0 + δ)
and the last inequality coupled with (5.333) implies that 0 ≤rank Bk(λ) ≤n is
a nondecreasing function with respect to λ for λ < λ00. Then there exists the
ﬁnite limit of rank Bk(λ) as λ →−∞, i.e., (5.336) holds. By a similar way,
condition (5.335) implies that rankXN+1(λ) = const for all sufﬁciently small
λ.
(ii) For the linear Hamiltonian difference system (5.144) condition (5.333) is
automatically satisﬁed under (5.3). Indeed, according to Example 5.37, con-
dition (5.3) is equivalent to
˙Hk(λ) ≥0,
Hk(λ) =
−Ck(λ) AT
k (λ)
Ak(λ) Bk(λ),

,
(5.337)
and then the symmetric matrix Bk(λ) is nondecreasing matrix function with
respect to λ. By Weyl’s inequality, see [270, Corollary 4.9], all eigenvalues
of Bk(λ) are nondecreasing with respect to λ. So we have that ind Bk(λ) and
ind [−Bk(λ)] are nonincreasing and nondecreasing functions of λ, so that there
exist ﬁnite limits
lim
λ→−∞ind Bk(λ) < ∞,
lim
λ→−∞ind [−Bk(λ)] < ∞.
This means that the number of positive and negative eigenvalues of Bk(λ) is constant
for all λ < ˜λ. Consequently, we obtain that rank Bk(λ) = rankBk(λ) = const for
all λ < ˜λ.
Recall now the notation from Sect. 5.1.3
n1(λ) ≤Nn,
n2(λ) :=

μ≤λ
θ(μ)
for the number of forward focal points of the principal solution Y [0](λ) in (1, N +1]
and for the number of ﬁnite eigenvalues of (E) less than or equal to λ, and introduce
a similar notation for the step function
nB(λ) :=

μ≤λ
N

k=0
ϑk(μ),
(5.338)

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
357
where ϑk(μ) is deﬁned by (5.329). Note that conditions (5.3) and (5.333) imply that
there exists ˜λ ∈R such that
n2(λ) ≡0,
nB(λ) ≡0,
n1(λ) ≡m
for all λ < ˜λ,
(5.339)
by formula (5.331) and Corollary 5.96. Under assumptions (5.3) and (5.333), the
functions n2(λ) and nB(λ) are also right-continuous, i.e.,
n2(λ) = n2(λ+),
nB(λ) = nB(λ+),
by Theorem 5.3 and Corollary 5.7. From Corollary 5.96 and Theorem 5.95, we then
derive the following generalization of the global oscillation theorem (Theorem 5.17)
for the case when rank Bk(λ) is not constant.
Theorem 5.98 (Global Oscillation Theorem for rank Bk(λ) ̸= const) Assume
that (5.3) and (5.333) hold. Then there exists m ∈[0, nN]Z such that for any λ ∈R,
we have
n2(λ) + m = n1(λ) + nB(λ),
(5.340)
where m is given by (5.339).
Proof The proof follows from (5.332) for b := λ, where we evaluate the ﬁnite limits
of the quantities n1(a), 
a<μ≤λ
N
k=0 ϑk(μ), and 
a<μ≤λ θ(μ) as a →−∞
according to (5.339), (5.335), and (5.333).
⊓⊔
Note that under the assumption (5.38), we have in (5.340) that nB(λ) = 0 for λ ∈
R. In this case Theorems 5.95 and 5.98 present the results of Theorems 5.15, 5.16,
and 5.17. However, the same results follow from Theorem 5.95 under the weakened
assumption rank Bk(λ) = rank Bk(λ−) for λ ∈R, i.e., the following corollary holds.
Corollary 5.99 Assume (5.3) and suppose that (5.333) holds for all λ ∈R. Then
under the notation of Theorem 5.98, there exists m ∈[0, nN]Z such that
n2(λ) + m = n1(λ)
for all λ ∈R,
(5.341)
where m is given by (5.339).
The previous version of the global oscillation theorem (Theorem 5.17) implies
that the number n1(λ) of focal points of Y [0](λ) in (0, N + 1] is monotonic in
λ on R. The present version (Theorem 5.98) says that this function in general is
not monotonic, but there exists a representation of the quantity n1(λ) −m as the
difference n2(λ) −nB(λ) of two nondecreasing functions in accordance with the
Jordan decomposition of a function of bounded variation. In the last part of this
subsection, we provide examples illustrating the results stated above.

358
5
Discrete Symplectic Eigenvalue Problems
Example 5.100 This example is devoted to applications of formula (5.332). Con-
sider problem (E) for the trigonometric difference system with
Sk(λ) =
 cos λ sin λ
−sin λ cos λ

,
k ∈[0, N]Z.
(5.342)
According to (5.3), we have (Sk(λ)) = I > 0, so that the the monotonicity
condition (5.3) holds for λ ∈R. The principal solution Y [0](λ) of (E) with (5.342)
has the form Y [0]
k (λ) =

sin(kλ), cos(kλ)
T , and then the ﬁnite eigenvalues of this
problem are λp = πp/(N + 1), p ∈Z. The multiplicities of focal points of Y [0](λ)
in (k, k + 1] are given by
mk(λ) =
⎧
⎪⎨
⎪⎩
1, λ = πp/(k + 1), λ ̸= πl, p, l ∈Z,
1, sin(λ) sin(kλ) sin((k + 1) λ) < 0,
0, otherwise.
So we see that mk(λ) = mk(λ + πl) for l ∈Z, and then n1(λ) is periodic with the
minimal period T = π and nondecreasing in any interval [a, b] ⊆[πl, π(l + 1)),
l ∈Z. For example, for N = 3, we have
n1(λ) =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0, λ ∈[0, π/4),
1, λ ∈[π/4, π/2),
2, λ ∈[π/2, 3π/4),
3, λ ∈[3π/4, π),
n1(λ) = n1(λ + πl),
l ∈Z,
and by (5.332) the quantity n1(b) −n1(a) presents the number of eigenvalues
of (E), (5.342) in (a, b] for πl ≤a < b < π(l + 1), l ∈Z. However, if
b = π(l + 1), then the function n1(λ) loses the monotonicity, and in accordance
with (5.332), we restore it by adding the total number of zeros of Bk(λ) = sin(λ),
k ∈[0, 3]Z, in (a, b]. So we have that n1(π(l + 1)) −n1(a) + 4 = 4 −n1(a)
presents the number of eigenvalues of (E), (5.342) in (a, π(l + 1)] for πl ≤
a ≤π(l + 1), l ∈Z. Similarly, for the case a = −π/2, b = 2π, and
N = 3, the number of focal points of Y [0](λ) equals to n1(2π) = n1(0) = 0,
n1(−π/2) = n1(π/2) = 2, the number of eigenvalues of (E) in (−π/2, 2π] equals
to 10, the sum 
−π/2<μ≤2π
3
k=0 ϑk(μ) = 4 · 3 = 12, and (5.332) says that
n1(2π) −n1(−π/2) + 
−π/2<μ≤2π
3
k=0 ϑk(μ) = −2 + 12 = 10.

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
359
Consider an example illustrating Theorem 5.98.
Example 5.101 Introduce symplectic difference system (E) with
Sk(λ) =
λ −k + 1
λ −k
−λ + k
1 −λ + k

.
Then we have k(Sk(λ)) =
 1 1
1 1

≥0 and condition (5.336) for the coefﬁcient
Bk(λ) = λ −k for k ∈[0, N]Z holds for all λ < ˜λ = 0. For this case, problem (E)
has only one ﬁnite eigenvalue for any N. For N = 3, the principal solution at 0 is
Y [0]
0 (λ) =
0
1

,
Y [0]
1 (λ) =

λ
1 −λ

,
Y [0]
2 (λ) =
2λ −1
2 −2λ

,
Y [0]
3 (λ) =
3λ −3
4 −3λ

,
Y [0]
4 (λ) =
4λ −6
7 −4λ

.
Then the problem has the unique eigenvalue λ1 = 3
2. The multiplicities of focal
points for this solution are
m0(λ) = 0,
m1(λ) =

1, λ ∈(−∞, 0) ∪[ 1
2, 1),
0, otherwise,
m2(λ) =

1, λ ∈(−∞, 1
2) ∪[1, 2),
0, otherwise,
m3(λ) =

1, λ ∈(−∞, 1) ∪[ 3
2, 3),
0, otherwise.
Then the numbers n1(λ) of focal points in (0, 4] and the function nB(λ) are
n1(λ) =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
3, λ ∈(−∞, 0),
2, λ ∈[0, 1) ∪[ 3
2, 2),
1, λ ∈[1, 3
2) ∪[2, 3)
0, λ ∈[3, ∞).
nB(λ) =
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
0, λ ∈(−∞, 0),
1, λ ∈[0, 1),
2, λ ∈[1, 2),
3, λ ∈[2, 3),
4, λ ∈[3, ∞).
So we see that the constant m in Theorem 5.98 equals to 3. Finally, we have
n1(λ) −m + nB(λ) = n2(λ) =

0, λ ∈(−∞, 3
2),
1, λ ∈[ 3
2, ∞).

360
5
Discrete Symplectic Eigenvalue Problems
Example 5.102 Consider 4 × 4 symplectic system (SDSλ) for k ∈[0, 2]Z with
S0(λ) = S2(λ) =
⎛
⎜⎜⎝
1
0 0 0
0
0 0 1
−λ 0 1 0
0 −1 0 −λ
⎞
⎟⎟⎠,
S1(λ) =
⎛
⎜⎜⎝
1
0
λ3
0
0
0
0
1
−λ 0 1 −λ4
0
0 −1
0
−λ −λ3
⎞
⎟⎟⎠.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.343)
Condition (5.3) takes the form (Sk(λ)) = diag{0, I} ≥0 for k ∈{0, 2} and
(S1(λ)) =
⎛
⎜⎜⎝
3λ4
0
3λ3 0
0 1 + 3λ2
0 0
3λ3
0
3λ2 0
0
0
0 0
⎞
⎟⎟⎠≥0.
The upper blocks of the principal solution Y [0](λ) are
X[0]
0 (λ) = 0,
X[0]
1 (λ) =
0 0
0 1

,
X[0]
2 (λ) =
λ3 0
0 −λ

,
X[0]
3 (λ) =
λ3
0
0 λ2 + λ4 −1

.
Then the ﬁnite eigenvalues of (E), (5.343) are
λ1 = −
5
−1 +
√
5
2
,
λ2 = 0,
λ3 =
5
−1 +
√
5
2
,
and the multiplicities of focal points of the principal solution Y [0](λ) are
m0(λ) = 0,
m1(λ) =

1, λ ≥0,
0, λ < 0,
m2(λ) =

1, λ (λ2 + 1−
√
5
2
) ≥0 and λ ̸= 0,
0, otherwise.

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
361
Then the function n1(λ) = m0(λ) + m1(λ) + m2(λ) takes the form
n1(λ) =
⎧
⎪⎨
⎪⎩
0, λ < λ1,
1, λ1 ≤λ < λ3,
2, λ3 ≤λ.
So we see that n1(λ) does not count the eigenvalue λ2 = 0. By (5.340), we need
to add the function nB(λ) = 0 for λ < 0 and nB(λ) = 1 for λ ≥0 to derive
n2(λ) = n1(λ) + nB(λ) counting all (real) eigenvalues of the problem.
In the last example, we illustrate Corollary 5.99.
Example 5.103 For the illustration of Corollary 5.99, we change the statement of
the problem in Example 5.102 replacing S1(λ) in (5.343) by the matrix
S1(λ) =
⎛
⎜⎜⎝
1
0 0 0
0
0 0 1
−λ 0 1 0
0 −1 0 −λ
⎞
⎟⎟⎠,
for λ ≤0,
S1(λ) =
⎛
⎜⎜⎝
1
0
λ3
0
0
0
0
1
−λ 0 1 −λ4
0
0 −1
0
−λ −λ3
⎞
⎟⎟⎠,
for λ > 0,
which is continuously differentiable with respect to λ ∈R. Then, the matrices Sk(λ)
for k ∈[0, 2]Z obey condition (5.333) for all λ ∈R. The modiﬁed principal solution
Y [0](λ) has the upper blocks
X[0]
0 (λ) = 0,
X[0]
1 (λ) =
0 0
0 1

,
X[0]
2 (λ) =
0 0
0 −λ

for λ ≤0,
X[0]
2 (λ) =
λ3 0
0 −λ

for λ > 0,
X[0]
3 (λ) =
0
0
0 λ2 −1

for λ ≤0,
X[0]
3 (λ) =
λ3
0
0 λ2 + λ4 −1

for λ > 0.
By Deﬁnition 5.5, this modiﬁed problem has the ﬁnite eigenvalues
˜λ1 = −1,
˜λ2 =
5
−1 +
√
5
2
,
and λ = 0 is not a ﬁnite eigenvalue anymore. The multiplicities m0(λ) and m1(λ)
of focal points for this principal solution are the same as in Example 5.102, while

362
5
Discrete Symplectic Eigenvalue Problems
m2(λ) is given by
m2(λ) =

1, λ ∈[−1, 0) ∪[˜λ2, ∞),
0, otherwise.
Then it follows that m0(λ) + m1(λ) + m2(λ) = n1(λ) = n2(λ) in accordance with
Corollary 5.99 (with m = 0).
5.6.2
Monotonicity and the Comparative Index
Since monotonicity condition (5.3) is formulated for the symplectic coefﬁcient
matrix Sk(λ), the main purpose of this subsection is to relate (5.3) with the limit
behavior of the comparative index for symplectic matrices introduced in Sect. 3.3.1.
Recall the notation
⟨W⟩=
X
U

,
X =
I 0
A B

,
U =
0 −I
C D

,
W =
A B
C D

(5.344)
for arbitrary symplectic matrix W separated into the n×n blocks A, B, C, D. Basic
properties of the comparative index μ(⟨W⟩, ⟨ˆW ⟩) are given in Lemma 3.21. Recall
that according to the duality principle (see Theorem 3.11 and Remark 3.41), all
identities in Lemma 3.21 hold also for the dual index μ∗(⟨W⟩, ⟨ˆW ⟩). For example,
instead of (v) in Lemma 3.21, we have
μ∗(⟨W⟩, ⟨ˆW ⟩) = μ∗(W(0 I)T, ˆW(0 I)T ) + μ∗(⟨ˆW −1W⟩, ⟨I⟩)
= μ(W −1(0 I)T, ˆW −1(0 I)T ) + μ∗(⟨W ˆW −1⟩, ⟨I⟩).

(5.345)
For the nonsingular case det B ̸= 0 and det ˆB ̸= 0, it follows that det X ̸= 0
and det ˆX ̸= 0 for X and ˆX deﬁned by (5.344), and then according to Sect. 3.3.3
(see (3.81))
μ(⟨W⟩, ⟨ˆW ⟩) = μ∗(⟨ˆW⟩, ⟨W⟩) = ind ( ˆQ −Q),
Q = UX −1 =
 B−1A −B−1
−BT −1 DB−1

,
⎫
⎪⎬
⎪⎭
(5.346)
with ˆQ deﬁned analogously to Q.
Connections between monotonicity condition (5.3) and the comparative index
theory are based on the following lemma.
Lemma 5.104 Assume that a symplectic matrix W(λ) is piecewise continuously
differentiable with respect to λ ∈R and obeys (1.201), i.e., (W(λ)) ≥0 for

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
363
all λ ∈R. Then for any λ0 ∈R, there exists δ = δ(λ0) > 0 such that for all
a, b ∈(λ0 −δ, λ0 + δ) with a ≤b
μ(⟨W(b)⟩, ⟨W(a)⟩) = μ(W(b) (0 I)T, W(a) (0 I)T ),
μ∗(⟨W(a)⟩, ⟨W(b)⟩) = μ∗(W(a) (0 I)T, W(b) (0 I)T ),

(5.347)
and
μ(⟨W(b)⟩, ⟨W(a)⟩) = 0
for all a, b ∈[λ0, λ0 + δ), a ≤b,
μ∗(⟨W(a)⟩, ⟨W(b)⟩) = 0
for all a, b ∈(λ0 −δ, λ0], a ≤b.

(5.348)
Proof For (5.347) we ﬁx λ0 ∈R and introduce the symplectic matrix
R =
I G
0 I

,
G := I −B(λ0)B†(λ0).
(5.349)
Note that the 2n×n submatrix [BT (λ) DT (λ)]T of W(λ) obeys condition (3.1), and
then ˜W(λ) = R−1W(λ) =
 ˜A(λ) ˜B(λ)
˜C(λ) ˜D(λ)

has the nonsingular block
˜B(λ) := B(λ) −GD(λ)
for λ = λ0; see formula (1.175) in Lemma 1.68. Since det ˜B(λ) is continuous
with respect to λ and det ˜B(λ0) ̸= 0, there exists δ = δ(λ0) > 0 such that
det ˜B(λ) ̸= 0 for all λ ∈(λ0 −δ, λ0 + δ). By Lemma 1.77 applied to the case
under the consideration (with P = I), condition (1.202) holds, then we have for all
a, b ∈(λ0 −δ, λ0 + δ) with a ≤b
μ(⟨R−1W(b)⟩, ⟨R−1W(a)⟩) = μ∗(⟨R−1W(a)⟩, ⟨R−1W(b)⟩)
= ind [ ˜Q(a) −˜Q(b)] = 0,

,
(5.350)
where ˜Q(λ) is deﬁned via the blocks of ˜W(λ) := R−1W(λ) according to (5.346).
By Lemma 3.21(v),(iii) condition (5.350) implies
μ(⟨W −1(a)W(b)⟩, ⟨I⟩) = μ∗(⟨W −1(b)W(a)⟩, ⟨I⟩) = 0
for all a, b ∈(λ0 −δ, λ0 + δ) with a ≤b. Then by Lemma 3.21(v) and (5.345),
we derive (5.347). Next we shall prove (5.348). By Lemma 3.21(v),(iv), it follows
from (5.350) that
μ(R−1W(b) (0 I)T, R−1W(a) (0 I)T ) = μ∗(R−1W(a) (0 I)T, R−1W(b) (0 I)T ) = 0,

364
5
Discrete Symplectic Eigenvalue Problems
where R is given by (5.349). Then, according to Theorem 3.6 and Theorem 3.5(ix),
μ(W(b) (0 I)T, W(a) (0 I)T ) = μ(R(R−1W(b)) (0 I)T, R(R−1W(a)) (0 I)T )
= μ(R−1W(b) (0 I)T, R−1W(a) (0 I)T ) + μ(W(b) (0 I)T, R(0 I)T )
−μ(W(a) (0 I)T, R(0 I)T )
= μ(R−1W(a) (0 I)T, R−1(0 I)T ) −μ(R−1W(b) (0 I)T, R−1(0 I)T ).
So we have derived that for all a, b ∈(λ0 −δ, λ0 + δ) with a ≤b
μ(⟨W(b)⟩, ⟨W(a)⟩) = μ(W(b) (0 I)T, W(a) (0 I)T )
= μ( ˜W(a) (0 I)T, R−1(0 I)T ) −μ( ˜W(b) (0 I)T, R−1(0 I)T ),

(5.351)
where the matrix ˜W(λ) = R−1W(λ) has the nonsingular block ˜B(λ). Evaluating
the ﬁrst comparative index on the right-hand side above according to Deﬁnition 3.1,
we obtain
μ( ˜W(λ) (0 I)T, R−1(0 I)T ) = μ2( ˜W(λ) (0 I)T, R−1(0 I)T )
= ind [−GB(λ) ˜B−1(λ) G].
Since GB(λ0) = 0 by (5.349), it follows that ind [−GB(λ0) ˜B(λ0)−1G] = 0.
Putting a := λ0 and b ∈[λ0, λ0 + δ) in (5.351), we derive
0 ≤μ(⟨W(b)⟩, ⟨W(λ0)⟩) = μ(W(b) (0 I)T, W(λ0) (0 I)T )
= μ( ˜W(λ0) (0 I)T, R−1(0 I)T ) −μ( ˜W(b) (0 I)T, R−1(0 I)T )
= −μ( ˜W(b) (0 I)T, R−1(0 I)T ) ≤0.
Thus, we have proved
μ( ˜W(λ) (0 I)T, R−1(0 I)T ) = 0
for all λ ∈[λ0, λ0 + δ).
(5.352)
Finally, we complete the proof of the ﬁrst equality in (5.348) by using (5.352)
and (5.351). The proof of the second equality in (5.348) is similar. Instead of (5.351),
we have for all a, b ∈(λ0 −δ, λ0 + δ) with a ≤b
μ∗(⟨W(a)⟩, ⟨W(b)⟩) = μ∗(W(a) (0 I)T, W(b) (0 I)T )
= μ∗( ˜W(b) (0 I)T, R−1(0 I)T ) −μ∗( ˜W(a) (0 I)T, R−1(0 I)T ),

(5.353)

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
365
where μ∗( ˜W(λ) (0 I)T, R−1(0 I)T ) = ind [GB(λ) ˜B−1(λ) G]. Then (5.352) is
replaced by
μ∗( ˜W(λ) (0 I)T, R−1(0 I)T ) = 0
for all λ ∈(λ0 −δ, λ0]
(5.354)
and we complete the proof of (5.348) combining (5.354) and (5.353).
⊓⊔
Using Lemma 3.21(iv), we see that conditions (5.348) are equivalent to
μ∗(⟨W(a)⟩, ⟨W(b)⟩) = rankB(b) −rankB(a)
for all a, b ∈[λ0, λ0 + δ), a ≤b,
μ(⟨W(b)⟩, ⟨W(a)⟩) = rankB(a) −rankB(b)
for all a, b ∈(λ0 −δ, λ0], a ≤b,
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.355)
where B(λ) is the block of W(λ) given by (5.344).
We proved Lemma 5.104 in the local sense, i.e., all results of this lemma hold in
a neighborhood of λ0. If we add the assumption
rankB(λ) is constant for λ ∈R,
(5.356)
then we can reformulate Lemma 5.104 in the global sense.
Lemma 5.105 Under the assumptions of Lemma 5.104 suppose additionally
that (5.356) holds. Then for all a, b ∈R with a ≤b
μ(⟨W(b)⟩, ⟨W(a)⟩) = μ∗(⟨W(a)⟩, ⟨W(b)⟩) = 0.
(5.357)
Proof First of all, let us recall that under assumption (1.201), condition (5.356) is
equivalent to
the set Im B(λ) is constant for λ ∈R
or
B(λ) B†(λ) is constant in λ ∈R;
(5.358)
see Theorem 1.82 and Corollary 1.83. Then one can prove (5.357) repeating
the main steps of the proof of Lemma 5.104 applied in the global sense under
assumption (5.356). We note that the same result follows also from Lemma 5.104 by
the “triangle inequality” (see Theorem 3.8) applied to the matrices ⟨W(a)⟩, ⟨W(b)⟩,
⟨W(c)⟩for a ≤b ≤c. Indeed, by (5.348), we have
μ(⟨W(b)⟩, ⟨W(a)⟩) = 0,
a, b ∈[λ0, λ0 + δ), a ≤b,

366
5
Discrete Symplectic Eigenvalue Problems
while from the second equality in (5.355) and (5.356), we get
μ(⟨W(b)⟩, ⟨W(a)⟩) = 0,
a, b ∈(λ0 + δ, λ0], a ≤b.
Applying inequality (3.18), we derive that μ(⟨W(b)⟩, ⟨W(a)⟩) = 0 holds for
all a, b ∈(λ0 + δ, λ0 + δ) with a ≤b. By a similar way, we obtain that
μ(⟨W(c)⟩, ⟨W(b)⟩) = 0 and μ(⟨W(b)⟩, ⟨W(a)⟩) = 0 imply μ(⟨W(c)⟩, ⟨W(a)⟩) =
0 for any a ≤b ≤c.
⊓⊔
Equalities (5.348) and (5.355) imply the main theorem of this section.
Theorem 5.106 Assume (5.3) and suppose that Z(λ) is a symplectic fundamental
matrix of (SDSλ) satisfying (5.11) for k = 0. Then for any λ0 ∈R, there exist the
ﬁnite limits
μ

⟨Sk(λ+
0 )⟩, ⟨Sk(λ0)⟩

= 0,
μ⟨Sk(λ0)⟩, ⟨Sk(λ−
0 )⟩ = ϑk(λ0),

k ∈[0, N]Z,
(5.359)
μ

⟨Zk(λ+
0 )⟩, ⟨Zk(λ0)⟩

= 0,
μ

⟨Zk(λ0)⟩, ⟨Zk(λ−
0 )⟩

= rankXk(λ−
0 ) −rankXk(λ0),

k ∈[0, N + 1]Z,
(5.360)
where Y T
k (λ)
=
(XT
k (λ), UT
k (λ))T
=
Zk(λ) (0 I)T , and ϑk(λ0) are given
by (5.329). In particular, for Z0(λ) = I, we have
μ

⟨ZN+1(λ+
0 )⟩, ⟨ZN+1(λ0)⟩

= 0,
μ

⟨ZN+1(λ0)⟩, ⟨ZN+1(λ−
0 )⟩

= θ(λ0),

(5.361)
where θ(λ0) is the (algebraic) multiplicity of the ﬁnite eigenvalue λ0 of (E).
Proof Recall that (5.3) and (5.11) for k = 0 imply (5.11) for k ∈[0, N]Z (see
Sect. 5.1.1). Then Sk(λ) and Zk(λ) meet all requirements of Lemma 5.104. The
existence of the limits μ(⟨Sk(λ+
0 )⟩, ⟨Sk(λ0)⟩) = 0 and μ(⟨Zk(λ+
0 )⟩, ⟨Zk(λ0)⟩) = 0
follows directly from (5.348) with a := λ0, where we put W(λ) := Sk(λ) and
W(λ) := Zk(λ). The existence of the limits rank Bk(λ−
0 ) and rank Xk(λ−
0 ) follows
from Theorems 5.1 and 5.3, but we can also prove this fact independently by using
Lemma 5.104. Indeed, for the case a ≤b < λ0, we have by the second equality
in (5.355) that rank Bk(λ) and rank Xk(λ) are nonincreasing with respect to λ ∈
(λ0 −δ, λ0), and then there exist the ﬁnite limits rank Bk(λ−
0 ) and rank Xk(λ−
0 ), i.e.,
there exists ˜δ ∈(0, δ] such that
rankBk(λ) ≡rankBk(λ−
0 ),
rank Xk(λ) ≡rankXk(λ−
0 ),

for all λ ∈(λ0 −˜δ, λ0).
(5.362)

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
367
By using the second equality in (5.355), putting W(λ) := Sk(λ) with b := λ0, we
derive the second equality in (5.359). By a similar way, putting W(λ) := Zk(λ), we
then complete the proof of (5.360).
⊓⊔
Remark 5.107
(i) We remark that the comparative index theory provides another proof of
Theorem 1.81 (or Theorems 5.1(ii) and 5.3(ii)) under assumption (1.201). For
example, applying (5.362) and the second equalities in (5.355) and (5.348), we
see that there exists ˜δ > 0 such that
μ

⟨Sk(b)⟩, ⟨Sk(a)⟩

= 0
for all a, b ∈(λ0 −˜δ, λ0), a ≤b,
μ∗
⟨Sk(a)⟩, ⟨Sk(b)⟩

= 0
for all a, b ∈(λ0 −˜δ, λ0], a ≤b.

(5.363)
Then by Lemma 3.21(v) and (5.345), we obtain
μ1(Sk(b) (0 I)T, Sk(a) (0 I)T ) = 0
for all a, b ∈(λ0 −˜δ, λ0), a ≤b,
μ∗
1(Sk(a) (0 I)T, Sk(b) (0 I)T ) = μ1(Sk(a) (0 I)T, Sk(b) (0 I)T ) = 0
for all a, b ∈(λ0 −˜δ, λ0], a ≤b.
By Theorem 3.2(iv), the last conditions are equivalent to (5.6) in Theo-
rem 5.1(ii). The case (5.7) can be derived similarly.
(ii) Analogously, Lemma 5.104 provides an alternative proof of Theorem 1.79
(or Theorems 5.1(i) 5.3(i)). In more details, by Lemma 3.21(v) and (5.345),
conditions (5.363) imply
μ∗
1(S−1
k (b) (0 I)T, S−1
k (a) (0 I)T ) = μ1(S−1
k (b) (0 I)T, S−1
k (a) (0 I)T ) = 0
for all a, b ∈(λ0 −˜δ, λ0), a ≤b,
μ1(S−1
k (a) (0 I)T, S−1
k (b) (0 I)T ) = 0
for all a, b ∈(λ0 −˜δ, λ0], a ≤b.
Then by Theorem 3.2(iv), we derive (5.4) in Theorem 5.1(i) (i.e., condi-
tion (5.327)). By a similar way, conditions (5.13) can be derived. By analogy
with the previous proof, we can also show that (5.328) and (5.14) follow from
the ﬁrst equalities in (5.355) and (5.348).
(iii) Note that Lemma 5.104 allows to count the jumps of rank Bk(λ) and
rankXk(λ) in the right neighborhood of λ0. By analogy with the proof of
Theorem 5.106, we can show that there exist the ﬁnite limits
μ∗
⟨Sk(λ0)⟩, ⟨Sk(λ+
0 )⟩

= rank Bk(λ+
0 ) −rank Bk(λ0),
μ∗
⟨Sk(λ−
0 )⟩, ⟨Sk(λ0)⟩

= 0,

k ∈[0, N]Z,
μ∗
⟨Zk(λ0)⟩, ⟨Zk(λ+
0 )⟩

= rank Xk(λ+
0 ) −rank Xk(λ0),
μ∗
⟨Zk(λ−
0 )⟩, ⟨Zk(λ0)⟩

= 0,

k ∈[0, N + 1]Z.

368
5
Discrete Symplectic Eigenvalue Problems
5.6.3
Monotonicity and the Cayley Transform
In this section we investigate oscillatory properties of the following 4n × 4n
symplectic system
Yk+1 = W c
k (λ) Yk,
k ∈[0, N]Z,
W c
k (λ) = 1
2

I + W T −1
k
(λ)
J [I −Wk(λ)]
−J [I −W T −1
k
(λ)]
I + Wk(λ)

⎫
⎪⎪⎬
⎪⎪⎭
(5.364)
associated with the 2n × 2n symplectic difference system
Yk+1 = Wk(λ) Yk,
k ∈[0, N]Z,
(5.365)
with the symplectic matrix Wk(λ) satisfying the monotonicity condition (1.201).
We will call (5.364) the Cayley system associated with (5.365). Motivation for this
terminology is the following. Assume additionally that
det [I + Wk(λ)] ̸= 0.
(5.366)
Then the Cayley transform of Wk(λ) is deﬁned as
C(Wk(λ)) := [I −Wk(λ)] [I + Wk(λ)]−1,
(5.367)
and the matrix J C(Wk(λ)) is symmetric (see [326, Lemma 1.1]). Then, if we
introduce the notation
W c
k (λ) =
Ak(λ) Bk(λ)
Ck(λ) Dk(λ)

,
(5.368)
for the blocks of the coefﬁcient matrix in (5.364), then the symmetric matrices
A−1
k (λ) Bk(λ),
Ck(λ) A−1
k (λ),
Bk(λ) D−1
k (λ),
D−1
k (λ) Ck(λ)
are connected with the Cayley transforms of the matrices W −1
k
(λ), W T −1
k
(λ),
Wk(λ), W T
k (λ) by the formulas
A−1
k (λ) Bk(λ) = −J C(W −1
k
(λ)), Ck(λ) A−1
k (λ) = −J C(W −1 T
k
(λ)),
Bk(λ) D−1
k (λ) = J C(Wk(λ)),
D−1
k (λ) Ck(λ) = J C(W T
k (λ)).

(5.369)
System (5.364) is taken into consideration by a modiﬁed version of Theo-
rem 4.45, which is the basic tool for the proof of Theorem 5.95. For that proof,
we will need the following equivalent form of formulas (4.111) and (4.109)

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
369
in Theorem 4.45 regarding the multiplicities of focal points of conjoined bases
of (4.98) and (SDS).
Proposition 5.108 (Sturmian Comparison Theorem) Equality (4.111) in Theo-
rem 4.45 can be written as
m( ˆYk) −m(Yk) + μ(⟨ˆSk⟩, ⟨Sk⟩)
= μ(⟨ˆZk⟩, ⟨Zk⟩) + μ(R−1⟨Z−1
k+1 ˆZk+1⟩, R−1⟨Z−1
k
ˆZk⟩),

(5.370)
where the symplectic orthogonal 4n × 4n matrix R is given by
R =
1
√
2
⎛
⎜⎜⎝
0 −I I
0
0
I
I
0
−I
0 0 −I
−I
0 0 I
⎞
⎟⎟⎠
(5.371)
and the comparative index
μ

R−1⟨Z−1
k+1 ˆZk+1⟩, R−1⟨Z−1
k
ˆZk⟩

= μ∗
R−1⟨ˆZ−1
k+1Zk+1⟩, R−1⟨ˆZ−1
k Zk⟩

(5.372)
is equal to the number of forward focal points in (k, k + 1] of the conjoined basis
Yk = R−1⟨Z−1
k
ˆZk⟩of the Cayley system
Yk+1 = W c
k Yk,
k ∈[0, N]Z,
with Wk := Z−1
k+1 ˆSkZk.
The proof of Proposition 5.108 is given in Sect. 5.6.4. Point out that we will
apply (5.370) to the case Sk := Sk(λ0) and ˆSk := Sk(λ), where λ belongs
to a neighborhood of λ0. Then we have to investigate the oscillatory properties
of (5.364) under the monotonicity condition (1.201).
Lemma 5.109 Let W(λ) ∈R2n×2n be a continuously differentiable symplectic
matrix. Then condition (1.201) is equivalent to
(W c(λ)) = J4n d
dλ [W c(λ)] J4n [W c(λ)]TJ4n ≥0,
where J4n ∈R4n×4n and W c(λ) is given by (5.364). Moreover, if we assume that
Wk(λ) obeys (5.366) for k ∈[0, N]Z, then each of the following conditions for the
blocks in (5.368)
d
dλ [A−1
k (λ) Bk(λ)] ≥0,
(5.373)
d
dλ [Ck(λ) A−1
k (λ)] ≤0,
(5.374)

370
5
Discrete Symplectic Eigenvalue Problems
d
dλ [Bk(λ) D−1
k (λ)] ≥0,
(5.375)
d
dλ [D−1
k (λ) Ck(λ)] ≤0
(5.376)
is equivalent to (1.201) for W(λ) := Wk(λ).
Proof For the proof of the ﬁrst claim, we use the following representation:
W c = R−1{I, W} R,
{I, W} =
⎛
⎜⎜⎝
I 0 0 0
0 A 0 B
0 0 I 0
0 C 0 D
⎞
⎟⎟⎠,
W =
A B
C D

,
(5.377)
where R is given by (5.371). Moreover, the matrix {I, W} is symplectic if and only
if so is W (see Sect. 3.3.5 for more details). Formula (5.377) justiﬁes the symplectic
structure of the matrices W c
k (λ) in (5.364) and the construction of conjoined bases
of (5.364) in the subsequent proofs (see Lemma 5.110). Applying (5.377), we have
(W c(λ)) = RT ({I, W(λ)}) R = RT {0, (W(λ))} R,
where the 4n × 4n matrix {0, (W(λ))} is symmetric and nonnegative if and only
if so is (W(λ)). The proof of the ﬁrst claim is completed. Next, it is easy to verify
that
d
dλ
1
2 J C(W(λ))

= [I + W(λ)]T −1W T (λ) (W(λ)) W(λ) [I + W(λ)]−1,
and then condition (1.201) for Wk(λ) is equivalent to
d
dλ

J C(Wk(λ))

≥0.
(5.378)
Moreover, using the representations (see Proposition 1.76(ii)), we get
(W −1(λ)) = −W T (λ) (W(λ)) W(λ),
(W T −1(λ)) = (J W(λ) J T ) = J (W(λ)) J T ,
and (W T (λ)) = (J W −1(λ) J T ) = J (W −1(λ)) J T . Thus, we see that
equation (1.201) holds if and only if (see Proposition 1.76 (iv))
(W −1(λ)) ≤0,
(W −1 T (λ)) ≥0,
(W T (λ)) ≤0.

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
371
So we also have that (1.201) for Wk(λ) is equivalent to each of the inequalities
d
dλ [J C(W −1
k
(λ)] ≤0,
d
dλ [J C(W T −1
k
(λ)] ≥0,
d
dλ [J C(W T
k (λ)] ≤0.

(5.379)
Using (5.369), (5.378), and (5.379), we then complete the proof of the second claim.
⊓⊔
Recall that we will apply the results of this section to the special case of sys-
tem (5.364) with Wk(λ) := Z−1
k+1(λ0) Sk(λ) Zk(λ0), where Z−1(λ) is a fundamental
matrix of (SDSλ). In this case obviously Wk(λ0) = I. In this connection, we prove
the following lemma.
Lemma 5.110 Assume that a symplectic matrix Wk(λ) is continuously differen-
tiable and (1.201) holds for W(λ) := Wk(λ) for k ∈[0, N]Z. Suppose that for
some λ0 ∈R
Wk(λ0) = I,
k ∈[0, N]Z.
(5.380)
Then there exists δ > 0 such that the principal solution Yk(λ) at 0 of the Cayley
system (5.364) does not have any forward focal points in the interval (0, N + 1] for
all λ ∈[λ0, λ0 + δ), i.e.,
m(Yk(λ)) = μ

Yk+1(λ), W c
k (λ) (0 I)T 
= 0,
k ∈[0, N]Z.
(5.381)
Similarly, there exists δ > 0 such that for all λ ∈(λ0 −δ, λ0]
μ∗
Yk+1(λ), W c
k (λ) (0 I)T 
= 0,
k ∈[0, N]Z.
(5.382)
Proof Note ﬁrst that (5.380) implies that there exists ˆδ > 0 such that (5.366) holds
for all λ ∈(λ0−ˆδ, λ0+ˆδ), i.e., the continuous matrices I+Wk(λ) = 2I+[Wk(λ)−I]
are nonsingular for k ∈[0, N]. Then condition (1.201) implies (5.373) and (5.374)
for the blocks of W c
k (λ). Moreover, by (5.380), we have A−1
k (λ0) Bk(λ0) =
Ck(λ0) A−1
k (λ0) = 0, and then we get
A−1
k (λ) Bk(λ) ≥0,
Ck(λ) A−1
k (λ) ≤0,
λ ∈[λ0, λ0 + ˆδ),
(5.383)
A−1
k (λ) Bk(λ) ≤0,
Ck(λ) A−1
k (λ) ≥0,
λ ∈(λ0 −ˆδ, λ0].
(5.384)
Secondly, point out that the principal solution at 0 of system (5.364) has the form
Yk(λ) =
Xk(λ)
Uk(λ)

= Zc
k(λ) (0 I)T = 1
2
J [I −Zk(λ)]
I + Zk(λ)

,
Z0(λ) = I,
(5.385)

372
5
Discrete Symplectic Eigenvalue Problems
where the symplectic 2n×2n matrix Zk(λ) solves (5.365). Then (5.11) holds for all
k ∈[0, N + 1]Z (see Proposition 5.2). Condition (5.380) implies that Zk(λ0) = I
for k ∈[0, N +1]Z, and then the continuous matrices I +Zk(λ) = 2I +[Zk(λ)−I]
for k ∈[0, N + 1]Z are nonsingular in a sufﬁciently small neighborhood of λ0.
Then, applying Lemma 5.109 for the case W(λ) := Zk(λ), we see by (5.375) that
d
dλ [Xk(λ) U−1
k (λ)] ≥0 and Xk(λ0) U−1
k (λ0) = 0. Then we can conclude that there
exists ˜δ > 0 such that
Xk(λ) U−1
k (λ) ≥0
for all λ ∈[λ0, λ0 + ˜δ),
(5.386)
Xk(λ) U−1
k (λ) ≤0
for all λ ∈(λ0 −˜δ, λ0].
(5.387)
Finally, we prove (5.381) and (5.382) by using relations which connect the number
of focal points of Yk(λ) and the transformed conjoined basis J T
4n Yk(λ), which has
the nonsingular upper block −Uk(λ), λ ∈(λ0 −˜δ, λ0 + ˜δ), given by (5.385). By
Corollary 4.66 (see (4.183)), we have
m(J T
4n Yk(λ)) −m(Yk(λ)) −μ

J T
4n Yk(λ), J T
4n (0 I)T 
= ind [−AT
k (λ) Ck(λ)] −ind [Ak(λ) BT
k (λ)],
μ(J T
4n Yk(λ), J T
4n (0 I)T ) = ind [X T
k (λ) Uk(λ)].
⎫
⎪⎬
⎪⎭
(5.388)
By (5.386) and (5.383), we have m(J T
4n Yk(λ)) = m(Yk(λ)) for all λ ∈[λ0, λ0 +δ),
where δ := min(ˆδ, ˜δ). Moreover, m(J T
4n Yk(λ)) = ind Pk(λ), where
Pk(λ) = −Uk(λ) U−1
k+1(λ) Ck(λ) = −AT
k (λ) Ck(λ) + CT
k (λ) Xk+1(λ) U−1
k+1(λ) Ck(λ)
and Pk(λ0) = 0. Then, by (5.386) and (5.383), we obtain
Pk(λ) = −Uk(λ) U−1
k+1(λ) Ck(λ) ≥0
for all λ ∈[λ0, λ0 + δ),
(5.389)
Pk(λ) = −Uk(λ) U−1
k+1(λ) Ck(λ) ≤0
for all λ ∈(λ0 −δ, λ0].
(5.390)
So we complete the proof of (5.381) using (5.389), which implies that m(Yk) =
m(J T
4n Yk) = ind Pk(λ) = 0 for λ ∈[λ0, λ0 + δ). For the proof of (5.382), we use
instead of (5.388) the dual identity (see Theorem 3.11)
μ∗
Yk+1(λ), W c
k (λ) (0 I)T 
−ind [−Pk(λ)] −μ∗
J T
4n Yk(λ), J T
4n (0 I)T )
= ind [AT
k (λ) Ck(λ)] −ind [−Ak(λ) BT
k (λ)],
μ∗(J T
4n Yk(λ), J T
4n (0 I)T ) = ind [−X T
k (λ) Uk(λ)],
⎫
⎪⎬
⎪⎭
which implies that μ∗(Yk+1(λ), W c
k (λ) (0 I)T ) = 0 for all λ ∈(λ0 −δ, λ0] due
to (5.390), (5.387), and (5.384). The proof is complete.
⊓⊔

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
373
5.6.4
Proofs of the Main Results
In this section we provide the rather technical proof of Proposition 5.108 (see
Sect. 5.6.3) and the proof of the local oscillation theorem (Theorem 5.95).
Proof of Proposition 5.108 For the proof of (5.370), we apply the main properties
of the comparative index presented in Chap. 3. Note ﬁrst that with R deﬁned
by (5.371), we have R (02n I2n)T = (1/
√
2) ⟨I2n⟩(see notation (5.344)), where
the constant
√
2 can be omitted in computations using Theorem 3.5(i). Applying
the properties of the comparative index, we have by Theorem 3.5(v)
μ(Yk, ˆYk) =  rankw(Yk, ˆYk) −μ( ˆYk, Yk),
and by Lemma 3.21(iv)
μ

⟨ˆZ−1
k Zk⟩, ⟨ˆZ−1
k+1Zk+1⟩

= μ∗
⟨ˆZ−1
k+1Zk+1⟩, ⟨ˆZ−1
k Zk⟩

+  rankw(Yk, ˆYk),
where w(Yk, ˆYk) is the Wronskian given by (3.2). Here we also use the calculation
(I 0) ˆZ−1
k Zk(0 I)T = wT (Yk, ˆYk) (see (4.100)). After substituting the last two
formulas into (4.111) and (4.109) in Theorem 4.45, we derive
m( ˆYk) −m(Yk) +μ

⟨ˆSk⟩, ⟨Sk⟩

= μ( ˆYk, Yk) + μ∗
⟨ˆZ−1
k Zk⟩, ⟨ˆZ−1
k+1Zk+1⟩

.

(5.391)
Now we apply the transformations in Lemma 3.21(v) to get
μ( ˆYk, Yk) = μ

⟨ˆZk⟩, ⟨Zk⟩

−μ

⟨Z−1
k
ˆZk⟩, ⟨I⟩

,
in Lemma 3.21(iii) to get
μ∗
⟨ˆZ−1
k+1Zk+1⟩, ⟨ˆZ−1
k Zk⟩

= μ

⟨Z−1
k+1 ˆZk+1⟩, ⟨Z−1
k
ˆZk⟩

,
and in Theorem 3.6 to get
μ

⟨Z−1
k+1 ˆZk+1⟩, ⟨Z−1
k
ˆZk⟩

= μ

R(R−1⟨Z−1
k+1 ˆZk+1⟩), R(R−1⟨Z−1
k
ˆZk⟩)

= μR−1⟨Z−1
k+1 ˆZk+1⟩, R−1⟨Z−1
k
ˆZk⟩ + μ⟨Z−1
k
ˆZk⟩, ⟨I⟩.
A subsequent substitution all the formulas derived above into (5.391) leads to the
proof of (5.370). Next, identity (5.372) follows from the relation
μ

R−1⟨W⟩, R−1⟨ˆW⟩

= μ∗
R−1⟨W −1⟩, R−1⟨ˆW −1⟩

,
(5.392)

374
5
Discrete Symplectic Eigenvalue Problems
which is similar to identity (iii) in Lemma 3.21. Consider the proof of (5.392).
Applying Theorem 3.6, we have
μ

R−1⟨W⟩, R−1⟨ˆW⟩

= μ

⟨W⟩, ⟨ˆW ⟩

+ μ

⟨ˆW⟩, ⟨I⟩

−μ

⟨W⟩, ⟨I⟩

,
where by Lemma 3.21(iii)
μ

⟨W⟩, ⟨ˆW ⟩

= μ∗
⟨W −1⟩, ⟨ˆW −1⟩

,
μ

⟨ˆW⟩, ⟨I⟩

= μ∗
⟨ˆW −1⟩, ⟨I⟩

,
μ

⟨W⟩, ⟨I⟩

= μ∗
⟨ˆW −1⟩, ⟨I⟩

.
Then
μ

R−1⟨W⟩, R−1⟨ˆW⟩

= μ∗
⟨W −1⟩, ⟨ˆW −1⟩

+ μ∗
⟨ˆW −1⟩, ⟨I⟩

−μ∗
⟨W −1⟩, ⟨I⟩

= μ∗R−1⟨W −1⟩, R−1⟨ˆW −1⟩
by the dual version of Theorem 3.6 (see Corollary 3.12). The proof of (5.392)
and (5.372) is completed. Finally, note that by Lemma 4.7, the comparative index
μ∗R−1⟨ˆZ−1
k+1Zk+1⟩, R−1⟨ˆZ−1
k Zk⟩
is equal to the number of forward focal points in (k, k + 1] of the conjoined basis
Yk = R−1⟨Z−1
k
ˆZk⟩= Zk(0 I)T associated with the fundamental matrix
Zk =
√
2 R−1 {I, Z−1
k
ˆZk} R
(see notation (5.377)). It is easy to verify directly that Zk solves the Cayley system
Yk+1 = W c
k Yk,
k ∈[0, N]Z,
with Wk := Z−1
k+1 ˆSkZk.
The proof of Proposition 5.108 is complete.
⊓⊔
Proof of Theorem 5.95 Fix any λ0 ∈R, and apply (5.370) to the case
ˆSk := Sk(λ),
Sk := Sk(λ0),
ˆZk := Zk(λ),
Zk := Zk(λ0),
λ ≥λ0,
where we assume that symplectic fundamental matrix Zk(λ) does not depend on λ
for k = 0, i.e., ˆZ0 = Z0. Using the notation m(Yk(λ)) = mk(λ), where Yk(λ) =
Zk(λ) (0 I)T , we have
mk(λ) −mk(λ0) + μ

⟨Sk(λ)⟩, ⟨Sk(λ0)⟩

= μ

⟨Zk(λ)⟩, ⟨Zk(λ0)⟩

+μ∗
R−1⟨Z−1
k+1(λ)Zk+1(λ0)⟩, R−1⟨Z−1
k (λ)Zk(λ0)⟩

,
⎫
⎪⎬
⎪⎭
(5.393)

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
375
where we rewrite the last term according to (5.372). By (5.359) and (5.360) in
Theorem 5.106, we evaluate the right-hand limits
μ

⟨Sk(λ+
0 )⟩, ⟨Sk(λ0)⟩

= μ

⟨Zk(λ+
0 )⟩, ⟨Zk(λ0)⟩

= 0.
According to Lemma 4.7, the last term in (5.393) is equal to the number of forward
focal points in (k, k + 1] of the conjoined basis
Yk(λ) = R−1⟨Z−1
k (λ0)Zk(λ)⟩
(5.394)
of the Cayley system (5.364) with the matrix
Wk(λ) := Z−1
k+1(λ0) Sk(λ) Zk(λ0).
(5.395)
Note that the matrices Zk(λ0) and Zk+1(λ0) in (5.395) are constant for the ﬁxed
λ0, and then (Wk(λ)) = ZT
k+1(λ0) (Sk(λ)) Zk+1(λ0) ≥0 and Wk(λ0) = I.
Applying (5.381) in Lemma 5.110 and Theorem 3.5(i), we then obtain
μ∗R−1⟨Z−1
k+1(λ+
0 )Zk+1(λ0)⟩, R−1⟨Z−1
k (λ+
0 )Zk(λ0)⟩
= m(Yk(λ+
0 )) = μ

Yk(λ+
0 ), W c
k (λ+
0 ) (0 I)T 
= 0,
where Yk(λ) and Wk(λ) are given by (5.394) and (5.395). Substituting all the limits
derived above into (5.393), we conclude that there exists the ﬁnite limit
mk(λ+
0 ) = mk(λ0).
Next, applying (5.370) to the case
ˆSk := Sk(λ0), Sk := Sk(λ),
ˆZk := Zk(λ0), Zk := Zk(λ),
ˆZ0 = Z0, λ < λ0,
we derive
mk(λ0) −mk(λ) + μ

⟨Sk(λ0)⟩, ⟨Sk(λ)⟩

= μ

⟨Zk(λ0)⟩, ⟨Zk(λ)⟩

+μ

R−1⟨Z−1
k+1(λ)Zk+1(λ0)⟩, R−1⟨Z−1
k (λ)Zk(λ0)⟩

.
⎫
⎪⎬
⎪⎭
(5.396)
Then we evaluate the left-hand limits
μ

⟨Sk(λ0)⟩, ⟨Sk(λ−
0 )⟩

= ϑk(λ0),
μ

⟨Zk(λ0)⟩, ⟨Zk(λ−
0 )⟩

=  [rankXk(λ−
0 ) −rankXk(λ0)],

376
5
Discrete Symplectic Eigenvalue Problems
according to (5.359) and (5.360) in Theorem 5.106. Note that the last term in (5.396)
is dual to the last term in (5.393), i.e.,
μ

R−1⟨Z−1
k+1(λ)Zk+1(λ0)⟩, R−1⟨Z−1
k (λ)Zk(λ0)⟩

= μ∗
Yk+1(λ), W c
k (λ) (0 I)T 
,
where Yk(λ) and Wk(λ) are given by (5.394) and (5.395). Then, by (5.382) the left-
hand limit of this quantity equals to zero, and we derive from (5.396) that
mk(λ0) −mk(λ−
0 ) + ϑk(λ0) =  [rankXk(λ−
0 ) −rankXk(λ0)].
The proof of (5.330) is completed. Upon summing equality (5.330) for k ∈[0, N]Z,
we get (5.331). The proof of Theorem 5.95 is complete.
⊓⊔
5.6.5
Weighted Focal Points
In this section we consider the discrete Hamiltonian eigenvalue problems (5.144)
presented in Example 5.37, i.e.,
xk(λ) = Ak(λ) xk+1(λ) + Bk(λ) uk(λ),
uk(λ) = Ck(λ) xk+1(λ) −AT
k (λ) uk(λ),

k ∈[0, N]Z,
with the Dirichlet boundary conditions
x0(λ) = 0 = xN+1(λ),
(5.397)
where the coefﬁcients Ak, Bk, Ck
: R →Rn×n are piecewise continuously
differentiable in λ, Bk(λ) and Ck(λ) are symmetric, and I −Ak(λ) is invertible
with ˜Ak(λ) := [I −Ak(λ)]−1. The symmetric Hamiltonian Hk(λ) is deﬁned by
Hk(λ) :=
−Ck(λ) AT
k (λ)
Ak(λ) Bk(λ)

and it obeys the monotonicity condition (5.337), i.e.,
˙Hk(λ) ≥0,
λ ∈R,
k ∈[0, N]Z.
Recall that problem (5.144), (5.397) is the most important special case of the
symplectic eigenvalue problem (E), where Sk(λ) has the form (5.145), i.e.,
Sk(λ) =

[I −Ak(λ)]−1
[I −Ak(λ)]−1Bk(λ)
Ck(λ) [I −Ak(λ)]−1 Ck(λ) [I −Ak(λ)]−1Bk(λ) + I −AT
k (λ)

.

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
377
In the previous sections, we have proved generalized versions of the discrete
oscillation theorems for (E) with the nonconstant rank of Bk(λ). This approach gives
a possibility to compute the number of ﬁnite eigenvalues of (E) between arbitrary
points a, b ∈R with a ≤b by using the information on the distribution of jumps of
the piecewise constant function rankBk(λ) for k ∈[0, N]Z in (a, b]. It is clear that
obtaining such an information for problems (E) of big dimension n ≫1 imposes
considerable computational difﬁculties.
In this section we offer an approach how to avoid these difﬁculties for the special
case (5.144), (5.397). For this purpose we introduce a new notion of weighted focal
points for a conjoined basis of (5.144). As it is shown in Sect. 5.6.1, the classical
function n1(λ) of the number of forward focal points in (0, N + 1] can lose the
monotonic character with respect to λ if (5.38) is not satisﬁed. We offer to introduce
a monotonic, but in general indeﬁnite by sign, function #(λ) of the number of
weighted focal points in (0, N +1] such that the quantity #(b)−#(a) represents the
number of ﬁnite eigenvalues of (5.144), (5.397) in (a, b] for arbitrary a, b ∈R with
a ≤b. We prove modiﬁed versions of Theorems 5.95 and 5.98, which refer to this
new notion of weighted focal points.
Based on the deﬁnition of forward focal points and its multiplicities (see
Deﬁnition 4.1), we introduce the number of weighted focal points as follows.
Deﬁnition 5.111 A conjoined basis Y(λ) of the linear Hamiltonian difference
system (5.144) has a weighted (forward) focal point in (k, k + 1] if
m(Yk(λ)) −ind Bk(λ) ̸= 0.
In this case the number of weighted (forward) focal points in (k, k + 1] is deﬁned as
the quantity
#k(λ) = #(Yk(λ)) := m(Yk(λ)) −ind Bk(λ),
(5.398)
where m(Yk(λ)) is given by (4.3).
Note that we have the estimate for m(Yk(λ)) (see Proposition 4.4(v))
0 ≤m(Yk(λ)) ≤rankBk(λ),
and then by the inequality ind Bk(λ) ≤rank Bk(λ), we derive that
|#(Yk(λ))| ≤rank Bk(λ) ≤n.
(5.399)
In the following example, we illustrate Deﬁnition 5.111 for the scalar case n = 1.
This example also shows that the quantity #(Yk(λ)) can be negative.

378
5
Discrete Symplectic Eigenvalue Problems
Example 5.112 Consider system (5.144) for the case n = 1 with Ak(λ) ≡0. Then
the number of weighted focal points takes values from the set {0, ±1} and
#(Yk(λ)) =
⎧
⎪⎨
⎪⎩
1,
if Bk(λ) > 0, xk(λ) ̸= 0, xk(λ) xk+1(λ) ≤0,
−1, if Bk(λ) < 0, xk+1(λ) ̸= 0, xk(λ) xk+1(λ) ≤0,
0,
otherwise.
Let Y(λ) = (X(λ), U(λ)) be a conjoined basis of (5.144) such that (5.20) holds.
We denote the number of weighted focal points of Y(λ) in (0, N + 1] by
#(λ) :=
N

k=0
#k(λ).
(5.400)
The main result of this section is the following modiﬁcation of the local oscillation
theorem (see Theorem 5.95) for problem (5.144), (5.397) dealing with the number
of weighted focal points of a conjoined basis Y(λ) of (5.144).
Theorem 5.113 (Generalized Local Oscillation Theorem)
Suppose that (5.337)
holds and Y(λ) = (X(λ), U(λ)) is a conjoined basis of (5.144) satisfying (5.20).
Then the left- and right-hand limits #k(λ−) and #k(λ+) exist for all k ∈[0, N]Z and
λ ∈R, and
#k(λ+) = #k(λ),
#k(λ+) −#k(λ−) = rankXk(λ) −rankXk(λ−)
+ rankXk+1(λ−) −rankXk+1(λ).
⎫
⎪⎬
⎪⎭
(5.401)
Moreover, with the notation in (5.400), we have
#(λ+) = #(λ),
|#(λ)| ≤n(N + 1),
#(λ+) −#(λ−) = rank XN+1(λ−) −rankXN+1(λ).

(5.402)
Proof Note that monotonicity condition (5.337) implies ˙Bk(λ) ≥0, and then all
eigenvalues μi(λ) for i ∈{1, 2, . . . , n} of Bk(λ) = BT
k (λ) are real nondecreasing
functions of λ. The function ind μi(λ) of the scalar argument μi(λ) is piecewise
constant, nonincreasing, and right-continuous, then so is the function ind Bk(λ)
for k ∈[0, N]Z. Similarly, the function ind [−μi(λ)] of the scalar argument
−μi(λ) is piecewise constant, nondecreasing, and left-continuous, so is the function
ind [−Bk(λ)] for k ∈[0, N]Z. Hence, for any λ ∈R, there exist the following left-
hand and right-hand limits
ind Bk(λ+) = ind Bk(λ),
ind [−Bk(λ−)] = ind [−Bk(λ)],
(5.403)
ind Bk(λ−) = rankBk(λ−) −rankBk(λ) + ind Bk(λ),
(5.404)
ind [−Bk(λ+)] = rankBk(λ+) −rankBk(λ) + ind [−Bk(λ)].
(5.405)

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
379
By Theorem 5.95 proved for (E) with the nonconstant rank of Bk(λ), we have that
(see (5.330))
mk(λ+) = mk(λ) ≤n,
mk(λ+) −mk(λ−) + rankBk(λ−) −rankBk(λ)
= rankXk(λ) −rank Xk(λ−) + rank Xk+1(λ−) −rankXk+1(λ),
where rank Bk(λ) = rank Bk(λ) according to (5.145). Then, by (5.403) and (5.330),
the function #k(λ) is right-continuous, and the left-hand side of the second equality
in (5.330) can be replaced by #k(λ+) −#k(λ−) according to (5.404). The proof
of (5.401) is completed. Note that the inequality in (5.402) follows from (5.399)
and the deﬁnition of #(λ). Summing (5.401) for k ∈[0, N]Z and using that Yk(λ)
does not depend on λ for k = 0, we complete the proof of (5.402).
⊓⊔
Note that (5.337) also implies that there exist the ﬁnite limits
lim
λ→−∞ind Bk(λ) = ck < ∞,
k ∈[0, N]Z,
i.e., there exists λ00 such that
ind Bk(λ) = ck,
N

k=0
ind Bk(λ) =
N

k=0
ck =: C,
for λ < λ00
(5.406)
(we use again that ind Bk(λ) is monotonic and bounded). Repeating the proof of
Theorem 5.16, we conclude by (5.402) that #(λ) is bounded and nondecreasing for
all λ ∈R, so that there exists λ0 such that
#(λ) ≡p,
p = m −C,
n1(λ) ≡m,
for λ < λ0.
(5.407)
Finally, we see by (5.407) and (5.402) that
rank XN+1(λ) = rank XN+1(λ−),
for λ < λ0,
(5.408)
i.e., the ﬁnite spectrum of (5.144), (5.397) is bounded from below (see also
Remark 5.97(ii)).
Recall the notation
n2(λ) :=

μ≤λ
θ(μ)
for the number of ﬁnite eigenvalues of (5.144), (5.397) less than or equal to λ. As
a corollary to Theorem 5.113, we derive the following modiﬁcation of Theorem 5.98
for the special case (5.144), (5.397).

380
5
Discrete Symplectic Eigenvalue Problems
Theorem 5.114 (Generalized Global Oscillation Theorem) Assume that (5.337)
holds. Then the ﬁnite spectrum of (5.144), (5.397) is bounded from below, and there
exists p ∈{0, ±1, . . ., ±n(N + 1)} such that
n2(λ) + p = #(λ)
for all λ ∈R,
(5.409)
where p is given by (5.407).
Proof Recall again that all break points of rank XN+1(λ) are isolated, then using
that #(λ) is right-continuous we derive for a, b ∈R with a < b that
#(b) −#(a) =

μ∈(a,b]
θ(μ),
(5.410)
where the sum 
μ∈(a,b] θ(μ) is ﬁnite. The proof of (5.409) follows from (5.410) for
b := λ, where we evaluate the ﬁnite limits of the quantities #(a) and 
μ∈(a,λ] θ(μ)
as a →−∞according to (5.407) and (5.408).
⊓⊔
Corollary 5.115 Assume (5.337) and suppose that the functions ind Bk(λ) for k ∈
[0, N]Z are constant in λ ∈R (depending on k), i.e., (5.406) holds for all λ ∈R.
Then under the notation of Theorem 5.114 we have that there exists m ∈[0, nN]Z
such that
n2(λ) + m = n1(λ)
for all λ ∈R,
(5.411)
where m is given by (5.407).
The next example shows that n1(λ) is generally nonmonotonic in the setting of
this section. However, the addition of the “correction term” −ind Bk, according
to (5.398), implies that under (5.337) the function #(λ) is monotonic in λ.
Example 5.116 Consider problem (5.144), (5.397) for n = 1 and N = 3 with the
Hamiltonian
Hk(λ) = diag{λ, λ −k −1},
for which condition (5.337) is obviously satisﬁed. By (5.145), the matrix of the
associated symplectic system is
Sk(λ) =
 1
λ −k −1
−λ
−λ2 + (k + 1) λ + 1

.

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
381
The upper components of the principal solution at k = 0 are
x[0]
0 (λ) = 0,
x[0]
1 (λ) = λ −1,
x[0]
2 (λ) = −λ3 + 3λ2 −3,
x[0]
3 (λ) = λ5 −6λ4 + 7λ3 + 10λ2 −11λ −6,
x[0]
4 (λ) = −λ7 + 10λ6 −29λ5 + 5λ4 + 69λ3 −20λ2 −50λ −10.
The ﬁnite eigenvalues are the roots of the equation x[0]
4 (λ) = 0, i.e.,
λ1 ≈−1.0710,
λ2 ≈−0.6565,
λ3 ≈−0.2422,
λ4 ≈1.4278,
λ5 ≈2.7005,
λ6 ≈3.5417,
λ7 ≈4.2998.
The ﬁrst graph of Fig. 5.1 illustrates nonmonotonic character of the number n1(λ)
of forward focal points in (0, 4] evaluated according to (5.49) in the interval λ ∈
[−1.3, 4.5]. The second graph of Fig. 5.1 shows the number of weighted focal points
in (0, 4] computed according to Example 5.112. We can see that the function #(λ)
is nondecreasing with respect to λ with the jump discontinuities located in the zeros
λi for i ∈{1, . . ., 7} of the function x[0]
4 (λ) (see the last graph of Fig. 5.1).
−1
0
1
2
3
4
0
1
2
3
number of focal points
−1
0
1
2
3
4
−4
−2
0
2
4
number of weighted focal points
−1
0
1
2
3
4
−50
0
50
function x4()
Fig. 5.1 The graphs of Example 5.116

382
5
Discrete Symplectic Eigenvalue Problems
5.6.6
General Endpoints and Nonconstant Rank
In this subsection we present generalizations of the results of Sect. 5.2.3 without the
assumptions (5.210), i.e., without
rank Bk(λ) is constant for λ ∈R for all k ∈[0, N]Z
rankR0(λ) and rank RN+1(λ) are constant for λ ∈R

in the global oscillation theorem for separated endpoints (Theorem 5.50). Or, for the
more general case, we omit assumptions (5.226), i.e.,
rank Bk(λ) is constant for λ ∈R for all k ∈[0, N]Z
rankR2(λ) is constant for λ ∈R

used in Theorem 5.55 for the joint endpoints.
Under the notation of Sect. 5.2.3, we consider the eigenvalue problem (5.1) with
general boundary conditions. First we consider the separated endpoints (5.155).
Recall that we use the notation ¯Y(λ) = ( ¯X(λ), ¯U(λ)) for the natural conjoined
basis of system (5.1) with the initial conditions (5.204)
¯X0(λ) = −RT
0 (λ),
¯U0(λ) = R∗T
0 (λ),
λ ∈R.
and the auxiliary matrices (5.205)
(λ) := R∗
N+1(λ) ¯XN+1(λ) + RN+1(λ) ¯UN+1(λ),
M(λ) := [I −(λ) †(λ)] RN+1(λ),
T (λ) := I −M†(λ) M(λ),
P(λ) := T (λ) ¯XN+1(λ) †(λ) RN+1(λ) T (λ).
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
Then, according to (5.207), (5.208), and (5.209), we introduce the numbers
n1(λ) := number of forward focal points of ¯Y(λ) in (0, N + 1],
(5.412)
p(λ) := rankM(λ) + ind P(λ).
(5.413)
The consideration is based on Theorem 5.41 about the equivalence between
problem (5.1), (5.155) and the extended problem (5.162). According to Theo-
rem 5.41, the matrices S−1(λ) and SN+1(λ) given by (5.171) and (5.173) obey the
monotonicity assumption (5.163). Then we consider the numbers
ϑ−1(λ0) := rankR0(λ−
0 ) −rank R0(λ0),
ϑN+1(λ0) := rankRN+1(λ−
0 ) −rank RN+1(λ0),

(5.414)

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
383
which count the jumps of the rank of the matrices R0(λ) and RN+1(λ) in the
boundary conditions (5.155). Moreover, one can apply the local oscillation theorem
with nonconstant rank (see Theorem 5.95) to the principal solution Y [−1]
k
(λ) for
k ∈[−1, N + 1]Z of problem (5.162) and then incorporate the connection between
Y [−1]
k
(λ) for k ∈[−1, N + 1]Z and ¯Y(λ) = ( ¯X(λ), ¯U(λ)) given by (5.216). By
analogy with the proof of Theorem 5.50, we derive the following result.
Theorem 5.117 (Local Oscillation Theorem for Separated Endpoints) Assume
that (5.3), (5.156), (5.157), and (5.158) hold. Let ¯Y(λ) be the natural conjoined
basis of (SDSλ) with (5.204) and mk(λ) for k ∈[0, N]Z are the multiplicities of
forward focal points of ¯Y(λ). Then mk(λ−) and mk(λ+) exist for all λ ∈R and
satisfy (5.330) for k ∈[0, N]Z. Moreover, for k ∈{−1, N + 1}, we have
ϑ−1(λ) = rankX0(λ−) −rank X0(λ)
(5.415)
and
p(λ+) = p(λ),
p(λ+) −p(λ−) + ϑN+1(λ) = rankXN+1(λ) −rankXN+1(λ−)
+ rank(λ−) −rank(λ),
⎫
⎪⎬
⎪⎭
(5.416)
where p(λ) is given by (5.209). Instead of (5.331), we have
n1(λ+) = n1(λ) ≤n(N + 1),
n1(λ+) −n1(λ−) + p(λ+) −p(λ−) +
N+1

k=−1
ϑk(λ)
= rank (λ−) −(λ) = θ(λ),
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.417)
where θ(λ) is given by (5.206).
Proof For the proof it is sufﬁcient to repeat the proof of Theorem 5.50 replacing
Theorems 5.15 and 5.16 by Theorem 5.95 formulated for the nonconstant rank of
Bk(λ). In particular, we incorporate that rank X[−1]
N+2(λ) = rank (λ),
m(Y [−1]
−1 (λ−)) = m(Y [−1]
−1 (λ)) = m(Y [−1]
−1 (λ+)) = 0,
m(Y [−1]
k
(λ)) = m( ¯Yk(λ)),
k ∈[0, N]Z,
m(Y [−1]
N+1(λ)) = p(λ)
which completes the proof.
⊓⊔

384
5
Discrete Symplectic Eigenvalue Problems
To formulate the global oscillation theorem, we assume as in the previous
sections that there exists λ00 < 0 such that
rankBk(λ) = rankBk(λ−)
for all λ < λ00,
k ∈[0, N]Z,
for the blocks of the symplectic matrix Sk(λ). In addition, we introduce a similar
assumption for the blocks of the separated boundary conditions (5.155)
rankR0(λ) = rankR0(λ−)
for all λ < λ00,
rankRN+1(λ) = rankRN+1(λ−)
for all λ < λ00.

(5.418)
Based on Corollary 5.96, we prove a necessary and sufﬁcient condition for
the boundedness from below the spectrum of problem (5.1), (5.155) with the
nonconstant rank of R0(λ) and RN+1(λ).
Corollary 5.118 Assume that (5.3), (5.156), (5.157), and (5.158) hold. Then
conditions (5.333) and (5.418) are satisﬁed if and only if the ﬁnite spectrum of (5.1),
(5.155) is bounded from below.
Proof The proof is based on Theorem 5.41 about the equivalence between prob-
lem (5.1), (5.155) and the extended problem (5.162). Then it is sufﬁcient to apply
Corollary 5.96 to the extended problem, where the blocks of S−1(λ) and SN+1(λ)
are deﬁned by the matrices in (5.155) according to (5.171) and (5.173).
⊓⊔
Under assumptions (5.333), (5.418), we introduce the notation
n2(λ) := number of ﬁnite eigenvalues of (5.1), (5.155)
in the interval (−∞, λ],
n ˜B(λ) :=

μ≤λ
N+1

k=−1
ϑk(μ),
(5.419)
where ϑk(μ) for k ∈[0, N + 1]Z are deﬁned by (5.329) and (5.414). Note that
conditions (5.3), (5.333), (5.418), and formula (5.417) imply the existence of ˜λ ∈R
such that
n2(λ) ≡0,
n ˜B(λ) ≡0,
n1(λ) + p(λ) ≡ℓ
for all λ < ˜λ.
(5.420)
Moreover, the functions n2(λ) and n ˜B(λ) are right-continuous by Theorems 5.3, 5.1
and Corollary 5.7. From Corollary 5.118 and Theorem 5.117, we derive the
following generalization of the global oscillation theorem (Theorem 5.50).

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
385
Theorem 5.119 (Global Oscillation Theorem for Separated Endpoints)
Assume (5.3), (5.156), (5.157), (5.158), (5.333), and (5.418). Then there exists
a number ℓ∈[0, (N + 2) n]Z such that
n1(λ) + p(λ) + n ˜B(λ) = n2(λ) + ℓ
for all λ ∈R,
(5.421)
where ℓis given by (5.420).
Proof The proof is based on Theorem 5.117, and it repeats the proof of Theo-
rem 5.98. The details are therefore omitted.
⊓⊔
Remark 5.120 Note that the number n ˜B(λ) given by (5.419) can be presented in the
form
n ˜B(λ) = nB(λ) + nL(λ) + nR(λ),
(5.422)
where
nL(λ) :=

μ≤λ
ϑ−1(μ),
(5.423)
nR(λ) :=

μ≤λ
ϑN+1(μ),
(5.424)
where nB(λ) is deﬁned by (5.338). These three numbers characterize the contri-
bution of Sk(λ) for k ∈[0, N]Z and the left and the right boundary condition,
respectively, to the spectrum of problem (5.1), (5.155). Point out that the matrices
Sk(λ) for k ∈[0, N]Z and R0(λ) and RN+1(λ) (and then also the quantities
nB(λ), nL(λ), and nR(λ)) are known, while the numbers n1(λ) and p(λ) have to
be computed using the conjoined basis ¯Y(λ).
In the following example, we analyze the main results of this subsection for
a discrete Sturm-Liouville eigenvalue problem with linear and quadratic dependence
on λ in the boundary conditions (5.155). This analysis is motivated by the results in
[150, 166, 167].
Example 5.121 Consider the Sturm-Liouville eigenvalue problem
−(rkxk) + qkxk+1 = λwkxk+1,
rk ̸= 0,
wk > 0,
k ∈[0, N −1]Z,
R∗
0(λ) x0 + R0(λ) (r0x0) = 0,
R∗
N+1(λ) xN+1 + RN+1(λ) (rNxN) = 0.

(5.425)
Recall that according to Example 5.35, problem (5.425) can be rewritten in
form (5.1) (or (SDSλ)), (5.155) after the replacement uk := rkxk, k ∈[0, N]Z,
uN+1 := uN, with the same matrices in the boundary conditions. For the case
n = 1, conditions (5.157)–(5.158) mean that the function in the left- hand side
is nonnegative, resp., nonpositive, for all λ ∈R, while (5.156) mean that the pair

386
5
Discrete Symplectic Eigenvalue Problems
of functions R∗
j (λ) and Rj(λ) for j ∈{0, N + 1} do not have common zeros.
In particular, if R∗
j (λ) and Rj(λ) are polynomials, then one can use the resultant
res (R∗
j (λ), Rj(λ)) (or the eliminant) of two polynomials (see [152]), which is
a polynomial expression of their coefﬁcients, to formulate a necessary and sufﬁcient
condition for the validity of (5.156). Note that we exclude the situation when
R∗
j (λ) ≡0 (or Rj(λ) ≡0) from the consideration, because in this case the boundary
conditions in (5.425) can be reduced to be independent on λ (after dividing by
Rj(λ) ̸= 0 for λ ∈R or, respectively, after dividing by R∗
j (λ) ̸= 0 for λ ∈R).
For example, for the general quadratic dependence on λ
R∗
j (λ) = a∗
j λ2+b∗
jλ+c∗
j,
Rj(λ) = ajλ2+bjλ+cj,
j ∈{0, N+1}
(5.426)
(including the general linear dependence for a∗
j = 0 = aj) conditions (5.157)–
(5.158) take the form
(−1)j/(N+1) (−mjλ2 + 2 lj λ −hj) ≥0,
mj := b∗
jaj −a∗
j bj,
lj := a∗
j cj −c∗
jaj,
hj := c∗
jbj −b∗
jcj,
j ∈{0, N + 1},
⎫
⎪⎬
⎪⎭
(5.427)
while condition (5.156) holds if and only if the resultant res (R∗
j (λ), Rj(λ)) ̸= 0,
λ ∈R (see [152]). In particular, for the linear dependence (a∗
j = 0 = aj), we
have res (R∗
j (λ), Rj(λ)) = −hj for the case bj ̸= 0 and b∗
j ̸= 0, and similarly
res (R∗
j (λ), Rj(λ)) = cj (or res (R∗
j (λ), Rj(λ)) = c∗
j) for the case bj = 0 and
b∗
j ̸= 0 (or b∗
j = 0 and bj ̸= 0). Therefore, the condition (−1)j/(N+1) hj < 0 for
j ∈{0, N + 1} is necessary and sufﬁcient for (b∗
j)2 + b2
j > 0, (5.427), and (5.156)
(compare with the main assumptions in [150, 166]). For the quadratic case with
aj ̸= 0 and a∗
j ̸= 0, the resultant takes the form res (R∗
j (λ), Rj(λ)) = mjhj −l2
j .
Therefore, we can see that in this case, the condition (−1)j/(N+1)Mj < 0 for
j ∈{0, N + 1}, where Mj :=
 hj lj
lj mj

, is necessary and sufﬁcient for (5.427),
(5.156). The situation when aj = 0 or a∗
j = 0 can be investigated separately by
using the general deﬁnition of res (R∗
j (λ), Rj(λ)) (see [152] for more details). In
any case one can use (−1)j/(N+1)Mj < 0 for j ∈{0, N + 1} as a sufﬁcient
condition for (5.427) and (5.156); see [167, Lemmas 4.6 and 4.7]. Regarding the
eigenvalues of problem (5.425), it was proven in [150, 166] that for the linear case
(a∗
j = 0 = aj) and under the assumptions rk > 0, (5.156), and (5.157)–(5.158), the
problem (5.425) has a ﬁnite number of real simple eigenvalues. The same property
is proven in [167] for the case (5.426) with the boundary conditions x0 = 0 and
R∗
N+1(λ) xN+1 + RN+1(λ) (rNxN) = 0, where MN+1 > 0.
Consider now the eigenvalue problem (5.425) with the quadratic dependence in
λ in the boundary conditions
−2xk −2xk+1 = λxk+1,
k ∈[0, N −1]Z,
x0 = 0,
(λ2 + λ −1) xN+1 −λxN = 0,

(5.428)

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
387
where the parameters lN+1 = 0, mN+1 = hN+1 = 1 given by (5.427) obey the
condition MN+1 > 0. Then by [167] the problem (5.428) has a ﬁnite number of
real simple eigenvalues. By putting uk := xk+1 and yk = (xk, uk)T , we rewrite this
problem in the form of (5.1), (5.155), i.e.,
yk+1 =
 0
1
−1 −λ

yk,
k ∈[0, N −1]Z,
x0 = 0,
λxN + (λ2 −1) uN = 0.
⎫
⎬
⎭
(5.429)
Note that assumptions (5.155), (5.156) hold for (5.429), and then consider the
application of Theorem 5.119 to this problem for the case N = 3. The principal
solution y[0](λ) of (5.429) at k = 0 and the function (λ) in (5.205) have the form
y[0]
0 (λ) =
0
1

,
y[0]
1 (λ) =
 1
−λ

,
y[0]
2 (λ) =

−λ
−1 + λ2

,
y[0]
3 (λ) =
−1 + λ2
2λ −λ3

,
(λ) = λ(λ2 −1) (3 −λ2).
In this case we have the real simple eigenvalues
λ1 = −
√
3,
λ2 = −1,
λ3 = 0,
λ4 = 1,
λ5 =
√
3.
We compute the multiplicities of focal points of the principal solution y[0](λ) by
m0(λ) = 0,
m1(λ) =
1, λ ∈[0, ∞),
0, λ ∈(−∞, 0),
m2(λ) =
 1, λ ∈[−1, 0) ∪[1, ∞),
0, otherwise,
and the number p(λ) given by (5.413) as follows
p(λ) =
1, λ ∈[−
√
3, −1) ∪[0, 1) ∪[
√
3, ∞),
0, otherwise.
Then we have
n1(λ) + p(λ) =
⎧
⎪⎪⎨
⎪⎪⎩
0, λ ∈(−∞, −
√
3),
1, λ ∈[−
√
3, 0),
2, λ ∈[0,
√
3),
3, λ ∈[
√
3, ∞),

388
5
Discrete Symplectic Eigenvalue Problems
and we can see that the sum n1(λ)+p(λ) does not calculate the eigenvalues λ2 = −1
and λ4 = 1. However, according to Theorem 5.119, we need to incorporate the
inﬂuence of the nonconstant rank of the coefﬁcient λ2 −1 in the right endpoint
N + 1. For this case we have by (5.424)
nB(λ) ≡0,
nL(λ) ≡0,
n ˜B(λ) = nR(λ) =
⎧
⎨
⎩
0, λ ∈(−∞, −1),
1, λ ∈[−1, 1),
2, λ ∈[1, ∞).
Then, by Theorem 5.119 with ℓ:= 0, the sum n1(λ) + p(λ) + n ˜B(λ) determines all
the eigenvalues of (5.429), i.e.,
n2(λ) = n1(λ) + p(λ) + n ˜B(λ) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
0, λ ∈(−∞, −
√
3),
1, λ ∈[−
√
3, −1),
2, λ ∈[−1, 0),
3, λ ∈[0, 1),
4, λ ∈[1,
√
3),
5, λ ∈[
√
3, ∞).
Now we consider the generalization of Theorem 5.55 for joint endpoints to the
case when assumptions (5.226) do not hold. Recall the notation (see (5.220))
Xk(λ) :=

I
0
˜Xk(λ) X[0]
k (λ)

,
Uk(λ) :=

0
−I
˜Uk(λ) U[0]
k (λ)

,
where Y [0](λ) is the principal solution of (5.1) at k = 0 and, by (5.221),
L(λ) := R1(λ) XN+1(λ) + R2(λ) UN+1(λ),
M(λ) := [I −L(λ) L†(λ)] R2(λ),
T (λ) := I −M†(λ) M(λ),
P(λ) := T (λ) XN+1(λ) L†(λ) R2(λ) T (λ).
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
The multiplicity ζ(λ0) of ﬁnite eigenvalue of problem (SDSλ), (5.151) is presented
by Deﬁnition 5.54.
The consideration is based on Theorem 5.44, which guarantees that under
assumptions (5.2), (5.3), (5.152), and (5.153), problem (SDSλ), (5.151) is equivalent
to the augmented problem (5.178) or (5.232) (see the proof of Theorem 5.55). Note
that problem (5.232) obeys all assumptions of the local oscillation theorem for the
separated endpoints (see Theorem 5.117). Moreover, we again use that
rank ˜Bk(λ) = rank diag{0, Bk(λ)} = rankBk(λ),
k ∈[0, N]Z,

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
389
where ˜Bk(λ) is the block of {Sk(λ)} (see (5.200)). Then the deﬁnition of the numbers
˜ϑk(λ0) for k ∈[0, N]Z for the augmented problem (5.232) stays the same as before
(see (5.329)), since
˜ϑk(λ0) = rank diag{0, Bk(λ−
0 )} −rank diag{0, Bk(λ0)}
= rankBk(λ−
0 ) −rank Bk(λ0) = ϑk(λ0),
k ∈[0, N]Z.

(5.430)
As the boundary condition for k = 0 does not depend on λ, we have according
to (5.414) applied to (5.232) that
˜ϑ−1(λ0) = 0,
˜ϑN+1(λ0) = rankR2(λ−
0 ) −rank R2(λ0).

(5.431)
Introduce the notation (see (5.223) and (5.225))
n1(λ) := number of forward focal points of Y [0](λ) in (0, N + 1],
q(λ) := rankM(λ) + ind P(λ).
Then Theorem 5.117 applied to augmented problem (5.232) reads as follows.
Theorem 5.122 (Local Oscillation Theorem for Joint Endpoints)
Assume (5.2),
(5.3), (5.152), and (5.153). Let Y [0](λ) be the principal solution at k = 0 of (SDSλ),
and mk(λ) for k ∈[0, N]Z are the multiplicities of focal points of Y [0](λ). Then
mk(λ−) and mk(λ+) exist for all λ ∈R and satisfy (5.330) for all k ∈[0, N]Z.
Moreover, for k = N + 1, we have
q(λ+) = q(λ),
q(λ+) −q(λ−) + ˜ϑN+1(λ) = rankXN+1(λ) −rankXN+1(λ−)
+ rankL(λ−) −rank L(λ),
⎫
⎪⎬
⎪⎭
(5.432)
where q(λ) is given by (5.225). Instead of (5.417), we have
n1(λ+) = n1(λ) ≤n(N + 1),
n1(λ+) −n1(λ−) + q(λ+) −q(λ−) +
N+1

k=0
˜ϑk(λ)
= rank L(λ−) −L(λ) = ζ(λ),
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(5.433)
where ζ(λ) is given by (5.222).

390
5
Discrete Symplectic Eigenvalue Problems
To formulate the global oscillation theorem, we replace assumptions (5.418) by
the more general assumption for joint endpoints
rank R2(λ) = rankR2(λ−)
for all λ < λ00.
(5.434)
Using the notion of ﬁnite eigenvalue of problem (SDSλ), (5.151) and condi-
tions (5.333) and (5.434), we prove an analog of Corollary 5.118 for joint endpoints.
Corollary 5.123 Under assumptions (5.2), (5.3), (5.152), and (5.153), condi-
tions (5.333) and (5.434) hold if and only if the ﬁnite spectrum of problem (SDSλ),
(5.151) is bounded from below.
Proof We apply Corollary 5.118 to augmented problem (5.178) or (5.232) replac-
ing (5.418) by (5.434).
⊓⊔
Introduce the notation
n2(λ) := number of ﬁnite eigenvalues of (5.1), (5.151) in (−∞, λ],
˜n ˜B(λ) :=

μ≤λ
N+1

k=0
˜ϑk(μ),
(5.435)
where ˜ϑk(λ) are given by (5.430) and (5.431). Note that under assumptions (5.3),
(5.333), and (5.434) instead of (5.420) for the separated endpoints, we have for some
˜λ ∈R
n2(λ) ≡0,
˜n ˜B(λ) ≡0,
n1(λ) + q(λ) ≡ℓ
for all λ < ˜λ,
(5.436)
where we apply (5.433). Based on Corollary 5.123 and Theorem 5.122, we derive
a generalization of Theorem 5.55 without assumptions (5.226).
Theorem 5.124 (Global Oscillation Theorem for Joint Endpoints)
Assume
that (5.2), (5.3), (5.152), (5.153), (5.333), and (5.434) hold. Then there exists
a number ℓ∈[0, (N + 2) n]Z such that
n1(λ) + q(λ) + ˜n ˜B(λ) = n2(λ) + ℓ
for all λ ∈R,
(5.437)
where ℓis given by (5.436).
Proof The proof repeats the proof of Theorem 5.55, where instead of Theorem 5.50,
we apply Theorem 5.119.
⊓⊔
Remark 5.125 For the case when the endpoints are separated, formula (5.437) turns
into (5.421). Indeed, in this case ˜n ˜B(λ) = n ˜B(λ) and by (5.218)
q(λ) = μ
 ˜VN+1(λ)⟨Z[0]
N+1(λ)⟩, ˜VN+1(λ) (02n I2n)T 
,
(5.438)

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
391
where the symplectic matrix ˜VN+1(λ) is associated with the boundary condition
in (5.232) at N + 1 by (5.173). For the case of separated endpoints, it is easy to
verify by direct calculations that
˜VN+1(λ) = {V −1
−1 (λ), VN+1(λ)},
(5.439)
where V−1(λ), VN+1(λ) are given by (5.171) and (5.173) for separated boundary
conditions (5.155) (remark that we use notation (3.101)). Then it is possible to prove
the relation
q(λ) = μ
 ¯YN+1(λ), Y [0]
N+1(λ)

+ p(λ),
λ ∈R,
(5.440)
which holds for q(λ) and p(λ) given by (5.209) and (5.225) for the case of the
separated boundary conditions. Here ¯YN+1(λ) is the conjoined basis of (5.1) given
by (5.204). Here we present the outline of the rather technical (direct) proof of this
result based on algebraic properties of the comparative index derived in Sect. 3.3.5.
Applying (5.438) and (5.439), we have by (3.105)
q(λ) = μ

⟨VN+1(λ)Z[0]
N+1V−1(λ)⟩, {V −1
−1 (λ), VN+1(λ)}(02n I2n)T 
,
where we applied Theorem 3.5(i). Next, by (3.116)
μ⟨VN+1(λ) Z[0]
N+1(λ) V−1(λ)⟩, {V −1
−1 (λ), VN+1(λ)}(02n I2n)T 
= μ∗
V −1
−1 (λ) [Z[0]
N+1(λ)]−1(0 I)T, V −1
−1 (λ) (0 I)T 
+ μVN+1(λ) ¯YN+1(λ), VN+1(λ) (0 I)T 
= μ
 ¯YN+1(λ), Y [0]
N+1(λ)

+ p(λ),
where we use Theorem 3.5(iii), (i) and the deﬁnition of p(λ) according to (5.218).
Applying the Sturmian separation result (see formula (4.60))
l( ¯Y(λ), 0, N + 1) −l(Y [0](λ), 0, N + 1) = μ
 ¯YN+1(λ), Y [0]
N+1(λ)

,
it is easy to see that (5.437) indeed turns into (5.421) for the separated endpoints.
Remark 5.126 The periodic and antiperiodic boundary conditions are included
in (5.155) for the special choice of constant matrices R1(λ) and R2(λ), as we
discussed in Remark 5.58 in Sect. 5.2.3. In both cases (5.236) and (5.237), the
matrices R2(λ) are constant in λ, so that by (5.430) and (5.431) the function ˜n ˜B(λ)
in (5.436) coincides with the function nB(λ) in (5.338). Recall now from (5.371)

392
5
Discrete Symplectic Eigenvalue Problems
the symplectic and orthogonal matrix
R :=
1
√
2
⎛
⎜⎜⎝
0 −I I
0
0
I
I
0
−I
0 0 −I
−I
0 0 I
⎞
⎟⎟⎠.
which was used in Sect. 5.6.3. Then the matrix ˜SN+1(λ) ∈Sp(4n) associated with
the boundary condition (5.236) at N + 1 of the augmented problem (5.178) can be
taken in the form ˜SN+1(λ) ≡˜SN+1 := RT . For this choice of ˜SN+1, the number
q(λ) given by (5.438) for the periodic boundary conditions (5.236) takes the form
(see the ﬁrst equality in formula (5.377) and (3.104))
q(λ) = q1(λ) := μ

RT ⟨Z[0]
N+1(λ)⟩, RT (0 I)T 
= μ
 
L1(λ)
J T L2(λ)

, RT (0 I)T 
≤n.
⎫
⎬
⎭
(5.441)
Moreover, consider a generalized version of the periodic and antiperiodic boun-
dary conditions (5.236) and (5.237) in the form
y0(λ) = ˆS(λ) yN+1(λ),
(5.442)
where ˆS(λ) is a given symplectic piecewise continuously differentiable matrix for
λ ∈R satisfying ( ˆS(λ)) ≥0, where ( ˆS(λ)) is given by (5.3). The problem (5.1),
(5.442) is then equivalent to the following extended problem with the periodic
endpoints
yk+1(λ) = Sk(λ) yk(λ),
k ∈[0, N + 1]Z,
SN+1(λ) := ˆS(λ),
y0(λ) = yN+2(λ),

(5.443)
where the matrix Sk(λ) obeys (5.3) for k ∈[0, N + 1]Z. Applying the above results
for (5.236) to problem (5.443), we get
L(λ) = J I −ˆS(λ) Z[0]
N+1(λ).
Here we used that Z[0]
N+2(λ) =
ˆS(λ) Z[0]
N+1(λ), while q(λ) given by (5.438) for
conditions (5.442) takes the form
q(λ) = μ

RT ⟨Z[0]
N+2(λ)⟩, RT (0 I)T 
+ μ

Y [0]
N+2(λ), ˆS(λ) (0 I)T 
= μ
 
L(λ)
I+ ˆS(λ) Z[0]
N+1(λ)

, RT (0 I)T

+ μ

ˆS(λ) Y [0]
N+1(λ), ˆS(λ) (0 I)T 
,

5.6
Oscillation Theorems for Variable Rank of Bk(λ)
393
where we incorporated the multiplicity μ

Y [0]
N+2(λ), ˆS(λ) (0 I)T 
of the focal point
of the principal solution of (5.443) at N + 1 according to (4.14) in Lemma 4.7. In
particular, for the antiperiodic boundary conditions (5.237), we have ˆS(λ) := −I
in (5.443), and then for this case
q(λ) = q2(λ) := μ

RT ⟨−Z[0]
N+1(λ)⟩, RT (0 I)T 
= μ
 
L2(λ)
J T L1(λ)

, RT (0 I)T 
≤n,
⎫
⎬
⎭
(5.444)
where L1(λ) and L2(λ) are given in (5.236) and (5.237) and where we used
Theorem 3.5(i) in the calculations.
Example 5.127 Consider problem (5.1), (5.151) in the form
yk+1(λ) =
I −Pkλ2 λI
−λPk
I

yk(λ),
k ∈[0, N]Z,
yN+1(λ) = G y0(λ),
(5.445)
where G is a constant symplectic matrix. It was proven in [165, Section 3] that under
the assumptions Pk symmetric, Pk ≥0, and N
k=0 Pk > 0, problem (5.445) is self-
adjoint, and the width of the central zone of stability of a discrete linear Hamiltonian
system associated with (5.445) can be estimated by using the properties of the
eigenvalues of this problem. Note that problem (5.445) satisﬁes all the assumptions
of Theorem 5.124, and then this theorem together with Remarks 5.58 and 5.126 can
be applied for the calculation of the eigenvalues of (5.445).
For example, consider the case n = 1, Pk = 1, G = I2, and N = 1. Then
the symplectic fundamental matrix of the symplectic system in (5.445) and L1(λ)
in (5.236) have the form
Z[0]
0 (λ) ≡I, Z[0]
1 (λ) =
I −λ2 λI
−λI
I

, Z[0]
2 (λ) =
λ4 −3λ2 + 1 2λ −λ3
−2λ + λ3
1 −λ2

,
L1(λ) =
 −2λ + λ3
−λ2
−λ4 + 3λ2 −2λ + λ3

,
with det L1(λ) = λ2(2 −λ) (2 + λ). Then λ2 = 0 has the multiplicity θ(λ2) = 2,
while the eigenvalues λ1 = −2 and λ3 = 2 are simple. The multiplicities mk(λ) =
mk(Y [0](λ)), k ∈[0, 1]Z of focal points of Y [0](λ) and the function ˜n ˜B(λ) = nB(λ)
given by (5.435) and (5.338) calculating the zeros of Bk(λ) = λ for k ∈[0, 1]Z are
m0(λ) = 0, m1(λ) =
⎧
⎪⎪⎨
⎪⎪⎩
0, λ ∈(−∞, −
√
2),
1, λ ∈[−
√
2, 0),
0, λ ∈[0,
√
2),
1, λ ∈[
√
2, ∞),
nB(λ) =
 0, λ ∈(−∞, 0),
2, λ ∈[0, ∞).

394
5
Discrete Symplectic Eigenvalue Problems
It follows that
m1(λ) + nB(λ) =
⎧
⎪⎪⎨
⎪⎪⎩
0, λ ∈(−∞, −
√
2),
1, λ ∈[−
√
2, 0),
2, λ ∈[0,
√
2),
3, λ ∈[
√
2, ∞),
q(λ) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
0, λ ∈(−∞, −2),
1, λ ∈[−2, −
√
2),
0, λ ∈[−
√
2, 0),
1, λ ∈[0,
√
2),
0, λ ∈[
√
2, 2),
1, λ ∈[2, ∞),
where by Theorem 5.98 the sum m1(λ) + nB(λ) calculates the eigenvalues of
problem (5.445) with the Dirichlet boundary conditions (i.e., the zeros of the
polynomial X[0]
2 (λ) = 2λ−λ3). On the other hand, for the calculation of the number
of eigenvalues of (5.445) according to Theorem 5.124, we have to add the number
q(λ) evaluated according to (5.441) and the deﬁnition of the comparative index (see
Deﬁnition 3.1). Finally, we have
n2(λ) = m1(λ) + nB(λ) + q(λ) =
⎧
⎪⎪⎨
⎪⎪⎩
0, λ ∈(−∞, −2),
1, λ ∈[−2, 0),
3, λ ∈[0, 2),
4, λ ∈[2, ∞),
where n2(λ) is the number of the eigenvalues (including multiplicities) of (5.445)
less than or equal to the given λ in the accordance with Theorem 5.124.
5.7
Notes and References
The oscillation theorems for symplectic eigenvalue problems were ﬁrst considered
in [55, 102] for systems with linear dependence on λ and the Dirichlet boundary
conditions. More precisely, in [55], a certain ﬁnite exceptional set of values λ was
excluded from the statement of the main result, while in [102] all values λ ∈R were
covered. This issue was also closely related to the introduction of the multiplicities
of (forward) focal points in [208], as shown in Deﬁnition 4.1. Extension of the
oscillation theorems in [102] to separated boundary conditions via a transformation
to the Dirichlet boundary conditions was derived in [55, 103].
Nonlinear dependence on the spectral parameter in symplectic difference sys-
tems, as it is presented in Sect. 5.1, was introduced in [297]. There the oscillation
theorems were proven under the condition that the coefﬁcient Bk(λ) ≡Bk is
constant in λ on R, which was also the case in [102]. This assumption was weakened
to the constant Im Bk(λ) ≡R in λ in [298] for the scalar second-order Sturm-
Liouville difference equations (SLλ) with (1.30) and to an arbitrary constant image
of Bk(λ) in λ in [210] for general symplectic difference systems with Dirichlet
boundary conditions. Discrete oscillation theorems for eigenvalue problems with

5.7
Notes and References
395
separated or general boundary conditions (under Im Bk(λ) constant in λ ∈R) were
obtained in [305] for systems with linear dependence on λ and in [301] for systems
with nonlinear dependence on λ.
Finally, oscillation theorems without condition Im Bk(λ) constant in λ for prob-
lem (E) under monotonicity assumption (5.3) were proven in [125, Theorem 2.4].
It particular, it was shown there that assumption (5.3) implies that rankBk(λ),
Im Bk(λ), and Ker Bk(λ) are piecewise constant in λ (see [125, Remark 3.6]).
Generalizations of these oscillation theorems to symplectic difference systems with
general boundary conditions, which also incorporate possible oscillations in the
rank of the coefﬁcients in the system and in the boundary conditions, were obtained
recently in [133].
The treatment of discrete symplectic eigenvalue problems with linear dependence
on λ both in the system and in the boundary conditions as presented in Sect. 5.3
is motivated by the results for linear Hamiltonian differential systems (1.103) in
[205, Section 2.2]. The positivity result in Proposition 5.67 was proven in [103,
Proposition 2]. The Rayleigh principle for symplectic eigenvalue problems with
Dirichlet boundary conditions (Theorem 5.83 and Corollary 5.84) was proven in
[56, Theorem 4.6]. Extensions of this result to separated or joint boundary condi-
tions (constant in λ) were obtained in [103, Theorem 2] and [305, Theorems 3.2],
respectively. In this respect the results with Rayleigh principle for symplectic
eigenvalue problems with separated and joint boundary conditions depending
(linearly) on λ (in Theorems 5.85 and 5.88 as well as in Corollaries 5.86 and 5.87)
are also new in the literature. The Sturmian comparison and separation theorems
obtained in Sect. 5.5.1 via the Rayleigh principle are from [56].
It should be noted that linear Hamiltonian system (5.144) with nonlinear
dependence on λ was ﬁrst studied in [44, 264] under a strict normality assumption,
which corresponds to the strict monotonicity of the matrix Hk(λ), and further in
[210, 297] under (5.337) and the restriction that Im [ ˜Ak(λ) Bk(λ)] is constant in
λ ∈R (see Example 5.37). This condition is equivalent to rank Bk(λ) constant in
λ ∈R by Theorem 5.1(iii). Later, in [95], the last condition was completely omitted.
There are several other topics in the spectral theory of discrete systems, which
were studied within symplectic difference systems (SDS) or linear Hamiltonian
difference systems (2.15). In this context we wish to mention ﬁrst the literature
regarding the discrete Weyl-Titchmarsh theory, i.e., the theory of square summable
solutions of symplectic difference systems depending on λ ∈C. The study in
this direction was initiated in [60, 67] for symplectic difference systems (5.238)
with special linear dependence on the spectral parameter λ, i.e., for the matrix
Sk(λ) in (5.239) with (5.241). Weyl-Titchmarsh theory for symplectic difference
systems with general linear dependence on λ was developed in [308–310, 313]
and with polynomial and analytic dependence on λ in [89, 90, 311, 312]. The
results in [310, 313] show an interesting phenomenon in the discrete time theory,
where the limit circle invariance (and the Weyl alternative) holds without any
additional assumption in comparison with the corresponding continuous time result.
Extensions of several classical concepts from the theory of square summable
solutions for symplectic difference systems to linear relations were presented in

396
5
Discrete Symplectic Eigenvalue Problems
[68, 335]. Properties of eigenvalues and the spectrum of symplectic difference
systems were studied in [120, 122].
In the literature, there are many studies about the spectral properties of linear
Hamiltonian difference systems (5.144). Those are special symplectic difference
systems as we showed in Example 5.37. Below we review the relevant literature
regarding selected spectral properties of (5.144) with various dependence on λ
(often linear in λ). Stability zones were studied in [242–244]. Spectral properties
of system (5.144) on bounded interval was studied in [264] and on unbounded
interval in [66, 221, 252, 253, 265, 271, 272, 275, 277]. Spectral properties of linear
subspaces associated with system (5.144) were studied in [251, 269]. Further results
from spectral theory of (5.144) were obtained in [33, 44, 144, 235, 261, 337].
We recommend the following additional related references for further reading
about the topics presented in this chapter: [31, 199, 273, 274, 276] for discrete
Sturm-Liouville eigenvalue problems and [18–20, 29, 72, 73, 158–161, 197, 198,
217, 218, 228–231, 233, 240, 260, 278, 320, 325, 329] for applications of symplectic
systems, symplectic algorithms, and related numerical analysis.

Chapter 6
Miscellaneous Topics on Symplectic
Systems
In this chapter we will present some additional topics from the theory of symplectic
difference systems (SDS), which are closely related to their oscillation or spectral
theory. These topics cover the relative and renormalized oscillation theorems for
symplectic eigenvalue problems with nonlinear dependence on spectral parameter
and the theory of symplectic difference systems, which do not impose any controlla-
bility assumption. The latter one includes, in particular, a general theory of recessive
and dominant solutions at ∞(as a generalization of Sect. 2.5) and their applications
in the singular Sturmian theory (as an extension and completion of Sect. 4.2.4).
6.1
Relative Oscillation Theory
Relative oscillation theory—rather than measuring the spectrum of one single
problem—measures the difference between the spectra of two different problems.
This is done by replacing focal points of conjoined bases of one problem by
matrix analogs of weighted zeros of Wronskians of conjoined bases of two different
problems. In this section we develop the relative oscillation theory for symplectic
boundary value problems.
We consider two symplectic eigenvalue problems which may differ both in the
coefﬁcient matrix of the system and also in the boundary conditions. Together
with problem (5.1), (5.151), we consider the problem in the same form for
system (5.318), i.e.,
ˆyk+1(λ) = ˆSk(λ) ˆyk(λ),
k ∈[0, N]Z,
(6.1)
© Springer Nature Switzerland AG 2019
O. Došlý et al., Symplectic Difference Systems: Oscillation and Spectral Theory,
Pathways in Mathematics, https://doi.org/10.1007/978-3-030-19373-7_6
397

398
6
Miscellaneous Topics on Symplectic Systems
where matrix ˆSk(λ) is symplectic for every λ ∈R and
ˆSk(λ) =

ˆAk(λ) ˆBk(λ)
ˆCk(λ) ˆDk(λ)

,
(6.2)
With (6.1) we consider boundary conditions of the same form as (5.151), that is,
ˆR1(λ)
 ˆx0(λ)
ˆxN+1(λ)

+ ˆR2(λ)
 −ˆu0(λ)
ˆuN+1(λ)

= 0,
(6.3)
where ˆR1(λ) and ˆR2(λ) are piecewise continuously differentiable matrix-valued
functions such that
ˆR1(λ), ˆR2(λ) ∈R2n×2n,
rank( ˆR1(λ) ˆR2(λ)) = 2n,
ˆR1(λ) ˆRT
2 (λ) = ˆR2(λ) ˆRT
1 (λ),
λ ∈R.

(6.4)
By analogy with (5.3) and (5.153), we impose the monotonicity assumptions
( ˆSk(λ)) := J ˙ˆSk(λ) J ˆST
k (λ) J ≥0,
k ∈[0, N]Z,
λ ∈R.
(6.5)
and, using the notation ˆR(λ) := ( ˆR1(λ) ˆR2(λ)) ∈R2n×4n,
˙ˆR1(λ) ˆRT
2 (λ) −˙ˆR2(λ) ˆRT
1 (λ) = ˙ˆR(λ) J4n ˆRT (λ) ≤0.
(6.6)
We will investigate the pair of eigenvalue problems (5.1), (5.151) and (6.1), (6.3)
and their particular cases which deal with separated and the Dirichlet boundary
conditions.
The fundamental role in our treatment is played by the relative oscillation
numbers introduced in Chap. 4 (see Deﬁnition 4.44) for systems (SDS) and (4.98)
and by Theorem 4.56 for the principal solutions of these systems. For this reason we
return in this chapter back to notation (4.10) and (4.11) from Chap. 4. In particular,
we use
l(Y, M, N + 1) :=
N

k=M
m(Yk),
l∗(Y, M, N + 1) :=
N

k=M
m∗(Yk)
(6.7)
for the numbers of forward and backward focal points of Y in (M, N + 1] and
[M, N + 1), instead of using n1(Y(λ)) and n∗
1(Y(λ)) as in Chap. 5. Moreover, we
introduce the notation
#{ν ∈σ | ν ∈I},
#{ν ∈ˆσ | ν ∈I}
(6.8)

6.1
Relative Oscillation Theory
399
for the number of ﬁnite eigenvalues of problems (5.1), (5.151) and (6.1), (6.3),
where σ and ˆσ are the ﬁnite spectra of these problems, respectively, and I ⊆R.
In particular, we have
n2(λ) = #{ν ∈σ | ν ≤λ},
where n2(λ) denotes the number of ﬁnite eigenvalues of (5.1), (5.151) (including
special cases) less than or equal to a given λ under the notation in Chap. 5.
6.1.1
Sturm-Liouville Difference Equations
In this subsection we brieﬂy recall the results of G. Teschl and his collaborators
in [22, 314, 315] concerning the relative oscillation and spectral theory for discrete
Sturm-Liouville eigenvalue problems.
Consider a pair of discrete Sturm-Liouville eigenvalue problems
−(rkxk) + pkxk+1 = λxk+1,
x0 = 0 = xN+1
(6.9)
and
−(rkˆxk) + ˆpkxk+1 = λˆxk+1,
ˆx0 = 0 = ˆxN+1
(6.10)
with rk > 0. Introduce the Wronskian for solutions x, ˆx of (6.9), (6.10)
wk(x, ˆx) = −rk(xk ˆxk+1 −xk+1 ˆxk) = −rk[xkˆxk −(xk) ˆxk].
(6.11)
In accordance with [314], Wronskian (6.11) has a generalized zero in the interval
[k, k + 1) if either wkwk+1 < 0 or wk = 0 and wk+1 ̸= 0. Then, by [314,
Theorem 4.3], in case pk = ˆpk for any a < b it holds
#(x[0](a), ˆx[N+1](b)) = #{λ ∈σ | a < λ < b},
(6.12)
where #(x, ˆx) denotes the number of generalized zeros of wk(x, ˆx) in (0, N + 1)
and #{λ ∈σ | a < λ < b} is the number of eigenvalues of (6.9) between a and b.
Here x[0](λ) and ˆx[N+1](λ) are the principal solutions of (6.9) (or (6.10)) at k = 0
and k = N + 1, respectively, i.e., the nontrivial solutions satisfying x[0]
0
= 0 and
ˆx[N+1]
N+1
= 0.
The main result of [22] extends (6.12) to the case pk ̸=
ˆpk. Assume that
ˆx[N+1](λ) is the principal solution of (6.10) at k = N + 1. According to [22,
Theorem 1.2], the number of weighted generalized zeros of the Wronskian on
(0, N + 1) equals the difference of the number of eigenvalues of (6.10) less than

400
6
Miscellaneous Topics on Symplectic Systems
b and the number of eigenvalues (6.9) less than or equal to a,
#(x[0](a), ˆx[N+1](b)) = #(x[N+1](a), ˆx[0](b))
= #{ν ∈ˆσ | ν < b} −#{ν ∈σ | ν ≤a}.
(6.13)
The concept of weighted zero of the Wronskian is deﬁned as follows.
Deﬁnition 6.1 Denote
qk(a, b) := pk −ˆpk + b −a.
(6.14)
The Wronskian wk(x, ˆx) has a weighted zero at k if #k(x, ˆx) = ±1, where
#k(x, ˆx) :=

1,
qk(a, b) > 0, wkwk+1 ≤0, wk+1 ̸= 0,
−1, qk(a, b) < 0, wk+1wk ≤0, wk ̸= 0,
(6.15)
and #k(x, ˆx) := 0 otherwise. The number of weighted zeros in (0, N + 1) is given
by the formula
#(x, ˆx) :=
N

k=0
#k(x, ˆx) −

0, if w0(x, ˆx) ̸= 0,
1, if w0(x, ˆx) = 0.
(6.16)
Note that in case pk = ˆpk and b > a the above deﬁnition reduces to the deﬁnition
of the generalized zero of the Wronskian in [k, k +1) and the number of generalized
zeros in (0, N + 1).
6.1.2
Dirichlet Boundary Value Problems
In this subsection we consider systems (5.1) and (6.1) together with the Dirichlet
boundary conditions
yk+1(λ) = Sk(λ) yk(λ), k ∈[0, N]Z, λ ∈R, x0(λ) = 0 = xN+1(λ),
ˆyk+1(λ) = ˆSk(λ) ˆyk(λ), k ∈[0, N]Z, λ ∈R, ˆx0(λ) = 0 = ˆxN+1(λ),

(6.17)
We assume the monotonicity conditions (5.3) and (6.5) and suppose that (5.38) hold
for both matrices Sk(λ) and ˆSk(λ) for k ∈[0, N]Z, i.e.,
rankBk(λ) and rank ˆBk(λ) are constant for λ ∈R and k ∈[0, N]Z.
(6.18)
The global oscillation theorems established for problem (E) (see Theorem 5.17)
relates the number of ﬁnite eigenvalues of (E) less than or equal to a given number

6.1
Relative Oscillation Theory
401
λ := λ1 to the number of focal points (counting multiplicity) of the principal
solution of the difference system in (E) with λ = λ1. Our aim is to add a new
aspect to this classical result by showing that matrix analogs of weighted zeros of
the Wronskian for two suitable matrix solutions of the difference systems in (6.17)
can be used to count the difference between the number of ﬁnite eigenvalues of
problems (6.17).
Note that problems (6.17) under conditions (5.3), (6.5), and (6.18) obey all
assumptions of Theorem 5.17, and then the ﬁnite spectra σ and ˆσ are bounded,
and we can order the ﬁnite eigenvalues of (6.17) into nondecreasing sequences
−∞< λ1 ≤λ2 ≤· · · ≤λm < +∞,
−∞< ˆλ1 ≤ˆλ2 ≤· · · ≤ˆλl < +∞.

(6.19)
We put λ1 = ∞(ˆλ1 = ∞) if σ = ∅(ˆσ = ∅). We also deﬁne
λ0 = min{λ1, ˆλ1}.
(6.20)
Recall that according to Theorem 5.17
l(Y [0](a), 0, N + 1) = #{ν ∈σ | ν ≤a} + m,
l( ˆY [0](b), 0, N + 1) = #{ν ∈ˆσ | ν ≤b} + ˆm,

(6.21)
where Y [0](λ) and ˆY [0](λ) are the principal solutions of the symplectic systems
in (6.17) at 0, and where
m := l(Y [0](λ), 0, N + 1),
ˆm = l( ˆY [0](λ), 0, N + 1),
λ < λ0.
(6.22)
Then we have from (6.21) and (6.22) that
#{ν ∈ˆσ| ν ≤b} −#{ν ∈σ | ν ≤a}
= l( ˆY [0](b), 0, N + 1) −l(Y [0](a), 0, N + 1) −( ˆm −m),

(6.23)
where
ˆm −m = l( ˆY [0](λ), 0, N + 1) −l(Y [0](λ), 0, N + 1),
λ < λ0.
(6.24)
Applying Theorem 4.56 connecting the differences of the multiplicities of focal
points on the right-hand sides of (6.23) and (6.24) with the relative oscillation
numbers (see Deﬁnition 4.44), we prove the following central results.
Theorem 6.2 (Relative Oscillation Theorem)
Assume that (5.3), (6.5), (6.18)
hold for systems (5.1) and (6.1). Let Z[0](λ) and ˆZ[N+1](λ) be symplectic funda-
mental matrices of these systems associated with the principal solutions Y [0](λ) =
Z[0](λ) (0 I)T and ˆY [N+1](λ) = ˆZ[N+1](λ) (0 I)T at k = 0 and k = N + 1,

402
6
Miscellaneous Topics on Symplectic Systems
respectively. Deﬁne the relative oscillation numbers according to (4.143) (with
ˆZ[N+1] := ˆZ[N+1](b), Z[M] := Z[M](a), M := 0), i.e.,
#k( ˆZ[N+1](b), Z[0](a)) := μ

⟨Gk(a, b)⟩, ⟨Gk+1(a, b)⟩

−μ

⟨ˆSk(b)⟩, ⟨Sk(a)⟩

(6.25)
for k ∈[0, N]Z, where
Gk(a, b) := [ ˆZ[N+1]
k
(b)]−1Z[0]
k (a).
Then there exists a constant P ∈[−nN, nN]Z such that for any a, b ∈R we have
for the spectra σ and ˆσ of problems (6.17)
#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤a} = #( ˆZ[N+1](b), Z[0](a), 0, N) −P,
(6.26)
where
#( ˆZ[N+1](b), Z[0](a), 0, N) =
N

k=0
#k( ˆZ[N+1](b), Z[0](a))
(6.27)
and, with m, ˆm, and λ0 given by (6.22) and (6.20),
P = ˆm −m = #( ˆZ[N+1](λ), Z[0](λ), 0, N),
λ < λ0.
(6.28)
Proof For the proof we use Theorem 4.56 (see (4.107)) and connections (6.23)
and (6.24) derived above.
⊓⊔
Remark 6.3 Recall that according to Lemma 4.55 the relative oscillation num-
bers (6.25) can be speciﬁed at the endpoints k = 0 and k = N as follows:
#0( ˆZ[N+1](b), Z[0](a)) = μ∗ ˆY [N+1]
0
(b), ˆS−1
0 (b) S0(a) (0 I)T 
−μ
 ˆS0(b) (0 I)T, S0(a) (0 I)T 
,
(6.29)
#N( ˆZ[N+1](b), Z[0](a)) = μ
 ˆSN(b) Y [0]
N (a), ˆSN(b) S−1
N (a) (0 I)T 
−μ∗ ˆS−1
N (b) (0 I)T, S−1
N (a) (0 I)T 
.
(6.30)
In particular, under assumptions (5.3), (6.5) for the case S0(λ) ≡ˆS0(λ) (respec-
tively, for SN(λ) ≡
ˆSN(λ)) we have that μ ˆS0(b) (0 I)T, S0(a) (0 I)T  = 0
(respectively, μ∗ ˆS−1
N (b) (0 I)T, S−1
N (a) (0 I)T  = 0); see the proof of Theorem 6.4
below.
For the case when Sk(λ) ≡
ˆSk(λ) for k ∈[0, N]Z, we have the so-called
renormalized oscillation theorem for (E), which is a corollary to Theorem 6.2.

6.1
Relative Oscillation Theory
403
Theorem 6.4 (Renormalized Oscillation Theorem) Assume that conditions (5.3)
and (6.18) hold for system (5.1). Let Z[0](λ) and Z[N+1](λ) be symplectic fun-
damental matrices of (5.1) such that Y [0](λ) = Z[0](λ) (0 I)T and Y [N+1](λ) =
Z[N+1](λ) (0 I)T are the principal solutions of this system at k = 0 and k = N +1,
respectively. Then for any a, b ∈R with a < b the number of ﬁnite eigenvalues of
problem (E) in (a, b] is given by the formula
#{ν ∈σ | a < ν ≤b} =
N

k=0
μ

⟨Gk(a, b)⟩, ⟨Gk+1(a, b)⟩

,
Gk(a, b) := [Z[N+1]
k
(b)]−1Z[0]
k (a).
⎫
⎪⎬
⎪⎭
(6.31)
Moreover, with λ1 = min σ we have
#{ν ∈σ | ν ≤b} =
N

k=0
μ

⟨Gk(λ, b)⟩, ⟨Gk+1(λ, b)⟩

,
Gk(λ, b) := [Z[N+1]
k
(b)]−1Z[0]
k (λ),
λ < λ1.
⎫
⎪⎬
⎪⎭
(6.32)
Proof Remark that for Sk(λ) ≡
ˆSk(λ) for k ∈[0, N]Z, we have according to
Lemma 5.105 that μ

⟨Sk(b)⟩, ⟨Sk(a)⟩

= 0 for k ∈[0, N]Z, where assump-
tion (6.18) plays a key role. Moreover, for the case Sk(λ) ≡ˆSk(λ) for k ∈[0, N]Z,
the left-hand side of (6.26) takes the form of the left-hand side of (6.31), while the
constant P = 0 by (6.28). Then the proof of (6.31) follows from (6.25) and (6.26).
Formula (6.32) is derived from (6.31) by taking the limit a →−∞and using that
the spectrum of (E) under conditions (5.3) and (6.18) is bounded.
⊓⊔
Remark 6.5
(i) Note that by (4.148) in Theorem 4.56, we have
#( ˆZ[N+1](b), Z[0](a), 0, N) = −#(Z[N+1](a), ˆZ[0](b), 0, N).
Then one can replace the relative oscillation numbers in (6.26) by
−#(Z[N+1](a), ˆZ[0](b), 0, N) = −
N

k=0
#k(Z[N+1](a), ˆZ[0](b)),
where, according to Deﬁnition 4.44, instead of (6.25) we have
#k(Z[N+1](a), ˆZ[0](b))
= μ

⟨˜Gk(a, b)⟩, ⟨˜Gk+1(a, b)⟩

−μ

⟨Sk(a)⟩, ⟨ˆSk(b)⟩

= μ

⟨ˆSk(b)⟩, ⟨Sk(a)⟩

−μ

⟨˜Gk+1(a, b)⟩, ⟨˜Gk(a, b)⟩

,
⎫
⎪⎬
⎪⎭
(6.33)

404
6
Miscellaneous Topics on Symplectic Systems
with
˜Gk(a, b) := [Z[N+1]
k
(a)]−1 ˆZ[0]
k (b).
In particular, this reformulation leads to the representation
#{ν ∈σ | a < ν ≤b} =
N

k=0
μ⟨˜Gk+1(a, b)⟩, ⟨˜Gk(a, b)⟩,
˜Gk(a, b) := [Z[N+1]
k
(a)]−1Z[0]
k (b),
⎫
⎪⎬
⎪⎭
(6.34)
which is equivalent to (6.31).
(ii) We recall that according to Remark 4.46(i) the value of the comparative
index μ⟨˜Gk+1(a, b)⟩, ⟨˜Gk(a, b)⟩ in (6.33) and (6.34) presents the multiplic-
ities of forward focal points of the transformed 4n-dimensional (Wronskian)
system (4.112) associated with (5.1) and (6.1). Similarly, the comparative index
μ⟨Gk(a, b)⟩, ⟨Gk+1(a, b)⟩ in (6.26) and (6.31) is equal to the multiplicity of
backward focal points of system (4.115) (see Remark 4.46(iii)). Recall also
that by Remark 4.46(ii), (iv) both comparative indices are presented by (4.113)
and (4.116), where the ﬁrst addends are associated with transformed 2n-
dimensional (Wronskian) systems (4.114) and (4.117), i.e., with the oscillations
of the Wronskian of Y(λ) and ˆY(λ). In subsequent sections we will present
some special cases, when formulas (6.25) and (6.33) can be simpliﬁed by using
the special structure of the symplectic coefﬁcient matrices Sk(λ) and ˆSk(λ). In
particular, in these cases they are associated with the oscillation of conjoined
bases of the 2n-dimensional Wronskian systems (4.114) and (4.117).
6.1.3
Lower Block-Triangular Perturbation
In this subsection we investigate the most closely related matrix analog of the
results for Sturm-Liouville problems (6.9), (6.10). Consider the pair of eigenvalue
problems (6.17) with the Dirichlet boundary conditions, whose coefﬁcient matrices
obey the conditions
Ak(λ) = ˆAk(λ) ≡Ak,
Bk(λ) = ˆBk(λ) ≡Bk,
k ∈[0, N]Z.
(6.35)
Then we have that conditions (6.18) are satisﬁed and under assumptions (5.3)
and (6.5) one can apply Theorems 6.2 and 6.4 to this special case. It follows
from (6.35) that for arbitrary ﬁxed λ := β we have
Sk(λ) S−1
k (β) =

I
0
Qk(λ, β) I

,
ˆSk(λ) ˆS−1
k (β) =

I
0
ˆQk(λ, β) I

,

6.1
Relative Oscillation Theory
405
or (compare with (5.239) where β = 0)
Sk(λ) =

I
0
Qk(λ, β) I

Sk(β),
ˆSk(λ) =

I
0
ˆQk(λ, β) I

ˆSk(β),
(6.36)
where the symmetric matrices Qk(λ, β) and ˆQk(λ, β) (see Sect. 1.6.1) can be
expressed in terms of the blocks of Sk(a) and ˆSk(b) as
Qk(λ, β) = Ck(λ) DT
k (β) −Dk(λ) CT
k (β) = Pk(λ) J PT
k (β),
Pk(λ) :=

Ck(λ) Dk(λ)

,
ˆQk(λ, β) = ˆCk(λ) ˆDT
k (β) −ˆDk(λ) ˆCT
k (β) = ˆPk(λ) J ˆPT
k (β),
ˆPk(λ) :=

ˆCk(λ) ˆDk(λ)

.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(6.37)
Moreover, for two arbitrary values of λ = a and λ = b, we have
Sk(a) ˆS−1
k (b) =

I
0
Qk(a, b) I

,
(6.38)
where the symmetric matrix Qk(a, b) = QT
k (a, b) is given by
Qk(a, b) = Ck(a) ˆDT
k (b) −Dk(a) ˆCT
k (b) = Pk(a) J ˆPT
k (b)
= Qk(a, β) −ˆQk(b, β) + Pk(β) J ˆPT
k (β),

(6.39)
with Qk(λ, β) and ˆQk(λ, β) deﬁned by (6.37). In particular,
Qk(a, b) = Pk(a) J PT
k (b) = Qk(a, β) −Qk(b, β),
for the case Sk(λ) ≡ˆSk(λ),
k ∈[0, N]Z.

(6.40)
The matrices (Sk(λ)) and ( ˆSk(λ)) in (5.3) and (6.5) for the case (6.35) take the
form (see (1.193))
(Sk(λ)) = diag{ ˙Pk(λ) J T PT
k (λ), 0},
( ˆSk(λ)) = diag{ ˙ˆPk(λ) J T ˆPT
k (λ), 0}.
Then the monotonicity conditions will be satisﬁed under the assumption
˙Pk(λ) J PT
k (λ) = ˙Qk(λ, β) ≤0,
˙ˆPk(λ) J ˆPT
k (λ) = ˙ˆQk(λ, β) ≤0,

k ∈[0, N]Z,
λ ∈R.
(6.41)

406
6
Miscellaneous Topics on Symplectic Systems
For the special linear dependence on λ (see (5.239) and (5.240)), we then have
Sk(λ) =

I
0
−λWk I
 Ak Bk
Ck Dk

,
ˆSk(λ) =

I
0
−λ ˆWk I
 Ak Bk
ˆCk
ˆDk

,
(6.42)
so that the matrices Sk(λ) and ˆSk(λ) obey (6.35), (6.38) for any a, b ∈R, and
Qk(a, b) = b ˆWk −aWk + Ck ˆDT
k −Dk ˆCT
k .
(6.43)
In particular,
Qk(a, b) = (b −a)Wk
for Sk(λ) ≡ˆSk(λ),
k ∈[0, N]Z.
(6.44)
For the scalar case of two Sturm-Liouville difference equations (6.9) and (6.10),
which are rewritten in the matrix form (2.36), we then have
Sk(λ) =

1
1/rk
pk −λ 1 + (pk −λ)/rk

,
ˆSk(λ) =

1
1/rk
ˆpk −λ 1 + ( ˆpk −λ)/rk

.
(6.45)
Then, with the number qk(a, b) from Deﬁnition 6.1,
Qk(a, b) = b −a + pk −ˆpk = qk(a, b).
Recall that, as we mentioned above, the relative oscillation theory for prob-
lems (6.17) under assumption (6.35) is the most closely related analog of the
results for Sturm-Liouville problems (6.9) and (6.10). Assume that Z(a) and ˆZ(b)
are symplectic fundamental matrices of systems (5.1) and (6.1) associated with
conjoined bases Y(λ) and ˆY(λ) of these systems according to (3.14). Then we have
the connections (see (4.100))
w( ˆYk(b), Yk(a)) = −(I 0) ˆZ−1
k (b) Yk(a),
w(Yk(a), ˆYk(b)) = −(I 0) Z−1
k (a) ˆYk(b),
w( ˆYk(b), Yk(a)) = −wT (Yk(a), ˆYk(b)).
⎫
⎪⎬
⎪⎭
(6.46)
We will show that for the case (6.35) the role of generalized zeros of the Wronskian
is played by focal points of the transformed conjoined bases
ˆZ−1
k (b) Yk(a),
Z−1
k (a) ˆYk(b)
of the discrete symplectic systems (4.114) and (4.117) rewritten in the form
˜Yk+1 = ˆZ−1
k+1(b) Sk(a) ˆS−1
k (b) ˆZk+1(b) ˜Yk,
˜Yk = ˆZ−1
k (b) Yk(a),
(6.47)
¯Yk+1 = Z−1
k+1(a) ˆSk(b) S−1
k (a) Zk+1(a) ¯Yk,
¯Yk = Z−1
k (a) ˆYk(b).
(6.48)

6.1
Relative Oscillation Theory
407
Assume (6.35), then by (6.38) the blocks Bk(a, b) and ˜Bk(a, b) in the right
upper corner of the symplectic matrices in (6.47) and (6.48) have the form
Bk(a, b) = −ˆXT
k+1(b) Qk(a, b) ˆXk+1(b),
˜Bk(a, b) = XT
k+1(a) Qk(a, b) Xk+1(a),

(6.49)
while systems (6.47) and (6.48) take the form
−J  ˜Yk = ˆZT
k+1(b) diag{−Qk(a, b), 0} ˆZk+1(b) ˜Yk,
˜Yk = ˆZ−1
k (b) Yk(a),
(6.50)
−J  ¯Yk = ZT
k+1(a) diag{Qk(a, b), 0} Zk+1(a) ¯Yk,
¯Yk = Z−1
k (a) ˆYk(b).
(6.51)
For systems (5.1) and (6.1) under restriction (6.35), the formulas for the relative
oscillation numbers in (6.26) and (6.31) are simpliﬁed as follows.
Lemma 6.6 Suppose that the matrices S(λ) and ˆS(λ) in (5.1) and (6.1) sat-
isfy (6.35) and that Z(λ) and ˆZ(λ) are symplectic fundamental matrices of (5.1)
and (6.1) associated with conjoined bases Y(λ) and ˆY(λ) of (5.1) and (6.1) such
that
Z(λ) (0 I)T = Y(λ) =
X(λ)
U(λ)

,
ˆZ(λ) (0 I)T = ˆY(λ) =
 ˆX(λ)
ˆU(λ)

.
Then the relative oscillation numbers (4.109) are given by the formulas
#k( ˆZ(b), Z(a)) = m∗(Z−1
k (a) ˆYk(b)) −ind ˜Bk(a, b)
= ind Bk(a, b) −m( ˆZ−1
k (b) Yk(a)),

(6.52)
where the matrices Bk(a, b) and ˜Bk(a, b) are given by (6.49), and where the quan-
tities m∗(Z−1
k (a) ˆYk(b)) and m( ˆZ−1
k (a) Yk(b)) represent the number of backward
focal points in [k, k + 1) and the number of forward focal points in (k, k + 1] of
the conjoined bases Z−1(a) ˆY(b) and ˆZ−1(a) Y(b) of systems (6.51) and (6.50),
respectively. In addition, we have the estimate
|#k( ˆZ(b), Z(a))| ≤min
(
rankBk(a, b), rank ˜Bk(a, b)
)
≤n.
(6.53)
Proof The matrices Sk(λ) and ˆSk(λ) in (5.1) and (6.1) satisfy (6.38), and then
assumption (4.107) of Lemma 4.43 is also satisﬁed. Using (4.108) and incorpo-
rating (4.16) in Lemma 4.8, we see that
μ ˆZ−1
k (b) Zk(a) (0 I)T, ˆZ−1
k+1(b) Zk+1(a) (0 I)T  = m∗(Z−1
k (a) ˆYk(b))

408
6
Miscellaneous Topics on Symplectic Systems
(compare with Remark 4.46(iii)). So we obtain the proof of the ﬁrst identity
in (6.52).
To prove the second identity, we use (4.118). Indeed, replacing the roles Y and ˆY
in the ﬁrst (already proved) identity (6.52), we obtain by (4.118) that
#k( ˆZ(b), Z(a)) =  rankw(Yk(a), ˆYk(b)) −#k(Z(a), ˆZ(b))
=  rankw(Yk(a), ˆYk(b)) −m∗( ˆZ−1
k (b) Yk(a))
+ ind [−ˆXT
k+1(b) Qk(a, b) ˆXk+1(b)]
= ind [−ˆXT
k+1(b) Qk(a, b) ˆXk+1(b)] −m( ˆZ−1
k (b) Yk(a)),
where, in addition, Proposition 4.4(vi) (see (4.9)) is used.
For the proof of estimate (6.53), we incorporate that the matrices Bk(a, b)
and ˜Bk(a, b) given by (6.49) are the blocks of the symplectic matrices in (6.47)
and (6.48) in the right upper corner. Then by Proposition 4.4(v)
m∗(Z−1
k (a) ˆYk(b)) ≤rank ˜Bk(a, b),
m( ˆZ−1
k (b) Yk(a)) ≤rankBk(a, b).
Taking in mind that ind ˜Bk(a, b)
≤
rank ˜Bk(a, b) and ind Bk(a, b)
≤
rankBk(a, b), we derive estimate (6.53). The proof is complete.
⊓⊔
As it was mentioned above, it is possible to relate the number (6.52) and the
notion of a weighted generalized zero of the Wronskian for solutions of second-
order Sturm-Liouville difference equations (6.9) and (6.10) presented in Sect. 6.1.1.
Proposition 6.7 The Wronskian of two nontrivial solutions xk(a) and ˆxk(b) of (6.9)
and (6.10) has a weighted generalized zero according to Deﬁnition 6.1 if and only
if the relative oscillation number deﬁned in Lemma 6.6 for n = 1 with Qk(a, b) :=
qk(a, b) takes the value ±1.
Proof Note that by (6.52) #k( ˆZ(b), Z(a)) = 1 if and only if m∗(Z−1
k (a) ˆYk(b)) = 1
and ind ˜Bk(a, b) = 0. According to Remark 4.14(iii), the case m∗(Z−1
k (a) ˆYk(b)) =
1 is equivalent to the existence of a generalized zero in [k, k + 1) of the solution
Z−1
k (a) ˆYk(b) for n = 1. Using connection (6.46) we have that the Wronskian
wk := w(Yk, ˆYk) is associated with the upper block of the solution Z−1
k (a) ˆYk(b)
(up to the sign, which is not important). Then by Remark 4.14(iii), we have that
m∗(Z−1
k (a) Yk(b) = 1 if and only if wk = 0, wk+1 ̸= 0, or wkwk+1 ˜Bk(a, b) < 0,
where ˜Bk(a, b) = x2
k+1qk(a, b) according to (6.49). Then, under the assump-
tion qk(a, b) > 0, the last condition is equivalent with wkwk+1 < 0 (note
that we use ind ˜Bk(a, b) = 0, and the case ˜Bk(a, b) = 0 is excluded by
Remark 4.14(iii)). Thus, we have proved that the conditions m∗(Z−1
k (a) ˆYk(b)) = 1
and ind ˜Bk(a, b) = 0 are equivalent to the ﬁrst case in Deﬁnition 6.1.

6.1
Relative Oscillation Theory
409
In a similar way, we have that #k( ˆZ(b), Z(a)) = −1 if and only if the two
conditions m( ˆZ−1
k (b)Yk(a)) = 1 and ind Bk(a, b) = 0 hold. By Remark 4.14(iii),
we have m( ˆZ−1
k (b) Yk(a))
=
1 if and only if wk+1
=
0, wk
̸=
0, or
wkwk+1Bk(a, b) < 0, where Bk(a, b) = −ˆx2
k+1qk(a, b) according to (6.49).
Then, under the assumption qk(a, b) < 0, the last condition is equivalent with
wkwk+1 < 0. Thus, we have proved that #k( ˆZ(b), Z(a)) = −1 or the two
conditions m( ˆZ−1
k (b)Yk(a)) = 1, ind Bk(a, b) = 0 are equivalent to the second
case in Deﬁnition 6.1.
⊓⊔
Based on Lemma 6.6, one can specialize the results of Theorem 6.2 as follows.
Theorem 6.8 Assume that conditions (6.35) and (6.41) hold for systems (5.1) and
(6.1). Let Z[0](λ) and ˆZ[N+1](λ) be symplectic fundamental matrices of (5.1)
and (6.1) associated with the principal solutions Y [0](λ) = Z[0](λ) (0 I)T and
ˆY [N+1](λ) =
ˆZ[N+1](λ) (0 I)T of these systems at k = 0 and k = N + 1,
respectively. Then we have that all the formulas in Theorem 6.2 hold with the relative
oscillation numbers deﬁned by (6.52), (6.49), (6.39), where ˆZ(λ) := ˆZ[N+1](λ) and
Z(λ) := Z[0](λ).
Proof For the proof we use Theorem 6.2 and Lemma 6.6.
⊓⊔
In a similar way, we derive the special case of Theorem 6.4.
Theorem 6.9 Assume that conditions (6.35) and (6.41) hold for system (5.1). Let
Z[0](λ) and Z[N+1](λ) be symplectic fundamental matrices of (5.1) such that
Y [0](λ) = Z[0](λ) (0 I)T and Y [N+1](λ) = Z[N+1](λ) (0 I)T are the principal
solutions of this system at k = 0 and k = N + 1, respectively. Then for any
a, b ∈R with a < b we have that the number of ﬁnite eigenvalues of problem (E)
in (a, b] is counted by the number of backward focal points of the conjoined basis
[Z[0](a)]−1Y [N+1](b) of system (6.51) in the interval [0, N + 1), i.e.,
#{ν ∈σ | a < ν ≤b} = l∗[Z[0](a)]−1Y [N+1](b), 0, N + 1,
(6.54)
where system (6.51) is considered for the case Sk(λ) ≡ˆSk(λ) with Zk(a) := Z[0]
k (a)
and Qk(a, b) ≥0 is given by (6.40). Moreover
#{ν ∈σ | ν ≤b} = l∗
[Z[0](λ)]−1Y [N+1](b), 0, N + 1

,
λ < λ1,
(6.55)
where λ1 = min σ.
Proof We use Theorem 6.4 for the case (6.35). Remark that for Sk(λ) ≡ˆSk(λ),
k ∈[0, N]Z, according to (6.40) and (6.41)
Qk(a, b) ≥0,
a, b ∈R, a ≤b,
(6.56)

410
6
Miscellaneous Topics on Symplectic Systems
Then the relative oscillation number in (6.52) takes the form (here we use that
˜Bk(a, b) ≥0 and Bk(a, b) ≤0)
#k( ˆZ(b), Z(a)) = m∗(Z[0] −1
k
(a) Y [N+1]
k
(b))
= rankBk(a, b) −m(Z[N+1] −1
k
(b) Y [0]
k (a)),
Bk(a, b) = −X[N+1] T
k+1
(b) Qk(a, b) X[N+1]
k+1
(b),
⎫
⎪⎪⎬
⎪⎪⎭
(6.57)
where Z[0]
k (λ) and Z[N+1](λ) are symplectic fundamental matrices of system (5.1)
associated with the principal solutions Y [0]
k (λ) and Y [N+1]
k
(λ). Substituting the ﬁrst
formula in (6.57) into (6.31), we derive (6.55).
⊓⊔
Remark 6.10
(i) Recall that according to Remark 6.5(i) one can modify the representations of
the relative oscillation numbers using (6.33). For the special case under the
consideration, we have instead of (6.33) the formula
#k(Z[N+1](a), ˆZ[0](b)) = m∗([ ˆZ[0]
k (b)]−1 Y [N+1]
k
(a)) −ind [Bk(a, b)]
= ind [ ˜Bk(a, b)] −m([Z[N+1]
k
(a)]−1 ˆY [0]
k (b)),

(6.58)
where the matrices Bk(a, b) and ˜Bk(a, b) are given by (6.49) with ˆX(b) :=
ˆX[0](b) and X(a) := X[N+1](a). In particular, this reformulation leads to the
representation
#{ν ∈σ | a < ν ≤b} = l[Z[N+1](a)]−1Y [0](b), 0, N + 1,
(6.59)
which is equivalent to (6.54) and presents #{ν
∈σ | a
< ν
≤b} in
terms of the multiplicities of forward focal points of the conjoined basis
[Z[N+1](a)]−1Y [0](b) of the transformed system (6.51).
(ii) For the special linear dependence on parameter λ according to (6.42), (6.43),
(6.44), one can use Theorem 5.93 and property (4.147) in Theorem 4.56 to
derive the formulas
#{ν ∈ˆσ | ν < b} −#{ν ∈σ | ν < a} = #( ˆZ[0](b), Z[N+1](a), 0, N) −P∗,
(6.60)
where
P∗= ˆm∗−m∗= #( ˆZ[0](λ), Z[N+1](λ), 0, N),
λ < λ0.
(6.61)
Similarly, instead of (6.54) we have
#{ν ∈σ | a ≤ν < b} = l∗
[Z[N+1](a)]−1Y [0](b), 0, N + 1

.
(6.62)
Formulas (6.60), (6.61), and (6.62) can be also derived directly from (6.26),
(6.28), and (6.31) by using Lemma 4.47(iii).

6.1
Relative Oscillation Theory
411
6.1.4
Matrix Sturm-Liouville Eigenvalue Problems
In this subsection we consider the discrete matrix Sturm-Liouville spectral problems
(see Example 5.36)
(Rk(λ) xk(λ)) −Qk(λ) xk+1(λ) = 0,
k ∈[0, N −1]Z,
x0(λ) = 0 = xN+1(λ),
det Rk(λ) ̸= 0,
k ∈[0, N]Z,

(6.63)
and
( ˆRk(λ) ˆxk(λ)) −ˆQk(λ) ˆxk+1(λ) = 0,
k ∈[0, N −1]Z,
ˆx0(λ) = 0 = ˆxN+1(λ),
det ˆRk(λ) ̸= 0,
k ∈[0, N]Z,

(6.64)
where xk(λ) ∈Rn for n ≥1 and λ ∈R is the spectral parameter and the
real symmetric n × n matrix-valued functions Rk(λ), ˆRk(λ), Qk(λ), ˆQk(λ) for
k ∈[0, N]Z are piecewise continuously differentiable in the variable λ and obey
the conditions
˙Rk(λ) ≤0,
˙Qk(λ) ≤0,
k ∈[0, N]Z,
(6.65)
˙ˆRk(λ) ≤0,
˙ˆQk(λ) ≤0,
k ∈[0, N]Z.
(6.66)
According to Example 5.36, conditions (6.65) and (6.65) together with the nonsin-
gularity assumptions det Rk(λ) ̸= 0 and det ˆRk(λ) ̸= 0 imply (5.3), (6.5), (6.18).
Then one can apply Theorems 6.2 and 6.4 to this special case. As in the previous
subsection, we reﬁne formulas for the relative oscillation numbers using the
special structure of the symplectic matrices Sk(λ) and ˆSk(λ) associated with (6.63)
and (6.64) (see Sect. 2.1.2), i.e.,
Sk(λ) =

I
R−1
k (λ)
Qk(λ) I + Qk(λ) R−1
k (λ)

= Lk(λ) Hk(λ),
Lk(λ) :=

I
0
Qk(λ) I

,
Hk(λ) :=
I R−1
k (λ)
0
I

,
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
(6.67)
and
ˆSk(λ) =

I
ˆR−1
k (λ)
ˆQk(λ) I + ˆQk(λ) ˆR−1
k (λ)

= ˆLk(λ) ˆHk(λ),
ˆLk(λ) :=

I
0
ˆQk(λ) I

,
ˆHk(λ) :=

I ˆR−1
k (λ)
0
I

.
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
(6.68)
Recall that spectral problems (6.63) and (6.64) are the matrix analogs of the Sturm-
Liouville problems (6.9) and (6.10) with different coefﬁcients rk(λ) and ˆrk(λ) and

412
6
Miscellaneous Topics on Symplectic Systems
with the Wronskian wk(x, ˆx) = −rkxk+1 ˆxk + ˆrkxk ˆxk+1 (compare with (6.11)). The
relative oscillation theory for (6.9) and (6.10) with rk ̸= ˆrk and pk ̸= ˆpk is presented
in [21].
By (6.67) and (6.68), we see that problems (6.63) and (6.64) under the additional
assumption Rk(λ) ≡ˆRk(λ) ≡Rk obey conditions (6.35). However, in the general
case of nonconstant Rk(λ) and/or ˆRk(λ), the theory developed in Sect. 6.1.3 needs
to be generalized to the case when the matrices Rk(λ) and ˆRk(λ) are different.
Consider two conjoined bases Y(λ) = X(λ)
U(λ)
 and ˆY(λ) =  ˆX(λ)
ˆU(λ)
 of the
symplectic systems in (6.17) associated with (6.63) and (6.64), where we have the
connections (see Sect. 2.1.2)
Uk(λ) = Rk(λ) Xk(λ),
ˆUk(λ) = ˆRk(λ)  ˆXk(λ),
k ∈[0, N]Z,
UN+1(λ) = UN(λ) + QN(λ) XN+1(λ),
ˆUN+1(λ) = ˆUN(λ) + ˆQN(λ) ˆXN+1(λ).
⎫
⎪⎬
⎪⎭
(6.69)
Note that the coefﬁcients QN(λ) and ˆQN(λ) are not needed in equations (6.63)
and (6.64), but for convenience we deﬁne them at k = N such that (6.65) and (6.66)
hold. In Remark 6.12 we will show that the results of this section do not depend on
the deﬁnition of QN(λ) and ˆQN(λ).
Recall that for two ﬁxed values λ = a and λ = b (a, b ∈R) and k ∈[0, N +1]Z,
the Wronskian (3.2) of Yk(a) and ˆYk(b) has the form
wk(Y(a), ˆY(b)) := wk(a, b) = XT
k (a) ˆUk(b) −UT
k (a) ˆXk(b).
(6.70)
Moreover, it satisﬁes for k ∈[0, N]Z the equation (see (4.99))
−wk(a, b) = (XT
k (a)) [Rk(a) −ˆRk(b)]  ˆXk(b)
+XT
k+1(a) [Qk(a) −ˆQk(b)] ˆXk+1(b).

(6.71)
For the convenience of the presentation of the subsequent results, we introduce
the notation for the number of backward focal points of the conjoined basis Z−1 ˆY
associated with the Wronskian by (6.46) using the notation
m∗(wk, wk+1) := m∗(Z−1
k
ˆYk) = μ
 ˆZ−1
k Yk, ˆZ−1
k+1Yk+1

.
Then we have using Deﬁnition 4.11
m∗(wk, wk+1) = rankMk + ind Pk,
Mk := (I −w†
kwk) wT
k+1, Tk := I −M†
kMk, Pk := Tkwk+1w†
k CkTk,
Ck := ( ˆZ−1
k Yk)TJ ˆZ−1
k+1Yk+1 = Y T
k J ˆS−1
k SkYk = Y T
k+1J Sk ˆS−1
k Yk+1.
⎫
⎪⎬
⎪⎭
(6.72)

6.1
Relative Oscillation Theory
413
Note that according to Proposition 4.4(v), we have the estimate
m∗(wk, wk+1) ≤rank Ck ≤n.
(6.73)
As we already mentioned above, for the case Rk(λ) ≡ˆRk(λ) ≡Rk, one can
apply Lemma 6.6 to evaluate the relative oscillation numbers for problems (6.63)
and (6.64). Here (for convenience) we reformulate this lemma in terms of the
coefﬁcients Qk(λ) and ˆQk(λ) in (6.63) and (6.64).
Lemma 6.11 (Case I) Assume that for the two spectral problems (6.63), (6.65)
and (6.64), (6.66), the matrices Rk(λ) and ˆRk(λ) obey the conditions
˙Rk(λ) = ˙ˆRk(λ) = 0,
Rk ≡ˆRk,
k ∈[0, N]Z.
(6.74)
Then the relative oscillation numbers (4.109) have the form
#I
k( ˆZ(b), Z(a)) = m∗(wk(a, b), wk+1(a, b)) −ind Ck(a, b),
Ck(a, b) = XT
k+1(a) [Qk(a) −ˆQk(b)] Xk+1(a),

(6.75)
where m∗(wk(a, b), wk+1(a, b)) := m∗(Z−1
k (a) ˆYk(b)) is the number of backward
focal points of the conjoined basis Z−1
k (a) ˆYk(b) according to (6.72) with the matrix
Ck := Ck(a, b).
Proof We use formula (6.52) putting Ck(a, b) := ˜Bk(a, b).
⊓⊔
Remark 6.12 Note that in the deﬁnition of symplectic systems (6.67) and (6.68)
one can put QN(λ) = ˆQN(λ) = 0 and then #I
N( ˆZ(b), Z(a)) = 0. However for the
case when ˆZk(b) := ˆZ[N+1]
k
(b), we have #I
N( ˆZ(b), Z(a)) = 0 for any choice of
QN(λ) and ˆQN(λ). Indeed, for this case by (6.49), the matrix BN(a, b) = 0, and
then #I
N( ˆZ(b), Z(a)) = 0 by estimate (6.53).
Lemma 6.13 (Case II) Assume that for the two spectral problems (6.63), (6.65)
and (6.64), (6.66), the matrices Qk(λ) and ˆQk(λ) obey the conditions
˙Qk(λ) = ˙ˆQk(λ) = 0,
Qk ≡ˆQk,
k ∈[0, N]Z.
(6.76)
Then the relative oscillation numbers (4.109) take the form
#II
k ( ˆZ(b), Z(a)) = m∗(wk(a, b), wk+1(a, b)) −ind ˜Ck(a, b) + Pk,
˜Ck(a, b) = UT
k (a) [ ˆR−1
k (b) −R−1
k (a)] Uk(a),
Uk(λ) := Rk(λ) Xk(λ).

(6.77)
Here m∗(wk(a, b), wk+1(a, b)) := m∗(Z−1
k (a) ˆYk(b)) is the number of backward
focal points of Z−1
k (a) ˆYk(b) given by (6.72) with Ck := ˜Ck(a, b), and Pk is the

414
6
Miscellaneous Topics on Symplectic Systems
constant (with respect to λ) deﬁned by
Pk := ind ˆRk(λ0) −ind Rk(λ0),
λ0 ∈R.
(6.78)
Proof Note that for case (6.76) matrices (6.67), (6.68) obey the condition
ˆS−1
k (b) Sk(a) = ˆH −1
k (b) Hk(a) =

I R−1
k (a) −ˆR−1
k (b)
0
I

.
(6.79)
The symplectic upper block-triangular factors Hk(a) and ˆHk(b) in (6.79) can be
represented in the form
Hk(a) = −J Kk(a) J ,
ˆHk(b) = −J ˆKk(b) J ,
where Kk(a) and
ˆKk(b) are the symplectic lower block-triangular matrices.
Assumption (6.76) then implies that Lk(a) ≡ˆLk(b) = Lk in (6.67) and (6.68).
Consider operator (3.56) introduced in Sect. 3.3.1 (see also (4.102)), i.e.,
L(Y, ˆY , S, ˆS) := μ( ˆS ˆY, ˆS (0 I)T ) −μ(SY, S (0 I)T ) + μ(SY, ˆS ˆY) −μ(Y, ˆY ).
(6.80)
Applying the multiplicative property of this operator in Lemma 3.22(ii) (with the
data p := 4, W1 = ˆW1 := J , W2 := Kk(a), ˆW2 := ˆKk(b), W3 = ˆW3 := −J ,
W4 = ˆW4 := Lk, Y := Yk, and ˆY := ˆYk), we obtain
L(Yk, ˆYk, Sk(a), ˆSk(b))
= L

Yk, ˆYk, −LkJ Kk(a) J , −LkJ ˆKk(b) J

= L(Yk, ˆYk, J , J ) + L
J Yk, J ˆYk, Kk(a), ˆKk(b)

+ L

Kk(a) J Yk, ˆKk(b) J ˆYk, −J , −J

+ L

Hk(a) Yk(a), ˆHk(b) ˆYk(b), Lk, Lk

+
(
μ

Hk(a) (0 I)T, −J (0 I)T 
−μ
 ˆHk(b) (0 I)T, −J (0 I)T )
,
where the addends in the braces correspond to the last sum in (3.57). Taking into
account that the right-hand side of operator (6.80) equals zero for Sk = ˆSk (see
Lemma 3.22(i)) and evaluating the difference
(
μ

Hk(a) (0 I)T, −J (0 I)T 
−μ
 ˆHk(b) (0 I)T, −J (0 I)T )
= μ −J (0 I)T, ˆHk(b) (0 I)T  −μ −J (0 I)T, Hk(a) (0 I)T 

6.1
Relative Oscillation Theory
415
using Theorem 3.5(v), we have
L

Yk(a), ˆYk(b), Sk(a), ˆSk(b)

= L

J Yk(a), J ˆYk(b), Kk(a), ˆKk(b)

+ ind ˆRk(b) −ind Rk(a).
Recall that the symmetric nonsingular matrices Rk(λ) and ˆRk(λ) are continuous
functions in λ and then their eigenvalues have the constant sign for λ ∈R. So we
have ind ˆRk(a) = ind ˆRk(λ0) and ind Rk(a) = ind Rk(λ0) for any λ0 ∈R.
Note that in the operator L

J Yk, J ˆYk, Kk(a), ˆKk(b)

, the symplectic matrices
Kk(a) and ˆKk(b) are unit lower block-triangular (see Sect. 1.6.1), and then they
obey condition (6.38), i.e.,
Kk(a) ˆK−1
k (b) = J Hk(a) ˆH −1
k (b) J T =

I
0
ˆR−1
k (b) −R−1
k (a) I

.
Evaluating LJ Yk(a), J ˆYk(b), Kk(a), ˆKk(b) according to Lemma 6.6, where the
quantity ˜Bk(a, b) is replaced by ˜Ck(a, b) = UT
k (a) [ ˆR−1
k (b) −R−1
k (a)] Uk(a), we
derive (6.77) with Pk given by (6.78) (note that the Wronskian w(J Yk(a), J ˆYk(b))
is equal to the Wronskian w(Yk(a), ˆYk(b))). The proof is completed.
⊓⊔
Consider the evaluation of the relative oscillation numbers for the general case.
Introduce the following Wronskian
wk∗(a, b) := XT
k+1(a) ˆUk(b) −UT
k (a) ˆXk+1(b),
k∗∈(k, k + 1),
(6.81)
for Uk(λ) and ˆUk(λ) deﬁned as in (6.69). Here we use the intermediate point k∗∈
(k, k + 1) for a convenient interpretation of the subsequent results. Note that
wk∗(a, b) = wk(a, b) −UT
k (a) [ ˆR−1
k (b) −R−1
k (a)] ˆUk(b),
(6.82)
wk+1(a, b) = wk∗(a, b) −XT
k+1(a) [Qk(a) −ˆQk(b)] ˆXk+1(b),
(6.83)
and then summing (6.82) and (6.83) we derive (6.71). In particular, if case I takes
place (i.e., conditions (6.74) hold), then we have wk∗(a, b) = wk(a, b) by (6.82),
and similarly wk+1(a, b) = wk∗(a, b) by (6.83) in case II.
Theorem 6.14 (General Case)
For spectral problems (6.63), (6.65) and (6.64),
(6.66) associated with symplectic matrices (6.67) and (6.68), the relative oscillation
numbers (4.109) have the form
#k( ˆZ(b), Z(a)) = #II(k, k∗) + #I(k∗, k + 1),
(6.84)

416
6
Miscellaneous Topics on Symplectic Systems
where the numbers
#II(k, k∗) := m∗(wk(a, b), wk∗(a, b)) −ind ˜Ck(a, b) + Pk,
(6.85)
#I(k∗, k + 1) := m∗(wk∗(a, b), wk+1(a, b)) −ind Ck(a, b)
(6.86)
are evaluated according to (6.77), (6.78), and (6.75), respectively, with the quanti-
ties wk+1(a, b) := wk∗(a, b) for case II and wk(a, b) := wk∗(a, b) for case I.
Proof For the proof we use factorizations (6.67), (6.68) and Lemma 3.22. Using
Lemma 3.22(ii) (with p = 2, W1 := Hk(a) ˆW1 := ˆHk(b), W2 := Lk(a), ˆW2 :=
ˆLk(b), Y := Yk(a), ˆY := ˆYk(b)), we derive
L

Yk(a), ˆYk(b), Sk(a), ˆSk(b)

= L

Yk(a), ˆYk(b), Lk(a) Hk(a), ˆLk(b) ˆHk(b)

= LYk(a), ˆYk(b), Hk(a), ˆHk(b) + LHk(a) Yk(a), ˆHk(b) ˆYk(b), Lk(a), ˆLk(b),
where L

Yk(a), ˆYk(b), Hk(a), ˆHk(b)

and L

Hk(a) Yk(a), ˆHk(b) ˆYk(b), Lk(a),
ˆLk(b)

can be evaluated according to cases II and I, respectively. For case II
we have that the conjoined bases Yk(a) and ˆYk(b) obey the symplectic systems
Yk∗(a) = Hk(a) Yk(a) and ˆYk∗(b) =
ˆHk(b) ˆYk(b) for k∗∈(k, k + 1), and then
we have to use the Wronskian Yk∗(a)TJ ˆYk∗(b) = wk∗(a, b) given by (6.81)
instead of wk+1(a, b). Similarly, in case I we use that Yk∗(a) and ˆYk∗(b) obey the
symplectic systems Yk+1(a) = Lk(a) Yk∗(a) and ˆYk+1(b) = ˆLk(b) ˆYk∗(b), and
then we apply (6.75) replacing wk(a, b) by wk∗(a, b). Finally, point out that such
modiﬁcations of (6.77) and (6.75) do not touch the matrices ˜Ck(a, b) and Ck(a, b)
according to their deﬁnitions in (6.77) and (6.75). The proof is completed.
⊓⊔
Now we formulate some properties of the relative oscillation numbers in
Theorem 6.14.
Proposition 6.15 The relative oscillation numbers in (6.84), (6.85), (6.86) satisfy
the following properties.
(i) If case I takes place (i.e., the conditions in (6.74) hold), then in (6.84) we
have #k( ˆZ(b), Z(a)) = #I
k( ˆZ(b), Z(a)) for #I
k( ˆZ(b), Z(a)) given by (6.75).
Similarly, for case II we have #k( ˆZ(b), Z(a)) =
#II
k ( ˆZ(b), Z(a)) with
#II
k ( ˆZ(b), Z(a)) given by (6.77).
(ii) If the conditions
Qk(a) ≥ˆQk(b),
Rk(a) ≥ˆRk(b)
(6.87)
hold, then the relative oscillation numbers given by (6.84), (6.85), ad (6.86)
are nonnegative. If additionally to (6.87) we have ˆRk(λ) > 0 or if
Qk(λ) ≡ˆQk(λ),
Rk(λ) ≡ˆRk(λ),
a ≤b,

6.1
Relative Oscillation Theory
417
then for a ≤b the relative oscillation numbers are presented in the form
#k( ˆZ(b), Z(a)) = m∗(wk(a, b), wi∗(a, b)) + m∗(wi∗(a, b), wk+1(a, b)) ≥0,
(6.88)
where m∗(wk, wi∗) and m∗(wi∗, wk+1) are deﬁned by (6.72) with the quanti-
ties Ck := ˜Ck(a, b) ≥0 and Ck := Ck(a, b) ≥0 given by (6.77) and (6.75),
respectively.
(iii) For the relative oscillation numbers in (6.84), we have the estimate
		#k( ˆZ(b), Z(a))
		 ≤rank [Rk(a) −ˆRk(b)] + rank[Qk(a) −ˆQk(b)] ≤2n.
(6.89)
Proof For the proof of (i), we use (6.82) and (6.83). Case I implies (see (6.82)) that
wk(a, b) = wi∗(a, b) and the matrices ˜Ck(a, b) and Pk in (6.77) and (6.78) are equal
to the zero matrix. Then #II(i, i∗) = 0 and #k( ˆZ(b), Z(a)) = #I
k( ˆZ(b), Z(a)).
In a similar way, for case II we have Ck(a, b) = 0, and we get from (6.83)
that wk+1(a, b) = wi∗(a, b). Finally, it follows that #I(i∗, i + 1) = 0 and
#k( ˆZ(b), Z(a)) = #II
k ( ˆZ(b), Z(a)).
For the proof of (ii), we note that under assumptions (6.87) we have by
Example 3.30 that μ(⟨ˆSk(b)⟩, ⟨Sk(a)⟩) = 0 and then the relative oscillation
numbers are nonnegative because of their deﬁnition in (6.25). If, additionally, we
have ˆRk(λ) > 0, then by (6.87) we have Rk(λ) > 0, and moreover (6.87) is
equivalent to ˆR−1
k (b) −R−1
k (a) ≥0 (see Remark 3.35) and Pk = 0, ind Ck(a, b) =
ind ˜Ck(a, b) = 0. A similar situation occurs for Qk(λ) ≡ˆQk(λ), Rk(λ) ≡ˆRk(λ),
and a ≤b, because of the monotonicity assumptions (6.65). Then the proof of (ii)
is completed.
By (4.119) (see also the proof of Theorem 6.14), the relative oscillation
numbers (6.85) and (6.86) obey the inequalities
|#II(i, i∗)| ≤rank [Hk(a) −ˆHk(b)] = rank[Rk(a) −ˆRk(b)],
|#I(i∗, i + 1)| ≤rank [Lk(a) −ˆLk(b)] = rank [Qk(a) −ˆQk(b)],
and then for the relative oscillation numbers in (6.84), we have estimate (6.89). The
proof is completed.
⊓⊔
Finally, we reformulate Theorems 6.2 and 6.4 for the special case (6.63)
and (6.64), i.e., for the matrix Sturm-Liouville difference equations.
Theorem 6.16 (Relative Oscillation Theorem) Let σ and ˆσ be the ﬁnite spectra,
and let Y [0](λ) and ˆY [N+1](λ) be the principal solutions of problems (6.63)
and (6.64) with conditions (6.65) and (6.66). Then there exists a constant P ∈
[−nN, nN]Z such that for all a, b ∈R the statements of Theorem 6.2 hold
with the relative oscillation numbers deﬁned by (6.84), (6.85), and (6.86), where
ˆZ(λ) := ˆZ[N+1](λ) and Z(λ) := Z[0](λ).

418
6
Miscellaneous Topics on Symplectic Systems
For the case Rk(λ) ≡ˆRk(λ) and Qk(λ) ≡
ˆQk(λ) for b > a, Theorem 6.16
presents the number of ﬁnite eigenvalues of (6.63) in (a, b].
Theorem 6.17 (Renormalized Oscillation Theorem)
For problem (6.63), (6.65),
all statements of Theorem 6.4 (including the case a := λ, λ < λ1) hold with
μ⟨Gk(a, b)⟩, ⟨Gk+1(a, b)⟩ = m∗(wk, wk∗) + m∗(wk∗, wk+1) ≥0,
(6.90)
where the Wronskians wl := wl(a, b), l = k, k + 1 and wk∗:= wk∗(a, b)
are deﬁned by (6.70) and (6.81) with Y(a) := Y [0](a), ˆY(b) := Y [N+1](b) and
where m∗(wk, wk∗) and m∗(wk∗, wk+1) are given by (6.72) with the quantities
Ck :=
˜Ck(a, b) ≥0 and Ck := Ck(a, b) ≥0 given by (6.77) and (6.75),
respectively.
Proof Applying Proposition 6.15(ii) for the case Rk(λ) ≡ˆRk(λ), Qk(λ) ≡ˆQk(λ)
and Theorem 6.4, we complete the proof of Theorem 6.17.
⊓⊔
Remark 6.18 In the deﬁnition of (6.85), we use the number Pk given by (6.78),
which does not depend on a and b. Then it makes sense to introduce the new
constant
˜P := P −
N

i=0
Pk
(6.91)
and use identity (6.26) in the form
#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤a}
=
N

k=0
(
#k( ˆZ[N+1](b), Z[0](a)) −Pk
)
−˜P.
⎫
⎪⎪⎬
⎪⎪⎭
(6.92)
For the numbers #k( ˆZ[N+1](b), Z[0](a)) −Pk, one can also improve the esti-
mate (6.89) as follows:
		#k( ˆZ(b), Z(a)) −Pk
		 ≤rank ˜Ck(a, b) + rankCk(a, b),
where ˜Ck(a, b) and Ck(a, b) are given by (6.77) and (6.75). The proof follows from
inequality (6.53).
6.1.5
Examples
This section is devoted to examples, which illustrate the applications of Theo-
rems 6.16 and 6.17 to the scalar spectral problems (6.63) and (6.64).

6.1
Relative Oscillation Theory
419
Example 6.19 Consider problem (6.63) for the scalar Sturm-Liouville difference
equation
(rk(λ) xk(λ)) −qk(λ) xk+1(λ) = 0,
k ∈[0, 3]Z,
x0(λ) = 0 = x4(λ),
rk(λ) := (−1)k+1 exp((−1)kλ),
qk(λ) = −λ.
⎫
⎪⎬
⎪⎭
(6.93)
Then for the principal solution of (6.93) at 0 deﬁned by the initial conditions
x[0]
0 (λ) = 0 and x[0]
1 (λ) = 1/r0(λ), we have
x[0]
4 (λ) = −λ3 −6λ2 sinh(λ) −8λ sinh2(λ) + 2λ + 4 sinh(λ),
and the ﬁnite eigenvalues of (6.93) are the zeros of x4(λ), i.e., λ1 ≈−0.6167186,
λ2 = 0, λ3 ≈0.6167186. According to Theorem 6.17, we have
#k( ˆZ(b), Z(a)) = m∗(wk, wk∗) + m∗(wk∗, wk+1),
where for the scalar case
m∗(wk, wk∗) =

1, wk∗̸= 0, wkwk∗≤0,
0, otherwise,
m∗(wk∗, wk+1) =

1, wk+1 ̸= 0, wk+1wk∗≤0,
0, otherwise.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.94)
Then, according to (6.94) and Theorem 6.17, the number of ﬁnite eigenvalues of
problem (6.93) in the interval (a, b] is equal to the total number of generalized
zeros of the Wronskian in all intervals [k, k∗) and [k∗, k + 1) for k ∈[0, N]Z
and k∗∈(k, k + 1). For example, if a = −0.8 and b = 1.8, then we have the
three sign changes of the Wronskian wk(a, b) (see Fig. 6.1), and then according
to Theorem 6.17, the three ﬁnite eigenvalues of problem (6.93), i.e., the numbers
λ1 ≈−0.6167186, λ2 = 0, λ3 ≈0.6167186, are located in the interval (−0.8, 1.8].
Note that the relative oscillation number 0 ≤#k( ˆZ[N+1](b), Z[0](a)) ≤2 achieves
its maximal value 2 at the point k = 2.
Example 6.20 Consider spectral problem (6.93) and the following spectral problem
(referred to as “problem 1” and “problem 2” in Fig. 6.2):
(ˆrk(λ) ˆxk(λ)) −ˆqk(λ) ˆxk+1(λ) = 0,
k ∈[0, 3]Z,
ˆx0(λ) = 0 = ˆx4(λ),
ˆrk(λ) = (−1)k,
ˆqk(λ) = −(λ + 2).
⎫
⎪⎬
⎪⎭
(6.95)

420
6
Miscellaneous Topics on Symplectic Systems
Fig. 6.1 The graphs of the sign of the Wronskian and the relative oscillation numbers in
Example 6.19 for the values a = −0.8 and b = 1.8
−4
−3
−2
−1
0
1
−1
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
Graphs of y1,2(λ)=#(μ ∈σ1,2,μ ≤λ)
problem1
problem2
Fig. 6.2 The graphs of the functions y1,2(λ) of the number of ﬁnite eigenvalues below or equal to
λ for λ ∈R for problems 1 and 2 in Example 6.20

6.1
Relative Oscillation Theory
421
The ﬁnite eigenvalues of (6.95) are the zeros of the equation
ˆx[0]
4 (λ) = (λ + 2)3 −2λ −4 = 0,
ˆλ1 = −
√
2 −2 ≈−3.4142136,
ˆλ2 = −2,
ˆλ3 =
√
2 −2 ≈−0.5857864.
The localization of the ﬁnite eigenvalues of (6.93) and (6.95) is shown in Fig. 6.2,
where we present the functions y1,2(λ) of the number of ﬁnite eigenvalues below or
equal to λ for λ ∈R.
According to Theorem 6.16, we can calculate the difference between numbers
of ﬁnite eigenvalues of (6.95) and (6.93) using the relative oscillation numbers
#k( ˆZ[N+1](b), Z[0](a)) = #II(k, k∗) + #I(k∗, k + 1). For the scalar case, we have
#II(k, k∗) −Pk =
⎧
⎪⎨
⎪⎩
1,
ˆr−1
k (b) −r−1
k (a) > 0 and wk∗̸= 0, wkwk∗≤0,
−1, ˆr−1
k (b) −r−1
k (a) < 0 and wk ̸= 0, wkwk∗≤0,
0,
otherwise,
(6.96)
where the number Pk given by (6.78) is deﬁned as
Pk := ind ˆrk(λ0) −ind rk(λ0) = (−1)k+1.
Then we can say that the Wronskian has a weighted zero at the point k if the
quantity #II(k, k∗) −Pk = ±1. According to Remark 6.18, we can consider the
sum 3
k=0 Pk = 0 as the parameter of problem (6.95).
Similarly, for the scalar case, we derive
#I(k∗, k + 1) =
⎧
⎪⎨
⎪⎩
1,
qk(a) −ˆqk(b) > 0 and wk+1 ̸= 0, wk+1wk∗≤0,
−1, qk(a) −ˆqk(b) < 0 and wk∗̸= 0, wk+1wk∗≤0,
0,
otherwise,
(6.97)
and say that the Wronskian has a weighted node at k∗if #I(k∗, k + 1) = ±1.
Denote Ck(a, b) = qk(a) −ˆqk(b) and Bk(a, b) = ˆr−1
k (b) −r−1
k (a), and observe
that we can calculate the constant ˜P given by (6.91) evaluating the sum of (6.96)
and (6.97) for a = b = λ0 < min{λ1, ˆλ1}. By Fig. 6.3 we conclude that ˜P = 0,
where we take a = b = λ0 = −4. Then, according to Theorem 6.16, we can
evaluate the difference #{ν ∈σ2 | ν ≤b} −#{ν ∈σ1 | ν ≤a} between the numbers
of ﬁnite eigenvalues of (6.95) and (6.93) calculating the total number of weighted
zeros of the Wronskian for all k and k∗, k ∈[0, N]Z, k∗∈(k, k + 1).

422
6
Miscellaneous Topics on Symplectic Systems
Fig. 6.3 The graphs of the signs of the Wronskian, Ck(a, b) = qk(a)−ˆqk(b), Bk(a, b) = ˆr−1
k (b)−
r−1
k (a), and the relative oscillation numbers in Example 6.20 for the values a = b = −4
Fig. 6.4 The graphs of the signs of the Wronskian, Ck(a, b) = qk(a)−ˆqk(b), Bk(a, b) = ˆr−1
k (b)−
r−1
k (a), and the relative oscillation numbers in Example 6.20 for the values a = −10 and b = 10

6.1
Relative Oscillation Theory
423
Next, in Fig. 6.4 we present the graphs of the signs of the Wronskian, Ck(a, b) =
qk(a) −ˆqk(b) and Bk(a, b) = ˆr−1
k (b) −r−1
k (a) and the relative oscillation numbers
#k( ˆZ[N+1](b), Z[0](a)) −Pk for k ∈[0, 3]Z. For example, we have
#0( ˆZ[N+1](b), Z[0](a)) −P0 = #II(0, k∗) −P0 + #I(k∗, 1) = 2,
k∗= 1/2,
because B0(a, b) > 0, w0wk∗< 0, C0(a, b) > 0, w1wk∗< 0. Similarly, in the next
point k = 1, we have B1(a, b) < 0, w1wk∗< 0, C1(a, b) > 0, w2wk∗< 0, and
k∗= 3/2, and then #1( ˆZ[N+1](b), Z[0](a)) −P1 = −1 + 1 = 0. Finally, according
to Fig. 6.2, we have
#{ν ∈σ2 | ν ≤10} −#{ν ∈σ1 | ν ≤−10} = 3 −0 = 3,
and the sum of the relative oscillation numbers by Fig. 6.4 equals to 3.
6.1.6
Separated Boundary Conditions
We consider symplectic systems (5.1) and (6.1) with different separated boundary
conditions (5.155), i.e., the problems
yk+1(λ) = Sk(λ) yk(λ),
k ∈[0, N]Z,
R∗
0(λ) x0(λ) + R0(λ) u0(λ) = 0,
R∗
N+1(λ) xN+1(λ) + RN+1(λ) uN+1(λ) = 0,
ˆyk+1(λ) = ˆSk(λ) ˆyk(λ),
k ∈[0, N]Z,
ˆR∗
0(λ) ˆx0(λ) + ˆR0(λ) ˆu0(λ) = 0,
ˆR∗
N+1(λ) ˆxN+1(λ) + ˆRN+1(λ) ˆuN+1(λ) = 0,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.98)
where we assume that the matrices R∗
k(λ), Rk(λ), ˆR∗
k(λ), ˆRk(λ) for k ∈{0, N +
1} obey assumptions (5.156), (5.157), and (5.158). Moreover, we derive all results
under restriction (5.210)
rank Bk(λ), rank ˆBk(λ) are constant for λ ∈R for all k ∈[0, N]Z
rankR0(λ) and rank RN+1(λ) are constant for λ ∈R,
rank ˆR0(λ) and rank ˆRN+1(λ) are constant for λ ∈R.
⎫
⎪⎬
⎪⎭
(6.99)
Under the assumptions of Theorem 5.50 for problems (6.98), the ﬁnite spectra σ and
ˆσ of (6.98) are bounded, and one can order the ﬁnite eigenvalues of (6.98) according
to (6.19) using notation (6.20).

424
6
Miscellaneous Topics on Symplectic Systems
The consideration in this subsection is based on Theorem 5.41. Introduce the
following matrices associated with the separated boundary conditions in (6.98)
(see (5.171) and (5.173))
V−1(λ) :=
R∗T
0 (λ) K0(λ) −RT
0 (λ)
RT
0 (λ) K0(λ) R∗T
0 (λ)

,
K0(λ) := [R∗
0(λ) R∗T
0 (λ) + R0(λ) RT
0 (λ)]−1,
VN+1(λ) :=

R∗
N+1(λ)
RN+1(λ)
−KN+1(λ) RN+1(λ) KN+1(λ) R∗
N+1(λ)

,
KN+1(λ) := [R∗
N+1(λ) R∗T
N+1(λ) + RN+1(λ) RT
N+1(λ)]−1.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.100)
and
ˆV−1(λ) :=
 ˆR∗T
0 (λ) ˆK0(λ) −ˆRT
0 (λ)
ˆRT
0 (λ) ˆK0(λ)
ˆR∗T
0 (λ)

,
ˆK0(λ) := [ ˆR∗
0(λ) ˆR∗T
0 (λ) + ˆR0(λ) ˆRT
0 (λ)]−1,
ˆVN+1(λ) :=

ˆR∗
N+1(λ)
ˆRN+1(λ)
−ˆKN+1(λ) ˆRN+1(λ) ˆKN+1(λ) ˆR∗
N+1(λ)

,
ˆKN+1(λ) := [ ˆR∗
N+1(λ) ˆR∗T
N+1(λ) + ˆRN+1(λ) ˆRT
N+1(λ)]−1.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.101)
Deﬁnition 6.21 Let ˆZ[R](λ) and Z[L](λ) for λ ∈R be symplectic fundamental
matrices of (6.1) and (5.1), respectively, satisfying
ˆZ[R]
N+1(λ) = ˆV −1
N+1(λ),
Z[L]
0 (λ) = V−1(λ),
(6.102)
where the matrices ˆVN+1(λ) and V−1(λ) are determined by (6.100) and (6.101).
Consider conjoined bases Y [L]
k
(λ) = Z[L]
k (λ) (0 I)T and ˆY [R]
k
(λ) = ˆZ[R]
k
(λ) (0 I)T .
Then the relative oscillation numbers for problems (6.98) with a, b ∈R are deﬁned
by the formulas
#k( ˆZ[R](b), Z[L](a)) := μ

⟨Gk(a, b)⟩, ⟨Gk+1(a, b)⟩

−μ

⟨ˆSk(b)⟩, ⟨Sk(a)⟩

,
Gk(a, b) := ˆZ[R] −1
k
(b)Z[L]
k (a),
k ∈[0, N]Z,
#−1( ˆZ[R](b), Z[L](a)) := μ∗ ˆV −1
−1 (b) ˆY [R]
0
(b), ˆV −1
−1 (b)V−1(a) (0 I)T 
−μ
 ˆV−1(b) (0 I)T, V−1(a) (0 I)T 
,
#N+1( ˆZ[R](b), Z[L](a)) = μ
 ˆVN+1(b)Y [L]
N+1(a), ˆVN+1(b)V −1
N+1(a) (0 I)T 
−μ∗ ˆV −1
N+1(b) (0 I)T, V −1
N+1(a) (0 I)T 
,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.103)

6.1
Relative Oscillation Theory
425
Moreover, we deﬁne the relative oscillation number on the interval [−1, N +1]Z for
the pair of eigenvalue problems (6.98) by the formula
#( ˆZ[R](b), Z[L](a), −1, N + 1) :=
N+1

k=−1
#k( ˆZ[R](b), Z[L](a))
= #( ˆZ[R](b), Z[L](a), 0, N) + #−1( ˆZ[R](b), Z[L](a))
+ #N+1( ˆZ[R](b), Z[L](a)).
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.104)
Recall that problems (6.98) are equivalent to extended problems with the
Dirichlet boundary conditions (see Lemma 5.40). Then applying Theorems 6.2
and 6.4 to these extended problems, we derive the corresponding results for the
separated endpoints.
Theorem 6.22 (Relative Oscillation Theorem for Separated Endpoints)
Assume that for problems (6.98) conditions (5.3), (6.5), (6.99), and (5.156)–
(5.158) hold for the boundary conditions in (6.98). Let Z[L](λ) and ˆZ[R](λ) be
the symplectic fundamental matrices of these systems deﬁned by (6.102) associated
with the conjoined bases Y [L](λ) = Z[L](λ) (0 I)T and ˆY [R](λ) = ˆZ[R](λ) (0 I)T .
For arbitrary a, b ∈R deﬁne the relative oscillation numbers according to (6.103)
and (6.104). Then there exists a constant P ∈[−(N + 2) n, (N + 2) n]Z such that
for any a, b ∈R we have for the spectra σ and ˆσ of problems (6.98)
#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤a}
= #( ˆZ[R](b), Z[L](a), −1, N + 1) −P,
(6.105)
where for λ0 deﬁned by (6.20) we have
P = ˆℓ−ℓ= #( ˆZ[R](λ), Z[L](λ), −1, N + 1),
λ < λ0
(6.106)
with ℓgiven by (5.215) and similarly deﬁned ˆℓ.
Proof For the proof we apply Theorem 6.2 to extended problems with the Dirichlet
boundary conditions. As in the proof of Theorem 5.50, one can show that the
matrices Q0(λ), QN+1(λ), ˆQ0(λ), ˆQN+1(λ) used in Theorem 5.41 for the con-
struction of the extended problems can be omitted (here we use the notation ˆQk
for k ∈{0, N + 1} for the matrices (5.168) associated with the second problem
in (6.98)). Indeed, a direct application of Theorem 6.2 to the extended problems
constructed via Theorem 5.41 leads to the relative oscillation numbers
#k( ˆZ[N+2](b), Z[−1](a))
:= μ

⟨˜Gk(a, b)⟩, ⟨˜Gk+1(a, b)⟩

−μ

⟨ˆSk(b)⟩, ⟨Sk(a)⟩


(6.107)

426
6
Miscellaneous Topics on Symplectic Systems
for k ∈[−1, N + 1]Z, where
˜Gk(a, b) := diag{ ˆQT
N+1(b), ˆQ−1
N+1(b)} [ ˆZ[R]
k (b)]−1Z[L]
k (a) diag{QT −1
0
(a), Q0(a)}
for k ∈[0, N]Z, and according to Corollary 3.40(i) the symplectic block diagonal
matrices (which are a particular case of low triangular matrices) can be omitted,
i.e., we have the relative oscillation numbers given by (6.103) for k ∈[0, N]Z. For
k = −1 we have according to Remark 6.3 (see (6.29), where we replace the point 0
by −1)
#−1( ˆZ[N+2](b), Z[−1](a)) = μ∗ ˆS−1
−1(b) ˆY [N+2]
0
(b) (0 I)T, ˆS−1
−1(b)S−1(a) (0 I)T 
−μ
 ˆS−1(b) (0 I)T, S−1(a) (0 I)T 
.
Using ˆY [N+2]
N+1 (b) = ˆS−1
N+1(b) (0 I)T (which implies ˆY [N+2]
0
(b) = ˆY [R]
0
(b) ˆQN+1(b))
and the connection between ˆSk(λ), Sk(λ) and ˆVk(λ), Vk(λ) for k ∈{−1, N + 1}
by Theorem 5.41, we derive the representation for #−1( ˆZ[R](b), Z[L](a)) according
to (6.103) omitting all block diagonal matrices with Q0(λ), QN+1(λ),
ˆQ0(λ),
ˆQN+1(λ) according to Theorem 3.5(i)–(ii). The proof for the point N + 1, where
we use (6.30) replacing N by N + 1, is similar.
So we have shown that the relative oscillation numbers for the extended problems
obey Deﬁnition 6.21, and then applying Theorem 6.2 and incorporating (5.215), we
complete the proof.
⊓⊔
For the case when Sk(λ) ≡
ˆSk(λ) for k ∈[0, N]Z and Rk(λ) ≡
ˆRk(λ) and
R∗
k(λ) ≡ˆR∗
k(λ) for k ∈{−1, N + 1}, we derive a generalization of Theorem 6.4 for
separated boundary conditions.
Theorem 6.23 (Renormalized Oscillation Theorem for Separated Endpoints)
Under the assumptions of Theorem 6.22, consider only one problem in (6.98). Let
Z[L](λ) and Z[R](λ) be symplectic fundamental matrices of system (5.1) deﬁned
by (6.102) (where we put ˆVN+1(λ) := VN+1(λ)) associated with the conjoined
bases Y [L](λ) = Z[L](λ) (0 I)T and Y [R](λ) = Z[R](λ) (0 I)T . Then for any a, b ∈
R with a < b, the number of ﬁnite eigenvalues of problem (5.1) with (5.155) in the
interval (a, b] is given by the formula
#{ν ∈σ | a < ν ≤b} =
N

k=0
μ

⟨Gk(a, b)⟩, ⟨Gk+1(a, b)⟩

+μ∗
V −1
−1 (b) Y [R]
0
(b), V −1
−1 (b) V−1(a) (0 I)T 
+μ

VN+1(b) Y [L]
N+1(a), VN+1(b) V −1
N+1(a) (0 I)T 
,
Gk(a, b) := [Z[R]
k (b)]−1Z[L]
k (a).
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
(6.108)

6.1
Relative Oscillation Theory
427
In particular, for a := λ, λ < λ1 = min σ, formula (6.108) presents the number of
ﬁnite eigenvalues of (5.1), (5.155), which are less than or equal to b.
Proof As in the proof of Theorem 6.4, we have for Sk(λ) ≡ˆSk(λ), k ∈[0, N]Z,
Rk(λ) ≡ˆRk(λ), and for R∗
k(λ) ≡ˆR∗
k(λ), k ∈{−1, N + 1}, that
μ⟨Sk(b)⟩, ⟨Sk(a)⟩ = 0,
k ∈[−1, N + 1]Z,
(6.109)
where we use Lemma 5.105 and additionally, for the cases k = −1 and k = N + 1,
we apply Theorem 5.41 and assumption (6.99). We note that condition (6.109) for
k = −1 and k = N + 1 implies
μ

S−1(b) (0 I)T, S−1(a) (0 I)T 
= μ

V−1(b) (0 I)T, V−1(a) (0 I)T 
= 0,
μ∗
S−1
N+1(b) (0 I)T, S−1
N+1(a) (0 I)T 
= μ∗
V −1
N+1(b) (0 I)T, V −1
N+1(a) (0 I)T 
= 0,
where we use Lemma 3.21(v) and Theorem 3.5(i)–(ii). Then formulas for the
relative oscillation numbers in (6.108) follow from (6.109) and Deﬁnition 6.21.
⊓⊔
Remark 6.24 Assume Rk(λ) ≡ˆRk(λ) ≡Rk and R∗
k(λ) ≡ˆR∗
k(λ) ≡R∗
k for k ∈
{−1, N + 1}, i.e., the boundary conditions of problems (6.98) are the same and do
not depend on λ. Then the relative oscillation numbers for k = −1 and k = N +1 in
Theorems 6.22 and 6.23 are equal to zero by Deﬁnition 6.21 and by the deﬁnition of
the comparative index. In particular, for the Dirichlet boundary conditions in (6.17),
we have by Deﬁnition 6.21 that the relative oscillation numbers for k ∈{−1, N +1}
are equal to zero, and in this case Theorems 6.22 and 6.23 reduce into Theorems 6.2
and 6.4, respectively.
Another important special case of (6.98) is connected with the situation when
Sk(λ) ≡ˆSk(λ), i.e., when the problems differ in the boundary conditions only. As
a corollary to Theorem 6.22, we have the following result.
Corollary 6.25 Under all assumptions of Theorem 6.22, suppose additionally that
Sk(λ) ≡ˆSk(λ) and a = b. Then the difference of the numbers of ﬁnite eigenvalues
of (6.98) is given by the formula
#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤b}
= #−1( ˆZ[R](b), Z[L](b)) + #N+1( ˆZ[R](b), Z[L](b)) −P,

(6.110)
with the constant P deﬁned by
P := #−1( ˆZ[R](λ), Z[L](λ)) + #N+1( ˆZ[R](λ), Z[L](λ)) = ˆl −l,
λ ≤λ0,
λ0 := min σ ∪ˆσ,

(6.111)

428
6
Miscellaneous Topics on Symplectic Systems
where the relative oscillation numbers for k = −1 and N + 1 are given by (6.103)
and the constants l and ˆl are determined in Theorem 6.23.
Remark 6.26 Under the assumptions of Corollary 6.25, suppose that the boundary
conditions in (6.98) do not depend on λ, i.e., Rk(λ) ≡Rk, ˆRk(λ) ≡ˆRk, R∗
k(λ) ≡
R∗
k, ˆR∗
k(λ) ≡ˆR∗
k for k ∈{−1, N + 1}. Then the second addends in the deﬁnitions
of #−1( ˆZ[R](λ), Z[L](λ)) and #N+1( ˆZ[R](λ), Z[L](λ)) do not depend on λ as well
and formula (6.110) can be rewritten in the form
#{ν ∈ˆσ | ν ≤b}−#{ν ∈σ | ν ≤b}
= μ∗ ˆV −1
−1 ˆY [R]
0
(b), ˆV −1
−1 V−1(0 I)T 
+μ
 ˆVN+1Y [L]
N+1(b), ˆVN+1V −1
N+1(0 I)T 
−˜P,
⎫
⎪⎬
⎪⎭
(6.112)
where the new constant ˜P is connected with P given by (6.111) as follows
˜P = P + μ ˆV−1(0 I)T, V−1(0 I)T  + μ∗ ˆV −1
N+1(0 I)T, V −1
N+1(0 I)T 
= P1 + P2,
P1 := μ∗ ˆV −1
−1 ˆY [R]
0
(λ), ˆV −1
−1 V−1(0 I)T 
,
P2 := μ ˆVN+1Y [L]
N+1(λ), ˆVN+1V −1
N+1(0 I)T , λ ≤λ0 := min σ ∪ˆσ.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(6.113)
6.1.7
General Boundary Conditions
In this subsection we consider the general case of problems (5.1), (5.151)
and (6.1), (6.3) under the assumptions of Theorem 5.55. In particular, we impose
the conditions
rankBk(λ) is constant for λ ∈R for all k ∈[0, N]Z,
rank R2(λ) is constant for λ ∈R,
rank ˆBk(λ) is constant for λ ∈R for all k ∈[0, N]Z,
rank ˆR2(λ) is constant for λ ∈R.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.114)
As for the case of the separated boundary conditions, the main role in the
consideration plays Theorem 5.44, which gives the possibility to consider prob-
lems (5.1), (5.151) and (6.1), (6.3) as a special case of problems with separated
boundary conditions, and then all results of Sect. 6.1.6 can be applied. Applying
Theorem 5.44, consider the pair of the augmented problems (5.178) associated

6.1
Relative Oscillation Theory
429
with (5.1), (5.151) and (6.1), (6.3)
˜yk+1(λ) = {Sk(λ)} ˜yk(λ),
k ∈[0, N]Z,
 0 0
−I I

˜x0(λ) +
−I −I
0
0

˜u0(λ) = 0,
R1(λ) ˜xN+1(λ) + R2(λ) ˜uN+1(λ) = 0
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
¯yk+1(λ) = { ˆSk(λ)} ¯yk(λ),
k ∈[0, N]Z,
 0 0
−I I

¯x0(λ) +
−I −I
0
0

¯u0(λ) = 0,
ˆR1(λ) ¯xN+1(λ) + ˆR2(λ) ¯uN+1(λ) = 0.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.115)
According to Deﬁnition 6.21, we introduce the 4n × 4n matrices associated with
Rk(λ) and ˆRk(λ) for k ∈{1, 2} (see (6.100), (6.101))
VN+1(λ) :=

R1(λ)
R2(λ)
−K(λ) R2(λ) K(λ) R1(λ)

,
K(λ) := [R1(λ) RT
1 (λ) + R2(λ) RT
2 (λ)]−1,
ˆVN+1(λ) :=

ˆR1(λ)
ˆR2(λ)
−ˆK(λ) ˆR2(λ) ˆK(λ) ˆR1(λ)

,
ˆK(λ) := [ ˆR1(λ) ˆRT
1 (λ) + ˆR2(λ) ˆRT
2 (λ)]−1.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.116)
The boundary conditions at the left endpoint in problems (6.115) are the same and
do not depend on λ, and then one can put V−1(λ) ≡ˆV−1(λ) ≡S0, where the matrix
S0 is given in (3.102). Applying the additional properties of the comparative index
for symplectic matrices (see Sect. 3.3.5), we derive the following representation of
the relative oscillation numbers for problems (6.115) using the notation introduced
in Sect. 3.3.5.
Lemma 6.27 Let ˆZ[N+1](λ) and Z[0](λ) be the symplectic fundamental matrices of
systems (6.1) and (5.1) satisfying for all λ ∈R the initial conditions ˆZ[N+1]
N+1 (λ) =
I = Z[0]
0 (λ) and
Z[L]
k
(λ) = {Z[0]
k (λ)} S0,
ˆZ[R]
k
(λ) = { ˆZ[N+1]
k
(λ)} ˆV−1
N+1(λ),

k ∈[0, N + 1]Z,
(6.117)
where S0 and ˆVN+1(λ) are deﬁned by (3.102) and (6.116). Then, concern-
ing the relative oscillation numbers #k( ˆZ[R](b), Z[L](a)) deﬁned by (6.103) for

430
6
Miscellaneous Topics on Symplectic Systems
problems (6.115), we have the identities
#k
 ˆZ[R](b), Z[L](a)

= μ
 ˆVN+1(b)⟨Gk(a, b)⟩, ˆVN+1(b)⟨Gk+1(a, b)⟩

−μ

⟨ˆSk(b)⟩, ⟨Sk(a)⟩

,
k ∈[0, N]Z,

(6.118)
where
Gk(a, b) = [ ˆZ[N+1]
k
(b)]−1Z[0]
k (a),
and
#−1( ˆZ[R](b), Z[L](a)) = 0,
#N+1( ˆZ[R](b), Z[L](a)) = μ
 ˆVN+1(b)⟨Z[0]
N+1(a)⟩, ˆVN+1(b)V−1
N+1(a) (0 I)T 
−μ∗ ˆV−1
N+1(b) (0 I)T, V−1
N+1(a) (0 I)T 
.
⎫
⎪⎪⎬
⎪⎪⎭
(6.119)
Proof By (6.103) and (6.117), we have for problems (6.115)
#k( ˆZ[R](b), Z[L(a))
= μ
/ ˆVN+1(b) WkS0
0
,
/ ˆVN+1(b) Wk+1S0
0
−μ

⟨{ ˆSk(b)}⟩, ⟨{Sk(a)}⟩

,
where k ∈[0, N]Z and
Wk :=
!
[ ˆZ[N+1]
k
(b)]−1Z[0]
k (a)
"
.
According to Corollary 3.40(v), by (3.122) with R := ˆV−1
N+1, we obtain that the
ﬁrst addend on the right-hand side of the previous formula is the same as the
ﬁrst summand on the right-hand side of (6.118). Next, using Proposition 3.37,
by (3.109), we obtain
μ

⟨{ ˆSk(b)}⟩, ⟨{Sk(a)}⟩

= μ

⟨ˆSk(b)⟩, ⟨Sk(a)⟩

.
Then, formula (6.118) is proved. For the proof of (6.119), we note that the relative
oscillation number at k = −1 equals to zero by Remark 6.24. The representation
of #N+1( ˆZ[R](b), Z[L](a)) follows from the deﬁnition (see (6.103)), where we
incorporate that
Y[L]
N+1(a) = Z[L]
N+1(a) (0 I)T = {Z[0]
N+1(a)} S0(0 I)T = ⟨Z[0]
N+1(a)⟩,
see (6.117) and (3.104). The proof is completed.
⊓⊔

6.1
Relative Oscillation Theory
431
Deﬁne the relative oscillation numbers for (6.115)
#( ˆZ[R](b), Z[L](a), 0, N + 1)
=
N+1

k=0
#k( ˆZ[R](b), Z[L](a))
= #( ˆZ[R](b), Z[L](a), 0, N) + #N+1( ˆZ[R](b), Z[L](a)).
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.120)
Based on Lemma 6.27, we present the relative and renormalized oscillation
theorems for joint endpoints as corollaries to Theorems 6.22 and 6.23.
Theorem 6.28 (Relative Oscillation Theorem for Joint Endpoints)
For prob-
lems (5.1), (5.151) and (6.1), (6.3) assume that (5.3), (5.152), (5.153), (6.4)– (6.6),
and (6.114) hold. Deﬁne the relative oscillation numbers according to Lemma 6.27
and (6.120). Then there exists a constant P ∈[−(N + 2) n, (N + 2) n]Z such
that for any a, b ∈R, we have for the spectra σ and ˆσ of problems (5.1), (5.151)
and (6.1), (6.3) the equality
#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤a}
= #( ˆZ[R](b), Z[L](a), 0, N + 1) −P,

(6.121)
where for λ0 in (6.20) and ℓgiven by (5.231) (and similarly deﬁned ˆℓ), we have
P = ˆℓ−ℓ= #( ˆZ[R](λ), Z[L](λ), 0, N + 1),
λ < λ0.
(6.122)
Proof The result follows from the relative oscillation theorem (Theorem 6.22)
applied to the two augmented problems (6.115), which have the separated endpoints.
⊓⊔
Theorem 6.29 (Renormalized Oscillation Theorem for Joint Endpoints)
Under the assumptions of Theorem 6.28, consider only one problem (5.1) , (5.151).
Assume that the relative oscillation numbers are deﬁned according to Lemma 6.27,
where ˆZ[N+1](λ) ≡Z[N+1](λ) and ˆVN+1(λ) ≡VN+1(λ). Then for any a, b ∈R
with a < b, the number of ﬁnite eigenvalues of problem (5.1) with (5.151) in the
interval (a, b] is given by the formula
#{ν ∈σ | a < ν ≤b}
=
N

k=0
μ

VN+1(b) ⟨Gk(a, b)⟩, VN+1(b) ⟨Gk+1(a, b)⟩

+μVN+1(b) ⟨Z[0]
N+1(a)⟩, VN+1(b)V−1
N+1(a) (0 I)T ,
Gk(a, b) := [Z[N+1]
k
(b)]−1Z[0]
k (a).
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
(6.123)

432
6
Miscellaneous Topics on Symplectic Systems
In particular, for a := λ and λ < λ1 = min σ, formula (6.123) presents the number
of ﬁnite eigenvalues of (5.1), (5.151), which are less than or equal to b.
Proof We use that problem (5.1), (5.151) is equivalent to the ﬁrst problem in (6.115)
with separated endpoints. The result then follows from Theorem 6.23.
⊓⊔
Remark 6.30
(i) Using Lemmas 4.47 and 3.39, it is possible to show (see [121, Remark 4.5.10])
that in case of separated boundary conditions, the identity
#( ˆZ[R](b), Z[L](a), 0, N + 1) = #( ˆZ[R](b), Z[L](a), −1, N + 1)
holds, where the number on the left-hand side is given by (6.118), (6.119),
(6.120) and on the right-hand side in accordance with Deﬁnition 6.21. It should
be emphasized that “componentwise” identities
#N+1( ˆZ[R](b), Z[L](a)) = #N+1( ˆZ[R](b), Z[L](a)) + #−1( ˆZ[R](b), Z[L](a))
and
#( ˆZ[R](b), Z[L](a), 0, N) = #( ˆZ[R](b), Z[L](a), 0, N)
do not necessarily hold in general, but they hold in the case when ˆSk(λ) ≡
Sk(λ) for all k ∈[0, N]Z.
(ii) For the case R1(λ) ≡ˆR1(λ) ≡R1 and R2(λ) ≡ˆR2(λ) ≡R2, i.e., when the
boundary conditions are the same and are independent on λ, we have by (6.119)
that #N+1( ˆZ[R](b), Z[L](a)) = 0. In particular, for the Dirichlet boundary
conditions, we put VN+1(λ) ≡ˆVN+1(λ) = I4n, and then Theorem 6.28 reduces
to Theorem 6.2.
By analogy with the case of separated endpoints, consider the case when
problems differ in the boundary conditions only.
Corollary 6.31 Under all assumptions of Theorem 6.28, suppose additionally that
Sk(λ) ≡ˆSk(λ) for all k ∈[0, N]Z and a = b. Then the difference of the numbers of
ﬁnite eigenvalues of problems (5.1), (5.151) and (6.1), (6.3) is given by the formula
#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤b} = #N+1( ˆZ[R](b), Z[L](b)) −P,
(6.124)
with the constant P deﬁned by
P := #N+1( ˆZ[R](λ), Z[L](λ)) = ˆℓ−ℓ,
λ ≤λ0 := min σ ∪ˆσ,
(6.125)
where the relative oscillation number for k = N + 1, a = b is given by (6.119) and
the constants ℓand ˆℓare determined in Theorem 6.28.

6.2
Inequalities for Finite Eigenvalues
433
Remark 6.32 As for the case of the separated boundary conditions, under the
assumptions of Corollary 6.31, suppose that the boundary conditions in (5.151)
, (6.3) do not depend on λ, i.e., Rk(λ) ≡Rk and ˆRk(λ) ≡ˆRk for k ∈{1, 2}. Then
the second addend in the deﬁnition of #N+1( ˆZ[R](λ), Z[L](λ)) does not depend on
λ as well and formula (6.124) takes the form
#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤b}
= μ
 ˆVN+1⟨Z[0]
N+1(b)⟩, ˆVN+1V−1
N+1(0 I)T 
−˜P,

(6.126)
where the new constant ˜P is connected with P given by (6.125) as follows
˜P = P + μ∗ ˆV−1
N+1(0 I)T, V−1
N+1(0 I)T 
= μ ˆVN+1⟨Z[0]
N+1(λ)⟩, ˆVN+1V−1
N+1(0 I)T ,
λ ≤λ0 := min σ ∪ˆσ.

(6.127)
6.2
Inequalities for Finite Eigenvalues
In this section we derive inequalities for eigenvalues of problems (6.17), resp.,
for (6.98), resp., for (5.1), (5.151) and (6.1), (6.3), by using inequalities for their
spectral functions. As in Sect. 6.1, we consider these spectral problems under the
assumptions, which guarantee that the ﬁnite spectra are bounded from below. We
order the ﬁnite eigenvalues λk ∈σ of (5.1), (5.151), resp., of ˆλk ∈ˆσ of (6.1), (6.3),
into nondecreasing sequences (see (6.19)) as
−∞< λ1 ≤λ2 ≤· · · ≤λm ≤. . . ,
−∞< ˆλ1 ≤ˆλ2 ≤· · · ≤ˆλl ≤. . . .
We also put λ1 := ∞(ˆλ1 := ∞) if σN = ∅(ˆσN = ∅). According to (6.20) we
deﬁne λ0 := min{λ1, ˆλ1}. For the proofs of the subsequent results, we need the
following.
Proposition 6.33 Assume that for some K ∈N ∪{0} the inequality
#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤b} ≥−K,
(6.128)
holds and that there exists a ﬁnite eigenvalue λj+K
∈σ, j
≥1, of prob-
lem (5.1), (5.151). Then there exists the ﬁnite eigenvalue ˆλj ∈ˆσ of (6.1), (6.3),
and it satisﬁes
ˆλj ≤λj+K.
(6.129)

434
6
Miscellaneous Topics on Symplectic Systems
If, instead of (6.128), we have
#{ν ∈ˆσ | ν < b} −#{ν ∈σ | ν ≤b} ≥−K,
(6.130)
then the inequality in (6.129) is strict, i.e.,
ˆλj < λj+K.
(6.131)
Proof If there exists λj+K ∈σ (with j ≥1), then #{ν ∈σ | ν ≤λj+K} ≥j + K
(note that #{ν ∈σ | ν ≤λj+K} = j + K if λj+K is simple) and, according to
assumption (6.128),
#{ν ∈ˆσ | ν ≤λj+K} ≥#{ν ∈σ | ν ≤λj+K} −K ≥j.
Consequently, there exist ˆλj ∈ˆσ and (6.129) holds. If (6.130) holds, then repeating
the arguments from the proof of the inequality ˆλj ≤λj+K, we obtain #{ν ∈ˆσ | ν <
λj+K} ≥j. Consequently, the strict inequality (6.131) holds.
⊓⊔
In Sect. 6.2.1 we compare the ﬁnite eigenvalues of (5.1), (5.151) and (6.1), (6.3)
under some majorant assumptions for their symplectic coefﬁcient matrices and
for the matrices in the boundary conditions. There, the consideration is based
on Theorems 6.22 and 6.28, whose assumptions guarantee that σ and ˆσ are
also bounded from above. In Sect. 6.2.2 we consider problems (5.1), (5.151)
and (6.1), (6.3), which differ only in the boundary conditions. For this case we admit
the oscillations of the block Bk(λ) of Sk(λ) at ∞, which implies that the spectra σ
and/or ˆσ may be unbounded from above. We derive the interlacing properties of
ﬁnite eigenvalues based on the pair of the inequalities
ˆλj ≤λj+K,
λp ≤ˆλp+ ˆK,
p ≥1,
j ≥1,
K ≥0,
ˆK ≥0,
(6.132)
for the ﬁnite eigenvalues of problems (5.1), (5.151) and (6.1), (6.3) with joint and
separated endpoints.
6.2.1
Comparison of Finite Eigenvalues
In this subsection we prove inequality (6.129) under suitable majorant conditions.
In particular, we investigate the case when (6.129) holds with K = 0 and the
ﬁnite eigenvalues ˆλj ∈ˆσ for (6.1), (6.3) lie below the ﬁnite eigenvalues λj ∈σ
for (5.1), (5.151), i.e.,
ˆλj ≤λj.
(6.133)

6.2
Inequalities for Finite Eigenvalues
435
This fact is also proven for problems (6.17) with the Dirichlet boundary conditions,
as well as for (6.98) with the separated endpoints. All these results can be derived
as corollaries to Theorems 6.2, 6.22, and 6.28.
The following theorem formulates sufﬁcient conditions for inequalities (6.129)
and (6.133), when the boundary conditions are separated.
Theorem 6.34 Under the assumptions of Theorem 6.22, suppose that for the
symplectic coefﬁcient matrices Sk(λ) and ˆSk(λ), we have the majorant condition
μ⟨ˆSk(λ)⟩, ⟨Sk(λ⟩ = 0,
λ ∈R,
k ∈[0, N]Z,
(6.134)
and the matrix coefﬁcients in the boundary conditions (6.98) obey the assumption
μ
−ˆRT
0 (λ)
ˆR∗T
0 (λ)

,
−RT
0 (λ)
R∗T
0
(λ)

= μ∗
−ˆRT
N+1(λ)
ˆR∗T
N+1(λ)

,
−RT
N+1(λ)
R∗T
N+1(λ)

= 0, λ ∈R.
(6.135)
Then for the constant P given by (6.106), we have that inequality (6.129) holds with
K := P ≥0, provided the ﬁnite eigenvalue λj+K ∈σ, j ≥1, exists. In particular,
inequality (6.133) holds when
P = 0.
(6.136)
Proof Note that condition (6.135) can be rewritten in the form
μ ˆV−1(λ)(0 I)T , V−1(λ)(0 I)T 
= μ∗ ˆV −1
N+1(λ) (0 I)T , V −1
N+1(λ) (0 I)T 
= 0,
λ ∈R,

(6.137)
for the matrices Vk(λ) and ˆVk(λ) for k ∈{−1, N+1} deﬁned by (6.100) and (6.101).
Then, under assumptions (6.134) and (6.137), the relative oscillation numbers given
by (6.103) with a = b := λ are nonnegative, i.e.,
#k
 ˆZ[N+2](λ), Z[−1](λ)

≥0,
k ∈[−1, N + 1]Z,
λ ∈R.
(6.138)
Next, by (6.138) we derive according to (6.105) that
#{ν ∈ˆσ | ν ≤λj} −#{ν ∈σ | ν ≤λj} ≥−P,
(6.139)
where P ≥0, because of (6.138) and (6.106). The result then follows from
Proposition 6.33.
⊓⊔

436
6
Miscellaneous Topics on Symplectic Systems
Remark 6.35
(i) The majorant condition in (6.134), including the main important special cases
of (SDS), is discussed in Remark 4.51. Note also that condition (5.321)
from Theorem 5.89 is sufﬁcient for (6.134), because of Lemma 3.25 and
formula (3.74); see Remark 4.51(i).
(ii) Note that condition
ˆℓ= 0
(6.140)
with the constant ˆℓgiven by (5.215) for the second problem in (6.98) (see
Theorem 6.22) is sufﬁcient for (6.136). Indeed, according to (6.106), under
assumptions (6.134) and (6.137), we have by (6.138) and (6.140) that
0 ≤P = #k
 ˆZ[N+2](λ), Z[−1](λ)

= −ℓ≤0,
k ∈[−1, N + 1]Z,
and then P = −ℓ= 0 as well. Finally, condition (6.140) implies (6.136).
(iii) Recall that condition (6.140) is equivalent to the existence of λ00 < 0 such
that the quadratic functional ˆG(y, λ) in (5.188) associated with the second
problem in (6.98) obeys the condition ˆG(·, λ00) > 0. For problems (6.98)
with the special linear dependence on λ (see (5.239) and (5.245)), sufﬁcient
conditions for ˆG(·, λ00) > 0 are given in Propositions 5.66 and 5.67 (see
also Remark 5.68). We recall that equivalent conditions to ˆG(·, λ00) > 0 are
presented in Theorem 2.50.
For the problems (6.17) with the Dirichlet boundary conditions, assump-
tion (6.137) is automatically satisﬁed. Then we have the following corollary to
Theorem 6.34.
Corollary 6.36 Under the assumptions of Theorem 6.2, suppose that (6.134) holds.
Then inequality (6.129) holds for the ﬁnite eigenvalues of problems (6.17) with K :=
P ≥0, where the constant P is given by (6.28). In particular, for the case (6.136),
we have inequality (6.133).
Remark 6.37
(i) It follows from Remark 6.35(ii) that the condition
ˆm = 0
(6.141)
with ˆm deﬁned by (5.47) for the second problem in (6.17) (see also Theo-
rem 6.2) is sufﬁcient for (6.136). Moreover, condition (6.141) is equivalent to
the existence of λ00 < 0 such that the quadratic functional ˆF(y, λ) in (5.138)
associated with the second problem in (6.17) obeys the condition ˆF(·, λ00) > 0
(see Proposition 5.30).

6.2
Inequalities for Finite Eigenvalues
437
(ii) For the case when problems (6.17) obey assumption (6.35), condition (6.134)
can be replaced by
[X[0]
k+1(λ)]T [Ck(λ) ˆDT
k (λ) −Dk(λ) ˆCT
k (λ)] X[0]
k+1(λ) ≥0,
(6.142)
where Ck(λ), Dk(λ) and ˆCk(λ), ˆDk(λ) are the blocks of Sk(λ) and ˆSk(λ),
respectively (see (5.2) and (6.2)). Here X[0]
k+1(λ) is the upper block of the
principal solution Y [0](λ) of the ﬁrst problem in (6.17) evaluated at k + 1.
In particular, condition (6.142) holds under the assumption
Ck(λ) ˆDT
k (λ) −Dk(λ) ˆCT
k (λ) ≥0.
(6.143)
The proof follows from Theorem 6.8 and from formula (6.52), where we use
that (6.142) is equivalent to ˜Bk(a, b) ≥0 with a = b := λ.
(iii) For the pair of eigenvalue problems (6.17) with the special linear dependence
on λ in (6.42), condition (6.142) takes the form
[X[0]
k+1(λ)]T [λ ( ˆWk −Wk) + Ck ˆDT
k −Dk ˆCT
k ] X[0]
k+1(λ) ≥0,
(6.144)
see (6.43). Note that under the additional assumption
ˆWk > 0 for k ∈
[0, N]Z condition (6.141) is satisﬁed due to Proposition 5.69, and then
inequality (6.133) holds. In particular, for Sturm-Liouville eigenvalue prob-
lems (6.9), (6.10), condition (6.144) takes the form [x[0]
k+1(λ)]2(pk −ˆpk) ≥0
for k ∈[0, N −1]Z, and it certainly implies (6.133), because for this case
ˆWk = 1.
(iv) For the discrete matrix Sturm-Liouville spectral problems in Sect. 6.1.4
(see (6.63) and (6.64)), condition (6.134) can be replaced by
[U[0]
k (λ)]T [ ˆR−1
k (λ) −R−1
k (λ)] U[0]
k (λ) ≥0,
[X[0]
k+1(λ)]T [Qk(λ) −ˆQk(λ)] X[0]
k+1(λ) ≥0,

(6.145)
where X[0]
k (λ) and U[0]
k (λ) are the blocks of the principal solution Y [0](λ)
of (6.63), i.e., X[0]
0 (λ) = 0 and U[0]
0 (λ) = R0(λ) X[0]
0 (λ) = I. By
analogy with the proof of Theorem 6.34, we use Remark 6.18 to show that
condition (6.145) implies (6.129) with K := ˜P given by (6.91). Moreover,
under the assumption ˜P = 0, we have inequality (6.133) for the ﬁnite
eigenvalues of (6.63) and (6.64). Note that the conditions in (6.145) follow
from the classical majorant assumptions Rk(λ) ≥ˆRk(λ) > 0 and Qk(λ) ≥
ˆQk(λ) (see Proposition 6.15 (ii) and Remark 3.35).
The last result of this subsection is devoted to problems (5.1), (5.151)
and (6.1), (6.3) with joint endpoints.

438
6
Miscellaneous Topics on Symplectic Systems
Theorem 6.38 Under the assumptions of Theorem 6.28, suppose that for the
symplectic coefﬁcient matrices Sk(λ) and ˆSk(λ), we have the majorant condi-
tion (6.134), while the matrices in the boundary conditions (5.151) and (6.3) obey
μ∗
−ˆRT
2 (λ)
ˆRT
1 (λ)

,
−RT
2 (λ)
RT
1 (λ)

= 0,
λ ∈R.
(6.146)
Then inequality (6.129) holds for the ﬁnite eigenvalues of
(5.1), (5.151)
and (6.1), (6.3) with the constant K := P ≥0 and P given by (6.122), provided
the ﬁnite eigenvalue λj+K exists. In particular, inequality (6.133) holds for the case
of (6.136).
Proof Condition (6.146) can be rewritten as
μ∗ ˆV−1
N+1(λ) (0 I)T, V−1
N+1(λ) (0 I)T 
= 0,
λ ∈R,
(6.147)
where the matrices VN+1(λ) and ˆVN+1(λ) are deﬁned by (6.116). It follows that
under assumptions (6.134) and (6.146), the relative oscillation numbers given
by (6.118) and (6.119) with a = b := λ are nonnegative, and then the proof repeats
the proof of Theorem 6.34. In particular, we derive inequality (6.139), which holds
with P given by (6.122).
⊓⊔
Remark 6.39 Note that, by analogy with Remark 6.35(ii), the condition (6.140)
with the constant ˆℓdeﬁned by (5.436) is sufﬁcient for (6.136) with the constant P
given by (6.122). In turn, one can apply the statement of Theorem 5.57 saying that
ˆℓdeﬁned by (5.436) equals to zero if and only if there exists λ00 < 0 such that the
quadratic functional ˆG(y, λ00) in (5.186) over (5.187) associated with (6.1), (6.3)
is positive deﬁnite. We recall that this positivity condition can be tested by the
properties of the principal solution Y [0](λ00) in Theorem 2.53.
6.2.2
Interlacing of Eigenvalues for Joint Endpoints
In this subsection we consider problems (5.1), (5.151) and (6.1), (6.3) with the same
coefﬁcient matrices Sk(λ) ≡ˆSk(λ) and with the boundary conditions which are
independent on λ, i.e.,
yk+1(λ) = Sk(λ) yk(λ),
k ∈[0, N]Z,
R1
 x0(λ)
xN+1(λ)

+ R2
 −u0(λ)
uN+1(λ)

= 0,
ˆyk+1(λ) = Sk(λ) ˆyk(λ),
k ∈[0, N]Z,
ˆR1
 ˆx0(λ)
ˆxN+1(λ)

+ ˆR2
 −ˆu0(λ)
ˆuN+1(λ)

= 0.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.148)

6.2
Inequalities for Finite Eigenvalues
439
We assume that the symplectic matrix Sk(λ) obeys the monotonicity condition (5.3)
and condition (5.333) holds. Then by Corollary 5.123 the spectra σ and ˆσ of (6.148)
are bounded from below.
We investigate the case when the ﬁnite eigenvalues of the ﬁrst problem in (6.148)
or in (6.181) interlace the ﬁnite eigenvalues of the second one, i.e., we derive the
inequalities (6.132):
ˆλj ≤λj+K,
λp ≤ˆλp+ ˆK,
p ≥1, j ≥1,
K ≥0, ˆK ≥0,
provided the ﬁnite eigenvalues λj+K and λp+ ˆK exist. Now we formulate conditions,
when inequalities (6.132) hold for the ﬁnite eigenvalues of (6.148). Introduce the
notation
Rw := rank [(R1 R2) J ( ˆR1 ˆR2)T ],
(6.149)
for the rank of the Wronskians of boundary conditions in (6.148), and the notation
(see (5.221), (5.220), and (3.52))
RI := max
λ∈I rank L(λ),
L(λ) := (R1 R2) /Z[0]
N+1(λ)0,
ˆRI := max
λ∈I rank ˆL(λ),
ˆL(λ) := ( ˆR1 ˆR2)
/
Z[0]
N+1(λ)
0
,
⎫
⎬
⎭
(6.150)
where Z[0](λ) is the fundamental matrix of the symplectic system in (6.148) such
that Z[0]
0 (λ) = I for λ ∈R and the interval I ⊆R can be of the form I = [a, b] or
I = (∞, b].
Remark 6.40
(i) Recall that by the monotonicity assumption for the matrix Sk(λ) (see (5.3)),
the functions rankL(λ) and rank ˆL(λ) are piecewise constant with respect to
λ ∈[a, b] and then the maxima in (6.150) are attained for some λ ∈[a, b].
Moreover, for the case I := (∞, b] we use (5.333) which guaranties that
rankL(λ) and rank ˆL(λ) are constant for all λ < λ0 = min{λ1, ˆλ1}, and then
for this case we evaluate RI and ˆRI according to
R(−∞,b] = R[λ,b],
ˆR(−∞,b] = R[λ,b],
λ < λ0.
(6.151)
(ii) For the case when the spectra are unbounded from above (recall that in this
subsection we assume only condition (5.333)), we introduce the notation
R := sup
λ∈R
rankL(λ) ≤2n,
ˆR := sup
λ∈R
rank ˆL(λ) ≤2n.
(6.152)

440
6
Miscellaneous Topics on Symplectic Systems
Theorem 6.41 Consider the pair of eigenvalue problems (6.148) under assump-
tions (5.2), (5.3), (5.152), (6.4), and (5.333). Then for a, b ∈R, a < b, we have
		# {ν ∈ˆσ | a < ν ≤b} −#{ν ∈σ | a < ν ≤b}
		
≤min{ra,b, ˆra,b, Rw} ≤min{RI, ˆRI, Rw} ≤2n,
I = [a, b],
ra,b := max{rankL(a), rank L(b)},
ˆra,b := max{rank ˆL(a), rank ˆL(b)}.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.153)
Proof The proof is based on Corollary 6.31 and Remark 6.32. The proof of (6.153)
is derived by analogy with the proof of Corollary 4.25 in Sect. 4.2.2. According
to (6.126) we have
#{ν ∈ˆσ | a < ν ≤b} −#{ν ∈σ | a < ν ≤b}
= #{ν ∈ˆσ | ν ≤b} −#{ν ∈ˆσ | ν ≤a} −#{ν ∈σ | ν ≤b} + #{ν ∈σ | ν ≤a}
= μ
 ˆVN+1⟨Z[0]
N+1(λ)⟩, ˆVN+1V−1
N+1(0 I)T 		b
a.
Then it follows that
				μ
 ˆVN+1⟨Z[0]
N+1(λ)⟩, ˆVN+1V−1
N+1(0 I)T 		b
a
				
≤max

μ
 ˆVN+1⟨Z[0]
N+1(λ)⟩, ˆVN+1V−1
N+1(0 I)T 
, λ ∈{a, b}
 
and by Theorem 3.5(vii) the comparative indices on the right-hand side of the above
inequality satisfy the estimate
μ
 ˆVN+1⟨Z[0]
N+1(λ)⟩, ˆVN+1V−1
N+1(0 I)T 
≤min{rankL(λ), Rw}.
(6.154)
Then we have that
		#{ν ∈ˆσ | a < ν ≤b} −#{ν ∈σ | a < ν ≤b}
		 ≤min{Rw, ra,b} ≤min{Rw, RI},
where ra,b is deﬁned by (6.153). Replacing the roles of the problems (6.148) (and
then interchanging VN+1 and ˆVN+1 in (6.154)), we have a similar estimate
		#{ν ∈ˆσ | a < ν ≤b} −#{ν ∈σ | a < ν ≤b}
		 ≤min{Rw, ˆra,b} ≤min{Rw, ˆRI}
with ˆra,b deﬁned by (6.153) for the second problem in (6.148). Finally, the
combination of the estimates above proves (6.153).
⊓⊔

6.2
Inequalities for Finite Eigenvalues
441
Using (5.333), which guaranties that the spectra of (6.148) are bounded from
below (see Corollary 5.123), one can formulate the following corollary to Theo-
rem 6.41.
Corollary 6.42 Under the assumptions of Theorem 6.41, we have for any b ∈R
		#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤b}
		 ≤˜K = min{R, ˆR, Rw} ≤2n
(6.155)
with the constants R, ˆR, and Rw given by (6.152) and (6.149). In particular,
inequalities (6.132) hold with K = ˆK := ˜K, where ˜K is deﬁned by (6.155).
Proof The proof of (6.155) follows from (6.153), where one can put a := λ with
λ < λ0 = min{λ1, ˆλ1}, and then by Proposition 6.33, we have that (6.132) hold
with K = ˆK := ˜K.
⊓⊔
Now we specify the results of Corollary 6.42 by imposing more restrictions on
the behavior of L(λ) and ˆL(λ) with respect to λ. In particular, the restrictions below
hold for problems (6.148) with the matrix Sk(λ), which is a polynomial in λ. For
this case L(λ) and ˆL(λ) are also polynomials. Note that the restrictions are also
satisﬁed for the special linear dependence in λ; see Sect. 5.3.
Theorem 6.43 Under the assumptions of Theorem 6.41, suppose additionally that
for R and ˆR given by (6.152) we have
R := sup
λ∈R
rank L(λ) = rankL(λ),
ˆR := sup
λ∈R
rank ˆL(λ) = rank ˆL(λ),
⎫
⎪⎬
⎪⎭
λ < λ0 = min{λ1, ˆλ1}.
(6.156)
Then
−ˆK ≤#{ν ∈σ | ν ≤b} −#{ν ∈ˆσ | ν ≤b} ≤K,
K := min{ ˜P, ˆR −Rw + ˜P} ≥0,
ˆK := min{Rw −˜P, R −˜P} ≥0,
ˆK + K ≤min{Rw, R, ˆR} ≤2n,
⎫
⎪⎬
⎪⎭
(6.157)
where the constant ˜P is deﬁned by (6.127). In particular, inequalities (6.132) for
the ﬁnite eigenvalues of (6.148) hold with K and ˆK given by (6.157).
Proof Under assumptions (6.156) we have for b ∈R
rank L(τ)
		b
λ ≤0,
rank ˆL(τ)
		b
λ ≤0,
λ < λ0.
(6.158)
Next we rewrite (6.126) in the form
#{ν ∈σ | ν ≤b} −#{ν ∈ˆσ | ν ≤b}
= ˜P −μ

ˆVN+1⟨Z[0]
N+1(b)⟩, ˆVN+1V−1
N+1(0 I)T 
.
⎫
⎬
⎭
(6.159)

442
6
Miscellaneous Topics on Symplectic Systems
Then we obtain
#{ν ∈σ | ν ≤b} −#{ν ∈ˆσ | ν ≤b} ≤˜P.
(6.160)
The difference in (6.159) can be also estimated using the relation
μ  ˆVN+1⟨Z[0]
N+1(λ)⟩, ˆVN+1V−1
N+1(0 I)T 
= Rw −rank ˆL(λ) + μ∗ ˆVN+1V−1
N+1(0 I)T , ˆVN+1⟨Z[0]
N+1(λ)⟩

,

(6.161)
where we apply Theorem 3.5(vi) and use the notation (6.149) and (6.150). Apply-
ing (6.161) to both addends in (6.159), we obtain
˜P −μ

ˆVN+1⟨Z[0]
N+1(b)⟩, ˆVN+1V−1
N+1(0 I)T 
= rank ˆL(τ)|b
λ + μ∗ ˆVN+1V−1
N+1(0 I)T , ˆVN+1⟨Z[0]
N+1(b)⟩
		λ
b
≤μ∗ ˆVN+1V−1
N+1(0 I)T , ˆVN+1⟨Z[0]
N+1(λ)⟩ = ˆR −Rw + ˜P,
λ < λ0,
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.162)
where λ0 = min{λ1, ˆλ1} and ˆR is given by (6.156). Then we also have
#{ν ∈σ | ν ≤b} −#{ν ∈ˆσ | ν ≤b} ≤ˆR −Rw + ˜P.
(6.163)
Combining (6.159) and (6.163), we derive the right estimate in (6.157). To prove the
left estimate, we replace the roles of the problems in the proof given above. Then
we have
#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤b} ≤ˆK := min{ ˆP, R −Rw + ˆP},
(6.164)
where by Theorem 3.5(ix)
ˆP := μ

VN+1⟨Z[0]
N+1(λ)⟩, VN+1 ˆV−1
N+1(0 I)T 
= Rw −μ
 ˆVN+1⟨Z[0]
N+1(λ)⟩, ˆVN+1V−1
N+1(0 I)T 
= Rw −˜P,
λ < λ0;
compare with (6.127). Using the last relation, we see that ˆK deﬁned in (6.164) is
the same as in (6.157). Finally, the estimate for the sum ˆK + K can be proved by
direct computations. Indeed, the values of K and ˆK depend on the signs of Rw −ˆR
and Rw −R, respectively. So we have
ˆK + K =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
R + ˆR −Rw, Rw > max{R, ˆR},
R,
R < Rw ≤ˆR,
ˆR,
R ≥Rw > ˆR,
Rw,
Rw ≤min{R, ˆR}.

6.2
Inequalities for Finite Eigenvalues
443
From the formula above, we derive the last estimate in (6.157). By Proposition 6.33
we then have that (6.132) hold with K and
ˆK given by (6.157). The proof is
completed.
⊓⊔
The last result for the case of joint endpoints concerns the strict inequalities
in (6.132).
Theorem 6.44 Under the assumptions of Theorem 6.43, suppose additionally
ˆR ≤Rw.
(6.165)
Then the ﬁrst inequality in (6.132) is strict, i.e., ˆλj < λj+K for j ≥1. Similarly, the
condition
R ≤Rw
(6.166)
implies that the second inequality in (6.132) is strict, i.e., λp < ˆλp+ ˆK for p ≥1.
Proof From (6.165) it follows that for K given by (6.157), we have for λ < λ0
K = ˆR −Rw + ˜P = μ∗ ˆVN+1V−1
N+1(0 I)T , ˆVN+1
/
Z[0]
N+1(λ)
0
.
On the other hand, according to (6.159) and (6.162), we have for λ < λ0
#{ν ∈σ | ν ≤b} −#{ν ∈ˆσ | ν ≤b}
= rank ˆL(τ)
		b
λ + μ∗ ˆVN+1V−1
N+1(0 I)T , ˆVN+1⟨Z[0]
N+1(b)⟩
		λ
b
= −ˆζ(b) + rank ˆL(τ)
		b−
λ + μ∗ ˆVN+1V−1
N+1(0 I)T , ˆVN+1⟨Z[0]
N+1(b)⟩
		λ
b
≤−ˆζ(b) + K,
where ˆζ(b) is the multiplicity of ˆλ = b ∈ˆσ, or ˆζ(b) = 0 if b /∈ˆσ, according to
Deﬁnition 5.54. That is,
ˆζ(λ) = rank ˆL(λ−) −rank ˆL(λ).
(6.167)
From the last estimate, we derive
#{ν ∈σ | ν ≤b} −#{ν ∈ˆσ | ν < b} ≤K,
or equivalently
#{ν ∈ˆσ | ν < b} −#{ν ∈σ | ν ≤b} ≥−K.
From the last inequality and Proposition 6.33, we obtain that ˆλj < λj+K for j ≥1.
The proof of the second strict inequality is similar.
⊓⊔

444
6
Miscellaneous Topics on Symplectic Systems
Remark 6.45
(i) It follows from the results of Corollary 6.42 that between the ﬁnite eigenvalues
λj−˜K, λj+ ˜K ∈σ (j >
˜K ≥0), there exists the ﬁnite eigenvalue ˆλj ∈ˆσ.
Similarly, between the ﬁnite eigenvalues ˆλp−˜K, ˆλp+ ˜K ∈ˆσ (p >
˜K ≥0),
there exists the ﬁnite eigenvalue λp
∈σ. By estimate (6.155) we have
˜K ≤2n, and then the distance between the indices of the ﬁnite eigenvalues
λj+ ˜K, λj−˜K ∈σ (or ˆλp+ ˜K, ˆλp−˜K ∈ˆσ) is equal to 2 ˜K ≤4n. At the same time,
by Theorem 6.43, similar arguments applied to λj−ˆK, λj+K ∈σ (j > ˆK ≥0)
imply that the distance between the indices of λj+K, λj−ˆK ∈σ (j > ˆK ≥0)
is much smaller than 4n, i.e., K + ˆK ≤2n (see (6.157)). A similar assertion
holds for ˆλp−K, ˆλp+ ˆK ∈ˆσ (p > K ≥0).
(ii) It follows from the formulas for the number ˆK in (6.157) that ˆK = 0 if and
only if the comparative index
μ
 ˆVN+1
/
Z[0]
N+1(λ)
0
, ˆVN+1V−1
N+1(0 I)T 
= ˜P,
λ < λ0,
attains one of its extremal values Rw or R (see property (vii) of Theorem 3.5
and (6.154)). For the case ˜P = R, we have ˆP = Rw −R ≥0 (see (6.164)), and
then the inequality λp < ˆλp (p ≥1) is strict according to the second assertion
in Theorem 6.44. A similar situation occurs for the constant K when changing
the roles of the problems in (6.148). In this case we have to demand ˜P = 0 or
˜P = Rw −ˆR ≥0. Note that for the last case the inequality ˆλj < λj (j ≥1) is
strict (see Theorem 6.44).
(iii) The simplest sufﬁcient condition for K = 0 in (6.157) is represented by the
majorant condition (6.146) (see Theorem 6.38) coupled with the condition ˆℓ=
0, where the constant ˆℓis deﬁned in Theorem 6.28 (see (6.122)). Indeed, by
Remark 6.39 and (6.127), these conditions are sufﬁcient for ˜P = 0 and then
also for K = 0.
(iv) One can construct an example of problems (6.148) with a piecewise continu-
ously differentiable matrix Sk(λ) such that conditions (6.156) do not hold (see
Example 5.103).
Next we present an example showing that estimates (6.157) are exact.
Example 6.46 Consider the eigenvalue problem for n = 2 and N = 4 with the
periodic boundary conditions
4yk+1 = Sk(λ)4yk,
4y0 = 4yN+1,
(6.168)
with the matrix Sk given by
Sk(λ) =
⎛
⎜⎜⎝
1 0 0 0
0 1 0 0
0 0 1 0
0 −λ 0 1
⎞
⎟⎟⎠
⎛
⎜⎜⎝
1 0 0 0
0 0 0 1
0 0 1 0
0 −1 0 0
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
1 0 0 0
0 0 0 1
0 0 1 0
0 −1 0 −λ
⎞
⎟⎟⎠.
(6.169)

6.2
Inequalities for Finite Eigenvalues
445
This eigenvalue problem is equivalent to the problem (see Remark 5.58)
4yk+1 = Sk(λ)4yk,
 0 0
−I I
  4x0
4xN+1

+
−I −I
0
0
  −4u0
4uN+1

= 0,
(6.170)
and consequently to the eigenvalue problem (5.180) with the separated endpoints
˜yk+1 = {Sk(λ)} ˜yk,
 0 0
−I I

˜x0 +
−I −I
0
0

˜u0 = 0,
 0 0
−I I

˜xN+1 +
−I −I
0
0

˜uN+1 = 0.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(6.171)
The matrix Yk = ⟨Zk⟩with Z0 = I is a conjoined basis of this system satisfying
the required boundary condition at k = 0. Hence, the matrix ˆL(λ) (see (5.221) and
Remark 5.58) for this problem is of the form
ˆL(λ) =
 0 0 −I −I
−I I
0
0

⟨ZN+1⟩= −J [ZN+1(λ) −I],
(6.172)
and the symplectic fundamental matrix of (6.168) is of the form
Z0(λ) = I,
Z1(λ) =
⎛
⎜⎜⎝
1 0 0 0
0 0 0 1
0 0 1 0
0 −1 0 −λ
⎞
⎟⎟⎠,
Z2(λ) =
⎛
⎜⎜⎝
1 0 0
0
0 −1 0
−λ
0 0 1
0
0 λ 0 −1 + λ2
⎞
⎟⎟⎠,
Z3(λ) =
⎛
⎜⎜⎝
1
0
0
0
0
λ
0 −1 + λ2
0
0
1
0
0 1 −λ2 0 2λ −λ3
⎞
⎟⎟⎠,
Z4(λ) =
⎛
⎜⎜⎝
1
0
0
0
0
1 −λ2
0
2λ −λ3
0
0
1
0
0 −2λ + λ3 0 1 −3λ2 + λ4
⎞
⎟⎟⎠,
Z5(λ) =
⎛
⎜⎜⎝
1
0
0
0
0
−2λ + λ3
0
1 −3λ2 + λ4
0
0
1
0
0 −1 + 3λ2 −λ4 0 −3λ + 4λ3 −λ5
⎞
⎟⎟⎠.

446
6
Miscellaneous Topics on Symplectic Systems
Then
rank [Z5(λ) −I] = rank T (λ),
T (λ) =

−2λ + λ3 −1
1 −3λ2 + λ4
−1 + 3λ2 −λ4 −3λ + 4λ3 −λ5 −1

, det T (λ) = (λ + 2)(λ2 −λ −1)2.
The spectrum ˆσ is given by the equation det T (λ) = 0, so that
ˆσ =

ˆλ1 = −2, ˆλ2 = ˆλ3 = 1
2 −
√
5
2 , ˆλ4 = ˆλ5 = 1
2 +
√
5
2
 
.
We compare the spectrum ˆσ of this problem with the spectrum σ of the following
eigenvalue problem
yk+1 = Sk(λ) yk,
u0 = xN+1, x0 = −uN+1.
(6.173)
This eigenvalue problem we can rewrite into the form
yk+1 = Sk(λ) yk,
 0 −I
−I
0
  x0
xN+1

+
−I
0
0 −I
  −u0
uN+1

= 0,
⎫
⎪⎬
⎪⎭
(6.174)
which is equivalent to the eigenvalue problem with separated boundary conditions
¯yk+1 = {Sk(λ)} ¯yk,
 0 0
−I I

¯x0 +
−I −I
0
0

¯u0 = 0,
 0 −I
−I
0

¯xN+1 +
−I
0
0 −I

¯uN+1 = 0.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(6.175)
The matrix L(λ) (see (5.221)), which we use to determine the ﬁnite eigenvalues, has
the form
L(λ) = J −ZN+1(λ).
(6.176)
Consequently, for N = 4 we have
rank[Z5(λ) −J ] = rank
⎛
⎜⎜⎝
1
0
−1
0
0 −2λ + λ3 0
−3λ2 + λ4
1
0
1
0
0 3λ2 −λ4
0 −3λ + 4λ3 −λ5
⎞
⎟⎟⎠
= 2 + rank
−2λ + λ3
−3λ2 + λ4
3λ2 −λ4 −3λ + 4λ3 −λ5

,

6.2
Inequalities for Finite Eigenvalues
447
with
det
−2λ + λ3
−3λ2 + λ4
3λ2 −λ4 −3λ + 4λ3 −λ5

= −2λ2(λ2 −3).
Hence, the spectrum is
σ =

λ1 = −
√
3, λ2 = 0, λ3 = 0, λ4 =
√
3
 
.
We have computed that the ﬁnite eigenvalues of (6.168), (6.173) satisfy
λp−2 < ˆλp, p ∈{3, 4, 5},
ˆλj < λj, j ∈{1, 2, 3, 4},
(6.177)
where we have additionally that
ˆλ2 = ˆλ3 < ˆλ4 = ˆλ5
(6.178)
and
λ2 = λ3.
(6.179)
Let us verify what we obtain as a result of the application of Theorems 6.41, 6.43,
and 6.44. The matrices VN+1 and ˆVN+1 of the boundary conditions have the form
ˆV−1
N+1 =
⎛
⎜⎜⎝
0
−0.5I I
0
0
0.5I I
0
−0.5I
0
0 −I
−0.5I
0
0 I
⎞
⎟⎟⎠,
V−1
N+1 =
⎛
⎜⎜⎝
0
−0.5I I
0
−0.5I
0
0
I
−0.5I
0
0 −I
0
−0.5I −I
0
⎞
⎟⎟⎠.
(6.180)
and Rw = 4, ˆR = 2, R = 4. By Theorem 6.41 (see (6.153)), in any half-open
interval (a, b], the number of ﬁnite eigenvalues of the investigated problems differs
at most by two. Since ˆR < Rw, we have K = ˆR −Rw + ˜P, and from R = Rw we
obtain ˆK = Rw −˜P. Consequently, ˆK + K = ˆR = 2. We show that K = 0. To this
end, note that
μ∗ ˆV−1
N+1(0 I)T , V−1
N+1(0 I)T 
= Rw −μ∗
V−1
N+1(0 I)T , ˆV−1
N+1(0 I)T 
= 4 −μ∗
2

V−1
N+1(0 I)T , ˆV−1
N+1(0 I)T 
= 4 −ind
-I 0
I 0
T 0 −I
−I
0
 I 0
I 0
.
= 4 −2 = 2.

448
6
Miscellaneous Topics on Symplectic Systems
Hence, the majorant condition (6.146) (see Remark 6.45(iii)) is not satisﬁed. Let us
compute the constant K. For λ < λ0 we have
K = ˆR −Rw + ˜P = μ∗ ˆVN+1V−1
N+1(0 I)T , ˆVN+1
/
Z5(λ)
0
= μ∗
⎛
⎜⎜⎝
⎛
⎜⎜⎝
I
I
−I
I
0.5I
0.5I
−0.5I 0.5I
⎞
⎟⎟⎠,
 J [I −Z5(λ)]
0.5 [I + Z5(λ)]

⎞
⎟⎟⎠
= μ∗
⎛
⎜⎜⎝
⎛
⎜⎜⎝
I
0
0
I
0.5I
0
0
0.5I
⎞
⎟⎟⎠C,
 J (I −Z5(λ))
0.5(I + Z5(λ))

⎞
⎟⎟⎠,
where the matrix C =
 I
I
−I I

is invertible and it can be neglected in view of the
property (i) of Theorem 3.5. Consequently, for λ < λ0 we have
K = ind ![I −Z5(λ)]T [I −Z5(λ)] −[I −Z5(λ)]T J T [I + Z5(λ)]"
= ind
!
[I −Z5(λ0)]T [I −Z5(λ)] + Z5(λ)TJ T + J Z5(λ)
"
= ind T1(λ),
where we have used that Zk(λ) is a symplectic matrix. The last index leads to the
computation of the index of the matrix
T1(λ) = AT A + B + BT ,
A :=
 2λ −λ3 + 1
−1 + 3λ2 −λ4
1 −3λ2 + λ4 3λ −4λ3 + λ5 + 1

,
B :=
−1 + 3λ2 −λ4 −3λ + 4λ3 −λ5
2λ −λ3
−1 + 3λ2 −λ4

.
We have that Tr T1(λ) ∼λ10, det T1(λ) ∼−4λ9 as λ →±∞. Hence, the
eigenvalues of T1(λ) with λ sufﬁciently close to −∞are positive. Then K = 0,
ˆK = 2, and by Theorem 6.44 we derive the same estimate (6.177). Finally, note
that Theorems 6.41, 6.43, and 6.44 do not provide the information in (6.178)
and (6.179). But if we know (6.178), then (6.177) gives the possibility to order
the ﬁnite eigenvalues of these problems as
ˆλ1 < λ1 < ˆλ2 = ˆλ3 < λ2 ≤λ3 < ˆλ4 = ˆλ5 < λ4.
So we see that this order is correct except of the fact that we cannot conclude
whether λ2 = λ3 or λ2 < λ3.

6.2
Inequalities for Finite Eigenvalues
449
6.2.3
Interlacing of Eigenvalues for Separated Endpoints
In this subsection we investigate the special case of (6.148) considering the
problems with the separated boundary conditions, whose coefﬁcient matrices do
not depend on λ, i.e.,
yk+1(λ) = Sk(λ) yk(λ),
k ∈[0, N]Z,
R∗
0 x0(λ) + R0 u0(λ) = 0,
R∗
N+1 xN+1(λ) + RN+1 uN+1(λ) = 0,
ˆyk+1(λ) = Sk(λ) ˆyk(λ),
k ∈[0, N]Z,
ˆR∗
0 ˆx0(λ) + ˆR0 ˆu0(λ) = 0,
ˆR∗
N+1 ˆxN+1(λ) + ˆRN+1 ˆuN+1(λ) = 0.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.181)
As in Sect. 6.2.2, we assume that the symplectic matrix Sk(λ) obeys the monotonic-
ity condition (5.3) and condition (5.333) holds. Then by Corollary 5.123, the spectra
σ and ˆσ of (6.148) are bounded from below.
Together with (6.181) we also consider two intermediate problems
˜yk+1(λ) = Sk(λ) ˜yk(λ),
k ∈[0, N]Z,
R∗
0 ˜x0(λ) + R0 ˜u0(λ) = 0,
ˆR∗
N+1 ˜xN+1(λ) + ˆRN+1 ˜uN+1(λ) = 0,
¯yk+1(λ) = Sk(λ) ¯yk(λ),
k ∈[0, N]Z,
ˆR∗
0 ¯x0(λ) + ˆR0 ¯u0(λ) = 0,
R∗
N+1 ¯xN+1(λ) + RN+1 ¯uN+1(λ) = 0,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.182)
under the same assumptions for Sk(λ), so that the ﬁnite spectra ˜σ, ¯σ of (6.182) are
also bounded from below. We introduce the notation for the ranks of the Wronskians
of the boundary conditions in (6.181) by
rL := rank

(R0 R∗
0) J ( ˆR0 ˆR∗
0)T 
,
rR := rank 
(RN+1 R∗
N+1) J ( ˆRN+1 ˆR∗
N+1)T ,

(6.183)
and the notation (see (5.205), (5.204), and Sect. 6.1.6)
rI := max
λ∈I rank(λ),
(λ) = (R∗
N+1 RN+1) Z[L]
N+1(λ) (0 I)T ,
ˆrI := max
λ∈I rank ˆ(λ),
ˆ(λ) = ( ˆR∗
N+1 ˆRN+1) ˆZ[L]
N+1(λ) (0 I)T ,
⎫
⎬
⎭
(6.184)

450
6
Miscellaneous Topics on Symplectic Systems
for Z[L]
k (λ) given by (6.102) and for similarly deﬁned ˆZ[L]
k (λ) with ˆZ[L]
0 (λ) = ˆV−1
with the constant matrix ˆV−1(λ) ≡ˆV−1 in (6.101). Finally, in estimates for the
ﬁnite eigenvalues of problems with the separated boundary conditions, we also use
maximal rank conditions for (6.182) with the spectra ˜σ and ¯σ in the form
˜rI := max
λ∈I rank ˜(λ),
˜(λ) = ( ˆR∗
N+1 ˆRN+1) Z[L]
N+1(λ) (0 I)T ,
¯rI := max
λ∈I rank ¯(λ),
¯(λ) = (R∗
N+1 RN+1) ˆZ[L]
N+1(λ) (0 I)T .
⎫
⎬
⎭
(6.185)
We also use the notation (see Remark 6.40(ii))
r := sup
λ∈R
rank (λ) ≤n,
ˆr := sup
λ∈R
rank ˆ(λ) ≤n,
˜r := sup
λ∈R
rank ˜(λ) ≤n,
¯r := sup
λ∈R
rank ¯(λ) ≤n.
⎫
⎬
⎭
(6.186)
Let λ0 be given by the condition
λ0 < min{λ1, ˆλ1, ˜λ1},
(6.187)
where ˜λ1 is the minimal ﬁnite eigenvalue of the ﬁrst problem in (6.182) (we put
˜λ1 := ∞if ˜σ = ∅). Now we present a modiﬁcation of Theorem 6.41 for the
separated endpoints.
Theorem 6.47 Consider
the
pair
of
eigenvalue
problems
(6.181)
under
assumptions (5.2), (5.3), (5.156) for all the matrices in the boundary conditions,
and (5.333). Then for any a, b ∈R with a < b, we have
		#{ν ∈ˆσ | a < ν ≤b} −#{ν ∈σ | a < ν ≤b}
		
≤min{rL, ˆra,b, ˜ra,b} + min{rR, ra,b, ˜ra,b}
≤min{˜rI, ˆrI, rL} + min{˜rI, rI, rR} ≤2n,
I := [a, b],
ra,b := max{rank(a), rank(b)},
ˆra,b := max{rank ˆ(a), rank ˆ(b)},
˜ra,b := max{rank ˜(a), rank ˜(b)},
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.188)
where ˜(λ) given by (6.185) is associated with the ﬁrst problem in (6.182).
Proof For the proof we use Remark 6.26. By (6.112) and similar arguments as in
the proof of Theorem 6.41, we derive
#{ν ∈ˆσ | a < ν ≤b} −#{ν ∈σ | a < ν ≤b}
= μ∗ ˆV −1
−1 ˆY [R]
0
(λ), ˆV −1
−1 V−1(0 I)T 		b
a
+μ
 ˆVN+1Y [L]
N+1(λ), ˆVN+1V −1
N+1(0 I)T 		b
a.
⎫
⎪⎪⎬
⎪⎪⎭
(6.189)

6.2
Inequalities for Finite Eigenvalues
451
Note that the rank of the Wronskian for the ﬁrst addend is
rank w( ˆV −1
−1 ˆY [R]
0
(λ), ˆV −1
−1 V−1(0 I)T )
= rankw((0 I)T , [ ˆZ[R]
0 (λ)]−1V−1(0 I)T )
= rankw((0 I)T , ˆVN+1Z[L]
N+1(λ)(0 I)T ) = rank ˜(λ) = ˜r(λ).
Then we estimate
				μ∗ ˆV −1
−1 ˆY [R]
0
(λ), ˆV −1
−1 V−1(0 I)T 		b
a
				
≤max{min{rL, ˜r(a)}, min{rL, ˜r(b)}} = min{rL, ˜ra,b},
and using the relation
μ∗ ˆV −1
−1 ˆY [R]
0
(λ), ˆV −1
−1 V−1(0 I)T 		b
a = μ∗
V −1
−1 ˆY [R]
0
(λ), V −1
−1 ˆV−1(0 I)T 		a
b,
(6.190)
where we apply Theorem 3.5(ix), we derive a similar estimate
				μ∗ ˆV −1
−1 ˆY [R]
0
(λ), ˆV −1
−1 V−1(0 I)T 		b
a
				 =
				μ∗
V −1
−1 ˆY [R]
0
(λ), V −1
−1 ˆV−1(0 I)T 		a
b
				
≤max{min{rL, ˆr(a)}, min{rL, ˆr(b)}} = min{rL, ˆra,b}.
Combining the estimates derived above, we obtain the estimate for the ﬁrst addend
in (6.189) in the form
				μ∗ ˆV −1
−1 ˆY [R]
0
(λ), ˆV −1
−1 V−1(0 I)T 		b
a
				 ≤min{˜ra,b, ˆra,b, rL}.
(6.191)
In a similar way, we derive the estimate for the second addend in (6.189), so that
ﬁnally we prove (6.188).
⊓⊔
By analogy with the consideration for the case of the joint endpoints, we recall
that (5.333) guaranties that the spectra of (6.181) and (6.182) are bounded from
below (see Corollary 5.118). Then one can formulate the following corollary to
Theorem 6.47.
Corollary 6.48 Under the assumptions of Theorem 6.47, we have for any b ∈R
		#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤b}
		 ≤˜K,
˜K := min{ˆr, ˜r, rL} + min{r, ˜r, rR} ≤2n,

(6.192)

452
6
Miscellaneous Topics on Symplectic Systems
with the constants r, ˆr, ˜r, rL, rR given by (6.186) and (6.183). In particular,
inequalities (6.132) hold with K = ˆK := ˜K, where ˜K is deﬁned by (6.192).
Now we specify the results of Corollary 6.48 by imposing more restrictions on
the behavior of (λ), ˆ(λ), ˜(λ) with respect to λ. For example, the restrictions
below hold for problems (6.181) with the matrix Sk(λ) being a polynomial in λ, in
particular, for problems with the special linear dependence in λ (see Sect. 5.3).
Theorem 6.49 Under the assumptions of Theorem 6.47, suppose additionally that
for r, ˆr, ˜r given by (6.186) we have
r := sup
λ∈R
rank(λ) = rank (λ),
ˆr := sup
λ∈R
rank ˆ(λ) = rank ˆ(λ),
˜r := sup
λ∈R
rank ˜(λ) = rank ˜(λ),
λ < λ0 = min{λ1, ˆλ1, ˜λ1}.
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
(6.193)
Then
ˆK ≤#{ν ∈σ | ν ≤b} −#{ν ∈ˆσ | ν ≤b} ≤K,
K = K1 + K2 := min{P1, ˆr −rL + P1} + min{P2, ˜r −rR + P2} ≥0,
ˆK = ˆK1 + ˆK2 := min{rL −P1, ˜r −P1} + min{rR −P2, r −P2} ≥0,
ˆK + K ≤min{rL, ˆr, ˜r} + min{rR, r, ˜r} ≤2n,
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.194)
where the constants P1 and P2 are given by (6.113). In particular, inequali-
ties (6.132) for the ﬁnite eigenvalues of (6.181) hold with K and ˆK given by (6.194).
Proof In the proof we follow the approach developed in the proof of Theorem 6.43.
Under assumptions (6.193) we have for b ∈R
rank(τ)
		b
λ ≤0,
rank ˆ(τ)
		b
λ ≤0,
rank ˜(τ)
		b
λ ≤0,
λ < λ0.
(6.195)
Next we rewrite (6.112) and (6.113) in the form
#{ν ∈σ | ν ≤b} −#{ν ∈ˆσ | ν ≤b}
= μ∗ ˆV −1
−1 ˆY [R]
0
(τ), ˆV −1
−1 V−1(0 I)T 		λ
b
+μ
 ˆVN+1Y [L]
N+1(τ), ˆVN+1V −1
N+1(0 I)T 		λ
b,
λ < λ0.
⎫
⎪⎪⎬
⎪⎪⎭
(6.196)
Then we have for the addends in (6.196) (compare with (6.160))
μ∗ ˆV −1
−1 ˆY [R]
0
(τ), ˆV −1
−1 V−1(0 I)T 		λ
b ≤P1,
μ
 ˆVN+1Y [L]
N+1(τ), ˆVN+1V −1
N+1(0 I)T 		λ
b ≤P2.

(6.197)

6.2
Inequalities for Finite Eigenvalues
453
On the other hand, the comparative indices above can be rewritten in the form
(compare with (6.161))
μ∗ ˆV −1
−1 ˆY [R]
0
(λ), ˆV −1
−1 V−1(0 I)T 
= rL −rank ˆ(λ) + μ
 ˆV −1
−1 V−1(0 I)T , ˆV −1
−1 ˆY [R]
0
(λ)

,
μ
 ˆVN+1Y [L]
N+1(λ), ˆVN+1V −1
N+1(0 I)T 
= rR −rank ˜(λ) + μ∗ ˆVN+1V −1
N+1(0 I)T , ˆVN+1Y [L]
N+1(λ),
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.198)
where we apply Theorem 3.5(vi). Using (6.198) and (6.195), we have for the
addends in (6.196) for λ < λ0 that
μ∗ ˆV −1
−1 ˆY [R]
0
(τ), ˆV −1
−1 V−1(0 I)T 		λ
b
= rank ˆ(τ)
		b
λ + μ ˆV −1
−1 V−1(0 I)T , ˆV −1
−1 ˆY [R]
0
(τ)		λ
b
≤μ ˆV −1
−1 V−1(0 I)T , ˆV −1
−1 ˆY [R]
0
(λ) = ˆr −rL + P1.
So we have proved
μ∗ ˆV −1
−1 ˆY [R]
0
(τ), ˆV −1
−1 V−1(0 I)T 		λ
b ≤ˆr −rL + P1,
λ < λ0.
(6.199)
In a similar way we prove the estimate for the second addend in (6.196)
μ
 ˆVN+1Y [L]
N+1(τ), ˆVN+1V −1
N+1(0 I)T 		λ
b ≤˜r −rR + P2,
λ < λ0.
(6.200)
Combining (6.197), (6.199), and (6.200), we derive the upper estimate in (6.194)
with the constant K. To prove the lower estimate, we rewrite (6.112) in the
equivalent form
#{ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤b}
= μ∗V −1
−1 ˆY [R]
0
(τ), V −1
−1 ˆV−1(0 I)T 		λ
b
+μ

VN+1Y [L]
N+1(τ), VN+1 ˆV −1
N+1(0 I)T 		λ
b,
λ < λ0,
⎫
⎪⎪⎬
⎪⎪⎭
(6.201)
which is derived using (6.190) for the ﬁrst addend in (6.112) and a similar relation
μ  ˆVN+1Y [L]
N+1(τ), ˆVN+1V −1
N+1(0 I)T 		b
a
= μ

VN+1Y [L]
N+1(τ), VN+1 ˆV −1
N+1(0 I)T 		a
b

(6.202)
for the second one (recall that we apply Theorem 3.5(ix)). Repeating the same
arguments as in the proof of the upper estimate and using formula (6.201) instead

454
6
Miscellaneous Topics on Symplectic Systems
of (6.196), we derive (compare with (6.164))
# {ν ∈ˆσ | ν ≤b} −#{ν ∈σ | ν ≤b}
≤ˆK = min{ ˆP1, ˜r −rL + ˆP1} + min{ ˆP2, r −rR + ˆP2},
λ < λ0,

(6.203)
where ˆP1 = rL −P1 and ˆP2 = rR −P2 by Theorem 3.5(ix). Substituting the last
representations into (6.203), we see that the constant ˆK in (6.203) is the same as
in the lower estimate in (6.194). The upper bound of the sum K + ˆK is derived by
analogy with the proof of the similar estimate in Theorem 6.43 (see (6.157)). By
Proposition 6.33 we have that (6.132) hold with K and ˆK given by (6.194). The
proof is completed.
⊓⊔
It follows from Theorem 6.49 that the ﬁnite eigenvalues of the intermediate
problem in (6.182) with the spectrum ˜σ separates the ﬁnite eigenvalues of (6.181).
So we have the following corollary.
Corollary 6.50 Under the assumptions of Theorem 6.49, suppose that there exists
the ﬁnite eigenvalue λj+K ∈σ (j ≥1), where K ≥0 is deﬁned in (6.194). Then
there exist the ﬁnite eigenvalues ˜λj+K1 ∈˜σ and ˆλj ∈ˆσ and the inequality
ˆλj ≤˜λj+K1 ≤λj+K,
(6.204)
holds with K1 = min{P1, ˆr −rL + P1}, where K1 is the ﬁrst addend in the sum
K = K1 + K2 given by (6.194). Similarly, if ˆλp+ ˆK (p ≥1) exists, then there exist
˜λp+ ˆK2 ∈˜σ and λp ∈σ, and we have the inequality
λp ≤˜λp+ ˆK2 ≤ˆλp+ ˆK,
(6.205)
where ˆK2 is the second addend in the sum ˆK = ˆK1 + ˆK2 given by (6.194).
Proof Replacing σ by ˜σ in (6.196), we get
#{ν ∈˜σ | ν ≤b} −#{ν ∈ˆσ | ν ≤b}
= μ∗ ˆV −1
−1 ˆY [R]
0
(τ), ˆV −1
−1 V−1(0 I)T 		λ
b,
λ ≤λ0.

(6.206)
i.e., we cancel the second addend in (6.196). For the ﬁrst addend in (6.196), we have
derived the estimates (6.197) and (6.199). That is, we have the inequality
#{ν ∈˜σ | ν ≤b} −#{ν ∈ˆσ | ν ≤b} ≤K1,
or equivalently the inequality
#{ν ∈ˆσ | ν ≤b} −#{ν ∈˜σ | ν ≤b} ≥−K1 ≥0.

6.2
Inequalities for Finite Eigenvalues
455
Then, by Proposition 6.33 we have ˆλj ≤˜λj+K1 provided ˜λj+K1 ∈˜σ exists. Next
we replace ˆσ by ˜σ in (6.196) to get
#{ν ∈σ | ν ≤b} −#{ν ∈˜σ | ν ≤b}
= μ ˆVN+1Y [L]
N+1(τ), ˆVN+1V −1
N+1(0 I)T 		λ
b,
λ ≤λ0.

(6.207)
i.e., we cancel the ﬁrst addend in (6.196). Then, arguing as above we derive the
inequality ˜λp ≤λp+K2, where K2 is the second addend in the sum for K = K1+K2
(see (6.194)). Applying the last inequality to the index p := j + K1, we derive
ˆλj ≤˜λj+K1 ≤λj+K or (6.204). For the proof of (6.205), we use (6.201). For the
ﬁrst step, we replace σ by ˜σ deriving ˜λj ≤ˆλj+ ˆK1. For the second step, we replace
ˆσ by ˜σ and derive λp ≤˜λp+ ˆK2. By putting j := p + ˆK2 we derive (6.205). The
proof is completed.
⊓⊔
Next we formulate sufﬁcient conditions for the strict inequalities in (6.132).
Theorem 6.51 Under the assumptions of Theorem 6.49, suppose that one of the
conditions
ˆr ≤rL
or
˜r ≤rR
(6.208)
holds. Then we have instead of (6.204) the inequalities
ˆλj < ˜λj+K1 ≤λj+K
or
ˆλj ≤˜λj+K1 < λj+K,
(6.209)
and hence the strict inequality ˆλj < λj+K in (6.132) holds. Similarly, if one of the
conditions
˜r ≤rL
or
r ≤rR
(6.210)
holds, then
λp ≤˜λp+ ˆK2 < ˆλp+ ˆK
or
λp < ˜λp+ ˆK2 ≤ˆλp+ ˆK,
(6.211)
and we have the strict inequality λp < ˆλp+ ˆK in (6.132).
Proof If ˆr ≤rL holds, then using (6.206) and the same arguments as in case
of general boundary conditions (see the proof of Theorem 6.44), we show that
ˆλj < ˜λj+K1. Then by combining this estimate with (6.204), we prove the ﬁrst
inequality in (6.209). Similarly, using ˜r ≤rR and (6.207), by analogy with the
proof of Theorem 6.44, we derive ˜λp < λp+K2. Then, by putting p := j + K1 and
combining this inequality with (6.204), we derive the second inequality in (6.209).
Inequalities (6.211) and λp < ˆλp+ ˆK, which are connected to at least one of the
conditions in (6.210), can be proved analogously by using (6.201).
⊓⊔

456
6
Miscellaneous Topics on Symplectic Systems
Remark 6.52
(i) Observe that the inequalities of Theorem 6.43 for the joint endpoints are
improved by formulas of Theorem 6.49 for the separated boundary conditions.
Indeed, we have the estimate
min{P1, ˆr −rL + P1} + min{P2, ˜r −rR + P2}
≤min{P1 + P2, ˆr + n −(rL + rR) + P1 + P2}
= min{ ˜P, ˆR −Rw + ˜P},
where the constants ˜P, ˆR, Rw, determined by formulas (6.127), (6.152),
(6.149), are evaluated for the case, when the boundary conditions are separated.
For the proof we use Rw = rL + rR (see (5.154)) and
rank ˆL(λ) = n + rank ˆ(λ),
(6.212)
where ˆL(λ) in (5.221) is computed for the case of the separated boundary
conditions and ˆ(λ) is deﬁned by (5.205). Indeed, for this case we have
by (5.439) and (3.105)
ˆVN+1⟨Z[0]
N+1(λ)⟩= ⟨ˆVN+1Z[0]
N+1(λ) ˆV−1⟩ˆV −1
−1 ,
and then using the relation rank ˆL(λ) = rank(I 0) ⟨ˆVN+1Z[0]
N+1(λ) ˆV−1⟩, we
derive (6.212). Moreover, by using Lemma 3.39 (see (3.115)), one can show
that the number ˜P given by (6.127) for the case of the separated boundary
conditions takes the form ˜P = P1 + P2 with the constants P1 and P2 given
by (6.113). Similarly,
min{rL −P1, ˜r −P1} + min{rR −P2, r −P2}
≤min{rL + rR −P1 −P2, n + r −P1 −P2}
= min{Rw −˜P, R −˜P}.
Consequently, inequalities of Theorem 6.43 are improved by formulas of
Theorem 6.49 for separated boundary conditions.
(ii) Observe that the consideration above was associated with the ﬁrst intermediate
eigenvalue problem in (6.182) with the spectrum ˜σ. But for the case of ¯r < ˜r,
the estimates in Theorem 6.49 can be improved by using the second problem
in (6.182) with the spectrum ¯σ. For the derivation of the estimates associated
with this problem, it is sufﬁcient to replace the roles of problems in (6.181),
i.e., replace σ by ˆσ and vice versa.
(iii) As it was already used in the proof of Corollary 6.50, the results in Theo-
rems 6.47, 6.49, and 6.51 and in Corollaries 6.48 and 6.50 can be applied to

6.2
Inequalities for Finite Eigenvalues
457
the case when the problems in (6.181) differ in only one boundary condition.
For example, if we consider the ﬁrst problem in (6.181) with the spectrum σ
and the ﬁrst problem in (6.182) with the spectrum ˜σ (instead of the second
problem in (6.181)), then in all estimates of Theorems 6.47, 6.49, and 6.51 and
Corollaries 6.48 and 6.50, we have rL = 0, P1 = 0, K1 = 0, and then the ﬁrst
addend is zero. A similar situation will arise if we consider the second problem
in (6.181) with the spectrum ˆσ and replace the ﬁrst problem with the spectrum
σ by the problem with the spectrum ˜σ. In this case rR = 0, P2 = 0, K2 = 0,
and then in all estimates the second addend is zero.
For the sake of convenience (see also Remark 6.52(iii)), we reformulate Theo-
rems 6.49 and 6.51 for a pair of eigenvalue problems (6.181) with the same boundary
condition at k = 0 but different boundary conditions at k = N + 1, i.e.,
yk+1(λ) = Sk(λ) yk(λ),
k ∈[0, N]Z,
R∗
0 x0(λ) + R0 u0(λ) = 0,
R∗
N+1 xN+1(λ) + RN+1 uN+1(λ) = 0,
˜yk+1(λ) = Sk(λ) ˜yk(λ),
k ∈[0, N]Z,
R∗
0 ˜x0(λ) + R0 ˜u0(λ) = 0,
ˆR∗
N+1 ˜xN+1(λ) + ˆRN+1 ˜uN+1(λ) = 0.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.213)
Theorem 6.53 Let the parameters rR, r, ˜r for eigenvalue problems (6.213) are
determined by (6.183)–(6.186) and for λ0 satisfying (6.187) (where we put ˆλ1 = ˜λ1)
assumption (6.193) for r and ˜r holds. Then we have the estimates
ˆK2 ≤#{ν ∈σ | ν ≤b} −#{ν ∈˜σ | ν ≤b} ≤K2,
K2 := min{P2, ˜r −rR + P2},
ˆK2 := min{rR −P2, r −P2},
ˆK2 + K2 ≤min{rR, r, ˜r} ≤n,
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.214)
where P2 is determined by (6.113) for ˆσ := ˜σ. In particular, if there exists the ﬁnite
eigenvalue λj+K2 ∈σ (j ≥1) of the ﬁrst problem in (6.213), then there exists the
ﬁnite eigenvalue ˜λj ∈˜σ of the second one with ˜λj ≤λj+K2. If
˜r ≤rR
(6.215)
holds, then we have the strict inequality ˜λj < λj+K2. Similarly, the existence of
˜λp+ ˆK2 ∈˜σ implies that of λp ∈σ with the inequality λp ≤˜λp+ ˆK2. Moreover, if
r ≤rR
(6.216)
holds, we have the strict inequality λp < ˜λp+ ˆK2.

458
6
Miscellaneous Topics on Symplectic Systems
Remark 6.54 We note that Theorem 6.53 can be applied to derive the interlacing
properties of the ﬁnite eigenvalues of problem (E) considered on the intervals
[0, N +1]Z and [0, N +2]Z. Since the theory developed in this subsection is devoted
to the case, when the matrices in the boundary conditions do not depend on λ,
we impose the restriction (6.35) for the blocks Ak(λ) and Bk(λ) (see Sect. 6.1.3),
i.e., Ak(λ) ≡Ak and Bk(λ) ≡Bk. In this case problem (E) considered on the
subsequent intervals [0, N + 1]Z and [0, N + 2]Z is equivalent to the special case
of (6.213) on the interval [0, N + 1]Z with the matrices
R0 = RN+1 = 0,
R∗
0 = R∗
N+1 = I,
ˆR∗
N+1 := AN+1,
ˆRN+1 := BN+1.
For this case we have rR = rank BN+1 and VN+1 = I. By (6.113) the constant
P2 in Theorem 6.53 takes the form P2 = mN+1(λ) for λ < λ0, where mN+1(λ)
is the multiplicity of a forward focal point of the principal solution Y [0](λ) of the
symplectic system in (6.213). In particular, for the nonsingular block BN+1, inequal-
ities (6.215) and (6.216) are necessary satisﬁed, and then, by Theorem 6.53, we have
strict interlacing properties for the ﬁnite eigenvalues of problem (E) considered on
the subsequent intervals [0, N + 1]Z and [0, N + 2]Z (see Example 6.55).
Example 6.55 Consider the Sturm-Liouville problem (6.9) on [0, N + 1]Z and the
same problem on [0, N + 2]Z, i.e., with the Dirichlet boundary conditions x0 = 0 =
xN+2. Then, according to Remark 6.54, we have rR = rank BN+1 = 1, P2 = 0,
˜r = r = 1, and K2 = 0, ˆK2 = 1. Conditions (6.215) and (6.216) are satisﬁed and
imply the strict inequalities
λN+2
j
< λN+1
j
< λN+2
j+1 .
These inequalities are in full agreement with the classical results (see, e.g., [28,
Chapter 4]) concerning the separation of roots of the polynomials det X[0]
N+1(λ)
and det X[0]
N+2(λ). In a similar way, consider the Dirichlet eigenvalue problems on
[0, N + 1]Z and [0, N + 2]Z for the matrix Sturm-Liouville equation
−(Rkxk) + Pkxk+1 = λWkxk+1,
x0 = 0 = xN+1,
(6.217)
with det Rk ̸= 0 and Wk > 0. Then, by Remark 6.54 and Corollary 5.69, we have
P2 = 0, rR = rank BN+1 = n, and ˜r = r = n. Conditions (6.215) and (6.216)
(sufﬁcient for the strict inequalities) are then satisﬁed, K2 = 0, ˆK2 = n, and
λN+2
j
< λN+1
j
< λN+2
j+n .
Next we show an example of spectra of two eigenvalue problems with separated
boundary conditions, for which estimates (6.194) are exact.

6.2
Inequalities for Finite Eigenvalues
459
Example 6.56 Consider the two problems with the matrix Sk(λ) deﬁned in (6.169)
and with the separated boundary conditions on [0, N + 1]Z in the form
u0 = 0 = xN+1,
(6.218)
x0 = 0 = uN+1.
(6.219)
Then for problem (6.169), (6.218) with the spectrum σ, we have V−1 = J and
VN+1 = I and (λ) = (I 0) Z5(λ) (I 0)T . According to Example 6.46 with N = 4
(see the formula for the fundamental matrix Z5(λ)), we have
rank(λ) = 1 + rank(−2λ + λ3),
and then
σ =
!
λ1 = −
√
2, λ2 = 0, λ3 =
√
2
"
.
Similarly, for problem (6.169), (6.219) with the spectrum ˆσ, we have ˆV−1 = I
and ˆVN+1 = J and ˆ(λ) = (0 I) Z5(λ) (0 I)T . According to Example 6.46 with
N = 4,
rank ˆ(λ) = 1 + rank (−3λ + 4λ3 −λ5),
and then
ˆσ = !ˆλ1 = −
√
3, ˆλ2 = −1, ˆλ3 = 0, ˆλ4 = 1, ˆλ5 =
√
3".
Let us compare the spectra σ and ˆσ. It is easy to see that
λj−2 < ˆλj (j ∈{3, 4, 5}),
ˆλp < λp (p ∈{1, 2, 3}).
(6.220)
Now we apply the general theory developed for separated boundary conditions. For
the intermediate eigenvalue problems deﬁned by (6.182), we have the boundary
conditions
u0 = 0 = uN+1,
(6.221)
x0 = 0 = xN+1.
(6.222)
According to formula for Z5(λ) in Example 6.46, the spectra of the intermediate
problems (6.169), (6.221) (with the spectrum ˜σ) and (6.169), (6.222) (with the
spectrum ¯σ) can be found by using the functions ˜(λ) = (0 I) Z5(λ) (I 0)T
and ˜(λ) = (I 0) Z5(λ) (0 I), respectively, which have the same maximal ranks
˜r = ¯r = 1 (and the same spectra ˜σ = ¯σ). Finally, we have rL = rR = 2, r = ˆr = 2,
˜r = 1, and by Theorem 6.49 then K = P1 + ˜r −rR + P2 = P1 + P2 −1, where the

460
6
Miscellaneous Topics on Symplectic Systems
constants P1 and P2 can be computed by using (6.113). Namely, we have
P1 = μ∗
Z−1
5 (λ) (I 0)T , (I 0)T 
= ind [(1 −3λ + λ4) (−3λ + 4λ3 −λ5)]
= 0
for λ →−∞,
and
P2 = μ

J Z5(λ) (I 0)T , (I 0)T 
= 1 + ind [(−1 + 3λ2 −λ4) (−2λ + λ3)]
= 1
for
λ →−∞.
Then K = 0, and hence, by Theorem 6.51, the inequality ˆλp < λp (p ∈{1, 2, 3})
holds. Next, by (6.194) we have ˆK = 2, and then λj−2 < ˆλj (j ∈{3, 4, 5}). Thus,
we have proved that (6.220) holds by Theorems 6.49 and 6.51. Observe also that the
sum of the comparative indices in (6.113) is
μ
 ˆV−1(0 I)T, V−1(0 I)T 
+ μ∗ ˆV −1
N+1(0 I)T, V −1
N+1(0 I)T 
= 2,
so that the majorant condition (6.137) used in Sect. 6.2.1 is not satisﬁed.
6.3
Symplectic Systems Without Controllability
In this section we will consider symplectic difference system (SDS) on a discrete
interval of the form [0, ∞)Z, which is bounded from below and unbounded from
above and for which we do not impose any controllability assumption. This assump-
tion was used in Sect. 2.5.1 in order to guarantee the existence of the recessive
solution of (SDS) at inﬁnity. We will see in this section that it is possible to develop
the theory of recessive and dominant solutions at inﬁnity without the controllability
assumption. The organization of this section is the following. In Sect. 6.3.1 we
discuss the order of abnormality of system (SDS). In Sect. 6.3.2 we introduce
basic notation and derive main properties of conjoined bases of nonoscillatory
system (SDS) at ∞. The content of Sects. 6.3.3–6.3.5 can be considered as technical
results, which are needed for the construction and classiﬁcation of recessive and
dominant solutions of (SDS) at ∞. These three subsections can be skipped for
the ﬁrst reading when one aims to see the main results ﬁrst. The two central
objects of this section, i.e., the recessive and dominant solutions of (SDS) at ∞,
are introduced in Sects. 6.3.6 and 6.3.7, respectively. In Sect. 6.3.8 we introduce the
notion of a genus of conjoined bases of (SDS) and study the existence of recessive
and dominant solutions of (SDS) at ∞in different genera of conjoined bases.
This provides a basis for the investigation of limit properties of the recessive and
dominant solutions of (SDS) at ∞in Sect. 6.3.9. In Sect. 6.3.10 we present a special

6.3
Symplectic Systems Without Controllability
461
construction of the (unique) minimal recessive solution of (SDS) at ∞, which can
be potentially useful for applications in the oscillation and spectral theory of (SDS).
6.3.1
Order of Abnormality
For N ∈[0, ∞)Z we denote by [N, ∞)Z the linear space of n-vector sequences
u = {uk}∞
k=N such that uk+1 = Dkuk and Bkuk = 0 on [N, ∞)Z. This means
that u ∈[N, ∞)Z if and only if the pair (x ≡0, u) solves (SDS) on [N, ∞)Z.
Moreover, we denote by 0[N, ∞)Z the subspace of Rn consisting of the initial
values uN of the elements u ∈[N, ∞)Z. Then the number
d[N, ∞)Z := dim [N, ∞)Z = dim 0[N, ∞)Z,
0 ≤d[N, ∞)Z ≤n,
(6.223)
is called the order of abnormality of (SDS) on the interval [N, ∞)Z. It is obvious that
if (x ≡0, u) solves (SDS) on [N1, ∞)Z, then it solves (SDS) also on [N2, ∞)Z for
every N2 ∈[N1, ∞)Z. This shows that [N1, ∞)Z ⊆[N2, ∞)Z for all N1 ≤N2,
i.e., the function d[k, ∞)Z is nondecreasing in k on [0, ∞)Z. Hence, the limit
d∞:= lim
k→∞d[k, ∞)Z = max
!
d[k, ∞)Z, k ∈[0, ∞)Z
"
,
0 ≤d∞≤n,
(6.224)
exists and is called the maximal order of abnormality of (SDS). In particular, the
subspace [N, ∞)Z with d[N, ∞)Z = d∞satisﬁes
[N, ∞)Z = [K, ∞)Z
for every K ∈[N, ∞)Z.
(6.225)
Note that for an eventually controllable system (SDS), we have d∞= 0. The
number d∞is one of the important parameters of system (SDS). In a similar
way to (6.223), we deﬁne the number d[N, L]Z as the order of abnormality of
system (SDS) on the interval [N, L]Z with the associated subspaces [N, L]Z and
0[N, L]Z. Also, for convenience we set [N, N]Z = Rn = 0[N, N]Z for the
case of L = N.
The following result provides a basic connection between the subspaces
0[N, k] for k ∈[N, ∞)Z and the principal solution Y [N] of (SDS). We recall
that the solution Y [N] is deﬁned by the initial conditions Y [N]
N
=
 0
I

.
Theorem 6.57 For any N ∈[0, ∞)Z, we have
0[N, k]Z =
6
j∈[N,k]
Ker X[N]
j
for every k ∈[N, ∞)Z,
(6.226)
d[N, ∞)Z = dim
6
k∈[N,∞)Z
Ker X[N]
k
.
(6.227)

462
6
Miscellaneous Topics on Symplectic Systems
Proof For k = N the equality in (6.226) holds trivially. Let k ≥N + 1. If c ∈
0[N, k], then y = (x ≡0, u) is a solution of (SDS) on [N, k −1]Z for some
u ∈[N, k] with uN = c. By the uniqueness of solutions of system (SDS), it
follows that y = Y [N]c on [N, k]. Hence, X[N]
j
c = 0 for all j ∈[N, k]. The
opposite direction is trivial. Equality (6.227) then follows from (6.226) and (6.223).
⊓⊔
Remark 6.58 We note that when system (SDS) is eventually controllable (in the
sense of Sect. 2.5.1) and nonoscillatory at ∞, then for every N
∈[0, ∞)Z
the principal solution Y [N] has the matrix X[N]
k
invertible for large k. Therefore,
Theorem 6.57 implies that 0[N, k]Z = {0} for large k and consequently d∞= 0.
6.3.2
Nonoscillatory Symplectic System
In this section we will use special symplectic fundamental matrices corresponding,
for a given index j ∈[0, ∞)Z, to the principal solution Y [j] of (SDS). More
precisely, we denote
[j]
k
=

Y [j]
k
¯Y [j]
k

,
k ∈[0, ∞)Z,
w(Y [j], ¯Y [j]) = I,
(6.228)
where ¯Y [j] is the conjoined basis of (SDS) such that w(Y [j], ¯Y [j]) = I, that is,
¯Y [j]
j
= −J (0 I)T = (−I 0)T . This means that the fundamental matrix in (6.228)
satisﬁes the normalization condition [j]
j
= −J . Note that the fundamental matrix
[j]
k
corresponds to the symplectic matrix ˜Z in Lemma 1.58(vi). We shall represent
conjoined bases of (SDS) in terms of the matrix [j]
k .
Lemma 6.59 Let j ∈[0, ∞)Z be ﬁxed, and let the symplectic fundamental matrix
[j]
k
of (SDS) be deﬁned by (6.228). Then for every conjoined basis Y of (SDS),
there exists a unique constant 2n × n matrix Dj such that
Yk = [j]
k Dj,
k ∈[0, ∞)Z,
JDj =
w(Y [j], Y)
w( ¯Y [j], Y)

.
(6.229)
Proof The form of the matrix Dj in (6.229) follows directly from the expression
Dj
= −J ([j]
k )TJ Yk by using the inversion formula S−1 = −J STJ for
a symplectic matrix (see Lemma 1.58(ii)) and from the constancy of the Wronskian
of two solutions of (SDS).
⊓⊔
Later in Sects. 6.3.11, 6.4.2, and 6.4.3, we will extend these considerations
to symplectic fundamental matrices of (SDS) involving the (minimal) recessive
solutions of (SDS) at ±∞, i.e., we will use the notation in (6.228) and the result
of Lemma 6.59 with j = ±∞.

6.3
Symplectic Systems Without Controllability
463
By using Lemma 6.59 and the comparative index, we can derive the following
additional formulas for the multiplicities of focal points.
Remark 6.60 Let Y be a conjoined basis of (SDS), and let Y [j] and Y [j+1] be the
principal solutions of (SDS) at j and j + 1. Then we have
m(j) = μ

JDj+1, JD[j]
j+1

,
m∗(j) = μ∗
JDj , JD[j+1]
j

,
(6.230)
where Dj+1 and D[j]
j+1 are the representation matrices in (6.229) of Y and Y [j] in
terms of the symplectic fundamental matrix [j+1] of (SDS), while Dj and D[j+1]
j
are the representation matrices in (6.229) of Y and Y [j+1] in terms of the symplectic
fundamental matrix [j]. Indeed, since [j+1]
j+1
= −J = [j]
j , we have
JDj+1 = ([j+1]
k
)TJ Yk = −Yj+1,
JD[j]
j+1 = ([j+1]
k
)TJ Y [j]
k
= −Y [j]
j+1
(6.231)
(when evaluated at k = j + 1), and similarly
JDj = ([j]
k )TJ Yk = −Yj,
JD[j+1]
j
= ([j]
k )TJ Y [j+1]
k
= −Y [j+1]
j
(6.232)
(when evaluated at k = j). Therefore,
μ

JDj+1, JD[j]
j+1
 (6.231)
=
μ(−Yj+1, −Y [j]
j+1) = μ(Yj+1, Y [j]
j+1)
(4.14)
=
m(j),
μ∗
JDj, JD[j+1]
j
 (6.232)
=
μ∗(−Yj, −Y [j+1]
j
) = μ∗(Yj, Y [j+1]
j
)
(4.16)
=
m∗(j).
In Sect. 6.4.1 we will derive analogs of (6.230) for unbounded intervals.
Fundamental results from the Sturmian theory of discrete symplectic sys-
tems (SDS) are established in Chap. 4. In particular, Theorem 4.24 and its
Corollaries 4.25 and 4.26 state that for a given index N ∈[0, ∞)Z the numbers of
forward (or left) focal points in the interval (0, N + 1] of any two conjoined bases
of (SDS) differ by at most n. Therefore, based on Corollary 4.25, we formulate the
following result, which will be useful for our future reference.
Proposition 6.61 The following statements are equivalent.
(i) System (SDS) is (non)oscillatory at ∞(resp., at −∞).
(ii) Every conjoined basis of (SDS) is (non)oscillatory at ∞(resp., at −∞).
(iii) There exists a conjoined basis of (SDS), which is (non)oscillatory at ∞(resp.,
at −∞).
Therefore, equivalently to Deﬁnition 4.37, we say that system (SDS) is nonoscil-
latory at ∞(resp., at −∞) if every conjoined basis of (SDS) has ﬁnitely many

464
6
Miscellaneous Topics on Symplectic Systems
forward focal points in (0, ∞) (resp., ﬁnitely many backward focal points in
(−∞, 0)). In the opposite case, we say that system (SDS) is oscillatory at ∞(resp.,
at −∞). We note that by formula (4.13) in Corollary 4.6, it is equivalent to use the
forward or the backward focal points to deﬁne the (non)oscillation of system (SDS)
at ∞(resp., at −∞).
In Example 4.73 we showed that the symplectic system corresponding to the
Fibonacci difference equation xk+2 = xk+1 + xk for k ∈[0, ∞)Z is oscillatory at
∞. Also, the symplectic systems in Examples 4.74 and 4.75 are nonoscillatory at
∞.
Example 6.62 Consider system (SDS) with Sk ≡J on [0, ∞)Z. Then d[0, ∞)Z =
d∞= 0 and all solutions of (SDS) are periodic with period four. Consider the
conjoined basis Y = {Yk}∞
k=0 with Y2j = (0, (−1)jI)T and Y2j+1 = ((−1)jI, 0)T
for j ∈[0, ∞)Z. Then for each k ∈[0, ∞)Z either Xk = 0 or Xk+1 = 0. Then for
every N ∈[0, ∞)Z, the kernel of Xk is not constant on [N, ∞)Z, and Y has inﬁnitely
many focal points in (N, ∞). These focal points of multiplicity n arise from the ﬁrst
term in (4.3) by violating the kernel condition in (2.37). The focal points are located
in the intervals (k, k + 1] = (2j, 2j + 1] (more precisely, at k + 1 = 2j + 1) for
every j ∈[0, ∞)Z in the terminology of Sect. 4.1.1. Thus, this system (SDS) is
oscillatory at ∞.
Example 6.63 Consider system (SDS) with Sk ≡I2n on [0, ∞)Z. Then we have
d[0, ∞)Z = d∞= n, and all solutions of (SDS) are constant on [0, ∞)Z. Therefore,
this system is nonoscillatory at ∞.
Further examples will be discussed in Sects. 6.3.6 and 6.3.7 in the relation with
the recessive and dominant solutions of (SDS) at ∞.
Remark 6.64 If V is a linear subspace in Rn, then we denote by PV the correspond-
ing n × n orthogonal projector onto V . It follows that the matrix PV is symmetric,
idempotent, and nonnegative deﬁnite.
Given a conjoined basis Y of (SDS), we denote by Pk the orthogonal projector
onto Im XT
k and by Rk the orthogonal projector onto Im Xk. That is, according to
Remarks 6.64 and 1.62, we have
Pk := PIm XT
k = X†
kXk,
Rk := PIm Xk = XkX†
k.
(6.233)
Note that rank Pk = rank XT
k = rankXk = rankRk. When Ker Xk = Ker Xk+1,
i.e., when Im XT
k = Im XT
k+1, we have Pk = Pk+1. Therefore, the projector
P := Pk
(6.234)
is constant on an interval [N, ∞)Z where Ker Xk is constant. For convenience but
with slight abuse in terminology, we say that Y has constant kernel on [N, ∞)Z
when the kernel of Xk is constant on this interval. Similarly, we say that Y has rank
r on [N, ∞)Z when the rank of Xk is equal to r on [N, ∞)Z.

6.3
Symplectic Systems Without Controllability
465
Next we derive important properties of conjoined bases of a nonoscillatory
system (SDS) at ∞, in particular of conjoined bases with constant kernel on a given
interval [N, ∞)Z and with no forward focal points in (N, ∞). The latter properties
are equivalent with the two conditions in (2.37), i.e., with
Ker Xk+1 = Ker Xk,
XkX†
k+1Bk ≥0,
k ∈[N, ∞)Z.
(6.235)
Later in this section, we will provide a construction of such conjoined bases from
a given conjoined basis with the same properties via the relation “being contained”
(see Sect. 6.3.3).
For every conjoined basis Y of (SDS), we deﬁne its corresponding S-matrix
Sk :=
k−1

j=N
X†
j+1 Bj X† T
j
.
(6.236)
We start with two monotonicity properties of the matrix Sk and the set Im Sk, which
leads to a deﬁnition of the T -matrix associated with Y.
Theorem 6.65 Assume that system (SDS) is nonoscillatory at ∞. Then for every
conjoined basis Y of (SDS), there exists N
∈[0, ∞)Z such that the matrix
X†
k+1BkX† T
k
is symmetric and nonnegative deﬁnite on [N, ∞)Z, the associated
matrix Sk in (6.236) is symmetric, nonnegative deﬁnite, and nondecreasing on
[N, ∞)Z with SN = 0. Moreover, the matrix S†
k is eventually nonincreasing, the
limit
T := lim
k→∞S†
k
(6.237)
exists, and the matrix T is symmetric and nonnegative deﬁnite.
Proof The result in Proposition 6.61 implies that for a conjoined basis Y of (SDS),
there exists N ∈[0, ∞)Z such that the kernel of Xk is constant on [N, ∞)Z and
XkX†
k+1Bk is symmetric and nonnegative deﬁnite on [N, ∞)Z. This implies that the
matrix
X†
k+1BkX† T
k
= Pk+1X†
k+1BkX† T
k
= PkX†
k+1BkX† T
k
= X†
kXkX†
k+1BkX† T
k
is symmetric and nonnegative deﬁnite on [N, ∞)Z as well. In turn, the matrix Sk
in (6.236) is symmetric, nonnegative deﬁnite, nondecreasing on [N, ∞)Z, and by
convention SN = 0. These properties imply that the matrix S†
k is also symmetric
and nonnegative deﬁnite on [N, ∞)Z, and nonincreasing for large k. More precisely,
S†
k is nonincreasing on intervals, where Sk has constant image, by Remark 1.60(vi).
Hence, the limit in (6.237) exists and the matrix T is symmetric and nonnegative
deﬁnite.
⊓⊔

466
6
Miscellaneous Topics on Symplectic Systems
Theorem 6.66 Let Y be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z, and let the matrices P, Rk, and Sk be deﬁned in (6.234), (6.233),
and (6.236). Then
(i) Im [Uk (I −P)] = Ker Rk, and hence RkUk = RkUkP, for all k ∈[N, ∞)Z,
(ii) Bk = Rk+1Bk and Bk = BkRk for all k ∈[N, ∞)Z,
(iii) PSk = Sk, i.e., Im Sk ⊆Im P, for all k ∈[N, ∞)Z.
If in addition Y has no forward focal points in (N, ∞), then
(iv) the set Im Sk is nondecreasing on [N, ∞)Z, hence it is eventually constant.
Proof (i) For a vector v ∈Im Uk(I −P), we have v = Uk(I −P) c for some
c ∈Rn. This yields that Rkv = X†T
k XT
k Uk(I −P) c = X†T
k UT
k Xk(I −P) c = 0,
since Xk(I −P) = 0. Therefore, Im Uk(I −P) ⊆Ker Rk holds, which yields
the inequality rank Uk(I −P) ≤def Rk. We will show the opposite inequality.
Choose w ∈Ker Uk(I −P), i.e., Uk(I −P) w = 0. Since also Xk(I −P) w = 0
and rank(XT
k , UT
k ) = n hold, we have (I −P) w = 0, i.e., w ∈Im P. Thus,
Ker Uk(I −P) ⊆Im P and def Uk(I −P) ≤rankP. By using rank Rk = rankP,
we then obtain the needed estimate
def Rk = n −rankRk = n −rank P ≤n −def Uk(I −P) = rank Uk(I −P).
Consequently, def Rk = rankUk(I −P). But since we have already proved that
Im Uk(I −P) is a subspace of Ker Rk, the statement in part (i) follows. The ﬁrst
identity in part (ii) is a reformulation of the fact that Xk+1X†
k+1Bk = Bk when
Ker Xk+1 = Ker Xk; see Lemma 2.15 in Sect. 2.2.1. For the second identity in
(ii), we note that Xk+1 = Xk+1Pk+1 = Xk+1Pk and that XkX†
k+1Bk and Rk are
symmetric. Therefore, we have
BkRk = Rk+1BkRk = Xk+1PkX†
k+1BkRk = Xk+1X†
kBT
k X†T
k+1XT
k Rk
= Xk+1X†
kBT
k X†T
k+1XT
k = Xk+1X†
kXkX†
k+1Bk = Xk+1PkX†
k+1Bk
= Xk+1X†
k+1Bk = Bk.
For part (iii) we use that the projector Pj is constant on [N, ∞)Z, so that we have
PSk =
k−1

j=N
PPj+1X†
j+1BjX†T
j
=
k−1

j=N
Pj+1X†
j+1BjX†T
j
= Sk
on [N, ∞)Z. This shows that Im Sk = Im PSk ⊆Im P on [N, ∞)Z. If in addition Y
has no forward focal points in (N, ∞), then XkX†
k+1Bk ≥0 on [N, ∞)Z and so Sk is
nonnegative deﬁnite and nondecreasing on [N, ∞)Z; compare with Theorem 6.65.
And since Sk is symmetric, it follows that Im Sk is nondecreasing on [N, ∞)Z and
hence eventually constant.
⊓⊔

6.3
Symplectic Systems Without Controllability
467
Based on Theorem 6.66(iv), we deﬁne for a conjoined basis Y of (SDS) with
constant kernel on [N, ∞)Z and no forward focal points in (N, ∞) the orthogonal
projectors
PSk := PIm Sk = S†
kSk = SkS†
k,
PS∞:= lim
k→∞PSk,
(6.238)
where PS∞is the constant projector onto the maximal set Im Sk according to
Theorem 6.66(iv). We then have the inclusions
Im Sk ⊆Im PS∞⊆Im P,
k ∈[N, ∞)Z,
Im T ⊆Im PS∞⊆Im P,
(6.239)
where we used that Im S†
k = Im ST
k = Im Sk by Remark 1.60(i).
In the following result, we show a construction and properties of a special
conjoined basis ¯Y of (SDS), which completes a given conjoined basis Y with
constant kernel on some interval [N, ∞)Z to a normalized pair (see Sect. 2.1.1).
This leads to a special symplectic fundamental matrix k = (Yk, ¯Yk) of (SDS),
which will be utilized in further development of the theory.
Proposition 6.67 Let Y be a conjoined basis of (SDS) with constant kernel
on [N, ∞)Z
with the associated matrices P, Rk, Sk, and PSk
deﬁned
in (6.234), (6.233), (6.236), and (6.238). Then there exists a conjoined basis ¯Y
of (SDS) such that
(i) w(Y, ¯Y ) = I,
(ii) X†
N ¯XN = 0.
Moreover, such a conjoined basis ¯Y then satisﬁes
(iii) X†
k ¯XkP = Sk for k ∈[N, ∞)Z,
(iv)
¯XkP = XkSk and
¯UkP = UkSk + X†T
k
+ Uk(I −P) ¯XT
k X†T
k
for every
k ∈[N, ∞)Z (in particular ¯XNP = 0), and the solution ¯YP of (SDS) is
uniquely determined by Y on [0, ∞)Z,
(v) the matrices ¯Xk for k ∈[N, ∞)Z are uniquely determined by Y,
(vi) Ker ¯Xk = Im (P −PSk) = Im P ∩Ker Sk for k ∈[N, ∞)Z,
(vii)
¯Pk = I −P + PSk for k ∈[N, ∞)Z, where ¯Pk := ¯X†
k ¯Xk,
(viii) S†
k = ¯X†
kXkPSk = ¯X†
kXk ¯Pk for k ∈[N, ∞)Z,
(ix) Im ¯XN = Im (I −RN) and Im ¯XT
N = Im (I −P),
(x) the matrix XN −¯XN is invertible with (XN −¯XN)−1 = X†
N −¯X†
N,
(xi)
¯X†
N = −(I −P) UT
N .
If in addition the conjoined basis Y has no forward focal points in (N, ∞), then
(xii) Xk ¯XT
k ≥0 for k ∈[N, ∞)Z.
Proof We will prove (i) and (ii). Let ˜Y be a conjoined basis of (SDS) such that
Y and ˜Y are normalized, i.e., w(Y, ˜Y ) = I (see Sect. 2.1.1). Then according to

468
6
Miscellaneous Topics on Symplectic Systems
Lemma 1.58 applied to the symplectic matrix S := k = (Yk, ¯Yk), we have
Xk ˜UT
k −˜XkUT
k = I,
Xk ˜XT
k and Uk ˜UT
k symmetric,
k ∈[0, ∞)Z.
(6.240)
Since the projector P is constant on [N, ∞)Z, we deﬁne the constant matrix
D := X†
N ˜XN(P −I) −˜XT
NX†T
N .
Then D is symmetric, because X†
N ˜XNP = X†
N ˜XNXT
NX†T
N is symmetric by (6.240).
Furthermore, PD = −X†
N ˜XN. Deﬁne the solution ¯Y of (SDS) by ¯Yk := ˜Yk + YkD
on [0, ∞)Z. Then ¯Y is a conjoined basis of (SDS) satisfying w(Y, ¯Y ) = I and
X†
N ¯XN = X†
N ˜XN + PD = 0. This completes the proof of parts (i) and (ii). For part
(iii), let k ∈[N, ∞)Z be ﬁxed. Then we have
(X†
k ¯XkP) = X†
k+1 ¯Xk+1P −X†
k ¯XkP
= X†
k+1( ¯Xk+1 −Ak ¯Xk) P + (X†
k+1Ak −X†
k) ¯XkP
= X†
k+1Bk ¯UkX†
kXk + X†
k+1AkXk ¯XT
k X†T
k
−X†
kXk ¯XT
k X†T
k
= X†
k+1Bk( ¯UkXT
k −Uk ¯XT
k ) X†T
k
+ (X†
k+1Xk+1 −P) ¯XT
k X†T
k
= X†
k+1BkX†T
k .
Since we already proved that X†
N ¯XN = 0, we obtain by the summation of the
above equality from j = N to j = k −1 that X†
k ¯XkP = Sk, as we claim in (iii).
The formulas for ¯XkP and ¯UkP on [N, ∞)Z in part (iv) follow from the above
construction of ¯Y. Then for k = N we obtain ¯XNP = XNSN = 0, as SN = 0.
Moreover, in view of part (v) proven below, these formulas depend only on the
values of Yk on [N, ∞)Z, so that part (iv) is proven. Regarding part (v), we assume
that there exists another conjoined basis ¯¯Y of (SDS) satisfying (i)–(iii). Then by
[205, Corollary 3.3.9], there exists a symmetric matrix ¯D with ¯¯Yk = ¯Yk + Yk ¯D
on [0, ∞)Z. From X†
N ¯¯XN = 0 = X†
N ¯XN, we then obtain P ¯D = X†
NXN ¯D =
X†
N( ¯¯XN −¯XN) = 0, so that Im ¯D ⊆Ker P = Ker Xk on [N, ∞)Z. Therefore,
Xk ¯D = 0 and the equality ¯¯Xk = ¯Xk on [N, ∞)Z follows. Therefore, part (v) holds.
For part (vi) we note that from the identity XT
k ¯Uk −UT
k ¯Xk = I on [N, ∞)Z (i.e.,
from w(Y, ¯Y ) = I), it follows that Ker ¯Xk ⊆Im XT
k = Im P for every k ∈[N, ∞)Z.
Moreover, by (iii) and (6.239), we get
Ker ¯Xk ⊆Im P ∩Ker Sk = Im P ∩Ker PSk
(6.239)
=
Im (P −PSk),
k ∈[N, ∞)Z.
Conversely, ﬁx an index k ∈[N, ∞)Z, and assume that v ∈Im (P −PSk) = Im P ∩
Ker Sk. The ﬁrst identity in (iv) then yields ¯Xkv = ¯XkPv = XkSkv = 0, and hence

6.3
Symplectic Systems Without Controllability
469
v ∈Ker ¯Xk. Therefore, the opposite inclusion Im (P −PSk) ⊆Ker ¯Xk also holds,
showing part (vi). In addition, the latter result is equivalent with the fact that the
matrix I −(P −PSk) is the orthogonal projector onto the space (Ker ¯Xk)⊥= Im ¯XT
k
for every k ∈[N, ∞)Z. More precisely, we have the formula
¯P = ¯X†
k ¯Xk = I −P + PSk,
k ∈[N, ∞)Z,
(6.241)
which proves part (vii). Next, upon combining the identities PSk = SkS†
k and
XkSk = ¯XkP from (iv) and PSk P = PSk on [N, ∞)Z with (6.241), we obtain
¯X†
k Xk PSk = ¯X†
k Xk SkS†
k
(iv)
= ¯X†
k ¯XkPS†
k
(6.241)
=
(I −P + PSk) PS†
k
= PSk PS†
k = S†
k
for every k ∈[N, ∞)Z, i.e., part (viii) holds. For part (ix) we ﬁrst observe that
PSN = S†
NSN = 0. Then by (vi) we obtain Ker ¯XN = Im P, i.e., by taking
the orthogonal complements Im ¯XT
N = Im (I −P). Moreover, from (ii) we have
Im ¯XN ⊆Ker X†
N = Im (I −RN). But since rank ¯XN = rank ¯XT
N = rank(I −P) =
rank(I −RN) (as rank P = rank Rk), it follows that Im ¯XN = Im (I −RN). For
part (x) we note from (ix) and (vii) that
¯XN ¯X†
N = I −RN,
¯PN = ¯X†
N ¯XN = I −P.
(6.242)
Then XN ¯X†
N = XNP(I −P) ¯X†
N = 0 and ¯XNX†
N = ¯XN(I −P) PX†
N = 0. Thus,
(XN −¯XN) (X†
N −¯X†
N) = XNX†
N −XN ¯X†
N −¯XNX†
N −¯XN ¯X†
N
(6.242)
=
RN + (I −RN) = I.
This shows that the matrices XN −¯XN and X†
N −¯X†
N are invertible and they are
inverses of each other. Part (xi) is a direct application of the deﬁnition of the Moore-
Penrose pseudoinverse (see Sect. 1.6.2). If we set A :=
¯XN and B := −(I −
P) UT
N , then we verify the four properties in (1.157) to conclude that B = A†. In
these calculations we use (6.242), property (i) in the form of equations ¯XNUT
N =
XN ¯UT
N −I and UT
N ¯XN = XT
N ¯UN −I, and the facts that (I −RN) XN = 0 and
(I −P) XT
N = 0. Finally, property (xii) follows from (iii) by showing that Xk ¯XT
k =
¯XkSk ¯XT
k ≥0 for k ∈[N, ∞)Z, since Sk ≥0 for k ∈[N, ∞)Z under the stated
additional assumption on Y. The proof of Proposition 6.67 is complete.
⊓⊔
Remark 6.68 From the proof of Proposition 6.67(iv), it follows that if ˜Y is any
conjoined basis of (SDS) such that w(Y, ˜Y ) = I, then Sk = X†
k ˜XkP −X†
N ˜XNP for
all k ∈[N, ∞)Z.

470
6
Miscellaneous Topics on Symplectic Systems
In the following result, we describe a mutual representation of conjoined bases
of (SDS) with constant kernel on [N, ∞)Z.
Theorem 6.69 Let Y (1) and Y (2) be conjoined bases of (SDS) with constant kernels
on [N, ∞)Z, and let P (1) and P (2) be the constant projectors deﬁned by (6.234)
through the functions X(1)
k
and X(2)
k , respectively. Let the conjoined basis Y (2) be
expressed in terms of Y (1) via the matrices M(1), N(1), and let conjoined basis Y (1)
be expressed in terms of Y (2) via the matrices M(2), N(2), that is,
Y (2)
k
=

Y (1)
k
¯Y (1)
k
 M(1)
N(1)

,
Y (1)
k
=

Y (2)
k
¯Y (2)
k
 M(2)
N(2)

(6.243)
for all k ∈[N, ∞)Z, where ¯Y (1) and ¯Y (2) are conjoined bases of (SDS) satisfying
the conclusion in Proposition 6.67 with regard to Y (1) and Y (2), respectively. If
Im X(1)
N = Im X(2)
N holds, then
(i) the matrices (M(1))T N(1) and (M(2))T N(2) are symmetric,
(ii) the matrices N(1) and N(2) satisfy N(1) + (N(2))T = 0,
(iii) the matrices M(1) and M(2) are invertible and M(1)M(2) = I = M(2)M(1),
(iv) Im N(1) ⊆Im P (1) and Im N(2) ⊆Im P (2).
Moreover, the matrices M(1), N(1) do not depend on the choice of ¯Y (1), and the
matrices M(2), N(2) do not depend on the choice of ¯Y (2). In fact,
M(1) = −w( ¯Y (1), Y (2)),
N(1) = w(Y (1), Y (2)),
M(2) = −w( ¯Y (2), Y (1)),
N(2) = w(Y (2), Y (1)).

(6.244)
Proof Since Y (i) and ¯Y (i) for i ∈{1, 2} are normalized conjoined bases of (SDS),
the 2n × 2n fundamental matrices in (6.243) are symplectic. By the formula for the
inverse of a symplectic matrix (Lemma 1.58(ii)), we obtain (6.244), i.e.,
M(1)
N(1)

=

( ¯U(1)
N )T X(2)
N −( ¯X(1)
N )T U(2)
N
(X(1)
N )T U(2)
N −(U(1)
N )T X(2)
N

,
(6.245)
M(2)
N(2)

=

( ¯U(2)
N )T X(1)
N −( ¯X(2)
N )T U(1)
N
(X(2)
N )T U(1)
N −(U(2)
N )T X(1)
N

.
(6.246)
Assertion (i) is then a direct consequence of the above expressions and of for-
mula (6.240) for the pairs Y (1),
¯Y (1) and Y (2), ¯Y (2). Condition (ii) follows
from (6.244), since w(Y (2), Y (1)) = −[w(Y (1), Y (2))]T . Regarding item (iii), we
deﬁne the matrices
L(1) := (X(1)
N )†X(2)
N ,
L(2) := (X(2)
N )†X(1)
N .
(6.247)

6.3
Symplectic Systems Without Controllability
471
The assumption Im X(1)
N = Im X(2)
N means that the projector R(1)
N onto Im X(1)
N and
the projector R(2)
N onto Im X(2)
N , which are deﬁned in (6.233), satisfy R(1)
N
= R(2)
N .
Consequently, it follows that
X(1)
N L(1) = R(1)
N X(2)
N = R(2)
N X(2)
N = X(2)
N ,
(6.248)
L(1)L(2) = (X(1)
N )†R(2)
N X(1)
N = (X(1)
N )†R(1)
N X(1)
N = (X(1)
N )†X(1)
N = P (1).
(6.249)
Similarly it can be shown that X(2)
N L(2) = X(1)
N and L(2)L(1) = P (2). In addition,
L(2) = (L(1))†,
(6.250)
which follows by an easy veriﬁcation of the four equalities in (1.157) and by
Im L(i) = Im P (i) for i ∈{1, 2}. If we now insert the formula X(1)
N L(1) = X(2)
N
from (6.248) into the expression for M(1) in (6.245) and similarly insert the
formula X(2)
N L(2) = X(1)
N into the expression for M(2) in (6.246), then we get via
w(Y (i), ¯Y (i)) = I for i ∈{1, 2} that
M(1) = ( ¯U(1)
N )T X(1)
N L(1) −( ¯X(1)
N )T U(2)
N
= [I + ( ¯X(1)
N )T U(1)
N ] L(1) −( ¯X(1)
N )T U(2)
N
= L(1) + ( ¯X(1)
N )T [U(1)
N L(1) −U(2)
N ],
(6.251)
M(2) = ( ¯U(2)
N )T X(2)
N L(2) −( ¯X(2)
N )T U(1)
N
= [I + ( ¯X(2)
N )T U(2)
N ] L(2) −( ¯X(2)
N )T U(1)
N
= L(2) + ( ¯X(2)
N )T [U(2)
N L(2) −U(1)
N ].
(6.252)
The product M(1)M(2) is then simpliﬁed to the identity matrix by using the identities
L(1)L(2) = P (1), Proposition 6.67(iv), (6.240), R(1)
N
= R(2)
N . This technical and
rather lengthy calculation is omitted. For part (iv), the matrices N(1) and N(2)
in (6.245) and (6.246) satisfy
N(1) = (X(1)
N )T U(2)
N −(U(1)
N )T X(1)
N L(1) = (X(1)
N )T [U(2)
N −U(1)
N L(1)],
(6.253)
N(2) = (X(2)
N )T U(1)
N −(U(2)
N )T X(2)
N L(2) = (X(2)
N )T [U(1)
N −U(2)
N L(2)].
(6.254)
Thus, Im N(1) ⊆Im (X(1)
N )T = Im P (1) and Im N(2) ⊆Im (X(2)
N )T = Im P (2).
In addition, the matrices M(1), N(1) and M(2), N(2) in (6.243) do not depend
on the choice of ¯Y (1) and ¯Y (2), because only the conjoined bases Y (1), Y (2) and
the matrices ¯X(1)
N , ¯X(2)
N , which are unique by Proposition 6.67(v), are used in
expressions (6.251)–(6.254) for M(1), M(2), N(1), N(2).
⊓⊔

472
6
Miscellaneous Topics on Symplectic Systems
Remark 6.70
(i) The equality in Proposition 6.67(iv) implies that for k = N in (6.243), the
conjoined bases Y (1) and Y (2) satisfy
X(2)
N = X(1)
N M(1),
U(2)
N
= U(1)
N M(1) + (X(1)
N )†T N(1),
(6.255)
X(1)
N = X(2)
N M(2),
U(1)
N
= U(2)
N M(2) + (X(2)
N )†T N(2).
(6.256)
(ii) Equations (6.249) and (6.250) are summarized as
L(1)(L(1))† = P (1),
L(2)(L(2))† = P (2).
(6.257)
This means that P (1) is the orthogonal projector onto Im L(1) and similarly
P (2) is the orthogonal projector onto Im L(2). Moreover, expressions (6.251)
and (6.252) for matrices M(1) and M(2) provide a connection between M(1)
and L(1) and between M(2) and L(2). In particular,
L(1) = P (1)M(1),
L(2) = P (2)M(2).
(6.258)
These equalities follow from equations (6.251) and (6.257), since ¯X(1)
N P (1) =
0 and ¯X(2)
N P (2) = 0, by Proposition 6.67(iv). Moreover, condition (iv) in Theo-
rem 6.69 means that N(1) = P (1)N(1) and N(2) = P (2)N(2), which shows that
the representations in (6.243) in fact contain the uniquely determined solutions
¯Y (1)P (1) and ¯Y (2)P (2). These facts allows one to rewrite expressions (6.243)
into a form, which is more convenient for the further analysis of the associated
S-matrices. More precisely, from Proposition 6.67(iv), we get
X(2)
k
= X(1)
k
(L(1) + S(1)
k N(1)),
X(1)
k
= X(2)
k
(L(2) + S(2)
k N(2)),

k ∈[N, ∞)Z.
(6.259)
(iii) A more detailed analysis of the statements in parts (i) and (ii) of this remark
shows that if only Y (1) has constant kernel on [N, ∞)Z and (6.255) holds (so
that Im X(2)
N ⊆Im X(1)
N ), then the ﬁrst equality in (6.259) is satisﬁed. Similarly,
if Y (2) has constant kernel on [N, ∞)Z and (6.256) holds (so that Im X(1)
N
⊆
Im X(2)
N ), then the second equality in (6.259) is satisﬁed.
Remark 6.71 From the previous remark and Theorem 6.69, it also follows that
the matrices (L(1))T N(1) and (L(2))T N(2) are symmetric. This can be seen from
the calculation (L(1))T N(1) = (M(1))T P (1)N(1) = (M(1))T N(1) and similarly for
(L(2))T N(2) = (M(2))T N(2).
Remark 6.72 As a completion of Theorem 6.69, we show that the matrices M(1) +
S(1)
k N(1) and M(2) + S(2)
k N(2) are invertible for k ∈[N, ∞)Z. Indeed, from
conditions (i) and (iv) in Theorem 6.69, we obtain that Im (N(1))T ⊆Im P (2),

6.3
Symplectic Systems Without Controllability
473
so that Ker X(2)
k
= Ker P (2) ⊆Ker N(1) on [N, ∞)Z. If for some vector v ∈Rn
we have (M(1) + S(1)
k N(1)) v = 0, then v ∈Ker X(2)
k
by (6.259) and by L(1) =
P (1)M(1), P (1)S(1)
k
= S(1)
k , and X(1)
k P (1) = X(1)
k . In turn, v ∈Ker N(1), and so
v ∈Ker M(1). But since M(1) is invertible by (iii) of Theorem 6.69, it follows that
v = 0. Similarly, one has that M(2) + S(2)
k N(2) is invertible on [N, ∞)Z.
The next statement is essentially a continuation of Theorem 6.69 and its proof.
It provides a relation of the S-matrices corresponding to the conjoined bases Y (1)
and Y (2). It will become one of the crucial tools for the development of the theory
of recessive and dominant solutions of (SDS) at ∞in Sects. 6.3.6 and 6.3.7.
Theorem 6.73 With
the assumptions and notation of Theorem 6.69 and
Remark 6.70, if the condition Im X(1)
N
= Im X(2)
N holds, then for all k ∈[N, ∞)Z
we have Im X(1)
k
= Im X(2)
k
and
(L(1) + S(1)
k N(1))† = L(2) + S(2)
k N(2),
(6.260)
Im (L(1) + S(1)
k N(1)) = Im P (1),
Im (L(2) + S(2)
k N(2)) = Im P (2),
(6.261)
S(2)
k
= (L(1) + S(1)
k N(1))† S(1)
k L†T
1 .
(6.262)
Proof Fix k
∈
[N, ∞)Z. The equality Im X(1)
k
= Im X(2)
k
is a direct con-
sequence of the identities in (6.259). This means that R(1)
k
=
R(2)
k . Using
Theorem 6.66(iii), (6.259), Im L(1) = Im P (1), and Im L(2) = Im P (2), we have
L(1) + S(1)
k N(1) = P (1)(L(1) + S(1)
k N(1))
= (X(1)
k )†X(1)
k (L(1) + S(1)
k N(1)) = (X(1)
k )†X(2)
k ,
(6.263)
L(2) + S(2)
k N(2) = P (2)(L(2) + S(2)
k N(2))
= (X(2)
k )†X(2)
k (L(2) + S(2)
k N(2)) = (X(2)
k )†X(1)
k .
(6.264)
Expressions (6.263) and (6.264) then imply formula (6.260) by the veriﬁcation of
the four equalities in (1.157). In particular, the third and fourth identity in (1.157)
read as
(L(1) + S(1)
k N(1)) (L(2) + S(2)
k N(2)) = (X(1)
k )†R(2)
k X(1)
k
= (X(1)
k )†R(1)
k X(1)
k
= P (1),
(6.265)
(L(2) + S(2)
k N(2)) (L(1) + S(1)
k N(1)) = (X(2)
k )†R(1)
k X(2)
k
= (X(2)
k )†R(2)
k X(2)
k
= P (2),
(6.266)

474
6
Miscellaneous Topics on Symplectic Systems
which imply the relations in (6.261). The proof of formula (6.262) is slightly more
complicated. Since by (6.249) we have L(1)L(2) = P (1), from (6.265) we get
(L(1) + S(1)
k N(1)) S(2)
k N(2) = −S(1)
k N(1)L(2).
Using (6.266), the fact Im S(2)
k
⊆Im P (2), and (6.260) we then obtain
S(2)
k N(2) = −(L(1) + S(1)
k N(1))†S(1)
k N(1)L(2).
By Theorem 6.69(ii) and (6.250), we have N(2) = −(N(1))T and L(2) = (L(1))†,
while from Remark 6.71 we know that (N(2))T L(2) is symmetric. This implies
N(1)L(2) = −(L(1))†T N(2), so that
S(2)
k (N(1))T = (L(1) + S(1)
k N(1))†S(1)
k (L(1))†T (N(1))T .
(6.267)
We will show that the matrix (N(1))T in (6.267) can be canceled. Indeed, if the
equality Im (N(1))T = Im N(2) = Im P (2) holds, then it sufﬁces to multiply (6.267)
from the right by (N(1))†T , because (N(1))T (N(1))†T = (N(1))†N(1) = P (2),
Im S(2)
k
⊆Im P (2), and Im (L(1))† = Im L(2) ⊆Im P (2). But in general we
only have Im (N(1))T = Im N(2) ⊆Im P (2), which shows that more analysis is
required in order to cancel (N(1))T in (6.267). Let us denote G := (L(1))T N(1).
The matrix G is symmetric, Im G ⊆Im P (2), and N(1) = (L(1))†T G. According
to Lemma 1.89, there exists a sequence {G{ν}}∞
ν=1 of symmetric matrices with
Im G{ν} = Im P (2) for all ν ∈N such that G{ν} →G for ν →∞. Furthermore,
with N{ν} := (L(1))†T G{ν}, we have N{ν} →(L(1))†T G = N(1) for ν →∞,
and in addition Im N{ν} = Im P (1) and Im (N{ν})T = Im P (2) for all ν ∈N. By
verifying the identities in (1.157), it follows that N{ν}† = G{ν}†(L(1))T , and in
particular N{ν}N{ν}† = P (1) and N{ν}†N{ν} = P (2) hold. Observe that the matrices
(M(1))T N{ν} and (L(1))T N{ν} are symmetric, since
(M(1))T N{ν} = (M(1))T (L(1))†T G{ν} = (M(1))T (L(2))T G{ν}
= (M(1))T (M(2))T P (2)G{ν} = G{ν},
(L(1))T N{ν} = (M(1))T P (1)N{ν} = (M(1))T N{ν},
where we used Remark 6.70(ii). For each ν ∈N, we now deﬁne the solution Y {ν} of
system (SDS) by
Y {ν}
k
:=

Y (1)
k
¯Y (1)
k
 M(1)
N{ν}

,
k ∈[N, ∞)Z.
(6.268)
Since MT
1 N{ν} is symmetric and rank ((M(1))T , (N{ν})T ) = n (as M(1) is invertible
by Theorem 6.69), it follows that Y {ν} is a conjoined basis of (SDS). Moreover,

6.3
Symplectic Systems Without Controllability
475
the sequence {Y {ν}}∞
ν=1 converges on [N, ∞)Z to the conjoined basis Y (2), which
follows from (6.243) and from the convergence of {(N(1)){ν}}∞
ν=1 to N(1). Since
Im N{ν} = Im P (1), the function X{ν}
k
in (6.268) will have the form as in (6.259).
That is, we have X{ν}
k
= X(1)
k (L(1) + S(1)
k N{ν}) on [N, ∞)Z for every ν ∈N. Thus,
for each k ∈[N, ∞)Z, we have the inclusions Im X{ν}
k
⊆Im X(1)
k
= Im X(2)
k
and
Im (X{ν}
k )T ⊆Im [(L(1))T + (N{ν})T S(1)
k ] ⊆Im P (2) = Im (X(2)
k )T for all ν ∈N.
Fix an index K ∈[N, ∞)Z. Then by Lemma 1.61 (with j := ν and using that the
interval [N, K]Z is a ﬁnite set), there exists ν0 ∈N such that for all ν ≥ν0
Im X{ν}
k
= Im X(2)
k ,
Im (X{ν}
k )T = Im (X(2)
k )T
for all k ∈[N, K]Z,
(6.269)
rankX{ν}
k
= rankX(2)
k
for all k ∈[N, K]Z.
(6.270)
Therefore, by (6.270) and the limit property of the Moore-Penrose pseudoinverse in
Remark 1.60(v), we obtain
(X{ν}
k )† →(X(2)
k )†
for ν →∞and every k ∈[N, K]Z.
(6.271)
Fix an index ν ≥ν0. Then from the second equality in (6.269), it follows that
Ker X{ν}
k
= Ker X(2)
k
= Ker P (2) on [N, K]Z, so that Y {ν} is a conjoined basis
of (SDS) with constant kernel on [N, K]Z and Im X{ν}
N
= Im X(2)
N
= Im X(1)
N .
Hence, Theorem 6.69 (on the interval [N, K]Z) can be applied, and the ﬁrst formula
in (6.261) proven above holds for the pair Y (1), Y {ν}, i.e.,
Im (L(1) + S(1)
k N{ν}) = Im P (1),
k ∈[N, K]Z.
(6.272)
Let S{ν}
k
be the S-matrix corresponding to the conjoined basis Y {ν}. Then
S{ν}
k (N{ν})T = (L(1) + S(1)
k N{ν})† S(1)
k (L(1))†T (N{ν})T ,
k ∈[N, K]Z,
according to (6.267). Since Im (N{ν})T
= Im P (2), the matrix (N{ν})T can be
canceled as we showed above, and then we obtain
S{ν}
k
= (L(1) + S(1)
k N{ν})† S(1)
k (L(1))T †,
k ∈[N, K]Z.
(6.273)
Assertion (6.271) yields that S{ν}
k
→S(2)
k
as ν →∞for every k ∈[N, K]Z. Since
L(1) + S(1)
k N{ν} →L(1) + S(1)
k N(1) as ν →∞for each k ∈[N, K]Z and (6.272)
holds, it follows from Remark 1.60(v) that
(L(1) + S(1)
k N{ν})† →(L(1) + S(1)
k N(1))†
as ν →∞for each k ∈[N, K]Z.

476
6
Miscellaneous Topics on Symplectic Systems
In turn, equation (6.273) implies that S{ν}
k
→S(2)
k
as ν →∞for each k ∈[N, K]Z
and that formula (6.262) holds for all k ∈[N, K]Z. But since the index K ∈
[N, ∞)Z was arbitrary, it follows that (6.262) holds for all k ∈[N, ∞)Z. The proof
of Theorem 6.73 is complete.
⊓⊔
Remark 6.74 Under the assumptions of Theorem 6.73, we take k = N in (6.260)
and obtain by using (6.258) and (6.250) the equality
(P (1)M(1))† = (L(1))† = L(2) = P (2)M(2) = P (2)(M(1))−1.
(6.274)
The following result provides a connection between a conjoined basis of (SDS)
with constant kernel on [N, ∞)Z and no forward focal points in (N, ∞) with the
principal solution Y [N] of (SDS) and with the order of abnormality. This result, and
in particular inequality (6.279), will serve as another crucial tool for the analysis of
recessive and dominant solutions of (SDS) at ∞.
Theorem 6.75 Let Y be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞). Let the matrices P, Sk, PS∞,
and T be deﬁned by (6.234), (6.236), (6.233), (6.238), and (6.237). Then for all
k ∈[N, ∞)Z
X[N]
k
= XkSkXT
N,
rankSk = rank X[N]
k
= n −d[N, k]Z,
(6.275)
rank PS∞= n −d[N, ∞)Z,
(6.276)
0 ≤rankT ≤n −d[N, ∞)Z,
(6.277)
0[N, ∞)Z = Im [X†T
N (I −PS∞)] ⊕Im [UN(I −P)],
(6.278)
n −d[N, ∞)Z ≤rankXk ≤n.
(6.279)
Proof As a consequence of estimate (4.62) (with Y [0] := Y [N]) or a consequence
of Theorems 2.39 and 2.36(iv), we know that the principal solution Y [N] has no
forward focal point in [N, ∞)Z. This means that the kernel of X[N]
k
is nonincreasing
on [N, ∞)Z. Using (6.226) in Theorem 6.57 obtain
0[N, k]Z = Ker X[N]
k
for all k ∈[N, ∞)Z.
(6.280)
Let Y [N] be expressed in terms of Y via the matrices M[N], N[N], i.e.,
Y [N]
k
=

Yk, ¯Yk
M[N]
N[N]

,
k ∈[N, ∞)Z,
(6.281)
where ¯Y is a conjoined basis of (SDS) from Proposition 6.67. By (6.244) in
Theorem 6.69, we have from (6.281) at k = N
M[N] = −w( ¯Y, Y [N]) = −¯XT
N,
N[N] = w(Y, Y [N]) = XT
N.
(6.282)

6.3
Symplectic Systems Without Controllability
477
Since ¯XN is uniquely determined by Proposition 6.67(v), the matrices M[N], N[N]
do not depend on the choice of ¯Y. Furthermore, inserting expressions (6.282)
into the representation of X[N] in (6.281) and using the deﬁnition of P and
Proposition 6.67(iv) yield for all k ∈[N, ∞)Z
X[N]
k
= −Xk ¯XT
N + ¯XkXT
N = −XkP ¯XT
N + ¯XkPXT
N
= −Xk(XNSN)T + (XkSk) XT
N = XkSkXT
N.
(6.283)
Since by Theorem 6.66(iii) the equalities PSk = Sk = SkP hold, (6.283) gives
X†
kX[N]
k
X†T
N
(6.283)
=
X†
kXkSkXT
NX†T
N = PSkP = Sk,
k ∈[N, ∞)Z.
(6.284)
The expressions in (6.283) and (6.284) show that rankX[N]
k
=
rankSk,
while (6.280) implies that rankX[N]
k
= n −d[N, k]Z. Therefore, the equalities
in (6.275) are established. The constancy of Im Sk for large k and the identity
rankSk = rank PSk then yield (6.276). Next, by Remark 1.60(i) we know that
rankS†
k = rank Sk = n−d[N, ∞)Z for large k, so that the deﬁnition of T in (6.237)
yields (6.277). In order to prove (6.278), ﬁx k ∈[N, ∞)Z, and let v ∈0[N, k]Z.
By (6.280) and (6.283), we have XkSkXT
Nv = 0, and using Theorem 6.66(iii), we
get
PSk XT
Nv = S†
k PSkXT
Nv = S†
k X†
kXkSkXT
Nv = 0.
Hence, XT
Nv = (I −PSk) v∗holds for some v∗∈Rn. Now we use that the vector
v can be uniquely decomposed as the sum v = v1 + v2, where v1 ∈Im XN =
Im X†T
N and v2 ∈(Im XN)⊥= Ker XT
N = Ker RN = Im [UN(I −P)], by (6.233)
and Theorem 6.66(i). Consequently, XT
Nv1 = (I −PSk) v∗which in turn implies
that v1 ∈Im [X†T
N (I −PSk)]. Thus, v ∈Im [X†T
N (I −PSk)] ⊕Im [UN(I −P)].
Conversely, every vector w ∈Im [X†T
N (I −PSk)] ⊕Im [UN(I −P)] has the form
w = X†T
N (I −PSk) w1 + UN(I −P) w2
for some w1, w2 ∈Rn.
Then by the aid of (6.283), SkP = Sk, and SkPSk = Sk, we get
X[N]
k
w = XkSkXT
NX†T
N (I −PSk) w1 + XkSkXT
NUN(I −P) w2
= XkSkP(I −PSk) w1 + XkSkUT
N XN(I −P) w2
= XkSk(I −PSk) w1 + XkSkUT
N XNP(I −P) w2 = 0.

478
6
Miscellaneous Topics on Symplectic Systems
Thus w ∈Ker X[N]
k
= 0[N, k]Z by (6.280), and
0[N, k]Z = Im [X†T
N (I −PSk)] ⊕Im [UN(I −P)],
k ∈[N, ∞)Z.
In view of (6.238), we conclude, by taking k →∞, that (6.278) holds. For (6.279)
we note that n −d[N, k]Z = rankX[N]
k
≤rankXk holds on [N, ∞)Z, by (6.275).
Moreover, the equality rank Sk = rankPSk and (6.276) yield that the number
n −d[N, ∞)Z is the maximum of rank X[N]
k
on [N, ∞)Z. Therefore, rank Xk lies
necessarily between n −d[N, ∞)Z and n, as we claim in (6.279).
⊓⊔
Remark 6.76 Formula (6.275) shows that for a conjoined basis Y of (SDS) with
constant kernel on [N, ∞)Z and no forward focal points in (N, ∞), the rank of
its corresponding S-matrix depends only on the rank of X[N] and hence, on the
abnormality of (SDS) on [N, ∞)Z, but not on the choice of Y itself. This means
that the changes in Im (X[N]
k
)T and Im Sk occur at the same points, i.e., according
to Theorem 6.66(iv), there exist a ﬁnite partition N0 := N < N1 < N2 < · · · <
Nm < ∞of [N, ∞)Z, which does not depend on the matrix function S, such that
Im Sk is constant on each subinterval [Nj, Nj+1 −1]Z, j ∈{0, . . ., m −1} and
Im Sk ≡Im SNj ⫋Im SNj+1,
k ∈[Nj, Nj+1 −1]Z,
j ∈{0, 1, . . . , m −1}
Im Sk ≡Im SNm,
rankSk = n −d[N, ∞)Z,
k ∈[Nm, ∞)Z.
The estimate in (6.279) is extremely important in the theory of conjoined bases
Y of (SDS). It gives in particular the lower bound for the rank of Xk. We shall
see that all integer values between n −d[N, ∞)Z and n are indeed attained by the
rank of Xk. Moreover, the second condition in (6.275) implies that for all conjoined
bases Y of (SDS) with constant kernel on [N, ∞)Z and no forward focal points in
(N, ∞), the rank of the matrices Sk changes at the same points in [N, ∞)Z, i.e., the
points where rankSk changes (increases) do not depend on the particular choice of
the conjoined basis Y.
At the end of this section, we present three extensions of the previous results. The
ﬁrst one is a generalization of Theorem 6.73 in a sense that the considered initial
condition can be taken at any index L ∈[N, ∞)Z. This statement then leads to the
deﬁnition of a genus of conjoined bases of (SDS) in Sect. 6.3.8.
Theorem 6.77 Let Y (1) and Y (2) be conjoined bases of (SDS) with constant kernel
on [N, ∞)Z, and let the equality Im X(1)
L
= Im X(2)
L
be satisﬁed for some index
L ∈[N, ∞)Z. Then Im X(1)
k
= Im X(2)
k
holds for all k ∈[N, ∞)Z.
Proof If k ∈[L, ∞)Z, then the result follows from Theorem 6.73 (with N := L),
i.e., by (6.259) from the equations
X(3−i)
k
= X(i)
k (M(i)
(L) + S(i)(L)
k
N(i)),
k ∈[L, ∞)Z,
P (i)M(i)
(L) = (X(i)
L )†X(3−i)
L
,
⎫
⎬
⎭
i ∈{1, 2},
(6.285)

6.3
Symplectic Systems Without Controllability
479
where S(i)(L)
k
:= S(i)
k −S(i)
L and where M(i)
(L) is the invertible matrix associated with
¯Y (i) in Proposition 6.67 and Theorem 6.69 (with N := L). We note that the matrix
N(i) does not depend on L ∈[N, ∞)Z by (6.244) in Theorem 6.69.
For k ∈[N, L]Z we use the backward system (2.10). Deﬁne the Riccati quotients
Q(i)
k
:= U(i)
k (X(i)
k )† + (X(i)
k )† T (U(i)
k )T (I −R(i)
k ),
k ∈[N, ∞)Z,
i ∈{1, 2},
(6.286)
where R(i)
k
is the orthogonal projector onto Im X(i)
k
as deﬁned in (6.233).
By (2.10), (6.286), and Theorem 6.66, we obtain
X(i)
k
= (DT
k −BT
k Q(i)
k+1) X(i)
k+1,
U(i)
k (X(i)
k )† = Q(i)
k R(i)
k ,

k ∈[N, ∞)Z,
i ∈{1, 2}.
(6.287)
The Wronskian N(1) = w(Y (1), Y (2)) is constant, which implies by (6.287) that
(X(1)
k )† T N(1) = R(1)
k U(2)
k
−R(1)
k Q(1)
k X(2)
k ,
k ∈[N, ∞)Z.
(6.288)
We now apply Theorem 6.66(ii) to Y (1) and use (2.10) and (6.288) to get
X(2)
k
= DT
k X(2)
k+1 −BT
k R(1)
k+1U(2)
k+1
(6.288)
=
(DT
k −BT
k Q(1)
k+1) X(2)
k+1 −BT
k (X(1)
k+1)† T N(1)
(6.289)
for all k ∈[N, ∞)Z. We will show that Im X(2)
k
⊆Im X(1)
k
on [N, L]Z. Deﬁne
Zk := X(1)
k (M(1)
(L) −FkN(1)),
Fk :=
L−1

j=k
(X(1)
j+1)†Bj(X(1)
j )† T ,
k ∈[N, L]Z,
with FL := 0. It follows by the symmetry of X(1)
k (X(1)
k+1)†Bk and by the identity
P (1)(X(1)
k+1)† = (X(1)
k+1)†, as X(1) has constant kernel on [N, ∞)Z, that
Zk −(DT
k −BT
k Q(1)
k+1) Zk+1
= X(1)
k (M(1)
(L) −FkN(1)) −(DT
k −BT
k Q(1)
k+1) X(1)
k+1(M(1)
(L) −Fk+1N(1))
(6.287)
=
X(1)
k (Fk) N(1) = −R(1)
k X(1)
k (X(1)
k+1)†Bk(X(1)
k )† T N(1)
= −R(1)
k BT
k (X(1)
k+1)† T P (1) N(1) = −BT
k (X(1)
k+1)† T N(1)

480
6
Miscellaneous Topics on Symplectic Systems
for k ∈[N, L −1]Z. Therefore, the sequence Zk satisﬁes the nonhomogeneous
equation (6.289) on [N, L −1]Z. Moreover, since R(1)
L
= R(2)
L
by the assumption
Im X(1)
L = Im X(2)
L , we have
ZL = X(1)
L P (1)M(1)
(L)
(6.285)
=
R(1)
L X(2)
L = R(2)
L X(2)
L = X(2)
L .
Thus, the uniqueness of backward solutions of equation (6.289) yields
X(2)
k
= Zk = X(1)
k (M(1)
(L) −FkN(1)),
k ∈[N, L]Z.
This implies that Im X(2)
k
⊆Im X(1)
k
on [N, L]Z. Upon interchanging the roles of
the conjoined bases Y (1) and Y (2), we derive that Im X(1)
k
⊆Im X(2)
k
on [N, L]Z.
Therefore, Im X(1)
k
= Im X(2)
k
holds for k ∈[N, L]Z, which completes the proof.
⊓⊔
Remark 6.78 The proof of Theorem 6.77 shows that the representation formulas
in (6.285) are in fact satisﬁed for every index k ∈[N, ∞)Z.
In the next result, we extend Theorem 6.69 by using a notion of representable
conjoined bases of (SDS).
Deﬁnition 6.79 Let Y be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z. We say that a solution ˜Y is representable by Y on the interval [N, ∞)Z if
in the relation
˜Yk = k ˜D,
k ∈[0, ∞)Z,
J ˜D =
w(Y, ˜Y )
w( ¯Y, ˜Y )

,
(6.290)
where k = (Yk,
¯Yk) is a symplectic fundamental matrix of (SDS) such that
w(Y, ¯Y ) = I, the matrix ˜D does not depend on the conjoined basis ¯Y.
Theorem 6.80 Assume that Y is a conjoined basis of (SDS) with constant kernel
on [N, ∞)Z with P deﬁned by (6.234), and let ˜Y be a solution of (SDS). Then the
following conditions are equivalent.
(i) The solution ˜Y is representable by Y on [N, ∞)Z.
(ii) The (constant) Wronskian w(Y, ˜Y ) of Y and ˜Y satisﬁes
Im w(Y, ˜Y ) ⊆Im P.
(6.291)
(iii) The inclusion Im ˜Xk ⊆Im Xk holds for all k ∈[N, ∞)Z.
(iv) The inclusion Im ˜XK ⊆Im XK holds for some K ∈[N, ∞)Z.
Proof Let ˜D be the matrix in (6.290), where ¯Y is the conjoined basis in Propo-
sition 6.67 associated with Y. First we assume condition (i). Since w(Y, ¯Y ) = I,
w(Y, Y) = 0, and X†
NXN = P hold, it is straightforward to verify that the solution

6.3
Symplectic Systems Without Controllability
481
¯Y ∗:= ¯Y + Y(I −P) of (SDS) also satisﬁes w(Y, ¯Y ∗) = I and X†
N ¯X∗
N = 0. Denote
by ˜D∗the matrix in (6.290) representing ˜Y in terms of the symplectic fundamental
matrix ∗
k := (Yk, ¯Y ∗
k ), i.e.,
˜Yk = ∗
k ˜D∗,
k ∈[0, ∞)Z,
J ˜D∗=
 w(Y, ˜Y )
w( ¯Y ∗, ˜Y)

.
(6.292)
Observe that w( ¯Y ∗, ¯Y) = I −P. Then we have
˜D∗(6.292)
=
(∗
k)−1 ˜Yk
(6.290)
=
−J (∗
k)TJ k ˜D
= −J

w(Y, Y)
w(Y, ¯Y )
w( ¯Y ∗, Y) w( ¯Y ∗, ¯Y)

˜D =
I P −I
0
I

˜D.
Using the block notation ˜D =
 F
G

and ˜D∗=
 F ∗
G∗

, we then obtain the equalities
F ∗= F −(I −P) G and G∗= w(Y, ˜Y ) = G. Since we now assume that ˜Y is
representable by Y, it follows that ˜D∗= ˜D, and hence F ∗= F. This yields that
(I −P) G = 0, i.e., Im G ⊆Ker (I −P) = Im P. Thus, condition (6.291) is
proven. Assume now (ii) and let G := w(Y, ˜Y ). Combining the identities PG = G
and PXT
k = XT
k on [N, ∞)Z yields that (I −P) UT
k ˜Xk = 0 on [N, ∞)Z. By using
the property Im [Uk(I −P)] = Ker Rk on [N, ∞)Z from Theorem 6.66(i), we then
get
Im ˜Xk ⊆Ker [(I −P) UT
k ] =
!
Im [Uk(I −P)]
"⊥= (Ker Rk)⊥= Im Rk = Im Xk
on [N, ∞)Z, which shows (iii). Next, part (iii) implies (iv) trivially. Finally, assume
that (iv) is satisﬁed. Since Im XK = Im RK, it follows that RK ˜XK = ˜XK. Hence,
˜XK = XKX†
K ˜XK and consequently the matrix F := −w( ¯Y, ˜Y) satisﬁes
F = −( ¯XT
K ˜UK −¯UT
K ˜XK) = ¯UT
KXKX†
K ˜XK −¯XT
K ˜UK
= (I + ¯XT
KUK) X†
K ˜XK −¯XT
K ˜UK.
This shows that the matrix F is determined by Y, ˜Y, and ¯XK. But since ¯Xk is
uniquely determined by Y on [N, ∞)Z by Proposition 6.67(v), we get that F is
determined by Y and ˜Y only. This implies that the matrix D =
 F
G

does not depend
on the choice of ¯Y, so that the solution ˜Y is representable by Y on [N, ∞)Z. The
proof is complete.
⊓⊔

482
6
Miscellaneous Topics on Symplectic Systems
6.3.3
Conjoined Bases with Given Rank
In this subsection we present a construction of conjoined bases Y with a given rank
satisfying (6.279). This construction is based on the notion of an equivalence of
two solutions of (SDS) and the relation being contained for two conjoined bases
of (SDS). More precisely, we say that two solutions Y and ˜Y are equivalent on the
interval [N, ∞)Z if
Xk = ˜Xk
for all k ∈[N, ∞)Z.
(6.293)
This means that
XN = ˜XN
and
Im (UN −˜UN) ⊆0[N, ∞)Z.
(6.294)
By using the representation of the space 0[N, ∞)Z in (6.278) we obtain the
following more precise statement.
Proposition 6.81 Let Y be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞). Let the matrices P and PS∞be
deﬁned by (6.234) and (6.238). Then two solutions Y (1) and Y (2) of (SDS) are
equivalent on [N, ∞)Z if and only if there exist unique n × n matrices G and H
such that
X(1)
N = X(2)
N ,
U(1)
N −U(2)
N
= X†T
N G + UNH,
(6.295)
Im G ⊆Im (P −PS∞),
Im H ⊆Im (I −P).
(6.296)
Proof The existence of matrices G and H satisfying (6.295) follows from (6.294)
and from the representation of 0[N, ∞)Z in (6.278) in Theorem 6.75. The latter
reference also gives the second property in (6.296) and the inclusion Im G ⊆
Im (I −PS∞). However, the matrix G can be chosen so that Im G ⊆Im P,
because in (6.295) we may take X†T XT X†T G = X†T P G instead of X†T G,
by the properties of the Moore-Penrose pseudoinverse. Equation (6.296) then also
implies the uniqueness of G and H, because Y is a conjoined basis. Conversely, the
conditions in (6.295) and (6.296) imply that (6.294) holds, and thus Y (1) and Y (2)
are equivalent.
⊓⊔
The equivalence of solutions on [N, ∞)Z leads to an ordering in the set of all
conjoined bases of (SDS). This ordering is phrased in terms of the relation “being
contained” deﬁned below.
Deﬁnition 6.82 Let Y be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞). Let P and PS∞be the associated
orthogonal projectors in (6.234) and (6.238). Consider an orthogonal projector P ∗
satisfying
Im PS∞⊆Im P ∗⊆Im P.
(6.297)

6.3
Symplectic Systems Without Controllability
483
We say that a conjoined basis Y ∗is contained in Y on [N, ∞)Z with respect to P ∗,
or that Y contains Y ∗on [N, ∞)Z with respect to P ∗, if the solutions Y ∗and YP ∗
are equivalent on [N, ∞)Z, i.e., if X∗
k = XkP ∗for all k ∈[N, ∞)Z.
Remark 6.83 The conjoined basis Y ∗, which is contained in Y on [N, ∞)Z with
respect to the projector P ∗, has also a constant kernel on [N, ∞)Z and no forward
focal points in (N, ∞). The ﬁrst property can be seen from X∗
k = XkP ∗and
Ker Xk = Ker P ⊆Ker P ∗, which imply that Ker X∗
k = Ker P ∗on [N, ∞)Z. This
also shows that P ∗= PIm (X∗
k)T is the projector associated with Y ∗through (6.233)
and (6.234). The fact that Y ∗has no forward focal points in (N, ∞) is proven as
follows. From the identity P ∗= PP ∗, we get
(X∗
k)† = P ∗(X∗
k)† = PP ∗(X∗
k)† = X†
kXkP ∗(X∗
k)†
= X†
kX∗
k(X∗
k)† = X†
kR∗
k.
(6.298)
Therefore, by Theorem 6.66(ii), we obtain on [N, ∞)Z
X∗
k(X∗
k+1)†Bk = XkP ∗(X∗
k+1)†Bk = Xk(X∗
k+1)†Bk
(6.298)
=
XkX†
k+1R∗
k+1Bk = XkX†
k+1Bk.
(6.299)
The last term is nonnegative deﬁnite, since Y has no forward focal points in (N, ∞).
We now utilize the matrices G and H in (6.295)–(6.296) in order to describe
the conjoined bases of (SDS), which are contained in a given conjoined basis
(Theorem 6.84) or which contain a given conjoined basis (Theorem 6.87). We
now introduce the set M(P ∗∗, P ∗, P) of pairs of matrices (G, H) associated with
orthogonal projectors P ∗∗, P ∗, P satisfying the inclusions Im P ∗∗⊆Im P ∗⊆
Im P. We deﬁne
M(P ∗∗, P ∗, P) =
!
(G, H) ∈Rn×n × Rn×n, rank (GT, H T, P ∗) = n,
P ∗∗G = 0, PG = G, P ∗G = GT P ∗, PH = 0
"
,

(6.300)
Note that the set M(P ∗∗, P ∗, P) is always nonempty, because the pair (G, H) with
G := P −P ∗and H := I −P belongs to M(P ∗∗, P ∗, P). Note also that if P = I,
then H = 0. The following result describes all conjoined bases Y ∗of (SDS), which
are contained in a given conjoined basis Y with respect to a ﬁxed projector P ∗.
Theorem 6.84 Let Y be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞). Let P and PS∞be the associated
orthogonal projectors in (6.234) and (6.238). Consider an orthogonal projector
P ∗satisfying (6.297). Then a conjoined basis Y ∗of (SDS) is contained in Y on
[N, ∞)Z with respect to P ∗if and only if for some (G, H) ∈M(PS∞, P ∗, P)
X∗
N = XNP ∗,
U∗
N = UNP ∗+ X†T
N G + UNH.
(6.301)

484
6
Miscellaneous Topics on Symplectic Systems
Proof Let Y ∗be a conjoined basis of (SDS) which is contained in Y on [N, ∞)Z
with respect to P ∗. Then Y and YP ∗are equivalent on [N, ∞)Z by Deﬁnition 6.82.
From Proposition 6.81 (with Y (1) := YP ∗and Y (2) := Y ∗), it follows that
Y ∗satisﬁes the initial conditions in (6.301) with the matrices G and H such
that PS∞G = 0, PG = G, and PH
= 0. We will show that (G, H) ∈
M(PS∞, P ∗, P). Multiplying (6.301) by (X∗
N)T , we get
(X∗
N)T U∗
N = P ∗XT
NUNP ∗+ P ∗XT
NXT †
N G + P ∗XT
NUNH.
(6.302)
The symmetry of XT
NUN and the identities XNP = XN and PH = 0 imply that
P ∗XT
NUNH = P ∗UT
NXNH = P ∗UT
N XNPH = 0.
(6.303)
Upon inserting (6.303) into (6.302) and using XT
NXT †
N
= P and PG = G, we
obtain
P ∗G = P ∗PG = P ∗XT
NXT †
N G = (X∗
N)T U∗
N −P ∗XT
NUNP ∗.
This shows that the matrix P ∗G is symmetric, i.e., P ∗G = GT P ∗. Furthermore, if
v ∈Rn is a vector such that v ∈Ker G ∩Ker H ∩Ker P ∗, then (6.301) implies that
v ∈Ker X∗
N ∩Ker U∗
N = {0}, because Y ∗is a conjoined basis. Therefore, Ker G ∩
Ker H ∩Ker P ∗= {0}, which is equivalent with rank (GT, H T, P ∗) = n. The
above properties of G and H imply that (G, H) ∈M(PS∞, P ∗, P). Conversely, it
is easy to see that for any pair (G, H) ∈M(PS∞, P ∗, P) the solution Y ∗of (SDS)
satisfying the initial conditions in (6.301) is a conjoined basis, which is contained
in Y on [N, ∞)Z with respect to P ∗.
⊓⊔
It follows from Proposition 6.81 that the pair (G, H) ∈M(PS∞, P ∗, P), which
determines the conjoined basis Y ∗in Theorem 6.84, is unique. For this reason we
also say that Y ∗is contained in Y on [N, ∞)Z through the pair (G, H). Moreover,
the matrix G in (6.301) satisﬁes G = w(Y, Y ∗), which can be veriﬁed by the
calculation of the Wronskian at k = N.
Remark 6.85 For every orthogonal projector P ∗satisfying (6.297), there always
exists a conjoined basis Y ∗which is contained in Y with respect to P ∗on
[N, ∞)Z, for example, it is the conjoined basis Y ∗of (SDS) given by the initial
conditions (6.301) with G := P −P ∗and H := I −P. This follows from
Theorem 6.84 and from the fact that the above choice of (G, H) belongs to
M(PS∞, P ∗, P).
In the following result, we derive an additional property of the conjoined bases
Y ∗of (SDS) which are contained in Y on [N, ∞)Z.
Proposition 6.86 Let Y be a conjoined basis of (SDS) with constant kernel
on [N, ∞)Z and no forward focal points in (N, ∞). Let Sk in (6.236) be its
corresponding S-matrix. If Y ∗is any conjoined basis of (SDS) which is contained

6.3
Symplectic Systems Without Controllability
485
in Y on [N, ∞)Z and if S∗
k is its corresponding S-matrix, then S∗
k = Sk for all
k ∈[N, ∞)Z.
Proof Let P ∗be an orthogonal projector satisfying (6.297) such that Y ∗is contained
in Y with respect to P ∗on [N, ∞)Z., i.e., X∗
k = XkP ∗on [N, ∞)Z. Then by
Remark 6.83, the conjoined basis Y ∗has constant kernel on [N, ∞)Z and no forward
focal points in (N, ∞). In turn, by Theorem 6.66(ii) and identity (6.298), we have
Sk
(6.236)
=
k−1

j=N
X†
j+1 Bj X†T
j
=
k−1

j=N
X†
j+1R∗
j+1BjR∗
j X†T
j
=
k−1

j=N
(X∗
j+1)† Bj (X∗
j )†T (6.236)
=
S∗
k
(6.304)
for all k ∈[N, ∞)Z, which completes the proof.
⊓⊔
Next we present a supplement to Theorem 6.84 in the sense that we construct
conjoined bases ˜Y of (SDS) with constant kernel on [N, ∞)Z, which contain a given
conjoined basis Y ∗on [N, ∞)Z according to Deﬁnition 6.82. This construction is
based on a suitable choice of the initial conditions of ˜Y at k = N. We shall see that
this choice is closely related with the set in (6.300)
Theorem 6.87 Let Y ∗be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞). Let P ∗, R∗
k, and PS∗∞be the
associated projectors deﬁned in (6.234), (6.233), and (6.238) through X∗. Let P
and R be orthogonal projectors satisfying
Im P ∗⊆Im P,
Im R∗
N ⊆Im R,
rankP = rankR,
(6.305)
and let (G, H) ∈M(PS∗∞, P ∗, P). If the n×n matrices X and U solve the system
XX† = R,
X†X = P,
(6.306)
XP ∗= X∗
N,
XT U = UT X,
X†T G + U(P ∗+ H) = U∗
N,
(6.307)
we denote by ˜Y the solution of (SDS) given by the initial conditions ˜XN = X and
˜UN = U. Then ˜Y has the following properties.
(i) The solution ˜Y is a conjoined basis of (SDS).
(ii) The conjoined basis ˜Y has constant kernel on [N, ∞)Z and no forward focal
points in (N, ∞). In addition, the associated projectors ˜P and ˜Rk deﬁned
in (6.234) and (6.233) through ˜X satisfy ˜P = P and ˜RN = R.
(iii) The conjoined basis Y ∗is contained in ˜Y on [N, ∞)Z through the pair (G, H)
or equivalently with respect to the projector P ∗.

486
6
Miscellaneous Topics on Symplectic Systems
Proof Let the conjoined basis Y ∗and the projectors P ∗, R∗, PS∗∞and P, R be as
in the above statement and let (G, H) ∈M(PS∗∞, P ∗, P). Let X and U solve
system (6.306)–(6.307), and let ˜Y be the solution of (SDS) given by the initial
conditions ˜XN = X and ˜UN = U. Let ˜Pk := ˜X†
k ˜Xk and ˜Rk := ˜Xk ˜X†
k be the
associated orthogonal projectors in (6.233). Then (6.306) implies that ˜PN = P
and ˜RN = R. For part (i) we observe that it is enough to check the two deﬁning
properties of a conjoined basis at k = N. The symmetry of ˜XT
N ˜UN follows from
the second equation in (6.307). Suppose now that ˜XNv = 0 and ˜UNv = 0 for
some v ∈Rn. Then Pv = 0 and P ∗v = 0, by the ﬁrst inclusion in (6.305). From
[285, Lemmas 5.3(iv) and 5.4], we then obtain that v ∈Im P ∗. This means that
v ∈Ker P ∗∩Im P ∗= {0}, i.e., v = 0. Thus, rank( ˜XT
N,
˜UT
N ) = n and ˜Y is
a conjoined basis. For part (ii) we ﬁrst observe that G = w( ˜Y , Y ∗), because by the
deﬁnition of M(PS∗∞, P ∗, P) in (6.300), we have PG = G and PH = 0 and
˜XT
NU∗
N −˜UT
NX∗
N
(6.307)
=
XT [X†T G+U(P ∗+H)]−UT XP ∗(6.306)
=
PG+UT XPH = G.
We deﬁne on [N, ∞)Z the symmetric matrix Q∗
k with the following property
Q∗
k := U∗
k (X∗
k)† + [U∗
k (X∗
k)†]T (I −R∗
k),
Q∗
kX∗
k = U∗
k (X∗
k)†X∗
k = U∗
k P ∗;
(6.308)
see [257, Section 2.1.1]. By (6.308) and Theorem 6.66(i)–(ii) applied to Y ∗, we get
R∗
kQ∗
k ˜Xk −R∗
k ˜Uk = (X∗
k)†T GT ,
BkR∗
k = Bk = R∗
k+1Bk,
R∗
kU∗
k = R∗
kU∗
k P ∗,

(6.309)
where the equalities in (6.309) are satisﬁed for k ∈[N, ∞)Z. It now follows that
X∗
k+1 = AkX∗
k + BkU∗
k = AkX∗
k + BkR∗
kU∗
k P ∗= (Ak + BkQ∗
k) X∗
k
(6.310)
on [N, ∞)Z. From (6.309) it follows that the function ˜X solves on [N, ∞)Z the
nonhomogeneous ﬁrst-order linear difference equation
˜Xk+1 = Ak ˜Xk + BkR∗
k ˜Uk = (Ak + BkQ∗
k) ˜Xk −Bk(X∗
k)†T GT .
(6.311)
Let k be the solution of the associated homogeneous equation
k+1 = (Ak + BkQ∗
k) k,
k ∈[N, ∞)Z,
N = X.
(6.312)
Note that k is correctly deﬁned in forward time on [N, ∞)Z and that the matrices
Ak + BkQ∗
k and hence k are not necessarily invertible. From (6.310) and the
fact NP ∗= X∗
N (see the ﬁrst condition in (6.307)), the uniqueness of solutions

6.3
Symplectic Systems Without Controllability
487
of (6.312) yields the equality X∗
k = kP ∗on [N, ∞)Z. Moreover,from the variation
of constants principle for equations (6.311) and (6.312), we get
˜Xk = k (P −S∗
k GT ),
k ∈[N, ∞)Z.
(6.313)
Indeed, for k = N we have N (P −S∗
NGT ) = XP = X = ˜XN, because S∗
N = 0,
and by (6.313), (6.312), P ∗(X∗
k+1)† = (X∗
k+1)†, and XP ∗= X∗
N from (6.307), we
get
˜Xk+1 −(Ak + BkQ∗
k) ˜Xk = −k+1 (S∗
k ) GT = −k+1 (X∗
k+1)†Bk(X∗
k)†T GT
= −k+1P ∗(X∗
k+1)†Bk(X∗
k)†T GT
= −X∗
k+1(X∗
k+1)†Bk(X∗
k)†T GT
= −R∗
k+1Bk(X∗
k)†T GT (6.309)
=
−Bk(X∗
k)†T GT .
Equality (6.313) and identities PP ∗= P ∗, GT P = GT , GT P ∗= P ∗G, PS∗∞G =
0, and P ∗S∗
k = S∗
k = S∗
k P ∗, S∗
k = S∗
k PS∗∞, from (6.305), (6.300), and (6.239)
imply that
˜XkP = ˜Xk,
˜XkP ∗= X∗
k
on [N, ∞)Z.
(6.314)
This shows that Im ˜XT
k ⊆Im P and Im X∗
k ⊆Im ˜Xk on [N, ∞)Z. Next we will
prove that for all k ∈[N, ∞)Z, we have
Im (P −S∗
k GT ) = Im P = Im (P −S∗
k GT )T = Im (P −S∗
k GT )†.
(6.315)
Fix an index k ∈[N, ∞)Z. Since Im S∗
k ⊆Im P ∗⊆Im P by Theorem 6.66(iv)
(with Y := Y ∗) and assumption (6.305), the equality S∗
k = PS∗
k holds. This in
turn implies that P −S∗
k GT = P −PS∗
k GT and hence Im (P −S∗
k GT ) ⊆Im P.
Next we consider a vector v ∈Ker (P −S∗
k GT ), so that by GT = GT P we get
Pv = S∗
k GT v = S∗
k GT Pv. Then the vector w := Pv satisﬁes w = S∗
k GT w,
i.e., w ∈Im S∗
k ⊆Im PS∗∞. Thus, w = PS∗∞w, and consequently with the aid of
GT PS∗∞= 0, we obtain w = S∗
k GT w = S∗
k GT PS∗∞w = 0. This shows that Pv =
w = 0, so that v ∈Ker P. Therefore, we proved that Ker (P −S∗
k GT ) ⊆Ker P,
which is equivalent with Im P ⊆Im (P −S∗
k GT )T . Altogether, we showed that
Im (P −S∗
k GT ) ⊆Im P ⊆Im (P −S∗
k GT )T .
(6.316)
Since the dimensions of the subspaces on the left-hand and right-hand sides above
are equal (as rank A = rank AT ), it follows that in (6.316) we actually have
the equalities. This proves the ﬁrst two equalities in (6.315). Finally, the last
equality in (6.315) follows from Remark 1.60(i). We now consider the time-reversed

488
6
Miscellaneous Topics on Symplectic Systems
symplectic system (2.47) for ˜Y, i.e.,
˜Xk = DT
k ˜Xk+1 −BT
k ˜Uk+1,
˜Uk = −CT
k ˜Xk+1 + AT
k ˜Uk+1,
k ∈[0, ∞)Z.
Its ﬁrst equation in combination with Bk = R∗
k+1Bk and with (6.308) and (6.309) at
k + 1 implies that ˜Xk satisﬁes on [N, ∞)Z the nonhomogeneous ﬁrst-order linear
difference equation
˜Xk = DT
k ˜Xk+1−BT
k ˜Uk+1 = (DT
k −BT
k Q∗
k+1) ˜Xk+1+BT
k (X∗
k+1)†T GT .
(6.317)
Fix an index M
∈[N, ∞)Z, and consider the solution k of the associated
homogeneous equation
k = (DT
k −BT
k Q∗
k+1) k+1,
k ∈[N, M −1]Z,
M = ˜XM(P −S∗
MGT )†.
(6.318)
From (6.318), (6.315), (6.313), and PP ∗= P ∗, we now obtain the equalities
k P = k,
k P ∗= X∗
k,
k ∈[N, M]Z,
(6.319)
since the sequences kP, k, kP ∗, and X∗
k satisfy the same difference equation
in (6.318) and
MP = ˜XM(P −S∗
MGT )†P
(6.315)
=
˜XM(P −S∗
MGT )† = M,
MP ∗= ˜XM(P −S∗
MGT )†P ∗(6.313)
=
M (P −S∗
MGT ) (P −S∗
MGT )†P ∗
(6.315)
=
MPP ∗= MP ∗= X∗
M.
We shall prove that
˜Xk = k (P −S∗
k GT ),
k ∈[N, M]Z.
(6.320)
Indeed, the sequence Vk = k (P −S∗
k GT ) satisﬁes on [N, M −1]Z the equation
Vk −(DT
k −BT
k Q∗
k+1) Vk+1
(6.318)
=
k (S∗
k ) GT = k (S∗
k )T GT
= k P ∗(X∗
k)†BT
k (X∗
k+1)†T GT
(6.319)
=
R∗
kBT
k (X∗
k+1)†T GT = BT
k (X∗
k+1)†T GT ,
which is the same equation as (6.317), and
VM =M(P −S∗
M GT )
(6.318)
=
˜XM(P −S∗
M GT )†(P −S∗
M GT )
(6.315)
=
˜XMP
(6.314)
=
˜XM.

6.3
Symplectic Systems Without Controllability
489
Therefore, the uniqueness of solutions of (6.317) in backward time yields that
equality (6.320) holds. Our next claim is to prove the equalities
Ker k = Ker P,
†
k k = P,
Ker ˜Xk = Ker P,
k ∈[N, M]Z.
(6.321)
We start with the ﬁrst condition in (6.321). By (6.318), the kernel of k is
nonincreasing on [N, M]Z, which yields through (6.320) that
Ker k ⊆Ker N = Ker ˜XN = Ker X = Ker P,
k ∈[N, M]Z.
On the other hand, the ﬁrst equality in (6.319) implies that Ker P ⊆Ker k on
[N, M]Z. Thus, Ker k = Ker P, which also yields the second condition in (6.321).
Finally, by Remark 1.62(iv) and S∗
k = PS∗
k , we have for k ∈[N, M]Z
Ker ˜Xk (6.320)
=
Ker k (P −S∗
k GT ) = Ker †
k k (P −S∗
k GT ) = Ker P (P −S∗
k GT )
= Ker (P −S∗
k GT ) = [ Im (P −S∗
k GT )T ]⊥(6.315)
=
(Im P )⊥= Ker P,
which completes the proof of (6.321). Since the index M ∈[N, ∞)Z was arbitrary,
it follows from the third condition in (6.321) that Ker ˜Xk = Ker P on [N, ∞)Z,
i.e., ˜Y has constant kernel on [N, ∞)Z and ˜Pk = ˜P = P on [N, ∞)Z. In addition,
by the same calculations as in (6.298) and (6.299) with Y := ˜Y, we can show that
˜X†
kR∗
k = (X∗
k)† and ˜Xk ˜X†
k+1Bk = X∗
k(X∗
k+1)†Bk ≥0 on [N, ∞)Z. This means that
the conjoined basis ˜Y has no forward focal points in (N, ∞) and the proof of part
(ii) is complete. For part (iii), we ﬁrst observe that Theorem 6.66(ii) and the same
calculation as in (6.304) imply ˜Sk = S∗
k on [N, ∞)Z. Therefore, P ˜S∞= PS∗∞and
consequently, Im P ˜S∞= Im PS∗∞⊆Im P ∗⊆Im P. Hence, Y ∗is contained in
˜Y by Deﬁnition 6.82. Moreover, since we now have (G, H) ∈M(P ˜S∞, P ∗, P), it
follows from (6.307) and Theorem 6.84 that Y ∗is contained in ˜Y through the pair
(G, H). The proof of Theorem 6.87 is complete.
⊓⊔
Remark 6.88 Note that in the discrete case we utilize both forward and backward
(time-reversed) systems (6.312) and (6.318) and that their solutions k and k
are in general singular. In the continuous case, both systems (6.312) and (6.318)
coincide, and the argument therein is more straightforward; see the proof of [285,
Theorem 5.6]. Moreover, the results in Theorem 6.87(ii) and Remark 6.83 show that
the relation being contained preserves not only the constant kernel on [N, ∞)Z of
the involved conjoined bases but also the property of having no forward focal points
in (N, ∞). This points to a fundamental difference in the discrete theory compared
with the continuous case.
In the next result, we provide a converse to Theorem 6.87. Namely, we prove that
the initial conditions of the conjoined bases ˜Y of (SDS), which contain a given Y ∗
on [N, ∞)Z, satisfy system (6.306)–(6.307).

490
6
Miscellaneous Topics on Symplectic Systems
Theorem 6.89 Let ˜Y and Y ∗be conjoined bases of (SDS) such that ˜Y has constant
kernel on [N, ∞)Z and no forward focal points in (N, ∞). If ˜Y contains Y ∗on
[N, ∞)Z with respect to the projector P ∗or equivalently through a pair (G, H) ∈
M(PS∗∞, P ∗, ˜P ), then the matrices ˜XN and ˜UN solve system (6.306)– (6.307) with
P := ˜P and R := ˜RN.
Proof By Theorem 6.84 with Y := ˜Y, condition (6.301) holds with a (unique)
pair (G, H) ∈M(P ˜S∞, P ∗, ˜P ). Since by Proposition 6.86 the corresponding
S-matrices satisfy ˜Sk = S∗
k on [N, ∞)Z, it follows that P ˜S∞= PS∗∞, and
consequently (G, H) ∈M(PS∗∞, P ∗, ˜P ). Set X := ˜XN and U := ˜UN. Then the
above choice of P and R yields that (6.305) as well as (6.306)–(6.307) are satisﬁed.
⊓⊔
The results in Theorems 6.87 and 6.89 show that the construction of conjoined
bases ˜Y of (SDS) with constant kernel on [N, ∞)Z and no forward focal points
in (N, ∞), which contain a given conjoined basis Y ∗with the same properties, is
completely characterized by the solutions of the algebraic system (6.306)–(6.307).
We note that this system is the same as in the continuous case; see [285, Section 5].
There it is known that system (6.306)–(6.307) is solvable with a suitable choice of
the matrices G and H; see [285, Theorem 5.7 and formula (5.25)] for more details.
At the same time, Theorem 6.84 allows to construct all conjoined bases of (SDS)
with the same properties as above, which are contained in the given Y ∗. Combining
these two results with estimate (6.279) yields a construction of conjoined bases Y
of (SDS) with constant kernel on [N, ∞)Z and no forward focal points in (N, ∞),
which have the rank of Xk equal to any value between n −d[N, ∞)Z and n.
This construction is based on the choice of the projectors P and P ∗in (6.297)
and (6.305).
Theorem 6.90 Assume that there exists a conjoined basis of (SDS) with constant
kernel on [N, ∞)Z and no forward focal points in (N, ∞). Then for any integer
value r between n −d[N, ∞)Z and n, there exists a conjoined basis Y of (SDS)
with constant kernel on [N, ∞)Z and no forward focal points in (N, ∞) such that
rankXk = r on [N, ∞)Z.
Proof Let Y ∗be the conjoined basis of (SDS) from the assumption of the theorem,
and let P ∗, R∗
k, and PS∗∞be its associated projectors from (6.234), (6.233),
and (6.238). Let r be an integer between n −d[N, ∞)Z and n. If r ≤rankP ∗, then
we choose an orthogonal projector P ∗∗such that rank P ∗∗= r and Im PS∗∞⊆
Im P ∗∗⊆Im P ∗holds; compare with (6.297). It follows by Theorem 6.84 that for
this projector P ∗∗, there exists a conjoined basis Y ∗∗of (SDS) with constant kernel
on [N, ∞)Z and no forward focal points in (N, ∞) such that Ker X∗∗
k = Ker P ∗∗on
[N, ∞)Z, i.e., rankX∗∗
k
= rankP ∗∗= r on [N, ∞)Z. Similarly, if r ≥rank P ∗, then
we choose orthogonal projectors P and R such that (6.305) holds and rankP = r.
Then the conjoined basis Y := ˜Y from Theorem 6.87 has the required properties
and rank Xk = rankP = r on [N, ∞)Z. The proof is complete.
⊓⊔

6.3
Symplectic Systems Without Controllability
491
In the next lemma, we derive a property of the Wronskian of two equivalent
conjoined bases of (SDS) with constant kernel on [N, ∞)Z.
Lemma 6.91 Let Y (1) and Y (2) be conjoined bases of (SDS) with constant kernel
on [N, ∞)Z, and let P (1), P (2) and PS(1)∞, PS(2)∞be the corresponding orthogonal
projectors deﬁned in (6.234) and (6.238) through X(1)
k
and X(2)
k , respectively. If Y (1)
is equivalent with Y (2) on [N, ∞)Z, then
Im w(Y (1), Y (2)) ⊆Im (P (1) −PS(1)∞),
Im [w(Y (1), Y (2))]T ⊆Im (P (2) −PS(2)∞).

(6.322)
Proof Let Y (1) be equivalent with Y (2) on [N, ∞)Z. By Proposition 6.81 (with Y :=
Y (1) and ˜Y := Y (2)), there exist matrices G, H ∈Rn×n such that
X(2)
N = X(1)
N ,
U(2)
N −U(1)
N
= (X(1)
N )†T G + U(1)
N H,
(6.323)
and Im G ⊆Im (P (1) −PS(1)∞) and Im H ⊆Im (I −P (1)). By (6.323) and the
symmetry of (X(1))T U(1), we obtain for the Wronskian N(1) := w(Y (1), Y (2)) at
k = N
N(1) = (X(1)
N )T (U(2)
N −U(1)
N ) = (X(1)
N )T (X(1)
N )†T G + (X(1)
N )T U(1)
N H.
(6.324)
From the equalities (X(1)
N )T (X(1)
N )†T = P (1) and X(1)
N P (1) = X(1)
N , we obtain
(X(1)
N )T (X(1)
N )†T G = P (1)G = G and
(X(1)
N )T U(1)
N H = (U(1)
N )T X(1)
N H = (U(1)
N )T X(1)
N P (1)H = 0.
Therefore, (6.324) gives N(1) = G and so Im N(1) = Im G ⊆Im (P (1) −
PS(1)∞). The second inclusion in (6.322) follows from the fact that −[N(1)]T is
the Wronskian of the solutions Y (2) and Y (1).
⊓⊔
In the next two results, we provide additional information about the relation
being contained, which will be utilized in the construction of dominant solutions
of (SDS) at ∞in Sect. 6.3.7. First we show that the relation being contained for
conjoined bases of (SDS) with constant kernel on [N, ∞)Z and no forward focal
points in (N, ∞) is invariant under suitable change of the interval [N, ∞)Z. Namely,
the point N can always be moved forward and under some additional conditions
also backward. We recall from Sect. 6.3.1 that the order of abnormality d[k, ∞)Z
of (SDS) is nondecreasing in k on [0, ∞)Z.
Proposition 6.92 Let Y and Y ∗be two conjoined bases of (SDS) with constant
kernel on [N, ∞)Z and no forward focal points in (N, ∞). Then the following
hold.

492
6
Miscellaneous Topics on Symplectic Systems
(i) If Y contains Y ∗on the interval [N, ∞)Z, then Y contains Y ∗also on the
interval [L, ∞)Z for all L ∈[N, ∞)Z.
(ii) Assume that d[N, ∞)Z = d∞. If Y contains Y ∗on the interval [L, ∞)Z for
some index L ∈[N, ∞)Z, then Y contains Y ∗also on the interval [N, ∞)Z,
and hence on [K, ∞)Z for every K ∈[N, ∞)Z.
Proof Fix L ∈[N, ∞)Z. We denote by Sk, S∗
k , resp., S(L)
k
, S∗(L)
k
, the S-matrices
corresponding to Y and Y ∗on the interval [N, ∞)Z, resp., on the interval [L, ∞)Z.
Then S(L)
k
= Sk −SL and S∗(L)
k
= S∗
k −S∗
L on [L, ∞)Z. Let P and P ∗be the
projectors in (6.234) deﬁned by the functions Xk and X∗
k. Moreover, let PS∞, PS∗∞
and PS(L)∞, PS∗(L)∞be the projectors associated with the matrices Sk, S∗
k and S(L)
k
,
S∗(L)
k
through (6.238). The inequalities 0 ≤S(L)
k
≤Sk and 0 ≤S∗(L)
k
≤S∗
k for
k ≥L or the inclusions in (6.239) then imply
Im PS(L)∞⊆Im PS∞,
Im PS∗(L)∞⊆Im PS∗∞.
(6.325)
For part (i) we suppose that Y contains Y ∗on [N, ∞)Z, that is, the inclusions
in (6.297) hold and Y ∗is equivalent with YP ∗on [N, ∞)Z by Deﬁnition 6.82.
Then Im PS(L)∞⊆Im P ∗⊆Im P as well, by the ﬁrst inclusion in (6.325), and Y ∗
is equivalent with YP ∗on [L, ∞)Z. Therefore, Y contains Y ∗also on [L, ∞)Z, by
Deﬁnition 6.82. For the proof of part (ii), we assume that d[N, ∞)Z = d∞. Then
d[N, ∞)Z = d[L, ∞)Z and by (6.276) and (6.325),
PS(L)∞= PS∞,
PS∗(L)∞= PS∗∞.
(6.326)
Now suppose that Y contains Y ∗on [L, ∞)Z. Moreover, let Y ∗∗be a conjoined
basis of (SDS) with constant kernel on [N, ∞)Z such that Y contains Y ∗∗on
[N, ∞)Z with respect to the projector P ∗. Such a conjoined basis always exists, by
Remark 6.85. According to part (i) of this theorem, Y contains Y ∗∗also on [L, ∞)Z
with respect to P ∗, and hence, Y ∗and Y ∗∗are equivalent on [L, ∞)Z. This means
that X∗
k = X∗∗
k
on [L, ∞)Z. We will show that the assumption d[N, ∞)Z = d∞
allows to extend the latter equality to the whole interval [N, ∞)Z. For this we deﬁne
N∗:= w(Y ∗, Y ∗∗). By Lemma 6.91 (with the initial index N := L, Y (1) := Y ∗,
Y (2) := Y ∗∗, P (1) := P ∗, PS(1)∞:= PS∗(L)∞, and the Wronskian w(Y ∗, Y ∗∗)), it
follows that Im N∗⊆Im (P ∗−PS∗(L)∞). Consequently, Im N∗⊆Im (P ∗−PS∗∞),
by the second equality in (6.326). On the other hand, the equality X∗
k = X∗∗
k
on
[L, ∞)Z implies that Im X∗
k = Im X∗∗
k
on [N, ∞)Z, by Theorem 6.77. Therefore,
the conjoined bases Y ∗and Y ∗∗are mutually representable on [N, ∞)Z in the sense
of Theorem 6.69. In particular, we have
Y ∗∗
k
= Y ∗
k M∗+ ¯Y ∗
k N∗,
k ∈[N, ∞)Z,
(6.327)
where ¯Y ∗is a conjoined basis from Proposition 6.67 associated with Y ∗and where
M∗∈Rn×n is a constant invertible matrix. By using (6.327), P ∗N∗= N∗, and

6.3
Symplectic Systems Without Controllability
493
Proposition 6.67(iv) (with Y := Y ∗and ¯Y := ¯Y ∗), we obtain on [N, ∞)Z
X∗∗
k = X∗
kM∗+ ¯X∗
kP ∗N∗= X∗
kM∗+ X∗
kS∗
k N∗= X∗
k(M∗+ S∗
k N∗).
(6.328)
But S∗
k N∗= S∗
k PS∗∞N∗= 0 on [N, ∞)Z. Therefore, (6.328) becomes X∗∗
k
=
X∗
kM∗on [N, ∞)Z. At the same time, we have X∗∗
k
= X∗
k on [L, ∞)Z, which gives
the formula X∗
kM∗= X∗
k on [L, ∞)Z. Multiplying the latter equation by (X∗
k)† from
the left and using the identity (X∗
k)†X∗= P ∗on [L, ∞)Z, we get P ∗M∗= P ∗.
Hence, X∗∗
k
= X∗
kM∗= X∗
kP ∗M∗= X∗
k on [N, ∞)Z. This shows that Y ∗and Y ∗∗
are equivalent on [N, ∞)Z, so that Y contains Y ∗also on [N, ∞)Z with respect to
P ∗. Finally, the fact that Y contains Y ∗on [K, ∞)Z for every K ≥N now follows
from part (i) of this theorem.
⊓⊔
Proposition 6.93 Let Y ∗be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞) with d[N, ∞)Z = d∞. Then the
following statements hold for every index L ∈[N, ∞)Z.
(i) If Y ∗∗is a conjoined basis of (SDS) with constant kernel on [L, ∞)Z and no
forward focal points in (L, ∞) and it is contained in Y ∗on [L, ∞)Z, then Y ∗∗
has constant kernel also on [N, ∞)Z and no forward focal points in (N, ∞).
(ii) If Y is a conjoined basis of (SDS) with constant kernel on [L, ∞)Z and no
forward focal points in (L, ∞) and it contains Y ∗on [L, ∞)Z, then Y has
constant kernel also on [N, ∞)Z and no forward focal points in (N, ∞).
Proof (i) Fix L ∈[N, ∞)Z and let Y ∗and Y ∗∗be as in the theorem. Furthermore,
let PS∗∞and PS∗(L)∞be the orthogonal projectors in (6.238) associated with Y ∗
on the intervals [N, ∞)Z and [L, ∞)Z, respectively, and let P ∗∗be the orthogonal
projector in (6.234) associated with Y ∗∗on [L, ∞)Z. Then X∗∗
k
= X∗
kP ∗∗for every
k ∈[L, ∞)Z. As in the proof of Proposition 6.92, the projector PS∗(L)∞is deﬁned
by the matrix S∗(L)
k
= S∗
k −S∗
L. Then 0 ≤S∗(L)
k
≤S∗
k for k ∈[L, ∞)Z and
hence Im PS∗(L)∞⊆Im PS∗∞. The assumption d[N, ∞)Z = d∞then implies the
identity [N, ∞)Z = [L, ∞)Z by (6.225), and the equality PS∗(L)∞= PS∗∞
by rank PS∗(L)∞= n −d[L, ∞)Z = n −d[N, ∞)Z = rankPS∗∞in (6.276).
From the former equality, it then follows that X∗∗
k
= X∗
kP ∗∗holds for every
k ∈[N, ∞)Z, which shows that Y ∗∗is contained in Y ∗also on [N, ∞)Z. Therefore,
Y ∗∗has constant kernel on [N, ∞)Z and no forward focal points in (N, ∞), by
Remark 6.83 (with Y := Y ∗and Y ∗:= Y ∗∗). For the proof of part (ii), assume that
Y is a conjoined basis of (SDS) with constant kernel on [L, ∞)Z and no forward
focal points in (L, ∞) such that Y ∗is contained in Y on [L, ∞)Z. Using similar
arguments as above, it then follows that X∗
k = XkP ∗for every k ∈[N, ∞)Z, where
P ∗is the constant orthogonal projector in (6.234) which corresponds to Y ∗. Let S∗
k
and Q∗
k be the matrices in (6.236) and (6.286) associated with Y ∗on [N, ∞)Z, and
let W := w(Y, Y ∗) be the (constant) Wronskian of Y and Y ∗. Then
PS∗∞W = PS∗(L)∞W = 0,
PW = W,
P ∗W = W T P ∗,
(6.329)

494
6
Miscellaneous Topics on Symplectic Systems
where P is the constant orthogonal projectors in (6.234) which corresponds to Y on
[L, ∞)Z. Similarly as in the proof of Theorem 6.87, we then obtain that
Im (P −S∗
k W T ) = Im P = Im (P −S∗
k W T )T= Im (P −S∗
k W T )†,
(6.330)
Xk+1 = (Ak + BkQ∗
k) Xk −Bk(X∗
k)† T W T ,
(6.331)
Xk = (DT
k −BT
k Q∗
k+1) Xk+1 + BT
k (X∗
k+1)† T W T ,
(6.332)
for all k ∈[N, ∞)Z. Furthermore, let k and k be, respectively, the solutions of
the associated homogeneous equations
k+1 = (Ak + BkQ∗
k) k,
k ∈[N, ∞)Z,
N = XN,
(6.333)
k = (DT
k −BT
k Q∗
k+1) k+1,
k ∈[N, L −1]Z,
L = XL(P −S∗
LW T )†.
(6.334)
The initial condition in (6.334) together with (6.330) and X†
LXL = P imply
Ker L = Ker [X†
LXL(P −S∗
LW T )†] = Ker [P(P −S∗
LW T )†] = Ker P.
(6.335)
The equalities XLP ∗= X∗
L and P ∗= PP ∗and the initial condition (6.334) give
X∗
L =XLPP ∗(6.330)
=
XL(P −S∗
LW T )†(P −S∗
LW T ) P ∗(6.329)
=
L (PP ∗−S∗
LP ∗W)
=L (P ∗−S∗
LW) = L (P ∗−S∗
LPS∗∞W) = L P ∗.
On the other hand, the matrix N satisﬁes NP ∗= XNP ∗= X∗
N by (6.333). By
using similar arguments as in the proof of Theorem 6.87, we get
Xk = k (P −S∗
k W T ),
k ∈[N, L]Z,
(6.336)
Xk = k (P −S∗
k W T ),
k ∈[N, ∞)Z,
(6.337)
Moreover, the kernel of k is nonincreasing on [N, L]Z, while the kernel of k is
nondecreasing on [N, ∞)Z by (6.333) and (6.334). In turn, by (6.335) we obtain the
identity kP = k for all k ∈[N, L]Z. Furthermore, equations (6.337) and (6.330)
yield the equality XkP = Xk on [N, ∞)Z. In particular, for k = N we have NP =
XNP = XN = N, which implies through (6.333) that kP = k for all k ∈
[N, ∞)Z. From (6.330), (6.336), and (6.337), it then follows that
k = kP = k (P −S∗
k W T ) (P −S∗
k W T )† = k (P −S∗
k W T ) (P −S∗
k W T )†
= kP = k

6.3
Symplectic Systems Without Controllability
495
for all k ∈[N, L]Z. Therefore, the matrix k = k has constant kernel on [N, L]Z
equal to Ker P by (6.335), and consequently Ker Xk = Ker P for all k ∈[N, L]Z
by (6.336) and (6.330). Thus, the conjoined basis Y has constant kernel on [N, ∞)Z.
Finally, according to (6.299) we have XkX†
k+1Bk = X∗
k(X∗
k+1)†Bk ≥0 for every
k ∈[N, ∞)Z. This means that Y has no forward focal points in (N, ∞) and the
proof is complete.
⊓⊔
6.3.4
Minimal Conjoined Bases
Inequality (6.279) motivates the following notions of a minimal conjoined basis
of (SDS) and a maximal conjoined basis of (SDS).
Deﬁnition 6.94 A conjoined basis Y of on [N, ∞)Z if rank Xk = n −d[N, ∞)Z
for all k ∈[N, ∞)Z. Similarly, Y is called a maximal conjoined basis on [N, ∞)Z
if rank Xk = n for all k ∈[N, ∞)Z.
Remark 6.95 The terminology in Deﬁnition 6.94 follows the estimate in (6.279),
as the minimal conjoined bases Y of (SDS) attain the smallest possible rank
of Xk on [N, ∞)Z, while the maximal conjoined bases Y of (SDS) attain the
largest possible rank of Xk on [N, ∞)Z (i.e., Xk is invertible on [N, ∞)Z in this
case). Sometimes we will call conjoined bases Y of (SDS), which satisfy the rank
condition n −d[N, ∞)Z < rankXk < n on [N, ∞)Z, as intermediate conjoined
bases on the interval [N, ∞)Z.
Minimal conjoined bases of (SDS) constitute an important tool in the investi-
gation of recessive solutions of (SDS). For example, given a conjoined basis Y
of (SDS) with constant kernel on [N, ∞)Z and no forward focal points in (N, ∞)
with the projectors P and PS∞in (6.234) and (6.238), then any conjoined basis
Y ∗of (SDS) which is contained in Y with respect to the projector P ∗:= PS∞is
a minimal conjoined basis of (SDS) on [N, ∞)Z. In fact, the property P = PS∞can
be shown to be characterizing the minimal conjoined bases Y of (SDS) on [N, ∞)Z.
Remark 6.96 If Y is a minimal conjoined basis of (SDS) on [N, ∞)Z, then
the abnormality of (SDS) on the interval [N, ∞)Z is necessarily maximal, i.e.,
d[N, ∞)Z = d∞. This follows from estimate (6.279) in Theorem 6.75, which yields
n −d[N, ∞)Z ≤rankXk = n −d∞,
k ∈[N, ∞)Z,
i.e.,
d[N, ∞)Z ≥d∞.
The opposite inequality d[N, ∞)Z ≤d∞holds by the deﬁnition of d∞in (6.224),
so that d[N, ∞)Z = d∞follows.
In the next result, we show further basic properties of minimal conjoined bases
of (SDS) on [N, ∞)Z.

496
6
Miscellaneous Topics on Symplectic Systems
Theorem 6.97 The following properties of minimal conjoined bases hold.
(i) Let Y be a minimal conjoined basis of (SDS) on [N, ∞)Z with the associated
projector P in (6.234). Then
0[N, ∞)Z = Im [UN(I −P)],
Im XN =

0[N, ∞)Z
⊥.
(6.338)
Consequently, the initial subspace Im XN is the same for all minimal conjoined
bases Y of (SDS) on [N, ∞)Z.
(ii) Let Y (1) and Y (2) be two minimal conjoined bases of (SDS) on [N, ∞)Z with
the corresponding projectors P (1) and P (2) deﬁned in (6.234) through X(1)
and X(2). Then Y (1) and Y (2) are equivalent on [N, ∞)Z if and only if
P (2) = P (1),
Y (2)
k
= Y (1)
k M,
k ∈[N, ∞)Z,
(6.339)
where M is a constant nonsingular matrix satisfying P (1)M = P (1).
(iii) Let Y (1) and Y (2) be two minimal conjoined bases of (SDS) on [N, ∞)Z with
their associated S-matrices S(1)
k
and S(2)
k
deﬁned in (6.236). If K ∈[N, ∞)Z
is an index such that
rankS(1)
k
= n −d[N, ∞)Z = rankS(2)
k
for all k ∈[K, ∞)Z,
then for k ∈[K, ∞)Z, we have the equality
(S(3−i)
k
)† = (L(i))T (S(i)
k )†L(i) + (L(i))T N(i),
i ∈{1, 2},
(6.340)
where the matrices L(i) and N(i) are from Theorem 6.69 and its proof, resp.,
from Remarks 6.70 and 6.71.
Proof The ﬁrst conclusion in (6.338) follows from equation (6.278) in Theo-
rem 6.75, since for a minimal conjoined basis Y on [N, ∞)Z, we have P = PS∞.
The second conclusion in (6.338) then follows from Theorem 6.66(i), since
Im XN = Im RN =

Im [UN(I −P)]
⊥=

0[N, ∞)Z
⊥.
This shows that the set Im XN does not depend on a choice of Y. Part (ii) is
a consequence of Proposition 6.81 with M := I + H. More precisely, by using
Proposition 6.81, the equivalence of the minimal conjoined bases Y (1) and Y (2)
on [N, ∞)Z means that X(2)
N
= X(1)
N and Im (U(2)
N
−U(1)
N ) ⊆0[N, ∞)Z, while
from (6.338) we get 0[N, ∞)Z = Im [U(1)
N (I −P (1))]. Therefore, the projectors
P (1) and P (2) satisfy P (2) = P (1) and U(2)
N
−U(1)
N
= U(1)
N H with a unique
matrix H satisfying P (1)H
= 0. Consequently, for M := I + H, we have
X(1)
N M = X(2)
N and U(1)
N M = U(2)
N . This completes the formulas in (6.339) by the
uniqueness of solutions of (SDS). In addition, the constant matrix M is nonsingular,

6.3
Symplectic Systems Without Controllability
497
because rankY (2)
k
= n. Finally, P (1)M = P (1) follows by the deﬁnition of M.
Conversely, if the minimal conjoined bases Y (1) and Y (2) satisfy the equalities
in (6.339) with a nonsingular matrix M such that P (1)M = P (1), then they
are equivalent on [N, ∞)Z. This follows from X(2)
N
= X(1)
N P (1)M = X(1)
N
and
U(2)
N
−U(1)
N
= U(1)
N (M −I), where Im (M −I) ⊆Im (I −P (1)). For part (iii)
we ﬁrst note that equality (6.262) holds on [N, ∞)Z, since Im X(1)
N
= Im X(2)
N by
part (i) of this theorem. We multiply (6.262) by (L(1))T from the right and use the
symmetry of L(1)(L(1))† = P (1) and the identity S(1)
k P (1) = S(1)
k
on [N, ∞)Z from
Theorem 6.66(iii) (with Y := Y (1)) to get
S(2)
k (L(1))T = (L(1) + S(1)
k N(1))† S(1)
k
for all k ∈[N, ∞)Z.
(6.341)
Fix k ∈[K, ∞)Z, where the index K satisﬁes K ≥Nm by Remark 6.76. Then
by the same remark, we have Im S(1)
k
= Im P (1) and Im S(2)
k
= Im P (2), since
P (1) = PS(1)∞and P (2) = PS(2)∞as Y (1) and Y (2) are minimal conjoined bases on
[N, ∞)Z. Moreover, with the aid of Remark 1.60(i) and (6.250), we get
Im (L(1))T = Im (L(1))† = Im L(2) = Im P (2),
(6.342)
and from (6.261) and Theorem 6.69(iv), we obtain
(L(1) + S(1)
k N(1)) (L(1) + S(1)
k N(1))† = P (1),
P (1)N(1) = N(1).
(6.343)
By using Remark 1.62 for the pseudoinverse of a product of two matrices and the
equalities S(1)
k (S(1)
k )† = PS(1)∞= P (1) and (S(2)
k )†S(2)
k
= PS(2)∞= P (2), and
(L(1))†L(1) = P (2) by (6.342), the Moore-Penrose pseudoinverse of the left-hand
side of (6.341) is equal to
[S(2)
k (L(1))T ]† = [P (2)(L(1))T ]† [S(2)
k P (2)]† (6.342)
=
(L(1))†T (S(2)
k )†,
(6.344)
while the Moore-Penrose pseudoinverse of the right-hand side of (6.341) is
[(L(1) + S(1)
k N(1))† S(1)
k ]† = (P (1)S(1)
k )† [(L(1) + S(1)
k N(1))† P (1)]†
(6.343)
=
(S(1)
k )†(L(1) + S(1)
k N(1)) = (S(1)
k )†L(1) + P (1)N(1)
= (S(1)
k )†L(1) + N(1).
(6.345)
Using (6.344) and (6.345) in the pseudoinverse of both sides of (6.341) then yields
(L(1))†T (S(2)
k )† = (S(1)
k )†L(1) + N(1).

498
6
Miscellaneous Topics on Symplectic Systems
If we now multiply the latter equation by (L(1))T from the left and use the symmetry
of (L(1))†L(1) = P (2), then formula (6.340) with i = 1 follows. The same formula
with i = 2 is then obtained by interchanging the roles of Y (1) and Y (2).
⊓⊔
Remark 6.98 The result in Theorem 6.97 implies that that any two minimal
conjoined bases Y ∗(1) and Y ∗(2) on [N, ∞)Z are always mutually representable in
the sense of Theorem 6.69, since by (6.338) they satisfy
Im X∗(1)
N
= (0[N, ∞)Z)⊥= Im X∗(2)
N
,
where 0[N, ∞)Z is the subspace of the initial conditions uN of the elements
u ∈[N, ∞)Z. Hence, there exist matrices M∗(1), N∗(1), M∗(2), N∗(2) as in
Theorem 6.69 such that
X∗(2)
N
= X∗(1)
N
M∗(1),
U∗(2)
N
= U∗(1)
N
M∗(1) + (X∗(1)
N
)†T N∗(1),
(6.346)
X∗(1)
N
= X∗(2)
N
M∗(2),
U∗(1)
N
= U∗(2)
N
M∗(2) + (X∗(2)
N
)†T N∗(2).
(6.347)
Moreover,by taking the limit as k →∞in (6.340), we obtain that the corresponding
matrices T ∗(1) and T ∗(2) in (6.237) associated with the minimal conjoined bases
Y ∗(1) and Y ∗(2) on [N, ∞)Z satisfy the equality
T ∗(2) = (M∗(1))T T ∗(1)M∗(1) + (M∗(1))T N∗(1).
(6.348)
In the next result, we study the situation when the minimal conjoined bases Y ∗(1)
and Y ∗(2) on [N, ∞)Z are constructed from already mutually representable con-
joined bases Y (1) and Y (2) with matrices M(1), N(1), M(2), N(2) from Theorem 6.69.
Proposition 6.99 Let Y (1) and Y (2) be conjoined bases of (SDS) with constant
kernel on [N, ∞)Z and no forward focal points in (N, ∞), and let P (1), P (2) and
PS(1)∞, PS(2)∞be the corresponding orthogonal projectors in (6.234) and (6.238).
Moreover, let Y ∗(1) be a minimal conjoined basis of (SDS), which is contained
in Y (1) on [N, ∞)Z with respect to PS(1)∞, and similarly, let Y ∗(2) be a minimal
conjoined basis of (SDS) which is contained in Y (2) on [N, ∞)Z with respect to
PS(2)∞. Suppose that Y (1) and Y (2) are mutually representable on [N, ∞)Z through
the matrices M(1), N(1), M(2), N(2) as in Theorem 6.69. If M∗(1), N∗(1) and M∗(2),
N∗(2) are the matrices corresponding to Y ∗(1) and Y ∗(2) in (6.346)– (6.347), then
P (i)M(i)PS(3−i)∞= PS(i)∞M∗(i),
N∗(i)(M∗(i))−1 = PS(i)∞N(i)(M(i))−1PS(i)∞,

i ∈{1, 2}.
(6.349)

6.3
Symplectic Systems Without Controllability
499
Proof With the notation from the proposition, we have by Theorem 6.84
X∗(1)
N
= X(1)
N PS(1)∞,
U∗(1)
N
= U(1)
N PS(1)∞+ (X(1)
N )†T G1 + U(1)
N H1,
(6.350)
X∗(2)
N
= X(2)
N PS(2)∞,
U∗(2)
N
= U(2)
N PS(2)∞+ (X(2)
N )†T G2 + U(2)
N H2
(6.351)
with (Gi, Hi) ∈M(PSi∞, PSi∞, Pi) for i = 1, 2. We consider the case i = 1,
since the other case i = 2 is obtained by interchanging the roles of the involved
conjoined bases. Inserting the ﬁrst equality from (6.350) and from (6.351) into
the ﬁrst formula in (6.346) gives X(2)
N PS(2)∞= X(1)
N PS(1)∞M∗(1), from which
we obtain by (6.255) that X(1)
N M(1)PS(2)∞= X(1)
N PS(1)∞M∗(1). Consequently,
multiplying the latter equality by (X(1)
N )† from the left and using the identities
(X(1)
N )†X(1)
N
= P (1), P (1) PS(1)∞= PS(1)∞, we get the ﬁrst formula in (6.349).
For the proof of the second formula in (6.349), we recall from Theorem 6.69 that
N(1) = w(Y (1), Y (2)) and N∗(1) = w(Y ∗(1), Y ∗(2)). In particular, at the point
k = N, we have
N(1) = (X(1)
N )T U(2)
N −(U(1)
N )T X(2)
N ,
N∗(1) = (X∗(1)
N
)T U∗(2)
N
−(U∗(1)
N
)T X∗(2)
N
.

(6.352)
Combining (6.352) with (6.350)–(6.351) leads to the expression
N∗(1) = PS(1)∞N(1)PS(2)∞
+ PS(1)∞(X(1)
N )T (X(2)
N )†T G2 + PS(1)∞(X(1)
N )T U(2)
N H2
−GT
1 (X(1)
N )†X(2)
N PS(2)∞−H T
1 (U(1)
N )T X(2)
N PS(2)∞.
(6.353)
We now calculate the last four terms on the right-hand side of (6.353) separately. By
the ﬁrst equality in (6.256) and the already proven ﬁrst formula in (6.349), by the
symmetry of (X(2))T U(2) and the identities X(2)
N H2 = 0, (X(2)
N )T (X(2)
N )†T = P (2),
and PS(2)∞G2 = 0, we have
PS(1)∞(X(1)
N )T (X(2)
N )†T G2 = PS(1)∞(M(2))T (X(2)
N )T (X(2)
N )†T G2
= PS(1)∞(M(2))T P (2)G2 = (M∗(2))T PS(2)∞G2 = 0,
PS(1)∞(X(1)
N )T U(2)
N H2 = PS(1)∞(M(2))T (X(2)
N )T U(2)
N H2
= PS(1)∞(M(2))T (U(2)
N )T X(2)
N H2 = 0.
Similarly, it follows by the ﬁrst equality in (6.255) and the ﬁrst part of (6.349), by
the symmetry of (X(1))T U(1) and the identities X(1)
N H1 = 0, (X(1)
N )†X(1)
N
= P (1),

500
6
Miscellaneous Topics on Symplectic Systems
and PS(1)∞G1 = 0 that the last two terms in (6.353) are equal to zero. Therefore, we
have N∗(1) = PS(1)∞N(1)PS(2)∞. Now we use the properties M∗(2) = (M∗(1))−1,
M(2) = (M(1))−1, N(1)P (2) = N(1) from Theorem 6.69 to get
N∗(1)(M∗(1))−1 = PS(1)∞N(1)PS(2)∞M∗(2) = PS(1)∞N(1)P (2)M(2)PS(1)∞
= PS(1)∞N(1)(M(1))−1PS(1)∞.
This shows the second equality in (6.349).
⊓⊔
In the last result of this subsection, we prove a relationship between conjoined
bases ¯Y (1) and ¯Y (2) in Proposition 6.67 associated with two minimal conjoined bases
Y (1) and Y (2).
Lemma 6.100 Let Y (1) and Y (2) be minimal conjoined bases of (SDS) on the
interval [N, ∞)Z, and let ¯Y (1) and ¯Y (2) be their associated conjoined bases from
Proposition 6.67, respectively. Then there exists a constant invertible n × n matrix
G such that
¯X(2)
k
= ¯X(1)
k G,
k ∈[N, ∞)Z.
(6.354)
Proof Let P (1), R(1)
k
and P (2), R(2)
k
be the orthogonal projectors in (6.234), (6.233)
associated with X(1) and X(2), respectively. First we note that Im X(1)
N
= Im X(2)
N
by Remark 6.98, so that R(1)
N
= R(2)
N . Next we represent Y (1) in terms of Y (2) and
¯Y (2), and both Y (2) and ¯Y (2) in terms of Y (1) and ¯Y (1). Thus, for i ∈{1, 2} and any
k ∈[N, ∞)Z, we have
Y (3−i)
k
= Y (i)
k M(i) + ¯Y (i)
k N(i),
¯Y (2)
k
= Y (1)
k
¯M(1) + ¯Y (1)
k
¯N(1),
(6.355)
where according to Theorem 6.69, the matrices M(1) and M(2) are invertible with
M(2) = (M(1))−1, N(2) = −(N(1))T , and
¯M(1) = −w( ¯Y (1), ¯Y (2)) = ( ¯U(1)
N )T ¯X(2)
N −( ¯X(1)
N )T ¯U(2)
N ,
(6.356)
¯N(1) = w(Y (1), ¯Y (2)) = (X(1)
N )T ¯U(2)
N −(U(1)
N )T ¯X(2)
N
= (M(2))T = (M(1))T −1.
(6.357)
Therefore, the matrix ¯N(1) is invertible. Since by Proposition 6.67(iv) at k = N,
we have P (1)( ¯U(1)
N )T = (X(1)
N )† and P (1)( ¯X(1)
N )T = S(1)
N (X(1)
N )T = 0, as well as
R(2)
N ¯X(2)
N = X(2)
N (X(2)
N )† ¯X(2)
N = 0, it follows from (6.356) that
P (1) ¯M(1) = (X(1)
N )† ¯X(2)
N = (X(1)
N )†R(1)
N ¯X(2)
N = (X(1)
N )†R(2)
N ¯X(2)
N = 0.

6.3
Symplectic Systems Without Controllability
501
Therefore, again by (6.355) we get for k ∈[N, ∞)Z
¯X(2)
k
= X(1)
k
¯M(1) + ¯X(1)
k
¯N(1) = X(1)
k P (1) ¯M(1) + ¯X(1)
k
¯N(1) = ¯X(1)
k
¯N(1).
This shows that (6.354) holds with the matrix G :=
¯N(1), which is by equal-
ity (6.357) invertible.
⊓⊔
6.3.5
Asymptotics of S-Matrices
In this subsection we derive some additional properties of the S-matrices for
conjoined bases of (SDS) with constant kernel on [N, ∞)Z and no forward focal
points in (N, ∞). First of all, by a diagonalization argument for symmetric matrices,
we can write the matrices Sk, S†
k, T in (6.236) and (6.237) in Theorem 6.65 as
Sk = V
Wk
0
0 0n−rk

V T,
S†
k = V
W −1
k
0
0
0n−rk

V T,
T = V
T⋆
0
0 0n−r∞

V T,
k ∈[N, ∞)Z.
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
(6.358)
Here V is a constant orthogonal n×n matrix, Wk are symmetric and positive deﬁnite
rk × rk matrices for k ∈[N, ∞)Z, and T⋆:= limk→∞W −1
k
is symmetric and
nonnegative deﬁnite r∞× r∞matrix. Note that by (6.239) the matrix T satisﬁes
Im T ⊆Im PS∞. The dimension rk = rankSk = rank ˆX[N]
k
of the matrices Wk
in (6.358) is calculated in the second condition in (6.275). This implies that the
rank of Sk changes (i.e., increases in view of Theorem 6.66(iv)) independently of
the conjoined basis Y from which Sk is constructed; compare with Remark 6.76.
Moreover, by using the formulas in (6.358), we can write the orthogonal projectors
PSk for k ∈[N, ∞)Z and PS∞deﬁned in (6.238) as
PSk = V
Irk
0
0 0n−rk

V T ,
PS∞= V
Ir∞
0
0 0n−r∞

V T .
(6.359)
In the following results, we analyze the properties of the S-matrices for such
conjoined bases of (SDS). In particular, we will see that the asymptotic properties
of the S-matrices are affected by the condition
d[N, ∞)Z = d[k, ∞)Z = d∞
for all k ∈[N, ∞)Z
(6.360)
on the maximal order of abnormality of (SDS) on [N, ∞)Z.

502
6
Miscellaneous Topics on Symplectic Systems
Proposition 6.101 Let Y be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞), let Sk be its corresponding S-
matrix in (6.236), and let T be the limit in (6.237). If (6.360) holds, then there exists
an index K ∈[N, ∞)Z such that
S†
k ≥T ≥0,
rank(S†
k −T ) = n −d∞,
k ∈[K, ∞)Z.
(6.361)
Proof Without loss of generality, we may assume that Y is a minimal conjoined
basis of (SDS) on [N, ∞)Z, since from a given Y we can always construct by
Theorem 6.84 a minimal conjoined basis, which is contained in Y and which by
Proposition 6.86 has the same matrix Sk (and hence T ). The assumptions then
imply that the associated projectors P and PS∞satisfy rankPS∞= n −d∞
and PS∞= P. This implies that P = S†
kSk = SkS†
k for k ∈[Nm, ∞)Z, where
Nm ∈[N, ∞)Z is the index from Remark 6.76, i.e., Im Sk = Im P is maximal
on [Nm, ∞)Z. Consider the auxiliary conjoined basis ˆY := Y −¯YT , where ¯Y
is the conjoined basis from Proposition 6.67. By Proposition 6.61, the conjoined
basis ˆY has constant kernel on [K, ∞)Z and no forward focal points in (K, ∞) for
some index K ∈[Nm, ∞)Z. By (6.239) the inclusion Im T ⊆Im P holds, i.e.,
T = PT . Moreover, Proposition 6.67(iv), X†
kXk = P, and PSk = Sk imply that for
k ∈[K, ∞)Z we have
ˆXk = Xk −¯XkT = XkP −¯XkPT = Xk(P −SkT ),
(6.362)
P −SkT = P(P −SkT ) = X†
kXk(P −SkT )
(6.362)
=
X†
k ˆXk.
(6.363)
Similarly, from S†
kSk = P = SkS†
k, we obtain for k ∈[K, ∞)Z
S†
k −T = S†
k −PT = S†
k(P −SkT ),
P −SkT = Sk(S†
k −T ).
(6.364)
Equations (6.362)–(6.364) imply that
Ker (P −SkT ) = Ker ˆXk = Ker (S†
k −T ),
k ∈[K, ∞)Z.
(6.365)
Let ˆP be the orthogonal projector deﬁned in (6.234) through the conjoined basis
ˆY on the interval [K, ∞)Z. We will show that Im ˆP = Im P, i.e., ˆP = P by
the uniqueness of orthogonal projectors. One inclusion follows from (6.365), since
Im ˆP = Im ˆXT
k = Im (S†
k −T ) ⊆Im P for k ∈[K, ∞)Z. On the other hand, by
assumption (6.360) and inequality (6.279) in Theorem 6.75 (with Y := ˆY and with
N := K), we have
rank P = n −d[N, ∞)Z
(6.360)
=
n −d[K, ∞)Z
(6.279)
≤
rank ˆP .

6.3
Symplectic Systems Without Controllability
503
Thus, Im P = Im ˆP follows. Since ˆP is the orthogonal projector onto Im ˆXT
k on
[K, ∞)Z and since ˆP = P as we just proved, the second equality in (6.365) reads
as Ker (S†
k −T ) = Ker P for all k ∈[K, ∞)Z. Consequently,
rank (S†
k −T ) = rankP = n −d[K, ∞)Z = n −d∞,
k ∈[K, ∞)Z,
(6.366)
as we claim in (6.361). Finally, the ﬁrst condition in (6.361) follows from the fact
that T is the limit of S†
k for k →∞and from the monotonicity of S†
k on [K, ∞)Z,
which is guaranteed on this interval by Theorem 6.65.
⊓⊔
Remark 6.102 The proof of Proposition 6.101 shows that the statement in (6.361)
can be extended to the whole interval [N, ∞)Z (instead only for large k), when the
maximal orthogonal projector PS∞is replaced by the projector PSk. Then
S†
k −PSk T PSk ≥0,
Ker (S†
k −PSk T PSk) = Ker PSk,
k ∈[N, ∞)Z.
(6.367)
The result in Proposition 6.101 implies that rank (S†
k −T ) does not depend on
the choice of the matrices Sk and T , i.e., it does not depend on the choice of the
conjoined basis Y with constant kernel on [N, ∞)Z and no forward focal points
in (N, ∞). In fact, we prove for such a conjoined basis Y of (SDS) the following
equivalence.
Theorem 6.103 Let Y be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞), and let Sk, T , PS∞be its
corresponding matrix in (6.236), (6.237), (6.238). Then the following two conditions
are equivalent:
rank (PS∞−SkT ) = n −d[N, ∞)Z,
k ∈[N, ∞)Z,
(6.368)
rank(S†
k −T ) = n −d[N, ∞)Z,
k ∈[M, ∞)Z for some M ∈[N, ∞)Z.
(6.369)
In this case we have the equalities
Im (PS∞−SkT ) = Im PS∞= Im (PS∞−SkT )T ,
k ∈[N, ∞)Z.
(6.370)
Proof In the proof of Proposition 6.101, we showed that (6.365) and (6.366) hold,
i.e., in the setting of the present theorem, we have
Ker (PS∞−SkT ) = Ker PS∞= Ker (S†
k −T ),
k ∈[K, ∞)Z,
(6.371)
where K ∈[Nm, ∞)Z and Nm is the index from Remark 6.76. That is, the space
Im Sk = Im PS∞is maximal on [K, ∞)Z. The implication (6.368) ⇒(6.369)
(with M := K) then follows from (6.371) and from (6.276) in Theorem 6.75. We
will prove the converse implication. Assume (6.369) and set L := max{M, K}.

504
6
Miscellaneous Topics on Symplectic Systems
Then (6.371) yields
Im (PS∞−SkT ) = Im PS∞,
k ∈[L, ∞)Z.
(6.372)
Multiplying (6.372) by T from the left and using that T PS∞= T , we get
Im (T −T SkT )] = Im [T (PS∞−SkT )] = Im T,
k ∈[L, ∞)Z.
(6.373)
We will show that this equality is satisﬁed even for all k ∈[N, ∞)Z. The matrix
function T SkT is symmetric, nondecreasing, and nonnegative deﬁnite on [N, ∞)Z,
and S†
k →T for k →∞by (6.237). Therefore, the limit theorem for monotone
matrix-valued functions (Theorem 1.88 in Sect. 1.6.5) implies that T SkT →T for
k →∞, where the convergence is monotone (nondecreasing). This implies that the
symmetric matrix function Gk := T −T SkT = T (PS∞−SkT ) is nonincreasing
and nonnegative deﬁnite on [N, ∞)Z and hence,
Im [T (PS∞−SkT )] = Im (T −T SkT )] ⊆Im T,
k ∈[N, ∞)Z.
(6.374)
The combination of (6.373) and (6.374) and the monotonicity of Gk implies that
Im [T (PS∞−SkT )] = Im T,
k ∈[N, ∞)Z,
(6.375)
or equivalently by taking the orthogonal complements
Ker [T (PS∞−SkT )] = Ker T,
k ∈[N, ∞)Z.
(6.376)
This then implies that
Ker (PS∞−SkT ) ⊆Ker T,
k ∈[N, ∞)Z,
(6.377)
since if (PS∞−SkT ) d = 0 for some vector d ∈Rn, then T (PS∞−SkT ) d = 0
and so d ∈Ker T by (6.376). Moreover, in this case PS∞d = (PS∞−SkT ) d = 0
as well, so that we also have the inclusion
Ker (PS∞−SkT ) ⊆Ker PS∞,
k ∈[N, ∞)Z.
(6.378)
On the other hand, we know by T = T P ˆS∞(even without assumption (6.369)) that
Ker (PS∞−SkT ) = Ker (PS∞−SkT PS∞) ⊇Ker PS∞,
k ∈[N, ∞)Z.
(6.379)
Combining (6.378) and (6.379) yields that
Ker (PS∞−SkT ) = Ker PS∞,
k ∈[N, ∞)Z,
(6.380)

6.3
Symplectic Systems Without Controllability
505
which implies through (6.276) the desired equality (6.368). Finally, the equality
Sk = PS∞Sk implies the inclusion Im (PS∞−SkT ) ⊆Im PS∞on [N, ∞)Z, while
under (6.368) or (6.369) the orthogonal complement of (6.380) yields the inclusion
Im PS∞⊆Im (PS∞−SkT )T on [N, ∞)Z. Therefore, the rank condition (6.368)
guarantees the equality of the subspaces in (6.370). The proof is complete.
⊓⊔
Remark 6.104 We note that the inequality Gk = T −T SkT ≥0 for large k,
derived in the proof of Theorem 6.103 through the limit theorem for monotone
matrix-valued functions (Theorem 1.88), also follows from the properties of the
Moore-Penrose pseudoinverse. Indeed, since we know that 0 ≤T ≤S†
k for large
k and S†
k →T monotonically for k →∞, it follows from Remark 1.60(vi) (with
A := T and B := S†
k) that 0 ≤T SkT ≤T for large k, i.e., Gk ≥0 for large k.
In this context the value of the limit limk→∞T SkT = T is an additional property,
which is in fact not needed for the proof of Theorem 6.103.
The next result shows that condition (6.368) turns out to be a characterization of
the interval, where the abnormality of system (SDS) is maximal.
Proposition 6.105 Equality (6.368) holds for some (and hence for any) S-matrix
Sk associated with a conjoined basis Y of (SDS) with constant kernel on [N, ∞)Z
and no forward focal points in (N, ∞) if and only if condition (6.360) holds.
Proof We have already proven in Proposition 6.101 and Theorem 6.103 that
condition (6.360) implies (6.368) for any S-matrix Sk associated with a conjoined
basis Y of (SDS) on [N, ∞)Z. Thus, we suppose that (6.368) holds for some such
a matrix Sk, which corresponds to a conjoined basis Y of (SDS) with constant kernel
on [N, ∞)Z and no forward focal points in (N, ∞). Without loss of generality (by
Theorem 6.84), we may assume that Y is a minimal conjoined basis on [N, ∞)Z.
Fix any index M ∈[N, ∞)Z. We will show that d[M, ∞)Z = d[N, ∞)Z by using
Theorem 6.75 and Remark 6.76. Consider the matrix function S(M)
k
deﬁned by
S(M)
k
:= Sk −SM for all k ∈[M, ∞)Z. Since the kernel of Y is constant on
[N, ∞)Z, and hence on [M, ∞)Z, it is straightforward to see from the deﬁnition
of Sk in (6.236) that S(M)
k
is the S-matrix corresponding to Y on [M, ∞)Z. The
following analysis shows that Im S(M)
k
= Im Sk for large k. By (6.239) and the
deﬁnition of PS∞in (6.238), we have Im SM ⊆Im PS∞and Im Sk = Im PS∞
for large k. This implies that SM = PS∞SM = SM PS∞and PS∞Sk = Sk with
PS∞= S†
kSk = SkS†
k for large k. Consequently,
S(M)
k
= PS∞Sk −SM S†
kSk = (PS∞−SMS†
k ) Sk
for large k.
(6.381)
If we now let k →∞, then PS∞−SMS†
k →PS∞−SMT . Moreover, this
limiting matrix satisﬁes Im (PS∞−SMT ) ⊆Im PS∞and also Im (PS∞−
SMT )T ⊆Im PS∞, because Im T ⊆Im PS∞. By using assumption (6.368) and
Theorem 6.103, we get
Im (PS∞−SMT ) = Im PS∞= Im (PS∞−SMT )T .

506
6
Miscellaneous Topics on Symplectic Systems
In a similar way, we have Im (PS∞−SMS†
k) ⊆Im PS∞= Im (PS∞−SMT )
and Im (PS∞−SMS†
k)T ⊆Im PS∞= Im (PS∞−SMT )T for all k ∈[N, ∞)Z.
Therefore,
Im (PS∞−SMS†
k) = Im (PS∞−SMS†
k)T = Im PS∞
for large k,
(6.382)
and by Lemma 1.61 (with Aj := PS∞−SMS†
k and A := PS∞−SMT ), we obtain
(PS∞−SMS†
k)† →(PS∞−SMT )†
for k →∞.
(6.383)
By Remark 1.62 and the equalities in (6.381) and (6.382), we now calculate
(S(M)
k
)† = (PS∞Sk)† [(PS∞−SMS†
k) PS∞]† = S†
k(PS∞−SMS†
k)†
(6.384)
for large k. By using Remark 1.62, the matrix S(M)
k
(S(M)
k
)† is the orthogonal
projector onto Im S(M)
k
. Thus, by (6.381) and (6.384), we have for large k that
S(M)
k
(S(M)
k
)† = (PS∞−SMS†
k) SkS†
k (PS∞−SMS†
k)†
= (PS∞−SMS†
k) (PS∞−SMS†
k)†,
(6.385)
where we used the identities SkS†
k = PS∞and S†
kPS∞= S†
k for large k. But since
by Remark 1.62 the matrix in (6.385) is the orthogonal projector onto the subspace
Im (PS∞−SMS†
k), we conclude from (6.382) and (6.385) that S(M)
k
(S(M)
k
)† = PS∞
for large t. This means that the two projectors onto Im S(M)
k
and Im Sk are the same
for large k (and they are equal to PS∞), so that Im S(M)
k
= Im Sk for large k. This
implies through Remark 6.76 that
n −d[M, ∞)Z = rankS(M)
k
= rankSk = n −d[N, ∞)Z
for large k.
This shows that d[N, ∞)Z = d[M, ∞)Z. Since the index M ∈[N, ∞)Z was
arbitrary, condition (6.368) holds, and the proof is complete.
⊓⊔
In our next result, we use the knowledge of the asymptotic properties of the
S-matrices and the T -matrices to provide a complete classiﬁcation of all minimal
conjoined bases of (SDS) on the given interval [N, ∞)Z, where the order of
abnormality d[N, ∞)Z is maximal. This turns out to be one of the crucial results
of this subsection, as it will be utilized in the characterization of the matrices T in
Theorem 6.107 below, as well as in the construction of minimal dominant solutions
of (SDS) at ∞in Sect. 6.3.7.
Below the letter N denotes also the matrix N from the representation formula in
Theorem 6.69. No confusion should arise regarding the notation for this matrix N
and for the index N used throughout this section for the interval [N, ∞)Z.

6.3
Symplectic Systems Without Controllability
507
Theorem 6.106 Let Y be a minimal conjoined basis of (SDS) on [N, ∞)Z with the
matrices PS∞and T deﬁned in (6.238) and (6.237), and assume d[N, ∞)Z = d∞.
Then a solution ˜Y of (SDS) is a minimal conjoined basis on [N, ∞)Z if and only if
there exist matrices M, N ∈Rn×n such that
˜XN = XN M,
˜UN = UN M + X†T
N N,
(6.386)
M is nonsingular,
MT N = NT M,
Im N ⊆Im PS∞,
(6.387)
NM−1 + T ≥0.
(6.388)
In this case the matrix ˜T in (6.237), which corresponds to ˜Y, satisﬁes
rank ˜T = rank (NM−1 + T ).
(6.389)
Proof Let Y and the index N ∈[0, ∞)Z be as in the theorem. Then the orthogonal
projector P deﬁned in (6.234) satisﬁes P = PS∞. If ˜Y is also a minimal conjoined
basis on [N, ∞)Z, then Im ˜XN
= Im XN by Theorem 6.97(i). Therefore, by
Theorem 6.69 (with Y (1) := Y and Y (2) := ˜Y), there exist matrices M, N ∈Rn×n
such that (6.386) and (6.387) hold. Moreover, let T and ˜T be the T -matrices deﬁned
in (6.237) through the functions Sk and ˜Sk in (6.236), which are associated with
Y and ˜Y, respectively. By using formula (6.348) (with T ∗(1) := T , T ∗(2) := ˜T ,
M∗(1) := M, and N∗(1) := N), we have
˜T = MT T M + MT N,
i.e.,
NM−1 + T = MT −1 ˜T M−1 ≥0,
(6.390)
since ˜T ≥0. This shows condition (6.388). Conversely, let ˜Y be a solution of (SDS)
satisfying (6.386)–(6.388). Then the conditions in (6.387) together with the identity
XT
N X†T
N = P = PS∞and the fact that Y is a conjoined basis imply that ˜Y is also
a conjoined basis of (SDS). Let Sk be the S-matrix in (6.236) corresponding to Y on
[N, ∞)Z. By Remark 6.70(iii), condition (6.386) then yields
˜Xk = Xk(PS∞M + SkN)
k ∈[N, ∞)Z.
(6.391)
We will show that ˜Y has constant kernel on [N, ∞)Z and that Ker ˜Xk = Ker PS∞M
on [N, ∞)Z. First we note that by the symmetry of MT N and PS∞N = N,
NM−1PS∞= MT −1NT PS∞= MT −1NT = NM−1
holds. Hence, by (6.391), we have for any k ∈[N, ∞)Z
˜Xk = Xk(PS∞M + SkNM−1M) = Xk(I + SkNM−1) PS∞M.
(6.392)
Therefore, Ker PS∞M ⊆Ker ˜Xk on [N, ∞)Z. Fix now k ∈[N, ∞)Z, v ∈Ker ˜Xk,
and set w := PS∞Mv. Then Xk(w + SkNM−1w) = 0 by (6.392). Multiplying the

508
6
Miscellaneous Topics on Symplectic Systems
latter equality by X†
k from the left and using the identities X†
kXk = PS∞, PS∞Sk =
Sk, and w = PS∞w, we get w = −SkNM−1w. This implies by using (6.238)
and (6.239) that w ∈Im Sk = Im PSk and consequently,
wT S†
kw = −wT S†
k SkNM−1w = −wT PSkNM−1PSkw.
(6.393)
Equality (6.393) and condition (6.388) then yield wT S†
kw ≤wT PSkT PSkw, or
equivalently wT (S†
k −PSkT PSk) w ≤0. But S†
k −PSkT PSk ≥0 according to
Remark 6.102 and thus, w ∈Ker (S†
k −PSkT PSk) = Ker PSk, by the second
formula in (6.367). Hence we obtain that w ∈Ker PSk ∩Im PSk = {0}. This shows
that w = 0, and then v ∈Ker PS∞M, i.e., Ker ˜Xk ⊆Ker PS∞M. Finally, (6.386)
and the invertibility of M imply that rank ˜Xk = rank ˜XN = rank XN = n −d∞
on [N, ∞)Z. This shows that ˜Y is a minimal conjoined basis of (SDS) on [N, ∞)Z.
The proof is complete.
⊓⊔
In the last result of this subsection, we present a criterion for the classiﬁcation
of all T -matrices, which correspond to (minimal) conjoined bases of (SDS) on
an interval [N, ∞)Z with the maximal order of abnormality.
Theorem 6.107 Assume that system (SDS) is nonoscillatory. Then D ∈Rn×n
is a T -matrix of some minimal conjoined basis Y of (SDS) on [N, ∞)Z with
d[N, ∞)Z = d∞if and only if
D is symmetric,
D ≥0,
rankD ≤n −d∞.
(6.394)
Proof Let D be a T -matrix associated with a minimal conjoined basis Y a given
interval [N, ∞)Z ⊆[0, ∞)Z. Let Sk and T be deﬁned in (6.236) and (6.237), so
that D = T . By Remark 6.96 we have d[N, ∞)Z = d∞. From Theorem 6.65
and (6.239), we obtain that D is symmetric, nonnegative deﬁnite and Im D ⊆
Im PS∞with PS∞deﬁned in (6.238). But since rank PS∞= n −d[N, ∞)Z =
n−d∞by (6.276), the condition rankD ≤n−d∞follows. Conversely, assume that
D ∈Rn×n satisﬁes (6.394). From the third condition in (6.394), we have that there
exists an orthogonal projector Q such that Im D ⊆Im Q and rank Q = n −d∞.
Furthermore, the nonoscillation of (SDS) and Theorem 6.90 (with r := n −d∞)
imply that there exists a minimal conjoined basis Y of (SDS) on an interval
[N, ∞)Z ⊆[0, ∞)Z. Let Sk, PS∞, and T be the matrices associated with Y
in (6.236), (6.238), and (6.237). Since d[N, ∞)Z = d∞, we have rank PS∞=
n −d∞= rankQ, and hence there exists an invertible matrix E satisfying
Im EPS∞= Im Q. The matrix E can be obtained, e.g., from the diagonalization
of PS∞and Q or from Theorem 1.91 in Sect. 1.6.6 (with P∗:= 0). In particular,
we then have Im E−1Q = Im PS∞, i.e., PS∞E−1Q = E−1Q. Deﬁne now the
matrices M, N ∈Rn×n by
M := ET ,
N := E−1D −T ET .
(6.395)

6.3
Symplectic Systems Without Controllability
509
We show that these matrices satisfy conditions (6.387) and (6.388) in Theo-
rem 6.106. The matrix M is invertible by its deﬁnition. The symmetry of D and
T implies that MT N = D −ET ET is also symmetric. Moreover, the equalities
QD = D, PS∞E−1Q = E−1Q, and PS∞T = T yield
PS∞N = PS∞E−1QD −T ET = E−1QD −T ET = E−1D −T ET = N.
This means that Im N ⊆Im PS∞. Finally, the inequality D ≥0 implies (6.388),
since NM−1 + T = (E−1D −T ET ) ET −1 + T = E−1D ET −1 ≥0. Therefore,
we proved that for a given D satisfying (6.394) and for any minimal conjoined basis
Y of (SDS) on [N, ∞)Z, the matrices M and N in (6.395) satisfy the conditions
in (6.387) and (6.388). Consider now the solution ˜Y of (SDS) given by the initial
conditions (6.386). By Theorem 6.106 it follows that ˜Y is a minimal conjoined
basis on [N, ∞)Z. Moreover, if ˜T is the matrix in (6.237) associated with ˜Y,
then by (6.390) satisﬁes ˜T = MT T M + MT N. By using (6.395) we then obtain
that ˜T = D. Therefore, the matrix D is a T -matrix associated with the minimal
conjoined basis ˜Y of (SDS) on [N, ∞)Z.
⊓⊔
Remark 6.108 We note that with the aid of Proposition 6.86 and Theorem 6.90, the
statement in Theorem 6.107 extends directly to any conjoined basis Y of (SDS) with
constant kernel on [N, ∞)Z and no forward focal points in (N, ∞).
6.3.6
Recessive Solutions at Inﬁnity
In this subsection we present the concept of a recessive solution of (SDS) at ∞. It is
deﬁned by the property T = 0 in (6.237). It is a generalization of the corresponding
notion in Deﬁnition 2.63 in Sect. 2.5.
Deﬁnition 6.109 (Recessive Solution at ∞) A conjoined basis ˆY of (SDS) is said
to be a recessive solution of (SDS) at ∞if there exists N ∈[0, ∞)Z such that ˆY has
constant kernel on [N, ∞)Z and no forward focal points in (N, ∞) and satisfying
lim
k→∞
ˆS†
k = 0,
ˆSk :=
k−1

j=N
ˆX†
j+1 Bj ˆXT †
j
,
(6.396)
i.e., the matrix ˆT in (6.237) associated with ˆY satisﬁes ˆT = 0.
If ˆY is a recessive solution of (SDS) at ∞such that ˆY has constant kernel
on [N, ∞)Z and no forward focal points in (N, ∞), then ˆXk satisﬁes the rank
condition in (6.279). Therefore, we introduce the following terminology regarding
the classiﬁcation of recessive solutions of (SDS) according to their rank.
Remark 6.110 Let ˆY be a recessive solution of (SDS) at ∞, and let r be its rank on
[N, ∞)Z. If r = n −d∞, then ˆY is called a minimal recessive solution of (SDS)

510
6
Miscellaneous Topics on Symplectic Systems
at ∞, while if r = n, then ˆY is called a maximal recessive solution of (SDS)
at ∞. This terminology corresponds to the two extreme cases in (6.279) or in
Theorem 6.115 below. We will use the special notation ˆY min = Y [∞] and ˆY max for
the recessive solutions of (SDS) at ∞, which are minimal and maximal, respectively,
according to the above deﬁnition. If n −d∞< r < n, then the recessive solution ˆY
will be called intermediate (of the rank r). We note that in the eventually controllable
case, we have d∞= 0 (see Remark 6.58), so that the minimal and maximal recessive
solutions of (SDS) at ∞coincide. Therefore, in this case all recessive solutions ˆY
of (SDS) at ∞have eventually ˆXk invertible, as it is known in Sect. 2.5, or in [16,
Section 3.11] and [81].
The next statement shows that moving the initial point of the interval [N, ∞)Z,
with respect to which a recessive solution at ∞is considered, to the right does not
change the property of being a recessive solution of (SDS).
Proposition 6.111 Let ˆY be a recessive solution of (SDS) at ∞on [N, ∞)Z. Then
condition (6.360) holds and ˆY is a recessive solution of (SDS) at ∞on [M, ∞)Z
for every M ∈[N, ∞)Z.
Proof Fix an index M ∈[N, ∞)Z. Let ˆSk deﬁned in (6.396) be the S-matrix
corresponding to ˆY on [N, ∞)Z with ˆT := limk→∞ˆS†
k = 0. Since by (6.276) we
have rank P ˆS∞= n−d[N, ∞)Z, it follows that condition (6.368) in Theorem 6.103
holds with T := ˆT = 0. This yields via Proposition 6.105 that (6.360) is satisﬁed,
in particular d[N, ∞)Z = d[M, ∞)Z. Now we consider the S-matrix
ˆS(M)
k
:=
k−1

j=M
ˆX†
j+1Bj ˆX†T
j
= ˆSk −ˆSM,
k ∈[M, ∞)Z,
(6.397)
for ˆY on [M, ∞)Z. Then as in the proof of Proposition 6.105, we have on [M, ∞)Z
( ˆS(M)
k
)† = (P ˆS∞ˆSk)† [(P ˆS∞−ˆSM ˆS†
k) P ˆS∞]† = ˆS†
k (P ˆS∞−ˆSM ˆS†
k )†.
(6.398)
Upon taking the limit as k →∞in (6.398), we obtain that
ˆT (M) := lim
k→∞( ˆS(M)
k
)† = ˆT (P ˆS∞−ˆSM ˆT )†.
(6.399)
Note that the limit of (P ˆS∞−ˆSM ˆS†
k)† for k →∞indeed exists and is equal
to (P ˆS∞−ˆSM ˆT )†, because the matrices P ˆS∞−ˆSM ˆS†
k have constant rank for
large k, which is equal to the rank of the limit matrix P ˆS∞−ˆSM ˆT ; see (6.368)
and Lemma 1.61 or Remark 1.60(v). Since ˆT = 0, equality (6.399) yields that
ˆT (M) = 0 and ˆY is a recessive solution of (SDS) at ∞on the interval [M, ∞)Z, by
Deﬁnition 6.109.
⊓⊔

6.3
Symplectic Systems Without Controllability
511
The following result shows that the relation “being contained” preserves the
property of being a recessive solution of (SDS).
Proposition 6.112 Let ˆY be a recessive solution of (SDS) at ∞on [N, ∞)Z. Then
every conjoined basis of (SDS) with constant kernel on [N, ∞)Z and no forward
focal points in (N, ∞), which is either contained in ˆY on [N, ∞)Z or which contains
ˆY on [N, ∞)Z, is also a recessive solution of (SDS) at ∞on the interval [N, ∞)Z.
Proof The result follows from Proposition 6.86 and from the proof of Theo-
rem 6.87(iii), since the relation “being contained” for conjoined bases of (SDS) with
constant kernel on [N, ∞)Z and no forward focal points in (N, ∞) preserves the
corresponding S-matrices. Speciﬁcally, the conjoined bases Y ∗which are contained
in ˆY on [N, ∞)Z satisfy S∗
k = ˆSk on [N, ∞)Z by Proposition 6.86, while the
conjoined bases ˜Y of (SDS) which contain ˆY on [N, ∞)Z satisfy ˜Sk = ˆSk on
[N, ∞)Z by the proof of Theorem 6.87(iii).
⊓⊔
In the next results, we discuss the existence and uniqueness of the minimal
recessive solution at ∞(Theorem 6.113) and the relationship of the nonoscillation
of (SDS) at ∞with the existence of the recessive solutions at ∞(Theorem 6.115).
The ﬁrst statement is a direct generalization of the results in Theorem 2.66 and
Remark 2.69, as we now drop the eventual controllability assumption.
Theorem 6.113 System (SDS) is nonoscillatory at ∞if and only if there exists
a minimal recessive solution of (SDS) at ∞. In this case the minimal recessive
solution is unique up to a right nonsingular multiple, that is, if ˆY is a minimal
recessive solution of (SDS) at ∞, then a solution ˆY (0) of (SDS) is a minimal
recessive solution at ∞if and only if ˆY (0)
k
= ˆYk ˆM on [0, ∞)Z for some invertible
matrix ˆM.
Proof Assume that system (SDS) is nonoscillatory. Let Y be any ﬁxed conjoined
basis of (SDS). Then there exists a sufﬁciently large N ∈[0, ∞)Z such that Y has
constant kernel on [N, ∞)Z and no forward focal points in (0, ∞) and such that
condition (6.360) holds. Let Sk and T be given in (6.236) and (6.237), and let P
and PS∞be the associated orthogonal projectors in (6.234) and (6.238). Without
loss of generality (by Theorem 6.84 and Remark 6.85 with P ∗:= PS∞), we may
assume that Y is a minimal conjoined basis on [N, ∞)Z, i.e., PS∞= P. Consider
the conjoined basis ˆY := Y −¯Y T , where ¯Y is given in Proposition 6.67. Then
ˆXk = Xk (P −SkT ),
Ker ˆXk = Ker (P −SkT ) = Ker P,
k ∈[N, ∞)Z,
(6.400)
where the second condition above follows from (6.370). Thus, ˆY has constant kernel
on [N, ∞)Z and the corresponding orthogonal projector ˆP satisﬁes ˆP = P. We shall
prove that ˆY has no forward focal points in (N, ∞). From (6.400) and (6.236), it

512
6
Miscellaneous Topics on Symplectic Systems
follows that
ˆX†
k = (P −SkT )†X†
k,
k ∈[N, ∞)Z,
(6.401)
P −SkT = P −Sk+1T + (Sk) T
= (P −Sk+1T ) + X†
k+1BkX†T
k T,
k ∈[N, ∞)Z.
(6.402)
Note that by (6.370) we have (P −Sk+1T ) (P −Sk+1T )† = P. Therefore,
by (6.400)–(6.402) we obtain
ˆXk ˆX†
k+1Bk = Xk (P −SkT ) (P −Sk+1T )†X†
k+1Bk
= Xk [ (P −Sk+1T ) + X†
k+1BkX†T
k T ] (P −Sk+1T )†X†
k+1Bk
= XkPX†
k+1Bk + XkX†
k+1BkX†T
k T [k+1]X†
k+1Bk
= XkX†
k+1Bk + BT
k X†T
k+1T [k+1]X†
k+1Bk,
(6.403)
where, similarly to (6.399), the matrix T [k+1] = T (P −Sk+1T )† and where
X†T
k+1P = X†T
k+1, as the projector P = PS∞is constant on [N, ∞)Z. Each term
in the sum in (6.403) is nonnegative deﬁnite, because Y has no forward focal points
in (0, ∞) and T [k+1] ≥0. This shows that ˆXk ˆX†
k+1Bk ≥0 on [N, ∞)Z, i.e., ˆY
has no forward focal points in (0, ∞) as well. Thus, ˆY is a minimal conjoined basis
on [N, ∞)Z. Let ˆSk be its corresponding S-matrix. From Proposition 6.97(iii) (with
Y (1) := Y, Y (2) := ˆY, L(1) := X†
N ˆXN = X†
NXN = P, and N(1) := −T ), we obtain
the equality ˆS†
k = S†
k −T for all k large enough. Formula (6.237) now implies
that ˆS†
k →0 for k →∞, i.e., ˆY is a minimal recessive solution of (SDS) at ∞.
Conversely, the existence of a minimal recessive solution of (SDS) at ∞, which is
a nonoscillatory conjoined basis of (SDS) at ∞, implies the nonoscillation of (SDS)
at ∞by Proposition 6.61.
Now we prove the uniqueness. Let ˆY be a minimal recessive solution of (SDS)
at ∞with respect to the interval [N, ∞)Z and let ˆY (0) be a minimal recessive
solution of (SDS) at ∞with respect to [N0, ∞)Z. Without loss of generality, we
may assume that N0 = N, since shifting the initial point to the right preserves
the property of being a recessive solution of (SDS) at ∞, by Proposition 6.111.
Let ˆP and ˆP (0) be the corresponding orthogonal projectors in (6.233) deﬁned
through ˆX and ˆX[0]. By Proposition 6.97(i), we know that Im ˆXN = Im ˆX[0]
N ,
which in turn implies by Theorem 6.69 (with Y (1) := ˆY (0) and Y (2) := ˆY) that
ˆY (0)
k
= ˆYk ˆM + ¯Y (2)
k
ˆN on [N, ∞)Z, where the matrix ˆM is constant and nonsingular
and the matrix ˆN satisﬁes Im ˆN ⊆Im ˆP . If the matrices ˆSk, ˆT and ˆS(0), ˆT (0) are now
deﬁned in (6.236), (6.237) through ˆX and ˆX(0), respectively, then formula (6.340)
in Proposition 6.97(iii) with i = 2 has the form
( ˆS(0)
k )† = ˆLT ˆS†
k ˆL + ˆLT ˆN
for large k,
(6.404)

6.3
Symplectic Systems Without Controllability
513
with ˆL = ˆP ˆM by Remark 6.70(ii). Upon taking the limit as k →∞in (6.404)
and using that ˆY and ˆY (0) are recessive solutions of (SDS) at ∞, i.e., ˆT = 0 =
ˆT (0), we obtain from (6.404) the equality 0 = ˆT (0) = ˆLT ˆT ˆL + ˆLT ˆN = ˆLT ˆN.
Multiplying this equation by ˆL†T from the left and using ˆL†T ˆLT = ˆP and ˆP ˆN =
ˆN (see Remark 6.70(ii) again), we obtain ˆN = 0. This means that ˆY (0)
k
= ˆYk ˆM
on [N, ∞)Z with
ˆM invertible. Conversely, if ˆY is a minimal recessive solution
of (SDS) at ∞with respect to the interval [N, ∞)Z, then the solution ˆY (0)
k
:= ˆYk ˆM
with an invertible ˆM obviously has constant kernel on [N, ∞)Z. Moreover, ˆY (0)
has also no forward focal points in (N, ∞), because the matrix ˆP =
ˆX†
k ˆXk =
ˆX†
k+1 ˆXk+1 is constant on [N, ∞)Z, the matrix
( ˆX(0)
k+1)† = ( ˆXk+1 ˆM)† = ( ˆX†
k+1 ˆXk+1 ˆM)†( ˆXk+1 ˆM ˆM−1)† = ( ˆP ˆM)† ˆX†
k+1
(6.405)
by Remark 1.62, and for k ∈[N, ∞)Z
ˆX(0)
k ( ˆX(0)
k+1)†Bk
(6.405)
=
ˆXk ˆM( ˆP ˆM)† ˆX†
k+1Bk = ˆXk ˆP ˆM( ˆP ˆM)† ˆP ˆX†
k+1Bk
= ˆXk( ˆP ˆM) ( ˆP ˆM)†( ˆP ˆM) ˆM−1 ˆX†
k+1Bk
= ˆXk( ˆP ˆM) ˆM−1 ˆX†
k+1Bk = ˆXk ˆX†
k+1Bk ≥0.
Hence, ˆY (0) is a minimal conjoined basis of (SDS) on [N, ∞)Z. Moreover, we have
the equality Im ˆX(0)
N
= Im ˆXN and, by Theorem 6.69 (with the same notation as
above), ˆN := N(2) = 0. As in (6.404) we then obtain ( ˆS(0)
k )† = ˆLT ˆS†
k ˆL for large k.
Since ˆS†
k →ˆT = 0 as k →∞, it follows that ( ˆS(0)
k )† →ˆT (0) = 0 for k →∞as
well. Therefore, ˆY (0) is a minimal recessive solution of (SDS) at ∞and the proof is
complete.
⊓⊔
In Proposition 6.151 and Theorem 6.153 in Sect. 6.3.10 below, we present further
characterizations of the minimal recessive solution of (SDS) at ∞.
Remark 6.114 In the last part of Theorem 6.113, we showed that if ˆY is a recessive
solution of (SDS) at ∞on [N, ∞)Z, then ˆY ˆM is also a recessive solution of (SDS)
at ∞on [N, ∞)Z with the same rank for any constant invertible matrix
ˆM.
This statement extends [16, Theorem 3.43(i)] to the general concept of recessive
solutions at ∞in this paper.
We note that Theorem 6.115 below is an extension of the existence part of
Theorem 6.113. In particular, we shall see that the existence of the minimal recessive
solution of (SDS) from Theorem 6.113 is used in the proof of the implication (i) ⇒
(ii) in Theorem 6.115. Also, the uniqueness (or some other classiﬁcation) of the
recessive solutions of (SDS) at ∞with rank r > n−d∞is not guaranteed; see, e.g.,
Remark 6.116.

514
6
Miscellaneous Topics on Symplectic Systems
Theorem 6.115 The following statements are equivalent.
(i) System (SDS) is nonoscillatory at ∞.
(ii) There exists a recessive solution ˆY of (SDS) at ∞.
(iii) For any integer value r between n−d∞and n, there exists a recessive solution
ˆY of (SDS) at ∞with the rank of ˆXk equal to r for large k.
Proof (i)⇒(ii) This follows from Theorem 6.113, as the nonoscillation of (SDS) at
∞implies the existence of the minimal recessive solution of (SDS) at ∞.
(ii)⇒(iii) Let ˆY be a recessive solution of (SDS) at ∞. By Deﬁnition 6.109,
there exists an index N ∈[0, ∞)Z such that ˆY is a conjoined basis of (SDS) with
constant kernel on [N, ∞)Z, no forward focal points in (N, ∞), and its associated
matrix ˆSk satisﬁes (6.396). From Proposition 6.111 we then have d[N, ∞)Z = d∞.
By Theorem 6.90, for any integer r between n −d∞= n −d[N, ∞)Z and n, there
exists a conjoined basis Y of (SDS) with constant kernel on [N, ∞)Z and no forward
focal points in (N, ∞) such that rank Xk = r on [N, ∞)Z. Moreover, the conjoined
basis Y is in Theorem 6.90 constructed in such a way that it is either contained in
or contains the recessive solution ˆY at ∞on [N, ∞)Z. In turn, Proposition 6.112
yields that Y is also a recessive solution of (SDS) at ∞on [N, ∞)Z.
(iii)⇒(i) The choice r := n −d∞yields the existence of the minimal recessive
solution of (SDS) at ∞, which through Theorem 6.113 implies the nonoscillation of
system (SDS) at ∞. Alternatively, we may use Proposition 6.61 to obtain the same
conclusion.
⊓⊔
In Theorem 6.113 we guarantee the uniqueness of the minimal recessive solution
of (SDS) at ∞. In the following remark, we show that the minimal recessive solution
is the only one for which this property is satisﬁed. This remark also shows that
nonunique recessive solutions of (SDS) will always exist as long as d∞≥1.
Remark 6.116 Let ˆY be a recessive solution of (SDS) at ∞with rank r satisfying
n −d∞≤r ≤n. Then ˆY is unique up to a right nonsingular multiple if and only
if r = n −d∞, that is, ˆY is a minimal recessive solution of (SDS). We shall prove
by construction the implication “⇒”, as the opposite direction “⇐” is contained
in Theorem 6.113. Let ˆY be a recessive solution of (SDS) at ∞on the interval
[N, ∞)Z with the projectors ˆP and P ˆS∞in (6.234) and (6.238). Set ˆM := 2I −ˆP
and ˆN := ˆP −P ˆS∞, and deﬁne the solution ˆY (0)
k
:= ˆYk ˆM + ¯Yk ˆN on [0, ∞)Z, where
¯Y is the conjoined basis of (SDS) associated with ˆY in Proposition 6.67. Since the
equality ˆP P ˆS∞= P ˆS∞holds by (6.239), it follows that ˆMT ˆN = ˆP −P ˆS∞is
symmetric. The invertibility of ˆM then yields that ˆY (0) is a conjoined basis of (SDS)
and ˆN = w( ˆY, ˆY (0)). Moreover, ¯Xk ˆP = ˆXk ˆSk by Proposition 6.67(iv) and ˆSk(I −
P ˆS∞) = 0 by the ﬁrst inclusion in (6.239) for all k ∈[N, ∞)Z. Then
X[0]
k
= ˆXk ˆM + ¯Xk ˆN = ˆXk ˆP (2I −ˆP) + ¯Xk ˆP (I −P ˆS∞)
= ˆXk ˆP + ˆXk ˆSk(I −P ˆS∞) = ˆXk

6.3
Symplectic Systems Without Controllability
515
on [N, ∞)Z, which shows that the solutions ˆY and ˆY (0) are equivalent on [N, ∞)Z.
Therefore, ˆY (0) is also a recessive solution of (SDS) at ∞on [N, ∞)Z with the
same rank r. Now if ˆY is unique up to a right nonsingular multiple, then necessarily
ˆN = 0. This means that ˆP = P ˆS∞, r = n −d∞, and ˆY is a minimal recessive
solution of (SDS) at ∞.
Remark 6.117 In the literature one can ﬁnd an alternative deﬁnition of a recessive
solution of (SDS) in terms of a limit. More precisely, by [16, pg. 115] a conjoined
basis ˆY of (SDS) is a recessive solution at ∞if for any other linearly independent
conjoined basis Y of (SDS) with Xk invertible for large k we have
lim
k→∞X−1
k
ˆXk = 0.
(6.406)
This property is known as the limit characterization (or the limit deﬁnition) of the
recessive solution at ∞and it goes back to the historical papers by Olver and
Sookne [238] and Gautschi [151]. When system (SDS) is eventually controllable,
then both concepts in Deﬁnition 2.63 and (6.406) coincide; see Theorem 2.67 and
[51, pg. 965] or [104, pg. 211]. However, the above deﬁnition (6.406) allows also
an eventually noncontrollable system (SDS) and its recessive solutions ˆY at ∞with
noninvertible ˆXk on [N, ∞)Z; see [16, pp. 116–117]. This poses a question on the
exact relationship between the limit property in (6.406) and the summation property
in (6.396) of a recessive solution of (SDS) at ∞. This question is also related with
the so-called dominant solutions of (SDS) at ∞considered in Sect. 2.5 or [16,
Theorems 3.35 and 3.43]. In the next subsections, we will complete this study and
show that the recessive solutions of (SDS) at ∞in Deﬁnition 6.109 are indeed the
smallest solutions at ∞when they are compared with suitable dominant solutions
of (SDS) at ∞.
As a ﬁnal result in this subsection, we present a block diagonal construction of
certain recessive solutions at ∞of symplectic systems arising in higher dimension.
With system (SDS) we consider another symplectic system (with possibly different
dimension n ∈N)
yk+1 = Skyk,
k ∈[0, ∞)Z,
yk =
xk
uk

,
Sk =
Ak Bk
Ck Dk

,
(SDS)
where Sk and J :=
 0 I
−I 0

are 2n × 2n matrices such that Sk is real symplectic,
i.e., ST
k J Sk = J for all k ∈[0, ∞)Z. Deﬁne the block diagonal matrices
A∗
k :=
Ak 0
0 Ak

, B∗
k :=
Bk 0
0 Bk

, C∗
k :=
Ck 0
0 Ck

, D∗
k :=
Dk 0
0 Dk


516
6
Miscellaneous Topics on Symplectic Systems
in the dimension n∗:= n + n, and consider the “augmented” system
y∗
k = S∗
k y∗
k ,
k ∈[0, ∞)Z,
y∗
k =
x∗
k
u∗
k

,
S∗
k =
A∗
k B∗
k
C∗
k D∗
k

,
(SDS∗)
where the dimension of S∗
k and J ∗:=
 0 I
−I 0

is 2n∗. Since the matrices Sk and Sk
are symplectic, it follows that (S∗
k )TJ ∗S∗
k = J ∗, i.e., the augmented system (SDS∗)
is a symplectic system. The following result shows that certain recessive solutions
of system (SDS∗) at ∞can be constructed from the recessive solutions of (SDS)
and (SDS) at ∞.
Theorem 6.118 Assume that the systems (SDS) and (SDS) are nonoscillatory at
∞. Let ˆY and ˆY be recessive solutions of (SDS) and (SDS) at ∞with rank equal
to r and r, respectively. Then the sequence ˆY ∗
k deﬁned by
ˆX∗
k :=
 ˆXk 0
0
ˆXk

,
ˆU∗
k :=
 ˆUk 0
0
ˆUk

,
k ∈[0, ∞)Z,
(6.407)
is a recessive solution of system (SDS∗) at ∞with rank equal to r∗:= r + r.
Moreover, the recessive solution ˆY ∗at ∞constructed in (6.407) is minimal
(maximal) if and only if the recessive solutions ˆY and ˆY at ∞are minimal
(maximal).
Proof By Proposition 6.111, there exists a common index N ∈[0, ∞)Z such that ˆY
and ˆY are recessive solutions of (SDS) and (SDS) at ∞with respect to the interval
[N, ∞)Z. It is obvious that the rank of ˆY ∗is equal to r∗= r + r and that the orders
of abnormality d[N, ∞)Z, d[N, ∞)Z, d∗[N, ∞)Z of systems (SDS), (SDS), (SDS∗)
satisfy d∗[N, ∞)Z = d[N, ∞)Z +d[N, ∞)Z. This follows from the structure of the
coefﬁcients in Sk. Moreover, if ˆMk, Mk, M∗
k and ˆTk, T k, T ∗
k and ˆPk, P k, P ∗
k and ˆSk,
ˆSk, ˆS∗
k are the matrices in (4.1) and (4.2) and (6.396) associated with the conjoined
bases ˆY, ˆY, ˆY ∗, then
M∗
k = diag{ ˆMk, Mk},
T ∗
k = diag{ ˆTk, T k},
P ∗
k = diag{ ˆPk, P k},
ˆS∗
k = diag{ ˆSk, ˆSk},
( ˆS∗
k )† = diag{ ˆS†
k, ˆS
†
k}.
This means that ˆY ∗has constant kernel on [N, ∞)Z, no forward focal points in
(N, ∞), and ( ˆS∗
k )† →0 as k →∞. Therefore, ˆY ∗is a recessive solution of the
augmented system (SDS∗) at ∞according to Deﬁnition 6.109.
⊓⊔
We conclude this subsection with three examples, which illustrate the presented
theory of recessive solutions of (SDS) at ∞.
Example 6.119 Let n = 1 and consider a scalar system (SDS) with Sk ≡
 1 1
0 1

.
This system is nonoscillatory at ∞, because it corresponds to the nonoscillatory
second-order Sturm-Liouville difference equation 2yk = 0 on [0, ∞)Z, and

6.3
Symplectic Systems Without Controllability
517
d[0, ∞)Z = d∞= 0. Therefore, by Theorem 6.115 (or Theorem 6.113 or
Theorem 2.66), the conjoined basis ˆYk ≡
 1
0

of (SDS) is the (unique) recessive
solution of (SDS) at ∞. The second linearly independent solution ˜Yk =
 k
1

of (SDS) is not in this case a recessive solution of (SDS) at ∞according to
Deﬁnition 6.109.
Example 6.120 Consider system (SDS) with Sk ≡I2n. This system is nonoscil-
latory at ∞, its solutions are constant on [0, ∞)Z, and its order of abnormality is
d[0, ∞)Z = d∞= n. Every conjoined basis of (SDS) is then a (constant) recessive
solution at ∞on the interval [0, ∞)Z. The recessive solutions of (SDS) at ∞with
a given rank r between 0 and n in Theorem 6.115 can be constructed by a suitable
choice of the orthogonal projector ˆP ∈Rn×n with rank ˆP = r. More precisely, the
solution ˆY with ˆX = ˆP and ˆU = I −ˆP is a recessive solution of (SDS) at ∞with
rank equal to rank ˆP . If ˆP = 0, then ˆY = ˆY min = Y [0] =
 0
I

is the (unique)
minimal recessive solution of (SDS) at ∞. In fact, it is this minimal recessive
solution at ∞, which is derived in [16, Example 3.39]. On the other hand, if ˆP = I,
then ˆY = ˆY max =
 I
0

is an example of a maximal recessive solution of (SDS) at
∞. Note that ˆY (0) =
 I
I

is also a maximal recessive solution of (SDS) at ∞, which
is not a constant multiple of the maximal recessive solution ˆY max =
 I
0

at ∞,
demonstrating the nonuniqueness of the recessive solutions at ∞with rank r ≥1 in
Remark 6.116.
Finally, we illustrate the construction of recessive solutions of (SDS) at ∞by
using Theorem 6.118.
Example 6.121 Consider symplectic system (SDS) with the coefﬁcients Ak ≡I3 =
diag{1, 1, 1}, Bk ≡diag{1, 0, 0}, Ck ≡03 = diag{0, 0, 0}, and Dk ≡I3 =
diag{1, 1, 1} on [0, ∞)Z. This system corresponds to the partitioned system (SDS∗),
which arises from the symplectic systems in Example 6.119 and Example 6.120
(with dimension two). System (SDS) is nonoscillatory at ∞and d[0, ∞)Z =
d∞= 2. By Theorem 6.118, ˆY min = (diag{1, 0, 0}, diag{0, 1, 1})T with rank
r = n −d∞= 1 is the minimal recessive solution of (SDS) at ∞, while ˆY max =
(I3, 03)T with rank r = n = 3 is one of the maximal recessive solutions of (SDS)
at ∞. Note that for both recessive solutions at ∞, we have ˆSk = diag{k, 0, 0} on
[0, ∞)Z, so that ˆS†
k = diag{1/k, 0, 0} →0 for k →∞, as required in (6.396).
6.3.7
Dominant Solutions at Inﬁnity
In this subsection we develop the notion of a dominant solution of (SDS) at
∞without any eventual controllability assumption. As a main result, we prove
(Theorem 6.128) an analog of Proposition 6.115 for dominant solutions at ∞.
Deﬁnition 6.122 (Dominant Solution at ∞)
A conjoined basis Y of (SDS) is said
to be a dominant solution of (SDS) at ∞if there exists an index N ∈[0, ∞)Z with

518
6
Miscellaneous Topics on Symplectic Systems
d[N, ∞)Z = d∞such that Y has constant kernel on [N, ∞)Z and no forward focal
points in (N, ∞) and the corresponding matrix T deﬁned in (6.237) satisﬁes
rank T = n −d∞.
(6.408)
From Theorem 6.107 and Remark 6.108, it follows that the dominant solutions
of (SDS) at ∞are deﬁned by the maximal possible rank of the associated matrix
T , while the recessive solutions of (SDS) at ∞are deﬁned by the minimal rank
of T . In this respect the dominant solution of (SDS) at ∞can also be called as
an antirecessive solution of (SDS) at ∞. This alternative terminology then complies
with the continuous time notions of principal and antiprincipal solutions at ∞of
linear Hamiltonian differential systems (1.103); see [285, Deﬁnition 7.1] and [286,
Deﬁnition 5.1]. We also note that with respect to (6.239) and (6.276) the matrix T
associated with a dominant solution Y of (SDS) at ∞has the property
Im T = Im PS∞.
(6.409)
In the following we introduce the notation for dominant solutions at ∞, which
is analogous to the terminology minimal recessive solution ˆY min and maximal
recessive solution ˆY max of (SDS) at ∞used in Remark 6.110.
Remark 6.123 Let Y be a dominant solution of (SDS) at ∞with r = rankXk on
[N, ∞)Z. If r = n −d∞, then Y is called a minimal dominant solution at ∞, while
if r = n, then Y is called a maximal dominant solution at ∞. This terminology
corresponds to the two extreme cases in formula (6.279). As before, we will use the
notation Y min and Y max for the minimal and maximal dominant solutions of (SDS)
at ∞, respectively. Moreover, if n −d∞< r < n, then the dominant solution Y at
∞is called intermediate.
When the system (SDS) is eventually controllable, then d∞= 0 and hence the
matrix T is positive deﬁnite. In this case Deﬁnition 6.122 reduces to Deﬁnition 2.63.
Our ﬁrst result shows that the dominant solutions of (SDS) at ∞can be
characterized by the limit of Sk alone instead of the limit of S†
k as k →∞. In some
situations this condition may be easier to verify in comparison with Deﬁnition 6.122.
Theorem 6.124 Let Y be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞), and assume that d[N, ∞)Z = d∞.
Let Sk and T be deﬁned in (6.236) and (6.237) through Y. Then the following
statements are equivalent.
(i) The conjoined basis Y is a dominant solution of (SDS) at ∞.
(ii) The limit of Sk exists as k →∞.
(iii) The matrices Sk and T satisfy the condition
lim
k→∞Sk = T †.
(6.410)

6.3
Symplectic Systems Without Controllability
519
Proof From (6.238) and (6.276) and from the assumption d[N, ∞)Z = d∞, we
know that the equalities rank S†
k = rankSk = rank PS∞= n −d∞hold for large k.
And since rank T = n−d∞is the deﬁning property for Y being a dominant solution
at ∞, it follows that rank S†
k = n −d∞= rankT for large k. This is equivalent by
Remark 1.60(v) (with Aj := S†
j and A := T ) with the existence of the limit of
(S†
k)† = Sk as k →∞, i.e., with condition (6.410).
⊓⊔
Next we discuss the dependence of Deﬁnition 6.122 on the initial index N in the
interval [0, ∞)Z. In this respect we obtain a similar statement to Proposition 6.111.
Proposition 6.125 Let Y be a dominant solution of (SDS) at ∞with respect to
the interval [N, ∞)Z. Then Y is a dominant solution at ∞also with respect to the
interval [M, ∞)Z for every M ∈[N, ∞)Z.
Proof Fix an index M ∈[N, ∞)Z, and let Sk and S(M)
k
be the matrices in (6.236)
and (6.397) corresponding to Y on the intervals [N, ∞)Z and [M, ∞)Z, respectively.
Furthermore, let PS∞and T be the associated matrices in (6.238) and (6.237). From
Deﬁnition 6.122 we have the conditions rank T = n −d∞and d[N, ∞)Z = d∞.
The latter equation implies that d[M, ∞)Z = d∞, by (6.224). By Proposition 6.101
and Theorem 6.103, we have that (6.370) holds, while from the proof of Proposi-
tion 6.111, we conclude that (6.398) is satisﬁed. Summarizing, the conjoined basis
Y satisﬁes
Im (PS∞−SkT ) = Im PS∞= Im (PS∞−SkT )T ,
k ∈[N, ∞)Z,
(6.411)
(S(M)
k
)† = S†
k (PS∞−SM S†
k)†,
k ∈[M, ∞)Z.
(6.412)
The matrices Gk := PS∞−SM S†
k satisfy Im Gk ⊆Im PS∞, which implies that
rankGk ≤rankPS∞for all k ∈[M, ∞)Z. Moreover, by (6.411) the limit matrix
G := limk→∞Gk = PS∞−SMT satisﬁes rankG = rankPS∞. On the other hand,
the inequality rank Gk ≥rankG for large k always holds for a limit of a sequence of
matrices. Therefore, rank Gk = rank G = rankPS∞for large k. Then the properties
of Moore-Penrose pseudoinverse in Remark 1.60(v) imply that by taking the limit
as k →∞in (6.412) the matrix T (M), deﬁned in (6.237) and corresponding to the
S-matrix S(M)
k
, satisﬁes
T (M) = lim
k→∞S†
k G†
k = T G† = T (PS∞−SMT )†.
(6.413)
We will show that Im T (M) = Im T . Indeed, identity (6.413) yields the inclusion
Im T (M) ⊆Im T . On the other hand, by (6.413) and (6.411), we obtain
T (M) (PS∞−SMT )
(6.413)
=
T (PS∞−SMT )†(PS∞−SMT )
(6.411)
=
T PS∞= T,

520
6
Miscellaneous Topics on Symplectic Systems
since Im T ⊆Im PS∞. This shows the opposite inclusion Im T ⊆Im T (M). Finally,
the equalities rankT (M) = rankT = n −d∞then imply that Y is a dominant
solution of (SDS) at ∞with respect to the interval [M, ∞)Z, by Deﬁnition 6.122.
⊓⊔
Remark 6.126 In the proof of Proposition 6.125 we show that the set Im T (M)
is preserved within the interval [N, ∞)Z, i.e., Im T (M) = Im T for every M ∈
[N, ∞)Z. Moreover, this statement obviously holds for any conjoined basis Y with
constant kernel on [N, ∞)Z and no forward focal points in (N, ∞) such that
d[N, ∞)Z = d∞.
In the following statement, we show that the property of Y being a dominant
solution of (SDS) at ∞is preserved under the relation being contained from
Sect. 6.3.3.
Proposition 6.127 Let Y be a dominant solution of (SDS) at ∞with respect to
the interval [N, ∞)Z. Then every conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞), which is either contained in Y on
[N, ∞)Z or which contains Y on [N, ∞)Z, is also a dominant solution of (SDS) at
∞with respect to the interval [N, ∞)Z.
Proof The result follows directly from Proposition 6.86 and Deﬁnition 6.122.
⊓⊔
Next we characterize the nonoscillation of system (SDS) in terms of the existence
of dominant solutions of (SDS) at ∞. It is an analog of Theorem 6.115.
Theorem 6.128 The following statements are equivalent.
(i) System (SDS) is nonoscillatory.
(ii) There exists a dominant solution of (SDS) at ∞.
(iii) For any integer r satisfying n −d∞≤r ≤n, there exists a dominant solution
Y of (SDS) at ∞with the rank of Xk equal to r for large k.
Proof If (SDS) is nonoscillatory, then by Theorem 6.107 for any symmetric and
nonnegative deﬁnite matrix D with rank D = n−d∞, there exists N ∈[0, ∞)Z and
a minimal conjoined basis Y of (SDS) on [N, ∞)Z such that d[N, ∞)Z = d∞and
the corresponding matrix T in (6.237) satisﬁes T = D, i.e., rankT = n−d∞. From
Deﬁnition 6.122 and Remark 6.123, it then follows that Y is a minimal dominant
solution of (SDS) at ∞. Suppose now that (ii) holds, and let Y be a dominant
solution of (SDS) at ∞, i.e., there exists N ∈[0, ∞)Z with d[N, ∞)Z = d∞such
that Y is a conjoined basis of (SDS) with constant kernel on [N, ∞)Z and no forward
focal points in (N, ∞), and its associated matrix T satisﬁes rankT = n −d∞. By
Theorem 6.90, for any integer r between n −d∞and n, there exists a conjoined
basis ˜Y of (SDS) with constant kernel on [N, ∞)Z and no forward focal points in
(N, ∞) and with rank ˜Xk = r on [N, ∞)Z such that ˜Y is either contained in Y or ˜Y
contains Y on [N, ∞)Z. Therefore, ˜Y is also a dominant solution of (SDS) at ∞, by
Proposition 6.127, showing part (iii). Finally, if (iii) is satisﬁed, then Y is a conjoined

6.3
Symplectic Systems Without Controllability
521
basis of (SDS) with ﬁnitely many focal points in (0, ∞). Therefore, system (SDS)
is nonoscillatory at ∞by Proposition 6.61. The proof is complete.
⊓⊔
For an eventually controllable system (SDS), we obtain from Theorem 6.128 the
following counterpart of Theorem 2.66 and Remark 2.69 in Sect. 2.5.1.
Corollary 6.129 Assume that (SDS) is eventually controllable. System (SDS) is
nonoscillatory at ∞if and only if there exists a dominant solution Y of (SDS)
at ∞with rank equal to n, i.e., with Xk eventually invertible. In this case the
corresponding matrix T in (6.237) is positive deﬁnite.
In the last result of this section, we present a construction of all recessive
and dominant solutions of (SDS) at ∞from the minimal recessive and dominant
solutions at ∞, respectively. In this construction we utilize two main ingredients:
(i) the properties in Theorem 6.90 applied to the minimal recessive and dominant
solutions at ∞and (ii) the uniqueness of the minimal recessive solution ˆY min
of (SDS) at ∞in Theorem 6.113.
Remark 6.130 If ˆY min is the (unique) minimal recessive solution of (SDS) at ∞,
then we deﬁne the point ˆKmin ∈[0, ∞)Z as the smallest index N ∈[0, ∞)Z such
that ˆY min has constant kernel on [N, ∞)Z and no forward focal points in (N, ∞).
In particular, we have the identities
d[ ˆKmin, ∞)Z = d[N, ∞)Z = d∞
for all N ∈[ ˆKmin, ∞)Z.
(6.414)
These equalities follow from Theorem 6.90, the deﬁnition of d∞in (6.224),
and from the fact that rank ˆXmin
k
= n −d∞on [ ˆKmin, ∞)Z. Moreover, by
Proposition 6.111 the conjoined basis ˆY min is a minimal recessive solution of (SDS)
at ∞with respect to the interval [N, ∞)Z for every N ∈[ ˆKmin, ∞)Z.
Theorem 6.131 Assume that system (SDS) is nonoscillatory at ∞with the index
ˆKmin deﬁned in Remark 6.130. Then the following statements hold.
(i) A solution Y of (SDS) is a recessive solution at ∞if and only if Y is a conjoined
basis of (SDS) with constant kernel on [ ˆKmin, ∞)Z and no forward focal points
in ( ˆKmin, ∞), which contains some minimal recessive solution of (SDS) on
[N, ∞)Z for some (and hence every) N ∈[ ˆKmin, ∞)Z.
(ii) A solution Y of (SDS) is a dominant solution at ∞if and only if Y is a conjoined
basis of (SDS), which contains some minimal dominant solution of (SDS) at
∞on [N, ∞)Z for some N ∈[ ˆKmin, ∞)Z.
Proof
(i) Let Y be a recessive solution of (SDS) at ∞. Then there exists an index
N ∈[0, ∞)Z such that Y has constant kernel on [N, ∞)Z and no forward
focal points in (N, ∞), and the corresponding matrix Sk in (6.236) satisﬁes
S†
k →0 for k →∞. From Proposition 6.111, we know that this property of Sk
is preserved under shifting the index N to the right. Therefore, we may assume

522
6
Miscellaneous Topics on Symplectic Systems
that N ∈[ ˆKmin, ∞)Z and hence, we have d[N, ∞)Z = d∞. Consequently, by
Theorem 6.90 there exists a conjoined basis Y ∗of (SDS) with constant kernel
on [N, ∞)Z and no forward focal points in (N, ∞) and with rank X∗
k = n−d∞
on [N, ∞)Z such that Y contains Y ∗on [N, ∞)Z. In turn, Proposition 6.112
implies that Y ∗is a minimal recessive solution of (SDS) at ∞with respect to
the interval [N, ∞)Z. From Proposition 6.92(i), we then obtain that Y contains
Y ∗also on [L, ∞)Z for all L ∈[N, ∞)Z. It remains to show that Y contains
Y ∗on [L, ∞)Z for all L ∈[ ˆKmin, N −1]Z. Let us ﬁx such an index L. By
Remark 6.130 we know that d[L, ∞)Z = d∞and that Y ∗has constant kernel
on [L, ∞)Z and no forward focal points in (L, ∞). Consequently, Y has also
constant kernel on [L, ∞)Z and no forward focal points in (L, ∞) according
to Proposition 6.93(ii). On the other hand, Proposition 6.92(ii) implies that Y
contains Y ∗also on [L, ∞)Z. This completes the proof of the ﬁrst implication.
Conversely, suppose that Y is a conjoined basis of (SDS) with constant kernel
on [ ˆKmin, ∞)Z and no forward focal points in ( ˆKmin, ∞). Let ˆY min be a minimal
recessive solution of (SDS) at ∞, which is contained in Y on [L, ∞)Z for some
L ∈[ ˆKmin, ∞)Z. By Remark 6.130, ˆY min has constant kernel on [ ˆKmin, ∞)Z
and no forward focal points in ( ˆKmin, ∞), and it is a minimal recessive solution
at ∞with respect to [N, ∞)Z for every N ∈[ ˆKmin, ∞)Z. Consequently,
Proposition 6.92(i) implies that ˆY min is contained in Y on [N, ∞)Z for every
N ∈[ ˆKmin, ∞)Z. Finally, from Proposition 6.111 it then follows that Y is
a recessive solution of (SDS) at ∞.
(ii) Let Y be a dominant solution of (SDS) at ∞with respect to an interval
[N, ∞)Z. By Proposition 6.125, we may assume that N ∈[ ˆKmin, ∞)Z. From
Theorem 6.90 we know that there exists a conjoined basis Y ∗of (SDS) with
constant kernel on [N, ∞)Z and no forward focal points in (N, ∞) such that Y
contains Y ∗on [N, ∞)Z and rank X∗
k = n −d[N, ∞)Z = n −d∞on [N, ∞)Z.
In turn, Proposition 6.127 and Remark 6.123 then imply that Y ∗is a minimal
dominant solution of (SDS) at ∞with respect to [N, ∞)Z. Conversely, let Y be
a conjoined basis of (SDS) with constant kernel on [N, ∞)Z ⊆[ ˆKmin, ∞)Z and
no forward focal points in (N, ∞) such that Y contains some minimal dominant
solution of (SDS) on [N, ∞)Z. Then d[N, ∞)Z = d∞by Deﬁnition 6.122, and
Y is also a dominant solution of (SDS) at ∞, by Proposition 6.127.
⊓⊔
Remark 6.132 From Theorem 6.131(i), it follows that every recessive solution ˆY
of (SDS) at ∞is a recessive solution at ∞with respect to the interval [N, ∞)Z
for every N ∈[ ˆKmin, ∞)Z. In addition, the orthogonal projector P ˆS∞in (6.238)
associated with ˆY through the matrix ˆSk in (6.236) is the same for all initial indices
N ∈[ ˆKmin, ∞)Z, since in this case d[N, ∞)Z = d∞for every N ∈[ ˆKmin, ∞)Z,
by Remark 6.130. Similarly, if Y is a dominant solution at ∞with respect to an
interval [N, ∞)Z ⊆[ ˆKmin, ∞)Z, then the corresponding orthogonal projector PS∞
in (6.238) is the same for all initial indices L ∈[N, ∞)Z.

6.3
Symplectic Systems Without Controllability
523
In Theorem 6.118 we showed that recessive solutions of (SDS) at ∞can be
constructed in higher dimensions from the recessive solutions of the corresponding
symplectic systems in lower dimensions by a block diagonal procedure. We note
that exactly the same statement holds also for dominant solutions of (SDS) at ∞.
Theorem 6.133 Assume that the systems (SDS) and (SDS) in Theorem 6.118 are
nonoscillatory at ∞. Let Y and Y be dominant solutions of (SDS) and (SDS) at ∞
with rank equal to r and r, respectively. Then the sequence Y ∗
k deﬁned by
X∗
k :=
Xk 0
0 Xk

,
U∗
k :=
Uk 0
0 Uk

,
k ∈[0, ∞)Z,
(6.415)
is a dominant solution of system (SDS∗) at ∞with rank equal to r∗:= r + r.
Moreover, the dominant solution Y ∗at ∞constructed in (6.415) is minimal
(maximal) if and only if the dominant solutions Y and Y at ∞are minimal
(maximal).
Proof The statement follows by the same arguments as in the proof of Theo-
rem 6.118 by noting that the maximal order of abnormality of the augmented
system (SDS∗) is d∗
∞= d∞+ d∞, so that the matrix T ∗in (6.237) associated
with Y ∗in (6.415) satisﬁes (using that n∗= n + n)
rankT ∗= rank diag{T, T } = rankT + rankT
= n −d∞+ n −d∞= n∗−d∗
∞.
The result then follows from Deﬁnition 6.122 applied system (SDS∗).
⊓⊔
6.3.8
Genus Conjoined Bases
In this section we introduce the concept of a genus of conjoined bases of (SDS) and
study its properties. As our main results, we prove the existence (Theorem 6.138)
and classiﬁcation (Theorems 6.139 and 6.141) of recessive and dominant solutions
of (SDS) in every such a genus.
Deﬁnition 6.134 (Genus of Conjoined Bases) Let Y (1) and Y (2) be two conjoined
bases of (SDS). We say that Y (1) and Y (2) have the same genus (or they belong
to the same genus) if there exists an index N ∈[0, ∞)Z such that the equality
Im X(1)
k
= Im X(2)
k
holds for all k ∈[N, ∞)Z.
The relation “having (or belonging to) the same genus” is an equivalence relation
on the set of all conjoined bases of (SDS). Therefore, there exists a partition of this
set into disjoint classes of conjoined bases of (SDS), which belong to the same
genus. This allows to interpret each such an equivalence class G as a genus itself. In

524
6
Miscellaneous Topics on Symplectic Systems
particular, when system (SDS) is nonoscillatory, we have the following property of
conjoined bases of (SDS) in one genus G.
Proposition 6.135 Assume that system (SDS) is nonoscillatory at ∞. Let Y (1) and
Y (2) be conjoined bases of (SDS). Then the following are equivalent.
(i) The conjoined bases Y (1) and Y (2) belong to the same genus G.
(ii) The equality Im X(1)
k
= Im X(2)
k
holds on some subinterval [N, ∞)Z, where
Y (1) and Y (2) have constant kernel.
(iii) The equality Im X(1)
k
= Im X(2)
k
holds on every subinterval [N, ∞)Z, where
Y (1) and Y (2) have constant kernel.
Proof The statement follows from Deﬁnition 6.134 and Theorem 6.77.
⊓⊔
Remark 6.136 For a nonoscillatory system (SDS) at ∞, there is only one genus of
conjoined bases (denoted by Gmin) containing all conjoined bases Y of (SDS) with
the minimal eventual rank of Xk in (6.279), i.e., with rank Xk = n −d∞for large k.
This is a direct consequence of the fact that any two conjoined bases Y (1) and Y (2)
of (SDS) with the eventual rank of X(1)
k
and X(2)
k
equal to n−d∞necessarily satisfy
Im X(1)
k
= Im X(2)
k
for large k. Indeed, if Y (1) and Y (2) have constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞), then the equality d[N, ∞)Z = d∞
holds by Remark 6.96. Therefore, Y (1) and Y (2) are minimal conjoined bases on
[N, ∞)Z, and by Remark 6.98 they satisfy Im X(1)
N
= (0[N, ∞)Z)⊥= Im X(2)
N .
In turn, Theorem 6.77 implies that Im X(1)
k
= Im X(2)
k
on [N, ∞)Z. In particular,
all minimal recessive and dominant solutions of (SDS) at ∞belong to the minimal
genus Gmin.
Remark 6.137 Similarly to Remark 6.136, there is only one genus of conjoined
bases (denoted by Gmax) containing all conjoined bases Y of (SDS) satisfying
Im Xk = Rn for large k, i.e., with Xk eventually invertible. All maximal recessive
and dominant solutions of (SDS) at ∞then belong to the maximal genus Gmax.
In the following result, we show that there exists both recessive and dominant
solutions of (SDS) at ∞in every genus G.
Theorem 6.138 Assume that system (SDS) is nonoscillatory at ∞. Let G be a genus
of conjoined bases of (SDS). Then there exists a recessive solution and a dominant
solution of (SDS) at ∞in the genus G.
Proof First we focus on the case of the recessive solutions at ∞. Let ˆY min be the
minimal recessive solution of (SDS) at ∞with ˆKmin deﬁned in Remark 6.130.
Furthermore, let Y be a conjoined basis of (SDS), which belongs to the genus
G. Then there exists an index N ∈[ ˆKmin, ∞)Z such that Y has constant kernel
on [N, ∞)Z and no forward focal points in (N, ∞). By Remark 6.130 we know
that the equality d[N, ∞)Z = d∞holds and at the same time ˆY min is a minimal
recessive solution with respect to [N, ∞)Z. Moreover, by Theorem 6.90 there
exists a conjoined basis Y ∗of (SDS) with constant kernel on [N, ∞)Z and no
forward focal points in (N, ∞) such that rankX∗
k = n −d∞on [N, ∞)Z and such

6.3
Symplectic Systems Without Controllability
525
that Y contains Y ∗on [N, ∞)Z. Therefore, ˆY min and Y ∗are minimal conjoined
bases of (SDS) on [N, ∞)Z, so that Im ˆXmin
N
= Im X∗
N by Remark 6.98. Denote
by ˆR min
k
, R∗
k, Rk the orthogonal projectors in (6.233) deﬁned by ˆXmin
k
, X∗
k, Xk,
respectively. Then ˆR min
N
= R∗
N and Im ˆR min
N
= Im R∗
N ⊆Im RN, by Remark 6.83.
From Theorem 6.87 we know that there exists a conjoined basis ˆY of (SDS) with
constant kernel on [N, ∞)Z and no forward focal points in (N, ∞) such that ˆY
contains ˆY min on [N, ∞)Z and the equality Im ˆXN = Im RN holds. Consequently,
by Proposition 6.86 the conjoined basis ˆY is a recessive solution of (SDS) at ∞
with respect to [N, ∞)Z. And since Im ˆXN = Im RN = Im XN, we have that
Im ˆXk = Im Xk on [N, ∞)Z by Theorem 6.77. This shows that the recessive
solution ˆY belongs to the genus G, according to Deﬁnition 6.134. The proof for
the dominant solution at ∞in G can be carried out by exactly the same arguments,
by considering a minimal dominant solution Y min at ∞from Theorem 6.128 instead
of the minimal recessive solution ˆY min at ∞in the above proof.
⊓⊔
In the next result, we provide a complete classiﬁcation of all recessive solutions
of (SDS) at ∞in a given genus G.
Theorem 6.139 Assume that system (SDS) is nonoscillatory at ∞with
ˆKmin
deﬁned in Remark 6.130. Let ˆY be a recessive solution of (SDS) at ∞, which
belongs to a genus G, and let ˆP and P ˆS∞be the constant orthogonal projectors
deﬁned in (6.234), (6.238), and Remark 6.132 on [ ˆKmin, ∞)Z through the matrix
ˆXk. Then a solution Y of (SDS) is a recessive solution at ∞belonging to the genus
G if and only if for some (and hence for every) index N ∈[ ˆKmin, ∞)Z there exist
matrices ˆM, ˆN ∈Rn×n such that
XN = ˆXN ˆM,
UN = ˆUN ˆM + ˆX†T
N ˆN,
(6.416)
ˆM is nonsingular,
ˆMT ˆN = ˆNT ˆM,
Im ˆN ⊆Im ˆP ,
(6.417)
P ˆS∞ˆN ˆM−1P ˆS∞= 0.
(6.418)
Proof Let Y be a recessive solution of (SDS), which belongs to the genus G. From
Theorem 6.131 we know that ˆY and Y have constant kernel on [ ˆKmin, ∞)Z and
no forward focal points in ( ˆKmin, ∞), and consequently, according to Proposi-
tion 6.135 they satisfy Im ˆXk = Im Xk on [ ˆKmin, ∞)Z. Therefore, by Theorem 6.69
and Remark 6.70 (with Y (1) := ˆY and Y (2) := Y) for every N ∈[ ˆKmin, ∞)Z, there
exist ˆM, ˆN ∈Rn×n such that (6.416) and (6.417) hold. We will prove (6.418).
Fix N ∈[ ˆKmin, ∞)Z. Denote by ˆY min and Y ∗the minimal recessive solutions
of (SDS) at ∞from Theorem 6.131, which are contained in ˆY and Y on [N, ∞)Z,
respectively. By Theorem 6.113 (or Theorem 6.69), we know that Y ∗
k = ˆY min
k
ˆMmin
on [0, ∞)Z for some nonsingular matrix
ˆMmin. This means that the Wronskian
ˆW min := w( ˆY min, Y ∗) = 0. On the other hand, the conjoined bases ˆY min and Y ∗
are minimal on [N, ∞)Z, and therefore, the formulas in (6.346) hold (with Y ∗(1) :=
ˆY min, Y ∗(2) := Y ∗, M∗(1) :=
ˆMmin, and N∗(1) := ˆW min = 0). Consequently, by

526
6
Miscellaneous Topics on Symplectic Systems
Proposition 6.99 (with PS(1)∞:= P ˆS∞), we obtain that P ˆS∞ˆN ˆM−1P ˆS∞= 0 is
satisﬁed.
Conversely, ﬁx N ∈[ ˆKmin, ∞)Z, and suppose that a solution Y of (SDS)
satisﬁes (6.416)–(6.418). From Remark 6.130 we have d[N, ∞)Z = d∞. The third
condition in (6.417) yields that ˆP ˆN = ˆN, so that XT
NUN = ˆMT ˆXT
N ˆUN ˆM + ˆMT ˆN
by (6.416). Therefore, XT
NUN is symmetric. If YNd = 0 for some d ∈Rn, then
ˆXN ˆMd = 0 and ˆUN ˆMd = −ˆX†T
N ˆNd again by (6.416). Multiplying the last equality
by ˆXT
N, we obtain that 0 = ˆUT
N ˆXN ˆMd = −ˆP ˆNd = −ˆNd. Therefore, ˆUN ˆMd = 0
as well. But since ˆY is a conjoined basis (rank ˆYN = n), it follows that ˆMd = 0.
The invertibility of ˆM then implies that d = 0, i.e., Y is a conjoined basis of (SDS).
Let ˆSk be the S-matrix in (6.236) corresponding to ˆY on [N, ∞)Z, and let ¯Y be
a conjoined basis of (SDS) from Proposition 6.67 (with Y := ˆY). With the aid of the
identities ˆP ˆN = ˆN, ˆX†
N ¯XN = 0, and ¯XN ˆP = ˆXN ˆSN = 0 from Proposition 6.67
(with Y := ˆY at the index k = N), we can rewrite (6.416) as YN = ˆYN ˆM + ¯YN ˆN.
Hence, Yk = ˆYk ˆM + ¯Yk ˆN for all k ∈[N, ∞)Z, by the uniqueness of solutions
of (SDS). In particular, by Proposition 6.67 again and ˆXk ˆP = ˆXk on [N, ∞)Z, we
get
Xk = ˆXk ˆM + ˆXk ˆSk ˆN = ˆXk( ˆP ˆM + ˆSk ˆN),
k ∈[N, ∞)Z.
(6.419)
We will show that Y has constant kernel on [N, ∞)Z. Namely, we will prove that
Ker Xk = Ker ˆP ˆM on [N, ∞)Z. First we note that the symmetry of ˆMT ˆN implies
the symmetry of ˆN ˆM−1 and that ˆN ˆM−1 ˆP = ˆN ˆM−1 holds. Hence, by (6.419) we
obtain
Xk = ˆXk ( ˆP ˆM + ˆSk ˆN ˆM−1 ˆM) = ˆXk (I + ˆSk ˆN ˆM−1) ˆP ˆM,
k ∈[N, ∞)Z.
This implies that Ker ˆP ˆM ⊆Ker Xk for all k ∈[N, ∞)Z. Fix now k ∈[N, ∞)Z,
v ∈Ker Xk, and put w := ˆP ˆMv. Then ˆXk (w + ˆSk ˆN ˆM−1w) = 0. Multiplying the
latter equality by ˆX†
k from the left and using the identities ˆP ˆSk = ˆSk and w = ˆP w,
we get w = −ˆSk ˆN ˆM−1w. Therefore, w ∈Im ˆSk ⊆Im P ˆS∞by (6.239), and thus,
w = −ˆSkP ˆS∞ˆN ˆM−1P ˆS∞w = 0 by (6.418). This shows that v ∈Ker ˆP ˆM, i.e.,
Ker Xk ⊆Ker ˆP ˆM. Therefore, Ker Xk = Ker ˆP ˆM on [N, ∞)Z, i.e., the conjoined
basis Y has constant kernel on [N, ∞)Z.
Note that the ﬁrst equation in (6.416) and the invertibility of
ˆM yield that
Im XN = Im ˆXN. Then it follows from Theorem 6.77 that Im Xk = Im ˆXk for
all k ∈[N, ∞)Z. In particular, Y belongs to the genus G.
We shall prove that Y has no forward focal points in (N, ∞). Since the conjoined
basis Y has constant kernel on [N, ∞)Z, we denote by P the associated projector
in (6.234). The fact that Im Xk = Im ˆXk on [N, ∞)Z implies that Y and ˆY are
mutually representable on [N, ∞)Z in the spirit of Theorem 6.69 and the subsequent
remarks. This means that by this theorem (with the notation Y (1) := ˆY, Y (2) := Y
and S(1)
k
:= ˆSk, S(2)
k
:= Sk, P (1) := ˆP, P (2) := P, and M(1) := ˆM, N(1) := ˆN),

6.3
Symplectic Systems Without Controllability
527
we have
Im ( ˆP ˆM + ˆSk ˆN) = Im ˆP,
Im ˆMSk = Im ˆSk,
( ˆP ˆM + ˆSk ˆN)† = P ˆM−1 −Sk ˆNT ,

k ∈[N, ∞)Z.
(6.420)
In turn, equalities (6.419), (6.420), and (6.236) yield on [N, ∞)Z the formulas
X†
k = ( ˆP ˆM + ˆSk ˆN)† ˆX†
k,
(6.421)
and
ˆP ˆM + ˆSk ˆN = ˆP ˆM + ˆSk+1 ˆN −( ˆSk) ˆN
= ( ˆP ˆM + ˆSk+1 ˆN) −ˆX†
k+1 Bk ˆX† T
k
ˆN.
(6.422)
Note that by the ﬁrst identity in (6.420), we have ( ˆP ˆM + ˆSk ˆN) ( ˆP ˆM + ˆSk ˆN)† = ˆP
on [N, ∞)Z. Therefore, equations (6.419)–(6.422) and the symmetry of ˆXk ˆX†
k+1 Bk
on [N, ∞)Z imply that
XkX†
k+1Bk
(6.419), (6.421)
=
ˆXk( ˆP ˆM + ˆSk ˆN) ( ˆP ˆM + ˆSk+1 ˆN)† ˆX†
k+1Bk
(6.422)
=
ˆXk[( ˆP ˆM + ˆSk+1 ˆN) −ˆX†
k+1Bk ˆX† T
k
ˆN] ( ˆP ˆM + ˆSk+1 ˆN)† ˆX†
k+1Bk
= ˆXk ˆP ˆX†
k+1Bk −( ˆXk ˆX†
k+1Bk) ˆX† T
k
ˆN( ˆP ˆM + ˆSk+1 ˆN)† ˆX†
k+1Bk
= ˆXk ˆX†
k+1Bk −BT
k ˆX† T
k+1 ˆP ˆN( ˆP ˆM + ˆSk+1 ˆN)† ˆX†
k+1Bk
(6.420)
=
ˆXk ˆX†
k+1Bk −Lk,
Lk := BT
k ˆX† T
k+1 ˆN(P ˆM−1 −Sk+1 ˆNT ) ˆX†
k+1Bk.
(6.423)
Our claim is to show that the matrix Lk in (6.423) is zero. Let ˆY min be the minimal
recessive solution of (SDS) from Theorem 6.131(i), which is contained in ˆY on
[N, ∞)Z, and let ˆR min
k
be the corresponding orthogonal projector in (6.234). Then
ˆY min is a minimal conjoined basis of (SDS) on [N, ∞)Z, so that P ˆS∞( ˆXmin
k
)† =
( ˆXmin
k
)† on [N, ∞)Z, by Proposition 6.86. Moreover, the equality ( ˆXmin
k
)† =
ˆX†
k ˆR min
k
holds for every k ∈[N, ∞)Z by (6.298) (with Y ∗:= ˆY min). Then by
ˆNP = ˆN, the second identity in (6.420), and Theorem 6.66(ii), we obtain
Lk = BT
k ˆR min
k
ˆX† T
k+1 ˆN(P ˆM−1 −Sk+1 ˆNT ) ˆX†
k+1 ˆR min
k
Bk
= BT
k ( ˆXmin
k+1)† T ˆN(P ˆM−1 −ˆM−1 ˆMSk+1 ˆNT ) ( ˆXmin
k+1)†Bk
(6.420)
=
BT
k ( ˆXmin
k+1)† T P ˆS∞ˆN

P ˆM−1 −ˆM−1P ˆS∞ˆMSk+1 ˆNT 
P ˆS∞( ˆXmin
k+1)†Bk

528
6
Miscellaneous Topics on Symplectic Systems
= BT
k ( ˆXmin
k+1)† T P ˆS∞( ˆN ˆM−1P ˆS∞−ˆN ˆM−1P ˆS∞ˆMSk+1 ˆNT P ˆS∞) ( ˆXmin
k+1)†Bk
= BT
k ( ˆXmin
k+1)† T P ˆS∞ˆN ˆM−1P ˆS∞(I −ˆMSk+1 ˆNT P ˆS∞) ( ˆXmin
k+1)†Bk
(6.418)
=
0.
Hence, from equality (6.423) we get XkX†
k+1Bk = ˆXk ˆX†
k+1Bk on [N, ∞)Z. Since
ˆY has no forward focal points in (N, ∞), i.e., ˆXk ˆX†
k+1Bk ≥0 on [N, ∞)Z, we
conclude that Y has no forward focal points in (N, ∞) as well.
In the ﬁnal step of the proof, we will show that Y is a recessive solution of (SDS)
at ∞. By Theorem 6.90, there exists a minimal conjoined basis Y ∗of (SDS) on
[N, ∞)Z, which is contained in Y on [N, ∞)Z. In particular, the equality rankX∗
k =
n −d∞holds on [N, ∞)Z. Then according to Remark 6.98, there exist matrices
ˆMmin, ˆNmin ∈Rn×n such that (6.346) is satisﬁed (with Y ∗(1) := ˆY min, Y ∗(2) :=
Y ∗, M∗(1) :=
ˆMmin, and N∗(1) :=
ˆNmin). In particular, ˆMmin is invertible and
ˆNmin = w( ˆY min, Y ∗) by (6.244). Consequently, by Proposition 6.99 (with Y (1) :=
ˆY, Y (2) := Y, M(1) :=
ˆM, N(1) :=
ˆN, and PS(1)∞:= P ˆS∞), we derive that
ˆNmin( ˆMmin)−1 = P ˆS∞ˆN ˆM−1P ˆS∞. In view of (6.418), we get ˆNmin( ˆMmin)−1 =
0, and hence ˆNmin = 0. This implies by (6.346) that Y ∗
N = ˆY min
N
ˆMmin, so that
Y ∗
k = ˆY min
k
ˆMmin on [0, ∞)Z by the uniqueness of solutions of (SDS). According to
Theorem 6.113, Y ∗is a minimal recessive solution of (SDS) at ∞with respect to
the interval [N, ∞)Z. Thus, we proved that Y ∗, being a minimal recessive solution
of (SDS) at ∞, is contained in Y on [N, ∞)Z. Therefore, Y is also a recessive
solution of (SDS) at ∞by Proposition 6.112. The proof is complete.
⊓⊔
As a special case of Theorem 6.139, we obtain a classiﬁcation of the recessive
solutions of (SDS) at ∞in the maximal genus Gmax.
Corollary 6.140 Assume that system (SDS) is nonoscillatory at ∞with
ˆKmin
deﬁned in Remark 6.130. Let ˆY be a maximal recessive solution of (SDS) at ∞.
Moreover, let P ˆS∞be the orthogonal projector deﬁned through the invertible matrix
ˆXk on [ ˆKmin, ∞)Z in (6.238) and Remark 6.132. Then a solution Y of (SDS) is
a maximal recessive solution at ∞if and only if for some (and hence for every)
index N ∈[ ˆKmin, ∞)Z, there exist matrices ˆM, ˆN ∈Rn×n such that
XN = ˆXN ˆM,
UN = ˆUN ˆM + ˆXT −1
k
ˆN,
ˆM is nonsingular,
ˆMT ˆN = ˆNT ˆM,
P ˆS∞ˆN ˆM−1P ˆS∞= 0.
Proof The result follows from Theorem 6.139 with ˆP = I.
⊓⊔
In remaining part of this section, we will derive several classiﬁcations of the
dominant solutions of (SDS) at ∞in a given genus G in terms of recessive solutions
of (SDS) at ∞from this genus.
Theorem 6.141 Assume that system (SDS) is nonoscillatory at ∞. Let G be
a genus of conjoined bases of (SDS). Let ˆY be a recessive solution of (SDS) at

6.3
Symplectic Systems Without Controllability
529
∞belonging to G, and let Y be a conjoined basis from G. Denote by P ˆS∞and
PS∞their associated orthogonal projectors in (6.238) and Remark 6.132. Then Y
is a dominant solution of (SDS) at ∞if and only if
rankP ˆS∞w( ˆY, Y) PS∞= n −d∞.
(6.424)
Proof Let ˆY be a recessive solution of (SDS) at ∞belonging to G, and let Y be
a conjoined basis from G. We denote ˆN := w( ˆY, Y). Then there exists an index
N ∈[ ˆKmin, ∞)Z such that ˆY and Y have constant kernel on [N, ∞)Z and no
forward focal points in (N, ∞). By Remark 6.130 we have d[N, ∞)Z = d∞. Since
ˆY and Y belong to the same genus G, we may assume without loss of generality that
Im ˆXk = Im Xk on [N, ∞)Z. From Theorem 6.69 and Remark 6.70, it then follows
that ˆY and Y are mutually representable on [N, ∞)Z and the matrix ˆN satisﬁes
ˆP ˆN =
ˆN =
ˆNP, where ˆP and P are the corresponding orthogonal projectors
in (6.234). Let ˆY ∗and Y ∗be minimal conjoined bases of (SDS) on [N, ∞)Z, which
are contained in ˆY and Y on [N, ∞)Z, respectively, and let ˆN∗:= w( ˆY ∗, Y ∗). In
particular, ˆY ∗is a minimal recessive solution of (SDS) by Proposition 6.112, i.e.,
the associated matrix ˆT ∗in (6.237) satisﬁes ˆT ∗= 0. We apply the representations
of ˆY, Y and of ˆY ∗, Y ∗in Theorem 6.69 and Remark 6.98, and in formula (6.349) in
Proposition 6.99 (with Y (1) := ˆY, Y (2) := Y, Y ∗(1) := ˆY ∗, Y ∗(2) := Y ∗, P (1) := ˆP ,
P (2) := P, PS(1)∞:= P ˆS∞, PS(2)∞:= PS∞, and N(1) := ˆN). Then there exist
invertible matrices ˆM and ˆM∗such that
ˆP ˆMPS∞= P ˆS∞ˆM∗,
P ˆM−1P ˆS∞= PS∞( ˆM∗)−1,
ˆN∗( ˆM∗)−1 = P ˆS∞ˆN ˆM−1P ˆS∞.

(6.425)
By using (6.425) and the equality ˆNP = ˆN, we then obtain
ˆN∗( ˆM∗)−1 = P ˆS∞ˆNP ˆM−1P ˆS∞= P ˆS∞ˆNPS∞( ˆM∗)−1.
(6.426)
Let Y be a dominant solution of (SDS) at ∞. Then also Y ∗is a dominant solution
at ∞, by Proposition 6.127. From (6.389) we know that the matrix T ∗in (6.237)
associated with Y ∗satisﬁes rank T ∗= rank[ ˆN∗( ˆM∗)−1 + ˆT ∗] = rank ˆN∗( ˆM∗)−1.
Since rankT ∗= n −d∞by Deﬁnition 6.122, we get from (6.426) that
rankP ˆS∞ˆNPS∞= rankP ˆS∞ˆNPS∞( ˆM∗)−1 = rank ˆN∗( ˆM∗)−1 = n −d∞,
i.e., formula (6.424) holds. Conversely, if (6.424) is satisﬁed, then from (6.426) we
obtain that rank ˆN∗( ˆM∗)−1 = n −d∞. Therefore, rank T ∗= n −d∞by (6.389),
and hence Y ∗is a dominant solution of (SDS) at ∞by Deﬁnition 6.122. Finally,
Proposition 6.127 implies that Y is a dominant solution at ∞as well.
⊓⊔
When we specialize Theorem 6.141 to the minimal genus Gmin, we obtain
a classiﬁcation of all minimal dominant solutions of (SDS) at ∞.

530
6
Miscellaneous Topics on Symplectic Systems
Corollary 6.142 Assume that system (SDS) is nonoscillatory at ∞. Let ˆY min be
the minimal recessive solution of (SDS) at ∞, and let Y be a minimal conjoined
basis of (SDS). Then Y is a minimal dominant solution of (SDS) at ∞if and only
if rank w( ˆY min, Y) = n −d∞.
Proof By Theorem 6.141 and its proof with ˆP := P ˆS∞and P := PS∞, we have
that P ˆS∞ˆN =
ˆN and ˆNPS∞=
ˆN, where ˆN := w( ˆY min, Y). Therefore, the
equality P ˆS∞ˆNPS∞= ˆN holds, and the statement follows from (6.424).
⊓⊔
In the following result, we provide important examples of minimal dominant
solutions of (SDS) at ∞. We recall that the principal solution Y [N] of (SDS) at the
index N ∈[0, ∞)Z is deﬁned as the solution of (SDS) given by the initial conditions
X[N]
N
= 0 and U[N]
N
= I.
Theorem 6.143 Assume that system (SDS) is nonoscillatory at ∞with
ˆKmin
deﬁned in Remark 6.130. Then for every N ∈[ ˆKmin, ∞)Z, the principal solution
Y [N] at the index N is a minimal dominant solution of (SDS) at ∞.
Proof Let ˆY min be the minimal recessive solution of (SDS) at ∞from Theo-
rem 6.113, and let N ∈[ ˆKmin, ∞)Z be ﬁxed. Then ˆY min has constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞), by Remark 6.130. In particular,
the condition d[N, ∞)Z = d∞holds, by (6.414). In order to simplify the notation,
we put ˆY :=
ˆY min and Y := Y [N]. Let ˆP , ˆRk, ˆSk, and P ˆS∞be the matrices
in (6.234), (6.236), (6.238), and Remark 6.132 associated with ˆY. Then ˆP = P ˆS∞
and by Theorem 6.75 the conjoined basis Y satisﬁes
Xk = ˆXk ˆSk ˆXT
N,
rank ˆSk = rank Xk,
k ∈[N, ∞)Z.
(6.427)
Now ﬁx an index L ∈[N, ∞)Z such that Y has constant kernel on [L, ∞)Z and no
forward focal points in (L, ∞). In particular, the rank of Xk is constant on [L, ∞)Z.
The second identity in (6.427) then implies that also the rank of ˆSk is constant
on [L, ∞)Z, which by Theorem 6.66(iv) and property (6.239) yields the equalities
rank ˆSk = n −d[N, ∞)Z = n −d∞on [L, ∞)Z. Therefore, rank Xk = n −d∞on
[L, ∞)Z by (6.427), i.e., Y is a minimal conjoined basis of (SDS) on [L, ∞)Z. We
will show that
X†
k = ˆX†T
N ˆS†
k ˆX†
k,
k ∈[L, ∞)Z.
(6.428)
Setting A := ˆXk ˆSk ˆXT
N and B := ˆX†T
N ˆS†
k ˆX†
k for a ﬁxed index k ∈[L, ∞)Z, we
verify that the matrices A and B satisfy the four deﬁning equations for the Moore-
Penrose pseudoinverse in (1.157). Namely, the identities ˆX†
k ˆXk = ˆP , ˆXk ˆX†
k = ˆRk,
and ˆS†
k ˆSk = P ˆS∞imply that BA = ˆRN and AB = ˆRk are symmetric. Moreover,
BAB = (BA) B = ˆRN ˆX†T
N ˆS†
k ˆX†
k = ˆX†T
N ˆS†
k ˆX†
k = B,
ABA = (AB) A = ˆRk ˆXk ˆSk ˆXT
N = ˆXk ˆSk ˆXT
N = A.

6.3
Symplectic Systems Without Controllability
531
Therefore, we get A† = B, showing equality (6.428). Let S(L)
k
be the S-matrix
in (6.236), which corresponds to Y on [L, ∞)Z, that is,
S(L)
k
:=
k−1

j=L
X†
j+1 Bj X† T
j
,
k ∈[L, ∞)Z.
(6.429)
By inserting (6.428) into (6.429) and using the identity  ˆSk =
ˆX†
k+1Bk ˆX† T
k
together with ˆSk ˆS†
k = ˆS†
k ˆSk = P ˆS∞on [L, ∞)Z, we obtain for every k ∈[L, ∞)Z
that
S(L)
k
(6.428)
=
k−1

j=L
ˆX†T
N ˆS†
j+1 ˆX†
j+1Bj ˆX† T
j
ˆS†
j ˆX†
N = ˆX†T
N
 k−1

j=L
ˆS†
j+1 ( ˆSj) ˆS†
j

ˆX†
N
= ˆX†T
N
 k−1

j=L
( ˆS†
j+1 ˆSj+1 ˆS†
j −ˆS†
j+1 ˆSj ˆS†
j )

ˆX†
N
= ˆX†T
N
 k−1

j=L
(P ˆS∞ˆS†
j −ˆS†
j+1P ˆS∞)

ˆX†
N
= −ˆX†T
N
 k−1

j=L
 ˆS†
j

ˆX†
N = ˆX†T
N
 ˆS†
L −ˆS†
k
 ˆX†
N.
(6.430)
Since ˆS†
k →0 as k →∞, equality (6.430) then implies that the limit of S(L)
k
as k →∞exists and it is equal to ˆX†T
N ˆS†
L ˆX†
N. Therefore, by Theorem 6.124 we
conclude that Y is a (minimal) dominant solution of (SDS) at ∞. We note that
by (6.410) the matrix T (L) in (6.237) associated with Y on [L, ∞)Z satisﬁes the
equality T (L) =  ˆX†T
N ˆS†
L ˆX†
N
† = ˆXN ˆSL ˆXT
N by (6.428) and (6.427) at k = N. This
additional information is however not needed in this proof.
⊓⊔
Remark 6.144 The result of Theorem 6.143 shows that a minimal dominant
solution of (SDS) at ∞is not uniquely determined (up to a constant invertible
multiple), in contrast with the minimal recessive solution of (SDS) at ∞. Moreover,
it implies that one cannot expect to have a unifying classiﬁcation of all minimal
dominant solutions of (SDS) at ∞in the spirit of Theorem 6.139. In addition, the
nonuniqueness of minimal dominant solutions of (SDS) at ∞(in the minimal genus
Gmin) yields through Theorem 6.131(ii) the same property for all dominant solutions
at ∞in every other genus G.
In the case of an eventually controllable system (SDS), we obtain from Corol-
lary 6.142 and Theorem 6.143 an interesting characterization of the dominant
solutions of (SDS) at ∞.

532
6
Miscellaneous Topics on Symplectic Systems
Corollary 6.145 Assume that system (SDS) is nonoscillatory at ∞and eventually
controllable. Let ˆY be the recessive solution of (SDS) at ∞. Then a conjoined basis
Y of (SDS) is a dominant solution at ∞if and only if the Wronskian w( ˆY , Y) is
invertible. In particular, for every index N ∈[ ˆKmin, ∞)Z the principal solution
Y [N] of (SDS) at the index N is a dominant solution at ∞. More generally, for
any index N ∈[0, ∞)Z, the principal solution Y [N] at the index N is a dominant
solution at ∞if and only if the matrix ˆXN is invertible.
Proof If (SDS) is eventually controllable, then d∞= 0, and for every conjoined
basis Y of (SDS), the matrix Xk is eventually invertible, by (6.279). Therefore,
according to Remarks 6.136 and 6.137, there is only one genus G = Gmin = Gmax
of conjoined bases of (SDS). Let ˆY be the recessive solution of (SDS) at ∞and Y
be a conjoined basis of (SDS). The ﬁrst and second statements follow directly from
Corollary 6.142 and Theorem 6.143, respectively. Finally, for Y := Y [N] we have
w( ˆY, Y) = ˆXT
N, so that by the ﬁrst part Y [N] is a dominant solution of (SDS) at ∞
if and only if the matrix ˆXN is invertible.
⊓⊔
6.3.9
Limit Properties of Recessive and Dominant Solutions
In this subsection we derive characterizations of recessive solutions of (SDS)
at ∞in terms of a limit involving dominant solutions of (SDS) at ∞. This is
a generalization of the limit property (2.134) in Theorem 2.67 to the abnormal case,
in particular to an arbitrary genus G; compare with Remark 6.117.
Theorem 6.146 Assume that system (SDS) is nonoscillatory at ∞with
ˆKmin
deﬁned in Remark 6.130. Let ˆY and Y be two conjoined bases of (SDS) from a given
genus G, and let N ∈[ ˆKmin, ∞)Z be such that ˆY and Y have constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞). Denote by ˆP , P ˆS∞, and PS∞their
associated projectors in (6.234), (6.238), and Remark 6.132. Then ˆY is a recessive
solution of (SDS) at ∞and rankP ˆS∞w( ˆY, Y) PS∞= n −d∞if and only if
lim
k→∞X†
k ˆXk = V
with
Im V T = Im ( ˆP −P ˆS∞).
(6.431)
In this case Y is a dominant solution of (SDS) at ∞.
Proof Let ˆSk and Sk be the S-matrices in (6.236) which are associated with ˆY and Y
on [N, ∞)Z. Denote ˆN := w( ˆY, Y). By (6.259) and (6.258) in Remark 6.70 (with
Y (1) := ˆY and Y (2) := Y) and by w(Y, ˆY ) = −[w( ˆY, Y)]T , we have
Xk = ˆXk ( ˆP ˆM+ ˆSk ˆN),
ˆXk = Xk (P ˆM−1−Sk ˆNT ),
k ∈[N, ∞)Z,
(6.432)

6.3
Symplectic Systems Without Controllability
533
where the matrix ˆM is invertible. By using the identities X†
kXk = P and PSk = Sk
on [N, ∞)Z, it follows from (6.432) that
X†
k ˆXk = P ˆM−1 −Sk ˆNT ,
k ∈[N, ∞)Z.
(6.433)
Let ˆT ∗and T ∗be the matrices in (6.237) deﬁned through the minimal conjoined
bases ˆY ∗and Y ∗, which are contained in ˆY and Y on [N, ∞)Z, respectively, as in
the proof of Theorem 6.141. It follows by (6.348) that
T ∗= ( ˆM∗)T ˆT ∗ˆM∗+ ( ˆM∗)T ˆN∗,
(6.434)
where ˆM∗is invertible and ˆN∗= w( ˆY ∗, Y ∗).
Suppose that ˆY is a recessive solution of (SDS) at ∞and (6.424) holds. Then
ˆT ∗= 0, and by (6.434), (6.426), and (6.424), we get
rank T ∗(6.434)
=
rank ˆN∗(6.426)
=
rankP ˆS∞ˆNPS∞
(6.424)
=
n −d∞.
(6.435)
Therefore, Y is a dominant solution of (SDS) at ∞by Deﬁnition 6.122. This means
that Im T ∗= Im PS∞since S∗
k = Sk on [N, ∞)Z, Im T ∗⊆Im PS∗∞= Im PS∞
by (6.239), and rank T ∗= n −d∞by (6.435). From (6.434) and ˆT ∗= 0, we know
that T ∗= ( ˆM∗)T ˆN∗= ( ˆN∗)T ˆM∗. Multiplying this equality by (T ∗)† from the left
and by ( ˆM∗)−1 from the right and using the identity (T ∗)† T ∗= PS∞yield
PS∞( ˆM∗)−1 = (T ∗)†( ˆN∗)T .
(6.436)
Furthermore, by (6.433) and Theorem 6.124, we obtain
lim
k→∞X†
k ˆXk = lim
k→∞(P ˆM−1 −Sk ˆNT ) = V := P ˆM−1 −(T ∗)† ˆNT .
(6.437)
We will show that Im V T = Im ( ˆP −P ˆS∞) = Im ˆP ∩Ker P ˆS∞. By using (6.437)
and the identities P ˆM−1 ˆP = P ˆM−1, ˆNT ˆP =
ˆNT , we get V ˆP = V , which
yields that Im V T
⊆Im ˆP . Moreover, the equality (T ∗)†PS∞= (T ∗)† and
formulas (6.425), (6.426), and (6.436) imply that
V P ˆS∞
(6.437)
=
P ˆM−1P ˆS∞−(T ∗)†PS∞ˆNT P ˆS∞
= PS∞( ˆM∗)−1−(T ∗)†( ˆN∗)T (6.436)
=
0.
Thus, Im V T ⊆Ker P ˆS∞. Hence, we proved that Im V T ⊆Im ˆP ∩Ker P ˆS∞.
Now we show the opposite inclusion Im ˆP ∩Ker P ˆS∞⊆Im V T , or equivalently
the inclusion Ker V ⊆Ker ˆP ⊕Im P ˆS∞. Let v ∈Ker V . Then v can be uniquely
decomposed as v = v1 + v2 with v1 ∈Ker ˆP and v2 ∈Im ˆP. The identity V ˆP = V

534
6
Miscellaneous Topics on Symplectic Systems
then implies that V v2 = V ˆP v = V v = 0, so that by the deﬁnition of V in (6.437)
we have [P ˆM−1 −(T ∗)† ˆNT ] v2 = V v2 = 0. Hence, P ˆM−1v2 = (T ∗)† ˆNT v2. The
vector w := P ˆM−1v2 then satisﬁes w ∈Im (T ∗)† = Im PS∞. By the equalities
ˆP ˆMP ˆM−1 = ˆP , ˆP v2 = v2, PS∞w = w, and the ﬁrst formula in (6.425), we get
v2 = ˆP ˆMP ˆM−1v2 = ˆP ˆMw = ˆP ˆMPS∞w = P ˆS∞ˆM∗w,
i.e.,
v2∈Im P ˆS∞.
This shows that v = v1 + v2 ∈Ker ˆP ⊕Im P ˆS∞, which completes the proof of this
direction.
Conversely, assume that (6.431) is satisﬁed. Denote by V0 := P ˆM−1 −V , where
V is given in (6.431). Then by (6.433) we get Sk ˆNT →V0 as k →∞. The equality
Sk = SkPS∞implies the inclusion Ker PS∞ˆNT ⊆Ker V0, and similarly Sk =
PS∞Sk implies that Im V0 ⊆Im PS∞. In particular, rank V0 ≤rank PS∞ˆNT .
Moreover, by using the identities P ˆM−1P ˆS∞= PS∞( ˆM∗)−1 and V P ˆS∞= 0,
we get V0 P ˆS∞= PS∞( ˆM∗)−1, which implies that Im PS∞⊆Im V0. Hence, we
proved the equality Im V0 = Im PS∞, so that rank V0 = rankPS∞= n −d∞. In
turn, the inequality n −d∞= rank V0 ≤rankPS∞ˆNT holds. On the other hand,
we always have rankPS∞ˆNT ≤rankPS∞= n −d∞. Thus, we conclude that
rankPS∞ˆNT = n −d∞. The deﬁnition of T ∗in (6.237) now yields
PS∞ˆNT = lim
k→∞S†
kSk ˆNT = lim
k→∞(S∗
k )†(Sk ˆNT ) = T ∗V0.
(6.438)
From (6.438) we then obtain the inequality
n −d∞= rankPS∞ˆNT = rank T ∗V0 ≤rankT ∗.
This yields by the third condition in (6.394) that rank T ∗= n −d∞. Therefore,
Y is a dominant solution of (SDS) at ∞, by Deﬁnition 6.122. Moreover, by
using (6.426), (6.438), the symmetry of ˆN∗( ˆM∗)−1, and the equalities V0 P ˆS∞=
PS∞( ˆM∗)−1 and T ∗PS∞= T ∗, we get
ˆN∗( ˆM∗)−1 (6.426)
=
( ˆM∗)T −1PS∞ˆNT P ˆS∞
(6.438)
=
( ˆM∗)T −1T ∗V0 P ˆS∞
= ( ˆM∗)T −1T ∗PS∞( ˆM∗)−1 = ( ˆM∗)T −1T ∗( ˆM∗)−1.
This implies that T ∗= ( ˆM∗)T ˆN∗. From (6.434) we then obtain the equality
( ˆM∗)T ˆT ∗ˆM∗= 0, i.e., ˆT ∗= 0, as the matrix
ˆM∗is invertible. Therefore, ˆY
is a recessive solution of (SDS) at ∞. Finally, Theorem 6.141 yields that (6.424)
holds, which completes the proof.
⊓⊔
Motivated by Theorem 2.67, we can now determine when is the limit V in (6.431)
equal to zero. It follows from condition (6.431) that this happens if and only if
ˆP = P ˆS∞, i.e., if and only if Theorem 6.146 is applied to the minimal genus Gmin.

6.3
Symplectic Systems Without Controllability
535
We note that if the system (SDS) is eventually controllable, then Corollary 6.147
below reduces to Theorem 2.67.
Corollary 6.147 Assume that system (SDS) is nonoscillatory at ∞. Let ˆY and Y be
two conjoined bases of (SDS) from the minimal genus Gmin. Then ˆY is a minimal
recessive solution of (SDS) at ∞and rankw( ˆY , Y) = n −d∞if and only if
lim
k→∞X†
k ˆXk = 0.
(6.439)
In this case Y is a minimal dominant solution of (SDS) at ∞.
Proof Let ˆY and Y be as in the corollary, and let K ∈[ ˆKmin, ∞)Z be such that ˆY and
Y have constant kernel on [N, ∞)Z and no forward focal points in (N, ∞). Then ˆY
and Y are minimal conjoined bases on [N, ∞)Z. Moreover, let ˆP , P, P ˆS∞, and PS∞
be the corresponding orthogonal projectors in (6.234), (6.238), and Remark 6.132.
Then ˆP = P ˆS∞, P = PS∞, and P ˆS∞ˆNPS∞= ˆN, where ˆN := w( ˆY , Y). The
statement now follows from Theorem 6.146 (with G := Gmin).
⊓⊔
Similarly, the limit in (6.431) involves an invertible Xk when Theorem 6.146 is
applied to the maximal genus Gmax.
Corollary 6.148 Assume that system (SDS) is nonoscillatory at ∞with
ˆKmin
deﬁned in Remark 6.130. Let ˆY and Y be two maximal conjoined bases of (SDS),
and let N ∈[ ˆKmin, ∞)Z be such that ˆY and Y have invertible ˆXk and Xk on
[N, ∞)Z and no forward focal points in (N, ∞). Let P ˆS∞and PS∞be their
associated projectors in (6.238) and Remark 6.132. Then ˆY is a maximal recessive
solution of (SDS) at ∞and rankP ˆS∞w( ˆY, Y) PS∞= n −d∞if and only if
lim
k→∞X−1
k
ˆXk = V
with
Im V T = Ker P ˆS∞.
In this case Y is a maximal dominant solution of (SDS) at ∞.
Proof The statement follows directly from Theorem 6.146 by using the fact that the
orthogonal projector ˆP in (6.234) associated with the maximal conjoined basis ˆY
satisﬁes ˆP = I.
⊓⊔
In the last result of this subsection, we present an extension of Theorem 6.146.
More precisely, we show that the limit in (6.431) always exists when Y is a dominant
solution of (SDS) at ∞. In this case we also obtain an additional information about
the structure of the space Im V T in (6.431).
Theorem 6.149 Assume that system (SDS) is nonoscillatory at ∞with
ˆKmin
deﬁned in Remark 6.130. Let ˜Y and Y be two conjoined bases from a given genus
G, such that Y is a dominant solution of (SDS) at ∞and such that ˜Y and Y have
constant kernel on [N, ∞)Z and no forward focal points in (N, ∞) for some index
N ∈[ ˆKmin, ∞)Z. Let ˜P , P ˜S∞, ˜T be the matrices in (6.234), (6.238), (6.237) deﬁned

536
6
Miscellaneous Topics on Symplectic Systems
through ˜Xk. Then the limit of X†
k ˜Xk as k →∞exists and satisﬁes
lim
k→∞X†
k ˜Xk = V
with
Im V T = Im ˜T ⊕Im ( ˜P −P ˜S∞).
(6.440)
Proof We proceed similarly as in the proof of Theorem 6.146 (with ˆY := ˜Y), since
some of those arguments did not depend on the fact that ˆY was a recessive solution
of (SDS) at ∞. Let ˜N := w( ˜Y, Y). Then, as in (6.432) and (6.433),
Xk = ˜Xk ( ˜P ˜M + ˜Sk ˜N),
˜Xk = Xk (P ˜M−1−Sk ˜NT ), X†
k ˜Xk = P ˜M−1−Sk ˜NT
on [N, ∞)Z, where ˜M is invertible. Let ˜T ∗and T ∗be the matrices in (6.237) deﬁned
through the minimal conjoined bases ˜Y ∗and Y ∗, which are contained in ˜Y and Y
on [N, ∞)Z, respectively. Then by (6.348) we have
T ∗= ( ˜M∗)T ˜T ∗˜M∗+ ( ˜M∗)T ˜N∗,
(6.441)
as in (6.434). Since Y is a dominant solution of (SDS) at ∞, we get by (6.409) the
equality Im T ∗= Im T = Im PS∞with T given in (6.237). Moreover, as in (6.437),
lim
k→∞X†
k ˜Xk = lim
k→∞(P ˜M−1 −Sk ˜NT ) = V := P ˜M−1 −(T ∗)† ˜NT
(6.442)
with V ˜P = V and Ker V ⊆Ker ˜P ⊕Im P ˜S∞. This shows that every vector
v ∈Ker V can be uniquely decomposed as v = v1 + v2, where v1 ∈Ker ˜P
and v2 ∈Im P ˜S∞. Then for the vector w := P ˜M−1v2, we have the equality
w = P ˜M−1P ˜S∞v2 = PS∞( ˜M∗)−1v2 by (6.349). Therefore, w ∈Im PS∞, and
w = (T ∗)† ˜NT v2 by (6.442). Moreover, since (P ˜S∞˜M∗)† = PS∞( ˜M∗)−1 holds
by (6.274), we then obtain
v2 = P ˜S∞v2 = (P ˜S∞˜M∗) (P ˜S∞˜M∗)†v2
= P ˜S∞˜M∗PS∞( ˜M∗)−1v2 = P ˜S∞˜M∗w.
(6.443)
With the identities (T ∗)†PS∞= (T ∗)† and PS∞˜NT P ˜S∞= ( ˜N∗)T , we then obtain
w = (T ∗)† ˜NT v2
(6.443)
=
(T ∗)†PS∞˜NT P ˜S∞˜M∗w = (T ∗)†( ˜N∗)T ˜M∗w.
(6.444)
This expression for the vector w will be utilized in the subsequent proof. Next we
will derive the formula
Ker V =  Ker ˜T ∗∩Im P ˜S∞
 ⊕Ker ˜P.
(6.445)

6.3
Symplectic Systems Without Controllability
537
Let v ∈Ker V and let v1, v2, and w be its associated vectors deﬁned above. If
we multiply (6.444) by T ∗from the left and use the identities T ∗(T ∗)† = PS∞
and PS∞( ˜N∗)T
= ( ˜N∗)T , then we get T ∗w = ( ˜N∗)T ˜M∗w = ( ˜M∗)T ˜N∗w.
By using (6.441) in the last equality, it follows that ( ˜M∗)T ˜T ∗˜M∗w = 0. The
invertibility of ˜M∗and the equality ˜T ∗= ˜T ∗P ˜S∞then imply that ˜T ∗P ˜S∞˜M∗w =
0. Therefore, the vector v2 = P ˜S∞˜M∗w satisﬁes v2 ∈Ker ˜T ∗∩Im P ˜S∞. Hence,
the inclusion ⊆in (6.445) holds. Conversely, assume that v ∈ Ker ˜T ∗∩Im P ˜S∞
⊕
Ker ˜P . Then we can write v = v1+v2 with v1 ∈Ker ˜T ∗∩Im P ˜S∞and v2 ∈Ker ˜P .
Since V ˜P = V , it follows from (6.442) that V v = V v1 = [P ˜M−1 −(T ∗)† ˜NT ] v1.
In turn, applying the identities v1
= P ˜S∞v1, P ˜M−1P ˜S∞
= PS∞( ˜M∗)−1
from (6.349), (T ∗)†PS∞= (T ∗)†, PS∞˜NT P ˜S∞= ( ˜N∗)T , and (T ∗)†T ∗= PS∞
then yields
V v = [P ˜M−1 −(T ∗)† ˜NT ] P ˜S∞v1 = [PS∞( ˜M∗)−1 −(T ∗)†( ˜N∗)T ] v1
= (T ∗)† [T ∗( ˜M∗)−1 −( ˜N∗)T ] v1 = (T ∗)†( ˜M∗)T ˜T ∗v1,
(6.446)
because T ∗( ˜M∗)−1 −( ˜N∗)T = ( ˜M∗)T ˜T ∗by (6.441), the invertibility of ˜M∗, and
the symmetry of ( ˜M∗)T ˜N∗. Since v1 ∈Ker ˜T ∗, formula (6.446) yields that V v = 0,
i.e., v ∈Ker V . Thus, the inclusion ⊇in (6.445) is satisﬁed as well. According
to Proposition 6.86, we have ˜T = ˜T ∗, and hence, the matrix ˜T ∗in (6.445) can
be replaced by ˜T . Finally, by (6.239) we have the inclusions Im ˜T ⊆Im ˜P and
Im ˜T ∩Ker P ˜S∞⊆Im P ˜S∞∩Ker P ˜S∞= {0}, which implies that
Im V T = ( Ker V )⊥=

Im ˜T ⊕Ker P ˜S∞

∩Im ˜P = Im ˜T ⊕

Ker P ˜S∞∩Im ˜P

.
This shows the second condition in (6.440) and the proof is complete.
⊓⊔
The rank of the limiting matrix V in (6.440) can be connected with the notions
of rank and defect of the genus G, which we introduce in the following remark.
Remark 6.150 Let (SDS) be a nonoscillatory symplectic system at ∞. For every
genus G, we introduce its rank and defect as follows. The number rankG is deﬁned
as the rank of ˜Y, where ˜Y is any conjoined basis from G. This quantity is well
deﬁned, since any two conjoined bases from G have eventually the same image of
their ﬁrst components. Then n −d∞≤rankG ≤n. Also, we deﬁne the quantity
def G := n −rank G, for which we have the inequalities 0 ≤def G ≤d∞. From
Theorem 6.149 it then follows that the matrix V in (6.440) satisﬁes
rank V = rank ˜T + d∞−def G,
(6.447)
since in this case rank V = rank ˜T +rank ˜P −rankP ˜S∞, while rank ˜P = rankG and
rankP ˜S∞= n −d∞. Formula (6.447) shows that the actual value of the rank of V
depends primarily on the rank of ˜T , because the quantities d∞and def G are ﬁxed
within one system (SDS) and its genus G. In particular, the rank of V is minimal

538
6
Miscellaneous Topics on Symplectic Systems
(equal to d∞−def G) if and only if the conjoined basis ˜Y is a recessive solution
of (SDS) at ∞. This property is well-known in the controllable case, for which
d∞= 0 = def G and hence, rankV = rank ˜T = 0 as in Theorem 2.67.
6.3.10
Reid’s Construction of Minimal Recessive Solution
In this subsection we present the so-called Reid construction of the (minimal)
recessive solution of (SDS) at ∞by means of a pointwise limit of certain
speciﬁcally chosen solutions of (SDS). First we derive a representation of the
minimal recessive solution of (SDS) at ∞in terms of any minimal conjoined basis
Y of (SDS). This result is based on the properties of the associated conjoined basis
¯Y from Proposition 6.67 and Theorem 6.69.
Proposition 6.151 Assume that system (SDS) is nonoscillatory at ∞. Suppose that
N ∈[0, ∞)Z is an index such that d[N, ∞)Z = d∞and there exists a conjoined
basis of (SDS) with constant kernel on [N, ∞)Z and no forward focal points in
(N, ∞). Then a solution ˆY of (SDS) is a minimal recessive solution at ∞with
respect to the interval [N, ∞)Z if and only if
ˆYk = Yk −¯YkT,
k ∈[N, ∞)Z,
(6.448)
for some minimal conjoined basis Y of (SDS) on [N, ∞)Z. Here ¯Y is the conjoined
basis of (SDS) from Proposition 6.67 associated with Y, and the matrix T is deﬁned
in (6.237).
Proof Let N be as in the proposition. If ˆY is a minimal recessive solution at ∞
with respect to [N, ∞)Z, then it is a minimal conjoined basis on [N, ∞)Z, and the
associated matrix ˆT in (6.237) satisﬁes ˆT = 0. Formula (6.448) then holds trivially
with Y := ˆY. The opposite implication follows from the proof of Theorem 6.113,
where it is shown that Y −¯YT is a minimal recessive solution of (SDS) at ∞.
⊓⊔
Remark 6.152 In the proof of Theorem 6.113, we showed that the minimal
recessive solution ˆY of (SDS) at ∞in Proposition 6.151 has constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞). The uniqueness of the minimal
recessive solution at ∞in Theorem 6.113 and the deﬁnition of the index ˆKmin in
Remark 6.130 then imply that the index N appearing in Proposition 6.151 satisﬁes
N ∈[ ˆKmin, ∞)Z.
Next we state and prove the main result of this subsection. We recall that by ˆY min
we denote the minimal recessive solution of (SDS) at ∞.
Theorem 6.153 Let Y be a minimal conjoined basis of (SDS) on [N, ∞)Z with
d[N, ∞)Z = d∞, and let ¯Y be the associated conjoined basis of (SDS) from
Proposition 6.67. Then there exists an index L ∈[N, ∞)Z such that ¯Xk is invertible

6.3
Symplectic Systems Without Controllability
539
for all k ∈[L, ∞)Z and the solutions Y (j) of (SDS) given by the initial conditions
X(j)
j
= 0,
U(j)
j
= −¯XT −1
j
,
j ∈[L, ∞)Z,
(6.449)
are conjoined bases satisfying
ˆY min
k
= lim
j→∞Y (j)
k
,
k ∈[0, ∞)Z.
(6.450)
Proof Let Y and ¯Y be as in the theorem, and let P, Sk, PSk, and PS∞be the matrices
in (6.234), (6.236), and (6.238), which correspond to Y. Since Y is a minimal
conjoined basis on [N, ∞)Z, we have the equality P = PS∞. Moreover, with
L := min{k ∈[N, ∞)Z, rankSk = n −d[N, ∞)Z = n −d∞}
(6.451)
it follows that PSk = PS∞for every k ∈[L, ∞)Z. The equality Ker ¯Xk =
Im (P −PSk) on [N, ∞)Z in Proposition 6.67(vi) then implies that Ker ¯Xk = {0}
for all k ∈[L, ∞)Z, i.e., the matrix ¯Xk is invertible on [L, ∞)Z. Consequently,
Proposition 6.67(viii) then reads as
S†
k = ¯X−1
k Xk PS∞= ¯X−1
k Xk,
k ∈[L, ∞)Z.
(6.452)
Fix now an index j ∈[L, ∞)Z, and let Y (j) be the solution of (SDS) given by the
initial conditions (6.449). It is easy to check that Y (j) is a conjoined basis of (SDS).
Moreover, consider the constant matrices M(j) and N(j) such that
Y (j)
k
= YkM(j) + ¯YkN(j),
k ∈[0, ∞)Z.
(6.453)
Since the conjoined bases Y and ¯Y are normalized by Proposition 6.67(i), the
fundamental matrix (Yk, ¯Yk) in (6.453) is symplectic with the formula for its inverse
(Yk, ¯Yk)−1 = −J (Yk, ¯Yk)TJ , by Lemma 1.58(ii). Thus, M(j) = −w( ¯Y, Y (j)) and
N(j) = w(Y, Y (j)), i.e.,
M(j) = ¯UT
k X(j)
k
−¯XT
k U(j)
k ,
N(j) = XT
k U(j)
k
−UT
k X(j)
k ,
k ∈[0, ∞)Z.
(6.454)
In particular, by evaluating (6.454) at k = j and using (6.449), we obtain that
M(j) = I,
N(j) = −XT
j ¯XT −1
j
(6.452)
=
−(S†
j )T = −S†
j .
(6.455)
It follows from (6.455) that M(j) →I and N(j) →−T as j →∞, where T is
the matrix in (6.237) associated with Y. Finally, the representation in (6.453) and
Proposition 6.151 then imply that for each k ∈[0, ∞)Z the limit of Y (j)
k
as j →∞
exists and it is equal to ˆY min
k
. Therefore, formula (6.450) holds and the proof is
complete.
⊓⊔

540
6
Miscellaneous Topics on Symplectic Systems
Remark 6.154 The initial conditions in (6.449) mean that for each j ∈[N, ∞)Z,
the solution Y (j) is a conjoined basis, which is a constant nonsingular multiple of
the principal solution Y [j], namely, Y (j)
k
= Y [j]
k Mj for all k ∈[0, ∞)Z, where
Mj := −¯XT −1
j
. Then it follows from Remark 6.152 and Theorem 6.143 that for
every index j ∈[L, ∞)Z, the conjoined basis Y (j)
k
used in the Reid construction of
ˆY min in Theorem 6.153 is a (minimal) dominant solution of (SDS) at ∞.
In Theorem 6.143 we presented examples of minimal dominant solutions
of (SDS) at ∞. Based on formula (6.452), we can now prove that the conjoined
basis ¯Y considered in Theorem 6.153 is an example of a maximal dominant solution
of (SDS) at ∞.
Proposition 6.155 Let Y be a minimal conjoined basis of (SDS) on an interval
[N, ∞)Z satisfying d[N, ∞)Z = d∞. Then the associated conjoined basis ¯Y from
Proposition 6.67 is a maximal dominant solution of (SDS) at ∞.
Proof Let Rk, Sk, T , PS∞be the matrices in (6.233), (6.236), (6.237), (6.238)
corresponding to Y. From the proof of Theorem 6.153, we know that the matrix
¯Xk is invertible on the interval [L, ∞)Z, where L is deﬁned in (6.451). Let ¯S(L)
k
be
the matrix in (6.236) deﬁned through ¯Xk on [L, ∞)Z, i.e.,
¯S(L)
k
:=
k−1

j=L
¯X−1
j+1Bj ¯XT −1
j
,
k ∈[L, ∞)Z.
(6.456)
We show that ¯S(L)
k
has a limit as k →∞. First we note that equality (6.452) yields
S†
kX†
k = ¯X−1
k XkX†
k = ¯X−1
k Rk on [L, ∞)Z. By using (6.456), Theorem 6.66(ii), and
the identities Sk = X†
k+1BkX†T
k
and SkS†
k = S†
kSk = PS∞on [L, ∞)Z, we get
for every k ∈[L, ∞)Z
¯S(L)
k
=
k−1

j=L
¯X−1
j+1Rj+1BjRj ¯XT −1
j
=
k−1

j=L
S†
j+1X†
j+1BjX†T
j S†
j
=
k−1

j=L
S†
j+1(Sj) S†
j =
k−1

j=L
(S†
j+1Sj+1S†
j −S†
j+1SjS†
j )
=
k−1

j=L
(PS∞S†
j −S†
j+1PS∞) = −
k−1

j=L
(S†
j ) = S†
L −S†
k.
Therefore, the limit of ¯S(L)
k
as k →∞exists and is equal to S†
L −T . By
Theorem 6.124 we conclude that ¯Y is a (maximal) dominant solution of (SDS) at ∞.
⊓⊔

6.3
Symplectic Systems Without Controllability
541
Remark 6.156 Assume, as in Proposition 6.155, that [N, ∞)Z is an interval such
that there exists a minimal conjoined basis of (SDS) on [N, ∞)Z. From Theo-
rem 6.75 it then follows that the index L in (6.451) also satisﬁes
L = min{k ∈[N, ∞)Z, Y [N] has constant kernel on [k, ∞)Z},
(6.457)
where Y [N] is the principal solution of (SDS) at the index N.
Next we shall comment the existence and uniqueness of the limit in (6.450).
Remark 6.157 The proof of Theorem 6.153 solves also the problem, when the limit
in (6.450) exists and what is its value depending on the chosen initial conditions
in (6.449). More precisely, let Y be a minimal conjoined basis of (SDS) on
[N, ∞)Z with d[N, ∞)Z = d∞, and let ¯Y be the associated conjoined basis
from Proposition 6.67. Assume that Y (j) is the solution of (SDS) given by the
initial conditions X(j)
j
= 0 and U(j)
j
= Wj with an invertible matrix Wj for all
j ∈[ ˜L, ∞)Z for some ˜L ∈[L, ∞)Z, where L is deﬁned in (6.457). Then for
k ∈[0, ∞)Z, the limit of Y (j)
k
as j →∞exists if and only if Wj = −¯XT −1
j
Ej for
j ∈[L, ∞)Z, where Ej are invertible matrices such that the limit E := limj→∞Ej
exists. In this case
lim
j→∞Y (j)
k
= ˆY min
k
E,
k ∈[0, ∞)Z,
where ˆY min is the minimal recessive solution of (SDS) at ∞from Remark 6.130.
This statement follows from the proof of Theorem 6.153, in which the matrices M(j)
and N(j) in (6.454) are given by M(j) = Ej and N(j) = −S†
j Ej.
In view of Proposition 6.155, we can deduce that the choice of initial condi-
tions (6.449) with ¯Y being a maximal dominant solution of (SDS) at ∞is natural
and the only possible in order to guarantee the existence of the limit in (6.450).
Remark 6.158 Let N, L, Y, and ¯Y be as in Remark 6.157. Let ˜Y be a conjoined
basis of (SDS) belonging to the maximal genus Gmax, i.e., the matrix ˜Xk is invertible
for every k ∈[ ˜L, ∞)Z for some ˜L ∈[L, ∞)Z. Without loss of generality, assume
that ¯Y and ˜Y have no forward focal points in ( ˜L, ∞). For j ∈[ ˜L, ∞)Z, let Y (j) be
the solution of (SDS) given by the initial conditions X(j)
j
= 0 and U(j)
j
= ˜XT −1
j
.
By applying Remark 6.157 with Wj := ˜XT −1
j
and Ej := −¯XT
j ˜XT −1
j
, we will show
that
limj→∞Y (j) exists if and only if
˜Y is a (maximal) dominant solution at ∞.

(6.458)
In this case the limit in (6.458) is the minimal recessive solution of (SDS) at
∞. Assume ﬁrst that ˜Y is a (maximal) dominant solution of (SDS) at ∞. From
Theorem 6.153 we know that the conjoined basis ¯Y belongs to the maximal genus

542
6
Miscellaneous Topics on Symplectic Systems
Gmax. By Theorem 6.149 and Remark 6.150 (with G := Gmax, N := ˜L, Y := ˜Y, and
˜Y := ¯Y), we then obtain
lim
j→∞(−ET
j ) = lim
j→∞
˜X−1
j
¯Xj = V,
rank V = rank ¯T + d∞,
(6.459)
where ¯T is the matrix in (6.237), which corresponds to ¯Y on [ ˜L, ∞)Z. Moreover,
from Proposition 6.155 it follows that ¯Y is a (maximal) dominant solution of (SDS)
at ∞, so that rank ¯T = n −d∞by Deﬁnition 6.122. Hence, the matrix V in (6.459)
is invertible and the sequence Ej →E := −V T as j →∞with E being invertible.
Therefore, the limit in (6.458) exists, and it is equal to the minimal recessive solution
of (SDS) at ∞, by Remark 6.157.
Conversely, assume that the limit in (6.458) exists. This means that
E := lim
j→∞Ej = −lim
j→∞
¯XT
j ˜XT −1
j
(6.460)
exists, by Remark 6.157. By using the fact that ¯Y is a maximal dominant solution at
∞, we obtain (through Theorem 6.149 and Remark 6.150) that
F := lim
j→∞
¯X−1
j
˜Xj = −lim
j→∞ET −1
j
,
rankF = rank ˜T + d∞
(6.461)
also exists, where ˜T is the matrix in (6.237) associated with ˜Y on [ ˜L, ∞)Z.
Identities (6.460) and (6.461) then imply that both the matrices E and F are
invertible with E = −F T −1. In turn, the limit in (6.458) is the minimal recessive
solution of (SDS) at ∞, again by Remark 6.157. Finally, from the second equality
in (6.461), it follows that rank ˜T = rank F −d∞= n −d∞. Thus, ˜Y is a (maximal)
dominant solution of (SDS) at ∞by Deﬁnition 6.122.
In the last part of this section, we will discuss the dependence of the construction
of the minimal recessive solution ˆY min at ∞in Theorem 6.153 with respect to the
used initial data, in particular with respect to the choice of the conjoined bases ¯Y,
Y, and the index N. First we observe that the representation of Y (j) in (6.453) with
M(j) = I and N(j) = −S†
j from (6.455) does not depend on the choice of ¯Y, since
in this case N(j) = PN(j) and the solution ¯YP is uniquely determined by Y on
[N, ∞)Z by Proposition 6.67(iv). Therefore,the construction of ˆY min in (6.450) does
not depend on the choice of ¯Y either. Moreover, in the following remark, we will
show the independence of the construction in Theorem 6.153 also on the minimal
conjoined basis Y.
Remark 6.159 Assume now that in addition to Y in Theorem 6.153, we start
with another minimal conjoined basis Y ∗of (SDS) on [N, ∞)Z and let ¯Y ∗be
its associated conjoined basis from Proposition 6.67. By Lemma 6.100 (with
Y (1) := Y and Y (2) := Y ∗), we then have ¯X∗
k = ¯XkG for all k ∈[N, ∞)Z with
a constant invertible matrix G. Following (6.449) we construct for j ∈[L, ∞)Z the

6.3
Symplectic Systems Without Controllability
543
conjoined bases Y ∗(j) of (SDS) satisfying X∗(j)
j
= 0 and U∗(j)
j
= −( ¯X∗
j )T −1 =
−¯XT −1
j
GT −1. Hence, we obtain by the uniqueness of solutions of (SDS) that
Y ∗(j)
k
= Y (j)
k
M on [0, ∞)Z with M := GT −1. This implies that
lim
j→∞Y ∗(j)
k
= lim
j→∞Y (j)
k
M = ˆY min
k
M,
k ∈[0, ∞)Z.
(6.462)
Thus, using a different minimal conjoined basis Y ∗of (SDS) in Theorem 6.153 leads
through (6.462) to a constant right nonsingular multiple of the minimal recessive
solution ˆY min. But since ˆY min is essentially unique by Theorem 6.113, it follows
that the construction in Theorem 6.153 does not depend on the choice of the minimal
conjoined basis Y.
Remark 6.160 Finally, we note that the result in Theorem 6.153 also does not
depend on the index N, i.e., on the interval [N, ∞)Z on which Y is a minimal
conjoined basis of (SDS). This follows immediately from the uniqueness of the
minimal recessive solution of (SDS) at ∞in Theorem 6.113. Hence, moving
the index N to the right yields a constant right nonsingular multiple in the
representation (6.450), similarly to formula (6.462) in the previous remark.
6.3.11
Additional Properties of Minimal Recessive Solution
In this subsection we derive some additional properties of the minimal recessive
solution ˆY min at ∞. These properties will be utilized in Sect. 6.4 in the singular
Sturmian theory for system (SDS). Our ﬁrst result represents a variant of Corol-
lary 6.147, but here we consider the conjoined bases ˆY min ∈Gmin and Y ∈G to be
from different genera of conjoined bases of (SDS).
Theorem 6.161 Assume that (SDS) is nonoscillatory at ∞, and let ˆKmin be the
index deﬁned in Remark 6.130 for ˆY min. Then for any N ∈[ ˆNmin, ∞)Z, the
conjoined basis ¯Y [∞] of (SDS), which is associated with ˆY min on [N, ∞)Z in
Proposition 6.67, is a maximal dominant solution of (SDS) at ∞, and it satisﬁes
lim
k→∞( ¯X[∞]
k
)−1 ˆXmin
k
= 0.
(6.463)
Proof Fix N ∈[ ˆKmin, ∞)Z, and let ¯Y [∞] be the conjoined basis of (SDS) from
Proposition 6.67, which is associated with ˆY min on [N, ∞)Z. Then (6.414) holds,
and the result in Proposition 6.155 (with Y := ˆY min) implies that ¯Y [∞] is a maximal
dominant solution of (SDS) at ∞. It remains to prove (6.463). For simplicity of the
notation, we set ˆY := ˆY min and Y := ¯Y [∞]. First we consider the maximal recessive
solution ˆY max of (SDS) at ∞, which contains ˆY on [N, ∞)Z with respect to the
orthogonal projector P ˆS∞deﬁned in (6.238) through ˆY. Such a maximal recessive

544
6
Miscellaneous Topics on Symplectic Systems
solution of (SDS) at ∞exists by Theorem 6.115, and it can be chosen so that it
contains ˆY on [N, ∞)Z by Theorem 6.87 (with P = I = R and P ∗= P ˆS∞). Then
we have
ˆXk = ˆXmax
k
P ˆS∞,
k ∈[N, ∞)Z.
(6.464)
By Theorem 6.141 for the maximal genus G := Gmax (with the given Y and with
ˆY := ˆY max), we obtain that rank [P ˆS∞w( ˆY max, Y) PS∞] = n −d∞. In turn, by
Corollary 6.148 (with ˆY := ˆY max), we get
lim
k→∞X−1
k
ˆXmax
k
= V
with
Im V T = Ker P ˆS∞.
(6.465)
Therefore, we conclude that
lim
k→∞X−1
k
ˆXk
(6.464)
=
lim
k→∞X−1
k
ˆXmax
k
P ˆS∞= V P ˆS∞
(6.465)
=
0,
since Im P ˆS∞= Ker V by (6.465). This shows that (6.463) holds.
⊓⊔
In the next statement, we apply Theorem 6.161 in order to derive a new property
of the Wronskians of the minimal recessive solution ˆY min of (SDS) at ∞with
a conjoined basis Y and its associated conjoined basis ¯Y. Here we use a symplectic
fundamental matrix [∞]
k
of (SDS), which is associated with the minimal recessive
solution ˆY min, i.e., following (6.228) we introduce the matrix
[∞]
k
:=

ˆY min
k
¯Y [∞]
k

,
k ∈[N, ∞)Z,
w( ˆY min, ¯Y [∞]) = I.
(6.466)
Then, as in Lemma 6.59, every conjoined basis Y of (SDS) can be uniquely
represented by a constant 2n × n matrix D∞such that
Yk = [∞]
k
D∞,
k ∈[0, ∞)Z,
JD∞=
w(Y [∞], Y)
w( ¯Y [∞], Y)

.
(6.467)
Theorem 6.162 Assume that (SDS) is nonoscillatory at ∞. Let Y be a conjoined
basis of (SDS) with constant kernel on [N, ∞)Z and no forward focal points in
(N, ∞) for some N ∈[0, ∞)Z, and let ¯Y be the conjoined basis of (SDS) from
Proposition 6.67 associated with Y on [N, ∞)Z. Then
w( ˆY min, Y) [w( ˆY min, ¯Y)]T ≥0.
Proof First we observe that by Lemma 2.4 (with the choice ˜Y := ˆY min) the matrix
w( ˆY min, Y) [w( ˆY min, ¯Y)]T is symmetric. Choose an index K ∈[N, ∞)Z so that
d[K, ∞)Z = d∞and the conjoined bases Y, ¯Y, and ˆY min have constant kernel
on [K, ∞)Z and no forward focal points in (K, ∞). Let ¯Y [∞] be the conjoined

6.3
Symplectic Systems Without Controllability
545
basis of (SDS) from Proposition 6.67 associated with ˆY min on the interval [K, ∞)Z,
i.e., ( ˆXmin
K )† ¯X[∞]
K
= 0. Let D∞be the representing matrix of Y in terms of the
symplectic fundamental matrix [∞]
k
, i.e., (6.228) with j = ∞and (6.467) hold.
As in the proof of Theorem 6.80, we split D∞=
 F
G

with F = −w( ¯Y [∞], Y) and
G = w( ˆY min, Y). Then
Xk = ˆXmin
k
F + ¯X[∞]
k
G,
k ∈[0, ∞)Z.
(6.468)
From Theorem 6.161 we know that ¯Y [∞] is a maximal dominant solution of (SDS)
at ∞, so that the matrix ¯X[∞]
k
is invertible for all k large enough. By applying
Theorem 6.161, we then obtain
lim
k→∞( ¯X[∞]
k
)−1Xk
(6.468)
=
lim
k→∞( ¯X[∞]
k
)−1 ˆXmin
k
F + G
(6.463)
=
G = w( ˆY min, Y).
(6.469)
By repeating the above argument with the conjoined basis ¯Y instead of Y, we
conclude similarly as in (6.469) that
lim
k→∞( ¯X[∞]
k
)−1 ¯Xk = w( ˆY min, ¯Y).
(6.470)
Therefore, upon combining (6.469) and (6.470), we get
w( ˆY min, Y) [w( ˆY min, ¯Y)]T = lim
k→∞( ¯X[∞]
k
)−1Xk ¯XT
k ( ¯X[∞]
k
)T −1 ≥0,
where the last inequality follows from Proposition 6.67(xii).
⊓⊔
6.3.12
Further Examples
In this subsection we present three examples, in which we illustrate the theory of
dominant solutions at ∞—their limit comparison with recessive solutions at ∞and
the Reid construction of the (minimal) recessive solution at ∞in terms of (maximal)
dominant solutions at ∞. For this purpose we will utilize Examples 6.119–6.121
from Sect. 6.3.6. In agreement with the notation in Remarks 6.110 and 6.123,
recessive solutions at ∞will be denoted by ˆY and in the special case of minimal and
maximal recessive solutions at ∞by ˆY min and ˆY max. Similarly, dominant solutions
at ∞will be denoted by Y and in the special case of minimal and maximal dominant
solutions at ∞by Y min and Y max.
Example 6.163 Let us continue the considerations initiated in Example 6.119.
Consider a nonoscillatory scalar system (SDS) with n = 1 and Sk ≡
 1 1
0 1

on
[0, ∞)Z. From Example 6.119 we know that d∞= 0 and that the conjoined basis

546
6
Miscellaneous Topics on Symplectic Systems
ˆYk ≡
 1
0

of (SDS) is the unique recessive solution at ∞. In this case we have
ˆKmin = 0. On the other hand, according to Corollary 6.145, the conjoined basis
Y = Y [0] =  k
1
 of (SDS) is a dominant solution at ∞, being at the same time
the principal solution of (SDS) at the index k = 0. Moreover, the solutions ˆY
and Y satisfy X−1
k
ˆXk = 1/k →0 as k →∞, as we claim in formula (6.439)
of Corollary 6.147. Finally, the Reid construction of the recessive solution ˆY in
Theorem 6.153 is the following. With ¯Y := Y we have N = 1, and for any index
j ∈[1, ∞)Z, the solution Y (j) of (SDS) from (6.449) satisfying the initial conditions
X(j)
j
= 0 and U(j)
j
= −¯XT −1
j
= −1/j is
X(j)
k
= (j −k)/j,
U(j)
k
= −1/j,
k ∈[0, ∞)Z.
Therefore, Y (j)
k
→
 1
0

= ˆYk as j →∞for every k ∈[0, ∞)Z, as we claim in
formula (6.450).
Example 6.164 We continue with Example 6.120. Consider a nonoscillatory sys-
tem (SDS) with n ∈N and Sk ≡I2n on [0, ∞)Z. Then d[0, ∞)Z = d∞= n
and every conjoined basis of (SDS) is a (constant) recessive and also dominant
solution at ∞with respect to the interval [0, ∞)Z. Therefore, we have ˆKmin = 0.
Every genus G of conjoined bases is associated with a unique orthogonal projector
P ∈Rn×n such that the conjoined basis ˆY = Y =

P
I−P

is a recessive and
dominant solution at ∞belonging to G. In addition, we have X†
k ˆXk = P †P = P for
all k →∞, so that V = P, ˆP = P, and P ˆS∞= 0 in formula (6.431). The special
choice of P = 0 then yields the solutions ˆY min = Y min =  0
I
, while the choice of
P = I yields the solutions ˆY max = Y max =  I
0
. Note that Y ∗=  I
I
 is another
maximal recessive and dominant solution at ∞, which illustrates the nonuniqueness
of these solutions in Remark 6.144.
Example 6.165 We continue with Example 6.121. Consider a nonoscillatory sys-
tem (SDS) with Ak = Dk ≡I3, Bk ≡diag{1, 0, 0}, and Ck ≡03 on [0, ∞)Z.
This system arises from the scalar system in Example 6.163 and from the system in
Example 6.164 with dimension two by a block diagonal construction. In this case
d[0, ∞)Z = d∞= 2 and ˆKmin = 0. From Theorems 6.118 and 6.133, we know that
some recessive and dominant solutions of (SDS) at ∞can be constructed from the
recessive and dominant solutions at ∞in Examples 6.163 and 6.164 by the same
block diagonal procedure.
(a) First we analyze the minimal genus Gmin with rank r = n −d∞= 1. Then
ˆY min
k
=  diag{1, 0, 0}, diag{0, 1, 1} T ,
Y min
k
=

diag{k, 0, 0}, diag{1, 1, 1}
T .

6.3
Symplectic Systems Without Controllability
547
In this case (Xmin
k
)† ˆXmin
k
= diag{1/k, 0, 0} →03 as k →∞, as we claim in
formula (6.439) of Corollary 6.147.
(b) Further, we examine the maximal genus Gmax, whose rank is r = n = 3. Then
we have
ˆY max
k
=

diag{1, 1, 1}, diag{0, 0, 0}
T ,
Y max
k
=

diag{k, 1, 1}, diag{1, 0, 0}
T ,
so that (Xmax
k
)† ˆXmax
k
= diag{1/k, 1, 1} →V := diag{0, 1, 1} as k →∞. In
this case we have P ˆS∞= diag{1, 0, 0} with Im V T = {0} × R2 = Ker P ˆS∞, as
we state in Corollary 6.148.
(c) Next we discuss three different genera with rank equal to r = 2. We note that
only two of them arise from the diagonal construction mentioned above. Let
G1 be the genus with rank r = 2, which contains the recessive and dominant
solutions at ∞
ˆY (1)
k
=

diag{1, 1, 0}, diag{0, 0, 1}
T ,
Y (1)
k
=  diag{k, 1, 0}, diag{1, 0, 1} T .
In Theorem 6.146 we then have (X(1)
k )† ˆX(1)
k
= diag{1/k, 1, 0} →V :=
diag{0, 1, 0} as k →∞, and ˆP = diag{1, 1, 0} and P ˆS∞= diag{1, 0, 0}. Let
G2 be the genus with rank r = 2 given by the recessive and dominant solutions
at ∞
ˆY (2)
k
=

diag{1, 0, 1}, diag{0, 1, 0} )T ,
Y (2)
k
=

diag{k, 0, 1}, diag{1, 1, 0} )T .
In this case we have (X(2)
k )† ˆX(2)
k
= diag{1/k, 0, 1} →V := diag{0, 0, 1} as
k →∞, and ˆP = diag{1, 0, 1} and P ˆS∞= diag{1, 0, 0}. Now we consider the
nondiagonal genus G3 with rank r = 2 deﬁned by the recessive and dominant
solutions at ∞
ˆY (3)
k
=
⎛
⎝
⎛
⎝
1 0 0
0 1 0
0 1 0
⎞
⎠,
⎛
⎝
0 0
0
0 1
1
0 −1 −1
⎞
⎠
⎞
⎠
T
,
Y (3)
k
=
⎛
⎝
⎛
⎝
k 0 0
0 1 −1
0 1 −1
⎞
⎠,
⎛
⎝
1 0 0
0 3 −2
0 1 −2
⎞
⎠
⎞
⎠
T
.

548
6
Miscellaneous Topics on Symplectic Systems
In Theorem 6.146 we then have
(X(3)
k )† ˆX(3)
k
=
⎛
⎝
1/k
0
0
0
1/2
0
0
−1/2
0
⎞
⎠→V :=
⎛
⎝
0
0
0
0
1/2
0
0 −1/2
0
⎞
⎠
as k →∞.
The orthogonal projectors
ˆP and P ˆS∞in (6.431) are given by
ˆP
=
diag{1, 1, 0}, P ˆS∞= diag{1, 0, 0}, and ˆP −P ˆS∞= diag{0, 1, 0}. Hence, in
this case we indeed have Im V T = Im ( ˆP −P ˆS∞), although V T ̸= ˆP −P ˆS∞.
(d) Finally, we present the Reid construction of the minimal recessive solution ˆY min
in Theorem 6.153. With Y := Y min given above in part (a) and with
¯Yk =

diag{k −1, −1, −1}, diag{1, 0, 0}
T
we have N = 1 and L = 2. For j ∈[2, ∞)Z the solution in (6.449) is
Y (j)
k
=

diag{(j −k)/(j −1), 0, 0}, diag{−1/(j −1), 1, 1} )T ,
k ∈[0, ∞)Z.
Then we have Y (j)
k
→(diag{1, 0, 0}, diag{0, 1, 1})T = ˆY min
k
as j →∞for
every k ∈[0, ∞)Z, as we claim in formula (6.450) of Theorem 6.153.
6.4
Singular Sturmian Separation Theorems
In this section we establish singular Sturmian separation theorems for conjoined
bases of (SDS) on unbounded intervals. We will consider discrete intervals, which
are unbounded from above, unbounded from below, or unbounded at both endpoints.
By using the theory of recessive and dominant solutions of (SDS) at ∞, we
essentially improve the results on singular Sturmian theory in Theorem 4.39 and
Remark 4.40 and at the same time provide singular versions of the separation
theorems presented in Sects. 4.2.2 and 4.2.3.
For convenience and easier reference, we will use the notation
Yk =
Xk
Uk

,
˜Yk =
 ˜Xk
˜Uk

, Y [j]
k
=

X[j]
k
U[j]
k

, Y [∞]
k
:= ˆY min
k
, E :=
0
I

,
(6.471)
for generic conjoined bases Y and ˜Y of (SDS), for the principal solution Y [j]
of (SDS) at the index j (satisfying Y [j]
j
= E), and for the minimal recessive solution
ˆY min at ∞. For a conjoined basis Y of (SDS), the multiplicity of forward (or left)
focal point in (k, k + 1] and the multiplicity of the backward (or right) focal point

6.4
Singular Sturmian Separation Theorems
549
in [k, k + 1) will be denoted by
mL(k, k + 1] := m(k) = μ(Yk+1, SkE) = μ(Yk+1, Y [k]
k+1),
(6.472)
mR[k, k + 1) := m∗(k) = μ∗(Yk, S−1
k E) = μ∗(Yk, Y [k+1]
k
),
(6.473)
where m(k) and m∗(k) are deﬁned by (4.3) and (4.6) and where we use the results
of Lemmas 4.7 and 4.8 to connect these multiplicities with the comparative index.
Moreover, using (4.10) and (4.11), we denote by
mL(M, N] := l(Y, M, N) =
N−1

k=M
mL(k, k + 1],
mR[M, N) := l∗(Y, M, N) =
N−1

k=M
mR[k, k + 1),
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(6.474)
the numbers of forward and backward focal points of Y, including their multiplic-
ities, in the intervals (M, N] and [M, N). We will use similar notation 7
mL(M, N]
and 7
mR[M, N) for the numbers of focal points of another conjoined basis ˜Y
of (SDS) in these intervals. We summarize the regular Sturmian separation theorems
from Sects. 4.1 and 4.2 as follows (see Theorems 4.23 and 4.24 and Corollary 4.6).
Proposition 6.166 For any conjoined bases Y and ˜Y of (SDS), we have
mL(M, N] −7
mL(M, N] = μ(YN, ˜YN) −μ(YM, ˜YM),
(6.475)
mR[M, N) −7
mR[M, N) = μ∗(YM, ˜YM) −μ∗(YN, ˜YN),
(6.476)
mL(M, N] + rankXN = mR[M, N) + rankXM.
(6.477)
Similarly as above, for the principal solution Y [j] of (SDS), we denote by
m[j]
L (M, N] and m[j]
R [M, N) the numbers of its forward and backward focal points
in the indicated intervals. Then we summarize the corresponding regular Sturmian
separation theorems as follows (see Theorems 4.34 and 4.35, formulas (4.62)
and (4.63), and Remark 4.28(iii)).
Proposition 6.167 For the principal solutions Y [M] and Y [N] of (SDS), we have
the identities
m[M]
L (M, N] = m[N]
R [M, N),
m[M]
R [M, N) = m[N]
L (M, N].
(6.478)
For any conjoined basis Y of (SDS), we have the estimates
m[M]
L (M, N] ≤mL(M, N] ≤m[N]
L (M, N],
(6.479)
m[N]
R [M, N) ≤mR[M, N) ≤m[M]
R [M, N).
(6.480)

550
6
Miscellaneous Topics on Symplectic Systems
For any conjoined bases Y and ˜Y of (SDS), we have the estimates
		 mL(M, N] −7
mL(M, N]
		 ≤rank X[M]
N
≤n,
(6.481)
		 mR[M, N) −7
mR[M, N)
		 ≤rank X[N]
M
≤n,
(6.482)
		 mL(M, N] −7
mR[M, N)
		 ≤rank X[N]
M
≤n.
(6.483)
In the following subsections, we will extend the results in Propositions 6.166
and 6.167 to unbounded intervals.
6.4.1
Multiplicity of Focal Point at Inﬁnity
In this subsection we assume that system (SDS) is deﬁned on the interval [0, ∞)Z
and that it is nonoscillatory at ∞. As a new notion, we deﬁne for a conjoined
basis Y of (SDS) the multiplicity of its focal point at ∞. In this deﬁnition we
employ the matrix T , which is associated with Y through (6.237). We then prove
a representation formula for the multiplicity at ∞in terms of the Wronskian of Y
with the minimal recessive solution Y [∞] of (SDS) at ∞.
Deﬁnition 6.168 For a conjoined basis Y of (SDS), we deﬁne the quantity
mL(∞) := n −d∞−rankT = def T −d∞,
(6.484)
where T is the matrix in (6.237) associated with Y on an interval [N, ∞)Z satisfying
d[N, ∞)Z = d∞. Moreover, we say that Y has a focal point at ∞if mL(∞) ≥1,
and then mL(∞) is called its multiplicity.
Remark 6.169
(i) Estimate (6.277) shows that the quantity mL(∞) in (6.484) is correctly deﬁned
and that 0 ≤mL(∞) ≤n −d∞. Moreover, by Remark 6.126 the number
mL(∞) does not depend on the index N satisfying d[N, ∞)Z = d∞.
(ii) It follows from Deﬁnition 6.168 that a conjoined basis Y is a recessive solution
of (SDS) at ∞if and only if mL(∞) = n −d∞(i.e., mL(∞) is maximal).
In particular, for the minimal recessive solution Y [∞] of (SDS) at ∞, we have
m[∞]
L
(∞) = n −d∞. Similarly, a conjoined basis Y is a dominant solution
of (SDS) at ∞if and only if Y has no focal point at ∞(i.e., mL(∞) = 0 is
minimal).
(iii) The quantity mL(∞) is preserved under the relation being contained, since
this relation preserves the corresponding matrices Sk and hence T (see
Deﬁnition 6.82 and Proposition 6.86).
In the following result, we provide an alternative formula for the multiplicity
mL(∞) in Deﬁnition 6.168 in terms of the Wronskian of Y with the minimal

6.4
Singular Sturmian Separation Theorems
551
recessive solution Y [∞] at ∞. We recall the quantity rank G for a genus G of
conjoined bases of (SDS) deﬁned in Remark 6.150.
Theorem 6.170 Assume that system (SDS) is nonoscillatory at ∞, and let Y,
belonging to a genus G, be a conjoined basis of (SDS) with constant kernel on
[N, ∞)Z and no forward focal points in (N, ∞), where the index N ∈[0, ∞)Z is
such that d[N, ∞)Z = d∞. Then
Im [w(Y [∞], Y)]T = Im T ⊕Im (P −PS∞),
(6.485)
rankT = rankw(Y [∞], Y) −rankG + n −d∞,
(6.486)
mL(∞) = rankG −rank w(Y [∞], Y),
(6.487)
where P, PS∞, T are the matrices in (6.234), (6.238), (6.237) associated with Y.
Proof Let the index N be as in the theorem. By Remark 6.126 the space Im T
is preserved, when N is replaced by any larger index. Therefore, without loss of
generality, we may assume that the index N ∈[0, ∞)Z is such that d[N, ∞)Z = d∞
holds and both conjoined bases Y and Y [∞] have constant kernel on [N, ∞)Z and
no forward focal points in (N, ∞), i.e., N ≥
ˆKmin according to Remark 6.130.
Let ¯Y [∞] be the conjoined basis from Proposition 6.67 associated with Y [∞] on
[N, ∞)Z. Then by Theorem 6.161, we know that ¯Y [∞] is a maximal dominant
solution of (SDS) at ∞. This yields that there exists an index M > N such that
¯Y [∞] has no forward focal points in (M, ∞) and ¯X[∞]
k
is invertible on [M, ∞)Z.
Moreover, by (6.469) in the proof of Theorem 6.162, we have
lim
k→∞( ¯X[∞]
k
)−1Xk = w(Y [∞], Y).
(6.488)
Let Rk for k ∈[N, ∞)Z be the orthogonal projector onto Im Xk deﬁned in (6.233).
If ¯Y [∞]∗∈G is a conjoined basis of (SDS), which is contained in ¯Y [∞] on [M, ∞)Z
according to Deﬁnition 6.82 and which belongs to the same genus G as Y, then
by (6.298) in Remark 6.83, we have the formula
( ¯X[∞]∗
k
)† = ( ¯X[∞]
k
)−1Rk,
k ∈[M, ∞)Z.
Moreover, ¯Y [∞]∗is also a dominant solution of (SDS) at ∞by Proposition 6.127.
Therefore, upon applying Theorem 6.149 (with Y := ¯Y [∞]∗and ˜Y := Y), we obtain
lim
k→∞( ¯X[∞]∗
k
)†Xk = V
with
Im V T = Im T ⊕Im (P −PS∞).
(6.489)
Consequently, by combining (6.488)–(6.489), we get
V = lim
k→∞( ¯X[∞]
k
)−1RkXk = lim
k→∞( ¯X[∞]
k
)−1Xk
(6.488)
=
w(Y [∞], Y).

552
6
Miscellaneous Topics on Symplectic Systems
This implies through the second part of (6.489) that the equality in (6.485) holds.
By evaluating the ranks of the subspaces in (6.485), we then get
rank w(Y [∞], Y) = rankT + rank P −rankPS∞= rankT + rank G −(n −d∞),
which shows (6.486). Finally, formula (6.487) follows from (6.486) by using the
deﬁnition of the multiplicity mL(∞) in (6.484). The proof is complete.
⊓⊔
Remark 6.171 If system (SDS) is nonoscillatory at ∞and eventually controllable
near ∞, then rank Gmin = n, i.e., G = Gmin = Gmax holds for every genus G (see
Remark 6.58). Hence, in this case (6.484) and (6.487) yield that for every conjoined
basis Y of (SDS)
mL(∞) = def T = n −rankw(Y [∞], Y) = def w(Y [∞], Y).
(6.490)
In order to count the numbers of focal points in unbounded intervals, we adopt
as in (6.474) for any M ∈[0, ∞)Z the notation
mL(M, ∞] := mL(M, ∞) + mL(∞),
mL(M, ∞) := l(Y, M, ∞) =
∞

k=M
mL(k, k + 1],
mR[M, ∞) := l∗(Y, M, ∞) =
∞

k=M
mR[k, k + 1),
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
(6.491)
where we used the deﬁnitions of l(Y, M, ∞) and l∗(Y, M, ∞) in (4.87).
The following result is an analog of the formulas in (6.472), (6.473), and (6.230)
for the unbounded interval [N, ∞)Z. We will use the representation of conjoined
bases of (SDS) in terms of the symplectic fundamental matrix [∞]
k
in (6.466)
and (6.467).
Theorem 6.172 If Y is a conjoined basis of (SDS) with constant kernel on [N, ∞)Z
and no forward focal points in (N, ∞), then
mR[N, ∞) = 0 = μ∗(YN, Y [∞]
N
),
(6.492)
mL(N, ∞] = mL(∞) = μ(JD∞, JD[N]
∞),
(6.493)
where D∞and D[N]
∞are the constant matrices in (6.467) corresponding to Y and
to the principal solution Y [N].
Proof Since Y [∞] is a minimal conjoined basis near ∞and Y has constant kernel
on [N, ∞)Z and no forward focal points in (N, ∞), there exists K ∈[N, ∞)Z such
that both Y [∞] and Y have constant kernel on [K, ∞)Z and no forward focal points
in (K, ∞), and hence Im X[∞]
k
⊆Im Xk on [K, ∞)Z. This means by Theorem 6.80
(with ˜Y := Y [∞]) that Y [∞] is representable by Y on [K, ∞)Z, i.e., the inclusion

6.4
Singular Sturmian Separation Theorems
553
Im w(Y, Y [∞]) ⊆Im P or equivalently the equality P w(Y, Y [∞]) = w(Y, Y [∞])
holds. We ﬁrst prove (6.492). Let ¯Y be the conjoined basis in Proposition 6.67
associated with Y on [N, ∞)Z (i.e., X†
N ¯XN
= 0 holds). Similarly, let ¯Y [∞]
be the conjoined basis in Proposition 6.67 associated with Y [∞] on [K, ∞)Z.
Following (6.466), we denote by k := (Y, ¯Y) and [∞]
k
:= (Y [∞]
k
, ¯Y [∞]
k
) the
corresponding symplectic fundamental matrices of (SDS), so that Yk = kJE and
Y [∞]
k
= [∞]
k
JE for all k ∈[0, ∞)Z. Then by Theorem 3.5(iii) (with Z := −J −1
N
and ˆZ := −J −1
N [∞]
N J ) we obtain
μ∗(YN, Y [∞]
N
) = μ∗(Z−1E, Z−1 ˆZE) = μ(ZE, ˆZE)
= μ

−T
NJE, −T
NJ [∞]
N JE

= μ
 XT
N
¯XT
N

,
w(Y, Y [∞])
w( ¯Y , Y [∞])
 
,
(6.494)
where in the last equality we also applied the property μ(−Y, −˜Y) = μ(Y, ˜Y )
(Theorem 3.5(i) with C1 = C2 := −I). We now calculate the comparative index
in (6.494) by Deﬁnition 3.1. Since ¯XNP = 0 (see Proposition 6.67(iv)), we have
w = XN w( ¯Y, Y [∞]) −¯XNP w(Y, Y [∞]) = XN w( ¯Y, Y [∞]),
M = (I −RN) w = (I −RN) XN w( ¯Y, Y [∞]) = 0,
T = I,
P = [w( ¯Y, Y [∞])]T P w(Y, Y [∞]) = w(Y [∞], ¯Y) [w(Y [∞], Y)]T .
Then by Theorem 6.162, we know that P ≥0, and hence the comparative index
in (6.494) is equal to rank M+ind P = 0. Therefore, we proved μ∗(YN, Y [∞]
N
) = 0.
But since rank Xk is constant on [N, ∞)Z and Y has no forward focal points in
(N, ∞), the calculation (using Lemma 4.38)
mR[N, ∞)
(4.88)
=
mL(N, ∞) + lim
k→∞rank Xk −rank XN = 0
then completes the proof of (6.492). To prove (6.493), we start with the facts that
D∞= ([∞]
N )−1YN = ([∞]
N )−1NJE,
D[N]
∞= ([∞]
N )−1E,
w(JD∞, JD[N]
∞) = Y T
N JE = XT
N.
Therefore, we obtain by Theorem 3.5(v) (with Y := JD∞and ˜Y := JD[N]
∞)
μ(JD∞, JD[N]
∞) = rank XN −μ(JD[N]
∞, JD∞).
(6.495)

554
6
Miscellaneous Topics on Symplectic Systems
Now rank XN = rankG and the last term in (6.495) we calculate by Theorem 3.5(iii)
(with Z := J ([∞]
N )−1 and ˆZ := J ([∞]
N )−1NJ ). Then
μ(JD∞, JD[N]
∞) = rankG −μ

J ([∞]
N )−1E, J ([∞]
N )−1NJE

(iii)
= rankG −μ∗
−[∞]
N JE, NJE

(i)= rankG −μ∗(Y [∞]
N
, YN)
(v)
= rankG −rank w(Y [∞]
N
, YN) + μ∗(YN, Y [∞]
N
)
(6.492)
=
rankG −rank w(YN, Y [∞]
N
)
(6.487)
=
mL(∞).
where above the equality signs we indicate the application of the corresponding
properties (iii) and (i) (with C1 = −C2 := −I) and (v) of Theorem 3.5. Observe
that in the last equality we applied Theorem 6.170. Finally, since Y has no forward
focal points in (N, ∞), it follows from (6.491) that mL(N, ∞] = mL(∞), which
completes the proof of (6.493).
⊓⊔
6.4.2
Singular Separation Theorems I
In this subsection we derive proper extensions of Propositions 6.166 and 6.167 to
unbounded intervals with singular right endpoint. Our results show that, instead of
considering the principal solution Y [N] at the right endpoint k = N, we have to
use the minimal recessive solution Y [∞] at ∞in the singular case. For convenience
we will utilize the notation in (6.491) for the numbers of forward and backward
focal points of conjoined bases Y, ˜Y, Y [M], Y [∞] in the corresponding unbounded
intervals. Our ﬁrst result is a singular version of Proposition 6.166.
Theorem 6.173 (Singular Sturmian Separation Theorem)
Assume that sys-
tem (SDS) is nonoscillatory at ∞. Then for any conjoined bases Y and ˜Y of (SDS),
we have the equalities
mL(M, ∞] −7
mL(M, ∞] = μ(JD∞, J ˜D∞) −μ(YM, ˜YM)
(6.496)
mR[M, ∞) −7
mR[M, ∞) = μ∗(YM, ˜YM) −μ∗(JD∞, J ˜D∞),
(6.497)
mL(M, ∞] + rank w(Y [∞], Y) = mR[M, ∞) + rank XM,
(6.498)
where D∞and ˜D∞are the constant matrices in (6.467) corresponding to Y and ˜Y,
respectively.
Proof Since system (SDS) is nonoscillatory at ∞, we have mL(M, ∞] < ∞and
7
mL(M, ∞] < ∞. Then we can choose N ∈[M, ∞)Z such that both conjoined

6.4
Singular Sturmian Separation Theorems
555
bases Y and ˜Y have constant kernel on [N, ∞)Z and no forward focal points in
(N, ∞). First we will prove that
mL(N, ∞] −7
mL(N, ∞] = μ(JD∞, J ˜D∞) −μ(YN, ˜YN)
(6.499)
mR[N, ∞) −7
mR[N, ∞) = μ∗(YN, ˜YN) −μ∗(JD∞, J ˜D∞).
(6.500)
Let [∞]
k
be the symplectic fundamental matrix of (SDS) in (6.467) with the
associated matrices D∞, ˜D∞, D[N]
∞, and D[∞]
∞
= JE for Y, ˜Y, Y [N], and Y [∞],
respectively. Further, consider the symplectic fundamental matrices k and ˜k
of (SDS) such that kE = Yk and ˜kE = ˜Yk on [0, ∞)Z, i.e., k = (∗, Yk)
and ˜k = (∗, ˜Yk) according to the deﬁnition of E in (6.471). Deﬁne the symplectic
matrix R := −J ([∞]
N )−1. Then (6.467) yields
JD∞= −R NE,
J ˜D∞= −R ˜NE,
JD[N]
∞= −RE.
(6.501)
By Theorem 6.172 and by the transformation formula (3.16) in Theorem 3.6 for the
comparative index (with W := −R, Z := N, ˆZ := ˜N), we get
mL(N, ∞] −7
mL(N, ∞]
(6.493)
=
μ(JD∞, JD[N]
∞) −μ(J ˜D∞, JD[N]
∞)
(6.501)
=
μ(−R NE, −RE) −μ(−R ˜NE, −RE)
(3.16)
= μ(−R NE, −R ˜NE) −μ(NE, ˜NE)
(6.501)
=
μ(JD∞, J ˜D∞) −μ(YN, ˜YN),
showing (6.499). Next, we have
Y [∞]
N
= R−1E,
YN = NE,
˜YN = ˜NE.
(6.502)
By Theorem 6.172 and by the transformation formula (3.27) for the dual compara-
tive index (with W := R−1, Z := R N, ˆZ := R ˜N), we get
mR[N, ∞) −7
mR[N, ∞)
(6.492)
=
μ∗(YN, Y [∞]
N
) −μ∗( ˜YN, Y [∞]
N
)
(6.502)
=
μ∗(R−1R NE, R−1E) −μ∗(R−1R ˜NE, R−1E)
(3.27)
=
μ∗(R−1R NE, R−1R ˜NE) −μ∗(R NE, R ˜NE)
(6.501), (6.502)
=
μ∗(YN, ˜YN) −μ∗(−JD∞, −J ˜D∞)
= μ∗(YN, ˜YN) −μ∗(JD∞, J ˜D∞),

556
6
Miscellaneous Topics on Symplectic Systems
showing (6.500), where in the last step we used that μ∗(−Y, −˜Y) = μ∗(Y, ˜Y ) (see
Theorem 3.5(i) with C1 = C2 = −I). Next we combine the above formulas (6.499)
and (6.500) with the formulas on the bounded interval in Proposition 6.166 to get
mL(M, ∞] −7
mL(M, ∞] = mL(M, N] + mL(N, ∞] −7mL(M, N] −7
mL(N, ∞]
(6.475), (6.499)
=
μ(JD∞, J ˜D∞) −μ(YM, ˜YM),
mR[M, ∞) −7
mR[M, ∞) = mR[M, N) + mR[N, ∞) −7
mR[M, N) −7
mR[N, ∞)
(6.476), (6.500)
=
μ∗(YM, ˜YM) −μ∗(JD∞, J ˜D∞).
Therefore, we proved (6.496) and (6.497). Finally, since the index N is chosen so
that mL(N, ∞) = 0 and mR[N, ∞) = 0, it follows that mL(M, N] = mL(M, ∞)
and mR[M, N) = mR[M, ∞) and rank XN = rankG, where G is the genus of Y
near ∞. By (6.477) in Proposition 6.166, we then get
mL(M, ∞) + rank G = mR[M, ∞) + rankXM,
which upon substituting for rankG
= mL(∞) + rankw(Y [∞], Y) from for-
mula (6.487) in Theorem 6.170 yields the required equation (6.498).
⊓⊔
The result in Theorem 4.39, or equivalently the limiting case of (6.475)
and (6.476) as N →∞, implies the equalities
mL(M, ∞) −7
mL(M, ∞) = μ∞(Y, ˜Y ) −μ(YM, ˜YM),
(6.503)
mR[M, ∞) −7
mR[M, ∞) = μ∗(YM, ˜YM) −μ∗
∞(Y, ˜Y ),
(6.504)
where the numbers μ∞(Y, ˜Y ) and μ∗
∞(Y, ˜Y) are deﬁned, respectively, as the limits
μ∞(Y, ˜Y ) := lim
k→∞μ(Yk, ˜Yk),
μ∗
∞(Y, ˜Y ) := lim
k→∞μ∗(Yk, ˜Yk);
(6.505)
see also (4.91) and (4.93). In other words, μ∞(Y, ˜Y ) and μ∗
∞(Y, ˜Y ) are deﬁned
by equations (6.503) and (6.504), since the quantities mL(M, ∞), 7mL(M, ∞) and
mR[M, ∞), 7
mR[M, ∞) are ﬁnite for a nonoscillatory system (SDS) at ∞. The
results in Theorem 6.173 then allow to calculate these numbers explicitly as values
of the comparative index, which was not possible by the methods of Sect. 4.2.4.
Corollary 6.174 Assume that system (SDS) is nonoscillatory at ∞. Then for any
conjoined bases Y and ˜Y of (SDS), the limits in (6.505) satisfy
μ∞(Y, ˜Y ) = μ(JD∞, J ˜D∞) −mL(∞) + 7
mL(∞),
(6.506)
μ∗
∞(Y, ˜Y ) = μ∗(JD∞, J ˜D∞),
(6.507)

6.4
Singular Sturmian Separation Theorems
557
where D∞and ˜D∞are the constant matrices in (6.467) corresponding to Y and ˜Y,
respectively.
Proof We use identities (6.503) and (6.504) to derive
μ∞(Y, ˜Y) (6.503)
=
mL(M, ∞) −7
mL(M, ∞) + μ(YM, ˜YM)
= mL(M, ∞] −mL(∞) −7
mL(M, ∞] + 7
mL(∞) + μ(YM, ˜YM)
(6.496)
=
μ(JD∞, J ˜D∞) −mL(∞) + 7
mL(∞),
μ∗
∞(Y, ˜Y) (6.504)
=
μ∗(YM, ˜YM) −mR[M, ∞) + 7
mR[M, ∞) (6.497)
=
μ∗(JD∞, J ˜D∞),
which shows the statement.
⊓⊔
We also note that the formulas in (6.506) and (6.507) are new even for
a controllable system (SDS) near ∞.
Remark 6.175 Given a conjoined basis Y of (SDS), equation (6.506) allows to
connect the limit μ∞(Y, Y [∞]) with the multiplicity mL(∞) and with the rank of
the associated matrix T . Namely, by taking ˜Y := Y [∞], we get ˜D∞= D[∞]
∞
= JE
(see the proof of Theorem 6.173), and then the property μ(Y, −E) = 0 implies
that μ(JD∞, JD[∞]
∞) = μ(JD∞, −E) = 0. Consequently, equation (6.506) and
Remark 6.169 yield the formula
μ∞(Y, Y [∞])
(6.506)
=
n −d∞−mL(∞)
(6.484)
=
rankT.
(6.508)
The results in Theorem 6.173 allow to present exact formulas for the numbers
of focal points of a given conjoined basis Y of (SDS) in terms of the corresponding
numbers of focal points of the the minimal recessive solution Y [∞] of (SDS) at ∞
and of the principal solution Y [M].
Corollary 6.176 Assume that system (SDS) is nonoscillatory at ∞. Then for any
conjoined basis Y of (SDS), we have the equalities
mL(M, ∞] = m[M]
L (M, ∞] + μ

JD∞, JD[M]
∞

,
(6.509)
mL(M, ∞] = m[∞]
L
(M, ∞] −μ(YM, Y [∞]
M ),
(6.510)
mR[M, ∞) = m[M]
R [M, ∞) −μ∗JD∞, JD[M]
∞
,
(6.511)
mR[M, ∞) = m[∞]
R [M, ∞) + μ∗(YM, Y [∞]
M ),
(6.512)
where D∞and D[M]
∞
are the constant matrices in (6.467) corresponding to Y and
Y [M], respectively.
Proof For (6.509) and (6.511), we apply (6.496) and (6.497) with ˜Y := Y [M]. In
this case ˜D∞= D[M]
∞
and μ(YM, E) = 0 = μ∗(YM, E). For (6.510) and (6.512),

558
6
Miscellaneous Topics on Symplectic Systems
we apply (6.496) and (6.497) with ˜Y := Y [∞] and ˜D∞= D[∞]
∞
= JE. In this case
μ(JD∞, −E) = 0 = μ∗(JD∞, −E).
⊓⊔
In the following result, we relate the numbers of forward and backward focal
points of the minimal recessive solution Y [∞] at ∞, resp., of the principal solution
Y [M], according to equalities (6.498), (6.509), and (6.512).
Corollary 6.177 Assume that system (SDS) is nonoscillatory at ∞. Then
m[∞]
L
(M, ∞] = m[∞]
R [M, ∞) + rank X[∞]
M ,
(6.513)
m[M]
R [M, ∞) = m[M]
L (M, ∞] + rankX[∞]
M ,
(6.514)
m[∞]
L
(M, ∞] = m[M]
L (M, ∞] + rankX[∞]
M ,
(6.515)
m[M]
R [M, ∞) = m[∞]
R [M, ∞) + rank X[∞]
M .
(6.516)
Proof For (6.513) we apply (6.498) with Y
:= Y [∞], while for (6.514) we
apply (6.498) with Y := Y [M], where we utilize the fact that w(Y [∞], Y [M]) =
(X[∞]
M )T . Next, for (6.515) we use (6.509) with Y := Y [∞] and D∞= D[∞]
∞
= JE.
Since by (6.467) the upper block of JD[M]
∞
is equal to w(Y [∞], Y [M]) = (X[∞]
M )T ,
it follows by μ(E, Y) = rank X (see Remark 3.4(iii)) that μ

JD∞, JD[M]
∞

=
μ(−E, JD[M]
∞)
=
rank X[∞]
M
and hence, (6.509) implies (6.515). Finally,
for (6.516) we use (6.512) with Y := Y [M], where μ∗(E, Y [∞]
M ) = rankX[∞]
M
according to the property μ∗(E, Y) = rank X.
⊓⊔
In the next result, we present a complete singular version of Proposition 6.167.
Theorem 6.178 (Singular Sturmian Separation Theorem)
Assume that sys-
tem (SDS) is nonoscillatory at ∞. Then for the minimal recessive solution Y [∞]
at ∞and for the principal solution Y [M], we have the identities
m[M]
L (M, ∞] = m[∞]
R [M, ∞),
m[M]
R [M, ∞) = m[∞]
L
(M, ∞].
(6.517)
For any conjoined basis Y of (SDS), we have the estimates
m[M]
L (M, ∞] ≤mL(M, ∞] ≤m[∞]
L
(M, ∞],
(6.518)
m[∞]
R [M, ∞) ≤mR[M, ∞) ≤m[M]
R [M, ∞).
(6.519)
For any conjoined bases Y and ˜Y of (SDS), we have the estimates
		 mL(M, ∞] −7
mL(M, ∞]
		 ≤rankX[∞]
M
≤n,
(6.520)
		 mR[M, ∞) −7
mR[M, ∞)
		 ≤rankX[∞]
M
≤n,
(6.521)
		 mL(M, ∞] −7
mR[M, ∞)
		 ≤rankX[∞]
M
≤n.
(6.522)

6.4
Singular Sturmian Separation Theorems
559
Proof The equalities in (6.517) follow by subtracting (6.514) and (6.516), respec-
tively, by subtracting (6.514) and (6.515). Next, the comparative index and the
dual comparative index are nonnegative, so that estimate (6.518) is a consequence
of (6.509) and (6.510). Similarly, estimate (6.519) follows from (6.512) and (6.511).
Finally, the lower and upper bounds in (6.518) and (6.519) differ by the same
number rank X[∞]
M
(by Corollary 6.177). Therefore, the inequalities in (6.520)–
(6.522) follow from the estimates in (6.518) and (6.519).
⊓⊔
We note that the lower and upper bounds in (6.518)–(6.522)are optimal in a sense
that they cannot be improved by better bounds, which would be independent on
the arbitrarily chosen conjoined bases Y and ˜Y. In this respect the two quantities
in (6.517) together with the number rank X[∞]
M
represent important parameters or
characteristics of the symplectic system (SDS) on the given unbounded interval
[M, ∞)Z.
In the remaining part of this subsection, we will study the multiplicities of focal
points of conjoined bases of (SDS) in the open interval (M, ∞). The above problem
is closely related with limiting the inequalities in Proposition 6.167 for N →∞. In
particular, we will see that by taking N →∞in Proposition 6.167, we do not obtain
the statements in Theorem 6.178. First we consider the upper bound in (6.479). We
have
lim
N→∞m[N]
L (M, N]
(6.478)
=
lim
N→∞m[M]
R [M, N)
(6.491)
=
m[M]
R [M, ∞)
(6.517)
=
m[∞]
L
(M, ∞],
(6.523)
which is the correct upper bound obtained in (6.518). Another expression of this
limit can be obtained as
lim
N→∞m[N]
L (M, N] = lim
N→∞
!m[M]
L (M, N] + rank X[M]
N
"
(6.491)
=
m[M]
L (M, ∞) + rank G[M],
(6.524)
where the ﬁrst equality follows from (6.475) with ˜Y := Y [M] and where G[M] is the
genus of Y [M] near ∞. On the other hand, by considering the lower bound in (6.480),
we get
lim
N→∞m[N]
R [M, N)
(6.478)
=
lim
N→∞m[M]
L (M, N]
(6.491)
=
m[M]
L (M, ∞)
= m[M]
L (M, ∞] −m[M]
L (∞)
(6.517)
=
m[∞]
R
[M, ∞) −m[M]
L (∞),
which is in general smaller than the optimal lower bound in (6.519). Moreover, it is
equal to the optimal lower bound m[∞]
R
[M, ∞) if and only if m[M]
L (∞) = 0, i.e., if
and only if the principal solution Y [M] is a dominant solution of (SDS) at inﬁnity

560
6
Miscellaneous Topics on Symplectic Systems
(by Remark 6.169(ii)). Altogether, limiting the inequalities in (6.480) for N →∞,
we obtain the estimate
m[∞]
R
[M, ∞) −m[M]
L (∞) ≤mR[M, ∞) ≤m[M]
R [M, ∞),
which contains a nonoptimal lower bound for the number of backward focal points
of Y in [M, ∞), while limiting the inequalities in (6.479) for N →∞, we obtain
a result counting the forward focal points of Y in the open interval (M, ∞).
Corollary 6.179 Assume that system (SDS) is nonoscillatory at ∞. Then for any
conjoined basis Y of (SDS), we have the estimates
m[M]
L (M, ∞) ≤mL(M, ∞) ≤m[∞]
L
(M, ∞] = m[M]
L (M, ∞) + rankG[M],
(6.525)
where G[M] is the genus of the principal solution Y [M] of (SDS) near ∞.
Proof The statement follows from (6.479) and from the calculations in (6.523)
and (6.524).
⊓⊔
Next we derive an exact relationship between the numbers of forward and
backward focal points of Y [∞] and Y [M] in the open interval (M, ∞).
Corollary 6.180 Assume that system (SDS) is nonoscillatory at ∞. Then
m[M]
L (M, ∞) + rank G[M] = m[∞]
L
(M, ∞) + n −d∞,
(6.526)
m[M]
L (M, ∞) = m[∞]
L
(M, ∞) ⇔rank G[M] = n −d∞⇔Y [M] ∈Gmin,
(6.527)
where G[M] and Gmin are the genera of conjoined bases of (SDS) near ∞
corresponding to Y [M] and Y [∞]. If in addition system (SDS) is controllable near
∞, then we have the equality
m[M]
L (M, ∞) = m[∞]
L
(M, ∞).
(6.528)
Proof Equation (6.526) is a reformulation of (6.515), since m[∞]
L
(∞) = n −d∞
by Remark 6.169(ii) and m[M]
L (∞) = rankG[M] −rank w(Y [∞], Y [M]), where
w(Y [∞], Y [M]) = (X[∞]
M )T . The equivalences in (6.527) then follow from equa-
tion (6.526). If in addition the system (SDS) is controllable near ∞, then d∞= 0
and rank Gmin = n = rankG[M]. In this case we obtain from (6.527) that (6.528) is
indeed satisﬁed.
⊓⊔
As the last result in this subsection, we present limit properties of the comparative
index and the dual comparative index involving a given conjoined basis Y and the
principal solution Y [k] for k →∞. Namely, we show that these limits are related to
the multiplicity mL(∞) and to the maximal order of abnormality d∞.

6.4
Singular Sturmian Separation Theorems
561
Theorem 6.181 Assume that system (SDS) is nonoscillatory at ∞. Then for
any conjoined basis Y of (SDS), the comparative indices μ(JD∞, JD[k]
∞) and
μ∗(JD∞, JD[k]
∞) have limits for k →∞, which satisfy
lim
k→∞μ(JD∞, JD[k]
∞) = mL(∞),
(6.529)
lim
k→∞μ∗(JD∞, JD[k]
∞) = n −d∞,
(6.530)
where D∞and D[k]
∞are the constant matrices in (6.467) corresponding to Y and
Y [k], respectively.
Proof For any k ∈[0, ∞)Z, we have by Corollary 6.176 and Theorem 6.178 on the
interval [k, ∞)Z that
μ(JD∞, JD[k]
∞)
(6.509)
=
mL(k, ∞] −m[k]
L (k, ∞]
(6.517)
=
mL(k, ∞) + mL(∞) −m[∞]
R [k, ∞).
(6.531)
Since system (SDS) is nonoscillatory at ∞, the conjoined bases Y and Y [∞] have
for large k no forward and backward focal points in the intervals (k, ∞) and [k, ∞),
i.e., mL(k, ∞) = 0 = m[∞]
R [k, ∞) for large k. Therefore, equality (6.529) follows
from (6.531). Similarly, we have
μ∗(JD∞, JD[k]
∞)
(6.511)
=
m[k]
R [k, ∞) −mR[k, ∞)
(6.517)
=
m[∞]
L
(k, ∞) + m[∞]
L
(∞) −mR[k, ∞).
(6.532)
And since m[∞]
L
(k, ∞) = 0 = mR[k, ∞) for large k and m[∞]
L
(∞) = n −d∞, we
obtain equality (6.530) from taking the limit k →∞in (6.532).
⊓⊔
It is interesting to realize that the value of the limit in (6.530) does not depend on
the chosen conjoined basis Y.
6.4.3
Singular Separation Theorems II
In this subsection we present singular separation theorems for system (SDS) on
unbounded intervals involving −∞, i.e., with the left singular endpoint. These
intervals will be either bounded from above or unbounded from above. We will
present results, which are in some sense analogous to those in Sect. 6.4.3.
The maximal order of abnormality of (SDS) near −∞is deﬁned by
d−∞:=
lim
k→−∞d(−∞, k]Z,
0 ≤d−∞≤n,

562
6
Miscellaneous Topics on Symplectic Systems
where d(−∞, N]Z is the order of abnormality of system (SDS) on the interval
(−∞, N]Z. The nonoscillation and eventual controllability of (SDS) near −∞then
imply that d−∞= 0 and that every conjoined basis Y of (SDS) has the matrix Xk
invertible for all negative k large enough.
If (SDS) is nonoscillatory at −∞, then we denote by Y [−∞] the (unique) minimal
recessive solution of (SDS) at −∞(by an analog with Theorem 6.115 and the
notation in (6.471)). For completeness we note that a conjoined basis Y of (SDS) is
a recessive solution of (SDS) at −∞if there exists N ∈(−∞, −1]Z such that Y has
constant kernel on (−∞, N + 1]Z and no backward focal points in (−∞, N + 1)
and the corresponding negative semideﬁnite matrix T−∞deﬁned by
T−∞:=
lim
k→−∞(S−∞
k
)†,
S−∞
k
:= −
N

j=k+1
X†
j BT
j X† T
j+1,
k ∈(−∞, N]Z,
(6.533)
with S−∞
N
= 0 satisﬁes T−∞= 0; compare with [300, Deﬁnition 4.2] in the
controllable case near −∞. Similarly, a conjoined basis Y of (SDS) is a dominant
solution of (SDS) at −∞if there exists N ∈(−∞, −1]Z such that
d(−∞, N + 1]Z = d−∞
(6.534)
holds, the conjoined basis Y has constant kernel on (−∞, N +1]Z and no backward
focal points in (−∞, N + 1), and the corresponding matrix T−∞deﬁned in (6.237)
satisﬁes rankT−∞= n −d−∞(i.e., the rank of T−∞is maximal).
Remark 6.182 In order to distinguish the notation for possibly different genera of
conjoined bases of (SDS) near ∞and −∞, we will denote the genera near ∞with
the upper or lower index + and the genera near −∞with upper or lower index −.
That is, G+ will denote a genus near ∞(with G+
min and G+
max being the minimal and
maximal genus near ∞), while G−will denote a genus near −∞(with G−
min and
G−
max being the minimal and maximal genus near −∞).
In view of Lemma 6.59 with j = −∞, every conjoined basis Y of (SDS) can be
uniquely represented by a constant 2n × n matrix D−∞such that
Yk = [−∞]
k
D−∞,
k ∈IZ,
JD−∞=
w(Y [−∞], Y)
w( ¯Y [−∞], Y)

,
(6.535)
where [−∞]
k
= (Y [−∞]
k
, ¯Y [−∞]
k
) is the symplectic fundamental matrix of (SDS)
deﬁned by (6.228) with j = −∞and where IZ is the unbounded interval (−∞, 0]Z
or (−∞, ∞)Z = Z.
In the rest of this subsection, we assume that system (SDS) is nonoscillatory at
−∞. In analogous way to Deﬁnition 6.168, we introduce the multiplicity mR(−∞)
as follows.

6.4
Singular Sturmian Separation Theorems
563
Deﬁnition 6.183 For a conjoined basis Y of (SDS), we deﬁne the quantity
mR(−∞) := n −d−∞−rank T−∞= def T−∞−d−∞,
(6.536)
where T−∞is the matrix in (6.533) associated with Y on (−∞, N]Z, which
satisﬁes (6.534). Moreover, we say that Y has a focal point at −∞if mR(−∞) ≥1
and then mR(−∞) is called its multiplicity.
Following (6.491) and (4.89), we adopt for any conjoined basis Y of (SDS) and
any N ∈(−∞, 0]Z the notation
mR[−∞, N) := mR(−∞, N) + mR(−∞),
mR(−∞, N) :=
N−1

k=−∞
mR[k, k + 1),
mL(−∞, N] := l(Y, −∞, N) =
N−1

k=−∞
mL(k, k + 1].
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
(6.537)
For intervals with two singular endpoints, we denote in a similar way
mL(−∞, ∞] := mL(−∞, ∞) + mL(∞),
mL(−∞, ∞) :=
∞

k=−∞
mL(k, k + 1],
mR[−∞, ∞) := mR(−∞, ∞) + mR(−∞),
mR(−∞, ∞) :=
∞

k=−∞
mR[k, k + 1).
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(6.538)
Similar notation will be used for another conjoined basis ˜Y, for the principal solution
Y [N], and for the minimal recessive solution Y [−∞] at −∞. Then we have the
following results.
Theorem 6.184 Assume that system (SDS) is nonoscillatory at −∞, and let Y,
belonging to a genus G−, be a conjoined basis of (SDS) with constant kernel on
(−∞, N + 1]Z and no backward focal points in (−∞, N + 1), where the index
N ∈(−∞, −1]Z is such that (6.534) holds. Then
Im [w(Y [−∞], Y)]T = Im T−∞⊕Im (P −PS−∞),
(6.539)
rankT−∞= rankw(Y [−∞], Y) −rank G−+ n −d−∞,
(6.540)
mR(−∞) = rankG−−rank w(Y [−∞], Y).
(6.541)
Proof The results are proven by analogy with Theorem 6.170.
⊓⊔

564
6
Miscellaneous Topics on Symplectic Systems
Theorem 6.185 (Singular Sturmian Separation Theorem) If (SDS) is nonoscil-
latory at −∞,, then for any conjoined bases Y and ˜Y of (SDS), we have
mL(−∞, N] −7
mL(−∞, N] = μ(YN, ˜YN) −μ(JD−∞, J ˜D−∞),
(6.542)
mR[−∞, N) −7
mR[−∞, N) = μ∗(JD−∞, J ˜D−∞) −μ∗(YN, ˜YN),
(6.543)
mL(−∞, N] + rank XN = mR[−∞, N) + rankw(Y [−∞], Y),
(6.544)
where D−∞and ˜D−∞are the constant matrices in (6.535) corresponding to Y and
˜Y, respectively. If (SDS) is nonoscillatory at ±∞, then for any conjoined bases Y
and ˜Y of (SDS), we have the equalities
mL(−∞, ∞] −7
mL(−∞, ∞] = μ(JD∞, J ˜D∞) −μ(JD−∞, J ˜D−∞),
(6.545)
mR[−∞, ∞)−7
mR[−∞, ∞) = μ∗(JD−∞, J ˜D−∞)−μ∗(JD∞, J ˜D∞),
(6.546)
mL(−∞, ∞]+rankw(Y [∞], Y) = mR[−∞, ∞)+rankw(Y [−∞], Y),
(6.547)
where D±∞and ˜D±∞are the constant matrices in (6.467) and (6.535) correspond-
ing to Y and ˜Y, respectively.
Proof Identities (6.542)–(6.544) are proven by analogy with Theorem 6.173.
Identities (6.545)–(6.547) follow by adding the corresponding identities in (6.496)–
(6.498) and (6.542)–(6.544).
⊓⊔
Following (6.506)–(6.507), Theorem 4.39, and Theorems 4.59 and 4.61 (for one
system (SDS)), we consider the equalities
mL(−∞, N] −7mL(−∞, N] = μ(YN, ˜YN) −μ−∞(Y, ˜Y ),
(6.548)
mR(−∞, N) −7
mR(−∞, N) = μ∗
−∞(Y, ˜Y ) −μ∗(YN, ˜YN),
(6.549)
as the limiting case of (6.475)–(6.476) when the left endpoint approaches −∞,
where the numbers μ−∞(Y, ˜Y ) and μ∗
−∞(Y, ˜Y ) are deﬁned as the limits
μ−∞(Y, ˜Y ) :=
lim
k→−∞μ(Yk, ˜Yk),
μ∗
−∞(Y, ˜Y ) :=
lim
k→−∞μ∗(Yk, ˜Yk).
(6.550)
When considering system (SDS) with two singular endpoints, we also have
mL(−∞, ∞) −7
mL(−∞, ∞) = μ∞(Y, ˜Y ) −μ−∞(Y, ˜Y ),
mR(−∞, ∞) −7mR(−∞, ∞) = μ∗
−∞(Y, ˜Y ) −μ∗
∞(Y, ˜Y ),

6.4
Singular Sturmian Separation Theorems
565
as a sum of (6.503) and (6.548), respectively, as a sum of (6.504) and (6.549). Then
by Theorem 6.185, we can calculate the limits in (6.550) explicitly as values of
the comparative index, which could not be done by the methods in Sects. 4.2.4
and 4.3.3.
Corollary 6.186 Assume that system (SDS) is nonoscillatory at −∞. Then for any
conjoined bases Y and ˜Y of (SDS), the limits in (6.550) satisfy
μ−∞(Y, ˜Y ) = μ(JD−∞, J ˜D−∞),
(6.551)
μ∗
−∞(Y, ˜Y ) = μ∗(JD−∞, J ˜D−∞) −mR(−∞) + 7
mR(−∞),
(6.552)
where D−∞and ˜D−∞are the constant matrices in (6.535) corresponding to Y and
˜Y, respectively. Moreover,
μ∗
−∞(Y, Y [−∞]) = rank T−∞.
(6.553)
Proof The statements in (6.551) and (6.552) follow from Theorem 6.185; compare
also with the proof of Corollary 6.174. Equality (6.553) then follows from (6.552)
with ˜Y := Y [−∞]; compare with formula (6.508) in Remark 6.175.
⊓⊔
Based on Theorem 6.185, we derive exact formulas for the numbers of focal
points of a given conjoined basis Y of (SDS) in terms of Y [−∞] and Y [N],
respectively, in terms of Y [−∞] and Y [∞].
Corollary 6.187 If (SDS) is nonoscillatory at −∞, then for any conjoined basis Y
of (SDS), we have
mL(−∞, N] = m[N]
L (−∞, N] −μ

JD−∞, JD[N]
−∞

,
(6.554)
mL(−∞, N] = m[−∞]
L
(−∞, N] + μ(YN, Y [−∞]
N
),
(6.555)
mR[−∞, N) = m[N]
R [−∞, N) + μ∗JD−∞, JD[N]
−∞
,
(6.556)
mR[−∞, N) = m[−∞]
R
[−∞, N) −μ∗(YN, Y [−∞]
N
),
(6.557)
where D−∞and D[N]
−∞are the constant matrices in (6.535) corresponding to Y and
Y [N], respectively. If (SDS) is nonoscillatory at ±∞, then for any conjoined basis
Y of (SDS), we have
mL(−∞, ∞] = m[−∞]
L
(−∞, ∞] + μ

JD∞, JD[−∞]
∞

,
(6.558)
mL(−∞, ∞] = m[∞]
L
(−∞, ∞] −μ

JD−∞, JD[∞]
−∞

,
(6.559)
mR[−∞, ∞) = m[−∞]
R
[−∞, ∞) −μ∗JD∞, JD[−∞]
∞
,
(6.560)
mR[−∞, ∞) = m[∞]
R
[−∞, ∞) + μ∗
JD−∞, JD[∞]
−∞

,
(6.561)

566
6
Miscellaneous Topics on Symplectic Systems
where D∞and D[−∞]
∞
are the constant matrices in (6.467) corresponding to Y
and Y [−∞], respectively, and D−∞and D[∞]
−∞are the constant matrices in (6.535)
corresponding to Y and Y [∞], respectively.
Proof Equalities (6.554) and (6.556) follow from (6.542) and (6.543) with the
choice ˜Y
:= Y [N], while equalities (6.555) and (6.557) follow from (6.542)
and (6.543) with the choice ˜Y := Y [−∞]. Equalities (6.558) and (6.560) follow
from (6.545) and (6.546) with the choice ˜Y := Y [−∞], while equalities (6.559)
and (6.561) follow from (6.545) and (6.546) with ˜Y := Y [∞].
⊓⊔
Next we present the relationship between the numbers of forward and backward
focal points of Y [N], Y [−∞], and Y [∞].
Corollary 6.188 If (SDS) is nonoscillatory at −∞, then
m[−∞]
R
[−∞, N) = m[−∞]
L
(−∞, N] + rank X[−∞]
N
,
(6.562)
m[N]
L (−∞, N] = m[N]
R [−∞, N) + rank X[−∞]
N
,
(6.563)
m[N]
L (−∞, N] = m[−∞]
L
(−∞, N] + rank X[−∞]
N
,
(6.564)
m[−∞]
R
[−∞, N) = m[N]
R [−∞, N) + rank X[−∞]
N
.
(6.565)
If (SDS) is nonoscillatory at ±∞, then
m[−∞]
R
[−∞, ∞) = m[−∞]
L
(−∞, ∞] + rank w(Y [∞], Y [−∞]),
(6.566)
m[∞]
L
(−∞, ∞] = m[∞]
R [−∞, ∞) + rank w(Y [−∞], Y [∞]),
(6.567)
m[∞]
L
(−∞, ∞] = m[−∞]
L
(−∞, ∞] + rank w(Y [∞], Y [−∞]),
(6.568)
m[−∞]
R
[−∞, ∞) = m[∞]
R [−∞, ∞) + rank w(Y [−∞], Y [∞]).
(6.569)
Proof Equations (6.562) and (6.566) follow from (6.544) and (6.547) with the
choice Y := Y [−∞]. Equation (6.563) follows from (6.544) with Y := Y [N],
while (6.567) follows from (6.547) with Y
:= Y [∞]. Next, equations (6.564)
and (6.565) follow from (6.555) and (6.557) with the choice Y := Y [N] or (6.554)
and (6.556) with the choice Y := Y [−∞], while equations (6.568) and (6.569) follow
from (6.558) and (6.560) with Y := Y [∞] or from (6.559) and (6.561) with the
choice Y := Y [−∞].
⊓⊔
The following two results are analogs of Theorem 6.178 for the singular endpoint
of (SDS) at −∞or for two singular endpoints of (SDS).
Theorem 6.189 (Singular Sturmian Separation Theorem) If (SDS) is nonoscil-
latory at −∞, then
m[−∞]
L
(−∞, N] = m[N]
R [−∞, N),
m[−∞]
R
[−∞, N) = m[N]
L (−∞, N].
(6.570)

6.4
Singular Sturmian Separation Theorems
567
Moreover, for any conjoined basis Y of (SDS), we have the estimates
m[−∞]
L
(−∞, N] ≤mL(−∞, N] ≤m[N]
L (−∞, N],
(6.571)
m[N]
R [−∞, N) ≤mR[−∞, N) ≤m[−∞]
R
[−∞, N),
(6.572)
and for any conjoined bases Y and ˜Y of (SDS), we have the estimates
		 mL(−∞, N] −7
mL(−∞, N]
		 ≤rank X[−∞]
N
≤n,
(6.573)
		 mR[−∞, N) −7
mR[−∞, N)
		 ≤rank X[−∞]
N
≤n,
(6.574)
		 mL(−∞, N] −7
mR[−∞, N)
		 ≤rank X[−∞]
N
≤n.
(6.575)
Proof The equalities in (6.570) follow from (6.562) in combination with (6.565)
and (6.564). The estimates in (6.571) follow from (6.555) and (6.554), while
the estimates in (6.572) follow from (6.556) and (6.557). Finally, the estimates
in (6.573)–(6.575) follow from (6.571)–(6.572) by using (6.570).
⊓⊔
Theorem 6.190 (Singular Sturmian Separation Theorem) If (SDS) is nonoscil-
latory at ±∞, then
m[−∞]
L
(−∞, ∞] = m[∞]
R
[−∞, ∞),
m[−∞]
R
[−∞, ∞) = m[∞]
L
(−∞, ∞].
(6.576)
Moreover, for any conjoined basis Y of (SDS), we have the estimates
m[−∞]
L
(−∞, ∞] ≤mL(−∞, ∞] ≤m[∞]
L
(−∞, ∞],
(6.577)
m[∞]
R [−∞, ∞) ≤mR[−∞, ∞) ≤m[−∞]
R
[−∞, ∞),
(6.578)
and for any conjoined bases Y and ˜Y of (SDS), we have the estimates
		 mL(−∞, ∞] −7
mL(−∞, ∞]
		 ≤rank w(Y [∞], Y [−∞]) ≤n,
(6.579)
		 mR[−∞, ∞) −7
mR[−∞, ∞)
		 ≤rank w(Y [∞], Y [−∞]) ≤n,
(6.580)
		 mL(−∞, ∞] −7
mR[−∞, ∞)
		 ≤rank w(Y [∞], Y [−∞]) ≤n.
(6.581)
Proof The equalities in (6.576) follow from (6.566) in combination with (6.569)
and (6.568). The estimates in (6.577) follow from (6.558) and (6.559), while
the estimates in (6.578) follow from (6.561) and (6.560). Finally, the estimates
in (6.579)–(6.581) follow from (6.577)–(6.578) by using (6.576).
⊓⊔
We note that the lower and upper bounds in Theorems 6.189 and 6.190 are
optimal in a sense that they cannot be improved by better bounds, which would
be independent on the arbitrarily chosen conjoined bases Y and ˜Y.
In the remaining part of this section, we will present the results regarding the
multiplicities of focal points of conjoined bases of (SDS) in the open intervals

568
6
Miscellaneous Topics on Symplectic Systems
(−∞, N) or (−∞, ∞). Similarly to Corollaries 6.179 and 6.180, we obtain the
following.
Corollary 6.191 If (SDS) is nonoscillatory at −∞, then for any conjoined basis Y
of (SDS), we have the estimates
m[N]
R (−∞, N) ≤mR(−∞, N) ≤m[−∞]
R
[−∞, N)
= m[N]
R (−∞, N) + rankG[N]
−,

(6.582)
where G[N]
−
is the genus of the principal solution Y [N] of (SDS) near −∞. Moreover,
we have
m[−∞]
R
(−∞, N) + n −d−∞= m[N]
R (−∞, N) + rankG[N]
−,
(6.583)
m[−∞]
R
(−∞, N) = m[N]
R (−∞, N) ⇔rankG[N]
−
= n −d−∞
⇔Y [N] ∈G−
min,

(6.584)
where G−
min is the genus of conjoined bases of (SDS) near −∞corresponding to
Y [−∞]. If in addition system (SDS) is controllable near −∞, then
m[−∞]
R
(−∞, N) = m[N]
R (−∞, N).
(6.585)
Corollary 6.192 If (SDS) is nonoscillatory at ±∞, then for any conjoined basis Y
of (SDS), we have the estimates
m[−∞]
L
(−∞, ∞) ≤mL(−∞, ∞) ≤m[∞]
L
(−∞, ∞]
= m[−∞]
L
(−∞, ∞) + rank G[−∞]
+
,

(6.586)
m[∞]
R
(−∞, ∞) ≤mR(−∞, ∞) ≤m[−∞]
R
[−∞, ∞)
= m[∞]
R
(−∞, ∞) + rankG[∞]
−
,

(6.587)
where G[−∞]
+
is the genus of Y [−∞] near ∞and G[∞]
−
is the genus of Y [∞] near
−∞. Moreover, we have
m[−∞]
L
(−∞, ∞) + rank G[−∞]
+
= m[∞]
L
(−∞, ∞) + n −d∞,
(6.588)
m[−∞]
R
(−∞, ∞) + n −d−∞= m[∞]
R
(−∞, ∞) + rankG[∞]
−
,
(6.589)
m[−∞]
L
(−∞, ∞) = m[∞]
L
(−∞, ∞) ⇔rankG[−∞]
+
= n −d∞
⇔Y [−∞] ∈G+
min,

(6.590)
m[−∞]
R
(−∞, ∞) = m[∞]
R (−∞, ∞) ⇔rankG[∞]
−
= n −d−∞
⇔Y [∞] ∈G−
min,

(6.591)

6.5
Notes and References
569
and consequently
m[−∞]
L
(−∞, ∞) + rankG[−∞]
+
= m[∞]
R
(−∞, ∞) + rankG[∞]
−
,
(6.592)
m[−∞]
R
(−∞, ∞) −d−∞= m[∞]
L
(−∞, ∞) −d∞,
(6.593)
m[−∞]
L
(−∞, ∞) = m[∞]
R (−∞, ∞) ⇔rank G[−∞]
+
= rankG[∞]
−
,
(6.594)
m[−∞]
R
(−∞, ∞) = m[∞]
L
(−∞, ∞) ⇔d−∞= d∞.
(6.595)
If in addition system (SDS) is controllable near ±∞, then
m[−∞]
L
(−∞, ∞) = m[−∞]
R
(−∞, ∞) = m[∞]
L
(−∞, ∞) = m[∞]
R (−∞, ∞).
(6.596)
In the ﬁnal result of this subsection, we present the limit properties of the
comparative index at −∞and its relation with the multiplicity mR(−∞) for
a conjoined basis Y and with the maximal order of abnormality d−∞. It represents
an analog of Theorem 6.181.
Theorem 6.193 Assume that system (SDS) is nonoscillatory at −∞. Then for
any conjoined basis Y of (SDS), the comparative indices μ(JD−∞, JD[k]
−∞) and
μ∗(JD−∞, JD[k]
−∞) have limits for k →−∞, which satisfy
lim
k→−∞μ(JD−∞, JD[k]
−∞) = n −d−∞,
(6.597)
lim
k→−∞μ∗(JD−∞, JD[k]
−∞) = mR(−∞),
(6.598)
where D−∞and D[k]
−∞are the constant matrices in (6.535) corresponding to Y and
Y [k], respectively.
Proof The proof is analogous to the proof of Theorem 6.181 by using formu-
las (6.554), (6.556), and (6.570). The details are therefore omitted.
⊓⊔
6.5
Notes and References
The results in Sect. 6.1.1 are based on [22, 314] and [315, Chapter 4]. In the paper
[21] the authors considered the case when equations (6.9), (6.10) differ also in
rk ̸= ˆrk. Note that all results in [21, 22, 315] are related to the case of the special
linear dependence on λ (see Sect. 5.3). The relative oscillation theorems for two
Sturm-Liouville equations with general nonlinear dependence on λ (see Sect. 1.2.5)
can be derived from [124, Theorems 3.8 and 3.9] (see Sect. 6.1.4). Moreover, in
[124, Subsection 3.2], we discussed connections between the results in [21] and
[124]. For differential Sturm-Liouville eigenvalue problems, the relative oscillation

570
6
Miscellaneous Topics on Symplectic Systems
theory is developed in [153, 213] (see also the references in these papers). The ﬁrst
results concerning the relative oscillation theory for discrete symplectic systems are
presented in [117, Section 3] and [118, 120, 121, 123, 124].
The results in Sect. 6.1.2 (see Theorems 6.2 and 6.4) are direct consequence of
the comparison results for focal points of the principal solutions of two symplectic
systems from [117, Corollary 2.4] (see Sect. 4.3.2). The consideration in Sect. 6.1.3
is based on algebraic properties of the comparative index for two symplectic
matrices under assumption (6.35) (see [117, Lemma 2.2] and Sect. 4.3.1, in
particular Lemma 4.43). The ﬁrst applications of the comparison results for this
special case to the spectral theory of symplectic systems with the special linear
dependence on λ (see Sect. 5.3) can be found in [117, Section 3] and [118]. In
particular, the results in Lemma 6.6 are from [118, Lemma 1] (note that the proofs
for the linear and nonlinear cases are the same). Formula (6.60) in Remark 6.10(ii)
concerning the number of eigenvalues in (−∞, b) (instead of (−∞, b]) was
derived in [118, Theorem 3, formula (3.7)]. In this connection we remark that
the renormalized version of (6.60), formula (6.62) (concerning the number of
eigenvalues of problem (E) in [a, b), compare with Theorem 6.9), is proved in [118,
Theorem 4, formula (3.11)]. Observe that the comments in Remark 6.10(ii) (as well
as [118, Theorem 4, formula (3.11)]) refer to the special linear dependence of on λ.
A generalization of (6.60) and (6.62) to the case of general nonlinear dependence on
λ demands introducing the notion of the so-called right ﬁnite eigenvalues, because
condition (5.274) is not in general satisﬁed.
Majority of the results in Sects. 6.1.4 and 6.1.5 are from the paper [124].
Note that from the results of Sect. 6.1.4, one can easily derive the results for the
scalar case n = 1 concerning a pair of Sturm-Liouville eigenvalue problems with
general nonlinear dependence on λ. The main results in Sects. 6.1.6 and 6.1.7 are
from [121, Chapter 4], where the consideration is restricted to the special linear
dependence on λ (see Sect. 5.3). Note, however, that all algebraic properties of the
comparative index used in the results of Sects. 6.1.6 and 6.1.7 remain the same
for the general nonlinear dependence on λ. Observe also, that the renormalized
oscillation theorems for separated and joint endpoints (see Theorems 6.23 and 6.29),
as well as Corollaries 6.25 and 6.31 and Remarks 6.26 and 6.32, for the case when
the boundary conditions do not depend on λ can be found in [120, Theorems 1
and 2] (for separated boundary conditions) and in [123, Theorems 5 and 6] (for
general boundary conditions).
The results of Sect. 6.2 belong to the classical research topic strongly developed
for differential Sturm-Liouville eigenvalue problems with some generalizations to
vector case, i.e., for differential matrix Sturm-Liouville eigenvalue problems and for
boundary value problems for linear Hamiltonian differential systems. Comparison
theorems for eigenvalues under some majorant conditions for scalar and vector
differential Sturm-Liouville equations can be found in [280, Chapters 1 and 3]
and in [205, Section 7.5]. For the discrete case, comparison results for eigenvalues
of scalar and vector Sturm-Liouville eigenvalue problems can be found in [262,
Theorem 3.21], [268, Theorem 3.7], and [267, Theorem 5.5]. See also the references
therein.

6.5
Notes and References
571
First comparison theorems for eigenvalues of discrete symplectic eigenvalue
problems with nonlinear dependence on λ were proven in [301, Theorems 1–3].
Similar results were proven in [121, Chapter 4], for the case of the special linear
dependence on λ (see Sect. 5.3). In Sect. 6.2.1 we present a modiﬁcation of the
results in [301] and [121, Chapter 4] for the general nonlinear dependence on λ
formulating majorant conditions for the symplectic matrices Sk(λ) and ˆSk(λ) and
for the matrices of the boundary conditions in terms of the comparative index and
using the relative oscillation theorems from Sect. 6.1 as the main tool for the proofs.
In Sects. 6.2.2 and 6.2.3, we present the comparative index theory as a new
tool for the proof of the interlacing properties of ﬁnite eigenvalues of symplectic
eigenvalue problems with nonlinear dependence on λ. For the scalar differen-
tial Sturm-Liouville eigenvalue problems, the classical interlacing properties of
eigenvalues are proven in [38] and [69, Chapter 8]; see also [37] and the ref-
erences in the latter paper. For the matrix case, these properties are proven in
[28, Theorem 10.5.1], [254, Theorem 2] (for matrix Sturm-Liouville eigenvalue
problems), in [28, Theorem 10.9.1], [156, Chapter 4] (for linear Hamiltonian
differential systems with general linear dependence on λ), and in [205, Section 7.5].
For geometrical interpretation of interlacing properties, we refer to [26]. For the
discrete case, results concerning interlacing properties of zeros for orthogonal
polynomials for scalar and matrix case can be found in [28, Chapters 4 and 6]
(see Theorems 4.3.2, 4.3.3, and 6.7.7 therein). Different results concerning the
interlacing properties of eigenvalues for discrete Sturm-Liouville problems can be
found in [149, 322] and the references therein.
For the discrete symplectic systems with special linear dependence on λ (see
Sect. 5.3), some interlacing properties of ﬁnite eigenvalues are proved in [120] (for
separated boundary conditions) and in [123] (for joint endpoints). These results were
generalized in [121, Chapter 4], but the consideration was restricted to the case of
the special linear dependence on λ.
Majority of the results in Sect. 6.3 about recessive and dominant solutions
of (SDS) at ∞
in the possibly uncontrollable case are derived from [284] and
[290]. More precisely, most of Sects. 6.3.1–6.3.6 are from [284]. The properties
of the important auxiliary conjoined basis ¯Y in Proposition 6.67 are collected from
several sources: parts (i)–(v) from [284, Proposition 4.3 and Remark 4.5]; parts
(vi)–(viii) from [290, Proposition 2.6]; and parts (ix)–(xi) are proven as a discrete
version of [281, Theorem 2.2.11]. The proof of Lemma 6.100 is a discrete version
of the proof of [288, Lemma 1]. The proofs of Theorems 6.69, 6.73, 6.75, 6.97(ii)–
(iii) and of Propositions 6.81 and 6.105 were not discussed in details in [284].
In the presented form, they are discrete analogs of the proofs of [283, Theo-
rems 4.6, 4.10, 5.2, 5.7, 5.15, 5.17, 6.9]. The proof of Theorem 6.84 is a discrete ana-
log of the proof of [285, Theorem 4.13]. Similarly, the proofs of Propositions 6.92
and 6.99 and Theorems 6.106 and 6.107 were not discussed in details in [290].
Their presented versions are discrete analogs of the proofs of [285, Theorem 6.7
and Lemma 6.9] and [286, Theorems 4.4 and 4.9]. Moreover, Lemma 6.91 is the
discrete version of [285, Lemma 6.6]. The concept of representable conjoined bases
in Deﬁnition 6.79 and Theorem 6.80 is a discrete version of [281, Theorem 2.3.3].

572
6
Miscellaneous Topics on Symplectic Systems
The statement in Theorem 6.153 extends the Reid construction of the recessive
solution of (SDS) at ∞known for the controllable case in [81, Section 4(iii)]. The
corresponding result for controllable linear Hamiltonian differential systems (1.103)
can be found in [70, pg. 44] and [248, Theorem VII.3.4], while for general
abnormal linear Hamiltonian systems, we refer to [288, Theorem 1]. Singular
Sturmian separation theorems on unbounded intervals presented in Sect. 6.4,
including the necessary tools in Sect. 6.3.11, are derived from [292]. The results
in Corollary 6.177 (singular Sturmian separation theorem) are correct singular
versions of [94, Eq. (4.14) and (4.29)]. The results in Corollaries 6.180 and 6.191
represent generalizations of [94, Corollary 4.2, Eqs. (4.14a), (4.16a)] to a possibly
uncontrollable system (SDS) near ∞or −∞. A similar statement was obtained in
[91, Theorem 2] and [104, Theorem 1] for an eventually controllable system (SDS)
on the interval (−∞, ∞); compare with Corollaries 6.191 and 6.192. The equalities
in (6.596) are known in [104, Theorem 1] and in [91, Theorem 2]. The equivalences
in (6.594) and (6.595) then generalize the result in (6.596) to possibly uncontrollable
system (SDS).
Regarding the basic notions in Sect. 6.4.3, it is shown in [300, Section 4] how the
notion of a recessive/dominant solution of (SDS) at −∞can be transformed into
the corresponding notion at ∞and vice versa. Several examples, which illustrate
the concepts and results in Sects. 6.4.2 and 6.4.3, are presented in [292, Section 7].
We note that the development of the presented discrete-time theory was
inﬂuenced by the corresponding theory of linear Hamiltonian differential
systems (1.103) in [283, 285–289, 293].
We recommend the following additional related references for further reading
about the topics presented in this chapter: [5, 52, 76] for Sturmian theory of
symplectic systems and their recessive and dominant solutions.

References
1. A.A. Abramov, A modiﬁcation of one method for solving nonlinear self-adjoint eigenvalue
problems for Hamiltonian systems of ordinary differential equations, Comput. Math. Math.
Phys. 51(1), 35–39 (2011)
2. R.P. Agarwal, Difference Equations and Inequalities. Theory, methods, and applications, 2nd
edn. Monographs and Textbooks in Pure and Applied Mathematics, vol. 228 (Marcel Dekker,
New York, 2000)
3. R.P. Agarwal, C.D. Ahlbrandt, M. Bohner, A. Peterson, Discrete linear Hamiltonian systems:
A survey. Dynam. Syst. Appl. 8(3–4), 307–333 (1999)
4. R.P. Agarwal, M. Bohner, S.R. Grace, D. O’Regan, Discrete Oscillation Theory (Hindawi
Publishing, New York, NY, 2005)
5. D. Aharonov, M. Bohner, U. Elias, Discrete Sturm comparison theorems on ﬁnite and inﬁnite
intervals. J. Differ. Equ. Appl. 18, 1763–1771 (2012)
6. C.D. Ahlbrandt, Principal and antiprincipal solutions of self-adjoint differential systems and
their reciprocals. Rocky Mt. J. Math. 2, 169–182 (1972)
7. C.D. Ahlbrandt, Linear independence of the principal solutions at ∞and −∞for formally
self-adjoint differential systems. J. Differ. Equ. 29(1), 15–27 (1978)
8. C.D. Ahlbrandt, Equivalence of discrete Euler equations and discrete Hamiltonian systems. J.
Math. Anal. Appl. 180, 498–517 (1993)
9. C.D. Ahlbrandt, Continued fractions representation of maximal and minimal solutions of a
discrete matrix Riccati equation. SIAM J. Math. Anal. 24, 1597–1621 (1993)
10. C.D. Ahlbrandt, Geometric, analytic, and arithmetic aspects of symplectic continued frac-
tions, in Analysis, Geometry and Groups: A Riemann Legacy Volume, Hadronic Press Collect.
Orig. Artic. (Hadronic Press, Palm Harbor, FL, 1993), pp. 1–26
11. C.D. Ahlbrandt, Dominant and recessive solutions of symmetric three term recurrences. J.
Differ. Equ. 107, 238–258 (1994)
12. C.D. Ahlbrandt, M. Bohner, J. Ridenhour, Hamiltonian systems on time scales. J. Math. Anal.
Appl. 250(2), 561–578 (2000)
13. C.D. Ahlbrandt, C. Chicone, S.L. Clark, W.T. Patula, D. Steiger, Approximate ﬁrst integrals
for discrete Hamiltonian systems. Dynam. Contin. Discrete Impuls. Syst. 2, 237–264 (1996)
14. C.D. Ahlbrandt, M. Heifetz, Discrete Riccati equations of ﬁltering and control, in Proceedings
of the First International Conference on Difference Equations (San Antonio, TX, 1994),
ed. by S. Elaydi, J. Graef, G. Ladas, A. Peterson (Gordon and Breach, Newark, NJ, 1996),
pp. 1–16
© Springer Nature Switzerland AG 2019
O. Došlý et al., Symplectic Difference Systems: Oscillation and Spectral Theory,
Pathways in Mathematics, https://doi.org/10.1007/978-3-030-19373-7
573

574
References
15. C.D. Ahlbrandt, J.W. Hooker, Recessive solutions of symmetric three term recurrence
relations, in Oscillations, Bifurcation and Chaos (Toronto, 1986), Canadian Math. Soc.
Conference Proceedings, Vol. 8, ed. by F. Atkinson, W. Langford, A. Mingarelli (Amer. Math.
Soc., Providence, RI, 1987), pp. 3–42
16. C.D. Ahlbrandt, A.C. Peterson, Discrete Hamiltonian Systems. Difference Equations, Con-
tinued Fractions, and Riccati Equations. Kluwer Texts in the Mathematical Sciences, Vol. 16
(Kluwer Academic Publishers, Dordrecht, 1996)
17. C.D. Ahlbrandt, A.C. Peterson, A general reduction of order theorem for discrete linear
symplectic systems, in Dynamical Systems and Differential Equations, Vol. I (Springﬁeld,
MO, 1996). Discrete Contin. Dynam. Syst. 1998, Added Volume I, 7–18 (1998)
18. L.D. Akulenko, S.V. Nesterov, A frequency-parametric analysis of natural oscillations of non-
uniform rods. J. Appl. Math. Mech. 67, 525–537 (2003)
19. L.D. Akulenko, S.V. Nesterov, High-Precision Methods in Eigenvalue Problems and Their
Applications (Chapman and Hall/Crc, Boca Raton, FL, 2005)
20. L.D. Akulenko, S.V. Nesterov, Flexural vibrations of a moving rod. J. Appl. Math. Mech. 72,
550–560 (2008)
21. K. Ammann, Relative oscillation theory for Jacobi matrices extended. Oper. Matrices 1, 99–
115 (2014)
22. K. Ammann, G. Teschl, Relative oscillation theory for Jacobi matrices, in Proceedings of the
14th International Conference on Difference Equations and Applications (Istanbul, 2008), ed.
by M. Bohner, Z. Došlá, G. Ladas, M. Ünal, A. Zafer (U˘gur-Bahçe¸sehir University Publishing
Company, Istanbul, 2009), pp. 105–115
23. W.O. Amrein, A.M. Hinz, D.B. Pearson (eds.), Sturm–Liouville Theory. Past and Present.
Including papers from the International Colloquium held at the University of Geneva (Geneva,
2003) (Birkhäuser Verlag, Basel, 2005)
24. D.R. Anderson, Discrete trigonometric matrix functions. PanAmer. Math. J. 7(1), 39–54
(1997)
25. D.R. Anderson, Normalized prepared bases for discrete symplectic matrix systems. Dynam.
Syst. Appl. 8(3–4), 335–344 (1999)
26. V.I. Arnold, Sturm theorems and symplectic geometry. Funct. Anal. Appl. 19(4), 251–259
(1985)
27. V.I. Arnold, Mathematical Methods of Classical Mechanics, 2nd edn. (Springer, Berlin, 1989)
28. F.V. Atkinson, Discrete and Continuous Boundary Value Problems (Academic Press,
New York, NY, 1964)
29. T. Auckenthalera, V. Blumb, H.J. Bungartza, et al., Parallel solution of partial symmetric
eigenvalue problems from electronic structure calculations. Parallel Comput. 37, 783–794
(2011)
30. P.B. Bailey, W.N. Everitt, A. Zettl, The SLEIGN2 Sturm–Liouville Code. ACM Trans. Math.
Software 21, 143–192 (2001)
31. B. Bandyrskii, I. Gavrilyuk, I. Lazurchak, V. Makarov, Functional-discrete method (fd-
method) for matrix Sturm–Liouville problems. Comput. Methods Appl. Math. 5, 362–386
(2005)
32. J.H. Barrett, A Prüfer transformation for matrix differential systems. Proc. Am. Math. Soc. 8,
510–518 (1957)
33. H. Behncke, Spectral theory of Hamiltonian difference systems with almost constant coefﬁ-
cients. J. Differ. Equ. Appl. 19(1), 1–12 (2013)
34. A. Ben-Israel, T.N.E. Greville, Generalized Inverses: Theory and Applications (Wiley,
New York, 1974)
35. P. Benner, Symplectic balancing of Hamiltonian matrices. SIAM J. Sci. Comput. 22, 1885–
1904 (2000)
36. D.S. Bernstein, Matrix Mathematics. Theory, Facts, and Formulas with Application to Linear
Systems Theory (Princeton University Press, Princeton, 2005)
37. P.A. Binding, H. Volkmer, Interlacing and oscillation for Sturm–Liouville problems with
separated and coupled boundary conditions. J. Comput. Appl. Math. 194(1), 75–93 (2006)

References
575
38. G.D. Birkhoff, Existence and oscillation theorem for a certain boundary value problem. Trans.
Am. Math. Soc. 10(2), 259–270 (1909)
39. M. Bohner, Linear Hamiltonian difference systems: disconjugacy and Jacobi-type conditions.
J. Math. Anal. Appl. 199(3), 804–826 (1996)
40. M. Bohner, Riccati matrix difference equations and linear Hamiltonian difference systems.
Dynam. Contin. Discrete Impuls. Syst. 2(2), 147–159 (1996)
41. M. Bohner, On disconjugacy for Sturm–Liouville difference equations, in Difference Equa-
tions: Theory and Applications (San Francisco, CA, 1995). J. Differ. Equ. Appl. 2(2), 227–237
(1996)
42. M. Bohner, Symplectic systems and related discrete quadratic functionals. Facta Univ. Ser.
Math. Inform. 12, 143–156 (1997)
43. M. Bohner, Discrete Sturmian theory. Math. Inequal. Appl. 1(3), 375–383 (1998)
44. M. Bohner, Discrete linear Hamiltonian eigenvalue problems. Comput. Math. Appl. 36(10–
12), 179–192 (1998)
45. M. Bohner, O. Došlý, Disconjugacy and transformations for symplectic systems. Rocky Mt.
J. Math. 27, 707–743 (1997)
46. M. Bohner, O. Došlý, Trigonometric transformations of symplectic difference systems. J.
Differ. Equ. 163, 113–129 (2000)
47. M. Bohner, O. Došlý, The discrete Prüfer transformation. Proc. Am. Math. Soc. 129, 2715–
2726 (2001)
48. M. Bohner, O. Došlý, Trigonometric systems in oscillation theory of difference equations,
in Dynamic Systems and Applications, Proceedings of the Third International Conference
on Dynamic Systems and Applications (Atlanta, GA, 1999), Vol. 3 (Dynamic, Atlanta, GA,
2001), pp. 99–104
49. M. Bohner, O. Došlý, R. Hilscher, Linear Hamiltonian dynamic systems on time scales:
Sturmian property of the principal solution, in Proceedings of the Third World Congress of
Nonlinear Analysts (Catania, 2000). Nonlinear Anal. 47, 849–860 (2001)
50. M. Bohner, O. Došlý, R. Hilscher, W. Kratz, Diagonalization approach to discrete quadratic
functionals. Arch. Inequal. Appl. 1(2), 261–274 (2003)
51. M. Bohner, O. Došlý, W. Kratz, Inequalities and asymptotics for Riccati matrix difference
operators. J. Math. Anal. Appl. 221, 262–286 (1998)
52. M. Bohner, O. Došlý, W. Kratz, A Sturmian theorem for recessive solutions of linear
Hamiltonian difference systems. Appl. Math. Lett. 12, 101–106 (1999)
53. M. Bohner, O. Došlý, W. Kratz, Discrete Reid roundabout theorems. Dynam. Syst. Appl.
8(3–4), 345–352 (1999)
54. M. Bohner, O. Došlý, W. Kratz, Positive semideﬁniteness of discrete quadratic functionals.
Proc. Edinb. Math. Soc. (2) 46(3), 627–636 (2003)
55. M. Bohner, O. Došlý, W. Kratz, An oscillation theorem for discrete eigenvalue problems.
Rocky Mt. J. Math. 33(4), 1233–1260 (2003)
56. M. Bohner, O. Došlý, W. Kratz, Sturmian and spectral theory for discrete symplectic systems.
Trans. Am. Math. Soc. 361, 3019–3123 (2009)
57. M. Bohner, W. Kratz, R. Šimon Hilscher, Oscillation and spectral theory for linear Hamilto-
nian systems with nonlinear dependence on the spectral parameter. Math. Nachr. 285(11–12),
1343–1356 (2012)
58. M. Bohner, A. Peterson, Dynamic Equations on Time Scales. An Introduction with Applica-
tions (Birkhäuser, Boston, 2001)
59. M. Bohner, A. Peterson (eds.), Advances in Dynamic Equations on Time Scales (Birkhäuser,
Boston, 2003)
60. M. Bohner, S. Sun, Weyl–Titchmarsh theory for symplectic difference systems. Appl. Math.
Comput. 216(10), 2855–2864 (2010)
61. D.L. Boley, P. Van Dooren, Placing zeroes and the Kronecker canonical form. Circuits
Systems Signal Process. 13(6), 783–802 (1994)
62. V.G. Boltyanskii, Optimal Control of Discrete Systems (Wiley, New York, NY, 1978)

576
References
63. A. Bunse-Gerstner, Matrix factorizations for symplectic QR-like methods. Linear Algebra
Appl. 83, 49–77 (1986)
64. S.L. Campbell, C.D. Meyer, Generalized Inverses of Linear Transformations. Reprint of the
1991 corrected reprint of the 1979 original, Classics in Applied Mathematics, Vol. 56 (Society
for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2009)
65. P.J. Channell, C. Scovel, Symplectic integration of Hamiltonian systems. Nonlinearity 3(2),
231–259 (1990)
66. S. Clark, F. Gesztesy, On Weyl–Titchmarsh theory for singular ﬁnite difference Hamiltonian
systems. J. Comput. Appl. Math. 171(1–2), 151–184 (2004)
67. S. Clark, P. Zemánek, On a Weyl–Titchmarsh theory for discrete symplectic systems on a half
line. Appl. Math. Comput. 217(7), 2952–2976 (2010)
68. S. Clark, P. Zemánek, On discrete symplectic systems: Associated maximal and minimal
linear relations and nonhomogeneous problems. J. Math. Anal. Appl. 421(1), 779–805 (2014)
69. E.A. Coddington, N. Levinson, Theory of Ordinary Differential Equations (McGraw–Hill,
New York, 1955)
70. W.A. Coppel, Disconjugacy. Lecture Notes in Mathematics, Vol. 220 (Springer, Berlin, 1971)
71. J.W. Demmel, Applied Numerical Linear Algebra (Society for Industrial and Applied
Mathematics (SIAM), Philadelphia, 1997)
72. J.W. Demmel, I.S. Dhillon, H. Ren, On the correctness of some bisection-like parallel
algorithms in ﬂoating point arithmetic. Electron. Trans. Numer. Anal. 3, 116–140 (1995)
73. J. Demmel, A. McKenney, A test matrix generation suite: LAPACK Working Note 9, technical
report, Department of Computer Science, Courant Institute, New York, 1989
74. F.M. Dopico, C.R. Johnson, Complementary bases in symplectic matrices and a proof that
their determinant is one. Linear Algebra Appl. 419, 772–778 (2006)
75. F.M. Dopico, C.R. Johnson, Parametrization of matrix symplectic group and applications.
SIAM J. Matrix Anal. 419, 1–24 (2009)
76. Z. Došlá, D. Škrabáková, Phases of second order linear difference equations and symplectic
systems. Math. Bohem. 128(3), 293–308 (2003)
77. O. Došlý, On transformations of self-adjoint linear differential systems and their reciprocals.
Ann. Polon. Math. 50, 223–234 (1990)
78. O. Došlý, Principal solutions and transformations of linear Hamiltonian systems. Arch. Math.
(Brno) 28, 113–120 (1992)
79. O. Došlý, Oscillation criteria for self-adjoint linear differential equations. Math. Nachr. 166,
141–153 (1994)
80. O. Došlý, Oscillation criteria for higher order Sturm–Liouville difference equations. J. Differ.
Equ. Appl. 4(5), 425–450 (1998)
81. O. Došlý, Principal and nonprincipal solutions of symplectic dynamic systems on time scales,
in Proceedings of the Sixth Colloquium on the Qualitative Theory of Differential Equations
(Szeged, Hungary, 1999), No. 5, 14 pp. (electronic). Electron. J. Qual. Theory Differ. Equ.,
Szeged, 2000
82. O. Došlý, Oscillation theory of linear difference equations, in CDDE Proceedings (Brno,
2000). Arch. Math. (Brno) 36, 329–342 (2000)
83. O. Došlý, Trigonometric transformation and oscillatory properties of second order difference
equations, in Communications in Difference Equations (Poznan, 1998) (Gordon and Breach,
Amsterdam, 2000), pp. 125–133
84. O. Došlý, Discrete quadratic functionals and symplectic difference systems. Funct. Differ.
Equ. 11(1–2), 49–58 (2004)
85. O. Došlý, Symplectic difference systems: oscillation theory and hyperbolic Prüfer transfor-
mation. Abstr. Appl. Anal. 2004(4), 285–294 (2004)
86. O. Došlý, The Bohl transformation and its applications, in 2004–Dynamical Systems and
Applications, Proceedings of the International Conference (Antalya, 2004) (GBS Publishers
& Distributors, Delhi, 2005), pp. 371–385
87. O. Došlý, Oscillation theory of symplectic difference systems, in Advances in Discrete
Dynamical Systems, Proceedings of the Eleventh International Conference on Difference

References
577
Equations and Applications (Kyoto, 2006), ed. by S. Elaydi, K. Nishimura, M. Shishikura,
N. Tose. Adv. Stud. Pure Math., Vol. 53 (Mathematical Society of Japan, Tokyo, 2009),
pp. 41–50
88. O. Došlý, Oscillation and conjugacy criteria for two-dimensional symplectic difference
systems. Comput. Math. Appl. 64(7), 2202–2208 (2012)
89. O. Došlý, Symplectic difference systems with periodic coefﬁcients: Krein’s trafﬁc rules for
multipliers. Adv. Differ. Equ. 2013(85), 13 pp. (2013)
90. O. Došlý, Symplectic difference systems: natural dependence on a parameter, in Proceedings
of the International Conference on Differential Equations, Difference Equations, and Special
Functions (Patras, 2012). Adv. Dyn. Syst. Appl. 8(2), 193–201 (2013)
91. O. Došlý, Focal points and recessive solutions of discrete symplectic systems. Appl. Math.
Comput. 243, 963–968 (2014)
92. O. Došlý, Relative oscillation of linear Hamiltonian differential systems. Math. Nachr.
290(14–15), 2234–2246 (2017)
93. O. Došlý, On some aspects of the Bohl transformation for Hamiltonian and symplectic
systems. J. Math. Anal. Appl. 448(1), 281–292 (2017)
94. O. Došlý, J. Elyseeva, Singular comparison theorems for discrete symplectic systems. J.
Differ. Equ. Appl. 20(8), 1268–1288 (2014)
95. O. Došlý, J. Elyseeva, Discrete oscillation theorems and weighted focal points for Hamilto-
nian difference systems with nonlinear dependence on a spectral parameter. Appl. Math. Lett.
43, 114–119 (2015)
96. O. Došlý, J. Elyseeva, An oscillation criterion for discrete trigonometric systems. J. Differ.
Equ. Appl. 21(12), 1256–1276 (2015)
97. O. Došlý, S. Hilger, R. Hilscher, Symplectic dynamic systems, in Advances in Dynamic
Equations on Time Scales, ed. by M. Bohner, A. Peterson (Birkhäuser, Boston, 2003),
pp. 293–334
98. O. Došlý, R. Hilscher, Linear Hamiltonian difference systems: transformations, recessive
solutions, generalized reciprocity. Dynam. Syst. Appl. 8(3–4), 401–420 (1999)
99. O. Došlý, R. Hilscher, Disconjugacy, transformations and quadratic functionals for symplectic
dynamic systems on time scales. J. Differ. Equ. Appl. 7, 265–295 (2001)
100. O. Došlý, R. Hilscher, V. Zeidan, Nonnegativity of discrete quadratic functionals correspond-
ing to symplectic difference systems. Linear Algebra Appl. 375, 21–44 (2003)
101. O. Došlý, W. Kratz, A Sturmian separation theorem for symplectic difference systems. J.
Math. Anal. Appl. 325, 333–341 (2007)
102. O. Došlý, W. Kratz, Oscillation theorems for symplectic difference systems. J. Differ. Equ.
Appl. 13, 585–60 (2007)
103. O. Došlý, W. Kratz, Oscilation and spectral theory for symplectic difference systems with
separated boundeary conditions. J. Differ. Equ. Appl. 16, 831–846 (2010)
104. O. Došlý, W. Kratz, A remark on focal points of recessive solutions of discrete symplectic
systems. J. Math. Anal. Appl. 363, 209–213 (2010)
105. O. Došlý, W. Kratz, Singular Sturmian theory for linear Hamiltonian differential systems.
Appl. Math. Lett. 26, 1187–1191 (2013)
106. O. Došlý, Š. Pechancová, Trigonometric recurrence relations and tridiagonal trigonometric
matrices. Int. J. Differ. Equ. 1(1), 19–29 (2006)
107. O. Došlý, Š. Pechancová, Generalized zeros of 2 × 2 symplectic difference system and of its
reciprocal system. Adv. Differ. Equ. 2011(Article ID 571935), 23 pp. (2011)
108. O. Došlý, Z. Pospíšil, Hyperbolic transformation and hyperbolic difference systems. Fasc.
Math. 32, 26–48 (2001)
109. H.I. Dwyer, A. Zettl, Computing eigenvalues of regular Sturm–Liouville problems. Electron.
J. Differ. Equ. 1994(6), 10 pp. (1994)
110. S. Elaydi, An Introduction to Difference Equations, 3rd edn. Undergraduate Texts in
Mathematics (Springer, New York, NY, 2005)
111. Yu.V. Eliseeva, An algorithm for solving the matrix difference Riccati equation. Comput.
Math. Math. Phys. 39(2), 177–184 (1999)

578
References
112. J.V. Elyseeva, A transformation for symplectic systems and the deﬁnition of a focal point.
Comput. Math. Appl. 47, 123–134 (2004)
113. J.V. Elyseeva, Symplectic factorizations and the deﬁnition of a focal point, in Proceedings of
the Eighth International Conference on Difference Equations and Applications (Brno, 2003),
ed. by S. Elaydi, G. Ladas, B. Aulbach, O. Došlý (Chapman & Hall/CRC, Boca Raton, FL,
2005), pp. 127–135
114. J.V. Elyseeva, The comparative index for conjoined bases of symplectic difference systems,
in Difference Equations, Special Functions, and Orthogonal Polynomials, Proceedings of the
International Conference (Munich, 2005), ed. by S. Elaydi, J. Cushing, R. Lasser, A. Rufﬁng,
V. Papageorgiou, W. Van Assche (World Scientiﬁc, London, 2007), pp. 168–177
115. Yu.V. Eliseeva, Comparative index for solutions of symplectic difference systems. Differential
Equations 45(3), 445–459 (2009)
116. J.V. Elyseeva, Transformations and the number of focal points for conjoined bases of
symplectic difference systems. J. Differ. Equ. Appl. 15(11–12), 1055–1066 (2009)
117. Yu.V. Eliseeva, Comparison theorems for symplectic systems of difference equations. Differ-
ential Equations 46(9), 1339–1352 (2010)
118. J.V. Elyseeva, On relative oscillation theory for symplectic eigenvalue problems. Appl. Math.
Lett. 23, 1231–1237 (2010)
119. J.V. Elyseeva, The comparative index and the number of focal points for conjoined bases of
symplectic difference systems, in Discrete Dynamics and Difference Equations, Proceedings
of the Twelfth International Conference on Difference Equations and Applications (Lisbon,
2007), ed. by S. Elaydi, H. Oliveira, J.M. Ferreira, J.F. Alves (World Scientiﬁc, London,
2010), pp. 231–238
120. Yu.V. Eliseeva, Spectra of discrete symplectic eigenvalue problems with separated boundary
conditions. Russ. Math. (Iz. VUZ) 55(11), 71–75 (2011)
121. J.V. Elyseeva, Comparative Index in Mathematical Modelling of Oscillations of Discrete
Symplectic Systems, (in Russian) (Moscow State University of Technology “Stankin”,
Moscow, 2011)
122. Yu.V. Eliseeva, An approach for computing eigenvalues of discrete symplectic boundary-
value problems. Russ. Math. (Iz. VUZ) 56(7), 47–51 (2012)
123. J.V. Elyseeva, A note on relative oscillation theory for symplectic difference systems with
general boundary conditions. Appl. Math. Lett. 25(11), 1809–1814 (2012)
124. J.V. Elyseeva, Relative oscillation theory for matrix Sturm-Liouville difference equations
extended. Adv. Differ. Equ. 2013(328), 25 pp. (2013)
125. J.V. Elyseeva, Generalized oscillation theorems for symplectic difference systems with
nonlinear dependence on spectral parameter. Appl. Math. Comput. 251, 92–107 (2015)
126. J.V. Elyseeva, Generalized reciprocity principle for discrete symplectic systems. Electron. J.
Qual. Theory Differ. Equ. 2015(95), 12 pp. (2015) (electronic)
127. J.V. Elyseeva, Comparison theorems for conjoined bases of linear Hamiltonian differential
systems and the comparative index. J. Math. Anal. Appl. 444(2), 1260–1273 (2016)
128. J.V. Elyseeva, Comparison theorems for weighted focal points of conjoined bases of
Hamiltonian difference systems, in Differential and Difference Equations with Applications,
Proceedings of the International Conference on Differential & Difference Equations and
Applications (Amadora, 2015), ed. by S. Pinelas, Z. Došlá, O. Došlý, P.E. Kloeden. Springer
Proceedings in Mathematics & Statistics, Vol. 164 (Springer, Berlin, 2016), pp. 225–233
129. J.V. Elyseeva, On symplectic transformations of linear Hamiltonian differential systems
without normality. Appl. Math. Lett. 68, 33–39 (2017)
130. J.V. Elyseeva, The comparative index and transformations of linear Hamiltonian differential
systems. Appl. Math. Comput. 330, 185–200 (2018)
131. J.V. Elyseeva, Oscillation theorems for linear Hamiltonian systems with nonlinear depen-
dence on the spectral parameter and the comparative index. Appl. Math. Lett. 90, 15–22
(2019)

References
579
132. J.V. Elyseeva, A.A. Bondarenko, The Schur complement in an algorithm for calculation of
focal points of conjoined bases of symplectic difference systems. Int. J. Pure Appl. Math.
67(4), 455–474 (2011)
133. J.V. Elyseeva, R. Šimon Hilscher, Discrete oscillation theorems for symplectic eigenvalue
problems with general boundary conditions depending nonlinearly on spectral parameter.
Linear Algebra Appl. 558, 108–145 (2018)
134. L. Erbe, P. Yan, Disconjugacy for linear Hamiltonian difference systems. J. Math. Anal. Appl.
167, 355–367 (1992)
135. L. Erbe, P. Yan, Qualitative properties of Hamiltonian differece systems. J. Math. Anal. Appl.
171, 334–345 (1992)
136. L. Erbe, P. Yan, Oscillation criteria for Hamiltonian matrix difference systems. Proc. Am.
Math. Soc. 119, 525–533 (1993)
137. L. Erbe, P. Yan, On the discrete Riccati equation and its application to discrete Hamiltonian
systems. Rocky Mt. J. Math. 25, 167–178 (1995)
138. R. Fabbri, R. Johnson, S. Novo, C. Núñez, Some remarks concerning weakly disconjugate
linear Hamiltonian systems. J. Math. Anal. Appl. 380(2), 853–864 (2011)
139. H. Fassbender, Symplectic Methods for the Symplectic Eigenproblem (Kluwer, New York.
NY, 2000)
140. K. Feng, The Hamiltonian way for computing Hamiltonian dynamics, in Applied and
Industrial Mathematics (Venice, 1989). Math. Appl., Vol. 56 (Kluwer, Dordrecht, 1991),
pp. 17–35
141. K. Feng, M. Qin, Symplectic Geometric Algorithms for Hamiltonian Systems. Translated and
revised from the Chinese original. With a foreword by Feng Duan, Zhejiang Science and
Technology Publishing House, Hangzhou, Springer, Heidelberg, 2010
142. K. Feng, D. Wang, A note on conservation laws of symplectic difference schemes for
Hamiltonian systems. J. Comput. Math. 9(3), 229–237 (1991)
143. K. Feng, H. Wu, M. Qin, Symplectic difference schemes for linear Hamiltonian canonical
systems. J. Comput. Math. 8(4), 371–380 (1990)
144. A. Fischer, C. Remling, The absolutely continuous spectrum of discrete canonical systems.
Trans. Am. Math. Soc. 361(2), 793–818 (2009)
145. S. Flach, A.V. Gorbach, Discrete breathers – advances in theory and applications. Phys. Rep.
467, 1–116 (2008)
146. T.E. Fortmann, A matrix inversion identity. IEEE Trans. Automat. Control 15, 599–599
(1970)
147. F.R. Gantmacher, Lectures in Analytical Mechanics (MIR Publischers, Moscow, 1975)
148. F.R. Gantmacher, Theory of Matrices (AMS Chelsea Publishing, Providence, RI, 1998)
149. C. Gao, R. Ma, Eigenvalues of discrete linear second-order periodic and antiperiodic
eigenvalue problems with sign-changing weight. Linear Algebra Appl. 467, 40–56 (2015)
150. C. Gao, R. Ma, Eigenvalues of discrete Sturm–Liouville problems with eigenparameter
dependent boundary conditions. Linear Algebra Appl. 503, 100–119 (2016)
151. W. Gautschi, Computational aspects of three term recurrence relations. SIAM Rev. 9, 24–82
(1967)
152. I.M. Gelfand, M.M. Kapranov, A.V. Zelevinsky, Discriminants, Resultants, and Multidimen-
sional Determinants (Birkhäuser, Boston, 1994)
153. F. Gesztesy, B. Simon, G. Teschl, Zeros of the Wronskian and renormalized oscillation theory.
Am. J. Math. 118(3), 571–594 (1996)
154. S.K. Godunov, Veriﬁcation of boundedness for the powers of symplectic matrices with the
help of averaging. Sib. Math. J. 33(6), 939–949 (1992)
155. S.K. Godunov, M. Sadkane, Numerical determination of a canonical form of a symplectic
matrix. Sib. Math. J. 42(4), 629–647 (2001)
156. I.C. Gohberg, M.G. Krein, Theory and Applications of Volterra Operators in Hilbert Space.
Translations of Mathematical Monographs, Vol. 24 (American Mathematical Society, 1970)
157. G.H. Golub, C.F. Van Loan, Matrix Computations. John Hopkins Series in Mathematical
Sciences, 2nd edn. (John Hopkins University Press, Baltimore, MD, 1989)

580
References
158. L. Greenberg, An oscillation method for fourth order self-adjoint two-point boundary value
problems with nonlinear eigenvalues. SIAM J. Math. Anal. 22, 1021–1042 (1991)
159. L. Greenberg, A Prüfer method for calculating eigenvalues of self-adjoint systems of ordinary
differential equations, technical report Tr91-24, University of Maryland, College Park, MD,
1991
160. L. Greenberg, M. Marletta, Algorithm 775: The code SLEUTH for solving fourth-order
Sturm–Liouville problems. ACM Trans. Math. Software 23, 453–493 (1997)
161. L. Greenberg, M. Marletta, Numerical methods for higher order Sturm–Liouville problems.
J. Comput. Appl. Math. 125, 367–383 (2000)
162. D. Greenspan, Discrete Models (Addison-Wesley, London, 1973)
163. E. Hairer, S.P. Norsett, G. Wanner, Ordinary Differential Equations I: Nonstiff Problems
(Springer, New York, NY, 2008)
164. E. Hairer, G. Wanner, Solving Ordinary Differential Equations II: Stiff and Differential-
Algebraic Problems (Springer, New York, NY, 2010)
165. A. Halanay, V. R˘asvan, Stability and boundary value problems for discrete-time linear
Hamiltonian systems. Dynam. Syst. Appl. 8(3–4), 439–459 (1999)
166. B.J. Harmsen, A. Li, Discrete Sturm–Liouville problems with parameter in the boundary
conditions. J. Differ. Equ. Appl. 8(11), 969–981 (2002)
167. B.J. Harmsen, A. Li, Discrete Sturm–Liouville problems with nonlinear parameter in the
boundary conditions. J. Differ. Equ. Appl. 13(7), 639–653 (2007)
168. P. Hartman, Ordinary Differential Equations (Wiley, New York, NY, 1964)
169. P. Hartman, Difference equations: disconjugacy, principal solutions, Green’s function, com-
plete monotonicity. Trans. Am. Math. Soc. 246, 1–30 (1978)
170. R.E. Hartwig, A note on the partial ordering of positive semi-deﬁnite matrices. Linear
Multilinear Algebra 6(3), 223–226 (1978/79)
171. R. Hilscher, Disconjugacy of symplectic systems and positivity of block tridiagonal matrices.
Rocky Mt. J. Math. 29(4), 1301–1319 (1999)
172. R. Hilscher, Reid roundabout theorem for symplectic dynamic systems on time scales. Appl.
Math. Optim. 43(2), 129–146 (2001)
173. R. Hilscher, V. R˚užiˇcková, Implicit Riccati equations and discrete symplectic systems. Int. J.
Differ. Equ. 1, 135–154 (2006)
174. R. Hilscher, V. R˚užiˇcková, Riccati inequality and other results for discrete symplectic systems.
J. Math. Anal. Appl. 322(2), 1083–1098 (2006)
175. R. Hilscher, V. R˚užiˇcková, Perturbation of time scale quadratic functionals with variable
endpoints. Adv. Dyn. Syst. Appl. 2(2), 207–224 (2007)
176. R. Hilscher, P. ˇRehák, Riccati inequality, disconjugacy, and reciprocity principle for linear
Hamiltonian dynamic systems. Dynam. Syst. Appl. 12(1–2), 171–189 (2003)
177. R. Hilscher, V. Zeidan, Discrete optimal control: the accessory problem and necessary
optimality conditions. J. Math. Anal. Appl. 243(2), 429–452 (2000)
178. R. Hilscher, V. Zeidan, Second order sufﬁciency criteria for a discrete optimal control
problem. J. Differ. Equ. Appl. 8(6), 573–602 (2002)
179. R. Hilscher, V. Zeidan, Discrete optimal control: second order optimality conditions. J. Differ.
Equ. Appl. 8(10), 875–896 (2002)
180. R. Hilscher, V. Zeidan, Coupled intervals in the discrete calculus of variations: necessity and
sufﬁciency. J. Math. Anal. Appl. 276(1), 396–421 (2002)
181. R. Hilscher, V. Zeidan, Symplectic difference systems: variable stepsize discretization and
discrete quadratic functionals. Linear Algebra Appl. 367, 67–104 (2003)
182. R. Hilscher, V. Zeidan, Nonnegativity of a discrete quadratic functional in terms of the
(strengthened) Legendre and Jacobi conditions. Comput. Math. Appl. 45(6–9), 1369–1383
(2003)
183. R. Hilscher, V. Zeidan, A remark on discrete quadratic functionals with separable endpoints.
Rocky Mt. J. Math. 33(4), 1337–1351 (2003)
184. R. Hilscher, V. Zeidan, Coupled intervals in the discrete optimal control. J. Differ. Equ. Appl.
10(2), 151–186 (2004)

References
581
185. R. Hilscher, V. Zeidan, Discrete quadratic functionals with jointly varying endpoints via
separable endpoints, in New Progress in Difference Equations, Proceedings of the Sixth
International Conference on Difference Equations (Augsburg, 2001), ed. by B. Aulbach,
S. Elaydi, G. Ladas (Chapman & Hall/CRC, Boca Raton, FL, 2004), pp. 461–470
186. R. Hilscher, V. Zeidan, Nonnegativity and positivity of a quadratic functional in the discrete
calculus of variations: A survey. J. Differ. Equ. Appl. 11(9), 857–875 (2005)
187. R. Hilscher, V. Zeidan, Time scale symplectic systems without normality. J. Differ. Equ.
230(1), 140–173 (2006)
188. R. Hilscher, V. Zeidan, Coupled intervals for discrete symplectic systems. Linear Algebra
Appl. 419(2–3), 750–764 (2006)
189. R. Hilscher, V. Zeidan, Extension of discrete LQR–problem to symplectic systems. Int. J.
Differ. Equ. 2(2), 197–208 (2007)
190. R. Hilscher, V. Zeidan, Applications of time scale symplectic systems without normality. J.
Math. Anal. Appl. 340(1), 451–465 (2008)
191. R. Hilscher, V. Zeidan, Riccati equations for abnormal time scale quadratic functionals. J.
Differ. Equ. 244(6), 1410–1447 (2008)
192. R. Hilscher, V. Zeidan, Weak maximum principle and accessory problem for control problems
on time scales. Nonlinear Anal. 70(9), 3209–3226 (2009)
193. R. Hilscher, V. Zeidan, Multiplicities of focal points for discrete symplectic systems: revisited.
J. Differ. Equ. Appl. 15(10), 1001–1010 (2009)
194. R. Hilscher, P. Zemánek, Trigonometric and hyperbolic systems on time scales. Dynam. Syst.
Appl. 18(3–4), 483–506 (2009)
195. R.A. Horn, C.R. Johnson, Topics in Matrix Analysis (Cambridge University Press, Cam-
bridge, 1991)
196. P. Howard, S. Jung, B. Kwon, The Maslov index and spectral counts for linear Hamiltonian
systems on [0, 1]. J. Dynam. Differ. Equ. 30(4), 1703–1729 (2018)
197. X. Huang, A. Jiang, Z. Zhang, H. Hua, Design and optimization of periodic structure
mechanical ﬁlter in suppression of foundation resonances. J. Sound Vib. 330, 4689–4712
(2011)
198. Y. Huanga, Z. Denga, L. Yaoa, An improved symplectic precise integration method for
analysis of the rotating rigid-ﬂexible coupled system. J. Sound Vib. 299, 229–246 (2007)
199. J. Ji, B. Yang, Eigenvalue comparison for boundary value problems for second order
difference equations. J. Math. Anal. Appl. 320(2), 964–972 (2006)
200. R. Johnson, S. Novo, C. Núñez, R. Obaya, Uniform weak disconjugacy and principal
solutions for linear Hamiltonian systems, in Recent Advances in Delay Differential and
Difference Equations (Balatonfuered, Hungary, 2013). Springer Proceedings in Mathematics
& Statistics, Vol. 94 (Springer, Berlin, 2014), pp. 131–159
201. R. Johnson, S. Novo, C. Núñez, R. Obaya, Nonautonomous linear-quadratic dissipative
control processes without uniform null controllability. J. Dynam. Differ. Equ. 29(2), 355–383
(2017)
202. R. Johnson, C. Núñez, R. Obaya, Dynamical methods for linear Hamiltonian systems with
applications to control processes. J. Dynam. Differ. Equ. 25(3), 679–713 (2013)
203. R. Johnson, R. Obaya, S. Novo, C. Núñez, R. Fabbri, Nonautonomous Linear Hamiltonian
Systems: Oscillation, Spectral Theory and Control. Developments in Mathematics, Vol. 36
(Springer, Cham, 2016)
204. W.G. Kelley, A. Peterson, Difference Equations: An Introduction with Applications (Aca-
demic Press, San Diego, CA, 1991)
205. W. Kratz, Quadratic Functionals in Variational Analysis and Control Theory. Mathematical
Topics, Vol. 6 (Akademie Verlag, Berlin, 1995)
206. W. Kratz, Banded matrices and difference equations. Linear Algebra Appl. 337(1–3), 1–20
(2001)
207. W. Kratz, Deﬁnitnes of quadratic functionals. Analysis (Munich) 23, 163–183 (2003)
208. W. Kratz, Discrete oscillation. J. Differ. Equ. Appl. 9, 127–135 (2003)

582
References
209. W. Kratz, R. Šimon Hilscher, Rayleigh principle for linear Hamiltonian systems without
controllability. ESAIM Control Optim. Calc. Var. 18(2), 501–519 (2012)
210. W. Kratz, R. Šimon Hilscher, A generalized index theorem for monotone matrix-valued
functions with applications to discrete oscillation theory. SIAM J. Matrix Anal. Appl. 34(1),
228–243 (2013)
211. W. Kratz, R. Šimon Hilscher, V. Zeidan, Eigenvalue and oscillation theorems for time scale
symplectic systems. Int. J. Dyn. Syst. Differ. Equ. 3(1–2), 84–131 (2011)
212. W. Kratz, M. Tentler, Recursion formulae for the characteristic polynomial of symmetric
banded matrices. Linear Algebra Appl. 428, 2482–2500 (2008)
213. H. Krüger, G. Teschl, Relative oscillation theory, weighted zeros of the Wronskian, and
spectral shift function. Comm. Math. Phys. 287, 613–640 (2009)
214. G. Ladas, E.A. Grove, M.R.S. Kulenovic, Progress report on rational difference equations. J.
Differ. Equ. Appl. 10, 1313–1327 (2004)
215. A.J. Laub, Matrix Analysis for Scientists and Engineers (SIAM, Philadelphia, PA, 2005)
216. P.D. Lax, Linear Algebra, Pure and Applied Mathematics (New York), A Wiley-Interscience
Publication (Wiley, New York, 1997)
217. V. Ledoux, M. Van Daele, G. Vanden Berghe, Efﬁcient computation of high index Sturm–
Liouville eigenvalues for problems in physics. Comput. Phys. Comm. 180, 241–250 (2009)
218. J.V. Lill, T.G. Schmalz, J.C. Light, Imbedded matrix Green’s functions in atomic and
molecular scattering theory. J. Chem. Phys. 78, 4456–4463 (1983)
219. W.W. Lin, V. Mehrmann, H. Xu, Canonical forms for Hamiltonian and symplectic matrices
and pencils. Linear Algebra Appl. 302–303, 469–533 (1999)
220. X.-S. Liu, Y.-Y. Qi, J.-F. He, P.-Z. Ding, Recent progress in symplectic algorithms for use in
quantum systems. Commun. Comput. Phys. 2(1), 1–53 (2007)
221. Y. Liu, Y. Shi, Regular approximations of spectra of singular discrete linear Hamiltonian
systems with one singular endpoint. Linear Algebra Appl. 451, 94–130 (2018)
222. V. Loan, A Symplectic method for approximating all the eigenvalues of a Hamiltonian matrix.
Linear Algebra Appl. 61, 233–251 (1984)
223. D.G. Luenberger, Linear and Nonlinear Programming, 2nd edn. (Addison-Wesley, Reading,
MA, 1984)
224. D.S. Mackey, N. Mackey, F. Tisseur, Structured tools for structured matrices. Electron. J.
Linear Algebra 10, 106–145 (2003)
225. B. Marinkovi´c, Optimality conditions in discrete optimal control problems with state
constraints. Numer. Funct. Anal. Optim. 28(7–8), 945–955 (2007)
226. B. Marinkovi´c, Optimality conditions for discrete optimal control problems with equality and
inequality type of constraints. Positivity 12(3), 535–545 (2008)
227. B. Marinkovi´c, Second order optimality conditions in a discrete optimal control problem.
Optimization 57(4), 539–548 (2008)
228. C.R. Maple, M. Marletta, Solving Hamiltonian systems arising from ODE eigenproblems.
Numer. Algorithms 22(3), 263–284 (1999)
229. M. Marletta, Automatic solution of regular and singular vector Sturm–Liouville problems.
Numer. Algorithms 4, 65–99 (1993)
230. M. Marletta, Numerical solution of eigenvalue problems for Hamiltonian systems. Adv.
Comput. Math. 2(2), 155–184 (1994)
231. G. Marsaglia, G.P.H. Styan, Equalities and inequalities for ranks of matrices. Linear
Multilinear Algebra 2, 269–292 (1974)
232. J. Marsden, M. West, Discrete mechanics and variational integrators. Acta Numer. 10, 357–
514 (2001)
233. J. McMahona, S. Grayb, G. Schatza, A discrete action principle for electrodynamics and the
construction of explicit symplectic integrators for linear, non-dispersive media. J. Comput.
Phys. 228, 3421–3432 (2009)
234. V. Mehrmann, A symplectic orthogonal method for single input or single output discrete time
optimal quadratic control problems. SIAM J. Matrix Anal. Appl. 9, 221–247 (1988)

References
583
235. S.J. Monaquel, K.M. Schmidt, On M-functions and operator theory for non-self-adjoint
discrete Hamiltonian systems. J. Comput. Appl. Math. 208(1), 82–101 (2007)
236. M. Morse, Variational Analysis: Critical Extremals and Sturmian Extensions (Willey,
New York, NY, 1973)
237. P. Nelson, On the effectiveness of the inverse Riccati transformation in the matrix case. J.
Math. Anal. Appl. 67, 201–210 (1978)
238. F.W.J. Olver, D.J. Sookne, Note on backward recurrence algorithms. Math. Comp. 26, 941–
947 (1972)
239. C. Paige, C. Van Loan, A Schur decomposition for Hamiltonian matrices. Linear Algebra
Appl. 41, 11–32 (1981)
240. H.J. Peng, Q. Gao, Z.G. Wu, W.X. Zhong, Symplectic adaptive algorithm for solving
nonlinear two-point boundary value problems in astrodynamics. Celestial Mech. Dynam.
Astronom. 110(4), 319–342 (2011)
241. C.H. Rasmussen, Oscillation and asymptotic behaviour of systems of ordinary linear differ-
ential equations. Trans. Am. Math. Soc. 256, 1–49 (1979)
242. V. R˘asvan, Stability zones for discrete time Hamiltonian systems. Arch. Math. (Brno) 36(5),
563–573 (2000) (electronic)
243. V. R˘asvan, Stability zones and parametric resonance for discrete-time Hamiltonian systems.
Dynamic Systems and Applications, Vol. 4 (Dynamic, Atlanta, GA, 2004), pp. 367–373
244. V. R˘asvan, On stability zones for discrete-time periodic linear Hamiltonian systems. Adv.
Differ. Equ. 2006(Art. 80757), 1–13 (2006)
245. W.T. Reid, A Prüfer transformation for differential systems. Paciﬁc J. Math. 8, 575–584
(1958)
246. W.T. Reid, Riccati matrix differential equations and non-oscillation criteria for associated
linear differential systems. Paciﬁc J. Math. 13, 665–685 (1963)
247. W.T. Reid, Generalized polar coordinate transformations for differential systems. Rocky
Mountain J. Math. 1(2), 383–406 (1971)
248. W.T. Reid, Ordinary Differential Equations (Wiley, New York, NY, 1971)
249. W.T. Reid, Riccati Differential Equations (Academic Press, New York, NY, 1972)
250. W.T. Reid, Sturmian Theory for Ordinary Differential Equations (Springer, New York, NY,
1980)
251. G. Ren, On the density of the minimal subspaces generated by discrete linear Hamiltonian
systems. Appl. Math. Lett. 27, 1–5 (2014)
252. G. Ren, Y. Shi, Defect indices and deﬁniteness conditions for a class of discrete linear
Hamiltonian systems. Appl. Math. Comput. 218(7), 3414–3429 (2011)
253. G. Ren, Y. Shi, Self-adjoint extensions for discrete linear Hamiltonian systems. Linear
Algebra Appl. 454, 1–48 (2014)
254. F.S. Rofe-Beketov, A.M. Kholkin, On the connection between spectral and oscillation
properties of the Sturm–Liouville matrix problem. Math. USSR-Sb. 31(3), 365–378 (1977)
255. F.S. Rofe-Beketov, A.M. Kholkin, Spectral Analysis of Differential Operators. World Scien-
tiﬁc Monograph Series in Mathematics, Vol. 7 (World Scientiﬁc, Hackensack, NJ, 2005)
256. R.D. Ruth, A canonical integration technique. IEEE Trans. Nuclear Sci. 30, 2669–2671
(1983)
257. V.
R˚užiˇcková,
Discrete
Symplectic
Systems
and
Deﬁniteness
of
Quadratic
Functionals,
PhD
dissertation,
Masaryk
University,
Brno,
2006.
Available
at
https://is.muni.cz/th/p9iz7/?lang=en
258. V. R˚užiˇcková, Perturbation of discrete quadratic functionals. Tatra Mountains Math. Publ.
38(1), 229–241 (2007)
259. P. ˇRehák, Oscillatory properties of second order half-linear difference equations. Czechoslo-
vak Math. J. 51(126)(2), 303–321 (2001)
260. J.M. Sanz-Serna, A. Portillo, Classical numerical integrators for wave-packet dynamics. J.
Chem. Phys. 104(6). 2349–2355 (1996)
261. H. Schulz-Baldes, Sturm intersection theory for periodic Jacobi matrices and linear Hamilto-
nian systems. Linear Algebra Appl. 436(3), 498–515 (2012)

584
References
262. G. Shi, H. Wu, Spectral theory of Sturm–Liouville difference operators. Linear Algebra Appl.
430, 830–846 (2009)
263. Y. Shi, Symplectic structure of discrete Hamiltonian systems. J. Math. Anal. Appl. 266(2),
472–478 (2002)
264. Y. Shi, Spectral theory of discrete linear Hamiltonian systems. J. Math. Anal. Appl. 289(2),
554–570 (2004)
265. Y. Shi, Weyl–Titchmarsh theory for a class of discrete linear Hamiltonian systems. Linear
Algebra Appl. 416, 452–519 (2006)
266. Y. Shi, Transformations for complex discrete linear Hamiltonian and symplectic systems.
Bull. Aust. Math. Soc. 75(2), 179–191 (2007)
267. Y. Shi, S. Chen, Spectral theory of second-order vector difference equations. J. Math. Anal.
Appl. 239(2), 195–212 (1999)
268. Y. Shi, S. Chen, Spectral theory of higher-order discrete vector Sturm—Liouville problems.
Linear Algebra Appl. 323, 7–36 (2001)
269. Y. Shi, C. Shao, G. Ren, Spectral properties of self-adjoint subspaces. Linear Algebra Appl.
438(1), 191–218 (2013)
270. G.W. Stewart, J. Sun, Matrix Perturbation Theory (Academic Press, Boston, MA, 1990)
271. H. Sun, Q. Kong, Y. Shi, Essential spectrum of singular discrete linear Hamiltonian systems.
Math. Nachr. 289(2–3), 343–359 (2016)
272. H. Sun, Y. Shi, Strong limit point criteria for a class of singular discrete linear Hamiltonian
systems. J. Math. Anal. Appl. 336(1), 224–242 (2007)
273. H. Sun, Y. Shi, Self-adjoint extensions for linear Hamiltonian systems with two singular
endpoints. J. Funct. Anal. 259(8), 2003–2027 (2010)
274. H. Sun, Y. Shi, Self-adjoint extensions for singular linear Hamiltonian systems. Math. Nachr.
284(5–6), 797–814 (2011)
275. H. Sun, Y. Shi, Spectral properties of singular discrete linear Hamiltonian systems. J. Differ.
Equ. Appl. 20(3), 379–405 (2014)
276. H. Sun, Y. Shi, On essential spectra of singular linear Hamiltonian systems. Linear Algebra
Appl. 469, 204–229 (2015)
277. S. Sun, Y. Shi, S. Chen, The Glazman–Krein–Naimark theory for a class of discrete
Hamiltonian systems. J. Math. Anal. Appl. 327(2), 1360–1380 (2007)
278. N.G. Stephen, Transfer matrix analysis of the elastostatics of one-dimensional repetitive
structures. Proc. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci. 462(2072), 2245–2270 (2006)
279. J.C.F. Sturm, Memoire sur une classe d’équations differénces partielles. J. Math. Pures Appl.
1, 373–444 (1836)
280. C.A. Swanson, Comparison and Oscillation Theory of Linear Differential Equations (Aca-
demic Press, New York, NY, 1968)
281. P.
Šepitka,
Theory
of
Principal
Solutions
at
Inﬁnity
for
Linear
Hamiltonian
Systems,
PhD
dissertation,
Masaryk
University,
Brno,
2014.
Available
at
https://is.muni.cz/th/vqad7/?lang=en
282. P. Šepitka, Riccati equations for linear Hamiltonian systems without controllability condition.
Discrete Contin. Dyn. Syst. 39(4), 1685–1730 (2019)
283. P. Šepitka, R. Šimon Hilscher, Minimal principal solution at inﬁnity of nonoscillatory linear
Hamiltonian systems. J. Dynam. Differ. Equ. 26(1), 57–91 (2014)
284. P. Šepitka, R. Šimon Hilscher, Recessive solutions for nonoscillatory discrete symplectic
systems. Linear Algebra Appl. 469, 243–275 (2015)
285. P. Šepitka, R. Šimon Hilscher, Principal solutions at inﬁnity of given ranks for nonoscillatory
linear Hamiltonian systems. J. Dynam. Differ. Equ. 27(1), 137–175 (2015)
286. P. Šepitka, R. Šimon Hilscher, Principal and antiprincipal solutions at inﬁnity of linear
Hamiltonian systems. J. Differ. Equ. 259(9), 4651–4682 (2015)
287. P. Šepitka, R. Šimon Hilscher, Genera of conjoined bases of linear Hamiltonian systems and
limit characterization of principal solutions at inﬁnity. J. Differ. Equ. 260(8), 6581–6603
(2016)

References
585
288. P. Šepitka, R. Šimon Hilscher, Reid’s construction of minimal principal solution at inﬁnity
for linear Hamiltonian systems, in Differential and Difference Equations with Applications,
Proceedings of the International Conference on Differential & Difference Equations and
Applications (Amadora, 2015), S. Pinelas, Z. Došlá, O. Došlý, P.E. Kloeden, (eds.), Springer
Proceedings in Mathematics & Statistics, Vol. 164 (Springer, Berlin, 2016), pp. 359–369
289. P. Šepitka, R. Šimon Hilscher, Comparative index and Sturmian theory for linear Hamiltonian
systems. J. Differ. Equ. 262(2), 914–944 (2017)
290. P. Šepitka, R. Šimon Hilscher, Dominant and recessive solutions at inﬁnity and genera of
conjoined bases for discrete symplectic systems. J. Differ. Equ. Appl. 23(4), 657–698 (2017)
291. P. Šepitka, R. Šimon Hilscher, Focal points and principal solutions of linear Hamiltonian
systems revisited. J. Differ. Equ. 264(9), 5541–5576 (2018)
292. P. Šepitka, R. Šimon Hilscher, Singular Sturmian separation theorems for nonoscillatory
symplectic difference systems. J. Differ. Equ. Appl. 24(12), 1894–1934 (2018)
293. P. Šepitka, R. Šimon Hilscher, Singular Sturmian separation theorems on unbounded intervals
for linear Hamiltonian systems. J. Differ. Equ. 266(11), 7481–7524 (2019)
294. R. Šimon Hilscher, A note on the time scale calculus of variations problems, in Ulmer
Seminare über Funktionalanalysis und Differentialgleichungen, Vol. 14 (University of Ulm,
Ulm, 2009), pp. 223–230
295. R. Šimon Hilscher, Sturmian theory for linear Hamiltonian systems without controllability.
Math. Nachr. 284(7), 831–843 (2011)
296. R. Šimon Hilscher, On general Sturmian theory for abnormal linear Hamiltonian systems, in:
Dynamical Systems, Differential Equations and Applications, Proceedings of the 8th AIMS
Conference on Dynamical Systems, Differential Equations and Applications (Dresden, 2010),
W. Feng, Z. Feng, M. Grasselli, A. Ibragimov, X. Lu, S. Siegmund, J. Voigt (eds.), Discrete
Contin. Dynam. Systems, Suppl. 2011 (American Institute of Mathematical Sciences (AIMS),
Springﬁeld, MO, 2011), pp. 684–691
297. R. Šimon Hilscher, Oscillation theorems for discrete symplectic systems with nonlinear
dependence in spectral parameter. Linear Algebra Appl. 437(12), 2922–2960 (2012)
298. R. Šimon Hilscher, Spectral and oscillation theory for general second order Sturm–Liouville
difference equations. Adv. Differ. Equ. 2012(82), 19 pp. (2012)
299. R. Šimon Hilscher, Eigenvalue theory for time scale symplectic systems depending nonlin-
early on spectral parameter. Appl. Math. Comput. 219(6), 2839–2860 (2012)
300. R. Šimon Hilscher, Asymptotic properties of solutions of Riccati matrix equations and
inequalities for discrete symplectic systems. Electron. J. Qual. Theory Differ. Equ. 2015(54),
16 pp. (2015) (electronic)
301. R. Šimon Hilscher, Eigenvalue comparison for discrete symplectic systems, in Difference
Equations, Discrete Dynamical Systems and Applications, Proceedings of the 20th Inter-
national Conference on Difference Equations and Applications (Wuhan, 2014), ed. by
M. Bohner, Y. Ding, O. Došlý (Springer, Berlin, 2015), pp. 95–107
302. R. Šimon Hilscher, V. Zeidan, Picone type identities and deﬁniteness of quadratic functionals
on time scales. Appl. Math. Comput. 215(7), 2425–2437 (2009)
303. R. Šimon Hilscher, V. Zeidan, Symplectic structure of Jacobi systems on time scales. Int. J.
Differ. Equ. 5(1), 55–81 (2010)
304. R. Šimon Hilscher, V. Zeidan, Symmetric three-term recurrence equations and their symplec-
tic structure. Adv. Differ. Equ. 2010(Article ID 626942), 17 pp. (2010)
305. R. Šimon Hilscher, V. Zeidan, Oscillation theorems and Rayleigh principle for linear
Hamiltonian and symplectic systems with general boundary conditions. Appl. Math. Comput.
218(17), 8309–8328 (2012)
306. R. Šimon Hilscher, P. Zemánek, Deﬁniteness of quadratic functionals for Hamiltonian and
symplectic systems: A survey. Int. J. Differ. Equ. 4(1), 49–67 (2009)
307. R. Šimon Hilscher, P. Zemánek, New results for time reversed symplectic dynamic systems
and quadratic functionals, in Proceedings of the 9th Colloquium on Qualitative Theory of
Differential Equations, Vol. 9, No. 15, Electron. J. Qual. Theory Differ. Equ. (electronic)
(Szeged, 2012), 11 pp.

586
References
308. R. Šimon Hilscher, P. Zemánek, Weyl disks and square summable solutions for discrete
symplectic systems with jointly varying endpoints. Adv. Differ. Equ. 2013(232), 18 pp. (2013)
309. R. Šimon Hilscher, P. Zemánek, Weyl–Titchmarsh theory for discrete symplectic systems with
general linear dependence on spectral parameter. J. Differ. Equ. Appl. 20(1), 84–117 (2014)
310. R. Šimon Hilscher, P. Zemánek, Limit point and limit circle classiﬁcation for symplectic
systems on time scales. Appl. Math. Comput. 233, 623–646 (2014)
311. R. Šimon Hilscher, P. Zemánek, Generalized Lagrange identity for discrete symplectic
systems and applications in Weyl–Titchmarsh theory, in Theory and Applications of Dif-
ference Equations and Discrete Dynamical Systems, Proceedings of the 19th International
Conference on Difference Equations and Applications (Muscat, 2013), ed. by Z. AlSharawi,
J. Cushing, S. Elaydi. Springer Proceedings in Mathematics & Statistics, Vol. 102 (Springer,
Berlin, 2014), pp. 187–202
312. R. Šimon Hilscher, P. Zemánek, Time scale symplectic systems with analytic dependence on
spectral parameter. J. Differ. Equ. Appl. 21(3), 209–239 (2015)
313. R. Šimon Hilscher, P. Zemánek, Limit circle invariance for two differential systems on time
scales. Math. Nachr. 288(5–6), 696–709 (2015)
314. G. Teschl, Oscillation theory and renormalized oscillation theory for Jacobi operators. J.
Differ. Equ. 129, 532–558 (1996)
315. G. Teschl, Jacobi Operators and Completely Integrable Nonlinear Lattices (AMS Mathemat-
ical Surveys and Monographs, Providence, RI, 1999)
316. Y. Tian, Equalities and inequalities for inertias of Hermitian matrices with applications. Linear
Algebra Appl. 433, 263–296 (2010)
317. Y. Tian, Rank and inertia of submatrices of the Moore–Penrose inverse of a Hermitian matrix.
Electron. J. Linear Algebra 20, 226–240 (2010)
318. P. Van Dooren, The computation of Kronecker’s canonical form of a singular pencil. Linear
Algebra Appl. 27, 103–140 (1979)
319. P. Van Dooren, P. Dewilde, The eigenstructure of an arbitrary polynomial matrix. Computa-
tional aspects. Linear Algebra Appl. 50, 545–579 (1983)
320. G. Verghese, P. Van Dooren, T. Kailath, Properties of the system matrix of a generalized
state-space system. Int. J. Control 30(2), 235–243 (1979)
321. M. Wahrheit, Eigenvalue problems and oscillation of linear Hamiltonian systems. Int. J.
Differ. Equ. 2, 221–244 (2007)
322. Y. Wang, Y. Shi, Eigenvalues of second-order difference equations with periodic and
antiperiodic boundary conditions. J. Math. Anal. Appl. 309(1), 56–69 (2005)
323. Y. Wang, Y. Shi, G. Ren, Transformations for complex discrete linear Hamiltonian and
symplectic systems. Bull. Aust. Math. Soc. 75(2), 179–191 (2007)
324. Y. Wu, Symplectic transformation and symplectic difference schemes. Chin. J. Numer. Math.
Appl. 12(1), 23–31 (1990)
325. L. Xue-Shen, Q. Yue-Ying, H. Jian-Feng, D. Pei-Zhu, Recent progress in symplectic
algorithms for use in quantum systems. Commun. Comput. Phys. 2(1), 1–53 (2007)
326. V.A. Yakubovich, Arguments on the group of symplectic matrices. Mat. Sb. 55, 255–280
(1961)
327. V.A. Yakubovich, Oscillatory properties of solutions of canonical equations. Mat. Sb. 56, 3–
42 (1962)
328. V.A. Yakubovich, V.M. Starzhinskii, Linear Differential Equations with Periodic Coefﬁcients,
2 volumes (Wiley, New York, 1975)
329. Y. Yalçin, L. Gören Sümer, S. Kurtulan, Discrete-time modeling of Hamiltonian systems.
Turk. J. Electr. Eng. Comput. Sci. 23, 149–170 (2015)
330. V. Zeidan, Continuous versus discrete nonlinear optimal control problems, in Proceedings
of the 14th International Conference on Difference Equations and Applications (Istanbul,
2008), ed. by M. Bohner, Z. Došlá, G. Ladas, M. Ünal, A. Zafer (U˘gur-Bahçe¸sehir University
Publishing Company, Istanbul, 2009), pp. 73–93
331. V. Zeidan, Constrained linear-quadratic control problems on time scales and weak normality.
Dynam. Syst. Appl. 26(3–4), 627–662 (2017)

References
587
332. M.I. Zelikin, Control Theory and Optimization. I. Homogeneous Spaces and the Riccati
Equation in the Calculus of Variations. Encyclopaedia of Mathematical Sciences, Vol. 86
(Springer, Berlin, 2000)
333. P. Zemánek, Discrete trigonometric and hyperbolic systems: An overview, in Ulmer Seminare
über Funktionalanalysis und Differentialgleichungen, Vol. 14 (University of Ulm, Ulm,
2009), pp. 345–359
334. P. Zemánek, Rofe-Beketov formula for symplectic systems. Adv. Differ. Equ. 2012(104), 9
pp. (2012)
335. P. Zemánek, S. Clark, Characterization of self-adjoint extensions for discrete symplectic
systems. J. Math. Anal. Appl. 440(1), 323–350 (2016)
336. A. Zettl, Sturm–Liouville Theory (AMS Mathematical Surveys and Monographs, Providence,
RI, 2005)
337. Z. Zheng, Invariance of deﬁciency indices under perturbation for discrete Hamiltonian
systems. J. Differ. Equ. Appl. 19(8), 1243–1250 (2013)

Index
Admissible
pair, 31, 100, 106, 211, 298
sequence, 26
Boundary conditions
Dirichlet, 305
joint
linear, 323
nonlinear, 304, 398
separated
linear, 324
nonlinear, 304
Calculus of variations problem, 25, 78
Comparative index, 199, 216
deﬁnition, 150
dual, 152
duality principle, 159
estimates, 159
limit at −∞, 569
limit at ∞, 556, 561
main theorem, 156, 161
properties, 150, 153
triangle inequality, 157
Comparative index for symplectic matrices
additional properties, 191–193
basic properties, 173, 178
computation
general case, 180, 182
special case, 181
invertible block A, 185, 186
invertible block B, 183
for linear Hamiltonian system, 187
for Sturm–Liouville equation, 183, 184,
189
Conjoined basis, 40, 84
with constant kernel, 464, 467, 470, 476,
480, 484, 485, 491, 493, 501, 521,
544, 551, 563
contained, 483–485, 491, 493, 511, 520,
521
with given rank, 464, 482, 490, 514, 520
intermediate, 495
maximal, 495
minimal, 495, 496, 507, 508, 540
natural, 117, 316, 382
normalized, 85
representation, 462, 470
Conjugate point, 49
Controllability matrix, 100
Controllable system, 42, 79, 128, 130, 133,
134, 510, 521, 531, 552, 560
Disconjugate equation, 7
Disconjugate system, 94, 104, 105
continuous case, 47, 49, 52
Dominant solution at −∞
deﬁnition, 562
Dominant solution at ∞, 11, 130, 133, 147,
550, 571
block diagonal construction, 523
classiﬁcation, 528
deﬁnition, 517
examples, 545
existence, 520
genus, 528
limit characterization, 543
© Springer Nature Switzerland AG 2019
O. Došlý et al., Symplectic Difference Systems: Oscillation and Spectral Theory,
Pathways in Mathematics, https://doi.org/10.1007/978-3-030-19373-7
589

590
Index
maximal, 518, 535, 540, 543, 551
minimal, 518, 521, 535
properties, 518–521, 531, 532
Eigenvalue problem
augmented, 309, 327
comparison of eigenvalues, 571
for Dirichlet endpoints, 16, 262, 338, 399,
400, 411
extended, 305, 326
intermediate, 454, 456
for joint endpoints, 304, 309, 310, 327
linear dependence, 323, 326, 327, 339, 394
for linear Hamiltonian system, 376
for separated endpoints, 304, 305, 307, 325,
326
Euler-Lagrange equation, 27, 36
Feasible pair, 30
Finite eigenfunction
for Dirichlet endpoints, 267, 335
for joint endpoints, 332
linear dependence, 332–334
for separated endpoints, 336
Finite eigenvalue, 338, 356
for Dirichlet endpoints, 265, 335
existence, 298, 299
interlacing properties, 571
for joint endpoints, 319, 331
linear dependence, 331, 335
properties, 266, 273, 332–334
for separated endpoints, 316, 335
variational description, 339
Focal point, 6
nonexistence backward, 96, 204, 222, 227,
240, 258, 562, 563
nonexistence forward, 92, 103, 107, 118,
119, 146, 204, 211, 221, 227, 240,
465, 484, 485, 491, 493, 501, 521,
544, 551
weighted forward, 377
Generalized zero, 4, 6, 94, 96, 210
of Wronskian, 399
of Wronskian, weighted, 400, 408
Genus of conjoined bases, 551
defect, 537
deﬁnition, 523
maximal, 524, 528, 552
minimal, 524, 530, 552, 560
near −∞, 562, 563, 568
for principal solution, 560, 568
properties, 524, 525, 528
rank, 537, 551, 563, 568
Hamiltonian matrix, 38
Hyperbolic system, 88, 142, 143
Identically normal system, 42
Image condition, 103, 110, 116, 117, 119
Index of block symmetric matrix, 166, 168
and comparative index, 167, 171
Index theorem, 81
application to spectral theory, 294
Inequalities for eigenvalues
for Dirichlet endpoints, 436
for joint endpoints, 438
for separated endpoints, 435
Inequalities for spectral functions, 433
for joint endpoints, 438, 440, 441
for separated endpoints, 435, 450, 451
Interlacing of eigenvalues
for Dirichlet endpoints, different intervals,
458
for joint endpoints, 441, 443, 444
for separated endpoints, 452, 454, 455, 457
Jacobi equation, 27, 41, 89, 146
Kernel condition, 92, 103, 106, 211
Legendre condition, 42, 53, 80, 257, 258
Linear Hamiltonian system, 28, 38, 87, 146,
187, 239, 302, 376, 395
continuous, v, 37, 40, 53, 79, 571, 572
Majorant condition, 238–240
for linear Hamiltonian systems, 239
for spectral problems
joint endpoints, 438
low block triangular perturbation, 437
matrix Sturm–Liouville equation, 437
separated endpoints, 435
Matrix pencil, 338
Monotone matrix-valued function, 73
index theorem, 73, 74
limit theorem, 74, 75
Monotonicity in parameter
assumption, 262, 376

Index
591
Cayley transformation, 368
comparative index, 362
for joint endpoints, 304, 324
linear, 323, 324
nonlinear, 304
properties, 263, 264
for separated endpoints, 304, 324
for symplectic matrices, 323, 362
Moore–Penrose generalized inverse, 58, 79
Multiplicity of ﬁnite eigenvalue
algebraic, 265, 316, 320, 331
for Dirichlet endpoints, 331
geometric, 267, 332
linear dependence, 331
relation of algebraic and geometric, 267,
332
Multiplicity of focal point
backward, 203, 209, 258, 548
and comparative index, 205, 206, 258
connection forward and backward, 204,
224
continuous case, 42
estimates, 204
forward, 202, 208, 548
at −∞, 563
at ∞, 550
properties, 203
symplectic transformation, 248, 250, 251
Nonoscillatory conjoined basis
continuous case, 43
limit properties, 227
Nonoscillatory equation, 9, 12
Nonoscillatory system, 94, 130, 133, 134, 141,
227, 246, 463, 511, 514, 520, 521,
524, 525, 528, 530–532, 535, 538,
543, 544, 554, 556–558, 560, 561,
566–568
at −∞, 247, 563–569
continuous case, 43, 51, 52
Nonprincipal solution
at ∞, continuous case, 52
Optimal control problem, 29, 78
Order of abnormality, 461, 476
maximal, 461, 501, 505, 551, 560, 561
Orthogonal projector, 59, 464, 467, 482
Oscillation theorem
constant rank
global, Dirichlet endpoints, 23, 273,
298, 337, 352
global, joint endpoints, 320, 322
global, separated endpoints, 317, 319
linear dependence, 303, 337, 352
for linear Hamiltonian system, 302
local, 19, 21, 271
for Sturm–Liouville equation, 10, 23,
300, 301, 303
global, continuous case, 53
nonconstant rank
generalized global, 380
generalized local, 378
global, Dirichlet endpoints, 357
global, separated endpoints, 385
local, 354, 373, 383, 389
Oscillatory conjoined basis
continuous case, 43
Oscillatory equation, 9
Oscillatory system, 94, 227, 464
continuous case, 43, 51
P -condition, 92, 106, 110, 117, 119, 211
Phase ﬂow, 38
Picone identity, 6, 102
continuous case, 5, 46
extended, 340
Pontryagin maximum principle
weak, 31, 78
Principal solution, 85, 105, 119, 217, 218, 222,
225, 243, 265, 331, 351, 356, 388,
401, 437, 461, 462, 476, 530, 531,
548, 552, 558, 560, 565, 566, 568,
569
at ∞, continuous case, 52, 79
Pseudoinverse matrix, 58, 79
Quadratic functional, 26, 31, 99, 137
augmented, 315
continuous case, 4, 46, 47
for Dirichlet endpoints, 105, 107, 110, 331,
436
extended, 313
for joint endpoints, 119, 315, 328, 438
linear dependence, 328–331
nonnegative deﬁnite, 27, 32, 49, 110, 115,
117, 119, 147
positive deﬁnite, 7, 27, 32, 47, 105, 107,
117, 119, 124, 211, 313, 315,
329–331, 338, 342, 344, 346, 436,
438
for separated endpoints, 117, 313, 329, 330,
436
transformation, 313, 315, 328, 329

592
Index
Rayleigh principle
for Dirichlet endpoints, 342, 395
for joint endpoints, 346
for separated endpoints, 344
Recessive solution at −∞
deﬁnition, 562
minimal, 565–568
Recessive solution at ∞, 11, 12, 130, 133, 134,
147, 550, 571
block diagonal construction, 515
classiﬁcation, 525, 528
deﬁnition, 509
examples, 516, 545
existence, 514
genus, 524, 525
limit characterization, 532, 535
maximal, 510, 535
minimal, 509, 511, 521, 530, 535, 538, 548,
550, 558, 565, 567, 568
properties, 510, 511, 514, 521
Reid’s construction, 538, 572
Reciprocity principle, 16, 255
examples, 256
generalized, 253, 254, 260
Reid roundabout theorem, 7, 104, 108, 147
continuous case, 47
Relative oscillation numbers
basic properties, 235
deﬁnition, 233
for Dirichlet endpoints, 402, 415
interpretation, 233
for joint endpoints, 429
for matrix Sturm–Liouville problems, 415
for principal solutions, 241
for separated endpoints, 424
Relative oscillation theorem, 569, 571
for Dirichlet endpoints, 401
for joint endpoints, 431
for low block triangular perturbations, 409
for matrix Sturm–Liouville problems, 417
for separated endpoints, 425
Relative oscillation theory, 259, 397, 570
continuous, 259
Renormalized oscillation theorem
for Dirichlet endpoints, 403
for joint endpoints, 431
for low block triangular perturbations, 409
for matrix Sturm–Liouville problems, 418
for separated endpoints, 426
Riccati inequality, 120, 121, 124, 147, 221,
222, 238, 240, 241
continuous case, 47
Riccati matrix equation, 80, 97, 134
continuous case, 45, 47
explicit, 107
implicit, 99, 105, 147
Riccati operator, 97
Self-reciprocal system, 139
Singular comparison theorems
limit properties, 247
S-matrix, 129, 562
deﬁnition, 465
dominant solution at inﬁnity, 518
properties, 484, 501, 503, 505
Solution
degenerate, 266
equivalent, 482
nondegenerate, 266
nonoscillatory, 9
oscillatory, 9
representable, 480
Sturmian comparison theorem, 347, 349, 395
global explicit relations, 237
inequalities for focal points, 244
local explicit relations, 230, 233, 369
for principal solutions, 243
singular, 246, 247
for triangular perturbations, 231
Sturmian separation theorem, 211, 350, 395,
549
continuous case, 43
estimates for focal points, 218–220, 224,
226
global explicit relations, 217, 218
local explicit relations, 216
for principal solutions, 225, 226, 549, 566
singular, 228, 554, 558, 564, 566, 567, 572
Sturm–Liouville equation, 3, 40, 78, 90, 92,
94, 183, 184, 189, 259, 300, 301,
303, 385, 399, 411, 569
Symplectic difference system, v, 28, 83
augmented, 309, 327
Cayley, 369
extended, 305, 326
linear dependence, 303, 322
nonoscillatory, 463
oscillatory, 464
time-reversed, 86, 108, 489
without controllability, 460
Symplectic matrix, 38, 55, 62, 79, 83
depending on parameter, 67, 261, 262
fundamental, 462, 480
Three-term recurrence equation, 89, 146
T -matrix, 130, 550, 551, 557, 562, 563

Index
593
classiﬁcation, 508
deﬁnition, 465
dominant solution at inﬁnity, 521
properties, 503, 507, 537
Transformation
Bohl, 49, 139
Cayley, 368
global explicit relations, 250
hyperbolic, 141, 148
of joint boundary conditions, 309, 310, 327
local explicit relations, 248, 250
low block triangular, 252
Prüfer, 14, 49, 50, 78, 144, 148
of separated boundary conditions, 305, 307,
325, 326
symplectic, 44, 135, 156, 248
J -transformation, 251
trigonometric, 49, 51, 139, 148, 260
Trigonometric system, 86, 139
continuous, 50
Wronskian, 42, 84, 150, 399, 480, 544, 550
forward difference, 230

