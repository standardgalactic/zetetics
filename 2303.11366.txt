Reﬂexion: an autonomous agent with dynamic
memory and self-reﬂection
Noah Shinn
Northeastern University
Boston, MA
shinn.n@northeastern.edu
Beck Labash
Northeastern University
Boston, MA
labash.b@northeastern.edu
Ashwin Gopinath
Massachusetts Institute of Technology
Cambridge, MA
agopi@mit.edu
Abstract
Recent advancements in decision-making large language model (LLM) agents have
demonstrated impressive performance across various benchmarks. However, these
state-of-the-art approaches typically necessitate internal model ﬁne-tuning, external
model ﬁne-tuning, or policy optimization over a deﬁned state space. Implementing
these methods can prove challenging due to the scarcity of high-quality training
data or the lack of well-deﬁned state space. Moreover, these agents do not possess
certain qualities inherent to human decision-making processes, speciﬁcally the
ability to learn from mistakes. Self-reﬂection allows humans to efﬁciently solve
novel problems through a process of trial and error. Building on recent research, we
propose Reﬂexion, an approach that endows an agent with dynamic memory and
self-reﬂection capabilities to enhance its existing reasoning trace and task-speciﬁc
action choice abilities. To achieve full automation, we introduce a straightforward
yet effective heuristic that enables the agent to pinpoint hallucination instances,
avoid repetition in action sequences, and, in some environments, construct an inter-
nal memory map of the given environment. To assess our approach, we evaluate
the agent’s ability to complete decision-making tasks in AlfWorld environments
and knowledge-intensive, search-based question-and-answer tasks in HotPotQA
environments. We observe success rates of 97% and 51%, respectively, and provide
a discussion on the emergent property of self-reﬂection.
1
Introduction
Mastering decision-making and knowledge-intensive search tasks in novel environments is a crucial
skill set for large-scale natural language agents. LLMs such as OpenAI’s GPT-3 (Brown et al.,
2020), Google’s PaLM (Chowdhery et al., 2022), and others have achieved impressive results on
various benchmarks (Kaplan et al., 2020; Rae et al., 2021; Nakano et al., 2021; Kojima et al., 2022;
Ouyang et al., 2022; Chung et al., 2022). These models exhibit human-like abilities to understand
tasks in given environments, marking signiﬁcant progress in the ﬁeld of natural language processing.
Grounding complex tasks in natural language allows agents to overcome high syntactic barriers that
may result in false-negative errors. However, learning optimal policies for natural language RL agents
is challenging due to vast and mostly unbound state spaces.
Several decision-making approaches have been proposed to enable natural language agents to select
their next action without a learned policy in text-based environments. Chain-of-thought (CoT)
Preprint. Under review.
arXiv:2303.11366v1  [cs.AI]  20 Mar 2023

reasoning leverages emergent properties such as reasoning and commonsense to solve tasks in a
single action but reasoned through several steps (Huang et al., 2022a; Wei et al., 2022b). However,
the accuracy of these approaches decrease as the number of required subtasks increase as the model
is more prone to hallucinate over longer sequences. ReAct (Yao et al., 2023) is an approach that
utilizes emergent properties in LLMs, such as verbal reasoning traces, to solve problems by allowing
the agent to reason and act, proving substantial performance in various text-based benchmarks. In
addition, several recent works have aimed to allow natural language agents to exhibit reﬂective-like
qualities to infer more intuitive future actions. The Describe, Explain, Plan, and Select (DEPS)
approach uses multi-step reasoning and sub-task error correction to solve long-range tasks (Wang
et al., 2023). DEPS demonstrates impressive performance due to its ability to explain mistakes in
sub-tasks within trials, but relies on immediate failure detection for subtasks and cannot explain
mistakes that may have developed over a long range of actions and subtasks. Huang et al. (2022b)
use inner monologue to further process next decisions within closed-loop feedback environments.
Huang et al. (2022b) use a success detection approach in which the agent will explicitly know if
an executed action has led to a successful state. (Huang et al., 2022a; Haluptzok et al., 2022) use
self-generated solutions to ﬁne-tune an LLM to improve performance without access to a labeled
dataset. Although these approaches have achieved remarkable accuracy across various decision-
making tasks or knowledge-intensive tasks, they lack the ability to utilize success detection cues to
improve their behavior over long trajectories. In addition, they often succumb to common mistakes,
such as repetitive action choice, cyclic hallucination, or random action choice. In other words, while
these methods achieve state-of-the-art results, a small subset of tasks remain unsolved due to the
agent’s inability to learn from its own mistakes over long trajectories to correct future action sequence
planning and execution.
To address common failure points, human-in-the-loop (HITL) approaches have been commonly
used to improve performance Fan et al. (2022); Wu et al. (2022) Yao et al. (2023) brieﬂy explore a
human-in-the-loop (HITL) approach to redirect the agent’s reasoning trace after erroneous actions.
While this approach achieves improved performance with minimal human intervention, it is not fully
autonomous by its reliance on human trainers to monitor trajectories at each time step. Large-scale
LLMs have shown to exhibit advanced human-like qualities that enable natural language agents to
solve tasks in more intuitive ways (Wei et al., 2022a). We hypothesize that LLMs possess an emergent
property of self-reﬂection and could effectively utilize self-optimization grounded in natural language
if given the opportunity to autonomously close the trial loop.
To test our hypothesis, we equip an LLM-based agent with a self-reﬂective LLM and a simple heuristic
for detecting hallucination and inefﬁcient action execution in an approach named Reﬂexion. We then
challenge the agent to learn from its own mistakes on the AlfWorld text-based benchmark (Shridhar
et al., 2021) and the HotPotQA question-answering benchmark (Yang et al., 2018). This results in
improved performance in decision-making and knowledge-intensive tasks. When combined with
the ReAct problem-solving technique (Yao et al., 2023), self-reﬂection guides the Reﬂexion agent
to achieve a 97% success discovery rate on the AlfWorld benchmark in just 12 autonomous trials,
outperforming the base ReAct agent with an accuracy of 75%. We also evaluated a Reﬂexion-based
ReAct agent on 100 questions from HotPotQA. The agent achieved a 51% success discovery rate by
iteratively reﬁning its content search and content extraction by receiving advice from its memory,
outperforming a base ReAct agent by 17%. It is essential to emphasize that Reﬂexion is not designed
to achieve near-perfect accuracy scores; instead, its goal is to demonstrate learning through trial and
error to enable discovery in tasks and environments previously considered nearly impossible to solve.
2
Architecture
The abstract architecture of our Reﬂexion agent is depicted in Figure 1. In this study, Reﬂexion
leverages ReAct (Yao et al., 2023), but any decision-making approach can be used in future imple-
mentations. In the ﬁrst trial, the agent is given a task from the environment which composes the initial
query. Then, the agent executes a series of actions generated by an LLM and receives observations
and rewards from the environment. For environments that provide descriptive or continuous rewards,
we constrain the output to a simple binary success status to ensure applicability; reward constraining
is explained in further detail later. After every action at, the agent computes a heuristic h, which may
suggest self-reﬂection. If self-reﬂection is recommended, the agent queries an LLM to reﬂect on its
current task, trajectory history, and last reward (which is simply the fact that the agent had failed in
2

Figure 1: Reﬂexion can be added to any decision-making approach. We enable ReAct agents to use
self-reﬂection to improve their own performance.
the given environment under the binary reward constraint). Then, the agent resets the environment
retry in a subsequent trial. If no self-reﬂection is advised, the agent adds the at and ot to its trajectory
history and queries the LLM for the next action. In practice, we set a hyperparameter limit of three
maximum reﬂections to be stored in the agent’s memory to avoiding queries beyond the limit of the
LLM. If the agent exceeds the maximum number of trials, fails to improve performance between two
consecutive trials, or completes the task, the run is terminated.
2.1
Setup
In a typical RL scenario, an agent is tasked to solve a problem by executing actions to learn from
observations within an environment. At time step t in state st, the agent receives an observation ot
from the environment and executes an action at based on its current policy π(at|ct). In text-based
environments, st considers ct, which is the context given to the agent based on its current state and
trajectory history, consisting of past observations and actions. In this study, we do not learn a policy
over a state space. In addition, we aim to constrain speciﬁc reward information given to the agent to
(1) prove the effectiveness of Reﬂexion in alignment with other works and (2) preserve the ability
to generalize the approach to a broad range of problems. Additionally, we equip the agent with a
heuristic function to detect common modes of failure.
2.2
Heuristics
A heuristic h(st, at, Ω, ε, [ao, o0, . . . , at−1, ot−1]) is deﬁned to tell the agent when to reﬂect, where t
is the time step, st is the current state, Ωand ε are hyperparameters for the maximum number of repet-
itive action cycles and the maximum number of total actions allowed, and [ao, o0 . . . , at−1, ot−1]) is
the trajectory history.
h(st, at, Ω, ε, [a0, o0 . . . , at−1, ot−1]) =



1
repeat([ao, o0 . . . , at−1, ot−1]) > Ω
1
t > ε
0
o′wise
repeat is a simple function that determines the number of repetitive action cycles that yield the same
observation(s). Ωis the maximum number of identical cycles, which is set to detect hallucination of
repeated consecutive actions. ε enforces efﬁcient planning by restricting the maximum number of
actions allowed in an environment per trial. h is designed to replace the role of a human-in-the-loop
in which a human trainer monitors the running trajectory to detect signs of hallucination or inefﬁcient
planning.
2.3
Reﬂexion
If the heuristic h suggests reﬂection at t, the agent initiates a self-reﬂective process on its current
state st, last reward rt, previous actions and observations [a0, o0, . . . , at, ot], and the agent’s existing
working memory, mem. The reﬂection loop aims to help the agent correct common cases of
hallucination and inefﬁciency through trial and error. The model used for self-reﬂection is an LLM
prompted with two-shot learning examples of domain-speciﬁc failed trajectory and ideal reﬂection
3

pairs. Few-shot examples for AlfWorld and HotPotQA reﬂections can be found in A.1. To prevent
the agent from memorizing correct AlfWorld trajectories or HotPotQA answers, we do not grant
access to domain-speciﬁc solutions for the given problems. This approach encourages the agent to
devise creative and novel techniques for future attempts.
Self-reﬂection is modeled in the following equation:
reflection = LLM(st, rt, [a0, o0, . . . , at, ot] , mem)
Finally, we add the reﬂection to the agent’s memory, reset the environment, and start the next trial.
2.4
Reward model
Typically, designing or training an effective yet broadly-applicable reward model can be challenging.
In this work, we limit the agent to a binary reward model. A binary reward model is a type of reward
function that assigns a value of 0 or 1 to an action taken by the agent in the current state. 1 indicates a
successful outcome and 0 indicates an unsuccessful outcome. A binary reward function was chosen
to tightly constrain the agent’s knowledge to the observations from the world and the status of success
or failure in a given environment, as opposed to a multi-value or continuous output from a more
descriptive reward model with which the agent may use to evaluate its current performance. By
limiting the agent’s knowledge to a binary success status within an environment, we force the agent
to make meaningful inferences about its performance without external input in order to effectively
improve its future decisions. Lastly, binary reward models are largely applicable to a variety of
language problems, such as code generation and code debugging. We propose that Reﬂexion agents
may be able to learn faster than random action sampling in arbitrary environments in which accurate
reward functions may be hard to design or compute. In AlfWorld environments, we query the
AlfWorld engine at every time step to check if the current state is a successful state. In the HotPotQA
benchmark, we grade the response from the agent using exact match (EM) after an answer is given.
2.5
Action space
Deﬁning a natural language action space for large language models is not straightforward, as the
number of possible actions is vast. However, constraining the action space to a discrete set of actions
may lead to unintended action choices. There is a trade-off between minimizing false positives
in which the agent blindly chooses the most similar action, which could be largely semantically
different from its initial intent, and minimizing false negatives in which the agent chooses an action
that is semantically similar to the closest action but uses a different syntactic representation. We
follow the work of (Yao et al., 2023) in which an action space ˆ
A = A ∪L is chosen, where L is the
space of language. We handle false negatives by responding with "Invalid Action" observations. In
the AlfWorld and HotPotQA benchmarks, we demonstrate permissible action syntax by few-shot
prompting the agent with two successful trajectories.
3
Evaluation
3.1
AlfWorld
AlfWorld is a suite of text-based environments that challenge an agent to solve multi-step tasks in a
variety of interactive environments based on TextWorld (Côté et al., 2019). In every environment,
the agent can choose to execute an action at from a list of admissible actions A∫,⊔and receive an
observation ot and reward R(st, at) from the environment which determines its next state. As stated
previously, we ﬁlter the reward to a binary reward by discarding the default reward output and simply
query the AlfWorld engine to determine if the task has been successfully completed.
AlfWorld provides six different tasks and over 3000 environments, which challenge the agent to
understand the goal task, develop a sequential plan of subtasks, and execute the actions in the given
environment. Within trials, we implement the ReAct problem solving strategy as Yao et al., 2022 has
shown great performance across a variety of sequential decision-making tasks. ReAct is an approach
that allows the agent to reason and act by explicitly stating current thoughts and executing actions
based on those thoughts. At every time step, the agent can execute a think: <thought> action to
verbalize its internal thought trace, or <action> to elicit a response from the environment. The action
4

space in each state is not explicitly deﬁned, which allows the agent to have full expression in its next
moves. To avoid syntactic errors, we provide two domain-speciﬁc few-shot trajectories to the agent.
Setup
Following (Yao et al., 2023), we ran the agent in 134 AlfWorld environments across six
different tasks, including ﬁnding hidden objects (e.g., ﬁnding a spatula in a drawer), moving objects
(e.g., moving a knife to the cutting board), and manipulating objects with other objects (e.g., chilling
a tomato in the fridge). To closely replicate baseline results, we used the same hand-selected few-shot
trajectory examples with GPT-3.0 for the LLM. At every time step, we evaluated h to determine if the
agent should self-reﬂect. In the baseline run, if reﬂection was suggested, we skipped the reﬂection
process, reset the environment, and started a new trial. In the Reﬂexion run, we allowed the agent
to use self-reﬂection to ﬁnd its mistake, reset the environment, and start a new trial. Intuitively, we
wanted to distinguish learning curves from self-improvement to simple probabilistic success over
time. AlfWorld tasks, ReAct few-shot prompts, and Reﬂexion examples are included in A.1.
Results
We ran the agent without reﬂection for a single trial to establish a starting point, which
achieved 63% accuracy (Fig. 2), which is in-line with the results from (Yao et al., 2023) Then, we
performed the runs with and without Reﬂexion starting from the initial trial until the agent’s ability
to improve was saturated, which we deﬁned as having a 0% improvement between two consecutive
trials.
Figure 2: AlfWorld performance across 134 tasks showing cumulative proportions of solved tasks
and error trajectories.
Given the ability to reﬂect, the agent used ReAct to solve 97% of the given environments in 12 trials,
failing to solve only 4 out of 134 tasks. It is important to note that the comparison between Reﬂexion
+ ReAct and ReAct is not one of accuracy for state-of-the-art performance, but rather a demonstration
of improved accuracy by self-improved learning rather than success by retry.
3.2
HotPotQA
HotPotQA is a Wikipedia-based dataset with 113k question-and-answer pairs that challenge agents
to parse content and reason over several supporting documents. Equipped with a Wikipedia search
engine, the agent must perform relevant searches to extract crucial information to answer questions
that may require information presented across multiple documents. Then, the agent must provide EM
answers given the context. In this study, we use EM to quickly automate the grading process between
trials. We experimented with methods to relax syntactical requirements by performing semantic
5

matching between inferred answers and correct answers, but observed several cases of false positives
and negatives that required further grading by a human, which rendered the retry process inefﬁcient
and subjective.
Setup
To replicate results from the ReAct study (Yao et al., 2023), we prompted the agent with
six few-shot examples of successful trajectories. We also equipped the agent with a Wikipedia API to
perform one of three actions at each time step: Search[entity], which returns the ﬁrst ﬁve sentences
of the wiki page or a list of the ﬁve most similar wiki pages according to the entity; Lookup[string],
which returns the sentence following the query within a wiki page; and Finish[answer], which
attempts to complete the task with the provided answer. Then, as stated before, EM was used as the
binary reward model. The baseline and reﬂexion runs followed the same constraints as the AlfWorld
benchmark. In the self-reﬂective process, the agent was given two few-shot example trajectories with
reﬂections. Reﬂexion few-shot prompts are shown in A.1. However, in this experiment, the heuristic
was not necessarily needed as we observed that the resulting trajectory lengths were highly dependent
on the trajectory lengths provided in the few-shot examples.
In the baseline run, if the agent provided an incorrect answer, the environment was reset and the agent
simply retried. In the reﬂexion run, if the agent provided an incorrect answer, the agent was given a
chance to self-reﬂect on its past searches, lookups, observations, working memory, and ﬁnal answer,
reset the environment, and try again with a new working memory.
Results
We tested the base agent and Reﬂexion agent on a 100-question dataset. Our initial plan
was to run the agents until they failed to improve their accuracy between consecutive trials. However,
the base agent did not improve in accuracy between trials 1 and 2. Therefore, we ran the Reﬂexion
agent until no improvement between two trials was observed, which occurred between trials 6 and 7
(Fig. 3), then ran the base agent for the equivalent number of trials, allowing retries on previously
incorrect answers. While the base ReAct agent achieved 34% accuracy and the Reﬂexion agent
achieved 32% accuracy in the ﬁrst trials (Fig. 3), the Reﬂexion agent was able to outperform the base
ReAct agent over the course of 7 trials.
Figure 3: HotPotQA performance across 100 question and answer pairs showing cumulative propor-
tions of correct EM answers.
Reﬂexion enabled the agent to successfully answer 54% of the questions from the dataset, outper-
forming the base ReAct agent by 20% (Fig. 3) and its ﬁrst trial attempt by 22%. In fact, by simply
retrying, the base ReAct agent was not able to successfully answer any additional questions. On the
6

other hand, the Reﬂexion agent was able to improve on its performance not only between trials 1 and
2 but between every consecutive trial, demonstrating that the growing, dynamic working memory
promoted learning through trial.
4
Discussion
4.1
Reﬂexion promotes learning through discovery
Fig. 4 shows two consecutive trials performed in the same environment. In the ﬁrst trial (Fig. 4), the
agent demonstrates unfamiliarity with the given environment as it searches for relevant objects in
every location. Eventually, the agent ﬁnds the desk lamp but its decision-making process is faulty as it
becomes distracted by the presence of the mug rather than focusing on the desk lamp. When the agent
reﬂects on its past behavior (Fig. 4), it recognizes a ﬂaw in its initial sequential plan. Additionally,
the agent chooses to remember that the desk lamp was found on desk 1. In the next trial (Fig. 4), the
agent is more efﬁcient as it recalls that the desk lamp was on desk 1. Then, it devises a more accurate
plan to ﬁnd the mug after obtaining the desk lamp. The agent succeeds after 2 trials, or 1 reﬂection.
In the ReAct-only approach, the agent solves the task on the 6th trial.
4.2
Hallucination vs. Inefﬁcient Planning
Next, we explored the reasons for failure in AlfWorld runs with and without Reﬂexion. Hallucination
is the most common reason for failure (Fig. 2). In the AlfWorld benchmark, we deﬁned hallucination
as the occurrence of two or more consecutive identical actions in which the environment responded
with the same observation and inefﬁcient planning as the occurrence of a trajectory in which the
agent executed more than 30 actions without reaching a successful state. We deﬁned inefﬁcient
planning as a mode of failure to encourage the agent to solve tasks by demonstrating strong, concise
decision-making ability—not merely attempts to execute every admissible action. An example of
hallucination is shown in Fig. 4 in which the agent hallucinates due to its belief that it has found the
desklamp on desk 2. Therefore, it executes the same action twice, believing that the next best action
is to use the desklamp.
Inefﬁcient planning errors occur in trajectories in which the agent reaches action 30 without showing
any signs of improvement. While the agent may be constructing a mental map of the environment as it
discovers and observes new items and locations, we penalize this behavior as it does not demonstrate
strong decision-making ability. Fig. 2 shows that although the agent can solve additional tasks
through trial, it still converges to the same rough 3:1 hallucination to inefﬁcient planning ratio as in
Trial 1. However, with reﬂection Fig. 2, the agent can correct all of its mistakes related to inefﬁcient
planning in all but four mistakes related to hallucination.
4.3
Reﬂexion Enables More Intuitive Search Queries
Given a knowledge-intensive task and a search engine, a key component to an agent’s success is its
ability to form intuitive search queries. Fig. 5 shows an improvement between trials in which the
agent learns from its past mistakes.
In Trial 1, the agent searches for "Grown-Ups" and "’Allo ’Allo!" independently and is unable to
produce a correct answer. In Trial 2, the agent reﬁnes its second search to target one of the actors
listed in the ﬁrst search. Following, the agent successfully produces the correct answer. In the baseline
run, the agent performs the same two searches and receives the same responses from the environment.
Without Reﬂexion, the agent was unable to learn that its chosen method was unproductive.
Reﬂexion describes a highly applicable approach that may be used to equip agents to solve a variety of
complex tasks. A Reﬂexion agent must have access to a heuristic for termination and a binary reward
model. In the AlfWorld experiment, we used a simple heuristic to demonstrate the applicability of
the approach. However, in other experiments, a more speciﬁc heuristic could be applied to capture a
larger set of erroneous cases. For example, the heuristic can be designed to match actions that are
associated with high probabilities of failure. In a household environment, the heuristic could signal
termination if the agent enters a room that would not contribute to a successful plan.
The second requirement for a Reﬂexion agent is a binary reward model. In the AlfWorld experiment,
we ﬁltered the reward from the AlfWorld environment at every step to determine if the agent had
7

Environment: You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk
2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a
laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1.
Task: examine the mug with the desklamp.
Trial #1
> think: To solve the task, I need to ﬁnd and take a mug, then ﬁnd and use a desklamp.
OK.
> think: First I need to ﬁnd a mug. A mug is more likely to appear in drawer (1-6), desk (1-2), shelf
(1-6), garbagecan (1), laundryhamper (1). I can check one by one, starting with drawer 1.
OK.
> go to drawer 1
The drawer 1 is closed.
. . .
> open drawer 6
You open the drawer 6. The drawer 6 is open. In it, you see a keychain 2.
> go to desk 1
On the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.
> think: Now I ﬁnd a mug (1). Next, I need to take it.
OK.
> take mug 1 from desk 1
You pick up the mug 1 from the desk 1.
> think: Now I take a mug (1). Next, I need to ﬁnd a desklamp. A desklamp is more likely to appear
in desk (1-2), sidetable (1-2), shelf (1-6), bed (1), drawer (1-6). I can check one by one, starting with
desk 1.
OK.
> go to desk 1
Nothing happens.
> go to desk 2
On the desk 2, you see a alarmclock 1, a bowl 1, a mug 3, a pencil 3, and a pencil 2.
> think: Now I ﬁnd a desklamp (1). Next, I need to use it.
OK.
> use desklamp 1
Nothing happens.
> use desklamp 1
Nothing happens.
Status: Fail
Reﬂection: In this environment, my plan was to ﬁnd a mug then ﬁnd and use a desklamp. However,
the task says to examine the mug with the desklamp. I should have looked for the desklamp ﬁrst, then
looked for the mug. I noticed that the desklamp was found on desk 1. In the next trial, I will go to
desk 1, ﬁnd the lamp, then look for the mug and examine it with the desklamp.
Trial #2
> go to desk 1
On the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.
> think: To solve the task, I need to ﬁnd and take a mug, then ﬁnd and use a desklamp.
OK.
> take mug 1 from desk 1
You pick up the mug 1 from the desk 1.
> think: To solve the task, I need to ﬁnd and take a mug, then ﬁnd and use a desklamp.
OK.
> use desklamp 1
You turn on the desklamp 1.
Status: Success
Figure 4: [Top] An AlfWorld trajectory in which the agent failed due to inefﬁcient planning. In the
reﬂection, the agent recognizes that it should have looked for the desklamp then the mug, not the mug
then the desklamp. [Bottom] The agent is able to correct its reasoning trace and execute a sequence
of actions in a concise manner.
8

Trial #1: Question: Grown-Ups starred the actor
who was best known for which role on "’Allo
’Allo!"?
Thought 1: I need to search Grown-Ups and
"’Allo ’Allo!", ﬁnd the actor who starred in
Grown-Ups, then ﬁnd the role he was best known
for in "’Allo ’Allo!".
Action 1: Search[Grown-Ups]
Observation 1: Grown-Ups is a 1980 British
BBC television ﬁlm devised and directed by
Mike Leigh. It stars Lesley Manville, Philip
Davis, Brenda Blethyn, Janine Duvitski, Lindsay
Duncan and Sam Kelly. It was edited by Robin
Sales and produced by Louis Marks for the BBC,
and originally shown on BBC 2 on 28 November
1980.
Thought 2: Grown-Ups starred Lesley Manville,
Philip Davis, Brenda Blethyn, Janine Duvitski,
Lindsay Duncan and Sam Kelly. I need to search
"’Allo ’Allo!" and ﬁnd which actor was best
known for which role.
Action 2: Search["’Allo ’Allo!"]
Observation 2: Could not ﬁnd ["’Allo ’Allo!"].
Similar: ["’Allo ’Allo!", "List of ’Allo ’Allo!
characters", ’Gorden Kaye’, "’Allo ’Allo! (series
2)", ’Allô, allô’, ’Francesca Gonshaw’, "’Allo
’Allo! (series 4)", ’Kim Hartman’, "’Allo ’Allo!
(series 6)", ’Jeremy Lloyd’]
Thought 3: To ﬁnd the actor who was best known
for which role on "’Allo ’Allo!", I can search
Gorden Kaye.
Action 3: Search[Gorden Kaye]
Observation 3: Gordon Irving Kaye (7 April
1941 – 23 January 2017), known profession-
ally as Gorden Kaye, was an English actor,
best known for playing womanising café owner
René Artois in the television comedy series ’Allo
’Allo!.
Thought 4: Gorden Kaye was best known for
playing womanising café owner René Artois in
the television comedy series "’Allo ’Allo!". So
the answer is René Artois.
Action 4: Finish[René Artois]
Observation 4: Answer is INCORRECT
Trial #2: Question: Grown-Ups starred the actor
who was best known for which role on "’Allo
’Allo!"?
Thought 1: I need to search the actor who was
best known for a role on "’Allo ’Allo!" and ﬁnd
out what role they were best known for.
Action 1: Search[Grown-Ups]
Observation 1: Grown-Ups is a 1980 British
BBC television ﬁlm devised and directed by
Mike Leigh. It stars Lesley Manville, Philip
Davis, Brenda Blethyn, Janine Duvitski, Lindsay
Duncan and Sam Kelly. It was edited by Robin
Sales and produced by Louis Marks for the BBC,
and originally shown on BBC 2 on 28 November
1980.
Thought 2: The paragraph does not mention the
actor who was best known for a role on "’Allo
’Allo!". I need to search the actor’s name instead.
Action 2: Search[Sam Kelly]
Observation 2: Roger Michael Kelly (19 Decem-
ber 1943 – 14 June 2014), known by the stage
name Sam Kelly, was an English actor who ap-
peared in ﬁlm, television, radio and theatre. He
is best known for his roles as Captain Hans Geer-
ing in ’Allo ’Allo!, Warren in Porridge, Sam in
On the Up, and Ted Liversidge in Barbara.
Thought 3: Sam Kelly is best known for his role
as Captain Hans Geering in "’Allo ’Allo!", so
the answer is Captain Hans Geering.
Action 3: Finish[Captain Hans Geering]
Observation 3: Answer is CORRECT
Reﬂection: I searched the wrong title for the show, "’Allo ’Allo!", which resulted in no results. I
should have searched the show’s main character, Gorden Kaye, to ﬁnd the role he was best known for
in the show.
Figure 5: 2 HotPotQA trials within the same environment and task.
9

solved the task. Likewise, in other experiments, it is straightforward to design a binary reward model.
For code-writing tasks, the binary reward model can be a compilation or type-check attempt. In a
capture-the-ﬂag task, the possession status of the ﬂag could be the binary reward model. For other
cases, an efﬁcient integration test could be written to assess the success status of the agent.
At its core, Reﬂexion is an approach designed to promote discovery for problem-solving LLM-agents
that may require several trials to solve given tasks, similar to the self-reﬂective learning methods
of humans when given new tasks. In the AlfWorld experiment, there were several environments
in which the agent was tasked to ﬁnd an object, manipulate it with another object, and then place
it in another location. In the baseline test, there were several trials that exceeded 50 actions and
observations. Reﬂexion served as a redirection mechanism in some cases and as a summarization
tool in others. In subsequent trials, the agent did not need to search every location to ﬁnd an object
that it had previously found. This behavior may be applicable in other experiments in which the
observations received from the environment may be signiﬁcantly long. In those cases, Reﬂexion is
needed to ensure that the query length does not exceed the maximum number of tokens while still
discovering important information.
4.4
Limitations of Reﬂexion
Reﬂexion relies on the emergent property of self-reﬂection that is present in several large language
models. In this study, we used GPT-3.0 and GPT 3.5 to power a ReAct agent (Yao et al., 2023) to
learn from its past mistakes. While Reﬂexion enabled the agent to discover new problem-solving
techniques in AlfWorld decision-making tasks and HotPotQA knowledge-intensive tasks, we observed
a shortcoming in its ability to improve on its baseline performance in a third benchmark, WebShop
(Yao et al., rint).
WebShop is a text-based problem-solving benchmark that tests natural language agents to navigate an
e-commerce website to ﬁnd and purchase products given requests from a client. We tested the agent in
100 environments, giving the agent two few-shot examples of successful WebShop trajectories using
the ReAct problem-solving technique. However, after only 4 trials, we terminated the baseline and
Reﬂexion runs as the agent did not show improvement in accuracy (Fig. 6) and was not generating
helpful, intuitive self-reﬂections. The agent achieved a 33% →34% accuracy improvement in the
baseline run and a mere 33% →35% accuracy improvement in the Reﬂexion run, which suggests that
the agent only successfully completed 1 additional task relative to the baseline agent’s performance.
However, after analyzing the failed trajectories, we noted that the chance of a successful item purchase
for an agent in a WebShop environment was not necessarily dependent on the agent’s ability to plan
and execute a correct sequence of actions, but rather on the quality of the WebShop search engine’s
results. This observation may not be a direct limitation of the Reﬂexion approach, but it highlights
the ability of a Reﬂexion agent to optimize reasoning trace and action execution but not complete
awareness of the quality of the tools that it may be using.
5
Conclusion
We proposed an approach that allows natural language agents to learn from past mistakes and redirect
future decisions in planning sequences which removes the human trainer in a human-in-the-middle
approach. We demonstrated learning curves on the AlfWorld and HotPotQA benchmarks that
signiﬁcantly outperform base ReAct agents. In addition, we include an inconclusive attempt to
improve performance on the WebShop benchmark and provide a discussion that highlights a few
limitations of this approach. Reﬂexion is a highly applicable method to improve performance between
trials on decision-making and knowledge-intensive tasks due to its sole dependence on a binary
reward model. In the AlfWorld and HotPotQA experiments, we constrained the reward model to
imitate environments in which informative reward models may be difﬁcult to design or compute. We
encourage others to apply Reﬂexion to more complex tasks in which the agent must learn to develop
new ideas, explore larger unseen state spaces, and form more accurate plans of action through its
experiences in past environments.
10

Figure 6: Reﬂexion vs React performance on WebShop across 100 customer shopping requests.
6
Reproducibility
Code and log ﬁles for these experiments are available at https://github.com/noahshinn024/
reflexion.
Acknowledgments and Disclosure of Funding
A.G. acknowledge funding from National Science Foundation (MCB2027165) for partially supporting
this effort.
11

References
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,
P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural
information processing systems, 33:1877–1901.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,
H. W., Sutton, C., Gehrmann, S., et al. (2022). Palm: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani,
M., Brahma, S., et al. (2022). Scaling instruction-ﬁnetuned language models. arXiv preprint
arXiv:2210.11416.
Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M.,
El Asri, L., Adada, M., et al. (2019). Textworld: A learning environment for text-based games. In
Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International
Conference on Artiﬁcial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised
Selected Papers 7, pages 41–75. Springer.
Fan, X., Lyu, Y., Liang, P. P., Salakhutdinov, R., and Morency, L.-P. (2022). Nano: Nested human-in-
the-loop reward learning for few-shot language model control. arXiv preprint arXiv:2211.05750.
Haluptzok, P., Bowers, M., and Kalai, A. T. (2022). Language models can teach themselves to
program better. arXiv preprint arXiv:2207.14502.
Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. (2022a). Language models as zero-shot planners:
Extracting actionable knowledge for embodied agents. In International Conference on Machine
Learning, pages 9118–9147. PMLR.
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch,
I., Chebotar, Y., et al. (2022b). Inner monologue: Embodied reasoning through planning with
language models. arXiv preprint arXiv:2207.05608.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford,
A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361.
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). Large language models are
zero-shot reasoners. arXiv preprint arXiv:2205.11916.
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V.,
Saunders, W., et al. (2021). Webgpt: Browser-assisted question-answering with human feedback.
arXiv preprint arXiv:2112.09332.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S.,
Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human
feedback. arXiv preprint arXiv:2203.02155.
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S.,
Ring, R., Young, S., et al. (2021). Scaling language models: Methods, analysis & insights from
training gopher. arXiv preprint arXiv:2112.11446.
Shridhar, M., Yuan, X., Côté, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. (2021). ALFWorld:
Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the
International Conference on Learning Representations (ICLR).
Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y. (2023). Describe, explain, plan and select:
Interactive planning with large language models enables open-world multi-task agents. arXiv
preprint arXiv:2302.01560.
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M.,
Zhou, D., Metzler, D., et al. (2022a). Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682.
12

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. (2022b). Chain of
thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.
Wu, X., Xiao, L., Sun, Y., Zhang, J., Ma, T., and He, L. (2022). A survey of human-in-the-loop for
machine learning. Future Generation Computer Systems.
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018).
HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on
Empirical Methods in Natural Language Processing (EMNLP).
Yao, S., Chen, H., Yang, J., and Narasimhan, K. (preprint). Webshop: Towards scalable real-world
web interaction with grounded language agents. In ArXiv.
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2023). ReAct: Synergizing
reasoning and acting in language models. In International Conference on Learning Representations
(ICLR).
13

A
Few-shot examples (Reﬂexion)
A.1
AlfWorld
You will be given the history of a past experience in which you were placed in an environment and
given a task to complete. You were unsuccessful in completing the task. Do not summarize your
environment, but rather think about the strategy and path you took to attempt to complete the task.
Devise a concise, new plan of action that accounts for your mistake with reference to speciﬁc actions
that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to
achieve C with environment-speciﬁc actions. You will need this later when you are solving the same
task. Give your plan after "Plan". Here are two examples:
You are in the middle of a room. Looking quickly around you, you see a cabinet 10, a cabinet 9, a
cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a
coffeemachine 1, a countertop 2, a countertop 1, a diningtable 2, a diningtable 1, a drawer 3, a drawer
2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a sinkbasin 1, a stoveburner 4, a stoveburner
3, a stoveburner 2, a stoveburner 1, and a toaster 1.
Your task is to: heat some mug and put it in coffeemachine.
> think: To solve the task, I need to ﬁnd and take a mug, then heat it with stoveburner, then put it in
coffeemachine.
Ok.
> look
You are in the middle of a room. Looking quickly around you, you see nothing.
> think: First I need to ﬁnd a mug. A mug is more likely to appear in countertop (1-2), diningtable
(1-2), drawer (1-3), fridge (1), sinkbasin (1), stoveburner (1-4), toaster (1). I can check one by one,
starting with countertop 1.
Ok.
> look
You are in the middle of a room. Looking quickly around you, you see nothing.
> go to countertop 1
On the countertop 1, you see a butterknife 1, a cup 1, a fork 2, and a mug 1.
> think: Now I ﬁnd a mug (1). Next, I need to take it.
Ok.
> take mug 1 from countertop 1
You pick up the mug 1 from the countertop 1.
> think: Now I take a mug (1). Next, I need to heat it with stoveburner (1-4).
Ok.
> go to stoveburner 1
On the stoveburner 1, you see a pan 2.
> examine stoveburner 1
On the stoveburner 1, you see a pan 2.
> examine stoveburner 1
On the stoveburner 1, you see a pan 2.
> examine stoveburner 1
On the stoveburner 1, you see a pan 2.
> examine stoveburner 1
On the stoveburner 1, you see a pan 2.
STATUS: FAIL
New plan: I was stuck in a loop in which I continually examined stoveburner 1 instead of heating mug
1 with stoveburner 1. I should have taken mug 1 from countertop 1, then heated it with stoveburner 1,
then put it in coffeemachine 1. It did not help to execute two identical actions in a row. I will try to
execute a different action if I am stuck in a loop again.
You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a
desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a
laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1.
Your task is to: look at bowl under the desklamp.
> think: To solve the task, I need to ﬁnd and take a bowl, then ﬁnd and use a desklamp.
OK.
14

> think: First I need to ﬁnd a bowl. A bowl is more likely to appear in drawer (1-6), desk (1-2), shelf
(1-6), garbagecan (1), laundryhamper (1). I can check one by one, starting with drawer 1.
OK.
> go to drawer 1
The drawer 1 is closed.
> open drawer 1
You open the drawer 1. The drawer 1 is open. In it, you see a cellphone 2.
> go to drawer 2
On the drawer 2, you see a keychain 1.
> go to drawer 3
Nothing happens.
> go to drawer 4
The drawer 4 is closed.
> open drawer 4
You open the drawer 4. The drawer 4 is open. In it, you see nothing.
> go to drawer 5
On the drawer 5, you see nothing.
> go to drawer 6
The drawer 6 is closed.
> open drawer 6
You open the drawer 6. The drawer 6 is open. In it, you see a book 2, and a pen 2.
> go to desk 1
On the desk 1, you see a alarmclock 1, a desklamp 1, a mug 1, and a pencil 1.
> think: Now I ﬁnd a desklamp (1). Next, I need to use it.
OK.
> use desklamp 1
You turn on the desklamp 1.
>
Nothing happens.
> go to desk 2
On the desk 2, you see a bowl 2, a bowl 1, a cd 2, a cd 1, and a creditcard 3.
> think: Now I ﬁnd a bowl (2). Next, I need to take it.
OK.
> take bowl 2 from desk 2
You pick up the bowl 2 from the desk 2.
>
Nothing happens.
>
Nothing happens.
STATUS: FAIL
New plan: In this environment, my plan was to ﬁnd a bowl then ﬁnd and use a desklamp. However,
the task says to look at bowl under the desklamp. I should have looked for the desklamp then looked
for the bowl. I noticed that the desklamp was found on desk 1. In the next trial, I will go to desk 1,
ﬁnd the lamp, then look for the bowl under the desklamp.
A.2
HotPotQA
Question: The Rome Protocols were signed by three Prime Ministers one of which was assassinated
as part of what?
Thought 1: I need to search Rome Protocols, ﬁnd the three Prime Ministers, then ﬁnd what they
were assassinated as part of.
Action 1: Search[Rome Protocols]
Observation 1: The Rome Protocols were a series of three international agreements concluded in
Rome on 17 March 1934 between the governments of Austria, Hungary and Italy. They were signed
by Italian Prime Minister Benito Mussolini, Austrian Chancellor Engelbert Dollfuss and Hungarian
Prime Minister Gyula Gömbös. All of the protocols went into effect on 12 July 1934 and were
registered in League of Nations Treaty Series on 12 December 1934.
Thought 2: The three Prime Ministers were Benito Mussolini, Engelbert Dollfuss, and Gyula
Gömbös. I need to search Benito Mussolini and ﬁnd what he was assassinated as part of.
15

Action 2: Search[Benito Mussolini]
Observation 2: Benito Amilcare Andrea Mussolini (UK: MU(U)SS-\textschwa-LEE-nee, US:
MOOSS-, Italian: [be\textasciitilde ni\textasciitilde to a\textasciitilde milkare an\textasciitilde
dr\textasciitilde ea musso\textasciitilde li\textasciitilde ni]; 29 July 1883 – 28 April 1945) was an
Italian politician and journalist who founded and led the National Fascist Party (PNF). He was Prime
Minister of Italy from the March on Rome in 1922 until his deposition in 1943, as well as ¨Duceöf
Italian fascism from the establishment of the Italian Fasces of Combat in 1919 until his summary
execution in 1945 by Italian partisans...
Thought 3: Benito Mussolini was assassinated as part of the collapse of the Axis powers during
World War II. So the Rome Protocols were signed by three Prime Ministers one of which was
assassinated as part of World War II.
Action 3: Finish[World War II]
Reﬂection: I searched one of the prime ministers involved in the signing, then attempted to answer
right away. I should have searched each of the prime ministers, then looked up ’death’ on each of
their pages in order to get more information before answering.
Question: Kam Heskin plays Paige Morgan in a 2004 ﬁlm directed by who?
Thought 1: I need to search Kam Heskin and Paige Morgan, ﬁnd the 2004 ﬁlm they are in, then ﬁnd
the director of the ﬁlm.
Action 1: Search[Kam Heskin]
Observation 1: Kam Erika Heskin (born May 8, 1973) is an American actress. She began her
career playing Caitlin Deschanel on the NBC daytime soap opera Sunset Beach (1998–1999), before
appearing in ﬁlms Planet of the Apes (2001) and Catch Me If You Can (2002). Heskin went on in
2003 to play Elizabeth Bennet in an independent ﬁlm Pride & Prejudice: A Latter-Day Comedy,
Robin in Charmed (2004), and Paige Morgan in The Prince and Me ﬁlm franchise (2006–2010).
Thought 2: Kam Heskin plays Paige Morgan in The Prince and Me ﬁlm franchise. I need to search
The Prince and Me ﬁlm franchise and ﬁnd the director of the 2004 ﬁlm.
Action 2: Search[The Prince and Me ﬁlm franchise]
Observation 2: Could not ﬁnd [The Prince and Me ﬁlm franchise]. Similar: [’Kam Heskin’,
’Prince of Persia’, ’Harry Potter and the Half-Blood Prince (ﬁlm)’, ’Prince of Persia: The Sands of
Time (ﬁlm)’, ’The Chronicles of Narnia (ﬁlm series)’, ’Majestic Prince (franchise)’, ’Kingsman
(franchise)’, ’The Little Prince (2015 ﬁlm)’, ’Shrek (franchise)’, ’Harry Potter (ﬁlm series)’]
Thought 3: I need to search The Prince and Me (2004 ﬁlm) instead.
Action 3: Search[The Prince and Me (2004 ﬁlm)]
Observation 3: Could not ﬁnd [The Prince and Me (2004 ﬁlm)]. Similar: [’The Prince & Me’, ’The
Prince & Me 2: The Royal Wedding’, ’Kam Heskin’, ’Prince of Darkness (ﬁlm)’, ’Prince of Persia:
The Sands of Time (ﬁlm)’, ’Rob Knox’, ’Alexander (2004 ﬁlm)’, ’Prince of Persia’, ’Brooklynn
Prince’, ’Prince (musician)’]
Thought 4: I need to search The Prince & Me (2004 ﬁlm).
Action 4: Search[The Prince & Me (2004 ﬁlm)]
Observation 4: Could not ﬁnd [The Prince & Me (2004 ﬁlm)]. Similar: [’The Prince & Me’, ’The
Prince & Me 2: The Royal Wedding’, ’Prince of Darkness (ﬁlm)’, ’Prince of Persia: The Sands of
Time (ﬁlm)’, ’Rob Knox’, ’Alexander (2004 ﬁlm)’, ’Prince (musician)’, ’Prince of Persia’, ’Kam
Heskin’, ’Brooklynn Prince’]
Thought 5: I need to search The Prince & Me (2004 ﬁlm) instead.
Action 5: Search[The Prince & Me (2004 ﬁlm)]
Observation 5: Could not ﬁnd [The Prince & Me (2004 ﬁlm)]. Similar: [’The Prince & Me’, ’The
Prince & Me 2: The Royal Wedding’, ’Prince of Darkness (ﬁlm)’, ’Prince of Persia: The Sands of
Time (ﬁlm)’, ’Rob Knox’, ’Alexander (2004 ﬁlm)’, ’Prince (musician)’, ’Prince of Persia’, ’Kam
Heskin’, ’Brooklynn Prince’]
Thought 6: I need to search The Prince & Me (2004 ﬁlm).
Action 6: Search[The Prince & Me (2004 ﬁlm)]
Observation 6: Could not ﬁnd [The Prince & Me (2004 ﬁlm)]. Similar: [’The Prince & Me’, ’The
Prince & Me 2: The Royal Wedding’, ’Prince of Darkness (ﬁlm)’, ’Prince of Persia: The Sands of
Time (ﬁlm)’, ’Rob Knox’, ’Alexander (2004 ﬁlm)’, ’Prince (musician)’, ’Prince of Persia’, ’Kam
Heskin’, ’Brooklynn Prince’]
Reﬂection: I got stuck in a loop where I kept trying to search ’The Prince & Me (2004 ﬁlm)’ but the
page could not be found. Instead I should have tried to search the similar results that had a similar
name to see and they were made in 2004.
16

A.3
WebShop
Instruction:
i am looking for dairy free and apple variety pack of chips, and price lower than 30.00 dollars
[Search]
Action: search[dairy free and apple variety pack of chips]
Observation:
[Back to Search]
Page 1 (Total results: 50)
[Next >]
[B07HRFJWP8]
Enjoy Life Foods Soft Baked Ovals, Breakfast Bars, Nut Free Bars, Soy Free, Dairy Free, Non GMO,
Gluten Free, Vegan, Variety Pack, 4 Boxes (20 Bars Total)
$100.0
[B01KMHY5PG]
Enjoy Life Soft Baked Chewy Bars, Variety Pack, Nut Free Bars, Soy Free, Dairy Free, Gluten Free,
6 Boxes (30 Total Bars)
$21.49
[B008D2X8C4]
Enjoy Life Lentil Chips Variety Pack, Dairy Free Chips, Soy Free, Nut Free, Non GMO, Vegan,
Gluten Free, 24 Bags (0.8 oz)
$100.0
Action: think[The ﬁrst two items are not what I want. The third item is the dairy free and apple variety pack of chips I want.]
Observation: OK.
Action: click[B008D2X8C4]
Observation:
[Back to Search]
[< Prev]
ﬂavor name [dill and sour cream][garlic & parmesan][light sea salt][margherita pizza][thai chili lime][variety pack]
size [0.8 ounce (pack of 24)][4 ounce (pack of 12)]
Enjoy Life Lentil Chips Variety Pack, Dairy Free Chips, Soy Free, Nut Free, Non GMO, Vegan,
Gluten Free, 24 Bags (0.8 oz)
Price: $100.0
Rating: N.A.
[Description]
[Features]
[Reviews]
[Buy Now]
Action: think[The item has options ’variety pack’ and ’0.8 ounce (pack of 24)’ and seems good to buy.]
Observation: OK.
Action: click[variety pack]
Observation: You have clicked variety pack.
Action: click[0.8 ounce (pack of 24)]
Observation: You have clicked 0.8 ounce (pack of 24).
Action: click[Buy Now]
STATUS: FAIL
Next plan: In this attempt, I was unsuccessful. I accidentally bought a product that was $100, which
is more than my budget of $30. Either way, the initial search results were not good. Next time, I will
do search["variety pack of chips"] and then check if the results meet the dairy free and the $30
budget constraints. I will continue to reﬁne my searches so that I can ﬁnd more products.
17

