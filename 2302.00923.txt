Multimodal Chain-of-Thought Reasoning in Language Models
Zhuosheng Zhang 1 Aston Zhang 2 Mu Li 2 Hai Zhao 1 George Karypis 2 Alex Smola 2
Abstract
Large language models (LLMs) have shown im-
pressive performance on complex reasoning by
leveraging chain-of-thought (CoT) prompting to
generate intermediate reasoning chains as the ra-
tionale to infer the answer. However, existing
CoT studies have focused on the language modal-
ity. We propose Multimodal-CoT that incorpo-
rates language (text) and vision (images) modal-
ities into a two-stage framework that separates
rationale generation and answer inference. In this
way, answer inference can leverage better gen-
erated rationales that are based on multimodal
information. With Multimodal-CoT, our model
under 1 billion parameters outperforms the previ-
ous state-of-the-art LLM (GPT-3.5) by 16 percent-
age points (75.17%→91.68% accuracy) and even
surpasses human performance on the ScienceQA
benchmark. Code is publicly available.1
1. Introduction
Imagine reading a textbook with no ﬁgures or tables. Our
ability to knowledge acquisition is greatly strengthened by
jointly modeling diverse data modalities, such as vision, lan-
guage, and audio. Recently, large language models (LLMs)
(Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;
Chowdhery et al., 2022) have shown impressive perfor-
mance in complex reasoning by generating intermediate
reasoning steps before inferring the answer. The intriguing
technique is called chain-of-thought (CoT) reasoning (Wei
et al., 2022b; Kojima et al., 2022; Zhang et al., 2022).
However, existing studies related to CoT reasoning are
largely isolated in the language modality (Wang et al.,
2022b; Zhou et al., 2022; Lu et al., 2022b; Fu et al., 2022),
with little consideration of multimodal scenarios. To elicit
CoT reasoning in multimodality, we advocate a Multimodal-
1Shanghai Jiao Tong University 2Amazon Web Services.
Correspondence to:
Zhuosheng Zhang (work done at Ama-
zon Web Services) <zhangzs@sjtu.edu.cn>, Aston Zhang
<az@astonzhang.com>.
1https://github.com/amazon-science/mm-cot
Options:
(B) salty
(A) soft
Output
Question: Which property do these
two objects have in common?
Context: Select the better answer.
Rationale: Look at each object. For each object, decide if it has
that property. Potato chips have a salty taste. Both objects are
salty. A soft object changes shape when you squeeze it. The fries
are soft, but the cracker is not. The property that both objects have
in common is salty.
Answer: The answer is (B).
Vision
Language
Input
Figure 1. Example of the multimodal CoT task.
CoT paradigm. Given the inputs in different modalities,
Multimodal-CoT decomposes multi-step problems into in-
termediate reasoning steps (rationale) and then infers the
answer. Since vision and language are the most popular
modalities, we focus on those two modalities in this work.
An example is shown in Figure 1. In general, there are two
ways to elicit Multimodal-CoT reasoning as follows: (i)
prompting LLMs and (ii) ﬁne-tuning small models.2
The most immediate way to perform Multimodal-CoT is to
transform the input of different modalities into one modality
and prompt LLMs to perform CoT. For example, it is possi-
ble to extract the caption of an image by a captioning model
and then concatenate the caption with the original language
input to be fed into LLMs (Lu et al., 2022a). However, there
is severe information loss in the captioning process; thus,
using the captions (as opposed to vision features) may suffer
from a lack of mutual synergy in the representation space
of different modalities.
To facilitate the interaction between modalities, another
potential solution is to ﬁne-tune smaller language models
(LMs) by fusing multimodal features (Zhang et al., 2023).
As this approach allows the ﬂexibility of adjusting model
architectures to incorporate multimodal features, we study
ﬁne-tuning models in this work instead of prompting LLMs.
The key challenge is that language models under 100 billion
parameters tend to generate hallucinated rationales that mis-
lead the answer inference (Ho et al., 2022; Magister et al.,
2In this work, we refer to small models as models with less
than 1 billion parameters (hereinafter dubbed as 1B-models).
arXiv:2302.00923v4  [cs.CL]  17 Feb 2023

Multimodal Chain-of-Thought Reasoning in Language Models
Table 1. Typical CoT techniques (FT: ﬁne-tuning; KD: knowledge distillation). Segment 1: in-context learning techniques; Segment 2:
ﬁne-tuning techniques. To the best of our knowledge, our work is the ﬁrst to study CoT reasoning in different modalities. Besides, we
focus on 1B-models, without relying on the outputs of LLMs.
Models
Mutimodal
w/o LLM
Model / Engine
Training
CoT Role
CoT Source
Zero-Shot-CoT (Kojima et al., 2022)


GPT-3.5 (175B)
ICL
Reasoning
Template
Few-Shot-CoT (Wei et al., 2022b)


PaLM (540B)
ICL
Reasoning
Hand-crafted
Self-Consistency-CoT (Wang et al., 2022a)


Codex (175B)
ICL
Reasoning
Hand-crafted
Least-to-Most Prompting (Zhou et al., 2022)


Codex (175B)
ICL
Reasoning
Hand-crafted
Retrieval-CoT (Zhang et al., 2022)


GPT-3.5 (175B)
ICL
Reasoning
Auto-generated
PromptPG-CoT (Lu et al., 2022b)


GPT-3.5 (175B)
ICL
Reasoning
Hand-crafted
Auto-CoT (Zhang et al., 2022)


Codex (175B)
ICL
Reasoning
Auto-generated
Complexity-CoT (Fu et al., 2022)


GPT-3.5 (175B)
ICL
Reasoning
Hand-crafted
Few-Shot-PoT (Chen et al., 2022)


GPT-3.5 (175B)
ICL
Reasoning
Hand-crafted
UniﬁedQA (Lu et al., 2022a)

✓
T5 (770M)
FT
Explanation
Crawled
Fine-Tuned T5 XXL (Magister et al., 2022)


T5 (11B)
KD
Reasoning
LLM-generated
Fine-Tune-CoT (Ho et al., 2022)


GPT-3 (6.7B)
KD
Reasoning
LLM-generated
Multimodal-CoT (our work)
✓
✓
T5 (770M)
FT
Reasoning
Crawled
2022; Ji et al., 2022).
To mitigate the challenge of hallucination, we propose
Multimodal-CoT that incorporates language (text) and vi-
sion (images) modalities into a two-stage framework that
separates rationale generation and answer inference. In
this way, answer inference can leverage better generated
rationales that are based on multimodal information. Our
experiments are conducted on the ScienceQA benchmark
(Lu et al., 2022a), which is the latest multimodal reasoning
benchmark with annotated reasoning chains. Experimental
results show that our method surpasses the previous state-of-
the-art GPT-3.5 model by +16% (75.17%→91.68%) on the
benchmark. Our contributions are summarized as follows:
(i) To the best of our knowledge, this work is the ﬁrst to
study CoT reasoning in different modalities.
(ii) We propose a two-stage framework by ﬁne-tuning lan-
guage models to fuse vision and language representations
to perform Multimodal-CoT. The model is able to generate
informative rationales to facilitate inferring ﬁnal answers.
(iii) Our method achieves new state-of-the-art performance
on the ScienceQA benchmark, outperforming accuracy of
GPT-3.5 by 16% and even surpassing human performance.
2. Background
This section reviews recent progress of eliciting CoT rea-
soning by prompting and ﬁne-tuning language models.
2.1. CoT Reasoning with LLMs
Recently, CoT has been widely used to elicit the multi-step
reasoning abilities of LLMs (Wei et al., 2022b). Concretely,
CoT techniques encourage the LLM to generate intermedi-
ate reasoning chains for solving a problem. Studies have
shown that LLMs can perform CoT reasoning with two ma-
jor paradigms of techniques: Zero-Shot-CoT (Kojima et al.,
2022) and Few-Shot-CoT (Wei et al., 2022b; Zhang et al.,
2022). For Zero-Shot-CoT, Kojima et al. (2022) showed that
LLMs are decent zero-shot reasoners by adding a prompt
like “Let’s think step by step” after the test question to in-
voke CoT reasoning. For Few-Shot-CoT, a few step-by-step
reasoning demonstrations are used as conditions for infer-
ence. Each demonstration has a question and a reasoning
chain that leads to the ﬁnal answer. The demonstrations are
commonly obtained by hand-crafting or automatic gener-
ation. The corresponding techniques are thus referred to
as Manual-CoT (Wei et al., 2022b) and Auto-CoT (Zhang
et al., 2022).
With effective demonstrations,
Few-Shot-CoT often
achieves stronger performance than Zero-Shot-CoT and has
attracted more research interest. Therefore, most recent
studies focused on how to improve Few-Shot-CoT. Those
studies are categorized into two major research lines: (i)
optimizing the demonstrations; (ii) optimizing the reasoning
chains. Table 1 compares typical CoT techniques.
Optimizing Demonstrations
The performance of Few-
Shot-CoT relies on the quality of demonstrations. As re-
ported in Wei et al. (2022b), using demonstrations written
by different annotators results in dramatic accuracy dispar-
ity in a symbolic reasoning task. Beyond hand-crafting the
demonstrations, recent studies have investigated ways to op-
timize the demonstration selection process. Notably, Rubin
et al. (2022) retrieved the semantically similar demonstra-
tions with the test instance. However, this approach shows
a degraded performance when there are mistakes in the rea-
soning chains (Zhang et al., 2022). To address the limitation,
Zhang et al. (2022) found that the key is the diversity of
demonstration questions and proposed Auto-CoT: (i) par-
tition questions of a given dataset into a few clusters; (ii)
sample a representative question from each cluster and gen-
erate its reasoning chain using Zero-Shot-CoT with simple
heuristics. In addition, reinforcement learning (RL) and

Multimodal Chain-of-Thought Reasoning in Language Models
complexity-based selection strategies were also proposed
to obtain effective demonstrations. Fu et al. (2022) chose
examples with complex reasoning chains (i.e., with more
reasoning steps) as the demonstrations. Lu et al. (2022b)
trained an agent to ﬁnd optimal in-context examples from
a candidate pool and maximize the prediction rewards on
given training examples when interacting with GPT-3.5.
Optimizing Reasoning Chains
A notable way to opti-
mize reasoning chains is problem decomposition. Zhou
et al. (2022) proposed least-to-most prompting to decom-
pose complex problems into sub-problems and then solve
these sub-problems sequentially. As a result, solving a
given sub-problem is facilitated by the answers to previ-
ously solved sub-problems. Similarly, Khot et al. (2022)
used diverse decomposition structures and designed differ-
ent prompts to answer each sub-question. In addition to
prompting the reasoning chains as natural language texts,
Chen et al. (2022) proposed program-of-thoughts (PoT),
which modeled the reasoning process as a program and
prompted LLMs to derive the answer by executing the gen-
erated programs. Another trend is to vote over multiple
reasoning paths for a test question. Wang et al. (2022a)
introduced a self-consistency decoding strategy to sample
multiple outputs of LLMs and then took a majority over
the ﬁnal answers. Wang et al. (2022b) and Li et al. (2022b)
introduced randomness in the input space to produce more
diverse outputs for voting.
2.2. Eliciting CoT Reasoning by Fine-Tuning Models
A recent interest is eliciting CoT reasoning by ﬁne-tuning
language models. Lu et al. (2022a) ﬁne-tuned the encoder-
decoder T5 model on a large-scale dataset with CoT annota-
tions. However, a dramatic performance decline is observed
when using CoT to infer the answer, i.e., generating the rea-
soning chain before the answer (reasoning). Instead, CoT
is only used as an explanation after the answer. Magister
et al. (2022) and Ho et al. (2022) employed knowledge
distillation by ﬁne-tuning a student model on the chain-of-
thought outputs generated by a larger teacher model. The
proposed methods showed performance gains in arithmetic,
commonsense, and symbolic reasoning tasks.
There is a key challenge in training 1B-models to be CoT
reasoners. As observed by Wei et al. (2022b), models un-
der 100 billion parameters tend to produce illogical CoT
that leads to wrong answers. In other words, it might be
harder for 1B-models to generate effective CoT than directly
generating the answer. It becomes even more challenging
in a multimodal setting where answering the question also
requires understanding the multimodal inputs. In the follow-
ing part, we will explore the challenge of Multimodal-CoT
and investigate how to perform effective multi-step reason-
ing.
3. Challenge of Multimodal-CoT
Existing studies have suggested that the CoT reasoning abil-
ity may emerge in language models at a certain scale, e.g.,
over 100 billion parameters (Wei et al., 2022a). However,
it remains an unresolved challenge to elicit such reasoning
abilities in 1B-models, let alone in the multimodal scenario.
This work focuses on 1B-models as they can be ﬁne-tuned
and deployed with consumer-grade GPUs (e.g., 32G mem-
ory). In this section, we will investigate why 1B-models
fail at CoT reasoning and study how to design an effective
approach to overcome the challenge.
3.1. Towards the Role of CoT
To begin with, we ﬁne-tune a text-only baseline for CoT rea-
soning on the ScienceQA benchmark (Lu et al., 2022a).
Following Lu et al. (2022a), we adopt UniﬁedQABase
(Khashabi et al., 2020) as the backbone language model.3
Our task is modeled as a text generation problem, where the
model takes the textual information as the input and gener-
ates the output sequence that consists of the rationale and
the answer. As an example shown in Figure 1, the model
takes the concatenation of tokens of the question text (Q),
the context text (C), and multiple options (M) as the input.
To study the effect of CoT, we compare the performance
with three variants: (i) No-CoT which predicts the answer
directly (QCM→A); (ii) Reasoning where answer inference
is conditioned to the rationale (QCM→RA); (iii) Explana-
tion where the rationale is used for explaining the answer
inference (QCM→AR).
Table 2. Effects of CoT in the one-stage setting.
Method
Format
Accuracy
No-CoT
QCM→A
80.40
Reasoning
QCM→RA
67.86
Explanation
QCM→AR
69.77
Surprisingly, we observe a ↓12.54% accuracy decrease
(80.40%→67.86%) if the model predicts rationales before
answers (QCM→RA). The results imply that the rationales
might not necessarily contribute to predicting the right an-
swer. A similar phenomenon was observed in Lu et al.
(2022a), where the plausible reason might be that the model
exceeds the maximum token limits before obtaining the
required answer or stops generating the prediction early.
However, we ﬁnd that the maximum length of the gener-
ated outputs (RA) is always less than 400 tokens, which
is below the length limit of language models (i.e., 512 in
UniﬁedQABase). Therefore, it deserves a more in-depth
investigation into why the rationales harm answer inference.
3UniﬁedQA (Khashabi et al., 2020) is adopted as it is the best
ﬁne-tuning model in Lu et al. (2022a). Model information and
implementation details are presented in Appendix B.1.

Multimodal Chain-of-Thought Reasoning in Language Models
Generated Rationale: Magnets can pull or push on each other
without touching. When magnets attract, they pull together. When
magnets repel, they push apart. Whether a magnet attracts or repels
other magnets depends on the positions of its poles, or ends. Every
magnet has two poles, called north and south. Here are some
examples of magnets. The north pole of each magnet is marked N,
and the south pole is marked S. If different poles are closest to each
other, the magnets attract. The magnets in the pair below attract. If the
same poles are closest to each other, the magnets repel. The magnets
in both pairs below repel. Will these magnets attract or repel? To find
out, look at which poles are closest to each other. The south pole of
one magnet is closest to the south pole of the other magnet. Poles that
are the same repel. So, these magnets will repel each other.
Answer: The answer is (B).
Options:
(B) repel
(A) attract
Problem
Baseline
Question: Will these magnets attract or repel each other?
Context: Two magnets are placed as shown. Hint: Magnets that attract pull together. Magnets that repel push apart.
Gold Rationale: Magnets can pull or push on each other without touching. When magnets attract, they pull together. When magnets repel, they push
apart. Whether a magnet attracts or repels other magnets depends on the positions of its poles, or ends. Every magnet has two poles, called north and
south. Here are some examples of magnets. The north pole of each magnet is marked N, and the south pole is marked S. If different poles are closest
to each other, the magnets attract. The magnets in the pair below attract. If the same poles are closest to each other, the magnets repel. The magnets
in both pairs below repel. Will these magnets attract or repel? To find out, look at which poles are closest to each other. The north pole of one magnet
is closest to the south pole of the other magnet. Poles that are different attract. So, these magnets will attract each other.
Answer: The answer is (A).
Generated Rationale: Magnets can pull or push on each other
without touching. When magnets attract, they pull together. When
magnets repel, they push apart. Whether a magnet attracts or repels
other magnets depends on the positions of its poles, or ends. Every
magnet has two poles, called north and south. Here are some
examples of magnets. The north pole of each magnet is marked N,
and the south pole is marked S. If different poles are closest to each
other, the magnets attract. The magnets in the pair below attract. If the
same poles are closest to each other, the magnets repel. The magnets
in both pairs below repel. Will these magnets attract or repel? To find
out, look at which poles are closest to each other. The north pole of
one magnet is closest to the south pole of the other magnet. Poles that
are different attract. So, these magnets will attract each other.
Answer: The answer is (A).
+ Vision Features
Vision
Figure 2. Example of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales
and predicting answers. The upper part presents the problem details with a gold rationale, and the lower part shows the outputs of the
baseline and our method incorporated with vision features. We observe that the baseline fails to predict the right answer due to the
misleading by hallucinated rationales. More examples are shown in Appendix A.1.
3.2. Misleading by Hallucinated Rationales
To dive into how the rationales affect the answer prediction,
we separate the CoT problem into two stages, rationale
generation and answer inference. We report the RougeL
score and accuracy for the rationale generation and answer
inference, respectively. Table 3 shows the results based
on the two-stage framework. Although the two-stage base-
line model achieves a 91.76 RougeL score of the rationale
generation, the answer inference accuracy is only 70.53%.
Compared with the QCM→A variant (80.40%) in Table 2,
the result shows that the generated rationale in the two-stage
framework does not improve answer accuracy.
Table 3. Two-stage setting of (i) rationale generation (RougeL) and
(ii) answer inference (Accuracy).
Method
(i) QCM→R
(ii) QCMR→A
Two-Stage Framework
91.76
70.53
w/ Captions
91.85
71.12
w/ Vision Features
96.97
84.91
Then, we randomly sample 50 error cases and ﬁnd that the
model tends to generate hallucinated rationales that mislead
the answer inference. As an example shown in Figure 2, the
model (left part) hallucinates that, “The south pole of one
magnet is closest to the south pole of the other magnet”, due
to the lack of reference to the vision content. We ﬁnd that
such mistakes occur at a ratio of 64% among the error cases
Others
(36%)
Resolved 
(62.5%)
Unresolved
(37.5%)
Hallucination
(64%)
(a) ratio of hallucination mistakes
(b) correction rate w/ vision features 
Figure 3. The ratio of hallucination mistakes (a) and correction
rate w/ vision features (b).
(Figure 3(a)).
3.3. Multimodality Contributes to Effective Rationales
We speculate that such a phenomenon of hallucination is
due to a lack of necessary vision contexts for performing
effective Multimodal-CoT. To inject vision information, a
simple way is to transform the paired image into a caption
(Lu et al., 2022a) and then append the caption in the input of
both stages. However, as shown in Table 3, using captions
only yields marginal performance gains (↑0.59%). Then,
we explore an advanced technique by incorporating vision
features into the language model. Concretely, we feed the
paired image to the DETR model (Carion et al., 2020) to
extract vision features. Then we fuse the vision features

Multimodal Chain-of-Thought Reasoning in Language Models
Vision
Language
Rationale 
Generation
Question: Which property do these two
objects have in common?
Context: Select the better answer.
Look at each object. For each
object, decide if it has that
property. Potato chips have a
salty taste. Both objects are
salty. A soft object changes
shape when you squeeze it.
The fries are soft, but the
cracker is not. The property
that both objects have in
common is salty.
Options:
(B) salty
(A) soft
Rationale
Answer 
Inference
The answer
is (B).
Answer
Figure 4. Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages: (i) rationale generation and (ii) answer
inference. Both stages share the same model architecture but differ in the input and output. In the ﬁrst stage, we feed the model with
language and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated
from the ﬁrst stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.
with the encoded language representations before feeding
to the decoder (more details will be presented in Section
4). Interestingly, with vision features, the RougeL score of
the rationale generation has boosted to 96.97% (QCM→R),
which correspondingly contributes to better answer accuracy
of 84.91% (QCMR→A). With those effective rationales,
the phenomenon of hallucination is mitigated — 62.5%
hallucination mistakes in Section 3.2 have been corrected
(Figure 3(b)), as an example shown in Figure 2 (right part).4
The analysis so far compellingly shows that vision features
are indeed beneﬁcial for generating effective rationales and
contributing to accurate answer inference. As the two-stage
method (QCMR→A) in Table 3 achieves better performance
than all the one-stage method in Table 2, we choose the two-
stage method in our Multimodal-CoT framework.
4. Multimodal-CoT
Based on the observations and discussions in Section 3, we
propose Multimodal-CoT to incorporate language (text) and
vision (images) modalities into a two-stage framework. In
this section, we will ﬁrst overview the procedure of the
framework and then elaborate on the technical design of the
model architecture.
4.1. Framework Overview
Multimodal-CoT consists of two training stages: (i) ratio-
nale generation and (ii) answer inference. Both stages share
the same model architecture but differ in the input X and
output Y . The overall procedure is illustrated in Figure 4.
We will take vision-language as an example to show how
Multimodal-CoT works.
4The left mistakes are mainly about map understanding, which
requires more advanced vision features. We will discuss them in
Section 6.4.
In the rationale generation stage, we feed the model with
X = {X1
language, Xvision} where X1
language represents the lan-
guage input in the ﬁrst stage and Xvision represents the vision
input, i.e., the image. For example, X can be instantiated as
a concatenation of question, context, and options of a multi-
ple choice reasoning problem (Lu et al., 2022a) as shown in
Figure 4. The goal is to learn a rationale generation model
R = F(X) where R is the rationale.
In the answer inference stage, the rationale R is appended
to the original language input X1
language to construct the lan-
guage input in the second stage, X2
language = X1
language ◦R
where ◦denotes concatenation. Then, we feed the updated
input X′ = {X2
language, Xvision} to the answer inference
model to infer the ﬁnal answer A = F(X′).
In both stages, we train two models with the same archi-
tecture independently. They take the annotated elements
(e.g., X →R, XR →A, respectively) from the training
set for supervised learning. During inference, given X, the
rationales for the test sets are generated using the model
trained in the ﬁrst stage; they are used in the second stage
for answer inference.
4.2. Model Architecture
Given the language input Xlanguage ∈{X1
language, X2
language}
and the vision input Xvision, we compute the probability of
generating target text Y (either the rationale or the answer
in Figure 4) of length N by
p(Y |Xlanguage, Xvision) =
N
Y
i=1
pθ (Yi | Xlanguage, Xvision, Y<i) ,
(1)
where pθ (Yi | Xlanguage, Xvision, Y<i) is implemented with
a Transformer-based network (Vaswani et al., 2017). The
network has three major procedures: encoding, interaction,

Multimodal Chain-of-Thought Reasoning in Language Models
Algorithm 1 Multimodal-CoT
Input: Language input X1
language, vision input Xvision
Output: Generated rationale R, inferred answer A
1: Construct the input X = {Xlanguage, Xvision}
2: Generate rationale R = F(X) using the model F(·)
3: Append the rationale R to the original language input
X2
language = X1
language ◦R.
4: Construct new input X′ = {X2
language, Xvision}
5: Infer the answer A by conditioning on the new input, A =
F(X′).
6: procedure F(X)
7:
Encode the language and vision inputs Hlanguage and Hvision,
respectively
8:
Build the interaction between language and vision features
by attention Hattn
vision
9:
Fuse Hlanguage and Hattn
vision by a gated fusion mechanism to
have Hfuse
10:
Feed Hfuse to the decoder to obtain the target prediction Y
11:
return Y
12: end procedure
and decoding. Speciﬁcally, we feed the language text into
a Transformer encoder to obtain a textual representation,
which is then interacted and fused with the vision represen-
tation before being fed into the Transformer decoder.
Encoding
The model F(X) takes both the language and
vision inputs and obtains the text representation Hlanguage
and the image feature Hvision by the following functions:
Hlanguage
=
LanguageEncoder(Xlanguage),
(2)
Hvision
=
Wh · VisionExtractor(Xvision),
(3)
where LanguageEncoder(·) is implemented as a Trans-
former model. We use the hidden states of the last layer
in the Transformer encoder as the language representation
Hlanguage ∈Rn×d where n denotes the length of the lan-
guage input, and d is the hidden dimension. Meanwhile,
VisionExtractor(·) is used to vectorize the input image into
vision features. Inspired by the recent success of Vision
Transformers (Dosovitskiy et al., 2021), we fetch the patch-
level features by off-the-shelf vision extraction models,5
such as DETR (Carion et al., 2020). After obtaining the
patch-level vision features, we apply a learnable projection
matrix Wh to convert the shape of VisionExtractor(Xvision)
into that of Hlanguage; thus we have Hvision ∈Rm×d where
m is the number of patches.
Interaction
After obtaining language and vision represen-
tations, we use a single-head attention network to correlate
text tokens with image patches, where the query (Q), key
(K) and value (V) are Hlanguage, Hvision and Hvision, respec-
5The parameters of the vision extraction are frozen.
tively. The attention output Hattn
vision ∈Rn×d is deﬁned as:
Hattn
vision
=
Softmax(QK⊤
√dk
)V,
(4)
where dk is the same as the dimension of Hlanguage because
a single head is used.
Then, we apply the gated fusion mechanism (Zhang et al.,
2020; Wu et al., 2021; Li et al., 2022a) to fuse Hlanguage and
Hvision. The fused output Hfuse ∈Rn×d is obtained by:
λ
=
Sigmoid(WlHlanguage + WvHattn
vision),
(5)
Hfuse
=
(1 −λ) · Hlanguage + λ · Hattn
vision,
(6)
where Wl and Wv are learnable parameters.
Decoding
Finally, the fused output Hfuse is fed into the
Transformer decoder to predict the target Y . The complete
procedure of Multimodal-CoT is shown in Algorithm 1.
5. Experiments
This section will present the benchmark dataset, the imple-
mentation of our technique, and the baselines for compar-
isons. Then, we will report our main results and ﬁndings.
5.1. Dataset
Our method is evaluated on the ScienceQA benchmark (Lu
et al., 2022a). ScienceQA is the ﬁrst large-scale multimodal
science question dataset that annotates the answers with de-
tailed lectures and explanations. It contains 21k multimodal
multiple choice questions with rich domain diversity across
3 subjects, 26 topics, 127 categories, and 379 skills. The
benchmark dataset is split into training, validation, and test
splits with 12726, 4241, and 4241 examples, respectively.
5.2. Implementation
The following part presents the experimental settings of
Multimodal-CoT and the baseline methods.
Experimental Settings
As the Multimodal-CoT task re-
quires generating the reasoning chains and leveraging the
vision features, we use the T5 encoder-decoder architec-
ture (Raffel et al., 2020). Speciﬁcally, we adopt UniﬁedQA
(Khashabi et al., 2020) to initialize our models in the two
stages because it achieves the best ﬁne-tuning results in
Lu et al. (2022a). To verify the generality of our approach
across different LMs, we also employ FLAN-T5 (Chung
et al., 2022) as the backbone in Section 6.3. As using im-
age captions does not yield signiﬁcant performance gains in
Section 3.3, we did not use the captions. We ﬁne-tune the
models up to 20 epochs, with a learning rate of 5e-5. The

Multimodal Chain-of-Thought Reasoning in Language Models
Table 4. Main results (%). Size = backbone model size. Question classes: NAT = natural science, SOC = social science, LAN = language
science, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. Results except ours are
taken from Lu et al. (2022a). Segment 1: Human performance; Segment 2: VQA baselines; Segment 3: UniﬁedQA baselines; Segment 4:
GPT-3.5 baselines; Segment 5: Our Multimodal-CoT results. Results in bold are the best performance.
Model
Size
NAT
SOC
LAN
TXT
IMG
NO
G1-6
G7-12
Avg
Human
-
90.23
84.97
87.48
89.60
87.50
88.10
91.59
82.42
88.40
MCAN (Yu et al., 2019)
95M
56.08
46.23
58.09
59.43
51.17
55.40
51.65
59.72
54.54
Top-Down (Anderson et al., 2018)
70M
59.50
54.33
61.82
62.90
54.88
59.79
57.27
62.16
59.02
BAN (Kim et al., 2018)
112M
60.88
46.57
66.64
62.61
52.60
65.51
56.83
63.94
59.37
DFAF (Gao et al., 2019)
74M
64.03
48.82
63.55
65.88
54.49
64.11
57.12
67.17
60.72
ViLT (Kim et al., 2021)
113M
60.48
63.89
60.27
63.20
61.38
57.00
60.72
61.90
61.14
Patch-TRM (Lu et al., 2021)
90M
65.19
46.79
65.55
66.96
55.28
64.95
58.04
67.50
61.42
VisualBERT (Li et al., 2019)
111M
59.33
69.18
61.18
62.71
62.17
58.54
62.96
59.92
61.87
UniﬁedQABase (Khashabi et al., 2020)
223M
68.16
69.18
74.91
63.78
61.38
77.84
72.98
65.00
70.12
UniﬁedQABase w/ CoT (Lu et al., 2022a)
223M
71.00
76.04
78.91
66.42
66.53
81.81
77.06
68.82
74.11
GPT-3.5 (Chen et al., 2020)
175B
74.64
69.74
76.00
74.44
67.28
77.42
76.80
68.89
73.97
GPT-3.5 w/ CoT (Lu et al., 2022a)
175B
75.44
70.87
78.09
74.68
67.43
79.93
78.23
69.68
75.17
Mutimodal-CoTBase
223M
87.52
77.17
85.82
87.88
82.90
86.83
84.65
85.37
84.91
Mutimodal-CoTLarge
738M
95.91
82.00
90.82
95.26
88.80
92.89
92.44
90.31
91.68
Table 5. Ablation results of Multimodal-CoT.
Model
NAT
SOC
LAN
TXT
IMG
NO
G1-6
G7-12
Avg
Multimodal-CoT
87.52
77.17
85.82
87.88
82.90
86.83
84.65
85.37
84.91
w/o Two-Stage Framework
80.99
87.40
81.91
80.25
78.83
83.62
82.78
82.20
82.57
w/o Vision Features
71.09
70.75
69.18
71.16
65.84
71.57
71.00
69.68
70.53
maximum input sequence length is 512. The batch sizes for
the base and large models are 16 and 8, respectively. Our
experiments are run on 4 NVIDIA Tesla V100 32G GPUs.
Baseline Models
Following Lu et al. (2022a), our base-
lines include (i) Visual question answering (VQA) models
(Anderson et al., 2018; Kim et al., 2018; Yu et al., 2019;
Gao et al., 2019; Kim et al., 2021; Lu et al., 2021; Li et al.,
2019); (ii) Text-to-text LM models. (Khashabi et al., 2020);
(iii) GPT-3.5 models (Chen et al., 2020). More details are
presented in Appendix B.1.
5.3. Main Results
Table 4 shows the main results. Mutimodal-CoTLarge out-
performs GPT-3.5 by 16.51% (75.17%→91.68%) and sur-
passes human performance.
Speciﬁcally, among the 8
question classes, Mutimodal-CoTLarge achieves a 21.37%
(67.43%→88.80%) performance gain for the questions with
paired images (IMG). Compared with existing UniﬁedQA
and GPT-3.5 methods that leverage image captions in the
context to provide vision semantics, the results indicate that
using image features is more effective. In addition, our
two-stage framework contributes to the superior results ac-
cording to our ablation study results in Table 5. Overall,
the results verify the effectiveness of multimodality and the
potential of achieving CoT reasoning with 1B-models via
our two-stage framework.
1
2
3
4
5
6
7
8
9
10
40
50
60
70
80
90
Epoch
Accuracy
One-stage Baseline
One-stage Multimodal
Two-Stage Baseline
Two-Stage Multimodal
Figure 5. Accuracy curve of the No-CoT baseline and Multimodal-
CoT variants across epochs.
6. Analysis
The following analysis will investigate how Multimodal-
CoT works and discuss contribution factors and limitations.
We use models under the base size for analysis unless
otherwise stated.
6.1. Multimodality Boosts Convergence
Figure 5 shows the evaluation accuracy curve of the baseline
and Multimodal-CoT in different training epochs. “One-
stage” is based on the QCM→A input-output format as it

Multimodal Chain-of-Thought Reasoning in Language Models
achieves the best performance in Table 2 and “Two-stage”
is our two-stage framework. We ﬁnd that the two-stage
methods achieve relatively higher accuracy at the beginning
than the one-stage baselines that generate the answer directly
without CoT. However, without the vision features, the two-
stage baseline could not yield better results as the training
goes on due to the low-quality rationales (as observed in
Section 3). In contrast, using vision features helps generate
more effective rationales that contribute to better answer
accuracy in our two-stage multimodal variant.
6.2. Using Different Vision Features
Different vision features may affect the model performance.
We compare three widely-used types of vision features,
CLIP (Radford et al., 2021), DETR (Carion et al., 2020),
and ResNet (He et al., 2016). CLIP and DETR are patch-like
features where DETR is based on object detection. For the
ResNet features, we repeat the pooled features of ResNet-
50 to the same length with the text sequence to imitate the
patch-like features, where each patch is the same as the
pooled image features. More details of the vision features
are presented in Appendix B.2.
Table 6. Accuracy (%) of using different vision features.
Method
One-stage
Two-Stage
w/ CLIP
81.21
84.81
w/ DETR
82.57
84.91
w/ ResNet
80.97
84.77
Table 6 shows the comparative results of vision features. We
observe that using vision features generally achieves better
performance than the language only baseline. Speciﬁcally,
DETR achieves relatively better performance in general.
Therefore, we use DETR by default in Multimodal-CoT.
6.3. General Effectiveness Across Backbone Models
To test the generality of the beneﬁts of our approach to
other backbone models, we alter the underlying LMs to
other variants in different sizes or types. As shown in Table
7, our approach is generally effective for the widely-used
backbone models.
Table 7. Accuracy (%) with different backbone language models.
Method
Size
Language Only Mutimodal-CoT
UniﬁedQABase
223M
80.40
84.91
UniﬁedQALarge 738M
83.60
91.68
FLAN-T5Base
248M
83.42
85.85
FLAN-T5Large
783M
85.19
93.02
6.4. Error Analysis
To better understand the behavior of Multimodal-CoT and
facilitate future studies, we manually investigate randomly
selected examples generated by our approach. Table 8 sum-
marizes the categorization results generated by Multimodal-
CoT. We randomly picked up 50 samples whose answers
were correct and 50 samples whose answers were incor-
rect. The corresponding examples from each category are
presented in Appendix C.
Table 8. Categorization analysis of Multimodal-CoT.
Answer
CoT Category
Percentage (%)
Correct
CoT is correct
90
CoT is incorrect
10
Incorrect
Commonsense Mistake
82
Logical Mistake
12
CoT is correct
6
We ﬁnd that the correct samples (i.e., whose answers are cor-
rect) contain a certain amount of incorrect chain-of-thought
(10%). The results indicate that CoT may not always beneﬁt
the answer inference, and the model is robust to some extent
— it can predict the correct answer by ignoring incorrect
rationales. For incorrect samples (i.e., whose answers are
incorrect), commonsense mistake in the CoT is the most
frequent error type (88%). The model often makes com-
monsense mistakes when answering the questions requires
commonsense knowledge, e.g., understand maps and count-
ing numbers in the images (Figure 9), and utilizing the
alphabet (Figure 10). The other type of mistake is a logical
mistake (12%), with contradictions in the reasoning chains
(Figure 11). In addition, there are cases with incorrect an-
swers while their CoT are correct (6%) but might not be
necessarily related to answer options (Figure 12).
The analysis indicates that there are prospective directions
for future studies. It is possible to improve Multimodal-
CoT by (i) incorporating more informative vision features
and improving language-vision interaction to be capable of
understanding maps and counting numbers; (ii) injecting
commonsense knowledge; (iii) applying a ﬁltering mecha-
nism, e.g., using only the effective CoT to infer the answer
and get rid of irrelevant CoT.
7. Conclusion
We formally study the problem of multimodal CoT. We pro-
pose Multimodal-CoT that incorporates language and vision
modalities into a two-stage framework that separates ratio-
nale generation and answer inference, so answer inference
can leverage better generated rationales from multimodal in-
formation. With Multimodal-CoT, we show that our method
surpasses GPT-3.5 by 16 percentage points in accuracy on
the ScienceQA benchmark. Our error analysis shows that
it is the potential to leverage more effective vision features,
inject commonsense knowledge, and apply ﬁltering mecha-
nisms to improve CoT reasoning in future studies.

Multimodal Chain-of-Thought Reasoning in Language Models
References
Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,
Gould, S., and Zhang, L. Bottom-up and top-down atten-
tion for image captioning and visual question answering.
In 2018 IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR 2018, Salt Lake City, UT, USA,
June 18-22, 2018, pp. 6077–6086. IEEE Computer Soci-
ety, 2018. doi: 10.1109/CVPR.2018.00636.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,
J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
S., Radford, A., Sutskever, I., and Amodei, D. Language
models are few-shot learners. In Larochelle, H., Ranzato,
M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances
in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
A., and Zagoruyko, S. End-to-end object detection with
transformers. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part I, pp. 213–229, 2020.
Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and
Hinton, G. E. Big self-supervised models are strong
semi-supervised learners. In Larochelle, H., Ranzato, M.,
Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances
in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program
of thoughts prompting: Disentangling computation from
reasoning for numerical reasoning tasks. ArXiv preprint,
abs/2211.12588, 2022.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton,
C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,
Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,
Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,
Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fe-
dus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,
B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,
S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,
K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,
Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,
J., Petrov, S., and Fiedel, N. Palm: Scaling language mod-
eling with pathways. ArXiv preprint, abs/2204.02311,
2022.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-ﬁnetuned language models.
arXiv preprint arXiv:2210.11416, 2022.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. In
The International Conference on Learning Representa-
tions (ICLR), 2021.
Fu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot, T.
Complexity-based prompting for multi-step reasoning.
ArXiv preprint, abs/2210.00720, 2022.
Gao, P., Jiang, Z., You, H., Lu, P., Hoi, S. C. H., Wang, X.,
and Li, H. Dynamic fusion with intra- and inter-modality
attention ﬂow for visual question answering. In IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp.
6639–6648. Computer Vision Foundation / IEEE, 2019.
doi: 10.1109/CVPR.2019.00680.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2016,
Las Vegas, NV, USA, June 27-30, 2016, pp. 770–778.
IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.
90.
Ho, N., Schmid, L., and Yun, S.-Y. Large language models
are reasoning teachers. arXiv preprint arXiv:2212.10071,
2022.
Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E.,
Bang, Y., Madotto, A., and Fung, P. Survey of halluci-
nation in natural language generation. ACM Computing
Surveys, 2022.
Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord,
O., Clark, P., and Hajishirzi, H. UNIFIEDQA: Crossing
format boundaries with a single QA system. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2020, pp. 1896–1907, Online, 2020. Association
for Computational Linguistics. doi: 10.18653/v1/2020.
ﬁndings-emnlp.171.
Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K.,
Clark, P., and Sabharwal, A. Decomposed prompting:
A modular approach for solving complex tasks. ArXiv
preprint, abs/2210.02406, 2022.

Multimodal Chain-of-Thought Reasoning in Language Models
Kim, J., Jun, J., and Zhang, B. Bilinear attention networks.
In Bengio, S., Wallach, H. M., Larochelle, H., Grauman,
K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances
in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems
2018, NeurIPS 2018, December 3-8, 2018, Montr´eal,
Canada, pp. 1571–1581, 2018.
Kim, W., Son, B., and Kim, I. Vilt: Vision-and-language
transformer without convolution or region supervision.
In Proceedings of the 38th International Conference on
Machine Learning (ICML), pp. 5583–5594, 2021.
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,
Y. Large language models are zero-shot reasoners. ArXiv
preprint, abs/2205.11916, 2022.
Li, B., Lv, C., Zhou, Z., Zhou, T., Xiao, T., Ma, A., and Zhu,
J. On vision features in multimodal machine translation.
In Proceedings of the 60th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1: Long
Papers), pp. 6327–6337, 2022a.
Li, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,
K.-W. Visualbert: A simple and performant baseline for
vision and language. ArXiv preprint, abs/1908.03557,
2019.
Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and
Chen, W. On the advance of making language models
better reasoners. ArXiv preprint, abs/2206.02336, 2022b.
Lu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y., Zhang, W., Yu,
Z., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark
for abstract diagram understanding and visual language
reasoning. In The 35th Conference on Neural Information
Processing Systems (NeurIPS) Track on Datasets and
Benchmarks, 2021.
Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,
S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to
explain: Multimodal reasoning via thought chains for sci-
ence question answering. ArXiv preprint, abs/2209.09513,
2022a.
Lu, P., Qiu, L., Chang, K.-W., Wu, Y. N., Zhu, S.-C., Ra-
jpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt
learning via policy gradient for semi-structured mathemat-
ical reasoning. ArXiv preprint, abs/2209.14610, 2022b.
Magister, L. C., Mallinson, J., Adamek, J., Malmi, E., and
Severyn, A. Teaching small language models to reason.
ArXiv preprint, abs/2212.08410, 2022.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
et al. Learning transferable visual models from natural
language supervision. In International Conference on
Machine Learning, pp. 8748–8763. PMLR, 2021.
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,
Song, F., Aslanides, J., Henderson, S., Ring, R., Young,
S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A.,
Powell, R., Driessche, G. v. d., Hendricks, L. A., Rauh,
M., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S.,
Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,
A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,
Buchatskaya, E., Budden, D., Sutherland, E., Simonyan,
K., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-
coro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,
Lazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,
M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,
Pohlen, T., Gong, Z., Toyama, D., d’Autume, C. d. M.,
Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A.,
Casas, D. d. L., Guy, A., Jones, C., Bradbury, J., Johnson,
M., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W.,
Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals,
O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D.,
Kavukcuoglu, K., and Irving, G. Scaling language mod-
els: Methods, analysis & insights from training gopher.
ArXiv preprint, abs/2112.11446, 2021.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the
limits of transfer learning with a uniﬁed text-to-text trans-
former. Journal of Machine Learning Research (JMLR),
21:1–67, 2020.
Rubin, O., Herzig, J., and Berant, J.
Learning to re-
trieve prompts for in-context learning. In Proceedings
of the 2022 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, pp. 2655–2671, 2022. doi:
10.18653/v1/2022.naacl-main.191.
Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-
shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,
Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Mene-
gali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J.,
Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M.,
Zhao, V., Zhou, Y., Chang, C.-C., Krivokon, I., Rusch,
W., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern,
K., Morris, M. R., Doshi, T., Santos, R. D., Duke, T.,
Soraker, J., Zevenbergen, B., Prabhakaran, V., Diaz, M.,
Hutchinson, B., Olson, K., Molina, A., Hoffman-John, E.,
Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm,
M., Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R.,
Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi,
E., and Le, Q. Lamda: Language models for dialog
applications. ArXiv preprint, abs/2201.08239, 2022.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention
is all you need. In Guyon, I., von Luxburg, U., Bengio,
S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N.,
and Garnett, R. (eds.), Advances in Neural Information

Multimodal Chain-of-Thought Reasoning in Language Models
Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA, pp. 5998–6008, 2017.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,
and Zhou, D.
Self-consistency improves chain of
thought reasoning in language models. ArXiv preprint,
abs/2203.11171, 2022a.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and
Zhou, D. Rationale-augmented ensembles in language
models. ArXiv preprint, abs/2207.00747, 2022b.
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,
Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-
zler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,
P., Dean, J., and Fedus, W. Emergent abilities of large
language models. Transactions on Machine Learning
Research, 2022a. Survey Certiﬁcation.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,
Le, Q., and Zhou, D. Chain of thought prompting elic-
its reasoning in large language models. ArXiv preprint,
abs/2201.11903, 2022b.
Wu, Z., Kong, L., Bi, W., Li, X., and Kao, B. Good for
misconceived reasons: An empirical revisiting on the
need for visual context in multimodal machine transla-
tion. In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pp. 6153–6166, Online,
2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.acl-long.480.
Yu, Z., Yu, J., Cui, Y., Tao, D., and Tian, Q. Deep modu-
lar co-attention networks for visual question answering.
In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June
16-20, 2019, pp. 6281–6290. Computer Vision Founda-
tion / IEEE, 2019. doi: 10.1109/CVPR.2019.00644.
Zhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,
Z., and Zhao, H. Neural machine translation with univer-
sal visual representation. In 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic
chain of thought prompting in large language models.
ArXiv preprint, abs/2210.03493, 2022.
Zhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li,
Z., and Zhao, H. Universal multimodal representation for
language understanding. IEEE Transactions on Pattern
Analysis and Machine Intelligence, pp. 1–18, 2023. doi:
10.1109/TPAMI.2023.3234170.
Zhou, D., Sch¨arli, N., Hou, L., Wei, J., Scales, N., Wang,
X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.
Least-to-most prompting enables complex reasoning in
large language models. ArXiv preprint, abs/2205.10625,
2022.

Multimodal Chain-of-Thought Reasoning in Language Models
A. Extended analysis for the challenge of Multimodal-CoT
A.1. More Examples of Misleading by Hallucinated Rationales
According to our case studies (Section 3.2), we ﬁnd that the baseline tends to generate hallucinated rationales. We provide
further examples as shown in Figure 6.
Options:
(B) lichen
(A) mushroom
Problem
Baseline
Question: Based on the arrows, which of the following organisms is a consumer?
Context: Below is a food web from a tundra ecosystem in Nunavut, a territory in Northern Canada.
A food web models how the matter eaten by organisms moves through an ecosystem. The arrows in a food web
represent how matter moves between organisms in an ecosystem.
Gold Rationale: A food web is a model … An organism in a food web can have more than one arrow pointing from it. This shows that the organism is
eaten by more than one other organism in the food web. An organism in a food web can also have more than one arrow pointing to it. This shows that
the organism eats more than one other organism in the food web. Consumers eat other organisms. So, there are arrows in a food web that point from
other organisms to consumers. The mushroom has arrows pointing to it from the barren-ground caribou and the grizzly bear. So, the mushroom is a
consumer. The lichen does not have any arrows pointing to it. So, the lichen is a producer, not a consumer.
Answer: The answer is (A).
Rationale: A food web is a model … This shows that the organism is
eaten by more than one other organism in the food web. An organism in
a food web can also have more than one arrow pointing to it. This
shows that the organism eats more than one other organism in the food
web. Consumers eat producers. So, in a food web, consumers have
arrows pointing to them from producers. The mushroom has an arrow
pointing to it from the mushroom. The mushroom is not a consumer, so
the mushroom is not a consumer. The mushroom does not have any
arrows pointing to it. So, the mushroom is not a consumer. The
mushroom does not have any arrows pointing to it. So, the mushroom is
not a consumer. The mushroom does not have any arrows pointing to it.
So, the mushroom is not a consumer. The mushroom does not have
any arrows pointing to it. So, the mushroom is not a consumer.
Answer: The answer is (B).
Rationale: A food web is a model … An organism in a food web can
have more than one arrow pointing from it. This shows that the
organism is eaten by more than one other organism in the food web. An
organism in a food web can also have more than one arrow pointing to it.
This shows that the organism eats more than one other organism in the
food web. Consumers eat producers organisms. So, in are arrows
a
food web that point from other organisms to consumers. The mushroom
has arrows pointing to it from the n-brow mushroomibou. the lichenzzly
bear. So, the mushroom is a consumer. The lichen has not have any
arrows pointing to it. So, the mushroom is a consumer. Lichen is not a
consumer.
Answer: The answer is (B).
Vision
+ Vision Features
Options:
(B) sample A
(A) neither; the samples have the same temperature
Problem
Baseline
Question: Compare the average kinetic energies of the particles in each sample. Which sample has the higher
temperature?
Context: The diagrams below show two pure samples of gas in identical closed, rigid containers. Each colored
ball represents one gas particle. Both samples have the same number of particles.
Gold Rationale: The temperature of a substance depends on the average kinetic energy of the particles in the
substance. The higher the average kinetic energy of the particles, the higher the temperature of the substance. The
kinetic energy of a particle is determined by its mass and speed. For a pure substance, the greater the mass of
each particle in the substance and the higher the average speed of the particles, the higher their average kinetic
energy. Each particle in sample A has more mass than each particle in sample B. The particles in sample A also
+ Vision Features
Vision
(C) sample B
have a higher average speed than the particles in sample B. So, the particles in sample A have a higher average kinetic energy than the particles in
sample B. Because the particles in sample A have the higher average kinetic energy, sample A must have the higher temperature.
Answer: The answer is (B).
Rationale: The temperature of a substance depends on the average
kinetic energy of the particles in the substance. The higher the average
kinetic energy of the particles, the higher the temperature of the
substance. The kinetic energy of a particle is determined by its mass and
speed. For a pure substance, the greater the mass of each particle in the
substance and the higher the average speed of the particles, the higher
their average kinetic energy. Each particle in sample B has more mass
than each particle in sample A. The particles in sample B also have a
higher average speed than the particles in sample A. So, the particles in
sample B have a higher average kinetic energy than the particles in
sample A. Because the particles in sample B have the higher average
kinetic energy, sample B must have the higher temperature.
Answer: The answer is (C).
Rationale: The temperature of a substance depends on the average
kinetic energy of the particles in the substance. The higher the average
kinetic energy of the particles, the higher the temperature of the
substance. The kinetic energy of a particle is determined by its mass and
speed. For a pure substance, the greater the mass of each particle in the
substance and the higher the average speed of the particles, the higher
their average kinetic energy. Each particle in sample A has more mass
than each particle in sample B. The particles in sample A also have a
higher average speed than the particles in sample B. So, the particles in
sample A have a higher average kinetic energy than the particles in
sample B. Because the particles in sample A have the higher average
kinetic energy, sample A must have the higher temperature.
Answer: The answer is (B).
Figure 6. Examples of the two-stage framework without vision features (baseline) and with vision features (ours) for generating rationales
and predicting answers. The upper part presents the problem details, and the lower part shows the outputs of the baseline and our method.

Multimodal Chain-of-Thought Reasoning in Language Models
A.2. Two-Stage Training Performance with Different Sizes of LMs.
In Section 3, we obverse that incorporating vision features helps generate more effective rationales, thus leading to improved
answer accuracy. Besides incorporating vision features, it is possible to scale the LM size to mitigate the issue of incorrect
rationales. Figure 7 shows the answer accuracy with UniﬁedQABase and UniﬁedQALarge. When using a larger LM, the
accuracy of the baseline (w/o vision features) is boosted. The result indicates that scaling the LM is possible to mitigate the
issue of incorrect rationales. However, the performance is still much inferior to using vision features. The result further
veriﬁes the effectiveness of our Multimodal-CoT with different sizes of LMs.
base
large
65
80
100
70.53
84.91
82.97
91.68
Accuracy (%)
w/o Vision Modality
w/ Vision Modality
Figure 7. Answer accuracy with different sizes of LMs.
B. Experimental Details
B.1. Baseline Methods
Following Lu et al. (2022a), our baselines include three types of methods:
(i) Visual question answering (VQA) models (Yu et al., 2019; Anderson et al., 2018; Kim et al., 2018; Gao et al., 2019; Lu
et al., 2021; Li et al., 2019). The VQA baselines take the question, the context, and choices as the textual input, take the
image as the vision input, and predict the score distribution over choice candidates via a linear classiﬁer.
(ii) Text-to-text LM models. UniﬁedQA (Khashabi et al., 2020) is adopted as it is the best ﬁne-tuning model in Lu et al.
(2022a). UniﬁedQA takes the textual information as the input and outputs the answer option. The image is converted into a
caption extracted by an image captioning model based on ViT and GPT-2.6 UniﬁedQA treats our task as a text generation
problem. In Lu et al. (2022a), it is trained to generate a target answer text, i.e., one of the candidate options. Then, the most
similar option is selected as the ﬁnal prediction to evaluate the question answering accuracy.
(iii) GPT-3.5 models (Chen et al., 2020) based on the text-davinci-002 engine. The inference is based on the few-shot
prompting, where two in-context examples from the training set are concatenated before the test instance.
For UniﬁedQA and GPT-3.5, CoT is applied after the answer (Lu et al., 2022a). Besides the above baselines, we develop a
stronger baseline with a slight modiﬁcation of the output format of UniﬁedQA. Instead of predicting the answer texts, our
baseline directly predicts the choice, e.g., the answer is B. This setting helps our baseline achieve better results than the
existing UniﬁedQA. Therefore, we use the stronger method as the language only baseline for analysis.
B.2. Details of Vision Features
In Section 6.2, we compared four types of vision features, CLIP (Radford et al., 2021), DETR (Carion et al., 2020), and
ResNet (He et al., 2016). The speciﬁc models are: (i) CLIP: RN101;7 (ii) DETR: detr resnet101 dc5;8 (iii) ResNet: we use
6https://huggingface.co/nlpconnect/vit-gpt2-image-captioning.
7https://github.com/jianjieluo/OpenAI-CLIP-Feature.
8https://github.com/facebookresearch/detr.

Multimodal Chain-of-Thought Reasoning in Language Models
the averaged pooled features of a pre-trained ResNet50 CNN. Table 9 presents the dimension of the vision features (after the
function VisionExtractor(·) in Eq. 3). For ResNet-50, we repeat the pooled features of ResNet-50 to the same length as the
text sequence to imitate the patch-like features, where each patch is the same as the pooled image features.
Table 9. Dimension of vision features
Method
Dimension
CLIP
(49, 2048)
DETR
(100, 256)
ResNet
(512, 2048)
C. Examples of Case Studies
To better understand the behavior of Multimodal-CoT, we manually investigate randomly selected examples generated by
our approach. Table 8 summarizes the categorization results generated by Multimodal-CoT. We randomly picked up 50
samples whose prediction results were correct and 50 samples whose prediction results were incorrect.
We ﬁnd that the correct samples contain a certain amount of incorrect chain-of-thought. As shown in Figure 8(b), the model
generates the incorrect rationale, “Animals cannot their food by digesting other organisms” but the predicted answer is
correct. The result indicates that CoT may not always beneﬁt the answer inference, and the model is robust to some extent —
it can predict the correct answer by ignoring incorrect rationales.
For incorrect samples, commonsense mistake is the most frequent error type. The model also makes commonsense mistakes
when answering the questions requires commonsense knowledge, e.g., understand maps and counting numbers in the
images (Figure 9), and utilizing the alphabet (Figure 10). The other type of mistake is the logical mistake, where there are
contradictions in the reasoning chains (Figure 11). In addition, there are cases that the CoT is correct but might not be
necessarily related to answer options; thus the model chooses the incorrect answer.
The analysis indicates that there are prospective directions for future studies. On the one hand, it is possible to improve the
quality of CoT by (i) using more ﬁne-grained interaction of language and vision features; and (ii) injecting commonsense
knowledge. On the other hand, applying a ﬁltering mechanism to using only the effective CoT to infer the answer and
eliminate irrelevant CoT.

Multimodal Chain-of-Thought Reasoning in Language Models
Rationale: Birds, mammals, fish, reptiles, and amphibians are groups of animals. The animals in each group have traits in common. Scientists sort
animals into groups based on traits they have in common. This process is called classification. A tiger shark is a fish. It lives underwater. It has fins, not
limbs. Tiger sharks can nocturnal. They means that they are active mostly at night. A red-tailed hawk is a bird. It has feathers, two wings, and a beak.
Red-tailed hawks live their be teethaks to push for pre. birds, and, rep amptiles…
Answer: The answer is (B).
Options:
(A) red-tailed hawk
Problem
Prediction
Question: Select the bird below.
Context: Birds have feathers, two wings, and a beak. A peregrine falcon is an example of a bird.
Rationale : Birds, mammals, fish, reptiles, and amphibians are groups of animals. The animals in each group have traits
in common. Scientists sort animals into groups based on traits they have in common. This process is called classification.
A tiger shark is a fish. It lives underwater. It has fins, not limbs. Tiger sharks can nocturnal. They means that they are
active mostly at night. A red-tailed hawk is a bird. It has feathers, two wings, and a beak. Red-tailed hawks live their be
teethaks to push for pre. birds, and, rep amptiles…
Answer: The answer is (A).
Vision
(B) tiger shark
(a) CoT is correct
Rationale:
Leopard the past, scientists classified living organisms into two groups: plants and animals. Over the past 300 years, scientists have
discovered many more types of organisms. Today, many scientists classify organisms into six broad groups, called kingdoms. Organisms in each
kingdom have specific traits. The table below shows some traits used to describe each kingdom. | Bacteria | Archaea | Protists | Fungi | Animals |
Plants. How many cells do they have? | one | one | one or many | one or many | many | many. Do their cells have a nucleus? | no | no | yes | yes | yes |
yesnCan their cells make food? | some species can | some species can | some species can | no | no | yes Leopardus wiedii is an animal. Animal cells
cannot make their own food. Animals cannot their food by digesting other organisms.
Answer: The answer is (B).
Options:
(A) no
Problem
Prediction
Question: Can Leopardus wiedii cells make their own food?
Context: This organism is Leopardus wiedii. It is a member of the animal kingdom. Leopardus wiedii is commonly called
a margay. Margays live in the forest in Central and South America. Margays are nocturnal. A nocturnal animal sleeps
most of the day and is awake at night.
Rationale : In the past, scientists classified living organisms into two groups: plants and animals. Over the past 300 years,
scientists have discovered many more types of organisms. Today, many scientists classify organisms into six broad
groups, called kingdoms. Organisms in each kingdom have specific traits. The table below shows some traits used to
describe each kingdom. | Bacteria | Archaea | Protists | Fungi | Animals | Plants. How many cells do they have? | one |
one | one or many | one or many | many | many Do their cells have a nucleus? | no | no | yes | yes | yes | yes Can their
cells make food? | some species can | some species can | some species can | no | no | yes Leopardus wiedii is an animal.
Animal cells cannot make their own food. Animals get their food by digesting other organisms.
Answer: The answer is (A).
Vision
(B) yes
(b) CoT is incorrect
Figure 8. Examples of answers are correct while the CoT is correct (a) or incorrect (b).

Multimodal Chain-of-Thought Reasoning in Language Models
Rationale: Oceans are huge bodies of salt water. The world has five oceans. All of the oceans are connected, making one world ocean. This is the
Indian Ocean.
Answer: The answer is (B).
Options:
(A) the Atlantic Ocean
Problem
Prediction
Question: Which ocean is highlighted?
Context:
N/A
Rationale : Oceans are huge bodies of salt water. The world has five oceans. All of the oceans are
connected, making one world ocean. This is the Pacific Ocean.
Answer: The answer is (C).
Vision
(B) the Indian Ocean
(a) Mistakes about maps.
Rationale: In a solution, solute particles move and spread throughout the solvent. The diagram below shows how a solution can change over time.
Solute particles move from the area where they are at a higher concentration to the area where they are at a lower concentration … Look at the
diagram again. It shows you how the solution changed during the process of diffusion. Before the solute particles reached equilibrium, there were 5
solute particles on the left side of the membrane and 4 solute particles on the right side of the membrane. When the solute particles reached
equilibrium, there were 5 solute particles on each side of the membrane. There was 1 more solute particle on the right side of the membrane than
before. So, for the solute particles to reach equilibrium, more solute particles must have moved across the membrane to the right than to the right.
Answer: The answer is (B).
Options:
(A) to the left than to the right
Problem
Prediction
Question: Complete the text to describe the diagram. Solute particles moved in both directions across the permeable
membrane. But more solute particles moved across the membrane (). When there was an equal concentration on both sides,
the particles reached equilibrium.
Context: The diagram below shows a solution with one solute. Each solute particle is represented by a green ball. The
solution fills a closed container that is divided in half by a membrane. The membrane, represented by a dotted line, is
permeable to the solute particles. The diagram shows how the solution can change over time during the process of diffusion.
Rationale :
In a solution, solute particles move and spread throughout the solvent. The diagram below shows how a
solution can change over time. Solute particles move from the area where they are at a higher concentration to the area
where they are at a lower concentration … Look at the diagram again. It shows you how the solution changed during the
process of diffusion. Before the solute particles reached equilibrium, there were 3 solute particles on the left side of the
membrane and 5 solute particles on the right side of the membrane. When the solute particles reached equilibrium, there
were 4 solute particles on each side of the membrane. There was 1 more solute particle on the left side of the membrane
than before. So, for the solute particles to reach equilibrium, more solute particles must have moved across the membrane
to the left than to the right.
Answer: The answer is (A).
Vision
(b) Mistakes about counting numbers in the image.
(C) the Pacific Ocean
(D) the Southern Ocean
(B) to the right than to the left
Figure 9. Examples of commonsense mistakes about understanding maps and counting numbers.

Multimodal Chain-of-Thought Reasoning in Language Models
Rationale: Guide words appear on each page of a dictionary. They tell you the first word and last word on the page. The other words on the page
come between the guide words in alphabetical order. To put words in alphabetical order, put them in order by their first letters. If the first letters are the
same, look at the second letters. If the second letters are the same, look at the third letters, and so on. If one word is shorter, and there are no more
letters to compare, then the shorter word comes first in alphabetical order. For example, be comes before bed. Put the words in alphabetical order.
Since hardship is between the guide words helping - hunter, it would be found on that page.
Answer: The answer is (B).
Options:
(A) hospital
Problem
Prediction
Question: Which word would you find on a dictionary page with the following guide words? helping - hunter
Context: The diagram below shows a solution with one solute. Each solute particle is represented by a green ball. The
solution fills a closed container that is divided in half by a membrane. The membrane, represented by a dotted line, is
permeable to the solute particles. The diagram shows how the solution can change over time during the process of diffusion.
Rationale : Guide words appear on each page of a dictionary. They tell you the first word and last word on the page. The other words on the page
come between the guide words in alphabetical order. To put words in alphabetical order, put them in order by their first letters. If the first letters are the
same, look at the second letters. If the second letters are the same, look at the third letters, and so on. If one word is shorter, and there are no more
letters to compare, then the shorter word comes first in alphabetical order. For example, be comes before bed. Put the words in alphabetical order.
Since hospital is between the guide words helping - hunter, it would be found on that page.
Answer: The answer is (A).
(B) hardship
Vision
N/A
Figure 10. Examples of commonsense mistakes about utilizing alphabet.

Multimodal Chain-of-Thought Reasoning in Language Models
Rationale: Measurements are written with both a number and a unit. The unit comes after the number. The unit shows what the number means.
Temperature can be written with units of degrees Fahrenheit (°F) or Celsius (°C). Use the list below to compare the two units. 212°F | Water boils | 100
° C 98.6 °F | Body temperature | 37°C 68°F | Room temperature | 20°C 32°F | Water freezes | 0°C. The better estimate for the temperature of the air on
a hot day is 36°F. 36°F is too hot.
Answer: The answer is (A).
Options:
(A) 36°F
Problem
Prediction
Question: What is the temperature of the air on a hot day?
Context: Select the better estimate.
Rationale : Measurements are written with both a number and a unit. The unit comes after the number. The unit shows what the number means.
Temperature can be written with units of degrees Fahrenheit (°F) or Celsius (°C). Use the list below to compare the two units. 212°F | Water boils |
100 ° C 98.6 °F | Body temperature | 37°C 68°F | Room temperature | 20°C 32°F | Water freezes | 0°C. The better estimate for the temperature of
the air on a hot day is 36°C. 36°F is too cold.
Answer: The answer is (B).
(B) 36°C
Vision
N/A
Rationale: :
A sentence is a group of words that expresses a complete thought. The band I'm in has been rehearsing daily because we have a
concert in two weeks. A run-on sentence is formed when two sentences are run together, joined by just a comma or by no punctuation at all. If only a
comma is used, the run-on is called a comma splice … The band I'm in has been rehearsing daily since we have a concert in two weeks. This is not a
run-on sentence. It is formed a comma splice formed from two sentences run together, joined without just a comma. Right-handedness is controlled by
genetics, interestingly enough, the same genes are also responsible for the brain becoming more specialized at certain tasks.
Answer: The answer is (B).
Options:
(A) yes
Problem
Prediction
Question: Is this a run-on sentence? Right-handedness is controlled by genetics, interestingly enough, the same genes are
also responsible for the brain becoming more specialized at certain tasks.
Context: N/A
Rationale : A sentence is a group of words that expresses a complete thought. The band I'm in has been rehearsing daily because we have a
concert in two weeks. A run-on sentence is formed when two sentences are run together, joined by just a comma or by no punctuation at all. If only a
comma is used, the run-on is called a comma splice … The band I'm in has been rehearsing daily since we have a concert in two weeks. This is a
run-on sentence. It is a comma splice formed from two sentences run together, joined by just a comma. Right-handedness is controlled by genetics,
interestingly enough, the same genes are also responsible for the brain becoming more specialized at certain tasks.
Answer: The answer is (A).
(B) no
Vision
N/A
(a) Logical mistake where the model fails at comparisons.
(b) Logical mistake where the there is a contradiction in the rationale.
Figure 11. Examples of logical mistakes.

Multimodal Chain-of-Thought Reasoning in Language Models
Options:
(A) black stripes on its skin
Problem
Prediction
Rationale : The way an organism looks or acts is called a trait. Scientists use fossils to learn more about the traits of ancient organisms. Fossils can
preserve the remains of body parts and activities. A fossil of a body part, such as a tail or a wing, can tell you what an organism looked like. A fossil of
an organism's activities, such as a burrow or a footprint, can tell you about the organism's behavior. Here are three examples of fossils and the traits
that you can observe from them: This is a fossil of an animal. This fossil tells you that the animal had a spiral-shaped shell. This is a fossil of a plant.
This fossil tells you that the plant had small leaves arranged in a branched pattern. This is a fossil of an animal's footprint. This fossil tells you that the
animal could walk on land. An organism's fossil may not show all of the organism's traits. This is because most body parts are destroyed during fossil
formation. When an organism's body turns into a fossil, only a few body parts are usually preserved.
Answer: The answer is (C).
(B) large fins on its body
Vision
(C) a long, thin body
Question: Which trait did Palaeopython have? Select the trait you can observe on the fossil.
Context: This picture shows a fossil of an ancient animal called Palaeopython. Palaeopython lived in trees and could
grow more than six feet long.
Rationale : The way an organism looks or acts is called a trait. Scientists use fossils to learn more about the traits of ancient organisms. Fossils can
preserve the remains of body parts and activities. A fossil of a body part, such as a tail or a wing, can tell you what an organism looked like. A fossil of
an organism's activities, such as a burrow or a footprint, can tell you about the organism's behavior. Here are three examples of fossils and the traits
that you can observe from them: This is a fossil of an animal. This fossil tells you that the animal had a spiral-shaped shell. This is a fossil of a plant.
This fossil tells you that the plant had small leaves arranged in a branched pattern. This is a fossil of an animal's footprint. This fossil tells you that the
animal could walk on land. An organism's fossil may not show all of the organism's traits. This is because most body parts are destroyed during fossil
formation. When an organism's body turns into a fossil, only a few body parts are usually preserved.
Answer: The answer is (B).
Figure 12. Examples of answers are incorrect while the CoT is correct.

