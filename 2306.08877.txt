Linguistic Binding in Diffusion Models:
Enhancing Attribute Correspondence
through Attention Map Alignment
Royi Rassin
Bar-Ilan University, Israel
rassinroyi@gmail.com
Eran Hirsch
Bar-Ilan University, Israel
eran.hirsch@biu.ac.il
Daniel Glickman
Bar-Ilan University, Israel
danielglickman1@gmail.com
Shauli Ravfogel
Bar-Ilan University, Israel
Allen Institute for AI, Israel
shauli.ravfogel@gmail.com
Yoav Goldberg
Bar-Ilan University, Israel
Allen Institute for AI, Israel
yoav.goldberg@gmail.com
Gal Chechik
Bar-Ilan University, Israel
NVIDIA, Israel
gal.chechik@biu.ac.il
Abstract
Text-conditioned image generation models often generate incorrect associations
between entities and their visual attributes. This reflects an impaired mapping
between linguistic binding of entities and modifiers in the prompt and visual
binding of the corresponding elements in the generated image. As one notable
example, a query like “a pink sunflower and a yellow flamingo” may incorrectly
produce an image of a yellow sunflower and a pink flamingo. To remedy this issue,
we propose SynGen, an approach which first syntactically analyses the prompt
to identify entities and their modifiers, and then uses a novel loss function that
encourages the cross-attention maps to agree with the linguistic binding reflected
by the syntax. Specifically, we encourage large overlap between attention maps
of entities and their modifiers, and small overlap with other entities and modifier
words. The loss is optimized during inference, without retraining or fine-tuning the
model. Human evaluation on three datasets, including one new and challenging set,
demonstrate significant improvements of SynGen compared with current state of
the art methods. This work highlights how making use of sentence structure during
inference can efficiently and substantially improve the faithfulness of text-to-image
generation.1
1
Introduction
Diffusion models for text-conditioned image generation produce impressive realistic images [1, 2, 3,
4]. Users control the generated content through natural-language text prompts that can be rich and
complex. Unfortunately, in many cases the generated images are not faithful to the text prompt [5, 6].
Specifically, one very common failure mode results from improper binding, where modifier words
fail to influence the visual attributes of the entity-nouns to which they are grammatically related.
1We make our code publicly available https://github.com/RoyiRa/Syntax-Guided-Generation
Preprint. Under review.
arXiv:2306.08877v1  [cs.CL]  15 Jun 2023

“a pink sunflower and a 
yellow flamingo”
Stable 
Diffusion
SynGen
(ours)
“a checkered bowl in a 
cluttered room”
“a horned lion and a 
spotted monkey”
(a) Semantic Leak in Prompt
(b) Semantic Leak out of Prompt
(c) Attribute Neglect
Figure 1: Visual bindings of objects and their attributes may fail to match the linguistic bindings
between entities and their modifiers. Our approach, SynGen, corrects these errors by matching the
cross-attention maps of entities and their modifiers.
As an illustration, consider the prompt “a pink sunflower and a yellow flamingo”. Given this prompt,
current models often confuse the modifiers of the two entity-nouns, and generate an image of a
yellow sunflower and a pink flamingo (Fig. 1, bottom left, semantic leak in prompt). In other cases,
the attribute may semantically leak to areas in the image that are not even mentioned in the prompt
(Fig. 1, bottom center, semantic leak outside prompt) or the attribute may be completely neglected
and missed from the generated image (Fig. 1, bottom right, attribute neglect). Such mismatch can be
addressed by providing non-textual control like visual examples [7, 8], but the problem of correctly
controlling generated images using text remains open.
A possible reason for these failures is that diffusion models use text encoders like CLIP [9], which
are known to fail to encode linguistic structure [10]. This makes the diffusion process “blind" to
the linguistic bindings, and as a result, generate objects that do not match their described attributes.
Building on this intuition, we propose to make the generation process aware of the linguistic structure
of the prompt. Specifically, we suggest to intervene with the generation process by steering the
cross-attention maps of the diffusion model. These cross-attention map serve as a link between
prompt terms and the set of image pixels that correspond to these terms. Our linguistics-based
approach therefore aims to generate an image where the visual bindings between objects and their
visual attributes adhere to the syntactic bindings between entity-nouns and their modifiers in the
prompt.
Several previous work devised solutions to improve the relations between prompt terms and visual
components, with relative success [11, 12, 13], but did not focus on the problem of modifier-entity
binding. Our approach specifically addresses this issue, by constructing a novel loss function that
quantifies the distance between the attention patterns of grammatically-related (modifier, entity-
noun) pairs, and the distance between pairs of unrelated words in the prompt. We then optimize
the latent denoised image in the direction that separates the attention map of a given modifier from
unrelated tokens and bring it closer to the noun to which it is grammatically related. We show that by
intervening in the latent code, we can markedly improve the pairing between attributes and objects in
the generated image while at the same time not compromising the quality of the generated image.
We evaluate our method on three datasets. (1) For a natural language setting, we use the natural
compositional prompts in the ABC-6K benchmark [13]; (2) To provide direct comparison with
previous state-of-the-art [11], we replicated prompts from their setting; (3) Finally, to evaluate
binding in a challenging setting, we design a set of prompts that includes a variety of modifiers and
entity-nouns. On all datasets, we find that SynGen shows significant improvement in performance
based on human evaluation, sometimes doubling the accuracy. Overall, our work highlights the
effectiveness of incorporating linguistic information into text-conditioned image generation models
and demonstrates a promising direction for future research in this area.
2

Language-driven cross attention losses 
Positive
Loss
 
 
U-Net
U-Net
… 
a red
crown
…
red
crown
amod
golden
strawberry
amod
… 
strawberry
a
golden
Extract cross attention maps 
(a)      Entity-Modiﬁer Identiﬁcation 
A
SynGen Step 
and
a
red crown  and  a golden 
strawberry
amod
amod
det
conj
cc
⇒
det
Negative
Loss
red
crown
(b)      Diﬀusion Process 
Figure 2: The SynGen workflow and architecture. (a) The text prompt is analyzed to extract entity-
nouns and their modifiers. (b) SynGen adds intermediates steps to the diffusion denoising process.
In that step, we update the latent representation to minimize a loss over the cross attention maps of
entity-nouns and their modifiers (Eq 3).
The main contributions of this paper are as follows: (1) A novel method to enrich the diffusion process
with syntactic information, using inference-time optimization with a loss over cross-attention maps;
(2) A new challenge set of prompts containing a rich number and types of modifiers and entities.
2
Syntax-Guided Generation
Our approach, which we call SynGen, builds on two key ideas. First, it is easy to analyze the syntactic
structure of natural language prompts to identify bindings of entity-nouns and their modifiers. Second,
one can steer the generation of images to adhere to these bindings by designing an appropriate loss
over the cross-attention maps of the diffusion model. We describe the two steps of our approach:
extracting syntactic bindings and then using them to control generation.
2.1
Identifying entity-nouns and their modifiers
To identify entity-nouns and their corresponding modifiers, we traverse the syntactic dependency
graph, which defines the syntactic relation between words in the sentence. Concretely, we parse the
prompt using spaCy’s transformer-based dependency parser [14] and identify all entity-nouns (either
proper-nouns or common-nouns) that are not serving as direct modifiers of other nouns.
These are the nouns that correspond to objects in the generated image (such as cats and dogs). We then
recursively collect all modifiers2 of the noun into its modifier set. The set of modifier-labels includes
a range of syntactic relations between nouns and their modifiers, such adjectivial modification (amod;
“the regal dog”), compounds (compound; “the treasure map”), nominal modification through an
intervening marker, adverbial modifiers (npadvmod; “A watermelon-styled chair”), and coordination
between modifiers (conj; “A black and white dog”).
2.2
Controlling generation with language-driven cross-attention losses
Consider a pair of a noun and its modifier. We expect the cross-attention map of the modifier to
largely overlap with the cross-attention map of the noun, while remaining largely disjoint with the
maps corresponding to other nouns and modifiers. To encourage the denoising process to obey these
spatial relations between the attention maps, we design a loss that operates on all cross-attention
maps. We then use this loss with a pretrained diffusion model during inference. Specifically, we
2We consider modifiers from the set {amod, nmod, compound, npadvmod, conj}. We exclude the conj
when determining the top-level nouns.
3

red
crown
golden
strawberry
Cross attention maps
Generation
Figure 3: Evolution of cross-attention
maps and latent representation along
denoising steps, for the prompt “a
red crown and a golden strawberry”.
At first, the attention maps of all
modifiers and entity-nouns are inter-
twined, regardless of the expected
binding. During denoising, attention
maps gradually becomes separated,
adhering the syntactic bindings. The
vertical line indicates that after 25
steps intervention stops, but the atten-
tion maps remain separated.
optimize the noised latents by taking a gradient step to reduce that loss. See illustration in Fig. 2.
Fig. 3 illustrates the effect of the loss over the cross-attention maps.
Loss functions:
Consider a text prompt with N tokens, for which our analysis extracted k noun-
modifier sets {S1, S2, . . . , Sk}. Let P(Si) represent all pairs (m, n) of tokens between the noun root
n and its modifier descendants m in the i-th set Si. For illustration, the set of “A black striped dog”
contains two pairs (“black”, “dog”) and (“striped”, “dog”). Next, denote by {A1, A2, . . . , AN} the
attention maps of all N tokens in the prompt, and denote by dist(Am, An) a measure of distance
(lack of overlap) between attention maps Am and An.
Our first loss aims to minimize that distance (maximize the overlap) over all pairs of modifiers and
their corresponding entity-nouns (m, n),
Lpos(A, S) =
k
X
i=1
X
(m,n)∈P (Si)
dist(Am, An).
(1)
We also construct a loss that compares pairs of modifiers and entity-nouns with the remaining words
in the prompt, which are grammatically unrelated to these pairs. In other words, this loss is defined
between words within the (modifiers, entity-nouns) set and words outside of it. Formally, let U(Si)
represent the set of unmatched words obtained by excluding the words in Si from the full set of
words and Au is the corresponding attention map for a given unrelated word u. The following loss
encourages moving apart grammatically-unrealted pairs of words:
Lneg = −
k
X
i=1
1
|U(Si)|
X
(m,n)∈P (Si)
X
u∈U(Si)
1
2

dist(Am, Au) + dist(Au, An)

.
(2)
Our final loss combines the two loss terms
L = Lpos + Lneg.
(3)
For a measure of distance between attention maps we use a Symmetric Kullback-Leibler divergence
dist(Ai, Aj) = 1
2 DKL(Ai||Aj) + 1
2 DKL(Aj||Ai), where Ai, Aj are attention maps normalized to
a sum of 1, i and j are generic indices, and DKL(Ai||Aj) = P
pixels Ai log(Ai/Aj).3
3It is reminiscent to the approach of [11], who also define a loss over the cross-attention maps to update the
latents. However, while their loss aims to maximize the presence of the smallest attention map at a given timestep,
to guarantee a set of selected tokens is included in the generated image, we define a loss that depends on pairwise
relations of linguistically-related words, and aims to align the diffusion process to the linguistic-structure of the
prompt.
4

2.3
The workflow
We use the above loss to intervene in the first 25 out of 50 diffusion steps. Empirically, a lower
number of steps fails to address the issue of improper binding, while a larger number of steps leads
to the generation of blurred images, as detailed in Appendix B. In each of these first 25 steps, a
pretrained denoiser (U-Net) was first used to denoise the latent variable zt. Then, we obtained the
cross-attention maps as in [15]. Next, we used the loss L to update the latent representation zt with a
gradient step z′t = zt −α·∇ztL. Finally, the U-Net architecture denoises the updated latent variable
z′
t for the next timestep.
3
Experiments
3.1
Compared baseline methods
We compare SynGen with three baseline methods. (1) Stable Diffusion 1.4 (SD) [1]; (2) Structured
Diffusion (StructDiff) [13], extracts noun-phrases from the prompt and embeds them separately,
to improve the mapping of the semantics in the cross-attention maps; and (3) Attend-and-Excite
(A&E) [11], a method that given a predetermined set of tokens, updates the latent a certain number of
timesteps, to eventually incorporate these tokens in the generated image. To automate token selection
in Attend-and-Excite, we follow the recommendation of the authors to select the nouns using a
part-of-speech tagger.
3.2
Datasets
We evaluate our approach using two existing benchmark datasets, and one new dataset that we
designed to challenge methods in this area.
(1) ABC-6K [13].
This benchmark consists of 3.2K natural compositional prompts from MSCOCO
[16], which were manually written by humans, using natural language and contain at least two color
words modifying different noun-entities. In addition, the dataset contains 3.2K counterparts, where
the position of modifiers in the original prompts are swapped. (e.g., “a white bench in front of a green
bush” and “a green bench in front of a white bush”). We randomly sample 600 prompts.
(2) Data from Attend-and-Excite [11].
This dataset was initially constructed to evaluate the
Attend-and-Excite method, which was successful compared to previous methods in tackling the
improper binding problem. The prompts in the dataset can be summarized to three categories: (1) “a
{color} {in-animate object} and a {color} {in-animate object}”; (2) “a {color} {in-animate object}
and an {animal}”; (3) “an {animal} and an {animal}”. Following the split in Attend-and-Excite, we
sample 33 prompts from type (1) and 144 prompts from type (2), but exclude type (3), as it does not
contain modifiers. This is a very simple dataset, which we use to facilitate direct comparison with
previous work.
(3) Diverse Visual Modifier Prompts (DVMP).
The datasets proposed by [11, 13] are basic in
the number and types of modifiers, as well as the number of entity-nouns per prompt. To challenge
our model, we design a dataset consisting of coordination sentences, in similar fashion to the dataset
from Attend-and-Excite, but with strong emphasis on the number and types of modifiers per prompt.
Specifically, we aim to compare the models with prompts that contain numerous and uncommon
modifiers, creating sentences that would not usually be found in natural language or training data,
such as “a pink spotted panda”. Key aspects of the DVMP dataset include:
Expanded modifiers: We have extended the number of modifiers referring to an entity-noun from
one to up to three. For instance, “a blue furry spotted bird”. We also added types of modifiers besides
colors, including material patterns (“a metal chair”), design patterns (“a checkered shoe”), and even
nouns modifying other noun-entities (“a baby zebra”).
Visually verifiable and semantically coherent: The modifiers selected for DVMP are visually
verifiable, with a deliberate avoidance of nuanced modifiers. For instance, “big” is a relative modifier
dependent on its spatial context, and emotional states, such as in the prompt “an excited dog”, are
largely excluded due to their subjective visual interpretation. Simultaneously, DVMP maintains
5

Concept Visual
Separation Appeal
Dataset
Model
ABC-6K [13]
SynGen (ours)
28.0% 18.3%
A&E [11]
11.1% 10.0%
Structure Diffusion [13]
5.8%
6.3%
Stable Diffusion [1]
4.8%
7.8%
No winner
50.1% 57.5%
Attend & Excite
dataset [11]
SynGen (ours)
38.4% 37.8%
A&E [11]
18.0% 18.64%
Structure Diffusion [13]
4.5%
4.5%
Stable Diffusion [1]
1.69%
2.3%
No winner
37.3% 36.7%
DVMP
(challenge set)
SynGen (ours)
24.8% 16.0%
A&E [11]
13.3% 12.1%
Structure Diffusion[13]
4.3%
7.8%
Stable Diffusion [1]
3.8%
7.1%
No winner
53.6% 56.8%
Table 1: Human eval-
uation of all methods
on the three datasets.
The
table
reports
scores for concept
separation (how well
the image matches
the
prompt)
and
visual appeal. Values
are the fraction of
majority vote of three
raters, normalized to
sum to 100.
semantic coherence by appropriately matching modifiers to noun-entities, thereby preventing the
creation of nonsensical prompts like “a sliced bowl” or “a curved zebra”.
In total, we have generated 600 prompts through random sampling. For a more comprehensive
description of the dataset’s creation, see Appendix F.
3.3
Human Evaluation
We evaluate image quality using the Amazon Mechanical Turk platform. Raters were provided with
a multiple-choice task, consisting of a single text prompt and four images, each generated by our
baselines and SynGen. Raters could also indicate that there is no clear winner, by selecting “equally
good” or “equally bad”. We provided each prompt to three raters, and report the majority decision.
When no majority decision was reached, we treat that prompt as a single "no winner" response.
We evaluate generated images in two main aspects: (1) concept separation (sometimes known as
editability [17]) and (2) visual appeal. Concept separation refers to the ability of the model to
distinctly represent different concepts or objects in the generated image. The effectiveness of concept
separation is assessed by asking raters, “Which image best matches the given description?”. To asses
visual quality, raters were asked “Which image is more visually appealing?”. To maintain fairness
and reduce biases, the order of images was randomized in each task. Full rater instructions and further
details are provided in Appendix G.2 of the supplemental materials.
4
Results
4.1
Quantitative Results
Table 1, provides results of the human evaluation experiment. SynGen is consistently ranked first
in all three datasets, and by a large margin, sometimes double the approval rate of the second
ranked method, A&E. These results are observed for concept separation, which measures directly the
semantic leak, and for visual appeal.
The high number of “no winner” cases reflects the large difficulty of some of the prompts, for which
no method provides good enough generated images. Population results, before majority aggregation,
are given in Appendix G.2 of the supplemental material. Comparisons with StableDiffusion are given
in Fig. 18 of the supplemental.
6

“a brown brush glides through 
beautiful blue hair”
“a blue and white dog sleeps in 
front of a black door”
“a white fire hydrant sitting in a 
field next to a red building”
Structure
Diffusion
Attend-and
Excite
SynGen
(ours)
Figure 4: Qualitative examples for ABC-6K prompts. For every prompt, the same three seeds are
used for all methods.
“a wooden crown and a furry 
baby rabbit”
Structure
Diffusion
Attend-and
Excite
SynGen
(ours)
“a red chair and a purple 
camera and a baby lion”
“a spiky bowl and a green cat”
Figure 5: Qualitative comparison for prompts from the DVMP dataset. For every prompt, the same
three seeds are used for all methods.
4.2
Qualitative Analysis
Figures 4–6 provide qualitative examples from the three datasets, comparing SynGen with the two
strongest baselines.
The qualitative examples illustrate several failure modes of our baselines. First, semantic leak in
prompt, occurs when a modifier of an entity-noun “leaks” onto a different entity-noun in the prompt,
as shown in Fig. 6, for the prompt “a pink clock and a brown chair”, in columns 3 and 4. In this
case, all baselines incorrectly apply pink hues to the chair, despite the prompt explicitly defining it
as brown. A more nuanced variant of this issue is semantic leak out of prompt, when a modifier is
assigned to an entity-noun that is not mentioned in the prompt. For instance, the “spiky” attribute in
“a spiky bowl and a green cat” leaks to a plant, which is not referred to in the prompt, or the green
coloration in the background of the images generated by the baselines, as seen in Fig. 5, columns 5
and 6.
7

“a monkey and a black bow”
“a pink clock and a brown chair”
“a frog and a brown apple”
Structured
Diffusion
Attend-and
Excite
SynGen
(ours)
Figure 6: Qualitative comparison for prompts from the Attend-and-Excite dataset. For every prompt,
the same three seeds are used for all methods.
Attribute neglect occurs when a modifier from the prompt is absent from the generated image. As
exhibited in Fig. 6, for “a frog and a brown apple”, both baselines do not include a brown color at all.
All methods, barring SynGen, grapple with entity entanglement [18, 19, 20], where some objects
tend to strongly associate with their most common attribute (e.g., tomatoes are typically red). This is
evident in columns 3 and 4 of Fig. 4, where other methods fail to correctly visually associate the blue
attribute with the dog in “a blue and white dog sleeps in front of a black door”. Instead, they resort to
typical attributes of the objects, generating a black and white dog.
Entity casting is another failure type where a modifier is treated as a standalone entity, a phenomenon
commonly observed with noun modifiers. For example, the prompt “a wooden crown and a furry baby
rabbit”, (column 1 in Fig. 5) has all methods, apart from ours, generate human infants. Presumably,
this occurs because “baby” is interpreted as a noun rather than as a modifier, leading other methods
to treat it as a separate object due to the lack of syntactic context. Conversely, SynGen correctly
interprets “baby” as a modifier and accurately binds it to the rabbit. Similarly, in the prompt “white
fire hydrant sitting in a field next to a red building”, “fire” is wrongly interpreted as an entity-noun,
which leads to the unwarranted inclusion of a fire in the scene.
4.3
Ablation study
The importance of both positive and negative losses.
We evaluated the relative importance of the
two terms in our loss Eq. (3). The positive term Lpos, which encourages alignment of the attention
map of an object and its modifier, and the negative loss term, Lneg, which discourages alignment
with other modifiers and objects. We sampled 100 prompts from the DVMP dataset and generated
images with and without each of the two loss terms. See example in Fig. 7. Then, a human rater was
asked to select the best option out of four variants. Table 2 shows that raters preferred the variant that
combined both the positive and the negative terms. More examples are given in the supplemental
Appendix B.
5
Related Work
Semantic leakage. [2] pointed out cases of semantic leakage in diffusion models, where properties
of one entity-noun influence the depiction of another. [21] attributed this to a lack of understanding
of syntax, specifically noting failures when processing texts requiring subtle syntactic binding
comprehension. [6] also identified semantic leakage issues in DALL-E, where the properties of one
entity-noun influence the way by which other entity nouns are depicted. In this work, we pinpoint the
leakage as a consequence of improper syntactic and visual binding mapping.
8

Loss
Concept
Separation
Visual
Appeal
Both losses Lpos+Lneg
27%
22%
Positive only Lpos
0%
11%
Negative only Lneg
3%
35%
Stable Diffusion
4%
28%
No winner
66%
4%
Table 2: Ablation of loss components. Hu-
man evaluation
Both
Losses
Only Positive
Loss 
Only Negative
Loss
No
Intervention
"A bird and a white crown"
Figure 7: Ablation of loss components. Removing
Lneg results in semantic leakage (the bird is white) and
entity neglect (there is no crown). Removing Lpos also
leads to semantic leakage (generating a bird and back-
ground with white parts), as well as failed attribution
binding (generating a crown that is not white).
Attention-based interventions. [15] demonstrated that the cross-attention mechanism determines
the spatial layout of entities in generated images. This result suggested that cross-attention is causally
involved in the aforementioned issues. The Attend-and-Excite method [11] addresses the problem of
entity omission, where certain entities mentioned in the prompt do not appear in the generated image.
They propose a loss function that encourages each noun token in the image to significantly attend to a
corresponding image patch, thereby preventing its omission. Our approach is similar to theirs in that
it intervenes in the latent generation process through attention guidance.
Syntax-based generation was also explored in the Structured Diffusion method [13]. It aims to
verify that the generated image represents all entities mentioned in the prompt and prevents semantic
leakage of attributes. This is achieved by parsing the prompt, extracting phrases corresponding to
nouns and modifiers, and encoding them separately. They also intervene in the attention patterns,
ensuring that each individual phrase influences the attention patterns. We demonstrate that it is
sufficient, and empirically better, to implicitly influence the attention patterns through our loss which
we dynamically optimize. In contrast, their intervention remains fixed.
Concurrent to this work, [22] proposed an alternative approach that combines syntactic control and
attention-based optimization. They extract entity nouns from prompts and train a layout predictor
to identify the corresponding pixels for each noun. Subsequently, they optimize the latents by
encouraging the pixels corresponding to the objects to attend to CLIP representations of simple
phrases containing those objects. While similar in spirit, this paper demonstrates intervention in the
generation process solely based on syntax, without explicitly learning the correspondence between
image entities and tokens.
6
Limitations
Like previous methods, the performance of SynGen degrades with the number of attributes to be
depicted (see supplemental Fig. 12). However, its decline is remarkably less pronounced. This decay
in performance, can be attributed to two primary factors: (1) an image begins to lose its visual appeal
when the negative loss term becomes excessively large; (2) an overly cluttered image poses challenges
in crafting a cohesive “narrative” for all the concepts.
Moreover, the effectiveness of our method is intrinsically tied to the efficiency of the parser. When
the parser falls short in extracting the stipulated syntactic relations, our method essentially operates
akin to SD.
7
Conclusions
In this work, we target the improper binding problem, a common failure mode of text-conditioned
diffusion models, where objects and their attributes incorrectly correspond to the entity-nouns and
their modifiers in the prompt. To address it, we propose SynGen, an inference-time intervention
method, with a loss function that encourages syntax-related modifiers and entity-nouns to have
overlapping cross-attention maps, and discourages an overlap from cross-attention maps of other
words in the prompt. We challenge our method with three datasets, including DVMP – a new
9

dataset that is specially-designed to draw out hard cases of improper-binding problem. Our method
demonstrates improvement of over 100% across all three datasets over the previous state-of-the-art.
Finally, our work highlights the importance of linguistic structure during denoising for attaining
faithful text-to-image generation, suggesting promising avenues for future research.
Acknowledgements
We would like to thank Avi Caciularu and Asaf Yehudai for their valuable feedback. This study
was funded by a grant to GC from the Israel Science Foundation (ISF 737/2018) and an equipment
grant to GC and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18). This project
has also received funding from the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme, grant agreement No. 802774 (iEXTRACT).
Shauli Ravfogel is grateful to be supported by the Bloomberg Data Science PhD Fellowship.
10

References
[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 10684–10695, 2022.
[2] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
[3] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho,
David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language
understanding, 2022.
[4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,
Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble
of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.
[5] Colin Conwell and Tomer Ullman. Testing relational understanding in text-guided image generation. arXiv
preprint arXiv:2208.00005, 2022.
[6] Royi Rassin, Shauli Ravfogel, and Yoav Goldberg. DALLE-2 is seeing double: Flaws in word-to-concept
mapping in Text2Image models. In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and
Interpreting Neural Networks for NLP, pages 335–345, Abu Dhabi, United Arab Emirates (Hybrid),
December 2022. Association for Computational Linguistics.
[7] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023.
[8] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image
personalization. In ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH ’23, 2023.
[9] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision, 2021.
[10] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why
vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International
Conference on Learning Representations, 2023.
[11] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-
based semantic guidance for text-to-image diffusion models. arXiv preprint arXiv:2301.13826, 2023.
[12] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual
generation with composable diffusion models. In Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XVII, pages 423–439. Springer, 2022.
[13] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu,
Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional
text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022.
[14] Matthew Honnibal and Ines Montani. spaCy 2: Natural language understanding with Bloom embeddings,
convolutional neural networks and incremental parsing. To appear, 2017.
[15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-
prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.
[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro
Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in
context, 2015.
[17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel
Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion,
2022.
[18] Yuval Atzmon, Felix Kreuk, Uri Shalit, and Gal Chechik. A causal view of compositional zero-shot
recognition. Advances in Neural Information Processing Systems, 33:1462–1473, 2020.
[19] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B.
Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning.
CoRR, abs/1612.06890, 2016.
[20] Ishan Misra, Abhinav Gupta, and Martial Hebert. From red wine to red tomato: Composition with context.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1160–1169, 2017.
[21] Evelina Leivada, Elliot Murphy, and Gary Marcus. Dall-e 2 fails to reliably capture common syntactic
processes. arXiv preprint arXiv:2210.12889, 2022.
[22] Qiucheng Wu, Yujian Liu, Handong Zhao, Trung Bui, Zhe Lin, Yang Zhang, and Shiyu Chang. Harnessing
the spatial-temporal attention of diffusion models for high-fidelity text-to-image synthesis. arXiv preprint
arXiv:2304.03869, 2023.
11

Supplementary Material
A
Implementation Details
Computing resources.
Experiments were run on an NVIDIA DGX Station with four v100-SXM2-
32GB GPUs. The overall duration of all experiments in the paper was about two weeks.
Hyperparameters.
The hyperparameters we used consist of 50 diffusion steps, a guidance scale of
7.5, a scale-factor of 20, and 25 latent update steps. The choices of scale factor and latent update
steps are described in Appendix B.2 and Appendix B.3 respectively.
Parser.
Throughout
this
project,
we
use
the
spacy
parser
with
the
out-of-the-box
en_core_web_trf model.
Attending word pieces.
When a relatively uncommon word is encountered by the tokenizer of the
text encoder, it is split to sub-words (i.e., word pieces). In the context of our loss function, when an
entity (or modifier) is split into word pieces, we compute our distance function (the Symmetric-KL)
for each word piece. Then, only the word piece that maximizes the distance is added to the loss.
Cross-attention maps.
We describe more formally the cross-attention maps on which we intervene.
Let N be the number of tokens in the prompt, and let D2 be the dimensionality of the latent
feature map in an intermediate denoising step. The denoising network defines a cross-attention
map Apatches→tokens ∈RD2×N between each of D2 patches in the latent feature map and each token.
Intuitively, the attention maps designates which tokens are relevant for generating each patch.
Our goal it to derive an attention distribution Atokens→patches ∈RN×D2 such that its i-th row
Atokens→patches
i
contains the attention distribution of token i over patches. For this goal, we de-
fine a score matrix S to be the transpose of Apatches→tokens, i.e,. a matrix whose ith row contains
the attention scores from each patch to token i. Since S is not normalized, we divide each row
by its sum to get a distribution over patches. Unless stated otherwise, across the paper, we refer
to Atokens→patches ∈RN×D2 when mentioning the “cross-attention maps” A and its ith row Ai
corresponding to the attention map from the ith token.
B
Additional Ablation Experiments
B.1
Further Investigation of the Positive and Negative Loss Terms
In Section 4.3, we investigate the importance of the positive and negative loss function terms using
a human rater. Here, we accompany the rating with a qualitative analysis, to examine the effect of
each term. To this end, we generate images for 15 randomly selected prompts, five from each dataset.
Fig. 8 depicts a sample of the generated prompts.
We find that proper binding necessitates both the positive and negative terms: excluding the negative
term from the loss function results in two noteworthy observations. First, the number of missing
objects increase, evident by the missing crown, cat, metal chair, and tomato, in columns 1, 2, 4, and 5
in Fig. 8. One consequence of missing objects is the apparent improper binding, indicated by the red
backpack and black shirt in columns 1 and 3.
On the other hand, excluding the positive term results in fuzzier separation between objects. For
instance, the cat is not completely formed, and is “merged” with the pillow; and while it appears that
there is some green residue on the dog, it is not colored green. Moreover, the grass is green, which
indicates a semantic leakage.
Putting these insights together, we observe that to some extent, the effect the loss terms is comple-
mentary. In addition to the increase of objects and proper binding, the images are more coherent (less
cases of objects mixed into each other, such as the cat in the only-negative loss or the elephant in the
only-positive loss).
12

“a beige spotted 
lion and a metal 
chair”
Both Losses
Only Positive
Loss
Only Negative
Loss
“a black and white 
cat is laying on a 
green pillow on top 
of a desk”
“an orange crown 
and a red 
backpack”
“a sliced tomato 
and a spotted 
elephant”
“a man reaches down 
to a black and green 
dog as they stand on 
white grass”
No
Intervention
Figure 8: We examine the effect of employing only one of the two losses instead of both. All images
were generated using the same random seed.
B.2
Number of Timesteps for Intervention
Recall that our method intervenes in latent denoising generation. In this appendix, we study the effect
of the hyperparameters determining the number of steps in which we intervene.
To identify an ideal number of timesteps to intervene, we experiment with 100 randomly selected
prompts from the DVMP dataset, a fixed random seed, and a number of update steps from 5 to 50, in
increments of 5. Examples of this experiment are shown in Fig. 9.
We observe that when intervening in a small number of timesteps, our method failed to adequately
mitigate semantic leakage or that images are not completely formed. For instance, the apple in
column 1 in the 15-steps example is cartoon-ish, while the dog is not. Conversely, intervening for
the full 50 timesteps resulted in an increase rate of blurred images (potentially due to the significant
modification of the latent, which shifts it away from the learned distribution). We conclude that the
optimal number of timesteps for intervention is 25, as this allows for effective mitigation of improper
binding, while still generating visually appealing images.
B.3
Setting the Scale Factor
The scale factor affects the update step size. Recall the update step stated in Section 2 z′t =
zt −α · ∇ztL. Here, α is the scale-factor.
To determine a good selection for the scale-factor, we generate 100 randomly sampled prompts from
the DVMP dataset, with a scale-factor value from 1 to 40, in increments of 10.
As can be seen in Fig. 10, we observe that merely updating the latent using a scale-factor of 1 yields
relatively good results in terms of improper binding, which confirms the utility of our loss function.
However, such a low scale-factor also consistently leads to missing objects.
Interestingly, for greater scale-factor values, the generations become alike in their overall look, but
are nonetheless very different. As an example, for both values, 10 and 30, the sliced banana is missing
from the image in column 2, but the 30-value does result in a spotted teal skateboard. In column 3,
values below 20 lead to images that contain two pandas (none of which are spotted), which indicates
the proper binding process, and that the latent was not updated enough. On the other hand, a value
greater than 20 leads to an image of a striped rabbit, instead of a spotted rabbit.
13

Every step
“a furry 
spotted dog 
and a black 
apple”
25 steps 
(SynGen)
15 
steps
"a sliced banana 
and a teal spotted 
skateboard and an 
orange elephant"
"a spotted 
baby rabbit 
and a baby 
panda"
No
intervention
"a spotted 
sleepy horse 
and a sliced 
apple"
"a red 
turtle and a 
pink apple"
Figure 9: We experiment with varying number of diffusion steps and examine the effect of changing
the number of diffusion steps for which we intervene with the cross attention maps. All images were
generated using the same random seed.
One interesting conclusion from this experiment is that the greater the scale-factor, the stronger the
concept separation. However, this is only true to a point. For a great enough value, generations
become too blurred or simply lose their visual appeal.
C
Additional Quantitative Analyses
To study the efficacy of SynGen relative to the baselines in improper binding setting, we analyze the
results under three perspectives. (1) as a function of repeating entities and modifiers; (2) as a function
of the number of modifiers; and (3) degree of entanglement. Samples of generations are shown in
Fig. 14.
Number of repeating modifiers and entities.
In this analysis, we examine the performance of all
methods for prompts containing recurring modifiers (e.g., “a sliced strawberry and a sliced tomato) or
entities (e.g., “a sliced tomato and a skewered tomato”). Aggregated results are illustrated in Fig. 11.
Our observations reveal a decrease in performance across all methods when modifiers are repeated.
However, the relative success between SynGen and the baselines in performance remains the same.
Moreover, there is no substantial decline in results when entities are repeated.
Number of modifiers in prompt.
We hypothesize that since our method is specifically designed to
tackle improper binding, it handles prompts containing many modifiers with more success. This is
confirmed in Fig. 12, which shows the gap between SynGen and the baselines widens as the number
of modifiers in a prompt increases.
Entangled entities.
As we describe in Section 4.2, entangled entities are strongly associated with
their most common attribute. For instance, a tomato is typically red, and thus, it is common for
images to depict red tomatoes.
14

Scale 
Factor = 30
“a furry 
spotted dog 
and a black 
apple”
Scale 
Factor = 20 
Scale 
Factor = 10
"a sliced banana 
and a teal spotted 
skateboard and an 
orange elephant"
"a spotted 
baby rabbit 
and a baby 
panda"
Scale 
Factor = 1
"a spotted sleepy 
horse and a 
sliced apple"
"a red turtle and 
a pink apple"
Scale 
Factor = 40 
Figure 10: Qualitative comparison between scale factor values for SynGen. For every prompt, the
same seeds are applied. We anecdotally show our scale-factor value (we use the value 20) provides
superior results.
We categorize the prompts into three groups: (1) entangled prompts, which contain entangled
entities with a modifier that overrides a common modifier (e.g., a purple strawberry); (2) common
entangled prompts, which contain entangled entities with their common modifiers; and (3) neutral
prompts, which do not contain entangled entities at all. Performance as a function of these groups is
demonstrated in Fig. 13.
D
Additional Qualitative Results
D.1
Comparison to Spatial-Temporal Diffusion
As described in Section 5, concurrent to this work, [22] developed a method to optimize the latents.
While they primarily attend spatial and temporal relations, they too report on improper binding,
namely, attribute mismatch. Thus, we extend the tables from Section 4, to include Spatial-Temporal
Diffusion, see Fig. 15, Fig. 16, Fig. 17.
Based on these 18 images, we observe that Spatial-Temporal Diffusion consistently misses at least
one entity from the prompt. As an example, see Fig. 15. The images in columns 1 and 2 miss a crown
(but include “wooden” objects), and columns 3 and 4 miss a lion and exhibit semantic leakage.
In other cases, we note many cases of semantic leakage in and out of the prompt. For instance, in
Fig. 17, in column 2 the clock is brown and the wall is pink, and in column 3, the chair is pink.
15

(a)
(b)
Figure 11: The performance of SynGen and the baselines in concept separation on prompts containing
(a) repeating modifiers; and (b) repeating entities in the DVMP dataset.
Figure 12: Concept Separation as a function of number of modifiers in a prompt in the DVMP
dataset, introduced in Section 3.2. Only the top-competing method (Attend-and-Excite) is plotted for
readability.
“a wooden crown and a furry 
baby rabbit”
Spatial-Temporal 
Diffusion
SynGen
(ours)
“a red chair and a purple 
camera and a baby lion”
“a spiky bowl and a green cat”
Figure 15: Extended qualitative comparison for prompts from the DMVP dataset. SynGen and
Spatial-Temporal Diffusion [22].
16

Figure 13: The performance of SynGen and the baselines in concept separation when grouping the
prompts with respect to entangled modifiers in the DVMP dataset.
Structure
Diffusion
SynGen
(ours)
“a furry zebra 
and a spotted 
metal 
bicycle”
“a skewered 
apple and a 
spiky 
wooden 
chair”
“a pink 
cat and a 
gray 
wooden 
camera”
Attend-and
Excite
“a blue 
bear and a 
beige 
rabbit”
(c)
“a beige 
modern 
balloon and 
a purple 
strawberry”
(d)
“a black 
apple 
and a 
black 
sliced 
apple”
(b)
“a skewered 
strawberry 
and a yellow 
strawberry”
(a)
(e)
(f)
(g)
Figure 14: Samples from the analyses in Appendix C. (a) a case of recurring entity (strawberry); (b)
a recurring modifier (black) and entity (apple); (c) and (d) contain entangled entities (a blue bear and
a purple strawberry); (e), (f), (g) are examples of prompts with more than two modifiers.
D.2
Stable Diffusion and Structured Diffusion Comparison
A comparison between Stable Diffusion and Structured Diffusion is depicted in Fig. 18. The findings
from the study by [11] suggest that the generated images from Structured Diffusion are often similar
to those generated by Stable Diffusion, with limited improvements in addressing semantic flaws
and enhancing image quality. This is further supported by the comparable results presented in our
findings Table 1. Therefore, while we include all baselines in our evaluations, our qualitative analysis
only showcases images produced by the slightly superior Structured Diffusion.
17

“a brown brush glides through 
beautiful blue hair”
Spatial-Temporal 
Diffusion
SynGen
(ours)
“a blue and white dog sleeps in 
front of a black door”
“a white fire hydrant sitting in a 
field next to a red building”
Figure 16: Extended qualitative comparison for prompts from the ABC6K dataset. SynGen and
Spatial-Temporal Diffusion [22].
“a monkey and a black bow”
Spatial-Temporal 
Diffusion
SynGen
(ours)
“a pink clock and a brown 
chair”
“a frog and a brown apple”
Figure 17: Extended qualitative comparison for prompts from the Attend-and-Excite dataset. SynGen
and Spatial-Temporal Diffusion [22].
“a wooden 
crown and 
a furry 
baby 
rabbit”
Structure
Diffusion
Stable
Diffusion
“a red chair 
and a 
purple 
camera and 
a baby 
lion”
“a spiky 
bowl and 
a green 
cat”
“a brown 
brush 
glides 
through 
beautiful 
blue hair”
“a blue and 
white dog 
sleeps in front 
of a black 
door”
“a white fire 
hydrant 
sitting in a 
field next to 
a red 
building”
“a 
monkey 
and a 
black 
bow”
Figure 18: Side-by-side generations of StableDiffusion and StructureDiffusion.
E
SynGen Failures
We observe three recurring types of failure SynGen displays Fig. 19. First, when there are many
modifiers and entities in the prompt, despite the results in Fig. 12, we note that sometimes the
negative loss component becomes exceedingly large, and thus, pushes the latent out of the distribution
the decoder was trained on. Consequently, images become blurred, or contain concepts which
are successfully separated, but are incoherent. This is likely because our method over-fixates on
incorporating all elements described in the prompt.
18

Second, while SynGen typically successfully addresses the possible error cases described in Sec-
tion 4.2, at times it can neglect generating all objects, unify separate entities, or neglect generating
attributes. We conjecture that it is because the cross-attention maps of the modifier and its corre-
sponding entity do not overlap enough. We note it usually occurs when there are many modifiers that
refer to the same entity.
Finally, as common with many diffusion models, we report a recurring issue with faithfulness to the
number of units specified in the prompt, for a certain entity. For instance, upon receiving prompts
containing “a strawberry”, SynGen generates images with multiple strawberries, instead of just one.
One explanation to this problem is that the representation of a certain entity begins “scattered”, and is
never quite formed into a single cluster. Interestingly, the opposite problem, where multiple units are
“merged” into one, occurs far less in the generations of SynGen. Possibly, because of the inherent
objective function of our loss, which “pushes away” foreign concepts from one another.
“a red spotted 
dog and a blue 
striped cat are 
playing with a 
yellow spiky 
ball”
“a brown 
skewered 
tomato and a 
baby furry 
zebra and a 
yellow modern 
skateboard”
“a pink baby 
zebra and a 
brown lion and 
a checkered 
wooden 
camera”
"a baby sleepy 
penguin and a 
furry gorilla 
and a sleepy 
lion"
“a sliced 
strawberry and 
a yellow furry 
zebra”
“a curved 
modern car and 
an orange baby 
sleepy elephant 
and a brown 
bench”
(b)
(a)
(c)
(d)
(e)
(f)
Figure 19: Frequent failure modes in SynGen. (a) depicts a case of blurred image, (b) incoherent
image which maintains concept separation. Both are a result of excessive updates to the latent,
resulting from a large negative loss term. In example (c), the zebra and lion are merged into a single
entity and (d) omits the sleepy lion. We conjecture (c) and (d) are a result of too little updates. (e)
and (f) exhibit the well-known issue of flawed mapping between the number of units an entity is
mentioned in the prompt to the generated image.
F
The Diverse Multiple Modifiers Prompts (DVMP) dataset
In Section 3.2 we describe DVMP, a new dataset containing rich and challenging combinations, for
the purpose of evaluating improper binding.
In total, DVMP has 18 types of objects, 16 types of animals, and 4 types of fruit. There are four
animal modifiers, 7 object modifiers, two fruit modifiers, and 13 colors. A comprehensive account of
the entities and their possible modifiers is shown in Table 3.
G
Extended Evaluation
G.1
Phrases-to-Image Similarity
A common approach to automatically assess text-based image generation is by computing the cosine
similarity between an image and prompt, using a vision-language model like CLIP [9]. However, the
very challenge we tackle here is rooted in CLIP’s failure in establishing correct mapping between
syntactic bindings and visual bindings, functioning like a bag-of-words model [10]. As an example,
suppose we give CLIP the prompt “a blue room with a yellow window”. If we present CLIP with an
image of a yellow room with a blue window, it may yield a similar score to an image that accurately
depicts a blue room with a yellow window.
In an attempt to address this flaw, we segment prompts to phrases containing entity-nouns and their
corresponding modifiers (e.g., “a blue room” and “a yellow window”), and compute the similarity
between these segmented phrases and the image. We then aggregate the result to a single score by
computing the mean. With this approach, we expect CLIP to properly associate the modifiers (e.g.,
“blue” and “yellow”) with the correct entity-noun (i.e., “room” and “window”) as there is only one
19

Table 3: List of entities and their modifiers in the DMVP dataset. Colors are not restricted to
categories.
Category
Entities
Modifiers
General
backpack, crown, suitcase, chair,
balloon, bow, car, bowl, bench, clock,
camera, umbrella, guitar, shoe, hat,
surfboard, skateboard, bicycle
modern, spotted, wooden, metal,
curved, spiky, checkered
Fruit
apple, tomato, banana, strawberry
sliced, skewered
Animals
cat, dog, bird, bear, lion, horse,
elephant, monkey, frog, turtle, rabbit,
mouse, panda, zebra, gorilla, penguin
furry, baby, spotted, sleepy
Color Modifiers
red, orange, yellow, green, blue, purple, pink, brown,
gray, black, white, beige, teal
entity-noun in each segment. Unfortunately, this metric achieves relatively low agreement with the
majority selection of human evaluation, only 43.5% of the time, where 25% is random selection.
Despite the low agreement, we note the overall trend of selections of this automatic metric is very
similar to the human majority selection. Table 4 shows the results of our automatic evaluation.
Table 4: Automatic evaluation of all methods on the three datasets. The table reports scores for
concept separation (how well the image matches the prompt) and visual appeal. Values are the
fraction of majority vote of three raters, normalized to sum to 100.
Method
DVMP (ours)
ABC-6K [13]
A&E dataset [11]
SynGen (ours)
47.33%
41.33%
44.63%
A&E [11]
27.66%
24.33%
27.11%
Structured Diff. [13]
12.84%
17.84%
11.87%
Stable Diff. [1]
12.17%
16.50%
16.39%
G.2
Additional Details on Human Evaluation Experiments
In the manual evaluation procedure detailed in Section 3.3 the evaluator is tasked with comparing
various image generations and selecting the optimal image based on multiple criteria. The guidelines
and examples given to the evaluators are presented in Fig. 20 and Fig. 21. Fig. 22 provides a
screenshot of the rating interface. The full results of the human evaluation are given in Table 5
Rater Compensation.
Raters were selected based on their performance history, requiring a mini-
mum of 5,000 approved HITs with an approval rate exceeding 98%. They were required to pass a
qualification exam with a perfect score before given access to the task. The hourly compensation was
$10, ensuring fair renumeration for their contributions.
20

Figure 20: The instructions that were given to the raters.
“a blue balloon and an orange bench”
Good match
Bad match
This is a good match because the image 
clearly shows a BLUE balloon and an 
ORANGE bench (even though we only see 
a part of the bench), which matches the 
description.
This is a bad match because the image 
shows a BLUE bench, which does not match 
the description.
This is a good match because the 
the image clearly shows an ORANGE 
car and a BLUE suitcase (an image 
that would show a clearer orange 
car or that would not have an orange 
fruit would be better)
"a blue suitcase and an orange car"
This is a bad match. While there's an 
orange object, it does not look more 
like a car than the ﬁrst option.
Figure 21: Examples given to raters in their instructions. Each example consists of a prompt and two
images: A good match (top) and a bad match (bottom) for the concept separation criterion. These
examples were accompanied by text explaining why the images are considered a good (or bad) match
to the prompt.
Table 5: The population vote of three raters was normalized to sum to 100 and the standard error
mean was added. The table reports the scores for concept separation (how well the image matches
the prompt) and visual appeal for different models on each dataset.
Concept Separation
Visual Appeal
Dataset
Model
DVMP (challenge set)
SynGen (ours)
29.22 ± 0.45
23.55 ± 0.42
A&E [11]
19.83 ± 0.39
19.00 ± 0.39
Structured Diffusion [13]
09.00 ± 0.28
15.55 ± 0.36
Stable Diffusion [1]
09.88 ± 0.29
15.55 ± 0.36
No winner
32.05 ± 0.46
26.33 ± 0.44
ABC-6K [13]
SynGen (ours)
33.00 ± 0.47
25.72 ± 0.43
A&E [11]
17.83 ± 0.38
17.27 ± 0.37
Structured Diffusion [13]
13.40 ± 0.34
14.50 ± 0.35
Stable Diffusion [1]
11.72 ± 0.32
14.50 ± 0.35
No winner
24.00 ± 0.42
28.00 ± 0.44
Dataset from
Attend-and-Excite [11]
SynGen (ours)
38.79 ± 0.48
40.11 ± 0.49
A&E [11]
22.59 ± 0.41
21.46 ± 0.41
Structured Diffusion [13]
09.98 ± 0.29
11.86 ± 0.32
Stable Diffusion [1]
08.85 ± 0.28
09.79 ± 0.29
No winner
19.77 ± 0.39
16.76 ± 0.37
21

Figure 22: A screenshot of the AMT task. ,The order of images was randomized per HIT. “equally
good” and “equally bad” were merged during post-processing into "no winner", to simplify presenta-
tion of results.
22

