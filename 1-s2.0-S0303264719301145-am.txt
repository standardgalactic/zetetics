Error-correcting codes and information in biology
G´erard Battail
´Ecole nationale sup´erieure
des T´el´ecommunications de Paris, retired
Abstract
Shannon’s channel coding theorem (1948), a major result of information theory, paradoxically
states that errorless communication is possible using an unreliable channel. Since then, engineers
developed many error-correcting codes and decoding algorithms. A performance close to the predicted
one was eventually achieved no earlier than the beginning of the nineties.
Many communication
facilities would not exist without error-correcting codes, e.g., mobile telephony and terrestrial digital
television.
This article explains ﬁrst how they work without mathematical formalism.
An error-
correcting code is a minority subset among some set of messages. Within this subset, the messages
are suﬃciently diﬀerent from each other to be exactly identiﬁed even if a number of their symbols,
up to a certain limit, are changed. Beyond this limit, another message can be erroneously identiﬁed.
An error-correcting code is interpreted as a set of messages subjected to constraints which make
their symbols mutually dependent.
Although mathematical constraints are conveniently used in
engineering, constraints of any other kind, possibly of natural origin, can generate error-correcting
codes.
Biologists implicitly assume that genomes were conserved during the geological ages, without re-
alizing that this is impossible without error-correcting means. Symbol errors occur during replication
of a genome; chemical reactions and radiations are other sources of errors. Their number increases
with time in the absence of correction. A genomic code will exactly regenerate the genome provided
its decoding is attempted after a short enough time interval. If the number of errors is too large,
however, the decoded genome will diﬀer from the initial one and a mutation will occur. Periodically
attempted decodings thus will conserve a genome except for very infrequent mutations if decoding
attempts are frequent enough. The better conservation of very ancient parts of genomes, like the
HOX genes, cannot be explained unless assuming that a genomic error-correcting code resulting from
a stepwise encoding exists: a ﬁrst encoding was followed later by a second one where a new in-
formation and the result of the ﬁrst encoding were jointly encoded, and this process was repeated
several times, eventually resulting in an overall code made of nested components where the older is
an information, the better it is protected. Organic codes in Barbieri’s meaning result from the same
process and have the same structure. Any new organic code induces new genomic constraints, hence
new components in a nested system of codes. Organic codes may thus be identiﬁed with the system
of nested error-correcting codes needed to conserve the genetic information.
A majority of biologists deny that information theory can be useful to them. It is shown on the
contrary that the living world cannot be understood if the scientiﬁc concept of information is ignored.
Heredity makes the present communicate with the past, and as a communication process is relevant
to information theory, which is thus a necessary basis of biology besides physics and chemistry.
The nested genomic error-correcting codes which are needed for conserving the genetic information
account for the hierarchical taxonomy which structures the living world. Moreover, the main features
of biological evolution, including its trend towards increasing complexity, ﬁnd an explanation within
this framework. Incorporating the scientiﬁc concept of information and the science based on it in the
foundations of biology can widely renew the discipline but meets epistemological diﬃculties which
must be overcome.
Keywords: Error-correcting codes, Errorless communication, Heredity, Information theory, Organic
codes
Introduction
This article intends to show that information is a fundamental entity that should be taken into account
in order to better understand the functioning of the living world, especially as regards heredity. It deals
ﬁrst with means for controlling errors, usually referred to as error-correcting codes, which paradoxically
1
© 2019. This manuscript version is made available under the Elsevier user license
https://www.elsevier.com/open-access/userlicense/1.0/
Version of Record: https://www.sciencedirect.com/science/article/pii/S0303264719301145
Manuscript_22cb5df833a841916505a86728f22864

enable reliable communication by means of unreliable channels. They have their origin in information
theory, a mathematical discipline initiated by Claude Shannon (1916–2001).
They made possible a
number of technical facilities now widely used, like mobile telephony or terrestrial digital television, which
would simply not exist without these codes. However, almost everybody ignores them: communication
technology has become invisible. This article gives strong support that natural error-correcting codes
exist in the living world and explain many of its features, including the permanence of its objects, its
taxonomy and its evolution.
1
Organic codes have error-controlling ability
Somebody trying to know about error-correcting codes as used in communication technology would be
exposed to a plentiful, highly mathematical literature, without any helpful guide.
His or her initial
willingness would soon be discouraged. Understanding these codes would it be so diﬃcult? I think that
it is possible to explain their operation by rather simple means. Their domain widely exceeds the technical
one since they have a ﬁrst-order role, although poorly recognized, in biology and in linguistics. They are
actually omnipresent in both nature and culture, so it is highly desirable that they are understood well
beyond the technical ﬁeld.
To begin with, this article tries to explain in a simple manner how error-correcting codes work,
ignoring the multiplicity of their families, the eventful story of their development, and the mathematical
structures they assume. It should ﬁrst be precisely stated what is intended by literal communication.
Literal communication
Literal communication consists of reproducing at a certain place a message which is available elsewhere.
This is the basic task of communication means, whether natural, cultural or technical. Telecommuni-
cation techniques widely increase the possible range of human literal communication. The message to
be reproduced is a sequence of symbols, each belonging to a predetermined ﬁnite set of signs called the
alphabet. These signs can be distinguished from each other without any ambiguity. The analysis of speech
shows that it can be interpreted as a sequence of elementary sound waveforms in ﬁnite number, referred to
as phonemes, the set of which constitues a phonetic alphabet speciﬁc to any language. A written alphabet
like the Latin one is intended to represent the oral language by means of an established (although often
rather loose) correspondence between its letters and the phonemes of the language. The communication
process is irreversible, in the sense that it is impossible to delete or change already transmitted symbols1.
The necessary agent of a communication is a sequence of signs which belong to the alphabet, referred
to as a message. For a spoken message, these signs are the phonemes of some language; for a written
one they are letters of the alphabet used by some human community. Events, objects, beings, and also
ideas, reasonings, feelings, myths . . .
can be evoked by a message. Because we perceive speech at a
very early age and because we learn how to read since childhood, we do not wonder about this almost
miraculous correspondence between combinations of a small number of signs and an unlimited number of
elements of the concrete and the mental world. This correspondence is realized by means of a language,
a complex system which comprises a number of sequences of signs from a set of phonemes (in its oral
form) or letters (in its written form): its words, the set of which constitutes the lexicon of this language.
A dictionary associates the words of the lexicon with concrete objects or abstract entities, or actions they
incur or relations between them. A grammar deﬁnes the rules which enable combining the words so as
to express the relations between the objects they represent. By the agency of the language, a message
establishes a communication between the speaker and the listener, or the writer and the reader, which
bears a meaning and is referred to as semantic.
An obvious prerequisite of semantic communication is that the message be available to its destination,
which implies that literal communication should come ﬁrst. For a document written on paper, this is the
modest function of the postman who brings a message (a “letter”). Communication of written messages
was also since the beginning of the XIXth century performed by the telegraph, and nowadays by the
electronic mail. The message is then borne by an electrical current or an electromagnetic wave. In any
case, it can be perceived only as inscribed into a physical medium.
In order to establish a mathematical theory of communication techniques, Shannon noticed in 1948
that solely literal communication is relevant to engineering, whereas semantic communication is com-
1Appending this important remark has been suggested by a referee, who is thanked here
2

pletely foreign to it [1]. When designing a communication system, the engineer has to make a message
available to its destination, whatever it is. Being a priori unknown, it must be dealt with as random.
Separating literal from semantic communication is the foundation of information theory, i.e., the science
of communication processes regardless of their physical means. Shannon enunciated this separation as
obvious but its misunderstanding is at the origin of diﬃculties met for teaching and popularizing infor-
mation theory, which demand that this separation be assimilated. This is all the more diﬃcult since the
word “information” which names it is used in a meaning quite foreign to the current usage, which always
implies semantics.
Before analysing literal communication and discussing the error-correcting codes which make it pos-
sible, it should be emphasized that this kind of communication is by no means an end in itself. Faithfully
communicating a message has no other usefulness than letting know or share a signiﬁcance, which de-
mands the agency of a language to endow the message with a meaning.
This semantic function of
communication is properly its raison d’ˆetre. Literal communication is just a preliminary but necessary
step for ensuring the availability of the message. Being the science of literal communication, information
theory is founded on the separation stated by Shannon. All the sequel will be devoted to literal commu-
nication. It implies that the successive symbols of the message to be communicated are identiﬁed. It is
an easy task as far as these symbols can be unambiguously distinguished, but perturbations can result in
wrong symbol identiﬁcations, hence in errors. In a printed message, a lack or an excess of ink can aﬀect
a letter identiﬁcation. In the case of speech, noise can hinder or prevent the identiﬁcation of one or more
phonemes. In daily life, although we generally do not notice it, speech is always received in the presence
of noise so the identiﬁcation of symbols is perturbed more or less intensely. If we can nevertheless easily
converse even in noisy environments, it is because our language contains very eﬃcient error-correcting
means, although we are not aware of their existence. The high complexity of language hinders using it
as an example for introducing how error-correcting codes work. Examining how engineers deal with the
problem will provide an easier introduction.
What are error-correcting codes?
An error-correcting code is a set of messages such that each of them can be unambiguously identiﬁed
even if a limited number of its symbols are aﬀected by errors. This deﬁnition of a code as a collection of
messages does not match that usually given, for instance by Barbieri for deﬁning the organic codes [2–5].
They are actually almost equivalent, as shown in the sequel, and the present one is more convenient for
introducing error-correcting codes.
Let us consider the set of n-symbol sequences (n will be said their length). The Hamming distance
between two sequences of this set is deﬁned as the number of symbols which need be changed in one of
them in order to obtain the other one. Given a set of objects of any kind, a distance between any two of
its elements a and b is mathematically deﬁned as a function d(a, b) such that:
• d(a, b) > 0 if a ̸= b, d(a, a) = 0;
• d(a, b) = d(b, a);
• d(a, c) ≤d(a, b) + d(b, c) for any element c of the set.
The given set of messages can then be interpreted as an abstract space and its elements as points of
this space. Their relative position is determined by their distance. The Hamming distance obviously
fullﬁls the three axioms above. No concept is more intuitive than that of distance, regardless of what the
elements of the set actually are. The word “distance” in the following will be understood in the Hamming
meaning, as well as the concepts of “nearness” and “neighbourhood”. Endowing a set of symbol sequences
with a distance measure enables a simple explanation of the operation of error-correcting codes.
The principle of an error-correcting code can be stated assuming that the binary alphabet, the simplest
one, is used. Its symbols are usually denoted by 0 and 1 but it should be kept in mind that these digits
do not represent numbers. They are signs which only need to be distinct and identiﬁable. They may be
replaced by any couple of signs satisfying this condition, e.g., a and b. A binary error-correcting code
of length n, dimension k and minimum distance d, say C(n, k, d), is a subset of 2k sequences among
the 2n symbol sequences of length n, such that a distance of at least d > 2 exists between any two of
3

them. These sequences are named the codewords2 and d is referred to as the “minimum distance” of the
code. The code is redundant, meaning that the length of its words, namely n symbols, exceeds k which
suﬃces to represent 2k words. The code redundancy entails that the codewords are sparse among the set
of n-symbol sequences. If they are distributed evenly within this set, each codeword can be far enough
from its closest neighbours so as to be uniquely distinguishable even if some of its symbols are in error,
provided the errors are not too many. Indeed, if a word belonging to C(n, k, d) is transmitted and the
corresponding received sequence is aﬀected by t erroneous symbols, it remains closer to the original one
provided t is smaller than d/2. The actually transmitted codeword can then be identiﬁed with certainty,
or decoded. The properties stated for the binary alphabet are easily extended to an alphabet of any size q.
Moreover, it will be shown that unintended error-correcting codes are widespread in many non-technical
instances, especially in biology and linguistics.
Let us for instance consider an n-symbol message written on a medium such that an error aﬀects each
of its symbols with a constant probability per time unit. Then the number t of errors in its symbols is
random and its average increases with time. In the absence of an error-correcting means, the message
would be degraded, perhaps slowly if the errors occur infrequently, but inexorably. If the message is a
word of an error-correcting code of minimum distance d, it can be recovered without any error provided
the number t of erroneous symbols remains less than d/2. This condition is almost surely fulﬁlled if
its decoding is attempted after a short enough time interval. Decoding3 can fail only if t exceeds d/2.
In case of decoding failure, the decoded word diﬀers in at least d symbols from the correct one. The
larger the minimum distance d, the less likely is a decoding failure, but the decoded word is the more
diﬀerent from the correct word when it occurs. Repeated decoding attempts separated by short enough
time intervals result in almost always correctly regenerating the codeword. This instance pertains to the
genomic message, as we shall see later.
The main property of an error-correcting code C(n, k, d) is its dilution within the set of n-symbol
sequences. The larger is this dilution, the more eﬃcient is the code. The minimum distance d between
its words (partially) measures it. Assuming k/n to be given, the dilution increases with the length n of
the words, and also with the code redundancy as measured by n −k. Explicit means for constructing a
code C(n, k, d) such that d is the largest possible given n and k are known only for a very small number
of couples (n, k). Moreover, the code performance depends on the whole distribution of the distances
between the codewords, not merely on the minimum distance d. It is why not only exactly optimum
codes in terms of their minimum distance, which are very few, but also good ones according to more
reﬁned criteria, were searched for. This research has been long and succeeded only by means foreign to
the initially tried ones. The scarceness of the codewords is ensured by the code redundancy. Obtaining
a good dilution, i.e., an equal distribution of these words within the set of n-symbol sequences, is a
diﬃcult problem which received approximate but practically satisfying solutions. Designing means for
determining what codeword is the closest to the received sequence, despite the errors which aﬀected its
symbols, is another crucial problem which had to be solved, that of decoding.
The phrase “error-correcting code”, which is traditionally used, is somewhat misleading inasmuch
as the correction is absolutely sure only if the number of errors having occurred is less than half the
minimum distance of the code. “Error-controlling code” would better express its function. However,
“error-correcting code” is widely used in the literature. It is why it will be used in the sequel. Moreover,
it is not the code itself which corrects the errors. A speciﬁc operation, decoding, must be performed in
order to exploit the distance distribution of its codewords. Successful decoding is the key to eﬀective
correction.
Historical outlook: from the fundamental theorem to realistic error-correcting
means
Shannon stated in 1948 [1] the fundamental theorem of channel coding. It paradoxically asserts that the
presence of errors within a received message does not prevent reliable communication, but only limits the
information quantity it can carry. If this limit is not exceeded, the information borne by the message
can be communicated with a probability of failure that a proper encoding by a long enough code can
make arbitrarily small. This statement was a strong incentive to the search for codes able to approach
2The reader should be warned that “codeword”, often abbreviated to “word”, does not designate some part of a message
as in the usual language, but a full-length message selected among the set of possibly transmitted sequences referred to as
the code.
3Decoding is assumed here to always result in a codeword, right or wrong.
4

this limit. Cancelling the eﬀect of errors is highly desirable. The means of error correction known when
Shannon stated the theorem were few and crude (e.g., mere repetition), so it opened to research an almost
pristine ﬁeld, promising very useful applications.
Shannon’s proof of the theorem (maybe inspired by Darwin?), as well as those which were proposed
later, especially that of Robert Gallager [6], rely on the extraordinary idea of random coding. For lack of
knowing how to design the best code, and even a good one, Shannon computed the average probability of
decoding success for a set of codes chosen at random. He proved the theorem for this average probability.
Thus a code at least as good as or better than this average existed in this set. However, the best code in
it remained unidentiﬁed.
Random coding, although a remarkable theoretical tool, cannot be practically implemented, in part
because nothing can a priori guarantee that some code chosen at random is good (actually, it is now
known that it is almost surely good)4. The main reason for not using random coding is that its decoding
would be prohibitively complex, all the more since a code should be long to be eﬃcient. Indeed, each
received sequence should be compared with all the (very many) codewords. To be useful, a code must
be endowed with a structure making its decoding of practically acceptable complexity. The search for
good error-correcting codes and decoding processes has been long and diﬃcult.
Practical means for
approaching the limit stated by the fundamental theorem were found no earlier than the beginning of
the nineties: the turbo-codes of Claude Berrou and Alain Glavieux [7].
This invention also enabled
rediscovering Gallager’s low-density parity-check (LDPC) codes [8] which date back to the beginning
of the sixties. Their performance is close to that of turbo-codes but it remained unknown for lack of
implementation or simulation means able to evaluate it when these codes were invented.
The problem of designing codes having a good minimum distance (if not exactly optimum) found
satisfying algebraic solutions by dealing with the alphabet symbols as belonging to a ﬁnite ﬁeld (a well
deﬁnite mathematical structure). Decoding by algebraic means had to deal with received symbols as
elements of this ﬁeld, and therefore was unable to take into account their individual probability, which
however is available at the receiving end and is useful in the decoding process.
The code C(n, k, d) considered above is referred to as a block code. The encoding process associates
a well-deﬁned n-symbol word with any k-symbol “information message”, thus can generate all the full-
length codewords. Convolutional codes [9] are another code family where the encoded symbols associated
with a single information symbol are computed in terms of this symbol and of the m information symbols
which immediately preceded it, where m is a small number. Then the length of the codewords is not
determined once and for all and encoded symbols are generated when, and only when, a new information
symbol needs to be encoded. The codewords are deﬁned by a local constraint, i.e., which only implies
at once a small number of information symbols. It is the partial covering of the local neighbourhoods
which endows the generated sequence as a whole with its correcting properties. This encoding process
is very ﬂexible since it generates the coded symbols as far as they are needed, thus matching the length
of the coded sequence to the length of the information message. Moreover, it is especially well ﬁtted
to decoding since optimal general decoding processes are known for convolutional codes, especially the
sequential decoding [10] and the Viterbi algorithm [11,12]. Still better, these processes can take advantage
of the individual reliability of the received symbols.
The ultimate source of errors in the physical world is thermal noise due to the random motion of
molecules, which is very well modelled as a random process with Gaussian probability density. More
generally, the Gaussian distribution is observed whenever a large number of independent events of similar
magnitude add their eﬀects. When the symbol errors result from Gaussian noise added to the binary
signals which represent them, it turns out that the decision margin of the optimal receiver, interpreted
as a real number5 to be referred to as likelihood, represents by its sign the most probably transmitted
binary symbol (e.g., + for 0 and – for 1) and by its magnitude the reliability of this very decision, a
function of the probability it is correct. The likelihood thus condenses the whole information available
about a received binary symbol, while the decision restricted to the mere choice of such a symbol, or
“hard decision”, degrades it as ignoring its reliability. A decoding algorithm using the symbol likelihoods
can then take into account the reliability of each individual received binary symbol.
In order to determine what codeword has the most likely been transmitted, algebraic decoding must
deal with each symbol of the received sequence as an element of a ﬁnite ﬁeld, e.g., the binary one. It
cannot take account of the individual reliability of symbols which is expressed by its likelihood, a real
4“All codes are good, except those we can think of”, was believed to be true before turbo-codes were invented.
5The adjective “real” is in mathematics the contrary of “imaginary”. It would be misleading to understand it as referring
to the physical reality.
5

number foreign to this ﬁeld. A completely diﬀerent decoding way consists of computing the likelihood
of each received symbol in terms of the likelihoods of other symbols, which is possible inasmuch as
the encoding generates constraints which make the symbols mutually dependent [13]. From this point
of view, the encoding means where the constraints are expressed by local relations, like convolutional
coding, have the advantage of being simpler. Moreover, decoding a symbol consists of computing its
likelihood in terms of the available data. The decoding process of a complex code can then be split into
successive computations of symbol likelihoods in terms of partial data, the result of which is the input
to a further decoding relying on more data. Any hard decision is avoided, hence the information loss it
would imply. Furthermore, the better estimation of a symbol likelihood enables improving the likelihood
estimation of symbols to which it is linked, and this process can be iterated until the decisions on symbols
reach a suﬃcient degree of certainty. Berrou and Glavieux forged the word “turbo-codes” to name the
combination of convolutional codes they invented such that it can be decoded using this process [7].
The correcting ability thus results from constraints which make the symbols mutually dependent and
enable calculating the likelihood of each received symbol in terms of the others’ likelihoods. Engineers use
mathematical constraints as well deﬁned and easily implemented by electronic means, but constraints
of any kind have similar results: they generate codes which are almost surely good. In biology or in
linguistics, these constraints are often exclusion rules which forbid certain symbol combinations.
Non-deliberate coding
Intense researches about error-correcting codes were made since the end of the fourties. Engineers now
have at their disposal many code families which rely on mathematical constraints. Together with their
decoding algorithms, they are widely implemented in objects of daily use like mobile phones. Error-
correcting codes have however a much larger extent since both nature and culture make use of them,
too. These non-deliberate coding means are little known despite their omnipresence and their crucial
importance. The phrase “non-deliberate coding” is used here in order to name an unintentional coding
process. This kind of coding is met in natural processes, especially in heredity, but also in linguistic
communication, which is at the basis of any human culture although the origin of language may rightfully
be considered as natural. The phrase “non-deliberate coding” has been preferred to “natural” so as to
avoid any ambiguity. For short, the result of non-deliberate coding will often be referred to as a “soft
code” in the following. It means that it cannot be expressed in mathematical terms, not that it is a loose
one.
Non-deliberate coding is actually hidden. It is only because engineers invented error-correcting codes
that its necessity and importance can be recognized. It turns out that heredity and biological evolution
cannot be understood unless it is realized that genomes are endowed with such codes. Similarly, the mere
possibility of literal communication by means of language cannot be explained otherwise. The general
ignorance of technology, the compartimentalization of knowledge as well as the divide between literary
and scientiﬁc disciplines unfortunately entail that neither the biologists nor the linguists are aware of
non-deliberate coding.
Comparing non-deliberate codes in language and genomes with engineering ones immediately reveals
that the former are far more redundant than the latter. A single line of a written text, say made of
70 printed characters of the Latin alphabet, assumes one among 2670, about 1099, distinct combinations
(neglecting the intervals between the words and the punctuation signs). For any written text, this number
should be raised to a power equal to the number of its lines. These ﬁgures should be compared with the
estimated number of atoms in the observable universe, about 1080 “only”.
Figures which similarly defy one’s imagination are found in genomics. For instance, the genome of
the bacteriophage ϕX174, the ﬁrst sequenced virus [16], is long of 5,400 nucleotides, which corresponds
to 45,400 distinct possible combinations, or about 3.245 × 103,200 in the absence of redundancy!. Raising
it to a power of a few thousands would result in the number of combinations corresponding to short
genomes of plants or animals. Such ﬁgures cannot be explained but by a huge redundancy. In strong
contrast, codes designed by engineers are much less redundant since, besides optimizing their distance
distribution, they also try to limit their length. The huge redundancy of the genomic codes has important
consequences on the living world as a whole.
6

Engineering codes vs soft codes
We present now a schematic comparison between error-correcting codes in the engineering meaning and
non- deliberate ones.
We will especially distinguish “decoding”, which pertains to engineering, from
“regeneration” pertaining to non-deliberate coding.
Communication by means of an error-correcting code in the engineering meaning can be represented
according to the following scheme:
IMes
encoding
−→
CodW
errors
−→RSeq
decoding
−→
IMes′,
where IMes represents the information message, a sequence of independent and equiprobable binary
symbols, hence redundancy-free, whose length k measures in binary units the information quantity it
carries. Encoding transforms the information message into a codeword CodW according to a one-to-one
correspondence. It may consist of appending to IMes n −k redundant symbols computed in terms of
IMes. CodW is a sequence of n binary symbols, with n > k, (or of n symbols of an alphabet of size q,
with qn > 2k). CodW belongs to a code with a minimum distance of at least d > 2 between any two of
its words. The received sequence RSeq consists of the codeword CodW where a number of symbols have
been changed. Due to these errors, RSeq does not most often belong to the code. Decoding consists of (1)
ﬁnding the word CodW’ of the code the closest to the received sequence RSeq according to the Hamming
distance, and (2) recovering the corresponding information message IMes’. The ﬁrst step can fail if the
number of erroneous symbols exceeds the correcting ability of the code, whereas the second one is a mere
one-to-one correspondence. As far as the code is eﬃcient, IMes’ is almost always identical to IMes.
Similarly, communication by means of a soft (non-deliberate) code is described by the following
scheme:
[IMes
encoding
−→] Seq
errors
−→RSeq
regenerating
−→
Seq′ [
decoding
−→
IMes′],
where Seq denotes a sequence of symbols which natural constraints make mutually dependent, so Seq
belongs to a soft code. IMes denotes as above an information message. The brackets between IMes are
intended to mean that it has a meaning for information theory, but needs not be explicit. IMes will in the
sequel be said “uncoded” as devoid of redundancy. Regenerating consists of ﬁnding the sequence Seq’ of
the code the closest to the received sequence RSeq according to the Hamming distance. Hence it has the
same meaning as the ﬁrst step of decoding in the scheme which represents engineering communication.
Seq’ is expected to be identical to Seq. IMes’ denotes the decoded version of IMes. Since IMes is not
explicitly needed, the last step is not necessary. However, IMes and IMes’ may be useful for describing
processes, and they will especially be used in the next section in order to describe the system of nested
component codes, illustrated by the fortress metaphor and depicted by Fig. 1.
In any case, a speciﬁc operation, decoding or regeneration, must be attempted so as to cancel the
eﬀects of symbol errors on the messages. If it is successful, errorless communication results.
Error-correcting means exist in genomes and consist of nested component
codes
Being a sequence of nucleotides, a genome can be considered as a written message which tells how to
assemble a living being according to the rules which pertain to its species. We may thus think of it as
performing a communication from the past to the present. It is more usual to think of communication as
linking two entities which are separated in space, hence as telecommunication. However, both situations
are formally identical, whether the partners are separated in time or in space. Information theory is thus
equally relevant to both cases.
The necessity of error-correcting codes in genomes is obvious. When a DNA molecule is replicated, the
estimated probability of error is one millionth (10−6) per nucleotide. This is indeed a small probability,
but an error in a single nucleotide suﬃces to make the whole sequence diﬀerent from the original one.
The probability of the exact replication of a very long genome is thus extremely small. For instance, the
human genome is made of about 3.2 × 109 base pairs (it is not by far the longest known) so each of its
replications results in the average in about 3,200 erroneous nucleotides. The probability of obtaining a
replicated genome exactly identical to the original one is extremely small6, so this event would be abso-
lutely miraculous. Furthermore, the DNA molecule is permanently exposed to thermal noise, chemicals
6About that of 4,616 successive heads when tossing up, or 10−1,390 !
7

and free radicals produced by the cellular metabolism, radiations, especially ultraviolet ones, and radioac-
tivity. The requirement that the genome be exactly replicated may look unnecessarily drastic, but the
conservation of basic genetic features demands that it resists hundreds of millions successive replications.
Without this requirement, the message it carries cannot persist during geological ages, whereas genome
mutations in somatic cells are observed during the incomparably shorter time of, for instance, a human
life. Genomic error-correcting codes must exist.
The existence of such codes is moreover the only means which explains why very ancient parts of
the genomes are better conserved than more recent ones. Let us ﬁrst examine how, and to what extent,
endowing a written message with error-correcting ability protects the information it carries when its
symbols constantly incur random errors. The average number of erroneous symbols increases with time
however small the error probability is. The message regeneration should thus be attempted after a time
interval such that this number remains less than the correcting ability of the code. Then the correct
regeneration of the message is very likely, although not absolutely sure, because the number of errors
is random. Since errors continue to occur after a ﬁrst successful regeneration, the message conservation
needs that successive regenerations are attempted at a periodic or quasi periodic pace. The time interval
between successive regeneration attempts is a critical parameter. If it is short enough, the exact recovery
of the message is almost sure except for the small probability that regeneration fails. As far as regeneration
is successful, the message is exactly conserved. In the infrequent case of regeneration failure, however,
it becomes markedly diﬀerent since it is then at a distance from the correct one equal at least to the
minimum distance of the code.
The more eﬃcient is the code, the less likely is thus a regeneration
failure, but the more diﬀerent the resulting message is when it occurs. Any regeneration failure endows
the message with new information.
This scheme obviously pertains to the conservation of genomes.
Mutations are then the infrequent results of regeneration failures.
It is an established fact that very ancient parts of genomes such as the HOX genes, which are common
to humans and ﬂies, are better conserved than more recent ones (see, for instance, [14]). This is an
irrefutable argument on the existence of genomic error-correcting codes since, without encoding, the
accumulation of errors as time passes would the more degrade parts of the genome, the older they are. The
better conservation of ancient parts cannot be explained unless assuming that a genomic error-correcting
code resulting from a stepwise encoding exists: a ﬁrst encoding has been followed later by a second one
where a new information message and the result of the ﬁrst encoding were jointly encoded [15]. This
process has been repeated several times. The resulting overall code is then a system of nested component
codes, where the number of components increased through the ages. Its redundancy is a fast increasing
function of the number of its components, which accounts for the huge redundancy of the genomes. We
shall refer to the successive encodings as “coding layers”, the deeper ones being the older.
Such a system can be described very sketchily by means of the fortress metaphor, as illustrated
in Fig. 1.
Although it concerns non-deliberate coding, it is easier to describe it by using the word
“encoding” in its engineering meaning of appending redundancy symbols to a redundancy-free, “uncoded”
information message (as introduced in the previous section and more lengthily described in Section “What
is information? What is an information?” below). A component code of this system is depicted as a closed
wall which protects what is inside it against outside attackers. (The reader should be warned that an
information bearing message is actually not protected by enclosing it within some narrow region as
suggested by Fig. 1 but, on the contrary, by its dilution within a wide one as discussed above.) The
more numerous walls enclose an information message, the better it is protected.
Notice that a very
eﬃcient protection of the most central information does not demand very eﬃcient component codes:
the multiplicity of enclosing walls is much safer than each of them separately.
Assuming they were
built successively during the geological ages entails that the older information is better protected. An
interesting problem would be to try to determine the number of component codes that protect, say, a given
gene (its ”geological age”), although much further research will be necessary before it can be solved. This
scheme is made compatible with crossing-over, hence with sexual reproduction, if it is further assumed
that the most lately appended information message has been left uncoded and that it concerns only the
small fraction of the total genome which accounts for individual diﬀerences among the members of a
species.
The structure of the system of nested component codes just described entails that genomes are very
heterogeneous, with successive nucleotides possibly belonging to component codes with very diﬀerent
properties. The most important information is hidden, since it is distributed within a large number of
nucleotides. Comparing genomes on the basis of their length is thus meaningless. It turns out that a
similar structure of nested component codes also exists in language, with the same consequences as regards
8





'
&
$
%
'
&
$
%
I1
I2
I3
I4
C1
C2
C3
Figure 1: System of nested component codes as illustrated by the fortress metaphor. A code is represented
as a closed wall which protects what is inside it. I1, I2, I3 and I4 are independent information messages.
The encodings by the codes C1, C2 and C3 occurred successively in time, in the order of increasing indices.
Therefore I1 is protected by a combination of 3 codes, I2 by a combination of 2 codes, I3 by a single code
and I4 is left uncoded.
its error-correcting ability and high redundancy. The main biological consequences of the structure of
nested component codes will be examined below in section “Information theory explains the main features
of the living world”.
Marcello Barbieri described the evolution of the living world as resulting from the successive appear-
ance of cumulative organic codes [2–5]. Similarly to the “genetic code”, these codes are correspondence
rules between the genome as a sequence of nucleotides and some external reality. These organic codes
were not explicitly introduced as error-correcting but, inasmuch as these correspondence rules induce
constraints aﬀecting the genomes, they possess an error correcting ability. Any new organic code induces
new genomic constraints, hence new components in this system. Barbieri’s organic codes may thus be
identiﬁed with the system of nested error-correcting codes needed to conserve the genetic information
which explains the better conservation of older parts of genomes.
2
Information as a fundamental biological entity
A necessary duality
Obviously, all the objects which belong to the living world are self-reproducing. Using logical arguments,
von Neumann proved that a necessary condition of the existence of such objects is that they should
contain their own symbolic description [17]. Interestingly, von Neumann’s result can be interpreted in
geometric (steric) terms. A newly duplicated molecule must be contiguous to the copied one used as a
template. If the arrangement of the molecules to be duplicated is one- or two-dimensional, one remaining
dimension spans enough space to contain the duplicated molecules. However, if the initial arrangement is
tridimensional, there is no room for newly duplicated molecules: a 3-dimentional object cannot be used
as a template because it already occupies all the space which its own replica would need. A symbolic
description, borne somewhere else in a memory, is needed for lack of a possible physical replication. Living
beings fulﬁll von Neumann’s condition since the genome they host bears such a symbolic description. Even
more, it is actually the recipe according to which a living thing is assembled and develops itself.
Von Neumann’s self-replicating scheme accounts for the biological reality if it is further taken into
account that a few individual diﬀerences exist in genomes. The concerned nucleotides have been in-
terpreted as uncoded information symbols in the system of nested component codes described above.
Moreover, it should be understood that the symbolic description of a living thing borne by the genome
is fragile so its integrity must be ensured. The protection it needs is twofold: (1) as an object of the
physical world, the DNA molecule should be shielded against mechanical and chemical aggressions; (2)
the symbolic description itself, as an abstract sequence of symbols, should be protected against errors
by an adequate code, as shown above. The shielding should result in a symbol error rate compatible
with the correction ability of the code. A living thing ensures both kinds of protection: the ﬁrst one by
means of membranes which contain the DNA molecules (that of the cell in prokaryotes, of the nucleus
in eukaryotes); the second one, indirectly, because phenotypic constraints induce a system of nested soft
codes within the genome. Their relationship has been represented in Fig. 2.
According to this scheme, a living thing and its own genome may be interpreted as involved in a
feedback loop. The genomic information instructs the assembly of living structures, but a living structure
9

genome
living thing
instructs the assembly
doubly protects
-

Figure 2: Relationship of a living thing and its genome.
imposes on the genome it hosts constraints which ensure its permanence by endowing it with error-
correcting ability. Once this loop is closed, it results in the conservation of both, hence of the living thing
as a whole.
An individual living thing is not protected against perturbations from the outer world with which,
as an open system, it exchanges matter, energy and information. Moreover, its assembly, development
and maintenance demand drastic environmental conditions. Such an individual lives only a ﬁnite time,
so it provides merely a provisional shielding to the genome. However, replicas of the genome instruct
the assembly of copies of the living thing. The multiplicity of replicas results in a population which is
fed with new copies as long as the genome remains properly regenerated thanks to the genomic error-
correcting code. What is conserved is thus a type common to the population members, the phenotype,
not individuals. Diﬀerences in individual genomes, which were interpreted above as consisting of uncoded
nucleotides, result in some diﬀerences within the population of copies. Sexual reproduction, when it is
present, tends to increase the genetic diversity within the population, but restricted to the comparatively
few uncoded nucleotides. Sporadic but normally infrequent regeneration failures result in innovations
in the genomic information of a much larger extent. Diﬀerences are targets of the Darwinian selection.
Mutations possibly generate populations of another type.
What constraints induce soft codes in the genome?
We now more closely examine the genomic error-correcting code and the biological consequences of its
existence. We ﬁrst look at the constraints which endow genomes with error-correction ability. They
may roughly be thought of as belonging to three main classes: (1) steric constraints directly aﬀecting
the DNA molecule; (2) steric constraints aﬀecting the proteins that the genes specify, which indirectly
forbid certain nucleotide sequences; and (3) syntactic constraints needed for instructing the assembly of
phenotypes. The ﬁrst two classes at least comprise soft codes.
As an example of steric constraints directly aﬀecting the double-stranded DNA molecule, it is closely
packed in the nucleosomes of eukaryotic cells, wrapped around histone octamers acting as spools. Bending
constraints result which forbid many nucleotide sequences. The autocorrelation and spectral properties
of genomes conﬁrm that their nucleotides, dealt with as symbols, are mutually dependent [18–20].
An example of steric constraints indirectly aﬀecting the DNA molecule is given by the synthesis of
proteins. The DNA molecule in a gene instructs the synthesis of a polypeptidic chain which, properly
folded, becomes a protein. This chain is a sequence of amino acids, each of which is speciﬁed by a “codon”,
i.e., a triplet of nucleotides, by means of a correspondence rule referred to as the “genetic code”. The
resulting protein exhibits substructures like α-helices and β-sheets which are not compatible with many
sequences of amino acids because of steric constraints [21]. Forbidding these sequences entails that the
sequences of codons which specify them, too, are forbidden. One may wonder that protein features control
the structure of DNA molecules, whereas DNA speciﬁes the synthesis of proteins. This may look as a
kind of teleology, but causality is not violated since it can be explained by feedback eﬀects [15,22].
Directly or not, steric constraints thus act as exclusion rules on the set of nucleotide sequences,
deﬁning a “soft code” as made of all the remaining ones. These constraints are moreover cumulative,
which entails that the codes they deﬁne constitute as expected a system of nested soft codes. Furthermore,
specifying phenotypic structures demands that genomes be endowed with some syntax which still restricts
the possible nucleotide sequences to those which are compatible with this syntax, and moreover which
are meaningful according to their semantics. Again, these constraints are cumulative with those of steric
origin and contribute further components in the nested system of component codes.
Another kind of genomic codes probably exists although it is foreign to constraints incurred by the
DNA molecule. Engineers often use error-correcting block codes where encoding a k-symbol information
message (i.e., redundancy-free) consists of appending to it a sequence of n−k redundant symbols computed
10

in terms of this information message. These codes are referred to as “systematic”. In eukaryotic genes,
only some parts of the DNA molecule, named the exons, control the synthesis of the polypeptidic chain
later becoming a protein; they may rightfully be identiﬁed as information messages. The remaining parts
of the gene, the introns, are spliced out before the synthesis begins. Donald Forsdyke suggested that
introns are made of the redundancy symbols appended to the exons according to some systematic block
code [23]. In order to check this hypothesis, one may compare the variability of exons and introns in
diﬀerent evolutive situations. A regeneration failure results in a codeword at a distance from the original
word equal at least to the minimum Hamming distance d of the code. Some of the d or more erroneous
symbols aﬀect the exons, and the remaining ones the introns.
The literature states that introns are
generally more variable than exons. A counter-example was however found by Forsdyke in genes which
direct the synthesis of snake venom proteins [24]. The usual greater variability of introns and Forsdyke’s
counter-example can both be explained by assuming that a eukaryotic gene is a word of a systematic code
where the exons are the information symbols and the introns the redundant ones. If the exons instruct
the synthesis of proteins of physiological importance, the most usual case, it may be expected that only
mutations with few errors within the exons, hence having no7 or little incidence on the speciﬁed proteins,
will survive natural selection. As regards venoms, however, snakes and their usual preys, rodents, are
involved in an evolutive “arms race”: some rodents incur mutations which provide an immunity to venom,
threatening snakes with starvation unless mutations in their own genes make their venom able to kill
mutated rodents [24]. Evolution should thus favour mutations producing venom proteins as diﬀerent
as possible from the original ones, hence concentrating most of the erroneous nucleotides in the exons.
Introns then incur few diﬀerences, as actually observed.
What are the means of genome regeneration?
Precisely identifying the genomic error-correcting codes remains to be done. Another open problem con-
cerns the genome regeneration: how can molecular machineries exploit the huge redundancy of genomic
codes so as to correct the errors which occurred? The engineers’ experience has shown that implementing
this crucial function is especially diﬃcult. Information theory and knowledge of the codes designed by
human engineers have been the main tools for reaching the general conclusions above. The detail of
molecular mechanisms, however, remains entirely to discover and information theory is of little help to
this end. Identifying the molecular means of genome regeneration thus remains an open problem. The
concept of “soft code” provides some clues, however. Deﬁning these codes as the mere consequences of
biological contraints hints at a possible identity, or at least similarity, of the means which perform DNA
generation and those of its regeneration. In other words, errors in nucleotides violate the constraints and
a process which tends to restore them would correct the errors. No decoding or regeneration process
distinct from those already at work is then needed as far as soft codes are concerned. This would also
explain why the possibility of genome regeneration is restricted to germinal cells.
Information theory explains the main features of the living world
We now show that information theory explains the main features of the living world and of its evolution.
Many fundamental features of the living world that biologists observe but do not explain are indeed mere
consequences of the existence of the error-correction means which conserve the genetic information.
Obviously, all the objects which belong to the living world are self-reproducing and this feature is spe-
ciﬁc to them. It has been stated above, according to von Neumann, that such objects must contain their
own symbolic description. Therefore, it exists in each of them a memory which carries this description
and can be replicated: its genome. It follows that the presence and use of information is a speciﬁc feature
of the living world.
Conserving the genomic information demands error-correcting means. Moreover, the better conserva-
tion of the older parts of the genome cannot be explained unless assuming that error-correction encoding
has been performed according to successive layers, such that more component codes protect the older
parts of the genome. The genomic code is then the system of nested component codes illustrated above
by Fig. 1.
The genome is a word of this error-correcting code, except that a small fraction (but a large number)
of its nucleotides are left uncoded, so they individually incur errors with some rather high probability. A
7This is possible since a same amino acid is speciﬁed in many cases by several distinct codons.
11

genome can thus be exactly regenerated except that (1) the nucleotides which belong to the small fraction
left uncoded individually incur errors with some fairly high probability; (2) the exact regeneration of the
encoded fraction of the genome is a chance event: the probability of a wrong regeneration, referred to as
a mutation, may be very small but is never exactly zero. The symbols of the genome, i.e., the nucleotides,
are exposed to errors with an almost steady probability. The more time passes, the more numerous the
erroneous symbols, so the probability of a wrong regeneration crucially depends on the time interval
which preceded the regeneration attempt. It is close to zero only if this interval is short enough. Since
errors steadily aﬀect nucleotides, a genome still incurs nucleotide errors after it has been successfully
regenerated. Regeneration attempts must thus be repeated according to a quasi periodic pace so as to
conserve the genomic information.
Biology associates species with genomes, and the necessity of quasi periodic regenerations attempts
accounts for the existence of successive generations.
Being codewords, the genomes are intrinsically
discrete, hence so are species. In the absence of a genomic error-correcting code, the living world would
be an indescribable chaos populated with innumerable short-lived singular chimeras.
If the time interval between regeneration attempts is short enough, regeneration is almost always
successful, and the successive replications of the genome result in a homogeneous population. In the
infrequent case of regeneration failure, a mutation occurs.
A diﬀerent genome results and its future
depends on its ability to withstand the Darwinian selection and initiate a new population.
In the (normally) infrequent case of mutation, the wrongly regenerated genome diﬀers from the initial
one in a number of nucleotides which widely depends on the depth of the encoding layer where it occurred:
the deeper this layer, the more numerous the erroneous nucleotides but, at the same time, the less likely
is the regeneration failure.
It is likely that only the genomes in germinal cells incur these quasi periodic regeneration attempts. In
eukaryotes, for instance, one may reasonable assume that it occurs during meiosis. Probably, the regener-
ation is attempted for the whole genome, hence simultaneously for all its component codes. But the nested
structure implies that the minimum Hamming distance of a code which combines component codes up
to some depth considerably increases as this depth increases, hence its correcting ability. Comparatively
frequent regeneration failures will occur when this depth is small but will have limited consequences on
the phenotypic structures they specify. Regeneration failures of the more eﬃcient codes which combine
component codes up to some larger depth in the nested system will entail larger diﬀerences. The more
infrequent will be the regeneration failures at some encoding depth, the larger the diﬀerences they entail.
The average time interval between regeneration attempts is subjected to the Darwinian selection. If
it is too large, regeneration failures will frequently occur so the genome can adapt itself to environmen-
tal changes but lacks stability, so many species appear and disappear (maybe the Cambrian explosion
corresponded to such a situation). If this time interval is too small, the genome is very stable but lacks
ﬂexibility to ﬁt changing conditions. A nearly optimum value has eventually been reached through the
ages.
The good conservation of very ancient parts of genomes has been explained by assuming that the
genomic code is made of a nested system as described above, the component codes of which appeared
successively during the ages. This implies a trend of the genomes towards lengthening. A number of
mechanisms having this result, collectively referred to as horizontal genetic transfer, are known. The
discreteness of genomes and the existence of a Hamming distance between them imply without ambiguity
that evolution is saltationist, since failing to exactly regenerate a genome results in another one. However,
the component codes of the nested system have very diﬀerent minimum distances depending on their
depth within the system of nested codes. For the less deep of them it is comparatively small, resulting in
rather frequent mutations of small amplitude. Deeper component codes, on the contrary, result in very
infrequent mutations of large amplitude. Evolution thus proceeds by jumps the amplitude of which is
the larger, the more infrequent they are. This also explains why species, besides being discrete, can be
classiﬁed according to a hierarchical taxonomy.
The trend of evolution towards increasing complexity, in the sense that more complex beings appeared
after simpler ones, receives an explanation from information theory, which tells that long codes can oﬀer
a better protection against errors than shorter ones. The resilience to symbol errors is clearly an evolutive
advantage, hence the Darwinian selection operating on genomic error-correcting codes favoured longer
ones. Lengthening genomes can also beneﬁt their ability to specify complex objects besides their resilience
to symbol errors. Resilience and complexity may very inequally beneﬁt from lengthening.
The total number of past and present species can be very roughly estimated to 109. Even if it is
underestimated, it is very small as compared with the number of combinations that even the smallest
12

genomes would assume in the absence of redundancy. We may thus think of the set of all past and
existing genomes as a collection of error-correcting codes with huge redundancy, all the more since there
is no limit to the length of genomes. Of course, the Darwinian selection acting on phenotypes contributes
in the rarity of species, in a proportion which is diﬃcult to estimate. Contrary to the redundancy of
genomic codes, this rarity has no eﬀect on their conservation. Selecting means eliminating, the contrary
of conserving.
Heredity is the main phenomenon which distinguishes the living things from other objects of the
physical world. It consists of communicating information. That living things contain and/or use infor-
mation, while inanimate things do not, may be considered as the critical discriminating feature between
the living and the inanimate. (However, certain artefacts produced by humans actually contain and/or
use information; although not living themselves, they originate in living things and beneﬁt them.)
Information theory is central in biology
Having in mind the sole problem of how the genome instructs the assembly of a phenotype, John Maynard
Smith (1920–2004) stated that information theory would be of little help in biology because it is exclusively
devoted to literal communication [25]. It is thus irrelevant to this problem of semantic communication.
Most biologists followed Maynard Smith in his a priori rejection of information theory, which avoided the
need for assimilating this mathematical science very foreign to their culture. This was however ignoring
that the meaning of a message is available only if this message itself is available, which demands its literal
communication. It is why it has been written above that the necessity of error-correcting codes in genomes
is obvious. Conservation of a genomic message during the geological ages through very many successive
replications is a very big problem that biologists overlooked. It is clearly relevant to information theory.
One of the main diﬃculties for convincing biologists of seriously considering information theory is
the polysemy of the word “information” and the seemingly strange fact that, in its scientiﬁc meaning,
it ignores semantics. In any current use of the word, on the contrary, it always refers to some meaning.
Information without semantics seems an oxymoron, at best an empty shell. It is however absolutely
necessary to distinguish literal communication, i.e., making some message available to its destination,
from semantic communication, which associates a message with some meaning, i.e., makes it refer to an
external reality [1]. Communication technology is concerned exclusively with the former, and information
theory is based on Shannon’s reﬂection about it.
The scientiﬁc concept of information needs thus be made precise and its epistemological status should
be understood. Remember that a message is a sequence of symbols, elements of some ﬁnite alphabet, i.e.,
of any predetermined set of signs, letters or digits, in an arbitrary but ﬁnite number. These symbols must
be perceived by humans or machines, hence they should have a physical representation. For instance,
they are represented by signs written on a sheet of paper, by vocal sounds uttered by a human speaker,
by electrical or electromagnetical signals used in telecommunication, or by nucleotides in a DNA molecule
. . .
However, their physical nature does not matter insofar as it is only their belonging to the alphabet
which is relevant to a communication process. Each symbol should be unambiguously identiﬁed as an
element of the alphabet. At the message level, the alphabet size can be changed. Moreover, for a given
alphabet size, it is possible to encode a message, i.e., to replace it by a shorter or longer one from which the
initial message can be exactly recovered. The encoded message can thus acquire some desirable property
that the initial message lacks, especially brevity or resilience to errors. Information theory distinguishes
two main types of coding, which are antagonistic as regards the length of the encoded messages: source
coding and channel coding. Source coding aims at obtaining the shortest possible message, while channel
(or error-correction) coding is intended to protect the message against errors aﬀecting its symbols, which
necessarily implies endowing it with redundancy, hence lengthening it. An information then appears as
an equivalence class of messages with respect to any change of alphabet size and encoding of any type.
It is thus a collective, abstract entity.
The abstract nature of information is not fully recognized and has even been denied. For instance, in
the early years of information theory, the physicist L´eon Brillouin tried to use it in order to understand
life [27]. This attempt was rather unsuccessful for lack of a reﬂection about the epistemological status
of information, which was dealt with as a physical entity. Rolf Landauer claimed later “information is
physical” [28]. This is plainly wrong. What he actually stated is that the representation of symbols must
be physical, which is obvious. It does not imply that the information itself is so. Moreover, information
is associated with symbol sequences, namely messages, hence with combinations of symbols and not with
individual ones.
13

Clearly, an information can interact with material objects only by the agency of a physical represen-
tation of the message which carries it. Copying a message does not multiply the information quantity it
contains, but only shares the information it carries, making it available elsewhere. Thus, a same infor-
mation is available within an arbitrary number of physical media, referred to in engineering as memories
(including none, in which case it is a purely mathematical object): an information is not restricted to a
speciﬁc location in the physical world. In living things, an exactly replicated genome carries the same
information as the original one. The members of any species, the existence of which results from multiple
successive replications, thus carry in their genome essentially the same information. Since it instructs the
assembly of their phenotypes, these members are very similar.
What is information? What is an information?
In a few years following the publication of Shannon’s “A mathematical theory of communication” [1], a
lot of papers were published by researchers in genetics, neurology, psycho-physics, psychology, economy,
linguistics, sociology, and many other ﬁelds, who intended to apply information theory in their own
discipline. It was actually an irrational fad, and most of the authors of these papers had only a superﬁcial
knowledge of information theory. The pioneers of information theory soon restricted their publication
policy so as to avoid that works actually relevant to it be diluted in the ﬂood of papers about more or less
judicious attempted applications. Shannon himself wrote a short article to discourage such attempts [26].
He did not deny the legitimacy of applying information theory outside the engineering ﬁeld, but he
warned that it “is not a trivial matter of translating words to a new domain, but rather the slow tedious
process of hypothesis and experimental veriﬁcation.” Of course this fad has been ephemeral and brought
little progress to the concerned disciplines. Far from helping the popularization of information theory, it
blurred its image, even for the scientiﬁc public.
The very concept of information is diﬃcult and needs be made more explicit. Up to now, we saw the
importance of channel coding in order to conserve information. We shall have recourse to source coding8
to help understanding the concept of information and its relationship with semantics. The fundamental
theorem of source coding tells that, under rather broad conditions, a message can be represented by
a shorter but fully equivalent one, the length of which has a lower limit. Contrary to channel coding,
explicit means of optimum source coding were designed very early, especially the Huﬀman algorithm
(described in [15], pp. 82–85). The simplest possible alphabet, namely the binary one, will be used in
order to describe the shortest message associated with a source. Its symbols are referred to as “bits”,
an acronym for “binary digits”. The shortest possible binary message which can be obtained from some
message by source coding will be referred to as its information message.
The successive bits of an
information message are equiprobable and mutually independent. The information messages associated
with all the outputs of a source can be represented by paths in a tree, as drawn in Fig. 3. At each
node of the tree, one of the two branches is randomly chosen wtth probability 1/2. Then, any path
of ﬁnite length in this tree is associated with an information. The sequence of binary labels along the
path is the corresponding information message. It is the more concise binary message which belongs to a
given information interpreted as an equivalence class of messages. The length of the path quantitatively
measures the information it carries in binary units, named “shannons”9.
The height of the tree is
unlimited, although continuing to draw it soon becomes impossible because of the exponential increase
of the number of its branches.
As deﬁned, an information message is a sequence of binary digits, but it is not the binary representation
of a number. Given any two diﬀerent numbers, one of them is always smaller than the other one. No
such ordering exists within the set of information messages, which are merely diﬀerent from each other.
The only distinctive feature of an information message is its singularity with respect to all others. The
concept of information is thus foreign to meaning. Maybe comparing it with a musical theme can be
useful to grasp it. In a sense, it is more basic than the concept of number as inherent to the function
of communication. The practical importance of numbers in the human culture explains the existence of
a plentiful vocabulary related to them. The scientiﬁc and philosophical importance of the information
concept began to become clear no earlier than the middle of the twentieth century, and no adequate
vocabulary existed to deal with it.
This is probably the main obstacle to popularizing information
theory, and it is a big one.
8“Source” means here a generator of messages.
9Shannon introduced this unit and named it “bit”, acronym of “binary digit”. Since a binary digit carries at most this
unit of information quantity, if any, using another word was highly preferable.
14

Root
A
A
A
A
A
A
A
A
A
A
AA











@
@
@
@
@@
      @
@
@
@
@@
      HHHHHH

HHHHHH

HHHHHH

HHHHHH

XXXXXX

XXXXXX

XXXXXX

XXXXXX

XXXXXX

XXXXXX

XXXXXX

XXXXXX

r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
















XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
Figure 3: “Natural” binary tree.
Its height has been limited to 4 branches for the sake of drawing
ease, but incomplete branches were drawn at right in order to mean that the tree construction can be
indeﬁnitely continued. Black dots represent the nodes.
15

Information theory can be useful to biology provided biologists and information theorists closely
collaborate. In particular, geneticists should get acquainted with information theory [29,30]. Overcoming
the diﬃculties of its popularization deserves large eﬀorts: the issue at stake is actually a conceptual
revolution.
Relationship between information and semantics
As regards the relationship between information and semantics, let us ﬁrst notice that, being intrinsically
foreign to semantics, the scientiﬁc concept of information is unrelated with that of truth: an information
is neither true nor false.
Representing information messages according to the tree of Fig. 3 sheds some light on the relationship
of information with semantics. We saw that no meaning is intrinsically associated with an information.
However, it can be arbitrarily endowed with a meaning by means of this tree. Let us assume that a
dichotomic question is associated with each node of the tree, and that the binary answer given to it
determines the next branch to be chosen. Then the sequence of answers from the root to a node along
a path in the tree constitutes a semantic statement. A plant guide is a practical example of questions
and answers associated with the paths of a tree, the sequence of which is a meaningful statement. Since
the questions and answers are unrelated with the path itself, semantic and literal communication are
separated in accordance with Shannon’s principle.
However, the length of a path in the tree, which
measures the information quantity that the corresponding message carries, also measures its semantic
speciﬁcity. The longer the message, the more detailed and/or precise will be the semantic statement it
carries.
Within a human community, endowing a message with a meaning results from cultural conventions.
But are similar conventions possible within the living world, named by Barbieri “natural conventions”,
and how are they established?
The answer lies in the existence of molecular machineries which can
operate on objects designated by sequences of nucleotides. Natural semantic conventions are possible
because of the high speciﬁcity of the enzymes as organic catalysts.
Let us ﬁrst examine how the semantic communication works in the human culture, say in written
form. An alphabet, i.e., a set of elementary symbols in ﬁnite number, say the Latin alphabet, is given
ﬁrst. Its elements, referred to as letters, can be unambiguously distinguished from each other. Then,
certain sequences of a ﬁnite number of letters, referred to as words, are distinguished from any others and
exclusively used. Certain words referred to as substantives refer to concrete objects or abstract entities,
while words of another kind, especially the verbs, are intended to represent the relations between the
objects named by the substantives, or actions involving them, with the help of grammatical rules. A
sequence of words, hence a sequence of letters, is then endowed with a meaning. There is no reason to
associate some word with some object unrelated with letter sequences other than a linguistic convention
shared by the members of some community, having its roots in the past. Then it becomes possible to
describe an external reality, however complex, by means of mere combinations of the ﬁnite number of
letters of the alphabet. Notice that the linguistic convention must be already established (anteriority)
and shared by the members of a community (universality).
Let us now consider the case of genetic communication; more speciﬁcally, how genes instruct the
synthesis of proteins. Messages are nucleotide sequences along a strand of DNA. Nucleotides belong to
the 4-element alphabet {A, T, G, C}, but it is more convenient to consider as basic the 64-element
alphabet consisting of all triplets of nucleotides, or codons. The elements of this alphabet will be referred
to here as the genetic letters. According to the genetic code, 61 of the genetic letters designate one of the
20 amino acids which are the regular constituents of proteins, any one of the remaining 3 being used to
indicate the end of a sequence (a function which, in human languages, is devoted to the interval which
separates the words, conveniently dealt with as belonging to the alphabet). The sequence beginning is
unambiguously indicated by diﬀerent, more complicated means. Complete sequences of genetic letters
are referred to as genes. A gene is read by the ribosomic machinery, which assembles the amino acids
corresponding to its letters in the same order, resulting in a polypeptidic chain which, properly folded,
eventually becomes a protein.
(This sketchy description is valid only in the simplest case, that of a
prokaryotic cell.)
We may think of a gene as the equivalent of a substantive in a human language, which “means” the
protein it speciﬁes. Then, the ribosomic machinery ensures the correspondence between a gene and a
protein, thus acting as the natural equivalent of the semantic convention which is at the heart of human
communication. The ribosomic machinery exists before it works and the genetic code is almost universal
16

within the living world, so the two conditions of anteriority and universality stated above are satisﬁed. We
may interpret in these terms Barbieri’s concept of “natural convention”. Of course, a linguistic convention
is an abstract entity speciﬁc to a human community while a molecular machinery is a physical object.
Their semantic functions, however, may be interpreted as having a strong analogy.
The number of objects which are speciﬁed by the genes and assembled by the ribosomic machinery,
namely, the proteins, is potentially inﬁnite. Certain proteins, the enzymes, possess the remarkable prop-
erty of acting as catalysts for very speciﬁc organic reactions, especially those which are needed to perform
the protein synthesis. Such enzymes are building blocks in molecular machineries, endowing them with
their semantic functions. These molecular machineries thus instruct the synthesis of components which
are crucially needed for their own operation, a fact which may be referred to as semantic feedback [15,22].
When the regeneration of a nucleotide sequence by means of the genomic error-correcting code happens
to fail, new genomic information is created so regeneration failures may result in the appearance of new
enzymes with new functions, providing targets to the Darwinian selection and therefore possibly initiating
an evolutive process.
But living things are not unorganized clusters of proteins.
Their organization is determined by
instructions written in sequences of nucleotides outside the genes, such as promoters, regulators, repressors
..., which control the operations of genes and their succession in time. Taken together, they constitute
a genetic regulation network. It exists in prokaryotic cells, but is much more important in eukaryotic
beings, especially in multicellular ones which are far more complex than prokaryotes. The genome in its
entirety, i.e., the genes together with their regulation networks, instructs the assembly of living objects,
which demands using grammatical rules. These rules entail constraints with error-correcting properties,
hence contribute further components to the nested system of error-correcting codes.
New organization rules successively appeared during the geological ages, each endowing the genomes
with new constraints without suppressing those already existing. The necessary increase of the genome
length resulted from “horizontal genetic transfer” means.
Since constraints on genomes endow them
with error-correction ability, they assume the structure of nested codes as described above, which can be
identiﬁed with Barbieri’s “organic codes” [2–5]. The increasing length of genomes enhances both their
information content, which enables specifying more complex beings since, as stated above, the information
quantity borne by a message also measures its semantic speciﬁcity, and their redundancy, which is the key
factor for conserving the information they carry. The increase in information content and in redundancy
can be very inequal. The redundancy increase makes possible improved genomic error-correcting codes,
which entails that longer genomes are favoured by the Darwinian selection. The products of evolution
then trend towards both an increasing complexity and an improved permanence.
Conclusion
For conserving the hereditary information through the ages, genomes must be endowed with codes which
protect them against errors. Error-correcting codes are a byproduct of information theory. As designed
by engineers, they owe their correcting ability to mathematical constraints, but constraints of natural
origin achieve the same protecting function. That very ancient parts of genomes are better conserved
than more recent ones cannot be explained unless assuming that genomic error-correcting codes result
from successive encodings, where an already encoded information is encoded again together with a new
one. This nested code structure may be identiﬁed with Barbieri’s organic codes. It accounts for the
hierarchical taxonomy of the living world, and explains its main features.
Information, a measurable abstract entity, is thus needed for understanding life. Its capital impor-
tance should be recognized. It should therefore be realized that physics and chemistry are not the sole
basic sciences which are needed to account for biological phenomena. Information theory is indeed an
inescapable basis of life science. As yet, it has been ignored by biologists. It should be used for renewing
the foundations of biology. This is an extremely vast, ambitious and potentially fruitful future task. This
goal is worth the eﬀort needed for assimilating information theory and, ﬁrst of all, the very concept of
information in its scientiﬁc meaning, despite its seeming strangeness and its diﬃculty.
Acknowledgements
The constant interest of Marcello Barbieri in my work is very gratefully acknowledged. Thanks are due
to the referees for their careful reading and helpful suggestions.
17

References
[1] Shannon, C.E., “A mathematical theory of communication”, Bell System Technical Journal, Vol. 27,
pp. 370–467 and 623–657, July and Oct. 1948.
[2] Barbieri, M., The organic codes: an introduction to semantic biology. Cambridge: Cambridge Uni-
versity Press, 2003.
[3] Barbieri, M., (Ed.), The codes of life: The rules of macroevolution, Dordrecht: Springer, 2007.
[4] Barbieri, M., “Introduction to code biology”, Biosemiotics, Vol. 7, pp. 167–179, 2014.
[5] Barbieri, M., Code biology. A new science of life. Heidelberg: Springer, 2015.
[6] Gallager, R.G., “A simple derivation of the coding theorem and some applications”, IEEE Trans.
on Information Theory, Vol. IT-13, No. 1, pp. 3–18, Jan. 1965.
[7] Berrou, C., Glavieux, A. & Thitimajshima, P., “Near Shannon limit error-correcting coding and
decoding: turbo-codes,” Proc. of ICC’93 , Geneva, pp. 1064–1070, May 1993.
[8] Gallager, R.G., Low density parity-check codes, Cambridge, MA: MIT Press, 1963.
[9] Elias, P., “Error-free coding”, IRE Trans. Inf. Th., pp. 29–37, 1954.
[10] Fano, R.M., “A heuristic discussion of probabilistic decoding”, IEEE Trans. Inf. Th., Vol. IT–9, No.
1, pp. 64–74, Jan. 1963.
[11] Viterbi, A.J., “Convolutional codes and their performance in communication systems”, IEEE Trans.
Com. Tech., Vol. COM–19, pp. 751–772, Oct. 1971.
[12] Forney G.D. Jr., “The Viterbi algorithm”, Proc. IEEE, Vol. 61, pp. 268–278, 1973.
[13] Battail, G., “Le d´ecodage pond´er´e en tant que proc´ed´e de r´e´evaluation d’une distribution de proba-
bilit´e”, Annales T´el´ecommunic., Vol. 42, No. 9-10, pp. 499–509, Sep.-Oct. 1987.
[14] Martindale, M.Q., and Kourakis, M.J., “Size doesn’t matter”, Nature, vol. 265, pp. 730–731, 1999.
[15] Battail, G., Information and life, Dordrecht: Springer, 2014.
[16] Sangler, F., Loulson, A.R., Friedmann, T., Air, G.M. et al., “Nucleotide sequence of bacteriophage
phiX174 DNA”. Nature, Vol. 399, pp. 687–695, 1977.
[17] Neumann, J. von, Theory of self-reproducing automata, completed by Burks, A.W., Urbana: Uni-
versity of Illinois Press, 1966.
[18] Voss R.F. (1992), “Evolution of long-range fractal correlation and 1/f noise in DNA base sequences”,
Phys Rev. Let., Vol. 68, pp. 3805–3808.
[19] Arn´eodo, A., Y. d’Aubenton-Carafa, B. Audit, E. Bacry, J.F. Muzy and C. Thermes (1998), “Nu-
cleotide composition eﬀects on the long-range correlations in human genes”, European Physical Jour-
nal B, Vol. 1, pp. 259–263.
[20] Audit, B., C. Vaillant, A. Arn´eodo, Y. d’Aubenton-Carafa and C. Thermes (2002), “Long-range
correlation between DNA bending sites: relation to the structure and dynamics of nucleosomes”, J.
Mol. Biol., Vol. 316, pp. 903–918.
[21] Branden, C., and Tooze, J. (1991), Introduction to protein structure, New York and London: Garland.
[22] Battail, G., La vie, jeu de l’information et de la mati`ere, Saarbr¨uck : ´Editions Universitaires Eu-
rop´eennes, 2017.
[23] Forsdyke, D.R., “Are introns in-series error-detecting sequences?”, J. Theor. Biol., Vol. 93, pp.
861–866, 1981.
18

[24] Forsdyke, D.R., “Conservation of stem-loop potential in introns of snake venom phospholipase A2
genes. An application of FORS-D analysis”, Mol. Biol. and Evol., Vol. 12, pp. 1157–1165, 1995.
[25] Maynard Smith, J., “The concept of information in biology”, Philosophy of science, Vol. 67, No. 2,
pp. 177-194, June 2000.
[26] Shannon, C.E., “The bandwagon”, IRE Trans. on Information theory, Vol. 2, March 1956.
[27] Brillouin, L., Vie, mati`ere et observation, Paris: Albin Michel, 1959.
[28] Landauer, R., “The physical nature of information”, Phys. Lett. A, Vol. 217, pp. 188–193, 1996.
[29] Battail, G., “Should genetics get an information-theoretic education?”, IEEE Engineering in
Medicine and Biology Magazine, Vol. 25, No. 1, pp. 34–45, Jan.-Feb. 2006.
[30] Battail, G., An outline of informational genetics, San Rafael, CA: Morgan & Claypool, 2008.
19

