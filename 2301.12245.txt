Supervision Complexity and its Role in Knowledge Distillation
Hrayr Harutyunyan* 1
Ankit Singh Rawat2
Aditya Krishna Menon2
Seungyeon Kim2
Sanjiv Kumar2
1 USC Information Sciences Institute
hrayrhar@usc.edu
2 Google Research NYC
{ankitsrawat,adityakmenon,seungyeonk,sanjivk}@google.com
Abstract
Despite the popularity and efﬁcacy of knowledge distillation, there is limited understanding of why
it helps. In order to study the generalization behavior of a distilled student, we propose a new theoretical
framework that leverages supervision complexity: a measure of alignment between teacher-provided su-
pervision and the student’s neural tangent kernel. The framework highlights a delicate interplay among
the teacher’s accuracy, the student’s margin with respect to the teacher predictions, and the complexity
of the teacher predictions. Speciﬁcally, it provides a rigorous justiﬁcation for the utility of various tech-
niques that are prevalent in the context of distillation, such as early stopping and temperature scaling.
Our analysis further suggests the use of online distillation, where a student receives increasingly more
complex supervision from teachers in different stages of their training. We demonstrate efﬁcacy of online
distillation and validate the theoretical ﬁndings on a range of image classiﬁcation benchmarks and model
architectures.
1
Introduction
Knowledge distillation (KD) [Buciluˇa et al., 2006, Hinton et al., 2015] is a popular method of compress-
ing a large “teacher” model into a more compact “student” model. In its most basic form, this involves
training the student to ﬁt the teacher’s predicted label distribution or soft labels for each sample. There is
strong empirical evidence that distilled students usually perform better than students trained on raw dataset
labels [Hinton et al., 2015, Furlanello et al., 2018, Stanton et al., 2021, Gou et al., 2021]. Multiple works
have devised novel KD procedures that further improve the student model performance (see Gou et al.
[2021] and references therein). Simultaneously, several works have aimed to rigorously formalize why KD
can improve the student model performance. Some prominent observations from this line of work are that
(self-)distillation induces certain favorable optimization biases in the training objective [Phuong and Lam-
pert, 2019, Ji and Zhu, 2020], lowers variance of the objective [Menon et al., 2021, Dao et al., 2021, Ren
et al., 2022], increases regularization towards learning “simpler” functions [Mobahi et al., 2020], transfers
information from different data views [Allen-Zhu and Li, 2020], and scales per-example gradients based on
the teacher’s conﬁdence [Furlanello et al., 2018, Tang et al., 2020].
Despite this remarkable progress, there are still many open problems and unexplained phenomena around
knowledge distillation; to name a few:
*Work done while interning at Google Research NYC.
1
arXiv:2301.12245v1  [cs.LG]  28 Jan 2023

— Why do soft labels (sometimes) help? It is agreed that teacher’s soft predictions carry information about
class similarities [Hinton et al., 2015, Furlanello et al., 2018], and that this softness of predictions has
a regularization effect similar to label smoothing [Yuan et al., 2020]. Nevertheless, KD also works in
binary classiﬁcation settings with limited class similarity information [M¨uller et al., 2020]. How exactly
the softness of teacher predictions (controlled by a temperature parameter) affects the student learning
remains far from well understood.
— The role of capacity gap. There is evidence that when there is a signiﬁcant capacity gap between the
teacher and the student, the distilled model usually falls behind its teacher [Mirzadeh et al., 2020, Cho
and Hariharan, 2019, Stanton et al., 2021]. It is unclear whether this is due to difﬁculties in optimization,
or due to insufﬁcient student capacity.
— What makes a good teacher? Sometimes less accurate models are better teachers [Cho and Hariharan,
2019, Mirzadeh et al., 2020]. Moreover, early stopped or exponentially averaged models are often better
teachers [Ren et al., 2022]. A comprehensive explanation of this remains elusive.
The aforementioned wide range of phenomena suggest that there is a complex interplay between teacher
accuracy, softness of teacher-provided targets, and complexity of the distillation objective.
This paper provides a new theoretically grounded perspective on KD through the lens of supervision com-
plexity. In a nutshell, this quantiﬁes why certain targets (e.g., temperature-scaled teacher probabilities) may
be “easier” for a student model to learn compared to others (e.g., raw one-hot labels), owing to better align-
ment with the student’s neural tangent kernel (NTK) [Jacot et al., 2018, Lee et al., 2019]. In particular, we
provide a novel theoretical analysis (§2, Thm. 3 and 4) of the role of supervision complexity on kernel clas-
siﬁer generalization, and use this to derive a new generalization bound for distillation (Prop. 5). The latter
highlights how student generalization is controlled by a balance of the teacher generalization, the student’s
margin with respect to the teacher predictions, and the complexity of the teacher’s predictions.
Based on the preceding analysis, we establish the conceptual and practical efﬁcacy of a simple online distil-
lation approach (§4), wherein the student is ﬁt to progressively more complex targets, in the form of teacher
predictions at various checkpoints during its training. This method can be seen as guiding the student in the
function space (see Fig. 1), and leads to better generalization compared to ofﬂine distillation. We provide
empirical results on a range of image classiﬁcation benchmarks conﬁrming the value of online distillation,
particularly for students with weak inductive biases.
Beyond practical beneﬁts, the supervision complexity view yields new insights into distillation:
— The role of temperature scaling and early-stopping. Temperature scaling and early-stopping of the
teacher have proven effective for KD. We show that both of these techniques reduce the supervision
complexity, at the expense of also lowering the classiﬁcation margin. Online distillation manages to
smoothly increase teacher complexity, without degrading the margin.
— Teaching a weak student. We show that for students with weak inductive biases, and/or with much less
capacity than the teacher, the ﬁnal teacher predictions are often as complex as dataset labels, particularly
during the early stages of training. In contrast, online distillation allows the supervision complexity to
progressively increase, thus allowing even a weak student to learn.
— NTK and relational transfer. We show that online distillation is highly effective at matching the teacher
and student NTK matrices. This transfers relational knowledge in the form of example-pair similarity,
as opposed to standard distillation which only transfers per-example knowledge.
Problem setting. We focus on classiﬁcation problems from input domain X to d classes. We are given
a training set of n labeled examples {(x1, y1), . . . , (xn, yn)}, with one-hot encoded labels yi ∈{0, 1}d.
2

(a) Ofﬂine distillation
(b) Online distillation
0
50
100
150
200
250
Epochs
0
1
2
3
4
Adjusted supervision
complexity
labels
random labels
offline teacher
online teacher
(c) Supervision complexity
Figure 1: Online vs. online distillation. Figures (a) and (b) illustrate possible teacher and student function
trajectories in ofﬂine and ofﬂine KD. The yellow dotted lines indicate KD. Figure (c) plots adjusted super-
vision complexity of various targets with respect to NTKs at different stages of training (see §4 for more
details).
Typically, a model fθ : X →Rd is trained with the softmax cross-entropy loss:
Lce(fθ) = −1
n
Xn
i=1 y⊤
i log σ(fθ(xi)),
(1)
where σ(·) is the softmax function. In standard KD, given a trained teacher model g : X →Rd that outputs
logits, one trains a student model fθ : X →Rd to ﬁt the teacher predictions. Hinton et al. [2015] propose
the following KD loss:
Lkd-ce(fθ; g, τ) = −τ 2
n
Xn
i=1 σ(g(xi)/τ)⊤log σ(fθ(xi)/τ),
(2)
where temperature τ > 0 controls the softness of teacher predictions. To highlight the effect of KD and
simplify exposition, we assume that the student is not trained with the dataset labels.
2
Supervision complexity and generalization
One apparent difference between standard training and KD (Eq. 1 and 2) is that the latter modiﬁes the targets
that the student attempts to ﬁt. The targets used during distillation ensure a better generalization for the
student; what is the reason for this? Towards answering this question, we present a new perspective on KD
in terms of supervision complexity. To begin, we show how the generalization of a kernel-based classiﬁer
is controlled by a measure of alignment between the target labels and the kernel matrix. We ﬁrst treat
binary kernel-based classiﬁers (Thm. 3), and later extend our analysis to multiclass kernel-based classiﬁers
(Thm. 4). Finally, by leveraging the neural tangent kernel machinery, we discuss the implications of our
analysis for neural classiﬁers in §2.2.
2.1
Supervision complexity controls kernel machine generalization
The notion of supervision complexity is easiest to introduce and study for kernel-based classiﬁers. We
brieﬂy review some necessary background [Scholkopf and Smola, 2001]. Let k : X × X →R be a positive
semideﬁnite kernel deﬁned over an input space X. Any such kernel uniquely determines a reproducing
kernel Hilbert space (RKHS) H of functions from X to R. This RKHS is the completion of the set of
functions of form f(x) = Pm
i=1 αik(xi, x), with xi ∈X, αi ∈R. Any f(x) = Pm
i=1 αik(xi, x) ∈H has
3

(RKHS) norm
∥f∥2
H =
m
X
i=1
m
X
j=1
αiαjk(xi, xj) = α⊤Kα,
(3)
where α = (α1, . . . , αn)⊤and Ki,j = k(xi, xj). Intuitively, ∥f∥2
H measures the smoothness of f, e.g., for
a Gaussian kernel it measures the Fourier spectrum decay of f [Scholkopf and Smola, 2001].
For simplicity, we start with the case of binary classiﬁcation. Suppose {(Xi, Yi)}i∈[n] are n i.i.d. examples
sampled from some probability distribution on X × Y, with Y ⊂R, where positive and negative labels
correspond to distinct classes. Let Ki,j = k(Xi, Xj) denote the kernel matrix, and Y = (Y1, . . . , Yn)⊤be
the concatenation of all training labels.
Deﬁnition 1 (Supervision complexity). The supervision complexity of targets Y1, . . . , Yn with respect to a
kernel k is deﬁned to be Y ⊤K−1Y in cases when K is invertible, and +∞otherwise.
We now establish how supervision complexity controls the smoothness of the optimal kernel classiﬁer.
Consider a classiﬁer obtained by solving a regularized kernel classiﬁcation problem:
f∗∈argmin
f∈H
1
n
Xn
i=1 ℓ(f(Xi), Yi) + λ
2 ∥f∥2
H ,
(4)
where ℓis a loss function and λ > 0. The following proposition shows whenever the supervision complexity
is small, the RKHS norm of any optimal solution f∗will also be small (see Appendix B for a proof). This is
an important learning bias that shall help us explain certain aspects of KD.
Proposition 2. Assume that K is full rank almost surely; ℓ(y, y′) ≥0, ∀y, y′ ∈Y; and ℓ(y, y) = 0, ∀y ∈Y.
Then, with probability 1, for any solution f∗of (4), we have ∥f∗∥2
H ≤Y ⊤K−1Y .
Equipped with the above result, we now show how supervision complexity controls generalization. In the
following, let φγ : R →[0, 1] be the margin loss [Mohri et al., 2018] with scale γ > 0:
φγ(α) =





1
if α ≤0
1 −α/γ
if 0 < α ≤γ
0
if α > γ.
(5)
Theorem 3. Assume that κ = supx∈X k(x, x) < ∞and K is full rank almost surely. Further, assume that
ℓ(y, y′) ≥0, ∀y, y′ ∈Y and ℓ(y, y) = 0, ∀y ∈Y. Let M0 = ⌈γ√n/(2√κ)⌉. Then, with probability at least
1 −δ, for any solution f∗of problem in Eq. (4), we have
PX,Y (Y f∗(X) ≤0) ≤1
n
n
X
i=1
φγ(sign (Yi) f∗(Xi)) + 2
√
Y ⊤K−1Y + 2
γn
p
Tr (K)
+ 3
r
ln (2M0/δ)
2n
.
(6)
The proof is available in Appendix B. One can compare Thm. 3 with the standard Rademacher bound
for kernel classiﬁers [Bartlett and Mendelson, 2002]. The latter typically consider learning over functions
with RKHS norm bounded by a constant M > 0. The corresponding complexity term then decays as
4

O(
p
M · Tr (K) /n), which is data-independent. Consequently, such a bound cannot adapt to the intrinsic
“difﬁculty” of the targets Y . In contrast, Thm. 3 considers functions with RKHS norm bounded by the data-
dependent supervision complexity term. This results in a more informative generalization bound, which
captures the “difﬁculty” of the targets. Here, we note that Arora et al. [2019] characterized the generalization
of an overparameterized two-layer neural network via a term closely related to the supervision complexity
(see §5 for additional discussion).
The supervision complexity Y ⊤K−1Y is small whenever Y is aligned with top eigenvectors of K and/or
Y has small scale. Furthermore, one cannot make the bound close to zero by just reducing the scale of
targets, as one would need a small γ to control the margin loss that would otherwise increase due to student
predictions getting closer to zero (as the student aims to match Yi).
To better understand the role of supervision complexity, it is instructive to consider two special cases that
lead to a poor generalization bound: (1) uninformative features, and (2) uninformative labels.
Complexity under uninformative features. Suppose the kernel matrix K is diagonal, so that the kernel
provides no information on example-pair similarity; i.e., the kernel is “uninformative”. An application of
Cauchy-Schwarz reveals the key expression in the second term in (6) satisﬁes:
1
n
q
Y ⊤K−1Y Tr(K) = 1
n
r Xn
i=1 Y 2
i · k(Xi, Xi)−1
 Xn
i=1 k(Xi, Xi)

≥1
n
n
X
i=1
|Yi|.
Consequently, this term is least constant in order, and does not vanish as n →∞.
Complexity under uninformative labels.
Suppose the labels Yi are purely random, and independent
from inputs Xi. Conditioned on {Xi}, Y ⊤K−1Y concentrates around its mean by the Hanson-Wright
inequality [Vershynin, 2018]. Hence, ∃ϵ(K, δ, n) such that with probability ≥1 −δ, Y ⊤K−1Y
≥
E{Yi}

Y ⊤K−1Y

−ϵ = E

Y 2
1

Tr(K−1) −ϵ. Thus, with the same probability,
1
n
q
Y ⊤K−1Y Tr(K) ≥1
n
q E

Y 2
1

Tr(K−1) −ϵ

Tr(K) ≥1
n
q
E

Y 2
1

n2 −ϵ Tr(K),
where the last inequality is by Cauchy-Schwarz. For sufﬁciently large n, the quantity E

Y 2
1

n2 dominates
ϵ Tr (K), rendering the bound of Thm. 3 close to a constant.
2.2
Extensions: multiclass classiﬁcation and neural networks
We now show that a result similar to Thm. 3 holds for multiclass classiﬁcation as well. In addition, we also
discuss how our results are instructive about the behavior of neural networks.
Extension to multiclass classiﬁcation. Let {(Xi, Yi)}i∈[n] be drawn i.i.d. from a distribution over X ×
Y, where Y ⊂Rd. Let k : X × X →Rd×d be a matrix-valued positive deﬁnite kernel and H be the
corresponding vector-valued RKHS. As in the binary classiﬁcation case, we consider a kernel problem in
Eq. (4). Let Y ⊤= (Y ⊤
1 , . . . , Y ⊤
n ) and K be the kernel matrix of training examples:
K =


k(X1, X1)
· · ·
k(X1, Xn)
· · ·
· · ·
· · ·
k(Xn, X1)
· · ·
k(Xn, Xn)

∈Rnd×nd.
(7)
For f : X →Rd and a labeled example (x, y), let ρf(x, y) = f(x)y −maxy′̸=y f(x)y′ be the prediction
margin. Then, the following analogue of Thm. 3 holds (see Appendix C for the proof).
5

Theorem 4. Assume that κ = supx∈X,y∈[d] k(x, x)y,y < ∞, and K is full rank almost surely. Further,
assume that ℓ(y, y′) ≥0, ∀y, y′ ∈Y and ℓ(y, y) = 0, ∀y ∈Y. Let M0 = ⌈γ√n/(4d√κ)⌉. Then, with
probability at least 1 −δ, for any solution f∗of problem in Eq. (4),
PX,Y (ρf∗(X, Y ) ≤0) ≤1
n
n
X
i=1
1{ρf∗(Xi, Yi) ≤γ} + 4d(Y ⊤K−1Y + 1)
γn
p
Tr (K)
+ 3
r
log(2M0/δ)
2n
.
(8)
Implications for neural classiﬁers. Our analysis has so far focused on kernel-based classiﬁers. While neu-
ral networks are not exactly kernel methods, many aspects of their performance can be understood via a
corresponding linearized neural network (see Ortiz-Jim´enez et al. [2021] and references therein). We follow
this approach, and given a neural network fθ with current weights θ0, we consider the corresponding lin-
earized neural network comprising the linear terms of the Taylor expansion of fθ(x) around θ0 [Jacot et al.,
2018, Lee et al., 2019]:
flin
θ (x) ≜fθ0(x) + ∇θfθ0(x)⊤(θ −θ0).
(9)
Let ω ≜θ −θ0. This network flin
ω (x) is a linear function with respect to the parameters ω, but is generally
non-linear with respect to the input x. Note that ∇θfθ0(x) acts as a feature representation, and induces the
neural tangent kernel (NTK) k0(x, x′) = ∇θfθ0(x)⊤∇θfθ0(x′) ∈Rd×d.
Given a labeled dataset S = {(xi, yi)}i∈[n] and a loss function L(f; S), the dynamics of gradient ﬂow
with learning rate η > 0 for flin
ω can be fully characterized in the function space, and depends only on the
predictions at θ0 and the NTK k0:
˙flin
t (x′) = −η · K0(x′, x1:n) ∇fL(flin
t (x1:n); S),
(10)
where f(x1:n) ∈Rnd denotes the concatenation of predictions on training examples and K0(x′, x1:n) =
∇θfθ0(x′)⊤∇θfθ0(x1:n). Lee et al. [2019] show that as one increases the width of the network or when
(θ −θ0) does not change much during training, the dynamics of the linearized and original neural network
become close.
When fθ is sufﬁciently overparameterized and L is convex with respect to ω, then ωt converges to an
interpolating solution. Furthermore, for the mean squared error objective, the solution has the minimum
Euclidean norm [Gunasekar et al., 2017]. As the Euclidean norm of ω corresponds to norm of flin
ω (x) −
fθ0(x) in the vector-valued RKHS H corresponding to k0, training a linearized network to interpolation is
equivalent to solving the following with a small λ > 0:
h∗= argmin
h∈H
1
n
Xn
i=1(fθ0(xi) + h(xi) −yi)2 + λ
2 ∥h∥2
H .
(11)
Therefore, the generalization bounds of Thm. 3 and 4 apply to h∗with supervision complexity of residual
targets yi−fθ0(xi). However, we are interested in the performance of fθ0 +h∗. As the proofs of these results
rely on bounding the Rademacher complexity of hypothesis sets of form {h ∈H: ∥h∥≤M}, and shifting
a hypothesis set by a constant function does not change the Rademacher complexity (see Remark 7), these
proofs can be easily modiﬁed to handle hypotheses shifted by the constant function fθ0.
6

3
Knowledge distillation: a supervision complexity lens
We now turn to KD, and explore how supervision complexity affects student’s generalization. We show
that student’s generalization depends on three terms: the teacher generalization, the student’s margin with
respect to the teacher predictions, and the complexity of the teacher’s predictions.
3.1
Trade-off between teacher accuracy, margin, and complexity
Consider the binary classiﬁcation setting of §2, and a ﬁxed teacher g : X →R that outputs a logit. Let
{(Xi, Y ∗
i )}i∈[n] be n i.i.d. labeled examples, where Y ∗
i
∈{−1, 1} denotes the ground truth labels. For
temperature τ > 0, let Yi ≜2 σ(g(Xi)/τ) −1 ∈[−1, +1] denote the teacher’s soft predictions, for sigmoid
function σ: z 7→(1+exp(−z))−1. Our key observation is: if the teacher predictions Yi are accurate enough
and have signiﬁcantly lower complexity compared to ground truth labels Y ∗
i , then a student kernel method
(cf. Eq. (4)) trained with Yi can generalize better than the one trained with Y ∗
i . The following result quantiﬁes
the trade-off between teacher accuracy, student prediction margin, and teacher prediction complexity (see
Appendix B.3 for a proof).
Proposition 5. Assume that κ = supx∈X k(x, x) < ∞and K is full rank almost surely, ℓ(y, y′) ≥
0, ∀y, y′ ∈Y, and ℓ(y, y) = 0, ∀y ∈Y. Let Yi and Y ∗
i be deﬁned as above. Let M0 = ⌈γ√n/(2√κ)⌉.
Then, with probability at least 1 −δ, any solution f∗of problem (4) satisﬁes
PX,Y ∗(Y ∗f∗(X) ≤0)
|
{z
}
student risk
≤PX,Y ∗(Y ∗g(X) ≤0)
|
{z
}
teacher risk
+
1
n
Xn
i=1 φγ(sign (Yi) f∗(Xi))
|
{z
}
student’s empirical margin loss w.r.t. teacher predictions
+

2
p
Y ⊤K−1Y + 2
p
Tr (K)/(γn)
|
{z
}
complexity of teacher’s predictions
+3
p
ln (2M0/δ)/(2n).
Note that a similar result is easy to establish for multiclass classiﬁcation using Thm. 4. The ﬁrst term in the
above accounts for the misclassiﬁcation rate of the teacher. While this term is not irreducible (it is possible
for a student to perform better than its teacher), generally a student performs worse that its teacher, especially
when there is a signiﬁcant teacher-student capacity gap. The second term is student’s empirical margin loss
w.r.t. teacher predictions. This captures the price of making teacher predictions too soft. Intuitively, the softer
(i.e., closer to zero) teacher predictions are, the harder it is for the student to learn the classiﬁcation rule.
The third term accounts for the supervision complexity and the margin parameter γ. Thus, one has to choose
γ carefully to achieve a good balance between empirical margin loss and margin-normalized supervision
complexity.
The effect of temperature. For a ﬁxed margin parameter γ > 0, increasing the temperature τ makes
teacher’s predictions Yi softer. On the one hand, the reduced scale decreases the supervision complexity
Y ⊤K−1Y . Moreover, we shall see that in the case of neural networks the complexity decreases even further
due to Y becoming more aligned with top eigenvectors of K. On the other hand, the scale of predictions of
the (possibly interpolating) student f∗will decrease too, increasing the empirical margin loss. This suggests
that setting the value of τ is not trivial: the optimal value can be different based on the kernel k and teacher
logits g(Xi).
7

Table 1: Results on CIFAR-100.
Setting
No KD
Ofﬂine KD
Online KD
Teacher
τ = 1
τ = 4
τ = 1
τ = 4
ResNet-56 →LeNet-5x8
47.3 ± 0.6
50.1 ± 0.4
59.9 ± 0.2
61.9 ± 0.2
66.1 ± 0.4
72.0
ResNet-56 →ResNet-20
67.7 ± 0.5
68.2 ± 0.3
71.6 ± 0.2
69.6 ± 0.3
71.4 ± 0.3
72.0
ResNet-110 →LeNet-5x8
47.2 ± 0.5
48.6 ± 0.8
59.0 ± 0.3
60.8 ± 0.2
65.8 ± 0.2
73.4
ResNet-110 →ResNet-20
67.8 ± 0.3
67.8 ± 0.2
71.2 ± 0.0
69.0 ± 0.3
71.4 ± 0.0
73.4
3.2
From Ofﬂine to Online knowledge distillation
We identiﬁed that supervision complexity plays a key role in determining the efﬁcacy of a distillation proce-
dure. The supervision from a fully trained teacher model can prove to be very complex for a student model
in an early stage of its training (Fig. 1c). This raises the question: is there value in providing progressively
difﬁcult supervision to the student? In this section, we describe a simple online distillation method, where
the the teacher is updated during the student training.
Over the course of their training, neural models learn functions of increasing complexity [Kalimeris et al.,
2019]. This provides a natural way to construct a set of teachers with varying prediction complexities. Simi-
lar to Jin et al. [2019], for practical considerations of not training the teacher and the student simultaneously,
we assume the availability of teacher checkpoints over the course of its training. Given m teacher check-
points at times T = {ti}i∈[m], during the t-th step of distillation, the student receives supervision from the
teacher checkpoint at time min{t′ ∈T : t′ > t}. Note that the student is trained for the same number of
epochs in total as in ofﬂine distillation. Throughout the paper we use the term “online distillation” for this
approach (cf. Algorithm 1 of Appendix A).
Online distillation can be seen as guiding the student network to follow the teacher’s trajectory in function
space (see Fig. 1). Given that NTK can be interpreted as a principled notion of example similarity and
controls which examples affect each other during training [Charpiat et al., 2019], it is desirable for the
student to have an NTK similar to that of its teacher at each time step. To test whether online distillation
also transfers NTK, we propose to measure similarity between the ﬁnal student and ﬁnal teacher NTKs.
For computational efﬁciency we work with NTK matrices corresponding to a batch of b examples (bd × bd
matrices). Explicit computation of even batch NTK matrices can be costly, especially when the number of
classes d is large. We propose to view student and teacher batch NTK matrices (denoted by Kf and Kg
respectively) as operators and measure their similarity by comparing their behavior on random vectors:
sim(Kf, Kg) = Ev∼N(0,I)
 ⟨Kfv, Kgv⟩
∥Kfv∥∥Kgv∥

.
(12)
Note that the cosine distance is used to account for scale differences of Kg and Kf. The kernel-vector prod-
ucts appearing in this similarity measure above can be computed efﬁciently without explicitly constructing
the kernel matrices. For example, Kfv = ∇θfθ(x1:b)⊤(∇θfθ(x1:b)v) can be computed with one vector-
Jacobian product followed by a Jacobian-vector product. The former can be computed efﬁciently using
backpropagation, while the latter can be computed efﬁciently using forward-mode differentiation.
8

Table 2: Results on Tiny ImageNet.
Setting
No KD
Ofﬂine KD
Online KD
Teacher
τ = 1
τ = 2
τ = 1
τ = 2
MobileNet-V3-125 →MobileNet-V3-35
58.5 ± 0.2
59.2 ± 0.1
60.2 ± 0.2
60.7 ± 0.2
62.3 ± 0.3
62.7
ResNet-101 →MobileNet-V3-35
58.5 ± 0.2
59.4 ± 0.5
61.6 ± 0.2
61.1 ± 0.3
62.0 ± 0.3
66.0
MobileNet-V3-125 →VGG-16
48.9 ± 0.3
54.1 ± 0.4
59.4 ± 0.4
58.9 ± 0.7
62.3 ± 0.3
62.7
ResNet-101 →VGG-16
48.6 ± 0.4
53.1 ± 0.4
60.6 ± 0.2
60.4 ± 0.2
64.0 ± 0.1
66.0
4
Experimental results
We now present experimental results to showcase the importance of supervision complexity in distillation,
and to establish efﬁcacy of online distillation.
4.1
Experimental setup
We consider standard image classiﬁcation benchmarks: CIFAR-10, CIFAR-100, and Tiny-ImageNet. Ad-
ditionally, we derive a binary classiﬁcation task from CIFAR-100 by grouping the ﬁrst and last 50 classes
into two meta-classes. We consider teacher and student architectures that are ResNets [He et al., 2016],
VGGs [Simonyan and Zisserman, 2015], and MobileNets [Howard et al., 2019] of various depths. As a stu-
dent architecture with relatively weaker inductive biases, we also consider the LeNet-5 [LeCun et al., 1998]
with 8 times wider hidden layers. We use standard hyperparameters (Appendix A) to train these models.
We compare (1) regular one-hot training (without any distillation), (2) regular ofﬂine distillation using the
temperature-scaled softmax cross-entropy, and (3) online distillation using the same loss. For CIFAR-10 and
binary CIFAR-100, we also consider training with mean-squared error (MSE) loss and its corresponding KD
loss:
Lmse(fθ) = 1
2n
n
X
i=1
∥yi −fθ(xi)∥2
2 ,
Lkd-mse(fθ; g, τ) = τ
2n
n
X
i=1
∥σ(g(xi)/τ) −fθ(xi)∥2
2 .
(13)
The MSE loss allows for interpolation in case of one-hot labels yi, making it amenable to the analysis in
§2 and §3. Moreover, Hui and Belkin [2021] show that under standard training, the CE and MSE losses
perform similarly; as we shall see, the same is true for distillation as well.
As mentioned in Sec. 1, in all KD experiments student networks receive supervision only through a knowl-
edge distillation loss (i.e., dataset labels are not used). This choice help us decrease differences between the
theory and experiments. Furthermore, in our preliminary experiments we observed that this choice does not
result in student performance degradation (see Appendix D).
4.2
Results and discussion
Tables 1, 2, 3 and Table 5 (Appendix D) present the results (mean and standard deviation of test accuracy
over 3 random trials). First, we see that online distillation with proper temperature scaling typically yields
the most accurate student. The gains over regular distillation are particularly pronounced when there is
a large teacher-student gap. For example, on CIFAR-100, ResNet to LeNet distillation with temperature
scaling appears to hit a limit of ∼60% accuracy. Online distillation however manages to further increase
accuracy by +6%, which is a ∼20% increase compared to standard training. Second, the similar results on
9

Table 3: Results on binary CIFAR-100. Every second line is an MSE student.
Setting
No KD
Ofﬂine KD
Online KD
Teacher
τ = 1
τ = 4
τ = 1
τ = 4
ResNet-56 →LeNet-5x8
71.5 ± 0.2
72.4 ± 0.1
73.6 ± 0.2
74.7 ± 0.2
76.1 ± 0.2
77.9
ResNet-56 →LeNet-5x8
71.5 ± 0.4
71.9 ± 0.3
73.0 ± 0.3
75.1 ± 0.3
75.1 ± 0.1
77.9
ResNet-56 →ResNet-20
75.8 ± 0.5
76.1 ± 0.2
77.1 ± 0.6
77.8 ± 0.3
78.1 ± 0.1
77.9
ResNet-56 →ResNet-20
76.1 ± 0.5
76.0 ± 0.2
77.4 ± 0.3
78.0 ± 0.2
78.4 ± 0.3
77.9
ResNet-110 →LeNet-5x8
71.4 ± 0.4
71.9 ± 0.1
72.9 ± 0.3
74.3 ± 0.3
75.4 ± 0.3
78.4
ResNet-110 →LeNet-5x8
71.6 ± 0.2
71.5 ± 0.4
72.6 ± 0.4
74.8 ± 0.4
74.6 ± 0.2
78.4
ResNet-110 →ResNet-20
76.0 ± 0.3
76.0 ± 0.2
77.0 ± 0.1
77.3 ± 0.2
78.0 ± 0.4
78.4
ResNet-110 →ResNet-20
76.1 ± 0.2
76.4 ± 0.3
77.6 ± 0.3
77.9 ± 0.2
78.1 ± 0.1
78.4
binary CIFAR-100 shows that “dark knowledge” in the form of membership information in multiple classes
is not necessary for distillation to succeed.
The results also demonstrate that knowledge distillation with the MSE loss of (13) has a qualita-
tively similar behavior to KD with CE objective. We use these MSE models to highlight the role
of supervision complexity. As an instructive case, we consider a LeNet-5x8 network trained on bi-
nary CIFAR-100 with the standard MSE loss function. For a given checkpoint of this network and a
given set of m labeled (test) examples {(Xi, Yi)}i∈[m], we compute the adjusted supervision complexity
1/n
p
(Y −f(X1:m))⊤K−1(Y −f(X1:m)) · Tr (K), where f denotes the current prediction function, and
K is derived from the current NTK. Note that the subtraction of initial predictions is the appropriate way to
measure complexity given the form of the optimization problem (11). As the training NTK matrix becomes
aligned with dataset labels during training (see Baratin et al. [2021] and Fig. 7 of Appendix D), we pick
{Xi}i∈m to be a set of 212 test examples.
Comparison of supervision complexities. We compare the adjusted supervision complexities of random
labels, dataset labels, and predictions of an ofﬂine and online ResNet-56 teacher predictions with respect
to various checkpoints of the LeNet-5x8 network. The results presented in Fig. 1c indicate that the dataset
labels and ofﬂine teacher predictions are as complex as random labels in the beginning. After some initial
decline, the complexity of these targets increases as the network starts to overﬁt. Given the lower bound on
the supervision complexity of random labels (see §2), this increase means that the NTK spectrum becomes
less uniform (see Fig. 6 of Appendix D).
In contrast to these static targets, the complexity of the online teacher predictions smoothly increases, and
is signiﬁcantly smaller for most of the epochs. To account for softness differences of the various targets, we
consider plotting the adjusted supervision complexity normalized by the target norm √m ∥Y ∥2. As shown
in Fig. 2a, the normalized complexity of ofﬂine and online teacher predictions is smaller compared to the
dataset labels, indicating a better alignment with top eigenvectors of the LeNet NTK. Importantly, we see
that the predictions of an online teacher have signiﬁcantly lower normalized complexity in the critical early
stages of training. Similar observations hold when complexities are measured with respect to a ResNet-20
network (see Appendix D).
Average teacher complexity. Ren et al. [2022] observed that teacher predictions ﬂuctuate over time, and
showed that using exponentially averaged teachers improves knowledge distillation. Fig. 2c demonstrates
that the supervision complexity of an online teacher predictions is always slightly larger than that of the
10

0
50
100
150
200
250
Epochs
1
2
3
4
Adjusted supervision
complexity over target norm
labels
random labels
offline teacher
online teacher
(a) Normalized complexity
0
5
10
15
20
Temperature
0.4
0.6
0.8
1.0
1.2
Adjusted supervision
complexity
0.8
1.0
1.2
1.4
Adjusted supervision
complexity over target norm
(b) Temperature effect
0
50
100
150
200
250
Epochs
0.5
1.0
1.5
2.0
Adjusted supervision
complexity
averaged teacher
current teacher
(c) Teacher averaging
Figure 2: Supervision complexity for various targets. On the left: Normalized adjusted supervision complex-
ities of various targets with respect to a LeNet-5x8 network at different stages of its training. In the middle:
The effect of temperature on the supervision complexity of an ofﬂine teacher for a LeNet-5x8 after training
for 25 epochs. On the right: The effect of averaging teacher predictions.
50
55
60
65
Test accuracy
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Train NTK similarity
correlation: 0.93
No KD
offline KD
online KD
(a) LeNet-5x8 student
68
69
70
71
Test accuracy
0.40
0.45
0.50
0.55
Train NTK similarity
correlation: 0.87
No KD
offline KD
online KD
(b) ResNet-20 student
72
74
76
Test accuracy
0.980
0.982
0.984
0.986
0.988
0.990
0.992
0.994
Train fidelity
correlation: -0.57
No KD
offline KD
online KD
(c) LeNet-5x8 student
Figure 3: Relationship between test accuracy, train NTK similarity, and train ﬁdelity for CIFAR-100 students
training with either ResNet-56 teacher (panels (a) and (c)) or ResNet-110 (panel (b)).
average of predictions of teachers of the last 10 preceding epochs.
Effect of temperature scaling. As discussed earlier, higher temperature makes the teacher predictions
softer, decreasing their norm. This has a large effect on supervision complexity (Fig. 2b). Even when one
controls for the norm of the predictions, the complexity still decreases (Fig. 2b).
NTK similarity. Remarkably, we observe that across all of our experiments, the ﬁnal test accuracy of the
student is strongly correlated with the similarity of ﬁnal teacher and student NTKs (see Figures 3,9, and 10).
This cannot be explained by better matching the teacher predictions. In fact, we see that the ﬁnal ﬁdelity (the
rate of classiﬁcation agreement of a teacher-student pair) measured on training set has no clear relationship
with test accuracy. Furthermore, we see that online KD results in better NTK transfer without an explicit
regularization loss enforcing such transfer.
5
Related work
Due to space constraints, we brieﬂy discuss the existing works that are the most closely related to our
exploration in this paper (see Appendix E for a more comprehensive account of related work.)
Supervision complexity. The key quantity in our work is supervision complexity Y ⊤K−1Y . Cristianini
11

et al. [2001] introduced a related quantity Y ⊤KY called kernel-target alignment and derived a general-
ization bound with it for expected Parzen window classiﬁers. Deshpande et al. [2021] use kernel-target
alignment for model selection in transfer learning. Ortiz-Jim´enez et al. [2021] demonstrate that when NTK-
target alignment is high, learning is faster and generalizes better. Arora et al. [2019] prove a generalization
bound for overparameterized two-layer neural networks with NTK parameterization, trained with gradient
ﬂow. Their bound is roughly
 Y ⊤(K∞)−1Y
1/2/√n, where K∞is the expected NTK matrix at a random
initialization. Our bound of Thm. 3 can be seen as a generalization of this result for all kernel methods,
including linearized neural networks of any depth and sufﬁcient width, with the only difference of using the
empirical NTK matrix. Belkin et al. [2018] warns that bounds based on RKHS complexity of the learned
function can fail to explain the good generalization of kernel methods under label noise.
Mobahi et al. [2020] prove that for kernel methods with RKHS norm regularization, self-distillation in-
creases regularization strength. Phuong and Lampert [2019], while studying self-distillation of deep linear
networks, derive a bound of the transfer risk that depends on the distribution of the acute angle between
teacher parameters and data points. This is in spirit related to supervision complexity as it measures an
“alignment ” between the distillation objective and data. Ji and Zhu [2020] extend this results to linearized
neural networks, showing that ∆⊤
z K−1∆z, where ∆z is the logit change during training, plays a key role in
estimating the bound. Their bound is qualitatively different than ours, and ∆⊤
z K−1∆z becomes ill-deﬁned
for hard labels.
Non-static teachers. Multiple works consider various approaches that train multiple students simultane-
ously to distill either from each other or from an ensemble [see, e.g., Zhang et al., 2018, Anil et al., 2018,
Guo et al., 2020]. Zhou et al. [2018] and Shi et al. [2021] train teacher and student together while having
a common architecture trunk and regularizing teacher predictions to close that of students, respectively. Jin
et al. [2019] study route constrained optimization which is closest to the online distillation in §3.2. They
employ a few teacher checkpoints to perform a multi-round KD. We complement this line of work by high-
lighting the role of supervision complexity and by demonstrating that online distillation can be very powerful
for students with weak inductive biases.
6
Conclusion and future work
We presented a treatment of knowledge distillation through the lens of supervision complexity. We formal-
ized how the student generalization is controlled by three key quantities: the teacher’s accuracy, the student’s
margin with respect to the teacher labels, and the supervision complexity of the teacher labels under the stu-
dent’s kernel. This motivated an online distillation procedure that gradually increases the complexity of the
targets that the student ﬁts.
There are several potential directions for future work. Adaptive temperature scaling for online distillation,
where the teacher predictions are smoothened so as to ensure low target complexity, is one such direction.
Another avenue is to explore alternative ways to smoothen teacher prediction besides temperature scaling;
e.g., can one perform sample-dependent scaling? There is large potential for improving online KD by mak-
ing more informed choices for the frequency and positions of teacher checkpoints, and controlling how
much the student is trained in between teacher updates. Finally, while we demonstrated that online distilla-
tion results in a better alignment with the teacher’s NTK matrix, understanding why this happens is an open
and interesting problem.
12

References
Zeyuan Allen-Zhu and Yuanzhi Li.
Towards understanding ensemble, knowledge distillation and self-
distillation in deep learning. CoRR, abs/2012.09816, 2020.
Rohan Anil, Gabriel Pereyra, Alexandre Passos, R´obert Orm´andi, George E. Dahl, and Geoffrey E. Hinton.
Large scale distributed neural network training through online distillation. CoRR, abs/1804.03235, 2018.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization
and generalization for overparameterized two-layer neural networks.
In International Conference on
Machine Learning, pages 322–332. PMLR, 2019.
Aristide Baratin, Thomas George, C´esar Laurent, R Devon Hjelm, Guillaume Lajoie, Pascal Vincent, and
Simon Lacoste-Julien. Implicit regularization via neural feature alignment. In International Conference
on Artiﬁcial Intelligence and Statistics, pages 2269–2277. PMLR, 2021.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel
learning. In International Conference on Machine Learning, pages 541–549. PMLR, 2018.
Lucas Beyer, Xiaohua Zhai, Am´elie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov.
Knowledge distillation: A good teacher is patient and consistent. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages 10925–10934, 2022.
Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of
the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’06,
page 535–541, New York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595933395.
doi: 10.1145/1150402.1150464.
Guillaume Charpiat, Nicolas Girard, Loris Felardos, and Yuliya Tarabalka. Input similarity from the neural
network perspective. Advances in Neural Information Processing Systems, 32, 2019.
Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun Chen. Online knowledge distillation with
diverse peers. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 3430–
3437, 2020.
Defang Chen, Jian-Ping Mei, Hailin Zhang, Can Wang, Yan Feng, and Chun Chen. Knowledge distillation
with the reused teacher classiﬁer. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 11933–11942, 2022.
Jang Hyun Cho and Bharath Hariharan. On the efﬁcacy of knowledge distillation. In Proceedings of the
IEEE/CVF international conference on computer vision, pages 4794–4802, 2019.
Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz Kandola. On kernel-target alignment. Ad-
vances in neural information processing systems, 14, 2001.
Tri Dao, Govinda M Kamath, Vasilis Syrgkanis, and Lester Mackey. Knowledge distillation as semipara-
metric inference. In International Conference on Learning Representations, 2021.
Aditya Deshpande, Alessandro Achille, Avinash Ravichandran, Hao Li, Luca Zancato, Charless Fowlkes,
Rahul Bhotika, Stefano Soatto, and Pietro Perona. A linearized framework and a new benchmark for
model selection for ﬁne-tuning. arXiv preprint arXiv:2102.00084, 2021.
13

Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born
again neural networks. In International Conference on Machine Learning, pages 1607–1616. PMLR,
2018.
Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey.
International Journal of Computer Vision, 129(6):1789–1819, 2021.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit
regularization in matrix factorization. Advances in Neural Information Processing Systems, 30, 2017.
Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin Hu, and Ping Luo. Online
knowledge distillation via collaborative learning. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 11020–11029, 2020.
Bobby He and Mete Ozay. Feature kernel distillation. In International Conference on Learning Represen-
tations, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2(7), 2015.
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,
Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the
IEEE/CVF international conference on computer vision, pages 1314–1324, 2019.
Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy
in classiﬁcation tasks. In International Conference on Learning Representations, 2021.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generaliza-
tion in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 8571–8580. Curran
Associates, Inc., 2018.
Guangda Ji and Zhanxing Zhu. Knowledge distillation in wide neural networks: Risk bound, data efﬁciency
and imperfect teacher. Advances in Neural Information Processing Systems, 33:20823–20833, 2020.
Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, and Xiaolin Hu. Knowl-
edge distillation via route constrained optimization. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV), October 2019.
Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and
Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in Neural
Information Processing Systems, 32, 2019.
Vitaly Kuznetsov, Mehryar Mohri, and U Syed. Rademacher complexity margin bounds for learning with
a large number of classes. In ICML Workshop on Extreme Classiﬁcation: Learning with a Very Large
Number of Labels, volume 2, 2015.
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
14

Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes, vol-
ume 23. Springer Science & Business Media, 1991.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
In Advances in neural information processing systems, pages 8572–8583, 2019.
Aditya K Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv Kumar. A statistical
perspective on distillation. In International Conference on Machine Learning, pages 7632–7642. PMLR,
2021.
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan
Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI confer-
ence on artiﬁcial intelligence, volume 34, pages 5191–5198, 2020.
Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation ampliﬁes regularization in hilbert
space. Advances in Neural Information Processing Systems, 33:3351–3361, 2020.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press,
2018.
Rafael M¨uller, Simon Kornblith, and Geoffrey E. Hinton. Subclass distillation. CoRR, abs/2002.03936,
2020.
Guillermo Ortiz-Jim´enez, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. What can linearized neu-
ral networks actually say about generalization? Advances in Neural Information Processing Systems, 34:
8998–9010, 2021.
Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3967–3976, 2019.
Nikolaos Passalis and Anastasios Tefas. Learning deep representations with probabilistic knowledge trans-
fer. In Proceedings of the European Conference on Computer Vision (ECCV), pages 268–284, 2018.
Mary Phuong and Christoph Lampert. Towards understanding knowledge distillation. In Kamalika Chaud-
huri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pages 5142–5151. PMLR, 09–15
Jun 2019.
Yi Ren, Shangmin Guo, and Danica J. Sutherland. Better supervisory signals by observing learning paths.
In International Conference on Learning Representations, 2022.
Mehdi Rezagholizadeh, Aref Jafari, Puneeth S.M. Saladi, Pranav Sharma, Ali Saheb Pasand, and Ali Ghodsi.
Pro-KD: Progressive distillation by following the footsteps of the teacher. In Proceedings of the 29th
International Conference on Computational Linguistics, pages 4714–4727, 2022.
Adriana Romero, Samira Ebrahimi Kahou, Polytechnique Montr´eal, Y. Bengio, Universit´e De Montr´eal,
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. Fitnets: Hints for thin deep nets. In in International Conference on Learning Representations
(ICLR, 2015.
Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines, Regular-
ization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA, 2001. ISBN 0262194759.
15

Wenxian Shi, Yuxuan Song, Hao Zhou, Bohan Li, and Lei Li. Follow your path: A progressive method for
knowledge distillation. In Nuria Oliver, Fernando P´erez-Cruz, Stefan Kramer, Jesse Read, and Jose A.
Lozano, editors, Machine Learning and Knowledge Discovery in Databases. Research Track, pages 596–
611, Cham, 2021. Springer International Publishing. ISBN 978-3-030-86523-8.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representa-
tions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A Alemi, and Andrew G Wilson. Does
knowledge distillation really work? Advances in Neural Information Processing Systems, 34:6906–6919,
2021.
Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi, and Sagar Jain. Understanding
and improving knowledge distillation. arXiv preprint arXiv:2002.03532, 2020.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In International
Conference on Learning Representations, 2020.
Frederick Tung and Greg Mori.
Similarity-preserving knowledge distillation.
In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 1365–1374, 2019.
Roman Vershynin.
High-Dimensional Probability: An Introduction with Applications in Data Science.
Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. doi:
10.1017/9781108231596.
Chenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille. Snapshot distillation: Teacher-student optimiza-
tion in one generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 2859–2868, 2019.
Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label
smoothing regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 3903–3911, 2020.
Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance
of convolutional neural networks via attention transfer. In International Conference on Learning Repre-
sentations, 2017.
Ruixiang Zhang, Shuangfei Zhai, Etai Littwin, and Joshua M. Susskind. Learning representation from neural
ﬁsher kernel with low-rank approximation. In International Conference on Learning Representations,
2022.
Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 4320–4328, 2018.
Guorui Zhou, Ying Fan, Runpeng Cui, Weijie Bian, Xiaoqiang Zhu, and Kun Gai. Rocket launching: A
universal and efﬁcient framework for training well-performing light net. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 32, 2018.
16

Algorithm 1 Online knowledge distillation.
Require: Training sample S; teacher checkpoints {g(t1), . . . , g(tm)}; temperature τ > 0; training steps T;
minibatch size b
1: for t = 1, . . . , T do
2:
Draw random b-sized minibatch S′ from S
3:
Compute nearest teacher checkpoint t∗= min{i ∈[m]: ti > t}
4:
Update student θ ←θ −ηt · ∇θLkd−ce(fθ; g(t∗), τ) over S′
5: end for
6: return fθ
A
Hyperparameters and implementation details
Dataset
Model
Learning rate
CIFAR-10, CIFAR-100, binary CIFAR-100
ResNet-56 (teacher)
0.1
ResNet-110 (teacher)
0.1
ResNet-20 (CE or MSE students)
0.1
LeNet-5x8 (CE or MSE students)
0.04
Tiny ImageNet
MobileNet-V3-125 (teacher)
0.04
ResNet-101 (teacher)
0.1
MobileNet-V3-35 (student)
0.04
VGG-16 (student)
0.01
Table 4: Initial learning rates for different dataset and model pairs.
In all experiments we use stochastic gradient descent optimizer with 128 batch size and 0.9 Nesterov mo-
mentum. The starting learning rates are presented in Table 4. All models for CIFAR datasets are trained for
256 epochs, with a learning schedule that divides the learning rate by 10 at epochs 96, 192, and 224. All
models for Tiny ImageNet are trained for 200 epochs, with a learning rate schedule that divides the learning
rate by 10 at epochs 75 and 135. The learning rate is warmed-up linearly to its initial value in the ﬁrst 10 and
5 epochs for CIFAR and Tiny ImageNet models respectively. All VGG and ResNet models use 2e-4 weight
decay, while MobileNet models use 1e-5 weight decay.
The LeNet-5 uses ReLU activations. We use the CIFAR variants of ResNets in experiments with CIFAR-
10 or (binary) CIFAR-100 datasets. Tiny ImageNet examples are resized to 224x224 resolution to suit the
original ResNet, VGG and MobileNet architectures. In all experiments we use standard data augmentation
– random cropping and random horizontal ﬂip. In all online learning methods we consider one teacher
checkpoint per epoch.
B
Proofs
In this appendix we present the deferred proofs. We use the following deﬁnition of Rademacher complex-
ity [Mohri et al., 2018].
Deﬁnition 6 (Rademacher complexity). Let G be a family of functions from Z to R, and Z1, . . . , Zn be n
i.i.d. examples from a distribution P on Z. Then, the empirical Rademacher complexity of G with respect to
17

(Z1, . . . , Zn) is deﬁned as
bRn(G) = Eσ1,...,σn
"
sup
g∈G
1
m
n
X
i=1
σig(Zi)
#
,
(14)
where σi are independent Rademacher random variables (i.e., uniform random variables taking values in
{−1, 1}). The Rademacher complexity of G is then deﬁned as
Rn(G) = EZ1,...,Zn
h
bRn(G)
i
.
(15)
Remark 7. Shifting the hypothesis class G by a constant function does not change the empirical Rademacher
complexity:
bRn ({f + g : g ∈G}) = Eσ1,...,σn
"
sup
g∈G
1
m
n
X
i=1
σi (f(Zi) + g(Zi))
#
(16)
= Eσ1,...,σn
"
1
m
n
X
i=1
σif(Zi) + sup
g∈G
1
m
n
X
i=1
σig(Zi)
#
(17)
= Eσ1,...,σn
"
sup
g∈G
1
m
n
X
i=1
σig(Zi)
#
= bRn(G).
(18)
Given the kernel classiﬁcation setting described in Sec. 2.1, we ﬁrst prove a slightly more general variant of
a classical generalization gap bound in Bartlett and Mendelson [2002, Theorem 21].
Theorem 8. Assume supx∈X k(x, x) < ∞. Fix any constant M > 0. Then with probability at least 1 −δ,
every function f ∈H with ∥f∥H ≤M satisﬁes
PX,Y (Y f(X) ≤0) ≤1
n
n
X
i=1
φγ(sign (Yi) f(Xi)) + 2M
γn
p
Tr (K) + 3
r
ln (2/δ)
2n
.
(19)
Proof. Let F = {f ∈H : ∥f∥≤M} and consider the following class of functions:
G = {(x, y) 7→φγ(sign(y)f(x)) : f ∈F} .
(20)
By the standard Rademacher complexity classiﬁcation generalization bound [Mohri et al., 2018, Theorem
3.3], for any δ > 0, with probability at least 1 −δ, the following holds for all f ∈F:
EX,Y [φγ(sign(Y )f(X))] ≤1
n
n
X
i=1
φγ(sign(Yi)f(Xi)) + 2bRn(G) + 3
r
log(2/δ)
2n
.
(21)
Therefore, with probability at least 1 −δ, for all f ∈F
PX,Y (Y f(X) ≤0) ≤1
n
n
X
i=1
φγ(sign(Yi)f(Xi)) + 2bRn(G) + 3
r
log(2/δ)
2n
.
(22)
To ﬁnish the proof, we upper bound bRn(G):
bRn(G) = Eσ1,...,σn
"
sup
g∈G
1
n
n
X
i=1
σig(Xi, Yi)
#
(23)
18

= Eσ1,...,σn
"
sup
f∈F
1
n
n
X
i=1
σiφγ (sign(Yi)f(Xi))
#
(24)
≤1
γ Eσ1,...,σn
"
sup
f∈F
1
n
n
X
i=1
σi sign(Yi)f(Xi)
#
(25)
= 1
γ Eσ1,...,σn
"
sup
f∈F
1
n
n
X
i=1
σif(Xi)
#
(26)
= 1
γ
bRn(F),
(27)
where the third line is due to Ledoux and Talagrand [1991]. By Lemma 22 of Bartlett and Mendelson [2002],
we thus conclude that
bRn(F) ≤M
n
p
Tr (K).
(28)
B.1
Proof of Proposition 2
Proof of Proposition 2. As K is a full rank matrix almost surely, then with probability 1 there exists a
vector α ∈Rn, such that Kα = Y . Consider the function f(x) = Pn
i=1 αik(Xi, x) ∈H. Clearly,
f(Xi) = Yi, ∀i ∈[n]. Furthermore, ∥f∥2
H = α⊤Kα = Y ⊤K−1Y . The existence of such f ∈H with
zero empirical loss and the assumptions on the loss function imply that any optimal solution of problem (4)
has a norm at most Y ⊤K−1Y .1
B.2
Proof of Thm. 3
Proof. To get a generalization bound for f∗it is tempting to use Thm. 8 with M = ∥f∗∥. However, ∥f∗∥is
a random variable depending on the training data and is an invalid choice for the constant M. This issue can
be resolved by paying a small logarithmic penalty.
For any M ≥M0 =
l
γ√n
2√κ
m
the bound of Thm. 8 is vacuous. Let us consider the set of integers M =
{1, 2, . . . , M0} and write Thm. 8 for each element of M with δ/M0 failure probability. By union bound, we
have that with probability at least 1−δ, all instances of Thm. 8 with M chosen from M hold simultaneously.
If Y ⊤K−1Y ≥M0, then the desired bound holds trivially, as the right-hand side becomes at least 1.
Otherwise, we set M =
l√
Y ⊤K−1Y
m
∈M and consider the corresponding part of the union bound. We
thus have that with at least 1 −δ probability, every function f ∈F with ∥f∥≤M satisﬁes
PX,Y (Y f(X) ≤0) ≤1
n
n
X
i=1
φγ(sign (Yi) f(Xi)) + 2M
γn
p
Tr (K) + 3
r
ln (2M0/δ)
2n
.
As by Prop. 2 any optimal solution f∗has norm at most
√
Y ⊤K−1Y and M ≤
√
Y ⊤K−1Y + 1, we have
with probability at least 1 −δ,
PX,Y (Y f∗(X) ≤0) ≤1
n
n
X
i=1
φγ(sign (Yi)f∗(Xi)) + 2
√
Y ⊤K−1Y + 2
γn
p
Tr (K) + 3
r
ln (2M0/δ)
2n
.
1For [0, 1]-bounded loss functions, it also holds that ∥f ∗∥2
H ≤2/λ. This is not of direct relevance to us, as we will be interested
in cases with small λ > 0.
19

B.3
Proof of Prop. 5
Prop. 5 is a simple corollary of Thm. 3.
Proof. We have that
PX,Y ∗(Y ∗f∗(X) ≤0) = PX,Y ∗(Y ∗f∗(X) ≤0 ∧Y ∗g(X) ≤0)
+ PX,Y ∗(Y ∗f∗(X) ≤0 ∧Y ∗g(X) > 0)
≤PX,Y ∗(Y ∗g(X) ≤0) + PX(g(X)f∗(X) ≤0).
The rest follows from bounding PX(g(X)f∗(X) ≤0) using Thm. 3.
C
Extension to multiclass classiﬁcation
Let us now consider the case of multiclass classiﬁcation with d classes. Let k : X ×X →Rd×d be a matrix-
valued positive deﬁnite kernel. For every x ∈X and a ∈Rd, let kxa = k(·, x)a be the function from X to
Rd deﬁned the following way:
kxa(x′) = k(x′, x)a, for all x′ ∈X.
(29)
With any such kernel k there is a unique vector-valued RKHS H of functions from X to Rd. This RKHS is
the completion of span

kxa : x ∈X, a ∈Rd	
, with the following inner product:
* n
X
i=1
kxiai,
m
X
j=1
kx′
ja′
j
+
H
=
n
X
i=1
m
X
j=1
a⊤
i k(xi, x′
j)a′
j.
(30)
For any f ∈H, the norm ∥f∥H is deﬁned as
p
⟨f, f⟩H. Therefore, if f(x) = Pn
i=1 kxiai then
⟨f, f⟩2
H =
n
X
i,j=1
a⊤
i k(xi, xj)aj
(31)
= a⊤Ka,
(32)
where a⊤= (a⊤
1 , . . . , a⊤
n ) ∈Rnd and
K =


k(x1, x1)
· · ·
k(x1, xn)
· · ·
· · ·
· · ·
k(xn, x1)
· · ·
k(xn, xn)

∈Rnd×nd.
(33)
Suppose {(Xi, Yi)}i∈[n] are n i.i.d. examples sampled from some probability distribution on X × Y, where
Y ⊂Rd. As in the binary classiﬁcation case, we consider the regularized kernel problem (4). Let Y ⊤=
(Y ⊤
1 , . . . , Y ⊤
n ) be the concatenation of targets. The following proposition is the analog of Prop. 2 in this
vector-valued setting.
20

Proposition 9. Assume K is full rank almost surely. Assume also ℓ(y, y′) ≥0, ∀y, y′ ∈Y, and ℓ(y, y) =
0, ∀y ∈Y. Then, with probability 1, for any solution f∗of (4), we have that
∥f∗∥2
H ≤Y ⊤K−1Y .
(34)
Proof. With probability 1, the kernel matrix K is full rank. Therefore, there exists a vector a⊤=
(a⊤
1 , . . . , a⊤
n ) ∈Rnd, with ai ∈Rd, such that Ka = Y . Consider the function f(x) = Pn
i=1 kXiai ∈H.
Clearly, f(Xi) = Yi, ∀i ∈[n]. Furthermore,
∥f∥2
H = a⊤Ka
(35)
= Y ⊤K−1Y .
(36)
The existence of such f(x) ∈H with zero empirical loss and assumptions on the loss function imply that
any optimal solution of problem (4) has a norm at most Y ⊤K−1Y .
As in the main text, for a hypothesis f : X →Rd and a labeled example (x, y), let ρf(x, y) = f(x)y −
maxy′̸=y f(x)y′ be the prediction margin. We now restate Thm. 4 and present a proof.
Theorem 10 (Thm. 4 restated). Assume that κ = supx∈X,y∈[d] k(x, x)y,y < ∞, and K is full rank almost
surely. Further, assume that ℓ(y, y′) ≥0, ∀y, y′ ∈Y and ℓ(y, y) = 0, ∀y ∈Y. Let M0 = ⌈γ√n/(4d√κ)⌉.
Then, with probability at least 1 −δ, for any solution f∗of problem in Eq. (4),
PX,Y (ρf∗(X, Y ) ≤0) ≤1
n
n
X
i=1
1{ρf∗(Xi, Yi) ≤γ} + 4d(Y ⊤K−1Y + 1)
γn
p
Tr (K)
+ 3
r
log(2M0/δ)
2n
.
(37)
Proof. Consider the class of functions F = {f ∈H : ∥f∥≤M} for some M > 0. By Theorem 2 of
Kuznetsov et al. [2015], for any γ > 0 and δ > 0, with probability at least 1 −δ, the following bound holds
for all f ∈F:2
PX,Y (ρf(X, Y ) ≤0) ≤1
n
n
X
i=1
1{ρf(Xi, Yi) ≤γ} + 4d
γ
bRn( ˜F) + 3
r
log(2/δ)
2n
,
(38)
where ˜F = {(x, y) 7→f(x)y : f ∈F, y ∈[d]}. Next we upper bound the empirical Rademacher complex-
ity of ˜F:
bRn( ˜F) = Eσ1,...,σn
"
sup
y∈[d],h∈H,∥h∥≤M
1
n
n
X
i=1
σih(Xi)y
#
(39)
= Eσ1,...,σn
"
sup
y∈[d],h∈H,∥h∥≤M
1
n
n
X
i=1
σih(Xi)⊤y
#
(y is the one-hot enc. of y)
(40)
2Note that their result is in terms of Rademacher complexity rather than empirical Rademacher complexity. The variant we use
can be proved with the same proof, with a single modiﬁcation of bounding R(g) with empirical Rademacher complexity of ˜G using
Theorem 3.3 of Mohri et al. [2018].
21

= Eσ1,...,σn
"
sup
y∈[d],h∈H,∥h∥≤M
*
h, 1
n
n
X
i=1
σikXiy
+
H
#
(reproducing property)
(41)
≤M
n Eσ1,...,σn
"
sup
y∈[d]

n
X
i=1
σikXiy

H
#
(Cauchy-Schwarz)
(42)
= M
n
v
u
u
u
tEσ1,...,σn

sup
y∈[d]

n
X
i=1
σikXiy

2
H


(Jensen’s inequality)
(43)
≤M
n
v
u
u
u
tEσ1,...,σn


d
X
y=1

n
X
i=1
σikXiy

2
H


(44)
= M
n
v
u
u
u
t
d
X
y=1
Eσ1,...,σn


n
X
i=1
∥σikXiy∥2
H +
X
i̸=j

σikXiy, σjkXjy



(45)
= M
n
v
u
u
t
d
X
y=1
Eσ1,...,σn
" n
X
i=1
∥σikXiy∥2
H
#
(independence of σi)
(46)
= M
n
v
u
u
t
d
X
y=1
" n
X
i=1
y⊤k(Xi, Xi)y
#
(47)
= M
n
p
Tr (K).
(48)
The proof is concluded with the same reasoning of the proof of Thm. 3.
D
Additional results and discussion
In this appendix we present additional results and discussion to support the main ﬁndings of this work.
On early stopped teachers. Cho and Hariharan [2019] observe that sometimes ofﬂine KD works better
with early stopped teachers. Such teachers have worse accuracy and perhaps results in a smaller student
margin, but they also have a signiﬁcantly smaller supervision complexity (see Fig. 2a), which provides a
possible explanation for this phenomenon.
Teaching students with weak inductive biases. As we saw earlier, a fully trained teacher can have predic-
tions as complex as random labels for a weak student at initialization. This low alignment of student NTK
and teacher predictions can result in memorization. In contrast, an early stopped teacher captures simple
patterns and has a better alignment with the student NTK, allowing the student to learn these patterns in a
generalizable fashion. This feature learning improves the student NTK and allows learning more complex
patterns in future iterations. We hypothesize that this is the mechanism that allows online distillation to
outperform ofﬂine distillation in some cases.
CIFAR-10 results. Table 5 presents the comparison of standard training, ofﬂine distillation, and online
distillation on CIFAR-10. We see that the results are qualitatively similar to CIFAR-100 results.
Comparison of supervision complexities. In the main text, we have introduced the notion of adjusted
supervision complexity, which for a given neural network f(x) with NTK k and a set of labeled examples
22

Table 5: Results on CIFAR-10. Every second line is an MSE student.
Setting
No KD
Ofﬂine KD
Online KD
Teacher
τ = 1
τ = 4
τ = 1
τ = 4
ResNet-56 →LeNet-5x8
81.8 ± 0.5
82.4 ± 0.5
86.0 ± 0.2
86.8 ± 0.2
88.6 ± 0.1
93.2
ResNet-56 →LeNet-5x8
83.4 ± 0.3
83.1 ± 0.2
84.9 ± 0.1
85.6 ± 0.1
87.1 ± 0.1
93.2
ResNet-110 →LeNet-5x8
81.7 ± 0.3
81.9 ± 0.5
85.8 ± 0.1
86.5 ± 0.1
88.8 ± 0.1
93.9
ResNet-110 →LeNet-5x8
83.2 ± 0.4
83.2 ± 0.1
85.0 ± 0.3
85.6 ± 0.1
87.1 ± 0.2
93.9
ResNet-110 →ResNet-20
91.4 ± 0.2
91.4 ± 0.1
92.8 ± 0.0
92.2 ± 0.3
93.1 ± 0.1
93.9
ResNet-110 →ResNet-20
90.9 ± 0.1
90.9 ± 0.2
91.6 ± 0.2
91.2 ± 0.1
92.1 ± 0.2
93.9
0
50
100
150
200
250
Epochs
0
1
2
3
4
Adjusted supervision
complexity*
labels
random labels
offline teacher
online teacher
0
50
100
150
200
250
Epochs
1
2
3
4
Adjusted supervision
complexity* over target norm
labels
random labels
offline teacher
online teacher
Figure 4: Adjusted supervision complexities* of various targets with respect to a LeNet-5x8 network at
different stages of its training. The experimental setup of the left and right plots matches that of Fig. 1c and
Fig. 2a respectively.
{(Xi, Yi)}i∈[m] is deﬁned as:
1
n
q
(Y −f(X1:m))⊤K−1(Y −f(X1:m)) · Tr (K).
(49)
As discussed earlier, the subtraction of initial predictions is the appropriate way to measure complexity given
the form of the optimization problem (11). Nevertheless, it is meaningful to consider the following quantity
as well:
1
n
q
Y ⊤K−1Y · Tr (K),
(50)
in order to measure “alignment” of targets Y with the NTK k. We call this quantity adjusted supervision
complexity*. We compare the adjusted supervision complexities* of random labels, dataset labels, and pre-
dictions of an ofﬂine and online ResNet-56 teacher predictions with respect to various checkpoints of the
LeNet-5x8 network. The results presented in Fig. 4 are remarkably similar to the results with adjusted su-
pervision complexity (Fig. 1c and Fig. 2a). We therefore, focus only on adjusted supervision complexity of
(49) when comparing various targets. The only other experiment where we compute adjusted supervision
complexities* (i.e., without subtracting the current predictions from labels) is presented in Fig. 7, where the
goal is to demonstrate that training labels become aligned with the training NTK matrix over the course of
training.
Fig. 5 presents the comparison of adjusted supervision complexities, but with respect to a ResNet-20 net-
work, instead of a LeNet-5x8 network. Again, we see that in the early epochs dataset labels and ofﬂine
23

0
50
100
150
200
250
Epochs
100
101
102
Adjusted supervision
complexity
labels
random labels
offline teacher
online teacher
0
50
100
150
200
250
Epochs
101
102
Adjusted supervision
complexity over target norm
labels
random labels
offline teacher
online teacher
Figure 5: Adjusted supervision complexities of various targets with respect to a ResNet-20 network at dif-
ferent stages of its training. Besides the network choice, the experimental setup of the left and right plots
matches that of Fig. 1c and Fig. 2a respectively. Note that the y-axes are in logarithmic scale.
0
50
100
150
200
250
Epochs
103
104
105
NTK condition number
(a) LeNet-5x8 student
0
50
100
150
200
250
Epochs
105
106
107
108
109
NTK condition number
(b) ResNet-20 student
Figure 6: Condition number of the NTK matrix of a LeNet5x8 (a) and ResNet-20 (b) students trained with
MSE loss on binary CIFAR-100. The NTK matrices are computed on 212 test examples.
teacher predictions are almost as complex as random labels. Unlike the case of the LeNet-5x8 network,
random labels, dataset labels, and ofﬂine teacher predictions do not exhibit a U-shaped behavior. As for the
LeNet-5x8 network, the shape of these curves is in agreement with the behavior of the condition number
of the NTK (Fig. 6). Importantly, we still observe that the complexity of the online teacher predictions is
signiﬁcantly smaller compared to the other targets, even when we account for the norm of predictions.
The effect of frequency of teacher checkpoints. As mentioned earlier, throughout this paper we used
one teacher checkpoint per epoch. While this served our goal of establishing efﬁcacy of online distillation,
this choice is prohibitive for large teacher networks. To understand the effect of the frequency of teacher
checkpoints, we conduct an experiment on CIFAR-100 with ResNet-56 and LeNet-5x8 student with vary-
ing frequency of teacher checkpoints. In particular, we consider checkpointing the teacher once in every
{1, 2, 4, 8, 16, 32, 64, 128} epochs. The results presented in Fig. 8 show that reducing the teacher check-
pointing frequency to once in 16 epochs results in only a minor performance drop for online distillation with
τ = 4.
On label supervision in KD. So far in all distillation methods dataset labels were not used as an additional
source of supervision for students. However, in practice it is common to train a student with a convex
combination of knowledge distillation and standard losses: (1 −α)Lce + αLkd-ce. To verify that the choice
of α = 1 does not produce unique conclusions regarding efﬁcacy of online distillation, we do experiments
24

0
50
100
150
200
250
Epochs
1
2
3
4
Adjusted supervision
complexity*
(a) LeNet-5x8 student
0
50
100
150
200
250
Epochs
100
101
102
Adjusted supervision
complexity*
(b) ResNet-20 student
Figure 7: Adjusted supervision complexity* of dataset labels measured on a subset of 212 training examples
of binary CIFAR-100. Complexities are measured with respect to either a LeNet-5x8 (on the left) or ResNet-
20 (on the right) models trained with MSE loss and without knowledge distillation. Note that the plot on the
right is in logarithmic scale.
Teacher update period
Online KD
in epochs
τ = 1
τ = 4
1 (the default value)
61.9 ± 0.2
66.1 ± 0.4
2
61.5 ± 0.4
66.0 ± 0.3
4
61.4 ± 0.2
65.6 ± 0.2
8
60.0 ± 0.3
65.4 ± 0.0
16
59.3 ± 0.6
65.4 ± 0.0
32
56.9 ± 0.0
64.1 ± 0.4
64
55.5 ± 0.4
62.8 ± 0.7
128
51.4 ± 0.5
61.3 ± 0.1
1
2
4
8
16
32
64 128
Teacher update period in epochs
55
60
65
Test accuracy
Online KD τ = 1
Online KD τ = 4
Figure 8: Online KD results for a LeNet-5x8 student on CIFAR-100 with varying frequency of a ResNet-56
teacher checkpoints.
on CIFAR-100 with varying values of α. The results presented in Table 6 conﬁrm our main conclusions on
online distillation. Furthermore, we observe that picking α = 1 does not result in signiﬁcant degradation of
student performance.
The effect of τ. To conﬁrm that our main conclusions regarding online distillation do not depend on the
temperature value, we present additional experiments on CIFAR-100 with τ = 2 in Table 7.
NTK similarity. Figures 9 and 10 presents additional evidence that (a) training NTK similarity of the
ﬁnal student and the teacher is correlated with the ﬁnal student test accuracy; and (b) that online distillation
manages to transfer teacher NTK better.
E
Expanded discussion on related work
The key contributions of this work are the demonstration of the role of supervision complexity in student
generalization, and the establishment of online knowledge distillation as a theoretically grounded and effec-
tive method. Both supervision complexity and online distillation have a number of relevant precedents in
25

Table 6: Knowledge distillation results on CIFAR-100 with varying loss mixture coefﬁcient α.
Setting
α
No KD
Ofﬂine KD
Online KD
Teacher
τ = 1
τ = 4
τ = 1
τ = 4
ResNet-56 →
LeNet-5x8
0.2
47.3 ± 0.6
47.6 ± 0.7
57.6 ± 0.2
54.3 ± 0.7
59.0 ± 0.6
72.0
0.4
48.9 ± 0.3
58.9 ± 0.4
56.7 ± 0.5
62.5 ± 0.2
0.6
49.4 ± 0.5
59.7 ± 0.0
61.1 ± 0.0
65.3 ± 0.2
0.8
49.8 ± 0.1
60.1 ± 0.1
62.0 ± 0.1
65.9 ± 0.2
1.0
50.1 ± 0.4
59.9 ± 0.2
61.9 ± 0.2
66.1 ± 0.4
ResNet-56 →
ResNet-20
0.2
67.7 ± 0.5
67.9 ± 0.3
70.3 ± 0.3
68.2 ± 0.3
70.3 ± 0.1
72.0
0.4
67.9 ± 0.1
71.0 ± 0.2
68.7 ± 0.2
71.4 ± 0.2
0.6
68.1 ± 0.3
71.3 ± 0.1
69.6 ± 0.4
71.5 ± 0.2
0.8
68.3 ± 0.2
71.4 ± 0.4
69.8 ± 0.3
71.1 ± 0.3
1.0
68.2 ± 0.3
71.6 ± 0.2
69.6 ± 0.3
71.4 ± 0.3
Table 7: Results on CIFAR-100.
Setting
No KD
Ofﬂine KD
Online KD
Teacher
τ = 1
τ = 2
τ = 1
τ = 2
ResNet-56 →LeNet-5x8
47.3 ± 0.6
50.1 ± 0.4
55.2 ± 0.1
61.9 ± 0.2
64.7 ± 0.2
72.0
ResNet-56 →ResNet-20
67.7 ± 0.5
68.2 ± 0.3
70.4 ± 0.3
69.6 ± 0.3
70.8 ± 0.3
72.0
ResNet-110 →LeNet-5x8
47.2 ± 0.5
48.6 ± 0.8
54.0 ± 0.5
60.8 ± 0.2
63.9 ± 0.2
73.4
ResNet-110 →ResNet-20
67.8 ± 0.3
67.8 ± 0.2
69.3 ± 0.2
69.0 ± 0.3
70.5 ± 0.3
73.4
the literature that are worth comment.
Transferring knowledge beyond logits. In the seminal works of Buciluˇa et al. [2006], Hinton et al. [2015]
transferred “knowledge” is in the form of output probabilities. Later works suggest other notions of “knowl-
edge“ and other ways of transferring knowledge [Gou et al., 2021]. These include activations of intermediate
layers [Romero et al., 2015], attention maps [Zagoruyko and Komodakis, 2017], classiﬁer head parame-
ters [Chen et al., 2022], and various notions of example similarity [Passalis and Tefas, 2018, Park et al.,
2019, Tung and Mori, 2019, Tian et al., 2020, He and Ozay, 2021]. Transferring teacher NTK matrix be-
longs to this latter category of methods. Zhang et al. [2022] propose to transfer a low-rank approximation of
a feature map corresponding the teacher NTK.
Non-static teachers. Some works on KD consider non-static teachers. In order to bridge teacher-student ca-
pacity gap, Mirzadeh et al. [2020] propose to perform a few rounds of distillation with teachers of increasing
capacity. In deep mutual learning [Zhang et al., 2018, Chen et al., 2020], codistillation [Anil et al., 2018],
and collaborative learning [Guo et al., 2020], multiple students are trained simultaneously, distilling from
each other or from an ensemble. In Zhou et al. [2018] and Shi et al. [2021], the teacher and the student are
trained together. In the former they have a common architecture trunk, while in the latter the teacher is pe-
nalized to keep its predictions close to the student’s predictions. The closest method to the online distillation
method of this work is route constrained optimization [Jin et al., 2019], where a few teacher checkpoints are
selected for a multi-round distillation. Rezagholizadeh et al. [2022] employ a similar procedure but with an
annealed temperature that decreases linearly with training time, followed by a phase of training with dataset
labels only. The idea of distilling from checkpoints also appears in Yang et al. [2019], where a network is
trained with a cosine learning rate schedule, simultaneously distilling from the checkpoint of the previous
26

learning rate cycle.
Fundamental understanding of distillation. The effects of temperature, teacher-student capacity gap, opti-
mization time, data augmentations, and other training details is non-trivial [Cho and Hariharan, 2019, Beyer
et al., 2022, Stanton et al., 2021]. It has been hypothesized and shown to some extent that teacher soft pre-
dictions capture class similarities, which is beneﬁcial for the student [Hinton et al., 2015, Furlanello et al.,
2018, Tang et al., 2020]. Yuan et al. [2020] demonstrate that this softness of teacher predictions also has
a regularization effect, similar to label smoothing. Menon et al. [2021] argue that teacher predictions are
sometimes closer to the Bayes classiﬁer than the hard labels of the dataset, reducing the variance of the
training objective. The vanilla knowledge distillation loss also introduces some optimization biases. Mobahi
et al. [2020] prove that for kernel methods with RKHS norm regularization, self-distillation increases regu-
larization strength, resulting in smaller norm RKHS norm solutions.
Phuong and Lampert [2019] prove that in a self-distillation setting, deep linear networks trained with gradi-
ent ﬂow converge to the projection of teacher parameters into the data span, effectively recovering teacher
parameters when the number of training points is large than the number of parameters. They derive a bound
of the transfer risk that depends on the distribution of the acute angle between teacher parameters and data
points. This is in spirit related to supervision complexity as it measures an “alignment ” between the distilla-
tion objective and data Ji and Zhu [2020] extend this results to linearized neural networks, showing that the
quantity ∆⊤
z K−1∆z, where ∆z is the logit change during training, plays a key role in estimating the bound.
The resulting bound is qualitatively different compared to ours, and the ∆⊤
z K−1∆z becomes ill-deﬁned for
hard labels.
Supervision complexity. The key quantity in our work is Y ⊤K−1Y . Cristianini et al. [2001] introduced
a related quantity Y ⊤KY called kernel-target alignment and derived a generalization bound with it for
expected Parzen window classiﬁers. As an easy-to-compute proxy to supervision complexity, Deshpande
et al. [2021] use kernel-target alignment for model selection in transfer learning. Ortiz-Jim´enez et al. [2021]
demonstrate that when NTK-target alignment is high, learning is faster and generalizes better. Arora et al.
[2019] prove a generalization bound for overparameterized two-layer neural networks with NTK parame-
terization trained with gradient ﬂow. Their bound is approximately
q
Y ⊤(K∞)−1Y /√n, where K∞is the
expected NTK matrix at a random initialization. Our bound of Thm. 3 can be seen as a generalization of
this result for all kernel methods, including linearized neural networks of any depth and sufﬁcient width,
with the only difference of using the empirical NTK matrix. Belkin et al. [2018] warns that bounds based on
RKHS complexity of the learned function can fail to explain the good generalization capabilities of kernel
methods in presence of label noise.
27

68
69
70
71
Test accuracy
0.80
0.82
0.84
0.86
0.88
0.90
Train fidelity
correlation: -0.74
No KD
offline KD
online KD
(a)
CIFAR-100,
ResNet-20
student,
ResNet-110 teacher
82
84
86
88
Test accuracy
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
Train NTK similarity
correlation: 0.80
No KD
offline KD
online KD
(b)
CIFAR-10,
LeNet-5x8
student,
ResNet-56 teacher
82
84
86
88
Test accuracy
0.986
0.988
0.990
0.992
0.994
0.996
0.998
1.000
Train fidelity
correlation: 0.24
No KD
offline KD
online KD
(c)
CIFAR-10,
LeNet-5x8
student,
ResNet-56 teacher
91.5
92.0
92.5
93.0
Test accuracy
0.68
0.70
0.72
0.74
0.76
0.78
Train NTK similarity
correlation: 0.90
No KD
offline KD
online KD
(d)
CIFAR-10,
ResNet-20
student,
ResNet-101 teacher
72
74
76
Test accuracy
0.45
0.50
0.55
0.60
0.65
Train NTK similarity
correlation: 0.71
No KD
offline KD
online KD
(e) Binary CIFAR-100, LeNet-5x8 stu-
dent, ResNet-56 teacher
72
74
76
Test accuracy
0.980
0.982
0.984
0.986
0.988
0.990
0.992
0.994
Train fidelity
correlation: -0.57
No KD
offline KD
online KD
(f) Binary CIFAR-100, LeNet-5x8 stu-
dent, ResNet-56 teacher
Figure 9: Relationship between test accuracy, train NTK similarity, and train ﬁdelity for various teacher,
student, and dataset conﬁgurations.
28

59
60
61
62
Test accuracy
0.24
0.26
0.28
0.30
0.32
0.34
0.36
0.38
Train NTK similarity
correlation: 0.40
No KD
offline KD
online KD
(a) Tiny ImageNet, MobileNet-V3-35
student, MobileNet-V3-125 teacher
59
60
61
62
Test accuracy
0.75
0.80
0.85
0.90
0.95
Train fidelity
correlation: -0.38
No KD
offline KD
online KD
(b) Tiny ImageNet, MobileNet-V3-35
student, MobileNet-V3-125 teacher
50
55
60
65
Test accuracy
0.30
0.35
0.40
0.45
0.50
0.55
Train NTK similarity
correlation: 0.98
No KD
offline KD
online KD
(c) Tiny ImageNet, VGG-16 student,
ResNet-101 teacher
50
55
60
65
Test accuracy
0.94
0.96
0.98
1.00
Train fidelity
correlation: -0.42
No KD
offline KD
online KD
(d) Tiny ImageNet, VGG-16 student,
ResNet-101 teacher
50
55
60
65
Test accuracy
0.35
0.40
0.45
0.50
0.55
Train NTK similarity
correlation: 0.99
No KD
offline KD
online KD
(e) Tiny ImageNet, VGG-16 student,
MobileNet-V3-125 teacher
50
55
60
65
Test accuracy
0.997
0.998
0.999
1.000
Train fidelity
correlation: -0.58
No KD
offline KD
online KD
(f) Tiny ImageNet, VGG-16 student,
MobileNet-V3-125 teacher
Figure 10: Relationship between test accuracy, train NTK similarity, and train ﬁdelity for various teacher,
student, and dataset conﬁgurations.
29

