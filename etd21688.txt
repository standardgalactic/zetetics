 
Adaptive and Learning-based Formation Control 
of Swarm Robots 
by 
Mahsoo Salimi 
M.Arch., University of Colorado at Denver, 2012 
 
Thesis Submitted in Partial Fulfillment of the  
Requirements for the Degree of  
Doctor of Philosophy 
 
in the 
School of Interactive Arts and Technology  
Faculty of Communication, Art and Technology 
 
© Mahsoo Salimi 2021 
SIMON FRASER UNIVERSITY 
Fall 2021 
 
Copyright in this work rests with the author. Please ensure that any reproduction or re-use is done in 
accordance with the relevant national copyright legislation. 
 
 

ii 
Declaration of Committee 
Name: 
Mahsoo Salimi 
Degree: 
Doctor of Philosophy (Interactive Arts and Technology) 
Thesis title: 
Adaptive and Learning-based Formation Control of 
Swarm Robots 
Committee: 
Chair: 
Gabriela Aceves-Sepulveda 
Associate Professor, Interactive Arts and 
Technology 
 
Philippe Pasquier 
Supervisor 
Professor, Interactive Arts and Technology 
 
Steve DiPaola 
Committee Member 
Professor, Interactive Arts and Technology 
 
Mo Chen 
Examiner 
Assistant Professor, Computing Science 
 
Simon Yang 
External Examiner 
Professor, Engineering 
University of Guelph 
 
 

iii 
Abstract 
Autonomous aerial and wheeled mobile robots play a major role in tasks such as search 
and rescue, transportation, monitoring, and inspection. However, these operations are faced 
with a few open challenges including robust autonomy, and adaptive coordination based 
on the environment and operating conditions, particularly in swarm robots with limited 
communication and perception capabilities. Furthermore, the computational complexity 
increases exponentially with the number of robots in the swarm. This thesis examines two 
different aspects of the formation control problem. On the one hand, we investigate how 
formation could be performed by swarm robots with limited communication and perception 
(e.g., Crazyflie nano quadrotor). On the other hand, we explore human-swarm interaction 
(HSI) and different shared-control mechanisms between human and swarm robots (e.g., 
BristleBot) for artistic creation. In particular, we combine bio-inspired (i.e., flocking, 
foraging) techniques with learning-based control strategies (using artificial neural 
networks) for adaptive control of multi- robots. We first review how learning-based control 
and networked dynamical systems can be used to assign distributed and decentralized 
policies to individual robots such that the desired formation emerges from their collective 
behavior. We proceed by presenting a novel flocking control for UAV swarm using deep 
reinforcement learning. We formulate the flocking formation problem as a partially 
observable Markov decision process (POMDP), and consider a leader-follower 
configuration, where consensus among all UAVs is used to train a shared control policy, 
and each UAV performs actions based on the local information it collects. In addition, to 
avoid collision among UAVs and guarantee flocking and navigation, a reward function is 
added with the global flocking maintenance, mutual reward, and a collision penalty. We 
adapt deep deterministic policy gradient (DDPG) with centralized training and 
decentralized execution to obtain the flocking control policy using actor-critic networks 
and a global state space matrix. In the context of swarm robotics in arts, we investigate 
how the formation paradigm can serve as an interaction modality for artists to aesthetically 
utilize swarms. In particular, we explore particle swarm optimization (PSO) and random 
walk to control the communication between a team of robots with swarming behavior for 
musical creation. 

iv 
 
Keywords: Swarm Control; Reinforcement Learning; Deep Learning; Swarm Robotics; 
Flocking; Foraging 
 
 

v 
Dedication 
I would like to dedicate this thesis to my family. Notably, I want to thank my parents for 
their endless love, support and encouragement.  
 
 

vi 
Acknowledgements 
My experience at SFU has been challenging, enriching, and rewarding in many ways. 
With all certainty, my doctoral advisor, Dr. Philippe Pasquier, had a significant 
impact on my transition and journey to defence. I am grateful to Philippe for encouraging 
me to follow my slightly unconventional interests during my Ph.D. and for helping me in 
developing an effective research agenda.  
There are no words to express my gratitude to Dr. Nikolaus Correll for his ongoing 
mentorship and inspiration over the last few years which has helped in framing the scope 
of my research. 
I acknowledge the support and help of all my lab-mates and friends at Metacreation 
Lab: Dr. Mirjana Prpa, Dr. Kıvanç Tatar, and Dr. Jianyu Fan throughout these years. I 
thank the support of collogues and collaborator Dr. Carlos Castellanos, Nouf Abukhodair, 
Weina Jin, and Dr. Xin Tong. I also, thank my co-supervisor Dr. Steve DiPaola, and mentor 
Dr. Gabriela Aceves Sepúlveda for their inspiration and positive energy. I want to specially 
thank Dr. Daniel Bisig, Leonel Moura and Linda Bouchard for their amazing insights and 
support.  
Finally, I owe everything to my parents. I am extremely grateful to them for 
providing me with an excellent education and for instilling in me the principles of 
perseverance and hard work. I would also like to express my gratitude to my sister, Setareh 
and my brothers, Salman and Saeed, for everything they have done and their unconditional 
love. 

vii 
Table of Contents 
Declaration of Committee ............................................................................................ ii 
Abstract ...................................................................................................................... iii 
Dedication ................................................................................................................... v 
Acknowledgements .................................................................................................... vi 
Table of Contents ...................................................................................................... vii 
List of Tables .............................................................................................................. x 
List of Figures ............................................................................................................ xi 
List of Acronyms and Abbreviations ........................................................................ xiv 
List of Notations ....................................................................................................... xvi 
Chapter 1. 
Introduction and Overview .................................................................... 1 
1.1. Introduction ........................................................................................................... 2 
1.2. Background ........................................................................................................... 3 
1.2.1. 
Learning-based Formation Control ........................................................... 3 
1.2.2. 
Related Work in Learning-based Formation Control ................................ 3 
1.2.3. 
Human-Swarm Interaction (HSI) – in Robotics ........................................ 5 
1.2.4. 
Related Work in Human-Swarm Interaction (HSI) for Aesthetic Control – 
Swarm Art ............................................................................................... 6 
1.3. Motivation and Research Questions ....................................................................... 7 
1.3.1. 
Thesis Focus 1 (TF1): Learning-based Formation Control of Swarm 
Robots...................................................................................................... 9 
1.3.2. 
Thesis Focus 2 (TF2): Human-Swarm Interaction (HSI) and Aesthetic 
Exploration of Swarm Formation ........................................................... 11 
1.4. Contributions ....................................................................................................... 12 
1.4.1. 
Formation Control of UAV Swarm ........................................................ 12 
1.4.2. 
Swarm Aesthetics ................................................................................... 13 
1.4.3. 
Publications ........................................................................................... 14 
1.5. Organization of the Thesis ................................................................................... 15 
1.5.1. 
Appendix Outline ................................................................................... 16 
Bibliography.............................................................................................................. 19 
Chapter 2. 
Learning-based Formation Control .................................................... 25 
2.1. Preliminaries ....................................................................................................... 26 
2.1.1. 
Consensus Formation ............................................................................. 26 
2.1.2. 
Formation Configuration ........................................................................ 27 
Bibliography.............................................................................................................. 38 
2.2. Advances in Formation of Multi-robot Systems using Reinforcement Learning and 
Neural Networks .............................................................................................. 41 
2.2.1. 
Introduction ........................................................................................... 41 

viii 
2.2.2. 
Typical Neural Network Architectures ................................................... 43 
2.2.3. 
Theoretical Developments ...................................................................... 59 
2.2.4. 
Formation Control in Swarm Robotics ................................................... 73 
2.2.5. 
Challenges and Open Research Questions .............................................. 76 
2.2.6. 
Conclusion ............................................................................................. 77 
Chapter 3. 
Multi-robot Formation Control using Deep Reinforcement Learning 
(DRL) 90 
3.1. Preliminaries ....................................................................................................... 91 
Quadcopter UAV ................................................................................................. 91 
Flocking Consensus ............................................................................................. 97 
Bibliography............................................................................................................ 101 
3.2. Deep Reinforcement Learning for Flocking Control of UAVs in Complex 
Environments ................................................................................................. 102 
3.2.1. 
Introduction ......................................................................................... 102 
3.2.2. 
Related Work ....................................................................................... 104 
3.2.3. 
Problem Formulation ........................................................................... 107 
3.2.4. 
Simulation Results and Discussion ....................................................... 116 
3.2.5. 
Conclusion ........................................................................................... 122 
Chapter 4. 
Human-Swarm Interaction (HSI)...................................................... 130 
4.1. Preliminaries ..................................................................................................... 131 
4.1.1. 
Flocking ............................................................................................... 131 
4.1.2. 
Foraging............................................................................................... 132 
Bibliography............................................................................................................ 135 
4.2. Swarm Art ......................................................................................................... 136 
4.2.1. 
Basic Definitions .................................................................................. 136 
4.2.2. 
Swarm Characteristics .......................................................................... 137 
4.2.3. 
Swarm Algorithms ............................................................................... 138 
Bibliography............................................................................................................ 141 
4.3. Exploiting Swarm Aesthetics in Sound Art ........................................................ 144 
4.3.1. 
Introduction ......................................................................................... 144 
4.3.2. 
Background .......................................................................................... 144 
4.3.3. 
Liminal Tones ...................................................................................... 148 
4.3.4. 
Discussion and Future Works ............................................................... 151 
4.4. Swarm Aesthetics and Materiality in Sound Composition .................................. 160 
4.4.1. 
Introduction ......................................................................................... 160 
4.4.2. 
Methods ............................................................................................... 164 
4.4.3. 
Results ................................................................................................. 168 
4.4.4. 
Discussion and Future Works ............................................................... 173 
Bibliography............................................................................................................ 175 

ix 
Chapter 5. 
Discussion and Future Work ............................................................. 178 
5.1. Summary and Revisiting Thesis Foci ................................................................. 179 
5.1.1. 
TF1: Learning-based Formation Control of Swarm Robots................... 179 
5.1.2. 
TF2: Human-Swarm Interaction (HSI) and Aesthetic Exploration of 
Swarm Formation ................................................................................. 180 
5.2. Limitations and Future Work ............................................................................. 181 
5.2.1. 
Flocking formation of UAV Swarm ..................................................... 181 
5.2.2. 
Robotic Swarm Music .......................................................................... 182 
5.3. Conclusion ........................................................................................................ 182 
Appendix A.  SIAT Guidelines for Writing a Cumulative Thesis ........................... 185 
Appendix B.  Quadcopter Equations of Motion ....................................................... 187 
Appendix C.  Swarm Systems in Art and Architecture- State of the Art (a section 
from chapter 3) ................................................................................................ 193 
Appendix D.  Liminal Scape, an interactive visual installation with expressive AI 209 

x 
List of Tables 
Table 1.1. 
Thesis outlines ....................................................................................... 18 
Table 2.1 
Comparison parameters, adapted from Verma & Ranga (2021) .............. 43 
Table 2.2 
Summary of Adaptive NNs for formation control ................................... 65 
Table 2.3 
Summary of Bio-inspired models for formation control .......................... 71 
Table 2.4 
Summary of formation control in Swarm Robotics ................................. 75 
Table 3.1 
Training parameters in Algorithm ........................................................ 122 
Table 4.1 
PID values for multiple identical DC motors and synchronous speed 
control.................................................................................................. 154 
 
 
 

xi 
List of Figures 
Figure 2.1 
Actor-critic architecture (Sutton & Barto, 1998). .................................... 34 
Figure 2.2 
A multi-layered feedforward neural network (FFNN) with an input vector, 
a hidden layer with tangent hyperbolic activation function, and an output 
layer with a linear activation function. Adapted from El Hamidi et al., 
2020. ...................................................................................................... 57 
Figure 2.3 
Fully-connected recurrent neural network (RNN) with an input layer, a 
hidden layer, a context layer, and an output layer. Adapted from Liu et al., 
2014. ...................................................................................................... 57 
Figure 2.4 
Graph neural network (GNN) with an input graph and a target node. 
Adapted from Hamilton et al., 2017........................................................ 57 
Figure 2.5 
Deep Q-Networks (DQN). Adapted from Choudhary et al., 2019. .......... 58 
Figure 2.6 
Deep deterministic policy gradients (DDPG) network. Adapted from 
Liessner et al., 2021. .............................................................................. 58 
Figure 2.7 
Fuzzy actor-critic reinforcement learning (FACRL) network. Adapted 
from Wang et al., 2007. .......................................................................... 58 
Figure 3.1 
The global frame (Green), and the (!)-configured body frame (Yellow). 
The positive direction of the rotors’ angular velocities in the body frame is 
also shown in the picture, along with the accompanying thrusts.............. 91 
Figure 3.2 
Sequence of Euler Angles Rotations from inertial to body-fixed frames 
with intermediate reference frames "′′ and "′. ........................................ 93 
Figure 3.3 
Network architecture of the proposed flocking control system. Three 
channels of information are taken as inputs: the leader UAV’s current state 
(!$, &$, '$, ($, )$), and the follower state (!*, &*, '*, (*, )*), and desired 
roll angle and velocity (',-*, ),-*). A joint representation of the inputs is 
created through a global plant encoder. ................................................ 114 
Figure 3.4 
The block diagram of the DDPG with the actor-critic architecture. ....... 114 
Figure 3.5 
Quadrotor UAVs’ initial actor-critic networks. A fully connected (FC) 
layer is featured by its type, number of neurons, and activation function. 
Other layers are represented by their type or name. .............................. 117 
Figure 3.6 
The integrated reward in the initial test for DDPG agents. .................... 118 
Figure 3.7 
The integrated reward with improved parameters for flocking DDPG 
agents. .................................................................................................. 119 
Figure 3.8 
The proposed (modified) actor-critic networks. A fully connected (FC) 
layer is featured by its type, number of neurons, and activation function. 
Other layers are represented by their type or name. .............................. 121 
Figure 3.9 
The integrated reward using the proposed network for flocking DDPG 
agents. .................................................................................................. 121 
Figure 4.1 
Frequencies (A), Sound performance of mechanically triggered tuning 
forks with pure digital soundwaves. Nicolas Bernier, 2013. .................. 146 

xii 
Figure 4.2 
a shot from Robotic Electronic Music (R.E.M), using music robots, 
mechanics and sound devices. Moritz Simon Geist, 2019. .................... 146 
Figure 4.3 
Material Music, a sound installation consists of a linear array of eight 
kinetic sound- sculptures at International Symposium on Electronic Art 
(ISEA). Mo H. Zareei, 2020. ................................................................ 149 
Figure 4.4 
an installation using 51 prepared dc-motors, 241m rope, cardboard sticks 
25 cm, Museum of Contemporary Art MAC, Santiago de Chile. Zimoun, 
2019. .................................................................................................... 149 
Figure 4.5 
The Simulink block diagram of our PID Controller and 8 DC motors. .. 155 
Figure 4.6 
The block diagram of our PID Controller with PSO algorithm. Adapted 
from Hashim & Mustafa (2020). .......................................................... 155 
Figure 4.7 
a close-up shot of Liminal Tones, control system using an Arduino board.
 ............................................................................................................ 156 
Figure 4.8 
Liminal Tones in action, 8 DC Motors swarming together. ................... 156 
Figure 4.9 
Spectrograms of 12 sound samples (each ranging from 15-30 seconds). 
Note the constant noisy profile of Wood and the mid-level frequencies and 
orders of Ceramic or Resonance of Granite. Some samples have different 
characteristics such as rhythmic patterns and high-low pass while others 
are noisy............................................................................................... 157 
Figure 4.10 
Model scheme and examples of trajectories for 5 BBots and 10,000 steps 
with fixed step distribution and high-speed during Phase 1 which follows a 
ballistic Lévy Walk. Different colors correspond to each BBot and its 
initial conditions (placement, speed, direction). When the value of ∆/0 is 
close to zero for a long time, the BBots move in a straight line with short 
steps in between. .................................................................................. 170 
Figure 4.11 
Model scheme and examples of trajectories for 5 BBots and 10,000 steps. 
There is random step distribution and low-speed movement during Phase 
2, similar to Brownian Motion. The internal dynamics ! and & produce 
agent movements in 2D space. Movement is produced by turning angles 
∆/0 over time (0). The trajectory of each BBot in a 2D space is 
represented by different colors corresponding to each BBot and its initial 
conditions (placement, speed, direction). .............................................. 170 
Figure 4.12 
Comparison of audio samples (left) with natural sounds (hail, rain, sleet). 
Examples were selected to be roughly similar in sound textures. The top 
row shows the waveforms. Note that our sample is more extremely 
periodic with high jumps compared to the other three. The bottom row 
shows the spectrograms. Here, the vertical lines represent step intervals. 
Note the constant tones around mid-levels in rain and the noisy profile of 
sleet sound. .......................................................................................... 171 
Figure 4.13 
Spectrograms of 12 sound samples (each ranging from 15-30 seconds). 
Note the constant noisy profile of wood and the mid-level frequencies and 
orders of ceramics, and resonance of granite. Some samples have different 
characteristics such as rhythmic patterns and high-low passes. Others are 

xiii 
noisy with a wide range of pitch and timbral qualities, which creates 
unique sound textures. .......................................................................... 172 
 
 

xiv 
List of Acronyms and Abbreviations 
ACA 
Ant-colony algorithm 
ACER 
Actor critic with experience replay 
APF 
Artificial potential field 
ANN 
Artificial neural network 
A3C 
Asynchronous advantage actor critic 
BA 
Bees algorithm 
B-EKF 
Bioinspired neural model based EKF approach 
BINN 
Bioinspired neural network 
BPANN 
Backpropagation artificial neural network 
CrKR 
Cost-regularized kernel regression 
CSPF 
Constraint satisfying parametric function 
CPG 
Central pattern generator 
CPG-NN-WT 
Central pattern generator—neural network—workspace trajectory 
CNN 
Convolutional neuronal network 
CPG 
Central pattern generator 
CPG-NN-WT 
Central pattern generator—neural network—workspace trajectory 
DDPG 
Deep deterministic policy gradients 
DIC-ANN 
Artificial neural network direct inverse control  
DNN 
Deep neural network 
DQN 
Deep Q-learning 
DWENN 
Dynamic wave expansion neural network 
EA 
Evolutionary algorithms 
FCGA 
Fuzzy-logic based chaos genetic algorithm 
FFBNN 
Feedforward backpropagation neural network 
FFNN 
Feedforward neural network 
FMP 
Force-based motion planning 
FNN 
Fuzzy neural network 
FTCC 
Fault-tolerant cooperative control 
FWNN 
Fuzzy wavelet neural network 
GA 
Genetic algorithm 

xv 
GBSOM 
Glasius bio-inspired self-organizing map  
GCN 
Graph convolutional network 
GNN 
Graph neural network 
GPG 
Graph policy gradients  
GRNN 
Generalized regression neural network 
GSP 
Graph signal processing 
HDDPG 
Hierarchical deep deterministic policy gradients 
LSTD 
Least-squares temporal difference 
MADDPG 
Multi-agent deep deterministic policy gradient 
MADRL 
Multi-agent deep reinforcement learning 
Meta-TD3 
Meta twin delayed deep deterministic policy gradient 
MSBE 
Mean squared bellman error  
MSE 
Mean square error 
mTSP 
Multiple travelling salesman 
NAC 
Normalized actor-critic 
NN-PV 
Neural network -proportional plus velocity 
PI 
Proportional–integral 
PID 
Proportional–integral–derivative 
POMDP 
Partially observable Markov decision process 
PPO 
Proximal policy optimization 
RBF 
Radial basis function 
RL-CRM 
Reinforcement learning closed-loop reference model 
RLNN 
Reinforcement learning neural network 
RTM 
Response threshold model 
SOM 
Self-organizing map 
SRWNN 
Recurrent wavelet neural network 
SSGA 
Steady-state genetic algorithm 
STAPP 
Simultaneous target assignment and path planning  
TD 
Temporal difference 
TRPO 
Trust region policy optimization 

xvi 
List of Notations 
! 
Scalar 
1 
Vector or a matrix 
12 
Unit base vector 
!3 
Vector expressed in the global frame 3 
!ℬ 
Vector expressed in the body frame ℬ 
567 
A rotation from the body frame to the global frame  
89: 
Inter-distance vector w.r.t quadrotor ; 
<=×= 
Zero matrix with size ? × ? 
@= 
Identity matrix with size ? 
3 
Graph of the interaction topology of agents 
ℰ 
Set of edges in the graph 3 
B 
Set of vertices in the graph 3 
6 
Interaction matrix 
3C 3E 3F 
Adjacency, degree and leader matrices  
G9 
The neighbor set of agent H 
I= 
Noise or uncertainty 
J 
Filter coefficients, or weights 
K 
Discount factor; penalty to uncertainty of future rewards 
*(!) 
The approximation of M(!) 
/ 
Control parameter 
NO(∙) 
Policy with parameters / 
NO
∗(∙) 
Optimal policy  
N(!|&) 
Stochastic policy (agent behavior strategy) 
S(!) 
Deterministic policy 
*TU(∙) 
Score function, which evaluates the policy  
V(/) 
A regularization function for parameter / 
*: 
A crisp function of !9 
W 
A conjunction operator in fuzzy set operations 
X(/) 
Performance objective 

xvii 
Y(!) 
State-value function (measures the expected return of state !) 
Z(!, &) 
Action-value function (assesses the expected return of a pair of state 
and action) 
YT(!) 
The value of state ! for a policy N 
ZT(!, &) 
The value of (state, action) pair for a policy π 
Z∗(!) 
Optimal value function 
S(!) 
Deterministic policy 
[(!, &) 
Advantage function: Z(!, &) −Y(!) 
/] 
Q-network 
/^ 
Deterministic policy network 
/]_  
Target Q-network 
/^́  
Target policy network 
 
 
 

1 
Chapter 1. 
 
 
Introduction and Overview 
 
 

2 
1.1. 
Introduction 
Cooperative and multi-robot formation control are beneficial and have many applications 
for unmanned aerial vehicles (UAVs), mobile robots, or autonomous underwater vehicles 
(AUVs) working together or in cooperation, including emergency response, search and 
rescue operations, sensing coverage, aerospace, and ocean exploration. Formation control 
can be defined as the requirement for robots to maintain relative position, trajectory 
tracking, and converge towards a region. Moving in formation has several advantages over 
conventional control (i.e., event-based control, behavior-based control, etc.) such as 
improved sensitivity, simplicity of finding appropriate controller gains, and flexibility in 
the network structure of multi-vehicle systems (Anderson et al., 2008; Kuriki & 
Namerikawa, 2014). 
Depending on the specific conditions, different control topologies can be used in 
formation control for a group of collaborative robots. There might be one or more leaders, 
which other robots follow in a specific way. Usually, each robot has onboard sensing, 
limited computation ability, and perception with no global information or centralized 
controller. Therefore, the controller must be based on local information. If no leader is 
chosen, all robots will coordinate to achieve a shared goal, relying on a global consensus. 
Designing a distributed controller for multi-robot formations rely on numerous 
factors, including formation stability, controllability, safety, and uncertainties. To 
overcome the challenges of formation, a variety of control systems have been proposed, 
for example, model-based (Zhang & Liu, 2018), behavior-based (Shiell, 2017; Pratama et 
al., 2019), and learning-based methods (Sui et al., 2019; Zhao et al., 2020).  
Given the difficulties in optimising and achieving optimal convergence in 
traditional multi-robot control, more adaptive and data-driven controllers are required. So, 
our research focuses on learning-based control of multi-robot systems to enable self-
learning, robust, and efficient operation of a multi-robot system in a dynamic and unknown 
environment. 

3 
1.2. 
Background 
1.2.1. Learning-based Formation Control 
Multi-robot formation control has traditionally been addressed by model-based or model-
free techniques to compute analytic robot control inputs by utilizing knowledge of the robot 
kinematic and/or dynamic model, as well as the communication graph (Oh et al., 2015). 
Model-based approaches rely on a mathematical model of the system (e.g., based on the 
Newtonian, Lagrangian, or Hamiltonian formalism) and some linearization around a fixed 
point or periodic orbit.  
Model-based approaches can be implemented efficiently in real-time, but their 
reliance on model accuracy and communication reliability make their performance 
vulnerable to the system and environment uncertainties and disturbances (Jiang et al., 
2019). Conversely, the model-free techniques do not rely on any mathematical model of 
the control system. Instead of first-principles modeling, they describe complex systems 
using observational data collected directly by embedded sensors. 
The demand for adaptive and intelligent robot controls is increasing as the 
complexity of formation control application domains grows, which is the primary 
motivation of this thesis for adapting learning-based control in multi-robot systems. 
Learning-based control aims to incorporate the benefits of both the model-based and 
model-free control paradigms into a single hybrid control strategy. 
The demand for adaptive and intelligent robot controls is increasing as the 
complexity of formation control application domains grows, which is the primary 
motivation of this thesis for adapting learning-based control in multi-robot systems. 
Learning-based control aims to incorporate the benefits of both the model-based and 
model-free control paradigms into a single hybrid control strategy. 
1.2.2. Related Work in Learning-based Formation Control 
The application of RL in formation control and there has been several successful studies 
using RL techniques: Wen et al. (2017) suggested a fuzzy logic reinforcement learning 
(FRL) control technique that deals with issues like unknown dynamics and inherent 

4 
nonlinearity. Broecker et al. (2018) presented a fully functional system that demonstrates 
decentralized coordination on micro aerial vehicles (MAVs) using distance information. 
They used a hybrid model by merging a recurrent neural network (RNN) and a DQN. RNN 
carries the bearing information, and DQN is responsible for moving actions to avoid 
collisions and reach the desired position.  
Xiao et al. (2019) propose a simultaneous target assignment and path planning 
(STAPP) technique based on the multi-agent deep deterministic policy gradient 
(MADDPG) algorithm for multi-UAV formation transformation. The suggested method 
can effectively deal with dynamic environments as its execution-only requires UAVs, 
targets, and threat areas.  
Li & Liang (2020) suggested a path planning method based on prior knowledge and 
a Q-learning algorithm to address low operational efficiency and slow learning speed in 
existing multi-robot coordination and collision avoidance systems. This method can be 
divided into two parts. First, the improved Q-learning method is used to design a single 
robot’s static collision-free path for a single robot. Then, the Q-learning algorithm is used 
to produce conflict-free motion among several robots using the Q-table (with the 
preliminary information collected in the previous stage). Nguyen et al. (2020) employed a 
hierarchical deep deterministic policy gradients (HDDPG) framework to decompose a 
complex shepherding problem of ground-air vehicles and train the UAV agent to obtain 
the desired behavior.  
Wu et al. (2020) proposed a dueling DQN method for the autonomous navigation 
and obstacle avoidance of USVs. Xiao et al. (2020) proposed a macro-action-based 
decentralized multi-agent deep double-Q learning approach (MacDec-MADDRQN) and 
Parallel-MacDec-MADDRQN to improve the learning of decentralized policies in multi-
robot systems. These methods enable each agent’s decentralized Q-network to be trained 
while capturing the effects of other agents’ actions by using a centralized Q-network for 
decentralized policy updating.  
Xu et al. (2020) employed the DQN algorithm in swarm communication aware 
formation control with target searching function. Their simulation results show that the 

5 
trained model with state observation space and one thrust action space could be applied in 
the larger swarm system’s group formation and target point tracking.  
Yan et al. (2020) proposed an RL framework with centralized training and 
decentralized execution manner using PPO algorithm to control UAV swarm flocking. 
They formulated the UAV flocking control problem as a partially observable Markov 
decision process (POMDP) where the constraints of the UAV’s communication and 
perception ranges are considered.  
Zhang et al. (2020) proposed a decentralized control approach for a multi-robot 
system using a DQN controller for robust performance against uncertainties to accomplish 
an oversized object transportation task. The computing bottleneck is deliberately avoided 
using this method since analogous controllers are distributed across robots. The presented 
multi-robot system learns abstract features of the task and exhibits cooperative behaviors.  
Zhu et al. (2020) solved the flocking control challenge of multi-robot systems in 
complex environments with dynamic impediments using DRL. They employed a 
MADDPG algorithm, which uses the information of multi-robots in the learning process 
to predict better the actions that robots will take. Moreover, they used a priority evaluation 
function to determine which experiences are sampled preferentially from the replay buffer 
using TD error. 
Bai et al. (2021) proposed an improved PPO algorithm for multi UAV formation 
by combining a centralized and a decentralized network. The proposed model reduces the 
action space, accelerates convergence, and adds more diversity for decision-making agents. 
Wang et al. (2021) employed a leader-follower-based formation control of USVs using the 
DDPG.  
Please refer to Singh et al. (2021) for a comprehensive review of trajectory tracking 
control of mobile robots. 
1.2.3. Human-Swarm Interaction (HSI) – in Robotics 
Human-swarm interaction (HSI) is a relatively young field investigating different methods 
for interacting with multi-robot systems. Most studies in HSI have adopted specific 

6 
problem scenarios to driven the development of particular mechanisms and modalities 
(Rule & Forlizzi, 2012). 
Vasile et al. (2011) proposed a multi-agent control architecture for swarm robotics 
applications using an innovative HSI interface with a graphical user interface and a pair of 
local and social agents.  
Nagi et al. (2014) presented a machine vision-based approach for localization and 
positioning tasks in HSI using spatial gestures, where human operators need to select 
individuals and groups of autonomous robots from a swarm of UAVs.  
Crandall et al. (2017) developed a shared-control by merging operator input with 
the swarm behavior to maintain fault-tolerant attribution of robot swarms while giving the 
operator adequate control to assure mission objectives. Diaz-Mercado et al. (2017) 
proposed using time-varying density functions to externally influence a robot swarm, 
which allows a human operator to design densities to manipulate the robot swarm as a 
whole instead of at the individual robot level. Suresh & Martínez (2020) developed a novel 
HSI framework for formation control using the notion of an interpreter, enabling the user 
to control a robotic swarm’s shape and formation using abstraction.  
Kakish et al. (2021) presented a HIS model that can be used to change the 
macroscopic behavior of a swarm of robots with decentralized sensing and control using 
hand gesture recognition CNN. Serpiva et al. (2021) developed a novel swarm control 
interface in which an operator leads the swarm by path drawing and formation control with 
the DNN-based gesture interface and trajectory generation systems.  
Please refer to Hussein & Abbass (2018) for a comprehensive review of HSI 
applications in multi-robot systems. 
1.2.4. Related Work in Human-Swarm Interaction (HSI) for Aesthetic Control 
– Swarm Art 
Human-Swarm Interaction (HSI) and swarm art has become an object of study as both 
researchers and artists push the boundaries of the traditional conceptions of different forms 
of art (Moura, 2002; Greenfield & Machado, 2014 & 2015; Moura, 2016). Commonly 
practiced form of swarm art are swarm music and improvisation with multi-swarms 

7 
(Blackwell, 2003 & 2007; Jones, 2008), pre-programmed musical compositions that 
emulates human playing (Bisig & Kocher, 2013), or interactively playing or improvising 
alongside people (Unemi & Bisig, 2005; Ventura & Bisig, 2016).  
Dance and choreography have also been explored in swarm art, where swarm 
agents follow dancer(s)’s movement and gestures (Bisig & Ventura, 2016), exploring 
stylistic patterns (Bisig & Palacio, 2012), and moving in real time according to music 
(Bisig et al., 2011). Movement and engaging interactions between humans and swarms has 
not been limited to dance, being also the object of study in the context of paintings (Moura 
& Ramos, 2007; Wanner, 2015; Moura, 2018), digital drawings (Monmarché et al., 2003; 
Urbano, 2006; Machado & Pereira, 2012; Santos & Egerstedt, 2020), and audio/visual 
performances (Carvalho, 2010; Mauceri & Majercik, 2017; Tatar et al., 2018). 
The aesthetic possibilities of robotic swarms have also been investigated on a 
smaller scale in the context of choreography or music (Alonso-Mora et al., 2014; Ackerman, 2014; 
Schoellig et al., 2014; Karimov et al., 2017), or interactive music generation based on the 
interactions between agents (Albin et al., 2012; Krzyżaniak, 2021). 
In this thesis, we explore formation (i.e., flocking, foraging) as an interaction 
modality between human artist and robotic swarms. The results are a series of experimental 
noise music called Liminal Tones (A / Autumn Swarm) and Liminal Tones (B/ Rain 
Dream), which we present in Chapter 4.  
1.3. 
Motivation and Research Questions 
Model-based control of multi-robot systems, particularly in unknown and complex 
environments or non-Markovian* dynamics, where robots need to adapt to their 
environment and interact with their neighbors, rather than taking static policies, is a 
difficult task and open research problem. This is more challenging for multi-robot systems 
and tasks with continuous state and action spaces (i.e., UAV trajectory tracking).  
                                                
* Any interaction between a system and its environment that later affects it is referred to as non-
Markovian dynamics (White et al., 2020). 

8 
There are three fundamental open challenges in the multi-robot control domain. 
The critical issue is the dynamic environment. In particular, where robots are working in 
an environment with dynamic random obstacle distribution (the obstacles may appear and 
disappear randomly). Furthermore, since numerous robots are working in parallel, while 
one robot is making decisions, other agents† may change the environment. Therefore, from 
the perspective of a single agent, the environment is dynamic. Because there will be hidden 
environment states that are unobservable to the agents, making decisions and learning 
optimal policies is very difficult in dynamic environments. 
Another common challenge in multi-robot control is the limited perception. 
Usually, each robot can only detect objects or barriers within a limited boundary. 
Therefore, the global environment is unknown to induvial agents. This is extremely more 
difficult in the presence of noise since the robots usually have unreliable communication 
capabilities. For example, before a robot moves close to an obstacle, it does not know its 
shape or exact location. It may select a wrong coordination strategy in the presence of 
disturbances. 
The unknown environment is another major challenge in the multi-robot domain. 
The robots have no prior knowledge about the global environment and its states – either 
due to the limited perception capabilities and dynamic changes in the environment – 
Therefore, the traditional planning strategies are not successful for mapping and decision 
making. 
To properly deploy multi-robot control and deal with the robots’ limited perception, 
communication, and disturbances, unique methodologies with boundless states and actions 
are required. For example, Alonso-Mora et al., 2017 proposed a technique to model the 
robot’s internal behavior or relationship-state estimation for formation control and object 
transportation in a dynamic environment.  Similarly, Zhu et al., 2019 suggested a state 
vector and distributed consensus for splitting and merging in dynamic environments. 
Another promising approach is to employ different machine learning techniques for 
self-deterministic, self-learning coordination, and coping with environmental uncertainty. 
                                                
† Agent and robot in this thesis are used interchangeably. 

9 
In particular, due to the fast development of deep reinforcement learning (DRL) and its 
successful application in multi-agent systems, many scholars consider different DRL 
algorithms for multi-robot control (Fan et al., 2018; Sampedro et al., 2018). DRL combines 
RL techniques with deep neural networks (DNNs) and is equipped with dynamic function 
approximation and representation learning properties to solve different continuous 
problems. Embedding learning techniques in multi-robots can encourage efficiency and 
cooperation between agents. Building on these motivations, this thesis focusses on the 
following research questions: 
RQ1: How to best design a distributed or decentralized controller to autonomously 
control a multi-robot system? 
RQ2: How to design a distributed or decentralized controller for consensus 
formation in multi-robot systems? 
RQ3: What are the aesthetic possibilities of swarm control approaches for artistic 
creation (i.e., musical interventions)? 
Following our initial research questions, this thesis focuses on two main foci: 
1.3.1. Thesis Focus 1 (TF1): Learning-based Formation Control of Swarm 
Robots 
Emergent flocking behavior has been of particular interest in the field of multi-robot 
control, and several applications for robotic flocking have been proposed, including mobile 
sensing networks, coordinated delivery, reconnaissance, and surveillance. Olfati-Saber 
(2007) presented flocking and multi-agent dynamic systems, focusing on algorithms and 
fundamental questions that a flocking model may need to address, including Scalability: 
How can the model deal with large numbers of agents? Formation Control: how does the 
flock keep its formation? Collision Avoidance: How can a flock avoid collision with both 
static and dynamic obstacles? 
Several researchers have developed different strategies regarding these points; 
more recent studies have used machine learning techniques. Raja et al. (2019) proposed an 
algorithm for optimal flocking control and collision-free trajectory tracking of UAVs using 

10 
one of the most popular algorithms in RL, deep Q-learning, where each UAV has the 
knowledge of other UAVs coordinates and attitude.  
Mehmood et al. (2020) proposed a distributed neural flocking controller based on 
supervised learning that uses deep learning from a centralized flocking controller. They 
achieved excellent performance and efficiency by combining a model predictive control 
(MPC)-based controller with learning features and distributed neural control capable of 
different attributes, including collision avoidance, obstacle avoidance, predator avoidance, 
and target seeking. Saulnier et al. (2017) described a graph approach that allows mobile 
robot teams to maintain robust formation in the presence of non-cooperative (defective or 
malicious) robots.  
Alternatively, Yu et al. (2020) presented an adaptive controller for region-based 
flocking control of mobile robots with communication delays, using a self-tuning adaptive 
control gain. They demonstrated that if the network topology graph is connected under 
particular initial position conditions, all the robots can always reach the objective region, 
achieve velocity matching, and avoid collision using the proposed control approach. 
However, to the best of our knowledge, little emphasis has been placed on adaptive 
flocking formation, such as reacting and adapting the formation according to the dynamic 
changes in the environment. Furthermore, despite the current advances and research, 
designing an adaptive control that can learn and perform in a complex environment is still 
an open problem.  
The main focus of this thesis (based on RQ1 & RQ2) is using a learning-based 
technique for adaptive control in multi-robot systems. So, we present an agent-based neural 
control for the flocking formation of a UAV swarm. In particular, we consider the problem 
of flocking formation in a dynamic environment (with obstacles) and offer a solution using 
a DRL algorithm. 
The flocking formation is formulated as a distributed control using leader-follower 
consensus formation. Each UAV’s distributed control policy is created using its neighbors’ 
relative position and velocity vectors (interaction matrix). Furthermore, we employ a 
decentralized and distributed control with a unique leader, where the formation trajectory 
is known to a portion of the UAVs called the leader group. 

11 
1.3.2. Thesis Focus 2 (TF2): Human-Swarm Interaction (HSI) and Aesthetic 
Exploration of Swarm Formation  
The second thesis focus (based on RQ3) is the research and design modalities interaction 
for aesthetic exploration of swarm formation (i.e., noise music). Here, we aim to provide a 
context for co-creation and explore the complex agency between humans and robots.  
Exploring swarm aesthetics is not new; in fact, there is a genre known as swarm 
art, which we briefly discuss in Section 4.2. For a comprehensive review of swarm art, 
please refer to our book listed in our publications (PA1). 
Swarm music is a commonly known subset of swarm art to create audio artworks, 
including music compositions, improvisations, orchestras, soundscapes, etc., as practiced 
by pioneers such as Tim Blackwell, Daniel Bisig. However, most previous application of 
swarm techniques was done using digital systems and focused on creating digital sounds. 
In this thesis, derived from our interest in robots and swarming behaviors, we study 
the application of swarms to create unique audio outputs using physical robots. We further 
explore the possibilities of interaction modalities between an artist and physical swarms 
(robots) for co-creation and improvisation. The results are a series of noise music knows 
as Liminal Tones (A / Autumn Swarm) and Liminal Tones (B/ Rain Dream), which we 
describe in detail in Sections 4.3 and 4.4. 
Note: It is essential mentioning that TF1 emerged from TF2. We began our 
research in swarm robotics and formation control and explored the use of swarm control in 
art (swarm art) by using conventional techniques (behavior-based control, e.g., foraging). 
However, manual control of swarm robots was challenging and limited. As a result, we 
studied the advances in multi-robot control and considered a data-driven methodology 
based on the machine learning paradigm to design a neural controller. We then developed 
the neural controller for a more challenging problem (collision-free flocking formation of 
UAV swarm) in a complex setting (continuous states and actions). 

12 
1.4. 
Contributions  
This thesis utilizes conventional and learning-based approaches and investigates relevant 
research issues for the formation control of autonomous multi-robot systems (i.e., 
quadrotor UAVs). In particular, we make original contributions in multi-robot formation 
and learning for robot decision-making and offer a solution to the learning and control 
issues in cooperative multi-robot systems with continuous states and actions. This will help 
improve the system’s autonomy and performance, particularly in unknown and complex 
environments.  
Furthermore, the thesis surveys different adaptive and learning-based control and 
swarm techniques and comprehensively reviews the state-of-the-art for future 
developments.  
In this thesis, we make two kinds of contributions. The first contribution is research-
creation in the domain of coordinated multi-robot systems that support formation control 
using a learning-based approach (TF1). The second contribution is the artistic investigation 
of formation control in HSI by utilizing different swarm techniques (i.e., particle swarm 
optimization) and its application in music improvisations (TF2). In the next section, we 
provide more details about each thesis focus and our contributions. 
1.4.1. Formation Control of UAV Swarm  
Problem: collision-free flocking control of UAV swarm in a dense environment 
The main contribution of this thesis (based on TF1) is a learning-based algorithm for 
adaptive control of multi-robot systems using reinforcement learning (RL). We develop a 
collision-free flocking formation control for UAV swarm in a dense environment using 
DNNs. 
The analysis of the underlying theoretical frameworks (multi-robot formation, 
learning-based, on and off policy approaches, and recent advances in learning-based 
formation control) of 50+ research papers shape the scope of what is valued and evaluated 
within the framework of a literature review and taxonomy (Chapter 2). This analysis 

13 
expands in Chapter 3 with a particular focus on a learning-based approach for formation 
control. 
The design and validation of flocking formation control using a DRL algorithm, 
called deep deterministic policy gradient (DDPG). We describe the architecture and 
validate our approach using the Reinforcement Learning Toolbox in MATLAB and 
Simulink (Chapter 3). 
1.4.2. Swarm Aesthetics 
Problem: interaction between artists and robotic swarms for music improvisation 
Our second thesis focus (based on RQ3) is researching and designing interaction 
modalities for aesthetic creation through embodied interactions between human and multi-
robot systems. Our goal here is to provide a context for co-creation and offer a complex 
agency between humans and robots by utilizing different swarm techniques and collective 
behaviors. 
We first did a comprehensive review on swarm art and analyzed 120+ swarm 
systems which act as the foundation for Chapter 4 and are published as part of a book listed 
in our publications (PA1). This taxonomy influenced our artistic investigation in Chapter 
4, focusing on noise as music and sound objects. 
Then we utilized a common swarm technique, called particle swarm optimization 
(PSO), and a typical PID controller to control a group of musical objects (made of DC 
motors) that can sync and swarm together and generate sounds. In this study, the robots are 
fixed in a position. The artist can control DC motors’ low-level parameters (i.e., speed, 
orientation), which influences the overall behavior of the robot swarm and the generated 
sounds. We then further explored swarm aesthetics utilizing another swarm technique 
commonly known as random walk to controlling a group of mobile robots (modified 
BristleBots) that can move around an environment and generate sounds (Chapter 4). The 
methods and the results are given here expand on swarm art and present the artistic 
promises of swarm robotics and distributed systems. We demonstrate the creative 
possibilities of robotic swarms and challenge the traditions and mind-body dualism in art-
making. 

14 
1.4.3. Publications 
The results of this cumulative thesis were published in peer-reviewed journals, a book, and 
conference papers listed below. P# indicates the index of the main publications, while PA# 
denotes the supplementary publications connected to the works included in the appendixes. 
Table 1.1 lists the contributions of publications and their relevance to the research 
questions. 
Book & Journal Papers 
P1: Salimi, Mahsoo, & Pasquier Philippe (2021). Advances in the formation of 
multi-robot systems using deep reinforcement learning and neural networks. Submitted to 
the 2021 Proceedings of the International Journal of Dynamics and Control. 
PA1: Salimi, M. (2021). “Swarm Systems in Art and Architecture: Taxonomy and 
State-of-the-Art.” in 2021 Proceedings of the Computational Synthesis and Creative 
Systems series. Springer Nature. Link: https://www.springer.com/gp/book/9789811643569 
Conference Papers 
P2: Salimi, Mahsoo, & Pasquier Philippe (2021). “Deep Reinforcement Learning 
for Flocking Control of UAVs in Complex Environments.” in 2021 Proceedings of the of 
the 6th International Conference on Robotics and Automation Engineering (ICRAE).  
P3: Salimi, M., Pasquier, P. (2021). “Liminal Tones: Swarm Aesthetics and 
Materiality in Sound Art.” in 2021 Proceedings of the International Conference on Swarm 
Intelligence (ICSI’21). Link: https://link.springer.com/chapter/10.1007/978-3-030-78743-1_5 
P4: Salimi, M., Pasquier, P. “Exploiting Swarm Aesthetics in Sound Art.” in 
Proceedings of the Art Machines 2: International Symposium on Machine Learning and 
Art 2021, Art Machines 2. Link: https://deepai.org/publication/exploiting-swarm-aesthetics-in-
sound-art 
P5: Salimi, Mahsoo, & Pasquier Philippe (2021). A reinforcement learning 
approach for controlling a fleet of UAVs. Submitted to the 2021 Proceedings of the 
International Journal of Control, Automation and Systems. 

15 
PA2: Salimi, M., Abukhodair, N., DiPaola, St., Castellanos, C., Pasquier, P. (2020). 
“Liminal Scape, an interactive visual installation with expressive AI.” In 2020 Proceedings 
of the 26th International Symposium on Electronic Art (ISEA). Link: https://isea2020.isea-
international.org/PROCEEDING_041120_LR.pdf 
1.5. 
Organization of the Thesis 
Table 1 shows the thesis structure and our contributions, and how they relate to the 
publications. This thesis is organized as follows:  
In Chapter 1, we introduce formation control and review learning-based techniques 
in the context of multi-robot systems. We present the central concept of consensus and 
different formation configurations (i.e., leader-follower scheme), followed by an overview 
of the recent development using various machine learning paradigms. The main emphasis 
is on RL for adaptive control, and we introduce commonly used algorithms in multi-robot 
systems and present our research focus and contributions. In addition, the remainder of the 
thesis chapters has been outlined in Chapter 1 as navigation the rest of the thesis.  
In Chapter 2, we present the advances in the formation of multi-robot systems in 
the literature using DRL and neural networks (NNs). Moreover, we outline the challenges 
and open research questions for future work. This Chapter presents the content of the P1 
paper.  
Note about the authorship: Mahsoo Salimi leads this review and works on the 
submitted paper, including conducting an initial literature review, writing the first draft, 
creating the figures and taxonomy tables, and revised the camera-ready. 
Subsequently, Chapter 3 presents a novel collision-free flocking formation of UAV 
swarm in the dense environment using DRL. The formation problem is formulated as a 
partially observable Markov decision process (POMDP), in which each UAV only has 
partial knowledge about the environment due to communication and perception limits. We 
employ a hybrid approach using centralized training and decentralized execution to 
maintain the flock and optimize the learning control. The data used in this Chapter is 
extracted from research submitted in P2. 

16 
Note about the authorship: Mahsoo Salimi leads this research on the submitted 
paper, including writing the code, conducting the simulation, writing the first draft, 
presenting the results, and revised the camera-ready. 
Chapter 4 presents our investigation in HSI in the context of art and for artistic 
expressions. In particular, we explore different swarm techniques (I.e., flocking, foraging) 
using particle swarm optimization (PSO) and random walk to control a team of robots and 
create new acoustic aesthetics. This Chapter presents the content of the P3 and P4 papers. 
Moreover, Appendixes C and D offer the supplementary and related material and our 
survey in swarm art and exploration in AI art. 
Note about the authorship: Mahsoo Salimi leads these works on the published 
papers, including designing and fine-tuning the robots, testing the system, and recording 
the audio materials, writing the first draft, presenting the results, and revised the camera-
ready. 
Finally, we conclude this thesis by outlining the main contributions and remarks on 
the development and implementation of learning-based techniques for adaptive control for 
swarm robots, as well as future recommendations.  
The preliminaries for Chapters 3 and 4 are included to offer background 
information and act as additional materials. Finally, the appendix content serves two 
purposes: first, it contains supplementary material connected to the Chapters. Second, 
appendices include sections from my book and a paper that are not directly related to the 
thesis but provide context.  
1.5.1. Appendix Outline  
Appendix B: this appendix presents the supplementary material for Chapter 3. 
Relation to the Thesis: the material presented in this section offers essential 
information and familiarizes the reader with the underlying foundation of motion 
(quadrotor) for the flocking formation problem. 
Appendix C: this appendix presents a section of my book publication listed in PA1. 

17 
Relation to the Thesis: this book serves as the foundation and the framework for 
our investigation in swarm art and the second part of the thesis (Chapter 4).  
Appendix D: this appendix presents the publication listed in PA2. 
Relation to the Thesis: this art paper explores AI art (digital painting) using deep 
learning related to Chapter 4 and our investigation in swarm art. To this end, we briefly 
tested and compared the emerging trends (e.g., Generative adversarial network (GAN)) and 
swarm (e.g., flocking, foraging) techniques for painting and music.  
 
 

18 
Table 1.1. 
Thesis outlines 
 
Chapter 
 
Contributions 
Publication  
Ch 1 
 
Introduction and Overview 
 
Ch 2 
RQ1, 
TF1 
Advances in Formation of 
Multi-robot Systems using 
Reinforcement Learning and 
Neural Networks 
International Journal of Dynamics 
and Control 
Ch 3 
RQ2, 
TF1 
Deep Reinforcement Learning 
for Flocking Control of UAVs 
in Complex Environments 
ICRAE 2021 
Ch 4 
RQ3, 
TF2 
Exploiting Swarm Aesthetics in 
Sound Art 
Art Machines 2 
Swarm Aesthetics and 
Materiality in Sound 
Composition 
ICSI 2021 
Ch 5 
 
Discussion and Future Work 
 
Appendix A 
 
SIAT Guidelines for Writing a 
Cumulative Thesis 
 
Appendix B 
 
Quadcopter Equations of 
Motion  
 
Appendix C 
 
Swarm Systems in Art and 
Architecture- State of the Art 
Computational Synthesis and 
Creative Systems 
Appendix D 
 
Liminal Scape, an interactive 
visual installation with 
expressive AI 
ISEA 2020 
 
 
 

19 
Bibliography 
Ackerman, E. (2014). Flying LampshadeBots Come Alive in Cirque du Soleil. iEEE 
Spectrum. 
Albin, A., Weinberg, G., & Egerstedt, M. (2012, October). Musical abstractions in 
distributed multi-robot systems. In 2012 IEEE/RSJ International Conference on 
Intelligent Robots and Systems (pp. 451-458). IEEE. 
Alonso-Mora, J., Baker, S., & Rus, D. (2017). Multi-robot formation control and object 
transport in dynamic environments via constrained optimization. The International 
Journal of Robotics Research, 36(9), 1000-1021. 
Alonso-Mora, J., Siegwart, R., & Beardsley, P. (2014, March). Human-robot swarm 
interaction for entertainment: From animation display to gesture based control. In 
Proceedings of the 2014 ACM/IEEE international conference on Human-robot 
interaction (pp. 98-98). 
Aupetit, S., Bordeau, V., Monmarché, N., Slimane, M., & Venturini, G. (2003, December). 
Interactive evolution of ant paintings. In The 2003 Congress on Evolutionary 
Computation, CEC’03. (Vol. 2, pp. 1376-1383). IEEE. 
Bai, X., Lu, C., Bao, Q., Zhu, S., & Xia, S. (2021). An Improved PPO for Multiple 
Unmanned Aerial Vehicles. In Journal of Physics: Conference Series (Vol. 1757, 
No. 1, p. 012156). IOP Publishing. 
Bisig, D., Kocher, P. (2013). Trails II. In Proceedings of the Consciousness Reframed 
Conference. Cairo, Egypt. 
Bisig, D., Palacio, P. (2012). STOCOS-Dance in a Synergistic Environment. In 
Proceedings of the Generative Art Conference (Lucca, 2012). 
Bisig, D., Schacher, J. C., Neukom, M. (2011). Flowspace-A Hybrid Ecosystem. In NIME 
(pp. 260-263). 
Blackwell, T. (2003). Swarm music: improvised music with multi-swarms. Artificial 
Intelligence and the Simulation of Behaviour, University of Wales, 10, 142-158. 
Blackwell, T. (2007). Swarming and music. In Evolutionary Computer Music (pp. 194-
217). Springer, London. https://doi.org/10.1007/978-1-84628-600-1_9 
Broecker, B., Tuyls, K., & Butterworth, J. (2018, May). Distance-based multi-robot 
coordination on pocket drones. In 2018 IEEE International Conference on Robotics 
and Automation (ICRA) (pp. 6389-6394). IEEE. 

20 
Carvalho, 
R. 
(2010). 
Dance 
with 
the 
Swarming 
Particles. 
https://www.academia.edu/5566193/Dance_with_the_Swarming_Particles_Projec
t_Report 
Crandall, J. W., Anderson, N., Ashcraft, C., Grosh, J., Henderson, J., McClellan, J., ... & 
Goodrich, M. A. (2017, July). Human-swarm interaction as shared control: 
Achieving flexible fault-tolerant systems. In International conference on 
engineering psychology and cognitive ergonomics (pp. 266-284). Springer, Cham. 
Diaz-Mercado, Y., Lee, S. G., & Egerstedt, M. (2017). Human–swarm interactions via 
coverage of time-varying densities. Trends in Control and Decision-Making for 
Human–Robot Collaboration Systems, 357-385. 
Fan, T., Long, P., Liu, W., & Pan, J. (2018). Fully distributed multi-robot collision 
avoidance via deep reinforcement learning for safe and efficient navigation in 
complex scenarios. arXiv preprint arXiv:1808.03841. 
Greenfield, G., & Machado, P. (2014). Swarm art. Leonardo, 47(1), 5-7. 
Greenfield, G., & Machado, P. (2015). Ant-and ant-colony-inspired alife visual art. 
Artificial life, 21(3), 293-306. https://doi.org/10.1162/ARTL_a_00170 
Hussein, A., & Abbass, H. (2018, October). Mixed initiative systems for human-swarm 
interaction: Opportunities and challenges. In 2018 2nd Annual Systems Modelling 
Conference (SMC) (pp. 1-8). IEEE. 
Jones, D. (2008, March). AtomSwarm: a framework for swarm improvisation. In 
Workshops on Applications of Evolutionary Computation (pp. 423-432). Springer, 
Berlin, Heidelberg. https://doi.org/10.1007/978-3-540-78761-7_45 
Kakish, Z., Vedartham, S., & Berman, S. (2021). Towards Decentralized Human-Swarm 
Interaction by Means of Sequential Hand Gesture Recognition. arXiv preprint 
arXiv:2102.02439. 
Karimov, A. I., Pesterev, D. O., Ostrovskii, V. Y., Butusov, D. N., & Kopets, E. E. (2017, 
September). Brushstroke rendering algorithm for a painting robot. In 2017 
International Conference Quality Management, Transport and Information 
Security, Information Technologies"(IT&QM&IS) (pp. 331-334). IEEE. 
Krzyżaniak, M. (2021). Musical robot swarms, timing, and equilibria. Journal of New 
Music Research, 1-19. 
Kuriki, Y., & Namerikawa, T. (2014). Formation control of UAVs with a fourth-order 
flight dynamics. SICE Journal of Control, Measurement, and System Integration, 
7(2), 74-81. 

21 
Li, B., & Liang, H. (2020, October). Multi-robot path planning method based on prior 
knowledge and Q-learning algorithms. In Journal of Physics: Conference Series 
(Vol. 1624, No. 4, p. 042008). IOP Publishing. 
Machado, P., & Pereira, L. (2012, July). Photogrowth: non-photorealistic renderings 
through ant paintings. In Proceedings of the 14th annual conference on Genetic 
and 
evolutionary 
computation 
(pp. 
233-240). 
ACM. 
https://doi.org/10.1145/2330163.2330197 
Mauceri, F., & Majercik, S. M. (2017, April). A Swarm Environment for Experimental 
Performance and Improvisation. In International Conference on Evolutionary and 
Biologically Inspired Music and Art (pp. 190-200). Springer, Cham. 
https://doi.org/10.1007/978-3-319-55750-2_13 
Mehmood, U., Roy, S., Grosu, R., Smolka, S. A., Stoller, S. D., & Tiwari, A. (2020, April). 
Neural flocking: MPC-based supervised learning of flocking controllers. In 
International conference on foundations of software science and computation 
structures (pp. 1-16). Springer, Cham. 
Moura, L. (2002). Swarm paintings-non-human. ARCHITOPIA Book, Art, Architecture 
and Science, 1-24. 
Moura, L. (2016). Machines that make art. In Robots and Art (pp. 255-269). Springer, 
Singapore. 
Moura, L. (2018, September). Robot Art: An Interview with Leonel Moura. In Arts (Vol. 
7, 
No. 
3, 
p. 
28). 
Multidisciplinary 
Digital 
Publishing 
Institute. 
https://doi.org/10.3390/arts7030028 https://doi.org/10.3390/arts7030028 
Moura, L., & Ramos, V. (2007). Swarm paintings-nonhuman art. ARCHITOPIA book, art, 
architecture and science, 5-24. 
Nagi, J., Giusti, A., Gambardella, L. M., & Di Caro, G. A. (2014, September). Human-
swarm interaction using spatial gestures. In 2014 IEEE/RSJ International 
Conference on Intelligent Robots and Systems (pp. 3834-3841). IEEE. 
Nguyen, H. T., Nguyen, T. D., Tran, V. P., Garratt, M., Kasmarik, K., Anavatti, S., ... & 
Abbass, H. A. (2020). Continuous deep hierarchical reinforcement learning for 
ground-air swarm shepherding. arXiv preprint arXiv:2004.11543. 
Oh, K. K., Park, M. C., & Ahn, H. S. (2015). A survey of multi-agent formation control. 
Automatica, 53, 424-440. 
Olfati-Saber, R., Fax, J. A., & Murray, R. M. (2007). Consensus and cooperation in 
networked multi-agent systems. Proceedings of the IEEE, 95(1), 215-233. 

22 
Pratama, B., Muis, A., & Subiantoro, A. (2019, October). Improved Distributed Formation 
Control and Trajectory Tracking of Multi Quadrotor in Leader-Follower 
Formation. In 2019 5th International Conference on New Media Studies 
(CONMEDIA) (pp. 129-134). IEEE. 
Raja, G., Anbalagan, S., Narayanan, V. S., Jayaram, S., & Ganapathisubramaniyan, A. 
(2019, October). Inter-UAV Collision Avoidance using Deep-Q-Learning in 
Flocking Environment. In 2019 IEEE 10th Annual Ubiquitous Computing, 
Electronics & Mobile Communication Conference (UEMCON) (pp. 1089-1095). 
IEEE. 
Reyes, L. A. V., & Tanner, H. G. (2014). Flocking, formation control, and path following 
for a group of mobile robots. IEEE Transactions on Control Systems Technology, 
23(4), 1268-1282. 
Rule, A., & Forlizzi, J. (2012, March). Designing interfaces for multi-user, multi-robot 
systems. In Proceedings of the seventh annual ACM/IEEE international conference 
on Human-Robot Interaction (pp. 97-104). 
Sampedro, C., Rodriguez-Ramos, A., Gil, I., Mejias, L., & Campoy, P. (2018, October). 
Image-based visual servoing controller for multirotor aerial robots using deep 
reinforcement learning. In 2018 IEEE/RSJ International Conference on Intelligent 
Robots and Systems (IROS) (pp. 979-986). IEEE. 
Santos, M., & Egerstedt, M. (2020). From motions to emotions: Can the fundamental 
emotions be expressed in a robot swarm?. International Journal of Social Robotics, 
1-14. 
Saulnier, K., Saldana, D., Prorok, A., Pappas, G. J., & Kumar, V. (2017). Resilient flocking 
for mobile robot teams. IEEE Robotics and Automation letters, 2(2), 1039-1046. 
Schoellig, A. P., Siegel, H., Augugliaro, F., & D’Andrea, R. (2014). So you think you can 
dance? Rhythmic flight performances with quadrocopters. Controls and Art, 73-
105. 
Serpiva, V., Karmanova, E., Fedoseev, A., Perminov, S., & Tsetserukou, D. (2021). 
SwarmPaint: Human-Swarm Interaction for Trajectory Generation and Formation 
Control by DNN-based Gesture Interface. arXiv preprint arXiv:2106.14698. 
Singh, B., Kumar, R., & Singh, V. P. (2021). Reinforcement learning in robotic 
applications: a comprehensive survey. Artificial Intelligence Review, 1-46. 
Sui, Z., Pu, Z., Yi, J., & Xiong, T. (2019, July). Formation control with collision avoidance 
through deep reinforcement learning. In 2019 International Joint Conference on 
Neural Networks (IJCNN) (pp. 1-8). IEEE. 

23 
Suresh, A., & Martínez, S. (2020). Human-swarm interactions for formation control using 
interpreters. International Journal of Control, Automation and Systems, 18(8), 
2131-2144. 
Tatar, K., Pasquier, P., & Siu, R. (2018, April). REVIVE: An Audio-visual Performance 
with Musical and Visual AI Agents. In Extended Abstracts of the 2018 CHI 
Conference on Human Factors in Computing Systems (p. Art13). ACM. 
https://doi.org/10.1145/3170427.3177771  
Unemi, T., & Bisig, D. (2005). Flocking Orchestra-to play a type of generative music by 
interaction between human and flocking agents. In Proceedings of the eighth 
Generative Art Conference (pp. 19-21). 
Urbano, P. (2006, April). Consensual paintings. In Workshops on Applications of 
Evolutionary Computation (pp. 622-632). Springer, Berlin, Heidelberg. 
https://doi.org/10.1007/11732242_59 
Ventura, P., & Bisig, D. (2016). Algorithmic Reflections on Choreography. Human 
technology, 12(2). 
Wang, S., Ma, F., Yan, X., Wu, P., & Liu, Y. (2021). Adaptive and extendable control of 
unmanned surface vehicle formations using distributed deep reinforcement 
learning. Applied Ocean Research, 110, 102590. 
Wanner, A. (2015). Signature Strokes. http://pixelstorm.ch/?lang=en 
Wen, G., Chen, C. P., Feng, J., & Zhou, N. (2017). Optimized multi-agent formation 
control based on an identifier–actor–critic reinforcement learning algorithm. IEEE 
Transactions on Fuzzy Systems, 26(5), 2719-2731. 
White, G. A., Hill, C. D., Pollock, F. A., Hollenberg, L. C., & Modi, K. (2020). 
Demonstration of non-Markovian process characterisation and control on a 
quantum processor. Nature Communications, 11(1), 1-10. 
Wu, X., Chen, H., Chen, C., Zhong, M., Xie, S., Guo, Y., & Fujita, H. (2020). The 
autonomous navigation and obstacle avoidance for USVs with ANOA deep 
reinforcement learning method. Knowledge-Based Systems, 196, 105201. 
Xiao, H., & Chen, C. P. (2019). Leader-follower consensus multi-robot formation control 
using neurodynamic-optimization-based nonlinear model predictive control. IEEE 
Access, 7, 43581-43590. 
Xiao, Y., Hoffman, J., Xia, T., & Amato, C. (2020, May). Learning multi-robot 
decentralized macro-action-based policies via a centralized Q-net. In 2020 IEEE 
International Conference on Robotics and Automation (ICRA) (pp. 10695-10701). 
IEEE. 

24 
Xu, C., Zhang, K., & Song, H. (2020, November). UAV Swarm Communication Aware 
Formation Control via Deep Q Network. In 2020 IEEE 39th International 
Performance Computing and Communications Conference (IPCCC) (pp. 1-2). 
IEEE. 
Yan, P., Bai, C., Zheng, H., & Guo, J. (2020, November). Flocking Control of UAV 
Swarms with Deep Reinforcement Leaming Approach. In 2020 3rd International 
Conference on Unmanned Systems (ICUS) (pp. 592-599). IEEE. 
Yu, J., Ji, J., Miao, Z., & Zhou, J. (2020). Region-based flocking control for networked 
robotic systems with communication delays. European Journal of Control, 52, 78-
86. 
Zhang, L., Sun, Y., Barth, A., & Ma, O. (2020). Decentralized control of multi-robot 
system in cooperative object transportation using deep reinforcement learning. 
IEEE Access, 8, 184109-184119. 
Zhang, Q., & Liu, H. H. (2018). Aerodynamic model-based robust adaptive control for 
close formation flight. Aerospace Science and Technology, 79, 5-16. 
Zhao, Z., Wang, J., Chen, Y., & Ju, S. (2020). Iterative learning-based formation control 
for multiple quadrotor unmanned aerial vehicles. International Journal of 
Advanced Robotic Systems, 17(2), 1729881420911520. 
Zhu, H., Juhl, J., Ferranti, L., & Alonso-Mora, J. (2019, May). Distributed multi-robot 
formation splitting and merging in dynamic environments. In 2019 International 
Conference on Robotics and Automation (ICRA) (pp. 9080-9086). IEEE. 
Zhu, P., Dai, W., Yao, W., Ma, J., Zeng, Z., & Lu, H. (2020). Multi-robot flocking control 
based on deep reinforcement learning. IEEE Access, 8, 150397-150406. 
 
 

25 
Chapter 2. 
 
 
Learning-based Formation Control 
 
 

26 
2.1. 
Preliminaries  
2.1.1. Consensus Formation 
In the literature, several definitions of consensus are offered, such as “In networks of agents 
(or dynamic systems), consensus means to reach an agreement regarding a certain quantity 
of interest that depends on the state of all agents.” by Olfati-Saber et al., 2007.  
There are two types of consensus problems: unconstrained consensus problems and 
constrained consensus problems. Both of these problems aim at achieving an agreement 
between all robots. In general, a restricted consensus problem has an objective function 
such that the state of all robots has to converge to this function asymptotically, but in an 
unconstrained consensus problem, the state of the robots must asymptotically become the 
same without computing any objective function. The consensus algorithm goal is to reach 
a decentralized agreement via local interactions between robots. Consensus formation is 
based on graph-topological definitions of multi-robot systems (Fax & Murray, 2002) and 
can be used for motion coordination, cooperative estimation, or synchronization.  
A typical application of consensus algorithm is for flocking formation using the 
leader-follower protocol, where a team of robots will converge and follow the leader’s 
heading and flock in the same direction. Other consensus applications are rendezvous, 
alignment, trajectory tracking, synchronization (Yanumula et al., 2017; Wang et al., 2017; 
Tang et al., 2019; Najm et al., 2020). Also, many problems with dynamical and continuous 
systems are closely related to consensus problems of multi-agent systems.  
We consider a team of n robots networked in a graph 3 = (B, ℰ) in which B =
1, 2, . . , n denotes the set of nodes and ℰ ⊂ B ×  B denotes the edges. Therefore, the 
consensus algorithm can be defined as a discrete-time update: 
 
 
!9(0 + 1) =
1
|G9| + 1 h!9(0) + i
!:(0)
:∈Gk
l 
(2.1) 
 

27 
where !9 is the robot’s heading (or another global variable of interest), and G9 =
{; ∈B|(;, H) ∈ℰ} denotes the neighboring robots of robot H. 
 
 
!9(0) = 1
|B| i
!9(0),
0 →∞
9∈B
 
(2.2) 
 
The outcome of consensus is that all agents that execute the consensus protocol will 
eventually converge to the average of their initial values. The convergence rate for standard 
consensus protocols is uniformly bounded and exponentially converges to a small residual 
set (Li et al., 2014).  
A typical application of consensus algorithm is for flocking formation using the 
leader-follower protocol, where a team of robots will converge and follow the leader’s 
heading and flock in the same direction. Other consensus applications are rendezvous, 
alignment, trajectory tracking, synchronization (Yanumula et al., 2017; Wang et al., 2017; 
Tang et al., 2019; Najm et al., 2020). Also, many problems with dynamical and continuous 
systems are closely related to consensus problems of multi-agent systems.  
2.1.2. Formation Configuration  
There are different approaches for multi-robot formation control: leader-follower scheme, 
virtual leader, and behavior-based arrangements (Kowalczyk, 2001). However, the leader-
follower formation control of multi-robot systems is particularly appealing since it is 
simple and scalable (Qian & Xi, 2018; Xiao & Chen, 2019). Within the leader-follower 
configuration, some agents are designated as leaders while others are treated as followers. 
Since the behaviors of the other vehicles in the formation are entirely specified once the 
leader states are known, the leader’s states constitute the coordination variable 
(Montenegro et al., 2014). Also, the leader-follower design is relatively efficient and 
straightforward because the moving trajectory of the flock is assigned to the leader (Fax & 
Murray, 2004). In the behavior-based approach without a leader, the agent in the flock 
usually has random behaviors to overcome local maxima or minima (Balch & Arkin, 1998). 

28 
Markov Decision Process (MDP)  
The state of the robot s can be expressed in two ways: continuous and discrete. In each 
state, the controller performs an action a that results in a change of its state. These actions 
are derived from a policy function N(∙), which transfers the state to a single action N(s) →
t, in the deterministic situation. The policy function is dependent on a random variable θ 
in the stochastic case, and the mapping is expressed as a probability distribution over 
actions N(t|s, /) (Polydoros & Nalpantidis, 2017). 
The goal of an RL algorithm is to find a policy that maximizes the expected return 
with a reward function R (Sutton & Barto, 1998): 
 
 
w(s, t) = i Kxyzwx{|
}
x~z
 
(2.3) 
 
We formalize a robotic task such as navigation or trajectory tracking as a Markov 
decision process (MDP), in which the agent interacts with the environment through a 
sequence of observations, actions, and reward signals (Tai et al., 2016).  
MDP provides a logical process for predicting or planning future movement when 
facing uncertainty and can be represented by a tuple (•, [, Ä, w, K), where S is a set of states 
which describe the environment. [ is a discrete set of actions that an agent performs. 
Ä(ś|s, t) ∶• → [ describes the behaviors of the agent, which is a probabilistic 
distribution over the possible actions. w(s, ś, t) ∶ • → ℝ refers to the reward function, 
also known as reinforcement. The set of possible actions depends on the state s, denoted 
as [(s). K ∈(0 , 1) is the discount factor of the reward function.  
Two alternative value functions are introduced to solve the control problem: state-
value function YT(s), and Q-value function ZT(s, t). The value function calculates the 
predicted future return of an agent in a given state and performs according to the policy N. 

29 
N(t|s) is the stochastic policy, where actions are drawn from a probability 
distribution defined by N(t|s), and S(s) is the deterministic policy, where actions are 
deterministically selected for a given state s. 
 
 
YT(s) = ÑT Öi Kxyzwx{||sz = s
}
x~z
Ü 
(2.4) 
 
 
ZT(s, t) = áT Öi Kxyzwx{||sz = s, tz = t
}
x~z
Ü 
(2.5) 
 
where YT ∶• →ℝ, and ZT: • ×  [ → ℝ  are the state-value, and Q (action-value) 
functions and K ∈(0, 1) is the discount factor. 
In an MDP, an agent operates so that it may expect to acquire the most long-run 
value, and the factor K determines how much future rewards influence current decisions. 
The optimal value function Y∗∶• →ℝ, and optimal Q-function Z∗: • ×  [ → ℝ follow: 
 
 
Y∗(s) = ât!
T
YT(s) 
Z∗(s, t) = ât!
T
ZT(s, t) 
(2.6) 
 
Finally, the optimal policy is defined as: 
 
 
N∗(t|s) = t,äât!
ã
Z∗(s, t) 
(2.7) 
 
A policy N is considered greedy if it always assigns probability 1 to an action 
t,äât!
ã
Z∗(s, t) in state s. 

30 
To generate the optimal actions in each state, value function methods estimate an 
ideal value function (state-value or action-value). As a result, the best actions in each state 
are used to derive the policy that maximizes the long-term reward. There are four types of 
value-function methods: i) Dynamic Programming (DP) methods, which require a model 
of the transition dynamics; ii) Monte Carlo (MC) methods, which are based on sampling; 
iii) Temporal Difference Learning (TDL) methods, which consider the difference in the 
value function between two state transitions; and iv) Differential Dynamic Programming 
methods (DDP). 
Alternatively, policy search methods are used, where the optimal policy is learned 
directly rather than reconstructing a policy from the optimal value function. This property 
allows state-of-the-art policy search methods to converge faster than value-function 
methods, particularly high-DOF robotic systems.  
There are many different ways to represent policies, ranging from linear functions 
to complex Dynamic Movement Primitives (DMPs) (Deisenroth et al., 2013). Policy search 
involves approaches including i) gradient-based methods, which use hill-climbing 
approaches to update the parametrized set based on the reward function’s gradient, ii) 
Expectation-Maximization (EM) methods, which maximize the probability of the rewards 
to infer the parameters, iii) information theory approaches, which use concepts like entropy 
to derive optimal policies, iv) Bayesian optimization methods, and v) evolutionary 
computation. 
Partially Observable Markov Decision Process (POMDP) 
A partially observable Markov decision process (POMDP) is a generalization of MDP for 
modeling system dynamics with a hidden Markov model, where the agent perceives an 
observation å ∈ Ω, instead of observing s directly. The distribution of the observation 
Ä(ǻ|ś, t) depends on the current state and previous action (Kaelbling et al., 1998).  
The discrete set of observations Ω = (å| , . . . , å=) represents all possible sensor 
readings the agent can receive. The observation function can be defined as < ∶ •  ×  Ω →
 ℕ in which the observation is independent of the last action. Given the previous belief 
state, the action is taken, and the current observation, POMDP algorithms often maintain a 
belief in the present state. 

31 
Reinforcement Learning (RL) 
Reinforcement learning (RL) is a form of machine learning in which an agent interacts with 
its environment, receiving feedback through a reward function, and then adapts its 
interaction. Typically, the agent is acting to achieve a goal by determining a policy (state-
to-action).  
The state-value function of an RL problem can be stated as a state with an 
immediate reward of probable future states and their discounted value function weighted 
by transition probabilities as Bellman equations: 
 
 
YT(s) = i N(s, t) i Ä(ś|s, t)
è́
ã
êw(s, ś, t)
+ KYT(ś)ë 
(2.8) 
 
The action-value function can also be expressed as: 
 
 
ZT(s, t) = i Ä(ś|s, t)êw(s, ś, t) + KZT(ś, t́)ë
è́
 
(2.9) 
 
It can be shown that there exists an optimal policy N∗ for which the value function 
is maximized. The optimal state-value function Y∗, and action-value function Z∗ are 
defined as: 
 
 
YT∗(s) = ât!
ã∈C(è) i Ä(ś|s, t) íw(s, ś, t)
è́
+ KYT∗(ś)ì 
(2.10) 
 
The action-value function can also be expressed as: 
 

32 
 
ZT∗(s, t) = i Ä(ś|s, t) íw(s, ś, t)
è́
+ K ât!
ã́
ZT∗(ś, t́)ì 
(2.11) 
 
The reward function is the desirability of a specific state or state-action pair, which 
is defined as: 
 
 
X(N) = i K9yzw(sz, tz)
}
z~9
 
(2.12) 
 
where K ∈(0, 1) is the discount factor, and î is the number of time steps per 
learning episode.  
If the state exhibits the Markov property or could be approximated as such, then we 
can think of reinforcement learning as an MDP. If the RL agent updates the Q-function 
based on a policy to take more actions, it is considered an on-policy RL (usually stochastic).  
Off-policy RL uses a different policy to generate Q-values, with multiple policies 
(either deterministic or stochastic). The RL algorithms can be divided into three categories: 
critic-only, actor-only, and actor-critic methods.  
Critic-only methods, such as Q-learning (Watkins & Dayan, 1992), use a state-
action value function without any explicit function for the policy. This will be an 
approximate state-action value function (Grondman et al., 2012). The critic-only algorithm 
learns the optimal values by finding an approximate solution to the Bellman equation 
(O’Donoghue et al., 2018). The deterministic policy, denoted by N ∶ • ×  [ → ℝ  is 
calculated by using an optimization procedure as: 
 
 
N(s) = t,äât!
ã
Z(s, t) 
(2.13) 
 

33 
The majority of policy gradient methods are actor-only and do not use any stored 
value function., Instead, most actor-only algorithms optimize the cost function through the 
parameter space of the policy. A policy gradient method is generally obtained by 
parameterizing the policy N by the parameter vector ï ∈ wñ .  
Assuming that the parameterization is differentiable concerning ï, the gradient of 
the cost function with respect to ï is described by: 
 
 
úùX = ûX
ûNù
ûNù
ûï  
(2.14) 
 
An optimal local solution of the cost X can be found using standard optimization 
techniques, where the gradient úùX is estimated per different time steps.  
Actor-critic methods (Barto et al., 1983) integrate the benefits of both actor-only 
and critic-only approaches. Similar to actor-only methods, they can produce continuous 
actions. However, while the large variance in the policy gradients of actor-only methods is 
countered by adding a critic optimizer, the actor-critic techniques can evaluate the current 
policy prescribed by the actor. In theory, this evaluation can be done by any policy 
evaluation method such as TD(λ) (Sutton, 1988; Bertsekas, 2011), the least-squares 
temporal difference (LSTD) (Bradtke & Barto, 1996; Boyan, 2002), or residual gradients 
(Baird, 1995). 
Moreover, the critic uses samples to approximate and update the value function. 
The actor’s policies are then updated to improve the performance using the value function 
(Q). In contrast to critic-only approaches, actor-critic methods’ policy (π) is not directly 
inferred from the value function. Instead, the policy is updated in the policy gradient 
direction using a small step (meaning that a change in the value function will only result in 
a slight change in the policy). Figure 2.1 shows the schematic structure of an actor-critic 
algorithm. 
In recent years, there has been a growing interest in using DNNs as function 
approximators in RL. Several actor-critic methods have been proposed using DNNs, 

34 
including deep Q-network (DQN), deterministic policy gradient (DPG), deep deterministic 
policy gradient (DDPG) and proximal policy optimization (PPO).  
 
Figure 2.1 
Actor-critic architecture (Sutton & Barto, 1998). 
Deep Q-network (DQN)  
Deep Q-network (DQN) algorithm has proven to be highly effective at tackling complex 
problems. Using DQN in robot control systems, such as motion planning and attitude 
control, is based on knowing the current state space of the system (•), the reward value 
(w), the transition probability (Ä), and the value function (Y) (Cao et al., 2019). The Q-
function can be defined as: 
 
 
ZT(s, t) = w(s, ś, t) + K i Ä(ś|s, t)YT(ś)
è́
 
(2.15) 
 
where N is the policy value, s is the state, t is the action, and K is the discount 
factor. 
We update the policy in the policy improvement phase by a greedy search for the 
action that maximizes the Q-value at each step: 
 
TD 
error 
action 
sz,tz,sz{| 
reward 
Critic 
Actor 
Environment 
Policy 
Value 
Function 

35 
 
N9{|(s) = t,äât!
ã
ZTk(s, t) , ∀s ∈• 
(2.16) 
 
As a value-based method, DQN approximates the optimal Q-value function with a 
deep CNN, called the deep Q-network: 
 
 
Z(s, t) = w(s, ś, t) + K ât!
ã́
Z(ś, t́) 
(2.17) 
 
and the loss function is defined as: 
 
 
X(/) = °è,ã,è́ íZ(s, t) −Z_ (s, t)ì
¢
 
(2.18) 
 
Dueling DQN was presented as an enhancement of DQN, which separates the 
networks into value networks and advantage networks (Wang et al., 2016). The value 
network’s goal is to assess the quality of current and each state, while the advantage 
network evaluates the quality of the actions via: 
 
 
Z(s, t, /, £, §) = Y(s, /, §) + 
•[(s, t, /, £) −ât!
ã́ ∈|C| [(s, t́, /, £) ¶ 
(2.19) 
 
where Y(s, /, §) is a scalar output of fully-connected layers, and the other stream 
output an |[| dimensional vector [(s, t, /, £). Here, / denotes the parameters of the 
convolutional layers, while α and β are the parameters of the two streams of fully-
connected layers (Wang et al., 2016). 
for t∗= t,äât!
ã́∈|C|
Z(s, t́, /, £, §) = t,äât!
ã́∈|C|
[(s, t́, /, £): 
 

36 
 
Y(s, /, §) = Z(s, t∗, /, £, §) 
(2.20) 
 
As a result, the stream Y(s, /, §) yields a value function estimate, while the other 
stream yields an advantage function estimate. 
An alternative option it to use an average instead of the max operator: 
 
 
Z(s, t, /, £, §) = Y(s, /, §) + [(s, t, /, £) 
−1
[ i [(s, t́, /, £) 
ã́
  
(2.21) 
 
Deep Deterministic Policy Gradient (DDPG) 
The DDPG algorithm is an off-policy actor-critic algorithm, first introduced in (Lillicrap 
et al., 2015). In this algorithm, the actor and the critic are approximated by two DNNs with 
parameter vectors /], and /^ respectively. The critic is trained by minimizing the squared 
Temporal Difference (TD) error: 
 
 
úOX ≈1
™i úãZ(s, t|/])úOS(s|/^) 
(2.22) 
 
where µ(s|θ¨), and Q(s, a) is the actor and critic networks.  
Proximal Policy Optimization (PPO) 
The PPO is a policy gradient algorithm used for environments with discrete or continuous 
action spaces. PPO algorithm replaces hard constraints with flexible ones, which are 
referred to as penalties. The probability ratio ,(/) between the new and old policy is 
defined as: 
 

37 
 
,(/) = NO(t|s)
NOy|(t|s) 
(2.23) 
 
And the objective function follows: 
 
 
úX(/) = ÑT ≠ ,(/)[ÆOy|(s, t) Ø 
(2.24) 
 
 
 

38 
Bibliography 
Anderson, B. D., Fidan, B., Yu, C., & Walle, D. (2008). UAV formation control: Theory 
and application. In Recent advances in learning and control (pp. 15-33). Springer, 
London. 
Baird, L. (1995). Residual algorithms: Reinforcement learning with function 
approximation. In Machine Learning Proceedings 1995 (pp. 30-37). Morgan 
Kaufmann. 
Balch, T., & Arkin, R. C. (1998). Behavior-based formation control for multirobot teams. 
IEEE transactions on robotics and automation, 14(6), 926-939. 
Barto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that 
can solve difficult learning control problems. IEEE transactions on systems, man, 
and cybernetics, (5), 834-846. 
Bertsekas, D. P. (2011). Dynamic programming and optimal control 3rd edition, volume 
II. Belmont, MA: Athena Scientific. 
Boyan, J. A. (2002). Technical update: Least-squares temporal difference learning. 
Machine learning, 49(2), 233-246. 
Bradtke, S. J., & Barto, A. G. (1996). Linear least-squares algorithms for temporal 
difference learning. Machine learning, 22(1), 33-57. 
Cao, W., Huang, X., & Shu, F. (2019, September). Unmanned rescue vehicle navigation 
with fused DQN algorithm. In Proceedings of the 2019 International Conference 
on Robotics, Intelligent Control and Artificial Intelligence (pp. 556-561). 
Deisenroth, M. P., Neumann, G., & Peters, J. (2013). A survey on policy search for 
robotics. Foundations and trends in Robotics, 2(1-2), 388-403. 
Fax, J. A., & Murray, R. M. (2002). Graph laplacians and stabilization of vehicle 
formations. IFAC Proceedings Volumes, 35(1), 55-60. 
Fax, J. A., & Murray, R. M. (2004). Information flow and cooperative control of vehicle 
formations. IEEE transactions on automatic control,49(9), 1465-1476. 
Grondman, I., Busoniu, L., Lopes, G. A., & Babuska, R. (2012). A survey of actor-critic 
reinforcement learning: Standard and natural policy gradients. IEEE Transactions 
on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6), 
1291-1307. 

39 
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in 
partially observable stochastic domains. Artificial intelligence, 101(1-2), 99-134. 
Kowalczyk, W. (2001, October). Multi-robot coordination. In Proceedings of the Second 
International Workshop on Robot Motion and Control. RoMoCo’01 (IEEE Cat. No. 
01EX535) (pp. 219-223). IEEE. 
Li, Z., Duan, Z., & Lewis, F. L. (2014). Distributed robust consensus control of multi-agent 
systems with heterogeneous matching uncertainties. Automatica, 50(3), 883-889. 
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... & Wierstra, D. 
(2015). Continuous control with deep reinforcement learning. arXiv preprint 
arXiv:1509.02971. 
Montenegro, S., Ali, Q., & Gageik, N. (2014). A review on distributed control of 
cooperating mini UAVs. 
Najm, A. A., Ibraheem, I. K., Azar, A. T., & Humaidi, A. J. (2020). Genetic optimization-
based consensus control of multi-agent 6-DoF UAV system. Sensors, 20(12), 3576. 
O’Donoghue, B., Osband, I., Munos, R., & Mnih, V. (2018, July). The uncertainty bellman 
equation and exploration. In International Conference on Machine Learning (pp. 
3836-3845). 
Olfati-Saber, R., Fax, J. A., & Murray, R. M. (2007). Consensus and cooperation in 
networked multi-agent systems. In Proceedings of the IEEE, 95(1), 215-233. 
Polydoros, A. S., & Nalpantidis, L. (2017). Survey of model-based reinforcement learning: 
Applications on robotics. Journal of Intelligent & Robotic Systems, 86(2), 153-173. 
Qian, D., & Xi, Y. (2018). Leader–follower formation maneuvers for multi-robot systems 
via derivative and integral terminal sliding mode. Applied Sciences, 8(7), 1045. 
Roldão, V., Cunha, R., Cabecinhas, D., Silvestre, C., & Oliveira, P. (2014). A leader-
following trajectory generator with application to quadrotor formation flight. 
Robotics and Autonomous Systems, 62(10), 1597-1609. 
Shiell, N. (2017). Behavior-based pattern formation in a swarm of anonymous robots 
(Doctoral dissertation, Memorial University of Newfoundland). 
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine 
learning, 3(1), 9-44. 
Sutton, R. S., & Barto, A. G. (1998). Introduction to reinforcement learning (Vol. 135). 
Cambridge: MIT press. 

40 
Tai, L., Zhang, J., Liu, M., Boedecker, J., & Burgard, W. (2016). A survey of deep network 
solutions for learning control in robotics: From reinforcement to imitation. arXiv 
preprint arXiv:1612.07139. 
Tang, Y., Xue, Z., Liu, X., Han, Q., & Tuo, X. (2019). Leader-following consensus control 
for multiple fixed-wing UAVs’ attitude system with time delays and external 
disturbances. IEEE Access, 7, 169773-169781. 
Vasile, C., Pavel, A., & Buiu, C. (2011, August). Integrating human swarm interaction in 
a distributed robotic control system. In 2011 IEEE International Conference on 
Automation Science and Engineering (pp. 743-748). IEEE. 
Wang, Y., Cheng, Z., & Xiao, M. (2020). UAVs’ formation keeping control based on 
Multi–Agent system consensus. IEEE Access, 8, 49000-49012. 
Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., & Freitas, N. (2016, June). 
Dueling network architectures for deep reinforcement learning. In International 
conference on machine learning (pp. 1995-2003). PMLR. 
Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine learning, 8(3-4), 279-292. 
Xiao, Y., Zhang, Y., Sun, Y., & Qian, J. (2019, October). Multi-UAV Formation 
Transformation Based on Improved Heuristically-Accelerated Reinforcement 
Learning. In 2019 International Conference on Cyber-Enabled Distributed 
Computing and Knowledge Discovery (CyberC) (pp. 341-347). IEEE. 
Yanumula, V. K., Kar, I., & Majhi, S. (2017). Consensus of second-order multi-agents with 
actuator saturation and asynchronous time-delays. IET Control Theory & 
Applications, 11(17), 3201-3210. 
 
 

41 
2.2. 
Advances in Formation of Multi-robot Systems using 
Reinforcement Learning and Neural Networks 
2.2.1. Introduction 
Several papers and surveys on formation control and its potential adaptation to different 
applications have been published in the previous decade. Formation control is used for 
surveillance, exploration, transportation, search and rescue missions, to name a few. In 
addition, formation control tasks may have a wide range of objectives, resulting in different 
control models. 
The formation control and analysis (inspired by natural systems) in multi-robot 
systems improve their efficiency, robustness, and performance for different tasks such as 
exploration (Lopez-Gonzalez et al., 2016; Amigoni et al., 2017) and transportation 
(Zhaohui et al., 2008; Eoh et al., 2011; Alonso-Mora et al., 2017; Tuci et al., 2018). 
Please refer to Issa & Rashid (2019) and Cohen & Agmon (2021) for a 
comprehensive review of formation control applications in mobile robots. 
Organization of the Survey  
The remaining structure is as follows. Section 2 reviews the typical neural networks 
commonly used in robotic control, followed an introduction of different machine learning 
algorithms including supervised, semi-supervised and reinforcement learning techniques 
for multi-robot formation control. Section 2.1.2 presents the taxonomy of most recent 
developments and proposed approaches related to multi-robot control, and describes the 
parameters related to group control (i.e., coordination, distribution strategy, etc.). The 
comparative analysis of presented techniques is also included in Section 2.1.3 The recent 
advances and various approaches for swarm-robot control is discussed in Section 2.1.4, 
along with comparative analysis and insights on the proposed approaches. Section 2.1.5 
presents the open challenges, strengths, challenges, future directions and research frontiers 
based on the existing work. Finally, Section 2.1.6 present our concluding remarks. 

42 
Problem Statement and Contributions  
The objective of this survey is to provide a comprehensive analysis of learning-based 
formation control in multi-robot systems. More specifically, we highlight the recent 
progress in adaptive control and bio-inspired control along with the classification and 
comparison (in terms of coordination, system dynamics, control architecture, composition, 
etc.).  
We consider both homogenous and heterogeneous multi-robot systems with either 
coordinated or cooperative environments. Unmanned aerial vehicles (UAVs), autonomous 
underwater vehicles (AUVs), and unmanned surface vehicles (USVs), or autonomous 
surface vehicles (ASVs) are a few examples of the robots we used in our taxonomy. 
However, we have not addressed robot manipulators or robotic arms extensively. The 
formation control is analyzed in different application domains, including trajectory 
tracking, collision, obstacle avoidance, search and rescue missions, path planning, etc. 
The comparison strategy for our taxonomy is adapted from Verma & Ranga (2021), 
as presented in Table 2.1. The main contributions of our survey are as follows: 
• A thorough review of existing literature related to different techniques in learning-
based formation control, which are compared in a unified framework. 
• A consideration on formation control and its application in distributed and 
decentralized systems using artificial neural networks. 
• A discussion about open issues, research direction, and objectives in multi-robot 
control. 
• Review and analysis of the learning-based formation control for various application 
domains of multi-robot systems, i.e., trajectory tracking, collision, and obstacle 
avoidance, etc. Summary and insights are also included for each approach (i.e., 
adaptive, bio-inspired).  
 
 

43 
Table 2.1 
Comparison parameters, adapted from Verma & Ranga (2021 
Parameters 
Objective 
Coordination 
The robots are cooperating or coordinating to complete the 
mission. 
Coordination Approach To find whether a formation strategy can work outside 
predefined scenarios or not (Static/Dynamic) 
Control Architecture 
To find how a formation strategy controls all the robots, 
distributed or decentralized or both. 
Composition 
To assess the formation strategy among heterogeneous or 
homogenous multi-robot systems. 
 
2.2.2. Typical Neural Network Architectures 
The environment is dynamic and uncertain in most multi-robot systems, which makes it 
challenging to implement model-based control methods since the designer cannot 
anticipate all possible world states that the robot may encounter and design the 
corresponding behavior. Therefore, learning capabilities are important for multi-robot 
systems to function properly in this type of environments. 
The three fundamental ML paradigms of supervised learning, unsupervised 
learning, and reinforcement learning are frequently used for robotic control. Many various 
variations of those paradigms are utilized in the literature, which we briefly discuss in this 
Section, including the feedforward neural network (FFNN), recurrent neural network 
(CNN), graph neural network (GNN), deep Q-network (DQN), deep deterministic policy 
gradients (DDPG), and fuzzy actor-critic reinforcement learning (FACRL). 
Learning algorithms in robotics describe how changes in a robot’s mobility are 
affected by the inputs it receives (sensor data from the environment) and make decisions 
about robot motion control.  
In a basic error correction feedback control architecture, the sensor data is sent to 
the learning algorithm, and the actual output (∞) is compared to the desired output (±) in 
a basic error feedback architecture. If ∞ and ± are identical, the error signal is zero, and 
the learning stops. If ∞ is not equal to ±, a continuous error known as mean square error 

44 
(MSE) correction process is required. The number of iterations necessary to rectify or 
minimize an error determines the algorithm’s performance. Less number of iteration 
(feedback loop) means the algorithm is better to perform faster data processing. 
Supervised/ Semi-Supervised Learning (SL/ SSL) 
The objective of an SL model is to approximate an unknown function M(!): w≤→w, given 
some noisy observations &= =  M(!=) + I=, where != is known and I= represents the 
noise or uncertainty. This approximation problem can be solved in two ways (Bensoussan 
et al., 2020): 
 
 
*(!, /) = âH?
O
 ≥ K|/|¢
+ i(*(!=;/) −&=)¢ 
µ
=~|
∂ 
(2.25) 
 
where *(!, /) is a minimization problem with the parameter /. 
Alternatively, we can find a functional space ℋ in the Kernel approach (non-
parametric method) to which the approximation of M(!) belongs. 
 
 
*(!) = âH?
∏∈ℋ≥  K‖*‖ℋ
¢ + i(*(!=) −&=)¢ 
µ
=~|
∂ 
(2.26) 
 
Feedforward Neural Networks (FFNN) 
There are various learning algorithms for training NNs, but the feedforward method (FF) 
and the backpropagation method (BP) are crucial for faster data analysis. The FF network 
is primarily utilized to assign initial weights and generate actual output during the learning 
phase, and BP modifies the weights of NNs by calculating the error of each layer.  

45 
Feedforward neural networks (FFNN) are composed of a hierarchy of processing 
units organized in a series of two or more sets of neurons or layers. Figure 2.2 depicts a 
typical feedforward network; here, the first layer serves as a receiving point for the 
network’s values, while the last layer represents the computational results. Between the 
input and output layers, there are usually one or more hidden layers. In an FF network, the 
output of a unit is scaled by the value of a connection weight and feedforward to deliver a 
portion of the activation for the units in the following layer. As a result, the network is a 
multi-layered feedforward network (Prabhu & Garg, 1996). 
The input vector of FFNN is denoted by: ∫(0) = {!|(0),… , !=(0)} at time t, the 
weighted sums of hidden neurons’ inputs are denoted by •(0) = {s|(0), …, sñ(0)}, and the 
output of hidden neurons is denoted by a vector t(0) = {t|(0), … , tñ(0)}.  
 
 
•:(0) = i æ9:
ø
=
9~|
!9(0) + ": 
t:(0) = * í•:(0)ì 
(2.27) 
 
where ; = 1, 2, . . , ¿, and the input weight vector, æø is the connection weight 
between externally applied inputs and the hidden layers.  
Consequently, the mathematical model of FFNN is as follows (El Hamidi et al., 
2020): 
 
 
&(0 + 1) = i t:(0) æ:
¡
ñ
:~|
(0) + ": 
(2.28) 
 

46 
Recurrent Neural Networks (RNN) 
The multi-layered feedforward networks discussed before are static non-linear mappings, 
meaning each input is unrelated to the preceding one. When feedback is added, a recurrent 
network is created that can store state information between inputs (in a dynamic system). 
A typical recurrent neural network (RNN) is shown in Figure 2.3.  Since recurrent networks 
are non-linear dynamic systems, their behavior can range from oscillations to chaos and 
determine whether a network can settle into a stable state. RNN typically sends the hidden 
layer output back to the hidden layer input. As a result of this recurrent connection, the NN 
can recognize and generate time-varying patterns (Jin et al., 2017).  
A typical RNN contains n dynamical hidden neurons and is described as follows 
(Zhang et al., 2019):  
 
 
!¬̇ = [!¬ + ƒæ|'(!¬) + ƒæ¢I(!¬)≈ 
&¬ = ∆!¬ 
(2.29) 
 
where !¬  ∈w=, ≈ ∈wñ, and &¬  ∈wñ are the state, input, and output, respectively. 
[ and ƒ are ? × ? matrices with negative eigenvalues and scalar elements. æ| and æ| are 
two ? × ? matrices with adjustable synaptic weights. '(!¬) is an n-dimensional vector with 
elements of «(!¬9), and I(!¬) is a ? × ¿ matrix with elements of »(!¬9). 
 
 
«(!¬9) =
â|
1 + -y…   ∧ 9 
»(!¬9) =
â¢
1 + -yÃ   ∧ 9 + Õ 
(2.30) 
 
where â|, â¢, £, § and are constants representing the bound and slope of the 
sigmoid curvature and Œ >  0 is a constant that causes the sigmoid to shift in such a way 
that »(!¬9) > 0 for all H =  1, 2, ⋯, ?. 

47 
Graph Neural Networks (GNN) 
Graph neural networks (GNNs) have recently gained interest in various fields, including 
robot control as a data processing architecture that employs pointwise nonlinearities and 
recursive neighborhood label aggregations.  
As shown in Figure 2.4, a typical GNN has some standard features, such as reusing 
coefficients across all graphs and indifference to the values of unique neighbors (Gama et 
al., 2020). Graph convolutional neural networks (GCNNs) by Gama et al. (2018) and graph 
recurrent neural networks (GRNNs) by Ruiz et al. (2020) exploit the operation of graph 
convolution and pointwise nonlinearities, resulting in local architectures that only 
communicate with neighboring agents. GNNs algorithms preserve the partial information 
structure and are stable and equivariant, making them scalable and transferable.  
The mathematical model of GNN is as follows (Gama et al., 2020): 
 
 
& = i J:•x∫
—
:~“
 
(2.31) 
 
where • ∈ w=×= is a matrix, which satisfies [•]9: = s9: = 0 whenever H ≠;. 
∫(0) ∈ w=×— is the state matrix, and w◊ is the set of filter coefficients or weights that are 
both local and distributed.  
Reinforcement Learning (RL) 
The majority of existing multi-robot learning research employs reinforcement learning for 
formation control because of its simplicity, real-time performance, and self-learning 
capabilities. Here, the goal of learning control is to characterize alternative policies that 
will be obtained by agents in the formation.  
Value-based Methods 
These methods are based on estimating the values of being in a certain state and then 
extracting control rules from those values. The Bellman Equations are used in the recursive 

48 
value estimation techniques. The Bellman Expectation and the Optimality Equations (Tai 
et al., 2016) are defined according to: 
 
 
ZT(s, t) = i Ä(ś|s, t)êw(s, ś, t)
è́
+ KZT(ś, t́)ë 
(2.32) 
 
 
ZT∗(s, t) = i Ä(ś|s, t) íw(s, ś, t)
è́
+ K ât!
ã́
ZT∗(ś, t́)ì 
(2.33) 
 
where ZT(s, t), and ZT∗(s, t) are the action-value function and the optimal value 
function, respectively. 
Policy-based Methods 
Policy-based approaches, unlike value-based methods, do not use value estimations and 
instead operate directly on policies. When dealing with high-dimensional or continuous 
action spaces, policy-based methods are usually far more effective than value-based 
methods. Instead of learning deterministic policies, they can learn stochastic policies, 
which have higher convergence features. 
More formally, given policy NO(∙) with parameters /, policy optimization searches 
for the best / that maximizes an objective function X(NO): 
 
 
X(NO) = áT≠*TU(∙)Ø 
(2.34) 
where *TU(∙) is a score function, which evaluates the policy.  

49 
There are a few choices for the score function, such as the model suggested by 
Schulman et al. (2015):  
 
 
úOX(NO) = áT≠úO $åä NO *TU(∙)Ø 
(2.35) 
 
Q-Actor-critic approaches 
Actor-critic approaches with deep learning versions are the most extensively utilized 
reinforcement learning (RL) algorithms for robotic control. Actor-critic models maintain 
an explicit representation of both the policy (the actor) and the value estimates (the critic) 
using the objective function: 
 
 
úOX(/) ≈°T •i
úO $åä NO(t9|s9)
9∈µ
Z(s9,t9)¶ 
(2.36) 
 
where NO(∙) is the policy with policy parameters /. 
Deep Reinforcement Learning (DRL) 
Deep reinforcement learning (DRL) uses deep NNs as powerful non-linear function 
approximators for optimal value functions Y∗(s), Z∗(s, t), [∗(s, t), and the optimal 
policies NO
∗(t|s), S∗(s). The Bellman Equation is usually solved with a Q-function 
utilizing an optimizer that aims to minimize the mean squared bellman error (MSBE) as 
the loss function in most DRL methods. Then, The Q-function is then used to obtain a 
policy. 
Subsequently, policy gradient approaches (i.e., DDPG, PPO) attempt to maximize 
the expected return. The policy gradient is defined as the derivative of the expected return 
in relation to the policy parameters. 
Deep Q-Networks (DQN) 
Deep Q-networks (DQN) approximates the optimal Q-value function with a deep 
convolutional neural network (DCNN). DQN is capable of dealing with high-dimensional 
state spaces but only discrete and low-dimensional action spaces. As shown in Figure 2.5, 
DQN can either take a state-action pair as input and provide a single output value, or it can 

50 
take only the state as input and return the Q-value of all possible actions (only if the action 
space is discrete). The purpose in both situations is to learn estimates Z(s, t) with a NN 
with parameters /. 
At every update iteration i, the current parameters / are updated to minimize the 
MSBE with respect to old parameters /y, by optimizing the following loss function: 
 
 
X(/) ≈1
™i
íZ(s9, t9) −Z_ (s9, t9)ì
¢
9∈µ
 
(2.37) 
 
where Z(s, t), and Z_ (s, t) are current and target Q-networks, respectively.  
Only the Bellman Equation is satisfied by Q-Learning, and the agent learns optimal 
policy using absolute greedy policy and behaves using other policies (i.e., ÿ-greedy policy). 
Since the updated policy is different from the behavior policy, and therefore DQN is off-
policy. 
Deep Deterministic Policy Gradients (DDPG)  
Deep deterministic policy gradients (DDPG) is an extension of the DQN, a value-based 
method. DDPG combines DQN with actor-critic methods to solve continuous control 
challenges based on raw pixel inputs and learns both a policy and value function (Q-
function). To stabilize training, DDPG (similar to DQN) uses experience replay buffers 
and a frozen target network. The critic in DDPG, on the other hand, varies from the critic 
in DQN in two key ways.  
The critic in DDPG accepts both states and actions as input, as shown in Figure 2.6. 
Also, instead of producing a Q-value for each action (which would result in an infinite 
number of outputs), the critic uses a single neuron to generate values for each state-activity 
pair. Training the critic network Z(s, t) in DDPG is very similar to how it is trained in 
DQN. However, training the actor relies on the Deterministic Policy Gradient Theorem 
(Silver et al., 2014): 
 
 
úOX(/) ≈1
™i úãZ(s, t|/])úOS(s|/^) 
(2.38) 

51 
 
The agent aims to find a strategy that maximizes the discounted reward from the 
start state, which is indicated by the performance objective X(/). 
Actor loss is computed using the mean of the value given by the critic network for 
the actions taken by the actor-network. Critic loss is calculated by MSE of & −Z(s, t), 
where & is the expected return as seen by the target network. The critic network is 
attempting to reach &, which is a moving target. 
The policy gradient in DDPG is calculated as an expectation across trajectories 
{s|, t|, ,|, … , s=, t=, ,=}, which is estimated by a sample mean. Therefore, DDPG’s policy 
gradient is off-policy. 
Proximal Policy Optimization (PPO) 
Policy gradient methods work by computing an estimator that estimates the policy by 
applying a stochastic gradient ascent. By running the policy in the environment to gather 
samples of the policy loss X(/) and its gradient, the gradients are estimated using a Monte 
Carlo (MC) approach. 
Proximal policy optimization (PPO) is a model-free, on-policy, actor-critic, policy-
gradient method and first-order approximation of trust region policy optimization (TRPO). 
PPO algorithm evaluates the Kullback–Leibler (KL) divergence of policy changes and 
ensures monotonic improvements over TRPO (Bøhn et al., 2019). 
 
 
XŸFø⁄(/) ≈°êâH? (,z(/)[Æz,€$H¿(,z(/),1 −‹, 1
+ ‹)[Æzë 
(2.39) 
 
where °, and [Æz denote the empirically determined estimates of the advantage 
function and expectation, respectively. ,z(/) is the probability ratio:  
 
 
,z(/) = NO(tz,sz)
NO›ﬁﬂ(tz,sz) 
(2.40) 
 

52 
Fuzzy Actor-Critic Reinforcement Learning (FACRL) 
In general, fuzzy systems can be implemented in a network design that is isomorphic to a 
NN, such as a multiple-layer perceptron (MLP) network, where each unit performs a 
function and the network as a whole becomes fully identical to a fuzzy system. 
The fuzzy actor-critic reinforcement learning (FACRL) method combines the fuzzy 
system as a function approximation with the traditional actor-critic method.  
There are two fuzzy systems: one for action generation (named actor) and the other 
for value function estimate (called critic). The architecture of FACRL is schematically 
shown in Figure 2.7. FACRL architecture is used if the defined reinforcement signal is the 
same for all agents, and because each agent is in a distinct state, their related critic function 
will be different as well, resulting in a better agent policy (Wang et al., 2007). 
Radial basis function (RBF) NNs can approximate any non-linear function with 
arbitrary precision, without local minimum (Wang et al., 2007). An RBF network with a 
single hidden layer and single output is described as: 
 
& = i J:«:
=
:~|
‡
·! −S:·
‚:
„ 
(2.41) 
 
where ! ∈w= denotes an n-dimensional input observation, w◊ is the weight, and «: 
is ;-th radial basis function or receptive field unit. S: ∈w= and ‚: ∈w= are the center 
(mean) and the width (variance) vectors of the ;-th basis function, respectively. 
If the RBF network’s basic function is a Gaussian function and the network’s output 
is normalized, then: 
 
 
& =
∑
J: ∏
-!¿
ñ
9~|
=
:~|
Ö−ê!9 −S9:ë
¢
2‚9:¢
Ü
∑
∏
-!¿
ñ
9~|
=
:~|
Ö−ê!9 −S9:ë
¢
2‚9:¢
Ü
 
(2.42) 

53 
The overall output of the fuzzy system can be calculated using the input vector by: 
 
 
& =
∑
≠*:W9~|
ñ «9:(!9)Ø
=
:~|
∑
W9~|
ñ «9:(!9)
=
:~|
 
(2.43) 
 
where «9: denotes the membership function for fuzzy set [9
:. *: is a crisp function 
of !9, and W is a conjunction operator in fuzzy set operations.  
«9: should be differentiable and normalized to create an adaptive fuzzy inference 
system. Therefore, the membership function «(∙) can be used to represent a multi-
dimensional Gaussian function. 
 
 

54 
 
Algorithm 1. DQN* Adapted from Mnih et al., 2015. 
Initialize replay memory D to capacity N 
Initialize action-value function, Q with random weights / 
Initialize target action-value function, Q with random weights θy| 
1 
for -¿Hså8- = 1, 2, … , Ê do 
2 
Initialize sequence s| = {!|} & preprocessed sequence '| = '(s|) 
3 
    for 0 = 1, 2, . . , î do 
4 
      With probabilityε select a random action tz 
5 
      otherwise select tz = t,ä ât!
ã
Z('9, t|/) 
6 
      Execute action tz in emulator and observe reward ,z and !z{| 
7 
      Set sz{| = sz, tz, !z{| and preprocess 'z{| = '(sz{|) 
8 
      Store transition 'z, tz, ,z, 'z{| in Ë 
9 
      Sample random minibatch of transitions '9, t9, ,9, '9{| from Ë 
10       Set &9 = È
,9                               if episode terminate at step H + 1
,9 + K ât!
ã
Z_('9{|, t́|/y|)                                otherwise 
11       Perform a gradient descent step on (&9 −Z('9, t9|/))¢ with respect  
             to the network parameters / 
12       Every ∆ steps reset Z_ = Z 
13     end for 
14 end for 
 
 
                                                
* DQN is off-policy based on Q-learning 
DQN with some additions: Target network, Double DQN, Dueling DQN and ACER (Actor critic 
with experience replay). 
DQN has a higher sample efficiency due to the replay buffer, but longer convergence time. 

55 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                                                
* DDPG is an off-policy algorithm 
Similar to DQN but only for continuous action spaces 
DDPG uses off-policy data and the Bellman equation to learn the Q-function and uses the Q-
function to learn the policy 
Algorithm 2. DDPG* Adapted from Lillicrap et al., 2015. 
Initialize actor & critic networks ←Z(s, t|/]), S(s|/^) randomly 
Initialize target network Z_ & Ś 
Initialize replay buffer R with capacity Ò 
1 
for iteration 0 = 1, 2, … ℰ  do 
2 
  Initialize a random process Gfor action exploration 
3 
  Receive initial observation s| 
4 
    for 0 = 1, 2, . . , î do 
5 
        Select the action tz = S(sz|/^) + Gz according to the current policy & exploration noise 
6 
        Execute the action aÚ, observe reward rÚ & new state sÚ{| 
7 
        Store transition (sz, tz, ,z, sz{|) in w if ‖w‖ > Ò 
8 
        Sample a random mini-batch of Ò" tuples (sx, tx, ,x, sx{|) from R 
9 
        Calculate error using: 
                  &x = ,x + KZ_ êsx{|, Śêsx{|Û/^́ ëÛ/]_ ë 
10         Update critic by minimizing the loss: 
|
µ ∑ê&x_ −Z(sx, tx|/])ë
¢
x_
 
11         Update actor using policy gradient:  
                  
|
µ ∑ú…Z(sx, tx|/])|sx, S(|sx)úOS(sx|/^)
x_
|sx 
12         Update target and critic networks: 
                  /]_ ←,/] + (1 −Ù)/]_ 
                  /^́ ←,/^ + (1 −Ù)/^́  
13     end for 
14 end for 

56 
Algorithm 3. PPO5 Adapted from Schulman et al., 2017. 
Initialize policy θ and value functions parameters ' 
1 
    for 0 = 1, 2, … , î do 
2 
        Collect set of trajectories Ëz = {Ù9} by running policy N = N(/) 
3 
        Compute reward ,z 
4 
        Compute advantage estimates [Æz based on the current value function Y 
5 
        Update the policy parameters: 
                  t,äât!
O
|
|Eı|} ∑
∑
âH?
}
z~“
Ù∈Eı
h
TUêtzÛszë
TUkêtzÛszë [TUk(sz,tz),ä(ÿ, [TUk(sz,tz))l 
6 
        Fit value function by regression on MSE: 
                  'z{| = t,äâH?
ˆ
|
|Eı|} ∑
∑
êYˆ(sz) −w˜zë
¢
}
z~“
Ù∈Eı
 
7 
    end for 
 
 
                                                
5 PPO is an on-policy algorithm. 
There are two variants of PPO: PPO-Penalty (penalized KL divergence), and PPO-Clip (clips the 
objective function) 

57 
Context Layer  
Input 
Layer 
Hidden Layer  
Output 
Layer
£ 
K 
CNN 
Input graph 
Target node 
K 
ℋC
¢ 
ℋC
| 
ℋGC
|  
ℋ¯
| 
ℋŸ
| 
ℋE
| 
A 
B 
C 
D 
E 
F 
Hidden Layer  
Output 
Layer 
Input 
Vector 
æ|
¡(Õ) 
æ¢
¡(Õ) 
æñ
¡(Õ) 
æ||
ø(Õ) 
t|(Õ) 
t¢(Õ) 
tñ(Õ) 
æ|¢
ø(Õ) 
!|(Õ) 
!¢(Õ) 
!=(Õ) 
&(Õ + 1) 
æ=|
ø(Õ) 
æ=ñ
ø(Õ) 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.2 
A multi-layered feedforward neural network (FFNN) with an input 
vector, a hidden layer with tangent hyperbolic activation function, and an output 
layer with a linear activation function. Adapted from El Hamidi et al., 2020. 
 
 
 
 
 
 
 
 
 
Figure 2.3 
Fully-connected recurrent neural network (RNN) with an input layer, 
a hidden layer, a context layer, and an output layer. Adapted from Liu et al., 2014. 
 
 
 
 
 
 
 
 
Figure 2.4 
Graph neural network (GNN) with an input graph and a target node. 
Adapted from Hamilton et al., 2017. 
  
 

58 
Target 
Z_  network 
Z network 
hí, + K max
ã́
Z(ś, t́, /y
9)ì −Z(s, t, /9)l
¢
 
predictio
sz, tz, sz{| 
Z(s, t|/]) 
S(s|/^) 
Actor 
Criti
s 
s 
/] 
t 
 
/^ 
Policy Gradient  
úOX ≈1
™i úãZˆês, tÛ/ˆëúOS(s|/^) 
Σ 
«|(!)
«:(!) 
«=(!
S||,‚|| 
S:|,‚:| 
Sñ|,‚ñ|
Π 
Output 
Layer  
Input 
Layer 
Sñ:,‚ñ:
S|:,‚|: 
S9:,‚9: 
Π 
Sñ=,‚ñ=
S|=,‚|=
S9=,‚9= 
Π 
Rule Layer 
Normalized 
Layer 
x| 
x9 
xñ 
[|(!) 
[|(!) 
Y(!) 
[—(!) 
 
 
 
 
 
 
 
 
Figure 2.5 
Deep Q-Networks (DQN). Adapted from Choudhary et al., 2019. 
 
 
 
 
 
 
 
 
Figure 2.6 
Deep deterministic policy gradients (DDPG) network. Adapted from 
Liessner et al., 2021. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.7 
Fuzzy actor-critic reinforcement learning (FACRL) network. Adapted 
from Wang et al., 2007. 
 

59 
2.2.3. Theoretical Developments 
To overcome the inherent complexity of control in multi-agent systems and for non-linear 
approximation and uncertainties, several scholars used learning-based methods (Chen, 
2014; Xu et al., 2014; Cheng et al., 2020). Consequently, many learning control strategies 
have been designed for robotic tasks such as trajectory tracking (Kumar et al., 2011; Lou 
& Guo, 2016), search and rescue missions (Wang et al., 2019; Vicmudo & Dadios, 2015; 
Niroui et al., 2019; Sadhu et al., 2020), etc. 
In this Section, we summarize the learning methods that have been commonly used 
for formation control. The main strategies employ adaptive neural networks, bio-inspired 
and/or evolutionary neural networks. 
Adaptive Neural Networks 
Adaptive NNs have powerful learning capability to estimate any complicated dynamics to 
arbitrary desired accuracy and, therefore, are widely used in a variety of complex tasks, 
including trajectory tracking, collision avoidance, and adaptive formation, to name a few. 
Trajectory Tracking 
Task and motion planning are two of the fundamental control problems in robotic control. 
We consider these two problems as key problems of multi-robot systems to complete any 
given task such as object transportation or exploration and mapping. Formation control and 
trajectory tracking is a subset of motion planning in coordinated or cooperative multi-robot 
systems to reach and follow a path or target while maintaining the desired formation. 
Often the best time-varying path for trajectory tracking is calculated using the 
robot’s dynamic model and a predetermined target and therefore it is challenging to deal 
with the impact of uncertain environments. A vast variety of approaches for trajectory 
tracking control have been developed in the last two decades, however there are still several 
challenges in practice.  
More recently, many studies have used NNs to solve control problems in robotics, 
particularly trajectory tracking. ANNs and data-driven ML method have shown success in 
the analysis of uncertain and environmentally sensitive control problems. Rossomando 
(2014) suggested using a feedback linearization model for an adaptive NN controller of a 

60 
non-holonomic mobile robot that combines an indirect neural adaptation technique with 
sliding mode control to compensate for the robot’s dynamics. Similarly, Dragoicea et al. 
(2015) suggested a hybrid control architecture for tracking a non-holonomic mobile robot 
using kinematic steering and velocity dynamics learning simultaneously.  
Pedro & Crouse (2015) proposed a direct adaptive neural network (DANN) 
controller for an unstable, non-linear and underactuated quadrotor unmanned aerial vehicle 
(UAV) using radial basis function neural networks (RBFNN). DANN can track the desired 
output variables (altitude, roll, pitch, and yaw angles) simultaneously and is robust to 
parameter variations and disturbances. Shirzadeh et al. (2015) developed a visual-based 
control mechanism using an indirect adaptive NN controller with a radial basis function 
(RBF) to control a quadrotor UAV pursuing a moving target.  
Peng et al. (2016) employed an adaptive NN controller for distributed consensus 
formation control of non-holonomic wheeled mobile robots (WMRs). They suggested a 
hybrid kinematic and NN torque control architecture such that a group of robots 
asymptotically converge to a desired geometric pattern along a reference trajectory. Zeng 
et al. (2016) proposed an adaptive NN controller and two high-gain observers to guarantee 
the stability of the closed-loop robot system and the convergence of tracking errors 
approximated by radial basis function NNs.  
Jabbari Asl & Janabi-Sharifi (2017) proposed an adaptive neural trajectory tracking 
controller for cable-driven parallel robots (CDPRs) that is more precise than conventional 
rigid-link robotic systems. Teng et al. (2017) suggested an adaptive attitude stabilization 
and position control for a quadrotor UAV with unknown variable payloads. They used BP 
to train the FFNN and proposed an adaptive PID controller to deal with the unknown 
variables. Similarly, Yang et al. (2017) employed RBF consensus-based distributed control 
for nonholonomic autonomous vehicles in a pre-defined formation with a reference 
trajectory.  
Yu et al. (2017) used DDPG to solve the control problem of trajectory tracking for 
AUVs. Li et al. (2018) employed an RL-based adaptive neural tracking algorithm for the 
non-linear discrete-time (DT) dynamic system of WMRs with skidding and slipping and to 
track the desired trajectories with time-varying forward direction.  

61 
Jiang et al. (2019) used a deep neural network (DNN) to learn decentralized control 
policies for multi-robot formation, inspired by recent deep learning advances that allow an 
intelligent agent to compute its actions directly from high-dimensional raw sensory inputs 
using end-to-end decision-making policies. They used the Euclidean loss function to define 
the loss for each iteration and adopted a centralized gradient-descent for training. Kim et 
al. (2019) suggested a direct adaptive dynamic surface control neural network (DSCNN) 
controller for tracking the expected trajectories of a four-wheel omnidirectional holonomic 
robot. Xu et al. (2019) employed an adaptive NN controller for the position and attitude 
tracking of quadrotor UAVs in the presence of input saturation, unmodeled non-linear 
dynamics, and external disturbances. 
Bekar et al. (2020) employed a DDPG algorithm to control a system design for 
Agile Maneuvering UAVs. They demonstrated that RL-based trajectory tracking and 
reinforcement learning closed-loop reference model (RL-CRM) controllers outperform 
PID-based trajectory tracking and classical adaptive controllers. Wang et al. (2020) 
suggested using Q-learning and PID controller for trajectory tracking of mobile robots in 
complex environments. Their result demonstrates that the Q-learning–PID can assure that 
the value function converges to zero, implying that the hybrid tracking is better than the 
single Q-learning or PID. Chu et al. (2021) adapted RBF and nonlinear model predictive 
control (NMPC) for trajectory tracking control of underwater vehicles. 
Please refer to Lee & Kim (2017) and Nascimento et al. (2018) for a comprehensive 
review of trajectory tracking control of mobile robots. 
Summary and Insights: A number of algorithms have been used such as, DANN 
(Pedro & Crouse, 2015; Yan et al., 2019), RBF (Shirzadeh et al., 2015; Yang et al., 2017; 
Chu et al., 2021), Deep learning (Jiang et al. 2019), Q-learning (Li et al., 2018; Wang et 
al., 2020), DDPG (Yu et al., 2017; Bekar et al., 2020) etc. The applicability of these 
approaches depends on a few factors including scalability (Liang et al., 2018; Zhang et al., 
2021), decentralized or centralized (Xiang et al., 2017; Peng et al., 2018; Muslimov & 
Munasypov, 2020), and robustness (Liu et al., 2019; Mazhar et al., 2020; Mehmood et al., 
2021). Most of the recent works use decentralized and cooperative coordination for 
adaptive tracking control of mobile robots. 

62 
Collision and Obstacle Avoidance 
Sun et al. (2013) employed an adaptive NN for non-holonomic mobile robots to track 
environmental boundaries. They used a reference velocity for the mobile robot and RBFNN 
architecture for environmental boundary tracking by mobile robots.  
Tsai et al. (2017) developed a distributed consensus formation control with 
collision and obstacle avoidance for a group of networked omnidirectional mobile robots 
with uncertainties using fuzzy wavelet neural networks (FWNNs).  
Ma et al. (2018) proposed an actor-critic RL algorithm for the obstacle avoidance 
of UAVs control in continuous spaces. Manko et al. (2018) suggested an adaptive NN for 
large-sized object transportation in a complex environment by a team of mobile robots. The 
presented model allows the multi-robot system to achieve its target position while avoiding 
obstacles and object orientation. Sun et al. (2018) proposed an adaptive NN consensus 
controller that can autonomously generate self-organized locomotion and obstacle 
negotiation of quadruped robots. Their control network consists of neural central pattern 
generators (CPGs), sensory feedback adaptation, and multiple neural reflex mechanisms.  
Yu et al. (2019) proposed a hybrid NN controller with adaptive compensator and 
control gain to achieve the region reaching formation control with collision and obstacle 
avoidance for multi-robot systems with model uncertainties and external disturbances. 
Zhou et al. (2019) employed a DQN control structure for USVs formation with advanced 
collision avoidance.  
An online path planning strategy for UAVs based on DRL is proposed by Li et al. 
(2021) to address the control problem of maneuvering target tracking and obstacle 
avoidance. The proposed approach can achieve environment sensing and continuous 
motion output control using end-to-end learning powered by NNs. They employed a DDPG 
control framework to provide learning and autonomous decision-making capability of 
UAVs.  
Please refer to Huang et al. (2019) for a comprehensive review of collision 
avoidance applications in UAVs. 

63 
Other Applications 
Ni & Yang (2011) developed a bio-inspired NN controller for real-time cooperative 
hunting by multi-robots in unknown and changing environments. Chang et al. (2012) used 
an adaptive neural fuzzy formation controller with online learning capability for better 
formation responses in multi-robot systems.  
Li et al. (2013) proposed a hybrid architecture for distribution optimization and 
control of quadruped robots with external disturbance forces using gradient and adaptive 
NNs to defy the uncertainties, including approximation error and external perturbation. 
Manoonpong et al. (2013) suggested an adaptive neural locomotion controller perform 
many different walking patterns, including insect-like leg movements, gaits, and energy-
efficient locomotion on walking robots. Their neural closed-loop controller consists of a 
CPG mechanism and local leg control. 
Zhu et al. (2015) suggested a bio-inspired NN controller for hunting tasks using 
AUVs. Sun et al. (2016) used a NN controller for biped robots to approximate the unknown 
model of the robot and deal with system uncertainties via both state and output feedback 
control.  
Omidshafiei et al. (2017) proposed a scalable approach for multi-robot navigation 
using a partially observable Markov decision process (POMDP). They present a 
centralized policy learning, which employs an actor-critic-based DRL. Lan et al. (2018) 
recommended an adaptive NN controller for the region-based formation control of a swarm 
of robots with unknown non-linear dynamics and disturbances.  
Bøhn et al. (2019) proposed employing a PPO controller for non-linear attitude 
control, allowing fixed-wing UAVs to fly over longer distances. 
Khan et al. (2020) proposed using graph policy gradients (GPG) to control many 
homogeneous robots. They employed graph convolutional networks (GCNs), which use a 
bank of graph filters to minimize the problem’s dimensionality by learning filters that 
aggregate input among robots locally, inspired by CNNs. Wang et al. (2021) proposed a 
motion planning approach based on flocking control and RL to ensure that all robots avoid 
collisions, maintain formation, and advance toward a target. 

64 
Some studies combine the previously stated FACRL and deep Learning techniques. 
Lin et al. (2020) present a unique DRL strategy for multi-robot navigation in unknown 
complex environments while avoiding collisions and maintaining connectivity. A 
constraint satisfying parametric function (CSPF) is proposed to represent the navigation 
policy to guarantee connectivity during the navigation. Sun et al. (2020) introduced an 
optimized sample pool and average motion critic network-based DDPG algorithm for the 
path following control of AUVs. For a complete analysis of multi-robot cooperative 
problems utilizing bio-inspired NNs, see Agrawal & Agrawal (2013) and Oh et al. (2017). 
 
 
 

65 
Table 2.2 
Summary of Adaptive NNs for formation control 
Reference 
Technique 
Coordination 
Coordination 
Approach 
Control 
Architecture 
Composition 
Robots 
Task(s) 
Kumar et al. (2011) 
FFNN 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
3R planar robot 
manipulators 
Trajectory 
tracking 
Sun et al. (2013) 
RBF 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
Non-holonomic 
robots 
Track desired 
separation & 
bearing 
Dragoicea et al. 
(2015) 
FFNN 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
Non-holonomic 
robots 
Trajectory 
tracking 
Pedro & Crouse 
(2015) 
RBF 
Coordinated 
Dynamic 
- 
- 
Quadrotor UAV 
Trajectory 
tracking 
Lou & Guo (2016) 
CrKR 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
Quadrotor UAV 
Trajectory 
tracking 
Shirzadeh et al. 
(2015) 
RBF 
Coordinated 
Dynamic 
- 
- 
Quadrotor UAV 
Search of a 
moving target 
Vicmudo & Dadios 
(2015) 
FFBNN 
Coordinated 
Dynamic 
Decentralized 
Homogeneous 
Underwater robots 
Search & rescue 
mission 
Peng et al. (2016) 
GNN 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
Non-holonomic 
wheeled robots 
Trajectory 
tracking 
Zeng et al. (2016) 
RBF 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
Nonholonomic 
unicycle robots 
Convergence 
Asl & Janabi-Sharifi 
(2017) 
FFNN 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
CDPRs 
Trajectory 
tracking 
Teng et al. (2017) 
FFNN 
Coordinated 
Dynamic 
- 
- 
Quadrotor UAV 
Attitude 
stabilization & 
position control 
 
 
 

66 
Table 2.2 (continued) 
Reference 
Technique 
Coordination 
Coordination 
Approach 
Control 
Architecture 
Composition 
Robots 
Task(s) 
Tsai et al. (2017) 
FWNN 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
Mecanum-wheeled 
omnidirectional 
robots (MWORs) 
Collision & 
obstacle 
avoidance 
Li et al. (2018) 
RLNN 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
WMRs 
Trajectory 
tracking 
Ma et al. (2018) 
RBF 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Collision & 
obstacle 
avoidance 
Manko et al. (2018) 
Q-learning 
Coordinated 
Dynamic 
Decentralized 
Homogeneous 
- 
Object 
transportation  
Sun et al. (2018) 
CPG 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
Mammal-like 
quadruped robot 
Locomotion & 
obstacle 
negotiation 
Bøhn et al., (2019) 
PPO 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Attitude control 
Jiang et al. (2019) 
DNN 
Coordinated 
Dynamic 
Decentralized 
Homogeneous 
2-wheel mobile 
robots 
Online formation 
Kim et al. (2019) 
RBF 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
4-wheel 
omnidirectional 
holonomic robot 
Trajectory 
tracking 
Niroui et al. (2019) 
A3C 
Coordinated 
Dynamic 
- 
- 
Turtlebot 
Search & rescue 
mission 
Wang et al. (2019) 
UCT 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Target search & 
tracking 
 
 

67 
Table 2.2 (continued) 
Reference 
Technique 
Coordination 
Coordination 
Approach 
Control 
Architecture 
Composition 
Robots 
Task(s) 
Xu et al. (2019) 
NN finite 
time 
backstepping 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Position & 
attitude tracking 
Yu et al. (2019) 
RBF 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
- 
Collision & 
obstacle 
avoidance 
Zhou et al. (2019) 
DQN 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
Unmanned surface 
vehicles (USVs) 
Collision 
avoidance 
Bekar et al. (2020) 
DDPG 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Trajectory 
tracking 
Sadhu et al. (2020) 
MADRL 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Search & rescue 
mission 
Sun et al. (2020) 
DDPG 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
Autonomous 
underwater vehicles 
(AUVs) 
Path following 
Li et al. (2021) 
DDPG 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Target tracking 
and obstacle 
avoidance 

68 
Bio-inspired and Evolutionary Neural Networks  
Task assignment & Path Planning 
Path planning or motion planning is an essential primitive in the autonomous mobile robot 
control field that lets robots find the optimal or suboptimal path between two the starting 
and target positions (Ni & Yang, 2011, 2012; Zhu et al., 2013). There are different 
techniques to deal with path planning problems, such as potential field and fuzzy control 
methods.  
Despite particular success in some applications, path planning in unknown and 
dynamic environments is still an open problem that requires more research. Some bio-
inspired algorithms have recently been proposed to solve challenges in mobile robot path 
planning. 
Zhu & Yang (2010) employed a NN approach to task assignment, based on a self-
organizing map (SOM) for a multi-robot system in dynamic environments subject to 
uncertainties. Huang et al. (2014) and Zhu et al. (2013) proposed an integrated multi- 
AUVs dynamic task assignment and path planning algorithm by combing the SOM and a 
novel velocity synthesis approach.  
Ni et al. (2015) employed a dynamic bio-inspired NN for real-time formation 
control of multi-robots, where the formation task can change during the moving process of 
the robots to the destination; the environment is significant and unknown. Ravankar et al. 
(2016) developed a hybrid communication framework that incorporates the repelling 
behavior of anti-aphrodisiac pheromones and attractive behavior of pheromones for 
efficient map exploration of mobile service robots.  
Zhu et al. (2018) proposed a Glasius bio-inspired self-organizing map (GBSOM) 
for task assignment and path planning for a team of AUVs. Ma et al. (2021) developed a 
hybrid SOM controller for path planning and task assignment for a group of AUVs within 
a mixed (dynamic and static) 3D environment. 
For a comprehensive review of path planning strategies for mobile robots, see Zafar 
& Mohanta (2018). 

69 
Simultaneous Localization and Mapping (SLAM) 
Solving simultaneous localization and mapping (SLAM) problem is a critical challenge in 
mobile robot control. The fundamental goal of SLAM is to explore an unknown 
environment by mapping its surroundings using onboard sensors and then using the map 
for maneuvering tasks. 
Toda et al. (2012) suggested an intelligent self-localization SLAM framework using 
evolutionary computation for multiple mobile robots. Ni et al. (2014) developed an 
extended Kalman filter (EKF) SLAM technique in which noise weights are adaptively 
changed. They also employed a bioinspired neural model to make these noise weight 
adjustments and ensure the filter’s stability. In a follow-up study, Ni et al. (2017) proposed 
a dynamic bioinspired neural network (BINN) that employs a virtual target method to assist 
an AUV in effectively determining an optimal or suboptimal path planning.  
Sun et al. (2019) recommended employing a global SLAM approach that divides 
the map into a series of small sub-regions based on the topological structure of the 
environment. The researchers next utilized a multiple traveling salesman (mTSP) model, 
in which a genetic algorithm (GA) assigns sub-regions to each robot and provides the 
robots their sub-region visiting instructions. Latif et al. (2021) demonstrated the RatSLAM 
algorithm, which was inspired by the rat’s hippocampus, for real-time interaction between 
a group of heterogeneous robots. 
For a comprehensive review of SLAM applications in mobile robots, see 
Kshirsagar et al. (2018) and Almadhoun et al. (2019). 
Other Applications 
There have been numerous studies on bio-inspired algorithms in mobile robot control, 
including machine vision, robotic exploration, and olfactory tracking. 
Quiñonez et al. (2011) suggested a self-coordination mechanism for tasks 
distribution in multi-robot systems using threshold models. Novitzky et al. (2012) proposed 
a cooperative mechanism inspired by the honey bee waggle dance for a team of AUVs and 
ASVs using behavior recognition. Qian & Cheng (2018) investigated the performance of 
the ant-colony algorithm (ACA) and GA for coalition formation of multi-robot systems. 

70 
Zeng et al. (2018) proposed a bio-inspired controller for the locomotion of a quadruped 
robot called central pattern generator neural network workspace trajectory (CPG-NN-WT). 
de Almeida et al. (2019) presented a cooperative and distributed navigation strategy for 
autonomous multi-robots using fuzzy controllers. Please refer to Li et al. (2019) for a 
comprehensive review of bio-inspired algorithms and their applications for mobile robot 
control.  

71 
Table 2.3 
Summary of Bio-inspired models for formation control 
Reference 
Technique 
Coordination 
Coordination 
Approach 
Control 
Architecture 
Composition 
Robots 
Task(s) 
Zhu & Yang (2010) 
SOM 
Coordinated 
Static or 
Dynamic 
Distributed 
Homogeneous 
- 
Task assignment 
Quiñonez et al. 
(2011) 
RTM 
Coordinated 
Dynamic 
Decentralized 
Heterogeneous 
- 
Task Allocation 
Ni & Yang (2011)  
ANN & EA 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
- 
Docking 
Ni & Yang (2012)  
FCGA 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
- 
Foraging 
Novitzky et al. (2012) 
BA 
Cooperative 
Dynamic 
Decentralized 
Homogeneous 
AUVs 
Formation 
Toda et al. (2012) 
SSGA & 
SOM 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
4-wheel 
omnidirectional 
robot 
SLAM 
Zhu et al. (2013) 
SOM 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
AUVs 
Task assignment 
& path planning 
Huang et al. (2014) 
SOM 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
AUVs 
Task assignment 
& path planning 
Ni et al. (2014) 
B-EKF 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
- 
SLAM 
Ni et al. (2015) 
SOM 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
- 
Adaptive 
formation 
Zhu et al. (2015) 
Shunting 
model 
Cooperative 
Static or 
Dynamic 
Distributed 
Homogeneous 
AUVs 
Hunting 
 
 
 

72 
Table 2.3 (continued) 
Reference 
Technique 
Coordinatio
n 
Coordination 
Approach 
Control 
Architecture 
Composition 
Robots 
Task(s) 
Ravankar et al. (2016) 
Pheromone 
signalling 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
Service Robots 
Map exploration 
Ni et al. (2017) 
BINN 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
AUVs 
Path planning 
Qian & Cheng (2018) 
ACA & GA 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
Linkbots 
Coalition 
formation 
Zeng et al. (2018) 
CPG-NN-
WT 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
Quadruped robot 
Locomotion 
Zhu et al. (2018) 
GBSOM 
Coordinated 
Static or 
Dynamic 
Distributed 
Homogeneous 
AUVs 
Task assignment 
& path planning 
de Almeida et al. 
(2019) 
Fuzzy system 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
- 
Trajectory 
planning 
Sun et al. (2019) 
mTSP & GA 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
- 
Complete 
coverage path 
planning (CPP) 
Latif et al. (2021) 
RatSLAM 
Cooperative 
Dynamic 
Distributed 
Heterogenous 
- 
SLAM 
Ma et al. (2021) 
SOM 
Coordinated 
Static or 
Dynamic 
Distributed 
Homogeneous 
AUVs 
Task assignment 
& path planning 

73 
2.2.4. Formation Control in Swarm Robotics  
Swarm robotics is a subset of multi-robot systems inspired by natural phenomena and 
involves the collective control of many robots. Basic coordination techniques are essential 
since the individual robots in the swarm have relatively limited processing, sensory, and 
communication capabilities. 
Maningo et al. (2015) employed a SOM to assign the correct coordinates to each 
swarm individual, allowing the swarm formation to exist in the given space while avoiding 
the obstacles. They used three obstruction patterns in a 3D environment to test the system. 
The findings revealed that the swarm was able to avoid all obstacles in every scenario. 
Nurmaini & Zarkasi (2015) proposed a simple pyramid RAM-based NN 
architecture to improve the localization process of mobile sensor nodes in indoor 
environments. They used the capabilities of learning and generalization to reduce the effect 
of incorrect information and increases the accuracy of the agent’s position.  
Galvez et al. (2017) used an artificial potential field (APF) to implement obstacle 
avoidance in a swarm of quadrotors. This is based on the assumptions that the target and 
obstacle will introduce a specific force that directs the robot to its destination. The 
effectiveness of this method was tested in MATLAB simulation and verified using real 
quadrotors. 
Lan et al. (2018) investigated a decentralized region-based formation control law 
for a swarm of robots. The unknown non-linear dynamics are simulated using an adaptive 
NN, and the desired formation shape is obtained by creating appropriate potential 
functions. The controller considers collision avoidance, velocity consensus, and region 
tracking. The proposed formation control legislation alters the formation to pass through a 
restricted space. When some robots fail due to accidents, the formation can be adaptively 
reconstructed using the surviving robots. In addition, the proposed region-based formation 
control technique is effectively addressing the problem of formation merging. Ahn et al. 
(2019) presented a machine learning-based anomaly detection scheme for swarm drone 
flights. The proposed method features two significant steps: the labeling step to label the 
unlabeled data based on lower-dimensional features, and the binary classification step 

74 
based on a one-dimensional CNN with cross-entropy loss function, which trained and 
verified with actual flight test data. Hüttenrauch et al. (2019) used mean feature 
embeddings (MFE) (Smola et al., 2007) for deep multi-agent RL by treating state 
information from nearby agents as samples of a random variable and the empirical mean 
embedding as input for a s policy.  
In a typical manned-and-unmanned (MUM) airborne network, Koushik et al. 
(2019) investigated the communication-oriented UAV placement issue. The MUM 
network comprises a few high-density UAVs in the upper layer and a few powerful aircraft 
nodes in the lower layer. While the airplane network is typically stable, the UAV swarm 
network topologies might vary. They created a DQN model to identify the best link 
between two UAV nodes to deal with dynamic swarm topology and time-varying link 
conditions. They then used an optimization algorithm to locally fine-tune the position of 
the UAV node to optimize the overall network performance. Song et al. (2020) used a NN 
to create a pheromone diffusion model of swarm foraging behavior. The pheromone 
diffusion is modeled using a dynamic wave expansion neural network (DWENN). The 
NN’s neurons correlate to various locations in the workspace. The appropriate neuron will 
receive an external input when the robots release pheromones. The pheromones will spread 
across the neurons’ local connections. and get updated based on the proposed pheromone 
evaporation model.  
Balasubramanian et al. (2021) evaluated the performance and functionality of a 
modified yet simplified force field algorithm and a NN approach for obstacle avoidance. 
The experiments and simulations used infrared sensors and tested three scenarios with one, 
four, or ten robots in open and dynamic environments. Cardona et al. (2021) presented a 
distributed approach for quadrotors CNN to minimize collisions in a search and rescue 
setting. Also, when a quadrotor identifies a potential victim, it forces its closest neighbors 
to leave the primary swarm and create a new sub-swarm around the victim, validating the 
victim’s status. As a result, a formation control that allows for the gaining of information 
is carried out using the rendezvous consensus algorithm (Lin et al., 2003). Furthermore, 
they used CNN to process the photographs to identify prospective victims in the region. 
Please refer to Hou et al. (2021) for a comprehensive review of formation control 
applications in swarm robots.  

75 
Table 2.4 
Summary of formation control in Swarm Robotics 
Reference 
Technique 
Coordination 
Coordination 
Approach 
Control 
Architecture 
Composition 
Robots 
Task(s) 
Maningo et al. (2015) 
SOM 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Mapping & 
obstacle 
avoidance 
Nurmaini & Zarkasi 
(2015) 
RAM-based 
NN 
Coordinated 
Dynamic 
Decentralized 
Homogeneous 
X-Bee robots 
Localization 
Galvez et al. (2017) 
APF 
Coordinated 
Static or 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Obstacle 
avoidance 
Lan et al. (2018) 
APF 
Coordinated 
Dynamic 
Decentralized 
Homogeneous 
- 
Region tracking 
& obstacle 
avoidance 
Ahn et al. (2019) 
RNN 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Anomaly 
Detection & 
Monitoring 
Hüttenrauch et al. 
(2019) 
TRPO 
Coordinated 
Dynamic 
Decentralized 
Homogeneous 
- 
Rendezvous & 
pursuit evasion 
Koushik et al. (2019) 
DQN 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Optimal 
communication 
Song et al. (2020) 
DWENN 
Cooperative 
Dynamic 
Distributed 
Homogeneous 
- 
Foraging 
Balasubramanian et 
al. (2021) 
FFNN 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
- 
Obstacle 
avoidance 
Cardona et al. (2021) 
CNN 
Coordinated 
Dynamic 
Distributed 
Homogeneous 
UAVs 
Search & rescue 
mission 

76 
2.2.5. Challenges and Open Research Questions 
The robotics community has set several objectives for the next 20 years, including mobile 
navigation, collaborative automation, autonomous driving and flight, and robotic disaster 
mitigation and recovery, to name a few. However, some open robotics challenges are 
crucial for reaching these goals. Machine learning approaches and bio-inspired algorithms 
can be used to develop potential solutions for learning-based control for multi-robot 
systems (Pierson & Gashler, 2017). 
However, scaling and stabilizing the algorithms mentioned above to meet the 
requirements of multi-robot operations (in real-world) remains a significant challenge, 
particularly for: 
• 
Complex, and high-dimensional dynamics  
Analytic derivation of complex dynamics is time-consuming, requires human experts, and 
involves a trade-off between state dimensionality and tractability. Moreover, it is difficult 
to make such models resistant to uncertainty since full state information is frequently 
unavailable. Robotic systems that can autonomously adapt to unknown dynamics are 
needed to solve problems such as grasping, traveling in an uncertain environment, or 
adapting to failure. These applications are more crucial for systems with high degrees of 
freedom (e.g., hundreds or more), high levels of uncertainty, and partial state information. 
• 
Dynamic environments  
Control systems with high degrees of freedom are required for multi-arm mobile 
manipulators and swarm robotics, just as they are for dynamics. In contexts with high 
uncertainty and limited state information, such systems will be required to work reliably 
and safely. 
• 
Interpreting and anticipating human actions 
In situations where robots must interact with or among people, interpreting human actions 
is a significant challenge, including collaborative robotics for manufacturing, autonomous 
driving, or navigating pedestrian areas.  
To address these challenges, some researchers have been utilized a new direction 
for learning-based control using reinforcement and imitation learning together. Vecerik et 

77 
al. (2017) proposed a general and model-free approach for RL on real robotics with sparse 
rewards, using DDPG algorithm with real-world demonstrations. They used both 
demonstrations and actual interactions to fill a replay buffer via a prioritized replay 
mechanism. Gao et al. (2018) used a normalized actor-critic (NAC) algorithm for RL from 
the demonstration as a unified method to learn from reward and demonstration. NAC 
normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration 
data. Nair et al. (2018) used demonstrations to overcome the exploration problem and 
successfully learn to perform long-horizon, multi-step robotics tasks with continuous 
control, such as stacking blocks with a robot arm. Hu et al. (2020) proposed an exploration 
strategy and DRL-based algorithm for collision avoidance using dynamic Voronoi 
partitions. 
These studies have introduced algorithms that combine RL and imitation learning, 
allowing the learning agent to benefit from both expert demonstrations and interactions 
with the environment and can benefit learning control.  
Another approach to deal with the challenges of multi-robot control is using meta-
learning techniques. Johannsmeier et al. (2019) introduced a framework for expressing and 
learning force-sensitive robot manipulation skills based on adaptive impedance control 
with meta-parameter learning and compatible skill specifications. Li et al. (2020) combined 
DRL with meta-learning and presented a novel approach, named meta twin delayed deep 
deterministic policy gradient (Meta-TD3), which allows a UAV to quickly track a target in 
an environment with uncertainties. Kuzhamuratov et al. (2021) demonstrated that meta-
reinforcement learning could be used to successfully train a robot capable of solving a wide 
range of locomotion tasks. 
2.2.6. Conclusion 
In this work, we present an analysis of learning-based control in multi-robot systems, 
emphasizing DRL algorithms. We review the formulations for different learning-based 
paradigms and typical robotics tasks (i.e., trajectory tracking, obstacle avoidance, etc.) and 
classify different approaches using artificial neural networks and bio-inspired techniques. 
Finally, we discuss the problems and possible research directions in the future. 

78 
Bibliography 
Agrawal, P., & Agrawal, H. (2013). A Review on Multi-Robot Cooperation Using Bio-
Inspired Neural Networks. International Journal of Soft Computing, Mathematics, 
and Control (IJSCMC), 2(4), 15-24. 
Ahn, H., Choi, H. L., Kang, M., & Moon, S. (2019). Learning-Based Anomaly Detection 
and Monitoring for Swarm Drone Flights. Applied Sciences, 9(24), 5477. 
Almadhoun, R., Taha, T., Seneviratne, L., & Zweiri, Y. (2019). A survey on multi-robot 
coverage path planning for model reconstruction and mapping. SN Applied 
Sciences, 1(8), 1-24. 
Alonso-Mora, J., Baker, S., & Rus, D. (2017). Multi-robot formation control and object 
transport in dynamic environments via constrained optimization. The International 
Journal of Robotics Research, 36(9), 1000-1021. 
Amigoni, F., Banfi, J., & Basilico, N. (2017). Multirobot exploration of communication-
restricted environments: A survey. IEEE Intelligent Systems, 32(6), 48-57. 
Balasubramanian, G., Muthukumaraswamy, S. A., & Kong, X. (2021). On the Performance 
Analyses of a Modified Force Field Algorithm and Neural Network Approach for 
Obstacle Avoidance in Swarm Robotics. SN Computer Science, 2(3), 1-8. 
Bekar, C., Yuksek, B., & Inalhan, G. (2020). High fidelity progressive reinforcement 
learning for agile maneuvering UAVs. In AIAA Scitech 2020 Forum (p. 0898). 
Bensoussan, A., Li, Y., Nguyen, D. P. C., Tran, M. B., Yam, S. C. P., & Zhou, X. (2020). 
Machine Learning and Control Theory. arXiv preprint arXiv:2006.05604. 
Bøhn, E., Coates, E. M., Moe, S., & Johansen, T. A. (2019, June). Deep reinforcement 
learning attitude control of fixed-wing UAVs using proximal policy optimization. 
In 2019 International Conference on Unmanned Aircraft Systems (ICUAS) (pp. 
523-533). IEEE. 
Cardona, G. A., Ramirez-Rugeles, J., Mojica-Nava, E., & Calderon, J. M. (2021). Visual 
victim detection and quadrotor-swarm coordination control in search and rescue 
environment. International Journal of Electrical & Computer Engineering (2088-
8708), 11(3). 
Chang, Y. H., Chan, W. S., Yang, C. Y., Chang, C. W., & Chung, T. C. (2012, June). 
Design of adaptive neural fuzzy formation controller for multi-robot systems. In 
2012 American Control Conference (ACC) (pp. 3161-3166). IEEE. 

79 
Chen, B., Chu, B., & Geng, H. (2020, December). Distributed Iterative Learning Control 
for Constrained Consensus Tracking Problem. In 2020 59th IEEE Conference on 
Decision and Control (CDC) (pp. 3745-3750). IEEE. 
Chen, C. P., Wen, G. X., Liu, Y. J., & Wang, F. Y. (2014). Adaptive consensus control for 
a class of non-linear multi-agent time-delay systems using neural networks. IEEE 
Transactions on Neural Networks and Learning Systems, 25(6), 1217-1226. 
Choudhary, A., Graduate, I. B., & Congress, I. N. (2019). Introduction to deep q-learning 
for reinforcement learning (in python). 
Chu, Z., Wang, D., & Meng, F. (2021). An Adaptive RBF-NMPC Architecture for 
Trajectory Tracking Control of Underwater Vehicles. Machines, 9(5), 105. 
Cohen, S., & Agmon, N. (2021). Recent Advances in Formations of Multiple Robots. 
Current Robotics Reports, 1-17. 
de Almeida, J. P. L. S., Nakashima, R. T., Neves-Jr, F., & de Arruda, L. V. R. (2019). Bio-
inspired online path planner for cooperative exploration of unknown environment 
by a Multi-Robot System. Robotics and Autonomous Systems, 112, 32-48. 
Dragoicea, M., Dumitrache, I., & Constantin, N. (2015). Adaptive neural control for mobile 
robots autonomous navigation. arXiv preprint arXiv:1512.03351. 
El Hamidi, K., Mjahed, M., El Kari, A., & Ayad, H. (2020). Adaptive Control Using Neural 
Networks and Approximate Models for Nonlinear Dynamic Systems. Modelling 
and Simulation in Engineering, 2020. 
Eoh, G., Jeon, J. D., Choi, J. S., & Lee, B. H. (2011, December). Multi-robot cooperative 
formation for overweight object transportation. In 2011 IEEE/SICE International 
Symposium on System Integration (SII) (pp. 726-731). IEEE. 
Galvez, R. L., Faelden, G. E. U., Maningo, J. M. Z., Nakano, R. C. S., Dadios, E. P., 
Bandala, A. A., ... & Fernando, A. H. (2017, November). Obstacle avoidance 
algorithm for swarm of quadrotor unmanned aerial vehicle using artificial potential 
fields. In TENCON 2017-2017 IEEE Region 10 Conference (pp. 2307-2312). IEEE. 
Gama, F., Isufi, E., Leus, G., & Ribeiro, A. (2020). From graph filters to graph neural 
networks. arXiv preprint arXiv:2003.03777. 
Gama, F., Marques, A. G., Leus, G., & Ribeiro, A. (2018). Convolutional neural network 
architectures for signals supported on graphs. IEEE Transactions on Signal 
Processing, 67(4), 1034-1049. 

80 
Gao, Y., Xu, H., Lin, J., Yu, F., Levine, S., & Darrell, T. (2018). Reinforcement learning 
from imperfect demonstrations. arXiv preprint arXiv:1802.05313. 
Hamilton, W. L., Ying, R., & Leskovec, J. (2017). Representation learning on graphs: 
Methods and applications. arXiv preprint arXiv:1709.05584. 
Hou, K., Yang, Y., Yang, X., & Lai, J. (2020). Cooperative control and communication of 
intelligent swarms: a survey. Control Theory and Technology, 18(2), 114-134. 
Hu, J., Niu, H., Carrasco, J., Lennox, B., & Arvin, F. (2020). Voronoi-based multi-robot 
autonomous exploration in unknown environments via deep reinforcement 
learning. IEEE Transactions on Vehicular Technology, 69(12), 14413-14423. 
Huang, H., Zhu, D., & Ding, F. (2014). Dynamic task assignment and path planning for 
multi-AUV system in variable ocean current environment. Journal of intelligent & 
robotic systems, 74(3), 999-1012. 
Huang, S., Teo, R. S. H., & Tan, K. K. (2019). Collision avoidance of multi unmanned 
aerial vehicles: A review. Annual Reviews in Control, 48, 147-164. 
Hüttenrauch, M., Adrian, S., & Neumann, G. (2019). Deep reinforcement learning for 
swarm systems. Journal of Machine Learning Research, 20(54), 1-31. 
Issa, B., & Rashid, A. T. (2019). A survey of Multi-Mobile Robots Formation Control. 
International Journal of Computer Applications,181(48), 12-16. 
Jabbari Asl, H., & Janabi-Sharifi, F. (2017). Adaptive neural network control of cable-
driven parallel robots with input saturation. Engineering applications of artificial 
intelligence, 65, 252-260. 
Jiang, C., Chen, Z., & Guo, Y. (2019, July). Learning decentralized control policies for 
multi-robot formation. In 2019 IEEE/ASME International Conference on Advanced 
Intelligent Mechatronics (AIM) (pp. 758-765). IEEE. 
Jin, L., Li, S., & Hu, B. (2017). RNN models for dynamic matrix inversion: A control-
theoretical perspective. IEEE Transactions on Industrial Informatics, 14(1), 189-
199. 
Johannsmeier, L., Gerchow, M., & Haddadin, S. (2019, May). A framework for robot 
manipulation: Skill formalism, meta-learning and adaptive control. In 2019 
International Conference on Robotics and Automation (ICRA) (pp. 5844-5850). 
IEEE. 
Khan, A., Tolstaya, E., Ribeiro, A., & Kumar, V. (2020, May). Graph policy gradients for 
large scale robot control. In Conference on robot learning (pp. 823-834). PMLR. 

81 
Kim, D. H. T., Manh, T. N., Van Bach, N. P., & Duc, T. P. (2019, January). Trajectory 
tracking control for omnidirectional mobile robots using direct adaptive neural 
network dynamic surface controller. In 2019 First International Symposium on 
Instrumentation, Control, Artificial Intelligence, and Robotics (ICA-SYMP) (pp. 
127-130). IEEE. 
Koushik, A. M., Hu, F., & Kumar, S. (2019). Deep Q- Learning-Based Node Positioning 
for Throughput-Optimal Communications in Dynamic UAV Swarm Network. 
IEEE Transactions on Cognitive Communications and Networking, 5(3), 554-566. 
Kshirsagar, J., Shue, S., & Conrad, J. M. (2018, April). A survey of implementation of 
multi-robot simultaneous localization and mapping. In SoutheastCon 2018 (pp. 1-
7). IEEE. 
Kumar, N., Panwar, V., Sukavanam, N., Sharma, S. P., & Borm, J. H. (2011). Neural 
network-based non-linear tracking control of kinematically redundant robot 
manipulators. Mathematical and Computer Modelling, 53(9-10), 1889-1901. 
Kuzhamuratov, A., Sorokin, D., Ulanov, A., & Lvovsky, A. I. (2021). Adaptation of 
Quadruped 
Robot 
Locomotion 
with 
Meta-Learning. 
arXiv 
preprint 
arXiv:2107.03741. 
Lan, X., Wu, Z., Xu, W., & Liu, G. (2018). Adaptive-neural-network-based shape control 
for a swarm of robots. Complexity, 2018. 
Latif, R., Dahmane, K., Amraoui, M., Saddik, A., & Elouardi, A. (2021). Evaluation of 
Bio-inspired SLAM algorithm based on a Heterogeneous System CPU-GPU. In 
E3S Web of Conferences (Vol. 229, p. 01023). EDP Sciences. 
Lee, H., & Kim, H. J. (2017). Trajectory tracking control of multirotors from modelling to 
experiments: A survey. International Journal of Control, Automation and 
Systems,15(1), 281-292. 
Li, B., Gan, Z., Chen, D., & Sergey Aleksandrovich, D. (2020). UAV Maneuvering Target 
Tracking in Uncertain Environments Based on Deep Reinforcement Learning and 
Meta-Learning. Remote Sensing, 12(22), 3789. 
Li, B., Yang, Z. P., Chen, D. Q., Liang, S. Y., & Ma, H. (2021). Maneuvering target 
tracking of UAV based on MN-DDPG and transfer learning. Defence Technology, 
17(2), 457-466. 
Li, J., Yang, S. X., & Xu, Z. (2019, December). A Survey on Robot Path Planning using 
Bio-inspired Algorithms. In 2019 IEEE International Conference on Robotics and 
Biomimetics (ROBIO) (pp. 2111-2116). IEEE. 

82 
Li, Q., Tai, C., & Weinan, E. (2019a). Stochastic modified equations and dynamics of 
stochastic gradient algorithms i: Mathematical foundations. The Journal of 
Machine Learning Research, 20(1), 1474-1520. 
Li, S., Ding, L., Gao, H., Chen, C., Liu, Z., & Deng, Z. (2018). Adaptive neural network 
tracking control-based reinforcement learning for wheeled mobile robots with 
skidding and slipping. Neurocomputing, 283, 20-30. 
Li, Z., Ge, S. S., & Liu, S. (2013). Contact-force distribution optimization and control for 
quadruped robots using both gradient and adaptive neural networks. IEEE 
transactions on neural networks and learning systems, 25(8), 1460-1473. 
Liessner, R., Dohmen, J., & Wiering, M. A. (2021). Explainable Reinforcement Learning 
for Longitudinal Control. In ICAART (2) (pp. 874-881). 
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... & Wierstra, D. 
(2015). Continuous control with deep reinforcement learning. arXiv preprint 
arXiv:1509.02971. 
Lin, J., Morse, A. S., & Anderson, B. D. (2003, December). The multi-agent rendezvous 
problem. In 42nd ieee international conference on decision and control (ieee cat. 
no. 03ch37475) (Vol. 2, pp. 1508-1513). IEEE. 
Lin, J., Yang, X., Zheng, P., & Cheng, H. (2020, May). Connectivity guaranteed multi-
robot navigation via deep reinforcement learning. In Conference on Robot Learning 
(pp. 661-670). PMLR. 
Liu, R., Shi, D., & Ma, C. (2014). Real-time control strategy of Elman neural network for 
the parallel hybrid electric vehicle. Journal of Applied Mathematics, 2014. 
Lopez-Gonzalez, A., Ferreira, E. D., Hernández-Martínez, E. G., Flores-Godoy, J. J., 
Fernandez-Anaya, G., & Paniagua-Contro, P. (2016). Multi-robot formation 
control using distance and orientation. Advanced Robotics,30(14), 901-913. 
Lou, W., & Guo, X. (2016). Adaptive trajectory tracking control using reinforcement 
learning for quadrotor. International Journal of Advanced Robotic Systems, 13(1), 
38. 
Ma, X., Chen, Y., Bai, G., Sha, Y., & Zhu, X. (2021). Path planning and task assignment 
of the multi-AUVs system based on the hybrid bio-inspired SOM algorithm with 
neural wave structure. Journal of the Brazilian Society of Mechanical Sciences and 
Engineering, 43(1), 1-15. 
Ma, Z., Wang, C., Niu, Y., Wang, X., & Shen, L. (2018). A saliency-based reinforcement 
learning approach for a UAV to avoid flying obstacles. Robotics and Autonomous 
Systems, 100, 108-118. 

83 
Maningo, J. M. Z., Faelden, G. E. U., Nakano, R. C. S., Bandala, A. A., & Dadios, E. P. 
(2015, December). Obstacle avoidance for quadrotor swarm using artificial neural 
network self-organizing map. In 2015 International Conference on Humanoid, 
Nanotechnology, 
Information 
Technology, 
Communication 
and 
Control, 
Environment and Management (HNICEM) (pp. 1-7). IEEE. 
Manko, S. V., Diane, S. A., Krivoshatskiy, A. E., Margolin, I. D., & Slepynina, E. A. (2018, 
January). Adaptive control of a multi-robot system for transportation of large-sized 
objects based on reinforcement learning. In 2018 IEEE Conference of Russian 
Young Researchers in Electrical and Electronic Engineering (EIConRus) (pp. 923-
927). IEEE. 
Manoonpong, P., Parlitz, U., & Wörgötter, F. (2013). Neural control and adaptive neural 
forward models for insect-like, energy-efficient, and adaptable locomotion of 
walking machines. Frontiers in neural circuits, 7, 12. 
Mazhar, N., Malik, F. M., Khan, R., Raza, A., Mazhar, S., & Irfan, M. (2020, April). Robust 
decentralized nonlinear formation control of multiagent quadrotor. In 2020 7th 
International conference on electrical and electronics engineering (ICEEE) (pp. 
303-309). IEEE. 
Mehmood, Y., Aslam, J., Ullah, N., Chowdhury, M., Techato, K., & Alzaed, A. N. (2021). 
Adaptive Robust Trajectory Tracking Control of Multiple Quad-Rotor UAVs with 
Parametric Uncertainties and Disturbances. Sensors, 21(7), 2401. 
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & 
Hassabis, D. (2015). Human-level control through deep reinforcement learning. 
Nature, 518(7540), 529-533. 
Muslimov, T. Z., & Munasypov, R. A. (2020). Adaptive decentralized flocking control of 
multi-UAV circular formations based on vector fields and backstepping. ISA 
transactions, 107, 143-159. 
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., & Abbeel, P. (2018, May). 
Overcoming exploration in reinforcement learning with demonstrations. In 2018 
IEEE International Conference on Robotics and Automation (ICRA) (pp. 6292-
6299). IEEE. 
Nascimento, T. P., Dórea, C. E., & Gonçalves, L. M. G. (2018). Nonholonomic mobile 
robots’ trajectory tracking model predictive control: a survey. Robotica, 36(5), 676. 
Ni, J., & Yang, S. X. (2011). Bioinspired neural network for real-time cooperative hunting 
by multi robots in unknown environments. IEEE Transactions on Neural Networks, 
22(12), 2062-2077. 

84 
Ni, J., & Yang, S. X. (2012). A fuzzy-logic based chaos GA for cooperative foraging of 
multi-robots in unknown environments. International Journal of Robotics and 
Automation, 27(1), 15. 
Ni, J., Wang, C., Fan, X., & Yang, S. X. (2014). A bioinspired neural model-based 
extended Kalman filter for robot SLAM. Mathematical Problems in Engineering, 
2014. 
Ni, J., Wu, L., Shi, P., & Yang, S. X. (2017). A dynamic bioinspired neural network-based 
real-time path planning method for autonomous underwater vehicles. 
Computational intelligence and neuroscience, 2017. 
Ni, J., Yang, X., Chen, J., & Yang, S. X. (2015). Dynamic bioinspired neural network for 
multi-robot formation control in unknown environments. International Journal of 
Robotics and Automation, 30(3), 256-266. 
Niroui, F., Zhang, K., Kashino, Z., & Nejat, G. (2019). Deep reinforcement learning robot 
for search and rescue applications: Exploration in unknown cluttered environments. 
IEEE Robotics and Automation Letters, 4(2), 610-617. 
Novitzky, M., Pippin, C., Collins, T. R., Balch, T. R., & West, M. E. (2012, December). 
Bio-inspired multi-robot communication through behavior recognition. In 2012 
IEEE International Conference on Robotics and Biomimetics (ROBIO) (pp. 771-
776). IEEE. 
Nurmaini, S., & Zarkasi, A. (2015). Simple Pyramid RAM-Based Neural Network 
Architecture for Localization of Swarm Robots. Journal of Information Processing 
Systems, 11(3). 
Oh, H., Shirazi, A. R., Sun, C., & Jin, Y. (2017). Bio-inspired self-organising multi-robot 
pattern formation: A review. Robotics and Autonomous Systems, 91, 83-100. 
Omidshafiei, S., Agha–Mohammadi, A. A., Amato, C., Liu, S. Y., How, J. P., & Vian, J. 
(2017). Decentralized control of multi-robot partially observable Markov decision 
processes using belief space macro-actions. The International Journal of Robotics 
Research, 36(2), 231-258. 
Pedro, J. O., & Crouse, A. J. (2015, May). Direct adaptive neural control of a quadrotor 
unmanned aerial vehicle. In 2015 10th Asian Control Conference (ASCC) (pp. 1-
6). IEEE. 
Peng, L., Guan, F., Perneel, L., Fayyad-Kazan, H., & Timmerman, M. (2018). 
Decentralized multi-robot formation control with communication delay and 
asynchronous clock. Journal of Intelligent & Robotic Systems, 89(3), 465-484. 

85 
Peng, Z., Wen, G., Yang, S., & Rahmani, A. (2016). Distributed consensus-based 
formation control for non-holonomic wheeled mobile robots using adaptive neural 
network. Nonlinear Dynamics, 86(1), 605-622. 
Pierson, H. A., & Gashler, M. S. (2017). Deep learning in robotics: a review of recent 
research. Advanced Robotics, 31(16), 821-835. 
Prabhu, S. M., & Garg, D. P. (1996). Artificial neural network-based robot control: An 
overview. Journal of Intelligent and Robotic Systems, 15(4), 333-365. 
Qian, B., & Cheng, H. H. (2018). Bio-inspired coalition formation algorithms for multi-
robot systems. Journal of Computing and Information Science in Engineering, 
18(2). 
Quiñonez, Y., de Lope, J., & Maravall, D. (2011, May). Bio-inspired decentralized self-
coordination algorithms for multi-heterogeneous specialized tasks distribution in 
multi-robot systems. In International Work-Conference on the Interplay Between 
Natural and Artificial Computation (pp. 30-39). Springer, Berlin, Heidelberg. 
Ravankar, A., Ravankar, A. A., Kobayashi, Y., & Emaru, T. (2016). On a bio-inspired 
hybrid pheromone signalling for efficient map exploration of multiple mobile 
service robots. Artificial life and robotics, 21(2), 221-231. 
Rossomando, F. G. (2014). Sliding mode control for trajectory tracking of a non-holonomic 
mobile robot using adaptive neural networks. Journal of Control Engineering and 
Applied Informatics, 16(1), 12-21. 
Ruiz, L., Gama, F., & Ribeiro, A. (2020). Gated graph recurrent neural networks. IEEE 
Transactions on Signal Processing, 68, 6303-6318. 
Sadhu, V., Sun, C., Karimian, A., Tron, R., & Pompili, D. (2020, December). Aerial-
DeepSearch: Distributed Multi-Agent Deep Reinforcement Learning for Search 
Missions. In 2020 IEEE 17th International Conference on Mobile Ad Hoc and 
Sensor Systems (MASS) (pp. 165-173). IEEE. 
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy 
optimization algorithms. arXiv preprint arXiv:1707.06347. 
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy 
optimization algorithms. arXiv preprint arXiv:1707.06347. 
Shirzadeh, M., Amirkhani, A., Jalali, A., & Mosavi, M. R. (2015). An indirect adaptive 
neural control of a visual-based quadrotor robot for pursuing a moving target. ISA 
transactions, 59, 290-302. 

86 
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M. (2014, January). 
Deterministic policy gradient algorithms. In International conference on machine 
learning (pp. 387-395). PMLR. 
Smola, A., Gretton, A., Song, L., & Schölkopf, B. (2007, October). A Hilbert space 
embedding for distributions. In International Conference on Algorithmic Learning 
Theory (pp. 13-31). Springer, Berlin, Heidelberg. 
Song, Y., Fang, X., Liu, B., Li, C., Li, Y., & Yang, S. X. (2020). A novel foraging algorithm 
for swarm robotics based on virtual pheromones and neural network. Applied Soft 
Computing, 90, 106156. 
Sun, C., He, W., Ge, W., & Chang, C. (2016). Adaptive neural network control of biped 
robots. IEEE transactions on systems, man, and cybernetics: systems, 47(2), 315-
326. 
Sun, R., Tang, C., Zheng, J., Zhou, Y., & Yu, S. (2019, August). Multi-robot Path Planning 
for Complete Coverage with Genetic Algorithms. In International Conference on 
Intelligent Robotics and Applications (pp. 349-361). Springer, Cham. 
Sun, T., Pei, H., Pan, Y., & Zhang, C. (2013). Robust adaptive neural network control for 
environmental boundary tracking by mobile robots. International Journal of Robust 
and Nonlinear Control, 23(2), 123-136. 
Sun, T., Shao, D., Dai, Z., & Manoonpong, P. (2018, August). Adaptive neural control for 
self-organized locomotion and obstacle negotiation of quadruped robots. In 2018 
27th IEEE International Symposium on Robot and Human Interactive 
Communication (RO-MAN) (pp. 1081-1086). IEEE. 
Sun, Y., Ran, X., Zhang, G., Wang, X., & Xu, H. (2020). AUV path following controlled 
by modified Deep Deterministic Policy Gradient. Ocean Engineering, 210, 107360. 
Tai, L., Zhang, J., Liu, M., Boedecker, J., & Burgard, W. (2016). A survey of deep network 
solutions for learning control in robotics: From reinforcement to imitation. arXiv 
preprint arXiv:1612.07139. 
Teng, Y. F., Hu, B., Liu, Z. W., Huang, J., & Guan, Z. H. (2017, December). Adaptive 
neural network control for quadrotor unmanned aerial vehicles. In 2017 11th Asian 
Control Conference (ASCC) (pp. 988-992). IEEE. 
Toda, Y., Suzuki, S., & Kubota, N. (2012, October). Evolutionary computation for 
intelligent self-localization in multiple mobile robots based on SLAM. In 
International Conference on Intelligent Robotics and Applications (pp. 229-239). 
Springer, Berlin, Heidelberg. 

87 
Tsai, C. C., Wu, H. L., Tai, F. C., & Chen, Y. S. (2017). Distributed consensus formation 
control with collision and obstacle avoidance for uncertain networked 
omnidirectional multi-robot systems using fuzzy wavelet neural networks. 
International Journal of Fuzzy Systems, 19(5), 1375-1391. 
Tuci, E., Alkilabi, M. H., & Akanyeti, O. (2018). Cooperative object transport in multi-
robot systems: A review of the state-of-the-art. Frontiers in Robotics and AI, 5, 59. 
Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., ... & Riedmiller, M. 
(2017). Leveraging demonstrations for deep reinforcement learning on robotics 
problems with sparse rewards. arXiv preprint arXiv:1707.08817. 
Verma, J. K., & Ranga, V. (2021). Multi-Robot Coordination Analysis, Taxonomy, 
Challenges and Future Scope. Journal of Intelligent & Robotic Systems, 102(1), 1-
36. 
Vicmudo, M. P., & Dadios, E. P. (2015, December). Artificial neural network controller 
for maintaining underwater swarm robots’ wireless connections. In 2015 
International Conference on Humanoid, Nanotechnology, Information Technology, 
Communication and Control, Environment and Management (HNICEM) (pp. 1-6). 
IEEE. 
Wang, M., Zeng, B., & Wang, Q. (2021). Research on Motion Planning Based on Flocking 
Control and Reinforcement Learning for Multi-Robot Systems. Machines, 9(4), 77. 
Wang, T., Qin, R., Chen, Y., Snoussi, H., & Choi, C. (2019). A reinforcement learning 
approach for UAV target searching and tracking. Multimedia Tools and 
Applications, 78(4), 4347-4364. 
Wang, X. S., Cheng, Y. H., & Yi, J. Q. (2007). A fuzzy Actor–Critic reinforcement learning 
network. Information Sciences, 177(18), 3764-3781. 
Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., & de Freitas, N. 
(2016). Sample efficient actor-critic with experience replay. arXiv preprint 
arXiv:1611.01224. 
Xu, B., Shi, Z., Yang, C., & Sun, F. (2014). Composite neural dynamic surface control of 
a class of uncertain non-linear systems in strict-feedback form. IEEE transactions 
on cybernetics, 44(12), 2626-2634. 
Xu, Q., Wang, Z., & Zhen, Z. (2019). Adaptive neural network finite-time control for 
quadrotor UAV with unknown input saturation. Nonlinear Dynamics, 98(3), 1973-
1998. 

88 
Yan, Z., Wang, M., & Xu, J. (2019). Global adaptive neural network control of 
underactuated autonomous underwater vehicles with parametric modeling 
uncertainty. Asian Journal of Control, 21(3), 1342-1354. 
Yang, S., Cao, Y., Peng, Z., Wen, G., & Guo, K. (2017). Distributed formation control of 
nonholonomic autonomous vehicle via RBF neural network. Mechanical Systems 
and Signal Processing, 87, 81-95. 
Yu, J., Ji, J., Miao, Z., & Zhou, J. (2019). Neural network-based region reaching formation 
control for multi-robot systems in obstacle environment. Neurocomputing, 333, 11-
21. 
Yu, R., Shi, Z., Huang, C., Li, T., & Ma, Q. (2017, July). Deep reinforcement learning 
based optimal trajectory tracking control of autonomous underwater vehicle. In 
2017 36th Chinese control conference (CCC) (pp. 4958-4965). IEEE. 
Zafar, M. N., & Mohanta, J. C. (2018). Methodology for path planning and optimization 
of mobile robots: A review. Procedia computer science, 133, 141-152. 
Zeng, W., Wang, Q., Liu, F., & Wang, Y. (2016). Learning from adaptive neural network 
output feedback control of a unicycle-type mobile robot. ISA transactions, 61, 337-
347. 
Zeng, Y., Li, J., Yang, S. X., & Ren, E. (2018). A bio-inspired control strategy for 
locomotion of a quadruped robot. Applied Sciences, 8(1), 56. 
Zhang, B., Sun, X., Liu, S., & Deng, X. (2019). Recurrent neural network-based model 
predictive control for multiple unmanned quadrotor formation flight. International 
journal of aerospace engineering, 2019. 
Zhaohui, D., Min, W., & Xin, C. (2008, July). Multi-robot cooperative transportation using 
formation control. In 2008 27th Chinese Control Conference (pp. 346-350). IEEE. 
Zhou, X., Wu, P., Zhang, H., Guo, W., & Liu, Y. (2019). Learn to navigate: cooperative 
path planning for unmanned surface vehicles using deep reinforcement learning. 
IEEE Access, 7, 165262-165278. 
Zhu, A., & Yang, S. X. (2010, July). An improved SOM-based approach to dynamic task 
assignment of multi-robots. In 2010 8th World Congress on Intelligent Control and 
Automation (pp. 2168-2173). IEEE. 
Zhu, D., Huang, H., & Yang, S. X. (2013). Dynamic task assignment and path planning of 
multi-AUV system based on an improved self-organizing map and velocity 
synthesis method in three-dimensional underwater workspace. IEEE Transactions 
on Cybernetics, 43(2), 504-514. 

89 
Zhu, D., Liu, Y., & Sun, B. (2018). Task assignment and path planning of a multi-AUV 
system based on a Glasius bio-inspired self-organising map algorithm. The Journal 
of Navigation, 71(2), 482. 
Zhu, D., Lv, R., Cao, X., & Yang, S. X. (2015). Multi-AUV hunting algorithm based on 
bio-inspired neural network in unknown environments. International Journal of 
Advanced Robotic Systems, 12(11), 166. 
 
 

90 
Chapter 3. 
 
 
Multi-robot 
Formation 
Control 
using 
Deep 
Reinforcement Learning (DRL) 
 
 

91 
3.1. 
Preliminaries  
Quadcopter UAV 
The content of this section is mainly based on the material presented in references (Fax & 
Murray, 2004; Hong et al., 2006; Olfati-Saber, 2006). We use Crazyflie 2.0 nano quadrotor 
to validate the proposed strategy, using the Crazyswarm package (Preiss et al., 2017).  
Frame definition 
We consider two frames: a fixed world frame (!), and a body frame (ℬ) for modelling of 
the quadrotor as shown in Figure 3.1. The position of the center of mass of the quadcopter 
is denoted as ξ (m) in the global frame, $ (rad) denotes the rotation in the global coordinate 
system, and ζ (rad/s) denotes the roll, pitch and yaw rate along ',( and ) axes. 
 
 
* = ['  (  )] 
$ = [.  /  0],
2 = [*  $  3] 
  4 = [5  6  7] 
(3.1) 
 
 
 
Figure 3.1 
The global frame (Green), and the (8)-configured body frame 
(Yellow). The positive direction of the rotors’ angular velocities in the body frame is 
also shown in the picture, along with the accompanying thrusts. 
 
 
0 
/ 
. 
)ℬ 
(ℬ 
'ℬ 
Ω: 
Ω; 
Ω< 
Ω= 
'! 
 
(! 
 
)! 
 

92 
The quadrotor translational kinematic is given by:  
 
 
*̇ = ? @ℬ 
(3.2) 
 
where *̇ (rad/s) denotes the absolute linear velocities, and @ℬ is the linear velocity 
expressed in body frame.  
We calculate the transformations of the rotations from the body frame (ℬ), to the 
world frame (!) using the Euler’s angles (roll, pitch and yaw) for each iteration as: 
 
 
A(.) = D
1
0
0
0
GHI .
IJK .
0
−IJK .
GHI .
M 
A(/) = D
GHI /
0
−IJK /
0
1
0
IJK /
0
GHI /
M 
A(0) = D
GHI 0
IJK 0
0
−IJK 0
GHI 0
0
0
0
1
M 
(3.3) 
 
Then, the rotation matrix ? = ?NO is defined as: 
 
 
D
G/G0
I.I/G0 −G.I0
G.I/G0 + I.I0
G/I0
I.I/I0 + G.G0
G.I/I0 −I.G0
−I/
I.G/
G.G/
M 
(3.4) 
 
where IQ = IJK Q, and GQ = GHI Q.  
The rotational kinematics of a quadrotor is characterized as: 
 
 
2 = D
.̇
0
0
M + ?(.) D
0
/̇
0
M + ?(.)?(/) D
0
0
0̇
M 
(3.5) 
 

93 
 
R
5
6
7
S = D
1
0
−I/
0
G.
I.G/
0
−I.
G.G/
M T
.̇
/̇
0̇
U = V T
.̇
/̇
0̇
U 
(3.6) 
 
 
T
.̇
/̇
0̇
U = 
⎣
⎢
⎢
⎢
⎡1
I. I/
G/
−G. I/
G/
0
G.
−I.
0
I.
G/
G.
G/
⎦
⎥
⎥
⎥
⎤
 R
5
6
7
S 
(3.7) 
 
whereT
.̇
/̇
0̇
U (rad) denotes the Euler angles derivatives vector as shown in Figure 3.2.  
 
 
Figure 3.2 
Sequence of Euler Angles Rotations from inertial to body-fixed frames 
with intermediate reference frames ]′′ and ]′. 
Quadrotor Dynamics 
To apply the above definitions and express the rotor dynamics using the principle of 
conservation of energy, we first define the inertia tensor in the body coordinate system as: 
 
 
_ℬ= T
`aa
0
0
0
`bb
0
0
0
`cc
U 
(3.8) 
 
' 
( 
) 
0 
d< 
d: 
d; 
/ 
d< 
d: 
d; 
. 
d< 
d: 
d; 
A(0) 
A(/) 
A(.) 
dee 
de 
d 
inertial 

94 
where `aa, `bb, and `cc are the moment of inertia around the body frame’s ', (, and 
) axes respectively.  
Finally, air resistance is introduced as a reactive force increasing with *̇, to make 
the UAV model more accurate. Therefore, the drag matrix in the body frame is defined as: 
 
 
fℬ= D
g<<
0
0
0
g::
0
0
0
g;;
M 
(3.9) 
 
where g<<, g::, and g;; can be approximated to be equal to hijk. 
The complete Lagrangian can be stated in terms of the systems kinetic, rotational, 
and potential energy using the above definitions: 
 
 
ℒm2, 2̇n = opjq + okrs + otrs 
= 1
2 v*̇w*̇ + 1
2 4O
w_O4O −vx*yz! 
(3.10) 
 
And the drag force can be found as (Palais & Palais, 2007): 
 
 
v?fℬ?ON = vhijk??ON_ = vfℬ 
(3.11) 
 
The translational dynamics of the quadcopter in the global frame can be expressed 
as follows using Newton’s second law of motion: 
 
 
v*̈ = ?|ℬ−fℬ*̇ −vxyz! 
(3.12) 
 

95 
where x is the acceleration of gravity constant, and yz! is the base vector of the )-
axis for the global frame. The rotation kinematics of the quadrotor is defined as:  
 
 
okrs = 1
2 4O
w_O4O = 1
2 $̇ w}w($)_O}($)$̇ 
(3.13) 
 
As a result, the angular portion of the Euler-Lagrange equation is defined as: 
 
 
~ = h
h• Ä1
2 $̇wÅ($) + 1
2 Å($)$̇Ç −1
2
É
É$ ($̇wÅ($)$̇) 
= Å($)$̈ + Å̇($)$̇ −1
2
É
É$ ($̇ wÅ($)$̇) 
= Å($)$̈ + Ñ($, $̇)$̇ 
(3.14) 
 
where Å($) = }w($)_O}($) 
In summary, the equations of motion are given by: 
 
 
*̈ = 1
v AÖℬ−1
v gℬ*̇ −x)̂! 
$̈ = áà<($)(~ℬ−â($, $̇)$̇) 
(3.15) 
 
We assume the quadrotor is a rigid body, and using rigid body mechanics, and the 
equations (3.5), (3.11), and (3.15) we can derive the complete dynamic model as: 

96 
 
⎩
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎧'̇ = é                                     
(̇ = è                                     
)̇ = ê                                     
é̇ = mGëIíGì + IëIìn î
v   
è̇ = mGëIíIì −IëGìn î
v   
ê̇ = mGëGín î
v −x             
.̇ = * + Ië
Ií
Gí
$ −Gë
Ií
Gí
3
/̇ = Gë$ −Ië3                    
0̇ =
Ië
Gí
$ +
Gë
Gí
3                  
*̇ = ~a
`aa
+
`bb −`cc
`aa
$3       
$̇ =
~b
`bb
+ `cc −`aa
`bb
*3       
3̇ = ~c
`cc
+ `aa −`cc
`cc
*$       
 
(3.16) 
 
Formation Control Structure 
Cao et al. (2013) categorized the formation control problem into two categories: the 
formation creating and formation tracking problems. In the absence of a group reference, 
formation creating refers to the algorithm design for a group of agents to reach some pre-
determined geometric pattern, which can also be called the control objective. Formation 
tracking, or formation control with a group reference, refers to the same task but with a 
predetermined group reference. 
Definition (formation task). A formation task for a group of UAVs with leader-
follower architecture represents a desired formation trajectory (the leader(s)) and a desired 
geometric pattern (desired inter-distance and orientation between neighboring UAVs) for 
a multi-quadrotor system with leader-follower architecture. 
The formation task specifies the integral behaviors of UAVs. Hence, the goal of 
formation control is to complete the formation task (i.e., trajectory tracking) using 
formation controllers.  

97 
In this thesis we use a decentralized and distributed formation control with leader-
follower configuration, where the formation trajectory is known to a portion of the UAVs 
called the leader group. 
Graph Theory 
The interaction topologies of agents in multi-agent systems are represented by a graph ! =
 (ï, ℰ) with sets of vertices ï = {1, 2, … , n} and edges ℰ. (|ï|) represents the cardinality 
of the set ï, which satisfies |ï| =  K. The collection of edges is represented by ℰ⊆ï × ï. 
A path between two vertices J, û is a sequence of edges in a graph of the form 
(J, J<), (J<, J:), . . . , (Jp, û). A graph ! is connected if there is a path between any two vertices, 
otherwise it is disconnected. The adjacency matrix of ! is denoted by !† = °j¢
i , where °j¢
i  
represents the entry on the ith row jth column of matrix !•. 
The neighbor set ¶ß  = {j ∈ ï ∶ (i, j) ∈ ℰ} of UAV J, is composed of the indices 
of the UAVs û, which are the neighbours of i. So, if °j¢
i > 0, UAV j is a neighbour of UAV J 
and the number of neighbours of the UAV i is equal to |¶ß|. 
!´ = hJ¨x(°<
≠, … , °q
≠) is a diagonal matrix that represents the status of leader 
UAVs, where the UAV J is leader if °j
≠> 0, otherwise it is a follower for J ∈ ï. 
Thus, the interaction matrix for leader-follower formation is defined as: 
 
 
N = !Æ −!• + !Ø 
(3.17) 
 
where 
!Æ 
is 
the 
degree 
matrix 
of 
N, 
which 
is 
denoted 
by 
hJ¨x{∑
°<¢
i
q
¢±<
, … , ∑
°q¢
i
q
¢±<
}. Also, !Æ −!• is called the Laplacian in graph theory. 
Flocking Consensus 
We analyze the consensus of the UAVs using the interaction matrix ! and graph 
Laplacians. In a 3D Euclidean space, the dynamics of UAV J is described as: 
 

98 
 
≤'̇j = èj                                   
è̇j = éj,
J = 1, 2, … , ≥ 
(3.18) 
 
The consensus algorithm éj
≠ for the leader is defined as: 
 
 
éj
≠= −¥j: µ
m'̇j −'̇¢n
¢∈∂∑
−¥j< µ
m'j −'¢n + 7̈
¢∈∂∑
−¥j: ('̇j −7̇) −¥j<('j −7) 
(3.19) 
 
And, the consensus algorithm éj
∏ for the follower is defined as: 
 
 
uß
∫= −kß: µ
mẋ ß −ẋ Ωn −kß< µ
mxß −xΩn
Ω∈æø
Ω∈æø
 
(3.20) 
 
where kß<, and kß: are some positive scalars. 
The goal of the neighbor-based consensus approach for UAV swarm leader-
follower formation is to ensure that error (¿j = 'j −7) converge to zero. Therefore: 
 
 
¿̈j
≠= −¥j: µ
m¿̇j −¿̇¢n
¢∈∂∑
−¥j< µ
m¿j −¿¢n −¥j:
¢∈∂∑
¿̇j
−¥j<¿j 
(3.21) 
 
 
¿̈j
∏= −¥j: µ
m¿̇j −¿̇¢n
¢∈∂∑
−¥j< µ
m¿j −¿¢n −7̈
¢∈∂∑
 
(3.22) 
 

99 
Remark. Comparing the dynamics of the tracking errors in (3.24) and (3.25), we 
observe that the second derivative of the reference trajectory exists in the error dynamics 
of the followers. Knowing that the reference trajectory is not available for the follower, the 
term 7̈ is an uncertain term for the followers. 
Following the definition of the adjacency matrix !•: 
 
 
°j¢
i = ≤1,
J¡ (J, û) ∈ℰ
0,
J¡ (J, û) ∉ℰ 
(3.23) 
 
Using the neighbor set of agents J, the degree matrix yields to: 
 
 
!√= hJ¨x{|¶<|,|¶:|, … , |¶q|} 
(3.24) 
 
And, 
 
 
°j
≠= ≤1,
•ℎ¿K J ∈ï´        
0,
•ℎ¿K J ∈ï −ï´ 
(3.25) 
 
We use ¿ to denote the collective tracking error of all the UAVs as: 
 
 
¿ = [¿<, ¿:,… , ¿q]w 
(3.26) 
 
Therefore, the dynamics of ¿ in the space follows: 
 
 
h
• ≈¿
¿̇∆= «
»q×q
`q
−…<(!√−!†)
−…:(!√−!†) ≈¿
¿̇∆
+ «
»q×q
»q×q
−…<(!´)
−…:(!´) ≈¿
¿̇∆
+ «
»q×q
(`q −!´)(7̈`q)  
(3.27) 
 

100 
 
…< =
⎣
⎢
⎢
⎢
⎡¥<<
0
0
0
0
0
¥:<
0
0
0
0
0
…
0
0
0
0
0
…
0
0
0
0
0
¥q<⎦
⎥
⎥
⎥
⎤
 
 
…: =
⎣
⎢
⎢
⎢
⎡¥:<
0
0
0
0
0
¥::
0
0
0
0
0
…
0
0
0
0
0
…
0
0
0
0
0
¥q:⎦
⎥
⎥
⎥
⎤
 
(3.28) 
 
 

101 
Bibliography 
Bangura, M., Melega, M., Naldi, R., & Mahony, R. (2016). Aerodynamics of rotor blades 
for quadrotors. arXiv preprint arXiv:1601.00733. 
Cao, Y., Yu, W., Ren, W., & Chen, G. (2012). An overview of recent progress in the study 
of distributed multi-agent coordination. IEEE Transactions on Industrial 
informatics, 9(1), 427-438. 
Fax, J. A., & Murray, R. M. (2004). Information flow and cooperative control of vehicle 
formations. IEEE transactions on automatic control, 49(9), 1465-1476. 
Hong, Y., Hu, J., & Gao, L. (2006). Tracking control for multi-agent consensus with an 
active leader and variable topology. Automatica, 42(7), 1177-1182. 
Hou, Z. (2016). Modeling and formation controller design for multi-quadrotor systems 
with leader-follower configuration (Doctoral dissertation, Compiègne). 
Luukkonen, T. (2011). Modelling and control of quadcopter. Independent research project 
in applied mathematics, Espoo, 22, 22. 
Olfati-Saber, R. (2006). Flocking for multi-agent dynamic systems: Algorithms and theory. 
IEEE Transactions on automatic control, 51(3), 401-420. 
Palais, B., & Palais, R. (2007). Euler’s fixed point theorem: The axis of a rotation. Journal 
of fixed point theory and applications, 2(2), 215-220. 
Preiss, J. A., Honig, W., Sukhatme, G. S., & Ayanian, N. (2017, May). Crazyswarm: A 
large nano-quadcopter swarm. In 2017 IEEE International Conference on Robotics 
and Automation (ICRA) (pp. 3299-3304). IEEE. 
 
 

102 
3.2. 
Deep Reinforcement Learning for Flocking Control of UAVs 
in Complex Environments 
3.2.1. Introduction 
Autonomous navigation and obstacle avoidance of swarm robots are essential, particularly 
in dynamic environments for mapping (McGuire et al., 2019, Madridano et al., 2021), 
coverage and inspection (Breitenmoser & Martinoli, 2016, Clark et al., 2017), search, and 
rescue (Arnold at al., 2018, Sampedro et al., 2019), and adaptive formation (Dang et al., 
2017, Skyrda, 2018, Kumar et al., 2019). 
A fundamental problem in swarm control formation (i.e., attitude and position) is 
stabilization and collision avoidance to ensure safe maneuvers while in formation. Thus, 
each robot should be aware of its neighbors' relative positions, either directly via different 
sensors (e.g., optical, audio) or indirectly via changes in the environment (Kagan et al., 
2019). However, such communication may not be available or reliable in practice. As a 
result, robust solutions for improving communication and predicting robot trajectories are 
required. This is especially crucial in complex and uncertain environments since robots 
have limited communication and perception capacities.  
The typical approach uses Reynolds’ boids (1987), in which agents flock together 
following a few simple rules: cohesion, collision avoidance, and velocity matching 
(alignment). This motion coordination technique has been widely used in robotics and 
control multi-robot systems for coordination and motion planning due to its simplicity and 
versatility (Kumar et al., 2017; Vásárhelyi et al., 2018; Khaledyan et al., 2019; Olcay et al., 
2020). 
However, setting rules to adapt to complex surroundings and systems is limited in 
these systems. Alternatively, researchers used reinforcement learning (RL) and deep 
reinforcement learning (DRL) approaches for adaptive and robust control. The goal of RL 
and DRL agents is to derive a control strategy and perform actions that maximize a reward 
function using the Markov decision process (MDP). MDP approximates the probability 
distribution of the reward over state-action pairs. 

103 
Hung et al. (2015) propose a Dyna-Q (Lambda) approach for the flocking formation 
of small fixed-wing UAVs in stochastic environments. Huang et al. (2017) proposed a deep 
Q-network (DQN) learning approach for connectivity preservation problems in multi-robot 
systems using a fully connected neural network (NN) with nonlinearities as the Q-function.  
Raja et al. (2019) suggested a method for flocking UAVs to discover optimal objective 
positions and a trajectory using DQN, which supports a collision-free movement to its 
assigned target. Young & La (2020) developed a hybrid multi-agent system that integrates 
consensus, cooperative learning, and flocking control to determine the direction of 
attacking predators and learn to flock away from them in a coordinated manner. 
Similar works can also be found in (Xu et al., 2018; Wang et al., 2021: Yan et al., 
2020; 2021). Several systems use flocking control to construct MDP state and action 
spaces, but the integration is still in the early phases. Furthermore, most of these research 
concentrate on flocking and predator avoidance behaviors while disregarding the 
navigation phase, and the environment is often free of obstacles or very simple. 
This work aims to develop a formation strategy that enables a UAV fleet to flock 
and navigate a complex environment (e.g., with an obstacle) without collision. We 
incorporate flocking control insights, formulate the problem of flocking UAVs in complex 
environments, and develop a learning control based on DDPG (Lillicrap et al., 2015) as 
one of the state-of-the-art DRL algorithms. 
Due to communication and perceptual limitations, the problem is framed as a 
partially observable Markov decision process (POMDP), in which each UAV only has 
partial information of the surroundings. We employ a hybrid approach using centralized 
training and decentralized execution to maintain the flock and optimize the learning 
control. Our main contributions are:  
• Learning-based flocking formation control for UAV swarm. 
• Decentralized formation control using POMDP for limited communication and 
perception. 
• We validate our approach using MATLAB and Simulink and compare the resulting 
performance to DQN and regular DDPG agents. 

104 
The rest of this chapter is organized as follows. The flocking problem is defined in 
next Section, followed by the details of the modeling process in the context of the MDP 
framework. In Section 3.3.3, we review the preliminaries and formulates the UAV flocking 
control as an RL problem. Section 3.3.4 describes the details about the proposed flocking 
control method with DRL. Simulation results and corresponding discussions are given in 
Section 3.3.5. Finally, in Section 3.3.6, the conclusions are presented. 
3.2.2. Related Work 
Multi-Robot Formation Control 
Distributed and decentralized formation control is initially inspired by natural swarms (i.e., 
bird flocking, fish schooling), where a collection of agents follow the same direction while 
maintaining a collective shape.  
To move the agents in formation, many control algorithms have been developed, 
which can be categorized into different groups depending on their control schemes, or 
sensing capabilities, including leader-follower (Mercado et al., 2013; Peng et al., 2013; Wu 
et al., 2017; He et al., 2018; Xuan-Mung & Hong, 2019), model-based and behavior-based 
(Xu et al., 2014; Shiell, 2017; Lee & Chwa, 2018), vision-based (Kumar et al., 2014; Lin 
et al., 2014; Wang et al., 2016 & 2018; Abbasi et al., 2017; Fathian et al., 2018; Li et al., 
2019) to name a few. 
The model-based formation control approaches need full or partial state 
measurement or a deliberate perception module that returns explicit measures (e.g., 
position, displacement, distance) from robot sensors via typical sensor fusion approaches. 
Hence, there is a trade-off between sensing capability and interaction topology. For 
example, consensus control (Huang et al., 2019) requires less sensing capability but more 
interactions among agents. 
In the last few years, a new research direction has been studied which utilizes neural 
networks (NNs) to analyze high-dimensional sensing data for autonomous driving directly 
(Sallab et al., 2017; Rausch et al., 2017; Tian et al., 2018; Kebria et al., 2019), and flight 
control (Zhang et al., 2017; Padhy et al., 2018; Amer et al., 2021).  

105 
Consequently, many researchers used different machine learning paradigms, 
including RL, for formation control. Lin et al. (2019) used a DRL-based model to guide a 
robot team through unknown complex terrain, where the geometric centroid of the robot 
team aims to reach the goal position while avoiding collisions and maintaining 
connectivity.  
Yu et al. (2019) developed a NN control scheme for multi-robot systems to achieve 
the region reaching formation control with collision and obstacle avoidance. Zhou et al. 
(2019) proposed a DRL methodology for the tracking, obstacle avoidance, and formation 
control of nonholonomic robots. They trained a DRL agent without using advanced physics 
or 3D modeling by splitting vision-based control into perception and controller modules. 
Furthermore, their modular framework avoids the time-consuming retraining of an image-
to-action end-to-end NN and allows the controller to be transferred to multiple robots. 
Fan et al. (2020) presented a DRL multi-scenario multistage training framework 
using NN to optimize multi-robot systems' fully decentralized sensor-level collision-
avoidance policy. Khan et al. (2020) proposed using graph policy gradients (GPG) to 
control many homogeneous robots. They employed graph convolutional networks (GCNs), 
which use a bank of graph filters to minimize the problem's dimensionality by learning 
filters that aggregate input among robots locally, inspired by convolutional neural networks 
(CNNs).  
Wang et al. (2021) proposed a motion planning approach based on flocking control 
and RL to ensure that all robots avoid collisions, maintain formation, and advance toward 
a target. 
Please refer to Grigorescu et al. (2020), Kiran et al. (2021), Choi & Cha (2019), and 
Mo & Farid (2019) for a comprehensive review of different machine learning techniques 
for mobile and aerial multi-robot systems. 
Neural Network Control for UAVs 
NN models have demonstrated robust results concerning the formation and trajectory 
tracking of unmanned aerial vehicles (UAVs). For example, Xiang et al. (2016) developed an 

106 
adaptive nonlinear controller based on dynamic inversion and NN for quadrotor UAVs in 
the presence of uncertainties and actuator dynamics.  
Heryanto et al. (2017) used a NN direct inverse control (DIC) for the attitude and 
altitude control of a UAV via a backpropagation (BP) learning algorithm. Jia & Duan 
(2017) developed an automatic target recognition method for UAVs based on a 
backpropagation artificial neural network (BPANN) algorithm to increase efficiency and 
decrease the recognition time. 
Muliadi & Kusumoputro (2018) proposed artificial neural network direct inverse 
control (DIC-ANN) to overcome the limitation of proportional–integral–derivative (PID) 
tuning for UAV altitude dynamics. Xiong et al. (2018) introduced a distributed adaptive 
control using radial basis function neural network (RBFNN) to tackle the time-varying 
formation tracking in the presence of delay for multi-UAV systems.  
Guzey et al. (2019) employed a nonlinear output feedback NN-based consensus 
controller for a group of quadrotor UAVs. Matthews & Yi (2019) proposed an adaptive 
NN controller to observe high-performance tracking in the presence of uncertainties for 
quadrotor UAVs.  
Rosales et al. (2019) developed an NN-PID controller for trajectory-tracking of 
hexacopter UAVs. Yu et al. (2019) presented a distributed fault-tolerant cooperative 
control (FTCC) strategy using fuzzy neural network (FNN) to achieve the attitude 
synchronization tracking of UAVs in the presence of actuator faults and model 
uncertainties.  
Semnani et al. (2020) developed a hybrid DRL and force-based motion planning 
(FMP) algorithm to solve distributed motion planning problems in dense and dynamic 
environments. Liu & Yi (2020) developed a NN modeling-based anti-disturbance tracking 
control framework for UAVs by combining the disturbance observer with proportional-
integral (PI) control.  
Zhang et al. (2020) proposed a NN adaptive control scheme for UAVs to achieve 
fixed-time stability and convergence time. Matthews & Yi (2021) employed a neural 
network -proportional plus velocity (NN-PV) controller for improved stability and 

107 
robustness of UAVs in the presence of external disturbances and model parametric 
uncertainty.  
3.2.3. Problem Formulation 
We use a simplified quadrotor model to tackle the UAV flocking control problem and 
assume UAVs fly at a nominal airspeed and constant altitude. The UAV's planar position 
and heading are measured globally, while its roll angle . is measured in a local body frame. 
Hence, we define the UAV state as a tuple * = (', (, ., 0, è), where (', () ∈ A: is the 
planar position, . ∈ V< is the roll angle, 0 ∈ V< is the yaw, and è is the nominal airspeed 
(Quintero et al., 2013). A block diagram of the proposed control system is shown in Figure 
3.3. 
The state of the UAV is assumed to evolve stochastically according to a Markov 
decision process (MDP), with the state transition probability function of transitioning from 
the current state * to the next state *À under the roll command .kÃ∏. 
Thus, the heading angle and speed are the UAV’s control command, and the 
kinematics of the UAV can be formulated as: 
 
 
*̇ = h
h• 
⎝
⎜
⎛ 
'
 (
 . 
0
è
 
⎠
⎟
⎞=
⎝
⎜⎜
⎛
 
è GHI 0
è IJK 0
 ¡(., .kÃ∏) 
−mQ”/èn •¨K .
¡(è, èkÃ∏)
 
⎠
⎟⎟
⎞
 
(3.29) 
 
where Q” is the acceleration due to gravity, ¡m., .kÃ∏n, .kÃ∏∈â defines the roll 
dynamics and I = Ö/v.  
MDP Flocking  
This section introduces the components of MDP flocking, including the state, action space, 
and reward functions adapted from Quintero et al., 2013. 
We consider a formation task for a UAV swarm composed of K UAVs with 
consensus architecture. We employ a decentralized and distributed control with a leader-

108 
follower configuration with a unique leader, where the formation trajectory is known to a 
portion of the UAVs called the leader group. 
State representation 
The flock of UAVs contains two groups: the leader (’) and the followers (¡, ∀ ¡ ≠’). As 
a result, the system state is divided into two parts: the leader’s state I≠, and the followers’ 
state I∏. As described by the kinematics model of UAV, the UAVs’ state can be represented 
by: *≠≔('≠, (≠,  .≠, 0≠, è≠), and *∏≔m'∏, (∏,  .∏, 0∏, è∏n. 
we define the system’s state to be the vector: I: = [I<, I:, … , I⁄]w, where [I< I:] 
denotes the 2D position of the follower relative to the leader as: 
 
 
≈I<
I:∆= « Gì¤
Iì¤
−Iì¤
Gì¤ ≈
'∏−'≠
(∏−(≠∆ 
(3.30) 
 
 
I; = 0∏−0≠ 
≈I=
I‹∆= «.∏
.≠
  
≈I›
Iﬁ∆= ≈è∏
è≠∆ 
≈Iﬂ
I⁄∆= R
.kÃ∏≠
èkÃ∏≠
S 
(3.31) 
 
where .kÃ∏≠ and èkÃ∏≠ are the leader’s roll angle and velocity setpoint, respectively. 
Action space 
UAVs employ control signals to maneuver by executing roll, heading, and velocity actions. 
Since UAV mobility is limited to a plane (', (), each UAV’s action space is defined by 
¨ ∶= (¨k , ¨‡), where ¨k(−·/2, ·/2) is the steering control signal and ¨‡(−1,1) is the 
velocity action. The next roll angle setpoint is defined by: 
 
 
.kÃ∏= ‚
7ë                J¡ . + ¨k > 7ë
−7ë               J¡ . + ¨k < −7ë
. + ¨k         H•ℎ¿7êJI¿           
 
(3.32) 
 

109 
where ‰−7ë, 7ëÂ is the allowed setpoint range of the roll angle. 
The next velocity setpoint is defined as:  
 
 
èkÃ∏= Ê
èÁia                J¡ è + ¨‡ > èÁia
èÁjq               J¡ è + ¨‡ < èÁjq
è + ¨‡           H•ℎ¿7êJI¿             
 
(3.33) 
 
where èÁia, and èÁjq represent the maximum and minimum velocity of the UAV, 
respectively. 
Reward function 
The goal of the flocking formation is to avoid collision between followers while following 
the leader. Therefore, the reward function consists of three components: flocking reward, 
mutual reward, and collision penalty.  
The flocking reward prompts the UAV swarm to remain as a flock (i.e., to keep 
each UAV close to the center of the swarm) and is defined as follows:  
 
 
7∏= (1 −∆0/·) + (1 −hj
È/hÁia
È
) 
(3.34) 
 
where, ∆0 is the difference between the current UAV’s heading angle and the 
average heading angle (
<
q ∑
0j
q
j±<
) of the swarm, and hj
È is the distance between the current 
UAV’s position and the center of the swarm. The center of the swarm is defined as: 
 
 
⎩
⎪
⎨
⎪
⎧'È = 1
K µ 'j
q
j±<
(È = 1
K µ (j
q
j±<
 
(3.35) 
 
The mutual or communication reward maintains the followers within a maximum 
distance of the leader. The goal is to maintain the graph connectivity by keeping the UAVs 
within the sensing range of each other. The mutual 7Á is defined as: 

110 
 
 
7Á = Ê−2     J¡ vJK µ hj
È
j
 < 10
0     ¿’I¿                            
 
(3.36) 
 
To prevent the collision of follower (J) with the leader (û) or other followers, the 
collision penalty 7È is defined as: 
 
 
7È = −v¨' Íh ,
h<|I;|
·(1 + Îh)Ï 
(3.37) 
 
where h = v¨'(h< −°, 0, ° −h:) is distance from the follower to an annulus Ì =
m(I<, I:) ∈A: ∶h< ≤° ≤h:n, and ° = ÔI<
: + I:
:. Î is a positive tuning parameter that 
adjusts the weight of h.  
The final reward function is specified as:  
 
 
7 = 7∏+ 7Á + µ 7Èj
j
 
(3.38) 
 
Assumption 1. During the flight, the roll angle of the quadrotor is relatively 
minimal, as a result: IJK . = 0, GHI . = 1.  
Remark 1. The quadrotor's operation is usually divided into four phases: takeoff, 
move, hover, and land. The quadrotor adjusts its attitude angles to vary its position. The 
roll angles are assumed to be relatively minimal in the traditional simple model. During the 
moving phase, these two angles should not be too large, limiting their mobility. We adjust 
its yaw and roll angles to ensure maneuverability while keeping its roll angle . ≈0 to 
guarantee the rationality of Assumption 1. 
DDPG Swarm Control  
This section introduces the major components of our methodology, including the network 
architecture and model training. 

111 
Since the control signals of UAVs are continuous, we consider a partially 
observable Markov decision process (POMDP) to solve the RL problem with continuous 
state and action space. Therefore, the goal of RL is to learn an optimal policy by 
maximizing a long-term reward r(Is, ¨s): V × Ì → A using a state-space V, an initial state 
Is, action space Ì, and an initial action ¨s. 
Network architecture 
We employ a deep deterministic policy gradient (DDPG) to approximate the actor-critic 
networks using DNNs. As illustrated in Figure 3.4, the actor-network takes states as input 
and consists of two convolutional layers with ReLU activation functions to approximate 
the actor-network Ú(I|/Û). The critic network's architecture is similar to the actor-network, 
but it takes both states and actions as inputs and approximates the action-state function 
Ù(I, ¨|/ı). Thus, its second hidden layer is concatenated with a control vector, and its 
output layer outputs a scalar.  
Note that the output layer of the critic employs a linear activation function. In 
contrast, the output layer of the actor uses a hyperbolic tangent (tanh) activation function, 
as shown in Figure 3.8. 
Training algorithm  
DDPG is a model-free, off-policy actor-critic algorithm that merges deterministic policy 
gradient (DPG) with DQN architecture. DDPG extends DQN to a continuous action space 
while learning a deterministic policy (using low-dimensional observations) (Lillicrap et al., 
2015). For exploration, DDPG employs a stochastic behavior strategy and deterministic 
policy for the target update. Furthermore, DDPG (similar to DQN) uses experience replay 
buffers and a frozen target network to stabilize the training. 
Training the critic network in DDPG is very similar to DQN. However, training the 
actor (policy) relies on the deterministic policy gradient theorem (Silver et al., 2014). 
DDPG employs four NNs: Q-network /ı, deterministic policy network /Û, a target 
Q-network /ıÀ , and a target policy network /Û́ . The agent aims to obtain a policy that 
maximizes the cumulative discounted reward from the start state, denoted by the 
performance objective á(/). 

112 
 
 
˜íá(/) ≈1
≥µ ˜iÙ(I, ¨|/ı)˜í¯Ú(I|/Û) 
(3.39) 
 
where /Û, and /ı are the actor and critic network's learning weights, respectively. 
The value network is updated similar to Q-learning, using the Bellman equation to 
update the revised Q-value as: 
 
 
(j = 7 + ˘ÙÀ mIj˙<, Ú́mIj˙<˚/Û́ n˚/ıÀ n 
(3.40) 
 
where ÙÀ  and Ú́ are the deterministic target and actor networks to calculate the next 
Q-values: 
 
 
ÙÀ = ÙÛmI, ¨˚/ıÀ n 
Ú́ =  ÚmI˚/Û́ n 
(3.41) 
 
Then it minimizes the mean-squared loss between the updated Q-value and the 
original Q-value, known as critic loss function: 
 
 
¸ = 1
≥µ m(j −Ù(Ij, ¨j|/ı)n
:
j
 
(3.42) 
 
Only a portion of learning weights in target networks get updated in each iteration 
using soft updates as: 
 
 
/ıÀ ←7/ı + Q(1 −˛)/ıÀ  
/Û́ ←7/Û + Q(1 −˛)/Û́  
(3.43) 
 
where Q and ˛ ≈1 determine the update rates of the actor-critic and target 
networks. 

113 
To summarize, DDPG develops a control policy utilizing the actor-critic approach 
by parameterizing the actor and the critic as two connected DNNs, and a replay memory. 
The proposed DDPG model for the UAV swarm is shown in Algorithm 1.  
 
 
 

114 
Action 
Follower 
Plant Encoder 
Leader  
Q Table  
I<, I:, … , I⁄ 
UAV 
Model 
DDPG 
'≠,(≠, .≠, 0≠, è≠ 
'∏,(∏, .∏,0∏, è∏ 
.kÃ∏, èkÃ∏ 
7∏, 7Á, µ 7È
j
j
 
.À∏,è́∏ 
Ij, ¨j 
LSTM layers 
                            Fully connected layer                           Quadrotor 
         Swarm 
 
 
 
 
 
 
 
Figure 3.3 
Network architecture of the proposed flocking control system. Three 
channels of information are taken as inputs: the leader UAV’s current state 
(8ˇ, !ˇ, "ˇ, #ˇ, $ˇ), and the follower state (8%, !%, "%, #%, $%), and desired roll angle 
and velocity ("&'%, $&'%). A joint representation of the inputs is created through a 
global plant encoder. 
 
 
 
 
 
 
 
Figure 3.4 
The block diagram of the DDPG with the actor-critic architecture. 
 
 
Reward  
Is 
Isà< 
7s 
∇íá(/) 
soft update 
/Û́ ←/Û 
Actor
Policy 
network /Û 
Target 
argument /Û́  
update 
/Û 
Deterministic 
Policy /Û 
soft update 
/ıÀ ←/ı 
Critic 
Target 
evaluation /ıÀ 
Policy 
network /ı 
update 
/ı 
Noise 
Action 
Is, ¨s, Is˙< 
Replay buffer 
Ij, ¨j, 7j, Ij˙< 
Ú(Ij|/Û) 
Environment 
Ú́mIj˙<˚/Û́ n 
Q-network 
/ı 
Loss 
function (j 

115 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Algorithm 1. DDPG for UAV Swarm Control 
       Input 
       ÖÁia maximum time steps  
       )d training batch size  
       ℰ  number of Epochs 
1 
Initialize actor & critic networks ←Ù(I, ¨|/ı), Ú(I|/Û)  
2 
Initialize target network (value functions) using (3.40) 
3 
Initialize the number of followers m, the leader & followers’ states 
4 
Initialize replay buffer R with capacity ) 
5 
for iteration • = 1, 2, … ℰ  do 
6 
  Initialize the environment setup 
7 
      for UAV j= 1, 2, . . , , do 
8 
        Observe the leader’s state *≠ and its own state ξ∫ 
                        ξ∫←ξ- 
9 
      end for 
10 
          for • = 1, 2, . . . , ÖÁia do 
11 
              for UAV i= 1, 2, . . , ≥ do 
12 
                Select the action aß (output of the actor network) with input sß 
13 
                Calculate .kÃ∏
∏ and èkÃ∏
∏ using (3.32) & (3.33)  
                                *À∏←(*∏, .kÃ∏
∏, èkÃ∏
∏) 
14 
              end for 
                                *À≠←m*≠, .kÃ∏
≠
, èkÃ∏
≠
n 
15 
            Choose .ÀkÃ∏
∏ and è́kÃ∏
∏ using  
16 
              for UAV i= 1, 2, . . , ≥ do 
17 
                Create subsequent system state Í ←(*À≠, *À∏, .À kÃ∏
≠
, è́kÃ∏
≠
) 
18 
                Calculate the reward r using (3.38) 
19 
                Store transition (Ij, ¨j, 7j, Ij˙<) in A if ‖A‖ > ) 
                                Ij ←Íj 
20 
              end for 
21 
            Sample a mini-batch of )d tuples (Ip, ¨p, 7p, Ip˙<) randomly 
22 
              for ¥ = 1, 2, . . , )d do 
23 
                Calculate error using: 
                                 (p = 7p + ˘ÙÀ 2Ip˙<, 2Ú́mIp˙<˚/Û́ n3˚/ıÀ 3 
24 
                Store tuple (¥) in A if (p > 0 
25 
              end for 
26                   Update critic by minimizing the loss: 
<
∂∑m(pÀ −Ù(Ip, ¨p|/ı)n
:
pÀ
 
27 
          end for 
28 
      Update actor using policy gradient:  
                      
<
∂∑˜4Ù(Ip, ¨p|/ı)|Ip,Ú(|Ip)˜íÚ(Ip|/Û)
pÀ
|Ip  
29 
      Update the weights of the target networks (soft updates) using (3.43) 
30 
end for 

116 
3.2.4. Simulation Results and Discussion 
We use Crazyflie 2.0 nano quadrotor dynamics to simulate and validate the proposed 
strategy. The simulation results and related comments are presented in this section.  
The deep neural networks are implemented using the Reinforcement Learning 
Toolbox in MATLAB and Simulink. Training is performed in simulation where we can 
train the network’s in a fast and safe way. In simulation, we place a quadrotor in a random 
configuration and velocity. The goal is to approach a target as quickly as possible. We 
validate that over time and after a few iterations, the performance is getting better. 
However, performance did not significantly change after 100 iterations, but we simply use 
more iterations, since training requires a relatively short time.  
We tested the flocking policy by dividing the training process into three parts. We 
first used 5 UAVs to practise flocking control in an environment with 20 obstacles, and 5 
targets. In Stage 2, based on the trained policy in Stage 1, 15 UAVs are used to train the 
flocking control policy in an environment with 35 obstacles, and 20 targets. Based on the 
policy trained in Stage 2, the flocking control policy is trained with 15 UAVs with 50 
obstacles, and 25 targets 50 in Stage 3. 
There are 1000 episodes in each training stage. The positions of the UAVs and 
obstacles are randomly reset at the start of each training episode. The UAVs flocking 
results generated by the trained policy trained are shown in Figures 3.6, 3.7 and 3.9 for 
different stages of the training.  
Our training results show that it is difficult for UAVs to form a flock in the early 
stages of training. However, in the second training the UAVs gradually learn to flock and 
receive a positive reward, which leads to the training process with increasing reward and 
robust flocking formation in the final experiment. The final experiment validates our 
approach and shows the trained flocking control policies can enable the UAV swarm to 
navigate in the training environments and get steady rewards, as shown in Figure 3.9. 
Initial Testing 
The flight transition is the most difficult portion of the flight, which depends on several 
inputs, has a high cost function and instability rate. The agents must balance the tilt angle 

117 
while controlling weights for the forward flight and quadrotor controllers to ensure the 
most efficient transition.  
 The agents can mistake measuring these weights, causing the simulation to become 
unstable and making training extremely difficult. We train the RL agents in the first 
experiment using minimal roll angles (~ 0) to avoid this complexity.  
The UAV is constrained by these adjustments, which compensate for any 
instabilities caused by control input combinations. The attitude controller was also utilized 
with minimal yaw (rate controller) to assure manoeuvrability. We used the Reinforcement 
Learning Toolbox in MATLAB and Simulink to simulate and train the RL agents using 
DNNs for both actor and critic networks. Figure 3.5 shows the configuration of the actor-
critic network for the initial testing. We found the initial actor-critic networks to be slow 
and unable to converge because the networks are large with many redundant perceptrons 
as shown in Figure 3.6. 
 
Figure 3.5 
Quadrotor UAVs’ initial actor-critic networks. A fully connected (FC) 
layer is featured by its type, number of neurons, and activation function. Other 
layers are represented by their type or name. 
256 I FC + ReLU  
256 I FC + ReLU 
I 
¨7xv¨' 
1 I FC    
+ Tanh 
I 
256 I FC + ReLU 
256 I FC + ReLU 
256 I FC + ReLU  
¨ 
1 I FC + Linear  
Actor Ú(I) 
Critic Ù (I, ¨) 

118 
 
Figure 3.6 
The integrated reward in the initial test for DDPG agents. 
UAVs Flocking with Planar Control 
The experiments show good results and the agents converge after approximately 1000 
episodes. Initially the number of targets was set to low (5) to make it easier for training, 
but increasing it at the start of next training to eventually 50 targets.  
At the start of the training the agents move somewhat randomly and collide. 
Throughout training and as the NNs are updated the agents converge to flying around the 
targets. The second training was done over 1000 episodes, sampled at intervals of 2, agents 
that reach the targets are the best agents from towards the end of training.  
The average reward provides a more accurate indication of how well the agents are 
learning. It is assumed that the actor and critic will initially be very distinct, and unable to 
converge quickly. However, after a few iterations the critic learns a good policy while the 
actor learns a close approximation of the reward function and begin to converge. This is 
demonstrated in Episode Q0, as the discounted long-term reward at the beginning of each 
episode.  

119 
The average reward initially decreases, but this is not concerning since the replay 
buffer is empty at first, and agents have no prior good experiences for training. After 400 
episodes, agents have found reasonable weights and good rewards as shown in Figure 3.7. 
 
 
 
 
 
 
 
 
Figure 3.7 
The integrated reward with improved parameters for flocking DDPG 
agents. 
In the next step we further train the agents in a dense environment (with up to 50 
obstacles). To improve the performance form previous training and ensure convergence, 
we altered the network architecture and added extra hidden layers as shown in Figure 3.8 
(this time beginning from the previously trained agent), with the maximum controller 
output increased from 0.5 m/s to 2 m/s. 
The increased controller output and the new architecture quickly improves the 
performance and the critic converges to the true reward after 400 episodes with more stable 
rewards. After only 200 episodes, the agent has advanced to the point where it began the 
previous training cycle, and around 350 episodes, the rewards match the best reward from 
the previous training cycle. The agents further improve their performance and surpass the 
previous training almost by two times rewards (~800). Once again, the agents are first 
trained with a low number of targets (~35) and as the performance improve this number is 
increased (to 50). Our results show stable convergence for actor network but the critic 
network was not successful. To improve the critic performance, we adjusted the noise 

120 
settings and rewards. These adjustments help to prevent local minima in exploration by 
increasing the amount of noise supplied to the policy and increasing exploration. By adding 
noise in the actor’s policy, the agents are forced to use considerably more control outputs, 
resulting in increased exploration, which reduces the training time and convergence. It was 
much more difficult than in the final experiment to choose appropriate hyper parameters 
for the UAV to fly with more output controllers, So, we tried different noise settings. 
Moreover, the agents require redundant neurons in their networks so during the training 
time, they have extra overhead to deal with transition.  
The episode reward is extremely noisy as shown in Figure 3.9. The average reward 
is a better indication of how effectively the agent is learning. The average reward initially 
gets worse, but this is not a cause for concern as initially the replay buffer will be empty, 
and the agent will not have any good experiences on which to base training. Initially it is 
expected that the actor and critic will be very different, and will not immediately start to 
converge. This is shown by the Episode Q0 – the discounted long-term reward at the start 
of each episode. As the training progresses, this should approach the true discounted long-
term reward. The parameters in Algorithm 1, and UAV model are listed in Table 3.1. 

121 
 
Figure 3.8 
The proposed (modified) actor-critic networks. A fully connected (FC) 
layer is featured by its type, number of neurons, and activation function. Other 
layers are represented by their type or name. 
 
 
 
 
 
 
 
 
 
Figure 3.9 
The integrated reward using the proposed network for flocking 
DDPG agents. 
Actor Ú(I)
Critic Ù(I, ¨) 
I 
256 I FC + 
64 I Conv 
+ ReLU 
256 I FC + 
Noise 
¨7xv¨' 
128 I Conv + 
1 I FC    
+
1 I FC + Tanh 
I 
64 I Conv + 
64 I Conv 
+ ReLU 
256 I FC + 
256 I 
FC +
256 I FC + 
¨ 
1 I FC + Linear  

122 
Table 3.1 
Training parameters in Algorithm 
Parameter 
Values 
T789 
1000 
N 
15 
M 
15 
γ 
0.99 
)b 
50 
ϕ, ψ 
~0° 
ϕ̇ , ψ̇  
1°/s 
x, y position 
[−20, 20] m 
d789
E
 
3 m 
x, y velocity 
[0.5,2] m/s 
vHI∫ 
1.5 m/s 
ϕHI∫ 
π/4° 
 
3.2.5. Conclusion  
We present a framework for the UAV swarm flocking control problem using DRL. The 
flocking control problem is formulated as a POMDP where each UAV has partial 
environment information due to its communication and perception limits. Then we utilize 
DDPG algorithm, to solve the RL problem using centralized training and decentralized 
execution process. The flocking control policy is designed as a DNN according to the 
UAV’s observation spaces and action spaces. Additionally, a reward function is added with 
the global flocking maintenance, mutual reward, and a collision penalty to avoid collision 
among UAVs and guarantee flocking and navigation. Finally, the simulation results are 
presented to validate the effectiveness of the proposed model. 
 
 

123 
Bibliography 
Abbasi, Y., Moosavian, S. A. A., & Novinzadeh, A. B. (2017). Vision-based formation 
control of aerial robots in the presence of sensor failure. Journal of Mechanical 
Science and Technology, 31(3), 1413-1426. 
Amer, K., Samy, M., Shaker, M., & ElHelw, M. (2021, January). Deep convolutional 
neural network based autonomous drone navigation. In Thirteenth International 
Conference on Machine Vision (Vol. 11605, p. 1160503). International Society for 
Optics and Photonics. 
Arnold, R. D., Yamaguchi, H., & Tanaka, T. (2018). Search and rescue with autonomous 
flying robots through behavior-based cooperative intelligence. Journal of 
International Humanitarian Action, 3(1), 1-18. 
Breitenmoser, A., & Martinoli, A. (2016). On combining multi-robot coverage and 
reciprocal collision avoidance. In Distributed Autonomous Robotic Systems (pp. 49-
64). Springer, Tokyo. 
Choi, S. Y., & Cha, D. (2019). Unmanned aerial vehicles using machine learning for 
autonomous flight; state-of-the-art. Advanced Robotics, 33(6), 265-277. 
Clark, R. A., Punzo, G., MacLeod, C. N., Dobie, G., Summan, R., Bolton, G., ... & 
Macdonald, M. (2017). Autonomous and scalable control for remote inspection 
with multiple aerial vehicles. Robotics and Autonomous Systems, 87, 258-268. 
Dang, A. D., La, H. M., Nguyen, T., & Horn, J. (2017). Distributed formation control for 
autonomous robots in dynamic environments. arXiv preprint arXiv:1705.02017. 
Fan, T., Long, P., Liu, W., & Pan, J. (2020). Distributed multi-robot collision avoidance 
via deep reinforcement learning for navigation in complex scenarios. The 
International Journal of Robotics Research, 39(7), 856-892. 
Fathian, K., Doucette, E., Curtis, J. W., & Gans, N. R. (2018). Vision-based distributed 
formation control of unmanned aerial vehicles. arXiv preprint arXiv:1809.00096. 
Grigorescu, S., Trasnea, B., Cocias, T., & Macesanu, G. (2020). A survey of deep learning 
techniques for autonomous driving. Journal of Field Robotics, 37(3), 362-386. 
Guzey, H. M., Dierks, T., Jagannathan, S., & Acar, L. (2019). Modified consensus-based 
output feedback control of quadrotor UAV formations using neural networks. 
Journal of Intelligent & Robotic Systems, 94(1), 283-300. 

124 
He, S., Wang, M., Dai, S. L., & Luo, F. (2018). Leader–follower formation control of USVs 
with prescribed performance and collision avoidance. IEEE Transactions on 
Industrial Informatics, 15(1), 572-581. 
Heryanto, M., Suprijono, H., Suprapto, B. Y., & Kusumoputro, B. (2017). Attitude and 
altitude control of a quadcopter using neural network based direct inverse control 
scheme. Advanced Science Letters, 23(5), 4060-4064. 
Huang, W., Wang, Y., & Yi, X. (2017, November). Deep q-learning to preserve 
connectivity in multi-robot systems. In Proceedings of the 9th International 
Conference on Signal Processing Systems (pp. 45-50). 
Huang, Z., Pan, Y. J., & Bauer, R. (2019). Leader–follower consensus control of multiple 
quadcopters under communication delays. Journal of Dynamic Systems, 
Measurement, and Control, 141(10). 
Hung, S. M., Givigi, S. N., & Noureldin, A. (2015, October). A dyna-q (lambda) approach 
to flocking with fixed-wing uavs in a stochastic environment. In 2015 IEEE 
International Conference on Systems, Man, and Cybernetics (pp. 1918-1923). 
IEEE. 
Jia, J., & Duan, H. (2017). Automatic target recognition system for unmanned aerial 
vehicle via backpropagation artificial neural network. Aircraft Engineering and 
Aerospace Technology. 
Kagan, E., Shvalb, N., & Ben-Gal, I. (Eds.). (2019). Autonomous Mobile Robots and 
Multi-Robot Systems: Motion-Planning, Communication, and Swarming. John 
Wiley & Sons. 
Kebria, P. M., Khosravi, A., Salaken, S. M., & Nahavandi, S. (2019). Deep imitation 
learning for autonomous vehicles based on convolutional neural networks. 
IEEE/CAA Journal of Automatica Sinica, 7(1), 82-95. 
Khaledyan, M., Liu, T., Fernandez-Kim, V., & de Queiroz, M. (2019). Flocking and target 
interception control for formations of nonholonomic kinematic agents. IEEE 
Transactions on Control Systems Technology, 28(4), 1603-1610. 
Khan, A., Tolstaya, E., Ribeiro, A., & Kumar, V. (2020, May). Graph policy gradients for 
large scale robot control. In Conference on Robot Learning (pp. 823-834). PMLR. 
Kiran, B. R., Sobh, I., Talpaert, V., Mannion, P., Al Sallab, A. A., Yogamani, S., & Pérez, 
P. (2021). Deep reinforcement learning for autonomous driving: A survey. IEEE 
Transactions on Intelligent Transportation Systems. 

125 
Kumar, S., Gupta, E., Kaushik, S., Srivastava, V. K., Saxena, J., Mehta, S., & Jyoti, A. 
(2019). Quantification of NETs formation in neutrophil and its correlation with the 
severity of sepsis and organ dysfunction. Clinica chimica acta, 495, 606-610. 
Lee, G., & Chwa, D. (2018). Decentralized behavior-based formation control of multiple 
robots considering obstacle avoidance. Intelligent Service Robotics, 11(1), 127-
138. 
Li, Z., Yuan, Y., Ke, F., He, W., & Su, C. Y. (2019). Robust vision-based tube model 
predictive control of multiple mobile robots for leader–follower formation. IEEE 
Transactions on Industrial Electronics, 67(4), 3096-3106. 
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... & Wierstra, D. 
(2015). Continuous control with deep reinforcement learning. arXiv preprint 
arXiv:1509.02971. 
Lin, F., Peng, K., Dong, X., Zhao, S., & Chen, B. M. (2014, June). Vision-based formation 
for UAVs. In 11th IEEE International Conference on Control & Automation 
(ICCA) (pp. 1375-1380). IEEE. 
Lin, J., Yang, X., Zheng, P., & Cheng, H. (2019, August). End-to-end decentralized multi-
robot navigation in unknown complex environments via deep reinforcement 
learning. In 2019 IEEE International Conference on Mechatronics and Automation 
(ICMA) (pp. 2493-2500). IEEE. 
Liu, B., & Yi, Y. (2020). Anti-disturbance Control for Unmanned Aerial Vehicles with NN 
Modeling. Dynamics, 16, 17. 
Madridano, Á., Al-Kaff, A., Gómez, D. M., & de la Escalera, A. (2019, September). Multi-
Path Planning Method for UAVs Swarm Purposes. In 2019 IEEE International 
Conference on Vehicular Electronics and Safety (ICVES) (pp. 1-6). IEEE. 
Matthews, M. T., & Yi, S. (2019, April). Model Reference Adaptive Control and Neural 
Network Based Control of Altitude of Unmanned Aerial Vehicles. In 2019 
SoutheastCon (pp. 1-8). IEEE. 
Matthews, M. T., & Yi, S. (2021, March). Neural Network Based Adaptive Flight Control 
of UAVs. In SoutheastCon 2021 (pp. 1-7). IEEE. 
McGuire, K. N., De Wagter, C., Tuyls, K., Kappen, H. J., & de Croon, G. C. (2019). 
Minimal navigation solution for a swarm of tiny flying robots to explore an 
unknown environment. Science Robotics,4(35). 
McGuire, K. N., De Wagter, C., Tuyls, K., Kappen, H. J., & de Croon, G. C. (2019). 
Minimal navigation solution for a swarm of tiny flying robots to explore an 
unknown environment. Science Robotics, 4(35). 

126 
Mercado, D. A., Castro, R., & Lozano, R. (2013, July). Quadrotors flight formation control 
using a leader-follower approach. In 2013 European Control Conference (ECC) 
(pp. 3858-3863). IEEE. 
Mo, H., & Farid, G. (2019). Nonlinear and adaptive intelligent control techniques for 
quadrotor uav–a survey. Asian Journal of Control, 21(2), 989-1008. 
Muliadi, J., & Kusumoputro, B. (2018). Neural network control system of UAV altitude 
dynamics and its comparison with the PID control system. Journal of Advanced 
Transportation, 2018. 
Olcay, E., Schuhmann, F., & Lohmann, B. (2020). Collective navigation of a multi-robot 
system in an unknown environment. Robotics and Autonomous Systems, 132, 
103604. 
Padhy, R. P., Verma, S., Ahmad, S., Choudhury, S. K., & Sa, P. K. (2018). Deep neural 
network for autonomous uav navigation in indoor corridor environments. Procedia 
computer science, 133, 643-650. 
Peng, Z., Wen, G., Rahmani, A., & Yu, Y. (2013). Leader–follower formation control of 
nonholonomic mobile robots based on a bioinspired neurodynamic based approach. 
Robotics and autonomous systems, 61(9), 988-996. 
Preiss, J. A., Honig, W., Sukhatme, G. S., & Ayanian, N. (2017, May). Crazyswarm: A 
large nano-quadcopter swarm. In 2017 IEEE International Conference on Robotics 
and Automation (ICRA) (pp. 3299-3304). IEEE. 
Quintero, S. A., Collins, G. E., & Hespanha, J. P. (2013, June). Flocking with fixed-wing 
UAVs for distributed sensing: A stochastic optimal control approach. In 2013 
American Control Conference (pp. 2025-2031). IEEE. 
Raja, G., Anbalagan, S., Narayanan, V. S., Jayaram, S., & Ganapathisubramaniyan, A. 
(2019, October). Inter-UAV Collision Avoidance using Deep-Q-Learning in 
Flocking Environment. In 2019 IEEE 10th Annual Ubiquitous Computing, 
Electronics & Mobile Communication Conference (UEMCON) (pp. 1089-1095). 
IEEE. 
Rausch, V., Hansen, A., Solowjow, E., Liu, C., Kreuzer, E., & Hedrick, J. K. (2017, May). 
Learning a deep neural net policy for end-to-end control of autonomous vehicles. 
In 2017 American Control Conference (ACC) (pp. 4914-4919). IEEE. 
Reynolds, C. W. (1987, August). Flocks, herds and schools: A distributed behavioral 
model. In Proceedings of the 14th annual conference on Computer graphics and 
interactive techniques (pp. 25-34). 

127 
Rosales, C., Soria, C. M., & Rossomando, F. G. (2019). Identification and adaptive PID 
Control of a hexacopter UAV based on neural networks. International Journal of 
Adaptive Control and Signal Processing, 33(1), 74-91. 
Sallab, A. E., Abdou, M., Perot, E., & Yogamani, S. (2017). Deep reinforcement learning 
framework for autonomous driving. Electronic Imaging, 2017(19), 70-76. 
Sampedro, C., Rodriguez-Ramos, A., Bavle, H., Carrio, A., de la Puente, P., & Campoy, 
P. (2019). A fully-autonomous aerial robot for search and rescue applications in 
indoor environments using learning-based techniques. Journal of Intelligent & 
Robotic Systems, 95(2), 601-627. 
Saulnier, K., Saldana, D., Prorok, A., Pappas, G. J., & Kumar, V. (2017). Resilient flocking 
for mobile robot teams. IEEE Robotics and Automation letters, 2(2), 1039-1046. 
Semnani, S. H., Liu, H., Everett, M., de Ruiter, A., & How, J. P. (2020). Multi-agent motion 
planning for dense and dynamic environments via deep reinforcement learning. 
IEEE Robotics and Automation Letters, 5(2), 3221-3226. 
Shiell, N. (2017). Behaviour-based pattern formation in a swarm of anonymous robots 
(Doctoral dissertation, Memorial University of Newfoundland). 
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M. (2014, January). 
Deterministic policy gradient algorithms. In International conference on machine 
learning (pp. 387-395). PMLR. 
Skyrda, I. (2018, May). Decentralized Autonomous Unmanned Aerial Vehicle Swarm 
Formation and Flight Control. In International Conference on Information and 
Communication Technologies in Education, Research, and Industrial Applications 
(pp. 197-219). Springer, Cham. 
Tian, Y., Pei, K., Jana, S., & Ray, B. (2018, May). Deeptest: Automated testing of deep-
neural-network-driven autonomous cars. In Proceedings of the 40th international 
conference on software engineering (pp. 303-314). 
Tron, R., Thomas, J., Loianno, G., Polin, J., Kumar, V., & Daniilidis, K. (2014). Vision-
based formation control of aerial vehicles. 
Vásárhelyi, G., Virágh, C., Somorjai, G., Nepusz, T., Eiben, A. E., & Vicsek, T. (2018). 
Optimized flocking of autonomous drones in confined environments. Science 
Robotics, 3(20). 
Wang, C., Wang, J., & Zhang, X. (2018, November). A deep reinforcement learning 
approach to flocking and navigation of uavs in large-scale complex environments. 
In 2018 IEEE Global Conference on Signal and Information Processing 
(GlobalSIP) (pp. 1228-1232). IEEE. 

128 
Wang, H., Guo, D., Liang, X., Chen, W., Hu, G., & Leang, K. K. (2016). Adaptive vision-
based leader–follower formation control of mobile robots. IEEE Transactions on 
Industrial Electronics, 64(4), 2893-2902. 
Wang, M., Zeng, B., & Wang, Q. (2021). Research on Motion Planning Based on Flocking 
Control and Reinforcement Learning for Multi-Robot Systems. Machines, 9(4), 77. 
Wu, F., Chen, J., & Liang, Y. (2017, March). Leader-follower formation control for 
quadrotors. In IOP Conference Series: Materials Science and Engineering (Vol. 
187, No. 1, p. 012016). IOP Publishing. 
Xiang, T., Jiang, F., Hao, Q., & Cong, W. (2016, September). Adaptive flight control for 
quadrotor UAVs with dynamic inversion and neural networks. In 2016 IEEE 
International Conference on Multisensor Fusion and Integration for Intelligent 
Systems (MFI) (pp. 174-179). IEEE. 
Xiong, T., Pu, Z., & Yi, J. (2018, July). Neural network based distributed adaptive time-
varying formation control for multi-uav systems with varying time delays. In 2018 
International Joint Conference on Neural Networks (IJCNN) (pp. 1-7). IEEE. 
Xu, D., Zhang, X., Zhu, Z., Chen, C., & Yang, P. (2014). Behavior-based formation control 
of swarm robots. Mathematical Problems in Engineering, 2014. 
Xu, Z., Lyu, Y., Pan, Q., Hu, J., Zhao, C., & Liu, S. (2018, June). Multi-vehicle flocking 
control with deep deterministic policy gradient method. In 2018 IEEE 14th 
International Conference on Control and Automation (ICCA) (pp. 306-311). IEEE. 
Xuan-Mung, N., & Hong, S. K. (2019). Robust adaptive formation control of quadcopters 
based on a leader–follower approach. International Journal of Advanced Robotic 
Systems, 16(4), 1729881419862733. 
Yan, C., Xiang, X., Wang, C., & Lan, Z. (2021). Collision-Free Flocking with a Dynamic 
Squad of Fixed-Wing UAVs Using Deep Reinforcement Learning. arXiv preprint 
arXiv:2101.08074. 
Yan, P., Bai, C., Zheng, H., & Guo, J. (2020, November). Flocking Control of UAV 
Swarms with Deep Reinforcement Leaming Approach. In 2020 3rd International 
Conference on Unmanned Systems (ICUS) (pp. 592-599). IEEE. 
Young, Z., & La, H. M. (2020). Consensus, cooperative learning, and flocking for 
multiagent predator avoidance. International Journal of Advanced Robotic 
Systems, 17(5), 1729881420960342. 
Yu, Z., Zhang, Y., Liu, Z., Qu, Y., & Su, C. Y. (2019). Distributed adaptive fractional-
order fault-tolerant cooperative control of networked unmanned aerial vehicles via 
fuzzy neural networks. IET Control Theory & Applications, 13(17), 2917-2929. 

129 
Zhang, C., Hu, H., & Wang, J. (2017). An adaptive neural network approach to the tracking 
control of micro aerial vehicles in constrained space. International Journal of 
Systems Science, 48(1), 84-94. 
Zhang, J., Li, Y., & Fei, W. (2020). Neural Network-Based Nonlinear Fixed-Time 
Adaptive Practical Tracking Control for Quadrotor Unmanned Aerial Vehicles. 
Complexity, 2020. 
Zhou, Y., Lu, F., Pu, G., Ma, X., Sun, R., Chen, H. Y., ... & Wu, D. (2019). Adaptive 
Leader-Follower Formation Control and Obstacle Avoidance via Deep 
Reinforcement Learning. arXiv preprint arXiv:1911.06882. 
 
 

130 
Chapter 4. 
 
 
Human-Swarm Interaction (HSI) 
 
 

131 
4.1. 
Preliminaries  
4.1.1. Flocking 
Flocking is a natural phenomenon observed in birds. It is a group formation for foraging or 
travelling toward a target location. Other examples are fish schooling, swarming behavior 
of insects, and ungulate herding. Flocking is an emergent global behavior that stems from 
the collective distribution of local interactions between individual autonomous agents. 
Many swarm robotic researchers have studied the mechanism of flocking and tried to 
imitate flocking in robotic swarms. 
In general, robots-with limited sensing capabilities- should be able to create a 
compact form by measuring distance and relative orientation of their neighbors. The usual 
assumption is that each robot has at least one adjacent neighbor, which connects them to 
the rest of the swarm.  
 Methods  
A central factor for imitating flocking behavior in swarming robots is the robot’s sensing 
ability. The current sensing and communication range technology is limited but has the 
practical implication: since only a limited number of neighbors, and not the entire 
population of the swarm, is detected by an agent. Also, this limited sensing ability is 
beneficial in regards to the scalability and processing complexity of the robots.  
Compared to other swarm techniques, flocking has an additional feature at the 
global level which is the alignment of the agent movement. This characteristic makes the 
group to collectively move towards a direction. Typically, animals who flock are able to 
orient themselves within the environment and therefore can move toward a common 
location that is known by all the individuals. To replicate this situation for a swarm of 
robots during flocking, either individual robots have the capability to reach a global target 
location or have a steering direction. However, most flocking studies in warm robotics 
favors the minimalistic approach with little or no global information shared by agents.  
Despite this lack of shared knowledge, the past studies demonstrated that flocking 
is also achievable without global information. In the following subsections, we review a 

132 
few notable proposed algorithms about the flocking behavior, either methods that relies on 
a global target location or direction for cases with no shared global knowledge.  
Direction by Global Target  
As mentioned before, shared knowledge of the target location by some or all agents of a 
swarm can guide robot movement and determine the heading direction of the flock. In case 
of having a shared knowledge about the target by all members, local interactions only serve 
to maintain the compact formation and avoid collision. If only some member, the informed 
robots have knowledge of the target, then local interactions help to spread this knowledge 
with the entire swarm. 
Emergent direction  
In previous section, we discussed the flocking behavior for either cases with shared 
knowledge of global direction or with some informed robots which steer the entire swarm 
toward the goal direction. However, swarm robotic studies showed that flocking can be 
obtained with absence of this information, in where agents initially move randomly and 
swarm flock emerges in global level form local interactions. 
4.1.2. Foraging  
Foraging is a commonly studied swarm behavior as the collective behavior of ants while 
searching for food. Ants and other social insects effectively search for food using indirect 
interactions with their nest-mates. In simulated foraging in an artificial swarm system, a 
specific area known as nest is needed. The objective of the swarm is to find scattered items 
in the arena and return them to the nest. In an extended version of a foraging task, different 
types of items must be collected and stored in specific nests.  
In simulated foraging in an artificial swarm robotics system, a specific area known 
as nest is needed. The objective of swarm is then to find scattered items within the arena 
and bringing them back to the nest. In an extended version of foraging task different types 
of items must be collected and stored to specific nests for the item type.  
Foraging is particularly useful for tasks such as demining, hazardous waste cleanup, 
search and rescue, and planetary exploration. Many swarm robotic studies analyzed the 

133 
dynamics for foraging activities which are generally about: collecting items by the robots 
and storing them to the nest.  
Methods  
The foraging task can be divided into two sequential sub-tasks: a robot is either searching 
for items in the environment, or carrying an item to the nest. The execution of either of 
these tasks depends on the mechanisms of cooperation between robots. Cooperation can 
also be useful for mitigating negative effects due to interference between robots and thus 
for improving the scalability of the system. 
The cooperation depends on some level of communication between agents, since 
the activity of the individuals affect the overall performance of the group. The 
communication can be achieved in different forms: via a shared memory, local 
modifications of the environment, or with direct exchange of information between agents. 
In the following subsections, we describe each form of communication in more details and 
review some existing swarm robotic studies about foraging.  
Shared memory  
In shared memory approach, all robots in a swarm are able to read and write information 
on a shared medium. A common analogy is to broadcast communication, by which each 
robot can exchange information with any other robot in the swarm.  
In principle, this strategy has some scalability and simplicity issues of individual 
robots. However, it can be useful to analyze the impact of shared information on foraging 
performance, and can offer insights for other communication mechanisms.  
Normally, the most important challenge of foraging task is finding the places of 
interest in the arena (the location of items and nest) and even if the robot has visited these 
places of interests in the past due to lack of global positioning system and/or the 
inaccuracies in calculating relative displacements, it might forget the path and forage to the 
same place in the future. So, a shared memory allows robot to communicate their recent 
experience and help other agents to localize places of interest and therefore be more 
efficient.  

134 
This partial and imperfect representation of the arena is better than the 
representational map that each robot can build alone (Vaughan et al., 2000 & 2000a). The 
representational map (the trail information) from foraging of robots can be used not only 
to follow the same trails to reach the same locations, but also to avoid trails used to reach 
different target locations (Sadat & Vaughan, 2010).  
Communication through the Environment  
Social insects such as ants are known to use pheromone (a chemical substance) to mark the 
environment and for communication. Swarm robots with pheromone capability can 
establish an indirect communication mechanism, which uses the environment as a medium 
for sharing information. The distributed pheromone can be detected form distance which 
augment the limited sensing can communication capabilities of robots. For example, in the 
foraging task, robots can use pheromone mediated communication to share their memory 
and to create routes in the environment (between nest and food sources) (Steels, 1990; 
Hamann & Wörn, 2006; Hecker et al., 2012).  
Robots can directly exchange information through transmitting data about a 
particular status. Usually to exchange of information robots should be within each other 
proximity range based on local communication principle. The robots then can modify their 
behavior to improve the forging task. In the noisy and crowded areas such as the nest, the 
effects of interference can be dramatically reduced by direct communication (Goldberg & 
Mataric, 2000). Robots can communicate simple by sensing the relative position of the 
neighboring robots (Hoff et al., 2013). 
 
 

135 
Bibliography 
Goldberg, D., & Mataric, M. J. (2000). Robust behavior-based control for distributed multi-
robot collection tasks. University of Southern California Los Angeles United 
States. 
Hamann, H., & Wörn, H. (2006, September). An analytical and spatial model of foraging 
in a swarm of robots. In International Workshop on Swarm Robotics (pp. 43-55). 
Springer, Berlin, Heidelberg. 
Hecker, J. P., Letendre, K., Stolleis, K., Washington, D., & Moses, M. E. (2012, 
September). Formica ex machina: ant swarm foraging from physical to virtual and 
back again. In International Conference on Swarm Intelligence (pp. 252-259). 
Springer, Berlin, Heidelberg. 
Hoff, N., Wood, R., & Nagpal, R. (2013). Distributed colony-level algorithm switching for 
robot swarm foraging. In Distributed Autonomous Robotic Systems (pp. 417-430). 
Springer, Berlin, Heidelberg. 
Sadat, S. A., & Vaughan, R. T. (2010, August). SO-LOST-An Ant-Trail Algorithm for 
Multi-Robot Navigation with Active Interference Reduction. In ALIFE (pp. 687-
693). 
Steels, L. (1990, July). Cooperation between distributed agents through self-organisation. 
In EEE International Workshop on Intelligent Robots and Systems, Towards a New 
Frontier of Applications (pp. 8-14). IEEE. 
Vaughan, R. T., Støy, K., Sukhatme, G. S., & Matarić, M. J. (2000, June). Whistling in the 
dark: cooperative trail following in uncertain localization space. In Proceedings of 
the fourth international conference on Autonomous agents (pp. 187-194). ACM. 
Vaughan, R. T., Støy, K., Sukhatme, G. S., & Matarić, M. J. (2000a). Blazing a trail: insect-
inspired resource transportation by a robot team. In Distributed autonomous robotic 
systems 4(pp. 111-120). Springer, Tokyo. 
 
 

136 
4.2. 
Swarm Art  
Swarm intelligence is one of the most beautiful and strange phenomena in nature. It 
emerges from the interaction of a group of agents (individuals) with their environment. The 
agents follow simple rules and, although there is no centralized control dictating how 
individuals should behave, actions or interactions between them lead to the emergence of 
intelligent global behavior, unknown to each agent. 
Swarm agents are autonomous agents. They are self-organized entities with at least 
two of the following characteristics. They have the ability to sense, actuate, communicate 
and make decisions without central or external control. Autonomous agents in the swarm 
perform in an ensemble and can perceived as a single entity or super-organism (Correll, 
2013). Widely recognized examples of swarms include but are not limited to birds flocking, 
bacteria growing, fish schooling, and the societal superorganisms of ant colonies (i.e., 
foraging). 
Gerardo Beni (1988) explored swarm intelligence in the context of cellular robotics 
where simple agents organize themselves through neighbourhood interactions. Beni also 
researched the use of swarm intelligence with colleagues (Beni & Wang, 1989; Beni & 
Hackwood, 1992; Branke, 1999). In the 1990s, studies of swarm behavior were mostly 
based on ant colony optimization (ACO) (Dorigo et al., 1991) and particle swarm 
optimization (PSO) (Kennedy & Eberhart, 1995).  
Computational developments inspired by swarm intelligence influenced a new 
category of art known as swarm art. Leonel Moura (2002) coined the term swarm art and 
was the first to identify a new kind of art based on swarm behaviors (i.e., flocking, foraging 
and stigmergy). The pioneer artworks are mostly inspired by ant colony behaviors and 
Reynolds Boids system (1987).  
4.2.1. Basic Definitions 
In this section, we give an overview of the origins and the concepts related to swarms. 

137 
What is an agent? 
The definition of agent varies widely depending on the field of study. Wooldridge (1997) 
defined agent as “an encapsulated computer system that is situated in some environment 
and that is capable of flexible, autonomous action in its environment in order to meet its 
design objectives.” At the extreme, an agent can be formalized as a function from percepts 
to actions with the following properties:  
• 
Autonomy: capability of cation without external intervention and control over the internal 
state 
• 
Situatedness: partial capability of preforming actions in the environment 
• 
Sociability: interacting with other agents (and possibly humans) 
• 
Reactivity: perceiving the environment and react to changes in a timely fashion 
• 
Proactivity: ability to not only react to the environment but also exhibit goal-oriented 
behavior (Wooldridge & Jennings, 1995) 
Intelligent agents can have additional characteristics including Mobility, 
Benevolence (Rosenschein & Genesereth, 1994), Rationality (Galliers, 1994), Adaptivity 
and Collaboration (Eichmann, 1994). 
Self-organization 
Self-organization is a unique and complex swarm behavior common in animals, especially 
social insects. Self-organization is the result of simple and local interactions between 
agents (members of the group), and emerges at the colony level. It explains different 
aspects of colony behavior and usually happens via four main mechanisms: multiple 
interactions between agents, positive or negative feedback, amplification of fluctuations, 
randomness, and reliance on multiple interactions (Bonabeau et al., 1999; Ducatelle et al., 
2011). Self-organization in social colonies usually happens by means of stigmergy, which 
is an indirect random communication strategy through the environment.  
4.2.2. Swarm Characteristics 
Swarm dynamics are a mystery and very complex to define. However, there are a few 
structural frameworks that aim to demystify swarm characteristics. Dréo et al. (2007) 
proposed the Adaptive Learning Search (ALS) framework to explain the structure of swarm 

138 
metaheuristics, Taillard et al. (2001) unified swarm dynamics of ant systems with genetic 
algorithms (GA) as adaptive memory programming (AMP) which is a memory-based 
metaheuristic. Chu at al. (2018) proposed the learning–interaction–diversification (LID) 
framework to understand the learning phases of swarms. Here, learning involves 
communication, processing of information, and synthesizing knowledge by each agent. An 
agent can be the self, local, global or naive learner. Interaction is the relationship between 
agents (i.e., cooperative or competitive). Diversification describes the control system and 
the algorithmic refinements such as diffuse and converging efforts. 
4.2.3. Swarm Algorithms  
As mentioned before, studies of swarm behavior are primarily based on ant colony 
optimization (ACO) (Dorigo et al., 1991) and particle swarm optimization (PSO) (Kennedy 
& Eberhart, 1995).  
Starting in the 2000s, scholars have been developing intelligent algorithms inspired 
by natural behaviors. Passino (2002) proposed bacterial foraging optimization which is 
inspired by the foraging behavior of bacterias (E. coli and M. xanthus). Krishnanand and 
Ghose (2005) developed glowworm swarm optimization (GSO) that simulates the behavior 
of the lightingworms. Pham et al. (2006) developed bees algorithm (BA), inspired by the 
foraging behavior of bees. Yang (2008) proposed the firefly algorithm (FA), inspired by 
the flashing behavior of fireflies. Similarly, Yang developed (2010) the bat algorithm (BA), 
a metaheuristic method based on the echolocation behavior of bats.  
Most contemporary algorithms are either deterministic or stochastic, and almost all 
swarm algorithms are stochastic in nature (Yang, 2016). Swarm algorithms are diverse in 
terms of their sources of inspiration in nature. There are several different ways of analyzing 
the essential components of swarm intelligence, such as focusing on the key factors for 
self-organization. Current research has shown that certain criteria are essential for the 
success of emergent intelligence through self-organizing behaviors  . They include 
feedbacks, stigmergy, multiple interactions, memory and environmental setting, all of 
which are very important (Yang, 2016). 

139 
All swarm algorithms share similar characteristics including having a population of 
individual agents capable of moving in a quasi-deterministic manner (algorithmic 
dynamics). These algorithmic dynamics regulate how the systems evolve, either by a set 
of predefined procedures (as used in a genetic algorithm) or by a set of equations (as used 
in PSO).  
Swarm algorithms also need a selection mechanism to find the best solution (or best 
solution to pass on to the next generation). The selection of these states/solutions together 
with the evolution of the population, allow the system to converge on a set of solutions 
(often the optimal set). As a consequence, some convergent states or solutions may emerge 
as iterations.  
Randomness is a key force that drives the system from equilibrium and to 
potentially abandon a local optimal situation. For example, both PSO and FA algorithms 
use randomization by selecting random numbers and stochastic models to generate 
solutions that are new in each iteration and sufficiently different from existing solutions. 
However, there are significant differences between PSO and FA. PSO uses the best solution 
found so far (x∗), but FA does not. Also, PSO is a linear system in terms of updating 
equations, but FA is nonlinear. The attraction mechanism in FA allows the swarm to 
subdivide into multiple small sub-swarms, which allows FA to solve multimodal problems 
more effectively. On the other hand, PSO cannot subdivide the swarm.  
Furthermore, most swarm algorithms show some level of evolution toward finding 
the best solution. At the initial stage, the found solutions have a much higher diversity and 
are usually different and uniformly distributed in the search space. In each iteration, due to 
their evolutionary nature, solutions become more similar to each other based on the 
selection mechanism and the fitness criteria (Lones, 2014). As noted before, the selection 
mechanism acts as a driving force for the evolution of the system and the fitness properties 
(based on some objective values), and dictates the behavior of the individual agents to adapt 
and react as they move toward the system goal (some specific, selected state or solution).  
There are many other ways of studying swarm intelligence to understand its 
algorithmic behavior (Corne, 2012; Yang, 2013) such as combinatorial metaheuristic 

140 
optimization (the use of exploration and exploitation) that was suggested by Dorigo et al., 
2003. 
In this thesis we utilized PSO and two common swarm behaviors: flocking and 
foraging as inspiration and control derivers of our multi-robot systems (to create noise 
music) which we discuss in detail in Sections 4.3 and 4.4.  
Please refer to my book (PA1) for a comprehensive review of swarm art and details 
about their design and architecture. We included a section from the book in Appendix B as 
a reference and to provide further information about swarm art. 
 
 

141 
Bibliography 
Beni, G. (1988, August). The concept of cellular robotic system. In Proceedings IEEE 
International Symposium on Intelligent Control 1988 (pp. 57-62). IEEE. 
Bonabeau, E., Marco, D. D. R. D. F., Dorigo, M., Théraulaz, G., & Theraulaz, G. (1999). 
Swarm intelligence: from natural to artificial systems (No. 1). Oxford university 
press. 
Branke, J. (1999, July). Memory enhanced evolutionary algorithms for changing 
optimization problems. In Proceedings of the 1999 Congress on Evolutionary 
Computation-CEC99 (Cat. No. 99TH8406) (Vol. 3, pp. 1875-1882). IEEE. 
Chu, X., Wu, T., Weir, J. D., Shi, Y., Niu, B., & Li, L. (2018). Learning-interaction-
diversification framework for swarm intelligence optimizers: a unified perspective. 
Neural Computing and Applications, 1-21. https://doi.org/10.1007/s00521-018-
3657-0 
Corne, D. W., Reynolds, A., & Bonabeau, E. (2012). Swarm intelligence. Handbook of 
natural computing, 1599-1622. 
Correll, N., Farrow, N., & Ma, S. (2013, February). Honeycomb: a platform for 
computational robotic materials. In Proceedings of the 7th International 
Conference on Tangible, Embedded and Embodied Interaction (pp. 419-422). 
ACM. https://doi.org/10.1145/2460625.2460718  
Dorigo, M., Birattari, M., Blum, C., Christensen, A. L., Engelbrecht, A., Groß, R., & 
Stützle, T. (2013). ANTS 2012 special issue. Swarm Intelligence, 7(2), 79-81. 
Dorigo, M., Maniezzo, V., & Colorni, A. (1991). The ant system: An autocatalytic 
optimizing process. 
Dréo, J., Aumasson, J. P., Tfaili, W., & Siarry, P. (2007). Adaptive learning search, a new 
tool to help comprehending metaheuristics. International Journal on Artificial 
Intelligence Tools, 16(03), 483-505. 
Ducatelle, F., Di Caro, G. A., Pinciroli, C., & Gambardella, L. M. (2011). Self-organized 
cooperation 
between 
robotic 
swarms. 
Swarm 
Intelligence, 
5(2), 
73. 
https://doi.org/10.1007/s11721-011-0053-0 
Eberhart, R., & Kennedy, J. (1995). A new optimizer using particle swarm theory. In 
MHS’95. Proceedings of the Sixth International Symposium on Micro Machine and 
Human 
Science 
(pp. 
39-43). 
Nagoya, 
Japan: 
IEEE. 
https://doi.org/10.1109/MHS.1995.494215 

142 
Eichmann, D. (1994). Ethical web agents. Computer Networks and ISDN Systems, 28(1-
2), 127-136. https://doi.org/10.1016/0169-7552(95)00107-3 
Galliers, J. R. (1994). A theoretical framework for computer models of cooperative 
dialogue, acknowledging multi-agent conflict. Doctoral dissertation, The Open 
University. pp. 49-54. 
Hackwood, S., & Beni, G. (1992, January). Self-organization of sensors for swarm 
intelligence. In Proceedings 1992 IEEE International Conference on Robotics and 
Automation (pp. 819-820). IEEE Computer Society. 
Krishnanand, K. N., & Ghose, D. (2005, June). Detection of multiple source locations using 
a glowworm metaphor with applications to collective robotics. In Proceedings 2005 
IEEE Swarm Intelligence Symposium, 2005. SIS 2005. (pp. 84-91). IEEE. 
Lones, M. A. (2014, July). Metaheuristics in nature-inspired algorithms. In Proceedings of 
the Companion Publication of the 2014 Annual Conference on Genetic and 
Evolutionary Computation (pp. 1419-1422). 
Moura, L. (2002). Swarm paintings-non-human. ARCHITOPIA Book, Art, Architecture 
and Science, 1-24. 
Passino, K. M. (2002). Biomimicry of bacterial foraging for distributed optimization and 
control. IEEE control systems magazine, 22(3), 52-67. 
Pham, D. T., Ghanbarzadeh, A., Koç, E., Otri, S., Rahim, S., & Zaidi, M. (2006). The bees 
algorithm—a novel tool for complex optimisation problems. In Intelligent 
production machines and systems (pp. 454-459). Elsevier Science Ltd. 
Reynolds, C. W. (1987). Flocks, herds and schools: A distributed behavioral model. In 
SIGGRAPH ‘87 Conference Proceedings, Vol. 21, No. 4, pp. 25-34. ACM Press, 
Anaheim, CA. https://doi.org/10.1145/37401.37406 
Rosenschein, J. S., & Genesereth, M. R. (1994). Deals among rational agents. In: 
Proceedings of the Ninth International Joint Conference on Artificial Intelligence 
(IJCAI-85), pp. 91-99. 
Taillard, É. D., Gambardella, L. M., Gendreau, M., & Potvin, J. Y. (2001). Adaptive 
memory programming: A unified view of metaheuristics. European Journal of 
Operational 
Research, 
135(1), 
1-16. 
https://doi.org/10.1016/S0377-
2217(00)00268-X 
Wang, J., & Beni, G. (1989, September). Cellular robotic system with stationary robots and 
its application to manufacturing lattices. In Proceedings. IEEE International 
Symposium on Intelligent Control 1989 (pp. 132-137). IEEE. 

143 
Wooldridge, M. (1997). Agent-based software engineering. IEE Proceedings-software, 
144(1), 26-37. https://doi.org/10.1049/ip-sen:19971026 
Wooldridge, M., & Jennings, N. R. (1995). Intelligent agents: Theory and practice. The 
knowledge 
engineering 
review, 
10(2), 
115-152. 
https://doi.org/10.1017/S0269888900008122 
Yang, X. S. (2010). Firefly algorithm, stochastic test functions and design optimisation. 
International journal of bio-inspired computation, 2(2), 78-84. 
Yang, X. S. (2011). Bat algorithm for multi-objective optimisation. International Journal 
of Bio-Inspired Computation, 3(5), 267-274. 
Yang, X. S., & Karamanoglu, M. (2013). Swarm intelligence and bio-inspired 
computation: an overview. Swarm intelligence and bio-inspired computation, 3-23. 
Yang, X. S., Deb, S., Fong, S., He, X., & Zhao, Y. X. (2016). From swarm intelligence to 
metaheuristics: nature-inspired optimization algorithms. Computer, 49(9), 52-59. 
 
 

144 
4.3. 
Exploiting Swarm Aesthetics in Sound Art  
4.3.1. Introduction 
Sound as a conceptual medium is influencing our art culture. Many contemporary artists 
began to explore sound in its pure state, simultaneously bridging and blurring the notion of 
sound, noise, and music. In the past few decades there has been several approaches using 
robotics, mechatronics and artificial intelligence (AI) for developing musical 
improvisations, sonification, orchestra, and sound art. The goal in most cases is to push the 
boundaries of conventional music and explore the infinite possibilists of randomness, 
chance, noise-sounds, and glitch. Furthermore, robotic and electromechanical machines 
with embedded automation and performative capabilities, extended the musical creation 
process.  
We discuss the related art about mechatronic sound objects and musical robots in 
Section 4.2.2, using a number of examples, followed by the emerging interest in noise. 
Then, we discuss the aesthetic values of chaos and swarming techniques in sound art. In 
Section 4.2.3 we present Liminal Tones (A / Autumn Swarm), its sound mechanism, 
technical features, swarm dynamics, and architecture. Finally, we discuss our initial results 
and the future work in Section 4.2.4. 
4.3.2. Background 
A key pioneer of the renewed interest in musical robots is Gottfried-Willem Raes, the 
founder of the Logos Foundation (1968). Logos was influenced by anti-authoritarianism 
opposition and radical denial of serialism and post-serialism in music. As Raes (1992) 
argues, this refusal was rooted in the late 1960s’ musical trends and the “desire to conquer 
the hierarchy of power involving music and its producers” (p. 29). Another music roboticist 
and key figure is Trimpin who mostly used physical objects, actuated mechatronic systems 
and obsolete machines to create sonic environments and drumming apparatus (Murphy et 
al., 2012).  
Both Raes and Trimpin’s artistic practice laid a rich foundation for the 
contemporary sound art and inspired a few of the current musical trends. Murphy (2012) 

145 
categorized these practices into three separate streams: “works making use of found 
objects, works consisting of purpose-built instruments, and sculptures using 
automatophonic instruments.” Sound sculptor Gordon Monahan (2011) creates sound 
sculpture, installation, and sonic environments exemplifying the subfield of found-object 
musical robotics. For example, in his installation Trembling Antennae for Henning 
Christiansen (2013) Monahan used electric motors as sound diaphragms to amplify audio 
signals into the exhibition space. Similarly, Jon Pigott (2011) explore sound, technology, 
and material system in sonic art and noise music.  
Other works in this group are the solenoid-based instruments of Chris Kaczmarek 
and the noise-making assemblages of Peter William Holden. Prominent in this category is 
Nicolas Bernier, the winner of Prix Ars Electronica Golden Nica prize in the Digital Music 
& Sound Art (2013) for his artwork “Frequencies (A)” as shown in Figure 4.1 Bernier’s 
sound performances and practice over the years evolved from chaotic noise-based approach 
to a more minimal and pure focus on sounds and exploration about the relation between 
music, conventional mediums and new technology. His works echo the interplay of digital 
sounds and light to create an elegant balance between the logical and the sensual. Similarly, 
Moritz Simon Geist makes electronic musical robots (Figure 4.2) and vibraphones trying 
to push and extend the boundaries of music and explores the unknown and futuristic world 
of techno robotics. Geist questions our perception of technology and artificial intelligence 
in a playful and entertaining style.  
In a different approach, a group of artists create purpose-built noise systems. 
Examples of using mechatronic sound-objects can be found in works of Zimoun and as 
shown in Figure 4.4 who combines visual, sonic, and spatial elements to create sound 
sculptures, sound architectures and installations. Zimoun and Pe Lang usually use large 
numbers of mechanical elements (generally hundreds of them) such as DC-motors, and 
other actuators, as sound-producing objects. They refer to these elements as “prepared DC 
motors or actuators” which often resemble biological systems and evoke eerie or uncanny 
feelings (Stoddart, 2015). In the third approach, sound artists create automatophonic sound 
sculpture made of altered instruments. For example, “Mechanical Orchestra of França 
Xica”, as an interconnected web of altered instruments by Roger Aixut or “Felix’s 
Machines” by Felix Thorn. 

146 
 
Figure 4.1 
Frequencies (A), Sound performance of mechanically triggered tuning 
forks with pure digital soundwaves. Nicolas Bernier, 2013. 
 
 
Figure 4.2 
a shot from Robotic Electronic Music (R.E.M), using music robots, 
mechanics and sound devices. Moritz Simon Geist, 2019. 
 

147 
Although these group of artists seem to have different tastes in form and 
approaches, all of them seem motivated by a desire to explore sounds and sonic 
characteristics of space that are not otherwise accessible through traditional music (Murphy 
et al., 2012). For example, both Monahan and Zimoun are focused on exploring acoustic 
sounds in space (Muecke & Zach, 2007). 
Noise as Music 
From late 20th century and onward there has been an emergent aesthetic and musical 
phenomenon known as “Noise Music”. More recently different artists used noise to create 
audiovisual performances (e.g. Frank Bretschneider, Michael Kummer and Ryoichi 
Kurokawa), compositions (e.g. Aoki Takamasa, Mika Vainio), or installations and sound 
sculptures (e.g. Ryoji Ikeda, Nicolas Bernier and Mo Zareei, Figure 4.3). 
These works share some common features such as dodging harmonic material and 
embracing sounds otherwise known as “extra-musical” including concrete sounds, noise, 
sonic glitches, etc. Also, they have a minimal approach and often use multiple sound-
objects, or pulse-based rhythms, complex noisy timbres, repeated patterns, or recurring 
images and stroboscopic visuals (Zareei, 2016). 
Order, Chaos and Sonic Swarms 
Throughout the history of western music many composers, and musicians used natural 
sound as a source of inspiration in their work particularly the sound of wind, water, and 
birds. The interest of mimicking the natural sound is also manifested in the works of a few 
contemporary sound artists such as Nelo Akamatsu, Pe Lang and Zimoun.  Nelo Akamatsu 
has a minimal approach to sound art that is rooted in Japanese culture and their delicate 
perception of nature. He often uses a few elements such as water, tumblers, wires to create 
sounds. These gentle sounds multiplied by several hundred, creates an organic symphony 
and a minimal expression of perceived nature (reminisce of natural swarms) in a mythical, 
magical and repetitive pattern. Pe Lang and Zimoun create sound sculptures and 
installations with rhythms and flow using basic mechanical components (as sound objects), 
in large numbers. In their practice, both together or individually, they create analogue 
rhythms and flow and study the creation and degeneration of patterns. Inspired by 
generative systems, and swarm behaviors, their works display both simplicity and 

148 
complexity. The emergent and intricate behaviors   of these sound objects (in sound and 
motion) appear to be organic or alive and sound like “the acoustic hum of natural 
phenomena” (Schlatter, 2013). 
4.3.3. Liminal Tones 
Concept 
Liminal Tones (A / Autumn Swarm) is a series of sound compositions generated by 8 
vibration motor, wires and actuators. We used a swarming technique and a specific control 
loop mechanism to regulate the DC motors and makes the wire move, twist and turn. The 
moving wires make tiny sounds, which is accompanied by noise of DC motors and form 
rhythmic sounds that is both organized and chaotic.  
System Overview 
We used a Laptop, an Arduino Uno board, and a multi-channel driver board (8 Channel 
DC 5V Relay Module). The output signals are generated by the Arduino in response to the 
incoming MIDI velocities, which in turn drive the DC motors, and attached wires as shown 
in Figures 4.6 and 4.7. Moreover, to control the frequency and speed of the DC motors we 
used a swarm control loop known as PSO-based PID Controller. 
PSO-based PID Controller  
It is inherently difficult to tune Proportional-Integral Derivative (PID) loops, and its 
parameters and most commonly the tuning process is done through trial and error. 
However, to be able to automatically control multiple DC motors we adapted a heuristic 
algorithm known as Particle swarm optimization (PSO) from Hashim & Mustafa (2020).  
PSO-based PID Controller is a robust and nonlinear parameter tuning process for 
synchronous and stabilizing E. PID controllers have been widely used to control the speed 
fluctuation and frequency of the DC motors in different control systems such as process 
control, motor drives, magnetic and optical memory, etc. In the most simplistic terms, PID 
controller calculates the P, I, and D parameters and multiplies each by an error (e) and then 
calculates the sum as Control Variable (CV) as illustrated in Figure 4.6. The proportional 
term (Gain or …M) is a ratio that controls how fast the DC motors responds. 
 

149 
 
 
Figure 4.3 
Material Music, a sound installation consists of a linear array of eight 
kinetic sound- sculptures at International Symposium on Electronic Art (ISEA). Mo 
H. Zareei, 2020. 
 
Figure 4.4 
an installation using 51 prepared dc-motors, 241m rope, cardboard 
sticks 25 cm, Museum of Contemporary Art MAC, Santiago de Chile. Zimoun, 
2019. 

150 
The integral term (I Constant or …N) determines how fast the error is removed. 
Finally, the derivative term (D Constant or …√) predicts the rate of change in the Process 
Variable (PV). The PID controller is described as:  
 
 
é(•) = …M¿(•) + …N O ¿(•)h• + …√
h¿(•)
h•  
(2.1) 
 
where ¿(•) = 7(•) −((•) represents the tracking error and the difference between 
the desired input value and the actual output. The main advantage of PSO algorithm is that 
it is an auto-tuning method and does not require detailed mathematical process to find the 
…M, …N and …√ and tune the PID process control parameters as illustrated in Figure 4.5. 
Particle Swarm Optimization (PSO)  
Particle swarm optimization (PSO) is a heuristic optimization technique that was developed 
by Kennedy and Eberhart in 1995, inspired by social behavior of animals such as bird 
flocking, or fish schooling.  
PSO begins by creating a number of artificial particles, and assigning them initial 
velocities, then it explores the space of the objective function and adjust the trails of each 
agent (or particle). The position of each particle is updated based on the agent’s history (the 
current and best previous locations), other members of the swarm (the global optimizer 
value) and some random perturbations (Brownlee, 2012). The new position of each particle 
is computed as the sum of its previous position with a quantity that is estimated according 
to several factors, depending on the PSO variant and eventually the swarm flock around a 
desired area. The particle position in PSO can be modeled as:  
 
 
'j = […M, …N, …√, …MP, …√P] 
(4.2) 
 
where ' is the particle position, …M, …N, …√ are the proportional, integral, and 
derivative values of PID controller to control speed, torque and voltage of DC motors, 

151 
correspondingly. While …MP and …√P are the proportional, and derivative values of PID 
controller to control the oscillation. The particles initialization is computed using: 
 
 
'j = 'Ájq + 7¨h('Áia −'Ájq) 
(4.3) 
 
where 'Ájq and 'Áia are the minimum and maximum values in the search space. 
Each particle is assessed by fitness function and particles with a minimum fitness value 
compare to best local and global values are updated. Each particle represents a candidate 
solution for PID parameters. A good set of PID controller parameters can yield to a flocking 
behavior, and optimal control of DC motors (Allaoua et al., 2009). 
Compositional Strategy 
Our approach can be used both for interactive music performance accompanying a 
performer, or to generate sound compositions. Repetitive, and complex patterns of Liminal 
Tones (A / Autumn Swarm) are reminiscent of works by Pe Lang, and Zimoun regardless 
of the choice of material or the architecture that drives the outputs as shown in Figures 4.6 
and 4.7.  
These common and key features are: minimalistic approach and use of multiple 
mechanical elements, following simple principles and resemblance to natural systems (in 
sound and motion). However instead of an analogue, and un-controlled approach that is 
common in Pe Lang and Zimoun works, we used an auto-tuning controller as a feedback 
loop to digitally mediate the movement of wires and the patterns of sounds. 
4.3.4. Discussion and Future Works 
Swarm Aesthetic 
Swarm intelligence (SI) is one of the most beautiful, and unusual phenomena in nature that 
emerges from the interaction between a group of decentralized simple agents and their 
environment. Widely recognized examples of swarms include but are not limited to birds’ 
flocking, bacterial growth, fish schooling, and the societal superorganisms of ant colonies 

152 
(i.e. foraging). Natural swarms are often perceived as a single entity or “super-organism” 
that exhibit cognitive behavior and emergent intelligence (Passino et al., 2011).  
Swarm systems inspired by swarm intelligence and natural ecosystems present 
unique frontiers for art. Many artists used artificial swarm systems in their practice and 
utilized swarming principles such as self-organization, and emergence (Barrass, 2006; 
Beyls, 2007) to create novel aesthetics.  
Self-organization is the spatio-temporal process resulting from: multiple 
interactions, positive or negative feedback, amplification of fluctuations, or randomness 
(Bonabeau et al., 1999). Another unique capacity of swarms is emergence, a complex 
collective phenomenon that arises from relatively simple lower-level interactions.  
In addition, the aesthetic richness of swarms and their compositional properties are 
often result from two core qualities: (1) swarm agents are autonomous and therefore useful 
to create generative art systems (e.g. Shiffman, 2004; Blackwell & Jefferies, 2005; Bisig 
& Unemi, 2005 & 2009). (2) artificial swarms or complex symbolic systems act like organic 
or living entities which makes them particularly attractive in ALife Art (e.g. Correll et al., 
2013; Greenfield & Machado, 2015).  
A Complex Multibody 
As artificial intelligence and robotics advance, their influence in the cultural imagination 
and art become inevitable which poses ontological questions: Can machines be creative? 
What is creativity? What new aesthetics can or will emerge?  
To ponder these questions and challenge the music traditions of using fixed 
instruments, we present Liminal Tones (A / Autumn Swarm), a series of sound 
compositions with a multibody architecture consists of identical mechanical elements (as 
shown in Figures 4.6 and 4.7) that sync and swarm together inspired by colony behaviors   
of social insects (i.e. ants foraging).  
Moreover, our goal is to explore a performative ontology and the potential 
aesthetics of swarm agents in sound art. In order to achieve this objective, we followed two 
criteria: (1) Exploring chaotic and emergent behaviors and (2) Embracing imperfections 

153 
and error.  Liminal Tones (A / Autumn Swarm), then is an attempt and critical reflection 
about a still-emergent field of work.  
Synchronous Speed Control and Spectrogram Analysis 
For quantitative assessment and influence of the PSO-PID controller, we evaluated the PID 
performance on the overall group behavior of multiple DC motors (speed and fluctuation). 
We first picked the PID parameters using random values for …M, …N and …√ followed by the 
step response calculation which resulted in unstable control.  
Then we tuned the initial values using PSO algorithm to reduce the peak overshoot 
and synchronize the DC motors as shown in Table 4.1. Our primary results illustrated that 
in order to eliminate any high fluctuations and synchronize DC motors the PID values 
should be in ranges of …M ∈(20,75), …N  ∈(18, 50) and …√∈(1, 40). 
For qualitative assessment and role of materiality in Liminal Tones (A / Autumn 
Swarm), we analyzed the sound waveform along with the corresponding spectrogram in 
Figure 4.9, which presents the spectrum of acoustic sound objects, and their pitch and 
timbral qualities on different surfaces (Wood, Ceramic and Granite). Here, vertical lines 
reveal the rhythmic structures on the horizontal axum, and horizontal lines the harmonic 
structures across frequencies.  
For some sound categories, the audio samples are very noisy meaning and all the 
frequencies are pretty much present, while other have fewer frequencies and show step 
intervals and rhythmic cycles which resulted from vibrating patterns, turn and twist of 
motors or errors (on-off). The speeding patterns can be identified too, in which the sound 
amplitudes also vary with distance and are resulted from fluctuations of batteries. 
Moreover, each material shows different timbral signatures for example the wood resonates 
in all frequencies while ceramic absorb sounds in a dry fashion and absorbing low and mid 
frequencies. 
In general, Liminal Tones (A / Autumn Swarm) generated rhythmic patterns with 
high jumps between different frequencies and exhibited similarity to the combination 
constant and rhythmic patterns of heavy hails and the noisy profile and calming pattern of 
sleet. 

154 
Combining different material qualities helps broadening the resulting timbre and 
frequency domains and enrich the audio expressivity. Considering the swarm’ inherent 
autonomy, it will be feasible to simultaneously engage multiple groups in the format of an 
ensemble with relatively wide timbral and frequency ranges such as a mechatronic noise-
ensemble.  
Therefore, with respect to future works, our plan is to investigate multi swarms and 
large number of DC motors as sound objects (100 or more) and further explore the 
collective behavior and the swarm aesthetic with relatively wide timbral and frequency 
range and mechanical tones. 
Table 4.1 
PID values for multiple identical DC motors and synchronous speed 
control 
Parameters 
 
Motor 
1 &2 
 
Motor 
3&4 
 
Motor 
5&6 
 
Motor 
7&8 
 
…M 
23 
35 
51 
67 
…N 
46 
18 
20 
23 
…√ 
2 
14 
27 
39 
 
 

155 
 
Figure 4.5 
The Simulink block diagram of our PID Controller and 8 DC motors.  
 
 
 
 
 
 
 
Figure 4.6 
The block diagram of our PID Controller with PSO algorithm. 
Adapted from Hashim & Mustafa (2020). 
 
 
 
Initial Parameters 
PSO 
PID 
…M 
…√ 
…N 
DC 
Motors 
âT 
o 
TUÃ∏ 
VWjsqÃXX
YT 
Σ

156 
 
 
Figure 4.7 
a close-up shot of Liminal Tones, control system using an Arduino 
board. 
 
Figure 4.8 
Liminal Tones in action, 8 DC Motors swarming together. 
 
 

157 
 
 
Figure 4.9 
Spectrograms of 12 sound samples (each ranging from 15-30 seconds). 
Note the constant noisy profile of Wood and the mid-level frequencies and orders of 
Ceramic or Resonance of Granite. Some samples have different characteristics such 
as rhythmic patterns and high-low pass while others are noisy. 
 
 

158 
Bibliography 
Allaoua, B., Gasbaoui, B., & Mebarki, B. (2009). Setting up PID DC motor speed control 
alteration parameters using particle swarm optimization strategy. Leonardo 
Electronic Journal of Practices and Technologies, 14, 19-32. 
Barrass, T. (2006) Soma (self-organizing ant maps), EvoMUSART 2006 Process Revealed 
Art Exhibition DVD. 
Beyls, P. (2007). Interaction and Self-organisation in a Society of Musical Agents. In 
Proceedings of ECAL 2007 Workshop on Music and Artificial Life (MusicAL 2007). 
Bisig, D., & Unemi, T. (2009). Swarms on stage-swarm simulations for dance 
performance. In Proceedings of the Generative Art Conference. Milano, Italy. 
Blackwell, T., & Jefferies, J. (2005). A sound you can touch. Proceedings of Generative 
Arts Practice, 125-133. 
Bonabeau, E., Marco, D. D. R. D. F., Dorigo, M., Théraulaz, G., & Theraulaz, G. (1999). 
Swarm intelligence: from natural to artificial systems (No. 1). Oxford university 
press. 
Brownlee, J. (2012). Clever Algorithms, Nature-Inspired Programming Recipes, Open 
Source Book. 
Correll, N., Farrow, N., Sugawara, K., & Theodore, M. (2013). The Swarm-Wall: Toward 
Life’s Uncanny Valley. In IEEE, International Conference on Robotics and 
Automation. 
Eberhart, R., & Kennedy, J. (1995). A new optimizer using particle swarm theory. In 
MHS’95. In Proceedings of the Sixth International Symposium on Micro Machine 
and Human Science (pp. 39-43). Nagoya, Japan: IEEE.  
Greenfield, G., & Machado, P. (2015). Ant-and ant-colony-inspired ALife visual art. 
Artificial life, 21(3), 293-306.  
Hashim, F., & Mustafa, N. (2020). Design of a Predictive PID Controller Using Particle 
Swarm 
Optimization. 
International 
Journal 
of 
Electronics 
and 
Telecommunications, 66(4), 737-743. 
Monahan, G. (2011). Seeing Sound: Sound Art, Performance and Music, 1978-2011. 
Robert McLaughlin Gallery. 
Muecke, M., & Zach, M. (2007). Essays on the Intersection of Music and Architecture. 
Resonance (Ames, Iowa). 

159 
Murphy, J., Kapur, A., & Carnegie, D. (2012). Musical robotics in a loudspeaker world: 
Developments in alternative approaches to localization and spatialization. 
Leonardo Music Journal, 41-48. 
Trianni, V., Tuci, E., Passino, K. M., & Marshall, J. A. (2011). Swarm cognition: an 
interdisciplinary 
approach 
to 
the 
study 
of 
self-organising 
biological 
collectives. Swarm Intelligence, 5(1), 3-18. 
Pigott, J. (2011). Vibration, Volts and Sonic Art: A Practice and Theory of 
Electromechanical Sound. In NIME (pp. 84-87). 
Raes, G. W. (1992). A personal story of music and technologies. Leonardo Music Journal, 
2(1), 29-35. 
Schlatter, N. E., Waller, R., & Matheson, S. (2013). Flow, Just Flow: Variations on a 
Theme. 
Shiffman, D. (2004) Swarm. ACM SIGGRAPH 2004 Emerging Technologies Proceedings, 
ACM Press, New York, p. 26.  
Stoddart, M. M. (2015). Swiss/Mecha-Swiss: An Investigation into the Kinetic, Sonic and 
Entropic Oeuvre of Zimoun (Doctoral dissertation, University of London, 
Courtauld Institute of Art). 
Unemi, T., & Bisig, D. (2005). Flocking Orchestra-to play a type of generative music by 
interaction between human and flocking agents. In Proceedings of the eighth 
Generative Art Conference (pp. 19-21). 
Zareei, M. H., Mckinnon, D., Carnegie, D. A., & Kapur, A. (2016). Sound-based 
Brutalism: An emergent aesthetic. Organised Sound, 21(1), 51. 
 

160 
4.4. 
Swarm Aesthetics and Materiality in Sound Composition  
4.4.1. Introduction 
Swarm systems inspired by swarm intelligence and natural ecosystems (e.g. social insects) 
present unique frontiers for art and many artists utilized swarm principles such as in-direct 
communication, self-organization, and emergent behaviors   to create music compositions, 
soundscapes or sonic environments. A pioneer system that utilizes swarm behavior for 
musical creation is SWARMUSIC (Blackwell, & Bentley, 2002), an interactive music 
improvisation with a swarm of musical events in which multiple swarms of particles move 
in a virtual 3D space, utilizing Boids flocking algorithm (Reynolds, 1987).  
Inspired by earlier swarm music systems, Bisig et al. (2011) created a series of 
experimental projects known as Interactive Swarm Orchestra (ISO) and Interactive Swarm 
Space (ISS). The ISO system explores flocking algorithms to control sound synthesis and 
sound spatialization. Similarly, the ISS is a MIDI-based virtual orchestra acting as a tool 
for creating meaningful interaction between artificial swarms and composers, as well as 
artistic expression. They further explored issues of multi-modal feedback and audio-visual 
spatialization for creative engagement with autonomous, self-organized and spatially 
distributed systems.  
Expanding on previous work, Davis and Karamanlis (2007) added a controllable 
leader to typical Boids simulations for musical swarms. The leader agent lets the user 
directly control the behavior of the other agents and the overall movement of the flock. In 
a different approach, Jones (2008) introduced AtomSwarm. This is a framework for sound-
based performance that uses swarm dynamics with genetically-encoded behaviors  , 
artificial pheromones and imitations. The result is a complex sonic ecosystem capable of 
sonic spatialization and self-organizing regulation. Please refer to Blackwell (2007) for a 
comprehensive review about improvised and evolutionarily swarm music. 
More recently, artistic used flocking behavior to create other outputs such as 
musical visualizations. For example, Musical Flocks (Ruslan et al., 2013) is a responsive 
system and produces animations by simulating the behavior of reactive agents to music. 
Similarly, Rodrigues et al. (2015) proposed Fireflies Visualization, a sound visualization 

161 
tool inspired by fireflies. In their system sound beats are represented by light sources, which 
attracts the artificial fireflies (agents) and create a fluid visualization (cumulating 
movements and artefacts) that express the perturbations caused by sound events. 
However, despite the wide interest in swarm music, most of the previous 
experiments utilize artificial swarms and simulations and to our knowledge, there is not 
much exploration to implement physical agents. Using Liminal Tones (B/ Rain Dream) as 
a case study we highlight the importance of physical space, materials and acoustic 
behaviors  , and challenge the traditional perception of music as an immaterial art form.  
First, we present the emerging interest in non-human music and the role of space 
and materiality in sound composition and sonic environments. In Section 2, we expand 
upon existing studies about chaos theory and complex systems and explain the application 
of emergent behaviors   for artistic practice, followed by discussing the relation between 
order and chaos and emergent behaviors   in Section 3. Then, we introduce Liminal Tones 
(B/ Rain Dream) and its architecture in Section 4 and further explain the underlying 
concepts and important behaviors   of swarm aesthetic for musical creation and present our 
initial results in Section 5. Finally, we discuss our further plan to further investigate the 
potential of physical agents and swarm aesthetics in Section 6.  
 Non-human Sounds – Sound as Action 
Using mechanical instruments and computer-controlled sound objects is not a new tradition 
in music composition or sound art. However, there is a new series of works that are 
concerned with non-human sounds. Such work has been particularly focused on exploring 
repetitive sonic processes and events combined with technological mediation. Here, 
concepts such as space, new sound experience, and the ontological properties of non-
human sound are more important than applying traditional interventions. The investigation 
of space as a compositional element, inflected by movement offers idiosyncrasies and 
aesthetic potentials. Over the last two decades, many composers and sound artists used 
space as a prominent aesthetic element in their work.  
The relative implications range from electro-acoustic experimentation, sonic 
environments, sound sculptures or drumming apparatus. However, despite the emerging 
interest the use of space usually has different aesthetic roles in relation to the experience of 

162 
sound. Some artists use space literally and use motion, direction and distance of sound as 
compositional means and sound spatialization. For example, composer and sound artist 
Trimpin employs the visual, spatial, and kinetic properties of sound in his works e.g. 
Klompen (1987), Conloninpurple (1997), Sheng High (2004) (2010). Other artists use 
space to evoke memories and imaginary environment or to stimulate emotions by the 
motion of sound (Di Bona, 2017). For example, Canadian artists Murray Schafer and Barry 
Truax create soundscapes using space as an eco-systemic source of sound to evoke 
imagination and creative interpretation. Furthermore, as Macedo (2015) noted the 
perception of acoustic space depends on the environment effect of sounds “generated by 
reflection, diffraction and resonance.” The shape, materiality and size of the space result in 
different timbres, pitch and rhythm.  
Sound artists Peter Bosch and Simone Simons (2005) explore the spatial 
characteristics of sound in their kinetic sound project Cantan un Huevo (2000). They use 
glass bottles, containers and metal springs as sound objects. The distribution of the sound 
sources in the space is an integral part of their work and results in different acoustic 
experiences in different parts of the space.  
Other artists use similar sound objects distributed evenly across space in their work. 
For example, Resonant Platinum Records (Monahan, 2011) is a sound installation 
consisting of aluminium plates and piano that resonate and transmit sound through space, 
thus engaging in a dialogue with the acoustics and materiality of the space. Dialogues with 
the materials of sound are central to the many sound installation such as Chijikinkutsu 
(Akamatsu, 2013-2019), a sound installation consists of simple elements such as water, 
sewing needles, glass tumblers. When electricity supplied to the coli attached to the exterior 
of the tumblers creates a temporary magnate field and draws the needle to the coil. The 
gentle sound of the glass hit by the needle (resonates from all around in the space) creates 
an organic symphony from mythical, magical and repetitive patterns (reminisce of natural 
swarms).  
Order and Chaos – Sound as Emergence  
Chaos theory and the study of complex systems (nonlinear dynamics), provide a framework 
for thinking about constant tensions and emergence from chaos and order. Deterministic 

163 
and dynamic systems regardless of their subject matters have universal characteristics such 
as repetition, self-organization, emergence, feedback loop and unpredictability. Chaos 
theory focuses on simple systems with unpredictable and emergent behaviors  . Complexity 
theory is mostly concerned about complex systems with numerous interacting parts, which 
often are self-organized and unexpected. Within such systems, emergent patterns arise 
from simple rules, local interactions between the individual elements (or agents) and 
adaptive behaviors  .  
Not surprisingly, many artists used multi-agent systems, and emergence in music 
improvisation, compositions and sound art. Despite the emergent behaviors   of dynamic 
system, the artist to some degree can control the musical outputs subject to the complexity 
of the ruleset and important variables. However, manual control of interconnected systems 
such as music generative systems is almost impossible since every agent’s movement is 
affected by other agents. So, to leverage more control artists have used simple 
computational models such as Cellular Automaton (Dorin, 2001; Candy, 2002; 
McCormack, 2003; Gage et al., 2005), swarming techniques (Blackwell, 2007; Shiffman, 
2004; Jones, 2008), or abstract constraints of John Cage (1946–48) or Steve Reich (1963).  
Throughout the last decades of sound art, there have been a few artists who applied 
emergence and chaotic principles in their work without any digital mediation. Examples 
include many of the works of Joe Jones, and more recently Zimoun and Pe Lang who use 
simple elements such as motors, wires, solenoids, etc. to create sound sculptures and 
installations. The rhythms and flow in these sonic environments result from repetition, 
randomness and imperfections or glitch. In their practice both together or individually they 
study the creation and degeneration of patterns. Also, inspired by generative systems, and 
swarm behaviors  , their works display both simplicity and complexity. Here the 
complexity grows out of complexity grows form simple rules with some randomness and 
emphasizes their oppositional position of order and chaos (Schlatter et al., 2013).  

164 
4.4.2. Methods 
Concept 
Liminal Tones (B/ Rain Dream) is a series of sound textures made by a group (5–10) of 
BBots (as sound objects) that move, twist and turn on the ground to generate sounds (BBot 
is a modified version of vibration-driven Bristlebot (Giomi et al., 2013; Becker et al., 2014) 
with no brush). Inspired by Pe Lang and Zimoun’s sound sculptures, we used DC vibrator 
motors, wires and electrical circuits to create the BBots and control their motion and sound. 
Liminal Tones (B/ Rain Dream) demonstrate collective behaviors   while embracing 
randomness and imperfections (due to battery degradation and DC perturbation).  
The resulting sound textures are both organized and chaotic. Liminal Tones (B/ 
Rain Dream) can be viewed as an experimental tool for emergent behaviors   and 
materiality in sound art (Flø, 2018) rather than an artwork. Using different materials (as 
surface) and tuning the initial conditions (placement, speed, direction), we were able to 
create different sound textures despite the identical shape and properties of BBots. 
Listening to the textures, one can recognize rhythms such as the clicking of a drum or 
natural sounds (e.g., raindrops on the metal roof). Audio samples can be found on our 
website*. 
Model 
BBots (sound objects) exhibit complex movements similar to the stigmergic foraging 
behavior of ants, in two phases. First, sound objects demonstrate Lévy Walk with high 
power and speed. Over time, the sound objects cycle to Brownian Motion as the battery 
degrades (with lower speed). 
Phase 1 – Lévy Walk  
At the start each BBot move quickly with large step-size similar to Lévy Walk motion – a 
modification of the standard random walk in which the step size has a heavy-tailed 
distribution (Viswanathan et al., 1999): 
 
                                                
* URL: https://metacreation.net/ 

165 
 
Y(I) = IàÛ 
(4.4) 
 
where I is the step size with 1 < Ú ≤3. With increasing values of Ú the movement 
becomes less super-diffusive (due to jumps with heavy-tail distribution) and more 
Brownian. Individual objects with super-diffusive movement paths will appear to move 
faster than those with normally diffusive (Brownian) or sub-diffusive movements 
(Viswanathan et al., 1999). Therefore, Lévy walks represent a spectrum of random walks, 
with ballistic motion at one extreme (Ú > 1) and Brownian Motion (Ú ≃1) at the other.  
Formal Asymptotics 
We used 5 BBots as sound objects with DC perturbation ranging between 1.5–3V. BBots 
move with a random heading and a step length selected from a power-law distribution with 
parameter Ú. The periodic vibration of DC motors paired with a friction mechanism lead 
to a propulsion interaction between the sound objects and the environment, alternating 
between high friction in some parts and low friction in others. BBots have a body with a 
rotational spring of stiffness ¥ and are in frictional contact with the surface without any 
legs.  
The force (¡\) resulted from the body mass oscillation and frequency Ω drives the 
internal movement of the sound objects. The modulation of friction of BBots results from 
the oscillations of the normal forces and leads to a stick-slip motion. DeSimone and Tatone 
(2012) modelled the tangential frictional force by: 
 
 
V = −Ú≥'̇ 
(4.5) 
 
where ≥ is the normal reaction force, '̇ is the velocity (denoted with a dot with 
respect to time), and µ is a constant. For simplicity, we assume that rotations of the BBots 
are not allowed and they are always in contact with the ground with two degree of 
freedoms: horizontal movement and deviation . from the rest angle Q = 0. Therefore, the 
motion equation is as follows (Cicconofri, &DeSimone, 2007): 
 
 
,'̈ = −Ú≥(•)'̇ 
(4.6) 

166 
 
 
,(̈ = ≥(•) −,” + ¡\(•) 
(4.7) 
 
 
¥_ = ≥(•) ¸ IJK(Q + `) −Ú ≥(•) ' ̇ ¸ GHI(Q + `) 
(4.8) 
 
where ≥ is the normal reaction force (≥= ∑
≥j
Á
j±<
), and ,” is the body mass. We 
consider the following ansatz for the normal force:  
 
 
≥(•) = ≥∗+ ≥aIJKb• 
(4.9) 
 
 
≥∗= ,” 
(4.10) 
 
 
≥ ̃/≥^ ∗ = $ ≪1 
(4.11) 
 
where $ is the ratio between the amplitude of a harmonic (≥a) and the average 
normal force (≥∗) and usually smaller than 1. To normalize the dynamic variables, we 
consider the following constants: 
 
 
f = IJK(Q) 
(4.12) 
 
 
g = GHI(Q) 
(4.15) 
 
 
 
 
 
b = √(¥/,)  4/¸g 
(4.17) 
 
where ¡ and 4 are the normalized force and frequency. Applying all the definitions 
above we can rewrite equations (4.6) and (4.8) as the equivalent system in respect to 
dynamical variables (/, ê). 
 
 
/ = K˛
Xjq(4˙í)
i
−*K˛ Äê + /̇ ÈrX (4˙í)
j
̇
Ç   GHI (Q + /)/g  
(4.18) 
 
¡\(•) = ≥∗¡(b•) 
(4.16) 

167 
 
 
ê ̇ = − kK˛(ê + (/ ̇  (GHI (Q + /))/g) ̇ ) 
(4.19) 
 
where ˛ = b•, * =
 l∂∗´m ÈrX 4Ω
p
 and k =  
l∂∗
nΩ. 
Phase 2 – Brownian Motion 
After a few minutes, BBots move slowly with smaller step sizes as the batteries degrade. 
In this phase each BBot acts as a particle with a normalized step-size distribution similar 
to Brownian motion and constantly moves in random directions. 
The Brownian motion is a complex random process with noise. There are different 
methods to formulate the Brownian motion in terms of the evolution of a nonstationary 
probability and here we use Langevin and Fokker-Plank equation (Klimontovich, 1994; 
Radpay, 2020) to study the evolution of the velocity distribution and interactions between 
the environment and Brownian agents. The dynamics and speed fluctuation of the 
Brownian particle are defined as: 
 
 
' ̇ = è 
(4.20) 
 
 
è̇ = −˘(', è)è + V(•) + *(•) 
(4.21) 
 
where V(•) represents a random external force, v and è the mass and the velocity 
of the particle, *(•) is a Gaussian noise, Q is the friction constant and ˘ =
4
Á.  
For simplicity, we assume there is no external forces, and therefore V(•) =  0. The 
Brownian particle with the state space (', è) has a distribution probability °(', è, •) as 
follows (Radpay, 2020): 
 
 
É 
É• °(', è, •) = −˜(°(', è, •)(' ̇, è ̇ ) 
(4.22) 
 
 
É
É• °(', è, •) = −É
É' (°' ̇ ) −É
Éè (°è ̇) 
(4.23) 
 
To simplify the equation 4.23, operators Ì and o are defined as: 

168 
 
 
Ì = è É/' −É
Éè (˘(', è)è) −˘(', è)è É
Éè 
(4.24) 
 
 
o = *(•) É
Éè 
(4.25) 
 
Hence: 
 
 
É
É• °(', è, •) = −Ì° −o° 
(4.26) 
 
4.4.3. Results 
In this section we present the initial results of Liminal Tones (B/ Rain Dream) and step-
length distributions for each phase of the model scheme. We analyze samples taken from 
different intervals and compare the sound quality of different motion (Lévy Walk or 
Brownian) and the surface material in Figure 4.10 and Figure 4.11. First, we show 
examples of movement trajectories of BBots of different surfaces (wood, ceramic, granite) 
and the dependence of those trajectories on control parameters p →∱(', () and DC motor 
speeds. When Ú > 1 and BBots have high turning angle and speed (interacting with the 
environment), the motion is ballistic with long, straight movements and many short steps 
as shown in Figure 4.10. In contrast, when Ú ≃1 the motion is Brownian as shown in 
Figure 4.11.  
The movement trajectories (different Ú) depends on the distribution of step lengths. 
With smaller and fixed Ú, the step-length distribution is more stable (Cauchy distribution). 
With random or higher Ú values, the step-length distribution becomes Gaussian. Moreover, 
the motions result from turning angles ∆/s over time (•). When the value of ∆/s is close to 
zero for a long time, BBots move in a straight line. In contrast, when ∆/s fluctuates 
dynamically, BBots twist and turn many times.  
To evaluate the quality of the generated sound textures, we compare them to natural 
ambient sounds with similar audio profiles. Usually, BBots generate rhythmic patterns with 
high jumps between different frequencies. This would be similar to the rhythmic pattern of 
heavy hail and the noisy profile and calming pattern of sleet, as illustrated in Figure 4.12. 

169 
To qualitatively assess the role of materiality in sound, we compare the spectrum 
of acoustic sound objects in relation to different materials, and their pitch and timbral 
aesthetic for 12 sound textures as shown in Figure 4.13. Here, vertical lines represent the 
rhythmic structures and horizontal lines represent the harmonic structures. For some sound 
categories, the audio samples are noisy, meaning most frequencies are present. Other 
categories have fewer frequencies and show step intervals and rhythmic cycles which 
resulted from vibrating patterns, turn and twist of motors, or errors (on-off interruptions).  
The speeding patterns can also be identified where the sound amplitudes vary due to power 
fluctuations of the batteries. Notably, each material shows different music signatures. For 
example, wood resonates at higher frequencies while ceramics absorb sounds and do not 
resonate as much (low, mid frequencies). 
 
 

170 
 
Figure 4.10 
Model scheme and examples of trajectories for 5 BBots and 10,000 
steps with fixed step distribution and high-speed during Phase 1 which follows a 
ballistic Lévy Walk. Different colors correspond to each BBot and its initial 
conditions (placement, speed, direction). When the value of ∆rs is close to zero for a 
long time, the BBots move in a straight line with short steps in between. 
 
Figure 4.11 
Model scheme and examples of trajectories for 5 BBots and 10,000 
steps. There is random step distribution and low-speed movement during Phase 2, 
similar to Brownian Motion. The internal dynamics 8 and ! produce agent 
movements in 2D space. Movement is produced by turning angles ∆rs over time (s). 
The trajectory of each BBot in a 2D space is represented by different colors 
corresponding to each BBot and its initial conditions (placement, speed, direction).  

171 
 
Figure 4.12 
Comparison of audio samples (left) with natural sounds (hail, rain, 
sleet). Examples were selected to be roughly similar in sound textures. The top row 
shows the waveforms. Note that our sample is more extremely periodic with high 
jumps compared to the other three. The bottom row shows the spectrograms. Here, 
the vertical lines represent step intervals. Note the constant tones around mid-levels 
in rain and the noisy profile of sleet sound. 

172 
Figure 4.13 
Spectrograms of 12 sound samples (each ranging from 15-30 seconds). 
Note the constant noisy profile of wood and the mid-level frequencies and orders of 
ceramics, and resonance of granite. Some samples have different characteristics 
such as rhythmic patterns and high-low passes. Others are noisy with a wide range 
of pitch and timbral qualities, which creates unique sound textures. 

173 
 
4.4.4. Discussion and Future Works  
Swarm intelligence is one of the most beautiful and unusual phenomena in nature. It is the 
product of the interaction between a group of decentralized agents and their environment. 
Widely recognized examples of swarms include but are not limited to bird flocking, 
bacterial growth, fish schooling, and the societal superorganisms of ant colonies (i.e., 
foraging). Due to their aesthetic qualities, swarm systems inspired by swarm intelligence 
and natural ecosystems present unique frontiers for art domains such as visual art 
(Shiffman, 2004; Jacob et al., 2007) and sound composition (Blackwell & Bentley, 2002; 
Bisig et al., 2011; Bouchard, 2021). 
Swarm aesthetics are mostly concerned with form, the collective patterns of 
artificial swarm agents, and intuitive visual and sonic representations in digital forms. 
There is a gap in the research and practice of using swarm techniques to create sounds 
mediated by robotic actions and spatio-temporal processes resulting from: multiple 
interactions, amplification of fluctuations, or randomness between physical agents (sound 
objects). We propose Liminal Tones (B/ Rain Dream) as a tool to create sounds from 
actions (of multiple sound objects) and explore swarm aesthetics in sound. 
Order and Chaos – Sound as Emergence 
Chaos theory and the study of complex systems (nonlinear dynamics), provide a framework 
for thinking about constant tensions and emergence from chaos and order. Deterministic 
and dynamic systems regardless of their subject matter have universal characteristics, 
including repetition, self-organization, emergence, feedback loop and unpredictability. 
Chaos theory focuses on simple systems with unpredictable and emergent behaviors  . 
Complexity theory focuses on complex systems that have numerous interacting parts which 
are often self-organized and unexpected. In such systems, emergent patterns arise from 
simple rules, local interactions between the individual elements (or agents) and adaptive 
behaviors  . 
Not surprisingly, many artists use multi-agent systems and emergence in music 
improvisation, compositions and sound art. Despite the emergent behaviors   of dynamic 

174 
systems, artists can control the musical outputs subject to the complexity of the rule set and 
important variables. Manual control of interconnected systems such as music generative 
systems is almost impossible because each agents’ every movement is affected by other 
agents. For more control, artists use simple computational models such as Cellular 
Automaton (Gage et al., 2005; Bosch & Simons, 2005), swarming techniques (Blackwell 
& Bentley, 2002; Jacob et al., 2007; Jones, 2008; Urbano, 2005) or abstract constraints 
(Bisig et al., 2011; Blackwell, 2007; McCormack, 2003). 
Throughout the past decades of sound art, there have been a few artists who applied 
emergence and chaos principles in their work without any digital mediation. Joe Jones, and 
more recently Zimoun and Pe Lang (Stoddart, 2015), use simple elements such as motors, 
wires and solenoids to create sound sculptures and installations. The rhythm and flow in 
these sonic environments result from repetition, randomness and imperfections or glitches. 
Zimoun and Pe Lang, together and individually, study the creation and degeneration of 
patterns. Inspired by generative systems and swarm behaviors  , their works display both 
simplicity and complexity. Here complexity grows from simple rules with some 
randomness and emphasizes their oppositional position of order and chaos (Satin &. 
Gangal, 2019). 
Inspired by current artistic applications and the rich aesthetic qualities of swarms, 
we explore robotic interventions and the role of materiality in sound art to create novel 
sound textures with different pitch and timbral qualities. 
Future Works 
While experimenting with different setups for Liminal Tones (B/ Rain Dream), we tested 
the use of physical swarming bodies to create sound. To achieve different aesthetic 
qualities, we explored chaotic and random behaviors  , and embraced imperfections and 
error (due to battery degradation and DC perturbation). Liminal Tones (B/ Rain Dream) 
that resulted are a critical reflection of a still-emergent field of work.  
With respect to our future work, our plan is to investigate multi- swarms (with 
different sound qualities) and large numbers of BBots (50 or more) to explore collective 
behaviors  , and swarm aesthetics with wide timbral and frequency range, and mechanical 
tones. 

175 
Bibliography 
Becker, Felix, et al. On the mechanics of bristle-bots-modeling, simulation and 
experiments. ISR/Robotik 2014; 41st international symposium on robotics. VDE, 
2014. 
Bisig, Daniel, Jan C. Schacher, and Martin Neukom. Flowspace-A Hybrid Ecosystem. In 
NIME, pp. 260-263. 2011. 
Blackwell, Tim M., and Peter Bentley. Improvised music with swarms. In Proceedings of 
the 2002 Congress on Evolutionary Computation. CEC’02 (Cat. No. 02TH8600), 
vol. 2, pp. 1462-1467. IEEE, 2002. 
Blackwell, Tim. Swarming and music. In Evolutionary computer music, pp. 194-217. 
Springer, London, 2007.  
Bosch, Peter, and Simone Simons. Our music machines. Organised Sound 10, no. 2 (2005): 
103.  
Bouchard Homepage, https://www.livestructures.com/flock-to-music/, last accessed 
2021/04/20. 
Candy, Linda, Ernest Edmonds, and Fabrizio Poltronieri. Explorations in art and 
technology. London: Springer, 2002.  
Cicconofri, Giancarlo, and Antonio DeSimone. Motility of a model bristle-bot: A 
theoretical analysis. International Journal of Non-Linear Mechanics 76 (2015): 
233-239.  
Davis, Thomas, and Orestis Karamanlis. Gestural control of sonic swarms: Composing 
with grouped sound objects. (2007). 
DeSimone, Antonio, and A. Tatone. Crawling motility through the analysis of model 
locomotors: two case studies. The European Physical Journal E 35, no. 9 (2012): 
1-8.  
Di Bona, Elvira. Towards a rich view of auditory experience. Philosophical Studies 174, 
no. 11 (2017): 2629-2643. 
Dorin, Alan. Aesthetic fitness and artificial evolution for the selection of imagery from the 
mythical infinite library. In European Conference on Artificial Life, pp. 659-668. 
Springer, Berlin, Heidelberg, 2001.  
Flø, Asbjørn Blokkum. Materiality in Sound Art. Organised Sound 23, no. 3 (2018): 225.  

176 
Gage, Dustin, Elizabeth Laub, and Briana McGarry. Cellular automata: is rule 30 random. 
In Proceedings of the Midwest NKS Conference, Indiana University. 2005. 
Giomi, L., N. Hawley-Weld, and L. Mahadevan. Swarming, swirling and stasis in 
sequestered bristle-bots. Proceedings of the Royal Society A: Mathematical, 
Physical and Engineering Sciences 469.2151 (2013): 20120637.  
Grasse, Pierre-P. Reconstruction of the nest and coordination between individuals in terms. 
Bellicositermes Natalensis and Cubitermes sp. the theory of stigmergy: test 
interpretation of termite constructions. Soc. Insect 6 (1959): 41-80.  
Jacob, Christian J., Gerald Hushlak, Jeffrey E. Boyd, Paul Nuytten, Maxwell Sayles, and 
Marcin Pilat. Swarmart: Interactive art from swarm intelligence. Leonardo 40, no. 
3 (2007): 248-254.  
Jones, Daniel. AtomSwarm: a framework for swarm improvisation. In Workshops on 
Applications of Evolutionary Computation, pp. 423-432. Springer, Berlin, 
Heidelberg, 2008.  
Klimontovich, Yu L. Nonlinear brownian motion. Physics-Uspekhi 37, no. 8 (1994). 
McCormack, Jon. Evolving sonic ecosystems. Kybernetes (2003).  
Metacreation Homepage, https://metacreation.net, last accessed 2021/04/20. 
Pe Lang Homepage, https://www.pelang.ch/pelang.html, last accessed 2021/04/20. 
Radpay, Parham. Langevin Equation and Fokker-Planck Equation. (2020). 
Reynolds, Craig W. Flocks, herds and schools: A distributed behavioral model. In 
Proceedings of the 14th annual conference on Computer graphics and interactive 
techniques, pp. 25-34. 1987.  
Salimi, Mahsoo., and Philippe Pasquier. Exploiting Swarm Aesthetics in Sound Art, In 
Proceedings of the Art Machines 2: International Symposium on Machine Learning 
and Art 2021, Art Machines 2 (2021). 
Satin, Seema, and A. D. Gangal. Random walk and broad distributions on fractal curves. 
Chaos, Solitons & Fractals 127 (2019): 17-23.  
Schlatter, N. Elizabeth, Richard Waller, and Sarah Matheson. Flow, Just Flow: Variations 
on a Theme. (2013). 
Shiffman, Daniel. Swarm. In ACM SIGGRAPH 2004 Emerging technologies, p. 26. 2004.  

177 
Stoddart, Madeleine M. Swiss/Mecha-Swiss: An Investigation Into the Kinetic, Sonic and 
Entropic Oeuvre of Zimoun. PhD dissertation., University of London (Courtauld 
Institute of Art), 2015. 
Urbano, Paulo. Playing in the pheromone playground: Experiences in swarm painting. In 
Workshops on Applications of Evolutionary Computation, pp. 527-532. Springer, 
Berlin, Heidelberg, 2005.  
Viswanathan, Gandimohan M., Sergey V. Buldyrev, Shlomo Havlin, M. G. E. Da Luz, E. 
P. Raposo, and H. Eugene Stanley. Optimizing the success of random searches. 
Nature 401, no. 6756 (1999): 911-914.  
Zimoun Homepage, https://www.zimoun.net, last accessed 2021/04/20.

178 
Chapter 5. 
 
 
Discussion and Future Work 
 
 

179 
This chapter presents a brief overview of the research challenges addressed in this thesis 
and the proposed swarm control, and the techniques developed to address them.  
Swarm control research using adaptive coordination and formation strategies is the 
main focus of this thesis. Our approach to answering the RQs is twofold: first, we study 
swarm control and its application in art (PA1, P3 & P4); then, to overcome the challenges 
of behavior-based control, we expand our research in formation control and present a 
learning-based controller for a multi-robot system inspired by recent advances in multi-
robot control using deep learning techniques (P1 & P2).  
Our research-creation process was nonlinear, and it emerged from our investigation 
of new concepts and methods. The iterative nature of this approach made us revisit the RQs 
and expand on our prior knowledge. In addition, many of the insights that shaped our thesis 
came through sharing our findings and participating in peer-reviewed conferences and 
journals. 
The work presented in this thesis is built upon the recent advances in swarm 
robotics, control, and machine learning. In this final chapter, I revisit our thesis foci through 
RQs and a summary of our contributions, discuss the future work and limitation of the 
proposed techniques in Section 5.2, followed by the final word and conclusion in Section 
5.3. 
5.1. 
Summary and Revisiting Thesis Foci 
5.1.1. TF1: Learning-based Formation Control of Swarm Robots 
This thesis explored two fundamental aspects of the swarm control problem: coordination 
and flexibility. 
In the robotic community, decentralized control and distributed systems with 
coordinated, intelligent mobile robots are gaining more interest. Indeed, a decentralized 
and distributed solution is the only viable approach for many real-world application 
domains (i.e., search and rescue, transportation) that are inherently distributed in time and 
space. As a result, there has been a spike in using autonomous mobile robots with 

180 
collaborative behavior that supports and complements one another. Coordinated robots can 
be more flexible, reliable, robust, and cost-effective than a single robot.  
However, dynamic formation (determining the proper coordination and task-
allocation schemes) in response to conditions in the environment and within the team is an 
open challenge. Solving dynamic formation is very difficult and will lead to optimal 
cooperative behavior, reducing interference or stagnation in multi-robot systems. 
Recent advances in multi-robot control suggest using learning-based techniques for 
efficient control and achieving optimal coordinated behavior in a multi-robot system. One 
of these proposed learning techniques is RL algorithms which have received increased 
attention for robot learning. RL agents can learn optimal formation, away from individual 
greediness towards global efficiency, and automatically increase cooperative and adaptive 
character. To further study the recent developments and the application of RL in multi-
robot control, we review the state-of-the-art and taxonomy of more than 50 recent studies 
in Chapter 2, which can be used as inspiration for future work. 
Additionally, inspired by the promising results of RL for formation control and to 
address the challenges associated with the conventional control (i.e., behavior-based), we 
employ a model-free and off-policy RL algorithm called DDPG for flocking formation 
UAV swarm.  We evaluate DDPG performance in dense environments and present or 
results in Chapter 3 and demonstrate that DDPG has high performance for complex task 
processing in high-dimension and continuous state and action spaces (i.e., UAV flocking 
consensus) compared to other state-of-the-art algorithms. Our results can be used for future 
research in swarm formation control and serve as a demonstration of RL applications for 
coordination control of UAV swarm. 
5.1.2. TF2: Human-Swarm Interaction (HSI) and Aesthetic Exploration of 
Swarm Formation  
This thesis also investigates the aesthetic application of formation control; in particular, we 
explore the potential of robotic swarms in (noise) music. Therefore, the second part of the 
thesis, Chapters 4, focused on using formation control and HSI as an interaction strategy 
for artistic creation, contributing to swarm music and inspiring future work. Our 

181 
experimental performance demonstrates good results and presents a setting for co-creation 
and the complex agency between humans and robots. 
We present numerous examples to explain topics and provide simulations and 
experiments to validate theoretical results throughout this thesis. 
5.2. 
Limitations and Future Work 
We complete the thesis by reviewing the limitations and challenges we faced during our 
work. 
5.2.1. Flocking formation of UAV Swarm 
As mentioned in Chapter 3, we trained the flocking formation control in simulation using 
MATLAB and Simulink and did not test on real robots. To deploy this controller to 
physical swarms, we anticipate the need for transfer learning from simulation to hardware 
(Quadrotor UAV). The systems are likely to see a change in the distribution of the 
observations due to stochasticity in the control rates and communication delays of UAVs, 
requiring re-training or fine-tuning. 
However, we expect while low-level collision avoidance controllers may need to 
be re-trained, the high-level principles for tasks like flocking formation and navigation 
could be transferred zero-shot to the real world. Moreover, depending on the UAV type 
(e.g., Tricopter, Quadcopter, Hexacopter, etc.), the model architecture and the 
implementation of control dynamics may need to be optimized for real-time inference.  
Another limitation is that we trained the flocking algorithm with teams of 
homogeneous agents with equal capabilities for control, sensing, and communication. 
Thus, applying our method to heterogeneous groups would need to encode various robot 
capabilities as part of the Q-table, and therefore re-training. 
Future Work: An extension could test our model for an adversarial task such as a 
malicious agent or environmental disturbances. An adversarial task requires team 
resilience, with robots reacting to the loss of team members or interference. Existing 
methods often require rounds of communication among all agents, which may be 

182 
impractical in large teams. However, most scenarios are not purely cooperative; for 
example, autonomous driving is a task in which agents have competing objectives but must 
cooperate for safety. Similarly, the autonomous flight should be robust to any changes in 
the environment and human behavior (Schlotfeldt et al., 2018; Casas et al., 2019). 
Learning-based control has previously been effectively applied to encode relationships 
between agents and predict any changes, which is a critical problem for autonomous 
driving in urban environments (Fujiyoshi et al., 2019; Cao et al., 2020; Chen et al., 2020). 
So, we expect that learning-based control would equally benefit from dealing with 
adversarial tasks in autonomous flight. 
5.2.2. Robotic Swarm Music 
Chapter 4 presents Liminal Tones experiments using flocking and foraging algorithms in 
simulation (MATLAB) to control the robots (BristleBots) and create sound textures. 
However, we assumed that the teams of robots are homogeneous, with the same capabilities 
for control, sensing, and communication. Moreover, our results demonstrated that the 
robots behave somewhat randomly and are challenging to control due to battery 
degradation and DC perturbation. 
Future Work: We predict for real-time improvisation and control, more advanced 
robots with higher sensing and communication capabilities are needed to overcome 
limitations on the robots’ performance, motion range, and audio quality due to battery 
degradation. Another promising approach is using swarm robots for imitation (artistic 
style/pattern) to add more diversity or balance the human-swarm performance.  
5.3. 
Conclusion 
Swarm robotics is a branch of multi-robot systems in which many mobile robots are 
controlled in coordination. Swarm Robots are desirable for applications such as 
transportation (e.g., task allocation), search and rescue (e.g., navigation and victim 
detection), and precision agriculture (e.g., monitoring and mapping, crops inspection) due 
to their inherent redundancy, increased spatial coverage, and lower cost. One of the most 
difficult challenges and an ongoing research topic for swarm robot coordination is 

183 
formation control due to various factors such as formation keeping, communication, and 
coordination between robots. Furthermore, when the number of robots increases (hundreds 
or more), the computation of the system will be staggering (Xu et al., 2014).  
In this thesis, we only tackled one of many challenges of swarm control (dynamic 
control) using a learning-based approach and consensus among agents through leader-
follower interaction. Yet, we highlight the role of different methodologies in such complex 
problems positioned in the intersection of control theory, machine learning, and swarm 
robotics. As the domain evolves, we look forward to exploring the emerging challenges in 
more depth. 
 
 

184 
Bibliography 
Cao, Z., Bıyık, E., Wang, W. Z., Raventos, A., Gaidon, A., Rosman, G., & Sadigh, D. 
(2020). Reinforcement learning based control of imitative policies for near-accident 
driving. arXiv preprint arXiv:2007.00178. 
Casas, S., Gulino, C., Liao, R., & Urtasun, R. (2019). Spatially-aware graph neural 
networks for relational behavior forecasting from sensor data. arXiv preprint 
arXiv:1910.08233. 
Chen, S., Wang, M., Song, W., Yang, Y., Li, Y., & Fu, M. (2020). Stabilization approaches 
for reinforcement learning-based end-to-end autonomous driving. IEEE 
Transactions on Vehicular Technology, 69(5), 4740-4750. 
Fujiyoshi, H., Hirakawa, T., & Yamashita, T. (2019). Deep learning-based image 
recognition for autonomous driving. IATSS research, 43(4), 244-252. 
Hashim, F., & Mustafa, N. (2020). Design of a Predictive PID Controller Using Particle 
Swarm 
Optimization. 
International 
Journal 
of 
Electronics 
and 
Telecommunications, 66(4), 737-743. 
Schlotfeldt, B., Tzoumas, V., Thakur, D., & Pappas, G. J. (2018, October). Resilient active 
information gathering with mobile robots. In 2018 IEEE/RSJ International 
Conference on Intelligent Robots and Systems (IROS) (pp. 4309-4316). IEEE. 
Xu, D., Zhang, X., Zhu, Z., Chen, C., & Yang, P. (2014). Behavior-based formation control 
of swarm robots. Mathematical Problems in Engineering, 2014. 
 
 

185 
Appendix A. 
 
SIAT Guidelines for Writing a Cumulative Thesis 
 
 

186 
 

187 
Appendix B. 
 
Quadcopter Equations of Motion 
 
 

188 
Planar (2D) Quadrotor Model 
We first consider a planer (2D) quadrotor with following parameters: mass v, moment of 
inertia `, and the distance from the center to the base of the propeller 7.  
We assume that the robot cannot deviate from this plan (no motion in the ' 
direction). We also suppose that there are not any pitch or yaw motions. Therefore, the 
quotations of motion are defined as: 
 
 
(̈ = −é<
v I. 
)̈ = −x + é<
v G. 
.̈ = é:
`aa
 
(A.1) 
 
 
D
(̈
)̈
.̈
M = D
0
−x
0
M +
⎣
⎢
⎢
⎡−1/vIJK.
0
1/vGHI.
0
0
1
`aa⎦
⎥
⎥
⎤
≈é<
é:∆ 
 
(A.2) 
 
where (̈, and )̈ are the linear accelerations and .̈ is the angular acceleration. 
 
 
é< = V< + V: 
é: = (V< −V:)’ 
(A.3) 
 
where é< is sum of the thrusts and é: is sum of the moments. 
Furthermore, the configuration and state vectors can be defined as:  
 

189 
 
6 =
⎣
⎢
⎢
⎢
⎡
'
(
)
.
/
0⎦
⎥
⎥
⎥
⎤
, I = ≈6
6̇∆ 
(A.4) 
 
where 6 is the 6D configuration vector, and I is a 12D vector that describes the state of the 
system. 
 
 
I = ≈I<
I:∆= [(
)
.
(̇
)̇
.̇] 
İ =
⎣
⎢
⎢
⎢
⎢
⎡(̇
)̇
.̇
0
−x
0 ⎦
⎥
⎥
⎥
⎥
⎤
+
⎣
⎢
⎢
⎢
⎢
⎢
⎡
0
0
0
−1/vIJK.
0
0
0
0
1/vGHI.
0
0
1
`aa⎦
⎥
⎥
⎥
⎥
⎥
⎤
≈é<
é:∆ 
(A.5) 
 
where I is the state vector and İ is the derivative of the state vector respectively. 
Subsequently, the linearized dynamics is defined as: 
 
 
(̈ = −x. 
)̈ = −x + é<
v 
.̈ = é:
`aa
 
(A.6) 
 
Nested and Hover Control  
We now present the nested controller to track trajectories in that are close to the nominal 
hover state where the roll and pitch angles are small. 

190 
 
 
é< = vmx + )̈tÃX + ¥t,c()̇tÃX −)̇) + ¥t,c()tÃX
−))n 
é: = `aam.̈È + ¥t,ëm.̇È −.̇n + ¥t,ë(.È −.)n 
(A.7) 
 
 
.È = −(̈È
x  
.È = −1
x 2(̈tÃX + ¥t,b((̇tÃX −(̇)
+ ¥t,b((tÃX −()3 
(A.8) 
 
If we combine the net force and the net moment, with the Newton-Euler equations 
we get these two sets of equations: 
 
 
v7̈ = D
0
0
−vx
M + A D
0
0
V< + V: + V; + V=
M 
(A.9) 
 
 
` D
5̇
6̇
7̇
M = D
¸(V: −V=)
¸(V; −V<)
,< −,: + ,; −,=
M −R
5
6
7
S × ` R
5
6
7
S 
(A.10) 
 
where  
 
 
é< = V< + V: + V; + V= 
é: = D
¸(V: −V=)
¸(V; −V<)
,< −,: + ,; −,=
M 
(A.11) 
 
Then we linearize the dynamics at the hover configuration: 
 

191 
 
é< ~ vx, / ~ 0, . ~ 0, 0 ~ 0u 
é: ~ 0, 5 ~ 0, 6 ~ 0, 7 ~ 0  
(A.12) 
 
 
é: = T
¥t,ë(5È −5) + ¥t,ë(.È −.)
¥t,í(6È −6) + ¥t,í(/È −/)  
¥t,ì(7È −7) + ¥t,ì(0È −0) 
U 
(A.13) 
 
 
7̈< = '̈ = x(/ GHI0 + . IJK0) 
7̈: = (̈ = x(/ IJK0 −. GHI0) 
(A.14) 
 
 
.È = 1
x m7̈<,ÈIJK0tÃX −7̈:,ÈGHI0tÃXn 
/È = 1
x m7̈<,ÈGHI0tÃX + 7̈:,ÈIJK0tÃXn 
0È = 0tÃX 
(A.15) 
 
Thrust and Torques 
The thrust force ¡j generated by each rotor in the body frame can be approximated by the 
rotor’s angular velocity bj (Bangura et al, 2016): 
 
 
¡j = ¥bj
:,
∀J ∈{1, 2, 3, 4} 
(A.16) 
 
where ¥ is the thrust constant that can be assumed to be the same for all the rotors. 
Thus, the total thrust vector in the body frame can be calculated as: 
 
 
|ℬ= D0  0  µ ¥bj
:
=
j±<
M 
(A.17) 
 

192 
Similarly, the torque around each motor axis is assumed:  
 
 
~ℬ
a = m~ë  ~í  ~ìn
=
⎝
⎛
¥’(−b<
: −b:
: + b;
: + b=
:)/√2
¥’(−b<
: + b:
: + b;
: −b=
:)/√2
∑
~k∑
=
j±<
⎠
⎞ 
(A.18) 
 
where ~ë, ~í, and ~ì indicates the torques that cause the quadrotor to rotate around 
the ', , and ) axes, respectively. ’ represent the distance from the rotors to the quadrotor 
(Luukkonen, 2011).  
 
 
[|ℬ  ~ℬ
a] = ,b: 
(A.19) 
 
where b: = ‰b<
:  b:
:  b;
:  b=
:Â
w, and , is a matrix. 

193 
Appendix C. 
 
Swarm Systems in Art and Architecture- State of the 
Art (a section from chapter 3) 
In Proceedings of the Computational Synthesis and Creative Systems, Springer, 2021. 
 
 

194 
 
 

195 
 

196 
 

197 
 

198 
 

199 
 

200 
 

201 
 

202 
 

203 
 

204 
 

205 
 

206 
 

207 
 

208 
 

209 
Appendix D. 
 
Liminal Scape, an interactive visual installation with 
expressive AI 
In Proceedings of the 26th International Symposium on Electronic Art (ISEA), 2020. 
 
 

210 
 
 

211 
 
 

212 
 
 

213 
 
 

214 
 
 

215 
  

