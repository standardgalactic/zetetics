MIT-CTP/5496
Aspects of scaling and scalability for ﬂow-based sampling of
lattice QCD
Ryan Abbott1,2, Michael S. Albergo3, Aleksandar Botev6, Denis Boyda4,1,2,
Kyle Cranmer5,3, Daniel C. Hackett1,2, Alexander G. D. G. Matthews6,
S´ebastien Racani`ere6, Ali Razavi6, Danilo J. Rezende6, Fernando Romero-L´opez1,2,
Phiala E. Shanahan1,2 and Julian M. Urban1,2
1Center for Theoretical Physics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA.
2The NSF AI Institute for Artiﬁcial Intelligence and Fundamental Interactions.
3Center for Cosmology and Particle Physics, New York University, New York, NY 10003 USA.
4Argonne Leadership Computing Facility, Argonne National Laboratory, Lemont, IL 60439 USA.
5Physics Department, University of Wisconsin-Madison, Madison, WI 53706, USA.
6DeepMind, London, UK.
Abstract
Recent applications of machine-learned normalizing ﬂows to sampling in lattice ﬁeld theory suggest
that such methods may be able to mitigate critical slowing down and topological freezing. However,
these demonstrations have been at the scale of toy models, and it remains to be determined whether
they can be applied to state-of-the-art lattice quantum chromodynamics calculations. Assessing the
viability of sampling algorithms for lattice ﬁeld theory at scale has traditionally been accomplished
using simple cost scaling laws, but as we discuss in this work, their utility is limited for ﬂow-based
approaches. We conclude that ﬂow-based approaches to sampling are better thought of as a broad fam-
ily of algorithms with diﬀerent scaling properties, and that scalability must be assessed experimentally.
1 Introduction
Lattice quantum ﬁeld theory (LQFT) is the only
known systematically improvable approach to cal-
culating physical observables in quantum ﬁeld
theories that exhibit non-perturbative dynamics.
LQFT has been applied to ﬁrst-principles studies
of quantum chromodynamics (QCD) at low energy
scales [1–7], to test proposed models for physics
beyond the Standard Model [8–11], and to investi-
gate various condensed matter systems [12, 13]. In
this framework, discretized path integrals are eval-
uated numerically using stochastic Monte Carlo
estimators,
⟨O⟩p ≡
Z
dφ p(φ)O(φ) ≈1
N
N
X
i=1
O(φi),
(1)
where φi are samples of the lattice ﬁeld degrees of
freedom drawn from the probability distribution
deﬁned by the Euclidean lattice action S,
p(φ) = 1
Z e−S(φ).
(2)
The partition function Z is typically unknown,
but this is not an obstacle to sampling with
Markov chain Monte Carlo (MCMC) algorithms.
At present, Hybrid/Hamiltonian Monte Carlo
(HMC) [14–16] is the state-of-the-art MCMC algo-
rithm for generating QCD ﬁeld conﬁgurations,
but its computational cost grows rapidly as the
continuum limit is approached [17–19].
Recent work has explored whether improved
conﬁguration
generation
algorithms
can
be
achieved
using
machine
learning
(ML)
[20–
61]. For example, promising proof-of-principle
results using normalizing ﬂows [62–64] have been
obtained in a range of theories with diﬀerent
properties including gauge symmetries [27–35],
1
arXiv:2211.07541v1  [hep-lat]  14 Nov 2022

2
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
fermionic degrees of freedom [32–36], and the exis-
tence of distinct topological sectors or multiple
modes of the probability distribution [23–27, 29–
35]. This includes a ﬁrst demonstration for lattice
QCD [35].
These results raise the question of whether
a ﬂow-based approach can be applied to lattice
QCD calculations at state-of-the-art scale. The
practical question is whether ﬂow models can
provide more cost-eﬀective sampling than HMC
for QCD at parameters and volumes of interest.
Discussions of this question often conﬂate two
separate concerns,
• Scalability: whether an approach can be prac-
tically applied to some target theory, for which
the ultimate question is the cost to generate N
samples at a given set of parameters;
• Cost scaling: how the cost of an approach
changes as certain parameters are varied—in
this case, the parameters of the target lattice
QCD theory, including the parameters of the
action and the lattice geometry.
When we ask whether an approach to gauge ﬁeld
sampling for QCD is “scalable,” we are asking not
about its precise scaling properties, but whether
it will work for QCD on state-of-the-art volumes
at state-of-the-art parameters, and whether it can
be used to push the state of the art further. For
the sampling of gauge ﬁeld conﬁgurations using
HMC, cost scaling relations often directly and
usefully predict scalability. However, the range of
applicability, and hence utility, of any practical
cost-scaling relations for ﬂow-based algorithms is
much more limited. The rest of this manuscript is
devoted to elucidating this statement.
To that end, Sec. 2 ﬁrst reviews HMC and
ﬂow-based sampling methods for lattice ﬁeld the-
ory, establishing deﬁnitions and notation for the
rest of the discussion. Sec. 3 then compares ﬂow-
based approaches with HMC, emphasizing coun-
terintuitive diﬀerences to the more familiar HMC
paradigm. Sec. 4 presents numerical illustrations
of diﬀerent aspects of the scaling of ﬂow-based
approaches, demonstrating the limitations of scal-
ing laws in assessing scalability. Finally, Sec. 5
speculates on potential paradigms for ﬂow-based
sampling at scale and discusses the outlook for
these methods.
2 Preliminaries
2.1 HMC
In the HMC approach, gauge ﬁeld conﬁgura-
tions are generated in a Markov chain, where
approximate molecular dynamics evolution in the
ﬁctitious “Monte Carlo time” direction is used
to propose updates to the chain. Each step of
the HMC algorithm proceeds by drawing ﬁctitious
momentum variables conjugate to each lattice ﬁeld
degree of freedom, integrating the Hamiltonian
equations of motion, then accepting or reject-
ing the resulting new ﬁeld conﬁguration with a
Metropolis test to correct for integrator errors
and guarantee exactness of sampling [14–16]. For
gauge theories with fermion content, such as QCD,
the gauge ﬁelds are evolved in a ﬁxed background
of auxiliary “pseudofermion” ﬁelds, which encode
fermionic eﬀects stochastically [65, 66]. In this
case, each integrator step requires solving large
sparse systems of linear equations using implicit
methods like conjugate gradient. For QCD, these
solves typically dominate the computational cost.
See e.g. Refs. [67–69] for more detailed reviews.
2.2 Normalizing ﬂows and
ﬂow-based sampling
Normalizing ﬂows are a framework for build-
ing exact, numerically tractable, machine-learned
maps between probability distributions. In this
construction, a diﬀeomorphic ﬂow function f is
applied to transform samples z drawn from a
base distribution r to obtain samples φ = f(z)
distributed according to a model distribution q.
The ﬂow f is parametrized by neural networks
and can be optimized to some objective by the
minimization of a “loss function”. Conservation
of probability gives the density of transformed
samples,
q(φ) =

∂f(z)
∂z

−1
r(z) .
(3)
Various
frameworks
have
been
developed
to
construct expressive, trainable ﬂow transforma-
tions for which this expression can be evaluated
tractably [27, 28, 34, 36, 63, 70–76].
Flow transformations are a general tool which
can be applied as components of diﬀerent sam-
pling approaches (and in non-sampling applica-
tions). In particular, in the “direct sampling”

Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
3
approach,
which
is
the
primary
concern
of
this work, ﬂows relate a simple, easy-to-sample
base distribution to a learned approximation
q ≈p of the target lattice ﬁeld distribution,
p = exp[−S]/Z. Evaluating the reweighting fac-
tors w(φ) = p(φ)/q(φ) between the model q and
target p provides suﬃcient information to com-
pute expectations under p. This may be accom-
plished e.g. by computing reweighted expectations
as ⟨O⟩p = ⟨wO⟩q. Alternatively, samples from
p can be generated by using ﬂow model sam-
ples as proposals for the Independence Metropo-
lis algorithm [77–79] with acceptance proba-
bility pacc(φ →φ′) = min[1, w(φ′)/w(φ)]. Power-
fully, this approach allows composition with other
MCMC algorithms with complementary prop-
erties [25]. As discussed further below, these
straightforward applications are only a few exam-
ples of how ﬂow-based sampling algorithms may
be constructed.
Even within a particular sampling framework,
there is no unique way to construct a ﬂow model.
Each concrete realization—a model architecture—
is deﬁned by many diﬀerent choices. From high-
to low-level, these are broadly:
• domain of the variables z, φ (e.g. R, SU(N),
multiple ﬁelds);
• the choice of base distribution r(z) (e.g. Gaus-
sian, Haar uniform, free theory);
• strategy
for
constructing
ﬂow
functions
(e.g.
convex
potential
ﬂows
[73–75],
neu-
ral ODEs [76], or coupling layers [63, 70]
constructed
using
aﬃne
transforma-
tions
[63],
non-compact
projections
[72],
neural
splines
[71],
or
gauge-equivariant
transformations [27, 28, 34]);
• structure of neural networks parametrizing
the ﬂow transformation (e.g. fully connected
vs. convolutional networks, choice of activation
functions);
• and various hyperparameters (e.g. number of
coupling layers, neural network depths and
widths).
While ﬂow-based sampling approaches in prin-
ciple guarantee unbiased results by construction
even for models with q arbitrarily diﬀerent from
p, performant sampling requires training the mod-
els. Just as for model architectures, many choices
deﬁne a particular training scheme, including:
• scheme
for
initializing
model
parameters
(e.g. random distribution, retraining);
• approach to training data (e.g. self-training,
training on existing conﬁgurations);
• choice of loss function to minimize (e.g. KL
divergence [80], Stein discrepancies [81, 82], or
gradient-based divergences like score matching
or Fisher divergence [83, 84]);
• optimization algorithm (e.g. SGD, Adam [85],
second-order optimizers);
• all hyperparameters of training (e.g. optimizer
parameters, batch size);
• and the schedule for training, which may vary
any or all of these choices over time.
Besides direct sampling, other approaches to
sampling lattice ﬁeld distributions have been
explored which use normalizing ﬂows for diﬀerent
statistical modeling tasks. For example, Ref. [33]
explored the use of ﬂows which model a localized
patch of a lattice ﬁeld, conditioned on its environ-
ment. Refs. [29, 31] employed ﬂows to generalize
the proposal distribution in HMC. Ref. [36] used
ﬂows to model conditional distributions for Gibbs
sampling. Stochastic normalizing ﬂows [86, 87]
and related approaches [44, 88, 89] use ﬂows
to relate a sequence of distributions interpolat-
ing between the base and target distributions.
Flows play a diﬀerent role in each case, requiring
diﬀerent architectures and training schemes.
2.3 Cost decomposition
For a generic sampling algorithm, the cost to
generate a dataset equivalent to Nindep indepen-
dent samples from a target distribution can be
decomposed as
Ctotal(Nindep) = Csetup + Csamp(Nindep) ,
(4)
where Csetup is any up-front cost that must be paid
before beginning data generation (e.g. the cost of
training for ﬂows, or of equilibration or decorre-
lating a forked stream for HMC), and Csamp is the
cost of sampling thereafter. Generically, sampling
costs may be further decomposed as
Csamp(Nindep) = ∆Csamp Nraw(Nindep)
= ∆Csamp
Nindep
ESS
,
(5)

4
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
where ∆Csamp is the cost to generate a single
conﬁguration (e.g. the cost of a single HMC tra-
jectory, or of drawing a sample from a ﬂow model)
and Nraw is the total number of conﬁgurations
output by the procedure. The eﬀective sample
size per conﬁguration ESS ≡Nindep/Nraw ∈[0, 1]
quantiﬁes the loss of statistical power due to
e.g. MCMC autocorrelations or the increase in
variance due to reweighting. For HMC, ESS =
1/2τint, where τint is the integrated autocorre-
lation time. For ﬂow-based direct sampling, the
reweighting-inspired metric ESS = 1/ ⟨w2⟩q [90,
91] is commonly employed (see Appendix A for
further discussion). Note however that no single
scalar metric may fully quantify the performance
of any sampling algorithm, as the true ESS is
always observable-dependent: for HMC, observ-
ables are sensitive to diﬀerent autocorrelation
times, whereas for ﬂows, each observable has dif-
ferent correlations with the reweighting factors.
Both Csetup and ∆Csamp carry units of compute
time (e.g. GPU hours), and depend strongly on the
precise details of the algorithm implementation
and hardware.
3 Costs and cost scaling:
HMC versus ﬂow-based
approaches
In this section, we ﬁrst analyze the role and limi-
tations of cost scaling relations in the familiar con-
text of HMC for QCD. Subsequently, we discuss
diﬀerent general aspects of ﬂow-based approaches
and their cost scaling properties, emphasizing dif-
ferences with HMC. Features better demonstrated
with numerical examples are deferred to Sec. 4.
3.1 Expectations for scaling laws
from HMC
The cost to sample QCD ﬁeld conﬁgurations using
HMC is often parametrized as a function of the
physical parameters of the target theory as [68, 92]
C(Nindep) ∝Nindep
L
a
zL
M −zM
π
a−za
(6)
where a is the lattice spacing, L/a is the extent
of the lattice in units of sites, and Mπ is the pion
mass. The exponents zL, zM, and za deﬁne the
cost scaling relation. A key aspect of the diﬃ-
culty in assessing scalability is that their values
depend on many factors, in particular both algo-
rithmic choices and the targeted regime of physical
parameters, i.e.,
1. Scaling
laws
depend
on
how
algorithm
parameters are varied, and
2. Scaling laws have limited regimes of applica-
bility.
Naturally, not only the parameters of an algo-
rithm are relevant, but also the structure of the
algorithm itself, with the important consequence
that
3. Algorithmic developments can improve scal-
ing properties.
Each of these key aspects are elaborated on below.
1. Scaling laws depend on how algorithm
parameters are varied. Deﬁning and quanti-
fying a scaling relation requires choosing some
scheme to vary algorithm parameters as the
parameters of the target theory are varied. Dif-
ferent choices will yield diﬀerent scaling relations.
For example, discussions of HMC scaling typi-
cally quote a value of zL = 5 for the exponent
of L/a in Eq. (6). Part of this, (L/a)4, is due to
simple operation counting on a four-dimensional
lattice. However, the remaining factor of L/a
follows from choosing a particular scheme to
vary the algorithm parameters with the volume,
speciﬁcally increasing the number of integrator
steps per trajectory to keep the acceptance rate
ﬁxed [93]. A diﬀerent choice would yield a dif-
ferent scaling relation: for example, keeping the
step size ﬁxed will result in a rapid decline of the
acceptance rate, inducing long autocorrelation
times and poorer scaling.
2. Scaling laws have limited regimes of
applicability. Although universal behavior may
be expected in some regimes of target physical
parameters, in practice a wide range of values of
the exponent of the lattice spacing za have been
quoted in the literature [68, 94]. For example,
many early studies [95–98] of HMC cost scal-
ing found za near its conjectured lower bound
of 2 [99]. However, approaching the continuum
limit, increasingly large potential barriers prevent
HMC from tunneling between disjoint topolog-
ical sectors. This eﬀect, known as “topological
freezing”, induces very long autocorrelation times

Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
5
and thus large za. In fact, the a-dependence may
instead be consistent with exponential [19, 100].
This large variation demonstrates the importance
of assessing the limitations of any scaling law:
extrapolating using an incorrect value of za for
the parameter regime under consideration will
not be predictive.
3. Algorithmic developments can improve
scaling properties. Historically, new algorithmic
developments have led to qualitative improve-
ments in the capabilities and reach of HMC. For
example, the scaling relation Eq. (6) was infa-
mously used to diagnose the “Berlin wall”: at
the time (ca. 2001), large measured values of
zM implied that calculations at physical pion
masses would be practically impossible with near-
future hardware [92, 96]. However, Hasenbusch
preconditioning [101] was developed soon after-
wards, reducing zM and opening access to the
physical pion mass regime. The development of
multigrid preconditioners has provided additional
improvements [102, 103].
3.2 Costs and cost scaling for
ﬂow-based approaches
History
has
demonstrated
that
algorithmic
advances can redeﬁne the limits of LQFT meth-
ods and enable new physics results. To that end,
ﬂow-based approaches provide an unprecedented
new space of algorithms to explore. However, their
costs and cost scaling properties can be counterin-
tuitive from the HMC perspective. To understand
these methods and their scaling properties, several
key features of ﬂow models must be appreciated,
in particular that:
1. Model weights are algorithm parameters;
2. Flow evaluation costs do not typically vary
with physical parameters, but model quality
does.
In addition, the practical requirement for model
training
introduces
conceptual
complications.
Speciﬁcally:
3. Training and sampling are diﬀerent dynami-
cal processes, but cannot be considered inde-
pendently;
4. Training costs may be signiﬁcant.
Finally, certain low-level computational concerns
must be considered:
5. Operation counting can predict raw compu-
tational costs; and
6. Flow-based approaches may parallelize more
eﬀectively than serial samplers.
Each of these points is discussed below, emphasiz-
ing the conceptual diﬀerences between ﬂow-based
sampling approaches and the HMC paradigm.
1. Model weights are algorithm parame-
ters. A ﬁxed ﬂow model architecture represents
a parametric family of probability distributions.
Within this family, a set of values for the neural
network parameters θ (“model weights”) speciﬁes
a particular model distribution, qθ. The ESS to
sample some target theory p will vary as a func-
tion of every model weight; their number thus
deﬁnes the dimension of the space of algorithms
to be explored. Relaxing the restriction to ﬁxed
architectures presents even further possibilities.
Conceptually, this is a drastically diﬀerent sit-
uation from HMC, where the space of algorithm
parameters is tractable to explore; ﬂow models
may have potentially billions of parameters. Of
course, in practice, hand-tuning is impossible
and model weights must instead be set implicitly
by training e.g. with stochastic gradient descent
methods. The practical analog to the set of HMC’s
algorithm parameters are thus the set of all of
the choices involved in deﬁning a training scheme
and architecture described in Sec. 2.2. Diﬀerent
scaling behavior results from how these choices
are varied with target parameters. Note that
although training schemes are often described
in terms of relatively few hyperparameters, this
apparent reduction in complexity is artiﬁcial. As
discussed further below, the need for training
only adds complexity, rather than providing any
simpliﬁcation.
2. Flow evaluation costs do not typically
vary with physical parameters, but model
quality does. For HMC, the cost to generate
each sample, ∆Csamp, varies strongly with the
parameters of the target theory due to the chang-
ing problem diﬃculty for the linear solvers. In
contrast, applying a ﬂow transformation typically
involves only explicit algebraic operations. In this
case, ∆Csamp is independent of the precise values
of the weights of the model, and thus the target
parameters they implicitly encode. It follows that,
for a ﬁxed architecture, the dominant cost scaling

6
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
with target parameters is due to the variation of
the ESS (note that this may not hold if ∆Csamp
is dominated by linear solves to evaluate p, or
e.g. for hybrid algorithms incorporating HMC
updates). This severely limits the applicability
of empirically measured cost scaling relations: as
explored below, model quality is an unpredictable
consequence of the complicated interplay between
architecture, training scheme, and target theory.
3.
Training
and
sampling
are
diﬀerent
dynamical processes, but cannot be con-
sidered independently.
For HMC, the same
process—evolution by Hamiltonian dynamics—is
used for both setup and sampling. However, the
same is not true for ﬂows. Training is a search
process in the high-dimensional space of model
parameters, typically involving stochastic gradient
descent, with dynamics arising from all the choices
described in Sec. 2.2. Sampling dynamics depend
on the interaction of ﬁne-grained details of model
quality with the role played by the model in the
sampling algorithm (giving rise to e.g. the distri-
bution of rejection run lengths in direct sampling
with Metropolis). Unlike for HMC sampling, there
is no reason to expect any common parametriza-
tion to apply to both cost components.
However, this does not mean that scaling
behaviors of training and sampling costs can be
studied in isolation. For ﬂows, sampling eﬃciency
is a function of expenditure on training, i.e.,
ESS = ESS(Ctrain). Among other consequences,
this implies that the scaling behavior of sam-
pling and training costs are not deﬁned without
precisely specifying the stopping condition for
training. Diﬀerent choices of stopping condition
result in qualitatively diﬀerent scaling relations.
For example, if training always uses a ﬁxed
amount of computation, then Ctrain scales triv-
ially with the target parameters by construction,
and all cost scaling is pushed on to Csamp, as
explored in Sec. 4.2. Instead, one may choose to
train until some target ESS is achieved. In this
case, the ESS scales trivially while Ctrain alone
varies, as explored in Sec. 4.1. As demonstrated in
Secs. 4.1 and 4.2, even the precise choice of ﬁxed
Ctrain or target ESS can strongly aﬀect scaling
properties. Other choices of stopping condition
between these extremes are possible, each induc-
ing diﬀerent scaling properties. This sensitivity
implies that scaling behaviors for ﬂows are highly
non-generic, thus the generalizability of empiri-
cally assessed scaling relations is severely limited.
4. Training costs may be signiﬁcant. Most
existing applications of ﬂows to sampling in LQFT
have employed training methods which apply the
ﬂow (or its inverse) to batches of ﬁeld samples.
For such a training approach, training costs can
be decomposed similarly to the decomposition of
sampling costs in Eq. (5):
Csetup = ∆CtrainNtrain
(7)
where Ntrain is the total number of samples ﬂowed
during training and ∆Ctrain is the combined cost
of ﬂowing a single sample and then backpropa-
gating gradients. Typically, ∆Ctrain is a few times
larger than ∆Csamp. Training from a random
initialization often involves ∼1000s of optimizer
steps using batches of ∼100s to 1000s of conﬁgu-
rations, meaning Ntrain can be much larger than
typical QCD ensemble sizes. This potentially
large scale for training costs is an important fac-
tor in determining what will be computationally
viable for QCD. However, eﬃcient parallelization
(as discussed below) may help mitigate this cost
in practice. Moreover, importantly, as explored in
Sec. 4, these costs are highly non-generic and can
be optimized signiﬁcantly.
5.
Operation
counting
can
predict
raw
computational costs. In some cases, simple
operation counting can yield relations describing
the dependence of ∆Csamp and ∆Ctrain (∆C,
collectively) on model hyperparameter choices
and certain target theory parameters, such as the
number of lattice sites Ω. For example, for ﬂow
models built from composed sub-transformations
(e.g., coupling layers), the cost of applying a ﬂow
is linear in the number of sub-transformations.
Similarly, for ﬂow transformations parametrized
by neural networks which interconnect all lattice
sites, operation counting predicts that ∆C ∝Ω2.
More favorably, transformations parametrized by
convolutions over the lattice geometry may have
∆C ∝Ω. (Note that these relations are often
broken by e.g. cache eﬀects or adaptive algorithm
swapping, and may not apply for implementa-
tions on hardware such as GPUs and TPUs if
their parallelism is not fully utilized.) Operation
counting thus provides a useful but limited tool

Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
7
for assessing scaling properties and scalability: if
the architecture itself is varied with the target,
operation counting can predict scaling of ∆C, but
cannot account for the (equally important) eﬀect
of varying model quality. Real-world resource con-
straints may induce further complications, e.g. if
limited memory necessitates a trade-oﬀbetween
model size and the batch size for training.
6. Flow-based approaches may parallelize
more eﬀectively than serial samplers. Sam-
pling from a ﬂow model is embarrassingly par-
allelizable, and thus ﬂow-based sampling admits
new strategies for parallelism unavailable in the
context of serial MCMC algorithms like HMC.
For example, multiple compute nodes might inde-
pendently and locally generate and store conﬁg-
urations φi, and pass only q(φi) and p(φi) to a
central coordinator which updates the state of
a global, distributed Markov chain with Inde-
pendence Metropolis. Potential new strategies
include not only the running of multiple indepen-
dent samplers (without incurring additional setup
costs), but also new schemes for problem divi-
sion such as pipeline parallelism. Separately, the
high arithmetic density of neural network opera-
tions suggests that ﬂow-based sampling may not
be communications-bound. Taken together, these
properties suggest potential for ﬂow-based meth-
ods to make more eﬃcient use of computational
resources than serial sampling algorithms, given
real-world walltime and hardware constraints.
4 Limitations of scaling
relations for ﬂow-based
approaches
The previous section discussed properties of ﬂow-
based sampling approaches for LQFT that are
apparent from the deﬁnition of the approach.
However, research and experimentation in this
area has identiﬁed other properties of ﬂows rele-
vant to assessing scalability. This section presents
simple numerical demonstrations of these features
and discusses their implications for the utility of
scaling relations. Further details of all numerical
experiments are provided in Appendix A.
0M
100M
200M
300M
Training cost [samples]
0.0
0.5
ESS
Schedule LR
Schedule BS
Fig. 1
Model quality as a function of training cost for
two approaches to training, learning rate (LR) scheduling
and batch size (BS) scheduling, for a ﬂow model target-
ing real scalar ﬁeld theory for m2 = −1 and λ = 1 on
a 10 × 10 lattice. Each training scheme is applied to the
same architecture. Cost is measured in units of the num-
ber of samples generated for training, Ntrain. Dashed black
lines indicate potential target ESSes of 0.2 and 0.6, as ref-
erenced in the main text. The batch size for “Schedule LR”
is 16384 throughout, while for “Schedule BS” it begins at
128 and doubles every 5000 steps until it reaches 16384.
For “Schedule LR”, the learning rate is halved every 5000
steps. The ESS is measured using the training batch size
every step, and smoothed over a rolling window of width
250 steps. Results vary by ∼5% under repeated experi-
ments with diﬀerent pseudorandom seeds. Further details
are provided in Appendix A.
4.1 Training costs
As discussed in Sec. 3.2, the scale of training costs
of ﬂow models can be signiﬁcant, requiring the
ﬂow to be applied to many more conﬁgurations
than typical ensemble sizes. However, as demon-
strated here, these costs are highly non-generic,
and may be optimized signiﬁcantly. Furthermore,
their scaling behavior depends on the precise
details of the training protocol. Speciﬁcally, we
demonstrate that:
1. Training costs may be optimized by orders of
magnitude,
2. Transfer learning can mitigate training costs,
and
3. Training cost scaling depends on training
protocol.
The necessary conclusion is that the scaling of
training costs is entirely speciﬁc to the approach
employed.
1. Training costs may be optimized by
orders of magnitude. Periodically reducing the
optimization step-size (i.e., the learning rate)—a
standard ML technique—can produce models of
better quality at lower computational cost than
training with a ﬁxed learning rate. An alternative

8
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
is to periodically increase the batch size during
training [104]. As demonstrated in Fig. 1, in some
cases this simple change can produce models of
equivalent quality for drastically reduced cost as
compared with those produced using learning
rate scheduling. The precise improvement factor
depends on the criterion used to stop training.
For example, in this demonstration, batch size
scheduling is ∼110 times less expensive than
learning rate scheduling for training to a tar-
get ESS of 0.2, while for a target ESS of 0.6
the improvement is only a factor of ∼12. Note
that the intent of this demonstration is not to
emphasize the utility of this batch size reduction
method, but rather to show the degree to which
training costs may vary with approach. The prac-
tical consequence for assessing scalability is that
training costs, and their scaling, are extremely
sensitive to even minor perturbations in training
protocol.
2. Transfer learning can mitigate training
costs. Flow models can be retrained between
theories with the same lattice geometry but dif-
ferent action parameters. Figure 2 illustrates the
utility of this technique—“parameter transfer”—
in training models for three diﬀerent sets of
Schwinger model target parameters. The large
cost of training a model from a random initial-
ization must always be paid once. However, as
demonstrated, models targeting further parame-
ters may be retrained from this initial model at
signiﬁcantly reduced cost compared with from-
scratch training. Sequentially retraining along a
trajectory through parameter space provides fur-
ther improvement [25], at the cost of serializing
training for diﬀerent parameters.
Certain
ﬂow
architectures,
such
as
those
parametrized by convolutional neural networks,
additionally permit “volume transfer”: a model
trained to generate conﬁgurations with one geom-
etry may be used to generate conﬁgurations with
another [28]. This makes it possible to perform the
bulk of training for smaller volumes than the ﬁnal
target, and enables methods that are intractable
for larger volumes, e.g., training with on-the-ﬂy
HMC generation [25] or using exact fermion deter-
minants [32, 36]. In practice, we often ﬁnd that
retraining on the larger volume is unnecessary. For
models trained on smaller volumes and applied to
larger ones with no retraining, there is a generic
cost scaling relationship with changing volume,
detailed in Sec. 4.2 below.
The consequence of these observations is that,
for the purposes of assessing scalability, the cost
scaling properties of training from a random
initialization are irrelevant if transfer learning is
employed. Instead, the relevant scaling proper-
ties are those of the cost of retraining each new
model, which will depend intimately on the set
of physical parameters that are of interest in a
particular study, and the order in which models
are retrained between them.
3. Training cost scaling depends on training
protocol. Training costs and their scaling prop-
erties are determined by various choices made in
training. As discussed in Sec. 3.2, this includes
the stopping condition for training, without which
cost scaling behavior is not well-deﬁned. For exam-
ple, training for a ﬁxed number of iterations
implies a ﬁxed training cost but varying model
quality. Alternatively, training to a target ESS
results in ﬁxed sampling costs while the train-
ing costs vary with the target theory. The scaling
of those training costs with the parameters of
the target theory depends sensitively on the pre-
cise choice of target ESS, as illustrated in Fig. 3.
As already discussed in Sec. 3.2, this demon-
stration that scaling relations are highly speciﬁc
to the training approach implies that, even if
scaling behaviour can be determined empirically
for some particular approach, it is unlikely to
be generic across diﬀerent training approaches,
let alone across diﬀerent theories or ﬂow model
architectures.
4.2 Sampling costs
In this section, we demonstrate various limita-
tions of scaling laws in describing sampling costs
in ﬂow-based approaches. In particular, as model
quality determines sampling eﬃciency, the scaling
of model quality with the parameters of the tar-
get theory is considered as a proxy for the scaling
of sampling costs. We provide arguments for and
numerical illustrations of several key properties.
First, deﬁning the maximum achievable model
quality is a subtle question, since
1. Achievable model quality is not a function of
architecture alone.

Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
9
0
1
0.263
0
1
0.25
0
3000
6000
9000
12000
15000
Total cost [Epochs]
0
1
κ = 0
ESS
Train from scratch
Retrain from κ = 0
Retrain from κ = 0.25
Fig. 2
Illustration of the reduction of training costs that can be obtained using transfer learning to train models for the
Schwinger model with β = 2.0 and lattice geometry L×T = 8×8 for three diﬀerent κ targets. The ESS is measured on 6144
samples each epoch. Training curves are sequentialized to illustrate the total cost of training models for κ = 0, 0.25, 0.263
in order. The vertical lines indicate the overall cost in each case: blue for training each model from a random initialization,
orange for retraining from an initial κ = 0 model, and green for sequentially retraining from κ = 0 →0.25 →0.263.
Training is stopped when the target ESS is achieved in average over 100 epochs. Horizontal dotted lines mark the target
ESS = 0.9, 0.7, 0.45 for κ = 0, 0.25, 0.263, respectively. Further details are provided in Appendix A.
0.4
0.6
0.8
1.0
1/
0
100
200
Training cost [steps]
Target ESS=0.45
Target ESS=0.65
Fig. 3
Demonstration in real scalar ﬁeld theory, with
m2 = −2 on a 10 × 10 lattice, of the diﬀerences in training
cost scaling behavior in 1/λ due to diﬀerent choices of the
stopping condition for training. Each model is retrained
from λ = 4 to the target λ, halting training on the ﬁrst
step where the target ESS is achieved. The lack of smooth-
ness is an inherent complication of quantifying scaling laws
when using stopping conditions based on model quality,
here due to the stochastic evaluation of the ESS well as the
inherent noise of training. Further details are provided in
Appendix A.
Moreover, the scaling of model quality with
parameters of the theory is complex, since
2. Model quality scaling depends on training
protocol, and
3. Diﬀerent architectures scale diﬀerently.
One scaling, however, can be derived generically:
4. For ﬁxed models, quality scales exponentially
in volume.
Nevertheless, assessing scalability—i.e., utility at
scale—from scaling laws is diﬃcult, since
5. Best scaling does not imply best perfor-
mance, and
6. Architecture
dependence
implies
theory
dependence.
Finally, we discuss the most relevant concern for
scalability at present:
7. Improving
performance
requires
physics-
informed algorithm design.
While the provided numerical demonstrations
exclusively use the direct sampling approach,
the conclusions apply more generally to sam-
pling algorithms including ﬂow models or other
machine-learned components.
1. Achievable model quality is not a func-
tion of architecture alone. It is typical in
gradient-descent-based optimization that after a
period of rapid initial learning, optimization will
enter a regime where model quality plateaus or
improves only very slowly with further training.
It is tempting to interpret the resulting model
as fully saturated in quality, i.e. independent of
training scheme and a function of architecture
and target theory alone. However, this is not the
case. As shown in Fig. 4, changing only the precise
choice of optimization algorithm or initial distri-
bution of model weights can result in signiﬁcantly
diﬀerent ﬁnal ESS. Further, training dynamics
may be sensitive even to the pseudorandom seed
used to generate the initial model parameters
and training data. The appearance of saturation
may also be misleading, as demonstrated by the
“second-wind” training dynamics in one of the
examples in Fig. 4, where an apparent plateau
is broken by a second period of rapid learning

10
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
0.0
0.5
ESS
Adam
SGD
Adadelta
0
50000
100000
150000
Training steps
0.0
0.5
ESS
Kaiming, width=1
Kaiming, width=2
Xavier, gain=1
Xavier, gain=2
Fig. 4
Example training curves demonstrating sensitiv-
ity of training dynamics and ﬁnal model quality to the
choice of optimization algorithm (top panel) and the dis-
tribution used to randomly initialize the model weights
(bottom panel), for real scalar ﬁeld theory on a 10 × 10
lattice with m2 = −2 and λ = 1 (top), and m2 = −4
and λ = 1.5 (bottom). Other than the optimizer or weights
initialization, architectures and training protocols are the
same for each curve. The legend for the top panel refers to
the Adam optimizer of Ref. [85], the Adadelta optimizer of
Ref. [105], and stochastic gradient descent (SGD). In the
legend for the bottom panel, Kaiming refers to the initial-
ization procedure of Ref. [106], the Pytorch 1.10 default,
with “width” an overall rescaling of the distribution. Xavier
refers to the procedure of Ref. [107], with “gain” a param-
eter of the method. The two red lines indicate examples
for two diﬀerent pseudorandom seeds; all other curves vary
only at the ∼5 −10% level for diﬀerent seeds. The learn-
ing rate is decayed by a factor of 2 every 20000 steps. The
ESS is smoothed using a rolling window of width 250 steps.
Further details are provided in Appendix A.
with no associated change in training protocol.
Although a ﬁnite ﬂow has ﬁnite expressivity, the
results of any particular training procedure can
only bound the capabilities of an architecture.
Practically, this implies that there is no notion
of a “fully trained” model, and furthermore that
model quality scaling relations cannot be quan-
tiﬁed for an architecture class independent of
training eﬀects.
2. Model quality scaling depends on train-
ing protocol.
The scaling behavior of model
quality is necessarily determined by the choice of
training scheme, which governs how model weights
vary with the parameters of the target theory.
As for training costs, this includes the stopping
condition for training. Fig. 5 demonstrates how
1
2
3
4
0.80
0.85
0.90
0.95
ESS
100 steps
1000 steps
5000 steps
Fig. 5
Demonstration in the context of real scalar ﬁeld
theory of how model quality as a function of λ depends on
the stopping condition used for training, for m2 = −2 on a
10 × 10 lattice. Models are retrained directly from λ = 1 to
the target λ, halting training after a ﬁxed number of steps.
Further details are provided in Appendix A.
diﬀerent stopping criteria—speciﬁcally, diﬀerent
choices of ﬁxed expenditure on training—induce
diﬀerent functional forms in model quality as
a function of target theory parameters when
retraining between target parameters. In this
ﬁgure, models for all parameters are directly
retrained
from
a
single
model;
sequentially
retraining instead would result in diﬀerent func-
tional forms again. The practical consequences
are that cost scaling laws are not unique even for
a particular architecture and training method.
3. Diﬀerent architectures scale diﬀerently.
Figure 6 provides a simple demonstration of how
the model quality of even very similar archi-
tectures may exhibit diﬀerent scaling behaviors
with the parameters of the target theory. In this
example, larger models (with more free param-
eters) exhibit better overall ESS and better
scaling properties towards criticality—although
the changing trade-oﬀbetween ESS and ∆Csamp
results in crossovers of the sampling performance,
as discussed further below. The smooth depen-
dence of ESS (and consequently performance)
on target parameters implies useful extrapola-
tion can be possible over a small range of target
parameters for a ﬁxed architecture, although we
emphasize that these functional forms are neces-
sarily speciﬁc to how the models are trained.
4. For ﬁxed models, quality scales exponen-
tially in volume. As demonstrated throughout
this work, generic cost scaling laws for ﬂow mod-
els are diﬃcult to obtain due to the complexity

Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
11
0.00
0.25
0.50
0.75
ESS
0.3
0.4
0.5
0.6
0.7
0.8
1/
0
10000
20000
30000
Performance [ESS/s]
    K  nL
3  12
3  24
5   6
5  24
Fig. 6
Scaling behavior of model quality with respect
to theory parameters, for diﬀerent model architectures
applied to real scalar ﬁeld theory with m2 = −4 on a
16 × 16 lattice. Each line denotes the ESS (top panel) and
sampling performance (bottom panel) of a diﬀerent aﬃne
coupling architecture, all trained using the same protocol.
Performance is measured in eﬀective samples per second,
quantiﬁed for each model as its ESS divided by the com-
putational cost of generating a conﬁguration in units of
RTX2080 Ti GPU-seconds. Models are trained as described
in Appendix A. In the legend, K denotes the convolutional
kernel size and nL denotes the number of coupling layers.
Further details are provided in Appendix A.
of the method. However, a particular restriction
allows one to be derived analytically. Speciﬁcally,
a straightforward argument implies that the qual-
ity of a ﬁxed model constructed from a local1
ﬂow transformation degrades exponentially under
volume transfer: if both p and q are deﬁned for
arbitrary volumes V and may be characterized by
correlation lengths ξp and ξq, then when L ≫
ξp, ξq the integral deﬁning the ESS factorizes over
decorrelated subvolumes and scales as
ESS(V ) = ESS(V0)V/V0 .
(8)
Figure 7 demonstrates the onset of this eﬀect in
the large-volume regime for the Schwinger model,
but we observe it to hold for other target theories
as well. This eﬀect was ﬁrst described in Ref. [33].
It is important to emphasize the limited appli-
cability of this scaling relation. Speciﬁcally, it
applies only when the physical extent L/ξp is
large, compared with the extent in lattice sites
L/a. Thus, while this relation may present an
1I.e., for which variables are transformed conditioned only
on information in a local neighborhood, such as for ﬂows
parametrized by convolutional neural networks.
obstacle to ﬂow-based sampling in the thermo-
dynamic limit L/ξp →∞, it does not obstruct
sampling in the continuum limit ξp/a →∞with
L/ξp ﬁxed. Away from the thermodynamic limit,
the breakdown of this relation can have counterin-
tuitive eﬀects: as shown in Fig. 7, we have observed
better-than-exponential scaling in the Schwinger
model when L/ξp is small. The exponential scal-
ing relation furthermore does not apply between
theories in diﬀerent dimensions or with diﬀer-
ent numbers of internal degrees of freedom. Most
importantly, however, it does not relate diﬀerent
approaches or otherwise constrain what overall
model quality (and thus quality of volume scaling
properties) is achievable. Finally, the argument for
this scaling relation holds only for ﬁxed models
transferred without retraining.
This eﬀect does not present any principled
obstacle
to
ﬂow-based
sampling
at
state-
of-the-art
QCD
parameters
where
typically
L/ξp = MπL ∼4 −10. In practice, we are
not interested in the thermodynamic limit, only
control over ﬁnite-volume eﬀects. However, it
does emphasize the importance of developing
high-quality models to reach volumes of interest.
Sampling approaches which require modeling only
subvolumes [33] may control the eﬀect directly.
5. Best scaling does not imply best perfor-
mance. Of course, scaling of model quality is not
the sole factor determining sampling eﬃciency.
In fact, in the demonstration of Fig. 6, an archi-
tecture with one of the least favorable scaling
behaviors provides the best performance. For this
small model, the lower ESS is more than compen-
sated for by its low cost of evaluation, ∆Csamp.
This emphasizes the important point that, ulti-
mately, only sampling performance at parameters
of interest matters; scaling laws are only impor-
tant insofar as they can be used to predict the
performance of an algorithm at one target set of
parameters from its performance elsewhere.
6. Architecture dependence implies theory
dependence. As demonstrated in Fig. 6, even
diﬀerent choices of hyperparameters within an
architecture class can lead to signiﬁcantly diﬀer-
ent scaling of model quality with the parameters
of the target theory. As discussed in Sec. 2.2,
much more signiﬁcant diﬀerences in architecture
are possible, from which we should expect an

12
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
1.0
0.6
0.2
ESS
κ = 0.0
0.25
0.263
82
122
162
202
242
L2
0.4
0.6
aMπ
Fig. 7
Volume scaling of the ESS for models trained to
sample the Schwinger model at β = 2 and three diﬀerent
values of the fermion mass, including the pure-gauge limit,
κ = 0. The top and bottom panels illustrate the volume
dependence of the ESS and pseudoscalar mass aMπ, respec-
tively. For each target κ, models are trained from scratch
at L = 8 and volume transferred without retraining. In
the top panel, the dotted lines are extrapolations back-
wards from the largest volume using Eq. (8). Deviations of
the markers below these lines correspond to better-than-
exponential scaling at smaller volumes, where the lower
panel shows signiﬁcant ﬁnite-size eﬀects in aMπ. The ESS
is evaluated on 104 samples at each volume. Error bars are
smaller than the markers. Further details are provided in
Appendix A.
even greater variety in scaling behaviors. Impor-
tantly, treating diﬀerent physical theories requires
structurally diﬀerent architectures. For example,
SU(N) variables cannot be ﬂowed using the same
transformations applicable to real scalar ﬁelds.
Further theory-speciﬁc engineering is necessary
to encode symmetries and other physical features.
Given
the
dissimilarity
between
architectures
required for diﬀerent theories, it should not be
assumed that features of model quality obtained
for one theory apply for any other theory of inter-
est; that is, studying the scaling of ﬂow model
sampling for toy models such as φ4-theory or
even the Schwinger model provides little informa-
tion about scaling properties for any ﬂow-based
approach to sampling gauge ﬁeld conﬁgurations
for QCD.
7. Improving performance requires physics-
informed algorithm design. As seen in Figs. 1,
2, and 4, training with a ﬁxed protocol even-
tually enters a slowly improving regime, after
which point the expense of additional training will
not reliably provide a practical increase in sam-
pling eﬃciency. Similarly, as illustrated in Fig. 8,
increasing the model size within ﬁxed architecture
classes often provides diminishing improvements
in the model quality achieved by a ﬁxed train-
ing protocol. Although further training or increase
in model size may provide sudden improvements
(e.g. second-wind training dynamics as in Fig. 4),
these saturation-like behaviors present a practi-
cal obstacle to increasing sampling eﬃciency using
“brute force” application of additional computa-
tional power.
Instead, improving sampler eﬃciency requires
more
qualitative
algorithmic
improvements.
Among the inﬁnite possible variations of model
architecture, it is natural to expect that choices
which incorporate a priori physics understand-
ing will lead to better model quality. This is
illustrated in Fig. 8, which compares two closely
related architectures for scalar ﬁeld theory. Here
it is clearly visible that the physics-informed
modiﬁcation is far more successful at improving
model quality than simply increasing the model
size. Along the same lines, developing eﬃcient
ﬂow-based sampling algorithms for QCD will
require developing and testing new architectures,
training schemes, and sampling approaches which
incorporate a priori physics knowledge.
5 Outlook
For ﬂow-based approaches to the sampling of lat-
tice ﬁeld conﬁgurations, even small diﬀerences in
the ML approach—architecture, training scheme,
and how they are varied with the parameters
of the target theory—result in not only diﬀerent
overall costs, but also diﬀerent cost scalings with
the parameters of the target theory. In eﬀect, each
ML approach deﬁnes a diﬀerent sampling algo-
rithm with diﬀerent cost scaling properties. Fur-
ther, because theory-speciﬁc modeling is required,
scaling properties assessed in one theory should
not be expected to generalize to others. Taken
together, this implies a very diﬀerent paradigm for
assessing algorithm scalability than has applied for
QCD algorithms thus far, where scaling properties
assessed in toy theories often translate directly to
QCD. For ﬂow-based methods, assessing scalabil-
ity will require direct, experimental investigation
of applications to QCD itself, which has only just
begun [35].

Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
13
20
40
60
80
100
nL
0.00
0.25
0.50
0.75
ESS
Normal
Free field
Fig. 8
Dependence of ESS on number of coupling layers
nL for two diﬀerent architectures, with all other architec-
ture and training hyperparameters ﬁxed, for real scalar ﬁeld
theory on a 16 × 16 lattice with m2 = −4 and λ = 1.25.
The architectures diﬀer by only the base distribution: inde-
pendent normal on each site (“Normal”), and the free-ﬁeld
distribution with a learned pole mass (“Free ﬁeld”; as
described in Appendix A). Each model is trained from a
random initialization for 150k steps, which is suﬃcient to
reach a slowly improving regime in all cases. Training uses
batch size 1024, and the learning rate is decayed by a factor
2 every 10000 steps. Uncertainties for each point include
the spread between two repeated experiments with diﬀer-
ent pseudorandom seeds. Further details are provided in
Appendix A.
It is not yet clear what at-scale applications
of ﬂow-based sampling methods to QCD will look
like, but we can speculate, and some key aspects
are already clear. For example, transfer learning
and retraining will play a central role in mitigating
training costs. Thus, architectures well-suited for
transfer learning across wide ranges of parameters
and volumes will be important in order to exploit
this technique. Exponential volume scaling sug-
gests that high-quality models will be necessary to
achieve eﬃcient sampling at scale. Coupled with
the large overall scale of training costs, this sug-
gests a paradigm of use very diﬀerent from HMC.
As seen for large ML models in industry applica-
tions, such as GPT [108] and MT-NLG [109], the
typical paradigm is that signiﬁcant computational
resources and human time are invested over years
in exploration and training. The resulting mod-
els can then be shared as a community resource,
amortizing the bulk of training and development
costs across the community as a whole.
Direct sampling with ﬂow models has signif-
icant natural advantages over serial algorithms
such as HMC, but requires high-quality models
for eﬃcient sampling. However, there are many
other sampling approaches where ﬂows may be
employed. “Hybrid” approaches involving both
ﬂow components and more traditional sampling
algorithms can exploit lower-quality models while
leveraging the decades of engineering invested in
algorithms like HMC. It is likely that the ﬁrst
at-scale application will involve such a hybrid
approach. Over the longer term, improving ﬂow
model technology will enable increasingly eﬃcient
sampling approaches.
While the growing body of work on ﬂow-based
sampling methods continues to provide promising
early results, we have only just begun to explore
the space of what is possible. The broader ﬁeld of
machine learning is advancing rapidly, and expe-
rience dictates that we cannot anticipate what
capabilities will be enabled by new developments.
Creativity guided by physical intuition remains
our most eﬀective means of making progress.
Acknowledgments.
The authors thank Heiko
Strathmann for detailed comments on a draft of
this manuscript, and Gurtej Kanwar for useful dis-
cussions. RA, DCH, FRL, PES, and JMU are sup-
ported in part by the U.S. Department of Energy,
Oﬃce of Science, Oﬃce of Nuclear Physics, under
grant Contract Number DE-SC0011090. PES is
additionally supported by the National Science
Foundation under EAGER grant 2035015, by the
U.S. DOE Early Career Award DE-SC0021006, by
a NEC research award, and by the Carl G and
Shirley Sontheimer Research Fund. KC and MSA
are supported by the National Science Founda-
tion under the award PHY-2141336. MSA thanks
the Flatiron Institute for their hospitality. DB
is supported by the Argonne Leadership Com-
puting Facility, which is a U.S. Department of
Energy Oﬃce of Science User Facility operated
under contract DE-AC02-06CH11357. This work
is funded by the U.S. National Science Founda-
tion under Cooperative Agreement PHY-2019786
(The NSF AI Institute for Artiﬁcial Intelligence
and Fundamental Interactions, http://iaiﬁ.org/).
This work is associated with an ALCF Aurora
Early Science Program project, and used resources
of the Argonne Leadership Computing Facility,
which is a DOE Oﬃce of Science User Facility
supported under Contract DEAC02-06CH11357.
The authors acknowledge the MIT SuperCloud
and Lincoln Laboratory Supercomputing Cen-
ter [110] for providing HPC resources that have
contributed to the research results reported within

14
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
this paper. Numerical experiments and data anal-
ysis used PyTorch [111], JAX [112], Haiku [113],
Horovod [114], NumPy [115], and SciPy [116].
Figures were produced using matplotlib [117].
Appendix A
Details of
numerical
examples
Numerical illustrations of ﬂow models trained to
sample scalar ﬁeld conﬁgurations are optimized to
model the action
S(φ) =
X
x
h
−
d
X
µ=1
φ(x)φ(x + ˆµ)
+1
2(m2 + 2d)φ(x)2 + λφ(x)4i
(A1)
where d = 2. Architectures are stacks of nL aﬃne
coupling transformations with checkerboard vari-
able partitioning, parametrized by convolutional
neural networks, as implemented in Ref. [22]. All
architectures have nL = 24 coupling layers, con-
volutional kernel size K = 5, and two layers of
hidden channels of width 12 within each neural
network, except for the model used to generate the
results shown in Fig. 6 where nL and K diﬀer as
noted. Reverse KL self-training protocols are also
similar to Ref. [22], but with the addition of learn-
ing rate or batch size scheduling where indicated.
Gradient clipping has also been applied, speciﬁ-
cally rescaling all gradients by a common factor as
necessary to prevent the norm over all gradients
from exceeding 100. Training and ESS evaluation
both use batch size 16384, except as noted in
Fig. 1. Results shown in Fig. 6 are initially trained
for 150000 steps at λ = 1.25, then for 500 steps at
each subsequent λ, one after the other. This is suf-
ﬁcient to reach a slowly improving regime for all
λ, and produces comparable ﬁnal results as from-
scratch training or starting instead from λ = 4.
For the results shown in Figs. 3, 5, and 6, when
retraining, the learning rate is set to 10−4 and the
optimizer state is never reset (i.e. it is carried over
from training for the previous parameters).
Sampling from the “Free ﬁeld” base distri-
bution used in the demonstration of Fig. 8 is
accomplished using a layer which transforms the
scalar ﬁeld variables in momentum space. We ﬁrst
perform a change of basis into Fourier space via
φ(k) = P
x φ(x)e−ik·x. We then transform the
resulting momentum-space variables as φ′(k) =
σkφ(k), with
σ2
k =
 
µ + 2d −2
4
X
ν=1
cos(kν)
!−1
(A2)
where µ is a learned parameter. Finally, the
updated variables are transformed back to posi-
tion space via φ′(x) = P
k φ′(k)eik·x/Ω. Com-
paring with the free-ﬁeld (i.e. λ = 0) action in
momentum space,
S = 1
2
X
k
 
m2 + 2d −2
X
ν
cos(kν)
!
|φ(k)|2 ,
(A3)
we see that if φ(k) are independent Gaussians for
each k, then the transformation amounts to sam-
pling from the free theory with learnable pole mass
µ. This holds when φ(x) are independent Gaus-
sians on each site, i.e. if the layer is applied directly
after the draw from the typical base distribution,
as employed above.
Results shown in Figs. 2 and 7 are obtained
for models trained to sample from the theory
deﬁned by the two-ﬂavor Wilson fermion action as
employed in Ref. [32], speciﬁcally
SE(U) = −β
X
x
ReP(x) −log det D[U]†D[U]
(A4)
where β = 2/g2
0 encodes the bare gauge coupling
g0, and
P(x) = U0(x)U1(x + ˆ0)U †
0(x + ˆ1)U †
1(x)
(A5)
is the plaquette. The Wilson discretization of the
Dirac operator is
D[U](y, x) = δ(y −x)
−κ
X
µ=0,1
h
(1 −σµ)Uµ(y)δ(y −x + ˆµ)
+ (1 + σµ)U †
µ(y −ˆµ)δ(y −x −ˆµ)
i
,
(A6)
where σ0, σ1 = σx, σy are the Pauli matrices and
κ = 1/(4 + 2m0) encodes the bare fermion mass
m0. Architectures are similar to those of Ref. [32],
with three diﬀerences: 1) models are built of
24 gauge-equivariant coupling layers rather than

Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
15
48; 2) neural networks use standard convolutions
with kernel size 3 rather than dilated convolu-
tions; and 3) intermediate layers have 32 channels
rather than 64. All models used to create the
results in Figs. 2 and 7 have the same architec-
ture. Note that training and sampling uses exact
computation of the fermion determinant, without
any stochastic pseudofermion estimators like in
Ref. [34].
To generate the results shown in Fig. 7, mod-
els are self-trained with batch size 6144, for ∼17k,
43k, and 85k steps for β = 0, 0.25, and 0.263,
respectively, at which points training has entered
a slowly improving regime. The initial learning
rate 3 × 10−4 is decayed by a factor 0.5 every 10k
steps. The pseudoscalar mass Mπ is obtained as
the eﬀective mass at the center of the lattice, as
measured on conﬁgurations generated using ﬂow-
model proposals with independence Metropolis.
Throughout this work, the ESS metric used to
evaluate model quality is estimated as
ESS =
1
⟨w2⟩q
≈(1/N P
i ˜wi)2
1/N P
i ˜w2
i
(A7)
where ˜wi = exp[−S(φi)]/q(φi). This estimator is
known to be positively biased at ﬁnite sample
size, especially in the presence of mode collapse,
where undersampling of mismodeled regions may
result in an apparently large ESS when the true
value may be near zero [25]. This can be diag-
nosed using validation data drawn from the target
distribution, using the reweighted estimator [25]
ESS =
1
⟨w⟩p
≈
1

1
N
P
i
1
˜
wi

(1/N P
i ˜wi)
.
(A8)
Using
the
target-sample
estimator
to
spot-
check the model-sample ESS estimates presented
throughout this work reveals no signiﬁcant dis-
crepancies.
References
[1] C. Morningstar, The Monte Carlo method
in quantum ﬁeld theory (2007). arXiv:hep-
lat/0702020
[2] C. Lehner, et al., Opportunities for Lat-
tice
QCD
in
Quark
and
Lepton
Fla-
vor Physics.
Eur. Phys. J. A 55(11),
195 (2019).
https://doi.org/10.1140/epja/
i2019-12891-2. arXiv:1904.09479 [hep-lat]
[3] A.S. Kronfeld, D.G. Richards, W. Det-
mold, R. Gupta, H.W. Lin, K.F. Liu, A.S.
Meyer, R. Suﬁan, S. Syritsyn, Lattice QCD
and Neutrino-Nucleus Scattering.
Eur.
Phys. J. A 55(11), 196 (2019).
https:
//doi.org/10.1140/epja/i2019-12916-x.
arXiv:1904.09931 [hep-lat]
[4] V. Cirigliano, Z. Davoudi, T. Bhattacharya,
T.
Izubuchi,
P.E.
Shanahan,
S.
Syrit-
syn, M.L. Wagman, The Role of Lat-
tice QCD in Searches for Violations of
Fundamental Symmetries and Signals for
New Physics.
Eur. Phys. J. A 55(11),
197 (2019).
https://doi.org/10.1140/epja/
i2019-12889-8. arXiv:1904.09704 [hep-lat]
[5] W. Detmold, R.G. Edwards, J.J. Dudek,
M.
Engelhardt,
H.W.
Lin,
S.
Meinel,
K. Orginos, P. Shanahan, Hadrons and
Nuclei. Eur. Phys. J. A 55(11), 193 (2019).
https://doi.org/10.1140/epja/i2019-12902-
4. arXiv:1904.09512 [hep-lat]
[6] A.
Bazavov,
F.
Karsch,
S.
Mukher-
jee,
P.
Petreczky,
Hot-dense
Lattice
QCD: USQCD whitepaper 2018.
Eur.
Phys. J. A 55(11), 194 (2019).
https:
//doi.org/10.1140/epja/i2019-12922-0.
arXiv:1904.09951 [hep-lat]
[7] B. Jo´o, C. Jung, N.H. Christ, W. Det-
mold, R. Edwards, M. Savage, P. Shanahan,
Status and Future Perspectives for Lattice
Gauge Theory Calculations to the Exas-
cale and Beyond. Eur. Phys. J. A 55(11),
199 (2019).
https://doi.org/10.1140/epja/
i2019-12919-7. arXiv:1904.09725 [hep-lat]
[8] R.C. Brower, A. Hasenfratz, E.T. Neil,
S.
Catterall,
G.
Fleming,
J.
Giedt,
E.
Rinaldi,
D.
Schaich,
E.
Weinberg,
O.
Witzel,
Lattice
Gauge
Theory
for
Physics Beyond the Standard Model. Eur.
Phys. J. A 55(11), 198 (2019).
https:
//doi.org/10.1140/epja/i2019-12901-5.

16
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
arXiv:1904.09964 [hep-lat]
[9] T. DeGrand, Lattice tests of beyond Stan-
dard Model dynamics. Rev. Mod. Phys. 88,
015,001 (2016).
https://doi.org/10.1103/
RevModPhys.88.015001.
arXiv:1510.05018
[hep-ph]
[10] B. Svetitsky, Looking behind the Standard
Model with lattice gauge theory.
EPJ
Web Conf. 175, 01,017 (2018).
https:
//doi.org/10.1051/epjconf/201817501017.
arXiv:1708.04840 [hep-lat]
[11] G.D. Kribs, E.T. Neil, Review of strongly-
coupled
composite
dark
matter
models
and lattice simulations.
Int. J. Mod.
Phys. A 31(22), 1643,004 (2016).
https:
//doi.org/10.1142/S0217751X16430041.
arXiv:1604.04627 [hep-ph]
[12] I.
Ichinose,
T.
Matsui,
Lattice
Gauge
Theory
for
Condensed
Matter
Physics:
Ferromagnetic
Superconductivity
as
its
Example.
Mod.
Phys.
Lett.
B
28,
1430,012 (2014).
https://doi.org/10.1142/
S0217984914300129. arXiv:1408.0089 [cond-
mat.str-el]
[13] M. Mathur, T.P. Sreeraj, Lattice gauge the-
ories and spin models.
Phys. Rev. D 94,
085,029 (2016).
https://doi.org/10.1103/
PhysRevD.94.085029.
arXiv:1604.00315
[hep-lat]
[14] S. Duane, A.D. Kennedy, B.J. Pendleton,
D. Roweth, Hybrid Monte Carlo. Phys. Lett.
B 195, 216–222 (1987).
https://doi.org/
10.1016/0370-2693(87)91197-X
[15] R.M. Neal, Probabilistic inference using
Markov
chain
Monte
Carlo
methods
(Department of Computer Science, Uni-
versity of Toronto Toronto, ON, Canada,
1993), chap. 5
[16] R.M. Neal, Bayesian Learning for Neural
Networks. Lecture Notes in Statistics 118
(1996)
[17] U. Wolﬀ, Critical slowing down. Nucl. Phys.
Proc. Suppl. 17, 93–102 (1990).
https://
doi.org/10.1016/0920-5632(90)90224-I
[18] S.
Schaefer,
R.
Sommer,
F.
Virotta,
Investigating
the
critical
slowing
down
of
QCD
simulations.
PoS LAT2009,
032
(2009).
https://doi.org/10.22323/
1.091.0032. arXiv:0910.1465 [hep-lat]
[19] S. Schaefer, R. Sommer, F. Virotta, Critical
slowing down and error analysis in lattice
QCD simulations.
Nucl. Phys. B 845,
93–119 (2011).
https://doi.org/10.1016/
j.nuclphysb.2010.11.020.
arXiv:1009.5228
[hep-lat]
[20] S.H.
Li,
L.
Wang,
Neural
Network
Renormalization
Group.
Phys.
Rev.
Lett.
121,
260,601
(2018).
https://
doi.org/10.1103/PhysRevLett.121.260601.
URL
https://link.aps.org/doi/10.1103/
PhysRevLett.121.260601
[21] M.S. Albergo, G. Kanwar, P.E. Shana-
han,
Flow-based
generative
models
for
Markov
chain
Monte
Carlo
in
lattice
ﬁeld
theory.
Phys.
Rev.
D
100(3),
034,515 (2019).
https://doi.org/10.1103/
PhysRevD.100.034515.
arXiv:1904.12072
[hep-lat]
[22] M.S. Albergo, D. Boyda, D.C. Hackett,
G. Kanwar, K. Cranmer, S. Racani`ere, D.J.
Rezende, P.E. Shanahan, Introduction to
Normalizing Flows for Lattice Field Theory
(2021). arXiv:2101.08176 [hep-lat]
[23] K.A. Nicoli, S. Nakajima, N. Strodthoﬀ,
W. Samek, K.R. M¨uller, P. Kessel, Asymp-
totically unbiased estimation of physical
observables with neural samplers.
Phys.
Rev. E 101(2), 023,304 (2020).
https:
//doi.org/10.1103/PhysRevE.101.023304.
arXiv:1910.13496 [cond-mat.stat-mech]
[24] K.A. Nicoli, C.J. Anders, L. Funcke, T. Har-
tung, K. Jansen, P. Kessel, S. Nakajima,
P. Stornati, On Estimation of Thermody-
namic Observables in Lattice Field Theo-
ries with Deep Generative Models (2020).
arXiv:2007.07115 [hep-lat]

Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
17
[25] D.C. Hackett, C.C. Hsieh, M.S. Albergo,
D. Boyda, J.W. Chen, K.F. Chen, K. Cran-
mer, G. Kanwar, P.E. Shanahan, Flow-
based
sampling
for
multimodal
distri-
butions
in
lattice
ﬁeld
theory
(2021).
arXiv:2107.00734 [hep-lat]
[26] L. Del Debbio, J.M. Rossney, M. Wil-
son, Eﬃcient Modelling of Trivializing Maps
for Lattice φ4 Theory Using Normalizing
Flows: A First Look at Scalability (2021).
arXiv:2105.12481 [hep-lat]
[27] G.
Kanwar,
M.S.
Albergo,
D.
Boyda,
K. Cranmer, D.C. Hackett, S. Racani`ere,
D.J. Rezende, P.E. Shanahan, Equivari-
ant ﬂow-based sampling for lattice gauge
theory.
Phys.
Rev.
Lett.
125(12),
121,601 (2020).
https://doi.org/10.1103/
PhysRevLett.125.121601. arXiv:2003.06413
[hep-lat]
[28] D. Boyda, G. Kanwar, S. Racani`ere, D.J.
Rezende, M.S. Albergo, K. Cranmer, D.C.
Hackett, P.E. Shanahan, Sampling using
SU(N) gauge equivariant ﬂows.
Phys.
Rev. D 103(7), 074,504 (2021).
https:
//doi.org/10.1103/PhysRevD.103.074504.
arXiv:2008.05456 [hep-lat]
[29] S. Foreman, X.Y. Jin, J.C. Osborn, Deep
Learning Hamiltonian Monte Carlo (2021).
arXiv:2105.03418 [hep-lat]
[30] S. Foreman, T. Izubuchi, L. Jin, X.Y. Jin,
J.C. Osborn, A. Tomiya, HMC with Nor-
malizing Flows (2021).
arXiv:2112.01586
[cs.LG]
[31] S.
Foreman,
X.Y.
Jin,
J.C.
Osborn,
LeapfrogLayers: A Trainable Framework for
Eﬀective Topological Sampling. PoS LAT-
TICE2021, 508 (2022).
https://doi.org/
10.22323/1.396.0508.
arXiv:2112.01582
[hep-lat]
[32] M.S.
Albergo,
D.
Boyda,
K.
Cranmer,
D.C. Hackett, G. Kanwar, S. Racani`ere,
D.J.
Rezende,
F.
Romero-L´opez,
P.E.
Shanahan, J.M. Urban, Flow-based sam-
pling
in
the
lattice
Schwinger
model
at
criticality.
Phys.
Rev.
D
106(1),
014,514 (2022).
https://doi.org/10.1103/
PhysRevD.106.014514.
arXiv:2202.11712
[hep-lat]
[33] J.
Finkenrath,
Tackling
critical
slowing
down using global correction steps with
equivariant ﬂows: the case of the Schwinger
model (2022). arXiv:2201.02216 [hep-lat]
[34] R.
Abbott,
M.S.
Albergo,
D.
Boyda,
K. Cranmer, D.C. Hackett, G. Kanwar,
S. Racani`ere, D.J. Rezende, F. Romero-
L´opez,
P.E.
Shanahan,
B.
Tian,
J.M.
Urban, Gauge-equivariant ﬂow models for
sampling
in
lattice
ﬁeld
theories
with
pseudofermions.
Phys. Rev. D 106(7),
074,506 (2022).
https://doi.org/10.1103/
PhysRevD.106.074506.
arXiv:2207.08945
[hep-lat]
[35] R.
Abbott,
M.S.
Albergo,
A.
Botev,
D.
Boyda,
K.
Cranmer,
D.C.
Hackett,
G. Kanwar, A.G. Matthews, S. Racani`ere,
A. Razavi, D.J. Rezende, F. Romero-L´opez,
P.E. Shanahan, J.M. Urban, Sampling QCD
ﬁeld conﬁgurations with gauge-equivariant
ﬂow models (2022). arXiv:2208.03832 [hep-
lat]
[36] M.S. Albergo, G. Kanwar, S. Racani`ere, D.J.
Rezende, J.M. Urban, D. Boyda, K. Cran-
mer, D.C. Hackett, P.E. Shanahan, Flow-
based sampling for fermionic lattice ﬁeld
theories (2021). arXiv:2106.05934 [hep-lat]
[37] M. Gabri´e, G.M. Rotskoﬀ, E. Vanden-
Eijnden,
Adaptive
Monte
Carlo
augmented with normalizing ﬂows (2021).
arXiv:2105.12603 [physics.data-an]
[38] P. de Haan, C. Rainone, M.C.N. Cheng,
R. Bondesan, Scaling Up Machine Learn-
ing
For
Quantum
Field
Theory
with
Equivariant
Continuous
Flows
(2021).
arXiv:2110.02673 [cs.LG]
[39] S.
Lawrence,
Y.
Yamauchi,
Normal-
izing
Flows
and
the
Real-Time
Sign
Problem.
Phys.
Rev.
D
103(11),
114,509 (2021).
https://doi.org/10.1103/
PhysRevD.103.114509.
arXiv:2101.05755
[hep-lat]

18
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
[40] X.Y. Jin, Neural Network Field Transfor-
mation and Its Application in HMC (2022).
arXiv:2201.01862 [hep-lat]
[41] J.M. Pawlowski, J.M. Urban, Flow-based
density of states for complex actions (2022).
arXiv:2203.01243 [hep-lat]
[42] M. Gerdes, P. de Haan, C. Rainone, R. Bon-
desan, M.C.N. Cheng, Learning Lattice
Quantum Field Theories with Equivariant
Continuous Flows (2022). arXiv:2207.00283
[hep-lat]
[43] A. Singha, D. Chakrabarti, V. Arora, Con-
ditional Normalizing ﬂow for Monte Carlo
sampling in lattice scalar ﬁeld theory (2022).
arXiv:2207.00980 [hep-lat]
[44] A. Matthews, M. Arbel, D.J. Rezende,
A.
Doucet,
Continual
repeated
annealed
ﬂow
transport
Monte
Carlo
162,
15,196–15,219
(2022).
URL
https://proceedings.mlr.press/v162/
matthews22a.html
[45] M. Caselle, E. Cellini, A. Nada, M. Panero,
Stochastic
normalizing
ﬂows
as
non-
equilibrium
transformations
(2022).
arXiv:2201.08862 [hep-lat]
[46] M. Caselle, E. Cellini, A. Nada, M. Panero,
Stochastic normalizing ﬂows for lattice ﬁeld
theory (2022). arXiv:2210.03139 [hep-lat]
[47] L. Wang, Exploring cluster Monte Carlo
updates with Boltzmann machines.
Phys.
Rev.
E
96,
051,301
(2017).
https:
//doi.org/10.1103/PhysRevE.96.051301.
URL
https://link.aps.org/doi/10.1103/
PhysRevE.96.051301
[48] L. Huang, L. Wang, Accelerated Monte
Carlo simulations with restricted Boltz-
mann
machines.
Physical
Review
B
95(3),
–
(2017).
https://doi.org/
10.1103/physrevb.95.035105.
URL http:
//dx.doi.org/10.1103/PhysRevB.95.035105
[49] J.
Song,
S.
Zhao,
S.
Ermon,
A-nice-
mc: Adversarial training for mcmc (2018).
arXiv:1706.07561 [stat.ML]
[50] A. Tanaka, A. Tomiya, Towards reduction of
autocorrelation in HMC by machine learn-
ing (2017). arXiv:1712.03893 [hep-lat]
[51] D. Levy, M.D. Hoﬀman, J. Sohl-Dickstein,
Generalizing hamiltonian monte carlo with
neural networks (2018).
arXiv:1711.09268
[stat.ML]
[52] J.M. Pawlowski,
J.M.
Urban,
Reducing
Autocorrelation Times in Lattice Simu-
lations with Generative Adversarial Net-
works. Mach. Learn. Sci. Tech. 1, 045,011
(2020). https://doi.org/10.1088/2632-2153/
abae73. arXiv:1811.03533 [hep-lat]
[53] G.
Cossu,
L.
Del
Debbio,
T.
Giani,
A. Khamseh, M. Wilson, Machine learn-
ing determination of dynamical parameters:
The Ising model case. Phys. Rev. B 100(6),
064,304 (2019).
https://doi.org/10.1103/
PhysRevB.100.064304.
arXiv:1810.11503
[physics.comp-ph]
[54] D.
Wu,
L.
Wang,
P.
Zhang,
Solving
Statistical
Mechanics
Using
Variational
Autoregressive
Networks.
Phys.
Rev.
Lett.
122,
080,602
(2019).
https://
doi.org/10.1103/PhysRevLett.122.080602.
URL
https://link.aps.org/doi/10.1103/
PhysRevLett.122.080602
[55] D. Bachtis, G. Aarts, B. Lucini, Extending
machine
learning
classiﬁcation
capabili-
ties with histogram reweighting.
Phys.
Rev. E 102(3), 033,303 (2020).
https:
//doi.org/10.1103/PhysRevE.102.033303.
arXiv:2004.14341 [cond-mat.stat-mech]
[56] Y. Nagai, A. Tanaka, A. Tomiya, Self-
learning Monte-Carlo for non-abelian gauge
theory with dynamical fermions (2020).
arXiv:2010.11900 [hep-lat]
[57] A. Tomiya, Y. Nagai, Gauge covariant neu-
ral network for 4 dimensional non-abelian
gauge theory (2021). arXiv:2103.11965 [hep-
lat]
[58] D. Bachtis, G. Aarts, F. Di Renzo, B. Lucini,
Inverse renormalization group in quantum
ﬁeld theory. Phys. Rev. Lett. (to appear)

Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
19
(2021). arXiv:2107.00466 [hep-lat]
[59] D. Wu, R. Rossi, G. Carleo, Unbiased Monte
Carlo Cluster Updates with Autoregressive
Neural Networks (2021). arXiv:2105.05650
[cond-mat.stat-mech]
[60] A. Tomiya, S. Terasaki, GomalizingFlow.jl:
A Julia package for Flow-based sampling
algorithm for lattice ﬁeld theory (2022).
arXiv:2208.08903 [hep-lat]
[61] B. M´at´e, F. Fleuret, Deformation The-
ory
of
Boltzmann
Distributions
(2022).
arXiv:2210.13772 [hep-lat]
[62] D.J. Rezende, S. Mohamed, Variational
inference with normalizing ﬂows (2016).
arXiv:1505.05770 [stat.ML]
[63] L. Dinh, J. Sohl-Dickstein, S. Bengio, Den-
sity estimation using Real NVP (2017).
arXiv:1605.08803 [cs.LG]
[64] G.
Papamakarios,
E.
Nalisnick,
D.J.
Rezende,
S.
Mohamed,
B.
Laksh-
minarayanan,
Normalizing
ﬂows
for
probabilistic modeling and inference (2019).
arXiv:1912.02762 [stat.ML]
[65] D.H.
Weingarten,
D.N.
Petcher,
Monte
Carlo Integration for Lattice Gauge The-
ories with Fermions.
Phys. Lett. B 99,
333–338 (1981).
https://doi.org/10.1016/
0370-2693(81)90112-X
[66] F. Fucito, E. Marinari, G. Parisi, C. Rebbi,
A Proposal for Monte Carlo Simulations of
Fermionic Systems.
Nucl. Phys. B 180,
369 (1981).
https://doi.org/10.1016/0550-
3213(81)90055-9
[67] R.M. Neal, et al., MCMC using Hamilto-
nian dynamics. Handbook of Markov chain
Monte Carlo 2(11), 2 (2011)
[68] C.
Gattringer,
C.B.
Lang,
Quan-
tum
chromodynamics
on
the
lattice,
vol.
788
(Springer,
Berlin,
2010).
https://doi.org/10.1007/978-3-642-01850-3
[69] T. DeGrand, C.E. Detar, Lattice methods for
quantum chromodynamics (2006)
[70] L. Dinh, D. Krueger, Y. Bengio, Nice: Non-
linear independent components estimation
(2014). arXiv:1410.8516
[71] C. Durkan, A. Bekasov, I. Murray, G. Papa-
makarios, Neural spline ﬂows. Advances in
neural information processing systems 32
(2019)
[72] D.J.
Rezende,
G.
Papamakarios,
S. Racani`ere, M.S. Albergo, G. Kanwar,
P.E. Shanahan,
K. Cranmer,
Normaliz-
ing Flows on Tori and Spheres (2020).
arXiv:2002.02428 [stat.ML]
[73] L. Zhang, W. E, L. Wang, Monge-Amp`ere
Flow
for
Generative
Modeling
(2018).
arXiv:1809.10188 [cs.LG]
[74] C.W. Huang, R.T. Chen, C. Tsirigotis,
A. Courville, Convex potential ﬂows: Uni-
versal probability distributions with optimal
transport and convex optimization (2020).
arXiv:2012.05942 [cs.LG]
[75] B. Amos, L. Xu, J.Z. Kolter, Input con-
vex neural networks 70, 146–155 (2017).
arXiv:1609.07152 [cs.LG]
[76] R.T. Chen, Y. Rubanova, J. Bettencourt,
D.K. Duvenaud, Neural ordinary diﬀerential
equations. Advances in neural information
processing systems 31 (2018)
[77] N.
Metropolis,
A.W.
Rosenbluth,
M.N.
Rosenbluth, A.H. Teller, E. Teller, Equation
of state calculations by fast computing
machines.
J. Chem. Phys. 21, 1087–1092
(1953). https://doi.org/10.1063/1.1699114
[78] W.K.
Hastings,
Monte
Carlo
Sampling
Methods Using Markov Chains and Their
Applications. Biometrika 57, 97–109 (1970).
https://doi.org/10.1093/biomet/57.1.97
[79] L. Tierney, Markov chains for exploring pos-
terior distributions. the Annals of Statistics
pp. 1701–1728 (1994)

20
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
[80] S. Kullback, R.A. Leibler, On Information
and Suﬃciency. The Annals of Mathemat-
ical Statistics 22(1), 79 – 86 (1951). https:
//doi.org/10.1214/aoms/1177729694
[81] Q. Liu, J. Lee, M. Jordan, A kernelized
Stein discrepancy for goodness-of-ﬁt tests
pp. 276–284 (2016)
[82] J. Gorham, L. Mackey, Measuring sample
quality with kernels pp. 1292–1301 (2017)
[83] A. Hyv¨arinen, P. Dayan, Estimation of
non-normalized statistical models by score
matching.
Journal of Machine Learning
Research 6(4) (2005)
[84] O. Johnson, Information theory and the cen-
tral limit theorem (World Scientiﬁc, 2004)
[85] D.P. Kingma, J. Ba, Adam: A method
for
stochastic
optimization
(2017).
arXiv:1412.6980 [cs.LG]
[86] H. Wu, J. K¨ohler, F. No´e, Stochastic nor-
malizing ﬂows.
Advances in Neural Infor-
mation Processing Systems 33, 5933–5944
(2020). arXiv:2002.06707 [stat.ML]
[87] D.
Nielsen,
P.
Jaini,
E.
Hoogeboom,
O. Winther, M. Welling, Survae ﬂows: Sur-
jections to bridge the gap between vaes
and ﬂows.
Advances in Neural Informa-
tion Processing Systems 33, 12,685–12,696
(2020)
[88] M. Dibak, L. Klein, F. No´e, Temperature-
steerable ﬂows (2020). arXiv:2012.00429
[89] M.
Arbel,
A.
Matthews,
A.
Doucet,
Annealed ﬂow transport Monte Carlo. Inter-
national Conference on Machine Learning
pp. 318–330 (2021)
[90] A. Doucet, N. De Freitas, N.J. Gordon,
et al., Sequential Monte Carlo methods in
practice, vol. 1 (Springer, 2001)
[91] J.S. Liu, J.S. Liu, Monte Carlo strategies in
scientiﬁc computing, vol. 10 (Springer, 2001)
[92] L. Del Debbio, Recent progress in sim-
ulations
of
gauge
theories
on
the
lat-
tice.
J. Phys. Conf. Ser. 640(1), 012,049
(2015). https://doi.org/10.1088/1742-6596/
640/1/012049
[93] A. Beskos, N. Pillai, G. Roberts, J.M. Sanz-
Serna, A. Stuart, Optimal tuning of the
hybrid monte carlo algorithm.
Bernoulli
19(5A), 1501–1534 (2013)
[94] S. Schaefer, Status and challenges of simula-
tions with dynamical fermions. PoS LAT-
TICE2012, 001 (2012).
https://doi.org/
10.22323/1.164.0001. arXiv:1211.5069 [hep-
lat]
[95] M.
Hasenbusch,
Full
QCD
algorithms
towards the chiral limit. Nucl. Phys. B Proc.
Suppl. 129, 27–33 (2004). https://doi.org/
10.1016/S0920-5632(03)02504-0. arXiv:hep-
lat/0310029
[96] A. Ukawa, Computational cost of full QCD
simulations experienced by CP-PACS and
JLQCD Collaborations.
Nucl. Phys. B
Proc. Suppl. 106, 195–196 (2002).
https:
//doi.org/10.1016/S0920-5632(01)01662-0
[97] F. Jegerlehner,
R.D. Kenway,
G. Mar-
tinelli, C. Michael, O. Pene, B. Petersson,
R. Petronzio, C.T. Sachrajda, K. Schilling,
Requirements for high performance comput-
ing for lattice QCD: Report of the ECFA
working panel (2000).
https://doi.org/
10.5170/CERN-2000-002
[98] T.
Lippert,
Cost
of
QCD
simulations
with n(f) = 2 dynamical Wilson fermions.
Nucl. Phys. B Proc. Suppl. 106, 193–
194 (2002). https://doi.org/10.1016/S0920-
5632(01)01661-9. arXiv:hep-lat/0203009
[99] M.
Luscher,
S.
Schaefer,
Non-
renormalizability
of
the
HMC
algorithm.
JHEP
04,
104
(2011).
https://doi.org/10.1007/JHEP04(2011)104.
arXiv:1103.1810 [hep-lat]
[100] L. Del Debbio, G.M. Manca, E. Vicari,
Critical slowing down of topological modes.
Phys. Lett. B 594, 315–323 (2004). https:

Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
21
//doi.org/10.1016/j.physletb.2004.05.038.
arXiv:hep-lat/0403001
[101] M. Hasenbusch, Speeding up the hybrid
Monte
Carlo
algorithm
for
dynamical
fermions.
Phys.
Lett.
B
519,
177–
182 (2001). https://doi.org/10.1016/S0370-
2693(01)01102-9. arXiv:hep-lat/0107019
[102] J. Brannick, R.C. Brower, M.A. Clark,
J.C. Osborn, C. Rebbi, Adaptive Multi-
grid Algorithm for Lattice QCD.
Phys.
Rev. Lett. 100, 041,601 (2008).
https://
doi.org/10.1103/PhysRevLett.100.041601.
arXiv:0707.4018 [hep-lat]
[103] R. Babich, J. Brannick, R.C. Brower, M.A.
Clark, T.A. Manteuﬀel, S.F. McCormick,
J.C. Osborn, C. Rebbi, Adaptive multi-
grid
algorithm
for
the
lattice
Wilson-
Dirac operator.
Phys. Rev. Lett. 105,
201,602 (2010).
https://doi.org/10.1103/
PhysRevLett.105.201602.
arXiv:1005.3043
[hep-lat]
[104] S.L. Smith, P.J. Kindermans, C. Ying, Q.V.
Le, Don’t decay the learning rate, increase
the batch size (2017). arXiv:1711.00489
[105] M.D. Zeiler, Adadelta: an adaptive learning
rate method (2012). 1212.5701
[106] K. He, X. Zhang, S. Ren, J. Sun, Delving
Deep into Rectiﬁers: Surpassing Human-
Level Performance on ImageNet Classiﬁca-
tion (2015). arXiv:1502.01852 [cs.CV]
[107] X. Glorot, Y. Bengio, Understanding the dif-
ﬁculty of training deep feedforward neural
networks pp. 249–256 (2010)
[108] T. Brown, B. Mann, N. Ryder, M. Sub-
biah, J.D. Kaplan, P. Dhariwal, A. Nee-
lakantan, P. Shyam, G. Sastry, A. Askell,
et al., Language models are few-shot learn-
ers. Advances in neural information process-
ing systems 33, 1877–1901 (2020)
[109] S. Smith, M. Patwary, B. Norick, P. LeGres-
ley, S. Rajbhandari, J. Casper, Z. Liu,
S. Prabhumoye, G. Zerveas, V. Korthikanti,
et al., Using deepspeed and megatron to
train megatron-turing nlg 530b, a large-
scale generative language model (2022).
2201.11990
[110] A. Reuther, J. Kepner, C. Byun, S. Samsi,
W.
Arcand,
D.
Bestor,
B.
Bergeron,
V.
Gadepally,
M.
Houle,
M.
Hubbell,
et
al.,
Interactive
supercomputing
on
40,000
cores
for
machine
learning
and
data
analysis.
2018
IEEE
High
Performance
extreme
Computing
Con-
ference (HPEC) pp. 1–6 (2018).
https:
//doi.org/10.1109/hpec.2018.8547629.
arXiv:1807.07814 [cs.DC]
[111] A. Paszke, et al., in Advances in Neural
Information
Processing
Systems
32,
ed.
by H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d'Alch´e-Buc, E. Fox, R. Garnett
(Curran Associates, Inc., 2019), pp. 8024–
8035.
URL
http://papers.neurips.cc/
paper/9015-pytorch-an-imperative-style-
high-performance-deep-learning-library.pdf
[112] J. Bradbury, R. Frostig, P. Hawkins, M.J.
Johnson, C. Leary, D. Maclaurin, G. Necula,
A. Paszke, J. VanderPlas, S. Wanderman-
Milne, Q. Zhang, JAX: composable trans-
formations of Python+NumPy programs
(2018). URL http://github.com/google/jax
[113] T.
Hennigan,
T.
Cai,
T.
Norman,
I.
Babuschkin,
Haiku:
Sonnet
for
JAX
(2020). URL http://github.com/deepmind/
dm-haiku
[114] A.
Sergeev,
M.
Del
Balso,
Horovod:
fast and easy distributed deep learning
in
TensorFlow
arXiv:1802.05799
(2018).
arXiv:1802.05799 [cs.LG]
[115] C.R.
Harris,
K.J.
Millman,
S.J.
Van
Der
Walt,
R.
Gommers,
P.
Virtanen,
D.
Cournapeau,
E.
Wieser,
J.
Taylor,
S. Berg, N.J. Smith, et al., Array pro-
gramming with numpy. Nature 585(7825),
357–362 (2020)
[116] P. Virtanen, R. Gommers, T.E. Oliphant,
M. Haberland, T. Reddy, D. Cournapeau,
E. Burovski, P. Peterson, W. Weckesser,

22
Aspects of scaling and scalability for ﬂow-based sampling of lattice QCD
J. Bright, et al., Scipy 1.0: fundamental algo-
rithms for scientiﬁc computing in python.
Nature methods 17(3), 261–272 (2020)
[117] J.D. Hunter, Matplotlib: A 2d graphics envi-
ronment.
Computing in Science & Engi-
neering 9(3), 90–95 (2007). https://doi.org/
10.1109/MCSE.2007.55

