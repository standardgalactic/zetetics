On the Activation Function Dependence of the
Spectral Bias of Neural Networks
Qingguo Hong
Department of Mathematics, Pennsylvania State University
University Park, PA 16801
huq11@psu.edu
Jonathan W. Siegel
Department of Mathematics, Pennsylvania State University
University Park, PA 16801
jus1949@psu.edu
Qinyang Tan
Department of Mathematics, University of Southern California
Los Angeles, CA 90089
qinyangt@usc.edu
Jinchao Xu
Department of Mathematics, Pennsylvania State University
University Park, PA 16801
xu@math.psu.edu
Abstract
Neural networks are universal function approximators which are known to general-
ize well despite being dramatically overparameterized. We study this phenomenon
from the point of view of the spectral bias of neural networks. Our contributions
are two-fold. First, we provide a theoretical explanation for the spectral bias of
ReLU neural networks by leveraging connections with the theory of ﬁnite element
methods. Second, based upon this theory we predict that switching the activation
function to a piecewise linear B-spline, namely the Hat function, will remove this
spectral bias, which we verify empirically in a variety of settings. Our empirical
studies also show that neural networks with the Hat activation function are trained
signiﬁcantly faster using stochastic gradient descent and ADAM. Combined with
previous work showing that the Hat activation function also improves general-
ization accuracy on image classiﬁcation tasks, this indicates that using the Hat
activation provides signiﬁcant advantages over the ReLU on certain problems.
1
Introduction
Despite being heavily overparameterized, deep neural networks have been shown to be remarkably
good at generalizing to natural data. This has raised the important problem of understanding the
implicit regularization effect of deep neural networks [29, 30, 33, 37]. Recently, it has been proposed
that the training of deep neural networks exhibits a spectral bias [30, 37], in other words that low
frequencies are learned faster via training by stochastic gradient descent. This is proposed as a
Preprint. Under review.
arXiv:2208.04924v3  [cs.LG]  6 Sep 2022

mechanism which biases networks toward low-complexity solutions [30], and also helps explain the
recent success in using neural networks to solve PDEs [14, 31].
In this work, we study the dependence of the spectral bias phenomenon on the activation function of
the neural network. We begin by giving a theoretical derivation of the spectral bias phenomenon for
neural networks with ReLU activation function. This analysis is based on the connections between
ReLU networks and ﬁnite element methods [15]. Based upon this theory, we predict that a simple
modiﬁcation of the ReLU activation function, called the Hat activation function, will remove the
spectral bias of neural networks. We empirically verify these predictions on a variety of problems,
including both shallow and deep networks and real and simulated data. Moreover, we demonstrate
that removing the spectral bias has the effect that neural networks with the Hat activation function are
trained much faster than ReLU neural networks. Combined with recent results showing that the Hat
activation can improve performance on image classiﬁcation tasks [34], this demonstrates a signiﬁcant
advantage to using the Hat activation function over the ReLU.
Contributions
1. We provide a theoretical explanation for the spectral bias of ReLU neural networks by
drawing a comparison with the analysis of ﬁnite element methods. Our analysis applies to
networks of arbitrary width, i.e. not only in the inﬁnite or large width limit, and also shows
how the spectral bias increases with increasing width.
2. Based upon this theory, we predict that neural networks with a simple modiﬁcation of the
ReLU, called the Hat activation function, will not experience the spectral bias observed in
ReLU networks.
3. We validate this prediction empirically. Speciﬁcally, we show that the spectral bias dis-
appears when the ReLU activation function is replaced by the Hat activation function or
scaled Hat activation function. This shows that the spectral bias of neural networks depends
critically upon the activation function used. Our empirical studies also show that the net-
works with Hat activation function are much easier to train by gradient descent type training
algorithm.
2
Related Work
Deep neural networks have been shown to be able to ﬁt random data with perfect accuracy [40, 1].
This has motivated the study of the implicit regularization which prevents overﬁtting for such networks
[29, 30, 9]. The spectral bias as a mechanism for regularization has been proposed and studied in
[30, 37, 7, 24, 36, 12, 41, 38]. The convergence of neural networks on pure frequencies, which
supports the spectral bias phenomenon, has been studied in [2]. For applications of neural networks
to differential equations, it has been observed that high-frequencies are often missing or appear late in
training, and potential solutions, including novel activation functions, have been proposed [32, 4, 6].
The eigenstructure of the Fisher information matrix of neural network models has been studied in
[20], where it has been shown that only a few eigenvalues are large for overparameterized neural
networks. The spectral bias has also been related to kernel methods and the neural tangent kernel
[19, 8, 18, 23, 25], and has been observed for GANs [21]. In addition, the spectral bias, or F-principle,
has been observed to be robust to the speciﬁc optimization methods used [26].
Our work is primarily concerned with the theoretical underpinning of the spectral bias. This has
previously been studied in [37] for the sigmoidal activation function. We provide a theory explaining
the spectral bias for the ReLU activation function. However, our theory and empirical work also
show that the spectral bias is not universal. In particular, it depends signiﬁcantly upon the activation
function used. A similar conclusion was reached by analyzing the spectrum of the neural tangent
kernel in [38], where it was shown that a sigmoidal network does not exhibit a spectral bias with
appropriate initialization.
In contrast, in our work we show that neural networks with a Hat activation function do not exhibit a
spectral bias. The Hat activation function has been empirically analyzed for computer vision problems
in [34], where is it shown that the can improve the performance of the neural network. The closely
related Mexican Hat function has also been proposed as an activation function in [28]. Instead of
considering the decay of the eigenvalues of the neural tangent kernel, which is adapted to the network
architecture, we consider Fourier modes and eigenfunctions of kernels which only depend upon the
2

data, which relates the spectral bias to more natural notions of frequency. The advantage of our
approach is that our analysis holds for networks of arbitrary width and shows how the spectral bias
increases with increasing width. Further, our empirical work demonstrates that neural networks with
the Hat activation function train signiﬁcantly more quickly than ReLU networks, which motivates
further study into the use of the Hat activation.
3
Spectral Bias of Neural Networks
The spectral bias of neural networks has been well-established in prior works. This concept of the
spectral bias depends upon an appropriate notion of frequency combined with the observation that
certain frequencies converge faster during training [30, 9].
3.1
Notion of Frequency
By frequency we mean the eigenfunctions of a self-adjoint compact operator T on L2(dµ) for an
appropriate measure dµ. This notion has been well-studied in the machine learning literature in the
context of kernels [11, 16]. We begin with two important examples, the ﬁrst corresponding to a
continuous measure dµ and the second to a discrete measure dµ on the training data.
Continuous measure: Consider the torus Rd/Zd with the Lebesgue measure dµ. Let T be the
convolution operator Ts given by
Ts(f)(x) =
 s
2π
d/2 Z
Rd e−s
2 |x−y|2f(y)dy.
(1)
Note that in the integral above f(y) is viewed as a periodic function on the whole space. It is well-
known that this convolution operator gives the solution to the heat equation with periodic boundary
conditions and initial condition f. Speciﬁcally, if u denotes the solution to
ut + 1
2∆u = 0, u(0, x) = f(x),
(2)
then we have u(s, x) = Ts−1(f)(x). Thus, the operator Ts : L2(Rd/Zd) →L2(Rd/Zd) can be
thought of as a smoothing or averaging operator. The eigenfunctions of this operator are given by
the Fourier modes e2πiω·x with ω ∈Zd and the eigenvalues in this case are given by the Fourier
coefﬁcients of the associated kernel ks : (Rd/Zd) × (Rd/Zd) given by
k(x, y) =
 s
4π
d/2 X
k∈Zd
e−(s/2)|x−y−k|2,
which are (4πs)−d/2e−(2s)−1|ω|2 [3, 16]. High frequencies correspond to small eigenvalues. This is
due to the intuitive notion that high frequencies decay rapidly under averaging.
Discrete measure: Consider a ﬁnite set of data D = {d1, ..., dN} in Rd. In high dimensions, the
number of data points N must scale exponentially with d in order for traditional Fourier modes to be
a useful notion of frequency. In particular, in d-dimensions, there are M = O(2d) vectors ωi ∈Rd
for i = 1, ..., M which satisfy |ωi −ωj| > 1
2 and |ωi| ≤1. However, if we only have N datapoints,
at most N of the functions cos(2πωi · x) can be linearly independent when evaluated at the data.
Thus, in order to distinguish even low frequencies the number of datapoints must be exponential in
the dimension.
Instead of considering Fourier modes in this situation, we follow the approach laid out in [30] to
deﬁne a useful notion of frequency on sparse high-dimensional data. Speciﬁcally, our notion of
frequency is given by the eigenfunctions of the following kernel map. Let X = L2(D) be the space
of functions deﬁned on the dataset D. Deﬁne the kernel map K : X →X by
K(f)(y) =
X
z∈D
k(y, z)f(z)
(3)
for a suitable positive deﬁnite kernel k : D × D →R. The Gaussian RBF kernel k(y, z) = e−s|y−z|2
is a natural choice. The eigenfunctions φi of the kernel map K resemble sinusoids and can be thought
of as a proxy for frequency when the data is sparse in high dimensions [30, 11]. This is due to the
3

fact that eigenfunctions corresponding to small eigenvalues decay rapidly when averaging using the
kernel, similar to the high frequency functions considering in the previous two examples. Thus this
notion gives a reasonable generalization of the notion of frequency on an arbitrary set of datapoints,
which is needed to formulate the spectral bias on high-dimensional real data.
3.2
Spectral Bias
The spectral bias of neural networks refers to the observation [30] that during training a neural network
ﬁts the low frequency components of a target faster than the high-frequency components. Speciﬁcally,
consider a basis of eigenfunctions φ1, φ2, ... corresponding to the eigenvalues µ1 ≥µ2 ≥· · · of a
suitable compact self-adjoint operator T on L2(dµ). Note that since the operator T is self-adjoint the
eigenfunctions φi can be taken to be orthonormal. If we let f ℓdenote the neural network function at
training iteration ℓand f ∗the true empirical minimizer, then expanding the difference in terms of the
eigenfunctions φi, we get
f ℓ−f ∗=
X
i
αℓ
iφi.
(4)
The spectral bias is the statement that αℓ
i decays more rapidly in ℓfor smaller frequencies i. The
spectral bias phenomenon has been observed experimentally for deep ReLU networks when the φi
are Fourier modes [9, 30] and when the φi are the eigenfunctions of a Gaussian RBF kernel [30].
This has been proposed as an explanation for the implicit regularization of neural networks [30],
since it biases neural networks toward smooth functions in a manner similar to using the T −1-norm
as a regularizer.
We remark that here the larger eigenvalues µi correspond to lower frequencies, which is due to our
deﬁnition in terms of the compact operator T. In some cases, T will be (formally) given by T = e−S
for an operator S with eigenfunctions φi and eigenvalues λi = e−µi. In this case, smaller eigenvalues
λi correspond to lower frequencies. A typical example of this is to take T to be the heat kernel and
S to be the Laplacian. The advantage of using the operator T instead of S is that S is usually an
unbounded operator and thus requires additional technical machinery to interpret correctly.
4
Spectral Analysis of Shallow Neural Networks
4.1
Preliminaries
Throughout this section we consider shallow neural network functions f : Rd →R deﬁned by a
shallow neural network of the form
f(x) =
n
X
i=1
aiσ(ωi · x + bi),
(5)
where ωi ∈Rd and bi ∈R. The activation functions we will consider are the ReLU activation
function σ(p) := ReLU(p) = max(0, p) and the Hat activation deﬁned by
σH(p) := Hat(p) =



0
p < 0 or p ≥2
p
0 ≤p < 1
2 −p
1 ≤p < 2.
(6)
The Hat activation function is not scale invariant. As a result, we will also consider more generally a
scaled Hat function σH(α·) as an activation function. It is known that shallow neural networks with
either the ReLU or Hat activation function can approximate arbitrary continuous functions [17]. This
universal approximation property partially explains the success of neural networks. We analyze the
spectral bias of shallow neural networks by leveraging its connections with ﬁnite element methods.
4.2
Spectral Bias in 1D for ReLU and Hat Networks
We consider ﬁtting a one dimensional function u : [0, 1] →R with a shallow ReLU neural network.
For the theoretical analysis, we consider the simpliﬁed situation where the inner weights are ﬁxed
and the network is given by
fNN(x,⃗a) =
n
X
i=1
aiσ

x −i
n

.
(7)
4

Here only the weights ai are learned and the ωi = 1 and bi = i/n are ﬁxed. We consider minimizing
the following loss function using gradient descent
L(⃗a) = 1
2
Z 1
0
(u(x) −fNN(x,⃗a))2dx.
(8)
Lemma 1, which is well-known in the theory of ﬁnite elements [10], describes the structure of the
loss function L (we refer to the appendix for proofs and technical details).
Lemma 1. The loss function L takes the form
L(⃗a) = ⃗aT Mσ⃗a −bT
u,σ⃗a,
(9)
where the components of bu,σ are given by
(bu,σ)i =
Z 1
0
u(x)σ

x −i
n

dx,
(10)
and the mass matrix Mσ is given by
(Mσ)ij =
Z 1
0
σ

x −i
n

σ

x −j
n

dx.
(11)
Further, the matrix M is positive deﬁnite.
The eigenvalues of the mass matrix Mσ play a key role in explaining the spectral bias. Theorems 1
and 2 describe the eigenstructure of Mσ for the ReLU activation function and for the Hat activation
function, respectively. The detailed proof of Theorem 1 can be found in Appendix A.4 and A.3.
Theorem 2 can be obtained in a straightforward manner from Theorem 5 in Appendix A.3.
Theorem 1. Let σ be the ReLU activation function and let λ1 ≤· · · ≤λn denote the eigenvalues of
Mσ. Then we have
λn
λj
∼n4
j4 .
(12)
Theorem 2. Let σ(x) = σH(nx) be a Hat activation function scaled to match a ﬁnite element
discretization of [0, 1] and let λ1 ≤· · · ≤λn denote the eigenvalues of Mσ. Then we have
λn
λj
= O(1).
(13)
Now consider training the weights ⃗a using gradient descent for the objective L. To guarantee
convergence of gradient descent the learning rate η will be taken as η ≤λ−1
n . Theorem 3, which
can be obtained by writing out the details of the gradient descent method, describes the evolution of
spectrum of the error.
Theorem 3. Consider training the parameters ⃗a using gradient descent on the loss function L with
step-size s = λ−1
n . Here λ1 ≤· · · ≤λn are the eigenvalues of the matrix Mσ. Let ⃗ψ1, ..., ⃗ψn denote
the corresponding eigenvectors and deﬁne functions φj by
φj(x) =
n
X
i=1
ψj
i σ

x −i
n

.
(14)
Let ⃗a1, ...,⃗ak denote the iterates generated by gradient descent. Consider the expansion
u(x) −fNN(x,⃗aℓ) =
n
X
i=1
αℓ
iφi(x).
(15)
Then the coefﬁcients αℓ
i satisfy
αℓ
i = αℓ
i

1 −λi
λn
ℓ
.
(16)
We remark that the functions φj appearing in Theorem 3 are orthogonal. This follows since from the
deﬁnition of Mσ, we see that
⟨φj, φk⟩L2([0,1]) = (ψj)T Mσ(ψk) = λk(ψj)T (ψk) = 0,
(17)
where the ﬁnal equality follows since ψj and ψk are eigenfunctions of the symmetric matrix Mσ.
5

4.3
Eigenfunction Analysis for ReLU Networks
Theorem 3 shows that the components of the error corresponding to the large eigenfunction of Mσ
decay fastest. The following Theorem 4, which can be obtained from Theorem 7 in Appendix A.5,
gives the structure of these eigenfunction when σ is the ReLU activation function.
Theorem 4. Consider the matrix Mσ for σ to ReLU activation function and let φj(x) be as in
Theorem 3. Then we have
R 1
0 φ′
j(x)2dx
R 1
0 φj(x)2dx
∼n4
j4 .
(18)
This theorem shows that for the ReLU activation function the eigenfunctions of Mσ with large
eigenvalue consists of smooth functions, while the eigenfunctions corresponding to small eigenvalues
consist of highly oscillatory, high-frequency functions. This can be seen clearly in Figure 1, where
we plot both the large and small eigenfunctions φ corresponding to the ReLU activation function.
0
20
40
60
80
100
120
140
-0.18
-0.16
-0.14
-0.12
-0.1
-0.08
-0.06
-0.04
-0.02
0
0
20
40
60
80
100
120
140
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0
20
40
60
80
100
120
140
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0
20
40
60
80
100
120
140
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
Figure 1: Eigenfunction of Mσ when σ is a ReLU. Eigenfunctions with the two largest eigenvalues
on the left. Eigenfunction with the two smallest eigenvalues on the right.
4.4
Discussion
Theorem 3 shows that the eigencomponents of the error decay faster for φi corresponding to large i.
Moreover, the difference in the convergence rate of different eigencomponents depends upon the ratio
λn
λ1 . Theorem 1 shows that if the activation function is taken to be the ReLU, then this ratio grows
rapidly with the width and the components of the error corresponding to large eigenfunctions decay
much faster than the components for small eigenfunction. Further, Theorem 4 shows that the large
eigenfunction are smooth functions, while the small eigenfunctions are highly oscillatory functions.
This means that the high frequency components of the solution are learned much more slowly than
the low frequency components. We argue that this is basis behind the spectral bias observed in [30].
On the other hand, Theorem 2 shows that when σ is a scaled Hat activation function, then the ratio
λn
λ1 remains bounded as the width n increases. In this case the different eigencomponents of the error
decay at roughly the same rate. For this reason, we expect that neural networks with Hat activation
function will not exhibit the spectral bias observed in [30].
Finally, we remark that other activation functions can be handled in a similar manner by analyzing
the mass matrix Mσ. For instance, a sinusoidal activation function results in a singular matrix (since
the set {sin(x + t), t ∈R} lies in a two-dimensional linear space) and we have also experimentally
observed that such networks also exhibit a spectral bias (see the appendix). Thus, new ideas are
required to explain the recent success of sinusoidal representation networks [32].
5
Experiments for Shallow Neural Networks
In this section, we report the result of two numerical experiments in which we use shallow neural
networks to ﬁt target functions in Rd(d = 1, 2). In all experiments in this section, we minimize the
mean square error (MSE) using the Adam optimizer [22]. We consider three different activation
functions
σ(p) =



tanh(p);
ReLU(p);
Hat(p).
(19)
Based upon the results of the following two experiments, we observe that:
6

Conclusion 1. The spectral bias holds for both tanh and ReLU shallow neural networks, but it does
not hold for Hat shallow neural networks.
Experiment 1. We use shallow neural networks with each of the activation functions in (19) to ﬁt
the following target function u on [−π, π]:
u(x) = sin(x) + sin(3x) + sin(5x).
(20)
For each activation function our network has one hidden layer with size 8000. The mean square error
(MSE) loss is given by
L(f, u) = 1
N
N
X
i=1
(f(xi) −u(xi))2,
(21)
where xi is a uniform grid of size N = 201 on [−π, π]. The three networks are trained using ADAM
with a learning rate of 0.0002. When using a tanh or ReLU activation function, all parameters
are initialized following a Gaussian distribution with mean 0 and standard deviation 0.1, while the
network with Hat activation function is initialized following a Gaussian distribution with mean 0 and
standard deviation 0.8.
Denote
∆f,u(k) =
 ˆfk −ˆuk
 / |ˆuk| ,
(22)
where k represents the frequency, |·| represents the norm of a complex number and ˆ· represents
discrete Fourier transform. We plot ∆f,u(1), ∆f,u(3), and ∆f,u(5) in Figure 2 for each of the three
networks. From these results, we observe the spectral bias for tanh [35] and ReLU neural networks
0
500
1000
1500
2000
2500
3000
epoch
10
3
10
2
10
1
100
error
Training Error
Lower
Medium
Higher
0
500
1000
1500
2000
2500
3000
epoch
10
3
10
2
10
1
100
error
Training Error
0
500
1000
1500
2000
2500
3000
epoch
10
7
10
5
10
3
10
1
101
error
Training Error
Figure 2: σ(p) = tanh(p) (left), σ(p) = ReLU(p) (middle), σ(p) = Hat(p) (right)
(see left and middle of Figure 2), while there is no spectral bias for the Hat neural network as shown
in right of Figure 2.
Experiment 2. Next, we ﬁt the target function u on [0, 1]2 given by:
u(x) = sin(2πx1) sin(2πx2) + sin(10πx1) sin(10πx2).
Here x = (x1, x2) ∈[0, 1]2. For this experiment, we use shallow neural networks with the ReLU
σ(p) = ReLU(p) and with a scaled Hat function σ(p) = Hat(100p) as activation function. Both
models have width 10000 and all parameters are initialized using the default initialization in Pytorch.
We use the ADAM optimizer for both experiments. The learning rate for the ReLU neural network is
initialized to 0.00005 and and the learning rate for the Hat neural network is initialized to 0.00075.
Both learning rates are decayed by half every 1000 epochs.
0
1000
2000
3000
4000
5000
epoch
10
2
10
1
100
error
relative frequency error
1
5
0
1000
2000
3000
4000
5000
epoch
10
4
10
3
10
2
10
1
100
error
relative frequency error
1
5
Figure 3: σ(p) = ReLU(p) (left), σ(p) = Hat(100p) (right)
To observe the spectral bias, we look at a slice of the target and network functions, deﬁned by setting
x2 = 31/128. For this one dimensional function, we plot ∆f,g(1) and ∆f,g(5) for both neural
network models with ReLU and Hat activation functions. Here ∆f,g is deﬁned in the same way as in
equation (35).
7

6
Experiments for Deep Neural Networks
6.1
Experiments with Synthetic Data
Experiment 3. The experimental setup here is an extension of the experiment presented in [30]. The
target function is:
u(x) =
10
X
k=1
sin (10πkx + ck),
(23)
where ck, i = 1, 2, · · · , 10 are sampled from the uniform distribution U(0, 2π). We ﬁt this target
function using a squared error loss sampled at 200 equally spaced points in [0, 1] using a neural
network with 6 layers. Three networks are trained, one with a ReLU activation function and two with
the Hat activation function. We train the models with Adam using the following hyperparameters:
• ReLU activation function with 256 units per layer: all parameters are initialized from a
Gaussian distribution N(0, 0.04). The learning rate is 0.001 and decreased by half every
10000 epochs. This corresponds to the experiment in [30].
• Hat activation function with 256 units per layer: all parameters are initialized from the
uniform distribution U(−1.0, 1.0). The learning rate is 0.00001 and is decreased by half
every 250 epochs.
• Hat activation function with 128 units per layer: all parameters are initialized from the
uniform distribution U(−2.0, 2.0). The learning rate is 0.0001 and decreased by half every
200 epochs.
0
10000 20000 30000 40000 50000 60000 70000 80000
epoch
10
3
10
2
10
1
100
101
102
error
MSE Loss
0
100
200
300
400
500
600
700
800
epoch
10
22
10
18
10
14
10
10
10
6
10
2
102
MSE
MSE Loss
0
100
200
300
400
500
600
700
800
epoch
10
22
10
18
10
14
10
10
10
6
10
2
102
MSE
MSE Loss
0
10000 20000 30000 40000 50000 60000 70000 80000
epoch
10
6
10
5
10
4
10
3
10
2
10
1
100
error
Training Error for Each Frequency
5
10
15
20
25
30
35
40
45
50
0
100
200
300
400
500
600
700
800
epoch
10
14
10
12
10
10
10
8
10
6
10
4
10
2
100
error
Training Error for Each Frequency
5
10
15
20
25
30
35
40
45
50
0
100
200
300
400
500
600
700
800
epoch
10
14
10
12
10
10
10
8
10
6
10
4
10
2
100
error
Training Error for Each Frequency
5
10
15
20
25
30
35
40
45
50
Figure 4: ReLU, width 256 [30] (left), Hat activation, width 256 (middle), Hat activation, width 128
(right).
From Figure 4, we see that the spectral bias is signiﬁcant for ReLU deep neural networks (DNN).
Some frequencies, for example 50, will not even converge below an error of 10−2. Changing the
activation function to a linear Hat function removes the spectral bias and the loss decreases rapidly
for all frequencies. In fact, the training loss for ReLU-DNN is only about 10−3 after 80000 epochs,
while the training loss for Hat neural networks are are already 10−22 after only 800 epochs. As shown
in Figure 5, Hat neural networks ﬁt the target function much better and faster than ReLU networks.
Experiment 4. In this experiment, we consider ﬁtting a grayscale image using deep neural networks.
We view the grayscale image on the left of Figure 6 as two dimensional function and ﬁt this function
using a deep neural network with hidden layers of size 2-4000-500-400-1. Our loss function is the
squared error loss. We consider using both the ReLU and Hat neural networks and compare their
performance. For both networks, we initialize all parameters from a normal distribution with standard
deviation 0.01. For the Hat network, the learning rate is 0.0005 and is reduced by half every 1000
epochs, while the learning rate for ReLU model is 0.0005 and reduced by half every 4000 epochs.
From Figure 6, we can see that the image is ﬁt much better using the Hat network, which is able to
capture the high frequencies in the image, while the ReLU network blurs the image due to its inability
to capture high frequencies.
8

0.0
0.2
0.4
0.6
0.8
1.0
4
2
0
2
4
Target and DNN Output
 with 1000 Epochs
NN
Target
0.0
0.2
0.4
0.6
0.8
1.0
4
2
0
2
4
Target and DNN Output
 with 10000 Epochs
NN
Target
0.0
0.2
0.4
0.6
0.8
1.0
4
2
0
2
4
Target and DNN Output
 with 80000 Epochs
NN
Target
0.0
0.2
0.4
0.6
0.8
1.0
8
6
4
2
0
2
4
6
Target and NN Output
 with 10 Epochs
NN
Target
0.0
0.2
0.4
0.6
0.8
1.0
4
2
0
2
4
Target and NN Output
 with 100 Epochs
NN
Target
0.0
0.2
0.4
0.6
0.8
1.0
4
2
0
2
4
Target and NN Output
 with 800 Epochs
NN
Target
Figure 5: ReLU, width 256 [30] (Top), Hat activation, width 256(Bottom).
0
20
40
60
80
100
120
0
20
40
60
80
100
120
0
20
40
60
80
100
120
0
20
40
60
80
100
120
0
20
40
60
80
100
120
0
20
40
60
80
100
120
Figure 6: Left: Original Image, Middle: Image ﬁt using deep ReLU network, Right: Image ﬁt using
deep Hat network.
6.2
Experiments with Real Data
Next, we test the spectral bias of neural networks on real data, speciﬁcally on the MNIST dataset.
Since the data lives in a very high dimension relative to the number of samples, we argue that Fourier
modes are not a suitable notion of frequency. To get around this issue, we consider the eigenfunctions
of a Gaussian RBF kernel [5]. We largely follow the experimental setup presented in [30], with the
notable difference that we compare neural networks with both a ReLU and Hat activation function
(6).
Speciﬁcally, we choose two digits a and b and consider ﬁtting the classiﬁcation function
u(x) =
0
x is the digit a
1
x is the digit b
(24)
via least squares on 2000 training images from MNIST. Following [30], we add a moderate amount
of high-frequency noise and plot the spectrum of the target function and the training iterates in Figure
7. We see that even with this generalized notion of frequency, the network with a Hat activation
function is able to ﬁt the higher frequencies much faster than a ReLU network.
7
Conclusion
We have provided a theoretical explanation for the spectral bias of ReLU networks and shown that
using the Hat activation function will remove this spectral bias, which we also conﬁrmed empirically.
Further research directions include studying the spectral bias for a wider variety of activation functions.
In addition, we view deepening the connections between ﬁnite element methods and neural networks
as a promising research focus.
9

0
20
40
60
80
100
Eigenfunction Idx [/20]
0.0
0.5
1.0
1.5
2.0
Absolute Coefficient
Iteration 0
Iteration 40
Iteration 200
Iteration 300
Iteration 400
Iteration 600
Noised Target
Pure Target
0
20
40
60
80
100
Eigenfunction Idx [/20]
0.0
0.5
1.0
1.5
2.0
Absolute Coefficient
Iteration 0
Iteration 40
Iteration 200
Iteration 300
Iteration 400
Iteration 600
Noised Target
Pure Target
Figure 7: Real Data Experiments on MNIST using Gaussian RBF kernel eigenfunctions. We can
clearly see that the error components in each eigenfunction have all converged by iteration 200 when
using the Hat activation function (left), while the higher frequency components converge much more
slowly when using the ReLU activation function (right).
References
[1] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,
Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al.
A closer look at memorization in deep networks. In International Conference on Machine
Learning, pages 233–242. PMLR, 2017.
[2] Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman.
Frequency bias in neural networks for input of non-uniform density. In International Conference
on Machine Learning, pages 685–694. PMLR, 2020.
[3] Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability
and statistics. Springer Science & Business Media, 2011.
[4] Simon Biland, Vinicius C Azevedo, Byungsoo Kim, and Barbara Solenthaler. Frequency-aware
reconstruction of ﬂuid simulations with generative networks. EUROGRAPHICS 2020, 2020.
[5] Mikio L Braun, Tilman Lange, and Joachim M Buhmann. Model selection in kernel methods
based on a spectral analysis of label information. In Joint Pattern Recognition Symposium,
pages 344–353. Springer, 2006.
[6] Wei Cai, Xiaoguang Li, and Lizuo Liu. A phase shift deep neural network for high frequency
approximation and wave problems. SIAM Journal on Scientiﬁc Computing, 42(5):A3285–
A3312, 2020.
[7] Wei Cai and Zhi-Qin John Xu. Multi-scale deep neural networks for solving high dimensional
pdes. arXiv preprint arXiv:1910.11710, 2019.
[8] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model
alignment explain generalization in kernel regression and inﬁnitely wide neural networks.
Nature communications, 12(1):1–12, 2021.
[9] Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding
the spectral bias of deep learning. In Proceedings of the Thirtieth International Joint Conference
on Artiﬁcial Intelligence, 2021.
[10] Philippe G Ciarlet. The ﬁnite element method for elliptic problems. SIAM, 2002.
[11] Gregory E Fasshauer. Positive deﬁnite kernels: past, present and future. Dolomites Research
Notes on Approximation, 4:21–63, 2011.
[12] Sara Fridovich-Keil, Raphael Gontijo-Lopes, and Rebecca Roelofs. Spectral bias in practice:
The role of function frequency in generalization. arXiv preprint arXiv:2110.02424, 2021.
[13] William Fulton. Eigenvalues, invariant factors, highest weights, and schubert calculus. Bulletin
of the American Mathematical Society, 37(3):209–249, 2000.
[14] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential
equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505–
8510, 2018.
[15] Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng. Relu deep neural networks and linear ﬁnite
elements. Journal of Computational Mathematics, 38(3):502–527, 2020.
10

[16] Thomas Hofmann, Bernhard Schölkopf, and Alexander J Smola. Kernel methods in machine
learning. The annals of statistics, 36(3):1171–1220, 2008.
[17] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359–366, 1989.
[18] Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the
early-time learning dynamics of neural networks. arXiv preprint arXiv:2006.14599, 2020.
[19] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems, 31,
2018.
[20] Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Pathological spectra of the ﬁsher in-
formation metric and its variants in deep neural networks. arXiv preprint arXiv:1910.05992,
2019.
[21] Mahyar Khayatkhoei and Ahmed Elgammal. Spatial frequency bias in convolutional generative
adversarial networks. arXiv preprint arXiv:2010.01473, 2020.
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[23] Dmitry Kopitkov and Vadim Indelman. Neural spectrum alignment: Empirical study. In
International Conference on Artiﬁcial Neural Networks, pages 168–179. Springer, 2020.
[24] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping
is provably robust to label noise for overparameterized neural networks. In International
conference on artiﬁcial intelligence and statistics, pages 4313–4324. PMLR, 2020.
[25] Tao Luo, Zheng Ma, Zhi-Qin John Xu, and Yaoyu Zhang. On the exact computation of linear
frequency principle dynamics and its generalization. arXiv preprint arXiv:2010.08153, 2020.
[26] Yuheng Ma, Zhi-Qin John Xu, and Jiwei Zhang. Frequency principle in deep learning beyond
gradient-descent-based training. arXiv preprint arXiv:2101.00747, 2021.
[27] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.
In Icml, 2010.
[28] Xiaobing Nie, Jinde Cao, and Shumin Fei. Multistability and instability of competitive neural
networks with mexican-hat-type activation functions. In Abstract and Applied Analysis, volume
2014. Hindawi, 2014.
[29] Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier
Boix, Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: the non-overﬁtting
puzzle. CBMM Memo. 073, 2018.
[30] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht,
Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In International
Conference on Machine Learning, pages 5301–5310. PMLR, 2019.
[31] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational physics, 378:686–707, 2019.
[32] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-
plicit neural representations with periodic activation functions. Advances in Neural Information
Processing Systems, 33, 2020.
[33] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The
implicit bias of gradient descent on separable data. The Journal of Machine Learning Research,
19(1):2822–2878, 2018.
[34] Jindong Wang, Jinchao Xu, and Jianqing Zhu. Cnns with compact activation function. In
International Conference on Computational Science, pages 319–327. Springer, 2022.
[35] Z.-Q. J. Xu, Y. Zhang, T. Luo, Y. Xiao, and Z. Ma. Frequency principle: Fourier analysis sheds
light on deep neural networks. Commun. Comput. Phys., 28:1745–1767, 2019.
[36] Zhi-Qin John Xu, Yaoyu Zhang, and Tao Luo. Overview frequency principle/spectral bias in
deep learning. arXiv preprint arXiv:2201.07395, 2022.
11

[37] Zhiqin John Xu. Understanding training and generalization in deep learning by fourier analysis.
arXiv preprint arXiv:1808.04295, 2018.
[38] Greg Yang and Hadi Salman. A ﬁne-grained spectral perspective on neural networks. arXiv
preprint arXiv:1907.10599, 2019.
[39] Wen-Chyuan Yueh. Eigenvalues of several tridiagonal matrices. Applied Mathematics E-Notes
[electronic only], 5:66–74, 2005.
[40] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
115, 2021.
[41] Xiao Zhang, Haoyi Xiong, and Dongrui Wu. Rethink the connections among generalization,
memorization and the spectral bias of dnns. In Proceedings of the Thirtieth International Joint
Conference on Artiﬁcial Intelligence, 2021.
A
Appendix
A.1
Additional experiments
In this subsection, we ﬁrst report several additional numerical experiments using shallow neural
networks to ﬁt target functions in Rd(d = 2, 3) showing that the spectral bias holds for ReLU shallow
neural networks, but it does not hold for Hat neural networks. In addition, we report a comparison
numerical experiment between the hat neural network and the neural network with sin activation
function and a comparison numerical experiment between hat neural network and ReLU neural
network with accurate approximation to the loss function. Also we add an experiment with fewer
neurons by rerunning the Experiments 1 and 2. Then we add an experiment using SGD method as
optimizer. Finally we add an experiment showing that when the shallow ReLU network gets wider,
the spectral bias gets stronger.
Experiment 5. In this experiment, we investigate how the validation performance depends on the
frequency of noise added to the training target in two dimension case. We consider the ground target
function on [0, 1]2
u0(x) = sin(2πx1) sin(2πx2).
(25)
Let ψk(x) be the noise function
ψk(x) = 0.2 sin(2kπx1) sin(2kπx2),
where k is the frequency of the noise. The ﬁnal target function u(x) is then given by u(x) =
u0(x) + ψk(x). We use two different shallow neural network models the same as the two used
in Experiment 2. The MSE loss function is computed by 4000 sampling points from the uniform
distribution U([0, 1]2).
The validation error is computed by
L2(f, u0) = 1
m


m
X
i,j=1
 f(x1,i, x2,j) −u0(x1,i, x2,j)
2


1
2
,
on a uniform grid points of [0, 1]2 with m = 27. Both models are trained with a learning rate of
0.001 and decreasing to its 3
4 for each 250 epochs. All parameters are initialized following a uniform
distribution U(−0.3, 0.3).
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
ReLU Test error history with 4000, k=2 
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
ReLU Test error history with 4000, k=3
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
ReLU Test error history with 4000, k=4
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
ReLU Test error history with 4000, k=5
Figure 8: Validation error history for σ(p) = ReLU(p) with k = 2, 3, 4, 5.
12

0
2000
4000
6000
8000
10000
epoch
10
1
100
error
Test error history with 4000, k=2
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
Test error history with 4000, k=3
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
Test error history with 4000, k=4
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
Test error history with 4000, k=5
Figure 9: Validation error history for σ(p) = Hat(100p) with k = 2, 3, 4, 5.
From the Figure 8, we can see that the proﬁle of the loss curves varies signiﬁcantly with the frequency
of noise added to the target when ReLU neural network is used. This is explained by the fact that the
ReLU neural network readily ﬁts the noise signal if it is low frequency, whereas the higher frequency
noise is only ﬁt later in the training. In the latter case, the dip in validation score early in the training
is when the network has learned the low frequency true target function u0(x); the remainder of the
training is spent learning the higher-frequencies in the training target u(x). When the frequency is
higher, the ReLU neural network ﬁts the target function slower indicated by the loss curves. From the
Figure 9, we can see that the proﬁle of the loss curves are very much the same with respect to the
frequency of noise added to the target when Hat neural network is used. This is explained by the fact
that Hat neural network does not have frequency bias.
Experiment 6. In this experiment, we investigate how the validation performance depends on the
frequency of noise added to the training target in three dimension case. We consider the target
function
u0(x) = sin(2πx1) sin(2πx2) sin(2πx3),
(26)
where x = (x1, x2, x3) ∈[0, 1]3. Let ψk(x) be the noise function
ψk(x) = 0.5 sin(2kπx1) sin(2kπx2) sin(2kπx3),
where k is the frequency of the noise. The ﬁnal target function u is then given by u(x) = u0(x) +
ψk(x).
We use shallow neural network models with two different activation functions to ﬁt u(x). One is
σ(p) = ReLU(p), and the other one is the scaled Hat function σ(p) = Hat(100p). Both two models
have only one hidden layer with size 3-30000-1.
The training MSE loss function is computed by 100000 sampling points from the uniform distribution
U([0, 1]2).
The validation error is computed by
L2(f, u) =
 
1
m
m
X
i=1
(f(xi) −u0(xi))2
! 1
2
,
(27)
where {xi}m
i=1, m = 100000 are sampling points from the uniform distribution U([0, 1]3). The
ReLU neural network are trained by Adam optimizer with learning rate of 0.001 and decreasing
to its 0.85 for each 300 epochs. The Hat neural network are trained with learning rate of 0.001
and decreasing to its 0.75 for each 250 epochs. All parameters are initialized following a uniform
distribution U(−0.3, 0.3).
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
     k=2
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
    k=3
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
k=4
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
    k=5
Figure 10: Validation error history for σ(p) = ReLU(p) with k = 2, 3, 4, 5.
From the Figure 10 and Figure 11, we can see that the numerical results are similar to the two
dimension case as shown in Experiment 5.
13

0
2000
4000
6000
8000
10000
epoch
100
error
 k=2
0
2000
4000
6000
8000
10000
epoch
100
error
 k=3
0
2000
4000
6000
8000
10000
epoch
100
error
  k=4
0
2000
4000
6000
8000
10000
epoch
100
error
   k=5
Figure 11: Validation error history for σ(p) = Hat(100p) with k = 2, 3, 4, 5.
Experiment 7. In this experiment, we investigate how the validation performance depends on the
frequency of noise added to the training target in three dimension case. We consider the ground target
function
u0(x) = sin(2πx1) sin(2πx2) sin(2πx3).
(28)
Let ψ1,k(x) and ψ2,k(x) be the noise functions
ψ1,k(x) = 0.5 sin(2πk∥x∥), ψ2,k(x) = 0.5 sin(2πk∥x∥)
∥x∥
,
where k is the frequency of the noise. The ﬁnal target function u(x) is then given by u(x) =
u0(x) + ψj,k(x), j = 1, 2.
We use models
f(x) = W2σ (W1x + b1)
with two different activation functions to ﬁt u(x). One is σ(p) = ReLU(p), and the other one is
the scaled Hat function σ(p) = Hat(100p). Both two models have only one hidden layer with size
3-30000-1. The training error is computed by
L1(f, u) =
 
1
N
N
X
i=1
(f(xi) −u(xi))2
! 1
2
,
(29)
where {xi}n
i=1, N = 100000 are sampling points from the uniform distribution U([0, 1]3).
The validation error is computed by
L2(f, u) =
 
1
m
m
X
i=1
(f(xi) −u0(xi))2
! 1
2
,
(30)
where {xi}m
i=1, m = 100000 are sampling points from the uniform distribution U([0, 1]3). Both
models are trained by Adam optimizer with a learning rate of 0.001 and decreasing to its 0.85 for
each 300 epochs. All parameters are initialized following a uniform distribution U(−0.3, 0.3).
0
2000
4000
6000
8000
10000
epoch
100
error
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
0
2000
4000
6000
8000
10000
epoch
10
1
100
101
error
Figure 12: Validation error history for σ(p) = ReLU(p) with ψ1,k(x) and k = 2, 3, 4, 5.
From the Figure 12, Figure 13, Figure 14 and Figure 15, we can see that the numerical results are
similar to the two dimension case and the three dimension case in Experiment 6.
Experiment 8. We consider to ﬁt the target function [6]
u(x) =

10(sin(x) + sin(3x)), x ∈[−π, 0];
10(sin(23x) + sin(137x) + sin(203x)), x ∈[0, π].
14

0
2000
4000
6000
8000
10000
epoch
100
4 × 10
1
6 × 10
1
2 × 100
0
2000
4000
6000
8000
10000
epoch
100
4 × 10
1
6 × 10
1
2 × 100
0
2000
4000
6000
8000
10000
epoch
100
4 × 10
1
6 × 10
1
2 × 100
0
2000
4000
6000
8000
10000
epoch
100
4 × 10
1
6 × 10
1
2 × 100
Figure 13: Validation error history for σ(p) = Hat(100p) with ψ1,k(x) and k = 2, 3, 4, 5.
0
2000
4000
6000
8000
10000
epoch
100
3 × 10
1
4 × 10
1
6 × 10
1
0
2000
4000
6000
8000
10000
epoch
100
error
0
2000
4000
6000
8000
10000
epoch
10
1
100
error
0
2000
4000
6000
8000
10000
epoch
10
1
100
101
error
Figure 14: Validation error history for σ(p) = ReLU(p) with ψ2,k(x) and k = 2, 3, 4, 5.
0
2000
4000
6000
8000
10000
epoch
100
6 × 10
1
2 × 100
0
2000
4000
6000
8000
10000
epoch
100
6 × 10
1
2 × 100
0
2000
4000
6000
8000
10000
epoch
100
6 × 10
1
2 × 100
0
2000
4000
6000
8000
10000
epoch
100
6 × 10
1
2 × 100
Figure 15: Validation error history for σ(p) = Hat(100p) with ψ2,k(x) and k = 2, 3, 4, 5.
• ReLU-DNN with phase shift by Cai et al. in [6]: 16 models of ReLU-DNN which have
the size of 1-40-40-40-40-1 and Fourier transform are used to train different frequency
components of the target function. Training data are the evenly mesh with 1000 grids from
−π to π. They train the mean square loss with Adam and the learning rate is 0.002.
• Our method: We use only one model with one hidden layer and width of 25000. In our
model, the activation function is σ(p) = hat(200p). Learning rate is 0.001 and decrease to
its half every 250 epochs. The weights of outer layers are sampled following the uniform
distribution U(−7, 7), and all the other parameters are sampled following the uniform
distribution U(−0.95, 0.95). Training data are the evenly mesh with 1000 grids from −π to
π. We train the mean square loss with Adam.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
Target and NN Output
 with 1000 Epochs.pdf
Target
DNN
3
2
1
0
1
2
3
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
Difference between Target and NN Output
 with 1000 Epochs
3
2
1
0
1
2
3
30
20
10
0
10
20
30
Target and NN Output
 with 1000 Epochs
Target
NN
3
2
1
0
1
2
3
6
4
2
0
2
4
6
8
1e
13
Difference between Target and NN Output
 with 1000 Epochs
Figure 16: ReLU-DNN with Phase Shift in [6] (left two), Our method (right two).
The model of our method has almost the same amount of parameters in the method of the ReLU-DNN
with phase shift in [6] which includes 16 models. From the Figure 16, after 1000 epochs for both
methods, we can see that the difference between the output neural network function obtained by
method of the ReLU-DNN with phase shift in [6] and the target function u(x) is around 10−1, while
the difference between the output neural network function obtained by our method and the target
function u(x) is around 10−13.
Experiment 9. We consider ﬁtting the target function on [0, 1]:
u(x) = sin(2πx) + sin(6πx) + sin(10πx).
15

We deﬁne the loss L(f, u) =
R 1
0 |f(x) −u(x)|2 dx and evaluate this integral with accurate approx-
imation. We use one-hidden-layer network with width of 128. The activation functions are ReLU
and hat(10x). Both models are trained with a learning rate of 0.0005. For the ReLU network
f(x) =
n
X
i=1
aiσ(ωix + bi) with σ(p) = ReLU(p), we initialize ωi’s as constant 1, and initialize ai’s
and bi’s following a uniform distribution on [−1, 1]. And all parameters are initialized following a
uniform distribution on [−1, 1] for hat neural network. Denote ∆f,u(k) =
 ˆfk −ˆuk
 / |ˆuk|, where
k represents the frequency, |·| represents the norm of a complex number and ˆ· represents Fourier
transform. We also can evaluate the accurate Fourier transform for frequencies k = 1, 3, 5. From the
Fourier transform of u(x), we select ∆f,u(1), ∆f,u(3), and ∆f,u(5) to observe the convergent rate.
And we apply the same thing to f(x). Figure 17 shows the convergent process of each frequency
0
200
400
600
800
1000
epoch
100
2 × 10
1
3 × 10
1
4 × 10
1
6 × 10
1
Error for Each Frequency
1
3
5
0
200
400
600
800
1000
epoch
10
1
100
error
Error for Each Frequency
1
3
5
Figure 17: σ(p) = ReLU(p) (left), σ(p) = Hat(10p) (right)
component of ReLU neural network and Hat neural network. Green line, yellow line, and blue
line denote the convergent process of ∆f,u(1), ∆f,u(3), and ∆f,u(5) respectively. Spectral bias is
obviously observed for ReLU neural network (see left of Figure 17), while the spectral bias does not
hold for Hat neural network as shown in right of Figure 17.
Experiment 10. We consider ﬁtting the target function on [−π, π]:
u(x) = sin(x) + sin(3x) + sin(5x).
(31)
We use shallow neural network models with activation function σ(p) = sin(p) and σ(p) = hat(p).
Both models have only one hidden layer with size 8000. The mean square error (MSE) function is
deﬁned as
L(f, u) = 1
N
N
X
i=1
(f(xi) −u(xi))2,
(32)
where xi is the uniform grid of size N = 201 from [−π, π]. Both models are trained with a
learning rate of 0.0002. And all parameters are initialized following a Gaussian distribution with
mean 0 and standard deviation 0.8 for both sin neural network and hat neural network. Denote
∆f,u(k) =
 ˆfk −ˆuk
 / |ˆuk|, where k represents the frequency, |·| represents the norm of a complex
number andˆ· represents discrete Fourier transform. From the Fourier transform of u(x), we select
∆f,u(1), ∆f,u(3), and ∆f,u(5) to observe the convergent rate. And we apply the same thing to f(x).
Figure 19 shows the convergent process of each frequency component of sin neural network and Hat
0
2
4
6
8
10
4
10
3
10
2
10
1
100
101
FFT of u
Figure 18: Fourier transform of u
0
500
1000
1500
2000
2500
3000
epoch
10
4
10
3
10
2
10
1
100
101
error
Training Error
0
500
1000
1500
2000
2500
3000
epoch
10
7
10
5
10
3
10
1
101
error
Training Error
Figure 19: σ(p) = sin(p) (left), σ(p) = Hat(p) (right)
neural network. Green line, yellow line, and blue line denote the convergent process of ∆f,u(1),
∆f,u(3), and ∆f,u(5) respectively. Spectral bias is obviously observed for sin neural network (see
left of Figure 19), while the spectral bias does not hold for Hat neural network as shown in right of
Figure 19.
16

Experiment 11. We run the Experiment 1 with fewer neurons, namely size 3000 (all the other settings
are the same as Experiment 1), and obtained similar results as follows (See Figure 20):
0
500
1000
1500
2000
2500
3000
epoch
10
4
10
3
10
2
10
1
100
error
Training Error
Lower
Medium
Higher
0
500
1000
1500
2000
2500
3000
epoch
10
3
10
2
10
1
100
error
Training Error
0
1000
2000
3000
4000
5000
epoch
10
6
10
4
10
2
100
error
Training Error
Lower
Medium
Higher
Figure 20: σ(p) = tanh(p) (left), σ(p) = ReLU(p) (middle), σ(p) = Hat(p) (right)
We also run the Experiment 2 with fewer neurons, namely size 2-5000-1 (all the other settings are the
same as Experiment 2), and obtained similar results as follows (See Figure 21):
0
1000
2000
3000
4000
5000
epoch
10
1
100
error
relative frequency error
1
5
0
1000
2000
3000
4000
5000
epoch
10
4
10
3
10
2
10
1
100
error
relative frequency error
1
5
Figure 21: σ(p) = relu(p) (left), σ(p) = Hat(100p) (right)
Experiment 12. We use shallow neural networks with each of the activation functions in (19) to ﬁt
the following target function u on [−π, π]:
u(x) = sin(x) + sin(3x) + sin(5x).
(33)
For each activation function our network has one hidden layer with size 8000. The mean square error
(MSE) loss is given by
L(f, u) = 1
N
N
X
i=1
(f(xi) −u(xi))2,
(34)
where xi is a uniform grid of size N = 201 on [−π, π]. The three networks are trained using
SDG with a learning rate of 0.001. When using a tanh or ReLU activation function, all parameters
are initialized following a Gaussian distribution with mean 0 and standard deviation 0.1, while the
network with Hat activation function is initialized following a Gaussian distribution with mean 0 and
standard deviation 0.8.
Denote
∆f,u(k) =
 ˆfk −ˆuk
 / |ˆuk| ,
(35)
where k represents the frequency, |·| represents the norm of a complex number and ˆ· represents
discrete Fourier transform. We plot ∆f,u(1), ∆f,u(3), and ∆f,u(5) in Figure 22 for each of the three
networks. From these results, we observe the spectral bias for tanh and ReLU neural networks (see
0
500
1000
1500
2000
2500
3000
epoch
10
4
10
3
10
2
10
1
100
error
Training Error
Lower
Medium
Higher
0
500
1000
1500
2000
2500
3000
epoch
10
3
10
2
10
1
100
error
Training Error
0
1000
2000
3000
4000
5000
epoch
10
6
10
4
10
2
100
102
error
Training Error
Lower
Medium
Higher
Figure 22: σ(p) = tanh(p) (left), σ(p) = ReLU(p) (middle), σ(p) = Hat(p) (right)
left and middle of Figure 22), while there is no spectral bias for the Hat neural network as shown in
17

right of Figure 22. These observations are similar to the results in Experiment 1 when ADAM is used
to training the network.
Experiment 13. We run the Experiment 1 using shallow ReLU neuron network with different sizes
n = 500, n = 2000 and n = 8000 (all the other settings are the same as Experiment 1), and obtained
results as follows (See Figure 23): From Figure 23, we can see that when the size of shallow ReLU
0
500
1000
1500
2000
2500
3000
epoch
10
3
10
2
10
1
100
error
Training Error
0
500
1000
1500
2000
2500
3000
epoch
10
4
10
3
10
2
10
1
100
error
Training Error
0
500
1000
1500
2000
2500
3000
epoch
10
3
10
2
10
1
100
error
Training Error
Figure 23: n = 500 (left), n = 2000 (middle), n = 8000 (right)
neuron network increases, the spectral bias becomes stronger.
A.2
Finite element bases
Let Th be a uniform mesh on [0, 1] with n grid points and mesh size h = 1
n. Deﬁne
Vn = {vh : vh is continuous and piecewise linear w.r.t.Th, vh(0) = 0}.
The space Vn is a standard linear ﬁnite element space in one dimension and has been well-studied,
see [10], for instance.
We denote two bases of Vn (see Figure 24), as follows:
• ReLU basis: ψi(x) = ReLU( x−xi−1
h
)
ψi(x) =

x−xi−1
h
,
x > xi−1;
0,
x ≤xi−1.
(36)
where ReLU(x) = max{0, x}.
• Hat basis: ϕi(x) = Hat(nx −i + 1)
ϕi(x) =



x−xi−1
h
,
x ∈[xi−1, xi];
xi+1−x
h
,
x ∈[xi, xi+1];
0,
elsewhere.
(37)
where Hat(x) =
(
x,
x ∈[0, 1];
2 −x,
x ∈[1, 2];
0,
elsewhere.
Note that the Hat basis is the standard basis typically used in the theory of ﬁnite element methods,
while the ReLU basis is based upon the common rectiﬁed linear activation function used in deep
learning [27]. Let Ψ(x) = (ψ1(x), ψ2(x), · · · , ψn(x))T and Φ(x) = (ϕ1(x), ϕ2(x), · · · , ϕn(x))T .
It is easy to verify that
Φ = CΨ,
(38)
and the change of basis matrix which converts between these bases is given by
C =








1
−2
1
1
−2
1
...
...
...
1
−2
1
1
−2
1








.
(39)
18

xi
2
xi
1
xi
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
xi
1
xi
xi + 1
0.0
0.2
0.4
0.6
0.8
1.0
Figure 24: ReLU basis function ψi(x) and Hat basis function ϕi(x)
and
C−1 =








1
2
3
4
· · ·
n
0
1
2
3
· · ·
n −1
0
0
1
2
· · ·
n −2
0
0
0
1
· · ·
n −3
...
...
...
...
...
...
0
0
0
0
· · ·
1








.
(40)
• When the basis in linear ﬁnite element space is chosen as Hat basis, which motivates the
activation function is chosen as σ(x) = Hat(x), we denote the mass matrix Mσ = MΦ.
Then it is easy to see that
MΦ = h
6 M,
(41)
where
M =






4
1
1
4
1
...
...
...
1
4
1
1
2






∈Rn×n.
(42)
• When the basis in linear ﬁnite element space is chosen as ReLU basis, which corresponds to
the activation function is chosen as σ(x) = ReLU(x) = max{0, x}, we denote the mass
matrix Mσ = MΨ.
• Due to (38), the relationship between MΦ and MΨ is given by
MΨ = C−1MΦC−T .
(43)
A.3
Spectral analysis of the matrices MΦ and MΨ
First we have estimates of the eigenvalues of matrix M:
Theorem 5. All the eigenvalues of M are in [ 1
6, 1].
Proof. Let λk,M be an eigenvalue of M, by the Gershgorin circle theorem, we know that
λk,M ∈{|λ −2
3| ≤1
6} ∪{|λ −2
3| ≤1
3} ∪{|λ −1
3| ≤1
6}
namely λk,M ∈[ 1
6, 1] for k = 1, 2, · · · , n.
From Theorem 5, the estimate of the eigenvalues of MΦ can be obtained immediately by (41) and
hence Theorem 2 is obtained.
19

Next, we estimate the eigenvalues of MΨ. We introduce the following matrix A which is related to
C.
A =






2
−1
−1
2
−1
...
...
...
−1
2
−1
−1
1






∈Rn×n.
(44)
The eigenvalues and corresponding eigenvectors of the matrix A, using the result in [39] can be
obtained:
Lemma 2. [39] The eigenvalues λk,A, 1 ≤k ≤n and corresponding eigenvectors ξk
A =
(ξk
A,j)n
j=1, 1 ≤k ≤n of A are
λk,A = 4 cos2
kπ
2n + 1,
ξk
A,j = −sin

(n + 1
2 −k)tjπ

(45)
with
tj =
2j
2n+1, 1 ≤j ≤n.
Lemma 3. Note that C is deﬁned by (39) and A is deﬁned by (44), we have
CCT = A2 + B,
(46)
where
B =








1
0
· · ·
0
0
0
0
0
· · ·
0
0
0
...
...
...
...
...
...
0
0
· · ·
0
0
0
0
0
· · ·
0
−1
1
0
0
· · ·
0
1
−1








= a0aT
0 −a1aT
1 ∈Rn×n.
(47)
and a0 =








1
0
...
0
0
0








∈Rn, a1 =








0
0
...
0
−1
1








∈Rn.
Proof. By direct computation, we ﬁnd that there is a relationship between A and C as follows
A = −C

0
1
In−1
0

+ B1
(48)
where In−1 ∈R(n−1)×(n−1) is the identity matrix and
B1 =








0
0
· · ·
0
0
1
0
0
· · ·
0
0
0
...
...
...
...
...
...
0
0
· · ·
0
0
0
0
0
· · ·
0
0
−1
0
0
· · ·
0
0
1








= a3aT
4 ∈Rn×n,
where a3 =








1
0
...
0
−1
1








and a4 =








0
0
...
0
0
1








.
20

Then
C = (−A + B1)

0
1
In−1
0
−1
and
CT =

0
1
In−1
0
−T
(−A + BT
1 ).
Noting that

0
1
In−1
0
−1
=

0
1
In−1
0
T
, by direct computation of CCT , (46) is desired.
Lemma 4. [13] Let R ∈Rn×n, N ∈Rn×n and L ∈Rn×n are symmetric matrices satisfying
R = N + L
(49)
and ν1 ≥ν2 ≥· · · ≥νn are the eigenvalues of R, γ1 ≥γ2 ≥· · · ≥γn are the eigenvalues of N
and β1 ≥β2 · · · ≥βn are the eigenvalues of L, then we have
γj + βk ≤νi ≤γp + βq
(50)
where j + k −n ≥i ≥p + q −1.
In order to estimate the eigenvalues of MΨ, we need to estimate the eigenvalues of CCT :
Lemma 5. Let ν1 ≥ν2 ≥· · · ≥νn be the eigenvalues of CCT and λ2
j,A, j = 1, · · · , n be the
eigenvalues of A2, then we have
λ2
2,A ≤ν1 ≤λ2
1,A + 1;
λ2
i+1,A ≤νi ≤λ2
i−1,A,
2 ≤i ≤n −1;
0 < νn ≤λ2
n−1,A.
(51)
In addition, we have
ν1 ≤16,
(52)
and
4
n2(n + 1)2 ≤νn.
(53)
Proof. By Weyl’s inequality for pertubation matrix, namely Lemma 4, we have
λ2
j,A + βk ≤νi ≤λ2
p,A + βq
(54)
where j + k −n ≥i ≥p + q −1 and λ2
j,A, j = 1, · · · , n are the eigenvalues of A2. Further noting
that CCT is positive deﬁnite and by direct computing we can obtain β1 = 1, β2 = · · · = βn−1 =
0, βn = −2 and hence (51) is proved. Next we only need to prove (52) and (53). Since CCT is
symmetric positive deﬁnite, we have
ν1 = λmax(CCT ) = ρ(CCT ) ≤∥CCT ∥∞
≤∥C∥∞∥CT ∥∞= 4 × 4 = 16,
where ρ(CCT ) is the spectral radius of CCT .
Since
λmax(C−T C−1) = ρ(C−T C−1) ≤∥C−T C−1∥∞
≤∥C−T ∥∞∥C−1∥∞
= n(n + 1)
2
n(n + 1)
2
= n2(n + 1)2
4
,
where ρ(C−T C−1) is the spectral radius of C−T C−1, then
νn = λmin(CCT ) =
1
λmax(C−T C−1) ≥
4
n2(n + 1)2 .
21

Lemma 6. (Courant-Fisher min-max theorem) For any given matrix E ∈Rn×n, E = ET , suppose
λ1 ≥λ2 ≥· · · ≥λn are the eigenvalues of E, then
λn+1−k =
min
{S|dim S=k} max
x∈S
(Ex, x)
(x, x) .
Now we give the eigenvalues of MΨ as follows:
Theorem 6. The eigenvalues of MΨ satisfy
λk,MΨ = mkhν−1
n+1−k,
k = 1, 2, · · · , n
(55)
where 1
6 ≤mk ≤1 is a constant, νn+1−k, k = 1, 2, · · · , n are the eigenvalues of CCT .
Proof. Noting (43), namely
MΨ = C−1MΦC−T .
(56)
For any given S with dimS = k, we have
max
x∈S
(MΨx, x)
(x, x)
= max
x∈S
(C−1MΦC−T x, x)
(x, x)
= max
x∈S
(MΦC−T x, C−T x)
(x, x)
For the above given S, let xs satisfy
max
x∈S
(MΦC−T x, C−T x)
(x, x)
= (MΦC−T xs, C−T xs)
(xs, xs)
,
then we have
max
x∈S
(MΨx, x)
(x, x)
= (MΦC−T xs, C−T xs)
(xs, xs)
≤λmax(MΦ)(C−T xs, C−T xs)
(xs, xs)
≤λmax(MΦ) max
x∈S
(C−T x, C−T x)
(x, x)
= λmax(MΦ) max
x∈S
(C−1C−T x, x)
(x, x)
.
Now let Sk satisfy
min
{S|dimS=k} max
x∈S
(C−1C−T x, x)
(x, x)
= max
x∈Sk
(C−1C−T x, x)
(x, x)
,
then we have
min
{S|dimS=k} max
x∈S
(MΨx, x)
(x, x)
≤max
x∈Sk
(MΨx, x)
(x, x)
≤λmax(MΦ) max
x∈Sk
(C−1C−T x, x)
(x, x)
= λmax(MΦ)
min
{S|dimS=k} max
x∈S
(C−1C−T x, x)
(x, x)
.
By Lemma 6 and noting that CCT = C(CT C)C−1, we have
λn+1−k,r ≤λmax(MΦ)λn+1−k(C−1C−T )
=λmax(MΦ)
1
λk(CT C)
=λmax(MΦ)
1
λk(CCT )
(57)
22

Similarly, we have
λmin(MΦ)
1
λk(CCT ) ≤λn+1−k,r
(58)
Combining (57) and (58), we have
λmin(MΦ)
1
λk(CCT ) ≤λn+1−k,r ≤λmax(MΦ)
1
λk(CCT ).
Noting that the eigenvalues of MΦ is in [ h
6 , h], then there exists a constant mn+1−k ∈[ 1
6, 1] such
that
λn+1−k,r = mn+1−kh
1
λk(CCT ).
(59)
Finally, by Lemma 4, we have
λk,MΨ = mkh
1
λn+1−k(CCT ) = mkhν−1
n+1−k.
(60)
A.4
Proof of Theorem 1
Proof. From the equation (55) shown in Theorem 6, Theorem 1 can be obtained easily.
A.5
Eigenvectors of MΨ
Theorem 7. Let λ1,Ψ > λ2,Ψ > · · · > λn,Ψ and ξ1
Ψ, ξ2
Ψ, · · · , ξn
Ψ be the eigenvalues and correspond-
ing eigenvectors of MΨ. Extend vk
h(x) = ξk
Ψ · Ψ(x) = C−T ξk
Ψ · Φ(x) ∈Vn by zero to [x−1, xn],
then vk
h(x) satisﬁes
R 1
0 |Dhvk
h(x)|2dx
R 1
0 |vk
h(x)|2dx
= m−1
k n4νn+1−k,
(61)
where mk ∈[1/6, 1] is a constant independent of h and Dh is the second order ﬁnite difference
operator. In particular,
1. for k = 1, we have
c′
0 ≥
R 1
0 |Dhv1
h(x)|2dx
R 1
0 |v1
h(x)|2dx
≥c0,
(62)
where c0 and c′
0 are constants independent of h.
2. for k = n, we have
c′
1n4 ≥
R 1
0 |Dhvn
h(x)|2dx
R 1
0 |vn
h(x)|2dx
≥c1n4,
(63)
where c1 and c′
1 are constants independent of h.
Proof. Noting that ξk
Ψ and λk,MΨ satisfy
MΨξk
Ψ = λk,MΨξk
Ψ,
and
MΨ = C−1MΦC−T ,
we have
C−1MΦC−T ξk
Ψ = λk,MΨξk
Ψ,
C−T C−1MΦC−T ξk
Ψ = λk,MΨC−T ξk
Ψ.
(64)
23

Deﬁne v = C−T ξk
Ψ, then we have
(CCT )−1MΦv = λk,MΨv,
(65)
CCT v = λ−1
k,MΨMΦv.
(66)
Denoting v = (v1, v2, · · · , vn)T , we have
CT v =






v1
−2v1 + v2
v1 −2v2 + v3
...
vn−2 −2vn−1 + vn






(67)
and
(CCT v, v) = (CT v, CT v)
=v2
1 + (−2v1 + v2)2 + (v1 −2v2 + v3)2
+ · · · + (vn−2 −2vn−1 + vn)2
=λ−1
k,MΨ(MΦv, v) = λ−1
k,MΨ(vk
h(x), vk
h(x)).
Noting that vk
h(x0) = 0 and the zero extension of vk
h(x), we have the corresponding extension for the
vector v by v0 = 0 and v−1 = 0.
Therefore, noting that Dh is the second order ﬁnite difference operator, we have
(CCT v, v)
=(v−1 −2v0 + v1)2 + (v0 −2v1 + v2)2
+ (v1 −2v2 + v3)2 + · · · + (vn−2 −2vn−1 + vn)2
=h4
n−1
X
i=1
|Dhvk
h(xi)|2 = h3
Z 1
0
|Dhvk
h(x)|2dx.
Hence, by (55), we have
h3
Z 1
0
|Dhvh(x)|2dx = (CCT v, v) = λ−1
k,MΨ(vk
h(x), vk
h(x))
= m−1
k h−1νn+1−k
Z 1
0
|vk
h(x)|2dx
namely
R 1
0 |Dhvk
h(x)|2dx
R 1
0 |vk
h(x)|2dx
= m−1
k n4νn+1−k,
(68)
where we used h = 1
n.
From Theorem 7, Theorem 4 can be obtained.
For example, when n = 128, we can see that v1
h(x) = ξ1
Ψ · Ψ(x) and v2
h(x) = ξ2
Ψ · Ψ(x) are very
smooth functions. And v127
h
(x) = ξ127
r
· Ψ(x) and v128
h
(x) = ξ128
r
· Ψ(x) are highly oscillatory
functions, see Figure 1.
24

