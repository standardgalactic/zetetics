Lectures for STP 421: Probability Theory and Bayesian
Inference
Jay Taylor
October 15, 2014

Contents
1
Overview and Conceptual Foundations of Probability
5
1.1
Determinism, Uncertainty, and Probability . . . . . . . . . . . . . . . . . . . . . .
5
1.2
Probability Spaces and Measure Theory . . . . . . . . . . . . . . . . . . . . . . .
6
1.3
Logical Probability and Plausible Inference
. . . . . . . . . . . . . . . . . . . . .
9
2
Probability Theory and Plausible Inference
12
2.1
Boolean Algebra and Deductive Inference
. . . . . . . . . . . . . . . . . . . . . .
12
2.2
Deductive and Plausible Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.3
Plausible Inference and Bayesian Probability Theory . . . . . . . . . . . . . . . .
14
2.3.1
The product rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.3.2
The sum rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.3.3
Bayes’ formula and plausible inference . . . . . . . . . . . . . . . . . . . .
19
3
Sample Spaces, Events and Probabilities
20
3.1
Probability Spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.2
Continuity Properties of Probability Measures . . . . . . . . . . . . . . . . . . . .
22
3.3
Conditional Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
4
Random Variables and Probability Distributions
29
4.1
Random Variables
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
4.1.1
Discrete random variables . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
4.1.2
Continuous random variables . . . . . . . . . . . . . . . . . . . . . . . . .
33
4.1.3
Properties of the Cumulative Distribution Function . . . . . . . . . . . . .
34
4.2
Expectations and Moments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
4.2.1
Moment generating functions . . . . . . . . . . . . . . . . . . . . . . . . .
38
5
A Menagerie of Distributions
40
5.1
The Binomial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2

CONTENTS
3
5.1.1
Sampling and the hypergeometric distribution . . . . . . . . . . . . . . . .
43
5.2
The Poisson Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
5.2.1
Fluctuation tests and the origin of adaptive mutations . . . . . . . . . . .
48
5.2.2
Poisson processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
5.3
Random Waiting Times
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
5.3.1
The geometric and exponential distributions . . . . . . . . . . . . . . . . .
51
5.3.2
The gamma distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
5.4
Continuous Distributions on Finite Intervals . . . . . . . . . . . . . . . . . . . . .
55
5.4.1
The uniform distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
5.4.2
The beta distribution
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
5.4.3
Bayesian estimation of proportions . . . . . . . . . . . . . . . . . . . . . .
57
5.4.4
Binary to analogue: construction of the uniform distribution . . . . . . . .
59
5.5
The Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
5.6
Transformations of Random Variables
. . . . . . . . . . . . . . . . . . . . . . . .
63
6
Random Vectors and Multivariate Distributions
68
6.1
Joint and Marginal Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
6.2
Independent Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
6.3
Conditional Distributions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
6.3.1
Discrete Conditional Distributions
. . . . . . . . . . . . . . . . . . . . . .
72
6.3.2
Continuous Conditional Distributions
. . . . . . . . . . . . . . . . . . . .
73
6.3.3
Conditional Expectations
. . . . . . . . . . . . . . . . . . . . . . . . . . .
74
6.4
Expectations of Sums and Products of Random Variables
. . . . . . . . . . . . .
76
6.4.1
Expectations of products
. . . . . . . . . . . . . . . . . . . . . . . . . . .
79
6.5
Covariance and Correlation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
6.6
The Law of Large Numbers and the Central Limit Theorem . . . . . . . . . . . .
83
6.6.1
The Weak and Strong Laws of Large Numbers
. . . . . . . . . . . . . . .
83
6.6.2
The Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . .
85
6.7
Sample Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
6.7.1
The χ2 Distribution
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
6.7.2
Student’s t Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
7
Entropy
92
7.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92

4
CONTENTS
7.2
The Maximum Entropy Principle . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
7.3
Maximum Entropy Distributions
. . . . . . . . . . . . . . . . . . . . . . . . . . .
95
7.3.1
Uniform distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
7.3.2
Geometric and exponential distributions . . . . . . . . . . . . . . . . . . .
98
7.3.3
Gaussian distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
8
Bayesian Inference for Gaussian Distributions
100
8.1
Estimation of a Mean
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
8.1.1
Unequal variances
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
8.1.2
Unknown mean and variance
. . . . . . . . . . . . . . . . . . . . . . . . . 103
8.2
Interlude: Fisher Information and Jeﬀreys Priors . . . . . . . . . . . . . . . . . . 104

Chapter 1
Overview and Conceptual Foundations of Probability
1.1
Determinism, Uncertainty, and Probability
Scientiﬁc determinism postulates that we can predict how a system will develop over time if
we know (i) the laws governing the system and (ii) the initial state of the system. In particular,
in a deterministic world, we should obtain the same outcome whenever we repeat an experiment
under identical conditions. Deterministic theories were particularly inﬂuential in the eighteenth
and nineteenth centuries in large part because of the remarkable successes of Newtonian mechan-
ics and remain important today, as can be seen in the continuing use of ordinary and partial
diﬀerential equations to model numerous physical phenomena. For example, Newton’s second
law - “force equals mass times acceleration” - can be expressed as a system of ODE’s,
md2x
dt2
=
F(x(0))
x(0)
=
x0
x′(0)
=
v0
which can be solved exactly (in principle) if we know the forces F acting on the system and
the initial state (x0, v0) of the system. In fact, Newtonian mechanics once seemed so eﬀective
at describing nature that the mathematician Pierre-Simon Laplace (1814) was led to theorize
that the entire future and past of the universe could determined exactly by an entity (Laplace’s
demon) that knew the current positions and momenta of all of the particles in the universe.
However, despite these successes, scientiﬁc applications of deterministic models are encumbered
by multiple sources of uncertainty, some of which are listed below.
• Imprecise knowledge of the state of the system due to measurement error.
• Imprecise knowledge of model parameters.
• Model uncertainty and approximation: George Box: “All models are wrong, but some are
useful.” For example, molecular and cosmological processes are usually neglected when we
formulate models of circulation or airplane ﬂight.
• Numerical solutions are usually approximate due to round-oﬀand truncation errors.
• Certain kinds of deterministic models (chaotic dynamical systems) are known to be exquisitely
sensitive to their initial conditions in the sense that trajectories that are initially arbitrarily
close diverge exponentially.
• Other models are ill-posed in the sense that they either don’t have strong solutions or they
have more than one solution consistent with initial and/or boundary data. For example, it
5

6
CHAPTER 1. OVERVIEW AND CONCEPTUAL FOUNDATIONS OF PROBABILITY
is possible that the Navier-Stokes equation, which is used to model ﬂuid ﬂow, lacks strong
solutions for some physically realistic parametrizations.
• According to some interpretations, quantum mechanics provides an intrinsically stochastic
description of nature, i.e., Schrödinger’s equation only allows us to calculate the probability
that a system occupies a given state.
For these reasons, scientists almost always must deal with uncertainty in one form or another.
While diﬀerent theoretical tools have been developed to quantify uncertainty, most approaches
make use of probability theory. As a mathematical subject, probability has its origins in the
seventeenth century in studies of games of chance and gambling. Since then, several diﬀerent
interpretations of what we mean by “probabilities” have been proposed and these diﬀer mainly
in the status that they give to probability as either a description of some feature of the physical
world or as a description of our knowledge of the world. The main interpretations are summarized
in the next list.
• Classical interpretation: Equivalent events are equally likely (J. Bernoulli, B. Pascal,
P.-S. Laplace).
• Frequentist interpretation: The probability of an event is equal to its frequency in a
series of independent, identical trials (R. Ellis, J. Venn).
• Propensity interpretation: The probability of an event is equal to its physical propensity
(C. S. Peirce, K. Popper).
• Subjective interpretation: The probability of a proposition is equal to an individual’s
degree of belief that the proposition is true (F. Ramsey, J. von Neumann, B. de Finetti).
• Logical interpretation: Probabilities quantify the degree of support that some evidence
E gives to a hypothesis H (J. M. Keynes, R. Jeﬀrey, R. T. Cox, E. Jaynes, G. Polya).
The frequentist and propensity interpretations are sometimes said to be objective or physical
because they postulate that probabilities are attributes of the physical world and are independent
of any observer. In contrast, the subjective and logical interpretations are said to be Bayesian
because they treat probabilities as properties of knowledge or belief rather than as properties of
the physical world.
1.2
Probability Spaces and Measure Theory
Although the mathematical study of probability began in the 17’th century, the ﬁrst comprehen-
sive mathematical theories were proposed in the early 20’th century. By far, the most important
of these is the measure theoretic formulation of probability introduced by Andrei Kolmogorov
in 1933. It should be emphasized that a mathematical theory is a formal description of the world
and does not specify its own interpretation, although it may well rule out certain interpretations
that are logically incompatible with the theory.
The most important object in measure theoretic probability is a probability space, which we
deﬁne below. To motivate the deﬁnition, imagine that you are conducting an experiment with a
random outcome and that you want to describe this scenario mathematically.
Deﬁnition 1.1. A probability space is a triple {Ω, F, P} where:

1.2. PROBABILITY SPACES AND MEASURE THEORY
7
• Ωis the sample space, i.e., the set of all possible outcomes of the experiment.
• F is a collection of subsets of Ωwhich we call events. F is called a σ-algebra and is
required to satisfy the following conditions:
1. The empty set and the sample space are both events: ∅, Ω∈F.
2. If E is an event, then its complement Ec = Ω\E is also an event.
3. If E1, E2, · · · are events, then their union ∪nEn is an event.
• P is a function from F into [0, 1]: if E is an event, then P(E) is the probability of E. P is
said to be a probability distribution or probability measure on F and is also required
to satisfy several conditions:
1. P(∅) = 0; P(Ω) = 1.
2. Countable additivity: If E1, E2, · · · are mutually exclusive events, i.e., Ei∩Ej =
∅whenever i ̸= j, then
P
 ∞
[
n=1
En

=
∞
X
n=1
P(En).
To make the deﬁnition a bit more concrete, consider the following two examples.
Example 1.1. Suppose that the experiment consists of tossing a fair coin twice and recording
whether we get heads or tails on each toss. Then
• Ω= {(H, H), (H, T), (T, H), (T, T)};
• F is the collection of all 16 subsets of Ω, which we call the power set of Ωand denote
P(Ω).
• if E is a subset of Ωcontaining k outcomes, then P(E) = k/4.
Example 1.2. Now suppose that the experiment consists of tossing a fair coin twice and recording
the total number of heads. Then
• Ω= {(H, H), (H, T), (T, H), (T, T)};
• F is the collection
n
∅, {(H, H)}, {(H, T), (T, H)}, {(T, T)}, {(H, H), (H, T), (T, H)},
{(H, T), (T, H), (T, T)}, {(H, H), (T, T)}, Ω
o
;
• P(0 heads) = P(2 heads) = 1/4, P(1 heads) = 1/2.
Notice that Examples 1.1 and 1.2 both have the same sample space, but have diﬀerent σ-algebras
that reﬂect the diﬀerent kinds of knowledge that are available in the two experiments. In the
ﬁrst case, we know the exact sequence of heads or tails obtained in the two coin tosses, whereas
in the second case, we only know the total number of heads, i.e., we have less knowledge and
this is reﬂected in the lesser number of sets contained in the σ-algebra.
We next consider why we need to restrict to countable additivity and why we only require
σ-algebras to be closed under countable unions.
Example 1.3. Suppose that we want to model an experiment in which the outcome is a real
number that is uniformly distributed between 0 and 1. Then

8
CHAPTER 1. OVERVIEW AND CONCEPTUAL FOUNDATIONS OF PROBABILITY
• Ω= [0, 1];
• F should at least contain all open and closed intervals contained in [0, 1];
• P([a, b]) = b −a whenever 0 ≤a ≤b ≤1.
The last deﬁnition asserts that the probability that the outcome lies in an interval contained in
[0, 1] is equal to the length of the interval, which is what we mean when we say the outcome is
uniformly distributed on [0, 1]. One consequence of this deﬁnition is that points have probability
0, i.e, P({x}) = P([x, x]) = x −x = 0 since {x} = [x, x]. However, it follows that the count-
able additivity of probability measures cannot generally be extended to uncountable collections of
events, since we don’t have identity between the left-hand and right-hand sides of the following
expression:
1 = P([0, 1]) ̸=
X
x∈[0,1]
P({x}) = 0.
Indeed, the problem here is that although the sets {x} and {y} are disjoint whenever x ̸= y, there
are uncountably many such sets contained in [0, 1], i.e, [0, 1] is an uncountably inﬁnite set as was
ﬁrst demonstrated by Cantor using the famous diagonalization argument. Later we will see that
such sums need to be replaced by integrals.
Notice that we haven’t speciﬁed the σ-algebra in Example 1.3, only that it should contain all of
the open and closed intervals contained in [0, 1]. One might hope that we could take F to be
the power set of [0, 1], i.e, we will let all subsets of [0, 1] be events, but it can be shown, using
the axiom of choice, that there is no function P deﬁned on the power set P([0, 1]) with values in
[0, 1] which satisﬁes the conditions required of a probability measure and has the property that
P([a, b]) = b−a for all [a, b] ⊂[0, 1]. Indeed, should you stipulate the existence of such a function,
one can show that this leads to logical contradictions. Fortunately, one can also show that there
is a σ-algebra F containing all of the open and closed intervals which is strictly smaller than
P([0, 1]) but which allows one to deﬁne P as above without running into any contradictions. This
is called the Borel σ-algebra on [0, 1] and it is deﬁned to be the smallest σ-algebra containing all
of the open intervals, i.e., it is the intersection of all σ-algebras which contain these intervals. The
sets contained in this σ-algebra are called Borel sets. In general, essentially any set that we might
reasonably expect to encounter in applications will be a Borel set, but there is no simple descrip-
tion of what these sets “look like”, i.e., the structure of a Borel set can be arbitrarily complicated.
Since much of the modern literature on probability theory and its applications are based on the
measure-theoretic formulation, the importance of this theory cannot be overstated. Furthermore,
as the previous example illustrates, trying to deﬁne probabilities on uncountable sets (such as
the real numbers) leads to serious technical diﬃculties and measure theory provides us with a
powerful set of tools that can be used to overcome these diﬃculties. At the same time, to be
able to understand and use these tools adeptly, one ﬁrst needs to have a fairly good grasp of
abstract analysis and set theory. Introductory textbooks on probability generally either ignore
these issues altogether or simply state the measure theoretic deﬁnition as I have above but don’t
actually use it in a rigorously. We will revisit the measure-theoretic formulation of probability on
several occasions during the semester, but most of our studies will be based on a formulation of
probability which is less formal in some respects but more closely tied to the Bayesian concepts
that we wish to emphasize.

1.3. LOGICAL PROBABILITY AND PLAUSIBLE INFERENCE
9
1.3
Logical Probability and Plausible Inference
In this course, we will base our treatment of probabilities on the work of G. Polya, R. T. Cox and
E. Jaynes, who regarded probability theory as an extension of deductive logic which can be used
to carry out plausible inference in a rigorous and objective fashion. Speciﬁcally, we will write
P(A|B) for the probability or plausibility of a proposition A given that we know (or assume)
another proposition B to be true. This notation immediately highlights two important diﬀerences
between the logical approach described here and the measure-theoretic approach described in the
previous section. First, whereas measure-theoretic probabilities take sets as arguments, logical
probabilities are deﬁned on propositions. Secondly, whereas conditional probabilities are deﬁned
secondarily in the measure-theoretic formulation, here we will regard every probability as a
conditional probability. Although this approach is arguably less satisfactory as a mathematical
theory, it has the advantage that it focuses immediate attention on two practical features of
Bayesian inference. The ﬁrst of these is that a Bayesian probability can be assigned to any
meaningful proposition, even when that proposition does not refer to an event that can be
ascertained in a sequence of independent identical trials. For example,in Bayesian inference, it is
meaningful to speak of the probability that the sun will rise tomorrow, that Barrack Obama will
be elected as president in 2012, or that the most recent common ancestor of all extant human
mitochondria DNA genomes (mitochondrial Eve) lived between 150,000 and 250,000 years ago.
The second feature that we wish to emphasize is that assessments of probabilities are always
made within some context, i.e., every probability is a conditional on some other information,
whether that comes in the form of data or a model that we assume to be true. Indeed, one of
the most powerful features of Bayesian inference is that it is designed to deal with problems in
which information arrives sequentially in time.
In the next chapter, we will argue that probabilities should satisfy the following two conditions:
Sum rule:
P(A|B) + P( ¯A|B)
=
1
Product rule:
P(A, B|C)
=
P(A|C)P(B|A, C)
=
P(B|C)P(A|B, C).
Here, A, B, and C are three propositions, ¯A denotes the negation of A, while A, B denotes the
compound proposition that asserts that both A and B are true. Morally, the sum rule has the
following interpretation: given any proposition A, either A is true or the negation of A is true,
but it is never the case that both are true, i.e., A and ¯A are mutually exclusive. As in Boolean
logic, we will let the plausibility of a statement that is known or assumed to be true be equal to
1. Notice that, in some respects, the sum rule is analogous to the measure-theoretic requirement
that the probability of the sample space be equal to 1 and that probabilities of disjoint sets are
additive. However, we will deduce the sum rule from other properties that we will require of
plausibilities rather than treat it as an axiom.
The product rule can be written in two distinct but equivalent forms because the compound
propositions A, B and B, A are equivalent and so it doesn’t matter whether we ﬁrst condition
on A or ﬁrst condition on B when we evaluate the probability that both A and B are true. Fur-
thermore, once we have the product rule in hand, we can immediately deduce Bayes’ formula,
which we write as
P(B|A, C) = P(B|C) · P(A|B, C)
P(A|C) ,
provided that P(A|C) > 0. In eﬀect, Bayes’ formula tells us how conditional probabilities change
when we interchange the proposition that we are conditioning on with the proposition that we
are assessing. For this reason, Bayesian inference is sometimes said to be concerned with inverse

10
CHAPTER 1. OVERVIEW AND CONCEPTUAL FOUNDATIONS OF PROBABILITY
probabilities. As we will discuss at length, we often apply Bayes’ formula to problems where we
wish to assess the plausibility of each member of a set of hypotheses, say H1, · · · , Hn, in the light
of some existing knowledge or assumptions, say I, as well as some new data D that has become
available to us. In this case, we can write
P(Hi|D, I) = P(Hi|I) · P(D|Hi, I)
P(D|I)
,
where P(Hi|I) is said to be the prior probability of hypothesis Hi before we consider the new
data, P(Hi|D, I) is the posterior probability of Hi once the new data has been accounted for,
and P(D|Hi, I) is the likelihood of the data D assuming that Hi is true. The expression in the
numerator is the marginal probability of the data, which we will see is equal to a weighted
sum of the likelihood functions,
P(D|I) =
n
X
i=1
P(Hi|I)P(D|Hi, I),
provided that the hypotheses are mutually exclusive and exhaustive, i.e., one and only one of
the hypotheses is true.
An important diﬀerence between the measure-theoretic formulation of probabilities and that
introduced in this section is in how the two handle limits. Whereas our deﬁnition of a probability
space came with several requirements concerning countable unions of events that are intended
to make probabilities behave well when certain kinds of limits are taken, no such machinery
has been built into the logical formulation of probability introduced by Cox and Jaynes. In
particular, both the sum rule and the product rule deal only with ﬁnite collections of events.
Thus, to take limits of probabilities within this framework, we must rely on the details of the
particular problem at hand. For example, for some kinds of problems, we may wish to consider
uncountably inﬁnite collections of hypotheses, such as when we wish to estimate the value of a
numerical parameter, say Θ, that is only constrained by prior knowledge to be contained income
subinterval of the real line. To be precise, let us write H(θ) for the hypothesis that Θ = θ,
i.e., that the parameter is equal to the particular value θ, where we only know in advance that
θ ∈[0, 1], say. In such cases, we can identify the collection of hypotheses with the interval [0, 1]
by identifying H(θ) with the value θ itself and we can sometimes deﬁne a probability density
p(θ) on [0, 1] by taking the limit
p(θ) =
lim
δθ→0+
1
δθP(θ ≤Θ ≤θ + δθ|I).
In general, there is no guarantee that this limit will exist, but for surprisingly many applications
it can be shown that it does. It should be emphasized that the probability density p(θ) is not
itself a probability, since it can easily be the case that p(θ) > 1, but the density can be used to
calculate probabilities by integrating, e.g.,
P(a ≤Θ ≤b|I) =
Z b
a
p(θ)dθ.
We close with an example from chapter 1 of Gregory (2005).
Example 1.4. Suppose that a test for HIV-1 infection is known to have a false positive rate of
2.3% and a false negative rate of 1.4%, and that the incidence of HIV-1 in a particular population
is equal to 1 : 10, 000. If an individual with no known risk factors takes the test and tests positive
for HIV-1 infection, what is the probability that they are, in fact, infected?

1.3. LOGICAL PROBABILITY AND PLAUSIBLE INFERENCE
11
We can use Bayes’ formula to solve this problem, but we ﬁrst introduce some notation. Let
H
≡
“the individual is infected by HIV-1”
¯H
≡
“the individual is not infected by HIV-1”
D
≡
“the individual tests positive”
I
≡
“the individual has no known risk factors; the false positive rate of the test is 2.3%;
the false negative rate is 1.4%; the incidence of HIV-1 in the population is 1 : 10, 000.”
As above, Bayes’ formula can be written as
P(H|D, I) = P(H|I)P(D|H, I)
P(D|I) .
Here H and ¯H are the two hypotheses of interest and these are both mutually exclusive and
exhaustive. Thus, the denominator in the fraction on the right-hand side can be expanded as
P(D|I) = P(H|I)P(D|H, I) + P( ¯H|I)P(D| ¯H, I).
In light of the prior information stipulated in I, we have
P(H|I)
=
0.0001
P( ¯H|I)
=
1 −P(H|I)
=
0.9999
P(D|H, I)
=
0.986
P(D| ¯H, I)
=
0.023.
Indeed, since I asserts that the individual has no known risk factors for HIV-1 infection, we
can take the overall incidence of HIV-1 infection in the population as our estimate of the prior
probability that the individual is infected, i.e., we are using frequency data to estimate (or elicit)
the prior probability of a hypothesis of interest. The second probability follows from the sum
rule. The third probability is concerned with the event where the individual tests positive (D)
when they are in fact infected H and when I is true; however, this is just the probability of
a true positive, which by the sum rule is equal to 1 minus the probability of a false negative,
which is 1 −0.014 = 0.986. Similarly, the fourth probability is that of a false positive, which
is 0.023 according to I. Substituting these quantities into the equation written above shows that
P(D|I) = 0.0230963. It then follows that
P(H|D, I) =
0.0001 × 0.986
0.0001 × 0.986 + 0.999 × 0.023 = 0.0042,
and so an individual with a positive test result but no known risk factors is still very unlikely to
be infected by HIV-1 despite the fact that the test is fairly accurate. The reason for this is that
the incidence of HIV-1 infection in the population is so low that an individual that has no reason
to suspect that they are HIV-1 infected is far more likely to receive a positive test result due to a
testing error than because they are, in fact, infected.

Chapter 2
Probability Theory and Plausible Inference
2.1
Boolean Algebra and Deductive Inference
We begin with a review of Boolean algebra as applied to propositional logic. Following Gregory
(2005), we will use capital letters A, B, C, · · · to represent logical propositions, each of which
asserts that something is true, e.g., A = “The earth is more than 4.6 billion years old.”
In
Boolean logic, a proposition is either true or false and we often represent these states by the
numerical values 1 or 0, respectively. There are several standard operations which can be used
to construct new propositions, including
• negation: ¯A asserts that “A is not true”;
• conjunction: A∧B asserts that “A is true and B is true”; in probability, it is also common
to use the shorthand notation A, B to mean the same;
• disjunction: A ∨B asserts that “A is true or B is true”;
• implication: A =⇒B asserts that “A implies B”.
In general, a compound proposition is just a function Φ(A1, · · · , An) of some other propositions
A1, · · · , An which is either true or false for each assignment of truth values to its arguments.
Such functions are often represented by truth tables. Two propositions depending on the same
arguments are said to be logically equivalent if they have the same truth values for all possible
assignments of truth values to their arguments. For example, the two compound propositions
A =⇒B and ¯A ∨B are logically equivalent since both are false if and only if A is true and B
is false. Two additional logical identities that are often useful concern the eﬀect of negation on
conjunction and disjunction:
A1 ∧· · · ∧An
=
¯
A1 ∨· · · ∨¯
An
A1 ∨· · · ∨An
=
¯
A1 ∧· · · ∧¯
An.
In words, these identities assert that the negation of a conjunction of propositions is equivalent
to the disjunction of the negations of those propositions, while the negation of a disjunction of
propositions is equivalent to the conjunction of the negations of those propositions. In other
words, propositional logic is invariant if we simultaneously interchange the values T and F and
the operations ∧and ∨. This property is known as duality.
There are several other identities which can be interpreted algebraically:
• Involution: ¯¯A = A;
12

2.1. BOOLEAN ALGEBRA AND DEDUCTIVE INFERENCE
13
• Idempotence: A ∧A = A ∨A = A;
• Commutativity: A ∧B = B ∧A; A ∨B = B ∨A;
• Associativity: A ∧(B ∧C) = (A ∧B) ∧C; A ∨(B ∨C) = (A ∨B) ∨C;
• Distributivity: A ∧(B ∨C) = (A ∧B) ∨(A ∧C); A ∨(B ∧C) = (A ∨B) ∧(A ∨C).
Up to logical equivalence, there are 22n diﬀerent compound propositions that depend on exactly
n Boolean arguments A1, · · · , An. Indeed, there are m = 2n diﬀerent truth assignments to these
variables and there are 2m distinct functions from a set containing m elements to a set containing
two elements. An important question concerns whether we can express every such function using
the basic operations consisting of negation, conjunction and disjunction. That the answer is
aﬃrmative can be shown using a method called reduction to disjunctive normal form. To
illustrate this method, suppose that Ψ(A1, · · · , An) is a Boolean function of n arguments. Notice
that any truth assignment to these arguments can be identiﬁed with a subset E of the index set
I = {1, · · · , n} by stipulating that i ∈E if and only the truth assignment assigns the value T to
Ai. Since Ψ is either true or false, Ψ can be identiﬁed with a possibly empty collection of such
subsets, say TΨ = {E1, E2, · · · , Em} with 0 ≤m ≤2n, where a subset E ⊂I is included in TΨ
if and only if Ψ(A1, · · · , An) is true when the arguments are given the truth assignment deﬁned
by E. Given such a subset E ⊂I, let ψE(A1, · · · , An) be the compound proposition obtained
by setting
ψE(A1, · · · , An) =
 ^
i∈E
Ai
!
∧
 ^
i/∈E
¯Ai
!
.
In other words, ψE is the compound proposition which is true if and only if its argument has the
truth assignment corresponding to E. Provided that TΨ is non-empty, i.e., that Ψ evaluates to
T for at least one truth assignment, then Ψ is logically equivalent to the following disjunction of
such propositions
Ψ(A1, · · · , An) =
_
E∈TΨ
ψE(A1, · · · , An).
On the other hand, if every truth assignment makes Ψ false, then TΨ = ∅and Ψ is equivalent to
any statement that always evaluates to F, for example, Ψ is logically equivalent to A1 ∧¯A1.
In fact, it is possible to write any Boolean expression using a single operation. We ﬁrst observe
that conjunction and disjunction are not both required, since by the duality relationship described
above any disjunction of two propositions A and B is equivalent to the negation of the conjunction
of the negations of A and B, i.e.,
A ∨B = ¯A ∧¯B.
This shows that any Boolean expression can be written using only conjunction and negation. To
reduce the number of operations to one, deﬁne the NAND operator by setting
A ↑B ≡A ∧B = ¯A ∨¯B
and observe that
¯A
=
A ↑A
A, B
=
(A ↑B) ↑(A ↑B).
Then, since any expression can be written using just negation and conjunction and both of these
operations can be written using the NAND operator, it follows that every Boolean expression
can be written using NAND operator alone. As shown in Gregory (2005), the NOR operator,
which is deﬁned by setting
A ↓B ≡A ∨B = ¯A ∧¯B
is also suﬃcient.

14
CHAPTER 2. PROBABILITY THEORY AND PLAUSIBLE INFERENCE
2.2
Deductive and Plausible Reasoning
Boolean logic is concerned with deductive inference, in which the truth of a proposition is deduced
logically from the truth of other propositions. Uncertainty plays no role in this kind of reasoning,
which is based on the repeated application of the following kinds of arguments (modus ponens
and modus tollens, respectively):
Major premise: If A is true, then B is true.
Minor premise: A is true.
Conclusion: Therefore, B is true.
and, conversely,
Major premise: If A is true, then B is true.
Minor premise: B is false.
Conclusion: Therefore, A is false.
For example, this kind of reasoning is used extensively in the proofs of mathematical theorems.
Although deductive reasoning plays an important role in science, it is unable to deal with un-
certainty, which is present whenever we deal with data. Instead, we need a set of rules that will
tell us how to reason when we are only able to assess the plausibilities of diﬀerent hypotheses.
For example, it is often necessary to rely on weaker arguments of the following types:
Major premise: If A is true, then B is true.
Minor premise: B is true.
Conclusion: Therefore, A becomes more plausible.
and
Major premise: If A is true, then B is true.
Minor premise: A is false.
Conclusion: Therefore, B become less plausible.
In each of these two cases, we are unable to draw any deﬁnitive conclusions from the information
at hand, i.e., it can certainly be the case that both A implies B is true and B is true, while A is
false, but knowing that B is true does make it less plausible or less likely that A is false, which
in turn makes A more plausible. In fact, in many problems, we would like to reason using even
weaker arguments than these, such as the following:
Major premise: If A is true, then B is more plausible.
Minor premise: B is true.
Conclusion: Therefore, A becomes more plausible.
Our challenge is to develop a theory of plausibilities which will allow us to rationally and con-
sistently calculation how much more plausible A is when the major and minor premises are
true.
2.3
Plausible Inference and Bayesian Probability Theory
In attempting to formulate a theory of logical probability, R. T. Cox introduced three desiderata
which were subsequently reﬁned by E. Jaynes. These can be stated as follows:

2.3. PLAUSIBLE INFERENCE AND BAYESIAN PROBABILITY THEORY
15
(I) Degrees of plausibility are represented by real numbers.
(II) Rationality: The plausibility of a proposition is an increasing continuous function of the
evidence in favor of that proposition. In particular, as the evidence favoring a proposition
increases, so does the plausibility of that proposition.
(III) Consistency:
• Structural consistency: If a conclusion can be reasoned out in more than one way,
every possible way must lead to the same plausibility.
• Propriety: Plausibilities are calculated using all available relevant information.
• Jaynes consistency: Equivalent states of knowledge have the same plausibilities, even
if they are represented in diﬀerent ways.
These are called desiderata because they describe some of the properties that we would like our
theory to satisfy, but they do not assert that anything is true. Our task in the remainder of this
section is to try to formulate a theory that has these properties. We begin with some notation.
If A and C are propositions, we will write (A|C) for the plausibility of A given C. In agreement
with desideratum (I), (A|C) is a real number.
2.3.1
The product rule
Suppose that A, B and C are propositions. Our ﬁrst task will be to investigate the plausibility of
the compound proposition A, B assuming that C is true, i.e., we would like to be able to calculate
(A, B|C) in terms of the plausibilities (A|C), (B|C), (A|B, C), (B|A, C). We ﬁrst observe that
since, in general, (A, B|C) depends on both A and B, it cannot be the case that (A, B|C) is
a function of just one of these four plausibilities. However, this leaves 11 possible relationships
which are listed below:
(A, B|C)
=
F1[(A|C), (B|C)]
(A, B|C)
=
F2[(A|C), (A|B, C)]
(A, B|C)
=
F3[(A|C), (B|A, C)]
(A, B|C)
=
F4[(B|C), (B|A, C)]
(A, B|C)
=
F5[(B|C), (A|B, C)]
(A, B|C)
=
F6[(A|B, C), (B|A, C)]
(A, B|C)
=
F7[(A|C), (B|C), (A|B, C)]
(A, B|C)
=
F8[(A|C), (B|C), (B|A, C)]
(A, B|C)
=
F9[(A|C), (A|B, C), (B|A, C)]
(A, B|C)
=
F10[(B|C), (A|B, C), (B|A, C)]
(A, B|C)
=
F11[(A|C), (B|C), (A|B, C), (B|A, C)].
Since A, B and B, A are equivalent propositions, we can immediately conclude that F2 = F4,
F3 = F5, F7 = F8, and F9 = F10, which reduces the number of possible functional relationships
to seven. We can then appeal to desideratum (II), which asserts that the plausibilities should be
in agreement with common sense, to exclude most of the remaining possibilities by constructing
suitable counter-examples. For example, to exclude F1, notice that we could either choose B = A,
in which case A, B = A so that by criterion (IIIc) we have (A, B|C) = (A|C), or we could choose
B = ¯A in which case A, B would always be false and have a smaller plausibility than A. However,
since F1 only depends on the plausibilities (A|C) and (B|C), it is unable to distinguish between
these cases even when they have diﬀerent plausibilities.

16
CHAPTER 2. PROBABILITY THEORY AND PLAUSIBLE INFERENCE
Likewise, to exclude F2 and F4, notice that if A is logically independent of B whenever C is true,
then (A|B, C) = (A|C) since knowing that B is true does not provide any additional evidence
for or against A. If the second case were true, then we would have (A, B|C) = F2[(A|C), (A|C)],
which cannot hold in general, since the left-hand side will generically depend on B while the
right-hand side has no dependence on B. Similarly, F6 can be excluded by taking B = A, since
then (A|B, C) = (A|B) and (B|A, C) = (B|A) in which case the plausibility (A, B|C), which
generically depends on C, could be written as a function F6[(A|B), (B|A)] which does not depend
on C, giving another contradiction.
Proceeding in this manner, it can be shown that the functions F7 = F8, F9 = F10, and F11 can
all be excluded, which only leaves the two equivalent identities
(A, B|C) = F[(A|C), (B|A, C)] = F[(B|C), (A|B, C)]
(2.1)
where we now write F for F2 = F4. Notice, however, that our arguments so far do not identify
which function this is, only that this function must depend on (A|C) and (B|A, C) or on (B|C)
and (B|A, C).
To learn more about the function F, let us consider a collection of four propositions A, B, C and
D and use the preceding results to calculate the plausibility (A, B, C|D). By associativity of the
conjunctions, we can write A, B, C = A, (B, C) in which case repeated application of (2.1) gives
(A, B, C|D)
=
(A, (B, C)|D)
=
F[(B, C|D), (A|B, C, D)]
=
F[F[(C|D), (B|C, D)], (A|B, C, D)].
Likewise, since A, B, C = (A, B), C, we also have
(A, B, C|D)
=
((A, B), C)|D)
=
F[(C|D), (A, B|C, D)]
=
F[(C|D), F[(B|C, D), (A|B, C, D)]].
For consistency to hold, both calculations should lead to same plausibility, which implies that
F[F[(C|D), (B|C, D)], (A|B, C, D)] = F[(C|D), F[(B|C, D), (A|B, C, D)]],
for any set of four propositions. If we write x = (C|D), y = (B|C, D) and z = (A|B, C, D), then
this identity implies that the function F must satisfy the following functional equation:
F[F[x, y], z] = F[x, F[y, z]]
(2.2)
for any three numbers x, y, and z which can be realized as plausibilities in the manner indicated.
Since A, B, C and D are arbitrary propositions, we expect the three numbers to be arbitrary
within the range of values that can be realized as plausibilities.
Equation (2.2) is known as the associativity equation. If we assume that F is a continuous
function of its arguments and that the arguments can take on any combination of values in some
open subset of R3, then it can be shown that there is a positive continuous monotonic real-valued
function w deﬁned on some interval [a, b] such that
w(F[x, y]) = w(x) · w(y).
This still does not tell us what F is, since the function w is not uniquely determined by (2.2),
but its does allow us rewrite the identities in (2.1) as
w(A, B|C)
=
w(A|C) · w(B|A, C)
=
w(B|C) · w(A|B, C),
(2.3)

2.3. PLAUSIBLE INFERENCE AND BAYESIAN PROBABILITY THEORY
17
which gives us a product rule for the quantities w(A|C), which are just monotonic transformations
of the quantities (A|C) that we originally called plausibilities.
We can obtain two numerical constraints on w by reasoning as follows. First suppose that A
is certain given C and that B, C is not necessarily false, i.e., B does not imply ¯C. Then, since
A, B|C = B|C and A|B, C = A|C (logical equivalence), the consistency condition (III) implies
that the plausibilities of these propositions must also be equal, i.e.,
(A, B|C)
=
(B|C)
(A|B, C)
=
(A|C).
Upon substituting these expressions into (2.3), we obtain
w(B|C) = w(B|C)w(A|C)
which, since the proposition B is (nearly) arbitrary, implies that w(A|C) = 1. In other words,
we require that w(·) = 1 whenever its argument is certain.
Similarly, suppose that A is impossible given C, i.e., C implies that ¯A is true. In this case, we
have A, B|C = A|C and A|B, C = A|C, which implies that
w(A|C) = w(B|C)w(A|C).
Since B still is an arbitrary proposition, it now follows that either w(A|C) = 0 or w(A|C) = ∞
whenever the argument is the plausibility of an impossible condition. This leaves us with two
possibilities:
1. w(x) is a positive, increasing function with values in [0, 1];
2. w(x) is a positive, decreasing function with values in [1, ∞].
Both choices are equally valid and indeed we can switch between them by taking reciprocals,
i.e., if w2(x) satisﬁes condition (2), then w1(x) ≡1/w2(x) will satisfy condition (1). We will
follow convention in adopting the ﬁrst choice, in which case w(·) is a continuous monotonic
function with values in the range [0, 1] and w(A|C) = 0 whenever A is impossible under C and
w(A|C) = 1 whenever A is certain under C.
2.3.2
The sum rule
Since any proposition A is true if and only if its negation ¯A is false, the plausibility of A should
be directly related to the plausibility of ¯A. In other words, there is a function S : [0, 1] →[0, 1]
such that
w( ¯A|C) = S(w(A|C)).
Likewise, since ¯¯A = A, it is also true that
w(A|C) = S(w( ¯A|C)) = S(S(w(A|C))),
which implies that S is an involution, i.e., S(S(x)) = x for every x ∈[0, 1]. In particular, since
w(A|C) = 1 and w( ¯A|C) = 0 whenever A is implied by C, it follows that S(1) = 0 and S(0) = 1.
If we assume that S is a continuous function on [0, 1], then it can be shown (Aczél 1966; Jaynes
2003) that
S(x) = (1 −xm)1/m

18
CHAPTER 2. PROBABILITY THEORY AND PLAUSIBLE INFERENCE
for some positive number m, which in turn implies that the quantities w(A|C) and w( ¯A|C)
satisfy a functional equation of the form of the form
wm(A|C) + wm( ¯A|C) = 1.
Notice that the powers wm(A, B|C) still obey the product rule
wm(A, B|C) = wm(A|C)wm(B|A, C).
Thus, if we deﬁne a new function p : [0, 1] →[0, 1] by setting
p(x) ≡wm(x),
then p() satisﬁes both the sum and the product rules,
p(A, B|C) = p(A|C)p(B|A, C) = p(B|C)p(A|B, C),
p(A|C) + p( ¯A|C) = 1,
and p(A|C) = 1 and p( ¯A|C) = 0 whenever A is implied by C. The quantity p(A|C) will be
called the conditional probability of A given C and will be denoted P(A|C).
The following theorem tells us how to calculate the probability of the disjunction of two propo-
sitions. This is sometimes called the extended sum rule.
Proposition 2.1. For any three propositions A, B and C, we have
P(A or B|C) = P(A|C) + P(B|C) −P(A, B|C).
Proof. Recalling that A or B = ¯A ∧¯B, repeated use of the product and sum rules shows that
P(A or B|C)
=
1 −P(A or B|C)
=
1 −P( ¯A, ¯B|C)
=
1 −P( ¯A|C)P( ¯B| ¯A, C)
=
1 −P( ¯A|C)(1 −P(B| ¯A, C))
=
1 −P( ¯A|C) + P( ¯A|C)P(B| ¯A, C)
=
1 −(1 −P(A|C)) + P( ¯A, B|C)
=
P(A|C) + P(B|C)P( ¯A|B, C)
=
P(A|C) + P(B|C)(1 −P(A|B, C))
=
P(A|C) + P(B|C) −P(B|C)P(A|B, C)
=
P(A|C) + P(B|C) −P(A, B|C).
Notice that if A and B are mutually exclusive, then P(A, B|C) = 0 and so
P(A or B|C) = P(A|C) + P(B|C).
In fact, a simple induction shows that a much stronger result is true.
Proposition 2.2. If A1, · · · , An are mutually exclusive propositions, then
P(A1 or · · · or An|C) =
n
X
i=1
P(Ai|C).
In other words, the probability of the disjunction of n mutually exclusive propositions is equal
to the sum of the probabilities of the individual propositions.

2.3. PLAUSIBLE INFERENCE AND BAYESIAN PROBABILITY THEORY
19
2.3.3
Bayes’ formula and plausible inference
We return to our earlier consideration of the role of weak syllogisms in plausible inference.
Suppose that A, B and I are propositions and that A implies B under I. Then P(B|A, I) = 1,
since B is true whenever A and I are both true, while P(B|I) ≤1. Substituting these results
into Bayes’ formula gives
P(A|B, I) = P(A|I)P(B|A, I)
P(B|I)
≥P(A|I),
which not only shows that the plausibility of A is increased by the knowledge that B is true, but
also allows us to quantify the size of the increase
P(A|B, I)
P(A|I)
= P(B|A, I)
P(B|I)
=
1
P(B|I) ≥1.
In particular, it follows that the increase in the plausibility of A is inversely proportional to
the plausibility of B under I, i.e., observations B that are unlikely in general provide stronger
evidence in favor of A than do observations that we would expect to make irrespective of whether
A is true or not.
Similarly, since it is also true that
P( ¯A|B, I) = 1 −P(A|B, I) ≤1 −P(A|I) = P( ¯A|I),
it follows from Bayes’ formula that
P(B| ¯A, I) = P(B|I)P( ¯A|B, I)
P( ¯A|I)
≤P(B|I),
which shows that the plausibility of B is decreased by the knowledge that A is false to a degree
that is proportional to
P(B| ¯A, I)
P(B|I)
= P( ¯A|B, I)
P( ¯A|I)
≤1.

Chapter 3
Sample Spaces, Events and Probabilities
3.1
Probability Spaces
Our focus throughout the much of the course will be on random variables and their applications.
Because we will be working extensively with the distributions of these variables and also because
we will sometimes want to take limits of probabilities, it will be convenient to be able to exploit
some of the mathematical tools provided by the measure-theoretic formulation of probability.
Although this approach is arguably less ‘intuitive’ than the Bayesian approach introduced ear-
lier, the measure theoretic approach has certain advantages when it comes to formulating and
proving mathematically rigorous statements concerning probabilities and random variables. In
this chapter, we brieﬂy introduce some of the basic ideas and concepts from measure-theoretic
probability, beginning with the deﬁnition of a probability space, which we recall from Section
1.2 of these notes.
Deﬁnition 3.1. A probability space is a triple {Ω, F, P} where:
• Ωis the sample space, i.e., the set of all possible outcomes of the experiment.
• F is a collection of subsets of Ωwhich we call events. F is called a σ-algebra and is
required to satisfy the following conditions:
1. The empty set and the sample space are both events: ∅, Ω∈F.
2. If E is an event, then its complement Ec = Ω\E is also an event.
3. If E1, E2, · · · are events, then their union ∪nEn is an event.
• P is a function from F into [0, 1]: if E is an event, then P(E) is the probability of E. P is
said to be a probability distribution or probability measure on F and is also required
to satisfy several conditions:
1. P(∅) = 0; P(Ω) = 1.
2. Countable additivity: If E1, E2, · · · are mutually exclusive events, i.e., Ei∩Ej =
∅whenever i ̸= j, then
P
 ∞
[
n=1
En

=
∞
X
n=1
P(En).
As we hint at in the deﬁnition, we can think of a probability space (Ω, F, P) as a mathematical
model for an experiment with a random outcome that lies in the sample space Ω. An event
E ⊂Ωis a set of possible outcomes which occurs with probability P(E) and F is the collection
20

3.1. PROBABILITY SPACES
21
of all possible events. For technical reasons that are discussed in Section (1.2), we sometimes
need to restrict the sets that belong to F. We now give several examples of probability spaces
and then compare the measure-theoretic and Bayesian formulations.
Example 3.1. Suppose that we toss a fair coin once and let Ω= {H, T} be the set of possible
outcomes, i.e., either H if we get heads or T if we get tails. Let F = {∅, {H}, {T}, Ω} be the
collection of all subsets of Ωand deﬁne the probability distribution P by setting
P(∅)
=
0
P({H}) = P({T})
=
1
2
P(Ω)
=
1.
Then (Ω, F, P) is a probability space which provides a model for the outcome of the coin toss.
Example 3.2. Now suppose that we toss a fair coin repeatedly and let N be the number of tosses
until we get the ﬁrst heads (including that toss). Let Ω= N = {1, 2, 3, · · · }, let F = P(Ω) be
the power set of Ω, i.e., the collection of all subsets of Ω, and deﬁne the probability of an event
E = {n1, n2, · · · } ⊂Ωby
P(E) ≡
X
ni∈E
1
2
ni
Then (Ω, F, P) is a probability space which provides a model for the the number of tosses that are
carried out until we get heads. The variable N is said to be a random variable.
Example 3.3. A probability space (Ω, F, P) is said to be a symmetric probability space if
the sample space Ω= {x1, · · · , xN} is ﬁnite; the σ-algebra F is the collection of all subsets of Ω,
i.e., F = P(Ω) is the power set of Ω; and the probability of any event E ⊂Ωis equal to
P(E) = |E|
|Ω| = |E|
N ,
where |E| is equal to the number of elements in E. In particular, it follows that the probability
of any single element x ∈Ωis equal to 1/N and so all elements are equally likely.
Example 3.4. Suppose that a deck contains 52 cards and let Ωbe the set containing all 52!
diﬀerent ways of ordering the deck. If we assume that the deck is randomly shuﬄed, then every
ordering of cards is equally likely and so we can use the symmetric probability space (Ω, F, P) as
a mathematical model for the eﬀect of shuﬄing on the order of the cards.
There are three main diﬀerences between the measure-theoretic formulation of probability and
the Bayesian logical formulation that we gave in Chapter 2.
• Measure-theoretic probabilities are deﬁned for sets (events), whereas logical probabilities
are deﬁned for logical propositions.
• Logical probabilities are conditional on other information, whereas measure-theoretic prob-
abilities are not in general.
• Logical probabilities are only required to be ﬁnitely additive, whereas measure-theoretic
probabilities are required to be countably additive.
Despite these diﬀerences, the two approaches agree in most respects if we identify each event in
the probability space with the proposition that asserts that that event occurs. Then we can also

22
CHAPTER 3. SAMPLE SPACES, EVENTS AND PROBABILITIES
identify complementation of sets with negation (e.g., Ec is identiﬁed with the proposition that
asserts that the event Ec occurs, which is logically equivalent to asserting that the event E does
not occur, i.e., the negation of the proposition asserting that E occurs), while the intersection
of two sets A ∩B corresponds to the assertion that both A and B occur, and the union A ∪B
corresponds to the proposition that asserts that either A or B (or both) occurs. The following
lemma states that measure-theoretic probabilities satisfy both the sum rule and the extended
sum rule.
Lemma 3.1. The following properties hold for any two events A, B in a probability space:
1. Complements: P(Ac) = 1 −P(A).
2. Subsets: if A ⊆B, then P(A) ≤P(B).
3. Disjoint events: If A and B are mutually exclusive, then
P(A ∩B) = 0.
4. Unions: For any two events A and B (not necessarily mutually exclusive), we have:
P(A ∪B) = P(A) + P(B) −P(A ∩B).
Proof. Since Ωis the disjoint union of any event A and its complement, it follows from the
countable additivity of P that
1 = P(Ω) = P(A ∪Ac) = P(A) + P(Ac).
To prove (2), suppose that A, B are events with A ⊂B. Then B is the disjoint union of A and
B \ A and so
P(B) = P(A) + P(B \ A) ≥P(A)
since P(B \ A) ≥0. The third result follows from the fact that A ∩B = ∅if and only if A and
B are mutually exclusive and P(∅) = 0. Finally, to prove (4), notice that the following identities
hold:
P(A)
=
P(A \ B) + P(A ∩B)
P(B)
=
P(B \ A) + P(A ∩B)
P(A ∪B)
=
P(A \ B) + P(B \ A) + P(A ∩B)
=
P(A \ B) + P(A ∩B) + P(B \ A) + P(A ∩B) −P(A ∩B)
=
P(A) + P(B) −P(A ∩B).
3.2
Continuity Properties of Probability Measures
One of the advantages of the measure-theoretic formulation of probability is that the countable
additivity property can be used to take the limit of the probabilities of certain sequences of
events. We ﬁrst recall what it means for a sequence of real numbers to converge to a limit.

3.2. CONTINUITY PROPERTIES OF PROBABILITY MEASURES
23
Deﬁnition 3.2. A sequence of real numbers x1, x2, · · · is said to converge to a limit x if for
every positive real number ϵ > 0, we can ﬁnd an integer N such that the diﬀerence |xn −x| is
less than ϵ whenever n ≥N. When this is true, we write xn →x or, more formally,
x = lim
n→∞xn.
The intuition behind this deﬁnition is that as n increases, the terms xn in a convergent sequence
should approach the limit arbitrarily closely and then remain close to that limit.
Example 3.5. If xn = n−1
n , then the sequence x1, x2, · · · converges to the limit x = 1. Indeed,
if ϵ > 0 is a positive real number and we take N to be any positive integer greater than 1/ϵ, then
for any integer n ≥N, we have
|x −xn| =
1 −n −1
n
 = 1
n ≤1
N < ϵ.
Deﬁnition 3.3. A sequence of nested sets {En; n ≥1} is said to be increasing if
E1 ⊂E2 ⊂E3 · · · ,
in which case we deﬁne
lim
n→∞En =
[
n≥1
En.
Similarly, a sequence of sets is said to be decreasing if
E1 ⊃E2 ⊃E3 · · · ,
and then we deﬁne
lim
n→∞En =
\
n≥1
En.
Notice that in both cases the sets En are nested.
Theorem 3.1. Suppose that {En; n ≥1} is either an increasing or decreasing sequence of sets.
Then,
lim
n→∞P(En) = P( lim
n→∞En).
Proof. Suppose that E1, E2, · · · is an increasing sequence of events and observe that this implies
that
En =
n[
k=1
Ek
(3.1)
for every n ≥1. We would like to use countable additivity to prove the result (since this is
the only tool currently at our disposal that involves the probability of countably inﬁnitely many
events) and to this end we will recursively deﬁne a new collection of events:
B1
=
E1
B2
=
E2\E1
B3
=
E3\E2
· · ·
· · ·
Bn
=
En\En−1.

24
CHAPTER 3. SAMPLE SPACES, EVENTS AND PROBABILITIES
We ﬁrst observe that the events B1, B2, · · · are disjoint. Indeed, given any two positive integers
k < n, notice that since Bn is disjoint from En−1 and since Ek is a subset of En−1, Bn is disjoint
from Ek. Furthermore, since Bk is itself a subset of Ek (look at how Bk is deﬁned), it follows
that Bn is disjoint from Bk, as claimed above.
I also claim that
n[
k=1
Bk =
n[
k=1
Ek
(3.2)
for every integer n ≥1. We can prove this by induction on n. First, notice that since B1 = E1,
this identity is true by deﬁnition when n = 1. Now suppose that it is true for an arbitrary value
of n. Then
n+1
[
k=1
Bk
=
Bn+1 ∪
 n[
k=1
Bk
!
=
En+1\En ∪
 n[
k=1
Bk
!
=
En+1\En ∪
 n[
k=1
Ek
!
=
En+1 ∪
 n[
k=1
Ek
!
=
n+1
[
k=1
Ek,
which completes the induction step. Notice that it immediately follows that
∞
[
k=1
Bk =
∞
[
k=1
Ek.
(3.3)
Consequently,
lim
n→∞P(En)
=
lim
n→∞P
 n[
k=1
Ek
!
by (1.1)
=
lim
n→∞P
 n[
k=1
Bk
!
by (1.2)
=
lim
n→∞
n
X
k=1
P(Bk)
since the Bk are disjoint
=
∞
X
k=1
P(Bk)
=
P
 ∞
[
k=1
Bk
!
by countable additivity
=
P
 ∞
[
k=1
Ek
!
by (1.3),
which completes the proof. DeMorgan’s laws can then be used to deduce the result for a de-
creasing sequence of events from the increasing case.
(To see this, notice that if E1, E2, · · ·
is a decreasing sequence of events, then their complements Ec
1, Ec
2, · · · form an increasing se-
quence.)

3.3. CONDITIONAL PROBABILITIES
25
The following corollary is an important special case of Theorem 3.1.
Corollary 3.1. Suppose that {En; n ≥1} is a decreasing sequence of events with limn→∞En = ∅.
Then limn→∞P(En) = 0.
Caveat: In general, the set limn→∞En is not deﬁned for an arbitrary sequence of events and
the sequence P(En) need not converge.
3.3
Conditional Probabilities
As mentioned above, the measure-theoretic formulation of probability diﬀers from the Bayesian
logical formulation in its treatment of conditional probabilities: whereas the Bayesian approach
begins with conditional probabilities, the measure-theoretic approach starts with the deﬁnition
of a probability space and only deﬁnes conditional probabilities secondarily.
Deﬁnition 3.4. Let E and F be events and assume that P(F) > 0. Then the conditional
probability of E given F is deﬁned by the ratio
P(E|F) = P(E ∩F)
P(F)
.
Example 3.6. Suppose that we roll a fair six-sided die and let E be the event that it lands on an
even number and let F be the event that it lands on a number greater than or equal to 4. Then
the conditional probability of E given F is
P(E|F) = P(E ∩F)
P(F)
= 2/6
3/6 = 2
3.
Geometric analogy: Suppose that we throw darts at a target Ωand for each E ⊂Ωlet the
probability P(E) that we hit the region E be proportional to the area of E. If F ⊂Ω, then
the conditional probability P(E|F) that we hit the region E given that we hit the region F is
proportional to the area of E ∩F divided by the area of F, i.e., P(E|F) = P(E ∩F)/P(F).
Frequentist interpretation: Suppose that we perform a series of independent, identical trials
that result in outcomes that can belong to E or F or both E ∩F. Deﬁne
nE(N)
=
the number of times that outcome E occurs in the ﬁrst N trials;
nF (N)
=
the number of times that outcome F occurs in the ﬁrst N trials;
nE∩F (N)
=
the number of times that outcome E ∩F occurs in the ﬁrst N trials.
If we interpret probabilities as limiting frequencies, then
P(E)
=
lim
N→∞nE(N)/N
P(F)
=
lim
N→∞nF (N)/N
P(E ∩F)
=
lim
N→∞nE∩F (N)/N.
Furthermore, for a frequentist, the conditional probability will be equal to the limiting frequency

26
CHAPTER 3. SAMPLE SPACES, EVENTS AND PROBABILITIES
of trials that result in both E and F out of those trials that at least result in F, i.e.,
P(E|F)
=
lim
N→∞
nN(E ∩F)
nN(F)
=
lim
N→∞
nN(E ∩F)/N
nN(F)/N
=
limN→∞nE∩F (N)/N
limN→∞nF (N)/N
=
P(E ∩F)
P(F)
.
F can be thought of as some additional piece of information that we have concerning the outcome
of an experiment. The conditional probability P(E|F) then summarizes how this additional in-
formation aﬀects our belief that the event E also has occurred. Notice that the ratio appearing
in the deﬁnition is only well-deﬁned if the numerator is positive, hence the requirement that
P(F) > 0. Furthermore, deﬁnition (3.4) leads immediately to the product rule.
Proposition 3.1. (Product Rule) Suppose that E and F are events and that P(F) > 0 and
P(E) > 0. Then
P (E ∩F)
=
P(F) · P (E|F)
=
P(E) · P(F|E).
Remark: We have now shown that measure-theoretic probabilities satisfy both the sum rule
(Lemma 3.1) and the product rule (Prop. 3.1). This implies that if we use the measure theoretic
formulation to carry out calculations involving probabilities, then our results should agree with
those obtained using the Bayesian formulation except possibly when those calculations require
us to take limits.
The law of total probability is one of the most important and useful consequences of the product
rule. Before we state this law, recall that we say that the sets F1, · · · , Fn are mutually exclusive
if every pairwise intersection Fi ∩Fj = ∅when i ̸= j.
Proposition 3.2. (Law of Total Probability) Suppose that F1, · · · , Fn are mutually exclusive
events such that P (Fi) > 0 for each i = 1, · · · , n and E ⊂F1 ∪· · · ∪Fn. Then
P(E)
=
n
X
i=1
P (E ∩Fi)
=
n
X
i=1
P (E|Fi) · P (Fi) .
Proof. The result follows from the countable additivity of P and the fact that E is the disjoint
union of the sets E ∩Fi for i = 1, · · · , n.
Example 3.7. Let µ(y) be the annual mortality rate for an individual of age y years and suppose
that the age distribution in the population (in years) is given by the function p(y) where y ranges
from 0 to 130. If an individual is chosen at random from that the population and D is the event
that they will die in that particular year and Y = y is the event that they are y years old, then
P(D) =
130
X
y=0
P(D|Y = y) · P(Y = y) =
130
X
y=0
µ(y) · p(y).

3.3. CONDITIONAL PROBABILITIES
27
We end this chapter by discussing what it means for two or more sets to be independent of each
other.
Deﬁnition 3.5. Two events E and F in a probability space (Ω, F, P) are said to be independent
if
P(E ∩F) = P(E)P(F).
To relate this deﬁnition to the logical notion of independence, notice that if P(E) > 0 and
P(F) > 0, then
P(E|F)
=
P(E ∩F)
P(F)
= P(E)P(F)
P(F)
= P(E)
P(F|E)
=
P(E ∩F)
P(E)
= P(E)P(F)
P(E)
= P(F).
In other words, if P(E), P(F) > 0, then E and F are independent if and only if conditioning on
one does not change the probability that the other is true.
The deﬁnition of independence when there are more than two events is more complicated.
Deﬁnition 3.6. A countable collection of events, E1, E2, · · · , is said to be independent if for
every ﬁnite sub-collection Ei1, Ei2, Ei3, · · · , Ein, we have:
P
 n\
r=1
Eir
!
=
n
Y
r=1
P(Eir).
This too can be interpreted in terms of conditional probabilities. For example, if E1, E2, · · · are
independent and if Ei1, Ei2, · · · , Ein and Ej1, Ej2, · · · , Ejm are two non-overlapping collections
of sets (i.e., no set occurs in both collections), then
P
 n\
r=1
Eir

m
\
s=1
Ejs
!
= P
 n\
r=1
Eir
!
.
In other words, knowing that the events Ej1, · · · , Ejm are all true does not change the probability
of the event Ei1 ∩· · · ∩Ein.
Remark: Although independence implies pairwise independence, the following example demon-
strates that the converse is not true.
Example 3.8. Pairwise independence does not imply independence: Suppose that two
fair dice are tossed and let E1 be the event that the ﬁrst die equals 4, let E2 be the event that
the second die equals 4, and let E3 be the event that the sum of the dice is 7. Then each pair of
events is independent, but it is not the case that all three events are independent. Veriﬁcation of
these statements is left as an exercise in PS2.
In some cases, a collection of events will be independent if we condition on another event. This
leads to the following deﬁnition.
Deﬁnition 3.7. Two events E and F are said to be conditionally independent given a third
event C if
P(E ∩F|C) = P(E|C) · P(F|C).

28
CHAPTER 3. SAMPLE SPACES, EVENTS AND PROBABILITIES
Similarly, a collection of events E1, E2, · · · is said to be conditionally independent given C if for
every ﬁnite sub-collection Ei1, Ei2, Ei3, · · · , Ein, we have:
P
 n\
r=1
Eir
C
!
=
n
Y
r=1
P(Eir|C).
As the following examples demonstrate, there is no simple relationship between independent and
conditional independence. In particular, two sets that are independent need not be conditionally
independent given a third set, while two sets that are conditionally independent need not be
independent.
Example 3.9. Suppose that we toss a fair coin twice and that the tosses are independent of one
another. If we let A be the event that the ﬁrst toss lands on heads and we let B be the event that
the second toss lands on heads, then A and B are independent since
1
4 = P(A, B) = P(A) · P(B) = 1
2 · 1
2.
Now let C be the event that exactly one toss lands on heads. Since this is equally likely to be the
ﬁrst or the second toss, it is clear that P(A|C) = P(B|C) = 1/2. However,
0 = P(A, B|C) ̸= P(A|C) · P(B|C),
which shows that A and B are not conditionally independent given C.
Example 3.10. Suppose that we have two urns and that the ﬁrst urn contains two white balls
and one black ball while the second urn contains two black balls and one white ball. We choose
one of the urns at random (with each urn equally likely to be chosen) and we then sample two
balls from that urn at random and with replacement. Let A be the event that the ﬁrst ball sampled
is white, let B be the event that the second ball sampled is white, and let C be the event that we
selected the ﬁrst urn. In this case, A and B are conditionally independent given C since
4
9 = P(A, B|C) = P(A|C) · P(B|C) = 2
3 × 2
3.
However, the following calculation shows that A and B are not independent. By the law of total
probability,
P(A)
=
P(A|C) · P(C) + P(B|C) · P(C)
=
2
3 × 1
2 + 1
3 × 1
2
=
1
2
and similarly P(B) = 1/2. However,
P(A, B)
=
P(A, B|C) · P(C) + P(A, B|C) · P(C)
=
4
9 × 1
2 + 1
9 × 1
2
=
5
18
which shows that P(A, B) ̸= P(A) · P(B).

Chapter 4
Random Variables and Probability Distributions
4.1
Random Variables
Imagine that an experiment is performed which results in a random outcome ω belonging to a set
Ω. We saw in the last chapter that we could model such a scenario by introducing a probability
space (Ω, F, P) in which Ωis the sample space, F is a collection of events in Ω, and P is a
distribution which speciﬁes the probability P(A) of each event A ∈F. However, depending on
the size and the complexity of the sample space as well as on the experimentalist’s resources, it
may not be possible to directly observe ω. Instead, what the experimentalist usually can do is to
measure some quantity X = X(ω) that depends on ω and which reveals some information about
the outcome. Because the value of X depends on the outcome of the experiment, which is ran-
dom, we say that X is a random variable. Random variables play a central role in probability
theory, statistics, and stochastic modeling and so this entire chapter is devoted to their properties.
Deﬁnition 4.1. Suppose that (Ω, F, P) is a probability space and that E is a set equipped with
a σ-algebra E of events. A function X : Ω→E is said to be an E-valued random variable if
for every set A ∈E we have
X−1(A) = {ω ∈Ω: X(ω) ∈A} = {X ∈A} ∈F.
In other words, for each set A ∈E, we must be able to decide whether the event {X ∈A} is true
or not.
Here E can be a set of objects of any type, including integers, real numbers, complex numbers,
vectors, matrices, functions, geometric objects, graphs, networks, and even probability distribu-
tions themselves. For example, if E is a set of matrices, then we would say that an E-valued
random variable X is a random matrix.
This level of generality allows us to apply random
variables to problems coming from many diﬀerent disciplines. However, most often E is a subset
of the real numbers, in which case we would say that X is a real-valued random variable.
In fact, for many authors random variables are synonymous with real-valued random variables.
Two examples follow.
Example 4.1. Suppose that we a toss a fair coin one hundred times and record the outcome of
each toss as H or T. We can model this experiment using a symmetric probability space (Ω, F, P)
in which Ωis the collection of sequences of length one hundred in which each element is either
H or T, F is the power set of Ω, and the probability of each outcome ω = (ω1, · · · , ω100) is
P({ω}) =
1
2100 .
29

30
CHAPTER 4. RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
Because Ωis a very large set, we might choose to restrict our attention to the number of tosses
that land on heads, which we can represent by a random variable X : Ω→E = {0, 1, · · · , 100},
where
X(ω) = #{i : ωi = H},
and we take E to be the power set of E. Notice that because F is the power set of Ω, every set
{X ∈A} belongs to F when A ∈E.
Example 4.2. Suppose that a fair die is rolled twice and that we record the outcome of each roll.
We can model this experiment with a symmetric probability space (Ω, F, P) where Ω= {(i, j) :
1 ≤i, j ≤6}, F is the power set of Ω, and the probability of each outcome ω = (ω1, ω2) is
P({ω}) = 1
36.
In this case, the sum of the two numbers rolled is a random variable X : Ω→E = {2, · · · , 12}
deﬁned by
X(ω) = ω1 + ω2.
Similarly, the product, the diﬀerence and the quotient of the two numbers rolled are all random
variables on Ω.
Arguably, the most important attribute of a random variable is its distribution, which tells us
the probabilities of events involving this variable.
Deﬁnition 4.2. Let X be an E-valued random variable deﬁned on a probability space (Ω, F, P).
Then the distribution of X is a probability distribution PX : E →[0, 1] on the space (E, E)
deﬁned by the formula
PX(A) = P(X ∈A)
whenever A ∈E. Furthermore, the triple (E, E, PX) is itself a probability space.
Example 4.3. Let X be the sum obtained when a fair die is rolled twice. Then the distribution
of X on the set E = {2, 3, · · · , 12} is determined by the following probabilities:
i
2
3
4
5
6
7
8
9
10
11
12
PX(i)
1/36
2/36
3/36
4/36
5/36
6/36
5/36
4/36
3/36
2/36
1/36
Furthermore, the probability of any subset A ⊂E can be calculated by summing the probabilities
of the elements of A:
PX(A) =
X
i∈A
PX(i).
Indeed, X is an example of a discrete random variable and the table shows the values of the
probability mass function of X (see below).
Remark: Although random variables are formally deﬁned as functions on probability spaces,
it is common practice to suppress all mention of the underlying probability space (Ω, F, P) and
instead only work with the variable X and its distribution PX on (E, E). This allows us to focus
on quantities that we can actually observe and measure. This will be the convention followed
in these lecture notes except in a few places where it will be convenient to use the underlying
probability space as a mathematical tool to prove results about random variables.

4.1. RANDOM VARIABLES
31
In the special case where X is a real-valued random variable, the distribution of X can be
summarized by a function called the cumulative distribution function of X (abbreviated c.d.f.).
This is useful because real-valued functions are often less complicated objects than probability
distributions deﬁned on σ-algebras on the real numbers. This is particularly true of continuous
random variables, which can assume any value in an interval.
Deﬁnition 4.3. If X is a real-valued random variable, then the cumulative distribution
function of X is the function FX : R →[0, 1] deﬁned by
FX(x) ≡P(X ≤x) = P

X ∈(−∞, x]

for any x ∈R.
Notice that FX(x) is said to be the cumulative distribution function of X because it measures
the cumulative probability of X from −∞up to x.
It is clear that the cumulative distribution function of a random variable is uniquely determined
by the distribution of the variable.
However, less obviously, the converse is also true: the
distribution of a random variable is uniquely determined by its cumulative distribution function.
In other words, if X and Y have the same c.d.f., i.e., if FX(x) = FY (x) for every x ∈R, then
X and Y have the same distribution: P(X ∈A) = P(Y ∈A) for every ‘nice’ subset A ⊂R.
Section (4.1.3) describes several other important properties of cumulative distribution functions.
We defer examples to the next two sections.
Having specialized to real-valued random variables, we now specialize further. Namely, in this
course we will mainly focus on two classes of real-valued random variables: discrete and continu-
ous, which are deﬁned in the next two sections. Random variables which are neither discrete nor
continuous also exist (e.g., a random variable with the Cantor distribution, which is supported
on the Cantor middle thirds set, is neither discrete nor continuous), but discrete and continuous
random variables are the ones most commonly encountered in applications.
4.1.1
Discrete random variables
Recall that a set E is said to be countable if it is either ﬁnite or countably inﬁnite. In the
latter case, E can be put in to one-to-one correspondence with the natural numbers, i.e., we can
enumerate the elements of E = {x1, x2, x3, · · · }. Examples of countably inﬁnite sets include the
integers, the non-negative even integers and the rationals. An example of a set that is inﬁnite
but not countably inﬁnite is the interval [0, 1]; this was ﬁrst established by Georg Cantor using
his famous diagonalization argument.
Deﬁnition 4.4. A real-valued random variable X is said to be discrete if there is a countable
set of values E = {x1, x2, · · · } ⊂R such that P(X ∈E) = 1. In this case, the probability mass
function of X is the function pX : E →[0, 1] deﬁned by
pX(y) = P(X = y).
The next proposition shows how the probability mass function (p.m.f.) of a discrete random
variable is related to the distribution of that variable. In particular, it follows that if two dis-
crete random variables have the same p.m.f., then they necessarily have the same distribution.

32
CHAPTER 4. RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
Proposition 4.1. Suppose that X is a discrete random variable with values in E and let pX be
the probability mass function of X. If A ⊂E, then
P(X ∈A) =
X
y∈A
pX(y),
i.e., the probability that X belongs to A is equal to the sum of the probability mass function
evaluated at all of the elements of A.
Proof. Since A can be written as a countable disjoint union of the singleton sets containing the
elements contained in A, i.e.,
A =
[
y∈A
{y},
we can use the countable additivity of probability distributions to conclude that
P(A) =
X
y∈A
P({y}) =
X
y∈A
pX(y).
In particular, it follows that the sum of the probability mass function over all of the points
contained in E is equal to 1:
1
=
P(Ω)
=
X
xi∈E
pX(xi).
We can also express the cumulative distribution function of a discrete random variable X in
terms of its probability mass function.
FX(x)
≡
P(X ≤x)
=
X
xi≤x
pX(xi).
This shows that the probability mass function of a discrete random variable with values in the
set E = {x1, x2, · · · } is a step function which is piecewise constant on the intervals [xi, xi+1)
with a jump of size pX(xi) at x = xi. (See Figure 5.1 in Gregory (2005).)
Example 4.4. A random variable X is said to be a Bernoulli random variable with parameter
p ∈[0, 1] if X takes values in the set E = {0, 1} and the p.m.f. of X is equal to
pX(x) =
 1 −p
if x = 0
p
if x = 1.
We also say that X has a Bernoulli distribution with parameter p. Notice that the cumulative
distribution function of X is the following step function
FX(x) =



0
if x < 0
1 −p
if x ∈[0, 1)
1
if x ≥1.
Bernoulli random variables are often used to describe trials that can result in just two possible
outcomes, e.g., a coin toss experiment can be represented by a Bernoulli random variable if we
assign X = 1 whenever we get heads and X = 0 whenever we get tails.

4.1. RANDOM VARIABLES
33
4.1.2
Continuous random variables
Deﬁnition 4.5. A real-valued random variable X is said to be continuous if there is a function
fX : R →[0, ∞], called the probability density function of X, such that
P(a ≤X ≤b) =
Z b
a
fX(x)dx
for every −∞≤a ≤b ≤∞.
If a random variable X is continuous, then its probability density function (or density for short)
can be calculated by evaluating the following limit:
fX(x) =
lim
δx→0+
P(x ≤X ≤x + δx)
δx
.
(4.1)
This shows that the probability density fX(x) evaluated at a point x is not equal to the proba-
bility that X = x, but instead is the limit of the ratio of two probabilities as each tends to zero.
This explains why densities can take on values that are greater than 1. On the other hand, there
is a one-to-one relationship between the probability density function of a continuous random
variable and the distribution of that variable. Our next proposition should be compared with
Proposition 4.2.
Proposition 4.2. Suppose that X is a continuous random variable with density fX. Then
P(X ∈A) =
Z
A
fX(x)dx
for any subset A ⊂R for which the integral is well-deﬁned.
The collection of subsets A ⊂R for which the integral is well-deﬁned is known as the Lebesgue
σ-algebra and includes all countable unions and intersections of closed or open intervals. See
Williams (1991) for a careful discussion of these issues. By taking A = R, it follows that that
integral of the density over the entire real line is equal to 1:
1 = P(X ∈R) =
Z ∞
−∞
fX(x)dx.
On the other hand, if we take A = {y} for any y ∈R, then
P(X = y) =
Z y
y
fX(x)dx = 0,
which shows that a continuous random variable has no atoms. This highlights an observation
that may seem paradoxical at ﬁrst, which is that events with probability zero are not, in general,
impossible. In this case, we know that the random variable Y will take on some value y even
though the probability of this event is zero: P(Y = y) = 0.
Proposition (4.2) also shows how the probability density function and the cumulative distribution
function are related. Since this relationship is so important in practice, we will state it as a
corollary.
Corollary 4.1. Suppose that X is a continuous random variable with density fX(x) and c.d.f.
FX(x). Then FX is the antiderivative of fX, while fX is the derivative of FX:
FX(x)
=
P(X ≤x)
=
Z x
−∞
fX(y)dy,
fX(x)
=
d
dxFX(x).

34
CHAPTER 4. RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
Thus, if we know the density of a random variable, then we can ﬁnd its cumulative distribution
function by integration, while if we know the cumulative distribution function, then we can ﬁnd
the density function by diﬀerentiation.
Example 4.5. Let X be a continuous random variable with probability density function
fX(x) =
 Cxα
if x ∈[0, 1]
0
if x < 0 or x > 1.
Here α > −1 is a parameter which characterizes the distribution, while C is a constant which
must be chosen so that the density integrates to 1:
1 =
Z 1
0
Cxαdx =
C
α + 1.
In other words, we must take C = α+1 to get a probability density. Notice that we must also take
α > −1 since otherwise the integral diverges, but values of α ∈(−1, 0) are perfectly legitimate
even though the density function diverges as x →0.
4.1.3
Properties of the Cumulative Distribution Function
We have seen that the cumulative distribution function of a random variable can be either
discontinuous or continuous depending on whether or not there are particular values that the
random variable assumes with positive probability. However, there are several properties that
are shared by every cumulative distribution function.
Proposition 4.3. Let X be a real-valued random variable (not necessarily discrete) with cumu-
lative distribution function F(x) = P(X ≤x). Then
1. F is non-decreasing, i.e., if x ≤y, then F(x) ≤F(y).
2. limx→∞F(x) = 1.
3. limx→−∞F(x) = 0.
4. F is right-continuous, i.e., for any x and any decreasing sequence (xn, n ≥1), that con-
verges to x, limn→∞F(xn) = F(x).
Proof. 1.) If x ≤y, then {X ≤x} ⊂{X ≤y}, which implies that F(x) ≤F(y).
2.) If (xn, n ≥1) is an increasing sequence such that xn →∞, then the events En = {X ≤xn}
form an increasing sequence with
{X < ∞} =
∞
[
n=1
En.
It follows from the continuity properties of probability measures (Lecture 2) that
lim
n→∞F(xn) = lim
n→∞P(En) = P(X < ∞) = 1.
3.)
Likewise, if (xn, n ≥1) is a decreasing sequence such that xn →−∞, then the events
En = {X ≤xn} form a decreasing sequence with
∅=
∞
\
n=1
En.

4.2. EXPECTATIONS AND MOMENTS
35
In this case, the continuity properties of measures imply that
lim
n→∞F(xn) = lim
n→∞P(En) = P(∅) = 0.
4.) If xn, n ≥1 is a decreasing sequence converging to x, then the sets En = {X ≤xn} also form
a decreasing sequence with
{X ≤x} =
∞
\
n=1
En.
Consequently,
lim
n→∞F(xn) = lim
n→∞P(En) = P{X ≤x} = F(x).
Some other useful properties of cumulative distribution functions are listed below.
(i) P(a < X ≤b) = F(b) −F(a) for all a < b.
(ii) Another application of the continuity property shows that the probability that X is strictly
less than x is equal to
P(X < x)
=
lim
n→∞P(X ≤x −1/n)
=
lim
n→∞F(x −1/n).
In other words, P(X < x) is equal to the left limit of F at x, which is often denoted F(x−).
However, notice that in general this limit need not be equal to F(x):
F(x) = P(X < x) + P(X = x),
and so P(X < x) = F(x) if and only if P(X = x) = 0.
(iii) The converse of this proposition is also true: namely, if F : R →[0, 1] is a function satisfying
properties (1) - (4), then F is the CDF of some random variable X. We will prove a partial
result in this direction later on in the course.
4.2
Expectations and Moments
Deﬁnition 4.6. Suppose that X is a real-valued random variable which is either discrete with
probability mass function pX(x) or continuous with probability density function fX(x). Then the
expected value of X is the weighted average of the values of X given by
E[X] =
(P
xi pX(xi) xi
if X is discrete,
R ∞
−∞fX(x) x dx
if X is continuous,
provided that the sum or the integral exists.
The expected value of a random variable X may also be called the expectation, the mean or
the ﬁrst moment of X and is sometimes denoted ⟨X⟩. The latter notation is especially common
in the physics and engineering literature and is also used by Gregory (2005).

36
CHAPTER 4. RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
Example 4.6. Let X be a Bernoulli random variable with parameter p (cf. Example 4.3). Then
the expected value of X is
E[X] = (1 −p) · 0 + p · 1 = p.
Example 4.7. Let X be the outcome when we roll a fair die. Then
E[X] =
6
X
i=1
1
6 · i = 3.5.
Example 4.8. Let X be the random variable described in Example 4.2., i.e., X is continuous
with density (α + 1)xα for x ∈[0, 1] where α > −1. Then the expected value of X is equal to
E[X] =
Z 1
0
(α + 1)xα · xdx = α + 1
α + 2.
Notice that in the ﬁrst two examples, the random variable X cannot be equal to its expected
value. In this sense, the name ‘expected value’ is somewhat misleading and it could be argued
that it would be better to refer to E[X] as the mean of X. On the other hand, we will later see
that if X1, X2, · · · are independent, identically-distributed real-valued random variables with the
same distribution as X, then
lim
N→∞
1
N
N
X
i=1
Xi = E[X]
provided that X has a ﬁnite mean, i.e., the sample mean converges to the expected value as the
size of the sample increases. This result is known as the strong law of large numbers.
One way to obtain new random variables is to compose a real-valued function with an existing
random variable. For example, suppose that X is a discrete random variable that takes values in
the set E = {x1, x2, . . . } with probabilities pX (xi), and let g : E →R be a real-valued function.
Then Y = g(X) is a discrete random variable with values in the countable set g(E) = {y1, y2, · · · }
and the probability mass function of Y is
pY (yi) ≡P (Y = yi) =
X
xj:g(xj)=yi
pX(xj).
Notice that more than one value xi may contribute to the probability of each point yj if g is not
one-to-one. Similarly, if X is a continuous random variable and g : R →R is a function, then
Y = g(X) is also a real-valued random variable, although it need not be continuous.
Example 4.9. Suppose that X is a discrete random variable that takes values in the set {−1, 0, 1}
with probabilities
pX(−1) = 1
4,
pX(0) = 1
2,
pX(1) = 1
4.
Then Y = X2 is a discrete random variable that takes values in the set {0, 1} with probabilities
pY (0) = pX(0) = 1
2
pY (1) = pX(−1) + pX(1) = 1
2
and so
E

X2
= 1
2 · 0 + 1
2 · 1 = 1
2.

4.2. EXPECTATIONS AND MOMENTS
37
The following proposition allows us to calculate the expectation E[g(X)] without having to
ﬁrst ﬁnd the probability mass function or density function of the random variable Y = g(X).
Although the identities contained in the proposition appear as a deﬁnition in Gregory (2005), in
fact, these must be derived from Deﬁnition 4.6. We will prove the result under the assumption
that X is discrete; see Williams (1991) for a proof of the general case.
Proposition 4.4. Suppose that X is a real-valued random variable which is either discrete with
probability mass function pX(x) or continuous with probability density function fX(x). Then, for
any real-valued function g : R →R,
E[g(X)] =
(P
xi pX(xi)g(xi)
if X is discrete,
R ∞
−∞fX(x)g(x) dx
if X is continuous,
provided that the sum or the integral exists.
Proof. Suppose that X is a discrete random variable with values in the set E = {x1, x2, · · · }
and probability mass function pX. If Y = g(X), then Y is also a discrete random variable which
takes on distinct values {y1, y2, · · · } with probabilities pY (yj) and we can deﬁne
Ej ≡g−1 (yj) = {xi ∈E : g(xi) = yj} .
Then the sets E1, E2, · · · are disjoint with union E = E1 ∪E2 ∪· · · , and
E[g(X)]
=
X
j
pY (yj) · yj
=
X
j

X
xi∈Ej
pX(xi)

· yj
=
X
j
X
xi∈Ej
pX(xi) · yj
=
X
j
X
xi∈Ej
pX(xi) · g(xi)
=
X
xi∈E
pX(xi) · g(xi).
An important consequence of Proposition 4.4 is that expectations are linear.
Corollary 4.2. If a and b are constants, then
E[aX + b] = aE[X] + b.
Proposition 4.4 can also be used to calculate the moments of a real-valued random variable.
Deﬁnition 4.7. The n’th moment of a real-valued random variable X is the quantity
µ′
n = E[Xn] =
(P
xi pX(xi)xn
i
if X is discrete,
R ∞
−∞fX(x)xn dx
if X is continuous,
provided that the sum or the integral exists. Similarly, if µ = E[X] is ﬁnite, then the n’th central
moment of X is the quantity
µn = E[(X −µ)n] =
(P
xi pX(xi)(xi −µ)n
if X is discrete,
R ∞
−∞fX(x)(x −µ)n dx
if X is continuous,
provided that the sum or the integral exists.

38
CHAPTER 4. RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
Notice that the ﬁrst central moment vanishes, since the linearity of expectations implies that
µ1 = E[X −µ] = µ −µ = 0.
On the other hand, the second, third and fourth central moments are suﬃciently important in
statistics and applied probability to warrant special names. The second central moment is the
most familiar and is called the variance of the random variable. The variance is usually denoted
σ2 and can be evaluated using the following formula:
σ2
=
E[(X −µ)2]
=
E[X2 −2µX + µ2]
=
E[X2] −2µ2 + µ2
=
E[X2] −µ2.
The square root of the variance is called the standard deviation. This is denoted σ and has
the same units as X, whereas the variance has the units of X2, e.g., if the units of X are cm,
then the units of the standard deviation are also cm, whereas the units of the variance are cm2.
The third and fourth central moments are also useful in some instances. The third moment is
known as the skewness and measures the asymmetry of the distribution of a random variable
about its mean. A random variable that is perfectly symmetric about its mean has skewness
µ3 = 0, whilst a variable is said to be either right-skewed or left-skewed if µ3 > 0 or µ3 < 0,
respectively. The fourth central moment is known as the kurtosis and measures how peaked the
distribution is around the mean. Since the skewness and kurtosis are dimensioned quantities with
units equal to the units of X raised to the third or fourth power, respectively, it is often preferred
to replace these by dimensionless quantities obtained by dividing by the standard deviation σ
raised to this power. This leads us to deﬁne the following quantities
Coeﬃcient of skewness: α3 = µ3
σ3
Coeﬃcient of kurtosis: α4 = µ4
σ4 .
See Figure 5.3 in Gregory (2005) for a selection of distributions with diﬀerent degrees of skewness
and kurtosis.
4.2.1
Moment generating functions
Moment generating functions have a number of important applications in probability theory.
In this section, we will see how they can be used to calculate all of the moments of a random
variable X in a single step. See the following sections for several examples of this approach.
Deﬁnition 4.8. If X is a real-valued random variable, then the moment generating function
of X is the function mX : R →[0, ∞] deﬁned by the formula
mX(t) = E

etX
=
(P
xi pX(xi)et·xi
if X is discrete,
R ∞
−∞fX(x)etx dx
if X is continuous.
Although the moment generating function is deﬁned for all t ∈R, it is possible for it to di-
verge everywhere except at t = 0 where mX(0) = 1 holds for any random variable X. On the
other hand, if X is a bounded random variable, i.e., if there is a constant K < ∞such that
P(|X| ≤K) = 1, then the moment generating function will be ﬁnite everywhere: mX(t) < ∞for
all t ∈R. Furthermore, it can be shown that the distribution of a random variable is uniquely

4.2. EXPECTATIONS AND MOMENTS
39
determined by the moment generating function whenever that function is ﬁnite in some open
interval containing 0. In particular, if two random variables have the same moment generating
function and this function is ﬁnite in some open interval, then the two variables also have the
same distribution.
To understand why the moment generating function has this name, suppose that the Taylor
series of the moment generating function mX(t) of a random variable X is convergent in some
open interval (−a, a) containing 0. Then
mX(t)
=
E[etX]
=
E
" ∞
X
n=0
1
n!tnXn
#
=
∞
X
n=0
1
n!µ′
ntn,
where µ′
n = E[Xn] is the n’th (non-central) moment of X and the last step requires that we be
able to interchange the sum with the expectation. This shows that the moments of X can be
found by ﬁrst ﬁnding its moment generating function and then expanding it in a Taylor series
and reading oﬀthe coeﬃcients. In particular, the n’th moment of X is simply the n’th derivative
of mX(t) evaluated at t = 0:
µ′
n = m(n)
X (0).
In some cases, this procedure may require less work than direct evaluation of the sum or the
integral expression of the moment.
Example 4.10. Suppose that λ > 0 is a positive constant and let X be a continuous random
variable with density fX(x) = λe−λx on [0, ∞) and fX(x) = 0 for x < 0. (X is said to be an
exponential random variable with parameter λ.) Then the moment generating function of X is
mX(t)
=
E

etX
=
Z ∞
0
etxλe−λxdx
=
(
λ
λ−t
if t < λ
∞
if t ≥λ.
Since this is a smooth function on an open interval (−∞, λ), it can be expanded in a Taylor series
as
λ
λ −t =
∞
X
n=0
λ−ntn
and it follows that the n’th moment of X is simply µ′
n = λ−nn!.

Chapter 5
A Menagerie of Distributions
5.1
The Binomial Distribution
Suppose that an experiment is repeated n times and that each trial has two possible outcomes,
which we denote Q and Q. Assume that the trials are independent of one another and that the
probability of outcome Q is the same in each trial, say p. If we let n = 3 and we let Qi be the
proposition that the i’th trial results in outcome Q, then there are eight possibilities:
Q1, Q2, Q3 + Q1, Q2, Q3 + Q1, Q2, Q3 + Q1, Q2, Q3
+Q1, Q2, Q3 + Q1, Q2, Q3 + Q1, Q2, Q3 + Q1, Q2, Q3.
We can calculate the probability of each of these possibilities using the product rule. For example,
the probability of outcome Q1, Q2, Q3 is equal to
P(Q1, Q2, Q3)
=
P(Q1) × P(Q2|Q1) × P(Q3|Q1, Q2)
=
P(Q1) × P(Q2) × P(Q3)
=
(1 −P(Q1)) × P(Q2) × P(Q3)
=
p2(1 −p).
Similarly, the probabilities of outcomes Q1, Q2, Q3 and P(Q1, Q2, Q3) are
P(Q1, Q2, Q3)
=
P(Q1) × P(Q2) × P(Q3)
=
p2(1 −p)
P(Q1, Q2, Q3)
=
P(Q1) × P(Q2) × P(Q3)
=
p2(1 −p).
Upon calculating the probabilities of the other ﬁve possible outcomes for a sequence of three
trials, we see that the probability of a particular outcome depends only on the number of trials
that result in Q or Q, but not on the order in which these occur. Let X be a random variable
that records the number of trials that result in outcome Q. Since there is exactly one way that
all three trials can result in three Q’s, three ways that the three trials can result in two Q’s,
three ways that they can result in two Q’s, and one way that they can result in 0 Q’s, it follows
that the probability mass function of X is equal to
pX(0) = (1 −p)3
pX(1) = 3p(1 −p)2
pX(3) = p3
pX(2) = 3p2(1 −p).
In general, suppose that n independent trials are performed and let X be the number of trials
40

5.1. THE BINOMIAL DISTRIBUTION
41
that result in outcome Q. Then, by reasoning as in the above example, we can show that
P(X = k)
=
number of ways that k Q’s can occur in n trials × pk(1 −p)n−k
=
number of ways of selecting a subset of size k from a set containing n elements
×pk(1 −p)n−k
=
n
k

pk(1 −p)n−k
=
n!
k!(n −k)!pk(1 −p)n−k,
where the constants
n
k

=
n!
k!(n −k)!
(5.1)
count the number of combinations of k elements from a set containing n elements. These are
sometimes denoted nCr or Cn
r , depending on the source, and they are called binomial coeﬃ-
cients because they appear as coeﬃcients of monomials in binomial expansions:
(x + y)n =
n
X
k=0
n
k

xkyn−k.
(5.2)
This distribution occurs so frequently in probability theory and statistics that it has its own
name.
Deﬁnition 5.1. A random variable X is said to have the binomial distribution with param-
eters n and p if it takes values in the set {0, 1, · · · , n} with probability mass function
pX(k) ≡P(X = k) =
n
k

pk(1 −p)n−k.
(5.3)
In this case, we write X ∼Binomial(n, p) to indicate that X has this distribution. and we say
that X is a binomial random variable.
The name of this distribution stems from the connection between the probabilities p(k) and the
coeﬃcients in a binomial expansion:
n
X
k=0
p(k) =
n
X
k=0
n
k

pk(1 −p)n−k = (p + 1 −p)n = 1.
Notice that a Bernoulli random variable with parameter p is also a binomial random variable
with parameters n = 1 and p. In fact, the next theorem shows that there is a close relationship
between Bernoulli distributions and binomial distributions even when n ≥1.
Theorem 5.1. Let X1, · · · , Xn be independent Bernoulli random variables, each with the same
parameter p. Then the sum X = X1 + · · · + Xn is a binomial random variable with parameters
n and p.
Proof. The random variable X counts the number of Bernoulli variables X1, · · · , Xn that are
equal to 1, i.e., the number of successes in the n independent trials. Clearly X takes values in
the set {0, · · · , n}. To calculate the probability that X = k, let E be the event that Xi1 = Xi2 =
· · · = Xik = 1 and Xj = 0 for all j /∈{i1, · · · , ik}. Then, because the Bernoulli variables are
independent, we know that
P(E) = pk(1 −p)n−k.

42
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
However, there are
 n
k

such sets of indices i1, · · · , ik, these corresponding to mutually exclusive
events, and so
P(X = k) =
n
k

pk(1 −p)n−k.
Example 5.1. Suppose that a fair coin is tossed n times. Then the number of heads that appear
is a binomial random variable with parameters n and p = 1/2.
Example 5.2. Sampling with replacement. Suppose that n balls are sampled with replace-
ment an urn which contains 3 red balls and 9 blue balls. If X denotes the number of red balls in
the sample, then X ∼Binomial(n, 0.25) since the probability of sampling a red ball on any given
draw is 3/3 + 9 = 0.25. More generally, if the urn contains r red balls and b blue balls, then
X ∼Binomial(n, p) where p = r/(r + b).
Example 5.3. Suppose that eye color in humans is under the control of a single gene. Individuals
with genotypes BB or Bb have brown eyes, while individuals with genotype bb have blue eyes.
Suppose that two brown-eyed individuals with genotypes Bb and Bb have a total of 4 children.
Assuming Mendelian inheritance, what is the distribution of the number of blue-eyed oﬀspring?
Answer: Let Xi equal 1 if the i’th child has blue eyes and 0 otherwise. Then Xi is a Bernoulli
random variable with parameter p = 1/4, since a child will be blue-eyed only if they inherit a blue
allele from each parent, which happens with probability 1/4. Since the genotypes of the diﬀerent
oﬀspring are independent, the total number of blue-eyed children is binomial with parameters
n = 4 and p = 1/4.
The mean and the variance of the binomial distribution can be calculated with the help of the
moment generating function. If X ∼Binomial(n, p), then the moment generating function of X
is equal to
mX(t)
≡
E

etX
=
n
X
k=0
P(X = k)etk
=
n
X
k=0
n
k

pk(1 −p)n−ketk
=
n
X
k=0
n
k

(pet)k(1 −p)n−k
=
 pet + 1 −p
n ,
where the ﬁnal identity follows from the binomial formula. With the help of the chain rule and
the product rule, diﬀerentiating twice (with respect to t) gives
m′
X(t)
=
npet  pet + 1 −p
n−1
m′′
X(t)
=
npet  pet + 1 −p
n−1 + n(n −1)p2e2t  pet + 1 −p
n−2 .
It follows that the ﬁrst two moments of X are
E[X]
=
m′
X(0)
=
np
E

X2
=
m′′
X(0)
=
np + n(n −1)p2,

5.1. THE BINOMIAL DISTRIBUTION
43
while the variance of X is
Var(X) = E

X2
−(E[X])2 = np(1 −p).
Thus, as we might expect, the mean of X is an increasing function of both parameters, n and p,
while the variance of X is an increasing function of n which is largest when p = 1/2. Further-
more, the variance is 0 whenever p = 0 or p = 1, since in these cases we either have X = 0 with
probability 1 or X = n with probability one, i.e, every trial results in the same outcome.
The ﬁnal result of this section asserts that the probability mass function of a binomial random
variable is always peaked around its mean.
Proposition 5.1. Let X be a binomial random variable with parameters (n, p). Then P(X = k)
is a unimodal function of k with its maximum at the largest integral value of k less than or equal
to (n + 1)p.
Proof. A simple calculation shows that
P(X = k)
P(X = k −1) = (n −k + 1)p
k(1 −p)
,
which is greater than or equal to 1 if and only if
k ≤(n + 1)p.
5.1.1
Sampling and the hypergeometric distribution
In Example 5.2, we noted that if n balls are sampled with replacement from an urn containing
r red balls and b blue balls, then the number of red balls in the sample is binomially distributed
with parameters n and p = r/(r + b). It should be emphasized that the assumption that we
are sampling with replacement is crucial here: the probability of sampling a red ball on the i’th
draw is still p no matter which balls were previously sampled because these are returned to the
urn before the next ball is sampled.
Perhaps more often than not, sampling is done without replacement, e.g., when we ‘sample’
the population by mailing out a survey to a random selection of individuals or when we enroll
individuals in a clinical trial, we usually do not want to include ‘duplicates’ in our sample. In this
case, the probability of sampling a particular type of object changes as we remove individuals
from the population. Consider the urn example again, but now suppose that we sample 3 balls
without replacement from an urn containing 3 red balls and 9 blue balls, and let X be the number
of red balls in our sample. If the order in which red and blue balls are sampled is considered, then
there are eight possible outcomes: RRR, BRR, RBR, RRB, BBR, BRB, BBR, and BBB,
which occur with probabilities:
P(RRR)
=
P(R) × P(R|R) × P(R|RR)
=
3
12 × 2
11 × 1
10
=
3 × 2 × 1
12 × 11 × 10
P(BRR)
=
P(B) × P(R|B) × P(R|BR)
=
9
12 × 3
11 × 2
10
=
3 × 2 × 9
12 × 11 × 10
P(RBR)
=
P(R) × P(B|R) × P(R|RB)
=
3
12 × 9
11 × 2
10
=
3 × 2 × 9
12 × 11 × 10
P(RRB)
=
P(R) × P(R|R) × P(B|RR)
=
3
12 × 2
11 × 9
10
=
3 × 2 × 9
12 × 11 × 10

44
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
P(BBR)
=
P(B) × P(B|B) × P(R|BB)
=
9
12 × 8
11 × 3
10
=
3 × 9 × 8
12 × 11 × 10
P(BRB)
=
P(B) × P(R|B) × P(B|BR)
=
9
12 × 3
11 × 8
10
=
3 × 9 × 8
12 × 11 × 10
P(RBB)
=
P(R) × P(B|R) × P(B|RB)
=
3
12 × 9
11 × 8
10
=
3 × 9 × 8
12 × 11 × 10
P(BBB)
=
P(B) × P(B|B) × P(B|BB)
=
9
12 × 8
11 × 7
10
=
9 × 8 × 7
12 × 11 × 10.
As with our previous example, the probability of each outcome depends only on the numbers of
red and blue balls contained in the sample, not on the order in which they appear. Thus the
probability mass function of X is equal to
pX(0) =
3
0
 3!
3! × 9!
6!
 .12!
9!
pX(1) =
3
1
 3!
2! × 9!
7!
 .12!
9!
pX(2) =
3
2
 3!
1! × 9!
8!
 .12!
9!
pX(3) =
3
3
 3!
0! × 9!
9!

/12!
9! .
More generally, suppose that n balls are sampled without replacement from an urn containing r
red balls and b blue balls and let X be the number of red balls contained in the sample. Clearly
n cannot be larger than the number of balls contained in the urn, so n ≤r + b. Furthermore,
the possible values that X can assume are integers between max{0, n −b} and min{n, r}. To
calculate P(X = k), where k lies between these two bounds, we ﬁrst observe that the probability
that k red balls are ﬁrst sampled and then n −k blue balls are sampled is

r
r + b
r −1
r + b −1 · · ·
r −(k −1)
r + b −(k −1)
 
b
r + b −k
b −1
r + b −(k + 1) · · · b −(n −k −1)
r + b −(n −1)

=

r!
(r −k)! ×
b!
(b −(n −k))!
 .
(r + b)!
(r + b −n)!.
However, since there are
 n
k

diﬀerent orders in which the k red balls can appear in the sample,
it follows that the probability that the sample contains k red balls is equal to
P(X = k)
=
n
k
 
r!
(r −k)! ×
b!
(b −(n −k))!
 .
(r + b)!
(r + b −n)!
=

r!
k!(r −k)! ×
b!
(n −k)!(b −(n −k))!
 .
(r + b)!
n!(r + b −n)!
=
 r
k
  b
n−k

 r+b
n

.
Like the binomial distribution, this distribution is of suﬃcient importance that it has its own
name.
Deﬁnition 5.2. A random variable X is said to have the hypergeometric distribution with
parameters (N, m, n) if X takes values in the set {max{0, n −(N −m)}, · · · , min{n, m}} with
probability mass function
pX(k) =
 m
k
 N−m
n−k

 N
n

.
(5.4)
In this case, we write X ∼Hypergeometric(N, m, n).

5.2. THE POISSON DISTRIBUTION
45
Unfortunately, the moment generating function is not of much use in this case.
(It can be
expressed in terms of a hypergeometric function, hence the name of the distribution.) However,
by direct calculation, it can be shown that the mean and the variance of a hypergeometric random
variable are
E[X]
=
n
m
N

Var(X)
=
n
m
N
 
1 −m
N
 N −n
N −1

.
In other words, if we let p = m/N be the probability of a ‘success’, then X has the same mean np
as a binomially distributed random variable with parameters (n, p), but it has a smaller variance
in general, since
n
m
N
 
1 −m
N
 N −n
N −1

≤np(1 −p).
On the other hand, the hypergeometric distribution with parameters (N, m, n) will almost be
the same as the binomial distribution with parameters (n, p) when p = m/N and N and m
are both much larger than n. To make this result precise, suppose that Xt is a sequence of
hypergeometric random variables with parameters (Nt, mt, n), where p = mt/Nt ∈(0, 1) for all
t ≥1 and Nt →∞and mt →∞as t →∞. Then
P(Xt = k)
=
n
k
 mt
Nt
× · · · × mt −(k −1)
Nt −(k −1)
 Nt −mt
Nt −k × · · · × Nt −mt −(n −k −1)
Nt −(n −1)

→
n
k

pk(1 −p)n−k,
as t →∞.
Intuitively, sampling a small number of balls without replacement from an urn
containing a very large number of balls is almost equivalent to sampling with replacement,
since in the latter case we are unlikely to ever sample the same ball more than once anyway.
For this reason, the binomial distribution is often regarded as an adequate approximation to
the hypergeometric distribution when analyzing data that was generated by sampling without
replacement, provided that the sample size is much smaller than the population size.
5.2
The Poisson Distribution
Suppose that we perform a large number of independent trials, say n ≥100, and that the
probability of a success on any one trial is small, say p = λ/n ≪1. If we let X denote the
total number of successes that occur in all n trials, then we know from the last lecture that
X ∼Binomial(n, p) and therefore
P(X = k)
=
n
k

pk(1 −p)n−k
=
n!
(n −k)!k!
λ
n
k 
1 −λ
n
n−k
=

n!
nk(n −k)!
 λk
k!
 
1 −λ
n
n 
1 −λ
n
−k
=
n(n −1)(n −2) · · · (n −k + 1)
nk
 λk
k!
 
1 −λ
n
n 
1 −λ
n
−k
≈
e−λ
λk
k!

,

46
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
where the approximation in the last line can be justiﬁed by the following three limits:
lim
n→∞

1 −λ
n
n
=
e−λ
lim
n→∞

1 −λ
n
−k
=
1
lim
n→∞
n(n −1)(n −2) · · · (n −k + 1)
nk

=
1.
Furthermore, since these approximations sum to 1 when k ranges over the non-negative integers,
e−λ
∞
X
k=0
λk
k! = e−λeλ = 1,
it follows that the function
p(k) = e−λ λk
k! ,
k ≥0
deﬁnes a probability distribution on the non-negative integers. Like the binomial distribution,
this distribution is so important in probability theory and statistics (and even in the sciences)
that it has its own name, which is taken from that of the 19’th century French mathematician,
Siméon Dennis Poisson (1781-1840).
Deﬁnition 5.3. A random variable X is said to have the Poisson distribution with param-
eter λ ≥0 if X takes values in the non-negative integers with probability mass function
pX(k) = P(X = k) = e−λ λk
k! .
In this case we write X ∼Poisson(λ).
To calculate the mean and the variance of this distribution, we ﬁrst identify its moment generating
function:
mX(t)
=
E

etX
=
∞
X
k=0
pX(k)etk
=
e−λ
∞
X
k=0
λketk
k!
=
e−λeλet
=
eλ(et−1).
Diﬀerentiating twice with respect to t gives
m′
X(t)
=
λet+λ(et−1)
m′′
X(t)
=
λ
 1 + λet
et+λ(et−1)
and we then ﬁnd
E[X]
=
m′
X(0)
=
λ
Var(X)
=
m′′
X(0) −(E[X])2
=
λ.
Thus, the parameter λ is equal to both the mean and the variance of the Poisson distribution.
It has long been recognized that the Poisson distribution provides a surprisingly accurate model
for the statistics of a large number of seemingly unrelated phenomena. Some examples include:

5.2. THE POISSON DISTRIBUTION
47
• the number of misprints per page of a book;
• the number of wrong telephone numbers dialed in a day;
• the number of customers entering a post oﬃce per day;
• the number of vacancies (per year) on the US Supreme Court (Table 5.1);
• the number of sites that are mutated when a gene is replicated;
• the number of α-particles discharged per day by a 14C source;
• the number of major earthquakes per year in a region.
Table 5.1: Data from Cole (2010) compared with a Poisson distribution with λ = 0.5.
Number of vacancies (x)
Probability
Years with x vacancies
1837-1932
1933-2007
Observed
Expected
Observed
Expected
0
0.6065
59
58.2
47
45.5
1
0.3033
27
29.1
21
22.7
2
0.0758
9
7.3
7
5.7
3
0.0126
1
1.2
0
1.0
≥3
0.0018
0
0.2
0
0.1
Since these phenomena are generated by very diﬀerent physical and biological processes, the fact
that they share similar statistical properties cannot be explained by the speciﬁc mechanisms
that operate in each instance. Instead, the widespread emergence of the Poisson distribution
appears to be a consequence of a more general mathematical result. As we saw at the beginning
of this section, if the probability of success per trial is small, then the total number of successes
that occur when a large number of trials is performed is approximately Poisson distributed with
parameter λ = np. This result is sometimes known as the Law of Rare Events because it
describes the statistics of count data for events that have a small probability of occurring, i.e.,
that are rare. In fact, as Theorem 5.2 illustrates, we can relax the assumption that every trial is
equally likely to result in a success provided that certain other conditions are satisﬁed. This is
an example of the so-called Poisson paradigm.
Theorem 5.2. For each n ≥1, let Xn,1, · · · , Xn,n be a family of independent Bernoulli random
variables with parameters pn,1, · · · , pn,n. Suppose that
lim
n→∞max
1≤i≤n pn,i
=
0
lim
n→∞
n
X
i=1
pn,i
=
λ.
Then,
lim
n→∞P(Xn,1 + · · · + Xn,n = k) = e−λ λk
k! .
Remark: The ﬁrst condition guarantees that each event is individually unlikely to occur, while
the second condition implies that the mean number of events that do occur is approximately λ
when n is large.

48
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
To see how the law of rare events might explain Poisson-distributed count data, consider the
number of misprints per page of a book. We can regard each word printed on a given page as
a trial, which can either result in the correct spelling of the word or in a misprint. Assuming
that the typist or editor is at least somewhat competent, the probability that any one word is a
misprint in likely to be small, while the number of words per page is probably on the order of
250. Furthermore, barring catastrophic failures, the presence or absence of misprints is proba-
bly nearly independent between words. However, under these conditions, the law of rare events
tells us that the total number of misprints per page should be approximately Poisson distributed.
Example 5.4. If, on average, a healthy adult contracts three colds per year, what is the probability
that they will contract at least two colds in given year? To answer this, we will assume that the
number of colds contracted per year by a healthy adult is Poisson distributed with parameter
λ = 3. If we let X denote this random variable, then
P(X ≥2)
=
1 −P(X < 2)
=
1 −P(X = 0) −P(X = 1)
=
1 −e−3 30
0! −e−3 31
1!
≈
0.801,
where 0! = 1 by convention.
5.2.1
Fluctuation tests and the origin of adaptive mutations
One of the classic experiments of molecular genetics is the ﬂuctuation test, which was devel-
oped by Salvador Luria and Max Delbrück in 1943 in the course of their investigations of the
mechanisms that give rise to adaptive mutations. An adaptive mutation is one that increases
the ﬁtness of the organism that carries it. Speciﬁcally, Luria and Delbrück wanted to determine
whether adaptive mutations occur spontaneously or else are directly induced by the environmen-
tal conditions to which they are adapted. To do this, they developed an experimental system
based on a bacterium, Escherischia coli, along with type of virus known as a T1 bacteriophage
that infects this bacterium. When T1 phage is added to a culture of E. coli, most of the bacteria
are killed, but a few resistant cells usually survive and will give rise to resistant colonies that
can be seen on the surface of a petri dish. In other words, resistance to T1 phage is a trait that
varies across E. coli bacteria and which is heritable, i.e., the descendants of resistant bacteria
are usually themselves resistant.
The ﬂuctuation test carried out by Luria and Delbrück was based on the following procedures:
1. A single E. coli culture was initiated from a single T1-susceptible cell and allowed to grow
to a population containing millions of bacteria. Several small samples were taken from this
colony and spread on agar plates that had also been inoculated with the T1 phage. These
plates were left for a period and the number of resistant colonies was counted.
2. The procedure described in (1) was repeated several times, using independently established
E. coli cultures and the resulting data were used to estimate both the mean and the variance
of the number of resistant colonies arising in each culture.
Luria and Delbrück then reasoned as follows. If resistance mutations do not arise spontaneously
but instead are induced by the presence of T1 phage, then because the mutation rate per cell
is low (as shown by the data) and because there are a large number of cells per agar plate that
could mutate and give rise to mutant colonies, the total number of mutant colonies generated

5.2. THE POISSON DISTRIBUTION
49
in each trial will be approximately Poisson distributed. In particular, under these assumptions,
the mean and the variance of the number of mutant colonies generated in each trial will be
approximately equal. On the other hand, if mutations occur spontaneously during expansion of
the bacterial populations used in each trial, then the proportion of resistant cells in each culture
prior to T1 plating will depend on the timing of the mutation relative to the expansion of the
population from a single cell. In particular, if the mutation occurs at an early stage of expansion,
then it will be inherited by many of the cells that are spread onto the plates containing the T1
phage and there will be many resistant colonies. However, if the mutation occurs at a late stage
of the expansion, then it will be inherited by few cells and so there will be few resistant colonies.
It can be shown that under this scenario, the number of mutant colonies does not follow the
Poisson distribution and has a variance that is much higher than the mean.
Luria and Delbrück repeated this experiment several times and found that the ratio of the
sample variance to the sample mean ranged from about 4 to 200. In no case was the sample
variance similar to the sample mean and additional controls were performed that showed that
sampling error could not account for the discrepancy between the two. In other words, the data
provide strong evidence against the hypothesis that resistance mutations are induced by the
T1 phage itself and are consistent with the predictions of the hypothesis that these mutations
arise spontaneously prior to exposure to the virus. Similar results have emerged from the many
studies of mutation that were carried out after Luria and Delbrück’s original work using diﬀerent
organisms and diﬀerent selection pressures, and indeed these ﬁndings are consistent with our
current understanding of the molecular basis of mutation.
5.2.2
Poisson processes
Poisson distributions can also be used to model scenarios in which events occur at a ‘constant
rate’ through space or time. Consider a sequence of events that occur repeatedly through time
(such as earthquakes or mutations to DNA) and assume that the following properties are satisﬁed:
• Property 1: Events occur at rate λ per unit time, i.e., the probability that exactly one
event occurs in a given interval [t, t + h] is equal to λh + o(h).
• Property 2: Events are not clustered, i.e., the probability that two or more events occur in
an interval of length h is o(h).
• Property 3: Independence. For any set of non-overlapping intervals [s1, t1], [s2, t2], · · · , [sr, tr],
the numbers of events occurring in these intervals are independent.
Here we have used Landau’s little-o notation: If f and g are real-valued functions on [0, ∞),
we write that f(h) = g(h) + o(h) if
lim
h→0
f(h) −g(h)
h
= 0.
In particular, we write f(h) = o(h) if f(h)/h converges to 0 as h →0. For example, if f(h) = h2,
then f(h) = o(h).
Let N(t) denote the number of events that occur in the interval [0, t]. Our goal is to calculate
the distribution of N(t), which we can do by breaking the interval [0, t] into n non-overlapping
subintervals, each of length t/n. Then,
P (N(t) = k) = P (An) + P (Bn)

50
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
where
An
=
{k subintervals each contain exactly one event, while the others contain no events}
Bn
=
{N(t) = k and at least one subinterval contains ≥2 events }.
Notice that An and Bn are mutually exclusive.
We ﬁrst show that P (Bn) can be made arbitrarily small by taking n suﬃciently large. Indeed,
P (Bn)
≤
P{at least one subinterval contains two or more events}
≤
n
X
k=1
P{the k’th subinterval contains two or more events}
≤
n
X
k=1
o
 t
n

=
n · o
 t
n

=
t
o(t/n)
t/n

.
However, since t/n →0 as n →∞, it follows from the deﬁnition of o(h) that this last expression
vanishes as n →∞, which then implies that
lim
n→∞P (Bn) = 0.
Next, using Properties 1 and 2, observe that
P {an interval of length t/n contains one event}
=
λt
n + o
 t
n

P {an interval of length t/n contains no events}
=
1 −λt
n + o
 t
n

.
Also, using Property 3, we have
P (An)
=
n
k
 λt
n + o
 t
n
k 
1 −λt
n + o
 t
n
n−k
→
e−λt (λt)k
k!
as n →∞,
since there are
 n
k

ways for the k events to be distributed across the n subintervals such that
each subinterval contains either 0 or 1 event.
Putting these results together shows that
P(N(t) = k)
=
lim
n→∞(P(An) + P(Bn))
=
e−λt (λt)k
k! .
In other words, for each t ≥0, the number of events N(t) to have occurred by time t is a Poisson
random variable with parameter λt. The collection of random variables (N(t) : t ≥0) is said to
be a Poisson process with intensity λ. Such processes are used to model a wide variety of
phenomena such as disease transmissions, earthquakes, and arrival of traﬃc at stoplights.

5.3. RANDOM WAITING TIMES
51
5.3
Random Waiting Times
5.3.1
The geometric and exponential distributions
Suppose that a sequence of independent Bernoulli trials is performed and that each trial has
probability p ∈(0, 1] of resulting in a success. If T denotes the number of trials performed until
the ﬁrst success is recorded, then T is a random variable that takes values in the positive integers
{1, 2, · · · , ∞} with probability mass function
pT (k) = (1 −p)k−1p,
k ≥1.
Indeed, T = k if and only if the ﬁrst k −1 trials all result in failures and the k’th trial results in
a success. A random variable with this distribution is said to have the geometric distribution
with success probability p, in which case we write T ∼Geometric(p).
The moment generating function of T is
mT (t)
=
∞
X
k=1
(1 −p)k−1petk
=
pet
∞
X
k=1
 (1 −p)etk−1
=





pet
1−(1−p)et
if t < −ln(1 −p)
∞
otherwise
with derivatives
m′
T (t)
=
pet
(1 −(1 −p)et)2
m′′
T (t)
=
−pet
(1 −(1 −p)et)2 +
2pet
(1 −(1 −p)et)3 .
It follows that the mean and the variance of T are equal to
E[T]
=
1
p
Var(T)
=
1 −p
p2 .
As expected, the smaller p is, the more trials that we have to conduct on average to obtain the
ﬁrst success.
Example 5.5. The probability of a mutation in a eukaryotic genome is approximately µ = 10−8
per site per generation. Thus it takes, on average, approximately µ−1 = 108 generations for a
particular site to mutate. To put this time span into perspective, the earliest undisputed animal
fossils date to the Cambrian explosion about 540 MYA, while the dinosaurs ﬁrst appeared in the
Triassic about 230 MYA and the non-avian dinosaurs went extinct at the end of the Cretaceous
around 65 MYA. Also, because eukaryotic genomes generally contain many millions to billions
of nucleotides, these typically mutate in every generation.
As illustrated in the preceding example, the geometric distribution is often used to model the
random waiting time until the ﬁrst “success” occurs. This makes the most sense if the trials are

52
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
performed at a constant rate, e.g., one trial per day, in which case T will have the same units as
the time between trials. However, if the probability of success per trial is very small, then this
waiting time will typically be very large, in which case the following approximation can be used.
Theorem 5.3. Let (ϵn; n ≥1) be a sequence of positive numbers tending to 0 and for each n ≥1
let Tn be a geometric random variable with parameter ϵn. Then, for every t ≥0,
lim
n→∞P{ϵnTn ≤t} = 1 −e−t.
Proof. The result follows from the calculation
P{ϵnTn ≤t}
=
1 −P

Tn > t
ϵn

=
1 −(1 −ϵn)t/ϵn
→
1 −e−t
as n →∞,
where we have used the following result
lim
n→∞(1 + an)bn = ec,
whenever (an : n ≥1) and (bn : n ≥1) are sequences of real numbers such that an →0, bn →∞,
and anbn →c as n tends to inﬁnity.
Theorem 5.3 not only provides us with an approximation for the geometric distribution, but
it also leads to a new distribution that is interesting in its own right.
Since the function
F(t) = 1 −e−t is nondecreasing and diﬀerentiable with F(0) = 0 and F(t) →1 as t →∞,
it is the cumulative distribution function of a continuous random variable X with probability
density pX(t) = F ′(t) = e−t for t ≥0. This distribution is so important that it has been named.
Deﬁnition 5.4. A continuous random variable X is said to have the exponential distribution
with parameter λ if the density of X is
pX(x) =



λe−λx
if x > 0
0
otherwise.
In this case we write X ∼Exponential(λ).
Theorem 5.3 illustrates an important theme in probability theory, which is that discrete random
variables that take on large integer values can often be approximated by continuous random
variables. In this particular case, Tn will typically assume values that are of the same order of
magnitude as its mean, E[Tn] = ϵ−1
n , which will be large whenever ϵn is small. In contrast, if
we normalize Tn by dividing by its mean, then we obtain a random variable ϵnTn which is no
longer integer-valued but which has mean E[ϵnTn] = 1 and typically assumes values that are of
the same order of magnitude. Notice that we can also interpret ϵnTn as the time to the ﬁrst
success, where time is measured in units proportional to ϵ−1
n .
Previously (Example 4.7) we showed that the moment generating function of X ∼Exponential(λ)
is
mX(t) =
(
λ
λ−t
if t < λ
∞
if t ≥λ.

5.3. RANDOM WAITING TIMES
53
which implies that the mean and the variance are
E[X]
=
1
λ
Var(X)
=
1
λ2 .
Thus, the mean of an exponential random variable is equal to the reciprocal of its parameter,
while the variance is equal to the mean squared. Like the geometric distribution, the exponential
distribution is often used to model the random time that elapses until some event occurs. With
this interpretation, the parameter λ is often referred to as the rate parameter.
An important property of the exponential distribution is that it is memoryless: if X is expo-
nentially distributed with parameter λ, then for all t, s ≥0,
P{X > t + s|X > t} = P{X > s}.
To verify this, use the CDF calculated above to obtain
P{X > t} = 1 −P{X ≤t} = e−λt,
and then calculate
P{X > t + s|X > t}
=
P{X > t + s}
P{X > t}
=
e−λ(t+s)
e−λt
=
e−λs
=
P{X > s}.
(5.5)
In other words, if we think of X as being the random time until some event occurs, then condi-
tional on the event not having occurred by time t, i.e., on X > t, the distribution of the time
remaining until the event does occur, X −t, is also exponential with parameter λ. It is as if the
process determining whether the event occurs between times t and t + s has no ‘memory’ of the
amount of time that has elapsed up to that point. This observation also sheds further light on
why λ is called the rate parameter. Indeed,
P(t < T ≤t + δt|T > T)
=
e−λt −e−λ(t+δt)
e−λt
=
1 −e−λδt
≈
λδt,
provided 0 < δt ≪1. This shows that the probability that the event occurs in a short time
interval (t, t + δt] given that it has not already occurred by time t is approximately proportional
to the rate λ times the duration of the interval δt. Furthermore, because this probability does
not depend on t, the rate is constant in time. Remarkably, these properties also uniquely char-
acterize the exponential distribution.
Proposition 5.2. Suppose that X is a continuous random variable with values in [0, ∞). Then
X is memoryless if and only X is exponentially distributed for some parameter λ > 0.
As an aside, the geometric distribution could also be said to be memoryless provided we restrict
to integer-valued times. To see this, let T ∼Geometric(p) and observe that for all positive

54
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
integers n, m ≥1,
P(T > n + m|T > n)
=
P(T > n + m)
P(T > n)
=
(1 −p)n+m−1p
(1 −p)n−1p
=
(1 −p)m−1p
=
P(T > m),
which is clearly a discrete analogue of (5.5). This is not surprising given our earlier observation
that the exponential distribution arises as a limiting case of the geometric distribution.
5.3.2
The gamma distribution
Although the memorylessness of the exponential distribution is often a convenient mathematical
property, there are many temporal phenomena in nature that occur at rates which vary over time
and which therefore are not memoryless. For example, because mortalities are age-dependent in
many species, e.g, the very young and the very old are usually much more likely to die in a given
period than are adult individuals, the exponential distribution does not provide a good statistical
model for the life spans of individuals of these species. For this reason, additional continuous
distributions on the non-negative real numbers are needed and one of the most important families
of such distributions is the gamma distribution.
Deﬁnition 5.5. A random variable X is said to have the gamma distribution with shape
parameter α > 0 and scale parameter λ > 0 if its density is
p(x) =



λα
Γ(α)xα−1e−λx
if x > 0
0
otherwise,
where the gamma function Γ(α) is deﬁned (for α > 0) by the formula
Γ(α) =
Z ∞
0
yα−1e−ydy.
In this case, we write X ∼Gamma(α, λ).
Notice that the exponential distribution with parameter λ is identical to the gamma distribution
with parameters (1, λ). Later we will show that if X1, · · · , Xn are independent exponential RVs,
each with parameter λ, then the sum X = X1 +· · ·+Xn is a gamma RV with parameters (n, λ).
Thus, gamma-distributed random variables are sometimes used as statistical models for lifespans
that are determined by a sequence of independent events which occur at constant rate (e.g., trans-
formation of a lineage of cells following the accumulation of mutations that disrupt regulation of
the cell cycle). Furthermore, unlike the exponential distribution, the gamma distribution is not
(in general) memoryless and its mode is positive whenever the shape parameter is greater than 1.
Before we can calculate the moments of X, we need to spend a little time investigating the
gamma function, which is important in its own right. Integration by parts shows that for α > 1,
the gamma function satisﬁes the following important recursion:
Γ(α)
=
−y−α−1e−y∞
0 + (α −1)
Z ∞
0
yα−2e−ydy
=
(α −1)Γ(α −1).

5.4. CONTINUOUS DISTRIBUTIONS ON FINITE INTERVALS
55
In particular, if α = n is a positive integer, then because
Γ(1) =
Z ∞
0
e−ydy = 1,
we obtain
Γ(n)
=
(n −1)Γ(n −2)
=
(n −1)(n −2)Γ(n −3)
=
(n −1)(n −2) · · · 3 · 2Γ(1)
=
(n −1)!
(5.6)
which shows that the gamma function is a continuous interpolation of the factorial function.
In fact, it can be shown that the gamma function is the unique interpolation of the factorial
function which can be expanded as a power series. Equation (5.6) can be used to calculate the
moments of the gamma distribution. If X has the gamma distribution with parameters (α, λ),
then
E[Xn]
=
Z ∞
0
xn λα
Γ(α)xα−1e−λxdx
=
λ−n Γ(n + α)
Γ(α)
Z ∞
0
λn+α
Γ(n + α)xn+α−1e−λxdx
=
λ−n
n−1
Y
k=0
(α + k).
In particular, taking n = 1 and n = 2 gives
E[X]
=
α
λ
V ar(X)
=
E[X2] −E[X]2
=
α(α + 1)
λ2
−α2
λ2
=
α
λ2 .
5.4
Continuous Distributions on Finite Intervals
5.4.1
The uniform distribution
The uniform distribution is arguably the simplest of all the continuous distributions since its
density is constant on an interval. Later in the course we will see that this distribution is special
in another sense, since it is the distribution with maximum entropy on any ﬁnite interval. For
these reasons, the uniform distribution is often used as a non-informative prior distribution
for variables when only the lower and upper bounds are known.
Deﬁnition 5.6. A continuous random variable X is said to have the uniform distribution
on the interval (a, b) if the density function of X is
p(x) =



1
b−a
if x ∈(a, b)
0
otherwise.
In this case we write X ∼Uniform(a, b) or X ∼U(a, b). In particular, if X ∼U(0, 1), then X
is said to be a standard uniform random variable.

56
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
To say that X is uniformly distributed on an interval (a, b) means that each point in (a, b) is
equally likely to be a value of X. In particular, the probability that X lies in any subinterval of
(a, b) is proportional to the length of the subinterval: if (l, r) ⊂(a, b), then
P
 X ∈(l, r)

= r −l
b −a.
Of course, since X is continuous, the probability that X = x for any x ∈(a, b) is 0.
Suppose that X has the U(a, b) distribution. Then the n’th moment of X is given by
E

Xn
=
1
b −a
Z b
a
xndx
=
1
b −a
1
n + 1xn+1
b
a
=
1
n + 1
bn+1 −an+1
b −a

.
It follows that the mean and the variance of X are
E[X]
=
1
2(a + b)
Var(X)
=
E

X2
−E[X]2 = 1
12(b −a)2.
5.4.2
The beta distribution
Although mathematically convenient, the uniform distribution is often too restrictive when mod-
eling the statistics of variables that take values in a ﬁnite interval. When this is true, we can
sometimes turn to the beta distribution as a fairly ﬂexible generalization of the standard uniform
distribution.
Deﬁnition 5.7. A random variable X is said to have the beta distribution with parameters
a, b > 0 if its density is
p(x) =



1
β(a,b)xa−1(1 −x)b−1
if x ∈(0, 1)
0
otherwise,
where the beta function β(a, b) is deﬁned (for a, b > 0) by the formula
β(a, b) =
Z 1
0
xa−1(1 −x)b−1dx.
Notice that if a = b = 1, then X is simply a standard uniform random variable. Also, if X1 and
X2 are independent gamma-distributed RVs with parameters (a, θ) and (b, θ), respectively, then
the random variable X = X1/(X1 +X2) is beta-distributed with parameters (a, b). In particular,
if X1 and X2 are independent exponential RVs with parameter λ, then X = X1/(X1 + X2) is
uniformly distributed on [0, 1]. The beta distribution is often used to model distributions of
frequencies.
It can be shown that the beta function and the gamma function are related by the following
identity:
β(a, b) = Γ(a)Γ(b)
Γ(a + b) .

5.4. CONTINUOUS DISTRIBUTIONS ON FINITE INTERVALS
57
In turn, we can use this identity to evaluate the moments of the beta distribution. Let X be a
beta random variable with parameters (a, b). Then
E[Xn]
=
1
β(a, b)
Z 1
0
xnxa−1(1 −x)b−1dx
=
β(n + a, b)
β(a, b)
=
Γ(n + a)Γ(b)
Γ(n + a + b)
Γ(a + b)
Γ(a)Γ(b)
=
Γ(n + a)
Γ(a)
Γ(a + b)
Γ(n + a + b)
=
n−1
Y
k=0
a + k
a + b + k.
Taking n = 1 and n = 2 gives
E[X]
=
a
a + b
V ar(X)
=
E[X2] −E[X]2
=
a(a + 1)
(a + b)(a + b + 1) −
a2
(a + b)2
=
ab
(a + b)2(a + b + 1).
5.4.3
Bayesian estimation of proportions
Suppose that we conduct n independent trials and that we ﬁnd that exactly k of them result
in a success. If we assume that each trial has the same unknown probability p of resulting in a
success, what does our data tell us about the value of p?
In problem set 4 you addressed this question by calculating the maximum likelihood esti-
mate of p. Here we will describe a Bayesian approach to the same problem. Let us deﬁne the
propositions I, D and Hp by
I = “n independent, identically-distributed trials have been conducted”
D = “exactly k of the trials have resulted in a success”
Hp = “the probability of success per trial is in (p, p + δp)”
Notice that there are inﬁnitely many hypotheses Hp which are indexed by values of p ∈[0, 1].
By Bayes’ formula, we know that the posterior probability of hypothesis Hp is equal to
P(Hp|D, I) = P(Hp|I)P(D|Hp, I)
P(D|I)
(5.7)
where P(Hp|I) is the prior probability of Hp, P(D|Hp, I) is the likelihood function, and P(D|I) is
the prior predictive probability of the data. Under the assumptions of the problem, the likelihood
function is given by a binomial probability
P(D|Hp, I) =
n
k

pk(1 −p)n−k.

58
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
Thus, to complete the analysis, we need to choose a prior distribution for the value of p. Recall
that in Bayesian inference the prior distribution quantiﬁes our degree of belief in each hypothesis
before we take into account the new data. This belief could be based on data generated by
previous experiments of the same type or on completely diﬀerent sources of information (or
both). For binomial trials, it is often convenient to choose a prior distribution on p that belongs
to the beta family, i.e., we will assume that the density of the prior of distribution on Hp is
P(Hp|I) =
1
β(a, b)pa−1(1 −p)b−1dp
for some pair of numbers a, b > 0. Because the variance of the beta distribution is a decreasing
function of a and b, if a and b are both large, then the prior will be concentrated around the
mean a/a+b, which would reﬂect a scenario in which we have very strong prior beliefs concerning
the likely values of p. On the other hand, if a = b = 1, then the beta distribution is just the
standard uniform distribution, which would reﬂect a scenario in which we have little (perhaps no)
information concerning the likely values of p. Whatever the values of these parameters, equation
(5.7) tells us that the posterior density of p is equal to
P(Hp|D, I)
=
1
β(a, b)pa−1(1 −p)b−1
 n
k

pk(1 −p)n−k
P(D|I)
=
1
C pk+a−1(1 −p)n−k+b−1,
where the constant C = β(a, b)P(D|I)/
 n
k

can either be calculated directly or it can be deter-
mined indirectly by noting that it is a normalizing constant for the density on the right-hand
side, which is just the density of a beta distribution with parameters a + k and b + n −k. Since
this density must still integrate to 1 over [0, 1], it follows that
C = β(a + k, b + n −k).
This shows that the posterior distribution belongs to the same parametric family of distributions
as the prior distribution, diﬀering only in the values of the parameters, which now depend on
the data.
Prior distributions with this property are said to be conjugate priors and have
the advantage that the posterior distribution can be determined immediately without having to
resort to expensive numerical estimates of the prior predictive probability.
In this particular problem, we see that the mean and the variance of the posterior distribution
are
E[p|D, I]
=
a + k
a + b + n
Var(p|D, I)
=
(a + k)(b + n −k)
(a + b + n)2(a + b + n + 1).
This shows that the variance of the posterior distribution is necessarily less than the variance of
the prior distribution by a quantity that is increasing in n, i.e., the more data that we collect,
the less uncertain we are about the true value of p. Furthermore, if k and n are large relative
to a and b, then the mean of the posterior distribution will be close to ˆpML = k/n, which is the
maximum likelihood estimate of p mentioned above. In fact, the relative magnitudes of a and b
on the one hand and k and n −k on the other hand reﬂect the relative importance of the two
sources of information that we have concerning the value of p. When a, b ≫k, n −k, the data is
only weakly informative about the value of p and then the posterior distribution depends mainly
on the prior distribution. On the other hand, if a, b ≪k, n−k, then the data is very informative
about the value of p and this too is reﬂected in the posterior distribution.

5.4. CONTINUOUS DISTRIBUTIONS ON FINITE INTERVALS
59
5.4.4
Binary to analogue: construction of the uniform distribution
Thus far we have simply assumed that continuous distributions exist, without taking care to
specify the σ-algebra of measurable subsets of R or to prove that there really are probability
distributions with these properties. Here we partially redress this absence by showing how to
construct a standard uniform random variable from a sequence of i.i.d. Bernoulli random vari-
ables.
Suppose that X1, X2, · · · is a sequence of independent Bernoulli random variable, each with
parameter 1/2. If we deﬁne
X =
∞
X
n=1
Xn
1
2
n
,
then X is a standard uniform random variable. Notice that the random sequence (X1, X2, · · · )
is the binary representation of the random number X. That X is uniformly distributed can be
deduced as follows:
Step 1: We say that a number x ∈R is a dyadic rational number if x has a ﬁnite binary
expansion, i.e., there exists an integer n ≥1 and numbers c1, · · · , cn ∈{0, 1} such that
x =
∞
X
k=1
ck
1
2
k
.
Notice that every dyadic rational number is rational, but that not every rational number is dyadic
rational, e.g., x = 1/3 is not dyadic rational. On the other hand, the set of dyadic rational num-
bers is dense in R, i.e., for every z ∈R and every ϵ > 0, there exists a dyadic rational number x
such that |z −x| < ϵ.
Step 2: Every number x ∈R either has a unique, non-terminating binary expansion, or it
has two binary expansions, one ending in an inﬁnite sequence of 0’s and the other ending in an
inﬁnite sequence of 1’s. In either case,
P{X = x} = 0.
Step 3: Given numbers c1, · · · , cn ∈{0, 1}, let x be the corresponding dyadic rational deﬁned
by the equation shown in Step 1 and let E(c1, · · · , cn) be the interval (x, x + 2−n]. Then
P{X ∈E(c1, · · · , cn)}
=
P{X1 = c1, · · · , Xn = cn} −P{X = x}
=
1
2
n
.
Furthermore, if 0 ≤a < b ≤1, where a and b are dyadic rational numbers, then because the
interval (a, b] can be written as the disjoint union of ﬁnitely many sets of the type E(c1, · · · , cn)
for suitable choices of the ci’s, we have that
P{X ∈(a, b]} = (b −a).
Step 4: Given any two numbers 0 ≤a < b ≤1, not necessarily dyadic rational, we can ﬁnd
a decreasing sequence (an; n ≥1) and an increasing sequence (bn; n ≥1) such that an < bn,

60
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
an →a and bn →b. Then the sets (an, bn) form an increasing sequence, with limit set (a, b),
and by the continuity properties of probability measures, we know that
P{X ∈(a, b)}
=
lim
n→∞P{X ∈(an, bn]}
=
lim
n→∞(bn −an)
=
b −a.
However, this result implies that X is uniform on [0, 1].
5.5
The Normal Distribution
As explained below, the standard normal distribution is one of the most important probability
distributions in all of mathematics. Not only does it play a central role in large sample statistics,
but it is also relevant to many physical and biological processes, including heat conduction,
diﬀusion, and the evolution of quantitative traits. Furthermore, the standard normal distribution
is the building block for an entire family of normal distributions, having diﬀerent means and
variances.
Deﬁnition 5.8. A continuous real-valued random variable X is said to have the normal dis-
tribution with mean µ and variance σ2 if the density of X is
p(x) =
1
σ
√
2πe−(x−µ)2/2σ2
−∞< x < ∞.
In this case we write X ∼N(µ, σ2). In particular, X is said to be a standard normal random
variable if X ∼N(0, 1).
Remark: Normal distributions are also called Gaussian distributions after Carl Friedrich
Gauss (1777-1855).
We can show that the integral of p(x) over R is equal to 1 by using a clever trick attributed to
Gauss. First, by making the substitution y = (x −µ)/σ, we see that
Z ∞
−∞
p(x)dx =
1
√
2π
Z ∞
−∞
e−y2/2dy.
Then, letting I =
R ∞
∞exp(−y2/2)dy, we obtain
I2
=
Z ∞
−∞
e−x2/2dx
Z ∞
−∞
e−y2/2dy
=
Z ∞
−∞
Z ∞
−∞
e−(x2+y2)/2dxdy
=
Z ∞
0
Z 2π
0
re−r2/2dθdr
=
2π
Z ∞
0
re−r2/2dr
=
2π.
Here we have changed to polar coordinates in passing from the second to the third line, i.e., we
set x = r cos(θ), y = r sin(θ), and dxdy = rdθdr. This calculation shows that I =
√
2π, which
conﬁrms that p(x) is a probability density.

5.5. THE NORMAL DISTRIBUTION
61
An important property of normal distributions is that any aﬃne transformation of a normal
random variable is also a normal random variable. Speciﬁcally, if X is normally distributed with
mean µ and variance σ2, then the random variable Y = aX + b is also normally distributed, but
with mean aµ + b and variance a2σ2. To verify this claim, assume that a > 0 and observe that
the c.d.f. of Y is
FY (x)
=
P(Y ≤x)
=
P(aX + b ≤x)
=
P

X ≤x −b
a

=
FX
x −b
a

.
The density of Y can be found by diﬀerentiating FY (x) and is equal to
pY (x)
=
d
dxFY (x)
=
d
dxFX
x −b
a

=
1
apX
x −b
a

=
1
aσ
√
2πe−(x−aµ−b)2/2a2σ2,
which shows that Y ∼N(aµ + b, a2σ2) as claimed. A similar argument works if a < 0. In
particular, by taking b = −µ and a = 1/σ, we see that the variable Z = (X −µ)/σ is a standard
normal random variable. For this reason, Z is said to be standardized. Similarly, if Z is a
standard normal random variable, then so is −Z.
For statistical applications of the normal distribution, we are often interested in probabilities of
the form P(X > x) = 1−P(X ≤x). Although a simple analytical expression is unavailable, these
quantities can be calculated numerically and have been extensively tabulated for the standard
normal distribution. An online copy of such a table can be found at Wikipedia under the topic
‘Standard normal table’. Furthermore, because every normal random variable can be expressed
in terms of a standard normal random variable, we can use these lookup tables for more general
problems. For example, if X ∼N(µ, σ2), then by deﬁning Z = (X −µ)/σ, we see that
P(X > x)
=
P(σZ + µ > x)
=
1 −P(Z ≤(x −µ)/σ)
=
1 −Φ((x −µ)/σ),
where Φ(x) is the CDF of the standard normal distribution:
Φ(x) =
1
√
2π
Z x
−∞
e−y2/2dy.
Alternatively, functions that evaluate the cumulative distribution function of any normal random
variable X with arbitrary mean and variance are available in computational software packages
such as Matlab, Mathematica and R. For example, the Matlab command
p = normcdf(x, mu, sigma)
will calculate the probability P(X ≤x) when X ∼N(µ, σ2). Note that the symbols x, mu, and
sigma in the command should be replaced by their intended values, e.g., p = normcdf(1, 0, 1).
Standardization is also used in the proof of the next proposition.
Proposition 5.3. Let X be a normal random variable with parameters µ and σ2. Then the
expected value of X is µ, while the variance of X is σ2.
Proof. Let Z = (X −µ)/σ, so that Z ∼N(0, 1). Then
E[Z]
=
1
√
2π
Z ∞
−∞
xe−x2/2dx = 0,

62
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
while integration-by-parts with u = x and dv = xe−x2/2 shows that
Var(Z)
=
E

Z2
=
1
√
2π
Z ∞
−∞
x2e−x2/2dx
=
1
√
2π

−xe−x2/2|∞
∞+
Z ∞
−∞
e−x2/2dx

=
1.
Since X = µ + σZ, the proposition follows from the identities
E[X]
=
µ + σE[Z]
=
µ
Var(X)
=
σ2Var(Z)
=
σ2.
Our next result helps to explain why normal distributions are so prevalent.
Recall that if
X1, · · · , Xn are independent Bernoulli random variables, each having parameter p, then the
distribution of the sum Sn = X1 + · · · + Xn is binomial with parameters (n, p). Furthermore, if
p is small, then, by the law of rare events, we know that Sn is approximately Poisson distributed
with parameter np. In this case, Sn is typically small, of order np, and so a discrete approxi-
mation is to be expected. If, instead, np is large, then Sn will typically be large and it makes
more sense to look for a continuous approximation, much as we did in Theorem 5.3. Such an
approximation is given in the next theorem.
Theorem 5.4. (DeMoivre-Laplace Limit Theorem) If Sn is a binomial random variable
with parameters n and p and Z is a standard normal random variable, then
lim
n→∞P
(
a ≤
Sn −np
p
np(1 −p)
≤b
)
= P(a ≤Z ≤b) =
Z b
a
1
√
2πe−x2/2dx
for all real numbers a ≤b.
In other words, the DeMoivre-Laplace Theorem asserts that when both n and np are large, the
distribution of the standardized random variable Zn =
Sn−np
√
np(1−p) can be approximated by the
distribution of a standard normal random variable Z. Furthermore, since aﬃne transformations
of normal distributions are still normal (see above), it follows that for large n the variable Sn
is approximately normal with mean np and variance np(1 −p). We illustrate the use of this
approximation in Example 5.6 below. The accuracy of this approximation depends on both n
and p, but it is usually accurate to within a few percent when np(1 −p) ≥10 and the error can
be shown to decay at a rate that is asymptotic to n−1/2. The DeMoivre-Laplace Theorem can
be illustrated mechanically with the help of a device known as a Galton board or a quincunx;
an applet simulating a Galton board can be found at the following link:
http://www.jcu.edu/math/isep/quincunx/quincunx.html.
To use this applet, set the number of bins to 10 or greater and run a series of simulations with
an increasing number of balls (e.g., 10, 100, 1000, 10000). Be sure to set the speed to Maximum
Speed for larger simulations.
In fact, the Demoivre-Laplace theorem is a special case of a much more powerful result which
is known as the Central Limit Theorem. This asserts that the distribution of the sum of a
large number of independent, identically-distributed random variables with any distribution with
ﬁnite mean and ﬁnite variance is approximately normal. In the setting of the Demoivre-Laplace
theorem, the individual random variables are all Bernoulli, but there is nothing special about

5.6. TRANSFORMATIONS OF RANDOM VARIABLES
63
the Bernoulli distribution. Indeed, we could just as easily add Poisson or exponential or beta-
distributed random variables and the sum would still be approximately normally distributed.
Later in the course we will use moment generating functions to prove that the CLT holds for
sums of random variables that have ﬁnite exponential moments and then we will be able to
deduce Theorem 5.4 as a corollary.
Example 5.6. Suppose that a fair coin is tossed 100 times. What is the probability that the
number of heads obtained is between 45 and 55 (inclusive)?
Solution: If X denotes the number of heads obtained in 100 tosses, then X is a binomial random
variable with parameters (100, 1/2). By the Demoivre-Laplace theorem, we know that
P{45 ≤X ≤55}
=
P

−1 ≤X −50
5
≤1

≈
Φ(1) −Φ(−1)
=
Φ(1) −(1 −Φ(1))
=
2Φ(1) −1 = 0.683,
where a table of Z-scores has been consulted for the numerical value of Φ(1) = 0.841. Here we
have made use of the identity
Φ(−x) = 1 −Φ(x),
which follows from the fact that if X ∼N(0, 1), then
P{X ≤−x} = P{X > x} = 1 −P{X ≤x}.
5.6
Transformations of Random Variables
In Section 5.5 we saw that an aﬃne transformation of a normal random variable is also normally
distributed and we used this observation to prove several results about normal distributions.
Since transformations of random variables are frequently used in statistics and applied probabil-
ity, we need a general set of techniques to determine their distributions. This is the subject of
this section.
Suppose that X is a real-valued random variable and let Y = g(X), where g : R →R is a
real-valued function. Then
P(Y ∈E) = P(g(X) ∈E) = P(X ∈g−1(E)),
where the set g−1(E) is the preimage of E under g
g−1(E) = {x ∈R : g(x) ∈E}.
Notice that we do not require g to be one-to-one and so g−1 may not be deﬁned as a function.
Furthermore, since in general E ̸= g−1(E), it is clear that Y and X will usually have diﬀerent
distributions. However, provided that the transformation is not too complicated, we can some-
times express the cumulative distribution function of Y in terms of that of X. We ﬁrst see how
this is done in two examples and then describe a general approach.

64
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
Example 5.7. Let X be a non-negative random variable with CDF FX and let Y = Xn, where
n ≥1. Then, for any x ≥0,
FY (x)
=
P(Y ≤x)
=
P(Xn ≤x)
=
P(X ≤x1/n)
=
FX
 x1/n
,
while for any x < 0, we have FY (x) = 0. Similarly, if X has a density pX = F ′
X(x), then Y also
has a density given by
pY (x) =
d
dxFY (x) =



0
if x < 0
1
nx
1
n−1pX
 x1/n
if x ≥0.
Example 5.8. Let X be a continuous real-valued random variable with density pX(x) = F ′
X(x)
and let Y = X2. Then, for x ≥0,
FY (x)
=
P{Y ≤x}
=
P{X2 ≤x}
=
P{−√x ≤X ≤√x}
=
P{−√x < X ≤√x}
=
FX(√x) −FX(−√x),
where we have used the fact that P(X = −√x) = 0 (since X is continuous). In contrast, if x < 0,
then FY (x) = 0. In this case, the density of Y is given by
pY (x) =
d
dxFY (x) =



0
if x < 0
1
2√x
 pX
 √x

+ pX
 −√x

if x ≥0.
Notice that we obtain diﬀerent expressions for FY (x) in Examples 5.7 and 5.8 in the case n = 2
depending on whether X is strictly non-negative or can take on both positive and negative val-
ues. The reason for this is that the function x →x2 is one-to-one when restricted to [0, ∞) but
is two-to-one when considered on the entire real line.
Monotonic transformations: Suppose that X is a real-valued random variable with CDF FX
and let Y = g(X), where the function g : R →(l, r) is strictly increasing, continuous, one-to-one
and onto, i.e., if x < y, then g(x) < g(y). Then g is invertible on the interval (l, r) and so the
CDF of Y can be expressed as
FY (x)
=
P{Y ≤x}
=
P{g(X) ≤x}
=
P{X ≤g−1(x)}
=
FX
 g−1(x)

,
for any x ∈(l, r). Of course, if x ≥r, then P{Y ≤x} = 1, while if x ≤l, then P{Y ≤x} = 0.
Combining these facts, we obtain the general expression
FY (x) =



0
if x ≤l
FX
 g−1(x)

if x ∈(l, r)
1
if x ≥r.

5.6. TRANSFORMATIONS OF RANDOM VARIABLES
65
If, instead, g is a decreasing function, then
FY (x)
=
P{g(X) ≤x}
=
P{X ≥g−1(x)}
=
1 −FX(g−1(x)−)
where FX(x−) = limxn↑x FX(xn) is the left-handed limit of FX at x. Thus, in this case, the
CDF of Y has the form
FY (x) =



0
if x ≤l
1 −FX
 g−1(x) −

if x ∈(l, r)
1
if x ≥r.
Of course, if X is continuous, then FX(x−) = FX(x) and the last result can be simpliﬁed.
Example 5.9. Suppose that U is a standard normal random variable and let g be the function
g(y) = (b−a)y +a, where b ̸= a, and set Y = g(U). If b > a, then a ≤Y ≤b and the cumulative
distribution function of Y is
FY (x)
=
P((b −a)U + a ≤x)
=
P

U ≤x −a
b −a

=
x −a
b −a ,
provided x ∈[a, b]. This shows that Y is uniform on the interval [a, b]. On the other hand, if
b < a, then b ≤Y ≤a and
FY (x)
=
P((b −a)U + a ≤x)
=
P

U ≥x −a
b −a

=
1 −P

U ≤x −a
b −a

=
1 −a −x
a −b
=
x −b
a −b,
whenever x ∈[b, a], which shows that Y is uniform on the interval [b, a]. In particular, by taking
a = 1 and b = 0, we see that Y ≡1 −U is uniform on [0, 1], i.e., 1 −U is also a standard
uniform random variable. We will use this observation below in Example 6.4.
The next proposition describes an important application of these identities.
Theorem 5.5. Suppose that U is a uniform random variable on (0, 1) and let X be a real-valued
random variable with a strictly increasing continuous CDF FX(·). Then the random variable
Y = F −1
X (U) has the same distribution as X, while the random variable Z = FX(X) is uniformly
distributed on (0, 1).
Proof. First consider the distribution of Y :
FY (x)
=
P{Y ≤x}
=
P{F −1
X (U) ≤x}
=
P{U ≤FX(x)}
=
FX(x),
since FX(x) ∈[0, 1] for all x and P{U ≤y} = y whenever y ∈[0, 1]. This shows that Y and X
have the same distribution.

66
CHAPTER 5. A MENAGERIE OF DISTRIBUTIONS
Similarly, the CDF of Z = FX(X) is
FZ(x)
=
P{Z ≤x}
=
P{FX(X) ≤x}
=
P{X ≤F −1
X (x)}
=
FX(F −1
X (x))
=
x,
for any x ∈[0, 1], which shows that Z is a uniform random variable on [0, 1].
This result has important applications in computational statistics and stochastic simulations. For
example, many computational statistics packages include routines that will simulate a sequence
of independent uniform random variables U1, U2, · · · (or some suitable approximation thereof).
Then, if X is a random variable with a strictly increasing CDF, we can generate a sequence of
independent random variables X1, X2, · · · , each having the same distribution as X, by setting
Xi = F −1
X (Ui). This is computationally eﬃcient only if the inverse F −1
X
can be easily evaluated.
Example 5.10. Recall that the CDF of the exponential distribution with parameter λ is
FX(x) = 1 −e−λx.
A simple calculation shows that
F −1
X (x) = −1
λ ln(1 −x),
and so it follows that if U is uniform on [0, 1], then
Y = −1
λ ln(1 −U)
is exponentially distributed with parameter λ. In fact, because the distribution of 1 −U is also
uniform on [0, 1], the random variable
Y ′ = −1
λ ln(U)
is also exponentially distributed with parameter λ.
If X is a continuous random variable and g is a strictly monotonic function with a diﬀerentiable
inverse, then it can be shown that the random variable Y = g(X) is also continuous and the
density of Y can be expressed in terms of g and the density of X. This relationship is described
in the next proposition.
Proposition 5.4. Let X be a continuous random variable with density pX(·) and suppose that
g is a strictly monotonic continuous function with diﬀerentiable inverse g−1. Then Y = g(X) is
a continuous random variable with density function
pY (y) =





 d
dyg−1(y)
 · pX
 g−1(y)

if y = g(x) for some x
0
otherwise.

5.6. TRANSFORMATIONS OF RANDOM VARIABLES
67
Proof. Without loss of generality, we may assume that g is increasing. Suppose that y = g(x)
for some x. Then, as shown above, FY (y) = FX
 g−1(y)

, and diﬀerentiating with respect to y
gives
pY (y)
=
d
dyFY (y)
=
 d
dyg−1(y)
 · pX
 g−1(y)
.

Chapter 6
Random Vectors and Multivariate Distributions
6.1
Joint and Marginal Distributions
Because the world is complex and multi-faceted, scientiﬁc investigations are usually faced with
a multitude of variables whose relationships to one another are only poorly understood. For
example, if we are interested in understanding temporal ﬂuctuations in quantities such as ocean
surface temperatures or currency exchange rates, then we will need to study time series of
data that can be represented as a sequence of random variables, X1, X2, · · · , where the variable
Xt denotes the quantity at interest at the time of the t’th measurement. Similarly, if we are
conducting a genome-wide association study (GWAS) to identify genetic variants that predis-
pose individuals to complex diseases such as cancer or diabetes or schizophrenia, then we need
to consider an array of random variables, Xi1, · · · , Xin, Yi, where Yi indicates whether the i’th
individual in the study is aﬀected (a case) or not (a control) and Xij indicates which nucleotide
{A, T, C, G} is present at the j’th site in this individual.
To build probabilistic models for these kinds of data, we need to consider groups of random
variables X1, · · · , Xn that are all deﬁned on the same probability space. Provided that these
are ordered in some manner, we can treat each group of variables as a random vector X =
(X1, · · · , Xn) which is a map from the sample space Ωinto Rn:
X(ω) ≡(X1(ω), · · · , Xn(ω)) ∈Rn.
Our main goal in this chapter is to develop some tools that can be used to describe and study
random vectors.
Deﬁnition 6.1. Suppose that X1, X2, · · · , Xn are real-valued random variables deﬁned on the
same probability space (Ω, F, P). Then the joint distribution of these variables is the probability
distribution Q of the random vector X = (X1, · · · , Xn) deﬁned by
Q(E) = P
 (X1, · · · , Xn) ∈E

for any measurable subset E ⊂Rn. Likewise, the joint cumulative distribution function of
(X1, · · · , Xn) is deﬁned by
F(x1, · · · , xn) = P (X1 ≤x1, · · · , Xn ≤xn) .
As with real-valued random variables, both the joint distribution and the joint CDF are fully
determined by the other.
This means that if two random vectors X = (X1, · · · , Xn) and
Y = (Y1, · · · , Yn) have the same joint cumulative distribution function, then their components
68

6.1. JOINT AND MARGINAL DISTRIBUTIONS
69
have the same joint distribution.
If X = (X1, · · · , Xn) is a random vector, then each variable Xi is itself a real-valued random
variable and the distribution of Xi is called the marginal distribution of Xi.
Note that
each component variable has a marginal distribution. By exploiting the continuity properties of
probability distributions, the marginal cumulative distribution function of Xi can be determined
from the joint cumulative distribution of the random vector. For example, the marginal CDF of
X1 is equal to
FX1(x)
=
P(X1 ≤x)
=
P(X1 ≤x, X2 < ∞, · · · , Xn < ∞)
=
P

[
M≥1
{X1 ≤x, X2 < M, · · · , Xn < M}


=
lim
M→∞FX(x1, M, · · · , M).
This process of recovering the marginal distribution of a single random variable from the joint
distribution of the random vector is called marginalisation.
Caveat: The joint distribution of the variables (X1, · · · , Xn) fully determines all of the marginal
distributions. However, knowledge of the marginal distributions alone is not suﬃcient to deter-
mine the joint distribution: there are usually inﬁnitely many joint distributions that have the
same marginal distributions.
Deﬁnition 6.2. If X1, · · · , Xn are discrete random variables deﬁned on the same probability
space and Xi takes values in the countable set Ei, then X = (X1, · · · , Xn) is a discrete random
vector with values in the product space E = E1 × · · · × En and the joint probability mass
function of X is deﬁned by
p(x1, · · · , xn) = P(X1 = x1, · · · , Xn = xn).
If we are given the joint probability mass function pX of a discrete random vector X = (X1, · · · , Xn),
then we can ﬁnd the marginal probability mass functions of the variables Xi by summing pX
over appropriate subsets of E. For example, the marginal probability mass function of X1 is
given by the following sum:
pX1(x) = P(X1 = x) =
X
x2,··· ,xn
pX(x, x2, · · · , xn).
In other words, we can calculate the marginal probability mass function for Xi at a point x ∈Ei
by summing pX over all possible vectors in E with the i’th coordinate held ﬁxed at x.
Example 6.1. Suppose that n independent experiments are performed, each of which can have
one of r possible outcomes (labeled 1, · · · , r) with respective probabilities p1, · · · , pr, where p1 +
· · · + pr = 1. Let Xi denote the number of experiments with outcome i. Then X = (X1, · · · , Xn)
is a discrete random vector with probability mass function
p(n1, · · · , nr) = P{X1 = n1, · · · , Xr = xr} =

n
n1, · · · , nr

pn1
1 · · · pnr
r ,
and X is said to have the multinomial distribution with parameters n and (p1, · · · , pr). No-
tice that X1 + · · · + Xn = n. Of course, if r = 2, then this is just the binomial distribution with
parameters n and p1.

70
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
Deﬁnition 6.3. The random variables X1, · · · , Xn are said to be jointly continuous if there
exists a function p(x1, · · · , xn), called the joint probability density function, such that
P{(X1, · · · , Xn) ∈E} =
Z Z
· · ·
Z
(x1,··· ,xn)∈E
p(x1, · · · , xn)dx1 · · · dxn
for any measurable subset E ⊂Rn.
As in the univariate case, the joint density function can be found by diﬀerentiating the joint
cumulative distribution function.
However, when there are multiple variables, we must take
partial derivatives for each component of the vector:
pX(x1, · · · , xn) = ∂x1 · · · ∂xnFX(x1, · · · , xn).
Furthermore, if the variables X1, · · · , Xn are jointly continuous, then each variable Xi is marginally
continuous and its marginal density function can be found by integrating the joint density func-
tion over all coordinates except xi:
pXi(x) =
Z ∞
−∞
· · ·
Z ∞
−∞
pX(x1, · · · , xi−1, x, xi+1, · · · , xn)dx1 · · · dxi−1dxi+1 · · · dxn.
Caveat: X1, · · · , Xn can all be individually continuous even when they are not jointly continu-
ous. For example, the random vector (X, X) is never continuous even if X is itself a continuous
random variable.
Example 6.2. The random vector X = (X1, · · · , Xn−1) is said to have the Dirichlet distribu-
tion with parameters α1, · · · , αn > 0 if its components are jointly continuous with joint density
function
pX(x1, · · · , xn−1) =
Γ(α)
Qn
i=1 Γ(αi)
n
Y
i=1
xαi−1
i
for all x1, · · · , xn−1 ≥0 satisfying x1 + · · · + xn−1 ≤1, where xn ≡1 −x1 −· · · −xn−1. Since
x1 + · · · + xn = 1 by construction, the Dirichlet distribution is usually regarded as a continuous
distribution on the standard n −1-dimensional simplex
Kn−1 =
(
(x1, · · · , xn) ∈Rn :
n
X
i=1
xi = 1 and xi ≥0 for all i
)
When n = 2, the Dirichlet distribution reduces to the Beta distribution.
6.2
Independent Random Variables
Deﬁnition 6.4. A collection of n random variables X1, · · · , Xn, all deﬁned on the same proba-
bility space, are said to be independent if for every collection of measurable sets A1, · · · , An,
P{X1 ∈A1, · · · , Xn ∈An} =
n
Y
i=1
P{Xi ∈Ai}.
In other words, the random variables X1, · · · , Xn are independent if and only if for all such sub-
sets A1, · · · , An ⊂R, the sets {X1 ∈A1}, · · · , {Xn ∈An} are independent. Also, we say that an
inﬁnite collection of random variables is independent if every ﬁnite subcollection is independent.

6.2. INDEPENDENT RANDOM VARIABLES
71
Heuristically, X1 and X2 are independent if knowing the value of X1 provides us with no infor-
mation about the value of X2, and vice versa.
Proposition 6.1. Suppose X1, · · · , Xn are jointly distributed random variables, i.e., they are
all deﬁned on the same probably space, and let X = (X1, · · · , Xn). Then each of the following
conditions implies independence:
(a) The joint cumulative distribution function factors into a product of the marginal cumulative
distribution functions, i.e., for all real numbers x1, · · · , xn,
FX(x1, · · · , xn) =
n
Y
i=1
FXi(xi).
(b) X1, · · · , Xn are discrete and the joint probability mass function factors into a product of
the marginal probability mass functions:
pX(x1, · · · , xn) =
n
Y
i=1
pXi(xi)
for all x1, · · · , xn.
(c) X1, · · · , Xn are jointly continuous and the joint probability density function factors into a
product of the marginal probability density functions:
fX(x1, · · · , xn) =
n
Y
i=1
fXi(xi)
for all x1, · · · , xn.
Example 6.3. Let Z be a Poisson random variable with parameter λ, let W1, W2, · · · be a
collection of independent Bernoulli random variables, each with parameter p, and deﬁne
X
=
Z
X
i=1
Wi
Y
=
Z −X.
Then X and Y are independent Poisson random variables with parameters λp and λ(1 −p),
respectively. To prove this, we calculate
P{X = i, Y = j}
=
P{X = i, Z = i + j}
=
P{X = i|Z = i + j} · P{Z = i + j}
=
i + j
i

pi(1 −p)j · e−λ λi+j
(i + j)!
=
e−λp(λp)i
i!
e−λ(1−p)(λ(1 −p))j
j!
≡
pX(i) · pY (j),
where pX and pY are the probability mass functions of Poisson random variables with parameters
λp and λ(1 −p).
Independence of X and Y then follows from the factorization of the joint
probability mass function. Furthermore, since Z = X + Y , we have also shown that the sum of
two independent Poisson-distributed random variables is also Poisson-distributed with parameter
equal to the sum of the parameters of those two variables.

72
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
6.3
Conditional Distributions
6.3.1
Discrete Conditional Distributions
Deﬁnition 6.5. If X and Y are discrete random variables with joint probability mass function
pX,Y (x, y), then we deﬁne the conditional probability mass function of X given Y = y by
pX|Y (x|y)
=
P{X = x|Y = y}
=
P{X = x, Y = y}
P{Y = y}
=
pX,Y (x, y)
pY (y)
,
provided pY (y) > 0.
In other words, since X and Y are deﬁned on the same probability space, we can consider the
distribution of X conditional on the event Y = y. This is of great importance in statistics, since
we are often interested in the distributions of variables that can only be indirectly estimated
from the values of other correlated random variables. Of course, if X and Y are independent,
then the conditional probability mass function of X given Y = y is equal to the unconditioned
probability mass function:
pX|Y (x|y)
=
pX,Y (x, y)
pY (y)
=
pX(x)pY (y)
pY (y)
=
pX(x).
In other words, if X and Y are independent, then Y provides us with no information concerning
the value of X.
Example 6.4. Suppose that the X and Y are independent Poisson RVs with parameters λ1 and
λ2 and let Z = X + Y . Then, using the fact (see Example 6.3) that Z is a Poisson RV with
parameter λ1 + λ2, the conditional probability mass function of X given Z = n is
pX|Z(k|n)
=
P{X = k, Z = n}
P{Z = n}
=
P{X = k, Y = n −k}
P{Z = n}
=
P{X = k}P{Y = n −k}
P{Z = n}
=
e−λ1 λk
1
k! · e−λ2
λn−k
2
(n −k)! ·

e−(λ1+λ2) (λ1 + λ2)n
n!
−1
=
n
k
 
λ1
λ1 + λ2
k 
λ2
λ1 + λ2
n−k
,
for k = 0, · · · , n, and so it follows that the conditional distribution of X given X + Y = n is
Binomial with parameters n and p = λ1/(λ1 + λ2). Notice that once we condition on the event
X + Y = n, X and Y are no longer independent.
The next example illustrates how the conditional joint distribution of more than two RVs can be
calculated.

6.3. CONDITIONAL DISTRIBUTIONS
73
Example 6.5. Recall that (X1, · · · , Xk) has the multinomial distribution with parameters n and
(p1, · · · , pk), p1 + · · · + pk = 1, if the joint probability mass function has the form
P{X1 = n1, · · · , Xk = nk} =
n!
n1! · · · nk!pn1
1 · · · pnk
k ,
ni ≥0,
k
X
i=1
ni = n.
Notice that by combining groups of the component variables, we again obtain a multinomial
RV. For example, if we combine the ﬁrst r categories into a single type and we deﬁne Fr =
1 −pr+1 −· · · −pk = p1 + · · · + pr, then the random vector (X1 + · · · + Xr, Xr+1, · · · , Xk) has
the multinomial distribution with parameters n and (Fr, pr+1, · · · , pk). This can be shown by
calculating
P{X1 + · · · + Xr = l, Xr+1 = nr+1, · · · , Xk = nk}
=
X
n1+···+nr=l
P{X1 = n1, · · · , Xk = nk}
=
X
n1+···+nr=l
n!
n1! · · · nk!pn1
1 · · · pnr
r pnr+1
r+1 · · · pnk
k
=
n!
l!nr+1! · · · nk!F l
rpnr+1
r+1 · · · pnk
k ,
provided l + nr+1 + · · · + nk = n. The last identity is a consequence of the multinomial formula.
Now suppose that we observe that Xr+1 = nr+1, · · · , Xk = nk. Then, if we set m = nr+1 +
· · · + nk and let Fr be deﬁned as in the preceding paragraph, the conditional joint distribution of
(X1, · · · , Xr) given this observation is equal to
P{X1 = n1, · · · , Xr = nr|Xr+1 = nr+1, · · · , Xk = nk}
=
P{X1 = n1, · · · , Xk = nk}
P{Xr+1 = nr+1, · · · , Xk = nk}
=
n!
n1! · · · nk!pn1
1 · · · pnr
r pnr+1
r+1 · · · pnk
k ·

n!
(n −m)!nr+1! · · · nk!F n−m
r
pnr+1
r+1 · · · pnr
k
−1
=
(n −m)!
n1! · · · nr!
 p1
Fr
n1
· · ·
 pr
Fr
nr
.
In other words, the conditional distribution of (X1, · · · , Xr) given the values of the random vari-
ables Xr+1, · · · , Xk is again multinomial with parameters n −m and (p1/Fr, · · · , pr/Fr).
6.3.2
Continuous Conditional Distributions
Deﬁnition 6.6. If X and Y are jointly continuous random variables with joint probability density
pX,Y (x, y) and marginal densities pX(x) and pY (y), then for any y such that pY (y) > 0, we can
deﬁne the conditional probability density of X given that Y = y by
pX|Y (x|y)
=
lim
h→0
1
hP{X ∈(x, x + h]|Y ∈(y, y + h]}
=
lim
h→0
1
h2 P{X ∈(x, x + h], Y ∈(y, y + h]}
1
hP{Y ∈(y, y + h]}
=
pX,Y (x, y)
pY (y)
,
provided pY (y) > 0.

74
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
Because Y is marginally continuous, we are eﬀectively conditioning on an event with zero prob-
ability: P{Y = y} = 0. That we are able to make sense of this is a consequence of the fact
that X and Y are jointly continuous: this controls the singularity in the ratio that deﬁnes the
conditional probability.
Remark: If X and Y are independent, then the conditional probability density of X given
Y = y is equal to the unconditioned probability density:
pX|Y (x|y)
=
pX,Y (x, y)
pY (y)
=
pX(x)pY (y)
pY (y)
=
pX(x).
Example 6.6. We say that a random vector (X, Y ) has a bivariate normal distribution if
there exist parameters (µx, µy) and σx > 0, σy > 0, −1 < ρ < 1 such that the joint density of X
and Y is
pX,Y (x, y)
=
1
2πσxσy
p
1 −ρ2 exp
(
−
1
2(1 −ρ2)
"x −µx
σx
2
+
y −µy
σy
2
−2ρ(x −µx)(y −µy)
σxσy
#)
.
The conditional density of X given that Y = y is equal to
pX|Y (x|y)
=
pX,Y (x, y)
pY (y)
=
C1pX,Y (x, y)
=
C2 exp
(
−
1
2(1 −ρ2)
"x −µx
σx
2
−2ρx(y −µy)
σxσy
#)
=
C3 exp

−
1
2σ2x(1 −ρ2)

x2 −2x

µx + ρσx
σy
(y −µy)

=
C4 exp
(
−
1
2σ2x(1 −ρ2)

x −

µx + ρσx
σy
(y −µy)
2)
.
Apart from the constant C4, which can be evaluated explicitly by using the fact that
R ∞
−∞pX|Y (x|y)dx =
1, we recognize the conditional density as the density of a normal RV with
• mean = µx + ρ(σx/σy)(y −µy)
• variance = σ2
x(1 −ρ2).
Remark: Notice that X and Y are independent if and only if ρ = 0.
6.3.3
Conditional Expectations
Deﬁnition 6.7. If X and Y are jointly discrete random variables with conditional probability
mass function pX|Y (x|y), then the conditional expectation of X given Y = y is the quantity
E

X|Y = y

=
X
x
x · P{X = x|Y = y} =
X
x
pX|Y (x|y) · x.
Similarly, if X and Y are jointly continuous with conditional density function pX|Y (x|y), then
the conditional expectation of X given Y = y is deﬁned by
E

X|Y = y

=
Z ∞
−∞
x · pX|Y (x|y)dx.

6.3. CONDITIONAL DISTRIBUTIONS
75
Example 6.7. Suppose that X and Y are independent Poisson RVs, both with parameter λ, and
let Z = X + Y . We wish to ﬁnd the conditional expectation of X given that Z = n. To do so,
we ﬁrst recall from Example 6.4 that the conditional distribution of X given Z = n is binomial
with parameters (n, 1/2):
pX|Z(k|n) =
n
k
 1
2
n
,
for k = 0, · · · , n. Consequently,
E

X|Z = n

=
n
X
k=0
n
k
 1
2
n
k = n
2 .
We can also solve this problem by observing that
n = E[Z|Z = n] = E[X|Z = n] + E[Y |Z = n] = 2E[X|Z = n],
using the fact that E[X|Z = n] = E[Y |Z = n] since X and Y have the same conditional distri-
bution given Z.
In general, all of the results that hold for expectations also hold for conditional expectations,
including the following useful identity:
E

g(X)|Y = y

=



P
x g(x)pX|Y (x|y)
discrete case
R ∞
∞g(x)pX|Y (x|y)dx
continuous case.
The next deﬁnition introduces a subtle concept that can be used to great eﬀect when calculating
expected values. This is particularly important in the theory of stochastic processes and Markov
chains.
Deﬁnition 6.8. The conditional expectation of X given Y is the random variable
E[X|Y ] = H(Y ),
where H(y) is the (deterministic) function deﬁned by the formula
H(y) = E[X|Y = y].
Deﬁnition 6.8 should be compared and contrasted with Deﬁnition 6.7: whereas the conditional
expectation E[X|Y = y] deﬁned in 6.7 is a number, the conditional expectation E[X|Y ] deﬁned
in 6.8 is a random variable. The diﬀerence arises because in the former case we are conditioning
on a speciﬁc event, namely Y = y, and asking for the mean value of X under the assumption
that this particular event is true, whereas in the latter case we are conditioning on a random
variable and asking for the mean value of X as a function of the unknown random value that Y
might take.
Example 6.8. Suppose that X, Y , and Z are as in Example 6.7. Then
E[X|Z] = 1
2Z
since h(z) = E[X|Z = z] = 1
2z.

76
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
Our next result states that the expected value of a random variable X can be calculated by aver-
aging the conditional expectation of X given any other random variable Y over the distribution
of Y .
Proposition 6.2. For any two random variables X and Y , we have
E[X] = E[E[X|Y ]],
provided that both E[X] and E[X|Y ] exist. In particular, if Y is discrete, then
E[X] =
X
y
E[X|Y = y]P{Y = y},
while if Y is continuous with density pY (y), then
E[X] =
Z ∞
−∞
E[X|Y = y]pY (y)dy.
Proof. We will assume that X and Y are both discrete. Then
X
y
E[X|Y = y]P{Y = y}
=
X
y
X
x
xP{X = x|Y = y}P{Y = y}
=
X
y
X
x
xP{X = x, Y = y}
P{Y = y}
P{Y = y}
=
X
y
X
x
xP{X = x, Y = y}
=
X
x
x
X
y
P{X = x, Y = y}
=
X
x
xP{X = x}
=
E[X].
6.4
Expectations of Sums and Products of Random Variables
We begin with a general result concerning the expectation of a function of two random variables.
Proposition 6.3. If X and Y have a joint probability mass function pX,Y (x, y) and g : R2 →R,
then
E[g(X, Y )] =
X
y
X
x
pX,Y (x, y)g(x, y).
Similarly, if X and Y are jointly continuous with joint density pX,Y (x, y), then
E[g(X, Y )] =
Z ∞
−∞
Z ∞
−∞
pX,Y (x, y)g(x, y)dxdy.
Proof. By expressing g as the diﬀerence between its positive and negative parts, g = g+ −g−,
it suﬃces to prove the result under the assumption that g is non-negative. Here we will assume

6.4. EXPECTATIONS OF SUMS AND PRODUCTS OF RANDOM VARIABLES
77
that X and Y are jointly continuous; a similar proof works in the case where both are discrete.
Then, by exercise 7 of PS3, we have
E[g(X, Y )]
=
Z ∞
0
P{g(X, Y ) > t}dt
=
Z ∞
0
Z Z
(x,y):g(x,y)>t
pX,Y (x, y)dx dy dt
=
Z ∞
−∞
Z ∞
−∞
Z g(x,y)
0
pX,Y (x, y)dt dx dy
=
Z ∞
−∞
Z ∞
−∞
pX,Y (x, y)g(x, y)dx dy,
where the interchange of integrals in passing from the second to the third line is justiﬁed by the
fact that the integrand is everywhere non-negative.
Example 6.9. Suppose that X and Y are independent RVs that are uniformly distributed on
[0, 1]. Then
E

|X −Y |

=
Z 1
0
Z 1
0
|x −y|dydx
=
Z 1
0
Z x
0
(x −y)dy +
Z 1
x
(y −x)dy

dx
=
Z 1
0
 x2 −x2/2 + (1 −x2)/2 −x(1 −x)

dx
=
Z 1
0

x2 −x + 1
2

dx
=
1
3.
The following corollary states that the expected value of a sum of random variables is equal
to the sum of the expected values of those variables. This is true even when the variables are
not independent. Put another way, expectation is a linear functional on the space of random
variables with ﬁnite expectations.
Corollary 6.1. Suppose that X1, X2, · · · , Xn are random variables deﬁned on the same proba-
bility space and that E[Xi] is ﬁnite for all i = 1, · · · , n. Then
E
" n
X
i=1
Xi
#
=
n
X
i=1
E[Xi].
Proof. If the Xi are all discrete or all jointly continuous, the corollary follows from Proposition
6.3 by taking g(x, y) = x + y and then proceeding by induction on n.
Example 6.10. Suppose that X1, · · · , Xn are independent, identically-distributed random vari-
ables, each with expected value µ. Then the sample mean of the Xi’s is just the unweighted
average of the sample:
¯X = 1
n
n
X
i=1
Xi.

78
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
Notice that
E[ ¯X]
=
E
"
1
n
n
X
i=1
Xi
#
=
1
n
n
X
i=1
E[Xi]
=
1
n · nµ
=
µ,
i.e., the expected value of the sample mean is equal to the mean of the distribution. For this
reason, the sample mean is said to be an unbiased estimator of the expected value.
Example 6.11. Expectation of a binomial RV: Recall that if X1, · · · , Xn are independent
Bernoulli RVs, each with parameter p, then their sum X = X1 · · · Xn is a Binomial RV with
parameters n and p. A simple calculation shows that E[Xi] = p · 1 + (1 −p) · 0 = p for each i.
Consequently,
E[X] =
n
X
i=1
E[Xi] = np,
as we showed earlier using moment generating functions.
Example 6.12. Let X be the number of cards that are left in their original position by a random
permutation of a deck of n cards. Then X can be written as the sum X = X1 + · · · + Xn, where
Xi is the indicator function of the event that the i’th card is left in its original position. Notice
that each Xi is a Bernoulli RV with parameter p = 1/N, but the Xi’s are not independent.
Nonetheless,
E[X]
=
n
X
i=1
E[Xi]
=
n · 1
n
=
1.
Example 6.13. Random Walks on Z. Suppose that X1, X2, · · · is a collection of independent
RVs, each with distribution P{Xi = 1} = P{Xi = −1} = 1
2, and let SN = X1 + · · · + XN. We
can think of SN as the position at time N of a particle which in each unit interval of time is
equally likely to move to the right or to the left. Notice that
E[SN] =
N
X
i=1
E[Xi] = 0,
since E[Xi] = 1
2 · 1 + 1
2 · (−1) = 0 for each i. Thus, the expected position of the particle never
changes. A better measure of the displacement of the particle away from its initial position at 0
is given by the root mean square displacement (RMSQ) of SN
DN ≡
q
E

S2
N

.
To ﬁnd DN, we begin by calculating
D2
N
=
E

S2
N

=
E

(X1 + · · · + XN)2
=
E
" N
X
i=1
X2
i
#
+ E


X
1≤i̸=j≤N
XiXj


=
NE

X2
1

+ N(N −1)E

X1X2

,

6.4. EXPECTATIONS OF SUMS AND PRODUCTS OF RANDOM VARIABLES
79
where to obtain the last line, we have the used the identities E[X2
i ] = E[X2
1] and E[XiXj] =
E[X1X2] for all i and j such that i ̸= j. Furthermore,
E

X2
1]
=
1
2 · 12 + 1
2 · (−1)2
=
1
E

X1X2

=
1
4 · 1 · 1 + 1
41 · (−1) + 1
4 · (−1) · 1 + 1
4 · (−1) · (−1)
=
0.
Substituting these values into the previous equation shows that D2
N = N and so the RMSD of the
particle is
DN = N1/2.
Notice that DN diverges as N →∞, but at a rate that is sublinear in N.
6.4.1
Expectations of products
Proposition 6.4. If X and Y are independent, then for any pair of real-valued functions h and
g,
E[h(X)g(Y )] = E[h(X)] · E[g(Y )],
provided that the expectations appearing on both sides of the identity exist.
Proof. We will prove this under the assumption that X and Y are jointly continuous with joint
density pX,Y = pX · pY . Then
E

h(X)g(Y )

=
Z ∞
−∞
Z ∞
−∞
h(x)g(y)pX,Y (x, y)dxdy
=
Z ∞
−∞
Z ∞
−∞
h(x)g(y)pX(x)pY (y)dxdy
=
Z ∞
−∞
pX(x)h(x)dx
Z ∞
−∞
pY (y)g(y)dy
=
E[h(X)] · E[g(Y )].
Corollary 6.2. Taking h(x) = x and g(y) = y in Proposition 7.2 yields the important identity:
E[XY ] = E[X] · E[Y ],
provided that X, Y and XY all have ﬁnite expectations.
Corollary 6.3. If X1, · · · , Xn are independent RVs, then a simple induction argument on n
shows that
E
" n
Y
i=1
fi(Xi)
#
=
n
Y
i=1
E[fi(Xi)],
again provided that all of the expectations appearing in the formula exist.
Our next proposition establishes an important property of moment generating functions of sums
of independent random variables.

80
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
Proposition 6.5. Suppose that X1, · · · , Xn are independent RVs with moment generating func-
tions ψX1(t), · · · , ψXn(t). Then the moment generating function of the sum X = X1 + · · · + Xn
is
ψX(t)
=
E
h
et(X1+···+Xn)i
=
n
Y
i=1
E

etXi
=
n
Y
i=1
ψXi(t),
i.e., the moment generating function of a sum of independent RVs is just the product of the
moment generating functions of the individual RVs.
Example 6.14. Suppose that X1, · · · , Xn are independent Poisson RVs with parameters λ1, · · · , λn,
respectively. Then the moment generating function of the sum X = X1 + · · · + Xn is
ψX(t)
=
n
Y
i=1
e−λi(1−et)
=
exp
 
−(1 −et)
n
X
i=1
λi
!
,
which shows that X is a Poisson RV with parameter Pn
i=1 λi. Compare this with Example 6.3.
6.5
Covariance and Correlation
Deﬁnition 6.9. If X and Y are RVs with means µX = E[X] and µY = E[Y ], then the covari-
ance between X and Y is the quantity
Cov(X, Y )
=
E

(X −µX)(Y −µY )]
=
E[XY ] −µX · µY .
Remark: The covariance of two random variables is a measure of their association. The covari-
ance is positive if when X is larger than µX, then Y also tends to be larger than µY , while the
covariance is negative if the opposite pattern holds. In particular, if X and Y are independent,
then
Cov(X, Y ) = E[X −µX]E[Y −µY ] = 0,
because knowledge of how X compares with its mean provides no information about Y or vice
versa. Observe that the converse is not true. There are random variables that have zero covari-
ance but which are not independent. For example, suppose that X has the distribution
P{X = 0} = P{X = 1} = P{X = −1} = 1
3,
and that the random variable Y is deﬁned by
Y =
 0
if X ̸= 0
1
if X = 0.
Then E[X] = 0 and also E[XY ] = 0 because XY = 0 with probability 1, so that
Cov(X, Y ) = E[XY ] −E[X]E[Y ] = 0,

6.5. COVARIANCE AND CORRELATION
81
but X and Y clearly are not independent. For example,
P{X = 0|Y = 1} = 1 ̸= 1
3 = P{X = 0}.
Any two random variables X and Y are said to be uncorrelated if Cov(X, Y ) = 0, regardless
of whether or not they are independent.
The next proposition lists some useful properties of covariances.
Proposition 6.6.
(a) Cov(X, Y ) = Cov(Y, X)
(b) Cov(X, X) = V ar(X)
(c) Cov(aX, Y ) = aCov(X, Y ) for any scalar a
(d) Cov


n
X
i=1
Xi,
m
X
j=1
Yj

=
n
X
i=1
m
X
j=1
Cov(Xi, Yj).
Each property can be proved directly from the deﬁnition of the covariance and the linearity
properties of expectations.
A very useful formula follows by combining parts (b) and (d) of the preceding proposition:
V ar
 n
X
i=1
Xi
!
=
n
X
i=1
V ar(Xi) +
X
1≤i̸=j≤n
Cov(Xi, Xj).
In particular, if the Xi’s are pairwise independent, then Cov(Xi, Xj) = 0 whenever i ̸= j, so
that
V ar
 n
X
i=1
Xi
!
=
n
X
i=1
V ar(Xi).
If, in addition to being pairwise independent, the Xi’s are identically distributed, say with
variance V ar(Xi) = σ2, then
V ar
 n
X
i=1
Xi
!
= nσ2.
Example 6.15. If X1, · · · , Xn are independent Bernoulli RVs, each with parameter p, then
X = X1 + · · · + Xn is a Binomial RV with parameters (n, p). Because the variance of each
Bernoulli RV is p(1 −p), the preceding formula for the variance of a sum of IID RVs shows that
V ar(X) = np(1 −p),
conﬁrming our previous calculation using moment generating functions.
Example 6.16. If X1, · · · , Xn is a collection of IID (independent, identically-distributed) ran-
dom variables with mean µ and variance σ2, then the sample mean ¯X and the sample variance
S2 are the quantities deﬁned by
¯X
=
1
n
n
X
i=1
Xi
S2
=
1
n −1
n
X
i=1
 Xi −¯X
2.

82
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
If we think of the Xi’s as the outcomes of a sequence of independent replicates of some experiment,
then the sample mean and sample variance are often used to estimate the true mean and true
variance of the underlying distribution. Recall that in the previous section we showed that the
sample mean is an unbiased estimator for the true mean:
E[ ¯X] = µ.
Although unbiasedness is a desirable property of an estimator, we also care about the variance of
the estimator about its expectation. For the sample mean, we can calculate
V ar( ¯X)
=
 1
n
2
V ar
 n
X
i=1
Xi
!
=
 1
n
2
n
X
i=1
V ar(Xi)
=
σ2
n ,
which shows that the variance of the sample mean is inversely proportional to the number of
independent replicates that have been performed. We can also show that the sample variance is
an unbiased estimator of the true variance. We ﬁrst observe that
S2
=
1
n −1
n
X
i=1
 Xi −µ + µ −¯X
2
=
1
n −1
 n
X
i=1
(Xi −µ)2 −2( ¯X −µ)
n
X
i=1
(Xi −µ) +
n
X
i=1
  ¯X −µ
2
!
=
1
n −1
n
X
i=1
(Xi −µ)2 −
n
n −1( ¯X −µ)2.
Taking expectations gives
E

S2
=
1
n −1
n
X
i=1
E

(Xi −µ)2
−
n
n −1V ar( ¯X)
=
n
n −1σ2 −
n
n −1
σ2
n
=
σ2.
Informally, the reason that we must divide by n −1 rather than n to obtain an unbiased estimate
of the variance is that the deviations between the Xi’s and the sample mean (which is computed
using the observed data) tend to be smaller than the deviations between the Xi’s and the true
mean.
The next deﬁnition provides us with a dimensionless measure of the association between two
random variables.
Deﬁnition 6.10. The correlation of two random variables X and Y with variances σ2
X > 0 and
σ2
Y > 0 is the quantity
ρ(X, Y ) = Cov(X, Y )
σXσY
.

6.6. THE LAW OF LARGE NUMBERS AND THE CENTRAL LIMIT THEOREM
83
Because
ρ(aX, bY ) = ab · Cov(X, Y )
aσX · bσY
= ρ(X, Y ),
for any pair of scalar quantities a, b ̸= 0, the correlation between two random variables does not
depend on the units in which they are measured, i.e., the correlation is dimensionless. Another
important property of the correlation is that
−1 ≤ρ(X, Y ) ≤1.
(6.1)
This can be proved as follows. First note that
0
≤
V ar
 X
σX
+ Y
σY

=
V ar(X)
σ2
X
+ V ar(Y )
σ2
Y
+ 2Cov(X, Y )
σXσY
=
2(1 + ρ(X, Y )),
which implies that ρ(X, Y ) ≥−1. A similar calculation involving the variance of the diﬀerence
of X/σX and Y/σY shows that ρ(X, Y ) ≤1. If we interpret the standard deviation of a random
variable as a measure of its length or norm, then (6.1) can be interpreted as a version of the
Cauchy-Schwartz inequality.
It can be shown that if ρ(X, Y ) = 1, then with probability one such that
Y = µY + σY
σX
(X −µX),
while if ρ(X, Y ) = −1, then
Y = µY −σY
σX
(X −µX).
Thus the correlation constant can also be treated as a measure of the collinearity of two random
variables.
6.6
The Law of Large Numbers and the Central Limit Theorem
In Example 6.16, we showed that the sample mean
¯XN of a series of N independent and
identically-distributed random variables is an unbiased estimator of the true mean µ = E[X1]
and that the variance of the sample mean is inversely proportional to N:
E
 ¯XN

=
µ
Var
  ¯XN

=
σ2
N .
This suggests that when N is large, the sample mean should be close to µ and that the diﬀerence
¯XN −µ should typically be of order N−1/2 in magnitude. Our aim in this section is to explain
in what sense these statements are true.
6.6.1
The Weak and Strong Laws of Large Numbers
Proposition 6.7. (Markov’s Inequality) If X is a nonnegative real-valued RV, then for any
positive real number a > 0,
P
 X ≥a

≤E[X]
a
.

84
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
Proof. Let Ia be the indicator function of the event {X ≥a}
Ia(ω) =
 1
if X(ω) ≥a
0
otherwise.
Then, since X ≥0, we have
Ia ≤X
a .
Taking expectations of both sides of this inequality gives
P
 X ≥a

= E[Ia] ≤E[X]
a
.
Proposition 6.8. (Chebyshev’s Inequality) If X is a RV with ﬁnite mean µ and ﬁnite vari-
ance σ2, then for any positive real number k > 0,
P
 |X −µ| ≥k

≤σ2
k2 .
Proof. The result follows by taking a = k2 and applying Markov’s inequality to the non-negative
random variable (X −µ)2
P
 |X −µ| ≥k

= P
 (X −µ)2 ≥k2
≤E

(X −µ)2
k2
≤σ2
k2 .
While Markov’s inequality provides a simple bound on the tail of the distribution of X, Cheby-
shev’s inequality allows us to estimate the probability that X diﬀers greatly from its mean. In
general, both estimates tend to be crude in the sense that the probabilities in question can be
much smaller than the upper bounds given by these inequalities. However, even these crude
inequalities can be used to prove some important results.
Our next result, called the weak law of large numbers, is one of the celebrated limit theorems of
probability theory. Before stating this theorem, we introduce one of the modes of convergence
of a sequence of random variables. In fact, there are several distinct senses in which a sequence of
random variables can be said to converge to a limit and we will encounter two more of these below.
Deﬁnition 6.11. We say that a sequence of random variables, X1, X2, · · · , converges in prob-
ability to a random variable X if for every ϵ > 0,
lim
n→∞P
 |Xn −X| > ϵ

= 0.
Notice that the variables X1, X2, · · · and X must all be deﬁned on the same probability space for
this deﬁnition to make sense.
Theorem 6.1. (The Weak Law of Large Numbers) Suppose that X1, X2, · · · is a sequence
of i.i.d. variables with ﬁnite mean µ = E[X1] and let Sn = X1 + · · · + Xn. Then, for any ϵ > 0,
lim
n→∞P

Sn
n −µ
 ≥ϵ

= 0,
i.e., the sample averages 1
nSn converge in probability to the mean.

6.6. THE LAW OF LARGE NUMBERS AND THE CENTRAL LIMIT THEOREM
85
Proof. We will prove the theorem under the additional assumption that Var(Xi) = σ2 < ∞. In
this case we have
E
 1
nSn

= µ
and
V ar
 1
nSn

= σ2
n ,
and so Chebyshev’s inequality implies that for any ϵ > 0,
P
 |Sn −µ| ≥ϵ

≤σ2
nϵ2 ,
which tends to 0 as n →∞.
In other words, given any ϵ > 0, the weak law of large numbers asserts that by collecting suﬃ-
ciently many independent samples, we can make the probability that the sample mean and the
true mean diﬀer by more than an amount ϵ as close to zero as we want. Here we think of ϵ as an
upper bound on the acceptable error in our estimate. In fact, there is an even stronger sense in
which the sample mean converges to the true mean as the number of samples tends to inﬁnity.
This is the content of the next theorem, which we state without proof.
Theorem 6.2. (Strong Law of Large Numbers) Let X1, X2, · · · be a sequence of I.I.D.
random variables, each with ﬁnite mean µ = E[X], and let Sn = X1 + · · · Xn. Then,
P

lim
n→∞
1
nSn = µ

= 1,
i.e., the sequence 1
nSn converges almost surely to µ.
The signiﬁcance of the strong law of large numbers is that it shows that the frequentist concept
of probabilities can be derived using the measure-theoretic machinery introduced by Kolmogorov
in the 1930’s. In particular, suppose that (Ω, F, P) is a probability space and that E ∈F is an
event, and let X1, X2, · · · be independent indicator variables for E:
Xi =
 1
if E occurs on the i’th trial
0
otherwise.
Then E[Xi] = P(E) and so the strong law tells us that
P
 
lim
n→∞
1
n
n
X
i=1
Xi = P(E)
!
= 1,
i.e., the limiting frequency of E is equal to its probability.
6.6.2
The Central Limit Theorem
Although the strong law of large numbers tells us that the sample means of a sequence of i.i.d.
random variables converge almost surely to the expected value of the sampling distribution, it
does not tell us anything about the rate of convergence or about the size of the ﬂuctuations of the
sample means about this limit. This information is important because we might like to know, for
example, how likely it is that the sample mean of say 10,000 independent variates will diﬀer from
the true mean by more than 10% of the value of the latter. The Central Limit Theorem is one of
several results that gives us some insight into the ﬂuctuations of the sequence of sample means
under the additional assumption that the sampling distribution has ﬁnite variance. Before stating
this theorem, we begin by deﬁning a new mode of convergence for a sequence of random variables.

86
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
Deﬁnition 6.12. Suppose that F and Fn, n ≥1 are cumulative distribution functions on R.
Then Fn is said to converge weakly to F if limn→∞Fn(x) = F(x) at every continuity point x
of F. Likewise, a sequence of random variables Xn is said to converge in distribution to a
random variable X if the cumulative distribution functions Fn(x) = P(Xn ≤x) converge weakly
to F(x) = P(X ≤x).
Example 6.17. To see why we only require pointwise convergence at continuity points, let Xn
and X be deﬁned by setting P(Xn = 1/n) = 1 and P(X = 0) = 1. Then
Fn(x) =
 0
if x < 1/n
1
otherwise
and
F(x) =
 0
if x < 0
1
otherwise,
which shows that Fn(x) converges to F(x) as n →∞at every x ̸= 0. Since F(x) is continuous
everywhere except at x = 0, it follows that the sequence Xn converges in distribution to X even
though the sequence Fn(0) = 0 does not converge to F(0) = 1.
The next proposition shows how we can use moment generating functions to deduce that a
sequence of random variables converges in distribution. The assumption that the moment gen-
erating function of the limit X is ﬁnite is essential.
Proposition 6.9. Let X1, X2, · · · be a sequence of random variables with moment generating
functions ψXn(t), and let X be a random variable with moment generating function ψX. Then
the sequence Xn converges in distribution to X if
lim
n→∞ψXn(t) = ψX(t) < ∞
for all t ∈R.
In eﬀect, this proposition says two random variables Xn and X are close (in the sense that their
distributions are close) if their moment generating functions are close. This will be key to our
proof of the central limit theorem, but we ﬁrst need to determine the moment generating function
of a normally-distributed random variable.
Example 6.18. If Z is a standard normal RV (with mean 0 and variance 1), then
MZ(t)
=
Z ∞
−∞
etx
1
√
2πe−x2/2dx
=
et2/2
1
√
2π
Z ∞
−∞
e−(x−t)2/2dx
=
et2/2
=
∞
X
n=0
1
2nn!t2n.
Thus, the moments of Z are
M2n
=
(2n)!
2nn!
M2n+1
=
0

6.6. THE LAW OF LARGE NUMBERS AND THE CENTRAL LIMIT THEOREM
87
for all n ≥0. To calculate the moment generating function of a normal RV with mean µ and σ,
recall that Z ≡(X −µ)/σ) is a standard normal RV. Then, writing X = σZ + µ, we have
ψX(t)
=
E

etX
=
E
h
et(σZ+µ)i
=
etµψZ(σt)
=
exp
σ2t2
2
+ µt

.
Theorem 6.3. (The Central Limit Theorem) Suppose that X1, X2, · · · is a sequence of I.I.D.
variables with ﬁnite mean µ and variance σ2, and let Sn = X1 + · · · + Xn. Then the sequence of
random variables
Zn ≡Sn −nµ
σ√n
= 1
σn1/2
 1
nSn −µ

converges in distribution to the standard normal N(0, 1), i.e., for every real number z,
lim
n→∞P{Zn ≤z} = Φ(z) ≡
1
√
2π
Z z
−∞
e−x2/2dx.
Proof. We will prove the C.L.T. under the assumption that the moment generating function of
the random variables Xi, denoted ψ(t), is ﬁnite on the entire real line. Let us ﬁrst assume that
µ = 0 and σ2 = 1. Notice that the moment generating function of the scaled random variable
Xi/√n is
E

exp
tXi
√n

= ψ
 t
√n

and that the moment generating function of the sum Zn = Pn
i=1 Xi/√n is equal to

ψ
 t
√n
n
.
Let
L(t) = log ψ(t)
and observe that L(0) = L′(0) = 0 and L′′(0) = 1. By Proposition 6.9, it suﬃces to show that
lim
n→∞

ψ
 t
√n
n
= et2/2,
which is equivalent to
lim
n→∞nL
 t
√n

= t2
2 .
However, this last identity can be veriﬁed using L’Hôpital’s rule
lim
n→∞
L(t/√n)
n−1
=
lim
x→0
L(tx)
x2
=
lim
x→0
tL′(tx)
2x
=
lim
x→0
t2L′′(tx)
2
=
t2
2 .
The general case can then be handled by applying this result to the standardized variables
X∗
i = (Xi −µ)/σ, which have mean 0 and variance 1.

88
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
What is remarkable about the central limit theorem is that it shows that the distribution of
a sum of suﬃciently many i.i.d. random variables with ﬁnite variance is approximately normal
no matter what distribution the individual variables have. This may explain why the normal
distribution is encountered in so many seemingly unrelated phenomena, such as the distribution
of height in a population of adult males, the distribution of particle velocities in a gas, or the
distribution of errors in an experiment.
Remark 6.1. The restriction imposed by the use of the moment generating function in the proof
of the C.L.T. can be surmounted by instead working with characteristic functions:
χX(t) ≡E

eitX
.
Provided that X is real-valued, we have |eitX| = 1 and so the modulus of the characteristic
function is less than or equal to 1: |χX(t)| ≤1 for all t ∈(−∞, ∞). This means that the charac-
teristic function of any real-valued random variable is ﬁnite on the entire real line. Furthermore,
Proposition (6.9) still holds if we replace pointwise convergence of moment generating functions
by pointwise convergence of characteristic functions.
Example 6.19. Let X1, · · · , X10 be independent random variables, each uniformly distributed
on [0, 1], and let X = X1 + · · · + X10. The central limit theorem can be used to approximate the
distribution of X. For example,
P{X > 6}
=
P
(
X −5
p
10/12
>
6 −5
p
10/12
)
≈
1 −Φ(
√
1.2)
≈
0.137.
A version of the central limit theorem holds even when the random variables X1, X2, · · · are
independent but not necessarily identically-distributed.
Theorem 6.4. Let X1, X2, · · · be a sequence of independent random variables with µi = E[Xi]
and σ2
i = V ar(Xi).
If the Xi are uniformly bounded, i.e., there exists M < ∞such that
P{Xi < M} = 1 for all i ≥1, and if P
i≥1 σ2
i = ∞, then
lim
n→∞P



Pn
i=1(Xi −µi)
qPn
i=1 σ2
i
≤x


= Φ(x)
for each x ∈R.
This more general result can be used to prove the following surprising statement about the
number of prime divisors of a randomly chosen positive integer.
Theorem 6.5. (Erdös-Kac) For each n ≥1, let Xn be uniformly distributed on the set
{1, · · · , n}, and deﬁne φ(n) to be the number of prime divisors of the integer n, e.g., φ(2) = 1,
φ(3) = 1, φ(4) = 1, φ(5) = 1, φ(6) = 2, etc. Then, for every real number x,
lim
n→∞P
 
φ(Xn) −log log(n)
p
log log(n)
≤x
!
= Φ(x).
In other words, if an integer is chosen at random between 1 and n, then for large n the number
of prime divisors of that integer is approximately normally distributed with mean and variance
both equal to log log(n).

6.7. SAMPLE STATISTICS
89
6.7
Sample Statistics
6.7.1
The χ2 Distribution
Suppose that we perform a series of i.i.d. trials that result in measurements X1, · · · , Xn and we
let ¯X and S2 denote the sample mean and sample variance:
¯X
=
1
n
n
X
i=1
Xi
S2
=
1
n −1
n
X
i=1
 Xi −¯X
2 .
If we assume that each of the variables Xi is normally distributed with unknown mean µ and
unknown variance σ2, then ¯X is also normally distributed with mean µ and variance σ2/n. The
sample variance, however, is not normally distributed.
To identify its distribution, we ﬁrst recall that if Z is a standard normal random variable, then
the variable Z2 has the χ2-distribution with one degree of freedom, which we previously showed
to be the same as the gamma distribution with shape parameter α = 1/2 and rate parameter
λ = 1/2:
fZ2(x) = (1/2)1/2
Γ(1
2)
x−1/2e−x/2 =
1
√
2πx−1/2e−x/2,
x > 0.
The moment generating function for this distribution is
mZ2(t)
≡
E
h
etZ2i
=
Z ∞
0
etx
1
√
2πx−1/2e−x/2dx
=
1
√
2π
Z ∞
0
x−1/2e−(1/2−t)xdx
=
1
√
2π2
Z ∞
0
e−(1−2t)y2/2dy
=
1
√
2π
√
2π
√1 −2t
=
(1 −2t)−1/2,
provided t < 1/2.
Now suppose that Z1, · · · , Zn are independent standard normal random variables and let R =
Z2
1 + · · · + Z2
n be the sum of the squares. Our goal is to identify the distribution of R. Because
the Zi are independent, we know that the moment generating function of their sum is equal to
the product of the moment generating functions of the individual terms, i.e.,
mR(t)
=
E

etR
=
(1 −2t)−n/2 .
I claim that this is also the moment generating function of a Gamma distribution with shape

90
CHAPTER 6. RANDOM VECTORS AND MULTIVARIATE DISTRIBUTIONS
parameter α = n/2 and rate parameter λ = 1/2. Indeed, if Y is such a random variable, then
mY (t)
=
E

etY 
=
Z ∞
0
ety (1/2)n/2
Γ(n/2) yn/2−1e−y/2dy
=
1
2
n/2
1
Γ(n/2)
Z ∞
0
yn/2−1e−(1/2−t)ydy
=
(1/2 −t)−n/2
1
2
n/2
1
Γ(n/2)
Z ∞
0
xn/2−1e−xdx
=
(1 −2t)−n/2
1
Γ(n/2)Γ(n/2)
=
(1 −2t)−n/2,
as claimed. This shows that Y and R have the same distribution. In practice, R is said to have
the χ2 distribution with n degrees of freedom, which is written R ∼χ2
n. The relevance of this
distribution to the sample variance is explained in the following proposition.
Proposition 6.10. If the trial outcomes X1, · · · , Xn are i.i.d. normal random variables, then
the normalized sample variance (n −1)S2/σ2 is χ2-distributed with n −1-degrees of freedom.
Proof. In Example (6.16), we showed that the sample variance can be rewritten in the following
form:
S2 =
1
n −1
n
X
i=1
(Xi −µ)2 −
n
n −1( ¯X −µ)2.
If we then rearrange these terms and divide each one by σ2, we arrive at the following expression:
(n −1)S2
σ2
+ ( ¯X −µ)2
σ2/n
=
n
X
i=1
(Xi −µ)2
σ2
.
Now we know that the right-hand side is χ2-distributed with n degrees of freedom, while the
second term on the left-hand side is also χ2-distributed but with only one degree of freedom.
By using moment generating functions, for example, we can show that this implies that the ﬁrst
term on the left-hand is χ2-distributed with n −1 degrees of freedom.
Example 6.20. Suppose that 16 individuals are sampled at random from a population in which
height is normally distributed with unknown mean µ and variance σ2. We know that the sample
variance S2 is an unbiased estimator of σ2, but we might also be interested in calculating the
probability that S2 is unusually small relative to σ2, e.g, we would like to calculate P(S2 ≤0.25σ2).
Since 15S2/σ2 ∼χ2
15, this can be done by evaluating the following probability,
P

S2 ≤1
4σ2

=
P
15S2
σ2
≤15
4

=
P
 χ2
15 ≤3.75

≈
0.0016,
which was done with the help of the Matlab function chi2cdf(x, df).
6.7.2
Student’s t Distribution
When the measurements X1, · · · , Xn are normally distributed, we know that the Z-score
Z =
¯X −µ
σn1/2

6.7. SAMPLE STATISTICS
91
is also normally-distributed. However, this observation is only useful in practice if we know the
actual value of the variance σ2. If not, then we can replace σ in this expression by the square
root S of the sample variance, but then the distribution of the statistic
T =
¯X −µ
Sn1/2
is no longer normal. Instead, T is said to have Student’s t-distribution with n −1 degrees of
freedom. This distribution is characterized in the next proposition.
Proposition 6.11. Let Z be a standard normal random variable and let V be an independent
χ2- distributed random variable with n degrees of freedom. Then the test statistic
T =
Z
p
V/n
has Student’s t distribution with n degrees of freedom and the density of T on (−∞, ∞) is
fT (t) =
Γ(n+1
2 )
√nπ Γ
  n
2


1 +
t2
n
−(n+1)/2
.
Proof. The density of T can be found by marginalizing over the joint density of T and V ,
pT (t) =
Z ∞
0
pT,V (t, v)dv,
and the joint density can be found with the help of the product rule:
pT,V (t, v) = pT|V (t|v)pV (v).
The ﬁrst term on the right-hand side can be found by noting that conditional on V = v, T is
normally distributed with mean 0 and variance n/v, i.e.,
pT|V (t|v) =
√v
√
2πne−vt2/2n.
Also, since V is χ2-distributed with n degrees of freedom, it follows that the density of V is that
of the gamma distribution with shape parameter n/2 and rate parameter 1/2:
pV (v) =
  1
2
n/2
Γ(n/2)vn/2−1e−v/2.
Multiplying these gives
pT,V (t, v) =
  1
2
n/2
√
2πnΓ(n/2)v
n+1
2 −1e−
“
1
2 + t2
2n
”
v
and marginalizing over v then gives
pT (t) =
Γ(n+1
2 )
√nπ Γ
  n
2


1 +
t2
n
−(n+1)/2
.

Chapter 7
Entropy
7.1
Motivation
Suppose that we toss a coin which has probability p of turning up heads and probability 1 −p
of turning up tails. Before the coin has landed, the outcome is uncertain and thus we gain some
information upon observing which of the two possible outcomes occurs. Notice, however, that
the amount of information gained depends on the value of p. For example, if we know that p = 1,
then we can be certain that the coin will land on heads and thus no information is gained upon
observing this outcome. Similarly, if p = 0, then the coin is certain to land on heads and so no
information is gained in this case either. On the other hand, if p = 0.5, then both outcomes
are equally likely and so in this case we do gain information upon learning which outcome has
actually occurred. If, instead, p = 0.95, then the coin is likely, but not certain to land on heads
and so at the conclusion of the experiment we will have gained some information, but arguably
less than in the preceding case.
This example illustrates the fact that the information gained in performing an experiment with
a random outcome depends on the distribution of the outcomes. Essentially, the more uncertain
the outcome, the greater the information gained upon learning which outcome did occur. The
need to quantify information in this sense led Claude Shannon to introduce the following deﬁni-
tion of the entropy of a discrete random variable.
Deﬁnition 7.1. Suppose that X is a discrete random variable with values in the set S =
{s1, s2, · · · } and let pi = P(X = si). Then the entropy of X is the quantity
H(X) ≡−
X
i
pi log2(pi),
(7.1)
which is measured in units of bits.
Notice that the entropy of the random variable X only depends on the distribution of X. For this
reason, we can deﬁne the entropy of the distribution itself to be the same as the entropy of any
random variable with this particular distribution. Although we have deﬁned the entropy using
base 2 logarithms, other bases are also used, in which case we write Hb(X), where b indicates
the base of the logarithm. For example, it is common to express the entropy in terms of natural
logarithms, i.e,
He(X) = −
X
i
pi ln(pi),
in which case the units are called nats. Furthermore, because logb(x) = logb(2) log2(x), changing
92

7.2. THE MAXIMUM ENTROPY PRINCIPLE
93
the base b only changes the units in which the entropy is expressed:
Hb(X) = logb(2)H2(X).
The form that the entropy takes in Deﬁnition 7.1 can be justiﬁed in several ways. One approach
is to proceed by stipulating several properties that we would want any measure of information
to satisfy and then derive a function that has these properties. This is the content, for example,
of the next theorem, which is also due to Shannon.
Theorem 7.1. Suppose that for each n ≥2, H(n)(p1, · · · , pn) is a real-valued function on prob-
ability distributions p1, · · · , pn which satisﬁes the following properties:
(i) Normalization: H(2)(1/2, 1/2) = 1;
(ii) Continuity: H(n) is a continuous function;
(iii) Symmetry: H(n) is a symmetric function of its arguments, e.g., H(n)(p1, p2, · · · , pn) =
H(n)(p2, p1, · · · , pn);
(iv) Additivity/Grouping:
H(n)(p1, p2, · · · , pn) = H(n−1)(p1 + p2, · · · , pn) + (p1 + p2) · H(2)

p1
p1 + p2
,
p2
p1 + p2

.
Then, for each n ≥2,
H(n)(p1, · · · , pn) = −
n
X
i=1
pi log2(pi).
The additivity/grouping axiom has the following interpretation. Suppose that we perform an ex-
periment with n possible outcomes. However, rather than being immediately told which outcome
occurs, we are ﬁrst told which of the following subsets contains the outcome: {s1, s2}, {s3}, · · · , {sn}.
Then, if the outcome belongs to the subset {s1, s2}, we are told which of these two possibilities
has occurred. Since we gain exactly the same information that we would have gained had we
just been told the outcome directly, the entropy of the distribution should be equal to the sum
of the entropies gained when we are given this information in stages.
We can also deﬁne the entropy of a continuous random variable by replacing the sum in Deﬁnition
7.1 by an integral and the probability mass function by a probability density function.
Deﬁnition 7.2. Suppose that X is a continuous random variable with density p(x) on Rn. Then
the entropy of X is the quantity
H(X) ≡−
Z
p(x) log2(p(x))dx.
As in the discrete case, this formula also serves as the deﬁnition of the entropy of the distribution
with density p(x).
7.2
The Maximum Entropy Principle
Arguably, one of the thorniest aspects of Bayesian inference is the selection of the prior distri-
bution. On the one hand, the prior distribution should reﬂect all of the information available
to us before we have seen the results of the new experiment. This information could take a
variety of forms, including constraints on the values of model parameters or estimates of certain

94
CHAPTER 7. ENTROPY
moments, e.g., the mean and the variance, of these parameters. At the same time, we want to
avoid selecting a prior distribution that is more informative than our prior knowledge allows. For
example, the prior distribution should certainly give positive probability to all possible values of
the model parameters, no matter how unlikely, since otherwise we are excluding possible values
without justiﬁcation. These considerations are incorporated into the following heuristic:
Guideline: Out of all the possible probability distributions which satisfy the given constraints,
select the distribution that is maximally ignorant or non-committal with regard to missing in-
formation.
To apply this heuristic, we need to know what it means to be “maximally ignorant with regard to
missing information” and this is where entropy comes in to the picture. The maximum entropy
principle was introduced by Edward Jaynes in 1957 as an information theoretic rationale for
statistical mechanics, but it also provides us with a concrete formulation of the above guideline.
Maximum Entropy Principle: Out of all the possible probability distributions which satisfy
the given constraints, select the distribution with the maximum entropy.
Although the MaxEnt principle is usually justiﬁed by appealing to information theory, the follow-
ing thought experiment, due to Graham Wallis, provides an alternative derivation. Suppose that
we wish to elicit a prior distribution on a set S = {s1, · · · , sM} that satisﬁes certain constraints
C1, C2, · · · that come from prior knowledge or beliefs. Let PC be the collection of probabil-
ity distributions (p1, · · · , pM) on S that satisfy these constraints. Typically, PC will contain
many distributions and so we must formulate a procedure to select just one of these as a prior
distribution.
Imagine performing a large number of independent trials with outcomes in S and let ni denote
the number of trials that result in outcome si and set N = n1 + · · · + nM. Initially, we have no
reason to favor any particular outcome over any of the others and so we will assume that each
outcome has probability 1/M of occurring. In fact, if we had no prior information available to
us apart from the speciﬁcation of the possible outcomes, then we could simply stop here and
take the uniform distribution on S as our prior distribution, i.e., set pi = 1/M. Likewise, if the
count data n1, · · · , nM was real rather than hypothetical, then we could use it to estimate the
probabilities pi by setting pi = ni/N and appealing to the law of large numbers.
Since we do not have the count data, but we do know that the prior distribution must come
from PC, we will assume that ni = Npi and then choose the distribution (p1, · · · , pM) ∈PC
that maximizes the likelihood of the unknown counts n1, · · · , nM. Under the assumptions of the
thought experiment, the counts (n1, · · · , nM) are multinomially distributed, with probability
p(n1, · · · , nM) =

N
n1, · · · , nM

M−N.
Rather than maximizing this expression directly, it is easier to work with the log-likelihood,
which is
L(n1, · · · , nM) = ln(N!) −
M
X
i=1
ln(ni!) −N ln(M).
Provided that N is large enough that each of the counts ni = Npi is large, we can use Stirling’s
approximation,
n! ≈
√
2πn
n
e
n
,
to replace each of the factorials by a more tractable expression. Stirling’s approximation describes
the asymptotic behavior of the factorial function and holds in the sense that
lim
n→∞
√
2πn
n
e
n 
n! = 1.

7.3. MAXIMUM ENTROPY DISTRIBUTIONS
95
This gives
L(n1, · · · , nM)
≈
ln(
√
2πN) + N ln(N) −N −
M
X
i=1
ln(
p
2πNpi)
−N
M
X
i=1
pi ln(Npi) + N
N
X
i=1
pi −N ln(M)
=
ln(
√
2πN) −
M
X
i=1
ln(
p
2πNpi) −N
M
X
i=1
pi ln(pi) −N ln(M).
Dividing both sides by N and taking the limit as N →∞gives
l(p1, · · · , pN) ≡H(p1, · · · , pM) −ln(M).
Since the second term on the right-hand side does not depend on the pi’s, it follows that maximiz-
ing the log-likelihood of the count data with respect to the probabilities p1, · · · , pM is equivalent
to maximizing the entropy of this distribution. However, we also need to take the prior constraints
into account and so Wallis’ argument suggests that we should choose the prior probabilities by
solving the constrained optimization problem:
(p1, · · · , pM) = arg max {H(p1, · · · , pM) : (p1, · · · , pM) ∈PC} .
(7.2)
This is precisely what the maximum entropy principle stipulates.
7.3
Maximum Entropy Distributions
A distribution P = (p1, p2, · · · ) on a countable set S = {s1, s2, · · · } is said to be a maximum
entropy distribution if there are ﬁnitely many (possibly zero) functions f1, · · · , fr : S →R
and constants ¯f1, · · · , ¯fr such that P is the solution to the constrained optimization problem:
P = arg max {H(P) : pi ≥0 for all i ≥1}
subject to the constraints:
X
i
pi
=
1
X
i
pif1(si)
=
¯f1
...
X
i
pifr(si)
=
¯fr.
Such problems can be solved with the help of Lagrange multipliers. In this case, we introduce
unknown constants λ, λ1, · · · , λr and we construct the Lagrange function
Λ(P, λ, λ1, · · · , λr)
≡
H(P) −λ
 X
i
pi −1
!
−
λ1
 X
i
pif1(si) −¯f1
!
−· · · −λr
 X
i
pifr(si) −¯fr
!
.

96
CHAPTER 7. ENTROPY
The maximum (assuming that it exists) can then be found by solving the system of equations
∂p1Λ(P, λ, λ1, · · · , λr)
=
0
∂p2Λ(P, λ, λ1, · · · , λr)
=
0
...
∂λΛ(P, λ, λ1, · · · , λr)
=
0
∂λ1Λ(P, λ, λ1, · · · , λr)
=
0
...
∂λrΛ(P, λ, λ1, · · · , λr)
=
0
both for the distribution P and for the constants λ, λ1, · · · , λr. These equations can be rewritten
in the following form:
−ln(pi) −1 −λ −
r
X
k=1
λkfk(si) = 0
for i ≥1 and
X
i
pi
=
1
X
i
pifk(si)
=
¯fk,
k = 1, · · · , r.
Notice that the ﬁrst set of equations allows us to express each of the probabilities pi as a function
of the Lagrange multipliers:
pi = e−1−λ exp
"
−
r
X
k=1
λkfk(si)
#
.
The Lagrange multipliers can then be found by substituting these expressions into the constraints:
1
=
X
i
pi = e−1−λ X
i
exp
"
−
r
X
k=1
λkfk(si)
#
¯fk
=
X
i
pifk(yi) = e−1−λ X
i
exp
"
−
r
X
k=1
λkfk(si)
#
fk(si).
In general, these equations will need to be solved numerically (e.g., with the help of Newton’s
method), but in some cases they can be solved analytically.
Maximum entropy distributions can also be deﬁned on intervals with the help of Deﬁnition 7.2.
Speciﬁcally, a continuous distribution with density p(x) on the interval S = (a, b) is said to be
a maximum entropy distribution if there are ﬁnitely many (possibly zero) functions f1, · · · , fr :
S →R and constants ¯f1, · · · , ¯fr such that p(x) is the solution to the constrained optimization
problem:
p(·) = arg max {H(p(·)) : p(x) ≥0 for all x ∈S}
H(p(·)) = −
Z
S
p(x) ln(p(x))dx

7.3. MAXIMUM ENTROPY DISTRIBUTIONS
97
subject to the constraints:
Z
S
p(x)dx
=
1
Z
S
p(x)f1(x)dx
=
¯f1
...
Z
S
p(x)fr(x)dx
=
¯fr.
In this case we are trying to maximize the entropy over the space of continuous probability
distributions on S. Since this space is inﬁnite-dimensional, new techniques are required and
these are provided by the calculus of variations. Without going into these methods in detail,
we begin by introducing the Euler-Lagrange functional for the problem:
Λ(p(·), λ, λ1, · · · , λr)
≡
H(p(·)) −λ
Z
S
p(x)dx −1

−
λ1
Z
S
p(x)f1(x)dx −¯f1

−· · · −λr
Z
S
p(x)fr(x)dx −¯fr

.
It can then be shown that if p(·) is a maximum of the constrained optimization problem, then
p(x) = e−1−λ exp
"
−
r
X
k=1
λkfk(x)
#
,
where the values of λ, λ1, · · · , λk are chosen to satisfy the given constraints. As in the discrete
case, these will usually need to be found numerically.
7.3.1
Uniform distribution
Suppose that we wish to ﬁnd the distribution on a ﬁnite set S = {s1, · · · , sM} with maximum
entropy, i.e., subject to no additional constraints beyond the requirement that p1 +· · ·+pM = 1.
The Lagrange function in this case is
Λ(p1, · · · , pM, λ) = H(p1, · · · , pM) −λ
 M
X
i=1
pi −1
!
and the method of Lagrange multipliers leads to the identities
0 = ∂piλ(p1, · · · , pM, λ) = −ln(pi) −1 −λ
for i = 1, · · · , M. This shows that
p1 = · · · = pM = e−1−λ,
while the constraint
1 = p1 + · · · + pM = Me−1−λ
shows that λ = ln(M) −1 and
p1 = · · · = pM = 1.
In other words, the maximum entropy distribution on a ﬁnite set is just the uniform distribution
on that set and the entropy of that distribution is ln(M).

98
CHAPTER 7. ENTROPY
If S = (a, b) is a ﬁnite interval, then the density p(x) of the maximum entropy distribution on S
will satisfy the equation
−ln(p(x)) −1 −λ = 0
for all x ∈(a, b), which shows that
p(x) = e−1−λ.
Since the density must integrate to 1 over S, it follows that λ = ln(b −a) −1 and so
p(x) =
1
b −a
if x ∈(a, b).
Thus the maximum entropy distribution on S is again just the uniform distribution on S, which
has entropy ln(b −a). This provides an alternative justiﬁcation for the selection of the uniform
distribution as an uninformative prior distribution for a parameter constrained to take
values in a bounded set S in the absence of any additional prior knowledge about the parameter.
7.3.2
Geometric and exponential distributions
Now suppose that we wish to ﬁnd the distribution on the set of natural numbers S = {0, 1, 2, · · · }
with maximum entropy subject to the constraint that the mean of the distribution is µ > 0. In
this case the method of Lagrange multipliers leads to the identities
−ln(pn) −1 −λ −λ1n = 0
for all n ≥0 and so
pn = e−1−λe−λ1n.
There are two constraints in this case:
1
=
e−1−λ
∞
X
n=0
e−λ1n = e−1−λ
1
1 −e−λ1
µ
=
e−1−λ
∞
X
n=0
ne−λ1ne−1−λ
e−λ1
(1 −e−λ1)2 .
The ﬁrst constraint shows that e−1−λ = 1 −e−λ1, while the second constraint gives
e−λ1 =
µ
1 + µ.
Taken together, these identities show that
pn =
1
1 + µ

µ
1 + µ
n
and so the maximum entropy distribution in this case is the geometric distribution with success
probability (1 + µ)−1.
Similarly, if we wish to ﬁnd the maximum entropy distribution on S = [0, ∞) subject to the
condition that the distribution has mean µ > 0, then the density of this distribution will satisfy
the identity
p(x) = e−1−λe−λ1x,
with
1
=
Z ∞
0
p(x)dx
µ
=
Z ∞
0
xp(x)dx.

7.3. MAXIMUM ENTROPY DISTRIBUTIONS
99
This shows that the maximum entropy distribution is the exponential distribution with rate
parameter µ−1:
p(x) = 1
µe−x/µ, x ≥0,
which has entropy
H(p(x)) = 1 + ln(µ).
7.3.3
Gaussian distributions
In Section (7.3.1), we saw that the maximum entropy distribution on a bounded interval (a, b) is
just the uniform distribution. Since there is no uniform distribution on the real line, it is clear
that this result does not extend to the case where (a, b) = (−∞, ∞) and, in fact, it can be shown
that there is no maximum entropy distribution in that case, since we can construct a sequence of
distributions with densities p(n)(x) such the entropies of these distributions tend to inﬁnite, e.g.,
take p(n) to be the density of the uniform distribution on [−n, n]. Furthermore, this example
also shows that there is no maximum entropy distribution on R1 with mean equal to µ. Instead,
we must specify both the mean µ and the variance σ2 < ∞if we want to identify a maximum
entropy distribution on R1. If we do so, then the density p(x) will have the form
p(x) = e−1−λe−λ1x−λ2x2
subject to the constraints
1
=
Z ∞
−∞
p(x)dx
µ
=
Z ∞
−∞
xp(x)dx
σ2
=
Z ∞
−∞
(x −µ)2p(x)dx.
This shows that the maximum entropy distribution on R1 with mean µ and variance σ2 < ∞is
the normal distribution with this mean and variance, i.e.,
p(x) =
1
σ
√
2πe−(x−µ)2/2σ2,
which provides an alternative justiﬁcation (beyond the Central Limit Theorem) for the widespread
use of Gaussian distributions in statistical models. A simple computation shows that the entropy
of this distribution is
H(p(x)) = 1
2
 1 + ln(2πσ2)

,
which is an increasing function of the variance.

Chapter 8
Bayesian Inference for Gaussian Distributions
8.1
Estimation of a Mean
Suppose that we perform a series of N independent experiments resulting in measurements
d1, · · · , dN and assume that the i’th measurement can be expressed as
di = µ + ei,
(8.1)
where µ is a ﬁxed parameter that we wish to estimate and e1, · · · , eN are independent Gaussian
random variables with mean 0 and variance σ2 that account for measurement error and any other
sources of uncertainty. In this case, µ is the expected value of each measurement and our goal is
to calculate the posterior distribution of µ given the experimental data D. As usual, we begin
by writing down Bayes’ formula
p(µ|D, I) = p(µ|I)p(D|µ, I)
p(D|I) ,
(8.2)
where p(µ|I) denotes the prior distribution for µ, p(D|µ, I) is the likelihood function, and I
designates any prior information about µ that is available to us before we examine the data.
Here we will assume that I speciﬁes an upper and lower bound for µ, say µL ≤µ ≤µH, but
favors no particular value within that interval. Appealing to the maximum entropy principle, we
will select the uniform distribution on the interval [µL, µH] as our prior distribution for µ,
p(µ|I) =
(
R−1
if µL ≤µ ≤µR
0
otherwise,
where R = µH −µL.
Because the noise terms are independent, the likelihood is
p(D|µ, I)
=
P(d1, · · · , dN|µ, I)
=
N
Y
i=1
1
σ
√
2πe−(di−µ)2/2σ2
=
σ−N(2π)−N/2 exp
(
−1
2σ2
N
X
i=1
(di −µ)2
)
.
Recalling that
N
X
i=1
(di −µ)2 = N
 µ −¯d
2 + Nr2,
100

8.1. ESTIMATION OF A MEAN
101
where
¯d
=
1
N
N
X
i=1
di
r2
=
1
N
N
X
i=1
 di −¯d
2
are the sample mean and the mean squared deviation from ¯d, respectively, the likelihood can be
rewritten as
p(D|µ, I) = σ−N(2π)−N/2e−Nr2/2σ2e−(µ−¯d)
2/(2σ2/N).
Then, since the prior predictive probability of the data is
p(D|I) =
Z µH
µL
R−1p(D|u, I)du,
it follows that the posterior density of µ ∈[µL, µH] is
p(µ|D, I)
=
R−1σ−N(2π)−N/2e−Nr2/2σ2 exp

−(µ−¯d)
2
(2σ2/N)

R−1σ−N(2π)−N/2e−Nr2/2σ2 R µH
µL exp

−(u−¯d)
2
(2σ2/N)

du
=
C−1 exp
(
−
 µ −¯d
2
(2σ2/N)
)
,
where
C ≡
Z µH
µL
exp
(
−
 u −¯d
2
(2σ2/N)
)
du
is a normalizing constant. This shows that the posterior distribution of µ is a normal distribution
with mean ¯d and variance σ2/N conditioned to take values in [µL, µH].
Provided that ¯d ∈
(µL, µH) and σ2/N ≪1, this distribution will be almost identical to the unconditioned normal
distribution.
The normalizing constant C can be expressed in terms of the error function, deﬁned as
erf(x) ≡
2
√π
Z x
0
e−t2dt.
Indeed, a simple change of variable gives
C =
r π
2N σ (erf(uH) −erf(uL)) ,
where
uH =
µH −¯d
(σ/
√
2N)
and uL =
µL −¯d
(σ/
√
2N)
.
The error function must be evaluated numerically. In Matlab, this is done with the command
erf(x).

102
CHAPTER 8. BAYESIAN INFERENCE FOR GAUSSIAN DISTRIBUTIONS
8.1.1
Unequal variances
Suppose that the data can be expressed as in equation (8.1), but that each noise term has its
own variance, which is known to us: ei ∼N(0, σ2
i ). In this case the likelihood function is equal
to
p(D|µ, I)
=
P(d1, · · · , dN|µ, I)
=
N
Y
i=1
1
σi
√
2πe−(di−µ)2/2σ2
i
=
" N
Y
i=1
1
σi
#
(2π)−N/2 exp
(
−1
2
N
X
i=1
wi(di −µ)2
)
,
where we interpret wi = σ−2
i
as the ‘weight’ of the i’th data point. To explain this terminology,
suppose that we wished to come up with a point estimate of the mean from the data.
In
general, data points with smaller variances will be closer on average to the mean and so should
be weighted more strongly.
In fact, under the (continuing) assumption that the noises are
independent normal random variables, one can show that the maximum likelihood estimate of µ
is given by the weighted empirical mean with the weights wi introduced above:
¯dw ≡
PN
i=1 widi
PN
i=1 wi
.
If we also deﬁne
r2
w =
1
PN
i=1 wi
( N
X
i=1
wi
 di −¯dw
2
)
to be the weighted mean squared deviation from ¯dw and
σ2
h =
 
1
N
N
X
i=1
wi
!−1
to be the harmonic mean of the variances, then it can be shown that
N
X
i=1
wi(di −µ)2 = N
σ2
h
h µ −¯dw
2 + r2
w
i
.
With this, we can rewrite the likelihood function as
p(D|µ, I) =
" N
Y
i=1
1
σi
#
(2π)−N/2e−Nr2
w/2σ2
he−(µ−¯dw)2/(2σ2
w/N),
which then shows that the posterior distribution of µ ∈[µL, µH] has density
p(µ|D, I) = C−1 exp
(
−
 µ −¯dw
2
(2σ2w/N)
)
,
where
C ≡
Z µH
µL
exp
(
−
 u −¯dw
2
(2σ2w/N)
)
du.
Of course, if all of the variances are identical, say σ2
1 = · · · = σ2
N = σ2, then ¯dw = ¯d and σ2
w = σ2,
in which case this posterior density reduces to the one derived in the previous section.

8.1. ESTIMATION OF A MEAN
103
8.1.2
Unknown mean and variance
Now let us assume that all of the measurements have the same variance, σ2, but that this quantity
is unknown to us. Because our primary objective is still to estimate the mean µ, the quantity
σ is said to be a nuisance parameter, since in order to ﬁnd the posterior distribution of µ we
must ﬁrst calculate the joint posterior distribution of µ and σ,
p(µ, σ|D, I) = p(µ, σ|I)p(D|µ, σ, I)
p(D|I)
,
and then marginalize over σ
p(µ|D, I) =
Z ∞
0
p(µ, σ|D, I)dσ.
In the absence of any information suggesting otherwise, we will assume that µ and σ are inde-
pendent under the prior distribution,
p(µ, σ|I) = p(µ|I) · p(σ|I)
and we will continue to assume that the prior distribution of µ is uniform on the interval [µL, µH].
We will also assume that the possible values of σ are bounded below by σL > 0 and above by
σH < ∞, and since σ is a scale parameter, we will choose the Jeﬀreys prior
p(σ|I) = K
σ
restricted to the interval [σL, σH], where the normalization constant K is equal to
K =
Z σH
σL
dσ
σ
−1
=
1
ln(σH/σL).
When µ ∈[µL, µH] and σ ∈[σL, σH], the likelihood function can be written as
p(D|µ, σ, I) = σ−N(2π)−N/2e−Q(µ)/2σ2,
where we deﬁne
Q(µ) = N(µ −¯d)2 + Nr2.
Substituting this expression into formula for the density of the posterior distribution of µ gives
p(µ|D, I) = C−1
Z σH
σL
σ−(N+1)e−Q(µ)/2σ2dσ,
where the normalization constant C is given by a double integral
C =
Z µH
µL
Z σH
σL
σ−(N+1)e−Q(u)/2σ2dσdu.
The integrals in the posterior density can all be evaluated numerically, but we can also obtain
a useful approximation that will exhibit the relationship between the Bayesian and frequentist
approaches to the problem of estimating the mean when the variance is not known. To this end,
let t = Q/2σ2, so that
σ =
r
Q
2t
and
dσ = −1
2t−3/2
r
Q
2 dt
and
tL =
Q
2σ2
H
and
tH =
Q
2σ2
L
.

104
CHAPTER 8. BAYESIAN INFERENCE FOR GAUSSIAN DISTRIBUTIONS
With this change of variables, the posterior density can be rewritten as
p(µ|D, I)
=
Q(µ)−N/2 R tH
tL tN/2−1e−tdt
R µH
µL Q(u)−N/2
R tH
tL tN/2−1e−tdt

du
≈
Q(µ)−N/2
R µH
µL Q(u)−N/2du,
where the approximation is accurate whenever σL ≪r ≪σH, i.e., the prior constraints on the
unknown variance must encompass the maximum likelihood estimate of the variance, which is
r. If we substitute the explicit formula for Q back into this approximation, then (after some
simpliﬁcation) we arrive at the alternative expression
p(µ|D, I) ≈C−1

1 + (µ −¯d)2
r2

,
with
C =
Z µH
µL

1 + (u −¯d)2
r2

du.
Furthermore, if µL/r ≪0 ≪µH/r, then
p(µ|D, I) ≈
Γ(N/2)
p
(N −1)πΓ((N −1)/2)

1 + (N −1)(µ −¯d)2/r2
N −1
−N/2
which shows that the posterior distribution of the normalized quantity
t ≡
µ −¯d
r2/(N −1)
is approximately Student’s t distribution with N −1 degrees of freedom. As we saw in Section
6.7.2, this distribution also arises in frequentist analyses of i.i.d. normal samples when the mean
and the variance are unknown and must be estimated from the data.
8.2
Interlude: Fisher Information and Jeﬀreys Priors
In the last section we used the Jeﬀreys prior p(σ) ∼σ−1 for the prior distribution of the standard
deviation of a sample of i.i.d. normal random variates with unknown mean and unknown variance.
In fact, Jeﬀreys priors can be deﬁned for a much larger class of parameter estimation problems
but to do so we will need to introduce the concept of the Fisher information of a parametric family
of distribution. We say that a family of probability distributions on a set E is a parametric
family if there is a set Θ ⊂Rn such that each distribution in the family is indexed by a single
point θ ∈Θ. In this case it is customary to write the probability mass function or the probability
density function in the form p(x; θ), where x ∈E and θ ∈Θ, to explicitly indicate the dependence
of the distribution on the parameter θ.
Deﬁnition 8.1. Suppose that p(x; θ) is the probability mass function or density function of a
parametric family of distributions with one-dimensional parameter θ ∈Θ ⊂R. Then the Fisher
information is the function I : Θ →[0, ∞) deﬁned by
I(θ) ≡Eθ
" ∂
∂θ ln
 p(X; θ)
2#
where Eθ[φ(X)] is the expected value of the function φ with respect to the distribution p(x; θ).

8.2. INTERLUDE: FISHER INFORMATION AND JEFFREYS PRIORS
105
Fisher information is named after the statistician and geneticist R. A. Fisher, who introduced
it as a way to quantify the average amount of information carried by a random variable about
a parameter on which its distribution depends. To understand this interpretation, let us deﬁne
the score function of the distribution to be the partial derivative of the log-likelihood function
with respect to the parameter θ:
S(x; θ) ≡∂
∂θ ln
 p(x; θ)

.
The score function shows how rapidly the likelihood function evaluated at a speciﬁc data point
x and a speciﬁc parameter value θ changes as a function of θ, e.g., when S(x; θ) is positive, the
likelihood at x will increase as θ increases. Notice that if ˆθ is a maximum likelihood estimate of
θ at x, then S(x; ˆθ) = 0. Under fairly mild conditions on the parametric family p(x; θ) it can be
shown that the expected value of the score function with respect to the distribution p(x; θ) is 0.
Indeed, since the identity
1 =
Z
E
p(x; θ)dx
holds for every value of θ ∈Θ, it follows that
0
=
∂
∂θ
Z
E
p(x; θ)dx
=
Z
E
∂
∂θp(x; θ)dx
=
Z
E

1
p(x; θ) · ∂
∂θp(x; θ)

p(x; θ)dx
=
Z
E
 ∂
∂θ ln
 p(x; θ)

p(x; θ)dx
=
Eθ [S(X; θ)] ,
provided that we can interchange diﬀerentiation and integration in passing from the ﬁrst to the
second line. (This is where suﬃcient regularity of p(x; θ) is required.) This shows that the Fisher
information is equal to the variance of the score function with respect to the distribution p(x; θ):
I(θ) = Varθ(S(X; θ)) = Eθ

S(X; θ)2
.
Since the score function takes on large absolute values precisely when the likelihood of the
observed data can be increased by adjusting the parameter value, the Fisher information will be
large when, on average, the likelihood of the data strongly depends on θ.
The Fisher information also has an interesting geometric interpretation. By the chain rule, the
partial derivative of the score function with respect to θ can be written as
∂2
∂θ2 ln
 p(x; θ)

= ∂
∂θ
∂θp(x; θ)
p(x; θ)

= ∂2
θp(x; θ)
p(x; θ)
−(∂θp(x; θ))2
p(x; θ)2
.
Then, since the identity
0 = ∂2
∂θ2
Z
E
p(x; θ)dx =
Z
E
∂2
∂θ2 p(x; θ)dx
holds under suitable regularity conditions, it follows that
Eθ
 ∂2
∂θ2 ln
 p(X; θ)

=
Eθ
"
∂2
θp(X; θ)
p(X; θ)
−(∂θp(X; θ))2
p(X; θ)2
#
=
Z
E
∂2
∂2
θ
p(x; θ)dx −
Z
E
 ∂
∂θ ln
 p(x; θ)
2
p(x; θ)dx
=
−I(θ),

106
CHAPTER 8. BAYESIAN INFERENCE FOR GAUSSIAN DISTRIBUTIONS
which shows that an alternative expression for the Fisher information is
I(θ) = −Eθ
 ∂2
∂θ2 ln
 p(X; θ)

.
Thus the Fisher information can also be interpreted as the expected curvature of the log-likelihood
function. In general, the greater the curvature of the likelihood function, the more information
the data will provide about the value of the unknown parameter.
Example: For ﬁxed n ≥1, the set of binomial distributions with parameter φ ∈Θ = [0, 1] is a
one-dimensional parametric family of distributions with probability mass function
p(k; φ) =
n
k

φk(1 −φ)n−k.
Since
ln
 p(k; φ)

= ln
n
k

+ k ln(φ) + (n −k) ln(1 −φ)
and
∂
∂φ ln
 p(k; φ)

= k
φ −n −k
1 −φ = k −nφ
φ(1 −φ)
it follows that the Fisher information for the parameter φ is
I(φ)
=
Eφ
" X −nφ
φ(1 −φ)
2#
=
1
(φ(1 −φ))2 Var(X)
=
n
φ(1 −φ).
Notice that I(φ) is an increasing function of n, which is consistent with our intuition that larger
sample sizes are proportionately more informative about the value of φ.
Although the Fisher information was originally developed as a frequentist concept, it has also seen
application in Bayesian statistics through its connection with Jeﬀreys priors. Speciﬁcally, Jeﬀreys
priors, which are named after the statistician Harold Jeﬀreys, are a class of non-informative prior
distributions on parameter spaces which satisfy Jeﬀreys’ invariance principle. This principle
states that any rule that is used to select a non-informative prior distribution on a parameter θ
should be consistent under a change of variables. In other words, if the parameter θ is replaced by
an alternative parameter φ = h(θ), where h is a one-to-one mapping, then the non-informative
prior distribution selected for φ should be the same as that obtained by change-of-variables
applied to the non-informative prior distribution selected for θ:
p(φ) = p(θ)

dθ
dφ
 .
It can be shown that this condition is satisﬁed if the prior distribution of each parameter is
chosen to be proportional to the square root of the Fisher information of the parameter,
p(θ) ∝
p
I(θ),

8.2. INTERLUDE: FISHER INFORMATION AND JEFFREYS PRIORS
107
where the distribution is normalized whenever possible. Indeed, by the chain rule, we have
p(φ)
=
p(θ)

dθ
dφ

∝
p
I(θ)

dθ
dφ

=
s
I(θ)
 dθ
dφ
2
=
v
u
u
tEθ
" ∂
∂θ ln
 p(X; θ)
2#  dθ
dφ
2
=
v
u
u
tEθ
" ∂
∂θ ln
 p(X; θ)

· dθ
dφ
2#
=
v
u
u
tEφ
" ∂
∂φ ln
 p(X; φ)
2#
=
p
I(φ),
which shows that p(φ) ∝
p
I(φ).
Example: The Jeﬀreys prior for the parameter φ ∈[0, 1] corresponding to the binomial distri-
bution with parameters n and φ is
p(φ) ∝
1
p
φ(1 −φ)
.
Since this is proportional to the density of the Beta distribution with parameters 1/2 and 1/2
and β(1/2, 1/2) = π, it follows that the Jeﬀreys prior in this case is
p(φ) =
1
π
p
φ(1 −φ)
,
which is also known as the arcsine distribution. It is notable that this is not the uniform dis-
tribution on [0, 1] and so in this case the uninformative prior distribution selected by Jeﬀreys’
invariance principle is not the same as the prior distribution selected by the maximum entropy
principle. On the other hand, it can be shown that the Jeﬀreys prior for φ is equivalent to
assuming that the parameter θ = arcsin(√φ) obtained by applying the arcsine transformation
to φ is uniformly distributed on [0, π/2]. Thus the Jeﬀreys prior is consistent with the maximum
entropy principle provided we apply the latter to the parameter θ rather than φ.
Example: Now consider the family of normal distributions with parameterized by the mean µ
and with ﬁxed variance σ2:
p(x; µ) =
1
σ
√
2πe−(x−µ)2/2σ2.
The log-likelihood function is
ln
 p(x; µ)

= 1
2 ln(2π) −ln(σ) −
1
2σ2 (x −µ)2
while the score function (with respect to µ) is
S(x; µ) = −x −µ
σ2
.

108
CHAPTER 8. BAYESIAN INFERENCE FOR GAUSSIAN DISTRIBUTIONS
From this we ﬁnd that the Fisher information for µ is
I(µ) = Varµ
X −µ
σ2

= 1
σ2 ,
which is independent of µ but inversely proportional to the variance. This shows that the Jeﬀreys’
prior for µ is constant
p(µ) ∝1
σ
and thus uniform on the set of possible values for µ.
To ﬁnd the Jeﬀreys prior for the standard deviation σ, we ﬁrst calculate the score function for σ
S(x; σ) = −1
σ + (x −µ)2
σ3
= (x −µ)2 −σ2
σ3
and then ﬁnd that the Fisher information is
I(σ)
=
Varσ(S(X; σ))
=
σ−6Eσ
h (X −µ)2 −σ22i
=
σ−6E

(X −µ)4 −2σ2(X −µ)2 + σ4
=
σ−6  3σ4 −2σ4 + σ4
=
2
σ2 .
This shows that the density of the Jeﬀreys prior for σ is proportional to
p(σ) ∝1
σ,
as was assumed in Section 8.1.2.

