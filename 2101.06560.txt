Adversarial Attacks On Multi-Agent Communication
James Tu1,2*
Tsunhsuan Wang3‚àó
Jingkang Wang1,2
Sivabalan Manivasagam1,2
Mengye Ren1,2
Raquel Urtasun1,2
1Waabi
2University of Toronto
3MIT
{jtu, wangjk, manivasagam, mren, urtasun}@cs.toronto.edu
johnsonwang0810@gmail.com
Abstract
Growing at a fast pace, modern autonomous systems will
soon be deployed at scale, opening up the possibility for
cooperative multi-agent systems. Sharing information and
distributing workloads allow autonomous agents to better
perform tasks and increase computation efÔ¨Åciency. How-
ever, shared information can be modiÔ¨Åed to execute adver-
sarial attacks on deep learning models that are widely em-
ployed in modern systems. Thus, we aim to study the robust-
ness of such systems and focus on exploring adversarial at-
tacks in a novel multi-agent setting where communication is
done through sharing learned intermediate representations
of neural networks. We observe that an indistinguishable
adversarial message can severely degrade performance, but
becomes weaker as the number of benign agents increases.
Furthermore, we show that black-box transfer attacks are
more difÔ¨Åcult in this setting when compared to directly per-
turbing the inputs, as it is necessary to align the distribution
of learned representations with domain adaptation. Our
work studies robustness at the neural network level to con-
tribute an additional layer of fault tolerance to modern se-
curity protocols for more secure multi-agent systems.
1. Introduction
With rapid improvements of modern autonomous sys-
tems, it is only a matter of time until they are deployed at
scale, opening up the possibility of cooperative multi-agent
systems. Individual agents can beneÔ¨Åt greatly from shared
information to better perform their tasks [26, 59]. For ex-
ample, by aggregating sensory information from multiple
viewpoints, a Ô¨Çeet of vehicles can perceive the world more
clearly, providing signiÔ¨Åcant safety beneÔ¨Åts [52]. More-
over, in a network of connected devices, distributed pro-
cessing across multiple agents can improve computation ef-
*Equal contribution.
Work done while all authors were at UberATG.
Ô¨Åciency [18]. While cooperative multi-agent systems are
promising, relying on communication between agents can
pose security threats as shared information can be malicious
or unreliable [54, 3, 37].
Meanwhile, modern autonomous systems typically rely
on deep neural networks known to be vulnerable to adver-
sarial attacks. Such attacks craft small and imperceivable
perturbations to drastically change a neural network‚Äôs be-
havior and induce false outputs [48, 21, 8, 30]. Even if an at-
tacker has the freedom to send any message, such small per-
turbations may be the most dangerous as they are indistin-
guishable from their benign counterparts, making corrupted
messages difÔ¨Åcult to detect while still highly malicious.
While modern cyber security algorithms provide ade-
quate protection against communication breaches, adversar-
ial robustness of multi-agent deep learning models has yet
to be studied. Meanwhile, when it comes to safety-critical
applications like self-driving, additional layers of redun-
dancy and improved security are always welcome. Thus,
by studying adversarial robustness, we can enhance modern
security protocols by introducing an additional layer of fault
tolerance at the neural network level.
Adversarial attacks have been studied extensively but ex-
isting approaches mostly consider attacks on input domains
like images [48, 21], point clouds [7, 50], and text [44, 14].
On the other hand, multi-agent systems often distribute
computation across different devices and transmit inter-
mediate representations instead of input sensory informa-
tion [52, 18]. SpeciÔ¨Åcally, when deep learning inference is
distributed across different devices, agents will communi-
cate by transmitting feature maps, which are activations of
intermediate neural network layers. Such learned commu-
nication has been shown to be superior due to transmitting
compact but expressive messages [52] as well as efÔ¨Åciently
distributing computation [18].
In this paper, we investigate adversarial attacks in this
novel multi-agent setting where perturbations are applied
to learned intermediate representations. An illustration is
arXiv:2101.06560v2  [cs.LG]  12 Oct 2021

Multi-Agent 
Aggregation
Message Encoder
Output Net
Output
Victim Agent
Attacker
ùíéùüë
ùíéùüè
Agent 3
Agent 1
ùíéùüê
Œµ
Sensory Input
Multi-Agent Setting
Figure 1. Overview of a multi-agent setting with one malicious agent (red). Here the malicious agent attempts to sabotage a victim agent
by sending an adversarial message. The adversarial message is indistinguishable from the original, making the attack difÔ¨Åcult to detect.
shown in Figure 1. We conduct experiments and showcase
vulnerabilities in two highly practical settings: multi-view
perception from images in a Ô¨Çeet of drones and multi-view
perception from LiDAR in a Ô¨Çeet of self-driving vehicles
(SDVs). By leveraging information from multiple view-
points, these multi-agent systems are able to signiÔ¨Åcantly
outperform those that do not exploit communication.
We show, however, that perturbed transmissions which
are indistinguishable from the original can severely degrade
the performance of receivers particularly as the ratio of ma-
licious to benign agents increases. With only a single at-
tacker, as the number of benign agents increase, attacks be-
come signiÔ¨Åcantly weaker as aggregating more messages
decreases the inÔ¨Çuence of malicious messages. When mul-
tiple attackers are present, they can coordinate and jointly
optimize their perturbations to strengthen the attack.
In
terms of defense, when the threat model is known, adver-
sarial training is highly effective, and adversarially trained
models can defend against perturbations almost perfectly
and even slightly enhance performance on natural exam-
ples.
Without knowledge of the threat model, we can
still achieve reasonable adversarial robustness by designing
more robust message aggregation modules.
We then move on to more practical attacks in a black box
setting where the model is unknown to the adversary. Since
query-based black box attacks need to excessively query a
target model that is often unaccessible, we focus on query-
free transfer attacks that are more feasible in practice. How-
ever, transfer attacks are much more difÔ¨Åcult to execute at
the feature-level than on input domains. In particular, since
perturbation domains are model dependent, vanilla transfer
attacks are ineffective because two neural networks with the
same functionality can have very different intermediate rep-
resentations. Here, we Ô¨Ånd that training the surrogate model
with domain adaptation is key to aligning the distribution of
intermediate features and achieve much better transferabil-
ity. To further enhance the practicality of attacks, we pro-
pose to exploit the temporal consistency of sensory infor-
mation processed by modern autonomous systems. When
frames of sensory information are collected milliseconds
apart, we can exploit the redundancy in adjacent frames to
create efÔ¨Åcient, low-budget attacks in an online manner.
2. Related Work
Multi-Agent Deep Learning Systems:
Multi-agent and
distributed systems are widely employed in real-world ap-
plications to improve computation efÔ¨Åciency [27, 17, 2],
collaboration [52, 59, 18, 41, 42], and safety [38, 35]. Re-
cently, autonomous systems have improved greatly with the
help of neural networks. New directions have opened up
in cooperative multi-agent deep learning systems e.g., fed-
erated learning [27, 2]. Although multi-agent communi-
cation introduces a multitude of beneÔ¨Åts, communication
channels are vulnerable to security breaches, as commu-
nication channels can be attacked [34, 45], encryption al-
gorithms can be broken [46], and agents can be compro-
mised [5, 61]. Thus, imperfect communication channels
may be used to execute adversarial attacks which are es-
pecially effective against deep learning systems. While ro-
bustness has been studied in the context of federated learn-
ing [20, 1, 56, 19], the threat models are different as dataset
poisoning and model poisoning are typically used. To the
best of our knowledge, few works study adversarial robust-
ness on multi-agent deep learning systems during inference.
Adversarial Attacks:
Adversarial attacks were Ô¨Årst dis-
covered in the context of image classiÔ¨Åcation [48], where
a small imperceivable perturbation can drastically change a
neural network‚Äôs behaviour and induce false outputs. Such
attacks were then extended to various applications such
as semantic segmentation [57] and reinforcement learn-
ing [24]. There are two main settings for adversarial at-

99% Bowl
92% Laptop
92% Laptop
93% Bottle
Laptop Proposal
Bowl Proposal
Bottle Proposal
Background Proposal
Detection Proposals
False Negative Generation
False Positive Generation
Change Proposal
Class
Generate New 
Proposals
Figure 2. Attacking object detection proposals: False positives are created by changing the class of background proposals and false
negatives are created by changing the class of the original proposals.
tacks - white box and black box.
In a white box set-
ting [48, 21, 30], the attacker has full access to the tar-
get neural network weights and adversarial examples can
be generated using gradient-based optimization to maxi-
mize the network‚Äôs error. In contrast, black box attacks are
conducted without knowledge of the target neural network
weights and therefore without any gradient computation.
In this case, attackers can leverage real world knowledge
to inject adversaries that resemble common real world ob-
jects [47, 36]. However, if the attacker is able to query the
target model, the literature proposes several different strate-
gies to perform query-based attacks [4, 12, 6, 10]. How-
ever, query-based attacks are infeasible for some applica-
tions as they typically require prohibitively large amounts
of queries and computation. Apart from query-based at-
tacks, a more practical but more challenging alternative is
to conduct transfer attacks [39, 58, 16] which do not require
querying the target model. In this setting, the attacker trains
a surrogate model that imitates the target model. By doing
so, the hope is that perturbations generated for the surrogate
model will transfer to the target model.
Perturbations In Feature Space:
While most works in
the literature focus on input domains like images, some
prior works have considered perturbations on intermediate
representations within neural networks. SpeciÔ¨Åcally, [25]
estimated the projection of adversarial gradients on a se-
lected subspace to reduce the queries to a target model.
[40, 44, 14] proposed to generate adversarial perturbation in
word embeddings for Ô¨Ånding adversarial but semantically-
close substitution words. [55, 60] showed that training on
adversarial embeddings could improve the robustness of
Transformer-based models for NLP tasks.
3. Attacks On Multi-Agent Communication
This section Ô¨Årst introduces the multi-agent framework
in which agents leverage information from multiple view-
points by transmitting intermediate feature maps. We then
present our method for generating adversarial perturbations
in this setting. Moving on to more practical settings, we
consider black box transfer attacks and Ô¨Ånd that it is nec-
essary to align the distribution of intermediate representa-
tions. Here, training a surrogate model with domain adapta-
tion can create transferable perturbations. Finally, we show
efÔ¨Åcient online attacks by exploiting the temporal consis-
tency of sensory inputs collected at high frequency.
3.1. Multi-Agent Communication
We consider a setting where multiple agents cooperate to
better perform their tasks by sharing observations from dif-
ferent viewpoints encoded via a learned intermediate rep-
resentation. Adopting prior work [52], we assume a ho-
mogeneous set of agents using the same neural network.
Then, each agent i processes sensor input xi to obtain an
intermediate representation mi = F(xi).
The interme-
diate feature map is then broadcasted to other agents in
the scene. Upon receiving messages, agent j will aggre-
gate and process all incoming messages to generate output
Zj = G(m1, . . . , mN), where N is the number of agents.
Suppose that an attacker agent i targets a victim agent j.
Here, the attacker attempts to send an indistinguishable ad-
versarial message m
‚Ä≤
i = mi + Œ¥ to maximize the error in
Z
‚Ä≤
j = G(m1, . . . mi + Œ¥, mN). The perturbation Œ¥ is con-
strained by ‚à•Œ¥‚à•p ‚â§œµ to ensure that the malicious message
is subtle and difÔ¨Åcult to detect. An overview of the multi-
agent setting is shown in Figure 1.
In this paper, we speciÔ¨Åcally focus on object detection
as it is a challenging task where aggregating information
from multiple viewpoints is particularly helpful. In addi-
tion, many downstream robotics tasks depend on detection
and thus a strong attack can jeopardize the performance of
the full system. In this case, output Z is a set of M bound-
ing box proposals z(1), . . . , z(M) at different spatial loca-
tions. Each proposal consists of class scores zœÉ0, . . . , zœÉk

D
F‚Äô
F
Output
Task 
Loss
Discriminator 
Loss
Surrogate Model
Victim Model
Training Surrogate
Transfer Attack
F
Adversarial 
Loss
Surrogate Model
Victim Model
G
G‚Äô
Input
Match 
Distribution
G‚Äô
Unpaired
Inputs
Figure 3. Our proposed transfer attack which incorporates domain adaptation when training the surrogate model. During training, the
discriminator forces F ‚Ä≤ to produce intermediate representations similar to F. As a result, G‚Ä≤ can generate perturbations that transfer to G.
and bounding box parameters describing the spatial loca-
tion and dimensions of the bounding box.
Here classes
0, . . . , k ‚àí1 are the object classes and k denotes the back-
ground class where no objects are detected.
When performing detection, models try to output the cor-
rect object class k and maximize the ratio of intersection
over union (IOU) of the proposed and ground truth bound-
ing boxes. In a post processing step, proposals with high
conÔ¨Ådence are selected and overlapping bounding boxes are
Ô¨Åltered with non-maximum suppression (NMS) to ideally
produce a single estimate per ground truth object.
3.2. Adversarial Perturbation Generation
We Ô¨Årst introduce our loss objective for generating ad-
versarial perturbations against object detection. To generate
false outputs, we aim to confuse the proposal class. For
detected objects, we suppress the score of the correct class
to generate false negatives. For background classes, false
positives are created by pushing up the score of an object
class. In addition, we also aim to minimize the intersection-
over-union (IoU) of the bounding box proposals to fur-
ther degrade performance by producing poorly localized ob-
jects. We deÔ¨Åne the adversarial loss of the perturbed output
z‚Ä≤ with respect to an unperturbed output z instead of the
ground truth, as it may not always be available to the at-
tacker. For each proposal z, let u = argmaxi{zœÉi|i =
0 . . . m} be the highest conÔ¨Ådence class. Given the origi-
nal object proposal z and the proposal after perturbation z‚Ä≤,
our loss function tries to push z‚Ä≤ away from z:
‚Ñìadv(z‚Ä≤, z) =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
‚àílog(1 ‚àíz‚Ä≤
œÉu) ¬∑ IoU(z‚Ä≤, z)
if u Ã∏= k and zœÉu > œÑ +,
‚àíŒª ¬∑ z‚Ä≤Œ≥
œÉv log(1 ‚àíz‚Ä≤
œÉv)
if u = k and zœÉu > œÑ ‚àí,
0
otherwise
(1)
An illustration of the attack objective is shown in Figure 2.
When u Ã∏= k and the original prediction is not a back-
ground class, we apply an untargetted loss to reduce the
likelihood of the intended class. When the intended pre-
diction is the background class k, we speciÔ¨Åcally target a
non-background class v to generate a false positive. We
simply choose v to be the class with the highest conÔ¨Ådence
that is not the background class. The IoU operator denotes
the intersection over union of two proposals, Œª is a weight-
ing coefÔ¨Åcient, and œÑ ‚àí, œÑ + Ô¨Ålter out proposals that are not
conÔ¨Ådent enough. We provide more analysis and ablations
to justify our loss function design in our experiments.
Following prior work [50], it is necessary to minimize
the adversarial loss over all proposals. Thus, the optimal
perturbation under an œµ - ‚Ñìp bound is
Œ¥‚ãÜ= argmin
‚à•Œ¥‚à•p‚â§œµ
M
X
m=1
‚Ñìadv(z‚Ä≤(m), z(m)).
(2)
Our work considers an inÔ¨Ånity norm p = ‚àûand we min-
imize this loss across all proposals using projected gradient
descent (PGD) [31], clipping Œ¥ to be within [‚àíœµ, œµ].
3.3. Transfer Attack
We also consider transfer attacks as they are the most
practical. White box attacks assume access to the victim
model‚Äôs weights which is difÔ¨Åcult to obtain in practice. On
the other hand, query-based optimization is too expensive to
execute in real time as state-of-the-art methods still require
thousands of queries [13, 11] on CIFAR-10. Instead, when
we do not have access to the weights of the victim model
G, we can imitate it with a surrogate model G‚Ä≤ such that
perturbations generated by the surrogate model can transfer
to the target model.
One major challenge for transfer attacks in our setting
is that perturbations are generated on intermediate feature
maps. Our experiments show that vanilla transfer attacks
are almost completely ineffective as two networks with the
same functionality do not necessarily have the same inter-
mediate representations.
When training F and G, there
is no direct supervision on the intermediate features m =
F(x). Therefore, even with the same architecture, dataset,

Figure 4. Two multi-agent datasets we use. On the left are images of ShapeNet objects taken from different view points. On the right are
LiDAR sweeps by different vehicles in the same scene.
and training schedule, a surrogate F ‚Ä≤ may produce mes-
sages m‚Ä≤ with very different distribution from m. As an ex-
ample, a permutation of feature channels carries the same
information but results in a different distribution. In gen-
eral, different random seeds, network initializations or non-
deterministic GPU operations can result in different inter-
mediate representations. It follows that if m‚Ä≤ does not faith-
fully replicate m, we cannot expect G‚Ä≤ to imitate G.
Thus, to execute transfer attacks, we must have access
to samples of the intermediate feature maps. SpeciÔ¨Åcally,
we consider a scenario where the attacker can spy on the
victim‚Äôs communication channel to obtain transmitted mes-
sages. However, since sensory information is not transmit-
ted, the attacker does not have access to pairs of input x
and intermediate representation m to directly supervise the
surrogate F ‚Ä≤ via distillation. Thus, we propose to use Ad-
versarial Discriminative Domain Adaptation (ADDA) [51]
to align the distribution of m and m‚Ä≤ without explicit input-
feature pairs. An overview is shown in Figure 3.
In the original training pipeline, F ‚Ä≤ and G‚Ä≤ would be
trained to minimize task loss
Ltask(z, y, b) =

‚àílog(zœÉy) ‚àíIoU(z, b)
if y Ã∏= k,
‚àílog(zœÉy)
if y = k,
(3)
where b is a ground truth bounding box and y is its class.
The task loss maximizes the log likelihood of the correct
class and the IoU between the proposal box and the ground
truth box. In addition, we encourage domain adaptation by
introducing a discriminator D to distinguish between real
messages m and surrogate messages m‚Ä≤. The three modules
F ‚Ä≤, G‚Ä≤, and D can be optimized using the following min-
max criterion:
min
F ‚Ä≤ G‚Ä≤ max
D Ltask(x) + Œ≤

log D(F(x)) + log(1 ‚àíD(F ‚Ä≤(x)))]
(4)
where Œ≤ is a weighting coefÔ¨Åcient and we use binary cross
entropy loss to supervise the discriminator. During training,
we adopt spectral normalization [33] in the discriminator
and the two-time update rule [22] for stability.
3.4. Online Attack
In modern applications of autonomous systems, consec-
utive frames of sensory information are typically collected
only milliseconds apart. Thus, there is a large amount of
redundancy between consecutive frames which can be ex-
ploited to achieve more efÔ¨Åcient adversarial attacks. Fol-
lowing previous work [53] in images, we propose to exploit
this redundancy by using the perturbation from the previous
time step as initialization for the current time step.
Furthermore, we note that intermediate feature maps
capture the spatial context of sensory observations, which
change due to the agent‚Äôs egomotion. Therefore, by apply-
ing a rigid transformation on the perturbation at every time
step to account for egomotion, we can generate stronger
perturbations that are synchronized with the movement of
sensory observations relative to the agent. In this case, the
perturbations are updated as follows:
Œ¥(t+1) ‚ÜêHt‚Üít+1(Œ¥(t)) ‚àíŒ±‚àáHt‚Üít+1(Œ¥)Ladv(Z‚Ä≤(t+1), Z(t+1)).
(5)
Here Ht‚Üít+1 is a rigid transformation mapping the at-
tacker‚Äôs pose at time t to t + 1 and Œ± is the step size.
By leveraging temporal consistency we can generate strong
perturbations with only one gradient update per time step,
making online attacks more feasible.
4. Experiments
4.1. Multi-Agent Settings
Multi-View ShapeNet:
We conduct our attacks on multi-
view detection from images, which is a common task for
a Ô¨Çeets of drones. Following prior work [15], we generate
a synthetic dataset by placing 10 classes of ShapeNet [9]
objects on a table (see Figure 4). From each class, we sub-

False Positive
True Positive
False Negative
Original
Original
Perturbed
Perturbed
Output
Feature Map
ShapeNet
V2V
Figure 5. Qualitative attack examples. Top: Messages sent by an-
other agent visualized in bird‚Äôs eye view. Bottom: outputs. Pertur-
bations are very subtle but severely degrade performance.
sample 50 meshes and use a 40/10 split for training and val-
idation. In every scene, we place 4 to 8 objects and perform
collision checking to ensure objects do not overlap. Then,
we capture 128√ó128 RGB-D images from 2 to 7 viewpoints
sampled from the upper half of a sphere centered at the ta-
ble center with a radius of 2.0 units. This dataset consists of
50,000 training scenes and 10,000 validation scenes. When
conducting attacks, we randomly sample one of the agents
to be the adversary. Our detection model uses an architec-
ture similar to the one introduced in [15]. SpeciÔ¨Åcally, we
process input RGB-D images using a U-Net [43] and then
unproject the features into 3D using the depth measures.
Features from all agents are then warped into the same co-
ordinate frame and aggregated with mean pooling. Finally,
aggregated features are processed by a 3D U-Net and a de-
tection header to generate 3D bounding box proposals.
Vehicle To Vehicle Communication:
We also consider a
self-driving setting with vehicle-to-vehicle(V2V) commu-
nication. Here, we adopt the dataset used in [52], where 3D
reconstructions of logs of real world LiDAR scans are sim-
ulated from the perspectives of other vehicles in the scene
using a high-Ô¨Ådelity LiDAR simulator [32].
These logs
are collected by self-driving vehicles equipped with LiDAR
sensors capturing 10 frames per second (see Figure 4). The
training set consists of 46,796 subsampled frames from the
logs and we do not subsample the validation set, resulting
in 96,862 frames. In every log we select one attacker vehi-
cle and sample others to be cooperative agents with up to
7 agents in each frame unless otherwise speciÔ¨Åed. This re-
sults in a consistent assignment of attackers and V2V agents
throughout the frames. In this setting, we use the state-of-
the-art perception and motion forecasting model V2VNet
[52]. Here, LiDAR inputs are Ô¨Årst encoded into bird‚Äôs eye
view (BEV) feature maps. Feature maps from all agents
are then warped into the ego coordinate frame and aggre-
gated with a GNN to produce BEV bounding box propos-
als. More details of the ShapeNet model and V2VNet are
provided in the supplementary material.
2
3
4
5
6
7
Number of Agents
0
10
20
30
40
50
60
70
AP at 0.7 IoU
ShapeNet
2
3
4
5
6
7
Number of Agents
20
40
60
80
AP at 0.7 IoU
V2V
No Attack
Noise
White Box
Transfer
Figure 6. Evaluation under no perturbation, uniform noise, transfer
attack, and white box attack. Results are grouped by the number
of agents in the scene where one agent is the attacker.
ShapeNet
V2V
Clean
Perturbed
Clean
Perturbed
Original
66.33
0.62
82.19
7.55
Adv Trained
67.29
66.00
82.60
83.44
Table 1. Results of adversarial training. Robustness increases sig-
niÔ¨Åcantly, matching clean inference. Furthermore performance on
clean data also improves slightly.
Implementation Details:
When conducting attacks, we
set œµ = 0.1.
For the proposed loss function, we set
Œª = 0.2, œÑ ‚àí= 0.7, œÑ + = 0.3, and Œ≥ = 1. Projected
gradient descent is done using Adam with learning rate 0.1
and we apply 15 PGD steps for ShapeNet and only 1 PGD
step for low budget online attacks in the V2V setting. The
surrogate models use the same architecture and dataset as
the victim models. When training the surrogate model, we
set Œ≤ = 0.01, model learning rate 0.001, and discrimina-
tor learning rate 0.0005. For evaluation, we compute area
under the precision-recall curve of bounding boxes, where
bounding boxes are correct if they have an IoU greater than
0.7 with a ground truth box of the same class. We refer to
this metric as AP at 0.7 in the following.
4.2. Results
Attack Results:
Visualizations of our attack are shown in
Figure 5 and we present quantitative results of our attack
and baselines in Figure 6. We split up the evaluation by
the number of agents in the scene and one of the agents is
always an attacker. As a baseline, we sample the perturba-
tion from U(‚àíœµ, œµ) to demonstrate that the same œµ bounded
uniform noise does not have any impact on detection perfor-
mance. The white box attack is especially strong when few
agents are in the scene, but becomes weaker as the number
of benign agents increase, causing the relative weight of the
adversarial features in mean pooling layers to decrease. Fi-
nally, our transfer attack with domain adaptation achieves
moderate success with few agents in the scene, but is sig-
niÔ¨Åcantly weaker than the white box attack.

Clean
Perturbed
Agents
2
4
6
2
4
6
Mean Pool
82.09
89.25
92.43
0.90
12.93
41.77
GNN(Mean)
82.19
89.93
92.94
7.55
52.31
76.18
GNN(Median)
82.11
87.12
90.75
12.8
67.70
86.30
GNN(Soft Med)
82.19
89.67
92.49
21.53
61.37
84.99
Table 2. Choice of fusion in V2VNet affects performance and ro-
bustness. We investigate using mean pooling and using a GNN
with various aggregation methods.
Robustifying Models:
To defend against our proposed at-
tack, we conduct adversarial training against the white box
adversary and show the results in Table 1. Here, we follow
the standard adversarial training set up, except perturbations
are applied to intermediate features instead of inputs. This
objective can be formulated as
min
Œ∏
E(x,y)‚àºD max
‚à•Œ¥‚à•‚àû<œµ œÜ((x, y, Œ¥); Œ∏) :=
Ltask (G(F(x0), . . . , F(xi) + Œ¥, . . . , F(xN); Œ∏)) ,
(6)
where D is the natural training distribution and Œ∏ denotes
model parameters. During training, we generate a new per-
turbation Œ¥ for each training sample. In the multi-agent set-
ting, we Ô¨Ånd it easier to recover from adversarial pertur-
bations when compared to traditional single-agent attacks.
Moreover, adversarial training is able to slightly improve
performance on clean data as well, while adversarial train-
ing has been known to hurt natural performance in previous
settings [28, 49].
While adversarial training is effective in this setting, it
requires knowledge of the threat model. When the threat
model is unknown, we can still naturally boost the robust-
ness of multi-agent models with the design of the aggrega-
tion module. SpeciÔ¨Åcally, we consider several alternatives
to V2VNet‚Äôs GNN fusion and present the performance un-
der attacked and clean data in Table 2. First, replacing the
entire GNN with an adaptive mean pooling layer signiÔ¨Å-
cantly decreases robustness. On the other hand, we swap
out the mean pooling in GNN nodes with median pooling
and Ô¨Ånd that it increases robustness at the cost of perfor-
mance on clean data with more agents, since more infor-
mation is discarded. We refer readers to the supplementary
materials for more details on implementation of the soft me-
dian pooling.
Multiple Attackers:
We previously focused on settings
with one attacker, and now conduct experiments with mul-
tiple attackers in the V2V setting. In each case, we also
consider if attackers are able to cooperate. In cooperation,
attackers jointly optimize their perturbations. Without co-
operation, attackers are blind to each other and optimize
Cooperative
Non-Cooperative
Agents
4
5
6
4
5
6
1 Attacker
52.31
65.00
76.18
52.31
65.00
76.18
2 Attacker
28.31
41.34
54.50
39.02
51.96
64.02
3 Attacker
12.07
22.84
35.13
24.27
38.17
51.58
Table 3. Multiple white box attackers in the V2V setting. Co-
operative attackers jointly optimize their perturbations and non-
cooperative attackers optimize without knowledge of each other.
Attackers
0
1
2
3
Train On 0
89.93
52.31
28.31
12.07
Train On 1
90.09
90.00
81.95
75.28
Train On 2
89.71
89.68
88.91
88.33
Train On 3
89.55
89.51
88.94
88.51
Table 4. Adversarial training with multiple attackers in the V2V
setting. We train on settings with various number of attackers and
evaluate the models across the settings.
their perturbations assuming other messages have not been
perturbed. Results with up to 3 attackers are shown in Ta-
ble 3. As expected, more attackers can increase the strength
of attack signiÔ¨Åcantly, furthermore, if multiple agents can
coordinate, a stronger attack can be generated.
Next, we apply adversarial training to the multi-attacker
setting and present results in Table 4. Here, all attacks are
done in the cooperative setting and we show results with 4
total agents. Similar to the single attacker setting, adver-
sarial training is highly effective. However, while adversar-
ial training against one attacker improves performance in
natural examples, being robust to stronger attacks sacriÔ¨Åces
performance on natural examples. This suggests that adver-
sarial training has the potential to improve general perfor-
mance when an appropriate threat model is selected. Fur-
thermore, we can see that training on fewer attacks does
not generalize perfectly to more attackers but the opposite
is true. Thus, it is necessary to train against an equal or
greater threat model to fully defend against such attacks.
Domain Adaptation:
More results of the transfer attack
are included in Table 5. First, we conduct an ablation and
show that a transfer attack without domain adaptation (DA)
is almost completely ineffective. On the contrary, surrogate
models trained with DA achieve signiÔ¨Åcant improvements.
A visual demonstration of feature map alignment with DA
is shown in Figure 7, visualizing 4 channels of the interme-
diate feature maps. Features from a surrogate trained with
DA is visually very similar to the victim, while a surrogate
trained without DA produces features with no resemblance.
Since our proposed DA improves the transferability of
the surrogate model, we can further improve our transfer

ShapeNet
V2V
Clean
66.28
82.19
Transfer
66.21
81.31
Transfer + DA
42.59
72.45
Transfer + DA + ILAP
35.69
71.76
Transfer + DA + DI
49.38
75.18
Table 5. Transfer attacks evaluated with 2 agents. Training the sur-
rogate with domain adaptation (DA) signiÔ¨Åcantly improves trans-
ferability. In addition, we attempt to enhance transferability with
ILAP [23] and DI [58].
Victim
DA
No DA
Figure 7. Visualization of how domain adaptation(DA) affects 4
channels of the intermediate feature map. Observe that the surro-
gate trained with DA closely imitates the victim model, while the
surrogate trained without DA produces different features.
attack by also adopting methods from the literature which
enhance the transferability of a given perturbation.
We
Ô¨Ånd that generating perturbations from diversiÔ¨Åed inputs
(DI) [58] is ineffective as resizing input feature maps dis-
torts spatial information which is important for localizing
objects detection. On the other hand, using an intermediate
level attack projection (ILAP) [23] yields a small improve-
ment. Overall, we Ô¨Ånd transfer attacks more challenging
when at the feature level. In standard attacks on sensory
inputs, perturbations are transferred into the same input do-
main. However, at a feature level the input domains are
model-dependent, making transfer attacks between differ-
ent models more difÔ¨Åcult.
Online Attacks:
We conduct an ablation on the proposed
methods for exploiting temporal redundancy in an online
V2V setting, shown in Table 6. First, if we ignore tempo-
ral redundancy and do not reuse the previous perturbation,
attacks are much weaker. In this evaluation we switch from
PGD [31] to FGSM [21] to obtain a stronger perturbation in
one update for fair comparison. We also show that applying
a rigid transformation on the perturbations at every frame to
compensate for egomotion provides a modest improvement
to the attack when compared to the No Warp ablation.
Loss Function Design:
We conduct an ablation study
on using our adversarial loss Ladv instead of the neg-
ative task loss ‚àíLtask in Table 7.
This ablation vali-
dates our loss function and showcase that for structured
outputs, properly designed adversarial losses is more ef-
fective than the naive negative task loss which is widely
AP @ 0.7
2 Agents
4 Agents
6 Agents
Our Attack
7.55
52.31
76.18
No Warping
7.17
52.35
77.37
Independent
56.98
80.21
87.05
Table 6. Ablation on online attacks in the V2V setting. Indepen-
dent refers to treating each frame independently and not reusing
previous perturbations. No warp refers to omitting the rigid trans-
formation to account for egomotion.
2 Agents
4 Agents
6 Agents
ShapeNet
‚àíLtask
6.10
20.07
29.00
Ladv
0.37
4.45
13.77
V2V
‚àíLtask
20.8
63.82
79.11
Ladv
7.55
52.31
76.18
Table 7. Ablation on loss function, it produces stronger adversarial
attacks than simply using the negative of the training task loss.
used in image classiÔ¨Åcation tasks.
Our choice for the
loss function design is motivated by our knowledge of the
post-processing non-maximum suppression (NMS). Since
NMS selects bounding boxes with the highest conÔ¨Ådence
in a local region, proposals with higher scores should re-
ceive stronger gradients. More speciÔ¨Åcally, an appropri-
ate loss function of f for proposal score œÉ should sat-
isfy (|‚àáœÉ2f(œÉ2)| ‚àí|‚àáœÉ1f(œÉ1)|) /(œÉ2 ‚àíœÉ1) > 0 so that
|‚àáœÉf(œÉ)| is monotonically increasing in œÉ. We can see
that the standard log likelihood does not satisfy this criteria,
which explains why our loss formulation is more effective.
In addition, we add the focal loss term [29] to generate more
false positives, as aggressively focusing on one proposal in
a local region is more effective due to NMS.
5. Conclusion
In this paper, we investigate adversarial attacks on com-
munication in multi-agent deep learning systems. Our ex-
periments in two practical settings demonstrate that com-
promised communication channels can be used to execute
adversarial attacks. However, robustness increases as the ra-
tio of benign to malicious actors increases. Furthermore, we
found that more practical transfer attacks are more challeng-
ing in this setting and require aligning the distributions of
intermediate representations. Finally, we propose a method
to achieve efÔ¨Åcient and practical online attacks by exploit-
ing temporal consistency of sensory inputs.
We believe
studying adversarial robustness on multi-agent deep learn-
ing models in real-world applications is an important step
towards more secure multi-agent systems.

References
[1] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal,
and Seraphin B. Calo. Analyzing federated learning through
an adversarial lens. In ICML, volume 97 of Proceedings of
Machine Learning Research, pages 634‚Äì643. PMLR, 2019.
2
[2] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp,
Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chlo¬¥e
Kiddon, Jakub Konecn¬¥y, Stefano Mazzocchi, H. Brendan
McMahan, Timon Van Overveldt, David Petrou, Daniel Ra-
mage, and Jason Roselander. Towards federated learning at
scale: System design. CoRR, 2019. 2
[3] Niklas Borselius. Mobile agent security. Electronics & Com-
munication Engineering Journal, 14(5), 2002. 1
[4] Wieland Brendel, Jonas Rauber, and Matthias Bethge.
Decision-based adversarial attacks: Reliable attacks against
black-box machine learning models. In ICLR, 2018. 3
[5] Thomas Brewster.
Watch chinese hackers control tesla‚Äôs
brakes from 12 miles away, 2016. 2
[6] Thomas Brunner, Frederik Diehl, Michael Truong-Le, and
Alois Knoll. Guessing smart: Biased sampling for efÔ¨Åcient
black-box adversarial attacks. CoRR, abs/1812.09803, 2018.
3
[7] Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou,
Won Park, Sara Rampazzi, Qi Alfred Chen, Kevin Fu, and
Z. Morley Mao.
Adversarial sensor attack on lidar-based
perception in autonomous driving. In CCS, 2019. 1
[8] Nicholas Carlini and David Wagner. Towards evaluating the
robustness of neural networks. In SP, 2017. 1
[9] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat
Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-
lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi,
and Fisher Yu. ShapeNet: An Information-Rich 3D Model
Repository. arXiv preprint arXiv:1512.03012, 2015. 5
[10] Jianbo Chen, Michael I Jordan, and Martin J Wainwright.
Hopskipjumpattack: A query-efÔ¨Åcient decision-based attack.
arXiv preprint arXiv:1904.02144, 3, 2019. 3
[11] Jianbo Chen, Michael I. Jordan, and Martin J. Wainwright.
Hopskipjumpattack: A query-efÔ¨Åcient decision-based attack.
In IEEE Symposium on Security and Privacy, pages 1277‚Äì
1294. IEEE, 2020. 4
[12] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and
Cho-Jui Hsieh. ZOO: zeroth order optimization based black-
box attacks to deep neural networks without training substi-
tute models. In AISec, 2017. 3
[13] Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, Jin-
Feng Yi, and Cho-Jui Hsieh.
Query-efÔ¨Åcient hard-label
black-box attack: An optimization-based approach. In ICLR,
2019. 4
[14] Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen,
and Cho-Jui Hsieh.
Seq2sick: Evaluating the robustness
of sequence-to-sequence models with adversarial examples.
CoRR, abs/1803.01128, 2018. 1, 3
[15] Ricson Cheng, Ziyan Wang, and Katerina Fragkiadaki.
Geometry-aware recurrent neural networks for active visual
recognition. In NeurIPS. 2018. 5, 6
[16] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and
Jun Zhu.
Improving black-box adversarial attacks with a
transfer-based prior. In NeurIPS, 2019. 3
[17] Tharam S. Dillon, Chen Wu, and Elizabeth Chang. Cloud
computing: Issues and challenges. In AINA, 2010. 2
[18] Amir Erfan Eshratifar and Massoud Pedram.
Energy and
performance efÔ¨Åcient computation ofÔ¨Çoading for deep neu-
ral networks in a mobile cloud computing environment. In
GLSVLSI, 2018. 1, 2
[19] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhen-
qiang Gong. Local model poisoning attacks to byzantine-
robust federated learning. In USENIX Security Symposium,
pages 1605‚Äì1622. USENIX Association, 2020. 2
[20] Avishek Ghosh, Justin Hong, Dong Yin, and Kannan Ram-
chandran. Robust federated learning in a heterogeneous en-
vironment. arXiv preprint arXiv:1906.06629, 2019. 2
[21] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples.
ICLR,
2015. 1, 3, 8
[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NIPS, 2017. 5
[23] Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge J.
Belongie, and Ser-Nam Lim. Enhancing adversarial exam-
ple transferability with an intermediate level attack. CoRR,
abs/1907.10823, 2019. 8
[24] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan,
and Pieter Abbeel. Adversarial attacks on neural network
policies. 2017. 2
[25] Linxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey,
and Yu-Gang Jiang. Black-box adversarial attacks on video
recognition models. In ACM MM, 2019. 3
[26] Jakub Konecn¬¥y, H. Brendan McMahan, Felix X. Yu, Peter
Richt¬¥arik, Ananda Theertha Suresh, and Dave Bacon. Fed-
erated learning: Strategies for improving communication ef-
Ô¨Åciency. CoRR, abs/1610.05492, 2016. 1
[27] Jakub Konecn¬¥y, H. Brendan McMahan, Felix X. Yu, Peter
Richt¬¥arik, Ananda Theertha Suresh, and Dave Bacon. Fed-
erated learning: Strategies for improving communication ef-
Ô¨Åciency. CoRR, 2016. 2
[28] Saehyung Lee, Hyungyu Lee, and Sungroh Yoon. Adversar-
ial vertex mixup: Toward better adversarially robust general-
ization. In CVPR, pages 269‚Äì278. IEEE, 2020. 7
[29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll¬¥ar. Focal loss for dense object detection. In ICCV,
2017. 8
[30] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn-
ing models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083, 2017. 1, 3
[31] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn-
ing models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083, 2017. 4, 8
[32] Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong,
Wenyuan Zeng, Wei-Chiu Ma, Mikita Sazanovich, Bin

Yang, and Raquel Urtasun. Lidarsim: Realistic lidar sim-
ulation by leveraging the real world. In CVPR, 2020. 6
[33] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative ad-
versarial networks. arXiv preprint arXiv:1802.05957, 2018.
5
[34] Bassem Mokhtar and Mohamed Azab. Survey on security
issues in vehicular ad hoc networks. Alexandria engineering
journal, 54(4):1115‚Äì1126, 2015. 2
[35] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash
system. Technical report, 2019. 2
[36] Ben Nassi, Dudi Nassi, Raz Ben-Netanel, Yisroel Mirsky,
Oleg Drokin, and Yuval Elovici. Phantom of the adas: Phan-
tom attacks on driver-assistance systems.
IACR, 2020:85,
2020. 3
[37] Petr Nov¬¥ak, Milan Rollo, Jir¬¥ƒ± Hod¬¥ƒ±k, and Tom¬¥as Vl-
cek.
Communication security in multi-agent systems.
In
CEEMAS, volume 2691 of Lecture Notes in Computer Sci-
ence. Springer, 2003. 1
[38] Marcus Obst, Laurens Hobert, and Pierre Reisdorf. Multi-
sensor data fusion for checking plausibility of v2v commu-
nications by vision-based multiple-object tracking. In VNC,
2014. 2
[39] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow,
Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practi-
cal black-box attacks against machine learning. In AsiaCCS,
2017. 3
[40] Nicolas Papernot, Patrick D. McDaniel, Ananthram Swami,
and Richard E. Harang. Crafting adversarial input sequences
for recurrent neural networks. In MILCOM, 2016. 3
[41] Andreas Rauch, Felix Klanner, Ralph Rasshofer, and Klaus
Dietmayer. Car2x-based perception in a high-level fusion
architecture for cooperative perception systems. In IV, 2012.
2
[42] Matthias Rockl, Thomas Strang, and Matthias Kranz. V2v
communications in automotive multi-sensor multi-target
tracking. In VTC, 2008. 2
[43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, 2015. 6
[44] Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji Mat-
sumoto. Interpretable adversarial perturbation in input em-
bedding space for text. In IJCAI, 2018. 1, 3
[45] Hichem Sedjelmaci and Sidi Mohammed Senouci. An ac-
curate and efÔ¨Åcient collaborative intrusion detection frame-
work to secure vehicular networks. Computers & Electrical
Engineering, 43, 2015. 2
[46] Catherine Stupp and James Rundle. Capital one breach high-
lights shortfalls of encryption, 2019. 2
[47] Jiachen Sun, Yulong Cao, Qi Alfred Chen, and Z Morley
Mao. Towards robust lidar-based perception in autonomous
driving: General black-box adversarial sensor attack and
countermeasures.
In USENIX Security, pages 877‚Äì894,
2020. 3
[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. In-
triguing properties of neural networks. ICLR, 2014. 1, 2,
3
[49] Dimitris Tsipras,
Shibani Santurkar,
Logan Engstrom,
Alexander Turner, and Aleksander Madry. Robustness may
be at odds with accuracy. In ICLR (Poster). OpenReview.net,
2019. 7
[50] James Tu, Mengye Ren, Sivabalan Manivasagam, Min
Liang, Bin Yang, Richard Du, Cheng Frank, and Raquel Ur-
tasun. Towards physically realistic adversarial examples for
lidar object detection. arXiv, 2020. 1, 4
[51] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
Adversarial discriminative domain adaptation.
In CVPR,
2017. 5
[52] Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang,
Bin Yang, Wenyuan Zeng, and Raquel Urtasun.
V2vnet:
Vehicle-to-vehicle communication for joint perception and
prediction. arXiv, 2020. 1, 2, 3, 6
[53] Xingxing Wei, Jun Zhu, Sha Yuan, and Hang Su. Sparse
adversarial perturbations for videos. In AAAI, 2019. 5
[54] H. Chi Wong and Katia P. Sycara.
Adding security and
trust to multiagent systems. Applied ArtiÔ¨Åcial Intelligence,
14(9):927‚Äì941, 2000. 1
[55] Yi Wu, David Bamman, and Stuart J. Russell. Adversarial
training for relation extraction. In EMNLP, 2017. 3
[56] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. DBA:
distributed backdoor attacks against federated learning. In
ICLR. OpenReview.net, 2020. 2
[57] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou,
Lingxi Xie, and Alan Yuille. Adversarial examples for se-
mantic segmentation and object detection. In ICCV, 2017.
2
[58] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu
Wang, Zhou Ren, and Alan L. Yuille. Improving transferabil-
ity of adversarial examples with input diversity. In CVPR,
2019. 3, 8
[59] Tengchan Zeng, Mohammad Mozaffari, Omid Semiari,
Walid Saad, Mehdi Bennis, and Merouane Debbah. Wire-
less communications and control for swarms of cellular-
connected uavs. In ACSSC, 2018. 1, 2
[60] Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein,
and Jingjing Liu. Freelb: Enhanced adversarial training for
natural language understanding. In ICLR, 2020. 3
[61] Zeljka Zorz. Researchers hack bmw cars, discover 14 vul-
nerabilities, 2018. 2

