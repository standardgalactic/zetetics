Metric Flow Matching for Smooth Interpolations on
the Data Manifold
Kacper Kapusniak1∗, Peter Potaptchik1, Teodora Reu1, Leo Zhang1,
Alexander Tong2,3, Michael Bronstein1, Avishek Joey Bose1, Francesco Di Giovanni1
1University of Oxford, 2Mila, 3Université de Montréal
Abstract
Matching objectives underpin the success of modern generative models and rely
on constructing conditional paths that transform a source distribution into a target
distribution. Despite being a fundamental building block, conditional paths have
been designed principally under the assumption of Euclidean geometry, resulting in
straight interpolations. However, this can be particularly restrictive for tasks such as
trajectory inference, where straight paths might lie outside the data manifold, thus
failing to capture the underlying dynamics giving rise to the observed marginals.
In this paper, we propose METRIC FLOW MATCHING (MFM), a novel simulation-
free framework for conditional flow matching where interpolants are approximate
geodesics learned by minimizing the kinetic energy of a data-induced Riemannian
metric. This way, the generative model matches vector fields on the data manifold,
which corresponds to lower uncertainty and more meaningful interpolations. We
prescribe general metrics to instantiate MFM, independent of the task, and test it on
a suite of challenging problems including LiDAR navigation, unpaired image trans-
lation, and modeling cellular dynamics. We observe that MFM outperforms the Eu-
clidean baselines, particularly achieving SOTA on single-cell trajectory prediction.
1
Introduction
A central task in many natural and scientific domains entails the inference of system dynamics of
an underlying (physical) process from noisy measurements. A core challenge, in these application
domains such as biomedical ones—e.g. tracking health metrics [Oeppen and Vaupel, 2002] or
diseases [Hay et al., 2021]—is that one typically lacks access to entire time-trajectories and can only
leverage cross-sectional samples. An even more poignant example is the case of single-cell RNA
sequencing [Macosko et al., 2015, Klein et al., 2015], where measurements are sparse and static, due
to the procedure being expensive and destructive. Consequently, the nature of these tasks demands
the design of frameworks capable of reconstructing the temporal dynamics of a system (e.g. cells)
from observed time marginals that contain finite samples. This overarching problem specification
is referred to as trajectory inference [Hashimoto et al., 2016, Lavenant et al., 2021].
To address this challenge, we rely on matching objectives, a powerful generative modeling paradigm
encompassing successful approaches including diffusion models [Sohl-Dickstein et al., 2015, Song
et al., 2021], flow matching [Lipman et al., 2023, Liu et al., 2022, Albergo and Vanden-Eijnden,
2022], and finding a Schrödinger Bridge [Schrödinger, 1932, Léonard, 2013]. Specifically, to
reconstruct the unknown dynamics t 7→p∗
t between observed time marginals p0 and p1, we leverage
Conditional Flow Matching (CFM) [Tong et al., 2023b], a simulation-free framework which
constructs a probability path pt through interpolants xt connecting samples of p0 to samples of
p1. In general, xt is designed under the assumption of Euclidean geometry, resulting in straight
trajectories. However, in light of the widely accepted “manifold hypothesis” [Tenenbaum et al.,
∗Corresponding author: kacper.kapusniak@keble.ox.ac.uk
Preprint. Under review.
arXiv:2405.14780v1  [cs.LG]  23 May 2024

Figure 1: In orange and violet the source and target distributions. On the left, straight interpolations vs
interpolations following a data-dependent Riemannian metric. On the right, densities of reconstructed
marginals at time t = 1
2, using Conditional Flow Matching and METRIC FLOW MATCHING (MFM),
respectively. MFM provides a more meaningful reconstruction supported on the data manifold.
2000, Belkin and Niyogi, 2003], the target time-evolving density p∗
t is supported on a curved
low-dimensional manifold M ⊂Rd—a condition satisfied by cells in the space of gene expressions
[Moon et al., 2018]. As such, straight interpolants stray away from the data manifold M, leading
to reconstructions pt that fail to model the nonlinear dynamics generated by the underlying process.
Present work. We aim to design interpolants xt that stay on the data manifold M associated with the
underlying dynamics. Nonetheless, parameterizing the lower-dimensional manifold M is prone to
instabilities [Loaiza-Ganem et al., 2022] and may require multiple coordinate systems [Salmona et al.,
2022]. Accordingly, we adopt the “metric learning” approach [Xing et al., 2002, Hauberg et al., 2012],
where we still work in the ambient space Rd, but equip it with a data-dependent Riemannian metric
g whose shortest-paths (geodesics) stay close to the data points, and hence to M [Arvanitidis et al.,
2021]. We introduce METRIC FLOW MATCHING (MFM), a simulation-free generalization of CFM
where interpolants xt are learned by minimizing a geodesic loss that penalizes the velocity measured
by the metric g. As a result, xt approximates the geodesics of g and hence tends towards the data, lead-
ing to the evaluation of the matching objective in regions of lower uncertainty. Therefore, the resulting
probability path pt is supported on the data manifold for all t ∈[0, 1], giving rise to a more natural
reconstruction of the underlying dynamics in the trajectory inference task, as depicted in Figure 1.
Our main contributions are:
(1) We prove that given a dataset D ⊂Rd, one can always construct a metric g on Rd such that the
geodesics connecting x0 sampled from p0 to x1 sampled from p1, always lie close to D (§3.1).
(2) We propose METRIC FLOW MATCHING, a novel framework generalizing CFM to the Riemannian
manifold associated with any data-dependent metric g. In MFM, before training the matching
objective, we learn interpolants that stay close to the data by minimizing a cost induced by g
(§3). MFM is simulation-free and stays relevant when geodesics lack closed form and hence
Riemannian Flow Matching [Chen and Lipman, 2023] is not easily applicable (§3.2).
(3) We prescribe a universal way of designing a data-dependent metric, independent of the specifics
of the task, which enforces interpolants xt to stay supported on the data manifold (§4.1). Through
the proposed metric, xt depends on the entire data manifold and not just the endpoints x0 and x1
sampled from the marginals. By accounting for the Riemannian geometry induced by the data,
MFM generalizes existing approaches that construct xt by minimizing energies (§4.2).
(4) We propose OT-MFM, an instance of MFM that relies on Optimal Transport to draw samples from
the marginals (§4). Empirically, we show that OT-MFM attains SOTA results for reconstructing
single-cell dynamics (§5). Additionally, we validate the versatility of the framework through
tasks such as 3D navigation with LiDAR point clouds and unpaired translation of images.
2
Preliminaries and Setting
We review Conditional Flow Matching, which forms the basis of METRIC FLOW MATCHING §3.
Next, we recall basic notions of Riemannian geometry, with emphasis on constructing geodesics.
Notation and convention. We let Rd be the ambient space where data points are embedded. A random
variable x with a distribution p is denoted as x ∼p(x). A function φ depending on time t, space x and
learnable parameters θ, is denoted by φt,θ(x), and its time derivative by ˙φt,θ. We also let δx0(x) be the
Dirac function centered at x0 and assume that all distributions are absolutely continuous, which allows
us to use densities. We denote the space of symmetric, positive definite d × d matrices as SPD(d),
and let G(x) ∈SPD(d) be the coordinate representation of a Riemannian metric at some point x.
2

Conditional Flow Matching. We consider a source distribution p0 and a target distribution p1
defined on Rd. We are interested in finding a map f that pushes forward p0 to p1, i.e. f#p0 = p1.
In line with Continuous Normalizing Flows [Chen et al., 2018], we look for a map of the form
f = ψ1, where the time-dependent diffeomorphism ψt : Rd →Rd is the flow generated by a vector
field ut, i.e. dψt(x)/dt = ut(ψt(x)), with ψ0(x) = x, for all x ∈Rd. If we define the density path
pt = [ψt]#p0, then pt and ut satisfy the continuity equation and we say that pt is generated by ut.
If density path pt and vector field ut are known, we could regress a vector field vt,θ, modeled as
a neural network, to ut by minimizing LFM(θ) = Et,pt(x)∥vt,θ(x) −ut(x)∥2. Since pt and ut are
typically intractable though, Conditional Flow Matching (CFM) [Lipman et al., 2023, Albergo and
Vanden-Eijnden, 2022, Liu et al., 2022] simplifies the problem by assuming that pt is a mixture of
conditional paths: pt(x) =
R
pt(x|z)q(z)dz, where z = (x0, x1) is sampled from a joint distribution
q with marginals p0 and p1, and pt(x|z) satisfy p0(x|z) ≈δx0(x) and p1(x|z) ≈δx1(x). If
ψt(x|x0, x1) denotes the flow generating pt(x|z), then the CFM objective is
LCFM(θ) = Et,(x0,x1)∼q ∥vt,θ(xt) −˙xt∥2 ,
(1)
where xt := ψt(x0|x0, x1) are the interpolants from x0 to x1. Since LFM and LCFM have same gra-
dients [Lipman et al., 2023, Tong et al., 2023b], we can use the tractable conditional paths to learn vθ.
As in §3 we design xt to approximate geodesics, we review key notions from Riemannian geometry.
Riemannian manifolds. A Riemannian manifold (M, g) is a smooth orientable manifold M
equipped with a smooth map g assigning to each point x an inner product ⟨·, ·⟩g defined on the
tangent space of M at x. We let G(x) ∈SPD(d) be the matrix representing g in coordinates, at
any point x, with d the dimension of M. Integration is taken with respect to the volume form dvol
(see Appendix §A for details). A continuous, positive function p is then a probability density on
(M, g), i.e. p ∈P(M), if
R
p(x)dvol(x) = 1. Naturally, it is possible to define curves γt, indexed
by time t. A geodesic is then the curve γ∗
t that minimizes the distance with respect to g. Specifically,
the geodesic connecting x0 to x1 in M, can be found by minimizing the length functional:
γ∗
t =
arg min
γt: γ0=x0,γ1=x1
Z 1
0
∥˙γt∥g(γt) dt,
∥˙γt∥g(γt) :=
p
⟨˙γt, G(γt) ˙γt⟩,
(2)
where ˙γt is velocity. From eq. (2), we see that geodesics tend towards regions where ∥G(x)∥is small.
In Euclidean geometry (i.e., M = Rd and G(x) = I), γ∗
t is a straight line with constant speed.
3
Metric Flow Matching
We introduce METRIC FLOW MATCHING (MFM), a new simulation-free framework that generalizes
CFM by constructing probability paths supported on the data manifold. MFM learns interpolants
xt in eq. (1) whose velocity minimizes a data-dependent Riemannian metric assigning a lower cost
to regions with high data concentration. Consequently, the CFM objective is evaluated in areas of low
data uncertainty, and the corresponding vector field, vθ in eq. (1), learns to pass through these regions.
We structure this section as follows. In §3.1 we discuss the trajectory inference problem and how
straight interpolants xt in CFM result in undesirable probability paths whose support is not defined
on the data manifold. We remedy this problem by choosing to represent the data manifold via a
Riemannian metric in Rd, such that geodesics avoid straying away from the samples in the training set.
In §3.2 we introduce MFM and compare it with Riemannian Flow Matching [Chen and Lipman, 2023].
3.1
Metric learning
Assume that p0 and p1 are empirical distributions and that we have access to a dataset of samples
D = {xi}N
i=1. We are interested in the problem of trajectory inference [Lavenant et al., 2021], where
we need to reconstruct an unknown dynamics t 7→p∗
t , with observed time marginals p∗
0 = p0 and
p∗
1 = p1—the extension to multiple timepoints is easy. In many realistic settings, including single-cell
RNA sequencing [Macosko et al., 2015], time measurements are sparse, and leveraging biases from
the data is hence key to achieving a faithful reconstruction. For this reason, we invoke the “manifold
hypothesis” [Bengio et al., 2013], where the data arises from a low-dimensional manifold M ⊂Rd—
a property satisfied by cells embedded in the space of gene expressions [Moon et al., 2018]:
supp(p∗
t ) := {x ∈Rd : p∗
t (x) > 0} ⊂M,
t ≥0.
(3)
3

Since any regular time dynamics can be described using the continuity equation generated by some
vector field v∗
t [Ambrosio et al., 2005, Neklyudov et al., 2023a], we rely on CFM, and approximate
p∗
t via the probability path pt associated with vt,θ in eq. (1). From eq. (3), it follows that a valid
reconstruction entails supp(pt) ⊂M. As the support of pt is determined by the interpolants
xt in eq. (1), we need xt to be constrained to stay on M. However, in the classical CFM setup,
this condition is violated since interpolants are often straight lines, agnostic of the data’s support:
xt = tx1 +(1−t)x0 [Tong et al., 2023b, Shaul et al., 2023], with x0, x1 sampled from the marginals.
In this case, if pt(x|x0, x1) ≈δxt(x), then the support of pt satisfies
supp(pt) ⊂{y ∈Rd : ∃(x0, x1) ∼q : y = tx1 + (1 −t)x0}.
However, the dynamics t 7→p∗
t is often nonlinear, as for single-cells [Moon et al., 2018], meaning
that supp(pt) ̸⊆M. Straight interpolants are hence too restrictive and should instead be supported
on M so to replicate actual trajectories from x0 to x1, which are generated by the underlying process.
Operating on a lower-dimensional M is challenging though, since it requires different coordinate sys-
tems [Schonsheck et al., 2019, Salmona et al., 2022] and may incur overfitting [Loaiza-Ganem et al.,
2022, 2024]. Nonetheless, a key property posited by the “manifold hypothesis” is that M concentrates
around the data points D [Arvanitidis et al., 2022, Chadebec and Allassonnière, 2022]. As such,
interpolants xt should remain close to D. Therefore, instead of changing the dimension, we design
xt to minimize a cost in Rd that is lower on regions close to D. We achieve this following the “metric
learning” approach, [Hauberg et al., 2012] where we equip Rd with a suitable Riemannian metric g.
Definition 1. A data-dependent metric g on Rd is a smooth map g : Rd →SPD(d) parameterized
by the dataset D = {xi}N
i=1 ⊂Rd, i.e. g(x) = G(x; D) ∈SPD(d), ∀x ∈Rd.
We describe a specific metric g in §4, and note that g can also enforce constraints from the task (§8).
Naturally, if G(x; D) = I, we recover the Euclidean metric. We show that we can always construct g
so that the geodesics γ∗
t stay close to the data D. For details, we refer to Theorem B.1 in Appendix §B.
Proposition 1 (Informal). Given a dataset D ⊂Rd, let g be any metric such that: (i) The eigenvalues
of G(x; D) do not approach zero when x is distant from D; (ii) ∥G(x; D)∥is sufficiently small if x
is close to D. Then for each (x0, x1) ∼q, the geodesic γ∗
t connecting x0 to x1 stays close to D.
Given g as in the statement, if xt = γ∗
t and pt(x|x0, x1) ≈δxt(x), then the probability path pt
generated by vθ in eq. (1) has support near D, i.e. supp(pt) lies close to the data manifold M, which is
our goal. Unfortunately, for all but the most trivial metrics g on Rd, it is not possible to obtain closed-
form expressions for the geodesics γ∗
t . As such, finding the geodesic γ∗
t necessitates the expensive
simulation of second-order nonlinear Euler-Lagrange equations [Hennig and Hauberg, 2014].
3.2
Parameterization and optimization of interpolants
Consider a metric g on Rd as in Definition 1 whose geodesics γ∗
t lie close to D as per Proposition 1.
We propose a simulation-free approximation to paths γ∗
t by introducing interpolants of the form
xt,η = (1 −t)x0 + tx1 + t(1 −t)φt,η(x0, x1),
(4)
where η are the parameters of a neural network φt,η acting as a nonlinear “correction” for straight in-
terpolants. Note that the boundary conditions are met and that xt,η reduces to the convex combination
between x0 and x1 if φt,η = 0, meaning that xt,η strictly generalize the straight paths used in [Lipman
et al., 2023, Liu et al., 2022]. Towards the goal of learning η so that xt,η approximates the geodesic
γ∗
t , we note that γ∗
t can be characterized as the path minimizing the convex functional Eg below:
γ∗
t =
arg min
γt: γ0=x0, γ1=x1
Eg(γt),
Eg(γt) :=
Z 1
0
˙γ⊤
t G(γt; D)˙γt dt.
(5)
Since γ∗
t minimizes Eg over all paths connecting x0 to x1, and xt,η in eq. (4) satisfies these boundary
conditions, we can estimate η by simply minimizing the convex functional Eg over xt,η, which leads
to the following geodesic objective (the training procedure is reported in Algorithm 1):
Lg(η) := E(x0,x1)∼q [Eg(xt,η)] = E(x0,x1)∼q,t

( ˙xt,η)⊤G(xt,η; D) ˙xt,η

.
(6)
Given interpolants that approximate γ∗
t and hence stay close to the data manifold, we can then rely on
the CFM objective in eq. (1) to regress the vector field vθ. Since g makes the ambient space Rd into a
4

Algorithm 1 Pseudocode for training of geodesic interpolants
Require: coupling q, initialized network φt,η, data-dependent metric G(·; D)
1: while Training do
2:
Sample (x0, x1) ∼q and t ∼U(0, 1)
3:
xt,η ←(1 −t)x0 + tx1 + t(1 −t)φt,η(x0, x1)
▷eq. (4)
4:
˙xt,η ←x1 −x0 + t(1 −t) ˙φt,η(x0, x1) + (1 −2t)φt,η(x0, x1)
5:
ℓ(η) ←( ˙xt,η)⊤G(xt,η; D) ˙xt,η
▷Estimate of objective Lg(η) from eq. (6)
6:
Update η using gradient ∇ηℓ(η)
return (approximate) geodesic interpolants parametrized by φt,η
Riemannian manifold (Rd, g), we need to replace the norm ∥·∥in eq. (1) with the one ∥·∥g induced by
the metric, and rescale the marginals p0, p1 using the volume form induced by g, so to extend p0, p1 to
densities in P(Rd, g) (see Appendix §A). Similar arguments work for the joint distribution q. We can
finally introduce our framework METRIC FLOW MATCHING that generalizes Conditional Flow Match-
ing (1) to leverage any data-dependent metric g, by using interpolants xt,η (4), whose parameters η
are obtained from minimizing the geodesic cost Eg. The MFM objective can be stated as:
LMFM(θ) = Et,(x0,x1)∼q
h
∥vt,θ(xt,η∗) −˙xt,η∗∥2
g(xt,η∗)
i
,
η∗= arg min
η
Lg(η).
(7)
A description of METRIC FLOW MATCHING is given in Algorithm 2. As the interpolants xt,η
approximate geodesics of g, in MFM the vector field vt,θ is regressed on the data manifold M,
where the underlying dynamics p∗
t is supported, resulting in better reconstructions. Crucially, eq. (6)
only depends on time derivatives of xt,η. Therefore, MFM avoids simulations and simply requires
training an additional (smaller) network φt,η in eq. (4), which can be done prior to training vt,θ.
MFM versus Riemannian Flow Matching. While CFM has already been extended to Riemannian
manifolds in the Riemannian Flow Matching (RFM) framework of Chen and Lipman [2023], MFM
crucially differs from RFM in two ways. To begin with, MFM relies on the data or task inducing
a Riemannian metric on the ambient space which is then accounted for in the matching objective.
This is in sharp contrast to RFM, which instead assumes that the metric of the ambient space is given
and is independent of the data points. Secondly, RMF does not incorporate conditional paths that
are learned. In fact, in the scenario above where g is a metric whose geodesics γ∗
t stay close to the
data support, adopting RMF would entail replacing the MFM objective LMFM in (7) with
LRFM(θ) = Et,(x0,x1)∼q ∥vt,θ(γ∗
t ) −˙γ∗
t ∥2
g(γ∗
t ) .
(8)
However, as argued above, for almost any metric g on Rd, geodesics γ∗
t can only be found via simu-
lations, which in high dimensions inhibits the easy application of RFM. Conversely, MFM designs in-
terpolants that minimize a geodesic cost (6) and hence approximate γ∗
t without incurring simulations.
4
Learning Riemannian Metrics in Ambient Space
In this section, we focus on a concrete choice of g, which can easily be used within MFM (§4.1).
We also introduce OT-MFM, a variant of MFM that leverages Optimal Transport to find a coupling
q between p0 and p1. Next, in §4.2 we discuss how MFM generalizes recent works that find
interpolants that minimize energies by accounting for the Riemannian geometry induced by the data.
4.1
A family of diagonal metrics: LAND and RBF
We consider a family of metrics gLAND as in Definition 1, independent of specifics of the data type or
task. For the ease of exposition, we omit to write the explicit dependence on the dataset D = {xi}N
i=1.
Given ε > 0, we let x 7→gLAND(x) ≡Gε(x) = (diag(h(x)) + εI)−1 be the “LAND” metric, where
hα(x) =
N
X
i=1
(xα
i −xα)2 exp

−∥x −xi∥2
2σ2

,
1 ≤α ≤d,
(9)
with σ the kernel size. We emphasize that Gε(x) was introduced by Arvanitidis et al. [2016]—from
which we borrow the name—but its algorithmic use in MFM is fundamentally different. In line with
5

Proposition 1, we see that ∥Gε(x)∥is larger away from D, thus pushing geodesics (2) to stay close
to the data support, as desired. While gLAND is flexible and directly accounts for all the samples in D,
in high-dimension selecting σ in eq. (9) can be challenging. For these reasons, we follow Arvanitidis
et al. [2021] and introduce a variation of gLAND of the form GRBF(x) = (diag(˜h(x)) + εI)−1, where
˜hα(x) =
K
X
k=1
ωα,k(x) exp

−λα,k
2 ∥x −ˆxk∥2
,
1 ≤α ≤d,
(10)
with K the number of clusters with centers ˆxk and λα,k the bandwidth of cluster k for channel α (see
Appendix §C for details). In particular, hα is realized via a Radial Basis Function (RBF) network
[Que and Belkin, 2016], where ωα,k > 0 are learned to enforce the behavior hα(xi) ≈1 for each data
point xi so that the resulting metric gRBF assigns lower cost to regions close to the centers ˆxk. In our
experiments, we then rely on gLAND in low dimensions, and instead use gRBF in high dimensions. We
also note that all metrics considered are diagonal, which makes MFM more efficient. Explicitly, given
the interpolants in eq. (4), the geometric loss in eq. (6) with respect to gRBF can be written as:
LgRBF(η) = E(x0,x1)∼q[ EgRBF(xt,η)] = Et,(x0,x1)∼q
"
d
X
α=1
( ˙xt,η)2
α
˜hα(xt,η) + ε
#
.
(11)
We see that the loss acts as a geometric regularization, with the velocity ˙xt,η penalized more in regions
away from the support of the dataset D, i.e. when λα,k∥xt,η−ˆxk∥is large for all centers ˆxk in eq. (10).
OT-MFM. In eq. (7), samples x0, x1 follow a joint distribution q, with marginals p0 and p1. Since
we are interested in the problem of trajectory inference, with emphasis on single-cell applications
where the principle of least action holds [Schiebinger, 2021], we focus on a coupling q that minimizes
the distance in probability space between the source and target distributions. Namely, we consider
the case where q is the 2-Wasserstein optimal transport plan π∗from p0 to p1 [Villani et al., 2009]:
π∗= arg min
π∈Π
Z
Rd×Rd c2(x, y)dπ(x, y),
(12)
where Π are the probability measures on Rd×Rd with marginals p0 and p1 and c is any cost. While we
could choose c based on gRBF, we instead select c to be the L2 distance in Rd to avoid additional compu-
tations and so we can study the role played by xt,η even when q = π∗is agnostic of the data manifold.
We then propose the OT-MFM objective, where η∗minimizes the geodesic loss in eq. (11):
LOT-MFMRBF(θ) = Et,(x0,x1)∼π∗
h
∥vt,θ(xt,η∗) −˙xt,η∗∥2
gRBF(xt,η∗)
i
.
(13)
We note that the case of gLAND is dealt with similarly. Additionally, different choices of the joint distri-
bution q, beyond Optimal Transport, can be adapted from CFM [Tong et al., 2023b] to MFM in eq. (7).
4.2
Understanding MFM through energies
In MFM we learn interpolants xt that are optimal according to eq. (6). Previous works have studied
the “optimality” of interpolants, but have ignored the data manifold. Shaul et al. [2023] proposed
to learn interpolants xt that minimize the kinetic energy K( ˙xt) = Et,(x0,x1)∼q ∥˙xt∥2. However, K
assigns each point in space the same cost, independent of the data, and leads to straight interpolants
that may stray away from D. Conversely, our objective in eq. (6) can equivalently be written as
Lg(η) = E(x0,x1)∼q [ Eg(xt,η)] ≡Et,(x0,x1)∼q[ ∥˙xt,η∥2
g(xt,η)].
As a result, the geodesic loss Lg(η) ≡Kg( ˙xt,η) is precisely the kinetic energy of vector fields with
respect to g and hence accounts for the cost induced by the data. In fact, choosing G(x) = I, recovers
K. We note that the parameterization xt,η in eq. (4) is more expressive than xt = a(t)x1 + b(t)x0,
which is studied in Albergo and Vanden-Eijnden [2022], Shaul et al. [2023]. Besides, xt,η not only
depends on the endpoints x0, x1 but, implicitly, on all the data points D through metrics such as gRBF.
Data-dependent potentials. Energies more general than the kinetic one K have been considered
in Neklyudov et al. [2023a], Liu et al. [2024], Neklyudov et al. [2023b]. In particular, adapting
GSBM [Liu et al., 2024] from the stochastic Schrödinger bridge setting to CFM, entails designing
6

interpolants xt that minimize U(xt, ˙xt) = K( ˙xt) + Vt(xt), with Vt a potential enforcing additional
constraints. However, GSBM does not prescribe a general recipe for Vt and instead leaves to the
modeler the task of constructing Vt, based on applications. Conversely, MFM relies on a Riemannian
approach to propose an explicit objective, i.e. eq. (11), that holds irrespective of the task and can
be learned prior to regressing the vector field vt,θ. In fact, eq. (11) can be rewritten as
LgRBF(η) = Et,(x0,x1)∼q

∥˙xt,η∥2 + Vt,η(xt,η, x0, x1)

.
(14)
where Vt,η is parametric function depending on the boundary points x0, x1 (see Appendix §C.1 for an
expression). In contrast to GSBM, MFM designs interpolants xt,η that jointly minimize K and a poten-
tial Vt,η that is not fixed but also updated with the same parameters η to bend paths towards the data.
5
Experiments
We test METRIC FLOW MATCHING on different tasks: artificial dynamic reconstruction and naviga-
tion through LiDAR surfaces §5.1; unpaired translation between classes in images §5.2; reconstruction
of cell dynamics. Further results and experimental details can be found in Appendices §D, §E and §F.
The model. In all the experiments, we test the OT-MFM method detailed in §4.1. As argued in §4.1,
for high-dimensional data, we leverage the RBF metric (10) and hence train with the objective (13).
We refer to this model as OT-MFMRBF. Conversely, for low-dimensional data we rely on the LAND
metric (9), replacing gRBF with gLAND in eq. (11) and eq. (13). We denote this model as OT-MFMLAND.
Both metrics are task-independent and do not require further manipulation from the modeler.
Baselines. MFM generalizes CFM by learning interpolants that account for the geometry of the data.
As such, in §5.1 and §5.2, we focus on validating that OT-MFM leads to more meaningful matching
than its Euclidean counterpart OT-CFM [Tong et al., 2023b]. For single-cell experiments instead, we
also compare with a variety of baselines, including models specific to the single-cell domain [Tong
et al., 2020, Koshizuka and Sato, 2023], Schrödinger Bridge models [De Bortoli et al., 2021, Shi et al.,
2023, Tong et al., 2023a], and ODE-flow methods [Tong et al., 2023b, Neklyudov et al., 2023b].
5.1
Synthetic experiments and LiDAR
Our first goal is to validate that MFM and specifically the family of interpolants xt,η in eq. (4), are
expressive enough to model matching of distributions when the data induce a nonlinear geometry.
Table 1: Wasserstein distance be-
tween reconstructed marginal at
time 1/2 and ground-truth.
Method
EMD (↓)
OT-CFM
0.6081 ± 0.023
OT-MFMLAND
0.0813 ± 0.009
To begin with, we consider the Arch dataset in Tong et al.
[2020], where we have access to the underlying paths. In Table 1
we report the Wasserstein distance (EMD) between the interpo-
lation according to true dynamics and the marginal at time 1/2
obtained using the Euclidean baseline OT-CFM and our Rie-
mannian model OT-MFMLAND. We see that OT-MFMLAND is able
to faithfully reconstruct the dynamics by learning interpolants
that minimize the geodesic cost induced by the metric in eq. (9).
Conversely, straight interpolants fail to stay on the manifold generated by the dynamics (see Figure 2).
To further highlight the ability of MFM to generate meaningful matching, we also compare the inter-
polants of OT-CFM and OT-MFMLAND for navigations through surfaces scanned by LiDAR [Legg and
Anderson, 2013]. From Figure 2 we find that straight paths result in unnatural trajectories, whereas
OT-MFM manages to construct meaningful interpolations that better navigate the complex surface.
OT-CFM
OT-MFMLAND
OT-CFM
OT-MFMLAND
Figure 2: Interpolants for the Arch dataset (left) and over LiDAR scans of Mt. Rainier (right). In both cases,
learning interpolants that minimize the LAND metric (9) leads to more meaningful matchings.
7

Figure 3: Qualitative comparison for image translation. By designing inter-
polants on the data manifold, OT-MFMRBF better preserves input features.
Table 2: FID (↓) and LPIPS (↓)
values on AFHQ for OT-CFM
and OT-MFMRBF.
Method
FID
LPIPS
OT-CFM
41.42
0.512
OT-MFMRBF
37.87
0.502
While OT-MFM can be further enhanced using potentials specific to the task, similar to [Liu et al.,
2024], here we focus on its ability to provide meaningful matching even with a task-agnostic metric.
5.2
Unpaired translation in latent space
To test the advantages of MFM for more meaningful generation, we consider the task of unpaired im-
age translation between dogs and cats in AFHQ [Choi et al., 2020]. Specifically, we perform unpaired
translation in the latent space of the Stable Diffusion v1 VAE [Rombach et al., 2022]. Figure 3 reports
a qualitative comparison between OT-CFM and OT-MFMRBF. Additionally, a quantitative comparison
can be found in Table 2, where we measure both the quality of images via Fréchet Inception Distance
(FID) [Heusel et al., 2017] and the perceptual similarity of generated cats to source dogs via Learned
Perceptual Image Patch Similarity (LPIPS) [Zhang et al., 2018]. We see that OT-MFM not only im-
proves image generation but also better retains meaningful semantic input features thanks to smoother
interpolations on the data manifold. Our results using MFM further highlight the role played by the
nonlinear geometry associated with latent representations, a topic studied extensively §6.
5.3
Trajectory inference for single-cell data
We finally test MFM for reconstructing cell dynamics, a central problem in biomedical appli-
cations [Lähnemann et al., 2020], which holds great promise thanks to the advancements of
single-cell RNA sequencing (scRNA-seq) [Macosko et al., 2015, Klein et al., 2015]. Since in
scRNA-seq trajectories cannot be tracked, we only assume access to K unpaired distributions
Table 3: Wasserstein-1 distance averaged over left-
out marginals for 100-dim PCA single-cell data for
corresponding datasets. Results averaged over 5 runs.
Method
Cite (100D)
Multi (100D)
SF2 M-Geo
44.498 ± 0.416
52.203 ± 1.957
SF2 M-Exact
46.530 ± 0.426
52.888 ± 1.986
OT-CFM
45.393 ± 0.416
54.814 ± 5.858
I-CFM
48.276 ± 3.281
57.262 ± 3.855
WLF-OT
44.821 ± 0.126
55.416 ± 6.097
WLF-UOT
43.731 ± 1.375
54.222 ± 5.827
WLF-SB
46.131 ± 0.083
55.065 ± 5.499
OT-MFMRBF
41.784 ± 1.020
50.906 ± 4.627
describing cell populations at K time points. We
then apply the matching objective in eq. (7) be-
tween every consecutive time points, sharing pa-
rameters for both the vector field vt,θ and the in-
terpolants xt,η—see eq. (20) for how to extend
xt,η to multiple timepoints. Following the setup
of Schiebinger et al. [2019], Tong et al. [2020,
2023a] we perform leave-one-out interpolation,
where we measure the Wasserstein-1 distance be-
tween the k-th left-out density and the one recon-
structed after training on the remaining timepoints.
We compare OT-MFM and baselines over Embry-
oid body (EB) [Moon et al., 2019], and CITE-seq
(Cite) and Multiome (Multi) data from [Lance
et al., 2022]. In Table 4 and Table 3 we consider
the first 5 and 100 principal components of the data, respectively—results with 50 principal com-
ponents can be found in Appendix D. We observe that OT-MFM significantly improves upon its Eu-
clidean counterpart OT-CFM, which resonates with the manifold hypothesis for single-cell data [Moon
et al., 2018]. In fact, OT-MFM surpasses all baselines, including those that add biases such as stochas-
ticity (SF2 Tong et al. [2023a]) or mass teleportation (WLF-UOT Neklyudov et al. [2023b]). OT-MFM
instead relies on metrics such as LAND and RBF to favor interpolations that remain close to the data.
8

Table 4: Wasserstein-1 distance (↓) averaged over left-out marginals for 5-dim PCA representation of
single-cell data for corresponding datasets. Results are averaged over 5 independent runs.
Method
Cite
EB
Multi
Reg. CNF [Finlay et al., 2020]
—
0.825± 0.429
—
TrajectoryNet [Tong et al., 2020]
—
0.848
—
NLSB [Koshizuka and Sato, 2023]
—
0.970
—
DSBM [Shi et al., 2023]
1.705 ± 0.160
1.775 ± 0.429
1.873 ± 0.631
DSB [De Bortoli et al., 2021]
0.953 ± 0.140
0.862 ± 0.023
1.079 ± 0.117
SF2 M-Sink [Tong et al., 2023a]
1.054 ± 0.087
1.198 ± 0.342
1.098 ± 0.308
SF2 M-Geo [Tong et al., 2023a]
1.017 ± 0.104
0.879 ± 0.148
1.255 ± 0.179
SF2 M-Exact [Tong et al., 2023a]
0.920 ± 0.049
0.793 ± 0.066
0.933 ± 0.054
OT-CFM [Tong et al., 2023b]
0.882 ± 0.058
0.790 ± 0.068
0.937 ± 0.054
I-CFM [Tong et al., 2023b]
0.965 ± 0.111
0.872 ± 0.087
1.085 ± 0.099
SB-CFM [Tong et al., 2023b]
1.067 ± 0.107
1.221 ± 0.380
1.129 ± 0.363
WLF-UOT [Neklyudov et al., 2023b]
0.733 ± 0.063
0.738 ± 0.014
0.911 ± 0.147
WLF-SB [Neklyudov et al., 2023b]
0.797 ± 0.022
0.746 ± 0.016
0.950 ± 0.205
WLF-OT [Neklyudov et al., 2023b]
0.802 ± 0.029
0.742 ± 0.012
0.949 ± 0.211
OT-MFMLAND
0.724 ± 0.070
0.713 ± 0.039
0.890 ± 0.123
6
Related Work
Geometry-aware generative models. The manifold hypothesis [Bengio et al., 2013], has been stud-
ied in the context of manifold learning [Tenenbaum et al., 2000, Belkin and Niyogi, 2003] and metric
learning [Xing et al., 2002, Weinberger and Saul, 2009, Hauberg et al., 2012]. Recently, this has also
been analyzed in relation to generative models for obtaining meaningful interpolations [Arvanitidis
et al., 2017, 2021, Chadebec and Allassonnière, 2022], diagnosing model instability [Cornish et al.,
2020, Loaiza-Ganem et al., 2022, 2024] assessing the ability to perform dimensionality reduction
[Stanczuk et al., 2022, Pidstrigach, 2022] and for the improved learning of curved, low-dimensional
data manifolds [Dupont et al., 2019, Schonsheck et al., 2019, Horvat and Pfister, 2021, De Bortoli,
2022]. Closely related is the extension of generative models to settings where the ambient space itself
is a Riemannian manifold [Mathieu and Nickel, 2020, Lou et al., 2020, Falorsi, 2021, De Bortoli
et al., 2022, Huang et al., 2022, Rozen et al., 2021, Ben-Hamu et al., 2022, Chen and Lipman, 2023].
Trajectory inference. Reconstructing dynamics from cross-sectional distributions [Hashimoto et al.,
2016, Lavenant et al., 2021] is an important problem within the natural sciences, especially in the
context of single-cell analysis [Macosko et al., 2015, Moon et al., 2018, Schiebinger et al., 2019].
Recently, diffusion and Continuous Normalizing Flow (CNFs) based methods have been proposed
[Tong et al., 2020, Bunne et al., 2022, 2023, Huguet et al., 2022, Koshizuka and Sato, 2023] but
require simulations, whereas Tong et al. [2023a], Neklyudov et al. [2023a], Palma et al. [2023] allow
for simulation-free training. In particular, Huguet et al. [2022], Palma et al. [2023] regularize CNF in
a latent space to enforce that straight paths correspond to interpolations on the original data manifold.
7
Conclusions and Limitations
We have presented METRIC FLOW MATCHING, a simulation-free framework that generalizes
Conditional Flow Matching to design probability paths whose support lies on the data manifold.
In MFM, this is achieved via interpolants that minimize the geodesic cost of a data-dependent
Riemannian metric. We have empirically shown that instances of MFM using prescribed task-agnostic
metrics, surpass Euclidean baselines, with emphasis on single-cell dynamics reconstruction. While
the universality of the metrics proposed in §4 is a benefit, we have not investigated how to further
encode biases into the metric that are specific to the downstream task—a topic reserved for future
work. Additionally, the principle of learning interpolants that minimize a geodesic cost can also
be adapted to score-based generative models such as diffusion models, beyond CFM. Lastly, our
approach requires the data to be embedded in Euclidean space for the interpolants to be defined;
it is an interesting direction to explore how one can learn interpolants that minimize a data-dependent
metric even when the ambient space itself is not Euclidean.
9

8
Acknowledgements
KK is supported by the EPSRC Centre for Doctoral Training in Health Data Science (EP/S02428X/1).
PP and LZ are supported by the EPSRC CDT in Modern Statistics and Statistical Machine Learning
(EP/S023151/1) and acknowledge helpful discussions with Yee Whye Teh. AJB is partially supported
by an NSERC Post-doc fellowship and an EPSRC Turing AI World-Leading Research Fellowship.
FDG and MB are supported by an EPSRC Turing AI World-Leading Research Fellowship.
References
M. S. Albergo and E. Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv
preprint arXiv:2209.15571, 2022. (Cited on pages 1, 3, and 6)
L. Ambrosio, N. Gigli, and G. Savaré. Gradient flows: in metric spaces and in the space of probability
measures. Springer Science & Business Media, 2005. (Cited on page 4)
G. Arvanitidis, L. K. Hansen, and S. Hauberg. A locally adaptive normal distribution. Advances in
Neural Information Processing Systems, 29, 2016. (Cited on pages 5 and 19)
G. Arvanitidis, L. K. Hansen, and S. Hauberg. Latent space oddity: on the curvature of deep
generative models. arXiv preprint arXiv:1710.11379, 2017. (Cited on page 9)
G. Arvanitidis, S. Hauberg, and B. Schölkopf. Geometrically enriched latent spaces. In International
Conference on Artificial Intelligence and Statistics, pages 631–639. PMLR, 2021. (Cited on pages 2,
6, 9, and 19)
G. Arvanitidis, M. González-Duque, A. Pouplin, D. Kalatzis, and S. Hauberg. Pulling back infor-
mation geometry. In 25th International Conference on Artificial Intelligence and Statistics, 2022.
(Cited on page 4)
M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.
Neural computation, 15(6):1373–1396, 2003. (Cited on pages 2 and 9)
H. Ben-Hamu, S. Cohen, J. Bose, B. Amos, A. Grover, M. Nickel, R. T. Chen, and Y. Lipman.
Matching normalizing flows and probability paths on manifolds. arXiv preprint arXiv:2207.04711,
2022. (Cited on page 9)
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.
IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013. (Cited on
pages 3 and 9)
C. Bunne, L. Papaxanthos, A. Krause, and M. Cuturi. Proximal optimal transport modeling of
population dynamics. In International Conference on Artificial Intelligence and Statistics, pages
6511–6528. PMLR, 2022. (Cited on page 9)
C. Bunne, Y.-P. Hsieh, M. Cuturi, and A. Krause. The schrödinger bridge between gaussian measures
has a closed form. In International Conference on Artificial Intelligence and Statistics, pages
5802–5833. PMLR, 2023. (Cited on page 9)
C. Chadebec and S. Allassonnière. A geometric perspective on variational autoencoders. Advances
in Neural Information Processing Systems, 35:19618–19630, 2022. (Cited on pages 4 and 9)
R. T. Chen and Y. Lipman. Riemannian flow matching on general geometries. arXiv preprint
arXiv:2302.03660, 2023. (Cited on pages 2, 3, 5, 9, and 18)
R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations.
Advances in neural information processing systems, 31, 2018. (Cited on page 3)
Y. Choi, Y. Uh, J. Yoo, and J.-W. Ha. Stargan v2: Diverse image synthesis for multiple domains.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
8188–8197, 2020. (Cited on pages 8 and 20)
10

R. Cornish, A. Caterini, G. Deligiannidis, and A. Doucet. Relaxing bijectivity constraints with
continuously indexed normalising flows. In International conference on machine learning, pages
2133–2143. PMLR, 2020. (Cited on page 9)
V. De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. arXiv
preprint arXiv:2208.05314, 2022. (Cited on page 9)
V. De Bortoli, J. Thornton, J. Heng, and A. Doucet. Diffusion schrödinger bridge with applications
to score-based generative modeling. Advances in Neural Information Processing Systems, 34:
17695–17709, 2021. (Cited on pages 7 and 9)
V. De Bortoli, E. Mathieu, M. Hutchinson, J. Thornton, Y. W. Teh, and A. Doucet. Riemannian score-
based generative modelling. Advances in Neural Information Processing Systems, 35:2406–2422,
2022. (Cited on page 9)
P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural
information processing systems, 34:8780–8794, 2021. (Cited on page 20)
E. Dupont, A. Doucet, and Y. W. Teh. Augmented neural odes. Advances in neural information
processing systems, 32, 2019. (Cited on page 9)
L. Falorsi. Continuous normalizing flows on manifolds. arXiv preprint arXiv:2104.14959, 2021.
(Cited on page 9)
C. Finlay, J.-H. Jacobsen, L. Nurbekyan, and A. Oberman. How to train your neural ode: the world
of jacobian and kinetic regularization. In International conference on machine learning, pages
3154–3164. PMLR, 2020. (Cited on page 9)
T. Hashimoto, D. Gifford, and T. Jaakkola. Learning population-level diffusions with generative
rnns. In International Conference on Machine Learning, pages 2417–2426. PMLR, 2016. (Cited
on pages 1 and 9)
S. Hauberg, O. Freifeld, and M. Black. A geometric take on metric learning. Advances in Neural
Information Processing Systems, 25, 2012. (Cited on pages 2, 4, and 9)
J. A. Hay, L. Kennedy-Shaffer, S. Kanjilal, N. J. Lennon, S. B. Gabriel, M. Lipsitch, and M. J. Mina.
Estimating epidemiologic dynamics from cross-sectional viral load distributions. Science, 373
(6552):eabh0635, 2021. (Cited on page 1)
P. Hennig and S. Hauberg. Probabilistic solutions to differential equations and their application to
riemannian statistics. In Artificial Intelligence and Statistics, pages 347–355. PMLR, 2014. (Cited
on page 4)
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. Advances in neural information
processing systems, 30, 2017. (Cited on pages 8 and 21)
C. Horvat and J.-P. Pfister. Denoising normalizing flow. Advances in Neural Information Processing
Systems, 34:9099–9111, 2021. (Cited on page 9)
C.-W. Huang, M. Aghajohari, J. Bose, P. Panangaden, and A. C. Courville. Riemannian diffusion
models. Advances in Neural Information Processing Systems, 35:2750–2761, 2022. (Cited on
page 9)
G. Huguet, D. S. Magruder, A. Tong, O. Fasina, M. Kuchroo, G. Wolf, and S. Krishnaswamy.
Manifold interpolating optimal-transport flows for trajectory inference.
Advances in neural
information processing systems, 35:29705–29718, 2022. (Cited on page 9)
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. (Cited on pages 20 and 21)
D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014. (Cited on page 20)
11

A. M. Klein, L. Mazutis, I. Akartuna, N. Tallapragada, A. Veres, V. Li, L. Peshkin, D. A. Weitz, and
M. W. Kirschner. Droplet barcoding for single-cell transcriptomics applied to embryonic stem
cells. Cell, 161(5):1187–1201, 2015. (Cited on pages 1 and 8)
T. Koshizuka and I. Sato. Neural lagrangian schrödinger bridge: Diffusion modeling for population
dynamics. In International Conference on Learning Representations, 2023. (Cited on pages 7 and 9)
D. Lähnemann, J. Köster, E. Szczurek, D. J. McCarthy, S. C. Hicks, M. D. Robinson, C. A. Vallejos,
K. R. Campbell, N. Beerenwinkel, A. Mahfouz, et al. Eleven grand challenges in single-cell data
science. Genome biology, 21:1–35, 2020. (Cited on page 8)
C. Lance, M. D. Luecken, D. B. Burkhardt, R. Cannoodt, P. Rautenstrauch, A. Laddach, A. Ub-
ingazhibov, Z.-J. Cao, K. Deng, S. Khan, et al. Multimodal single cell data integration challenge:
results and lessons learned. BioRxiv, pages 2022–04, 2022. (Cited on pages 8 and 21)
H. Lavenant, S. Zhang, Y.-H. Kim, and G. Schiebinger. Towards a mathematical theory of trajectory
inference. arXiv preprint arXiv:2102.09204, 2021. (Cited on pages 1, 3, and 9)
J. M. Lee. Smooth manifolds. Springer, 2012. (Cited on page 15)
N. Legg and S. Anderson. Southwest flank of Mt.Rainier, wa, 2013. URL https://doi.org/10.
5069/G9PZ56R1.Accessed:2024-05-21. (Cited on pages 7 and 20)
C. Léonard. A survey of the schr\" odinger problem and some of its connections with optimal
transport. arXiv preprint arXiv:1308.0215, 2013. (Cited on page 1)
Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative
modeling. International Conference on Learning Representations (ICLR), 2023. (Cited on pages 1,
3, and 4)
G.-H. Liu, Y. Lipman, M. Nickel, B. Karrer, E. Theodorou, and R. T. Chen. Generalized schrödinger
bridge matching. In ICLR, 2024. (Cited on pages 6, 8, 19, and 20)
X. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data with
rectified flow. arXiv preprint arXiv:2209.03003, 2022. (Cited on pages 1, 3, and 4)
G. Loaiza-Ganem, B. L. Ross, J. C. Cresswell, and A. L. Caterini. Diagnosing and fixing manifold
overfitting in deep generative models. arXiv preprint arXiv:2204.07172, 2022. (Cited on pages 2, 4,
and 9)
G. Loaiza-Ganem, B. L. Ross, R. Hosseinzadeh, A. L. Caterini, and J. C. Cresswell. Deep generative
models through the lens of the manifold hypothesis: A survey and new connections. arXiv preprint
arXiv:2404.02954, 2024. (Cited on pages 4 and 9)
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,
2017. (Cited on pages 20 and 21)
A. Lou, D. Lim, I. Katsman, L. Huang, Q. Jiang, S. N. Lim, and C. M. De Sa. Neural manifold
ordinary differential equations. Advances in Neural Information Processing Systems, 33:17548–
17558, 2020. (Cited on page 9)
E. Z. Macosko, A. Basu, R. Satija, J. Nemesh, K. Shekhar, M. Goldman, I. Tirosh, A. R. Bialas,
N. Kamitaki, E. M. Martersteck, et al. Highly parallel genome-wide expression profiling of
individual cells using nanoliter droplets. Cell, 161(5):1202–1214, 2015. (Cited on pages 1, 3, 8,
and 9)
E. Mathieu and M. Nickel. Riemannian continuous normalizing flows. Advances in Neural Informa-
tion Processing Systems, 33:2503–2515, 2020. (Cited on page 9)
M. G. Monera, A. Montesinos-Amilibia, and E. Sanabria-Codesal. The taylor expansion of the
exponential map and geometric applications. Revista de la Real Academia de Ciencias Exactas,
Fisicas y Naturales. Serie A. Matematicas, 108:881–906, 2014. (Cited on page 18)
12

K. R. Moon, J. S. Stanley III, D. Burkhardt, D. van Dijk, G. Wolf, and S. Krishnaswamy. Manifold
learning-based methods for analyzing single-cell rna-sequencing data. Current Opinion in Systems
Biology, 7:36–46, 2018. (Cited on pages 2, 3, 4, 8, and 9)
K. R. Moon, D. Van Dijk, Z. Wang, S. Gigante, D. B. Burkhardt, W. S. Chen, K. Yim, A. v. d.
Elzen, M. J. Hirn, R. R. Coifman, et al. Visualizing structure and transitions in high-dimensional
biological data. Nature biotechnology, 37(12):1482–1492, 2019. (Cited on pages 8 and 21)
K. Neklyudov, R. Brekelmans, D. Severo, and A. Makhzani. Action matching: Learning stochastic
dynamics from samples. In International Conference on Machine Learning, pages 25858–25889.
PMLR, 2023a. (Cited on pages 4, 6, and 9)
K. Neklyudov, R. Brekelmans, A. Tong, L. Atanackovic, Q. Liu, and A. Makhzani. A computational
framework for solving wasserstein lagrangian flows. arXiv preprint arXiv:2310.10649, 2023b.
(Cited on pages 6, 7, 8, 9, 21, and 22)
J. Oeppen and J. W. Vaupel. Broken limits to life expectancy, 2002. (Cited on page 1)
A. Palma, S. Rybakov, L. Hetzel, and F. J. Theis. Modelling single-cell rna-seq trajectories on a flat
statistical manifold. In NeurIPS 2023 AI for Science Workshop, 2023. (Cited on page 9)
J. Pidstrigach. Score-based generative models detect manifolds. Advances in Neural Information
Processing Systems, 35:35852–35865, 2022. (Cited on page 9)
Q. Que and M. Belkin. Back to the future: Radial basis function networks revisited. In Artificial
intelligence and statistics, pages 1375–1383. PMLR, 2016. (Cited on page 6)
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis
with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 10684–10695, 2022. (Cited on pages 8, 20, and 21)
N. Rozen, A. Grover, M. Nickel, and Y. Lipman. Moser flow: Divergence-based generative modeling
on manifolds. Advances in Neural Information Processing Systems, 34:17669–17680, 2021. (Cited
on page 9)
A. Salmona, V. De Bortoli, J. Delon, and A. Desolneux. Can push-forward generative models fit
multimodal distributions? Advances in Neural Information Processing Systems, 35:10766–10779,
2022. (Cited on pages 2 and 4)
G. Schiebinger. Reconstructing developmental landscapes and trajectories from single-cell data.
Current Opinion in Systems Biology, 27:100351, 2021. (Cited on page 6)
G. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V. Subramanian, A. Solomon, J. Gould, S. Liu, S. Lin,
P. Berube, et al. Optimal-transport analysis of single-cell gene expression identifies developmental
trajectories in reprogramming. Cell, 176(4):928–943, 2019. (Cited on pages 8 and 9)
S. Schonsheck, J. Chen, and R. Lai. Chart auto-encoders for manifold structured data. arXiv preprint
arXiv:1912.10094, 2019. (Cited on pages 4 and 9)
E. Schrödinger. Sur la théorie relativiste de l’électron et l’interprétation de la mécanique quantique.
In Annales de l’institut Henri Poincaré, volume 2, pages 269–310, 1932. (Cited on page 1)
N. Shaul, R. T. Chen, M. Nickel, M. Le, and Y. Lipman. On kinetic optimal probability paths for
generative models. In International Conference on Machine Learning, pages 30883–30907. PMLR,
2023. (Cited on pages 4 and 6)
Y. Shi, V. De Bortoli, A. Campbell, and A. Doucet. Diffusion schrödinger bridge matching. In
Thirty-seventh Conference on Neural Information Processing Systems, 2023. (Cited on pages 7 and 9)
J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. International Conference on Machine Learning (ICML), 2015.
(Cited on page 1)
13

Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based gener-
ative modeling through stochastic differential equations. International Conference on Learning
Representations (ICLR), 2021. (Cited on page 1)
J. Stanczuk, G. Batzolis, T. Deveney, and C.-B. Schönlieb. Your diffusion model secretly knows the
dimension of the data manifold. arXiv preprint arXiv:2212.12611, 2022. (Cited on page 9)
J. B. Tenenbaum, V. d. Silva, and J. C. Langford. A global geometric framework for nonlinear
dimensionality reduction. science, 290(5500):2319–2323, 2000. (Cited on pages 1 and 9)
A. Tong, J. Huang, G. Wolf, D. Van Dijk, and S. Krishnaswamy. Trajectorynet: A dynamic optimal
transport network for modeling cellular dynamics. In International conference on machine learning,
pages 9526–9536. PMLR, 2020. (Cited on pages 7, 8, 9, 20, and 21)
A. Tong, N. Malkin, K. Fatras, L. Atanackovic, Y. Zhang, G. Huguet, G. Wolf, and Y. Bengio.
Simulation-free schrödinger bridges via score and flow matching, 2023a. (Cited on pages 7, 8, 9,
and 21)
A. Tong, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, K. Fatras, G. Wolf, and Y. Bengio.
Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv
preprint arXiv:2302.00482, 2023b. (Cited on pages 1, 3, 4, 6, 7, 9, and 21)
C. Villani et al. Optimal transport: old and new, volume 338. Springer, 2009. (Cited on page 6)
K. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor
classification. Journal of machine learning research, 10(2), 2009. (Cited on page 9)
E. Xing, M. Jordan, S. J. Russell, and A. Ng. Distance metric learning with application to clustering
with side-information. Advances in neural information processing systems, 15, 2002. (Cited on
pages 2 and 9)
R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 586–595, 2018. (Cited on pages 8 and 21)
14

Outline of Appendix
In Appendix A we give a brief overview of relevant notions from differential and Riemannian geom-
etry. Appendix B provides more details for Section 3 including the formal statement and proof of
Proposition 1. We also derive how to rigorously extend flow matching to the Riemannian manifold
induced by a data-dependent metric. Finally, we report a pseudocode for MFM in Algorithm 2.
Appendix C provides more details regarding the data-dependent Riemannian metrics we use, and
relevant training procedures. In Appendix D we supply more details regarding the various experi-
ments. Finally, Appendix E and Appendix F contain supplementary figures and tables for single-cell
reconstruction and unpaired translation tasks, respectively.
A
Primer on Riemannian Geometry
Riemannian geometry is the study of smooth manifolds M equipped with a (Riemannian) metric
g. Intuitively, this corresponds to spaces that can be considered locally Euclidean, and which allow
for a consistent notion of measuring distances, angles, curvature, shortest paths, etc. on abstract
geometric objects. We provide a primer on the relevant concepts of Riemannian geometry for our
work, covering smooth manifolds and their tangent spaces, Riemannian metrics and geodesics, and
integration on Riemannian manifolds. For a comprehensive introduction, see Lee [2012].
Smooth manifolds. Formally, we say a topological space2 M is a d-dimensional smooth manifold
if we have a collection of charts (Ui, φi) where Ui are open subsets of M and ∪iUi = M,
φi : Ui →Rd are homeomorphisms onto their image, and when Ui ∩Uj ̸= ∅, the transition maps
φj ◦φ−1
i
: φi(Ui ∩Uj) →φj(Ui ∩Uj) are diffeomorphisms. These charts allow us to represent
quantities on M through the local coordinates obtained by mapping back to Euclidean space via
φi, as well as allowing us to define smooth maps between manifolds.
In particular, we define a smooth path in M passing through x ∈M as a function γ : (−ϵ, ϵ) →M,
for some ϵ > 0 and γ(0) = x, such that the local coordinate representation φi ◦γ of γ is smooth
in the standard sense (for any suitable chart (Ui, φi)). The derivatives ˙γ(0) of smooth paths γ
passing through x ∈M form a d-dimensional vector space called the tangent space TxM at
x. We can represent TxM in local coordinates with φi by the identification of ˙γ(0) ∈TxM to
(a′
1(0), . . . , a′
d(0))⊤∈Rd where (a1(t), . . . , ad(t))⊤= φi ◦γ(t).
Riemannian structure. To introduce geometric notions of length and distances to smooth manifolds,
we define a Riemannian metric3 g on M as a map providing a smooth assignment of points x ∈M
on the manifold to a positive definite inner product ⟨·, ·⟩g(x) defined on the corresponding tangent
space TxM. In local coordinates about x ∈M given by a suitable chart (Ui, φi), we have the local
representation ⟨v, w⟩g(x) = v⊤G(x)w where G(x) ∈Rd×d is a positive definite matrix (which
implicitly depends on the choice of chart). We call the pair (M, g) a Riemannian manifold.
Through the Riemannian metric, we can now define the norm of tangent vectors by ∥v∥g(x) =
⟨v, v⟩1/2
g(x) for v ∈TxM, as well as the length of a smooth path γ : [0, 1] →M by
Len(γ) =
Z 1
0
∥˙γ(t)∥g(γ(t))dt.
(15)
We can then define a geodesic γ∗between x0 and x1 in M as the shortest possible path between the
two points - i.e.
γ∗= arg min
γ
Z 1
0
∥˙γ(t)∥g(γ(t)) dt,
γ(0) = x0, γ(1) = x1.
(16)
We assume that all Riemannian manifolds being considered are (geodesically) complete, meaning
that geodesics can be extended indefinitely. In particular, for any pair of points x0, x1, there exists a
unique geodesic γ∗
t starting at x0 and ending at x1.
2With the technical assumptions that M is second-countable and Hausdorff.
3Formally, this is a choice of a smooth symmetric covariant 2-tensor field on M which is positive definite at
each point.
15

Integration on Riemannian manifolds. To introduce integration on Riemannian manifolds, we use
the fact that under the technical assumption that M is orientable, the Riemannian manifold (M, g)
has a canonical volume form dvol. This can be used to define a measure on M, where in local
coordinates, we have that dvol(x) =
p
|G(x)|dx where dx denotes the Lebesgue measure in Rd
and | · | denotes the determinant. Hence, for a chart (Ui, φi) and a continuous function f : M →R
which is compactly supported in Ui, we define the integral
Z
M
fdvol =
Z
φi(Ui)
f ◦φ−1
i (x)
p
|G(x)|dx.
(17)
This definition can be extended to more general functions through the use of partitions of unity and
the Riesz–Markov–Kakutani representation theorem.
The Riemannian geometry of Metric Flow Matching. Finally, we note that in our work, we take
our smooth manifold to be M = Rd with the trivial chart Ui = Rd, φi = id. This means we can
work in the usual Euclidean coordinates instead of requiring charts and local coordinates, simplifying
our framework. For example, we can define g through G directly in Definition 1 without the need
to check consistency across the choice of local coordinates, and we have the trivial identification
of TxM to Rd. In addition, for a continuous f : M →R, we have the simplified definition of the
Riemannian integral (when the integral exists) as
Z
M
fdvol =
Z
Rd f(x)
p
|G(x)|dx.
(18)
We use this to define probability densities on (M, g) to extend CFM to our setting in §B.1.
B
Additional Details for Section 3
To begin with, we provide a formal statement covering Proposition 1 and report a proof below. We
introduce some notation. We write γt ⊂U when the image set of γ : [0, 1] →Rd is contained in U,
i.e. γt ∈U for each t ∈[0, 1]. Moreover, we let
Br(D) := {y ∈Rd : ∃xi ∈D s.t. ∥y −xi∥≤r},
denote the set of points in Rd whose distance from the dataset D is at most r > 0.
Theorem B.1 (Formal statement of Proposition 1). Consider a closed dataset D ⊂Rd (e.g. finite).
Assume that for each (x0, x1) ∼q, there exists at least a path γt connecting x0 to x1 whose
length is at most Γ and such that γt ⊂Bδ(D), with δ > 0. Let κ > 0, ρ > δ and g be any
data-dependent metric satisfying: (i) v⊤G(x; D)v ≥κ∥v∥2 for each x ∈Rd \ Bρ(D) and v ∈Rd;
(ii) ∥G(x; D)∥≤κ(ρ/Γ)2 for each x ∈Bδ(D). Then for any (x0, x1) ∼q, the geodesic γ∗
t of g
connecting x0 to x1 satisfies γ∗
t ⊂B2ρ(D).
Proof of Theorem B.1. We argue by contradiction and assume that there exist x ∈Rd \ B2ρ(D) and
(x0, x1) ∼q such that the geodesic γ∗
t of g connecting x0 to x1 passes through x, i.e. there is a time
t0 ∈(0, 1) such that γ∗
t0 = x.
By assumption, there exists a path γt that connects x0 to x1 and stays within Bδ(D), which means that
x0 = γ0 = γ∗
0 ∈Bδ(D). Since D is closed, the function x 7→dE(x, D) := infy∈D ∥x −y∥, i.e. the
Euclidean distance of x from the dataset D, is continuous. In particular, t 7→dE,D(t) := dE(γ∗
t , D)
is also continuous, due to the geodesic being a smooth function in the interval [0, 1]. From the
continuity of dE,D and the fact that dE,D(0) ≤δ and dE,D(t0) > 2ρ, it follows that there must be a
time 0 < t′ < t0 such that dE,D(t) ≥ρ for all t ∈(t′, t0] and dE,D(t′) = ρ.
If we unpack the definition of dE,D, we have just shown that γ∗
t ∈Rd \ Bρ(D) for all t ∈(t′, t0].
Accordingly, we can estimate the length of γ∗
t with respect to the Riemannian metric g as
Leng(γ∗
t ) =
Z 1
0
∥˙γ∗
t ∥g(γ∗
t ) dt >
Z t0
t′
∥˙γ∗
t ∥g(γ∗
t ) dt
=
Z t0
t′
q
(˙γ∗
t )⊤G(γ∗
t ; D)˙γ∗
t dt ≥√κ
Z t0
t′
∥˙γ∗
t ∥dt ≥√κ∥γ∗
t′ −γ∗
t0∥,
16

where in the second-to-last inequality we have used the assumption (i) on the metric having minimal
eigenvalue √κ in Rd \ Bρ(D), while the final inequality simply follows from the Euclidean length of
any curve between the points γ∗
t′ and γ∗
t0 being larger than their Euclidean distance.
We claim that ∥γ∗
t′ −γ∗
t0∥≥ρ. To validate the latter point, take ϵ > 0. It follows that there exists
xi ∈D such that ∥xi −γ∗
t′∥≤ρ + ϵ, because dE,D(t′) = ρ, i.e. γ∗
t′ ∈Bρ(D). If ∥γ∗
t′ −γ∗
t0∥< ρ,
then we could apply the triangle inequality and derive that
∥γ∗
t0 −xi∥≤∥γ∗
t0 −γ∗
t′∥+ ∥γ∗
t′ −xi∥< ρ + ρ + ϵ = 2ρ + ϵ.
Since ϵ > 0 was arbitrary, it would follow that γ∗
t0 ≡x ∈B2ρ(D), which is a contradiction to our
starting point. Therefore, ∥γ∗
t′ −γ∗
t0∥≥ρ, and hence
Leng(γ∗
t ) > √κρ.
(19)
On the other hand, the assumptions guarantee the existence of another path γt connecting x0 to x1
whose image is always contained in Bδ(D). It follows that
Leng(γt) =
Z 1
0
∥˙γt∥g(γt) dt =
Z 1
0
q
(˙γ∗
t )⊤G(γ∗
t ; D)˙γ∗
t dt
≤√κ ρ
Γ
Z 1
0
∥˙γt∥dt = √κ ρ
ΓΓ = √κρ,
where we have used the assumption (ii) on g and that the length of γt is at most Γ. We can then
combine the last inequality and eq. (19), and conclude that
Leng(γt) ≤√κρ < Leng(γ∗
t ),
which means that we have found a path connecting x0 to x1, whose length with respect to the metric
g is shorter than the one of the geodesic γ∗
t from x0 to x1. This is a contradiction and concludes the
proof.
B.1
Extending CFM to the manifold (Rd, g)
In this subsection, we demonstrate how to generalize the CFM objective in eq. (1) to the Riemannian
manifold (Rd, g), defined by a data-dependent metric g as per Definition 1.
Densities on the manifold. To begin with, we extend the densities p0, p1 and the joint density q to
valid densities on the manifold (Rd, g). First, we recall that the Riemannian volume form induced by
g can be written in coordinates as
dvol(x) =
p
|G(x; D)|dx,
where | · | denotes the determinant of the matrix G(x; D), while dx is the standard Lebesgue measure
on Rd. Accordingly, given a density p ∈P(Rd), we can derive the associated density ˆp ∈P(Rd, g)
by rescaling:
ˆp(x) :=
p(x)
p
|G(x; D)|
,
which guarantees that ˆp is continuous, positive, and
Z
Rd ˆp(x)dvol(x) =
Z
Rd p(x)dx = 1.
This in particular applies to the marginals p0, p1 given by our problem. A similar argument applies to
the joint density q whose marginals are p0 and p1, respectively. In fact, we can now define a density
ˆq on the product manifold, i.e. ˆq ∈P(Rd × Rd, g × g), by simply taking
ˆq(x0, x1) :=
q(x0, x1)
p
|G(x0; D) · G(x1; D)|
,
17

where the denominator is exactly the pointwise volume form induced by the product metric g × g.
Therefore, the MFM objective in (7) is interpreted as
LMFM(θ) = Et,(x0,x1)∼ˆq
h
∥vt,θ(xt,η∗) −˙xt,η∗∥2
g(xt,η∗)
i
= Et
Z
Rd×Rd ∥vt,θ(xt,η∗) −˙xt,η∗∥2
g(xt,η∗)ˆq(x0, x1)
p
|G(x0; D) · G(x1; D)|dx0dx1
= Et
Z
Rd×Rd ∥vt,θ(xt,η∗) −˙xt,η∗∥2
g(xt,η∗)q(x0, x1)dx0dx1
= Et,(x0,x1)∼q
h
∥vt,θ(xt,η∗) −˙xt,η∗∥2
g(xt,η∗)
i
,
which is exactly eq. (7). We highlight that for this reason, we slightly abuse notation, and write
an expectation with respect to ˆq, regarded as a density on the manifold (Rd, g), the same as an
expectation over the density q with respect to Lebesgue measure.
The matching objective. We also emphasize that, similarly to Riemannian Flow Matching [Chen
and Lipman, 2023], we normalize the regression objective by G−1/2(x; D) to avoid the need for
initialization schemes that are specific to the metric. In fact, introducing a high-dimensional, non-
trivial norm ∥· ∥g in the CFM objective could introduce instabilities in the optimization of CFM.
As such, the objective in LMFM effectively reduces to LCFM with interpolants xt,η∗minimizing the
geodesic loss Lg. Equivalently, METRIC FLOW MATCHING consists of a 2-step procedure: we first
learn interpolants that approximate the geodesics of a data-dependent metric (Algorithm 1), and then
we regress the vector field vθ using the interpolants from the first stage as per the standard CFM
objective. A pseudocode for the MFM-pipeline is reported in Algorithm 2.
Inference. We finally note that, in principle, the differential equation generated by vθ is defined
over the tangent bundle of (Rd, g) and hence solving it through Euler discretization, would require
adopting the exponential map associated with the metric g to project the tangent vector to the manifold.
However, since the underlying space is still Rd, the Euclidean Euler-step integration provides a first-
order approximation of the Riemannian exponential associated with g (see for example Monera et al.
[2014]). Accordingly, at inference, we can simply rely on the canonical Euler-discrete integration to
approximate the exponential map associated with the data-dependent metric g, provided that the step
size is small.
Algorithm 2 Pseudocode for METRIC FLOW MATCHING
Require: coupling q, initialized network vt,θ, trained network φt,η, data-dependent metric g(·)
1: while Training do
2:
Sample (x0, x1) ∼q and t ∼U(0, 1)
3:
xt,η ←(1 −t)x0 + tx1 + t(1 −t)φt,η(x0, x1)
▷eq. (4)
4:
˙xt,η ←x1 −x0 + t(1 −t) ˙φt,η(x0, x1) + (1 −2t)φt,η(x0, x1)
5:
ℓ(θ) ←∥vt,θ(xt,η) −˙xt,η∥2
g(xt,η)
▷Estimate of objective LMFM(θ) from eq. (7)
6:
Update θ using gradient ∇θℓ(θ)
return vector field vt,θ
B.1.1
Support of pt
We conclude this section by justifying our claims about the relation of the support of the proba-
bility path pt and the interpolants. Consider a family of conditional paths pt(x|x0, x1) such that
pt(x|x0, x1) ≈δ(x−xt), where xt are the interpolants connecting x0 to x1, and δ is the Dirac distribu-
tion. Assume that xt are straight interpolants, i.e. for any (x0, x1) ∼q, we define xt = tx1+(1−t)x0.
Let us now consider y ∈Rd such that there is no (x0, x1) ∼q : y = tx1 + (1 −t)x0. Accordingly:
pt(y) :=
Z
Rd×Rd pt(y|x0, x1)q(x0, x1)dx0dx1 =
Z
Rd×Rd δ(y −xt)q(x0, x1)dx0dx1 = 0,
since there is no (x0, x1) in the support of q, such that y = xt. We have then shown that
supp(pt) ⊂{y ∈Rd : ∃(x0, x1) ∼q : y = tx1 + (1 −t)x0},
which can be limiting for tasks where the data induce a nonlinear geometry, as validated in Section 5.
18

C
Additional Details on the Riemannian Metrics used
In this Section, we provide further details on the diagonal metrics introduced in Section 4, which we
adopt in METRIC FLOW MATCHING. In particular, we comment on important differences between
the LAND metric and the RBF metric. We finally derive an explicit connection between MFM and
recent methods that learn interpolant minimizing generalized energies.
Learning the RBF metric gRBF. For the RBF metric in (10), we follow the metric design of
Arvanitidis et al. [2021] and find the centroids by performing k-means clustering. Similarly, we
define the bandwidth λk associated with cluster Ck as:
λk = 1
2
 
κ
|Ck|
X
x∈Ck
∥x −ˆxk∥2
!−2
,
where κ is a tunable hyperparameter. We note that the bandwidth is chosen to assign smaller decay
in (10) to the centroids of clusters that have high spread to better enable attraction of trajectories.
Finally, the weights ωα,k are determined by training the loss function,
LRBF({ωα,k}) =
X
xi∈D

1 −˜hα(xi)
2
=
X
xi∈D

1 −
K
X
k=1
ωα,k exp

−λα,k
2 ∥xi −ˆxk∥22
.
Two important comments are in order. First, by learning the RBF metric, the framework MFM
effectively entails jointly learning a data-dependent Riemannian metric and a suitable matching
objective along approximate geodesics xt,η. Second, we note that more general training objectives
can be adopted for ˜h, to enforce specific properties for the metric that are task-aware. We reserve the
exploration of this topic for future work.
LAND vs RBF metrics. While similar in spirit, since they both assign a lower cost to regions of space
close to the support of the data points D, the LAND metric [Arvanitidis et al., 2016] and the RBF
metric [Arvanitidis et al., 2021] differ in two fundamental aspects. Crucially, RBF is learned based
on the data points, requiring less tuning than LAND, which is a key advantage in high-dimensions
and the main motivation for why we resort to RBF for experiments on images and single-cell data
with more than 50 principal components. Additionally, the RBF metric assigns similar cost to regions
with data and is, in principle, more robust than the LAND metric to variations in the concentration
of samples in D. While this is neither a benefit nor a downside in general, we observe that if a
metric such as LAND consistently assigns much lower cost to regions of space with higher data
concentration, then the geodesic objective in eq. (6) could always bias to learn interpolants xt,η
moving through these regions independent of the starting point—this is a consequence of the fact that
we never compute the actual length of the paths to avoid simulations. Note though that this has not
been observed as an issue in practice whenever we adopted the LAND metric for low-dimensional
data.
C.1
Connection between Riemmanian approach and data potentials
We detail how the geometric loss (6) used to learn interpolants xt,η can be recast as a generalization
of the GSBM framework in Liu et al. [2024]. In general, for any data-dependent metric g, we can
indeed rewrite Lg(η) in eq. (6) as:
Lg(η) = Et,(x0,x1)∼q[⟨˙xt,η, G(xt,η; D) ˙xt,η⟩]
= Et,(x0,x1)∼q

∥˙xt,η∥2 + ⟨˙xt,η, (G(xt,η; D) −I) ˙xt,η⟩

= Et,(x0,x1)∼q

∥˙xt,η∥2 + Vt,η(xt,η, x0, x1)

,
where the parametric potential Vt,η has the form
Vt,η(xt,η, x0, x1) = ⟨˙xt,η, (G(xt,η; D) −I) ˙xt,η⟩
˙xt,η = x1 −x0 + t(1 −t) ˙φt,η(x0, x1) + (1 −2t)φt,η(x0, x1),
where we have used the parameterization of xt,η in eq. (4). By replacing G(x; D) with the explicit
expression given by the RBF metric in eq. (10), this provides a concrete formulation for eq. (14). We
19

also note that an equivalent perspective amounts to replacing the parametric potential with a fixed
function Ut(xt,η, ˙xt,η) that depends not only on xt,η as the potentials in Liu et al. [2024], but also
on the velocities ˙xt,η. Once again, we emphasize that our framework prescribes explicit parametric
potentials Vt,η via the diagonal Riemannian metrics in Section 3.1 and hence differs from Liu et al.
[2024] which leave to the user the task of designing potentials based on applications.
D
Experimental Details
We parameterize the models φt,η(x0, x1) in eq. (4) and vt,θ(xt) in eq. (7) as neural networks, and
train them separately and sequentially. We start with φt,η(x0, x1), so that interpolants are learned
prior to regressing vt,θ(xt) in the matching objective. Synthetic Arch, LiDAR, and single-cell
experiments were run on a single CPU. A single run of the LiDAR architecture and 5-dimensional
single-cell experiments trains in under 10 minutes, while higher-dimensional single-cell experiments
typically train in under an hour. The unpaired translation experiment on AFHQ was trained on a GPU
cluster with NVIDIA A100 and V100 GPUs. Training time for AFHQ varies by GPU, ranging from
12 hours (A100) to 1 day (V100).
D.1
Synthetic arch and LiDAR experiments
In the synthetic arch and LiDAR experiments, we parameterized both φt,η(x0, x1) and vt,θ(xt) as
3-layer MLP networks with a width of 64 and SeLU activation. The networks were trained for up to
1000 epochs with early stopping (patience of 3 based on validation loss). We employed the Adam
optimizer Kingma and Ba [2014] with a learning rate of 0.0001 for φt,η(x0, x1) and the AdamW
optimizer Loshchilov and Hutter [2017] with a learning rate of 10−3 and a weight decay of 10−5 for
vt,θ(xt). We used a 90%/10% train/validation split. Training samples served as source and target
distributions and for calculating the LAND metric, while validation samples were used for early
stopping. We used σ = 0.125 and ϵ = 0.001 in the LAND metric. During inference, we solved for
pt using the Euler integrator for 100 steps.
Synthetic arch
To generate the arch dataset, we follow the experimental setup from [Tong et al.,
2020]. We sampled 5000 points from two half Gaussians N(0, 1
2π) and N(1, 1
2π). The exact optimal
transport interpolant at t = 1
2 was used as the test distribution. We then embedded the points on a
half circle of radius 1 with noise N(0, 0.1) added to the radius. The Earth Mover’s Distance between
the sampled and test sets was averaged across five independent runs.
LiDAR
For LiDAR, the data consists of point clouds within [−5, 5]3 ⊂R3, representing scans
of Mt. Rainier [Legg and Anderson, 2013]. The source and target distributions are generated as
Gaussian Mixture Models and projected onto the LiDAR manifold as in Liu et al. [2024]. We then
standardized all the LiDAR, source, and target points.
D.2
Unpaired translation in latent space
In the unpaired translation experiments, we utilized the U-Net architecture setup from Dhariwal
and Nichol [2021] for both φt,η(x0, x1) and vt,θ(xt). The exact hyperparameters are reported in
Table 5. We used the Adam optimizer Kingma and Ba [2014] for both networks and applied early
stopping only for φt,η(x0, x1) based on training loss. During inference, we solved for pt at t = 1
using the adaptive step-size solver Tsit5 with 100 steps. We trained the RBF metric with κ = 0.5
and ϵ = 0.0001 with k-means clustering, as described in Appendix C. For this experiment only, we
enforce stronger bending by using (˜hα(x))8, in the loss function LgRBF(η) in (11). Note that higher
powers of ˜h ensures that regions away from the support of D, where ˜h < 1, are penalized even more
in the geodesic objective, which highlights the role played by the biases introduced via the metric.
Our method operates in the latent space of the Stable Diffusion v1 VAE Kingma and Welling [2014],
Rombach et al. [2022], except for the k-means clustering step in RBF metric pretraining, which was
performed in the ambient space.
Dataset
We used the Animal Face dataset from Choi et al. [2020], adhering to the splitting
predefined by dataset authors for train and validation sets, with validation treated as the test set.
20

Table 5: U-Net architecture hyperparameters for unpaired image translation on AFHQ.
φt,η(x0, x1)
vt,θ(xt)
Channels
128
128
ResNet blocks
2
4
Channels multiple
1, 1
2, 2, 2
Heads
1
1
Heads channels
64
64
Attention resolution
16
16
Dropout
0
0.1
Batch size
256
256
Epochs
100
8k
Learning rate
1e-4
1e-4
EMA-decay
0.9999
0.9999
Standard preprocessing was applied: upsizing to 313x256, center cropping to 256x256, resizing to
128x128, and using VAE encoders for preprocessing. Finally, we computed all embeddings using
pretrained Stable Diffusion v1 VAE Rombach et al. [2022]. FID Heusel et al. [2017] and LPIPS
Zhang et al. [2018] were computed using the validation sets. FID was measured with respect to the
cat validation set, while LPIPS Zhang et al. [2018] between pairs of source dogs and generated cats.
D.3
Trajectory inference for single-cell data
We performed both low-dimensional and high-dimensional single-cell experiments following the
setups in Tong et al. [2023a,b]. For each experiment, the single-cell datasets were partitioned by
excluding an intermediary timepoint, resulting in multiple subsets. Independent models were then
trained on each subset. Test metrics were calculated on the left-out marginals and averaged across all
model predictions.
We employed the Adam optimizer Kingma and Ba [2014] with a learning rate of 10−4 for φt,η(x0, x1)
and the AdamW optimizer Loshchilov and Hutter [2017] with a learning rate of 10−3 and a weight
decay of 10−5 for vt,θ(xt). During inference, we solved for pt at t being left-out marginal using the
Euler integrator for 100 steps.
We used a 90%/10% train/validation split, excluding left-out marginals from both sets. Training
samples served as source and target distributions and for calculating the metrics, while validation
samples were used for early stopping. We note that these settings are slightly more restrictive (and
realistic) than those reported for SF2M-Geo Tong et al. [2023a], where the left-out timepoint was
also included in the validation set.
Embryoid Body dataset
We used the Embryoid Body (EB) data Moon et al. [2019] preprocessed
by Tong et al. [2020], focusing on the first five whitened dimensions. The dataset, consisting of five
time points over 30 days, was used to train separate models across the full-time scale, each time
leaving out one of the time points 1, 2, or 3.
Cite and Multi datasets
We utilized the Cite and Multi datasets from the Multimodal Single-cell
Integration Challenge at NeurIPS 2022 Lance et al. [2022], preprocessed by Tong et al. [2023a]. These
datasets include single-cell measurements from CD4+ hematopoietic stem and progenitor cells for
1000 highly variable genes across four time points (days 2, 3, 4, and 7). We trained separate models
each time leaving out one of the time points 3 or 4. The data was whitened only for 5-dimensional
experiments.
Multiple constraints setting
Following a similar approach to Neklyudov et al. [2023b], we
modified our sampling procedure to interpolate between two intermediate dataset marginals, with
neural network parameters η shared across timesteps. The interpolation is defined as:
xt = ti+1 −t
ti+1 −ti
xti +
t −ti
ti+1 −ti
xti+1 +
 
1 −
 ti+1 −t
ti+1 −ti
2
−
 t −ti
ti+1 −ti
2!
φt,η(x0, x1). (20)
21

Low-Dimensional experiments
We parameterized both φt,η(x0, x1) and vt,θ(xt) as 3-layer MLP
networks with a width of 64 and SeLU activation. In the LAND metric, we used σ = 0.125 and
ϵ = 0.001 for all EB sets and the first leave-out time point in the Cite and Multi datasets. For the
second leave-out time point in Cite and Multi, we used σ = 0.25 and ϵ = 0.001.
High-Dimensional experiments
Both φt,η(x0, x1) and vt,θ(xt) were parameterized as 3-layer
MLP networks with a width of 1024 and SeLU activation. We set κ = 1.5 and ϵ to be the complement
of the final metric pretraining loss to maintain consistent regularization across datasets and leave-out
timesteps.
Baselines
We reproduced the results reported by Neklyudov et al. [2023b] to ensure consistent
reporting of standard deviations and the same versions of EB dataset used across all experiments.
The standard deviations were calculated across all leave-out timesteps and seeds for each dataset and
dimension. We used the provided code and hyperparameters for training, averaging the results across
5 seeds.
E
Supplementary Single-Cell Reconstruction
We report additional results for single-cell reconstruction using 50 principal components in Table 6.
Again, we note that OT-MFM performs strongly, and it is marginally surpassed only by SF2 on
Multi(50D)(results have not been reproduced). Two important remarks are in order. First, the baseline
SF2 M-Geo leverages a geodesic cost in the formulation of the Optimal Transport coupling, which
enforces similar biases to OT-MFM. In fact, as argued in Section 4, OT-MFM can similarly consider
data-aware costs in the formulation of the optimal transport coupling. In this work though, we wanted
to focus on the ability of interpolants to lead to meaningful matching in settings where the optimal
transport coupling is agnostic of the data support. Additionally, we highlight that as we move to more
realistic high-dimensional settings (100 principal components, shown in Table 3) the advantages of
our framework become even more apparent.
Table 6: Wasserstein-1 distance averaged over left-out marginals (↓better) for 5-dim PCA representa-
tion of single-cell data for corresponding datasets. Results are averaged over 5 independent runs.
Method
Cite (50D)
Multi (50D)
SF2 M-Geo
38.524 ± 0.293
44.795 ± 1.911
SF2 M-Exact
40.009 ± 0.783
45.337 ± 2.833
OT-CFM
38.756 ± 0.398
47.576 ± 6.622
I-CFM
41.834 ± 3.284
49.779 ± 4.430
WLF-UOT
37.007 ± 1.200
46.286 ± 5.841
WLF-SB
39.695 ± 1.935
47.828 ± 6.382
WLF-OT
38.352 ± 0.203
47.890 ± 6.492
OT-MFMRBF
36.394 ± 1.886
45.16 ± 4.96
22

F
Supplementary Unpaired Translation Results
Figure 4: Additional qualitative comparison for the task of unpaired transla-
tion between OT-CFM and OT-MFMRBF.
23

