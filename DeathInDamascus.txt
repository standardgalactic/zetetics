Cheating Death in Damascus∗
Benjamin A. Levinstein
University of Illinois at
Urbana-Champaign
benlevin@illinois.edu
Nate Soares
Machine Intelligence Research Institute
nate@intelligence.org
Abstract
Evidential and Causal Decision Theory are the leading contenders as theories
of rational action, but both face fatal counterexamples. We present some new
counterexamples, including one in which the optimal action is causally domi-
nated. We also present a novel decision theory, Functional Decision Theory
(fdt), which simultaneously solves both sets of counterexamples. Instead of
considering which physical action of theirs would give rise to the best outcomes,
fdt agents consider which output of their decision function would give rise
to the best outcome. This theory relies on a notion of subjunctive depen-
dence, where multiple implementations of the same mathematical function are
considered (even counterfactually) to have identical results for logical rather
than causal reasons.
Taking these subjunctive dependencies into account
allows fdt agents to outperform cdt and edt agents in, e.g., the presence of
accurate predictors. While not necessary for considering classic decision theory
problems, we note that a full speciﬁcation of fdt will require a non-trivial
theory of logical counterfactuals and algorithmic similarity.
1
Introduction
Evidential and Causal Decision Theory (edt and cdt) are the two main contenders
as theories of rational action, but neither has managed to persuade enough critics to
establish itself as the consensus choice. This division is in part due to the fact that
philosophers disagree about the correct solution to several decision problems, such as
Newcomb’s problem. In order to resolve this dispute, we will explore some problems
that we believe should be uncontroversially fatal to both theories. The ﬂaws of edt
and cdt will then lead us to a new theory called functional decision theory (fdt)
that not only gets these less controversial problems right but has strong theoretical
motivation as well.
The decision problems that we focus on are akin to Gibbard and Harper’s (1978)
classic Death in Damascus problem. Traditional solutions to such problems are
unstable. Once you grow conﬁdent you’ll perform one action, you will expect that
action to produce bad consequences. These cases lead to more univocal intuitive
verdicts, so they provide probative data for deciding between theories.
edt gets these cases right, while cdt gets them wrong. Nonetheless, edt faces
its own well-known and (to our minds) decisive counterexamples. Although it arrives
at the right answer in the unstable problems we’ll discuss, it does so for the wrong
reason.
In short, edt tells you to perform the action that would be the best indication of a
good outcome, whereas cdt tells you to perform the action that will tend to causally
∗. Cite: Levinstein, Benjamin A. and Soares, Nate. “Cheating Death in Damascus.”
The Journal of Philosophy 117, no. 5 (May 2020): Pages 237-266. https://doi.org/10.
5840/jphil2020117516.
Research supported by the Machine Intelligence Research Institute (intelligence.org).
1

bring about good outcomes. Unfortunately, the former treats spurious correlations
as decision-relevant; the latter treats decision-relevant correlations as spurious. fdt
is structurally similar to cdt, but it rectiﬁes this mistake by recognizing that logical
dependencies are decision-relevant as well.
To get the gist of the diﬀerences between the three theories, consider the following
problem as illustration:
Twin Prisoner’s Dilemma An agent has the option of cooperating with or de-
fecting against her psychological twin, who is in a separate room, faces the
same options, and has the same state of knowledge. Both rank the outcomes
the same way, where “I defect and she cooperates” > “We both cooperate” >
“We both defect” > “I cooperate and she defects.” Should the agent defect or
cooperate?
cdt reasons that because there’s no causal link between the two agents’ choices,
defecting strictly dominates cooperating. So, it prescribes defection. edt reasons that
defection is strongly correlated with the other agent defecting and that cooperation is
correlated with the other agent cooperating. Since it prefers the mutual cooperation
outcome to the mutual defection outcome it prescribes cooperation.
The fdt agent reasons as follows: If my decision algorithm were to output
defect, then the other agent would output defect too because she’s running (a close
approximation of) my algorithm. If my algorithm were to output cooperate, then
the other agent would choose that as well. The latter results in a better outcome for
me, so I cooperate.
More generally, functional decision theorists think of agents as instantiations
of a particular decision algorithm (combined with credences and utilities). fdt
advises agents to consider what would happen if their decision algorithm were to
produce a diﬀerent output and to choose the output that tends toward the best
results. Because functions can be multiply instantiated, all instantiations of that
function will counterfactually co-vary (according to the fdt-counterfactuals in the
hypothetical scenarios imagined by an fdt agent).
According to cdt, you should only take potential eﬀects that depend causally
on your action into account, where causation is restricted to the physical dynamics
of the universe. Functional decision theory has a diﬀerent notion of dependence.
The output of your own algorithm and the output of your twin’s algorithm are
subjunctively interdependent. You have control over your algorithm (since you
are free either to cooperate or defect) and therefore you also have control over
the algorithm that your twin implements (because those are the same algorithm).
Thus, the correlation between your action and the twin’s is not spurious, and your
counterfactual reasoning should take this connection between your action and the
twin’s action into account.
We claim such reasoning is both natural and leads to a better theory than
either cdt or edt. fdt gets the right answer in unstable problems like Death in
Damascus while avoiding the pitfalls of edt.
Functional decision theory has been developed in many parts through (largely
unpublished) dialogue between a number of collaborators. fdt is a generalization of
Dai’s (2009) “updateless decision theory” and a successor to the “timeless decision
theory” of Yudkowsky (2010). Related ideas have also been proposed in the past by
Spohn (2012), Meacham (2010), Gauthier (1994), and others.
fdt does face new conceptual problems of its own. Most importantly, on any
formulation we’re aware of, a full speciﬁcation of fdt requires a notion of non-
trivial logical counterfactuals (with logically impossible antecedents) and a notion of
similarity between algorithms. We will leave exploration of these topics to future
work, since the machinations and verdicts of fdt will be clear in the cases under
consideration.
Here’s the plan. Section 2 gives a whirlwind summary of edt and explains
why it fails. Section 3 describes cdt, discusses Joyce’s (2012) extension of cdt
to cover unstable cases like Death in Damascus, and presents Ahmed’s (2014)
counterexample. We argue that this counterexample shows that cdt’s dependency
2

hypotheses are systematically impoverished. This oversight motivates fdt, which
we develop in section 4. Section 5 runs through a number of cases that contrast
cdt and fdt. Our ﬁrst case, Death on Olympus, presents an option which
fdt rightly recognizes as optimal despite being causally dominated. Psychopath
Button characterizes fdt further. Our ﬁnal case, Murder Lesion, is an alleged
counterexample to fdt. However, we argue that when correctly spelled out, fdt
arrives at the right verdict. Section 6 wraps up.
2
Evidential Decision Theory
We begin with a familiar rehearsal of the motivations and machinations behind the
two current leading theories of ideal rational action.
According to Evidential Decision Theory, you should perform the action that is
the best indicator of good outcomes. That is, edt tells agents to perform the act
that leads to the greatest expected value conditional on performing it.
To spell this view out abstractly, we’ll loosely follow Savage’s (1972) model of
decision making: an agent uses her credences about which state of the world is
actual to choose between possible actions that lead to better or worse outcomes.
With Jeﬀrey (1983), we’ll think of both states and actions as propositions: elements
of the set S = {s1, . . . , sn} and A = {a1, . . . , am} respectively. Jointly, an action
a and a state s determine an outcome o[a, s] ∈O. Outcomes are the objects of
intrinsic value, or ﬁnal ends for the agent. So, o1 = o2 only if the agent is indiﬀerent
between o1 and o2.
We take for granted that the agent comes equipped with a (probabilistically
coherent) credence function P that measures her subjective degrees of conﬁdence
and a utility function u : O →R that represents how good or bad she judges each
outcome.
According to edt, agents should perform the action that maximizes
Uedt(a) =
X
s∈S
P(s | a)u (o[a, s])
(1)
That is, edt agents calculate the expected utility of actions by considering their
likely consequences on the indicative supposition that the action is performed.
The problem, however, is that equation (1) requires agents to take into account
every correlation between their actions and the state of the world, even if those
correlations are spurious. As Lewis (1981) puts it, edt endorses “an irrational policy
of managing the news” (p. 5). To help illustrate this ﬂaw, consider:1
XOR Blackmail An agent has been alerted to a rumor that her house has a
terrible termite infestation, which would cost her $1,000,000 in damages. She
does not know whether this rumor is true. A greedy and accurate predictor
with a strong reputation for honesty has learned whether or not it’s true, and
drafts a letter:
I know whether or not you have termites, and I have sent you this
letter iﬀexactly one of the following is true: (i) the rumor is false,
and you are going to pay me $1,000 upon receiving this letter; or
(ii) the rumor is true, and you will not pay me upon receiving this
letter.
The predictor then predicts what the agent would do upon receiving the letter,
and sends the agent the letter iﬀexactly one of (i) or (ii) is true. Thus, the
claim made by the letter is true. Assume the agent receives the letter. Should
she pay up?
1. We use this case due to Soares and Fallenstein (2015) instead of the more familiar
Smoking Lesion from Skyrms (1980) for two reasons. First, the smoking lesion problem
requires the agent to be uncertain about their own desires. Second, the present case is
immune to the so-called “tickle defense” (Eells 1984).
3

What she does now will not aﬀect whether she has termites or not and will in no
way improve her situation. While paying the blackmail is a good indication that
there are not termites, this correlation between her action and the state of the world
is spurious. How frequently people end up with termite infestations does not change
regardless of their willingness to pay blackmailers of this kind. (Some readers may
feel a temptation to pay the blackmailer anyway, which we hope will fade after
reading fdt’s approach to the problem in Section 4.3.)
3
Causal Decision Theory
Although edt still has a number of adherents, we think cases like XOR Blackmail
show it is fatally ﬂawed.
Rational action requires trying to bring about good
outcomes. What matters is how the world depends upon your actions. Instead of
asking what is likely the case given that you perform a, you should ask how the
world would change were you to perform a.
Accordingly, Causal Decision Theory does not think every correlation between
act and state is relevant to decision making, unlike edt. Instead, it directs agents
to consider only the potential causal consequences of their actions. Not paying the
blackmail is strong evidence that your house is infested, but paying would not cause
a better outcome because paying the blackmailer will not actually bring about a
lower chance of an infestation. In fact, paying is sure to cause a worse outcome
regardless of whether or not there’s an infestation. Thus, you should not pay.
One natural way to spell out cdt is to invoke the notion of dependency hy-
potheses (Joyce 1999; Lewis 1981). A dependency hypothesis is a conjunction of
counterfactuals of the form V
a∈A a  s, where s is a state of the world. I.e., a
dependency hypothesis takes a stand on which state of the world would result from
any action. The counterfactual conditional  is interpreted causally: if I were to
perform a, s would occur.
CDT Agents form credences over competing dependency hypotheses, which they
use to guide their decisions. P(a  s) is greater than, equal to, or less than
P(s) exactly in the cases where the agent judges a to causally promote, be causally
independent of, or causally inhibit bringing about state s. According to cdt, these
counterfactuals encode how the state of the world (and the outcome) are seen by
the agent to depend on her actions.
In turn, we can deﬁne causal expected utility as:
Ucdt1(a) =
X
s∈S
P(a  s)u (o[a, s])
Given a probability function that is deﬁned over a set of dependency hypotheses, cdt
tells agents to maximize causal expected utility. In contrast to edt, this equation
weights the utility of an outcome (a state-action pair) by the probability of the state
on the subjunctive supposition that the action is performed.
While there are a number of diﬀerent versions of cdt and ways of understanding
, these diﬀerences will not matter to us. For expositional purposes, however, it
will be useful to identify P(a  s) with P(s | do(Act = a)) in accord with Judea
Pearl’s (1996; 2009) do-calculus. In this version, an agent is equipped with both a
probability function P and a directed graph G. The nodes of G represent variables
potentially relevant to the outcome, and the edges describe potential directions of
causal inﬂuence.2
P(· | do(Var = val)) is P modiﬁed so that all variables causally downstream
(according to G) of Var, including Var itself, are updated to reﬂect the intervention
setting Var equal to val, and the other variables are left unchanged. To calculate
expected utility, cdt agents intervene on the Act variable and use P(· | do(Act =
a)) to calculate the expected value of an action a. So, our oﬃcial equation for cdt’s
2. More generally: a probability distribution over possible graphs. See fn. 3.
4

expected utility formula is:3
Ucdt(a) =
X
s∈S
P(s | do(Act = a))u (o[a, s])
(2)
For example, in ﬁgure 1, whether there’s an infestation aﬀects whether the
predictor sends the letter, which also aﬀects the agent’s decision to pay. Together, the
agent’s decision and whether there’s an infestation determine her payoﬀ. To calculate
the value of paying, the agent uses P(· | do(Act = pay)). Since the Predictor variable
is upstream of Act, P(infestation | do(Act = ¬pay)) = P(infestation | do(Act =
pay)) = P(infestation), even though P(infestation | pay) < P(infestation | ¬pay).
This makes sense. If the agent were to refuse to pay, she would not change whether
there’s an infestation or not. The correlation between not paying and the lack of an
infestation is thereby counterfactually broken and correctly diagnosed as spurious.
Infestation
Predisposition
Predictor
Act
V
Figure 1: A causal graph for cdt agents facing XOR Blackmail. The agent
intervenes on the Act variable, setting it either to pay or do not pay. Infestation
is either ‘yes’ or ‘no’, and Predictor is either send or do not send depending on
whether there’s an infestation and on the agent’s predisposition. Whether there’s an
infestation and whether Act is pay or not determines the agent’s payoﬀs, represented
by the V -node.
3.1
CDT and Instability
To complete the discussion of Causal Decision Theory, we’ll now consider a new kind
of case that appears unstable for cdt. First, we’ll discuss how one leading version of
cdt might handle such cases and argue that it is doomed to get the wrong answer.
We’ll then use the reasons for this failure—namely, that cdt has an impoverished
conception of dependence—as motivation for fdt. After presenting fdt, we provide
a more general counterexample (Death on Olympus) to any standard version of
cdt.
We’ll begin with an asymmetric variant of a problem originally discussed by
Gibbard and Harper (1978):
Death in Damascus You are currently in Damascus. death knocks on your door
and tells you I am coming for you tomorrow. You value your life at
3. Equation (2) is not suﬃciently general because the agent may be uncertain which
causal graph represents her decision problem. So, cdt in its general form requires credences
over not just states and actions, but also over graphs. We will use P as the joint distribution
over all three, which we marginalize out as necessary. Therefore, the generalized equation
for our version of cdt is:
Ucdt(a) =
X
i
X
s∈S
P(Gi)P(s | doi(Act = a))u (o[a, s])
where Gi is a graph, and doi is the do-operator for Gi. Because the structure of the graph is
known for all cases in this paper, we avoid this added complexity in the main presentation.
5

$1,000 and would like to escape death. You have the option of staying in
Damascus or paying $1 to ﬂee to Aleppo. If you and death are in the same
city tomorrow, you die. Otherwise, you will survive. Although death tells
you today that you will meet tomorrow, he made his prediction of whether
you’ll stay or ﬂee yesterday and must stick to his prediction no matter what.
Unfortunately for you, death is a perfect predictor of your actions. All of this
information is known to you.
Predisposition
Act
death
Outcome
V
Figure 2: A causal graph for cdt agents facing Death in Damascus, Random
Coin, and Death on Olympus. In the ﬁrst two cases, the agent can set Act to
either stay or ﬂee and has the additional option of randomize in the second case.
In Death on Olympus, the agent can stay in Damascus, ﬂee to Aleppo, or climb
Olympus. The death variable is either Damascus or Aleppo in the ﬁrst two cases,
and possibly Olympus in the third.
First, note that because death has privately made his decision of where to
go before speaking to you, your decision of whether to stay in Damascus or ﬂee
to Aleppo is causally independent of death’s decision. That is, death’s decision
of where to go does not causally aﬀect your decision, and your decision does not
causally aﬀect death’s decision.
However, your decision is strong evidence for what the causal structure of the
world is. Recall, cdt requires you to consider credences about what would happen
if you were to perform an action, i.e., credences of the form P(s | do(a)). In the
cases we’ve discussed, P(a) does not aﬀect the value of P(s | do(a)). In this case, it
does, because death is a perfect predictor.4
More explicitly, let S and F refer to the actions of staying in Damascus and ﬂeeing
to Aleppo, and let D and A denote the propositions that death is in Damascus
and that death is in Aleppo. Suppose you’re sure you’ll stay in Damascus. That
is, P(S) = 1. You’re then sure that that death will be in Damascus tomorrow.
But, because your choice is causally independent of death’s location (i.e., death’s
location is neither upstream nor downstream of your choice on your causal graph),
P(D | do(S)) = P(D | do(F)) = P(D) = 1. In this case, ﬂeeing to Aleppo brings
you more causal expected utility because you think that ﬂeeing to Aleppo would
cause you to live. However, if you’re sure you’ll go to Aleppo, then your opinions
about the causal consequences of your actions are diﬀerent. In that circumstance,
staying in Damascus brings you more causal expected utility.
As we see, the problem in scenarios like Death in Damascus is that the agent
cannot be certain of what she’ll do before she decides, on pain of irrationality. If she’s
4. Some readers may object to the stipulation that death is a perfect predictor. This is
inessential and merely simpliﬁes the discussion. For our purposes, little is lost if death is
a nearly perfect predictor instead.
6

sure she’ll go to Aleppo, then either she as a matter of fact remains in Damascus
despite her certainty, or she performs an action that violates the injunction to
maximize expected causal utility.5
Now, there is a bit of disagreement about what cdt recommends in cases such
as these. One option is to say that cdt fails to render a verdict at all. This route is
certainly unattractive, as it prevents cdt from being a general theory of rational
action, so we won’t consider it further.
Recently, Arntzenius (2008) and Joyce (2012) have presented solutions, both
of which rely primarily on the model of rational deliberation developed in Skyrms
(1990). The diﬀerences between Arntzenius’s and Joyce’s views will not matter much
for us, so we’ll primarily focus on Joyce’s view.
According to Joyce, because the agent cannot know ex ante what she’ll do,
she must initially have non-extremal credences over her actions. To make matters
concrete, let P0 be her initial credence function, and suppose P0(S) = .9 and
P0(F) = .1. If she uses this credence function to make her decision, she assigns
ﬂeeing higher expected utility than remaining. To be precise: U0(S) = 100 and
U0(F) = 899.
It’s tempting here to conclude that cdt requires her to ﬂee, but that verdict is
premature. Although she initially evaluates F as a more favorable action than S,
this very evaluation is important information for a number of reasons:
1. The agent regards herself as rational, i.e., as a causal expected utility maximizer.
However P0 assigns lower credence to F than to S even though she currently
evaluates F as a better option than S. Surely, this should lead her to raise
her credence in F and lower it in S.
2. Given (1), the fact that she regards F as more attractive than S is evidence
about death’s location. Once she raises her credence that she will go to
Aleppo, she should become more conﬁdent that death is in Aleppo as well.
So, P0(· | U0) ̸= P0.6
Thus, P0 did not take into account all freely available
information—viz., the value of U0. So, the agent should not use P0 to guide her ﬁnal
action. Instead, she should revise her credence to P1(·) = P0(· | U0) and re-evaluate
her options. Of course, it might be that P1(· | U1) ̸= P1, in which case she should
iterate this process.
It’s a subtle matter how exactly she should update on her own utility calculations,
but the important requirements are just that she grows more conﬁdent she’ll perform
attractive options, and that she does not instantly become certain she’ll perform
such options.
Eventually, if she updates in the right way, this iterative process will terminate in
an equilibrium state Pe, such that Pe = Pe+1 = Pe(· | Ue). In this case, equilibrium
occurs when the agent is indiﬀerent between ﬂeeing and remaining, where:
• Pe(S) = .5005
• Pe(F) = .4995
At this point, the agent has taken into account all freely available, causally relevant
information, so Pe is her credence function at the end of deliberation. Strictly
speaking Pe only tells her what she believes she’ll do at the end of deliberation.
There remains the further question of what she actually should do.
CDTers disagree about this question, and Joyce says that she is permitted either
to go to Aleppo or stay in Damascus. Arntzenius thinks that cdt only ultimately
tells you about what you should believe and is silent on which actions are rational.
This latter position strikes us as unattractive, since we want decision theory to guide
our actions, not just to tell us what we should think.
5. The former option is at the very least uncomfortable for the cdt theorist. Decision
theory should lead to verdicts about what to do at the end of rational deliberation. If the
verdict is diﬀerent from what the agent actually should do, then that is the fault of the
underlying decision theory.
6. We use the notation P0(· | U0) to refer to P0 conditioned on the value of U0.
7

For the sake of generality, we’ll now assume the agent has not only the pure
actions of staying in Damascus and ﬂeeing to Aleppo, but also has a certain type
of mixed action available to her as well. She can, if she wishes, ﬂip a mental coin
(or use some randomization process) of any bias and let the outcome of that coin
ﬂip determine what she ends up doing. Given such an expanded action space, she’ll
most want to ﬂip a coin with biases matching the equilibrium credences over the act
space.7 So, in this case, she will ﬂip a coin that she has credence .5005 will land
heads and will stay in Damascus only when it does.
However, in keeping with the set-up of the case and unlike in standard treatment
of mixed acts in decision-theory, we assume any mental randomization she performs
is still predictable from death’s perspective. If it weren’t, then death could not be a
perfect or nearly perfect predictor, which is the entire motivation for the original case.
The coin, then, is not truly random, but has no detectable pattern from the agent’s
perspective beyond the frequency with which it lands heads or tails. Moreover, the
agent is aware that the outcome of the coinﬂip is predictable to death. We’ll call
such a coin pseudo-random.
So, cdt agents remain in Damascus just over half the time, and they ﬂee to
Aleppo just under half the time. In each case, they end up dead.8
3.2
Random Coin
Although Joyce’s solution in this initial case seems natural, it does lead to an odd
verdict. You will sometimes end up paying money to ﬂee to Aleppo even though
you’re sure to die there. It’s true, if we use cdt counterfactuals, that when you stay
in Damascus, you would have lived if you had ﬂed. And it’s true that when you ﬂee,
you would have lived if you had stayed. However, as a matter of fact you will always
die, and you know that you will always die. The cdt counterfactual provides cold
comfort regardless of where you end up. Why pay $1 when you do not have to?
We can bring out the central problem with this solution with a case due to
Ahmed (2014):
Random Coin As before, death tells you I am coming for you tomorrow,
and you die only if you end up in the same city. As before, death has made
his prediction long ago, and he cannot change it at this point. You now have
the option of staying in Damascus or ﬂeeing (for free) to Aleppo. This time,
however, you run into Rhinehart, who oﬀers to sell you a truly indeterministic
coin for $1 that lands heads with probability .5. Although death can predict
whether you will buy the coin, he cannot do better than chance at predicting
how the coin lands. On the other hand, if you do not buy the coin, death is
a perfect predictor of where you’ll end up.
It seems clear that you really should buy the coin. If you do not, you will die
with certainty. If you do, you have a 50% chance of survival. By making yourself
unpredictable to death even after death has made his irrevocable prediction, you
end up better oﬀ.
However, cdt—at least on the versions currently under discussion—advises you
not to pay. To see why, note that, structurally, the causal graph here is the same as
in Figure 2. The only diﬀerence is that in this variant, you have a new potential
action: Pay $1 for a truly random coin.
Buying the coin is causally independent of death’s location. If death is in
Aleppo, the coin will not aﬀect that, and the same goes for Damascus. With or
without the coin, you are free to go to either place, and where you go does not
7. To see why, imagine she ﬂipped a coin she has credence .6 will land heads and decides
to stay in Damascus when and only when it does. She’d then have credence .6 she’d end
up in Damascus instead of retaining her equilibrium credence of .5005. So, after adopting
credence .6 in staying, she’d view staying as more attractive.
8. If we restrict our attention to the case in which the agent can’t perform mixed acts,
then cdt will still allow agents to pay to go to Aleppo. Our case Death on Olympus
below will serve as a counterexample to cdt regardless of whether the action space is pure
or mixed.
8

causally aﬀect where death is. So, buying the coin costs a dollar, and it does not
causally bring about a better world.
More explicitly, we can calculate the value of your options of staying and ﬂeeing
(S and F) as follows:
U(S) = P(D | do(S))u(S, D) + P(A | do(S))u(S, A)
U(F) = P(D | do(F))u(F, D) + P(A | do(F))u(F, A)
Again, D (A) refers to death ending up in Damascus (Aleppo). Whether you’re in
equilibrium or not, one of these options always has expected utility of at least 500,
since staying and ﬂeeing themselves cost nothing.
Randomizing has a 50% chance of resulting in ﬂeeing and a 50% chance of
resulting in staying, but is sure to cost $1. So,
U(R) = P(D | do(R))[.5(u(S, D) + u(F, D)) −1]
+ P(A | do(R))[.5(u(S, A) + u(F, A)) −1]
= 499
Something must have gone wrong here.
Regardless of any theoretical pre-
commitments, we have a hard time believing even trenchant defenders of cdt
would not buy the coin if they found themselves in this situation.9
And the reason to buy a coin seems deeper than mere correlation. Although
death already made his choice before you, it seems that your choice and death’s
choice are importantly and counterfactually linked. Because death knows your
mind so well, if you were to choose Aleppo (without the beneﬁt of a truly random
coin), he would have chosen it too. Buying the coin allows you to make yourself
opaque to death even though your choice comes after death’s choice. The upshot
here is that cdt is blind to some features of the world that depend on your action.
4
Functional Decision Theory
edt gets cases like Random Coin right. After all, ﬂipping the random coin is
correlated with escaping death and gaining more utility than not ﬂipping the coin.
However, this is only because ﬂipping and survival are correlated in some way. As
we’ve seen, edt does not have the apparatus to distinguish between spurious and
non-spurious correlations and thereby gets a number of other cases wrong, including
XOR Blackmail.
The lesson from Random Coin is that cdt is too parsimonious in what it counts
as a non-spurious correlation. In this case, the correlation between death’s choice
and yours if you do not buy the coin is intuitively robust. If you had not bought
the coin and had gone to Aleppo, death certainly would have met you there, at
least on a very natural counterfactual.
Because death knows how (or at least bases his decisions on how) you make
your decisions, his choice and your choice—barring randomness—are non-spuriously
correlated. This is the primary motivation behind functional decision theory. Func-
tional decision theory agrees with cdt that not every correlation between action and
state should be decision-relevant. Instead, what matters is counterfactual correlation
based on what depends on your actions. However, according to fdt, cdt has the
wrong view of what should count as a legitimate dependency hypothesis.
In brief, fdt suggests that you should think of yourself as instantiating some
decision algorithm. Given certain inputs, that algorithm will output some action.
Because algorithms are multiply realizable, however, there can be other instantiations
of this same (or a very similar) algorithm, and, more generally, the state of the
world can non-causally depend on what your decision algorithm does. Your decision
9. Although this case is a counterexample to the Joyce and Arntzenius versions of cdt,
we do not rule out the possibility that an alternative version would recommend buying
the random coin in this case. Our case below—Death on Olympus—is a more general
counterexample.
9

procedure is not local to your own mind, and rationality requires taking this fact
into account.10
One thing we know about mathematical functions is that diﬀerent instances of
the same function do not behave diﬀerently on the same input. For example, if
you are playing a Prisoner’s Dilemma against your psychological twin, you know
that you both run the same decision procedure. If your decision procedure were to
output defect, then your twin’s would as well, given that she’s computing the same
procedure.11
One way to spell out the details of Random Coin is in similar terms: Simply
stipulate that death runs (as a subroutine) a copy or near copy of whatever
procedure you use to decide whether to stay, ﬂee, or randomize. In this way, you
can know that whatever your procedure outputs, death’s will output the same.
Of course, given the explicit set up of the case, death is not necessarily running
a “copy” of you in his head in order to make his prediction. However, regardless of
the means he employs, if he’s reliable, then his decision will depend on the output
of your decision algorithm. For instance, death may simply have an appointment
book that either lists your location on any given day or says you will randomize. In
this case, death doesn’t personally know the ins and outs of your algorithm, but
simply looks up what you’ll do. According to the fdt counterfactuals developed
below, however, if you think the appointment book is a reliable predictor of where
you’ll be, then what the appointment book itself says depends, in your view, on
how your decision procedure behaves. The reason it says “Damascus” is that your
decision procedure outputs stay. So, since you are free to choose between staying,
ﬂeeing, and randomizing, where death ends up depends on your choice even though
his location is causally independent of your decision as a matter of physical fact.
fdt agents consider only what depends on their decision, but they imagine
intervening not on the action directly (as cdt does) but on the algorithm that
determines their action. That is, they imagine what would happen diﬀerently if they
were to come to diﬀerent verdicts at the end of deliberation.12
Let’s consider a second example to get a better sense of how this works. Consider
ﬁgure 3. In Death in Damascus, the output of the agent’s decision algorithm
determines both her action (S or F), and death’s location. If she sets her algorithm
to output F, then that changes death’s location to Aleppo and results in her ﬂeeing
(in the counterfactual world that the agent imagines). Thus, according to that
counterfactual, if she ﬂed to Aleppo, she would both die and lose $1. On the other
hand, if she sets her algorithm to output S, then she stays and death ends up in
Damascus. According to that counterfactual, if she stayed in Damascus she would
die but not lose any money. Since outputting S results in a better outcome, she
stays.
Somehow or other the predictions of reliably good predictors depend on the
output of your decision function. You are still free to choose between all the actions
in the act-space, but because of this mathematical relationship between you and the
predictor, her prediction depends on what you decide to do even in the absence of
any causal connection between you. Indeed, the prediction could have already taken
place before you made your decision or before you were even born. Subjunctive
dependence is, in this case, not time sensitive.
4.1
Subjunctive Dependency and Logical Counterfactuals
Formally, then, fdt requires agents to have a probability and utility function along
with a set of dependency hypotheses. This time, the hypotheses are of the form
^
a∈A
MyAlgorithm(input) = a  s
10. We borrow this phrasing from Andrew Critch.
11. We assume the procedure is deterministic here for simplicity.
12. To be clear, like cdt and edt, fdt tells the agent which actions to perform, not which
algorithm to have. It arrives at the recommendation, however, by considering potential
interventions on the agent’s algorithm.
10

where MyAlgorithm is the agent’s decision algorithm, s is some state of the world, and
input is a vector of inputs. The input vector contains parameters that may aﬀect
the agent’s decision, such as her probability function, utility function, observations,
and so on.
In other words, the agent’s dependency hypotheses are conjunctions of counter-
factuals about what would happen if her decision algorithm on a given input were
to output a particular action. Because other agents may have the same algorithm,
in the hypothetical scenario where her algorithm outputs a on input x, theirs does
too. More broadly, the state of the world can non-causally depend on the output of
the agent’s algorithm according to fdt’s dependency hypotheses.
One worry is that this notion of dependence seems to involve counter-logicals,
i.e., questions about what would be the case if a particular function were to produce
a diﬀerent output.13 This worry is correct. On the best speciﬁcation of fdt that we
know of, counter-logicals or counterpossibles as discussed in Bennett (1974), Cohen
(1990), and Bjerring (2013) are required. Indeed, we also need a notion of functional
similarity between distinct but related algorithms. If the predictor is running, say, a
decent but imperfect approximation of you, then the output of your algorithm and
hers should still counterfactually covary.
Unfortunately for us, there is as yet no full theory of counterlogicals or functional
similarity, and for fdt to be successful, a more worked out theory is necessary,
unless some other speciﬁcation is forthcoming. Nonetheless, exactly how these
counterlogicals work is a topic for another discussion. On all the problems we
consider in this paper, it’s clear what the dependency hypotheses should be for
fdt agents. The most important consideration is that whatever the right theory of
logical counterfactuals, multiple instances of the same function move in subjunctive
lock-step: if one copy were to output a on input t, then the others would as well.
4.2
FDT’s Notion of Expected Utility
Aside from the diﬀerence in dependency hypotheses, fdt is structurally very much
like cdt.14 The primary diﬀerence is that, instead of advising agents to consider the
causal consequences of what would happen if their limbs were to move diﬀerently, it
instead advises them to consider the consequences of what would happen if their
algorithm were to arrive at a diﬀerent verdict at the end of rational deliberation.
To home in on such a notion, we ﬁrst consider:
Ufdt1(a) =
X
s∈S
P(MyAlgorithm(t) = a  s)u(o[a, s])
The counterfactual arrow here  is now not meant to indicate physical (cdt-style)
counterfactuals, but instead to indicate the counterfactual based on her logical
dependency hypotheses discussed in the previous section. The t, as before, is a
vector of inputs fed into MyAlgorithm.
For the sake of speciﬁcity, we’ll primarily be interested in agents who themselves
follow algorithms that comply with fdt’s recommendations. Such algorithms take
as input a probability function, a causal graph that encodes the agent’s views on
subjunctive dependence, the agent’s utility function, and the set of the agent’s
observations. Because the latter two parameters will be speciﬁed in each decision
problem we discuss below, we’ll focus on the ﬁrst two parameters: the probability
function P and the graph G. (Here, G is similar to the causal graphs we have already
seen, except it also captures logical inﬂuence as in ﬁgure 3.)
Because the agent we’re interested in follows fdt’s recommendations, we use
fdt(p,g) as a variable indicating the agent’s algorithm when fed a full set of
13. For more on why counterlogicals appear to be necessary for a full speciﬁcation of fdt,
see Yudkowsky and Soares 2017.
14. On the authors’ favored version of fdt there are in fact a few more important
diﬀerences, but these need not concern us here. For details, see Dai (2009).
11

parameters. We then have:
Ufdt2(a) =
X
s∈S
P(fdt(p,g) = a  s)u(o[a, s])
Expected utility for the fdt agent is determined by considering what would happen
if fdt-style reasoning were to lead to a given act when fed parameters P, G, and so
on.
So that we can graphically depict these relationships, we’ll again use a Pearl-style
formulation of fdt so that our oﬃcial equation is:15
Ufdt(a) =
X
s∈S
P(s | do(fdt(p,g) = a))u(o[a, s])
(3)
fdt advises agents to perform the act that maximizes equation (3). Note that
although fdt tells agents which act to perform, the place where the agent intervenes
in her causal graph when she is considering her options is the node representing
the deliberative process that will determine her act—not the node representing the
act itself. That is, the agent thinks about possible future outcomes by considering
diﬀerent possible conclusions her rational deliberation might reach, not by considering
diﬀerent possible actions her body might carry out (though the latter can obviously
change based on the conclusion she reaches). In a slogan, FDT says: Intervene on
your algorithm, not on your action.
fdt(p,g)
Act
death
Outcome
V
Figure 3: A causal graph for fdt agents facing Death in Damascus, Random
Coin, and Death on Olympus. The agent intervenes on fdt(p,g), which controls
the Act variable.
4.3
FDT Distinguished from EDT and CDT
fdt, cdt, and edt all agree in scenarios where every correlation between the agent’s
action and an event in the world is caused by the agent’s action. They disagree
15. As with equation (2) for cdt, equation (3) is not fully general, since the agent may
be uncertain over various causal graphs. (See footnote 3.) Therefore, the fully generalized
equation for fdt is:
Ufdt(a) =
X
i
X
s∈S
P(Gi)P(s | doi(fdt(p,g) = a))u(o[a, s])
where Gi is a possible graph, and doi is the do-operator for that graph. Again, we avoid
this added complexity since the structure of the cases considered in this paper always
determines a unique graph.
12

fdt(p,g)
Act
Prediction
Box B
V
Figure 4: Newcomb for fdt agents. Changing the output of fdt(p,g) changes
both the agent’s action and the predictor’s prediction, which in turn aﬀects whether
there is money in the opaque box.
when this assumption is violated, such as in situations where the behavior of an
accurate predictor correlates with, but is not caused by, the agent’s action. fdt
gets diﬀerent verdicts from cdt in the cases we’ll discuss below, but also in more
standard cases like Newcomb’s problem (Nozick 1969):
Newcomb An agent ﬁnds herself standing in front of a transparent box labeled
“A” that contains $1,000, and an opaque box labeled “B” that contains either
$1,000,000 or $0. A reliable predictor, who has made similar predictions in the
past and been correct 99% of the time, claims to have placed $1,000,000 in
box B iﬀshe predicted that the agent would only take box B. The predictor
has already made her prediction and left. Box B is now already full or already
empty. Should the agent take both boxes (“two-box”), or leave the transparent
box containing $1,000 behind (“one-box”)?
On fdt’s view, the predictor decides whether to place money in Box B based on an
accurate prediction of the agent. We assume ﬁrst that the predictor’s prediction is
dependent in some way upon the agent’s algorithm. So, although the predictor’s and
agent’s actions are causally independent, they are not subjunctively independent ac-
cording to fdt, since they both are determined by the output of the same (or similar)
algorithms. Therefore, P(full | do(fdt(p,g) = two-box)) ≪P(full | do(fdt(p,g) =
one-box)). So, fdt recommends one-boxing. For the causal graph, see ﬁgure 4.
Note, however, that in Newcomb, as elsewhere, dependency between the predic-
tion and the output of the algorithm is required. Suppose, for instance, that 99% of
blue-eyed people take two-boxes, and the fdt agent is herself blue-eyed. Suppose
further that she knows the predictor actually makes her prediction based only on the
eye-color of the agent. In that case, the agent will not think there is any dependence
between her algorithm’s output and the prediction and will instead two-box.16
Because fdt gets the same verdict as edt in the primary cases we’ll be focused
on, it’s important to distinguish it from edt as well. To that end, we revisit XOR
Blackmail from the perspective of an fdt agent. In this case, the blackmailer
decides whether to send the letter based on (i) what the agent would do upon receipt,
and (ii) whether the agent in fact has termites. Furthermore, the agent knows that
the output of her algorithm has an eﬀect on what the predictor does but not on
whether her house is infested. If she gets the letter, she reasons: If I were to pay,
then that might change whether I would have gotten this letter, but it would not
change whether I have termites. If there are termites and I were to pay, I’d be out
a million and a thousand dollars. If there are not, I’d be out a thousand dollars.
16. fdt theorists, of course, need a more general theory of when a prediction is dependent
upon the agent’s algorithm as mentioned above.
Note, however, that since fdt uses
Pearl-style causal graphs, such dependencies can partially be read oﬀfrom the agent’s
conditional probabilities.
13

On the other hand, if I were to avoid paying and there are termites, I would lose a
million dollars. If there are not, I’d lose nothing. Therefore, not paying dominates.
More explicitly, in XOR Blackmail, there are four states of the world: (i) the
letter is sent, and there are termites (st), (ii) the letter is sent, and there are no
termites (st), (iii) the letter is not sent, and there are termites (st), and (iv) the
letter is not sent, and there are no termites (st).
Payoﬀs are determined by whether there are termites, and whether the agent pays.
Furthermore, although the exact counterfactual probabilities are not determined by
the set up, we know that paying does not counterfactually inﬂuence whether there
are termites. I.e., whether there’s in fact a termite infestation does not depend on
the output of the agent’s algorithm. So, P(t | do(fdt(p,g) = pay)) = P(t) and
P(t | do(fdt(p,g) = don’t)) = P(t). We then have:
Ufdt(pay) = P(st | do(fdt(p,g) = pay))u(st, pay)
+ P(¯st | do(fdt(p,g) = pay))u(¯st, pay)
+ P(s¯t | do(fdt(p,g) = pay))u(s¯t, pay)
+ P(st | do(fdt(p,g) = pay))u(st, pay)
= P(t | do(fdt(p,g) = pay))u(t, pay)
+ P(¯t | do(fdt(p,g) = pay))u(¯t, pay)
= −1, 001, 000P(t) −1, 000P(¯t)
(4)
On the other hand, by similar reasoning, we have:
Ufdt(don’t) = P(t | (fdt(p,g) = don’t)u(t, don’t)
+ P(¯t | (fdt(p,g) = don’t)u(¯t, don’t)
= −1, 000, 000P(t) −0P(¯t)
(5)
Since (4) is always less than (5), not paying is always superior. The eﬀect of paying
the blackmail is simply changing the conditions under which the letter is sent, which
is not a real improvement in the agent’s situation. fdt, then, is a genuinely new
theory, since its verdicts diverge both from those of cdt and from those of edt.
5
Cases
We now consider a variety of unstable cases, in each one contrasting the response
of cdt with fdt.
Each is designed to illustrate both the virtues of fdt and
cdt’s failure to recognize the counterfactual link between multiple instances of an
algorithm.
5.1
Olympus
First, consider the following case of causal dominance.
Death on Olympus You have three options. You can remain in Damascus, you
can travel for free to Aleppo, or you can pay $1,001 to climb Mount Olympus.
death ﬁxes a day when he will try to meet you. He predicts ahead of time
where you’ll be when you die, and if you are somewhere else then you get to
cheat death and live forever. The day before you die, death tells you I am
coming for you tomorrow. You value immortality at $1,000. However,
if you end up climbing Olympus and dying there, you get to speak with the
gods post-mortem. Such a conversation is worth $1,501 to you. So, on net,
between the cost of the climb and the chat with the deities, dying on Olympus
is worth $500, but surviving on Olympus is worth −$1.
14

Death
Aleppo
Damascus
Olympus
Carl
Aleppo
0
1000
1000
Damascus
1000
0
1000
Olympus
−1
−1
500
Table 1: The cdt agent’s payoﬀmatrix in Death on Olympus. Because death’s
location and Carl’s choice are causally independent, he considers climbing Mt.
Olympus a dominated option.
Suppose death tells Carl, the cdt agent, I am coming for you tomorrow.
What does he do?
Answer: he either remains in Damascus or ﬂees to Aleppo. To see why, ﬁrst
consider Carl’s payoﬀmatrix shown in Table 1.
death’s location and Carl’s location are causally independent. So, from Carl’s
standpoint, death is either in Aleppo, Damascus, or Olympus, and there’s nothing
he can do about that. Carl reasons that no matter where death is, both staying in
Damascus and ﬂeeing to Aleppo strictly dominate climbing Olympus (according to
cdt counterfactuals).
Since choosing dominated options is irrational, Carl will therefore end up in either
Aleppo or Damascus, where he is sure to die and receive a payoﬀof 0. Note that this
answer is mandatory for defenders of any standard version of cdt.17 Although some
readers may disagree with Joyce’s or Arntzenius’s solutions to unstable problems,
every version of cdt is committed to denying that going to Olympus is rational
because it’s dominated by both alternatives. cdt’s dependency hypotheses are
therefore hopelessly impoverished.
Fiona, the fdt agent, fares much better. For her, death’s location counterfac-
tually varies with her own choice. Since death uses the same algorithm she does
to predict where she’ll be, she thinks that if she were to go to either Aleppo or
Damascus, death would be there, and she’d consequently bring about a payoﬀof
0.18 On the other hand, if she were to go to Olympus, death would be there, and
she’d get a payoﬀof 500. So Fiona packs her gear, ponders the questions she’ll ask
the gods, and begins her climb.
Both edt and fdt arrive at the right answer, but only fdt does so for the right
reasons. When Fiona sets out for Olympus, that is good evidence that death will
be there, which is why edt recommends climbing. What fdt sees is that this is a
special kind of evidence whose link to death’s location is counterfactually robust.
Where Fiona decides to go determines where death goes, but for logical and not
for causal reasons. death eﬀectively has a copy of Fiona that he runs in his mind
before making his choice, and multiple copies of the same program are sure to get
the same result. cdt misses this dependency entirely, and edt considers it just
another kind of correlation.
5.2
Psychopath Button
We now turn to a much-discussed example from Egan (2007):
Psychopath Button You are debating whether to press the “kill all psychopaths”
button.
It would, you think, be much better to live in a world with no
17. The one possible exception we’re aware of is Wolfgang Spohn. 2012. “Reversing 30
Years of Discussion: Why Causal Decision Theorists Should One-Box.” Synthese 187 (1):
95–122, which defends a very non-standard version of cdt (that, e.g., prescribes taking
one box on Newcomb’s problem).
18. As before, death may make his decision in some way other than by running a copy
of her algorithm but that nonetheless depends on her algorithm. Since these alternatives
won’t aﬀect fdt’s verdicts in the cases here and below, we will ignore them henceforth.
15

psychopaths. Unfortunately, you are quite conﬁdent that only a psychopath
would press such a button. You very strongly prefer living in a world with
psychopaths to dying.
Psychopath
Act
V
(a) A graph for cdt agents.
fdt(p,g)
Psychopath
Act
V
(b) A graph for fdt agents
Figure 5: A graph of Psychopath Button for cdt and fdt agents.
From the cdt point of view (represented in Figure 5a), whether you’re a psy-
chopath or not is simply a feature of the environment you cannot change. However,
it inﬂuences both your actions (whether you press the button), and the outcome.
Although we did not plug in exact utilities this time, under a Joycean solution,
cdt seems to get this case close to right (pace Egan). The more conﬁdent Carl is
that he’ll press the button, the more conﬁdent he becomes he’s a psychopath. In
turn, he becomes more attracted to not pressing the button because he strongly
wishes not to die. This strong preference leads him to an equilibrium probability
with high conﬁdence that he will not, in fact, press the button. (Though he still
may toss a heavily biased coin and push the button with small probability.)
It’s less clear how we should model this case from the point of view of fdt, and
there are a variety of options.19 The most natural and illustrative, we think, is to
assume that what actions you would or would not perform in various (hypothetical or
real) circumstances determines whether you’re a psychopath. What actions you would
perform in which circumstances is in turn determined by your decision algorithm.
On this reading of this case, the potential outputs of your decision algorithm aﬀect
both whether you’d press the button and whether you’re a psychopath.
In practice, this leads the fdt agent always to refrain from pressing the button,
but for very diﬀerent reasons from the cdt agent. Fiona reasons that if she were to
press the button that kills so many people, then she would be a psychopath. She
does not regard her psychopathic tendencies—or lack thereof—as a ﬁxed state of
the world isolated from what decision she actually makes here.
This seems like the right reasoning, at least on this understanding of what
psychopathy is. Psychopaths just are people who tend to act (or would act) in
certain kinds of ways in certain kinds of circumstances. We can take for granted
that everyone is either born a psychopath or born a non-psychopath, and that
Fiona’s action cannot causally change this condition she was born with. Yet if this
condition consists in dispositions to behave in certain ways, then whether Fiona is a
psychopath is subjunctively tied to the decisions she actually makes. If you would
not perform A in circumstances C, then you also would not be the kind of person
who performs actions like A in circumstances like C. fdt vindicates just this sort of
reasoning, and refrains from pressing the button for the intuitively simple reason of
“if I pressed it, I’d be a psychopath” (without any need for complex and laborious
19. Our interpretation of Murder Lesion below is eﬀectively another way of modeling
Psychopath Button for fdt. On that alternative, whether you are a psychopath is not
determined by the cognitive algorithm you use to make decisions, but instead routes around
your reasoning process to (partially) control your actions directly.
16

ratiﬁcation procedures). When we intervene on the value of the fdt(p,g) variable,
we change not just what you actually do, but also what kind of person you are.
5.3
Murder Lesion
Lesion
Act
V
Shaky
Figure 6: A graph of Murder Lesion for cdt agents.
Finally, consider the following case discussed at length in Joyce (2012):
Murder Lesion Life in your country would be better if you killed the despot Alfred.
You have a gun aimed at his head and are deciding whether to shoot. You
have no moral qualms about killing; your sole concern is whether shooting
Alfred will leave your fellow citizens better oﬀ. Of course, not everyone has
the nerve to pull the trigger, and even those who do sometimes miss. By
shooting and missing you would anger Alfred and cause him to make life in
your country much worse. But, if you shoot and aim true the Crown Prince
will ascend to the throne and life in your country will improve. Your situation
is complicated by the fact that you are a random member of a population in
which 20% of people have a brain lesion that both fortiﬁes their nerve and
causes their hands to tremble when they shoot. Eight in ten people who have
the lesion can bring themselves to shoot, but they invariably miss. Those who
lack the lesion shoot only one time in 10, but always hit their targets.
For deﬁniteness, assume that the payoﬀs are as described in Table 2, with S denoting
the act of shooting, and L denoting the state of having the lesion.
L
¬L
S
−30
10
¬S
0
0
Table 2: Payoﬀmatrix for Murder Lesion, where S refers to the action of shooting,
and L is the proposition that you have the lesion.
The story for cdt agents is familiar. The causal graph for cdt agents is depicted
in Figure 6, and this case is equivalent to Death in Damascus for such agents with
only variables and numerical values changed. If you’re conﬁdent you’ll shoot, you
end up evaluating ¬S more favorably, which in turn provides reason not to shoot.
You then lower your credence you’ll shoot, eventually achieving equilibrium when
Ue(S) = Ue(¬S) = 0. In this particular case, P(L | do(S)) = P(L | do(¬S)) = P(L)
because the Act-node is a descendant of the Lesion-node. So, the expected utility of
shooting and not shooting are respectively calculated as follows:
U(S)
=P(L)u(S, L) + P(¬L)u(S, ¬L)
U(¬S) =P(L)u(¬S, L) + P(¬L)u(¬S, ¬L)
17

fdt(p,g)
Lesion
Act
Algorithm
Shaky
V
Figure 7: The graph for fdt agents facing Murder Lesion. Both the lesion and
the fdt algorithm partially determine which algorithm the agent actually uses to
decide.
Murder Lesion
Lesion
Yes
Algs
shoot
fdt(p,g)
Algn
abstain
fdt(p,g)
No
Algs
shoot
fdt(p,g)
Algn
abstain
fdt(p,g)
0.2
x
1 −x
0.8
1 −x
x
y
1 −y
y
1 −y
y
1 −y
y
1 −y
−30
−30
0
−30
0
10
10
10
Figure 8: One version of Murder Lesion, where fdt will advise agents to shoot.
Whether the agent has the lesion determines how likely she is to be pre-wired with
either Algs or Algn, which in turn determines how likely she is to either shoot
automatically, abstain automatically, or follow the verdict of fdt. The conditional
probability of moving from a node to a child is given on the edge between them.
The payoﬀs of each leaf are listed below it.
18

In equilibrium (i.e., for Pe), the two equations both take on the same value of 0 and
Pe(L) = .25.
However, note that once we’ve achieved equilibrium, the correlation between S
and L is no longer decision-relevant. That is, even though Pe(L | S) is not necessarily
equal to Pe(L), this stochastic relationship has no bearing on the agent’s decision.
What this means is that no matter how strong the correlation between shooting
and shaky hands, cdt agents will shoot with 25% probability. To bring out the
severity of the problem, imagine that the correlation were near perfect. Conditional
on shooting, you’re almost certain to have the lesion and miss, and conditional on
not shooting, you’re almost certain not to have the lesion but have steady hands. In
this case, not shooting guarantees a payoﬀof 0, while shooting guarantees a payoﬀ
of near −30. Nonetheless, cdt agents sometimes still shoot and on average receive a
payoﬀof −7.5. That cdt agents do predictably worse than agents who never shoot
in cases like this seems irrational.
It is not immediately clear how fdt handles this problem. One might think
that fdt’s answer could be calculated by a graph like Figure 6, with Act replaced
by fdt(p,g). If so, then it would reach the same answer as cdt, and the fact
that fdt mishandles this problem would call into question the diagnosis that cdt’s
fundamental problem is ignoring subjunctive dependence.
However, using that graph would set up a contradiction with the problem
statement. As deﬁned here, fdt(p,g) is a deterministic function. Once it is fed
a causal graph, utility function, and probability distribution, its output is fully
determined. But the arrow between the lesion and fdt(p,g) would imply that the
presence or absence of the lesion aﬀects the output of the (ﬁxed) fdt equations,
which is false. In other words, the Lesion-node cannot be an ancestor of the fdt(p,g)-
node. Therefore, in order for it to be possible for the graph to comply with the case
description, we require that the agent’s action not be fully determined by the output
of the fdt(p,g) algorithm alone.
At this point, the oﬃcial description of the case leaves underspeciﬁed exactly how
the lesion and fdt jointly end up aﬀecting the action. Thus the case is underspeciﬁed
from the point of view of the functional decision theorist, which explains the initial
lack of clarity. Critically, this is not true of a causal decision theorist who accepts
Figure 6 as a full speciﬁcation, which she might do because she believes she can
intervene directly on actions.
One Speciﬁcation of Murder Lesion
We will spell out one of many possible ways of ﬁlling in the details, which we hope
will be illuminating. However, diﬀerent formulations of the details can yield diﬀerent
problems, and thus diﬀerent correct solutions.
Assume that the lesion aﬀects two things: whether the agent’s hands are shaky,
and what the agent’s actual decision algorithm is. To make the case interesting,
we’ll assume that fdt gets some say as well. Its recommendation at least sometimes
determines what the agent’s actual decision algorithm does. Figure 7 provides a
causal graph of this scenario.
Suppose that as before, 20% of the population have the lesion and 80% do not. If
you have the lesion, you come hard-wired with one of two algorithms: Algs or Algn.
Algs automatically returns shoot with probability y regardless of what fdt says to
do. Similarly, Algn automatically returns abstain with probability y regardless of
fdt’s advice. With probability 1 −y, both algorithms return whatever fdt says to
do. Given the presence of the lesion, there’s an x chance Algs directly determines
your action, and a 1 −x chance Algn does. Given the absence of the lesion, there’s
an x chance Algn controls your actions, and a 1 −x chance Algs does. (See Figure
8.)
Under this speciﬁcation, the fdt algorithm is uncertain about whether the agent
has the lesion or not. Since y proportion of the time, the agent will not listen to
fdt regardless of what it says (because she is an auto-shooter or auto-abstainer),
those cases do not factor into fdt’s deliberations. The remaining 1 −y proportion
19

of the time, fdt does have control over the agent’s actions, in which case outputting
abstain adds 0 utility, but outputting shoot adds 2 −2y utility in expectation.
fdt therefore outputs shoot in this scenario. The higher y, the less likely fdt has
control, and the less fdt can contribute in expectation to her payoﬀ.
This seems to us like the correct behavior. Insofar as the lesion prevents fdt
from aﬀecting the agent’s actions, the decision of fdt is irrelevant. On the other
hand, insofar as fdt does control the agent’s actions, there’s no reason to take into
account any correlation between the presence or absence of the lesion and fdt’s
output, since that connection is rightly severed when we condition on the action
being determined by fdt. So, insofar as fdt has control over what you do, it should
only take into account the initial probability that you have the lesion, the graphical
structure, and how much better or worse it would be to kill, miss, or abstain from
shooting.
There are, of course, other ways of interpreting Murder Lesion. However, as
far as we can tell, none create a problem for fdt. Although agents who shoot may
end up worse oﬀon average than agents who do not, this is only because of factors
outside of fdt’s control and invariant under the actions a decision theory should
recommend.
6
Conclusion
cdt’s ability to distinguish causal links from mere correlations in its decision-making
is rightly recognized as an improvement over edt. Not every correlation between
act and state is decision-relevant. Instead, rational agents should only take into
account potential ways in which the world depends on the act they perform.
Unfortunately, cdt is blind to non-causal dependencies that can be crucial to
rational decision-making. When predictors have access to your decision procedure,
their prediction is logically at least partially determined by your action. That is
because multiple instantiations of the same function will return the same output on
the same input.
We sketch a theory of rational action, fdt, to be fully developed over the course
of a series of papers, which uses this idea of subjunctive dependency to correctly
prescribe rational actions across a wide range of cases.
The cases we used to argue for fdt and against cdt had, to our minds, relatively
unambiguously correct verdicts. These provide strong data in favor of the former
theory and, in turn, for the decision-relevance of logical dependencies. fdt also
diverges from cdt in more contentious cases like Newcomb. Given that intuitions
are strongly divided on that case, it cannot be counted as dispositive. However, if
fdt becomes the consensus theory, one-boxing should also become the consensus
strategy.
As we noted, fdt still faces a number of foundational issues. Regardless of
how fdt stacks up against cdt and edt, it remains the case that we need some
account of reasoning under uncertainty about tautologies and contradictions or some
reformulation of fdt that avoids these problems. Without such an account, fdt
will remain underspeciﬁed in important respects. These questions appear ripe for
further investigation.
References
Ahmed, Arif. 2014. “Dicing With Death.” Analysis 74 (4): 587–592.
Arntzenius, Frank. 2008. “No Regrets, or: Edith Piaf Revamps Decision Theory.” Erkenntnis
68 (2): 277–297.
Bennett, Jonathan. 1974. “Counterfactuals And Possible Worlds.” Canadian Journal of
Philosophy 4 (2): 381–402.
Bjerring, Jens Christian. 2013. “On Counterpossibles.” Philosophical Studies, no. 2: 1–27.
20

Cohen, Daniel. 1990. “On What Cannot Be.” In Truth or Consequences: Essays in Honor
of Nuel Belnap, 123–132. Kluwer Academic Publishers.
Dai, Wei. 2009. “Towards a New Decision Theory.” Less Wrong (blog), August 13. http:
//lesswrong.com/lw/15m/towards_a_new_decision_theory/.
Eells, Ellery. 1984. “Newcomb’s Many Solutions.” Theory and Decision 16 (1): 59–105.
Egan, Andy. 2007. “Some Counterexamples to Causal Decision Theory.” Philosophical
Review 116 (1): 93–114.
Gauthier, David. 1994. “Assure and Threaten.” Ethics 104 (4): 690–721.
Gibbard, Allan, and William L. Harper. 1978. “Counterfactuals and Two Kinds of Expected
Utility: Theoretical Foundations.” In Foundations and Applications of Decision Theory,
edited by Cliﬀord Alan Hooker, James J. Leach, and Edward F. McClennen, vol. 1.
The Western Ontario Series in Philosophy of Science 13. Boston: D. Reidel.
Jeﬀrey, Richard C. 1983. The Logic of Decision. 2nd ed. Chicago: Chicago University Press.
Joyce, James M. 1999. The Foundations of Causal Decision Theory. Cambridge Studies in
Probability, Induction and Decision Theory. New York: Cambridge University Press.
. 2012. “Regret and Instability in Causal Decision Theory.” Synthese 187 (1): 123–
145.
Lewis, David. 1981. “Causal Decision Theory.” Australasian Journal of Philosophy 59 (1):
5–30.
Meacham, Christopher J. G. 2010. “Binding and its Consequences.” Philosophical studies
149 (1): 49–71.
Nozick, Robert. 1969. “Newcomb’s Problem and Two Principles of Choice.” In Essays in
Honor of Carl G. Hempel: A Tribute on the Occasion of His Sixty-Fifth Birthday,
edited by Nicholas Rescher, 114–146. Synthese Library 24. Dordrecht, The Netherlands:
D. Reidel.
Pearl, Judea. 1996. “Causation, Action, and Counterfactuals.” In 6th Conference on
Theoretical Aspects of Rationality and Knowledge (TARK-’96), edited by Yoav Shoham,
51–73. San Francisco, CA: Morgan Kaufmann.
. 2009. Causality: Models, Reasoning, and Inference. 2nd ed. New York: Cambridge
University Press.
Savage, Leonard J. 1972. The Foundations of Statistics. Dover books on Mathematics.
Dover Publications.
Skyrms, Brian. 1980. Causal Necessity: A Pragmatic Investigation of the Necessity of Laws.
New Haven, CT: Yale University Press.
. 1990. The Dynamics of Rational Deliberation. Cambridge, UK: Cambridge Univer-
sity Press.
Soares, Nate, and Benja Fallenstein. 2015. “Toward Idealized Decision Theory.” arXiv:
1507.01986 [cs.AI].
Spohn, Wolfgang. 2012. “Reversing 30 Years of Discussion: Why Causal Decision Theorists
Should One-Box.” Synthese 187 (1): 95–122.
Yudkowsky, Eliezer. 2010. Timeless Decision Theory. The Singularity Institute, San Fran-
cisco, CA. http://intelligence.org/files/TDT.pdf.
Yudkowsky, Eliezer, and Nate Soares. 2017. “Functional Decision Theory: A New Theory
of Instrumental Rationality.” arXiv: 1710.05060 [cs.AI].
21

