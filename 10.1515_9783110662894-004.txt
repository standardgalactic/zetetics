Pleomorphism in  
Thought and the  
Computational Metaphor 
Seed, Symbol, and the “Grounding Problem”  
A Conversation between Oswald Wiener   
and Michael Schwarz

MS:   In our conversation I would like to refer to your sketch on 
the “computational metaphor,”1 which you wrote by the end of 
2018 to initiate a discussion.2 You intended the text to be a basis for 
a comparison of different formal descriptions of natural thought 
by means of concepts from computer science, which you tried to 
track in its history, with results from introspection. You begin by 
emphasizing that these attempts conceive of token or symbol too 
statically. 
OW:  Yes, with its symbol concept, computer science polarizes the 
dynamics of natural thought into static symbols on the one hand and 
transformation rules on the other. The symbols thus become the 
atoms of the respective formal system. They have the structure of 
their corresponding read-write-head only,3 and “symbol structures” 
are defined by the transformation rules only. If these rewrite rules are 
themselves implemented as token strings, then all strings have the 
same structure, namely the structure of any suitable universal 
machine. I wanted to call attention to this loss of structure.4 
MS:   If I understand you correctly, you aimed at extending the 
dynamics, which we experienced in introspection and then critically 
related to the all too static and sensualistic concept of mental image 
(as exemplified by several contributions to the book Selbstbeobach-
tung of 20155), to the question of “internal symbols” compared to the 
symbol concept in computer science. 
OW:  That’s right. 
MS:   Time and again we found, that even the image-like impression/ 
memory is, as Frederic Bartlett had already emphasized, pre- 
structured and schematic. Above all, it presupposes the temporally 
dynamic process of assembly. The reproduction of closed line 
sketches, for instance, depends on amodal, i.e., meta-sensory opera-
tions. I am thinking of my study on the PELOT figures in the book.6 
OW:  Any critical introspection immediately shows that connecting 
imagining to perceiving (emulation of perception) by positing its pic-
torial character, e.g., “thinking as the manipulation of images,” is mis-
leading. Instead, the often cited “vividness” of imagination basically 
102

103
depends on how many details the current running environment has. 
So it’s not a matter of richness of the mental image. 
MS:   In your glossary entry on the seed,7 you elaborated on such 
dynamic relations registered in introspection. They enable us to even 
understand and specify the historically first criticisms of the notion 
of mental image in the psychology of thought of the Würzburg School. 
OW:  Yes, you could see it that way. However, there adequate introspec-
tion to sufficiently illustrate the stages of expansion of seed phenom-
ena and their functional embedding, for example, is still lacking. Some 
specifications remain at least didactically helpful, such as the distinc-
tion between “implication,” “irritation,” and “quasi-image” ...8 
MS:   The second crucial point in your sketch pertains to the “ground-
ing problem” (Stevan Harnad) at the foundation of artificial intelli-
gence. Roughly speaking, the problem is how thought (or its formal 
description) relates to the environment, or rather how the formalism 
itself relates to the environment, especially if we don’t want to con-
ceive of the relationship as a mapping. 
OW:  The “grounding problem” is anything but only a problem of 
“purely symbolic models of the mind” (Harnad), that is, of symbol sys-
tems. It equally (and in an interesting way) affects “connectionism” 
too. Structures are always virtual structures residing in the head of 
the observer, who can at least attribute them to physical processes 
discerning tokens (at the level of the read-write head of a Turing 
machine) and strings (at the level of the transformation rules of a 
Turing machine). But s/he cannot relate these distinctions (“is 
accepted by transformation rule no. k”) to entities outside of the 
respective formal system. This would require other virtual structures, 
which the observer must assign to the strings. 
MS:  I find it very exciting to discuss these two aspects, since I have 
been trying for some time to trace the origins of this relation on the 
basis of Jean Piaget’s descriptions of the formation of “symbols” 
in children’s thought. While researching this topic I got surprised 
by Piaget’s interpretation of Sigmund Freud’s notion of “symbolic 
thinking” in some of his early writings on psychoanalysis,9 and also 

how close Piaget’s interpretation gets to the concept of seed phe-
nomena.10 
 
 
The computational metaphor 
 
MS:   Maybe we could begin with the computational metaphor. How 
do you think it relates to so-called Artificial Intelligence (AI) today? 
OW:  In a narrow sense, the computational metaphor has been 
advanced by many authors based on Allen Newell’s and Herbert A. 
Simon’s “Physical Symbol System Hypothesis” (PSSH). Additionally, I 
also include all attempts to create mock intelligence by means of sta-
tistics under the general heading of “shallow formalisms” because 
they shallowly map the causal relations of human thought onto a for-
mal description. These include the “neural network” hypothesis, 
information theory, and Bayesian philosophy, and even more so the 
project to save the PSSH by combining it with artificial “neural net-
works” to hybrid models. 
MS:   Nevertheless, you believe that automata theory — and the PSSH 
is based on the idea of a universal formal system — is crucial for the 
understanding of productive thought because it provides a frame of 
description in terms of effective procedures (in the form of Turing 
machines) as epitomized in your definition of structure.11 
OW:  Yes, the notion of formal system is immensely important for con-
ceptualizing human thought. Both working on the construction of for-
mal systems and symbol manipulation are characteristic activities of 
human intelligence. Obviously, the notion itself derives from the 
observation of human behavior — in particular from idealizing regu-
larities, which the individual grasps in his or her own thinking. If you 
analyze any achievement of human beings, no matter what kind, you 
are forced to conclude that we build machines in the sense of Turing. 
That doesn’t necessarily mean that these machines take on the form 
of a Turing machine. Nonetheless, it is the biggest flaw of artificial neu-
ral networks so far to be unable to construct formal systems. 
104

105
MS:   Do you mean the fact that such nets cannot produce Turing 
machines by means of their method of “learning”? And do you want 
to contrast the creation of recursive procedures performed by some-
one observing the sequence of events in the machine and relating it 
to its environment with the statistical mapping methods you dubbed 
mock intelligence? 
OW:  Yes, exactly. The creation of recursive procedures is the core fea-
ture of human thought. Yet, the question remains, whether the appa-
ratus creating these constructs and manipulating them is itself a for-
mal system (which seems be the most obvious hypothesis in the sense 
of Occam’s razor). To what extent can it be described as such a system 
and how detailed would the description have to be so that it performs 
equivalently (as a program of a universal machine)? 
 
 
Alan Turing’s Behaviorism 
 
MS:   By referring to the observation of human behavior, you alluded 
to Alan Turing and, in particular, to his “discovery” of the universal 
machine relating machines to thought? 
OW:  Yes. More specifically I mean section 9 of his paper “On Comput-
able Numbers.”12 There, Turing talks about a man who computes. He 
indicates clearly that he himself observes this “computer” from the 
outside, thereby realizing what he is doing in behaviorist fashion. He 
observes how the computer reckons with pen and paper, draws sym-
bols on the paper, reads and rewrites them — while multiplying two 
numbers, for instance. What is happening inside the computer, Turing 
does not mention. But I believe nonetheless, that of course there was 
also a big portion of introspection involved. 
MS:   You mean Turing overlooks the cognitive experiences we our-
selves found and described in living thought? Even while multiplying 
two two-digit numbers you can make a wealth of observations, which 
should, when inspected more closely, totally amaze you. This is lack-
ing in Turing’s paper … 

OW:  Generally speaking, behaviorism developed during radical times 
in psychology. So Turing didn’t want to talk too much about “states of 
mind.” Of course, the notion of state of mind is indeed dubious. That’s 
why he replaced it by the table of a Turing machine. Yet, we have to 
emphasize that he does not say a word about how the notes of instruc-
tions in the table come about. In short, he does not say anything about 
the creative aspect of formalization and how it looks like from the 
inside. Turing said and wrote not one word about it, in none of his 
writings. 
MS:   Another essential aspect you keep emphasizing is that Turing 
decomposes the behavior of the computer, or rather his computa-
tions, into static “symbols” and dynamic “operations.” These symbols 
and operations are supposed to be so elementary that you cannot 
decompose them further. 
OW:  I believe that the concrete steps of the run of a Turing machine 
are overly concrete in a way. In any case, you need some ingenuity in 
order to implement, say, the search-and-find of an analogy. Yet, in 
human thought this kind of recursivity is obviously a primary and 
main feature. 
MS:   So an analogy is recursive because it is repeatedly applied to sim-
ilarities across several phenomena. As humans we are able perceive 
analogies and use them instantly. But we would have to explain how 
analogies and detection are to be implemented. 
OW:  Yes, because that is a fundamental feature of the human mind. 
But however the implementation takes place, this does not affect the 
fact that the implemented process must result in the representation 
of a recursive procedure. Therefore, my definition of structure still 
stands. It is not devalued by the fact that in introspection we do not 
find structures as concrete ready-to-use processes. So it would be nec-
essary to sketch what one should do in order to find something cor-
responding to recursivity and to the concept of structure in introspec-
tion. 
MS:   Certainly one problem is Turing’s convention of description. Do 
you believe that, if we introspect objects or internal proxy objects and 
106

107
then try to describe them, recursion must inevitably occur in the 
description. Like: “there on the top right is something similar as 
further down” etc.? 
OW:  Exactly. In introspection we do discover sub-modules for multiple 
use just like in computer programs. If the concept of the Turing 
machine is a description of the notion of computation, this is a for-
malization of observable processes, as Turing observed in a person 
who computes. But tacitly he also relied on formal descriptions or 
assumptions, which aren’t observable in the external word and, there-
fore, not in a behaviorist fashion. The point is that Turing’s description 
is lacking this internal component about what is going on in the head 
of his computer. That is missing completely. 
MS:   I think there is a good way to observe this primary factual “effec-
tiveness” of living thought we experience before we formalize it by a 
symbol system. Like everybody else, Turing initially had to learn the 
application of his general descriptive convention for machine opera-
tion on strings. Only after that was he able to perform any arbitrary 
run of a special machine presented to him (in the form of a table) on 
any tape (with pencil and paper). Consequently, he had developed a 
scheme of action corresponding to a Universal Turing Machine (UTM). 
He had become a UTM. 
OW:  That is entirely correct. Manuel Bonik, Robert Hödicke, and I 
already wrote about it in our introduction to the theory of Turing 
machines.13 I used the argument you just brought up to prove by way 
of the Church-Turing-thesis that a universal machine must exist. The 
proof goes like this: I myself am able to run any Turing machine table, 
so I am able turn myself, so to speak, into any arbitrary effective 
machine. And since the way I handle the tables is effective, there must 
be a machine that does the same. 
MS:   Yes, I got the example from your book! I mentioned it, because it 
clarifies that first I formed the universal procedure as an embodied 
action scheme. Then I understood this action scheme intellectually, 
before I was able to formalize it into a general descriptive convention 
on the basis of introspections and a lot of effort. After all, you did just 

that in order to be able to write your textbook. You wrote down (pro-
grammed) a UTM in the form of a table, which then operated within 
your convention on that section of the string intended to represent 
the Turing machine “tables.” The rules therein are applied to the other 
section of string intended to represent the input of the computation. 
Yet, as you just indicated, the steps you translated into this convention 
are “overly concrete” because the fairly simple sensorimotor content 
of an elementary action, e.g., replacing one token on the tape with 
another, has to be broken down into quite a few smaller mechanical 
sub-steps on the linear tape. It requires a lot of concentration to even 
recognize these original processes in the run of your UTM, although 
they seem so simple to us. 
OW:  That’s right! Universality follows from the basic idea that you can 
code the description of the machine on a tape, and that there exists 
this universal machine, which is only universal because one can write 
any description of any procedure or of any calculation on the tape or 
indeed of any finite description out of the recursively enumerable infi-
nite set of descriptions already existing or yet to be invented. That’s 
why the machine is “universal.” But to represent a universal machine 
is only one of our manifold human abilities. 
 
 
The symbol concept of the Physical Symbol System 
Hypothesis (PSSH) 
 
MS:   That’s right. Moreover, the operations of a UTM are not even com-
plex. But you just referred to Newell and Simon, whose definition of 
the PSSH aimed at suspending the strict separation of data into tokens 
and processes. So they wanted to extend the dynamics to the “control 
system” itself. Practically, this means to permit the rule-governed 
change of the TM tables. 
OW:  Yes, I refer to Newell’s and Simon’s definitions of the concept of 
symbol using the computational metaphor, as exposed especially in 
their 1976 paper.14 The state of the art in artificial intelligence at that 
108

109
time had been physically realized by LISP machines that were specif-
ically optimized to run LISP programs. Their typical symbols were 
John McCarthy’s symbol expressions (S-expressions), which he had 
developed together with the LISP programming language. Some years 
ago, I held a seminar introducing “PC-Scheme,” a current LISP dialect, 
to students. 
MS:   Yes, this is a purely procedural language in which everything is 
formulated in bracketed expressions. The expressions are “lists” nested 
in parentheses containing arbitrarily chosen symbols (atoms). This 
“structure” (binary trees) specifies the order in which a mechanical 
“interpreter” computes the lists. Since the contained symbols are able 
to denote expressions, strings, and also procedures to transform ex -
pressions (of the lists), they are no longer “symbols” in Turing’s 
sense, because they can also be considered pointers to operations 
(machines). Turing’s symbols neither designate TM tables nor point 
to operations producing, copying or transforming them! 
OW:  You see, that is how Newell and Simon must have understood the 
notion of symbol at that time. As an internal representation (in the 
physics of the machine), these expressions must be retrievable and 
modifiable as “list structures” and the symbols as “pointers” to other 
expressions, implemented as memory addresses in a random access 
memory. Thus, processes for generating and modifying expressions 
can already be carried out as transformations of the pointers. The 
symbols are nothing other than variables for the operations possibly 
referring to other expressions and operations. Since the symbols must 
be universally applicable, they must not have any additional function 
(no meaning) beyond simply referring. They must only be distin-
guished from each other to be unambiguously assignable. Put simply, 
the arbitrary symbols within this metaphor are both pointers to oper-
ations and the operands of operations. 
        Of course, the physical symbol system has to be an instance of a 
universal machine. So Newell and Simon hypothesized that such a sys-
tem has the necessary and sufficient means for general intelligent 
action, as they put it! 

MS:   This remains a matter of discussion to this day. So the hypothesis 
is that any general intelligence, also in the form of “artificial” machines, 
must necessarily be based on the very class of symbol systems we just 
defined, and that every adequately organized (correctly programmed) 
symbol system of this kind is sufficient to display intelligence! The little 
word “action” indicates that its interaction with the environment is 
also intended. But isn’t this formulation a bit shortsighted from a psy-
chological perspective? 
OW:  In my opinion, it would be a mistake to deny any merit to this 
kind of Gordian solution attempted by Newell and Simon. It helps my 
understanding of conscious processes in natural thought about as 
much as the idea of “neural networks” helps my understanding of the 
“encapsulated” sensory domain. This is no small feat, if I take into 
account the evolution of my views while dealing with these hypoth-
eses. Yet, apart from the engineering problems of implementing them, 
which would probably determine their success or failure, they omit 
the crucial part of the more abstract problem, namely all the parts 
between the first stages of perception and the use of language, which 
are so essential to natural intelligence. 
MS:   That was precisely the reason why I dedicated myself once more 
to the psychogenesis of the symbol in the child to write a separate 
chapter for this book.15 So far we do not have much more than a uni-
versal framework for description, the core of which is captured a little 
differently in your own definition of structure. After all, the formalism 
producing such recursive processes is not yet included in the defini-
tion of the PSSH. And if I interpret the more speculative parts of it 
generously, these processes are understood (in anticipation of an 
implementation) as “sensorimotor” actions denoted by symbols, the 
dynamics of which should also serve to adapt to the environment. Yet, 
nowhere do I see an adequate psychological model for the transition 
from these behavioral processes to the processes of imagination and 
thinking. 
 
 
110

111
Formal description and AI – Searle’s problem  
of “causal powers” 
 
OW:  That’s why I speak of procedures only as far as they are clearly 
captured! So now our problem is exactly the problem of the cognitive 
genesis of structure. Like many other problems it can be traced back 
to one question: How far must formal description go in order to 
achieve the same as the formalized system? 
MS:   You mean, how far does the formal description of the processes 
seen at work in human thought or, say, the processes attributed to 
thought by neurology, have to go in order for the formalized to per-
form equivalently to human thought? 
OW:  There are two methods to achieve artificial intelligence. One is to 
describe thought while expecting that the description — which of 
course would have to be very concise and consistent to be trans-
formed into an executable computer program — is performing just 
like the thing described. In contrast, the other method is to formalize 
an idealized description of neural information processing. It seems 
that psychology has engendered the PSSH, while, in turn, the neural 
net hypothesis (NNH) inspires psychological theories mathematically. 
MS:   Some linguists still believe that thinking is a matter of words, i.e., 
of symbols, and that insight is structured in language. But if thought 
consisted only of sentences to be transformed into new sentences via 
transformation rules, and the new sentence is supposed to be the 
result of thinking, then it seems easy to believe that the simulation 
performs equal to the simulated, e.g., such as in Joseph Weizenbaum’s 
“psychotherapy” program ELIZA. But between a question and the 
answer there is a cognitive process engaging all of my brain resources 
by regrouping my entire orientation. 
OW:  Against this background, I would like to mention John Searle’s 
notion of “causal powers.” By the latter I, and probably Searle too, 
don’t mean anything metaphysical at all. The “causal powers” seem 
to reside in a level not yet co-formalized — in a carrier level, so to 
speak. Regarding this, we are used to thinking about objects by 

decomposing them into levels: molecules, then further into atoms, 
and so on, for instance. Then we stipulate one of these levels to be 
causally effective for a given problem. But we can ask: How does the 
underlying level, which seems to produce the laws of its upper level, 
function, and so on? 
        In his book The Sciences of the Artificial, Herbert A. Simon 
described such decompositions into levels as something we apply to 
everything we see.16 As examples, he mentioned the levels of 
nucleons, atoms, molecules, molecular complexes, cells, or the hier-
archies of social organization, in which the stronger social binding 
forces, e.g., the emotional ones, are used up in the smaller units. For 
this, Simon found an ingenious simplification. 
        He thought that this is due to the fact that there are essentially 
different kinds of forces, such as in physics there is the strong nuclear 
force and the weak nuclear force as well as the electro-dynamic force 
and gravity. These four are, I believe, still the four basic forces of 
physics. Simon now believed that the strongest binding forces neu-
tralize each other to form an atomic nucleus. The forces effective at 
this level hold the elementary particles together so that one does not 
notice anything of them at the next level, which is the level of the 
weak nuclear force. Similarly, electrodynamics does not apply at the 
level of the gravity at all, but rays of light are deflected by a mass. At 
this level, it becomes irrelevant what light rays or matter jets consist 
of. And since the consumed binding forces do not penetrate to the 
upper levels, you are able to describe each level separately by its 
respective “shallow” formalism. 
        Yet, this model describes a basic feature of our operational think-
ing. As we apply it to everything, I think, one should scrutinize it more 
precisely. This way of thinking just creates the hierarchies defining the 
elements to be made visible by measurement methods and to be 
arranged by the respective formalism. It seems to me that these are 
essential problems. 
MS:   Yes, you had already interpreted not only Turing’s test this way17 
but also symbolic AI research, which tries to clarify intelligence at the 
112

113
isolated level of a “language,” thereby ignoring “deeper” levels such 
as the thought relations experienced in introspection. They then 
implement searches for relations, which, as you say, generate a correct 
answer in the form of a sentence defined as a subset of all possible 
sentences within the linguistic domain. So your point is, that explicitly 
observable phenomena of thought such as seeds or even words, and 
what is implicit, i.e., what is implied by my orientation, cannot be 
decomposed easily into levels to be formalized independently. Do you 
think that Searle means something similar with his notion of “causal 
powers”? 
OW:  The fact that we are thinking in this way, conceiving of things in, 
say, effective levels that are hierarchically ordered, is obviously inher-
ited. Maybe this is the reason why we are unable to understand some 
crucial relations. The ideal model for such a decomposable system is 
the layered structural concept of computer science, where each layer 
performs its own elementary operations. On top of the machine code 
sit the assembly languages, on top of which the higher programming 
languages up to normal user programs operate. But in introspection 
not all our ideas appear decomposable. Again and again we experience 
surfaces, whose “main body” remains unconscious. In the essay you 
mentioned I said:18 We have to assume that a large part of the work of 
consciousness consists of attempts to represent these surface points 
of a model, whose largest part remains unconscious, as a decompos-
able system. But this is not a property of language, but of the models. 
 
“Causal powers” 
By introducing the concept of “causal powers,” John R. Searle initi-
ated a discussion on the scientiﬁc foundation of the concept of 
“intentionality,” which has been widespread in phenomenology 
since it was ﬁrst formulated by Franz Brentano. Searle locates these 
causal powers – as a necessary condition for “intentionality” – in 
the chemical and physical processes that, as “a product of causal 
features of the brain,” exist only in humans and animals. Since these 
“causal powers” are found only in a “certain sort of organism with a 

certain biological structure, and this structure, under certain con-
ditions, is causally capable of producing perception, action, under-
standing, learning, and other intentional phenomena.” 
      In his 1980 paper “Minds, Brains, and Programs,” Searle didn’t 
aim at clarifying the concept of intentionality, but at juxtaposing 
intentional phenomena, which are supposed to be known by every 
human consciousness, in particular the phenomenon of understand-
ing (a language, for instance), with the execution of purely formal 
processes (e.g., translation following formal rules). Based on his 
famous “Chinese Room Argument,” he pleaded that machines with-
out such “causal powers,” i.e., machines operating exclusively by 
computational processes over formally deﬁned elements “instanti-
ating a computer program,” are principally unable to generate some-
thing like intentionality. A computer program alone does not need 
the same “causal powers,” but only enough of them to induce the 
next level of formalism, when instantiated. Therefore, according to 
Searle, the causal powers of a formalism can also be implemented 
by all sorts of materials, just as Joseph Weizenbaum sketched out 
how to construct a computer using a roll of toilet paper and a pile of 
small stones. But all powers beyond that, e.g., the ones necessary 
for understanding, the formalism cannot reach. Thus, only a machine 
having the biological structure of a brain is able to produce them. 
      “And that is why strong AI has little to tell us about thinking, 
since it is not about machines but about programs, and no program 
by itself is sufficient for thinking” (Searle 1980: 417). 
 
Searle, John R., 1980. Minds, Brains, and Programs. The Behavioral and 
Brain Sciences, 3, 417–457. 
MS 
 
MS:   Some pioneers and supporters of the PSSH, e.g., Nils J. Nilsson, 
wrote that Searle’s criticism of the meaninglessness of formally 
manipulated symbols and of the lacking foundation of symbol sys-
tems is unjustified. I suspect, however, that these reactions fail to take 
114

115
into account that his objection was not only aimed at the lack of inter-
action between a physical machine and its environment, but at the 
nature of their relationship (“intentionality”)! This is how I understand 
why you also include an “observer” in your model. Now do you 
believe that the main question is: Down to which carrier level and 
which biological structure must our understanding and formal 
description of intelligence reach? 
OW:  Logic represents a formal description of the very highest level, 
so to speak. Logic does not think. It will never think, because it is an 
abstraction on a level lacking Searle’s “causal powers.” What is the 
level directly below? 
MS:   Well, we have been working at this level for years. It’s the level of 
experiences, which we observe in introspection (attunements, seeds, 
quasi-images, and so on). It is also significant that, according to Piaget, 
purely formal thinking constitutes the last developmental stage and 
that symbolic pre-conceptual as well as intuitive thought develops 
much earlier. I also hesitate to locate the formal description of neu-
rons by artificial neuronal networks (ANN) at the level directly below 
without first considering the complex issues of biological and psycho-
logical structure. 
OW:  The question is this: Down to which level do you have to go with 
your description, so that the description thinks by itself? It seems 
clear that we don’t have to go down to the individual atom, but I’m 
just guessing. I am not completely, but ninety-eight percent, sure that 
we don’t even have to go down to the big molecules. But then we 
might have to go down to biopsychology and nerve cells. I am sure 
you know that huge efforts are currently being made in this direction. 
On the other hand, Michael Nielsen, for example, talks about billions 
of “units” in a single artificial network. I bet that this technological 
development will still go on for a while. The technological equipment 
will get even cheaper and even faster, and even smaller... At the end, 
parsimony — both in Occam’s sense and in the sense of material econ-
omy — could be simply out of the question. Then, understanding intel-
ligence and recursivity won’t be a scientific goal any more. 

MS:   At this point, let me pin down that through introspection you 
have taken up the topic of “symbols” in thought again, that is, the topic 
of seed phenomena, because it is impossible to reduce such phenom-
ena to the PSSH’s narrow definition of symbol! There, symbols are 
arbitrary and function as pointers, which make them unsuitable for 
descriptions of introspections. Furthermore, the question of ground-
ing these systems remains unanswered. 
OW:  Yes! Well, the dynamics of the processes — including assembly, 
scaffolding, even our notion of attunement — are far too complex to 
be reduced directly to the symbol concept of the PSSH. Without a 
model of the “symbols” in human thought these theories remain 
stranded. In this regard, I would later like to add something on the 
topic of how to approach the unfolding of “inner symbols” via boot-
strapping. 
        But again, this is not to say that the problem cannot be formu-
lated at all on the basis of a PSSH. This is exactly what the “Church-
Turing thesis” amounts to, which in my version goes like this: Every-
thing that I understand clearly, I can represent as a Turing machine, 
and thus also in a von Neumann computer, which is in fact a Turing 
machine. This implies that, if I understood exactly what thinking is, 
then I should be able to implement it with a von Neumann computer. 
Conversely, if my program cannot think, then I have not yet under-
stood (and formalized) thinking adequately! 
MS:   Yes, especially if you also consider the formalized portion of 
thinking in its interrelation with the environment. In the 1990s, 
Searle’s criticism and this question of the “causal” interrelation with 
the environment made connectionist networks fashionable. Until 
then, this method had been somewhat ridiculed by “symbolic AI,” but 
from then to the present day it has been on a “triumphal march” 
based on a wave of technological progress. “Artificial Neural Net-
works” began to “recognize” objects in photos, steer automobiles, and 
play out their superiority over humans in board games and quizzes. 
This certainly leaves an impression on the general public. 
 
 
116

117
Artificial Neural Networks (ANNs), hybrid models of 
thought, and the “grounding problem” 
 
OW:  It was clear from the outset that the performance of today’s neu-
ral networks would not be sufficient for intelligence. Of course, pat-
tern “learning” and “recognition” is a part of intelligence, but not with-
out rhyme or reason like here. First of all, it is clear even to everyone 
tackling ANNs optimistically, i.e., especially to the people who make 
money with them, that human thinking does not follow a gradient 
descent algorithm. I think this is evident, and it is already a serious 
disadvantage of the neural network metaphor for intelligence. 
MS:   But scruples about the “grounding” of symbol systems being 
decoupled from the environment, as well as about the lack of coordi-
nation of ANNs triggered the development of hybrid systems trying 
to circumvent Searle’s criticism. ANNs should play the subsymbolic, 
causal part providing both “meaning” and “grounding” to the arbi-
trary symbols of the PSSH part. Thus coupled with the environment, 
these systems should be able to yield higher “symbolic representa-
tions.” But the metaphor is misleading, as you suggested, and the 
“grounding” of ANNs remains absolutely inexistent. In my opinion, 
the statistical evaluation of sensory data does not yet enact causal 
experiences, which are necessary for any formation of grounded sen-
sorimotor schemas. Moreover, phylogenetically acquired as well as 
inherited schemas enabling intentional action are still missing. Rep-
resentations binding together sensory “elements” only cannot be 
expected to be able to anticipate the behavior of objects. 
OW:  Most of these researchers refer to a paper by Stevan Harnad pro-
posing hybrid machines.19 If my interpretation is correct, Harnad still 
meant that all these processes, which he called bottom-up processes, 
can be implemented by connectionist networks. These include not 
only sensory invariants, but also “categorical representations,” which 
can be trained to identify objects as members of a category. All these 
processes should yield symbols conceived of as mere names, but 
which should suffice as “symbolic representations.” Actually, this 

sounds a bit naive. I don’t see any “representation” in such a system 
at all. All information is lost, and what I understand by representa-
tions, i.e., what I have called “models,”20 is entirely missing in this bulk 
of literature. 
        In this context, we must also mention Geoffrey Hinton, because 
he had concurrently pointed out the almost insurmountable diffi-
culties inherent in such a transition from parallel to recursive pro-
cesses, if the ANN approach is taken seriously. 
 
Hybrid models of cognition 
After the criticism of Searle, who measured up freestanding symbol 
systems to the “causal powers” (and the performance) of animal and 
human consciousness (intentionality), researchers at the end of the 
1980s proposed hybrid upgrades to the concept of symbol. The arbi-
trary shapes of a symbol system’s tokens were supposed to be 
“grounded” by a non-symbolic, causal nexus to the environment. In 
1990 Stevan Harnad, for instance, outlined a hybrid non-
symbolic/symbolic model of the mind. 
       According to Harnad, non-symbolic representations also com-
prise of the sensorially invariant features of object and event cate-
gories (both innate and learned) computed in a bottom-up manner. 
Built on connectionist networks (such as in trained image rec-
ognition), his model yields symbols enabling iconic representation 
by distributed processes. For Harnad, it is only the symbol designat-
ing the representation that allows for the comparison of sameness, 
distinctness, or the degree of similarity of objects. However, Harnad 
thinks that for categorical representation (identiﬁcation of an 
object), icons are useless, since they must ﬁrst be selectively 
reduced to invariant features in order to reliably distinguish objects 
as members of a category from non-members. According to Harnad, 
this selective abstraction process would also lie within the technical 
scope of connectionist processes. 
       For Harnad, the elementary symbols computed by bottom-up 
processes are mere “names” to be used by the higher-order opera-
118

119
tions of symbolic thought. Their “intrinsic meaning” refers to the 
objects and categories of objects, from which they originate. For rep-
resentations to be recognized as symbolic, the criteria of a “physical 
symbol system” must be met. Among others, the symbols at this 
level can be arbitrary and thus denote anything, but have to follow 
explicit rules that are part of a formal system. Additionally, their syn-
tactic manipulation must be semantically interpretable. 
       It remains unclear whether Harnad, by equating “intrinsic mean-
ing” with Searle’s “intentionality,” implies that the “causal,” “non-
symbolic” processes already effect the aforementioned “causal 
powers.” It also remains doubtful whether Harnad’s notions of iconic 
and categorical representation, which yield symbols as mere 
“names,” sufﬁce for object reference, especially when the objects 
referred to are out of the sensors’ ranges. Furthermore, it is difﬁcult 
to judge whether such symbols allow for becoming aware of one’s 
own intermediary means in goal-directed action, which, e.g., Jean 
Piaget postulated as a necessary condition for “intentionality.” 
 
Harnad, Stevan, 1990. The Symbol Grounding Problem. Physica D, 42, 335–346. 
MS 
 
MS:   You mentioned that the physical symbol system must be an 
instance of a universal machine. You also mentioned that Turing con-
ceived of the UTM and its rewrite rules as a common structure of all 
strings on the tape. Why do you think this is relevant? 
OW:  It is relevant because there is no other structure!21 
MS:   In this regard, you say that an “observer” is necessary, who relates 
any formal system to entities outside of it. 
OW:  Yes, entities outside the formal system just are the grounding 
problem! How does a formal system relate to “reality,” so that it plays 
the same role as a cognitive system or, better say, as in man. Harnad 
tries to construe this with rather casual methods. But then any effort 
to understand starts at a simple level … You have to ask the correct 
questions. How is human language founded on reality? How does this 

come about? Questions like these we have been asking ourselves for 
a long time: What is an object? What takes place if you say: This organ-
ism “internally” manipulates a proxy of an object? 
MS:   I agree. In my view, Harnad doesn’t realize that it’s he himself who 
relates his “causal” “nonsymbolic mechanisms” to an environment 
and not the mechanisms of his “symbolic representations.” He does 
not even intend that, not least because in his sketch there are no inter-
nal models at all (no representations in your sense), which could be 
related to the environment. 
OW:  I think, grounding simply means that a human being, who is 
grounded in or based on reality in the aforementioned sense, must 
have a representation of this reality. And the fact that this represen-
tation is similar to or, in some respects, even isomorphic with reality, 
which can be judged by a third person, is already sufficient to satisfy 
this grounding. 
MS:   I also have the impression that in Harnad’s model the entire 
content of the “representation” is lost as soon as its object is no longer 
within sensor range. In one example, he speaks of a “symbolic rep-
resentation” of a “zebra” produced by a linkage of the symbols “horse” 
and “stripes,” if these two symbols are “subsymbolically” grounded. 
This means that for him they are grounded both in an “iconic” and a 
“categorical” way, that is, they are recognized and assigned to their 
class by ANNs. Then he is talking about the fact that a person, who has 
never seen a zebra before, but who possesses the grounding of both 
elements as well as their linkage, will recognize a zebra with the help 
of this “symbolic representation” even at the very first encounter. But 
he doesn’t mention whether or not he could put a pencil or a piece of 
clay into the “hands” of this physically grounded symbol system, 
which should then, even without ever having experienced a zebra, 
draw or form a clay model of this linkage. Could it even get a vague 
idea of this combination? Something quite normal for a human being 
is missing here! 
OW:  Yes, that’s right. To have a proxy of an object means to have a rep-
resentation of the object just in the sense I have described in my glos-
120

121
sary entry figurative.22 You have to have processes of type 1 and of 
type 2 within a running environment. That already suffices to make the 
emerging object become a proxy. Therefore, a being that doesn’t have 
the same depth of representation as I do — of this lamp, for example — 
cannot have the lamp as a proxy in this sense either. But does a dog 
have the lamp as a proxy? That is hard to judge, but I suspect, to a cer-
tain extent, yes! The dog will not only recognize the lamp, but it will 
perhaps even be able to do something with it. Now what about a fly? In 
this case, I am rather inclined to say a fly does not have objects as proxy. 
And I am totally sure that until this day no computer has one either. 
MS:   Yes, and it is just this mental model which the “observer” can 
relate to the event of coming up against a zebra in reality! Naturally, 
he or she can judge the adequacy or inadequacy of this model by ref-
erence to the real zebra. 
OW:  In a certain sense, a fly is also grounded in reality, of course. It 
can also handle reality, but it is questionable whether one should use 
the philosophical word “grounded” for an organism, of which one 
does not expect to have ideas, for example, and to which one there-
fore does not attribute thought. In any case, I wouldn’t say categori-
cally that you must not call a housefly grounded. I suppose ground-
edness is gradual. The fact that my behavior fits the world is already 
a matter of groundedness. I can deal with the world by somehow mak-
ing it compliant to my goals. At least, I can try to. And I fit into the 
world insofar as I can predict what will happen. I can imagine, for 
example, what might happen, if I drive carelessly down an icy road. 
All of that is part and parcel of my representation of the external 
world. If I have such a representation, then my thinking is grounded 
in reality. 
MS:   Harnad’s model already fails on the basic level because the 
content of the symbols is lost. As we have just said, we cannot com-
pare its symbols with what one could hesitantly call “symbol” in 
human thinking, namely all seed phenomena. 
OW:  Well, the problem becomes evident in artificial neural networks. 
Let me briefly explain. It is claimed that in ANNs information is dis-

tributed, but the computation surprisingly yields a discrete statement. 
In face “recognition,” for instance, all sorts of things happen in this 
network, but we have no coherent understanding of what exactly 
happens. The bottom line, though, is that the net computes that it’s 
Mr. A’s face. Or that it’s an elephant, but the elephant itself, of course, 
is obviously as opaque to the net as anything else in it. Yet, to the 
human observer it’s more than just an output string. 
        Now, if you claim that there are “patterns of activity” in the net, 
and these “patterns of activity” are either already a structure on the 
level we are interested in or on a level below, only that it is “subsym-
bolic,” then in my opinion this amounts to admitting that the net is a 
black box for you, of which you just cannot say anything at all in terms 
of its function. Especially concerning the latest developments of 
ANNs, which are so successful, this is the biggest mystery. 
MS:   I think, from the perspective of introspection we can say quite a 
bit more about the “subsymbolic.” At least we can intersubjectively 
talk about concepts such as attunement, seed, and expansion on the 
basis of our experiences. According to the computational metaphor, 
a linguistic or informational “token” or “symbol” would actually reside 
on a superordinate level of these processes. Does it reduce the entire 
content of a human “symbol,” which is embodied in a seed and its 
elaboration, to a naked token? 
OW:  Many are perplexed by such questions, since hardly anyone uses 
introspection. I think we can already say a little bit more about this 
matter, but we’ll get to that later. I’ll only give an outline here. 
        For example, a neural network performing face recognition or so-
called object recognition could also be conceived of as executing a 
superposition of images, because a specific input image yields a spe-
cific output, e.g., the word “elephant.” It does so just like some other 
input image, although both images look completely different. In a first 
approximation, one could say that the two inputs are superposed in 
the network, which is of course only a manner of speaking. But at least 
they can then be sorted out afterwards! What is missing completely in 
cognitive science are human construction processes. What is called — 
122

123
I think: wrongly called — distributed information in ANNs in fact rep-
resents those traces you need to achieve an assembly. The infor-
mation, if you even want to call it that, doesn’t exist yet, but enables a 
kind of “bootstrapping,” that is, the construction of a situation. And 
this assembly is, by its nature, a very different process than a mere 
unifying computation of distributed information. Because what we 
need in order to build a model is, after all, already unified information. 
We want to have the elephant and not a symbol string or even sub-
symbolic entities, which amount to even less than a symbol string. 
What we need to deal with an elephant has to be bundled so that we 
can apply it by translating it into motor activity. Above all, then, this 
means that we can apply ourselves to this object. In short, our rep-
resentation of the elephant starts with a collection of readinesses for 
action. And these readinesses are seeds for running environments, so 
to speak! 
MS:   That’s why we have always talked about orientation. By observing 
the first formation of symbols in children like Piaget did, interpreting 
it with the help of your concept of prototype, i.e., the elaboration and, 
later, the evocation of typical situations (in the absence of the object) 
connected to the actual orientation, we perhaps get a little closer to 
the psychological concept of symbol or seed. 
 
 
Bootstrapping and symbols conceived of  
as formalized seeds 
 
OW:  I think that symbol or token was intended to theoretically formal-
ize what I call a seed, which is the internal counterpart of a symbol. 
As it were, the symbol can even function as the name of a seed. What 
I mean by seed I have already explained in more detail elsewhere.23 A 
seed is the incipient unfolding of a system of readinesses for action, 
which makes it possible to construct — in the process whose formal 
description is called “bootstrapping” — a problem as well as the means 
to solve it. 

MS:   Maybe you could briefly explain your idea of bootstrapping. You 
mean an equivalent to the process of “booting” a computer? 
OW:  Yes, to me bootstrapping is a term from the same area of com-
puter science as “stack,” which we should also talk about later. Boot-
strapping is happening every time a computer boots up. It consists of 
simple, very elementary algorithms being executed, which then step 
by step do not install the entire operating system, but activate it and 
set it up, so that simple operations create complicated contexts, in 
which you are then able to work. 
MS:   It sets up the running environment in which a computation 
becomes executable? 
OW:  Even more than that. The computer has to get set first so that it 
can start to operate. Bootstrapping manifests itself in the time it takes 
to read in something, calculate this or that, or move data. This method 
is cheaper and better than storing the full content. So, there is a whole 
period of time that passes before the computer is even ready-to- 
operate. Everything that happens during that time is called bootstrap-
ping. It formalizes, I think, the intuition of what I call “expanding” or 
“assembling.” Formalizing means that formal computer processes 
simulate introspectively experienced processes. That doesn’t mean 
that the formal description captures the essence of what is formal-
ized. It captures a particular feature of it, and only in a particular 
respect. Bootstrapping captures that there is something not yet ready 
in terms of random access memory, but made ready in the process. 
The computer sets up a running environment to create the most 
urgent, but by no means all, conditions so that the project, which had 
begun by a mere seed, can now be tackled thoroughly. 
MS:   Maybe it would be helpful to bring up an example for a seed phe-
nomenon from living thought. Here is the description of a typical epi-
sode from my introspection. The question emerged as to whether a 
particular screw-nut runs clockwise or counterclockwise? Now, while 
I try to orient myself mentally, I experience a seed suggesting a situ-
ation, which in turn assures me that I am able to answer the question. 
However, the seed isn’t the answer yet. The following development 
124

125
ensues: Suddenly, I find myself in a situation in front of a bottle with 
a screw cap, in fact ready to screw it down. To do so, I know how I 
would have to grasp the bottle and in which direction I would have 
to turn the screw cap. And I know that this direction it is called right-
handed. So while discussing this with you, I expand crucial details of 
the task through knowledge actualization. The situation or mental 
episode now represents the question of the nut on the thread of the 
bottle. Yet after that, further mental tunings are necessary to adjust 
the situation to the question of whether the screw cap functions 
clockwise or not. Only by assembly can I try to figure out that, if my 
hand rotates in “this way,” then it is just like the hand of a clock moving 
“this way.” So further mental coordination is necessary to answer even 
this simple question. 
OW:  Yes, that is a good example. The most important conclusion is 
that there are no internal symbols, but only processes, which them-
selves — and that’s why I had to go into some detail here — run deter-
ministically nonetheless. Therefore, in a lower level something must 
be static and fixed. Some memory must exist, otherwise bootstrap-
ping would not be possible. In any case, something must be stored in 
the sense of Bergson’s Matter and Memory, in order to make boot-
strapping possible, that is, a constant expanding of seeds and assem-
bling of running environments. That is necessary, even if doubtlessly 
a lot of processes support themselves reciprocally, or perhaps support 
themselves in parallel. It is even certain that the construction process 
begins in parallel. What is constructed then influences in one way or 
another what happens in other places of the running environment. 
All in all, the process already has similarities with the “settling” of a 
neural net,24 i.e., at least it takes on the form of a gradual grinding in 
of parallel processes. 
        In the end, however, very concrete clues and determining proce-
dures have developed. But as far as I can tell from introspection, 
things always have to be constructed or assembled first. Moreover, in 
my experience, this constant construction process almost always goes 
from the general to the specific. Something begins totally vaguely, and 

already I believe I have reached the goal. But it depends on what this 
proxy under construction must ensure in the following. It is a matter 
of how many details I have to construct, so that I can use the proxy 
further. A lot can often be done even after the very first steps of expan-
sion. Let’s ask ourselves, for instance, how much twenty-eight times 
twenty-eight is. If I only have to estimate the product’s order mag-
nitude, then I can immediately answer: Well, it will have three digits, 
I suppose. But if I want to compute the product exactly, I am forced 
to go into the exact details of the calculation. 
MS:   Yes, I can confirm that. In my example above, the first step of 
problem-solving was also a very general analogy, namely one between 
nut and screw cap, of which I knew the correct sense of rotation. This 
embodied “semantic” parallel is sufficient as an interpretation to take 
the first steps towards a solution. What I did not mention before, 
however, and what became clearer only during my follow-up concre-
tization, was how the bottle situation was given to me in the first place. 
How did I experience this episode? 
        It was instantly clear to me that I had not really seen a bottle, not 
even my hands. Rather, I found myself adequately postured in front 
of a red screw cap. A quasi-image was also present. While introspect-
ing I asked myself just which cap it was. Immediately I noticed that I 
found myself already amidst a kind of dream analysis, i.e., the situ-
ation was given to me just as like a dream fragment, as it is the case 
so often. I wondered if it was the red lid of a Coke bottle, but knew 
instantly that there was more to it — that there was some pattern on 
the lid and that there was also something white on it. So I wondered 
from what time in my life this “memory” could originate from, and 
now found myself attuned to the time of the 1972 Olympics, when I 
was five years old. It dawned on me that the pattern on the lid was 
related to the Olympics logo, a sort of fanned out rosette. And sud-
denly I remembered exactly which lid it was that I was fixated on. It 
was the round tin lid (with a red and white pattern) of a can for dark 
block chocolate that my mother had bought at the time, even if not 
for me. It came from the child’s desire for chocolate, which was 
126

127
denied (Zeigarnik effect25)! So you see, I’m already moving into psy-
choanalytic territory. It is very important, I think, to address these 
aspects. You have spoken so often about similar experiences, too. I 
discover them all the time, perhaps more often when I have an intro-
spective attitude. 
OW:  Yes, of course! Your story reminds me of my example of a “hyp-
nagogic quasi-image” in Silberer’s sense,26 which I reported in said 
glossary entry for “seed.” There, I described the connection between 
quasi-movements when cutting a bacon rind (“Speckschwarte” in Ger-
man) and huge tomes pejoratively called “Schwarte” in German. 
MS:   What is so often overwhelming in such situations is its over- 
determination, even if one can only sometimes decipher it. In my case, 
I had to research the Internet to find out what kind of chocolate can 
it was that I had experienced. And I found it! Its lid has a fanned white 
rosette, very simple in design but similar to the 1972 Olympics logo. 
On the can it reads “SCHO-KA-KOLA. Die Energie-Schokolade. 
16 Ecken · Zartbitter” (CHO-CA-COLA. The energy chocolate. 16 slices · 
bittersweet). This I did not remember! Caffeine! That’s why my 
mother did not give it to me! I’m not sure whether or not the can had 
a screw top or whether or not turning it right applied to it. But here 
one can observe how deeply rooted in my orientation its prototype 
is. It became actualized when I had the task to identify the rotation 
sense in an almost egocentric situation. This little episode alone is full 
of condensations and displacements (in the Freudian sense), because 
in German the word for screw-nut is “Schraubenmutter,” i.e., “screw-
mother” playing a double role here. Furthermore, not only the two 
emblems are similar, but also the word “KOLA” on the lid resembles a 
Coke screw cap and so on. So the seed is quite personal, too. No one 
except me is able to fully understand it. And it is a symbol because it 
functions as a quite general analogy to the screw-nut-turning problem. 
As a prototype it is only the tip of an iceberg of schemas, and perhaps 
this depth was not even necessary in order to solve the problem by 
assembly. In any case, this kind of symbol is absolutely not arbitrary, 
nor can it be separated from personal experience. 

Assemblies are slightly different every time 
 
OW:  The development from general to more concrete is perhaps more 
than the mounting of a structure, because the individual additional 
accommodations of orientation are already moves of a machine. 
During construction, that is, during assembly of the running environ-
ment, there is already an ordered procedure at work. However, the 
assembly is slightly different every time one works on the same prob-
lem, because there are so many inputs, so that they can probably 
never be exactly the same. Yet, certain essentials remain and just 
change over time. 
MS:   Yes, the assembly of seeds and prototypes into a scaffold is not a 
process always developing the same way. The more general the anal-
ogy discovered, the easier it fits all sorts of problems. 
OW:  Anyway, this constructive assembly may be the counterpart of 
“distributed information.” It is, of course, a very different process from 
linking distributed information in ANNs, where distributed infor-
mation means the distribution of weights of the “units.” In a neural 
network it is always crucial whether it is in the learning mode, as it is 
called, or whether it is in the working mode. The working mode is 
totally uninteresting, because nothing changes within the net. A com-
pletely normal function has converged, stubbornly computing an out-
put for every input. The only interesting thing is the learning phase. 
What does “information” mean here? 
 
 
From the general to the specific 
 
MS:   It’s highly mysterious that, as you emphasize, every assembly 
develops from the general to the specific. The running environment 
has not been calibrated yet, but the first step is a seed. Or just clutter, 
that is, an unordered group of seeds, which are then accepted or dis-
carded during further assembly. You once said, and I found this very 
apt, that the origin of assembly in the seed is a very general but not a 
128

129
reduced description. Basically, it is not a description at all, but the ini-
tial setup of a running environment, which is so general that practi-
cally any further development is possible. 
OW:  Yes, the seed’s function resembles the function of a stem cell, to 
engage another analogy. It can take a lot of developments depending 
on which features are carved out. I think this has a lot to do with 
two papers of Anthony Marcel, which are very important to me.27 
Although he was not really able to prove it, Marcel argues plausibly 
that truly understanding a word implies a processual development 
from the general to the specific. During this development, a whole 
range of directions inchoately start off, which would lead to different 
interpretations. But in the end, the entire apparatus of understanding 
yields one interpretation. As it is clear that the entire setting is 
involved, we could compare Marcel’s results with our concept of run-
ning environment. Different running environments translate the same 
task into different results. I think this is comparable to results of intro-
spection, when a seed begins to expand. 
MS:   The fact that different running environments yield to different 
results already ran like a golden thread through the experiments of 
the members of the Würzburg School. Their concept of “task” may 
even be considered an antecedent of our concept of orientation. Only 
their experiments have perhaps been too much focused on words and 
laboratory conditions. From today’s perspective, you could criticize 
that they have not been enough founded in the “ecology” of living 
thought? Yet, Narciss Ach conducted experiments with unconscious 
tasks. And looking at the introspective analyses of Karl Bühler and 
Otto Selz one can only be astonished by how much important mate-
rial they discovered. Bühler, for instance, already spoke of the tempo-
ral aspect we just mentioned, of successive experiential episodes until 
a “finished thought” emerges.28 
 
 
 
 

Expansion and assembly 
 
OW:  I draw the following distinction. The expansion of a seed com-
prises those processes already included in the seed, i.e., which runs 
without any constraints of a “task,” if you will (anyhow, task was much 
too narrowly defined by the Würzburg School). Perhaps that is its 
most general meaning. Then, during assembly the constraints of the 
momentary situation and of orientation take effect: “Well, okay, but I 
still have to consider this and that... and that too.” Expansion pertains, 
so to speak, to the level of sensorimotor schemas, and assembly to the 
level of additional constraints, which these sensorimotor schemas 
must satisfy. In the course of assembly analogies always occur in spe-
cific running environments, because the only important fact is that 
an analogy emerges — or is rather suspected. 
MS:   What strikes me is that you say that there are different levels. I 
also often find this kind of over-determination in seeds that serve as 
analogies. There are different areas that coincide, connections that I 
discover only later and under specific circumstances, that the 
observed analogy meant something else still. These observations 
seem important to me. 
OW:  Yes, exactly, but that is already included in what I said before. 
Whether or not you see a specific analogy depends very much on the 
stratification of your current orientation and on said process of devel-
opment from the general to the more specific. If an analogy is very 
general, then it is perhaps only distinguished from other analogies by 
the fact that it takes place in this specific orientation, and that the gen-
eral meaning needs a few moves to become specific. So on the same 
level of generality analogies to entirely different things may occur. 
According to Anthony Marcel, this is also the case in the process of 
understanding a word. Instantly, perhaps within a tenth of a second, 
the word activates five or six different interpretations at once. These 
are precisely different analogous objects. Sound alone may also serve 
as an analogy, so it suffices that the objects have acoustically similar 
names. I think that is quite clear. 
130

131
MS:   The assembly process is rather different from expansion, as you 
have indicated before. One intervenes in the process of a seed 
expanding to become more concrete, partly by practical-logical con-
siderations, by rejecting developments that seem to lead nowhere, or 
by starting to construct something anew. Often while expanding 
seeds, though, many phenomena and analogies occur, which you just 
haven’t observed before or which you only recognize as such in ret-
rospect. Assembly seems rather like a purposeful shaping and synthe-
sis of this richness of phenomena. 
OW:  I think that is quite correct. One could add that assembly means, 
in a sense, the perspective of the running environment while expan-
sion means the perspective of the seed. That again implies, I would 
say, that assembly and expansion are basically two aspects of the same 
process. On the one hand, expansion extends the seed, that is, clarifies 
and holds together what has appeared in the first instance. At the 
same time, however, as the running environment somehow consoli-
dates, something becomes explicit by establishing the aspects, which 
the specific running environment is supposed to represent. But it is 
the same process. The development from the more abstract to the 
more concrete obviously passes through a phase, during which a lot 
of analogies are possible. 
 
 
Pleomorphism, i.e., potential polymorphism 
 
OW:  We could give this phase of potential polymorphism the name 
pleomorphism. As far as I know, this phase has never been included 
in any technical token or symbol concept, let alone implemented. 
There has to be a transition from the stage of attunement (more about 
it in a moment) to the stage of proxies (of objects). 
MS:   I think that what has been said so far concerning the expansion 
of a seed culminating in its pleomorphism, as you call it, overlaps with 
Freud’s notion of “symbolic thought.” Or rather it represents an 
approach to clarify his notion as well as what Piaget has observed on 

the psychogenesis of the “symbol” during children’s development. 
Piaget called this the infantile “syncretism.”29 For it is clear that where 
analogies are diverse because of the generality of the thought, con-
tradictions, condensations and displacements accumulate. 
        Freud believed that the distortions necessarily resulting from this 
generality serve to make a wish unrecognizable and are even pro-
duced especially for this purpose by an unconscious agent (named 
censor). I think that this finalistic interpretation is at best superfluous, 
but, considering my previous example, understandable. My “prob-
lem” of the screw-nut and the seed for opening and closing a Coke 
bottle lid as an analogy, which in retrospect turned out to be ambig-
uous, clearly refers the biographically earlier “problem” of opening 
and closing of the KOLA chocolate can, which indeed represents a 
denied wish. However, it seems inevitable to me that any theory of 
the “cognitive symbol” must at least potentially model this kind of 
deeper content in order to meet the minimum requirements of a psy-
chological explanation. 
OW:  Well, on the polymorphous character and the loss of dynamics I 
have written the following in the style of a manifesto: “I don’t have any 
‘internal symbols, signs, or tokens;’ if I talk of symbols in order to mean 
something specific, I mean marks on paper, which not only I am able 
to see, but which other organisms can see too.” This is my definition of 
symbol. And that’s why it was mistakenly believed that the ‘internal 
symbol’ is just a mark, not on paper but somewhere else — in my mind. 
Yet, I don’t have such marks, but all that I observe is always processes, 
attempts at reconstruction or construction. Everything happens in a 
specific constellation of constraints which enable me to infer, in a mul-
tiplication for instance, that the product is, say, 56. During the process 
I have the lightning-fast illusion that I see number 5 or 6, of course. But 
I know that I don’t really see them, and certainly not as a symbol. 
        Just as we believe we see something in the mind, we believe that 
there is a symbol as soon as we want to identify something constant 
in thought. But all that is constant is the process. And there is abso-
lutely no doubt that written signs, marks, or symbols help thinking. 
132

133
But given this interpretation, there are no “internal symbols.” There 
is the written symbol, the written word, the written sentence, and 
when reading them these symbols evoke seeds in me. These seeds are 
the essence of thought, while linguistics believe that it’s words. Words 
are symbols, but seeds are not. 
        If the Physical Symbol System Hypothesis wants to offer some-
thing tempting, the seeds must not be “pointers” either. The seed is 
the process itself but not yet in full progress. It is present as a growth 
impulse. So the excitation of specific pathways is there, but this exci-
tation is not enough to concretize the machine that is supposed to 
run. Yet, together with the rest of the environment, it is enough to con-
struct this machine, lightning fast — or rather more or less fast, to say 
it cautiously. 
MS:   It is also very hard to imagine how the kind of ambiguity we men-
tioned, which begins with a seed, could be reduced to a pointer whose 
essential property is to be unambiguous. I have taken the habit of using 
“cognitive symbol” instead of the term “internal symbol” in order to at 
least imply its processual aspect. It is clear that this will again lead to 
misunderstandings, but seed is opaque to anyone not accustomed to 
our terminology, while symbol in common language carries such an 
enormous literary ballast that it shouldn’t really be used anymore. Ulti-
mately, after all, we want to distinguish this unfolding of an insight 
from the conventional use of the word “sign” or “symbol.” 
 
Symbol in thought (cognitive symbol) 
When a “cognitive symbol” becomes manifest, it means the begin-
ning of the actualization of recognition or of knowledge of an inten-
tional situation with a proxy object from the past (memory). Accord-
ing to Otto Selz, this amounts to a “knowledge actualization.” As 
soon as this remembered and directed activity/method for achiev-
ing a goal becomes focal in thought, the actualized aspect enables 
a new view onto the current relationship under scrutiny (thought or 
action). In the best case, the current orientation will offer a solution 
to a possible perturbation or gap within this relationship. When a 

cognitive symbol becomes conscious in action or perception, the 
current situation is grasped, recognized, or interpreted under the 
aspect or schema of an earlier situation. This enables anticipation 
as well as pre-practical choice. However, if a perturbation or gap 
occurs in thought or action as a result of a current relational com-
plex, which the cognitive symbol cannot reproductively remedy, a 
productive process of problem solving or construction may ensue. 
Such a dynamic cognitive process allows for “gedankenexperi-
ments” by selecting and expanding the cognitive symbols and their 
assembly into running environments with which problems can be 
solved. 
 
The cognitive symbol in introspection 
In introspection, a “seed” and, more broadly, the connection between 
attunement, irritation, manifestation, expansion, and assembly 
potentially enable the unfolding of a “cognitive symbol.” On the basis 
of several examples, Oswald Wiener suggested that seeds are not 
pointers to something recognized but the nucleus of the recognition 
process. Furthermore, the cognitive symbol is neither a symbol in 
the sense of computer science or mathematics nor a sign to be 
assigned arbitrarily. Nor is it a symbol of social origin, although it 
might emerge from a social conﬂict. 
 
Wiener, Oswald, 2015. Glossar: Weiser. In: Eder, Thomas, and Thomas Raab (eds.), 
Selbstbeobachtung: Oswald Wieners Denkpsychologie. Berlin, 59–98. 
MS 
 
OW:  To sum up: By comparing crucial concepts of the computational 
metaphor with insights from introspection, we can show that, con-
trary to living thought, “symbols” (in the sense of the PSSH) are noth-
ing but external. “Internal symbols” are not symbols at all, but rep-
resentations, i.e., structures in the making. What folk psychology calls 
“internal symbol” or “mental image” is the nascent understanding 
(e.g., of a word) in the form of the corresponding seed, especially if it 
134

135
occurs in thought without external symbols. This comparison particu-
larly shows that in this perspective the computational metaphor of 
the PSSH cannot help, as Newell and Simon believed, “to gain a rather 
deep understanding of how human intelligence works in many situ-
ations.”30 On the contrary, it has prevented and still hinders a deeper 
understanding of thinking. 
 
 
Attunement, complex quality, and orientation 
 
OW:  Several terms that are of great importance to me, although they 
are unexplained or largely unexplained, have not even been men-
tioned in our discussion so far. One of them is attunement (or mood; 
Stimmung in German). You have talked before about such an attune-
ment episode from your childhood. That brings me to the following 
which may seem unrelated but isn’t. I’ve been doing some digging in 
the literature to try to solidify the concept of attunement a bit, and 
was struck by things that probably mean something similar. Above all, 
I am thinking of the term “complex quality” frequently used by Hans 
Volkelt. Yet, Volkelt was an avowed Nazi and, therefore, later excom-
municated from the “society of democrats.” Of all the writings of Frie-
drich Sander, Felix Krueger, and others, I consider an early work by 
Volkelt to be the most interesting: Über die Vorstellungen der Tiere (On 
concepts in animals, 1912). 
MS:   Yes, Volkelt was a problematic person, a big shot in National 
Socialist pedagogy. I have read some writings of the authors you men-
tion. I believe it is his dissertation you’re talking about, which he sub-
mitted to Wilhelm Wundt in Leipzig. It is certainly indispensable to 
comparative psychology. 
OW:  In it Volkelt frequently talks about the concept of complex quality. 
I believe that he meant what I call attunement, but his choice of words 
is interesting because he uses quality obviously in the sense of quale. 
Since I myself also ponder on qualia, this immediately appealed to 
me. He means the overall impression of a situation, which you cannot 

articulate, but experience as a quale, that is, you know it without being 
able to articulate it. This is exactly what we call attunement. 
MS:   Yes, in his dissertation he also deals with a form of conscious reg-
ulation of actions in animals, which is not structured by proxies. He 
is concerned with behavior that “instinctively” depends, so to speak, 
on the overall situation but does not yet necessitate thinking or imag-
ining and not even an object transcending perception, i.e., a proxy. 
Yet this behavior, Volkelt believed, is neither controlled by the sum of 
sensations, but by an attunement or complex quality attributable to 
the overall situation. 
OW:  Yes. Very early, already in an essay in the seventies or even earlier, 
I speculated that an attunement is the preform of a concept, and I still 
believe this is right. But what does it mean? 
MS:   Yes, that was in your essay on “Why art at all?”31 That is a very 
important text of yours which also serves as an introduction to aes-
thetics, because much is anticipated in it — from the idea of aesthetics 
as “being touched” to the idea of orientation and its loss. I read it now 
and again with art students. 
OW:  If we could define the word readiness a little more precisely, we 
could just say, an attunement is a specific, but yet unorganized mixture 
of readinesses, similar to a seed, but without quasi-sensory compo-
nents. A seed may, of course, be part of it, but then seeds appear 
momentarily and disappear immediately, while attunements some-
times last for longer, possibly for a very long time. Attunements, in 
this context, are part of the cognitive development of structures. They 
are at its beginning, on the “first day of the creation” of a concept, to 
speak ironically. Where heaven and earth are hardly differentiated 
yet, or only a little differentiated, and where perhaps already water 
and land exist, but not much else has happened. 
 
Complex quality 
With the term complex quality Hans Volkelt refers to primitive con-
sciousness – predominantly ascribed to “lower” animals (as exem-
pliﬁed by his experiments with oak spiders) – but not beyond. It per-
136

137
tains to a consciousness of unstructured “feelings” (attunements). 
Volkelt conceives of these mostly hereditary qualities as “generic 
perception” of “vitally important facts” (situations and objects), which 
he denies have any object character. Volkelt showed that for the rec-
ognition of an object (e.g., prey, which to the spider would be a mos-
quito) the behaviorally effective situation is the decisive trigger, i.e., 
the mosquito in its speciﬁcally vibrating web. If the spider 
encounters the mosquito outside this situation, e.g., as a visually 
presented object, it will not recognize it as an independent object, 
and thus the prey schema won’t be activated. In “vitally indifferent 
situations” we cannot detect any “object-like” or cross-sensory con-
stants, which the animal follows. Yet, these constants are part and 
parcel of human imagining. Just like Ehrenfels with his “gestalt qual-
ities” Volkelt declines to explain such qualities as the mere sum of 
sensory data or unstructured sensations. For the complex quality is, 
beyond the conception of Ehrenfels, “a psychic whole other than the 
sum of its parts. The primary complex has its peculiar quality, namely 
its ‘complex quality’.” 
 
Ehrenfels, Christian von, 1890. On Gestalt Qualities. In: Smith, Barry (ed.), 
1988. Foundations of Gestalt Theory. Munich, Vienna, 121–123. 
Volkelt, Hans, 1912. Über die Vorstellungen der Tiere: Ein Beitrag zur Ent-
wicklungspsychologie. Leipzig. 
MS 
 
MS:   If we define attunement as an unequivocally attributable con-
sciousness of specific action readinesses occurring before the devel-
opment of any proxy functioning as an object, we should perhaps 
conclude that attunements arise before the aforementioned ambig-
uous phase, i.e., before pleomorphism. So we could hardly speak of 
ambiguity in attunements, could we? If in human thought “symbols” 
(in the form of seed phenomena) are much more complex than the 
“symbols” of the PSSH, why not assume that attunements are such 
symbols? Yet, you immediately see that attunements are even less 

arbitrary than seed phenomena. The PSSH postulates symbols, which 
according to introspection only exist at the end of an entire devel-
opment. They are general abstractions from expansions and 
assemblies. Just like you said before: Logic itself does not think but is 
an abstraction of thought processes. 
OW:  In particular, it would be important to specify the word “quality” 
here. A quale is, by its very definition, so to speak, an atom — some-
thing that is not further analyzable. The epitome of a quale is, for 
instance, a color experience, where one says in the vast majority of 
cases: Okay this is, say, green. At most you can say: Oh, that’s this spe-
cific green, and then you might have names for it such as olive-green 
or frog-green or whatever. But that’s it. I have been doing such qualia 
studies again and again for years, such as listening to tonal dyads. 
Among them there are some that I hear as a quale, i.e., I know imme-
diately what kind of dyad it is. The easiest for me is the tritone, which 
I hear quite clearly. My quale of a fifth, however, can easily turn out 
to be a fourth. Sometimes I confuse them. These tonal qualia are 
experiences that I don’t have to decompose to identify: the tritone 
dyad, for example. But others I do have to analyze. I necessarily have 
to decompose major and minor sixths, for instance. I have to sing the 
two notes, and then I may find out that this is not a minor sixth, but a 
major sixth. For the quale itself is not clear, not precise enough. So on 
the one hand, there is something like atoms, as in the case of the tri-
tone, because I don’t need to seek out its two component tones sep-
arately. On the other hand, in the case of the sixth, there is such a 
quale impression too, but nonetheless I have to dissect it at times. 
Then the two separate tones are the qualia, and the dyad is a mixture 
of these two qualia. I think, you have to conceive of attunements quite 
similarly. What are these smallest atoms of experience? Well, they are 
not analyzable only regarding conscious access, only for conscious 
analysis. For the physiologist, however, they are of course analyzable. 
So you must conceive of qualia as being relative, I believe. 
MS:   Let’s pin that down. If thought begins with an attunement, or the 
attunement is the origin of a thought already leading into a certain 
138

139
direction, which is clearly differentiated from other possible direc-
tions, but without yet becoming a proxy object, then the question is: 
To what extent can I make the attunement itself a proxy for “con-
scious access”? 
        You just talked about sensually imaginable qualities, which bey-
ond a certain point evade conscious analysis. They are encapsulated. 
But there is sensorimotor conduct, which feels exactly the same way. 
So are there “instinctive” qualia of actions? Plus there are all those 
sensorimotor acts being learned and developed every day. Basically, 
I want to refer to your concept of prosthesis and that qualia can be 
trained. Automated schemas of thought and action such as learning 
the small multiplication table or handwriting — do they too become 
qualities by such training? 
 
 
Prostheses 
 
OW:  Yes, this concept is very important to me and it applies here. If 
some behavior is automated, you might be able to remember how you 
learned it, but this isn’t necessary. On the contrary, it would impede 
behavior greatly if you had to constantly remember how you learned 
it. For instance, you don’t even think about how multiplication works. 
You just seem to have memorized a ready-made table. But does that 
mean that it’s made of symbols? No, it’s not symbols but sequences of 
transformations that take place. Seven times eight is fifty-six — that’s 
not symbols, that’s a movement, a kind of trivial Turing machine. 
MS:   In order to enact some behavior I don’t have to remember, how 
and where I’ve learned it. But while introspecting, it often strikes me 
that, as soon as the thought process falters and I am unable to expand 
the seed in the desired direction, because I can’t find the necessary 
means to do so, a specific place comes to my mind, where I have seen 
or learned what I now can’t remember. Sometimes, it may even be a 
passage in a book where I could possibly find the missing “sense.” So 
perhaps here the orientation to an already known problem becomes 

conscious again. As far as the difference between conscious and auto-
matic processes is concerned, something is on the tip of my tongue … 
But maybe you could meanwhile elaborate on the concept of prosthe-
sis? 
OW:  In the language of Turing machines, this is quite clear. A prosthesis 
is a read-write head, and there are many of them. So they depend on 
the existing hardware, e.g., on upstream sensors and their input-out-
put relation to their connected unchangeable structures. But for the 
most part they can be conceived of as software read-write heads, 
which can very well be changed or newly created — like programs run-
ning in a universal machine, in a computer. We can describe the con-
cept of prosthesis as a simple Turing machine. In our Turing textbook, 
for instance, I gave simple machines that translate from one positional 
notation system into another, e.g., from unary numbers into binary 
or decimal numbers.32 Now imagine a “meshed” (non-modularized) 
Turing machine, which reads a binary number, transforms it into dec-
imal form for a more essential machine only beginning its operation 
when it reads decimal numbers on its tape. 
        The aggregate machine can be viewed as two separate devices. 
The upstream one does nothing but read “one zero zero one,” for 
example, and return “nine” in the decimal system. The downstream 
device then reads this number to process it further. So the first device 
functions as a prosthesis for the second. It doesn’t have to be visible 
at all for the downstream machine, which computes, say, a multipli-
cation. For the latter, the upstream machine is transparent. This kind 
of modularization we can now generalize, because in between the two 
machines we could put additional read-write heads, and the multipli-
cation machine may also serve as a prosthesis for downstream 
machines, i.e., mediate between machines. Put slightly differently, you 
could also think of it as a camera, in front of which you strap one lens 
and then another lens, and so on, until finally a picture appears at its 
back. 
        This is how you should see it if you want to formalize our experi-
ence of, say, the process of reading. When we read a narrative, we 
140

141
experience the situation described in it. Every clue in the text may 
mobilize seeds, change my orientation, or be integrated into my 
attunement built-up so far. However, what is entirely transparent in 
the consumption of the story are all processes of deciphering letters 
and words, the structure of sentences, the sounds that are pro-
nounced — everything that I once learned with great effort. You will 
surely remember how laborious it was as a child to decipher one 
letter after another. Then you formed words from the letters already 
mastered. Then you had to bring homework to school and so on. I 
remember that well. 
MS:   Yes. In the beginning, reading is even tied to sound production, 
which only disappears with a lot of practice. In the end, you are able 
to read completely without moving your lips or tongue. When paying 
attention to it, I sometimes perceive motor rudiments even today. 
OW:  Okay. So as an adult the reading process has become transparent. 
Now a prosthesis, viz., a software prosthesis has been constructed. I 
am usually not aware of it today but — and this is of utmost impor-
tance — I am able to give attention to the task of deciphering. If I do 
so, something interesting happens. If I attend, for instance, to the 
shape of single letters, then I often miss the meaning of the text. It’s 
simply not passed on. Or some meaning somehow reaches me, slips 
in like “oh, it’s about a man who apparently has red hair” or so. It all 
depends on how intense my concentration on deciphering is. So the 
processes seem to be connected in series. This is why I named the 
transparent processes prostheses. Because the underlying meaning of 
the prosthesis is something that is strapped on. 
        But that is only one example. So the question, which we always 
try to come back to, is how to relate our concept of structure to what 
we really experience in introspection without skewing what we 
observe, as the PSSH does by frivolously applying the concept of sym-
bol to thinking. 
MS:   Earlier I tried to differentiate between “conscious” thinking, 
where the term “conscious” is but a crutch, and the transparent auto-
matic run of a Turing machine. The moment I look at something con-

sciously, I still have access to what the prosthesis does — to what lies 
beneath. 
OW:  Yes, that’s right, and that is not a crutch. As I said, you can draw 
your attention to it. Then the situation is different because you don’t 
use the prosthesis anymore, but you make it the proxy object of your 
thinking. 
MS:   Yes. In your description I find this aspect of making something 
the proxy object very important. Although it somehow digresses from 
introspective experience, you could connect that to Piaget’s attempt 
to look for the origin of symbolic thought in the genesis of children’s 
play and imitation behavior. 
        After all, what has been said so far would mean that I am nowhere 
near being conscious of an action itself, even if I’m oriented to a specific 
situation and possess a specific attunement to the respective action 
complex, because I become aware of the action execution by (repeat-
ing) the consequences (the results) of it. In order to become aware of 
my learned or inherited prostheses, I must not simply use them, but 
make them the proxy object, i.e., draw attention to them, as you say. 
        Here I see a connection to Piaget’s descriptions of a child playing 
an action “as-if” instead of executing it on the object itself. This may 
serve to transform these prostheses and their readinesses into a proxy 
object, so that the child would be able to analyze them and “construct” 
a prototype of the attunements related to them. This would yield to a 
“symbolic schema,” as Piaget called it. He uses the term symbolic, 
because these schemas act in place of the “real” action situations in 
the absence of objects. Perhaps, then, a mental reproduction of the 
prototype could reenact the orientation to a past situation, i.e., evoke 
a complex of attunements. 
OW:  So far it is correct that orientation is first directed to the result of 
an action, especially in learning movements and directed action. It is 
directed to establishing the relation between cause and effect. I have 
also called this relation “servo-mechanism,” because you don’t know, 
for instance, all the muscles enacting a movement with the right 
strength and in the right order. You only know the effects of your 
142

143
action on the senses including the kinesthetic sense. Hermann von 
Helmholtz already stressed this in his description of the relationship 
between “motor impulse” and its “result.” By result (“Erfolg” in Ger-
man) he means the first change to be perceived by the sense organs, 
just as I alluded to earlier when speaking about the musical tone, 
which I can intonate and sing, and then, in a strange way, identify 
whether or not the sung tone I now hear is correct. But all the mechan-
ics in between — of the involved laryngeal musculature, for example — 
is lacking completely. This is very important for the theory of thought. 
For what we are, at best, able to mentally represent is the immediate 
relationship between the impulse (the attunement) and its result. In 
between there is a cascade of events within an encapsulated realm, 
which we are unable to reach even when trying to attend to it. 
MS:   Okay, there are limits to conscious analysis. What you experience, 
when you reach them, can only be called a quale, as you said. But then 
there are also complexes, which you are able to isolate from the over-
all situation by thinking. In the psychogenesis of play, the child from 
a certain point on is able to ignore the execution of an action on the 
object. This enables it to analyze and isolate its own action. According 
to Piaget, this behavior is the prerequisite for recognizing the same 
action performed by someone else. But Piaget went on to observe and 
describe a second type of behavior in the child. He called it “imita-
tion,” that is, behavior which looks for similar motor results in order 
to relate the initially isolated prototypes of one’s own action to the 
action perceived in others. The child tries to repeat sounds, syllables, 
and gestures, which, once recognized, it reproduces and imitates with 
its own body. The development culminates in the objectification and 
internalization of imitation. In my view, this amounts to the “symbolic 
thinking” of possible situations. 
        This could be the incipient development of what you called a 
model (or processes of type 1 and 2, respectively). However, it’s still a 
long way. I also wonder whether what we call assembly comes about 
gradually at a more advanced stage of mental development. But at 
least the kernel of the most important structural relations is already 

there. Unfortunately, these are neglected in the sensualistic hybrid 
models, I think. What develops is the “symbolic” representation of a 
situational orientation in absence of the “real” situation. Thereby the 
child is able to relate a “model” to “reality”! Moreover, by this process 
the minimum requirement of what Piaget calls the semiotic function 
has emerged, because from this stage on the child is able to master 
language and the use of linguistic symbols. 
 
The psychogenesis of symbolic thought 
According to Jean Piaget, during psychological development the ﬁrst 
symbolic behaviors (symbolic play) yield “symbolic schemas” playfully 
reproducing own actions as prototypes instead of performing them on 
real objects. Yet, by progressing to “imitative schemas,” actions of others 
become related to the child’s own prototypical actions. Thus it becomes 
able to reproduce them as well. Such behaviors reach from body move-
ments to facial expressions to the imitation of action results, that is, of 
sounds and more complex sensorimotor relations. 
       It is likely that these new prototypes, as already grasped schematic 
contexts or aspects, continue to closely interact with the orientations of 
the original sensorimotor actions, because they have been constructed 
from these orientations’ action readinesses. By this postponed repro-
duction of prototypical imitations, the episodic situations of interest to 
the child can become actualized without the situation’s object being 
present. The prototypical action just mobilizes the corresponding orien-
tation or, as is the case with the complementary type of imitation, rep-
resents the object itself. Initially, the analogy is limited to the action of 
others alone, i.e., to the production of their sounds and movements. Later, 
it pertains to the imitation of situations and things themselves too. 
       The formation and application of these two types – symbolic sche-
mas and imitative schemas – are not designed for the use of objects, 
but of their proxies. They are symbolic surrogates in their internal real-
ization. In their “artiﬁciality” these schemas ﬁrst serve the separation 
of subject and object (of action on the one side and object on the 
other). I say “artiﬁciality,” because the ﬁrst type of schema substitutes 
144

145
the real act “as if” it was performed, and the second type of schema 
acts “as if” it brings forth the physical object. 
       In advanced symbolic play, both types of imitation intermingle 
more and more. They are projected or transferred to the most diverse 
objects (toys now “walk,” “cry,” “eat,” or even imitate actions of people). 
Thus, schemas further detach from bodily imitation, i.e., from the child’s 
own actions. However, before they become internalized as prototypes, 
i.e., before they are episodically “imagined,” “remembered,” and no 
longer executed, the formation of prototypes extends to all possible 
things. They become objectiﬁed, that is, constructed on the ﬂy and 
independently of habitual action schemas. As such, they ﬁnally become 
internalized cognitive symbols. These symbols, however, “unfold” pro-
totypes of orientations. Thus, they are not symbols in the conventional 
sense. They are bound to the overall orientation of sensorimotor devel-
opment, from which they receive meaning and true depth. 
       Already in symbolic play it becomes evident that the child again 
and again imitates the (prototypically “reproducible” or “recognized”) 
episodic relations impressing on it. It does so for various reasons, e.g., 
for wish fulﬁllment, compensation, or anticipation. Due to the projec-
tions already used in play before, it notably becomes able to easily 
transfer “similarities” to various relations. So it projects own actions to 
the actions of other people or things. The action of others is not only 
imitated, but also projected onto the material world. Moreover, objec-
tive relations stand in for bodily ones (or vice versa), prototypical rela-
tions from one object reappear in others. By this a “syncretism” 
develops, which ﬂexibly culminates in the largely implicit orientation of 
“symbolic thought,” which functions at quite a general level but is nev-
ertheless rooted in sensorimotor schemas. 
 
Piaget, Jean, 1945. La formation du symbole chez l’enfant: Imitation, jeu et 
rêve – Image et représentation. Neuchâtel, Paris (Engl.: Play, Dreams and 
Imitation in Childhood. London 1951). 
MS 
 

OW:  Let’s get back to our specific questions. What I meant earlier is 
that, if we conclude that a seed or an intrusion could be interpreted 
or viewed under different aspects, then you have to ask yourself: Okay, 
but why then is it in reality viewed under this one specific aspect? 
MS:   In introspection I mostly experience what you are referring to as 
a seed feeling over-determined. Of course, this pertains especially to 
dreams, perhaps because there the running environment doesn’t 
restrict thought so much. In any case, I experience that the seed 
already seems to appear together with structural relations presum-
ably used to support the overall situation. Yet I don’t necessarily 
become aware of the content of these relations. Perhaps they are pre-
conscious, so that I usually notice them only in retrospect. In dreams, 
however, it is almost the opposite. After waking up I experience 
almost exclusively relations of content, whereas the structural or for-
mal relations are either captured immediately after waking up or 
otherwise only by willful analysis. That’s probably the reason why 
Freud believed that the manifest content of dreams only is the sur-
face, whereas the images dreamt are symbols encoding the true or 
latent content. 
OW:  As such, this is hardly surprising. I have already attempted to 
explain the pleomorphism of the phase in which many analogies are 
possible. Moreover, we must not forget that the experiences from the 
recent past continue to live on in us all the time. They constitute an 
extended running environment, so to speak. Things we talked about 
yesterday, for example, still make up a background today. This back-
ground will weaken over time, but parts of it will remain effective. 
What is important is this embedding of the incident, which we are 
momentarily conscious of. This embedding makes the incident 
appear different from all other incidents. This is, of course, the prem-
ise of our thinking to be clear. Often, however, we do realize that some-
thing is constantly taking place in this staggered background, even if 
our thinking seems very focused. 
        Let me give an example. The other day I took on a task from rec-
reational mathematics: a sequence of numbers, where the first five 
146

147
members of the sequence are given, for which I had to find the law of 
formation. So it was a very common problem. I sat down with the 
intention to introspect and tried to write down everything that went 
through my mind while solving it. One of my first thoughts was: 
“Alright, I actually know quite a lot about sequences, not about this 
specific one, but...” These were wordless thoughts, of course. The 
numbers already reminded me of something, because there is not 
only a two in the sequence, but even two twos next to each other. That 
is remarkable and unusual, because normally in a sequence, each 
member is different. So I had already registered these two twos as 
something strange, and this fact alone already determined that I 
would come back to it — that with every conjecture coming to my 
mind from now on, I would have to test it on this pair of twos. 
Although that was implicitly established, I noticed it. And suddenly it 
dawned on me out of nowhere: “That’s like my new neighbor.” Period! 
Well, I had to think about this: Two or three houses down the road, a 
new neighbor had moved into a house that has been empty for a year. 
But “the new neighbor” is actually a married couple! So here it came: 
I already knew quite a lot about the new neighbor, though nothing 
concrete at all. But at least I knew which expectations he could fulfill 
and which not. This was the connection to the mathematical 
sequence: I know what a sequence is, and I approximately know what 
is to be expected. The same holds true for the neighbor. Certain things 
that I saw — that his car has an English license plate, for instance — 
made me think: “Ah, that’s probably an Englishman.” Then this was 
confirmed. It doesn’t amount to much, but at least I knew much more 
than before. The woman is from our region here — and already the 
draft of a romance emerges, to speak poetically: A southern Styrian 
woman meets an English gentleman, or something like that ... 
MS:   Yes, yes, I know exactly what you want to say! I also regularly reg-
ister such analogies which abruptly contribute to my orientation and 
which determine an order. These may be incidents, which have stuck 
to me only recently or a week or a year ago, but quite often they even 
stem from my childhood. 

OW:  All the time this couple obviously exerted an influence in the 
background, so that when I seemed to focus on the mathematical 
problem, it was projected onto the sequence of numbers. So the quite 
global relation intruded: “That’s just like my neighbor.” 
        But this means only that the overall problem-solving attunement 
grounding the “construction site” of the running environment and its 
current proxy object is completely open. But it may concretize in the 
blink of an eye, if some specific aspect becomes salient. This aspect is 
only potentially part of the running environment, but then all aspects 
are only potentially there. That’s exactly why we have to scrutinize 
what the proxy object exactly “looks like.” Above all, these potential-
ities are also graded, i.e., certain aspects are realized more easily than 
others. In principle, a small change within the broader running envi-
ronment seems to make an aspect virulent, which before had hardly 
any chance to become actualized. 
MS:   Well, all these aspects are preconscious, that is, accessible to con-
sciousness in principle, if one may say so. I realize the relations within 
the proxy object if I rethink what connections had already been 
anchored before. 
OW:  Yes, you realize this in retrospect. It is not clear from the begin-
ning that my new neighbor has something to do with this mathemat-
ical sequence, that he shares a similarity with my situation facing the 
problem. In retrospect, however, I can see that similarity quite well, 
because I have tried to reconstruct it to some extent. You see, what is 
similar is really very global. The neighbor intruded because he was 
already there in the running environment, albeit not in connection 
with the number sequence, of course. But this can only work at this 
very general level, at which so much is possible. With regard to your 
mention of over-determination earlier: one thing determines the 
other. The running environment “new neighbor” determines the run-
ning environment “number sequence” to such an extent that, as soon 
as a similarity is discovered, it breaks through into consciousness. 
MS:   That sounds reasonable to me, and I don’t see that such a phe-
nomenon could even possibly be implemented in artificial neural net-
148

149
works. That’s why I talked about conscious access. If I recognize a sim-
ilarity, then usually I am able to explicate it. I am able to express it in 
some medium, to describe it, just as you now did. The minimum 
requirement to call something recognizing is probably to register an 
attunement just as you described. I have to be able to analyze a dis-
tinction within the current readiness enabling me to reproduce what 
I have recognized, just as in your example of the tonal dyad. 
        This is the case even with the simplest figurative analogies. For 
instance, if I try to feel out a cut-out figure in the form of a boot, blind-
folded and with my hands only, “Italy” comes to mind. But the same 
is true when I see the same two-dimensional figure. This explicit 
knowledge transfer between different sensory modes, in this case 
between palpation and vision, won’t be possible by merely hearing 
the word or seeing the written word “Italy.” So in my opinion it cannot 
be done by an ANN either. Of course, one would have to ask exactly 
whether and if yes, how this could be related to a “symbolic expres-
sion” of the PSSH. 
OW:  In ANNs, this would only make sense at all if there were a device 
belonging to the network, which makes these “patterns” effective, i.e. 
ensures that the patterns that “light up” within the ANN are rendered 
a causal factor for further developments. But in neural networks there 
is nothing that reads these patterns. Moreover, there would have to 
be a device that takes these different patterns as examples to find a 
common structure of them, that is, a structure accepting all these pat-
terns and only them or only those, which also refer to an object. Such 
a machine is completely missing in ANNs. Yet a human interpreter is 
able to see patterns and say: “Ah, this time this and that has lit up, so 
the net’s output was that one,” or ask: “This is the same object, but in 
this case the node weight distribution is completely different, so what 
do these patterns actually have in common?” 
MS:   So now you are talking about assembly and the invention of 
recursive procedures? But your example with the neighbor again 
shows that the most general features, which appear almost together 
with and become tangible by the seed, are present as prototype. It 

would be very interesting to know how these original relations of 
objects came about. 
 
 
Variables and prototypes 
 
OW:  I have already pointed this out with respect to bootstrapping: 
What neural network developers call distributed information would, 
according to introspection, be a candidate for those clues that one 
uses to assemble a running environment. After all, the clues are not 
the running environment; they just determine the proxy object indi-
rectly by creating the preconditions for it. This is achieved by mod-
eling out, for example, the implicit laws that govern it, but we do not 
yet know this. Thereby, a coarser form of it is provided. The coarsest 
form of a proxy object we used to name prototype. 
        Furthermore, we should not forget that the gradual construction 
process automatically settles the question of variables. It is quite clear 
that the prototype has an internal dynamic, which, unlike common 
formalisms, does not depend at all on the assignment of variables. 
Relations already exist, although what they constitute does not. 
Assembly, hence, always proceeds with potential extensions, whereas 
all formalisms so far functioned by assigning variables. 
MS:   What do you mean by being unlike common formalisms? 
OW:  I mean programs such as WolframAlpha, a well-known tool on 
the Internet, which draws graphs of arbitrary functions. Programs like 
that are able to differentiate, and that is all in the sense of other kinds 
of formalism. Programs such as these compute difference quotients, 
and for this they use variables effectively like we do, when we make 
calculations with x and y on a sheet of paper. 
MS:   By “differentiate” you mean deriving mathematical functions? 
That again has to do with the symbol concept of the PSSH, because as 
far as I know, McCarthy invented his symbol expressions, i.e., the 
manipulation of symbols without any number value, exactly for this 
reason. 
150

151
OW:  Yes, exactly, I just wanted to emphasize that. I think we can also 
relate this to something we discussed at the beginning, namely Her-
bert Simon’s metaphor of different layers and different carriers of 
each layer consisting of elements held together by binding forces with 
different magnitudes of strength neutralizing each other. 
        To speak colloquially, if you compute an equation with un -
knowns, then these unknowns are called variables. Now the task is 
not the solution of the equation, but the equation is regarded as a 
description of a function yielding different values depending on what 
you assign to the variables, as mathematicians say. So an equation can 
be viewed as a formal description or a metaphor for the thought pro-
cesses proceeding in a similar fashion, insofar as in this equation the 
lowest level consists of “atomic” variables. The latters’ internal struc-
ture, however, is irrelevant. They just have to be of a certain type. 
        In mathematics, what you assign to x and y has to be numbers. 
But you can use whatever number you want. So you can assign com-
plex numbers, integers, real numbers, and rational numbers — but not 
anything else. In computers this mapping is exact. There is the 
dynamics, e.g., that a variable is squared, and the two squares are 
summed up. But what is to be squared and what is to be summed up, 
you enter as a concrete number. You cannot enter a word, otherwise 
the machine will crap out or output an error. Now the important thing 
is that the relations are always the same, no matter at which level. Yet 
there is input besides numbers, which you can also assign to variables, 
e.g., letters or words. The freer you are in choosing these variable 
types, the more general the expression or function will be. As you see, 
we are coming closer to the concept of prototype, where at first it is 
not even certain of what type the variables are. 
MS:   But the big question remains: How do human organisms build up 
this prototype from attunements representing a situation by sensori-
motor orientation and action readinesses? 
OW:  At any rate, by constructing the prototype you define what then 
becomes more and more concrete, until you reach a level correspon-
ding to your task at hand, that is, corresponding to the orientation 

you want to reach in order to be able to work with it. On this level, 
things get rather concrete — even very concrete if you calculate with 
numbers. 
MS:   This is now a rough formal description of our experiences in 
introspection, where insight progresses from an attunement to a seed, 
and further on to expansion and assembly, i.e., from the general to the 
specific. 
OW:  Yes. Expressing this by a kind of equation is quite new to math-
ematics too. I don’t think it’s more than five hundred years old, when 
it looked much clumsier. 
        Besides, this is, just as the Turing machine, an idealized descrip-
tion of observations of what the mind does and then condensed to 
rules. I think, it has the same background, viz., what the human mind 
does all the time. Its main tool is this stage of expansion, because it 
always proceeds to the degree of concreteness, which can be afforded 
or achieved. There are, after all, many problems that we cannot even 
reduce to the adequate level of understanding because we simply lack 
the respective knowledge. But that is, of course, another question. 
 
 
Recursion 
 
MS:   At the beginning of this conversation you addressed one of the 
central problems of AI. A physical symbol system provides the means 
to formalize any effective procedure, even recursive ones. Neverthe-
less it is certain that we do not yet have machines able to recognize 
and produce genuinely recursive procedures. It has not even been 
proved that this would be feasible by formal description. By using the 
term genuine recursion, which is not a technical term, I allude to your 
distinction between trivial and folding structures from your book on 
epistemology.33 There, you define the creativity of human intelligence 
precisely as the ability to “fold” trivially repeatable structures, i.e., sim-
ple analogies, into generalizing procedures.34 Generalizing problems 
and solutions affords to give regularities already discovered the form 
152

153
of modules, which then can be used multiple times according to their 
function. The computing traces, i.e., the spatiotemporal sequences 
the processes leave behind, are sometimes more important for fold-
ing than the result of the function. In these traces you can more easily 
recognize structures or loops, which you are then able to transfer to 
other data. Now, just these operational traces are invisible in parallel 
processing, e.g. in ANNs, because there they dissolve into distributed 
information. 
OW:  Yes, if engineers want to render their neural nets recursive, they 
face the problem of having to use one part of the net under different 
constraints multiple times. How to do that? The recursivity of artificial 
nets consists in the iterative “learning” process alone, i.e., in the 
approximation of a function. Yet, the execution of this function itself, 
mapping input to output, runs in parallel. Hence, it is not truly recur-
sive. The process can be made recursive only by the intervention of 
an engineer analyzing and partitioning the function into sequences. 
Already in the 1990s, Geoffrey Hinton discussed this problem in his 
seminal paper.35 He demonstrated that for every finite or fixed net-
work there are problems, which the net will only solve, if parts of it 
are used multiple times. So by parallel processing alone these prob-
lems are not solvable. Furthermore, Hinton emphasized that the mul-
tiple use of network parts still remains without orientation, because 
its “role,” that is, its operative function, to “search” for patterns and 
“copy them to the right,” for instance, must be assigned at hierarchi-
cally different levels of the net. 
        In humans, however, a trivial recursion already begins when we 
perceive an analogy. So the concept of mathematical recursivity for-
malizes psychological similarity. Speaking of recursion, we mean anal-
ogies, which is actually the clearest meaning reaching back even to 
colloquial language. I’ve always said that an analogy of two strings is 
a Turing machine accepting or generating both strings. So this implies 
that a structure that has two strings in common is an analogy of these 
two strings. This, of course, is especially evident under changing con-
straints, when in a high-level programming language a particular 

module can be called by various other modules. In theory, Hinton 
proposed a “timesharing” of network parts. Yet, as far as I am 
informed, it has never been successfully implemented. 
 
The problem of recursion in connectionist networks 
At the same time, when Harnad attempted to ground his model of 
“symbolic” thought in “causal” connectionist processes, Geoffrey E. 
Hinton analyzed the general difﬁculties of mapping sequential, ﬂex-
ible, cross-level (i.e., recursive) processes onto connectionist net-
works. Before, these problems had been neglected in the discussion 
of neural networks inspired by physics or biology, as he wrote. 
       On the one hand, Hinton defends research on these networks 
against the objections of Jerry Fodor and Zenon Pylyshyn, who 
deemed them no more than a new edition of psychological associa-
tionism. Using the example of a speciﬁc network for the “family trees 
task,” which was supposed to learn by back propagation the relations 
of two different family trees (twelve ﬁrst names of persons and their 
kinship relations each, e.g., daughter, son, uncle, mother of etc.), Hin-
ton showed by his content analyses of the optimized node weights 
that the network encoded much more than a lookup table mecha-
nism associating the “explicitly” entered values and their connec-
tions. He showed that in this “trained” artiﬁcial network, the implicit 
isomorphism of the two family trees is used for the parsimony of pro-
cessing. So it does not represent a mere association, but would have 
to be dubbed “intuitive” reasoning and could, therefore, be distin-
guished from the “rational” ﬁnding of regularities. 
       On the other hand, Hinton made clear that the ﬁxed input-to-
output mapping by such a network, i.e., by exclusively parallel pro-
cesses, is by no means sufﬁcient to solve more complex, hierarchi-
cally structured tasks. For every ﬁxed network, new, unsolvable tasks 
can be constructed, e.g., tasks requiring the solution of subtasks, 
which in turn require the same processes to be applied to different 
types of data. Hinton concluded that the criterion for distinguishing 
between “intuitive” and “rational” reasoning is not seriality (the 
154

155
repetitive execution of network parts) but the sequential aspect per-
taining to different data and levels. 
       Hinton suggested overcoming these limitations by “timesharing” 
(e.g., in visual word recognition). One part of the network is to be 
reused at different times with respect to its function (e.g., represent-
ing a letter pattern in each and every word). Hinton examined this 
reuse of network parts (or modules) ﬁrst at one functional level (e.g., 
the level of letter recognition) and then as a full-ﬂedged discrete-
step ascent and descent across different levels of part-whole hier-
archies. For the fans of “pure connectionism,” his results were sober-
ing. Reusing network parts across different levels does allow for the 
capturing of certain kinds of regularities, but only by resorting to 
sequential processes and, thereby, the loss of parallel processing. 
       Until this day, Hinton’s analyses prove the ongoing difﬁculties 
of implementing sequential recursive processes in connectionist 
networks. His proposal to “timeshare” parallel processes across dif-
ferent layers remained theoretical. There has been no successful 
implementation so far. In this context, the later attempts by Alex 
Graves et al. to implement “neural Turing machines” should be men-
tioned. 
 
Hinton, Geoffrey E., 1990. Mapping Part-Whole Hierarchies into Connection-
ist Networks. Artiﬁcial Intelligence, 46, 47–75. 
Fodor, Jerry A., and Zenon W. Pylyshyn, 1988. Connectionism and Cognitive 
Architecture: A Critical Analysis. Cognition, 28/1–2, 3–71. 
Graves, Alex, Greg Wayne, and Ivo Danihelka, 2014. Neural Turing Machines. 
arXiv:1410.5401. 
MS 
 
MS:   Perhaps it is fair to say that an essential part of human intelligence 
consists not only in the discovery or recognition of patterns but also 
in the recognition and generalization of the procedures effectuating 
this feature. When it comes to generalizing the possible emergence of 
regularities by a method, it’s not only the aforementioned devel-

opment from the general to the concrete, but perhaps also from 
implicit, sensorimotor action to an explicit model. Does the process 
of assembly in human thought imprint into the over-determined and 
contradictive landscape of seeds an order leading to generalization 
and the construction of explicit procedures? 
        This question certainly is remotely connected to Hinton’s 
attempts to train ANNs on family trees and then search for implicitly 
trained relations within these nets. But it was Hinton himself and not 
the network, who analyzed the implicitly computed weights of the 
units in the trained network. It was also he himself, who recognized 
the analogy that the network uses the same input-output structure 
for two different family trees, one with English and the other with Ital-
ian names. The recognition of over-determinacy of the network relat-
ing two phylogenetic trees may, in the broadest sense, be interpreted 
as the first step towards generalization or even to the recursive use of 
network parts. Only that in Hinton’s case this creative achievement 
was not performed by a second ANN, but by Hinton himself. It would 
be interesting to investigate the relation between the apparently triv-
ial or brute force search for regularities, on the one hand, and their 
true recursive generalization, on the other. 
OW:  Have you ever looked at how a Turing machine trivially writes 
down the multiplication table up to 100? 
MS:   Yes, to illustrate the difference in my classes, I even show the 
recursive multiplication M.TM from your textbook,36 contrasting it to 
a trivial Turing machine computing a single number only. 
OW:  You can program a trivial Turing machine for the multiplication 
table up to 100, which of course writes it on the tape much faster than 
a recursive one. It reads the two numbers of the multiplication, i.e., 
the factors, digit by digit. For each digit there are branches until the 
correct place in the table, that is the correct product, is assigned an 
output. Trivial machines are very important, firstly as examples of 
primitive recursivity, because in fact they represent the zero level of 
recursivity enabling the general statement that this level is also recur-
sive. Secondly, they are crucial, because the method of table lookup 
156

157
is the fastest implementation. Here again, we have a trade-off between 
memory and speed, because the trivial solution consumes a lot of 
memory. These are all things that need to be considered and certainly 
also play a role in our cognitive abilities. 
MS:   But then you define a very broad concept of recursivity instead 
of the usual one of calling one and the same module several times. I 
must also emphasize that the trivial machine is exclusively tailored to 
the application of the multiplication table only up to 100, whereas the 
recursive M.TM can in principle multiply any number by any other. 
The history of the invention of true recursion in multiplication would 
be relevant here. If I already have the trivial mechanism, how do I gen-
eralize it into a recursive multiplication machine? Equally relevant 
would be the question, why I am able to see what M.TM is doing, when 
watching its operations multiplying two numbers on the tape string? 
And why I am also able to see that it is doing exactly the same thing 
in two different places on the string? Why can I see this analogy 
instantly, while I have to fundamentally analyze an ANN to see it, as 
was the case in Hinton’s example of the family tree? 
OW:  That’s an interesting question. I didn’t include the following point 
in my Turing machine book, but as a paradigm I introduced the mul-
tiplication machine you are talking about.37 It has ten states and oper-
ates on a string of only two symbols, one and zero (and blank as sep-
arator). M.TM multiplies two blocks of ones, which it interprets, so to 
speak, as natural numbers in unary format, and outputs their product 
as another block of ones to the right side. After completing the multi-
plication it stops. 
        Now the basic idea was the following: I have M.TM with ten states. 
When observing the operations of M.TM on the string, everyone sees 
immediately that there are two cycles, which one can represent 
graphically as two circles. In fact the two are identical, but they occur 
twice in the table, i.e., in the program code. A run of M.TM consists of 
three cycles, of which two are actually identical. 
MS:   From a purely technical perspective the first factor is processed 
in exactly the same way as the second. It is also important that I rec-

ognize this on the basis of the trace of the run of the machine, even 
though the repetition occurs in two different areas of the string. 
OW:  Yes, exactly. My idea was to use one of these two parts of the pro-
gram twice, which is the basic idea of recursion anyway. The impor-
tant thing is that these two cycles are separately represented in the 
ten-state machine, that is, there are two different loops coded in the 
TM table. But both loops do exactly the same thing. The programming 
task I devised was how to avoid this and pack them into only one loop, 
which is then called twice. In order to achieve this you have to do 
quite amazing things: you have to outsource to the tape, i.e., to the 
string, by arranging extra compartments there. But because you have 
to arrange that in the code, it allows you to save only one state in the 
table, although one loop has four states. So the new M.TM’ code still 
comprised nine states. But it now contained a module that could be 
called alternately for two different areas of the string. If the computa-
tion were more complicated ... 
MS:   … then this ratio would become much more efficient and you 
would save many more states. 
OW:  If such a cycle does not comprise four states, but, say, hundred, 
then you could apply the same truncation method. At the same time, 
the truncation would become more and more potent, the more exten-
sive and complicated the cycles are. 
MS:   So perceiving an analogy between the two cycles does not yet suf-
fice to apply it in a recursive sense. The discovery alone is not enough. 
This is what I meant earlier. You must design or construct the machine 
as such recursively. If you want to apply a module analogously multi-
ple times, then the machine has to “remember” where it is applying 
the module in its “role,” as Hinton would say, the module has just per-
formed. Has its isolated function been applied on the first factor or 
the second? That’s what you mean by the additional “compartments,” 
isn’t it? You have to arrange for something we used to call an external 
memory: a symbol with marker function is written on the tape that 
identifies which factor is currently processed. This symbol is erased 
when a cycle is complete. 
158

159
OW:  Yes, yes, but that doesn’t really amount to an external memory. 
Practically, it’s an orientation as to which level the machine is cur-
rently working on. The machine itself doesn’t know that. Machines 
such as this lack any coarse description or prototype of what they are 
currently computing. Only we as human beings are able to observe 
the machine’s activity and then guess what it is doing. This is a kind 
of meaning production, which functions differently in humans than 
in shallow formalisms. 
MS:   If you ask me how I recognize a cycle of M.TM, I can tell you 
exactly which meaning I project onto it and what I see it repeating on 
the tape areas of the first and second factor. So I roughly grasp the 
content of a process: First, it “remembers” a symbol, “copies” this sym-
bol, and then “carries” it to the other side across the blocks out of 
ones. Then it “drops” it in the result field as a “copy” and returns back, 
and so on. Of course, the machine does not really carry symbols 
across other symbols, nor does it drop a copy of a symbol anywhere! 
This meaning manifests itself only in my head when I recognize these 
procedures. Several seeds become conscious as I watch the machine’s 
run, and they organize my orientation into a prototype of the 
observed sequence. This sequence, arranged sequentially according 
to content, helps me to interpret and recognize the concretely visible 
operations of the machine. The TM itself doesn’t know any meaning-
ful operations but is only executing state transitions. Of course, the 
Turing machine also doesn’t know at which level it currently operates 
if this information is not implemented. And this implementation 
would have to be sequential. So this has very much in common with 
the problem mentioned by Hinton about how parts of a neural net 
are to be used as generalized method over multiple levels. 
 
 
Stacks 
 
OW:  As far as orientation on different levels is concerned, in von 
Neumann computers this is implemented with a stack. In a Turing 

machine, you have to implement it in fact quite similarly. You have to 
construct some kind of stack. So I had to rebuild the M.TM into an 
M.TM’, which then uses one module twice via the coordination of a 
stack. Certainly, the same principle is used today in ANNs as well. It is 
John von Neumann himself who is credited with this wonderful inven-
tion. Actually, stacks were already patented in 1957 under the name 
of the “cellar principle” by Friedrich L. Bauer and Klaus Samelson as 
well as described independently by Charles Hamblin in Australia at 
roughly the same time. 
        The stack or stack memory is an address list, as they say, function-
ing by the method of last in/first out. By “push” you put an object onto 
a stack, which in our example is the processed first factor of the mul-
tiplication, while the second factor is being processed with the same 
module, when called. By “pull” you then take the top object out of the 
stack again, e.g., if you want to go back to the first level after having 
processed the second factor. In more complex programs, of course, 
there are many more levels, so that stacking must be conceived of as 
a dynamic tree. If one module begins operation, then an address is 
added, and as soon as this module is processed, control is passed on 
or given back and the uppermost address is deleted, until ideally at 
the end the stack is again empty. 
MS:   You quote the stack as an example of a formalized aspect of orien-
tation, which we also experience in thinking, namely to know what 
level we are at when operating with generalized methods at different 
levels. 
OW:  Yes, but I use the formalization of the stack method to compare 
them to the thought process. Well, since I suspect we’ve gone a bit too 
far with the technical aspect of the TM metaphor, I conclude by rec-
ommending everyone to think about how to implement such a stack 
as a Turing machine. Sometimes, you have deep insights based on 
finding solutions to even such simple tasks. 
MS:   One must not be discouraged by the banality of simple problems 
and solutions. For even in complex programs, these simple principles 
of modularization and of arranging heterarchies are inevitable. Most 
160

161
of all, they are certainly relevant for ordered thought. Even the most 
difficult problems of computability are based on such simple steps as 
you have shown in your textbook. 
OW:  We relate technical terms to introspection because we need clear 
formulations to delineate our thought experiences. If I want to gain 
insights into thinking, the computational metaphor — taken as an 
attempt to formalize living thought — is still a suitable method of com-
parison with the results of introspection, e.g., 
object (in the sense of object-oriented programming) <-> representa-
tion38 
symbol, pointer, token, “index entry” <-> seed 
concatenation <-> running environment, scaffold 
tree structure <-> heterarchy 
data structure <-> structure 
bootstrapping <-> expansion (of seeds), assembly, scaffold, prototype 
“pattern of activity” <-> structure 
“distributed information” <-> parameters of cognitive dynamics 
“subsymbolic” <-> tacit processes (readiness, attunement) 
“designation” <-> meaning 
“interpretation” <-> understanding 
etc. 
We need to supplement these concepts with some new concepts or 
formulations, and then summarize what has accumulated so far to 
build up an alternative theory of thought and intelligence. 
 
1          Computational metaphor refers to all efforts from the 1940s onwards to represent pro-
cesses analysed by cognitive, perceptual, or behavioral psychology — specifically human 
intelligence — by a universal descriptive computational framework (e.g., the universal 
Turing machine) or information processing (e.g., artificial neural networks). The premise 
is functional equivalence of man and machine. Understanding biological and psychological 
processes and structures on the basis of such functional metaphors or simulating their 
performance (on computers) gradually replaced the physical metaphors, e.g., of mental 
energies and forces and their causation, or of field theories. 
2         Wiener, Oswald, 2018. compMet1 (Version 21 December 2018), unpublished. 
3         Wiener, Oswald, Manuel Bonik, and Robert Hödicke, 1998. Eine elementare Einführung in 
die Theorie der Turing-Maschinen. Vienna, 10—27. 
4         Wiener, Oswald, 2007. Über das „Sehen“ im Traum: Zweiter Teil. manuskripte, 178, 170f. 

5         Eder, Thomas, and Thomas Raab (eds.), 2015. Selbstbeobachtung: Oswald Wieners Denkpsy-
chologie. Berlin. 
6         Schwarz, Michael, 2015. Geometrie und Lernen: Aspekte des Operativen und Figurativen 
in Bezug zur Laufumgebung. Ibid., 165—189. 
7         Wiener, Oswald, 2015. Glossar: Weiser. Ibid., 59—98. 
8         In German “Implikation,” “Irritation,” and “qBild,” ibid., 64—65. 
9         Piaget, Jean, 1923. La pensée symbolique et la pensée de l’enfant. Archives de Psychologie, 
18, 273—304. 
10       See “Cognitive Symbols: From Ontogeny to Their Actual Genesis. A Psychogenesis Based 
on James Mark Baldwin and Jean Piaget” by Michael Schwarz in this volume. 
11        According to Oswald Wiener “a structure of a string is a Turing machine generating or 
accepting this string.” Cf. Wiener, Oswald, 2015. Glossar: figurativ. In: Eder, Thomas, and 
Thomas Raab (eds.), Selbstbeobachtung: Oswald Wieners Denkpsychologie. Berlin, 100; id., 
1988. Form and Content in Thinking Turing Machines. In: Herken, Rolf (ed.), The Universal 
Turing Machine. Oxford, 631—657; id., 1996. Schriften zur Erkenntnistheorie. Vienna, New 
York, 112—144. 
12       Turing, Alan M., 1937. On Computable Numbers, with an Application to the Entscheidungs -
problem. Proceedings of the London Mathematical Society, 42/1, 249—254. 
13       Wiener, Oswald, Manuel Bonik, and Robert Hödicke, 1998. Eine elementare Einführung in 
die Theorie der Turing-Maschinen. Vienna, 121. 
14       Newell, Allen, and Herbert A. Simon, 1976. Computer Science as Empirical Enquiry: Sym-
bols and Search. Communications of the ACM, 19/3, 113—126. 
15       See “Cognitive Symbols: From Ontogeny to Their Actual Genesis. A Psychogenesis Based 
on James Mark Baldwin and Jean Piaget” by Michael Schwarz in this volume. 
16       Simon, Herbert A., 1969/1981. The Sciences of the Artificial. Cambridge, MA, London. 
17       Wiener, Oswald, 1996. Schriften zur Erkenntnistheorie. Vienna, New York, 82. 
18       Ibid., 83; id., 1984. Turings Test. Kursbuch, 75, 12—37. 
19       Harnad, Stevan, 1990. The Symbol Grounding Problem. Physica D, 42, 335—346. 
20      Wiener, Oswald, 2007. Über das “Sehen” im Traum: Zweiter Teil. manuskripte, 178, 168. 
21       Ibid., 170f. 
22       Wiener, Oswald, 2015. Glossar: figurativ. In: Eder, Thomas, and Thomas Raab (eds.), 2015. 
Selbstbeobachtung: Oswald Wieners Denkpsychologie. Berlin, 99—142. 
23       Wiener, Oswald, 2015. Glossar: Weiser. Ibid., 59—98. 
24       Hinton, Geoffrey E., 1990. Mapping Part-Whole Hierarchies into Connectionist Networks. 
Artificial Intelligence, 46, 47—75. 
25       According to the experiments of Bluma Zeigarnik, unfinished tasks are kept in mind espe-
cially firmly. Zeigarnik, Bluma, 1927. On Finished and Unfinished Tasks. In: Ellis, Willis D. 
(ed.), A Source Book of Gestalt Psychology. New York 1938, 300—314. Cf. “Fantasy, Repression, 
and Motivation in an Ecological Model of Memory” by Thomas Raab in this volume. 
26      Silberer, Herbert, 1909. Report on a Method of Eliciting and Observing Certain Symbolic 
Hallucination-Phenomena. In: Rapaport, David (ed.), 1965. Organization and Pathology of 
Thought. New York, 195—207. 
27       Marcel, Anthony J., 1980. Conscious and Preconscious Recognition of Polysemous. Words: 
Locating the Selective Effects of Prior Verbal Context. In: Nickerson, Raymond S. (ed.), 
Attention and Performance VIII. Hillsdale, NJ, 435—457; id., 1983. Conscious and Unconscious 
Perception: Experiments on Visual Masking and Word Recognition. Cognitive Psychology, 
15, 197—237. Cf. “Literature, Language, Thought: The Beginnings. A Conversation Between 
Oswald Wiener and Thomas Eder” in this volume, 65f.  
162

163
28       Bühler, Karl, 1908. Tatsachen und Probleme zu einer Psychologie der Denkvorgänge:  
II. Über Gedankenzusammenhänge. Archiv für die gesamte Psychologie, 12, 1—23. 
29      Piaget, Jean, 1923. La pensée symbolique et la pensée de l’enfant. Archives de Psychologie, 
18, 273—304. 
30      Newell, Allen, and Herbert A. Simon, 1976. Computer Science as Empirical Enquiry: Sym-
bols and Search. Communications of the ACM 19/3, 126. 
31       Wiener, Oswald, 1998. Wozu überhaupt Kunst? In: id. Literarische Aufsätze. Vienna, 21—41. 
32       Wiener, Oswald, Manuel Bonik, and Robert Hödicke, 1998. Eine elementare Einführung in 
die Theorie der Turing-Maschinen. Vienna, 59—63. 
33       Wiener, Oswald, 1996. Schriften zur Erkenntnistheorie. Vienna, New York. 
34      Ibid., 237. 
35       Hinton, Geoffrey E., 1990. Mapping Part-Whole Hierarchies into Connectionist Networks. 
Artificial Intelligence, 46, 47—75. 
36      Wiener, Oswald, Manuel Bonik, and Robert Hödicke, 1998. Eine elementare Einführung in 
die Theorie der Turing-Maschinen. Vienna, 14—26. 
37       Ibid. 
38      Wiener, Oswald, 2007. Über das „Sehen“ im Traum: Zweiter Teil. manuskripte, 178, 168. 
 


