arXiv:2108.08735v1  [cs.IR]  19 Aug 2021
1
SiReN: Sign-Aware Recommendation
Using Graph Neural Networks
Changwon Seo, Kyeong-Joong Jeong, Sungsu Lim, Member, IEEE, and Won-Yong Shin, Senior Member, IEEE
Abstract—In recent years, many recommender systems using
network embedding (NE) such as graph neural networks (GNNs)
have been extensively studied in the sense of improving rec-
ommendation accuracy. However, such attempts have focused
mostly on utilizing only the information of positive user–item
interactions with high ratings. Thus, there is a challenge on
how to make use of low rating scores for representing users’
preferences since low ratings can be still informative in designing
NE-based recommender systems. In this study, we present SiReN,
a new Sign-aware Recommender system based on GNN models.
Speciﬁcally, SiReN has three key components: 1) constructing
a signed bipartite graph for more precisely representing users’
preferences, which is split into two edge-disjoint graphs with
positive and negative edges each, 2) generating two embeddings
for the partitioned graphs with positive and negative edges via
a GNN model and a multi-layer perceptron (MLP), respectively,
and then using an attention model to obtain the ﬁnal embeddings,
and 3) establishing a sign-aware Bayesian personalized ranking
(BPR) loss function in the process of optimization. Through
comprehensive experiments, we empirically demonstrate that
SiReN consistently outperforms state-of-the-art NE-aided rec-
ommendation methods.
Index Terms—Bayesian personalized ranking (BPR) loss,
graph neural network, network embedding, recommender sys-
tem, signed bipartite graph.
I. INTRODUCTION
A. Background and Motivation
R
COMMENDER systems have been widely advocated as
a way of providing suitable recommendation solutions
to customers in various ﬁelds such as e-commerce, advertis-
ing, and social media sites. One of the most important and
popular techniques in recommender systems is collaborative
ﬁltering (CF), which computes similarities between users and
items from historical interactions (e.g., clicks and purchases)
to suggest relevant items to users by assuming that users
who have behaved similarly with each other exhibit similar
preferences for items [1]–[3]. Moreover, following up the great
success of network embedding (NE), also known as network
representation learning, considerable research attention has
been paid to NE-based recommender systems that attempt to
model high-order connectivity information from user–item in-
teractions viewed as a bipartite graph [4]–[7].1 In recent years,
C. Seo, K.-J. Jeong, and W.-Y. Shin are with the School of Mathe-
matics and Computing (Computational Science and Engineering), Yonsei
University, Seoul 03722, Republic of Korea (e-mail: {changwoni, jeongkj,
wy.shin}@yonsei.ac.kr).
S. Lim is with the Department of Computer Science and Engineering,
Chungnam National University, Daejeon 34134, Republic of Korea (e-mail:
sungsu@cnu.ac.kr).
(Corresponding author: Won-Yong Shin.)
1In the following, we use the terms “graph” and “network” interchangeably.
graph neural networks (GNNs) [8]–[12] have emerged as a
powerful neural architecture to learn vector representations
of nodes and graphs. By virtue of great prowess in solving
various downstream machine learning problems, GNN-based
recommender systems [13]–[19] have also been developed for
improving the recommendation accuracy.
GNN models are basically trained by aggregating the infor-
mation of direct neighbor nodes via message passing under
the homophily (or assortativity) assumption that a target node
and its neighbors are similar to each other [12]. Due to
the neighborhood aggregation mechanism, existing literature
posits that high homophily of the underlying graph is a
necessity for GNNs to achieve good performance especially
on node classiﬁcation [20]–[22]. On the other hand, in rec-
ommender systems, while users’ feedback on many online
websites (e.g., likes/dislikes on YouTube and high/low ratings
on Amazon) can be positive and negative, existing GNN-
based recommender systems overlook the existence of neg-
ative feedback (i.e., user–item interactions with low ratings)
due to their ease of modeling. Precisely, most GNN-based
approaches utilize only positive feedback by removing the
negative interactions in order to exhibit strong homophily in
neighbors (see Fig. 1).2 It is worthwhile to note that, despite
the remarkable performance boost by existing GNN-based
recommender systems, the low ratings can be still informative.
This is because such information expresses signs of what
users dislike. In other words, full exploitation of two types of
feedback in GNNs may have the potential to further improve
the recommendation performance, which remains a new design
challenge.
B. Main Contributions
Even with the wide applications of GNNs to recommender
systems [13]–[19], a question that arises naturally is: “How
can we make use of negative feedback (i.e., low rating scores)
for representing users’ preferences via GNNs?”. In this paper,
to answer this question, we introduce SiReN, a Sign-aware
Recommender system based on GNN models. To this end, we
design and optimize our new GNN-aided learning model while
distinguishing users’ positive and negative feedback.
Our proposed SiReN method includes three key com-
ponents: 1) signed graph construction and partitioning, 2)
model architecture design, and 3) model optimization. First,
2A graph is usually referred to as homophilous (or assortative) if connected
nodes are much more likely to have the same class label than if edges are
independent of labels. In our study, a homophilous bipartite graph assumes
that user–item relations are signiﬁcantly more likely to be positive.

2
Fig. 1: New challenges in GNN-based recommender systems.
to overcome the primary problem of existing GNN-based
recommender systems [15], [18], [19] that fail to learn both
positive and negative relations between users and items, we
start by constructing a signed bipartite graph, which is split
into two edge-disjoint graphs with positive and negative edges
each. This signed graph construction and partitioning process
enables us to more distinctly identify users’ preferences for
observed items. Second, we show how to design our model
architecture for discovering embeddings of all users and items
in the signed bipartite graph. Although several GNN models
were introduced in [23], [24] for signed unipartite graphs,
simply applying them to recommender systems, corresponding
to user–item bipartite graphs, may not be desirable. This is
because such GNN models were built upon the assumption
of balance theory [25], which implies that “the friend of my
friend is my friend” and “the enemy of my enemy is my friend”.
However, the balance theory no longer holds in recommender
systems since users’ preferences cannot be dichotomous. In
other words, users are likely to have dissimilar preferences
even if they dislike the same item(s). This motivates us to
design our own model architecture for each partitioned graph,
rather than employing existing GNN approaches based on
the balance theory. Concretely, SiReN contains three learning
models. For the graph with positive edges, we employ a GNN
model suit for recommender systems. For the graph with
negative edges, we adopt a multi-layer perceptron (MLP) due
to the fact that negative edges can weaken the homophily
and thus message passing to such dissimilar nodes would
not be feasible. To obtain the ﬁnal embeddings, we then
use an attention model that learns the importance of two
embeddings generated from GNN and MLP models. Third,
as our objective function in the process of optimization, we
present a sign-aware Bayesian personalized ranking (BPR)
loss function, which is built upon the original BPR loss
[26] widely used in recommender systems. More speciﬁcally,
unlike the original BPR loss, our objective function takes into
account two types of observed items, including both positive
and negative relations between users and items, as well as
unobserved ones.
To validate the superiority of our SiReN method, we
comprehensively perform empirical evaluations using various
real-world datasets. Experimental results show that our method
consistently outperforms state-of-the-art GNN methods for
top-N recommendation in terms of several recommendation
accuracy metrics. Such a gain is possible owing to the use of
low rating information along with judicious model design and
optimization. We also empirically validate the effectiveness of
MLP in comparison with other model architectures used for
the graph with negative edges. Additionally, our experimental
results demonstrate the robustness of our SiReN method to
more challenging interaction sparsity levels.
It is worth noting that our method is GNN-model-agnostic
and thus any competitive GNN architectures can be appro-
priately chosen for potentially better performance. The main
technical contributions of this paper are four-fold and summa-
rized as follows:
• We propose SiReN, a novel GNN-aided recommender
system that makes full use of the user–item interaction
information after signed graph construction and partition-
ing;
• We design our model architecture using three learning
models in the sense of utilizing sign awareness so as to
generate embeddings of users and items;
• We establish the sign-aware BPR loss as our objective
function;
• We validate SiReN through extensive experiments using
three real-world datasets while showing the superiority of
our method over state-of-the-art NE-aided methods under
diverse conditions.
C. Organization and Notation
The remainder of this paper is organized as follows. In
Section II, we present prior studies related to our work. In
Section III, we explain the methodology of our study, includ-
ing basic settings and an overview of our SiReN method.
Section IV describes technical details of the proposed method.
Comprehensive experimental results are shown in Section V.
Finally, we provide a summary and concluding remarks in
Section VI.
Table I summarizes the notation that is used in this paper.
This notation will be formally deﬁned in the following sections
when we introduce our methodology and the technical details.
II. RELATED WORK
The method that we proposed in this study is related to two
broader research lines, namely standard CF approaches and
NE-based recommendation approaches.

3
Notation
Description
G
Given weighted bipartite graph
U
Set of users in G
V
Set of items in G
E
Set of weighted edges in G
M
Number of users
N
Number of items
Gs
Signed bipartite graph constructed from G
Es
Set of signed edges in Gs
Gp
Partitioned graph with positive edges
Ep
Set of positive edges in Gp
Gn
Partitioned graph with negative edges
En
Set of negative edges in Gn
Zp
Embeddings for all nodes in Gp
Zn
Embeddings for all nodes in Gn
Z
Final embeddings
αp
Importance of Zp
αn
Importance of Zn
θ1
Model parameters of GNN
θ2
Model parameters of MLP
θ3
Model parameters of ATTENTION
TABLE I: Summary of notations.
A. Standard CF Approaches
As one of the most popular techniques, CF in recommender
systems aims to capture the relationships between users and
items from historical interactions (e.g., ratings and purchases)
by discovering learnable vector representations for users and
items based on a rating matrix [2], [27]. Matrix factorization
(MF) [28] decomposes the user–item interaction matrix into
a product of two low-dimensional matrices and models their
similarities with the dot product of two matrices. ANLF [29]
was designed based on a non-negative MF (NMF) model
[30] using the alternating direction method. In DMF [31] and
NCF [32], MF models with neural network architectures were
proposed to project users and items into a latent structured
space. Moreover, SLIM [33] proposed a sparse linear model by
directly reconstructing the low-density user–item interaction
matrix. To capture complex relationships between items in
sparse datasets, FISM [34] and NPE [35] showed how to
learn item–item similarities as the product of two latent factor
matrices. While MF-based models such as [31], [32] rely
on the dot product of user and item latent vectors as a
similarity measure, CML [1] showed how to learn a joint
metric space to encode not only users’ preferences but also
user–user and item–item similarities in the Euclidean distance.
In NAIS [36], the attention mechanism was incorporated to
obtain the importance of each item from user–item interactions
for preference prediction. DLMF [37] designed a trust-aware
recommender system that leverages deep learning to determine
the initialization in MF by synthesizing the users’ interests
and their trusted friends’ interests in addition to the user–item
interactions.
B. NE-Based Recommendation Approaches
Recently, it has been comprehensively studied how to de-
velop recommender systems using NE. While standard CF
approaches are capable of only modeling the ﬁrst-order con-
nectivity between users and items, NE-based approaches aim
to exploit high-order proximity among users and items through
the user–item bipartite graph structure [15], [19]. BiNE [6] and
CSE [7] were developed based on random walks in order to
infer similar users (or items) in the underlying bipartite graph
(i.e., user–item interactions).
Moreover, encouraged by the success of GNNs in solving
many graph mining tasks [12], GNN-based recommender
systems have more recently emerged as promising techniques
[13]–[19], [38], [39]. Existing GNN models for top-N recom-
mendation were generally developed using implicit feedback,
which treats observed user–item interactions as positive rela-
tions. GC-MC [38] presented a graph autoencoder including
graph convolutions for rating matrix completion. PinSage [13]
showed a scalable GNN framework developed in production
at Pinterest by improving upon GraphSAGE [9] along with
several aggregation functions. Spectral CF [14] presented a
spectral convolution operation to learn the rich information
of connectivity between users and items in the spectral
domain. As state-of-the-art NE-based recommender systems
using only user–item interactions as input, NGCF [15], LR-
GCCF [18], and LightGCN [19] were developed based on
GCN [8], the ﬁrst attempt to apply convolutions to graph
domains, while performing layer aggregation to solve the over-
smoothing problem. IGMC [39] proposed an inductive GNN-
based matrix completion model that learns local subgraphs
in the underlying user–item interaction matrix. To explore
users’ latent purchasing motivations (e.g., cost effectiveness
and appearance), MCCF [16] presented a CF approach based
on GNNs for generating multiple user/item embeddings and
then combining them via attention mechanisms. AGCN [17]
proposed a GCN approach for joint item recommendation and
attribute inference in an attributed user–item bipartite graph
with incomplete attribute values.
To alleviate the sparsity of user–item interactions, each
user’s local neighbors’ preferences in social networks were
also utilized in [40]–[42] for better user embedding modeling,
thus enabling us to improve the recommendation accuracy.
C. Discussions
Despite the aforementioned contributions, leveraging ex-
plicit feedback data (i.e., user–item interactions with ratings)
in NE-based approaches has been largely underexplored in the
literature. To learn vector representations for users and items
based on the NE, it is common to utilize only positive user–
item interactions as observed data when explicit feedback data
are given (see [7], [40], [42] and references therein). However,
negative interactions with low ratings can be still informative
since such information shows signs of what users dislike. It
remains open how to make use of low ratings in designing
NE-aided recommender systems.
On the other hand, several GNN models were developed in
[23], [24] to learn vector representations of nodes in signed

4
(unipartite) graphs with both positive and negative edges based
on the structural balance theory [25]. However, it is worth
noting that the balance theory does not hold in recommender
systems due to the fact that users’ preferences cannot be
dichotomous, i.e., a behavior of users disliking similar items
does not always imply the same degree of user preferences
about items. Therefore, adopting such GNN methods designed
for signed graphs would not be desirable to capture different
levels of user preferences.
III. METHODOLOGY
In this section, we describe our network model with basic
settings. Next, we explain an overview of the proposed SiReN
method as a solution to the problem of making full use of low
ratings in GNN models.
A. Network Model and Basic Settings
In recommender systems, the basic input is the historical
user–item interactions with ratings, which is represented as a
weighted bipartite graph. Let us denote the underlying bipartite
graph as G = (U, V, E), where U and V are the set of
M users and the set of N items, respectively, and E is the
set of weighted edges between U and V. A weighted edge
(u, v, wuv) ∈E can be interpreted as the rating wuv with
which the user u ∈U has given to the item v ∈V. We assume
G to be a static network without repeated edges, where ratings
(i.e., user preferences) do not change over time.
In our study, we aim at designing a new GNN-aided
recommender system for improving the accuracy of top-N
recommendation by making full use of the user–item rating
information in G, including low ratings that have not been
explored by conventional GNN-based recommender systems
[18], [19], without any side information.
B. Overview of SiReN
In this subsection, we explain our methodology along with
the overview of the proposed SiReN method. We recall that
our study is motivated by the fact that recent recommender
systems built upon GNN models such as [18], [19] take
advantage of only high rating scores as observed data by
deleting some edges, corresponding to low ratings (e.g., the
rating scores of 1 and 2 in the 1–5 rating scale), in the set
E over the weighted bipartite graph G [7], [40], [42]. This
is because such a removal of low ratings from G enables us
to aggregate the positively connected neighbors via message
passing in GNNs. However, the set of negative interactions
indicates what users dislike and thus is still quite informative.
In other words, it remains open how to fully exploit the rating
information in building GNN-based recommender systems as
recently developed models fail to capture the effect of low
rating scores.
To tackle this challenge, we present SiReN, a new sign-
aware recommender system using GNNs, which is basically
composed of the following three core components (refer to
Fig. 2):
• signed bipartite graph construction and partitioning
• embedding generation for each partitioned graph
• optimization via a sign-aware BPR loss.
First, we describe how to construct a signed bipartite
graph Gs that enables us to more distinctly identify users’
preferences based on all the user–item interactions. More
speciﬁcally, we construct Gs = (U, V, Es) with a parameter
wo > 0, representing a criterion for dividing high and low
ratings, where
Es =

(u, v, ws
uv)
ws
uv = wuv −wo, (u, v, wuv) ∈E
	
. (1)
Here, wo can be determined according to characteristics (e.g.,
the rating scale and the popularity distribution of items) of
a given dataset; from an algorithm design perspective, we
assume that a user u likes an item v if ws
uv > 0 and he/she
dislikes v otherwise, where ws
uv = wuv −wo corresponds to
the edge weight in the signed bipartite graph Gs. Note that,
although representation learning, including GNNs, on signed
graphs has been studied in [23], [24], [43], designing GNN-
based recommender systems on signed bipartite graphs has
never been conducted in the literature. Basically, GNN models
are trained by aggregating the information of neighbor nodes
under the homophily assumption [12]. However, due to the
fact that the signed graph Gs includes negative edges (i.e.,
interactions with low ratings), aggregating the information of
such negatively connected neighbors may not be desirable. As
illustrated in Fig. 2, to more delicately capture each relation
of positively and negatively connected neighbors, we then
partition the signed bipartite graph Gs into two edge-disjoint
graphs Gp = (U, V, Ep) and Gn = (U, V, En), consisting of
the set of positive edges and negative edges, respectively. Here,
it follows that Es = Ep ∪En, where
Ep =

(u, v, ws
uv)
 ws
uv > 0, (u, v, ws
uv) ∈Es	
(2a)
En =

(u, v, ws
uv)
 ws
uv < 0, (u, v, ws
uv) ∈Es	
.
(2b)
The purpose of this graph partitioning is to make the graphs
Gp and Gn, respectively, assortative and disassortative so that
each partitioned graph is used as input to the most appropriate
learning model.
Second, we describe how to generate embeddings of M
users and N items along with three learning models in our
SiReN method. Using the graph Gp having positive edges,
we adopt a GNN model suit for recommender systems (e.g.,
[18], [19]) to calculate embedding vectors Zp ∈R(M+N)×d
for the nodes in U ∪V:
Zp = GNNθ1(Gp),
(3)
where d is the dimension of the embedding space and θ1 is
the learned model parameters of GNN. On the other hand,
we adopt an MLP for the graph Gn to calculate embedding
vectors Zn ∈R(M+N)×d for the same nodes as those in (3):
Zn = MLPθ2(Gn),
(4)
where θ2 is the learned model parameters of MLP. We would
like to state the following two remarks to explain the model
selection according to types of graphs.
Remark 1. It is worth noting that existing GNN models,
built upon message passing architectures, work on the basic

5
Fig. 2: The schematic overview of our SiReN method.
assumption of homophily [22]. We recall that users who
dislike similar items may not have similar preferences (i.e.,
the tendency in rating items) with each other. Negative edges
in Gn can undermine the effect of homophily and thus message
passing to such dissimilar nodes would not be feasible. For
these reasons, adopting GNNs in the graph with negative edges
may not be desirable, based on the fact that many GNNs fail
to generalize to disassortative graphs, i.e., graphs with low
levels of homophily [20], [21], [44].
Remark 2. Additionally, note that the MLP architecture itself
does not exploit the topological information. However, it does
not imply that the connectivity information in the graph Gn
is not used at all. In the optimization step, we update the
embedding vectors in (3) and (4) by fully taking advantage of
the set En in Gn as well as the set Ep in Gp, which will be
speciﬁed later.
Next, let us mention another training model in SiReN,
the so-called attention model. To get the importance of two
embeddings Zp and Zn, we use the attention mechanism [45]
that learns the corresponding importance (αp, αn) as follows:
(αp, αn) = ATTENTIONθ3(Zp, Zn),
(5)
which results in the ﬁnal embeddings:
Z = (αp1attn) ⊙Zp + (αn1attn) ⊙Zn,
(6)
where αp, αn ∈R(M+N)×1; 1attn ∈R1×d is the all-ones
vector; ⊙denotes the Hadamard (element-wise) product; θ3 is
the learned model parameters of ATTENTION; and each row
of Z ∈R(M+N)×d indicates the embedding vector of each
node in U ∪V.
Third, we turn to the optimization of model parameters
{θ1, θ2, θ3}, which updates the embeddings Z accordingly.
In our study, we adopt the BPR loss [26], which has been
widely used in recommender systems to comprehensively learn
what users prefer from the historical user–item interactions.
Nevertheless, simply applying the existing BPR loss to our
setting does not precisely capture the relations of negatively
connected neighbors; thus, we establish a sign-aware BPR
loss, which is a new BPR-based loss that takes into account
both positive and negative relations in the signed bipartite
graph Gs, while accommodating the sign of edges in Gs as
an indicator of what users like and dislike.
In the next section, we shall describe implementation details
of the proposed SiReN method.
IV. PROPOSED SIREN METHOD
In this section, we elaborate on our SiReN method, de-
signed for top-N recommendation. SiReN has the following
three key components: 1) constructing a signed bipartite graph
Gs for more precisely representing users’ preferences, which
is split into two graphs Gp and Gn with positive and negative
edges each, 2) generating two embeddings Zp and Zn for
the partitioned graphs with positive and negative edges via
a GNN model and an MLP, respectively, and then using the
attention model to learn the importance of Zp and Zn, and 3)

6
Algorithm 1 : SiReN
Input: G, wo, Θ ≜{θ1, θ2, θ3}, K (number of negative
samples), λreg (regularization coefﬁcient)
Output: Z
1: Initialization: Θ ←random initialization
2: /* Graph partitioning */
3: Construction of Gs from G along with wo
4: Splitting Gs into Gp and Gn
5: while not converged do
6:
Sample a batch DS from Gs with K negative samples
7:
for D′
S ⊂DS do
8:
/* Generation of embeddings */
9:
Zp ←GNNθ1(Gp)
10:
Zn ←MLPθ2(Gn)
11:
(αp, αn) ←ATTENTIONθ3(Zp, Zn)
12:
Z ←(αp1attn) ⊙Zp + (αn1attn) ⊙Zn
13:
/* Optimization */
14:
L0 ←sign-aware BPR loss
15:
L ←L0 + λreg∥Θ∥2
16:
Update Θ by taking one step of gradient descent
17:
end for
18: end while
19: return Z
establishing a sign-aware BPR loss function in the process of
optimization. The overall procedure of the proposed SiReN
method is summarized in Algorithm 1.
As one of main contributions to the design of our method,
we start by constructing the signed bipartite graph Gs and then
partitioning Gs into two edge-disjoint graphs Gp and Gn for
exploiting the relation of positively and negatively connected
neighbors.
In the following subsections, we explain how we discover
embeddings of all nodes (i.e., users and items) and optimize
our model via the sign-aware BPR loss during the training
phase.
A. Network Architecture
In this subsection, we describe how to generate the em-
beddings of M users and N items in our SiReN method.
As stated in Section III-B, SiReN basically contains three
learning models, i.e., GNNθ1, MLPθ2, and ATTENTIONθ3.
To generally indicate either users or items interchangeably,
we denote a node in the graphs Gp and Gn, which can be in
either U or V, by x.
First, we describe the GNN model, which is designed to
be model-agnostic. To this end, we show a general form of
the message passing mechanism [9], [10], [46] in which we
iteratively update the representation of each node by aggre-
gating representations of its neighbors using two functions,
namely AGGREGATEℓ
x and UPDATEℓ
x, along with model
parameters of GNNθ1 in (3). Formally, at the ℓ-th layer of a
GNN, AGGREGATEℓ
x aggregates (latent) feature information
from the local neighborhood of node x in Gp at the (ℓ−1)-th
GNN layer as follows:
mℓ
x ←AGGREGATEℓ
x
 
hℓ−1
y
y ∈Nx ∪{x}
	
,
(7)
Fig. 3: The GNN architecture in our SiReN method, composed
of three functions in (7)–(9).
where hℓ−1
x
∈R1×dℓ−1
GNN denotes the dℓ−1
GNN-dimensional latent
representation vector of node x at the (ℓ−1)-th GNN layer, Nx
is the set of neighbor nodes of x in Gp, and mℓ
x ∈R1×dℓ−1
GNN is
the aggregated information for node x at the ℓ-th GNN layer.
Since x belongs to a node in either U or V, AGGREGATEℓ
x
aggregates feature information of connected items if x is a user
node, and vice versa. In the update step, we use UPDATEℓ
x
to obtain the ℓ-th embedding vector hℓ
x from the aggregated
information mℓ
x as follows:
hℓ
x ←UPDATEℓ
x
 x, mℓ
x

.
(8)
We note that, for each node x, we randomly initialize the
learnable 0-th embeddings (i.e., h0
x) due to the fact that we
have no side information for users and items in our setting as
in [18], [19]. Additionally, we present another function in our
GNN model, namely LAYER-AGGLGNN
x
, which performs layer
aggregation similarly as in [47]. This operation is motivated by
the argument that oversmoothing tends to occur in GNN-based
recommender systems if the last GNN layer’s embedding vec-
tors are used as the ﬁnal embedding Zp [48]. To alleviate the
oversmoothing problem, we calculate the embedding vector
zp
x ∈R1×d of node x via layer aggregation as follows:
zp
x ←LAYER-AGGLGNN
x

hℓ
x
	ℓ=LGNN
ℓ=0

,
(9)
which results in the embeddings Zp for the graph Gp where
LGNN is the number of GNN layers. The GNN architecture
of our SiReN method including the above three functions
AGGREGATEℓ
x, UPDATEℓ
x, and LAYER-AGGLGNN
x
is illus-
trated in Fig. 3.

7
Remark 3. Now, let us state how the above three functions
in (7)–(9) can be speciﬁed by several types of GNN-based
recommender systems. As one state-of-the-art method, LR-
GCCF [18] can be implemented by using
AGGREGATEℓ
x =
X
y∈Nx∪{x}
1
p
|Nx| + 1
p
|Ny| + 1
hℓ−1
y
(10a)
UPDATEℓ
x = mℓ
x · W ℓ
GNN
(10b)
LAYER-AGGLGNN
x
= h0
x
h1
x
 · · ·
hLGNN
x
,
(10c)
where W ℓ
GNN ∈Rdℓ−1
GNN ×dℓ
GNN is a learnable weight transforma-
tion matrix,
 is the concatenation operator, and LGNN is the
number of GNN layers. In addition, as another state-of-the-art
method for recommendation, LightGCN [19] can be speciﬁed
according to the following function setting:
AGGREGATEℓ
x =
X
y∈Nx
1
p
|Nx|
p
|Ny|
hℓ−1
y
(11a)
UPDATEℓ
x = mℓ
x
(11b)
LAYER-AGGLGNN
x
=
1
LGNN + 1
LGNN
X
ℓ=0
hℓ
x.
(11c)
Second, we pay our attention to the MLP architecture
designed for the graph Gn having negative edges. We calculate
the embeddings Zn using the MLP as follows:
Zn
ℓ= ReLU
 Zn
ℓ−1W ℓ
MLP + 1MLPbℓ
MLP

(12a)
Zn = Zn
LMLP,
(12b)
where ℓ= 1, 2, · · · , LMLP; LMLP is the number of MLP layers;
ReLU(x) = max (0, x); W ℓ
MLP ∈Rdℓ−1
MLP ×dℓ
MLP is a learnable
weight transformation matrix; bℓ
MLP ∈R1×dℓ
MLP is a bias vec-
tor; dℓ
MLP is the dimension of the latent representation vector
Zn
ℓat the ℓ-th MLP layer; 1MLP ∈R(M+N)×1 is the all-ones
vector; and Zn
0 ∈R(M+N)×d0
MLP is the learnable 0-th layer’s
embedding matrix for all nodes in the set U ∪V, which is
randomly initialized. That is,

W ℓ
MLP
	ℓ=LMLP
ℓ=1
,

bℓ
MLP
	ℓ=LMLP
ℓ=1
,
and Zn
0 correspond to the model parameters of MLPθ2 in (4).
Third, we turn to describing the attention model. The
importance (αp, αn) in (5) represents the attention values of
two embeddings Zp and Zn for all nodes in U∪V. Let us focus
on node x ∈U∪V whose embedding vectors calculated for the
graphs Gp and Gn are given by zp
x, zn
x ∈R1×d, respectively.
Let wp
x and wn
x denote attention values of the two embeddings
zp
x and zn
x, respectively, for node x. Then, our attention model
learns a weight transformation matrix Wattn ∈Rd′×d, an
attention vector q ∈Rd′×1, and a bias vector b ∈Rd′×1
with a dimension d′, corresponding to the model parameters
of ATTENTIONθ3 in (5), as follows:
wp
x = qT tanh(Wattnzp
x
T + b)
(13a)
wn
x = qT tanh(Wattnzn
x
T + b),
(13b)
where tanh(x) =
exp(x)−exp(−x)
exp(x)+exp(−x) is the hyperbolic tangent
activation function. By normalizing the attention values in
(13a) and (13b) according to the softmax function, we have
αp
x =
exp(wp
x)
exp(wp
x) + exp(wnx)
(14a)
αn
x =
exp(wn
x)
exp(wp
x) + exp(wnx),
(14b)
where αp
x and αn
x are the resulting importance of two em-
beddings zp
x and zn
x, respectively, which thus yields the ﬁnal
embedding zx = αp
xzp
x + αn
xzn
x for each node x in (6).
B. Optimization
In this subsection, we explain the optimization of SiReN
method in the training phase via our proposed loss function.
We start by randomly initializing the learnable model param-
eters Θ = {θ1, θ2, θ3} in (3)–(5) (refer to line 1 in Algorithm
1). To train our learning models (i.e., the GNN, MLP, and
attention models), we use a batch DS consisting of multiple
samples of a triplet (u, i, j), where (u, i, ws
ui) ∈Es and j ∈V
is a negative sample (i.e., an unobserved item), which is not
in the set of direct neighbors of user u in the signed bipartite
graph Gs (refer to line 6).3 More speciﬁcally, we ﬁrst acquire
the set of edges, Es, in Gs and then sample K negative
samples {jn}K
n=1 for each (u, i, ws
ui) ∈Es in order to create
new samples of a node triplet (u, i, jn) for all n ∈{1, · · · , K},
which yields the batch DS as follows:
DS =

(u, i, jn)
(u, i, ws
ui) ∈Es, jn /∈Nu, n ∈{1, · · · , K}
	
,
(15)
where Nu is the set of neighbor nodes of user u in Gs. We
further subsample mini-batches D′
S ⊂DS to efﬁciently train
our learning models (refer to line 7). The sampled triplets are
fed into the loss function in the training loop along with the
calculated embeddings Z for all nodes in U ∪V (refer to lines
9–12).
Now, we present our sign-aware BPR loss function, which
is built upon the original BPR loss [26] widely used in
recommender systems (see [14], [15], [17]–[19], [40], [42] and
references therein). To this end, we deﬁne a user u’s predicted
preference for an item i as the inner product of user and item
ﬁnal embeddings:
ˆrui ≜zuzT
i ,
(16)
which is used for establishing our loss function.
However, simply employing the original BPR loss is not
desirable in our setting since it is a pairwise loss based on
the relative order between observed and unobserved items by
basically assuming that high ratings are more reﬂective of a
user’s preferences with higher prediction values of ˆrui than
the case of unobserved ones. On the other hand, our objective
function should account for two types of observed items,
which include both positive and negative relations between
users and items, as well as unobserved ones. The proposed
sign-aware BPR loss function is designed in such a way that
3For sampling j ∈V, we follow a negative sampling strategy using the
degree-based unigram noise distribution [49], [50].

8
the predicted preference for an observed item (or its negative
value when the observed item is negatively connected) is
higher than its unobserved counterparts.
More formally, we deﬁne a ternary relation >u (i, j, w) ≜
{(i, j, w)|ˆrui > ˆruj if w > 0 and −ˆrui > ˆruj otherwise} ⊂
V × V × (R \ {0}). Based on the relation >u, we aim at
minimizing the following loss function L for a given mini-
batch D′
S with the L2 regularization:
L = L0 + λreg∥Θ∥2 ,
(17)
where λreg is a hyperparameter that controls the L2 regular-
ization strength; Θ represents the model parameters; and L0
is the sign-aware BPR loss term realized by
L0 = −
X
(u,i,j)∈D′
S
log p(>u (i, j, ws
ui)
Θ).
(18)
Here, to capture the above relation >u (i, j, w) for each
(u, i, j) ∈D′
S, we model the likelihood in (18) as
p(>u (i, j, ws
ui)
Θ) ≜σ
 sgn(ws
ui)ˆrui −ˆruj

,
(19)
where sgn(·) is the sign function and σ(x) =
1
1+exp(−x) is the
sigmoid function. By training our models through the loss in
(17), it is possible to more elaborately learn representations of
nodes depending on both positive and negative relations.
V. EXPERIMENTAL EVALUATION
In this section, we ﬁrst describe real-world datasets used
in the evaluation. We also present ﬁve competing methods
including two baseline MF methods and three state-of-the-
art GNN-based methods for comparison. After describing
performance metrics and our experimental settings, we com-
prehensively evaluate the performance of our SiReN method
and ﬁve benchmark methods. The source code for SiReN can
be accessed via https://github.com/woni-seo/SiReN-reco.
A. Datasets
We conduct experiments on three real-world datasets, which
are widely adopted for evaluating recommender systems. For
all experiments, we use user–item interactions with ratings
in each dataset as the input. The main statistics of each
dataset, including the number of users, the number of items,
the number of ratings, the density, and the rating scale, are
summarized in Table II. In the following, we explain important
characteristics of the datasets brieﬂy.
MovieLens-1M (ML-1M)4. This is the most popular
dataset in movie recommender systems, which consists of 5-
star ratings (i.e., integer values from 1 to 5) of movies given
by users [51].
Amazon-Book5. Among the Amazon-Review dataset con-
taining product reviews and metadata, we select the Amazon-
Book dataset, which consists of 5-star ratings [52]. We remove
users/items that have less than 20 interactions similarly as in
[53].
4https://grouplens.org/datasets/movielens/1m/.
5https://jmcauley.ucsd.edu/data/amazon/index.html.
Dataset
ML-1M
Amazon-Book
Yelp
# of users (M)
6,040
35,736
41,772
# of items (N)
3,952
38,121
30,037
# of ratings
1,000,209
1,960,674
2,116,215
Density (%)
4.19
0.14
0.16
Rating scale
1–5
1–5
1–5
TABLE II: Statistics of three real-world datasets.
Yelp6. This dataset is a local business review data consisting
of 5-star ratings. As in the Amazon-Book dataset, we remove
users/items that have less than 20 interactions.
B. Benchmark Methods
In this subsection, we present two baseline MF methods and
three state-of-the-art GNN methods for comparison.
BPRMF [26]. This baseline method is a MF model opti-
mized by the BPR loss, which assumes that each user prefers
the items with which he/she has interacted to items with no
interaction.
NeuMF [32]. As another popular baseline, NeuMF is a
neural CF model, which generalizes standard MF and uses
multiple hidden layers to generate user and item embeddings.
NGCF [15]. This state-of-the-art GNN-based approach
follows basic operations inherited from the standard GCN
[8] to explore the high-order connectivity information. More
speciﬁcally, NGCF stacks embedding layers and concatenates
embeddings obtained in all layers to constitute the ﬁnal
embeddings.
LR-GCCF [18]. LR-GCCF is a state-of-the-art GCN-based
CF model. As two main characteristics, this model uses
only linear transformation without nonlinear activation and
concatenates all layers’ embeddings to alleviate oversmoothing
at deeper layers [48].
LightGCN [19]. LightGCN simpliﬁes the design of GCN
[8] to make the model more appropriate for recommendation
by including only the most essential component such as
neighborhood aggregation without nonlinear activation and
weight transformation operations. Similarly as in LR-GCCF,
this approach uses the weighted sum of embeddings learned
at all layers as the ﬁnal embedding.
C. Performance Metrics
To validate the performance of the proposed SiReN method
and the ﬁve benchmark methods, we adopt three metrics,
which are widely used to evaluate the accuracy of top-N
recommendation. Let T e+
u and Ru(N) denote the ground truth
set (i.e., the set of items rated by user u in the test set) and
the top-N recommendation list for user u, respectively. In the
following, we describe each of metrics for recommendation
accuracy.
The precision P@N is deﬁned as the ratio of relevant items
to the set of recommended items and is expressed as
P@N = 1
M
X
u∈U
T e+
u ∩Ru(N)

N
.
(20)
6https://www.yelp.com/dataset.

9
P @10
R@10
nDCG@10
0.1
0.15
0.2
0.25
0.3
0.35
0.4
(a) ML-1M
P @10
R@10
nDCG@10
1
2
3
4
5
6
7
8 ·10−2
(b) Amazon-Book
P @10
R@10
nDCG@10
1
1.5
2
2.5
3
3.5
4
4.5
5 ·10−2
(c) Yelp
SiReN-LightGCN
SiReN-LRGCCF
SiReN-NGCF
Fig. 4: Performance comparison according to different GNN models for each dataset in our SiReN method.
The recall R@N is deﬁned as the ratio of relevant items to
the ground truth set and is expressed as
R@N = 1
M
X
u∈U
T e+
u ∩Ru(N)

T e+
u

.
(21)
The normalized discounted cumulative gain nDCG@N [54]
measures a ranking quality of the recommendation list by
assigning higher scores to relevant items at top-N ranking
positions in the list:
nDCG@N = 1
M
X
u∈U
nDCGu@N.
(22)
Let yk be the binary relevance of the k-th item ik in Ru(N)
for each user u: yk = 1 if ik ∈T e+
u and 0 otherwise. Then,
nDCGu@N can be computed as
nDCGu@N = DCGu@N
IDCGu@N ,
(23)
where DCGu@N is
DCGu@N =
N
X
i=1
2yi −1
log2(i + 1)
(24)
and IDCGu@N indicates the ideal case of DCGu@N (i.e.,
all relevant items are at the top rank in Ru(N)). Note that all
metrics are in a range of [0, 1], and higher values represent
better performance.
D. Experimental Setup
In this subsection, we describe the experimental settings
of neural networks in our SiReN method. We implement
SiReN via PyTorch Geometric [55], which is a geometric
deep learning extension library in PyTorch. In our experiments,
we adopt GNN models for the graph Gp with positive edges
and the 2-layer MLP architecture for the graph Gn with
negative edges. We use the Xavier initializer [56] to initialize
the model parameters Θ = {θ1, θ2, θ3}. We use dropout
regularization [57] with the probability of 0.5 for the MLP
and attention models in (4) and (5). We set the dimension of
the embedding space and all hidden latent spaces to 64; the
number of negative samples, K, to 40; and the strength of L2
regularization, λreg, to 0.1 for the ML-1M dataset and 0.05
for the Amazon-Book and Yelp datasets. We train our model
using the Adam optimizer [58] with a learning rate of 0.005.
For each dataset, we conduct 5-fold cross-validation by
splitting it into two subsets: 80% of the ratings (i.e., user–item
interactions) as the training set and 20% of the ratings as the
test set. In the training set, when we implement ﬁve benchmark
methods, we regard only the items with the rating scores of 4
and 5 as observed interactions by removing the ratings whose
scores are lower than 4 as in [7], [40], [42]; however, when
we implement our SiReN method, we utilize all user–item
interactions including low ratings in the training set while
the parameter wo, indicating the design criterion for signed
bipartite graph construction, is set to 3.5.7 It is worthwhile to
note that, for fair comparison, the test set consists of only the
ratings of 4 and 5 as the ground truth set for all the methods
including SiReN.
E. Experimental Results
In this subsection, our empirical study is designed to answer
the following four key research questions.
RQ1. How do underlying GNN models affect the performance
of the SiReN method?
RQ2. Which model architecture is appropriate to the graph Gn
with negative edges?
RQ3. How much does the SiReN method improve the top-
N recommendation over baseline and state-of-the-art
methods?
RQ4. How robust is our SiReN method with respect to inter-
action sparsity levels?
To answer these research questions, we comprehensively carry
out experiments in the following.
7Our empirical ﬁndings reveal that such a setting in SiReN consistently
leads to superior performance to that of other values of wo for all datasets
having the 1–5 rating scale.

10
P @10
R@10
nDCG@10
0.1
0.15
0.2
0.25
0.3
0.35
0.4
(a) ML-1M
P @10
R@10
nDCG@10
1
2
3
4
5
6
7
8 ·10−2
(b) Amazon-Book
P @10
R@10
nDCG@10
1
1.5
2
2.5
3
3.5
4
4.5
5 ·10−2
(c) Yelp
SiReNMLP-Gn
SiReNGNN-Gn
SiReNNo-Gn
Fig. 5: Performance comparison according to several model architectures used for the graph Gn for each dataset in our SiReN
method.
a) Comparative Study Among GNN Models Used for Gp
(RQ1): In Fig. 4, for all datasets, we evaluate the accuracy
of top-N recommendation in terms of P@N, R@N, and
nDCG@N when N is set to 10 while using various GNN
models used for the graph Gp in our SiReN method. Since
our method is GNN-model-agnostic, any existing GNN models
can be adopted; however, in our experiments, we adopt three
state-of-the-art GNN models that exhibit superior performance
in recommender systems from the literature, namely NGCF
[15] (SiReN-NGCF), LR-GCCF [18] (SiReN-LRGCCF), and
LightGCN [19] (SiReN-LightGCN). From Fig. 4, we observe
that SiReN-LightGCN consistently outperforms other models
for all performance metrics. As discussed in [19], this is
because nonlinear activation and weight transformation op-
erations in GNNs rather tend to degrade the recommendation
accuracy; LightGCN thus attempted to simplify the design of
GCN by removing such operations. It turns out that such a gain
achieved by LightGCN is also possible in our SiReN model
that contains three learning models including the GNN, MLP,
and attention models.
From these ﬁndings, we use SiReN-LightGCN in our
subsequent experiments unless otherwise stated.
b) Comparative Study Among Model Architectures Used
for Gn (RQ2): We perform another comparative study among
model architectures used for the graph Gn with negative edges.
We adopted the MLP for Gn since negative edges in Gn can
undermine the assortativity and thus message passing to such
dissimilar nodes would not be feasible. In this experiment,
we empirically validate this claim by taking into account two
other design scenarios. We evaluate the accuracy of top-N
recommendation when N is set to 10 for all datasets. First,
we recall the original SiReN method employing MLP for Gn,
dubbed SiReNMLP-Gn.
Second,
instead
of
employing
MLP,
we
introduce
SiReNGNN-Gn that uses an additional GNN model for the
graph Gn to calculate the embedding vectors Zn:
Zn = GNNθ′
1(Gn),
(25)
where θ′
1 is the learned model parameters of GNN for the
graph Gn. In this experiment, we adopt LightGCN [19] among
GNN models.
Third, as an ablation study, we introduce SiReNNo-Gn that
does not calculate the embedding vectors Zn. In other words,
we only use the embedding vectors Zp in (3) as the ﬁnal
embeddings Z in (6) (i.e., Z = Zp). Note that SiReNNo-Gn
is identical to the model architecture of LightGCN [19],
which generates embedding vectors by aggregating only the
information of positively connected neighbors. However, un-
like LightGCN, SiReNNo-Gn utilizes the sign-aware BPR
loss in (17)–(19) as our objective function in the process of
optimization.
From Fig. 5, our ﬁndings are as follows:
• SiReNMLP-Gn is always superior to SiReNGNN-Gn re-
gardless of performance metrics, which indeed validates
our claim addressed in Remark 1.
• SiReNNo-Gn is even inferior to SiReNGNN-Gn. This im-
plies that, although the model in SiReNNo-Gn is trained
via our sign-aware BPR loss, both positive and negative
relations in the signed bipartite graph Gs are not precisely
captured during training unless an appropriate model
architecture is designed for the graph Gn.
From these ﬁndings, we use SiReNMLP-Gn in our subse-
quent experiments unless otherwise stated.
c) Comparison With Benchmark Methods (RQ3): The
performance comparison between our SiReN method and
three state-of-the-art GNN methods, including NGCF [15],
LR-GCCF [18], and LightGCN [19], as well as two baseline
MF methods, including BPRMF [26] and NeuMF [32], for
top-N recommendation is comprehensively presented in Table
III with respect to three performance metrics using three real-
world datasets, where N ∈{10, 15, 20}. We note that the
hyperparameters in all the aforementioned benchmark methods
are tuned differently according to each dataset so as to provide
the best performance. In Table III, the value with an underline
indicates the best performer for each case. We would like to
make the following insightful observations:

11
N = 10
N = 15
N = 20
Dataset
Method
P @N
R@N
nDCG@N
P @N
R@N
nDCG@N
P @N
R@N
nDCG@N
ML-1M
BPRMF
0.1999±0.0044 0.1227±0.0019 0.2363±0.0063
0.1772±0.0033 0.1608±0.0016 0.2314±0.0053
0.1615±0.0027 0.1930±0.0021 0.2321±0.0050
NeuMF
0.2397±0.0030 0.1599±0.0011 0.2847±0.0035
0.2138±0.0027 0.2098±0.0023 0.2827±0.0032
0.1956±0.0025 0.2513±0.0028 0.2861±0.0032
NGCF
0.2477±0.0023 0.1748±0.0025 0.3031±0.0033
0.2174±0.0022 0.2229±0.0027 0.2985±0.0029
0.1960±0.0020 0.2627±0.0026 0.3002±0.0030
LR-GCCF 0.2539±0.0027 0.1802±0.0031 0.3117±0.0039
0.2220±0.0025 0.2292±0.0046 0.3066±0.0042
0.1998±0.0024 0.2687±0.0046 0.3079±0.0045
LightGCN 0.2679±0.0013 0.1909±0.0016 0.3297±0.0018
0.2349±0.0016 0.2432±0.0029 0.3249±0.0022
0.2109±0.0013 0.2837±0.0025 0.3258±0.0021
SiReN
0.2838±0.0011 0.1965±0.0016 0.3482±0.0017
0.2480±0.0008 0.2489±0.0013 0.3415±0.0015 0.2228±0.0009 0.2909±0.0016 0.3419±0.0017
Amazon-Book
BPRMF
0.0263±0.0033 0.0365±0.0057 0.0371±0.0053
0.0243±0.0029 0.0500±0.0074 0.0419±0.0058
0.0228±0.0027 0.0621±0.0090 0.0461±0.0064
NeuMF
0.0339±0.0018 0.0452±0.0010 0.0483±0.0018
0.0303±0.0014 0.0597±0.0012 0.0530±0.0017
0.0277±0.0012 0.0722±0.0012 0.0573±0.0017
NGCF
0.0391±0.0014 0.0532±0.0008 0.0562±0.0010
0.0403±0.0127 0.0706±0.0007 0.0618±0.0009
0.0321±0.0009 0.0858±0.0007 0.0671±0.0008
LR-GCCF 0.0399±0.0014 0.0544±0.0005 0.0574±0.0009
0.0357±0.0013 0.0721±0.0004 0.0631±0.0008
0.0327±0.0012 0.0874±0.0004 0.0684±0.0009
LightGCN 0.0443±0.0013 0.0595±0.0008 0.0638±0.0007
0.0393±0.0011 0.0781±0.0008 0.0698±0.0007
0.0358±0.0010 0.0942±0.0009 0.0753±0.0006
SiReN
0.0497±0.0015 0.0674±0.0003 0.0724±0.0009
0.0438±0.0015 0.0878±0.0003 0.0788±0.0001 0.0397±0.0013 0.1049±0.0005 0.0846±0.0012
Yelp
BPRMF
0.0116±0.0014 0.0180±0.0020 0.0165±0.0017
0.0111±0.0013 0.0257±0.0026 0.0194±0.0021
0.0107±0.0013 0.0329±0.0033 0.0221±0.0024
NeuMF
0.0174±0.0009 0.0262±0.0017 0.0254±0.0015
0.0159±0.0007 0.0358±0.0019 0.0288±0.0016
0.0149±0.0006 0.0445±0.0023 0.0319±0.0017
NGCF
0.0243±0.0009 0.0383±0.0010 0.0368±0.0010
0.0219±0.0007 0.0515±0.0008 0.0413±0.0009
0.0202±0.0007 0.0632±0.0009 0.0455±0.0009
LR-GCCF 0.0258±0.0010 0.0405±0.0009 0.0392±0.0011
0.0232±0.0008 0.0543±0.0010 0.0439±0.0012
0.0214±0.0007 0.0665±0.0012 0.0481±0.0013
LightGCN 0.0281±0.0009 0.0435±0.0007 0.0427±0.0008
0.0251±0.0008 0.0582±0.0010 0.0476±0.0008
0.0231±0.0007 0.0711±0.0011 0.0521±0.0009
SiReN
0.0293±0.0010 0.0448±0.0005 0.0441±0.0010
0.0262±0.0009 0.0600±0.0006 0.0492±0.0010 0.0241±0.0008 0.0731±0.0005 0.0537±0.0010
TABLE III: Performance comparison among SiReN and ﬁve benchmark methods in terms of three performance metrics
(average ± standard deviation) when N ∈{10, 15, 20}. Here, the best method for each case is highlighted using underlines.
N = 10
N = 15
N = 20
Group
Method
P @N
R@N
nDCG@N
P @N
R@N
nDCG@N
P @N
R@N
nDCG@N
[0, 20)
(405.4)
BPRMF
0.0485±0.0056 0.1348±0.0200 0.1046±0.0148
0.0433±0.0031 0.1810±0.0145 0.1223±0.0128
0.0384±0.0017 0.2121±0.0133 0.1333±0.0123
NeuMF
0.0675±0.0050 0.1873±0.0182 0.1449±0.0147
0.0576±0.0036 0.2397±0.0204 0.1653±0.0149
0.0505±0.0023 0.2808±0.0159 0.1794±0.0135
NGCF
0.0760±0.0040 0.2141±0.0126 0.1679±0.0129
0.0638±0.0015 0.2660±0.0100 0.1884±0.0101
0.0553±0.0014 0.3074±0.0102 0.2028±0.0095
LR-GCCF 0.0776±0.0037 0.2148±0.0145 0.1736±0.0162
0.0643±0.0025 0.2670±0.0139 0.1938±0.0147
0.0555±0.0023 0.3070±0.0175 0.2077±0.0161
LightGCN 0.0832±0.0029 0.2326±0.0116 0.1847±0.0108
0.0690±0.0028 0.2878±0.0146 0.2063±0.0108
0.0592±0.0011 0.3272±0.0130 0.2202±0.0102
SiReN
0.0826±0.0041 0.2289±0.0120 0.1836±0.0114
0.0679±0.0023 0.2810±0.0120 0.2040±0.0108
0.0580±0.0025 0.3211±0.0176 0.2178±0.0126
[20, 50)
(1773)
BPRMF
0.0731±0.0018 0.1358±0.0026 0.1194±0.0041
0.0635±0.0013 0.1773±0.0046 0.1364±0.0037
0.0571±0.0011 0.2121±0.0036 0.1502±0.0034
NeuMF
0.1010±0.0020 0.1916±0.0052 0.1679±0.0038
0.0877±0.0014 0.2485±0.0061 0.1913±0.0041
0.0784±0.0009 0.2944±0.0064 0.2096±0.0045
NGCF
0.1149±0.0021 0.2198±0.0073 0.1961±0.0060
0.0959±0.0015 0.2727±0.0086 0.2177±0.0061
0.0843±0.0005 0.3175±0.0051 0.2356±0.0047
LR-GCCF 0.1193±0.0026 0.2298±0.0050 0.2062±0.0048
0.0993±0.0022 0.2844±0.0077 0.2284±0.0055
0.0866±0.0021 0.3276±0.0076 0.2458±0.0058
LightGCN 0.1262±0.0020 0.2424±0.0044 0.2194±0.0037
0.1055±0.0015 0.3013±0.0055 0.2434±0.0036
0.0914±0.0013 0.3455±0.0062 0.2611±0.0039
SiReN
0.1285±0.0013 0.2438±0.0035 0.2238±0.0031
0.1068±0.0013 0.3006±0.0042 0.2470±0.0035
0.0925±0.0010 0.3453±0.0041 0.2650±0.0035
[50, ∞)
(3861.6)
BPRMF
0.2722±0.0068 0.1156±0.0035 0.3021±0.0083
0.2418±0.0052 0.1514±0.0037 0.2852±0.0071
0.2208±0.0044 0.1825±0.0043 0.2789±0.0067
NeuMF
0.3194±0.0055 0.1430±0.0033 0.3514±0.0066
0.2862±0.0043 0.1893±0.0037 0.3355±0.0059
0.2629±0.0038 0.2290±0.0041 0.3312±0.0056
NGCF
0.3247±0.0040 0.1505±0.0031 0.3649±0.0052
0.2875±0.0039 0.1962±0.0037 0.3459±0.0052
0.2604±0.0032 0.2335±0.0034 0.3390±0.0047
LR-GCCF 0.3321±0.0033 0.1544±0.0030 0.3730±0.0046
0.2930±0.0031 0.2006±0.0032 0.3531±0.0043
0.2652±0.0028 0.2383±0.0033 0.3459±0.0043
LightGCN 0.3502±0.0026 0.1635±0.0029 0.3898±0.0109
0.3098±0.0024 0.2125±0.0034 0.3736±0.0043
0.2798±0.0020 0.2516±0.0035 0.3656±0.0042
SiReN
0.3738±0.0014 0.1720±0.0025 0.4208±0.0025
0.3296±0.0014 0.2224±0.0024 0.3978±0.0028
0.2979±0.0015 0.2634±0.0029 0.3890±0.0031
TABLE IV: Performance comparison among SiReN and ﬁve benchmark methods in terms of three performance metrics
(average ± standard deviation) for the ML-1M dataset according to three different user groups. For each user group, the
number in the parentheses indicates the average number of users in the belonging group. Here, the best method for each case
is highlighted using underlines.
• Our SiReN method consistently outperforms ﬁve bench-
mark methods for all datasets regardless of the perfor-
mance metrics and the values of N. The superiority of
our method comes from the fact that we are capable of
more precisely representing users’ preferences without
any information loss. This implies that low ratings are
indeed informative as long as the low rating information
is well exploited through judicious model design and
optimization.
• The second best performer is LightGCN for all the cases.
The performance gap between our SiReN method (X)
and the second best performer (Y ) is the largest when
the Amazon-Book dataset is used; the maximum improve-
ment rates of 12.19%, 13.28%, and 13.48% are achieved
in terms of P@10, R@10, and nDCG@10, respectively,
where the improvement rate (%) is given by X−Y
Y
×100.
We recall that the Amazon-Book dataset has the lowest
density (i.e., the highest sparsity) out of three datasets
(refer to Table II). Thus, from the above empirical ﬁnding,
it is seen that exploiting negative user–item interactions
in sparser datasets would be more beneﬁcial and effective
in improving the recommendation accuracy.
• Two baseline MF methods reveal worse performance than
that of three state-of-the-art GNN methods. This indicates
that exploring the high-order connectivity information via
GNNs indeed signiﬁcantly improves the recommendation

12
N = 10
N = 15
N = 20
Group
Method
P @N
R@N
nDCG@N
P @N
R@N
nDCG@N
P @N
R@N
nDCG@N
[0, 20)
(8651.8)
BPRMF
0.0157±0.0021 0.0455±0.0080 0.0330±0.0054
0.0142±0.0019 0.0619±0.0103 0.0393±0.0062
0.0132±0.0017 0.0760±0.0123 0.0442±0.0069
NeuMF
0.0199±0.0016 0.0542±0.0013 0.0420±0.0020
0.0171±0.0012 0.0699±0.0017 0.0482±0.0021
0.0154±0.0001 0.0839±0.0020 0.0532±0.0021
NGCF
0.0235±0.0012 0.0649±0.0019 0.0497±0.0014
0.0204±0.0011 0.0848±0.0015 0.0576±0.0013
0.0183±0.0010 0.1019±0.0008 0.0636±0.0012
LR-GCCF 0.0241±0.0013 0.0665±0.0013 0.0508±0.0009
0.0208±0.0012 0.0863±0.0010 0.0586±0.0009
0.0188±0.0011 0.1042±0.0012 0.0649±0.0011
LightGCN 0.0258±0.0011 0.0712±0.0021 0.0551±0.0008
0.0224±0.0011 0.0923±0.0022 0.0635±0.0009
0.0200±0.0010 0.1102±0.0023 0.0697±0.0010
SiReN
0.0304±0.0021 0.0824±0.0010 0.0652±0.0015
0.0258±0.0018 0.1053±0.0013 0.0743±0.0018
0.0228±0.0015 0.1244±0.0013 0.0811±0.0018
[20, 50)
(19006.2)
BPRMF
0.0211±0.0026 0.0371±0.0058 0.0320±0.0046
0.0194±0.0023 0.0508±0.0077 0.0383±0.0055
0.0181±0.0021 0.0633±0.0092 0.0435±0.0060
NeuMF
0.0281±0.0019 0.0468±0.0008 0.0426±0.0015
0.0249±0.0016 0.0622±0.0010 0.0497±0.0015
0.0225±0.0013 0.0749±0.0011 0.0551±0.0016
NGCF
0.0322±0.0016 0.0547±0.0007 0.0494±0.0008
0.0286±0.0014 0.0727±0.0009 0.0576±0.0009
0.0261±0.0012 0.0885±0.0012 0.0642±0.0009
LR-GCCF 0.0330±0.0016 0.0560±0.0007 0.0507±0.0007
0.0292±0.0014 0.0744±0.0006 0.0591±0.0007
0.0265±0.0013 0.0900±0.0006 0.0656±0.0008
LightGCN 0.0364±0.0016 0.0615±0.0009 0.0562±0.0005
0.0319±0.0014 0.0807±0.0009 0.0650±0.0005
0.0288±0.0013 0.0971±0.0012 0.0719±0.0006
SiReN
0.0414±0.0018 0.0696±0.0008 0.0643±0.0007
0.0361±0.0018 0.0906±0.0003 0.0740±0.0010
0.0323±0.0016 0.1082±0.0003 0.0814±0.0011
[50, ∞)
(8078)
BPRMF
0.0497±0.0058 0.0255±0.0034 0.0533±0.0063
0.0466±0.0052 0.0357±0.0046 0.0527±0.0062
0.0439±0.0049 0.0449±0.0057 0.0543±0.0065
NeuMF
0.0625±0.0026 0.0318±0.0007 0.0683±0.0030
0.0570±0.0023 0.0433±0.0008 0.0658±0.0025
0.0530±0.0017 0.0536±0.0006 0.0669±0.0020
NGCF
0.0718±0.0020 0.0371±0.0003 0.0789±0.0019
0.0652±0.0014 0.0505±0.0004 0.0761±0.0012
0.0609±0.0013 0.0626±0.0007 0.0777±0.0009
LR-GCCF 0.0729±0.0022 0.0378±0.0002 0.0799±0.0026
0.0665±0.0019 0.0515±0.0004 0.0774±0.0020
0.0618±0.0018 0.0636±0.0006 0.0787±0.0018
LightGCN 0.0823±0.0021 0.0423±0.0003 0.0911±0.0025
0.0747±0.0018 0.0574±0.0004 0.0877±0.0018
0.0692±0.0016 0.0707±0.0007 0.0889±0.0015
SiReN
0.0896±0.0017 0.0463±0.0005 0.0988±0.0018
0.0808±0.0018 0.0623±0.0006 0.0947±0.0016
0.0747±0.0018 0.0764±0.0006 0.0960±0.0015
TABLE V: Performance comparison among SiReN and ﬁve benchmark methods in terms of three performance metrics (average
± standard deviation) for the Amazon-Book dataset according to three different user groups. For each user group, the number
in the parentheses indicates the average number of users in the belonging group. Here, the best method for each case is
highlighted using underlines.
N = 10
N = 15
N = 20
Group
Method
P @N
R@N
nDCG@N
P @N
R@N
nDCG@N
P @N
R@N
nDCG@N
[0, 20)
(10469.8)
BPRMF
0.0061±0.0009 0.0197±0.0024 0.0132±0.0018
0.0057±0.0008 0.0276±0.0031 0.0161±0.0022
0.0055±0.0008 0.0352±0.0036 0.0187±0.0024
NeuMF
0.0083±0.0009 0.0268±0.0024 0.0189±0.0020
0.0075±0.0007 0.0364±0.0026 0.0224±0.0021
0.0069±0.0006 0.0453±0.0031 0.0254±0.0022
NGCF
0.0128±0.0010 0.0413±0.0017 0.0295±0.0016
0.0114±0.0009 0.0552±0.0014 0.0347±0.0016
0.0104±0.0008 0.0679±0.0016 0.0388±0.0017
LR-GCCF 0.0134±0.0013 0.0434±0.0023 0.0310±0.0022
0.0120±0.0001 0.0584±0.0022 0.0366±0.0021
0.0109±0.0008 0.0710±0.0025 0.0408±0.0023
LightGCN 0.0144±0.0010 0.0462±0.0012 0.0332±0.0015
0.0127±0.0009 0.0615±0.0014 0.0389±0.0016
0.0116±0.0008 0.0749±0.0015 0.0434±0.0016
SiReN
0.0145±0.0011 0.0468±0.0012 0.0333±0.0012
0.0129±0.0010 0.0629±0.0016 0.0393±0.0014
0.0117±0.0008 0.0759±0.0017 0.0436±0.0015
[20, 50)
(22866.2)
BPRMF
0.0094±0.0014 0.0183±0.0019 0.0146±0.0016
0.0090±0.0012 0.0263±0.0025 0.0181±0.0019
0.0086±0.0012 0.0335±0.0034 0.0210±0.0024
NeuMF
0.0137±0.0008 0.0267±0.0017 0.0222±0.0013
0.0124±0.0008 0.0365±0.0020 0.0265±0.0015
0.0116±0.0007 0.0451±0.0023 0.0300±0.0015
NGCF
0.0198±0.0010 0.0392±0.0011 0.0329±0.0009
0.0177±0.0009 0.0526±0.0010 0.0388±0.0009
0.0163±0.0008 0.0645±0.0013 0.0435±0.0010
LR-GCCF 0.0208±0.0011 0.0413±0.0007 0.0348±0.0008
0.0186±0.0009 0.0552±0.0010 0.0409±0.0009
0.0171±0.0009 0.0677±0.0011 0.0458±0.0010
LightGCN 0.0224±0.0012 0.0443±0.0008 0.0377±0.0009
0.0200±0.0010 0.0592±0.0013 0.0442±0.0009
0.0183±0.0009 0.0724±0.0015 0.0494±0.0010
SiReN
0.0231±0.0013 0.0455±0.0007 0.0387±0.0012
0.0206±0.0011 0.0607±0.0006 0.0454±0.0013
0.0189±0.0009 0.0741±0.0005 0.0507±0.0013
[50, ∞)
(8436)
BPRMF
0.0241±0.0024 0.0150±0.0015 0.0256±0.0020
0.0236±0.0025 0.0221±0.0024 0.0269±0.0024
0.0229±0.0022 0.0286±0.0028 0.0292±0.0025
NeuMF
0.0384±0.0018 0.0240±0.0014 0.0420±0.0023
0.0352±0.0013 0.0331±0.0014 0.0423±0.0020
0.0331±0.0010 0.0416±0.0018 0.0447±0.0021
NGCF
0.0506±0.0016 0.0323±0.0004 0.0562±0.0017
0.0458±0.0012 0.0439±0.0003 0.0562±0.0012
0.0425±0.0010 0.0542±0.0004 0.0588±0.0011
LR-GCCF 0.0542±0.0012 0.0346±0.0003 0.0608±0.0014
0.0492±0.0013 0.0470±0.0006 0.0606±0.0013
0.0455±0.0010 0.0579±0.0005 0.0633±0.0011
LightGCN 0.0599±0.0013 0.0380±0.0004 0.0675±0.0012
0.0540±0.0013 0.0514±0.0004 0.0670±0.0008
0.0497±0.0011 0.0631±0.0003 0.0696±0.0006
SiReN
0.0639±0.0016 0.0406±0.0006 0.0718±0.0018
0.0574±0.0012 0.0547±0.0005 0.0713±0.0013
0.0529±0.0014 0.0670±0.0008 0.0740±0.0014
TABLE VI: Performance comparison among SiReN and ﬁve benchmark methods in terms of three performance metrics
(average ± standard deviation) for the Yelp dataset according to three different user groups. For each user group, the number
in the parentheses indicates the average number of users in the belonging group. Here, the best method for each case is
highlighted using underlines.
accuracy.
• The gain of LightGCN over LR-GCCF and NGCF is
consistently observed. We note that, in contrast to LR-
GCCF and NGCF, LightGCN aggregates the information
of neighbors without both nonlinear activation and weight
transformation operations. Our experimental results coin-
cide with the argument in [19] that a simple aggregator
of the information of neighbors using a weighted sum is
the most effective as long as GNN-based recommender
systems are associated.
d) Robustness to Interaction Sparsity Levels (RQ4):
Needless to say, the sparsity issue is one of crucial challenges
on designing recommender systems since few user–item in-
teractions are insufﬁcient to generate high-quality embeddings
[15], [35]. In this experiment, we demonstrate that making use
of low rating scores for better representing users’ preferences
enables us to alleviate this sparsity issue. To this end, we
partition the set of users in the test set into three groups
according to the number of interactions in the training set
as in [15], [35]. More precisely, for each dataset, we split
the users into three groups, each of which is composed of

13
the users whose number of interactions in the training set
ranges between [0, 20), [20, 50), and [50, ∞), respectively. In
Tables IV–VI, we comprehensively carry out the performance
comparison between our SiReN method and ﬁve benchmark
methods with respect to three performance metrics of top-N
recommendation using the ML-1M, Amazon-Book, and Yelp
datasets, respectively, where experimental results are shown
according to three interaction sparsity levels for each dataset.
Our ﬁndings can be summarized as follows:
• Our SiReN method almost consistently outperforms
benchmark methods except for only one case of user
groups in the ML-1M dataset. This comes from the fact
that the ML-1M dataset is relatively less sparse; thus, in
this case, exploiting the set of negative interactions in
designing GNN-based recommender systems may not be
often useful in improving the recommendation accuracy.
• The performance gap between our SiReN method and
the second best performer is the largest for the user group
having [0, 20) interactions in the Amazon-Book dataset
(refer to Table V); the maximum improvement rates of
17.83%, 15.73%, and 18.33% are achieved in terms of
P@10, R@10, and nDCG@10, respectively. As stated
above, the performance improvement of SiReN over
competing methods is signiﬁcant when sparse datasets
are used.
• As the number of interactions per user increases, the
performance is likely to be enhanced for all the methods
regardless of types of datasets. It is obvious that more
user–item interactions yield higher recommendation ac-
curacy.
VI. CONCLUDING REMARKS
In this paper, we explored a fundamentally important prob-
lem of how to take advantage of both high and low rating
scores in developing GNN-based recommender systems. To
tackle this challenge, we introduced a novel method, termed
SiReN, that is designed based on sign-aware learning and op-
timization models along with a GNN architecture. Speciﬁcally,
we presented an approach to 1) constructing a signed bipartite
graph Gs to distinguish users’ positive and negative feedback
and then partitioning Gs into two edge-disjoint graphs Gp
and Gn with positive and negative edges each, respectively, 2)
generating two embeddings for Gp and Gn via a GNN model
and an MLP, respectively, and then using an attention model
to discover the ﬁnal embeddings, and 3) training our learning
models by establishing a sign-aware BPR loss function that
captures each relation of positively and negatively connected
neighbors. Using three real-world datasets, we demonstrated
that our SiReN method remarkably outperforms three state-
of-the-art GNN methods as well as two baseline MF methods
while showing gains over the second best performer (i.e.,
LightGCN) by up to 13.48% in terms of the recommendation
accuracy. We also demonstrated that our proposed method is
robust to more challenging situations according to interaction
sparsity levels by investigating that the performance improve-
ment of SiReN over state-of-the-art methods is signiﬁcant
when sparse datasets are used. Additionally, we empirically
showed the effectiveness of MLP used for the graph Gn with
negative edges.
Potential avenues of future research include the design of a
more sophisticated GNN model that ﬁts well into Gn in signed
bipartite graphs. Here, the challenges lie in developing a new
information aggregation and propagation mechanism.
ACKNOWLEDGMENT
This research was supported by the National Research
Foundation of Korea (NRF) grant funded by the Korea gov-
ernment (MSIT) (No. 2021R1A2C3004345), by the Institute
of Information & Communications Technology Planning &
Evaluation (IITP) grant funded by the Korea Government
(MSIT) (No. 2020-0-01441, Artiﬁcal Intelligence Convergence
Research Center (Chungnam National University)), and by the
Yonsei University, Republic of Korea Research Fund of 2021
(2021-22-0083). The authors would like to thank Dr. Hajoon
Ko from Harvard University for his helpful comments.
REFERENCES
[1] C.-K. Hsieh, L. Yang, Y. Cui, T.-Y. Lin, S. Belongie, and D. Estrin,
“Collaborative metric learning,” in Proc. 26th Int. Conf. World Wide
Web (WWW’17), Perth, Aust., Apr. 2017, pp. 193–201.
[2] T. Ebesu, B. Shen, and Y. Fang, “Collaborative memory network for
recommendation systems,” in Proc. 41st Int. ACM SIGIR Conf. Res.
Develop. Inf. Retrieval (SIGIR’18), Ann Arbor, MI, Jul. 2018, pp. 515–
524.
[3] J. Han, L. Zheng, Y. Xu, B. Zhang, F. Zhuang, S. Y. Philip, and W. Zuo,
“Adaptive deep modeling of users and items using side information for
recommendation,” IEEE Trans. Neural Netw. Learn. Syst., vol. 31, no. 3,
pp. 737–748, Mar. 2020.
[4] M. Gori, A. Pucci, V. Roma, and I. Siena, “ItemRank: A random-walk
based scoring algorithm for recommender engines.” in Proc. 20th Int.
Joint Conf. Artif. Intell. (IJCAI’07), Hyderabad, India, Jan. 2007, pp.
2766–2771.
[5] J.-H. Yang, C.-M. Chen, C.-J. Wang, and M.-F. Tsai, “HOP-rec: High-
order proximity for implicit recommendation,” in Proc. 12th ACM Conf.
Recommender Syst. (RecSys’18), Vancouver, Canada, Oct. 2018, pp.
140–144.
[6] M. Gao, L. Chen, X. He, and A. Zhou, “BiNE: Bipartite network
embedding,” in Proc. 41st Int. ACM SIGIR Conf. Res. Develop. Inf.
Retrieval (SIGIR’18), Ann Arbor, MI, Jul. 2018, pp. 715–724.
[7] C.-M. Chen, C.-J. Wang, M.-F. Tsai, and Y.-H. Yang, “Collaborative
similarity embedding for recommender systems,” in Proc. 28th Int. Conf.
World Wide Web (WWW’19), San Francisco, CA, May 2019, pp. 2637–
2643.
[8] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph
convolutional networks,” in Proc. 5th Int. Conf. Learn. Representations
(ICLR’17), Toulon, France, Apr. 2017.
[9] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation
learning on large graphs,” in Proc. 28th Int. Conf. Neural Inf. Process.
Syst. (NIPS’17), Long Beach, CA, Dec. 2017, pp. 1025–1035.
[10] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are
graph neural networks?” in Proc. 7th Int. Conf. Learn. Representations.
(ICLR’19), New Orleans, LA, May 2019.
[11] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and
Y. Bengio, “Graph attention networks,” in Proc. 6th Int. Conf. Learn.
Representations. (ICLR’18), Vancouver, Canada, Apr.–May 2018.
[12] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, “A
comprehensive survey on graph neural networks,” IEEE Trans. Neural
Netw. Learn. Syst., vol. 32, no. 1, pp. 4–24, Jan. 2021.
[13] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and
J. Leskovec, “Graph convolutional neural networks for web-scale rec-
ommender systems,” in Proc. 24th ACM SIGKDD Int. Conf. Knowl.
Discovery and Data Mining (KDD’18), London, UK, Aug. 2018, pp.
974–983.
[14] L. Zheng, C.-T. Lu, F. Jiang, J. Zhang, and P. S. Yu, “Spectral
collaborative ﬁltering,” in Proc. 12th ACM Conf. Recommender Syst.
(RecSys’18), Vancouver, Canada, Oct. 2018, pp. 311–319.

14
[15] X. Wang, X. He, M. Wang, F. Feng, and T.-S. Chua, “Neural graph
collaborative ﬁltering,” in Proc. 42nd Int. ACM SIGIR Conf. Res.
Develop. Inf. Retrieval (SIGIR’19), Paris, France, Jul. 2019, pp. 165–
174.
[16] X. Wang, R. Wang, C. Shi, G. Song, and Q. Li, “Multi-component
graph convolutional collaborative ﬁltering,” in Proc. 34th AAAI Conf.
Artif. Intell. (AAAI’20), New York, NY, Feb. 2020, pp. 6267–6274.
[17] L. Wu, Y. Yang, K. Zhang, R. Hong, Y. Fu, and M. Wang, “Joint item
recommendation and attribute inference: An adaptive graph convolu-
tional network approach,” in Proc. 43rd Int. ACM SIGIR Conf. Res.
Develop. Inf. Retrieval (SIGIR’20), Virtual Event, China, Jul. 2020, pp.
679–688.
[18] L. Chen, L. Wu, R. Hong, K. Zhang, and M. Wang, “Revisiting
graph based collaborative ﬁltering: A linear residual graph convolutional
network approach,” in Proc. 34th AAAI Conf. Artif. Intell. (AAAI’20),
New York, NY, Feb. 2020, pp. 27–34.
[19] X. He, K. Deng, X. Wang, Y. Li, Y. Zhang, and M. Wang, “LightGCN:
Simplifying and powering graph convolution network for recommenda-
tion,” in Proc. 43rd Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval
(SIGIR’20), Virtual Event, China, Jul. 2020, pp. 639–648.
[20] J. Zhu, Y. Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra,
“Beyond homophily in graph neural networks: Current limitations and
effective designs,” in Proc. 34st Int. Conf. Neural Inf. Process. Syst.
(NIPS’20), Virtual Event, Dec. 2020, pp. 7793–7804.
[21] H. Pei, B. Wei, K. C.-C. Chang, Y. Lei, and B. Yang, “Geom-GCN:
Geometric graph convolutional networks,” in Proc. 8th Int. Conf. Learn.
Representations (ICLR’20), Virtual Event, Apr. 2020.
[22] J. Zhu, R. A. Rossi, A. Rao, T. Mai, N. Lipka, N. K. Ahmed, and
D. Koutra, “Graph neural networks with heterophily,” in Proc. 35th AAAI
Conf. Artif. Intell. (AAAI’21), Virtual Event, Feb. 2021, pp. 11 168–
11 176.
[23] T. Derr, Y. Ma, and J. Tang, “Signed graph convolutional networks,”
in Proc. 18th IEEE Int. Conf. Data Mining (ICDM’18), Singap., Nov.
2018, pp. 929–934.
[24] J. Huang, H. Shen, L. Hou, and X. Cheng, “Signed graph attention
networks,” in Proc. 28th Int. Conf. Artif. Neural Netw. (ICANN’19),
Munich, Germany, Sep. 2019, pp. 566–577.
[25] F. Heider, “Attitudes and cognitive organization,” J. psychol., vol. 21,
no. 1, pp. 107–112, 1946.
[26] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme, “BPR:
Bayesian personalized ranking from implicit feedback,” in Proc. 25th
Conf. Uncertainty Artif. Intell. (UAI’09), Montreal, Canada, Jun. 2009,
pp. 452––461.
[27] Y. Koren, “Factorization meets the neighborhood: A multifaceted collab-
orative ﬁltering model,” in Proc. 14th ACM SIGKDD Int. Conf. Knowl.
Discovery and Data Mining (KDD’08), Las Vegas, NV, Aug. 2008, pp.
426–434.
[28] Y. Koren, R. Bell, and C. Volinsky, “Matrix factorization techniques
for recommender systems,” IEEE Computer, vol. 42, no. 8, pp. 30–37,
2009.
[29] X. Luo, M. Zhou, S. Li, Z. You, Y. Xia, and Q. Zhu, “A nonnegative
latent factor model for large-scale sparse matrices in recommender
systems via alternating direction method,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 3, pp. 579–592, 2016.
[30] C.-J. Lin, “Projected gradient methods for nonnegative matrix factoriza-
tion,” Neural Comput., vol. 19, no. 10, pp. 2756–2779, 2007.
[31] H. Xue, X. Dai, J. Zhang, S. Huang, and J. Chen, “Deep matrix
factorization models for recommender systems.” in Proc. 26th Int. Joint
Conf. Artif. Intell. (IJCAI’17), Melbourne, Aust., Aug. 2017, pp. 3203–
3209.
[32] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T. Chua, “Neural
collaborative ﬁltering,” in Proc. 26th Int. Conf. World Wide Web
(WWW’17), Perth, Aust., Apr. 2017, pp. 173–182.
[33] X. Ning and G. Karypis, “SLIM: Sparse linear methods for top-N
recommender systems,” in Proc. 11th IEEE Int. Conf. Data Mining
(ICDM’11), Vancouver, Canada, Dec. 2011, pp. 497–506.
[34] S. Kabbur, X. Ning, and G. Karypis, “FISM: Factored item similarity
models for top-N recommender systems,” in Proc. 19th ACM SIGKDD
Int. Conf. Knowl. Discovery and Data Mining (KDD’13).
Chicago, IL:
ACM, Aug. 2013, p. 659–667.
[35] T. Nguyen and A. Takasu, “NPE: Neural personalized embedding
for collaborative ﬁltering,” in Proc. 27th Int. Joint Conf. Artif. Intell.
(IJCAI’18), Stockholm, Sweden, Jul. 2018, pp. 1583–1589.
[36] X. He, Z. He, J. Song, Z. Liu, Y.-G. Jiang, and T.-S. Chua, “NAIS:
Neural attentive item similarity model for recommendation,” IEEE
Trans. Knowl. Data Eng., vol. 30, no. 12, pp. 2354–2366, Dec. 2018.
[37] S. Deng, L. Huang, G. Xu, X. Wu, and Z. Wu, “On deep learning for
trust-aware recommendations in social networks,” IEEE Trans. Neural
Netw. Learn. Syst., vol. 28, no. 5, pp. 1164–1177, 2017.
[38] R. van den Berg, T. N. Kipf, and M. Welling, “Graph convolutional
matrix completion,” in Proc. KDD’18 Deep Learning Day, 2018.
[39] M. Zhang and Y. Chen, “Inductive matrix completion based on
graph neural networks,” in Proc. 8th Int. Conf. Learn. Representations
(ICLR’20), Virtual Event, Apr. 2020.
[40] L. Wu, P. Sun, Y. Fu, R. Hong, X. Wang, and M. Wang, “A neural
inﬂuence diffusion model for social recommendation,” in Proc. 42nd
Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval (SIGIR’19), Paris,
France, Jul. 2019, pp. 235–244.
[41] W. Fan, Y. Ma, Q. Li, Y. He, E. Zhao, J. Tang, and D. Yin, “Graph
neural networks for social recommendation,” in Proc. 28th Int. Conf.
World Wide Web (WWW’19), San Francisco, CA, May 2019, pp. 417–
426.
[42] L. Wu, J. Li, P. Sun, R. Hong, Y. Ge, and M. Wang, “DiffNet++: A neural
inﬂuence and interest diffusion network for social recommendation,”
IEEE Trans. Knowl. and Data Eng., to appear.
[43] J. Kim, H. Park, J.-E. Lee, and U. Kang, “SIDE: Representation learning
in signed directed networks,” in Proc. 27th Int. Conf. World Wide Web
(WWW’18), Lyon, France, Apr. 2018, pp. 509–518.
[44] W. Jin, T. Derr, Y. Wang, Y. Ma, Z. Liu, and J. Tang, “Node similarity
preserving graph convolutional networks,” in Proc. 14th ACM Int. Conf.
Web Search and Data Mining (WSDM’21), Virtual Event, Isr., Mar. 2021,
pp. 148–156.
[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. 31st
Int. Conf. Neural Inf. Process. Syst. (NIPS’17), Long Beach, CA, Dec.
2017, pp. 5998–6008.
[46] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
“Neural message passing for quantum chemistry,” in Proc. 34th Int.
Conf. Mach. Learn. (ICML’17), Sydney, Aust., Aug. 2017, pp. 1263–
1272.
[47] K. Xu, C. Li, Y. Tian, T. Sonobe, K. Kawarabayashi, and S. Jegelka,
“Representation learning on graphs with jumping knowledge networks,”
in Proc. 35th Int. Conf. Mach. Learn. (ICML’18), Stockholm, Sweden,
Jul. 2018, pp. 5449–5458.
[48] Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional
networks for semi-supervised learning,” in Proc. 32nd AAAI Conf. Artif.
Intell. (AAAI’18), New Orleans, LA, Feb. 2018, pp. 3538–3545.
[49] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their composi-
tionality,” in Proc. 27th Conf. Neural Inf. Process. Systems (NIPS’13),
Lake Tahoe, NV, Dec. 2013, pp. 3111–3119.
[50] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-
scale information network embedding,” in Proc. 24th Int. Conf. World
Wide Web (WWW’15), Florence, Italy, May 2015, pp. 1067–1077.
[51] F. M. Harper and J. A. Konstan, “The movielens datasets: History and
context,” ACM Trans. Interactive Intell. Syst., vol. 5, no. 4, pp. 1–19,
Dec. 2015.
[52] R. He and J. McAuley, “Ups and downs: Modeling the visual evolution
of fashion trends with one-class collaborative ﬁltering,” in Proc. 25th
Int. Conf. World Wide Web (WWW’16), Montreal, Canada, Apr. 2016,
pp. 507–517.
[53] ——, “VBPR: Visual bayesian personalized ranking from implicit
feedback,” in Proc. 30th AAAI Conf. Artif. Intell. (AAAI’16), Phoenix,
AZ, Feb. 2016, pp. 144–150.
[54] K. J¨arvelin and J. Kek¨al¨ainen, “Cumulated gain-based evaluation of IR
techniques,” ACM Trans. Inf. Syst., vol. 20, no. 4, pp. 422–446, Oct.
2002.
[55] M. Fey and J. E. Lenssen, “Fast graph representation learning with py-
torch geometric,” in Proc. ICLR Workshop on Representation Learning
on Graphs and Manifolds, New Orleans, LA, May 2019.
[56] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep
feedforward neural networks,” in Proc. 13th Int. Conf. Artif. Intell.
Statist. (AISTATS’10), Sardinia, Italy, May 2010, pp. 249–256.
[57] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: A simple way to prevent neural networks from over-
ﬁtting,” J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958, Jan. 2014.
[58] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in Proc. 3rd Int. Conf. Learn. Representations (ICLR’15), San Diego,
CA, May 2015.

