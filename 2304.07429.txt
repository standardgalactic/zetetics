Identity Encoder for Personalized Diffusion
Yu-Chuan Su
Kelvin C.K. Chan
Yandong Li
Yang Zhao
Han Zhang
Boqing Gong
Huisheng Wang
Xuhui Jia
Abstract
Many applications can beneﬁt from personalized image
generation models, including image enhancement, video
conferences, just to name a few. Existing works achieved
personalization by ﬁne-tuning one model for each person.
While being successful, this approach incurs additional
computation and storage overhead for each new identity.
Furthermore, it usually expects tens or hundreds of exam-
ples per identity to achieve the best performance. To over-
come these challenges, we propose an encoder-based ap-
proach for personalization. We learn an identity encoder
which can extract an identity representation from a set of
reference images of a subject, together with a diffusion gen-
erator that can generate new images of the subject condi-
tioned on the identity representation. Once being trained,
the model can be used to generate images of arbitrary iden-
tities given a few examples even if the model hasn’t been
trained on the identity. Our approach greatly reduces the
overhead for personalized image generation and is more
applicable in many potential applications. Empirical re-
sults show that our approach consistently outperforms ex-
isting ﬁne-tuning based approach in both image generation
and reconstruction, and the outputs is preferred by users
more than 95% of the time compared with the best perform-
ing baseline.
1. Introduction
Recent years have seen a tremendous success in the
task of face generation.
The emerge of VAEs [34, 7],
GANs [4, 13, 15, 16, 14], and diffusion models [12, 29] has
enabled unlimited possibility in synthesizing faces with var-
ious content given different forms of conditions. However,
models trained on generic facial images are often incapable
of generating images of a speciﬁc identity, which is a highly
desirable feature in practical uses. In order to achieve this,
the model must understand the appearance of the desired
identity, which are likely not included in the training set.
Therefore, a prevalent approach [26, 9, 30, 24] is to ﬁne-
tune a pre-trained synthesis model using multiple images
capturing the corresponding face. The ﬁne-tuned models
Identity 
Encoder
Diffusion Generator
All Images of 
the Subject 
Samples
Similar 
Distribution
Figure 1. Given reference images of a subject, our goal is to learn
an identity encoder and a diffusion generator such that the iden-
tity encoder can extract an identity representation from the refer-
ences, and the diffusion generator can generate diverse images of
the subject conditioning on the representation. The same model
can be used to generate images of different subjects given differ-
ent references, and the identity encoder can extracts a high-quality
representation from just a few or even a single image.
have been shown capable of synthesizing personalized im-
ages with diverse content.
Despite the simplicity and remarkable performance, the
applicability of the aforementioned ﬁne-tuning paradigm is
inevitably limited by the need of lengthy optimization pro-
cess. Speciﬁcally, these methods require model ﬁne-tuning
for each identity, which takes minutes to hours to complete.
As a result, they are infeasible in cases that require fast re-
sponse, such as interactive editing. More importantly, they
often require up to hundreds of images for ﬁne-tuning due
to the identity-agnostic nature of the pre-trained models.
Furthermore, the storage cost increases with the number of
identities, essentially prohibiting the methods to generalize
to arbitrary identities. Therefore, a framework that does not
require identity-speciﬁc ﬁne-tuning is necessary.
arXiv:2304.07429v1  [cs.CV]  14 Apr 2023

We take the ﬁrst step to explore the possibility of de-
veloping a feed-forward framework for personalized face
generation.
Contrary to existing methods that obtain an
identity-speciﬁc embedding through prompt and model tun-
ing, we employ a learnable encoder to generate identity
embeddings from input images provided by the user. See
Fig. 1. Our method, termed identity encoder, obtains an em-
bedding without any optimization, achieving up to 29 times
speedup over existing methods.
While our method is conceptually simple, the acquisi-
tion of an effective encoder remains formidable. Specif-
ically, directly training the encoder and diffusion model
jointly leads to degenerated outputs, resulting in images
with no variations. In this work, we explore the keys in
training to achieve balance between identity preservation
and output diversity. First, during training, we impose an
identity-preservation constraint on the learned embeddings,
in which the convex hull formed by the embeddings from
the same person must represent the same identity. This is es-
sential in forcing the model to learn an identity-speciﬁc em-
bedding that preserve variations among outputs. Second, to
further improve identity preservation, we introduce a soft-
nearest neighbor identity loss, which localizes the embed-
dings obtained from the same identity. Third, to alleviate
the lack of identity-aware datasets, we propose an identity-
agnostic joint training scheme. Speciﬁcally, in addition to
the identity-preservation constraint, we embrace the con-
ventional large-scale face datasets FFHQ [15] to guide the
encoder to learn a meaningful embedding through recon-
struction. In this case, the number of identities perceived by
the model increases dramatically, signiﬁcantly boosting the
output quality and the generalizability. With the aforemen-
tioned components, our identity encoder is able to produce
personalized embeddings with as few as one image, without
any test-time optimization.
In addition to generation, our solution enables the possi-
bility of multiple downstream tasks that require personaliza-
tion. In particular, we demonstrate through experiments our
method can be extended to personalized super-resolution
and inpainting, achieving enhanced identity preservation
without harming output quality.
Empirical results show
that our method consistently outperforms prior ﬁne-tuning
based approach in terms of image quality, identity preser-
vation, and personalized reconstruction, and the outputs of
our model is preferred by raters more than 95% of the time
compared with the best performing baseline.
As image synthesis becomes ubiquitous, the increasing
demand of personalized synthesis is inevitable. There is an
urge to develop a practical framework that efﬁciently gener-
ates images with customized identities. We show that with
proper designs, it is possible to obtain generalizable em-
beddings without the needs of time-consuming optimiza-
tion and storage of multiple models, signiﬁcantly reducing
the burdens in the user end.
2. Related Work
Face Generation and Editing. The emerge of high-quality
datasets [13, 15, 17] and sophisticated generative models,
such as VAEs [34, 7], GANs [4, 13, 15, 16, 14], and dif-
fusion models [12, 29], has enabled unlimited possibility
in the ﬁelds of face generation and editing.
In particu-
lar, training on the high-quality FFHQ [15] dataset, Style-
GANs [13, 15, 16, 14] obtain unprecedented performance
with a novel style-based architecture, where a latent code
is used to control the characteristics of the generated faces.
Diffusion models [12, 29] synthesize faces through itera-
tively denoise the intermediate outputs, starting from a stan-
dard Gaussian noise.
Instead of training with two com-
peting objectives, diffusion models are trained with a sin-
gle denoising objective, thus generally possessing better
training stability. Editing is achieved usually by manipu-
lating the latent space. For example, StyleGAN-based ap-
proaches [1, 31, 19, 32, 20, 33, 28, 2, 10] edit the attributes
of an image, such as smile and age, by obtaining an editing
direction in the latent space. Analogous to the StyleGAN
paradigm, Diffusion Autoencoder [27] learns an additional
encoder to generate an embedding, which is conditioned by
the diffusion model for generation. Similarly, editing can be
achieved by manipulating the latent embeddings produced
by the encoder. In this work, instead of controlling the at-
tributes of faces, we are interested in controlling the identity
of the generated faces. Speciﬁcally, we follow the pipeline
of Diffusion Autoencoder and explore the construction of an
effective identity-speciﬁc latent code for personalized face
generation.
Personalized Face Generation. Personalized face genera-
tion imposes an additional constraint that the model should
generate faces of the same identity. This is usually achieved
by ﬁne-tuning a pre-trained model with multiple images
provided by the user.
Speciﬁcally, MyStyle [26] adopts
a pre-trained StyleGAN, and then optimizes the object la-
tent code, followed by ﬁne-tuning the model with the la-
tent codes. By using up to hundreds of images for test-time
optimization, MyStyle produces promising outputs. In this
work, we take one step further and consider methods that
do not require test-time optimization and need as few as
one image for personalized face generation. This is of great
value as one can bypass the computation and storage over-
head incurred from model ﬁne-tuning, enhancing the prac-
ticality of personalalied face generation.
3. Approach
In this section, we present the identity encoder for per-
sonalized image generation using diffusion model.
We
ﬁrst introduce diffusion models and diffusion autoencoder.
Next, we provide the problem deﬁnition and overview for

our approach, followed by the details for learning the iden-
tity encoder. Finally, we describe how to adapt the person-
alized model to conditional generation applications.
3.1. Preliminary
Our method is built on top of diffusion model and diffu-
sion autoencoder. This section provides the background of
our approach.
Diffusion Models
Diffusion process is deﬁned as a
Markov chain of length T that gradually inserts Gaussian
noise to the data with a variance schedule {βt}T
t=1. Let xt
be the latent at timestep t of the process, their distributions
can be written as
q(xt|xt−1) = N(xt;
p
1 −βtxt−1, βtI).
In cases when βt is small, the distribution has the following
closed-form solution [12]:
q(xt|x0) = N(xt; √¯αtx0, (1 −¯αt)I),
where αt = 1−βt and ¯αt = Qt
s=1 αs. Here x0 is the target
data with distribution q(x0). To model the target distribu-
tion, Ho et al. [12] propose to learn a mapping ϵθ(·) that es-
timates the noise ϵt added to x0 to produce xt. Speciﬁcally,
the mapping is optimized with the following loss function:
L = ||ϵθ(xt, t) −ϵt||,
where ϵθ is instantiate by a dense prediction model such as
a U-Net. During inference, samples in the target distribu-
tion are obtained through reverse diffusion process that iter-
atively removes the noise using the trained noise estimator,
starting from a pure Gaussian noise xT ∼N(0, I).
Diffusion Autoencoder
Mimicking the latent space ma-
nipulation in StyleGAN [13, 16, 14], Diffusion Autoen-
coder [27] employs an auxiliary encoder to discover a
meaningful latent space for image editing in the diffusion
model paradigm. Speciﬁcally, a learnable encoder is used
to produce an embedding as a condition, and the diffusion
model acts as a decoder to produce output images given the
conditions. The entire autoencoder is jointly trained with
the following loss:
L = ||ϵθ(xt, t, zsem) −ϵt||,
where zsem represents the semantic embedding learned by
the encoder. For unconditional sampling, an additional dif-
fusion model ϵω(·) is trained to sample zsem.
Embedding Space
Image Space
Embedding Space
Image Space
Image Embedding
Identity Embedding
Figure 2.
Instead of learning an one-to-one mapping between
the embedding and image space (top panel), the identity encoder
learns a set-to-set mapping. Different combinations of reference
images all correspond to the identity representation of the same
subject, and each identity embedding maps to all possible images
of the subject (bottom panel).
3.2. Overview
We deﬁne our problem as follows. Let Pi be the distribu-
tion of all images of subject si and Yi = {yj
i ∈Pi}N
j=1 be
a set of reference images of the subject. Our goal is to learn
an encoder E together with an image generator G such that
the generator is able to synthesize images of the same sub-
ject conditioned on the identity representation extracted by
the encoder from Yi:
G(ϵ, E(Yi)) ∼Pi,
(1)
where ϵ is a random noise. Note that G and E do not depend
on the subject, meaning that the same encoder and genera-
tor are used to generate images of an arbitrary subject given
example images. We further impose two requirements on
the model: 1) it should generalize to identities unseen dur-
ing training, and 2) it should work with a small number of
references for each identity (i.e., N < 10).
In this work, we adopt a diffusion model as the gener-
ator due to its promising results achieved in recent works.
In particular, we build the generator and encoder based on
Diffusion Autoencoder [27], which also learns a diffusion
decoder jointly with an image encoder.
Unlike Diffusion Autoencoder, which conditions only on
a single image, our goal is to learn a model conditioned
on a set of reference images to improve robustness and
capture intra-identity variations. Moreover, the proposed
method should allow arbitrary numbers of reference images

as condition. To this end, we extend Diffusion Autoencoder
by aggregating the encoder outputs from multiple images.
Speciﬁcally, let Enc be an encoder that extracts a feature
representation from an image, i.e. z = Enc(y), we instan-
tiate the encoder E by aggregating the output of Enc:
E(Yi) = Aggregate

{Enc(yj
i )}N
j=1

.
(2)
In this work, we implement the Aggregate operation
with average pooling as it allows the model to be gener-
alized to arbitrary numbers of reference images.
3.3. Identity Encoder
Given the proposed approach, we apply the follow-
ing training strategies to strike a balance between identity
preservation, output diversity, and image quality.
Random average embedding
In our approach, the map-
ping between the identity embedding and the output image
is essentially a many-to-many mapping, since 1) an identity
embedding should correspond to multiple images of a sub-
ject with different appearances, and 2) an image could con-
tribute to different embeddings due to the average pooling
of different references. See Fig. 2. Therefore, directly ap-
plying Diffusion Autoencoder does not yield good results,
as its training process intrinsically leads to a one-to-one
mapping. Furthermore, without being trained on average
embeddings, the model is unable to produce high-quality
outputs when a subject’s average embedding changes from
one set of reference images to another.
To encourage such a many-to-many mapping, we adopt
a random-weighted average embedding as the condition for
the generator during training. Speciﬁcally, given N images
of a person {yj
i }N
j=1, we use
˜zi =
N
X
j=1
wjEnc(yj
i )
(3)
as the condition embedding for the diffusion model dur-
ing training.
Here wj ∈[0.0, 1.0] are randomly sampled
weights with the constraint P wj = 1. The model is trained
to reconstruct each yj
i using the same embedding ˜zi. In
other words, we require that each point within the convex
hull of {Enc(yj
i )}N
j=1 is a plausible identity embedding for
subject i, and the diffusion generator should generate all im-
ages of the person conditioned on an identity embedding in
the convex hull. Hence, we relax the one-to-one constraint
in Diffusion Autoencoder.
Identity Loss
While training with mean embeddings pro-
motes many-to-many mapping, it could lead to embedding
degeneration blurring the boundaries between identities. To
Figure 3. To encourage the identity representation to ignore intra-
identity variations and focus on inter-identity variations, we im-
pose an identity loss on the identity representation during training.
It encourages / discourage the inter-identity / intra-identity repre-
sentation distances and helps learn a representation that separates
different identities.
avoid this caveat, we encourage the encoder to separate dif-
ferent identities in the embedding space as shown in Fig. 3.
This is achieved by adding a soft nearest-neighbor loss [8]
to the encoder output. Speciﬁcally, let {zk}K
k=1 be the en-
coder output within a batch, and sk be their corresponding
identity labels, we minimize the following:
Lid = −1
K
K
X
k=1
log
P
j̸=k,sk=sj e−
∥zk−zj ∥2
T
P
j̸=k e−
∥zk−zj ∥2
T
,
(4)
where T is the temperature controlling the radius of the loss.
Multi-task Learning
Since both the formation of average
embedding and the identity require identity information, the
model can only be trained on datasets with identity labels.
This greatly limits the amount of available training data,
which is amongst the most critical components towards the
success of generative models.
To overcome this problem, we apply multi-task learning
during training. Speciﬁcally, we train the model on images
both with and without identity labels. For images with iden-
tity labels, the model is trained with average embeddings
and the identity loss described above. For images without
identity labels, we treat the model as a Diffusion Autoen-
coder and reconstruct the image conditioned on the embed-
ding of the target image. Our multi-task learning scheme
effectively increases the training data size, boosting output
quality. Therefore, the ﬁnal training loss is
L = Ldiff(Iid) + α1Lid(Iid) + α2Ldiff(Ig),
(5)
where Ldiff is the standard diffusion loss and Iid and Ig
are images with and without identity information respec-
tively. In practice, half of the images in a training batch are
identity-agnostic, and we use α1 = α2 = 0.01.

We hypothesize that reconstruction is an easier task than
identity representation extraction, and curriculum learn-
ing [3] could be applied to improve the effectiveness of the
encoder. To instantiate this, we randomly drop the identity
information with probability p, with p decreasing linearly
from 1.0 to 0.05. In this case, the encoder learns meaningful
representations through reconstruction, and gradually distill
identity information through our delicate training designs.
3.4. Personalized Conditional Generation
Besides unconditional generation for a subject (i.e., con-
ditioned only on the identity), many potential applications
of generative models require conditional generation. For
example, when using generative models for image enhance-
ment like super-resolution, the low-resolution image should
be taken as an additional condition. In this work, we in-
troduce an extension of our framework for accommodating
extra conditions.
We start with a trained personalized generation model. A
new condition encoder is used to extract a feature map from
the conditional image (or other formats of conditions). The
feature map is then injected to the generation model through
an attention layer inserted into the diffusion generator. This
layer computes the cross-attention between the intermedi-
ate feature map of the diffusion generator and the feature
map extracted from the conditional image. See Fig. 4 for an
illustration. This is similar to the conditioning mechanism
of Stable Diffusion [29]. We then train the condition en-
coder and cross-attention layer while freezing the original
personalized generation model.
4. Experiments
Datasets
To train and evaluate our personalized genera-
tion model, we compile a new dataset from existing datasets
with identity information:
• CelebA [21] consists 202,599 images from 10,177
celebrities. Among them, 9,564 have more than one im-
age, and the average number of images per identity is 21.
• CelebRef-HQ [18] consists of 10,555 images from 1,005
celebrities. Each identity has 3–21 images.
• MyStyle [26] consists of 2,154 images from 14 celebri-
ties. Each identity has more than 100 images. The dataset
also provides a ﬁxed training and test split, where the av-
erage number of test images per identity is 9.
We merge the three datasets with the following modiﬁca-
tions. First, we remove identities with less than four im-
ages. Second, we manually removes identities in CelebA
and CelebRef-HQ that overlap with MyStyle.
We use
CelebA, CelebRef-HQ, and the training split of MyStyle
as the training set and the test split of MyStyle as the test
set. This leads to the ﬁnal dataset consisting of 213,487
images from 10,568 identities in the training set and 126
Identity 
Encoder
Diffusion Generator
Diffusion Generator
Condition 
Encoder
Cross-attention
Identity 
Encoder
Figure 4. The personalized generation model, once learned, can
be used to personalize other conditional generation tasks (e.g. su-
per resolution). This is achieved by injecting the conditional in-
formation extracted by a condition encoder into the diffusion gen-
erator using cross attention layers. During training for conditional
generation, we only update the cross attention layers and the con-
dition encoder.
images from 14 identities in the test set1. Note that our goal
is to build a personalized generation model that can gener-
alize to identities not available during training. Therefore,
we train our model with and without MyStyle training set
to compare the generalizability to new identities.
Besides the datasets with identity information, we also
use the FFHQ [15] dataset for pre-training. It consists of
70,000 face images without identity information.
Baselines
We compare our methods with the following
baselines covering both 1) ﬁnetuning-based personalized
generation model, and 2) diffusion-based autoencoder:
• Diffusion Autoencoder (DiffAE) [27]—we train a diffu-
sion autoencoder on the personalized dataset. Instead of
using the original model during inference, which use the
semantic embedding from a single image as the condition
for the diffusion decoder, we use the average semantic
embedding of the same identity as the condition.
• MyStyle [26]—following the original paper, we ﬁnetune
a StyleGAN generator [15] for each identity using the
reference images.
For DiffAE, we use the same architecture for the condi-
tional diffusion generator for a fair comparison. Note that
unlike MyStyle which assumes that there are hundreds of
images for each identity, we target on scenarios where each
identity only has a few reference images, i.e. less than ten,
during test time.
Evaluation Metrics
We evaluate our method on both
personalized generation, i.e. conditioned only on identity,
1We do not deduplicate identities between CelebA and CelebRef-HQ.

Table 1. Personalized generation performance. The arrows indicate lowers (↓) or higher (↑) better for each metric. Best numbers in bold
and second-best in underlined.
Generation
Inpainting
Super-resolution
ID ↓
FID ↓
Diversity ↑
ID ↓
FID ↓
LPIPS ↓
ID ↓
FID ↓
LPIPS ↓
DiffAE
0.135
118.0
0.028
0.153
105.1
0.256
0.170
102.7
0.312
MyStyle
0.117
214.9
0.027
0.155
195.8
0.255
0.159
216.1
0.265
Ours
0.119
92.9
0.115
0.110
94.0
0.125
0.118
98.6
0.146
Ours (w/ MyStyle)
0.115
91.1
0.125
0.111
94.9
0.129
0.117
97.0
0.143
and conditional generation applications. We compare each
method using the following metrics for image generation
and reconstruction:
• Identity score (ID) computes the distance between the
generated images and real images of the given identity. It
measures how well the model preserves the identity infor-
mation and is computed for both personalized generation
and conditional generation applications.
• Fr´echet inception distance (FID) [11] measures the dis-
tribution distance between generated and real images. It
measures the quality of generated image images and is
computed for both personalized generation and condi-
tional generation applications.
• Learned
Perceptual
Image
Patch
Similarity
(LPIPS) [35] measures the embedding distance be-
tween generated and target images using a recognition
model.
It measures the perceptual similarity between
generated and target image and is computed for
conditional generation applications.
• Generation Diversity is a simple metric we propose to
measure the diversity of the outputs.
Given multiple
outputs, it ﬁrst computes the mean of each image, and
then computes the standard deviation of the numbers. A
higher value indicates larger diversity.
Implementation Details
Our model architecture follows
that of Diffusion Autoencoder [27], except that we re-
place the attention layers with the implementation of Vision
Transformer [6]. For all diffusion-based methods, we ini-
tialize the diffusion generator using a diffusion model pre-
trained on FFHQ. The identity encoder is randomly initial-
ized. We train the model using ADAM for 1,000,000 steps
with batch size 128 and learning rate 5.0×10−5. We train
the models on 128×128 image resolution.2 During infer-
ence, we run the model for 1, 000 steps using DDPM [12].
Please refer to the supplementary material for details.
4.1. Personalized Generation
We ﬁrst evaluate our method on personalized image gen-
eration. The results are in Table 1. Our method consistently
outperforms the baselines in terms of image quality and is
2We were unable to train larger models due to resource constraints.
DiffAE 
Ours
Input
MyStyle
DiffAE 
Ours
Input
MyStyle*
Figure 5. Qualitative results on personalized generation. For fair
comparison, all methods use eight reference images of the subject.
The only exception is MyStyle*, which is trained on the entire
training set to demonstrate the necessity of large number of refer-
ences.
on par with the best performing baseline in terms of iden-
tity preservation. The results verify the effectiveness of the
proposed identity encoder. Note that our method performs
similarly when trained with and without MyStyle training
set, which demonstrate the generalizability of the identity
encoder to unseen identities.
Among the baseline methods, MyStyle achieves the best
identity preservation but the worst image quality and out-
put diversity.
This shows the limitation of ﬁnetuneing-
based method, where the performance may drop signiﬁ-
cantly when there is only limited number of references (less

Table 2. Subjective evaluation results on conditional image gener-
ation. We report the percentage of raters that prefer our method.
Inpainting (%)
Super-resolution (%)
Ours vs. DiffAE
99.2
97.5
Ours vs. MyStyle
95.8
96.7
than 15 images in the experiment) for the subject. While
DiffAE improves the generated image quality, it also fails
to generate diverse outputs, and the identity information is
not well preserved. Our identity encoder strikes a better
balance between identity preservation and generated image
quality.
Fig. 5 shows the qualitative examples. The results are
consistent with the quantitative metrics, where the outputs
of DiffAE and MyStyle lack diversity. As a result, the out-
puts of the models may not be sufﬁcient to cover all the
variations of the subject, which limits their usefulness in
potential applications.
Note that the output diversity of
MyStyle signiﬁcantly improves when it is trained on the
entire MyStyle training set, which shows that it requires a
large number of references in order to achieve its best per-
formance. In contrast, our method can generate diverse out-
puts of the subject from just a few examples. Interestingly,
the outputs of DiffAE look like an average image of the
subject. This is caused by the fact that DiffAE only learns a
point-to-point mapping between the embedding and image
space, and the average embedding may not correspond to an
real image. The results verify the importance of learning an
identity embedding rather than image embedding.
4.2. Conditional Generation Applications
Next, we compare the models on conditional image gen-
eration. Speciﬁcally, we evaluate the models on 8× super-
resolution and image inpainting. For inpainting, we ran-
domly apply a square mask to the image, where the size of
the mask is set to 60% the image size. The results are in
Table 1. The results are consistent with that of generation,
where our methods achieves the best output image quality
and reconstruction accuracy. The results further verify the
effectiveness of our identity encoder.
Among the baselines, DiffAE performs particularly
worse in terms of reconstruction accuracy, because the ref-
erence image dominates the generator due to the design
of DiffAE. As a result, DiffAE tends to ignore the addi-
tional condition when trained for conditional generation,
which leads to a poor reconstruction. MyStyle also per-
forms poorly in terms of reconstruction accuracy, because
the outputs of the model are conﬁned by the reference im-
ages. When the number of references is small, they may
not be sufﬁcient to cover the variations of the subject and
limit the output space. As a result, the personalized model
is not expressive enough to reconstruct arbitrary images of
the subject.
DiffAE 
MyStyle
Ours
Inpainting
Super-Resolution (8x)
Input
GT
Figure 6.
Qualitative results for conditional image generation.
The last row show failure cases.
Fig. 8 shows qualitative results.
We can clearly see
that DiffAE and MyStyle struggle with authentically re-
constructing the inputs, despite that the outputs may look
realistic and preserve the identity.
The last row of each
panel show the failure cases. For inpainting, our method
fail to recover ﬁne-grained attribute, i.e. the age of the sub-
ject. For super-resolution, it fails to generate a realistic face
with extreme pose. This may be attributed to the training
data, where most of the images contain frontal face, and the
model does not generalize well to proﬁle faces.
We further conduct an user study on the conditional gen-
eration results. We randomly samples 20 images from the
test set for each task and ask the raters to compare the out-
puts of different methods. For each image, we show the
output of the baseline and our method, together with the in-
put and ground truth, and ask raters which method better
reconstructs the ground truth. The order of the generated
images is randomly permuted to avoid bias. Each image is
rated by 6 raters, and the average results are in Table 2. Our
method is preferred by the raters more than 95% of the time,
which is consistent with the objective evaluation results.
4.3. Ablation Study
Finally, we conduct ablation study on our method. We
ﬁrst evaluate how each component in our method con-
tributes to the ﬁnal results. The results are in Table 3. The

Table 3. Ablation study on personalized generation.
ID
FID
DiffAE
0.135
118.0
+ average embedding
0.146
102.2
+ multi-task learning
0.126
93.4
+ identity loss
0.119
92.9
results verify the necessity of all three major components in
Sec. 3.3. While the average embedding leads to worse iden-
tity preservation, qualitative results suggest that it is impor-
tant to generate realistic images from the average embed-
ding of multiple references.
Next, we study how the number of references affects
the output quality. The results are in Fig. 7, were we vary
the number of reference images for both our method and
MyStyle. We can see that the output quality of MyStyle de-
grades when the number of references decreases. In con-
trast, the results of our method remain similar.
In fact,
our method works with even a single reference. The re-
sults show that our method is robust to the number of ref-
erences and is applicable to more scenarios compared with
ﬁnetuning-based methods.
5. Limitations and Social Impacts
In this section, we discuss the limitations and potential
social impacts of our approach. From the model perspec-
tive, one limitation of our approach is that the average em-
bedding strategy may be sub-optimal. Speciﬁcally, it may
be difﬁcult to capture all potential variations of a subject,
and the average embedding may also “average” some ﬁne-
grained attributes. As the result, the beneﬁt of additional
reference may be limited. Future work will explore a more
effective method to utilize a large number of references to
improve the performance.
Another limitation we observed is that the output quality
is subject dependent. A possible reason is the distribution
of the training data, which makes the model perform better
for highly represented groups. Similar problems has been
observed in prior research [23]. Future work should ex-
plore using stronger prior knowledge to build a more robust
model. Furthermore, we would like to evaluate the models
on larger datasets to study and eliminate potential bias with
respect to different demographic groups.
Although personalized image generation is beneﬁcial for
many applications, it also increases the risk of abuse. One
obvious example is identity forgery, where the model can be
used to generate fake images of a person. While prior efforts
try to address this problem by detecting images generated
by generative model, future work should also consider tak-
ing a more active role from the generator side, e.g. provide
a mechanism to recognize images generated by the model.
Ours
Input
MyStyle
Figure 7. Qualitative results showing the effect of number of ref-
erence images.
6. Conclusion
We propose personalized image generation using iden-
tity encoder. The identity encoder extracts an identity rep-
resentation from a set of references of the subject, and a dif-
fusion generator generates new images of the subject condi-
tioning on the identity representation. The same model can
generate new images of arbitrary subjects given their refer-
ence images. Compared with ﬁnetuning-based approaches,
our method does not incur additional computation and stor-
age overhead for new subjects, and empirical results show
that it achieves better generation quality when there are only
a small number of references for each subject.
References
[1] Rameen Abdal, Yipeng Qin, and Peter Wonka.
Im-
age2StyleGAN: How to embed images into the stylegan la-
tent space? In ICCV, 2019. 2
[2] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. ReStyle:
A residual-based stylegan encoder via iterative reﬁnement.
In ICCV, 2021. 2
[3] Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Ja-
son Weston. Curriculum learning. In ICML, 2009. 5
[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high ﬁdelity natural image synthe-
sis. In International Conference on Learning Representa-
tions, 2018. 1, 2
[5] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in Neural Informa-
tion Processing Systems, 34:8780–8794, 2021. 10
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR, 2020. 6
[7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In CVPR,
pages 12873–12883, 2021. 1, 2

[8] Nicholas Frosst, Nicolas Papernot, and Geoffrey Hinton. An-
alyzing and improving representations with the soft nearest
neighbor loss. In International conference on machine learn-
ing, 2019. 4
[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or.
An image is worth one word: Personalizing text-to-
image generation using textual inversion.
arXiv preprint
arXiv:2208.01618, 2022. 1
[10] Erik H¨ark¨onen, Aaron Hertzmann, Jaakko Lehtinen, and
Sylvain Paris.
GANSpace: Discovering interpretable gan
controls. In NeurIPS, 2020. 2
[11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In Advances in Neural Information Processing Sys-
tems, 2017. 6
[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS, 2020. 1, 2, 3, 6
[13] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of GANs for improved quality, stabil-
ity, and variation. In International Conference on Learning
Representations, 2018. 1, 2, 3
[14] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. In NeurIPS, 2021. 1, 2, 3
[15] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR, 2019. 1, 2, 5
[16] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of StyleGAN. In CVPR, pages 8110–8119,
2020. 1, 2, 3
[17] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo.
MaskGAN: Towards diverse and interactive facial image ma-
nipulation. In CVPR, 2020. 2
[18] Xiaoming Li, Shiguang Zhang, Shangchen Zhou, Lei Zhang,
and Wangmeng Zuo. Learning dual memory dictionaries for
blind face restoration. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 2022. 5
[19] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim,
Antonio Torralba, and Sanja Fidler.
EditGAN: High-
precision semantic image editing. In NeurIPS, 2021. 2
[20] Hongyu Liu, Yibing Song, and Qifeng Chen. Delving Style-
GAN inversion for image editing: A foundation latent space
viewpoint. arXiv preprint arXiv:2211.11448, 2022. 2
[21] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV), 2015.
5
[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101, 2017. 10
[23] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi,
and Cynthia Rudin. Pulse: Self-supervised photo upsam-
pling via latent space exploration of generative models. In
Proceedings of the ieee/cvf conference on computer vision
and pattern recognition, pages 2437–2445, 2020. 8
[24] Ron Mokady, Amir Hertz, Kﬁr Aberman, Yael Pritch,
and Daniel Cohen-Or. Null-text inversion for editing real
images using guided diffusion models.
arXiv preprint
arXiv:2211.09794, 2022. 1
[25] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models.
In International
Conference on Machine Learning, pages 8162–8171. PMLR,
2021. 10
[26] Yotam Nitzan, Kﬁr Aberman, Qiurui He, Orly Liba, Michal
Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and
Daniel Cohen-Or. Mystyle: A personalized generative prior.
arXiv preprint arXiv:2203.17272, 2022. 1, 2, 5
[27] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizad-
wongsa, and Supasorn Suwajanakorn.
Diffusion autoen-
coders: Toward a meaningful and decodable representation.
In CVPR, 2022. 2, 3, 5, 6, 10
[28] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,
Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding
in style: a stylegan encoder for image-to-image translation.
In CVPR. 2
[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR, 2022. 1, 2, 5
[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. DreamBooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR, 2022. 1
[31] Yujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou.
InterFaceGAN: Interpreting the disentangled face represen-
tation learned by gans. IEEE transactions on pattern analysis
and machine intelligence, 44(4):2004–2018, 2020. 2
[32] Jaskirat Singh, Liang Zheng, Cameron Smith, and Jose
Echevarria. Paint2Pix: interactive painting based progres-
sive image synthesis and editing. In ECCV, 2022. 2
[33] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and
Daniel Cohen-Or.
Designing an encoder for stylegan im-
age manipulation. ACM Transactions on Graphics (TOG),
40(4):1–14, 2021. 2
[34] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. In NeurIPS, volume 30, 2017. 1, 2
[35] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR, 2018. 6
Appendices
The appendix consist of:
A Implementation details
B Additional qualitative examples
A. Implementation Details
In this section, we describe the implementation details
that could not ﬁt in the main paper.

DiffAE 
MyStyle
Ours
Inpainting
Super-Resolution (8x)
Input
GT
Figure 8. Qualitative results for conditional image generation.
Model architecture
In this paper, we modify the archi-
tecture design from DiffAE [27], similar to the one used
in OpenAI guided-diffusion [5]. The diffusion generator is
a U-Net of a stacked convolutional encoder and a stacked
convolutional decoder. Dense skip connections are in be-
tween. For both the encoder and the decoder, the number
of base channels is 128 and we set the channel multiplier
as [1, 1, 2, 2, 4, 4] at resolution [1282, 642, 322, 162, 82, 42].
In each resolution block, there are two layers each of which
contains a timestep-dependent residual module and a self-
attention module. Attention is applied at resolution 8 ×
8and4×4 with eight heads. For the identity and conditional
encoder, we set the channel multiplier as [1, 1, 2, 2, 4, 4, 4]
at resolution [1282, 642, 322, 162, 82, 42, 22] and apply at-
tention at resolution 8 × 8, 4 × 4, and 2 × 2. The condition
feature map is from the ﬁnal layer of the encoder.
Model learning and inference
For training, we set
the number of diffusion steps T to 1000.
Optimizer
AdamW [22] is applied with β1 = 0.9, β2 = 0.999. We
keep the learning rate a constant of lr = 5e−5 with 10e4
linear warm-up steps. For inference, we run DDPM sam-
pler for 1000 steps. We use cosine β scheduler [25] and set
β0 = 0.0001, βT = 0.02.
B. Qualitative Results
In this section, we present additional qualitative results.
Fig. 9 and Fig. 8 extend Fig. 5 and Fig. 6 in the main pa-
per respectively. The results are consistent with those in the
main paper, where our method consistently provides better
reconstruction accuracy in inpainting and super-resolution
and generate more diverse output in unconditional genera-
tion.

DiffAE 
Ours
Input
MyStyle
DiffAE 
Ours
Input
MyStyle
DiffAE 
Ours
Input
MyStyle
DiffAE 
Ours
Input
MyStyle
Figure 9. Qualitative results for image generation.

