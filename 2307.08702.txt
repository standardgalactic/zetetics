Diffusion Models Beat GANs on Image Classification
Soumik Mukhopadhyay∗
Matthew Gwilliam∗
Vatsal Agarwal
Namitha Padmanabhan
Archana Swaminathan
Srinidhi Hegde
Tianyi Zhou
Abhinav Shrivastava
University of Maryland, College Park
Abstract
While many unsupervised learning models focus on one family of tasks, either
generative or discriminative, we explore the possibility of a unified representation
learner: a model which uses a single pre-training stage to address both families
of tasks simultaneously. We identify diffusion models as a prime candidate. Dif-
fusion models have risen to prominence as a state-of-the-art method for image
generation, denoising, inpainting, super-resolution, manipulation, etc. Such models
involve training a U-Net to iteratively predict and remove noise, and the resulting
model can synthesize high fidelity, diverse, novel images. The U-Net architecture,
as a convolution-based architecture, generates a diverse set of feature represen-
tations in the form of intermediate feature maps. We present our findings that
these embeddings are useful beyond the noise prediction task, as they contain
discriminative information and can also be leveraged for classification. We explore
optimal methods for extracting and using these embeddings for classification tasks,
demonstrating promising results on the ImageNet classification task. We find that
with careful feature selection and pooling, diffusion models outperform compara-
ble generative-discriminative methods such as BigBiGAN for classification tasks.
We investigate diffusion models in the transfer learning regime, examining their
performance on several fine-grained visual classification datasets. We compare
these embeddings to those generated by competing architectures and pre-trainings
for classification tasks.
1
Introduction
Unified unsupervised image representation learning is a critical but challenging problem. Many
computer vision tasks can be broadly classified into two families, discriminative and generative. With
discriminative representation learning, one seeks to train a model that can apply labels to images or
parts of images. For generative learning, one would design a model that generates or edits images,
and performs similar tasks like inpainting, super-resolution, etc. Unified representation learners
seek to achieve both simultaneously, and the resulting model would be able to both discriminate and
generate novel visual artifacts.
Such unified representation learning is an arduous undertaking. BigBiGAN [2, 3] is one of the earliest
deep learning methods to address both families of tasks simultaneously. However, more recent
approaches outperform BigBiGAN in terms of both classification and generation performance by more
specialized models. Beyond BigBiGAN’s key accuracy and FID deficiencies, it is also much more
*These authors contributed equally to this work
Preprint. Under review.
arXiv:2307.08702v1  [cs.CV]  17 Jul 2023

2. Pool Sizes
UNet
Q K
V
Q K
V
Probing
1. Extract Features from Network
t
3. Heads
Model Efficiency on ImageNet-50
A
   C
D
D
B
Figure 1: An overview of our method and results. We propose that diffusion models are unified
self-supervised image representation learners, with impressive performance not only for generation,
but also for classification. We explore the feature extraction process in terms of U-Net block number
and diffusion noise time step. We also explore different sizes for the feature map pooling. We
examine several lightweight architectures for feature classification, including linear (A), multi-layer
perceptron (B), CNN (C), and attention-based heads (D). We show the results on such explorations
on the right, for classification heads trained on frozen features for ImageNet-50 [1], computed at
block number 24 and noise time step 90. See Section 4.1 for a detailed discussion.
burdensome to train than other methods; its encoder makes it larger and slower than comparable GANs,
and its GAN makes it more expensive than ResNet-based discriminative methods [4]. PatchVAE [5]
attempts to adapt VAE [6] to perform better for recognition tasks by focusing on learning mid-level
patches. Unfortunately, its classification gains still fall well short of supervised methods, and come at
great cost to image generation performance. Recent works have taken valuable steps by delivering
good performance in both generation and classification, both with [7] and without [8] supervision.
However, this field is relatively underexplored in comparison to the volume of work in self-supervised
image representation learning, and therefore, unified self-supervised representation learning remains
largely under-addressed.
As a result of previous shortcomings, some researchers have argued that there are inherent differences
between discriminative and generative models, and the representations learned by one are not well-
suited for the other [9]. Generative models naturally need representations that capture low-level, pixel
and texture details which are necessary for high fidelity reconstruction and generation. Discriminative
models, on the other hand, primarily rely on high-level information which differentiates objects at a
coarse level based not on individual pixel values, but rather on the semantics of the content of the
image. Despite these preconceptions, we suggest that the early success of BigBiGAN is endorsed by
recent approaches such as MAE [10] and MAGE [8], where the model must tend to low-level pixel
information, but learns models which are also very good for classification tasks.
Furthermore, state-of-the-art diffusion models have already achieved great success for generative
objectives. However, their classification capacity is largely ignored and unexplored. Thus, rather
than build a unified representation learner from the ground up, we posit that state-of-the-art diffusion
models, which are powerful image generation models, already possess potent emergent classification
properties. We demonstrate their impressive performance on these two very different tasks in Figure 1.
Our method for utilizing diffusion models results in much better image generation performance than
BigBiGAN, with better image classification performance as well. Thus, in terms of optimizing for
both classification and generation simultaneously, we show that diffusion models are already near
state-of-the-art unified self-supervised representation learners.
One of the main challenges with diffusion models is feature selection. In particular, the selection
of noise steps and feature block is not trivial. So, we investigate and compare the suitability of
the various features. Additionally, these feature maps can be quite large, in terms of both spatial
resolution and channel depth. To address this, we also suggest various classification heads to take the
place of the linear classification layer, which can improve classification results, without any addition
of parameters or sacrifice in generation performance. Critically, we demonstrate that with proper
feature extraction, diffusion models work very well as classifiers out-of-the-box, such that diffusion
models can be used for classification tasks without the need to modify the diffusion pre-training. As
2

such, our approach is flexible for any pre-trained diffusion model and can thus benefit from future
improvements to such models in terms of size, speed, and image quality.
We also investigate the performance of diffusion features for transfer learning on downstream tasks,
and we compare the features themselves directly to those from other methods. For downstream
tasks, we choose fine-grained visual classification (FGVC), an appealing area to use unsupervised
features due to implied scarcity of data for many FGVC datasets. This task is of particular interest
with a diffusion-based method since it does not rely on the sorts of color invariances that previous
works suggest may limit unsupervised methods in the FGVC transfer setting [11, 12]. To compare
the features, we rely on the popular centered kernel alignment (CKA) [13], which allows for a rich
exploration of the importance of feature selection as well as how similar diffusion model features are
to those from ResNets [4] and ViTs [14].
In summary, our contributions are as follows:
• We demonstrate that diffusion models can be used as unified representation learners, with
26.21 FID (-12.37 vs. BigBiGAN) for unconditional image generation and 61.95% accuracy
(+1.15% vs. BigBiGAN) for linear probing on ImageNet.
• We present analysis and distill principles for extracting useful feature representations from
the diffusion process.
• We compare standard linear probing to specialized MLP, CNN, and attention-based heads
for leveraging diffusion representations in a classification paradigm.
• We analyze the transfer learning properties of diffusion models, with fine-grained visual
categorization (FGVC) as a downstream task, on several popular datasets.
• We use CKA to compare the various representations learned by diffusion models, both
in terms of different layers and diffusion properties, as well as to other architectures and
pre-training methods.
2
Related Work
Generative Models.
Generative Adversarial Networks (GANs) [15] constitute a class of deep
neural networks which are capable of generating novel images in a data distribution given a random
latent vector z ∈Z as input, and are trained by optimizing a min-max game objective. GANs can be
class-conditioned, where they generate images given noise and class input, or unconditional, where
they generate random images from noise alone. Popular examples of GANs which have produced
high quality images include PGGAN [16], BigGAN [17], and StyleGANs [18–22]. Recent work in
GAN inversion finds that images can be mapped to GAN latent space [23], meaning that the GAN
learns a representation for the image in noise/latent space. Some of these approaches directly optimize
latent vectors to reconstruct the input image [24]. Others train encoders to generate the latent vector
corresponding to a given input image [25, 26]. Hybrid approaches are also popular, where an encoder
generates a latent vector which is then optimized to generate better reconstructions [27–30].
Diffusion denoising probabilistic models (DDPM) [31], a.k.a. diffusion models, are a class of
likelihood-based generative models which learn a denoising Markov chain using variational inference.
Diffusion models have proven to produce high-quality images [32] beating previous SOTA generative
models like BigGAN [17], VQVAE-2 [33] on FID metric on ImageNet[34]. These models enjoy
the benefit of having a likelihood-based objective like VAEs as well as high visual sample quality
like GANs even on high variability datasets. Recent advances in this area have also shown amazing
results in text-to-image generation including works like DALLE2 [35], Imagen [36], and Stable
Diffusion [37]. Application of these models is not just limited to generation but spans tasks like
object detection [38], and image segmentation [39]. While these are all trained and evaluated for
generative tasks, we observe they have discriminative capacity as well, and thus investigate their
potential for classification tasks.
Discriminative Models.
Discriminative models learn to represent images, and extract useful infor-
mation from images that can then be used to solve downstream tasks. Early representation learning
methods tried training neural network backbones with partially degraded inputs and learn image
representation by making the model predict the rest of the information in the actual image like Colori-
sation [40], Jigsaw [41], PIRL [42], Inpainting [43]. More recently, many approaches have emerged
3

that revolve around a contrastive loss objective, maximizing distance between positive-negative pairs,
such as SimCLR [44, 45], MoCo [46–48], Barlow Twins [49], and ReLICv2 [50]. On the other
hand, BYOL [9], SimSiam [51], and VICReg [52] introduce methods that work without negative
samples. DeepCluster [53] uses offline clustering whereas SwAV [54] introduces online clustering
and multi-view augmentation methods to get a better representation. DINO [55] uses self supervised
knowledge distillation between various views of an image in Visual Transformers [14]. PatchGame
introduces a referential games where two unsupervised models develop a mutual representation
through goal-oriented communication [56]. SEER [57] demonstrates the success of strong self-
supervised pre-training methods at the scale of billions of images. With all the recent advances, the
latest self-supervised methods have leveraged transformers and iteratively improved upon contrastive
and clustering objectives to surpass supervised methods on many key baselines [58–61].
Unified Models.
Other methods leverage the unsupervised nature of GANs to learn good image
representations [2, 62–64]. BiGAN [2] does joint Encoder-Generator training with a discriminator
which jointly discriminates image-latent pair. ALI [62] uses reparameterized sampling from the
encoder output. BigBiGAN [3] is the most popular among these methods – it is a BiGAN with a
BigGAN [17] generator and a discriminator with additional unary loss terms for image and latent
codes. In spite of their promising performance for downstream classification tasks, subsequent
contrastive pre-training methods that train more quickly, reliably, and with fewer parameters have
beaten their performance.
Distinct from GANs, autoencoders are a natural fit for the unified paradigm. ALAE attempts to learn
an encoder-generator map to perform both generation and classification [65]. PatchVAE improves on
the classification performance of VAE [6] by encouraging the model to learn good mid-level patch
representations [5]. MAE [10] and iBOT [66] train an autoencoder via masked image modeling, and
several other transformer-based methods have been built under that paradigm [67–69]. MAGE [8],
which uses a variable masking ratio to optimize for both recognition and generation, is the first
method to achieve both high-quality unconditional image generation and good classification results.
3
Approach
3.1
Diffusion Models Fundamentals
Diffusion models first define a forward noising process where gradual Gaussian noise is iteratively
added to an image x0, which is sampled from the data distribution q(x0), to get a completely
noised image xT in T steps. This forward process is defined as a Markov chain with latents
x1, x2 . . . , xt, . . . , xT −1, xT which represent noised images of various degrees. Formally, the forward
diffusion process is defined as
q(x1, . . . xT |x0) :=
T
Y
t=1
q(xt|xt−1)
q(xt|xt−1) := N(xt;
p
1 −βtxt−1, βtI)
(1)
where {βt}T
t=1 is the variance schedule and N is a normal distribution. As T →∞, xT nearly is
equivalent to the isotropic Gaussian distribution. With αt := 1 −βt and ¯αt := Qt
i=0 αi one can
sample a noised image xt at diffusion step t directly from a real image x0 using
xt = √¯αtx0 +
√
1 −¯αtϵ, ϵ ∼N(0, I)
(2)
The reverse diffusion process aims to reverse the forward process and sample from the posterior
distribution q(xt−1|xt) which depends on the entire data distribution. Doing this iteratively can
denoise a completely noisy image xT , such that one can sample from the data distribution q(x0).
This is typically approximated using a neural network ϵθ as
pθ(xt−1|xt) := N

xt−1;
1
√αt

xt −
βt
√1 −¯αt
ϵθ(xt, t)

, Σθ(xt, t)

(3)
When p and q are interpreted as a VAE, a simplified version of the variational lower bound objective
turns out to be just a mean squared error loss [31]. This can be used to train ϵθ which learns to
approximate the Gaussian noise ϵ added to the real image x0 in Eq. 2 as
Lsimple = Ex0,t,ϵ[∥ϵθ(xt, t) −ϵ∥2
2]
(4)
4

As for Σθ(xt, t), previous works keep it either fixed [31] or learn it using the original variational
lower-bound objective [70, 32].
3.2
Diffusion Models Feature Extraction
In this work, we use the guided diffusion (GD) implementation, which uses a U-Net-style architecture
with residual blocks for ϵθ. This implementation improves over the original [31] architecture by
adding multi-head self-attention at multiple resolutions, scale-shift norm, and using BigGAN [17]
residual blocks for upsampling and downsampling. We consider each of these residual blocks,
residual+attention blocks, and downsampling/upsampling residual blocks as individual blocks and
number them as b ∈{1, 2, ..., 37} for the pre-trained unconditional 256×256 guided diffusion model.
Our feature extraction is parameterized with the diffusion step t and model block number b. We show
an illustration of how input images vary at different time steps in Figure 3. For feature extraction of
image x0, we use Eq. 2 to get noised image xt. In the forward pass through the network ϵθ(xt, t), we
use the activation after the block number b as our feature vector fθ(x0, t, b).
3.3
Linear Probing and Alternatives
The two most common methods for evaluating the effectiveness of self-supervised pre-training are
linear probing and finetuning, and we match the popular recipes documented by VISSL [71] to the
extent possible. While correlated, these test different properties of the pre-training. Linear probing,
which learns a batch normalization and linear layer on top of frozen features, tests the utility of the
learned feature representations – it shows whether the pre-training learns disentangled representations,
and whether these feature meaningful semantic correlations. Finetuning, on the other hand, learns a
batch normalization and linear layer but with no frozen features. In the finetuning regime, we treat the
pre-training method as an expensive weight initialization method, and retrain the entire architecture
for classification.
In this paper, we focus more on the representative capacity of the frozen features, which is of
particular interest in areas like fine-grained classification and few shot learning, where data may be
insufficient for finetuning. Additionally, this allows us to make statements with respect to the utility
of the learned features, rather than the learned weights. We note that the diffusion models are like
regular convolutional nets in the sense that they do not natively produce a linear feature, instead
generating a series of feature maps at various points in the network. Thus, similar to other CNNs, we
use a combination of pooling and flattening to yield a vector feature representation for each image.
The channel depth and feature map size are naturally quite large, so in addition to standard pooling,
we also try other methods. We investigate multi-layer perceptron heads. Due to the large size, we
also try CNNs as a learned pooling mechanism, and give more complete details for the design in the
appendix. We also investigate the ability of attention heads to perform appropriate aggregation of
both spatial and channel information, with full details in the appendix.
4
Experiments
We first provide some preliminaries for setup and replication purposes, specifically with respect to
model architecture, critical hyperparameters, and hardware details. Then, we give statistics for the
datasets we use for our experiments. We give our primary results in Section 4.1 – we compare our
diffusion extraction to baselines as well as competing unified representation methods. We provide
ablations in Section 4.1.1 to discover optimal block numbers, pooling sizes, and time steps for feature
extraction. We evaluate the fitness of diffusion for downstream classification tasks by providing
results for popular FGVC datasets in Section 4.2. We perform the analysis of our representations
in Section 4.3 to compare representations both internally (between blocks of the U-Net) as well as
externally (between different U-Nets and with other self-supervised learning architectures).
Experiment Details. Unless otherwise specified, we use the unconditional ADM U-Net architecture
from Guided Diffusion [32] with total timesteps T = 1000. We use the 256×256 checkpoint; thus
we resize all inputs to this size and use center-crop and flipping for data augmentation. We use an
adaptive average pool to reduce the spatial dimension, followed by a single linear layer. For linear
probing, we train only this single layer. We use cross entropy loss with an Adam optimizer [72].
We follow the VISSL protocol for linear probing – 28 epochs, with StepLR at 0.1 gamma every 8
5

Table 1: Main results. We compare unified learn-
ers in terms of classification and generation at
resolution 256.
Method
Accuracy
FID
BigBiGAN*
60.8%
28.54
MAGE
78.9%
9.10
U-Net Encoder
64.32%
n/a
GD (L, pool 1×1)
61.95%
26.21
GD (L, pool 2×2)
64.96%
26.21
GD (Attention)
71.89%
26.21
*BigBiGAN’s best FID is at generator resolution 128.
Table 2: Finetuning results. Non-GD methods use
ViT-L. Except for MAGE, all other methods use
224×224 images.
Method
Accuracy
Supervised
82.5%
MoCo v3
84.1%
MAE
84.9%
MAGE
84.3%
GD (Linear, pool 2×2)
73.17%
GD (Linear, pool 4×4)
73.50%
Table 3: Attention head ImageNet-1k classifica-
tion results.
b
t
Accuracy (L)
Accuracy (A)
19
90
55.09%
66.03%
19
150
54.77%
64.85%
24
90
61.95%
71.89%
24
150
61.86%
70.98%
Table 4: Stable Diffusion linear probe results.
Condition
b
Size
Accuracy
Null Text
18
512
64.67%
Null Text
15
512
55.77%
Null Text
18
256
41.37%
Learnable
18
512
65.18%
Guided Diffusion
24
256
61.86 %
epochs. However, we do not use random cropping or batch norm. For hardware, the majority of our
experiments are run on 4 NVIDIA RTX A5000 GPUs.
Datasets. The dataset we use for our main result is ImageNet-1k [34]. Additionally, we run ablations
and similar explorations on ImageNet-50, which is a selection of 50 classes of ImageNet as also used
in [1]. Please see Table 5 for exact datasets and details.
4.1
Main Results: ImageNet Classification
First, we show the promising linear probing performance of diffusion in Table 1, using settings we
select via the ablations described in Section 4.1.1. As a baseline, we compare to the diffusion pre-
trained classifier, since it uses the same U-Net encoder. We also offer a comparison to other unified
models: BigBiGAN [3] and MAGE [8]. We outperform BigBiGAN in terms of both generation and
classification, especially when BigBiGAN is forced to handle the higher resolution, 256×256 images.
Hence, diffusion models beat GANs for image classification (and generation). We acknowledge
that diffusion is not yet state-of-the-art compared to classification-only models, with a gap of over
10% top-1 accuracy, or compared to the powerful unified MAGE model. However, we note that we
are unable to completely match the resources necessary to mimic the linear probe settings of other
methods. MAE [10], for example, trains their linear layer for 100 epochs with 16,384 images per
batch. Thus, it is difficult to present “fair” comparisons with such methods.
We perform finetuning, under similar conditions. Shown in Table 2, guided diffusion lags behind
other methods which use classification specific adjustments. Regardless, this is a better result than
the U-Net encoder by a fair margin (+9.38%), which suggests that guided diffusion is a useful
pre-training for classification.
As described previously, we also propose several approaches to deal with the large spatial and
channel dimensions of U-Net representations. Naively, we can use a single linear layer with different
preliminary pooling, and we show results for various pooling dimensions. Alternatively, we can use a
more powerful MLP, CNN, or attention head to address varying aspects of the feature map height,
width, and depth. For fairness, we train CNNs, MLPs, and attention heads with comparable parameter
counts to our linear layers under the various pooling settings. We show results for such heads, on
ImageNet-50, in Figure 1 (right), with full numerical results and model details in the appendix. We
note that the attention head performs the best by a fair margin. In Table 3, we try the best-performing
attention head on ImageNet (all classes), and find it significantly outperforms the simple linear probe,
6

0
100
200
300
400
500
Time Step
30
35
40
45
50
55
Accuracy (%)
b=19
0
5
10
15
20
25
30
35
Block Number
10
20
30
40
50
60
t=90
t=150
1
2
4
8
Pooling
50
55
60
65
70
t=90, b=24
t=150, b=19
Figure 2: Ablations on ImageNet (1000 classes) with varying block numbers, time steps, and pooling
size, for a linear classification head on frozen features. We find the model is least sensitive to pooling,
and most sensitive to block number, although there is also a steep drop-off in performance as inputs
and predictions become noisier.
t=0
t=300
t=150
t=90
t=50
t=30
t=10
t=500
Figure 3: Images at different time steps of the diffusion process, with noise added successively. We
observe that the best accuracies are obtained at t = 90.
regardless of pooling. This suggests the classification head is an important mechanism for extracting
useful representations from diffusion models, and it could be extended to other generative models.
4.1.1
Ablations
As shown in Figure 1, extracting good features from diffusion models requires careful consideration
of noise step, block number, and pooling size. We initiate a search of that hyperparameter space
for ImageNet. We set a search space of roughly log-equidistant time steps for the noise. We try
several blocks at even intervals around the U-Net bottleneck. We also address the feature height
and width (pooling). From our linear search, shown in Figure 2, we find t should be set to 90 and
b to 24. However, as we discuss in Section 4.2, we find that such settings are at least somewhat
data dependent. Thus, while in this work we distill some general settings and principles, automatic
selection and combination of features could be explored in future work.
For further ablations, we explore to what extent our idea is valid for other diffusion models. We
specifically examine stable diffusion, training a classifier on frozen features for 15 epochs, with t
fixed at 150. Thus, in Table 4, we show that stable diffusion features also lend themselves well to
classification. Critically, this means not only that our approach is flexible, but that lighter diffusion
models with better performance that are developed in the future could be immediately leveraged as
unified representation models by our method.
4.2
Results: Fine-grained Visual Classification (FGVC)
Here, we give results for applying our method in the transfer setting to the datasets defined in Table 5.
We use both standard linear probing, as well as each of our classification heads (with their best
ImageNet-50 configurations). We show these results in Figure 4. Note that there is a performance
gap between the diffusion model and SimCLR, regardless of classification head used. One notable
exception is Aircraft, where diffusion outperforms SimCLR for 3 of the 4 heads; this is indicative of
its promising performance.
Additionally, we find that feature selection is not trivial, and often the settings that work for various
FGVC datasets do not correspond to the ideal ImageNet settings. For example, consider that attention,
the best head for ImageNet-50, tends to perform the worst for FGVC. This may be due to their reliance
on the amount of data to learn properly. Furthermore, as we explore the feature selection problem
on CUB on Figure 5, we find that the ideal block number for ImageNet (b = 24) underperforms
substantially for CUB compared to b = 19. Hyperparameter changes that have a more subdued effect
7

Aircraft
Cars
CUB
Dogs
Flowers
NABirds
Dataset
20
40
60
80
100
Accuracy (%)
GD (L)
GD (MLP)
GD (CNN)
GD (Attention)
SimCLR (L)
SwAV (L)
Figure 4: Fine-Grained Visual Classification (FGVC) results. We train our best classification heads
from our ImageNet-50 explorations on FGVC datasets (denoted with GD), and compare against
the results from linear probing a SimCLR ResNet-50 on the same datasets. Linear is denoted by
(L). While SimCLR and SwAV tend to perform better, diffusion achieves promising results, slightly
outperforming SimCLR for Aircraft.
100
200
300
400
500
Time Step
5
10
15
20
25
30
35
Accuracy (%)
Block 15
100
200
300
400
500
Time Step
Block 19
100
200
300
400
500
Time Step
Block 24
Pool Size
1
2
4
8
Figure 5: FGVC feature extraction analysis. We show accuracy for different block numbers, time
steps, and pooling sizes. Block 19 is superior for FGVC, in contrast to ImageNet where 24 was ideal.
on ImageNet, such as pooling size, can result in up to 3× change in performance on accuracy for
CUB. Thus, determining a more robust feature selection procedure or introducing some regularization
during the diffusion training might be important future work to make transfer more reliable.
4.3
Representation Analysis
We use linear centered kernel alignment (CKA) [13] to find the degree of similarity between the
representations of different blocks of the diffusion model. Following conventions from prior work
that use samples for CKA [12, 73], we use the 2,500 image test set of ImageNet-50 (see Table 5). We
first examine differences in the representations between guided diffusion blocks at various time steps
and feature dimensions (pooling size) within our diffusion method in Figure 6. We also compare
our standard setting (t = 90 and d = 4096) against ResNet-50 and ViT representations with a
representative set of popular pre-training methods, as well as stable diffusion. For ResNet-50, we
extract the features from each bottleneck block while for ViT we extract features from each hidden
layer.
We note that the early layers tend to have higher similarity in all cases, suggesting that diffusion
models likely capture similar low-level details in the first few blocks. Also note the impact of
the time step: the representations are very dissimilar at later layers when the representations are
computed using images from different noise time steps. However, interestingly, we find that around
the bottleneck, the layers of GD tend to have similar representations to ResNets and ViTs, suggesting
that GD’s later layers naturally learn discriminative properties. This further supports our findings in
Table 1 and Table 3, where we show the promising classification performance with GD features.
8

Figure 6: Feature representation comparisons via centered kernel alignment (CKA). On the top 2
rows, we compare guided diffusion (GD) representations between its own layers, at varying time
steps and feature size. On the bottom 2 rows, we compare GD, with standard t = 90 and d = 4096,
against both ResNets and ViTs with various pre-training methods. For the bottom right corner we
compare against Stable Diffusion (SD), b = 18, size = 512.
5
Conclusion
In this paper, we present an approach for using the representations learned by diffusion models
for classification tasks. This re-positions diffusion models as potential state-of-the-art unified self-
supervised representation learners. We explain best practices for identifying these representations
and provide initial guidance for extracting high-utility discriminative embeddings from the diffusion
process. We demonstrate promising transfer learning properties and investigate how different datasets
require different approaches to feature extraction. We compare the diffusion representations in
terms of CKA, both to show what diffusion models learn at different layers as well as how diffusion
representations compare to those from other methods.
Broader Impacts. With our paper, we analyze algorithms; we do not provide new real-world
applications. Nevertheless, our work deals with image generation, which carries ethical concerns with
respect to potential misinformation generation. However, we do not improve over existing generation
approaches, so the potential harms seem negligible.
Limitations. Training diffusion models, even just for linear probing, is very computationally intensive.
So, we could not provide an analysis of variability in this work. Nevertheless, our work is an important
first step for leveraging the capacity of diffusion models for discriminative tasks.
9

References
[1] W. Van Gansbeke, S. Vandenhende, S. Georgoulis, M. Proesmans, and L. Van Gool, “Scan: Learning to
classify images without labels,” in Proceedings of the European Conference on Computer Vision, 2020.
[2] J. Donahue, P. Krähenbühl, and T. Darrell, “Adversarial feature learning,” arXiv preprint arXiv:1605.09782,
2016.
[3] J. Donahue and K. Simonyan, “Large scale adversarial representation learning,” Advances in neural
information processing systems, vol. 32, 2019.
[4] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” 2015.
[5] K. Gupta, S. Singh, and A. Shrivastava, “Patchvae: Learning local latent codes for recognition,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June
2020.
[6] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” 2022.
[7] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman, “Maskgit: Masked generative image transformer,”
in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.
[8] T. Li, H. Chang, S. K. Mishra, H. Zhang, D. Katabi, and D. Krishnan, “Mage: Masked generative encoder
to unify representation learning and image synthesis,” 2022.
[9] J. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. Á. Pires, Z. D.
Guo, M. G. Azar, B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko, “Bootstrap your own latent: A new
approach to self-supervised learning,” CoRR, vol. abs/2006.07733, 2020.
[10] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. B. Girshick, “Masked autoencoders are scalable vision
learners,” CoRR, vol. abs/2111.06377, 2021.
[11] T. Xiao, X. Wang, A. A. Efros, and T. Darrell, “What should not be contrastive in contrastive learning,”
2021.
[12] M. Gwilliam and A. Shrivastava, “Beyond supervised vs. unsupervised: Representative benchmarking
and analysis of image representation learning,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 9642–9652, June 2022.
[13] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton, “Similarity of neural network representations revisited,”
in Proceedings of the 36th International Conference on Machine Learning (K. Chaudhuri and R. Salakhut-
dinov, eds.), vol. 97 of Proceedings of Machine Learning Research, pp. 3519–3529, PMLR, 09–15 Jun
2019.
[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-
derer, G. Heigold, S. Gelly, et al., “An image is worth 16x16 words: Transformers for image recognition at
scale,” arXiv preprint arXiv:2010.11929, 2020.
[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,
“Generative adversarial networks,” Communications of the ACM, vol. 63, no. 11, pp. 139–144, 2020.
[16] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of gans for improved quality, stability,
and variation,” arXiv preprint arXiv:1710.10196, 2017.
[17] A. Brock, J. Donahue, and K. Simonyan, “Large scale gan training for high fidelity natural image synthesis,”
arXiv preprint arXiv:1809.11096, 2018.
[18] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June
2019.
[19] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila, “Analyzing and improving the
image quality of stylegan,” in Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 8110–8119, 2020.
[20] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and T. Aila, “Training generative adversarial
networks with limited data,” Advances in Neural Information Processing Systems, vol. 33, pp. 12104–12114,
2020.
10

[21] T. Karras, M. Aittala, S. Laine, E. Härkönen, J. Hellsten, J. Lehtinen, and T. Aila, “Alias-free generative
adversarial networks,” Advances in Neural Information Processing Systems, vol. 34, pp. 852–863, 2021.
[22] A. Sauer, K. Schwarz, and A. Geiger, “Stylegan-xl: Scaling stylegan to large diverse datasets,”
vol. abs/2201.00273, 2022.
[23] W. Xia, Y. Zhang, Y. Yang, J. Xue, B. Zhou, and M. Yang, “GAN inversion: A survey,” CoRR,
vol. abs/2101.05278, 2021.
[24] R. Abdal, Y. Qin, and P. Wonka, “Image2stylegan: How to embed images into the stylegan latent space?,”
CoRR, vol. abs/1904.03189, 2019.
[25] E. Richardson, Y. Alaluf, O. Patashnik, Y. Nitzan, Y. Azar, S. Shapiro, and D. Cohen-Or, “Encoding in
style: a stylegan encoder for image-to-image translation,” CoRR, vol. abs/2008.00951, 2020.
[26] O. Tov, Y. Alaluf, Y. Nitzan, O. Patashnik, and D. Cohen-Or, “Designing an encoder for stylegan image
manipulation,” CoRR, vol. abs/2102.02766, 2021.
[27] J.-Y. Zhu, P. Krähenbühl, E. Shechtman, and A. A. Efros, “Generative visual manipulation on the natural
image manifold,” in European conference on computer vision, pp. 597–613, Springer, 2016.
[28] J. Zhu, Y. Shen, D. Zhao, and B. Zhou, “In-domain gan inversion for real image editing,” in European
conference on computer vision, pp. 592–608, Springer, 2020.
[29] D. Roich, R. Mokady, A. H. Bermano, and D. Cohen-Or, “Pivotal tuning for latent-based editing of real
images,” ACM Transactions on Graphics (TOG), vol. 42, no. 1, pp. 1–13, 2022.
[30] Y. Alaluf, O. Tov, R. Mokady, R. Gal, and A. Bermano, “Hyperstyle: Stylegan inversion with hypernetworks
for real image editing,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 18511–18521, 2022.
[31] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” Advances in Neural Information
Processing Systems, vol. 33, pp. 6840–6851, 2020.
[32] P. Dhariwal and A. Nichol, “Diffusion models beat gans on image synthesis,” 2021.
[33] A. Razavi, A. Van den Oord, and O. Vinyals, “Generating diverse high-fidelity images with vq-vae-2,”
Advances in neural information processing systems, vol. 32, 2019.
[34] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image
database,” in 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255, Ieee, 2009.
[35] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical text-conditional image generation
with clip latents,” arXiv preprint arXiv:2204.06125, 2022.
[36] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,
B. Karagol Ayan, T. Salimans, et al., “Photorealistic text-to-image diffusion models with deep language
understanding,” Advances in Neural Information Processing Systems, vol. 35, pp. 36479–36494, 2022.
[37] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with
latent diffusion models,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 10684–10695, 2022.
[38] S. Chen, P. Sun, Y. Song, and P. Luo, “Diffusiondet: Diffusion model for object detection,” arXiv preprint
arXiv:2211.09788, 2022.
[39] R. Burgert, K. Ranasinghe, X. Li, and M. S. Ryoo, “Peekaboo: Text to image diffusion models are zero-shot
segmentors,” arXiv preprint arXiv:2211.13224, 2022.
[40] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” in European conference on computer
vision, pp. 649–666, Springer, 2016.
[41] M. Noroozi and P. Favaro, “Unsupervised learning of visual representations by solving jigsaw puzzles,” in
European conference on computer vision, pp. 69–84, Springer, 2016.
[42] I. Misra and L. v. d. Maaten, “Self-supervised learning of pretext-invariant representations,” in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6707–6717, 2020.
11

[43] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, “Context encoders: Feature learning
by inpainting,” in Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 2536–2544, 2016.
[44] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual
representations,” in International conference on machine learning, pp. 1597–1607, PMLR, 2020.
[45] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton, “Big self-supervised models are strong
semi-supervised learners,” Advances in neural information processing systems, vol. 33, pp. 22243–22255,
2020.
[46] X. Chen, H. Fan, R. B. Girshick, and K. He, “Improved baselines with momentum contrastive learning,”
CoRR, vol. abs/2003.04297, 2020.
[47] X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines with momentum contrastive learning,” arXiv
preprint arXiv:2003.04297, 2020.
[48] X. Chen*, S. Xie*, and K. He, “An empirical study of training self-supervised vision transformers,” arXiv
preprint arXiv:2104.02057, 2021.
[49] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny, “Barlow twins: Self-supervised learning via redundancy
reduction,” in Proceedings of the 38th International Conference on Machine Learning (M. Meila and
T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 12310–12320, PMLR, 18–24
Jul 2021.
[50] N. Tomasev, I. Bica, B. McWilliams, L. Buesing, R. Pascanu, C. Blundell, and J. Mitrovic, “Pushing the
limits of self-supervised resnets: Can we outperform supervised learning without labels on imagenet?,”
2022.
[51] X. Chen and K. He, “Exploring simple siamese representation learning,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pp. 15750–15758, 2021.
[52] A. Bardes, J. Ponce, and Y. LeCun, “Vicreg: Variance-invariance-covariance regularization for self-
supervised learning,” ArXiv, vol. abs/2105.04906, 2021.
[53] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for unsupervised learning of visual
features,” in Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018.
[54] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, “Unsupervised learning of visual
features by contrasting cluster assignments,” Advances in Neural Information Processing Systems, vol. 33,
pp. 9912–9924, 2020.
[55] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin, “Emerging properties in
self-supervised vision transformers,” in Proceedings of the International Conference on Computer Vision
(ICCV), 2021.
[56] K. Gupta, G. Somepalli, A. Anubhav, V. Y. J. Magalle Hewa, M. Zwicker, and A. Shrivastava, “Patchgame:
Learning to signal mid-level patches in referential games,” in Advances in Neural Information Processing
Systems (M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 26015–
26027, Curran Associates, Inc., 2021.
[57] P. Goyal, Q. Duval, I. Seessel, M. Caron, I. Misra, L. Sagun, A. Joulin, and P. Bojanowski, “Vision models
are more robust and fair when pretrained on uncurated images without supervision,” 2022.
[58] B. Pang, Y. Zhang, Y. Li, J. Cai, and C. Lu, “Unsupervised visual representation learning by synchronous
momentum grouping,” 2022.
[59] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa,
A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat,
V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski, “Dinov2:
Learning robust visual features without supervision,” 2023.
[60] P. Zhou, Y. Zhou, C. Si, W. Yu, T. K. Ng, and S. Yan, “Mugs: A multi-granular self-supervised learning
framework,” 2022.
[61] C. Li, J. Yang, P. Zhang, M. Gao, B. Xiao, X. Dai, L. Yuan, and J. Gao, “Efficient self-supervised vision
transformers for representation learning,” 2022.
12

[62] V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and A. Courville, “Adversari-
ally learned inference,” arXiv preprint arXiv:1606.00704, 2016.
[63] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, “Infogan: Interpretable
representation learning by information maximizing generative adversarial nets,” Advances in neural
information processing systems, vol. 29, 2016.
[64] W. Nie, T. Karras, A. Garg, S. Debnath, A. Patney, A. B. Patel, and A. Anandkumar, “Semi-supervised
stylegan for disentanglement learning,” in Proceedings of the 37th International Conference on Machine
Learning, pp. 7360–7369, 2020.
[65] S. Pidhorskyi, D. A. Adjeroh, and G. Doretto, “Adversarial latent autoencoders,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14104–14113, 2020.
[66] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, “ibot: Image bert pre-training with
online tokenizer,” 2022.
[67] M. Assran, M. Caron, I. Misra, P. Bojanowski, F. Bordes, P. Vincent, A. Joulin, M. Rabbat, and N. Ballas,
“Masked siamese networks for label-efficient learning,” 2022.
[68] H. Bao, L. Dong, S. Piao, and F. Wei, “Beit: Bert pre-training of image transformers,” 2022.
[69] Z. Huang, X. Jin, C. Lu, Q. Hou, M.-M. Cheng, D. Fu, X. Shen, and J. Feng, “Contrastive masked
autoencoders are stronger vision learners,” 2022.
[70] A. Q. Nichol and P. Dhariwal, “Improved denoising diffusion probabilistic models,” in International
Conference on Machine Learning, pp. 8162–8171, PMLR, 2021.
[71] P. Goyal, Q. Duval, J. Reizenstein, M. Leavitt, M. Xu, B. Lefaudeux, M. Singh, V. Reis, M. Caron,
P. Bojanowski, A. Joulin, and I. Misra, “Vissl.” https://github.com/facebookresearch/vissl,
2021.
[72] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” 2017.
[73] M. Walmer, S. Suri, K. Gupta, and A. Shrivastava, “Teaching matters: Investigating the role of supervision
in vision transformers,” 2023.
[74] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi, “Fine-grained visual classification of aircraft,”
tech. rep., 2013.
[75] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d object representations for fine-grained categorization,” in
4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), (Sydney, Australia),
2013.
[76] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The Caltech-UCSD Birds-200-2011 Dataset,”
Tech. Rep. CNS-TR-2011-001, California Institute of Technology, 2011.
[77] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei, “Novel dataset for fine-grained image categoriza-
tion,” in First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision
and Pattern Recognition, (Colorado Springs, CO), June 2011.
[78] M.-E. Nilsback and A. Zisserman, “Automated flower classification over a large number of classes,” in
Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.
[79] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis, P. Perona, and S. Belongie, “Building a
bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset
collection,” in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 595–604,
2015.
13

Diffusion Models Beat GANs on Image Classification
Supplementary Material
A
Method Details
Convolutional Heads. In the context of standard linear probing, we treat our convolutional head as a
replacement for the feature pooling step. That is, instead of an adaptive average pool followed by a
linear layer, we train a convolutional head, followed by a linear layer. While we explore different
channel dimensions, the architecture consists of 2 blocks, each with a 2×2 convolution followed by a
2×2 maximum pooling operation. We perform a 2×2 adaptive average pool, flattening the result so
it can be used by a linear layer. We indicate the convolution heads we train in Table 8, where the first
(input) channels is always 1024 (the number of channels of the feature maps at the chosen U-Net
block), but the output channels of both learned convolution operations is treated as a hyperparameter.
Attention Heads.
In addition to applying convolutional heads, we experiment with a more sophisticated architecture
applying Transformer blocks. Specifically, we first use a 2×2 pooling layer to reduce the spatial
resolution of the feature maps to 8×8. Each token has a channel dimension of 1024, corresponding
to the number of channels of the feature maps extracted from the U-Net block. We then flatten
these features to generate 64 tokens for our Transformer. We append a CLS token to these set of
tokens which we then use to make the final classification. We follow the standard Transformer block
architecture consisting of Layer Normalization and QKV-Attenton. We treat the number of attention
blocks as a hyperparameter and our choices and the corresponding results are shown in Table 9.
B
Experiment Details
We provide additional clarifying details for our experiments. First, the ablations shown in Figure 2 are
the result of training on ImageNet for 15 epochs, instead of our complete linear probing recipe, due
to resource constraints. Second, the stable diffusion model we compare to was text-guided during its
pre-training, and thus, unlike our guided diffusion checkpoint, it was not fully unsupervised. Finally,
we provide parameter count comparisons for our method and the other unified representation learners
in Table 10.
C
Ablations
Aircraft
Cars
CUB
Dogs
Flowers
NABirds
Dataset
20
40
60
80
100
Accuracy (%)
GD (L)
GD (MLP)
GD (CNN)
GD (Attention)
BigBiGAN (L)
SimCLR (L)
SwAV (L)
Figure 7: Fine-Grained Visual Classification (FGVC) results, where the classification heads were
trained with random resize cropping. Similar to Figure 4 in terms of methods chosen, except now we
provide results for BigBiGAN also.
As noted, in the main paper, we use only random flipping for our linear probe, unlike standard
settings which also use a random resized crop. This is in part due to the fact that while standard
unsupervised pre-training would use random cropping, guided diffusion does not. However, for the
sake of thorough comparison, we also try FGVC linear probing with the standard, random resize
1

cropping setup. We provide these results in Figure 7. Note that the diffusion heads tend to perform
worse with the extra augmentations, possibly due to a lack of learned invariance. We also include
BigBiGAN. Note that this model was implemented in Tensorflow, and we also perform the linear
probing in Tensorflow for this model only.
Table 5: Dataset details used in this work.
Dataset
#Cls
#Train
#Test
Aircraft [74]
100
6,667
3,333
Cars [75]
196
8,144
8,041
CUB [76]
200
5,994
5,794
Dogs [77]
120
12,000
8,580
Flowers [78]
102
2,040
6,149
NABirds [79]
555
23,929
24,633
ImageNet [34]
1000
1.3mil
50,000
ImageNet-50 [1]
50
64,274
2,500
Table 6: Linear probing result with our best setting
and augmentation.
Setting
Accuracy
Linear Probe (t = 90, b = 24, Pool=2)
65.07 %
We also try data augmentations for our best linear probe setting, with results provided in Table 6
after 15 epochs. We do not observe any significant improvement over the original setting without
augmentation.
D
Figure 1 Details
Now, we provide more details, in Table 7, Table 8, and Table 9 for the information shown in Figure 1.
We show exact accuracies and parameter counts. We also specify our hyperparameter selection for
each head.
Table 7: Linear and MLP results. For linear, 1k,
4k, 16k, and 65k indicate the size of the feature
after pooling and flattening. For MLP, the first
number is the size of the feature after pooling
and flattening, and the succeeding numbers are
hidden sizes of layers before the last (classifica-
tion) layer.
Head
Params
Accuracy
Linear-1k
1M
87.20%
Linear-4k
4M
89.41%
Linear-16k
16M
89.58%
Linear-65k
65M
88.28%
MLP-1k-2k
4M
87.93%
MLP-4k-2k
10M
88.50%
MLP-4k-2k-2k
14M
88.67%
MLP-16k-2k
34M
89.76%
Table 8: CNN head results. Channel sizes are
separated by dashes. The first is the channel size
of the input feature maps, the next is the output
channels from the first convolution, and the last
is the output dimension of the second convolu-
tion. These are treated as hyperparameters. For
more detail, see our code.
Head
Params
Accuracy
CNN-1k-256-256
2.5M
88.98%
CNN-1k-512-256
3.5M
88.67%
CNN-1k-1k-256
6M
88.93%
CNN-1k-1k-1k
12M
89.50%
CNN-1k-2k-512
14M
89.19%
CNN-1k-2k-2k
32M
89.76%
CNN-1k-4k-2k
48M
89.89%
CNN-1k-4k-2.5k
66M
89.50%
Table 9: Attention head results. The hyperparameters are
denoted following the dashes. The first hyperparameter is
similarly the channel size of the input feature maps and the
next is the number of Transformer blocks used.
Head
Params
Accuracy
Attention-1K-1
13.7M
91.67%
Attention-1K-2
26.2M
91.58%
Attention-1K-3
38.8M
92.01%
Attention-1K-4
51.4M
92.27%
Attention-1K-5
64.0M
92.71%
Table 10: Parameter counts of ma-
jor unified unsupervised represen-
tation learning methods. For each,
we consider the whole system, not
just the encoding network.
Method
# Params
BigBiGAN
502M
MAGE
439M
Ours
553M
2

