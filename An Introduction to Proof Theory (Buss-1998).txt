CHAPTER I
An Introduction to Proof Theory
Samuel R. Buss
Departments of Mathematics and Computer Science, University of California, San Diego
La Jolla, California 92093-0112, USA
Contents
1. Proof theory of propositional logic
. . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1. Frege proof systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.2. The propositional sequent calculus . . . . . . . . . . . . . . . . . . . . . .
10
1.3. Propositional resolution refutations . . . . . . . . . . . . . . . . . . . . . .
18
2. Proof theory of ﬁrst-order logic . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.1. Syntax and semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.2. Hilbert-style proof systems
. . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.3. The ﬁrst-order sequent calculus . . . . . . . . . . . . . . . . . . . . . . . .
31
2.4. Cut elimination
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.5. Herbrand’s theorem, interpolation and deﬁnability theorems
. . . . . . . .
48
2.6. First-order logic and resolution refutations . . . . . . . . . . . . . . . . . .
59
3. Proof theory for other logics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
3.1. Intuitionistic logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
3.2. Linear logic
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
HANDBOOK OF PROOF THEORY
Edited by S. R. Buss
c⃝1998 Elsevier Science B.V. All rights reserved

2
S. Buss
Proof Theory is the area of mathematics which studies the concepts of mathemat-
ical proof and mathematical provability. Since the notion of “proof” plays a central
role in mathematics as the means by which the truth or falsity of mathematical
propositions is established; Proof Theory is, in principle at least, the study of
the foundations of all of mathematics.
Of course, the use of Proof Theory as a
foundation for mathematics is of necessity somewhat circular, since Proof Theory is
itself a subﬁeld of mathematics.
There are two distinct viewpoints of what a mathematical proof is. The ﬁrst view
is that proofs are social conventions by which mathematicians convince one another
of the truth of theorems. That is to say, a proof is expressed in natural language
plus possibly symbols and ﬁgures, and is suﬃcient to convince an expert of the
correctness of a theorem. Examples of social proofs include the kinds of proofs that
are presented in conversations or published in articles. Of course, it is impossible to
precisely deﬁne what constitutes a valid proof in this social sense; and, the standards
for valid proofs may vary with the audience and over time. The second view of proofs
is more narrow in scope: in this view, a proof consists of a string of symbols which
satisfy some precisely stated set of rules and which prove a theorem, which itself must
also be expressed as a string of symbols. According to this view, mathematics can
be regarded as a ‘game’ played with strings of symbols according to some precisely
deﬁned rules. Proofs of the latter kind are called “formal” proofs to distinguish them
from “social” proofs.
In practice, social proofs and formal proofs are very closely related.
Firstly,
a formal proof can serve as a social proof (although it may be very tedious and
unintuitive) provided it is formalized in a proof system whose validity is trusted.
Secondly, the standards for social proofs are suﬃciently high that, in order for a
proof to be socially accepted, it should be possible (in principle!) to generate a formal
proof corresponding to the social proof. Indeed, this oﬀers an explanation for the fact
that there are generally accepted standards for social proofs; namely, the implicit
requirement that proofs can be expressed, in principle, in a formal proof system
enforces and determines the generally accepted standards for social proofs.
Proof Theory is concerned almost exclusively with the study of formal proofs:
this is justiﬁed, in part, by the close connection between social and formal proofs,
and it is necessitated by the fact that only formal proofs are subject to mathematical
analysis. The principal tasks of Proof Theory can be summarized as follows. First, to
formulate systems of logic and sets of axioms which are appropriate for formalizing
mathematical proofs and to characterize what results of mathematics follow from
certain axioms; or, in other words, to investigate the proof-theoretic strength of
particular formal systems.
Second, to study the structure of formal proofs; for
instance, to ﬁnd normal forms for proofs and to establish syntactic facts about
proofs. This is the study of proofs as objects of independent interest. Third, to study
what kind of additional information can be extracted from proofs beyond the truth
of the theorem being proved. In certain cases, proofs may contain computational or
constructive information. Fourth, to study how best to construct formal proofs; e.g.,
what kinds of proofs can be eﬃciently generated by computers?

Introduction to Proof Theory
3
The study of Proof Theory is traditionally motivated by the problem of formaliz-
ing mathematical proofs; the original formulation of ﬁrst-order logic by Frege [1879]
was the ﬁrst successful step in this direction. Increasingly, there have been attempts
to extend Mathematical Logic to be applicable to other domains; for example,
intuitionistic logic deals with the formalization of constructive proofs, and logic
programming is a widely used tool for artiﬁcial intelligence.
In these and other
domains, Proof Theory is of central importance because of the possibility of computer
generation and manipulation of formal proofs.
This handbook covers the central areas of Proof Theory, especially the math-
ematical aspects of Proof Theory, but largely omits the philosophical aspects of
proof theory. This ﬁrst chapter is intended to be an overview and introduction to
mathematical proof theory. It concentrates on the proof theory of classical logic,
especially propositional logic and ﬁrst-order logic. This is for two reasons: ﬁrstly,
classical ﬁrst-order logic is by far the most widely used framework for mathematical
reasoning, and secondly, many results and techniques of classical ﬁrst-order logic
frequently carryover with relatively minor modiﬁcations to other logics.
This introductory chapter will deal primarily with the sequent calculus, and
resolution, and to lesser extent, the Hilbert-style proof systems and the natural
deduction proof system.
We ﬁrst examine proof systems for propositional logic,
then proof systems for ﬁrst-order logic. Next we consider some applications of cut
elimination, which is arguably the central theorem of proof theory. Finally, we review
the proof theory of some non-classical logics, including intuitionistic logic and linear
logic.
1. Proof theory of propositional logic
Classical propositional logic, also called sentential logic, deals with sentences and
propositions as abstract units which take on distinct True/False values. The basic
syntactic units of propositional logic are variables which represent atomic propo-
sitions which may have value either True or False.
Propositional variables are
combined with Boolean functions (also called connectives): a k-ary Boolean function
is a mapping from {T, F}k to {T, F} where we use T and F to represent True and
False. The most frequently used examples of Boolean functions are the connectives ⊤
and ⊥which are the 0-ary functions with values T and F , respectively; the binary
connectives ∧, ∨, ⊃, ↔and ⊕for “and”, “or”, “if-then”, “if-and-only-if” and
“parity”; and the unary connective ¬ for negation. Note that ∨is the inclusive-or
and ⊕is the exclusive-or.
We shall henceforth let the set of propositional variables be V = {p1, p2, p3, . . .};
however, our theorems below hold also for uncountable sets of propositional variables.
The set of formulas is inductively deﬁned by stating that every propositional variable
is a formula, and that if A and B are formulas, then (¬A), (A∧B), (A∨B), (A ⊃B),
etc., are formulas. A truth assignment consists of an assignment of True/False values
to the propositional variables, i.e., a truth assignment is a mapping τ : V →{T, F}.

4
S. Buss
A truth assignment can be extended to have domain the set of all formulas in the
obvious way, according to Table 1; we write τ(A) for the truth value of the formula A
induced by the truth assignment τ .
Table 1
Values of a truth assignment τ
A
B
(¬A)
(A ∧B)
(A ∨B)
(A ⊃B)
(A ↔B)
(A ⊕B)
T
T
F
T
T
T
T
F
T
F
F
F
T
F
F
T
F
T
T
F
T
T
F
T
F
F
T
F
F
T
T
F
A formula A involving only variables among p1, . . . , pk deﬁnes a k-ary Boolean
function fA, by letting fA(x1, ..., xk) equal the truth value τ(A) where τ(pi) = xi
for all i. A language is a set of connectives which may be used in the formation of
L-formulas. A language L is complete if and only if every Boolean function can be
deﬁned by an L-formula. Propositional logic can be formulated with any complete
(usually ﬁnite) language L — for the time being, we shall use the language ¬, ∧, ∨
and ⊃.
A propositional formula A is said to be a tautology or to be (classically) valid if
A is assigned the value T by every truth assignment. We write ⊨A to denote that
A is a tautology. The formula A is satisﬁable if there is some truth assignment that
gives it value T . If Γ is a set of propositional formulas, then Γ is satisﬁable if there
is some truth assignment that simultaneously satisﬁes all members of Γ. We say
Γ tautologically implies A, or Γ ⊨A, if every truth assignment which satisﬁes Γ also
satisﬁes A.
One of the central problems of propositional logic is to ﬁnd useful methods for
recognizing tautologies; since A is a tautology if and only if ¬A is not satisﬁable, this
is essentially the same as the problem of ﬁnding methods for recognizing satisﬁable
formulas. Of course, the set of tautologies is decidable, since to verify that a formula A
with n distinct propositional variables is a tautology, one need merely check that
the 2n distinct truth assignments to these variables all give A the value T . This
brute-force ‘method of truth-tables’ is not entirely satisfactory; ﬁrstly, because it can
involve an exorbitant amount of computation, and secondly, because it provides no
intuition as to why the formula is, or is not, a tautology.
For these reasons, it is often advantageous to prove that A is a tautology instead
of using the method of truth-tables. The next three sections discuss three commonly
used propositional proof systems. The so-called Frege proof systems are perhaps the
most widely used and are based on modus ponens. The sequent calculus systems
provide an elegant proof system which combines both the possibility of elegant proofs
and the advantage of an extremely useful normal form for proofs. The resolution
refutation proof systems are designed to allow for eﬃcient computerized search for
proofs. Later, we will extend these three systems to ﬁrst-order logic.

Introduction to Proof Theory
5
1.1. Frege proof systems
The mostly commonly used propositional proof systems are based on the use of
modus ponens as the sole rule of inference. Modus ponens is the inference rule, which
allows, for arbitrary A and B, the formula B to be inferred from the two hypotheses
A ⊃B and A; this is pictorially represented as
A
A ⊃B
B
In addition to this rule of inference, we need logical axioms that allow the inference
of ‘self-evident’ tautologies from no hypotheses. There are many possible choices for
sets of axioms: obviously, we wish to have a suﬃciently strong set of axioms so that
every tautology can be derived from the axioms by use of modus ponens. In addition,
we wish to specify the axioms by a ﬁnite set of schemes.
1.1.1. Deﬁnition.
A substitution σ is a mapping from the set of propositional
variables to the set of propositional formulas. If A is a propositional formula, then
the result of applying σ to A is denoted Aσ and is equal to the formula obtained by
simultaneously replacing each variable appearing in A by its image under σ.
An example of a set of axiom schemes over the language ¬, ∧, ∨and ⊃is given in
the next deﬁnition. We adopt conventions for omitting parentheses from descriptions
of formulas by specifying that unary operators have the highest precedence, the
connectives ∧and ∨have second highest precedence, and that ⊃and ↔have
lowest precedence. All connectives of the same precedence are to be associated from
right to left; for example, A ⊃¬B ⊃C is a shorthand representation for the formula
(A ⊃((¬B) ⊃C)).
1.1.2. Deﬁnition.
Consider the following set of axiom schemes:
p1 ⊃(p2 ⊃p1)
(p1 ⊃p2) ⊃(p1 ⊃¬p2) ⊃¬p1
(p1 ⊃p2) ⊃(p1 ⊃(p2 ⊃p3)) ⊃(p1 ⊃p3)
(¬¬p1) ⊃p1
p1 ⊃p1 ∨p2
p1 ∧p2 ⊃p1
p2 ⊃p1 ∨p2
p1 ∧p2 ⊃p2
(p1 ⊃p3) ⊃(p2 ⊃p3) ⊃(p1 ∨p2 ⊃p3)
p1 ⊃p2 ⊃p1 ∧p2
The propositional proof system F is deﬁned to have as its axioms every substitution
instance of the above formulas and to have modus ponens as its only rule.
An
F -proof of a formula A is a sequence of formulas, each of which is either an F -axiom
or is inferred by modus ponens from two earlier formulas in the proof, such that the
ﬁnal formula in the proof is A.
We write
F A, or just ⊢A, to say that A has an F -proof. We write Γ
F A,
or just Γ ⊢A, to say that A has a proof in which each formula either is deduced
according the axioms or inference rule of F or is in Γ. In this case, we say that A is
proved from the extra-logical hypotheses Γ; note that Γ may contain formulas which
are not tautologies.

6
S. Buss
1.1.3. Soundness and completeness of F .
It is easy to prove that every
F -provable formula is a tautology, by noting that all axioms of F are valid and that
modus ponens preserves the property of being valid. Similarly, whenever Γ
F A,
then Γ tautologically implies A. In other words, F is (implicationally) sound; which
means that all provable formulas are valid (or, are consequences of the extra-logical
hypotheses Γ).
Of course, any useful proof system ought to be sound, since the purpose of creating
proofs is to establish the validity of a sentence. Remarkably, the system F is also
complete in that it can prove any valid formula. Thus the semantic notion of validity
and the syntactic notion of provability coincide, and a formula is valid if and only if
it is provable in F .
Theorem. The propositional proof system F is complete and is implicationally
complete; namely,
(1) If A is a tautology, then
F A.
(2) If Γ ⊨A, then Γ
F A.
The philosophical signiﬁcance of the completeness theorem is that a ﬁnite set of
(schematic) axioms and rules of inference are suﬃcient to establish the validity of
any tautology. In hindsight, it is not surprising that this holds, since the method of
truth-tables already provides an algorithmic way of recognizing tautologies. Indeed,
the proof of the completeness theorem given below, can be viewed as showing that
the method of truth tables can be formalized within the system F .
1.1.4. Proof.
We ﬁrst observe that part (2) of the completeness theorem can be
reduced to part (1) by a two step process. Firstly, note that the compactness theorem
for propositional logic states that if Γ ⊨A then there is a ﬁnite subset Γ0 of Γ which
also tautologically implies A. A topological proof of the compactness theorem for
propositional logic is sketched in 1.1.5 below. Thus Γ may, without loss of generality,
be assumed to be a ﬁnite set of formulas, say Γ = {B1, . . . , Bk}. Secondly, note that
Γ ⊨A implies that B1 ⊃B2 ⊃· · · ⊃A is a tautology. So, by part (1), the latter
formula has an F -proof, and by k additional modus ponens inferences, Γ ⊢A. (To
simplify notation, we write ⊢instead of
F .)
It remains to prove part (1). We begin by establishing a series of special cases,
(a)–(k), of the completeness theorem, in order to “bootstrap” the propositional
system F . We use symbols φ, ψ, χ for arbitrary formulas and Π to represent any
set of formulas.
(a) ⊢φ ⊃φ.
Proof: Combine the three axioms (φ ⊃φ ⊃φ) ⊃(φ ⊃(φ ⊃φ) ⊃φ) ⊃(φ ⊃φ),
φ ⊃(φ ⊃φ) and φ ⊃(φ ⊃φ) ⊃φ with two uses of modus ponens.
(b) Deduction Theorem: Γ, φ ⊢ψ if and only if Γ ⊢φ ⊃ψ.
Proof: The reverse implication is trivial. To prove the forward implication, suppose
C1, C2, . . . , Ck is an F -proof of ψ from Γ, φ. This means that Ck is ψ and that each

Introduction to Proof Theory
7
Ci is φ, is in Γ, is an axiom, or is inferred by modus ponens. It is straightforward to
prove, by induction on i, that Γ ⊢φ ⊃Ci for each Ci.
(c) φ ⊃ψ ⊢¬ψ ⊃¬φ.
Proof: By the deduction theorem, it suﬃces to prove that φ ⊃ψ, ¬ψ ⊢¬φ. To prove
this, use the two axioms ¬ψ ⊃(φ ⊃¬ψ) and (φ ⊃ψ) ⊃(φ ⊃¬ψ) ⊃¬φ and three
uses of modus ponens.
(d) φ, ¬φ ⊢ψ.
Proof: From the axiom φ ⊃(¬ψ ⊃φ), we have φ ⊢¬ψ ⊃φ. Thus, by (c) we get
φ ⊢¬φ ⊃¬¬ψ, and by (b), φ, ¬φ ⊢¬¬ψ. Finally modus ponens with the axiom
¬¬ψ ⊃ψ gives the desired result.
(e) ¬φ ⊢φ ⊃ψ and ψ ⊢φ ⊃ψ.
Proof: This former follows from (d) and the deduction theorem, and the latter follows
from the axiom ψ ⊃(φ ⊃ψ).
(f) φ, ¬ψ ⊢¬(φ ⊃ψ).
Proof: It suﬃces to prove φ ⊢¬ψ ⊃¬(φ ⊃ψ). Thus, by (c) and the deduction
theorem, it suﬃces to prove φ, φ ⊃ψ ⊢ψ. The latter assertion is immediate from
modus ponens.
(g) φ, ψ ⊢φ ∧ψ.
Proof: Two uses of modus ponens with the axiom φ ⊃ψ ⊃(φ ∧ψ).
(h) ¬φ ⊢¬(φ ∧ψ) and ¬ψ ⊢¬(φ ∧ψ).
Proof: For the ﬁrst part, it suﬃces to show ⊢¬φ ⊃¬(φ ∧ψ), and thus, by (c), it
suﬃces to show ⊢(φ ∧ψ) ⊃φ, which is an axiom. The proof that ¬ψ ⊢¬(φ ∧ψ) is
similar.
(i) φ ⊢φ ∨ψ and ψ ⊢φ ∨ψ.
Proof: φ ⊃(φ ∨ψ) and ψ ⊃(φ ∨ψ) are axioms.
(j) ¬φ, ¬ψ ⊢¬(φ ∨ψ).
Proof: It suﬃces to prove ¬φ ⊢¬ψ ⊃¬(φ ∨ψ), and this, by (c), follows from
¬φ ⊢(φ ∨ψ) ⊃ψ. For this, we combine
(i) ¬φ ⊢φ ⊃ψ,
by (e),
(ii) ⊢ψ ⊃ψ,
by (a),
(iii) ⊢(φ ⊃ψ) ⊃(ψ ⊃ψ) ⊃((φ ∨ψ) ⊃ψ),
an axiom,
with two uses of modus ponens.
(k) φ ⊢¬¬φ.
Proof: By (d), φ, ¬φ ⊢¬¬φ, and obviously, φ, ¬¬φ ⊢¬¬φ. So φ ⊢¬¬φ follows
from the next lemma.
1.1.4.1. Lemma.
If Γ, φ ⊢ψ and Γ, ¬φ ⊢ψ, then Γ ⊢ψ.

8
S. Buss
Proof. By (b) and (c), the two hypotheses imply that Γ ⊢¬ψ ⊃¬φ and Γ ⊢¬ψ ⊃
¬¬φ. These plus the two axioms (¬ψ ⊃¬φ) ⊃(¬ψ ⊃¬¬φ) ⊃¬¬ψ and ¬¬ψ ⊃ψ
give Γ ⊢ψ. 2
1.1.4.2. Lemma.
Let the formula A involve only the propositional variables among
p1, . . . , pn. For 1 ≤i ≤n, suppose that Bi is either pi or ¬pi. Then, either
B1, . . . , Bn ⊢A
or
B1, . . . , Bn ⊢¬A.
Proof. Deﬁne τ to be a truth assignment that makes each Bi true. By the soundness
theorem, A (respectively, ¬A), can be proved from the hypotheses B1, . . . , Bn only
if τ(A) = T (respectively τ(A) = F ). Lemma 1.1.4.2 asserts that the converse holds
too.
The lemma is proved by induction on the complexity of A. In the base case,
A is just pi: this case is trivial to prove since Bi is either pi or ¬pi. Now suppose
A is a formula A1 ∨A2. If σ(A) = T , then we must have τ(Ai) = T for some
i ∈{1, 2}; the induction hypothesis implies that B1, . . . , Bn ⊢Ai and thus, by (i)
above, B1, . . . , Bn ⊢A. On the other hand, if τ(A) = F , then τ(A1) = τ(A2) = F ,
so the induction hypothesis implies that B1, . . . , Bn ⊢¬Ai for both i = 1 and i = 2.
From this, (j) implies that B1, . . . , Bn ⊢¬A. The cases where A has outermost
connective ∧, ⊃or ¬ are proved similarly. 2.
We are now ready to complete the proof of the Completeness Theorem 1.1.3.
Suppose A is a tautology. We claim that Lemma 1.1.4.2 can be strengthened to have
B1, . . . , Bk ⊢A
where, as before each Bi is either pi or ¬pi, but now 0 ≤k ≤n is permitted.
We prove this by induction on k = n, n −1, . . . , 1, 0.
For k = n, this is just
Lemma 1.1.4.2.
For the induction step, note that B1, . . . , Bk ⊢A follows from
B1, . . . , Bk, pk+1 ⊢A and B1, . . . , Bk, ¬pk+1 ⊢A by Lemma 1.1.4.1. When k = 0,
we have that ⊢A, which proves the Completeness Theorem.
Q.E.D. Theorem 1.1.3
1.1.5.
It still remains to prove the compactness theorem for propositional logic.
This theorem states:
Compactness Theorem. Let Γ be a set of propositional formulas.
(1) Γ is satisﬁable if and only if every ﬁnite subset of Γ is satisﬁable.
(2) Γ ⊨A if and only if there is a ﬁnite subset Γ0 of Γ such that Γ0 ⊨A.
Since Γ ⊨A is equivalent to Γ ∪{¬A} being unsatisﬁable, (2) is implied by (1). It is
fairly easy to prove the compactness theorem directly, and most introductory books
in mathematical logic present such a proof. Here, we shall instead, give a proof based
on the Tychonoﬀtheorem; obviously this connection to topology is the reason for the
name ‘compactness theorem.’

Introduction to Proof Theory
9
Proof. Let V be the set of propositional variables used in Γ; the sets Γ and V
need not necessarily be countable. Let 2V denote the set of truth assignments on V
and endow 2V with the product topology by viewing it as the product of |V | copies
of the two element space with the discrete topology. That is to say, the subbasis
elements of 2V are the sets Bp,i = {τ : τ(p) = i} for p ∈V and i ∈{T, F}. Note
that these subbasis elements are both open and closed. Recall that the Tychonoﬀ
theorem states that an arbitrary product of compact spaces is compact; in particular,
2V is compact. (See Munkres [1975] for background material on topology.)
For φ ∈Γ, deﬁne Dφ = {τ ∈2V : τ ⊨φ}. Since φ only involves ﬁnitely many
variables, each Dφ is both open and closed. Now Γ is satisﬁable if and only if ∩φ∈ΓDφ
is non-empty. By the compactness of 2V , the latter condition is equivalent to the
sets ∩φ∈Γ0Dφ being non-empty for all ﬁnite Γ0 ⊂Γ. This, in turn is equivalent to
each ﬁnite subset Γ0 of Γ being satisﬁable. 2
The compactness theorem for ﬁrst-order logic is more diﬃcult; a purely model-
theoretic proof can be given with ultraﬁlters (see, e.g., Eklof [1977]). We include a
proof-theoretic proof of the compactness theorem for ﬁrst-order logic for countable
languages in section 2.3.7 below.
1.1.6. Remarks.
There are of course a large number of possible ways to give
sound and complete proof systems for propositional logic.
The particular proof
system F used above is adapted from Kleene [1952]. A more detailed proof of the
completeness theorem for F and for related systems can be found in the textbook
of Mendelson [1987]. The system F is an example of a class of proof systems called
Frege proof systems: a Frege proof system is any proof system in which all axioms and
rules are schematic and which is implicationally sound and implicationally complete.
Most of the commonly used proof systems similar to F are based on modus ponens
as the only rule of inference; however, some (non-Frege) systems also incorporate a
version of the deduction theorem as a rule of inference. In these systems, if B has
been inferred from A, then the formula A ⊃B may also be inferred. An example
of such a system is the propositional fragment of the natural deduction proof system
described in section 2.4.8 below.
Other rules of inference that are commonly allowed in propositional proof systems
include the substitution rule which allows any instance of φ to be inferred from φ, and
the extension rule which permits the introduction of abbreviations for long formulas.
These two systems appear to be more powerful than Frege systems in that they seem
to allow substantially shorter proofs of certain tautologies. However, whether they
actually are signiﬁcantly more powerful than Frege systems is an open problem. This
issues are discussed more fully by Pudl´ak in Chapter VIII.
There are several currently active areas of research in the proof theory of propo-
sitional logic. Of course, the central open problem is the P versus NP question of
whether there exists a polynomial time method of recognizing tautologies. Research
on the proof theory of propositional logic can be, roughly speaking, separated into
three problem areas.
Firstly, the problem of “proof-search” is the question of

10
S. Buss
what are the best algorithmic methods for searching for propositional proofs. The
proof-search problem is important for artiﬁcial intelligence, for automated theorem
proving and for logic programming. The most common propositional proof systems
used for proof-search algorithms are variations of the resolution system discussed
in 1.3 below. A second, related research area is the question of proof lengths. In
this area, the central questions concern the minimum lengths of proofs needed for
tautologies in particular proof systems.
This topic is treated in more depth in
Chapter VIII in this volume.
A third research area concerns the investigation of fragments of the propositional
proof system F .
For example, propositional intuitionist logic is the logic which
is axiomatized by the system F without the axiom scheme ¬¬A ⊃A. Another
important example is linear logic. Brief discussions of these two logics can be found
in section 3.
1.2. The propositional sequent calculus
The sequent calculus, ﬁrst introduced by Gentzen [1935] as an extension of his earlier
natural deduction proof systems, is arguably the most elegant and ﬂexible system for
writing proofs. In this section, the propositional sequent calculus for classical logic
is developed; the extension to ﬁrst-order logic is treated in 2.3 below.
1.2.1. Sequents and Cedents.
In the Hilbert-style systems, each line in a proof
is a formula; however, in sequent calculus proofs, each line in a proof is a sequent: a
sequent is written in the form
A1, . . . , Ak→B1, . . . , Bℓ
where the symbol →is a new symbol called the sequent arrow (not to be confused
with the implication symbol ⊃) and where each Ai and Bj is a formula. The intuitive
meaning of the sequent is that the conjunction of the Ai’s implies the disjunction of
the Bj ’s. Thus, a sequent is equivalent in meaning to the formula
k^
i=1
Ai ⊃
ℓ_
j=1
Bj.
The symbols
^
and
_
represent conjunctions and disjunctions, respectively, of
multiple formulas. We adopt the convention that an empty conjunction (say, when
k = 0 above) has value “True”, and that an empty disjunction (say, when ℓ= 0
above) has value “False”. Thus the sequent →A has the same meaning as the
formula A, and the empty sequent →is false. A sequent is deﬁned to be valid or a
tautology if and only if its corresponding formula is.
The sequence of formulas A1, . . . , Ak is called the antecedent of the sequent
displayed above; B1, . . . , Bℓis called its succedent . They are both referred to as
cedents.

Introduction to Proof Theory
11
1.2.2. Inferences and proofs.
We now deﬁne the propositional sequent calculus
proof system PK. A sequent calculus proof consists of a rooted tree (or sometimes a
directed acyclic graph) in which the nodes are sequents. The root of the tree, written
at the bottom, is called the endsequent and is the sequent proved by the proof. The
leaves, at the top of the tree, are called initial sequents or axioms. Usually, the only
initial sequents allowed are the logical axioms of the form A→A, where we further
require that A be atomic.
Other than the initial sequents, each sequent in a PK-proof must be inferred by
one of the rules of inference given below. A rule of inference is denoted by a ﬁgure
S1
S or
S1
S2
S
indicating that the sequent S may be inferred from S1 or from the
pair S1 and S2. The conclusion, S, is called the lower sequent of the inference; each
hypotheses is an upper sequent of the inference. The valid rules of inference for PK
are as follows; they are essentially schematic, in that A and B denote arbitrary
formulas and Γ, ∆, etc. denote arbitrary cedents.
Weak Structural Rules
Exchange:left
Γ, A, B, Π→∆
Γ, B, A, Π→∆
Exchange:right
Γ→∆, A, B, Λ
Γ→∆, B, A, Λ
Contraction:left
A, A, Γ→∆
A, Γ→∆
Contraction:right Γ→∆, A, A
Γ→∆, A
Weakening:left
Γ→∆
A, Γ→∆
Weakening:right
Γ→∆
Γ→∆, A
The weak structural rules are also referred to as just weak inference rules. The rest
of the rules are called strong inference rules. The structural rules consist of the weak
structural rules and the cut rule.
The Cut Rule
Γ→∆, A
A, Γ→∆
Γ→∆
The Propositional Rules1
¬:left
Γ→∆, A
¬A, Γ→∆
¬:right
A, Γ→∆
Γ→∆, ¬A
∧:left
A, B, Γ→∆
A ∧B, Γ→∆
∧:right Γ→∆, A
Γ→∆, B
Γ→∆, A ∧B
∨:left A, Γ→∆
B, Γ→∆
A ∨B, Γ→∆
∨:right
Γ→∆, A, B
Γ→∆, A ∨B
⊃:left Γ→∆, A
B, Γ→∆
A ⊃B, Γ→∆
⊃:right
A, Γ→∆, B
Γ→∆, A ⊃B
1 We have stated the ∧:left and the ∨:right rules diﬀerently than the traditional form. The
traditional deﬁnitions use the following two ∨:right rules of inference

12
S. Buss
The above completes the deﬁnition of PK. We write PK ⊢Γ→∆to denote that
the sequent Γ→∆has a PK-proof. When A is a formula, we write PK ⊢A to
mean that PK ⊢→A.
The cut rule plays a special role in the sequent calculus, since, as is shown in
section 1.2.8, the system PK is complete even without the cut rule; however, the use
of the cut rule can signiﬁcantly shorten proofs. A proof is said to be cut-free if does
not contain any cut inferences.
1.2.3. Ancestors, descendents and the subformula property.
All of the
inferences of PK, with the exception of the cut rule, have a principal formula which
is, by deﬁnition, the formula occurring in the lower sequent of the inference which is
not in the cedents Γ or ∆(or Π or Λ). The exchange inferences have two principal
formulas. Every inference, except weakenings, has one or more auxiliary formulas
which are the formulas A and B, occurring in the upper sequent(s) of the inference.
The formulas which occur in the cedents Γ, ∆, Π or Λ are called side formulas of the
inference. The two auxiliary formulas of a cut inference are called the cut formulas.
We now deﬁne the notions of descendents and ancestors of formulas occurring in
a sequent calculus proof. First we deﬁne immediate descendents as follows: If C is
a side formula in an upper sequent of an inference, say C is the i-th subformula of
a cedent Γ, Π, ∆or Λ, then C ’s only immediate descendent is the corresponding
occurrence of the same formula in the same position in the same cedent in the
lower sequent of the inference. If C is an auxiliary formula of any inference except an
exchange or cut inference, then the principal formula of the inference is the immediate
descendent of C . For an exchange inference, the immediate descendent of the A
or B in the upper sequent is the A or B, respectively, in the lower sequent. The
cut formulas of a cut inference do not have immediate descendents. We say that
C is an immediate ancestor of D if and only if D is an immediate descendent of C .
Note that the only formulas in a proof that do not have immediate ancestors are the
formulas in initial sequents and the principal formulas of weakening inferences.
The ancestor relation is deﬁned to be the reﬂexive, transitive closure of the
immediate ancestor relation; thus, C is an ancestor of D if and only if there is a
chain of zero or more immediate ancestors from D to C . A direct ancestor of D is an
ancestor C of D such that C is the same formula as D. The concepts of descendent
and direct descendent are deﬁned similarly as the converses of the ancestor and direct
ancestor relations.
A simple, but important, observation is that if C is an ancestor of D, then C is
a subformula of D. This immediately gives the following subformula property:
Γ→∆, A
Γ→∆, A ∨B
and
Γ→∆, A
Γ→∆, B ∨A
and two dual rules of inference for ∧:left. Our method has the advantage of reducing the number
of rules of inference, and also simplifying somewhat the upper bounds on cut-free proof length we
obtain below.

Introduction to Proof Theory
13
1.2.4. Proposition.
(The Subformula Property) If P is a cut-free PK-proof, then
every formula occurring in P is a subformula of a formula in the endsequent of P .
1.2.5. Lengths of proofs.
There are a number of ways to measure the length
of a sequent calculus proof P ; most notably, one can measure either the number of
symbols or the number of sequents occurring in P . Furthermore, one can require
P to be tree-like or to be dag-like; in the case of dag-like proofs no sequent needs to
be derived, or counted, twice. (‘Dag’ abbreviates ‘directed acyclic graph’, another
name for such proofs is ‘sequence-like’.)
For this chapter, we adopt the following conventions for measuring lengths of
sequent calculus proofs:
proofs are always presumed to be tree-like, unless we
explicitly state otherwise, and we let ||P|| denote the number of strong inferences
in a tree-like proof P . The value ||P|| is polynomially related to the number of
sequents in P . If P has n sequents, then, of course, ||P|| < n. On the other hand,
it is not hard to prove that for any tree-like proof P of a sequent Γ→∆, there is
a (still tree-like) proof of an endsequent Γ′→∆′ with at most ||P||2 sequents and
with Γ′ ⊆Γ and ∆′ ⊆∆. The reason we use ||P|| instead of merely counting the
actual number of sequents in P , is that using ||P|| often makes bounds on proof size
signiﬁcantly more elegant to state and prove.
Occasionally, we use ||P||dag to denote the number of strong inferences in a
dag-like proof P .
1.2.6. Soundness Theorem.
The propositional sequent calculus PK is sound.
That is to say, any PK-provable sequent or formula is a tautology.
The soundness theorem is proved by observing that the rules of inference of PK
preserve the property of sequents being tautologies.
The implicational form of the soundness theorem also holds. If S is a set of
sequents, let an S-proof be any sequent calculus proof in which sequents from S are
permitted as initial sequents (in addition to the logical axioms). The implicational
soundness theorem states that if a sequent Γ→∆has an S-proof, then Γ→∆is
made true by every truth assignment which satisﬁes S.
1.2.7. The inversion theorem.
The inversion theorem is a kind of inverse to
the implicational soundness theorem, since it says that, for any inference except
weakening inferences, if the conclusion of the inference is valid, then so are all of its
hypotheses.
Theorem. Let I be a propositional inference, a cut inference, an exchange inference
or a contraction inference. If I ’s lower sequent is valid, then so are are all of I ’s
upper sequents. Likewise, if I ’s lower sequent is true under a truth assignment τ ,
then so are are all of I ’s upper sequents.
The inversion theorem is easily proved by checking the eight propositional inference
rules; it is obvious for exchange and contraction inferences.

14
S. Buss
Note that the inversion theorem can fail for weakening inferences. Most authors
deﬁne the ∧:left and ∨:right rules of inference diﬀerently than we deﬁned them
for PK, and the inversion theorem can fail for these alternative formulations (see the
footnote on page 11).
1.2.8. The completeness theorem.
The completeness theorem for PK states
that every valid sequent (tautology) can be proved in the propositional sequent
calculus. This, together with the soundness theorem, shows that the PK-provable
sequents are precisely the valid sequents.
Theorem. If Γ→∆is a tautology, then it has a PK-proof in which no cuts appear.
1.2.9.
In order to prove Theorem 1.2.8 we prove the following stronger lemma
which includes bounds on the size of the PK-proof.
Lemma. Let Γ→∆be a valid sequent in which there are m occurrences of logical
connectives. Then there is a tree-like, cut free PK-proof P of Γ→∆containing
fewer than 2 m strong inferences.
Proof. The proof is by induction on m. In the base case, m = 0, the sequent
Γ→∆contains no logical connectives and thus every formula in the sequent is a
propositional variable. Since the sequent is valid, there must be some variable, p,
which occurs both in Γ and in ∆. Thus Γ→∆can be proved with zero strong
inferences from the initial sequent p→p.
The induction step, m > 0, is handled by cases according to which connectives
are used as outermost connectives of formulas in the cedents Γ and ∆. First suppose
there is a formula of the form (¬A) in Γ. Letting Γ′ be the cedent obtained from Γ
by removing occurrences of ¬A, we can infer Γ→∆by:
Γ′→∆, A
¬A, Γ′→∆
Γ→∆
where the double line indicates a series of weak inferences. By the inversion theorem,
Γ′→∆, A is valid, and hence, since it has at most m −1 logical connectives, the
induction hypothesis implies that it has a cut-free proof with fewer than 2m−1 strong
inferences. This gives Γ→∆a cut-free proof with fewer than 2m−1 +1 ≤2 m strong
inferences. The case where a formula (¬A) occurs in ∆is handled similarly.
Second, consider the case where a formula of the form A∧B appears in Γ. Then,
letting Γ′ be the cedent Γ minus the formula A ∧B, we can infer Γ→∆by:
A, B, Γ′→∆
A ∧B, Γ′→∆
Γ→∆
By the inversion theorem and the induction hypothesis, A, B, Γ′→∆has a cut-free
proof with fewer than 2m−1 strong inferences. Thus Γ→∆has a cut-free proof with
fewer than 2 m strong inferences. Third, suppose there is a formula of the A ∧B

Introduction to Proof Theory
15
appearing in the succedent ∆. Letting ∆′ be the the succedent ∆minus the formula
A ∧B, we can infer
Γ→∆′, A
Γ→∆′, B
Γ→∆′, A ∧B
Γ→∆
By the inversion theorem, both of upper sequents above are valid. Furthermore, they
each have fewer than m logical connectives, so by the induction hypothesis, they
have cut-free proofs with fewer than 2m−1 strong inferences. This gives the sequent
Γ→∆a cut-free proof with fewer than 2 m strong inferences.
The remaining cases are when a formula in the sequent Γ→∆has outermost
connective ∨or ⊃. These are handled with the inversion theorem and the induction
hypothesis similarly to the above cases. 2
1.2.10.
The bounds on the proof size in Lemma 1.2.9 can be improved somewhat
by counting only the occurrences of distinct subformulas in Γ→∆.
To make
this precise, we need to deﬁne the concepts of positively and negatively occurring
subformulas. Given a formula A, an occurrence of a subformula B of A, and a
occurrence of a logical connective α in A, we say that B is negatively bound by α if
either (1) α is a negation sign, ¬, and B is in its scope, or (2) α is an implication sign,
⊃, and B is a subformula of its ﬁrst argument. Then, B is said to occur negatively
(respectively, positively) in A if B is negatively bound by an odd (respectively, even)
number of connectives in A. A subformula occurring in a sequent Γ→∆is said to
be positively occurring if it occurs positively in ∆or negatively in Γ; otherwise, it
occurs negatively in the sequent.
Lemma. Let Γ→∆be a valid sequent.
Let m′ equal the number of distinct
subformulas occurring positively in the sequent and m′′ equal the number of distinct
subformulas occurring negatively in the sequent. Let m = m′ + m′′. Then there is a
tree-like, cut free PK-proof P containing fewer than 2 m strong inferences.
Proof. (Sketch) Recall that the proof of Lemma 1.2.9 built a proof from the bottom-
up, by choosing a formula in the endsequent to eliminate (i.e., to be inferred) and
thereby reducing the total number of logical connectives and then appealing to the
induction hypothesis. The construction for the proof of the present lemma is exactly
the same, except that now care must be taken to reduce the total number of distinct
positively or negatively occurring subformulas, instead of just reducing the total
number of connectives. This is easily accomplished by always choosing a formula
from the endsequent which contains a maximal number of connectives and which is
therefore not a proper subformula of any other subformula in the endsequent. 2
1.2.11.
The cut elimination theorem states that if a sequent has a PK-proof,
then it has a cut-free proof. This is an immediate consequence of the soundness
and completeness theorems, since any PK-provable sequent must be valid, by the
soundness theorem, and hence has a cut-free proof, by the completeness theorem.

16
S. Buss
This is a rather slick method of proving the cut elimination theorem, but unfor-
tunately, does not shed any light on how a given PK-proof can be constructively
transformed into a cut-free proof. In section 2.3.7 below, we shall give a step-by-step
procedure for converting ﬁrst-order sequent calculus proofs into cut-free proofs; the
same methods work also for propositional sequent calculus proofs. We shall not,
however, describe this constructive proof transformation procedure here; instead, we
will only state, without proof, the following upper bound on the increase in proof
length which can occur when a proof is transformed into a cut-free proof. (A proof
can be given using the methods of Sections 2.4.2 and 2.4.3.)
Cut-Elimination Theorem. Suppose P be a (possibly dag-like) PK-proof of
Γ→∆.
Then Γ→∆has a cut-free, tree-like PK-proof with less than or equal
to 2||P||dag strong inferences.
1.2.12. Free-cut elimination.
Let S be a set of sequents and, as above, deﬁne
an S-proof to be a sequent calculus proof which may contain sequents from S as
initial sequents, in addition to the logical sequents. If I is a cut inference occurring
in an S-proof P , then we say I ’s cut formulas are directly descended from S if they
have at least one direct ancestor which occurs as a formula in an initial sequent which
is in S. A cut I is said to be free if neither of I ’s auxiliary formulas is directly
descended from S. A proof is free-cut free if and only if it contains no free cuts. (See
Deﬁnition 2.4.4.1 for a better deﬁnition of free cuts.) The cut elimination theorem
can be generalized to show that the free-cut free fragment of the sequent calculus is
implicationally complete:
Free-cut Elimination Theorem. Let S be a sequent and S a set of sequents. If
S ⊨S, then there is a free-cut free S-proof of S.
We shall not prove this theorem here; instead, we prove the generalization of this
for ﬁrst-order logic in 2.4.4 below. This theorem is essentially due to Takeuti [1987]
based on the cut elimination method of Gentzen [1935].
1.2.13. Some remarks.
We have developed the sequent calculus only for classical
propositional logic; however, one of the advantages of the sequent calculus is its
ﬂexibility in being adapted for non-classical logics.
For instance, propositional
intuitionistic logic can be formalized by a sequent calculus PJ which is deﬁned
exactly like PK except that succedents in the lower sequents of strong inferences
are restricted to contain at most one formula. As another example, minimal logic
is formalized like PJ , except with the restriction that every succedent contain
exactly one formula. Linear logic, relevant logic, modal logics and others can also be
formulated elegantly with the sequent calculus.
1.2.14. The Tait calculus.
Tait [1968] gave a proof system similar in spirit to the
sequent calculus. Tait’s proof system incorporates a number of simpliﬁcations with
regard to the sequent calculus; namely, it uses sets of formulas in place of sequents,

Introduction to Proof Theory
17
it allows only propositional formulas to be negated, and there are no weak structural
rules at all.
Since the Tait calculus is often used for analyzing fragments of arithmetic, es-
pecially in the framework of inﬁnitary logic, we brieﬂy describe it here. Formulas
are built up from atomic formulas p, from negated atomic formulas ¬p, and with
the connectives ∧and ∨.
A negated atomic formula is usually denoted p; and
the negation operator is extended to all formulas by deﬁning p to be just p, and
inductively deﬁning A ∨B and A ∧B to be A ∧B and A ∨B, respectively.
Frequently, the Tait calculus is used for inﬁnitary logic. In this case, formulas
are deﬁned so that whenever Γ is a set of formulas, then so are
_
Γ and
^
Γ. The
intended meaning of these formulas is the disjunction, or conjunction, respectively,
of all the formulas in Γ.
Each line in a Tait calculus proof is a set Γ of formulas with the intended meaning
of Γ being the disjunction of the formulas in Γ. A Tait calculus proof can be tree-like
or dag-like. The initial sets, or logical axioms, of a proof are sets of the form Γ∪{p, p}.
In the inﬁnitary setting, there are three rules of inference; namely,
Γ ∪{Aj}
Γ ∪{
_
i∈IAi}
where j ∈I,
Γ ∪{Aj : j ∈I}
Γ ∪{
^
j∈IAj}
(there are |I| many hypotheses), and
Γ ∪{A}
Γ ∪{A}
Γ
the cut rule.
In the ﬁnitary setting, the same rules of inference may also be used. It is evident
that the Tait calculus is practically isomorphic to the sequent calculus.
This is
because a sequent Γ→∆may be transformed into the equivalent set of formulas
containing the formulas from ∆and the negations of the formulas from Γ. The
exchange and contraction rules are superﬂuous once one works with sets of formulas,
the weakening rule of the sequent calculus is replaced by allowing axioms to contain
extra side formulas (this, in essence, means that weakenings are pushed up to the
initial sequents of the proof). The strong rules of inference for the sequent calculus
translate, by this means, to the rules of the Tait calculus.
Recall that we adopted the convention that the length of a sequent calculus proof
is equal to the number of strong inferences in the proof. When we work with tree-like
proofs, this corresponds exactly to the number of inferences in the corresponding
Tait-style proof.
The cut elimination theorem for the (ﬁnitary) sequent calculus immediately
implies the cut elimination theorem for the Tait calculus for ﬁnitary logic; this
is commonly called the normalization theorem for Tait-style systems. For general
inﬁnitary logics, the cut elimination/normalization theorems may not hold; however,
Lopez-Escobar [1965] has shown that the cut elimination theorem does hold for
inﬁnitary logic with formulas of countably inﬁnite length. Also, Chapters III and IV

18
S. Buss
of this volume discuss cut elimination in some inﬁnitary logics corresponding to
theories of arithmetic.
1.3. Propositional resolution refutations
The Hilbert-style and sequent calculus proof systems described earlier are quite
powerful; however, they have the disadvantage that it has so far proved to be
very diﬃcult to implement computerized procedures to search for propositional
Hilbert-style or sequent calculus proofs. Typically, a computerized procedure for
proof search will start with a formula A for which a proof is desired, and will
then construct possible proofs of A by working backwards from the conclusion A
towards initial axioms.
When cut-free proofs are being constructed this is fairly
straightforward, but cut-free proofs may be much longer than necessary and may
even be too long to be feasibly constructed by a computer. General, non-cut-free,
proofs may be quite short; however, the diﬃculty with proof search arises from the
need to determine what formulas make suitable cut formulas. For example, when
trying to construct a proof of Γ→∆that ends with a cut inference; one has to
consider all formulas C and try to construct proofs of the sequents Γ→∆, C and
C, Γ→∆. In practice, it has been impossible to choose cut formulas C eﬀectively
enough to eﬀectively generate general proofs. A similar diﬃculty arises in trying to
construct Hilbert-style proofs which must end with a modus ponens inference.
Thus to have a propositional proof system which would be amenable to comput-
erized proof search, it is desirable to have a proof system in which (1) proof search
is eﬃcient and does not require too many ‘arbitrary’ choices, and (2) proof lengths
are not excessively long. Of course, the latter requirement is intended to reﬂect the
amount of available computer memory and time; thus proofs of many millions of
steps might well be acceptable. Indeed, for computerized proof search, having an
easy-to-ﬁnd proof which is millions of steps long may well be preferable to having a
hard-to-ﬁnd proof which has only hundreds of steps.
The principal propositional proof system which meets the above requirements is
based on resolution. As we shall see, the expressive power and implicational power of
resolution is weaker than that of the full propositional logic; in particular, resolution
is, in essence, restricted to formulas in conjunctive normal form. However, resolution
has the advantage of being amenable to eﬃcient proof search.
Propositional and ﬁrst-order resolution were introduced in the inﬂuential work
of Robinson [1965b] and Davis and Putnam [1960]. Propositional resolution proof
systems are discussed immediately below. A large part of the importance of proposi-
tional resolution lies in the fact that it leads to eﬃcient proof methods in ﬁrst-order
logic: ﬁrst-order resolution is discussed in section 2.6 below.
1.3.1. Deﬁnition.
A literal is deﬁned to be either a propositional variable pi or
the negation of a propositional variable ¬pi. The literal ¬pi is also denoted pi; and if
x is the literal pi, then x denotes the unnegated literal pi. The literal x is called the

Introduction to Proof Theory
19
complement of x. A positive literal is one which is an unnegated variable; a negative
literal is one which is a negated variable.
A clause C is a ﬁnite set of literals. The intended meaning of C is the disjunction
of its members; thus, for σ a truth assignment, σ(C) equals True if and only if
σ(x) =True for some x ∈C . Note that the empty clause, ∅, always has value False.
Since clauses that contain both x and x are always true, it is often assumed w.l.o.g.
that no clause contains both x and x. A clause is deﬁned to be positive (respectively,
negative) if it contains only positive (resp., only negative) literals. The empty clause
is the only clause which is both positive and negative. A clause which is neither
positive nor negative is said to be mixed.
A non-empty set Γ of clauses is used to represent the conjunction of its members.
Thus σ(Γ) is True if and only if σ(C) is True for all C ∈Γ. Obviously, the meaning
of Γ is the same as the conjunctive normal form formula consisting of the conjunction
of the disjunctions of the clauses in Γ. A set of clauses is said to be satisﬁable if there
is at least one truth assignment that makes it true.
Resolution proofs are used to prove that a set Γ of clauses is unsatisﬁable: this
is done by using the resolution rule (deﬁned below) to derive the empty clause
from Γ. Since the empty clause is unsatisﬁable this will be suﬃcient to show that Γ
is unsatisﬁable.
1.3.2. Deﬁnition.
Suppose that C and D are clauses and that x ∈C and x ∈D
are literals. The resolution rule applied to C and D is the inference
C
D
(C \ {x}) ∪(D \ {x})
The conclusion (C \{x})∪(D\{x}) is called the resolvent of C and D (with respect
to x).2
Since the resolvent of C and D is satisﬁed by any truth assignment that satisﬁes
both C and D, the resolution rule is sound in the following sense: if Γ is satisﬁable
and B is the resolvent of two clauses in Γ, then Γ ∪{B} is satisﬁable. Since the
empty clause is not satisﬁable, this yields the following deﬁnition of the resolution
proof system.
Deﬁnition. A resolution refutation of Γ is a sequence C1, C2, . . . , Ck of clauses such
that each Ci is either in Γ or is inferred from earlier member of the sequence by the
resolution rule, and such that Ck is the empty clause.
1.3.3.
Resolution is deﬁned to be a refutation procedure which refutes the satis-
ﬁability of a set of clauses, but it also functions as a proof procedure for proving
the validity of propositional formulas; namely, to prove a formula A, one forms a
set ΓA of clauses such that A is a tautology if and only if ΓA is unsatisﬁable. Then
2Note that x is uniquely determined by C and D if we adopt the (optional) convention that
clauses never contain any literal and its complement.

20
S. Buss
a resolution proof of A is, by deﬁnition, a resolution refutation of ΓA. There are two
principal means of forming ΓA from A. The ﬁrst method is to express the negation
of A in conjunctive normal form and let ΓA be the set of clauses which express that
conjunctive normal form formula. The principal drawback of this deﬁnition of ΓA is
that the conjunctive normal form of ¬A may be exponentially longer than A, and
ΓA may have exponentially many clauses.
The second method is the method of extension, introduced by Tsejtin [1968],
which involves introducing new propositional variables in addition to the variables
occurring in the formula A. The new propositional variables are called extension
variables and allow each distinct subformula B in A to be assigned a literal xB
by the following deﬁnition: (1) for propositional variables appearing in A, xp is p;
(2) for negated subformulas ¬B, x¬B is xB ; (3) for any other subformula B, xB is
a new propositional variable. We then deﬁne ΓA to contain the following clauses:
(1) the singleton clause {xA}, (2) for each subformula of the form B ∧C in A, the
clauses
{xB∧C, xB},
{xB∧C, xC}
and
{xB, xC, xB∧C};
and (3) for each subformula of the form B ∨C in A, the clauses
{xB∨C, xB},
{xB∨C, xC}
and
{xB, xC, xB∨C}.
If there are additional logical connectives in A then similar sets of clauses can be
used. It is easy to check that the set of three clauses for B ∧C (respectively, B ∨C )
is satisﬁed by exactly those truth assignments that assign xB∧C (respectively, xB∨C )
the same truth value as xB ∧xC (resp., xB ∨xC ). Thus, a truth assignment satisﬁes
ΓA only if it falsiﬁes A and, conversely, if a truth assignment falsiﬁes A then there
is a unique assignment of truth values to the extension variables of A which lets
it satisfy ΓA. The primary advantage of forming ΓA by the method of extension
is that ΓA is linear-size in the size of A. The disadvantage, of course, is that the
introduction of additional variables changes the meaning and structure of A.
1.3.4. Completeness Theorem for Propositional Resolution.
If Γ is an
unsatisﬁable set of clauses, then there is a resolution refutation of Γ.
Proof. We shall brieﬂy sketch two proofs of this theorem. The ﬁrst, and usual,
proof is based on the Davis-Putnam procedure. First note, that by the Compactness
Theorem we may assume w.l.o.g. that Γ is ﬁnite. Therefore, we may use induction on
the number of distinct variables appearing in Γ. The base case, where no variables
appear in Γ is trivial, since Γ must contain just the empty clause. For the induction
step, let p be a ﬁxed variable in Γ and deﬁne Γ′ to be the set of clauses containing
the following clauses:
a. For all clauses C and D in Γ such that p ∈C and p ∈D, the resolvent of
C and D w.r.t. p is in Γ′, and
b. Every clause C in Γ which contains neither p nor p is in Γ′.

Introduction to Proof Theory
21
Assuming, without loss of generality, that no clause in Γ contained both p and p, it
is clear that the variable p does not occur in Γ′. Now, it is not hard to show that Γ′
is satisﬁable if and only if Γ is, from whence the theorem follows by the induction
hypothesis.
The second proof reduces resolution to the free-cut free sequent calculus. For this,
if C is a clause, let ∆C be the cedent containing the variables which occur positively
in C and ΠC be the variables which occur negatively in C .
Then the sequent
ΠC→∆C is a sequent with no non-logical symbols which is identical in meaning
to C . For example, if C = {p1, p2, p3}, then the associated sequent is p2→p1, p3.
Clearly, if C and D are clauses with a resolvent E, then the sequent ΠE→∆E is
obtained from ΠC→∆C and ΠD→∆D with a single cut on the resolution variable.
Now suppose Γ is unsatisﬁable. By the completeness theorem for the free-cut free
sequent calculus, there is a free-cut free proof of the empty sequent from the sequents
ΠC→∆C with C ∈Γ. Since the proof is free-cut free and there are no non-logical
symbols appearing in any initial sequents, every cut formula in the proof must be
atomic.
Therefore no non-logical symbol appear anywhere in the proof and, by
identifying the sequents in the free-cut free proof with clauses and replacing each cut
inference with the corresponding resolution inference, a resolution refutation of the
empty clause is obtained. 2
1.3.5. Restricted forms of resolution.
One of the principal advantages of
resolution is that it is easier for computers to search for resolution refutations than
to search for arbitrary Hilbert-style or sequent calculus proofs. The reason for this
is that resolution proofs are less powerful and more restricted than Hilbert-style and
sequent calculus proofs and, in particular, there are fewer options on how to form
resolution proofs. This explains the paradoxical situation that a less-powerful proof
system can be preferable to a more powerful system. Thus it makes sense to consider
further restrictions on resolution which may reduce the proof search space even more.
Of course there is a tradeoﬀinvolved in using more restricted forms of resolution,
since one may ﬁnd that although restricted proofs are easier to search for, they are
a lot less plentiful. Often, however, the ease of proof search is more important than
the existence of short proofs; in fact, it is sometimes even preferable to use a proof
system which is not complete, provided its proofs are easy to ﬁnd.
Although we do not discuss this until section 2.6, the second main advantage of
resolution is that propositional refutations can be ‘lifted’ to ﬁrst-order refutations of
ﬁrst-order formulas. It is important that the restricted forms of resolution discussed
next also apply to ﬁrst-order resolution refutations.
One example of a restricted form of resolution is implicit in the ﬁrst proof of the
Completeness Theorem 1.3.4 based on the Davis-Putnam procedure; namely, for any
ordering of the variables p1, . . . , pm, it can be required that a resolution refutation
has ﬁrst resolutions with respect to p1, then resolutions with respect to p2, etc.,
concluding with resolutions with respect to pm.
This particular strategy is not
particularly useful since it does not reduce the search space suﬃciently. We consider
next several strategies that have been somewhat more useful.

22
S. Buss
1.3.5.1. Subsumption.
A clause C is said to subsume a clause D if and only if
C ⊆D. The subsumption principle states that if two clauses C and D have been
derived such that C subsumes D, then D should be discarded and not used further
in the refutation. The subsumption principle is supported by the following theorem:
Theorem. If Γ is unsatisﬁable and if C ⊂D, then Γ′ = (Γ \ {D}) ∪{C} is also
unsatisﬁable. Furthermore, Γ′ has a resolution refutation which is no longer than the
shortest refutation of Γ.
1.3.5.2. Positive resolution and hyperresolution.
Robinson [1965a] intro-
duced positive resolution and hyperresolution. A positive resolution inference is one
in which one of the hypotheses is a positive clause. The completeness of positive
resolution is shown by:
Theorem. (Robinson [1965a]) If Γ is unsatisﬁable, then Γ has a refutation contain-
ing only positive resolution inferences.
Proof. (Sketch) It will suﬃce to show that it is impossible for an unsatisﬁable Γ to
be closed under positive resolution inferences and not contain the empty clause. Let
A be the set of positive clauses in Γ; A must be non-empty since Γ is unsatisﬁable.
Pick a truth assignment τ that satisﬁes all clauses in A and assigns the minimum
possible number of “true” values. Pick a clause L in Γ \ A which is falsiﬁed by τ
and has the minimum number of negative literals; and let p be one of the negative
literals in L. Note that L exists since Γ is unsatisﬁable and that τ(p) = True. Pick
a clause J ∈A that contains p and has the rest of its members assigned false by τ ;
such a clause J exists by the choice of τ . Considering the resolvent of J and L, we
obtain a contradiction. 2
Positive resolution is nice in that it restricts the kinds of resolution refutations that
need to be attempted; however, it is particularly important as the basis for hyperres-
olution. The basic idea behind hyperresolution is that multiple positive resolution
inferences can be combined into a single inference with a positive conclusion. To
justify hyperresolution, note that if R is a positive resolution refutation then the
inferences in R can be uniquely partitioned into subproofs of the form
An
A3
A2
A1
B1
B2
B3
B4
...
Bn
An+1
where each of the clauses A1, . . . , An+1 are positive (and hence the clauses B1, . . . , Bn
are not positive). These n + 1 positive resolution inferences are combined into the
single hyperresolution inference

Introduction to Proof Theory
23
A1
A2
A3
· · ·
An
B1
An+1
(This construction is the deﬁnition of hyperresolution inferences.)
It follows immediately from the above theorem that hyperresolution is complete.
The importance of hyperresolution lies in the fact that one can search for refutations
containing only positive resolutions and that as clauses are derived, only the positive
clauses need to be saved for possible future use as hypotheses.
Negative resolution is deﬁned similarly to positive resolution and is likewise
complete.
1.3.5.3. Semantic resolution.
Semantic resolution, independently introduced
by Slagle [1967] and Luckham [1970], can be viewed as a generalization of positive
resolution. For semantic resolution, one uses a ﬁxed truth assignment (interpreta-
tion) τ to restrict the permissible resolution inferences. A resolution inference is said
to be τ -supported if one of its hypotheses is given value False by τ . Note that at most
one hypothesis can have value False, since the hypotheses contain complementary
occurrences of the resolvent variable.
A resolution refutation is said to be τ -supported if each of its resolution inferences
are τ -supported. If τF is the truth assignment which assigns every variable the value
False, then a τF -supported resolution refutation is deﬁnitionally the same as a
positive resolution refutation. Conversely, if Γ is a set of clauses and if τ is any truth
assignment, then one can form a set Γ′ by complementing every variable in Γ which
has τ -value True: clearly, a τ -supported resolution refutation of Γ is isomorphic to
a positive resolution refutation of Γ′. Thus, Theorem 1.3.5.2 is equivalent to the
following Completeness Theorem for semantic resolution:
Theorem. For any τ and Γ, Γ is unsatisﬁable if and only if Γ has a τ -supported
resolution refutation.
It is possible to deﬁne semantic-hyperresolution in terms of semantic resolution, just
as hyperresolution was deﬁned in terms of positive resolution.
1.3.5.4. Set-of-support resolution.
Wos, Robinson and Carson [1965] intro-
duced set-of-support resolution as another principle for guiding a search for resolution
refutations. Formally, set of support is deﬁned as follows: if Γ is a set of clauses and
if Π ⊂Γ and Γ\Π is satisﬁable, then Π is a set of support for Γ; a refutation R of Γ
is said to be supported by Π if every inference in R is derived (possibly indirectly)
from at least one clause in Π. (An alternative, almost equivalent, deﬁnition would
be to require that no two members of Γ \ Π are resolved together.) The intuitive
idea behind set of support resolution is that when trying to refute Γ, one should
concentrate on trying to derive a contradiction from the part Π of Γ which is not
known to be consistent. For example, Γ \ Π might be a database of facts which is
presumed to be consistent, and Π a clause which we are trying to refute.

24
S. Buss
Theorem. If Γ is unsatisﬁable and Π is a set of support for Γ, then Γ has a
refutation supported by Π.
This theorem is immediate from Theorem 1.3.5.3. Let τ be any truth assignment
which satisﬁes Γ \ Π, then a τ -supported refutation is also supported by Π.
The main advantage of set of support resolution over semantic resolution is that
it does not require knowing or using a satisfying assignment for Γ \ Π.
1.3.5.5. Unit and input resolution.
A unit clause is deﬁned to be a clause
containing a single literal; a unit resolution inference is an inference in at least one
of the hypotheses is a unit clause. As a general rule, it is desirable to perform unit
resolutions whenever possible. If Γ contains a unit clause {x}, then by combining
unit resolutions with the subsumption principle, one can remove from Γ every clause
which contains x and also every occurrence of x from the rest of the clauses in Γ.
(The situation is a little more diﬃcult when working in ﬁrst-order logic, however.)
This completely eliminates the literal x and reduces the number of and sizes of
clauses to consider.
A unit resolution refutation is a refutation which contains only unit resolutions.
Unfortunately, unit resolution is not complete: for example, an unsatisﬁable set Γ
with no unit clauses cannot have a unit resolution refutation.
An input resolution refutation of Γ is deﬁned to be a refutation of Γ in which every
resolution inference has at least one of its hypotheses in Γ. Obviously, a minimal
length input refutation will be tree-like. Input resolution is also not complete; in
fact, it can refute exactly the same sets as unit resolution:
Theorem. (Chang [1970]) A set of clauses has a unit refutation if and only if it has
a input refutation.
1.3.5.6. Linear resolution.
Linear resolution is a generalization of input resolu-
tion which has the advantage of being complete: a linear resolution refutation of Γ is
a refutation A1, A2, . . . , An−1, An = ∅such that each Ai is either in Γ or is obtained
by resolution from Ai−1 and Aj for some j < i −1. Thus a linear refutation has
the same linear structure as an input resolution, but is allowed to reuse intermediate
clauses which are not in Γ.
Theorem. (Loveland [1970] and Luckham [1970]) If Γ is unsatisﬁable, then Γ has
a linear resolution refutation.
Linear and input resolution both lend themselves well to depth-ﬁrst proof search
strategies. Linear resolution is still complete when used in conjunction with set-of-
support resolution.
Further reading. We have only covered some of the basic strategies for proposition
resolution proof search. The original paper of Robinson [1965b] still provides an
excellent introduction to resolution; this and many other foundational papers on

Introduction to Proof Theory
25
this topic have been reprinted in Siekmann and Wrightson [1983]. In addition, the
textbooks by Chang and Lee [1973], Loveland [1978], and Wos et al. [1992] give a
more complete description of various forms of resolution than we have given above.
Horn clauses. A Horn clause is a clause which contains at most one positive literal.
Thus a Horn clause must be of the form {p, q1, . . . , qn} or of the form {q1, . . . , qn}
with n ≥0. If a Horn clause is rewritten as sequents of atomic variables, it will
have at most one variable in the antecedent; typically, Horn clauses are written in
reverse-sequent format so, for example, the two Horn clauses above would be written
as implications
p ⇐q1, . . . , qn
and
⇐q1, . . . , qn.
In this reverse-sequent notation, the antecedent is written after the ⇐, and the
commas are interpreted as conjunctions (∧’s). Horn clauses are of particular interest
both because they are expressive enough to handle many situations and because
deciding the satisﬁability of sets of Horn clauses is more feasible than deciding the
satisﬁability of arbitrary sets of clauses. For these reasons, many logic programming
environments such as PROLOG are based partly on Horn clause logic.
In propositional logic, it is an easy matter to decide the satisﬁability of a set
of Horn clauses; the most straightforward method is to restrict oneself to positive
unit resolution. A positive unit inference is a resolution inference in which one of
the hypotheses is a unit clause containing a positive literal only. A positive unit
refutation is a refutation containing only positive unit resolution inferences.
Theorem. A set of Horn clauses is unsatisﬁable if and only if it has a positive unit
resolution refutation.
Proof. Let Γ be an unsatisﬁable set of Horn clauses. Γ must contain at least one
positive unit clause {p}, since otherwise the truth assignment that assigned False to
all variables would satisfy Γ. By resolving {p} against all clauses containing p, and
then discarding all clauses which contain p or p, one obtains a smaller unsatisﬁable
set of Horn clauses. Iterating this yields the desired positive unit refutation. 2
Positive unit resolutions are quite adequate in propositional logic, however, they
do not lift well to applications in ﬁrst-order logic and logic programming.
For
this, a more useful method of search for refutations is based on combining semantic
resolution, linear resolution and set-of-support resolution:
1.3.5.7. Theorem.
Henschen and Wos [1974]. Suppose Γ is an unsatisﬁable set
of Horn clauses with Π ⊆Γ a set of support for Γ, and suppose that every clause
in Γ \ Π contains a positive literal. Then Γ has a refutation which is simultaneously
a negative resolution refutation and a linear refutation and which is supported by Π.

26
S. Buss
Note that the condition that every clause in Γ \ Π contains a positive literal means
that the truth assignment τ that assigns True to every variable satisﬁes Γ \ Π. Thus
a negative resolution refutation is the same as a τ -supported refutation and hence is
supported by Π.
The theorem is fairly straightforward to prove, and we leave the details to the
reader. However, note that since every clause in Γ \ Π is presumed to contain a
positive literal, it is impossible to get rid of all positive literals only by resolving
against clauses in Γ \ Π. Therefore, Π must contain a negative clause C such that
there is a linear derivation that begins with C , always resolves against clauses in Γ\Π
yielding negative clauses only, and ending with the empty clause. The resolution
refutations of Theorem 1.3.5.7, or rather the lifting of these to Horn clauses described
in section 2.6.5, can be combined with restrictions on the order in which literals are
resolved to give what is commonly called SLD-resolution.
2. Proof theory of ﬁrst-order logic
2.1. Syntax and semantics
2.1.1. Syntax of ﬁrst-order logic.
First-order logic is a substantial extension
of propositional logic, and allows reasoning about individuals using functions and
predicates that act on individuals.
The symbols allowed in ﬁrst-order formulas
include the propositional connectives, quantiﬁers, variables, function symbols, con-
stant symbols and relation symbols.
We take ¬, ∧, ∨and ⊃as the allowed
propositional connectives.
There is an inﬁnite set of variable symbols; we use
x, y, z, . . . and a, b, c, . . . as metasymbols for variables.
The quantiﬁers are the
existential quantiﬁers, (∃x), and the universal quantiﬁers, (∀x), which mean “there
exists x” and “for all x”. A given ﬁrst-order language contains a set of function
symbols of speciﬁed arities, denoted by metasymbols f, g, h, . . . and a set of relation
symbols of speciﬁed arities, denoted by metasymbols P, Q, R, . . .. Function symbols
of arity zero are called constant symbols. Sometimes the ﬁrst-order language contains
a distinguished two-place relation symbol = for equality.
The formulas of ﬁrst-order logic are deﬁned as follows. Firstly, terms are built up
from function symbols, constant symbols and variables. Thus, any variable x is a
term, and if t1, . . . , tk are terms and f is k-ary, then f(t1, . . . , tk) is a term. Second,
atomic formulas are deﬁned to be of the form P(t1, . . . , tk) for P a k-ary relation
symbol. Finally, formulas are inductively to be built up from atomic formulas and
logical connectives; namely, any atomic formula is a formula, and if A and B are
formulas, then so are (¬A), (A∧B), (A∨B), (A ⊃B), (∀x)A and (∃x)A. To avoid
writing too many parentheses, we adopt the conventions on omitting parentheses in
propositional logic with the additional convention that quantiﬁers bind more tightly
than binary propositional connectives. In addition, binary predicate symbols, such
as = and <, are frequently written in inﬁx notation.
Consider the formula (x = 0 ∨(∀x)(x ̸= f(x))). (We use x ̸= f(x) to abbreviate
(¬x = f(x)).) This formula uses the variable x in two diﬀerent ways: on one hand, it

Introduction to Proof Theory
27
asserts something about an object x; and on the other hand, it (re)uses the variable x
to state a general property about all objects. Obviously it would be less confusing to
write instead (x = 0 ∨(∀y)(y ̸= f(y))); however, there is nothing wrong, formally
speaking, with using x in two ways in the same formula. To keep track of this, we
need to deﬁne free and bound occurrences of variables. An occurrence of a variable x
in a formula A is deﬁned to be any place that the symbol x occurs in A, except in
quantifer symbols (Qx). (We write (Qx) to mean either (∀x) or (∃x).) If (Qx)(· · ·)
is a subformula of A, then the scope of this occurrence of (Qx) is deﬁned to be the
subformula denoted (· · ·). An occurrence of x in A is said to be bound if and only if
it is in the scope of a quantiﬁer (Qx); otherwise the occurrence of x is called free. If
x is a bound occurrence, it is bound by the rightmost quantiﬁer (Qx) which it is in
the scope of. A formula in which no variables appear freely is called a sentence.
The intuitive idea of free occurrences of variables in A is that A says something
about the free variables. If t is a term, we deﬁne the substitution of t for x in A,
denoted A(t/x) to be the formula obtained from A by replacing each free occurrence
of x in A by the term t. To avoid unwanted eﬀects, we generally want t to be
freely substitutable for x, which means that no free variable in t becomes bound
in A as a result of this substitution; formally deﬁned, this means that no free
occurrence of x in A occurs in the scope of a quantiﬁer (Qy) with y a variable
occurring in t.
The simultaneous substitution of t1, . . . , tk for x1, . . . , xk in A,
denoted A(t1/x1, . . . , tk/xk), is deﬁned similarly in the obvious way.
To simplify notation, we adopt some conventions for denoting substitution.
Firstly, if we write A(x) and A(t) in the same context, this indicates that A = A(x)
is a formula, and that A(t) is A(t/x). Secondly, if we write A(s) and A(t) in the
same context, this is to mean that A is formula, x is some variable, and that A(s) is
A(s/x) and A(t) is A(t/x).
The sequent calculus, discussed in 2.3, has diﬀerent conventions on variables than
the Hilbert-style systems discussed in 2.2; most notably, it has distinct classes of
symbols for free variables and bound variables. See section 2.3.1 for the discussion of
the usage of variables in the sequent calculus.
2.1.2. Semantics of ﬁrst-order logic.
In this section, we deﬁne the semantics,
or ‘meaning’, of ﬁrst-order formulas. Since this is really model theory instead of proof
theory, and since the semantics of ﬁrst-order logic is well-covered in any introductory
textbook on mathematical logic, we give only a very concise description of the
notation and conventions used in this chapter.
In order to ascribe a truth value to a formula, it is necessary to give an interpre-
tation of the non-logical symbols appearing in it; namely, we must specify a domain
or universe of objects, and we must assign meanings to each variable occurring
freely and to each function symbol and relation symbol appearing in the formula. A
structure M (also called an interpretation), for a given language L, consists of the
following:
(1) A non-empty universe M of objects, intended to be the universe of objects over
which variables and terms range;

28
S. Buss
(2) For each k-ary function f of the language, an interpretation f M : M k 7→M ;
and
(3) For each k-ary relation symbol P of the language, an interpretation P M ⊆M k
containing all k-tuples for which P is intended to hold.
If the ﬁrst-order
language contains the symbol for equality, then =M must be the true equality
predicate on M .
We shall next deﬁne the true/false value of a sentence in a structure. This is possible
since the structure speciﬁes the meaning of all the function symbols and relation
symbols in the formula, and the quantiﬁers and propositional connectives take their
usual meanings. For A a sentence and M a structure, we write M ⊨A to denote
A being true in the structure M. In this case, we say that M is a model of A, or
that A is satisﬁed by M. Often, we wish to talk about the meaning of a formula A
in which free variables occur. For this, we need not only a structure, but also an
object assignment, which is a mapping σ from the set of variables (at least the ones
free in A) to the universe M . The object assignment σ gives meanings to the freely
occurring variables, and it is straightforward to deﬁne the property of A being true
in a given structure M under a given object assignment σ, denoted M ⊨A[σ].
To give the formal deﬁnition of M ⊨A[σ], we ﬁrst need to deﬁne the interpre-
tation of terms, i.e, we need to formally deﬁne the manner in which arbitrary terms
represent objects in the universe M . To this end, we deﬁne tM[σ] by induction on
the complexity of t. For x a variable, xM[σ] is just σ(x). For t a term of the form
f(t1, . . . , tk), we deﬁne tM[σ] to equal f M(tM
1 [σ], . . . , tM
k [σ]). If t is a closed term,
i.e., contains no variables, then tM[σ] is independent of σ and is denoted by just tM.
If σ is an object assignment and m ∈M , then σ(m/x) is the object assignment
which is identical to σ except that it maps x to m. We are now ready to deﬁne
the truth of A in M with respect to σ, by induction on the complexity of A. For
A an atomic formula P(t1, . . . , tk), then M ⊨A[σ] holds if and only if the k-tuple
(tM
1 [σ], . . . , tM
k [σ]) is in P M. Also, M ⊨¬A[σ] holds if and only M ⊭A[σ], where
⊭is the negation of ⊨. Likewise, the value of M ⊨A ⊙B[σ] with ⊙one of the
binary propositional connectives depends only on the truth values of M ⊨A[σ]
and M ⊨B[σ], in the obvious way according to Table 1 above. If A is (Qx)B
with Q denoting either ∃or ∀, then M ⊨A[σ] holds if and only if the property
M ⊨B[σ(m/x)] holds for some (respectively, for all) m ∈M .
We say that a formula A is valid in M, if M ⊨A[σ] holds for all appropriate
object assignments. A formula is deﬁned to be valid if and only if it is valid in all
structures. If Γ is a set of formulas, we say Γ is valid in M, written M ⊨Γ, if and
only if every formula in Γ is valid in M . When M ⊨Γ, we say that Γ is satisﬁed
by M. The set Γ is satisﬁable if and only if it is satisﬁed by some model.
For Γ a set of formulas and A a formula, we deﬁne Γ logically implies A, Γ ⊨A,
to hold if A is valid in every structure in which Γ is valid. If Γ is the empty set, we
write just ⊨A, which means of course that A is valid.3
3This deﬁnition of Γ ⊨A has the slightly unexpected result that free variables appearing in Γ
are treated as if they are universally quantiﬁed. For example, according to our deﬁnition, we have

Introduction to Proof Theory
29
2.1.3.
For languages which contain the equality symbol, =, the interpretation of
=M in any structure M must be true equality. Of course, this restriction inﬂuences
the deﬁnition of Γ ⊨A in languages which contain equality. Let ⊨′ indicate logical
implication deﬁned with respect to all structures, including structures in which
equality is not interpreted by true equality. It is an elementary, but important, fact
that ⊨can be deﬁned in terms of ⊨′. Namely, let EQ be the set of equality axioms
deﬁned in section 2.2.1 below; then Γ ⊨A holds if and only if Γ, EQ ⊨′ A holds.
2.1.4. Deﬁnition.
A ﬁrst-order theory is a set T of sentences closed under logical
implication; i.e., if T ⊨A then A ∈T . An axiomatization of T is a set Γ of sentences
such that T is precisely the set of sentences logically implied by Γ.
2.2. Hilbert-style proof systems
2.2.1. A proof system.
We give here an example of a proof system FFO for
ﬁrst-order logic, which is an extension of F to ﬁrst-order logic. In addition to the
axioms of F and the rule modus ponens, there are two axiom schemes:
A(t) ⊃(∃x)A(x)
and
(∀x)A(x) ⊃A(t)
where A may be any formula and t any term. There are also two quantiﬁer rules of
inference:
C ⊃A(x)
C ⊃(∀x)A(x)
and
A(x) ⊃C
(∃x)A(x) ⊃C
where, in both inferences, x may not appear freely in C .
If the ﬁrst-order language under consideration contains the distinguished equality
sign (=), then the equality axioms must also be included; namely,
(∀x)(x = x)
(∀⃗x)(∀⃗y)(x1 = y1 ∧· · · ∧xk = yk ⊃f(⃗x) = f(⃗y))
(∀⃗x)(∀⃗y)(x1 = y1 ∧· · · ∧xk = yk ∧P(⃗x) ⊃P(⃗y))
where f and P are arbitrary function and predicate symbols and k is their arity.
We leave it to the reader to check that the symmetry and transitivity of equality
follow from these axioms, since in the third equality axiom, P may be the equality
sign. We write FFO ⊢A to denote A having an FFO-proof.
A(x) ⊨(∀x)A(x), and thus A(x) ⊨A(y). An alternative deﬁnition of logical implication is often
used (but not in this chapter!) in which free variables occurring in Γ are treated as syntactically
as constants; under this deﬁnition A(x) ⊨(∀x)A(x) does not hold in general.
The advantage our choice for the deﬁnition of ⊨, is that it yields a simpler and more elegant
proof-theory.
This choice does not involve any loss of expressive power since, instead of free
variables in Γ, one can use new constant symbols, and thus obtain the same eﬀect as the
alternative deﬁnition of ⊨. In any event, the two deﬁnitions of ⊨coincide when Γ is a set of
sentences.

30
S. Buss
2.2.2. The Soundness Theorem.
(1) If FFO ⊢A, then ⊨A.
(2) Let Γ be a set of sentences. If there is an FFO-proof of A using sentences from
Γ as additional axioms, then Γ ⊨A.
The Soundness Theorem states that FFO proves only valid formulas. Both parts
of the soundness theorem are readily proved by induction on the number of lines in
a proof.
2.2.3. The Completeness Theorem.
(1) If ⊨A, then FFO ⊢A.
(2) Let Γ be a set of formulas. If Γ ⊨A, then there is an FFO-proof of A using
sentences from Γ as additional axioms.
The completeness theorem and soundness theorem together show that FFO is an
adequate system for formalizing ﬁrst-order logic. The completeness theorem was
proved originally by G¨odel [1930]; the now-standard textbook proof is due to Henkin
[1949]. We shall not give a proof for the completeness of FFO here; instead, we
shall prove the completeness of the cut-free fragment of LK in 2.3.7 below. It is
straightforward to see that FFO can simulate the LK proof system, and this gives
an indirect proof of the completeness of FFO.
2.2.4.
A set Γ of sentences is consistent if and only if there is no sentence A such
that both A and ¬A are provable from Γ and, equivalently, if and only if there
is some formula A such that there is no proof of A from Γ. The soundness and
completeness theorems immediately imply that Γ is consistent if and only if Γ is
satisﬁable.
2.2.5. Historical remarks.
Frege [1879] gave the ﬁrst full formulation of ﬁrst-
order logic. Frege used a pictorial representation of propositional connectives and
quantiﬁers; one remnant of his notation that is still in use is “⊢A”, which was Frege’s
notation for “A is asserted to be true”. Frege used the pictorial notation
A
to express a proposition A; whereas he used the notation
A to express the
assertion that the proposition A is true (or has been proved). Negation, implication,
and universal quantiﬁcation were represented by Frege with pictures such as
A,
A
B
and
§¦
x
A,
which represent the propositions ¬A, B ⊃A, and (∀x)A, respectively.
These
constructions can be iterated to form arbitrary ﬁrst-order expression; that is to say,
in the pictures above, A and B may be replaced by arbitrary pictorial propositions.
The addition of a vertical line to the left end of a proposition indicates that the
proposition has been established to be true (e.g., proved). Thus, for instance

Introduction to Proof Theory
31
§¦
x
A
B
and
§¦
x
A
B
denote the proposition B ⊃(∃x)A and the assertion that this proposition has been
established, respectively.
Frege also developed extensions to his ﬁrst-order logic to include functionals and
predicates; however, these extensions were later discovered by Russell to introduce
set-theoretic paradoxes.
Peano [1889] introduced, independently of Frege, a fragment of ﬁrst-order logic
for reasoning about integers. Based on the work of Frege and Peano, Whitehead
and Russell [1910] gave a consistent framework for formalizing mathematics based
on ﬁrst-order logic, using universal and existential quantiﬁers denoted (x) and (∃x).
The ‘Hilbert-style’ system given above is apparently so-named because it is closely
related to a system used by Hilbert and Ackermann [1928], which is based on earlier
lectures of Hilbert.
The later work of Hilbert and Bernays [1934-39], however,
used the ϵ-calculus instead of a ‘Hilbert-style’ system. The ϵ-calculus contains no
quantiﬁers, but in their place uses symbols ϵx(A(x)) which are intended to denote
an object x such that A(x) holds, if there is such an object. Note that other free
variables and other uses of ϵ symbols may appear in A(x). The ϵ-symbol can be
used to express quantiﬁers by the informal equivalences (∃x)A(x) ⇔A(ϵx(A(x)))
and (∀x)A(x) ⇔A(ϵx(¬A(x))).
The Hilbert-style system we used above is essentially that of Kleene [1952].
2.3. The ﬁrst-order sequent calculus
In this section, the propositional sequent calculus, introduced in section 1.2, is
enlarged to a proof system for ﬁrst-order logic.
2.3.1. Free and bound variables.
The ﬁrst-order sequent calculus has two
classes of variables, called free variables and bound variables. There are inﬁnitely
many variables of each type; free variables are denoted by the metavariables
a, b, c, . . ., and bound variables by the metavariables z, y, x, . . .. The essential idea is
that free variables may not be quantiﬁed, while bound variables may not occur freely
in formulas. The syntactic distinction between free and bound variables necessitates
a change to the deﬁnitions of terms and formulas. Firstly, terms are now deﬁned
as being built up from free variables and function symbols; whereas, the semiterms
are deﬁned as being built up from free and bound variables and function symbols.
Secondly, only bound variables may ever be quantiﬁed. The set of formulas is now
redeﬁned with the additional requirement that only free variables may occur freely
in formulas. Semiformulas are like formulas, except that bound variables may occur
freely in semiformulas. We henceforth use r, s, t, . . . as metavariables for terms, and
A, B, C, . . . as metavariables for formulas.
Note that in general, a subformula of a formula will be a semiformula instead of
a formula.

32
S. Buss
One advantage of these conventions on free and bound variables, is that it avoids
some of the diﬃculties involved in deﬁning A(t), since it is always the case that
the term t will be freely substitutable for b in a formula A(b). Also, without these
conventions, the cut elimination theorem proved below would have to be reformulated
slightly. For example, it is not hard to see that P(x, y)→(∃y)(∃x)P(y, x) would
not have a cut-free proof (this example is from Feferman [1968]).
2.3.2. Deﬁnition.
The ﬁrst-order sequent calculus LK is deﬁned as an extension
of the propositional system PK. LK contains all the rules of inference of PK plus
the following additional rules of inference:
The Quantiﬁer Rules
∀:left
A(t), Γ→∆
(∀x)A(x), Γ→∆
∀:right Γ→∆, A(b)
Γ→∆, (∀x)A(x)
∃:left
A(b), Γ→∆
(∃x)A(x), Γ→∆
∃:right Γ→∆, A(t)
Γ→∆, (∃x)A(x)
In quantiﬁer rules, A may be an arbitrary formula, t an arbitrary term, and the
free variable b of the ∀:right and ∃:left inferences is called the eigenvariable of the
inference and must not appear in Γ, ∆. The propositional rules and the quantiﬁer
rules are collectively called logical rules.
Most of the syntactic deﬁnitions of PK carry over to LK . For example, the
notions of (direct) descendents and ancestors are identically deﬁned; and proof
lengths are still measured in terms of the number of strong inferences in the proof.
The quantiﬁer inferences are, of course, considered strong rules of inferences.
If S is a sequent Γ→∆, then we let AS be the formula (V Γ) ⊃(W ∆). Taking
S to have the same meaning as AS , all the deﬁnitions of ‘validity’ and ‘logical
implication’ of section 2.1.2 apply also to sequents.
Let the free variables of AS
be ⃗b, so AS = AS(⃗b). We let ∀S denote the universal closure, (∀⃗x)AS(⃗x), of the
formula AS .
2.3.3.
LK is deﬁned to allow only initial sequents of the form A→A with
A atomic. However, it is often convenient to allow other initial sequents; so if S is a
set of sequents, we deﬁne LKS to be the proof system deﬁned like LK , but allowing
initial sequents to be from S too.
An important example is the theory LKe for ﬁrst-order logic with equality.
LKe is LK with the addition of the following initial sequents for equality:
→s = s
s1 = t1, . . . , sk = tk→f(⃗s) = f(⃗t)
s1 = t1, . . . , sk = tk, P(⃗s)→P(⃗t)

Introduction to Proof Theory
33
We say that a set S is closed under substitution, if whenever Γ(a)→∆(a) is
in S and t is a term, then Γ(t)→∆(t) is also in S.
2.3.4.
When P is a proof, we write P(a) and P(t) to indicate that P(t) is the
result of replacing every free occurrence of a in formulas of P with t.
Theorem. Let S be a set of sequents which is closed under substitution. If P(b)
is an LKS-proof, and if neither b nor any variable in t is used as an eigenvariable
in P(b), then P(t) is a valid LKS-proof.
2.3.5. Deﬁnition.
A free variable in the endsequent of a proof is called a parameter
variable of the proof. A proof P is said to be in free variable normal form provided
that (1) no parameter variable is used as an eigenvariable, and (2) every other free
variable appearing in P is used exactly once as an eigenvariable and appears in P
only in sequents above the inference for which it is used as an eigenvariable.
In this chapter, we consider only tree-like proofs and thus any proof may be put in
free variable normal form by merely renaming variables.
2.3.6. Soundness Theorem.
Let Γ→∆be an arbitrary sequent.
(1) If Γ→∆has an LK -proof, then Γ→∆is valid.
(2) Let S be a set of sequents. If Γ→∆has an LKS-proof, then S ⊨Γ→∆.
The soundness theorem is readily proved by induction on the number of inferences
in a proof.
2.3.7. Completeness of cut-free LK .
We next prove the completeness of the
cut-free fragment of LK and, more generally, the completeness of LKS. Since our
proofs of completeness also give a proof of the (countable) compactness theorem, we
include the compactness theorem as part (2) of the completeness theorem.
Cut-free Completeness Theorem. Let Γ→∆be a sequent in a ﬁrst-order lan-
guage L which does not contain equality.
(1) If Γ→∆is valid, then it has a cut-free LK -proof.
(2) Let Π be a set of sentences.
If Π logically implies Γ→∆, then there are
C1, . . . , Ck ∈Π such that C1, . . . , Ck, Γ→∆has a cut-free LK -proof.
Corollary. Let S be a set of sequents. If S logically implies Γ→∆, then Γ→∆
has an LKS-proof.
Note that in the corollary, the LKS-proof may not be cut-free. An important special
case of the corollary is that LKe is complete. Although, in general, the cut-free
fragment of LKS may not be complete; we shall see later that it is always possible
to get LKS proofs which contain no ‘free cuts’.

34
S. Buss
The corollary is an immediate consequence of the cut-free completeness theorem.
This is because, if S ⊨Γ→∆, then part (2) of the theorem implies that there are
S1, . . . , Sk ∈S so that ∀S1, . . . , ∀Sk, Γ→∆has an LK -proof. But clearly each
→∀Si has an LKS-proof, so with k further cut inferences, Γ→∆also has an
LKS-proof.
Proof. Part (2) of the cut-free completeness theorem clearly implies part (1); we
shall prove (2) only for the case where Π is countable (and hence the language L is,
w.l.o.g., countable). Assume Π logically implies Γ→∆. The general idea of the
argument is to try to build an LK -proof of Γ→∆from the bottom up, working
backwards from Γ→∆to initial sequents.
This is, of course, analogous to the
procedure used to prove Theorem 1.2.8; but because of the presence of quantiﬁers,
the process of searching for a proof of Γ→∆is no longer ﬁnite. Thus, we shall
have to show the proof-search process eventually terminates — this will be done by
showing that if the proof-search does not yield a proof, then Γ→∆is not valid.
Since the language is countable,
we may enumerate all
L-formulas as
A1, A2, A3, . . . so that every L-formula occurs inﬁnitely often in the enumeration.
Likewise, we enumerate all L-terms as t1, t2, t3, . . . with each term repeated inﬁnitely
often. We shall attempt to construct a cut-free proof P of Γ→∆. The construction
of P will proceed in stages: initially, P consists of just the sequent Γ→∆; at
each stage, P will be modiﬁed (we keep the same name P for the new partially
constructed P ). A sequent in P is said to be active provided it is a leaf sequent of P
and there is no formula that occurs in both its antecedent and its succedent. Note
that a non-active leaf sequent is derivable with a cut-free proof.
We enumerate all pairs ⟨Ai, tj⟩by a diagonal enumeration; each stage of the
construction of P considers one such pair. Initially, P is the single sequent Γ→∆.
At each stage we do the following:
Loop: Let ⟨Ai, tj⟩be the next pair in the enumeration.
Step (1): If Ai is in Π, then replace every sequent Γ′→∆′ in P with the sequent
Γ′, Ai→∆′.
Step (2): If Ai is atomic, do nothing and proceed to the next stage. Otherwise,
we will modify P at the active sequents which contain Ai by doing one of
the following:
Case (2a): If Ai is ¬B, then every active sequent in P which contains Ai, say
of form Γ′, ¬B, Γ′′→∆′, is replaced by the derivation
Γ′, ¬B, Γ′′→∆′, B
Γ′, ¬B, Γ′′→∆′
and similarly, every active sequent in P of the form Γ′→∆′, ¬B, ∆′′ is
replaced by the derivation
B, Γ′→∆′, ¬B, ∆′′
Γ′→∆′, ¬B, ∆′′

Introduction to Proof Theory
35
Case (2b): If Ai is of the form B ∨C , then every active sequent in P of the
form Γ′, B ∨C, Γ′′→∆′, is replaced by the derivation
B, Γ′, B ∨C, Γ′′→∆′
C, Γ′, B ∨C, Γ′′→∆′
Γ′, B ∨C, Γ′′→∆′
And, every active sequent in P of the form Γ′→∆′, B ∨C, ∆′′ is replaced
by the derivation
Γ′→∆′, B ∨C, ∆′′, B, C
Γ′→∆′, B ∨C, ∆′′
Cases (2c)-(2d): The cases where Ai has outermost connective ⊃or ∧are
similar (dual) to case (2b), and are omitted here.
Case (2e): If Ai is of the form (∃x)B(x), then every active sequent in P of the
form Γ′, (∃x)B(x), Γ′′→∆′ is replaced by the derivation
B(c), Γ′, (∃x)B(x), Γ′′→∆′
Γ′, (∃x)B(x), Γ′′→∆′
where c is a new free variable, not used in P yet. In addition, any active
sequent of the form Γ′→∆′, (∃x)B(x), ∆′′ is replaced by the derivation
Γ′→∆′, (∃x)B(x), ∆′′, B(tj)
Γ′→∆′, (∃x)B(x), ∆′′
Note that this, and the dual ∀:left case, are the only cases where tj is used.
These two cases are also the only two cases where it is really necessary to
keep the formula Ai in the new active sequent; we have however, always
kept Ai since it makes our arguments a little simpler.
Case (2f): The case where Ai begins with a universal quantiﬁer is dual to
case (2e).
Step (3): If there are no active sequents remaining in P , exit from the loop;
otherwise, continue with the next loop iteration.
End loop.
If the algorithm constructing P ever halts, then P gives a cut-free proof of
C1, . . . , Ck, Γ→∆for some C1, . . . , Ck ∈Π. This is since each (non-active) initial
sequent has a formula A which appears in both its antecedent and succedent and
since, by induction on the complexity of A, every sequent A→A is derivable with
a cut-free proof.
It remains to show that if the above construction of P never halts, then the
sequent Γ→∆is not logically implied by Π. So suppose the above construction
of P never halts and consider the result of applying the entire inﬁnite construction
process. From the details of the construction of P , P will be an inﬁnite tree (except
in the exceptional case where Γ→∆contains only atomic formulas and Π is empty,

36
S. Buss
in which case P is a single sequent). If Π is empty, then each node in the inﬁnite
tree P will be a sequent; however, in the general case, each node in the inﬁnite
tree will be a generalized sequent of the form Γ′, Π→∆′ with an inﬁnite number
of formulas in its antecedent. (At each stage of the construction of P , the sequents
contain only ﬁnitely many formulas, but in the limit the antecedents contain every
formula from Π since these are introduced by step (1).) P is a ﬁnitely branching tree,
so by K¨onig’s lemma, there is at least one inﬁnite branch π in P starting at the roots
and proceeding up through the tree (except, in the exceptional case, π is to contain
just the endsequent). We use π to construct a structure M and object assignment σ,
for which Γ→∆is not true. The universe of M is equal to the set of L-terms.
The object assignment σ just maps a variable a to itself. The interpretation of a
function symbol is deﬁned so that f M(r1, . . . , rk) is the term f(r1, . . . , rk). Finally,
the interpretation of a predicate symbol P is deﬁned by letting P M(r1, . . . , rk) hold
if and only the formula P(r1, . . . , rk) appears in an antecedent of a sequent contained
in the branch π.
To ﬁnish the proof of the theorem, it suﬃces to show that every formula A
occurring in an antecedent (respectively, a succedent) along π is true (respectively,
false) in M with respect to σ; since this implies that Γ→∆is not valid. This claim
is proved by induction on the complexity of A. For A atomic, it is true by deﬁnition.
Consider the case where A is of the form (∃x)B(x). If A appears in an antecedent,
then so does a formula of the form B(c); by the induction hypothesis, B(c) is true
in M w.r.t. σ, so hence A is. If A appears in an succedent, then, for every term t,
B(t) eventually appears in an succedent; hence every B(t) is false in M w.r.t. σ,
which implies A is also false. Note that A cannot appear in both an antecedent and
a succedent along π, since the nodes in π were never active initial sequents. The
rest of cases, for diﬀerent outermost connectives of A, are similar and we omit their
proofs.
2.3.8.
There are number of semantic tableau proof systems, independently due to
Beth [1956], Hintikka [1955], Kanger [1957] and Sch¨utte [1965], which are very similar
to the ﬁrst-order cut-free sequent calculus. The proof above, of the completeness of
the cut-free sequent calculus, is based on the proofs of the completeness of semantic
tableau systems given by these four authors. The original proof, due to Gentzen, was
based on the completeness of LK with cut and on a process of eliminating cuts from
a proof similar to the construction of the next section.
2.4. Cut elimination.
The cut-free completeness theorem, as established above,
has a couple of drawbacks. Firstly, we have proved cut-free completeness only for
pure LK and it is desirable to also establish a version of cut-free completeness (called
‘free-cut free’ completeness) for LKe and for more general systems LKS. Secondly,
the proof is completely non-constructive and gives no bounds on the sizes of cut-free
proofs. Of course, the undecidability of validity in ﬁrst-order logic implies that the
size of proofs (cut-free or otherwise) cannot be recursively bounded in terms of the
formula being proved; instead, we wish to give an upper bound on the size of a

Introduction to Proof Theory
37
cut-free LK -proof in terms of the size of a general LK -proof.
Accordingly, we shall give a constructive proof of the cut elimination theorem —
this proof will give an eﬀective procedure for converting a general LK -proof into a
cut-free LK -proof. As part of our analysis, we compute an upper bound on the size
of the cut-free proof constructed by this procedure. With some modiﬁcation, the
procedure can also be used to construct free-cut free proofs in LKe or LKS; this will
be discussed in section 2.4.4.
It is worth noting that the cut elimination theorem proved next, together with
the Completeness Theorem 2.2.3 for the Hilbert-style calculus, implies the Cut-free
Completeness Theorem 2.3.7. This is because LK can easily simulate Hilbert-style
proofs and is thus complete; and then, since any valid sequent has an LK -proof, it
also has a cut-free LK -proof.
2.4.1. Deﬁnition.
The depth, dp(A), of a formula A is deﬁned to equal the height
of the tree representation of the formula; that is to say:
• dp(A) = 0, for A atomic,
• dp(A ∧B) = dp(A ∨B) = dp(A ⊃B) = 1 + max{dp(A), dp(B)},
• dp(¬A) = dp( (∃x)A ) = dp( (∀x)A ) = 1 + dp(A).
The depth of a cut inference is deﬁned to equal the depth of its cut formula.
Deﬁnition. The superexponentiation function 2x
i , for i, x ≥0, is deﬁned inductively
by 2x
0 = x and 2x
i+1 = 22x
i . Thus 2x
i is the number which is expressed in exponential
notation as a stack of i many 2’s with an x at the top.
2.4.2. Cut-Elimination Theorem.
Let P be an LK -proof and suppose every cut
formula in P has depth less than or equal to d. Then there is a cut-free LK -proof P ∗
with the same endsequent as P , with size
||P ∗|| < 2||P||
2d+2.
Most proofs of this theorem are based on the original proof of Gentzen [1935]
which involved making local changes to a proof to reduce the depth of cuts, the
number of cuts, or the so-called rank of a cut. We present here a somewhat diﬀerently
structured proof in which the depth or number of cuts is reduced by making global
changes to a proof. We feel that our approach has the advantage of making the
overall cut elimination process clearer and more intuitive.
The main step in proving the cut elimination theorem will be to establish the
following lemma:
2.4.2.1. Lemma.
Let P be an LK -proof with ﬁnal inference a cut of depth d
such that every other cut in P has depth strictly less than d.
Then there is an
LK -proof P ∗with the same endsequent as P with all cuts in P ∗of depth less
than d and with ||P ∗|| < ||P||2.

38
S. Buss
Proof. The proof P ends with a cut inference
... ... ...Q
Γ→∆, A
... ... ...R
A, Γ→∆
Γ→∆
where the depth of the cut formula A equals d and where all cuts in the subproofs
Q and R have depth strictly less than d. The lemma is proved by cases, based on
the outermost logical connective of the cut formula A. We can assume w.l.o.g. that
both Q and R contain at least one strong inference; since otherwise, we must have
A in Γ or in ∆, or have a formula which occurs in both Γ and ∆, and in the former
case, the sequent Γ→∆is obtainable by weak inferences from one of the upper
sequents and the cut can therefore be eliminated, and in the second case, Γ→∆can
be inferred with no cut inference at all. The proof P is also assumed to be in free
variable normal form.
Case (a): Suppose A is a formula of the form ¬B. We shall form new proofs
Q∗and R∗of the sequents B, Γ→∆and Γ→∆, B, which can then be combined
with a cut inference of depth d −1 to give the proof P ∗of Γ→∆. To form Q∗,
ﬁrst form Q′ by replacing every sequent Π→Λ in Q with the sequent Π, B→Λ−,
where Λ−is obtained from Λ by removing all direct ancestors of the cut formula A.
Of course, Q′ is not a valid proof; for example, a ¬:right inference in Q of the form
B, Π→Λ
Π→Λ, ¬B
could become in Q′
B, Π, B→Λ−
Π, B→Λ−
This is not, strictly speaking, a valid inference; but of course, it can be modiﬁed
to be valid by inserting some exchanges and a contraction.
In this fashion, it is
straightforward to modify Q′ so that it becomes a valid proof Q∗by removing some
¬:left inferences and inserting some weak inferences. We leave it to the reader to
check that Q∗can be validly formed in this way: it should be noted that we are using
the assumption that initial sequents C→C must have C atomic. The proof, R∗,
of Γ→∆, B is formed in a similar fashion from R. Obviously, no new cuts are
introduced by this process and, since we do not count weak inferences, ||Q∗|| ≤||Q||
and ||R∗|| ≤||R||; thus P ∗has only cuts of depth < d and has ||P ∗|| ≤||P||.
Case (b): Now suppose the cut formula A is of the form B ∨C . We deﬁne Q′ as a
tree of sequents, with root labeled Γ→∆, B, C , by replacing every sequent Π→Λ
in Q with the sequent Π→Λ−, B, C , where Λ−is Λ minus all occurrences of direct
ancestors of the cut formula. By removing some formerly ∨:right inferences from Q′
and by adding some weak inferences, Q′ can be transformed into a valid proof Q∗.
Now construct RB from R by replacing every occurrence in R of B ∨C as a direct
ancestor of the cut formula with just the formula B. One way that RB can fail to
be a valid proof is that an ∨:left inference

Introduction to Proof Theory
39
B, Π→Λ
C, Π→Λ
B ∨C, Π→Λ
may become just
B, Π→Λ
C, Π→Λ
B, Π→Λ
in RB . This is no longer a valid inference, but it can be ﬁxed up by discarding the
inference and its upper right hypothesis, including discarding the entire subproof of
the upper right hypothesis. The only other changes needed to make RB valid are
the addition of weak inferences, and in this way, a valid proof RB of B, Γ→∆is
formed. A similar process forms a valid proof RC of C, Γ→∆. The proof P ∗can
now be deﬁned to be
... ... ...Q∗
Γ→∆, B, C
... ... ...RC
C, Γ→∆
Γ→∆, B
... ... ...RB
B, Γ→∆
Γ→∆
The processes of forming Q∗, RB , and RC did not introduce any new cuts or any
new strong inferences. Thus we clearly have that every cut in P ∗has depth < d,
and that ||P ∗|| ≤||Q||+2||R||+2. Since ||P|| = ||Q||+||R||+1 and ||Q||, ||R|| ≥1,
this suﬃces to prove the lemma for this case.
Cases (c),(d): The cases where A has outermost connective ∧or ⊃are very
similar to the previous case, and are omitted.
Case (e): Now suppose A is of the form (∃x)B(x). First consider how the formula
(∃x)B(x) can be introduced in the subproof Q. Since it is not atomic, it cannot be
introduced in an initial sequent; thus, it can only be introduced by weakenings and
by ∃:right inferences. Suppose that there are k ≥0 many ∃:right inferences in Q
which have their principal formula a direct ancestor of the cut formula. These can
be enumerated as
Πi→Λi, B(ti)
Πi→Λi, (∃x)B(x)
with 1 ≤i ≤k.
Similarly, we locate all the ∃:left inferences in R which have
principal formula a direct ancestor of the cut formula and enumerate these as
B(ai), Π′
i→Λ′
i
(∃x)B(x), Π′
i→Λ′
i
for 1 ≤i ≤ℓ.
For each i ≤k, we form a proof Ri of the sequent B(ti), Γ→∆by replacing
all ℓof the variables aj with the term ti everywhere in R, replacing every direct
ancestor of the cut formula (∃x)B(x) in R with B(ti), and then removing the ℓ∃:left
inferences. It is easy to see that this yields a valid proof; note that the fact that P is
in free variable normal form ensures that replacing the aj ’s with ti will not impact
the eigenvariable conditions for inferences in R.
Now, form Q′ from Q by replacing each sequent Π→Λ in Q with the sequent
Π, Γ→∆, Λ−, where Λ−is Λ minus all direct ancestors of the cut formula A.

40
S. Buss
Clearly, Q′ ends with the sequent Γ, Γ→∆, ∆; however, it is not a valid proof. To
ﬁx it up to be a valid proof, we need to do the following. First, an initial sequent
in Q′ will be of the form A, Γ→∆, A; this can be validly derived by using the initial
sequent A→A followed by weakenings and exchanges. Second, for 1 ≤i ≤k, the
i-th ∃:right inference enumerated above, will be of the form
Πi, Γ→∆, Λi, B(ti)
Πi, Γ→∆, Λi
in Q. This can be replaced by the following inferences
Πi, Γ→∆, Λi, B(ti)
... ... ...Ri
B(ti), Γ→∆
Πi, Γ→∆, Λi
Note that this has replaced the ∃:right inference of Q with a cut of depth d −1 and
some weak inferences.
No further changes are needed to Q′ to make it a valid proof. In particular,
the eigenvariable conditions still hold since no free variable in Γ→∆is used as an
eigenvariable in Q. By adding some exchanges and contractions to the end of this
proof, the desired proof P ∗of Γ→∆is obtained. It is clear that every cut in P ∗
has depth < d. From inspection of the construction of P ∗, the size of P ∗can be
bounded by
||P ∗|| ≤||Q|| · (||R|| + 1) < ||P||2.
Case (f): The case where A is of the form (∀x)B(x) is completely dual to case (e),
and is omitted here.
Case (g): Finally, consider the case where A is atomic. Form R′ from R by
replacing every sequent Π→Λ in R with the sequent Π−, Γ→∆, Λ, where Π−
is Π minus all occurrences of direct ancestors of A. R′ will end with the sequent
Γ, Γ→∆, ∆and will be valid as a proof, except for its initial sequents. The initial
sequents B→B in R, with B not a direct ancestor of the cut formula A, become
B, Γ→∆, B in R′; these are readily inferred from the initial sequent B→B with
only weak inferences. On the other hand, the other initial sequents A→A in R
become Γ→∆, A which is just the endsequent of Q. The desired proof P ∗of
Γ→∆is thus formed from R′ by adding some weak inferences and adding some
copies of the subproof Q to the leaves of R′, and by adding some exchanges and
contractions to the end of R′.
Since Q and R have only cuts of degree < d (i.e., have no cuts, since d = 0),
P ∗likewise has only cuts of degree < d. Also, since the number of initial sequents
in R′ is bounded by ||R|| + 1, the size of P ∗can be bounded by
||P ∗|| ≤||R|| + ||Q|| · (||R|| + 1) < (||Q|| + 1)(||R|| + 1) < ||P||2
That completes the proof of Lemma 2.4.2.1.

Introduction to Proof Theory
41
Lemma 2.4.2.1 shows how to replace a single cut by lower depth cut inferences.
By iterating this construction, it is possible to remove all cuts of the maximum
depth d in a proof. This is stated as Lemma 2.4.2.2: the Cut-Elimination Theorem
is an immediate consequence of this lemma.
2.4.2.2. Lemma.
If P is an LK -proof with all cuts of depth at most d, there is
an LK -proof P ∗with the same endsequent which has all cuts of depth strictly less
than d and with size ||P ∗|| < 22||P || .
Proof. Lemma 2.4.2.2 will be proved by induction on the number of depth d cuts
in P . The base case, where there are no depth d cuts is trivial, of course, since
||P|| < 22||P || . For the induction step, it suﬃces to prove the lemma in the case where
P ends with a cut inference
... ... ...Q
Γ→∆, A
... ... ...R
A, Γ→∆
Γ→∆
where the cut formula A is of depth d.
First suppose that one of the subproofs, say R, does not have any strong in-
ferences; i.e., ||R|| = 0. Therefore, R must either contain the axiom A→A, or
must have direct ancestors of the cut formula A introduced only by weakenings. In
the former case, A must appear in ∆, and the desired proof P ∗can be obtained
from Q by adding some exchanges and a contraction to the end of Q. In the second
case, P ∗can be obtained from R by removing all the Weakening:left inferences
that introduce direct ancestors of the cut formula A (and possibly removing some
exchanges and contractions involving these A’s). A similar argument works for the
case ||Q|| = 0. In both cases, ||P ∗|| < ||P|| < 22||P || .
Second, suppose that ||Q|| and ||P|| are both nonzero. By the induction hypoth-
esis, there are proofs Q∗and R∗of the same endsequents, with all cuts of depth less
than d, and with
||Q∗|| < 22||Q||
and
||R∗|| < 22||R||
Applying Lemma 2.4.2.1 to the proof
... ... ...Q∗
Γ→∆, A
... ... ...R∗
A, Γ→∆
Γ→∆
gives a proof P ∗of Γ→∆with all cuts of depth < d, so that
||P ∗|| <
¡
||Q∗|| + ||R∗|| + 1
¢2 ≤
³
22||Q|| + 22||R|| −1
´2
< 22||Q||+||R||+1 = 22||P ||.
The ﬁnal inequality holds since ||Q||, ||R|| ≥1.
Q.E.D. Lemma 2.4.2.2 and the Cut Elimination Theorem.

42
S. Buss
2.4.3. A general bound on cut elimination.
The upper bound 2||P||
2d+2 in the Cut
Elimination Theorem is based not only on the size of P , but also on the maximum
depth of the cut formulas in P . However, there is a general method that allows the
bound to be expressed in terms of just ||P||. This is based on the following:
Proposition. Suppose P is an LK -proof of the sequent Γ→∆. Then there is a
cut-free proof P ∗of the same sequent with size ||P ∗|| < 2||P||
2||P||.
Proof. The proposition is obviously true if P has no cuts, so suppose it contains
at least one.
We need some deﬁnitions: Two semiformulas are said to be term
variants if they have identical logical structure and thus one can be transformed
into the other by changing only its semiterms. Obviously the ‘term variant’ relation
is an equivalence relation.
For any equivalence class of term variants, there is a
formula skeleton R(τ1, . . . , τk) such that the equivalence class contains exactly the
semiformulas of the form R(t1, . . . , tk) where t1, . . . , tk are semiterms.
Let c1, c2, c3, . . . be an inﬁnite sequence of new free variables.
For R(· · ·) a
formula skeleton for a term variant class, we pick a new predicate symbol PR. Given
a member R(t1, . . . , tk) of the term variant class, we can form the atomic semiformula
PR(t1, . . . , tk) and its bound variables correspond to the bound variables which are
freely occurring in R(⃗t).
A formula A is deﬁned to be active in P if some strong inference in P has a term
variant of A as a principal formula. Suppose A is non-atomic and not active in P ;
let R(· · ·) give the term variant class of A. Consider the following transformation
of P : for each term variant R(t1, . . . , tk) which appears as a subformula of a formula
in P , replace it with PR(t1, . . . , tk). It can be checked that this transformation yields
a valid proof, which contains exactly the same kinds of inferences as P .
By repeatedly applying this transformation, a valid proof P ′ is obtained in which
every subformula either is atomic or is a term variant of a principal formula of a
strong inference in P ′. Since there are at most ||P|| many strong inferences in P ′,
including at least one cut inference, and since initial sequents contain only atomic
formulas, this implies that every formula in P ′ has depth less than ||P||. (To prove
the last assertion, note that every formula in P ′ either is atomic or has an atomic
ancestor.) Therefore, by the cut elimination theorem, there is a cut-free proof P ′′
with the same endsequent of size ||P ′′|| < 2||P||
2||P||.
This proof P ′′ has the desired size, but it may no longer be a proof of Γ→∆,
since it may contain subformulas of the form PR(t1, . . . , tk).
However, we may
(iteratively) replace all such subformulas in P ′′ with the formula R(t1, . . . , tk). This
yields the desired proof of Γ→∆. 2
2.4.4. Free-cut elimination.
We next investigate the possibility of eliminating
cuts in LKS-proofs; that is to say, in proofs in which initial sequents may come
from S. The set S is presumed to be a ﬁxed set of sequents closed under substitution.
An important example of such an S is the set of equality axioms of LKe; however,
S can also be the axioms of any ﬁrst-order theory.

Introduction to Proof Theory
43
The Cut Elimination Theorem 2.4.2 applied only to LK -proofs; on the other
hand, the proof of Corollary 2.3.7 gave a partial cut elimination theorem for LKS-
proofs. These results can be signiﬁcantly improved by introducing a notion of ‘free’
cuts and giving a construction eliminating free cuts from LKS-proofs by a method
similar to the proof of Theorem 2.4.2.
2.4.4.1. Deﬁnition.
Let P be an LKS-proof.
A formula occurring in P is
anchored (by an S-sequent) if it is a direct descendent of a formula occurring in an
initial sequent in S. A cut inference in P is anchored if either:
(i) the cut formula is not atomic and at least one of the two occurrences of the cut
formula in the upper sequents is anchored, or
(ii) the cut formula is atomic and both of the occurrences of the cut formula in the
upper sequents are anchored.
A cut inference which is not anchored is said to be free. The proof P is free-cut free
if it contains no free cuts.
2.4.4.2.
An occurrence of a formula in a proof P is said to be only weakly introduced
in P if it does not have a direct ancestor which appears in an initial sequent or which
is a principal formula of a strong inference. It is often convenient to assume that a
proof P satisﬁes the condition that no cut formula is only weakly introduced; that is
to say, that every cut inference satisﬁes the condition that neither occurrence of its
cut formula is only weakly introduced in P .
Note that if P is an LKS-proof, then P can be assumed w.l.o.g. to satisfy this
extra condition without an increase in the size of the proof or in the depth of cuts
in the proof. However, conforming to this convention might increase the number
and depth of free cuts in the proof P . To see this suppose that P contains the cut
inference with one of its cut formulas weakly introduced; for instance, suppose that
an inference
Γ→∆, A
A, Γ→∆
Γ→∆
has the A occurring in the right upper sequent A, Γ→∆only weakly introduced.
Now, it is possible that the subproof of Γ→∆, A causes a formula B in Γ or ∆
to be anchored, and that the corresponding B is not anchored in the subproof
of A, Γ→∆. The obvious way to eliminate this cut is to remove all direct ancestors
of the A in the right upper sequent, thereby getting a proof of Γ→∆, and then
discard the left upper sequent and its proof. This, of course, causes B to be only
weakly introduced in the new subproof of Γ→∆. In other words, the occurrence
of B in the sequent Γ→∆becomes unanchored. Then, if a direct descendent of B
is later used as a cut formula, the elimination of the cut on A could have changed
the cut from an anchored cut into a free cut.
Our proof of the free-cut elimination theorem will use induction on the maximum
depth of free cuts appearing in a proof. For this induction to work, it is important
that anchored cuts do not change into free cuts, introducing new free cuts of higher

44
S. Buss
depth. To avoid this, we will assume that no cut formulas in the proof are only
weakly introduced.
2.4.5. Free-cut Elimination Theorem.
Let S be a set of sequents closed under
substitution.
(1) If LKS ⊢Γ→∆, then there is a free-cut free LKS-proof of Γ→∆.
(2) Let P be an LKS-proof satisfying condition 2.4.4.2 and suppose every anchored
cut formula in P has depth less than or equal to d. Then there is a free-cut free
LKS-proof P ∗with the same endsequent as P , with size
||P ∗|| < 2||P||
2d+2.
The Free-cut Elimination Theorem is essentially due to Gentzen; Takeuti [1987]
states a related construction for use with induction rules, which we describe in
section 2.4.6.
Proof. Obviously, it suﬃces to prove part (2) of Theorem 2.4.5. For this proof,
we shall give a procedure (Lemma 2.4.5.1) for removing one maximum depth free
cut. It is not possible to use the procedure from the proof of Lemma 2.4.2.1 without
modiﬁcation, because it may remove S-sequents from the proof and thereby change
some anchored cuts into free cuts (and thereby increase the maximum depth of
free cuts). This is unacceptable since our proof uses induction on the maximum
depth of free cuts in P . This undesirable situation can happen in case (b) of the
proof of Lemma 2.4.2.1 where the outermost connective of the cut formula A is ∨,
since at various points, subproofs ending with C, Π→Λ or with B, Π→Λ are just
discarded in R′
B and in R′
C . Subformulas in Π and Λ may become unanchored by
this process. This can happen also in the similar cases (c) and (d). In addition, it
could also happen in cases (e), (f) and (g) if the cut formula occurring in the right
upper sequent is only weakly introduced, since in these cases the subproof on the
left is completely discarded; however, since condition 2.4.4.2 holds, this never occurs.
Therefore, we need the following analogue of Lemma 2.4.2.1:
2.4.5.1. Lemma.
Let P be an LKS-proof with ﬁnal inference a free cut of depth d
such that every other free cut in P has depth strictly less than d. Then there is
an LKS-proof P ∗with the same endsequent as P with all free cuts in P ∗of
depth less than d and with ||P ∗|| < ||P||2. Furthermore, every formula occurring
in the endsequent of P which was anchored by an S-sequent is still anchored in the
proof P ∗, and every formula in the endsequent of P ∗which is only weakly introduced
in P ∗was already only weakly introduced in P .
Proof. We indicate how to modify the proof of Lemma 2.4.2.1. Assume that the
proof P ends with a free cut inference
... ... ...Q
Γ→∆, A
... ... ...R
A, Γ→∆
Γ→∆

Introduction to Proof Theory
45
If the cut formula A is nonatomic, then Q and R both contain at least one strong
inference with principal formula equal to A. As before the proof splits into cases
depending on the outermost connective of A. When A has outermost connective
¬, ∃or ∀then the argument used in cases (a), (e) and (f) of Lemma 2.4.2.1 still
works: we leave it to the reader to check that, in these cases, any formulas in the
sequent Γ→∆which were anchored or were not only weakly introduced in P still
have these properties in P ∗.
For the case where A is of the form B ∨C , a diﬀerent construction is needed. In
this case, form a proof Q∗with endsequent Γ→∆, B, C by the construction used
in case (b) of the proof of Lemma 2.4.2.1. Also form R′ from R be replacing every
sequent Π→Λ in R with the sequent Π−, Γ→∆, Λ where Π−is Π minus all direct
ancestors of the cut formula B ∨C . An ∨:left inference in R with principal formula
B ∨C will no longer be a valid inference since, in R′ it will become
B, Π−, Γ→∆, Λ
C, Π−, Γ→∆, Λ
Π−, Γ→∆, Λ
This can be transformed into a valid proof by replacing it with
... ... ...Q
Γ→∆, B, C
... ... ...
C, Π−, Γ→∆, Λ
Π−, Γ→∆, Λ, B
... ... ...
B, Π−, Γ→∆, Λ
Π−, Γ→∆, Λ
Fixing up R′ in this way, plus adding some weak inferences yields a valid proof R∗
of Γ, Γ→∆, ∆. Appending some exchange and contraction inferences yields the
desired proof P ∗. It is readily checked that ||P ∗|| ≤||R|| · (||Q|| + 1) < ||P||2.
The ﬁnal case to consider is the case where A is atomic. Since the cut is not
anchored, the cut formula will either not be anchored in Q or not be anchored in R.
In the latter case, since the cut formula is not only weakly introduced, it must have
a direct ancestor in an initial sequent A→A of R. Therefore, the argument from
case (g) of the proof of Lemma 2.4.2.1 still works. In the former case, where the cut
formula is not anchored in Q, the dual argument works.
That completes the proof of Lemma 2.4.5.1. The Free-cut Elimination Theorem
follows immediately from this lemma in the same way that the Cut-Elimination
Theorem followed from Lemma 2.4.2.1.
Q.E.D. Free-cut Elimination Theorem.
2.4.6. Free-cut elimination with induction rules.
A very useful application
of free-cut elimination is for analyzing subtheories of arithmetic in which induction
is restricted to certain classes of formulas. For these fragments of arithmetic, the
free-cut elimination theorem becomes easier to use if induction rules are used in
place of induction axioms. The most common formulation of induction rules is as
inferences of the form:

46
S. Buss
A(b), Γ→∆, A(b + 1)
A(0), Γ→∆, A(t)
where b is an eigenvariable and may appear only as indicated, and t is an arbitrary
term. It is easily checked that, because of the presence of the side formulas Γ and
∆, the induction rule for A is equivalent to the induction axiom for A:
A(0) ∧(∀x)(A(x) ⊃A(x + 1)) ⊃(∀x)A(x).
For Φ a set of formulas, Φ-IND indicates the theory with the above induction rules
for all formulas A ∈Φ. We shall always assume that such a set Φ is closed under
(term) substitution.
The classic examples of arithmetic theories axiomatized by induction are the
theories IΣn. These are axiomatized by the six axioms of Robinson’s theory Q plus
induction for Σn formulas (see Chapter II). When IΣn is formalized in the sequent
calculus, it has the non-logical initial sequents expressing the axioms of Q plus the
induction rule for Σn formulas. The initial sequents of IΣn proofs are always purely
existential. Along the same lines, the fragments T n
2 and Sn
2 of bounded arithmetic are
commonly formalized in the sequent calculus with quantiﬁer-free non-logical initial
sequents from BASIC, plus induction on Σb
n-formulas (see Chapter II or Buss [1986]).
For T n
2 , the induction rules are as shown above; for Sn
2 the following PIND rules are
used:
A(⌊1
2b⌋), Γ→∆, A(b)
A(0), Γ→∆, A(t)
Deﬁnition. Let P be a sequent calculus proof in an arithmetic theory S + Φ-IND
(or S + Φ-PIND). The principal formulas of an induction inference are the formulas
denoted by A(0) and A(t) in the lower sequent. An occurrence of a formula in P is
deﬁned to be anchored if it is a direct descendent of a formula occurring in an initial
sequent from S or it is a direct descendent of a principal formula of an induction
inference.
Using this deﬁnition of anchored formulas, the notions of anchored cut inference
and free cut inference are deﬁned identically to Deﬁnition 2.4.4.1.
Free-cut Elimination Theorem. Let T = S + Φ-IND be a theory of arithmetic,
with S and Φ closed under term substitution. Let Γ→∆be a logical consequence
of T . Then there is a free-cut free T -proof of Γ→∆. Furthermore, the size bounds
of Theorem 2.4.5(2) apply to T -proofs.
This theorem is proved by an argument identical to the proof of Theorem 2.4.5.
The theorem also applies without modiﬁcation to theories axiomatized with PIND
in place of IND. It is also straightforward to modify the free cut elimination to
work with sequent calculus formulations of inference rules other than induction. For
example, the collection (replacement) axioms for fragments of Peano arithmetic can
be equivalently formulated as sequent calculus rules. The obvious straightforward

Introduction to Proof Theory
47
modiﬁcation of the Free-cut Elimination theorem applies to theories with collection
rules.
A particularly useful corollary of the Free-Cut Elimination Theorem is:
2.4.7. Corollary.
Let S be a set of sequents and Φ be a set of formulas closed
under term substitution and under subformulas, such that every sequent in S contains
only formulas from Φ. Let T be the theory S + Φ-IND (or S + Φ-PIND). Suppose
that Γ→∆is a consequence of T and that every formula in Γ→∆is in Φ. Then
there is a T -proof P of Γ→∆such that every formula appearing in P is in Φ.
To prove this corollary, just note that it holds for every free-cut free T -proof of
Γ→∆; this is because every formula in the proof must be an ancestor of either a
cut formula or a formula in the endsequent.
2.4.8. Natural deduction.
Natural deduction proof systems were introduced by
Gentzen [1935] in the same paper which introduced the ﬁrst-order sequent calculus.
Gentzen’s motivation in deﬁning natural deduction was (in his words) “to set up
a formula system which comes as close as possible to actual reasoning.”4
The
importance of natural deduction was established by the classic result of Prawitz [1965]
that a version of the cut elimination holds also for natural deduction.
It is fair to say that the process of constructing natural deduction proofs is indeed
‘natural’ in that it corresponds closely to the human reasoning process. On the other
hand, a fully constructed natural deduction proof can be very confusing to read; in
particular, because of the non-local nature of natural deduction proofs, it is diﬃcult
to quickly ascertain which formulas depends on which hypotheses.
Natural deduction proof systems are particularly elegant for intuitionistic logic,
especially with respect to the Curry-Howard formulas-as-types interpretation (see
section 3.1.5).
A good modern treatment of applications of natural deduction is
given by Girard [1989].
We give a short deﬁnition of the natural deduction proof system here; however,
the reader should refer to Prawitz [1965] for a full treatment and for statements of the
normalization theorems. The deﬁnitions of terms and formulas and the conventions
on free and bound variables are the same for natural deduction as for the sequent
calculus, except that negation, ¬, is not a basic symbol; instead, a new atomic
formula ⊥is used to denote absurdity (the constant False), and ¬A abbreviates
A ⊃⊥. A natural deduction proof is a tree of formulas; any formula may appear at
a leaf, as a hypothesis. Various inferences may close or discharge the hypotheses; in
a completed proof all hypotheses must be discharged and the proof is a proof of the
formula appearing at the root node at the bottom of the tree. It is best to picture the
construction of a natural deduction proof as an ongoing process; at any point in this
process some hypotheses may already be discharged, whereas others remain open.
The valid rules of inference are given below; they are classiﬁed as introduction rules
4Gentzen [1969], p. 68

48
S. Buss
or elimination rules. Hypotheses discharged by an inference are shown in square
brackets.
∧-intro
A
B
A ∧B
∧-elim
A ∧B
A
A ∧B
B
∨-intro
A
A ∨B
B
A ∨B
∨-elim
A ∨B
[A]
C
[B]
C
C
⊃-intro
[A]
B
A ⊃B
⊃-elim
A
A ⊃B
B
∀-intro
A(b)
(∀x)A(x)
∀-elim
(∀x)A(x)
A(t)
∃-intro
A(t)
(∃x)A(x)
∃-elim
(∃x)A(x)
[A(b)]
B
B
⊥I
⊥
A
⊥C
[A ⊃⊥]
⊥
A
In the ∀-intro and ∃-elim rules, the free variable b is an eigenvariable: this means
it must not appear in any non-discharged hypothesis above the ∀-intro inference or
in any non-discharged hypotheses other than A(b) above the hypothesis B of the
∃-elim inference. The ⊥I rule is used for both intuitionistic and classical logic; the
⊥C rule is included for classical logic. If both rules for ⊥are omitted, then minimal
logic is obtained.
2.5. Herbrand’s theorem, interpolation and deﬁnability theorems
2.5.1. Herbrand’s theorem.
One of the fundamental theorems of mathematical
logic is the theorem of Herbrand [1930] which allows a certain type of reduction of
ﬁrst-order logic to propositional logic. In its basic form it states:
Herbrand’s Theorem. Let T be a theory axiomatized by purely universal formulas.
Suppose that T ⊨(∀⃗x)(∃y1, . . . , yk)B(⃗x, ⃗y) with B(⃗x, ⃗y) a quantiﬁer-free formula.
Then there is a ﬁnite sequence of terms ti,j = ti,j(⃗x), with 1 ≤i ≤r and 1 ≤j ≤k
so that
T ⊢(∀⃗x)
Ã r_
i=1
B(⃗x, ti,1, . . . , ti,k)
!
.
Proof. Since T is axiomatized by purely universal formulas, it may, without loss
of generality, be axiomatized by quantiﬁer-free formulas (obtained by removing the
universal quantiﬁers). Let T be the set of sequents →A with A a (quantiﬁer-free)
axiom of T .
Since T ⊨(∀⃗x)(∃⃗y)B(⃗x, ⃗y), there is a LKT-proof of the sequent

Introduction to Proof Theory
49
→(∃⃗y)B(⃗a, ⃗y). By the free-cut elimination theorem, there is a free-cut free proof P
of this sequent.
Since the T-sequents contain only quantiﬁer-free formulas, all cut formulas in P
are quantiﬁer-free. Thus, any non-quantiﬁer-free formula in P must be of the form
(∃yj) · · · (∃yk)B(⃗a, t1, . . . , tj−1, yj, . . . , yk) with 1 ≤j < k. We claim that P can be
modiﬁed to be a valid proof of a sequent of the form
→B(⃗a, t1,1, . . . , t1,k), . . . , B(⃗a, tr,1, . . . , tr,k).
The general idea is to remove all ∃:right inferences in P and remove all existential
quantiﬁers, replacing the bound variables by appropriate terms. Since there may
have been contractions on existential formulas that are no longer identical after terms
are substituted for variables it will also be necessary to remove contractions and add
additional formulas to the sequents. To do this more formally, we know that any
sequent in P is of the form Γ→∆, ∆′ (up to order of the formulas in the sequent),
where each formula in Γ and ∆is quantiﬁer-free and where each formula in ∆′ is
not quantiﬁer-free but is purely existential. We can then prove by induction on the
number of lines in the free-cut free proof of Γ→∆, ∆′ that there is an r ≥0 and a
cedent ∆′′ of the form
B(⃗a, t1,1, . . . , t1,k), . . . , B(⃗a, tr,1, . . . , tr,k)
such that Γ→∆, ∆′′ is provable. We leave the rest of the details to the reader. 2
We momentarily deﬁne an instance of a universal formula (∀⃗x)A(⃗x) to be any
quantiﬁer-free formula A(⃗t).
It is not hard to see using cut elimination, that if
a quantiﬁer-free formula C is a consequence of a universal theory T , then it is
a tautological consequence of some ﬁnite set of instances of axioms of T and of
equality axioms.
In the special case where T is the null theory, we have that
C is a consequence of instances of equality axioms (and C is therefore called a
quasitautology). If, in addition, C does not involve equality, C will be tautologically
valid. Thus, Herbrand’s theorem reduces provability in ﬁrst-order logic to generation
of (quasi)tautologies.
2.5.2.
As stated above, Herbrand’s theorem has limited applicability since it
applies only to Π2-consequences of universal theories: fortunately, however, there
are several ways to extend Herbrand’s theorem to more general situations. In 2.5.3
below, we explain one such generalization; but ﬁrst, in this paragraph, we give a
simpler method of widening the applicability of Herbrand’s theorem, based on the
introduction of new function symbols, which we call Herbrand and Skolem functions,
that allow quantiﬁer alternations to be reduced.
For notational convenience, we will consider only formulas in prenex form in this
paragraph; however, the deﬁnitions and proposition below can be readily generalized
to arbitrary formulas.

50
S. Buss
Deﬁnition. Let (∃x)A(x,⃗c) be a formula with ⃗c all of its free variables. The Skolem
function for (∃x)A is represented by a function symbol f∃xA and has the deﬁning
axiom:
Sk-def(f∃xA) :
(∀⃗y)(∀x) (A(x, ⃗y) ⊃A(f∃xA(⃗y), ⃗y)) .
Note that Sk-def(f∃xA) implies (∀⃗y) ((∃x)A(x, ⃗y) ↔A(f∃xA(⃗y), ⃗y)).
Deﬁnition. Let A(⃗c) be a formula in prenex form. The Skolemization, AS(⃗c), of A
is the formula deﬁned inductively by:
(1) If A(⃗c) is quantiﬁer-free, then AS(⃗c) is A(⃗c).
(2) If A(⃗c) is (∀y)B(⃗c, y), then AS(⃗c) is the formula (∀y)BS(⃗c, y).
(3) If A(⃗c) is (∃y)B(⃗c, y), then AS(⃗c) is BS(⃗c, fA(⃗c)), where fA is the Skolem
function for A.
It is a simple, but important fact that AS ⊨A.
The Skolemization of a theory T is the theory T S = {AS : A ∈T}. Note that T S
is a purely universal theory. Incidentally, the set of all Sk-def axioms of the Skolem
functions is equivalent to a set of universal formulas; however, they are not included
in theory T S . From model-theoretic considerations, it is not diﬃcult to see that T S
contains and is conservative over T .
We next deﬁne the concept of ‘Herbrandization’ which is completely dual to the
notion of Skolemization:
Deﬁnition. Let (∀x)A(x,⃗c) be a formula with ⃗c all of its free variables.
The
Herbrand function for (∀x)A is represented by a function symbol h∀xA and has the
deﬁning axiom:
(∀⃗y)(∀x) (¬A(x, ⃗y) ⊃¬A(h∀xA(⃗y), ⃗y)) .
Note that this implies (∀⃗y) ((∀x)A(x, ⃗y) ↔A(h∀xA(⃗y), ⃗y)). The Herbrand function
can also be thought of as a ‘counterexample function’; in that (∀x)A(x) is false if
and only if h∀xA provides a value x which is a counterexample to the truth of (∀x)A.
Deﬁnition. Let A(⃗c) be a formula in prenex form. The Herbrandization, AH(⃗c),
of A is the formula deﬁned inductively by:
(1) If A(⃗c) is quantiﬁer-free, then AH(⃗c) is A(⃗c).
(2) If A(⃗c) is (∃y)B(⃗c, y), then AH(⃗c) is the formula (∃y)BH(⃗c, y).
(3) If A(⃗c) is (∀y)B(⃗c, y), then AH(⃗c) is BH(⃗c, hA(⃗c)), where hA is the Herbrand
function for A.
It is not hard to see that A ⊨AH . Note that AH is purely existential.

Introduction to Proof Theory
51
Proposition. Let T be a set of prenex formulas and A any prenex formula. Then
the following are equivalent:
(1) T ⊨A,
(2) T S ⊨A,
(3) T ⊨AH ,
(4) T S ⊨AH ,
This proposition is easily proved from the above deﬁnitions and remarks. The
importance of the proposition lies in the fact that T S is a universal theory and
that AH is an existential formula, and that therefore Herbrand’s theorem applies
to T S ⊨AH . Therefore, Herbrand’s theorem can be applied to an arbitrary logical
implication T ⊨A, at the cost of converting formulas to prenex form and introducing
Herbrand and Skolem functions.
2.5.3. A generalized Herbrand’s theorem.
Herbrand actually proved a more
general version of Theorem 2.5.1 which applied directly whenever ⊨A, for A a general
formula, not necessarily existential. His result avoids the use of Skolem/Herbrand
functions but is somewhat more diﬃcult to state and comprehend. The theorem we
state next is a generalization of Herbrand’s theorem which is quite similar in spirit
and power to the theorem as stated originally by Herbrand [1930].
In this section, we shall consider a ﬁrst-order formula A such that ⊨A. Without
loss of generality, we shall suppose that the propositional connectives in A are
restricted to be ∧, ∨and ¬, and that the ¬ connective appears only in front of
atomic subformulas of A.
(The only reason for this convention is that it avoids
having to keep track of whether quantiﬁers appear positively and negatively in A.)
Deﬁnition. Let A satisfy the above convention on negations. An ∨-expansion of A
is any formula that can be obtained from A by a ﬁnite number of applications of the
following operation:
(α) If B is a subformula of an ∨-expansion A′ of A, replacing B in A′ with B ∨B
produces another ∨-expansion of A.
A strong ∨-expansion of A is deﬁned similarly, except that now the formula B is
restricted to be a subformula with outermost connective an existential quantiﬁer.
Deﬁnition. Let A be a formula.
A prenexiﬁcation of A is a formula obtained
from A by ﬁrst renaming bound variables in A so that no variable is quantiﬁed
more than once in A and then using prenex operations to put the formula in prenex
normal form.
Note that there will generally be more than one prenexiﬁcation of A since prenex
operations may be applied in diﬀerent orders resulting in a diﬀerent order of the
quantiﬁers in the prenex normal form formula.

52
S. Buss
Deﬁnition. Let A be a valid ﬁrst-order formula in prenex normal form, with no
variable quantiﬁed twice in A. If A has r ≥0 existential quantiﬁers, then A is of
the following form with B quantiﬁer-free:
(∀x1 · · · xn1)(∃y1)(∀xn1+1 · · · xn2)(∃y2) · · · (∃yr)(∀xnr+1 · · · xnr+1)B(⃗x, ⃗y)
with 0 ≤n1 ≤n2 ≤· · · ≤nr+1. A witnessing substitution for A is a sequence of
terms (actually, semiterms) t1, . . . tr such that (1) each ti contains arbitrary free vari-
ables but only bound variables from x1, . . . , xni and (2) the formula B(⃗x, t1, . . . , tr)
is a quasitautology (i.e., a tautological consequence of instances of equality axioms
only). In the case where B does not contain the equality sign, then (2) is equivalent
to B being a tautology.
Let T be a ﬁrst-order theory. A sequence of terms is said to witness A over T if
the above conditions hold except with condition (2) replaced by the weaker condition
that T ⊨(∀⃗x)B(x,⃗t).
Deﬁnition. A Herbrand proof of a ﬁrst-order formula A consists of a prenexiﬁca-
tion A∗of a strong ∨-expansion of A plus a witnessing substitution σ for A∗.
A Herbrand T -proof of A consists of a prenexiﬁcation A∗of a strong ∨-expansion
of A plus a substitution which witnesses A over T .
We are now in a position to state the general form of Herbrand’s theorem:
Theorem. A ﬁrst-order formula A is valid if and only if A has a Herbrand proof.
More generally, if T is a universal theory, then T ⊨A if and only if A has a Herbrand
T -proof.
Proof. We shall sketch a proof of only the ﬁrst part of the theorem since the proof
of the second part is almost identical. Of course it is immediate from the deﬁnitions
that if A has a Herbrand proof, then A is valid. So suppose A is valid, and therefore
has a cut-free LK -proof P . We shall modify P in stages so as to extract a Herbrand
proof of P .
The ﬁrst stage will involve restricting the formulas which can be combined by
a contraction inference. One problem that arises in this regard is that inferences
with two hypothesis, such as ∨:left and ∧:right, contain implicit contractions on side
formulas. To avoid dealing with this complication, we modify the rules of inference
so that no implicit contractions occur; e.g., the ∨:left inference rule is replaced by
the rule
A, Γ→∆
B, Γ′→∆′
A ∨B, Γ, Γ′→∆, ∆′
and the ∧:right and ⊃:left are modiﬁed analogously. It is easy to see that changing
the rules of inference in this way, makes no diﬀerence to what strong inferences are
needed in a proof: instead, it merely makes contractions explicit. In particular, the
cut elimination theorems still hold with the new inference rules.

Introduction to Proof Theory
53
A contraction inference is said to be a propositional contraction (resp., an ∃-
contraction) provided that the principal formula of the contraction is quantiﬁer-
free (resp., its outermost connective is an existential quantiﬁer). The ﬁrst step in
modifying P is to form a cut-free proof P1, also with endsequent →A such that all
contraction inferences in P1 are propositional or ∃-contractions. The construction
of P1 from P is formally done by an induction process analogous to the proof of the
Cut-elimination Theorem 2.4.2. Namely, deﬁne the E-depth of a formula similarly to
the deﬁnition of depth in 2.4.1, except deﬁne the E-depth of a quantiﬁer-free formula
or of a formula with outermost connective an existential quantiﬁer to be zero. Then
prove, by double induction on the maximum E-depth d of contraction formulas
and the number of contractions of formulas of this maximum E-depth, that P1 can
be transformed into a proof in which all contractions are on formulas of E-depth
zero. The induction step consists of removing a topmost contraction inference of
the maximum E-depth d. For example, suppose that the following inference is a
topmost contraction with principal formula of E-depth d;
... ... ...R
Γ→∆, (∀x)B, (∀x)B
Γ→∆, (∀x)B
Since P1 is w.l.o.g. in free variable normal form and since this is a topmost con-
traction of E-depth d, we can modify the subproof R of P1 by removing at
most two ∀:right inferences and/or changing some Weakening:right inferences to
get a proof of Γ→∆, B(a), B(a′), where a and a′ are free variables not appearing
in the endsequent of R.
Further replacing a′ everywhere by a gives a proof of
Γ→∆, B(a), B(a): we use this get to a proof ending:
... ... ...
Γ→∆, B(a), B(a)
Γ→∆, B(a)
Γ→∆, (∀x)B
Thus we have reduced the E-depth of the contraction inference. A similar procedure
works for contractions of E-depth d with outermost connective a propositional
connective—we leave the details to the reader. Note that the construction of P1
depends on the fact that propositional inferences and ∀:right inferences can be
pushed downward in the proof. It is not generally possible to push ∃:right inferences
downward in a proof without violating eigenvariable conditions.
The second step in modifying P is to convert P1 into a cut-free proof P2 of
some strong ∨-expansion A′ of A such that every contraction in P2 is propositional.
This is done by the simple expedient of replacing every ∃-contraction in P1 with
an ∨:right inference, and then making the induced changes to all descendents of
the principal formula of the inference. More precisely, starting with a lowermost
∃-contraction in P1, say
Γ→∆, (∃x)B, (∃x)B
Γ→∆, (∃x)B

54
S. Buss
replace this with an ∨:left inference
Γ→∆, (∃x)B, (∃x)B
Γ→∆, (∃x)B ∨(∃x)B
and then, in order to get a syntactically correct proof, replace, as necessary, subfor-
mulas (∃x)B′ of formulas in P with (∃x)B′ ∨(∃x)B′ (we use the notation B′ since
terms in B may be diﬀerent in the descendents). Iterating this process yields the
desired proof P2 of a strong ∨-expansion A′ of A. By renaming bound variables
in P2 we can assume w.l.o.g. that no variable is quantiﬁed twice in any single sequent
in P2.
Thirdly, from P2 we can construct a prenexiﬁcation A∗of A′ together with
a witnessing substitution, thereby obtaining a Herbrand proof of A. To do this,
we iterate the following procedure for pulling quantiﬁers to the front of the proved
formula.
Find any lowest quantiﬁer inference in P2 which has not already been
handled: this quantiﬁer inference corresponds to a unique quantiﬁer, (Qx), appearing
in the endsequent of the proof (and conversely, each quantiﬁer in the endsequent of
the proof corresponds to a unique quantiﬁer inference, since all contraction formulas
are quantiﬁer-free). Use prenex operations to pull (Qx) as far to the front of the
endsequent formula as possible (but not past the quantiﬁers that have already been
moved to the front of the endsequent formula). Also, push the quantiﬁer inference
downward in the proof until it reaches the group of quantiﬁer inferences that have
already been pushed downward in the proof. It is straightforward to check that this
procedure preserves the property of having a syntactically valid proof. When we are
done iterating this procedure, we obtain a proof P3 of a prenexiﬁcation →A∗of
A. It remains to deﬁne a witnessing substitution for A∗, which is now easy: for each
existential quantiﬁer (∃yi) in A∗, ﬁnd the corresponding ∃:right inference
Γ→∆, B(ti)
Γ→∆, (∃yi)B(yi)
and let the term ti be from this inference. That this is a witnessing substitution
for A∗is easily proved by noting that by removing the ∃:right inference from P3, a
proof of A∗
M(⃗x,⃗t) is obtained where A∗
M is the quantiﬁer-free portion of A∗. 2
The above theorem can be used to obtain the following ‘no-counterexample inter-
pretation’ which has been very useful recently in the study of bounded arithmetic (see
Kraj´ıˇcek, Pudl´ak and Takeuti [1991], or section ?? of Chapter II).5
Corollary. Let T be a universal theory and suppose T ⊨(∃x)(∀y)A(x, y,⃗c) with
A a quantiﬁer-free formula.
Then there is a k > 0 and terms t1(⃗c), t2(⃗c, y1),
t3(⃗c, y1, y2), . . . , tk(⃗c, y1, . . . yk−1) such that
T ⊨(∀y1)[A(t1(⃗c), y1,⃗c) ∨(∀y2)[A(t2(⃗c, y1), y2,⃗c)∨
(∀y3)[A(t3(⃗c, y1, y2), y3,⃗c) ∨· · · ∨(∀yk)[A(tk(⃗c, y1, . . . , yk−1), yk,⃗c))] · · ·]]]
5This corollary is named after the more sophisticated no-counterexample interpretations of
Kreisel [1951,1952].

Introduction to Proof Theory
55
To prove the corollary, note that the only strong ∨-expansions of A are formulas
of the form W(∃x)(∀y)A(x, y,⃗c) and apply the previous theorem.
2.5.4. No recursive bounds on number of terms.
It is interesting to ask
whether it is possible to bound the value of r in Herbrand’s Theorem 2.5.1. For
this, consider the special case where the theory T is empty, so that we have an
LK -proof P of (∃x1, . . . , xk)B(⃗a,⃗x) where B is quantiﬁer-free. There are two ways
in which one might wish to bound the number r needed for Herbrand’s theorem:
as a function of the size of P , or alternatively, as a function of the size of the
formula (∃⃗x)B. For the ﬁrst approach, it follows immediately from Theorem 2.4.3
and the proof of Herbrand’s theorem, that r ≤2||P||
2||P||. For the second approach, we
shall sketch a proof below that r can not be recursively bounded as a function of the
formula (∃⃗x)B.
To show that r cannot be recursively bounded as a function of (∃⃗x)B, we
shall prove that having a recursive bound on r would give a decision procedure
for determining if a given existential formula is valid. Since it is well known that
validity of existential ﬁrst-order formulas is undecidable, this implies that r cannot
be recursively bounded in terms of the formula size.
What we shall show is that, given a formula B as in Theorem 2.5.1 and given
an r > 0, it is decidable whether there are terms t1,1, . . . , tr,k which make the formula
r_
i=1
B(⃗a, ti,1, . . . , ti,k)
(1)
a tautology.6 This will suﬃce to show that r cannot be recursively bounded. The
quantiﬁer-free formula B is expressible as a Boolean combination C(D1, . . . , Dℓ)
where each Di is an atomic formula and C(· · ·) is a propositional formula. If the
formula (1) is a tautology, it is by virtue of certain formulas Di(⃗a, ti,1, . . . , ti,k) being
identical. That is to say there is a ﬁnite set X of equalities of the form
Di(⃗a, ti,1, . . . , ti,k) = Di′(⃗a, ti′,1, . . . , ti′,k)
such that, any set of terms t1,1, . . . , tr,k which makes all the equalities in X true will
make (1) a tautology.
But now the question of whether there exist terms t1,1, . . . , tr,k which satisfy such
a ﬁnite set X of equations is easily seen to be a ﬁrst-order uniﬁcation problem, as
described in section 2.6.1 below. This means that there is an algorithm which can
either determine that no choice of terms will satisfy all the equations in X or will
ﬁnd a most general uniﬁer which speciﬁes all possible ways to satisfy the equations
of X .
Since, for a ﬁxed r > 0, there are only ﬁnitely many possible sets X of equalities,
we have the following algorithm for determining if there are terms which make (1)
a tautology: for each possible set X of equalities, check if it has a solution (i.e., a
6 This was ﬁrst proved by Herbrand [1930] by the same argument that we sketch here.

56
S. Buss
most general uniﬁer), and if so, check if the equalities are suﬃcient to make (1) a
tautology. 2
2.5.5. Interpolation theorem
Suppose we are given two formulas A and B such that A ⊃B is valid. An
interpolant for A and B is a formula C such that A ⊃C and C ⊃B are both
valid. It is a surprising, and fundamental, fact that it is always possible to ﬁnd an
interpolant C such that C contains only non-logical symbols which occur in both
A and B.
We shall assume for this section that ﬁrst-order logic has been augmented to
include the logical symbols ⊤and ⊥. For this, the sequent calculus has two new
initial sequents →⊤and ⊥→. We write L(A) to denote the set of non-logical
symbols occurring in A plus all free variables occurring in A, i.e., the constant,
symbols, function symbols, predicate symbols and free variables used in A. For Π a
cedent, L(Π) is deﬁned similarly.
Craig’s Interpolation Theorem. Craig [1957a].
(a) Let A and B be ﬁrst-order formulas such that ⊨A ⊃B.
Then there is a
formula C such that L(C) ⊆L(A) ∩ÃL(B) and such that ⊨A ⊃C and
⊨C ⊃B.
(b) Suppose Γ1, Γ2→∆1, ∆2 is a valid sequent. Then there is a formula C such
that L(C) is a subset of L(Γ1, ∆1) ∩L(Γ2, ∆2) and such that Γ1→∆1, C and
C, Γ2→∆2 are both valid.
Craig’s interpolation can be proved straightforwardly from the cut elimination
theorem. We shall outline some of the key points of the proof, but leave a full proof
to the reader. First it is easy to see that part (a) is just a special case of (b), so it
suﬃces to prove (b). To prove (b), we ﬁrst use the cut elimination theorem to obtain
a cut-free proof P of Γ1, Γ2→∆1, ∆2. We then prove by induction on the number
of strong inferences in P that there is a formula C with only the desired non-logical
symbols and there are proofs P1 and P2 of Γ1→∆1, C and C, Γ2→∆2. In fact, the
proofs P1 and P2 are also cut-free and have lengths linearly bounded by the length
of P . For an example of how the proof by induction goes, let’s make the simplifying
assumption that there are no function symbols in our languages, and then assume
that the ﬁnal strong inference of P is an ∃:right inference with principal formula
in ∆2. That is to say, suppose P ends with the inference
... ... ...
Γ1, Γ2→∆1, ∆′
2, A(t)
Γ1, Γ2→∆1, ∆′
2, (∃x)A(x)
with ∆2 is the cedent ∆′
2, (∃x)A(x). Since we are assuming there are no function
symbols, t is just a free variable or a constant symbol. The induction hypothesis
states that there is an interpolant C(t) with an appropriate ﬁrst-order language such

Introduction to Proof Theory
57
that Γ1→∆1, C(t) and C(t), Γ2→∆2, A(t) are LK -provable. The interpolant C∗
for the endsequent of P is deﬁned as follows: if the symbol t does not appear in the
sequent Γ2→∆2, then C∗is (∃y)C(y); otherwise, if the symbol t does not appear
in the sequent Γ1→∆1, then C∗is (∀y)C(y); and if t appears in both sequents,
C∗is just C . It can be checked that in all three cases, the sequents Γ1→∆1, C∗and
C∗, Γ2→∆2, (∃x)A(x) are LK -provable. Therefore, C∗is an interpolant for the
endsequent of P ; also, it is obvious that the language L(C) of C is still appropriate
for the endsequent.
Secondly, suppose P ends with the inference
... ... ...
Γ1, Γ2→∆1, ∆′
2, A(b)
Γ1, Γ2→∆1, ∆′
2, (∀x)A(x)
with the principal formula still presumed to be in ∆2. The induction hypothesis
states that there is an interpolant C with an appropriate ﬁrst-order language such
that Γ1→∆1, C(b) and C(b), Γ2→∆2, A(b). Since, by the eigenvariable condition,
b does not occur except as indicated; we get immediately LK -proofs of the sequents
Γ1→∆1, (∀y)C(y) and (∀y)C(y), Γ2→∆2, (∀x)A(x). Therefore, (∀y)C(y) serves
as an interpolant for the endsequent of P .
There are a number of other cases that must be considered, depending on the
type of the ﬁnal strong inference in P and on whether its principal formula is (an
ancestor of) a formula in ∆1, ∆2, Γ1 or Γ2. These cases are given in full detail in
textbooks such as Takeuti [1987] or Girard [1987b].
It remains to consider the case where there are function symbols in the language.
The usual method of handling this case is to just reduce it to the case where there
are no function symbols by removing function symbols in favor of predicate symbols
which deﬁne the graphs of the functions. Alternatively, one can carry out directly
a proof on induction on the number of strong inferences in P even when function
symbols are present. This involves a more careful analysis of the ‘ﬂow’ of terms in
the proofs, but still gives cut-free proofs P1 and P2 with size linear in the size of P .
We leave the details of the function-symbol case to the reader.
Other interpolation theorems. A useful strengthening of the Craig interpolation
theorem is due to Lyndon [1959]. This theorem states that Craig’s interpolation
theorem may be strengthened by further requiring that every predicate symbol which
occurs positively (resp., negatively) in C also occurs positively (resp., negatively) in
both A and B. The proof of Lyndon’s theorem is identical to the proof sketched
above, except that now one keeps track of positive and negative occurrences of
predicate symbols.
Craig [1957b] gives a generalization of the Craig interpolation theorem which
applies to interpolants of cedents.
Lopez-Escobar [1965] proved that the interpolation theorem holds for some in-
ﬁnitary logics. Barwise [1975,§III.6] proved that the interpolation theorem holds
for a wider class of inﬁnitary logics.
Lopez-Escobar’s proof was proof-theoretic,

58
S. Buss
based on a sequent calculus formalization of inﬁnitary logic. Barwise’s proof was
model-theoretic; Feferman [1968] gives a proof-theoretic treatment of these general
interpolation theorems, based on the sequent calculus.
2.5.6. Beth’s deﬁnability theorem
Deﬁnition. Let P and P ′ be predicate symbols with the same arity. Let Γ(P) be
an arbitrary set of ﬁrst-order sentences not involving P ′, and let Γ(P ′) be the same
set of sentences with every occurrence of P replaced with P ′.
The set Γ(P) is said to explicitly deﬁne the predicate P if there is a formula A(⃗c)
such that
Γ(P) ⊢(∀⃗x)(A(⃗x) ↔P(⃗x)).
The set Γ(P) is said to implicitly deﬁne the predicate P if
Γ(P) ∪Γ(P ′) ⊨(∀⃗x)(P(⃗x) ↔P ′(⃗x)).
The Deﬁnability Theorem of Beth [1953] states the fundamental fact that the
notions of explicit and implicit deﬁnability coincide. One way to understand the
importance of this is to consider implicit deﬁnability of P as equivalent to being
able to uniquely characterize P . Thus, Beth’s theorem states, loosely speaking, that
if a predicate can be uniquely characterized, then it can be explicitly deﬁned by a
formula not involving P .
One common, elementary mistake is to confuse implicit deﬁnability by a set of
sentences Γ(P) with implicit deﬁnability in a particular model. For example, consider
the theory T of sentences which are true in the standard model (N, 0, S, +) of natural
numbers with zero, successor and addition. One might attempt to implicitly deﬁne
multiplication in terms of zero and addition letting Γ(M) be the theory
T ∪{(∀x)(M(x, 0) = 0), (∀x)(∀y)(M(x, S(y)) = M(x, y) + x)}
It is true that this uniquely characterizes the multiplication function M(x, y) in the
sense that there is only one way to expand (N, 0, S, +) to a model of Γ(M); however,
this is not an implicit deﬁnition of M since there are nonstandard models of T which
have more than one expansion to a model of Γ(M).
Beth’s Deﬁnability Theorem. Γ(P) implicitly deﬁnes P if and only if it explicitly
deﬁnes P .
Proof. Beth’s theorem is readily proved from the Craig interpolation theorem as
follows.
First note that if P is explicitly deﬁnable, then it is clearly implicitly
deﬁnable. For the converse, assume that P is implicitly deﬁnable. By compactness,
we may assume without loss of generality that Γ(P) is a single sentence. Then we
have that
Γ(P) ∧P(⃗c) ⊨Γ(P ′) ⊃P ′(⃗c).
By the Craig Interpolation Theorem, there is a interpolant A(⃗c) for Γ(P)∧P(⃗c) and
Γ(P ′) ⊃P ′(⃗c). This interpolant is the desired formula explicitly deﬁning P . 2

Introduction to Proof Theory
59
It is also possible to prove the Craig Interpolation Theorem from the Beth Deﬁnability
Theorem. In addition, both theorems are equivalent to the model-theoretic Joint
Consistency Theorem of Robinson [1956].
2.6. First-order logic and resolution refutations
The importance of the resolution proof method for propositional logic (described
in section 1.3) lies in large part in the fact that it also serves as a foundation
for theorem-proving in ﬁrst-order logic. Recall that by introducing Herbrand and
Skolem functions, theorem-proving in ﬁrst-order logic can be reduced to proving
Π2-formulas of the form (∀⃗x)(∃⃗y)A(⃗x, ⃗y) with A quantiﬁer-free (see §2.5.2). Also,
by Herbrand’s Theorem 2.5.1, the problem of proving (∀⃗x)(∃⃗y)A(⃗x, ⃗y) is reducible
to the problem of ﬁnding terms ⃗r1,⃗r2, . . . ,⃗rk so that W
i A(⃗x,⃗ri) is tautologically
valid. Now, given the terms ⃗r1, . . . ,⃗rk, determining tautological validity is ‘merely’
a problem in propositional logic; and hence is amenable to theorem proving methods
such as propositional resolution. Thus one hopes that if one had a good scheme for
choosing terms ⃗r1, . . . ,⃗rk, then one could have a reasonable method of ﬁrst-order
theorem proving.
This latter point is exactly the problem that was solved by Robinson [1965b];
namely, he introduced the resolution proof method and showed that by using a
uniﬁcation algorithm to select terms, the entire problem of which terms to use could
be solved eﬃciently by using the “most general” possible terms. In essence, this
reduces the problem of ﬁrst-order theorem proving to propositional theorem proving.
(Of course, this last statement is not entirely true for two reasons: ﬁrstly, there may
be a very large number (not recursively bounded) of terms that are needed, and
secondly, it is entirely possible that foreknowledge of what terms are suﬃcient, might
help guide the search for a propositional proof.)
2.6.1. Uniﬁcation.
We shall now describe the uniﬁcation algorithm for ﬁnding
most general uniﬁers. We shall let t denote a term containing function symbols,
constant symbols and variables. A substitution, σ, is a partial map from variables
to terms; we write xσ to denote σ(x), and when x is not in the domain of σ, we
let xσ be x. If σ is a substitution, then tσ denotes the result of simultaneously
replacing every variable x in t with xσ.
We extend σ to atomic relations by
letting R(t1, . . . , tk)σ denote R(t1σ, . . . , tkσ). We use concatenation στ to denote
the substitution which is equivalent to an application of σ followed by an application
of τ .
Deﬁnition. Let A1, . . . , Ak be atomic formulas. A uniﬁer for the set {A1, . . . , Ak}
is a substitution σ such that A1σ = A2σ = · · · = Akσ, where = represents the
property of being the identical formula.
A substitution is said to be a variable renaming substitution, if the substitution
maps variables only to terms which are variables.

60
S. Buss
A uniﬁer σ is said to be a most general uniﬁer if, for every uniﬁer τ for the same
set, there is a uniﬁer ρ such that τ = σρ. Note that up to renaming of variables, a
most general uniﬁer must be unique.
Uniﬁcation Theorem. If {A1, . . . , Ak} has a uniﬁer then it has a most general
uniﬁer.
Proof. We shall prove the theorem by outlining an eﬃcient algorithm for determin-
ing whether a uniﬁer exists and, if so, ﬁnding a most general uniﬁer. The algorithm
is described as an iterative procedure which, at stage s has a set Es of equations
and a substitution σs. The equations in Es are of the form α .= β where α and β
may be formulas or terms. The meaning of this equation is that the sought-for most
general uniﬁer must be a substitution which makes α and β identical. Initially, E0 is
the set of k −1 equations Aj .= Aj+1 and σ0 is the identity. Given Es and σs, the
algorithm does any one of the following operations to choose Es+1 and σs+1:
(1) If Es is empty, we are done and σs is a most general uniﬁer.
(2) If Es contains an equation of the form
F(t1, . . . , ti) .= F(t′
1, . . . , t′
i),
then σs+1 = σs and Es+1 is obtained from Es by removing this equation and
adding the i equations ti .= t′
i. Here F is permitted to be a function symbol, a
constant symbol or a predicate symbol.
(3) Suppose Es contains an equation of the form
x .= t
or
t .= x,
with x a variable and t a term. Firstly, if t is equal to x, then this equation
is discarded, so Es+1 is Es minus this equation and σs+1 = σs. Secondly, if
t is a non-trivial term in which x occurs, then the algorithm halts outputting
that no uniﬁer exists.7 Thirdly, if x does not occur in t, then let [x/t] denote
the substitution that maps x to t and deﬁne Es+1 to be the set of equations
s[x/t] .= s′[x/t] such that s .= s′ is in Es, and deﬁne σs+1 = σs[x/t].
We leave it to the reader to prove that this algorithm always halts with a most
general uniﬁer if a uniﬁer exists. The fact that the algorithm does halt can be
proved by noting that each iteration of the algorithm either reduces the number
of distinct variables in the equations, or reduces the sum of the lengths of the
terms occurring in the equations. 2
The algorithm as given above is relatively eﬃcient;
however,
the sizes of
the terms involved may grow exponentially large.
Indeed, there are uniﬁ-
cation problems where the size of the most general uniﬁer is exponential in
the size of the uniﬁcation problem; for example, the most general uniﬁer for
{f(x1, x2, . . . xk), f(g(x2, x2), . . . , g(xk+1, xk+1))} maps x1 to a term of height k with
2k atoms.
7This failure condition is known as the occurs check.

Introduction to Proof Theory
61
If one is willing to use a dag (directed acyclic graph) representation for terms,
then this exponential growth rate does not occur. Paterson and Wegman [1978] have
given an eﬃcient linear-time uniﬁcation algorithm based on representing terms as
dags.
2.6.2. Resolution and factoring inferences.
We now describe the resolution
inference used by Robinson [1965b] for ﬁrst-order logic. The starting point is Her-
brand’s theorem: we assume that we are attempting to prove a ﬁrst-order sentence,
which without loss of generality is of the form (∃⃗x)A(⃗y). (This may be assumed
without loss of generality since, if necessary, Herbrand functions may be introduced.)
We assume, in addition, that A is in disjunctive normal form. Instead of proving this
sentence, we shall instead attempt to refute the sentence (∀⃗x)(¬A(x)). Since A is
in disjunctive normal form, we may view ¬A as a set ΓA of clauses with the literals
in the clauses being atomic formulas or negated atomic formulas. We extend the
deﬁnition of substitutions to act on clauses in the obvious way so that {C1, . . . , Ck}σ
is deﬁned to be {C1σ, . . . , Ckσ}.
When we refute (∀⃗x)(¬A), we are showing that there is no structure M which
satisﬁes (∀⃗x)(¬A). Consider a clause C in ΓA. The clause C is a set of atomic
and negated atomic formulas, and a structure M is said to satisfy C provided it
satisﬁes (∀⃗x)(W
φ∈C φ(x)). Thus, it is immediate that if M satisﬁes C , and if σ is
a substitution, then M also satisﬁes Cσ.
From this, we see that the following
version of resolution is sound in that it preserves the property of being satisﬁed by
a model M : If B and C are clauses, if φ is an atomic formula and if σ and τ are
substitutions, let D be the clause (Bσ \ {φ}) ∪(Cτ \ {φ}). It is easy to verify that
if B and C are satisﬁed in M , then so is D.
Following Robinson [1965b], we use a restricted form of this inference principle as
the sole rule of inference for ﬁrst-order resolution refutations:
Deﬁnition. Let B and C be clauses and suppose P(⃗s1), P(⃗s2), . . . P(⃗sk) are atomic
formulas in B and that ¬P(⃗t1), ¬P(⃗t2), . . . ¬P(⃗tℓ) are negated atomic formulas in C .
Choose a variable renaming substitution τ so that Cτ has no variables in common
with B. Also suppose that the k + ℓformulas P(⃗si) and P(⃗ti)τ have a most general
uniﬁer σ. Then the clause D deﬁned by
(Bσ \ {P(⃗s1)σ}) ∪(Cτσ \ {¬P(⃗s1)τσ})
is deﬁned to be an R-resolvent of B and C .
The reason for using the renaming substitution τ is that the variables in the clauses
B and C are implicitly universally quantiﬁed; thus if the same variable occurs in
both B and C we allow that variable to be instantiated in B by a diﬀerent term
than in C when we perform the uniﬁcation. Applying τ before the uniﬁcation allows
this to happen automatically.
One often views R-resolution as the amalgamation of two distinct operations:
ﬁrst, the factoring operation ﬁnds a most general uniﬁer of a subset of clause, and

62
S. Buss
second, the unitary resolution operation which resolves two clauses with respect to
a single literal. Thus, R-resolution consists of (a) choosing subsets of the clauses
B and C and factoring them, and then (b) applying resolution w.r.t. to the literal
obtained by the factoring.
Completeness of R-resolution. A set Γ of ﬁrst-order clauses is unsatisﬁable if
and only if the empty clause can be derived from Γ by R-resolution.
This theorem is proved by the discussion in the next paragraph.
2.6.3. Lifting ground resolution to ﬁrst-order resolution.
A ground literal is
deﬁned to be a literal in which no variables occur; a ground clause is a set of ground
literals. We assume, with no loss of generality, that our ﬁrst-order language contains
a constant symbol and that therefore ground literals exist.
Ground literals may
independently be assigned truth values8 and therefore play the same role that literals
played in propositional logic. A ground resolution refutation is a propositional-style
refutation involving ground clauses only, with ground literals in place of propositional
literals. By the Completeness Theorem 1.3.4 for propositional resolution, a set of
ground clauses is unsatisﬁable if and only if it has a ground resolution refutation.
For sets of ground clauses, R-resolution is identical to propositional-style resolu-
tion. Suppose, however, that Γ is an unsatisﬁable set of ﬁrst-order (not necessarily
ground) clauses. Since Γ is unsatisﬁable there is, by Herbrand’s theorem, a set of
substitutions σ1, . . . , σr so that each Γσr is a set of ground clauses and so that the
set Π = S
i Γσi of clauses is propositionally unsatisﬁable. Therefore there is a ground
resolution refutation of Π.
To justify the completeness of R-resolution, we shall show that any ground
resolution refutation of Π can be ‘lifted’ to an R-resolution refutation of Γ. In fact,
we shall prove the following: if C1, C2, . . . , Cn = ∅is a resolution refutation of Π,
then there are clauses D1, D2, . . . , Dm = ∅which form an R-resolution refutation
of Γ and there are substitutions σ1, σ2, . . . , σm so that Diσi = Ci. We deﬁne Di and
σi by induction on i as follows. Firstly, if Ci ∈Π, then it must be equal to Diσi
for some Di ∈Γ and some substitution σi by the deﬁnition of Π. Secondly, if Ci is
inferred from Cj and Ck, with j, k < i by resolution w.r.t. the literal P(⃗r), then
deﬁne Ej to be the subset of Dj which is mapped to P(⃗r) by σj , and deﬁne Ek
similarly. Now, form the R-resolution inference which factors the subsets Ej and Ek
of Dj and Dk and forms the resolvent. This resolvent is Di and it is straightforward
to show that the desired σi exists.
That ﬁnishes the proof of the Completeness Theorem for R-resolution. It should
be noted that the method of proof shows that R-resolution refutations are the shortest
possible refutations, even if arbitrary substitutions are allowed for factoring infer-
ences. Even more importantly, the method by which ground resolution refutations
were ‘lifted’ to R-resolution refutations preserves many of the search strategies that
8We are assuming that the equality sign (=) is not present.

Introduction to Proof Theory
63
were discussed in section 1.3.5. This means that these search strategies can be used
for ﬁrst-order theorem proving.9
2.6.4. Paramodulation.
The above discussion of R-resolution assumed that
equality was not present in the language.
In the case where equality is in the
language, one must either add additional initial clauses as axioms that express the
equality axioms or one must add additional inference rules. For the ﬁrst approach,
one could add clauses which express the equality axioms from section 2.2.1; for
instance the third equality axiom can be expressed with the clause
{x1 ̸= y1, ..., xk ̸= yk, ¬P(⃗x), P(⃗y)},
and the other equality axioms can similarly be expressed as clauses. More computa-
tional eﬃciency can be obtained with equality clauses of the form
{x ̸= y, A(x), A(y)}
where A(x) indicates an arbitrary literal.
For the second approach, the paramodulation inference is used instead of equality
clauses; this inference is a little complicated to deﬁne, but goes as follows: Suppose B
and C are clauses with no free variables in common and that r = s is a literal in C ;
let t be a term appearing somewhere in B and let σ be a most general uniﬁer of
r and t (or of s and t); let B′ be the clause which is obtained from Bσ by replacing
occurrences of tσ with sσ (or with rσ, respectively) and let C′ be (C \ {r = s})σ.
Under these circumstances, paramodulation allows B′ ∪C′ to be inferred from
B and C . Paramodulation was introduced and shown to be complete by Robinson
and Wos [1969] and Wos and Robinson [1973]: for completeness, paramodulation
must be combined with R-resolution, with factoring and with application of variable
renaming substitutions.
2.6.5. Horn clauses.
An important special case of ﬁrst-order resolution is when
the clauses are restricted to be Horn clauses. The propositional refutation search
strategies described in section 1.3.5.6 still apply; and, in particular, an unsatisﬁable
set Γ of Horn clauses always has a linear refutation supported by a negative clause
in Γ. In addition, the factoring portion of R-resolution is not necessary in refutations
of Γ.
A typical use of Horn clause refutations is as follows: a set ∆of Horn clauses is
assumed as a ‘database’ of knowledge, such that every clause in ∆contains a positive
literal. A query, which is an atomic formula P(s1, ..., sk), is chosen; the object is to
determine if there is an instance of P(⃗s) which is a logical consequence of ∆. In
other words, the object is to determine if (∃⃗x)P(⃗s) is a consequence of ∆where ⃗x is
the vector of variables in P(⃗s). To solve this problem, one forms the clause {P(⃗s)}
9Historically, it was the desire to ﬁnd strategies for ﬁrst-order theorem proving and the ability to
lift results from propositional theorem proving, that motivated the research into search strategies
for propositional resolution.

64
S. Buss
and lets Γ be the set ∆∪{P(⃗s)}; one then searches for a linear refutation of Γ
which is supported by P(⃗s). If successful, such a linear refutation R also yields a
substitution σ, such that ∆⊢P(⃗s)σ; and indeed, σ is the most general substitution
such that R gives a refutation of ∆∪{P(⃗s)σ}. From this, what one actually has is a
proof of (∀⃗y)(P(⃗s)σ) where ⃗y is the vector of free variables in the terms P(⃗s)σ. Note
that there may be more than one refutation, and that diﬀerent refutations can give
diﬀerent substitutions σ, so there is not necessarily a unique most general uniﬁer.
What we have described is essentially a pure form of PROLOG, which is a logic
programming language based on searching for refutations of Horn clauses, usually in
a depth-ﬁrst search. PROLOG also contains conventions for restricting the order of
the proof search procedure.
For further reading. There is an extensive literature on logic programming, au-
tomated reasoning and automated theorem proving which we cannot survey here.
The paper of Robinson [1965b] still provides a good introduction to the foundations
of logic programming; the textbooks of Chang and Lee [1973] and Loveland [1978]
provide a more detailed treatment of the subject matter above, and the textbooks of
Kowalski [1979] and Clocksin and Mellish [1981] provide good detailed introductions
to logic programming and PROLOG. Chapter IX, by G. J¨ager and R. St¨ark, discusses
the proof-theory and semantics of extensions of logic programming to non-Horn
clauses.
3. Proof theory for other logics
In the ﬁnal section of this chapter, we shall brieﬂy discuss two important non-
classical logics, intuitionistic logic and linear logic.
3.1. Intuitionistic logic
Intuitionistic logic is a subsystem of classical logic which historically arose out
of various attempts to formulate a more constructive foundation for mathematics.
For example, in intuitionistic logic, the law of the excluded middle, A ∨¬A, does
not hold in general; furthermore, it is not possible to intuitionistically prove A ∨B
unless already at least one of A or B is already intuitionistically provable. We shall
discuss below primarily mathematical aspects of the intuitionistic logic, and shall
omit philosophical or foundational issues: the books of Troelstra and van Dalen [1988]
provide a good introduction to the latter aspects of intuitionistic logic.
The intuitionistic sequent calculus, LJ , is deﬁned similarly to the classical sequent
calculus LK , except with the following modiﬁcations:
(1) To simplify the exposition, we adopt the convention that negation (¬) is not a
propositional symbol. In its place, we include the absurdity symbol ⊥in the
language; ⊥is a nullary propositional symbol which is intended to always have
value False. The two ¬ rules of LK are replaced with the single ⊥:left initial
sequent, namely ⊥→. Henceforth, ¬A is used as an abbreviation for A ⊃⊥.

Introduction to Proof Theory
65
(2) In LJ , the ∨:right rule used in the deﬁnition of LK in section 1.2.2 is replaced
by the two rules
Γ→∆, A
Γ→∆, A ∨B
and
Γ→∆, A
Γ→∆, B ∨A
(3) Otherwise, LJ is deﬁned like LK , except with the important proviso that at
most one formula may appear in the antecedent of any sequent. In particular,
this means that rules which have the principal formula to the right of the
sequent arrow may not have any side formulas to the right of the sequent
arrow.
3.1.1. Cut elimination.
An important property of intuitionistic logic is that the
cut elimination and free-cut elimination theorems still apply:
Theorem.
(1) Let Γ→A be LJ -provable. Then there is a cut-free LJ -proof of Γ→A.
(2) Let S be a set of sequents closed under substitution such that each sequent in S
has at most one formula in its antecedent. Let LJS be LJ augmented with initial
sequents from S. If LKS ⊢Γ→A, then there is a free-cut free LKS-proof of
Γ→A.
The (free) cut elimination theorem for LJ can be proved similarly to the proof-
theoretic proofs used above for classical logic.
3.1.2. Constructivism and intuitionism.
Intuitionistic logic is intended to
provide a ‘constructive’ subset of classical logic:
that is to say, it is designed
to not allow non-constructive methods of reasoning.
An important example of
the constructive aspect of intuitionistic logic is the Brouwer-Heyting-Kolmogorov
(BHK) constructive interpretation of logic. In the BHK interpretation, the logical
connectives ∨, ∃and ⊃have non-classical, constructive meanings. In particular, in
order to have a proof of (or knowledge of) a statement (∃x)A(x), it is necessary to
have a particular object t and a proof of (or knowledge of) A(t). Likewise, in order
to have a proof of A ∨B the BHK interpretation requires one to have a proof either
of A or of B, along with a indication of which one it is a proof of. In addition, in
order to have a proof of A ⊃B, one must have a method of converting any proof of A
into a proof B. The BHK interpretation of the connectives ∧and ∀are similar to
the classical interpretation; thus, a proof of A ∧B or of (∀x)A(x), consists of proofs
of both A and B or of a method of constructing a proof of A(t) for all objects t.
It is not diﬃcult to see that the intuitionistic logic LJ is sound under the BHK
interpretation, in that any LJ -provable formula has a proof in the sense of the BHK
interpretation.
The BHK interpretation provides the philosophical and historical underpinnings
of intuitionistic logic; however, it has the drawback of characterizing what constitutes
a valid proof rather than characterizing the meanings of the formulas directly. It
is possible to extend the BHK interpretation to give meanings to formulas directly,

66
S. Buss
e.g., by using realizability; under this approach, an existential quantiﬁer (∃x) might
mean “there is a constructive procedure which produces x, and (possibly) produces
evidence that x is correct”. This approach has the disadvantage of requiring one to
pick a notion of ‘constructive procedure’ in an ad-hoc fashion; nonetheless it can be
very fruitful in applications such a intuitionistic theories of arithmetic where there
are natural notions of ‘constructive procedure’.
For pure intuitionistic logic, there is a very satisfactory model theory based
on Kripke models. Kripke models provide a semantics for intuitionistic formulas
which is analogous to model theory for classical logic in many ways, including a
model theoretic proof of the cut-free completeness theorem, analogous to the proof
of Theorem 2.3.7 given above.
For an accessible account of the BHK interpretation and Kripke model semantics
for intuitionistic logic, the reader can refer to Troelstra and van Dalen [1988,vol. 1];
Chapter VI contains an thorough discussion of realizability.
In this section we
shall give only the following theorem of Harrop, which generalizes the statements
that A ∨B is intuitionistically valid if and only if either A or B is, and that
(∃x)A(x) is intuitionistically valid if and only if there is a term t such that A(t) is
intuitionistically valid.
3.1.3. Deﬁnition.
The Harrop formulas are inductively deﬁned by:
(1) Every atomic formula is a Harrop formula,
(2) If B is a Harrop formula and A is an arbitrary formula, then A ⊃B is a Harrop
formula.
(3) If B is a Harrop formula, then (∀x)B is a Harrop formula.
(4) If A and B are Harrop formulas, then so is A ∧B.
Intuitively, a Harrop formula is a formula in which all existential quantiﬁers and all
disjunctions are ‘hidden’ inside the left-hand scope of an implication.
Theorem. (Harrop [1960]) Let Γ be a cedent containing only Harrop formulas, and
let A and B be arbitrary formulas.
(a) If LJ ⊢Γ→(∃x)B(x), then there exists a term t such that LJ proves Γ→B(t).
(b) If LJ ⊢Γ→A ∨B, then at least one of Γ→A and Γ→B is LJ -provable.
We omit the proof of the theorem, which is readily provable by using induction on
the length of cut-free LJ -proofs.
3.1.4. Interpretation into classical logic.
The ‘negative translation’ provides
a translation of classical logic into intuitionistic logic; an immediate corollary of the
negative translation is a simple, constructive proof of the consistency of classical
logic from the consistency of intuitionistic logic.
There are a variety of negative
translations: the ﬁrst ones were independently discovered by Kolmogorov, G¨odel
and Gentzen.

Introduction to Proof Theory
67
Deﬁnition. Let A be a formula. The negative translation, A−, of A is inductively
deﬁned by:
(1) If A is atomic, then A−is ¬¬A,
(2) (¬B)−is ¬(B−).
(3) (B ∧C)−is (B−) ∧(C−),
(4) (B ⊃C)−is (B−) ⊃(C−),
(5) (B ∨C)−is ¬(¬(B−) ∧¬(C−)),
(6) (∀xB)−is ∀x(B−),
(7) (∃xB)−is ¬∀x(¬(B−)),
Theorem. Let A be a formula. Then LK ⊢A if and only if LJ ⊢A−.
Clearly A−is classically equivalent to A, so if A−is intuitionistically valid, then
A is classically valid.
For the converse, we use the following two lemmas which
immediately imply the theorem.
Lemma. Let A be a formula. Then LJ proves ¬¬A−→A−.
Proof. The proof is by induction on the complexity of A.
We will do only
one of the more diﬃcult cases.
Suppose A is B ⊃C .
We need to show
that LJ proves ¬¬(B−⊃C−)→B−⊃C−, for which, it suﬃces to prove
B−, ¬¬(B−⊃C−)→C−. By the induction hypothesis, LJ proves ¬¬C−→C−,
so it will suﬃce to show that B−, ¬¬(B−⊃C−), ¬C−→is LJ -provable. To do
this it suﬃces to show that B−, B−⊃C−→C−is LJ -provable, which is easy to
prove. 2
If Γ is a cedent B1, . . . , Bk, then Γ−denotes the cedent B−
1 , . . . , B−
k
and ¬Γ−
denotes the cedent ¬B−
1 , . . . , ¬B−
k .
Lemma. Suppose that LK
proves Γ→∆.
Then LJ
proves the sequent
Γ−, ¬∆−→.
Proof. The proof of the lemma is by induction on the number of inferences in an
LK -proof, possibly containing cuts. We’ll do only one case and leave the rest to the
reader. Suppose the LK -proof ends with the inference
B, Γ→∆, C
Γ→∆, B ⊃C
The induction hypothesis is that B−, Γ−, ¬∆−, ¬C−→is provable in LJ .
By
the previous lemma, the induction hypothesis implies that B−, Γ−, ¬∆−→C−is
provable, and from this it follows that Γ−, ¬∆−, ¬(B−⊃C−)→is also LJ -
provable, which is what we needed to show. 2

68
S. Buss
One consequence of the proof of the above theorem is a constructive proof that
the consistency of intuitionistic logic is equivalent to the consistency of classical
logic; thus intuitionistic logic does not provide a better foundations for mathematics
from the point of view of sheer consistency. This is, however, of little interest to
a constructivist, since the translation of classical logic into intuitionistic logic does
not preserve the constructive meaning (under, e.g., the BHK interpretation) of the
formula.
It is possible to extend the negative translation to theories of arithmetic and to
set theory; see, e.g., Troelstra and van Dalen [1988,vol. 1].
3.1.5. Formulas as types.
Section 3.1.2 discussed a connection between intu-
itionistic logic and constructivism. An important strengthening of this connection is
the Curry-Howard formulas-as-types isomorphism, which provides a direct correspon-
dence between intuitionistic proofs and λ-terms representing computable functions,
under which intuitionistic proofs can be regarded as computable functions. This
section will discuss the formulas-as-types isomorphism for intuitionistic propositional
logic; our treatment is based on the development by Howard [1980]. Girard [1989]
contains further material, including the formulas-as-types isomorphism for ﬁrst-order
intuitionistic logic.
The {⊃}-fragment.. We will begin with the fragment of propositional logic in
which the only logical connective is ⊃. We work in the sequent calculus, and the
only strong rules of inference are the ⊃:left and ⊃:right rules. Firstly we must deﬁne
the set of types:
Deﬁnition. The types are deﬁned as follows:
(a) Any propositional variable pi is a type.
(b) If σ and τ are types, then (σ →τ) is a type.
If we identify the symbols ⊃and →, the types are exactly the same as formulas, since
⊃is the only logical connective allowed. We shall henceforth make this identiﬁcation
of types and formulas without comment.
Secondly, we must deﬁne the terms of the λ-calculus; each term t has a unique
associated type. We write tτ to mean that t is a term of type τ .
Deﬁnition. For each type τ , there is an inﬁnite set of variables, xτ
1, xτ
2, xτ
3, . . . of
type τ . Note that if σ ̸= τ , then xσ
i and xτ
i are distinct variables.
The terms are inductively deﬁned by:
(a) Any variable of type τ is a term of type τ .
(b) If sσ→τ and tσ are terms of the indicated types, then (st) is a term of type τ .
This term may also be denoted (st)τ or even (sσ→τtσ)τ .
(c) If tτ is a term then λxσ.t is a term of type σ →τ . This term can also be denoted
(λxσ.t)σ→τ or (λxσ.tτ)σ→τ

Introduction to Proof Theory
69
Traditionally, a type σ →τ is viewed as a set of mappings from the objects
of type σ into the objects of type τ . For intuitionistic logic, it is often useful to
think of a type σ as being the set of proofs of the formula σ. Note that under the
BKH-interpretation this is compatible with the traditional view of types as a set of
mappings.
The λx connective serves to bind occurrences of x; thus one can deﬁne the
notions of bound and free variables in terms in the obvious way. A term is closed
provided it has no free variables. The computational content of a term is deﬁned
by letting (st) represent the composition of the terms s and t, and letting λxσ.t
represent the mapping that takes objects d of type τ to the object t(x/d). Clearly
this gives a constructive computational meaning to terms.
Theorem. Let B be a formula. LJ ⊢B if and only if there is a closed term of
type B.
An example of this theorem is illustrated by the closed term K deﬁned as
λx.λy.x where x and y are of type σ and τ , respectively, and therefore the term
K has type σ →(τ →σ), which is a valid formula. A second important example
is the closed term S which is deﬁned by λx.λy.λz.((xz)(yz)) where the types of x,
y and z are σ →(τ →µ), σ →τ and σ, respectively, and therefore, S has type
(σ →(τ →µ)) →((σ →τ) →(σ →µ)) which is a valid formula. The terms K
and S are examples of combinators.
The import of this theorem is that a closed term of type B corresponds to a proof
of B. We shall brieﬂy sketch the proof of this for the sequent calculus; however, the
correspondence between closed terms and natural deduction proofs is even stronger,
namely, the closed terms are isomorphic to natural deduction proofs in intuitionistic
logic (see Girard [1989]). To prove the theorem, we prove the following lemma:
Lemma. LJ proves the sequent A1, . . . , An→B if and only if there is a term tB of
the indicated type involving only the free variables xA1
1 , . . . , xAn
n .
Proof. It is straightforward to prove, by induction on the complexity of tB , that
the desired LJ -proof exists. For the other direction, use induction on the length of
the LJ -proof to prove that the term tB exists. We will consider only the case where
the last inference of the LJ -proof is
Γ→A
B, Γ→C
A ⊃B, Γ→C
The induction hypotheses give terms rA(xΓ) and sC(xB
1 , xΓ) where xΓ actually
represents a vector of variables, one for each formula in Γ.
The desired term
tC(xA⊃B
2
, xΓ) is deﬁned to be sC((xA⊃B
2
rA(xΓ)), xΓ). 2
The {⊃, ⊥}-fragment. The above treatment of the formulas-as-types isomorphism
allowed only the connective ⊃. The formulas-as-types paradigm can be extended
to allow also the symbol ⊥and thereby negation, by making the following changes.

70
S. Buss
Firstly, enlarge the deﬁnition of types, by adding a clause specifying that ∅is a type.
Identifying ∅with ⊥makes types identical to formulas. The type ∅corresponds to
the empty set; this is consistent with the BHK interpretation since ⊥has no proof.
Secondly, enlarge the deﬁnition of terms by adding to the deﬁnition a new clause
stating that, for every type σ, there is a term f ∅→σ of type ∅→σ.
Now the Theorem and Lemma of section 3.1.5 still hold verbatim for formulas
with connectives {⊃, ⊥} and with the new deﬁnitions of types and terms.
The {⊃, ⊥, ∧}-fragment. To further expand the formulas-as-types paradigm to
intuitionistic logic with connectives ⊃, ⊥and ∧, we must make the following
modiﬁcations to the deﬁnitions of terms and types. Firstly, add to the deﬁnition
of types, a clause stating that if σ and τ are types, then (σ × τ) is a type. By
identifying × with ∧, we still have the types identiﬁed with the formulas. Consistent
with the BHK-interpretation, the type (σ × τ) may be viewed as the cross-product
of its constituent types. Secondly, add to the deﬁnition of terms the following two
clauses: (i) if sσ and tτ are terms, then ⟨s, t⟩is a term of type σ × τ , and (ii) if
sσ×τ is a term, then (π1s)σ and (π2s)τ are terms. The former term uses the pairing
function; and the latter terms use the projection functions.
Again, the Theorem and Lemma above still holds with these new deﬁnitions of
types and terms.
The {⊃, ⊥, ∧, ∨}-fragment. To incorporate also the connective ∨into the
formulas-as-types isomorphism we expand the deﬁnitions of types and terms as
follows.
Firstly, add to the deﬁnition of types that if σ and τ are types, then
(σ +τ) is a type. To identify formulas with types, the symbol + is identiﬁed with ∨.
The type σ + τ is viewed as the disjoint union of σ and τ , consistent with the
BHK-interpretation.
Secondly, add to the deﬁnitions of terms the following two
clauses
• If sσ and tτ are terms of the indicated types, then (ισ→(σ+τ)
1
s) and (ιτ→(σ+τ)
2
t)
are terms of type σ + τ .
• For all types σ, τ and µ, there is a constant symbol d(σ→µ)→((τ→µ)→((σ+τ)→µ))
of the indicated type. In other words, if sσ→µ and tτ→µ are terms, then (dst)
is a term of type (σ + τ) →µ.
Once again, the Theorem and Corollary of section 3.1.5 holds with these enlarged
deﬁnitions for types and terms.
3.2. Linear logic
Linear logic was ﬁrst introduced by Girard [1987a] as a reﬁnement of classical
logic, which uses additional connectives and in which weakening and contractions
are not allowed. Linear logic is best understood as a ‘resource logic’ in that proofs
in linear logic must use each formula exactly once. In this point view, assumptions
in a proof are viewed as resources; each resource must be used once and only once
during the proof. Thus, loosely speaking, an implication A ⊢B can be proved in

Introduction to Proof Theory
71
linear logic only if A is exactly what is needed to prove B; thus if the assumption A
is either too weak or too strong, it is not possible to give a linear logic proof of ‘if
A then B’. As an example of this, A ⊢B being provable in linear logic does not
generally imply that A, A ⊢B is provable in linear logic; this is because A ⊢B is
asserting that one use of the resource A gives B, whereas A, A ⊢B asserts that two
uses of the resource A gives B.
We shall introduce only the propositional fragment of linear logic, since already
the main features of linear logic are present in its propositional fragment. We are
particularly interested in giving a intuitive understanding of linear logic; accordingly,
we shall frequently make vague or even not-quite-true assertions, sometimes, but not
always, preceded by phrases such as “loosely speaking” or “intuitively”, etc.
Linear logic will be formalized as a sequent calculus.10 The initial logical sequents
are just A→A; the weakening and contraction structural rules are not allowed and
the only structural rules are the exchange rule and the cut rule. Since contraction is
not permitted, this prompts one to reformulate the rules, such as ∧:right and ∨:left,
which contain implicit contraction of side formulas.
To begin with conjunction,
linear logic has two diﬀerent diﬀerent conjunction symbols, denoted ⊗and &: the
multiplicative connective ⊗does not allow contractions on side formulas, whereas
the additive conjunction & does.
The rules of inference for the two varieties of
conjunction are:
A, B, Γ→∆
⊗:left A ⊗B, Γ→∆
Γ1→∆1, A
Γ2→∆2, B
⊗:right
Γ1, Γ2→∆1, ∆2, A ⊗B
and
A, Γ→∆
&:left A&B, Γ→∆
Γ→∆, A
Γ→∆, B
&:right
Γ→∆, A&B
B, Γ→∆
&:left A&B, Γ→∆
The obvious question now arises of what the diﬀerence is in the meanings of the two
diﬀerent conjunctions ⊗and &. For this, one should think in terms of resource
usage. Consider ﬁrst the ⊗:left inference: the upper sequent contains both A and B
so that resources for both A and B are available. The lower sequent contains A ⊗B
in their stead; the obvious intuition is that the resource (for) A ⊗B is equivalent
to having resources for both A and B. Thus, loosely speaking, we translate ⊗as
meaning “both”. Now consider the &:left inference; here the lower sequent has the
principal formula A&B in place of either A or B, depending on which form of the
inference is used. This means that a resource A&B is equivalent to having a resource
for A or B, whichever is desired. So we can translate & as “whichever”.
The translations “both” and “whichever” also make sense for the other inferences
for the conjunctions.
Consider the ⊗:right inference and, since we have not yet
discussed the disjunctions, assume ∆1 and ∆2 are empty. In this case, the upper
hypotheses say that from a resource Γ1 one can obtain A, and from a resource Γ2
10The customary convention for linear logic is to use “⊢” as the symbol for the sequent
connection; we shall continue to use the symbol “→” however.

72
S. Buss
one can obtain B. The conclusion then says that from (the disjoint union of) both
these resources, one obtains both A and B. Now consider a &:right inference
Γ→A
Γ→B
Γ→A&B
For the lower sequent, consider an agent who has the responsibility of providing
(outputting) a resource A&B using exactly the resource Γ. To provide the resource
A&B, the agent must be prepared to either provide a resource A or a resource B,
whichever one is needed; in either case, there is an upper sequent which says the
agent can accomplish this.
Linear logic also has two diﬀerent forms of disjunction: the multiplicative dis-
junction
and the additive disjunction ⊕. The former connective is dual to ⊗and
the latter is dual to &. Accordingly the rule of inference for the disjunctions are:
A, Γ1→∆1
B, Γ2→∆2
:left
AB, Γ1, Γ2→∆1, ∆2
Γ→∆, A, B
:right Γ→∆, AB
and
A, Γ→∆
B, Γ→∆
⊕:left
A ⊕B, Γ→∆
Γ→∆, A
⊕:right Γ→∆, A ⊕B
Γ→∆, B
⊕:right Γ→∆, A ⊕B
By examining these inferences, we can see that the multiplicative disjunction, , is a a
kind of ‘indeterminate OR’ or ‘unspeciﬁc OR’, in that a resource for AB consists of
a either a resource either for A or for B, but without a speciﬁcation of which one it
is a resource for. On the other hand, the additive conjunction, ⊕, is a ‘determinate
OR’ or a ‘speciﬁc OR’, so that a resource for A ⊕B is either a resource for A
or a resource for B together with a speciﬁcation of which one it is a resource for.
To give a picturesque example, consider a military general who has to counter an
invasion which will come either on the western front or on the eastern front. If the
connective ‘or’ is the multiplicative or, then the general needs suﬃcient resources to
counter both threats, whereas if the ‘or’ is an additive disjunction then the general
needs only enough resources to counter either threat. In the latter case, the general
is told ahead of time where the invasion will occur and has the ability to redeploy
resources so as to counter the actual invasion; whereas in the former case, the general
must separately deploy resources suﬃcient to counter the attack from the west and
resources suﬃcient to counter the attack from the east. From the rules of inference
for disjunction, it is clear that the commas in the succedent of a sequent intended to
be interpreted as the unspeciﬁc or, .
The linear implication symbol, ⊸, is deﬁned by letting A ⊸B be an abbrevia-
tion for A⊥B, where A⊥is deﬁned below.
Linear logic also has two nullary connectives, 1 and ⊤, for truth and two nullary
connectives, 0 and ⊥, for falsity.
Associated to these connectives are the initial
sequents →1 and 0, Γ→∆, and the one rule of inference:
Γ→∆
1, Γ→∆

Introduction to Proof Theory
73
The intuitive idea for the multiplicatives is that 1 has the null resource and that
there is no resource for ⊥. For the additive connectives, ⊤has a ‘arbitrary’ resource,
which means that anything is a resource for ⊤, whereas any resource for 0 is a ‘wild
card’ resource which may be used for any purpose.
In addition, linear logic has a negation operation A⊥which is involutive in that
(A⊥)⊥is the same as formula A. For each propositional variable p, p⊥is the negation
of p. The operation of the negation is inductively deﬁned by letting (A ⊗B)⊥be
A⊥B⊥, (A&B)⊥be A⊥⊕B, 1⊥be ⊥, and ⊤⊥be 0. This plus the involution
property deﬁnes the operation A⊥for all formulas A. The two rules of inference
associated with negation are
A, Γ→∆
⊥:right
Γ→∆, A⊥
and
Γ→∆, A
⊥:left A⊥, Γ→∆
Deﬁnition. The multiplicative/additive fragment of propositional linear logic is
denoted MALL and contains all the logical connectives, initial sequents and rules of
inference discussed above.
The absence of weakening and contraction rules, makes the linear logic sequent
calculus particularly well behaved; in particular, the cut elimination theorem for
MALL is quite easy to prove. In fact, if P is a MALL proof, then there is a shorter
MALL proof P ∗with the same endsequent which contains no cut inferences.
In addition, the number of strong inferences in P ∗is bounded by the number
of connectives in the endsequent. Bellin [1990] contains an in-depth discussion of
MALL and related systems.
As an exercise, consider the distributive laws A ⊗(B ⊕C)→(A ⊗B) ⊕(A ⊗B)
and (A⊗B)⊕(A⊗B)→A⊗(B ⊕C). The reader should check that these are valid
under the intuitive interpretation of the connectives given above; and, as expected,
they can be proved in MALL. Similar reasoning shows that & is distributive over .
On the other hand, ⊕is not distributive over ; this can be seen intuitively by
considering the meanings of the connectives, or formally by using the cut elimination
theorem.
MALL is not the full propositional linear logic.
Linear logic (LL) has, in
addition, two modalities ! and ? which allow contraction and weakening to be used
in certain situations. The negation operator is extended to LL by deﬁning (!A)⊥to
be ?(A⊥). The four rules of inference for ! are:
Γ→∆
!:weakening0 !A, Γ→∆
A, Γ→∆
!:weakening1 !A, Γ→∆
!A, !A, Γ→∆
!:contraction
!A, Γ→∆
!Γ→?∆, A
!Γ→?∆, !A
where in the last inference, !Γ (respectively, ?∆) represents a cedent containing only
formulas beginning with the ! symbol (respectively, the ? symbol).
The dual rules for ? are obtained from these using the negation operation. One
should intuitively think of a resource for !A as consisting of zero or more resources

74
S. Buss
for A. The nature of full linear logic with the modalities is quite diﬀerent from that
of either MALL or propositional classical logic; in particular, Lincoln et al. [1992]
show that LL is undecidable.
The above development of the intuitive meanings of the connectives in linear logic
has not been as rigorous as one would desire; in particular, it would be nice have a
completeness theorem for linear logic based on these intuitive meanings. This has
been achieved for some fragments of linear logic by Blass [1992] and Abramsky and
Jagadeesan [1994], but has not yet been attained for the full propositional linear
logic.
In addition to the references above, more information on linear logic may
be found in Troelstra [1992], although his notation is diﬀerent from the standard
notation we have used.
Acknowledgements. We are grateful to S. Cook, E. Gertz, C. Guti´errez, A. Jonas-
son, C. Lautemann, A. Maciel, R. Parikh C. Pollett, R. St¨ark, J. Tor´an and S. Wainer
for reading and suggesting corrections to preliminary versions of this chapter. Prepa-
ration of this article was partially supported by NSF grant DMS-9503247 and by
cooperative research grant INT-9600919/ME-103 of the NSF and the Czech Republic
Ministry of Education.
References
S. Abramsky and R. Jagadeesan
[1994]
Games and full completeness for multiplicative linear logic, Journal of Symbolic Logic,
59, pp. 543–574.
J. Barwise
[1975]
Admissable Sets and Structures: An Approach to Deﬁnability Theory, Springer-Verlag,
Berlin.
[1977]
Handbook of Mathematical Logic, North-Holland, Amsterdam.
G. Bellin
[1990]
Mechanizing Proof Theory: Resource-Aware Logics and Proof-Transformations to Ex-
tract Explicit Information, PhD thesis, Stanford University.
E. W. Beth
[1953]
On Padoa’s method in the theory of deﬁnition, Indagationes Mathematicae, 15, pp. 330–
339.
[1956]
Semantic entailment and formal derivability, Indagationes Mathematicae, 19, pp. 357–
388.
A. Blass
[1992]
A game semantics for linear logic, Annals of Pure and Applied Logic, 56, pp. 183–220.
S. R. Buss
[1986]
Bounded Arithmetic, Bibliopolis, Napoli. Revision of 1985 Princeton University Ph.D.
thesis.
C.-L. Chang
[1970]
The unit proof and the input proof in theorem proving, J. Assoc. Comput. Mach., 17,
pp. 698–707. Reprinted in: Siekmann and Wrightson [1983,vol 2].
C.-L. Chang and R. C.-T. Lee
[1973]
Symbolic Logic and Mechanical Theorem Proving, Academic Press, New York.

Introduction to Proof Theory
75
W. F. Clocksin and C. S. Mellish
[1981]
Programming in Prolog, North-Holland, Amsterdam, 4th ed.
W. Craig
[1957a] Linear reasoning. A new form of the Herbrand-Gentzen theorem, Journal of Symbolic
Logic, 22, pp. 250–268.
[1957b] Three uses of the Herbrand-Gentzen theorem in relating model theory and proof theory,
Journal of Symbolic Logic, 22, pp. 269–285.
M. Davis and H. Putnam
[1960]
A computing procedure for quantiﬁcation theory, J. Assoc. Comput. Mach., 7, pp. 201–
215. Reprinted in: Siekmann and Wrightson [1983,vol 1].
P. C. Eklof
[1977]
Ultraproducts for algebraists, in: Barwise [1977], pp. 105–137.
S. Feferman
[1968]
Lectures on proof theory, in: Lectures on proof theory, Proceedings of the Summer
School in Logic, Leeds, 1967, M. H. L¨ob, ed., Lecture Notes in Mathematics #70,
Springer-Verlag, Berlin, pp. 1–107.
G. Frege
[1879]
Begriﬀsschrift, eine der arithmetischen nachgebildete Formelsprache des reinen Denkens,
Halle. English translation: in van Heijenoort [1967], pp. 1-82.
G. Gentzen
[1935]
Untersuchungen ¨uber das logische Schliessen, Mathematische Zeitschrift, 39, pp. 176–
210, 405–431. English translation in: Gentzen [1969], pp. 68-131.
[1969]
Collected Papers of Gerhard Gentzen, North-Holland, Amsterdam. Edited by M. E. Sz-
abo.
J.-Y. Girard
[1987a] Linear logic, Theoretical Computer Science, 50, pp. 1–102.
[1987b] Proof Theory and Logical Complexity, vol. I, Bibliopolis, Napoli.
[1989]
Proofs and Types, Cambridge tracts in theoretical computer science #7, Cambridge
University Press. Translation and appendices by P. Taylor and Y. Lafont.
K. G¨odel
[1930]
Die Vollst¨andigkeit der Axiome des logischen Funktionenkalk¨uls, Monatshefte f¨ur Math-
ematik und Physik, 37, pp. 349–360.
R. Harrop
[1960]
Concerning formulas of the types A →B ∨C , A →(Ex)B(x) in intuitionistic formal
systems, Journal of Symbolic Logic, 25, pp. 27–32.
J. van Heijenoort
[1967]
From Frege to G¨odel: A sourcebook in mathematical logic, 1879-1931, Harvard University
Press.
L. Henkin
[1949]
The completeness of the ﬁrst-order functional calculus, Journal of Symbolic Logic, 14,
pp. 159–166.
L. Henschen and L. Wos
[1974]
Unit refutations and Horn sets, J. Assoc. Comput. Mach., 21, pp. 590–605.
J. Herbrand
[1930]
Recherches sur la th´eorie de la d´emonstration, PhD thesis, University of Paris. English
translation in Herbrand [1971] and translation of chapter 5 in van Heijenoort [1967],
pp. 525-581.

76
S. Buss
[1971]
Logical Writings, D. Reidel, Dordrecht, Holland. ed. by W. Goldfarb.
D. Hilbert and W. Ackermann
[1928]
Grundz¨uge der theoretischen Logik, Springer-Verlag, Berlin.
D. Hilbert and P. Bernays
[1934-39] Grundlagen der Mathematik, I & II, Springer, Berlin.
J. Hintikka
[1955]
Form and content in quantiﬁcation theory, two papers on symbolic logic, Acta Philo-
sophica Fennica, 8, pp. 7–55.
W. A. Howard
[1980]
The formulas-as-types notion of construction, in: To H. B. Curry: Essays in Combi-
natory Logic, Lambda Calculus and Formalism, J. P. Seldin and J. R. Hindley, eds.,
Academic Press, New York, pp. 479–491.
S. Kanger
[1957]
Provability in Logic, Almqvist & Wiksell, Stockholm.
S. C. Kleene
[1952]
Introduction to Metamathematics, Wolters-Noordhoﬀ, Groningen and North-Holland,
Amsterdam.
R. Kowalski
[1979]
Logic for Problem Solving, North-Holland, Amsterdam.
J. Kraj´ıˇcek, P. Pudl´ak, and G. Takeuti
[1991]
Bounded arithmetic and the polynomial hierarchy, Annals of Pure and Applied Logic,
52, pp. 143–153.
G. Kreisel
[1951]
On the interpretation of non-ﬁnitist proofs–part I, Journal of Symbolic Logic, 16,
pp. 241–267.
[1952]
On the interpretation of non-ﬁnitist proofs, part II. interpretation of number theory,
applications, Journal of Symbolic Logic, 17, pp. 43–58.
P. Lincoln, J. C. Mitchell, A. Scedrov, and N. Shankar
[1992]
Decision problems for linear logic, Annals of Pure and Applied Logic, 56, pp. 239–311.
E. G. K. Lopez-Escobar
[1965]
An interpolation theorem for denumerably long formulas, Fundamenta Mathematicae,
57, pp. 253–272.
D. W. Loveland
[1970]
A linear format for resolution, in: Symp. on Automatic Demonstration, Lecture Notes
in Mathematics #125, Springer-Verlag, Berlin, pp. 147–162.
[1978]
Automated Theorem Proving: A Logical Basis, North-Holland, Amsterdam.
D. Luckham
[1970]
Reﬁnement theorems in resolution theory, in: Symp. on Automatic Demonstration,
Lecture Notes in Mathematics #125, Springer-Verlag, Berlin, pp. 163–190.
R. C. Lyndon
[1959]
An interpolation theorem in the predicate calculus, Paciﬁc Journal of Mathematics, 9,
pp. 129–142.
E. Mendelson
[1987]
Introduction to Mathematical Logic, Wadsworth & Brooks/Cole, Monterey.
J. R. Munkres
[1975]
Topology: A First Course, Prentice-Hall, Englewood Cliﬀs, New Jersey.

Introduction to Proof Theory
77
M. S. Paterson and M. N. Wegman
[1978]
Linear uniﬁcation, J. Comput. System Sci., 16, pp. 158–167.
G. Peano
[1889]
Arithmetices Principia, noca methodo exposito, Turin.
English translation in:
van
Heijenoort [1967], pp. 83-97.
D. Prawitz
[1965]
Natural Deduction: A Proof-Theoretical Study, Almqvist & Wiksell, Stockholm.
A. Robinson
[1956]
A result on consistency and it application to the theory of deﬁnability, Indagationes
Mathematicae, 18, pp. 47–58.
G. Robinson and L. Wos
[1969]
Paramodulation and theorem-proving in ﬁrst-order theories with equality, in: Machine
Intelligence 4, pp. 135–150.
J. A. Robinson
[1965a] Automatic deduction with hyper-resolution, International Journal of Computer Math-
ematics, 1, pp. 227–234. Reprinted in: Siekmann and Wrightson [1983,vol 1].
[1965b] A machine-oriented logic based on the resolution principle, J. Assoc. Comput. Mach.,
12, pp. 23–41. Reprinted in: Siekmann and Wrightson [1983,vol 1].
K. Sch¨utte
[1965]
Ein System des verkn¨upfenden Schliessens, Archiv f¨ur Mathematische Logik und Grund-
lagenforschung, 2, pp. 55–67.
J. Siekmann and G. Wrightson
[1983]
Automation of Reasoning, vol. 1&2, Springer-Verlag, Berlin.
J. R. Slagle
[1967]
Automatic theorem proving with renamable and semantic resolution, J. Assoc. Comput.
Mach., 14, pp. 687–697. Reprinted in: Siekmann and Wrightson [1983,vol 1].
W. W. Tait
[1968]
Normal derivability in classical logic, in:
The Syntax and Semantics of Inﬁnitary
Languages, Lecture Notes in Mathematics #72, J. Barwise, ed., Springer-Verlag, Berlin,
pp. 204–236.
G. Takeuti
[1987]
Proof Theory, North-Holland, Amsterdam, 2nd ed.
A. S. Troelstra
[1992]
Lectures on Linear Logic, Center for the Study of Logic and Information, Stanford.
A. S. Troelstra and D. van Dalen
[1988]
Constructivism in Mathematics: An Introduction, vol. I&II, North-Holland, Amsterdam.
G. S. Tsejtin
[1968]
On the complexity of derivation in propositional logic, Studies in Constructive Mathe-
matics and Mathematical Logic, 2, pp. 115–125. Reprinted in: Siekmann and Wright-
son [1983,vol 2].
A. N. Whitehead and B. Russell
[1910]
Principia Mathematica, vol. 1, Cambridge University Press.
L. Wos, R. Overbeek, E. Lusk, and J. Boyle
[1992]
Automated Reasoning: Introduction and Applications, McGraw-Hill, New York, 2nd ed.

78
S. Buss
L. Wos and G. Robinson
[1973]
Maximal models and refutation completeness: Semidecision procedures in automatic
theorem proving, in: Word Problems: Decision Problems and the Burnside Problem in
Group Theory, W. W. Boone, F. B. Cannonito, and R. C. Lyndon, eds., North-Holland,
Amsterdam, pp. 609–639.
L. Wos, G. Robinson, and D. F. Carson
[1965]
Eﬃciency and completeness of the set of support stategy in theorem proving, J. Assoc.
Comput. Mach., 12, pp. 201–215. Reprinted in: Siekmann and Wrightson [1983,vol 1].

