UNISON: Unpaired Cross-lingual Image Captioning
Jiahui Gao1, Yi Zhou2, Philip L.H. Yu3*, Shaﬁq Joty4, Jiuxiang Gu5
1 The University of Hong Kong, Hong Kong 2 Johns Hopkins University, USA
3 The Education University of Hong Kong, Hong Kong 4 Nanyang Technological University, Singapore
5 Adobe Research, USA
sumiler@hku.hk, yzhou188@jhu.edu, plhyu@eduhk.hk, srjoty@ntu.edu.sg, jigu@adobe.com
Abstract
Image captioning has emerged as an interesting research ﬁeld
in recent years due to its broad application scenarios. The tra-
ditional paradigm of image captioning relies on paired image-
caption datasets to train the model in a supervised manner.
However, creating such paired datasets for every target lan-
guage is prohibitively expensive, which hinders the extensi-
bility of captioning technology and deprives a large part of
the world population of its beneﬁt. In this work, we present a
novel unpaired cross-lingual method to generate image cap-
tions without relying on any caption corpus in the source or
the target language. Speciﬁcally, our method consists of two
phases: (i) a cross-lingual auto-encoding process, which uti-
lizing a sentence parallel (bitext) corpus to learn the map-
ping from the source to the target language in the scene graph
encoding space and decode sentences in the target language,
and (ii) a cross-modal unsupervised feature mapping, which
seeks to map the encoded scene graph features from image
modality to language modality. We verify the effectiveness of
our proposed method on the Chinese image caption genera-
tion task. The comparisons against several existing methods
demonstrate the effectiveness of our approach.
1
Introduction
Image captioning has attracted a lot of attention in recent
years due to its emerging applications, including assisting
visually impaired people, image indexing, virtual assistants,
etc. Despite the impressive results achieved by the existing
captioning techniques, most of them focus on English be-
cause of the availability of image-caption paired datasets,
which can not generalize for languages where such paired
dataset is not available. In reality, there are more than 7,100
different languages spoken by billions of people worldwide
(source: Ethnologue(2019)). Building visual-language tech-
nologies only for English would deprive a signiﬁcantly large
population of non-English speakers of AI beneﬁts and also
leads to ethical concerns, such as unequal access to re-
sources. Therefore, similar to other NLP tasks (e.g. parsing,
question answering) (Hu et al. 2020; Conneau et al. 2018;
Gu et al. 2020), visual-language tasks should also be ex-
tended to multiple languages. However, creating paired cap-
tioning datasets for each target language is infeasible, since
*Corresponding author.
Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
the labeling process is very time consuming and requires ex-
cessive human labor.
To alleviate the aforementioned problem, there have been
several attempts in relaxing the requirement of image-
caption paired data in the target language (Gu et al. 2018;
Song et al. 2019), which rely on paired image-caption data
in a pivot language to generate captions in the target lan-
guage via sentence-level translation. However, even for En-
glish, the existing captioning datasets (e.g., MS-COCO (Lin
et al. 2014)) are not sufﬁciently large and comprise only lim-
ited object categories, making it challenging to generalize
the trained captioners to scenarios in the wild (Tran et al.
2016). In addition, sentence-level translation relies purely
on the text description and can not observe the entire im-
age, which may ignore important contextual semantics and
lead to inaccurate translation. Thus, such pivot-based meth-
ods fail to fully address the problem.
Recently, a few works explore image captioning task in
an unpaired setting (Feng et al. 2019; Gu et al. 2019; Laina,
Rupprecht, and Navab 2019). Nevertheless, these methods
still rely on manually labeled caption corpus. For example,
Gu et al. (2019) train their model based on shufﬂed image-
caption pairs of MS-COCO; Feng et al. (2019) use an image
descriptions corpus from Shutterstock; Lania et al. (2019)
create training dataset by sampling the images and captions
from different image-caption datasets. Despite they belong
to unpaired methods in spirit, one could still argue that they
depend heavily on the collected caption corpus to get a rea-
sonable cross-modal mapping between vision and language
distributions – a resource that is not always practical to as-
sume. It therefore remains questionable how these methods
would perform when there is no caption data at all. To the
best of our knowledge, there is yet no work that investigates
image captioning without relying on any caption corpus.
Despite the giant gap between images and texts, they
are essentially different mediums to describe the same en-
tities and how they are related in the objective world. Such
internal logic is the most essential information carried by
the medium, which can be leveraged as the bridge to con-
nect data in different modalities. Scene graph(Wang et al.
2018), a structural representation that contains 1) the ob-
jects, 2) their respective attributes and 3) how they are re-
lated as described by the medium (image or text), which
has been developed into a mature technique for visual un-
arXiv:2010.01288v3  [cs.CL]  7 Feb 2022

derstanding tasks in recent years(Johnson, Gupta, and Fei-
Fei 2018). Previous researches on scene graph generation
have demonstrated its effectiveness in aiding cross-modal
alignment (Yang et al. 2019; Gu et al. 2019). However,
existing scene graph generators are only available in En-
glish, which poses challenges for extending its application
on other languages. One naive approach is to perform cross-
lingual alignment to other target languages by conducting
a superﬁcial word-to-word translation on the scene graphs
nodes, which neglects the contextual information of the sen-
tence or the image. Since a word on a single node can carry
drastically different meanings in various contexts, such an
approach often leads to sub-optimal cross-lingual mapping.
To address this issue, we propose a novel Cross-lingual Hi-
erarchical Graph Mapping (HGM) to effectively conduct
the alignment between languages in the scene graph en-
coding space, which beneﬁts from contextual information
by gathering semantics across different levels of the scene
graph. Notably, the scene graph translation process can be
enhanced by the large-scale parallel corpus (bi-text), which
is easily accessible for many languages (Espl`a et al. 2019).
In this paper, we propose UNpaIred crosS-lingual image
captiONing (UNISON), a novel approach to generate im-
age captions in the target language without relying on any
caption corpus. Our UNISON framework consists of two
phases: (i) a cross-lingual auto-encoding process and (ii) a
cross-modal unsupervised feature mapping (Fig. 1). Using
the parallel corpus, the cross-lingual auto-encoding process
aims to train the HGM to map a scene graph derived from the
source language (English) sentence to the space of the tar-
get language (Chinese), and learns to generate a sentence in
the target language based on the mapped scene graph. Then,
a cross-modal feature mapping (CMM) function is learned
in an unsupervised manner, which aligns the image scene
graph features from image modality to language modality.
The features in language modality is subsequently mapped
by HGM, and then fed to the decoder in phase (i) to gener-
ate image captions in the target language. Our experiments
show 1) the effectiveness of the proposed HGM when con-
ducting cross-lingual alignment(§5.2) in the scene graph en-
coding space and 2) the superior performance of our UNI-
SON framework as a whole(§5.1).
2
Related Work
Paired Image Captioning. Previous studies on super-
vised image captioning mostly follow the popular encoder-
decoder framework (Vinyals et al. 2015; Rennie et al. 2017;
Anderson et al. 2018), which mostly focus on generat-
ing captions in English since the neural image captioning
models require large-scale data of annotated image-caption
pairs to achieve good performance. To relax the requirement
of human effort in caption annotation, Lan, Li, and Dong
(2017) propose a ﬂuency-guided learning framework to gen-
erate Chinese captions based on pseudo captions, which
are translated from English captions. Yang et al. (2019)
adopt the scene graph as the structured representation to
connect image-text domains and generate captions. Zhong
et al. (2020) propose a method to select the important sub-
graphs of scene graphs to generate comprehensive caption-
ing. Nguyen et al. (2021) further close the semantic gap
between image and text scene graphs by Human-Object-
Interaction labels.
Unpaired Image Captioning. The main challenge in un-
paired image captioning is to learn the captioner without any
image-caption pairs. Gu et al. (2018) ﬁrst propose an ap-
proach based on pivot language. They obviate the require-
ment of paired image-caption data in the target language but
still rely on paired image-caption data in the pivot language.
Feng et al. (2019) use a concept-to-sentence model to gen-
erate pseudo-image-caption pairs, and align image features
and text features in an adversarial manner. Song et al. (2019)
introduce a self-supervised reward to train the pivot-based
captioning model on pseudo image-caption pairs. Gu et al.
(2019) propose a scene graph-based method for unpaired
image captioning on disordered images and captions.
Summary. While several attempts have been made to-
wards unpaired image captioning, they require caption cor-
pus to learn a reasonable cross-modal mapping between vi-
sion and language distributions, e.g. the corpus in (Feng
et al. 2019) is collected from Shutterstock image descrip-
tions, Gu et al. (2019) use the MSCOCO corpus after
shufﬂing the image-caption pairs. Thus, arguably these ap-
proaches are not entirely “unpaired” as they rely on the la-
belled corpus, limiting their applicability to different lan-
guages. Meanwhile, our method generates captions in target
language without relying on any caption corpus.
3
Methods
3.1
Preliminary and Our Setting
In the conventional paired paradigm, image captioning aims
to learn a captioner which can generate an image caption ˆS
for a given image I, such that ˆS is similar to the ground-truth
(GT) caption. Given the image-caption pairs {Ii, Si}NI
i=1 ,
the popular encoder-decoder framework is formulated as:
I →S :
I →v →ˆS
(1)
where v denotes the encoded image feature. The training
objective for Eq. 1 is to maximize the probability of words in
the GT caption given the previous GT words and the image.
Compared with paired setting, which relies on paired
image-caption data and can not generalize beyond the lan-
guage used to label the caption, our unpaired setting does
not depend on any image-caption pairs and can be extended
to other target languages. Speciﬁcally, we assume that we
have an image dataset {Ii}NI
i=1 and a source-target parallel
corpus dataset {(Sx
i , Sy
i )}NS
i=1. Our goal is to generate cap-
tion ˆSy in the target language y (Chinese) for an image I
with the help of unpaired images and parallel corpus.
3.2
Overall Framework
As shown in Fig. 1, there are two phases in our framework:
(i) a cross-lingual auto-encoding process and (ii) a cross-
modal unsupervised feature mapping, which can be for-
mulated as the following equations, respectively:
Sx →Sy : Sx →Gx ⇒Gy →zy →ˆSy
(2)
I →Sy
: I →Gx,I ⇒Gy,I →zy,I ⇒zy →ˆSy
(3)

 
He can see through the 
window a vista of green 
field
(b) Unsupervised Cross-modal 
Feature Mapping
Sentence 
Parser
He can see through the window a vista 
of green field
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
(* Through the window he can see the 
green field scenery)
KL Divergence
? ? ? ? ? ? ? ? ? ? ? ?
(* There is a big vase in the center of the 
window)
Cross-modal Feature 
Mapping? CMM?
Shared
Shared
Cross-lingual 
Scene Graph Mapping
(a) Cross-Lingual Auto-Encoding
Cross-lingual
Scene Graph Mapping
Sentence (English)
Sentence (English)
Sentence (Chinese)
Caption (Chinese)
Training Data (Paired)
Training Data (Unpaired)
flower
sill
in
vase
of
pictur e
rest upon
sit in
tulip
window
by
pink
plant
in
blue/clear
pink
fill with
in
potted
meadow
at
window
field
view/vista
see
green
of
green
Figure 1: Overview of our UNISON framework. It has two phases: cross-lingual auto-encoding process and unsupervised
cross-modal feature mapping. The cross-lingual scene graph mapping in the ﬁrst phase (Top) is designed to map the scene
graph from the source language (e.g. English) to the target language (e.g. Chinese) without relying on a scene graph parser in
the target language. The unsupervised cross-modal feature mapping in the second phase (Bottom) is designed to align the visual
modality to textual modality. We mark the object, relationship, and attribute nodes yellow, blue, and grey in the scene graph.
The English sentences (marked in gray) in parentheses are translated by google translator for better understanding.
where l ∈{x, y} is the source or target language; Sl is the
sentence in laugnage l; Gl and Gl,I are the scene graphs for
the language l in sentence modality and image modality (I),
respectively; zl and zl,I are the encoded scene graph fea-
tures for Gl and Gl,I, respectively.
The cross-lingual auto-encoding process (shown in top
of Fig. 1) aims to generate a sentence in the target language
given a scene graph in the source language: we ﬁrst extract
the sentence scene graph Gx from each (English) sentence
Sx using a sentence scene graph parser, and map it to Gy via
our proposed HGM (detail in later section). Then we feed Gy
to the encoder to produce the scene graph features zy, which
the decoder then takes as inputs to generate ˆSy. Note that
the mapping from Gx to Gy is done at the embedding level,
i.e. no symbolic Gy is constructed. This phase addresses the
misalignment among different language domains.
The
cross-modal
unsupervised
feature
mapping
(shown in the bottom part of Fig. 1) closes the gap between
image modality and language modality: we ﬁrst extract the
image scene graph Gx,I from image I, which is in source
language x (English). After that, we map the Gx,I to Gy,I
with the HGM (shared with the ﬁrst phase). As shown
in Eq. 3, a cross-modal mapping function (zy,I ⇒zy)
is learned, which maps the encoded image scene graph
features from image modality to language modality. Once
mapped to zy, we can use the sentence decoder to generate
ˆSy. We further elaborate each phase in detail below.
3.3
Cross-lingual Auto-encoding Process
Scene Graph.
A scene graph G = (V, E) contains three
kinds of nodes: object, relationship and attribute nodes. Let
object oi denote the i-th object. The triplet ⟨oi, ri,j, oj⟩in
G is composed of two objects: oi (as subject role) and oj
(as object role), along with their relation ri,j. As each ob-
ject may have a set of attributes, we denote ak
i as the k-th
attribute of object oi. To generate an image scene graph
GI, we build the image scene graph generator based on
Faster-RCNN (Ren et al. 2015) and MOTIFS (Zellers et al.
2018). To generate sentence scene graph Gx, we ﬁrst con-
vert each sentence into a dependency tree with a syntactic
parser (Anderson et al. 2016), and then apply a rule-based
method (Schuster et al. 2015) to build the graph. The Gy is
mapped from Gx through our HGM module.
Cross-lingual Hierarchical Graph Mapping (HGM).
Our hierarchical graph mapping contains three levels:
(i) word-level mapping, (ii) sub-graph mapping, and
(iii) full-graph mapping. The semantic information from all
three levels are fused in an self-adaptive manner via a self-
gated mechanism, which effectively takes into account the
structures and relations from the context.
The proposed HGM is illustrated in Fig. 2. Let
⟨el
oi, el
ri,j, el
oj⟩∈Gl denote the triplet for relation rl
i,j in
language l, where el
oi, el
oj and el
ri,j are the embeddings
representing subject ol
i, object ol
i, and relationship rl
i,j. For-
mally, our hierarchical graph mapping from language x to
language y can be expressed as:
⟨ey
oi, ey
ri,j, ey
oj⟩=⟨fHGM(ex
oi, Gx), fWord(ex
ri,j),
fHGM(ex
oj, Gx)⟩
(4)
fHGM(ex
oi, Gx) =αwfWord(ex
oi) + αsfSub(ex
oi, Gx)
+ αffFull(ex
oi, Gx)
(5)
⟨αw, αs, αf⟩=softmax
 fMLP(fWord(ex
oi))

(6)
where ey
oi, ey
oj and ey
ri,j are the mapped embeddings in
target language y; αw, αs, and αf are the level-wise im-
portance weights calculated by Eq. 6; fMLP(·) represents
a multi-layer perception (MLP) composed of three fully-
connected (FC) layers with ReLU activations.
Word-level Mapping. The word-level mapping relies on a
retrieval function fWord(.): after obtaining an embedding in
language x, fWord(.) retrieves the most similar embedding

in language y from a cross-lingual word embedding space
as illustrated in Fig. 2(a), where cosine similarity is used to
measure the distance. In practice, we adopt the pre-trained
common space trained on Wikipedia following (Joulin et al.
2018). The retrieved embedding is then passed to an FC
layer to obtain a high-dimension embedding in language y.
Graph-level Mapping. Since the relation and structure
of the surrounding nodes encode crucial context informa-
tion, we also introduce the node mapping with graph-
level information (as illustrated by Fig. 2(b) and 2(c)):
namely sub-graph mapping (fSub) and full-graph mapping
(fFull), which ﬁrst construct the contextualized embedding in
graph-level and then conduct the cross-lingual mapping on
the produced embedding. More speciﬁcally, for sub-graph
mapping, the contextualized embedding is computed by:
PN′
o
k=1 sconv(ex
oi, ex
ok)/N ′
o, where N ′
o is the total number of
nodes directly connected to node oi, and sconv(·) is the spa-
tial convolution operation (Yang et al. 2019). For full-graph
mapping, the contextualized embedding is calculated by an
attention module: PNo
k=1 αkex
ok, where αk is calculated by
softmax over all the object embeddings ex
o1:No. Both fSub
and fFull use a linear mapping to project the resulted con-
textualized (English) embedding to the target (Chinese) em-
bedding space. We consider graph-level mapping only for
the object nodes since relationships only exist between ob-
jects. For relationship and attribute nodes, only word-level
mapping is performed.
Self-gated Adaptive Fusion. To leverage the complemen-
tary advantages of information in different levels, we pro-
pose a self-gate mechanism to adaptively adjust the impor-
tance weights when fusing the embeddings. Speciﬁcally, the
importance scores are calculated based on the word-level
embeddings by passing it through a three-class MLP and a
softmax function (Eq. 6). Compared with directly concate-
nating the embeddings from different levels, which assigns
them with equal importance, our fusing mechanism adap-
tively concentrates on important information and suppress
the noises when the context becomes sophisticated.
Scene
Graph
Encoder.
We
encode
the
Gx
and
Gy(mapped by the HGM) with two scene graph en-
coders Gx
Enc(·) and Gy
Enc(·), which are implemented by
spatial graph convolutions. The output of each scene graph
encoder can be formulated as:
f l
o1:Nlo, f l
r1:Nlr , f l
a1:Nla = Gl
Enc(Gl),
l ∈{x, y}
(7)
where f l
o1:Nlo, f l
r1:Nlr , and f l
a1:Nla denote the set of en-
coded object embeddings, relationship embeddings, and
attribute embeddings, respectively. Each object embed-
ding f l
oi is calculated by considering relationship triplets
⟨el
sub(oi), el
rsub(oi),i, el
oi⟩and ⟨el
oj, el
rj,obj(oi), el
obj(oi)⟩; sub(oi)
represents the subjects where oi acts as an object, and
obj(oi) represents the objects where oi plays the sub-
ject role. f l
ri is calculated based on relationship triplet
⟨el
oi, el
ri,j, el
oj⟩. f l
ai is the attribute embedding calculated
by object oi and its associated attributes.
(a) Word-level Mapping (b) Sub-Graph Mapping
(c) Full-Graph Mapping
Figure 2: Illustration of our hierarchical scene graph map-
ping. Sub-graph mapping only considers those directly con-
nected nodes, while full-graph mapping considers all the
nodes in the scene graph.
Sentence Decoder.
As shown in Fig. 1, we have two de-
coders: Gx
Dec(·) and Gy
Dec(·). Each decoder is composed of
three attention modules and an LSTM-based decoder. It
takes the encoded scene graph features as input and gener-
ates the captions. The decoding process is deﬁned as:
ol
t, hl
t = Gl
Dec

fTriplet
 [zl
o, zl
r, zl
a]

, hl
t−1, ˆsl
t−1

(8)
ˆsl
t ∼softmax(W ool
t)
(9)
where l ∈{x, y}, ˆsl
t is the t-th decoded word drawn from
the dictionary according to the softmax probability, W o is
a learnable weight matrix, ol
t is the cell output of the de-
coder, hl
t is the hidden state. fTriplet(·) is a non-linear map-
ping function that takes the concatenated features as input
and outputs the triplet level feature. zl
oi is calculated by the
attention module deﬁned as: PNl
o
i
αl
oif l
oi, where αl
oi is the
attention weight calculated by the softmax operation over
f l
o1:Nlo. zl
ri and zl
ai are calculated in a similar way.
Joint-training Mechanism.
Inspired by the fact that com-
mon structures exist in the encoded scene graph space that
are language-agnostic, which may be leveraged to beneﬁt
the encoding process, we propose joint training mechanism
to enhance the features in target language with the help of
features in the source language. In practice, we train a sepa-
rate scene graph encoder for each language in parallel, then
align the encoded scene graph features by enforcing them to
be semantically close.
Speciﬁcally, we train the scene graph encoders (Gx
Enc and
Gy
Enc), sentence decoders (Gx
Dec and Gy
Dec), and the cross-
lingual HGM module (Gx ⇒Gy), supervised by a parallel
corpus. The two graph encoders encode Gx and Gy into fea-
ture representations and predict sentences ( ˆSx and ˆSy) with
the decoders. We minimize the following loss:
LXE = −
X
t
log PθGx→Sx(sx
t |sx
0:t−1, Gx)
−
X
t
log PθGy→Sy (sy
t |sy
0:t−1, Gy)
(10)
where the sx
t and sy
t are the ground truth words, Gx and Gy
are the sentence scene graphs in different languages with
Gy being derived from Gx using our HGM, θGx→Sx and
θGy→Sy are the parameters of two encoder-decoder models.
To close the semantic gap between the encoded scene
graph features {zx
o, zx
a, zx
r} and {zy
o, zy
a, zy
r}, we introduce
a Kullback–Leibler (KL) divergence loss:
LKL = exp
 KL(p(zx
o)||p(zy
o)

+ exp
 KL(p(zx
a)||p(zy
a)

+ exp
 KL(p(zx
r)||p(zy
r)

(11)

where p(·) is composed of a linear layer that maps the input
features to a low-dimension dc, followed by a softmax to get
a probability distribution. The overall objective of our joint
training mechanism is as follows: LPhase 1 = LXE + LKL.
3.4
Unsupervised Cross-modal Feature Mapping
To adapt the learned model from sentence modality to im-
age modality, we drew inspiration from (Gu et al. 2019) and
adopt CycleGAN (Zhu et al. 2017) to align the features. For
each type p ∈{o, r, a} of triplet embedding in Eq. 8, we
have two mapping functions: gp
I→y(·) and gp
y→I(·), where
gp
I→y(·) maps the features from image modality to the sen-
tence modality, and gp
y→I(·) maps from sentence modality
to the image modality. Note that we freeze the cross-lingual
mapping module trained in the ﬁrst phase. The training ob-
jective for cross-modal feature mapping is deﬁned as:
Lp
CycleGAN = LI→y
GAN + Ly→I
GAN + λLI↔y
cyc
(12)
where LI↔y
cyc
is a cycle consistency loss, LI→y
GAN and Ly→I
GAN are
the adversarial losses for the mapping functions with respect
to the discriminators.
Speciﬁcally, the objective of the mapping function
gp
I→y(·) is to fool the discriminator Dp
y through adversar-
ial learning. We formulate the objective function for cross-
modal mapping as:
LI→y
GAN = ES[log Dp
y(zy
p)] + EI[log(1 −Dp
y(gp
I→y(zI
p))]
(13)
where zy
p and zI
p are the encoded embeddings for sentence
scene graph Gy and image scene graph Gy,I, respectively.
The adversarial loss for sentence to image mapping Ly→I
GAN
is similarly deﬁned. The cycle consistency loss LI↔y
cyc
is de-
signed to regularize the training and make the mapping func-
tions cycle-consistent:
LI↔y
cyc
=EI[∥gp
S→I
 gp
I→S(zI
p)

−zI
p∥1]
+ Ey[∥gp
I→y
 gp
y→I(zy
p)

−zy
p∥1]
(14)
The overall training objective for phase 2 becomes:
LPhase 2 = Lo
CycleGAN + La
CycleGAN + Lr
CycleGAN.
3.5
Inference of the UNISON Framework
During inference, given an image I, we ﬁrst extract the im-
age scene graph Gx,I with a pre-trained image scene graph
generator and then map the Gx,I in x (English) to Gy,I in y
(Chinese) with our HGM module. After that, we encode Gy,I
with Gy
Enc(·) and map the encoded features to the language
domain through gp
I→y(·). The mapped features are then fed
to the LSTM-based sentence decoder Gy
Dec(·) to generate the
image caption ˆSy in target language y.
4
Experiments
4.1
Datasets and Setting
Datasets.
In this paper, we use English and Chinese as
the source and target languages, respectively. For cross-
lingual auto-encoding, we collect a paired English-Chinese
corpus from existing MT datasets, including WMT19 (Bar-
rault et al. 2019), AIC MT (Wu et al. 2017), UM (Tian
et al. 2014), and Trans-zh
(Xu 2019). We ﬁlter the sen-
tences in MT datasets according to an existing caption-style
dictionary containing 7,096 words in Li et al. (2019). For
the ﬁrst phase, we use 151,613 sentence pairs for training,
5,000 sentence pairs for validation, and 5,000 pairs for test-
ing. For the second phase, following Li et al. (2019), we use
18,341 training images from MS-COCO and randomly se-
lect 18,341 Chinese sentences from the training split of the
MT corpus. During evaluation, we use the validation and
testing splits in COCO-CN. More details are in the appendix.
Corpus
0 Obj/G
1 Obj/G
2 Obj/G
⩾3 Obj/G
Raw
17.7%
42.6%
24.4%
15.4%
Back-Trans.
12.3%
13.3%
15.1%
59.3%
Table 1: Statistics of the English sentence scene graphs,
where n Obj/G denotes the number of object in a scene
graph, ⩾means greater than or equal to 3.
Preprocessing.
We extract the image scene graph with
MOTIFS (Zellers et al. 2018) pretrained on VG (Krishna
et al. 2017). We tokenize and lowercase the English sen-
tences, then replace the tokens appeared less than ﬁve times
with UNK, resulting in a vocabulary size of 13,194. We seg-
ment the Chinese sentences with Jieba1 and apply the same
treatment for words appeared less than ﬁve times, resulting
in a vocabulary size of 11,731. The English sentence scene
graphs are extracted with the parser proposed by (Ander-
son et al. 2016). We augment the English sentences with
the pre-trained back-translators (Ng et al. 2019), resulting
in 808,065 English sentences in total, which helps enrich
the English sentence scene graphs. Speciﬁcally, the statis-
tics in Table 1 shows that the percentage of scene graphs
containing more than 3 objects is increased from 15.4%
to 59.3%. Quantitative and qualitative results in Appendix
further shows that data augmentation effectively enrich the
scene graphs and beneﬁt the sentence generation.
4.2
Implementation Details
During cross-lingual auto-encoding phase, we set the di-
mension of scene graph embeddings to 1,000 and dc to
100. LSTM with 2 layers is adopted to construct the de-
coder, whose hidden size is 1000. We start by initializing
the graph mapping from a pre-trained common space (Joulin
et al. 2018) to stabilize training. The cross-lingual encoder-
decoder is ﬁrstly trained with the LXE for 80 epochs, then
with joint loss LPhase 1 for 20 epochs.
During unsupervised cross-modal mapping phase, we
learn the cross-modal feature mapping on the unpaired MS-
COCO images and translation corpus. Speciﬁcally, we in-
herit and freeze the parameters of the Chinese scene graph
encoder, HGM, and Chinese sentence decoder from cross-
lingual auto-encoding process. The cross-modal mapping
functions and discriminators are learned with LPhase 2. We
optimize the model with Adam, batch size of 50, and learn-
ing rate of 5 × 10−5. The discriminators are implemented
1https://github.com/fxsjy/jieba

Method
B@1 B@2 B@3 B@4 METEOR ROUGH CIDEr
Setting w/o caption corpus
Un.
Graph-Aligh(En)(Gu et al. 2019) +GoogleTrans.
39.2
16.7
6.5
2.3
13.2
26.5
9.3
UNISON
44.9
19.9
8.6
3.3
16.5
29.6
12.7
Setting w/ caption corpus
Pair
FC-2k (En)(Rennie et al. 2017)+GoogleTrans.
58.9
38.0
23.5
14.3
23.5
40.2
47.3
FC-2k (Cn, Pseudo COCO)(Rennie et al. 2017)
60.4
40.7
26.8
17.3
24.0
43.6
52.7
Un.
UNISON
63.4
43.2
29.5
17.9
24.5
45.1
53.5
Table 2: Performance comparisons on the test split of COCO-CN. ‘Un.’ is short for Unpaired. B@n is short for BLEU-n. ‘En’
and ‘Cn’ in the parentheses represent English and Chinese, respectively. ‘GoogleTrans’ stands for google translator.
with a linear layer of dimension 1,000 and a LeakyReLU
activation. We set λ to 10. During inference, we use beam
search with a beam size of 5. We use the popular BLEU (Pa-
pineni et al. 2002), CIDEr (Vedantam, Lawrence Zitnick,
and Parikh 2015), METEOR (Denkowski and Lavie 2014)
and ROUGE (Lin 2004) for evaluation. More details are at-
tached to the Appendix.
4.3
Model Statement
To gain insights into the effectiveness of our HGM, we con-
struct ablative models by progressively introducing cross-
lingual graph-mappings in different levels:
GMBASE is our baseline model, which adopts Google’s MT
system (Wu et al. 2016) to symbolically map the scene graph
from English to Chinese in a node-to-node manner.
GMWORD maps the English scene graph to Chinese through
word-level mapping in the scene graph encoding space.
GMWORD+SUB. considers both word-level and subgraph-level
mappings by directly concatenating them.
HGMBASE considers mappings across all levels, which are
directly concatenated and passed through an FC layer.
HGM is similar to HGM-base, except that it adopts a self-
gated fusion to adaptively fuse the three features, as illus-
trated by Eq. 5 and Eq. 6.
5
Results and Analysis
5.1
Overall Results
We demonstrate the superior performance of the proposed
UNISON framework on Chinese image captions generation
task. We ﬁrst compare UNISON with the SOTA unpaired
method Graph-Align(Gu et al. 2019) under the setting with-
out using any caption corpus. More speciﬁcally, we run the
Graph-Align2 and translate the generated English captions
to Chinese by google translator for comparison. From the
result in Table 2, we can ﬁnd that our method signiﬁcantly
surpasses Graph-Align with translation, demonstrating that
translation in graph level is superior to translation in sen-
tence level. This is reasonable since the graph level align-
ment is able to consider structural and relational informa-
tion of the whole image, while sentence level translation
suffers from information loss as it can only observe the pre-
dicted sentences, and can be severely affected if the transla-
tion tools perform poorly. We do not compare with the other
unpaired method (Song et al. 2019) here, as the dataset and
codes are not publicly available.
2Code is acquired from the ﬁrst author of (Gu et al. 2019).
To further verify the effectiveness of our framework, we
compare UNISON with the supervised pipeline methods:
(i) FC-2k(En)+Trans. We train the FC-2k model on image-
caption pairs(En) of MS-COCO and translate the gener-
ated captions(En) to caption(Cn) using Google translator;
(ii) FC-2k(Pseudo). We train the FC-2k model on pseudo
Chinese image-caption pairs of MS-COCO, where the cap-
tions(Cn) are translated by Google translator from cap-
tions(En). For such comparisons, we ﬁne-tune our cross-
lingual mapping on the unpaired captions. The results show
that our method signiﬁcantly and consistently outperforms
the FC-2k(En)+Trans. and FC-2k(Pseudo) models in all
metrics, despite our unpaired setting is much weaker.
5.2
Effectiveness of Cross-lingual Alignment
Analyzing the superior performance of HGM.
We con-
duct experiments on MT task to demonstrate our HGM’s ef-
fectiveness in cross-lingual alignment, which is shown in Ta-
ble 3. The advantage of HGM lies in four aspects: (1) The
cross-lingual graph translation is effective. Our HGM and its
variants achieve considerably higher performance compared
with GEN, which directly generates Chinese sentences based
on English scene graphs. (2) The cross-lingual alignment in
the encoding space is superior than direct symbolic trans-
lation. GMWORD achieve considerably higher performance
compared with GMBASE, which proves that the scene graph
encoding contains richer information and is more suitable
for cross-lingual alignment. (3) Performing node mapping
considering features across different graph levels boosts the
performance. When we consider full-graph and sub-graph
level features, the cross-lingual alignment starts achieving
signiﬁcant performance improvement, which veriﬁes the im-
portance of structural and relational information in the con-
text. E.g., HGM outperforms GMWORD in B@1, B@2, B@3,
B@4, METEOR, and ROUGE metrics by 8.2%, 17.9%,
29.9%, 38.8%, 10.4%, 13.1%, respectively. (4) The adaptive
self-gate fusion mechanism is beneﬁcial. We can observe
that HGMBASE is surpassed by HGM by a large margin. As
shown in Table 5, the role of self-gate fusion becomes more
essential when HGM is applied to image scene graphs.
Joint training beneﬁts the enconding process.
We train
our models using the joint loss LPhase 1, where LKL enforces
the distributions of latent scene graph embeddings between
different languages to be close. Table 4 shows that the mod-
els trained with joint loss consistently outperforms their
counterparts with only LXE for all metrics, which indicates

Method
SG-Map S-Gate B@1 B@2 B@3 B@4
M
R
GEN


25.0 13.8
8.2
5.2 14.4 27.3
GMBASE


26.6 15.4 10.0
7.3 15.1 28.1
GMWORD


28.1 16.2 10.7
8.0 15.4 28.2
GMWORD+SUB.


29.2 17.9 12.6
9.9 16.3 30.2
HGMBASE


29.6 18.1 12.8
9.9 16.5 30.4
HGM


30.4 19.1 13.9 11.1 17.0 31.9
Table 3: Performance comparison between variants of HGM
on Chinese sentence generation task. Test split of MT cor-
pus is used for evaluation. ‘SG-Map’ is cross-lingual scene
graphs mapping. ‘S-Gate’ is self-gate fusion mechanism.
Method
LXE LKL B@1 B@2 B@3 B@4
M
R
GMWORD


28.1 16.2 10.7
8.0 15.4 28.2
-w/o joint


-0.4
-0.4
-0.3
-0.3 -0.1 -0.2
GMWORD+SUB.


29.2 17.9 12.6
9.9 16.3 30.2
-w/o joint


-0.3
-0.3
-0.3
-0.4 -0.2 -0.1
HGM


30.4 19.1 13.9 11.1 17.0 31.9
-w/o joint


-0.5
-0.3
-0.3
-0.3 -0.2 -0.4
Table 4: Effectiveness of joint training in cross-lingual auto-
encoding. Performance comparison of the Chinese sentence
generation on the test split of MT corpus.
that the encoding process of the target language can beneﬁt
from the source language.
5.3
Effectiveness of cross-modal feature mapping
To bridge the gap between image and language modalities,
we learn the cross-modal feature mapping in an unsuper-
vised manner.
Table 5 shows the performance of Chinese image cap-
tioners with and without CMM. We can see that adver-
sarial training can consistently improve the model’s per-
formance. Speciﬁcally, CMM can boost the performance
of our HGM by 3.8%(B@1), 2.8%(B@2), 1.7%(B@3),
0.7%(B@4), 1.2%(ROUGE), 3.0%(CIDER), respectively.
Notably, GMWORD+SUB. and HGMBASE perform even worse
than GMBASE, which is because the generated image scene
graphs are noisy with repeated relation triples (as explained
in §5.5), leading to degradation on contextualized cross-
lingual graph mapping (sub-graph and full-graph), whereas
self-gated fusion can tackle this problem by decreasing the
importance of noisy graph-level mapping.
Method
B@1 B@2 B@3 B@4
M
R
C
GMWORD
40.1 16.4
6.7
2.2 15.6 28.4 9.5
GMWORD+CMM
43.1 19.4
8.3
3.0 16.5 29.4 12.6
GMWORD+SUB.
37.3 14.9
6.1
2.5 14.3 27.0 7.9
GMWORD+SUB.+CMM 40.6 17.8
7.6
2.8 15.2 28.3 10.8
HGMBASE
38.0 15.5
6.2
2.4 14.4 27.3 8.0
HGMBASE+CMM
39.8 16.9
7.3
2.6 14.8 27.7 10.2
HGM
41.1 17.1
6.9
2.6 15.7 28.4 9.7
HGM+CMM
44.9 19.9
8.6
3.3 16.5 29.6 12.7
Table 5: Effectiveness of cross-model feature mapping. Per-
formance on the test split of COCO-CN is used for compar-
ison. C is short for CIDEr.
5.4
Human Evaluation
Table 6 shows the results of human evaluation for baseline
models with GAN-based cross-modal feature mapping. The
ID: 356293
shir t
gr ass
on
on
field
*  A girl who just threw the frisbee
? ? ? ? ? ? ? ? ? ? ? ?
* A young young woman on the grass
? ? ? ? ? ? ? ? ? ? ? ? ? ?
* A young girl standing on the lawn of a field
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
* A young girl standing on a golden meadow
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
of
ar m
in
per son/gir l
wear
sidewalk
wheel
on
bike
with
str eet
on
people
with
white
gr ey/empty
GT (Chinese)
*  A man riding a bicycle through the city streets
? ? ? ? ? ? ? ? ? ? ? ? ? ?
*  A tall man pedestrian walks by the road on the street
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
*  On the street sits a tricycle tricycle
? ? ? ? ? ? ? ? ? ? ? ?
*  A man riding a bicycle on the sidewalk
? ? ? ? ? ? ? ? ? ? ? ? ? ?
GT (Chinese)
ID: 451550
jeans
on
on
m an
on
r id
building
on
blue
air
young/white/play/stand
in
br own/lar ge
yellow
long/bar e
br own/dr y
r ound/black
sky
above
lar ge/tall
blue/white/cloudy
white
tr ee
distant/gr een
Figure 3: Qualitative results of different unsupervised cross-
lingual caption generation models.
quality of captions is measured by the ﬂuency and relevancy
metrics. The ﬂuency measures whether the generated caption
is ﬂuent and similar to human-generated captions. The rele-
vancy measures whether the caption correctly describes the
relevant information of the image. These metrics are graded
by 5 levels: 1-Very poor, 2-Poor, 3-Adequate, 4-Good, 5-
Excellent. We invite 10 Chinese native speakers to partic-
ipate in the evaluation, who are from diverse professional
backgrounds. Speciﬁcally, each participant is randomly as-
signed with 100 images from COCO-CN test split (1,000
samples in total). The results in Table 6 report the mean
scores, which illustrate that our method can generate rele-
vant and human-like captions.
Metric
GMWORD GMWORD+SUB. HGM HGM⋆GT
Relevancy
2.78
2.96
3.22
3.96
4.86
Fluency
2.49
2.76
3.05
4.06
4.91
Table 6: Human evaluation on COCO-CN test split. HGM⋆
represents ﬁne-tuned HGM. Models are trained with CMM.
5.5
Qualitative Results
We provide some Chinese captioning examples for MS-
COCO images in Fig. 3. We can see that our method can
generate reasonable image descriptions without using any
paired image-caption data. Also, we observe that the image
scene graphs are quite noisy, which potentially explains the
performance degradation when introducing graph-mappings
without self-fusion mechanism (see Table 5). Figure in Ap-
pendix A also visualizes some examples of generated scene
graphs with or without augmentation, and its correspond-
ingly generated sentences.
6
Conclusion
In this paper, we propose a novel framework to learn a
cross-lingual image captioning model without any image-
caption pairs. Extensive experiments demonstrate our pro-
posed methods can achieve promising results for caption
generation in the target language without using any caption
corpus for training. We hope our work can provide inspira-
tion for unpaired image captioning in the future.

Acknowledgments
We would like to thank Lingpeng Kong, Renjie Pi and the
anonymous reviewers for insightful suggestions that have
signiﬁcantly improved the paper. This work was supported
by TCL Corporate Research (Hong Kong). The research of
Philip L.H. Yu was supported by a start-up research grant
from the Education University of Hong Kong (#R4162).
A
Effectiveness of Data Augmentation
Quantitative Results.
We investigate the effectiveness of
the data augmentation in Table 7. For ‘GMBASE, Raw’, we
train GMBASE with the original MT corpus. For GMBASE, we
train the model with the augmented MT corpus. We can see
that model trained on augmented MT corpus achieves bet-
ter performance. This is reasonable since the sentence scene
graph generated from ﬁve sentences (one original sentence
and four augmented sentences) contains richer information
than the original sentence scene graph.
constr uction
center
of
r esidence
of
business/r egional
highgr ade
constr uction/building
center
of
r esidence
of
business/r egional
highgr ade
r esidential/highend
/highquality/com m er cial
facility
housing/highend
? ?
? ?
?
? ?
?
? ? /? ? ?
? ?
? ? ? /? ?
/? ? ? /? ? ?
? ?
? ? /? ?
Suitable to construction of high-grade residences and regional business center.
Suitable for high-end housing and commercial facilities.
Suitable for high-end housing and commercial construction.
Suitable for the construction of high-end residential buildings and a regional business centre.
Suitable for the construction of high-quality residential buildings and regional business centers.
Gr ound-Tr uth (Zh)
? ? ? ? ? ? ? ? ? ? ?
* The center is a residential center.
? ? ? ? ? ? ? ?
* The construction center of the center.
? ? ? ? ? ? ? ? ? ? ? ? ? ?
* Suitable for building high-end residential and commercial facilities.
Gr ound-Tr uth (En)
? ? ? ? ? ? ? ? ? ? ? ? ? ?
 
Raw
Aug
Aug
Back-Tr ans.
Sentence Scene Gr aph 
(En, Raw)
Sentence Scene Gr aph
(En, Aug)
Sentence Scene Gr aph
(Zh, Mapped)
kid
idea
have
dism anlting
after
Sentence Scene Gr aph 
(En, Raw)
Sentence Scene Gr aph
(En, Aug)
Sentence Scene Gr aph
(Zh, Mapped)
After dismantling the toy, the kid has no idea how to reassemble it.
The little boy opened the toy, but he never knew how to make up again.
The little boy had broken up his toys and was in a disarray.
After the toy has been dismantled, the child does not know how to return it.
After dismantling the toy, the child has no idea how to reassemble it.
Gr ound-Tr uth (Zh)
? ? ? ? ? ? ? ? ? ? ? ?
* The kids gave me their toys.
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
* This kid took the toy apart but didn't know how to assemble again.
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
* This kid took the toy apart but didn't know how to assemble it again.
Gr ound-Tr uth (En)
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
Raw
Aug
Aug
Back-Tr ans.
toy
r eassem ble
kid/child
idea
have
dism anlting
after
r eassem ble
toy
open
dism anlting
r eassem ble
little
? ?
? ?
?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ? ?
Figure 4: Qualitative results of models trained on differ-
ent sentence scene graphs. ‘Raw’ represents that model is
trained on the original corpus. ‘Aug’ represents that model is
trained on the augmented corpus. * represents English trans-
lation translated by google translator.
Method
LXE LKL ⩾3 Obj/G B@1 B@4
M
R
GMBASE, Raw


15.4%
22.5
5.1 12.8 24.6
GMBASE


59.3%
26.6
7.3 15.1 28.1
Table 7: Performance comparison of the Chinese sentence
generation on the test split of MT corpus. ⩾3 Obj/G means
the percentage of scene graphs whose number of objects is
greater than or equal to 3. B@n is short for BLEU-n, M is
short for METEOR and R is short for ROUGE.
Qualitative Results.
Fig. 4 also visualizes some aug-
mented scene graphs and examples of generated sentences.
Generally, compared with the raw English sentence scene
graphs, the English sentence scene graphs generated from
the augmented sentences are mucher richer. Also, the HGM
model in both ﬁgures can generate more accurate and de-
scriptive sentences than other baselines, demonstrating that
our hierarchical graph mapping can better map the scene
graph from one language to another.
B
Implementation details
In this paper, we use English and Chinese as the source and
target languages, respectively.We collect a paired English-
Chinese corpus from existing MT datasets as the paral-
lel corpus in cross-lingual auto-encoding phase. For cross-
lingual auto-encoding, we collect a paired English-Chinese
corpus from existing MT datasets, including WMT19 (Bar-
rault et al. 2019), AIC MT (Wu et al. 2017), UM (Tian et al.
2014), and Trans-zh
(Xu 2019). Since the vocabulary in
the collected MT corpus is quite different from the vocab-
ulary in image caption datasets, we ﬁlter the sentences in
MT datasets according to an existing caption-style dictio-
nary containing 7,096 words in Li et al. (2019). All the sen-
tences longer than 16 or shorter than 10 words are dropped.
We further ﬁlter the sentences in MT corpus by reserving
those sentences with 90% of words in the sentence that ap-
peared in the dictionary, resulting in a ﬁltered MT corpus of
161,613 sentence pairs.
References
Anderson, P.; Fernando, B.; Johnson, M.; and Gould, S.
2016. Spice: Semantic propositional image caption evalu-
ation. In ECCV.
Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;
Gould, S.; and Zhang, L. 2018. Bottom-up and top-down at-
tention for image captioning and visual question answering.
In CVPR.
Barrault, L.; Bojar, O.; Costa-Juss`a, M. R.; Federmann, C.;
Fishel, M.; Graham, Y.; Haddow, B.; Huck, M.; Koehn, P.;
Malmasi, S.; et al. 2019. Findings of the 2019 conference
on machine translation (wmt19). In WMT.
Conneau, A.; Rinott, R.; Lample, G.; Williams, A.; Bow-
man, S.; Schwenk, H.; and Stoyanov, V. 2018. XNLI: Eval-
uating Cross-lingual Sentence Representations. In EMNLP.
Denkowski, M.; and Lavie, A. 2014. Meteor universal: Lan-
guage speciﬁc translation evaluation for any target language.
In ACL.
Eberhard, D. M.; Simons, G. F.; and Fennig, C. D., eds.
2019. Ethnologue: Languages of the World. SIL Interna-
tional, 22 edition.
Espl`a, M.; Forcada, M.; Ram´ırez-S´anchez, G.; and Hoang,
H. 2019.
ParaCrawl: Web-scale parallel corpora for the
languages of the EU. In Proceedings of Machine Transla-
tion Summit XVII Volume 2: Translator, Project and User
Tracks, 118–119. Dublin, Ireland: European Association for
Machine Translation.

Feng, Y.; Ma, L.; Liu, W.; and Luo, J. 2019. Unsupervised
image captioning. In CVPR.
Gu, J.; Joty, S.; Cai, J.; and Wang, G. 2018. Unpaired image
captioning by language pivoting. In ECCV.
Gu, J.; Joty, S.; Cai, J.; Zhao, H.; Yang, X.; and Wang, G.
2019.
Unpaired image captioning via scene graph align-
ments. In ICCV.
Gu, J.; Kuen, J.; Joty, S.; Cai, J.; Morariu, V.; Zhao, H.;
and Sun, T. 2020.
Self-supervised relationship probing.
NeurIPS.
Hu, J.; Ruder, S.; Siddhant, A.; Neubig, G.; Firat, O.; and
Johnson, M. 2020.
XTREME: A Massively Multilingual
Multi-task Benchmark for Evaluating Cross-lingual Gener-
alization. CoRR, abs/2003.11080.
Johnson, J.; Gupta, A.; and Fei-Fei, L. 2018. Image genera-
tion from scene graphs. In CVPR.
Joulin, A.; Bojanowski, P.; Mikolov, T.; J´egou, H.; and
Grave, E. 2018.
Loss in Translation: Learning Bilingual
Word Mapping with a Retrieval Criterion. In EMNLP.
Krishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.;
Kravitz, J.; Chen, S.; Kalantidis, Y.; Li, L.-J.; Shamma,
D. A.; et al. 2017. Visual genome: Connecting language and
vision using crowdsourced dense image annotations. IJCV.
Laina, I.; Rupprecht, C.; and Navab, N. 2019. Towards Un-
supervised Image Captioning with Shared Multimodal Em-
beddings. In ICCV.
Lan, W.; Li, X.; and Dong, J. 2017. Fluency-guided cross-
lingual image captioning. In ACMMM.
Li, X.; Xu, C.; Wang, X.; Lan, W.; Jia, Z.; Yang, G.; and
Xu, J. 2019. COCO-CN for Cross-Lingual Image Tagging,
Captioning, and Retrieval. TMM.
Lin, C.-Y. 2004. Rouge: A package for automatic evaluation
of summaries. In ACL.
Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-
manan, D.; Doll´ar, P.; and Zitnick, C. L. 2014. Microsoft
coco: Common objects in context. In ECCV.
Ng, N.; Yee, K.; Baevski, A.; Ott, M.; Auli, M.; and Edunov,
S. 2019. Facebook FAIR’s WMT19 News Translation Task
Submission. arXiv preprint arXiv:1907.06616.
Nguyen, K.; Tripathi, S.; Du, B.; Guha, T.; and Nguyen,
T. Q. 2021. In Defense of Scene Graphs for Image Cap-
tioning. In ICCV.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
BLEU: a method for automatic evaluation of machine trans-
lation. In ACL.
Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-
cnn: Towards real-time object detection with region proposal
networks. In NeurIPS.
Rennie, S. J.; Marcheret, E.; Mroueh, Y.; Ross, J.; and Goel,
V. 2017. Self-critical sequence training for image caption-
ing. In CVPR.
Schuster, S.; Krishna, R.; Chang, A.; Fei-Fei, L.; and Man-
ning, C. D. 2015.
Generating semantically precise scene
graphs from textual descriptions for improved image re-
trieval. In ACL.
Song, Y.; Chen, S.; Zhao, Y.; and Jin, Q. 2019.
Un-
paired Cross-lingual Image Caption Generation with Self-
Supervised Rewards. In ACMMM.
Tian, L.; Wong, D. F.; Chao, L. S.; Quaresma, P.; Oliveira,
F.; and Yi, L. 2014. UM-Corpus: A Large English-Chinese
Parallel Corpus for Statistical Machine Translation.
In
LREC.
Tran, K.; He, X.; Zhang, L.; Sun, J.; Carapcea, C.; Thrasher,
C.; Buehler, C.; and Sienkiewicz, C. 2016. Rich image cap-
tioning in the wild. In CVPRW.
Vedantam, R.; Lawrence Zitnick, C.; and Parikh, D. 2015.
Cider: Consensus-based image description evaluation.
In
CVPR.
Vinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015.
Show and tell: A neural image caption generator. In CVPR.
Wang, Y.-S.; Liu, C.; Zeng, X.; and Yuille, A. 2018. Scene
graph parsing as dependency parsing.
arXiv preprint
arXiv:1803.09189.
Wu, J.; Zheng, H.; Zhao, B.; Li, Y.; Yan, B.; Liang, R.;
Wang, W.; Zhou, S.; Lin, G.; Fu, Y.; et al. 2017. AI Chal-
lenger: A Large-scale Dataset for Going Deeper in Image
Understanding. arXiv preprint arXiv:1711.06475.
Wu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.; Norouzi, M.;
Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.;
et al. 2016.
Google’s neural machine translation system:
Bridging the gap between human and machine translation.
arXiv preprint arXiv:1609.08144.
Xu, B. 2019. NLP Chinese Corpus: Large Scale Chinese
Corpus for NLP.
Yang, X.; Tang, K.; Zhang, H.; and Cai, J. 2019.
Auto-
encoding scene graphs for image captioning. In CVPR.
Zellers, R.; Yatskar, M.; Thomson, S.; and Choi, Y. 2018.
Neural motifs: Scene graph parsing with global context. In
CVPR.
Zhong, Y.; Wang, L.; Chen, J.; Yu, D.; and Li, Y. 2020. Com-
prehensive Image Captioning via Scene Graph Decomposi-
tion. In Vedaldi, A.; Bischof, H.; Brox, T.; and Frahm, J.,
eds., ECCV.
Zhu, J.-Y.; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-
paired image-to-image translation using cycle-consistent ad-
versarial networks. In ICCV.

