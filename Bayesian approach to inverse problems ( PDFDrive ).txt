
Bayesian Approach to Inverse Problems  

Bayesian Approach to 
Inverse Problems 
 
 
 
 
 
 
 
 
 
 
 
 
Edited by 
Jérôme Idier 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
First published in France in 2001 by Hermes Science/Lavoisier entitled “Approche bayésienne pour les 
problèmes inverses” 
First published in Great Britain and the United States in 2008 by ISTE Ltd and John Wiley & Sons, Inc. 
Translation made by Mrs Becker 
 
Apart from any fair dealing for the purposes of research or private study, or criticism or review, as 
permitted under the Copyright, Designs and Patents Act 1988, this publication may only be reproduced, 
stored or transmitted, in any form or by any means, with the prior permission in writing of the publishers, 
or in the case of reprographic reproduction in accordance with the terms and licenses issued by the CLA. 
Enquiries concerning reproduction outside these terms should be sent to the publishers at the 
undermentioned address: 
 
ISTE Ltd  
John Wiley & Sons, Inc.  
6 Fitzroy Square  
111 River Street 
London W1T 5DX  
Hoboken, NJ 07030 
UK  
USA  
www.iste.co.uk  
www.wiley.com 
 
© ISTE Ltd, 2008 
© LAVOISIER, 2001 
 
The rights of Jérôme Idier to be identified as the author of this work have been asserted by him in 
accordance with the Copyright, Designs and Patents Act 1988. 
 
Library of Congress Cataloging-in-Publication Data 
 
Bayesian approach to inverse problems / edited by Jérôme Idier. 
       p. cm. 
  Includes bibliographical references and index. 
  ISBN: 978-1-84821-032-5 
 1.  Inverse problems (Differential equations) 2.  Bayesian statistical decision theory  I. Idier, Jérôme.  
  QA371.B365 2008 
  515'.357--dc22 
                                                            2007047723 
 
British Library Cataloguing-in-Publication Data 
A CIP record for this book is available from the British Library  
ISBN: 978-1-84821-032-5 
Printed and bound in Great Britain by Antony Rowe Ltd, Chippenham, Wiltshire. 
 

Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
Jérôme IDIER
PART I. FUNDAMENTAL PROBLEMS AND TOOLS . . . . . . . . . . . . . . .
23
Chapter 1. Inverse Problems, Ill-posed Problems
. . . . . . . . . . . . . . .
25
Guy DEMOMENT, Jérôme IDIER
1.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
1.2. Basic example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.3. Ill-posed problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
1.3.1. Case of discrete data . . . . . . . . . . . . . . . . . . . . . . . . . .
31
1.3.2. Continuous case . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
1.4. Generalized inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
1.4.1. Pseudo-solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
1.4.2. Generalized solutions . . . . . . . . . . . . . . . . . . . . . . . . .
35
1.4.3. Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
1.5. Discretization and conditioning . . . . . . . . . . . . . . . . . . . . . . .
36
1.6. Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
1.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
Chapter 2. Main Approaches to the Regularization of Ill-posed Problems .
41
Guy DEMOMENT, Jérôme IDIER
2.1. Regularization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.1.1. Dimensionality control
. . . . . . . . . . . . . . . . . . . . . . . .
42
2.1.1.1. Truncated singular value decomposition . . . . . . . . . . . .
42
2.1.1.2. Change of discretization . . . . . . . . . . . . . . . . . . . . .
43
2.1.1.3. Iterative methods . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.1.2. Minimization of a composite criterion . . . . . . . . . . . . . . . .
44
2.1.2.1. Euclidian distances . . . . . . . . . . . . . . . . . . . . . . . .
45
Table o f Contents

6
Bayesian Approach to Inverse Problems
2.1.2.2. Roughness measures . . . . . . . . . . . . . . . . . . . . . . .
46
2.1.2.3. Non-quadratic penalization . . . . . . . . . . . . . . . . . . .
47
2.1.2.4. Kullback pseudo-distance . . . . . . . . . . . . . . . . . . . .
47
2.2. Criterion descent methods . . . . . . . . . . . . . . . . . . . . . . . . . .
48
2.2.1. Criterion minimization for inversion . . . . . . . . . . . . . . . . .
48
2.2.2. The quadratic case . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
2.2.2.1. Non-iterative techniques . . . . . . . . . . . . . . . . . . . . .
49
2.2.2.2. Iterative techniques
. . . . . . . . . . . . . . . . . . . . . . .
50
2.2.3. The convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
2.2.4. General case
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
2.3. Choice of regularization coefﬁcient . . . . . . . . . . . . . . . . . . . . .
53
2.3.1. Residual error energy control . . . . . . . . . . . . . . . . . . . . .
53
2.3.2. “L-curve” method . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
2.3.3. Cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
2.4. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
Chapter 3. Inversion within the Probabilistic Framework
. . . . . . . . . .
59
Guy DEMOMENT, Yves GOUSSARD
3.1. Inversion and inference
. . . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.2. Statistical inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.2.1. Noise law and direct distribution for data . . . . . . . . . . . . . .
61
3.2.2. Maximum likelihood estimation . . . . . . . . . . . . . . . . . . .
63
3.3. Bayesian approach to inversion . . . . . . . . . . . . . . . . . . . . . . .
64
3.4. Links with deterministic methods . . . . . . . . . . . . . . . . . . . . . .
66
3.5. Choice of hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . .
67
3.6. A priori model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
3.7. Choice of criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.8. The linear, Gaussian case . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.8.1. Statistical properties of the solution . . . . . . . . . . . . . . . . .
71
3.8.2. Calculation of marginal likelihood . . . . . . . . . . . . . . . . . .
73
3.8.3. Wiener ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
3.9. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
PART II. DECONVOLUTION . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
Chapter 4. Inverse Filtering and Other Linear Methods
. . . . . . . . . . .
81
Guy LE BESNERAIS, Jean-François GIOVANNELLI, Guy DEMOMENT
4.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
4.2. Continuous-time deconvolution . . . . . . . . . . . . . . . . . . . . . . .
82
4.2.1. Inverse ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
4.2.2. Wiener ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.3. Discretization of the problem . . . . . . . . . . . . . . . . . . . . . . . .
85
4.3.1. Choice of a quadrature method . . . . . . . . . . . . . . . . . . . .
85

7
4.3.2. Structure of observation matrix H . . . . . . . . . . . . . . . . . .
87
4.3.3. Usual boundary conditions . . . . . . . . . . . . . . . . . . . . . .
89
4.3.4. Problem conditioning . . . . . . . . . . . . . . . . . . . . . . . . .
89
4.3.4.1. Case of the circulant matrix . . . . . . . . . . . . . . . . . . .
90
4.3.4.2. Case of the Toeplitz matrix . . . . . . . . . . . . . . . . . . .
90
4.3.4.3. Opposition between resolution and conditioning . . . . . . .
91
4.3.5. Generalized inversion . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.4. Batch deconvolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
4.4.1. Preliminary choices . . . . . . . . . . . . . . . . . . . . . . . . . .
92
4.4.2. Matrix form of the estimate . . . . . . . . . . . . . . . . . . . . . .
93
4.4.3. Hunt’s method (periodic boundary hypothesis) . . . . . . . . . . .
94
4.4.4. Exact inversion methods in the stationary case . . . . . . . . . . .
96
4.4.5. Case of non-stationary signals
. . . . . . . . . . . . . . . . . . . .
98
4.4.6. Results and discussion on examples . . . . . . . . . . . . . . . . .
98
4.4.6.1. Compromise between bias and variance in 1D deconvolution
98
4.4.6.2. Results for 2D processing . . . . . . . . . . . . . . . . . . . .
100
4.5. Recursive deconvolution . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
4.5.1. Kalman ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
4.5.2. Degenerate state model and recursive least squares . . . . . . . . .
104
4.5.3. Autoregressive state model . . . . . . . . . . . . . . . . . . . . . .
105
4.5.3.1. Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
4.5.3.2. Criterion minimized by Kalman smoother . . . . . . . . . . .
107
4.5.3.3. Example of result
. . . . . . . . . . . . . . . . . . . . . . . .
108
4.5.4. Fast Kalman ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . .
108
4.5.5. Asymptotic techniques in the stationary case . . . . . . . . . . . .
110
4.5.5.1. Asymptotic Kalman ﬁltering . . . . . . . . . . . . . . . . . .
110
4.5.5.2. Small kernel Wiener ﬁlter . . . . . . . . . . . . . . . . . . . .
111
4.5.6. ARMA model and non-standard Kalman ﬁltering
. . . . . . . . .
111
4.5.7. Case of non-stationary signals
. . . . . . . . . . . . . . . . . . . .
111
4.5.8. On-line processing: 2D case
. . . . . . . . . . . . . . . . . . . . .
112
4.6. Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
4.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
Chapter 5. Deconvolution of Spike Trains . . . . . . . . . . . . . . . . . . . .
117
Frédéric CHAMPAGNAT, Yves GOUSSARD, Stéphane GAUTIER, Jérôme IDIER
5.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
5.2. Penalization of reﬂectivities, L2LP/L2Hy deconvolutions . . . . . . . .
119
5.2.1. Quadratic regularization . . . . . . . . . . . . . . . . . . . . . . . .
121
5.2.2. Non-quadratic regularization . . . . . . . . . . . . . . . . . . . . .
122
5.2.3. L2LP or L2Hy deconvolution . . . . . . . . . . . . . . . . . . . .
123
5.3. Bernoulli-Gaussian deconvolution . . . . . . . . . . . . . . . . . . . . .
124
5.3.1. Compound BG model . . . . . . . . . . . . . . . . . . . . . . . . .
124
5.3.2. Various strategies for estimation . . . . . . . . . . . . . . . . . . .
124
Table of Contents

8
Bayesian Approach to Inverse Problems
5.3.3. General expression for marginal likelihood . . . . . . . . . . . . .
125
5.3.4. An iterative method for BG deconvolution
. . . . . . . . . . . . .
126
5.3.5. Other methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
128
5.4. Examples of processing and discussion
. . . . . . . . . . . . . . . . . .
130
5.4.1. Nature of the solutions . . . . . . . . . . . . . . . . . . . . . . . . .
130
5.4.2. Setting the parameters . . . . . . . . . . . . . . . . . . . . . . . . .
132
5.4.3. Numerical complexity . . . . . . . . . . . . . . . . . . . . . . . . .
133
5.5. Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
5.5.1. Generalization of structures of R and H . . . . . . . . . . . . . . .
134
5.5.2. Estimation of the impulse response . . . . . . . . . . . . . . . . . .
134
5.6. Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
5.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
Chapter 6. Deconvolution of Images
. . . . . . . . . . . . . . . . . . . . . . .
141
Jérôme IDIER, Laure BLANC-FÉRAUD
6.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
6.2. Regularization in the Tikhonov sense . . . . . . . . . . . . . . . . . . . .
142
6.2.1. Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
6.2.1.1. Case of a monovariate signal . . . . . . . . . . . . . . . . . .
142
6.2.1.2. Multivariate extensions . . . . . . . . . . . . . . . . . . . . .
143
6.2.1.3. Discrete framework . . . . . . . . . . . . . . . . . . . . . . .
144
6.2.2. Connection with image processing by linear PDE . . . . . . . . .
144
6.2.3. Limits of Tikhonov’s approach . . . . . . . . . . . . . . . . . . . .
145
6.3. Detection-estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
6.3.1. Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
6.3.2. Disadvantages
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
6.4. Non-quadratic approach . . . . . . . . . . . . . . . . . . . . . . . . . . .
150
6.4.1. Detection-estimation and non-convex penalization . . . . . . . . .
154
6.4.2. Anisotropic diffusion by PDE . . . . . . . . . . . . . . . . . . . . .
155
6.5. Half-quadratic augmented criteria
. . . . . . . . . . . . . . . . . . . . .
156
6.5.1. Duality between non-quadratic criteria and HQ criteria . . . . . .
157
6.5.2. Minimization of HQ criteria . . . . . . . . . . . . . . . . . . . . . .
158
6.5.2.1. Principle of relaxation . . . . . . . . . . . . . . . . . . . . . .
158
6.5.2.2. Case of a convex function φ . . . . . . . . . . . . . . . . . . .
159
6.5.2.3. Case of a non-convex function φ . . . . . . . . . . . . . . . .
159
6.6. Application in image deconvolution . . . . . . . . . . . . . . . . . . . .
159
6.6.1. Calculation of the solution
. . . . . . . . . . . . . . . . . . . . . .
159
6.6.2. Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
6.7. Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
6.8. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165

9
PART III. ADVANCED PROBLEMS AND TOOLS
. . . . . . . . . . . . . . . .
169
Chapter 7. Gibbs-Markov Image Models
. . . . . . . . . . . . . . . . . . . .
171
Jérôme IDIER
7.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
7.2. Bayesian statistical framework . . . . . . . . . . . . . . . . . . . . . . .
172
7.3. Gibbs-Markov ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
7.3.1. Gibbs ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
7.3.1.1. Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
7.3.1.2. Trivial examples . . . . . . . . . . . . . . . . . . . . . . . . .
175
7.3.1.3. Pairwise interactions, improper laws . . . . . . . . . . . . . .
176
7.3.1.4. Markov chains . . . . . . . . . . . . . . . . . . . . . . . . . .
176
7.3.1.5. Minimum cliques, non-uniqueness of potential . . . . . . . .
177
7.3.2. Gibbs-Markov equivalence . . . . . . . . . . . . . . . . . . . . . .
177
7.3.2.1. Neighborhood relationship . . . . . . . . . . . . . . . . . . .
177
7.3.2.2. Deﬁnition of a Markov ﬁeld
. . . . . . . . . . . . . . . . . .
178
7.3.2.3. A Gibbs ﬁeld is a Markov ﬁeld . . . . . . . . . . . . . . . . .
179
7.3.2.4. Hammersley-Clifford theorem . . . . . . . . . . . . . . . . .
179
7.3.3. Posterior law of a GMRF . . . . . . . . . . . . . . . . . . . . . . .
180
7.3.4. Gibbs-Markov models for images
. . . . . . . . . . . . . . . . . .
181
7.3.4.1. Pixels with discrete values and label ﬁelds for classiﬁcation
181
7.3.4.2. Gaussian GMRF . . . . . . . . . . . . . . . . . . . . . . . . .
182
7.3.4.3. Edge variables, composite GMRF . . . . . . . . . . . . . . .
183
7.3.4.4. Interactive edge variables . . . . . . . . . . . . . . . . . . . .
184
7.3.4.5. Non-Gaussian GMRFs
. . . . . . . . . . . . . . . . . . . . .
185
7.4. Statistical tools, stochastic sampling . . . . . . . . . . . . . . . . . . . .
185
7.4.1. Statistical tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
7.4.2. Stochastic sampling . . . . . . . . . . . . . . . . . . . . . . . . . .
188
7.4.2.1. Iterative sampling methods . . . . . . . . . . . . . . . . . . .
189
7.4.2.2. Monte Carlo method of the MCMC kind . . . . . . . . . . .
192
7.4.2.3. Simulated annealing . . . . . . . . . . . . . . . . . . . . . . .
193
7.5. Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
7.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
Chapter 8. Unsupervised Problems . . . . . . . . . . . . . . . . . . . . . . . .
197
Xavier DESCOMBES, Yves GOUSSARD
8.1. Introduction and statement of problem . . . . . . . . . . . . . . . . . . .
197
8.2. Directly observed ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
8.2.1. Likelihood properties
. . . . . . . . . . . . . . . . . . . . . . . . .
199
8.2.2. Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
200
8.2.2.1. Gradient descent . . . . . . . . . . . . . . . . . . . . . . . . .
200
8.2.2.2. Importance sampling
. . . . . . . . . . . . . . . . . . . . . .
200
8.2.3. Approximations
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
Table of Contents

10
Bayesian Approach to Inverse Problems
8.2.3.1. Encoding methods . . . . . . . . . . . . . . . . . . . . . . . .
202
8.2.3.2. Pseudo-likelihood . . . . . . . . . . . . . . . . . . . . . . . .
203
8.2.3.3. Mean ﬁeld
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
8.3. Indirectly observed ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
8.3.1. Statement of problem . . . . . . . . . . . . . . . . . . . . . . . . .
205
8.3.2. EM algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
206
8.3.3. Application to estimation of the parameters of a GMRF . . . . . .
207
8.3.4. EM algorithm and gradient . . . . . . . . . . . . . . . . . . . . . .
208
8.3.5. Linear GMRF relative to hyperparameters . . . . . . . . . . . . . .
210
8.3.6. Extensions and approximations . . . . . . . . . . . . . . . . . . . .
212
8.3.6.1. Generalized maximum likelihood . . . . . . . . . . . . . . .
212
8.3.6.2. Full Bayesian approach . . . . . . . . . . . . . . . . . . . . .
213
8.4. Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
8.5. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
PART IV. SOME APPLICATIONS
. . . . . . . . . . . . . . . . . . . . . . . . .
219
Chapter 9. DeconvolutionApplied to Ultrasonic Non-destructiveEvaluation 221
Stéphane GAUTIER, Frédéric CHAMPAGNAT, Jérôme IDIER
9.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
9.2. Example of evaluation and difﬁculties of interpretation
. . . . . . . . .
222
9.2.1. Description of the part to be inspected . . . . . . . . . . . . . . . .
222
9.2.2. Evaluation principle . . . . . . . . . . . . . . . . . . . . . . . . . .
222
9.2.3. Evaluation results and interpretation . . . . . . . . . . . . . . . . .
223
9.2.4. Help with interpretation by restoration of discontinuities . . . . .
224
9.3. Deﬁnition of direct convolution model . . . . . . . . . . . . . . . . . . .
225
9.4. Blind deconvolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226
9.4.1. Overview of approaches for blind deconvolution . . . . . . . . . .
226
9.4.1.1. Predictive deconvolution . . . . . . . . . . . . . . . . . . . .
226
9.4.1.2. Minimum entropy deconvolution . . . . . . . . . . . . . . . .
228
9.4.1.3. Deconvolution by “multipulse” technique . . . . . . . . . . .
228
9.4.1.4. Sequential estimation: estimation of the kernel, then the input 228
9.4.1.5. Joint estimation of kernel and input . . . . . . . . . . . . . .
229
9.4.2. DL2Hy/DBG deconvolution
. . . . . . . . . . . . . . . . . . . . .
230
9.4.2.1. Improved direct model . . . . . . . . . . . . . . . . . . . . . .
230
9.4.2.2. Prior information on double reﬂectivity . . . . . . . . . . . .
230
9.4.2.3. Double Bernoulli-Gaussian (DBG) deconvolution . . . . . .
230
9.4.2.4. Double hyperbolic (DL2Hy) deconvolution . . . . . . . . . .
231
9.4.2.5. Behavior of DL2Hy/DBG deconvolution methods . . . . . .
231
9.4.3. Blind DL2Hy/DBG deconvolution . . . . . . . . . . . . . . . . . .
232
9.5. Processing real data
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
9.5.1. Processing by blind deconvolution . . . . . . . . . . . . . . . . . .
233
9.5.2. Deconvolution with a measured wave . . . . . . . . . . . . . . . .
234

11
9.5.3. Comparison between DL2Hy and DBG . . . . . . . . . . . . . . .
237
9.5.4. Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240
9.6. Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240
9.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
Chapter 10. Inversion in Optical Imaging through Atmospheric Turbulence 243
Laurent MUGNIER, Guy LE BESNERAIS, Serge MEIMON
10.1. Optical imaging through turbulence . . . . . . . . . . . . . . . . . . . .
243
10.1.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
10.1.2. Image formation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
244
10.1.2.1. Diffraction . . . . . . . . . . . . . . . . . . . . . . . . . . . .
244
10.1.2.2. Principle of optical interferometry . . . . . . . . . . . . . .
245
10.1.3. Effect of turbulence on image formation . . . . . . . . . . . . . .
246
10.1.3.1. Turbulence and phase
. . . . . . . . . . . . . . . . . . . . .
246
10.1.3.2. Long-exposure imaging . . . . . . . . . . . . . . . . . . . .
247
10.1.3.3. Short-exposure imaging . . . . . . . . . . . . . . . . . . . .
247
10.1.3.4. Case of a long-baseline interferometer . . . . . . . . . . . .
248
10.1.4. Imaging techniques . . . . . . . . . . . . . . . . . . . . . . . . . .
249
10.1.4.1. Speckle techniques . . . . . . . . . . . . . . . . . . . . . . .
249
10.1.4.2. Deconvolution from wavefront sensing (DWFS) . . . . . .
250
10.1.4.3. Adaptive optics . . . . . . . . . . . . . . . . . . . . . . . . .
251
10.1.4.4. Optical interferometry . . . . . . . . . . . . . . . . . . . . .
251
10.2. Inversion approach and regularization criteria used . . . . . . . . . . .
253
10.3. Measurement of aberrations . . . . . . . . . . . . . . . . . . . . . . . .
254
10.3.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
254
10.3.2. Hartmann-Shack sensor . . . . . . . . . . . . . . . . . . . . . . .
255
10.3.3. Phase retrieval and phase diversity . . . . . . . . . . . . . . . . .
257
10.4. Myopic restoration in imaging . . . . . . . . . . . . . . . . . . . . . . .
258
10.4.1. Motivation and noise statistic . . . . . . . . . . . . . . . . . . . .
258
10.4.2. Data processing in deconvolution from wavefront sensing . . . .
259
10.4.2.1. Conventional processing of short-exposure images . . . . .
259
10.4.2.2. Myopic deconvolution of short-exposure images . . . . . .
260
10.4.2.3. Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
10.4.2.4. Experimental results . . . . . . . . . . . . . . . . . . . . . .
262
10.4.3. Restoration of images corrected by adaptive optics . . . . . . . .
263
10.4.3.1. Myopicdeconvolutionof images correctedby adaptiveoptics 263
10.4.3.2. Experimental results . . . . . . . . . . . . . . . . . . . . . .
265
10.4.4. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
10.5. Image reconstruction in optical interferometry (OI) . . . . . . . . . . .
268
10.5.1. Observation model . . . . . . . . . . . . . . . . . . . . . . . . . .
268
10.5.2. Traditional Bayesian approach
. . . . . . . . . . . . . . . . . . .
271
10.5.3. Myopic modeling . . . . . . . . . . . . . . . . . . . . . . . . . . .
272
10.5.4. Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
Table of Contents

12
Bayesian Approach to Inverse Problems
10.5.4.1. Processing of synthetic data . . . . . . . . . . . . . . . . . .
274
10.5.4.2. Processing of experimental data . . . . . . . . . . . . . . . .
276
10.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
Chapter 11. Spectral Characterization in Ultrasonic Doppler Velocimetry
285
Jean-François GIOVANNELLI, Alain HERMENT
11.1. Velocity measurement in medical imaging . . . . . . . . . . . . . . . .
285
11.1.1. Principle of velocity measurement in ultrasound imaging . . . .
286
11.1.2. Information carried by Doppler signals . . . . . . . . . . . . . . .
286
11.1.3. Some characteristics and limitations . . . . . . . . . . . . . . . .
288
11.1.4. Data and problems treated . . . . . . . . . . . . . . . . . . . . . .
288
11.2. Adaptive spectral analysis . . . . . . . . . . . . . . . . . . . . . . . . .
290
11.2.1. Least squares and traditional extensions . . . . . . . . . . . . . .
290
11.2.2. Long AR models – spectral smoothness – spatial continuity . . .
291
11.2.2.1. Spatial regularity . . . . . . . . . . . . . . . . . . . . . . . .
291
11.2.2.2. Spectral smoothness . . . . . . . . . . . . . . . . . . . . . .
292
11.2.2.3. Regularized least squares . . . . . . . . . . . . . . . . . . .
292
11.2.2.4. Optimization
. . . . . . . . . . . . . . . . . . . . . . . . . .
293
11.2.3. Kalman smoothing . . . . . . . . . . . . . . . . . . . . . . . . . .
293
11.2.3.1. State and observation equations . . . . . . . . . . . . . . . .
293
11.2.3.2. Equivalence between parameterizations . . . . . . . . . . .
294
11.2.4. Estimation of hyperparameters . . . . . . . . . . . . . . . . . . .
294
11.2.5. Processing results and comparisons . . . . . . . . . . . . . . . . .
296
11.2.5.1. Hyperparameter tuning . . . . . . . . . . . . . . . . . . . . .
296
11.2.5.2. Qualitative comparison . . . . . . . . . . . . . . . . . . . . .
296
11.3. Tracking spectral moments . . . . . . . . . . . . . . . . . . . . . . . . .
297
11.3.1. Proposed method . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
11.3.1.1. Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
11.3.1.2. Amplitudes: prior distribution and marginalization . . . . .
298
11.3.1.3. Frequencies: prior law and posterior law . . . . . . . . . . .
300
11.3.1.4. Viterbi algorithm . . . . . . . . . . . . . . . . . . . . . . . .
302
11.3.2. Likelihood of the hyperparameters . . . . . . . . . . . . . . . . .
302
11.3.2.1. Forward-Backward algorithm
. . . . . . . . . . . . . . . .
302
11.3.2.2. Likelihood gradient . . . . . . . . . . . . . . . . . . . . . . .
303
11.3.3. Processing results and comparisons . . . . . . . . . . . . . . . . .
304
11.3.3.1. Tuning the hyperparameters . . . . . . . . . . . . . . . . . .
304
11.3.3.2. Qualitative comparison . . . . . . . . . . . . . . . . . . . . .
305
11.4. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
306
11.5. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
307
Chapter 12. Tomographic Reconstruction from Few Projections
. . . . . .
311
Ali MOHAMMAD-DJAFARI, Jean-Marc DINTEN
12.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
311

13
12.2. Projection generation model . . . . . . . . . . . . . . . . . . . . . . . .
312
12.3. 2D analytical methods . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
12.4. 3D analytical methods . . . . . . . . . . . . . . . . . . . . . . . . . . .
317
12.5. Limitations of analytical methods . . . . . . . . . . . . . . . . . . . . .
317
12.6. Discrete approach to reconstruction . . . . . . . . . . . . . . . . . . . .
319
12.7. Choice of criterion and reconstruction methods . . . . . . . . . . . . .
321
12.8. Reconstruction algorithms . . . . . . . . . . . . . . . . . . . . . . . . .
323
12.8.1. Optimization algorithms for convex criteria . . . . . . . . . . . .
323
12.8.1.1. Gradient algorithms
. . . . . . . . . . . . . . . . . . . . . .
324
12.8.1.2. SIRT (Simultaneous Iterative Relaxation Techniques) . . .
325
12.8.1.3. ART (Algebraic Reconstruction Technique) . . . . . . . . .
325
12.8.1.4. ART by blocks . . . . . . . . . . . . . . . . . . . . . . . . .
326
12.8.1.5. ICD (Iterative Coordinate Descent) algorithms . . . . . . .
326
12.8.1.6. Richardson-Lucy algorithm . . . . . . . . . . . . . . . . . .
326
12.8.2. Optimization or integration algorithms . . . . . . . . . . . . . . .
327
12.9. Speciﬁc models for binary objects . . . . . . . . . . . . . . . . . . . . .
328
12.10. Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
328
12.10.1. 2D reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . .
328
12.10.2. 3D reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . .
329
12.11. Conclusions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
12.12. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
332
Chapter 13. Diffraction Tomography
. . . . . . . . . . . . . . . . . . . . . .
335
Hervé CARFANTAN, Ali MOHAMMAD-DJAFARI
13.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
335
13.2. Modeling the problem . . . . . . . . . . . . . . . . . . . . . . . . . . .
336
13.2.1. Examples of diffraction tomography applications . . . . . . . . .
336
13.2.1.1. Microwave imaging
. . . . . . . . . . . . . . . . . . . . . .
337
13.2.1.2. Non-destructive evaluation of conducting materials using
eddy currents . . . . . . . . . . . . . . . . . . . . . . . . . .
337
13.2.1.3. Geophysical exploration . . . . . . . . . . . . . . . . . . . .
338
13.2.2. Modeling the direct problem . . . . . . . . . . . . . . . . . . . . .
338
13.2.2.1. Equations of propagation in an inhomogeneous medium . .
338
13.2.2.2. Integral modeling of the direct problem . . . . . . . . . . .
339
13.3. Discretization of the direct problem . . . . . . . . . . . . . . . . . . . .
340
13.3.1. Choice of algebraic framework . . . . . . . . . . . . . . . . . . .
340
13.3.2. Method of moments
. . . . . . . . . . . . . . . . . . . . . . . . .
341
13.3.3. Discretization by the method of moments . . . . . . . . . . . . .
342
13.4. Construction of criteria for solving the inverse problem
. . . . . . . .
343
13.4.1. First formulation: estimation of x . . . . . . . . . . . . . . . . . .
344
13.4.2. Second formulation: simultaneous estimation of x and φφφ
. . . .
345
13.4.3. Properties of the criteria . . . . . . . . . . . . . . . . . . . . . . .
347
13.5. Solving the inverse problem . . . . . . . . . . . . . . . . . . . . . . . .
347
Table of Contents

14
Bayesian Approach to Inverse Problems
13.5.1. Successive linearizations . . . . . . . . . . . . . . . . . . . . . . .
348
13.5.1.1. Approximations . . . . . . . . . . . . . . . . . . . . . . . . .
348
13.5.1.2. Regularization
. . . . . . . . . . . . . . . . . . . . . . . . .
349
13.5.1.3. Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . .
349
13.5.2. Joint minimization . . . . . . . . . . . . . . . . . . . . . . . . . .
350
13.5.3. Minimizing MAP criterion . . . . . . . . . . . . . . . . . . . . . .
351
13.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
353
13.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354
Chapter 14. Imaging from Low-intensity Data . . . . . . . . . . . . . . . . .
357
Ken SAUER, Jean-Baptiste THIBAULT
14.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
357
14.2. Statistical properties of common low-intensity image data . . . . . . .
359
14.2.1. Likelihood functions and limiting behavior . . . . . . . . . . . .
359
14.2.2. Purely Poisson measurements . . . . . . . . . . . . . . . . . . . .
360
14.2.3. Inclusion of background counting noise . . . . . . . . . . . . . .
362
14.2.4. Compound noise models with Poisson information . . . . . . . .
362
14.3. Quantum-limited measurements in inverse problems . . . . . . . . . .
363
. . . . . . . . . . . . . . . . . .
363
14.3.2. Bayesian estimation
. . . . . . . . . . . . . . . . . . . . . . . . .
366
14.4. Implementation and calculation of Bayesian estimates . . . . . . . . .
368
14.4.1. Implementation for pure Poisson model . . . . . . . . . . . . . .
368
14.4.2. Bayesian implementation for a compound data model . . . . . .
370
14.5. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
372
14.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
372
List of Authors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
377
14.3.1. Maximum likelihood properties

Introduction
When a physical quantity is not directly accessible for measurement, it is common
to proceed by observing other quantities that are connected with it by physical laws.
The notion of an inverse problem corresponds to the idea of inverting these physical
laws to gain indirect access to the quantity we are interested in.
For example, in electromagnetism, calculating the electric ﬁeld induced by a
known distribution of electric charges is a direct problem, i.e., a problem posed in
“the natural direction” of physics as we are used to practising and controlling it. De-
ducing the distribution of the electric charges from measurements of the ﬁeld is, on
the other hand, an inverse problem.
Similarly, in signal processing, modeling a transmission channel that introduces
distortion, interference and parasitic signals corresponds to solving a direct problem.
Reconstructing the shape of a signal input to the channel from measurements made at
the output is an inverse problem.
The situation where the quantity of interest is directly accessible for measurement
is obviously more favorable. Nevertheless, direct measurement does not signify per-
fect measurement: the instrumental response of a piece of measuring apparatus and the
various error sources connected with the observation process (systematic error, ﬂuc-
tuations connected with the physical sensors or electronic components, quantization,
etc.) are degradations that can also be encompassed in the question of inversion.
When all is said and done, the concept of inverse problems underlies the processing
of experimental data in its broadest sense. In the experience of the authors of this book,
making it explicit that a data processing chain actually carries out an inversion is also
often a very worthwhile exercise. It brings to light ad hoc hypotheses and arbitrary
Introduction written by Jérôme IDIER.

16
Bayesian Approach to Inverse Problems
Known source distribution
(a) direct problem
of sources?
sensor
Spatial distribution
(b) inverse problem
Figure 1. A simple example of the direct problem/inverse problem pair in electromagnetism:
the direct problem consists of ﬁnding the ﬁeld at the circular boundary of the domain from the
distribution of the sources; the inverse problem consists of deducing the distribution of the
sources from measurements of the ﬁeld at the boundary
constraints, and provides a rational, modular framework for designing data processing
methods and analyzing their efﬁciency and their faults.
When approached with no special precautions, the inversion problems we meet
with in practice, unlike direct problems, have a nasty tendency to be “naturally unsta-
ble”: if there are errors, however tiny, on the data, the behavior of “naive” inversion
methods is not robust.
Let us take the example of inverse ﬁltering (or deconvolution), a classic in signal
processing. Figure 2 proposes two digital experiments:
– The ﬁrst consists of inverting a discrete convolution relationship, y = h ⋆x.
The input signal x is triangular, of length M = 101 (Figure 2a), and the impulse
response (IR) h is a discretized, truncated Gaussian of length L = 31 (Figure 2b).
The output y, of length N = M + L −1 = 131, is calculated using the Matlab
language in the form y=conv(h,x) (Figure 2c). Matlab also offers a deconvolution
method (by polynomial division) that, here, faithfully gives x again from y, in the
form deconv(y,h) (Figure 2d).
– Now let us suppose that output y was measured imperfectly, e.g. with no mea-
surement error but uniformly quantized to 10 bits, i.e., 1,024 levels: z is the quantized
output (Figure 2e), calculated in Matlab in the form z=round(y*2ˆ10)/2ˆ10.
The difference between y and z is imperceptible; and yet, deconv(z,h) (Figure 2f)
is very different from x. Note in passing that the on-line help provided by Matlab (ver-
sion 7.5) for deconv contains no warning of the unstable nature of this operation.

Introduction
17
0
50
100
0
0.2
0.4
0.6
0.8
1
(a) input signal x
0 10 20 30
0
0.02
0.04
0.06
0.08
0.1
(b) IR h
0
50
100
0
0.2
0.4
0.6
0.8
1
(c) y=conv(x,h)
0
50
100
0
0.2
0.4
0.6
0.8
1
(d) deconv(y,h)
0
50
100
0
0.2
0.4
0.6
0.8
1
(e) quantized output z
(on 10 bits)
0
50
100
−4
−3
−2
−1
0
1x 10
8
(f) deconv(z,h)
0
50
100
0
0.2
0.4
0.6
0.8
1
(g) a regularized
solution
0
50
100
−2
−1
0
1
2
3
(h) an “exact” solution
Figure 2. An example of a “naturally unstable” inverse problem: deconvolution. A tiny
difference between z and conv(x,h) is enough to make deconv(z,h) very different from
x. Like “conv”, “deconv” is an instruction in Matlab, version 7.5; it uses a non-regularized
deconvolution method which is thus not robust. In comparison, (g) corresponds to a robust
solution, obtained by Tikhonov regularization. Finally, (h) is an “exact” solution in the same
way as x, in the sense that it reproduces z exactly by quantized convolution
In the early 20th century, Hadamard characterized these unstable problems math-
ematically, qualifying them as ill posed, in the sense that they did not lend themselves
to being satisfactorily solved mathematically (and physically) [HAD 01]. Chapter 1 of
this book develops the notion of ill posed problems and makes the non-robust behavior
of “naive” inversion methods mathematically explicit.
In the 1960s, the Russian mathematician Tikhonov laid down the theoretical ba-
sis of modern inversion methods by introducing the concept of regularized solutions
[TIK 63]. These solutions result from two ingredients being brought together. They
are faithful to the data but this ingredient is not discriminating enough if the problem is
ill posed. Among other solutions that are faithful to the data, they are the most regular,
in a pragmatic sense that depends on the context. Tikhonov formalizes this trade-off
between ﬁdelity to the data and regularity by deﬁning regularized solutions as those
that minimize a composite criterion. He shows that the problem thus reformulated is
well-posed. The principle of regularization in Tikhonov’s sense is one of the main
subjects of Chapter 2.
Figure 2g illustrates the use of a method, regularized in Tikhonov’s sense, that
reproduces input x very acceptably from imperfect data z. The calculation of this

18
Bayesian Approach to Inverse Problems
type of solution is dealt with in Chapters 3 and 4. The only imperfection visible on
Figure 2g concerns the point of the triangle, which is slightly blunt. This is a logical
effect of the regularity imposed uniformly on a signal which is, in fact, locally irregular
at its midpoint (its derivative is discontinuous). The case of signals or images that are
globally regular but have localized irregularities is a very important one in practice. It
is treated in the context of deconvolution in Chapter 5 for irregularities that are “bright
spots” and in Chapter 6 for irregularities that are borders between homogeneous areas.
The latter situation is precisely the one we have in Figure 2.
The problem of Figure 2 could also be solved by positioning straight lines through
the minimization of a least squares criterion. A parametric approach of this kind,
which regularizes the problem by dimension control (Chapter 2, section 2.1.1), can be
considered as the oldest of the inversion methods as it was invented around the same
time as the least squares method in the late 18th century. One of the ﬁrst times the
principle was put into practice was when Gauss estimated the coefﬁcient of ellipticity
of the Earth from arc length measurements, having modeled the Earth’s proﬁle in the
form of an ellipse [STI 81]. This was indeed a case of an inverse problem being solved
by a parametric approach, even though the concept actually appeared much later.
Finally, we could think that the inversion would naturally become stable if the
direct problem could be modeled with no errors. If this were true, it is a more detailed
description of the direct problem that would lead to stabilization of the inversion. Let
us take the example of Figure 2, for which the exact mathematical relation linking x
and z includes quantization: the inversion of this exact relation remains unstable. In
fact, x is only one of an inﬁnite number of solutions, some of which stay remarkably
far from x; Figure 2h is an example.
Figure 2 is instructive but simplistic. In a more realistic situation, attaining a de-
scription of a direct problem – including the measuring system – in a mathematically
perfect form is, in any case, more than we can hope for. In the inversion ﬁeld, it
is widely accepted that a credible inverse method must possess a minimum of robust-
ness with respect to imperfect modeling of the direct problem. Adding pseudo-random
noise to simulated data is a way of testing this robustness. Testing inversion only with
“exact” simulated data is sometimes called the inverse crime.
Regularization in the Tikhonov sense, dimensionality control, adoption of such or
such a parametric model, etc., there is no universal way of stabilizing an ill-posed
inversion problem. The regularity of the solution must be deﬁned case by case, in
a form that may therefore appear subjective. Due to this, the concept of regulariza-
tion is sometimes criticized or misunderstood. In fact, regularization is part of an
application-oriented approach: it is not a question of inverting abstract problems that
can be characterized by an input-output equation, but of solving real problems, where
there is always advantage to be drawn from a few general characteristics of the quan-
tity we are interested in, which may have been neglected in the initial formulation.

Introduction
19
The practical success of Tikhonov’s regularized approach and its subsequent evolu-
tions has demonstrated that the approach is well founded. It is now accepted that an
ill-posed inverse problem cannot be satisfactorily solved without some prior informa-
tion, and this prior information is often qualitative or partial. For example, in an image
restoration problem, it is desirable to take into consideration the fact that an image is
generally composed of homogeneous regions, but this characteristic is qualitative and
does not directly correspond to a mathematical model.
The encoding of uncertain or partial information can be envisaged within a
Bayesian probabilistic framework. Work published a considerable time ago [FRA 70]
showed that Tikhonov’s contribution could be interpreted in this framework. For di-
rect problems formulated deterministically, the handling of probabilistic rules for the
inversion gives rise to comprehension difﬁculties. It has to be understood that these
probabilistic rules are inference rules: they enable states of knowledge to be quantiﬁed
and their evolution, through measurements, is itself uncertain because of errors. So
we are not judging the fundamentally deterministic or random nature of the observed
phenomena or even of the measurement errors. In this respect, Jaynes’ work, brought
together in [JAY 03], provides a reference for understanding the Bayesian approach in
the data processing ﬁeld. Chapter 3 of this book takes its inspiration from this work.
Modern methods for solving inverse problems have been arousing increasing in-
dustry, medical imaging, the nuclear electricity industry and civil engineering. Added
up over all these sectors, its scientiﬁc and economic impact is enormous.
As far as the structure of inverse problems is concerned, very different ﬁelds may
have very similar needs. However, the compartmentalization of scientiﬁc disciplines
makes it difﬁcult for ideas and methods to circulate. In this respect, it is the role of
the signal processing community to respond to needs common to other disciplines in
terms of data processing methods and algorithms.
It was with this in mind that this
book was written. Its 14 chapters are grouped together in four parts.
Part I is devoted to introducing the problems and the basic inversion tools and is
the most abstract. It comprises three chapters:
– Chapter 1 introduces the problem of inversion as a whole, in a structured mathe-
matical framework. It gives the characteristics of inverse problems posed in a contin-
uous or discrete framework, and of ill-posed problems. It introduces the ideas of the
pseudo-solutions and generalized inverse;
– Chapter 2 introduces the essential characteristics of regularization theory, and
describes Tikhonov’s approach and its subsequent evolutions together with other ap-
proaches to regularization. Reminders are then given of methods for minimizing cri-
teria, followed by techniques for estimating the regularization parameter, so that an
automatic choice can be made for the trade-off between ﬁdelity to data and regularity;
of sectors of activity, such as Earth and space sciences, meteorology, the aerospace in-
terest since their beginnings in the 1960s. The question of inversion concerns a variety

20
Bayesian Approach to Inverse Problems
– Chapter 3 deals with solving inverse problems in the framework of statistical
inference. It becomes apparent that the conventional estimation technique, known as
maximum likelihood, corresponds to a non-regularized solution, whereas a Tikhonov
regularized approach ﬁnds a natural interpretation in the framework of Bayesian es-
timation. A number of questions are then re-examined in this context: the automatic
choice of parameters, and the building of models and criteria. The end of the chapter
is devoted to the Gaussian linear framework, which constitutes a fundamental special
case.
Part II is made up of Chapters 4, 5 and 6, and is entirely given over to deconvolu-
tion, as a case that is very widespread in practice and also as a very instructive case
where many of the tools introduced in these chapters can be adapted to the inversion
of problems that are structured differently:
– Chapter 4 deals with deconvolution methods yielding solutions that are linear
functions of the data. It ﬁrst studies the general properties of the solutions, then the
various algorithm structures that enable them to be calculated. Traditional signal pro-
cessing tools, such as Wiener and Kalman ﬁlters, ﬁgure among these structures;
– Chapter 5 looks at the more speciﬁc problem of deconvolution when the signal of
interest is a series of pulses. This situation is very common in numerous domains such
as non-destructive evaluation and medical imaging. Taking the pulse character of the
input signal into account leads us to two classes of nonlinear solutions according to the
data. One follows a detection-estimation approach and the other uses the minimization
of convex criteria and robust estimation;
– Chapter 6 is devoted to deconvolution when the unknown signal is “regular al-
most everywhere” and, in particular, takes this characteristic into consideration for im-
ages rather than monovariate signals. As in the previous chapter, we ﬁnd two classes
of solutions, according to whether the problem is approached in terms of detection of
edges or as a problem of image restoration in robust form.
Part III groups together two chapters introducing “advanced tools” speciﬁc to the
Bayesian framework presented in Chapter 3:
– Chapter 7 is concerned with imaging from a probabilistic point of view. The
composite criteria of Chapter 6 are reinterpreted in this framework, which leads us to
the Gibbs-Markov models. Various sub-classes are introduced as models for images.
We next look at the statistical aspects connected with calculating the estimators and
evaluating their performance. Finally, the principle of iterative methods for random
sampling of the Gibbs-Markov models is presented;
– Chapter 8 is entirely devoted to the problem of non-supervised inversion, i.e.,
inversion without a regularization parameter ﬁxed by the user. This question, already
mentioned in Chapters 2 and 3, is of considerable theoretical and practical interest but
brings together several types of methodological and algorithmic difﬁculties. Chapter 8
studies the case of a linear Markov penalization function with respect to the parameters

Introduction
21
to be estimated and proposes, in particular, deterministic or stochastic techniques for
implementing maximum likelihood, exact or approximate estimators.
Part IV, in six chapters, presents some inverse problems in their applications. This
is by no means a complete review of all the domains involving inversion; several
important ﬁelds such as heat, mechanics and geophysics are not covered1. For the
applications that are mentioned, we do not give an overall synthesis of the inversion
problems encountered but rather some typical examples chosen by the authors as con-
crete illustrations of how regularized solutions are implemented in a particular domain.
This last part also contains some important methodology extensions – myopic decon-
volution (Chapters 9 and 10), Fourier synthesis (Chapter 10), spectral estimation and
handling of hidden Markov chains (Chapter 11), and tomography problem solving
(Chapters 12, 13 and 14):
– Chapter 9 concerns industrial non-destructive evaluation using ultrasound. It
compares the implementation of the spike train deconvolution methods presented in
Chapter 5. The problem of an impulse response that is poorly known or that introduces
deformation is speciﬁcally studied. The result of this is some extended versions of the
algorithms looked at in Chapter 5;
– Chapter 10 is about the inversion problems encountered in optical imaging in as-
tronomy and, more speciﬁcally, for ground-based telescopes. In this case, atmospheric
turbulence considerably reduces the resolution of the images. Various conﬁgurations
intended to limit this degradation are considered. On the one hand, it is possible to ap-
proach the image restoration problem through myopic deconvolution; on the other, the
effects of turbulence can be partially compensated by a technique known as adaptive
optics, where the deconvolution of the images thus acquired remains a helpful step.
The end of Chapter 10 is devoted to optical interferometry, which leads to a Fourier
synthesis problem complicated by aberration due to atmospheric turbulence;
– Chapter 11 is devoted to Doppler ultrasound velocimetry, an imaging technique
that is widespread in medicine. Two data inversion problems are seen to arise: time-
frequency analysis and frequency tracking. These are spectral characterization prob-
lems that are particularly difﬁcult for two reasons: ﬁrstly, the number of observed data
points is very small and, secondly, the Shannon sampling conditions are not always
respected. Chapter 11 covers these two problems in the regularization framework in
order to compensate, at least partially, for the lack of information in the data;
– Chapter 12 considers the problem of reconstruction in X-ray tomography using
a small number of projections. The Radon transform is introduced, and after a brief
reminder of the various conventional approaches for its inversion, the algebraic and
probabilistic methods are developed more speciﬁcally. In fact these are the only meth-
ods that can be used effectively in cases where the projections are limited in number
and contain noise;
1. See [BEC 85, BUI 94, DES 90] as respective entry points to these ﬁelds.

22
Bayesian Approach to Inverse Problems
– Chapter 13 looks at how the Bayesian approach can be used to solve the problem
of diffraction tomography. For this type of problem, the measurements collected are
the waves scattered by an object and depend nonlinearly on the physical parameters
we are trying to image. This chapter deals with diffraction tomography without the
usual linear approximations, whose domains of validity do not cover all the situations
encountered in practice. The wave propagation equations provide a integral direct
model in the form of two coupled equations. These are discretized by the method of
moments. The Bayesian approach then allows the inversion to be approached by min-
imizing a penalized criterion. However, the nonlinearity of the direct model leads to
non-convexity of this criterion. In particularly difﬁcult situations where local minima
exist, the use of global optimization techniques is recommended;
– Chapter 14 studies, in the framework of medical imaging techniques such as
positron emission tomography, the situations in which the corpuscular character of
the counting measurements needs to be taken into account. Poisson’s law serves as
the reference statistical distribution here to deﬁne the likelihood of the observations.
There are also composite cases, in which Gaussian noise is added to the Poisson vari-
ables. It is sometimes possible to approximate the Poisson likelihood by a Gaussian
law. When this is not the case, the properties of the log-likelihood criterion are stud-
ied and algorithms are put forward for various situations: emission or transmission
tomography and composite cases.
Bibliography
[BEC 85] BECK J., BLACKWELL B., ST. CLAIR C., Inverse Heat Conduction, Ill-posed Prob-
lems, Wiley Interscience, New York, NY, 1985.
[BUI 94] BUI H. D., Inverse Problems in the Mechanics of Materials: An Introduction, CRC
Press, Boca Raton, FL, 1994.
[DES 90] DESAUBIES Y., TARANTOLA A., ZINN-JUSTIN J., Eds., Oceanographic and Geo-
physical Tomography, Amsterdam, The Netherlands, Elsevier Science Publishers (North-
Holland), 1990.
[FRA 70] FRANKLIN J. N., “Well-posed stochastic extensions of ill-posed linear problems”,
J. Math. Anal. Appl., vol. 31, p. 682-716, 1970.
[HAD 01] HADAMARD J., “Sur les problèmes aux dérivées partielles et leur signiﬁcation
physique”, Princeton University Bull., vol. 13, 1901.
[JAY 03] JAYNES E. T., Probability Theory – The Logic of Science, Cambridge University
Press, Apr. 2003.
[STI 81] STIGLER S. M., “Gauss and the invention of least squares”, Annals Statist., vol. 9,
num. 3, p. 465-474, 1981.
[TIK 63] TIKHONOV A., “Regularization of incorrectly posed problems”, Soviet. Math. Dokl.,
vol. 4, p. 1624-1627, 1963.


PART I
Fundamental Problems and Tools

Chapter 1
Inverse Problems, Ill-posed Problems
1.1. Introduction
In many ﬁelds of applied physics, such as optics, radar, heat, spectroscopy, geo-
physics, acoustics, radioastronomy, non-destructive evaluation, biomedical engineer-
ing, instrumentation and imaging in general, we are faced with the problem of
determining the spatial distribution of a scalar or vector quantity – we often talk
about an object – from direct measurements – called an image – or indirect mea-
surements – called projections in the case of tomography, for example – of this
object. Solving such imaging problems can habitually be broken down into three
stages [HER 87, KAK 88]:
– a direct problem where, knowing the object and the observation mechanism,
we establish a mathematical description of the data observed. This model needs to
be accurate enough to provide a correct description of the physical observation phe-
nomenon and yet simple enough to lend itself to subsequent digital processing;
– an instrumentation problem in which the most informative data possible must be
acquired so that the imaging problem can be solved in the best conditions;
– an inverse problem where the object has to be estimated from the preceding
model and data.
Obtaining a good estimate of the object obviously requires these three sub-pro-
blems to be studied in a coordinated way. However, the characteristic that these image
reconstruction or restoration problems have in common is that they are often ill-posed
or ill-conditioned. Higher level problems that are found in computer vision, such as
Chapter written by Guy DEMOMENT and Jérôme IDIER.

26
Bayesian Approach to Inverse Problems
image segmentation, optical ﬂow processing and shape reconstruction from shading,
are also inverse problems and suffer from the same difﬁculties [AND 77, BER 88,
MAR 87]. In the same way, a problem such as spectral analysis, which has similarities
with the Fourier synthesis used in radio-astronomy, for example, and which is not
usually treated as an inverse problem, can gain from being approached this way, as we
will see later.
Schematically, there are two broad communities that are interested in these inverse
problems from a methodological point of view:
– the mathematical physics community, with the seminal works of Phillips,
Twomey and Tikhonov in the 1960s [PHI 62, TIK 63, TWO 62]. Sabatier was one
of the pioneers in France [SAB 78]. A representative journal is Inverse Problems;
– the statistical data processing community, which can be linked to the work of
Franklin in the late 1960s [FRA 70], although the ideas involved – the basis of Wiener
ﬁltering – had been bubbling beneath the surface in many works for several years
[FOS 61]. The Geman brothers gave a major boost to image processing about twenty
years ago [GEM 84] A representative journal is IEEE Transactions on Image Process-
ing.
A very rough distinction can be made between these two communities by saying that
the former deals with the problem in an inﬁnite dimension, with the questions of ex-
istence, uniqueness and stability, which become very complicated for nonlinear direct
problems, and solves it numerically in ﬁnite dimensions, while the latter starts with a
problem for which the discretization has already been performed and is not called into
question, and takes advantage of the ﬁnite nature of the problem to introduce prior
information built up from probabilistic models.
In this chapter, we propose to use a basic example to point out the difﬁculties that
arise when we try to solve these inverse problems.
1.2. Basic example
We will now illustrate the basic concepts introduced in this chapter by an artiﬁcial
example that mixes the essential characteristics of several types of inverse problems.
We are looking for a spectrum, the square of the modulus of a function
⌢x(ν), ν ∈
 but, because of the experimental constraints, we only have access to the dual domain
of the variable ν, through the function x(t) of which
⌢x(ν) is the Fourier transform
(FT). What is more, imperfections in the apparatus mean that the function x(t) is only
observable as weighted by a “window” h(t), which gives the observable function y(t):
y(t) = h(t) x(t) .
(1.1)

Inverse Problems, Ill-posed Problems
27
To make our ideas clear, let us think of a visible optical interferometry device like
that by Michelson. To have access to the emission spectrum of the light source, we
measure an energy ﬂux as a function of the phase difference between two optical
paths. The interferogram obtained is, ignoring the additional constant, the Fourier
transform of the function we are looking for but the limitations of the apparatus make
the interferogram observable only in a limited area of space, which is equivalent to its
being modulated by a weighting function h(t). This is assumed to be known but the
experimental data that is actually available is made up of a ﬁnite number of regularly-
spaced samples of the function y(t), which inevitably contain measuring errors that
we assume to be additive. If we take a unit sampling step, we can write:
yn = hn xn + bn ,
n = 1, . . . , N,
(1.2)
where yn designates the available data, hn and xn the samples of the functions h(t)
and x(t) respectively, and bn the measurement “noise”. This is a special case of a
system of linear equations of the form:
y = Ax + b
(1.3)
that we will ﬁnd repeatedly throughout this book. Here we have a diagonal matrix A
which, at ﬁrst glance, appears to be a simple situation.
A ﬁrst difﬁculty appears, however, independently of the presence of the weighting
h(t): the discrete nature of the data means that we only have information on
⌢x1(ν),
ν ∈[0, 1], a 1-periodic function deduced from
⌢x(ν) by the periodization due to the
sampling, since we have:
xn =
 1
0
⌢x1(ν) exp {2jπνn} dν .
(1.4)
The samples xn are in fact the Fourier series development coefﬁcients of
⌢x1. To have
any hope of accessing
⌢x, it is necessary for
⌢x(ν) to have limited support and for the
sampling step to be such that there is no aliasing. Observation model (1.2) can thus be
written indifferently:
yn =
 1
0
⌢
h ⋆
⌢x1(ν) exp {2jπνn} dν + bn ,
(1.5)
where
⌢
h(ν) is the FT of h(t). The presence of this convolution core expresses the loss
of resolving power of the instrument due to the weighting by h(t).
A simulated example is presented in Figure 1.1. Signal x(t) is composed of three
sine waves, the spectrum of which is marked by the circles in Figure 1.1a. Two have
frequencies that are close together (relative frequency difference less than 0.008). Re-
sponse
⌢
h(ν) is a Gaussian of standard deviation σ⌢h = 0.0094 intentionally chosen

28
Bayesian Approach to Inverse Problems
Spectrum
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Relative frequency
(a)
20
40
60
80
100
120
−1
−0.5
0
0.5
1
1.5
Relative frequency
(b)
Figure 1.1. (a) Spectrum
⌢x of a linear combination of three sine waves, indicated by circles,
and
⌢h ⋆
⌢x, where
⌢h is a Gaussian of standard deviation close to 0.01 in relative frequency;
(b) 128 data points y simulating an interferogram that contains noise and is quantiﬁed,
corresponding to model (1.5)
high to point out clearly the difﬁculties of inversion. The “non-resolved” spectrum
⌢
h ⋆
⌢x is also represented in Figure 1.1a. Figure 1.1b superposes the N = 128 simu-
lated data yn and the series of weighting coefﬁcients hn, which also have a Gaussian
form (of standard deviation 1/2πσ⌢h = 17).
A second difﬁculty comes from the impossibility of inverting equation (1.5) in a
mathematically exact way, i.e., of ﬁnding the “true” function
⌢x1 among other candi-
date functions, even in the absence of noise. Consider, for example, the FT
⌢x of a
stable series {xn}
  such that:
xn = yn/hn
if
n ∈{1, . . . , N}
and
hn ̸= 0 .
(1.6)
Since this series is only deﬁned for N values at most, there is an inﬁnite number of
solutions
⌢x that satisfy constraints (1.6), and are equivalent considering the data. The
problem is therefore indeterminate. In this respect, the periodogram of the data:
Γ(ν)
Δ= 1
N

N

n=1
yn exp {−2jπνn}

2
,
ν ∈[0, 1] ,
(1.7)
calculable by fast discrete FT on a ﬁne, regularly spaced grid, is a particular solution
for hn close to 1 (i.e.,
⌢
h close to a Dirac). It is obtained by extending xn = yn with
zeros on either side of the observation window.
The small number of data points and the spread of the instrument response
⌢
h give
the periodogram very low resolving power (see Figure 1.2a, curve (P1)). We can try to
get around the need to have
⌢
h by calculating the periodogram associated with yn/hn

Inverse Problems, Ill-posed Problems
29
or, in other words, by making a spectral estimator
⌢x from a time series extrapolating
yn/hn with zeros. It is also worth noting that this is none other than the trivial solution
to the problem of ﬁnding a series {xn}
  that is stable and has a minimal norm, and
which minimizes the least squares criterion – even reducing it to zero in this case:
N

n=1
(yn −hn xn)2 .
The result is disappointing (see Figure 1.2a, curve (P2)). In fact, this is not really
surprising as the series yn/hn contains aberrant values at its extremities because of
the measurement noise and quantiﬁcation. These error terms, which are ampliﬁed by
1/hn when hn is small, make a contribution to the estimated spectrum that completely
masks the peaks of the theoretical spectrum.
Spectrum (dB)
0.2
0.25
0.3
0.35
0.4
0.45
0.5
10
−4
10
−3
10
−2
10
−1
10
0
(P2)
(P1)
Relative frequency
(a)
Spectrum (dB)
0.2
0.25
0.3
0.35
0.4
0.45
0.5
10
−4
10
−3
10
−2
10
−1
10
0
Relative frequency
(b)
Figure 1.2. (a) Curve (P1) is the periodogram of the data yn represented in dB; the lack of
resolution is a result of the lack of data but also of the spread response of the instrument.
Curve (P2) is the periodogram associated with yn/hn; (b) spectral estimate obtained as the
minimizer of criterion (1.8), calculated by approximation on a discrete grid of 1,024 points,
for “well chosen” values of λ and τ
These negative results could lead us to think that the data is too poor to be used.
This is not the case, as shown by the spectral estimate whose modulus is represented
in Figure 1.2b, and which is obtained as the function
⌢x that minimizes the regularized
criterion:
N

n=1
(yn −hn xn)2 + λ
 1
0

τ 2 + |
⌢x(ν)|2 dν ,
(1.8)
where xn is connected to
⌢x by (1.4) for “well chosen” values of hyperparameters λ
and τ. As the FT
⌢x is discretized on 1,024 points, this process is strictly equivalent
to extrapolating the series of 128 observed xn by 896 values that are not necessarily
zero, unlike in the periodogram.
This example is typical of the difﬁculties that arise in the solving of numerous in-
verse problems [AND 77, BER 88, HER 87]. Certain conventional signal processing

30
Bayesian Approach to Inverse Problems
tools prove to be unsuitable whereas others provide qualitatively and quantitatively
exploitable solutions. The improvement obtained with this regularized criterion (1.8)
is striking and several questions immediately come to mind: why do we need to pe-
nalize the least squares criterion in this way? How do we obtain the argument of the
minimum of such a criterion? How do we choose the values of the hyperparameters
that are part of it? It can be said that the main part of this book is devoted to just that:
the construction and use of regularized criteria. However, it is important to under-
stand the nature of the difﬁculties encountered during inversion before studying the
regularized solutions that allow them to be solved.
1.3. Ill-posed problem
The aim of this section is to correct the false impression that the difﬁculties en-
countered in solving an inverse problem come from the discrete nature of the data and
its ﬁnite amount and that, if we had access to a continuum of values, i.e., the function
y(t) in the example above, everything would be ﬁne. Often unsuspected difﬁculties
are already present at this level. They are proper to problems known as ill-posed prob-
lems. When the problem is inevitably discretized as in the previous example, some
of these difﬁculties paradoxically disappear, but the problem most often remains ill-
conditioned.
Hadamard has deﬁned three conditions for a mathematical problem to be well-
posed [AND 80, HAD 01, NAS 81, TIK 77] (by default, it will be called ill-posed):
(a) for each item of data y in a deﬁned class Y, there exists a solution x in a stipulated
class X (existence);
(b) the solution is unique in X (uniqueness);
(c) the dependence of x on y is continuous, i.e., when the error δy on data item y
tends towards zero, the error δx induced on the solution x also tends towards zero
(continuity).
The continuity requirement is connected to that of stability or robustness of the so-
lution (with respect to the errors that inevitably occur on the data). Continuity is,
however, a necessary but not a sufﬁcient condition for robustness [COU 62]. A well-
posed problem can be ill-conditioned, which makes its solution non-robust, as we shall
see in section 1.5.
All the traditional problems of mathematical physics, such as the Dirichlet problem
for elliptical equations or the Cauchy problem for hyperbolic equations, are well-
posed in Hadamard’s sense [AND 80]. However, the “inverse” problems obtained
from “direct” problems by exchanging the roles of the solution and data are generally
not well-posed.

Inverse Problems, Ill-posed Problems
31
The example of section 1.2 clearly comes into this category of ill-posed problems
since, with a ﬁnite number of discrete data items, a solution
⌢x(ν) exists, but it is not
unique. It is interesting to note that this same problem, before discretization, is a
special case of the general problem of solving a Fredholm integral equation of the ﬁrst
kind:
y(s) =

k(s, r) x(r) dr ,
(1.9)
where y(s), x(r) and k(s, r) are replaced by y(t),
⌢x(ν) and h(t) exp {2jπνt} re-
spectively. Later in this book we will ﬁnd the same type of integral equations for other
forms of kernel k(s, r), for deconvolution, tomographic reconstruction and Fourier
synthesis.
As the data is uncertain or noisy, we cannot hope to solve this equation exactly and
the solution needs to be approached from a certain direction. The concept of distance
between functions is thus a natural way of evaluating the quality of an approximation,
which explains why x and y are often assumed to belong to Hilbert spaces. Problem
(1.9) can thus be rewritten as:
y = A x ,
x ∈X,
y ∈Y,
(1.10)
where x and y are now elements of functional spaces of inﬁnite dimension X and Y,
respectively, and where A: X →Y is the linear operator corresponding to (1.9). The
necessary and sufﬁcient conditions for the existence, uniqueness and continuity of the
solution can thus be written respectively [NAS 81]:
Y = Im A ,
Ker A = {0} ,
Im A = Im A ,
(1.11)
where Im A is the image of A (i.e., the set of y that are images of an x ∈X), Ker A
its kernel (i.e., the set of solutions to the equation A x = 0) and Im A the closure of
Im A [BRE 83].
The manner in which conditions (1.11) are stated gives rise to several comments.
On the one hand, Y = Im A implies Im A = Im A (a Hilbert space is closed upon
itself). In other words, the very existence of a solution to problem (1.9) ∀y ∈Y
implies the continuity of this solution. In contrast, if the existence condition Y =
Im A is not veriﬁed, the continuity condition seems to become pointless; in fact, it
applies to pseudo-solutions, which will be deﬁned in section 1.4.1 as minimizing the
norm ∥A x −y∥Y (without systematically reducing it to zero).
1.3.1. Case of discrete data
When the data is discrete, y is a vector of dimension N in a Euclidian space.
Ignoring errors on the data, a linear inverse problem with discrete data can be stated

32
Bayesian Approach to Inverse Problems
as follows. Given a set {Fn(x)}N
n=1 of linear functionals deﬁned on X and a set
{yn}N
n=1 of numbers, ﬁnd a function x ∈X such that:
yn = Fn(x) ,
n = 1, . . . , N.
In particular, when functionals Fn are continuous on X, Riesz theorem [BRE 83] states
that functions ψ1, . . . , ψN exist such that:
Fn(x) = ⟨x, ψn⟩X ,
where the notation ⟨· , ·⟩X designates the scalar product used in space X. The example
of equation (1.9) takes this form when y(s) is measured on a ﬁnite number of points
s1, . . . , sN, and X is an L2space. In this case we have:
ψn(r) = k(sn, r) .
This problem is a particular case of that of equation (1.10) if we deﬁne an operator A
of X in Y by the relation:
(A x)n = ⟨x, ψn⟩X
n = 1, . . . , N.
Operator A is not injective: Ker A is the closed subspace of inﬁnite dimension of all
the functions x orthogonal to the subspace engendered by the functions ψn. Con-
versely, the image of A, Im A is closed: Im A is simply Y when the functions ψn are
linearly independent; otherwise it is a subspace of dimension N ′ < N. We thus see
clearly why the example of section 1.2 is an ill-posed problem: the difﬁculty does not
lie in a lack of continuity but in a lack of uniqueness.
1.3.2. Continuous case
Let us now assume that x and y belong to the same Hilbert space and that k is
square integrable, a condition fulﬁlled by many imaging systems – it would be the
situation if our example of section 1.2 was modiﬁed so that the function
⌢y(ν) =
⌢
h ⋆
⌢x1(ν) was continuously observed. The direct problem is thus well-posed: a small
error δx on the data entails a small error δy on the solution. This condition is not,
however, fulﬁlled in the corresponding inverse problem, where it is object x that must
be calculated from response y: x = A−1 y. In fact, when kernel k is square integrable
– which would be the case for a Gaussian kernel in our example – the Riesz-Fréchet
theorem indicates that operator A is bounded and compact [BRE 83]. However, the
image of a compact operator is not closed (except in the degenerate case where its
dimension is ﬁnite). This signiﬁes that the inverse operator A−1 is not bounded, or
stable, its image is not closed and the third of Hadamard’s conditions is not satisﬁed
for the inverse problem [NAS 81].

Inverse Problems, Ill-posed Problems
33
To get a better grasp of these abstract ideas, it is handy to use the spectral properties
of compact operators in Hilbert spaces. The most remarkable property of these oper-
ators is that they can be decomposed into singular values, like matrices (the famous
singular value decomposition, or SVD). The singular system of a compact operator is
deﬁned as the set of solutions of the coupled equations:
A un = σn vn
and
A∗vn = σn un ,
(1.12)
where the singular values σn are positive numbers, where the singular functions un
and vn are elements of X and Yrespectively, and where A∗is the adjoint operator of
A, which exists since A is continuous and therefore such that: ⟨A x, y⟩Y = ⟨x, A∗y⟩X
for any x ∈X and y ∈Y 1.
When A is compact, it always possesses a singular system {un, vn; σn} with the
following properties [NAS 81]:
– σn being ordered and counted with their multiplicity (which is ﬁnite):
σ1 ≥σ2 ≥. . . ≥σn ≥. . . 0, σn tends towards 0 when n →∞and either the
limit is reached for n = n0 (in which case operator A is degenerate), or it is not
reached for any ﬁnite value of n;
– functions un form an orthonormal basis of (Ker A)⊥, the orthogonal comple-
ment of Ker A in the decomposition: X = Ker A ⊕(Ker A)⊥and the functions
vn form an orthonormal basis of (Ker (A∗))⊥, i.e., Im A, orthogonal complement of
Ker (A∗) in the decomposition Y = Ker (A∗) ⊕(Ker (A∗))⊥.
Let E ⊆
 be the set of indices n such that σn ̸= 0. The Picard criterion [NAS 81]
ensures that a function y ∈Y is in Im A if and only if:
y ∈(Ker (A∗))⊥
and

n∈E
σ−2
n
⟨y, vn⟩2 < +∞.
(1.13)
For the second condition (1.13) to be satisﬁed, it is necessary, when operator A is
not degenerate (E ≡
), for the components ⟨y, un⟩of the development of image
y on the set of eigenfunctions {vn} to tend towards zero faster than the eigenvalues
σ2
n when n →∞. This strict condition has no reason to be satisﬁed by an arbitrary
function of (Ker (A∗))⊥. Note, however, that it is naturally satisﬁed if y = A x is the
perfect image resulting from an object x of ﬁnite energy. The solution is thus written:
x =

n∈E
σ−1
n
⟨y, vn⟩un .
(1.14)
1. Note that the self-adjoint operator A∗A, which appears in the symmetrized problem A∗y =
A∗A x, veriﬁes: A∗A vn = σ2
n vn. It is thus deﬁned as non-negative since its eigenvalues are
σ2
n (which are also those of A A∗). This property will be used in section 2.1.1.

34
Bayesian Approach to Inverse Problems
However, this solution, when it exists, is unstable: a small additive perturbation
δy = ε vN, for example, on the perfect data y leads to a perturbation δx on the solution
calculated with the data y + δy:
δx = σ−1
N
⟨δy, vN⟩uN = σ−1
N ε uN .
(1.15)
The ratio ∥δx∥/ ∥δy∥equals σ−1
N , which can be arbitrarily large. The inverse linear
operator A−1 : Y →X, deﬁned by (1.14), is thus not bounded as it is not possible to
ﬁnd a constant C such that, for all y ∈Y, we have ∥A−1y∥X ≤C ∥y∥Y, which is a
necessary and sufﬁcient condition for A−1 to be continuous [BRE 83]. The ill-posed
nature of the problem stems this time from the lack of continuity and not from the lack
of uniqueness.
The need for a deeper understanding of these problems that are not mathematically
well-posed but are of great interest in engineering sciences is at the origin of two recent
branches of analysis: generalized inversion theory [NAS 76], which is summarized
below, and regularization theory, which will be the subject of the next chapter.
1.4. Generalized inversion
Let us suppose that the equation A x = 0 has non-trivial solutions.
The set
Ker A ̸= {0} of these solutions is a closed subspace of X. It is the set of “invisi-
ble objects” as they produce an image y that is zero. Let us also suppose that Im A is a
closed subspace of Y. An example is provided by the integral operator corresponding
to an ideal low-pass ﬁlter of cut-off pulsation Ω [BER 87]:
(A x)(r) =
 +∞
−∞
sin Ω(r −r′)
π(r −r′)
x(r′) dr′ .
(1.16)
If we choose X = Y = L2
, the kernel is the set of all the functions x whose FT
is zero in the band [−Ω, +Ω], while the image of A is the set of functions having a
limited band in the same interval, which is a closed subspace of L2
.
A means of re-establishing the existence and the uniqueness of the solution in the
above conditions is to redeﬁne both the space X of the solutions and the space Y of
the data. If we choose a new space X ′ which is the set of all the functions orthogonal
to Ker A (in the case of equation (1.16), X ′ is the set of functions with summable
squares and band limited to the interval [−Ω, +Ω]), and if y is restrained to a new
data space Y′ = Im A (which is, once again, in the case of equation (1.16), the set of
functions with summable squares and band limited to the interval [−Ω, +Ω]), thus, for
any y ∈Y′, there exists a unique x ∈X ′ such that A x = y (in our example (1.16),
the solution is even trivial: x = y) and the new problem is thus well-posed.
It is often possible to choose the spaces X and Y so that the problem becomes
well-posed but the practical interest of the choice is limited because it is generally the

Inverse Problems, Ill-posed Problems
35
intended application that imposes the appropriate spaces. Another means that could
be envisaged is to change the notion of a solution itself.
1.4.1. Pseudo-solutions
Let us ﬁrst consider the case where A is injective (Ker A = {0}) but not surjective
(Im A ̸= Y). The set of functions x that are solutions of the variational problem:
x ∈X minimizes ∥A x −y∥Y ,
(1.17)
where ∥·∥Y designates the norm in Y, are called pseudosolutions or least squares so-
lutions of the problem (1.10). If Im A is closed, (1.17) always has a solution, but it is
not unique if the kernel Ker A is not trivial. When it is, as we assume here, it can be
said that the well-posed character has been restored by reformulating the problem in
the form (1.17).
By making the ﬁrst variation of the function minimized in (1.17) zero, we obtain
Euler’s equation:
A∗A x = A∗y ,
(1.18)
which brings in the self-adjoint operator A∗A, the eigensystem of which can be de-
duced from the singular system of A.
1.4.2. Generalized solutions
Let us now consider the case where the uniqueness condition is not satisﬁed
(Ker A ̸= {0}, the problem is indeterminate). The set of solutions of (1.18) being
a convex, closed subset of X, it contains a single element with a minimal norm, noted
x† or xGI and called the generalized solution of (1.10). As x† is orthogonal to Ker A,
this way of deﬁning the solution is equivalent to choosing X ′ = (Ker A)⊥. In other
words, the generalized solution is a least squares solution having the minimal norm
among these solutions. As there is a single x† for every y ∈Y, a linear application
A† of Y in X is deﬁned by:
A†y = x† = x
GI .
(1.19)
The operator A† is called the generalized inverse of A and is continuous [NAS 76].
1.4.3. Example
To illustrate the idea of generalized inversion, let us go back to our example of
section 1.2 and, ﬁrst of all, neglect the weighting h(t). To impose a unique solution in

36
Bayesian Approach to Inverse Problems
the class of possible solutions prolonging the series of known xn, we can choose the
generalized inverse solution of the initial problem (1.10):
⌢x
GI(ν) = arg min
⌢x1∈L2
 [0,1]
 1
0
|
⌢x1(ν)|2 dν subject to (s. t.) xn = yn, n = 1, . . . , N.
The Plancherel and Parseval relations show us that this is equivalent to ﬁnding coefﬁ-
cients:
xn =
 1
0
⌢x(ν) exp {2jπnν} dν ,
n ∈
 ,
such that:
x = arg min
x∈ℓ2
 
n∈ |xn|2, s. t. xn = yn , n = 1, . . . , N.
The solution is trivial since the problem is separable:
xn =
 yn
if
n ∈{1, 2, . . . , N},
0
otherwise,
=⇒
⌢x
GI(ν) =
N

n=1
yn e−2jπnν,
(1.20)
whose squared modulus, with just the difference of a coefﬁcient, gives the Schuster
periodogram of equation (1.7). The case of weighting by h(t) is treated in the same
way, by replacing yn by yn/hn. It can thus be seen that the periodogram is the gen-
eralized inverse solution of a spectral analysis problem that is ill-posed because the
number of data points is ﬁnite.
1.5. Discretization and conditioning
A ﬁrst description of a direct problem often brings in functions of real variables
(time, frequency, space variables, etc.), representing the physical quantities involved:
quantities accessible for measurement and quantities of interest that are unknown. The
analysis of the problem at this level of description has provided an explanation for the
difﬁculties that arise during inversion, by situating us in functional spaces of inﬁnite
dimension. We have thus seen that, in the case of a direct problem described by an
integral equation of the ﬁrst kind, the inversion is often an ill-posed problem as it is
unstable.
This analysis is, however, insufﬁcient. The available experimental data are almost
always composed of measurements of physical quantities accessible at a necessarily
ﬁnite number of points in the domain of deﬁnition of their variables. They are thus
naturally discrete and we group them together in the vector y as we did in section 1.4
above. However, the unknown object is also discretized, either right from the start,
or during the process of solution (as in the example of the periodogram above), by

Inverse Problems, Ill-posed Problems
37
decomposition over a ﬁnite number of functions. If these are basic elements of the
space to which the object belongs, the decomposition is necessarily truncated. In
imaging, for example, in the vast majority of cases, pixel indicators or cardinal sines
are used as basic functions, according to whether the object is implicitly assumed
to have a limited support or a limited spectrum. Basic wavelets or wavelet packets
are also coming into use [STA 02, KAL 03]. The starting point is thus composed
of a model parametrized by the vector x of the decomposition coefﬁcients, in other
words by a set of exclusive hypotheses, each of which is indexed by the value of the
coefﬁcients. This hypothesis space is thus the set of possible values of these unknown
parameters, H = {xi}. The choice of these basic functions obviously forms part of
the inversion problem, even if it is not often touched upon.
In the discrete case (or more exactly the “discrete-discrete” case), the problem
changes noticeably as x and y belong to spaces of ﬁnite dimensions and the linear
operator A becomes a matrix A. Equation (1.10) has a unique solution with min-
imal norm xGI = A†y which depends continuously on y since the generalized in-
verse A† is then always bounded [NAS 76]. The problem is thus always well-posed
in Hadamard’s sense. However, even in this framework, the inversion problem still
has an unstable nature, this time from a numerical point of view. The spectral de-
composition (1.15) is still valid, the only difference being that the number of singular
values of matrix A is now ﬁnite. These singular values can rarely be calculated explic-
itly [KLE 80]. From this point of view, the example in section 1.2 is not representative
because, if we choose to decompose the solution over M > N complex exponentials
of the Fourier basis:
⌢x(ν) =
M

m=1
xm exp {−2jπmν} ,
model (1.2) can be written y = Ax + b in a matrix-vector notation, where A is a
rectangular N × M matrix composed of the diagonal matrix diag {hn}, juxtaposed
with the zero matrix of size N × (M −N). Its singular values are thus σn = hn
for n = 1, 2, . . . , N and σn = 0 otherwise. Even if we exclude all the zero singular
values, as by using A†, there are always some singular values close to zero with the
weighting h(t) of our example. Matrix A is thus ill-conditioned. The coefﬁcients
σ−1
n ⟨δy, un⟩in equation (1.15) become very large for the σn that are close to zero,
even if δy is small.
Generally speaking, whether we have the discrete case or not, let us assume that
Im A is closed so that the generalized inverse A† exists ∀y ∈Y (and is continuous).
Let us designate an error on the data y as δy and the error induced on the generalized
inverse solution x† as δx†. The linearity of (1.19) leads to δx† = A†δy, which implies
∥δx†∥X ≤∥A†∥∥δy∥Y ,

38
Bayesian Approach to Inverse Problems
where ∥A†∥designates the norm of the continuous operator A†, that is to say the
quantity: supy∈Y ∥A†y∥X/ ∥y∥Y [BRE 83]. In a similar way, (1.10) implies:
∥y∥Y ≤∥A∥∥x†∥X ,
where ∥A∥= supx∈X ∥A x∥Y / ∥x∥X. By combining these two relations, we obtain
the inequality:
∥δx†∥X
∥x†∥X
≤∥A∥∥A†∥∥δy∥Y
∥y∥Y
.
(1.21)
It is important to note that this inequality is precise in a certain sense. When A is a
matrix of dimensions (N × M) or corresponds to an inverse problem with discrete
data, the inequality can become an equality for certain (y, δy) pairs. When A is an
operator on spaces of inﬁnite dimension, it can only be established that the left hand
side of inequality (1.21) can be arbitrarily close to the right hand side. The quantity:
c = ∥A∥∥A†∥≥1
(1.22)
is called the condition number of the problem. When c is close to one, the problem
is said to be well-conditioned, whereas when it is considerably larger than one, the
problem is said to be ill-conditioned.
In practice, it is useful to have an estimate of the condition number, which gives
an idea of the numerical stability of the problem. When A = A is a matrix of di-
mensions (N × M), ∥A∥is the square root of the largest of the eigenvalues of the
positive semideﬁnite symmetric matrix A∗A, of dimensions (M × M) (the positive
eigenvalues of this matrix coincide with those of the matrix AA∗) and ∥A†∥is the
inverse of the square root of the smallest of these eigenvalues:
c =

λmax/λmin .
In our example in section 1.2, we obtain c = |h|max/|h|min and we understand why the
weighting by h(t) can degrade the conditioning of the generalized inversion problem
which is otherwise well-posed.
1.6. Conclusion
To sum up the above, when we have a simple situation where we are dealing with
a direct, linear problem in an inﬁnite dimension, bringing in an operator A : X →Y
deﬁned in the Hilbert spaces X and Y, we have three main situations:
– if A is continuous and injective (the only solution to the equation A x = 0 is
the trivial solution x = 0, thus Ker A = {0}) and its image is closed and given by
Im A = Y, the inverse problem is well-posed, since the inverse operator is continuous;

Inverse Problems, Ill-posed Problems
39
– if A is not injective, but Im A is closed, then, if we look for a pseudosolution, the
inverse problem becomes well-posed in as far as the generalized inverse is continuous;
– if the image Im A is not closed, using a pseudo-solution cannot, in itself, guar-
antee the existence and the continuity of the inverse solution.
When we are dealing with a linear operator deﬁned in spaces of ﬁnite dimension
 N and
 M and of the type A:
 M →
 N, we again have three main situations:
– if p is the rank of the matrix associated with the operator A and if p = N = M,
then A is bijective. A solution always exists, it is unique, and the inverse problem is
well deﬁned;
– if p < M, then the uniqueness is not certain but can be established by consider-
ing a generalized inversion;
– if p < N, then the existence is not certain for any given data but can be ensured
by again considering a generalized inversion.
To conclude this chapter, we see that an inverse problem is often ill-posed or ill-
conditioned, and that generalized inversion does not, in general, provide a satisfactory
solution. In the next chapter we will see that another development of modern analysis,
regularization, allows us to get around these difﬁculties and gives a generic framework
for inversion.
1.7. Bibliography
[AND 77] ANDREWS H. C., HUNT B. R., Digital Image Restoration, Prentice-Hall, Engle-
wood Cliffs, NJ, 1977.
[AND 80] ANDERSSEN R. S., DE HOOG F. R., LUKAS M. A., The Application and Numeri-
cal Solution of Integral Equations, Sijthoff and Noordhoff, Alplen aan den Rijn, 1980.
[BER 87] BERTERO M., POGGIO T., TORRE V., Ill-posed Problems in Early Vision, Memo
924, MIT, May 1987.
[BER 88] BERTERO M., DE MOL C., PIKE E. R., “Linear inverse problems with discrete
data: II. Stability and regularization”, Inverse Problems, vol. 4, p. 573-594, 1988.
[BRE 83] BREZIS H., Analyse fonctionnelle : théorie et applications, Masson, Paris, 1983.
[COU 62] COURANT R., HILBERT D., Methods of Mathematical Physics, Interscience, Lon-
don, 1962.
[FOS 61] FOSTER M., “An application of the Wiener-Kolmogorov smoothing theory to matrix
inversion”, J. Soc. Indust. Appl. Math., vol. 9, p. 387-392, 1961.
[FRA 70] FRANKLIN J. N., “Well-posed stochastic extensions of ill-posed linear problems”,
J. Math. Anal. Appl., vol. 31, p. 682-716, 1970.

40
Bayesian Approach to Inverse Problems
[GEM 84] GEMAN S., GEMAN D., “Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images”, IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-6,
num. 6, p. 721-741, Nov. 1984.
[HAD 01] HADAMARD J., “Sur les problèmes aux dérivées partielles et leur signiﬁcation
physique”, Princeton University Bull., vol. 13, 1901.
[HER 87] HERMAN G. T., TUY H. K., LANGENBERG K. J., SABATIER P. C., Basic Methods
of Tomography and Inverse Problems, Adam Hilger, Bristol, UK, 1987.
[KAK 88] KAK A. C., SLANEY M., Principles of Computerized Tomographic Imaging, IEEE
Press, New York, NY, 1988.
[KAL 03] KALIFA J., MALLAT S., ROUGÉ B., “Deconvolution by thresholding in mirror
wavelet bases”, IEEE Trans. Image Processing, vol. 12, num. 4, p. 446-457, Apr. 2003.
[KLE 80] KLEMA V. C., LAUB A. J., “The singular value decomposition: its computation
and some applications”, IEEE Trans. Automat. Contr., vol. AC-25, p. 164-176, 1980.
[MAR 87] MARROQUIN J. L., MITTER S. K., POGGIO T. A., “Probabilistic solution of ill-
posed problems in computational vision”, J. Amer. Stat. Assoc., vol. 82, p. 76-89, 1987.
[NAS 76] NASHED M. Z., Generalized Inverses and Applications,
Academic Press, New
York, 1976.
[NAS 81] NASHED M. Z., “Operator-theoretic and computational approaches to ill-posed
problems with applications to antenna theory”, IEEE Trans. Ant. Propag., vol. 29, p. 220-
231, 1981.
[PHI 62] PHILLIPS D. L., “A technique for the numerical solution of certain integral equation
of the ﬁrst kind”, J. Ass. Comput. Mach., vol. 9, p. 84-97, 1962.
[SAB 78] SABATIER P. C., “Introduction to applied inverse problems”, in SABATIER P. C.
(Ed.), Applied Inverse Problems, p. 2-26, Springer Verlag, Berlin, Germany, 1978.
[STA 02] STARK J.-L., PANTIN E., MURTAGH F., “Deconvolution in astronomy: a review”,
Publ. Astr. Soc. Pac., vol. 114, p. 1051-1069, 2002.
[TIK 63] TIKHONOV A., “Regularization of incorrectly posed problems”, Soviet. Math. Dokl.,
vol. 4, p. 1624-1627, 1963.
[TIK 77] TIKHONOV A., ARSENIN V., Solutions of Ill-Posed Problems, Winston, Washington,
DC, 1977.
[TWO 62] TWOMEY S., “On the numerical solution of Fredholm integral equations of the ﬁrst
kind by the inversion of the linear system produced by quadrature”, J. Assoc. Comp. Mach.,
vol. 10, p. 97-101, 1962.

Chapter 2
Main Approaches to the Regularization of
Ill-posed Problems
In the previous chapter, we saw that, when the image Im A of a linear operator we
want to invert is not closed, then the inverse A−1, or the generalized inverse A†, is not
deﬁned everywhere in the data space Y and is not continuous. This is the case, for
example, of compact, non-degenerate (or non-ﬁnite rank) operators and it is easy to
see that the condition number of the problem is inﬁnite. Suitable solving techniques
are thus required.
We also saw that, in a ﬁnite dimension, the inverse or the generalized inverse is
always continuous. In consequence, the use of a generalized inversion is sufﬁcient
to guarantee that the problem is well posed in this case. However, it must not be
forgotten that a problem that is well posed but severely ill-conditioned behaves in
practice like an ill-posed problem and has to be treated with the same regularization
methods, which we present below.
2.1. Regularization
In a ﬁnite or inﬁnite dimension, a regularizer of equation (1.10) y = Ax is a family
of operators {Rα; α ∈Λ} such that [NAS 81, TIK 63]:

∀α ∈Λ,
Rα is a continuous operator of Y in X;
∀y ∈Im A,
limα→0 Rα y = A†y.
(2.1)
Chapter written by Guy DEMOMENT and Jérôme IDIER.

42
Bayesian Approach to Inverse Problems
In other words, since the inverse operator A−1 does not have the required continuity
or stability properties, we construct a family of continuous operators, indexed by a
regulating parameter α (called the regularization coefﬁcient) and including A† as a
limit case. Applied to perfect data y, Rα gives an approximation of x† that is all the
better as α →0. However, when Rα is applied to data yε = Ax + b that inevitably
contain noise, b, we obtain an approximate solution xε = Rα yε and we have:
Rα yε = Rα y + Rα b .
(2.2)
The second term diverges when α →0. It follows that a trade-off has to be made
between two opposing terms, the approximation error (ﬁrst term) and the error due
to noise (second term). This can be done, within a given family of operators Rα, by
adjusting the value of the coefﬁcient of regularization α.
Most of the methods that have been put forward for solving and stabilizing ill-
posed problems in the past 30 years fall into this general scheme in one way or another.
They can be divided into two broad families: those that proceed by dimensionality
control – Λ is thus a discrete set – and those that work by minimization of a composite
criterion or by optimization under constraint – Λ is thus
 +. In what follows, we will
mainly concern ourselves with the second family of regularization methods.
2.1.1. Dimensionality control
In the case of an ill-posed or ill-conditioned problem, the methods of regularization
by dimensionality control get around the difﬁculty in two ways:
– by minimizing the criterion ∥y −Ax∥(or, more generally, G(y −Ax)) in a
subspace of reduced dimension, after an appropriate change of basis if necessary;
– by minimizing the criterion G(y −Ax) in the space initially chosen but by an
iterative method in which the number of iterations is limited.
2.1.1.1. Truncated singular value decomposition
A typical example of methods of the ﬁrst family can be found by examining equa-
tion (1.15): to suppress the ill-conditioned nature of the problem, we just truncate
the development, keeping the components corresponding to singular values that are
large enough for error terms of the form σ−1
n ⟨δy, vn⟩un to remain small. This is
truncated singular value decomposition, or TSVD [AND 77, NAS 81]. It is very ef-
fective for ensuring numerical stability. However, the problem arises of choosing the
truncation order, which plays the role, here, of the inverse of a regularization coefﬁ-
cient. However, the main failing of this approach is that we give up the possibility of
re-establishing the spectral components that have been too degraded by the imaging
device. As for the deﬁnition of the Rayleigh resolution criterion in optics, we use no
information about the object sought other than the fact that its energy is ﬁnite although,

Main Approaches to the Regularization of Ill-posed Problems
43
most of the time, we know that it is, for example, positive, or that it contains regions of
smooth spatial variation separated by sharp boundaries, or that it has bounded values,
or bounded support, etc. If we want to go beyond Rayleigh resolution, it is indispens-
able to be able to take this type of prior information into account.
2.1.1.2. Change of discretization
In truncated singular value decomposition, it is the imaging device that more or
less imposes the discretization through singular functions of the corresponding op-
erator. However, we can also avoid the difﬁculties raised by the poor condition-
ing of matrix A, as a consequence of the object’s being descretized on a Cartesian
grid for example, by choosing a parsimonious parameterization of the object bet-
ter suited to its prior properties. This is what wavelet-based decomposition meth-
ods [STA 02, KAL 03] do, for example. The principle remains the same: thresholding
is applied to the coefﬁcients of the decomposition so as to eliminate the subspace
dominated by the noise components.
This mode of discretization solves the problem of stability or poor conditioning an-
alyzed above but, even so, does not always provide a satisfactory solution. Everything
depends on the decomposition that is chosen.
2.1.1.3. Iterative methods
A very popular family of methods is made up of iterative methods of the form:
x(n+1) = x(n) + α (y −Ax(n)) ,
n = 0, 1, . . .
(2.3)
where 0 < α < 2/ ∥A∥(Bialy’s method [BIA 59]). If A is a non-negative, bounded,
linear operator (i.e., ⟨Ax, x⟩≥0, ∀x ∈X) and if y = Ax has at least one solution,
then the series of x(n) converges and:
lim
n→∞x(n) = P x(0) + x
GI,
where P is the orthogonal projection operator on Ker A and xGI the generalized inverse
solution. In fact, this method looks for the ﬁxed point of the operator G : Gx =
α y + (I −α A) x, but if A is compact and X is of inﬁnite dimension, then I −α A is
not a contraction and the method diverges. Moreover, we have also seen that, even in
ﬁnite dimensions, the generalized inverse solution is often dominated by the noise.
The non-negativity condition excludes a lot of operators but the method can be
applied to solve the normal equation A∗y = A∗Ax since A∗A is a non-negative
operator. We thus obtain Landweber’s method [LAN 51]:
x(n+1) = x(n) + α A∗(y −Ax(n)) ,
n = 0, 1, . . .
(2.4)
with 0 < α < 2/ ∥A∗A∥. The well known Gerchberg-Saxton-Papoulis-VanCittert
[BUR 31] method for extrapolating a limited-spectrum signal is a special case of the

44
Bayesian Approach to Inverse Problems
Bialy-Landweber method. It is in this same category of iterative methods that we can
place Lucy’s method [LUC 74], which is very popular in astronomy.
All these methods can provide an acceptable solution only on the condition that the
number of iterations is limited (which plays the role of the inverse of a regularization
coefﬁcient) [DIA 70]. This is often done empirically, as the initial framework does not
take observation noise into account and a theory regulating the number of iterations so
as to limit noise ampliﬁcation cannot but be heteronomous [LUC 94]. This explains
why the rest of this book will focus on regularization methods of the second family,
which operate by minimization under constraint and are, from this point of view, more
autonomous.
2.1.2. Minimization of a composite criterion
The principal characteristic of the regularization methods of this second large fam-
ily is to require the solution to be a trade-off between ﬁdelity to the measured data and
ﬁdelity to the prior information [TIT 85]. This trade-off is reached using a single
optimality criterion. The approach can be interpreted as follows.
The least squares solutions to equation (1.17) minimize the energy of the discrep-
ancy between the model Ax and the data y. In this sense, they achieve the greatest
ﬁdelity to the data. However, when the observation noise is broadband, relation (1.15)
shows that the high spatial frequency components of the restored or reconstructed ob-
ject have large amplitudes because of noise ampliﬁcation. The least squares solutions
thus prove unacceptable because we expect the real object to have markedly smoother
spatial variations. We therefore need to introduce a little inﬁdelity to the data to obtain
a solution that is smoother than the least squares solution and closer to the idea that
we have a priori. A widely accepted means of doing this is by the minimization of a
composite criterion [NAS 81, TIK 63, TIK 77]. The basic idea is to give up any hope
of reaching the exact solution from imperfect data, to consider as admissible any solu-
tion for which Ax is not far from y, and to look among the admissible solutions to ﬁnd
the one that can be considered as the physically most reasonable, i.e., compatible with
certain prior information. This is usually done by ﬁnding a solution xα that minimizes
a criterion of the form:
J (x) = G(y −Ax) + α F(x) ,
0 < α < +∞,
(2.5)
speciﬁcally designed so that:
– certain desirable properties that sum up our prior knowledge about the solution
are reinforced (second term).
The choice of the functionals F and G is qualitative and determines how the
regularization is carried out. Conversely, the choice of α, which is the coefﬁcient
– the solution is faithful to the data up to a certain point (ﬁrst term of the criterion);

Main Approaches to the Regularization of Ill-posed Problems
45
of regularization here, is quantitative and allows the compromise between the two
sources of information to be adjusted. Perfect ﬁdelity to the data is obtained with
α = 0, while perfect ﬁdelity to the prior information is obtained if α = ∞.
One of the most widely studied regularization methods is obtained by minimizing
the functional:
J (x) = ∥y −Ax∥2
Y + α ∥Cx∥2
X ,
(2.6)
where C is a constraint operator [NAS 76]. The existence of a solution is ensured
when C is bounded with Im C, for example, but that excludes the very interesting
case of a differential operator, as in Tikhonov’s seminal article [TIK 63]:
∥Cx∥2
X =
P

p=0

cp(r)
x(p)(r)
2 dr ,
where the weighting functions cp(r) are strictly positive and x(p) designates the pth
order derivative of x. The corresponding regularizer can be written:
Rα = (A∗A + α C∗C)−1A∗.
(2.7)
In this case, xα = Rα y exists and is unique when the domain of C is dense in X and
the equations Ax = 0 and Cx = 0 only have in common the trivial solution x = 0.
This solution takes a very simple form when A is compact and C = I, the identity
operator in X. By using the singular value decomposition of A of section 1.3, we
obtain:
xα =

n∈E
σn
σn + α
1
σn
⟨y, vn⟩un .
(2.8)
It is thus essentially a “ﬁltered” version of the non-regularized solution (1.14), or
generalized inverse, of equation (1.10). We will often ﬁnd this idea of linear ﬁltering
later, associated with the oldest regularization methods, but, for the moment, we will
concern ourselves mainly with discrete problems in ﬁnite dimensions.
In the discrete case, the literature on the subject is dominated by a few functionals.
2.1.2.1. Euclidian distances
The squared Euclidian distance between two objects x1 and x2 is deﬁned by:
∥x1 −x2∥2
P = (x1 −x2)T P(x1 −x2) ,
where P is a symmetric positive semi-deﬁnite matrix, chosen to express certain de-
sirable characteristics of the proximity measurement. Such a squared distance is the
Below are the ones most frequently found [TIT 85].

46
Bayesian Approach to Inverse Problems
habitual choice for G in the case where the noise b is assumed to be zero-mean, Gaus-
sian, independent of x, and of probability density:
p(b) ∝exp
	
−1
2 bT P b

,
(2.9)
i.e., of covariance matrix P−1. Such a distance is also very often used for F in order
to penalize objects x of large amplitude.
Applied to the interferometry example of Chapter 1 (which, we recall, is continu-
ous-discrete), this mode of regularization leads us to look for:
⌢x(α) = arg min
⌢x∈L2
 [0,1]

∥xN −y∥2 + α
 1
0
|
⌢x(ν)|2 dν

,
where xN = [x1, . . . , xN]T and xk =
 1
0
⌢x(ν) exp {2jπkν} dν. The solution reads:
⌢x(α)(ν) =
1
α + 1
⌢x
GI(ν) .
The spectrum thus regularized is therefore proportional to the periodogram (1.7) that
is obtained as a limit case (α →0). Hence, this type of regularization is not suitable
in this example.
However, we will see that the linear-quadratic frameworkabove (that combines the
linear nature of the direct model(1.3) and the quadratic nature of the functionals F
and G) turns out to be very handy in practice. The minimization of a criterion such as:
J (x) = ∥y −Ax∥2
P + α ∥x −x∥2
Q ,
(2.10)
where x is a default solution (it is the solution obtained when α →∞, i.e., when the
weight given to the data tends towards zero), provides an explicit expression of the
minimizer:
x = (AT PA + αQ)−1(AT Py −Q x)
(2.11)
which, thanks to the matrix inversion lemma [SCH 17, SCH 18], can be written:
x = x + Q−1 AT (AQ−1 AT + α−1 P−1)−1 (y −A x) .
(2.12)
In this expression, the matrix to be inverted has, in general, different dimensions from
that of (2.11).
2.1.2.2. Roughness measures
A very simple way of measuring the roughness of an image is to apply an ap-
propriate difference operator and then calculate the Euclidian norm of the result. As
the differentiation operation is linear with respect to the original image, the resulting
measure of roughness is quadratic:
F(x) = ∥∇k(x)∥2 = ∥Dkx∥2 .
(2.13)
The order k of the difference operator ∇k is habitually 1 or 2. Measure (2.13) is
minimum when x is constant (k = 1), afﬁne (k = 2), etc.

Main Approaches to the Regularization of Ill-posed Problems
47
2.1.2.3. Non-quadratic penalization
Another way of preserving the discontinuities in an object, better than the reg-
ularization methods using quadratic criteria, is to use non-quadratic penalty func-
tions [IDI 99]. This is precisely what was done to process the interferometry data of
Chapter 1, section 1.2, by ﬁnally choosing criterion (1.8). The principle is to use a
function that increases more slowly than a parabola so as to apply smaller penalties to
large variations. These functions are of two main types:
– L2L1 functions, i.e., continuously differentiable, convex functions that behave
quadratically at the origin and are asymptotically linear. A typical example is the
branch of a hyperbola;
– L2L0 functions, which differ from the previous ones by being asymptotically
constant and thus non-convex.
This time, it is no longer possible to obtain an explicit solution but the ﬁrst func-
tions have the advantage of being convex, so the standard minimization techniques
are sure to converge to the global minimum and give some robustness to the solu-
tion [BOU 93]. The others enable the discontinuities to be effectively detected but at
the expense of some instability and high computing costs [GEM 92].
2.1.2.4. Kullback pseudo-distance
In many image processing problems, it is essential to preserve the positivity of
the pixel intensities. One way of doing this is to consider that the positive object
can be identiﬁed, after normalization, with a probability distribution, and then to use
the distance measures between probability laws. In particular, the Kullback pseudo-
distance (or divergence, or information) of a probability π with respect to a probability
π0 (such that π is absolutely continuous with respect to π0) can be written:
K(π0, π) =
 
−log dπ
dπ0

dπ0.
For a reference object whose components mj are positive,
F(x) = K(x, m) =
M

j=1
xj log xj
mj
(2.14)
is often used. Here again, it is not possible to obtain an explicit expression for the
solution; it has to be calculated iteratively [LEB 99].
Criterion (2.5) sums up a view of regularization that can be called deterministic,
since the only probability law used is, at least implicitly though the choice of the func-
tional G, the one for noise. It has led to important theoretical developments, essentially
in mathematical physics. However, the questions of choice of the regularizing func-
tional F(x) [CUL 79] and the adjustment of the regularization coefﬁcient α [THO 91]

48
Bayesian Approach to Inverse Problems
are still very open. Section 2.3 presents the principal methods for adjusting the reg-
ularization coefﬁcient that exist in this framework. However, whether we want to set
this hyperparameter by such a supervised method or not, we need to be capable of
minimizing the regularized criterion (2.5) in practice afterwards, as a function of x.
This important aspect of inversion is dealt with below.
2.2. Criterion descent methods
Implicitly or explicitly, most inversion methods are based on the minimization of a
criterion. According to the properties of the latter, the computing cost of the solution
can vary enormously, typically by a factor of a thousand between the minimization of
a quadratic criterion by inversion of a linear system and that of a multimodal criterion
by a relaxation technique such as simulated annealing, everything else being equal.
Finally, the choice of the “right” inversion method depends on the computing facilities
available. And we still need to know which algorithm to use for a given optimization
problem. For instance, in the comparison above, it would be possible, but completely
inefﬁcient, to use simulated annealing to minimize a quadratic criterion. This section
gives a non-exhaustive overview of optimization problems in the context of inverse
problems in signal and image processing, with the associated algorithms. It obviously
cannot replace the literature devoted to optimization as a whole, such as [NOC 99]
or [BER 95].
2.2.1. Criterion minimization for inversion
By criterion minimization, we understand: ﬁnding the x that minimizes J (x)
among the elements of X. In the rest of this section, we consider the case of real
vectors1: X ⊂
 M. The criterion J and the set X may depend on the data, and struc-
tural properties (additional terms in the expression for J expressing “soft” constraints,
whereas the speciﬁcation of X is likely to impose “hard” constraints), hyperparame-
ters managing the compromise between ﬁdelity to data and regularity.
Thus, in the case of the generalized inverse of section 1.4, J (x) = ∥x∥and
X =

x, AT Ax = AT y

is the set of minimizers of ∥y −Ax∥. In the case of the
speciﬁcation of composite criteria dealt with in section 2.1.2,
J (x) = G(y −Ax) + α F(x),
(2.15a)
with
X =
 M (non-constrained case)
(2.15b)
or
X =
 M
+ (positivity constraint)
(2.15c)
1. The case where x is a function (more precisely, the case of a space X of inﬁnite dimension)
poses mathematical difﬁculties that come under functional analysis.

Main Approaches to the Regularization of Ill-posed Problems
49
Deﬁning x formally as the minimizer of a criterion hides three main levels of difﬁculty
in terms of implementation. In order of increasing complexity we have:
1 J is quadratic: J = xT M x−2 vTx+const. and X =
 M, or else X is afﬁne:
X =

x0 + Bu, u ∈
 P , P < M

;
2 J is a differentiable convex function and X =
 M or a convex (closed) subset
of
 M;
3 J has no known properties.
2.2.2. The quadratic case
In situation
1 , with X =
 M assuming M is symmetric and invertible, x is the
solution of the linear system M x = v of dimensions M×M, which expresses the fact
that the gradient becomes zero, ∇J (x) = 0. We have already encountered a similar
expression in (2.11) and we will meet it again in the Gaussian linear probabilistic
framework of Chapter 3.
In the variant constrained to a space X that is afﬁne, we need only to replace x
by its expression in u to get back to the unconstrained minimization of a quadratic
criterion, in
 P.
2.2.2.1. Non-iterative techniques
A ﬁnite number of operations is sufﬁcient to invert any linear system: of the order
of M 3 operations (and M 2 memory locations) for an M × M system. If the normal
matrix M = {mij} has a particular structure, the system inversion cost may decrease.
In signal processing, the stationary nature of a signal is expressed by the Toeplitz
character of the normal matrix (i.e., mij = μj−i). In image processing using a station-
ary hypothesis, the normal matrix is Toeplitz-block-Toeplitz (i.e., Toeplitz by blocks,
the blocks of sub-matrices themselves being Toeplitz). In both these cases, we ﬁnd
inversion algorithms costing of the order of M 2 operations and M memory locations
(Levinson algorithm) and even fast algorithms using a fast Fourier transform costing
only of the order of M log M operations. The spectral expression for the “Wiener
ﬁlter” of Chapter 4 is a special case where “fast” implementation is possible for the
case of a circulant normal matrix (i.e., mij = μj−i mod M).
The sparse nature of the normal matrix can also be used to good advantage: if
only ML coefﬁcients of M are non-zero, we can hope to decrease the inversion cost
in terms of the number of operations and variables to be stored. For example, if M is
a band matrix (mij = 0 if |j −i| ≥ℓ< M: a band matrix is sparse and L is of the
same order as ℓ), the inversion cost does not exceed Mℓ2 operations and Mℓmemory
locations. In particular, a normal matrix M = AT A is band if A corresponds to
ﬁltering by a small ﬁnite impulse response.

50
Bayesian Approach to Inverse Problems
2.2.2.2. Iterative techniques
If the number of unknowns M is very large (e.g., pixels in image restoration, or
voxels for 3D objects), the memory cost of non-iterative techniques often becomes
prohibitive. It is then preferable to use a ﬁxed point method, iteratively engendering a
series x(i) with a limit x = M−1v. All the conventional variants verify J (x(i+1)) ≤
J (x(i)). Three families can be distinguished.
2.2.2.2.1. “Column-action” algorithms
A single component differs between x(i) and x(i+1). The M components are
scanned cyclicly during the iterations. This is the principle of the Gauss-Seidel me-
thod, or coordinate descent [BER 95, p. 143], also called ICM (iterative conditional
modes) or ICD (iterative coordinate descent) in image restoration [BES 86, BOU 93].
It can be generalized for blocks of components and is all the more interesting and
partially parallelizable if A is sparse.
2.2.2.2.2. “Row-action” algorithms
A single component of y is taken into account to calculate x(i+1) from x(i).
The N data are scanned cyclicly during the iterations, which makes this approach
inevitable if the data are too numerous to be processed simultaneously. The alge-
braic reconstruction techniques (ART), long-standing references in medical imaging
by X-ray tomography, follow this principle to minimize the least squares criterion
∥y −Ax∥2 [GIL 72]. They can be generalized to the penalized criterion ∥y −Ax∥2
+α ∥x∥2 [HER 79], can process blocks of data and are all the more interesting and
partially parallelizable if A is sparse.
2.2.2.2.3. “Global” techniques
At each iteration, all the unknowns are updated according to all the data. The
gradient algorithms are prototypes of the global approach:
x(i+1) = x(i) −λ(x(i)) ∇J (x(i)),
with ∇J (x(i)) = 2 M x(i) −2 v. Note that the Landweber method deﬁned by (2.4)
is in fact a gradient technique minimizing the non-regularized criterion ∥y −Ax∥2.
The conjugate gradient or pseudo-conjugategradient algorithms are variants that con-
verge more rapidly, in which the successive descent directions combine the previously
calculated gradients to avoid the zigzag trajectory of the simple gradient [PRE 86,
p. 303]. These variants are of ﬁrst order, thus occupying little memory; they use only
the M ﬁrst derivatives ∂J /∂xm. The preconditioning technique can further increase
the efﬁciency of CG algorithms, as explained in Chapter 4, section 4.4.4, in the context
of deconvolution.
We will end with second order techniques. In situation
1 , with X =
 M, and
taking M to be symmetric and invertible, each iteration of the standard Newton’s

Main Approaches to the Regularization of Ill-posed Problems
51
method can be written:
x(i+1) = x(i) −

∇2J (x(i))
−1∇J (x(i)) = M−1v,
considering that ∇J (x) = 2Mx −2v and ∇2J (x) = 2M. In other words, a single
iteration of this algorithm is equivalent to solving the problem itself. Unless M has
a speciﬁc structure, the computation cost is prohibitive for most realistic inversion
problems. Some quasi-Newton variants become iterative again by approaching M−1
by a series of matrices P(i). The most popular among them is the BFGS (Broyden-
Fletcher-Goldfarb-Shanno) method [NOC 99, Chapter 8]. For large-sized problems,
the computation burden of such quasi-Newton methods is still too high. A better
choice is to resort to limited-memory BFGS, which can be seen as an extension of the
CG method, in-between ﬁrst and second order techniques [NOC 99, Chapter 9].
2.2.3. The convex case
The quadratic criteria are part of a larger family of functions J that are convex,
i.e., such that ∀x1, x2 ∈Ω, θ ∈(0, 1),
J (θ x1 + (1 −θ) x2) ≤θ J (x1) + (1 −θ) J (x2)
with X =
 M. Similarly, X is a convex set if ∀x1, x2 ∈X, θ ∈(0, 1), we have
θ x1 + (1 −θ) x2 ∈X.
The speciﬁcation that the criteria be convex but not necessarily quadratic gives a
wider choice as far as modeling is concerned. The Kullback pseudo-distance (2.14) is
convex over
 M
+ ; the Markov penalty functions F(x) = 
j ϕ(xj −xj+1) are convex
over
 M if ϕ is a convex scalar function such as ϕ(x) =
√
τ 2 + x2, which was used
in the spectrometry example of Chapter 1, section 1.2.
The minimization of non-quadratic convex criteria, although more difﬁcult and
more costly than the minimization of quadratic criteria, remains altogether compatible
with modern computing resources, which explains the increasingly frequent use of
convex penalty functions in signal and image restoration [IDI 99]. Let us start by
recalling a few fundamental properties of convex criteria [BER 95, App. B]:
– a convex continuous criterion J is unimodal: any local minimum is global and
the set of its minimizers is convex;
– if J1, J2 are convex and α1, α2 ≥0, then α1J1 + α2J2 is convex2;
2. This property “explains” why we are interested in convexity rather than unimodality: for
example, the penalized criterion (2.15a) is convex (so unimodal) if G and F are convex, whereas
the unimodality of G and F would not be enough to guarantee the unimodality of the criterion.

52
Bayesian Approach to Inverse Problems
– if J is strictly convex, there exists one and only one minimizer x in any convex
X that is closed (i.e., the boundary of X belongs to X).
On the other hand, if the criterion is non-quadratic, the minimizer x is a function
of the data that is generally neither linear, nor explicit. Owing to this, the non-iterative
inversion techniques for linear systems of section 2.2.2 are no longer valid. In con-
trast, the three families of iterative techniques based on the successive reduction of
the criterion give algorithms that converge towards x if J is convex and differentiable
and if X =
 M. The case of a criterion that is convex but not differentiable is slightly
trickier; modern techniques, known as interior point techniques, approach the solu-
tion by minimizing a succession of differentiable convex approximations [BER 95,
p. 312].
There are also other possible families of convergent techniques: reweighted least
squares, also called semi-quadratic algorithms (see Chapter 6), or the approaches
based on maximizing a dual criterion [BER 95, HEI 00, LUE 69].
If X is a closed convex subset of
 M, some adaptation is necessary: projected gra-
dient or conditional gradient versions in the family of “global techniques” [BER 95,
Chapter 2], and techniques of projection on convex sets [SEZ 82, YOU 82] in the
family of “row-action” techniques. As for “column-action” techniques, they remain
particularly simple if the constraints are separable, i.e., if X is a Cartesian product,
e.g. the positivity corresponds to X =
 + × · · · ×
 +. Finally, certain constrained
problems are equivalent to a non-constrained problem in the dual domain, which jus-
tiﬁes the use of dual methods.
2.2.4. General case
In the case of non-convex criteria, the possible existence of local minima makes
the use of descent techniques risky, in the sense that any local minimizer is a possible
ﬁxed point for most of these techniques. Whether we have convergence towards x
rather than towards a local solution then depends on the initialization. Several strate-
gies can be envisaged for avoiding these local solutions. Apart from exceptional cases,
they are notoriously more costly than the descent methods and yet still do not guar-
antee convergence towards the global minimizer. Without guaranteeing convergence
mathematically, some techniques are nevertheless sufﬁciently robust to avoid aberrant
solutions. They then give results that could not have been obtained by minimization
of a convex criterion, for applications such as automatic image segmentation or object
detection.
Two types of approach can be distinguished. On the one hand we have determinis-
tic methods that, in the absence of mathematical convergence properties, favor robust-
ness. For instance, the principle of gradual non-convexity (GNC) [BLA 87, NIK 98]

Main Approaches to the Regularization of Ill-posed Problems
53
consists of gradually minimizing a series of criteria using a conventional descent tech-
nique, starting with a convex criterion and ﬁnishing with the non-convex criterion.
The robustness of this technique comes from the quality of the initial solution. Its im-
plementation cost and complexity are relatively low. On the other hand, we have
the pseudo-random methods (simulated annealing [GEM 84] and adaptive random
search [PRO 84]), which make use of the generation of a large number of random
samples to avoid the traps. Simulated annealing has (probabilistic) convergence prop-
erties but the high computing cost of such techniques explains why their use is still
limited in the signal and image restoration ﬁeld.
2.3. Choice of regularization coefﬁcient
There are few methods for determining the hyperparameters in the framework of
this chapter [THO 91]. The most frequently used are the following.
2.3.1. Residual error energy control
One of the most intuitive and oldest ideas for setting the value of α that comes into
the regularized, or penalized, criterion (2.5) is to consider α as a Lagrange multiplier
in the equivalent problem:
x = arg min
x
F(x) s. t. G(y −Ax) = c .
(2.16)
The degree of regularization is ﬁxed by the value of c, which can be considered as a
statistic for which the probability distribution can be deduced from p(y | x). When
G = ∥·∥2 and x0 is the true solution, the vector of the residuals y −Ax0 follows the
law of the noise, which is implicitly taken to be homogeneous, zero-mean, white and
Gaussian. It results from this that c/σ2 is a variable of χ2 with N degrees of freedom
if σ2 is the variance of the noise. It is then recommended to set c to its expectation
value, i.e., Nσ2. However, such a choice often leads to overregularization of the
solution. One explanation is that the regularized solution x inevitably differs from
the true solution and that the residual errors y −Ax that are effectively calculable
to obtain the value of G do not follow any known distribution. Moreover, in many
problems, the graph of the function G(y −Ax) = G(α) is practically horizontal over
a large range of values of α: any error in the estimation of σ2 thus leads to large
variations in the value of α that satisﬁes constraint (2.16).
2.3.2. “L-curve” method
It is also possible to use an alternative method that has proved its worth in linear in-
verse problems of form (2.5) and in the case where the regularization functional F(x)

54
Bayesian Approach to Inverse Problems
is quadratic. This is the “L-curve” method [HAN 92]. It consists of using a log-log
scale to plot the regularization functional F(x(α)) against the least squares criterion
∥y −Ax(α)∥2 by varying the regularization coefﬁcient α. This curve generally has
a characteristic L shape (whence its name) and the value of α corresponding to the
corner of the L provides a good compromise between the contradictory requirements
of ﬁdelity to the data and ﬁdelity to the prior information.
To understand why this is so, we know that, if x0 is the exact solution, then the
error x(α)−x0 can be divided into two parts: a perturbation error due to the presence
of the measuring error b and a regularization error due to the use of a regularizing
operator instead of an inverse operator (see (2.2)). The vertical part of the L-curve,
described for low values of α, corresponds to solutions for which F(x(α)) is very
sensitive to variations in α, as the measurement error b dominates x(α) and does
not satisfy the discrete Picard condition [HAN 92]. The horizontal part of the curve,
described for high values of α, corresponds to solutions for which it is the sum of
the squares of the residuals ∥y −Ax(α)∥2 that is the most sensitive to variations of
α, since x(α) is dominated by the regularization error as long as y −b satisﬁes the
discrete Picard condition.
2.3.3. Cross-validation
In the case where the hyperparameters of problem (2.5) are limited simply to the
regularization coefﬁcient and where F and G are quadratic, cross-validation methods
also provide acceptable solutions [GOL 79, WAH 77].
The aim is to ﬁnd a value of the regularization coefﬁcient α such that the regular-
ized solution:
x(α, y) = arg min
x

G(y −Ax) + α F(x)

(2.17)
is as close as possible to the actual object x. Let Δx be a measure of the distance
between x(α, y) and x. With the choice of quadratic distances for F and G, it is
natural to also choose a quadratic distance for Δx:
Δx(α, x, y) = ∥x −x(α, y)∥2 .
(2.18)
Δx can be interpreted as a loss function measuring the risk involved in using x(α, y)
instead of x. A reasonable method for choosing α would be to choose the value that
minimizes this risk on the average, i.e., the mean square error (MSE):
MSE(α, x) =

Δx(α, x, y) p(y | x) dy
(2.19)
which is an expectation with respect to the noise probability distribution (2.9). Unfor-
tunately, the solution to this problem:
α
MSE(y, x) = arg min
α
MSE(α, x)
(2.20)

Main Approaches to the Regularization of Ill-posed Problems
55
depends on the real object which, obviously, is unknown. As the regularized solu-
tion x(α, y) can also be seen as a predictor of the observations through y(α, y) =
Ax(α, y), it is possible to measure the difference between the real and predicted ob-
servations with the following loss function:
Δy(α, x, y) = ∥Ax −Ax(α, y)∥2 .
(2.21)
The value of α could be obtained by minimizing the corresponding mean risk, which,
in this case, is the MSE on the prediction:
MSEP(α, x) =

Δy(α, x, y) p(y | x) dy
(2.22)
but, there again, the solution depends on the real object. The difﬁculty can, how-
ever, be overcome because the criterion MSEP(α, x) can be estimated by general-
ized cross-validation (GCV). Its basic principle is the following. Let x(α, y[−k]) be
the minimizer of the criterion:
J [−k](x) =

n̸=k
|yn −(Ax)n|2 + α ∥x∥2
Q ,
(2.23)
i.e., the object restored by using all the data except sample yk. It is possible to use
x(α, y[−k]) next to predict the missing data item:
y[−k]
k
(α) =

A x(α, y[−k])

k .
(2.24)
The method consists of looking for the value of α that minimizes a weighted energy
of the prediction error αGCV = arg minα V (α), with:
V (α) = 1
N
N

k=1
w2
k(α)

yk −y[−k]
k
(α)
2,
(2.25)
where the coefﬁcients w2
k(α) are introduced to avoid criterion (2.25) having undesir-
able properties, such as a lack of invariance during arbitrary rotations of the observa-
tion space, or absence of a minimum. They are given by:
wk(α) =
1 −Bkk(α)
1 −trace (B(α)) /M ,
where Bkk is the kth diagonal element of the matrix B(α) = A(AAT + αQ)−1AT.
The calculation of the minimum relies on the “linear-quadratic” nature of the problem,
which allows a simpler relation to be established:
V (α) = N ∥(I −B(α)) y∥2
(trace (I −B(α)))2 .
(2.26)

56
Bayesian Approach to Inverse Problems
This clearly shows that the GCV function V (α) is, in fact, the sum of the squares
of the residual errors weighted by a coefﬁcient that depends on α. This method has
interesting asymptotic statistical properties. For example [LI 86], x(αGCV, y) gives
almost surely the minimum of ∥Ax −Ax(α, y)∥2 when N →∞. Nevertheless, it
has to be understood that such a result is of interest only in the case of parsimonious
parameterization of the object sought, with a number M of parameters much smaller
than the number N of data points. These asymptotic properties and numerous practical
results explain why this method has so often been used in 1-D problems. Its use in
image processing is more recent [FOR 93, REE 90].
These methods for choosing the regularization coefﬁcient are only clearly justi-
ﬁed in the framework of quadratic regularized criteria. The stochastic extension of
Chapter 3 will allow us to go beyond this framework.
2.4. Bibliography
[AND 77] ANDREWS H. C., HUNT B. R., Digital Image Restoration, Prentice-Hall, Engle-
wood Cliffs, NJ, 1977.
[BER 95] BERTSEKAS D. P., Nonlinear Programming,
Athena Scientiﬁc, Belmont, MA,
1995.
[BES 86] BESAG J. E., “On the statistical analysis of dirty pictures (with discussion)”, J. R.
Statist. Soc. B, vol. 48, num. 3, p. 259-302, 1986.
[BIA 59] BIALY H., “Iterative Behandlung linearen Funktionalgleichungen”, Arch. Ration.
Mech. Anal., vol. 4, p. 166-176, 1959.
[BLA 87] BLAKE A., ZISSERMAN A., Visual Reconstruction, The MIT Press, Cambridge,
MA, 1987.
[BOU 93] BOUMAN C. A., SAUER K. D., “A generalized Gaussian image model for edge-
preserving MAP estimation”, IEEE Trans. Image Processing, vol. 2, num. 3, p. 296-310,
July 1993.
[BUR 31] BURGER H. S., VAN CITTERT P. H., “Wahre und Scheinbare Intensitätsventeilung
in Spektrallinier”, Z. Phys., vol. 79, p. 722, 1931.
[CUL 79] CULLUM J., “The effective choice of the smoothing norm in regularization”, Math.
Comp., vol. 33, p. 149-170, 1979.
[DIA 70] DIAZ J. B., METCALF F. T., “On iteration procedures for equation of the ﬁrst kind,
Ax = y, and Picard’s criterion for the existence of a solution”, Math. Comp., vol. 24,
p. 923-935, 1970.
[FOR 93] FORTIER N., DEMOMENT G., GOUSSARD Y., “GCV and ML methods of deter-
mining parameters in image restoration by regularization: fast computation in the spatial
domain and experimental comparison”, J. Visual Comm. Image Repres., vol. 4, num. 2,
p. 157-170, June 1993.

Main Approaches to the Regularization of Ill-posed Problems
57
[GEM 84] GEMAN S., GEMAN D., “Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images”, IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-6,
num. 6, p. 721-741, Nov. 1984.
[GEM 92] GEMAN D., REYNOLDS G., “Constrained restoration and the recovery of disconti-
nuities”, IEEE Trans. Pattern Anal. Mach. Intell., vol. 14, num. 3, p. 367-383, Mar. 1992.
[GIL 72] GILBERT P., “Iterative methods for the three-dimensional reconstruction of an object
from projections”, J. Theor. Biol., vol. 36, p. 105-117, 1972.
[GOL 79] GOLUB G. H., HEATH M., WAHBA G., “Generalized cross-validation as a method
for choosing a good ridge parameter”, Technometrics, vol. 21, num. 2, p. 215-223, May
1979.
[HAN 92] HANSEN P., “Analysis of discrete ill-posed problems by means of the L-curve”,
SIAM Rev., vol. 34, p. 561-580, 1992.
[HEI 00] HEINRICH C., DEMOMENT G., “Minimization of strictly convex functions: an im-
proved optimality test based on Fenchel duality”, Inverse Problems, vol. 16, p. 795-810,
2000.
[HER 79] HERMAN G. T., HURWITZ H., LENT A., LUNG H. P., “On the Bayesian approach
to image reconstruction”, Inform. Contr., vol. 42, p. 60-71, 1979.
[IDI 99] IDIER J., “Regularization tools and models for image and signal reconstruction”, in
3nd Intern. Conf. Inverse Problems in Engng., Port Ludlow, WA, p. 23-29, June 1999.
[KAL 03] KALIFA J., MALLAT S., ROUGÉ B., “Deconvolution by thresholding in mirror
wavelet bases”, IEEE Trans. Image Processing, vol. 12, num. 4, p. 446-457, Apr. 2003.
[LAN 51] LANDWEBER L., “An iteration formula for Fredholm integral equations of the ﬁrst
kind”, Amer. J. Math., vol. 73, p. 615-624, 1951.
[LEB 99] LE BESNERAIS G., BERCHER J.-F., DEMOMENT G., “A new look at entropy for
solving linear inverse problems”, IEEE Trans. Inf. Theory, vol. 45, num. 5, p. 1565-1578,
July 1999.
[LI 86] LI K. C., “Asymptotic optimality of CL and GCV in ridge regression with application
to spline smoothing”, Ann. Statist., vol. 14, p. 1101-1112, 1986.
[LUC 74] LUCY L. B., “An iterative technique for the rectiﬁcation of observed distributions”,
Astron. J., vol. 79, num. 6, p. 745-754, 1974.
[LUC 94] LUCY L. B., “Optimum strategies for inverse problems in statistical astronomy”,
Astron. Astrophys., vol. 289, num. 3, p. 983-994, 1994.
[LUE 69] LUENBERGER D. G., Optimization by Vector Space Methods, John Wiley, New
York, NY, 1st edition, 1969.
[NAS 76] NASHED M. Z., Generalized Inverses and Applications,
Academic Press, New
York, 1976.
[NAS 81] NASHED M. Z., “Operator-theoretic and computational approaches to ill-posed
problems with applications to antenna theory”, IEEE Trans. Ant. Propag., vol. 29, p. 220-
231, 1981.

58
Bayesian Approach to Inverse Problems
[NIK 98] NIKOLOVA M., IDIER J., MOHAMMAD-DJAFARI A., “Inversion of large-support
ill-posed linear operators using a piecewise Gaussian MRF”, IEEE Trans. Image Process-
ing, vol. 7, num. 4, p. 571-585, Apr. 1998.
[NOC 99] NOCEDAL J., WRIGHT S. J., Numerical Optimization, Springer Texts in Operations
Research, Springer-Verlag, New York, NY, 1999.
[PRE 86] PRESS W. H., FLANNERY B. P., TEUKOLSKY S. A., VETTERLING W. T., Numeri-
cal Recipes, the Art of Scientiﬁc Computing, Cambridge University Press, Cambridge, MA,
1986.
[PRO 84] PRONZATO L., WALTER E., VENOT A., LEBRUCHEC J.-F., “A general-purpose
global optimizer: implementation and applications”, Mathematics and Computers in Simu-
lation, vol. 26, p. 412-422, 1984.
[REE 90] REEVES S. J., MERSEREAU R. M., “Optimal estimation of the regularization pa-
rameter and stabilizing functional for regularized image restoration”, Opt. Engng., vol. 29,
p. 446-454, 1990.
[SCH 17] SCHUR I., “Uber Potenzreihen, die im Innern des Einheitskreises beschränkt sind”,
J. Reine Angew. Math., vol. 147, p. 205-232, 1917.
[SCH 18] SCHUR I., “Uber Potenzreihen, die im Innern des Einheitskreises beschränkt sind”,
J. Reine Angew. Math., vol. 148, p. 122-145, 1918.
[SEZ 82] SEZAN M. I., STARK H., “Image restoration by the method of convex projections:
Part 2 – Applications and numerical results”, IEEE Trans. Medical Imaging, vol. MI-1,
num. 2, p. 95-101, Oct. 1982.
[STA 02] STARK J.-L., PANTIN E., MURTAGH F., “Deconvolution in astronomy: a review”,
Publ. Astr. Soc. Pac., vol. 114, p. 1051-1069, 2002.
[THO 91] THOMPSON A., BROWN J. C., KAY J. W., TITTERINGTON D. M., “A study of
methods of choosing the smoothing parameter in image restoration by regularization”, IEEE
Trans. Pattern Anal. Mach. Intell., vol. PAMI-13, num. 4, p. 326-339, Apr. 1991.
[TIK 63] TIKHONOV A., “Regularization of incorrectly posed problems”, Soviet. Math. Dokl.,
vol. 4, p. 1624-1627, 1963.
[TIK 77] TIKHONOV A., ARSENIN V., Solutions of Ill-Posed Problems, Winston, Washington,
DC, 1977.
[TIT 85] TITTERINGTON D. M., “Common structure of smoothing techniques in statistics”,
Int. Statist. Rev., vol. 53, num. 2, p. 141-170, 1985.
[WAH 77] WAHBA G., “Practical approximate solutions to linear operator equations when the
data are noisy”, SIAM J. Num. Anal., vol. 14, p. 651-667, 1977.
[YOU 82] YOULA D. C., WEBB H., “Image restoration by the method of convex projection:
part 1 – Theory”, IEEE Trans. Medical Imaging, vol. MI-1, num. 2, p. 81-94, Oct. 1982.

Chapter 3
Inversion within the Probabilistic Framework
There are at least two reasons that encourage us to consider solving inverse prob-
lems in a Bayesian framework [DEM 89]. It was in this framework that local energy
functions and Markov modeling, which have had a lasting inﬂuence on low-level im-
age processing, were introduced. It is also this same framework that provides the most
consistent and complete answers to problems left in abeyance in other approaches,
such as the choice of hyperparameters or the optimization of a multimodal criterion.
3.1. Inversion and inference
To make the link between inversion and statistical inference more explicit, it is
useful at this stage to sum up the analysis carried out in Chapter 1. After discretization,
the direct problem takes the general form A(x, y) = 0, where A is an operator linking
the unknown object x ∈
 M to the experimental data y ∈
 N. Often, it even takes
the explicit form y = A(x) or the linear form y = Ax, A being a matrix. Inversion,
i.e., the calculation of x when A and y are known, is very often an ill-posed problem
in two senses.
Firstly, the operator A is often singular, in the sense that there is a class K of
solutions x ∈K such that Ax = 0 (the kernel Ker A = K is thus not empty). Any
element of K can be added to any solution to give another solution and we cannot,
therefore, invert the the direct relation to determine x uniquely from y. This lack of
uniqueness makes the discrete inverse problem ill-posed in Hadamard’s sense. This
situation occurs whenever the instrument response destroys part of the information
Chapter written by Guy DEMOMENT and Yves GOUSSARD.

60
Bayesian Approach to Inverse Problems
necessary for the reconstruction of the object. Let us not forget, however, that this
ambiguity can be removed by using a more or less empirical rule for choosing among
all the solutions, such as taking the minimum norm solution, for example.
Secondly, and more critically, no experimental device is completely free of uncer-
tainty, the simplest source being the ﬁnite accuracy of the measurements. It is thus
more realistic to consider that the object sought and the measurements taken are con-
nected by an equation of the form y = A(x) ⋄b, in which A is an operator describing
the essential part of the experiment and ⋄b accounts for the the deterioration of this
ideal representation by various sources of error (of discretization, measurement, etc.)
grouped together in the noise term. When the observation mechanism can be approx-
imated by a linear distortion and the addition of noise, this equation reduces to (1.3):
y = Ax + b. The presence of this noise has the effect of “spreading” the set K, since
any element x such that Ax = ε, where ε is “small” relative to the assumed level
of noise, can be added to any possible solution to obtain another acceptable solution.
However, above all, if the ambiguity is removed by taking a rule for choosing an ac-
ceptable solution, it is observed in practice that the latter behaves in an unstable way;
This
data, i.e., when the problem is well-posed in Hadamard’s sense. In fact, the instability
comes from the fact that A is ill-conditioned (see section 1.5).
So we see that, in ill-posed problems, obtaining a solution is not so much a prob-
lem of mathematical deduction as a problem of inference, i.e., of information process-
ing, which can be summed up in the following question: “how can we draw the best
possible conclusions from the incomplete information at our disposal?”
To be acceptable, any scientiﬁc inference method should: 1) take all the available
pertinent information into account; 2) carefully avoid assuming information is avail-
able when it is not. Probabilistic modeling is a handy, consistent way of describing
a situation of incomplete information. We will now see how it leads to a Bayesian
statistical approach.
3.2. Statistical inference
It should be made clear from the start that any problem dealt with through a
Bayesian approach has to be well-posed in the sense that enough information must
be provided to allow the probability distributions needed for the calculation to be at-
tributed without ambiguity. This means, at least, that an exhaustive set of possibilities
must be speciﬁed at the start of each problem. We will call this the data space (or
proof space) if it concerns possible results of the experiment, or the hypothesis space
if it speciﬁes the hypotheses that we wish to verify. It is also useful to distinguish
between two classes of problems, called estimation and choice of model. The ﬁrst
solution.
calculated
the
in
the data entail large variations
in
small changes
can easily happen even when the solution is unique and depends continuously on the

Inversion within the Probabilistic Framework
61
studies the consequences of choosing a particular model that is assumed “true”, while
the aim of the choice of model is to select one model by comparison with one or more
other possible candidates.
In an estimation problem, we assume that the model is true for one (unknown)
value x0 of its parameters and we explore the constraints imposed on the parameters
by the data. The hypothesis space is thus the set of all possible values of the parameters
H = {xi}. The data consist of one or more samples. For the problem to be well-posed,
the space of all the possible samples, S = {zi}, must also be stated. The spaces H
and S can both be discrete or continuous.
Before making the estimation, it is necessary to state a logical environment I which
deﬁnes our working framework (hypothesis space, data space, relationships between
parameters and data, any additional information). Typically, I is deﬁned as a logical
proposition stating:
– that the true value of the parameter is in H;
– that the observed data consist of N samples of the space SN;
– how the parameters are connected with the data (this is the role of the direct
model A);
– any additional information.
Of course, the physical nature of the parameters and data is implicitly speciﬁed in H,
S and A. Implicitly, all the developments that follow will be within the framework
deﬁned by I, which signiﬁes that any probability distribution will be conditioned by
I. This conditioning will not be indicated explicitly in order to lighten the notation.
We can now get started on the estimation problem by calculating the probability
that each of the possible values of the parameter is the actual value. Let D designate
the proposition afﬁrming the values of the experimental data actually observed and H
the proposition x0 = x afﬁrming that one of the possible values of the parameter x is
the actual value x0.
3.2.1. Noise law and direct distribution for data
In any statistical inference method intended to solve a problem such as (1.3), it is
necessary to start by choosing a probability law q(b) describing our information – or
our uncertainty – on the errors b. This is an essential step as it allows the direct, or
sampling, distribution to be found:
p(y | x) = q (y −A(x)) .
(3.1)

62
Bayesian Approach to Inverse Problems
In the vast majority of cases, a centered Gaussian distribution, independent of x, is
chosen for the errors, which gives:
p(y | x) = (2π)−N/2 |R|−1/2 exp
	
−1
2 ∥y −A(x)∥2
R−1

,
where R designates the covariance matrix of the distribution q(b). It is often diagonal,
or even proportional to the identity. A question arises immediately: What sense is to
be given to such a choice and in what situations is such a model appropriate?
should be that of the frequencies of its values in a very large number of repeated
measurements. It is then justiﬁed by reference to the central limit theorem which
says, under fairly broad conditions, that if the noise in a sample of data is the result of
a large number of accumulated elementary effects that are “random” and independent,
the Gaussian distribution is a good approximation of the real frequency distribution.
However, except for ﬂuctuations of electronic origin in a measurement system, the
noise is not generally the result of independent effects (think, for example, of the
discretization errors that depend on the solution x0). Moreover, to be able to make
an inference with this interpretation, it would be necessary for us to have numerous
results of other measurements so as to be able to determine these frequencies, which
is an extremely rare experimental situation.
This Gaussian “hypothesis” is thus not a hypothesis on the “random” nature of the
noise. We are not at all claiming that whatever gives rise to the noise is really random
and follows a Gaussian distribution. It is not even a hypothesis in the true sense of
the word; it is rather the least compromising – or the most conservative – choice that
we can make for the noise distribution in a situation of uncertainty. We are assuming
two things here: 1) that the noise can take any real value but that its average value
is zero; in other words, there is no systematic measurement error (or if there is, we
have been able to detect and correct it), and 2) that we expect there to be a “typical
scale” of noise; in other words, large contributions to the noise are not as probable as
small ones. To put it another way, we think that the distribution for the noise should
have a mean value of zero and a ﬁnite standard deviation, even if we have no precise
idea of the value of the latter. On the other hand, we have no idea as to the existence
or otherwise of cumulants of order greater than two. In these conditions, the least
compromising choice with respect to the characteristics that we do not know – which
can be justiﬁed by information principles [JAY 82] – is that of a Gaussian distribution.
In addition, if we suspect that the noise components affecting the N samples have
different scales and are correlated, the covariance matrix of the distribution is there to
express this hypothesis. It is not necessary to specify its value but if it is unknown,
its elements, grouped together in a vector of hyperparameters θ, will in general only
complicate the problem. They are called nuisance parameters for this reason.
With a frequentist’s interpretation of a probability, the distribution for the noise

Inversion within the Probabilistic Framework
63
This choice is appropriate whenever this information is all we know about the
noise. As this is a frequent situation, the choice is often made. If we have additional
information about the noise, which leads us to choose a non-Gaussian distribution,
we can include it in the same way but the result will be signiﬁcantly better only if
the distribution is very different from a Gaussian one. There are situations – such as
imaging with a low particle count – where the data are integers and have low values.
Choosing a binomial or Poisson distribution can then improve the results.
3.2.2. Maximum likelihood estimation
With simply this direct distribution p(y | x, θ), we could deﬁne the solution of the
inverse problem as being that of maximum likelihood (ML), the likelihood being the
direct distribution in which y takes its observed value and parameter x becomes the
variable:
x
ML = arg max
x∈H
p(y | x, θ).
In general, the justiﬁcation for this choice comes from the “good” statistical charac-
teristics (more often than not asymptotic) of this estimator. The least squares solution
is the special case of the maximum likelihood solution when the direct distribution is
Gaussian:
x
LS = arg min
x∈H
(y −A(x))T R−1 (y −A(x)) .
Introduced in this way, it is still a weighted least squares method (weighted by the
matrix R−1) that possesses the indispensable property of invariance under changes of
units in H and S. In many simple situations, this inference method provides all the
information we are looking for. However, in inverse problems where the parameteri-
zation is not parsimonious, the direct distribution does not contain all the information
needed to make the problem well-posed and it does not provide all the technical appa-
ratus necessary for the calculation:
1) In the special case of an indeterminate linear problem y = Ax, where A is
singular (a problem known as generalized inversion), there is no “noise” and so no
direct distribution, except in the rudimentary sense where p(y | x) is constant if x is
in the class C of possible antecedents of y, and zero otherwise. As the likelihood is
constant in class C, maximizing it is of no help for the choice within this class. The
essence of the problem does not lie in the presence of “random” noise perturbing the
data, but rather in the fact that our information is incomplete, although essentially
noise free.
2) In the linear case (1.3), matrix A of the direct problem is often ill-conditioned.
The solving operator A† = (AT R−1A)−1AT R−1 is unstable and the solution xML =
A† y is unacceptable: the ampliﬁcation of the noise is excessive.
3) The problem can have nuisance parameters that are of no interest to us, and they
may be numerous. When matrix R is full, N (N −1)/2 hyperparameters are added

64
Bayesian Approach to Inverse Problems
which, when they are unknown, generally have to be estimated by ML as parameters
of interest x, and the global maximum may then no longer be a point but a whole
region.
4) We may have highly pertinent information on the solution we are looking for.
For example, we may know that it has to be positive, or satisfy certain constraints (as in
astronomical imaging where the integral of the object may already be known), or that it
is made up of homogeneous regions separated by clear boundaries. Such information
is not contained in the direct distribution but it would be most unreasonable to ignore
it.
5) In many problems, it is necessary to obtain not only a solution but also an in-
dication of the conﬁdence we can have in it. If we simply have the direct distribution
(3.1), the conﬁdence intervals given by the frequency approach only give us informa-
tion on the long term behavior of the solution, i.e., its average behavior over a very
large number of repeats of the experiment. However, we only possess the results of a
single experiment, which often cannot be reproduced.
6) Finally, the estimation of the parameters of a model that is assumed to be valid
is often just one step and we may need to judge the relative merits of various models.
It is therefore necessary to go beyond inference by ML. All the extensions men-
tioned above are “automatically” provided by the Bayesian approach.
3.3. Bayesian approach to inversion
Bayesian inference is so named because it makes great use of Bayes’ rule, which
itself is a consequence of a fundamental rule in probability calculation, the product
rule [COX 61]. Let H be a hypothesis whose truth we want to evaluate and D a set of
data connected with this hypothesis. The product rule stipulates that:
Pr(H, D) = Pr(H | D) Pr(D) = Pr(D | H) Pr(H)
where, for example, Pr(H | D) usually designates the probability that H is true know-
ing D. From this we draw Bayes’ rule:
Pr(H | D) = Pr(H) Pr(D | H)/ Pr(D)
which is none other than a learning rule. It tells us how we should adjust the proba-
bility attributed to the truth of a hypothesis when our state of knowledge changes with
the acquisition of data. The probability a posteriori for H, Pr(H | D), is obtained
by multiplying its probability a priori, Pr(H), by the probability of having observed
the data D assuming the hypothesis is true, Pr(D | H), and dividing the whole by the
probability of having observed the data independently of whether the hypothesis is
true or not, Pr(D). This last term, sometimes called the global likelihood, plays the
role of a normalization constant.

Inversion within the Probabilistic Framework
65
A large part of statistical inference is based on the use of prior information on the
quantities to be estimated, which adds to the information given by the data. Thus, it is
not surprising, if we think about the deep nature of the regularization principle set out
in Chapter 2, that it shows a close link with Bayesian inference.
In the case of an inverse problem such as (1.3) and assuming that the probability
distributions concerned admit a density, the prior information on object x is expressed,
in a Bayesian context, in the form of an a priori probability density function (pdf)
p(x | θ). Bayes’ rule allows us to combine this with the information contained in the
data to obtain the a posteriori law:
p(x | y, A, θ) = p(x | θ) p(y | x, A, θ)
p(y | A, θ)
= p(x, y | A, θ)
p(y | A, θ)
.
(3.2)
In this equation, θ is a vector of hyperparameters composed of the parameters of the a
priori distributions of the errors and the object, and p(y | x, A, θ) designates the data
law conditioned by the true solution x. It is completely determined by the knowledge
of the direct model (1.3) and the noise probability law. The last term ensures the
normalization of the a posteriori law:
p(y | A, θ) =

p(y | x, A, θ) p(x | θ) dx .
(3.3)
In the Bayesian approach, the knowledge (or uncertainty) about object x after obser-
vation of data y only is wholly described by the probability distribution (3.2). This
probability is equal, with just a multiplying factor, to the product of the likelihood
introduced in section 3.2 by the a priori probability p(x | θ). If we assume that, in
the case of section 3.2, the knowledge of x (which then comes purely from obser-
vations and from the structure of the problem) is represented by the likelihood, we
observe that, in the Bayesian approach, taking prior information into consideration
by means of p(x | θ) modiﬁes our knowledge and, in general, has the effect of re-
ducing the uncertainty on the parameter x. But above all, because of the framework
adopted, the Bayesian approach enables a wider range of answers to the question
“given a probability distribution for a continuous or discrete parameter x, what best
estimate can be made and with what accuracy?”. There is not a single answer to this
question; the problem concerns the theory of the decision that answers the question
“what should we do?”. This implies value judgements and consequently goes beyond
the principles of inference, which only answers the question “what do we know?”.
Thus, we can equally well deduce a point estimator or a region of uncertainty from
(3.3) [MAR 87, TAR 87]. The maximum a posteriori is a frequent choice for the esti-
mator. It consists of giving x the value that maximizes the distribution a posteriori:
x
MAP = arg max p(x | y, A, θ) .
(3.4)
However, this is only one of the possible solutions. This MAP estimation corresponds
to the minimization of a mean decision cost with an all-or-nothing cost function, the

66
Bayesian Approach to Inverse Problems
limit (when ε →0) of the mean cost Pr(∥x −x0 ∥> ε). Other cost functions have
been proposed in the framework of image modeling by Markov ﬁelds. They lead to
the maximization of the marginal probabilities [BES 86, MAR 87].
3.4. Links with deterministic methods
In the case that interests us here, i.e., an inverse problem in a ﬁnite dimension, it is
clear that regularizing according to the general principle indicated in Chapter 2, and
thus minimizing a criterion such as (2.5), is equivalent to choosing the solution that
maximizes the following a posteriori law:
p(x | y, A, θ) ∝exp
	
−1
2 σ2

G (y −A(x)) + α F(x)

.
(3.5)
where σ2 is the variance of the noise. The above probability law is only one of the
possible choices since any strictly monotonic function other than an exponential would
do. However, this choice is particularly suitable here because, with the linear model
(1.3), taking the usual hypotheses that the noise is Gaussian and independent, as G is
a Euclidian norm, the conditional law p(y | x, A, θ) is really:
p(y | x, A, θ) ∝exp
	 1
2σ2 G (y −A(x))

.
(3.6)
For the analogy to be complete, the a priori law must take the following form:
p(x | θ) ∝exp
	
−α
2σ2 F(x)

,
(3.7)
and, for it to be rigorous, the a posteriori law (3.5) must be proper, a sufﬁcient condi-
tion being that (3.6) and (3.7) are also proper:

N exp
	
−1
2σ2 G (y −A(x))

dy < +∞,

M exp
	
−α
2σ2 F(x)

dx < +∞.
Many local energy functions used in image processing were introduced in a Bayes-
ian framework. They deﬁne x as a Markov ﬁeld (see Chapter 7). Although the energy
point of view is also held by some members of the image processing community,
criteria of form (2.5) can generally be reinterpreted in a Bayesian framework, even if
it means making minor changes in F to ensure the normalization of equation (3.7).
In consequence, the maximum a posteriori estimator, which is the Bayesian esti-
mator the most used in inversion, becomes the same as the minimizer of the penalized
criterion (2.5):
x
MAP = arg max
x
p(x | y, A, θ) = arg max
x
p(x, y | A, θ)
= arg min
x
G(y −A(x)) + α F(x)

Inversion within the Probabilistic Framework
67
under the technical conditions that allow this development (principally, that the prob-
lem brings in a ﬁnite number of variables). It is thus obvious that the Bayesian frame-
work gives a statistical sense to the minimization of penalized criteria. The question is
not, however, whether the Bayesian approach is a justiﬁcation of the other approaches.
We could also, and conversely, say that the same result gives a deterministic interpre-
tation of the probabilistic estimator of the maximum a posteriori and that an estimator,
once deﬁned, depends no more on the formal framework that engendered it than on
the digital means used to calculate it. The question is rather one of seeing that the
Bayesian approach provides an answer to the problems raised in section 3.2. In addi-
tion to its great consistency, it makes original tools available:
– marginalization (everything that does not interest us is simply integrated out of
the problem);
– regression (the conditional expectation does not have an equivalent in the energy
framework);
– stochastic sampling (Monte Carlo methods, simulated annealing algorithms, ge-
netic algorithms), not conceivable without the Bayesian approach (on this point, see
Chapter 7, section 7.4.2).
3.5. Choice of hyperparameters
The Bayesian framework appreciably extends the range of methods available for
determining the hyperparameters. To be applied effectively, all the methods described
in Chapter 2 require us to choose the value of the regularization coefﬁcient α and, more
generally, all hyperparameters θ deﬁning the F and G distance measures: the variance
of the noise, the object correlation parameters and the parameters of the local energy
functions. The determination of θ is the most delicate step in image restoration and
reconstruction methods. Although the problem is still open, the Bayesian approach
provides consistent tools for tackling it.
Hyperparameters θ constitute a second level in the description of the problem,
which is indispensable to “rigidify” the ﬁrst level composed of the parameters them-
selves – i.e., the object x. In an ill-posed problem, the value of the parameters is im-
portant for obtaining an acceptable solution but has no intrinsic interest. In a Bayesian
approach, two levels of inference can be distinguished. The ﬁrst is inference on x, for
a given value of θ, through the a posteriori distribution of equation (3.2). The second
is inference on θ through the analog relationship:
p(θ | y, A) = p(θ | A) p(y | θ, A)/p(y | A) .
(3.8)
Here again, we ﬁnd a characteristic of the use of Bayes’ rule: the marginal likelihood
p(y | θ, A) attached to the data in the second level is the coefﬁcient of normalization
in the ﬁrst.

68
Bayesian Approach to Inverse Problems
If, as is often the case, this term is sufﬁciently peaked, i.e., if the data y contain
enough information, the inﬂuence of the a priori distribution p(θ | A) is negligible and
the second level of inference can be solved by maximizing the likelihood. But to do
this, we have to solve the marginalization problem corresponding to the calculation
of the integral in (3.3). Such integrals rarely lead to an explicit result. One notable
exception is the joint Gaussian distribution p(x, y | θ, A), as we will see in section 3.8.
To get around the problem posed by the explicit calculation of a marginal likeli-
hood, we can introduce “hidden variables” q which complete the observations y in
such a way that the new likelihood p(y, q | θ, A) is simpler to calculate. We are then
led to maximize the conditional expectations by iterative, deterministic or stochastic
techniques (EM and SEM algorithms) [DEM 77], the algorithm converging towards
the solution of ML. The need for such stochastic approaches appeared when it was
found to be impossible to implement convergent likelihood maximization methods by
conventional optimization techniques, as the likelihood was not calculable.
Furthermore, the joint distribution or generalized likelihood:
p(y, x | θ, A) = p(x | y, θ, A) p(y | θ, A) = p(y | x, θ, A) p(x | θ)
(3.9)
sums up all the information at the ﬁrst level of inference. Its maximization with re-
spect to x and θ can be envisaged. Thus, the integration problem raised by (3.3) is
obviously removed. At ﬁxed θ, the generalized maximum likelihood (GML) coin-
cides with the MAP; at ﬁxed x, it corresponds to the usual ML for θ, x and y being
known. Nevertheless, repeated alternation of these two steps is hazardous: the char-
acteristics of the corresponding estimator are not those of the usual ML [LIT 83]. It
can even happen sometimes that the GML is not deﬁned because the likelihood may
have no maximum, even local [GAS 92]. This technique thus has a marked empirical
character.
Thus, the Bayesian approach leads fairly naturally to the use of estimators based
on likelihood for the estimation of the hyperparameters. Despite deﬁnite difﬁcul-
ties of implementation, interesting results have been obtained in a one-dimensional
framework. In a two- or three-dimensional framework, we have to be more cautious.
Although it is possible to estimate the hyperparameters in several cases, the values
obtained using this approach do not necessarily lead to good results for the estimation
of the parameter of interest x, particularly when the latter comes from “natural” data.
The cause could lie in there being too great a difference between these natural data
and the behavior of the a priori model. The question of hyperparameter estimation
thus remains wide open.
3.6. A priori model
A reproach that is often levelled against Bayesian estimation is that it depends on
the knowledge of a hypothetical, uncertain “true model” that engendered the object

Inversion within the Probabilistic Framework
69
to be reconstructed. To formulate this reproach, we have to implicitly accept that
reality can be “enclosed” in a mathematical model. This opens up a huge philosoph-
ical debate... In the case of the probabilistic approach to inversion as we see it, the
It
does, however, seem important to recall that our probabilistic hypotheses are not hy-
potheses on the “random” character of the object but choices of a way of representing
incomplete prior information – or uncertain knowledge – compatible with the chosen
inference tool. This situation is far from unusual, as it is rare for the prior informa-
tion available in a real problem to come in a form directly suited to the theoretical
framework chosen for its processing.
Let us remember that the advantages of the Bayesian approach stem not so much
from the additional information introduced by the prior – the energy and deterministic
interpretations of the functional of regularization F(x) of section 3.4 show that this
information is not proper to the Bayesian approach, and the information on nuisance
parameters is diffuse most of the time – as from the access it provides to a layer of
tools that does not exist in the other approaches, such as marginalization, regression
and pseudo-random algorithms.
Having said this, the conversion of prior information into probabilities is a tricky
problem that is still far from being solved. To describe object x, the prior is often
chosen pragmatically, as we will see later. There are, however, some formal rules that
lead to reasonable choices [BER 94, KAS 94, ROB 97] and are used in particular for
the hyperparameters. They often lead to an improper law, which does not cause any
special difﬁculty if it is handled correctly [JEF 39]. Here are a few examples.
Some methods rely on transformation group theory to determine the “natural” ref-
erence measure for the problem and to satisfy certain invariance principles. In practice
though, this approach has done little more than justify the use of Lebesgue’s method
for the localization parameters (thus providing an extension to the continuous case
of the uniform distribution resulting from the application of Bernouilli’s “indifference
principle” in the discrete case) and the Jeffreys measure in the case of scale parame-
ters [JEF 39, POL 92].
Other methods are based on information principles. These are mainly maximum
entropy methods (MEM), in which we look for the distribution that is closest to the
reference distribution (in the Kullback divergence sense) whilst verifying incomplete
prior information [JAY 82]. There again, this approach has mainly just helped to
justify certain choices after the event. In addition, it is only really workable when the
prior information is made up of linear constraints on the distribution we are looking
for (moments). We are thus working in the family of exponential distributions.
Another formal principle consists of using a conjugate prior, i.e., a prior belonging
to the same family as the direct distribution of the problem, to obtain an a posteriori
frequentist’s interpretation of the probabilities maintains an annoying confusion.

70
Bayesian Approach to Inverse Problems
distribution in the family [ROB 97]. This is only of interest if the family in question
is as small as possible and parametrized. In this case, the step from a priori to a pos-
teriori, by application of Bayes’ rule, comes down to updating the parameters. The
interest of this method is essentially technical, as the a posteriori is always calculable,
at least up to a certain point. A partial justiﬁcation can also be found by invariance
reasoning: if the data y change p(x) into p(x | y), the information that y contributes
about x is clearly limited; it should not lead to a change of the whole structure of p(x),
but only of its parameters. It is obvious though that the main motivation for using the
method is its convenience. However, only certain families of direct distributions, such
as exponential families [BRO 86], guarantee the existence of conjugate priors and it
is often necessary to limit use of the method to this class of distributions. In addi-
tion, the “automatic” nature of this way of making choices is rather deceptive because
additional hyperparameters – the values of which have to be speciﬁed – inevitably
appear.
A last, very important class is composed of “tailor made” constructions, in other
words, constructions that are not based on general principles like the previous ones
but make pragmatic use of probabilistic methods that express the properties expected
of the solutions as well as possible. It is into this category that we must put the Gibbs-
Markov ﬁelds, which have undergone spectacular development in imaging since 1984
[GEM 84] and which allow essential local properties that an object must possess to be
incorporated into an a priori distribution. The construction of these models requires
considerable know-how but is a very powerful way of incorporating elaborate prior
information. The price to be paid for this is high complexity, both in the handling of
the models and in the implementation of the resulting estimators. Chapter 7 is entirely
devoted to Gibbs-Markov models.
3.7. Choice of criteria
The Bayesian approach brings inversion down to the determination of an a pos-
teriori law. Since we cannot envisage calculating such laws completely, we content
ourselves with looking for a point estimator, which is often the maximum a posteri-
etc.) but it is important to assess the consequences of such a choice carefully and, if
necessary, think about alternatives.
It is reasonable to raise the question of the necessity for the solution to be contin-
uous with respect to the data and, consequently, the need for convexity of the regu-
larization criteria. While quadratic and entropy approaches are well known for mak-
ing inverse problems well-posed, the minimization of a non-convex functional cannot
guarantee that the solution will be continuous: a small variation in the data can induce
a “jump” from one valley to another and thus a loss of continuity. However, in many
ori one. There are alternatives (marginal maximum a posteriori, mean a posteriori,

Inversion within the Probabilistic Framework
71
problems, these transitions are not only desirable but necessary to restore discontinu-
ities, edges, interfaces, bright spots, etc. without limits in terms of spatial resolution.
We can shed a different light on this problem by noting that certain non-convex cri-
teria introduced in imaging have an equivalent expression implying hidden variables.
In this case, the problem leaves convex analysis and incorporates a measure of com-
binatory analysis or hypothesis testing, which comes more under decision theory than
estimation. Bayesian analysis remains pertinent in this combined detection-estimation
context. Much recent work has followed this direction, combining several levels of
variables, mixing low- and high-level descriptions, or data acquired by different ex-
perimental means. It is in this sense that the conventional concepts of regularization,
such as continuity with respect to the data, are not completely appropriate and an effort
should be made to extend them.
3.8. The linear, Gaussian case
The Gaussian laws associated with linear direct models provide a linear estima-
tion structure and thus a very convenient algorithmic framework. However, they only
allow us to incorporate crude information, basically limited to second order charac-
teristics. Thus, in standard regularization theory [TIT 85], the choice of a quadratic
term for ﬁdelity to the data: G(y −Ax) = ∥y −Ax∥2
P is equivalent to choos-
ing a Gaussian distribution for the noise: q(b | Rb) ∼N(0, Rb), with Rb ∝P−1.
Similarly, choosing quadratic penalization: F(x) = ∥Dkx∥2 is also equivalent to
choosing a Gaussian prior distribution for the object: p(x | Rx) ∼N(0, Rx), with
Rx ∝(DT
k Dk)−1, assuming, of course, that the matrix DT
k Dk is deﬁned as positive.
Deterministic “linear-quadratic” regularization is thus rigorously equivalent to Gaus-
sian linear estimation and the solution, which is explicit, is given by equations (2.11)
and (2.12):
x = (AT R−1
b A + R−1
x )−1AT R−1
b y ,
(3.10)
= Rx AT (A Rx AT + Rb)−1 y ,
(3.11)
and has the remarkable characteristic of being a linear function of data y. This “linear-
quadratic” or linear Gaussian inversion holds a dominant position in inversion prob-
lems and it is a common reaction to say “inverse problems aren’t complicated; you
just need to smooth the data before doing the inversion”. This way of seeing things
is not wrong and is, in fact, sufﬁcient for many problems but it is limiting; it stops us
from going further and induces a cascading scheme – linear ﬁltering of a generalized
inverse solution – that is only justiﬁed in the “linear-quadratic” framework.
3.8.1. Statistical properties of the solution
Solution (3.10) is, in the Gaussian case, the mode, the mean and the median of
the a posteriori probability distribution (3.5) all at once. It minimizes several very

72
Bayesian Approach to Inverse Problems
commonly used cost criteria, in particular the mean quadratic error. Obviously, in
this case, we are talking about a mean with respect to the a posteriori distribution,
but many physicists and engineers only know the mean square error (MSE) deﬁned
as a mean with respect to the direct distribution (3.6). It is therefore useful to study
the MSE, which is the sum of the bias energy and the trace of the covariance matrix:
MSE(x) = ∥E(x) −x0∥2 + trace Cov(x) , designating the “true” solution by x0.
For the sake of simplicity, we will assume that the noise is stationary and white: Rb =
σ2
b I and that we can write Rx = σ2
x (DT D)−1. We thus have α = σ2
b/σ2
x.
The expectation of regularized solution (2.11), for direct distribution (3.6), can be
written:
E(x) = E

(AT A + α DT D)−1 AT (Ax0 + b)

= (AT A + α DT D)−1 AT Ax0 .
Thus, for the bias to be zero (E(x) −x0 = 0), we would need α = 0, i.e., we must
not regularize! The bias energy is:
∥E(x) −x0∥2 =

(AT A + α DT D)−1 AT A −I

x0
2 ,
an increasing function of α, that equals zero and has a zero derivative at α = 0 and
that tends towards ∥x0∥2 when α →∞.
The covariance matrix of the solution can be written:
Cov(x) = E

(x −E(x)) (x −E(x))T 
= σ2
b (AT A + α DT D)−1 AT A (AT A + α DT D)−1 .
To calculate its trace, we assume that matrices A and D have the same singular vec-
tors1, so that we have the factorizations:
AT A = U Λ2
a UT
and
DT D = U Λ2
d UT ,
where Λa and Λd are diagonal matrices composed respectively of the singular values
λa(k) of A and λd(k) of D, k = 1, 2, . . . , M. We thus obtain:
trace Cov(x) = σ2
b
M

k=1
λ2
a(k)
(λ2a(k) + α λ2
d(k))2 ,
a strictly decreasing function of α, tending towards zero when α →∞.
1. This is the case, for example, if D is the identity matrix, or if A and D are two circulant
matrices, such as those we will encounter in Chapter 4.

Inversion within the Probabilistic Framework
73
0 through
an approach, frequent in statistics, consisting of looking for estimators without bias
and, if a degree of freedom remains, with minimum variance2, leads to the generalized
inverse solution, the MSE of which is:
MSE(x
GI) = trace Cov(x
GI) = σ2
b
M

k=1
1
λ2a(k) .
This can be considerable when some singular values λa(k) are small, which is pre-
cisely the case in discretized and ill-conditioned problems. It can thus be said that,
in terms of MSE, regularization consists of voluntarily introducing a bias in order to
considerably reduce the variance of the solution.
3.8.2. Calculation of marginal likelihood
The linear, Gaussian case is one of the few that allow an explicit calculation of the
marginal likelihood of equation (3.3), used to adjust the values of hyperparameters θ.
When these are limited to the variances σ2
b and σ2
x for example (or to the pair σ2
b and
α = σ2
b/σ2
x), we have:
p(x, y | σ2
x, σ2
b) =

2π σ2
b
−N/2 
2π σ2
x
−M/2 DT D
1/2 e−Q/2σ2
b ,
where
Q = (y −Ax)T (y −Ax) + α xT DT D x .
To calculate the ordinary, or marginal, likelihood of α and σ2
b, we have to “integrate
x out of the problem”. To prepare this integration, a perfect square is conventionally
made to appear in Q:
Q = (x −x)T (AT A + α DT D) (x −x) + S(α) ,
with S(α) = yT (y −Ax), which leads to a Gaussian integral:
py | α, σ2
b) =

px, y | σ2
x, σ2
b) dx
=

2πσ2
b
−N/2 αM/2 DT D
1/2 AT A + αDT D
−1/2 e−S(α)/2σ2
b .
2. This strategy has no serious basis. Good asymptotic properties (when N →∞) are often
mentioned for these estimators without bias and with minimum variance, but an estimator such
as (3.10) also converges towards x0 in the same conditions, and faster, since for any ﬁnite N,
its MSE is smaller.
the bias energy and that efforts to ﬁnd it would therefore be in vain. Also note that
Thus, there is an “optimum”, strictly positive, value of α, that makes the MSE
minimum. It is worth noting, however, that it depends on the true solution x

74
Bayesian Approach to Inverse Problems
By switching to logarithms, we obtain the log-marginal likelihood:
L(α, σ2
b) =
M
2 log α −N
2 log(2πσ2
b) + 1
2 log
DT D
 −1
2 log
AT A + αDT D
 −S(α)
2σ2
b
.
If this likelihood is sufﬁciently peaked, we can then satisfy ourselves with ﬁnding the
(α, σ2
b) pair that maximizes L(α, σ2
b). We have:
∂L
∂σ2
b
= −N
2 σ2
b
+ S(α)
2 σ4
b
= 0
=⇒
σ2
b = S(α)
N
,
the“usual” estimator for variance. It is, however, difﬁcult to maximize L as a function
of α. We will thus content ourselves with ﬁnding α by exploring a discrete grid, since
the result x(α) is, in general, sensitive only to variations of the order of magnitude of
α [FOR 93, THO 91].
3.8.3. Wiener ﬁltering
The “linear-quadratic” framework is the only one that allows a statistical interpre-
tation to be given in the inﬁnite dimension problem [FRA 70]:
y = A x + b ,
x ∈X , y ∈Y .
(3.12)
For this, we assume that the functions x, y and b appearing in equation (3.12) are
particular trajectories or realizations, of stochastic processes X, Y and B, linked by
an analog relation3:
Y = A X + B .
(3.13)
If the zero-mean process X depends on a variable r, its covariance function is deﬁned
as ΓX(r, r′) = E(X(r) X(r′)), and we assume that the functions x, trajectories of
the process X, belong to a Hilbert space X and the functions y and b, the respective
trajectories of Y and B, belong to the same Hilbert space Y (which may be distinct
from X). The covariance (function) of X can thus be considered as the kernel of an
operator RX deﬁned on the space X:
(RX φ)(r) =

ΓX(r, r′) φ(r′) dr′,
φ ∈X .
The inverse problem is to estimate a realization x of X, given the observation data
of the realization y of Y and probabilistic prior knowledge on the processes X and B.
3. Here, for the sake of simplicity, we also assume that processes X, Y and B have zero mean.
This hypothesis is not restrictive as, if they do not, the processes can always be centered and,
thanks to the linearity of A, relation (3.13) remains true for the centered processes.

Inversion within the Probabilistic Framework
75
In the special case where X is a Gaussian process (or any linear transformation – such
as the derivative – of a Gaussian process), the a priori probability law for X can be
written symbolically4:
pX(x) ∝exp
	
−1
2

x, R−1
X x

X

.
If we take the hypothesis that the noise process B is additive, white and Gaussian of
variance σ2, the a posteriori law can be written:
pX(x | Y = y) ∝exp
	
−1
2σ2

∥y −A x∥2
Y + σ2 
x, R−1
X x

X

.
The best estimator of x, given the observation data y, depends on the choice of the
optimality criterion but, in this case, if we choose the maximum of the a posteriori
law or the MSE and if we factorize the covariance operator according to:
RX = (C∗C)−1,
(3.14)
the solution minimizes the criterion ∥y −Ax∥2
Y +σ2 ∥Cx∥2
X. It follows that x = Gy,
where G is given by (2.7) with α = σ2. Moreover, if we deﬁne the operator RB =
σ2 Id, where Id is the identity operator in Y (RB is the covariance operator of white
noise), then G can also be written in the form:
G = RX A∗(A RX A∗+ RB)−1,
(3.15)
which is the form of the Wiener ﬁlter. Put differently, the Tikhonov regularizer (2.7)
is analogous to a Wiener ﬁlter in the case of white noise, provided that the constraint
operator C that appears in it is linked to the covariance operator RX by relation (3.14).
Note, however, that second order ergodic processes have trajectories of ﬁnite power
but inﬁnite energy: X is not a summable square function space.
Equation (3.15) differs from the usual expression for a Wiener ﬁlter expressed in
the Fourier domain. In fact, the expression above is more general. We will ﬁnd the
usual formulation again in Chapter 4, by taking advantage of additional hypotheses
such as the convolutional structure of operator A and the weak stationarity (second
order) of random processes X and B.
In contrast, in the case of non-quadratic functionals G or F, the minimization
of criterion (2.5) does not have a systematic statistical interpretation. In substance,
the difﬁculty comes from the fact that the mathematical quantity characterizing the
probability of a random process indexed on a space of ﬁnite dimension is, in this case,
a set of functions having no direct relation with (2.5) and not allowing the likelihood
function to be deﬁned naturally.
4. In fact, the law of a process is given by the joint law of the n random variables
X(r1), X(r2), . . . , X(rn), ∀n ∈
 , ∀(r1, r2, . . . , rn) ∈
n.

76
Bayesian Approach to Inverse Problems
3.9. Bibliography
[BER 94] BERNARDO J. M., SMITH A. F. M., Bayesian Theory, Wiley, Chichester, UK,
1994.
[BES 86] BESAG J. E., “On the statistical analysis of dirty pictures (with discussion)”, J. R.
Statist. Soc. B, vol. 48, num. 3, p. 259-302, 1986.
[BRO 86]
9, Hayward, CA, IMS
Lecture Notes, Monograph Series edition, 1986.
[COX 61] COX R., The Algebra of Probable Inference, Johns Hopkins University Press, Bal-
timore, MD, 1961.
[DEM 77] DEMPSTER A. P., LAIRD N. M., RUBIN D. B., “Maximum likelihood from in-
complete data via the EM algorithm”, J. R. Statist. Soc. B, vol. 39, p. 1-38, 1977.
[DEM 89] DEMOMENT G., “Image reconstruction and restoration: overview of common
estimation structure and problems”,
IEEE Trans. Acoust. Speech, Signal Processing,
vol. ASSP-37, num. 12, p. 2024-2036, Dec. 1989.
[FOR 93] FORTIER N., DEMOMENT G., GOUSSARD Y., “GCV and ML methods of deter-
mining parameters in image restoration by regularization: fast computation in the spatial
domain and experimental comparison”, J. Visual Comm. Image Repres., vol. 4, num. 2,
p. 157-170, June 1993.
[FRA 70] FRANKLIN J. N., “Well-posed stochastic extensions of ill-posed linear problems”,
J. Math. Anal. Appl., vol. 31, p. 682-716, 1970.
[GAS 92] GASSIAT E., MONFRONT F., GOUSSARD Y., “On simultaneous signal estimation
and parameter identiﬁcation using a generalized likelihood approach”, IEEE Trans. Inf.
Theory, vol. 38, p. 157-162, Jan. 1992.
[GEM 84] GEMAN S., GEMAN D., “Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images”, IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-6,
num. 6, p. 721-741, Nov. 1984.
[JAY 82] JAYNES E. T., “On the rationale of maximum-entropy methods”, Proc. IEEE, vol. 70,
num. 9, p. 939-952, Sep. 1982.
[JEF 39] JEFFREYS, Theory of Probability, Oxford Clarendon Press, Oxford, UK, 1939.
[KAS 94] KASS R. E., WASSERMAN L., Formal Rules for Selecting Prior Distributions: A
Review and Annotated Bibliography, Technical report no. 583, Department of Statistics,
Carnegie Mellon University, 1994.
[LIT 83] LITTLE R. J. A., RUBIN D. B., “On jointly estimating parameters and missing data
by maximizing the complete-data likelihood”,
Amer. Statist., vol. 37, p. 218-220, Aug.
1983.
[MAR 87] MARROQUIN J. L., MITTER S. K., POGGIO T. A., “Probabilistic solution of ill-
posed problems in computational vision”, J. Amer. Stat. Assoc., vol. 82, p. 76-89, 1987.
[POL 92] POLSON N. G., “On the expected amount of information from a non-linear model”,
J. R. Statist. Soc., vol. 54, num. B, p. 889-895, 1992.
BROWN L. D., Foundations of Exponential Families, vol.

Inversion within the Probabilistic Framework
77
[ROB 97] ROBERT C. P., The Bayesian Choice. A Decision-Theoretic Motivation, Springer
Texts in Statistics, Springer Verlag, New York, NY, 1997.
[TAR 87] TARANTOLA A., Inverse Problem Theory: Methods for Data Fitting and Model
Parameter Estimation, Elsevier Science Publishers, Amsterdam, The Netherlands, 1987.
[THO 91] THOMPSON A., BROWN J. C., KAY J. W., TITTERINGTON D. M., “A study of
methods of choosing the smoothing parameter in image restoration by regularization”, IEEE
Trans. Pattern Anal. Mach. Intell., vol. PAMI-13, num. 4, p. 326-339, Apr. 1991.
[TIT 85] TITTERINGTON D. M., “Common structure of smoothing techniques in statistics”,
Int. Statist. Rev., vol. 53, num. 2, p. 141-170, 1985.


PART II
Deconvolution


Chapter 4
Inverse Filtering and Other Linear Methods
4.1. Introduction
Many physical systems can, as a good approximation, be considered as linear with
respect to their inputs and invariant under translation of the variables of the input
signal (e.g. one time variable or two space variables). They can then be modeled
using convolution. For this reason, deconvolution is a generic inverse problem that
comes into many applications: non-destructive testing, geophysics, medical imaging,
astronomy, communications, etc. Several of these applications have chapters devoted
to them in Part II of this book.
In this chapter, we present the speciﬁcities of convolution, and then the techniques
for linear deconvolution. This is equivalent to spectral equalization; and therefore
cannot restore frequency components that have been cut out by the observation sys-
tem; it can simply compensate for some of the attenuations introduced. The resolution
of linear solutions is fundamentally limited by the spectral content of the data. To
go beyond this limit, we need to use nonlinear techniques, which require additional
information or hypotheses on the object to be restored and will be seen in later
chapters.
The main interest of linear solutions lies in their speed and ease of use – although
implementation techniques have evolved greatly in the past 20 years – and in their
remarkable robustness with respect to hypotheses on the object, precisely because of
Chapter written by Guy LE BESNERAIS, Jean-François GIOVANNELLI and Guy DEMOMENT.

82
Bayesian Approach to Inverse Problems
their simplicity1. These solutions thus serve as references to which all more elaborate
solutions are compared. They can also be intermediate means in calculations, as in the
semi-quadratic methods presented in Chapter 6. So we will pay particular attention to
these questions of implementation here.
Finally, tuning linear solutions is relatively simple: it consists of choosing the
second-order characteristics of the signals under consideration. It is possible to use the
numerous results on the second-order modeling of signals and specify their correlation
function, or their spectral density, or a difference equations model. As a last resort, a
single scalar parameter, the coefﬁcient of regularization, can be adjusted. In this case,
it is essentially a ratio between the input powers and the noise, and thus a signal-to-
noise ratio (SNR), one of the most familiar notions we could wish for. Furthermore,
there are simple, proven techniques for choosing the regularization parameter, which
are presented in section 3.8 and section 2.3.3.
4.2. Continuous-time deconvolution
In a continuous-time setting, the convolution equation can be written:
y(t) = (A x)(t) = (h ⋆x)(t) =


x(t −t′) h(t′) dt′,
t ∈
  .
(4.1)
In this equation, x is the input to be restored, h the kernel of the convolution operator A
or the impulse response (IR) of the observation system2, and y represents the observed
data. This is a special case of equation (1.9) of Chapter 1.
4.2.1. Inverse ﬁltering
If we restrict ourselves to the case where x, y and h are generalized functions or
tempered distributions possessing Fourier transforms (FT), one solving method seems
to impose itself. By Fourier transformation, equation (4.1) becomes:
⌢y(ν) =
⌢x(ν)
⌢
h(ν) ,
ν ∈
  ,
(4.2)
whence the solution by inverse ﬁltering:
⌢x(ν) =
⌢y(ν)/
⌢
h(ν)
and
x(t) = F−1{
⌢x(ν)} .
(4.3)
1. It is always wise to remember the old law of cybernetics, somewhat neglected nowadays,
that says “[. . . ] the quantity of information required is a measure of the machine’s tendency to
go wrong” [WIE 71].
2. In this chapter, it is assumed to be known. See also Chapter 8, which deals with situations
where this is not the case.

Inverse Filtering and Other Linear Methods
83
As the transform
⌢x(ν) determines x(t) completely, it is sufﬁcient, in principle, to
know
⌢y(ν) and
⌢
h(ν). However, things are not that simple [ARS 66].
If we take the situation of a convolution algebra ℵ, the necessary and sufﬁcient
condition for ∀y ∈ℵ, y = h⋆x to have a solution in ℵis that h possesses a convolution
inverse h−1 such that h ⋆h−1 = δ.
y ⋆h−1 = x. However, h−1 does not always exist if h is, for example, a distribution.
Even worse, h−1 may exist only for certain functions y. Moreover, if ℵadmits zero
divisors, there is an inﬁnite number of solutions.
We should not lose sight of the purely formal aspect of equation (4.3). For it to
have meaning, 1/
⌢
h(ν) has to exist and be a tempered distribution. Thus
⌢
h(ν) has to
be a function that is never zero for any value of ν and does not tend towards zero at
inﬁnity faster than any power of 1/ν. These are very strict conditions that are never
satisﬁed in practice. What is more, for most real observation systems, |
⌢
h(ν)| →0
when ν →∞and the solution given by (4.3) is inevitably unstable. The experimental
data always contain errors:
y(t) =


x(t −t′) h(t′) dt′ + b(t) ,
(4.4)
and there is no reason why the FT of the error, or the measurement noise, b(t), should
decrease at inﬁnity like
⌢
h(ν). Finally, introducing boundedness hypotheses on the
support of
⌢x(ν) or x(t) does not solve anything:
– if the class of inputs is restricted to limited bandwidth functions, it is necessary,
if a solution is to be found, for the support of the input spectrum to be included in the
frequency band where
⌢
h(ν) is known, which is the classic Rayleigh criterion used in
instrumentation. If we consider the instability due to small values of h, it could seem
useful to assume a bounded support for FT x if such a support was much narrower than
the support of FT h and restricted to regions with high SNR. However, this would
imply having an observation system of much higher resolution than that needed for
signal recovery, which is a rare situation of little interest;
– if, conversely, the class of inputs is limited to functions with bounded support,
we know that
⌢x(ν) is an analytical function that can, since Weïerstrass, be extended
starting from our knowledge of it on an interval. However, these analytical extension
methods require the function and all its derivatives to be known exactly, and their
implementation is too sensitive to measurement errors [KHU 77].
The example of a moving average (MA) ﬁlter:
y(t) = 1
T
 t
t−T
x(t′) dt′,
(4.5)
that will be used throughout this chapter and whose frequency response is shown in
Figure 4.1 accumulates several difﬁculties. Its frequency response is a cardinal sine
immediately:
solution
the
obtain
then
We

84
Bayesian Approach to Inverse Problems
−6
−4
−2
0
2
4
6
0
0.2
0.4
0.6
0.8
1
−6
−4
−2
0
2
4
6
10
−3
10
−2
10
−1
10
0
Figure 4.1. Modulus of transfer function of the MA ﬁlter deﬁned by (4.5) (T = 1). Scale is
linear on the left, logarithmic on the right
that attenuates the high frequencies – they are inevitably drowned in the noise – and,
what is more, is canceled out for frequencies that are multiples of 1/T. Inverse ﬁltering
cannot provide a solution.
4.2.2. Wiener ﬁltering
The simplicity of form of equation (4.3) is deceptive and we ﬁnd ourselves in
the heart of the difﬁculties described in Chapter 1. If x, y and h are assumed to be
functions with summable squares – a hypothesis that is very often acceptable from
a physical point of view – the associated operator A is compact and bounded but its
image Im A is not closed. The deconvolution problem is thus ill posed and regular-
ization methods are needed. In the framework of this chapter, we will limit ourselves
to penalty methods using quadratic regularization functionals, and will choose the
Tikhonov regularizer (2.7) or the Wiener ﬁlter (3.15).
The regularized solutions to the continuous-time deconvolution problem date back
to Wiener’s work on optimal ﬁltering in the 1950s, which was applied immediately to
solve equation (4.4) (e.g. in geophysics [ROB 54]). In Wiener’s approach, presented
in Chapter 3, the input signal and the noise are modeled by second-order random
processes, characterized by their covariance function. In the stationary case, the co-
variance functions are invariant under translation and are thus functions of a single
variable, the correlation functions rx(t) and rb(t) which can also be speciﬁed by their
FT, the power spectral densities Sx(ν) and Sb(ν).
With these hypotheses of stationarity and operator A being a convolution, the gen-
eral form of Wiener ﬁlter (3.15) takes a particularly simple expression by passing into
the Fourier domain as its frequency transfer function can be written:
⌢g(ν) =
⌢
h∗(ν) Sx(ν)
|
⌢
h(ν)|2 Sx(ν) + Sb(ν)
=
1
⌢
h(ν)
|
⌢
h(ν)|2 Sx(ν)
|
⌢
h(ν)|2 Sx(ν) + Sb(ν)
.
(4.6)
Thus, we see that the Wiener ﬁlter is, in fact, a cascade of an inverse ﬁlter – of transfer
function 1/
⌢
h(ν) – and a stabilizing ﬁlter. The latter has a practically unit transfer in

Inverse Filtering and Other Linear Methods
85
frequency bands with a high SNR, i.e., when |
⌢
h(ν)|2 Sx(ν) ≫Sb(ν) and the whole
then behaves like an inverse ﬁlter. On the other hand, when the SNR becomes zero in
other frequency bands, the transfer becomes zero, as does that of the stabilizing ﬁlter,
thus controlling the divergence of the inverse ﬁlter. Thus we see clearly why Wiener’s
solution, or “linear quadratic” regularization can only manage to equalize the spectral
content of the data in the band of frequencies where the SNR is sufﬁcient.
Filter (4.6) is generally not realizable (being non-causal), which is obviously a
nuisance for implementation using an analog electronic device. Many authors have
thus taken an interest in obtaining solutions under constraints, e.g. of causality or
ﬁnite duration. The ﬁrst causal solutions used spectral factorization of the signals
under consideration, i.e., they represented a signal as the output from a ﬁlter with
white noise as the input [VAN 68].
In the 1960s, such representations were replaced by state representations (see sec-
tion 4.5), which not only enabled the implementation problems for constrained ver-
sions of the Wiener ﬁlter to be solved but, above all, allowed the results to be eas-
ily extended to the non-stationary case. Kalman-Bucy ﬁltering [KAL 60] was born,
and was rapidly applied to deconvolution, e.g. in geophysics, in the 1970s. The refer-
ence [BAY 70] gives a good introduction to continuous-time Kalman ﬁltering together
with examples of its use in geophysics, illustrated by the processing of analog data.
Nowadays, with the development of personal computers, the signals are most often
discretized as soon as they are acquired. So, before presenting the corresponding
methods, we are going to study the consequences of this discretization on the nature
of the problem.
4.3. Discretization of the problem
As the signal to be deconvolved y is discretized, the input to be restored x is also
discretized most of the time, which leads to the processing of a discrete-discrete in-
verse problem (see Chapter 1). The discretization of the output often comprises an
integration (as, for example, with a CCD camera). The developments that follow, con-
cerning the case of simple sampling, can be mostly used as they stand, the integration
being included in the system response h.
4.3.1. Choice of a quadrature method
To discretize the problem, the input to be restored x ∈X (see section 1.3) is
decomposed on a family of functions {gm}:
x(t) =
M

m=1
xm gm(t) + x∗(t) ,
(4.7)

86
Bayesian Approach to Inverse Problems
where the xm are the decomposition coefﬁcients and x∗is a residual truncation error
term.
If the observed signal y is sampled regularly at a ﬁnite number, N, of instants
tn = nΔt, we can write:
yn = y(nΔt) =
M

m=1
hn,m xm + bn , n = 1 . . . , N,
(4.8)
where hn,m =

gm(nΔt −t) h(t) dt, and bn = b(tn) represents the sum of the
measurement noise and the ﬁltered truncation error

x∗(nΔt −t) h(t) dt.
It is common practice to decompose x on a Δt-shifted kernel basis gm(t) =
g(t −mΔt), which allows the problem to be written in the form of a discrete-time
convolution:
yn =
M

m=1
hn−m xm + bn ,
n = 1, 2, . . . , N,
but it should be noted that the discrete IR is then sampled from a ﬁltered version of h:
hk = (h ⋆g)(kΔt).
(4.9)
Hence, in general, the cut-off frequency of the discrete-time system is different from
that of the continuous-time system.
When the basis kernel g is an interval indicator of width Δt, the coefﬁcients xm
and hk are the mean values of the input and the IR, respectively, over intervals of
width Δt. When the basis kernel is a cardinal sine of pseudo-period Δt, convolution
(4.9) truncates
⌢
h to the band (−Δt/2, Δt/2]. If h is of limited bandwidth included in
this interval, Shannon’s condition is veriﬁed and hk = h(kΔt). Otherwise, the cut-off
frequency of the discrete system is lower than that of the continuous problem, which
in fact corresponds to a regularization by dimension control.
Another element to be taken into consideration is the time-domain truncation of the
IR hk, which is indispensable to obtain a ﬁnite-dimension problem. From this point of
view, choosing a cardinal sine, which has a very slow decay, as the basis kernel leads
to greater truncation effects than the use of indicators. Intermediate choices exist, of
course, e.g. spline functions or prolate spheroidal functions.
By concatenating the N equations (4.8) describing the relations among the values
of the observed signal yn, those of the signal to be restored xn and the coefﬁcients of
the IR hn (direct problem), we obtain a linear N × M system of the form (1.3)
y = H x + b .
(4.10)

Inverse Filtering and Other Linear Methods
87
We know (see Chapter 2) that the difﬁculties of inverting such a system are connected
with the conditioning of matrix H, which we will study after having clariﬁed its struc-
ture.
4.3.2. Structure of observation matrix H
In what follows, the notation hk will be used to mean the IR of the discrete
problem and we will take it that its effective support – i.e., the domain for which
the coefﬁcients hk have signiﬁcant values – is smaller than the time interval NΔt
over which y is observed.
As this IR is not necessarily causal, we write: h =
[h−Q, . . . , h0, . . . , h+P ]T. These P + Q + 1 signiﬁcant coefﬁcients are thus such
that: P + Q + 1 < N. The discrete convolution equation is then:
yn =
P

p=−Q
hp xn−p + bn ,
n = 1, 2, . . . , N,
(4.11)
The deconvolution problem is often considered to consist of estimating a vector
x = [x1, . . . , xN]T from an observed vector y = [y1, . . . , yN]T having the same
support. Unfortunately, equation (4.11) shows that it is then impossible to establish a
relationship like (4.10) between these two vectors, because of boundary effects. Actu-
ally, if we write out the components of matrix equation (4.10), we obtain:
⎡
⎢⎢⎢⎢⎢⎣
y1
y2
...
yN
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎣
hP . . . h0 . . . h−Q 0
. . .
0
0
hP . . . h0 . . .
h−Q 0
. . .
... ...
...
... ...
. . . 0
hP . . .
h0
. . . h−Q 0
0
. . . 0
hP
. . .
h0 . . .
h−Q
⎤
⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x−P +1
...
x0
x1
...
xN
xN+1
...
xN+Q
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(4.12)
and the problem is undetermined because M = dim x = N +P +Q > dim y = N.
Matrix H thus has a Toeplitz form with a band structure, as P + Q + 1 < N.
In the case of an image (2D problem), discrete convolution (4.11) involves summing
over two indices and it is usually brought down to the same matrix expression (4.10)
by constructing vectors x and y by a lexicographical scan – e.g. line by line from left
to right and top to bottom – of the tables containing the values of the pixels of the
object to be restored and of the blurred observed image [HUN 73, JAI 89]. Matrix H
deduced from this has a block-Toeplitz structure and each block is itself Toeplitz. To

88
Bayesian Approach to Inverse Problems
simplify the notation, we will call them Toeplitz-block-Toeplitz matrices from now
on.
It should ﬁrst of all be noted that the structure of H allows the product Hx to be
calculated quickly. It is always possible to complete this matrix with additional rows
so as to obtain a circulant square matrix Ch of order M = N + P + Q. A circulant
matrix is a Toeplitz matrix entirely deﬁned by its ﬁrst row: a row is obtained from the
previous one by circular permutation. Circulant square matrices can be factorized in
the Fourier basis [HUN 71]:
Ch = W∗Λh W ,
(4.13)
where Wk,ℓ= e−2jπ(k−1)(ℓ−1)/L/
√
Mis the unitary Fourier matrix of order M and
Λh is a diagonal matrix in which the diagonal elements are obtained by discrete
Fourier transform (DFT calculable by fast Fourier transform — FFT) of the ﬁrst col-
umn of Ch. The product Hx can then be obtained by the following operations:
1) DFT of x and of the ﬁrst column of Ch;
2) component by component product of the two DFTs obtained;
3) inverse DFT of result;
4) extraction of y (of dimension N) in the resulting vector.
This technique, which uses zero-padding, is usually very advantageous in terms of
computing cost. In 2D, it can be generalized by making use of the properties of
circulant-block-circulantmatrices (i.e., circulant by blocks and where the blocks them-
selves are circulant) with square blocks [HUN 73].
To return to boundary problems: if we deﬁne the vectors xl = [x−P +1, . . . , x0]T
and xr = [xN+1, . . . , xN+Q]T to be the left and right “borders” of the unknown
vector x, linear system (4.12) can be written [NG 99]:
y = Hl xl + Hc x + Hr xr + b ,
(4.14)
where from now on we note x = [x1, . . . , xN]T. Hl is made up of the ﬁrst P columns
of H, Hr of the last Q columns, and Hc designates the central part of H.
There are then two cases:
1) Vectors xl and xr are unknown and we want to estimate them, thus trying to
obtain M = N + P + Q values from N equations. The resulting indetermination
is not a problem in a regularized framework (see Chapter 2). On the other hand, the
convolution matrix does not have a circulant structure, which leads to more costly
inversion algorithms.
2) Vectors xl and xr are unknown and we do not want to estimate them. Thus
we want to approximate equation (4.14) by a square N × N system. There are sev-
eral approximations that correspond to boundary conditions added to the statement

Inverse Filtering and Other Linear Methods
89
of the problem. The prewindowing (respectively postwindowing) consists of taking
xl (respectively xr) to be zero, which can be justiﬁed in certain situations (causal or
transient phenomenon). Below, we describe two other boundary hypotheses that are
very advantageous in terms of computing cost.
4.3.3. Usual boundary conditions
A much-used approximation involves taking the hypothesis that the signal to be
restored is N-periodic: xl = [xN−P +1, . . . , xN]T and xr = [x1, . . . , xQ]T. Equa-
tion (4.14) then becomes y = HP x + b, where the new convolution matrix:
HP =

0N×(N−P ) | Hl

+ Hc +

Hr | 0N×(N−Q)

(4.15)
is circulant and can thus be diagonalized in a Fourier basis: HP = W∗Λh W. Its
eigenvalues are obtained by DFT of the ﬁrst column hP of HP, which is an N-periodic
zero-padded RI. In the case of the non-causal RI h introduced in section 4.3.2, it writes
hP = [h0, . . . , hP , 0, 0, . . . , 0, h−Q, . . . , h−1]T .
Another, less frequent, approximation imposes a “mirror condition” (also known
as a Neumann boundary condition [NG 99]) on the boundaries or, in other words,
assumes that xl = [xQ, xQ−1, . . . , x1]T and xr = [xN, xN−1, . . . , xN−P +1]T.
The new convolution matrix coming from equations (4.10) and (4.14) takes the form:
HM =

0N×(N−P ) | Hl

J + Hc +

Hr | 0N×(N−Q)

J
(4.16)
where J is the unit Hankel matrix, or reversal matrix, of dimensions N × N. In this
case, matrix HM is neither Toeplitz nor circulant, but “Toeplitz-plus-Hankel”. When
the IR of the system is even (hk = h−k and P = Q), these matrices can be factorized
like circulant matrices but by discrete cosine transform (DCT).
These approximations can be immediately generalized to the 2D case [NG 99]. It
is easy to see the interest here: as circulant or Toeplitz-plus-Hankel matrices can be
diagonalized in a complex exponential or cosine basis, their factorization, which is a
preparation for the subsequent inversion, has a reduced computing cost thanks to the
DFT or DCT. It remains to be seen what consequences these approximations have on
signals that satisfy neither a periodic nor a mirror boundary hypothesis. We will look
at this later using an example.
4.3.4. Problem conditioning
The calculation of the conditioning of the problem is based on the study of the
non-zero eigenvalues of matrix HT H (the problem of zero eigenvalues being settled
by resorting to a generalized inverse). According to the boundary hypotheses, either

90
Bayesian Approach to Inverse Problems
HT H is circulant or one of the two matrices HHT or HT H is Toeplitz (or Toeplitz-
block-Toeplitz in 2D). We have seen in equation (4.13) that eigenvalues of circulant
matrices are easy to calculate via DFT. For non-circulant Toeplitz matrices, however,
there are only asymptotic results. In both cases, the results bring in the transfer func-
tion associated with the discrete IR:
⌢
H(ν) =
P

p=−Q
hpe−2jπpν,
ν ∈[0, 1) .
4.3.4.1. Case of the circulant matrix
When the signal to be restored is N-periodic, we have seen that the resulting con-
volution matrix HP is circulant of order N, and equation (4.13) shows that the eigen-
values of the normal matrix of the problem HT H form a regular sampling of the
square of the modulus of
⌢
H: λ2
h(k) = |
⌢
H(k/N)|2, 0 ≤k ≤N −1.
The conditioning of the matrix is thus directly linked to a framing of the values of
the function |
⌢
H(ν)|2: an example is given in Figure 4.2 for the case of the MA ﬁlter
of equation (4.5). The problem was discretized with a sampling period Δt = T/20.
Although none of the eigenvalues is identically zero in this example, the condition
number c of problem (1.22) is very high: c > 1010 and we can expect a few difﬁculties
during generalized inversion, as we will see in section 4.3.5.
0
50
100
150
200
250
300
350
400
0
0.2
0.4
0.6
0.8
1
0
50
100
150
200
250
300
350
400
10
−10
10
−5
10
0
Figure 4.2. Spectrum of eigenvalues of the normal matrix HT H for the case of a signal
deteriorated by the MA ﬁlter (4.5) (periodic boundary condition)
4.3.4.2. Case of the Toeplitz matrix
In the general case of equation (4.12), matrix HHT (which is of order N) is sym-
metric band-Toeplitz. It is always possible to include it in a circulant matrix of order
N + P + Q where the ﬁrst row is a permutation of the autocorrelation of the IR
completed by zeros and has eigenvalues |
⌢
H(k/(N + P + Q))|2. For large N, matrix
HHT appears as a perturbation of the circulant matrix. Szegö’s theorem [GRE 58] de-
scribes the convergence of the spectra of these two matrices towards each other, which
is another way of saying that the eigenvalues of HHT tend to be evenly spread over

Inverse Filtering and Other Linear Methods
91
|
⌢
H(ν)|2 when N tends towards inﬁnity. It can thus be shown that the conditioning of
the problem tends towards the ratio:
maxν∈[0,1) |
⌢
H(ν)|2
minν∈[0,1) |
⌢
H(ν)|2 .
4.3.4.3. Opposition between resolution and conditioning
We inevitably ﬁnd ourselves faced with the following dilemma:
– either the IR has a limited spectrum and we choose a sampling period Δt small
enough to respect Shannon’s condition. There is no change in the cut-off frequency
due to the sampling (see section 4.3) but the problem is very ill-conditioned as the
function |
⌢
H(ν)|2 takes very small values;
– or the sampling period is chosen big enough for there to be a change in the cut-
off frequency of the system: the conditioning is better but we give up the possibility
of reaching a high-resolution solution during deconvolution.
This situation is unusual in numerical analysis: choosing an increasingly ﬁne dis-
cretization of equation (4.1) in order to reduce the quadrature error makes solving
equation (4.10) more and more delicate. This is precisely the distinctive feature of
ill-posed problems.
4.3.5. Generalized inversion
To illustrate the inﬂuence of poor conditioning on deconvolution, Figure 4.3 pre-
sents an example of a least squares solution (1.17) in the case of a signal ﬁltered by
the MA ﬁlter of equation (4.5). If the normal matrix has zero eigenvalues, the gener-
alized inverse (1.19) behaves in a similar way. We observe, as predicted in Chapter 1
(section 1.3), a strong ampliﬁcation of the noise components at the frequencies atten-
uated by the ﬁlter, even though the SNR is very favorable (30 dB). In particular, we
50
100
150
200
250
300
350
400
−4
−2
0
2
4
(a) ideal input and data
50
100
150
200
250
300
350
400
−20
−10
0
10
20
30
40
(b) least squares solution
Figure 4.3. Example of 1D deconvolution (MA ﬁlter of (4.5) with a SNR of 30 dB): the
solution (b) is unacceptable

92
Bayesian Approach to Inverse Problems
ﬁnd again the inﬂuence of the ﬁrst zero transmission in the neighborhood of which the
eigenvalues of HT H are very small (see Figure 4.2), which engenders an oscillation
of period T , the duration of the integration window of the ﬁlter.
4.4. Batch deconvolution
Processing the data as a batch supposes having computing and memory capacity
suited to the value of N. If these conditions are satisﬁed, the implementation essen-
tially comes down to the inversion of a matrix, which is often large but structured.
This is what we are going to see in this section. When the above conditions are not
satisﬁed, we can turn to recursive solutions as presented in section 4.5.
4.4.1. Preliminary choices
The methodology explained in this book consists of choosing and minimizing a
composite regularized criterion such as (2.5):
J (x) = G(y −H x) + α F(x) ,
and this chapter is devoted to the quadratic terms (we denote ∥x∥A = xT Ax):
G(y −H x) = ∥y −H x∥2
R−1
b
and
F(x) = ∥x −mx∥2
Q .
According to the Bayesian interpretation of Chapter 3, this criterion is equivalent to
maximum a posteriori estimation with a Gaussian distribution for the noise: b ∼
N(0, Rb) and a Gaussian prior for the object: x ∼N(mx, Rx), with Rx ∝Q−1,
assuming of course that Q is deﬁned positive.
A dominant characteristic of the deconvolution methods presented in this chapter
is that they are based on two strong properties: 1) the invariance under translation
of the observation system, characteristic of a convolution, which makes H a Toeplitz
matrix, and 2) the stationarity of the phenomena being considered, which leads to
Toeplitz covariance matrices Rb and Rx.
With generalization in mind, we could choose a non-zero mean mb in the distri-
bution for the noise so as to take account of a systematic error during data acquisition.
If this additional degree of freedom is to be really useful, it is necessary for this error
to be accessible, otherwise it would be impossible, in this “linear-quadratic” frame-
work, to separate mb and Hx. If this error is accessible, we just have to subtract it
from the data before performing the inversion. In the case of a stationary white noise,
i.e., Rb = σ2
b I, the regularization parameter appears equivalent to an inverse SNR
α = σ2
b/σ2
x.

Inverse Filtering and Other Linear Methods
93
The a priori mean for the object mx, or default solution, is not obliged to be sta-
tionary, i.e., constant, and the use of an appropriately chosen, non-constant mean has
been suggested in order to “Gaussianize” the histogram of the experimental values y
and thus come closer to the normality hypothesis implicit in these methods [HUN 76].
However, not too much should be expected of this technique. The quality of the solu-
tion is fundamentally limited by its linearity with respect to the data and the use of a
non-zero mean mx does not greatly change the performance of the estimator.
As far as matrix Q is concerned, a simple and usual choice for the quadratic regu-
larization term is
α F(x) = α0

x2
n + α1

(xn+1 −xn)2.
(4.17)
This allows penalties to be applied to the norm of the solution and to that of the
ﬁrst differences (penalization of non-smooth solutions). The two terms of (4.17) are
often called the zeroth-order and ﬁrst-order penalties. Note that the choice α1 > 0
and α0 = 0 leads to a singular matrix Q (and an improper a priori distribution, see
Chapter 7, section 7.3.1.3).
This type of regularization is generalized by introducing a discretized derivation
operator D (differences of order p in 1D, Laplacian in 2D) and deﬁning:
Q = DT D .
(4.18)
4.4.2. Matrix form of the estimate
With the above choices, the criterion writes
J (x) = ∥y −H x∥2
R−1
b
+ α ∥Dx∥2
(4.19)
and the solution is
x = (HT R−1
b H + α DT D)−1HT R−1
b
y .
(4.20)
An example of such a solution is shown in the right-hand part of Figure 4.4 in
the example of the MA ﬁlter of equation (4.5), and using a ﬁrst-order regularization
(α0 = 0 in (4.17)). It suffers from the defects belonging to the quadratic framework.
Relation (2.8) shows that it is impossible to correctly restore all the components of the
solution (which are the coefﬁcients of its Fourier series expansion under the periodic
hypothesis). Those in the neighborhood of the eigenvalues λ2
h(k) that are too small or
zero are forced to zero and the discontinuities of the input signal, which require a very
large number of Fourier coefﬁcients to be correctly restored, are thus ﬁltered. This
results in parasite oscillations in their neighborhood, known as ringing (or the Gibbs
phenomenon).

94
Bayesian Approach to Inverse Problems
Calculating solution (4.20) has a priori a cost proportional to O(M 3) elementary
arithmetic operations (scalar multiplications and additions). With speciﬁc boundary
hypotheses, such as the periodic hypothesis, the structure of the matrix to be inverted
makes it possible to use algorithms with a cost proportional to O(M log M), but the
boundary effects can be large. For instance in the example in Figure 4.4, the periodic
hypothesis enables a rapid calculation (see section 4.4.3) but estimates a signal x that
has a very marked boundary effect expressed by large-amplitude oscillations (see left-
hand part of the ﬁgure). It is worth noting however that the central part of the solution
with periodic hypothesis is practically the same as using an exact calculation.
50
100
150
200
250
300
350
400
−4
−2
0
2
4
50
100
150
200
250
300
350
400
−4
−2
0
2
4
Figure 4.4. 1D deconvolution: “periodic” solution (left) and “exact” solution (right). The
actual input and the data are shown in Figure 4.3a. The choice of α1 = 0.05 is obtained by
minimizing the quadratic error between the exact solution and the true input, which is
calculable in this simulated example
To avoid boundary effects, border signals xl and xr must be estimated as well as
the central part. It is thus necessary to invert the matrices that are only Toeplitz or
close to Toeplitz. The inversion of such matrices has been the subject of many works.
The algorithms given by Levinson [LEV 47] enable inversion to be carried out with
a computing cost proportional to O(N 2) [GOL 96].
In the 1980s, algorithms in
O(N(log N)2) were proposed [MOR 80]. We will not go into the detail of these works
as we prefer to present fast solutions based on the DFT [HUN 73] or the conjugate gra-
dient (CG) algorithm [COM 84, CHA 88, CHA 93, CHA 96, NAG 96], which keep us
in an algebraic framework close to that used in the other chapters of this book.
4.4.3. Hunt’s method (periodic boundary hypothesis)
Hunt’s method [HUN 73] is based on the periodic approximation introduced in
section 4.3.3 and on a circulant approximation DP of the ﬁnite difference operator D.
Both circulant operators are diagonalized in the Fourier basis
HP = W∗Λh W ,
and
DP = W∗Λd W .
Diagonal components of Λh are obtained by DFT of the ﬁrst column hP of HP:
⌢
h =
WhP. We have seen in section 4.3.3 that hP is a N-periodic zero-padded RI and it is

Inverse Filtering and Other Linear Methods
95
easy to show that its DFT forms a regular sampling of
⌢
H:
[Λh]ℓ,ℓ=
⌢
hℓ=
⌢
H((ℓ−1)/N),
1 ≤ℓ≤N.
The diagonal of Λd is the vector
⌢
d = WdP, where dP denotes the ﬁrst column of
the circulant derivation matrix DP and forms a regular sampling of the correspond-
ing transfer function
⌢
D. For instance, in the case of a ﬁrst-order penalty (see sec-
tion 4.4.1), the derivation kernel is d = [−1, 1]T, dP = [−1, 1, 0, . . ., 0]T ,
⌢
D(ν) =
exp(−2jπν) −1 and
⌢
dℓ=
⌢
D((ℓ−1)/N).
In the sequel we focus on the case of a stationary white observation noise Rb = I.
Using diagonalized operators in (4.19-4.20), we obtain a separable criterion in the
Fourier domain, in terms of the DFT
⌢x = Wx and
⌢y = Wy:
J (x) =
N

ℓ=1
	
|
⌢yℓ−
⌢
hℓ
⌢xℓ|2 + α |
⌢
dℓ
⌢xℓ|2
,
(4.21)
and the solution is

⌢xℓ=
⌢
h∗
ℓ
⌢yℓ
|
⌢
hℓ|2 + α |
⌢
dℓ|2 ,
1 ≤ℓ≤N.
(4.22)
Hunt’s algorithm can then be summarized by the following operations:
1) calculate the DFT of the data
⌢y = Wy;
2) calculate the DFT of the N-periodic zero-padded RI
⌢
h = WhP;
3) calculate the DFT of the N-periodic zero-padded derivation kernel
⌢
d = WdP;
4) calculate the solution in the Fourier domain using (4.22) and the time-domain
solution x by inverse DFT.
As each Fourier transform is performed using the FFT, the resulting algorithm is
very fast. Note that 2D periodic deconvolution with stationary covariances, which
beneﬁts from the circulant-block-circulant structure of the matrices, leads to exactly
the same operations but using 2D DFT, the 2D RI of the observation system and a 2D
derivation mask (for instance a Laplacian mask).
Solution (4.22) can be considered as a time-limited implementation of the Wiener
deconvolution ﬁlter (4.6) in the case of white noise (Sb(ν) = const.) the signal spec-
trum Sx being replaced by 1/|
⌢
d|2. This is not only an analogy, but matches the
Bayesian interpretation of matrix Q = DT D as the inverse of the correlation ma-
trix of the signal to be restored (except for a multiplicative coefﬁcient, which is ac-
counted for in the regularization parameter α). For a ﬁrst order regularization, we get
Sx(ν) ∝1/(1 −cos 2πν) and, by expanding the cosine in ν < 1, we can observe that

96
Bayesian Approach to Inverse Problems
this regularization amounts to choosing a signal power spectra decreasing as 1/ν2. A
generalization is to choose a power decrease model for the signal spectrum such as
Sx(νℓ) =
1
|
⌢
dℓ|2 =
1
1 + (νℓ/νc)p , νℓ= ℓ−1
N
, 1 ≤ℓ≤N,
(4.23)
with 2 ≤p ≤4, where νℓis the normalized discrete frequency and νc is a cut-off
frequency, or the inverse of a mean correlation length.
Finally, it should be noted that the above calculations (diagonalization in a trans-
formed space) are similar to those of [NG 99] for a mirror condition when the IR is
even. The diagonalization is then carried out by DCT.
4.4.4. Exact inversion methods in the stationary case
In the stationary case, it may be necessary to make boundary hypotheses that do
not lead to a fast solution. In fact, we often satisfy ourselves with a fast solution for
one of the following reasons:
1) the object is conﬁned to the center of the reconstructed support and has identical
values at each of its boundaries;
2) the object is spread but we are interested in its values in the central zone only
(so the boundaries will be eliminated after restoration);
3) the noise level is such that the regularization used “erases” the boundary effects;
4) the IR is symmetric and the use of the mirror condition is sufﬁcient.
Thus, it is relatively rare to deal with the exact stationary problem, i.e., without an
approximate boundary hypothesis3.
If the case arises, it is possible to use a conjugate gradient (CG) algorithm prefer-
ably with a preconditioner (PCG). Circulant preconditioner CG (CPCG) algorithms
are particularly well suited to deconvolution problems.
We know that conjugate gradient algorithms enable quadratic problems of dimen-
sion N to be solved in N iterations. The cost of each iteration is dominated by that of
multiplying a matrix by a vector, the dimensions of which are those of the data. In the
case of Toeplitz matrices, this cost falls to O(N log N), thanks to the use of the FFT.
The use of preconditioners allows us to obtain algorithms that converge towards
the solution linearly (and sometimes even a little faster; see [CHA 96]). This reduces
3. For instance, the 2D example which is dealt with in an exact stationary setting in the ref-
erences [CHA 93, CHA 96, NAG 96] comes into category 1 and could be suitably treated by
periodic approximation.

Inverse Filtering and Other Linear Methods
97
the cost of a solution, which is not exact but acceptable in the sense of a given stop
criterion, to O(N log N) operations. In fact, in many cases, the CPCG algorithms
can been stopped after less than about ten iterations, which explains the saving with
respect to the standard CG. Using CPCG for deconvolution was proposed in 1984 by
Commenges [COM 84]. Rediscovered somewhat later [CHA 88], deconvolution by
CPCG has become a reference method today, as it is as fast as the Toeplitz matrix
inversion algorithms and easier to use [CHA 96, NAG 96, NG 99].
To simplify the notation, in this section we will consider that the noise is white and
use the regularization based on a discrete operator D as in (4.18) so that the criterion
to be minimized can be written:
J (x) = ∥y −H x∥2 + α ∥D x∥2
= ∥v −S x∥2,
v =
!
y
0
"
,
S =
!
H
√α D
"
.
The preconditioning consists of introducing a new vector u = Π x, where Π is
a matrix (the preconditioner) that is close to S while being quick to compose and to
invert. We then use the CG algorithm to minimize:
J (u) = ∥v −S Π−1u∥2,
which leads to a much smaller number of iterations than for the minimization of x, but
implies an added cost per iteration corresponding to the products of Π−1 and (Π−1)T
by the vectors. By using circulant preconditioners [CHA 88], we can carry out these
products by FFT, which allows the cost of an iteration to be kept to O(N log N).
For example, taking our inspiration from deconvolution in the periodic hypothesis,
we can use a preconditioner deduced from the circulant matrix:
C = W∗diag
	#
|
⌢
hℓ|2 + α |
⌢dℓ|2

W ,
ℓ= 1, 2, . . . , M,
where
⌢
hℓand
⌢
dℓare the coordinates of the respective DFT of h and d, the ﬁrst columns
of matrices H and D [COM 84, NAG 96]. This method can be extended to the 2D case
by using a circulant-block-circulant matrix constructed from the 2D transfer function
of the instrument and the DFT of the derivation kernel; see [NAG 96]. By initializing
the CPCG algorithm to zero, the ﬁrst iteration of the algorithm coincides with Hunt’s
solution that we saw in the previous section. So, after a ﬁrst circulant inversion step,
we may very well decide to continue with the CPCG algorithm of [NAG 96] if solution
(4.22) has boundary effects that are too pronounced.

98
Bayesian Approach to Inverse Problems
4.4.5. Case of non-stationary signals
In the non-stationary case, the Toeplitz or Toeplitz-block-Toeplitz characteristic of
the matrices is lost and the computing cost of a matrix inversion is, a priori, propor-
tional to O(N 3). It is always possible to implement a PCG algorithm to calculate the
solution, but choosing the preconditioner is trickier. On this subject, see the recent
works by Fessler et al. [FES 99]. Also note the possibility of using Kalman smooth-
ing, which will be presented in section 4.5.
4.4.6. Results and discussion on examples
4.4.6.1. Compromise between bias and variance in 1D deconvolution
The expressions for the bias and the covariance matrix of the linear operators were
recalled in section 3.8 for the general case. In the speciﬁc case of deconvolution with
the periodic, stationary hypothesis, all the matrices involved are square, circulant and
can be diagonalized in the Fourier basis, and it is interesting to work in the frequency
domain.
In the example of the MA ﬁlter of equation (4.5) regularized according to (4.17),
relation (4.22) shows that the regularizer Rα of equation (2.7), deﬁned by x = Rα y,
has a frequency transfer function:
⌢g ℓ=
⌢
h∗
ℓ
|
⌢
hℓ|2 + α0 + α1|
⌢
dℓ|2 ,
ℓ= 1, . . . , N .
The DFT of the bias is thus easy to calculate (xo is the original object):
E(x) −xo
DFT
−−−−−→
	⌢g ℓ
⌢
hℓ−1
 ⌢xo
ℓ

N
ℓ=1 ,
(4.24)
The squares of the moduli of each of the coordinates of this DFT – the sum of which
gives the bias energy, by isometry – are plotted on the left-hand part of Figure 4.5, as
a function of the reduced discrete frequency νℓ= (ℓ−1)/N. We observe that, for
frequencies where the SNR is high (|
⌢
hℓ|2 ≫α0 + α1 |
⌢
dℓ|2 in the example consid-
ered), the frequency components of the bias energy of the regularized solution have a
negligible modulus. On the other hand, at frequencies that are very much degraded by
the system (high frequencies and the neighborhood of the transmission zeros), these
components take notable values, which are nevertheless lower than the values reached
by the true solution (section 3.8.1).

Inverse Filtering and Other Linear Methods
99
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
10
20
30
40
50
60
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
0.5
1
1.5
2 x 10
5
Figure 4.5. Components of the bias energy (normalized by
⌢x0), left, and components of the
trace of the covariance matrix of the estimator (normalized by σ2
b), right, versus the reduced
discrete frequency for the example of the MA ﬁlter of section 4.2. Solution regularized with the
prior (4.17) at top (α0 = 0, α1 = 0.05), least squares solution at bottom (note the scale of the
vertical axes)
Similarly, the trace of the covariance matrix Rα of the solution can be expressed
as a sum of frequency components:
trace Cov(x) = σ2
b
N

ℓ=1
|
⌢g ℓ|2,
which are plotted on the right-hand side of Figure 4.5, as a function of the reduced
discrete frequency. This time, we observe that, for frequencies that are very much
degraded by the system, these frequency components have an amplitude that is clearly
reduced with respect to that obtained with a non-regularized solution.
Overall, the mean quadratic error, the sum of the bias energy and the trace of the
covariance matrix of the solution, decreases as soon as α1 > 0, i.e., as soon as regular-
ization is performed. This is what Figure 4.6 clearly shows. This error has a minimum
for α1 close to α1 = 0.02, which approximately corresponds to the value α1 = 0.05
determined by minimizing the simple quadratic error, deﬁned as ∥x −xo∥2. We also
observe a general characteristic of these quadratic regularization methods: the restora-
tion error only varies signiﬁcantly for variations of about an order of magnitude in the
regularization parameter. There is thus little point in trying to ﬁne tune it. In practice,
the true solution xo is obviously unknown and the optimum value of the regulariza-
tion coefﬁcient cannot be set by minimizing the mean quadratic error. It can be set
directly by the user (supervised mode), or estimated from the only data available (non-
supervised mode), by cross validation or maximization of a marginal likelihood, the
expressions for which were given in sections 2.3.3 and 3.8.2. The results obtained in

100
Bayesian Approach to Inverse Problems
10
−8
10
−6
10
−4
10
−2
10
0
10
2
10
4
10
6
−1
0
1
2
3
10
−8
10
−6
10
−4
10
−2
10
0
10
2
10
4
10
6
0
5
10
15
10
−8
10
−6
10
−4
10
−2
10
0
10
2
10
4
10
6
−2
0
2
4
6
8
10
12
14
Figure 4.6. Bias energy (top left), trace of the covariance matrix of the solution (bottom left)
and mean quadratic error (right) versus α1 (α0 = 0), on a logarithmic scale. The minimum
quadratic error is obtained for α1 = 0.02, and is marked by a star. The cross marks the value
used previously (α1 = 0.05). The values corresponding to the least squares solution are
marked by a circle (α1 = 0)
the example of the MA ﬁlter (not presented here) show that the value of the regular-
ization coefﬁcient obtained in this way is very close to the value that minimizes the
mean quadratic error.
4.4.6.2. Results for 2D processing
In this section, we present a result of 2D deconvolution on real data provided by
L.M. Mugnier and J-M.Conan (ONERA/DOTA/HRA). It concerns the observation of
Ganymede (one of the moons of Jupiter) by the Haute-Provence observatory, France,
using an adaptive optics system developed by ONERA. These data are presented in
more detail in Chapter 10.
Ganymede, discovered by Galileo, is visible from the Earth with an ordinary tele-
scope. Using a telescope with adaptive optics partly corrects the inﬂuence of atmo-
spheric turbulence but the image obtained, shown on the left of Figure 4.7, still has
marked defects due to residual errors of the wavefront correction. This degradation is
only approximately modeled by a convolution, using a time average of the residues as
the instrument response4. The response of the system is given in Figure 4.7. It shows
a fairly ﬁne peak on a broad circular plateau. The object is conﬁned to the center of
the ﬁeld of observation, which enables the doubly periodic boundary hypothesis to be
employed successfully, with Hunt’s method.
4. Chapter 10 presents a treatment of this problem by myopic deconvolution, which is more
suitable.

Inverse Filtering and Other Linear Methods
101
Figure 4.7. Left, the observed image; center, the instrument response (IR) on a linear scale;
and right, the logarithm of this response thresholded at 1/1,000th of its maximum. On the IR,
a central peak can be observed, with a vast plateau spreading over most of the image support
Figure 4.8 left shows that a truncated singular value decomposition (TSVD; see
section 2.1.1) leads to a very noisy result in this example, despite the truncation. Using
a quadratic regularization – which is equivalent to damping the singular values that are
too small, see (2.8), rather than truncating them abruptly below a certain threshold –
allows the noise ampliﬁcation to be controlled better, as the central part of Figure 4.8
shows, in comparison with the reference image on the right which was reconstructed
from images taken by probes exploring the solar system5. Several details visible in the
restored image are conﬁrmed by the reference image: a dark zone at upper left, and a
light zone at lower center.
Figure 4.8. Left, deconvolution using TSVD; center using Hunt’s method with Laplacian
regularization (4.23); right, the reference
5. Data from the NASA/JPL base; see http://space.jpl.nasa.gov/.

102
Bayesian Approach to Inverse Problems
Figure 4.9 presents a comparison between two a priori models of the 2D power
spectral density for the object deduced from (4.23). The law for the decrease of
the power spectral density as a function of the modulus of the spatial frequency is
different. We note that the solutions are close and the general characteristics of lin-
ear solutions are again present: no spectral extrapolation, and ringing (Gibbs phe-
nomenon) at intensity jumps on the edges of the object.
10
−2
10
−1
10
0
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
10
0
Figure 4.9. Comparison of results obtained with a prior spectrum of type (4.23) and two
different laws for the power spectral density of the object (shown on left): one in 1/ν4
(Laplacian regularization, result in center) and the other in 1/ν3 (result shown on right)
4.5. Recursive deconvolution
It is not always possible, or even desirable, to process the data wholesale, either
because the matrix to be inverted exceeds the available computer capacity, or because
we want to do the processing on-line, or in real time as the data are acquired. If we
give up the idea of processing the data recursively several times6, as in the last case,
Kalman ﬁltering naturally provides a suitable method. We will start by restricting
ourselves to 1D signals but the 2D case will be rapidly reviewed in section 4.5.8.
4.5.1. Kalman ﬁltering
For the applications that interest us here, the Kalman ﬁlter equations are based on
the following state-space representation:

xn+1 = Fn xn + Gn un ,
yn
= Hn xn + bn ,
n = 1, 2, . . . ,
(4.25)
6. Otherwise, an item-by-item iterative technique can be used as in section 2.2.2.

Inverse Filtering and Other Linear Methods
103
in which the observation yn is scalar; matched with the data for the moments of order 1
and 2:
E
!un
bn
"
= 0 ,
E(x0) = mx
0 ,
E
⎛
⎝
⎡
⎣
x0 −mx
0
un
bn
⎤
⎦
&
(x0 −mx
0 )T , uT
n, bn
'
⎞
⎠=
⎡
⎣
Rx
0
0
0
0
Ru
n 0
0
0
rb
n
⎤
⎦.
In a Bayesian interpretation, as chosen here, the Kalman ﬁlter enables us to recur-
sively compute the mean and the covariance matrix of the a posteriori probability
distribution of the state vector xn, which is normal, knowing the data y1, . . . , ym.
These are denoted xn|m and Rx
n|m respectively. Strictly speaking, Kalman “ﬁltering”
corresponds to the case where m = n. When m > n, we tend to speak of “Kalman
smoothing” and we have two possibilities:
– the ﬁxed-lag smoother which calculates xn|n+p for any n;
– the ﬁxed interval smoother, which calculates xn|N for any n ≤N in a given
time interval [1, N]; this solution corresponds rigorously to batch processing of the
data.
Below, we recall the recurrence equations of the ﬁlter then the smoother. For a demon-
stration of these equations, the reader is invited to consult the references [AND 79,
JAZ 70, VAN 68].
For the so-called “covariance” form chosen here, the ﬁlter recurrence repeatedly
links the two following operations at each instant, in the sense of increasing time
(n = 1, 2, . . . , N):
– one-step prediction:
xn|n−1 = Fn−1 xn−1|n−1
Rx
n|n−1 = Fn−1 Rx
n−1|n−1 FT
n−1 + Gn−1 Ru
n−1 GT
n−1
– correction:
re
n = Hn Rx
n|n−1 HT
n + rb
n
kn = Rx
n|n−1 HT
n (re
n)−1
(4.26)
xn|n = xn|n−1 + kn (yn −Hn xn|n−1)
Rx
n|n = (I −kn Hn) Rx
n|n−1
When it is not necessary to calculate the two means and the two covariance matri-
ces explicitly at each instant, these two sets of equations can be merged into one.

104
Bayesian Approach to Inverse Problems
Kalman smoothing consists of calculating the estimates xN−1|N, ..., x1|N, in a
backward recursive procedure starting from the estimates xn|n obtained in the forward
processing with the conventional Kalman ﬁlter, and initializing the procedure with
xN|N. The recurrence equations of the Kalman smoother are the following:
– update of the mean:
xn|N = xn|n + Sn
xn+1|N −Fn xn|n

Sn = Rx
n|n FT
n (Rx
n+1|n)−1
– update of the covariance:
Rx
n|N = Rx
n|n + Sn (Rx
n+1|N −Rx
n+1|n) ST
n
A few observations can be made:
– we are indeed concerned with smoothing the estimators coming from the forward
procedure, which is performed according to the state evolution model. Neither the data
nor the observation model come into these equations; the smoother only requires the
ﬁltered estimates xn|n and the covariances Rx
n|n and Rx
n+1|n;
– calculating the smoothing gain Sn means inverting matrix Rx
n+1|n which is of
the order of the state dimension. The additional cost of the smoothing is thus largely
dependent on the a priori model chosen. For an autoregressive (AR) model of order
1, such as the random walk model (4.29) used below, we can use scalar equations;
– as in the ﬁltering case, there is nothing to stop us using the smoother to treat
non-stationary state models.
For an application of Kalman ﬁltering to deconvolution, it is clear that the ob-
servation equation of state model (4.25) must be linked, at least partially, with the
initial discrete convolution (4.11). However, there are several ways of working which,
combined with different choices for xn, Fn and Gn, lead to a variety of solutions.
4.5.2. Degenerate state model and recursive least squares
A careful look at equation (4.12) shows that a ﬁrst possible choice for the state
evolution equation is:
xn+1 = xn = x =

x−P +1, x−P +2, . . . , xN+Q
T,
(4.27)
which entails:
Fn = I ,
Gn = 0 ,
Hn =

0, . . . , 0, hP , . . . , h0, . . . , h−Q, 0, . . . , 0

.
This model is thus degenerate, as the state vector is static and the Kalman ﬁlter is
initialized simply by choosing x0|0 = mx
0 and Rx
0|0 = Rx
0 ∝Q−1. It is easy to

Inverse Filtering and Other Linear Methods
105
verify that there is no reason to distinguish between one-step prediction and ﬁltering
(xn+1|n = xn|n and Rx
n+1|n = Rx
n|n) and that the smoothing stage is not useful
because the last recursion in the forward time direction gives exactly the result of
batch processing: xN|N and Rx
N|N. The recursive equations obtained:
kn = Rx
n−1|n−1 HT
n (Hn Rx
n−1|n−1 HT
n + rb
n)−1
xn|n = xn−1|n−1 + kn (yn −Hn xn−1|n−1)
(4.28)
Rx
n|n = (I −kn Hn) Rx
n−1|n−1
are in fact those of a recursive least squares algorithm for solving the regularized
least squares problem (4.26) one data-item at a time. The main characteristics of this
algorithm are the following:
– no boundary hypothesis is introduced, unlike in the fast batch inversion algo-
rithms described earlier;
– the innovation and its variance are scalar, so there is no matrix inversion;
– a smoothed estimate is obtained on-line, without the need for a backward pass,
as mentioned above;
– the exact computing cost depends on the composition of the a priori covariance
matrix Rx
0 , but is in O(N 2) per recursion, i.e., O(N 3) in total, as for inversion of a
matrix of dimension N.
This algorithmic solution is thus of little practical interest. To reduce the comput-
ing and memory load at each recursion, there are two possible paths: 1) reduce the
dimension of the state vector by other choices of matrices Fn, Gn and Hn, as in sec-
tions 4.5.3 and 4.5.6 below, or 2) take advantage of the shift invariance property of
matrix Hn of the degenerate state model above so as to avoid having to solve a Riccati
equation to calculate gain vector kn at each recursion, as in section 4.5.4 below.
4.5.3. Autoregressive state model
The signal to be deconvolved is often modeled in the form of an AR signal of
order L:
xn =
L

ℓ=1
aℓxn−ℓ+ un .
(4.29)
In this section, we will restrict ourselves to causal systems (Q = 0); extending to gen-
eral FIR systems is easy. By deﬁning the state vector as xn = [xn, . . . , xn−K+1]T, of
dimension K ≥max(P + 1, L) and deﬁning the vector of the regression coefﬁcients
as a = [a1, . . . , aL]T, we can write the state evolution equation in form (4.25) on

106
Bayesian Approach to Inverse Problems
condition that we choose matrices Fn and Gn in the following way:
Fn = F =
! [aT , 01×(K−L)] 0
IK×K0K×1
"
,
(4.30)
and
Gn = G =

1, 0, . . . , 0
T .
(4.31)
The state evolution noise un is thus the non-correlated generating process un which
is scalar.
To complete the modeling, it remains to deﬁne matrix H in the form
H = hT = [h0, . . . , hP , 01×(K−P −1)], a line-vector of dimensions 1 × K. The
corresponding state equation (4.25) is called the snake chain model, or model in com-
panion form according to the ﬁeld of application (communications, automatic sys-
tems, etc.)7.
Kalman ﬁlter (4.26) applied to this model gives the vector: xn|n =

x(n | n),
x(n −1 | n), . . . , x(n −K + 1 | n)
T at each recursion. Its coordinates are all es-
timators of the signal to be restored xn, obtained by ﬁxed-lag smoothing for delays
up to K −1. If this smoothing is sufﬁcient to ensure an acceptable mean quadratic
error, a smoothed solution {x(n −p | n)}n=1, 2,... (0 ≤p ≤K −1) is obtained in a
single pass by extracting the appropriate coordinates from the succession of ﬁltered
vectors xn|n. If not, it is necessary to use a ﬁxed-interval smoother, which requires an
additional, backward pass. We will see more about this later, in an example.
4.5.3.1. Initialization
If the second-order prior information available on the signal xn is composed of
the correlation function rx
n, a factorization algorithm such as Levinson’s gives vector
a and the variance of the generating process ru. It is also possible to choose the
regression coefﬁcients directly. Among the choices made by various authors, it is
interesting to note:
– the white noise model (aℓ= 0, 1 ≤ℓ≤L), used, for example, in geo-
physics [CRU 74];
– the random walk model (a1 = 1 and aℓ= 0, 2 ≤ℓ≤L), which is non-
stationary but does not lead to divergence of the ﬁlter if the mean of the signal is ob-
served. It is then a convenient choice from the regularization point of view [COM 84].
As far as implementation itself is concerned, it consists of choosing vector mx
0 and
initial covariance matrix Rx
0 . The latter can be immediately calculated from rx
n and
7. This model is different from that used in predictive deconvolution [ROB 54], where it is the
observed signal yn that is taken to be autoregressive. The transfer function of the instrument is
thus “all-pole” and no longer FIR. A myopic inversion is then performed, but the signal to be
restored (the process generating the AR model) is non-correlated and the phase of the estimated
transfer function is minimum, unless there are speciﬁc constraints (see also section 9.4.1).

Inverse Filtering and Other Linear Methods
107
can also be deduced from a and ru by an inverse Levinson algorithm. For mx
0 , how-
ever, the choice is more delicate: obviously, the true signal is not known for n ≤0.
We ﬁnd again, here, the questions of boundary choices encountered in batch process-
ing. Note that, for a stationary model, inexact initialization with a zero-mean white
vector with a large variance is often enough.
4.5.3.2. Criterion minimized by Kalman smoother
In practice, we often content ourselves with carrying out ﬁxed-lag smoothing in
a single pass. However, the solution {x(n −p | n)}n=1, 2,... thus obtained is not the
minimizer of the regularized criterion (4.19) and it is important to state the underlying
criterion. For this, let us examine the estimations made at instants 0 and 1 for an
invariant state model such as equation (4.25). Let p(x0) be the a priori pdf for x0, a
Gaussian with mean mx
0 and covariance Rx
0 . At instant 0, we measure y0, for which
the direct law has the density:
p(y0) ∝exp
	
−1
2rb (y0 −hT x0)2
.
Estimator x0|0 maximizes the conditional law p(x0 | y0) ∝p(y0) p(x0), and thus
minimizes the quadratic criterion:
J0(x0) = 1
rb (y0 −hT x0)2 + ∥x0 −mx
0 ∥2
(Rx
0 )−1 .
At instant 1, the conditional law p(x1 | y0, y1) is the marginal of the joint law
p(x0, x1 | y0, y1) which can be factorized as follows:
p(x0, x1 | y0, y1) = p(y0 | x0) p(y1 | x1) p(x1 | x0) p(x0) ,
and, in the same way, corresponds to the following quadratic criterion:
J1(x0, x1) = 1
rb
1

k=0
(yk −hTxk)2+∥x1 −F x0∥2
(GRuGT )−1 +∥x0 −mx
0 ∥2
(Rx
0 )−1
The minimum of this criterion is reached for the pair of vectors {x(1)
0 , x(1)
1 }, such
that x(1)
0
= x0|1 and x(1)
1
= x1|1. Let us generalize this procedure: at instant n, we
deﬁne the joint criterion:
Jn(x0, . . . , xn)
= 1
rb
n

k=0
(yk −hT xk)2 +
n

k=1
∥xk −Fxk−1∥2
(GRuGT )−1 + ∥x0 −mx
0 ∥2
(Rx
0 )−1
which has its minimum for the (n + 1)-uplet of vectors
x(n)
0 , . . . , x(n)
n

. We note
that the estimator xn|n produced by the Kalman ﬁlter at instant n is x(n)
n . Moreover,

108
Bayesian Approach to Inverse Problems
the n + 1 estimators of the ﬁxed-interval Kalman smoother x0|n, . . . , xn|n are the
n + 1 vectors that minimize Jn.
The above results do not apply directly to the a priori AR model (4.29) above, as
matrix G Ru GT is not invertible with the choice made for G in (4.31). To reach
criterion Jn, we replace it in the previous criterion by a diagonal matrix having the
diagonal elements {ru, ε2, . . . , ε2}. By making ε tend towards 0, we ﬁnd that two
consecutive estimated vectors xm−1|n and xm|n must have K −1 equal components,
in agreement with the state evolution equation. We thus deﬁne the deconvolved vector
xKS
n as the concatenation of the ﬁrst components of each vector xm|n, and show that
this vector minimizes the following criterion:
J
KS(xn) = 1
rb ∥y −H x∥2 + 1
ru
n

m=1

xm −
L

ℓ=0
aℓxm−ℓ
2
+ 1
ru x2
0,
(4.32)
where H is the observation block-matrix. The initialization term comes from the
choice of the initial a priori moments.
The above criterion is clearly connected with the quadratic regularized criteria of
the beginning of the chapter. In particular, choosing a random walk model (a1 = 1
and aℓ= 0, ℓ≥2) corresponds to regularizing the ﬁrst differences of the signal (equa-
tion (4.17) with α0 = 0). Other choices correspond to more general regularaization
terms, see for example Chapter 11.
4.5.3.3. Example of result
Figure 4.10 illustrates the results that can be obtained with these on-line deconvo-
lution techniques for the example of an MA ﬁlter and with the random walk model.
The ﬁxed-interval smoothing solution, which minimizes J KS, is comparable to the re-
sults of the batch processing presented in Figure 4.4. The solutions by simple Kalman
ﬁltering, or by ﬁxed-lag smoothing, give a greater estimation variance than the ﬁxed-
interval smoothing, thus inducing a higher relative quadratic error. The second plot on
the ﬁgure shows the variation of this error for increasing lag and demonstrates clearly
that, in this example, a large lag is necessary if it is to come close to the error of the
ﬁxed-interval smoother.
4.5.4. Fast Kalman ﬁltering
The on-line deconvolution by the two variants of Kalman ﬁltering described above
does not reduce the computing cost. They are methods in O(N 3). The reason for
this is that they use the standard equations (4.26), which are too general because they
apply to non-stationary problems whereas, in this chapter, the observation model is
invariant and the signals are stationary.

Inverse Filtering and Other Linear Methods
109
50
100
150
200
250
300
350
400
−4
−2
0
2
4
0
10
20
30
40
50
60
70
20
25
30
35
Figure 4.10. Results of on-line deconvolution of a 1D signal. Top, the actual input is
represented by a dashed line, the result of Kalman ﬁltering is shown as a dotted line, and the
ﬁxed-interval smoother result as a solid line. Bottom, variation of the relative quadratic error
for increasing lags in the ﬁxed-lag smoother is plotted as a solid line and the error of the
ﬁxed-interval smoother is shown as a dotted line. The SNR is 20 dB
When model (4.25) is stationary (i.e., when F, G, H, Ru and rb do not depend
on n), it is possible to markedly reduce the computing load by making the recursion
act on increments of the nominal quantities of the ﬁlter Rx
n|n−1 and kn rather than on
the quantities themselves [DEM 89].
Thus, in the case of the AR model (4.30-4.31), we can show [COM 84] that this
recursion can also be written:
!
(re
n+1)1/2
0
kn+1(re
n+1)−1/2 vn+1
"
=
!
(re
n)1/2
sn
kn(re
n)−1/2 Fvn
"
Θn ,
(4.33)
where sn and vn are auxiliary quantities deﬁned at initialization and Θn is a J-
orthogonal transformation matrix8. With a random walk model, this algorithm only
requires 5 (P + Q + 1) scalar multiplications per recursion and thus has a total
cost in O(N(P + Q + 1)). However, the nature of transformation Θn shows that
8. A J-orthogonal transformation is such that Θn J Θn
T = J, where J is a signature matrix,
i.e., a diagonal matrix in which the diagonal elements have the values + 1 or −1. It does
not preserve the Euclidian norm, unlike orthogonal transformations, and we lose a means of
controlling the propagation of numerical errors.

110
Bayesian Approach to Inverse Problems
this gain itself has a price: this type of algorithm has a potential numerical insta-
bility and it is not enough to use a square root form (4.33) as is usually recom-
mended [AND 79, VER 86]. The choice of the corresponding hyperbolic rotation
is very important [LEB 93].
In the case of a degenerate model (4.27), we are no longer dealing with a strictly
invariant model but a shift-invariant one (since we move from Hn to Hn+1 by shift-
ing the coordinates). The technique described above is generalized by changing the
deﬁnition of the increments in the nominal quantities of the ﬁlter [DEM 85]. The
corresponding algorithm this time needs n + P + Q + 1 scalar multiplications per re-
cursion, giving a total cost in O(N 2), which is higher than the previous one. However,
it should be remembered that, in a single pass, we obtain the ﬁxed-interval smoother
result without boundary hypotheses.
These fast ﬁltering techniques and the asymptotic ﬁltering results below are cur-
rently used essentially for on-line, real-time processing. The implementation prob-
lems raised come under IT engineering and are beyond the scope of this book. Inter-
ested readers are encouraged to refer to [MAS 99, MOZ 99].
4.5.5. Asymptotic techniques in the stationary case
4.5.5.1. Asymptotic Kalman ﬁltering
In the stationary case, it is well known that the covariance matrix Rx
n|n that in-
tervenes in the ﬁlter tends, when n →∞, towards a ﬁxed matrix R∞that veriﬁes a
Riccati equation of the form:
R∞= F R∞FT −F R∞H (H R∞HT + rb)−1 + G Ru GT .
(4.34)
The Kalman gain kn then tends towards a constant vector and, as the calculation of
these two quantities does not bring in the observed data yn, the idea arose, as soon as
Kalman ﬁltering was invented, of calculating these asymptotic quantities in advance
and carrying out invariant recursive ﬁltering:
xn|n = F xn−1|n−1 + k (yn −H F xn|n−1) ,
in order to reduce the computing cost of a recursion.
The asymptotic gain k nevertheless remains to be calculated and the stability of
the recursive ﬁlter obtained has to be veriﬁed. Abundant literature has been devoted
to these questions [AND 79]. Equation (4.34) can be solved by a wide variety of
methods, iterative or not, among which Chandrasekhar factorization methods, simple
or dual, are particularly suited to invariant models in the broad sense and to varied
initial conditions [DU 87].

Inverse Filtering and Other Linear Methods
111
4.5.5.2. Small kernel Wiener ﬁlter
Asymptotic Kalman ﬁltering provides an optimum causal solution at low cost.
However, in many problems, particularly in 2D, it appears more advantageous to use a
local solution taking into account the past and future of the instant under consideration.
A non-causal Wiener FIR estimator, or small kernel Wiener ﬁlter, can thus be chosen:
xn =
J

j=−J
gj yn−j
the optimality of which is deﬁned locally by considering a sub-set of the Wiener-Hopf
equations (3.15) in discrete time. This leads to a matrix system Ry g = rxy, where
Ry is the covariance matrix of the process y and rxy the vector of the input/output
covariances for lags {−J, . . ., 0, . . . , J}. These quantities are easily expressed as
functions of Rx, Rb and h.
This approach has been proposed for handling 3D problems [PER 97] and for
restoring large sized images in satellite imaging [REI 95] – although we are not deal-
ing with a simple deconvolution problem in the latter case.
4.5.6. ARMA model and non-standard Kalman ﬁltering
When the input to be restored is a priori non-correlated, another means of reducing
the computing load of a Kalman ﬁlter is to abandon the idea of modeling the direct
problem by the discrete convolution used since the beginning of this chapter and to
replace it with a minimal realization, i.e., a minimum order ARMA model, identiﬁed
from the IR of the system [MEN 83]. The coefﬁcients of the numerator of the transfer
function obtained (MA part) serve to construct matrix H of the state model and those
of the denominator (AR part) matrix F. The signal to be restored then becomes the
noise, un, of the state evolution, which leads to a non-standard Kalman ﬁltering or
smoothing problem. This method, introduced for seismic reﬂection problems, has
the advantage of being able to deal with non-stationary deconvolution problems for a
reasonable computing cost.
4.5.7. Case of non-stationary signals
The principle of extending Kalman ﬁltering to the non-stationary context (variable
a priori model, variable response h, variable noise variance) poses no problem in as far
as the time laws of all the parameters are sufﬁciently well known for the state model to
be completely deﬁned (4.25). In practice, however, non-stationarity is often associated
with a poor knowledge of these laws and we are faced with a myopic problem, having
a large number of parameters to be determined in addition to the samples of the input
signal. This question will be tackled in Chapter 8.

112
Bayesian Approach to Inverse Problems
4.5.8. On-line processing: 2D case
The use of Kalman ﬁltering for 2D deconvolution was put forward in the 1970s.
Once the method of scanning the image, usually in lexicographic order, has been cho-
sen, the problem is formally a simple extension of the Kalman ﬁltering seen above.
Practically, it is difﬁcult to reconcile this scanning mode with satisfactory proba-
bilistic modeling of the image because of the absence of a 2D spectral factoriza-
tion theorem. We are led to choose a very large state vector, which considerably
increases the complexity of the algorithm. Various approximations have been used
to reduce the calculations.
Among them, we note the “RUM” (Reduced Update
Model) [WOO 81] then “ROM” (Reduced Order Model) versions [ANG 89]. At the
same time, fast versions of the Kalman ﬁlter were used for stationary problems; see,
for example, [MAH 87, SAI 85, SAI 87]. Added to these difﬁculties is the handling
of edge conditions, which can greatly complicate implementation.
It can be considered that this type of algorithm has practically fallen into disuse
in image restoration. Most problems of reasonable size (up to 106 pixels) can be
batch processed with the solutions presented in the ﬁrst part of this chapter. For very
large-sized problems (3D problems and 2D problems with 108 pixels), we can content
ourselves with small kernel ﬁltering based on stationary modeling of the problem as
presented in section 4.5.5.
4.6. Conclusion
The deconvolution methods introduced in this chapter are fundamental for several
reasons. First of all, many physical phenomena can be modeled, at least to a ﬁrst ap-
proximation, by convolution. Secondly, this direct model can be interpreted easily in
the frequency domain, which enables an extensive, intuitive analysis of the phenom-
ena and difﬁculties encountered. Finally, the inversion methods proposed are based on
a priori models that remain simple: quadratic regularization terms or Gaussian a pri-
ori laws, deﬁned by their second-order properties. We thus arrive at estimators that are
very simple, as they are linear, and we can thus turn our attention to questions of im-
plementation: block methods (Hunt, preconditioned gradient) and recursive methods
(Kalman ﬁltering and smoothing).
In return, there are limitations in terms of resolution of the solutions obtained.
The methods obtained perform partial spectral equalization, i.e., they only manage to
compensate for certain attenuations in the sensor bandwidth; they do, however, avoid
an explosive restitution of the frequencies that are too strongly attenuated. Thus, the
resolution of the linear solutions is fundamentally limited by the spectral content of
the data.
Any increase in resolution (apart from improvements to the measuring systems,
which is outside the scope of this book) relies on taking into account more speciﬁc

Inverse Filtering and Other Linear Methods
113
information on the object to be reconstructed: positivity, pulse nature (see Chapter 5),
or presence of contours in an image (see Chapter 6) for example. In other domains,
such as digital communications, the fact that the input parameters belong to a ﬁnite
alphabet can contribute to the same result.
4.7. Bibliography
[AND 79] ANDERSON B. D. O., MOORE J. B., Optimal Filtering, Prentice-Hall, Englewood
Cliffs, NJ, 1979.
[ANG 89] ANGWIN D. L., KAUFMAN H., “Image restoration using reduced order models”,
Signal Processing, vol. 16, p. 21-28, 1989.
[ARS 66] ARSAC J., Fourier Transform and the Theory of Distributions, Prentice-Hall, En-
glewood Cliffs, NJ, 1966.
[BAY 70] BAYLESS J. W., BRIGHAM E. O., “Application of the Kalman ﬁlter to continuous
signal restoration”, Geophysics, vol. 35, num. 1, p. 2-23, 1970.
[CHA 88] CHAN R. H., “An optimal circulant preconditionner for Toeplitz systems”, SIAM
J. Sci. Comput., vol. 9, p. 766-771, 1988.
[CHA 93] CHAN R. H., NAGY J. G., PLEMMONS R. J., “FFT-based preconditionners for
Toeplitz-block least squares problems”, SIAM J. Num. Anal., vol. 30, num. 6, p. 1740-
1768, Dec. 1993.
[CHA 96] CHAN R. H., NG M. K., “Conjugate gradient methods for Toeplitz systems”, SIAM
Rev., vol. 38, num. 3, p. 427-482, Sep. 1996.
[COM 84] COMMENGES D., “The deconvolution problem: fast algorithms including the pre-
conditioned conjugate-gradient to compute a MAP estimator”,
IEEE Trans. Automat.
Contr., vol. AC-29, p. 229-243, 1984.
[CRU 74] CRUMP N. D., “A Kalman ﬁlter approach to the deconvolution of seismic signals”,
Geophysics, vol. 39, p. 1-13, 1974.
[DEM 85] DEMOMENT G., REYNAUD R., “Fast minimum-variance deconvolution”, IEEE
Trans. Acoust. Speech, Signal Processing, vol. ASSP-33, p. 1324-1326, 1985.
[DEM 89] DEMOMENT G., “Equations de Chandrasekhar et algorithmes rapides pour le traite-
ment du signal et des images”, Traitement du Signal, vol. 6, p. 103-115, 1989.
[DU 87] DU X.-C., SAINT-FELIX D., DEMOMENT G., “Comparison between a factorization
method and a partitioning method to derive invariant Kalman ﬁlters for fast image restora-
tion”, in DURRANI T. S., ABBIS J. B., HUDSON J. E., MADAN R. N., MCWHIRTER
J. G., MOORE T. A. (Eds.), Mathematics in Signal Processing, p. 349-362, Clarendon
Press, Oxford, UK, 1987.
[FES 99] FESSLER J. A., BOOTH S. D., “Conjugate-gradient preconditionning methods for
shift-variant PET image reconstruction”, IEEE Trans. Image Processing, vol. 8, num. 5,
p. 668-699, May 1999.
[GOL 96] GOLUB G. H., VAN LOAN C. F., Matrix Computations, John Hopkins University
Press, Baltimore, 3rd edition, 1996.

114
Bayesian Approach to Inverse Problems
[GRE 58] GRENANDER U., SZEGÖ G., Toeplitz Forms and their Applications, University of
California Press, Berkeley, 1958.
[HUN 71] HUNT B. R., “A matrix theory proof of the discrete convolution theorem”, IEEE
Trans. Automat. Contr., vol. AC-19, p. 285-288, 1971.
[HUN 73] HUNT B. R., “The application of constrained least squares estimation to image
restoration by digital computer”, IEEE Trans. Computers, vol. C-22, p. 805-812, 1973.
[HUN 76] HUNT B. R., CANNON T. M., “Nonstationary assumptions for Gaussian models of
images”, IEEE Trans. Systems, Man, Cybern., p. 876-882, Dec. 1976.
[JAI 89] JAIN A., Fundamental of Digital Image Processing, Prentice-Hall, Englewood Cliffs,
NJ, 1989.
[JAZ 70] JAZWINSKI A. H., Stochastic Process and Filtering Theory, Academic Press, New
York, NY, 1970.
[KAL 60] KALMAN R. E., “A new approach to linear ﬁltering and prediction problems”, J.
Basic Engng., vol. 82-D, p. 35-45, 1960.
[KHU 77] KHURGIN Y. I., YAKOVLEV V. P., “Progress in the Soviet Union on the theory and
applications of bandlimited functions”, Proc. IEEE, vol. 65, p. 1005-1029, 1977.
[LEB 93] LE BESNERAIS G., GOUSSARD Y., “Improved square-root forms of fast linear least
squares estimation algorithms”, IEEE Trans. Signal Processing, vol. 41, num. 3, p. 1415-
1421, Mar. 1993.
[LEV 47] LEVINSON N., “The Wiener RMS error criterion in ﬁlter design and prediction”, J.
Math. Physics, vol. 25, p. 261-278, Jan. 1947.
[MAH 87] MAHALANABIS A.-K., XUE K., “An efﬁcient two-dimensionnal Chandrasekhar
ﬁlter for restoration of images degraded by spatial blur and noise”, IEEE Trans. Acoust.
Speech, Signal Processing, vol. 35, p. 1603-1610, 1987.
[MAS 99] MASSICOTTE D., “A parallel VLSI architecture of Kalman-ﬁlter-based algorithms
for signal reconstruction”, Integration VLSI J., vol. 28, p. 185-196, 1999.
[MEN 83] MENDEL J. M., Optimal Seismic Deconvolution, Academic Press, New York, NY,
1983.
[MOR 80] MORF M., “Doubling algorithms for Toeplitz and related equations”, in Proc. IEEE
ICASSP, Denver, CO, p. 954-959, 1980.
[MOZ 99] MOZIPO A., MASSICOTTE D., QUINTON P., RISSET T., “A parallel architecture
for adaptive channel equalization based on Kalman ﬁlter using MMAlpha”, in Proc. IEEE
Canadian Conf. on Electrical and Computer Engng., Alberta, Canada, p. 554-559, May
1999.
[NAG 96] NAGY J. G., PLEMMONS R. J., TORGENSEN T., “Iterative image restoration using
approximate inverse preconditionning”, IEEE Trans. Image Processing, vol. 5, num. 7,
p. 1151-1162, July 1996.
[NG 99] NG M. K., CHAN R. H., TANG W.-C., “A fast algorithm for deblurring models with
Neumann boundary conditions”, SIAM J. Sci. Comput., vol. 21, num. 3, p. 851-866, 1999.

Inverse Filtering and Other Linear Methods
115
[PER 97] PEREIRA S., GOUSSARD Y., “Unsupervised 3-D restoration of tomographic images
by constrained Wiener ﬁltering”, in Proc. IEEE EMB Conf., Chicago, IL, p. 557-560, 1997.
[REI 95] REICHENBACH S. E., KOEHLER D. E., STRELOW D. W., “Restoration and recon-
struction of AVHRR images”, IEEE Trans. Geosci. Remote Sensing, vol. 33, num. 4, p. 997-
1007, July 1995.
[ROB 54] ROBINSON E. A., “Predictive decomposition of seismic traces”,
Geophysics,
vol. 27, p. 767-778, 1954.
[SAI 85] SAINT-FELIX D., HERMENT A., DU X.-C., “Fast deconvolution: application to
acoustical imaging”, in J.M. THIJSSEN, V. MASSEO (Eds.), Ultrasonic Tissue Characteri-
zation and Echographic Imaging, Nijmegen, The Netherlands, Faculty of Medicine Printing
Ofﬁce, p. 161-172, 1985.
[SAI 87] SAINT-FELIX D., DU X.-C., DEMOMENT G., “Filtres de Kalman 2D rapides à mod-
èle d’état non causal pour la restauration d’image”, Traitement du Signal, vol. 4, p. 399-410,
1987.
[VAN 68] VAN TREES H. L., Detection, Estimation and Modulation Theory, Part 1, John
Wiley, New York, NY, 1968.
[VER 86] VERHAEGEN M., VAN DOOREN P., “Numerical aspects of different Kalman ﬁlter
implementations”, IEEE Trans. Automat. Contr., vol. AC-31, num. 10, p. 907-917, Oct.
1986.
[WIE 71] WIENER N., Cybernétique et société, Union générale d’édition, Paris, 1971.
[WOO 81] WOODS J. W., INGLE V. K., “Kalman ﬁltering in two dimensions: further results”,
IEEE Trans. Acoust. Speech, Signal Processing, vol. 29, num. 2, p. 568-577, Apr. 1981.


Chapter 5
Deconvolution of Spike Trains
5.1. Introduction
A point source is the idealization of a physical source that has a duration or dimen-
sion markedly smaller than the resolution of the sensor observing it. In practice, this
type of source is found in signals or images observed in the form of an echo or spot
whose shape is essentially a characteristic of the instrument: impulse response (IR).
In these conditions, the only features of the observed signal that can be attributed to
the source are its location and amplitude.
This type of source is encountered in astronomy, in stellar imaging, when the ap-
parent diameter of the stars imaged is smaller than the parameter λ/D, where λ is
the wavelength and D the diameter of the receiving antenna (see Chapter 10, sec-
tion 10.1.2). It is also found in ultrasound echography or seismology whenever the
scale of an inhomogeneity or transition is small with respect to the wavelength used
(see Chapter 9).
The detection and location of a single source can be satisfactorily handled by
“matched ﬁlter” techniques [VAN 68]. By extension, the matched ﬁlter will work
when the “echoes” due to the sources are well separated from one another. As soon as
the echoes due to several sources overlap signiﬁcantly, matched ﬁltering is no longer
effective.
In such a context, spike train deconvolution aims to process an observed signal
made up of overlapping echoes in order to extract the position and amplitude of the
Chapter written by Frédéric CHAMPAGNAT, Yves GOUSSARD, Stéphane GAUTIER and Jérôme
IDIER.

118
Bayesian Approach to Inverse Problems
point sources that generated the echoes. It corresponds to the observation model
y(t) = K
k=1 rkh(t −tk) + b(t), in which K is the number of sources, rk the am-
plitude of the kth source (in echography, this quantity represents the reﬂectivity of
the medium, whence the traditional use of the variable r to represent this quantity),
tk is the time of arrival of the signal due to the kth source, h is the IR of the instru-
ment, which is assumed to be known in the framework of this chapter, and b is the
noise, which includes everything we do not intend to model deterministically. The
noise is almost always assumed to be Gaussian and, in most cases, white, zero-mean
and stationary. The developments that follow will use these hypotheses unless stated
otherwise. A more realistic formulation of the problem takes the sampled and ﬁnite
nature of the observations into consideration:
y(nT ) =
K

k=1
rkh(nT −tk) + b(nT ), n = 0, . . . , N −1,
(5.1)
where T is the signal sampling period. In this form, the problem can be tackled from
the standpoint of identifying a parameteric model, the order, K, of which is unknown
and must be estimated [WAL 97].
A noteworthy contribution adopting this approach has been made by [KWA 80],
taking his inspiration from the CLEAN technique [HOG 74], which can be qualiﬁed as
an “iterative matched ﬁlter”. In this respect, the impulse positioning algorithm called
multipulse, which appeared in speech coding [ATA 82], works on a similar principle
(but in a noise-free context) where h is estimated by linear prediction (see Chapter 9,
section 9.4.1.3).
The same type of model has been much studied in the context of spectral ray
analysis, which corresponds to a model structure close to equation (5.1) [DJU 96,
DUB 97, STO 89, WON 92]. These approaches lead to a very tricky, non-convex
optimization problem, one of the difﬁculties of which is the unknown size, K, of the
space of parameters to be identiﬁed.
The approaches developed below get around this difﬁculty by considerably sim-
plifying model (5.1). In practice this means substantially reducing the algorithmic
complexity. The time of arrival space is discretized with a sampling step T and it is
thus possible to rewrite (5.1) in the form:
y(nT ) =

m
rmh((n −m)T ) + b(nT ), n = 0, . . . , N −1,
(5.2)
where we assume rm = 0 if mT ̸= tk, ∀k. With the notation yn = y(nT ) and
hn = h(nT ), (5.2) gives the invariant linear model y = h ⋆r + b (4.11) of Chapter 4.
With this model, the estimation of K, of the times of arrival and of the amplitudes is
transformed into a dual problem. At each instant m we need to estimate the ampli-
tude of a pulse that may (or may not) be located at m. Thus, estimating the number

Deconvolution of Spike Trains
119
of pulses and the positions of the times of arrival merges with the “detection” of the
presence or absence (rm = 0) of a pulse at each instant m. In the signal process-
ing community, this problem is referred to as a detection-estimation problem. It is
the detection operation that makes the corresponding optimization problem globally
nonlinear.
In what follows, we will only consider a matrix version of (5.2):
y = Hr + b
(5.3)
where y, b and r are vectors containing, respectively, N samples of y and of b, and the
corresponding M samples of r. M depends on N, on the number of samples used to
represent the IR, and on the boundary assumption used (see section 4.3.3). The time-
shift invariance implicit in equation (5.2) is expressed in matrix model (5.3) by the
Toeplitz structure of H, the N × M convolution matrix constructed from the samples
of h.
The aim of spike deconvolution is to estimate r. We will look at two families of
approaches:
1) those that place the emphasis on the estimation aspect, by deferring the deci-
sional aspect. In this context, the deconvolution operation is considered as a contrast
enhancement technique. The natural framework for presenting these techniques is
deterministic regularization;
2) those that consider the detection-estimation problem right from the start and
seek to solve detection and estimation together. The traditional, and probably most
appropriate, framework for setting out these techniques is the Bayesian one, with
Bernoulli-Gaussian (BG) priors.
Whatever the family and the interpretation framework, these approaches all result in
an optimization problem. The ﬁrst family leads to techniques of nonlinear optimiza-
tion on
 M, while the second brings in combinatorial optimization on the discrete
space {0, 1}M.
5.2. Penalization of reﬂectivities, L2LP/L2Hy deconvolutions
The synthetic example of Figure 5.1 illustrates the failure of the trivial matched ﬁl-
ter and least squares solutions on equation (5.3). The spike train signal to be estimated
is the Mendel sequence proposed in [KOR 82] and often used since as a benchmark.
It is depicted in panel (a) of Figure 5.1. The IR is represented in (c). The output (b) is
corrupted by additive noise, with a signal-to-noise ratio (SNR, i.e., the mean empirical
power of h ⋆r over variance of the noise) of 10 dB. (e) shows the result of a matched
ﬁltering, i.e., ﬁltering of observations y (continuous line) by the time-reverse of h,
followed by thresholding at 0.05 (dotted lines) giving the detections marked by ×s. It

120
Bayesian Approach to Inverse Problems
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(a) Mendel sequence
0
50
100
150
200
250
300
−0.15
−0.1
−0.05
0
0.05
0.1
(b) noisy ﬁltered signal
0
5
10
15
20
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
(c) IR
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
10
−3
10
−2
10
−1
10
0
10
1
(d) IR spectrum
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(e) matched ﬁlter solution
0
50
100
150
200
250
300
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
(f) least squares solution
Figure 5.1. Synthetic ﬁlter based on Mendel sequence (a), synthetic spike train signal
introduced in [KOR 82]. This signal is ﬁltered by the IR (c), additive Gaussian noise of
variance rb = 5.4.10−5 is added to give the signal (b). The crosses of (e) represent the spikes
detected by a matched ﬁlter followed by thresholding, while the circles mark the spikes to be
found. The least-squares solution (f) is worthless for detection purposes

Deconvolution of Spike Trains
121
turns out that no threshold gives acceptable false alarm and correct detection rates at
the same time. Furthermore, the least-squares solution (f) is unusable. This is a result
of the limited bandwidth of the IR (d), when we are trying to ﬁnd a broadband signal.
Following the methodology presented in Chapter 2, we want to add prior informa-
tion about the rarity of non-zero samples on to input r. Since the solution is assumed
to be broadband, no interaction between neighboring or distant samples is introduced
in our priors. In a deterministic regularization framework, this leads us to look for
solutions as minimizers of a penalized least squares criterion of the form:
J(r, μ) = ∥y −Hr∥2 + μ

m
φ(rm),
(5.4)
in which the regularization term can be decomposed into a sum of monovariate func-
tions to express the a priori independence of samples.
The difﬁculty is to specify function φ to fully characterize the type of regulariza-
tion. φ needs to be chosen in a way that favors values close to zero while, at the same
time, exceptionally allowing values very different from zero. The deﬁnition of φ thus
raises the same difﬁculties as those found in image restoration for correlated models
(see Chapter 6). The evolution followed in the choice of the functions φ is also simi-
lar: quadratic functions, then non-convex functions and, more recently, non-quadratic
convex functions.
5.2.1. Quadratic regularization
The simplest choice is to take φ(r) = r2, since the resulting estimator is linear:
r = (HT H + μI)−1HT y.
(5.5)
This can be implemented using low-cost Wiener or Kalman ﬁltering techniques
[CRU 74, DEM 84, FAT 80, WOO 75]. These techniques are very widely used in
deconvolution but do not, in general, yield interesting results for spike restoration un-
less the spectral content of the IR is sufﬁciently rich in high frequencies. They can
only perform spectral equalization in the band where the SNR is large enough (see
(4.6) and discussion in Chapter 4).
As an illustration, let us go back to the synthetic example of Figure 5.1. In Fig-
ure 5.2 we present the solution r of (5.5) for various values of the regularization
parameter. The solution obtained for μ = 1 is clearly over-regularized, while the one
obtained for μ = 10−3 is under-regularized. There is no intermediate value of μ that
gives a satisfactory result as the IR has too strong a low frequency behavior (see Fig-
ure 5.1d), which makes linear processing ineffective, even with a moderate noise level.

122
Bayesian Approach to Inverse Problems
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(a) μ = 10−3
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(b) μ = 10−2
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(c) μ = 10−1
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(d) μ = 1
Figure 5.2. Results provided by quadratic regularization of the example of Figure 5.1. The
circles mark the spikes of the Mendel sequence. The result r is plotted for various values of the
regularization parameter μ
5.2.2. Non-quadratic regularization
The quadratic function penalizes large values too much and various authors have
looked into functions that increase more slowly than r2 as r increases.
The ﬁrst contribution proposing φ(r) = |r| appeared in the geophysics community
in the late 1970s [TAY 79] and was followed, among others, by [OLD 86] and, more
recently, [O’B 94]. The various authors try to deal with the algorithmic problems
connected with the non-differentiable nature of the criterion by using simplex-type
algorithms. Using a simplex means giving up the quadratic term on the data and re-
placing it by an absolute-value term. In practice, the method does, indeed, produce
solutions with a marked spiky character. It converges in a ﬁnite time but is very costly,
one of the most costly that exists according to Kaaresen’s comparison [KAA 98a].

Deconvolution of Spike Trains
123
Much more efﬁcient techniques to minimize (5.4) with φ(r) = |r| have been made
available by recent advances in the ﬁeld of variable selection based on the homotopy
approach of [OSB 00] (see also [EFR 04]). Moreover, such techniques do not calcu-
late r(μ) for single values of μ. Instead, they characterize the full family of solutions
{r(μ), μ > μmin}, which yields efﬁcient ways of tuning μ either on qualitative or
quantitative grounds.
On the other hand, Saito [SAI 90], taking his inspiration from the works of Leclerc
[LEC 89] in computer vision, has proposed φ(r) = 1 if r is non-zero, and φ(r) = 0
otherwise. The regularizing part of J thus increases linearly with the number of non-
zero samples. The corresponding criterion is neither convex nor differentiable (φ is
discontinuous at r = 0) and Saito recommends gradual non-convexity techniques
(GNC; see section 2.2.4). Saito’s method produces solutions with a very marked spiky
character but suffers from the optimization problems inherent in non-convex criteria.
More generally, the non-convex functions φ used in image restoration can be used
here (see section 6.4). The corresponding models contain hidden decision processes,
formally identical to the line processes in image restoration (see section 6.4.1). From
this point of view, such models can be seen as close to a Bernoulli-Gaussian model as
presented in section 5.3.
5.2.3. L2LP or L2Hy deconvolution
An alternative to quadratic regularization and non-convex or non-differentiable
functions is to choose strictly convex and differentiable functions φ(r) that increase
more slowly than r2 when r increases. It is remarkable that the use of such func-
tions should have been suggested only relatively recently [GAU 95]. Two classes of
functions of this type have been put forward in the context of non-destructive test-
ing (see Chapter 9): the function |r|p for 2 > p > 1 and the hyperbolic function
√
T 2 + r2, T > 0 with which “L2LP deconvolution” and “L2Hy deconvolution” are
respectively associated. Such functions allow us to approach the behavior of function
|r|, and thus to avoid over-penalizing large-amplitude reﬂectivities, while remaining
differentiable and strictly convex. The strict convexity of φ ensures that of J and thus
the existence and uniqueness of the solution and its continuity with respect to the data
and parameters. The differentiability of the criterion allows standard descent tech-
niques to be used that are very easy to adjust and very cost-competitive relative to the
simplex or GNC techniques mentioned in section 5.2.2. From a practical standpoint,
the uniqueness of the solution simpliﬁes the initialization. Concretely, the continuity
of the solution ensures the robustness of L2LP/L2Hy deconvolutions with respect to
noise and errors in the models or the choice of parameters. Finally, the approach pre-
sented here is easy to understand, which makes it accessible to the non-expert user.
The convex differentiable penalization approach is thus an excellent compromise in

124
Bayesian Approach to Inverse Problems
terms of cost, simplicity, accuracy and robustness. Section 5.4 completes this analysis
by comparative tests concerning robustness.
5.3. Bernoulli-Gaussian deconvolution
5.3.1. Compound BG model
The distinctive feature of the family of methods we are now going to look at is
that they aim to explicitly materialize the presence of a spike and dissociate it from
the spike amplitude r. To do this, they associate an auxiliary variable q with each
sample of the signal. Each variable is binary and shows the presence (q = 1) or
absence (q = 0) of a spike. The objective of the deconvolution is to simultaneously
estimate the position (indicated by q = 1) and amplitude of each spike. The BG model
corresponds to the simplest random model of this type, for which:
– Q is a Bernoulli variable of parameter λ
Δ= Pr(Q = 1) ≪1;
– the distribution of R, given that Q = q, is Gaussian and zero-mean with variance
qrx.
λ and rx are two hyperparameters assumed to be known. The simplicity of the BG
model in fact masks a difﬁculty for its use in maximum likelihood estimation: when
q = 0, the distribution of r is Gaussian with zero mean and variance, i.e., a Dirac
distribution. In general, Dirac distributions appear in the posterior likelihood function,
thereby making a straightforward application of the MAP paradigm irrelevant. To get
around the problem, it is possible replace the BG model by a mixed Gaussian model
with very small but non-zero variance. Another possibility is to consider the process,
re, of the spike amplitudes: it is Gaussian, zero-mean with variance rx, and is only
deﬁned when q ̸= 0. This approach is developed below.
5.3.2. Various strategies for estimation
The previous set of hypotheses (observation model (5.3), white, stationary Gaus-
sian noise, BG input model) allows the posterior likelihood of (Q, Re | Y = y) to
be deﬁned without ambiguity. However, Bayesian methodology leaves us a great deal
of freedom in the choice of the type of likelihood to be optimized, even though there
are limited choices in practice. Owing to the composite nature of the BG model, we
can envisage performing the deconvolution either by maximizing the joint likelihood
p(re | q, y) Pr(q | y), or by proceeding sequentially, estimating ﬁrst q by maximizing
the marginal likelihood Pr(q | y), then re by maximizing p(re | q, y). In simulation,
the joint approach leads to poorer quality results than the marginal approach when
the true values of the parameters are used. More precisely, the joint approach gives

Deconvolution of Spike Trains
125
a false alarm rate that is too high for an equivalent good detection rate. Neverthe-
less, it is possible to obtain comparable results for the two approaches with different
hyperparameter settings.
The marginal likelihood does not give rise to difﬁculties of deﬁnition due to the
Dirac distributions mentioned above, and the various deﬁnitions of the BG process
found in the literature all lead to the same marginal likelihood. For these reasons, we
describe the approach by maximum marginal likelihood below, making three points
clear:
1) the joint and marginal criteria differ structurally only by a matrix determinant;
2) most of the methods used to optimize one of these criteria can also be applied
to optimize the other (this is the case of the SMLR presented in section 5.3.4);
3) the q conditional estimation of the amplitudes is the same in the joint and se-
quential approaches.
5.3.3. General expression for marginal likelihood
By applying Bayes’ rule, the marginal likelihood can be written:
Pr(q | y) ∝p(y | q) Pr(q).
(5.6)
The expression for Pr(q) results from Bernoulli’s hypothesis of section 5.3.1
Pr(q) = λMe(1 −λ)M−Me
where Me is the number of non-zero components of vector q. Considering the deﬁni-
tion of BG processes given in section 5.3.1, the components of vector re are Gaussian,
zero-mean, independent and with variance rx. As recalled in section 5.3.2, noise b is
also white, Gaussian, zero-mean, stationary and with variance rb. We thus deduce
that:
p(y | q) = N(0, B)
with
B
Δ= rx

k
htkhT
tk + rbI,
(5.7)
where hn is the nth column of H, because of the conditionally Gaussian nature of
(Re | Q = q) and the linearity of input-output relationship (5.3). It should be stressed
that B can also be expressed:
B = HΠHT + rbI
with
Π
Δ= rxDiag {q(m)}1≤m≤M
(5.8)
and it is the latter expression that we will use in what follows.
By setting aside the terms that do not depend on q, we deduce from (5.6)-(5.7) that
maximizing Pr(q | y) is equivalent to maximizing:
L(q)
Δ= −yT B−1y −log |B| −2Me log 1 −λ
λ
.
(5.9)

126
Bayesian Approach to Inverse Problems
Before tackling the practical problems connected with this maximization, let us ex-
amine the second step of the sequential approach, i.e., the estimation of the spike am-
plitudes when the sequence q is assumed to be known. Maximizing p(re | q, y) comes
down to estimating the maximum a posteriori of the Gaussian variable re observed
through the linear system H. According to the results stated previously (Chapter 3,
section 3.8), the estimate takes the form:
∀k,
(re)k = hT
tk B−1 y
and can be calculated in the standard way except, perhaps, for very large signals.
Numerous choices – generally dictated by the operating conditions – remain to be
made if we are to implement the maximization of L(q). Particularly worth mentioning
are the choices of recursive or batch processing, and the type of representation for the
linear system h. A detailed examination of all these situations is beyond the scope of
this chapter. For this reason, we will give a precise description of only one method,
corresponding to hypotheses that will be stated, whilst nevertheless trying to bring out
the general mechanisms of BG deconvolution and the trade-offs that always have to
be made. We will then brieﬂy mention other important techniques, highlighting the
main characteristics and situating them with respect to the chosen method.
5.3.4. An iterative method for BG deconvolution
Due to the discrete nature of q, the exact maximization of L is a combinatorial
problem. Formally, the problem is simple as all we need to do is calculate L for the
2M possible conﬁgurations of q so as to ﬁnd the maximizer of q. Unfortunately, this
is impossible to envisage in practice, even for signals of moderate size; the calculation
would be far too voluminous. To obtain a realistic method, we will conﬁne ourselves
to exploring a subset of the possible conﬁgurations of q and, as far as possible, avoid
calculating L by straightforward application of equation (5.9). To do this, we de-
ﬁne the notion of neighboring sequences and iteratively maximize the likelihood on
the neighborhoods thus deﬁned. The efﬁciency of the resulting method essentially
depends on three factors: the nature of the neighborhoods, the existence of simple
formulae connecting the likelihood of two neighboring sequences, and the strategy
for exploring the neighborhoods. In the example given here, we deﬁne the neigh-
borhood of a sequence q0 as the set of sequences qk that differ from q0 by exactly
one component. We then establish formulae connecting the likelihoods of qk and q0.
These formulae serve as the basis of a suboptimal SMLR-type [KOR 82] procedure
for optimizing L, which consists of maximizing L over the whole neighborhood of a
sequence q0, then repeating the process until a local maximum is reached. We will
look at some more reﬁned variants in the next section.
In what follows, the indices 0 and k concern quantities relative to q0 and qk re-
spectively and the kth vector of the canonical basis of
 Mis designated vk. To obtain

Deconvolution of Spike Trains
127
formulae for updating L that do not require many calculations, we introduce the fol-
lowing auxiliary quantities:
A
Δ= HT B−1H,
w
Δ= HT B−1y,
ρk
Δ= εkr−1
x
+ vT
k A0vk,
where εk takes the value ± 1 according to whether a spike is added to or taken away
from q0 in position k. According to the expression for L established in equation (5.9),
the relation between Bk and B0 plays a big part in establishing the formulae we are
looking for. From equation (5.8), we have:
Πk = Π0 + εkvkrxvT
k ,
which, by substitution in (5.7) and by application of the matrix inversion lemma, leads
to:
B−1
k
= B−1
0
−B−1
0 Hvkρ−1
k vT
k HT B−1
0 .
(5.10)
From this we deduce that:
yT B−1
k y = yT B−1
0 y −wT
0 vkρ−1
k vT
k w0
and, using another traditional result [GOO 77, Appendix E], that:
|Bk| = εkrxρk |B0| .
If we assume that all the auxiliary quantities relative to sequence q0 are known, the
likelihood calculation for a sequence qk of its neighborhood can be performed using
the following algorithm:
kk = A0vk,
ρk = εkr−1
x
+ vT
k kk,
(5.11)
L(qk) = L(q0) + wT
0 vkρ−1
k vT
k w0 −log(εkrxρk) −2εk log 1 −λ
λ
.
(5.12)
Once the whole neighborhood of q0 has been explored, the sequence qk that maxi-
mizes L is chosen as a new starting point. To reduce the volume of the calculations,
it is better not to completely re-evaluate the auxiliary quantities but to calculate them
iteratively. From equation (5.10), we have the following formulae:
wk = w0 −kkρ−1
k vT
k w0
(5.13)
Ak = A0 −kkρ−1
k kT
k
(5.14)
and, after initialization, the two sets of equations above make up the complete BG
deconvolution algorithm, (5.11)-(5.12) being used to explore the neighborhood of the
current sequence and (5.13)-(5.14) to select a new sequence.

128
Bayesian Approach to Inverse Problems
5.3.5. Other methods
The method presented above allows us to restore signals modeled as BG processes
with the hypothesis of linear distortion and white, Gaussian, stationary observation
noise. There are several other techniques for dealing with this problem and various
extensions have also been proposed. Below, we look brieﬂy at the most important of
these methods.
If we restrict ourselves to the framework adopted so far, several elements have to
be chosen: type of representation of the system IR, exact nature of the likelihood (joint
or marginal), and technique for maximizing the likelihood. We stress once again that
the choice of the type of likelihood has little inﬂuence on the algorithmic questions
and appropriate adjustment of the hyperparameters generally gives similar results in
both cases. It should, however, be pointed out that, for joint likelihood, the fact that we
have to give the hyperparametervalues that are far from their “empirical” values can be
troublesome in a non-supervised framework (see section 5.4.2). In compensation, the
joint likelihood offers a little more ﬂexibility in the development of the optimization
procedures.
The choice of how to represent the IR has a signiﬁcant inﬂuence on the devel-
opment of the maximization algorithms in as much as it affects the way the input-
output relation of the system is written. If we ignore the all-pole representations,
which are little used because of the constraints they impose on the phase of h, the
ﬁrst works on BG deconvolution were based on a zero-pole representation of the
IR [KOR 82, MEN 83]. Due to its parsimonious character, this representation lim-
its the memory resources and, to a lesser extent, the computing power required for the
optimization procedure. The importance of such considerations has obviously greatly
decreased since that time. Furthermore, using such a representation can lead to algo-
rithms of a type and algebraic structure remarkably similar to those of the procedure
described in section 5.3.4, even though the details of the calculations are notably more
dled and takes advantage of the structure of matrix H.
It is using the likelihood optimization techniques that BG deconvolution methods
can be distinguished from one another. These techniques have three component parts:
1 an algorithm for evaluating the increment of the likelihood when there is an
elementary modiﬁcation of the Bernoulli sequence q;
2 an algorithm for updating all the quantities when a new current sequence q is
accepted;
3 a strategy for partial exploration of these sequences.
have
complex. Finally, it should be noted that most of the works of the last 10 years
used a representation of h by a discretized IR, which simpliﬁes the algebra to be han-

Deconvolution of Spike Trains
129
In the case where all the observed data y are available (off-line processing), several
deterministic likelihood maximization procedures have been proposed: SMLR and
MMLR methods and their variants [CHA 96, KOR 82, MEN 90] use restricted neigh-
borhoods (two neighboring sequences differ by one, or at most two, samples) and
explore all the sequences that are neighbors of the current sequence then select the
one that maximizes the likelihood before going on to a new iteration; ICM [LAV 93]
methods, based on a neighborhood system similar to the previous one, select one
neighboring sequence of the current sequence in a predetermined or random way and
accept it if it leads to an increase in the likelihood before iterating the procedure; IWM
techniques [KAA 97] are based on more extensive neighborhoods and compensate for
the resulting increased numerical complexity by maximizing the joint likelihood, not
sequentially but alternately with respect to variables q and re, which considerably
lightens the calculations. We should stress that these deterministic optimization meth-
ods do not guarantee convergence towards the global maximum of the likelihood.
The choice among the various methods should be guided by the trade-off to be
made between the more or less exhaustive character of the exploration of possible
sequences q on the one hand, and the numerical complexity of the method on the other.
This last point depends on the strategy chosen and the distribution of the volume of
calculations between components 1 and 2 of the method.
We should also mention that recursive techniques suitable for on-line processing
of the data have also been proposed [CHI 85, GOU 89, IDI 90]. These have the same
three components as above but the recursive processing imposes signiﬁcant constraints
on the strategy for exploring sequences q. In general, only the components of q cor-
responding to the current or very recent samples of y can vary, the more distant past
of the Bernoulli sequence being ﬁxed. As decisions concerning this more distant past
are not called into question, the exploration of all sequences q is more partial than in
the previous case. This leads to a generally more modest performance, the price to be
paid for recursive processing.
Note that components
1 and
2 of the deterministic BG deconvolution methods
also lie at the heart of stochastic optimization methods of the simulated annealing
type. The (theoretical) interest of these approaches is that they guarantee convergence
towards the global maximum of the likelihood but, unfortunately, at the cost of very
heavy calculations. Formulae such as (5.11)-(5.12) and (5.13)-(5.14) lend themselves
well to the development of such approaches but they have been used very little in prac-
tice as the increase in volume of the calculations relative to deterministic optimization
methods is out of all proportion with the potential improvement in the results.
An interesting extension would be to replace the BG model employed up to now
by a “BG + Gaussian” model. In applications such as echography, this type of model
enables us, among other things, to take account of low-amplitude reﬂectors located
between high-amplitude reﬂectors. The inﬂuence of the use of this model on the

130
Bayesian Approach to Inverse Problems
way the problem is formulated and the way the likelihood is expressed is relatively
limited. Several deconvolution methods similar to those mentioned so far and based
on deterministic [LAV 93, MEN 90] and stochastic [LAV 93] optimization procedures
have been put forward. The case of a non-Gaussian distribution of the amplitudes has
also been treated formally [KAA 97], but does not appear to have led to any practical
applications.
Finally, BG deconvolution techniques based not on maximum likelihood but on
posterior mean estimators have recently been proposed [CHE 96, DOU 97]. Such es-
timators require samples of (q, re) to be drawn at random according to the distribution
p(re | q, y) Pr(q | y), which is carried out using pseudo-random sampling procedures
known as Markov Chain Monte Carlo (MCMC, see Chapter 7). The main interest of
these approaches is that they allow the blind deconvolution problem (see section 5.5.2)
to be treated elegantly. We should insist here that components
1 and
2 of the de-
terministic methods of BG deconvolution mentioned earlier are again at the heart of
the algorithms for drawing samples according to the distribution p(re | q, y) Pr(q | y),
and thus numerically highly efﬁcient algorithms such as that of formulae (5.11)-(5.12)
and (5.13)-(5.14) are once again very useful.
5.4. Examples of processing and discussion
In this section, we will discuss the comparative natures of the solutions obtained
by BG deconvolution and L2Hy deconvolution in more detail and look at the practical
considerations that distinguish these two methods.
5.4.1. Nature of the solutions
Figure 5.3a gives the result of a BG deconvolution by SMLR on the data of Fig-
ure 5.1b. The values used for the hyperparameters are (λ, rx, rb) = (0.07, 0.01,
5.10−5), close to the “true” values used for the synthesis: (λ⋆, r⋆
x, r⋆
b) = (0.05, 0.01,
5.4.10−5). This result is better than that of Figure 5.3b obtained by L2Hy convex reg-
ularization with φ(r) =
√
r2 + T 2 (μ = 0.02 and T = 10−4), which is already very
good. Appropriate thresholding of the latter gives results very close to the SMLR;
only the group of four spikes around index 170 is still better restored by SMLR. In
addition, three spikes are doubtless impossible to restore with the data used: the very
low-amplitude one situated around 140 and the last two, which are minimally repre-
sented in the data because of the boundary hypothesis used. The BG deconvolution
result obviously has a more markedly spiky character than the result from convex reg-
ularization. In general, the quality of the estimation of the spike amplitudes is lower
with L2Hy deconvolution, as it is spread over several points. Finally, relative to the
case of quadratic regularization, the results of BG/L2Hy deconvolution bring a clear
improvement (compare Figures 5.3 and 5.2).

Deconvolution of Spike Trains
131
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(a) BG deconvolution by SMLR
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(b) L2Hy deconvolution
Figure 5.3. Comparison of behavior of BG and L2Hy deconvolution
methods on the data of Figure 5.1b
To complete this analysis, we reran the two algorithms, with the same set of pa-
rameters, on data that only differed from those of Figure 5.1b by the realization of
the Gaussian white noise that was added. We thus obtained Figure 5.4, which should
be compared with Figure 5.3. The results of L2Hy deconvolution, Figures 5.3b and
5.4b, are qualitatively much closer to one another than the results of BG deconvolu-
tion, Figures 5.3a and 5.4a. Thresholding of 0.01 of both L2Hy solutions would give
identical spike positions. This better stability of the L2Hy solution corresponds to the
robustness expected for this method.
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(a) BG deconvolution by SMLR
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(b) L2Hy deconvolution
Figure 5.4. Test of robustness of the BG and L2Hy methods with respect to noise realization

132
Bayesian Approach to Inverse Problems
The quality of the spike deconvolution mainly depends on the bandwidth of the IR
and on the SNR, parameters that condition the quality of the deconvolution in gen-
eral. Let us add a characteristic of BG deconvolution: a high sensitivity to imperfect
knowledge of the IR. As an illustration, the data of Figure 5.1b were processed again,
for the same hyperparameters, but a perturbed IR. The perturbation was a phase ro-
tation of ten degrees (see Chapter 9, section 9.4.3), the effect of which is shown in
Figure 5.5a. It is a moderate perturbation that only affects the phase of the frequency
response of the IR (its energy spectrum is unchanged). Comparing Figures 5.5b and c
with Figures 5.3a and b, we note that the L2Hy deconvolution shows better robustness
to perturbation generated by the inadequacy of the IR.
0
5
10
15
20
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
(a) original IR (
) and IR after a 10° phase rotation (---)
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(b) BG deconvolution by SMLR
0
50
100
150
200
250
300
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
(c) L2Hy deconvolution
Figure 5.5. Test of robustness of the BG and L2Hy methods with
respect to poor speciﬁcation of the IR
5.4.2. Setting the parameters
In addition to the observations, both methods require initialization, here taken
as zero, and speciﬁcation of the numerical values for the IR and the hyperparame-
ters. There are two hyperparameters for the L2Hy regularization and three for the BG

Deconvolution of Spike Trains
133
see section 5.3.5). In fact, the L2Hy deconvolution does not take any decisions itself,
so to obtain a result of the same kind as the BG, a threshold parameter would be
needed. To these parameters must be added parameters having less inﬂuence but nec-
essary for the test to stop the descent method used to minimize the criterion. In com-
parison, the SMLR deconvolution presented here does not use a parametrized stopping
test as the optimization is performed in a discrete state space and a local minimum is
obtained in a ﬁnite time.
In practice neither of these methods requires ﬁne adjustment of the parameters.
There is no adjustment method that is at the same time low-cost, universal and statis-
tically well founded. The interested reader will ﬁnd some empirical, common sense
recipes in the articles mentioned above.
The statistically sound hyperparameterestimators are also the most cumbersome to
use [CHA 96, GOU 92]. They bring in MCMC techniques of a similar nature to those
mentioned in section 5.5.2. As an example, the method known as SEM, applied to BG
deconvolution in [CHA 96], gives the estimates (λ, rx, rb) = (0.08, 0.008, 5.8.10−5)
in the case of the data of Figure 5.1b. Application of BG deconvolution with these
parameters gives a result of quality between those of Figures 5.3a and 5.5b.
5.4.3. Numerical complexity
It is quite hard to precisely evaluate the numerical complexity of the BG or L2Hy
deconvolution algorithms because of the iterative character of the methods and the
difﬁculty of predicting the number of iterations needed for convergence. The BG
methods get their efﬁciency from specialized techniques that exploit the sparsity of
the spikes and only keep their advantage when the spikes come at a low rate (≤0.1).
The specialization of these techniques also makes them more complex and more difﬁ-
cult to implement than the standard descent techniques used for L2Hy deconvolution,
which are numerically more costly. The cost nevertheless remains very reasonable:
processing the 300 samples of the synthetic example only takes about one second of a
CG algorithm using the linesearch strategy proposed in [LAB 08], written in Matlab
and run on a PC (Intel Pentium 4, 2 GHz, 1 GB).
5.5. Extensions
The methods presented in this chapter admit numerous extensions concerning:
– the structures of noise covariance matrix R and observation matrix H, which
have so far been taken as diagonal and Toeplitz respectively;
– the estimation of the IR on the basis of observations y, of unknown reﬂectivity;
deconvolution described here (the method recommended by Kaaresen only needs two;

134
Bayesian Approach to Inverse Problems
– multichannel deconvolution, to take advantage of lateral correlations among sig-
nals received in neighboring positions, a standard context in NDE (see Chapter 9) and
seismology. This theme has many connections with image restoration and will not be
developed here. For further information, see [IDI 93, KAA 98b, LAV 91].
5.5.1. Generalization of structures of R and H
All the observations on methodology remain valid for any matrices R and H. Due
to the matrix standpoint adopted here, a change in their structure has little effect on the
algorithms – on a macroscopic scale – but the computational complexity may increase
by a factor of N. However, when matrices R and H of the direct problem are strongly
structured, the techniques presented can be generalized without any notable loss of
numerical efﬁciency.
In the case of colored noise, effective implementations of BG deconvolution have
been proposed in the case of noise modeled in autoregressive form, provided that the
order of the model is not too high [CHA 93, MEN 83]. In fact, the most interesting
extensions concern other structures of H; some particularly noteworthy extensions
are:
– spectral ray analysis where H represents a Fourier matrix.
The spike train
restoration problem is dealt with using extensions of BG techniques in [DUB 97,
BOU 06], and by convex and non-convex regularization in [BOU 07] and [SAC 98],
respectively;
– basis selection in decomposition into packets of wavelets: H thus represents a
wavelet transform [PES 96];
– “double BG” [CHA 93] and “double L2Hy” [GAU 01] deconvolution in which
H models any non-homogeneousphase rotations of the convolution kernel (see Chap-
ter 9, section 9.4.2).
Note that, in these examples, matrix H generally has many more columns than rows
and the apparently very under-determined aspect of the underlying problem is over-
come using impulse priors.
5.5.2. Estimation of the impulse response
In almost all applications, the IR – and thus the resulting matrix H – is not an
input of the problem: because the convolution is often only a crude physical model,
the IR has no existence in itself and must be estimated. If possible, speciﬁc auxiliary
measurements are used as in instrument calibration. If not, the problem can be tackled
by blind deconvolution, i.e., both the IR and the reﬂectivity are estimated from the
observed signal. This estimation is obviously valid up to an amplitude factor and a

Deconvolution of Spike Trains
135
time-shift factor, which cannot be identiﬁed from the data. The phase of the frequency
response of the IR cannot be identiﬁed either if the input signal is Gaussian [LII 82].
From this point of view, a spiky input signal corresponds to a more favorable situation.
The main methods for estimating the IR are based on the various ways of deﬁning,
interpreting and exploiting the non-Gaussian character of the reﬂectivity. An account
of these methods is given in Chapter 9.
In this section, we will only discuss the methods speciﬁcally using BG mod-
els or their variants, such as a Gaussian mixture.
The simplest blind BG meth-
ods [GOU 86, KAA 98b] are based on maximization of the generalized likelihood
(GML), deﬁned as the probability distribution of all random quantities (observation,
reﬂectivity, h) conditionally on all deterministic parameters (noise variances, λ, etc.).
The generalized likelihood is formally deﬁned by equation (3.9) in Chapter 3. Unlike
maximization of the exact likelihood – deﬁned as the probability of the observations
alone, knowing the parameters; see Chapter 3, equation (3.3) – generalized maxi-
mum likelihood (GML) techniques have no asymptotic convergence properties. On
the other hand, they are the only ones that can be implemented by simple iterative
deterministic algorithms such as, for example, alternating an SMLR BG deconvolu-
tion step with a step to re-estimate h and the hyperparameters. In the implementation
proposed by Kaaresen [KAA 98b], the GML gives good results for an observed signal
of 1, 000 samples synthesized with a wavelet of narrower bandwidth than that used in
our simulations and a favorable SNR of 15 dB, but with an impulse density parameter
ﬁxed in advance (this parameter probably cannot be identiﬁed by GML). In addition,
the method seems to be convergent for the estimation of the IR in the sense where a
decrease in the SNR to 7 dB can be compensated for by processing 10 times as much
data.
The most recent contributions on blind BG deconvolution [ROS 03, LAB 06] are
based on MCMC techniques following Cheng et al. [CHE 96]. The MCMC tech-
niques are the most statistically sound, since they give access to the exact likelihood,
but they require intensive calculation. They consist of probabilizing the IR and hy-
perparameters and sampling the posterior law of the reﬂectivity, the IR and the hyper-
parameters conditionally to the observations. If the estimators chosen for the reﬂec-
tivity and the IR are conditional expectations, they can be approached by averages of
pseudo-random realizations drawn according to the posterior law. The example pro-
posed by Cheng et al. [CHE 96] is composed of 2,000 samples obtained with three
values of the SNR (26 dB, 18.6 dB and 4 dB) and a wavelet of bandwidth compara-
ble to that of our simulations. The examples at 18.6 dB and 26 dB give good results
on the IR and input, whereas the 4 dB case only gives acceptable results for the IR.
It is difﬁcult to compare the respective merits of the methods put forward by Cheng
et al. and Kaaresen using simulation results, as none of the experimental conditions
coincide. However, Cheng et al.’s method estimates the density of the impulses unlike
Kaaresen’s, for which this parameter is ﬁxed. This illustrates the fact that a method

136
Bayesian Approach to Inverse Problems
giving access to the exact likelihood offers more possibilities than a method based
on maximization of the generalized likelihood. As regards real data processing, the
potential of the MCMC method by Cheng et al. as been investigated by Rosec et
al. [ROS 03] on marine seismic data. They show that these techniques enable us to
improve seismic image resolution in at least two respects: better interface localization
and layer detection.
Finally, although it remains implicit in the above mentioned articles, only the mul-
tichannel blind approaches seem to provide satisfactory results for the processing of
real data.
5.6. Conclusion
We ﬁnd ourselves faced with a problem of spike train deconvolution whenever
two point sources become indiscernible in the observed signal because of the limited
resolution of the sensor and the small distance between the sources. If we use the
hypotheses of linearity and shift-invariance, after discretization the problem comes
down to a discrete deconvolution in which the input is a discrete-time sparse spike
train. The limited bandwidth of the sensor usually makes inverse ﬁltering methods
ineffective to restore the broadband character of the input.
We have presented two families of techniques aimed speciﬁcally at restoring spiky
signals:
– the ﬁrst favors estimation of the input by optimization of a criterion regularized
on
 n which penalizes non-impulse solutions;
– the second insists particularly on the source detection aspect and places a re-
duced number of spikes by optimizing a criterion that depends on the position of the
impulses. As these positions are discrete, these methods solve a problem of combina-
torial optimization, processed by suboptimal algorithms.
Finally, we have described two methods, L2Hy deconvolution by convex penalization
and Bernoulli-Gaussian deconvolution by SMLR. These methods are representative
of the two families respectively.
L2Hy deconvolution gives solutions with a marked spiky character, in the sense
that they are composed of a majority of samples with a low value and a few samples
with a modulus that is much higher. Impulse detection, if necessary, has to take place
in a second step by thresholding these solutions. Due to the convexity of the optimized
criterion, the solution obtained is continuous and depends on the observations, the IR
and the hyperparameters, which ensures that the solution is less sensitive to these pa-
rameters. In comparison, BG deconvolution produces a solution that includes spike
detection. The solutions provided are thus discontinuous with respect to the parame-
ters. These characteristics of the solutions provided by the two methods are illustrated

Deconvolution of Spike Trains
137
by a synthetic example that allows them to be compared. In the conditions where the
hypotheses of the model are properly fulﬁlled, BG deconvolution gives solutions of a
quality better than or equivalent to L2Hy deconvolution, depending on the noise re-
alization. In contrast, it is not as robust as the latter to modeling errors such as poor
knowledge of the IR.
This crucial problem of modeling errors will be considered again in Chapter 9
where it will be analyzed by means of an example on real data from non-destructive
testing using ultrasound. Chapter 9 illustrates the gap between the application of the
techniques presented here and the practical processing of data, which, in addition to
the observed signal, requires the speciﬁcation of the quantities that were assumed to
be known here but which are often unknown in practice, such as the IR and the hy-
perparameters. These quantities are all the more difﬁcult to adjust in practice when
the assumed convolutional model is only a rough, ﬁrst-order representation of the
underlying physics. Processing real data often makes it necessary to implement ex-
tensions of the methods presented here. Chapter 9 describes one of these extensions,
sections 5.3.5 and 5.5 point the interested reader towards the variants and extensions
that are not covered in detail in this book.
5.7. Bibliography
[ATA 82] ATAL B. S., REMDE J. R., “A new method of LPC excitation for producing natural
sounding speech at low bit rates”, in Proc. IEEE ICASSP, vol. 1, Paris, France, p. 614-617,
May 1982.
[BOU 06] BOURGUIGNON S., CARFANTAN H., “Spectral analysis of irregularly sampled data
using a Bernoulli-Gaussian model with free frequencies”, in Proc. IEEE ICASSP, Toulouse,
France, May 2006.
[BOU 07] BOURGUIGNON S., CARFANTAN H., IDIER J., “A sparsity-based method for the
estimation of spectral lines from irregularly sampled data”, IEEE J. Selected Topics Sig.
Proc., vol. 1, num. 4, p. 575-585, Dec. 2007, Issue: Convex Optimization Methods for
Signal Processing.
[CHA 93] CHAMPAGNAT F., IDIER J., DEMOMENT G., “Deconvolution of sparse spike trains
accounting for wavelet phase shifts and colored noise”, in Proc. IEEE ICASSP, Minneapo-
lis, MN, p. 452-455, 1993.
[CHA 96] CHAMPAGNAT F., GOUSSARD Y., IDIER J., “Unsupervised deconvolution of
sparse spike trains using stochastic approximation”, IEEE Trans. Signal Processing, vol. 44,
num. 12, p. 2988-2998, Dec. 1996.
[CHE 96] CHENG Q., CHEN R., LI T.-H., “Simultaneous wavelet estimation and deconvolu-
tion of reﬂection seismic signals”, IEEE Trans. Geosci. Remote Sensing, vol. 34, p. 377-
384, Mar. 1996.
[CHI 85] CHI C. Y., GOUTSIAS J., MENDEL J. M., “A fast maximum-likelihood estimation
and detection algorithm for Bernoulli-Gaussian processes”, in Proc. IEEE ICASSP, Tampa,
FL, p. 1297-1300, Apr. 1985.

138
Bayesian Approach to Inverse Problems
[CRU 74] CRUMP N. D., “A Kalman ﬁlter approach to the deconvolution of seismic signals”,
Geophysics, vol. 39, p. 1-13, 1974.
[DEM 84] DEMOMENT G., REYNAUD R., HERMENT A., “Range resolution improvement by
a fast deconvolution method”, Ultrasonic Imaging, vol. 6, p. 435-451, 1984.
[DJU 96] DJURIC P., “A model selection rule for sinusoids in white Gaussian noise”, IEEE
Trans. Signal Processing, vol. 44, num. 7, p. 1744-1751, July 1996.
[DOU 97] DOUCET A., DUVAUT P., “Bayesian estimation of state space models applied to
deconvolution of Bernoulli-Gaussian processes”, Signal Processing, vol. 57, p. 147-161,
1997.
[DUB 97] DUBLANCHET F., IDIER J., DUVAUT P., “Direction-of-arrival and frequency es-
timation using Poisson-Gaussian modeling”, in Proc. IEEE ICASSP, Munich, Germany,
p. 3501-3504, Apr. 1997.
[EFR 04] EFRON B., HASTIE T., JOHNSTONE I., TIBSHIRANI R., “Least angle regression”,
Annals Statist., vol. 32, num. 2, p. 407–451, 2004.
[FAT 80] FATEMI M., KAK A. C., “Ultrasonic B-scan imaging: Theory of image formation
and a technique for restoration”, Ultrasonic Imaging, vol. 2, p. 1-47, 1980.
[GAU 95] GAUTIER S., LE BESNERAIS G., MOHAMMAD-DJAFARI A., LAVAYSSIÈRE B.,
“Data fusion in the ﬁeld of non destructive testing”, in K. HANSON (Ed.), Maximum En-
tropy and Bayesian Methods, Santa Fe, NM, Kluwer Academic Publ., p. 311-316, 1995.
[GAU 01] GAUTIER S., IDIER J., CHAMPAGNAT F., VILLARD D., “Restoring separate dis-
continuities from ultrasonic data”, in Review of Progress in Quantitative Nondestructive
Evaluation, AIP Conf. Proc. Vol 615(1), Brunswick, ME, p. 686-690, July 2001.
[GOO 77] GOODWIN G. C., PAYNE R. L., Dynamic System Identiﬁcation. Experiment Design
and Data Analysis, Academic Press, 1977.
[GOU 86] GOUTSIAS J. K., MENDEL J. M., “Maximum-likelihood deconvolution: An opti-
mization theory perspective”, Geophysics, vol. 51, p. 1206-1220, 1986.
[GOU 89] GOUSSARD Y., DEMOMENT G., “Recursive deconvolution of Bernoulli-Gaussian
processes using a MA representation”, IEEE Trans. Geosci. Remote Sensing, vol. GE-27,
p. 384-394, 1989.
[GOU 92] GOUSSARD Y., “Blind Deconvolution of sparse spike trains using stochastic opti-
mization”, in Proc. IEEE ICASSP, vol. IV, San Francisco, CA, p. 593-596, Mar. 1992.
[HOG 74] HOGBOM J., “Aperture synthesis with a non-regular distribution of interferometer
baselines”, Astron. Astrophys. Suppl., vol. 15, p. 417-426, 1974.
[IDI 90] IDIER J., GOUSSARD Y., “Stack algorithm for recursive deconvolution of Bernoulli-
Gaussian processes”, IEEE Trans. Geosci. Remote Sensing, vol. 28, num. 5, p. 975-978,
Sep. 1990.
[IDI 93] IDIER J., GOUSSARD Y., “Multichannel seismic deconvolution”,
IEEE Trans.
Geosci. Remote Sensing, vol. 31, num. 5, p. 961-979, Sep. 1993.
[KAA 97] KAARESEN K. F., “Deconvolution of sparse spike trains by iterated window maxi-
mization”, IEEE Trans. Signal Processing, vol. 45, num. 5, p. 1173-1183, May 1997.

Deconvolution of Spike Trains
139
[KAA 98a] KAARESEN K. F., “Evaluation and applications of the iterated window maximiza-
tion method for sparse deconvolution”, IEEE Trans. Signal Processing, vol. 46, num. 3,
p. 609-624, Mar. 1998.
[KAA 98b] KAARESEN K. F., “Multichannel blind deconvolution of seismic signals”, Geo-
physics, vol. 63, num. 6, p. 2093-2107, Nov.-Dec. 1998.
[KOR 82] KORMYLO J. J., MENDEL J. M., “Maximum-likelihood detection and estimation
of Bernoulli-Gaussian processes”, IEEE Trans. Inf. Theory, vol. 28, p. 482-488, 1982.
[KWA 80] KWAKERNAAK H., “Estimation of pulse heights and arrival times”, Automatica,
vol. 16, p. 367-377, 1980.
[LAB 06] LABAT C., IDIER J., “Sparse blind deconvolution accounting for time-shift ambi-
guity”, in Proc. IEEE ICASSP, vol. III, Toulouse, France, p. 616-619, May 2006.
[LAB 08] LABAT C., IDIER J., “Convergence of conjugate gradient methods with a closed-
form stepsize formula”, J. Optim. Theory Appl., vol. 136, num. 1, Jan. 2008.
[LAV 91] LAVIELLE M., “2-D Bayesian deconvolution”, Geophysics, vol. 56, p. 2008-2018,
1991.
[LAV 93] LAVIELLE M., “Bayesian deconvolution of Bernoulli-Gaussian processes”, Signal
Processing, vol. 33, p. 67-79, 1993.
[LEC 89] LECLERC Y. G., “Constructing simple stable description for image partitioning”,
Int. J. Computer Vision, vol. 3, p. 73-102, 1989.
[LII 82] LII K. S., ROSENBLATT M., “Deconvolution and estimation of transfer function
phase and coefﬁcients for non Gaussian linear processes”, Annals Statist., vol. 10, num. 4,
p. 1195-1208, 1982.
[MEN 83] MENDEL J. M., Optimal Seismic Deconvolution, Academic Press, New York, NY,
1983.
[MEN 90] MENDEL J. M., Maximum-Likelihood Deconvolution – A Journey into Model-
Based Signal Processing, Springer Verlag, New York, NY, 1990.
[O’B 94] O’BRIEN M. S., SINCLAIR A. N., KRAMER S. M., “Recovery of a sparse spike
time series by L1 norm deconvolution”, IEEE Trans. Signal Processing, vol. 42, num. 12,
p. 3353-3365, Dec. 1994.
[OLD 86] OLDENBURG D. W., LEVY S., STINSON K., “Inversion of band-limited reﬂection
seismograms: Theory and practice”, Proc. IEEE, vol. 74, p. 487-497, 1986.
[OSB 00] OSBORNE M. R., PRESNELL B., TURLACH B. A., “A new approach to variable
selection in least squares problems”, IMA J. Numer. Anal., vol. 20, num. 3, p. 389–403,
2000.
[PES 96] PESQUET J.-C., KRIM H., LEPORINI D., HAMMAN E., “Bayesian approach to best
basis selection”, in Proc. IEEE ICASSP, Atlanta, GA, p. 2634-2637, May 1996.
[ROS 03] ROSEC O., BOUCHER J.-M., NSIRI B., CHONAVEL T., “Blind marine seismic de-
convolution using statistical MCMC methods”, IEEE Trans. Ocean. Eng., vol. 28, num. 3,
p. 502-512, 2003.

140
Bayesian Approach to Inverse Problems
[SAC 98] SACCHI M. D., ULRYCH T. J., WALKER C. J., “Interpolation and extrapolation
using a high-resolution discrete Fourier transform”, IEEE Trans. Signal Processing, vol. 46,
num. 1, p. 31-38, Jan. 1998.
[SAI 90] SAITO N., “Superresolution of noisy band-limited data by data adaptive regulariza-
tion and its application to seismic trace inversion”, in Proc. IEEE ICASSP, Albuquerque,
NM, p. 1237-1240, Apr. 1990.
[STO 89] STOICA P., MOSES R. L., FREIDLANDER B., SÖDERSTRÖM T., “Maximum likeli-
hood estimation of the parameters of multiple sinusoids from noisy measurements”, IEEE
Trans. Acoust. Speech, Signal Processing, vol. 37, num. 3, p. 378-392, Mar. 1989.
[TAY 79] TAYLOR H., BANKS S., MCCOY F., “Deconvolution with the L1 norm”,
Geo-
physics, vol. 44, num. 1, p. 39-52, 1979.
[VAN 68] VAN TREES H. L., Detection, Estimation and Modulation Theory, Part 1, John
Wiley, New York, NY, 1968.
[WAL 97] WALTER E., PRONZATO L., Identiﬁcation of Parametric Models from Experimental
Data, Springer-Verlag, Heidelberg, Germany, 1997.
[WON 92] WONG K. M., REILLY J. P., WU Q., QIAO S., “Estimation of directions of arrival
of signals in unknown correlated noise, part I: The MAP approach and its implementation”,
IEEE Trans. Signal Processing, vol. 40, num. 8, p. 2007-2017, Aug. 1992.
[WOO 75] WOOD J. C., TREITEL S., “Seismic signal processing”,
Proc. IEEE, vol. 63,
p. 649-661, 1975.

Chapter 6
Deconvolution of Images
6.1. Introduction
As explained in Chapter 1, constructing admissible solutions for ill-posed prob-
lems such as image deconvolution necessarily implies restricting ourselves to a limited
class of solutions, given some prior knowledge. In the case of images, the generally
expected solution possesses a certain degree of local regularity, measurable by norms
of derivatives or ﬁnite directional differences. More precisely, it is legitimate to as-
sume that the variations of intensity are limited except in transitions between regions,
if we exclude the case of strongly textured regions.
This chapter is above all devoted to how to take account of this qualitative property
of regularity “almost everywhere” as prior knowledge. Research work on the subject
is plentiful and the target ﬁeld of application particularly vast. In Chapters 12 to 14 of
this book we will see that this property is also an essential regularizing tool in imaging
for inverse problems other than deconvolution.
Looking for a discrete solution by minimizing a penalized criterion is one of the
simplest techniques. In some cases, such a discrete solution can be presented as an ap-
proximation of a continuous solution deﬁned in a functional framework. However, the
conditions for a functional solution to exist and be unique are mathematically more
difﬁcult to establish. In practice, immersion in the functional framework is not an
indispensable prerequisite. It is touched on in this chapter, without the in-depth math-
ematical treatment that would be necessary to guarantee the existence of functional
Chapter written by Jérôme IDIER and Laure BLANC-FÉRAUD.

142
Bayesian Approach to Inverse Problems
solutions. Similarly, the connection between penalization of the intensity variables
and calculation of the solution by isotropic or anisotropic diffusion is mentioned.
The Bayesian probabilistic interpretation could also have served as the mathemat-
ical framework, justifying the minimization of penalized criteria by looking for the
maximum a posteriori estimator (see Chapter 3). There again, immersion in this sta-
tistical framework is not an indispensable precondition for handling the basic tools
given in this chapter. On the other hand, it becomes necessary when certain “ad-
vanced” tools are introduced, such as resampling techniques, simulated annealing,
and the estimation of hyperparameters by maximum likelihood. This is why these two
chapters will refer much more explicitly to the Bayesian framework.
This chapter is principally divided into three and progresses in approximately
chronological order:
– Their structural simplicity and ease of implementation justify the fact that the lin-
ear solutions obtained by the minimization of “Tikhonov-type”penalized least squares
criteria play a central, historical role in the “inverse ﬁltering” domain. The deﬁnition,
and also the limitations, of these solutions are discussed in section 6.2. As far as their
practical calculation is concerned, Chapter 4 is partly devoted to the subject.
– In the 1980s, following on from work such as that by the Geman broth-
ers [GEM 84], a more sophisticated and ambitious approach appeared, which con-
sisted not only of estimating an image from imperfect data, but also of associating
with it a step for detection of discrete hidden variables materializing outlines or re-
gions. Section 6.3 is devoted to the detection-estimation methods which result from
this.
– Since the 1990s, matters have started to evolve towards greater simplicity. This
has led to penalty functions being chosen from a large family of convex, non-quadratic
functions and the detection step has been abandoned in favor of solving the problem
by convex optimization (section 6.4).
For the sake of completeness, we should add that minimizing penalized criteria
is not the only possible option in image deconvolution. More speciﬁcally, multires-
olution methods, based on decomposing the observed image in a wavelet domain,
have been developed in recent years and applied to astronomical [STA 02] and satel-
lite [KAL 03] imaging in particular.
6.2. Regularization in the Tikhonov sense
6.2.1. Principle
6.2.1.1. Case of a monovariate signal
Following the approach developed by Tikhonov in [TIK 77] and earlier articles
(the oldest date from 1963), the penalty function for candidate solutions is a quadratic

Deconvolution of Images
143
norm on the signal and its derivatives, measuring their regularity. Let us suppose ﬁrst
of all that a discrete or continuous monovariate function (a 1D signal rather than a 2D
image) x⋆is to be estimated from imperfect data:
y = Hx⋆+ noise,
(6.1)
where H is a (bounded) linear operator. The approach introduced by Tikhonov re-
quires the choice of a regularizing function ∥Dx∥2, where D is also a linear operator,
e.g. a differential operator. The estimated solution x is then deﬁned as the minimizer
of:
J (x) = ∥y −Hx∥2 + λ ∥Dx∥2 ,
(6.2)
where λ is a regularization parameter (strictly positive). In practice, it is possible to
impose conditions that are not very restrictive and ensure that J is strictly convex and
has a unique minimizer. In particular, this is the case if ∥Dx∥is a norm for x.
The ﬁrst term of J in (6.2) is a quadratic norm that penalizes the difference be-
tween the data y and the observation model for an admissible function x. In what
follows, we will concern ourselves more with the second term of J , i.e., the construc-
tion of the regularizing functional. In Tikhonov’s original contribution, x is a function
of a continuous variable of an interval Ω ⊂
  in
 , and:
∥Dx∥2 =
R

r=0

Ω
cr(s)

x(r)(s)
2 ds,
(6.3)
where the weights cr are strictly positive functions and x(r) is the rth order derivative
of x . Qualitatively, it is clear that such a choice corresponds to a prior hypothesis on
the smoothness of the signal to be estimated x⋆.
6.2.1.2. Multivariate extensions
Multivariate extensions have been proposed. In dimension d, to penalize only the
gradient of x (noted ∇x here), we can deﬁne:
∥Dx∥2 =

Ω
∇x(s)
2 ds =

Ω
d

i=1
 ∂x
∂si
(s)
2
ds.
(6.4)
The penalty functionals can also involve partial derivatives of higher order.
In
imaging (Ω ⊂
 2), several contributions bring in partial derivatives of order two
(e.g. [TER 83]).
If functional J deﬁned by equation (6.2) has a minimizer, this minimizer is the
solution of a Euler-Lagrange equation [AUB 06]). In the case of equation (6.4), this
can be written:
H∗Hx −λΔx = H∗y
(6.5)

144
Bayesian Approach to Inverse Problems
(where Δ = d
i=1 ∂2/∂s2
i is the Laplacian with respect to the space coordinates and
H∗is the adjoint operator of H), with the boundary condition:
∂x
∂n

∂Ω
= 0,
(6.6)
where ∂Ω represents the boundary of Ω and n a normal vector outward to ∂Ω.
6.2.1.3. Discrete framework
Discrete equivalents of functionals such as (6.3) or (6.4) appear in two types of
work.
Some try to approach the minimizer of functional J by discrete approximation,
usually by means of a scheme involving ﬁnite differences. For example, the penalizing
functional (6.4) can be approached by:
M

m=2
N

n=1
(xm,n −xm−1,n)2 +
M

m=1
N

n=2
(xm,n −xm,n−1)2
(6.7)
in the case of a rectangular domain Ω, divided into squares using M × N sites (m, n)
having the same step distance for the rows and columns. For conventional discretiza-
tion schemes (ﬁnite differences, and also ﬁnite elements), there are results showing the
convergence of the discrete minimizers towards the function that minimizes J when
the grid becomes ﬁner (see, for example, [TER 83]).
Others directly position themselves in the discrete framework or discretize the
signal to be restored over a ﬁxed number of points right from the start, before pos-
ing the inversion problem. Penalty functions such as (6.7) are then often found, e.g.
in [TIT 85], without reference to the regularization functions they come from.
6.2.2. Connection with image processing by linear PDE
Partial derivative equations (PDE) were introduced for the restoration of noisy
images by Koenderink [KOE 84], on the basis of a mathematical analogy between
the calculation of regular images and the diffusion of heat. In this formulation, it is
assumed that a noisy image y = {y(s), s ∈Ω} has been observed over a continuum Ω
(typically a block
 2, which is obviously not realistic in practice). A series of images
x(s, t) is deﬁned depending on an outside parameter t (the time or the scale) and an
equation for the evolution of x with time is introduced. The ﬁrst diffusion equation
applied to images, known as the heat equation, is the following linear parabolic PDE:
∂x(s, t)
∂t
= Δx(s, t),
(6.8)

Deconvolution of Images
145
with the conditions (6.6) and
x(s, 0) = y(s),
(6.9)
The system formed of (6.6), (6.8) and (6.9) applies to image restoration from a noisy
version y (i.e., H = Id in (6.1)). The idea is to “diffuse” the intensity of the image
isotropically as time goes on. It can be shown that this diffusion process corresponds
to the application of a circular, Gaussian, linear convolution operator of variance 2t
on y:
x(s, t) = (Gt ⋆y)(s)
with
Gt(s) = exp

−∥s∥2 /4t

/4πt.
In this formulation by PDE, the time parameter can be considered as a scale parameter.
In the image x(s, t), the information at scales smaller than t is blurred but the infor-
mation at coarser scales is preserved. For t →∞, x(s, t) tends towards a constant
(the mean of y): the diffusion process must be stopped after a certain time, and this
plays the role of a regularization parameter. To avoid the solution converging towards
a constant solution, and to be able to take account of a degradation process H, (6.8)
can be replaced by a “biased” PDE [NOR 90]:
∂x(s, t)
∂t
= μH∗(y(s) −Hx(s, t)) + Δx(s, t).
(6.10)
A stationary state of this equation is obtained by making the left hand side zero, thus by
solving the Euler-Lagrange equation (6.5) for μ = 1/λ. Also note that equation (6.10)
can be written in the form:
∂x(s, t)
∂t
= −∇J (x),
which, once discretized (in time), corresponds to a gradient algorithm for the mini-
mization of functional (6.2).
6.2.3. Limits of Tikhonov’s approach
When the image to be restored is made up of distinct regions, or when the aim
of the imaging procedure is to display isolated defects in a homogeneous medium (in
non-destructive testing), Tikhonov’s method proves limited in its capacity to detect
discontinuities, or even to indicate their approximate positions.
As an illustration, let us consider the following simulated experiment:
let
y = [y1, . . . , yN]T be a vector of noisy data, regularly sampled using a function
x⋆on [0, 1] which is univariate and piecewise smooth: yn = x⋆(n/N) + bn. The
function x⋆and the vector y are represented in Figure 6.1.
With R = 2, c0 = c1 = 0, c2 = 1 and ∥y −Hx∥2 = N
n=0 (yn −x(n/N))2,
equation (6.2) can be written:
J (x) = ∥y −Hx∥2 + λ
 1
0
(x′′(s))2 ds.
(6.11)

146
Bayesian Approach to Inverse Problems
Figure 6.1. A piecewise smooth monovariate function x⋆and 50 data items containing noise
yn, n = 1, . . . , N = 50
Let us introduce a discrete approximation for (6.11), by ﬁnite differences:
JM(x) = ∥y −Hx∥2 + λM 3
M−1

m=2
(2xm −xm−1 −xm+1)2 ,
(6.12)
where ∥y −Hx∥2 = N
n=1

yn −xnM/N
2, x = [x1, . . . , xM]T, and M is a mul-
tiple of N. We then deﬁne the estimated vector xλ as the minimizer of JM. The point
to point convergence of xλ towards the unique minimizer of (6.11) when M →∞is
a traditional result [NAS 81]. Figure 6.2 represents xλ, a vector of length M = 400,
obtained for the “best” value of λ in the L1 sense, i.e., the value that minimizes
Figure 6.2. Smooth linear estimate xλ obtained as the minimizer of equation (6.12), for
M = 400 and for the optimum value of λ in the L1 sense: C(xλ, x⋆) = 18.16%

Deconvolution of Images
147
C(xλ, x⋆), with:
C(x, x⋆) =
M

m=1
|xm −x⋆(m/M)| /
M

m=1
|x⋆(m/M)| .
This procedure for choosing hyperparameters is artiﬁcial, since it requires x⋆to be
known, but it allows relatively fair comparisons among methods for estimating the
signal we are looking for.
The solution xλ (Figure 6.2) is not satisfactory, as it is uniformly smooth. In
comparison, a simple piecewise linear interpolation of successive data gives an error
of the norm L1 of 17.04%, which is notably lower.
If the number I and the positions τ = [τ1, . . . , τI] of the discontinuities were
known, a suitable approach would be to replace criterion J by:
Jτ (x) = ∥y −Hx∥2 + λ
I

i=0
 τi+1
τi
(x′′(s))2 ds,
with τ0 = 0 and τI+1 = 1, a criterion to be minimized in x without the hypothesis of
derivability in τ1, . . . , τI. (This is why we do not write the sum of the integrals in the
form
 1
0 (x(s)′′)2 ds.) Similarly, in 2 dimensions, if the discontinuities form a known
set Γ of curves in the plane, regularity is imposed everywhere except in Γ. This idea
can be generalized to any dimension d for a set Γ ⊂Ω of dimension d −1.
The discrete equivalent can be expressed as:
Jℓ(x) = ∥y −Hx∥2 + λM 3
M−1

m=2
(1 −ℓm) (2xm −xm−1 −xm+1)2 ,
(6.13)
where ℓ= [ℓ2, . . . , ℓM−1] is a binary vector of edge variables: ℓm = 1 corresponds
to the presence of a discontinuity1 at position m.
In practice, this approach is very limited as our ignorance of the positions of the
discontinuities is an integral part of the problem. The following sections are devoted
to the main ideas and tools introduced in the signal and image processing community
to deal with the question of restoration of piecewise regular functions.
1. Note that these “second order” discontinuities correspond to breaks in the slope. The breaks
in the intensity of x⋆(Figure 6.1) thus correspond to two consecutive slope breaks: for example,
ℓm = ℓm+1 = 1 for a break in intensity between xm and xm+1.

148
Bayesian Approach to Inverse Problems
6.3. Detection-estimation
6.3.1. Principle
forward, both theoretically and practically [BLA 87, GEM 84, MUM 85, TER 83].
The idea was put forward of considering the problem of estimating x and that of
tinuities, according to the context) jointly. To do this, joint minimization of criteria
such as Jτ(x) in (x, τ) or Jℓ(x) in (x, ℓ) is not adequate. It is not difﬁcult to see
that this strategy leads to a maximum number of discontinuities (i.e., Γ = Ω in con-
tinuous and ℓ= [1, . . . , 1]T in discrete). This does not happen if a “price to be paid”
α > 0 is imposed per discontinuity introduced [EVA 92, MUM 85]. The result is an
augmented criterion:
K(x, ℓ) = Jℓ(x) + α
M−1

m=2
ℓm
(6.14)
in the discrete univariate case and, in the continuous univariate case,
K(x, τ) = Jτ (x) + αI.
(6.15)
In the functional case of dimension 2, the penalization of the discontinuities becomes
proportional to the total length of the curves making up Γ. More generally, in di-
mension d, it can be written αHd−1(Γ), where Hd−1 is the Hausdorff measure of
dimension d −1 [EVA 92, MUM 85].
In equations (6.14) and (6.15), the penalty depends only on the number of discon-
tinuities, not on their relative positions: they are said to be uncoupled. Variants can
Figure 6.3. Smooth piecewise estimate xα;λ obtained as the joint minimizer of equation (6.14)
for optimum values of λ, α in the L1 sense: C(xα;λ, x⋆) = 16.22%
In the mid-1980s, the controlled management of discontinuities made a big step
detecting the discontinuities (in the form of τ, ℓor, more generally, set Γ of discon-

Deconvolution of Images
149
be envisaged; for example, it could be decided that nearby discontinuities should be
over-penalized by introducing a sliding price to be paid that falls with increasing dis-
tance between neighboring discontinuities. When the penalty depends on the relative
positions of the discontinuities, the edge variables are said to be interactive. In im-
age segmentation, speciﬁc models of interactive variables have been introduced (see
Chapter 7).
Finally, we can point out several characteristics common to works such as
[BLA 87, GEM 84, MUM 85, TER 86] in image restoration and computer vision:
– The image discontinuities are modeled explicitly, in the form of a set in the con-
tinuous setting, and more simply by Boolean variables in the discrete setting. These
variables can be qualiﬁed as hidden (with respect to the observation procedure) in as
far as they are excluded from observation equation (6.1). Conceptually and practically,
the use of hidden variables provides a wide variety of tools for taking prior knowledge
into consideration in a penalized form.
– In most cases, the augmented criterion is half-quadratic (HQ): a function K is
said to be HQ if it depends on two sets of variables, say x and ℓ, in such a way that
K is quadratic in x but not in (x, ℓ). Numerous works in image restoration have
exploited this characteristic recently and section 6.5 is devoted to it.
6.3.2. Disadvantages
The calculating cost is the main practical disadvantage of detection-estimation.
Without mentioning the continuous case, numerically heavy methods are already nec-
essary to correctly handle the combinatory problem induced by the introduction of the
binary variables in the discrete case. Most of these methods are based on the principle
of relaxation, in a stochastic (simulated annealing; see [GEM 84] and Chapter 7) or
deterministic (continuation methods such as graduated non-convexity – [BLA 87]; see
also [NIK 98, NIK 99]) framework. Figure 6.3 was calculated by GNC deterministic
relaxation.
Several authors [BOU 93, LI 95] have stressed the lack of stability of the estimate
xα;λ as another weakness of detection-estimation. This lack of stability comes from
the fact that xα;λ is not a continuous function of the data. In other words, Hadamard’s
third condition is not satisﬁed by xα;λ, and the problem is still ill-posed despite the
penalization. Figure 6.4 illustrates this aspect, following an example given in [LI 95].
In fact, xα;λ is a piecewise continuous function of the data, a behavior that is in-
trinsic in the discontinuity detection capacity of such an approach. In fact, solving a
decision problem with multiple hypotheses comes down to partitioning the data space
(here,
 N) into as many regions Ek as there are competing hypotheses [BRÉ 94].
Ek is deﬁned as the subset of the values of y for which the hypothesis adopted is the

150
Bayesian Approach to Inverse Problems
Figure 6.4. Instability of xα;λ as a function of the data: continuous line, two estimates
xα;λcoming from the same data set, except for one value. The data are represented by circles
for one set and by crosses for the other. Parameters λ and α are kept constant
kth according to the decision rule considered. Here, there are as many hypotheses as
possible values of ℓ, i.e., 2M−2, and xα;λ is continuous when y is inside any Ek (for
ﬁxed ℓ, xα;λ minimizes quadratic criterion (6.13)). In other words, the discontinuities
occur only at the edges of subsets, i.e., when the variation of the data causes a change
of decision. In certain cases, far from being a disadvantage, this characteristic is de-
sirable since it testiﬁes to automatic decision making. On the other hand, if the signal
restoration is only a decision-making aid for a human expert, detection-estimation is
doubtless not to be recommended as it is both risky and numerically costly.
6.4. Non-quadratic approach
Penalized non-quadratic approaches took on increasing importance in signal and
image restoration just before the mid 1990s [BOU 93, KÜN 94, LI 95]. The principle
is to replace Tikhonov quadratic penalization by another even function better able to
preserve discontinuities. For example,
Jφ(x) = ∥y −Hx∥2 + λ
 1
0
φ(x′′(s)) ds
(6.16)
is a generalization of (6.11) for the continuous 1D case, and similarly:
Jφ(x) = ∥y −Hx∥2 + λ
M
M−1

m=2
φ
2xm −xm−1 −xm+1
1/M 2

(6.17)

Deconvolution of Images
151
generalizes (6.12) for the discrete 1D case. Or again,

Ω
φ(∥∇x(s)∥) ds
(6.18)
generalizes (6.4) for the continuous 2D case, and:
ν2
M

m=2
N

n=1
φ
xm,n −xm−1,n
ν

+ ν2
M

m=1
N

n=2
φ
xm,n −xm,n−1
ν

(6.19)
is a possible generalization of (6.7) for the discrete 2D case (if necessary, parameter
ν is a discretization step). Penalty function structure (6.19) is the most usual in image
restoration practice. As [AUB 97, section 2] points out, it does not correspond to a
discretized version (with ﬁnite differences) of equation (6.18) but rather to:

Ω
φ
∂x(s)
∂s1

ds +

Ω
φ
∂x(s)
∂s2

ds,
which is not invariant under rotation of the image axes. As for equation (6.18), an
associated discrete scheme can be written:
ν2
M

m=2
N

n=2
φ
 1
ν
#
(xm,n −xm−1,n)2 + (xm,n −xm,n−1)2

.
(6.20)
Starting from (6.19) and to approach a model that is invariant under rotation, it is more
usual to add supplementary diagonal terms [HUR 96]:
ν2
M

m=2
N

n=2
φ
xm,n −xm−1,n−1
ν
√
2

+ ν2
M

m=2
N

n=2
φ
xm−1,n −xm,n−1
ν
√
2

, (6.21)
than to opt for criterion (6.20). On this subject, see [BLA 87, section 6.1.1].
The essential thing is thus to choose the behavior of function φ. In order to preserve
the edges between homogeneous regions and thus to authorize large variations of the
estimate at some points, φ must increase more slowly than a parabola. The choice
of φ and the related arguments are ﬁnally very close to the use of robust norms in
statistics [HUB 81, REY 83]. Two groups of functions have mainly been proposed in
the literature.
L2L1 functions
These are even, non-constant functions that are convex C1, C2 at 0 and asymptot-
ically linear. A simple, often used example is the branch of the hyperbola (Figure 6.6c
and [CHA 97]):
φ(u) =

η2 + u2, η > 0.

152
Bayesian Approach to Inverse Problems
The minimizer of (6.17) that corresponds to this choice is represented in Figure 6.5.
It provides a fairly good compromise between restoration of the smooth areas and
preservation of the edges.
Other families of convex functions give a qualitatively close result, for example
the norms Lp [BOU 93]:
φ(u) = |u|p , 1 ≤p < 2
(which are not C2 at 0, and even not differentiable for p = 1, and increase faster than
the linear regime), or again, the “fair” function [REY 83], which, although it remains
convex, increases a little more slowly than the linear regime:
φ(u) = |u/η| −log(1 + |u/η|), η > 0.
Certain, more rare contributions propose penalizations that are convex but do not
act on the derivatives or ﬁnite differences of the function to be estimated. Worth noting
are functions inspired from entropy [O’S 95], such as, for the restoration of positive
images:
M

m=2
N

n=1
(xm,n−xm−1,n) log xm−1,n
xm,n
+
M

m=1
N

n=2
(xm,n−xm,n−1) log xm,n−1
xm,n
. (6.22)
Figure 6.5. Smooth, nonlinear estimate xη,λ obtained as the minimizer of equation (6.17),
with φ(u) =

η2 + u2 for optimal values of λ and s in the L1 sense:
C(xη,λ, x⋆) = 16.57%
L2L0 functions
These are non-constant, even functions, C2 at 0, increasing on
 + and asymp-
totically constant. In consequence, the price to be paid for a variation of intensity

Deconvolution of Images
153
(a) u2
(b) |u|1,2
(c)

η2 + u2 −η
(d) |u|
η −log

1 + |u|
η

(e)
u2
η2 + u2
(f) min{u2, η2}
Figure 6.6. Examples of penalizing functions, classiﬁed by rapidity of increase at inﬁnity.
Among the ﬁrst four, which are convex, (c) and (d) are L2L1.
The last two, non-convex, are L2L0
increases with its amplitude, but tends towards a constant. A typical example, given
in [GEM 87] and shown in Figure 6.6e, can be written:
φ(u) =
u2
η2 + u2 , η > 0.
L2L0 functions are not convex and the global minimization of functional (6.16) does
not necessarily have a mathematical sense. However, in the discrete setting, the global
minimization of function (6.17) is always mathematically well-deﬁned, as it is posed
in a space of ﬁnite dimension. From a practical point of view, of course, the possible
existence of local minima makes global minimization of function (6.17) much more
difﬁcult. The minimizer of (6.17) is not represented here for the example treated;
its behavior is very similar to that of the solution obtained by detection-estimation
(Figure 6.3).
Variants are sometimes recommended in the literature, e.g. [GEM 92] advocates
even, convex functions, increasing on
 +, which are thus non-derivable at 0, such
that:
φ(u) =
|u|
η + |u|, η > 0.

154
Bayesian Approach to Inverse Problems
Comparisons
In practice, it is observed that the two classes, L2L1 and L2L0, give very different
solutions in terms of behavior and computing cost.
On the one hand, the L2L1 approach ensures the convexity of criteria Jφ and Jφ.
This property ensures the existence of a unique global minimum for equation (6.16)
(in an appropriate space, such as the space BV (Ω) of functions with bounded vari-
ations [EVA 92]). The same is true for equation (6.17). Moreover, the convex crite-
ria do not admit local minima, the convergence towards a global minimizer of equa-
tion (6.17) is thus proved for standard minimization algorithms (of the gradient type
or for descent coordinate by coordinate, e.g. [BER 95]). Another interesting prop-
erty of the solution thus obtained is its “stability” [BOU 93, KÜN 94, LI 95], that is to
say, it satisﬁes Hadamard’s third condition, unlike the solutions obtained by detection-
estimation.
On the other hand, the L2L0 approach shares the characteristics of the “detection-
estimation” approach: the restored boundaries are very clear, the solution is only
piecewise stable depending on the data and hyperparameters – see Figure 6.7; and
the computing cost of algorithms that avoid the possible local minima is high. The
similarity of the characteristics of the L2L0 and “detection-estimation” approaches is
not an accident, as the next section shows.
J(x)
x1 = x
x2
(a)
J(x)
x1
x2 = x
(b)
Figure 6.7. A small deformation of a multimodal criterion can completely change the position
of the minimizer, when a local minimum becomes global. That is why the behavior of an
estimate deﬁned as the minimizer of a non-convex criterion can change abruptly after a slight
modiﬁcation of the data or the setting of the hyperparameters
6.4.1. Detection-estimation and non-convex penalization
Blake and Zisserman [BLA 87] observed that a HQ criterion (see section 6.3.1)
with uncoupled edge variables such as equation (6.14) could be considered as an aug-
mented equivalent of a certain non-quadratic criterion J(x), in the sense that:
min
ℓ∈{0,1}M−2 K(·, ℓ) = J.

Deconvolution of Images
155
Qualifying criterion K as augmented stresses the fact that it depends not only on x,
but also on auxiliary variables ℓ, whereas J only depends on x. More precisely, from
equations (6.13) and (6.14), we have:
min
ℓ∈{0,1}M−2 K(x, ℓ)
= ∥y −Hx∥2 +
M−1

m=2
min
ℓm∈{0,1}

λM 3(1 −ℓm) (2xm −xm−1 −xm+1)2 + αℓm

= ∥y −Hx∥2 +
M−1

m=2
min
	
α, λM 3 (2xm −xm−1 −xm+1)2
= Jφη(x),
where Jφη is deﬁned by equation (6.17) for φη(u) = min{u2, η2}, i.e., for the trun-
cated quadratic function represented in Figure 6.6f, with η =

Mα/λ. Conse-
quently, J = Jφη and K have the same minimum and, more importantly, the same
minimizer x on all closed X of
 M, in the sense that:
x minimizes J on X ⇐⇒∃ℓ/ (x,ℓ) minimizes K on X × {0, 1}M−2.
To ﬁnd x in practice, we therefore have the choice of concentrating on the minimiza-
tion of J or on that of K. Blake and Zisserman deﬁne K as the detection-estimation
criterion, deducing the expression for J from that for K and dealing with the prob-
lem of minimizing the non-convex criterion J by graduated non-convexity rather than
attacking the combinatory problem posed by the minimization of K.
6.4.2. Anisotropic diffusion by PDE
In the same way as a strong connection can be established between isotropic dif-
fusion by PDE and quadratic penalization (see section 6.2.2), it is also possible to
link anisotropic ﬁltering of the image by PDE and the minimization of non-quadratic
functional criteria (see [TEB 98] and other articles in the same special issue). The
ﬁrst anisotropic PDE of images, i.e., taking the edges into consideration, was that
introduced by Perona and Malik [PER 90]. The idea is to encourage diffusion in
zones with weak gradients (corresponding to homogeneous areas), while preserving
the areas with strong gradients (corresponding to edges). This anisotropic diffusion is
formally expressed as follows2:
∂x(s, t)
∂t
= div

c(∥∇x(s, t)∥) ∇x(s, t)

,
2. div = d
i=1 ∂/∂si is the spatial divergence.

156
Bayesian Approach to Inverse Problems
with conditions (6.6). The coefﬁcient of conduction c(.) is chosen as one in the uni-
form zones (with weak gradients) and tends to zero in the zones of strong gradients.
Diffusion is thus delayed at the edges. Here again, the process has to be stopped after
a certain time and this plays the role of a regularization parameter.
If a ﬁdelity to data term is introduced in the PDE, as proposed by Nordström
[NOR 90], the solution is likely to converge towards a suitable stationary state, from
any initial condition x(s, 0):
∂x(s, t)
∂t
= μH∗
y(s) −Hx(s, t)

+ div

c(∥∇x(s, t)∥) ∇x(s, t)

.
(6.23)
This dynamic equation has similarities with the use of the penalizing functional (6.18).
Euler’s equation expressing a necessary condition for obtaining an extremum of (6.18)
can be written:
H∗(Hx −y) −λ div
φ′(∥∇x∥)
2 ∥∇x∥∇x

= 0.
This equation is veriﬁed by any asymptotic solution x(s, +∞) of (6.23) if:
μ = 1/λ and c(t) = φ′(t)/2t.
(6.24)
Functions c proposed by Perona and Malik and the corresponding φ are:
c(t) = exp

−t2
=⇒φ(t) = 1 −exp

−t2
+ const.
c(t) =
1
1 + t2
=⇒φ(t) = log

1 + t2
+ const.
The ﬁrst function φ is L2L0, the second is a non-convex function (but with no asymp-
tote) proposed in [HEB 89] for regularizing a reconstruction problem in medical imag-
ing. In the case of a non-convex function φ, note that the intervention of a PDE such
as (6.23) does not provide a “miracle” method for ensuring the existence of a global
minimizer and performing the calculation.
6.5. Half-quadratic augmented criteria
The use of HQ criteria has recently imposed itself as a powerful numerical tool for
image restoration with preservation of discontinuities [BRE 96, CHA 94, CHA 97,
VOG 98]. As mentioned above in the framework of the “detection-estimation” ap-
proach, the HQ criteria ﬁrst included exclusively binary edge variables that were ei-
ther interactive or uncoupled. Following [GEM 92], this section shows that the HQ
criteria cover a much larger domain. More precisely, many penalized, non-quadratic
approaches admit an equivalent formulation in the HQ framework. This has a double
consequence: from a formal point of view, this equivalence leads to a better under-
standing of the real choices of signal and image models; from a practical point of view,
the HQ formulation furnishes new algorithmic tools for minimizing non-quadratic cri-
teria.

Deconvolution of Images
157
6.5.1. Duality between non-quadratic criteria and HQ criteria
D. Geman et al. [GEM 92, GEM 95] generalized Blake and Zisserman’s construc-
tion (section 6.4.1) to non-discrete auxiliary uncoupled variables. By reversing the
construction order, they showed the existence of augmented HQ criteria K for broad
classes of non-quadratic criteria J, in the sense where:
inf
b∈B K(·, b) = J
(6.25)
for a set B to be deﬁned, in general different from {0, 1}. In a form that was sub-
sequently extended [CHA 97, IDI 01], this construction applies to functions φ that
satisfy the following hypotheses:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
φ is even,
φ(√·) is concave on
 +,
φ is continuous at zero and C1 on
  \ {0}
(6.26)
Under these conditions, the following duality relations can be obtained from conven-
tional convex analysis results [ROC 70]:
φ(u) = inf
b∈+

bu2 + ψ(b)

,
(6.27)
ψ(b) = sup
u∈

φ(u) −bu2
.
From equations (6.17) and (6.27), it is easy to deduce that equation (6.25) is true
for the augmented HQ criterion:
K(x, b) =
∥y −Hx∥2 + λM 3
M−1

m=2
bm (2xm −xm−1 −xm+1)2 + λ
M
M−1

m=2
ψ(bm). (6.28)
Formally, we observe that the ﬁrst part of this criterion can be identiﬁed with J1−b(x),
where Jℓhas been deﬁned for binary ℓby equation (6.13). To within a multiplying
factor, the auxiliary variables bm play the role of 1 −ℓm in equations (6.13)-(6.14),
but the bm are not binary.
Whether they are convex or not, most of the functions φ proposed in the literature
satisfy (6.26). For example, we have:
φ(u) =

η2 + u2, η > 0 =⇒ψ(b) =

η2b + 1/4b
if b ∈]0, b∞= 1/2η],
1
if b ≥b∞.
φ(u) =
u2
η2 + u2 , η > 0
=⇒ψ(b) =

(1 −η
√
b)2
if b ∈]0, b∞= 1/η2],
1
if b ≥b∞.

158
Bayesian Approach to Inverse Problems
6.5.2. Minimization of HQ criteria
6.5.2.1. Principle of relaxation
[GEM 92] points out the structural advantages connected with half-quadratic prop-
erties for the minimization of the augmented criterion K rather than J. More precisely,
we proceed by relaxation, i.e., by considering the sub-problem with ﬁxed x, then ﬁxed
b alternately until convergence is obtained. These two sub-problems are simple since
one is quadratic and the other separable. [GEM 92] is speciﬁcally concerned with the
case where φ is non-convex and develops an HQ version of simulated annealing based
on the principle of alternate sampling.
Several other authors [BRE 96, CHA 94, CHA 97, VOG 98] have taken an interest
in the deterministic counterpart, which is well suited to the case where φ is convex.
Starting from an initial pair (x, b), they intertwine minimization of x with b ﬁxed and
minimization of b with x ﬁxed:
1 augmented criterion K(x, b) is a quadratic function of x. Calculating its mini-
mizer for x with b ﬁxed is therefore a simple problem of inverting a linear system. It
is worth noting that this step corresponds to an adaptive version of Tikhonov regular-
ization, in as much as the regularization parameter in front of each quadratic penalty
term in expression (6.28) is multiplied by a factor bm;
2 criterion K(x, b) being a separable function of variables bm, it can be mini-
mized with respect to b in a parallel form. Moreover, the updating equation for each
bm is explicit: it can be shown that the inﬁmum of equation (6.27) is reached for3:
∀u ̸= 0, b(u) = φ′(u)/2u,
(6.29)
prolonged by continuity in 0; the expression for ψ is thus not required for expressing
b(u).
(a)
(b)
1
2

η2 + u2
(c)
η2
(η2 + u2)2
(d)
1 if |u| < η, 0 otherwise
Figure 6.8. Function b(u) = φ′(u)/2u, respectively in the case where φ is:
(a) quadratic (Figure 6.6a); (b) parabolic (Figure 6.6c); (c) the Geman and McClure function
(Figure 6.6e); (d) truncated quadratic (Figure 6.6f)
3. Note that, in this expression of b, the conduction coefﬁcient (6.24) is retrieved.

Deconvolution of Images
159
6.5.2.2. Case of a convex function φ
When φ is convex, it has been shown that this procedure for descent block of co-
ordinates by block of coordinates, given the name of ARTUR in [CHA 94], converges
towards a unique minimum under broad conditions [CHA 97, IDI 01, NIK 05]. In a
very slightly different formulation, belonging to robust estimation, a similar descent
technique (identiﬁed as a reweighted least squares (RLS) method) and the associated
convergence result have been known for a long time [YAR 85].
Although other descent techniques could be envisaged for calculating x
in the same conditions of regularity (e.g. conjugate gradient algorithm tech-
niques [LAB 08]), the RLS approach has a remarkable advantage. If the inverse
problem under consideration has previously been solved by Tikhonov quadratic pe-
nalization, the non-quadratic extensions can be obtained by trivial adaptation:
– introduce adaptive regularization into Tikhonov’s method, which thus becomes
step 1 of the RLS method;
– add step
2 to update auxiliary variables (6.29) (this is the only step that brings
in the expression for φ); loop.
6.5.2.3. Case of a non-convex function φ
When the function φ is not convex, the RLS technique converges, if the minima are
isolated, towards one of the local or global minima of J [DEL 98], such as other sim-
ple deterministic descent algorithms. It is thus necessary to use numerically heavier
methods such as simulated annealing or GNC to avoid the local minima.
In the same way as there are versions of simulated annealing that take advantage
of the half-quadratic nature of the augmented criterion [GEM 92], there is a certain
interest in using an RLS version of GNC. Let us consider two functions φ0 and φ1,
which are L2L0 and L2L1respectively. Let us take φθ = (1−θ)φ0 +θφ1 and suppose
that θ goes from one to zero in small increments. We then ﬁnd an interesting property:
in step
2 of the RLS, the expression for updating (6.29) based on φθ is a simple
linear combination of the expressions valid for φ0 and φ1 respectively (whereas the
dual function ψθ is not a linear combination of ψ0 and ψ1). Finally, as step 1 does not
depend on φθ, we thus immediately obtain an RLS version of GNC. The 1D estimate
of Figure 6.3 was calculated using this principle.
6.6. Application in image deconvolution
6.6.1. Calculation of the solution
In this part, we will concentrate on the practical problem of ﬁnding the minimizer
of a criterion of the type:
J(x) = ∥y −Hx∥2 + λΦ(x),
(6.30)

160
Bayesian Approach to Inverse Problems
where x is a discrete rectangular image composed of M × N pixels, represented in
the form of a column vector (of length M × N), concatenating the columns of the
image. This arrangement is an algebraic device that is usual in imaging and allows the
convolution of two vectors to be represented as the product of a matrix by a vector.
The structure of convolution matrix H is thus Toeplitz-block-Toeplitz (see Chapter 4,
section 4.3.2).
Let us choose a penalization Φ(x) such as (6.19) or (6.20). If function φ is chosen
to be quadratic, (6.19) and (6.20) are identical and their common minimizer in
 MN
is a linear function of the data (Chapter 2, section 2.2.2), the calculation of which is
dealt with in Chapter 4. If the function φ is chosen to be L2L1, (6.19) and (6.20) are
different criteria, both convex. The minimization of (6.30) then comes under the case
treated in section 2.2.3 of Chapter 2. We recall that the minimizer x (if we assume it
is unique, which is the case if (6.30) is strictly convex) is not, in general, an explicit
function of the data, but that numerous iterative techniques based on successively
decreasing the criterion engender algorithms that converge towards x.
Conceptually, descent coordinate by coordinate (also called the Gauss-Seidel me-
thod, or relaxation, or ICM for iterated conditional modes, or ICD for iterative co-
ordinate descent) is certainly the simplest. It is a question of cyclicly solving the
sub-problem of minimizing J(x) as a function of pixel xmn, the other pixels being
ﬁxed at their current values. One iteration of the algorithm corresponds to a com-
plete scan of the image. Although hardly recommended as a generic minimization
method [PRE 86, p. 303], this method is not always the slowest in the imaging con-
text [SAU 93].
In general, the most efﬁcient algorithms are those that exploit the convolutive na-
ture of the problem. For example, in a gradient descent method, the gradient vector
of the term ∥y −Hx∥2, which is written 2HT (Hx −y), can be calculated by fast
convolution in the Fourier domain, or in the direct domain by taking advantage of the
sparse nature of H. In the same way, it is possible to use the fact that a ﬁnite difference
operator is a convolution. More generally, “modern” algorithms draw proﬁt from the
fast techniques developed for inverting linear systems with speciﬁc structures, partic-
ularly of the convolutive type. These techniques are sometimes used as they stand,
or use “elementary bricks” in an iterative structure, e.g. of the weighted least squares
type such as the ARTUR algorithm presented in section 6.5.2.2. Worthy of special
mention is the recent appearance of preconditioning in image restoration [VOG 98].
LEGEND is an algorithmic structure close to ARTUR [CHA 94, IDI 01, ALL 06]
(see also [YAR 85] in the robust estimation framework, [AUB 97, AUB 06] in the
functional case and [GEM 95] for a simulated annealing version). Like ARTUR, LEG-
END minimizes a HQ augmented criterion. It is in its special structure that the interest

Deconvolution of Images
161
of LEGEND lies. Let us consider a penalized criterion written:
J(x) = ∥y −Hx∥2 + λ

i∈I
φ(vT
i x),
(6.31)
where I = {1, . . . , I} (I < +∞) allows us to arbitrarily number the column vectors
vi ﬁxed, for example, to engender ﬁnite differences such as (6.17), (6.19), (6.21). An
iteration of the LEGEND algorithm is written:
x(k) = (2HT H + λVVT )−1
2HT y + λVb(k)
,
(6.32)
where b(k) is a column vector of size I calculated component by component:
b(k)
i
= vT
i x(k−1) −φ′(vT
i x(k−1)),
(6.33)
and V = [v1, . . . , vI]. The normal matrix M = 2HTH + λVVT is taken to be
invertible. Note that one iteration (6.32) performed for b(k) = 0 is sufﬁcient to mini-
mize (6.31) if φ is quadratic. If φ is non-quadratic, several iterations will be necessary
to converge to a ﬁxed point. The computing cost of (6.33) is assumed to be negli-
gible, so the cost of each iteration remains that of the inversion of the linear system
(6.32), the normal matrix of which does not vary during the iterations, a property that
is speciﬁc to structures of the LEGEND type.
Depending on the case, matrix V is Toeplitz in 1D, or Toeplitz-block-Toeplitz
in 2D, or made up of blocks having these structures (the exact structure of V also
depends on the boundary conditions at ∂Ω). Similarly, the convolution matrix H is
Toeplitz or Toeplitz-block-Toeplitz. Matrix M thus has a structure close to Toeplitz,
and the fast calculation of (6.32) can be envisaged.
Convergence towards the minimizer of (6.30), is guaranteed for all x(0) if φ is a
convex function such as an L2L1function [IDI 01, NIK 05]. Local convergence is also
ensured in a broader context [ALL 06].
6.6.2. Example
To illustrate this, let us calculate the minimizer of criterion (6.30), penalized in
the form (6.19) (with ν = 1), in the case of an L2L1 φ function, for an image de-
convolution problem. The original image x⋆(202 rows, 99 columns, Figure 6.9c) is
quantiﬁed on 256 grey levels. Blurring is Gaussian circular, with a standard deviation
equivalent to one pixel, and the convolved image (190 rows, 87 columns, Figure 6.9a)
has additive white Gaussian noise with a standard deviation of 3, corresponding to a
SNR of 33 dB. φ is the Huber function [HUB 81], a C1 function, deﬁned piecewise:
φ
Huber
η
(u) =
 u2
if |u| ≤η,
2η |u| −η2
otherwise.
(6.34)

162
Bayesian Approach to Inverse Problems
LEGEND is an appropriate algorithm for calculating the minimizer xη,λ. As con-
volution by a circular Gaussian kernel is separable (i.e., can be decomposed into
two 1D convolutions, one horizontal and one vertical), the algorithm proposed in
[BRE 96], which is similar to ARTUR whilst making use of the separability, is also
suitable. It is this algorithm that served to calculate the restored images of Figures 6.9d
to f. Note that a conjugate gradient algorithm would also be a valid option [LAB 08].
For a quantitative evaluation of the restoration quality according to the hyperpa-
rameters (η, λ), it is known that the norm L1 of the error is often preferable to the
norm L2. Empirically, the quasi-norm L1/2:
c(η, λ) ∝
xη,λ −x⋆
1/2 =
.
i,j
#
xη,λ
i,j −x⋆
i,j
/2
seemed even better to us. In itself, this choice is not essential. The stakes are lim-
ited in comparison with the problem of non-supervised adjustment of the parameters,
considered in Chapter 8. In addition, good restitution of some local details that are
important from a visual standpoint is not necessarily taken into account in a global
quality measure.
The interest of the trace of c(η, λ), Figure 6.9b, is therefore primarily didactic. To
obtain this, we proceeded by successive lines (λ constant) and from right to left (s
decreasing), initializing each minimization with the result of the previous one. For
η = 100, the calculation was initialized by linear inversion with the circulant approx-
imation. When s is large (on the scale of differences of intensity between neighbor-
ing pixels), criterion (6.19) becomes quadratic, and thus easily minimizable. When
s decreases, the criterion is less and less convex and would eventually become non-
differentiable for η = 0. As a method for minimizing non-differentiable criteria, this
procedure is known as a regularization method [GLO 81].
On the bivariate function c(η, λ), the “right” settings form a relatively narrow val-
ley that separates the two over-regularized solutions. The points of the valley are not
all equivalent: the restored edges are all the sharper when threshold η is chosen to
be small. The optimum threshold in the sense of the norm L1/2 is η = 9. This set-
ting corresponds quite well to a visual judgement: the restored image contains few
artefacts except for the blocky effect on the derrick, hoist and the V-shaped mooring
cable, oriented along the diagonals of the image. A penalization less sensitive to the
orientation, such as (6.20) or the sum of (6.19) and (6.21), would doubtless correct
this fault.
The best setting for λ for a very small threshold (η = 1) produces a slightly “over-
segmented” restoration which seems less “natural”. This effect is not ﬂagrant on the
image tested as it shows up in shaded areas, which are few in this image (inﬂatable

Deconvolution of Images
163
(a) Blurred and noisy
image y
  1
  2
  5
 10
 20
 50
100
0
0.5
1
1.5
2
2.5
3
3.5
4
Threshold η  (log scale)
Regularization parameter λ
(b) c(η, λ) ∝
xη,λ −x⋆
1/2
• c(9, 0.54)
= 4.24 ⇝(d)
◦c(100, 0.27) = 4.30 ⇝(e)
∗c(1, 4)
= 4.34 ⇝(f)
(c) Original image x⋆
(d) x9,0.54
(e) x100,0.27≃x∞,0.27
(f) x1,4
Figure 6.9. Synthetic image-deconvolution problem, approached by minimization of a convex
criterion such as (6.30), where Φ is deﬁned by (6.19) for ν = 1 and φ is the Huber function
(6.34). The error norm in the L1/2 sense, image edges excluded,was adopted to assess the
quality of the restoration; (b) represents the level lines of
c(η, λ) = ∥xη,λ −x⋆∥1/2/(number of pixels), spaced at 0.5. The minimum value, indicated
by a black spot, is 4.24; it corresponds to (d).The quadratic case is obtained for η →∞; in
this case, the minimum value, indicated by a white spot, is 4.30; it corresponds to (e). In
contrast, we obtain the case L1 for η = 0; for the smallest grid value, η = 1, the minimum
value, indicated by a star, is 4.34; this corresponds to (f)

164
Bayesian Approach to Inverse Problems
dinghy and lower part of the hull). We could envisage correcting this defect by pe-
nalizing differences of order two rather than order one. Nevertheless, the choice of a
threshold η that is systematically small, or even zero, is not really desirable unless the
aim is to segment the image rather than restore it.
6.7. Conclusion
Within the framework of penalized methods for the restoration of objects that are
regular “almost everywhere” such as natural images, three types of approach have
been presented.
Tikhonov’s original approach corresponds to quadratic penalization. This is both
the oldest and the least costly, as it provides a solution by simple inversion of a linear
system. Except in favorable situations (highly redundant data with little noise), it is not
satisfactory for restoring piecewise homogeneous objects. In the examples illustrating
this chapter, the method reaches, at best, an error norm that is not very competitive, of
18.16% (Figure 6.2).
Convex non-quadratic penalization gives robust estimators that demonstrate a
much better capacity for preserving the boundaries between regions. The error norm
attained becomes 16.57% (Figure 6.5). Moreover, these estimators can be calculated
iteratively by ﬁxed point methods, e.g. gradient methods. Other ﬁxed point descent
methods were proposed in the 1990s [BRE 96, CHA 94, CHA 97]. They work on
half-quadratic augmented forms of the criterion to be minimized. They have the inter-
esting structure of quasi-Newton, ﬁxed-step algorithms [VOG 98], with the guarantee
of convergence [CHA 97, IDI 01, NIK 05, ALL 06].
The detection-estimation approach is the equivalent half-quadratic formulation for
a non-convex penalized approach (more accurately, using truncated quadratic penal-
ization functions). This is the only one that has a real capacity for detecting boundaries
between distinct regions. In the 1D example that illustrates this chapter, this capacity
is expressed by a slight decrease in the L1 error norm: 16.22% (Figure 6.3) instead of
16.57% for convex penalization. However, this decrease is obtained at an increased
computing cost. Moreover, it becomes more difﬁcult to set the hyperparameters em-
pirically because of the instability of the solution (as it is a discontinuous function of
the data, it is not generally continuous as a function of the hyperparameters).
It is interesting to end on a “historical” note. Since the pioneering work of the
1960s and 1970s, the evolution of the methods commonly studied and applied in sig-
nal and image restoration has not always been towards increasing complexity. Linear
ﬁltering methods (seen in more detail in Chapter 4), associated with the minimiza-
tion of quadratic criteria, gave way, after the Geman brothers’ work [GEM 84], to
much more costly methods based on discrete hidden variables and taking simulated

Deconvolution of Images
165
annealing as the optimization technique. Only in the 1990s did the much less expen-
sive use of minimization of convex non-quadraticcriteria become preponderant thanks
to works such as [BOU 93], although already present in speciﬁc forms in the 1980s
(e.g. in the maximum entropy framework [MOH 88]).
6.8. Bibliography
[ALL 06] ALLAIN M., IDIER J., GOUSSARD Y., “On global and local convergence of half-
quadratic algorithms”, IEEE Trans. Image Processing, vol. 15, num. 5, p. 1130-1142, May
2006.
[AUB 97] AUBERT G., VESE L., “A variational method in image recovery”, SIAM J. Num.
Anal., vol. 34, num. 5, p. 1948-1979, Oct. 1997.
[AUB 06] AUBERT G., KORNPROBST P., Mathematical Problems in Image Processing: Par-
tial Differential Equations and the Calculus of Variations, vol. 147 of Applied Mathemati-
cal Sciences, Springer Verlag, New York, NY, 2nd edition, 2006.
[BER 95] BERTSEKAS D. P., Nonlinear Programming,
Athena Scientiﬁc, Belmont, MA,
1995.
[BLA 87] BLAKE A., ZISSERMAN A., Visual Reconstruction, The MIT Press, Cambridge,
MA, 1987.
[BOU 93] BOUMAN C. A., SAUER K. D., “A generalized Gaussian image model for edge-
preserving MAP estimation”, IEEE Trans. Image Processing, vol. 2, num. 3, p. 296-310,
July 1993.
[BRÉ 94] BRÉMAUD P., An Introduction to Probabilistic Modeling, Undergraduate Texts in
Mathematics, Springer Verlag, Berlin, Germany, 2nd edition, 1994.
[BRE 96] BRETTE S., IDIER J., “Optimized single site update algorithms for image deblur-
ring”, in Proc. IEEE ICIP, Lausanne, Switzerland, p. 65-68, 1996.
[CHA 94] CHARBONNIER P., BLANC-FERAUD L., AUBERT G., BARLAUD M., “Two deter-
ministic half-quadratic regularization algorithms for computed imaging”, in Proc. IEEE
ICIP, vol. 2, Austin, TX, p. 168-172, Nov. 1994.
[CHA 97] CHARBONNIER P., BLANC-FERAUD L., AUBERT G., BARLAUD M., “Deter-
ministic edge-preserving regularization in computed imaging”, IEEE Trans. Image Pro-
cessing, vol. 6, num. 2, p. 298-311, Feb. 1997.
[DEL 98] DELANEY A. H., BRESLER Y., “Globally convergent edge-preserving regularized
reconstruction: an application to limited-angle tomography”, IEEE Trans. Image Process-
ing, vol. 7, num. 2, p. 204-221, Feb. 1998.
[EVA 92] EVANS L. C., GARIEPY R. F., Measure Theory and Fine Properties of Functions,
CRC Press, Berlin, Germany, 1992.
[GEM 84] GEMAN S., GEMAN D., “Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images”, IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-6,
num. 6, p. 721-741, Nov. 1984.

166
Bayesian Approach to Inverse Problems
[GEM 87] GEMAN S., MCCLURE D., “Statistical methods for tomographic image reconstruc-
tion”, in Proc. 46th Session of the ICI, Bulletin of the ICI, vol. 52, p. 5-21, 1987.
[GEM 92] GEMAN D., REYNOLDS G., “Constrained restoration and the recovery of disconti-
nuities”, IEEE Trans. Pattern Anal. Mach. Intell., vol. 14, num. 3, p. 367-383, Mar. 1992.
[GEM 95] GEMAN D., YANG C., “Nonlinear image recovery with half-quadratic regulariza-
tion”, IEEE Trans. Image Processing, vol. 4, num. 7, p. 932-946, July 1995.
[GLO 81] GLOWINSKI R., LIONS J. L., TRÉMOLIÈRES R., Numerical Analysis of Variational
Inequalities, Elsevier Science Publishers (North-Holland), Amsterdam, The Netherlands,
1981.
[HEB 89] HEBERT T., LEAHY R., “A generalized EM algorithm for 3-D Bayesian reconstruc-
tion from Poisson data using Gibbs priors”, IEEE Trans. Medical Imaging, vol. 8, num. 2,
p. 194-202, June 1989.
[HUB 81] HUBER P. J., Robust Statistics, John Wiley, New York, NY, 1981.
[HUR 96] HURN M., JENNISON C., “An extension of Geman and Reynolds’ approach to
constrained restoration and the recovery of discontinuities”, IEEE Trans. Pattern Anal.
Mach. Intell., vol. PAMI-18, num. 6, p. 657-662, June 1996.
[IDI 01] IDIER J., “Convex half-quadratic criteria and interacting auxiliary variables for image
restoration”, IEEE Trans. Image Processing, vol. 10, num. 7, p. 1001-1009, July 2001.
[KAL 03] KALIFA J., MALLAT S., ROUGÉ B., “Deconvolution by thresholding in mirror
wavelet bases”, IEEE Trans. Image Processing, vol. 12, num. 4, p. 446-457, Apr. 2003.
[KOE 84] KOENDERINK J. J., “The structure of images”, Biological Cybernetics, vol. 50,
p. 363-370, 1984.
[KÜN 94] KÜNSCH H. R., “Robust priors for smoothing and image restoration”, Ann. Inst.
Stat. Math., vol. 46, num. 1, p. 1-19, 1994.
[LAB 08] LABAT C., IDIER J., “Convergence of conjugate gradient methods with a closed-
form stepsize formula”, J. Optim. Theory Appl., vol. 136, num. 1, Jan. 2008.
[LI 95] LI S. Z., HUANG Y. H., FU J. S., “Convex MRF potential functions”, in Proc. IEEE
ICIP, vol. 2, Washington DC, p. 296-299, 1995.
[MOH 88] MOHAMMAD-DJAFARI A., DEMOMENT G., “Image restoration and reconstruc-
tion using entropy as a regularization functional”, in ERIKSON G. J., RAY S. C. (Eds.),
Maximum Entropy and Bayesian Methods in Science and Engineering, vol. 2, Dordrecht,
The Netherlands, MaxEnt Workshops, Kluwer Academic Publishers, p. 341-355, 1988.
[MUM 85] MUMFORD D., SHAH J., “Boundary detection by minimizing functionals”,
in
IEEE Conf. Comp. Vision Pattern Recogn., San Francisco, CA, p. 22-26, 1985.
[NAS 81] NASHED M. Z., “Operator-theoretic and computational approaches to ill-posed
problems with applications to antenna theory”, IEEE Trans. Ant. Propag., vol. 29, p. 220-
231, 1981.
[NIK 98] NIKOLOVA M., IDIER J., MOHAMMAD-DJAFARI A., “Inversion of large-support
ill-posed linear operators using a piecewise Gaussian MRF”, IEEE Trans. Image Process-
ing, vol. 7, num. 4, p. 571-585, Apr. 1998.

Deconvolution of Images
167
[NIK 99] NIKOLOVA M., “Markovian reconstruction using a GNC approach”, IEEE Trans.
Image Processing, vol. 8, num. 9, p. 1204-1220, Sep. 1999.
[NIK 05] NIKOLOVA M., NG M., “Analysis of half-quadratic minimization methods for signal
and image recovery”, SIAM J. Sci. Comput., vol. 27, p. 937-966, 2005.
[NOR 90] NORDSTROM N., “Biased anisotropic diffusion: a uniﬁed regularization and diffu-
sion approach to edge detection”, Image and Vision Computing, vol. 8, num. 4, p. 318-327,
1990.
[O’S 95] O’SULLIVAN J. A., “Roughness penalties on ﬁnite domains”, IEEE Trans. Image
Processing, vol. 4, num. 9, p. 1258-1268, Sep. 1995.
[PER 90] PERONA P., MALIK J., “Scale-space and edge detection using anisotropic diffu-
sion”, IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-12, p. 629-639, July 1990.
[PRE 86] PRESS W. H., FLANNERY B. P., TEUKOLSKY S. A., VETTERLING W. T., Numeri-
cal Recipes, the Art of Scientiﬁc Computing, Cambridge University Press, Cambridge, MA,
1986.
[REY 83] REY W. J., Introduction to Robust and Quasi-robust Statistical Methods, Springer
Verlag, Berlin, 1983.
[ROC 70] ROCKAFELLAR R. T., Convex Analysis, Princeton University Press, 1970.
[SAU 93] SAUER K. D., BOUMAN C. A., “A local update strategy for iterative reconstruction
from projections”, IEEE Trans. Signal Processing, vol. 41, num. 2, p. 534-548, Feb. 1993.
[STA 02] STARK J.-L., PANTIN E., MURTAGH F., “Deconvolution in astronomy: a review”,
Publ. Astr. Soc. Pac., vol. 114, p. 1051-1069, 2002.
[TEB 98] TEBOUL S., BLANC-FERAUD L., AUBERT G., BARLAUD M., “Variational ap-
proach for edge-preserving regularization using coupled PDE’s”, IEEE Trans. Image Pro-
cessing, vol. 7, num. 3, p. 387-397, Mar. 1998, Special Issue on Partial Differential Equa-
tions and Geometry Driven Diffusion in Image Processing and Analysis.
[TER 83] TERZOPOULOS D., “Multilevel computational process for visual surface reconstruc-
tion”, Comput. Vision Graphics Image Process., vol. 24, p. 52-96, 1983.
[TER 86] TERZOPOULOS D., “Regularization of inverse visual problems involving disconti-
nuities”, IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-8, num. 4, p. 413-424, July
1986.
[TIK 77] TIKHONOV A., ARSENIN V., Solutions of Ill-Posed Problems, Winston, Washington,
DC, 1977.
[TIT 85] TITTERINGTON D. M., “General structure of regularization procedures in image re-
construction”, Astron. Astrophys., vol. 144, p. 381-387, 1985.
[VOG 98] VOGEL R. V., OMAN M. E., “Fast, robust total variation-based reconstruction of
noisy, blurred images”, IEEE Trans. Image Processing, vol. 7, num. 6, p. 813-823, June
1998.
[YAR 85] YARLAGADDA R., BEDNAR J. B., WATT T. L., “Fast algorithms for lp deconvolu-
tion”, IEEE Trans. Acoust. Speech, Signal Processing, vol. ASSP-33, num. 1, p. 174-182,
Feb. 1985.


PART III
Advanced Problems and Tools


Chapter 7
Gibbs-Markov Image Models
7.1. Introduction
Chapter 6 introduced an important class of tools for signal and image restoration
using solutions constructed as minimizers of penalized criteria. For a direct model
y = H(x⋆) + noise, this approach ﬁrst requires choices to be made for:
– a function measuring goodness of ﬁt Ψ(y−H(x)), which was taken as quadratic
in Chapter 6;
– a regularizing function Φ(x);
– a value for hyperparameter λ that governs the compromise between regulariza-
tion and goodness of ﬁt.
With the proviso that it exists and is unique, solution x is thus deﬁned as the minimizer
of a criterion:
J (x) = Ψ(y −H(x)) + λΦ(x).
(7.1)
Several “big” questions come to mind concerning x:
– the choice of Φ was discussed in Chapter 6, essentially on the basis of qualitative
arguments: a signal to be estimated may be “smooth”, another “regular almost every-
where”. Are there more “objective” arguments on which to base the choice of Φ (and
Ψ)?;
Chapter written by Jérôme IDIER.

172
Bayesian Approach to Inverse Problems
– since the solution x depends on the hyperparameter λ and possibly additional
parameters (e.g. the threshold parameter for L2L0 or L2L1 functions), is it possible
to estimate these hyperparameters rather than choosing them empirically?;
– how much conﬁdence can be given to x ? Can a quantiﬁed uncertainty be asso-
ciated with it?
Generally, one of the aims of statistical modeling is precisely to take vague or un-
certain information into account. More speciﬁcally, we saw in Chapter 3 that Bayesian
inference is a natural framework for approaching inverse problems. Many contribu-
tions in image restoration explicitly use this statistical framework [BES 86, BOU 93,
DEM 89, GEM 84]. It is thus legitimate to give more details of how Bayesian infer-
ence contributes to signal and image restoration and to look more particularly at the
extent to which it provides answers to questions that are difﬁcult to deal with in the
deterministic framework of the previous chapter.
7.2. Bayesian statistical framework
Proposing a probabilistic model for a signal means assuming that the signal is
the realization of a random process in the hope of better structuring the available
information by expressing it in the form of statistical characteristics of this process.
Strictly speaking, and in spite of what often seems to be suggested, this does not mean
that we assume the signal itself is random.
Under certain conditions, the Bayesian statistical framework is appropriate for in-
⋆
From now on, unless stated otherwise, we will take it that the observation, noise
and signal are real vectors, respectively y ∈
 N, b ∈
 N and x⋆= x⋆∈
 M:
y = H(x⋆) + b.
(7.2)
The cases in which the signal or observation belongs to a space of inﬁnite dimension
lend themselves only partially to a statistical formulation. It has long been known
that Tikhonov’s approach can be interpreted in Bayesian statistics, even without dis-
cretization [FRA 70]. In contrast, in the case of non-quadratic functionals Ψ or Φ,
the minimization of criterion (7.1) has no systematic statistical interpretation. In sub-
stance, the difﬁculty stems from the fact that the probability of a random process
indexed on a space of inﬁnite dimension is characterized by an inﬁnite collection of
ﬁnite-dimensional distributions. This collection has no direct relation with (7.1) and
does not allow a likelihood function to be deﬁned naturally.
(the noise) and signal x
terpreting (7.1). Adopting this framework means supposing that both the uncertainties
come from random quantities.

Gibbs-Markov Image Models
173
Let us also assume that functions Φ and Ψ verify the normalization condition:
∃Tx > 0, Zx(Tx) =

M e−Φ(x)/Tx dx < +∞,
(7.3)
∃Tb > 0, Zb(Tb) =

N e−Ψ(b)/Tb db < +∞
(7.4)
(Tx and Tb are so-called temperature parameters; their role will become clear later).
We can thus deﬁne functions that have the characteristics of probability densities:
pX(x) = pX(x ; Tx) =
1
Zx(Tx) e−Φ(x)/Tx,
(7.5)
pB (b) = pB (b ; Tb) =
1
Zb(Tb) e−Ψ(b)/Tb.
(7.6)
We then call pX the prior law.
pY |X(y | x) = pB(y −H(x))
(7.7)
designates the likelihood of the data, knowing x, and
pX|Y (x | y) = pY |X(y | x)pX(x)
pY (y)
=
pX,Y (x, y)

M pX,Y (x, y) dx,
(7.8)
the posterior likelihood. The joint law for the pair (X, Y ) can be written:
pX,Y (x, y) = pY |X(y | x)pX(x).
(7.9)
The maximum a posteriori (MAP) is the Bayesian estimator most commonly used in
imaging:
x
MAP = arg max
x
pX|Y (x | y)
(7.10)
= arg min
x
Ψ(y −H(x))/Tb + Φ(x)/Tx
from equations (7.5)-(7.9). Consequently, the MAP is the same as the minimizer of
the penalized criterion (7.1) for λ = Tb/Tx. This traditional identity has already been
demonstrated and commented on in section 3.4.
7.3. Gibbs-Markov ﬁelds
Penalty functions appeared in the previous chapter in equations (6.7), (6.12),
(6.17), (6.19), (6.21) and even (6.22), sharing a common characteristic: in the sta-
tistical interpretation of the previous section, they all correspond to a Gibbs-Markov
ﬁeld prior law. We can therefore conclude that “Markov” tools, and tools that can

174
Bayesian Approach to Inverse Problems
be interpreted as such, are both very widely used in signal and image restoration and
relatively simple to handle despite their reputation, since it is possible to “do Markov”
without being aware of it.
Nevertheless, to be able to discuss more advanced questions such as those posed in
the introduction to this chapter, it is necessary to make the deﬁnitions and fundamental
properties of Gibbs-Markov ﬁelds explicit. Let us start with the notion of a Gibbs ﬁeld
on a ﬁnite set and the associated vocabulary.
7.3.1. Gibbs ﬁelds
7.3.1.1. Deﬁnition
Consider a ﬁnite set of variables x = {x1, . . . , xS}, e.g. the pixels of an image,
having values in a set E. Depending on the case, we may be interested in E =
 ,
E =
 +, E = {1, . . . , 256}, etc. We deﬁne a set of sites S = {1, . . . , S} by
bringing together the indices of the elements of x. In the case of a rectangular image
of size I × J, the sites are the pairs (i, j), artiﬁcially ordered from 1 to S = IJ.
Let C be a set of C cliques, i.e., subsets of S and the same number of real functions
of x, noted Wc, c ∈C, respecting the following constraint: although noted Wc(x),
the value taken by Wc, this function only depends on the variables xc = {xs, s ∈c}
and not on all of x. The set of functions Wc is called a Gibbs potential.
If E is
  or a subset of
  of non-zero measure (typically,
 + or an interval of
 ),
situations that can be globally designated as the continuous case1, we deﬁne a Gibbs
ﬁeld X having a value in E, of energy:
Φ =

c∈C
Wc,
(7.11)
at temperature T , as the random process with the following pdf:
pX(x) =
1
Z(T ) e−Φ(x)/T ,
(7.12)
where the normalization constant:
Z(T ) =

ES e−Φ(x)/T dx,
called the partition function here, is assumed to be ﬁnite.
1. In this chapter, the opposition between continuous case and discrete case concerns the pixel
values, whereas Chapter 6 distinguished between continuous functions and discrete functions
according to their support, i.e., their domain of deﬁnition instead of their range.

Gibbs-Markov Image Models
175
In addition, a Gibbs ﬁeld is said to be homogeneous if the deﬁnition of functions
Wc respects invariance under translation (which implicitly assumes that x is spatially
organized): for any pair of cliques c, c′ that are superimposable by translation:
Wc(x) = Wc′(x′), ∀x, x′ such that xc = x′
c′.
The deﬁnition of a Gibbs ﬁeld is comparable to equations (7.5) and (7.6): the pro-
cess inducing a probability law from an “energy” is identical in the three cases. On
the other hand, a Gibbs ﬁeld differs from a generic random process by the “local”
structure of its energy (7.11), a “macroscopic” quantity deﬁned as the sum of “micro-
scopic” contributions.
In the case where the variables x = {x1, . . . , xS} take discrete values (the dis-
crete case), e.g. pixels with quantiﬁed grey levels, we deﬁne the probability of a Gibbs
ﬁeld X with discrete values in a similar way:
Pr(X = x) =
1
Z(T ) e−Φ(x)/T ,
(7.13)
where Φ is again deﬁned by (7.11) and the partition function is now written:
Z(T ) =

x∈ES
e−Φ(x)/T .
By a variety of construction procedures, ranging from the simplest (all-pole ﬁlter-
ing of white, multivariate noise; unilateral ﬁelds) to the most sophisticated (study of
(7.13) at the limit S →∞), we can also deﬁne Gibbs ﬁelds on networks of inﬁnite
size [GUY 95], or with continuous indices, e.g. Gaussian ﬁelds [MOU 97] or spatial
point processes [BAD 89]. As the use of these extended models remains infrequent
in image restoration, the deﬁnition of Gibbs ﬁelds on ﬁnite networks will be quite
sufﬁcient for our purposes.
7.3.1.2. Trivial examples
The special case of a Gibbs ﬁeld admitting only singleton cliques c = {s}:
pX(x) =
1
Z(T ) exp

−

s∈S
Ws(x)/T
0
=
1
Z(T )
1
s∈S
e−Ws(x)/T =
1
s∈S
pXs(xs),
corresponds to a hypothesis that the components of the ﬁeld are independent. At the
other extreme, when C contains set S itself, all the random processes on S become
Gibbs ﬁelds. However, “Gibbsian” analysis is obviously of no interest in these two
trivial cases.

176
Bayesian Approach to Inverse Problems
7.3.1.3. Pairwise interactions, improper laws
As a Gibbs energy, a penalty function such as:
I

i=2
J

j=1
(xi,j −xi−1,j)2 +
I

i=1
J

j=2
(xi,j −xi,j−1)2,
(7.14)
can be decomposed over horizontal and vertical cliques composed of pairs of adjacent
sites, {(i −1, j), (i, j)} and {(i, j −1), (i, j)}.
Similarly, many other penalty functions used in image restoration can be written

r∼s φrs(xr, xs), where r∼s describes a set of pairs of sites that are “spatial neigh-
bors”. These functions are Gibbs energies for which the interactions are limited to
pairs.
Among these functions, the most used can be written Φ(x) = 
r∼s φ(xr −xs),
following the example of (7.14). As the energy of a (homogeneous) Gibbs ﬁeld, this
expression poses a problem if E =
 , since it is easy to show that ∀T, Z(T ) =
+∞by a linear invertible change of variables t1 = x1, t2 = x2 −x1, t3 = x3 −
x1, . . . , tS = xS −x1. The prior law exp {−Φ(x)/T }, which is not normalizable, is
said to be improper.
When the prior law is improper, the posterior likelihood pX|Y can still be the pdf
of a proper law since, from (7.8) it can be written:
pX|Y (x | y) = e−Ψ(y−H(x))/Tb−Φ(x)/Tx/Zx|z(Tx, Tb)
(7.15)
where Zx|z(Tx, Tb) = 
M e−Ψ(y−H(x))/Tb−Φ(x)/Tx dx < +∞is a less restrictive
condition than (7.3)-(7.4). As shown in [SAQ 98, p. 1031, footnote], an improper law
can be “managed” as the limit of a family of pdfs, e.g. by introducing the augmented
energy:
Φε(x) =

r∼s
φrs(xr −xs) + ε ∥x∥2 −→
ε→0+ Φ(x).
7.3.1.4. Markov chains
In this subsection and the rest of the chapter, we will use the shortened notation
p(a | b) for the conditional density pA|B(a | b), unless it is ambiguous. In the same
way, we will use p(a) for pA(a).
In the continuous case, a Markov chain is deﬁned as a collection of random vari-
ables X = (X1, . . . , XS) that admit a joint probability density pX such that:
∀s ∈{2, . . . , S}, (x1, . . . , xs) ∈Es, p(xs | x1, . . . , xs−1) = p(xs | xs−1).

Gibbs-Markov Image Models
177
For example, an autoregressive signal of order one is a Markov chain. In the case
when s is a time index, we can say that the random evolution with time of such a
process depends only on the most recent past.
Markov chains are Gibbs ﬁelds. In fact, we have:
p(x) = p(x1)
1
s>1
p(xs | x1, . . . , xs−1) = p(x1)
1
s>1
p(xs | xs−1)
(7.16)
from the sequential Bayes’ rule and the deﬁnition of a Markov chain; or again:
p(x) = exp

log p(x1) +

s>1
log p(xs | xs−1)
0
.
(7.17)
Inversely, it can be shown by descending recurrence that a Gibbs ﬁeld, the cliques of
which are the pairs {s −1, s} , s = 2, . . . , S, is a Markov chain [QIA 90]. In a much
more general context, there is again a “Gibbs-Markov” equivalence, which we will
look at in the next section.
7.3.1.5. Minimum cliques, non-uniqueness of potential
Is the singleton {1} a clique for Gibbs ﬁeld (7.17)? Yes, for a potential composed
of the S elements −log p(x1), −log p(x2 | x1)... No, if the ﬁrst two terms are re-
placed by their sum −log p(x1) −log p(x2 | x1). More generally, we can consider
that any non-empty subset of a clique is a clique; or, looking at things the opposite
way, we can restrict ourselves to maximum cliques only, i.e., those that are not subsets
of any other set.
For a given Gibbs energy, both conventions “maximum cliques and subsets” and
“maximum cliques only” make the list of cliques unique. However, there remain
several potentials to describe the same energy (except in the trivial cases of disjunct
cliques, in the convention “maximum cliques only”), since the sum Wc + Wd of the
contributions associated with two cliques c and d having a non-empty intersection can
also be written (Wc + W ′
c∩d) + (Wd −W ′
c∩d) for any function W ′
c∩d deﬁned on c ∩d.
7.3.2. Gibbs-Markov equivalence
7.3.2.1. Neighborhood relationship
Let X be a random ﬁeld deﬁned on S = {1, . . . , S}, with values in E, admitting
a density pX strictly positive on ES. Let Vs, s ∈S be subsets of S, Vs being called
the set of neighbors of s. We will only concern ourselves with sets Vs that induce a
neighborhood relationship that is antireﬂexive (s is not its own neighbor: s ̸∈Vs) and
symmetric (if s is a neighbor of r, r is a neighbor of s: s ∈Vr ⇐⇒r ∈Vs). In what

178
Bayesian Approach to Inverse Problems
follows, we will note the neighborhood relationship as ∼in the sense where r ∼s is
equivalent to r ∈Vs.
This deﬁnition of the relation ∼is abstract. In practice when the idea of neigh-
borhood pre-exists in S, it is frequent (but not obligatory) to make the neighborhood
relationship coincide with it. For example, if the pre-existing idea of neighborhood is
induced by a distance Δ, we can deﬁne Vs = {r ̸= s, Δ(r, s) ≤D}, by choosing the
neighborhood “range” D > 0. If S ⊂
p, if Δ is the usual Euclidean distance and if
D2 is an integer, we thus deﬁne the neighborhood relation of order D2.
0
1
1
1
1
2
2
2
2
4
4
4
4
Figure 7.1. A site of
2 admits four neighbors of order one, four more neighbors of order two,
no neighbor of order three, four of order four. More generally, a site of
p has 21 C1
p = 2p
neighbors of order one, 22 C2
p = 2p(p −1) of order two, 23 C3
p = 4p(p −1)(p −2)/3 of
order three; but the rest of the sequence shows that the general term is not 2q Cq
p
7.3.2.2. Deﬁnition of a Markov ﬁeld
X is said to be a Markov ﬁeld for the neighborhood relationship ∼if:
∀s ∈S, x ∈ES, p(xs | xS\s) = p(xs | xVs).
(7.18)
In other words, the probability of Xs, conditionally on all the other variables of X,
does not depend on S variables, but only on the values taken by Xs and its neigh-
bors. Once again, we ﬁnd the possibility of specifying the model of a signal or image
through its local properties.
This deﬁnition concerns the continuous case. In the discrete case it is transposed
in the form: ∀s ∈S, x ∈ES,
Pr(Xs = xs | XS\s = xS\s) = Pr(Xs = xs | XVs = xVs).
(7.19)
Specifying the local characteristics of X, i.e., the S conditional laws “Xs knowing
XS\s”, is sufﬁcient to deﬁne the law of X. In the discrete case, this assertion is proved

Gibbs-Markov Image Models
179
by the identity:
Pr(x)
Pr(w) =
S
1
s=1
Pr(xs | x1, . . . , xs−1, ws+1, . . . , wS)
Pr(ws | x1, . . . , xs−1, ws+1, . . . , wS),
valid for any pair x, w ∈ES [BES 74] and transposable to the continuous case.
It could be thought that this identity allows Markov ﬁelds to be constructed from
freely speciﬁed conditional laws (7.19). This is not so. Except for “lucky accidents”,
arbitrarily speciﬁed conditional laws do not correspond to any joint law. In this sense,
(7.18) and (7.19) are not constructive deﬁnitions but, rather, characteristic properties.
To specify a Markov ﬁeld, it is necessary to proceed indirectly, ﬁrst specifying a joint
law, then checking that it satisﬁes the Markov condition (7.18) or (7.19) as the case
may be. This condition is automatically satisﬁed by Gibbs ﬁelds, as the next subsec-
tion shows.
7.3.2.3. A Gibbs ﬁeld is a Markov ﬁeld
We consider the continuous case here. The discrete case lends itself to a devel-
opment that is equivalent throughout. Let us examine the structure of the conditional
laws p(xs | xS\s) when X is a Gibbs ﬁeld:
p(xs | xS\s) =
p(x)
p(xS\s) =
p(x)

E p(x) dxs
=
exp

−
c∈C Wc(x)/T

/Z(T )

E exp

−
c∈C Wc(x)/T

/Z(T ) dxs
.
The equations of the ﬁrst line are true whether X is a Gibbs ﬁeld or not. The Gibbsian
structure appears on the second line and entails simpliﬁcations for all the multiplying
terms that do not depend on xs, i.e., 1/Z(T ) and any exp {−Wc(x)/T } term indexed
by a clique c that does not contain s. After simpliﬁcation,we obtain:
p(xs | xS\s) =
exp

−
c∋s Wc(x)/T


E exp

−
c∋s Wc(x)/T

dxs
,
where the sums are restricted to the cliques that contain s. It is important to note
that this expression does not depend on all the variables xr, r ∈S: those that do
not belong to any clique containing s are not included. It results that property (7.18)
is true: a Gibbs ﬁeld is a Markov ﬁeld for a neighborhood structure described by
Vs = {r, ∃c / r ∈c ∋s}.
7.3.2.4. Hammersley-Clifford theorem
Are there Markov ﬁelds that are not Gibbs ﬁelds? This question was answered by
an equivalence theorem stated by Hammersley and Clifford in an unpublished report

180
Bayesian Approach to Inverse Problems
in 1968. This theorem says that, in the discrete case, Markov ﬁelds verifying the pos-
itivity condition ∀x, Pr(X = x) > 0 are Gibbs ﬁelds, whose cliques are singletons
or sets of sites that are two-by-two neighbors. The demonstration, even when mod-
ernized, is too technical to be presented here. It can be found in [WIN 03, section 3.3]
and in [BRÉ 99, section 7.2], for example. This result is important from a theoretical
point of view since it indicates that Gibbs and Markov ﬁelds are essentially the same
mathematical objects, called Gibbs-Markov random ﬁelds (GMRFs) in what follows.
7.3.3. Posterior law of a GMRF
Analysis of the posterior law of a GMRFX brings us a result that is useful in image
restoration: under not very restrictive conditions, this law is itself that of a GMRF, for
which it is interesting to study the neighborhood structure. For a GMRFX of energy
(7.11) and in the observation conditions of section 7.2, it has already been pointed out,
in equation (7.15), that the energy of the posterior law can be written
Ψ(y −H(x))/Tb + Φ(x)/Tx.
(7.20)
This corresponds to the Gibbs potential of the prior law augmented by terms express-
ing the co-log-likelihood −log pY |X = −log pB(y −H(x)). Thus, in general, we
can say that the posterior law is that of a Markov ﬁeld for which the neighborhood
graph comes from the decomposition of the log-likelihood over cliques. A ﬁner anal-
ysis is possible case by case.
Consider, for example, the common situation of a Gaussian linear observation y =
Hx + b, where b is the realization of Gaussian noise independent of x, of mean mB
and invertible covariance RB. The conditional co-log-likelihood of the observations
is thus quadratic. As a function of x, it can be decomposed in the form:
−log pB(y −Hx) = (y −Hx)T R−1
B (y −Hx)/2 + const.
= −yT R−1
B Hx + xT HT R−1
B Hx/2 + const.
= −

s
αsxs +

r,s
βrsxrxs/2 + const.
(7.21)
where αs and βrs are the components of vector HR−1
B y and matrix HT R−1
B H re-
spectively. The only cliques involved in expression (7.21) are singletons and pairs.
All the sites become neighbors if HT R−1
B H is a full matrix. The more sparse it is,
the fewer new connections appear in the posterior-law neighborhood graph. In the
extreme, if H and RB are diagonal matrices (which is the case of denoising or inter-
polation), the prior and posterior neighborhood structures are identical.

Gibbs-Markov Image Models
181
7.3.4. Gibbs-Markov models for images
In this section, we will draw up a schematic classiﬁcation of GMRF published
in the image processing literature, which takes its inspiration from [BES 86]. Note
that most of these models are mathematical expressions of the qualitative notion of
“regularity almost everywhere”, already frequently mentioned in the previous chapter.
7.3.4.1. Pixels with discrete values and label ﬁelds for classiﬁcation
The oldest models use discrete values, no doubt for two reasons: ﬁrst, for a long
time, the capacity of computers limited image coding to one byte per pixel or even
less; second, estimating an image on a limited number of hues is a way of grouping
the pixels together in classes, taking their neighborhood into account, i.e., approaching
a contextual classiﬁcation problem.
Ising’s model is the simplest and the oldest of non-trivial, discrete-valued GM-
RFs [PIC 87]. It is a binary model with interactions in pairs having a neighborhood
of order one. Deﬁned on
2, it was proposed by Ising in 1925 to explain the behavior
of ferromagnetic materials, in particular phase transition. This occurs below a certain
temperature, called the critical temperature, by non-zero correlation among inﬁnitely
distant pixels. In imaging, the models are deﬁned on networks of ﬁnite size so the role
of phase transitions has been little studied [MOR 96]. The energy of an Ising ﬁeld is
written:
Φ(x) = α

s
xs + β

r∼s
xrxs,
(7.22)
with E = {−1, 1}. If α ̸= 0, the ﬁrst term favors states of opposite sign to α. If
β > 0, the low-energy (so most probable) conﬁgurations are composed of dissimilar
neighbors and the behavior is repulsive. In contrast, β < 0 produces attractive behav-
ior between neighbors. This is the desired behavior for classiﬁcation since it favors
homogeneous regions.
Ising’s model can easily be generalized to K colors, e.g. in the form of an energy
written:
Φ(x) =
K

k=1
αknk +
K

k=1
K

l=k
βklvkl,
nk being the number of pixels of color k and vkl = vlk the number of pairs of
neighbors of colors respectively k and l. We ﬁnd (7.22) for α1 = −α−1 = α and
β1,1 = β−1,−1 = −β−1,1 = β.
In the case of ordered colors, such as grey levels, the βkl can be chosen as increas-
ing functions of |k −l| to encourage gradual changes of shade. Minimizing a criterion
such as ∥y −x∥2 + λΦ(x), where y is an image observed over a number of shades at
least equal to K, then gives a version of y reduced to K levels, favoring homogeneous
regions.

182
Bayesian Approach to Inverse Problems
In the case of non-ordered colors, the model can be simpliﬁed by considering, for
example, βkl = β < 0 if k = l, 0 otherwise. We thus obtain Potts’ model. The
case of “non-ordered colors” may seem marginal but it brings us to the concept of
the label ﬁeld, which is fundamental in the modern approaches to image classiﬁca-
tion. From a very general point of view, we can consider that contextual classiﬁcation
consists of distributing the pixels into classes that are homogeneous in a sense to be
deﬁned, and marked by labels that are not colors or grey levels. Choosing a spatial
model for the labels, such as Potts’ model (or an improved version such as the chien-
model [DES 95]), encourages the formation of aggregates of pixels assigned to the
same class (this is known as contextual classiﬁcation). A second level of K models
(one model per class), enables the pixel law to be speciﬁed region by region, and thus
the homogeneity for each class to be deﬁned. In this way, it is possible to distin-
guish regions by their mean intensity, their variance or their texture. In the case of
textures, the second-level models can be specialized GMRF (such as the autobinomial
model [BES 74]; see also [CRO 83]) but also marked point processes, assemblies of
primitives, fractal models, etc. [WIN 03].
Image modeling for classiﬁcation can also have the aspect of a sophisticated stack
of models, which contrasts with the relatively rustic character of the models the most
used in image restoration. There is an explanation for this difference: whereas classi-
ﬁcation seeks to give a simpliﬁed representation of a complex but known object (i.e.,
an image), the aim of restoration is to give the most faithful possible image starting
from imperfect knowledge of it. The poorer the data, the less useful it is to model
details, such as texture, which it will, in any case, be impossible to restore.
In what follows, we will take the continuous case (E =
 ) and give greater place to
GMRFs which are more suited to image restoration than to classiﬁcation. In essence,
the Gibbs energies of these models are the penalty functions presented in the pre-
vious chapter: ﬁrst quadratic energies, then half-quadratic energies associated with
detection-estimation, and ﬁnally non-quadratic energies, convex or non-convex.
7.3.4.2. Gaussian GMRF
Penalty (7.14) is the energy of a Gaussian GMRF (improper). More generally, a
quadratic energy deﬁned as positive (M symmetric positive matrix):
Φ(x) = xT Mx −2mT x + μ =
S

r=1
S

s=1
Mrsxrxs −
S

s=1
mrxs + μ
(7.23)
corresponds, at any temperature T, to a Gaussian GMRF. The improper case is reached
when M is positive in the broad sense only.
By identifying Φ(x)/T with (x −mx)T R−1
x (x −mx)/2 to within an additive
constant, the mean and covariance of X can be found:
mx = M−1m, Rx = T M−1/2.

Gibbs-Markov Image Models
183
Moreover, it is clear from (7.23) that the cliques of a Gaussian GMRF are singletons
and pairs and that the neighborhood relationship can be “read” directly in the matrix
M (or in R−1
x ): r is a neighbor of s if s ̸= r and Mrs ̸= 0. Gaussian GMRFs pos-
sess many other structural properties, including the explicit character of the partition
function on
 S:

S exp
	
−1
2(x −mx)T R−1
x (x −mx)

dx = (2π)S/2 |Rx|1/2 .
(7.24)
7.3.4.3. Edge variables, composite GMRF
The detection-estimation approach of Chapter 6 (section 6.3) favors half-quadratic
penalty functions, the simplest of which can be written:
Φ(x, ℓ) =

r∼s
(1 −ℓrs)(xr −xs)2 + α

r∼s
ℓrs,
(7.25)
where the neighborhood is of order one; more rarely, it is of order two as in (6.13).
Each variable ℓrs has the value 0 or 1 and models the presence (ℓrs = 1) or absence
(ℓrs = 0) of a discontinuity between pixels r and s, and α is the price to be paid per
discontinuity. Interpreted as a Gibbs energy, (7.25) deﬁnes a composite (x, ℓ) GMRF,
distributed over pixel sites and edge sites, and having a value in ES × {0, 1}C, where
C is the number of pairs of neighboring pixel sites.
r ⃝
⃝
⃝
rs
s ⃝
⃝
⃝
(a) Interspersed pixel sites and edge
sites
r ⃝
rs
s ⃝
(b) Vertical
cliques
r rs s
⃝
⃝
(c) Horizontal
cliques
Figure 7.2. Spatial structure of composite GMRF of energy (7.25)
Composite GMRFs integrating continuous and discrete variables are deﬁned nei-
ther by density function nor by discrete distribution but in the form of a mixed product:
p(x | ℓ) Pr(L = ℓ) =
1
Z(T ) e−Φ(x, ℓ)/T , Z(T ) =

ℓ

x
e−Φ(x, ℓ)/T dx.
(7.26)
In the case of (7.25), Z(T ) = +∞if E =
 . The law thus deﬁned is improper.
It could be thought that the two terms of (7.25) correspond to p(x | ℓ) and Pr(L =
ℓ) respectively. However, to reason correctly, Pr(L = ℓ) of (7.26) should be deduced

184
Bayesian Approach to Inverse Problems
by marginalization. Let us do this starting from the energy Φε(x, ℓ) = Φ(x, ℓ) +
ε ∥x∥2, making the law of (X, L) proper if ε > 0:
Pr(L = ℓ) =
1
Z(T )

ES e−Φε(x, ℓ)/T dx
=
1
Z(T ) exp
	
−α
T

r∼s
ℓrs

 
ES exp
	
−ε
T ∥x∥2 −1
T

r∼s
(1 −ℓrs)(xr −xs)2
dx.
The last integral is the partition function of the Gaussian GMRF X | L: from (7.24),
Zx|l(T ) = (πT )S/2 |M|−1/2 if E =
 , where M is deﬁned by:
Mss = ε +

r,r∼s
(1 −ℓrs); Mrs = −2 if r ∼s and ℓrs = 0, 0 otherwise.
Finally, the law of L is not iid, since Zx|l(T ) depends on ℓ:
Pr(L = ℓ) = Zx|l(T )
Z(T ) exp
	
−α
T

r∼s
ℓrs

̸∝exp
	
−α
T

r∼s
ℓrs

.
7.3.4.4. Interactive edge variables
The role of edge variables is to surround homogeneous regions, thus segmenting
the image. Classiﬁcation and segmentation are not identical operations as the second
works without pre-speciﬁed classes.
The minimization of a criterion such as (7.25) does not guarantee that the outlines
will be closed. To give more importance to closure, [GEM 84] introduced interactive
edge variables. This does not signify that these variables become correlated; we noted
above that they already are in the model induced by energy (7.25). The new feature is
a set CL of cliques composed of four edge sites that are added to penalize interrupted
outlines:
Φ(x, ℓ) =

r∼s
(1 −ℓrs)(xr −xs)2 + α

c∈CL
Gc(ℓ).
(7.27)
The values of Gc(ℓ) proposed by [GEM 84] are indicated in Figure 7.3. The resulting
criterion (7.27) is not exactly the one proposed in [GEM 84], since the model put for-
ward in this article was based on pixels with discrete values (with non-ordered colors,
see p. 182). To guarantee systematic closing of the outlines, Gc would have to be
inﬁnite for type 2 conﬁgurations. However, this condition would not respect the pos-
itivity condition of the Hammersley-Clifford theorem and would make the posterior
energy minimization problem (even more) tricky.
The interactivity of the edge variables not only encourages outline closure but
also allows their appearance to be modiﬁed. Horizontal or vertical straight edges are

Gibbs-Markov Image Models
185
c ∈CL
Type
1
2
3
4
5
6
Gc(ℓ)
0
2.7
1.8
0.9
1.8
2.7
Figure 7.3. Clique c composed of four edge sites and values of Gc(ℓ) for various
conﬁgurations of ℓ. The “activated” variables ℓrs = 1 are represented in black. Gc(ℓ) is
invariant by symmetry and rotation
favored: type
4 conﬁgurations are less costly than type
3 . Other interactive edge
models have been proposed in the same vein, e.g. to reﬁne the outlines [MAR 87] or
reduce directional effects [SIL 90].
7.3.4.5. Non-Gaussian GMRFs
Among the non-Gaussian GMRFs, those most used in image restoration are of
order one and their energy is a function of interpixel differences:
Φ(x) =

r∼s
φ(xr −xs),
(7.28)
where φ is typically an L2L1, L2L0 function [KÜN 94], or a related function such as
φ(u) = |u|p [BOU 93]. The use of these models as priors in a procedure for estimation
in the MAP sense is the same as adopting the non-quadratic penalized approach of
section 6.4 (which can usefully be looked at again).
7.4. Statistical tools, stochastic sampling
7.4.1. Statistical tools
Section 7.3 sets the penalty functions of the previous chapter in a Bayesian prob-
abilistic framework. Some of the “big” questions posed in the introduction now ﬁnd
a natural answer, at least from the formal standpoint. In particular, it becomes possi-
ble to quantify the uncertainty via the density of the posterior law pX|Y (x | y) (see
Chapter 3), and to extract simpler indices from this law, such as the posterior mean
and covariance:
x
PM = E(X | y) =

ES x pX|Y (x | y) dx
(7.29)
R
CP = Cov(X | y) = E

(X −x
PM)(X −x
PM)T | y

=

ES(x −x
PM)(x −x
PM)T pX|Y (x | y) dx.
(7.30)

186
Bayesian Approach to Inverse Problems
The posterior mean xPM has S components. It is a Bayesian estimator than could
be a rival for the MAP. Its use in image restoration was advocated by Marroquin
[MAR 87] but has failed to become widespread.
The posterior covariance RCP = (RCP
rs) is a matrix of size S × S. It can be used to
generate a conﬁdence region for the estimated intensities. For example, from the stan-
dard deviations σs = (var (Xs | y))1/2 =

RCP
ss, we can deﬁne a Cartesian product
with intervals I(μ) = 2
s Is(μ) and extremities:
b±
s = x
PM
s ± μσs, μ > 0.
This approach neglects the correlations in the posterior law but is attractive as it is
relatively simple and visual: the b±
s are error bars on each pixel, estimated by xPM
s .
Another fundamental statistical tool, this time concerning hyperparameter estima-
tion, is the likelihood of the data, obtained by integrating X out of the problem. In the
continuous case, the result is the data probability density, which also depends on the
hyperparameters of the problem:
pY (y ; θ) =

ES pX,Y (x, y) dx =

ES pB(y −H(x))pX(x) dx.
(7.31)
Vector θ designates the set of hyperparameters on which estimators x constructed us-
ing the joint law (7.9), such as xMAP and xPM, depend: temperature parameters Tx and
Tb determining the regularization coefﬁcient λ = Tb/Tx, together with all other de-
grees of freedom parametrizing Φ or Ψ (e.g. the threshold of the L2L1 or L2L0 func-
tions). Finally, to recall that the density pY (y) depends on θ, it is noted pY (y ; θ).
The maximum likelihood estimator θMV is deﬁned as maximizing pY (y ; θ) (see
section 3.5). This is the preferred tool for tackling inverse problems in a non-super-
vised framework, which is the question covered in the next chapter.
The practical use of these statistical tools raises two important questions at two
very different levels.
At the information level, what meaning should be given to these statistical quanti-
ties? For example, let us assume that μ deﬁnes a 95% conﬁdence region:
Pr(X ∈I(μ) | y) =

I(μ)
p(x | y) dx = 0.95.
(7.32)
As the context is Bayesian, this is a mean conﬁdence region, which takes account
of the uncertainties connected with the measurements y, quantiﬁed by the noise law,
and also our uncertain knowledge of x, quantiﬁed by the prior law. In accordance

Gibbs-Markov Image Models
187
(a) original image x⋆,
37 × 30 pixels with
quantiﬁed intensities between
0 and 256
(b) noisy image y = x⋆+ b,
b is iid N (0, σn = 20)
(c) xPM = E(X | y)
(d) standard deviations (σs)S
Figure 7.4. Mean and posterior standard deviation, (c) and (d) respectively, for a prior
Gibbs-Markov energy law (7.28), with φ of type L2L1. These quantities were calculated by an
MCMC method (see section 7.4.2.2). They depend on three parameters: the standard deviation
of the noise, ﬁxed at its true value σn = 20, and (T, η), the GMRF temperature and threshold
of function φ, chosen empirically. The grey-level scale is dilated for the standard deviation, the
maximum value of which does not exceed 16
with Bayesian principles, this prior should sum up a state of knowledge gained inde-
pendently of the data. In these conditions, it can be observed empirically that equa-
tion (7.32) is true in the sense where the same experiment, repeated N times in the
same state of knowledge, would ﬁnd the event X ∈I(μ) realized about 0.95 N times
if N is large. In most practical situations, it is obvious that the Gibbs-Markov models
of section 7.3.4 disobey these principles, in the sense that they only take account of
a part of the prior information available. The information taken into consideration is
sufﬁcient to produce usable estimators and indications of the associated uncertainties
but it does not allow these uncertainties to be accurately quantiﬁed. The meaning

188
Bayesian Approach to Inverse Problems
of the standard deviations of Figure 7.4d is thus only qualitative. As expected, they
decrease inside a homogeneous domain such as the background and increase at the
edges. However, their precise value is not signiﬁcant. For example, we can verify that
x⋆/∈I(7), i.e., there are sites s such that |xPM
s −x⋆
s| > 7σs. For these sites, the value
of σs is clearly underestimated. Similarly, the posterior mean of Figure 7.4c is not
sufﬁciently contrasted.
From an operational point of view, to evaluate quantities (7.29), (7.30) and (7.31),
it is necessary to be able to calculate the integrals over ES. The ﬁrst two can be written
component by component as simple integrals:
x
PM
s = E(Xs | y) =

E
xspXs|Y (xs | y) dxs,
var (Xs | y) =

E
(xs −x
PM
s )2pXs|Y (xs | y) dxs,
but these expressions do not allow a direct calculation as the marginal densities pXs|Y
are themselves integrals over ES−1. As for equation (7.31), we will concern ourselves
with its maximization rather than its evaluation, which does not simplify the problem,
at least at ﬁrst glance.
Outside special cases such as the Gaussian case, the calculations may appear insur-
mountable, or at least incompatible with practical uses. Fortunately, special pseudo-
random techniques, called Markov Chain Monte Carlo (MCMC) techniques, help us
get around many of the calculation difﬁculties. These techniques are introduced in the
next section. The reader may refer to [ROB 04] for a more comprehensive presenta-
tion in a generic inferential context, and to [BRÉ 99, WIN 03] in the context of spatial
statistics and image analysis.
7.4.2. Stochastic sampling
By sampling, we mean the pseudo-random generation of realizations of a prob-
ability law. In imaging, and particularly as far as GMRFs are concerned, sampling
has several roles. The most obvious is in image synthesis. GMRF realizations can
be used to “dress” objects, by giving them a suitable textured appearance [CRO 83].
The realizations of GMRFs of energy (7.28), with φ of the L2L1 type, such as that
represented in Figure 7.5, have a “grained” texture.
In image restoration, it is the posterior law pX|Y =y that it is interesting to simu-
late, in the aim of calculating Bayesian estimators such as the posterior mean xPM =
E(X | y). In fact, if X(1), X(2), . . . , X(K) is a series of random variables having

Gibbs-Markov Image Models
189
Figure 7.5. Simulation of a GMRF of energy (7.28), where φ is the Huber function (L2L1
function), on a 80 × 100 grid. The value of the boundary pixels is kept to zero
to avoid the improper character of the law
the law pX|Y =y, we have:
E(F(X) | Y = y) = lim
K→∞
1
K
K

k=1
F(X(k))
(7.33)
almost surely for any function F for which this series is ergodic. In particular, we
can approach xPM through the empirical mean of K realizations of the posterior law if
these realizations are extracted from a series that is ergodic for the mean.
In the case of an independent series, the ergodicity is guaranteed provided that
E(F(X) | y) exists, which is the case if E(|F(X)| | y) < ∞. This is the strong law
of large numbers. We could thus envisage applying equation (7.33) starting from an
iid series X(1), X(2), . . . , X(K) of instantaneous law pX|Y =y. The standard Monte
Carlo method works precisely on this principle [HAM 64, Chapter 5]. Unfortunately,
this method is not suitable as generating a single sample of the law pX|Y =y already
involves an iterative process too costly to be repeated enough. We will nevertheless
examine this procedure because it remains at the heart of the MCMC methods, which
are better suited to the situation (see section 7.4.2.2).
7.4.2.1. Iterative sampling methods
Generating realizations of random quantities is a ﬁeld of study in itself, which
employs a vast range of techniques. When the quantities to be generated are of small
dimensions (typically a real random variable) there are direct techniques for many
families of probability laws. One of the most commonly used is the change of variable
using a law with well-controlled sampling, the “generic” law in this respect being the
uniform law [PRE 86, section 7.2]. For more complicated laws, rejection techniques
are worthy of attention [PRE 86, section 7.3].

190
Bayesian Approach to Inverse Problems
When the dimension of the quantity to be sampled exceeds a few units, direct
techniques become inefﬁcient, except in special cases such as the Gaussian case: a
random vector X with values in
 S for which the law is N(m, R) can be sampled
in the form AE +m, where E is a normalized Gaussian vector (i.e., of law N(0, I)),
provided that the matrix A veriﬁes AAT = R (A is not necessarily square). In
other words, sampling a Gaussian vector of arbitrary covariance comes down to a
problem of factorizing the covariance (R = AAT ), generating independent Gaussian
variables, then making linear combinations (X = AE + m).
In more general cases, sampling by Markov chain becomes inevitable. A series
X(1), X(2), . . . , X(k), . . . is generated, which is a Markov chain and converges in
distribution towards the desired law. In the rest of this section, we assume that this
law has a density p, making it implicit that this notation can designate a conditional
density, such as pX|Y .
The series (X(k)), Markovian by construction, is obtained as follows: a ﬁrst quan-
tity X(1) with values in ES is generated from an arbitrary law, preferably a simple
one. The following components are approached gradually by random transformation.
This transformation is deﬁned by a conditional density t(x′ | x) called the transition
kernel. There are several possible forms for the kernel. The oldest is the Metropo-
lis algorithm [MET 53] (Table 7.1a). The more recent Gibbs sampler [GEM 84] is
speciﬁcally adapted for sampling a GMRF, in as much as the transition takes place by
sampling of the conditional laws p(xs | xS\s), that are local according to (7.18) (Ta-
ble 7.1b). These are two particular cases of a more general structure, the Metropolis-
Hastings sampler [HAS 70].
The ﬁrst property expected of these structures is convergence in distribution to-
wards p. Whether this property is satisﬁed obviously depends on the structure of the
transition kernel t(x′ | x), i.e., on the probability law that governs the generation of
X(k+1) when X(k) is known. We have the following result, which is a simpliﬁed,
restricted statement of [ROB 04, theorem 6.53].
Let (X(k))k∈ be a homogeneous, aperiodic Markov chain, having a transition
kernel t that admits the law π of density p for the invariant law:

ES t(x′ | x) p(x) dx = p(x′)
Thus, X(k) converges in distribution towards π.
The homogeneity corresponds to the fact that the transition kernel does not evolve
during the iterations. The meaning of the invariance of π is the following: if the law
of X(k) is π, that of X(k+1) is also π. On this subject, note that the equilibrium
condition t(x′ | x)p(x) = t(x | x′)p(x′) implies the invariance of π. Aperiodicity is

Gibbs-Markov Image Models
191
(a) Metropolis algorithm
Initialization: choose X(1) arbitrarily.
1) Current conﬁguration: X(k) = x.
2) Propose x′ by sampling a symmetric proposition kernel p(x′ | x) =
p(x | x′).
3) If p(x′) ≥p(x), X(k+1) = x′. Otherwise, still accept X(k+1) = x′
with the probability p(x′)/p(x); in case of rejection, X(k+1) = x.
(b) Gibbs sampler
Initialization: choose X(1) arbitrarily.
1) Current conﬁguration: X(k) = x.
2) Choose a site s “at random” (e.g. equiprobably).
3) Sample X(k+1)
s
according to p(xs | xS\s), the conditional law deduced
from p.
4) X(k+1)
S\s
= xS\s = X(k)
S\s
Table 7.1. Metropolis and Gibbs algorithms for sampling probability density p
a more technical concept [ROB 04, section 6.3.3] that allows cyclic behavior to be
excluded. These conditions are naturally respected by simple, intuitive choices. For
example, for the proposition kernel p of the Metropolis algorithm (Table 7.1a, step 2),
in the case of S =
 , the random walk x′ = x + ε ensures the symmetry of p if ε
follows a centrosymmetric density law pε: pε(−ε) = pε(ε).
For the choice of the site to be resampled in the Gibbs sampler (Table 7.1b, step 2),
it is logical to favor equiprobability but the convergence in distribution also occurs
for deterministic strategies of site exploration. This option makes the Gibbs sampler
partially parallelizable: pixels that are not two-by-two neighbors can be resampled
independently, and therefore simultaneously. In this way, Figure 7.5 was generated
by “chequer-board” style updating, half the sites being resampled at each iteration
(alternatively, all the “whites” or all the “blacks”).
Sampling and optimization are related problems. The simple fact of never accept-
ing the proposition X(k+1) = x′ if p(x′) < p(x) (Table 7.1a, step 3) would transform
the Metropolis algorithm into a random search optimization method. Similarly, the
Gibbs sampler is a “stochastic version” of the Gauss-Seidel descent technique (sec-
tion 2.2.2.2.1). In this respect, it can be said that Markov chain sampling methods
are to sampling as iterative methods of criterion descent are to optimization. Like the

192
Bayesian Approach to Inverse Problems
descent techniques, sampling techniques can have a multitude of variations. For in-
stance, [GEM 92] and [GEM 95] stress global updating methods based on sampling
semi-quadratic energy models that are supposed to be faster than methods proceeding
pixel by pixel. These algorithms are “stochastic versions” of the descent algorithms
presented in Chapter 6.
7.4.2.2. Monte Carlo method of the MCMC kind
Rather than in the convergence in distribution, the interest of iterative methods lies
in a “generalized law of large numbers” for the calculation of stochastic quantities:
Property (7.33) is true when (X(k))k∈ is a Markov chain obtained in the conditions
of the previous subsection. At ﬁrst glance, this result is surprising since the density of
the successive variables X(k) is not necessarily p; the important thing here is that it
approaches p when k increases.
The MCMC methods take advantage of this result by approaching a statistical
expectation through the empirical mean of a ﬁnite number of successive iterations of
a sampler. This is how Figures 7.4c and d were obtained. First, K = 500 samples
X(1), . . . , X(K) converging towards the posterior law of the GMRF were generated
by a chequer-board Gibbs sampler (going from k to k + 1 corresponds to a complete
update, all the “whites” then all the “blacks”). We deduce from this:
3
m =
1
K −k0 + 1
K

k=k0
X(k),
v =
1
K −k0 + 1
K

k=k0

X(k)[2],
and ﬁnally σ =
v −3
m[2] [1/2], where [p] means taking the exponent term by term.
The summations leading to 3
m and v can be performed recursively during sampling. In
addtion, the introduction of a burn-in time k0 means that the ﬁrst k0 samples, for which
the distribution is the furthest from the objective, are not used [GEY 92, section 3.7].
The choice of k0 for a ﬁxed K is the result of a “bias-variance compromise”, which is
empirically set at k0 = 10 here.
A more thorough mathematical presentation of MCMC methods would lead us
to consider the speed of convergence of the various structures. Like minimization
algorithms, sampling algorithms lend themselves to analytical calculations of the con-
vergence rate [ROB 04]. In fact, some sampling algorithms are extremely long. The
most important thing is to come to a qualitative understanding of why. The main pit-
fall to be avoided is cyclic sampling of conditional laws Xs | XS\s when the random
variables Xs are strongly dependent. Although it is true that a Gibbs sampler pro-
ceeding in this way is likely to converge towards the joint law, the convergence will
be very slow, successive samples being strongly correlated and the empirical means
calculated having a residual variance that decreases slowly. This problem is similar to
the more common one posed by trying to ﬁnd the minimum of a narrow valley by a
Gauss-Seidel technique (Chapter 2, section 2.2.2.2.1), which is illustrated in [PRE 86,
Figure 10.6.1].

Gibbs-Markov Image Models
193
7.4.2.3. Simulated annealing
The posterior expectation xPM = E(X | y) can be approached by empirical means
coming from an MCMC method. More generally, as shown in [MAR 87], the same
is true for other Bayesian estimators with separable cost. This is not the case for the
MAP (7.10). When the posterior energy is convex (or even quadratic) in x, it would
be absurd to try to calculate xMAP using a stochastic algorithm. On the other hand,
when this energy has local minima, or when the state space is a large discrete space,
the idea of using a stochastic search method to ﬁnd xMAP is attractive as traps such as
local minima can then be avoided.
Simulated annealing possesses this property, which was ﬁrst demonstrated in the
discrete case [GEM 84], then extended to the continuous case (see the references given
in [GEM 92]). Its basic structure is an iterative sampling method made inhomoge-
neous by the slow decrease of a temperature parameter during the iterations. The brief
presentation below considers the continuous case. Let p be the density of the law to
be maximized and let
pT (x) =
1
Z(T ) (p(x))1/T
with
Z(T ) =

ES (p(x))1/T dx
be the renormalized density at temperature T . Finally, let tT (x′ | x) be a transition
kernel allowing pT to be sampled.
Simulated annealing combines two properties. One concerns the behavior of the
inhomogeneous sampling method that generates a random series (X(k))k∈ follow-
ing tT (k)(x′ | x), where (T (k))k∈ is a deterministic, decreasing series tending to-
wards zero. If the initial temperature is “sufﬁciently high” and if the decrease is “suf-
ﬁciently slow”, it can be shown that the law of X(k) has a density close to pT (k), when
k is large. The other property concerns the behavior of pT as a function of T . As il-
lustrated in Figure 7.6, this density becomes uniform at high temperatures, whereas it
is concentrated on xMAP at a temperature tending to zero.
The ﬁrst simulated annealing algorithm in image processing (deconvolution, seg-
mentation) proposed by [GEM 84] is based on a Gibbs sampler working in the discrete
case with interactive edge variables. Faster forms were subsequently put forward, by
moving on to the continuous case with sampling of models having half-quadratic en-
ergy [GEM 92, GEM 95]. Nevertheless, simulated annealing remains little used as an
optimization method in imaging. It is clearly more costly to minimize a multimodal
criterion by simulated annealing than a convex criterion by a well chosen descent al-
gorithm. And when the convex or non-convex nature of a criterion depends on the
choice of model, calculating cost and simplicity are often decisive arguments.

194
Bayesian Approach to Inverse Problems
(a) p4(x)
(b) p2(x)
(c) p(x)
(d) p1/2(x)
(e) p1/5(x)
Figure 7.6. Probability densities deduced from one another by temperature changes:
pT (x) ∝(p(x))1/T. When T increases, the density becomes uniform over its deﬁnition set.
Conversely, a decrease in T accentuates the differences between the probabilities of the most
probable and least probable events. When T ↘0, the probability becomes evenly distributed
over the values of x maximizing p(x). In the case of a unique maximizer, the limit law is a
Dirac distribution
7.5. Conclusion
In the Bayesian probabilistic framework, local penalty functions can be interpreted
as Gibbs potentials and criterion minimization as ﬁnding a maximum a posteriori.
Does this probabilistic interpretation provide the tools for constructing more objective
models than the qualitative approach of Chapter 6? The answer is “yes” in certain
speciﬁc cases: the Kolmogorov model for imaging through atmospheric turbulence
(Chapter 10) and the Poisson process for counting particles in corpuscular imaging
(Chapter 14). However, for many imaging problems, the models are still “hand built”.
Although it may be true that the probabilistic framework enables, among other things,
this property is not constructive, in the sense that it does not allow models to be ef-
fectively speciﬁed, apart from a few notable exceptions such as Markov chains and
unilateral ﬁelds [IDI 01].
operations such as marginalization and conditioning, which have no natural equiv-
alents in the deterministic framework. This allows estimators other than the maxi-
mum a posteriori to be deﬁned, and also enables uncertainties to be quantiﬁed (sec-
tion 7.4.1) and “second-level problems” such as hyperparameter estimation to be for-
malized (Chapter 8).
Finally, on the subject of calculation aspects, it is incontestable that the emergence
of MCMC methods in signal and image processing was an important practical ad-
vance. In particular, these numerical tools enable statistical indicators to be evaluated
in a realistic environment (small amounts of data and presence of noise), whereas
very few analytical results are available on the behavior of the estimators outside the
asymptotic regime.
the characterization of Gibbs-Markov models by a conditional independence property,
Thus, Gibbs-Markov ﬁelds are often deﬁned by specifying an ad hoc Gibbs energy.
In addition, the Bayesian probabilistic framework gives a sense to mathematical

Gibbs-Markov Image Models
195
7.6. Bibliography
[BAD 89] BADDELEY A. J., MØLLER J., “Nearest-neighbour Markov point processes and
random sets”, Int. Statist. Rev., vol. 57, p. 89-121, 1989.
[BES 74] BESAG J. E., “Spatial interaction and the statistical analysis of lattice systems (with
discussion)”, J. R. Statist. Soc. B, vol. 36, num. 2, p. 192-236, 1974.
[BES 86] BESAG J. E., “On the statistical analysis of dirty pictures (with discussion)”, J. R.
Statist. Soc. B, vol. 48, num. 3, p. 259-302, 1986.
[BOU 93] BOUMAN C. A., SAUER K. D., “A generalized Gaussian image model for edge-
preserving MAP estimation”, IEEE Trans. Image Processing, vol. 2, num. 3, p. 296-310,
July 1993.
[BRÉ 99] BRÉMAUD P., Markov Chains. Gibbs Fields, Monte Carlo Simulation, and Queues,
Texts in Applied Mathematics 31, Springer, New York, NY, 1999.
[CRO 83] CROSS G. R., JAIN A. K., “Markov random ﬁeld texture models”, IEEE Trans.
Pattern Anal. Mach. Intell., vol. PAMI-5, p. 25-39, 1983.
[DEM 89] DEMOMENT G., “Image reconstruction and restoration: overview of common
estimation structure and problems”,
IEEE Trans. Acoust. Speech, Signal Processing,
vol. ASSP-37, num. 12, p. 2024-2036, Dec. 1989.
[DES 95] DESCOMBES X., MANGIN J.-F., PECHERSKY E., SIGELLE M., “Fine structure
preserving Markov model for image processing”,
in 9th Scand. Conf. Image Analysis
SCIA’95, Uppsala, Sweden, p. 349-356, 1995.
[FRA 70] FRANKLIN J. N., “Well-posed stochastic extensions of ill-posed linear problems”,
J. Math. Anal. Appl., vol. 31, p. 682-716, 1970.
[GEM 84] GEMAN S., GEMAN D., “Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images”, IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-6,
num. 6, p. 721-741, Nov. 1984.
[GEM 92] GEMAN D., REYNOLDS G., “Constrained restoration and the recovery of disconti-
nuities”, IEEE Trans. Pattern Anal. Mach. Intell., vol. 14, num. 3, p. 367-383, Mar. 1992.
[GEM 95] GEMAN D., YANG C., “Nonlinear image recovery with half-quadratic regulariza-
tion”, IEEE Trans. Image Processing, vol. 4, num. 7, p. 932-946, July 1995.
[GEY 92] GEYER C. J., “Practical Markov chain Monte-Carlo (with discussion)”, Statistical
Science, vol. 7, p. 473-511, 1992.
[GUY 95] GUYON X., Random Fields on a Network. Modeling, Statistics, and Applications,
Springer Verlag, New York, NY, 1995.
[HAM 64] HAMMERSLEY J. M., HANDSCOMB D. C., Monte Carlo Methods, Methuen, Lon-
don, UK, 1964.
[HAS 70] HASTINGS W. K., “Monte Carlo sampling methods using Markov Chains and their
applications”, Biometrika, vol. 57, p. 97, Jan. 1970.

196
Bayesian Approach to Inverse Problems
[IDI 01] IDIER J., GOUSSARD Y., RIDOLFI A., “Unsupervised image segmentation using a
telegraph parameterization of Pickard random ﬁelds”, in MOORE M. (Ed.), Spatial statis-
tics: Methodological Aspects and Applications, vol. 159 of Lecture notes in Statistics,
p. 115-140, Springer Verlag, New York, NY, 2001.
[KÜN 94] KÜNSCH H. R., “Robust priors for smoothing and image restoration”, Ann. Inst.
Stat. Math., vol. 46, num. 1, p. 1-19, 1994.
[MAR 87] MARROQUIN J. L., MITTER S. K., POGGIO T. A., “Probabilistic solution of ill-
posed problems in computational vision”, J. Amer. Stat. Assoc., vol. 82, p. 76-89, 1987.
[MET 53] METROPOLIS N., ROSENBLUTH A. W., ROSENBLUTH M. N., TELLER A. H.,
TELLER E., “Equations of state calculations by fast computing machines”, J. Chem. Phys.,
vol. 21, p. 1087-1092, June 1953.
[MOR 96] MORRIS R., DESCOMBES X., ZERUBIA J., An analysis of some models used in
image segementation, Research Report num. 3016, INRIA, Sophia Antipolis, France, Oct.
1996.
[MOU 97] MOURA J. M. F., SAURAJ G., “Gauss-Markov random ﬁelds (GMrf) with contin-
uous indices”, IEEE Trans. Inf. Theory, vol. 43, num. 5, p. 1560-1573, Sep. 1997.
[PIC 87] PICKARD D. K., “Inference for discrete Markov ﬁelds: The simplest nontrivial case”,
J. Acoust. Soc. Am., vol. 82, p. 90-96, 1987.
[PRE 86] PRESS W. H., FLANNERY B. P., TEUKOLSKY S. A., VETTERLING W. T., Numeri-
cal Recipes, the Art of Scientiﬁc Computing, Cambridge University Press, Cambridge, MA,
1986.
[QIA 90] QIAN W., TITTERINGTON D. M., “Parameter estimation for hidden Gibbs chains”,
Statistics & Probability Letters, vol. 10, p. 49-58, June 1990.
[ROB 04] ROBERT C. P., CASELLA G., Monte Carlo Statistical Methods, Springer Texts in
Statistics, Springer Verlag, New York, NY, 2nd edition, 2004.
[SAQ 98] SAQUIB S. S., BOUMAN C. A., SAUER K. D., “ML parameter estimation for
Markov random ﬁelds with applications to Bayesian tomography”,
IEEE Trans. Image
Processing, vol. 7, num. 7, p. 1029-1044, July 1998.
[SIL 90] SILVERMAN B. W., JENNISON C., STANDER J., BROWN T. C., “The speciﬁcation
of edge penalties for regular and irregular pixel images”, IEEE Trans. Pattern Anal. Mach.
Intell., vol. PAMI-12, num. 10, p. 1017-1024, Oct. 1990.
[WIN 03] WINKLER G., Image Analysis, Random Fields and Dynamic Monte Carlo Methods,
Springer Verlag, Berlin, Germany, 2nd edition, 2003.

Chapter 8
Unsupervised Problems
8.1. Introduction and statement of problem
We are going to consider the “generic” problem of estimating a physical quan-
tity degraded by a linear process and corrupted by noise, which will be taken to be
white. This formulation, with reasonable approximations, includes many problems
commonly encountered in signal and image processing, such as segmentation, decon-
volution and the reconstruction of one- or multi-dimensional quantities. Here, we will
limit ourselves to the case where the quantities of interest are indexed by discrete, ﬁ-
nite variables, a usual situation in the processing of sampled data of ﬁnite dimension.
The system considered is thus governed by the equation:
y = Hx + b
(8.1)
where y, x and b are vectors that contain the observed data, the unknown quantity to
be estimated, and the samples of the noise respectively. H is the matrix representing
the linear degradation acting on x.
As shown in the previous chapters, the estimation of x is often an ill-posed prob-
lem and regularization is generally required if acceptable results are to be obtained.
In this chapter, we adopt the Bayesian framework from the start, so the information
about x is summed up in the form of the posterior law:
p(x | y) ∝p(y | x) p(x),
(8.2)
where p(y | x) and p(x) denote the conditional density of y when x is known and the
density of the prior law for x respectively. Without loss of generality, we can write
Chapter written by Xavier DESCOMBES and Yves GOUSSARD.

198
Bayesian Approach to Inverse Problems
p(y | x) and p(x) in the form:
p(y | x) = p(y | x ; ϑ) ∝exp {−Ψϑ(y −Hx)} ,
(8.3)
p(x) = p(x ; θ) ∝exp {−Φθ(x)} .
(8.4)
This means that specifying laws p(y | x) and p(x) is equivalent to specifying the func-
tions Ψ and Φ, which, moreover, depend on parameters denoted ϑ for Ψ and θ for Φ.
Once these functions have been speciﬁed, the quality of the results is strongly de-
pendent on the accuracy with which the linear degradation H is known and the values
attributed to θ and ϑ. In some situations, H can be determined precisely, either from a
knowledge of the phenomena under study or by preliminary tests. Similarly, it is often
possible to specify the values of θ and ϑ empirically, by trial and error on simulated
problems for example. However, approaches of this type are not always applicable,
e.g. if H is impossible to identify beforehand or undergoes large variations over time
(case of communication channels), or if the method needs to be used by people who
are not signal or image processing specialists to treat a variety of cases requiring the
values θ or ϑ to be adapted. In such situations, it is desirable to estimate not only x,
but also H, θ and ϑ (or some of these quantities) from the observed data y. This is
the problem of unsupervised estimation, which is the subject of this chapter.
Unsupervised estimation is a vast, difﬁcult ﬁeld. A variety of methods have been
developed and it is impossible to present them fully here. For this reason, we will
concentrate on a particular type of problem that will nevertheless allow us to bring
out the difﬁculties intrinsic in unsupervised estimation and present a set of important
methods in a consistent way. The type of problem we have chosen is that of a penalty
function Φ that is linear with respect to the parameters1 and Markovian, and can be
expressed as:
Φθ(x) =

i
θiNi(x)
(8.5)
where each Ni(x) expresses local interactions between elements of x that are neigh-
bors (see Chapter 7). It should be stressed that many prior distributions in current
use correspond to functions Φ having the above form. This is the case for Gaussian
densities in particular.
First of all, we will consider that H is perfectly known and equal to identity matrix
I, and that the noise, b, is zero. This academic case of a directly observed random ﬁeld
will allow us to point out the difﬁculties in estimating θ and present the fundamental
concepts and basic techniques. Next, we will tackle the situation where H is known
but different from the identity matrix and where noise b is present. This will make it
necessary to adapt and extend the techniques introduced previously.
1. The probability laws thus deﬁned form exponential families.

Unsupervised Problems
199
From the choices mentioned above, it appears that the techniques described in this
chapter will be particularly suitable for the case where x represents an image. It should
be noted that the extension to 3D quantities does not pose any methodological prob-
lem; the extra difﬁculty comes essentially from the volume of data to be processed. In
contrast, the 1D case can present great simpliﬁcations due to the causal structure of x.
Such simpliﬁcations will be pointed out as necessary as we go along.
8.2. Directly observed ﬁeld
First, we consider the case of complete data, i.e., of a ﬁeld X, the realization of
which, x0, is known, and from which we intend to estimate parameters θ. Taking
once again the model of an energy linearly dependent on the parameters, we have:
p(x ; θ) =
1
Z(θ) exp

−

i
θiNi(x)
4
(8.6)
where Z(θ) is the partition function (normalization constant):
Z(θ) =

exp

−

i
θiNi(x)
4
dx.
(8.7)
It should be noted that the integral deﬁned in equation (8.7) is taken with respect to
variable x, i.e., in the conﬁguration space. It is therefore not possible to envisage
its numerical evaluation by computer. Furthermore, it is impossible to formulate it
analytically except in very special cases.
8.2.1. Likelihood properties
To make the derivations simpler, we prefer to handle the log-likelihood, which
according to (8.6) is given by:
log L(θ) = −

i
θiNi(x0) −log Z(θ).
(8.8)
If we take the ﬁrst derivative of this functional, we obtain:
∂
∂θi
log L(θ) = −Ni(x0) −
1
Z(θ)
∂
∂θi

exp

−

i
θiNi(x)
4
dx
= −Ni(x0) +
1
Z(θ)

Ni(v) exp

−

i
θiNi(x)
4
dx
= −Ni(x0) + Eθ(Ni(x))
(8.9)

200
Bayesian Approach to Inverse Problems
where Eθ(Ni(x)) represents the moment – i.e., the mathematical expectation – of
Ni(x) with respect to the law p(x ; θ). Similarly, we can obtain the expression for
the elements of the Hessian H, in the form:
∂2 log L(θ)
∂θi∂θj
= Eθ(Ni(x)) Eθ(Nj(x)) −Eθ(Ni(x)Nj(x)) = −cov(Ni(x)Nj(x)).
Matrix H is negative, as it is the opposite of a covariance matrix. The log-likelihood
is therefore concave. Algorithms of the gradient descent or conjugate gradient type
are thus suitable for calculating the maximum likelihood (ML) for θ, as there are no
local extrema. On the other hand, calculating the Hessian requires the evaluation of
the moments Eθ(Ni(x)) and Eθ(Ni(x)Nj(x)). As an analytical calculation of these
quantities is not feasible, it is necessary to sample the distribution by Monte Carlo
methods in order to evaluate them.
8.2.2. Optimization
8.2.2.1. Gradient descent
The maximization of the log-likelihood can be carried out using a Newton-type
iterative scheme:
θn+1 = θn −H−1∇θ log L(θn).
For the sake of simplicity, we can approach the expectations expressing the gradient
and the Hessian by the corresponding value on a sample. In this vein, Younes has
proved that the convergence of the following algorithm is almost sure [YOU 88]:
θn+1
i
= θn
i + Ni(xn+1) −Ni(x0)
(n + 1) V
(8.10)
where xn+1 is a sample of the law p(x ; θn) and V a sufﬁciently large positive con-
stant. Note that it is nevertheless necessary to sample the law at each step of the
algorithm, which makes its use impossible in practice because of the computing time
needed.
8.2.2.2. Importance sampling
The notion of importance sampling provides a less costly solution [DES 99,
GEY92]. Let us consider the ratio of the partition functions for two distinct parameter
vectors:
Z(θ)
Z(ω) =

exp

−
i θiNi(x)

dx

exp

−
i ωiNi(x)

dx .
(8.11)

Unsupervised Problems
201
We then have:
Z(θ)
Z(ω) =

exp

−
i(θi −ωi)Ni(x)

exp

−
i ωiNi(x)

dx

exp

−
i ωiNi(x)

dx
=

exp

−
i(θi −ωi)Ni(x)

p(x ; ω) dx

exp

−
i ωiNi(x)

dx
= Eω

exp

−
i(θi −ωi)Ni(x)

.
Let us slightly modify the deﬁnition of the log-likelihood by adding a constant (which
leaves the argument of the minimum value unchanged):
log Lω(θ) = −

i
θiNi(x0) −log Z(θ)
Z(ω) .
(8.12)
The ﬁrst derivatives can then be written:
∂log Lω(θ)
∂θi
= −Ni(x0) −Z(ω)
Z(θ)
1
Z(ω)
∂
∂θi

exp
	
−
j(θj −ωj)Nj(x)

p(x ; ω) dx
= −Ni(x0) +
Eω

Ni(x) exp

−
j(θj −ωj)Nj(x)

Eω

exp

−
j(θj −ωj)Nj(x)

(8.13)
and the second derivatives can be expressed as:
∂2 log Lω(θ)
∂θj∂θi
= Eω

Ni(x)e−
k(θk−ωk)Nk(x)
Eω

Nj(x)e−
k(θk−ωk)Nk(x)

Eω

e−
k(θk−ωk)Nk(x)2
−Eω

Ni(x)Ni(x)e−
k(θk−ωk)Nk(x)
Eω

e−
k(θk−ωk)Nk(x)

Eω

e−
k(θk−ωk)Nk(x)2
.
(8.14)
The expressions obtained, although apparently more complex, lead to much faster
algorithms. Here, the various statistical moments are taken with respect to the law
p(x ; ω). In theory, it is thus sufﬁcient to sample the law once with respect to a given
set of parameters ω. The moments concerning the law for the various values θn are
then evaluated on the samples of the law p(x ; ω). In practice, some precautions
should be taken however. If the two vectors of parameters θ and ω are too far apart,
the estimation of the expectations will not be accurate. If the estimation of the ex-
pectations is to be robust, there must be enough overlap between the two distributions
p(x ; ω) and p(x ; θ). During optimization, if the current value θn is too far from ω,
the law must be resampled taking ω = θn.

202
Bayesian Approach to Inverse Problems
The algorithmic scheme then becomes the following:
1) calculate the image statistics Ni(x0);
2) initialize θ0 and set n = 0;
3) sample the distribution with the current values of the parameters θn;
4) estimate a conﬁdence interval around θn;
5) ﬁnd θn+1 as the maximizer of the log-likelihood deﬁned by equation (8.12)
with ω = θn using formulae (8.13) and (8.14);
6) if θn+1 is at the edge of the conﬁdence interval, set n = n + 1 and go back to
3; otherwise θ = θn+1.
8.2.3. Approximations
Estimation in the ML sense thus proves very costly in computing time even if
importance sampling is used. Other estimators, approximating ML, give algorithms
that are sub-optimal but much more efﬁcient from the computing time standpoint.
8.2.3.1. Encoding methods
The encoding method proposed by Besag [BES 74], is a ﬁrst, simple alternative to
ML. It consists of deﬁning disjunct subsets of the image such that the pixels of a given
subset are conditionally independent.
Let us assume that X is a Gibbs-Markov ﬁeld (GMRF):
∀s ∈S, p(xs | xt, t ̸= s) = p(xs | xt, t ∈νs)
(8.15)
where νs is a neighborhood of s. Let S′ be a subset of S such that:
{s, t} ⊂S′ =⇒s /∈νt, t /∈νs.
(8.16)
The likelihood relative to the encoding set S′ can then be written:
Lc(θ) = p(xs, s ∈S′ | xt, t ∈S \ S′) =
1
s∈S′
p(xs | xt, t ∈νs).
(8.17)
In the case of a GMRF, the Ni(x) are written as a sum of local functions:
Ni(x) =

c(i)∈C
ni(xs, s ∈c(i))
(8.18)
c(i) is called a clique and contains a ﬁnite set of pixels such that:
{s, t} ⊂c(i) =⇒t ∈νs
(8.19)

Unsupervised Problems
203
The log-likelihood relative to the encoding set S′ can then be written:
log Lc(θ) =

s∈S′
log p(xs | xt, t ∈νs)
=

s∈S′
.
−

i
θi
 
c(i)∋s
ni(r, r ∈c(i))

−log Zs(xt, t ∈νs)
/
.
The advantage is that, here, we are dealing with a local partition function, which can
be evaluated without resorting to sampling:
Zs(θ) =

exp

−

i
θi
 
c(i)∋s
ni(r, r ∈c(i))
0
dxs.
(8.20)
Several estimators can be obtained by considering different encoding sets. How-
ever, for each estimator, only part of the data is used and the “optimum” way of com-
bining the various resulting estimators is still an open question. In the literature, the
estimator considered is usually deﬁned by the mean of the estimators over the various
encoding sets.
Let us take the example of 4-connexity. The neighborhood of a site s = (u, v) is
then deﬁned as follows:
ν(u,v) =

(u −1, v) ; (u + 1, v) ; (u, v −1) ; (u, v + 1)

.
(8.21)
The probability of the realization of x can then be written:
f(x ; θ) =
1
Zθ
exp
.
−

(u,v)∈S

θ1n1(x(u,v)) + θ2n2(x(u,v),(u+1,v)) + θ3n3(x(u,v),(u,v+1))
/
.
Two encoding sets can be deﬁned:
S1 =

(2u, 2v), (2u, 2v) ∈S

∪

(2u + 1, 2v + 1), (2u + 1, 2v + 1) ∈S

,
S2 =

(2u + 1, 2v), (2u + 1, 2v) ∈S

∪

(2u, 2v + 1), (2u, 2v + 1) ∈S

.
8.2.3.2. Pseudo-likelihood
At the cost of an approximation, pseudo-likelihood allows all the data to be taken
into account [BES 74]. It is deﬁned simply by extending equation (8.17) to the com-
plete set of sites:
PL(θ) =
1
s∈S
p(xs | xt, t ∈νs).
(8.22)

204
Bayesian Approach to Inverse Problems
As for the encoding method, equation (8.22) only brings in local partition functions.
If the pixels are conditionally independent, the pseudo-likelihood is equivalent to the
likelihood. If not, it is just an approximation. It often gives more accurate results than
the encoding method.
8.2.3.3. Mean ﬁeld
The mean ﬁeld approximation is a technique that was initially developed in sta-
tistical physics to study phase transition phenomena. It was then applied to image
processing [CEL 03, GEI 91, ZER 93, ZHA 93] based on GMRF models. At each
site, the mean ﬁeld approximation neglects the ﬂuctuations of the other sites, approx-
imating their states by the mean. The random ﬁeld is then approximated by a set of
independent random variables.
The energy function deﬁned in equation (8.5), approximated by the mean ﬁeld,
can be written:
Φ
MF
θ,s(xs) = Φθ

x = (xs, mt, t ̸= s)

,
(8.23)
where mt represents the mean of site t. Let us write function ΦMF
θ,s(xs) in the form:
Φ
MF
θ,s(xs) = Φ
MF,ν
θ,s (xs) + ¯Φ
MF
θ,s(mt, t ̸= s),
(8.24)
where ¯ΦMF
θ,s(xt, t ̸= s) does not depend on xs. The Markov structure (local interac-
tions) implies that function Φ
MF,ν
θ,s (xs) only depends on xs and the means of its various
neighbors mt, t ∈νs.
The mean ﬁeld approximation consists of approximating the marginal law in xs,
deﬁned by:
p(xs) =
1
Z(θ)

xt,t̸=s
exp {−Φθ(x)} dxt,
(8.25)
by the law:
p
MF
s (xs) =
1
Z MF
s (θ) exp

−Φ
MF
θ,s(xs)

,
(8.26)
which can also be written:
p
MF
s (xs) =
1
Z
MF,ν
s
(θ) exp

−Φ
MF,ν
θ,s (xs)

.
(8.27)
The Markov property then allows the mean ﬁeld approximation to be written as the
local conditional probability:
p
MF
s (xs) = p(xs | xt = mt, t ∈νs).
(8.28)
Thus, at each site, we have deﬁned marginal laws associated with independent ran-
dom variables. The mean ﬁeld approximation of the random ﬁeld under consideration

Unsupervised Problems
205
can then be written as follows:
p
MF(x) =
1
s∈S
p
MF
s (xs).
(8.29)
Mean ﬁeld estimation thus consists of maximizing the likelihood deﬁned by:
L
MF(θ) = p
MF(x0) =
1
s∈S
p
MF
s (x0(s)).
(8.30)
It is worth noting that the likelihood obtained by the mean ﬁeld approximation is very
close to the pseudo-likelihood. The difference lies in the conditional variables since,
here, neighboring sites are ﬁxed at their means whereas, for pseudo-likelihood, they
are ﬁxed at the values of the realization under consideration xt = x0(t). The difﬁculty
lies in estimating the mts as, unlike in the case of pseudo-likelihood, the values of the
conditioning variables are unknown here. In order to estimate these values, we state
that the mean of a site s, calculated for the approximation in s, must be equal to
that used for the approximation of the laws of the sites that are neighbors of s. By
deﬁnition, the mean of a site s, in the approximation sense, is given by:
ms = E
MF
s (xs) =

xsp
MF
s (xs) dxs =
1
Z
MF,ν
s
(θ)

xs exp

−Φ
MF,ν
θ,s (xs)

dxs.
This expectation depends on the means of the neighbors of s. We thus have an equa-
tion of the type:
ms = p
MF(mt, t ∈νs).
(8.31)
If we consider each of the sites, we obtain a system of equations. Solving this sys-
tem by an iterative algorithm gives an approximation of the mean values ms. The
likelihood, given by (8.30), is then easy to maximize as the terms of the product are
independent.
8.3. Indirectly observed ﬁeld
8.3.1. Statement of problem
We will now concern ourselves with the case where the system is governed by
equation (8.1) in which the noise process is present. We also assume that the linear
distortion H is known but can be different from the identity matrix. The aim here is
to estimate parameters θ and ϑ that control the probability distributions p(x ; θ) and
p(y | x ; ϑ) respectively. Here again, we will limit ourselves to ML estimators, so the
estimates are deﬁned by:
(θ, ϑ) = arg max
θ,ϑ
p(y ; θ, ϑ).
(8.32)

206
Bayesian Approach to Inverse Problems
An initial difﬁculty arises from the fact that, in most cases, it is not possible to obtain
the closed-form expression for p(y ; θ, ϑ), even to within a normalization factor. We
only have the relation:
p(y ; θ, ϑ) =

p(y, x ; θ, ϑ) dx =

p(y | x ; ϑ)p(x ; θ) dx
(8.33)
where p(y | x ; ϑ) and p(x ; θ) are part of the problem speciﬁcations but where the
integral above cannot be evaluated analytically. Expectation Maximization (EM) tech-
niques provide a fairly general approach for maximizing p(y ; θ, ϑ) on the basis of the
joint likelihood p(y, x ; θ, ϑ) = p(y | x ; ϑ)p(x ; θ), but without explicit evaluation
of the integral present in equation (8.33). We should stress that bringing in the quan-
tities p(y | x ; ϑ) and p(x ; θ) will lead to the difﬁculties mentioned in section 8.2.
We now present the EM algorithm before examining its application to hyperparameter
estimation.
8.3.2. EM algorithm
Let θ be a vector parameter that we intend to estimate by maximizing the like-
lihood p(y ; θ), where y is a vector of observed data. The EM algorithm is an it-
erative procedure that produces an increase in the likelihood at each iteration and
uses an auxiliary variable X whose role is, roughly, to make the extended likelihood
p(y | x ; θ)p(x ; θ) easier to evaluate than the original likelihood p(y ; θ) [DEM 77].
Let θ0 be the current value of the estimate and let us deﬁne the following quantities:
Q(θ, θ0 ; y)
Δ=

p(x | y ; θ0) log p(y, x ; θ) dx,
(8.34)
D(θ || θ0)
Δ=

p(x | y ; θ0) log p(x | y ; θ0)
p(x | y ; θ) dx.
(8.35)
We observe that Q(θ, θ0 ; y) can be interpreted as the following mathematical expec-
tation:
Q(θ, θ0 ; y) = E

log p(y, x ; θ) | y ; θ0
(8.36)
and that D(θ || θ0) is the Kullback pseudo-distance between the probability densities
p(x | y ; θ) and p(x | y ; θ0), which is always positive or zero [DAC 86]. In addition,
it is easy to obtain the relation:
log p(y ; θ) −log p(y ; θ0) = Q(θ, θ0 ; y) −Q(θ0, θ0 ; y) + D(θ || θ0).
(8.37)
By
the
positivity
of
the function D(θ || θ0),
any
value of
θ
such
that
Q(θ, θ0 ; y) > Q(θ0, θ0 ; y) produces an increase in the likelihood. The general
idea of the EM algorithm is to increase the likelihood at each iteration by choosing the

Unsupervised Problems
207
value of θ that maximizes Q(θ, θ0 ; y). One iteration of the algorithm comprises the
following two steps:
Expectation (E):
calculation of Q(θ, θk ; y),
(8.38)
Maximization (M):
θk+1 = arg max
θ
Q(θ, θk ; y).
(8.39)
It can be shown that this algorithm has a rate of convergence sufﬁcient to ensure con-
vergence towards a local maximum of the likelihood [DEM 77]. The EM algorithm
will only be of interest if the calculation and then the maximization of Q(θ, θk ; y)
are simpler than the maximization of the likelihood p(y ; θ).
8.3.3. Application to estimation of the parameters of a GMRF
Using the EM approach to maximize the likelihood p(y ; θ, ϑ) ﬁrst requires us
to choose the auxiliary variable X. Here, the obvious choice is to take the indirectly
observed ﬁeld as the auxiliary variable. It remains to be seen whether, with such a
choice, it is possible to evaluate the quantity Q deﬁned in equation (8.34), and then
to maximize it with respect to the hyperparameters. We recall that p(y | x) and p(x)
take the form given by (8.3)-(8.4). By putting these expressions into (8.36), we ob-
tain [ZHA 94]:
Q(θ, ϑ) =
E

−Φθ(x) −log Z(θ) | y ; θ0, ϑ0
+ E

−Ψϑ(y −Hx) −log Z(ϑ) | y ; θ0, ϑ0
where the dependence of Q on the parameters θ0 and ϑ0 and on y has been omitted
in order to lighten the notation. The expectations are taken relative to the probability
density p(x | y ; θ0, ϑ0) and it is clear that evaluating them analytically, which would
require speciﬁc calculation of integrals similar to that of equation (8.34), is not possi-
ble in general. However, because of the Markov nature of X, the MCMC techniques
described in the previous chapter are well suited to the numerical evaluation of such
expectations. Using the stochastic sampling techniques presented in section 7.4.2, it
is possible to draw N values {xn, 1 ≤n ≤N} at random, distributed according to
p(x | y ; θ0, ϑ0). The mathematical expectation of a quantity g(X) thus veriﬁes the
relation:
E

g(X) | y ; θ0, ϑ0
=
lim
N→+∞
1
N
N

n=1
g(xn)
and, for “sufﬁciently” large N:
E

g(X) | y ; θ0, ϑ0
≃1
N
N

n=1
g(xn).
Thus, the hyperparameters of a GMRF can be estimated by an EM algorithm accord-
ing to the following procedure:

208
Bayesian Approach to Inverse Problems
1) determine initial values θ0 and ϑ0;
2) at iteration k (current value of hyperparameters: θk and ϑk):
– draw N values {xn, 1 ≤n ≤N} according to p(x | y ; θk, ϑk),
– approximate E

−Φθ(x) −log Z(θ) | y ; θk, ϑk
by:
Qk
θ
Δ= 1
N
N

n=1

−Φθ(xn) −log Z(θ)

and E

−Ψϑ(y −Hx) −log Z(ϑ) | y ; θk, ϑk
by:
Qk
ϑ
Δ= 1
N
N

n=1

−Ψϑ(y −Hxn) −log Z(ϑ)

;
– θk+1 = arg maxθ Qk
θ et ϑk+1 = arg maxϑ Qk
ϑ;
3) stop iterations by a convergence criterion typically based upon the difference
between successive estimates of θ and ϑ.
We must obviously ask ourselves about the difﬁculties involved in evaluating then
maximizing the quantities Qθ and Qϑ. It should be stressed that, as far as Qθ is
concerned, we ﬁnd ourselves in a situation resembling that studied in the ﬁrst part of
this chapter, except that the direct observation of a realization of X is replaced by the
indirect observation of N realizations. This change has few practical consequences
in as much as most of the difﬁculties come from the normalization term Z(θ), which
is independent of the number of realizations. We will therefore need to use the tech-
niques presented in section 8.2: evaluate Qθ, then maximize it with respect to θ, at
each iteration, k, of the EM algorithm. Thus, the resulting procedure is numerically
very heavy. As for the parameter ϑ, the difﬁculty depends on the nature of the noise
b. If we keep to simple noise models, the evaluation of Qϑ and its maximization with
respect to ϑ can be carried out relatively easily. For an identically distributed, inde-
pendent, zero-mean, Gaussian noise, it is possible to obtain a closed-form expression
for Z(ϑ), then ϑk+1 [BRU 00, RED 84].
8.3.4. EM algorithm and gradient
The EM algorithm is not the only procedure that enables recursive maximization
of the likelihood. Often, when it is possible to use an EM algorithm, it is also possible
to maximize the log-likelihood by a gradient technique. If we return to the framework
deﬁned in section 8.3.2; the gradient of the log-likelihood is deﬁned by:
∇θ log p(y ; θ) =
1
p(y ; θ) ∇θp(y ; θ).
(8.40)

Unsupervised Problems
209
Under weak conditions of regularity for the extended likelihood p(y, x ; θ), we can
differentiate under the integral sign and write:
∇θ log p(y ; θ) =
1
p(y ; θ)

∇θp(y, x ; θ) dx
=
1
p(y ; θ)
 ∇θp(y, x ; θ)
p(x | y ; θ) p(x | y ; θ) dx
=
 ∇θp(y, x ; θ)
p(y, x ; θ) p(x | y ; θ) dx
=

∇θ log p(y, x ; θ) p(x | y ; θ) dx
(8.41)
= E(∇θ log p(y, x ; θ) | y ; θ) .
(8.42)
Equations (8.41-8.42) show that the gradient of the log-likelihood takes an expression
analogous to that for quantity Q, the term log p(y, x ; θ) being replaced by its gradient
with respect to θ.
The above equations suggest that it is possible to estimate the parameters of a
GMRF by a gradient method according to a scheme very close to that of an EM tech-
nique. If we go back to the notation of section 8.3.3, we see that the gradient of
log p(y, x ; θ, ϑ) with respect to the hyperparameters can be broken down into two
terms that can be expressed:
∇θ log p(y, x ; θ, ϑ) = ∇θ log p(x ; θ) = −∇θΦθ(x) −∇θZ(θ)
Z(θ)
∇ϑ log p(y, x ; θ, ϑ) = ∇ϑ log p(y | x ; ϑ) = −∇ϑΨϑ(y −Hx) −∇ϑZ(ϑ)
Z(ϑ)
.
The estimation of θ and ϑ by a gradient method can thus be implemented according
to the following procedure:
1) determine initial values θ0
0
k and ϑk):
– draw N values {xn, 1 ≤n ≤N} according to p(X | y ; θk, ϑk),
– approximate E

∇θ log p(y, x ; θk, ϑk) | y ; θk, ϑk
by:
Gk
θ
Δ= 1
N
N

n=1

−∇θΦθk(xn) −∇θZ(θk)/Z(θk)

and E

∇ϑ log p(y, x ; θk, ϑk) | y ; θk, ϑk
by:
Gk
ϑ
Δ= 1
N
N

n=1

−∇ϑΨϑk(y −Hxn) −∇ϑZ(ϑk)/Z(ϑk)

,
and ϑ ;
2) at iteration k (current values of hyperparameters θ

210
Bayesian Approach to Inverse Problems
– θk+1 = θk + μkGk
θ and ϑk+1 = ϑk + μkGk
ϑ, where μk is the step of the
gradient algorithm;
3) stop iterations by a convergence criterion typically based upon the difference
between successive estimates of θ and ϑ.
If we compare the EM and gradient approaches, we see that, in both cases, the dif-
ﬁculty of evaluating the likelihood p(y ; θ) is avoided by making use of the extended
likelihood p(y, x ; θ). Also in both cases, the mathematical expectation is evaluated
numerically by resorting to stochastic sampling techniques (see Chapter 7). As far
as differences are concerned, the EM approach includes a maximization step while
the gradient approach requires the evaluation of derivatives with respect to the hyper-
parameters. The difﬁculty involved in one or the other approach depends largely on
the form of p(y, x ; θ) and the choice between them is thus very dependent on the
problem to be tackled.
Concerning questions of convergence, both approaches are local iterative tech-
niques that can only be guaranteed to converge towards a local maximum of the like-
lihood, which, in general, is not convex. Both approaches have a linear convergence
rate but the effective speed of convergence depends strongly on the conditioning of
the estimation problem and the distance between the current value of the parameters
and the solution. Here again, the choice of an approach must be adapted to the char-
acteristics of the problem at hand. It is worth mentioning that, when both approaches
are possible, we can envisage using one of them for the ﬁrst iterations and the other
for the rest, or even alternating between the two so as to maximize the convergence
speed. An example of this type of technique is to be found in [RID 97].
8.3.5. Linear GMRF relative to hyperparameters
To illustrate what we have just seen, we give a few details below on the use of EM
and gradient techniques when X is a ﬁeld whose energy is linear with respect to the
parameters. p(x) thus takes the form given in equation (8.6). We also assume that
matrix H is known and that the additive noise b is Gaussian, zero-mean, independent,
identically distributed with variance ϑ. We thus have:
p(b) = (2πϑ)−P/2 exp
	
−∥b∥2 /2ϑ

(8.43)
where P is the size of b.
Whatever the technique used, each iteration requires N realizations of X to be
drawn according to p(x | y ; θk, ϑk). This is a traditional operation, amply described
in Chapter 7. Its computational volume essentially depends on the extent of the sup-
port of H and on the complexity of the neighborhood system of ﬁeld X.

Unsupervised Problems
211
If we use an EM technique, we next have to evaluate then maximize the quantities
Qk
θ and Qk
ϑ relative to θ and ϑ respectively. Note ﬁrst of all that the term Qk
ϑ does
not present any difﬁculty since, according to equation (8.43), the normalization term
Z(ϑ) is known explicitly and is equal to (2πϑ)P/2. From this, we simply deduce that
the equation for updating ϑ is:
ϑk+1 =
1
NP
N

n=1
∥y −Hxn∥2 .
(8.44)
The main difﬁculty occurs at the level of Qk
θ and, more precisely, the normalization
term Z(θ). As mentioned in section 8.3.3, the expression for Qk
θ is very close to
that for the log-likelihood of a directly observed ﬁeld given in equation (8.8) and all
the techniques for determining θ for a directly observed ﬁeld presented in section 8.2
can, in principle, be used for the step of maximization with respect to θ of the EM
algorithm. However, in practice, it is not possible to envisage using a cumbersome,
iterative, optimization method at each iteration of an EM algorithm that is itself quite
heavy. If the current values of the parameters θk and ϑk are not too far from the
solution, we can consider using the importance sampling technique, taking θk as the
reference parameter, or even keeping the same reference parameter over several iter-
ations of the EM algorithm. However, to avoid being faced with an insurmountable
volume of calculations, it is more often than not better to use the approximations de-
scribed in section 8.2.3.
Now let us look at the gradient approach. Once the N realizations of X have
been drawn according to p(x | y ; θk, ϑk), it is necessary to evaluate quantities Gk
θ
and Gk
ϑ. Here again, the term in ϑ causes little difﬁculty; it is easy to show that it can
be calculated explicitly and has the expression:
Gk
ϑ = 1
N
N

n=1
∥y −Hxn∥2
2ϑ2
−P
2ϑ
(8.45)
As far as the evaluation of Gk
θ is concerned, the calculations are similar to those lead-
ing to equation (8.9) and each component of Gk
θ can be written:

Gk
θ

i = −1
N
N

n=1
Ni(xn) + Eθk(Ni(x)) .
(8.46)
The evaluation of Eθk(Ni(x)) thus makes it necessary to sample X at each iteration,
this time according to the law p(x ; θk)2. To avoid this operation, we can also ap-
proach p(x ; θ) here as indicated in section 8.2.3, then calculate the gradient of the
approximation obtained.
2. Note that the resulting procedure has the stochastic approximation algorithm proposed
in [YOU 91] as a special case, for which the author gives the convergence conditions.

212
Bayesian Approach to Inverse Problems
The elements presented above point out the difﬁculty of unsupervised problems
and the complexity of the methods for solving them. In the example above, whatever
the approach chosen, at least one, and possibly two, stochastic sampling steps are
necessary at each iteration. The approximations that may lighten the implementation
of the approach cannot avoid reducing its robustness and reducing the quality of the
estimator. Great caution is therefore advised when employing such techniques.
8.3.6. Extensions and approximations
8.3.6.1. Generalized maximum likelihood
As mentioned above, estimating the hyperparameters of an indirectly observed
quantity generally necessitates a great volume of calculations, leading to the interest
of any approach intended to reduce this numerical complexity. Generalized maximum
likelihood (GML) methods were developed precisely with a view to simplifying the
calculations.
These techniques fall within the Bayesian framework used up to now. They con-
sist of simultaneously estimating the quantity of interest X and the hyperparameters
(θ, ϑ) by maximizing the generalized likelihood p(x, y ; θ, ϑ). We have:
(x∗, θ∗, ϑ∗) = arg max
x,θ,ϑ
p(x, y ; θ, ϑ)
(8.47)
where x∗, θ∗and ϑ∗are the estimates of x, θ and ϑ respectively in the GML sense.
Maximizing the generalized likelihood jointly with respect to all the variables
is still difﬁcult.
For this reason, we generally use a sub-optimal iterative block-
maximization procedure according to the following scheme:
xk+1 = arg max
x
p(x, y ; θk, ϑk)
(8.48)
(θk+1, ϑk+1) = arg max
θ,ϑ
p(xk+1, y ; θ, ϑ)
(8.49)
where k is the iteration index. Decomposing the joint law in the form p(x, y ; θ, ϑ) =
p(y | x ; ϑ)p(x ; θ) decouples the estimation of the hyperparameters as follows:
θk+1 = arg max
θ
p(xk+1 ; θ),
(8.50)
ϑk+1 = arg max
ϑ
p(y | xk+1 ; ϑ).
(8.51)
This approach is attractive in that the quantity of interest, x, and the hyperparame-
ters are estimated in a single framework by means of a single criterion: the general-
ized likelihood. Furthermore, the determination of x by equation (8.48) corresponds

Unsupervised Problems
213
exactly to the estimation of x with a MAP criterion, the hyperparameters taking the
value (θk, ϑk). In a similar way, the determination of the hyperparameters using equa-
tions (8.50-8.51) coincides exactly with their estimation by ML with the data observed
directly, x taking the value xk+1. At each iteration, we just need to solve a problem of
the same type as the one covered in section 8.2, which is a remarkable simpliﬁcation
with respect to the EM or gradient methods described in sections 8.3.2 and 8.3.4.
Nevertheless, the use of this approach leads to difﬁculties essentially linked with
the characteristics of the GML estimator and the relevance of the generalized like-
lihood criterion. Unlike the estimators of the maximum simple or posterior likeli-
hood [DAC 86], the characteristics of the GML estimator are not well known and it is
difﬁcult to link the estimates obtained with the true values of the parameters, even in an
asymptotic framework. Furthermore, it sometimes happens that the generalized likeli-
hood has no upper bound in the natural domain of the parameters (x, θ, ϑ), which can
cause the iterative procedure deﬁned by equations (8.48-8.49) to diverge [GAS 92].
From a practical point of view, several authors have reported obtaining interesting
results with the GML approach, both in the processing of images modeled as GM-
RFs [KHO 98, LAK 89] and in the processing of one-dimensional signals [CHA 96].
It is clear that the GML approach deserves attention because of its simplicity of imple-
mentation but the theoretical difﬁculties highlighted above should put us very much
on our guard when interpreting the results obtained.
8.3.6.2. Full Bayesian approach
The general idea underlying what are known as full Bayesian approaches is to
probabilize the hyperparameters of the problem by introducing second rank priors.
Staying within the framework of the hypotheses adopted in section 8.3.5, this means
that we probabilize quantities θ and ϑ for which the corresponding prior probabilities
will be noted p(θ) and p(ϑ) respectively. The quantity of interest x and hyperparam-
eters θ and ϑ are estimated from the full posterior likelihood p(x, θ, ϑ | y). Note that
this makes sense since θ and ϑ are now probabilized.
The question then arises as to how to use the full posterior likelihood. The most
immediate approach would be to maximize it jointly with respect to x, θ and ϑ. How-
ever, this maximization can prove tricky, depending on the form given to the second
ranking priors. Above all though, the characteristics of the corresponding estimator
are as poorly known as those of the GML estimator described in the section 8.3.6.1.
To convince ourselves of this, we just need to note that when p(θ) and p(ϑ) are den-
sities that are uniform over the domain of the parameters, the full likelihood becomes
identical to the generalized likelihood and maximizing the full likelihood then has the
same limitations as those of the GML approach. This is why full Bayesian approaches
are based on sampling quantities X, θ and ϑ according to probability p(x, θ, ϑ | y),
the sampling being performed in practice using the stochastic sampling methods pre-
sented in section 7.4.2. We thus construct a Markov chain {(Xk, θk, ϑk)} the elements

214
Bayesian Approach to Inverse Problems
of which are chosen at random by means of the following complete conditionals:
xk+1 :
p(x | y, θk, ϑk) ∝p(y | x, ϑk) p(x | θk),
(8.52)
θk+1 :
p(θ | y, xk+1, ϑk) ∝p(xk+1 | θ) p(θ),
(8.53)
ϑk+1 :
p(ϑ | y, xk+1, θk+1) ∝p(y | xk+1, ϑ) p(ϑ).
(8.54)
Sampling X according to equation (8.52) has already been mentioned several times in
this chapter and poses no particular problems. At this stage, two main choices remain
to be made: (i) the choice of p(θ) and p(ϑ) that both makes the second rank priors
pertinent and gives the possibility of evaluating full marginals, or sampling according
to the latter; (ii) the choice of the estimator which will use the quantities produced by
the sampler of equations (8.52) to (8.54).
Concerning point (ii), as we saw in section 8.3.3, the sampling methods allow
the mathematical expectation to be calculated empirically by simple averaging once
the Markov chain has reached its equilibrium state. This mathematical expectation is
taken with respect to p(x, θ, ϑ | y), which, for θ and ϑ, gives the estimators according
to the posterior mean θ = E(θ | y) and ϑ = E(ϑ | y). Averaging the values of θ and
ϑ provided by the sampler thus approaches the above estimates. We also observe that,
although the stochastic sampler uses full conditional probabilities, the estimate ob-
tained is marginalized with respect to all the quantities other than the observations y.
For point (i), the way to proceed largely depends on the availability of precise prior
information on the hyperparameters. If this information exists, the problem is close to
that of traditional Bayesian estimation, the difﬁculty being to formalize the prior infor-
mation in the form of a probability law and adjust the trade-off between the accuracy of
the prior model and the complexity of the resulting estimator. However, in general we
have only very little prior information on the hyperparameters, except perhaps support
information. A simple, prudent solution is then to take a function that is uniform over
the deﬁnition domain of the parameter as the second rank prior. A conjugate prior
can also be chosen, i.e., such that the second rank prior and the corresponding full
conditional belong to the same parameterized family of probabilities. This technique,
which is fairly widespread in the literature [CHE 96, DUN 97, MCM 96], seems to
correspond to esthetic rather that practical concerns as introducing a conjugate prior
rarely simpliﬁes the main problem, that of sampling the hyperparameters according to
the full conditional. To do this, all the techniques conventionally used in statistics can
be employed [PRE 92].
Once the hyperparameters have been estimated according to the above procedure,
we can go on to estimate X in a supervised framework by using either one of the
methods presented in Chapter 7 or the same technique and the same estimator as for
the hyperparameters [MCM 96]. By averaging the values of x obtained during the
iterations deﬁned by equations (8.52) to (8.54), we obtain an approximate value of

Unsupervised Problems
215
E(X | y), so it is not necessary to use a different procedure to estimate X. This type
of approach is adopted in [CHE 96] for a one-dimensional problem and in [DUN 97],
where a simpliﬁed image model involving a Markov network is also used. The choice
of one approach or the other for estimating X is essentially a question of ﬁnding an
acceptable compromise between simplicity of use and quality of results. We should
also stress that, by adopting the full Bayesian approach, it is possible to include lin-
ear degradation H among the quantities to be estimated. This possibility has been
successfully used in a one-dimensional framework [CHE 96] but the same does not
appear to be true in 2D, perhaps because of the volume of the calculations and the
convergence of the stochastic sampler.
8.4. Conclusion
In this chapter, we have presented the main tools for estimating the hyperparam-
eters when solving multidimensional inverse problems with Markov regularization.
One of the major characteristics of the techniques presented is their complexity or
heaviness, from both the methodological and algorithmic points of view. It is thus
legitimate to wonder if it is really useful or desirable to resort to such methods.
Of course, it is always possible to set the hyperparameters empirically. For a given
application with a well speciﬁed type of data, the parameters can be ﬁxed after a cal-
ibration phase. However, as a general rule, the results of the estimation of X vary
greatly depending on the hyperparameters and the determination of an acceptable
range of values can be difﬁcult, particularly for an operator who is not a specialist
in signal or image processing. Furthermore, for a given problem, the range of accept-
able values can vary according to the experimental conditions. The development of
unsupervised methods is thus of real interest.
Here, we have only presented methods based on the use of a likelihood. There are
other approaches for estimating hyperparameters, such as cross-validation [GOL 79]
or the “L-curve” [HAN 92], but their application to the problem treated here seems
neither very realistic from an algorithmic point of view nor well founded from a
methodological one.
As already pointed out, the methods we have described are relatively complex,
which may be seen as an indication of the difﬁculty of the problem of hyperparameter
estimation. We can wonder if the quality of the results is proportional to the efforts
needed to obtain them. From this point of view, the situation is contrasted. For 1D
signals, synthetic or real, the likelihood-based techniques generally produce accept-
able results. In 2D, in a simulation framework where the image to be processed is a
realization of the GMRF that regularizes the inversion, the results are very encourag-
ing (see, for example, [FOR 93]). On the other hand, for real data, the same approach
shows a signiﬁcant lack of robustness with respect to the image to be processed. The

216
Bayesian Approach to Inverse Problems
parameters that lead to the best results are not always those obtained by maximum
likelihood [DES 99]. This seems to suggest that GMRFs, although useful for regu-
larizing the estimation of X, do not model images coming from the real world very
faithfully. This is why the search for more effective estimation methods and also for
models that are more robust and more faithful to the solution sought is an interesting
path to follow in the aim of improving the methods for estimating hyperparameters. It
is also why, as things stand at the moment, it is necessary to be meticulous in making
validations when such methods are employed.
8.5. Bibliography
[BES 74] BESAG J. E., “Spatial interaction and the statistical analysis of lattice systems (with
discussion)”, J. R. Statist. Soc. B, vol. 36, num. 2, p. 192-236, 1974.
[BRU 00] BRUZZONE L., FERNANDEZ PRIETO D., “Automatic analysis of the difference
image for unsupervised change detection”, IEEE Trans. Geosci. Remote Sensing, vol. 38,
p. 1171-1182, 2000.
[CEL 03] CELEUX G., FORBES F., PEYRARD N., “EM procedures using mean ﬁeld-like ap-
proximations for Markov model-based image segmentation”, Pattern Recognition, vol. 36,
num. 1, p. 131-144, 2003.
[CHA 96] CHAMPAGNAT F., GOUSSARD Y., IDIER J., “Unsupervised deconvolution of
sparse spike trains using stochastic approximation”, IEEE Trans. Signal Processing, vol. 44,
num. 12, p. 2988-2998, Dec. 1996.
[CHE 96] CHENG Q., CHEN R., LI T.-H., “Simultaneous wavelet estimation and deconvolu-
tion of reﬂection seismic signals”, IEEE Trans. Geosci. Remote Sensing, vol. 34, p. 377-
384, Mar. 1996.
[DAC 86] DACUNHA-CASTELLE D., DUFLO M., Probability and Statistics, vol. 1, Springer
Verlag, New York, NY, 1986.
[DEM 77] DEMPSTER A. P., LAIRD N. M., RUBIN D. B., “Maximum likelihood from in-
complete data via the EM algorithm”, J. R. Statist. Soc. B, vol. 39, p. 1-38, 1977.
[DES 99] DESCOMBES X., MORRIS R., ZERUBIA J., BERTHOD M., “Estimation of Markov
random ﬁeld prior parameters using Markov chain Monte Carlo maximum likelihood”,
IEEE Trans. Image Processing, vol. 8, p. 954-963, 1999.
[DUN 97] DUNMUR A. P., TITTERINGTON D. M., “Computational Bayesian analysis of
hidden Markov mesh models”, IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-19,
num. 11, p. 1296-1300, Nov. 1997.
[FOR 93] FORTIER N., DEMOMENT G., GOUSSARD Y., “GCV and ML methods of deter-
mining parameters in image restoration by regularization: fast computation in the spatial
domain and experimental comparison”, J. Visual Comm. Image Repres., vol. 4, num. 2,
p. 157-170, June 1993.
[GAS 92] GASSIAT E., MONFRONT F., GOUSSARD Y., “On simultaneous signal estimation
and parameter identiﬁcation using a generalized likelihood approach”, IEEE Trans. Inf.
Theory, vol. 38, p. 157-162, Jan. 1992.

Unsupervised Problems
217
[GEI 91] GEIGER D., GIROSI F., “Parallel and deterministic algorithms from MRF’s: Surface
reconstruction”, IEEE Trans. Pattern Anal. Mach. Intell., vol. 13, num. 5, p. 401-412, May
1991.
[GEY 92] GEYER C. J., THOMPSON E. A., “Constrained Monte Carlo maximum likelihood
for dependent data”, J. R. Statist. Soc. B, vol. 54, p. 657-699, 1992.
[GOL 79] GOLUB G. H., HEATH M., WAHBA G., “Generalized cross-validation as a method
for choosing a good ridge parameter”, Technometrics, vol. 21, num. 2, p. 215-223, May
1979.
[HAN 92] HANSEN P., “Analysis of discrete ill-posed problems by means of the L-curve”,
SIAM Rev., vol. 34, p. 561-580, 1992.
[KHO 98] KHOUMRI M., BLANC-FERAUD L., ZERUBIA J., “Unsupervised deconvolution of
satellite images”, in Proc. IEEE ICIP, vol. 2, Chicago, IL, p. 84-87, 1998.
[LAK 89] LAKSHMANAN S., DERIN H., “Simultaneous parameter estimation and segmenta-
tion of Gibbs random ﬁelds using simulated annealing”, IEEE Trans. Pattern Anal. Mach.
Intell., vol. PAMI-11, num. 8, p. 799-813, Aug. 1989.
[MCM 96] MCMILLAN N. J., BERLINER L. M., “Hierarchical image reconstruction using
Markov random ﬁelds”, in Bayesian Statistics 5, Spain, Fifth Valencia Int. Meeting on
Bayesian Statistics, June 1996.
[PRE 92] PRESS W. H., TEUKOLSKY S. A., VETTERLING W. T., FLANNERY B. P., Numer-
ical Recipes in C, the Art of Scientiﬁc Computing, Cambridge University Press, New York,
2nd edition, 1992.
[RED 84] REDNER R. A., WALKER H. F., “Mixture densities, maximum likelihood and the
EM algorithm”, SIAM Rev., vol. 26, num. 2, p. 195-239, Apr. 1984.
[RID 97] RIDOLFI A., Maximum Likelihood Estimation of Hidden Markov Model Param-
eters, with Application to Medical Image Segmentation, Thesis, Politecnico di Milano,
Facoltà di Ingegneria, Milan, Italy, 1997.
[YOU 88] YOUNES L., “Estimation and annealing for Gibbsian ﬁelds”,
Ann. Inst. Henri
Poincaré, vol. 24, num. 2, p. 269-294, Feb. 1988.
[YOU 91] YOUNES L., “Maximum likelihood estimation for Gibbs ﬁelds”, in POSSOLO A.
(Ed.), Spatial Statistics and Imaging: Proceedings of an AMS-IMS-SIAM Joint Conference,
Lecture Notes – Monograph Series, Hayward, Institute of Mathematical Statistics, 1991.
[ZER 93] ZERUBIA J., CHELLAPPA R., “Mean ﬁeld annealing using compound GMRF for
edge detection and image estimation”, IEEE Transactions on Neural Networks, vol. TNN-
4, p. 703-709, 1993.
[ZHA 93] ZHANG J., “The mean ﬁeld theory in EM procedures for blind Markov random ﬁeld
image restoration”, IEEE Trans. Image Processing, vol. 2, num. 1, p. 27-40, Jan. 1993.
[ZHA 94] ZHANG J., MODESTINO J. W., LANGAN D. A., “Maximum-likelihood parameter
estimation for unsupervised stochastic model-based image segmentation”,
IEEE Trans.
Image Processing, vol. 3, p. 404-420, 1994.


PART IV
Some Applications


Chapter 9
Deconvolution Applied to Ultrasonic
Non-destructive Evaluation
9.1. Introduction
In non-destructive evaluation (NDE) using ultrasound, a wave is emitted at the
outer surface of the part under inspection and propagates through the material. At each
discontinuity the incident beam meets, a wave is reﬂected and the receiver located
on the surface of the part thus receives a succession of reﬂected waves. Thus, the
received signal results from the interaction of the wave with the material and gives
information on the discontinuities in the medium. The operator’s goal is to ﬁnd out,
from the received signal, whether the part has defects and, if so, to determine their
characteristics. In concrete terms, a discontinuity shows up in the measurements as
an echo having a duration equivalent to the period of the incident wave. However,
the wavelength is of the same order as or even longer than the distance between two
discontinuities and the presence of two defects close together gives a single common
echo. Deconvolution helps with the interpretation of the signals by getting around the
lack of resolution of the measurements.
We will start by presenting the difﬁculties involved in interpreting the measure-
ments by means of an example of a weld evaluation using ultrasound. Then, the direct
model will be introduced in the form of a convolution: the data is taken to be the
result of the convolution between the discontinuities and a convolution kernel formed
from the transmitted wave. When the waveform is known, it is possible to infer the
discontinuities by applying the approaches already proposed in Chapter 5 concerning
Chapter written by Stéphane GAUTIER, Frédéric CHAMPAGNAT and Jérôme IDIER.

222
Bayesian Approach to Inverse Problems
spike train deconvolution. In reality, the incident wave is very often unknown and
estimating the discontinuities is similar to solving a blind deconvolution problem for
which the discontinuities and the kernel are unknown. The heart of this chapter is
therefore devoted to the blind deconvolution techniques put forward for various types
of applications. Finally, in the case of NDE, the contributions made by deconvolution
are illustrated on the weld evaluation example.
9.2. Example of evaluation and difﬁculties of interpretation
Checking a weld was chosen as the example because the data concerned is notori-
ously difﬁcult to interpret. The characteristics of welding and the evaluation principle
will be recalled ﬁrst. Then the conclusions drawn simply from the exploitation of the
raw measurements will be presented. Finally, we will deﬁne the objectives ﬁxed for
the processing to help with the interpretation of the measurements.
The data used in this chapter were provided by EDF-R&D, the R&D division
of the French power utility EDF. The expert NDE knowledge required to write the
chapter was generously contributed by Daniel Villard, EDF-R&D, to whom we are
most grateful.
9.2.1. Description of the part to be inspected
The part to be inspected is a stainless steel/stainless steel weld with a notch 3 mm
high and 0.2 mm thick made by electro-erosion at the level of the welding cord, paral-
lel to the weld axis, see Figure 9.1. This notch is representative of a crack; it consti-
tutes a planar defect, so called by opposition to “volume” defects, which are thicker.
40 mm
weld
3mm high notch,
0.2mm thick
Figure 9.1. Cross-section of the part to be inspected
9.2.2. Evaluation principle
The evaluation takes place at the upper surface of the block (Figure 9.2). Both the
emitter and the receiver are embedded in the sensor, which scans the surface of the

Deconvolution Applied to Ultrasonic Non-destructive Evaluation
223
part, perpendicularly to the welding axis, emitting 2 MHz longitudinal waves inclined
at 60◦to the normal of the surface of the block.
Two sorts of echoes are generated in response to the incident wave: a corner echo
backscattered by reﬂection at the bottom of the notch and a diffraction echo created
at the top of the notch and scattered in all directions (see Figure 9.2). The latter is
only generated when a diffracting point is present and would be absent in the case of
a volume defect. It is thus characteristic of a planar defect. For some positions of the
sensor, these two echoes may appear simultaneously because of the width of the beam.
The evaluation should thus (i) detect a planar defect characterized by the presence of
a diffraction echo in addition to the corner echo and (ii) determine the height of the
defect from the time difference between the two echoes.
sensor shifting
Figure 9.2. Corner and diffraction echoes
9.2.3. Evaluation results and interpretation
The signal recorded for a given sensor position is called an A-scan. The image
produced by expressing the A-scans obtained for successive sensor positions as grey
levels is called a B-scan. The B-scan corresponding to the evaluation described above
is shown in Figure 9.3a. The presence of a high amplitude echo can be distinguished
over a large part of it. This echo appears to be slanted, which signiﬁes that the time to
cover the path decreased as the sensor moved.
Using this echo, the operator can detect the presence of a defect but cannot ﬁnd its
characteristics. It is very difﬁcult to distinguish between corner and diffraction echoes.
In A-scan number 40, which is given as an example (see Figure 9.3b), the diffraction
and corner echoes merge into one another: the top and bottom of the notch are very
close together and result in a common echo.
In the evaluation case, the raw data are very difﬁcult to use; it is impossible to state
that the defect is planar and, a fortiori, to determine its height. Interpreting the data is
thus a problem.

224
Bayesian Approach to Inverse Problems
A−Scan number
Time (μs)
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
Clue on the presence
of a defect 
(a) B-scan
0
5
10
15
20
25
30
35
40
45
50
−60
−40
−20
0
20
40
60
Time (μs)
Amplitude
Overlapping corner
and diffraction echoes
(b) A-scan # 40
Figure 9.3. Raw data corresponding to the system of Figure 9.2
9.2.4. Help with interpretation by restoration of discontinuities
It can be seen from the example above that one of the main difﬁculties in interpret-
ing ultrasonic test data stems from the lack of resolution of the measurements. The
presence of several reﬂectors very close together results in a common echo. Thus,
processing that could bring out the discontinuities masked in the data would be of
great help for the interpretation. In the above mentioned example, this would enable
the two reﬂectors associated with the angle and the top of the notch to be separated
and the height of the defect to be deduced. In what follows, we will therefore study
processing methods for restoring discontinuities.

Deconvolution Applied to Ultrasonic Non-destructive Evaluation
225
9.3. Deﬁnition of direct convolution model
As the aim of the processing is to restore the discontinuities, the part is repre-
sented by a function that is characteristic of what we are looking for. The reﬂectivity
is chosen for this purpose as this function is related to the spatial derivative of the
acoustic impedance: it is non-zero at a discontinuity (the impedance changes) and is
zero everywhere else (the impedance is constant). Starting with this representation,
the reﬂectivity is connected to the measurements by a convolution model: each A-
scan is interpreted as the result of convolution between the reﬂectivity function and a
kernel representing the wavelet that propagates in the object. A rigorous justiﬁcation
of this model would necessitate a series of restrictive hypotheses on the properties of
the material under study: the velocity, C, of the wave in the medium must be constant,
so the material must have only small inhomogeneities. Finally, the propagation is as-
sumed to be one-dimensional and the measurements are considered A-scan by A-scan.
We will use the notations r(d) for the reﬂectivity at depth d, h(t) for the convolution
kernel, and y(t) for the measurement. The observation equation can thus be written:
y(t) =

h(s) r(C(t −s)) ds.
The numerical framework of the processing performed on the data leads us to
choose a discrete representation of the model. Thus, a sampled echogram y is assumed
to be the result of a one-dimensional, discrete convolution between wavelet h emitted
by the sensor and the sequence of reﬂectivities r situated along the path followed by
the beam. To take account of measurement and model errors, this convolution result
is perturbed by additive noise b, assumed to be independent of r, white and Gaussian.
The direct model for an echogram can thus be written in the form:
y = Hr + b
(9.1)
where H is a Toeplitz matrix composed of elements of h.
This model is rather rough. In particular, it does not take account of the atten-
uation and deformation of the wave during propagation. Also, the width of the ul-
trasound beam is not modeled: this would make it necessary to use a two- or even
three-dimensional kernel [SAI 85]. Finally, reﬂections connected with the structure of
the material are not modeled although they may be non-negligible for some types of
steel. Rigourous modeling of ultrasonic wave propagation in a material is a very com-
plex task and use of the simpliﬁed model in the form of a convolution is rarely seen in
the NDE community. However, the direct model is chosen for processing and, from
this point of view, the convolution model provides a good compromise between repre-
sentativeness of the physical phenomena and the possibilities for efﬁcient exploitation
of the processing.

226
Bayesian Approach to Inverse Problems
9.4. Blind deconvolution
In as far as the insoniﬁed medium is made up of homogeneous zones, the reﬂec-
tivities can be modeled by a succession of spikes where each spike corresponds to a
discontinuity. In light of this prior information on the reﬂectivity and the direct con-
volution model (9.1), estimating the reﬂectivities is similar to solving a spike train
deconvolution problem for which the input and the kernel are the reﬂectivity and the
wavelet respectively. In the case where the wave is known, the spike train decon-
volution techniques presented in Chapter 5 should enable the discontinuities to be
recovered. In fact, the incident wave is often unknown and it is necessary to solve
a blind deconvolution problem where both the input and the convolution kernel are
unknown.
This section will ﬁrst give an overview of the various approaches to be found in
the literature for solving blind deconvolution problems. An extension of the L2Hy
and BG spike deconvolution methods will then be presented. The resulting methods
make it possible to take account of phase deformations occurring in the wavelet as
it propagates and corresponding to an enrichment of the direct model, introduced via
bivariate reﬂectivity. They are christened DL2Hy and DBG, the “D” referring to the
double reﬂectivity. Initially put forward to take phase rotations into account in ultra-
sound imaging, these methods also palliate possible phase imperfections in the wavelet
introduced for the deconvolution. Finally, an original blind deconvolution approach
will be developed using sequential exploitation of a sub-optimal wavelet estimation
technique and the DL2Hy and DBG deconvolution methods.
9.4.1. Overview of approaches for blind deconvolution
This subsection presents the principal approaches suggested for solving a blind
deconvolution problem. The presentation centers around the methodology and thus
concerns a broader ﬁeld of application than ultrasound imaging. We recall that, in the
speciﬁc case of deconvolution in NDE, the input and kernel are the reﬂectivity and the
wavelet respectively.
The most widespread approach to be found in the literature gets around the lack
of knowledge of the kernel by ﬁnding an estimator of the input that does not depend
explicitly on the kernel. This ﬁrst path is followed by the predictive deconvolution,
minimum entropy deconvolution, and “multipulse” techniques. A second approach
estimates the kernel then applies a spike deconvolution technique. Finally, some ap-
proaches set out to estimate the kernel and input jointly.
9.4.1.1. Predictive deconvolution
The ﬁrst work on blind deconvolution was carried out for geophysical applica-
tions. Among these works, the 1950s studies by Robinson on predictive deconvolu-
tion [ROB 67] have conditioned much research activity up to the present day. In this

Deconvolution Applied to Ultrasonic Non-destructive Evaluation
227
approach, the convolution model is implicitly noise-free; the measurement is the result
of input ﬁltering using a recursive ﬁlter according to:
yn =
q

ℓ=1
aℓyn−ℓ+ rn,
(9.2)
which comes down to considering the kernel as the IR of a stable, causal, autore-
gressive ﬁlter. If the input is also assumed to be white, predictive deconvolution ﬁrst
estimates the prediction coefﬁcients, and then identiﬁes the input to the prediction
error. The estimator of the prediction vector a = [a1, . . . , aq]T is obtained by min-
imizing a least squares criterion formed from equation (9.2), for an ad hoc boundary
hypothesis such that yn = 0, ∀n < 1:
a = arg min
a

n
.
yn −
q

ℓ=1
aℓyn−ℓ
/2
.
(9.3)
The input is then given by the prediction error of the minimum norm as:
rn = yn −
q

ℓ=1
aℓyn−ℓ,
(9.4)
and any reference to the coefﬁcients of the IR has thus disappeared from the deﬁnition
of the estimated input.
This approach only makes use of the second order characteristics of the measure-
ments. In addition, the estimation of vector a according to (9.3) is equivalent to solv-
ing an AR spectral analysis problem: it gives a recursive ﬁlter having an amplitude
spectrum which matches that of the measurements. For some boundary hypotheses,
the ﬁlter obtained is causal and stable. In the literature, this ﬁlter is said to be mini-
mum phase [ORF 85]. Other stable (but not causal) AR ﬁlters with the same amplitude
spectrum could be identiﬁed by taking the poles outside the unit circle.
This way of looking at deconvolution where, on the one hand, only the spectral
characteristics of the kernel are used and, on the other, the reﬂectivity is identiﬁed
with the error of estimation of the coefﬁcients of a ﬁlter, is still widely shared today.
The restriction to only second order characteristics is often put forward to ex-
plain the limited performance of predictive deconvolution. Many studies have been
conducted to exploit higher order (higher than two) statistics of the measurements
and thus design estimators sensitive to the phase. However, these approaches require
a large number of measurement points, which severely limits their ﬁeld of applica-
tion [LAZ 93].
Finally, the choice of a criterion for the quadratic prediction error comes down
to implicitly supposing that the input is Gaussian. This shows us that, to the great

228
Bayesian Approach to Inverse Problems
displeasure of many supporters of predictive deconvolution, this approach introduces
prior information on the input. Thus, application of an Lp (p < 2) norm on the
prediction error recovers spikier inputs than in the case of conventional predictive
deconvolution without, however, reaching the performance of L2LP deconvolution. In
fact, not taking observation noise and modeling errors into consideration constitutes
the weak point of approaches connected with predictive deconvolution.
9.4.1.2. Minimum entropy deconvolution
In the seismic-reﬂection context, Wiggins [WIG 78] proposes ﬁnding the input by
linear ﬁltering of the measurements, in the form (9.4), but his method for obtaining
the coefﬁcients of the ﬁlter differs from predictive deconvolution. Wiggins deﬁnes the
“varimax” norm to measure the disorder in a signal. Assuming the input to be made up
of impulses, he ﬁnds the ﬁlter coefﬁcients that minimize the disorder of the input. As-
sociating the notion of disorder with entropy, the author calls his approach minimum
entropy deconvolution. Here again, any reference to the kernel has disappeared; im-
plicitly, the latter remains the IR of a causal recursive ﬁlter. The empirical approach in
fact consists of ﬁnding the “inverse ﬁlter” of the convolution kernel by cleverly taking
advantage of prior information on the input. Nevertheless, its performance remains
limited by the implicit hypothesis of ﬁlter causality and the failure to take observation
noise and modeling errors into account.
9.4.1.3. Deconvolution by “multipulse” technique
This approach was put forward in [ATA 82] as a speech encoding method of the
LPC (linear predictive coding) type but its principle can be adapted to pulse train de-
convolution [COO 90]. It is close to that of the minimum entropy method: model the
input in AR form, as in predictive deconvolution, and introduce the pulse character
of the input. Here, this character is imposed in an ad hoc way by locating the max-
imum prediction error peaks and iterating (or not) a procedure alternating detection
of the peaks, re-estimation of the amplitudes of the previously detected peaks and re-
estimation of the coefﬁcients of the ﬁlter. Overall, this method suffers from the same
restrictions as Wiggins’ method.
9.4.1.4. Sequential estimation: estimation of the kernel, then the input
Using a deconvolution method suited to the case of a known IR, the solution of a
blind deconvolution problem can be carried out in two stages: estimation of the ker-
nel, then estimation of the input with the help of available deconvolution techniques.
Historically, this way of proceeding did not ﬁnd favor because efﬁcient deconvolution
tools were not available. However, now that more efﬁcient deconvolution methods
have been developed, this approach seems natural to us. Relative to conventional
deconvolution, the extra difﬁculty lies in estimating the kernel from the measure-
ments when the input is unknown. In fact, little work has been done in this direction.
The main method for estimating the kernel presented here was initially proposed by
Vivet [VIV 89] for NDE, but it can be extended to other applications.

Deconvolution Applied to Ultrasonic Non-destructive Evaluation
229
With the hypothesis of the input being white and there being no noise, the ampli-
tude spectrum of the measurements corresponds to that of the kernel. The proposed
method thus consists of obtaining a wavelet having an amplitude spectrum identical
to that of the measurements. As recalled in section 9.4.1.1, the estimator (9.3) pro-
vides precisely the coefﬁcients of such a ﬁlter in recursive form. In the absence of
knowledge on the phase of the wavelet, the kernel selected is the truncated IR of the
minimum phase ﬁlter. This IR is calculated by recursively applying
hn =
q

ℓ=1
aℓhn−ℓ+ δn,
(9.5)
with hn = 0, ∀n < 0 as initialization. With reference to the name of the ﬁlter, this
kernel is also called “minimum phase”.
The main limitation of this approach is, of course, the arbitrary choice of the phase
of the kernel. This limitation can be compensated by the extensions of the deconvolu-
tion techniques that will be presented below.
9.4.1.5. Joint estimation of kernel and input
In the framework of a Bayesian approach, the way of working proposed for spike
train deconvolution can be extended perfectly naturally by using a joint density that is
a function of the kernel and the input (see Chapter 3). For this purpose, a prior model
is deﬁned for the kernel in the same way as for the prior model of the input. Typically,
in the case of geophysical or NDE applications, the prior model of the kernel can be a
correlated Gaussian process corresponding to smoothness prior information. Various
paths can then be explored to exploit the joint density.
We will look at the maximum a posteriori estimator ﬁrst, using Bernoulli-Gaussian
(BG) prior models for the input [GOU 89] at the beginning, then convex Lp mod-
els [GAU 96, GAU 97]. The calculation requires the minimization of criteria whose
properties are poorly known1. It is carried out by alternately minimizing the crite-
rion according to the kernel and the input. In both cases, the results of the pro-
cessing on simulated data appear convincing. However, in the case of NDE appli-
cations [GAU 96], this approach has proved to be less efﬁcient than the one proposed
in the subsections that follow.
Finally, more recently, the estimator of the posterior mean [CHE 96] has been
proposed for a BG-type input. To this end, a series of samples corresponding to real-
izations according to the posterior joint density are generated using a Gibbs sampler
(see Table 7.1). Then the estimator of the posterior mean is approached by empirical
means on the available samples.
1. Here we can again voice the reserves mentioned in Chapter 3 concerning the generalized
maximum likelihood — see section 3.5.

230
Bayesian Approach to Inverse Problems
9.4.2. DL2Hy/DBG deconvolution
In the direct model proposed in section 9.3, the wave is assumed not to deform as
it propagates. However, this hypothesis is not very realistic in ultrasound imaging. In
particular, the form and/or the polarity of the echoes is reputed to vary with the type of
defect encountered. These phenomena probably correspond to changes in the phase of
the wave but remain poorly understood. The direct model can be improved by taking
into account possible wave deformations, modeled by phase “rotations” of the wavelet
at the level of each reﬂectivity. Modeling the rotations leads to the deﬁnition of a
double reﬂectivity, with which some prior information is associated. This approach
was ﬁrst put forward in Bernoulli-Gaussian deconvolution [CHA 93], then extended
to convex prior models [GAU 97]. It was initially developed for ultrasonic NDE but
would be suitable for other applications, in particular when the wave undergoes phase
deformations.
9.4.2.1. Improved direct model
Let h be a function of a real variable. g, its Hilbert transform (HT), is deﬁned as
the function that has the same amplitude spectrum as h but whose phase is shifted by
π/2. Function hθ, such that hθ = h cos θ+g sin θ, has a phase that is shifted by θ with
respect to that of h; but, hθ and h have the same amplitude spectrum. Thus, a signal
with a phase that has been shifted by a constant can be expressed as a linear combina-
tion of the initial signal and its HT. By making use of the linearity of the convolution,
the model of the phase rotations for each reﬂectivity can be transferred on to a split
reﬂectivity sequence (r, s), where r and s are convolved by the wavelet and its HT
respectively. The direct model can thus be expressed as y = Hr + Gs + b, where y
is the observed trace, H the convolution matrix corresponding to the known wavelet,
G the convolution matrix associated with the HT of the wavelet, and b white, zero-
mean, Gaussian noise independent of r and s. This model is purely an enrichment of
the direct model with respect to the simple convolution model: the double reﬂectivity
is simply introduced to help model the wavelet phase rotation phenomena.
9.4.2.2. Prior information on double reﬂectivity
The deconvolution process inverts this direct model to estimate an (r, s) pair. As
before, the inversion requires prior information to be brought in. The reﬂectivity is
taken to be spiky: the components of r and s are, a priori, zero except for where
discontinuities occur. Similarly, the reﬂectivity at one point is not, a priori, connected
with the reﬂectivity at other points along the path: for i ̸= j, the pairs (ri, si) and
(rj, sj) are independent.
9.4.2.3. Double Bernoulli-Gaussian (DBG) deconvolution
In order to exploit these prior hypotheses, [CHA 93] introduces the triplet (q, r, s)
composed of independent variables qi, ri, si where:

Deconvolution Applied to Ultrasonic Non-destructive Evaluation
231
– q is a Bernoulli process indicating the presence/absence of a discontinuity;
– conditionally on qi, the pair (ri, si) is Gaussian, zero-mean, and has a covariance
matrix rxqiI, with rx being the prior reﬂectivity variance.
Then, as in “simple” Bernoulli-Gaussian deconvolution (see Chapter 5), [CHA 93]
proposes a sequential estimation approach based ﬁrst on determining vector q from
the measurements, then jointly estimating (r, s) conditionally on q. The estimators
can be obtained by tweaking the algorithms described in Chapter 5. The deconvolution
method thus obtained is called “double” Bernoulli-Gaussian (DBG) deconvolution.
9.4.2.4. Double hyperbolic (DL2Hy) deconvolution
[GAU 97] has adapted the principle of “double” deconvolution to convex prior
models, in the following form:
(r, s) = arg min
r,s

∥y −Hr + Gs∥2 + λ

i
ρ(ri, si)

,
λ ≥0,
where λ is a regularization parameter and the bivariate function ρ allows the prior
information on the double reﬂectivity sequence to be included. The desire to obtain a
strictly convex, differentiable criterion naturally leads to a choice of a function ρ that
is also strictly convex and differentiable. Moreover, considering the prior information,
the monovariate restriction of this bivariate function must behave in a way very similar
to that seen in the simple deconvolution case. The transposition of the hyperbolic case
to the bivariate case leads us to look for a function ρ(u, v) that is quadratic for small
values of u and v and conic for large values of one or other of the variables. The
“bivariate hyperbolic” function ρ(u, v) =
√
T 2 + u2 + v2 has these properties (see
Figure 9.4). The parameter T allows the quadratic zone to be dilated or reduced,
which introduces more or less strong interdependence between u and v: the smaller
T is, the more u and v are linked (at the extreme when T tends to inﬁnity, u and v are
independent).
The associated estimator is ﬁnally given by:
(r, s) = arg min
r,s

∥y −Hr + Gs∥2 + λ

i
#
T 2 + r2
i + s2
i

.
As the criterion to be minimized is jointly convex in r and s, the solution can be
obtained by a descent algorithm. Considering the regularization function chosen, the
method associated with this estimator is called double hyperbolic (DL2Hy) deconvo-
lution.
9.4.2.5. Behavior of DL2Hy/DBG deconvolution methods
The behavior of these methods is mainly the result of the modeling of the prior
information. Thus, the discussion of the behavior of L2Hy/BG methods can be ex-
tended to DL2Hy/DBG cases. Since the DBG deconvolution includes a decision step,

232
Bayesian Approach to Inverse Problems
−T  0  T 
   
 T 
 0 
−T 
   
Figure 9.4. Graphic representation of a bivariate hyperbolic function
it gives more marked peaks than DL2Hy deconvolution. From another point of view,
the absence of a decision process in the DL2Hy case makes DL2Hy deconvolution
more robust with respect to variations in the data and the hyperparameters.
9.4.3. Blind DL2Hy/DBG deconvolution
On the basis of the elements above, a sequential approach to blind deconvolution
can be proposed using the two following steps [GAU 97]. First of all, the convolution
kernel is estimated by AR estimation, as in section 9.4.1. Then the input is estimated
by conventional DL2Hy or DBG deconvolution. In this case, the phase of the kernel
is chosen arbitrarily, but the DL2Hy/DBG deconvolution makes it possible to adapt to
phase rotations of the kernel and thus, in particular, to compensate for a phase shift in
the wave introduced into the deconvolution process.
From now on, this approach will be referred to simply as “blind DL2Hy/DBG
deconvolution”. Its application should not be restricted to cases where a phase rotation
is foreseen; the DL2Hy/DBG deconvolution no doubt gives the method a degree of
adaptability sufﬁcient to compensate for kernel deformations more general than just
phase rotations. Thus, using DL2Hy/DBG deconvolution allows us to take account of
any phase changes of the kernel and also to make up for imperfections of the kernel
estimation technique.
9.5. Processing real data
The performance of the deconvolution will be illustrated using the weld evaluation
example of section 9.2. In this data, the top and bottom of the notch appear in a

Deconvolution Applied to Ultrasonic Non-destructive Evaluation
233
common echo. The aim of the processing is to bring out the reﬂectors associated with
the extremities of the notch so as to characterize the presence of a planar defect and
estimate the height of the notch.
For this work, the wave emitted by the ultrasonic sensor was measured experimen-
tally, but such measurements are not always available. To carry out blind deconvolu-
tion of the raw B-scan of Figure 9.3a, a “minimum phase” wavelet of 50 samples was
estimated for each A-scan from eight AR coefﬁcients. Figure 9.5 shows the estimated
wavelet for A-scan number 40 and the measured wave. Comparing them reveals the
limits of the wave estimation technique: in particular, the energy of the wavelet is
concentrated in the ﬁrst instants (whence the term “minimum phase”), which is not
the case for the measured wave.
All the processing performed is one-dimensional: a deconvolved B-scan is simply
the result of the juxtaposition of the separately processed A-scans. First, the realistic
case of blind deconvolution will be tackled. Then, the reﬂectivities restored using
the measured wave for the deconvolution will be presented. Finally, elements for
comparing the DL2Hy and DBG approaches will be provided.
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
−2
−1.5
−1
−0.5
0
0.5
1
Time (μs)
Amplitude
(a)
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
−1.5
−1
−0.5
0
0.5
1
1.5
Time (μs)
Amplitude
(b)
Figure 9.5. (a) Estimated wavelet for A-scan # 40, using AR modeling,
q = 8; (b) measured wavelet
9.5.1. Processing by blind deconvolution
The result of the predictive deconvolution of Figure 9.6 illustrates the failure of
this approach, which does not bring out the two distinct reﬂectivities corresponding to
the top and bottom of the notch at all. In the rest of this section, the discussion will
concern the behavior of the L2Hy/BG and DL2Hy/DBG approaches implemented by
using an estimated wave for each A-scan.

234
Bayesian Approach to Inverse Problems
0
5
10
15
20
25
30
35
40
45
50
−20
−10
0
10
20
30
Time (μs)
Amplitude
Figure 9.6. Predictive deconvolution of the A-scan of Figure 9.3b for q = 8
The L2Hy deconvolution brings out peaks connected with the top and the bottom
of the notch, expressing the presence of this planar defect in the weld, Figures 9.7a
and b. The result of BG deconvolution is comparable (see Figure 9.7c). However, the
assistance provided by these methods is perturbed by the splitting of the reﬂectivities,
probably due to poor estimation of the phase of the wavelet.
The use of DL2Hy deconvolution allows us to place ourselves directly in the more
favorable framework described in section 9.4.3 and, particularly, to improve the pre-
vious results: the two reﬂectivities connected with the extremities of the notch ap-
pear even more clearly than in the L2Hy deconvolution case (compare Figures 9.8a
and 9.7a); the splitting of the reﬂectivity has disappeared (Figure 9.7b relative to Fig-
ure 9.8b). Finally, DBG deconvolution gives a comparable result here too (Figure 9.7c
relative to Figure 9.8c).
Blind deconvolution considerably improves the resolution of ultrasonic evaluation:
the reﬂectivities initially drowned in a common echo appear separately after deconvo-
lution. Exploiting the time difference between the reﬂectivities would enable the depth
of the notch to be estimated easily. The DL2Hy/DBG deconvolutions give better re-
sults than the L2Hy/BG methods and effectively compensate for the poor estimation
of the wavelet phase.
9.5.2. Deconvolution with a measured wave
Using the measured wave in L2Hy deconvolution brings an improvement over the
blind L2Hy version (see Figures 9.7a and 9.9a) but does not reach the performance
levels of blind DL2Hy deconvolution (Figures 9.8a and 9.9a). The measured wave

Deconvolution Applied to Ultrasonic Non-destructive Evaluation
235
A−Scan number
Time (μs)
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
top 
down
(a) L2Hy deconvolution of B-scan, (λ, T) = (100, 0.005)
0
5
10
15
20
25
30
35
40
45
50
−20
−10
0
10
20
Time (μs)
Amplitude
bottom
top
(b) L2Hy deconvolution of A-scan # 40, (λ, T) = (100, 0.005)
0
5
10
15
20
25
30
35
40
45
50
−20
−10
0
10
20
Time (μs)
Amplitude
(c) BG deconvolution of A-scan # 40, (λ, rx, rn) = (0.005, 50, 30)
Figure 9.7. Blind L2Hy/BG deconvolution of the data of Figure 9.3. The wavelet was
estimated beforehand, A-scan by A-scan, by AR estimation, q = 8

236
Bayesian Approach to Inverse Problems
A−scan number
Time (μs)
top
bottom
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
(a) blind DL2Hy deconvolution of B-scan, (λ, T) = (100, 0.005)
0
5
10
15
20
25
30
35
40
45
50
0
2
4
6
8
10
Time (μs)
Amplitude
bottom
top
(b) DL2Hy deconvolution of A-scan # 40, (λ, T) = (100, 0.005)
0
5
10
15
20
25
30
35
40
45
50
0
5
10
15
20
Time (μs)
Amplitude
(c) DBG deconvolution of A-scan # 40, (λ, rx, rn) = (0.005, 50, 30)
Figure 9.8. Blind DL2Hy/DBG deconvolution of the data of Figure 9.3. The wavelet was
estimated beforehand, A-scan by A-scan, by AR estimation, q = 8

Deconvolution Applied to Ultrasonic Non-destructive Evaluation
237
doubtless does not precisely correspond to the wave actually transmitted and more-
over, unlike L2Hy deconvolution, DL2Hy deconvolution allows deformations of the
wave as it propagates to be taken into account to some degree.
DL2Hy deconvolution gives a slightly “cleaner” deconvolved B-scan than when
the estimated wavelet is used (see Figures 9.8a and 9.9b). This better result can no
doubt be explained by the better ﬁdelity to reality of the spectral content of the mea-
sured wave.
Thus, blind DL2Hy/DBG deconvolution gives results which, although degraded,
are very close to those obtained using the measured wave. When we consider the
more realistic situation in which the measured wave is difﬁcult to obtain, the approach
proposed for blind DL2Hy/DBG deconvolution appears particularly efﬁcient.
9.5.3. Comparison between DL2Hy and DBG
As mentioned in the presentation of the results of blind deconvolution, the DL2Hy
and DBG deconvolutions produce similar results. Thus, the comparison made here
concerns the detailed behavior of the methods. The study is centered on restoring
the reﬂectivities that are the most interesting from an application point of view with
respect to those of the reﬂectors for the top and bottom of the notch. It was carried
out for the case where the measured wave was used in order to study, in particular, the
stability of the results for the various A-scans.
Close-ups around the interesting reﬂectors restored by the DL2Hy and DBG de-
convolutions are shown in Figures 9.10a and 9.10b. From these B-scans, the results
do indeed look very similar. However, the comparison can be made ﬁner by studying
the amplitudes of the two reﬂectors and the time difference between them.
Theoretically, the corner echo travels in a single direction while the diffraction
scatters in all directions. Therefore, except for the limit case when the bottom of the
notch is at the edge of the ultrasound beam, the amplitude of the reﬂector associ-
ated with the bottom of the notch should be higher than that of the second reﬂector.
This amplitude difference is indeed found in the case of DL2Hy deconvolution (see
Figure 9.11a). However, in DBG deconvolution, the reﬂector connected with the top
of the notch sometimes has an amplitude comparable to that restored for the bottom
(see Figure 9.11b). Furthermore, if we compare Figures 9.11a and b, it appears that
the variation of the amplitudes of the reﬂectors is more stable in the case of DL2Hy
deconvolution than for DBG.
The time difference between the two reﬂectors allows the height of the defect to
be estimated from the speed of propagation and the angle of incidence of the beam.
Theoretically, this difference should be stable from one sensor position to another. The

238
Bayesian Approach to Inverse Problems
A−Scan number
Time (μs)
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
(a) L2Hy deconvolution
A−Scan number
Time (μs)
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
(b) DL2Hy deconvolution
Figure 9.9. Comparison between L2Hy and DL2Hy deconvolution of B-scan of Figure 9.3a by
measured wavelet; (λ, T) = (100, 0.005) in both cases
processing results for the number of samples between the two reﬂectors is reported in
Figure 9.11c for each sensor position. Overall, the difference is smaller for DBG
than DL2Hy. The difference is not discriminating, however, as the uncertainty on the
height measurement depends on other parameters such as errors of estimation of the
velocity at which the wave propagates in the material. In contrast, the dispersion of

Deconvolution Applied to Ultrasonic Non-destructive Evaluation
239
A−Scan number
Time (μs)
15
20
25
30
35
40
45
50
55
60
15
20
25
30
35
40
(a) zoom on DL2Hy deconvolution, (λ, T) = (100, 0.005)
A−Scan number
Time (μs)
15
20
25
30
35
40
45
50
55
60
15
20
25
30
35
40
(b) zoom on DBG deconvolution, (λ, rx, rn) = (0.005, 50, 25)
Figure 9.10. Comparison between DL2Hy and DBG deconvolution of B-scan of Figure 9.3a
by measured wavelet (close-up of zone of interest)
the difference is greater in the DBG case: between ﬁve and eight samples, whereas
it is between seven and eight in the DL2Hy case. Here again, DL2Hy is more stable
than DBG.

240
Bayesian Approach to Inverse Problems
30
32
34
36
38
40
42
44
46
48
50
0
2
4
6
8
10
12
14
16
18
A−Scan number
Amplitude
(a) DL2Hy deconvolution,
(λ, T) = (100, 0.005)
30
32
34
36
38
40
42
44
46
48
50
10
15
20
25
30
35
A−Scan number
Amplitude
(b) DBG deconvolution,
(λ, rx, rn) = (0.005, 50, 25)
30
32
34
36
38
40
42
44
46
48
50
3
4
5
6
7
8
9
A−Scan number
Number of samples
(c) Notch sizing
Figure 9.11. Amplitude of reﬂectors connected with the top ( ) and bottom of the notch (---),
restored by deconvolution by measured wavelet: (a) DL2Hy method; (b) DBG method; (c)
number of samples separating the reﬂectors connected with the top and bottom of the notch for
DL2Hy ( ) and DBG (---) deconvolution
9.5.4. Summary
This processing of real data illustrates how NDE can gain from deconvolution. In
particular, blind DL2Hy/DBG deconvolution provides a great improvement in resolu-
tion and enables an accurate inter-reﬂector distance measurement. This approach im-
proves testing performance and opens up new perspectives. NDE experts sometimes
ﬁnd these results disconcerting as they give access to information that was masked in
the measurements. The application of these new techniques by non-destructive evalu-
ation specialists should therefore be preceded by the decisive step of an introduction
to deconvolution. The results provided by DL2Hy and DBG deconvolution are quali-
tatively similar. However, the stability study on the amplitude of the reﬂectivities and
the distance between reﬂectors shows that DL2Hy deconvolution is the more robust
of the two. The interest of robustness does not appear clearly in the case of a notch
but can prove decisive in the case of a crack.
9.6. Conclusion
Its lack of time resolution is a limitation of ultrasonic inspection: two reﬂectors
that are close together are merged into a single echo. It is possible to model measure-
ment acquisition as the result of one-dimensional convolution between a sequence of
reﬂectors representative of the object under inspection and a kernel associated with the
incident wave. In this framework, improving the resolution of ultrasonic evaluation
has been treated here as a blind pulse train deconvolution problem.
We have given an overview of blind deconvolution techniques going beyond the
NDE framework, then presented an original approach for blind pulse train

Deconvolution Applied to Ultrasonic Non-destructive Evaluation
241
deconvolution based on imperfect estimation of the kernel followed by DL2Hy/DBG
pulse deconvolution. The second step compensates for both the imperfections in the
wave estimation technique and possible phase deformations of the kernel during prop-
agation.
Applied to ultrasonic NDE, this approach pushes back the limits of inspection by
providing a marked improvement in the time-resolution and appreciable help for the
interpretation of measurements. These new results are surprising for NDE experts and
must be accompanied by an effort at explanation and training from signal process-
ing specialists. From this point of view, presenting deconvolution as a problem of
minimizing a penalized criterion has the advantage of simplicity.
The approach put forward for blind deconvolution could be applied to other types
of imaging by propagation of mechanical waves: biomedical ultrasound scans, seismic
reﬂection in geophysics, etc. Concerning the use of DL2Hy/DBG deconvolutions
only, they would be suitable for numerous pulse deconvolution problems, particularly
when the kernel is poorly known or tends to deform with time.
From a methodological point of view, DL2Hy/DBG deconvolution can be adapted
to the case of a two-dimensional kernel. Applied in ultrasonic NDE, it has been suc-
cessfully achieved by Labat et al. [LAB 05] in the aim of improving the resolution
in the lateral direction. In parallel or independently, the introduction of a prior two-
dimensional model for the reﬂectivity would enable the lateral continuity of the reﬂec-
tors to be taken into account [IDI 93]. Finally, a way of taking correlated structures in
the noise into consideration could be easily introduced by modifying the data ﬁtting
term and would be advantageous for dealing with measurements containing strong
structural noise.
9.7. Bibliography
[ATA 82] ATAL B. S., REMDE J. R., “A new method of LPC excitation for producing natural
sounding speech at low bit rates”, in Proc. IEEE ICASSP, vol. 1, Paris, France, p. 614-617,
May 1982.
[CHA 93] CHAMPAGNAT F., IDIER J., DEMOMENT G., “Deconvolution of sparse spike trains
accounting for wavelet phase shifts and colored noise”, in Proc. IEEE ICASSP, Minneapo-
lis, MN, p. 452-455, 1993.
[CHE 96] CHENG Q., CHEN R., LI T.-H., “Simultaneous wavelet estimation and deconvolu-
tion of reﬂection seismic signals”, IEEE Trans. Geosci. Remote Sensing, vol. 34, p. 377-
384, Mar. 1996.
[COO 90] COOKEY M., TRUSSELL H. J., WON I. J., “Seismic deconvolution by multipulse
coding”, IEEE Trans. Acoust. Speech, Signal Processing, vol. 38, num. 1, p. 156-160, Jan.
1990.

242
Bayesian Approach to Inverse Problems
[GAU 96] GAUTIER S., Fusion de données gammagraphiques et ultrasonores. Application au
contrôle non destructif, PhD thesis, University of Paris XI, France, Dec. 1996.
[GAU 97] GAUTIER S., IDIER J., MOHAMMAD-DJAFARI A., LAVAYSSIÈRE B., “Traite-
ment d’échogrammes ultrasonores par déconvolution aveugle”, in Actes 16e coll. GRETSI,
Grenoble, France, p. 1431-1434, Sep. 1997.
[GOU 89] GOUSSARD Y., DEMOMENT G., “Recursive deconvolution of Bernoulli-Gaussian
processes using a MA representation”, IEEE Trans. Geosci. Remote Sensing, vol. GE-27,
p. 384-394, 1989.
[IDI 93] IDIER J., GOUSSARD Y., “Multichannel seismic deconvolution”,
IEEE Trans.
Geosci. Remote Sensing, vol. 31, num. 5, p. 961-979, Sep. 1993.
[LAB 05] LABAT C., IDIER J., RICHARD B., CHATELLIER L., “Ultrasonic nondestructive
testing based on 2D deconvolution”, in PSIP’2005 : Physics in Signal and Image Process-
ing, Toulouse, France, Jan. 2005.
[LAZ 93] LAZEAR G. D., “Mixed-phase wavelet estimation using fourth-order cumumants”,
Geophysics, vol. 58, p. 1042-1049, 1993.
[ORF 85] ORFANIDIS S. J., Optimal Signal Processing – An Introduction, Macmillan, New
York, NY, 1985.
[ROB 67] ROBINSON E. A., “Predictive decomposition of time series with application to seis-
mic exploration”, Geophysics, vol. 32, p. 418-484, 1967.
[SAI 85] SAINT-FELIX D., HERMENT A., DU X.-C., “Fast deconvolution: application to
acoustical imaging”, in J.M. THIJSSEN, V. MASSEO (Eds.), Ultrasonic Tissue Characteri-
zation and Echographic Imaging, Nijmegen, The Netherlands, Faculty of Medicine Printing
Ofﬁce, p. 161-172, 1985.
[VIV 89] VIVET L., Amélioration de la résolution des méthodes d’échographie ultrasonore en
contrôle non destructif par déconvolution adaptative, PhD thesis, University of Paris XI,
France, Sep. 1989.
[WIG 78] WIGGINS R. A., “Minimum entropy deconvolution”,
Geoexploration, vol. 16,
p. 21-35, 1978.

Chapter 10
Inversion in Optical Imaging through
Atmospheric Turbulence
10.1. Optical imaging through turbulence
10.1.1. Introduction
The theoretical resolving power of a telescope is limited by its diameter. In real in-
struments, this theoretical limit, called the diffraction-limit resolution, often cannot be
reached because of the presence of optical aberrations. These aberrations may come
from the telescope itself or from the light wave propagation medium. In the case of
ground-based astronomy, aberrations are mostly due to atmospheric turbulence. Sev-
eral techniques have been developed to improve the resolution of observation instru-
ments and avoid the degradation caused by turbulence. In this section, we recall some
essential ideas in optical imaging, in particular on the optical effects of turbulence,
then review the various techniques of high-resolution imaging through turbulence.
Section 10.2 gives a brief presentation of the inversion approach and the regular-
ization criteria used in this chapter. Section 10.3 is an introduction to wavefront sen-
sors (WFSs) and the processing problems that arise from their use. WFSs are devices
Chapter written by Laurent MUGNIER, Guy LE BESNERAIS and Serge MEIMON.
Laurent Mugnier and Serge Meimon are grateful to their colleagues of the High Angular Res-
olution team and particularly to its successive leaders Marc Séchaud and Vincent Michau, who
created and maintained a stimulating team spirit made up of intellectual curiosity and a desire
to share knowledge. With special thanks from Laurent Mugnier for the discussions with Jean-
Marc Conan that introduced him to this ﬁeld and from Serge Meimon for his talks with Frédéric
Cassaing.

244
Bayesian Approach to Inverse Problems
that measure optical aberrations and are essential components of many high resolution
optical imaging instruments today.
Three imaging techniques are illustrated by the inverse problems associated with
them. These inverse problems are: image restoration for deconvolution from wave-
front sensing and for imaging using adaptive optics, discussed in section 10.4, and
image reconstruction for optical interferometry (section 10.5).
10.1.2. Image formation
10.1.2.1. Diffraction
Image formation is well described by the scalar theory of diffraction, presented
in detail in reference works such as [GOO 68, BOR 93]. A modern introductory
overview can be found in [MAR 89]. Image formation can be modeled by a con-
volution, at least within the instrument’s so-called isoplanatic patch. At visible wave-
lengths, this patch is typically of the order of a degree when only aberrations due to
the telescope itself are considered and a few arcseconds (1 arcsec = 1/3 600◦) for a
telescope observing space through turbulence.
The instantaneous point-spread function (PSF) of a telescope or “telescope + at-
mosphere” system is equal to the square modulus of the Fourier transform (FT) of
the complex amplitude of the ﬁeld ψ = P exp (jϕ) present in the aperture of the
instrument when the object observed is a point source:
h(ξ) =
FT−1 
P(λu) ejϕ(λu)
2
(ξ)
(10.1)
where λ is the imaging wavelength and imaging is assumed quasi-monochromatic.
This PSF is conventionally normalized to a unit integral. In expression (10.1), the FT
is the ﬁeld transformation performed by the telescope between the pupil plane and
the focal plane, and the square modulus is due to the detection being quadratic i.e., in
intensity. Vector ξ = [ξ, ζ]T is composed of angles in the sky, in radians. For a perfect
telescope in the absence of turbulence, P is constant in the aperture and ϕ is zero1.
For a real telescope, the variations of the ﬁeld P exp (jϕ) are due both to aberrations
belonging to the telescope and to those introduced by the turbulence.
In what follows, we assume that P is simply the aperture indicatrix, i.e., that the
variations of intensity in the input pupil are negligible. This hypothesis is generally
valid in astronomical imaging and is called the near-ﬁeld approximation.
1. The corresponding PSF is called the Airy pattern.

Inversion in Optical Imaging through Atmospheric Turbulence
245
Equation (10.1) indicates that the optical transfer function, or OTF, is the autocor-
relation of ψ = P ejϕ dilated by the inverse of the wavelength, which is written
⌢
h(u) = P ejϕ ⊗P ejϕ(λu).
(10.2)
In the absence of aberrations, i.e., when the phase ϕ is zero, the OTF is the autocorre-
lation of the aperture P. It has a spatial cut-off frequency equal to D/λ rad−1, where
D is the diameter of the aperture, and is strictly zero beyond it. The ultimate resolution
of a telescope (sometimes called a monolithic telescope in contrast to the interferom-
eters described below) is thus limited by its diameter D. Today’s technology limits
diameters to ten meters or so for ground-based telescopes and a few meters for tele-
scopes on board satellites because of size and mass constraints. Optical interferometry
(OI) is a technique that allows us to go beyond the resulting resolution limitation.
10.1.2.2. Principle of optical interferometry
This technique consists of taking the electromagnetic ﬁelds received at each of the
them interfere.
information at (or around) the angular spatial frequency Bk,ℓ/λ, where Bk,ℓis the
vector separating the apertures, or baseline. This spatial frequency can be much larger
than the cut-off frequency D/λ of the individual apertures.
Depending on the type of interferometer and beam combination, it is possible ei-
ther to form and measure an image of the object directly (the interferometer is then
called an imaging interferometer) or to measure a discrete set of spatial frequencies
of the object of interest (the interferometer can then be called a “correlation inter-
ferometer” as it measures the correlation of the electromagnetic ﬁelds between aper-
tures [CAS 97]). The reader interested in a more precise description of the different
types of optical interferometers is invited to consult [ROU 01].
For a monolithic telescope, as for an interferometer, the transfer function is the
autocorrelation of the input pupil (see equation (10.2)) provided that, if the interfer-
ometer is of the correlation type, the apertures are assimilated to points. For a long-
baseline interferometer, i.e., when the baselines are large relative to the diameter of
the individual apertures – which is generally the case for correlation interferometers –
the difference between imaging and correlation interferometers becomes negligible as
far as the information recorded in the data is concerned. The transfer functions of a
monolithic telescope, an imaging interferometer and a correlation interferometer are
illustrated in Figure 10.1. For an imaging interferometer, the processing required is, to
a good approximation, a deconvolution, with a PSF still given by equation (10.1) but
more irregular than with a monolithic telescope because of the shape of the aperture.
For a correlation interferometer, the nature of the data processing problem chan-
ges: here, the aim is to reconstruct an object from Fourier coefﬁcients, a problem
called Fourier synthesis. This is the problem that will be tackled in section 10.5.
apertures of an array of apertures (basic telescopes or mirror segments) and making
For each pair (k, ℓ) of apertures, the data contains high-resolution

246
Bayesian Approach to Inverse Problems
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Figure 10.1. Cross-sections of transfer functions of a monolithic telescope (left), a
three-telescope imaging interferometer (center) and a two-telescope correlation
interferometer (right)
An intuitive way of representing data formation in a long-baseline interferome-
ter is Young’s double hole experiment, in which the aperture of each telescope is a
(small) hole letting through the light coming from an object located at a great dis-
tance. Each pair (k, ℓ) of telescopes then gives a fringe pattern with a spatial fre-
quency of Bk,ℓ/λ, where Bk,ℓis the vector linking telescopes k and ℓ. The con-
trasts and positions of these fringes can be measured and grouped together naturally
in a number called the “complex visibility”, which, in an ideal situation and in the
absence of turbulence, gives the value of
⌢x(Bk,ℓ/λ)/
⌢x(0) (Van Cittert-Zernike theo-
rem [GOO 85, MAR 89]).
10.1.3. Effect of turbulence on image formation
10.1.3.1. Turbulence and phase
The inhomogeneities in the air temperature of the atmosphere generate inhomo-
geneities in the refractive index of the air, which perturb the propagation of light waves
through the atmosphere. These perturbations lead to space and time variations of the
pupil phase ϕ, which can be modeled by a random process. In this section we recall
a few results that enable the turbulent aperture phase to be modeled up to the second
order. We will use the assumption, which is generally well veriﬁed, at least for scales
of less than about ten meters, that the random variations of the refractive index of the
air obey Kolmogorov’s law: they follow a Gaussian probability law with zero mean
and power spectral density (PSD) proportional to |ν|−11/3, where ν is the 3D spatial
frequency [ROD 81].
By integrating the phase along the optical path and in the framework of the near-
ﬁeld approximation, the spatial statistics of the phase in the telescope aperture can
be deduced for a plane wave entering the atmosphere. The phase in the aperture is
Gaussian since it is the result of the sum of all the index perturbations from the upper

Inversion in Optical Imaging through Atmospheric Turbulence
247
atmosphere down to the ground [ROD 81]. The PSD of this phase is [NOL 76]:
Sϕ(u) = 0.023 r−5/3
0
u−11/3
(10.3)
where u is the 2D spatial frequency in the aperture, u is its modulus, and r0 is the key
parameter quantifying the strength of the turbulence, called Fried’s diameter [FRI 65].
The smaller r0, the stronger the turbulence. Typically, its value is about 10 cm in the
visible range at a relatively good site.
The typical variation time τ of the turbulent phase in the aperture is given by the
ratio of characteristic scale r0 of this phase to mean wind speed Δv (which, more
accurately, is a standard deviation of the distribution of the moduli of wind veloci-
ties [ROD 82]):
τ = r0/Δv.
(10.4)
For r0 ≃10 cm and Δv ≃10 m.s−1, we obtain τ ≃10−2 s. Thus, we talk about long
exposures for images corresponding to an integration markedly longer than this time
and short exposures for images with shorter integration times. For a full treatment of
the time statistics of the turbulent phase, see [CON 95].
10.1.3.2. Long-exposure imaging
The turbulent long-exposure OTF is the product of the so-called static OTF,
⌢
hs, of
the telescope without the atmosphere and an atmospheric transfer function,
⌢
ha, which
has a cut-off frequency r0/λ [ROD 81]:
⌢
h(u)
Δ= ⟨
⌢
ht(u)⟩=
⌢
hs(u)
⌢
ha(u) with
⌢
ha(u) = exp{−3.44 (λu/r0)5/3},
(10.5)
where the angle brackets ⟨·⟩denote a temporal mean over an arbitrarily long time.
Thus we see that, for a telescope with a large diameter D ≫r0, the long-exposure
imaging resolution is limited by the turbulence and is no better than for a telescope of
diameter r0.
10.1.3.3. Short-exposure imaging
As noted by Labeyrie [LAB 70], when the exposure time is short enough to freeze
the turbulence (typically less than 10 ms, see equation (10.4)), the images retain the
high-frequency information in the form of speckles, having a typical size λ/D and
random positions. This is illustrated in Figure 10.2, which shows the simulated image
of a star viewed through turbulence (D/r0 = 10) using short (left) and long (right)
exposures.
It is possible to quantify the high-frequency information present in short-exposure
images by evaluating the speckle transfer function (STF), deﬁned as the second order

248
Bayesian Approach to Inverse Problems
Figure 10.2. Images of a star simulated without atmospheric turbulence (left) and through
turbulence (short exposure in center and long exposure on right). The strength of the
turbulence is D/r0 = 10. Image sampling respects Shannon’s condition
moment of the instantaneous transfer function, ⟨|
⌢
ht(u)|2⟩. For a large-diameter tele-
scope (D ≫r0), if we take an approximation on the turbulence statistics, we can ﬁnd
an approximate expression for the STF [ROD 81]:
⟨|
⌢
ht(u)|2⟩≃⟨
⌢
ht(u)⟩2 + 0.435 (r0/D)2 ⌢
hs
0(u)
(10.6)
where
⌢
hs
0 is the transfer function of a perfect telescope (i.e., with no aberration) of
diameter D.
This expression allows us to describe the STF as the sum of the square of the long-
exposure transfer function, which is low-frequency (LF), and a high-frequency (HF)
component that extends up to the cut-off frequency of the telescope with an atten-
uation proportional to (D/r0)2. Thus, if we process a set of short-exposure images
using a more judicious method than a simple average, it is possible to recover a high-
resolution image of the observed object.
10.1.3.4. Case of a long-baseline interferometer
Equation (10.5) applies whatever the shape of the instrument’s aperture and thus,
in particular, applies to an interferometer. In a long exposure, the contrast of the
fringes measured for a baseline Bk,ℓ/λ is therefore multiplied by
⌢
ha(Bk,ℓ/λ) and so
strongly attenuated as to make the measurement of
⌢x(Bk,ℓ/λ) unusable.
In a short exposure, for an interferometer where each aperture has a diameter
smaller than the Fried’s diameter, r0, or where turbulence is corrected using adaptive
optics (see section 10.1.4.3), the impact of turbulence on the interferometer measure-
ments is easy to model: in the Young’s holes analogy mentioned above, each hole k
adds a phase shift (or piston) ϕk(t) to the wave going through it, because of aberra-
tions introduced by the turbulence in front of this aperture. The interference between

Inversion in Optical Imaging through Atmospheric Turbulence
249
two apertures k and ℓare thus out of phase by the “differential piston” ϕℓ(t) −ϕk(t),
which, in a short exposure, is expressed by a random displacement of the fringes
without attenuation of the contrast. The contrast attenuation in long exposures results
from the averaging of these random displacements. Section 10.5 will present aver-
aging techniques that get around the differential pistons. The short-exposure transfer
function, at frequency Bk,ℓ/λ, can be written:
⌢
ht(Bk,ℓ/λ) = ηk,ℓ(t) ej(ϕℓ(t)−ϕk(t))
(10.7)
where ηk,ℓ(t) is a number that is often called the “instrumental visibility”. In the
absence of the many potential sources of visibility loss (residual perturbations of the
wavefront at each telescope, differential tilts between telescopes, differential polariza-
tion effects, non-zero spectral width, etc.), the value of ηk,ℓ(t) is the inverse of the
number of apertures interfering simultaneously (equation (10.2) considering that P is
a sum of Dirac delta functions). In practice, this instrumental visibility is calibrated
on a star known to be unresolved by the interferometer. Taking this calibration into
account, we can thus replace ηk,ℓ(t) by 1 in equation (10.7).
Note that the measurement baseline Bk,ℓbetween apertures k and ℓdepends on
time: the aperture conﬁguration as seen from the object changes as the Earth rotates.
This is used in “super-synthesis”, a technique that consists, when the source emis-
sion does not vary in time, of repeating the measurements in the course of a night of
observation to increase the frequency coverage of the interferometer.
10.1.4. Imaging techniques
The aim of high-resolution imaging through turbulence is to restore the HFs be-
yond the cut-off frequency r0/λ of the long-exposure imaging. This is made possible
by various experimental techniques that avoid the time-integration of phase defects
introduced by the turbulence. A measure of the quality of the technique is thus the
resulting signal-to-noise ratio (SNR) at high spatial frequencies.
10.1.4.1. Speckle techniques
The ﬁrst high-resolution techniques were based on the acquisition of a series of
short-exposure images and the calculation of empirical moments. Speckle interferom-
etry2 [LAB 70] uses the quadratic mean of the FTs of the images, which allows the
autocorrelation of the observed object to be estimated. Knox and Thomson [KNO 74],
then Weigelt [WEI 77] put forward processing methods using the cross-spectrum and
2. The term interferometry could mislead the reader into thinking that the instrument used here
is an interferometer. This is by no means the case; the interferences in question arise from the
aperture of a monolithic telescope.

250
Bayesian Approach to Inverse Problems
the bi-spectrum respectively of the short-exposure images so as to estimate the object
and not only its autocorrelation. These methods require the averages to be taken over
a large number of images, even for simple objects, both to make the estimation of the
statistical quantities valid and to improve the SNR.
10.1.4.2. Deconvolution from wavefront sensing (DWFS)
A notable enhancement of short-exposure imaging through turbulence was thus
brought about, not by improving the processing of measurements but by changing the
experimental technique itself. In 1985, Fontanella [FON 85] proposed a new imaging
technique: deconvolution from wavefront sensing. This technique, based on the use
of a device called a wavefront sensor (WFS), was experimentally validated shortly
afterwards [PRI 88, PRI 90].
The aim of WFSs, which had so far only been used for controlling the surface qual-
ity of telescope mirrors, is to measure the aberrations of optical systems (the phase ϕ
of equation (10.1)). Some of them, such as the Hartmann-Shack sensor used in de-
convolution from wavefront sensing, work even if the object of interest is extended
(rather than being a point source).
The technique of deconvolution from wavefront sensing consists of simultaneously
recording a series of short-exposure images and Hartmann-Shack wavefront measure-
ments. In practice, at least ten or so short-exposure images are typically needed to
give correct spatial frequency coverage up to the telescope cut-off (equation (10.6)).
The number of images required is greater if the observed object is not very bright.
Deconvolution from wavefront sensing is a considerable improvement on the other
short-exposure techniques mentioned above. First of all, like the Knox-Thomson or
bi-spectral techniques, it enables us to recover not the autocorrelation of the object
but the object itself. Then, unlike the previous short-exposure techniques, this one
does not need images of a reference star to be recorded, and it is called self-referenced
speckle interferometry for this reason. Finally, its measurements are efﬁcient in terms
of photons collected: as the short-exposure images must be quasi-monochromatic to
keep the speckles unblurred, all the remaining photons can be diverted towards the
WFS without any loss of signal on the image channel. This technique thus makes
it possible to record more information than the previous short-exposure techniques
and, unlike those techniques, has a SNR that is not limited by the speckle noise at
high ﬂux [ROD 88b], because of its self-referenced nature. This explains why speckle
interferometry has fallen into disuse nowadays.
Section 10.4.2 gives more details on the data processing in this technique, which is
a double inverse problem (estimation of wavefronts from WFS measurements, which
allows the instantaneous PSF corresponding to each image to be calculated, and esti-
mation of the object from images and WFS measurements).

Inversion in Optical Imaging through Atmospheric Turbulence
251
10.1.4.3. Adaptive optics
The imaging technique with the best performance in terms of SNR is adaptive
optics (AO), which provides a real-time compensation for the aberrations introduced
by atmospheric turbulence, generally by use of a mirror whose surface is deformed at
all instants via a servo-loop, according to the measurements made by a WFS.
This technique thus enables long-exposure images (typically exposed for between
a few seconds and several tens of minutes) to be recorded while retaining the HFs
of the observed object up to the cut-off frequency of the telescope. The HFs are
nevertheless attenuated as the correction is only partial [CON 94] and deconvolution
is necessary. This deconvolution, for which the PSF is often imperfectly known, is
presented in section 10.4.3.
The most commonly used WFS is the Hartmann-Shack sensor (see section
10.3.2). The associated deformable mirror has actuators made of, e.g., stacked piezo-
electric material controlled by high voltages. The AO technique was proposed by
Babcock as early as 1953, and developed from the 1970s for defence purposes, ﬁrst
in the USA, then in France, but it was not until the late 1980s that the ﬁrst AO system
for astronomy came into being [ROU 90]. Any reader interested in a detailed account
of AO should consult a reference work such as [ROD 99].
10.1.4.4. Optical interferometry
This section describes some of the major steps in the development of ground-based
stellar interferometry and takes its inspiration partly from [MON 03].
10.1.4.4.1. The ﬁrst measurements of stars
The use of interferometry for observing stars was ﬁrst suggested by Fizeau in 1868,
the aim being simply to measure the size of celestial bodies. However, it was not until
who measured the diameters of Jupiter’s moons by masking a telescope with two ﬁne
slits four inches apart. In 1920-21, he and Pease measured the diameter of the star
Pease’s unsuccessful attempts to reach a baseline of 50 feet marked the start of
a difﬁcult period for optical interferometry. At the same time, the advances made
in radar during the Second World War led to the development of interferometry at
radio wavelengths. Resolutions smaller than a milliarcsecond were reached in radio
interferometry while its optical counterpart rather fell into neglect because of the many
technical difﬁculties involved in coherently combining the beams from two telescopes.
In optics, it is impossible to record the phase, and the beams therefore have to be
combined in real time. Another handicap for optics is that the effects of turbulence
evolve much faster than for radio.
Betelgeuse using a 20 foot (6 meter) interferometer [MIC 21].
1890 that the technique was implemented experimentally by Michelson [MIC 91],

252
Bayesian Approach to Inverse Problems
10.1.4.4.2. Renewed interest in optical interferometry
The ﬁrst coherent combination of optical beams emitted by a star using a long
baseline interferometer was achieved by Labeyrie in 1974 [LAB 75], with a base-
line of 12 meters on a 2-telescope interferometer called I2T. This was followed by a
more ambitious version composed of telescopes 1.5 meter in diameter with a base-
line of up to 65 meters, which was named the Grand Interféromètre à 2 Téléscopes
(GI2T) [MOU 94]. Up to that time, interferometry had been used with two aper-
tures to measure the spatial spectrum of an astronomical scene from which a few
parameters were extracted to validate or reject an astrophysical model. In particu-
lar, only the modulus of the spatial spectrum could be used. In addition, without the
phase, it is generally impossible to determine the geometry of the observed scene. In
1987, by masking a monolithic telescope, Hannif et al. showed that it was possible to
obtain interferometric arrays [HAN 87], i.e., to form interference fringes simultane-
ously for each pair of telescopes of the array. This technique, in addition to providing
several measurements at once (15 frequencies per exposure for a 6-telescope inter-
ferometer), gave access to the phase of the spatial spectrum of the object [BAL 86],
thus making interferometric imaging possible for scenes more complex than a uni-
form disk or a binary system. The remarkable potential of this method encouraged
several teams to build such instruments. In 1995, the COAST interferometer made
the ﬁrst simultaneous combination with three telescopes [BAL 96], and was followed
a few months later by NPOI [BEN 97] then IOTA [IOT] (now decommissioned).
Since these instruments are evolving quickly, the interested reader is advised to visit
http://olbin.jpl.nasa.gov/ for up-to-date information.
10.1.4.4.3. Future of interferometry
The technology needed to build optical interferometric arrays has now come of age
and draws upon the sister ﬁelds of integrated optics, adaptive optics and ﬁber optics:
– integrated optics has been successfully used in multi-telescope simultaneous
combination for several years, in particular on the IOTA interferometer [BER 03] (the
experimental data processed at the end of this chapter were obtained with this system);
– adaptive optics on large telescopes such as those of the Very Large Telescope
Interferometer makes it possible to observe objects of low luminosity;
– ﬁber optics provides monomode ﬁbers that will allow telescopes to be connected
interferometrically over very large distances. The OHANA project plans to combine
the seven largest telescopes on the summit of Mauna Kea in Hawaii to form an inter-
ferometer. The array thus formed will have a maximum baseline of 800 m [PER 06].
In parallel with the development of these large correlation interferometers, the
technology for making imaging interferometers is now available. These instruments
should eventually lead to considerable savings in volume and mass relative to an
equivalent monolithic telescope, which would make them ideal candidates for space

Inversion in Optical Imaging through Atmospheric Turbulence
253
missions.
On the ground, they would be an alternative to giant monolithic tele-
scopes (of several tens of meters). The ﬁrst of them is the Large Binocular Telescope
LBT [HIL 04], which will combine two eight-meter telescopes corrected by adaptive
optics.
Segmented telescopes, such as those of the Keck Observatory [KEC], have been
in use for several years and are at the boundary between imaging interferometers and
telescopes. Their primary mirrors are composed of joined petals that are easier to
manufacture than a monolithic mirror and this technique has been chosen for the future
European giant telescope E-ELT (for European Extremely Large Telescope) and the
US TMT (for Thirty Meter Telescope).
In addition to the correlation and imaging interferometers described in this section,
there are other sorts of interferometers. P. Lawson [LAW 97] has collected together
a selection of reference publications in this ﬁeld as a whole. In this chapter, we shall
only deal with the problem of processing the data collected by means of a correlation
interferometer observing space from the ground through turbulence.
10.2. Inversion approach and regularization criteria used
Inversion in optical imaging through turbulence is generally an ill-posed problem
in the case of a monolithic telescope and an under-determined problem in the case of
an interferometer.
In deconvolution from wavefront sensing and in adaptive optics, we need to solve
an image restoration problem for which the Bayesian approach already described in
this book can be used directly. In the case of so-called conventional deconvolution,
where the PSF is taken to be perfectly known, the estimated object is deﬁned as the
minimizer of a compound criterion containing a data ﬁdelity term Jy and a prior ﬁ-
delity term Jx. In OI the image is to be reconstructed from heterogeneous data and
with a knowledge of the transfer function which is very incomplete because of the
turbulence. There are several possible approaches for handling this type of data; the
details are given below.
In all cases it is necessary to regularize the inversion to reach acceptable solutions.
This will be done here by using a regularization term Jx in the minimized criterion
to obtain the solution. The regularization criteria used in this chapter are taken from
those described below and are all convex.
Quadratic criteria are the most widely used. We will use a criterion of this type
in DWFS and OI with a parametric model of the object spectrum such as the one
proposed for adaptive optics in [CON 98b]. An advantage of these criteria is that it is
possible to estimate the parameters of the model easily, by maximum likelihood for

254
Bayesian Approach to Inverse Problems
example. See [BLA 03] for the identiﬁcation of the spectrum model of [CON 98b]
with simultaneous estimation of the aberrations and [GRA 06] for an application to
adaptive optics with known PSF. This model can also be identiﬁed from the data in
OI [MEI 05a].
For objects with sharp edges such as artiﬁcial satellites, asteroids or planets, a
quadratic criterion tends to oversmooth the edges and introduce spurious oscillations,
or ringing, in their neighborhood. A solution is thus to use an edge-preserving cri-
terion such as the so-called quadratic-linear, or L2L1, criteria, which are quadratic
for weak gradients of the object and linear for the stronger ones. The quadratic part
ensures good noise smoothing and the linear part cancels out edge penalization (see
Chapter 6). Here, for DWFS (section 10.4.2) and AO (section 10.4.3), we will use an
isotropic version [MUG 01] of the criterion proposed by Rey [REY 83] in the context
of robust estimation and used by Brette and Idier in image restoration [BRE 96]:
Jx(x) = μδ2 
r

Λx(ℓ, m)/δ −log (1 + Λx(ℓ, m)/δ)

(10.8)
where Λx(ℓ, m) =

∇ξ x(ℓ, m)2 + ∇ζ x(ℓ, m)2, with ∇ξ x and ∇ζ x as the gradient
approximations by ﬁnite differences in the two spatial directions.
For objects composed of bright points on a fairly smooth background, such as are
often found in astronomy, we can consider an L2L1 prior that is white, i.e., where
pixels are independent. Such a prior is obtained by using the regularization of equa-
tion (10.8) but with Λx = x. This is what we will do for all the interferometric data of
section 10.5.4. Unlike for the case of quadratic regularization with an object spectrum
model, the tuning of the hyperparameters has to be supervised here.
10.3. Measurement of aberrations
10.3.1. Introduction
The WFS is a key element of modern high-resolution imaging instruments as it
allows the instrument aberrations and the atmospheric turbulence to be measured so
that they can be compensated for, either in real time (AO) or by post-processing.
Many WFSs are currently available, and a thorough review is given in [ROU 99].
They can be divided into two families: focal-plane sensors and pupil-plane sensors.
Present-day AO systems use either a Hartmann-Shack sensor [SHA 71], which is
well described in [FON 85], or a curvature sensor [ROD 88a]. Both belong to the
pupil-plane family and use a fraction of the incident light, which is diverted by a
dichroic beam-splitter. For AO, both have the appealing properties that they work
with a broad spectral band (because they can be well described by geometrical optics)

Inversion in Optical Imaging through Atmospheric Turbulence
255
and that the relationship between the unknown aberrations and the data is linear, so
the inversion can be performed in real time. The next subsection presents the principle
of the Hartmann-Shack sensor. This sensor will be seen later in the DWFS technique
and is the most widely used in AO.
The focal-plane family of sensors was born from the very natural idea that an
image of a given object contains information not only about the object, but also about
the wavefront. A focal-plane sensor thus requires little or no optics other than the
imaging sensor; it is also the only way to be sensitive to all aberrations down to the
focal plane.
Section 10.3.3 brieﬂy presents the focal-plane wavefront sensing technique known
as phase diversity [GON 82]. This technique is simple in its hardware requirements
and, like the Hartmann-Shack, works on very extended objects. Finally, it should
be noted that there are special WFSs called co-phasing sensors that can measure the
differential piston between apertures, which are aberrations speciﬁc to interferometers.
Phase diversity can be used both as a WFS and as a co-phasing sensor. Differential
pistons are not yet corrected on the interferometers in operation at present.
10.3.2. Hartmann-Shack sensor
The principle of this sensor is illustrated in Figure 10.3: an array of Nml ×Nml
micro-lenses is placed in a pupil plane (image of the telescope entrance pupil). It
samples or, in other words, cuts up the incident wavefront. At the focus of the array,
a set of detectors (CCD camera, for example) records the N 2
ml sub-images, each of
which is the image of the object observed through the part of the pupil cut out by
the corresponding micro-lens. When the wavefront is perturbed by aberrations, each
micro-lens sees approximately a tilted plane wavefront and the corresponding sub-
image is thus shifted relative to its reference position by an amount proportional to the
mean slope of the wavefront. In the case of aberrations due to atmospheric turbulence,
Nml should be chosen so that the size of each micro-lens relative to the entrance pupil
of the instrument is of the order of the Fried’s diameter r0. The position of the center
of gravity of each sub-image is measured, thus giving a map of the mean slopes of the
wavefront on a Nml × Nml grid3.
3. It is possible to envisage taking the measurements to be not this map of local slopes but
directly the set of raw sub-images. In practice, these sub-images generate a large data ﬂow and
are therefore not generally stored on a disk: in imaging through turbulence, the wavefront has
to be sampled at several tens, or even hundreds, of Hertz.

256
Bayesian Approach to Inverse Problems
α
αf
f
Sensor
Sub-images
Microlens array
Incident wavefront
Figure 10.3. Principle of the Hartmann-Shack sensor
The unknown phase at instant t, denoted ϕt, is generally expanded into Zernike
polynomials [NOL 76] and the coefﬁcients of this expansion are denoted φq
t :
ϕt(r) =

q
φq
tZq(r)
(10.9)
where r is the current point in the pupil. The direct problem can then be put in the
form:
st = Dφt + b′
t
where st is the vector concatenating the 2N 2
ml slope measurements (x and y), φt is
the vector of the coordinates of the unknown phase and D is essentially a sampled
derivation operator called the “interaction matrix”.
The noise is generally assumed to be iid Gaussian. The independence among
the measurements of the various sub-pupils is natural and the Gaussian character is
justiﬁed because it results from an estimation of the center of gravity over a “large”
number of pixels (typically a few tens).
The solution traditionally used for estimating the phase, in particular under real-
time constraints (AO), is the least squares estimation. Matrix DT D is not invertible a
priori , because the number of measurements is ﬁnite (2N 2
ml), whereas dimension K
of vector φ is, in theory, inﬁnite. In practice, even when K is chosen to be slightly
smaller than 2N 2
ml, DT D is ill-conditioned. The usual remedy is to reduce dimension
K of the space of unknowns φ and to ﬁlter a few modes that are not seen or are poorly
seen by the sensor. These correspond to the few zero or very small eigenvalues of
DT D. It is common to take K ≃N 2
ml.

Inversion in Optical Imaging through Atmospheric Turbulence
257
This remedy works correctly because the Zernike polynomial basis is well suited
to atmospheric turbulence. Firstly, these polynomials are in an order corresponding
to higher and higher spatial frequencies and the turbulence has a PSD that decreases
quite fast (see equation (10.3)), so the diagonal of the turbulent phase covariance ma-
trix on the basis of the Zernikes is decreasing. Secondly, this matrix is concentrated
around its diagonal. In other words, the Zernike polynomials Zi are quite close to the
eigenmodes (Karhunen-Loève) of the turbulence. In consequence, truncation of the
basis {Zi} at a ZK selects a space containing the most energetic modes. Choosing
the best K is quite problematic, as it depends on both the strength of the turbulence
r0 and the noise level on the WFS.
As our statistical knowledge of the turbulence is quite good (see references of
section 10.1.3), a Bayesian approach is more appropriate and gives better results.
Since the problem of reconstructing the phase is linear and Gaussian, it leads to an
analytical MMSE/MAP estimator, in covariance form in [WAL 83] and in information
form in [BAK 94, SAS 85] (see Chapter 3). MAP estimation of each of the phases
corresponds to minimizing the mixed criterion Jφ
MAP = Js + Jφ, with:
Js = 1
2(st −Dφt)T C−1
b′ (st −Dφt)
(10.10)
and
Jφ = 1
2φT
t C−1
φ φt
(10.11)
where Cb′ is the covariance matrix of the slope measurement noise (diagonal, with a
practically constant diagonal) and Cφ is the covariance matrix of the turbulent phase
in the Zernike basis, which is deduced from equation (10.3) [NOL 76] depending only
on r0. The well known solution is:
φt = (DT C−1
b′ D + C−1
φ )−1DT C−1
b′ st.
(10.12)
This solution takes advantage of our knowledge of the spatial statistics of the turbu-
lence. For use in AO, where the sampling frequency is generally well above 1/τ0,
it is judicious to opt for a natural extension of this MMSE estimator that also uses
prior knowledge on the time statistics of the turbulence. This extension is the optimal
estimator of Kalman ﬁltering [LER 04, PET 05, KUL 06].
10.3.3. Phase retrieval and phase diversity
Phase retrieval consists of estimating the aberrations seen by an instrument from
the image of a point source. This comes down to inverting equation (10.1), i.e., es-
timating its phase ϕ from a measurement of h. This technique, ﬁrst used in electron

258
Bayesian Approach to Inverse Problems
microscopy [GER 72] then rediscovered in optics [GON 76], has two main limita-
tions: (i) it only works with a point object and (ii) the solution obtained suffers from
sign ambiguity and is generally not unique.
Gonsalves [GON 82] has shown that, by using a second image containing a known
variation in the aberrations with respect to the ﬁrst, e.g. a slight defocus, it is possible
to estimate the aberrations even if the object is spatially extended and unknown. More-
over, this second image lifts the indetermination mentioned above and the estimated
aberrations are unique, in practice, for small aberrations. This technique is called
phase diversity by analogy with a technique used in wireless telecommunications.
Phase diversity is used in two different contexts. We may wish to obtain an image
of a remote object, e.g. in solar astronomy, or we may wish to measure the aberrations
seen by an instrument in order to correct them in real time or off-line. These two
problems are connected but nevertheless distinct. In both cases, the basis of the in-
version is to estimate the aberrations and object that are the most compatible with the
measured images. The conventional approach is a joint estimation of the object and
the phase [GON 82] possibly with a regularization for both unknowns. Although this
type of joint estimation usually has poor statistical properties, in the speciﬁc case of
phase diversity, it has been shown that joint estimation leads to a consistent estimator
for aberrations [IDI 05]. In addition, a so-called marginal approach that integrates the
object out of the problem so as to only estimate the phase has been proposed recently
and leads to better robustness with respect to noise [BLA 03].
The interested reader will ﬁnd a more complete history and a review of the ap-
plications of this WFS in [MUG 06], which also contains a detailed study of the two
estimators mentioned above.
10.4. Myopic restoration in imaging
10.4.1. Motivation and noise statistic
In imaging through turbulence with a monolithic telescope, the data processing
needed is essentially a deconvolution. Nevertheless, the estimation or the measure-
ment of the PSF is often imperfect and the best deconvolution results are generally
obtained by speciﬁcally taking the partial lack of knowledge of the instrument’s re-
sponse into account. This is what we will call myopic deconvolution, which can take
different forms depending on whether the turbulence is corrected off-line, by DWFS
(section 10.4.2) or in real time by AO (section 10.4.3).
The most used data-ﬁdelity term is the ordinary least squares criterion. In a prob-
abilistic interpretation, this criterion corresponds to the assumption that the noise is

Inversion in Optical Imaging through Atmospheric Turbulence
259
white, Gaussian and stationary (see Chapter 3):
Jy(x) =
1
2σ2
b
∥Hx −y∥2
(10.13)
where x is the observed object, y the recorded image, H the imaging operator and
σb the standard deviation of the noise. In astronomical imaging, this interpretation is
generally a crude approximation, except for a large bright object, as the predominant
noise is generally photonic and thus follows Poisson statistics leading to the following
data-ﬁdelity term:
Jy(x) =

ℓ,m
(Hx −y log Hx)(ℓ, m)
(10.14)
This non-quadratic criterion can cause practical difﬁculties for the minimization when
gradient-based numerical methods are used. What is more, in very dark parts of the
image, the electronic noises of the sensor become non-negligible relative to the pho-
tonic noise and ﬁne modeling of the noise must take the simultaneous presence of
noise from the sensor (typically a CCD device) and photonic noise into account. These
speciﬁc difﬁculties will be examined more closely in Chapter 14.
A good compromise between ﬁne modeling of the noise and efﬁcient minimization
can be obtained as follows. A quadratic approximation of (10.14) is deduced ﬁrst,
which corresponds to purely photonic noise. For an image that is not too dark (in
practice, ten or so photons per pixel can sufﬁce), the approximation Hx −y ≪y can
be taken and (10.14) expanded to the second order. The result corresponds to white,
non-stationary, Gaussian noise with variance equal to the image at each point. Then,
by simply summing the variances, a data-ﬁdelity criterion is obtained that models the
simultaneous presence of sensor and photonic noise [MUG 04]:
Jy(x) =

ℓ,m
1
2(σ2
ph(ℓ, m) + σ2
det)|(Hx)(ℓ, m) −y(ℓ, m)|2
(10.15)
where σ2
ph(ℓ, m) = max {y(ℓ, m), 0} is an estimator of the variance of the photonic
noise at each pixel and σ2
det the variance of the sensor noise, estimated beforehand.
10.4.2. Data processing in deconvolution from wavefront sensing
10.4.2.1. Conventional processing of short-exposure images
In this subsection, we describe non-myopic multiframe deconvolution; in other
words, we consider that the PSFs deduced from the WFS measurements are true. In
the next subsection, we show how a myopic deconvolution, i.e., the joint processing
of WFS data and images, can improve the estimation of the observed object.

260
Bayesian Approach to Inverse Problems
We have a series of Nim short-exposure images of an object that is smaller than
the isoplanatic patch. The equation of the discretized direct problem can be written:
yt = ht ⋆x + bt = Htx + bt,
1 ≤t ≤Nim
(10.16)
where x and yt are the discretized object and image respectively at time t and where
the PSF ht is related to the phase ϕt in the pupil at the same instant by equation (10.1).
We also have wavefront measurements, in this case Hartmann-Shack slope measure-
ments st associated with each image.
The conventional DWFS data processing is sequential: ﬁrst we estimate the phases
φt via (10.12), then deduce the PSFs ht via (10.1) and ﬁnally estimate the object by
multiframe deconvolution. The details of this sequential processing are given below.
The image processing used in the early days of the DWFS technique was a sim-
ple multiframe least squares method [PRI 90]; the solution was thus the multiframe
inverse ﬁlter, which in practice had to be regularized by adding a small constant to
the denominator in the Fourier domain. A better approach is to explicitly regularize
the criterion to be minimized. For objects with clearly marked edges such as artiﬁcial
satellites, the regularization criterion to be used is the L2L1 of equation (10.8).
Using the Bayesian framework presented in Chapter 3, we estimate the object
in the MAP sense. Two considerations allow the likelihood of the set of images to
be simpliﬁed: ﬁrstly, the noise is independent between images and, secondly, the
delay between successive acquisitions is generally longer than the typical turbulence
evolution time. The likelihood can thus be rewritten as the product of the likelihoods
of the individual images, each being conditioned simply by the object and the phase
at the same instant. The estimate of the object is then the one that minimizes
Jx
MAP(x) =
Nim

t=1
Jy(x; φt, yt) + Jx(x),
(10.17)
where Jy(x; φt, yt) = −log p(yt | x, φt). In practice, for both simulations (see sec-
tion 10.4.2.3) and experimental data (see section 10.4.2.4), the data-ﬁdelity term used
for Jy will be the least squares term of equation (10.13). The minimization is per-
formed numerically on the object variables and the presence of the φt in the Jx
MAP
criterion above is simply a reminder of the dependence of the criterion on the phase.
10.4.2.2. Myopic deconvolution of short-exposure images
In conventional DWFS data processing, the information concerning the wavefronts
is extracted from the WFS data only, not from the images. And yet there is exploitable
information on the PSF in the short-exposure images, as proved by the results some
authors [SCH 93, THI 95] have obtained by blind deconvolution (i.e., without WFS,
but using the models (10.1) and (10.9)).

Inversion in Optical Imaging through Atmospheric Turbulence
261
However, the criteria to be minimized in blind deconvolution generally have local
minima and parametrizing the PSF by the pupil phase is not sufﬁcient to ensure that
the solution is unique. This is why WFS data should certainly not be ignored but
should instead be used in conjunction with the images.
Myopic deconvolution consists of searching for the most probable object x and
turbulent phases φt jointly, given the images yt, the WFS measurements st and the
prior information on x and φt [MUG 01]. Using Bayes’ rule and the same indepen-
dence hypotheses as in section 10.4.2.1, it can be shown that the estimates (x, {φt})
in the joint MAP sense are those that minimize:
JMAP(x, {φt}) =
Nim

t=1
Jy(x, φt ; yt) + Jx(x) +
Nim

t=1
Js(φt ; st) +
Nim

t=1
Jφ(φt)
where:
– the Jy are the image-ﬁdelity terms; Jy(x, φt; yt) is the anti-log-likelihood of the
tth image; it is now a function of the object and the phases;
– Jx(x) is the object prior, which, in what follows, will be the L2L1 model of
equation (10.8);
– the Js are the ﬁdelity to WFS data terms; with the hypotheses used, they are
quadratic and given by equation (10.10);
– the Jφ are the a priori terms on the phases given by equation (10.11).
The criterion is minimized by a method based on conjugate gradients, alternating
minimizations on the object x (for the current phase estimate) and on the set of phases
φt (for the current object estimate).
To speed up the minimization and also to avoid, in practice, the local minima often
present in joint criteria, the initial object and phases are taken to be the MAP estimates
obtained in the sequential processing described in section 10.4.2.1.
10.4.2.3. Simulations
A set of 100 images were simulated with the associated WFS measurements. The
100 wavefronts were obtained by a modal method [ROD 90] in which each phase is
expanded on a basis of Zernike polynomials (see equation (10.9)) and follows Kol-
mogorov statistics (see equation (10.3)). The turbulence strength corresponds to a
ratio D/r0 = 10. Each of the turbulent wavefronts is used to calculate a short-
exposure image of dimensions 128 × 128, sampled at the Shannon frequency using
equations (10.1) and (10.16). The noise added to the images is white, Gaussian and
stationary with a variance equal to the mean ﬂux to be simulated, i.e., 104/1282 =
0.61 photon/pixel. Figure 10.4 shows the object, which is a numerical model of the
SPOT satellite, and one of the 100 simulated images. The corresponding PSF is the

262
Bayesian Approach to Inverse Problems
Figure 10.4. Original object (SPOT satellite, left) and one of the 100 short-exposure images
(D/r0 = 10, right)
image on the left of Figure 10.2. The simulated WFS is a Hartmann-Shack having
20 × 20 sub-apertures, without central obscuration. White Gaussian noise is added to
the local slopes of the wavefront so that the SNR of the slopes measured, deﬁned as
the variance of the slopes over the variance of the noise, is 1.
Figure 10.5 compares the results of the sequential and myopic estimations for the
same L2L1 prior on the object (equation (10.8)), associated with a positivity con-
straint4. On the left, the non-myopic restoration is the MAP estimation of the wave-
fronts followed by a restoration with PSFs deduced from the estimated wavefronts and
gives an MSE with the actual object of 0.45 photon (per pixel). On the right, the joint
estimation gives an MSE of 0.39 photon.
In addition, the myopic estimation also allows the quality of the reconstructed
wavefronts to be improved [MUG 01].
10.4.2.4. Experimental results
ages of the double star Capella recorded on 8th November 1990 with the DWFS
Palma, Canary Islands). The experimental conditions were the following: a ﬂux of
67,500 photons per image, an exposure time of 5 ms, a D/r0 of 13 and a SNR of 5 on
the WFS. The WFS was a Hartmann-Shack with 29 × 29 sub-apertures, 560 of which
were used.
Figure 10.6, taken from [MUG 01], shows the results of the deconvolution. On the
left, the sequential processing consisted of an estimation of the wavefronts by MAP,
then a quadratic image restoration. The binary nature of Capella is visible, but is
4. Our thanks to Clélia Robert for processing the DWFS data.
system of ONERA, installed on the 4.20m diameter William Herschel telescope (La
The processing methods described above were applied to ten experimental im-

Inversion in Optical Imaging through Atmospheric Turbulence
263
Figure 10.5. Object restored by non-myopic (left) and myopic (right) estimation. In both
cases, an L2L1 prior and an object positivity constraint were used. The MSE with the actual
object are 0.45 and 0.39 photon respectively
Figure 10.6. Deconvolved experimental images of Capella: left, estimation of wavefronts by
MAP then quadratic deconvolution; right, myopic deconvolution. In both cases, the prior used
was Gaussian with a constant PSD deduced from the measured ﬂux, with a positivity
constraint
almost drowned in strong ﬂuctuations. On the right, the myopic deconvolution has
eliminated almost all the artefacts of the non-myopic deconvolution. In both cases,
the same quadratic object regularization with a positivity constraint was used, with a
constant PSD whose value was deduced from the measured ﬂux.
10.4.3. Restoration of images corrected by adaptive optics
10.4.3.1. Myopic deconvolution of images corrected by adaptive optics
Long-exposure images corrected by AO must be deconvolved, since the correction
is only partial [CON 94]. If we take the PSF as known, the object estimated in the
MAP sense, denoted xMAP, is the one that maximizes p(x | y ; h), and thus minimizes

264
Bayesian Approach to Inverse Problems
Jy(x ; h, y) + Jx(x). The most usual method for estimating the PSF is to record the
image of a star just before or just after the image of the object of interest. This star
image may be noticeably different from the PSF corresponding to the image we are in-
terested in for a variety of reasons: ﬁrst, the turbulence changes with time [CON 98a];
thus, the response of the AO may be different when going from a spatially extended
object to a point object, even if the star is of the same magnitude as the object, since
the wavefront sensing error increases with the extent of the object; and ﬁnally, there is
noise on the star image itself. A method has been proposed and validated for estimat-
ing the turbulent part of the long-exposure transfer function corrected by the AO from
measurements of the residual wavefront of the control loop [VER 97a, VER 97b].
Nevertheless, apart from the fact that the static or slowly varying aberrations of the
telescope may not be properly known, the accuracy of this estimation of the transfer
function is limited by the noise on the WFS. Thus, it is often necessary to consider
that the PSF is imperfectly known.
Many authors have tackled the problem of deconvolving an image degraded by
turbulence with unknown PSF. Ayers and Dainty [AYE 88] used a Gerchberg-Saxton-
Papoulis algorithm [GER 72] and came up against problems of convergence with this
type of algorithm. Others have used maximum likelihood methods, with an EM algo-
rithm [HOL 92] or minimization of an explicit criterion [JEF 93, LAN 92, LAN 96,
THI 95]. They generally recognize the need for regularization rather than just positiv-
ity (of the object and the PSF) and, in particular, have introduced a (legitimate) limited
bandwidth constraint on the PSF through an ad hoc prior [HOL 92, JEF 93].
The Bayesian framework allows this joint estimation (called myopic estimation)
of the object and the PSF to be made with a natural regularization for the PSF and
without having to adjust any additional hyperparameters. The joint MAP estimator is
given by:
(x, h) = arg max
x,h
p(x, h|y) = arg max
x,h
p(y|x, h) × p(x) × p(h)
= arg min
x,h
(Jy(x, h; y) + Jx(x) + Jh(h))
The long-exposure PSF can be considered as the sum of a large number of inde-
pendent short-exposure PSFs, and thus modeled by a Gaussian prior (truncated to pos-
itive values). We also assume that the difference between the PSF and the mean PSF
is approximately stationary. The regularization of the PSF is thus a quadratic penal-
ization of the transfer function, which is independent between frequencies [CON 98b,
FUS 99, MUG 04]:
Jh(h) = 1
2

f
|
⌢
h(u) −
⌢
hm(u)|2/Sh(u)

Inversion in Optical Imaging through Atmospheric Turbulence
265
where
⌢hm = E(
⌢h) is the mean transfer function and Sh = E(|
⌢h(u) −
⌢hm(u)|2)
the energy spectral density (ESD) of the PSF. Note that Sh is zero beyond the cut-
off frequency of the telescope and that this regularization, in particular, forces h to
comply with the limited bandwidth constraint.
In practice, the mean transfer function and the ESD of the PSF are estimated by
replacing the expectations in their deﬁnitions by empirical means on the various im-
ages of the star acquired before or after the object of interest. If only a single image of
the star is available, the expectation can be replaced by a circular mean in the Fourier
domain because of the isotropy of the quantities to be estimated.
In order to be able to restore objects with a large dynamic range, which are frequent
in astronomy, the data-ﬁdelity term Jy must include ﬁne modeling of the noise, such
as the mixture of photonic and electronic noise of equation (10.15), rather than a
simple least squares. The regularization criterion Jx used here is the L2L1 model of
equation (10.8), which is well suited to objects with sharp edges such as planets and
asteroids.
The restoration method known as MISTRAL [MUG 04] combines the myopic es-
timation of the object and PSF described earlier with the white inhomogeneous data-
ﬁdelity term and the L2L1 regularization just mentioned. This method was used to
obtain the deconvolution results presented below. The criterion was minimized by
the conjugate gradient method, jointly on the object and PSF variables. A positivity
constraint was added on x and on h.
10.4.3.2. Experimental results
Figure 10.7a shows an AO-corrected long-exposureimage of Ganymede, a satellite
of Jupiter. This image was recorded on 28th September 1997 on the ONERA AO sys-
tem installed on the 1.52 m telescope of the Haute-Provence observatory. This system
has an 80 Hz passband; it comprises a Hartmann-Shack wavefront sensor with 9 × 9
sub-apertures (64 of which are active) and a deformable mirror with 10 × 10 piezo ac-
tuators, 88 of which are active. The imaging wavelength is λ = 0.85 μm and the expo-
sure time 100 s. The total estimated ﬂux is 8.107 photons and the estimated D/rx ratio
is 23. The total ﬁeld is 7.9 arcsec, only half of which is shown here. The mean PSF and
its ESD were estimated from 50 recorded images of a nearby bright star. Figures 10.7b
and c show the restorations obtained by the Richardson-Lucy algorithm (maximum
likelihood for a Poisson noise), interrupted at 200 and 3,000 iterations respectively.
In the ﬁrst case, the restored image is quite blurred and shows ringing, and in the sec-
ond case, the noise dominates the restoration. The image of Figure 10.8a illustrates
myopic deconvolution [MUG 04] with an L2L1 prior5. Figure 10.8b shows a wide-
band synthetic image obtained from photos taken by a NASA/JPL space probe (see
5. Our thanks to Thierry Fusco for processing the AO images.

266
Bayesian Approach to Inverse Problems
(a) image corrected by AO
(b) Richardson-Lucy, 200 it.
(c) Richardson-Lucy, 3,000 it.
Figure 10.7. (a) Observation of Ganymede with the ONERA AO system on 28th September
1997; (b) Richardson-Lucy restoration interrupted at 200 iterations; (c) at 3,000 iterations
(a) L2L1 myopic deconvolution
(b) JPL database
(NASA/JPL/Caltech)
(c) image (b) + PSF of perfect
telescope
Figure 10.8. (a) L2L1 myopic deconvolution of the image of Ganymede of Figure 10.7; (b) for
comparison, a wideband synthetic image obtained from the NASA/JPL database; (c) same
synthetic image convolved by the perfect PSF of a 1.52 m-diameter telescope
http://space.jpl.nasa.gov/) as it passed near Ganymede. The comparison
shows that many features of Ganymede have been correctly restored. A fairer com-
parison is to examine the myopic deconvolution performed by MISTRAL together
with the image of Figure 10.8b convolved with the perfect PSF of a 1.52 m telescope,
presented in Figure 10.8c.

Inversion in Optical Imaging through Atmospheric Turbulence
267
Figure 10.9 shows three images of Neptune recorded at half-hour intervals on 6th
July 1998 with the curvature-based adaptive optics system of the Institute for As-
tronomy of the University of Hawaii6 called Hokupa’a. This system, which was
in operation until 2003, had 36 actuators and was installed on the Canada-France-
Hawaii (CFH) 3.6 m telescope. It produced the ﬁrst high-resolution infrared images
of Neptune in November 1997 and July 1998 [ROD 98]. The imaging wavelength
was 1.72 μm, which is situated in a methane absorption band. The exposure time was
10 minutes per image. The images restored by myopic deconvolution with an prior
are shown in Figure 10.10 [CON 00]. The image of a star near Neptune was also
recorded in order to estimate the mean PSF and the ESD of the PSF by the circular
mean in the Fourier domain. Because the atmosphere of Neptune is very dark at the
imaging wavelength, these images show the ﬁne structures of the cloud bands in the
upper atmosphere with good contrast. Note, in particular, that the ﬁne structures of
the cloud bands can be followed from image to image as the planet turns. This was the
ﬁrst time it had been possible to study the details of Neptune’s atmospheric activity
from the ground.
Figure 10.9. Images of Neptune obtained at 30-minute intervals on 6th July 1998 with the
Hokupa’a adaptive optics system on the Canada-France-Hawaii telescope. The imaging
wavelength was 1.72 μm and the exposure time for each image was 10 minutes
10.4.4. Conclusion
The restoration of images degraded by turbulence and corresponding to a convo-
lutive imaging model is now a well mastered technique. The observation systems
currently being developed have more complex acquisition modes for which process-
ing will no doubt be largely called upon. Representative examples are the wide-ﬁeld
6. Our thanks to François and Claude Roddier for so kindly providing us with these images.

268
Bayesian Approach to Inverse Problems
Figure 10.10. Images of Figure 10.9 restored by myopic deconvolution with L2L1 prior
systems with so-called multiconjugate AO [CON 05], for which the PSF cannot be
considered as spatially invariant, and the systems such as SPHERE [DOH 06] or GPI
that combine high-performance AO (known as extreme AO) with a coronograph in
the aim of detecting exoplanets. For such systems, imaging is fundamentally non-
convolutive and speciﬁc processing has to be developed. AO has also found an ap-
plication in retinal imaging in recent years and several teams are developing opera-
tional systems (see for example [GLA 02, GLA 04] and the references therein). In
this context, the image measured and the object to be restored are three-dimensional
[CHE 07].
10.5. Image reconstruction in optical interferometry (OI)
This section is devoted to the reconstruction of images from data coming from a
correlation interferometer. Section 10.1.2.2 presented the measurement principle and
the type of transfer function associated with these systems. The observation model is
presented more precisely in section 10.5.1, then sections 10.5.2 and 10.5.3 describe
the main avenues towards image reconstruction at present. Finally, results on synthetic
and real data are the subject of section 10.5.4.
10.5.1. Observation model
Let us consider a two-telescope interferometer. The positions of the telescopes in
a plane normal to the observation direction are r1 and r2. Due to the Earth’s rotation,
the observation direction changes with time and baseline r2 −r1 thus varies, as does
the spatial frequency corresponding to it:
u12(t)
Δ= (r2(t) −r1(t)) /λ.

Inversion in Optical Imaging through Atmospheric Turbulence
269
When a complete interferometer array is used, i.e., one in which all the possible two-
telescope baselines can be formed simultaneously, there are Nb = Nt(Nt −1)/2
measurement frequencies given by
ukℓ(t) = (rℓ(t) −rk(t)) /λ, 1 ≤k < ℓ≤Nt.
Each baseline (Tk, Tℓ) produces interference fringes. The measurement of the
contrast and position of these fringes deﬁnes the complex visibility ydata
kℓ
(t) and gives
information on the modulus akℓ(x, t) and the phase φkℓ(x, t) of the FT of object x at
spatial frequency ukℓ.
When the instrument is calibrated, generally by prior observation of an unresolved
object, we no longer have to consider the possibly complex gains, that come into
measurement (10.7) presented in section 10.1.2.2. On the other hand, the effects of
turbulence, which vary rapidly, cannot be precalibrated. We can thus take it that the
main perturbation affecting the short-exposure phase measurement is an additive term
ϕℓ(t) −ϕk(t) known as the differential piston term:
φdata
kℓ
(t) = φkℓ(x, t) + ϕℓ(t) −ϕk(t) + noise [2π]
(10.18)
where φdata
kℓ
(t) is the phase of ydata
kℓ
(t). Thus, in matrix form, φdata(t) = φ(x, t) +
Bϕ(t) + noise [2π], where the baseline operator B has dimensions Nb × Nt.
As mentioned in section 10.1.2.2, the differential piston is the result of the random
differences introduced in the optical path between the apertures of the system by tur-
bulence. For a long baseline (relative to the Fried diameter), the optical path difference
may be very much greater than the observation wavelength and thus lead to random
phase differences much larger than 2π. The aliased perturbation that affects the phase
(10.18) is then practically uniformly distributed in [0, 2π]. In consequence, averaging
the phases of short-exposure visibility (10.18) does not improve the signal-to-noise
ratio. A solution is to carry out phase closures [JEN 58] before the averaging. For any
set of three telescopes (Tk, Tℓ, Tm) the short-exposure visibility phase data is
⎧
⎪
⎪
⎨
⎪
⎪
⎩
φdata
kℓ
(t) = φkℓ(x, t) + ϕℓ(t) −ϕk(t) + noise
φdata
ℓm (t) = φℓm(x, t) + ϕm(t) −ϕℓ(t) + noise
φdata
mk (t) = φmk(x, t) + ϕk(t) −ϕm(t) + noise
(10.19)
and the turbulent pistons are canceled out in the closure phase deﬁned by:
βdata
kℓm (t)
Δ= φdata
kℓ
(t) + φdata
ℓm (t) + φdata
mk (t) + noise
= φkℓ(x, t) −φℓm(x, t) + φmk(x, t) + noise
= βkℓm(x, t) + noise
(10.20)

270
Bayesian Approach to Inverse Problems
To form this type of expression it is necessary to measure 3 visibility phases simulta-
neously, and thus to use an array of 3 telescopes or more. For a complete array made
up of Nt telescopes, the set of closure phases that can be formed is generated by, for
example, the βdata
1kℓ(t), 1 < k < ℓ≤Nt, i.e., the closure phases measured on the tri-
angles of telescopes including T1. It is easy to see that there are (Nt−1)(Nt−2)/2 of
these independent closure phases. In what follows, the vector grouping together these
independent closure phases will be noted βdata and a closure operator C is deﬁned
such that
βdata Δ= Cφdata = Cφ(x, t) + noise.
The second equation is a matrix version of (10.20): the closure operator cancels the
differential pistons, a property that can be written CB = 0. It can be shown that this
equation implies that the closure operator has a kernel of dimension Nt −1, given by
Ker C = { ¯Bα, α ∈
 Nt−1}
(10.21)
where ¯B is obtained by removing the ﬁrst column from B. The closure phase mea-
surement thus does not allow all the phase information to be measured. This result can
also be obtained by counting up the phase unknowns, i.e., Nt(Nt −1)/2 object visi-
bility phases minus the number of independent closures, (Nt −1)(Nt −2)/2, which
gives Nt −1 missing phase data. In other words, optical interferometry through tur-
bulence comes under Fourier synthesis with partial phase information. Note that, the
more apertures there are in the array, the smaller the proportion of missing information
will be.
We are now in a position to deﬁne the long-exposure observables of a correlation
interferometer:
– mean square amplitudes sdata(t) =

(adata(t + τ))2
τ, in preference to mean
moduli as they have an easy-to-calculate bias, which can be substracted from the mea-
surements;
– bispectra V data
1kℓ(t), k < ℓ, deﬁned by
V data
1kℓ(t) =

ydata
1k
(t + τ)ydata
kℓ
(t + τ)ydata
l1
(t + τ)

τ.
The modulus of the bispectrum is redundant with the squares of the amplitudes and
is thus not used in image reconstruction. The phases of the bispectra βdata
1kℓ(t), k < ℓ
constitute unbiased long-exposure closure phase estimators.
Notation τ expresses the averaging in a time interval around instant t, an interval
that must be short enough for the spatial frequency to be considered constant during
the integration despite the rotation of the Earth. The integration time also determines
the standard deviations of the residual noises on the measurements.

Inversion in Optical Imaging through Atmospheric Turbulence
271
The long-exposure observation model is ﬁnally:
 sdata(t) = a2(x, t) + snoise(t),
snoise(t) ∼N

0, Rs(t)

βdata(t) = Cφ(x, t) + βnoise(t),
βnoise(t) ∼N

0, Rβ(t)

(10.22)
Estimating an object from such Fourier data is called Fourier synthesis. Matrices
Rs(t) and Rβ(t) are generally assumed to be diagonal. In terms of prior knowledge,
the object we are looking for is positive. Moreover, as visibilities are ﬂux-normalized
quantities, it is convenient to work with the constraint of unit ﬂux. The constraints on
the object are thus

k,ℓ
x(k, ℓ) = 1,
∀k, ℓ, x(k, ℓ) ≥0.
(10.23)
10.5.2. Traditional Bayesian approach
This approach ﬁrst forms the anti-log-likelihood according to model (10.22)
Jdata(x) =

t
Jdata(x, t) =

t
χ2
s(t)(x) + χ2
β(t)(x)
(10.24)
with the notation
χ2
m(t)(x)
Δ=

mdata(t) −m(x, t)
T R−1
m(t)

mdata(t) −m(x, t)

,
then associates Jdata with a regularization term such as those presented in section10.2.
The problem thus is to minimize the composite criterion
J(x) = Jdata(x) + Jx(x)
(10.25)
obtained under the constraints (10.23). Among the references that adopt this approach
for processing optical interferometry data, [THI 03] is one of the most noteworthy.
Such works are based on the use of local descent methods. Unfortunately, criterion
J is non-convex. To be more precise, the difﬁculty of the problem can be summed up
in the following three points:
1) the small number of Fourier coefﬁcients makes the problem under-determined:
high frequencies of the reconstructed object [LAN 98];
makes the Fourier synthesis problem non-convex and adding a regularization term
does not generally correct the problem;
2) the turbulence implies phase indetermination. This type of indetermination
the regularization term can get around this under-determination, e.g. by limiting the

272
Bayesian Approach to Inverse Problems
3) ﬁnally, the fact that we have phase modulus measurements with Gaussian noise
leads to a non-Gaussian likelihood in x and a non-convex log-likelihood. This point,
which has long been known in the ﬁeld of radar was identiﬁed only very recently
in optical interferometry [MEI 05c]. In other words, even if we had all the complex
visibility phase measurements instead of just the closure phases, the data ﬁdelity term
would still be non-convex.
These characteristics imply that optimizing J by a local descent algorithm can only
work if the initialization puts us in the “right” valley of the criterion. The use of
a global optimization algorithm has never been proposed in optical interferometry
as far as we know. It would no doubt be useful to explore this path as long as the
number of variables remained reasonable, in particular in comparison with the very
large dimension maps that are reconstructed in radio interferometry.
10.5.3. Myopic modeling
Another approach is to put the problem in terms of missing data; this is phase data
that is eliminated by the use of a closure operator, i.e., elements of the kernel of C
(10.21). The myopic approach thus consists of ﬁnding object x and missing phase data
α jointly. This technique is called self-calibration in radio-interferometry [COR 81]
and has enabled reliable images to be reconstructed in situations of partial phase inde-
termination. The ﬁrst myopic approaches put forward in optical interferometry were
strongly inﬂuenced by this work [LAN 98]. Recent ﬁndings indicate that these trans-
positions were based on too great a simpliﬁcation of the measuring procedure belong-
ing to optical interferometry. This section outlines a precise myopic approach applied
to OI.
The construction of a myopic model starts from a generalized inverse solution to
the phase closure equation of (10.22), using the operator
C† Δ= CT 
CCT−1.
By applying C† on the left to (10.22) and (10.21) we have
∃α(t) | C†βdata(t) = φ(x, t) + ¯Bα(t) + C†βnoise(t).
It is thus tempting to deﬁne a pseudo-equation of visibility phase measurement by
identifying the last term of the latter equation with a measurement pseudo-noise:
φdata(t) = φ(x, t) + Bα(t)
5
67
8
φ(x,α(t),t)
+φnoise(t).
(10.26)
This approach is similar to that presented in reference [LAN 01]. Unfortunately, as
matrix C† is singular, this identiﬁcation is not rigorously possible and we are led to as-
sociate an ad hoc covariance matrix Rφ with the term φnoise(t) so as to approximately

Inversion in Optical Imaging through Atmospheric Turbulence
273
ﬁt the statistical behavior of the closures. These problems of covariance approxima-
tion are ignored in [LAN 01]. The more recent references [MEI 05a, MUG 07] discuss
the possible choices for Rφ and propose the use of the following diagonal matrix:
Rφ ∝Diag

C†RβC†,T 
where the expression Diag {M} designates the diagonal matrix formed with the di-
agonal of M.
Finding a suitable approximation for the covariance of the amplitude measure-
ments (10.22), see [MEI 05a, MUG 07], gives a myopic measurement model, i.e., one
that depends on the unknowns x and α:
 adata(t) = a(x, t) + anoise(t),
anoise(t) ∼N
¯a(t), Ra(t)

φdata(t) = φ(x, α(t), t) + φnoise(t),
φnoise(t) ∼N
¯φ(t), Rφ(t)

(10.27)
We now have an explicit model of the phase indetemination noted in section 10.5.2.
At this stage, it is possible to envisage using, for example, alternating descent algo-
rithms that successively optimize a regularized criterion coming from (10.27), accord-
ing to x and α. However, it is still true that, as this model is given in modulus and
phase, it always leads to a data-ﬁdelity term that is non-convex in x, for ﬁxed α.
Below, we brieﬂy present a convex approximation of this model.
From the pseudo-measurements adata(t) and φdata(t), let us form complex pseu-
do visibilities
ydata(t)
Δ= adata(t) ejφdata(t).
The data model is thus
ydata(t) =

a(x, t) + anoise(t)

ej(φ(x,α(t),t)+φnoise(t)).
The noise on these measurements, although additive and Gaussian in modulus and
phase separately, is not a complex additive Gaussian noise. In reference [MEI 05b],
the authors show how this distribution can be best approximated by an additive Gaus-
sian noise ynoise(t).
ydata(t) = y(x, α(t), t) + ynoise(t)
(10.28)
with
y(x, α(t), t)
Δ= a(x, t)eφ(x,α(t),t)
(10.29)
In general, this approximation leads to a data ﬁtting term Jpseudo that is quadratic
in the real and imaginary parts of the residuals ydata
kℓ
(t) −ykℓ(x, α(t), t). By asso-
ciating this term with a convex regularization term, we obtain a composite criterion
that is convex in x at ﬁxed α. The WISARD algorithm [MUG 07] makes use of this
property by minimizing this composite criterion alternately in x for the current α and
in α for the current x.

274
Bayesian Approach to Inverse Problems
10.5.4. Results
This section presents some results of processing using the WISARD algorithm
[MUG 07] based on the myopic approach described in 10.5.3.
10.5.4.1. Processing of synthetic data
The ﬁrst example takes synthetic interferometric data that was used in the inter-
national Imaging Beauty Contest organized by P. Lawson for the International Astro-
nomical Union (IAU) [LAW 04]. This data simulates the observation of the synthetic
object shown in Figure 10.11 with the NPOI [NPO] 6-telescope interferometer. The
corresponding frequency coverage, shown in Figure 10.11, has a circular structure
typical of the super-synthesis technique. We recall that super-synthesis consists of re-
peating the measurements over several instants of measurement (possibly over several
nights of observation) so that the same baselines access different spatial frequencies
because of the Earth’s rotation. In total, there are 195 square visibility modules and
130 closure phases, together with the associated variances.
Figure 10.11. Synthetic object (right) and frequency coverage (left) from
the Imaging Beauty Contest 2004
Three reconstructions obtained with WISARD are shown in Figure 10.12. On the
left is a reconstruction using a quadratic regularization based on a PSD model in 1/|u|3
for a weak regularization parameter, in the center a reconstruction with a correct pa-
rameter. The latter gives a satisfactory level of smoothing but does not restore the peak
in the center of the object. The peak is visible in the under-regularized reconstruction
on the left but at the cost of too high a residual variance.
The reconstruction presented on the right is a good trade-off between smoothing
and restoration of the central peak thanks to the use of the white prior term introduced

Inversion in Optical Imaging through Atmospheric Turbulence
275
in section 10.2. The goodness of ﬁt of the L2L1 reconstruction can be appreciated in
Figure 10.13. The crosses show the reconstructed visibility moduli (i.e., of the FT of
the reconstructed object at the measurement frequencies) and the squares the moduli
of the measured visibilities. The difference between the two, weighted by 10 times
the standard deviation of the moduli, is shown as the line. The mean value of this
difference is 0.1, which shows a good ﬁt, to one standard deviation.
Figure 10.12. Reconstructions with WISARD. Left: under-regularized quadratic model;
center: quadratic model with correct regularization parameter;
right: white L2L1 model of equation (10.8)
0
2.0•107
4.0•107
6.0•107
8.0•107
1.0•108
1.2•108
frequency
0.0
0.2
0.4
0.6
0.8
1.0
Abs(Reconstructed Vis.)
Abs(Measured Vis.)
Abs(Difference)/(10 x stddev)
Figure 10.13. Goodness of ﬁt at WISARD convergence

276
Bayesian Approach to Inverse Problems
10.5.4.2. Processing of experimental data
Here, we present the reconstruction of the star χ Cygni from experimental data
using the WISARD [MUG 07] algorithm. The data was obtained by S. Lacour and
S. Meimon under the leadership of G. Perrin during a measuring campaign on the
IOTA interferometer [IOT] in May 2005. As already mentioned, each measurement
has to be calibrated by observation of an object that acts as a point source at the instru-
ment’s resolving power. The calibrators chosen were HD 180450 and HD 176670.
χ Cygni is a Mira-type star, Mira itself being an example of such stars. Perrin et
al. [PER 04] propose a laminar model of Mira-type stars, composed of a photosphere,
an empty layer, and a ﬁne molecular layer. The aim of the mission was to obtain
images of χ Cygni in the H band (1.65 μm ± 175 nm) and, in particular, to highlight
possible dissymmetries in the structure of the molecular layer.
Figure 10.14 shows, on the left, the u −v coverage obtained, i.e., the set of spa-
tial frequencies measured, multiplied by the observation wavelength. As the sky is
habitually represented with the west on the right, the coordinates used are, in fact,
−u, v. The domain of the accessible u −v plane is constrained by the geometry of
the interferometer and the position of the star in the sky. The “hour-glass” shape is
characteristic of the IOTA interferometer, and entails non-uniform resolution that af-
fects the image reconstruction, shown on the right. The reconstructed angular ﬁeld has
sides of 30 milliarcseconds. In addition to the positivity constraint, the regularization
used is the white L2L1 criterion described in section 10.2. The interested reader will
ﬁnd an astrophysical interpretation of this result in [LAC 07].
Figure 10.14. Frequency coverage (left) and reconstruction of the star χ Cygni (right)

Inversion in Optical Imaging through Atmospheric Turbulence
277
10.6. Bibliography
[AYE 88] AYERS G. R., DAINTY J. C., “Iterative blind deconvolution and its applications”,
Opt. Lett., vol. 13, p. 547-549, 1988.
[BAK 94] BAKUT P. A., KIRAKOSYANTS V. E., LOGINOV V. A., SOLOMON C. J., DAINTY
J. C., “Optimal wavefront reconstruction from a Shack-Hartmann sensor by use of a
Bayesian algorithm”, Opt. Commun., vol. 109, p. 10-15, June 1994.
[BAL 86] BALDWIN J. E., HANIFF C. A., MACKAY, WARNER P. J., “Closure phase in high-
resolution optical imaging”, Nature (London), vol. 320, p. 595-597, Apr. 1986.
[BAL 96] BALDWIN J. E., BECKETT M. G., BOYSEN R. C., BURNS D., BUSCHER D. F.,
COX G. C., HANIFF C. A., MACKAY C. D., NIGHTINGALE N. S., ROGERS J., SCHEUER
P. A. G., SCOTT T. R., TUTHILL P. G., WARNER P. J., WILSON D. M. A., WILSON
R. W., “The ﬁrst images from an optical aperture synthesis array: mapping of Capella with
COAST at two epochs”, Astron. Astrophys., vol. 306, p. L13+, Feb. 1996.
[BEN 97] BENSON J. A., HUTTER D. J., ELIAS II N. M., BOWERS P. F., JOHNSTON K. J.,
HAJIAN A. R., ARMSTRONG J. T., MOZURKEWICH D., PAULS T. A., RICKARD L. J.,
HUMMEL C. A., WHITE N. M., BLACK D., DENISON C. S., “Multichannel optical aper-
ture synthesis imaging of zeta1 Ursae majoris with the Navy Prototype Optical Interferom-
eter”, Astron. J., vol. 114, p. 1221–1226, Sep. 1997.
[BER 03] BERGER J.-P., HAGUENAUER P., KERN P. Y., ROUSSELET-PERRAUT K., MAL-
BET F., GLUCK S., LAGNY L., SCHANEN-DUPORT I., LAURENT E., DELBOULBE A.,
TATULLI E., TRAUB W. A., CARLETON N., MILLAN-GABET R., MONNIER J. D., PE-
DRETTI E., RAGLAND S., “An integrated-optics 3-way beam combiner for IOTA”,
in
TRAUB W. A. (Ed.), Interferometry for Optical Astronomy II. Proc. SPIE, Vol. 4838, pp.
1099-1106 (2003), p. 1099–1106, Feb. 2003.
[BLA 03] BLANC A., MUGNIER L. M., IDIER J., “Marginal estimation of aberrations and
image restoration by use of phase diversity”, J. Opt. Soc. Am. (A), vol. 20, num. 6, p. 1035–
1045, 2003.
[BOR 93] BORN M., WOLF E., Principles of Optics, Pergamon Press, 6th (corrected) edition,
1993.
[BRE 96] BRETTE S., IDIER J., “Optimized single site update algorithms for image deblur-
ring”, in Proc. IEEE ICIP, Lausanne, Switzerland, p. 65-68, 1996.
[CAS 97] CASSAING F., Analyse d’un instrument à synthèse d’ouverture optique : méthodes
de cophasage et imagerie à haute résolution angulaire, PhD thesis, University of Paris XI,
France, Dec. 1997.
[CHE 07] CHENEGROS G., MUGNIER L. M., LACOMBE F., GLANC M., “3D phase diversity:
a myopic deconvolution method for short-exposure images. Application to retinal imaging”,
J. Opt. Soc. Am. (A), vol. 24, num. 5, p. 1349–1357, May 2007.
[CON 94] CONAN J.-M., Etude de la correction partielle en optique adaptative, PhD thesis,
University of Paris XI, France, Oct. 1994.

278
Bayesian Approach to Inverse Problems
[CON 95] CONAN J.-M., ROUSSET G., MADEC P.-Y., “Wave-front temporal spectra in high-
resolution imaging through turbulence”, J. Opt. Soc. Am. (A), vol. 12, num. 12, p. 1559-
1570, July 1995.
[CON 98a] CONAN J.-M., FUSCO T., MUGNIER L. M., KERSALE E., MICHAU V., “Decon-
volution of adaptive optics images with imprecise knowledge of the point spread function:
results on astronomical objects”, in BONACCINI D. (Ed.), Astronomy with adaptive optics:
present results and future programs, num. 56in ESO Conf. and Workshop Proc., Sonthofen,
Germany, p. 121-132, Sep. 1998.
[CON 98b] CONAN J.-M., MUGNIER L. M., FUSCO T., MICHAU V., ROUSSET G., “My-
opic deconvolution of adaptive optics images using object and point spread function power
spectra”, Appl. Opt., vol. 37, num. 21, p. 4614-4622, July 1998.
[CON 00] CONAN J.-M., FUSCO T., MUGNIER L., MARCHIS F., RODDIER C., F. ROD-
DIER, “Deconvolution of adaptive optics images: from theory to practice”, in WIZINOWICH
P. (Ed.), Adaptive Optical Systems Technology, vol. 4007, Munich, Germany, Proc. Soc.
Photo-Opt. Instrum. Eng., p. 913-924, 2000.
[CON 05] CONAN J.-M., ROUSSET G. (Eds.), Multi-Conjugate Adaptive Optics for Very
Large Telescopes Dossier, vol. 6 fascicule 10 of C. R. Physique, Académie des Sciences,
Elsevier, Paris, Dec. 2005.
[COR 81] CORNWELL T. J., WILKINSON P. N., “A new method for making maps with un-
stable radio interferometers”, Month. Not. Roy. Astr. Soc., vol. 196, p. 1067–1086, 1981.
[DOH 06] DOHLEN K., BEUZIT J.-L., FELDT M., MOUILLET D., PUGET P., ANTICHI J.,
BARUFFOLO A., BAUDOZ P., BERTON A., BOCCALETTI A., CARBILLET M., CHARTON
J., CLAUDI R., DOWNING M., FABRON C., FEAUTRIER P., FEDRIGO E., FUSCO T.,
GACH J.-L., GRATTON R., HUBIN N., KASPER M., LANGLOIS M., LONGMORE A.,
MOUTOU C., PETIT C., PRAGT J., RABOU P., ROUSSET G., SAISSE M., SCHMID H.-
M., STADLER E., STAMM D., TURATTO M., WATERS R., WILDI F., “SPHERE: A planet
ﬁnder instrument for the VLT”, in MCLEAN I. S., IYE M. (Eds.), Ground-based and
Airborne Instrumentation for Astronomy, vol. 6269, Proc. Soc. Photo-Opt. Instrum. Eng.,
2006.
[FON 85] FONTANELLA J.-C., “Analyse de surface d’onde, déconvolution et optique active”,
J. of Optics (Paris), vol. 16, num. 6, p. 257-268, 1985.
[FRI 65] FRIED D. L., “Statistics of a geometric representation of wavefront distortion”, J.
Opt. Soc. Am., vol. 55, num. 11, p. 1427-1435, 1965.
[FUS 99] FUSCO T., VERAN J.-P., CONAN J.-M., MUGNIER L., “Myopic deconvolution
method for adaptive optics images of stellar ﬁelds”, Astron. Astrophys. Suppl., vol. 134,
p. 1-10, Jan. 1999.
[GER 72] GERCHBERG R. W., SAXTON W. O., “A practical algorithm for the determination
of phase from image and diffraction plane pictures”, Optik, vol. 35, p. 237-246, 1972.
[GLA 02] GLANC M., Applications Ophtalmologiques de l’Optique Adaptative, PhD thesis,
University of Paris XI, France, 2002.

Inversion in Optical Imaging through Atmospheric Turbulence
279
[GLA 04] GLANC M., GENDRON E., LACOMBE F., LAFAILLE D., LE GARGASSON J.-
F., P.LÉNA, “Towards wide-ﬁeld retinal imaging with adaptive optics”, Opt. Commun.,
vol. 230, p. 225–238, 2004.
[GON 76] GONSALVES R. A., “Phase retrieval from modulus data”, J. Opt. Soc. Am., vol. 66,
num. 9, p. 961–964, 1976.
[GON 82] GONSALVES R. A., “Phase retrieval and diversity in adaptive optics”, Opt. Eng.,
vol. 21, num. 5, p. 829–832, 1982.
[GOO 68] GOODMAN J. W., Introduction to Fourier Optics, McGraw-Hill, New York, 1968.
[GOO 85] GOODMAN J. W., Statistical Optics, John Wiley, New York, 1985.
[GRA 06] GRATADOUR D., ROUAN D., MUGNIER L. M., FUSCO T., CLÉNET Y., GENDRON
E., LACOMBE F., “Near-IR AO dissection of the core of NGC 1068 with NaCo”, Astron.
Astrophys., vol. 446, num. 3, p. 813–825, Feb. 2006.
[HAN 87] HANIFF C. A., MACKAY C. D., TITTERINGTON D. J., SIVIA D., BALDWIN J. E.,
“The ﬁrst images from optical aperture synthesis”, Nature (London), vol. 328, p. 694-696,
Aug. 1987.
[HIL 04] HILL J. M., SALINARI P., “The Large Binocular Telescope project”, in OSCHMANN
JR. J. M. (Ed.), Ground-based Telescopes. Proc. SPIE, Vol. 5489, pp. 603-614 (2004),
vol. 5489 of Presented at the Society of Photo-Optical Instrumentation Engineers (SPIE)
Conference, p. 603-614, Oct. 2004.
[HOL 92] HOLMES T. J., “Blind deconvolution of speckle images quantum-limited incoherent
imagery: maximum-likehood approach”, J. Opt. Soc. Am. (A), vol. 9, num. 7, p. 1052- 1061,
1992.
[IDI 05] IDIER J., MUGNIER L., BLANC A., “Statistical behavior of joint least square esti-
mation in the phase diversity context”, IEEE Trans. Image Processing, vol. 14, num. 12,
p. 2107–2116, Dec. 2005.
[IOT] http://tdc-www.harvard.edu/IOTA/.
[JEF 93] JEFFERIES S. M., CHRISTOU J. C., “Restoration of astronomical images by iterative
blind deconvolution”, Astrophys. J., vol. 415, p. 862-874, 1993.
[JEN 58] JENNISON R. C., “A phase sensitive interferometer technique for the measurement
of the Fourier transforms of spatial brightness distribution of small angular extent”, Month.
Not. Roy. Astr. Soc., vol. 118, p. 276–284, 1958.
[KEC] http://www.keckobservatory.org/.
[KNO 74] KNOX K. T., THOMPSON B. J., “Recovery of images from atmospherically de-
graded short exposure photographs”, Astrophys. J. Lett., vol. 193, p. L45-L48, 1974.
[KUL 06] KULCSÁR C., RAYNAUD H.-F., PETIT C., CONAN J.-M., VIARIS DE LESEGNO
P., “Optimal control, observers and integrators in adaptive optics”, Opt. Express, vol. 14,
num. 17, p. 7464-7476, 2006.
[LAB 70] LABEYRIE A., “Attainment of diffraction-limited resolution in large telescopes by
Fourier analysing speckle patterns”, Astron. Astrophys., vol. 6, p. 85-87, 1970.

280
Bayesian Approach to Inverse Problems
[LAB 75] LABEYRIE A., “Interference fringes obtained on VEGA with two optical tele-
scopes”, Astrophys. J. Lett., vol. 196, p. L71-L75, Mar. 1975.
[LAC 07] LACOUR S., Imagerie des étoiles évoluées par interférométrie. Réarrangement de
pupille, PhD thesis, University of Paris VI, France, 2007.
[LAN 92] LANE R. G., “Blind deconvolution of speckle images”, J. Opt. Soc. Am. (A), vol. 9,
num. 9, p. 1508-1514, 1992.
[LAN 96] LANE R. G., “Methods for maximum-likelihood deconvolution”, J. Opt. Soc. Am.
(A), vol. 13, num. 10, p. 1992-1998, 1996.
[LAN 98] LANNES A., “Weak-phase imaging in optical interferometry”, J. Opt. Soc. Am. (A),
vol. 15, num. 4, p. 811–824, Apr. 1998.
[LAN 01] LANNES A., “Integer ambiguity resolution in phase closure imaging”, Opt. Soc.
Am. J. A, vol. 18, p. 1046–1055, May 2001.
[LAW 97] LAWSON P. R. (Ed.), Long Baseline Stellar Interferometry, Bellingham, SPIE Op-
tical Engineering Press, 1997.
[LAW 04] LAWSON P. R., COTTON W. D., HUMMEL C. A., MONNIER J. D., ZHAO M.,
YOUNG J. S., THORSTEINSSON H., MEIMON S. C., MUGNIER L., LE BESNERAIS G.,
THIÉBAUT E., TUTHILL P. G., “An interferometric imaging beauty contest”, in TRAUB
W. A. (Ed.), New Frontiers in Stellar Interferometry, vol. 5491, Proc. Soc. Photo-Opt.
Instrum. Eng., p. 886–899, 2004.
[LER 04] LE ROUX B., CONAN J.-M., KULCSÁR C., RAYNAUD H.-F., MUGNIER L. M.,
FUSCO T., “Optimal control law for classical and multiconjugate adaptive optics”, J. Opt.
Soc. Am. (A), vol. 21, num. 7, July 2004.
[MAR 89] MARIOTTI J.-M., “Introduction to Fourier optics and coherence”,
in ALLOIN
D. M., MARIOTTI J.-M. (Eds.), Diffraction-limited Imaging with Very Large Telescopes,
vol. 274 of NATO ASI Series C, p. 3-31, Kluwer Academic, Cargese, France, 1989.
[MEI 05a] MEIMON S., Reconstruction d’images astronomiques en interférométrie optique,
PhD thesis, University of Paris XI, France, 2005.
[MEI 05b] MEIMON S., MUGNIER L. M., LE BESNERAIS G., “Reconstruction method for
weak-phase optical interferometry”, Opt. Lett., vol. 30, num. 14, p. 1809–1811, July 2005.
[MEI 05c] MEIMON S., MUGNIER L. M., LE BESNERAIS G., “A convex approximation of
the likelihood in optical interferometry”, J. Opt. Soc. Am. (A), Nov. 2005.
[MER 88] MERKLE F. (Ed.), High-resolution imaging by interferometry, part II, num. 29in
ESO Conf. and Workshop Proc., Garching bei München, Germany, July 1988.
[MIC 91] MICHELSON A. A., “Measurement of Jupiter’s satellites by interference”, Nature
(London), vol. 45, p. 160-161, Dec. 1891.
[MIC 21] MICHELSON A. A., PEASE F. G., “Measurement of the diameter of alpha Orionis
with the interferometer”, Astrophys. J., vol. 53, p. 249-259, May 1921.
[MON 03] MONNIER J. D., “Optical interferometry in astronomy”, Reports of Progress in
Physics, vol. 66, p. 789-857, May 2003.

Inversion in Optical Imaging through Atmospheric Turbulence
281
[MOU 94] MOURARD D., TALLON-BOSC I., BLAZIT A., BONNEAU D., MERLIN G.,
MORAND F., VAKILI F., LABEYRIE A., “The GI2T interferometer on Plateau de Calern”,
Adv. Appl. Prob., vol. 283, p. 705-713, Mar. 1994.
[MUG 01] MUGNIER L. M., ROBERT C., CONAN J.-M., MICHAU V., SALEM S., “Myopic
deconvolution from wavefront sensing”, J. Opt. Soc. Am. (A), vol. 18, p. 862-872, Apr.
2001.
[MUG 04] MUGNIER L. M., FUSCO T., CONAN J.-M., “MISTRAL: a myopic edge-
preserving image restoration method, with application to astronomical adaptive-optics-
corrected long-exposure images”, J. Opt. Soc. Am. (A), vol. 21, num. 10, p. 1841–1854,
Oct. 2004.
[MUG 06] MUGNIER L. M., BLANC A., IDIER J., “Phase diversity: a technique for wave-
front sensing and for diffraction-limited imaging”, in HAWKES P. (Ed.), Advances in Imag-
ing and Electron Physics, vol. 141, Chapter 1, p. 1–76, Elsevier, 2006.
[MUG 07] MUGNIER L., MEIMON S., WISARD software documentation, Technical report,
ONERA, 2007, European Interferometry Initiative, Joint Research Action 4, 6th Framework
Programme of the EU.
[NOL 76] NOLL R. J., “Zernike polynomials and atmospheric turbulence”, J. Opt. Soc. Am.,
vol. 66, num. 3, p. 207-211, 1976.
[NPO] http://ftp.nofs.navy.mil/projects/npoi/.
[PER 04] PERRIN G., RIDGWAY S., MENNESSON B., COTTON W., WOILLEZ J., VER-
HOELST T., SCHULLER P., COUDÉ DU FORESTO V., TRAUB W., MILLAN-GALBET R.,
LACASSE M., “Unveiling Mira stars behind the molecules. Conﬁrmation of the molecular
layer model with narrow band near-infrared interferometry”, Astron. Astrophys., vol. 426,
p. 279–296, Oct. 2004.
[PER 06] PERRIN G., WOILLEZ J., LAI O., GUÉRIN J., KOTANI T., WIZINOWICH P. L.,
LE MIGNANT D., HRYNEVYCH M., GATHRIGHT J., LÉNA P., CHAFFEE F., VERGNOLE
S., DELAGE L., REYNAUD F., ADAMSON A. J., BERTHOD C., BRIENT B., COLLIN C.,
CRÉTENET J., DAUNY F., DELÉGLISE C., FÉDOU P., GOELTZENLICHTER T., GUYON O.,
HULIN R., MARLOT C., MARTEAUD M., MELSE B.-T., NISHIKAWA J., REESS J.-M.,
RIDGWAY S. T., RIGAUT F., ROTH K., TOKUNAGA A. T., ZIEGLER D., “Interferometric
coupling of the Keck telescopes with single-mode ﬁbers”, Science, vol. 311, p. 194-+, Jan.
2006.
[PET 05] PETIT C., CONAN J.-M., KULCSAR C., RAYNAUD H.-F., FUSCO T., MONTRI J.,
RABAUD D., “Optimal control for multi-conjugate adaptive optics”, C. R. Physique, vol. 6,
num. 10, p. 1059–1069, 2005.
[PRI 88] PRIMOT J., ROUSSET G., FONTANELLA J.-C., “Image deconvolution from wave-
front sensing: atmospheric turbulence simulation cell results”, in Merkle [MER 88], p. 683-
692.
[PRI 90] PRIMOT J., ROUSSET G., FONTANELLA J.-C., “Deconvolution from wavefront
sensing: a new technique for compensating turbulence-degraded images”,
J. Opt. Soc.
Am. (A), vol. 7, num. 9, p. 1598-1608, 1990.

282
Bayesian Approach to Inverse Problems
[REY 83] REY W. J., Introduction to Robust and Quasi-robust Statistical Methods, Springer
Verlag, Berlin, 1983.
[ROD 81] RODDIER F., “The effects of atmospherical turbulence in optical astronomy”, in
WOLF E. (Ed.), Progress in Optics, vol. XIX, p. 281-376, North Holland, Amsterdam,
1981.
[ROD 82] RODDIER F., GILLI J. M., LUND G., “On the origin of speckle boiling and its
effects in stellar speckle interferometry”, J. of Optics (Paris), vol. 13, num. 5, p. 263-271,
1982.
[ROD 88a] RODDIER F., “Curvature sensing and compensation: a new concept in adaptive
optics”, Appl. Opt., vol. 27, num. 7, p. 1223-1225, Apr. 1988.
[ROD 88b] RODDIER F., “Passive versus active methods in optical interferometry”, in Merkle
[MER 88], p. 565-574, July 1988.
[ROD 90] RODDIER N., “Atmospheric wavefront simulation using Zernike polynomials”,
Opt. Eng., vol. 29, num. 10, p. 1174-1180, 1990.
[ROD 98] RODDIER F., RODDIER C., GRAVES J. E., NORTHCOTT M. J., OWEN T., “Nep-
tune cloud structure and activity: Ground based monitoring with adaptive optics”, Icarus,
vol. 136, p. 168-172, 1998.
[ROD 99] RODDIER F. (Ed.), Adaptive Optics in Astronomy, Cambridge University Press,
Cambridge, 1999.
[ROU 90] ROUSSET G., FONTANELLA J.-C., KERN P., GIGAN P., RIGAUT F., LENA P.,
BOYER C., JAGOUREL P., GAFFARD J.-P., MERKLE F., “First diffraction-limited astro-
nomical images with adaptive optics”, Astron. Astrophys., vol. 230, p. 29-32, 1990.
[ROU 99]
[ROU 01] ROUSSET G., MUGNIER L. M., CASSAING F., SORRENTE B., “Imaging with
2, num. 1, p. 17-25, Jan. 2001.
[SAS 85] SASIELA R. J., MOONEY J. G., “An optical phase reconstructor based on using a
multiplier-accumulator approach”, in Proc. Soc. Photo-Opt. Instrum. Eng., vol. 551, Proc.
Soc. Photo-Opt. Instrum. Eng., p. 170-176, 1985.
[SCH 93] SCHULZ T. J., “Multiframe blind deconvolution of astronomical images”, J. Opt.
Soc. Am. (A), vol. 10, num. 5, p. 1064-1073, 1993.
[SHA 71] SHACK R. B., PLACK B. C., “Production and use of a lenticular Hartmann screen
(abstract)”, J. Opt. Soc. Am., vol. 61, p. 656, 1971.
[THI 95]
blind deconvolution”, J. Opt. Soc. Am. (A), vol. 12, num. 3, p. 485-492, 1995.
[THI 03]
microjets”, Astrophys. Space. Sci., vol. 286, p. 171–176, 2003.
[VER 97a] VERAN J.-P., Estimation de la réponse impulsionnelle et restauration d’image
en optique adaptative. Application au système d’optique adaptative du Télescope Canada-
France-Hawaii, PhD thesis, ENST, Paris, France, Nov. 1997.
multi-aperture optical telescopes and an application”, C. R. Acad. Sci. Paris, vol. IV, vo l.
ROUSSET G., “Wave-front sensors”, in Roddier [ROD 99], Chapter 5, p. 91-130.
THIÉBAUT E., GARCIA P. J. V., FOY R., “Imaging with Amber/VLTI: the case of
THIÉBAUT E., CONAN J.-M., “Strict a priori constraints for maximum-likelihood

Inversion in Optical Imaging through Atmospheric Turbulence
283
[VER 97b] VERAN J.-P., RIGAUT F., MAITRE H., ROUAN D., “Estimation of the adaptive
optics long exposure point spread function using control loop data”, J. Opt. Soc. Am. (A),
vol. 14, num. 11, p. 3057-3069, 1997.
[WAL 83] WALLNER E. P., “Optimal wave-front correction using slope measurements”, J.
Opt. Soc. Am. (A), vol. 73, num. 12, p. pp 1771-1776, Dec. 1983.
[WEI 77] WEIGELT G., “Modiﬁed astronomical speckle interferometry ‘speckle masking”’,
Opt. Commun., vol. 21, num. 1, p. 55-59, 1977.


Chapter 11
Spectral Characterization in Ultrasonic
Doppler Velocimetry
11.1. Velocity measurement in medical imaging
This chapter is devoted to velocimetry, i.e., the imaging of the velocity of moving
structures. Velocimetry is employed in atmosphere imaging, industrial control, medi-
cal imaging, etc. In the medical ﬁeld, it is essentially used to characterize blood ﬂow
and heart movements for the purpose of diagnosing cardiovascular pathologies.
These mainly concern stenosis, i.e., narrowing of the arteries connected with the
presence of atheromatous plaque, and its repercussions on the organs supplied by the
affected vessels (heart, brain, etc.). The resulting pathologies may be chronic (silent is-
chaemia) or acute (infarction). The information obtained is morphological: reduction
of the cross-sectional area of the artery at the stenosis, estimation of the dimensions
of the heart (cardiac hypertrophy) and monitoring of its contractions (hypokinesis,
dyskinesia).
Ultrasound (US) and magnetic resonance imaging (MRI) both allow local velocity
images to be acquired and thus give access to speciﬁc information that is complemen-
tary to that provided by morphological and functional imaging. Although the physical
principles underlying these two techniques are radically different, the data measured
and the problems posed show strong similarities. This chapter focuses on Doppler
ultrasound, which is still the most delicate to interpret. We will see that the problems
arising concern spectral characterization:
Chapter written by Jean-François GIOVANNELLI and Alain HERMENT.

286
Bayesian Approach to Inverse Problems
– spectral analysis and its adaptive extension (time-frequency), covered in sec-
tion 11.2;
– tracking of the mean frequencies or spectral moments, dealt with in section 11.3.
As for the data measured, in both cases we have:
– short signals: only 16 to 48 samples to estimate a power spectral density (PSD)
and only 4 to 8 samples to estimate the mean frequency;
– possible violation of Shannon’s condition, hence possible spectral aliasing which
needs to be detected and inverted;
– an unfavorable signal-to-noise ratio (SNR).
The data is globally very poor from an information standpoint. However, we also have
information on the spectral, temporal and spatial coherence of the ﬂow and structures.
The haemodynamic mechanisms (viscosity of the blood) tend to organize the physio-
logical ﬂows. The hypothesis of the space and time variation of velocity in the ﬂow
being gradual is credible. If the ﬂow is very turbulent, local dispersion of the ve-
locities tends to invalidate the hypothesis but, nevertheless, the mean velocities of the
erythrocytes (red corpuscles) remain similar in a plane perpendicular to the vessel axis
expressing the progression of the ﬂow in the vessel.
11.1.1. Principle of velocity measurement in ultrasound imaging
An acoustic wave having a frequency that can vary between 2 and 12 MHz, de-
pending on the desired penetration depth, is emitted in a focused US beam by a probe.
The wave is partly backscattered by the red blood cells and by mobile ﬁbers of the
tissues. Signal ym obtained after time windowing, which enables a given depth to be
isolated (“range bin” m), is thus affected by frequency shifts induced by the velocities
of the backscattering structures. The Doppler shift for a single target having a constant
velocity is:
fd = 2v cos θ
c
fe
(11.1)
where θ represents the angle between the US beam and the velocity, c is the speed of
sound (c = 1,470 m/s) and fe is the US frequency emitted.
11.1.2. Information carried by Doppler signals
The most intuitive way of characterizing the local complexity of a stationary ﬂow
is to represent it by a histogram of the erythrocyte velocities. A laminar ﬂow observed
locally will give a narrow histogram (the red blood cells move in the same direction
at similar speeds) whereas turbulent ﬂow downstream of a stenosis will have a much

Spectral Characterization in Ultrasonic Doppler Velocimetry
287
US beam
Transducer
⃗v
θ
Blood ﬂow
−−−−−−−−−−−−−→
Figure 11.1. Principle of acquisition of a US signal. The transducer emits a US wave then
receives the echoes reﬂected by the vessel walls and blood cells. Signal ym coming from the
“measurement volume” (or “range bin”) indicated in black is selected by means of an
electronic gate synchronized on the emitted signal
wider histogram (the speeds and directions are very different). In medical practice, it
is accepted that the power of the backscattered signal is proportional to the erythrocyte
concentration. Equation (11.1) establishes the proportionality between the erythrocyte
velocity and the frequency of the Doppler signal, so the histogram can be identiﬁed
with the PSD Sm(ν) of the Doppler signal ym.
The Doppler signal is non-stationary: the ﬂow velocity and the movement of the
walls vary rapidly during the cardiac ejection phase. This obliges us to use non-
stationary signal analysis methods. The standard systems use sliding spectrogram or
periodogram techniques. They thus process fairly long signal horizons (128 or 256
samples acquired over a duration of about 10 ms) resulting from a trade-off between
the non-stationary nature of the signal and performance of the stationary periodogram
methods. However, the literature [TAL 88] indicates that much shorter analysis hori-
zons (less than 2 ms, i.e., about 16 samples) are indispensable if ﬂow perturbations are
to be characterized. The problem thus arises of tracking the spectral content of signals
observed on short horizons. This is tackled in section 11.2.
Furthermore, in imaging systems, the spectral information is also reduced to a few
parameters: the standard deviation of the spectrum gives an image of how dispersed
the velocities are, the maximum frequency sometimes lifts ambiguities in the ﬂow
characterization, and the mean (or central) frequency, νm, provides a two-dimensional
plot of local mean velocities. If images are to be constructed at a rate reasonable for
the characterization of ﬂow non-stationarities, the number of data points acquired has
to be reduced yet again, down to eight and sometimes just four samples. We are then
faced with the problem of tracking the mean frequencies of noisy signals observed on
very short horizons.

288
Bayesian Approach to Inverse Problems
Finally, in most systems, the emission is in pulsed mode in order to obtain spatial
discrimination: wave trains are emitted with a recurrence frequency fr that is lim-
ited by the desired image depth d, since an emission is held back until the deepest
echoes of the previous emission have been received. The Doppler signal is thus “sam-
pled” at the frequency fr = c/2d. For high ﬂow velocities (fd > fr/2), a spectral
aliasing phenomenon occurs, which causes ambiguity in the velocity measurements.
At present, the alternatives are either to lower the emission frequency at the cost of
losing spatial resolution, or to reduce the angle between the ultrasound beam and the
ﬂow, at the cost of losing sensitivity. We will see how to invert the aliasing without
relinquishing resolution or accuracy in section 11.3.
11.1.3. Some characteristics and limitations
The orders of magnitude (v varies by a few cm/s or m/s and the walls move at less
than 20 cm/s) show that the Doppler shift fd is of the order of 10−3 to 10−4 times the
emitted frequency. Baseband demodulation thus allows only the useful information to
be kept. The Doppler signal is thus a signal with complex values in a band of about
0 ± 20 kHz.
This signal is the result of backscattering of the acoustic wave by erythrocytes. The
absolute value of the backscattering is low because the erythrocytes are small relative
to the acoustic wavelength and their acoustic impedance is close to that of blood. In
addition, the US wave is strongly attenuated (0.5 to 1 dB/cm/MHz) by the tissues
located between the probe and the ﬂow. Very often, the SNR is in the [0 dB, 15 dB]
range.
The analysis of the spectral parameters should be approached with caution. The
information collected is simply the projection of the actual velocity on the axis of the
US beam, hence the presence of cos θ in (11.1). Any dispersion in the direction of
propagation of the erythrocytes and any modiﬁcation of the incidence of the beam on
the ﬂow will introduce a change in the velocity measurement.
Finally, a number of artefacts are more difﬁcult to analyze: large variation of the
amplitude of the Doppler signal according to the nature of the ﬂow, parasite echoes
from multiple reﬂections on other structures, renewal of the erythrocytes present in
the volume being measured (all the more signiﬁcant at high ﬂow speeds), etc.
11.1.4. Data and problems treated
The data considered in this chapter comes in the form of M complex signals Y =
[y1, . . . , yM] spatially juxtaposed in M “range bins”. Each ym is a vector of N sam-
ples extracted from a signal assumed to be stationary: ym = [ym(1), . . . , ym(N)]T.

Spectral Characterization in Ultrasonic Doppler Velocimetry
289
The data was acquired through apparatus capable of reproducingvarious character-
istics of medical practice but in comfortable study conditions. The measuring system
had a tube supplied with a permanent ﬂow and equipped with a Doppler scanner (AU3
ESAOTE1 – Italy)2. The scanner operated at an emission frequency fe = 8 MHz, a
repetition frequency fr = 10 kHz and an angle of incidence θ = 60o. The apparent
diameter of the vessel was 22.4 mm, and it was divided into M = 64 range bins of
0.35 mm (see Figures 11.1 and 11.2). The system provided signals sampled at the
frequency fr in real time and for each bin simultaneously. They were encoded on 14
bits. The ﬁle processed contained 1.64 s of recording, i.e., N0 = 214 samples in each
distance cell. An extract of this data is shown in Figures 11.2ar and ai.
This experimental set-up ﬁrst allowed us to use all the data in order to obtain
the characteristics of the ﬂow. As the ﬂow was permanent, the signal recorded for
each range bin was a stationary signal of N0 samples. Dividing it into 64 signals
of 256 points each enabled a mean periodogram to be calculated with a reasonable
compromise between bias and variance. It could thus be assumed, at least as a ﬁrst
approximation, that this was the “true” sequence of the PSDs. It is represented in
Figure 11.2b and will serve as an element of comparison for the rest of the chapter.
Figure 11.2c shows the series of frequencies that maximize each of the periodograms.
It will also serve as an element of comparison for the rest of the chapter.
1
2
4
6
8
10
20
30
40
50
60
1
2
4
6
8
10
20
30
40
50
60
0
0.2
0.4
0.6
0.8
1
10
20
30
40
50
60
0
0.2
0.4
0.6
0.8
1
60
50
40
30
20
10
(ar)
(ai)
(b)
(c)
Figure 11.2. Figures ar and ai give the real and imaginary parts respectively of eight samples
in each bin. Figures b and c show the averaged periodograms and the mean frequency
sequence respectively (parabolic ﬂow). In the four ﬁgures, the vertical axis represents depth
(m = 1 to m = M = 64). The horizontal axis is time (1 to 8) for ar and ai,
and frequency (0 to 1) for b and c
In clinical practice, the ﬂows are obviously not permanent and are even strongly
non-stationary sometimes. As explained in section 11.1.2, we chose to process signals
of N = 16 samples in order to assess the capacities of the methods to solve spectral
1. BIOMED 2, contract no. BMH4-CT98-3782 (DG 12 - SSMI).
2. We thank P. Tortoli of the University of Florence for the acquisition of the Doppler signals.

290
Bayesian Approach to Inverse Problems
characterization problems in conditions that were both realistic and difﬁcult. We chose
the two principle problems of this type:
(α) adaptive spectral analysis, which consists of estimating the sequence of PSD
Sm(ν) of the ym. We will tackle this using long AR models in section 11.2;
(β) frequency tracking and inversion of the spectral aliasing, which means estimat-
ing the series of frequencies νm of signals ym. This is covered in section 11.3 on the
basis of pure frequency models.
In both cases, a Gaussian Markov chain is introduced for the AR parameters (α)
and the frequencies (β) in order to take spatial continuity into account. The criteria
thus constructed are optimized by suitable algorithms: a Kalman smoother (α) and a
Viterbi algorithm (β). The question of the hyperparameters is settled by maximum
likelihood in both cases. The likelihood is optimized by a coordinate-wise descent
algorithm (α) and a gradient algorithm (β). Each of the two sections ends with char-
acteristic results3 for the signals described above.
11.2. Adaptive spectral analysis
The literature on adaptive spectral analysis contains several possible approaches:
periodograms and spectrograms (and their variants), ARMA methods, AR by least
squares (LS) and their adaptive extensions with sliding windows or forgetting coefﬁ-
cients, Wigner-Ville methods, etc. See [GUO 94, HER 97] for a broad comparative
study in the US velocimetry context, the conclusion of which recommends parametric
AR methods by LS. For this reason, we will turn directly to this class of methods.
In the AR spectral analysis framework, ﬁnding the sequence of PSD of signals
ym requires estimation of the AR parameters am = [amp], where m is the index of
the range bin under consideration m ∈{1, . . . , M} =
∗
M and p is the order of
coefﬁcient (p ∈
∗
P ). Let A = [a1, . . . , aM] be the regressor sequence and let re
m
and rm be the powers of the input noises and the corresponding signals.
11.2.1. Least squares and traditional extensions
In one of the M range bins, estimation of the regressor am from the corresponding
data ym by LS relies on a prediction error em = ym −Ymam and the criterion:
Q
LS
m(am) = e†
mem = (ym −Ymam)†(ym −Ymam) .
(11.2)
3. The algorithms described here have been implemented in Matlab on a Pentium III PC, run-
ning at 450 MHz and equipped with 128 MB of RAM.

Spectral Characterization in Ultrasonic Doppler Velocimetry
291
Vector ym (of size L × 1) and matrix Ym (of size L × P −1) are deﬁned according
to the type of windowing [KAY 81, equation (2)], [MAR 87, p. 217]. There are
four types: non-windowed (called covariance), pre-windowed, post-windowed and
double windowed or pre- and post-windowed (also called autocorrelation). Depending
on the case, L = N −P, L = N, L = N + P. This choice is important as it
strongly inﬂuences the conditioning of the normal matrix and the spectral resolution,
particularly when the number of data points is small [MAR 87, p. 225]. Whatever the
type of windowing, minimizing the LS criterion (11.2) leads to [SOR 80]:
a
LS
m = arg min
am
Q
LS
m(am) = (Y †
mYm)−1Y †
mym .
(11.3)
For the set of M windows, adaptive LS techniques (ALS) work by taking account
of the criteria (11.2) around the current bin in a window of variable width or by geo-
metrical weighting4.
There are a number of drawbacks to these algorithms, connected with the choice
of the parameters, at least in our context:
– The LS methods can only be used in combination with a principle of parsimony
that limits the order of the model [AZE 86] and thus avoids parasite peaks in the
spectrum5. This compromise can be found automatically through criteria such as
FPE [AKA 70], AIC [AKA 74], CAT [PAR 74] or MDL [RIS 78], but they become
inefﬁcient when there is not enough data [ULR 76].
– Even if the order of the model is adjusted empirically, from a small amount of
data, the acceptable orders remain too low to describe the great variety of spectra
that can be encountered in velocimetry. We will see that the method presented allows
high-order models to be estimated.
– From a spatial point of view, in the ALS method framework, no automatic
method for adjusting the trade-off exists, either for estimating the width of the window
or for the forgetting coefﬁcient.
To ﬁnd a remedy for these disadvantages, we will look into regularized techniques
in the next subsection.
11.2.2. Long AR models – spectral smoothness – spatial continuity
11.2.2.1. Spatial regularity
The idea is to restate the problem including the notions of spatial regularity and
spectral smoothness within the criterion itself. To do this, we generalize the seminal
4. This process directly brings an idea of spatial regularity into the sequence of spectra.
5. This limitation of the order is a roundabout way of inducing spectral smoothness.

292
Bayesian Approach to Inverse Problems
work of Kitagawa and Gersch [KIT 85] to construct a measure of the distance between
two AR spectra. Starting from the expression for the PSD:
Sm(ν) =
re
m
|1 −Am(ν)|2 with Am(ν) =
P

p=1
ampe2jπνp ,
we measure the spectral distance between Sm and Sm′
m
m′:
Dk(m, m′) =
 1
0

dk
dνk

Am(ν) −Am′(ν)

2
dν .
It can be easily shown that a quadratic form can be obtained:
Dk(m, m′) = (am −am′)†Δk(am −am′) ,
(11.4)
where Δk is a simply diagonal matrix Δk = diag

12k, 22k, . . . , P 2k
called the
smoothness matrix of order k.
11.2.2.2. Spectral smoothness
m′
proposed by Kitagawa and Gersch:
Dk(m) ∝a†
mΔkam .
(11.5)
Note 1 Strictly speaking, this is not a measure of spectral distance or spectral smooth-
ness, since the Dks are not built from the PSD Sm but from functions Am. Neverthe-
less, it does measure the spatial regularity and spectral smoothness in a certain sense.
Furthermore, its quadratic nature greatly simpliﬁes the optimization with respect to
the ams (section 11.2.2.4) and the question of estimating the hyperparameters (sec-
tion 11.2.4).
11.2.2.3. Regularized least squares
Starting from the two expressions (11.4)-(11.5) and the LS criteria (11.2), we con-
struct the regularized LS criterion (LSReg), as in (2.5) of Chapter 2:
Q
Reg(A) =
M

m=1
1
rem
(ym −Ymam)†(ym −Ymam)
(11.6)
+ 1
rs
M

m=1
a†
mΔkam + 1
rd
M−1

m=1
(am −am+1)†Δk(am −am+1)
of order k between functions A
and A
by using the Sobolev distance
distance
to
need
we just
smoothness,
spectral
the
To measure
the
= 0, which again gives a quadratic form, initially
to a constant spectrum, i.e., A
measure

Spectral Characterization in Ultrasonic Doppler Velocimetry
293
which has three terms: the ﬁrst measures ﬁdelity to the data, the second the spectral
smoothness of each spectrum and the third the spatial regularity. The relative weight
of each term is determined by the powers of the input noises re
m and, above all, the
spectral parameter (rs) and spatial parameter (rd). We will also use λs = 1/rs and
λd = 1/rd, so that the constraints increase with the parameters.
11.2.2.4. Optimization
Given its quadratic structure, several competing options can be considered for the
minimization of (11.6); see Chapter 2, section 2.2.2, which is devoted to this question.
It is possible to obtain the minimum explicitly by solving a sparse linear system of
dimension MP × MP. The criterion in question is convex and differentiable, so
gradient techniques are also a possibility [BER 95]. However, with a view to carrying
out the processing on-line, we opted for Kalman ﬁltering (KF) and Kalman smoothing
(KS), which was also Kitagawa and Gersch’s initial point of view in [KIT 85]. See
Chapter 4, a part of which covers KF and KS.
11.2.3. Kalman smoothing
11.2.3.1. State and observation equations
To use this alternative, it is necessary to express the model in a state representation
form. We will not return to the more general Kalman formulation and it will be seen
that the following form is sufﬁcient for optimization purposes.
The evolution of the successive regressors am is guided by the state model:
am+1 = αmam + εm ,
(11.7)
where each εm is a zero-mean, circular, complex vector with covariance matrix P ε
m =
rε
mΔ−1
k , and sequence εm, for m ∈
∗
M, is spatially white. This is a generalized
version of the one proposed by Kitagawa and Gersch [KIT 85].
The state model also brings in the mean and the initial covariance of the state: the
zero vector and P a = raΔ−1
k
respectively.
The observation equation is simply the recurrence equation that guides the AR
model in each range bin, in matrix form:
ym = Ymam + em .
(11.8)
Each em is a zero-mean, circular, complex vector of covariance re
mIL. The sequence
em, m ∈
∗
M, is also spatially white. This, too, is a generalization of the form
proposed in [KIT 85].

294
Bayesian Approach to Inverse Problems
11.2.3.2. Equivalence between parameterizations
To implement the KS, the equations for which are given in Chapter 4, section 4.5.1,
it is necessary to determine its parameters (ra and αm, rε
m for m ∈
∗
M−1). They
are determined according to rd, rs so that the associated KS effectively minimizes
criterion (11.6). [JAZ 70, p. 150] gives the criterion minimized by the KS associated
with (11.7)-(11.8):
Q
KS(A) =
M

m=1
1
rem
(ym −Ymam)†(ym −Ymam)
+
M−1

m=1
1
rεm
(am+1 −αmam)†Δk(am+1 −αmam) + 1
ra a†
1Δka1 . (11.9)
By developing and identifying (11.6) and (11.9), we establish the link between the
two sets of parameters in the form of a descending recurrence:
1 initialization (m = M −1):
αM−1 = (1 + ρ)−1
and rε
M−1 = rdαM−1;
2 recursion (m = M −2, . . . , 1):
αm = (2 + ρ −αm+1)−1
and rε
m = rdαm;
3 the last step gives the initial power:
ra = rd(1 + ρ −α1)−1
with ρ = rd/rs > 0 .
These equations allow the coefﬁcients of the KS (ra and the αm, rε
m) to be calculated
in advance according to rd, rs so as to minimize (11.6).
Note 2 It can be shown [GIO 01] that the above system admits a stationary limit and
that the corresponding “stationary” criterion differs from the homogeneous criterion
(11.6) only by the two terms connected with the ﬁrst and last regressors, which are
both proportional to α(α −1)/rε. The simpler, stationary form is thus most often
used in practice.
11.2.4. Estimation of hyperparameters
The method has M + 4 hyperparameters: k, the order of smoothness; P, the order
of the AR model; re
m for m ∈
∗
M, the sequence of the prediction errors, λs for
spectral continuity and λd for spatial continuity.

Spectral Characterization in Ultrasonic Doppler Velocimetry
295
The problem of estimating the hyperparameters is one of the questions that have
not been completely resolved yet. See Chapter 8, which is devoted to this question.
The approach chosen here is to maximize the likelihood (ML) of the hyperparameters.
The quadratic character of criterion (11.6) allows it to have a Bayesian interpretation
in Gaussian terms and enables the observation law to be explicitly deduced given the
hyperparameters p(y1, . . . , yM ; k, P, re
1, re
2, . . . , re
M, λs, λd) (see Chapter 3). In this
context, a completely satisfactory approach would be to maximize the likelihood with
respect to the M + 4 hyperparameters. For the sake of computational efﬁciency, the
likelihood is only maximized with respect to λs and λd.
The order parameters are ﬁxed independently of the data. The order P of the AR
model no longer inﬂuences the form of the spectra if it is chosen large enough, e.g.
P > N/2. This is why it is kept at its maximum value P = N −1 in practice (hence
the expression “long AR”). The smoothness order also has little inﬂuence on the form
of the spectra obtained provided that it is chosen to be non-zero. In practice, it is set
at k = 1.
Parameters re
m for the power of the input noises play the role of weighting the
data of each range bin in the criterion for LSReg (11.6). In practice, they are replaced
by the powers of signals rm in order to simplify the estimation procedure. These
parameters can be estimated independently from the data by the standard empirical
estimator of the power rm = y†
mym/N. Practically, they have only a weak inﬂuence
on the shape of the PSD.
We now come to the two parameters that principally inﬂuence the shape of the
depth-frequency map: λs and λd, which are both set automatically by ML. We know
how to calculate the likelihood from the sub-products of the KF. Up to some constants,
the co-log-likelihood (CLL) reads:
CLL(λs, λd) =
M

m=1
log |Rm| + e†
mR−1
m em ,
which is to be minimized with respect to (λs, λd). This calculation requires two sub-
products of the KF (Rm and em) and, in particular, the inversion and calculation of
the determinant of Rm. This is a square matrix of size L that varies from 1 to 2N −1
depending on the type of windowing chosen. In practice, this matrix remains small
and it does not appear crucial to use speciﬁc inversion algorithms.
As far as the minimization of the CLL is concerned, several approaches are in
competition but none of them can guarantee that the global minimum will be obtained.
This aspect is speciﬁcally covered in Chapter 8. The model used is a coordinate-wise
descent method with golden section directional search [BER 95].

296
Bayesian Approach to Inverse Problems
11.2.5. Processing results and comparisons
This subsection is devoted to processing results based on data presented in sec-
tion 11.1.4. The method described above is compared to the periodogram method
(used in commercial systems). In particular, we have excluded the LS methods as
their order selecting methods are not reliable, as explained earlier.
11.2.5.1. Hyperparameter tuning
The two important parameters (λs, λd) were adjusted automatically by ML. First,
the CLL was calculated on a logarithmic grid of 100 × 100 values of (λs, λd). The
corresponding contours, given in Figure 11.3, are pretty regular and have a single,
clearly marked minimum at λs = −1.26 and λd = 2.14 (on a log10 scale). These
values can be obtained in practice in only 2.35 s using the descent algorithm described
above. Some typical trajectories are also shown in Figure 11.3.
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
−1.8
−1.6
−1.4
−1.2
−1
−0.8
−0.6
−0.4
−0.2
Figure 11.3. CLL(λs, λd) contours to be minimized in (λs, λd). The minimum is indicated by
a star (∗). The spectral parameter λs is shown on the vertical axis and the spatial parameter
λd on the horizontal axis (log10 scale in both cases). The ﬁgure also shows the trajectories of
the optimization algorithm for three different initializations
Note 3 It is noteworthy that, for variations of less than a decade in the hyperparam-
eters, the change in the PSD map is very slight. This aspect is particularly important
for qualifying the overall robustness of the method. Unlike the AR model order in
standard LS methods, which can have an abrupt effect on the shape of the spectra, the
choice of (λs, λd) offers more ﬂexibility for automatic or handmade adjustments.
11.2.5.2. Qualitative comparison
As far as the spectra themselves are concerned, the typical results obtained by peri-
odogam and the method presented here are shown in Figure 11.4. A simple qualitative

Spectral Characterization in Ultrasonic Doppler Velocimetry
297
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10
20
30
40
50
60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10
20
30
40
50
60
Figure 11.4. Depth-frequency contours obtained, left, by standard periodogram; right, by the
method presented (with hyperparameters by ML)
comparison with the reference of Figure 11.2 allows several conclusions to be drawn.
First, the gain relative to the periodogram is obvious: the spatial regularity (in the ver-
tical depth direction) clearly improves the series of spectra that are in conformity with
the reference of Figure 11.2. Secondly, the frequency dynamics is respected: from 0.2
“at the edge” of the vessel to 0.6 at the center. Third, the spectral resolution is also
increased and ﬁts the reference better.
11.3. Tracking spectral moments
We now come to the second problem, that of frequency tracking, possibly beyond
Shannon’s frequency limit. The signals to be analyzed are the same as before but the
number of data points available per range bin is generally lower, e.g. N = 8, for the
reasons given in section 11.1.2. These same signals are now modeled by a pure sine
wave drowned in noise. Using notations am and νm for the complex amplitude and
the frequency, and bm for the measuring or modeling noise:
ym = am z(νm) + bm
with z(νm) = [1, ej2πνm, . . . , ej2πνm(N−1)]T
(11.10)
This model is well known in spectral analysis but gives rise to two observations: ﬁrst,
it is clearly periodic with respect to νm. In one sense, this characteristic is the corner-
stone of the problem discussed here: it expresses the spectral aliasing and constitutes
am, it is not linear with respect to the frequency νm: the problem to be dealt with is
therefore nonlinear.
Let us also note the vectors of the frequencies and amplitudes as ν
=
[ν1, . . . , νM]T and a = [a1, . . . , aM]T . Finally, the “true parameters” are noted
the key to its inversion. Secondly, although it is linear with respect to the amplitude

298
Bayesian Approach to Inverse Problems
with a star: ν⋆, a⋆, etc. What we need to do is to construct an estimator ν for the pa-
rameters of interest ν⋆. The a⋆parameters are less interesting and are called nuisance
parameters). The method described is Bayesian and is based on several elements:
– marginalization of the nuisance parameters;
– modeling of the sequence of frequences ν by a Markov chain so as to take the
spatial continuity into account;
– choice of a chain of discrete states allowing speciﬁc algorithms to be used (for
the calculation of both the solution ν and the hyperparameters).
11.3.1. Proposed method
11.3.1.1. Likelihood
With the hypothesis that the bm are Gaussian, zero-mean, white, uniform, of vari-
ance rb and spatially independent, it is easy to construct the likelihood of the set of
frequency and amplitude parameters as the product:
p(Y | ν, a) =
M
1
m=1
p(ym | νm, am) = (πrb)−NM exp {−CLL(ν, a)/rb}
(11.11)
where the CLL has the form of an LS criterion:
CLL(ν, a) =
M

m=1
(ym −amz(νm))† (ym −amz(νm))
11.3.1.2. Amplitudes: prior distribution and marginalization
Deﬁning the law for parameters ν and a necessitates the construction of a joint
law in (ν, a). The absence of information concerning the links between amplitudes
and frequencies naturally leads to a separable choice:
p(ν, a) = p(ν) p(a) .
(11.12)
For the amplitudes, a separable choice is also dictated by the absence of informa-
tion on a possible interdependence between range bins. As far as the form of the law
is concerned, since the marginalization of the amplitudes is to be carried out, we resort
to a Gaussian law:
p(a) = (πra)−M exp

−a†a/ra

.
(11.13)
Given the separability of likelihood (11.11) and prior law (11.13), the marginal-
ization of the amplitudes yields:
p(Y | ν) =
M
1
m=1

am
p(ym | νm, am)p(am) dam =
M
1
m=1
p(ym | νm) .
(11.14)

Spectral Characterization in Ultrasonic Doppler Velocimetry
299
In addition, since (11.10) is linear with respect to am and bm and since am and bm
are Gaussian and independent; the law for (ym | νm) is also Gaussian, zero-mean and
of covariance:
Rm = E

ymy†
m

= raz(νm)z(νm)† + rbIN .
The expression for the law of (ym | νm) obviously brings in its determinant and its
inverse, which can be written explicitly in the form:
R−1
m = r−1
b IN −Nαz(νm)z(νm)†
and
|Rm| = rN−1
b
(rb + Nra) ,
with α = Nra/(rb(Nra + rb)). The complete law can thus be written:
p(ym | νm) = π−N R−1
m
 exp

−y†
mR−1
m ym

(11.15)
= β exp {−γm + NαPm(νm)}
(11.16)
where Pm is the periodogram of signal ym at frequency νm:
Pm(νm) =

z(νm)†ym
† 
z(νm)†ym

= 1
N

N

n=1
ym(n)e2jπνmn

2
,
and
β = π−Nr1−N
b
/(Nra + rb),
γm = y†
mym/rb.
Finally, the joint law for the set of observations, given the frequencies, can be
written as product (11.14):
p(Y | ν) = βM exp {−γ} exp {−α CLML(ν)} with γ =
M

m=1
γm
(11.17)
where the co-log-marginal-likelihood (CLML) CLML(ν) is the opposite of the sum
of the periodograms in each range bin.
CLML(ν) = −
M

m=1
Pm(νm)
(11.18)
Note 4 The fundamental property of this function, mentioned in the introduction, is its
1-periodic character with respect to each of the variables: ∀km ∈
, m = 1, . . . , M:
CLML(ν1, ν2, . . . , νm) = CLML(ν1 + k1, ν2 + k2, . . . , νm + km)
(11.19)
In consequence, the information contributed by the data leaves the set of frequencies
undetermined.

300
Bayesian Approach to Inverse Problems
11.3.1.3. Frequencies: prior law and posterior law
As announced in the introduction, the frequency law is built with discrete states.
We postulate a minimum value νm and a maximum value νM for the frequencies and
regularly discretize the interval [νm, νM] on an arbitrarily ﬁne grid of P values. The
possible values for the frequency are denoted νp, for p ∈
P .
In contrast to the amplitude law, the law chosen for the frequencies takes the idea
of spatial continuity into account through a Markov chain associated with a quadratic
Gibbs energy:
CLP(ν) =
M−1

m=1
(νm+1 −νm)2 ,
(11.20)
where CLP is used for Co-Log-Prior. We obtain the probabilities of transitions be-
tween the states of the chain by “discretizing and renormalizing” a Gaussian law of
variance rν:
m(p, q) = Pr(νm+1 = νp | νm = νq) =
exp

−(νp −νq)2/2rν

P
p′=1 exp {−(νp′ −νq)2/2rν}
(11.21)
which is independent of m. A uniform initial probability is attributed to the frequen-
cies, i.e.,
(p) = Pr(ν1 = νp) = 1/P .
Finally, in what follows, the “probability6 of observations” will be denoted
m(p) =
p(ym | νm = νp).
The prior information and the information provided by the data is merged using
Bayes’ rule, which gives the posterior law for ν:
p(ν | Y) ∝p(Y | ν) p(ν) ∝exp {−α CLPL(ν)}
where the Co-Log-Posterior-Likelihood (CLPL) can be written in the form:
CLPL(ν) = −
M

m=1
Pm(νm) + λ
M−1

m=1
(νm+1 −νm)2
(11.22)
with λ = 1/2αrν. In a deterministic framework, the CLPL is a regularized LS crite-
rion. It contains two terms that measure the conﬁdence we have in the measurements
and in the a priori idea of spatial regularity respectively. The regularization parameter
λ (which depends on the hyperparameters r = [ra, rb, rν]) adjusts the compromise
between the two.
6. Strictly speaking, this is a density and not a probability but we will nevertheless use this
notation as it is the usual one in the literature on Markov chains.

Spectral Characterization in Ultrasonic Doppler Velocimetry
301
−5
−4
−3
−2
−1
0
1
2
3
4
5
−1960
−1940
−1920
−1900
−1880
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
20
40
60
−5
−4
−3
−2
−1
0
1
2
3
4
5
−1950
−1900
−1850
−1800
−1750
Figure 11.5. Typical form of the criteria as a function of one of the νm (m = 50). From top to
bottom: CLML(ν) (1-periodic), CLP(ν) (quadratic) and their sum CLPL(ν). The
regularization breaks the periodicity and removes the indeterminations
Note 5 This is the “regularized counterpart” of note 4. While the CLML is 1-periodic
in all directions νm, m = 1, . . . , M, i.e., it veriﬁes (11.19), the CLPL does not have
this property. The regularization term removes the indeterminations.
However, a global indetermination remains. CLML(ν) is 1-periodic with respect
to each νm and the regularization term only depends on the successive differences
between frequencies, so it is globally insensitive to a constant level. We then have
∀k0 ∈
,
CLPL([ν1, ν2, . . . , νm]) = CLPL([ν1 + k0, ν2 + k0, . . . , νm + k0]) .
Two frequency proﬁles differing by a constant integer level remain equiprobable a
posteriori. This indetermination is removed by specifying that the ﬁrst frequency is
within [−1/2 , +1/2).
All that remains is to choose a punctual estimator ν for ν⋆. Our ﬁrst choice was
the maximum a posteriori (MAP), i.e., the minimizer of regularized criterion (11.22):
ν
MAP = arg max
ν
p(ν | Y) = arg min
ν
CLPL(ν) .
(11.23)
There are possible alternatives: the marginal maximum a posteriori (MMAP), the
mean a posteriori, etc. They are not presented here.

302
Bayesian Approach to Inverse Problems
11.3.1.4. Viterbi algorithm
The Viterbi algorithm gives the MAP (11.23) at low computing cost. It is a tradi-
tional dynamic programming algorithm that exactly minimizes criterion (11.22) step
by step to obtain (11.23). Its principle is to minimize the criterion with respect to
νM (depending on νM−1), then with respect to νM−1 (depending on νM−2) and
so on to gradually approach the desired minimization. For a detailed description,
see [FOR 73, RAB 86]. We will simply note here that the observation probability
structure
m(p) given by (11.18) and (11.17) allows all the probabilities to be calcu-
lated by M fast Fourier transform calculations on P points.
11.3.2. Likelihood of the hyperparameters
The estimation of the hyperparameters r = [ra, rb, rν] by maximum likelihood
makes use of the fact that the frequencies ν are probabilized to marginalize the joint
density in ν and Y with respect to ν so as to obtain the likelihood of the parameters
attached to the data:
HLY(r) = Pr(Y ; r) =

ν
Pr(Y, ν)
(11.24)
=
P

p1=1
. . .
P

pM =1
Pr(Y, ν1 = νp1, . . . , νM = νpM ) .
In general, we work with the opposite of the logarithm of the likelihood, denoted
HCLL for Hyperparameters-Co-Log-Likelihood:
HCLLY(r) = −log HLY(r),
which is minimized with respect to the vector of the hyperparameters r:
r
ML = arg min
r
HCLLY(r).
The known properties of HCLLY(r) do not allow a global optimization. A ﬁrst ap-
proach could be to use a coordinate-wise descent algorithm as in the case of adaptive
spectral analysis (section 11.2). Here, we have chosen to implement gradient descent
techniques. The two following sections are devoted to the calculation of HCLLY(r)
and its gradient.
11.3.2.1. Forward-Backward algorithm
The sum in (11.24) extends over P M states of the chain and cannot be calculated in
practice. However, the “Forward” algorithm presented in [FOR 73, RAB 86] performs
the step-wise marginalization of the joint law and gives access to the likelihood for a
relatively low computing cost.

Spectral Characterization in Ultrasonic Doppler Velocimetry
303
This algorithm, in its normalized form (recommended by [DEV 85] for numerical
stability reasons), is based on the probabilities:
Fm(p) = Pr(Ym
1 , νm = νp)
Pr(Ym
1 )
and Bm(p) = Pr

YM
m+1 | νm = νp
Pr

YM
m+1 | Ym
1

,
the partial observations at instants m to m′ being denoted: Ym′
m
= [ym, . . . , ym′].
The likelihood can be deduced from the sub-products of the “Forward” phase. The
Forward-Backward algorithm also gives the posterior marginal probabilities of the
states of the Markov chain, given an observation sequence and the model parameters
( ,
 and
), (which would allow the MMAP to be determined):
pm(p) = Pr(νm = νp | Y) = Fm(p) Bm(p)
(11.25)
together with the double marginal posterior which will be useful for calculating the
gradient of the likelihood [LEV 83]:
pm(im−1, im) = Pr

νm−1 = νim−1, νm = νim | Y

= Nm Fm−1(p) Bm(q)
(p, q)
m(q) .
(11.26)
11.3.2.2. Likelihood gradient
The gradient calculation is based on the properties of the auxiliary function (gener-
ally denoted Q) of the EM (Expectation Maximization) algorithm [BAU 70, LIP 82].
It is built on two sets of hyperparameters r and r′, by “completing” data Y by object
ν to be marginalized:
Q(r, r′) = Eν

log Pr(ν, Y ; r′)
 Y ; r

=

ν
log Pr(ν, Y ; r′) Pr(ν | Y ; r) .
Here, we obtain the following expression for Q:
Q(r, r′) =
M

m=2
P

im−1=1
P

im=1
pm(im−1, im) log
′(im−1, im)
+
P

p=1
(p) log
′(p) +
M

m=1
P

im=1
pm(im) log
′
m(im)
(11.27)
– (′,
′,
′) and (,
,
) are the characteristics of the model for hyperparam-
eters r′ and r, respectively;
– pm(im) and pm(im−1, im) are the posterior marginal laws deﬁned by (11.25)
and (11.26) for hyperparameters r.

304
Bayesian Approach to Inverse Problems
The standard estimation strategy using the EM algorithm does not apply directly
here as the chain is not parametrized by its natural parameters (,
), but by the
hyperparameters r. Function Q is still of great interest, however, because it possesses
the following property:
∂Q(r, r′)
∂r′

r′=r = −∂HCLLY(r)
∂r
.
This property gives the gradient of HCLLY(r) when (11.27) is derived:
∂Q
∂r′a
=
M

m=1
P

im=1
pm(im)∂log
′
m(im)
∂r′a
(11.28)
∂Q
∂r′
b
=
M

m=1
P

im=1
pm(im)∂log
′
m(im)
∂r′
b
(11.29)
∂Q
∂r′ν
=
M

m=2
P

im−1=1
P

im=1
pm(im−1, im)∂log
′(im−1, im)
∂r′ν
.
(11.30)
Derivatives
′ and
′ are obtained by deriving (11.15) and (11.21) respectively:
∂log
′
∂ra
= −
N
Nra + rb
−
Nrb
(Nra + rb)2 Pm(νi
m)
∂log
′
∂rb
= −N −1
rb
−
1
Nra + rb
+ 1
r2
b
y†
mym +
Nra
(Nra + rb)2 Pm(νi
m)
∂log
′
∂rν
=
1
2r2ν

(νim −νim−1)2 −
P

q=1
(νq −νim−1)2
(im−1, q)

.
11.3.3. Processing results and comparisons
This subsection presents some typical results. As in the case of AR adaptive spec-
tral analysis, the section is divided into two parts: ﬁrst, the estimation of the hyper-
parameters, then the reconstruction of the frequencies. The signals processed are the
same as in the adaptive spectral analysis except for the fact that we have only kept one
sample in two to “simulate” a real spectral aliasing situation.
11.3.3.1. Tuning the hyperparameters
First of all, the hyperparameter likelihood HCLL was calculated on a grid of
25 × 25 × 25 values and the likelihood contours are given in Figure 11.6. Like that
in Figure 11.3, the function is regular and has a single, clearly marked minimum:
rML
a
= 0.292, rML
b
= −0.700 and rML
ν
= −2.583 (on a logarithmic scale). It is also

Spectral Characterization in Ultrasonic Doppler Velocimetry
305
worth noting that a variation of 1/10 (still on the logarithmic scale) produces an im-
perceptible modiﬁcation in the corresponding frequency proﬁle, which demonstrates
a certain robustness of the method.
−3
−2.5
−2
−1.5
−1
−0.5
−0.5
−0.4
−0.3
−0.2
−0.1
−3
−2.5
−2
−1.5
−1
−0.5
−1
−0.5
0
0.5
1
−0.5
−0.4
−0.3
−0.2
−0.1
−1
−0.5
0
0.5
1
Figure 11.6. Likelihood of the hyperparameters: typical behavior. HCLL contour lines ( ),
minimum (∗), initializations (·), trajectory of the descent algorithm (---) and minimum reached
(◦). All the ﬁgures are on a logarithmic scale and from left to right: on the horizontal axis rν,
rν, rb, and on the vertical axis rb, ra, ra
As far as the optimization itself is concerned, several descent directions were
compared: usual gradient, Vignes’ correction, bisector correction, and the pseudo-
conjugate direction of Polak-Ribière [BER 95]. Our investigations show that, as was
to be expected, the direction of the gradient generates zig-zag trajectories in the pa-
rameter space, whereas this is not so for the three corrected directions. These three
corrections allow a 25% to 40% gain in calculation time relative to the direction of
the gradient, with a clear advantage for the Polak-Ribière pseudo-conjugate direction.
Three line search techniques were also compared: dichotomic search, and quadratic
and cubic interpolation. The ﬁrst was the fastest. Finally, about 3 s sufﬁced for the op-
timum to be reached. The convergence of the algorithm is illustrated in Figure 11.6.
11.3.3.2. Qualitative comparison
Figure 11.7 compares typical results. Figure 11.7a was obtained by maximum like-
lihood, i.e., by choosing the frequency that maximized each of the 64 periodograms
(on ν ∈[0, 1]). By construction, this solution obviously does not allow frequencies to
be tracked outside ν ∈[0, 1]. It is particularly irregular, and an algorithm progressing
in the standard way (by gradual approximation) cannot track the frequencies beyond
ν = 1, as Figure 11.7b shows. In contrast, the solution shown in Figure 11.7c is much
more satisfactory:
– the effect of the regularization is clear. The solution is more regular and incon-
testably closer to the reference of Figure 11.2;
– even beyond the limit frequency ν = 1, the MAP tracks the true frequencies
correctly;

306
Bayesian Approach to Inverse Problems
0
0.5
1
1.5
2
60
50
40
30
20
10
(a) ML (i.e., periodogram
maximizer)
0
0.5
1
1.5
2
60
50
40
30
20
10
(b) Unwrapped ML
0
0.5
1
1.5
2
60
50
40
30
20
10
(c) MAP (with ML
hyperparameters)
Figure 11.7. Comparison of estimated frequency proﬁles
– the dynamics are respected: frequencies up to ν = 1.15 can be read (i.e., double
the reference frequencies of Figure 11.2, since we subsampled once).
In addition, it is obtained in an entirely automatic way (the hyperparameters are
at maximum likelihood) and only takes a tenth of a second of work for the Viterbi
algorithm.
11.4. Conclusion
The developments of this chapter (see also [GIO 02, BER 01]) were chosen for
their general character. The concepts developed for the spectral analysis have also
been used for analyzing meteorological radar clutter [GIO 01] and could be used
in other ﬁelds such as speech analysis. The methods proposed for estimating the
mean velocity have been used for characterizing skin tissue by its acoustic atten-
uation [GIO 94]. The tracking capacity beyond the Shannon frequency is directly
transposable to magnetic resonance velocimetry [HER 99], where it can signiﬁcantly
improve the SNR on the velocity images and reduce the duration of the clinical study.
Speciﬁc improvements can be made to these methods by introducing more speciﬁc
prior information. For example, the possible break in ﬂow continuity at the vessel wall
is not taken into consideration but it could be. For parabolic laminar ﬂows, the velocity
decreases regularly from the center of the ﬂow towards the wall. The hypothesis of
a connection between the ﬂow velocity and the vessel wall makes sense. However,
in plug ﬂows there are very high velocities near the vessel wall and the possibility of
large discontinuities in the velocity near the wall must be maintained. Joint detection
(at the same time as the velocity estimation) of the limits of the vessel can thus be
envisaged, allowing an exception to be made to spatial continuity.
It has been established that there are variations in the Doppler signal amplitude
during the cardiac cycle. An increase is observed during the acceleration of the blood,

Spectral Characterization in Ultrasonic Doppler Velocimetry
307
with a maximum occurring early after peak systole.The ﬂow regimen (laminar or tur-
bulent) also inﬂuences the Doppler signal amplitude. It would thus be possible to take
this phenomenon into account to further adapt the temporal regularization.
Experiment and expert assessment of the results obtained point out balances be-
tween the various alternatives for data processing, particularly regularization.
The ﬁrst element is a balance between a) the quality of the results and b) the
computing cost and method complexity. This requires efﬁcient algorithms that are
suited to the calculations to be made (Kalman ﬁlter, Viterbi algorithm, gradient, etc.).
The second element concerns the “visual” aspect of the results. It encourages the
use of convex functionals. Regularization by a more discriminating functional (such
as Blake and Zisserman’s truncated quadratic [BLA 87]) gives a “binary” aspect to the
maps that is not very helpful for subsequent interpretation.
The third element is the sensitivity of the solution to the hyperparameters and the
capacity to estimate these hyperparameters. Using likelihood criteria such as those
presented here seems to be an efﬁcient way of tuning the hyperparameters. However,
in routine use, it is indispensable to offer the user a set of suitable hyperparameters
straight away. The authors’ experience suggests that the choices proposed above lead
to methods that are sufﬁciently robust with respect to their hyperparameters and that,
for a given type of application, a single set of hyperparameters provides signiﬁcant, if
not optimal, improvement in the results.
11.5. Bibliography
[AKA 70] AKAIKE H., “Statistical predictor identiﬁcation”, Ann. Inst. Stat. Math., vol. 22,
p. 207-217, 1970.
[AKA 74] AKAIKE H., “A new look at the statistical model identiﬁcation”,
IEEE Trans.
Automat. Contr., vol. AC-19, num. 6, p. 716-723, Dec. 1974.
[AZE 86] AZENCOTT R., DACUNHA-CASTELLE D., Series of Irregular Observations: Fore-
casting and Model Building, Springer Verlag, New York, NY, 1986.
[BAU 70] BAUM L. E., PETRIE T., SOULES G., WEISS N., “A maximization technique oc-
curing in the statistical analysis of probabilistic functions of Markov chains”, Ann. Math.
Stat., vol. 41, num. 1, p. 164-171, 1970.
[BER 95] BERTSEKAS D. P., Nonlinear Programming,
Athena Scientiﬁc, Belmont, MA,
1995.
[BER 01] BERTHOMIER C., HERMENT A., GIOVANNELLI J.-F., GUIDI G., POURCELOT L.,
DIEBOLD B., “Multigate Doppler signal analysis using 3-D regularized long AR model-
ing”, Ultrasound Med. Biol., vol. 27, num. 11, p. 1515–1523, 2001.

308
Bayesian Approach to Inverse Problems
[BLA 87] BLAKE A., ZISSERMAN A., Visual Reconstruction, The MIT Press, Cambridge,
MA, 1987.
[DEV 85] DEVIJVER P. A., “Baum’s forward-backward algorithm revisited”, Pattern Recog-
nition Letters, vol. 3, p. 369-373, Dec. 1985.
[FOR 73] FORNEY G. D., “The Viterbi algorithm”, Proc. IEEE, vol. 61, num. 3, p. 268-278,
Mar. 1973.
[GIO 94] GIOVANNELLI J.-F., IDIER J., QUERLEUX B., HERMENT A., DEMOMENT G.,
“Maximum likelihood and maximum a posteriori estimation of Gaussian spectra. Applica-
tion to attenuation measurement and color Doppler velocimetry”, in Proc. Int. Ultrasonics
Symp., vol. 3, Cannes, France, p. 1721-1724, Nov. 1994.
[GIO 01] GIOVANNELLI J.-F., IDIER J., DESODT G., MULLER D., “Regularized adaptive
long autoregressive spectral analysis”,
IEEE Trans. Geosci. Remote Sensing, vol. 39,
num. 10, p. 2194-2202, Oct. 2001.
[GIO 02] GIOVANNELLI J.-F., IDIER J., BOUBERTAKH R., HERMENT A., “Unsupervised
frequency tracking beyond the Nyquist limit using Markov chains”, IEEE Trans. Signal
Processing, vol. 50, num. 12, p. 1–10, Dec. 2002.
[GUO 94] GUO Z., DURAND J.-G., LEE H. C., “Comparison of time-frequency distribution
techniques for analysis of simulated Doppler ultrasound signals of the femoral artery”, IEEE
Trans. Biomed. Eng., vol. BME-41, num. 4, p. 332-342, Apr. 1994.
[HER 97] HERMENT A., GIOVANNELLI J.-F., DEMOMENT G., DIEBOLD B., DELOUCHE
A., “Improved characterization of non-stationary ﬂows using a regularized spectral analysis
of ultrasound Doppler signals”, Journal de Physique III, vol. 7, num. 10, p. 2079-2102, Oct.
1997.
[HER 99] HERMENT A., MOUSSEAUX E., DE CESARE A., JOLIVET O., DUMEE P., TODD-
POKROPEK A., BITTOUN J., GUGLIELMI J. P., “Spatial regularization of ﬂow patterns
in magnetic resonance velocity mapping”, J. of Magn. Reson. Imaging, vol. 10, num. 5,
p. 851-860, 1999.
[JAZ 70] JAZWINSKI A. H., Stochastic Process and Filtering Theory, Academic Press, New
York, NY, 1970.
[KAY 81] KAY S. M., MARPLE S. L., “Spectrum analysis – A modern perpective”, Proc.
IEEE, vol. 69, num. 11, p. 1380-1419, Nov. 1981.
[KIT 85] KITAGAWA G., GERSCH W., “A Smoothness Priors Time-Varying AR Coefﬁ-
cient Modeling of Nonstationary Covariance Time Series”, IEEE Trans. Automat. Contr.,
vol. AC-30, num. 1, p. 48-56, Jan. 1985.
[LEV 83] LEVINSON S. E., RABINER L. R., SONDHI M. M., “An introduction to the ap-
plication of the theory of probabilistic functions of a Markov process to automatic speech
recognition”, Bell Syst. Tech. J., vol. 62, num. 4, p. 1035-1074, Apr. 1983.
[LIP 82] LIPORACE L. A., “Maximum likelihood estimation for multivariate observations of
Markov sources”, IEEE Trans. Inf. Theory, vol. 28, p. 729-734, Sep. 1982.
[MAR 87] MARPLE S. L., Digital Spectral Analysis with Applications, Prentice-Hall, Engle-
wood Cliffs, NJ, 1987.

Spectral Characterization in Ultrasonic Doppler Velocimetry
309
[PAR 74] PARZEN E., “Some recent advances in time series modeling”, IEEE Trans. Automat.
Contr., vol. AC-19, num. 6, p. 723-730, Dec. 1974.
[RAB 86] RABINER L. R., JUANG B. H., “An introduction to hidden Markov models”, IEEE
ASSP Mag., vol. 3, num. 1, p. 4-16, 1986.
[RIS 78] RISSANEN J., “Modeling by shortest data description”, Automatica, vol. 14, p. 465-
471, 1978.
[SOR 80] SORENSON H. W., Parameter Estimation, Marcel Dekker, New York, NY, 1980.
[TAL 88] TALHAMI H. E., KITNEY R. I., “Maximum likelihood frequency tracking of the
audio pulsed Doppler ultrasound signal using a Kalman ﬁlter”, Ultrasound Med. Biol.,
vol. 14, num. 7, p. 599-609, 1988.
[ULR 76] ULRYCH T. J., CLAYTON R. W., “Time series modelling and maximum entropy”,
Phys. Earth Planet. Interiors, vol. 12, p. 188-200, 1976.


Chapter 12
Tomographic Reconstruction
from Few Projections
12.1. Introduction
The aim of tomography is to non-destructively reconstruct a map of a parameter
that is characteristic of an object, e.g. its density. Its principle is based on analyzing the
interaction between the object and radiation (X-ray, electronic or optical, for example)
that is made to propagate through it. The characteristic quantity we are trying to ﬁnd
can be obtained from the observations by inversion of the equations of transport in the
material.
Tomography has many ﬁelds of application. In medicine, systems such as ultra-
sound scanners or MRI allow us to observe internal clues to pathologies (tumours,
aneurisms, etc.). In nuclear medicine, the behavior of a tracer inside the body can be
followed by functional imaging so as to show up possible dysfunctions of organs or tis-
sues. In industry, tomography is used in the development of manufacturing processes
(welding, molding, etc.), in production quality control, and for safety and security
(checking the contents of luggage). It is also an important investigation tool in many
research ﬁelds: architecture of bone structure, ﬂow analysis, etc.
In several conﬁgurations, the number of projections available is limited, e.g. be-
cause of restrictions on the dose of radiation that can be transmitted to the patient in
medical applications, or because of testing rates to be respected or the transient na-
ture of the phenomenon to be observed in industry. Moreover, geometrical constraints
Chapter written by Ali MOHAMMAD-DJAFARI and Jean-Marc DINTEN.

312
Bayesian Approach to Inverse Problems
may limit the distribution of the projections to certain angles. This chapter concen-
trates particularly on reconstruction methods suited to such contexts. The methods
will be presented for the case where the irradiation is by X-ray transmission.
We will ﬁrst present the relationship between the observations and the attenuation
data we are trying to ﬁnd. Then we will recall the analytical 2D and 3D reconstruc-
tion methods, point out their limits in the case of a small number of projections, and
introduce the discrete reconstruction formalism. In the subsections that follow, we
will present reconstruction approaches suited to a discrete framework and a limited
number of projections. Finally, we will use some examples to illustrate the results
obtained by the different approaches.
12.2. Projection generation model
If an object composed of homogeneous material is illuminated by X-rays and a
narrow monoenergetic ray crosses the object, the intensity of the ray measured as it
leaves the object can be obtained from:
I = I0 exp {−μL}
(Beer-Lambert law)
(12.1)
where I0 is the intensity of the input ray, L the distance covered by the ray in the
object, and μ the linear attenuation coefﬁcient of the object, which depends on the
density of the material, its nuclear composition and the energy of the X-ray ﬂux. This
relationship can be generalized to a cross-section of a non-homogeneous object per-
pendicular to the axis oz crossed by the rays.
y
6
x
-
f(x, y)
r
φ
p(r, φ)
S
D
φ
Figure 12.1. X-ray tomography

Tomographic Reconstruction from Few Projections
313
If we characterize the object by the distribution of its linear attenuation coefﬁcient
μ, which is a continuous function of the two space variables μ(x, y) = f(x, y); we
have:
I = I0 exp

−

L
f(x, y) dl
4
=⇒−log
 I
I0

=

L
f(x, y) dl
(12.2)
where
0 and moving the
an angle φ with the ox axis (see Figure 12.1), we obtain what is called a projection
p(r, φ):
p(r, φ) =

L
f(x, y) dl =

D
f(x, y) δ(r −x cos φ −y sin φ) dx dy
(12.3)
X-ray tomography image reconstruction consists of ﬁnding an estimation f(x, y)
of f(x, y) from the projections p(r, φi), i = 1, . . . , M. This can easily be extended
to the 3D case where the aim is to ﬁnd an estimation f(x, y, z) of f(x, y, z) from
the X-ray images p(r1, r2, ⃗ui), i = 1, . . . , M, where ⃗ui represents the direction of
projection i. In this chapter, we will particularly take the context where we have few
projections and they are limited to angles φi.
12.3. 2D analytical methods
If we consider the ideal case where function p(r, φ) is known perfectly for all
values of r and φ, the reconstruction problem comes down to the inversion of the
Radon transform (RT) R [DEA 83]:
p(r, φ) =

f(x, y) δ(r −x cos φ −y sin φ) dx dy
(12.4)
for which the analytical expression of the inverse is:
f(x, y) =
1
2π2
 π
0
 ∞
0
∂p(r, φ)/∂r
r −x cos φ −y sin φ dr dφ.
(12.5)
In practice, we only have the measurements of p(r, φ) for discrete values of φ and r.
Thus, the integrals have to be approximated by sums. Two major difﬁculties then
become apparent: the approximation of ∂p(r, φ)/∂r and the approximate calculation
of the integral in r (because of the singularity of its kernel). In addition, although the
discretization along the r axis can be reasonably ﬁne, the discretization of angle φ is
generally more parsimonious. These constraints have resulted in the direct application
of the expression for the inverse of the RT being unused in practice for many years, as
dl is the basic length of path L.By noting p = −log I/I
emitter (source S) and receiver (detector D) in parallel along a straight line making

314
Bayesian Approach to Inverse Problems
f(x,y)
x
y
−150
−100
−50
0
50
100
150
−150
−100
−50
0
50
100
150
f(x, y) -
RT
- p(r, φ) -
?
- f(x, y)
phi
r
p(r,phi)
  0
 45
 90
135
180
225
270
315
?
=⇒
−60
−40
−20
0
20
40
60
−60
−40
−20
0
20
40
60
Figure 12.2. Problem of X-ray tomography image reconstruction in 2D
Figure 12.3. Problem of X-ray tomography image reconstruction in 3D

Tomographic Reconstruction from Few Projections
315
outside the ideal, theoretical case, direct application of the inversion formula does not
give convincing results.
A considerable number of methods have been put forward [BRO 78, BUD 79,
CHO 74, HER 80, HER 87, MOH 88, NAT 80] with the aim of obtaining more sat-
isfactory approximate solutions. Some operate directly on the equations, others use
auxiliary transformations: Hilbert transform (HT), or Fourier transform (FT). These
methods as a whole can be summed up by deﬁning the following operators:
– derivation D:
p(r, φ) = ∂p(r, φ)/∂r,
– HTH:
g(r′, φ) = 1
π
 ∞
0
p(r, φ)
(r −r′) dr,
– backprojection (BP) B: f(x, y) = 1
2π
 π
0
g(x cos φ + y sin φ, φ) dφ,
– 1D FT F1:
P(Ω, φ) =

p(r, φ) e−jΩr dr,
– 1D inverse FT F−1
1 :
p(r, φ) = 1
2π

P(Ω, φ) ejΩr dΩ,
– 2D FT F2:
F(ωx, ωy) =

f(x, y) e−j(ωxx+ωyy) dx dy,
– 2D inverse FT F−1
2 :
f(x, y) =
1
4π2

F(ωx, ωy) e−j(ωxx+ωyy) dωx dωy.
With these deﬁnitions, the following analytical relations can be demonstrated:
f = B H D R f = B F−1
1
|Ω| F1 R f = B C1R f
= F−1
2
|Ω| F2 B R f = C2 B R f
where C1 = F−1
1
|Ω| F1 and C2 = F−1
2
|Ω| F2 are convolution operators:
C1p(r, φ) = (h1 ⋆p)(r, φ),
h1(r) =

|Ω|e−jΩr dr
C2b(x, y) = (h2 ⋆b)(x, y),
h2(x, y) =
 #
ω2x + ω2ye−j(ωxx+ωyy) dx dy.
Finally, we should also mention another relation known as the projection slice theo-
rem:
F(ωx, ωy) = P(Ω, φ)
for
ωx = Ω cos φ
and
ωy = Ω sin φ
(12.6)
which is at the heart of many inversion techniques in tomography and turns the RT
inversion problem into a Fourier synthesis problem [MOH 88]. These relationships
summarize the various algorithms that are used to obtain approximate solutions for
the RT inversion, and thus for the image reconstruction problem in X-ray tomography.
They are shown schematically below:

316
Bayesian Approach to Inverse Problems
– direct inversion of RT:
p(r, φ) →D →p(r, φ) →H →g(r′, φ) →B →f(x, y)
– backprojection of ﬁltered projections:
p(r, φ) →F1 →ﬁlter |Ω| →F−1
1
→g(r′, φ) →B →f(x, y)
– backprojection and ﬁltering by convolution:
p(r, φ) →ﬁlter 1D h1(r) →g(r′, φ) →B →f(x, y)
– backprojection followed by 2D ﬁltering:
p(r, φ) →B →b(x, y) →F2 →
ﬁlter
|Ω| =

ω2x + ω2y →F−1
2
→f(x, y)
– backprojection followed by 2D convolution ﬁltering:
p(r, φ) →B →b(x, y) →ﬁlter 2D h2(x, y) →f(x, y)
– padding and interpolation in the Fourier domain and 2D inverse FT:
p(r, φ) →F1 →P(Ω, φ) →
interpolation
ωx = Ω cos φ
ωy = Ω sin φ →F(ωx, ωy) →F−1
2
→f(x, y)
It is worth noting, however, that we come up against two difﬁculties when implement-
ing these methods numerically:
– the projections have limited supports, both in the angles and in their ﬁnite num-
ber. The integrals are approximated by sums and the various transforms are approxi-
mated by their discrete versions;
– the ﬁnal image is often represented in Cartesian coordinates in the form of pixels
or voxels, and there is necessarily an interpolation stage in the central backprojection
step. This interpolation stage is more explicit in the methods using Fourier synthesis,
which pass through the Fourier domain. The interpolation is itself an inverse prob-
lem. It makes prior information on the object necessary, a fact often eclipsed in the
algorithms.
We have not mentioned the speciﬁc methods which, by using certain properties of
the object f(x, y), transform the RT inversion problem into the inversion of another
transform, often of lower dimension. For example, if the object possesses revolution
symmetry (f(x, y) = f(ρ) and p(r, φ) = p(r)), the problem can be reduced to the
inversion of the Abel transform in one dimension:
p(r) =
 r
0
f(ρ)
√r −ρ dρ .

Tomographic Reconstruction from Few Projections
317
12.4. 3D analytical methods
A ﬁrst approach in 3D reconstruction is to carry out successive reconstructions of
2D slices using the reconstruction techniques mentioned above and starting from pro-
jections obtained on linear strip sensors. However, this is beset by practical difﬁculties
as it involves successive rotations of the detection system around the object followed
by translation movements slice by slice.
Thus, for an acquisition system composed of an X-ray source and a linear sensor,
helicoidal acquisition geometry has been proposed: while the acquisition system is
rotating, the patient’s bed moves at a constant velocity. Provided that there is at least
one half to one turn of the acquisition system for a bed movement of the thickness
of reconstruction slice, the reconstruction techniques can be adapted to yield a 3D
object. The difference between this and the slice-by-slice geometry is that we have
to take account of the fact that one projection is involved in several reconstructed
slices [CRA 90, KAL 95]. To shorten the acquisition time, current systems are evolv-
ing towards multiline or even two-dimensional sensors. The problem of an acquisition
ray intersecting several slices is then even trickier.
In certain conﬁgurations, the whole reconstruction zone can be handled by one
2D detector. The analytical reconstruction algorithms must take the conical geometry
into account, i.e., the fact that a projection ray crosses several slices of the object to
be reconstructed. Algorithms have been proposed for this type of geometry: [FEL 84]
is suitable for small angles of aperture of the beams and [GRA 91] deals with larger
angles.
12.5. Limitations of analytical methods
Analytical methods have been used successfully in medicine but their use remains
more limited in other applications such as non-destructive evaluation. This can be ex-
plained by stricter acquisition constraints in non-destructive evaluation: the angles of
incidence, the number of measurements and the signal-to-noise ratio (SNR) are often
limited. In consequence, the analytical solutions are rarely satisfactory. Figure 12.4
illustrates these difﬁculties.
It is also worth mentioning that, even in medical imaging, the number of projec-
tions is limited and the acquisition system has to be optimized, since a great effort
is made to keep the quantity of X-rays received by the patient to a minimum. This
is the case, for example, in helicoidal tomography, where traditional methods cannot
be applied directly. To make up for the small amount of data, prior information has
to be added. This means developing methods more speciﬁc to the class of objects
to  be examined: objects where variations are smooth, objects with abrupt changes,
binary objects, composed of
constant,
or piecewise
objects that are continuous

318
Bayesian Approach to Inverse Problems
(a) initial object
(b) 64 projections
(c) 16 projections
(d) 8 projections
(e) 16 projections between 0
and π/2
(f) 8 projections between 0
and π/2
Figure 12.4. Five cases of reconstruction by ﬁltered backprojection in 2D: 64, 16 and 8
projections spread between 0 and π, and 16 and 8 projections between 0 and π/2
(a) original object
(b) 24 projections
(c) 12 projections
Figure 12.5. Two cases of reconstruction by ﬁltered backprojection in 3D: 24 and 12
projections, distributed between 0 and 2π around the object

Tomographic Reconstruction from Few Projections
319
geometrical shapes (ellipses or polygons in 2D, ellipsoids or polyhedra in 3D), etc.
Analytical methods do not readily allow prior information to be introduced and used.
This type of prior information can be handled more easily by a formal description of
the object and discrete modeling, which thus seems more suitable.
12.6. Discrete approach to reconstruction
Since there is a ﬁnite amount of data and the calculations are performed numer-
ically, there is no alternative but to discretize the problem, in the broad sense of the
term. Three approaches are possible:
– represent the object by an appropriate parametric model, model the connection
between the data and the parameters of the model and estimate the parameters directly
from the data;
– model the image by projecting it onto a basis of appropriate functions with a
reasonable number of coefﬁcients to be estimated;
– discretize the object by a set of pixels or voxels using the ﬁnest desired resolution
and bring in prior information suited to the nature of the object under examination:
continuous, piecewise continuous, sparse, binary, etc.
An example of methods based on the ﬁrst approach is the modeling of an object
by superimposing a ﬁnite number of ellipsoids and the estimation of their parame-
ters {(xi, yi, zi), (ai, bi, ci), (θi, φi), fi} by least squares or maximum likelihood. The
main difﬁculty lies in the fact that the relationship between the data and these param-
eters is not linear (although explicit and analytical). We thus have to make use of
non-trivial optimization techniques to calculate the solution.
The last two of the three approaches mentioned above have one point in common:
projection of the object f(x, y) on a basis of functions:
f(x, y) ≃
N

n=1
xnbn(x, y).
In the latter case, the basis functions, bn(x, y), are indicative of the supports of the
pixels or voxels (zero-order splines). This projection allows the problem to be trans-
formed into a discrete system of linear equations:
for i = 1, . . . , M1; j = 1, . . . , M2,
p(rj, φi) ≃

f(x, y) δ(rj cos φi −rj sin φi) dx dy
=
N

n=1
xn

bn(x, y) δ(rj cos φi −rj sin φi) dx dy.

320
Bayesian Approach to Inverse Problems
By reorganizing and renaming the data in the form ym = p(rj, φi), m = (i −1) ×
M1 + j, we ﬁnd y ≃Ax, where y = [y1, . . . , yM]T and x = [x1, . . . , xN]T are
column vectors containing the data and the unknown coefﬁcients of the decomposition
respectively, and A is the matrix with elements:
Amn =

bn(x, y) δ(rj cos φi −rj sin φi) dx dy,
where m corresponds to the observation index p(rj, φi). The problem thus comes
down to obtaining a satisfactory approximate solution for an equation of the type y =
Ax+b, where b represents both the errors connected with the various approximations
and the measurement noise proper. We can thus use all the methods described in the
preceding chapters, while taking advantage of the particular structure of matrix A and
the speciﬁc models for noise b. We consider some of these below.
Of course, the choice of the basis functions bn(x, y) (pixels, natural pixels, har-
monic functions, wavelets, etc.) has important consequences for the structure and
properties of matrix A and for the meaning and computational complexity of its ele-
ments.
Choice of basis functions
Two approaches can be distinguished: either the basis is chosen according to a
prior on the object, independently of the geometry (e.g. the pixels or voxels), or it
is induced by the geometry (e.g. the natural pixels) [GAR 87]. In the ﬁrst case, the
elements of x can have a physical meaning but the elements of matrix A are often
more costly to calculate. In the second case, the calculation is less costly but the basis
is not necessarily orthogonal and complete and it is more difﬁcult to give a physical
meaning to the elements of x.
In the ﬁrst case, three categories of functions can again be distinguished: global
functions (e.g. Fourier series), local functions (e.g. splines) and hybrid functions (e.g.
wavelets).
In all these cases, the meaning of the elements of x depends on this choice. In the
same way, the meaning of the elements and the structure of matrix A depend on the
choice and the geometry of the acquisition of the projections. For example, in conic
geometry, for complete data (uniform angular coverage between 0 and 2π), the matrix
A will have a block-circulant structure.
From now on, to present the principle of the inversion methods, we will use the
general case without taking this feature into consideration. Of course, it is absolutely
necessary to take it into account in implementation, so as to obtain algorithms with
reasonable computing costs.

Tomographic Reconstruction from Few Projections
321
12.7. Choice of criterion and reconstruction methods
As we have just seen, the discrete formulation of the problem leads us to look for
a satisfactory solution x for y = Ax + b. Matrix A has large dimensions and is
often ill-conditioned or even singular. Its conditioning depends particularly on the ba-
sis chosen for the object decomposition, the geometry and the ratio of the number of
independent data to the number of parameters describing the object. In the preceding
chapters, we have seen that it is often illusory to think that we can obtain a satisfactory
solution to the problem by looking for an exact solution A−1y, a least squares solu-
tion arg minx ∥y −Ax∥2, or even a generalized inverse solution A†y. A satisfactory
solution can only be obtained by somehow introducing prior information on the solu-
tion. This may be done through a judicious choice of the basis functions so that the
object can actually be described by a small number of parameters that are simply esti-
mated in the least squares sense. However, this approach is too restrictive and speciﬁc:
a basis of functions has to be built for a particular object. The alternative, when these
functions are chosen independently of the operator (pixels or voxels for example), is
the regularization approach that allows more generic priors to be expressed.
In what follows, we will focus more on the methods that deﬁne the solution as
the optimizer of a regularized criterion. Three categories of solutions can thus be
distinguished:
1) Those deﬁned as the unconstrained minimizer of a composite criterion:
x = arg min
x
J(x) = Q(y, Ax) + λ F(x, x0)
where Q and F are generally two distances. Various expressions can be obtained for
Q and F depending on the hypotheses on the nature of the noise, the data and the
nature of the object. A traditional example is J(x) = ∥y −Ax∥2 + λ∥Dx∥2, where
D is a derivation operator matrix to make the solution smooth. In this case, x can be
expressed analytically:
x = (AT A + λDT D)−1AT y,
and this expression can be interpreted in terms of analytical methods if we observe that
AT corresponds to a backprojection operation and (AT A + λDT D)−1 to a ﬁltering
operation in 2D.
2) Those deﬁned as the constrained minimizer of a simple criterion:
x = arg min
x
F(x, x0)
s. t. y = Ax
where F is also generally a distance or a measure of divergence between x and a
default solution x0. A traditional example is x0 = 0 and F(x) = ∥Dx∥2. In this
case too, the solution has an analytical expression:
x = (DT D)−1AT (A(DT D)−1AT )−1y,

322
Bayesian Approach to Inverse Problems
which is equivalent to a backprojection of the ﬁltered projections. Another traditional
example consists of choosing:
F(x, x0) = KL(x, x0) =

j
xj log xj/x0
j + xj −x0
j
where KL(x, x0) is the Kullback-Leibler divergence of x with respect to x0. This
brings us to the maximum entropy methods.
3) Those deﬁned as the minimizer of a probabilistic criterion:
x = arg min
x
¯C(x)
where ¯C(x) is the expectation of a cost function C(x, x) with respect to the posterior
probability law p(x | y):
¯C(x) =

C(x, x)p(x | y) dx
It is the choice of this cost function that brings us to different estimation structures.
With a Gaussian hypothesis for the noise and object laws and a quadratic cost, we
ﬁnd the previous solutions again. However, this probabilistic approach allows us to go
beyond deterministic solutions through other choices for the prior laws and other cost
functions, or through the idea of marginalization.
Types of estimators
In the Bayesian probabilistic approach, according to the choice made for the cost
function C(x, x), a variety of expressions can be obtained for estimator x depending
on the data. Among these estimators, the following are of particular interest:
– maximum a posteriori (MAP):
x = arg max
x
p(x | y) = arg max
x
p(x, y);
– posterior mean (PM):
x = E(x | y) =

x p(x | y) dx =

x p(x, y) dx

p(x, y) dx ;
– marginal maximum a posteriori (marginal MAP or Marginal Posterior Mode,
MPM):
fj = arg max
xj
p(xj | y).

Tomographic Reconstruction from Few Projections
323
One of the elements that help in the choice of these various estimators is the com-
puting cost. Depending on the type of estimator chosen, it is necessary to perform
either an optimization or an integration. The optimization is either multivariate, as
in the MAP case, or scalar, as in the marginal MAP case. Note, too, that in the
marginal MAP case, there is an integration step that is the calculation of the marginal
laws [BOU 96, DIN 90, GEM 87, SAQ 98].
Except for the linear Gaussian case, it is often difﬁcult to ﬁnd analytical expres-
sions for these integrals and the criteria to be optimized are non-quadratic. Special
attention thus needs to be paid to the integration and optimization aspects. For this
reason, the next two subsections describe a number of algorithms that implant these
different estimators.
12.8. Reconstruction algorithms
A reconstruction algorithm can be deﬁned as a sequence of operations carried out
on projections in order to construct an image. An algorithm that contented itself with
this deﬁnition without stating the method on which the algorithm was based or the
properties of the images obtained by using it would be of little value. We will thus
restrict ourselves to algorithms that optimize a deterministic or probabilistic criterion.
In the ﬁrst subsection below, we give an overview of the optimization algorithms that
can be used for convex criteria in particular and, in the second, we take a very brief
look at algorithms using probabilistic criteria.
12.8.1. Optimization algorithms for convex criteria
We have seen that an important family of methods for solving inverse problems de-
ﬁnes a solution to the problem by minimizing a criterion that is least squares J(x) =
∥y −Ax∥2, regularized least squares ∥y −Ax∥2 + λΦ(x) or, more generally,
Q(y, Ax) + λF(x, x0), with Q and F two distances or two measures of diver-
gence. Similarly, in probabilistic approaches, we ﬁnd MAP J(x) = −log p(x | y)
or marginal MAP J(xj) = −log p(xj | y) criteria. In this section, we give a synthetic
presentation of the optimization algorithms that can be used particularly for convex
criteria. The convex criteria can be subdivided into three groups of increasing gener-
ality and complexity: i) strictly convex and quadratic, ii) continuous, strictly convex
but non-quadratic, iii) continuous, convex but possibly non-derivable at a few points.
In the ﬁrst case, the solution exists, is unique and is a linear function of the data.
It can be calculated by any descent algorithm. It is even possible to obtain an explicit,
direct solution.
In the second case, the solution exists and is unique but is not a linear function of
the data. It can easily be calculated using any descent algorithm.

324
Bayesian Approach to Inverse Problems
In the third case, the solution generally exists but may not be unique. Its calculation
requires some caution and the solution is not a linear function of the data.
Let us not forget that a non-convex criterion may be multimodal, which means that
we need to resort to global optimization algorithms. The solution is not generally a
linear function of the data and the solution is not easy to calculate.
From an algorithmic point of view, the algorithms for calculating the optimum
solution can be divided into two categories: those that change the set of unknowns x
at each iteration and those that change a single element xj (or a block of elements) at
a time. The algorithms of each category can again be subdivided into those that use
the whole set of data y at each iteration and those that use only one data item yi (or a
block of data) at each iteration.
12.8.1.1. Gradient algorithms
An important family of algorithms for optimizing a criterion updates the estimate
x(k−1) additively in a descent direction (often the opposite direction to that of the
gradient of the criterion).
To look more closely at the traditional descent methods in tomographic reconstruc-
tion, let us consider the special case when the criterion to be optimized has the generic
form:
J(x | y) =

i
qyi(zi) + λ

j
φj(tj),
with zi = [Ax]i = aT
i∗x and tj = [Dx]j = xj −xj−1, where aT
i∗= [ai1, . . . , aiN]
is row i of matrix A, D a ﬁnite differences matrix, qy and φj convex functions that
reach their minimum at y and 0, respectively, and λ a regularization parameter.
To distinguish between the various classes of algorithms mentioned above, we will
use the following notation:
– for the algorithms that use all the data y and update all the unknowns x at each
iteration, we write:
g(x | y) =
!∂J(x | y)
∂x1
, . . . , ∂J(x | y)
∂xN
"T
= AT q′
y(z) + λDT φ′(t)
for the gradient of J(x | y) with respect to x, with φ′(t) = [φ′
1(t1), . . . , φ′
N(tN)]
and q′
y = [q′
y1, . . . , q′
yM ];
– for those that use only a single data item yi but update all the unknowns x:
J(x | yi) = qyi(zi) + λ

j
φj(tj),
g(x | yi) = q′
yi(zi) ai∗+ λDT φ′(t) ;

Tomographic Reconstruction from Few Projections
325
– for the algorithms that use all the data but only update a single variable xk at
each iteration:
J(xk | y; x\k) = J(x | y)
g(xk | y; x\k) = ∂J(x | y)
∂xk
=

i
aikq′
yi(zi) + λ

j
djkφ′
j(tj) ;
– for the algorithms that use a single data item yi and update only a single variable
xk at each iteration:
J(xk | yi; x\k) = qyi(zi) + λ

j
φj(tj)
g(xk | yi; x\k) = ∂J(xk | yi; x\k)
∂xk
= aikq′
yi(zi) + λ

j
djkφ′
j(tj).
We are now going to look at a number of special cases used in tomography. Here,
aij represents the length of the path of ray i in pixel j. We will observe that most
of the conventional algorithms in tomography are in fact gradient algorithms applied
to least squares criteria: qyi(zi) = (yi −zi)2, or are based on a Kullback-Leibler
divergence: qyi(zi) = KL(yi, zi).
12.8.1.2. SIRT (Simultaneous Iterative Relaxation Techniques)
Proposed by [GIL 72], this is a gradient algorithm operating on J(x | y) = ∥y −
Ax∥2:
x(k) = x(k−1) + α(k) D AT (y −Ax(k−1)),
k = 1, 2, . . .
with D = Diag {1/ 
i aij}. For α(k), there are several possible variants: ﬁxed,
optimal, etc.
The calculations performed by this algorithm at each iteration are mainly a pro-
jection Ax and a backprojection AT (y −Ax). When α is ﬁxed, it should verify
0 < α ∥DAT A∥< 2. The word simultaneous signiﬁes that all the data is used at
each iteration of the algorithm.
12.8.1.3. ART (Algebraic Reconstruction Technique)
This algorithm, widely used in tomography, corresponds to the minimization of
J(x | yi) = (yi −aT
i∗x)2 by a variable or optimal step-size gradient algorithm:
x(k) = x(k−1) + α(k) yi −aT
i∗x(k−1)
∥ai∗∥2
ai∗,
k = 1, 2, . . . ; i = 1 mod M,
with x(0) = 0. It is also worth noting that, when α(k) = 1, x(k) can be obtained
from x(k−1) in the direction of projection of x(k−1) on the subset deﬁned by the

326
Bayesian Approach to Inverse Problems
equation yi = aT
i∗x, i = k mod M. At each update of x, only one data item is used.
There are also versions in which certain constraints (e.g. positivity) are imposed on
the solution at each iteration. ART is a special case of projection onto convex sets
(POCS), originally presented by Kaczmarz [KAC 37].
The order in which the data is used is crucial for the efﬁciency of the algorithm.
The underlying idea is that the successive iterations should be as independent as pos-
sible [HER 93, MAT 96].
12.8.1.4. ART by blocks
ART modiﬁes the values of pixels x by backprojecting the differences between the
observed and calculated values of a single projection ray. A variant backprojects the
difference for a whole block of projection rays. The updating equation thus becomes:
x(k) = x(k−1) + α(k) AT
ik(yik −Aikx(k−1))
∥AT
ikAik∥
,
where yik is a block of data and AT
ik the matrix corresponding to the ray projection
equations for the block.
ART corresponds to the case where ik is scalar and the S-ART corresponds to the
case where ik represents the set of rays for a projection direction. Herman [HER 93]
showed that the reconstruction could be accelerated by choosing a successively or-
thogonal series of blocks. This type of method is known as Ordered Subset M ART,
where M indicates the number of block projection directions.
12.8.1.5. ICD (Iterative Coordinate Descent) algorithms
These are relaxation algorithms (Chapter 2). For the basic case J(x | y) = ∥y −
Ax∥2, this structure is equivalent to that of the Gauss-Seidel algorithm [PAT 99].
12.8.1.6. Richardson-Lucy algorithm
Let us consider the following criterion:
J(x | y) = KL(y, Ax) = −

i
yi log aT
i∗x/yi + yi −aT
i∗x,
for which we have:
∂J(x | y)
∂xj
=

i
aij
yi
aT
i∗x −1,
∂2J(x | y)
∂xj2
= −

i
a2
ij
yi
(aT
i∗x)2 .

Tomographic Reconstruction from Few Projections
327
The approximation

∂2J(x | y)/∂x2
j
−1 ≃−xj/ 
i aij gives an approximate New-
ton algorithm that can be written:
x(k)
j
= x(k−1)
j
1

i aij

i
aij
yi

ℓaiℓx(k−1)
ℓ
, k = 1, 2, . . . ; j = k mod N,
and is known as the Richardson-Lucy algorithm [LUC 74, RIC 72]. It can also be
interpreted as an EM algorithm in a probabilistic approach with Poisson distribu-
tions [SHE 82] or as a projection onto convex sets (POCS) algorithm using the
Kullback-Leibler divergence [SNY 92].
12.8.2. Optimization or integration algorithms
Probabilistic models make it easier to describe situations where the image to be
reconstructed is a real quantity that is continuous, piecewise constant or takes dis-
crete values. A typical example is Markov modeling (see Chapter 7). In these cases,
it is often necessary to optimize a criterion (MAP or marginal MAP estimators), or
to calculate a posterior mean (PM estimator). We have already described a number
of algorithms for optimizing criteria when they are convex. In this subsection, we
will focus on the algorithms that are used either to optimize multimodal criteria or to
calculate PM estimators.
As mentioned above, stochastic approaches lead to the construction of a posterior
probability distribution of the type exp {−J(x)} over the whole set of reconstructions,
which expresses both the ﬁdelity to data (likelihood) and the regularization terms.
The reconstruction is deﬁned as an estimator coming from this distribution: MAP
estimator (maximum of the probability distribution), MPM estimator (maximum of
the marginal posterior distribution), or PM estimator (mean of the posterior distribu-
tion), for example. According to the type of prior considered (Markov, non-convex
function), the above estimators cannot be calculated directly from the distribution,
exp {−J(x)}, but are determined by sampling. Two types of algorithms are mainly
used for this:
– the Gibbs sampler is particularly suitable for Markov priors as the expression
for the conditional probabilities is then simple. This is presented in section 7.4.2. By
simulating a series of conﬁgurations representative of the probability distribution P,
we can estimate the marginals of P, and thus deﬁne the MPM estimator of the pos-
terior marginal maximum. Similarly, this algorithm enables the mean of the posterior
distribution to be estimated;
– the simulated annealing algorithm can construct a random series having a law
that converges towards a uniform distribution on the minima of P. It thus enables the
MAP to be estimated (see section 7.4.2.3).

328
Bayesian Approach to Inverse Problems
12.9. Speciﬁc models for binary objects
The methods we have presented so far are very general and can be applied to most
image reconstruction problems. However, in some applications, particularly in NDE,
we want to reconstruct the image of a defect (air pocket) in a homogeneous medium
(metal). The problem then comes down to the reconstruction of a binary object. Three
categories of methods for handling this problem can be distinguished:
1) Methods that model the object by a set of binary voxels [BOU 96] using Markov
modeling for the binary image and ﬁnally estimate this image using one of the estima-
tors mentioned in the preceding sections. In these methods, the direct model linking
the projections to the voxels is linear, but it is often difﬁcult to take account of the
prior information on the closing of the defect’s outline using a simple Markov model.
2) Other methods propose a direct reconstruction of the closed contour of the ob-
ject, either by modeling the contour as the solution to a partial derivative equation
(active contours, Snakes), or by modeling it as the passage of a higher dimension
function to zero (level-set methods) [SAN 96]. The main drawbacks of these methods
are (i) the high computing cost (e.g. because of the updating of the function represent-
ing the wavefront in the level-set method) and (ii) the difﬁculty of implementation and
the lack of tools for choosing the step for the propagation of the wavefront.
3) Finally, there are methods that model the contour by deformable geometri-
cal shapes, the parameters of which are estimated from the data [AMI 93, BAT 98,
HAN 94]. Several categories of geometrical models have been put forward: elliptical
or super-quadratic curves or surfaces, harmonics and curves or surfaces described by
splines. The ﬁrst is too simple and restrictive, the second is broader and is suitable for
star-shaped curves and surfaces, and the last is more general. The price to be paid is
an increase in the number of parameters.
A comparison of these various methods can be found in [MOH 97], together with
a speciﬁc method for cases where the object is modeled by a polygon in 2D or a
polyhedron in 3D (see [SOU 04] for a detailed presentation of the latter approach).
12.10. Illustrations
The principal objective of these illustrations is to show what can be achieved using
the different methods, particularly in the case where there are very few projections at
limited angles.
12.10.1. 2D reconstruction
We chose a binary object with dimensions 128 × 128 (Figure 12.6a), simulated
ﬁve projections for the angles −π/4, −π/8, 0, π/8, π/4, added Gaussian white noise

Tomographic Reconstruction from Few Projections
329
(a) original
(b) backprojection
(c) ﬁltered backprojection
(d) quadratic regularization
(e) quadratic regularization with
positivity constraint
(f) Lp regularization with
positivity constraint
Figure 12.6. Comparison of 2D reconstruction methods
to obtain an SNR of 20 dB and then used this data to perform reconstructions using
the various methods.
12.10.2. 3D reconstruction
We chose a binary object with dimensions 128 × 128 × 128 (Figure 12.7). Nine
projections were distributed uniformly, either between 0 and π or between 0 and π/2.
In both cases, we simulated the data without and with noise (SNR of 20 dB). We then
tested the various methods.
The reconstructions by ART (Figure 12.8) show strong perturbations induced by
the noise when regularization is not used. In the next reconstruction, by ICM (Fig-
ure 12.9), we used a regularization particularly well suited to the binary situation: an
Ising model (see section 7.3.4.1), which favors compact zones. In this reconstruction,
the effects of the noise are signiﬁcantly attenuated. However, the cap of the mushroom
is separated from its stalk.

330
Bayesian Approach to Inverse Problems
Figure 12.7. Original object for 3D tests
(a) 9 projections in [0, π], data without
noise
(b) 9 projections in [0, π/2], data
without noise
(c) 9 projections in [0, π], data with
noise
(d) 9 projections in [0, π/2], data with
noise
Figure 12.8. Reconstruction by ART without regularization.
Thresholding was applied for display purposes

Tomographic Reconstruction from Few Projections
331
(a) 9 projections in [0, π], data without
noise
(b) 9 projections in [0, π/2], data
without noise
(c) 9 projections in [0, π], data with
noise
(d) 9 projections in [0, π/2], data with
noise
Figure 12.9. Reconstruction by ICM and regularization by an Ising model
12.11. Conclusions
The aim of this chapter was to give an overview of the various reconstruction
methods in X-ray tomography, focusing on the methods that can be used in difﬁcult
situations: when the number of projections is small or the angular coverage is very re-
stricted, i.e., when conventional analytical methods are unable to provide satisfactory
results. These more sophisticated methods obviously induce higher computing costs,
which, even today, limit their practical use in medical systems or in everyday NDE.
However, the trend for the future will always be to reduce the amount of radiation
transmitted to a patient or to reduce the cost and acquisition time in NDE applica-
tions. For these reasons, it will be necessary to use the more sophisticated methods.
Nevertheless, it is still a good idea to continue developing more speciﬁc methods with
reasonable computing cost for speciﬁc applications. This is true, for example, for the

332
Bayesian Approach to Inverse Problems
methods recently developed for reconstructing the closed surface of a compact, homo-
geneous object directly from X-ray data, without going through a voxel reconstruction
step.
12.12. Bibliography
[AMI 93] AMIT Y., MANBECK K. M., “Deformable template models for emission tomogra-
phy”, IEEE Trans. Medical Imaging, vol. 12, num. 2, p. 260-268, June 1993.
[BAT 98] BATTLE X. L., CUNNINGHAM G. S., HANSON K. M., “Tomographic reconstruc-
tion using 3D deformable models”, Phys. Med. Biol., vol. 43, p. 983-990, 1998.
[BOU 96] BOUMAN C. A., SAUER K. D., “A uniﬁed approach to statistical tomography using
coordinate descent optimization”, IEEE Trans. Image Processing, vol. 5, num. 3, p. 480-
492, Mar. 1996.
[BRO 78] BROOKS R. A., WEISS G. H., TALBERT A. J., “A new approach to interpolation
in computed tomography”, J. Comput. Assist. Tomogr., vol. 2, p. 577-585, 1978.
[BUD 79] BUDINGER T. F., GULLBERG W. L., HUESMAN R. H., “Emission computed to-
mography”, in HERMAN G. T. (Ed.), Image Reconstruction from Projections: Implemen-
tation and Application, New York, NY, Springer Verlag, p. 147-246, 1979.
[CHO 74] CHO Z. H., “General views on 3-D image reconstruction and computerized trans-
verse axial tomography”, IEEE Trans. Nuclear Sciences, vol. 21, p. 44-71, 1974.
[CRA 90] CRAWFORD C. R., KING K. F., “Computed tomography scanning with simultane-
ous patient translation”, Med. Phys., vol. 17, num. 6, p. 967-982, Jan. 1990.
[DEA 83] DEANS S. R., The Radon Transform and Some of its Applications, Wiley Inter-
science, New York, NY, 1983.
[DIN 90] DINTEN J.-M., Tomographie à partir d’un nombre limité de projections : régulari-
sation par champs markoviens, PhD thesis, University of Paris XI, Jan. 1990.
[FEL 84] FELDKAMP L. A., DAVIS L. C., KRESS J. W., “Practical cone-beam algorithm”, J.
Opt. Soc. Am. (A), vol. 1, num. 6, p. 612-619, 1984.
[GAR 87] GARNERO L., Reconstruction d’images tomographiques à partir d’un ensemble
limité de projections, Doctoral thesis, University of Paris XI, Jan. 1987.
[GEM 87] GEMAN S., MCCLURE D., “Statistical methods for tomographic image reconstruc-
tion”, in Proc. 46th Session of the ICI, Bulletin of the ICI, vol. 52, p. 5-21, 1987.
[GIL 72] GILBERT P., “Iterative methods for the three-dimensional reconstruction of an object
from projections”, J. Theor. Biol., vol. 36, p. 105-117, 1972.
[GRA 91] GRANGEAT P., “Mathematical framework of cone beam 3D reconstruction via the
ﬁrst derivative of the Radon transform”, in HERMAN G. T., LOUIS A. K., NATTERER
F. (Eds.), Mathematical Methods in Tomography, vol. 1497, New York, Springer Verlag,
p. 66-97, 1991.

Tomographic Reconstruction from Few Projections
333
[HAN 94] HANSON K. M., CUNNINGHAM G. S., JENNINGS G. R. J., WOLF D. R., “Tomo-
graphic reconstruction based on ﬂexible geometric models”, in Proc. IEEE ICIP, vol. 2,
Austin, TX, p. 145-147, Nov. 1994.
[HER 80] HERMAN G. T., Image Reconstruction from Projections. The Fundamentals of
Computerized Tomography, Academic Press, New York, NY, 1980.
[HER 87] HERMAN G. T., TUY H. K., LANGENBERG K. J., SABATIER P. C., Basic Methods
of Tomography and Inverse Problems, Adam Hilger, Bristol, UK, 1987.
[HER 93] HERMAN G. T., MEYER L. B., “Algebraic reconstruction techniques can be made
computationally efﬁcient”, IEEE Trans. Medical Imaging, vol. 12, p. 600-609, 1993.
[KAC 37] KACZMARZ S., “Angenaherte Auﬂosung von Systemen linearer Gleichungen”,
Bull. Intern. Acad. Polon. Sci. Lett., vol. A-35, p. 355-357, 1937.
[KAL 95] KALENDER W. A., “Principles and Performance of Spiral CT”, in GOODMAN
L. W., FOWLKES J. B. (Eds.), Medical CT and Ultrasound: Current Technology and Ap-
plications, Madison, WI, Advanced Medical Publ., p. 379-410, 1995.
[LUC 74] LUCY L. B., “An iterative technique for the rectiﬁcation of observed distributions”,
Astron. J., vol. 79, num. 6, p. 745-754, 1974.
[MAT 96] MATEJ S., LEWITT R. M., “Practical considerations for 3-D image reconstruction
using spherically symmetric volume elements”, IEEE Trans. Medical Imaging, vol. 15,
p. 68-78, 1996.
[MOH 88] MOHAMMAD-DJAFARI A., DEMOMENT G., “Maximum entropy reconstruction
in X ray and diffraction tomography”, IEEE Trans. Medical Imaging, vol. MI-7, num. 4,
p. 345-354, 1988.
[MOH 97] MOHAMMAD-DJAFARI A., “Shape reconstruction in X-ray tomography”, in Proc.
SPIE, vol. 3170, San Diego, CA, p. 240-251, July 1997.
[NAT 80] NATTERER F., “Efﬁcient implementation of optimal algorithms in computerized to-
mography”, Math. Methods Appl. Sci., vol. 2, p. 545-555, 1980.
[PAT 99] PATRIKSSON M., Nonlinear Programming and Variational Inequality Problems. A
Uniﬁed Approach, Applied Optimization, Kluwer Academic Publishers, Dordrecht, The
Netherlands, May 1999.
[RIC 72] RICHARDSON W. H., “Bayesian-based iterative method of image restoration”, J.
Opt. Soc. Am., vol. 62, p. 55-59, Jan. 1972.
[SAN 96] SANTOSA F., “A level-set approach for inverse problems involving obstacles”,
ESAIM : COCV, vol. 1, p. 17-33, Jan. 1996.
[SAQ 98] SAQUIB S. S., BOUMAN C. A., SAUER K. D., “ML parameter estimation for
Markov random ﬁelds with applications to Bayesian tomography”,
IEEE Trans. Image
Processing, vol. 7, num. 7, p. 1029-1044, July 1998.
[SHE 82] SHEPP L. A., VARDI Y., “Maximum likelihood reconstruction for emission tomog-
raphy”, IEEE Trans. Medical Imaging, vol. MI-1, p. 113-122, 1982.

334
Bayesian Approach to Inverse Problems
[SNY 92] SNYDER D. L., SCHULZ T. J., O’SULLIVAN J. A., “Deblurring subject to nonneg-
ativity constraints”, IEEE Trans. Signal Processing, vol. 40, num. 5, p. 1143-1150, May
1992.
[SOU 04] SOUSSEN C., MOHAMMAD-DJAFARI A., “Polygonal and polyhedral contour re-
construction in computed tomography”, IEEE Trans. Image Processing, vol. 13, num. 11,
p. 1507–1523, Nov. 2004.

Chapter 13
Diffraction Tomography
13.1. Introduction
In this chapter, we will look at the use of the Bayesian approach to solve the diffrac-
tion tomography problem. For this type of problem, the measurements collected, the
waves scattered by an object, depend on the physical parameters of the object in a
nonlinear way. Many works have considered linear approximations of the propaga-
tion phenomenon – the best known being Born and Rytov’s approximations – but we
do not intend to follow this path. We will insist on the solution to the problem that
takes the nonlinear model into account.
For a nonlinear inverse problem, if we consider an explicit relationship between
the parameters of object x and measurements y in the form y = A (x) , the inversion
consists of deducing parameters x from measured data y. A natural approach to this
problem would be to minimize the least squares criterion:
J(x) = ∥y −A(x)∥2 .
In the case of ill posed problems, as illustrated in Chapter 1, this type of solution is
not suitable because of its great sensitivity to variations in the measurements, which
inevitably contain errors. The Bayesian framework enables a regularized solution to
be deﬁned for this problem by using a probabilistic model of the object sought, as in
the case of linear inverse problems.
Nonlinearity is, by its very nature, a non-property. Here, we will not go into a gen-
eralization of Bayesian regularization for nonlinear inverse problems but will insist on
Chapter written by Hervé CARFANTAN and Ali MOHAMMAD-DJAFARI.

336
Bayesian Approach to Inverse Problems
the speciﬁcities of the diffraction tomography problem and how they can be exploited
to deﬁne and optimize criteria. We will present the wave propagation equations which
allow an integral direct model to be deduced in the form of two coupled equations. We
will describe the method of moments, which is often used to discretize the direct inte-
gral model and gives an algebraic model, again in the form of two coupled equations.
Estimation in the maximum a posteriori (MAP) sense will thus enable us, from this
model, to deﬁne regularized solutions as minimizing certain criteria. The nonlinearity
of the direct model makes these criteria non-convex.
This criterion minimization framework will allow us to look at most of the methods
used to solve this problem from the same point of view. We will start by presenting
methods that optimize the criteria locally by successive linearizations of the direct
model. Then we will point out the connection with techniques that aim to simulta-
neously reconstruct the object parameters and the ﬁeld in the domain of the object.
Finally, we will stress the importance of using global optimization techniques in par-
ticularly difﬁcult situations where the criteria have local minima.
13.2. Modeling the problem
The generic term diffraction tomography is used here in connection with a large
number of imaging modes for which the phenomenon measured is related to the
diffraction of a wave in an inhomogeneous medium. We consider that the object to be
imaged is symmetric about one of its axes (cylindrical symmetry) so as to bring the
problem down to two dimensions. Moreover, measurements are taken in a domain DM
outside domain DO in which the object is present, hence the connection with conven-
tional X-ray tomography. Let us also stress the fact that the objective is to construct an
image (grid of values) of the physical characteristics of the object, unlike what is done
in a number of works where only the outline of the object or the ﬁeld in the domain of
the object is considered.
We will only present here, and consider in what follows, applications involving
electromagnetic waves. Nevertheless, it is interesting to note that strong similari-
ties exist with certain applications using acoustic waves (e.g. ultrasonic tomographic
imaging [KAK 88]).
13.2.1. Examples of diffraction tomography applications
Applications of diffraction tomography techniques are to be found in many areas,
such as biomedical engineering, non-destructive evaluation of conducting materials
and geophysical exploration.

Diffraction Tomography
337
13.2.1.1. Microwave imaging
In the 1970s, X-ray tomography enabled images to be taken of the human body.
The fact that ionizing radiation can be harmful encouraged studies of other forms of
energy such as microwaves (at low power levels), ultrasound, and magnetic resonance.
Research on active microwave imaging has developed greatly since the early 1980s
[BOL 83].
The aim of such imaging techniques is to determine the propagation character-
istics of an inhomogeneous medium (the human body or at least a part of it), with
spatial variation of the conductivity σ(r) and dielectric permittivity ϵ(r) (r ∈DO).
This inhomogeneous medium is surrounded by a homogeneous medium (e.g. water)
having known characteristics σ0 and ϵ0. The geometric conﬁgurations envisaged (Fig-
ure 13.1a) are taken directly from those of X-ray tomography. Unlike in the case for
X-rays, diffraction phenomena cannot be neglected for this mode of imaging; the
waves can no longer be assumed to travel through the medium in straight lines, which
complicates the imaging problem considerably [KAK 88]. The frequencies involved
in this type of application vary between a hundred or so MHz and several hundred
GHz.
DM
DO
S
(a) microwave imaging
DM
DO
S
(b) NDE by eddy currents
DM
DM
DO
S
(c) well to well geotomography
Figure 13.1. Various tomography conﬁgurations: a plane wave is emitted by a source S and
propagates in object domain DO. The scattered ﬁeld is measured by a ﬁnite number of
receivers placed in domain DM
13.2.1.2. Non-destructive evaluation of conducting materials using eddy currents
The aim of the non-destructive evaluation (NDE) of conducting materials is to de-
tect and study the defects in these materials. In such a process, an imaging technique
can characterize the defects and intervenes after a detection step. For this, an electro-
magnetic wave is emitted from a homogeneous medium known to be non-conducting
(generally air) and having known characteristics σ = 0 and ϵ0. The measurements of
the ﬁeld that has propagated through the conducting medium (Figure 13.1b) are ac-
quired in the same homogeneous medium. The physical characteristic to be imaged is

338
Bayesian Approach to Inverse Problems
the conductivity σ(r) of a possible defect in the conducting object which has known
characteristics (σ0 = 107 S/m and ϵ0) [ZOR 91]. The frequency range considered
covers kHz to MHz.
13.2.1.3. Geophysical exploration
Geotomography has proved to be an interesting tool in geophysics, for instance
when searching for petroleum oil or monitoring its extraction. An example of a pos-
sible application is well to well tomography where two wells are bored, one on either
side of a region to be explored (Figure 13.1c). Electromagnetic waves are emitted by a
source placed in one of the wells and the measurements are taken by receivers located
in the two wells (measuring the waves reﬂected and transmitted by the medium). The
aim is to image the conductivity σ(r) and permittivity ϵ(r) of the medium [HOW 86].
The frequencies used are of the order of one MHz.
13.2.2. Modeling the direct problem
We will not go into the details of the development of the integral equations to
model the imaging problem but we will insist on the hypotheses used and the form of
the resulting coupled equations.
13.2.2.1. Equations of propagation in an inhomogeneous medium
We are going to take the case of electromagnetic waves, e.g. in microwave tomog-
raphy, but similar expressions can be written in other applications and for acoustic
waves [COL 92].
Let us consider the propagation of plane electromagnetic waves in the harmonic
regime of pulsation ω in non-magnetic media. An inhomogeneous medium is sur-
rounded by an ambient homogeneous medium (having known characteristics: permit-
tivity ϵ0, conductivity σ0, magnetic permeability of vacuum μ0 = 4π×10−7 N.A−2).
We can adopt a two-dimensional framework by considering that the object is uniform
about one of its axes (cylindrical symmetry) and the incident wave is polarized along
this axis (transverse magnetic conﬁguration). This will allow us to work with scalar
ﬁelds. In this framework, Maxwell’s equations give φ, the component of the electric
ﬁeld along the axis of the object, from:
Δφ(r) + k2(r)φ(r) = −jωμ0J(r)
(13.1)
where r is the position vector in the 2-D space, Δ is the Laplacian, and J is induced by
the source. The wave number k2(r) = ω2μ0 (ϵ(r) + jσ(r)/ω) of the inhomogeneous
object at r is connected to its electrical permittivity ϵ(r) and its conductivity σ(r) (the
surrounding homogeneous medium is characterized by its wave number k2
0). From the
equation above, we can deduce a propagation equation in integral form:

Diffraction Tomography
339
φ(r) = φ0(r) +

D
G(r, r′) (k2(r′) −k2
0) φ(r′) dr′,
(13.2)
where φ0 is the incident ﬁeld; G is the free space Green’s function which, in the two-
dimensional case, can be written:
G(r, r′) = j
4H(1)
0
(k0|r −r′|) ,
where H(1)
0
is the zero-order Hankel transform of the ﬁrst kind.
13.2.2.2. Integral modeling of the direct problem
We have just described the scalar equations corresponding to the propagation of
waves in an inhomogeneous medium. Now, we are going to describe the propagation
phenomenon involved in the imaging problem. To do this, we will consider the near-
ﬁeld geometrical conﬁguration shown in Figure 13.2.
-
φ0
DO
(φ, x)
DM (y)
Figure 13.2. Chosen two-dimensional conﬁguration
The measurements are acquired by sensors placed in a discrete set of positions
denoted DM in the surrounding medium1. From now on we will use y(ri) to denote
the scattered ﬁeld at the measuring points ri ∈DM. The total ﬁeld in the object do-
main will be noted φ(r), r ∈DO. The object is characterized by its complex contrast
x(r) = k2(r) −k2
0. We will now describe the propagation of the waves from the
source to the measuring points, the incident ﬁeld in the object domain φ0(r), r ∈DO
– corresponding to the emitted wave – being known.
1. It is sometimes necessary to take the measurements outside the surrounding medium. It
is thus no longer possible to work with the free space Green’s functions and the interfaces
between the two media have to be taken into account through their own Green’s functions (see
for example [ZOR 91] on NDE using eddy currents).

340
Bayesian Approach to Inverse Problems
Using equation (13.2), the scattered ﬁeld at the measuring points can be written in
terms of the contrast and the total ﬁeld in the object:
y(ri) =

DO
G(ri, r′) x(r′) φ(r′) dr′,
ri ∈DM.
(13.3)
This equation is often called the observation equation. Field φ in the object domain
veriﬁes an equation of the same type:
φ(r) = φ0(r) +

DO
G(r, r′) x(r′) φ(r′) dr′,
r ∈DO,
(13.4)
sometimes called the coupling equation. Note that this equation is implicit in φ.
The direct problem is modeled by the relation connecting the inhomogeneous ob-
ject x(r), r ∈DO with the scattered ﬁeld at the measuring points y(ri), ri ∈DM and
is thus written in the form of the two coupled equations (13.3)-(13.4). This model is
nonlinear as the scattered ﬁeld is linearly dependent on the product of the contrast and
the ﬁeld in the object domain (13.3), and the ﬁeld itself depends on the contrast (13.4).
As a general rule, there is no analytical expression for the solution of the direct
problem but various numerical methods exist for solving it [COL 92]. In section 13.3
we will present the discretization of the direct model in integral form and a numerical
method for solving the direct problem will follow on naturally from this.
13.3. Discretization of the direct problem
Much of the work in mathematical physics on solving inverse problems in general
– and the problem of diffraction tomography in particular – is presented in a functional
framework. The discretization is only carried out in a ﬁnal step, for the numerical
calculation of a functionally deﬁned solution. A. Tarantola, for example, writes, “This
is a quite general conclusion in any inverse problem theory: discretization, if any, must
be reserved for the ﬁnal computations, and not for developing the formulae” [TAR 82].
In statistical data processing, on the other hand, the discretization is generally taken
into account as early as the direct model. This is the framework we have chosen here
and we will point out the advantages for the problem at hand. To obtain an algebraic
model, we will use the method of moments, as described brieﬂy below, to discretize
the coupled equations (13.3)-(13.4).
13.3.1. Choice of algebraic framework
The deliberate choice of using an algebraic rather than a functional framework is
tricky to justify and can give rise to considerable discussion. For the problem posed
here, several arguments can be put forward:

Diffraction Tomography
341
– the measurements are taken using a ﬁnite number of sensors and are thus discrete
by nature, whereas a functional framework very often assumes that the measured val-
ues are known continuously around the object;
– apart from rare special cases where the object can be parametrized in a simple
way (e.g. if we know we are dealing with a homogeneous disk), the solution is calcu-
lated numerically;
– for the diffraction tomography problem, the nonlinear equations are not easy to
handle and it is interesting to observe, as we will point out in section 13.5.1, that
several methods suggested in a functional framework, starting from distinct ways of
reasoning, lead to solutions that are functionally different but numerically identical;
– ﬁnally, the equations to be handled are simpler in an algebraic framework, even
though they are not linear. It is then easier to focus on the fact that the problem is ill
posed and on the way to regularize it by introducing prior models elaborated on the
solution sought. Note that the probabilistic framework may also be understood from a
functional point of view as in [TAR 87] but the models then become rather difﬁcult to
handle outside the Gaussian case.
13.3.2. Method of moments
The method of moments is a generic method that allows linear equations to be dis-
cretized so that they can be solved numerically when their solution cannot be reached
in the functional domain. An excellent overview of this method, also showing that
most discretization techniques can be interpreted as methods of moments, is to be
found in [HAR 87].
The general framework for using this method is to solve a functional equation of
the type: Lf = g, where L is a linear operator, g a known function and f the unknown
function to be determined. The aim is to discretize this equation so as to obtain a ma-
trix relation of the form Lf = g which can be solved numerically. To do this, function
f is represented as a linear combination of a set of functions {b1, b2, . . . , bn}, called
basis functions:
f =
n

i=1
fi bi,
where coefﬁcients fi are to be determined. An approximate solution is considered in
which the number n of basis functions is ﬁnite. The linear equation to be solved thus
becomes:
n

i=1
fi L bi = g.
By projecting this relation on to a set of functions {t1, t2, . . . , tn}, called testing func-
tions, the equation can be written in matrix form:
Lf = g,
with Li,j = ⟨ti, L bj⟩and gi = ⟨ti, g⟩;

342
Bayesian Approach to Inverse Problems
where L = (Li,j) is a matrix of size n×n and f = (fi), g = (gi) are column vectors
of length n.
The choice of the basis and testing functions is a tricky problem in the method
of moments. The choice should be based on practical considerations, such as ease
of calculation of the various scalar products, but also on physical considerations so
as to take account of the speciﬁcity of the problem being dealt with. Finally, these
choices will have a direct inﬂuence on the structure of the matrix L and thus on the
ease with which the algebraic problem can be solved numerically. We note that, for
testing functions of the form ti(r) = ri, gi =

rig(r)dr is the ith order moment of
g, hence the name given to the method.
13.3.3. Discretization by the method of moments
Solving the direct problem, i.e., calculating the scattered ﬁeld at the measurement
points that corresponds to a given contrast function, does not fall directly within the
application framework of the method of moments since the relation linking the object
with the data is not linear. Nevertheless, the method can be applied to each of the
coupled equations (13.3-13.4). We will use the discretization performed in [HOW 86]
(called the volume current method in the article) which is often employed for this
problem.
Equation (13.4) is discretized by considering a square grid on object domain DO.
For basis functions {bi}i=1, ..., nO and testing functions {ti}i=1, ..., nO , we take the
indicative functions on the square regions Di (pixels), of side c, of DO (φ, φ0 and x
are assumed constant over these regions and represented by column vectors of length
nO). Thus, the algebraic equation corresponding to (13.4) can be written in the form:
φ = φ0 + GOXφ,
(13.5)
in which X is a diagonal matrix X = Diag {xi}i=1, ..., nO , and GO is a matrix of size
nO × nO, the elements of which verify:
(GO)i,j = 1
c2

Di

Dj
G(r, r′) dr′ dr.
Similarly, relation (13.3) can be written in the algebraic form:
y = GMXφ,
(13.6)
with y = (y(ri))i=1, ..., nM, where nM is the number of measurement points, GM a
matrix of size nM × nO, the elements of which can be written:
(GM)i,j = 1
c2

DM

Dj
G(r, r′) dr′ti(r) dr,

Diffraction Tomography
343
where ti are testing functions on measurement domain DM.
In [HOW 86] it is suggested that the integrals of the Green’s function should be
approached by integrating, not over the square regions Di, but over disks having an
equivalent surface area. This gives an analytical formula for the integrals and avoids
the use of numerical integration methods. With this approach, the discretization errors
can be considered to be negligible if c is of the order of a tenth of the wavelength in
the surrounding homogeneous medium [HOW 86].
The direct model was presented in integral form with the coupled equations (13.3)-
(13.4). In algebraic form, we obtain the coupled equations (13.5)-(13.6). This model
can be expressed in an explicit form directly linking the object with the measurements
without bringing in the ﬁeld in the object domain. For a given object of contrast x,
φ = (I −GOX)−1φ0.
By replacing this relation in equation (13.6), the explicit relation linking x to y can
be deduced:
y = A(x),
with A(x) = GMX(I −GOX)−1φ0.
(13.7)
Note that, to solve the direct problem, equation (13.7) brings in the inversion of a
matrix. In an equivalent way (mathematically speaking but not in terms of computing
cost), equation (13.5) requires a linear system of nO equations with nO unknowns to
be solved. Nevertheless, this inversion does not pose any numerical problems as the
linear system is well conditioned.
Finally, it is worth noting that these relations were established for vectors and ma-
trices with complex values. It is sometimes useful to take the case of real variables,
by separating the real and imaginary parts. In this case, exactly the same matrix rela-
tionships can be obtained by modifying the deﬁnition of matrices GM and GO. These
relationships can also be generalized to the multi-source case where we consider mea-
surements acquired for distinct emitting sources [CAR 96].
13.4. Construction of criteria for solving the inverse problem
We now have an algebraic model of the direct problem which allows us to calculate
scattered ﬁeld y at the measuring points from a knowledge of contrast x of the object,
possibly using a calculation of the ﬁeld at object φ. Now we are going to turn our
attention to solving the inverse problem by reconstructing contrast x (object) from
measurements of scattered ﬁeld y (data). Like most inverse problems, this one is ill-
posed. In particular, the solution is very unstable with respect to variations, even very
slight ones, in the data [COL 92]. Moreover, even in a functional framework, where
equation (13.5) allows ﬁeld φ to be written in the form:

344
Bayesian Approach to Inverse Problems
measurements are assumed to be known at all points of a continuous domain DM
surrounding the object, the uniqueness of the solution is not proven. Starting from this
algebraic model, we will use the Bayesian framework to construct criteria deﬁning
regularized solutions of this inverse problem.
To do this, we need to have a model for the errors that perturb the measurements
(modeling errors, discretization error and noise on the measurements). As no par-
ticular information is available here on the statistics of measurement noise, unlike in
some applications (see Chapter 14, for example), we will consider zero-mean, circu-
lar, white, Gaussian errors of variance σ2
M. As pointed out in section 3.2, we are not
saying that the perturbations actually are Gaussian but the Gaussian hypothesis is the
least compromising choice we can make for the error distribution, the errors being
considered to have a mean value of zero and ﬁnite variance.
We also need a prior model of the object. It should be noted that the object to be
imaged, x, has complex values. Given the speciﬁc physical meanings of the real part
(connected with the permittivity of the object) and the imaginary part (connected with
its conductivity), we will consider a separable probability density here
p(x) = pR(Re{x}) pI(Im{x}).
As we want to reconstruct images composed of homogeneous zones, we will consider
densities corresponding to a Gibbs-Markov model, as presented in Chapter 7, and
written in the form2:
p(x) ∝exp {−Φ(x)/T }.
However, it is prudent to content ourselves with a convex function Φ at ﬁrst, to
avoid increasing the minimization difﬁculties due to the nonlinearity of the direct
model. To get our ideas clear, let us consider a traditional generalized Gauss-Markov
model [BOU 93] (of potential function | · |p, 1 < p ≤2).
13.4.1. First formulation: estimation of x
The parameters to be estimated for the imaging problem are the values of the
contrast function on the image pixels, i.e., vector x.
The likelihood of the object, which corresponds to the direct distribution of the
data, can be written:
p(y | x) =

πσ2
M
−nM/2 exp

−1
σ2
M
∥y −A (x)∥2
4
.
2. From now on, we will, incorrectly, write the probability density of the object in this form,
the separation between the real and imaginary parts and any differences of parameters of these
densities being implicit.

Diffraction Tomography
345
Bayes’ rule enables the information provided by the measurements and the prior
information to be fused into the posterior density of the parameters to be estimated:
p(x | y) = p(y | x) p(x)/p(y),
where p(y) is a normalization coefﬁcient independent of x.
The maximum a posteriori estimate maximizes the posterior distribution:
x
MAP = arg max
x
p(x | y),
or, in other words, minimizes the criterion:
J
MAP(x) = ∥y −A(x)∥2 + λΦ(x),
with, here:
A(x) = GMX (I −GOX)−1 φ0,
and λ = σ2
M/T playing the role of the regularization parameter governing the trade-
off between ﬁdelity to the data and the prior model. Note that the minimum should
be understood in the sense of the global minimum of the criterion. As a shortcut, the
J MAP criterion will often be called the maximum a posteriori criterion in what follows,
or simply the MAP criterion.
13.4.2. Second formulation: simultaneous estimation of x and φφφ
The problem can also be considered from another point of view by trying to es-
timate the object, x, and the ﬁeld at the object, φ, simultaneously from the data.
Starting with the same hypotheses as above (additive zero-mean, circular, white Gaus-
sian noise on the measurements), this time we will exploit the relationship linking the
object to the measurements using the coupled equations (13.5)-(13.6).
Bayes’ rule allows us to express the joint posterior distribution of x and φ in the
form:
p(x, φ | y) = p(y | x, φ) p(φ | x) p(x)/p(y).
(13.8)
In this relation, we are only interested in the three terms of the numerator. The de-
nominator term p(y) is independent of φ and x and does not come into the calculation
of the MAP estimate. Let us examine these three terms one by one:
– as the model used for the measurement errors and equation (13.6), the ﬁrst term
can be written:
p(y | x, φ) ∝exp

−1
σ2
M
∥y −GMXφ∥2
4
;

346
Bayesian Approach to Inverse Problems
– the second term corresponds to the probability density of φ for known x. φ
being the total ﬁeld at the object, it is uniquely determined by the second of the coupled
equations (13.5) when x is known. This leads us to consider a probability measure
(and not a probability density) of the type:
δ(φ −φ0 −GOXφ),
(13.9)
where δ is Dirac’s delta function;
– the last term corresponds to the prior model for the object.
To be able to develop equation (13.8) using probability densities, we are going to
use a neat calculation trick. Consider an error e0 that perturbs the coupling equation
(13.5). If we consider these perturbations to be zero-mean, circular, white and Gaus-
sian with a variance σ2
O and independent of object x, we can show that the probability
density of φ, knowing x, can be written in the form:
p(φ | x) ∝exp

−1
σ2
O
∥φ −φ0 −GOXφ∥2
4
.
The joint x and φ posterior density can be expressed in the form:
p(x, φ | y) ∝exp {−J
MAPJ(x, φ)} ,
with:
J
MAPJ(x, φ) = 1
σ2
M
∥y −GMXφ∥2 + 1
σ2
O
∥φ −φ0 −GOXφ∥2 + Φ(x)
T
.
(13.10)
The estimate in the MAP sense of the pair (x, φ) maximizes p(x, φ | y), or, in other
words, minimizes criterion J MAPJ. We recall that the criterion thus obtained is based
on hypotheses – perturbations of the coupling equation (13.5) – that have no physical
or statistical justiﬁcation. They are simply ad hoc hypotheses that allow the J MAPJ
criterion to be established. It can be seen that, if these perturbations are applied at
the level of data y, the hypotheses are equivalent to considering errors of the form
eM + GMX (I −GOX)−1 eO in which eM and eO are random, independent, zero-
mean, Gaussian variables, the errors considered on the data depending on object x.
To break free of these hypotheses, we have to make variance σ2
O of perturbations eO
tend towards zero. The Gaussian distribution p(φ | x) thus tends towards the Dirac
distribution (13.9) in the sense of generalized functions. In this case, the weighting
coefﬁcient 1/σ2
O of the second term of the J MAPJ criterion tends towards inﬁnity, which
is the same as saying that the estimate in the MAP sense of the pair (x, φ) minimizes
the criterion:
J
MAPC(x, φ) = ∥y −GMXφ∥2 + λΦ(x),
(13.11)
(with λ = σ2
M/T ) under the bilinear constraints:
φ −φ0 −GOXφ = 0.
(13.12)

Diffraction Tomography
347
In general, from a probabilistic point of view, the solution in x of the estimate
in (x,φ) in the joint MAP sense has no reason to correspond to the estimate of x in
the MAP sense. It is obvious that we arrive at the same solution here because the
deterministic relation linking φ and x, and the J MAPC criterion is none other than the
J MAP criterion in which we have set φ = (I −GOX)−1φ0, the relationship we took
into account as a constraint linking x and φ.
13.4.3. Properties of the criteria
We have deﬁned a regularized solution to the problem of diffraction tomography,
bringing us to the minimization of a criterion with or without constraints. It is clear
that, before trying to implement optimization techniques, it is important to study the
properties of these criteria.
As we saw in Chapter 3, in the case of a linear problem in x with a Gaussian
model of additive noise, the data ﬁdelity term is quadratic in x and thus convex; any
non-convexity comes from the prior model of the unknowns. In our case, the non-
convexity of the criteria presented and of the constraint comes from the nonlinearity
of the model of the direct problem (recall that we chose a prior model giving a convex
function Φ). We are thus not completely sure that these criteria are unimodal, which
does not necessarily mean that they have local minima3. Apart from the situations
where we can make use of a linear approximation of the direct problem, two types of
situation can be envisaged: the ﬁrst, relatively favorable, for which, although they are
not convex, the criteria do not have local minima; the second, more difﬁcult, for which
local minima are present. The latter appear in particular when the contrast function
takes high values and the number of data values is small (possibly even lower than the
number of unknowns). Depending on the type of situation, we can see already that it
is not possible to envisage implementing the same optimization techniques.
13.5. Solving the inverse problem
Since the late 1980s, many methods have been put forward for solving this nonlin-
ear inverse problem. They are generally presented in a functional framework and it is
not always very easy to compare them from a theoretical point of view. The Bayesian
algebraic framework adopted in this chapter, i.e., the minimization of the criteria de-
ﬁned in section 13.4, allows us to take a uniﬁed view of most of the approaches used.
The aim here is not to go into the details of the various inversion methods employed
but, above all, to present their strengths and weaknesses, and points of comparison
in terms of optimization and regularization. Three classes of methods can be distin-
guished, which we will study below.
3. Recall that convexity is only a sufﬁcient condition for unimodality.

348
Bayesian Approach to Inverse Problems
13.5.1. Successive linearizations
The ﬁrst methods employed to solve this inverse problem used successive afﬁne
approximations of the model of the direct problem and thus successively solved linear
inverse problems. Let us consider a two-step iterative process:
1) afﬁne approximation of the model of the direct problem around a current value
for the object xn: A(x) = Anx + bn;
2) calculation of xn+1 solution of the linear inverse problem corresponding to this
approximation.
Note that this type of approach can be envisaged for solving any nonlinear inverse
problem, even if no convergence property can be established in general.
These methods have generally been proposed on the basis of approximations of
wave propagation equations and thus of the direct model in integral form. To com-
pare them, it is necessary to study the afﬁne approximation and the regularization
performed.
13.5.1.1. Approximations
From a theoretical point of view, the most consistent of the afﬁne approximations
of a function A in the neighborhood of a point xn is given by its ﬁrst order Taylor
expansion4 around xn:
A(x) = A(xn) + ∇xA(xn)(x −xn) + O

(x −xn)2
.
In the algebraic framework, the calculation of An
Δ= ∇xA(xn) can be done sim-
ply. If φn = (I −GOXn)−1φ0 represents the ﬁeld at the object corresponding
to contrast xn and Ψn is the corresponding diagonal matrix, An can be written:
An = GM

I + Xn(I −GOXn)−1GO

Ψn.
Nevertheless, such an approximation has not always been used. The Born iterative
method [WAN 89] (BIM), which aims to solve each of the coupled equations (13.3)-
(13.4) successively for one of the variables (contrast or ﬁeld at the object), considers
an approximation via a matrix of form An = GMΨn corresponding to a rougher
linearization of the model (note that, in this case, bn = 0 since A(xn) = Anxn).
The above type of approximation is, however, taken into account in the distorted
Born iterative method (DBIM) [CHE 90] and the Newton-Kantorovitch method
4. Note, however, that such an expansion cannot be deﬁned so simply for functions of complex
variables, with complex values. In fact, the gradient operator is not deﬁned for such functions.
Nevertheless, this relation can be applied and calculated from an equivalent real notation (sep-
arating the real and imaginary parts) of the direct model equations.

Diffraction Tomography
349
(NKM) [JOA 91], which have proved to be the same although they were put forward
using distinct approaches.
Note that the computing cost for approximation matrices is more or less equivalent
to that of solving the direct problem.
13.5.1.2. Regularization
The regularization performed by the BIM method is of the Tikhonov-type, i.e., via
quadratic penalization on x. This method can thus be directly interpreted in terms of
minimizing J MAP.
The NKM and DBIM methods introduce a penalization that is still quadratic but
on x −xn and not directly on x.
13.5.1.3. Interpretation
In the framework proposed in section 13.4, a successive linearization scheme can
be envisaged for minimizing the J MAP criterion [CAR 97a]:
1) calculation of matrix An and vector bn corresponding to the afﬁne approxima-
tion of the direct problem around a current value for object xn;
2) minimization of the criterion Jn = ∥y −Anx + bn∥2 + λΦ(x).
At each iteration of this algorithm, criterion J MAP is approximated by a convex cri-
terion Jn having the same value at xn and the same slope at this point. Of course,
there is no theoretical guarantee of convergence for this algorithm and it may diverge.
Similarly, any convergence that may exist and the stationary point reached depend on
how the algorithm is initialized. However, when the algorithm does converge towards
a stationary point x∞, this point is a stationary point of the J MAP criterion (i.e., its
gradient is zero).
Note that the algorithm corresponding to BIM also consists of approximating the
J MAP criterion (for Φ(x) = ∥x∥2) by a series of convex criteria having the same
value at xn but not having the same slope at xn. Thus, even when this algorithm
converges, the solution does not necessarily correspond to a stationary point of J MAP.
The regularization performed on x−xn for DBIM and NKM does not allow them to be
interpreted in this framework but it has been observed [CHE 90] that DBIM diverges
more easily than BIM, apparently because of this regularization and, for NKM, a
scheme to adapt the regularization parameters had to be introduced in [JOA 91] to
avoid this drawback.
To conclude on these methods, it is worth noting that all the regularization methods
for linear inverse problems can be used to solve nonlinear problems by successive
linearizations (in particular the well known truncated singular value decomposition

350
Bayesian Approach to Inverse Problems
(TSVD) as proposed in [ERI 96]). However, such methods only try to stabilize each
linear inverse problem. In contrast, the successive linearization scheme to minimize
the criterion deﬁned in section 13.4 allows the nonlinear problem to be regularized by
introducing prior information on the object under study.
13.5.2. Joint minimization
A second approach used for solving this nonlinear problem aims at the simultane-
ous calculation of x and φ that satisfy the coupled equations (13.3)-(13.4). For this, a
joint criterion on x and φ is deﬁned, having the form:
K(x, φ) = αM ∥y −GMXφ∥2 + αO ∥φ −φ0 −GOXφ∥2 + λΦ(x, φ).
(13.13)
Note that, here, we are in an algebraic framework whereas the methods based on such
a criterion have usually been introduced in a functional framework. Such criteria ap-
peared for the diffraction tomography problem in [KLE 92], in parallel with [SAB 93],
and have been widely used since then.
Certain differences have become apparent between the proposed methods, which
we will look at more closely now:
– various values have been proposed for parameters αM and αO present in the crite-
rion (e.g. in [KLE 92] and [SAB 93]), without the values being justiﬁed theoretically;
– differences also appear in the regularization term sometimes introduced into the
criterion. At the beginning, no regularization was taken into consideration [KLE 92,
SAB 93]. Then a joint penalization on x and φ was proposed (e.g. in [BAR 94]).
Finally, a regularization on x only was considered, with an energy Φ corresponding
to Gibbs-Markov models (e.g. in [CAO 95] and [BER 95]);
– ﬁnally, various optimization techniques have been used in order to calculate this
solution: gradient-type local optimization (e.g. in [KLE 92, SAB 93, BAR 94]); or
global stochastic optimization based on simulated annealing [CAO 95] (for a non-
convex function Φ).
The link between criteria K (13.13) and J MAPJ (13.10), presented in section 13.4.2
as an artiﬁce of calculation, is obvious (the only difference coming from the presence
of φ in function Φ). Furthermore, this is the approach used in [CAO 95], considering
the errors on each of the coupled equations, without justiﬁcation of these hypotheses.
If regularization only concerns x, criterion (13.13) corresponds to the J MAPC criterion
penalized by the error of the norm on constraint (13.12). The result of such mini-
mization is not guaranteed to verify constraint (13.12) and the solution thus does not
correspond to that deﬁned by the joint approach in section 13.4.2. To this end, an
algorithm for optimization under constraints, based on the augmented Lagrangian has
been proposed [CAR 96]. Note that, as the convexity conditions of the criterion and
of the constraint are not fulﬁlled, such an algorithm has no guarantee of convergence.

Diffraction Tomography
351
Finally, we observe that the calculation of the criterion, and thus its minimization,
do not require the direct problem to be solved. The criterion computing cost is thus
reasonable compared with the solution of the direct problem. The price to be paid
to be able to have this reduced computing cost is the multiplication of the number
of unknowns since x and φ are sought simultaneously (recall that, in a multi-source
case, φ is the ﬁeld in the object domain for each source, and the number of unknowns
is thus multiplied by the number of sources plus one).
13.5.3. Minimizing MAP criterion
A third approach to the problem is to directly minimize the least squares criterion
∥y −A(x)∥2, regularized (i.e., the J MAP criterion deﬁned in section 13.4.1) or not.
Note that, for such an approach, it is not necessary to deﬁne operator A explicitly; it is
sufﬁcient to have an algorithm that allows the direct problem to be solved. However,
knowledge of the direct problem model and the highlighting of some of its particular
structures can enable speciﬁc optimization algorithms to be implemented.
The importance of regularization has been amply stated in this book. Neverthe-
less, it should be pointed out that many works deﬁne the solution as minimizing the
non-regularized criterion. For example, in [GAR 91], a simulated annealing global
optimization algorithm is used. As the simulations are performed without noise on
the measurements (some call this the inverse crime), this algorithm has given good
results, even without regularization. In [HAR 95], a conjugate gradient algorithm is
proposed in a functional framework and reconstructions from noisy measurements are
presented. Regularization is not explicitly introduced in the criterion but the algorithm
is stopped before the solution ceases to be satisfactory.
As we saw in section 13.4.3, the nonlinearity of the direct model makes the J MAP
criterion non-convex and local minima may be present. Thus, prudence is necessary
before implementing an algorithm to minimize this criterion.
In the most favorable situations, for which the least squares criterion is unimodal
– the same is true for the J MAP criterion when function Φ is itself convex – the meth-
ods of minimization by successive linearizations of section 13.5.1 and by optimization
under constraints of section 13.5.2 generally enable the solution to be calculated. In
addition, such methods have a considerably lower computation cost than the conven-
tional gradient optimization techniques, which require the direct problem to be solved
for each evaluation of the J MAP criterion. Thus, it does not seem useful to try to min-
imize the J MAP criterion directly in these situations, except if a prior model with a
non-convex function Φ is taken into account [LOB 97].
In more difﬁcult situations for which the criterion has local minima, the use of
global optimization techniques to minimize the J MAP criterion have to be envisaged.

352
Bayesian Approach to Inverse Problems
Indeed, local optimization techniques – in particular those presented in sections 13.5.1
and 13.5.2 when they converge – may get stuck in a local minimum. This problem
is raised in [GAR 91], where the amount of available data is small and the interest of
a global optimization technique, simulated annealing, is pointed out. In [CAR 95] a
deterministic global optimization algorithm, based on gradual non-convexity (GNC),
is proposed and, in [CAR 97b], an algorithm for coordinate-wise global optimization
(Iterative Coordinate Descent ICD) is put forward. Both these algorithms take advan-
tage of particular structures of direct model A(x) to try to carry out the optimization
globally. Nevertheless, theoretical convergence towards the global minimum is not
guaranteed.
As an illustration, we show the results of simulations in a difﬁcult conﬁguration
(see [CAR 96] for more details on these simulations). We have at our disposal mea-
surements taken at 8 sensors for waves emitted from 8 distinct positions. The measure-
ments contain noise (signal-to-noise ratio 20 dB) and the object has been discretized
with a grid of 11 × 11 pixels (121 unknowns for 64 data points). The maximum con-
trast is 5.5. In Figure 13.3, we have represented the permittivity of the original object,
(a) original image
(b) minimization by conjugate gradient
(c) minimization by GNC
(d) minimization by ICD
Figure 13.3. Example of reconstruction by minimization of J MAP in a difﬁcult conﬁguration

Diffraction Tomography
353
its reconstruction by minimization of the J MAP criterion by conjugate gradient and by
GNC for a potential function | · |1,1, then by ICD for a potential function | · |. On this
example, we can clearly see the presence of a local minimum in which the conjugate
gradient algorithm gets stuck. Note that a Tikhonov regularization (Gaussian model:
Φ(x) = ∥x∥2) would give a far less satisfactory solution than the Markov models
considered here.
13.6. Conclusion
This study of a speciﬁc nonlinear inverse problem emphasizes some important
points for solving other nonlinear problems. In the Bayesian framework, a regularized
solution to the problem can be deﬁned as an estimate in the maximum a posteriori
sense, by explicitly introducing prior information on the object to be reconstructed.
Such a solution can be deﬁned as soon as the direct model presents an explicit rela-
tionship between the unknowns and the data. If Gaussian perturbations on these data
are considered, the solution is a regularized version of the least squares solution. How-
ever, it can be interesting to take advantage of formulations of the direct model other
than this explicit relationship when deﬁning the solution. These different formulations
lead us to solve a non-convex optimization problem, with or without constraints.
As for linear inverse problems, the prior information is introduced in the form
of probabilistic models or, equivalently, as a penalization term on the least squares
criterion. A large number of image models have been used for linear inverse problems
in the past 15 years or so and have proved to be much more efﬁcient than energy
penalization as used by Tikhonov.
Calculation of the solution requires optimization algorithms to be implemented,
which themselves require the direct problem to be solved, in its explicit form or in
another form. Discretizing this model in a simple form that can be calculated at low
cost is an important step in solving the problem.
It has been possible to study most of the existing solving methods in this frame-
work, in terms of optimization techniques. Various techniques need to be used accord-
ing to the difﬁculty of the problem. For this problem, strong contrasts in the object
and a small amount of data entail the presence of local minima in the criterion. In
favorable situations, successive linearization of the direct model can be used for lo-
cal optimization of the criterion. Similarly, the speciﬁc form of the model – coupled
equations in our case – can be used to calculate the solution by performing a local
optimization under constraints. Nevertheless, in situations where local minima are
present, global optimization techniques, with much higher computing costs, have to
be used. Obviously, the latter can take advantage of the structure of the discretized
direct model.

354
Bayesian Approach to Inverse Problems
13.7. Bibliography
[BAR 94] BARKESHLI S., LAUTZENHEISER R. G., “An iterative method for inverse scatter-
ing problems based on an exact gradient search”, Radio Sci., vol. 29, num. 4, p. 1119-1130,
July-Aug. 1994.
[BER 95] VAN DEN BERG P. M., KLEINMAN R. E., “A total variation enhanced modiﬁed
gradient algorithm for proﬁle reconstruction”, Inverse Problems, vol. 11, p. L5-L10, 1995.
[BOL 83] BOLOMEY J. C., PERONNET G., PICHOT C., JOFRE L., GAUTHERIE M.,
GUERQUIN KERN J. L., “L’imagerie micro-onde active en génie biomédical”,
in
L’imagerie du corps humain, p. 53-76, Les Éditions de physique, Paris, 1983.
[BOU 93] BOUMAN C. A., SAUER K. D., “A generalized Gaussian image model for edge-
preserving MAP estimation”, IEEE Trans. Image Processing, vol. 2, num. 3, p. 296-310,
July 1993.
[CAO 95] CAORSI S., GRAGNANI G. L., MEDICINA S., PASTORINO M., PINTO A., “A
Gibbs random ﬁelds-based active electromagnetic method for noninvasive diagnostics in
biomedical applications”, Radio Sci., vol. 30, num. 1, p. 291-301, Jan.-Feb. 1995.
[CAR 95] CARFANTAN H., MOHAMMAD-DJAFARI A., “A Bayesian Approach for Nonlinear
Inverse Scattering Tomographic Imaging”, in Proc. IEEE ICASSP, vol. IV, Detroit, MI,
p. 2311-2314, May 1995.
[CAR 96] CARFANTAN H., Approche bayésienne pour un problème inverse non linéaire en
imagerie à ondes diffractées, PhD thesis, University of Paris XI, France, Dec. 1996.
[CAR 97a] CARFANTAN H., MOHAMMAD-DJAFARI A., “An overview of nonlinear diffrac-
tion tomography within the Bayesian estimation framework”, in Inverse Problems of Wave
Propagation and Diffraction, p. 107-124, Lecture Notes in Physics, Springer Verlag, New
York, NY, 1997.
[CAR 97b] CARFANTAN H., MOHAMMAD-DJAFARI A., IDIER J., “A single site update al-
gorithm for nonlinear diffraction tomography”, in Proc. IEEE ICASSP, Munich, Germany,
p. 2837-2840, Apr. 1997.
[CHE 90] CHEW W. C., WANG Y. M., “Reconstruction of two-dimensional permittivity dis-
tribution using the distorted Born iterative method”, IEEE Trans. Medical Imaging, vol. 9,
p. 218-225, June 1990.
[COL 92] COLTON D., KRESS R., Inverse Acoustic and Electromagnetic Scattering Theory,
Springer Verlag, New York, NY, 1992.
[ERI 96] ERIKSSON J., Optimization and regularization of nonlinear least squares problems,
PhD thesis, Umeå University, Sweden, June 1996.
[GAR 91] GARNERO L., FRANCHOIS A., HUGONIN J.-P., PICHOT C., JOACHIMOWICZ N.,
“Microwave imaging – Complex permittivity reconstruction by simulated annealing”, IEEE
Trans. Microwave Theory Tech., vol. 39, num. 11, p. 1801-1807, Nov. 1991.
[HAR 87] HARRINGTON R. F., “The method of moments in electromagnetics”, J. Electro-
magnetic Waves Appl., vol. 1, num. 3, p. 181-200, 1987.

Diffraction Tomography
355
[HAR 95] HARADA H., WALL D. J. N., TAKENAKA T., TANAKA M., “Conjugate gradient
method applied to inverse scattering problem”, IEEE Trans. Ant. Propag., vol. 43, num. 8,
p. 784-791, 1995.
[HOW 86] HOWARD A. Q. J., KRETZSCHMAR J. L., “Synthesis of EM geophysical tomo-
graphic data”, Proc. IEEE, vol. 74, num. 2, p. 353-360, Feb. 1986.
[JOA 91] JOACHIMOWICZ N., PICHOT C., HUGONIN J.-P., “Inverse scattering: An iterative
numerical method for electromagnetic imaging”, IEEE Trans. Ant. Propag., vol. AP-39,
num. 12, p. 1742-1752, Dec. 1991.
[KAK 88] KAK A. C., SLANEY M., Principles of Computerized Tomographic Imaging, IEEE
Press, New York, NY, 1988.
[KLE 92] KLEINMAN R. E., VAN DEN BERG P. M., “A modiﬁed gradient method for two-
dimensional problems in tomography”, J. Comput. Appl. Math., vol. 42, p. 17-35, 1992.
[LOB 97] LOBEL P., BLANC-FERAUD L., PICHOT C., BARLAUD M., “A new regularization
scheme for inverse scattering”, Inverse Problems, vol. 13, num. 2, p. 403-410, Apr. 1997.
[SAB 93] SABBAGH H. A., LAUTZENHEISER R. G., “Inverse problems in electromagnetic
nondestructive evaluation”, Int. J. Appl. Electromag. Mat., vol. 3, p. 235-261, 1993.
[TAR 82] TARANTOLA A., VALETTE B., “Inverse problems = quest for information”, J. Geo-
phys., vol. 50, p. 159-170, 1982.
[TAR 87] TARANTOLA A., Inverse Problem Theory: Methods for Data Fitting and Model
Parameter Estimation, Elsevier Science Publishers, Amsterdam, The Netherlands, 1987.
[WAN 89] WANG Y. M., CHEW W. C., “An iterative solution of the two-dimensional electro-
magnetic inverse scattering problem”, Int. J. Imag. Syst. Tech., vol. 1, p. 100-108, 1989.
[ZOR 91] ZORGATI R., DUCHENE B., LESSELIER D., PONS F., “Eddy current testing of
anomalies in conductive materials, part I: Qualitative imaging via diffraction tomography
techniques”, IEEE Trans. Magnetics, vol. 27, num. 6, p. 4416-4437, 1991.


Chapter 14
Imaging from Low-intensity Data
14.1. Introduction
Estimation of digital imagery giving rise to photon-limited data forms an impor-
tant subset of inverse problems. Its applications are numerous in medical diagnostic
imaging, astronomy and industrial inspection. Due to the nature of the data collec-
tion process, this class of inverse problems presents a number of challenges speciﬁc to
low signal-to-noise ratios (SNRs) and Poisson likelihood functions. These problems
are discussed in this chapter within a Bayesian estimation framework, which enables
numerous modeling and optimization options and leads to improved image estimates.
At the most fundamental physical level, many measurement devices register the
superposition of discrete physical events. Emissions from light-emitting diodes, cur-
rents from charge-coupled devices (CCDs), or even measurement of simple electrical
currents have discrete phenomena as their basis. The data available as input to most in-
verse problems represents the superposition of a sufﬁcient number of events to regard
each datum, after appropriate normalization, as continuously distributed. In some
cases as well, underlying Poisson characteristics of a measurement process may be
of low enough variance to make secondary system noise effects the dominant noise
mechanism. Many image recovery tasks, for example, deal with sufﬁcient quantities
of light at each detector that corruption due to shot noise and other disturbances can be
quite well approximated as contributing to the variance of Gaussian random variables.
This typically holds as well for problems with limited numbers of measurements, as in
Chapter 12, where each attenuation measurement may involve enough X-ray photons
Chapter written by Ken SAUER and Jean-Baptiste THIBAULT.

358
Bayesian Approach to Inverse Problems
Figure 14.1. Reconstructions from clinical scans on General Electric’s Advance positron
emission tomographic diagnostic imaging system (data courtesy
of General Electric Imaging Systems, Inc.)
that the standard Gaussian asymptotic approximation for the distribution of maximum
likelihood (ML) attenuation estimates [BIC 77] is sufﬁciently accurate.
A growing number of estimation settings feature photon-limited measurements, in
which the Poisson nature of the distribution is of importance, as we will see below.
This situation may arise when newly developed, highly sensitive instruments are avail-
able to measure very weak signals, as in image recovery from faint signals emitted far
from the earth and received by optical telescopes [SNY 90]. Scintigraphic images,
highly useful for medical diagnostics or industrial inspection, are formed from very
low intensity planar measurements [IIN 67, NGU 99]. Emission tomographic imag-
ing relies on measurements often consisting of a handful of photons due to patient
safety requirements, for both emission rate estimates and attenuation maps derived
from low-intensity transmission scans [OLL 97]. All these applications require esti-
mation techniques tailored to the nature of the data in order to produce usable imagery.
Figure 14.1 shows the level of image quality achievable with current clinical positron
emission tomography (PET) scanners. The quality of these images is signiﬁcantly
lower than is possible with X-ray CT scans, but the images are also of different clini-
cal signiﬁcance: X-ray offers excellent detail in anatomical diagnostics, whereas PET
provides physiological information.
In any of the above settings, it may be worthwhile or even necessary to depart
from standard quadratic measures of estimate afﬁnity to data and consequent linear
techniques for estimation. Poisson-distributed data has variance equal to its mean,
which makes measurement noise properties dependent on the unknown signal. The
signal is typically physically constrained to be non-negative, a constraint which can
aid estimates but complicate the optimization necessary for calculation of estimators.
However, application of models true to the quantum nature of data can yield improve-
ment in both the quality of recovered signals and the inference of system parameters.

Imaging from Low-intensity Data
359
In subsequent sections we will explore key elements of commonly occurring weak-
signal cases in modern inverse problems. The unknown parameters to be estimated, be
they elements of the object of an inverse problem or “hyperparameters” underlying the
models, are assumed continuously distributed. After considering important statistical
modeling issues, we examine various algorithms designed to efﬁciently calculate the
desired estimates.
14.2. Statistical properties of common low-intensity image data
A number of probability distributions may be of interest for low intensity mea-
surements. Those most unique to this framework involve data which takes the form of
counting discrete events and is typically modeled as Poisson-distributed. The follow-
ing discussion, however, could be applied with small modiﬁcations to others, such as
binomial.
14.2.1. Likelihood functions and limiting behavior
We shall use the vector X to represent the unknown object of the inverse problem
at hand, and Y as the random observations. Scalar entries in any of the vectors will
be subscripted, as for example Xj for a single image pixel and Yi for one measure-
ment datum. Should X not be viewed as random, of course, no distribution will be
necessary for it. As in many other problems, the data in weak signal cases are often
most accurately modeled as the superposition of the signal and multiple noise sources.
Given appropriate independence characteristics, this superposition is frequently mod-
eled as Gaussian through application of the Central Limit Theorem. The primary dis-
tinguishing feature here of data in Y is that the information-bearing component of Y
will be restricted to the non-negative integer values due to small numbers of discrete
events in the signal. We will assume Y to be the sum of perhaps several independent
phenomena, with one component purely discretely valued and the other continuous,
or
Y = N + B,
where each element in B will be Gaussian for our discussion, with known mean γi
and variance σ2
i independent of X. Each Ni will consist of two Poisson components:
one has a mean μi which is dependent on X and another has a mean βi which is
independent of X. The distribution of Ni, conditioned on X, has the form
Pr(Ni = ni | X = x) = 1
ni! exp {−(μi(x) + βi)} (μi(x) + βi)n
i
(14.1)
and the likelihood function Pr(Y = y | X = x) will be formed by the convolution of
the Gaussian probability density of Bi with equation (14.1). The mean of Yi is
μYi(x) = μi(x) + βi + γi.

360
Bayesian Approach to Inverse Problems
Most problems in the conventional Bayesian framework use a linear description of the
relationship between x and μ, which we will express as
μ(x) = Ax,
(14.2)
with matrix A as a discrete description of, for example, the blurring by camera motion
or system optics of image pixel values, or the line integrals in emission tomographic
measurements. A may also be adjusted to include calibrated detector efﬁciencies
and internal attenuation of measured photons. Transmission tomographic problems
have a slightly more complicated function as μ, but it is typically modeled with x
entering the distribution in form Ax. This makes the characteristics of the two basic
linear tomography systems sufﬁciently similar for us to discuss primarily the simpler
emission case and point to the transmission case only where it is of particular interest.
Emission tomographic data is often corrected before image reconstruction for the
effects of attenuation by the patient [MEI 93, OGA 91], scatter [OLL 93, PAN 97]
and accidental coincidence accidental coincidences [POL 91]. Such corrections are
critical in advance of using deterministic methods such as ﬁltered backprojection. In
contrast, statistical methods enable inclusion of models for these degradations in the
observation model. Attenuation reduces the probability of detection of each emitted
photon; this translates mathematically into a multiplicative factor for each element in
A, similarly to a loss in detector efﬁciency. Rather than subtracting accidental coin-
cidences and scatter from raw data, we may model them as contributing to random
vector N with expected values in β. In the following, we assume that these effects
are included in A and β. A more precise approach would model scatter by a smooth-
ing among the columns of A, but the spatial extent of these interactions make this
computationally costly.
14.2.2. Purely Poisson measurements
The degree of importance of the Poisson nature of N in formulating and solving
the low-intensity estimation problem depends on the relative variances of the compo-
nents and on the absolute intensity of the signal component expressed in μi(x). Let us
ﬁrst consider the simplest case of the Poisson problem, where σ2
i , γi, βi ≪μi(x) and
the distribution of Yi is approximately that of Ni. In this case the measure for Yi con-
ditioned on {X = x} is a probability mass function Pr(Yi = yi | X = x), illustrated
for a scalar case of both variables in Figure 14.2. Poisson-distributed data possesses
a fundamentally important second characteristic in having a single parameter for both
their means and variances. Assuming X controls the rate of photon or particle emis-
sion, Pr(Yi = yi | X = x) will be independent of any other parameters. This may
be an advantage in modeling simplicity, but introduces a signal-dependent noise vari-
ance which has to some extent impeded the application of least-squares estimation
techniques to such problems. The Central Limit Theorem can be used to argue that

Imaging from Low-intensity Data
361
as the expected rate of events rises, the distribution of a Poisson variable normalized
by the square root of its mean converges to a Gaussian. However, though the discrete
form of the distribution may be of decreasing importance for large numbers of events,
the dependence of both mean and variance on X remains. As mean and variance are
equal, the SNR increases linearly with variance, opposite the Gaussian.
For estimation or reconstruction of an estimate 3
X from such data, we are interested
in the characteristics of the likelihood functions in terms of the variable representing
underlying intensity, reﬂectivity, or whatever parameter in X determines the rate of
counts at detectors. Therefore, the estimation problem differs from those whose prin-
ciple corruption is not at the photon counting level only in the shape of the likelihood
function as indexed by the continuously-varying intensity parameter. The distribu-
tion of variates Yi approaches a Gaussian for large means, but the Poisson differs
signiﬁcantly for single-digit means. In Figure 14.2 we plot Poisson densities and the
corresponding log-likelihood functions with the observed variables taking their mean
0
5
10
15
20
25
30
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
(a) μi = 10
60
70
80
90
100
110
120
130
140
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
(b) μi = 100
0
5
10
15
20
25
30
−25
−20
−15
−10
−5
0
5
10
15
(c) yi = 10
60
70
80
90
100
110
120
130
140
348
350
352
354
356
358
360
362
(d) yi = 100
Figure 14.2. Above: probability mass function of Poisson-distributed variable with parameter
μi; below, log-likelihood function of the Poisson parameter for observation Yi = yi

362
Bayesian Approach to Inverse Problems
values. In other words, in the upper plots we ﬁx μi and consider the distribution of Yi
while on the right we ﬁx the outcome Yi = yi and plot the likelihood in μi(x).
Just as asymptotic approximations are often made to describe the distribution of
Yi, the likelihood function in X may proﬁtably be approximated by simpler functions.
As a sort of counterpart to the notion of the similarity of the distribution of high-mean
Poisson counts to Gaussian (see Figure 14.2), the log-likelihood in μi(x) is locally
well-approximated by an appropriate polynomial. This may be used for both design
of simpliﬁed estimators and analysis of their behavior, as discussed below.
14.2.3. Inclusion of background counting noise
Adding a βi to the discussion above is relatively simple, as it adds only a shift in
μi(x) relative to the plots of Figure 14.2. Such an additive component of the Poisson
mean is in practice often due to background noise, such as accidental coincidences in
PET, or scattered photons with little or no dependence on local characteristics of X.
In emission tomography,provided equation (14.2) is valid, βi may be treated as though
it were a member of an augmented x if its estimation is desired. From equation (14.1),
it is simple to show that the log-likelihood is also a concave function of the unknown
parameters.
In transmission tomography, the addition of the background counts may destroy
the concavity of the log-likelihood. This is an important consideration in optimization,
as concave functions do not have multiple local minima. Here, the Poisson parameter
is, for integral density li(x) and input dosage di, di exp(−li(x)) + βi. For βi = 0,
the log-likelihood is strictly convex, but as demonstrated in the plots of Figure 14.3, it
loses this property in the presence of relatively weak background counts. The gravity
of the non-convexity in optimization may vary with initial conditions and consistency
among separate measurements. Should the background count rate approach the ob-
served count yi, as in the dashed line, low curvature of the log-likelihood at its max-
imum indicates potentially high variance in any estimator and serious consequences
of the non-convexity phenomenon in estimator performance despite its lesser severity.
With βi ≫yi, the concavity is recovered; however, this case is of minimal practical
importance.
14.2.4. Compound noise models with Poisson information
While the purely Poisson model is considered reasonable for the output of solid-
state detectors in most nuclear medicine imaging systems, data from arrays of CCDs in
common optical sensors are corrupted by additional noise inherent in reading out the
charge at each pixel. This read-out noise is characterized as Gaussian, independent
of the underlying image data x [SNY 93]. These Gaussian parameters (γi, σi) are
generally available via calibration measurements.

Imaging from Low-intensity Data
363
0
2
4
6
8
10
−400
−300
−200
−100
0
100
200
300
400
Attenuation parameter
Log−likelihood
Figure 14.3. Log-likelihoods for integral attenuation parameter (li) in transmission
tomography with background Poisson noise observed at count y. Total Poisson parameter =
di exp(−li) + βi; di = 1,000 for all plots. ( ) yi = 100, βi = 0;
(···) yi = 100, βi = 10; (---) yi = 100, βi = 50
The resulting compound Poisson-Gaussian model for Yi has a density resulting
from the convolution of the two, or
p(yi | X = x) =
∞

n=0
1
n! exp {−μi(x) + βi} (μi(x) + βi)n
1
σ
√
2π
exp
−(yi −n −γi)2
2σ2
i
4
. (14.3)
The Gaussian component will normally be assumed to have variance signiﬁcantly
greater than one, yielding a density and log-likelihood as illustrated in Figure 14.4.
Such compound noise will have distributions depending on the components’ param-
eters, but as is clear from the plots, a Gaussian approximation is quite close in the
vicinity of the maximum.
14.3. Quantum-limited measurements in inverse problems
The ML estimate of scalar means from the single quantum-limited observations
above is easily seen as μi = yi. However, μi(x) for each measurement is typically
parameterized by a non-trivial function of vector x, and Y is of high dimension,
which classiﬁes the estimation as a nontrivial inverse problem. The dependence is
often modeled as linear, with the means μ(x) satisying (14.2). A may represent such
factors as local blurring due to limited instrument resolution, integrals along typical
14.3.1. Maximum likelihood properties

364
Bayesian Approach to Inverse Problems
0
5
10
15
20
25
30
35
40
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Probability density
(a)
0
5
10
15
20
25
30
35
40
−10
−9
−8
−7
−6
−5
−4
−3
−2
Log−likelihood
Mean parameter
(b)
Figure 14.4. Compound noise model p(y | X = x) with observations equal to the sum of
independent Poissons and Gaussians. (a) ( ) Probability density for μi(x) = 20,
βi = γi = 0, σi = 5; (···) Gaussian pdf of mean 20 and variance 45; (b) ( ) log-likelihood as
function of μi(x) with yi = 20, βi = γi = 0, σi = 5; (···) Poisson log-likelihood as a function
of μi(x) with mean of μi(x) + 25 and yi = 45; (---) Gaussian log-likelihood as function of
μi(x) with mean of μi(x), variance μi(x) + 25, and yi = 20
paths of photons in tomography or any expected fractions of detections of photons
from various points of origin. Quantum-limited estimation problems are therefore
more complicated than the simple scalar plots of Figures 14.2-14.4 might indicate.
Fortunately for the problems’ tractability, the individual data can usually be accurately
modeled as independent conditioned on X, making the total log-likelihood the sum
of those of scalar measurements. A standard formulation of an emission tomographic
data distribution, for example, is
Pr(Y = y | X = x) =
M
1
i=1
1
yi!exp

−(aT
i∗x + βi)

(aT
i∗x + βi)yi
with ai∗x the product of the ith row of A with x.
The ML estimator
3
X
ML = arg max
x
log Pr(Y = y | X = x)
may be roughly approximated by solution of the equation
E(Y | X = x) ≃y
or, in the linear case
Ax + β + γ ≃y.

Imaging from Low-intensity Data
365
Thus, the low-intensity inverse problems share the principle features of more conven-
tional ones. A good number of Poisson data problems can, in fact, be solved accurately
with the actual log-likelihood replaced by
log Pr(Y = y | X = x) ≃−1
2(y −μY (x))T D(y −μY (x)) + const.,
(14.4)
with D diagonal and a simple function of the data [BOU 96]. Such weighted least-
squares formulations are more widely understood than the exact ML and may provide
acceptable estimates in many applications [TSU 91, KOU 96]. Higher degree polyno-
mials may extend the applicability of such approximations [FES 95].
As explained in section 14.2.2, a principal difference in the low-intensity case is
that lower SNR mean lower curvature in the log-likelihood functions above. This re-
sults in greater inconsistencies in measurements due to noise, and values of μi(3
X ML)
which are distant from the maxima of the scalar log-likelihoods. Thus, while the poly-
nomial approximations to these functions may be quite effective in high-count data,
estimator accuracy may demand the precise likelihood functions in low SNR [THI 00].
Our discussions thus center on solving the problems posed by exact discrete-data like-
lihoods.
Performance bounds such as Cramèr-Rao may be used to analyze ML perfor-
mance, similarly to canonical inverse problems in estimation.These measures depend
on the characteristics of the expectation of the log-likelihood Hessian, with the ap-
proximate form AT DA for common Poisson problems [BOU 96]. The diagonal ma-
trix D weights according to values in y, with values inversely proportional to data for
μ(x) = Ax. While this suggests poorer performance for higher counts, we recall that
for the Poisson in general, rising data variance is accompanied by increasing means
and increasing SNR and estimator performance improves in higher counts.
The character of the Hessian varies among applications, but the primary feature of
interest in inverse problems is that eigenvalues corresponding to high frequency infor-
mation are small, indicating its suppression in the forward problem. This translates
into very high variances in these components of 3
X ML. The ML estimator typically
has unacceptably poor performance in low intensity data [SNY 85]. A popular rem-
edy to this problem is iterative approximation of the ML solution, commencing with a
smooth image and stopping iterations of algorithms such as expectation-maximization
(EM) long before convergence [LLA 89]. Alternatively, the ML problem is attacked
by block-iterative techniques such as ordered subsets EM (OS-EM) [HUD 94]. OS-
EM achieves rapid initial convergence toward the ML estimate but requires modiﬁca-
tion to guarantee complete convergence.

366
Bayesian Approach to Inverse Problems
14.3.2. Bayesian estimation
The inadequacy of least-squares and ML methods in low-intensity measurements
is well known and has given rise to alternative formulations in a variety of low-
intensity application areas. Bayesian methods, in particular, have found frequent ap-
plication and success in these problems. Among often-cited early developments in
Bayesian image analysis were applications to emission tomography [GEM 85,
SNY 85, LEV 87]. Our particular set of reconstruction problems differs little in fun-
damentals from many others in this volume. The maximum a posteriori probability
(MAP) estimator has the usual form
3
X
MAP = arg max
x
(log Pr(Y = y | X = x) + log p(x)) .
(14.5)
The attributes of the MAP estimate depends on the balance between the two terms in
equation (14.5). For a ﬁxed a priori density p(x), low SNR results in relatively light
weighting of the log-likelihood term in (14.5). This makes Bayesian MAP estimation
in this problem potentially sensitive to the choice of the a priori model expressed in
density p(x). The log a priori may, in fact, dominate the cost function if its parameters
are not heuristically adjusted to data levels.
Any of the image models discussed in previous chapters may in principle be ap-
plied to our problem. The most commonly found are variations of Markov random
ﬁelds (MRFs), which provide relative simplicity in calculation and hyperparameter
estimation. The examples in this chapter all include the generalized Gaussian MRF
(GGMRF) [BOU 93], whose potential function has the form
U(x) =

{i,j}∈C
|xi −xj|q
qαq
.
For q > 1, the GGMRF is among the convex models in x, which maintain continuity
of MAP solutions as functions of data, and relative simplicity of optimization. A host
of other models are available with a similar performance given appropriate selection
of hyperparameters (see Chapter 7). As is true of many others, the GGMRF includes
the Gaussian MRF as a limiting case of q = 2. Much has been written concerning the
advantages of edge-preserving prior models such as the GGMRF in Bayesian image
reconstruction [BLA 87, GEM 84, GRE 90]. The Gaussian MRF applies heavy penal-
ties to large differences among neighboring pixels, thereby discouraging the sharp dis-
continuities typical of many real images. Those models with a potential increasing less
rapidly, such as GGMRF with q ≃1 or the logcosh model near its edge-preserving
limit, allow these discontinuities to form naturally in estimates and can greatly im-
prove reconstructions, particularly for piece-wise homogeneous objects. Non-convex
priors produce even more dramatic edge rendering [BLA 87, CHA 97], often at the
cost of desirable convergence properties. In more general, low SNR inverse problems
the formation of sharp, low-contrast edges may not be a universal beneﬁt. Contour-
ing and unnatural texture may be visually disturbing to human interpreters of image

Imaging from Low-intensity Data
367
content, an important consideration in most medical imaging applications. The two
estimates in Figure 14.5 demonstrate strong qualitative difference in limited-count
data. The Gaussian MRF has two strong advantages: it helps form a simpler, more
rapidly converging iterative estimate, and the solution’s degradation in falling SNR
is graceful and of a nature understood by experienced observers. We expect that the
Gaussian will remain a popular option in low-intensity data.
In all the applications discussed here, unknown image X may be assumed non-
negative due to physical considerations. Should high quality measurements be avail-
able, such as in conventional X-ray computed tomography, a Bayesian image esti-
mate may be little affected by enforcement of a non-negativity constraint. Higher-
variance estimates resulting from low-intensity data violate this constraint much more
(a) ﬁltered backprojection (linear) reconstruction
(b) Gaussian prior model for p(x)
(c) edge-preserving version of GGMRF (q = 1.1)
Figure 14.5. MAP reconstructions from single photon emission computed tomography
(SPECT) heart perfusion data. Both (b) and (c) use ML values of scale parameter α. (Data
courtesy of T.-S. Pan, M. King and University of Massachusetts)

368
Bayesian Approach to Inverse Problems
frequently at their unconstrained optimum and are thus often appreciably improved
by limiting estimates to non-negative values. In the SPECT heart imaging example of
Figure 14.6, we see the constraint has a marked effect on the quality of reconstruction,
particularly in less densely sampled portions of the image. This data is uncorrected
for attenuation and scatter and may be improved by accurate inclusion of these effects
in the model.
Characteristics related to the low-count Poisson data model in imaging have given
rise to special models for X as well. For example, the known positivity of X also
serves as motivation for the I-divergence prior model, which has been shown to be
useful for low-intensity data [O’S 94]. The additivity of Poisson variates also facili-
tates multiscale representation of emission densities and specialized prior models for
relations among nodes [TIM 99]. Paucity of data as well as low intensity has also led
to more global, geometric approaches to reconstruction [CUN 98].
14.4. Implementation and calculation of Bayesian estimates
Any of the inverse problem formulations in the previous sections poses questions
of implementation. Some aspects are very problem-dependent, such as attenuation
and scatter correction in emission tomographic imaging. These issues arise nearly
independently of the overall rates of emission and are thus not particularly a low-
intensity imaging issue. For present purposes, we assume appropriate calibrations and
corrections have been made such that the data used as input to the Bayesian inverse
match the compound model of (14.3). The principal remaining question of implemen-
tation is thus the computational aspects of determining the Bayesian image estimate.
Our discussion centers on the MAP reconstruction, which involves relatively direct
optimization. More computationally demanding estimators such as the a posteriori
mean [TIE 94] are applied relatively infrequently. Only rarely do such inverse prob-
lems allow the tractable direct calculation of 3
X, and iterative optimization is the norm.
14.4.1. Implementation for pure Poisson model
It is physically realistic and potentially beneﬁcial to constrain 3
X to be non-
negative. This property appears to have been a major inﬂuence in the ascendance
of EM type algorithms [KAU 87, SHE 82] as the most common option for optimiz-
ing over the Poisson log-likelihood. Though EM iterations for this problem are often
viewed as a type of gradient descent, they yield a multiplicative correction at each
iteration which guarantees preservation of positivity under a strictly positive initial
condition. The kth update at pixel xj has the form
xk+1
j
= xk
j
M

i=1
Aijyi
aT
i∗xk + βi
.
(14.6)

Imaging from Low-intensity Data
369
The EM algorithm is quite general and powerful for ML estimation, though the update
step is not always so simple. The emission reconstruction update of (14.6) is as easy as
a gradient step, but no practical version exists for the transmission case. Unmodiﬁed
(a) reconstruction under non-negativity constraint
(b) reconstruction without positivity constraint
Figure 14.6. MAP reconstruction with Gaussian prior model, with or without positivity
constraint. In the latter case, maximally negative values have
greater magnitude than positive maxima

370
Bayesian Approach to Inverse Problems
EM is generally rather slow to converge, and has widely been replaced by OS-EM,
which cycles among subsets of Y, updating pixels according to equation (14.6) for
each subset. This optimization technique has also been proposed under the name
“block iterative” [BYR 96]. This sacriﬁces the guaranteed convergence of EM, but
since convergenceto the ML image is seldom desired, heuristic termination times have
proved useful. Generalizations of EM (and OS-EM) to handle the Bayesian estimate
have been developed as well [DEP 95, HEB 89], but convergence limitations remain.
Viewing the Bayesian problem as in section 14.2, a broader view of optimization
choices seems appropriate. Though we remain with the exact log-likelihood com-
ponent in our problem, its proximity to a quadratic makes conjugate gradient (CG)
methods [BEC 60] natural options. CG avoids oscillatory behavior often observed
in such ill-conditioned problems. Preconditioned conjugate gradient (PCG) may fur-
ther accelerate convergence by deﬁning a form of conjugacy closer to optimal for
the system at hand [LUE 73]. A difﬁculty for CG methods lies in the application
of non-negativity constraints on 3
X, for which several alternatives have been pro-
posed [BIE 91, MUM 94]. These constraints necessarily alter the directions of up-
dates at the expense of some convergence speed. Particularly in cases where large
numbers of pixels encounter the constraint, CG falters somewhat. Figure 14.7 shows
PCG in comparison to De Pierro’s adaptation of EM [DEP 95] and Lange’s Convex
method [LAN 90], which is a similar algorithm for transmission tomography. The
irregular path for PCG is due to the solution’s encountering the constraint.
Such optimization problems may also be solved via updates of single pixels or
small subsets of the image, just as subsets of projection data or even single measure-
ments (as in the algebraic reconstruction technique (ART) [HER 80]) are used for
OS-EM. The advantage in this “column-action” approach is that guaranteed conver-
gence can be maintained, and the reconstruction can easily be made independent of
the starting condition with convex models. Two such methods are included in Fig-
ure 14.7: space-alternating expectation-maximization (SAGE), which develops pixel
updates from an EM formulation [FES 93] and iterative coordinate descent (ICD),
which optimizes the log-likelihood directly via local quadratic approximations in one
dimension [BOU 96]. As updates are made in one dimension, enforcement of positiv-
ity is trivial for these techniques.
14.4.2. Bayesian implementation for a compound data model
Should the Gaussian component of equation (14.3) have large variance, we must
deal with a more complex likelihood function under lower SNR, and without simple,
explicit update expressions. A multiplicative, iterative algorithm for this case, sim-
ilar to EM, has been presented [SNY 93], in which each update requires evaluation
of a summation similar to (14.3). As in the preceding section, in which we consid-
ered the approximation of a Poisson log-likelihood with a low-order polynomial, we

Imaging from Low-intensity Data
371
Log a posteriori probability
0
5
10
15
20
25
30
17.0
17.5
ICD/FS
ICD/NR
SAGE-3
DePierro’s
Prec. Conj. Gradient
Iterations
(a) SPECT image
Log a posteriori probability
0
5
10
15
20
6642.5
6644.5
ICD/FS - Regular Scan
ICD/NR - Regular Scan
ICD/FS - Random Order
Convex
Prec. Conj. Gradient
Iterations
(b) X-ray CT image
Figure 14.7. Convergence of various optimization methods for MAP reconstruction with a
Gaussian prior image model
may make an approximation of the Poisson-Gaussian in order to simplify calculation,
with little effect on the quality of the estimate. Alternatively, we may apply com-
mon iterative approaches after replacing each piece of data yi with the approximation
yi + σi −γi [SNY 93]. Using this transformation, the data variance becomes equal to
the mean, which invites approximation by a Poisson distribution. Figure 14.4b shows
that, within the range of parameters illustrated, this Poisson approximation yields a
similar log-likelihood function to a conditional Gaussian whose mean and variance
both depend on μi(x). The Poisson, though, appears to offer a simpler implementa-
tion than the Gaussian.

372
Bayesian Approach to Inverse Problems
14.5. Conclusion
This chapter is dedicated to the reconstruction of images from limited intensity
data, whose discrete nature is inherent in the counting of individual photons. Though
we may view the inverse problem quite similarly to the standard Gaussian data case,
dealing with the Poisson likelihood directly is necessary to gain the full advantages of
Bayesian methods when event counts become few. We have provided a brief overview
of unique features of these problems and effective approaches for the formulation and
solution of their Bayesian estimators. The most appropriate formulation will depend
on the speciﬁcs of the problem at hand. An understanding of the physical systems
generating the observed phenomena must guide the choice of models for the most
robust, accurate reconstructions.
14.6. Bibliography
[BEC 60] BECKMAN F., “The solution of linear equations by the conjugate gradient method”,
in RALSTON A., WILF H., ENSLEIN K. (Eds.), Mathematical Methods for Digital Com-
puters, Wiley, 1960.
[BIC 77] BICKEL P. J., DOKSUM K. A., Mathematical Statistics: Basic Ideas and Selected
Topics, Holden-Day, Oakland, CA, 1977.
[BIE 91] BIERLAIRE M., TOINT P. L., TUYTTENS D., “On iterative algorithms for linear
least squares problems with bound constraints”, Linear Alg. Appl., vol. 143, p. 111-143,
1991.
[BLA 87] BLAKE A., ZISSERMAN A., Visual Reconstruction, The MIT Press, Cambridge,
MA, 1987.
[BOU 93] BOUMAN C. A., SAUER K. D., “A generalized Gaussian image model for edge-
preserving MAP estimation”, IEEE Trans. Image Processing, vol. 2, num. 3, p. 296-310,
July 1993.
[BOU 96] BOUMAN C. A., SAUER K. D., “A uniﬁed approach to statistical tomography using
coordinate descent optimization”, IEEE Trans. Image Processing, vol. 5, num. 3, p. 480-
492, Mar. 1996.
[BYR 96] BYRNE C. L., “Block-iterative methods for image reconstruction from projections”,
IEEE Trans. Image Processing, vol. 5, p. 792-794, May 1996.
[CHA 97] CHARBONNIER P., BLANC-FERAUD L., AUBERT G., BARLAUD M., “Deter-
ministic edge-preserving regularization in computed imaging”, IEEE Trans. Image Pro-
cessing, vol. 6, num. 2, p. 298-311, Feb. 1997.
[CUN 98] CUNNINGHAM G. S., HANSON K. M., BATTLE X. L., “Three-dimensional recon-
structions from low-count SPECT data using deformable models”, Optics Express, vol. 2,
p. 227-236, 1998.
[DEP 95] DE PIERRO A. R., “A modiﬁed expectation maximization algorithm for penalized
likelihood estimation in emission tomography”, IEEE Trans. Medical Imaging, vol. 14,
num. 1, p. 132-137, 1995.

Imaging from Low-intensity Data
373
[FES 93] FESSLER J. A., HERO A. O., “Complete data spaces and generalized EM algo-
rithms”, in Proc. IEEE ICASSP, Minneapolis, MN, p. IV 1-4, 1993.
[FES 95] FESSLER J., “Hybrid Poisson/polynomial objective functions for tomographic image
reconstruction from transmission scans”, IEEE Trans. Image Processing, vol. 4, num. 10,
p. 1439-1450, Oct. 1995.
[GEM 84] GEMAN S., GEMAN D., “Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images”, IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-6,
num. 6, p. 721-741, Nov. 1984.
[GEM 85] GEMAN S., MCCLURE D., “Bayesian images analysis: An application to single
photon emission tomography”, in Proc. Statist. Comput. Sect. Amer. Stat. Assoc., Washing-
ton, DC, p. 12-18, 1985.
[GRE 90] GREEN P. J., “Bayesian reconstructions from emission tomography data using a
modiﬁed EM algorithm”, IEEE Trans. Medical Imaging, vol. 9, num. 1, p. 84-93, Mar.
1990.
[HEB 89] HEBERT T., LEAHY R., “A generalized EM algorithm for 3-D Bayesian reconstruc-
tion from Poisson data using Gibbs priors”, IEEE Trans. Medical Imaging, vol. 8, num. 2,
p. 194-202, June 1989.
[HER 80] HERMAN G. T., Image Reconstruction from Projections. The Fundamentals of
Computerized Tomography, Academic Press, New York, NY, 1980.
[HUD 94] HUDSON H., LARKIN R., “Accelerated image reconstruction using ordered subsets
of projection data”, IEEE Trans. Medical Imaging, vol. 13, num. 4, p. 601-609, Dec. 1994.
[IIN 67] IINUMA T. A., NAGAI T., “Image restoration in radioisotope imaging system”, Phys.
Med. Biol., vol. 12, num. 4, p. 501-509, Mar. 1967.
[KAU 87] KAUFMAN L., “Implementing and accelerating the EM algorithm for positron emis-
sion tomography”, IEEE Trans. Medical Imaging, vol. MI-6, num. 1, p. 37-51, 1987.
[KOU 96] KOULIBALY
P.,
Régularisation
et
corrections
physiques
en
tomographie
d’émission, PhD thesis, University of Nice-Sophia Antipolis, Nice, France, Oct. 1996.
[LAN 90] LANGE K., “An overview of Bayesian methods in image reconstruction”, in Proc.
SPIE Conf. on Digital Image Synth. and Inv. Optics, vol. 1351, San Diego, CA, p. 270-287,
July 1990.
[LEV 87] LEVITAN E., HERMAN G., “A maximum a posteriori probability expectation max-
imization algorithm for image reconstruction in emission tomography”, IEEE Trans. Med-
ical Imaging, vol. MI-6, p. 185-192, Sep. 1987.
[LLA 89] LLACER J., VEKLEROV E., “Feasible images and practical stopping rules for itera-
tive algorithms in emission tomography”, IEEE Trans. Medical Imaging, vol. 8, p. 186-193,
1989.
[LUE 73] LUENBERGER D. G., Introduction to Linear and Nonlinear Programming,
Addison-Wesley, New York, NY, 1st edition, 1973.
[MEI 93] MEIKLE S. R., DAHLBOM M., CHERRY S. R., “Attenuation correction using count-
limited transmission data in positron emission tomography”,
J. Nuclear Med., vol. 34,
num. 1, p. 143-144, 1993.

374
Bayesian Approach to Inverse Problems
[MUM 94] MUMCUOGLU E. U., LEAHY R., CHERRY S. R., ZHOU Z., “Fast gradient-based
methods for Bayesian reconstruction of transmission and emission PET images”, IEEE
Trans. Medical Imaging, vol. 13, num. 4, p. 687-701, Dec. 1994.
[NGU 99] NGUYEN M. K., GUILLEMIN H., FAYE C., “Regularized restoration of scinti-
graphic images in Bayesian frameworks”, in Proc. IEEE ICIP, Kobe, Japan, p. 194-197,
Oct. 1999.
[OGA 91] OGAWA K., HARATA Y., ICHIHARA T., KUBO A., HASHIMOTO S., “A practical
method for position-dependent Compton scatter correction in single photon emission CT”,
IEEE Trans. Medical Imaging, vol. 10, p. 408-412, Sep. 1991.
[OLL 93] OLLINGER J. M., “Model-based scatter correction for fully 3D PET”, in IEEE
Nuclear Science Symp. & Medical Imaging Conf., San Francisco, CA, p. 1264-1268, 1993.
[OLL 97] OLLINGER J. M., FESSLER J. A., “Positron-emission tomography”, IEEE Signal
Processing Mag., vol. 14, num. 1, p. 43-55, Jan. 1997.
[O’S 94] O’SULLIVAN J. A., “Divergence penalty for image regularization”, in Proc. IEEE
ICASSP, vol. V, Adelaide, Australia, p. 541-544, Apr. 1994.
[PAN 97] PAN T.-S., KING M. A., DE VRIES D. J., DAHLBERG S. T., VILLEGAS B. J., “Es-
timation of attenuation maps from single photon emission computed tomographic images
of technetium 99m-labeled sestamibi”, J. Nucl. Cardiol., vol. 4, num. 1, p. 42-51, 1997.
[POL 91] POLITTE D., SNYDER D. L., “Corrections for accidental coincidences and atten-
uation in maximum-likelihood image reconstruction for positron-emission tomography”,
IEEE Trans. Medical Imaging, vol. 10, p. 82-89, 1991.
[SHE 82] SHEPP L. A., VARDI Y., “Maximum likelihood reconstruction for emission tomog-
raphy”, IEEE Trans. Medical Imaging, vol. MI-1, p. 113-122, 1982.
[SNY 85] SNYDER D., MILLER M., “The use of sieves to stabilize images produced with
the EM algorithm for emission tomography”, IEEE Trans. Nuclear Sciences, vol. NS-32,
num. 5, p. 3864-3871, Oct. 1985.
[SNY 90] SNYDER D. L., SCHULZ T. J., “High-resolution imaging at Low Light Levels
through Weak Turbulence”, J. Opt. Soc. Am. (A), vol. 7, p. 1251-1265, 1990.
[SNY 93] SNYDER D. L., HAMMOUD A. M., WHITE R. L., “Image recovery from data
acquired with a charge-coupled-device camera”, J. Opt. Soc. Am. (A), vol. 10, p. 1014-
1023, 1993.
[THI 00] THIBAULT J.-B., SAUER K., BOUMAN C., “Newton-style optimization for emission
tomographic estimation”, J. Electr. Imag., vol. 9, num. 3, p. 269-282, 2000.
[TIE 94] TIERNEY L., “Markov chain for exploring posterior distribution”, Annals Statist.,
vol. 22, num. 4, p. 1701-1762, Dec. 1994.
[TIM 99] TIMMERMAN K. E., NOWAK R. D., “Multiscale modeling and estimation of Pois-
son processes with application to photon-limited imaging”,
IEEE Trans. Inf. Theory,
vol. 45, num. 3, p. 846-862, Apr. 1999.
[TSU 91] TSUI B., FREY E., GULLBERG G., “Comparison between ML-EM and WLS-CG
algorithms for SPECT image reconstruction”,
IEEE Trans. Nuclear Sciences, vol. 38,
num. 6, p. 1766-1772, Dec. 1991.

List of Authors
Laure BLANC-FÉRAUD
CNRS
Nice-Sophia Antipolis
France
Hervé CARFANTAN
Observatoire Midi-Pyrénées
Toulouse
France
Frédéric CHAMPAGNAT
ONERA
Châtillon
France
Guy DEMOMENT
L2S
University of Paris XI
France
Xavier DESCOMBES
INRIA
Nice-Sophia Antipolis
France
Jean-Marc DINTEN
LETI
CEA
Grenoble
France
Stéphane GAUTIER
EDF R&D
Chatou
France
Jean-François GIOVANNELLI
L2S
University of Paris XI
France
Yves GOUSSARD
Ecole Polytechnique Montreal
Quebec, Canada
Alain HERMENT
INSERM
Paris
France
Jérôme IDIER
IRCCyN
CNRS
Nantes
France
Guy LE BESNERAIS
ONERA
Châtillon
France

376
Bayesian Approach to Inverse Problems
Serge MEIMON
ONERA
Châtillon
France
Ali MOHAMMAD-DJAFARI
L2S
CNRS
Gif-sur-Yvette
France
Laurent MUGNIER
ONERA
Châtillon
France
Ken SAUER
University of Notre Dame, IN
USA
Jean-Baptiste THIBAULT
GE Medical Systems
Milwaukee, WI
USA

Index
A
A-scan 223
accidental coincidence 360, 362
adaptive optics (AO) 251, 263-267
afﬁne approximation 348
algorithm
ART 50, 325, 329
CLEAN 118
column-action 50, 370
EM 206, 208, 264, 303, 365, 368
forward-backward 302
Gerchberg-Saxton-Papoulis-
Van Cittert 43, 264
Gibbs sampling 190, 327
golden section 295
gradient 50, 145, 208, 303, 324
conjugate (CG) 50, 94, 133, 370
preconditioned conjugate (PCG)
50, 96, 370
ICD 158, 160, 295, 326, 352, 370
ICM 129, 329
Levinson 49, 94
limited-memory 51
Metropolis sampling 190
Newton, quasi-Newton 51
reconstruction 323
Richardson-Lucy 265, 326
row-action 50
SAGE 370
SIRT 325
SMLR 126, 129
Viterbi 302
AO see adaptive optics
astronomy 243-276
atmospheric turbulence 243-276
attenuation 360
B
B-scan 223
backprojection (BP) 315
Bayes’ rule 64, 197
bias 98
bias-variance compromise 98
boundary effect 87, 96
BP see backprojection
C
Chandrasekhar factorization 110
clique 174
complete conditional 214
condition
boundary 88, 89, 107
Hadamard 30, 149, 154
number,conditioning 38, 89, 291
positivity 180
conductivity 338
covariance 72, 75
a posteriori 186
criterion
augmented 148-158
convex 51, 123, 231, 323, 366
half-quadratic (HQ) 149-158, 183

378
Bayesian Approach to Inverse Problems
Picard 33
quadratic 323
Rayleigh 83
regularized 29, 92
cross-validation 54
D
DCT see transform (discrete cosine)
deconvolution 16-18
blind 134, 226, 232
conventional 253
DBG 230
DL2Hy 230, 231
from wavefront sensing (DWFS) 250,
259-263
L2Hy 123
L2LP 123
minimum entropy 228
multipulse 118, 228
myopic
long-exposure images 264-267
short-exposure images 260-263
predictive 106, 226
detection-estimation 119, 148, 154
DFT see transform (discrete Fourier)
diffraction 244
diffusion
anisotropic 155
isotropic 145
discontinuity 147, 148
discretization 340
distribution see law
Doppler 285, 286
DWFS see deconvolution from wavefront
sensing
E
electrical permittivity 338
equation
coupled 340, 343
diffusion 144
Euler-Lagrange 35, 143, 145
Fredholm 31
observation 293
partial derivative (PDE) 144
propagation 338
Riccati 105
state see model (state)
ergodicity 189
estimation
Bayesian 64-75, 172-174
joint 258, 261, 345
linear 81
marginal 258
robust 159
exponential family 70, 198
F
FFT see transform (fast Fourier)
ﬁeld
Gibbs-Markov (GMRF) 70, 173-185,
202-204, 207, 210, 366
compound 183-185
Gaussian 182
non-Gaussian 185
mean 204
ﬁlter
inverse 82, 84
matched 117, 119
minimum phase 227, 229
stabilizing 84
Wiener 75, 84, 95, 111, 121
small kernel 111
Fourier
series 27, 93
synthesis 245, 271, 315
frequency
mean 286, 297
pure 290, 297
frequentist interpretation 62
Fried’s diameter 247, 255
FT see transform (Fourier)
full Bayesian approach 213
function
L2L1 47, 151, 254-276
L2L0 47, 152
basis 320, 341
Huber 161, 189
partition 174, 183
G
generalized inverse 35, 91

Index
379
geophysics 338
Gibbs
phenomenon 102
potential 174
GML see maximum likelihood
(generalized)
GMRF see ﬁeld (Gibbs-Markov)
GNC see gradual non-convexity
graduated non-convexity 123, 149, 155,
159, 352
H
Hessian 200, 365
HT see transform (Hilbert)
hyperparameter 53, 67, 147, 162, 186, 294,
302, 366
I
image registration 181-182
imaging
magnetic resonance 285
medical 285
microwave 337
optical 243-276
scintigraphic 358
ultrasound 221-241, 285, 286
importance sampling 200
interferometry 27, 46
inverse crime 18
K
Kalman
ﬁlter 102, 121, 293
asymptotic 110
fast 108
non-standard 111
smoother 293
ﬁxed-interval 103, 107
ﬁxed-lag 103
Kullback divergence, Kullback
pseudo-distance 47, 69, 206
L
L-curve 53
law
Beer-Lambert 312
compound 363
direct 61
Gaussian 62
improper 69, 176, 183
Kolmogorov 246
Poisson 359, 360
proper 66
least squares 18, 290, 365
adaptive 290
recursive 105
reweighted (RLS) 159
likelihood 63-64, 186, 199, 295, 298, 362
generalized 68, 212
joint 124
marginal 67, 73, 124
local minimum 51, 52, 347
M
MAP see maximum a posteriori
marginalization 298
Markov chain 176, 298
Monte carlo see method (MCMC)
snake 106
matrix
circulant 49, 88
circulant-block-circulant 88
Hankel 89
ill-conditioned 37
sparse 49
Toeplitz 49, 87, 161
Toeplitz-block-Toeplitz 49, 88, 160,
161
Toeplitz-plus-Hankel 89
maximum
a posteriori (MAP) 65, 173, 301, 322,
366
likelihood (ML) 63, 186, 200, 302, 364
mean
a posteriori 186
a priori 93
mean square error (MSE) 54, 72, 73
measure
Hausdorff 148
roughness 46
generalized (GML) 68, 212, 213

380
Bayesian Approach to Inverse Problems
method
analytical 317
Bialy’s 43
coding 202
Gauss-Seidel 50
Hunt 94
Landweber 43, 50
Lucy’s 44
maximum entropy (MEM) 69
MCMC 130, 192-193
Monte Carlo 189, 200
Newton 200
of moments 341
quadrature 85
ML see maximum likelihood
model
AR 105, 290
long 290
ARMA 290
Bernoulli-Gaussian (BG) 119, 124
binary 328
geometrical 328
in companion form 106
Ising 181
nonlinear 335
Potts 182
random walk 106
state 85, 102, 293
degenerate 104, 110
white noise 106
MSE see mean square error
N
NDE see non-destructive evaluation
neighborhood relationship 177
non-destructive evaluation (NDE) 337
ultrasonic 221-241
non-negativity constraint 367
O
optical interferometry (OI) 251, 268
optimization
constrained 351
global 351
joint 350
local 352
P
PDE see partial derivative equations
periodogram 28, 36, 46, 287, 299
PET see tomography (positron emission)
phase indetermination 252, 271
polygon, polyhedron 328
preconditioning 96, 160
probability density function (pdf) 65
problem
direct 15-19, 25
ill-posed 16-19, 30-34
instrumentation 25
inverse 15-19, 25
well-posed 30
pseudo-likelihood 203
pseudo-solution 35
R
random search 53, 191
reﬂectivity 118, 225
regularization
L2L1 see function (L2L1)
L2L0 see function (L2L0)
quadratic see Tikhonov regularization
regularizer 41
relaxation see algorithm (ICD)
RLS see least squares (reweighted)
robust norm 151
RT see transform (Radon)
S
scattered photon 360, 362
simulated annealing 53, 193, 327, 351
single most likely replacement see SMLR
algorithm
singular value 33, 72
decomposition
truncated (TSVD) 42, 101
decomposition (SVD) 33
Sobolev distance 292
speckle 247, 250
spectral
aliasing 286, 297
analysis 286
characterization 285
density

Index
381
energy (ESD) 265, 267
power (PSD) 84, 102, 246, 247,
257, 263, 274, 286-297
equalization 81, 85, 112, 121
moment 286, 297
regularization 292
spectrogram 287
state representation see model (state)
successive linearization 348, 351
SVD see singular value decomposition
T
theorem
Hammersley-Clifford 179
Riesz 32
Szegö’s 90
Van Cittert-Zernike 246
Tikhonov regularization 17, 45, 92, 93,
142, 158, 164, 292, 300
time-frequency analysis 286
tomography 312
diffraction 335
discrete 319
emission 358, 367
positron (PET) 358
transmission 362
transform
discrete cosine (DCT) 89, 96
Fourier (FT) 26-29, 315
discrete (DFT) 88-89, 94-96, 98
fast (FFT) 88, 97, 302
Hilbert (HT) 230, 315
J-orthogonal 109
Radon (RT) 313
transformation group 69
V
variable
complex 344
edge 147
hidden 68, 149
interactive 149, 184-185
uncoupled 157
variance 99
W
wavefront sensor (WFS) 251, 260, 261,
264
Hartmann-Shack 251, 255-257, 260,
262, 265
phase
diversity 255, 257
retrieval 257
WFS see wavefront sensor
windowing 291
Z
Zernike polynomial 256, 257, 261

