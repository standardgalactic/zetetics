Emergence, Complexity and Computation ECC
Automata, 
Universality, 
Computation
Andrew Adamatzky   Editor
Tribute to Maurice Margenstern

Emergence, Complexity and Computation
Volume 12
Series editors
Ivan Zelinka, Technical University of Ostrava, Ostrava, Czech Republic
e-mail: ivan.zelinka@vsb.cz
Andrew Adamatzky, University of the West of England, Bristol, United Kingdom
e-mail: adamatzky@gmail.com
Guanrong Chen, City University of Hong Kong, Hong Kong
e-mail: eegchen@cityu.edu.hk
Editorial Board
Ajith Abraham, MirLabs, USA
Ana Lucia C. Bazzan, Universidade Federal do Rio Grande do Sul, Porto Alegre
RS Brasil
Juan C. Burguillo, University of Vigo, Spain
Sergej ˇCelikovský, Academy of Sciences of the Czech Republic, Czech Republic
Mohammed Chadli, University of Jules Verne, France
Emilio Corchado, University of Salamanca, Spain
Donald Davendra, Technical University of Ostrava, Czech Republic
Andrew Ilachinski, Center for Naval Analyses, USA
Jouni Lampinen, University of Vaasa, Finland
Martin Middendorf, University of Leipzig, Germany
Edward Ott, University of Maryland, USA
Linqiang Pan, Huazhong University of Science and Technology, Wuhan, China
Gheorghe P˘aun, Romanian Academy, Bucharest, Romania
Hendrik Richter, HTWK Leipzig University of Applied Sciences, Germany
Juan A. Rodriguez-Aguilar, IIIA-CSIC, Spain
Otto Rössler, Institute of Physical and Theoretical Chemistry, Tübingen, Germany
Vaclav Snasel, Technical University of Ostrava, Czech Republic
Ivo Vondrák, Technical University of Ostrava, Czech Republic
Hector Zenil, Karolinska Institute, Sweden

About this Series
The Emergence, Complexity and Computation (ECC) series publishes new devel-
opments, advancements and selected topics in the ﬁelds of complexity, computation
and emergence. The series focuses on all aspects of reality-based computation ap-
proaches from an interdisciplinary point of view especially from applied sciences,
biology, physics, or Chemistry. It presents new ideas and interdisciplinary insight
on the mutual intersection of subareas of computation, complexity and emergence
and its impact and limits to any computing based on physical limits (thermodynamic
and quantum limits, Bremermann’s limit, Seth Lloyd limits...) as well as algorith-
mic limits (Gödel’s proof and its impact on calculation, algorithmic complexity,
the Chaitin’s Omega number and Kolmogorov complexity, non-traditional calcula-
tions like Turing machine process and its consequences,...) and limitations arising
in artiﬁcial intelligence ﬁeld. The topics are (but not limited to) membrane comput-
ing, DNA computing, immune computing, quantum computing, swarm computing,
analogic computing, chaos computing and computing on the edge of chaos, com-
putational aspects of dynamics of complex systems (systems with self-organization,
multiagent systems, cellular automata, artiﬁcial life,...), emergence of complex sys-
tems and its computational aspects, and agent based computation. The main aim of
this series it to discuss the above mentioned topics from an interdisciplinary point
of view and present new ideas coming from mutual intersection of classical as well
as modern methods of computation. Within the scope of the series are monographs,
lecture notes, selected contributions from specialized conferences and workshops,
special contribution from international experts.
More information about this series at http://www.springer.com/series/10624

Andrew Adamatzky
Editor
Automata, Universality,
Computation
Tribute to Maurice Margenstern
ABC

Editor
Andrew Adamatzky
Unconventional Computing Centre
University of the West of England
Bristol
United Kingdom
ISSN 2194-7287
ISSN 2194-7295
(electronic)
Emergence, Complexity and Computation
ISBN 978-3-319-09038-2
ISBN 978-3-319-09039-9
(eBook)
DOI 10.1007/978-3-319-09039-9
Library of Congress Control Number: 2014945747
Springer Cham Heidelberg New York Dordrecht London
c⃝Springer International Publishing Switzerland 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
Printed on acid-free paper
[Springer International Publishing AG Switzerland] is part of Springer Science+Business Media
(www.springer.com)

Preface
A Tribute to Maurice Margenstern
A few ﬁgures witness Maurice Margenstern’s scientiﬁc accomplishments. He au-
thored or co-authored 210 papers (some more being on the way...). His 54
co-authors are from all over the world: France, Belgium, Switzerland, Germany,
Austria, Italy, Spain, Ireland, United Kingdom, Finland, Hungary, Moldavia, Roma-
nia, Russia, Israel, China, Malaysia, Japan, Chili, USA. Besides his research papers,
he wrote 6 books, organized and edited the proceedings of 8 international confer-
ences, supervised 5 students.
And, most importantly, he pioneered new areas of research at the intersection of
mathematics and computer science.
How did Maurice become the scientist we so much appreciate?
Maurice was born in Paris on June 6, 1947. After high school he was accepted
in prestigious French “grandes ´ecoles”. Resigning from the ´Ecole Polytechnique
he entered the ´Ecole Normale Sup´erieure de Cachan. After he passed the French
“agr´egation de math´ematiques”, he got an academic position at the mathematics
department of the University of Paris-Sud (Orsay) in 1970.
Attracted by constructive mathematics, then a very marginal subject in France, he
went in the early 70’s to Leningrad to study with Nicola¨ı A. Shanin, then the head of
the famous Russian school in constructive mathematics founded by A.A. Markov.
This is where he worked on his thesis on Constructive topological properties of
spaces of almost periodic functions.
Then his scientiﬁc inclinations gradually moved from constructive mathematics
to computability, a subject then much revived after Matijasevich’s famous result on
Hilbert’s 10th problem. In the late 80’s he joined the computer science laboratory
LITP (now LIAFA) in Paris and started his amazing investigation of the simplest
(resp. most complex) machines with an undecidable (resp. decidable) halting prob-
lem. Despite the previous works by Yuri Rogogine in Moldavia and Ludmila Pavlot-
skaia in Russia, this topic was then rather conﬁdential. Maurice is to be credited
for putting it up as one of the important themes in computability. He did so via his

VI
Preface
own contributions and via a triennial international conference Machines, Computa-
tions and Universality he set up in 1995 and which still goes on. I remember how
Josef Gruska (speaking as the last chairman of the ﬁrst conference, held in Paris)
stressed how much energy Maurice had to show in order to create from scratch such
a successful international event.
Maurice passed his Habilitation in 1994 “Study of the frontier between decidabil-
ity and undecidability of the halting problem of small or constrained Turing ma-
chines.” He was then appointed as a professor in computer science by the university
of Lorraine at Metz. There he created the LITA laboratory and spent much energy
to make it a solid research center. Some years ago, he was promoted “professeur de
classe exceptionnelle”, the highest rank in French Academia.
In the early 2000’s Maurice developed the subject of computation with cellu-
lar automata in the hyperbolic world (dimension 2, 3 or 4) and brought solutions
to long open difﬁcult tiling problems. Besides these technical achievements, Mau-
rice proved that hyperbolic computability allows to overcome known unfeasibility
results of the Euclidean world.
The classical tool in hyperbolic geometry is the theory of groups but Maurice
invented completely new tools: combinatorial ones. As is usual with new ideas,
experts in hyperbolic geometry showed reluctance towards such an innovative ap-
proach and it was a real relief for Maurice when Donald Knuth wrote to him to
express that he found these new tools “quite fascinating” and will refer to them in
the Art of Computer Programming.
Of course, the two themes presented above do not exhaust the subjects Maurice
investigated but they witness his creative talent and impressive energy.
Maurice recently retired and is now emeritus professor at the university of Lor-
raine. Having no more to teach and manage a research team, Maurice simply spends
more time to continue producing prominent works.
Having had the privilege to know him since the mid 70’s, it is my pleasure to
say, in the name of all contributors of this volume: Maurice, thank you for what you
have accomplished and let’s continue our friendship and scientiﬁc collaboration.
Serge Grigorieff
Paris

Contents
1
The Common Structure of the Curves Having
a Same Gauss Word . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Bruno Courcelle
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.3
Atoms of Graphs and Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.4
Planar t-Graphs and t-Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.5
Curves in the Plane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.6
Some Open Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2
Logical Theory of the Additive Monoid of Subsets of Natural
Integers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
Christian Choffrut, Serge Grigorieff
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.2
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.3
Using Submonoids to Approximate and Emulate . . . . . . . . . . . . . .
48
2.4
Complexity of the Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
2.5
Non Deﬁnable Predicates. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
2.6
Logical Deﬁnability in ⟨P(N);+,=⟩. . . . . . . . . . . . . . . . . . . . . . . .
68
2.7
Remarkable Deﬁnable Sets and Classes . . . . . . . . . . . . . . . . . . . . . .
69
2.8
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
3
Some Reﬂections on Mathematics and Its Relation to Computer
Science. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
Liesbeth De Mol
3.1
Mathematical Logic, the Computer and Mathematics . . . . . . . . . .
76
3.2
A Number-Theorist’s Point of View . . . . . . . . . . . . . . . . . . . . . . . . .
80

VIII
Contents
3.3
The Impact of the Computer on Mathematics: Some
Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.4
Computer-Assisted Explorative Mathematics: Characteristics
and Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
3.5
Some (New and Open) Problems . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
3.6
Some Afterthoughts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
4
Sampling a Two-Way Finite Automaton . . . . . . . . . . . . . . . . . . . . . . . . . 103
Zhe Dang, Oscar H. Ibarra, Qin Li
4.1
Introduction and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
4.2
Information Dependency and Information Flow in 2NFAs . . . . . . 105
4.3
Language Properties of Sampled Runs of Some Two-Way
Inﬁnite-State Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
4.4
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
5
Maurice Margenstern’s Contributions to the Field of Small
Universal Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
Turlough Neary, Damien Woods
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.2
Simulating the Collatz Function with Small Turing Machines . . . 118
5.3
Frontiers between Universality and Non-universality . . . . . . . . . . . 120
5.4
Restricted Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
6
Constructing Reversible Turing Machines by Reversible Logic
Element with Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Kenichi Morita
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
6.2
Reversible Logic Element with Memory (RLEM) . . . . . . . . . . . . . 128
6.3
Relation to Physical Reversibility . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
6.4
Constructing Reversible Turing Machines by Rotary Element . . . 132
6.5
Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7
The Grossone Methodology Perspective on Turing Machines . . . . . . . 139
Yaroslav D. Sergeyev, Alfredo Garro
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
7.2
Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
7.3
The Grossone Language and Methodology . . . . . . . . . . . . . . . . . . . 147
7.4
Observing Turing Machines through the Lens of the Grossone
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
7.5
Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167

Contents
IX
8
On Parallel Array P Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
Linqiang Pan, Gheorghe P˘aun
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
8.2
Deﬁnitions and Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
8.3
Parallel Array P Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
8.4
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
8.5
Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
9
Small P Systems Deﬁning Non-semilinear Sets . . . . . . . . . . . . . . . . . . . . 183
Artiom Alhazov, Rudolf Freund
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
9.2
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
9.3
Accepting P Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
9.4
Generating by Doubling in the Maximally Parallel Mode . . . . . . . 195
9.5
Asynchronous and Sequential P systems . . . . . . . . . . . . . . . . . . . . . 202
9.6
P Systems with Catalysts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
9.7
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
10
Generalized Communicating P Automata . . . . . . . . . . . . . . . . . . . . . . . . 219
Erzs´ebet Csuhaj-Varj´u, Gy¨orgy Vaszil
10.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
10.2
Preliminaries and Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
10.3
The Power of P Automata with Generalized Communication . . . . 226
10.4
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
11
Computational Models Based on Splicing . . . . . . . . . . . . . . . . . . . . . . . . 237
Yurii Rogozhin, Sergey Verlan
11.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
11.2
Splicing Operation and H Systems . . . . . . . . . . . . . . . . . . . . . . . . . . 238
11.3
Controlled Splicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
11.4
Distributed Splicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
11.5
Reﬁning the Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
11.6
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
12
Linear Cellular Automata and Decidability . . . . . . . . . . . . . . . . . . . . . . 259
Klaus Sutner
12.1
Linear Cellular Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
12.2
The First-Order Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
12.3
Undecidability and Hardness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
12.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274

X
Contents
13
Algorithms with Active Cells Modeled by Cellular Automata with
Write-Access (CA-w) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
Rolf Hoffmann
13.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
13.2
CA-w Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
13.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
14
Broadcasting Automata and Patterns on Z2 . . . . . . . . . . . . . . . . . . . . . . 297
Thomas Nickson, Igor Potapov
14.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
14.2
Broadcasting Automata Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
14.3
Variable Radius Broadcasting over Z2 . . . . . . . . . . . . . . . . . . . . . . . 306
14.4
Neighbourhood and Broadcasting Sequences . . . . . . . . . . . . . . . . . 308
14.5
Geometrical Properties of Discrete Circles . . . . . . . . . . . . . . . . . . . 310
14.6
Iterative Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
14.7
Broadcasting Sequences and Their Limitations. . . . . . . . . . . . . . . . 322
14.8
Reducing Restrictions through Aggregation . . . . . . . . . . . . . . . . . . 323
14.9
Formation of Polygons through Aggregation. . . . . . . . . . . . . . . . . . 326
14.10 Approximating Lp Metrics with Broadcasting Automata . . . . . . . . 328
14.11 Pattern Formations and Periodic Structures in Z2 . . . . . . . . . . . . . . 334
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
15
Real-Time Prime Generators Implemented on Small-State
Cellular Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
Hiroshi Umeo, Kunio Miyamoto, Yasuyuki Abe
15.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
15.2
Prime Generator on One-Bit Communication
Cellular Automata. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
15.3
Prime Generator on Constant-Bit Communication Cellular
Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
15.4
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
16
Phyllosilicate Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
Andrew Adamatzky
16.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
16.2
Phillosilicate Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
16.3
Principal Morphologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
16.4
Game of Life Phyllosilicate Automata . . . . . . . . . . . . . . . . . . . . . . . 367
16.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379

Contents
XI
17
DC Programming and DCA for Challenging Problems in
Bioinformatics and Computational Biology . . . . . . . . . . . . . . . . . . . . . . 383
Le Thi Hoai An
17.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
17.2
A Brief Introduction to DC Programming and DCA . . . . . . . . . . . 384
17.3
Multiple Sequence Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
17.4
Molecular Conformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
17.5
Phylogenetic Tree Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . 409
17.6
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
Subject Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415

Chapter 1
The Common Structure of the Curves
Having a Same Gauss Word
Bruno Courcelle
Abstract. Gauss words are ﬁnite sequences of letters associated with self-intersecting
closed curves in the plane. (These curves have no “triple” self-intersection). These
sequences encode the order of intersections on the curves. We characterize, up to
homeomorphism, all curves having a given Gauss word. We extend this character-
ization to the n-tuples of closed curves having a given n-tuple of words, that we
call a Gauss multiword. These words encode the self-intersections of the curves and
their pairwise intersections. Our characterization uses decompositions of strongly
connected graphs in 3-edge-connected components and algebraic terms formalizing
these decompositions.
1.1
Introduction
Many geometric conﬁgurations can be represented by ﬁnite combinatorial objects,
up to appropriate equivalence relations, like homeomorphism in the case of em-
beddings of graphs in surfaces. Gauss words are sequences of letters intended to
describe the self-intersections of closed curves in the plane (with no triple intersec-
tion): each crossing is named by a letter, and a word with two occurrences of each
letter is obtained by following the curve and writing the letter seen at each crossing.
This deﬁnition raises the following questions:
(1) What are these words ?
(2) Which curves can be uniquely reconstructed, up to homeomorphism, from
the corresponding word ?
(3) What is the common structure of all curves having a same associated word ?
Gauss words are characterized in several articles by Lovasz and Marx [12],
Rosenstiehl [16] and de Fraysseix and Ossona de Mendez [8] to name a few. These
Bruno Courcelle
LaBRI, Bordeaux University and CNRS, France
e-mail: courcell@labri.fr
c⃝Springer International Publishing Switzerland 2015
1
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_1

2
B. Courcelle
Fig. 1.1 Curves with Gauss word aabb
Fig. 1.2 Curve with Gauss word abcabc
works answer Question (1). Some of them are reviewed in the book by Godsil and
Royle [10].
We will address Questions (2) and (3). A word is unambiguous if it characterizes
a unique curve up to homeomorphism. Otherwise, it is ambiguous. Figure 1.1 shows
two curves that are not related by any homeomorphism of the plane or of the sphere
but yield the same ambiguous Gauss word aabb. On the other hand, the word abcabc
is unambiguous and Figure 1.2 shows a (the) corresponding curve.
It is natural and convenient to extend the deﬁnitions and the corresponding ques-
tions to tuples of curves, described by tuples of words such that any letter has exactly
two occurrences in one word or one occurrence in two of them. Figure 1.3 shows
three curves with corresponding unambiguousGauss multiword (abcd,aecf,b fde).
We will give in Theorem 24 a natural characterization of Gauss multiwords dif-
ferent from those of [8, 12, 16]. By means of Proposition 14, it yields a linear-time
recognition algorithm. This algorithm provides also a tuple of corresponding curves
if there exists one.
For answering Questions (2) and (3), we will use several notions. First we observe
that intersecting and self-intersecting closed curves without triple intersections are
nothing but plane 4-regular graphs. We recall that a map is a graph equipped, at each
vertex, with a circular order of edges around it. This order is called a rotation (see
the book by Mohar and Thomassen [14], Chapter 3). Every embedding of the graph
in an oriented surface yields a map. Intersecting closed curves can be described up
to homeomorphism by 4-regular planar maps (we omit here some technical details).

1
Gauss Words
3
Fig. 1.3 Three intersecting circles
Second, we use a single combinatorial object, constructed from an n-tuple W of
words where each letter has two occurrences, from which the planar maps repre-
senting the n-tuples of curves with associated multiword W can be deﬁned. This
object is a 4-regular graph with transitions, that is, equipped, at each vertex v, with
a pairing of the half-edges, called darts, incident with v. We call it a t-graph. In
its embeddings in surfaces, two paired darts must form a line that crosses the one
formed by the two other incident darts. See Figure 1.5 for an example. (Pairings are
represented by thickenings of certain darts. For example, edges a and b are paired
at their common vertex, and so are b and c). A t-graph is essentially a 4-regular
map in which we forget the orientation of the surface around each vertex (we only
retain how darts alternate around a vertex). Hence, a t-graph contains more infor-
mation than the underlying graph and less than a map of this graph. A t-graph with
an underlying planar graph may have no plane embedding that satisﬁes the ”cross-
ing condition” on pairs of opposite darts (see the right part of Figure 1.10). We
will prove in Theorem 15 that, if the underlying graph of a planar t-graph is loop-
free and 3-edge-connected, then this t-graph has a unique plane embedding, up to
homeomorphism.
Third, we start our investigation with tuples of oriented curves (see Figure 1.4).
The corresponding t-graphs are directed and each vertex has 2 incoming edges and
2 outgoing edges. We say that they are (2,2)-regular. Figure 1.4 shows the difference
between oriented and nonorienting curves regarding the ambiguity of Gauss multi-
words. It shows two plane embeddings of a (2,2)-regular t-graph associated with
the Gauss multiword (abcd,bc,ad) that are not homeomorphic by any homeomor-
phism of the sphere. (To check this, just compare the directions of the edges incident
with the two faces bordered by 4 edges). However, forgetting the directions of the
edges yields two homeomorphic embeddings. Hence the multiword (abcd,bc,ad)
is unambiguous for representing intersections of nonoriented closed curves, but it is
ambiguous for representing intersections of oriented curves. It is actually easier to

4
B. Courcelle
Fig. 1.4 Two nonhomeomorphic embeddings of a same t-map
characterize the unambiguous Gauss multiwords representing oriented curves and
we will start by this case.
Our key tool is a decomposition of strongly connected (directed) graphs in 3-
edge-connected components that we have studied in [5] under the name of atomic
decomposition. Figure 1.9 in Section 2.2 below shows an atomic decomposition:
the graphs G1,...,G9 are undecomposable, they are the atoms of the decomposed
graph.This decomposition works for t-graphs and for strongly connected maps.
From the atomic decomposition of the t-graph associated with a Gauss multiword
W, and its expression by an algebraic term over a binary operation that composes
disjoint graphs, we can describe all tuples of oriented curves (up to homeomor-
phism) whose associated Gauss multiword is W. Then, we extend this character-
ization to solve the original questions concerning nonoriented curves. The basic
deﬁnitions and facts are given and proved in Sections 2.1 and 3. This article can be
read independently of [5].
The article is organized as follows. Section 1 reviews deﬁnitions about graphs
and maps. Section 2 reviews the decomposition of strongly connected graphs in 3-
edge-connected components. Section 3 examines the corresponding decompositions
of planar graphs and maps. Section 4 develops the applications to the curves in the
plane described by given multiwords, and Theorems 24, 26 and 36 answer Questions
(1-3) for Gauss multiwords that describe oriented or nonoriented curves. Section
5 reviews open questions. For the reader’s convenience, an appendix reviews the
various equivalence and isomorphism notions used in this article.
1.2
Deﬁnitions
All graphs and related objects (t-graphs, maps) will be ﬁnite. By saying that
(e1,...,ek) is a circular sequence, we mean that it can also be speciﬁed as (ei+1,...,ek,

1
Gauss Words
5
e1,...,ei−1) and that its properties and associated constructions do not depend on the
initial element e1. (See also Section 4.1.1).
1.2.1
Graphs
1.2.1.1
Terminology and Notation
A directed graph G is a triple (VG,EG,vertG) consisting the set of vertices VG, the
set of edges EG (with VG∩EG = /0) and a mapping vertG : EG →VG×VG that deﬁnes
incidences. If vertG(e) = (x,y), we say that x is the tail of e, denoted by α(e), that
y is its head, denoted by β(e), we also write e : x →G y and we say that x and y are
the ends of e. If G is undirected, then vertG(e) is a set {x,y} of one or two vertices,
called the ends of e and we write e : x −G y. In both cases, e is a loop if x = y. We
denote by Und(G) the undirected graph obtained from G by taking as incidence
function vertUnd(G)(e) := {x,y} whenever vertG(e) = (x,y). Graphs can have loops
and multiple edges; we do not identify an edge with the pair or the set of its ends.
Walks and paths
Let G be a graph and x,y ∈VG. A walk from x to y is a sequence (x0,e1,x1,e2,...,
en,xn) such that x0,x1,...,xn ∈VG, x0 = x, xn = y, e1,...,en ∈EG, ei : xi−1 →xi
(ei : xi−1 −xi if G is undirected) for each i = 1,...,n, and ei ̸= ej if 1 ≤i < j ≤n.
It is a path if we (also) have xi ̸= xj for 0 ≤i < j ≤n, except possibly if i = 0 and
j = n. A walk is closed if x0 = xn. A directed cycle is a closed path in a directed
graph. A cycle is similar in an undirected graph (a cycle with two vertices consists
of two parallel edges).
An edge never occurs twice in a walk. A vertex never occurs twice in a path
except if x0 = xn. A walk (x0,e1,x1,e2,...,en,xn) of a directed graph can be described
without ambiguity by the sequence (e1,e2,...,en). A walk, a path or a cycle in a
directed graph is said to be undirected if its edges can be traversed in any direction
(i.e., with ei : xi−1 →xi or ei : xi →xi−1 in the above deﬁnition).
A directed graph is strongly connected if, for any two distinct vertices x and y,
there is path from x to y. The class of strongly connected graphs is denoted by SC.
As usual, a directed graph G is connected if Und(G) is connected and similarly for
notions deﬁned for undirected graphs.
Subgraphs
We write G ⊆H (resp. G ⊆i H) if G is a subgraph (resp. an induced subgraph) of H.
If F ⊆VG ∪EG, then G−F is the subgraph of G obtained by deleting the edges and
vertices in F and the edges incident with a vertex in F. We write it G−x if F = {x}.
If X ⊆VG, we denote by G[X] the graph G−(VG −X): it is the induced subgraph of
G with vertex set X.
Edge-cuts
A bridge in a connected graph is an edge whose removal disconnects the graph. A
cut-pair is a pair of edges that are not bridges and whose removal disconnects the

6
B. Courcelle
graph. A strongly connected graph G has no bridge. Conversely, if an undirected
graph H is 2-edge-connected, i.e., is connected and has no bridge, then it is Und(G)
for some strongly connected graph G.
We now review some deﬁnitions and results from the preliminary sections of
[9,15] and [17]. Let G be strongly connected. If F is a cut-pair of G, and G1 and G2
the two connected components of G−F, then one edge of F links G1 to G2 and the
other G2 to G1; we say that F separates x and y if x ∈VG1 and y ∈VG2. We deﬁne
an equivalence relation on VG by x ∼y if and only if x and y are not separated by
any cut-pair, if and only they are linked by 3 edge-disjoint undirected paths (by a
classical result by Menger, cf. [7], Theorem 3.3.6). We denote it by ∼G if G must
be speciﬁed.
The quotient graph H := G/ ∼is deﬁned as follows: its vertices are the equiv-
alence classes [x]∼of the vertices x of G; it has edge e : [x]∼→H [y]∼if and only
if [x]≁= [y]∼and e : u →G v where u ∈[x]∼and v ∈[y]∼. (It is useful to desig-
nate in the same way an edge e of G and its ”quotient” in H; their end vertices are of
course different in G and H). ThenUnd(H)(=Und(G)/ ∼) is a cactus (a connected
undirected graph whose biconnected components are cycles) without bridges. Since
G is strongly connected, these cycles are directed cycles in H. We will say that H
is a strongly connected cactus, i.e., a strongly connected graph whose biconnected
components are directed cycles. Note that a directed graph is a strongly connected
cactus if and only if, for every two distinct vertices x and y there is a unique directed
path from x to y.
Isomorphisms
An isomorphism of G = (VG,EG,vertG) to G′ = (VG′,EG′,vertG′) is a bijection h :
VG ∪EG →VG′ ∪EG′ that maps vertices to vertices, edges to edges and preserves
incidences, that is : vertG′(h(e)) = (h(x),h(y)) (resp. {h(x),h(y)}) if vertG(e) =
(x,y) (resp. vertG(e) = {x,y}). It is a v-isomorphism if VG = VG′. In this case, one
can consider EG and EG′ as different sets of names used to designate the edges of
a graph with vertex set VG. If G′ = G, we get the notions of automorphism and of
v-automorphism. The graph of Figure 1.2 has several v-automorphisms. We denote
by G ∼= G′ the existence of an isomorphism between G and G′.
Darts
To discuss plane embeddings it will be useful to split a directed edge e into two
darts: e−,e+ with incidences deﬁned by a function γ such that γ(e−) = x and
γ(e+) = y if e : x →y. We denote by D+
G the set of darts e+, by D−
G the set of
darts e−and by DG the set D+
G ∪D−
G. It is clear that D+
G ⊆D+
G′ and D−
G ⊆D−
G′ if
G ⊆G′. We will only use this notion for directed graphs.
1.2.1.2
Graphs with Transitions
Deﬁnition 1: (2,2)-regular graph
A (2,2)-regular graph is a directed, 4-regular graph, each vertex of which has 2
incoming edges and 2 outgoing edges. We denote by G2,2 the class of connected

1
Gauss Words
7
Fig. 1.5 A planar t-map
(2,2)-regular graphs. Each such graph has an Eulerian tour, i.e., is covered by a
closed walk (covered means that the walk goes through all edges). (See [7], Section
1.8, where the proof given for undirected graphs extends easily to directed ones). It
is thus strongly connected.
Deﬁnition 2: Graph with transitions
A (2,2)-regular graph with transitions is a pair (G,τ) consisting of a (2,2)-regular
graph G and a transition function deﬁned as a bijection τ: D+
G →D−
G such that
γ(τ(d)) = γ(d) for every d ∈D+
G. We say in this case that d and τ(d) are opposite
darts. For shortness sake, we will say that there is a transition from e to f (or even
that (e, f) is a transition) if γ(f −) = γ(e+) and τ(e+) = f −. If H = (G,τ), we
let Graph(H) denote G, VH denote VG and similarly for other items. A t-graph is
a connected (2,2)-regular graph with transitions. We denote by Gt
2,2 the class of
t-graphs.
An isomorphism or a v-isomorphism of t-graphs must respect transitions in an
obvious way. (An appendix reviews the different notions of isomorphism and equiv-
alence relation used in this article.)
Deﬁnition 3: Straight walk
A walk (x0,e1,x1,e2,...,en,xn) in a (2,2)-regular graph with transitions is straight
if it is not closed and τ(e+
i ) = e−
i+1 for each i = 1,...,n −1 or if it is closed and, in
addition to this condition, τ(e+
n ) = e−
1 . Every t-graph is the union of a set of pairwise
edge-disjoint closed straight walks, in a unique way.

8
B. Courcelle
Figure 1.5 shows a (plane embedding of a) t-graph. The transition function is
represented by thickening some darts: two bold darts are related by the transition,
and so are two light darts. For example τ(a+) = b−and τ(f +) = g−. This graph is
covered by the closed straight walks (a,b,c,d) and (f,g,i, j,k,h). Its Eulerian tour
(a,b,c,d, f,g,i, j,k,h) is not straight.
1.2.2
Maps
Maps are combinatorial objects that represent embeddings of connected graphs in
oriented surfaces, up to orientation preserving homeomorphisms. We review the
classical deﬁnitions (cf. [14], Chapter 3 for detailed deﬁnitions), and we introduce
some new notions.
If E is an embedding of a graph G in a surface, we denote by E(u) the point
representing a vertex u, by E(e) the line segment representing an edge e and by
E(W) the union of the segments representing the edges of a walk W. As usual, we
call plane an embedding of a graph in the sphere, and we deﬁne a homeomorphism
of plane embeddings as a homeomorphism of the sphere that maps an embedding
onto the other.
Deﬁnition 4: Map
A map is a pair M = (G,ρ) consisting of a connected and directed graph G and a
bijection ρ : DG →DG such that, for every d in DG, the set {ρi(d) | i ≥0} is the set
of darts incident with γ(d). This bijection is called the rotation of M. We denote G
by Graph(M) and DG by DM.
From an embedding E of a connected and directed graph G in an orientable sur-
face, we get a map (G,ρ) by letting ρ(d) be the dart following d in the circular
order, ”around the vertex γ(d)” (and according to the orientation of the surface) of
the darts incident with γ(d). For any two embeddings of G in the sphere with same
associated map, there is an orientation preserving homeomorphism of the sphere
that maps E to E′. (See [14], Theorem 3.2.4 for the proof, and a more general state-
ment concerning orientable surfaces.) A map is planar if it is associated with a plane
embedding, i.e., an embedding in the sphere.
If E is a plane embedding of a connected and directed graph G with map
M = (G,ρ), then M−1 := (G,ρ−1) is the symmetric map of M: it corresponds to
an embedding E′ of G that is homeomorphic to E with a reversal of orientation. We
say that two maps M and M′ are equivalent if M′ = M or M′ = M−1. (They have
the same underlying graph).
We denote respectively by MSC and M2,2 the classes of maps of graphs in SC
and in G2,2.
Deﬁnition 5: Transitions deﬁned from rotations
Let M = (G,ρ) be a map in M2,2. It is a t-map if the mapping τ deﬁned by τ(d) :=
ρ2(d) for d ∈D+
G is a transition function. The associated t-graph is Grapht(M) :=
(G,τ). We let Mt
2,2 denote the class of t-maps. Two t-maps M and N are t-equivalent
if Grapht(M) = Grapht(N), which we denote by M ∼t N. Clearly, M ∼t M−1. Figure

1
Gauss Words
9
Fig. 1.6 Three planar maps M,N,P ∈M2,2
1.4 shows two t-maps M and N that are t-equivalent but not equivalent: they are
different but N ̸= M−1. (Another similar example can be obtained from Figure 1.1).
In Figure 1.6, the planar map M at the left is a t-map because ρ(a+) = b+,
ρ(b+) = b−and ρ(b−) = a−, hence ρ2(a+) = b−and ρ2(b+) = a−, so that ρ2
is a transition function. The map N in the middle is not because ρ(c+) = c−and
ρ(c−) = d+, so that ρ2(c+) = d+. (We will denote by 8 the corresponding graph in
G2,2). Similarly, the rightmost map P is not either because ρ2(d+) = b+. The planar
map of Figure 1.5 is a t-map.
Deﬁnition 6 : Planar t-graph
A plane embedding of a t-graph G respects the transition if the corresponding map
M is such that Grapht(M) = G. A t-graph G is planar if it has a plane embedding,
hence, if and only if it is Grapht(M) for some planar t-map M.
We denote by PSC, PG2,2, PGt
2,2, PMSC, PM2,2 and PMt
2,2 the classes of
graphs, t-graphs and maps that belong respectively to SC, G2,2, Gt
2,2, MSC, M2,2
and Mt
2,2 and are planar. These classes of graphs and maps are related by inclusions
and by the mappings Graph and Grapht. We have in particular :
M2,2 ⊇Mt
2,2
↓
↓
G2,2
←
Gt
2,2.
This square diagram shows that:
Graph(M) = Graph(Grapht(M)) for every M ∈Mt
2,2.
These relations and facts extend to planar graphs and maps. Hence, we have the
similar square diagram with analogous meaning:
PM2,2 ⊇PMt
2,2
↓
↓
PG2,2 ←PGt
2,2.

10
B. Courcelle
The symmetrization mapping M →M−1 preserves each of the classes MSC,
M2,2, Mt
2,2, PMSC, PM2,2 and PMt
2,2.
1.3
Atoms of Graphs and Maps
As recalled in Section 1.1.1, every strongly connected graph G has a canonical struc-
ture of strongly connected cactus whose vertices are its 3-edge-connected compo-
nents. We call atoms these components and atomic decomposition the global cactus
structure. For our applications to Gauss words, it is useful to express these decompo-
sitions by algebraic terms based on an appropriate binary operation that composes
strongly connected graphs. This operation also composes maps. We review some
deﬁnitions and show that they are also applicable to t-graphs and to the graphs,
t-graphs and maps of the classes of Deﬁnition 6.
1.3.1
Circular Composition of Directed Graphs and Maps
We deﬁne an operation that composes directed graphs and maps. We will actually
use it mainly to decompose these objects.
Deﬁnition 7 : Circular composition
We let G1,G2 be disjoint directed graphs and ei ∈EGi for i = 1,2. We deﬁne
G1 ⊞e1,e2 G2 as the graph H such that VH := VG1 ∪VG2, EH := EG1 ∪EG2 and the
incidence function vertH is deﬁned as follows:
vertH(e1) := (α(e1),β(e2)),
vertH(e2) := (α(e2),β(e1)),
vertH(e) := vertGi(e) if e ∈EGi −{ei},i = 1,2.
We call G1 ⊞e1,e2 G2 a circular composition of G1 and G2. If G1,...,Gk are pair-
wise disjoint and ei is an edge of Gi for each i, we let :
⊞e1,...,ek(G1,...,Gk) := (...(G1 ⊞e1,e2 G2)⊞e2,e3 G3)...)⊞ek−1,ek Gk
= ⊞e1,...,ek−1(G1,...,Gk−1)⊞ek−1,ek Gk.
Here are examples. We let G,H,K be the respective graphs of the maps M,N,P
of Figure 1.6 : we have K = G ⊞b,c H. The left part of Figure 1.7 shows graphs
G1,...,G4 with distinguished edges e1,...,e4 (note that e2 is a loop). The right part
shows ⊞e1,...,e4(G1,G2,G3,G4). This ﬁgure explains the terminology.
The following properties are easy to check from deﬁnitions.
Proposition 8 : Let G1,G2,G3 be pairwise disjoint directed graphs and ei ∈EGi
for i = 1,2,3. We have the following equalities (in each case, both handsides are
deﬁned):

1
Gauss Words
11
Fig. 1.7 The graph to the right is a circular composition of G1,...,G4
(1) G1 ⊞e1,e2 G2 = G2 ⊞e2,e1 G1,
(2) (G1 ⊞e1,e2 G2)⊞e2,e3 G3 = G1 ⊞e1,e3 (G2 ⊞e2,e3 G3),
(3) (G1 ⊞e1,e2 G2)⊞f,e3 G3 = G1 ⊞e1,e2 (G2 ⊞f,e3 G3) if f ∈EG2 −{e2}.
From (1) and (2) we get the following circular associativity :
(G1 ⊞e1,e2 G2)⊞e2,e3 G3 = (G2 ⊞e2,e3 G3)⊞e3,e1 G1.
The graph H = ⊞e1,...,e4(G1,G2,G3,G4) at the right of Figure 1.7 can be ex-
pressed as (G1 ⊞e1,e2 G2)⊞e2,e4 (G3 ⊞e3,e4 G4).
It is easy to check that G1 ⊞e1,e2 G2 is strongly connected if G1 and G2 are so.
However, the connectedness of G1 and G2 does not imply that of G1 ⊞e1,e2 G2 : just
take G1 = e1 and G2 = e2. Furthermore, {e1,e2} is a cut-pair of G1 ⊞e1,e2 G2. Con-
versely, if G is strongly connected and has a cut-pair {e1,e2}, then G = G1 ⊞e1,e2 G2
where G1 and G2 are strongly connected. This decomposition step will be applied
to G1 and G2 if possible, and to the graphs resulting from their decompositions.
Deﬁnition 9: Circular composition of t-graphs and maps
(a) We recall that G2,2 ⊆SC. If G1 and G2 belong to G2,2, then so does any circular
composition G1 ⊞e1,e2 G2. If they are t-graphs (i.e., if they belong to Gt
2,2), then, we
deﬁne H = G1 ⊞e1,e2 G2 as the t-graph with vertices and edges as in Deﬁnition 7 for
graphs, and transition function τH deﬁned by :
τH(e+
1 ) := τG2(e+
2 ),
τH(e+
2 ) := τG1(e+
1 ),
τH(d) := τGi(d) if d ∈D+
Gi −{e+
i },i = 1,2.
Hence, H belongs to Gt
2,2 because it is strongly connected and the degree condi-
tions at each vertex are satisﬁed.
(b) Similarly, for pairwise disjoint strongly connected maps M1 and M2, we deﬁne
a map N = M1 ⊞e1,e2 M2 with underlying graph H = Graph(M1)⊞e1,e2 Graph(M2)
and rotation ρN deﬁned as follows:

12
B. Courcelle
Fig. 1.8 Composition of planar maps
ρN(e+
1 ) := ρM2(e+
2 ),
ρN(e+
2 ) := ρM1(e+
1 ),
ρN(d) := e+
1 if d ∈DM2 −{e+
2 } and ρM2(d) = e+
2 ,
ρN(d) := e+
2 if d ∈DM1 −{e+
1 } and ρM1(d) = e+
1 ,
ρN(d) := ρMi(d) if d ∈DGi ,i = 1,2 and the above cases do not apply.
Then, H is strongly connected and N is a map. If furthermore M1 and M2 belong
to M2,2 or Mt
2,2, then N belongs to the same class.
For the maps of Figure 1.6, we have the equality P = M ⊞b,c N. Figure 1.7
shows the circular composition of four planar maps represented in the plane, with
distinguished edges drawn ”on the outer face”. Figure 1.8 shows the composition
G1 ⊞e1,e2 G2 of two planar maps G1 and G2, represented similarly, but with e2 not
on the outer face.
The equational properties of circular composition stated in Proposition 8 are also
valid for t-graphs and maps.
In Deﬁnitions 7 and 9, circular composition is a partial operation because of the
disjointness condition on the arguments. We will use it to decompose given maps
and graphs, that is, for given H, we will try to ﬁnd graphs (or maps) G1 and G2
such that H = G1 ⊞e1,e2 G2. Hence, the disjointness condition does not raise any
difﬁculty.
By the following proposition, the results about decompositions of strongly con-
nected graphs extend to the classes of graphs, t-graphs and maps of Deﬁnition 6.
Proposition 10: (1) If G,H1,H2 are directed graphs such that G = H1 ⊞e1,e2 H2,
then G is strongly connected (resp. is in G2,2) if and only if H1 and H2 are strongly
connected (resp. are in G2,2). Furthermore, if G,H1,H2 are strongly connected, then
G is planar if and only if H1 and H2 are planar.

1
Gauss Words
13
(2) If G is a t-graph such that Graph(G) = H1 ⊞e1,e2 H2 for some directed graphs
H1 and H2, then there are unique transition functions τ1,τ2 such that G = G1 ⊞e1,e2
G2 and for each i, Gi = (Hi,τi) ∈Gt
2,2. Furthermore, G is planar if and only if G1
and G2 are planar.
(3) If M is a strongly connected map (resp. a map in M2,2 or a t-map) such that
Graph(M) = H1⊞e1,e2 H2 for some directed graphs H1 and H2, then there are unique
rotations ρ1,ρ2 such that M = N1 ⊞e1,e2 N2 where, for each i, Ni = (Hi,ρi) is a map
(resp. is in M2,2 or in Mt
2,2). Furthermore, M is planar and strongly connected if
and only if N1 and N2 are planar and strongly connected.
Proof: (1) Let G,H1,H2 be directed graphs such that G = H1⊞e1,e2 H2. Every closed
walk of G that goes through e1 must also go through e2. It follows then that G is
strongly connected if and only if H1 and H2 are so. The ”local” conditions on edges
for membership in G2,2 are easy to check.
Let G,H1,H2 be strongly connected. If H1 and H2 are planar, then a plane em-
bedding of G can be built from plane embeddings of H1 and H2 (cf. Figures 1.7 and
1.8). Conversely, assume that G = H1 ⊞e1,e2 H2 is planar, with plane embedding E.
Let P be a path in G from x = α(e1) to y = β(e2) (y is a vertex of H2) that goes
through e1, some edges of H2 and ﬁnally e2. Then, E(P) links E(x) to E(y). We
obtain in this way a plane embedding of H1. That H2 is planar is proved similarly.
(If we only assume that H1 ⊞e1,e2 H2 is planar and H2 is connected, then H1 may not
be planar.)
(2) Let G be a t-graph such that Graph(G) = H1 ⊞e1,e2 H2 for some H1,H2
in G2,2. The graphs Graph(G),H1 and H2 are strongly connected. The existence
of unique transition functions τ1,τ2 such that G = (H1,τ1) ⊞e1,e2 (H2,τ2) and
(H1,τ1),(H2,τ2) ∈Gt
2,2 follows easily. From plane embeddings of (H1,τ2) and
(H2,τ2), one can build a plane embedding of G. The converse is proved as in (1).
(3) Let M be a strongly connected map such that Graph(M) = H1 ⊞e1,e2 H2 for
some graphs H1,H2. These graphs must be strongly connected. The proof goes as
for (2).
□
1.3.2
Terms Deﬁning Graphs and Maps
Deﬁnition 11: Atoms and terms
An atom is a strongly connected graph that cannot be expressed as G1 ⊞e,f G2,
equivalently, that has no cut-pair. The latter condition means that it is 3-edge con-
nected, hence that any two vertices are linked by 3 edge-disjoint undirected paths
(by a theorem by Menger, cf. [7], Theorem 3.3.6). A t-atom (resp. an atomic map or
t-map) is a t-graph (resp. a map or t-map) that cannot be expressed as the circular
composition of two t-graphs or maps. If the context makes things clear, we will call
them also atoms. By Proposition 10 they are the t-graphs, maps or t-maps whose
underlying graphs are atoms.

14
B. Courcelle
The graph Graph(M) = Graph(N) where M,N are at the left of Figure 1.6 and
the graph of Figure 1.5 are atoms.
Let A be a set of pairwise disjoint graphs, t-graphs or maps included in one of
the classes of Deﬁnition 6. A term over A is a term t built with cicular composition
and elements of A used as constants, each of them having at most one occurrence
in the term. It deﬁnes a graph, a t-graph or a map, depending on the types of the
elements of A. We denote this object by val(t): it is the value of t. We also say that
t deﬁnes val(t). We say that t uses A if each element of A has one (and only one)
occurrence in t.
Every strongly connected graph, t-graph or strongly connected map G is deﬁned
by a term over atoms (of the corresponding type): if it is deﬁned by a term t using
G1,...,Gk such that Gi is not an atom, then Gi = H ⊞e,f K and Gi can be replaced in
t by H ⊞e,f K. Proposition 8 shows that different terms (even over atoms) can have
the same value. We will say that two such terms are equivalent. For example, the
following two terms deﬁne the graph shown in Figure 1.9:
t = ⊞e1,e2,e3(G1,[(G2 ⊞h2,h7 G7)⊞f2,f4 (⊞g4,g5,g6(G4,G5,G6))],G3)
t′ = G7 ⊞h7,h2 [⊞e1,e2,e3(G1,G2,G3)⊞f2,f4 (⊞g4,g5,g6(G4,G5,G6))].
Theorem 12 : There exists a linear-time algorithm that constructs, for every strongly
connected graph, its set of atoms and a term over atoms that deﬁnes it.
Proof: As observed in Deﬁnition 11, every strongly connected graph G is deﬁned
by a term over atoms.
The linear-time algorithm of [17] constructs the vertex sets of the 3-edge-
connected components of Und(G), i.e., the equivalence classes [x]∼for x ∈VG,
hence, also in linear time, the strongly connected cactus H underlying the atomic
decomposition of G.
We denote by A(G) the set {[x]∼| x ∈VG}. The graph H has vertex set VH equal
to (or in an implementation, in bijection with) A(G). Its set of edges EH consists of
the edges of G whose ends are in different members of A(G). If e links x to y in G,
then it links [x]∼to [y]∼in H if [x]≁= [y]∼.
From this cactus we now build a term tG over atoms as desired. We use an induc-
tion on its size.
If H has a unique vertex, then G is an atom and tG is this atom.
Otherwise, Lemma 2.1 of [9] proves that {e, f} is a cut-pair of G if and only if it is
one of H. Let us choose a vertex [x]∼and edges e : [x]∼−→[y]∼and f : [z]∼−→[x]∼
that belong to a same cycle of H. Then G = G1 ⊞e,f G2 where G1 and G2 are deﬁned
as follows:
X1 is the union of the sets [u]∼that are the vertices of the connected
component of H −{e, f} that contains [x]∼,
X2 := VG −X1,
G1 is G[X1] augmented with edge e : αG(e) −→β G(f),
G2 is G[X2] augmented with edge f : αG(f) −→β G(e).
Hence, we can take tG := tG1 ⊞e,f tG2. The terms tG1 and tG2 can be constructed
from the cactuses H1 and H2 of G1 and, respectively, G2 that, by Lemma 2.1 of [9],

1
Gauss Words
15
Fig. 1.9 The graph deﬁned by term t
can be obtained as follows in constant time from H, e : [x]∼−→[y]∼and f : [z]∼−→
[x]∼:
H1 is the connected component of H −{e, f} that contains [x]∼and
H2 is the other connected component of H −{e, f} augmented with
the edge f : [z]∼−→[y]∼if [z]≁= [y]∼. (The edge e is not an edge
of H1 because after its redirection towards β G(f), it is an edge of
G1[[x]∼] and similarly for f in H2 if [z]∼= [y]∼.)
It follows that tG can be constructed in linear time.
□
The set of atoms of tG is the same for all such terms and is called the set of atoms
of G. The atomic decomposition of G is the cactus whose vertices are its atoms.
Figure 1.9 shows an example.
Theorem 13: Let C be any one of the classes PSC, G2,2,PG2,2, Gt
2,2, PGt
2,2, MSC,
PMSC, M2,2, PM2,2, Mt
2,2 or PMt
2,2. Every element of C is deﬁned by a term
over atoms of C. There exists a linear-time algorithm that constructs, for every ele-
ment of C, a term over atoms that deﬁnes it.
Proof: The proof is the same for all cases. Let us do it for, say, C = PGt
2,2.
Let G ∈C. The graph H = Graph(G) is strongly connected, hence deﬁned by
a term t using the atoms H1,...,Hk. By means of an induction on the structure of t,
Proposition 10 entails the existence of unique transition functions τ1,...,τk such that
(Hi,τi) ∈C and G is deﬁned by a term using (H1,τ1),...,(Hk,τk). Since H1,...,Hk
are atoms in SC, (H1,τ1),...,(Hk,τk) are atoms in C.
For proving the second assertion, consider a term s′ using the atoms G′
1,...,G′
ℓ
in C that deﬁnes G. We get from it a term s using the atoms Graph(G′
1),...,
Graph(G′
ℓ) in SC that deﬁnes H. Hence, k = ℓand {H1,...,Hk} = {Graph(G′
1),

16
B. Courcelle
Fig. 1.10 Three atomic t-maps
...,Graph(G′
k)}. Since in Proposition 10(2) the transition functions τ1,τ2 are
uniquely deﬁned, we have {(H1,τ1),...,(Hk,τk)} = {G′
1,...,G′
k}.
The algorithm of Theorem 12 can construct the atoms H1,...,Hk of H. By Deﬁ-
nition 9, one can get the transition functions τ1,...,τk in linear-time.
□
Figure 1.5 shows an atomic planar t-map. Figure 1.6 shows an atomic planar t-
map M, and an atomic planar map N that is not a t-map. The ﬁrst two drawings of
Figure 1.10 represent atoms that are planar considered as graphs, t-graphs or maps.
The third one shows an atom that is planar as a graph but is not planar as a t-graph
or map.
We will need the following proposition of independent interest. A loop e of G ∈
Gt
2,2 has a transition to itself if τ(e+) = e−.
Proposition 14: There is a linear-time algorithm that checks if a t-graph is planar,
and constructs if possible a plane embedding.
Proof: Let G ∈Gt
2,2. If Graph(G) has 1 or 2 vertices, we check if G is planar by
looking at its transition function.
Otherwise, we ﬁrst eliminate the loops. Let G have a loop e at vertex u. If e has
a transition to itself, then G is not planar. Otherwise, we remove e, we fuse the two
remaining edges incident with u (hence, we also remove u) and we repeat these
removals until there is no more loop or we get a t-graph with 1 or 2 vertices. We
obtain a t-graph G′ ∈Gt
2,2 that is planar if and only if G is. If G′ has 1 or 2 vertices,
we conclude by looking at the transition function.
Otherwise G′ has at least three vertices and no loop, and we construct a graph H
as follows :
(1) We subdivide each edge e into two edges by putting a new vertex xe in its
”middle”.
(2) Let u be any vertex. For any two edges e and f ̸= e incident with u such that
there is no transition at u between e and f, we add an edge between xe and xf . We
say that we make G′ rigid at u. We deﬁne H by making G′ rigid at all its vertices.

1
Gauss Words
17
It is clear that H is planar if and only if G′ is, and that every plane embedding
of H yields one of the t-graph G′ (that respects its transitions), from which we get
easily one of the given t-graph G.
The constructions of G′ from G and of H from G′ can be done in time O(|VG|+
|EG|); the size of H deﬁned as |VH| + |EH| is also O(|VG| + |EG|). The classical
linear-time planarity test applied to H gives the answer. This algorithm constructs
plane embeddings of H, G′ and G when they exist.
□
1.4
Planar t-Graphs and t-Maps
In order to answer our initial questions about curves in the plane, we now focus our
attention on planar t-graphs and t-maps.
We ﬁrst recall a basic fact. Let M be a map and G = Graph(M). If P is an undi-
rected path in G from x to y (cf. Section 1.1.1 for deﬁnitions) whose edge sequence
is (e1,...,ek), if d is the dart of e1 such that γ(d) = x and d′ is that of ek such that
γ(d′) = y, then we say that d and d′ are the end darts of P. Let M be planar, and let
P1,P2,P3 be vertex-disjoint undirected paths from x to y with respective end darts
d1,d2,d3 that are incident with x and d′
1,d′
2,d′
3 incident with y. If the circular order
of d1,d2,d3 (around x) is (d1,d2,d3), then that of d′
1,d′
2,d′
3 around y is (d′
3,d′
2,d′
1)
(see [4]). It follows that the circular order of d1,d2,d3 determines that of d′
1,d′
2,d′
3.
We recall from Deﬁnition 5 that two maps M and N in Mt
2,2 are t-equivalent
if and only if Grapht(M) = Grapht(N), and that they are equivalent if and only if
M = N or M = N−1. Two equivalent maps are t-equivalent. For atoms, we have the
following converse.
Theorem 15: If two maps M and N in PMt
2,2 are t-equivalent and Graph(M) is an
atom, then, they are equivalent. A planar t-atom has a unique plane embedding.
As atoms are 3-edge connected, this result is similar to the fact that a 3-connected
planar graph has a unique plane embedding ( [7,14]). In the following deﬁnition and
proofs, we will consider undirected paths and walks in directed graphs, that we will
simply call paths and walks.
Deﬁnition 16: Let G ∈Gt
2,2 and let P and Q be two edge-disjoint walks from x to
y ̸= x. We say that they cross at a vertex u belonging to P and Q if u ̸= x,u ̸= y
and the two darts of the edges of P that are incident with u are opposite (and so
must be the two darts of the edges of Q that are incident with u). If P and Q are
vertex-disjoint (except for x and y), they do not cross (at any vertex).
Similarly, we say that P crosses itself at u if it is of the form (...,e,u,e′,
..., f,u, f ′,...) where the darts of e and e′ incident with u are opposite (and so must
be the darts of f and f ′ incident with u).
Lemma 17: Let G ∈Gt
2,2 be atomic. Between any two distinct vertices, there are
three edge-disjoint paths that pairwise do not cross.

18
B. Courcelle
Proof: If G = 8 the assertion is trivially true. Otherwise, since G is atomic, there
are three edge-disjoint paths between any two distinct vertices (cf. Deﬁnition 11).
Let P,Q,R be edge-disjoint paths from x to y ̸= x and let p be the total number of
vertices at which they cross. Let u be a vertex at which P and Q cross. Since G is
(2,2)-regular, u is not on R. Then P = P1P2 and Q = Q1Q2 such that the paths P1 and
Q1 go from x to u, the paths P2 and Q2 go from u to y. Then, P1Q2 and Q1P2 are two
walks from x to y. They are still edge-disjoint (and also edge-disjoint with R). The
total number of self-crossings of P1Q2 and Q1P2 and of crossings between P1Q2,
Q1P2 and R is p −1. If P1Q2 is not a path, we can shorten it into a path P′ from x to
y by removing some closed subwalks. Similarly, we can shorten Q1P2 into a path Q′
from x to y. We get a triple P′,Q′,R of edge-disjoint paths from x to y whose total
number of crossings is at most p−1 (because the shortenings of P1Q2 and Q1P2 into
P′ and Q′ can only reduce the number of crossings). By repeating this step, we get
three edge-disjoint paths from x to y that pairwise do not cross.
□
Proof of Theorem 15: Let M, N ∈PMt
2,2 be t-equivalent such that Graph(M) is
an atom. If Graph(M) = 8, then the result is clear. Otherwise, G = Graph(M) is
loop-free because if it has a loop, it has two edges e, f such that G = H1 ⊞e,f H2 and
is not an atom.
Let x ∈VG. If the rotations ρM and ρN are not the same on the darts incident with
x, we have ρM = ρN−1 at x because Grapht(M) = Grapht(N) and so, there are only
two rotations around a vertex that yield a given transition function (cf. Deﬁnition 5).
We now replace N by N−1, and then ρM and ρN are the same on the darts incident
with x.
We now prove that for every vertex y ̸= x, the rotations ρM and ρN are the same
on the darts incident with y. We ﬁrst consider the case where x and y are linked
by three vertex-disjoint paths. Then, since the circular order of the corresponding
three end darts incident with x is the same in M and in N, the same holds for the
circular order of the three darts around y because of the three vertex-disjoint paths.
The position of the fourth dart incident with y among the ﬁrst three is determined
by the transition. Since we have Grapht(M) = Grapht(N), the rotations ρM and ρN
are the same on the four darts incident with y.
We now consider the general case, where x and y are linked by three edge-disjoint
paths P,Q,R that are not vertex-disjoint. By Lemma 17, we can assume that they
pairwise do not cross. Let W be the set of vertices different from x and y that belong
to two of these paths. We add vertices to subdivide each edge of G that has an end
in W and we also add edges between these new vertices, so as to make G rigid at
all vertices of W (this notion is deﬁned in the proof of Proposition 14). We obtain a
graph H. Thanks to the newly added edges and because P,Q,R do not cross pairwise,
x and y are linked in H by three vertex-disjoint paths P′,Q′,R′ that avoid the vertices
of W. We identify in a obvious way the darts of G incident with x and y with the
corresponding ones in H.
Consider plane embeddings E and E′ of M and N that respect the transition rela-
tions. By adding some line segments to E, one can make it into a plane embedding
E1 of H, and similarly, one can make E′ into a plane embedding E′
1 of H. The circular

1
Gauss Words
19
orders of darts around x are the same in M and N, and thus also in E1 and E′
1. Since
we have three vertex-disjoint paths P′,Q′,R′ between x and y, the corresponding cir-
cular orders of their end darts around y are the same in E1 and E′
1, hence in M and
in N. As observed above, and since M and N are t-equivalent, the rotations ρM and
ρN are the same on the four darts incident with y.
We obtain that M = N, which gives the result. (We recall that the initial map N
may have been replaced by N−1 at the beginning.)
□
Our objective is now to describe all maps in PMt
2,2 that are t-equivalent to a
given map. For doing that we will use terms over atoms.
Lemma 18: Let M,M′,N,N′,P,P′ ∈PMt
2,2 be such that M ∼t M′, M = N ⊞e,f P,
M′ = N′ ⊞e,f P′, Graph(N) = Graph(N′) and Graph(P) = Graph(P′). Then N ∼t N′
and P ∼t P′.
Proof: The transition functions of Grapht(N) and Grapht(M) are related as follows:
τGrapht(N)(e+) = τGrapht(M)(f +),
τGrapht(N)(d) = τGrapht(M)(d) for every d ∈D+
Grapht(N) −{e+},
and similarly for M′ and N′. Since Grapht(M) = Grapht(M′), we have Grapht(N)
= Grapht(N′) and similarly, Grapht(P) = Grapht(P′). Hence N ∼t N′ and
P ∼t P′.
□
Let t be a term over atomic maps. We deﬁne as follows a set ▽(t) of terms written
with atomic maps, the circular composition ⊞and also the symmetrization −1 that
is well-deﬁned on maps:
▽(t) := {t,t−1} if t is an atomic map,
▽(t) := {s′ ⊞e,f s′′ | s′ ∈▽(t′),s′′ ∈▽(t′′)} if t = t′ ⊞e,f t′′.
Since for maps M and N we have (M ⊞e,f N)−1 = M−1 ⊞e,f N−1, we need not
add the terms (s′ ⊞e,f s′′)−1 to ▽(t) in the second case. In other words, the operation
−1 can be used equivalently anywhere in a term or only on atoms.
Theorem 19: Let M ∈PMt
2,2 be deﬁned by a term t over atomic t-maps. The maps
that are t-equivalent to M are those deﬁned by the terms in ▽(t).
Proof: We use an induction on the structure of t. If t is an atom, the result follows
from Theorem 15.
Otherwise, t = t′ ⊞e,f t′′. Lemma 18 shows that the t-maps that are t-equivalent to
M are those of the form N ⊞e,f P where N ∼t val(t′) and P ∼t val(t′′). By induction,
these maps N and P are those deﬁned respectively by the terms in ▽(t′) and in ▽(t′′).
Hence, the t-maps t-equivalent to M are those deﬁned by the terms in ▽(t).
□
Corollary 20: Let M ∈PMt
2,2 be deﬁned by a term t using p atomic maps. There
are 2p−1 pairwise inequivalent maps in PMt
2,2 for the t-graph Grapht(M). One can
determine them from t.

20
B. Courcelle
Proof: We ﬁrst observe that for all M and N in PMt
2,2, the t-maps M ⊞e,f N and
M ⊞e,f N−1 are t-equivalent but not equal. To prove this, we let P = M ⊞e,f N and
P′ = M ⊞e,f N−1. Then, ρP(e+) = ρN(f +), ρP′(e+) = ρN−1(f +) and ρN−1(f +) ̸=
ρN(f +) because the head of f has degree 4 in N. Hence, P ̸= P′. Furthermore,
P′−1 = (M ⊞e,f N−1)−1 = M−1 ⊞e,f N, hence P ̸= P′−1 by the same argument and
M ⊞e,f N and M ⊞e,f N−1 are not equivalent either.
It follows that two distinct terms t′ and t′′ in ▽(t) deﬁne different (but t-
equivalent) maps. The set ▽(t) contains 2p terms that deﬁne 2p maps. Each of these
maps Q is equivalent to a unique map Q−1 in the same set, and Q ̸= Q−1 because
these maps are 4-regular. Hence, we obtain exactly 2p−1 pairwise inequivalent maps.
By ﬁxing one atom of a term t and by replacing in all possible ways some of the
other atoms M by M−1, we obtain 2p−1 terms that deﬁne the 2p−1 pairwise inequiv-
alent maps in PMt
2,2 for the t-graph Grapht(M).
□
For an example, the multiword W = (abcd,bc,ad) is ambiguous (see the intro-
duction or the next section for the deﬁnition) since it is associated with the two
triples of curves of Figure 1.4. The t-graph tGra(W) has two atoms, hence, has two
inequivalent planar maps, shown in Figure 1.4.
Remark : The maps M ⊞e,f N and M ⊞e,f N−1 are not equivalent because M and
N are 4-regular. For comparison, we have P = P−1 if P is the (unique) map of a
path or a directed cycle. It follows that M ⊞e,f N = M ⊞e,f N−1 if N is the map of a
directed cycle containing the edge f.
1.5
Curves in the Plane
1.5.1
Oriented Curves
1.5.1.1
Deﬁnitions and Basic Facts
We make precise some deﬁnitions sketched in the introduction.
Circular sequences
We denote by V ∗the set of possibly empty sequences of elements of a ﬁnite set
V, also considered as words over V (whence the term ”Gauss word”). The empty
sequence is denoted by ε. Two sequences u and v are conjugate, which we denote
by u ≡v, if u = w1w2 and v = w2w1 for some w1,w2 ∈V ∗. This relation is an
equivalence and its classes are the circular sequences of elements of V. Their set is
denoted by V ⊛. We denote by Conj(u) the set of sequences conjugate with u.
The reversal (or mirror image) of a sequence u is denoted by u. Two sequences u
and v are reversal-conjugate, which we denote by u ≍v, if u ≡v or u ≡v. This is also
an equivalence relation. Its equivalence classes are the reversal-circular sequences
over V and their set is denoted by V ⊚. We will usually designate an element of V ⊛
or V ⊚by some sequence of the corresponding equivalence class.

1
Gauss Words
21
A double occurrence multiword over V is a tuple of circular sequences over V
where each element of V has two occurrences (either two occurrences in one se-
quence or one occurrence in two sequences). We denote by DOn(V) the set of such
n-tuples. If X ⊆V and W ∈DOn(V), we denote by W ↾X the multiword in DOn(X)
obtained by removing from the sequences composing W the elements not in X. If
W,W ′ ∈DOn(V), we write W ≍W ′ if, for each i, the i-th component of W is ≍-
equivalent to the i-th component of W ′.
Curves in the plane
Let C be a closed oriented curve in the plane, with ﬁnitely many self-intersections
but no triple (or more complex) self-intersection. The term oriented means that C
is given with a traversal direction. We let V be the set of self-intersection points,
or more precisely, a set of letters naming these points. By following the curve
from some of these points according to its traversal direction, we get a circular
sequence w(C) of elements of V where each element of V has two occurrences
(hence, (w(C)) ∈DO1(V)). If C has no self-intersection, i.e., if it is homeomorphic
to a circle, then, w(C) = ε. The sequence w(C) is circular because the starting point
is arbitrary. If C is not oriented, then w(C) ∈V ⊚because the traversal direction is
also arbitrary. Such sequences are called Gauss words, and have been studied in
many articles (e.g., [3,8,11,12,16]).
Some sequences, for instance abab, are not Gauss words as one checks easily.
(But a curve yielding it can be drawn on the torus). The Gauss word abcabc repre-
sents the curve of Figure 1.2. A square, i.e., a sequence of the form uu (where u is
a nonempty sequence) is a Gauss word if and only if u has odd length.
We wish to characterize the cases where C can be reconstructed from w(C), in a
unique way up to homeomorphism. This is not always the case : Figure 1.1 shows
two curves that do not correspond via any homeomorphism of the sphere but have
the same Gauss word aabb. We will characterize the Gauss words which correspond
to a unique curve, where as usual, unicity is understood up to homeomorphism.
Tuples of curves and graphs
We will consider more generally ﬁnite tuples of intersecting and self-intersec- ting
closed curves on the plane. With such an n-tuple of oriented curves (C1,...,Cn),
we associate the n-tuple of circular sequences W(C1,...,Cn) = (w(C1), ...,w(Cn))
∈DOn(V) where V is the set of intersection and self-intersection points, or rather
a set of names for these points. We call W(C1,...,Cn) a Gauss multiword. Examples
are (ε,ε,ε), (abcd,b fde, aecf) and (abcd,abcd). The ﬁrst one corresponds to
three disjoint circles, the second one corresponds to the three curves of Figure 1.3
and the third one to two ellipses with 4 intersections. This last example shows that
a same sequence may occur twice in a multiword. The 2-tuple (a,a) represents two
curves on the torus with a single intersection, but does not represent two curves on
the plane, this is a consequence of the following proposition.

22
B. Courcelle
Proposition 21: Each component of a Gauss multiword has even length. The num-
ber of letters that occur in any two components of a Gauss multiword is even.
Proof: It follows from the Jordan Curve Theorem (see [14], Chapter 2) that any two
closed curves without self-intersections (they are homeomorphic to circles) cross at
an even number of points. Hence, the two assertions hold for the Gauss multiwords
associated with curves without self-intersections. (They are the Gauss multiwords
such that each letter occurs in two different components.)
We now prove the general case by induction on the total number of self-
intersections of the curves C1,...,Cn that deﬁne the considered multiword. We let
W = W(C1,...,Cn) and, without loss of generality, we assume that C1 has a self-
intersection named a. Hence w(C1) ≡auav. One can delete the self-intersection a
by replacing C1 by C′
1 such that w(C′
1) = uv. The two assertions hold for W if they
hold for W(C′
1,C2,...,Cn). The result follows.
□
t-graphs from multiwords
We will mainly consider connected tuples of curves (C1,...,Cn), i.e., tuples whose
union is a connected subset of the sphere (in which we deﬁne our ”plane embed-
dings”). If n > 1, each curve has intersections with other curves. Except if the tuple
consists of one circle, the union of C1,...,Cn is a plane embedding E of a (nonempty)
t-graph G = G(C1,...,Cn) ∈PGt
2,2 that we now deﬁne. We split each curve Ci into a
union of consecutive segments si,1,si,2,...,si,ni whose ends are the intersection and
self-intersection points, and we let V be (in bijection with) the set of ends of these
segments. We deﬁne G as follows:
its vertex set VG is V,
its edges are the pairs (i, j) such that 1 ≤i ≤n and 1 ≤j ≤ni,
vertG((i, j)) is the pair (v,w) such that the segment si,j links v to w,
the transition is deﬁned by : τG((i, j)+) := (i, j + 1)−if 1 ≤j < ni, and
τG((i,ni)+) := (i,1)−.
A plane embedding E of G is deﬁned by E((i, j)) := si,j. It is clear that G is a
planar t-graph (in particular because (C1,...,Cn) is assumed connected) and that E
respects its transition (cf. Deﬁnition 6). This t-graph has n closed straight walks
ω1,...,ωn: the edges of ωi are those of the form (i, j) and E(ω1),..., E(ωn) are the
curves C1,...,Cn.
For the example of Figure 1.3 with trigonometric orientation of the curve, the
graph G(C1,C2,C3) has vertices {a,b,..., f}, edges a →b, b →c, c →d,..., b →f,
f →d,..., a →e, e →c,..., f →a and transitions (a →b,b →c), (b →c,c →d), ...,
(b →f, f →d), ... , (c →f, f →a).
We have a bijection between the edges of G(C1,...,Cn) and the segments si,j of
the curves C1,...,Cn. Every other plane embedding E′ of G(C1,...,Cn) yields a tu-
ple of oriented curves (C′
1,...,C′
n) (with C′
i = E′(ωi) ) such that W(C′
1,...,C′
n) =
W(C1,...,Cn). We now show that the t-graph G(C1,...,Cn) can be constructed from
W(C1,..., Cn).

1
Gauss Words
23
Deﬁnition 22: (a) A multiword W ∈DOn(V) is connected if there is no bipartition
(V1,V2) of V (with V1,V2 possibly empty) such that some components ofW are in V ∗
1
and others in V ∗
2 . Hence (ε) is connected but (ε,w2,...,wn) is not. If W has no empty
component, this is equivalent to the property that the graph (V,RW) is connected
where RW := {(a,b) | wabw′ or bwa is a component of W for some words w,w′}. If
L ⊆V ∗, u,v ∈L, we say that u and v are connected via L if there exist z1,...,zp in L
such that u = z1,v = zp and for each i, zi and zi+1 have at least one letter in common.
ThenW is connected if and only if any two distinct components are connected via its
set of components. It is clear thatW(C1,...,Cn) is connected if and only if (C1,...,Cn)
is connected.
(b) A multiword W = (w1,w2,...,wn) ∈DOn(V) is prime if it is connected, dif-
ferent from (ε) and there is no bipartition (V1,V2) of V (with V1,V2 nonempty) and
index i such that wi is conjugate to a word in V +
1 V +
2 and wj ∈V +
1 ∪V +
2 for every
j ̸= i, (X+ := X∗−{ε}). For example, (aa) and (abab) are prime whereas aabb is
not. The multiword (abcd,abcd) is prime whereas (abcd,ab,cd) is not. The prime
multiword (abcd,abef,cd fe) corresponds to the curves of Figure 1.3.
(c) The t-graph associated with a connected multiword.
Let W = (w1,w2,...,wn) ∈DOn(V) where each wi is nonempty and written
vi,1...vi,ni with vi,j ∈V. Each letter of V has occurrences in W. We deﬁne a t-graph
tGra(W) as follows:
VtGra(W) := V,
EtGra(W) := {(i, j) | 1 ≤j ≤ni},
verttGra(W)((i, j)) := (vi,j,vi,j+1) if j < ni and
verttGra(W)((i,ni)) := (vi,ni,vi,1),
τtGra(W)((i, j)+) := (i, j + 1)−if j < ni and
τtGra(W)((i,ni)+) := (i,1)−.
If W ′ = (w′
1,...,w′
n) where w′
i ≡wi, then tGra(W ′) is v-isomorphic to tGra(W):
some edges (i, j) of tGra(W) are just mapped to (i, j′). Here is an example. ForW =
(ab,abcc), we have v1,1 = v2,1 = a, v1,2 = v2,1 = b, v2,1 = v2,2 = c. Then tGra(W) is
the t-graph G with vertices a,b,c, edges (1,1) : a →b, (1,2) : b →a, (2,1) : a →b,
(2,2) : b →c, (2,3) : c →c and (2,4) : c →a. If W ′ = (ab,ccab), then tGra(W′) is
v-isomorphic to tGra(W) by the bijection on edges that exchanges (2,1) and (2,3),
and (2,2) and (2,4).
If W is not connected, tGra(W) is deﬁned as above but may not be connected. It
is a (2,2)-regular graph with transitions. (It may still be connected if W has empty
components).
Lemma 23: A double occurrence multiword W without empty components is prime
if and only if tGra(W) is atomic.
Proof: Let W = (w1,w2,...,wn) ∈DOn(V). We assume that it is not prime because
of a bipartition (V1,V2). Without loss of generality, we assume that w1 = uv with

24
B. Courcelle
u ∈V +
1 , v ∈V +
2 , w2,...,wn ∈V +
1 ∪V +
2 . We let a be the ﬁrst letter of u, b be its last
letter, c be the ﬁrst letter of v and d be its last letter. We may have a = b,c = d
and even u = a = b or v = c = d. (An example is (ac,agg,chh) that is not a Gauss
multiword). The only edges of
tGra(W) between V1
and V2 are e : b →c and
f : d →a. It follows that Graph(tGra(W)) = H ⊞e,f K for some graphs H with
vertex set V1 and K with vertex set V2. Hence, tGra(W) is not prime.
For the converse, we assume that G = Graph(tGra(W)) = H ⊞e,f K for some
graphs H and K. Every closed walk of G that goes through e must go through f.
The t-graph tGra(W) is covered by a union of pairwise edge-disjoint closed straight
walk. One of them contains e and f and can be written ePfQ where P and Q are
paths, respectively in K and in H. The other closed straight walks are either in H or
in K. It follows that the bipartition (VH,VK) of VG shows that W is not prime.
□
Theorem 24: (1) If (C1,...,Cn) is a connected n-tuple of curves with intersections,
then
tGra(W(C1,...,Cn)) is v-isomorphic to G(C1,...,Cn) in such a way that the
i-th closed straight walk of tGra(W(C1,...,Cn)) is transformed into the i-th closed
straight walk of G(C1,...,Cn).
(2) Let W ∈DOn(V) be connected. It is a Gauss multiword if and only if it is (ε)
or the t-graph tGra(W) is planar.
Proof: (1) We let W(C1,...,Cn) = (w1,...,wn), with each sequence wi nonempty
and written (vi,1,...,vi,ni). We ﬁrst assume that each segment si,j of Ci links vi,j to
vi,j+1 (and si,ni links vi,ni to vi,1). Then, tGra(W(C1,...,Cn)) = G(C1,...,Cn). Other-
wise, a segment si,j of Ci may link vi,j′ to vi,j′+1 and then, tGra(W(C1,...,Cn)) and
G(C1,...,Cn) are v-isomorphic.
(2) Consider a plane embedding of the t-graph tGra(W) such that W ̸= (ε). Its
closed straight walks deﬁne curves C1,...,Cn such that W = W(C1,...,Cn). Con-
versely, if W is a Gauss multiword deﬁned as W(C1,...,Cn), then C1,...,Cn form
a plane embedding of tGra(W).
□
This theorem answers Question (1) of the introduction for multiwords. Its second
assertion yields a natural characterization of Gauss multiwords and a linear-time
recognition algorithm based on that of Proposition 14.
Corollary 25: Let W ∈DOn(V)−{(ε)} be a connected Gauss multiword.
(1) The tuples of curves (C1,...,Cn) such thatW(C1,...,Cn) =W are obtained from
the plane embeddings E of the t-graph tGra(W) by deﬁning Ci = E(ωi) where ωi
is the closed straight walk of tGra(W) consisting of the edges (i, j).
(2) Two such tuples are homeomorphicif and only the corresponding embeddings
of tGra(W) are homeomorphic.
Proof: (1) Clear from the previous constructions.
(2) If E and E′ are two homeomorphic embeddings of tGra(W), the correspond-
ing tuples of curves are homeomorphic.
The converse may look evident, but a careful proof is necessary. Assume that
ϕ : S →S is a homeomorphism of the sphere S to itself that maps (C1,...,Cn)

1
Gauss Words
25
deﬁned from E to (C′
1,...,C′
n) deﬁned from E′. Since the intersection points are
designated by the elements of V, we have ϕ(E(v)) = E′(v) for each v ∈V. Fur-
thermore, ϕ(E(ωi)) = E′(ωi) for each i. Hence, for each edge (i, j) we have:
ϕ(E((i, j))) = E′((i, j′)) where j′ may be different from j.
Let us examine this case : the edges (i, j) and (i, j′) have same tail and head.
Since E and E′ preserve the transitions of tGra(W) and ϕ maps E(ωi) to E′(ωi),
we also have ϕ(E((i, j+1))) = E′((i, j′ +1)) and thus (i, j+1) and (i, j′ +1) have
also same tail and head. Since every v ∈V has at most two occurrences in each walk
ωi, this means that the tails of (i, j) and (i, j′) are the two occurrences (in W) of
some v, and the same is true for their heads, and for those of (i, j+1) and (i, j′ +1).
Hence, since W is connected, we have n = 1 and W = (ww) for some sequence w,
hence is a square.
Hence, if W is not a square, ϕ(E((i, j))) = E′((i, j′)) implies j′ = j, and E and
E′ are homeomorphic embeddings of tGra(W). If W is a square, then tGra(W)
is a planar t-atom (cf. Figure 1.2) and has a unique plane embedding up to homeo-
morphism by Theorem 15. So the result holds in both cases.
□
The proof of Assertion (2) shows that, unless W is a square, there is a unique v-
automorphism of tGra(W) that preserves each closed straight walk. If W is a square
there are two such v-automorphisms. Note for comparison that if W = (w,w), then
there is a v-automorphism of tGra(W) that exchanges the two closed straight walks.
1.5.1.2
Unambiguous Gauss Multiwords
We say that a Gauss multiword W is unambiguous if any two systems of oriented
curves that deﬁne it are homeomorphic. It is thus unambiguous if and only if it is
(ε) or
tGra(W) has a unique embedding (by Corollary 25), hence if and only if
tGra(W) is an atom by Corollary 20. We now characterize this property in terms of
W, which answers Question (2) for oriented curves.
Theorem 26: A connected Gauss multiword W is unambiguous if and only if
it is prime or equal to (ε). If it is ambiguous, all tuples (C1,...,Cn) such that
W(C1,...,Cn) = W can be determined from the planar maps of the t-graph tGra(W)
by means of any term over t-atoms that deﬁnes it.
Proof: A connected Gauss multiwordW ̸= (ε) is unambigous if and only if tGra(W)
is atomic by Corollary 20. Furthermore, tGra(W) is atomic if and only if W is prime
by Lemma 23.
If a connected Gauss multiword W is ambiguous, the tuples of curves (C1,...,Cn)
such that W(C1,...,Cn) = W
are in bijection with the planar maps of the t-graph
tGra(W) by Corollary 25. Theorem 19 and Corollary 20 show that these maps can
be determined from any term over t-atoms that deﬁnes this t-graph.
□
If in a Gauss multiword W = W(C1,...,Cn) deﬁned by (w1,...,wn) ∈(V ∗)n we
replace some wi by wi, then we obtain the Gauss multiword W ′ corresponding to the

26
B. Courcelle
same tuple of curves where the orientation of Ci is reversed. Hence, the ambiguity
of W does not depend on the orientations of the curves it represents. Furthermore,
there is a bijective correspondence between the tuples of curves corresponding to W
and to W ′.
1.5.1.3
Small Components of Multiwords
We now consider tuples of curves such that at least one curve has exactly two inter-
section points with the others. The associated multiwords have several components
and at least one of them has length 2. By ”reducing” these multiwords, we will be
able to extend Theorem 26 to the description by multiwords of tuples of nonoriented
curves.
Deﬁnition 27: Small components and reduction
(1) Let W = (w1,...,wn) be a Gauss multiword. A component wi of W of the form
ab with a ̸= b is a small component. Some other component wj must contain a and
b. We deﬁne W ′ from W by removing wi and the letters a and b from wj; it is a
Gauss multiword because, if W is associated with a tuple of curves (C1,...,Cn), then
W ′ is associated with the (n −1)-tuple obtained from (C1,...,Cn) by removing Ci.
We write this W →W ′ and we call this transformation a reduction step. It is clear
that W is connected if and only if W ′ is. (Since (ε) is deﬁned as connected, this is
true for W = (ab,ab)).
(2) A Gauss multiword W is reducible if it has small components; if furthermore
it is connected, either it is of the form (ab,ab) or it can be written without loss of
generality (by reordering its components if necessary):
W = (a1b1,a2b2,...,apbp,w1,...,wm)
where a1,b1,...,ap,bp, are pairwise distinct and each component w1,...,wm has
length at least 4. Each pair ai,bi occurs in one of the components w1,...,wm. We
let X = {a1,b1,a2,b2,...,ap,bp} and we deﬁne
Red(W) as (w′
1,...,w′
m) where
w′
i := wi ↾(V −X) (cf. Section 4.1.1 for notation). Clearly,W →∗Red(W). However,
Red(W) may have small components that we do not remove. For an example
Red((ab,cd,abef,cdef)) = (ef,ef). We may have Red(W) = (ε). If W is con-
nected then Red(W) is so.
Reduction viewed in terms of graphs
Our objective is to show that if W is a connected Gauss multiword that reduces into
W ′ by the removal of a small component ab, then there exists a planar t-atom D with
vertices a and b such that (up to technical details):
tGra(W′) = H ⊞K and tGra(W) = H ⊞D⊞K
for some unique t-graphs H and K that can be deﬁned from W. (H and/or K may
be empty).

1
Gauss Words
27
Before starting the description of H and K, we deﬁne D4(x,y;e, f,g,h) as the
planar t-atom with vertices x and y, edges e,h : x →y and f,g : y →x and tran-
sitions (e,g),(g,e),(f,h) and (h, f) (that yield the two closed straight walks (e,g)
and (f,h)). Figure 1.11 (right part) shows D4(a,b;g, f ′,g′,e′).
Let W ∈DOn(V) be a connected Gauss multiword deﬁned as W(C1,...,Cn) where
C1 has two intersections, a and b, with one of the other curves. Without loss of
generality (by reordering the tuple and taking a conjugate of w2), we can assume
that W = (ab,aw′
2bw′′
2,w3,...,wn). We have W →W ′ = (w′
2w′′
2,w3,...,wn).
We ﬁrst assume that w′
2 and w′′
2 are not empty. The curve C1 separates the sphere
into two connected open sets F1 and F2. (Our plane embeddings are in the sphere).
Let F1 be the one that contains the intersections represented by the letters of w′
2. Let
V1 and V2 be the set of letters of the components wi, 3 ≤i ≤n, that are connected,
respectively, to w′
2 and to w′′
2 via the components of W. The curves whose words are
connected to w′
2 (resp. to w′′
2) are in F1 (resp. in F2), hence, (V1,V2) is a bipartition
of V −{a,b}. (If W is not a Gauss multiword, then V1 and V2 may not be disjoint :
take for instance W = (ab,acbd,cdee)).
We can reorder the components of W so that w′
2,w3,...,wp ∈V ∗
1 and w′′
2,wp+1,...,
wn ∈V ∗
2 . Note that W1 = (w′
2,w3,...,wp) and W2 = (w′′
2,wp+1,...,wn) are connected
Gauss multiwords, respectively W(C′
2,C3,...,Cp) and W(C′′
2,Cp+1,...,Cn) for some
C′
2 and C′′
2.
We let G := tGra(W), G′ := tGra(W ′) and Gi := tGra(Wi) for i = 1,2. We will
denote in the same way the associated underlying graphs. In order to relate precisely
G1 and G2 to G, we let c and c′ be respectively the ﬁrst and last letter of w′
2, and d
and d′ be similarly the ﬁrst and last letter of w′′
2. In G, we have the following edges
that we name e,e′, f, f ′,g and g′ :
e : c′ →b, e′ : a →c, f : d′ →a , f ′ : b →d, g : a →b and g′ : b →a,
cf. the left part of Figure 1.11.
Then G1 is G[V1] augmented with an edge : c′ →c that we name e, and, similarly,
G2 is G[V2] augmented with f : d′ →d. The transitions making them into t-graphs
are as follows: τG1(e+) := τG(e′+) and τG2(f +) := τG(f ′+); otherwise, τG1(x) and
τG2(x) are τG(x). It is then clear that
G′ = G1 ⊞e,f G2 and G = G1 ⊞e,e′ D4(a,b;g, f ′,g′,e′)⊞f ′,f G2.
If w′
2 ̸= ε and w′′
2 = ε, then V1 ̸= /0, V2 = /0 and G2 is undeﬁned. In G := tGra(W),
we have the following edges :
e : c′ →b, e′ : a →c, f ′ : b →a,g : a →b and g′ : b →a,
with transitions(e, f ′),(f ′,e′),(g,g′) and (g′,g).
Here, W ′ = (w′
2,w3,...,wn) and we have :
G := tGra(W ′)⊞e,e′ D4(a,b;g, f ′,g′,e′).
If w′
2 = w′′
2 = ε, then V1 = V2 = /0, W = (ab,ab),W ′ = (ε) and :
tGra(W) = D4(a,b;g, f ′,g′,e′)
for an appropriate naming of the edges. These constructions establish the
following:
Proposition 28: Let W = (ab,aw′
2bw′′
2,w3,...,wn) be a connected Gauss multiword
that reduces to W ′ = (w′
2w′′
2,w3,...,wn).

28
B. Courcelle
Fig. 1.11 Assertion (1) of Proposition 28
Fig. 1.12 The graph H of Remark 29(1)
(1) If w′
2,w′′
2 ̸= ε, then tGra(W ′) = G1 ⊞e,f G2 and
tGra(W) = G1 ⊞e,e′ D4(a,b;g, f ′,g′,e′)⊞f ′,f G2,
(2) if w′
2 ̸= ε and w′′
2 = ε, then
tGra(W) = tGra(W ′)⊞e,e′ D4(a,b;g, f ′,g′,e′),
(3) if w′
2 = w′′
2 = ε, then n = 2 and tGra(W) = D4(a,b;g, f ′,g′,e′),
where G1,G2, e,e′ etc. are as in the construction.
□
Remarks 29:(1) The order in which the edges occur in D4(a,b;g, f ′,g′,e′) in
Proposition 28 is important. Figure 1.12 shows for comparison the graph
H = G1 ⊞e,e′ D4(a,b;g,g′, f ′,e′)⊞f ′,f G2.
The corresponding Gauss multiword is W = (baw′
2,abw′′
2,w3,...,wn).
(2) The t-graph tGra(W ′) has exactly one atom less than tGra(W) : this atom is
D4(a,b;g, f ′,g′,e′).
Construction of curves
A face of a plane embedding E of a graph G is a connected component of S −E.
It is an open set and its (topological) border is the union of some line segments
representing edges of G.

1
Gauss Words
29
Lemma 30: A plane embedding of a graph G = H ⊞e,f K ∈SC has two faces whose
borders contain the line segments representing e and f.
Proof: Let E be a plane embedding of G = H ⊞e,f K ∈SC. The graphs H −e and
K −f are connected, disjoint and G −{e, f} = (H −e) ∪(K −f). The restriction
of E to G −{e, f}, denoted by E ↾(G −{e, f}) is the union of E ↾(H −e) and
E ↾(K −f). Then, E ↾(H −e) is included in a face FK of E ↾(K −f) whose border
we denote by B. Similarly, E ↾(K −f) is included in a face FH of E ↾(H −e) whose
border we denote by B′. Then E ↾(G −{e, f}) has a face F = FH ∩FK
whose
border is B∪B′. Since E(e) ∩F ̸= /0 and E(f) ∩F ̸= /0, each of e and f has one end
vertex in B and the other in B′ (otherwise E is not plane). It follows that E(e) and
E(f) divide F into two faces of E whose borders contain both of them.
□
Proposition 31: Let W be a connected Gauss multiword that reduces to W ′
by
deletion of one small component, and S′ be a tuple of oriented curves such that
W ′ = W(S′).
(1) If W ′ = (ε), there is a unique way, up to homeomorphism, to extend S′ into a
pair S such that W(S) = W.
(2) If W ′ ̸= (ε), there are exactly two ways, up to homeomorphism, to extend S′
into a tuple S such that W(S) = W.
Proof: (1) In this case, W = (ab,ab), S′ is one circle (without self-intersection),
and there is a unique way to extend S′ into S such that W(S) = (ab,ab) because
tGra(W) is a t-atom.
(2) Assume ﬁrst that tGra(W ′) = G1⊞e,f G2 (cf. Proposition 28(1)) and consider
its embedding E′ deﬁned by S′. One of its curves contains E′(e) and E′(f). Note that
the edges e and f are the unique ones in
tGra(W ′) that link respectively V1 to V2
(i.e., a vertex of V1 to a vertex of V2) and V2 to V1 (so that E′(e) and E′(f) are deﬁned
in a unique way as segments of a line). The curve to be added to S′ must cross these
two segments and no other one. By Lemma 30, E′ has two faces F and F′ whose
borders contain these segments. Let us subdivide these segments by adding two
points xe and xf representing the two vertices of
tGra(W) not in
tGra(W ′). The
t-graph tGra(W) has edges g: xf →xe and g′: xe →xf with transitions (g,g′) and
(g′,g). We make E′ into a plane embedding E of tGra(W) by adding line segments
in exactly two possible ways (up to homeomorphism) : we can draw g in F and g′
in F′ or vice versa. Figure 1.11 shows one possibilty. The other one is obtained by
exchanging g and g′.
If now V2 = /0, then G = tGra(W ′) ⊞e,e′ D4(a,b;g, f ′,g′,e′) (Proposition 28(2)).
Consider the embedding E′ of
tGra(W ′) deﬁned by S′. The edge e of
tGra(W ′)
links c to c′. These letters are respectively the ﬁrst and last ones of w′
2, cf. Lemma
30. If W ′ is not a square, there is a unique segment of some line that is E′(e),
see Corollary 25(2). The new line must intersect it twice, at a and b, in the or-
der c,b,a,c′. Again there are exactly two ways, up to homeomorphism to deﬁne it.

30
B. Courcelle
If W ′ is a square (ww), there are two line segments representing edges from c to c′.
This seems to yield four ways to extend S′ into S, but actually we get only two, up
to homeomorphism, because these two segments correspond to two edges related
by a v-automorphism of tGra(W′).
□
Anticipating on the sequel, we observe that in Case (2), there are two ways to
extend S′ into S because the curves are oriented. If we consider nonoriented curves,
we get a unique extension, up to homeomorphism.
1.5.2
Curves without Orientation
We now consider curves C1,...,Cn having no particular orientation. The components
of W(C1,...,Cn) are thus reversal-circular: we need not distinguish a sequence from
its reversal. For the three nonoriented curves of Figure 1.3, we get W(C1,C2,C3)
deﬁned by (abcd,aecf,b fde) as well as by (abcd, fcea,b fde) or (cdab,a fce,
deb f).
Consider again the oriented curves of Figure 1.4. The corresponding multiword
W = (abcd,bc,ad) is ambiguous as already observed. However, if we omit the ori-
entations, the two triples of curves are homeomorphic and W is not ambiguous for
describing nonoriented curves. The difference between the oriented and nonoriented
cases is due to the presence of curves with two intersections because reversing the
orientation of such a curve that has two crossings with the others changes the corre-
sponding map in PMt
2,2 but not its undirected version.
If the curves of a tuple (C1,...,Cn) are not oriented, we deﬁne G(C1,...,Cn) as
Und(G(C′
1,...,C′
n)) where C′
i is any orientation of Ci.
Let (C1,...,Cn) and (C′
1,...,C′
n) be two tuples of oriented curves such thatW(C1,...,
Cn) ≍W(C′
1,...,C′
n). (The equivalence ≍combines conjugacy and reversal.) If they
are homeomorphic, then, the tuples of corresponding nonoriented curves are also
homeomorphic. Hence if W(C1,...,Cn) = W is ambiguous for nonoriented curves,
it is also for oriented ones. The converse is not always true. However, it is for
a multiword W provided the nonoriented curves of any tuple that yields it can
be equipped with an orientation depending only on W. This fact motivates the
following deﬁnition.
Deﬁnition 32:Orientable components of multiwords
(1) A word w ∈V ∗is orientable if Conj(w) ̸= Conj(w) (Conj(w) is the set of con-
jugate words of w, cf. Section 4.1.1). It is easy to see that, if this condition is true
for w, then it is also true for every sequence ≍-equivalent to w. A palindrome is
a sequence w such that w = w. A sequence ≍-equivalent to a palindrome is not
orientable. Neither is ab, where a,b ∈V.

1
Gauss Words
31
(2) Let ≤be an arbitrary but ﬁxed linear order on V. From it we get a lexico-
graphic linear order on the set of ﬁnite subsets of V ∗, that we also denote by ≤. For
every orientable w ∈V ∗, we deﬁne:
(2.1) μ(w) = w if Conj(w) < Conj(w), and
(2.2) μ(w) = w if Conj(w) < Conj(w).
If all components of W = (w1,...,wn) ∈(V ∗)n are orientable, we say that W is
orientable and we deﬁne μ(W) := (μ(w1),...,μ(wn)).
Lemma 33: A component of a Gauss multiword is orientable except if it is (conju-
gate to) a palindrome or has length 2.
Proof: In the following proof, a,b,c denote elements of V and u,v,w,w1 etc. denote
elements of V ∗.
Fact: A Gauss multiword has no component of the form aubu or auau with u ̸= ε.
(Otherwise, by constructing the closed straight walk associated with aubu or auau
we get a contradiction with planarity).
■
Let w be a nonempty component of a Gauss multiword such that Conj(w) =
Conj(w). We prove that it is of the form ab, with a ̸= b or is conjugate to a palin-
drome. We distinguish several cases. Recall that w can be replaced by any conjugate
sequence, and that its length is even.
Case 1: w has at least two different letters a,b that have only one occurrence. Then
w = aubv and w ∈Conj(w) = Conj(vbua). We must have w = avbu, hence v = u. But
this contradicts the initially observed fact, except if u = v = ε, which gives w = ab.
In the next two cases, all letters in w have two occurrences in this word.
Case 2: w = uv where u and v are sequences over two disjoint nonempty alphabets
(or w is conjugate to such a sequence). Then w ∈Conj(w) = Conj(vu) implies w =
uv, hence u = u and v = v are two palindromes. If they have both even length, then
w = u1 u1v1 v1. It is conjugate to the palindrome u1v1 v1u1. If they have odd length,
then w = u1a u1v1b v1. But a and b belong to disjoint alphabets, hence a ̸= b and a
has a single occurrence. This case cannot happen.
Hence, the ﬁrst letter of w differs from the last one. The only remaining possibil-
ities are :
Case 3: w = auavbxb and w = aubvaxb.
In the ﬁrst case, we have w ∈Conj(bxbvaua). We must have auavbxb = auabxbv,
hence, v = ε
and u and x are palindromes. If u and x have even length, then
w is conjugate to a palindrome (cf. Case 2). If they have odd length, we have
w = au1c u1abx1c x1b, hence w is conjugate to c u1abx1c x1bau1 which contradicts
the initial fact.
Very similar arguments using the initial fact eliminate the case where w =
aubvaxb.
□
If (C1,...,Cn) is an n-tuple of oriented curves, we denote by NO(C1,...,Cn) the
tuple of nonoriented curves obtained by forgetting the orientations.

32
B. Courcelle
Proposition 34: Let W be a connected Gauss multiword that is orientable or has a
single component.
(1) W is unambiguous if and only if it is for describing nonoriented curves.
(2) If W is ambiguous, there is a bijection that preserves homeomorphisms be-
tween the n-tuples of oriented and of nonoriented curves described by W. Both sets
of n-tuples can be described from any term over atoms that deﬁnes tGra(W).
Proof: We ﬁrst assume that W = (w1,...,wn) is orientable.
Claim: There exists a bijection ν between the n-tuples (C1,...,Cn) of nonoriented
curves described by W and the n-tuples of oriented ones described by μ(W) such
that (C1,...,Cn) = NO(ν(C1,...,Cn)). Furthermore, if (D1,...,Dn) is homeomorphic
to (C1,...,Cn), then ν(D1,...,Dn) is homeomorphic to ν(C1,...,Cn) by the same
homeomorphism of the sphere to itself.
Proof of the claim: Let W and (C1,...,Cn) be as stated. Then (C1,...,Cn) =
NO(C′′
1,...,C′′
n) where C′′
1,...,C′′
n
are oriented curves such that W = W(C′′
1,...,C′′
n).
Let I be the set of indices i such that μ(wi) = wi (i.e., Case (2.2) of Deﬁnition
32 applies). Then we deﬁne ν(C1,...,Cn) as the n-tuple (C′′
1,...,C′′
n) except that we
reverse the orientation of C′′
i
whenever i ∈I.
It is then clear that NO(ν(C1,...,Cn)) = (C1,...,Cn). The mapping ν is a bijection
because the orientations of the curves in ν(C1,...,Cn) are determined from W. The
last assertion is also clear.
■
This claim and the remark after Theorem 26 show that there is a bijection between
the homeomorphism classes of the n-tuples of nonoriented and of oriented curves
described by W, which proves (1) and (2) for orientable multiwords.
It remains to consider the case where n = 1 and W is not orientable. By Lemma
33, this means that W = (w) and w is a palindrome. The results hold if W = (ε).
Otherwise, let M be a planar map of
tGra(W). The map M′ obtained from M by
reversing all edge directions is a planar map of tGra((w)) that is v-isomorphic to
M−1. Hence, up to homeomorphism, the same oriented and nonoriented curves are
described by w and by w which proves (1) and (2). (Note that tGra(W) is prime,
hence that w is unambiguous if and only if w = aa.)
□
With the hypotheses of this proposition, if p is the number of atoms of the t-graph
tGra(W), then the number of pairwise nonhomeomorphic tuples of oriented (resp.
nonoriented) curves described by W is 2p−1 by Corollary 20. Another consequence
of this proposition is that a Gauss word (representing a single curve) is unambiguous
for describing nonoriented curves if and only if it is prime. This fact is proved in a
completely different way in [3].
We now use the reduction of Deﬁnition 27 to handle the cases not covered by
Proposition 34.

1
Gauss Words
33
Lemma 35: Let W be a connected Gauss multiword W that has several components
and is not orientable. It is reducible. There is a bijection θ between the homeo-
morphism classes of the tuples of nonoriented curves described by W and those of
oriented curves described by Red(W).
Proof: Let W be as stated. None of its components is a palindrome because it is
connected and has several components. It has small components and can be written
(ab,ab) or (a1b1,a2b2,...,apbp,w1,..., wm) ∈(V ∗)n as in Deﬁnition 27. In the ﬁrst
case, Red(W) = (ε) and W describes a circle, so the result holds. Otherwise, the
sequences μ(w1),...,μ(wm) are deﬁned because w1,...,wm have length at least 4
and are not palindromes. We let w′
i := μ(wi) ↾(V −{a1,...,ap,b1,...,bp}. Hence,
Red(W) ≍(w′
1,...,w′
m).
For every n-tuple of nonoriented curves (C1,...,Cn) such that W(C1,...,Cn) ≍W,
we let (C′
1,...,C′
n) be an n-tuple of oriented curves such that NO(C′
1,...,C′
n) =
(C1,...,Cn) and
w(C′
i) = μ(wi−p) for each i = p + 1,...,n. Hence, the curves
C′
p+1,...,C′
n have a canonical orientation based on the orientability of their asso-
ciated sequences. For the curves C′
1,...,C′
p, we take any orientation. It is then clear
that W(C′
p+1,...,C′
n) = (w′
1,...,w′
m). We deﬁne θ(C1,...,Cn) as (C′
p+1,...,C′
n).
Every m-tuple of oriented curves (E1,...,Em) such that W(E1,...,Em) = (w′
1,...,
w′
m) is θ(C1,...,Cn) for some C1,...,Cn such that W(C1,...,Cn) ≍W.
Let (D1,...,Dn) be an n-tuple of nonoriented curves such that W(D1,...,Dn) ≍
W. If it is homeomorphic to (C1,...,Cn), then θ(D1,...,Dn) is homeomorphic to
θ(C1,...,Cn). This is so because the n-tuple (D′
1,...,D′
n) associated with (D1,...,Dn)
as (C′
1,...,C′
n) is with (C1,...,Cn) is homeomorphic to (C′
1,...,C′
n) up to the orienta-
tions of the curves D′
1,...,D′
p that have two intersections with the others.
Note that NO(θ(C1,...,Cn)) = (Cp+1,...,Cn). Proposition 31 shows that, up to
homeomorphism, there is a unique way to extend this (n −p)-tuple by nonoriented
curves C1,...,Cp in such a way that W(C1,...,Cn) ≍W. This observation shows that if
θ(D1,...,Dn) is homeomorphic to θ(C1,...,Cn), then (D1,...,Dn) is homeomorphic
to (C1,...,Cn).
□
Figure 1.13 shows an 11-tuple of nonoriented curves. The small components of
the associated multiword W correspond to curves 1 to 6. The reduced multiword
Red(W) has small components corresponding to curves 7 and 9.
The following theorem answers Questions (2) and (3) of the introduction for the
description of nonoriented curves.
Theorem 36: Let W be a connected Gauss multiword. In the following statements,
ambiguity refers to the description of nonoriented curves.
(1) If W has a single component, it is unambigous if and only if it is (ε) or
tGra(W) is prime.
(2) If it has several components, it is unambigous if and only if Red(W) = (ε) or
tGra(Red(W)) is prime.
(3) In both cases, all corresponding nonoriented curves can be determined from
a term over t-atoms that deﬁnes tGra(W) in Case (1) or tGra(Red(W)) in Case (2).

34
B. Courcelle
Fig. 1.13 An 11-tuple of nonoriented curves
Proof: (1) holds by Proposition 34(1) and Theorem 26.
(2) and (3) hold by Proposition 34, Lemma 35,and Theorem 26.
□
The following algorithm analyses a double occurrence multiword W intended to
describe nonoriented curves.
Algorithm 37: Analysis of a double occurrence multiword.
1. We ﬁrst check if it is connected. If it is not, we treat separately each of its
connected submultiwords.
2. Unless W = (ε), we construct the t-graph tGra(W) and we check its planarity.
3. If it is planar, then W is a connected Gauss multiword, and we reduce it into
W ′ := Red(W) (cf. Deﬁnition 27; Red(W) = W if W has a single component) by
keeping track of the reduction steps.
4. We construct the (planar)t-graph tGra(W ′) and a term t over atoms that deﬁnes
it (we use Theorem 13); we let p be the number of atoms used by t.
5. By using Proposition 14, we determine a planar t-map for each t-atom occur-
ring in t.
6. By using Corollary 20, we compute the 2p−1 (terms deﬁning the) planar t-maps
that represent the different plane embeddings of
tGra(W ′), hence, the different
tuples of curves that yield W ′.
7. Since we have recorded the sequence of reductions done at Step 3, and by using
this sequence backwards, we can compute the 2p−1 planar t-maps of
tGra(W),
hence, the different tuples of curves (extending those we have by step 6) described
by W.
Its correctness follows from Theorem 36 and the quoted results.

1
Gauss Words
35
1.6
Some Open Questions
Knots and knot diagrams
The reader will ﬁnd the deﬁnitions and basic facts in the books [1, 2, 10]. Let us
consider oriented links and knots. A (connected) link diagram is nothing but a map
in Mt
2,2 whose vertices are labelled by over or under, to indicate the type of cross-
ing. A knot diagram is such a map that has a unique straight walk. We let k map a
diagram to the corresponding link or knot. Knots can be composed by a binary (mul-
tivalued) operation denoted in [1] by # such that for every two knot diagrams D and
D′, every edge e of D and every edge f of D′, we have k(D⊞e,f D′) = k(D)#k(D′).
It follows that the diagrams of prime knots are atoms. We leave as a research topic
to investigate the relationships between the unique factorization of a knot and the
atomic decompositions of its diagrams.
Planar t-graphs
A t-graph G contains more information than the underlying graph Graph(G) and
less information than a map M in Mt
2,2 such that Grapht(M) = G. Planar graphs
and planar maps are characterized by ﬁnitely many forbidden minors ( [7, 14] for
graphs, [4,6] for maps). Our question is:
Are planar t-graphs characterized by ﬁnitely many forbidden
substructures of some kind?
We have given a natural characterization of Gauss multiwords (Theorem 24(2)),
hence also, of Gauss words. However, Gauss words also a characterization in terms
of forbiden subwords [12].
Can it be extended to Gauss multiwords?
Many extensions remain to be investigated. First, intersections of curves on other
surfaces than the plane (see [11]). Our characterization of Gauss multiwords in terms
of the planarity of an associated t-graph extends easily to other surfaces. Since the
embeddability of a graph in an arbitrary given surface can be checked in linear time
by an algorithm that produces embeddings ( [13]), the algorithm of Proposition
14 can be turned into a linear-time algorithm for checking if a double occurrence
multiword is associated with curves in a given surface.
Can one extend to other surfaces the characterization (resulting from
Corollary 20 and its consequences) of all tuples of curves in a given
surface that correspond to a given double occurrence multiword ?
Some other questions:
Can the results of this article be generalized to the description of
curves with multiple intersections?
Gauss multiwords are deﬁned from curves that cross, themselves or pairwise.
One could also consider curves that have touching points i.e., that can share a vertex
without crossing at it. This notion is used in [8]. Special labels in double occurrence
multiwords could encode touching points.

36
B. Courcelle
What are the associated multiwords and when are they unambiguous ?
What are the tuples of curves with crossing and touching points
associated with a given multiword ?
Acknowledgements. I thank E. Gioan and the late M. Las Vergnas for many helpful
comments.
References
1. Adams, C.: The knot book. AMS (2004)
2. Bollobas, B.: Modern graph theory. Springer (2001)
3. Chaves, N., Weber, C.: Plombages de rubans et probl`eme des mots de Gauss. Exp.
Math. 12, 53–77 (1994)
4. Courcelle, B.: The monadic second-order logic of graphs XII: Planar graphs and planar
maps. Theoret. Comput. Sci. 237, 1–32 (2000)
5. Courcelle, B.: The atomic decomposition of strongly connected graphs, Research report
(June 2013), http://hal.archives-ouvertes.fr/hal-00875661
6. Courcelle, B., Dussaux, V.: Map genus, forbidden maps and monadic second-order logic.
The Electronic Journal of Combinatorics 9(1), 40 (2002),
http://www.combinatorics.org/
Volume 9/Abstracts/v9i1r40.html
7. Diestel, R.: Graph theory, 4th edn. Springer (2010),
http://diestel-graph-theory.com
8. de Fraysseix, H., Ossona de Mendez, P.: On a characterization of Gauss codes. Discrete
Comput. Geom. 22, 287–295 (1999)
9. Galil, Z., Italiano, G.: Maintaining the 3-edge-connected components of a graph on-line.
SIAM J. Comput. 22, 11–28 (1993)
10. Godsil, C., Royle, G.: Algebraic graph theory. Springer (2001)
11. Lins, S., Richter, B., Shank, H.: The Gauss code problem off the plane. Aequationes
Mathematicae 33, 81–95 (1987)
12. Lovasz, L., Marx, M.: A forbidden substructure characterization of Gauss codes. Bulletin
of the AMS 82, 121–122 (1976)
13. Mohar, B.: A linear time algorithm for embedding graphs in an arbitrary surface. SIAM
J. Discrete Math. 12, 6–26 (1999)
14. Mohar, B., Thomassen, C.: Graphs on surfaces. The Johns Hopkins University Press
(2001)
15. Naor, D., Gusﬁeld, D., Martel, C.: A fast algorithm for optimally increasing the edge
connectivity. SIAM J. Comput. 26, 1139–1165 (1997)
16. Rosenstiehl, P.: Solution alg´ebrique du probl`eme Gauss sur la permutation des points
d’intersection d’une ou plusieurs courbes ferm´ees du plan. C. R. Acad. Sc. Paris, S´er.
A 283, 551–553 (1976)
17. Tsin, Y.H.: A simple 3-edge-connected component algorithm. Theory Comput. Syst. 40,
125–142 (2007)

1
Gauss Words
37
Appendix
Index to some deﬁnitions.
Isomorphisms
Isomorphism of graphs : notation G ∼= H, Section 1.1.1.
v-isomorphism of graphs (identity on vertices) : Section 1.1.1.
Automorphisms and v-automorphisms : Section 1.1.1.
v isomorphisms of t-graphs : Section 1.1.2, Deﬁnition 2.
Homeomorphisms
Homeomorphisms and equivalence of maps : Section 1.2, Deﬁnition 4.
Equivalences
Conjugacy of sequences : notation ≡, Section 4.1.1.
Conjugacy combined with reversal : notation ≍, Section 4.1.1.
Equivalent maps (M = N or M = N−1) : Section 1.2, Deﬁnition 4.
t-equivalent maps (same associated t-graph, it is implied by equivalence) : nota-
tion ∼t, Section 1.2, Deﬁnition 5.
Equivalent multiwords (by reversal of some components) : notation W ≍W ′,
Section 4.1.1.
Equivalent terms (same value) : Section 2.2, Deﬁnition 11.

Chapter 2
Logical Theory of the Additive Monoid
of Subsets of Natural Integers
Christian Choffrut and Serge Grigorieff
Abstract. We consider the logical theory of the monoid of subsets of N endowed
solely with addition lifted to sets: no other set theoretical predicate or function, no
constant (contrarily to previous work by ˙Jez and Okhotin cited below). We prove that
the class of true Σ5 formulas is undecidable and that the whole theory is recursively
isomorphic to second-order arithmetic. Also, each ultimately periodic set A (viewed
as a predicate X = A) is Π4 deﬁnable and their collection is Σ6. Though these un-
decidability results are not surprising, they involve technical difﬁculties witnessed
by the following facts: 1) no elementary predicate or operation on sets (inclusion,
union, intersection, complementation, adjunction of 0) is deﬁnable, 2) The class of
subsemigroups is not deﬁnable though that of submonoids is easily deﬁnable. To get
our results, we code integers by a Π3 deﬁnable class of submonoids and arithmetic
operations on N by Δ5 operations on this class.
2.1
Introduction
The object of this paper could hardly be more elementary since we are concerned
with the class of subsets of nonnegative integers equipped with addition as unique
operation. Presburger studied in 1929 the ﬁrst-order logic of integers with addition
and showed that this logic admits quantiﬁer elimination on the language enriched
with the order relation and all arithmetic congruences. Consequently, the theory is
decidable and it was proved in 1974 [3] by Michael J. Fischer and Michael O. Ra-
bin that its time complexity is upper bounded by a double exponential. In the sixties,
the class of relations deﬁned by the logic received a simple algebraic characteriza-
tion by Seymour Ginsburg as the semilinear subsets of integers, [5]. It can thus be
reasonably said that this logic is well-understood.
Christian Choffrut · Serge Grigorieff
LIAFA, CNRS and Universit´e Paris 7 Denis Diderot, France
e-mail: seg@liafa.univ-paris-diderot.fr
c⃝Springer International Publishing Switzerland 2015
39
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_2

40
C. Choffrut and S. Grigorieff
Our purpose is still a ﬁrst-order theory but the domain is the power set of N with
set addition and equality, formally ⟨P(N);+,=⟩. At the beginning of our investi-
gation we came up every day with different properties. Some could be considered
as the source of inspiration for exercises in an introductory course in logic or as
entertaining mathematical recreation. Others played a more important role and are
kept in this paper, but all had a low complexity in the arithmetical hierarchy, i.e.,
they were expressible with very few quantiﬁer alternations. However, we could not
build on them to get a consistent and general view of the problem. E.g., we were
not even able to answer the question whether or not the property for a subset to be
recognizable by a ﬁnite automaton is expressible. The ﬁnal picture to the contrary is
that the expressiveness of the theory is extremely powerful but, at least the way we
did it, this was obtained by working out predicates of higher complexity.
It should not be surprising that the submonoids of N play a crucial role since
a nonempty subset X is a submonoid if and only if it satisﬁes the condition X +
X = X. Submonoids of N have a deceiving simplicity. Contrarily to submonoids of
nonunary free monoids, they are ﬁnitely generated. They are related to an intrigu-
ing and well-celebrated problem attributed to Frobenius which asks the following.
Say a submonoid is numerical if it is generated by a ﬁnite subset of integers with
greatest common divisor equal to 1. These submonoids are known to be coﬁnite in
N but what precisely is the largest integer not in the submonoid? There is a rich
literature on the topic and many conferences are dedicated to the classiﬁcation of
these monoids [2,4,9,11], nonetheless the problem seems to be far from solved and
we could not ﬁnd any result that would help us in our investigation.
We now turn to a quick presentation of our work. Section 2.2 gathers all basic
material of algebraic or logical type used in the sequel and is essentially meant for
the reader unfamiliar with the domain. Section 2.3 establishes the main properties of
our paper which can be summarized as saying that under certain restrictions which
cannot be relaxed, membership of an element to a subset and subset inclusion can be
expressed via special (and simple) submonoids. Based on these results, Section 2.4
shows that the theory is highly undecidable by interpreting the second-order theory
of arithmetic in ⟨P(N);+,=⟩. Actually, the Σ5 fragment is already undecidable, see
Theorem 7. At this point of the article the only useful subsets all contain the integer
0. Section 2.5 investigates to what extent other classes of subsets are expressible.
We show in particular that we cannot express the fact that a subset is obtained from
another by just adding 0. The same is true of the simplest set theoretical predicates
(inclusion, union, intersection, complementation) and of the class of subsemigroups
of N. Nevertheless, this leaves the problem of trying to extend the classes of subsets
expressible in the logic which is done in section 2.6. In particular, we show that
a singleton class {A} is deﬁnable with set addition if and only if it is deﬁnable in
second-order arithmetic. An inventory of typical predicates expressible in the theory
to be found in Section 2.7 serves an illustrative purpose among which the class of
regular subsets of N which is Σ6 deﬁnable.
It is worthwhile mentioning the works of ˙Jez and Okhotin since they can be in-
terpreted as studying the Diophantine theory of the current structure enriched with

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
41
all ultimately periodic subsets of N (as set constants). In [6] they show that there
exists an encoding of the subsets of N under which each recursive subset of N is the
encoding of the unique solution of some system of equations involving the operation
of sum of subsets and the regular subsets as unique constants. Furthermore they
prove the satisﬁability of this theory to be Π 0
1 -complete. Because ultimately periodic
constants are Π4 deﬁnable, their undecidable result is in accordance with ours and
leaves the open question of ﬁnding the minimum undecidable fragment.
2.2
Preliminaries
In this section we recall classical and introduce elementary properties of two types:
algebraic and logic.
2.2.1
Submonoids
Given a non negative integer n and two subsets X,Y ⊆N we deﬁne
nX = {nx | x ∈X}
(2.1)
X +Y = {x+ y | x ∈X,y ∈Y}
(2.2)
Observe that 2X ̸= X + X.
Deﬁnition 1. A subset X ⊆N is a subsemigroup if it is closed under addition, i.e.,
X +X ⊆X. A subsemigroup is a submonoid if it is nonempty and contains 0. Equiv-
alently a submonoid is a nonempty subset satisfying the condition X + X = X.
The submonoid generated by Y, denoted Y ∗, is the minimum submonoid contain-
ing Y, i.e. containing every ﬁnite sum of elements of Y
Y ∗= {0} ∪

n≥1
n times



Y + ···+Y
The subset Y is a generating subset of Y ∗.
We are concerned with the ﬁrst-order theory of ⟨P(N);+,=⟩. It is not surprising
that the submonoids of N play a central role since a set X containing 0 is a sub-
monoid if and only if X = X + X holds. Most of the following is folklore and does
not contain anything new. For the sake of completeness we recall it with some detail.
We start with two trivial observations the proofs of which are omitted.
Proposition 1. A set X ⊆N such that 0 ∈X is a monoid if and only if X \ {0} is a
subsemigroup.

42
C. Choffrut and S. Grigorieff
Proposition 2. If X and Y are two submonoids, so is X +Y.
A remarkable property of N is that its submonoids are ﬁnitely generated. This
can be stated more precisely.
Proposition 3. 1. Every submonoid X of N is ﬁnitely generated and has a minimum
generating set G(X) which is equal to
G(X) = S \ (S + S)
where S = X \ {0}
(2.3)
The set G(X) is called the minimum generator or the minimum generating set of X
and its elements are called the generators of X.
2. A submonoid X is of the form {0} or of the form
X = b

F ∪(a + N)
	
(2.4)
where b ≥1 and 0 ∈F ⊆{0,...,a −1}.
Observe that the numeric submonoids (i.e. those generated by a ﬁnite subset of
N with greatest common divisor equal to 1, cf. [4, 11]) correspond to b = 1. They
are exactly the submonoids which are coﬁnite.
Proof. 1. Let b be the greatest common divisor of the elements in X. Then
X ⊆bN. Let a1,...,an ∈X such that g.c.d.(a1,...,an) = b. By B´ezout there exist
x1,...,xℓ,y1,...,yn−ℓ∈N such that
x1a1 + ···+ xℓaℓ−y1aℓ+1 −···−yn−ℓan = b
(up to a permutation of a1,...,an). Let L = x1a1 + ··· + xℓaℓand R = y1aℓ+1 +
... + yn−ℓan and observe that L,R ⊆bN. For all integers k, set k = qa1 + r where
0 ≤r < a1 and q ∈N. Then
(a1 −1)R+ kb = qba1 + (a1 −1 −r)R+ rL ∈a1N+ ···+ anN.
which shows that all multiples of b greater than (a1−1)R are generated by a1,...,an
hence that X is generated by the ﬁnite class consisting of all its elements less than
or equal to
max{a1,...,an,(a1 −1)R}
(2.5)
It is clear that G(X) as in equation (2.3) generates X. Assume by contradiction that
there exists a generating set H not containing G(X) and let α be an element in
G(X)\ H. We assume without loss of generality that 0 ̸∈H. Since X = H∗we have
α = β + γ where β ∈H and γ ∈H∗\ {0}. Then α ∈S + S which contradicts the
deﬁnition of G(X).
2. Observe that the integer in (2.5) is a multiple of b, say ab. Then in order to obtain
(2.4) it sufﬁces to consider the subset F satisfying bF = X ∩b{0,...,a −1}
The speciﬁc form of nonzero submonoids leads us to the following general
notion.

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
43
Deﬁnition 2. A set X ⊆N has ultimate period b if there exists an integer K such
that for all n ≥K we have
n ∈X ⇐⇒n + b ∈X
If this holds for some b ≥1, X is ultimately periodic.
The following result is classical.
Proposition 4. Let X ⊆N.
1. The following conditions are equivalent:
(i) X is ultimately periodic,
(ii) X is a regular subset of N.
(iii) X = A∪(B+ bN) where a,b ∈N, /0 ̸= A ⊆[0,a[ and B ⊆[a,a + p[.
The set X is ﬁnite if and only if B = /0 in (iii).
2. If X has ultimate period b then all multiples of b are ultimate periods.
3. When X is a submonoid, the integer b of equation (2.4) is its minimum ultimate
period.
Remark 1. Equation (2.3) of Proposition 3 gives a simple algorithm to compute
G(X) = {g0,...,gn} provided its ultimate period b is known. Suppose the sub-
monoid X has minimum nonzero element m. Let g0 = m and let gi+1 be the mini-
mum element of X not in ∑j=i
j=0 g jN. Halt when {g0,...,gn} generate m/b successive
elements of the periodic tail of X.
The above representation of submonoids allows us to express the inclusion and
intersection of submonoids simply.
Proposition 5. Any pair of nonzero submonoids X,Y is of the form

X = bF ∪(ad + bN)
Y = cG∪(ad + cN)
with
⎧
⎨
⎩
b ≥1, c ≥1, d = l.c.m.(b,c)
0 ∈F ∩G,
F,G ﬁnite
bF ∪cG ⊆{0,...,ad −1}
The intersection monoid is X ∩Y = (bF ∩cG)∪(ad + dN).
In particular, X ⊆Y if and only if bF ⊆cG and c divides b.
2.2.2
Maximal Submonoids
Notation 1. We write X ◁Y whenever X is an inclusion-maximal proper submonoid
of the submonoid Y.
Proposition 6. Let X be a submonoid of N with G(X) as minimum generating set.
1. The proper maximal submonoids of X are the sets X \ {g} where g ∈G(X).
2. Every generator of X distinct from g is a generator of X \ {g} (but there may be
other ones, cf. Example 1). In other words,

44
C. Choffrut and S. Grigorieff
G(X)\ {g} ⊆G(X \ {g})
(2.6)
Proof. Let g ∈G(X). All elements in X \ {g} are of the form
∑
h∈G
xhh
with
xg = 0
or
∑
h∈G
xh ≥2
These elements clearly deﬁne a proper submonoid of X and this monoid is maximal.
Conversely, consider a proper submonoid X′ of X. There exists some g ∈G(X)\X′
hence X′ ⊆X \ {g} and equality holds in case X′ is maximal.
Finally, for g ∈G(X), the following inclusion is straightforward:

S \ (S + S)
	
\ {g} ⊆(S \ {g})\

(S \ {g})+ (S \ {g})
	
.
Thus, every X-generator distinct from g is an (X \ {g})-generator.
Example 1. The subset X = {0}∪{3,5,6}∪8+N is the submonoid with minimum
generating set {3,5} (use Remark 1). It thus has two proper maximal submonoids
X1◁X and X2 ◁X. The minimum generating set of X1 = X \{3} is {5,6,8,9} and the
minimum generating set of X2 = X \{5} is {3,8,10}. Thus X1 has 4 proper maximal
submonoids and X2 has 3 proper maximal submonoids.
Consequently, every submonoid which is not reduced to 0 has ﬁnitely many
proper maximal submonoids but some monoids fail to have minimal proper super-
monoids. The following result characterizes them.
Proposition 7. The submonoids which have no minimal supermonoid are {0} and
the sets bN, b ≥0.
Proof. Suppose Y is a minimal nonzero supermonoid of X and let G(Y) be its min-
imum generating set. Then X = Y \{g} for some g ∈G(Y) and G(Y)\{g} ⊆G(X).
First, using Proposition 3, we show that {0} and the sets bN, b ≥1 have no minimal
supermonoid. We argue by way of contradiction.
Case X = {0}. Then G(X) = /0 hence G(Y) = {g} so that Y = gN. Now, 2gN is
a submonoid strictly between {0} and Y, contradicting the minimality of Y over X.
Case G(X) = {b}, i.e. X = bN. For b = 1 it is trivial so we assume b > 1. Then
G(Y) has at most two generators. It cannot have only one generator because G(X)
would be empty. Thus, G(Y) = {b,c} for some c /∈bN. Then by Propostion 2, bN+
(b+1)cN is a submonoid strictly between X = bN and Y = bN+cN, contradicting
the minimality of Y over X.
We now show that every submonoid X distinct from {0} and the sets bN, b ≥
1, has a minimal supermonoid. This condition on X, together with Equation (2.4)
supra, insure that X = b(F ∪(a + N)) with b ≥1, 0 ∈F, a ≥2 and a −1 /∈F. The
set Y = {0} ∪b((a −1) + N) is clearly a submonoid. By Proposition 2, X +Y is a
supermonoid of X. A simple computation shows that (X +Y)\X = {b(a−1)}, i.e.,
that X +Y is a minimal supermonoid of X.

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
45
2.2.3
Basic Predicates
In further sections we try to evaluate the complexity of the predicates which are ex-
pressible in the logic. Here we content ourselves with gathering the most elementary
predicates.
We recall that a predicate is Σn (resp. Πn) if it is deﬁned by a formula that begins
with some existential (resp. universal) quantiﬁers and alternates n−1 times between
series of existential and universal quantiﬁers. It is Δn if it is both Σn and Πn. It is
Σn ∧Πn if it is equivalent to a conjunction of a Σn and a Πn formulas. We assume the
reader has some familiarity with computing the logical complexity. As an example
of the type of computation consider the Σ1 ∧Π1 formula
θ(x,y,z,t,u) ≡∃t φ(x,y,t) ∧∀u ψ(x,z,u)
Assume that x is the sole common free variable of φ and ψ. Then
θ(x,y,z,t,u) ⇐⇒∃t ∀u

φ(x,y,t)∧ψ(x,z,u)
	
⇐⇒∀u ∃t

φ(x,y,t)∧ψ(x,z,u)
	
∃x θ(x,y,z,t,u) ⇐⇒∃x ∃t ∀u

φ(x,y,t)∧ψ(x,z,u)
	
showing both that the predicate associated to θ is Δ2 and that ∃xθ is Σ2. Such a type
of computation will not be explicitly carried out in the sequel.
2.2.3.1
Removing Deﬁnable Constants
Since we deal with deﬁnability in a particular structure, the following classical result
in logic will be useful.
Proposition 8. Let M be any logical structure and a an element of M. Let n, p ∈N.
Suppose a is Δn deﬁnable in M, i.e. x = a is equivalent to a Σn formula and to a Πn
formula. Then, for every Σp (resp. Πp) formula φ(x,y) (with free variables x and
possibly some other ones), there exists a Σmax(n,p) (resp. Πmax(n,p)) formula ψ(y)
such that φ(a,y) is equivalent to ψ(y).
Proof. Suppose x = a is equivalent to formulas
∃v1 ∀v2...Qnvn F(v1,...,vn,x)
,
∀v1 ∃v2...Rnvn G(v1,...,vn,x)
where Qn (resp. Rn) is ∀if n is even (resp. odd) and is ∃otherwise.
Letting s = max(n, p), if φ is ∃z1 ∀z2 ...Qpzp A(z1,...,zp,x,y) then
φ(a,y) ⇐⇒∃z1 ∃v1 ∀z2 ∀v2...Qszs Qsvs

F(v1,...,vn,x) ∧A(z1,...,zp,x,y)
	
and if φ is ∀z1 ∃z2 ...Rpzp B(z1,...,zp,x,y) then

46
C. Choffrut and S. Grigorieff
φ(a,y) ⇐⇒∀z1 ∀v1 ∃z2 ∃v2...Rszs Rsvs

G(v1,...,vn,x) ⇒B(z1,...,zp,x,y)
	
2.2.3.2
Basic Constants
Proposition 9. 1. The predicate X = /0 is Π1.
2. The predicate X = {0} is Π1.
3. The predicate 0 ∈X is Σ1.
4. The predicate X = N is Σ1 ∧Π1.
Proof. 1. X = /0 if and only if ∀Y X +Y = X.
2. {0} is the neutral element of P(N) hence is the unique set satisfying ∀Y X +Y =
Y.
3. 0 ∈X if and only if ∃Y (Y ̸= /0 ∧X +Y = Y).
4. X = N if and only if 0 ∈X ∧∀Y (0 ∈Y ⇒X +Y = X).
2.2.3.3
Some Classes of Subsets
The following two classes of subsets are easily deﬁnable.
Proposition 10. 1. The class of submonoids of N is Σ1 deﬁnable.
2. The class of ﬁnal segments {n + N | n ∈N} is Σ1 ∧Π1 deﬁnable.
Proof. 1. X is a submonoid if and only if it is nonempty and X + X = X.
2. X ∈{n + N | n ∈N} if and only if X ̸= /0 ∧∀Y (0 ∈Y ⇒X +Y = X).
2.2.3.4
Minimum Element in a Set
The minimum element of a nonempty set X is denoted by minX.
Proposition 11. The following predicates are deﬁnable by formulas of the stated
complexity:
1. (a) minX ≤k is Π2
for k ≥1
(b) minX = 0 is Σ1
(c) minX = 1 is Π2
(d) minX = k is Σ2 ∧Π2 for k ≥2
2. (a) minX ≤minY
is Σ1
(b) minX = minY
is Σ1
(c) minX ≤minY + k is Π2
for k ≥1
(d) minX = minY + 1 is Π2
(e) minX = minY + k is Σ2 ∧Π2 for k ≥2
3. (a) minX + minY ≤minZ is Σ1
(b) minX + minY = minZ is Σ1

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
47
Proof. 1a. minX ≤k if and only if X ̸= /0 and X is not the sum of k + 1 sets which
do not contain 0 :
X ̸= /0 ∧∀X1,...,Xk+1 (X = X1 + ···+ Xk+1 ⇒
i=k+1

i=1
0 ∈Xi)
Claim 3 of Proposition 9 yields the stated logical complexity.
1b. Again use claim 3 of Proposition 9.
1c. Express that minX ≤1 and minX ̸= 0.
1d. Express that minX ≤k and minX ̸≤k −1.
2a. minX ≤minY if and only if
∃A,B,R,S (0 ∈R ∧0 ∈S ∧X = A+ R ∧Y = A+ B+ S)
Only if. Set A = {minX}, B = {minY −minX}, and R = X −minX and S = Y −
minY.
If. minX = minA ≤minA+ minB = minY.
2b. Express that minX ≤minY and minY ≤minX.
2c. minX ≤minY + k if and only if
∀A,R,B1,...,Bk+1
((Y = A+ R ∧0 ∈R ∧X = A+ B1 + ···+ Bk+1) ⇒
i=k+1

i=1
0 ∈Bi)
2d & 2e. Express that minX ≤minY + k and minX ̸≤minY + k −1.
3a. minX + minY ≤minZ if and only if
∃A,B,R,S,T (0 ∈R ∧0 ∈S
∧X = A+ R ∧Y = B+ S ∧Z = A+ B+ T)
3b. minX + minY = minZ if and only if
∃A,B,R,S,T (0 ∈R ∧0 ∈S ∧0 ∈T
∧X = A+ R ∧Y = B+ S ∧Z = A+ B+ T)
2.2.3.5
Singleton Sets
Proposition 12. 1. The predicate “X is a singleton” is Π2.
2. The predicate X = {0} is Π1.
3. The predicate X = {k} is Σ2 ∧Π2 for k ≥1.
4. The predicate Y ̸= /0 ∧X = {minY} is Π2

48
C. Choffrut and S. Grigorieff
Proof. 1. X is a singleton if and only if
X ̸= /0 and ∀Y ((Y ̸= /0 ∧minY = minX) ⇒∃Z Y = X + Z)
Only if. Let X = {ℓ}. Since minY = ℓwe have (Y −ℓ)+ {ℓ} = Y hence we can let
Z = Y −ℓ.
If. Let Y = {minX}. Equality {minX} = X + Z implies X and Z are singleton sets
and X = {minX} and Z = {0}.
Complexity: to remove the constant /0, apply Proposition 8.
2. Already done in claim 2 of Proposition 9.
3. X = {k} if and only if X is a singleton and minX = k. We conclude with claim 3
of Proposition 11.
4. We express that X is a singleton and minX = minY.
2.3
Using Submonoids to Approximate and Emulate
2.3.1
Special Cases of Inclusion and Membership
The importance of the submonoids of N relies on the fact that they provide some
approximation of two important relations, namely subset inclusion Y ⊆X and mem-
bership x ∈X.
2.3.1.1
Special Cases of Inclusion
We will prove in Theorem 11 that the inclusion predicate Y ⊆X is not expressible
in ⟨P(N);+,=⟩. However, when X is a submonoid and Y contains 0 the inclusion
is expressible.
Proposition 13. Let 0 ∈Y and let X be a submonoid. Then
Y ⊆X ⇐⇒Y + X = X
Proof. Indeed, from right to left we have X = Y + X ⊇Y + {0} = Y. Conversely,
since 0 ∈Y we have X ⊆Y + X and since Y ⊆X and X is a submonoid we have
Y + X ⊆X + X = X.
The following result helps us to tightly evaluate syntactical complexity.
Proposition 14. The predicate ◁is Σ1 ∧Π1. Also, it is Π1 on submonoids: there
exists a Π1 formula F(X,Y) such that
X,Y are submonoids ⇒(F(X,Y) ⇔X ◁Y)
Proof. Indeed, Y ◁X if and only if
X,Y are submonoids ∧Y ⊆X ∧Y ̸= X
∧∀Z

(Z is a submonoid ∧Y ⊆Z ⊆X) ⇒(Z = Y ∨Z = X)
	
To conclude we use Propositions 10 and 13.

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
49
2.3.1.2
Special Cases of Membership
Concerning the approximation of membership we use a special (whence the no-
tation) type of submonoids which is appropriate to evaluate the complexity of the
logic.
Deﬁnition 3. For each integer n ≥1 we let
Sn = {0} ∪(n + N)
These submonoids are called special. The class of special submonoids is denoted
by Special.
E.g., S1 = N and S2 = {0} ∪(2 + N) = N \ {1} is the largest proper submonoid of
N since the minimum generating subset of N is {1}, cf. Proposition 6 Claim 1.
The following shows that the submonoids Sn, n ≥1, can be used to test member-
ship of a ﬁxed n in X provided n is strictly greater than min(X). In particular, if X
contains 0 then the property “n > 0 and belongs to X” for a ﬁxed n is expressible
since the restriction n greater than 0 is no longer necessary, see Proposition 9 Claim
3. Deﬁnability of the subset Sn is done in Theorem 2.
Lemma 1. Let m ∈N and n ≥1. If X ̸= /0 and m = minX then m+n ∈X if and only
if X + Sn = X + Sn+1.
Proof. Observe that
X + Sn = X + ({0} ∪(n + N))
= (X + {0})∪(X + (n + N))
= X ∪((m+ n)+ N)
X + Sn+1 = X ∪((m+ n + 1)+ N) .
Thus, X + Sn+1 ⊆X + Sn and (X + Sn)\ (X + Sn+1) = {m+ n} \ X.
2.3.2
Deﬁnability Issues of Special Submonoids
We ﬁrst show that each Sn can be deﬁned. This is done by carefully investigating
their proper maximal submonoids. In a second step we show that the fact of being
an Sn, i.e., the class {Sn | n ≥1} is deﬁnable. This also relies on properties of the
containment relation between submonoids.

50
C. Choffrut and S. Grigorieff
2.3.2.1
Deﬁnability of Each Special Submonoid
Recall the notation G(X) for the minimum generating set of X, Proposition 3.
Lemma 2. Assume n ≥1.
1. G(Sn) = {n,...,2n −1}. Thus (cf. Proposition 6), Sn is a monoid with exactly n
maximal proper submonoids.
2. G(Sn \ {n}) = G(Sn+1) = {n + 1,...,2n + 1}. Thus, Sn \ {n} is a monoid with
exactly n + 1 maximal proper submonoids.
3. G(Sn \ {n + 1}) = {n} ∪{n + 2,...,2n −1} ∪{2n + 1}. Thus, Sn \ {n + 1} is a
monoid with exactly n maximal proper submonoids.
4. If n+2 ≤i ≤2n−1 then G(Sn\{i}) = {n,n+1,...,i−1,i+1,...,2n−1}. Thus,
Sn \ {i} is a monoid with exactly n−1 maximal proper submonoids.
Proof. The right to left inclusions of the form G(...) ⊇... of the four claims are
straightforward. We check the left to right inclusions.
1. For all integers k ≥0 we have 2n + k = n + (n + k) ∈(X \ {0})+ (X \ {0}).
2. We apply Claim 1 with n + 1 in place of n.
3. For all k ≥2 we have 2n + k = n + (n + k) ∈(X \ {0})+ (X \ {0}).
4. For all 0 ≤k ̸= i we have 2n + k = n + (n + k) ∈(X \ {0}) + (X \ {0}) and for
k = i we have 2n + i = (n + 1)+ (n + i−1).
We now convert the previous result formally in our logic.
Theorem 2. 1. The predicate X = S1 is Σ1 ∧Π1.
2. For each n ≥2, the predicate X = Sn is Δ2.
Proof. 1. Since S1 = N this is Proposition 9 Claim 4.
2. In view of applying Lemma 2, we introduce variables X1,...,Xn to represent
S1,...,Sn and consider some formulas involving these variables.
(1) Let A be the formula expressing X1 = N.
(2) Let B be the formula expressing Xn ◁Xn−1 ◁···◁X2 ◁X1.
(3) Let C be the formula which expresses that, for m = 1,...,n, there exist m+ 1
pairwise distinct maximal proper submonoids of Xm.
Lemma 2 insures that every maximal submonoid of Sm has at most m maximal
submonoids, except Sm \{m} = Sm+1 which has m+1 maximal submonoids. Thus,
a straightforward induction on m = 1,...,n shows that the formula A∧B∧C implies
that Xm = Sm. As a consequence, both formulas
∃X1 ...Xn (A∧B∧C∧Xn = X)
and ∀X1 ...Xn (A∧B∧C ⇒Xn = X)
express that X = Sn. By Proposition 14, formulas A, B are Σ1 ∧Π1 and formula C is
Σ2. This shows that the predicate X = Sn is Σ2 and is Π2.

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
51
2.3.2.2
Deﬁnability of the Class of Special Submonoids
We now look for a formula deﬁning the class of special submonoids Sn, n ≥1.
Deﬁnition 4. If M is a submonoid of N with m as minimum nonzero element, we
denote by ∂M the submonoid M \ {m} of M.
The following technical result based on Proposition 6 is crucial for a deﬁnition of
the submonoids Sn. It is general and relates the generators of a submonoid with its
maximal submonoids.
Proposition 15. Let M be a submonoid of N, K a maximal submonoid of M and g
a generator of M such that K = M \ {g}. For k ∈N, the following conditions are
equivalent:
(1) There are exactly k generators of M \ {g} which are not in G(M), i.e. G(M \
{g}) is equal to G(M)\ {g} augmented with k elements.
(2) There are exactly k sets in the class Z of maximal submonoids L of K such
that K is the unique submonoid Z satisfying L◁Z ◁M.
(3) There are exactly k sets in the class Y of maximal submonoids L of K such
that K is the unique submonoid Z satisfying L ⊊Z ⊊M.
Proof. We use Proposition 3. Any maximal submonoid L of K is of the form L =
K \ {h} = M \ {g,h} for some h ∈G(K). There are obviously only two sets Z such
that M \{g,h} ⊊Z ⊊M, namely M \{g} = K and M \{h}. Thus, K ∈Y if and only
if M \ {h} is not a submonoid. Observe that the sole possible reason for a failure
of L ◁M \ {h} ◁M is that M \ {h} is not a submonoid. Thus, K ∈Z if and only if
M \ {h} is not a submonoid. To conclude, observe that M \ {h} is not a submonoid
if and only if h /∈G(M).
Deﬁnition 5. Let M be a submonoid of N and k ∈N. A generator g of M is k-creative
if condition (1) of Proposition 15 holds. A maximal submonoid K of M is k-creative
if condition (2) of Proposition 15 holds, i.e. if K = M \ {g} where g is a k-creative
generator of M.
We shall write (≥ℓ)-creative to mean k-creative for some k ≥ℓ.
Proposition 3 yields the following result.
Lemma 3. If M is a submonoid different from {0} then m = min(M\{0}) is a (≥2)-
creative generator of M. Thus, ∂M is a (≥2)-creative maximal submonoid of M.
Proof. Since m cannot be a sum of two nonzero elements of M we see that m ∈
G(M). Also, G(M \ {m}) ⊇{2m,m+ p} hence g is (≥2)-creative.
Deﬁnition 6. A submonoid M of N is good if ∂M is its unique maximal submonoid
which is is (≥2)-creative, i.e. minM is the sole (≥2)-creative generator of M.
Lemma 4. The submonoids Sn, n ≥2, are good.

52
C. Choffrut and S. Grigorieff
Proof. We use Lemma 2. The generators of Sn are n,...,2n −1.
Case of Sn \ {n}. The generators of Sn \ {n} = Sn+1 are n + 1,...,2n + 1. Only two
of them (namely, 2n,2n+1) are not in G(Sn). Thus, n is a 2-creative generator of Sn.
Case of Sn \ {n + 1}. The generators of Sn \ {n + 1} are the elements in {n} ∪{n +
2,...,2n −1} ∪{2n + 1}. Only one of them (namely, 2n + 1) is not in G(Sn). Thus,
n + 1 is a 1-creative generator of Sn.
Case of Sn \{i} with n+2 ≤i ≤2n−1. The generators of Sn \{i} are the elements
in {n,...,2n −1} \ {i}. All of them are in G(Sn). Thus, i is a 0-creative generator
of Sn.
This shows that n = minSn is the sole (≥2)-creative generator of Sn.
The submonoids Sn are not the sole examples of good submonoids.
Example 2. The monoid M = {0,6,7,8,9} ∪(11 + N) with minimum generating
set {6,7,8,9,11} is good. Indeed, using Remark 1, we get the facts shown in the
following table:
g
M \ {g}
G(M \ {g})
6 {0,7,8,9} ∪(11 + N) {7,8,9,11,12,13}
6 is 2-creative
7 {0,6,8,9} ∪(11 + N)
{6,8,9,11,13}
7 is 1-creative
8 {0,6,7,9} ∪(11 + N)
{6,7,9,11}
8 is 0-creative
9 {0,6,7,8} ∪(11 + N)
{6,7,8,11}
9 is 0-creative
11 {0,6,7,8,9} ∪(12 + N)
{6,7,8,9}
11 is 0-creative
Remark 2. Not every submonoid is good. For instance, the submonoid X =
{0,3,5,6} ∪(8 + N) in Example 1 has two (≥2)-creative generators. Indeed,
G(X) = {3,5} and G(X \ {3}) = {5,6,8,9} and G(X \ {5}) = {3,8,10} hence 3
is 3-creative and 5 is 2-creative.
Lemma 5. 1. The following predicate is Σ2 :
K is a (≥2)-creative maximal submonoid of the submonoid M
2. The class of good submonoids is Π2.
Proof. 1. Using condition (2) of Proposition 15, let A(M,K) be the formula
K ◁M ∧∃L1,L2

L1 ̸= L2 ∧L1 ◁K ∧L2 ◁K
∧∀L (L is a submonoid ⇒

L = K ⇔(L1 ⊊L ⊊M)) ∧(L = K ⇔L2 ⊊L ⊊M)
		
Since the class of submonoids is Σ1 and the predicate ◁is Σ1 ∧Π1 and inclusion of
submonoids is Σ0 (cf. Propositions 13, 10, 14), the above formula A(M,K) is Σ2.

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
53
2. By deﬁnition M is good if and only if M is a submonoid and there exists a unique
K such that A(M,K). Using Lemma 3, we see that it sufﬁces to say that there exists
at most one K such that A(M,K), i.e.
M is a submonoid ∧∀K′∀K′′ ((A(M,K′)∧A(M,K′′)) ⇒K′ = K′′)
Since formula A is Σ2, this formula is Π2.
Lemma 6. The following predicate is Σ2 ∧Π2
{(L,M) | M is a good submonoid and L = ∂M}
Proof. By Lemma 3, when M is good, ∂M is the unique K such that A(M,K). Thus,
the formula M is good ∧A(M,L) deﬁnes the considered predicate. Since A(M,L)
is Σ2 (cf. the proof of Lemma 5), this formula is Σ2 ∧Π2.
We can now get a useful extension of Lemma 1.
Lemma 7. Let L,M be submonoids and m be the minimum nonzero element of M.
Then m ∈L if and only if L+ ∂M = L+ M.
Proof. Trivially, if m ∈L then L + ∂M = L + M. Suppose now L + ∂M = L + M.
Since m ∈L + M we have m ∈L + ∂M hence m = x + y with x ∈L and y ∈∂M.
Since all nonzero elements of ∂M are strictly greater than m we have y = 0 hence
m = x ∈L.
The following proves the deﬁnability of the class of special submonoids Sn.
Theorem 3. The class Special = {Sn | n ≥1} is Π3.
Proof. Consider the following formula Special(X) which (by Lemma 6) is Π4
and (by Lemma 7 and Proposition 13) expresses that X is a submonoid and, for any
good M with m as minimum nonzero element, if m ∈X then M ⊆X :
X is a submonoid and ∀M ∀L

M is a good submonoid and L = ∂M and X + L = X + M
⇒X + M = X
	
The submonoids Sn clearly satisfy this property. Conversely, if X satisﬁes this prop-
erty and n is the minimum nonzero element of X then, applying the property with
the good submonoid M = Sn, we get Sn ⊆X hence Sn = X.
Proposition 16. The following predicates are Π3 :
Succ1(X,Y) ≡(X,Y) ∈{(Sn,Sn+1) | n ≥1}
Succk(X,Y) ≡(X,Y) ∈{(Sn,Sn+k) | n ≥1}
Succ∗(X,Y) ≡(X,Y) ∈{(Sn,Sn+k) | n,k ≥1}
For k = 1 we simply write Succ in place of Succ1.

54
C. Choffrut and S. Grigorieff
Proof. Observe that
Succ1(X,Y) ⇐⇒X is special and Y = ∂X
Succk(X0,Y) ⇐⇒Special(X0)∧∀X1...∀Xk
k−1

0
Xi is good and Xi+1 = ∂Xi

⇒Y = Xk

Succ∗(X,Y) ⇐⇒X,Y are special and Y ⊆X
then apply Theorem 3, Lemma 6 and Proposition 13.
2.3.3
Addition and Multiplication on Special Submonoids
Here we show that the set of submonoids of the form Sn, with n ≥1, can be equipped
with two deﬁnable operations ⊕and ⊗which make it isomorphic to ⟨N \ {0};
+,×,=⟩.
2.3.3.1
Insight into the Proof
This paragraph is meant to give some intuition behind the formal proofs of the next
two ones. The idea is to deﬁne two operations on the family of special submonoids,
namely an addition (Sn,Sp) →Sn+p and a multiplication (Sn,Sp) →Sn×p.
The addition is deﬁned via the ﬁnite initial segments by observing that {0,...,n+
p} = {0,...,n}+{0,..., p} holds and by using the correspondence Sn →{0,...,n}
(which is deﬁnable, cf. Proposition 20).
Based on a number theoretic result which we recall below, multiplication can be
expressed by using addition and divisibility. Divisibility is deﬁned via the sets of
the form nN by observing that n divides p if and only if pN ⊆nN and by using the
correspondence Sn →nN (which is deﬁnable, cf. Proposition 22).
Lemma 8. Multiplication on N is deﬁnable from addition and divisibility. More pre-
cisely, it is Δ1 relative to addition and some predicates which are themselves Π1
relative to divisibility.
Proof. Schnirelman’s famous result (1931) insures the existence of a constant K
such that every integer ≥2 is the sum of at most K primes. Olivier Ramar´e, [8],
showed that K ≤7. If x = ∑a
i=1 pi and y = ∑b
j=1q j with a,b ≤7 and the numbers
pi,q j are primes then x×y = ∑a
i=1 ∑b
j=1 pi ×q j. Now, the product of two primes p,q
(distinct or not) is the unique number with p,q as sole proper divisors. Let s|t mean
that s is a divisor of t, let P(x) mean that x is prime and let A(x, p,q) mean that p,q
are prime and x = p × q. Then

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
55
P(p) ≡p ̸= 1 ∧∀s (s| p ⇐⇒s = 1 ∨s = p)
A(x, p,q) ≡P(p)∧P(q)∧x ̸= p,q
∧∀s (s|x ⇐⇒s = 1 ∨s = x∨s = p ∨s = q)
are Π1 relative to divisibility. Also, the predicate z = x × y is expressed as the con-
junction of the formulas (x = 0 ∨y = 0) ⇒z = 0, x = 1 ⇒z = y and y = 1 ⇒z = x
and any one of the following formulas:
E(x,y,z) ≡x,y ≥2 ⇒
a,b∈{1,...,7} ∃(xi,yj,zi,j)1≤j≤b
1≤i≤a

ϕ ∧z = ∑i,j zi,j
	
F(x,y,z) ≡x,y ≥2 ⇒
a,b∈{1,...,7} ∀(xi,yj,zi,j)1≤j≤b
1≤i≤a

ϕ ⇒z = ∑i,j zi,j
	
where ϕ is x = ∑i xi ∧y = ∑j yi ∧
i,j P(xi)∧P(yj)∧A(zi,j,xi,yj).
2.3.3.2
Addition on Special Submonoids
Deﬁnition 7. We denote by Initial the class of all initial segments {0,...,n} for
some n ∈N.
Proposition 17. The class Initial is Π4
Proof. Observe that X ∈Initial if and only if 0 ∈X and X ̸= N and, for all x ≥2,
if x ∈X then x−1 ∈X. This is expressible as follows:
0 ∈X ∧X ̸= N ∧∀Z,T,U

(Succ(Z,T)∧Succ(T,U)∧X + T = X +U) ⇒X + Z = X + T
	
using the predicate Succ deﬁned in Proposition 16 and Lemma 1.
As explained in paragraph 2.3.3.1 we view an integer as the maximal element
of an initial segment which allows us to indirectly express that it belongs to some
subset containing 0.
Proposition 18. The following two predicates are Π4
X ∈Initial ∧0 ∈Y ∧maxX ∈Y , X ∈Initial ∧0 ∈Y ∧maxX /∈Y
Proof. Observe that the maximum element of a ﬁnite initial segment X non reduced
to {0} is the integer n such that n ∈X and n + 1 /∈X. Thus, maxX ∈Y is expressed
by the formula
Initial(X) ∧0 ∈Y ∧

X ̸= {0} ⇒∀Z ∀T ∀U

Succ(Z,T)∧Succ(T,U)∧(X + Z = X + T)∧(X + T ̸= X +U)
⇒Y + Z = Y + T
	

56
C. Choffrut and S. Grigorieff
Idem for the second predicate with maxX /∈Y : just replace the last equalityY +Z =
Y +T by an inequality. The stated complexity comes from Propositions 17, 16 and 9.
Proposition 19. The following predicate is Π4
X,Y,Z ∈Initial ∧maxX + maxY = maxZ
Proof. Observe that equality maxX + maxY = maxZ is equivalent to X +Y = Z
when X,Y,Z are ﬁnite initial segments.
Proposition 20. The following predicate is Π4 :
X ∈Initial ∧X ̸= {0} ∧Y = SmaxX
Proof. Observe that maxX is the largest n such that maxX ∈Sn. Thus, the predicate
can be expressed as follows:
X ∈Initial ∧X ̸= {0} ∧maxX ∈Y ∧Special(Y)
∧∀Z (Succ(Y,Z) ⇒maxX /∈Z)
using Proposition 18 and Lemma 1.
Theorem 4. The relation {(Sn,Sp,Sn+p) | n, p ≥1} is Δ5.
We write T ⊕U = V if (T,U,V) is in this relation.
Proof. Using Propositions 20 and 19, T ⊕U = V holds if and only if it any one of
the following formulas holds
ϕ ∧∃I ∃J ∃K (ψ ∧I + J = K)
,
ϕ ∧∀I ∀J ∀K (ψ ⇒I + J = K)
where
⎧
⎨
⎩
ϕ ≡Special(T) ∧Special(U) ∧Special(V)
ψ ≡I,J,K ∈Initial ∧I,J,K ̸= {0}
∧T = SmaxI ∧U = SmaxJ ∧V = SmaxK
.
2.3.3.3
Multiplication on Special Submonoids
We introduce two useful predicates.
Proposition 21. The predicate Periodic = {nN | n ≥1} is Π2.
Proof. By Proposition 7, the sets nN, n ≥1, are the nonzero submonoids with no
minimal supermonoid:
Periodic(X) ⇐⇒(X ̸= {0} is a submonoid and ∀Y ¬(X ◁Y))
Conclude with Proposition 14.

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
57
Proposition 22. 1. The predicate B = {(Sn,nN) | n ≥1} is Π3.
2. The relation {(Sn,Sp) | n ≥1 and n divides p} is Δ4.
Proof. 1. Recall that Periodic(X) is the Π2 predicate of Proposition 21. Observ-
ing that nN is the unique submonoid which is included in Sn and not in ∂(Sn) = Sn+1,
the predicate (X,Y) ∈B is expressible as
Special(X) ∧Periodic(Y) ∧Y ⊆X ∧∀Z (Z = ∂(X) ⇒Y ̸⊆Z)
Theorem 3, Proposition 13 and Lemma 6 give the complexity.
2. Recall that n divides p if and only if pN ⊆nN. Thus, the formulas
 ∃Y ∃Y ′ (B(X,Y) ∧B(X′,Y ′) ∧Y ′ ⊆Y)
∀Y ∀Y ′ ((B(X,Y) ∧B(X′,Y ′)) ⇒Y ′ ⊆Y)
(which are Σ4 and Π4) deﬁne the divisibility predicate on the submonoids Sn.
Theorem 5. The relation {(Sn,Sp,Sn×p) | n, p ≥1} is Δ5. We write T ⊗U = V if
(T,U,V) is in this relation.
Proof. Do the following in the formulas E(x,y,z) and F(x,y,z) given in the proof
of Lemma 8 (and involving the predicates P and A):
- replace the variables x,y,z,xi,yj,zi,j by X,Y,Z,Xi,Yj,Zi,j,
- replace addition by the Δ5 predicate ⊕(cf. Theorem 4),
- using claim 2 of Proposition 22, replace the predicates P and A which are Π1
relative to divisibility, by Π4 predicates in X,Y,Z,Xi,Yj,Zi,j.
We now extend the two elementary operations of addition and multiplication to
an arbitrary polynomial.
Corollary 1. Let T(x1,...,xk) be a polynomial with non zero coefﬁcients in N and
variables in {x1,...,xn} with n ≥1. The following relation is Δ5 :
{(Sn1,...,Snk,Sp) | n1,...,nk ≥1 and p = T(n1,...,nk)}
Proof. There are polynomials T0,...,Ts such that T = Ts and, for 0 ≤i ≤s, Ti is the
constant 1 or a variable or Tj +Tℓor Tj ×Tℓwith j,ℓ< i. Let I be the set of numbers
i such that Ti = 1, let J be the set of pairs (i,m) such that Ti = xm, A (resp. M) be
the set of triples (i, j,ℓ) such that Ti = Tj + Tℓ(resp. Ti = Tj × Tℓ). The relation is
expressed by either of the following formulas with free variables X1,...,Xk,X :
∃Z1 ...∃Zs (ϕ ∧X = Zs)
,
∀Z1 ...∀Zs (ϕ ⇒X = Zs)
where ϕ is 
i∈I Zi = S1 ∧
(i,m)∈J Zi = Xm
∧
(i,j,ℓ)∈A Zi = Zj ⊕Zℓ∧
(i,j,ℓ)∈M Zi = Zj ⊗Zℓ
By Theorems 2, 4, 5, these formulas are respectively Σ5 and Π5.

58
C. Choffrut and S. Grigorieff
2.4
Complexity of the Theory
2.4.1
Emulating Second-Order Arithmetic
Here, we show that we can interpret the second-order theory of arithmetic in the
theory of ⟨P(N);=,+⟩.
Theorem 6. 1. To each second-order arithmetical formula ϕ one can computably
associate a formula Trad(ϕ) so that if ϕ has m free ﬁrst-order variables and n
free second-order variables then Trad(ϕ) has m + n free variables and, for all
a1,...,am ∈N and A1,...,An ⊆N,
⟨N,P(N);=,∈,1,+,×⟩|= ϕ(a1,...,am,A1,...,An) ⇐⇒
⟨P(N);=,+⟩|= Trad(ϕ)(S1+a1,...,S1+am,{0} ∪(1 + A1),...,{0} ∪(1 + A1))
2. If ϕ is quantiﬁer-free then Trad(ϕ) can be taken either Σ5 or Π5. If ϕ is in
prenex form with a nonempty quantiﬁer preﬁx of the form Q1ξ 1 ...Qkξ k where the
variables ξ i are ﬁrst or second order variables and there are ℓalternating blocks of
quantiﬁers ∃,∀in Q1 ...Qk then Trad(ϕ) can be taken Σℓ+4 if Q1 = ∃and Πℓ+4 if
Q1 = ∀.
Proof. 1. The transformation Trad is deﬁned as the composition Ω ◦Θ of two
reductions. The ﬁrst reduction Θ allows to go from the second-order arithmetical
structure of N to that of N\{0}. The second reduction Ω allows to go to the structure
⟨P(N);+,=⟩.
The reduction Θ maps a second-order arithmetical formula ϕ to another such
formula Θ(ϕ) with the same free variables so that, for all a1,...,am ∈N and
A1,...,An ⊆N,
⟨N,P(N);=,∈,1,+,×⟩|= ϕ(a1,...,am,A1,...,An) ⇐⇒
⟨N\{0},P(N\{0});=,∈,1,+,×⟩|=Θ(ϕ)(1+a1,...,1+am,1+A1,...,1+An)
Thus, in Θ(ϕ), integer quantiﬁcations are over N \ {0} and set quantiﬁcations are
over P(N\ {0}). This is simply done by replacing in ϕ any equation over integers
Q(x1,...,xk) = R(x1,...,xk) by the equation obtained from Q(x1 −1,...,xk −1) =
R(x1 −1,...,xk −1) by developing and moving any monomial with negative coef-
ﬁcient from one side to the other side of the equation. For instance, Θ(x+ y = z) is
obtained by the above process from (x−1)+ (y−1) = z−1 hence Θ(x+ y = z) is
x+ y = z+ 1; similarly Θ(x× y = z) is obtained from (x−1)× (y−1) = z−1 and
is therefore x×y+2 = x+y+z. As for subformulas x ∈X, they are left unchanged.
We now deﬁne Ω which maps a second-orderarithmetical formula ψ to a formula
Ω(ψ) with the same number of free variables so that, for all b1,...,bm ∈N \ {0}
and B1,...,Bn ⊆N\ {0},

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
59
⟨N\ {0},P(N\ {0});=,∈,1,+,×⟩|= ψ(b1,...,bm,B1,...,Bn)
⇐⇒⟨P(N);=,+,⟩|= Ω(ψ)(Sb1,...,Sbm,{0} ∪B1,...,{0} ∪Bn)
First, we distinguish two disjoint inﬁnite families of set variables (Ui)i∈N and (Vi)i∈N.
The variables Ui in Ω(ψ) are to vary over the class of special submonoids (i.e. the
sets Sn), they correspond to ﬁrst-order variables in ψ: if xi takes value n ∈N \ {0}
then Ui is to take value Sn ∈P(N). The variables Vi in Ω(ψ) correspond to the
second-order variables in ψ : if Xi takes value B ∈P(N \ {0}) then Vi is to take
value {0} ∪B ∈P(N).
In view of Claim 2, we inductively deﬁne two variants of Ω, namely Ω∃and Ω ∀.
(1) If ψ is an atomic formula Q(x1,...,xk) = R(x1,...,xk) then Ω ∃(ψ) and Ω ∀(ψ)
are the Σ5 and Π5 formulas
Ω ∃(ψ) ≡∃U

A(U1,...,Uk,U) ∧B(U1,...,Uk,U)
	
Ω ∀(ψ) ≡Special(Uk) ∧...∧Special(Uk) ∧∀U ∀U′
((A(U1,...,Uk,U) ∧B(U1,...,Uk,U′)) ⇒U = U′)
where A,B are Σ6 formulas associated to Q and R by Corollary 1 (thus, the i-
th ﬁrst-order variable xi is replaced by the variable Ui). Note: the subformulas
Special(Ui) are omittted in Ω ∃(ψ) since they are implied by A(U1,...,Uk,U).
(2) If ψ is xi ∈Xm then, relying on Lemma 1, Ω ∃(ψ) and Ω ∀(ψ) are the Σ4 and
Π4 formulas (cf. Proposition 16)
Ω ∃(ψ) ≡0 ∈Vm ∧∃U (Succ(Ui,U) ∧Vm +Ui = Vm +U)
Ω ∀(ψ) ≡Special(Ui) ∧0 ∈Vm
∧∀U (Succ(Ui,U) ⇒Vm +Ui = Vm +U)
(3) Ω ∃and Ω ∀commute with conjunction and disjunction.
(4) Ω ∃(¬ϕ) = ¬Ω ∀(ϕ) and Ω ∀(¬ϕ) = ¬Ω ∃(ϕ).
(5) Ω ∃(∃xi ψ) = Ω ∀(∃xi ψ) = ∃Ui (Special(Ui)∧Ω ∃(ψ))
Ω ∃(∀xi ψ) = Ω ∀(∀xi ψ) = ∀Ui (Special(Ui) ⇒Ω ∀(ψ))
(6) Ω ∃(∃Xm ψ) = Ω ∀(∃Xm ψ) = ∃Vm (0 ∈Vm ∧Ω ∃(ψ))
Ω ∃(∀Xm ψ) = Ω ∀(∀Xm ψ) = ∀Vm (0 ∈Vm ⇒Ω ∀(ψ))
Letting Ω be either Ω ∃or Ω ∀, Corollary 1 and Lemma 1, show that the above
clauses insure the wanted property of Ω hence also those of Trad : ϕ →Ω(Θ(ϕ)).
2. The assertion about quantiﬁer-free formulas ϕ is clear from clauses (1) and (2).
An easy induction on the complexity of ϕ shows that if ϕ has ℓalternating blocks
of quantiﬁers and the last one is a Q-block then Trad(Ω Q(Θ(ϕ))) is Σℓ+4 if the ﬁrst
block is a ∃-block and is Πℓ+4 if the ﬁrst block is a ∀-block.

60
C. Choffrut and S. Grigorieff
2.4.2
The Theory of Addition on Sets Is Undecidable
The complexity results concerning the theory ⟨P(N);+,=⟩are direct consequences
of the results in the previous sections.
Theorem 7. The class of Σ5 sentences true in ⟨P(N);+,=⟩is undecidable.
Proof. Use the undecidability of the Diophantine theory of ⟨N;=,1,+,×⟩(Matiya-
sevich’s celebrated result) and Theorem 6.
The theory of addition on sets is, in fact, highly undecidable.
Theorem 8. The class T of sentences true in ⟨P(N);=,+⟩is recursively isomor-
phic to the second order theory A of ⟨N;=,+,×⟩, i.e. there exists a computable
bijection θ between the set of ﬁrst-order formulas in the language {=,+} and the
set of second-order formulas in the language {=,+,×} such that T = θ −1(A).
Remark 3. The class of sentences with quantiﬁcations over N only which are true
in ⟨N;=,1,+,×⟩(i.e. the ﬁrst order theory of arithmetic) is Δ1
1 and not Σ0
n for any
n ∈N. As for the class of sentences with quantiﬁcations over N and over P(N) which
are true in ⟨N,P(N);=,∈,1,+,×⟩(i.e. the second order theory of arithmetic), it is
Δ2
1 and not Σ1
n for any n ∈N. Thus, the Turing degree of the second order theory of
arithmetic is an order of magnitude higher than that of the ﬁrst order theory.
Proof (Proof of Theorem 8). Recall Myhill’s isomorphism theorem which is the
computable analog of Cantor-Bernstein’s theorem in set theory (cf. [10] Theorem VI
page 85, or [7] Theorem III.7.13 page 325): if X,Y ⊆N and there exists computable
injective maps ϕ : N →N and ψ : N →N such that X = ϕ−1(Y) and Y = ψ−1(X)
then there exists a computable bijective map θ : N →N such that X = θ −1(Y). Thus,
to prove the theorem it sufﬁces to get injective computable reductions of T to A and
of A to T .
Recursive reduction of T to A. To each sentence about ⟨P(N);=,+⟩associate
the second order arithmetical formula obtained by replacing any subformula X +
Y = Z by
∀r (r ∈Z ⇐⇒∃p,q (n = p + q ∧p ∈X ∧q ∈Y) .
Recursive reduction of A to T . Use Theorem 6.
2.5
Non Deﬁnable Predicates
2.5.1
What Is So Special about the Predicate 0 ∈X?
As implicitly used in numerous instances in the previous sections, the existence of 0
in a subset seems to be the crux for proving remarkable properties such as those in
paragraph 2.3.1. If the logic could allow us to add 0 to an arbitrary subset then we

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
61
could extend these properties to all subsets. However this is not possible. We show
that the following predicate is not deﬁnable:
Y = X ∪{0}
(2.7)
The proof uses two ingredients. The ﬁrst one, cf. Lemma 9 below, insures that
equations and inequations between set variables can be split into conditions on el-
ements of N and conditions on subsets which are either empty or contain 0. This
transformation is lifted to formulas in Lemma 10 below. The second ingredient, cf.
Lemma 11 below, is a general result about formulas consisting of combinations of
claims on disjoint sets of variables.
Notation 9. Let P0(N) be the class of sets which contain 0. Let P0,/0(N) = P0(N)∪
{/0} be the class of sets which contain 0 or are empty. Let
(1) E be the subset {/0} of P0,/0
(2) +N and =N be addition and equality on N,
(3) +P0,/0(N) and =P0,/0(N) be addition and equality of sets in P0,/0(N).
We consider the following two sort structure:
M = ⟨N,P0,/0(N);+N,+P0,/0,E,=N,=P0,/0⟩
Lemma 9. Let I,J be disjoint subsets such that I ∪J = {1,...,n}. Then, for all
integers a1,...,an ∈N and sets A1,...,An ∈P0,/0,
⟨P(N);+,=⟩|= ∑
i∈I
ai + Ai = ∑
j∈J
a j + A j ⇐⇒M |= ψ(a1,...,an,A1,...,An)
where ψ(x1,...,xn,X1,...,Xn) is a Boolean combination of formulas E(Xℓ), for ℓ=
1,...,n, and equalities ∑i∈I xi = ∑j∈J xj and ∑i∈I Xi = ∑j∈J Xj.
Proof. Consider the class S of solutions of equation ∑i∈I Xi = ∑j∈J Xj in P(N).
Then S = Z ∪(S \ Z) where
(i) Z is the class of n-tuples of sets satisfying Xi = Xj = /0 for some i ∈I and
j ∈J.
(ii) S \ Z is the class of n-tuples in S consisting of nonempty sets.
Since a + /0 = /0 for all a ∈N, we have
Z = {(a1 + A1,...,an + An) | a1,...,an ∈N, A1,...,An ∈P0,/0(N),
(2.8)
Ai = A j = /0 for some i ∈I and j ∈J} .
Let S0 = S ∩

P0(N)
	n and R = {(a1,...,an) ∈Nn | ∑i∈I ai = ∑j∈J a j}. Observe
that if B1,...,Bn ∈P(N) are all nonempty and ai = minBi, Ai = Bi −ai (i.e. Bi =
ai + Ai with Ai ∈P0(N)) for i = 1,...,n, then equality ∑i∈I Bi = ∑j∈J B j holds if
and only if both equalities ∑i∈I bi = ∑j∈J b j and ∑i∈I Ai = ∑j∈J A j hold. Thus,
S \ Z = {(a1 + A1,...,an + An) | −→a ∈R, −→
A ∈S0} .
(2.9)

62
C. Choffrut and S. Grigorieff
Consider the following formulas (recall E(X) expresses X = /0):
ψZ(x1,...,xn,X1,...,Xn) ≡

i∈I,j∈J
E(Xi)∧E(Xj)
ψS\Z(x1,...,xn,X1,...,Xn) ≡¬E(X1) ∧...∧¬E(Xn)
∧∑
i∈I
xi = ∑
j∈J
xj ∧∑
i∈I
Xi = ∑
j∈J
Xj
Equalities (2.8) and (2.9) show that, for a1,...,an ∈N and A1,...,An ∈P0,/0(N),
⟨P(N);+,=⟩|= ∑
i∈I
ai + Ai = ∑
j∈J
a j + A j
⇐⇒M |= ψZ(a1,...,an,A1,...,An) ∨ψS\Z(a1,...,an,A1,...,An)
We now lift Lemma 9 to formulas with quantiﬁcations over P(N).
Lemma 10. For any Boolean combination F(−→
Y ,−→
X ) of equalities between sums of
the set variables X1,...,Xn and Y1,...,Yk, there exists a Boolean combination T (F)
of
(1) equalities between sums of the set variables U1,...,Un,V1,...,Vk,
(2) equalities between sums of the integer variables
(3) formulas E(X1),...,E(Xn),E(Y1),...,E(Yk),
such that, for all integers a1,...,an ∈N, all sets A1,...,An in P0,/0(N) (i.e. each Ai
is either empty or contains 0), any sequence Q1,...,Qk of quantiﬁers ∃or ∀,
⟨P(N);+,=⟩|= Q1Y1 ···QkYk F(−→
Y ,a1 + A1,...,an + An)
⇐⇒M |= Q1v1 Q1V1 ···Qkvk QkVk T (F)(−→v ,−→a ,−→
V ,−→
A )
(2.10)
Proof. If E is an equation then Lemma 9 gives a formula ψE which is a convenient
T (E). For a Boolean combination F of equations E1,...,Ep, let T(F) be the same
Boolean combination with ψE1,...,ψEp.
Having deﬁned T (F), we now prove the Lemma by induction on k. The case k =
0 (i.e. no preﬁx of quantiﬁcations) is clear from Lemma 9. Suppose (2.10) holds
for the quantiﬁcation preﬁx Q1 ...Qk and any Boolean combination F. Then, for
a1,...,an ∈N, A1,...,An in P0,/0(N) and Q ∈{∃,∀},
⟨P(N);+,=⟩|= QZ Q1Y1 ···QkYk F(−→
Y ,Z,a1 + A1,...,an + An)
⇐⇒Qb ∈N QB ∈P0,/0(N)
⟨P(N);+,=⟩|= −→
QY F(−→
Y ,b + B,a1 + A1,...,an + An)
(†)
⇐⇒Qb ∈N QB ∈P0,/0(N)
M |= Q1v1 Q1V1···Qkvk QkVk T (F)(−→v ,b,−→a ,−→
V ,B,−→A )
⇐⇒M |= Qw QW Q1v1 Q1V1···Qkvk QkVk
T (F)(−→v ,w,−→a ,−→
V ,W,−→A )
where line (†) is obtained using the induction hypothesis.

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
63
Lemma 11 (Splitting
lemma).
Consider
two
disjoint
sets
of
variables
z1,...,zk,Z1,...,Zk and t1,...,tn,T1,...,Tn and let Φ(−→t ,−→
T ) be a formula
with 2n free variables of the form
Φ(−→t ,−→
T ) ≡QkzkQkZk ··· Q1z1Q1Z1 B(−→z ,−→Z ,−→t ,−→
T )
where the Qi’s are quantiﬁers in {∃,∀} and where B(−→z ,−→
Z ,−→t ,−→
T ) is a Boolean
combination of atomic formulas depending on the variables −→z ,−→t only and atomic
formulas depending on the variables −→
Z ,−→
T only. Then there exists r ≥1 and ﬁnitely
many formulas φℓ,ψℓ, for ℓ= 1,...,r each having n variables, such that Φ(−→t ,−→
T )
is logically equivalent to

ℓ=1,...,r
φ ℓ(−→t )∧ψℓ(−→
T )
Proof. We argue by induction on k ∈N. The initial case k = 0 is an instance of
the classical disjunctive normal form. We now show the induction step. Suppose
the Lemma is true for k −1 with k ≥1. Then there exists r ≥1 and φk−1,ℓ(zk,−→t ),
ψk−1,ℓ(Zk,−→
T ), for ℓ= 1,...,r, such that
Qk−1zk−1Qk−1Zk−1 ···Q1z1Q1Z1 B(z1,...,zk−1,zkZ1,...,Zk−1,Zk,−→t ,−→
T )
⇐⇒

ℓ=1,...,r
φ k−1,ℓ(zk,−→t )∧ψk−1,ℓ(Zk,−→
T )
Then, in case Qk is ∃,
∃zk∃Zk Qk−1zk−1Qk−1Zk−1 ···Q1z1Q1Z1 B(−→z ,−→
Z ,−→t ,−→
T )
⇐⇒∃zk∃Zk

ℓ=1,...,r φk−1,ℓ(zk,−→t )∧ψk−1,ℓ(Zk,−→
T )
⇐⇒
ℓ=1,...,r

∃zkφk−1,ℓ(zk,−→t )
	
∧

∃Zkψk−1,ℓ(Zk,−→
T )
	
⇐⇒
ℓ=1,...,r φ k,ℓ(−→t )∧ψk,ℓ(−→
T )
where φ k,ℓ(−→t ) is ∃zkφk−1,ℓ(−→t ) and the same with ψk,ℓ(−→
T ). Finally, the case Qk is
∀is treated similarly by ﬁrst converting from disjunctive to conjunctive form and,
after distributing the ∀quantiﬁers, converting back from conjunctive to disjunctive
form.
We ﬁnally come to the wanted nondeﬁnability result.
Theorem 10. The predicate Y = {0} ∪X is not deﬁnable in ⟨P(N);+,=⟩.
Proof. By way of contradiction, assume that the predicate Y = {0} ∪X can be de-
ﬁned in ⟨P(N);+,=⟩. There exists a Boolean combination F(−→Z ,X,Y) of equalities
of sums of the sets Zi and X,Y such that
⟨P(N);+,=⟩|= Y = {0} ∪X ⇐⇒Q1Z1 ···QkZk F(−→
Z ,X,Y)
(2.11)

64
C. Choffrut and S. Grigorieff
By Lemma 10, for all a,b ∈N and sets A,B ∈P0,/0(N),
⟨P(N);+,=⟩|= A = N ∧b + B = {0} ∪(a + A)
⇐⇒M |= Q1v1 Q1V1 ···Qkvk QkVk T (F)(−→v ,−→
V ,a,b,A,B)
(2.12)
where T (F)(−→v ,−→
V ,u,v,U,V) is a Boolean combination of formulas with free vari-
ables among u,v and formulas with free variables among U,V. By Lemma 11, we
get formulas ϕℓ(u,v), ψℓ(U,V), ℓ= 1,...,L, such that
⟨P(N);+,=⟩|= b + B = {0} ∪(a + A)
⇐⇒M |=

ℓ=1,...,L
ϕℓ(a,b)∧ψℓ(A,B)
(2.13)
Now, for each n ≥1, we have 0 + Sn = {0} ∪(n + N) hence there exists ℓsuch that
ϕℓ(n,0) and ψℓ(N,Sn) are true. Since there are ﬁnitely many such indices ℓ, there
exists two values of n, say p,q ≥1, p ̸= q, with the same associated ℓ. In particu-
lar, ϕℓ(p,0)∧ψℓ(N,Sq) is true yielding equality p + N = 0 + Sq, contradicting the
condition p ̸= q.
2.5.2
Other Nondeﬁnable Predicates
Theorem 10 implies the nondeﬁnability of many other predicates. We select some
of them in this section. In particular, we compare four ways to code integers by sets
in deﬁnable classes:
Final
= {n + N | n ≥1}
(see Proposition 9)
Single
= {{n} | n ≥1}
(see Proposition 12)
Special
= {{0} ∪n + N | n ≥1} (see Theorem 3)
Periodic = {nN | n ≥1}
(see Proposition 21)
Deﬁnition 8. 1. Given predicates A1,...,Ak and B over P(N), we say that B is de-
ﬁnable from A1,...,Ak in ⟨P(N);+,=⟩if B is ﬁrst-order deﬁnable in the structure
⟨P(N);+,=,A1,...,Ak⟩. We then write (A1,...,Ak) ; B.
2. A and B are deﬁnable from each other in ⟨P(N);+,=⟩if A ; B and B ; A.
Theorem 11. No predicate in Table 1 is deﬁnable in ⟨P(N);+,=⟩. Moreover, any
two of them are deﬁnable from each other in ⟨P(N);+,=⟩.
Open problem 12. Is there a non deﬁnable predicate which is not deﬁnable from
each other with the predicates in Table 1?
Proof (Proof of Theorem 11). Together with the predicates Final, Single,
Special and Periodic recalled supra, we also freely use some deﬁnable predi-
cates such as minX ≤minY (cf. § 2.2.3.4) and

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
65
Table 2.1 Some predicates not deﬁnable in ⟨P(N);+,=⟩
E(X,Y)
Inclusion
X ⊆Y
F(X,Y)
Adjoin 0
Y = X ∪{0}
F1(X,Y,Z) Union
X ∪Y = Z
F2(X,Y,Z) Intersection X ∩Y = Z
F3(X,Y)
Complement X = N\Y
F4(X,Y)
Star
Y = X∗
G(X,Y)
Coding
(X,Y) ∈{(n+N,Sn) | n ≥1}
G1(X,Y)
interchange
(X,Y) ∈{({n},Sn) | n ≥1}
G2(X,Y)
(X,Y) ∈{(n+N,nN) | n ≥1}
G3(X,Y)
(X,Y) ∈{({n},nN) | n ≥1}
H(X,Y)
Membership (X,Y) ∈{(n+N,B) | n ∈B}
H1(X,Y)
(X,Y) ∈{({n},B) | n ∈B}
H2(X,Y)
(X,Y) ∈{(nN,B) | n ∈B}
H3(X,Y)
(X,Y) ∈{(Sn,B) | n ∈B}
H4(X,Y)
(X,Y) ∈{(A,B) | minA ∈B}
H5(X,Y)
(X,Y) ∈{(A,B) | the 2d elem. of A is in B}
J(X)
Semigroup
X is a semigroup (i.e. X +X ⊆X)
J1(X)
X ∈{an+nN | n ≥1,a ∈N}
J2(X)
X ∈{n+nN | n ≥1}
θ(X,Y) = {(A,Sn) | (minA)+ n ∈A}
(cf. Lemma 1)
σ(X,Y) ≡X is a submonoid containing Y ∧0 ∈Y
(cf. Propositions 9, 10, 13)
We also freely use deﬁnable constants and functions (cf. §2.2.3.1): /0, {0}, N, X →
{minX} (deﬁned for X ̸= /0) and
Succ = Sn →Sn+1 for n ≥1
(cf. Proposition 16)
S
= nN →Sn
for n ≥1
(cf. Proposition 22)
π
= Sn →nN
for n ≥1
(cf. Proposition 22)
Smax
= A →SmaxA for initial segments A ̸= {0} (cf. Proposition 20)
The structure of the proof is as follows:
E ; F ;
⎧
⎪
⎪
⎨
⎪
⎪
⎩
F4 ; G ; G2 ; G3 ; G1 ; G
F3 ; G
G1 ; E ; H ; H1 ; G1
F2 ; E
(E,G3) ; H2 ; H3 ; G1
F1 ; E ; J ; J1 ; J2 ; G3
H1 ; H4 ; H5 ; H3
E ; F. Use E to express that Y is the smallest set containing X and {0}.
F ; Fi for i = 1,2,3. For i = 3, we have to relate, for any n, condition n ∈X with
condition n ∈Y. The case n = 0 is treated apart whereas the case n ≥1 can be
treated with θ relatively to X ∪{0} (which is obtained using F). The cases i = 1,2
are similar. Thus,

66
C. Choffrut and S. Grigorieff
F3(X,Y)
⇔(0 ∈X ⇔0 /∈Y)∧∀X0 ∀Y0

F(X,X0)∧F(Y,Y0)
⇒∀T (Special(T) ⇒(θ(X0,T) ⇔¬θ(Y0,T))
	
F1(X,Y,Z) ⇔(0 ∈Z ⇔(0 ∈X ∨0 ∈Y))
∧∀X0 ∀Y0 ∀Z0

F(X,X0)∧F(Y,Y0)
⇒∀T (Special(T) ⇒(θ(Z0,T) ⇔(θ(X0,T)∨θ(Y0,T))
	
F2(X,Y,Z) ⇔(0 ∈Z ⇔(0 ∈X ∧0 ∈Y))
∧∀X0 ∀Y0 ∀Z0

F(X,X0)∧F(Y,Y0)
⇒∀T (Special(T) ⇒(θ(Z0,T) ⇔(θ(X0,T)∧θ(Y0,T))
	
F ; F4. Express that X∗is the smallest submonoid containing X ∪{0}.
F4(X,Y) ⇔∃X0 (F(X,X0)∧σ(Y,X0)∧∀T (σ(T,X0) ⇔σ(T,Y)))
F1 ; E, F2 ; E. E(X,Y) ⇔F1(X,Y,Y) ⇔F2(X,Y,X).
F3 ; G. Observe that if X is a ﬁnal segment then its complement is an initial seg-
ment and apply Proposition 20:
G(X,Y) ⇔Final(X)∧(X = 1 + N ⇒Y = N)
∧(X ̸= 1 + N ⇒∃Z (F3(X,Z)∧Y = Succ(Smax(Z))))
F4 ; G. G(X,Y) ⇔Final(X)∧F4(X,Y).
G ; G2. G2(X,Y) ⇔Periodic(Y)∧G(X,S(Y)).
G2 ; G3. G3(X,Y) ⇔Single(X)∧G2(X + N,Y).
G3 ; G1. G1(X,Y) ⇔Periodic(Y)∧Single(X)∧G3(X,S(Y))
G1 ; G. G(X,Y) ⇔Final(X)∧G1(min(X),Y).
G1 ; E. Observe that X ⊆Y if and only if three conditions hold: 1) minX ≥minY
2) minX ∈Y and 3) for all p,q ≥1, if (minX) + p = (minY) + q then (minX) +
p ∈X implies (minY) + q ∈Y. Now, minX ∈Y if and only if minX = minY or
(minY)+a ∈Y with a = minX −minY ≥1. In the next formula X, Y denote SminX,
SminY and P denotes Sp (p > 0) and Q denotes Sq (with q = p + a and p ≥0). Also
line 3 expresses minX ∈Y (in the sole necessary case where minX > minY) and
line 4 expresses that if p,q ≥1 and minX + p = minY +q holds then minX + p ∈X
implies minY + q ∈Y.
E(X,Y) ⇔minX ≥minY ∧∃X ∃Y ∀P ∀Q

G1({minX}, X)∧G1({minY},Y)∧Special(P)∧Special(Q)
	
⇒

((minX > minY ∧X = Y ⊕Q) ⇒θ(Y,Q)))
∧X ⊕P = Y ⊕Q ⇒(θ(X,P) ⇒θ(Y,Q)
	

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
67
E ; H. H(X,Y) ⇔Final(X)∧E({minX},Y).
H ; H1. H1(X,Y) ⇔Single(X)∧H(X + N,Y).
H1 ; G1. G1(X,Y) ⇔Special(Y)∧H1(X,Y)∧¬H1(X,Succ(Y)).
(E,G3) ; H2. H2(X,Y) ⇔Periodic(X)∧∃Z (G3(Z,X)∧E(Z,Y)).
H2 ; H3. H3(X,Y) ⇔Special(X)∧H2(π(X),Y).
H3 ; G1. G1(X,Y) ⇔Single(X)∧Special(Y)∧H3(Y,X)∧¬H3(Succ(Y),X).
H1 ; H4. H4(X,Y) ⇔H1(X + N,Y).
H4 ; H5. H5(X,Y) ⇔X ̸= /0 ∧¬Single(X)
∧∃Z

Single(Z)∧(minZ > minX)∧H4(Z,X)∧H4(Z,Y)
∧∀T ((Single(T)∧minX < minT < minZ) ⇒¬H4(T,X))
	
.
H5 ; H3. H3(X,Y) ⇔Special(X)∧H5(X,Y).
E ; J. Observe that X is a semigroup if and only if X + X ⊆X.
J ; J1. Let T = {a + nN | a ∈N,n ≥1}. Then T (X) if and only if X = Y + Z
for some Y,Z satisfying Single(Y) and Periodic(Z). Now, J1(X) if and only if
T (X) and X is a semigroup: implication ⇒is obvious, conversely, if X = a+nN is a
semigroup then its ultimate period is n. It divides all elements of X and in particular
a and J1(X) holds.
J1 ; J2. Observe that J2(X) if and only if J1(X) and minX ̸= 0 and {minX} is
the unique singleton set such that X = T + Y for some T,Y such that T ̸= {0},
Single(T) and J1(Y).
J2 ; G3. G3(X,Y) ⇔∃Z (J2(Z)∧X = {minZ} ∧Z = X +Y).
Out of the six pairs of the four codings of N\{0} by Final, Single, Special
and Periodic, Table 2.1 tells that four conversions are incomparable with respect
to ; (cf. predicates G and Gi, i = 1,2,3). In contrast the two remaining ones are
deﬁnable as shown in the next result.
Proposition 23. The following predicates are respectively Π2 and Π3.
(X,Y) ∈{(n + N,{n}) | n ≥1}
,
(X,Y) ∈{(nN,Sn) | n ≥1}
Proof. Observe that (X,Y) ∈{(n + N,{n}) | n ≥1} if and only if 0 /∈Yand Y is a
singleton set and Y + N = X. Expressing the last equality as ∀Z (Z = N ⇒Y + Z =
X) Propositions 9 and 12 give the Π2 complexity.
For the second predicate see Proposition 22.

68
C. Choffrut and S. Grigorieff
2.6
Logical Deﬁnability in ⟨P(N);+,=⟩
2.6.1
Families of Sets All Containing 0
A simple application of Theorem 6 proves the following result.
Theorem 13. Suppose F ⊆P(N) is a class of sets all containing 0. Then F is
deﬁnable in ⟨P(N);+,=⟩if and only if it is deﬁnable in second-order arithmetic.
Proof. Observe that F is deﬁnable in second-order arithmetic if and only if so is
{A | {0} ∪(1 + A) ∈F} and apply Theorem 6.
Corollary 2. Suppose F ⊆P(N) is a class of sets all containing an integer in
{0,...,n}. Then F is deﬁnable in ⟨P(N);+,=⟩if and only if it is deﬁnable in
second-order arithmetic.
Proof. The ⇒implication is trivial. Conversely, suppose F is deﬁnable in second-
order arithmetic. For i = 1,...,n, let Fi = F ∩{X | minX = i} and Gi = {X −minX |
X ∈Fi}. Then the formulas Gi are also deﬁnable in second-order arithmetic. Since
all sets in the formulas Gi contain 0, these formulas Gi are deﬁnable in ⟨P(N);+,=⟩
(use Theorem 13). Then so are the sets {i+X | X ∈Gi} = Fi hence also their union
which is F.
2.6.2
Families of Sets Invariant by Translation
There is yet another application of Theorem 6 which extends the class of subsets
deﬁnable in ⟨P(N);+,=⟩.
Theorem 14. Suppose F ⊆P(N) is a class of subsets such that for all subsets A ⊆N
and all integers a ∈N it holds
A ∈F ⇔A+ a ∈F
Then F is deﬁnable in ⟨P(N);+,=⟩if and only if it is deﬁnable in second-order
arithmetic.
Proof. Let F0 be the subclass of subsets in F containing 0. For all B ∈F the sub-
set B −minB is in F0. Clearly F0 is deﬁnable in second-order arithmetic, thus in
⟨P(N);+,=⟩by a formula φ(X). Then F is deﬁnable in ⟨P(N);+,=⟩by the for-
mula
∃X ∃Y (φ(X)∧Sing(Y)∧Z = Y + X)
Corollary 3. The following classes of subsets are deﬁnable
(i) {A ⊆N | A is ﬁnite},
(ii) {A ⊆N | A is coﬁnite},
(iii) {A ⊆N | A is regular by a ﬁnite automaton}).

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
69
Proof. Assertions (i) and (ii) are clear. Concerning assertion (iii) recall (cf. Propo-
sition 4) that a subset A ⊆N is recognizable by a ﬁnite automaton if and only if it is
a ﬁnite union of subsets of the form a + bN with a,b ∈N. Thus, this class satisﬁes
the conditions of Theorem 14.
2.6.3
Deﬁnable Sets of Integers
Theorem 15. Let A ∈P(N). The following conditions are equivalent:
(1) As a set of integers, A is deﬁnable in second-order arithmetic,
(2) As a class of sets, {A} is deﬁnable in second-order arithmetic,
(3) {A} is deﬁnable in ⟨P(N);+,=⟩.
Proof. (1) ⇔(2) is straightforward. (3) ⇒(2) is obvious.
(2) ⇒(3). Let a = minA. If {A} is deﬁnable in second-order arithmetic then so is
{A −a}. By Theorem 13 {A −a} is deﬁnable in ⟨P(N);+,=⟩(since 0 ∈A −a)
hence so is {A} = {X + a | X ∈{A−a}}.
2.6.4
Deﬁnability with an Extra Predicate
Theorem 16. Let F ⊆P(N) and A be any predicate in Table 1. Then F is de-
ﬁnable in second-order arithmetic if and only if F is deﬁnable in the structure
⟨P(N);+,A,=⟩.
Proof. Implication ⇐is obvious since all predicates in Table 1 are deﬁnable in
second-order arithmetic. Conversely, suppose F is deﬁnable in second-order arith-
metic. Then so are the classes
F+ = F ∩{A | 0 ∈A} , F−= F ∩{A | 0 /∈A} , H = {{0} ∪A | A ∈F−}
By Theorem 13 F+ and H are deﬁnable in ⟨P(N);+,=⟩. Using the predicate
F(X,Y) (which insures Y = {0} ∪X) one can then deﬁne F−from H. Finally,
F = F+ ∪F−is deﬁnable in ⟨P(N);+,F,=⟩. Since all predicates in Table 1 are
deﬁnable from each other, F = F+ ∪F−is also deﬁnable in ⟨P(N);+,A,=⟩for
any A in Table 1.
2.7
Remarkable Deﬁnable Sets and Classes
2.7.1
Operations on Sets with Close Minimum Elements
General set-theoretical operations are not deﬁnable. Here we show sufﬁcient condi-
tions for some of these operations to be deﬁnable.
Proposition 24. The predicate minX = minY ∧φ(X,Y,Z) is Π4 when φ(X,Y,Z)
is either X ∪Y = Z or X ∩Y = Z or X ⊆Y.

70
C. Choffrut and S. Grigorieff
Proof. We argue for union. The minX = minY condition is Σ1 (cf. Proposition 11)
and, by Lemma 1, X ∪Y = Z if and only if, for all n ≥1,
Z + Sn = Z + Sn+1 ⇐⇒((X + Sn = X + Sn+1)∨(Y + Sn = Y + Sn+1))
Theorem 3 and Proposition 16 yield the stated logical complexity.
Proposition 25. 1. The predicate |minX −minY| ≤k ∧φ(X,Y,Z) is Π4 when the
integer k ≥1 is ﬁxed and φ(X,Y,Z) is either X ∪Y = Z or X = N\Y or X ⊆Y.
2. The following predicate is Π4 when the integer k ∈N is ﬁxed:
max(|min(X)−min(Y)|,|minX −minZ|) ≤k ∧X ∩Y = Z
Proof. Point 1. Since the condition |minX −minY| ≤k is a disjunction of condi-
tions minX = minY + ℓwith |ℓ| ≤k, it sufﬁces to prove that the predicate minX =
minY + k ∧φ(X,Y,Z) is Π5. We argue for union. The min(X) = min(Y)+ k con-
dition is Σ2 ∧Π2 (cf. Proposition 11). Assuming min(X) = min(Y) + k, Lemma 1
insures that X ∪Y = Z if and only if
1) minZ = minY,
2) Z + Sn = Z + Sn+1 ⇐⇒Y + Sn = Y + Sn+1 for all n ∈{1,...,k −1},
3) Z + Sk = Z + Sk+1,
4) for all n ≥1,
Z + Sk+n = Z + Sk+n+1
⇐⇒(X + Sn = Z + Sn+1 ∨Y + Sk+n = Y + Sk+n+1).
Theorems 2, 3 and Proposition 16 yield the stated logical complexity. The proof
of Point 2 is similar.
Remark 4. Observe that the intersection is not deﬁnable even when the minima of
the two subsets differ by 1 : we have to also bound the difference between min(X ∩
Y) and minX. For instance, {(n + N,Sn) | n ≥2} which is G(X,Y) of Theorem 11
is expressed as follows
∃Z (Special(Z)∧Y = ∂Z ∧(X = Y ∩(Z + 1)))
Indeed, if Z = Sn then Y and 1+Z have close minimum elements since min(1+Z) =
1 and minY = 0 but min(Y ∩(1 + Z)) = n + 1 is not close to minY.
2.7.2
Fixed Submonoids
Proposition 11 can be generalized as follows.
Lemma 12. Let X be a subset with minimum element equal to m and let a1 < ... <
an be elements of N. The following predicates φm(X) and ψm(X) are Δ2 :
φ m(X) : minX = m and a1,...,an > m belong to X
ψm(X) : minX = m and a1,...,an > m do not belong to X

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
71
Proof. Using Lemma 1, we have:
φ m(X) ≡minX = m∧

1≤i≤n
X + Sai−m = X + Sai−m+1
ψm(X) ≡minX = m∧

1≤i≤n
X + Sai−m ̸= X + Sai−m+1
The Δ2 complexity is a corollary of Theorem 2 and Proposition 8.
Theorem 17. Let M be a submonoid. The predicate X = M is deﬁnable and its
complexity is as follows
1. Case M = {0}. The predicate X = M is Π1.
2. Case M = N. The predicate X = M is Σ1 ∧Π1.
3. Case M = {0} ∪(a + N) = Sa with a ≥2. The predicate X = M is Δ2.
4. For the general case the predicate X = M is Π2.
Proof. Claims 1, 2: cf. Proposition 9. Claim 3: cf. Theorem 2.
4. Let G = {g1,...,gn} be the minimum generating set for M which we assume
ordered. Then M is the smallest submonoid containing G. If φ0(X) is the formula of
Lemma 12 with g1,...,gn in place of a1,...,an the predicate X = M is equivalent to
φ0(X)∧X + X = X ∧∀Y

(φ 0(Y)∧(Y +Y = Y)) ⇒X +Y = Y
	
expressing that X is the smallest submonoid containing G. The Π2 complexity
comes from Lemma 12.
2.7.3
Regular Subsets of N
Here we give a precise estimate of the structural complexity of some subsets and
classes of subsets of particular importance, the deﬁnability of which is a conse-
quence of Theorem 14.
2.7.3.1
Fixed Regular Subsets of N
Theorem 18. If R ⊆N is regular then the predicate X = R is Π4. In case R = /0 or
R = {0} it is Π1. In case R is a singleton different from 0 it is Σ2 ∧Π2.
Proof. By Proposition 4, R = A ∪(B + pN) with a, p ∈N and /0 ̸= A ⊆[0,a[ and
B ⊆[a,a + p[. First, we introduce some formulas. We set m = minA and b = a + p
with the convention b = a when B is empty. The following Σ4 and Π4 predicates tell
which elements in the initial interval [0,b[ belong to the set and which do not.

72
C. Choffrut and S. Grigorieff
F∃(X) ≡m = minX ∧∃Y1,...,Yb−m−1Z1,...,Zb−m−1

i∈A∪B\{m}
(Yi = Si−m ∧Succ(Yi,Zi) ∧X +Yi = X + Zi)
∧

i∈{m+1,...,b−1}\(A∪B)
(Yi = Si−m ∧Succ(Yi,Zi) ∧X +Yi ̸= X + Zi)
F∀(X) ≡m = minX ∧∀Y,Z

i∈A∪B\{m}
((Y = Si−m ∧Succ(Y,Z)) ⇒X +Y = X + Z)
∧

i∈{m+1,...,b−1}\(A∪B)
((Y = Si−m ∧Succ(Y,Z)) ⇒X +Y ̸= X + Z)
If the subset is ﬁnite, it sufﬁces to express the fact that it does not contain any
integer greater than or equal to a. This leads to the Π4 formula
G(X) ≡∀Y,Z,T

(T = Sa−m ∧Succ(Y,Z) ∧Y + T = T)
⇒X +Y ̸= X + Z
	
Thus when R is ﬁnite it is expressed by the predicate F∀(X)∧G(X).
When the subset is inﬁnite, i.e., when B ̸= /0 we must say that the subset of R con-
sisting of all elements greater than or equal to a is periodic of period p, equivalently
for all x ≥a we have x ∈R ⇔x+ p ∈R which is expressed by the Π4 formula
H(X) ≡∀T,Y,Y ′,Z,Z′ :
T = Sa−m ∧Succ(Y,Y ′)∧Succ(Z,Z′)∧Succp(Y,Z)
⇒(X +Y = X +Y′ ⇔X + Z = X + Z′)
Thus, when R is inﬁnite it is expressed by the formula F∀(X)∧H(X).
The remaining cases are a consequence of Proposition 12.
2.7.3.2
Finite and Coﬁnite Subsets of N
Proposition 26. The predicates “X is ﬁnite” and X is coﬁnite” are Σ5.
Proof. For the ﬁniteness predicate, use Lemma 1 to express that min(X)+n /∈X for
all large enough n :
∃Y

Special(Y) ∧∀Z,W ((Y + Z = Y ∧Succ(Z,W)) ⇒X + Z ̸= X +W
	
For the coﬁniteness predicate, express that min(X) + n ∈X for all large enough n.
The logical complexity is given by Theorem 3 and Proposition 16.
2.7.3.3
The Class of Regular Subsets of N
Theorem 19. The predicate “X is regular” is Σ6.

2
Logical Theory of the Additive Monoid of Subsets of Natural Integers
73
Proof. Observe that X is regular if and only if X is ﬁnite or is periodic with period
p ≥1. This latter assertion means that there exist two integers n and p such that all
for all integers x ≥n we have x ∈X if and only if x+ p ∈X. Using Theorems 4 and
Proposition 26 this can be expressed as follows (where the variables N,P encode
the above integers n and p and where the pairs of variables (V,V ′) and (W,W ′)
respectively encode x and x+ p):
X is ﬁnite ∨∃N ∃P ∀V,W,V ′,W ′

Special(N) ∧Special(P) ∧N +V = N ∧V ⊕P = W
∧Succ(V,V ′) ∧Succ(W,W ′)
=⇒(X +V = X +V ′ ⇔X +W = X +W′)
	
2.8
Conclusion
This paper proves the undecidability of the Σ5 theory of additive monoid of subsets
of N. The decision problem for positive Σ1 formulas (i.e. no negation) is trivially
decidable since every equation is satisﬁed when all the variables are equal to the
emptyset. What about the full Σ1 theory? Care: we are looking at formulas in which
the atomic subformulas are equations between variables (such as XYXZ = ZXX): no
parameter is allowed. When regular sets are allowed as parameters then the decision
problem for systems of equations becomes undecidable, cf. [6].
The question “What is deﬁnable and what is not deﬁnable in the additive monoid
of subsets of N ?” is largely answered in this paper. Some deﬁnability results in-
volve logically complex deﬁnitions: up to Σ6 for the class of regular sets. Are such
complex deﬁnitions optimal?
The additive monoid of subsets of N can be seen as the monoid of tally languages.
What about the monoid of languages over an alphabet with at least two letters. This
question is investigated in a forthcoming paper [?].
References
1. Choffrut, C., Grigorieff, S.: Logical theory of the monoid of languages over a non tally
alphabet (in preparation)
2. Cohn, L.: On the submonoids of the additive group of integers,
http://www.macs.citadel.edu/cohnl/submonoids2002.pdf
3. Fischer, M.J., Rabin, M.O.: Super-exponential complexity of presburger arithmetic. In:
SIAM-AMS Symposium in Applied Mathematics, vol. 7, pp. 27–41 (1974)
4. Garc´ıa-S´anchez, P.A.: Numerical semigroups minicourse,
http://www.ugr.es/˜pedro/minicurso-porto.pdf
5. Ginsburg, S., Spanier, E.H.: Semigroups, Presburger formulas, and languages. Paciﬁc J.
Math. 16, 285–296 (1966)
6. ˙Jez, A., Okhotin, A.: Equations over sets of natural numbers with addition only. In:
STACS, pp. 577–588 (2009)

74
C. Choffrut and S. Grigorieff
7. Odifreddi, P.: Classical recursion theory. The theory of functions and sets of natural
integers, vol. 1. North Holland (1989)
8. Ramar´e, O.: On Shnirelman’s constant 22(4), 645–706 (1995)
9. Ram´ırez-Alfons´ın, J.L.: The Diophantine Frobenius Problem. Oxford University Press
(2005)
10. Rogers, H.: Theory of recursive functions and effective computability. McGraw Hill
(1967)
11. Rosales, J.C., Garc´ıa-S´anchez, P.A.: Numerical semigroups. Developments in Mathe-
matics, vol. 20. Springer (2009)

Chapter 3
Some Reﬂections on Mathematics
and Its Relation to Computer Science
Liesbeth De Mol
This paper resulted from a talk I gave at Machines, Computations and Universality
2013 in Z¨urich and I am very much indebted to the organizers and the participants of
this conference for a very fruitful discussion. I am particularly grateful to Maurice
Margenstern who, since he was a reader of my PhD, has given me several useful
advices related to my work and has often motivated me for inquiring further into
problems of decidability and undecidability in the context of tag systems, and more
generally, for developing my thoughts on experimental mathematics and computer
science
“No paradigm should ever be allowed to dominate education”
Benoˆıt Mandelbrot, 2002
In [24, p. 325] Knuth recounts the following story:
[At some time,] I wondered how to calculate the greatest common right divisor of two
given matrices. A few days later I happened to be attending a conference where I met
the mathematician H.B. Mann, and I felt that he would know how to solve this problem.
I asked him, and he did indeed know the correct answer; but it was a mathematician’s
answer, not a computer scientist’s answer! He said, “Let R be the ring of n×n integer
matrices; in this ring, the sum of two principal left ideals is principal, so let D be such
that
RA+RB = RD
Then D is the greatest common right divisor of A and B.” This formula is certainly
the simplest possible one, we need only eight symbols to write it down; and it relies
on rigorously-proved theorems of mathematical algebra. But from the standpoint of a
computer scientist, it is worthless.
Liesbeth De Mol
Centre Nationale de la Recherche Scientiﬁque, UMR Savoirs,
Textes Langage, Universit´e de Lille 3
e-mail: liesbeth.demol@univ-lille3.fr
c⃝Springer International Publishing Switzerland 2015
75
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_3

76
L. De Mol
Knuth used this story to explain how he considered mathematics and computer sci-
ence to be distinct from each other: it illustrates very clearly the different ways by
which mathematicians and computer scientists approach a given problem. Despite
these differences in thinking, it is not uncommon for computer scientists to stress
also the similarities between their discipline and mathematics. For instance, Dijk-
stra has argued that the methods of programming are mathematical in nature [14]
and Wegner has emphasized on several occasions that computer science is “in part
a mathematical discipline” [38].
Such reﬂections on the relation between mathematics and computer science usu-
ally take place in a computer science context since, today, most mathematicians do
not really feel the need to deﬁne their discipline with respect to another. However,
since the signiﬁcance of the computer is increasing within mathematics, some math-
ematicians have also started to reﬂect on this issue. Most well-known at this time is
probably the work by mathematicians like Borwein who has extensive experience
with so-called computer-assisted experimental mathematics and who has argued on
multiple occasions that “[t]he computer is changing the way we are doing mathe-
matics” [3].
The aim of this paper is to revisit the historically-developed question on the na-
ture of the relation between mathematics and computer science by (mainly) focusing
on mathematical practices that involve the use of the computer. This will allow me
to highlight some aspects of mathematics that are being affected by what I like to
call a computer-science way of thinking. By doing so I not only want to place this
“way of thinking” into a broader historical context of mathematics but also offer
some reﬂections on the computer science discipline itself.
By considering this relation between mathematics and computer science, it is
at some points almost unavoidable to represent things as if they were black and
white whereas in real practice this is only rarely ever true. It is for this reason that I
would like to recall here, as a kind of warning to the reader, the words following by
Hamming who stated during his Turing award lecture [21]:
We live in a world of shades of grey, but in order to argue, indeed even to think it
is often necessary to dichotomize and say “black” or “white”. Of course in doing so
we do violence to the truth, but there seems to be no other way to proceed. I trust,
therefore, that you will take many of my small distinctions in this light – in a sense, I
do not believe them myself, but there seems to be no other simple way of discussing
the matter.
3.1
Mathematical Logic, the Computer and Mathematics
Several decades before computer science was recognized as a discipline, mathemat-
ics was already inexorably tied up with what were to become some of the foun-
dational sources for computer science. The papers by people like Church, Curry,
G¨odel, Hilbert, Kleene, Post and Turing are nowadays considered as fundamental
sources of (theoretical) computer science but resulted in a context of reﬂections
on the foundations of mathematics. Hilbert’s formalistic and ﬁnitist program was

3
Mathematics and Computer Science
77
one important school of thought within this foundational context.1 He understood
formalism and the method of ﬁnite axiomatization as a way to tackle several foun-
dational problems of mathematics, viz., problems about all possible assertions in
mathematics, most notably, consistency. He believed that by addressing such prob-
lems through ﬁnitist and formalist means, mathematics could be provided with a
ﬁrm foundation (See for instance [31]).
One important conviction of Hilbert was that within mathematics, there is
no Ignorabimus. Indeed, as Hilbert famously stated during a lecture in 1930 in
K¨onigsberg2 (translated from [23, p. 963]):3
The true reason why Comte could not ﬁnd an unsolvable problem, lies in my opinion
in the assertion that there exists no unsolvable problem. Instead of the stupid Ignora-
bimus, our solution should be: We must know. We shall know.
Others however were less positive about the prospect of having a mechanical so-
lution to decide any mathematical proposition. As Von Neumann stated in 1927
(quoted from [16]):
[T]he contemporary practice of mathematics, using as it does heuristic methods, only
makes sense because of this undecidability. When the undecidability fails then math-
ematics, as we now understand it, will cease to exist; in its place there will be a me-
chanical prescription for deciding whether a given sentence is provable or not.
As we all know now, Hilbert was in fact too optimistic: in 1930-31 G¨odel proved
the limitations of the method of ﬁnite axiomatization and formalization through
his incompleteness theorems and in 1936 Church and Turing independently proved
the undecidability of the decision problem for ﬁrst-order logic.4 These results are
often interpreted as the death-knell to Hilbert’s ﬁnitist and formalist program. They
certainly were for his dream of a mathematics without Ignorabimus!
Ironically, it were exactly the different formalist devices and techniques used
and/or developed as tools to obtain such impossibility results, that would turn out
to be very useful instruments for developing (some of) the theoretical foundations
of and tools for the machine that can be seen as th´e formalist device per se, viz.
the computer. The computer is not only ﬁnite per deﬁnition but its actions are those
of the clich´e formalist practice, viz. the blind manipulation of meaningless symbols
according to some rules. It was exactly for this reason that people like Dijkstra
made a plea on multiple occasions for the signiﬁcance of formalism in the context
of computer science [15]:
1 One other such school is Brouwer’s intuitionism which today surely also has an important
impact on computer science by means of constructive type theory.
2 Ironically, G¨odel would announce his incompleteness results at the same meeting!
3 Hilbert repeated his belief in the non-existence of Ignorabimus in mathematics on several
occasions.
4 It is less well-known that Emil Post had already obtained incompleteness and undecidabil-
ity results in the early 20s in the context of so-called normal systems. These results were
later published as [33].

78
L. De Mol
The manipulation of uninterpreted formulae is [...] a most familiar operation for the
computing scientist: it is the one and only operation computers are very good at. [...]
The manipulation of uninterpreted formulae requires unambiguous formalisms [...]
Our disappointing experiences with formality should not be interpreted as something
being wrong with formality; the experiences were disappointing because we were in-
competent amateurs. But this has been changed by our exposure to computing.
Today, the idea to use formalist techniques as a tool has become part of the standard
practice within computer science. A leading thought is that of controlling problems
of unreliability, unpredictability and complexity which frequently occur in the con-
text of computing and is rooted in a belief that, to quote Dijkstra again, “[m]astery
of the reaction of the computer must not only be a theoretical possibility but a real,
practical one” [13]. As such, formal veriﬁcation, denotational semantics, Chomsky
grammars etc are today used within (the development of) programming languages
and compilers in order to deal with problems of error, non-termination, security, etc.
Also within mathematics, this formal approach is applied: proof assistants like Coq,
which help to formally specify and verify mathematical proofs, have been used or
are being used to formalize contested mathematical results, like the four-color the-
orem or the sphere packing problem. Such formalized proofs allow to control and
verify that the steps taken by the machine are correctly executed and indeed result
in a given theorem.
However, such practical realizations of formalism(s) are just one aspect of the
computer and its surrounding practices. There is also a “non-rigorous” side which
relies on experimentation, statistics, etc. to deal with problems of unpredictability,
unreliability and complexity on the level of hardware, software, the humans that rely
on it and the problems that are studied with it. For instance, in compiler design, ex-
periments have been executed to determine the optimal size of a hash table; within
hardware design statistical methods are used to regulate the cache memory; dur-
ing programming, the debugging and testing of code is often preferred over formal
veriﬁcation, etc. The interplay between the different levels involved in computing is
often too complex to permit for a feasible (complex of) formal method(s) and is thus
often bound to escape its formal control. This more “ugly” side of computing, is,
in a certain sense, more in line with von Neumann’s preference for a mathematical
practice which relies on heuristics.
It is exactly within this historically developed mathematical practice that we ﬁnd
another fundamental connection between mathematics and computer science, viz.
computations and algorithms as the means to execute them. Indeed, algorithms and
computations have always been a part of mathematics. However, this signiﬁcance
of computation and algorithms within mathematics, evident though it may seem
from our contemporary perspective, has not always been properly acknowledged.
Indeed, many of us have been educated with a mathematics that is more about ab-
stract and general structures and theories than about concretely computed objects.
The reason for this is that for a long time, a majority of mathematicians simply

3
Mathematics and Computer Science
79
preferred general and abstract results over particular computation-based results.5 As
is suggested in [9], the separation between “pure” mathematics, on the one hand, and
computation-intensive mathematics on the other, became sharper over the course of
the 19th century and the beginning of the 20th century. Interestingly enough, Hilbert
played an important role in this trend of treating “computation” rather pejoratively:
in an inﬂuential report on algebraic number theory, known as Zahlbericht and pub-
lished in 1897, Hilbert favors a more conceptual approach over a more algorithmic
approach and certainly preferred “pure” ideas over computation (Quoted from [9]):
I have tried to avoid Kummer’s elaborate computational machinery, so that here too
Riemann’s principle may be realized and the proof completed not by computations but
purely by ideas.
This idea to obtain results purely by ideas rather than “computational machinery” is
still quite popular within mathematics. However, with the rise of the computer one
also sees a slow renaissance of computations and algorithms: so-called “experimen-
tal” or “explorative” mathematics is becoming more popular, curricula no longer
shy away from discrete mathematics and a growing number of mathematicians is
focusing on mathematics of computation. It is exactly this “style” of mathematics
that von Neumann, who himself was for a long time a strong supporter of the for-
malist school of thought (See e.g. [35]), promoted and practiced in the last 10 to 15
years of his life, viz. a mathematics that is rooted in computational results. In fact,
von Neumann understood this possibility, which the new computing machines were
offering to the mathematician, as a way to escape from a mathematics “in danger of
degeneration [after] much ‘abstract inbreeding’” [36].
As becomes clear through this account, the computer and with it, computer sci-
ence, incarnates (at least) two different “styles” or “approaches”: on one side of the
spectrum one ﬁnds a more Hilbertian style which we can associate with abstract,
elegant, rigorous and formal approaches on computation, on the other side, we ﬁnd
a late-von Neumann style which is usually considered more ugly, computation-
intensive and more experimental. Evidently, in the context of computer science,
both approaches are strongly connected through the computer and its computations.
As such, they cannot be strictly separated from one another. Both can also be traced
within the history of mathematics, even though the former seems to express the
current more dominant view on mathematics.
Today, a growing community of mathematicians is embracing the computer to
advance their work and, with it, the so-conceived less elegant style of doing math-
ematics. Indeed, despite the fact that the computer is, from a certain point of view,
a formalist device, its effect on mathematics proper6 apparently lies exactly in the
opposite of being formal. So what exactly is it with the computer which encourages
this style of mathematics and how does it relate to more formalist approaches within
computer science? More generally, how is the computer affecting mathematics?
5 This does not necessarily mean that for some period in the history of mathematics, com-
putations and algorithms were no longer used or developed. Rather it means that they were
not explicitly a part of the general mathematical discourse.
6 Viz., not a mathematically-oriented computer science.

80
L. De Mol
3.2
A Number-Theorist’s Point of View
What is the impact of the computer on mathematics? An important source of inspira-
tion for my own work on this question is Derrick H. Lehmer: he was one of the ﬁrst
mathematicians, a number theorist, to use a computer for doing mathematics and
he has written on several occasions on the potential of extensive computation for
mathematics. He identiﬁed two schools of thought within mathematics [29, p. 745]:
The most popular school now-a-days favors the extension of existing methods of proof
to more general situations. This procedure tends to weaken hypothesis rather than to
strengthen conclusions. It favors the proliferation of existence theorems and is psy-
chologically comforting in that one is less likely to run across theorems one cannot
prove. Under this regime mathematics would become an expanding universe of gen-
erality and abstraction, spreading out over a multi-dimensional featureless landscape
in which every stone becomes a nugget by deﬁnition. Fortunately, there is a second
school of thought. This school favors exploration [m.i.] as a means of discovery. [B]y
more or less elaborate expeditions into the dark mathematical world one sometimes
glimpses outlines of what appear to be mountains and one tries to beat a new path in
their direction. [N]ew methods, not old ones are needed, but are wanting. Besides the
frequent lack of success, the exploration procedure has other difﬁculties. One of these
is distraction. One can ﬁnd a small world of its own under every overturned stone.
Lehmer clearly had a preference for the more “experimental” school of thought:
having been raised by Derrick N. Lehmer, also a number theorist and well-known
table-maker of prime numbers and factors, he was convinced of the experimental
nature of mathematics and especially number theory. As he explains himself: “My
father did many things to make me realize at an early age that mathematics, and
especially number theory, is an experimental science.” Such experimental approach
usually requires exploration and hence also something to explore. It is thus not sur-
prising that Lehmer – and with him several other mathematicians – regard(ed) the
computer as the perfect partner to support the “exploratory-minded”. Indeed, the
increase of “several orders of magnitude” in speed and memory combined with
the capability of the machine to deal with combinatorial complexities on the exe-
cutional level of a program, makes it possible “to explore the terrain that has been
staked out so freely and that something worth proving will be discovered in the
rapidly expanding universe of mathematics” [28, p. 146].
But what kind of mathematics is such “explorative” computer-assisted mathe-
matics? Let us ﬁrst look at a simple example and then relate it to Lehmer’s views.
The example comes from the early history of computer-assisted mathematics and
concerns the study of the decimal expansion of π and e on ENIAC, one of the ﬁrst
electronic and (externally) programmable machines.7 It is rather well-known that
one of the applications of ENIAC was the computation of multiplication rates of
7 ENIAC in its initial form was not a stored-program computer. Instead one had to manually
wire the machine for each computation. Nonetheless, it was Turing complete in the same
sense as modern machines are, viz., making abstraction from its memory limitations, it
could simulate any Turing machine (amongst others, it had a conditional). For a detailed
example of a program that was actually ran on ENIAC see [6].

3
Mathematics and Computer Science
81
neutrons in ﬁssion devices to estimate the size of the device for triggering fusion in
an H-bomb. One of the people involved with this project was John von Neumann. He
was convinced that a more experimental approach was the most suitable for study-
ing this problem and it was decided to let ENIAC “numerically simulate”, amongst
others, the multiplication rates. It was in this context that the now so widely used
but rarely questioned Monte Carlo method was introduced by Ulam. Of course, in
order to compute with the Monte Carlo method one needs a source of random num-
bers. The most obvious method at the time was to use numbers generated by some
other device (e.g. a roulette wheel) but, since electronic memory was limited at the
time, these numbers had to be fed mechanically to the machine and signiﬁcantly
slowed down the computational process. Von Neumann and others started to reﬂect
on the possibility of letting the machine generate its own random numbers. This
resulted in the construction of so-called pseudo-random generators. However, how
should one construct an algorithm for generating random numbers? Isn’t it para-
doxical to generate randomness through deterministic procedures? It is against this
background that one should understand the computation of 2,000 digits of π and e
by ENIAC: the aim was to investigate the statistical distribution of these numbers.8
ENIAC was used to compute these digits and then a team of human computers was
used to perform the statistical analysis. Also other methods were studied, includ-
ing von Neumann’s middle square method9 – which was eventually used – and the
quadratic iterator (xi = ax(1−xi−1)) which is now known as one of the paradigmatic
cases of chaotic non-linear equations.
This example is a very simple but clear example of explorative mathematics:
not only because it explicitly concerns the exploration of the digits of π and e but
also because of its wider background: the search for and study of pseudo-random
generators by exploring several possible instances. This act of exploration assumes
several other activities. Amongst others, it necessitates the development of several
special techniques: the development or selection of feasible approximating iterative
algorithms that are ﬁtted to the digital ENIAC; techniques to check for possible
errors in the computation; etc.
This study of the decimal expansion of π and e was not new but in fact ﬁts into a
wider computational and explorative tradition: long before ENIAC, expansions of π
and e were computed for several different reasons – as an exploration or test of good
approximation algorithms, as a study of whether π is rational are not, etc.10 In other
words, even though the local context changes signiﬁcantly, there is an almost natu-
ral bond between this type of explorative mathematics and (extensive) computation.
Since the electronic computer is the machine which opens up the path of exten-
sive computation, its usage in such explorative contexts is certainly not surprising.
8 The exact nature of the distribution of these numbers is still unknown today even though
it has been surmised more than once that they are normal.
9 This method is known to be a very bad random generator in general. It functions as follows:
pick a number with n digits, square it, resulting in a number of 2n digits, take its n middle
digits, square it, etc.
10 It was only in the 18th century that the irrationality of π was proven.

82
L. De Mol
However, by using the computer, the mathematician also engages with a particular
type of thinking summarized by Lehmer as follows [29]:
I should like to speculate brieﬂy on the overall impact of mechanization upon mathe-
matics of the not too distant future. Mechanization tends to emphasize practice rather
than theory, deeds rather than words, explicit answers rather than existence statements,
deﬁnitions that are formalized rather than behavioristic, local rather than global phe-
nomena, the limited rather than the inﬁnite, the concrete rather than the abstract, and
one could almost say, the scientiﬁc rather than the artistic [...] The computer is the in-
strument of our observatory, our window to the hard facts of the world of mathematics.
If one were to differentiate computer science from mathematics, the ﬁnite vs. the
inﬁnite, the concrete vs. the more abstract, etc are indeed the kind of typical differ-
ences one immediately comes up with. Even though abstraction from the machine,
the development of formal and uniform methods and inﬁnite models are an im-
portant aspect of the usual computer science practice, the object remains limited,
discrete, local, concrete and practical. It is perhaps for this reason that, as Knuth ex-
plains, computer scientists are more familiar with non-uniformity and case-by-case
analyses [27]:
If I had to put my ﬁnger on the greatest difference between mathematicians and com-
puter scientists, I would say that mathematicians have a strong preference for
non-uniform rules, coupled with a strong dislike for case-by-case analysis; computer
scientists, by contrast, are comfortable and ﬂuent with highly non-uniform structures
(like the different operations performed by real computers, or like the various steps in
long and complex algorithms).
It is also this type of thinking that ﬁts well with exploration: one needs to deﬁne
local methods and a complex of subroutines that work in one concrete context, the
methods need to be ﬁnite (one cannot “explore” the inﬁnite as such, only its ﬁnite
approximations) and one often needs to proceed on a case-by-case basis. In other
words, what I claim here and what Lehmer claims is that some of the typical features
of what one could tentatively call a computer science way of thinking has a common
basis with that fraction of the mathematical discourse which is usually associated
with exploration and computation. It is for this reason that, in view of a supposed
increasing signiﬁcance of the computer, Lehmer makes the following two possible
predictions about the future of mathematics [29]:
It would be a pleasure to predict that, as time goes on, the use of the instrument will
become widespread and the nature of mathematics will slowly change from the dan-
gerously unstable ﬂuid art that it is apparently approaching today to a more and more
structured and explicit science. There is an alternate prediction. Already we see, in-
stead, a splitting from mathematics of a new branch commonly called computer sci-
ence, which includes enough technology to frighten away your topologist or functional
analyst. Soon disciplinary fences will be erected. It has been said that the invention of
photography relieved the graphic artist of his obligation to depict nature and drove him
into impressionism and ﬁnally to abstraction. This, it seems to me, is apt to be also the
future of mathematics.

3
Mathematics and Computer Science
83
It is impossible at this point in history to verify which of these two predictions is the
most correct one even though disciplinary fences have been built. However, what we
can do is to partially verify the current tendencies within mathematics with respect
to computation and exploration.
3.3
The Impact of the Computer on Mathematics: Some
Quantitative Results
In order to ﬁnd at least a heuristic guide to trace the impact the computer is hav-
ing on mathematics, I studied the quantitative impact of the computer on mathe-
matics. The method consists in measuring the frequency by which particular terms
are being used within the mathematics discipline as represented in databases such
as MathSciNet and Zentralblatt. This study is not completed yet, but the initial
results at least give some indications for further research. This far, I focused on
MathSciNet and three groups/clusters of terms: Group I consists of the words:
comput∗, calcula∗, machine∗; group II consists of: experiment∗, heurist∗, conject∗,
explor∗, inspect∗and, ﬁnally, group III consists of: Algorith*, program∗, automat∗,
digital∗, simulation∗. Figs. 3.1–3.3 show a plot of the evolution of the frequency
of these three groups of terms as used within title or review text of items listed in
MathSciNet between 1940–2010.
These plots indicate that the signiﬁcance of these three groups within mathemat-
ics increases as a function of time: by 2010, almost 10% of all items listed in Math-
SciNet explicitly refer to terms that are related to computers and computing in title
or review text. Also the usage of terms that are often associated with exploration,
terms such as experiment, conjecture, etc, show a steady increase with about 7% in
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
 1940
 1950
 1960
 1970
 1980
 1990
 2000
 2010
% of publications
Years
comput* or calcula* or machine
Fig. 3.1 Evolution of the frequency of the terms comput∗, calcula∗, machine∗in MathSciNet
1940–2010

84
L. De Mol
 1
 2
 3
 4
 5
 6
 7
 8
 1940
 1950
 1960
 1970
 1980
 1990
 2000
 2010
% of publications
Years
Group II
Fig. 3.2 Evolution of the frequency of the terms experiment∗, heurist∗, conject∗, explor∗,
inspect∗in MathSciNet 1940–2010
 0
 2
 4
 6
 8
 10
 12
 14
 1940
 1950
 1960
 1970
 1980
 1990
 2000
 2010
% of publications
Years
Group III
Fig. 3.3 Evolution of the frequency of the terms Algorith∗, program∗, automat∗, digital∗,
simulation∗in MathSciNet 1940–2010
2010. But most interestingly is the evolution of group III with no less than +/- 13%
of the publications included in MathSciNet containing one of the terms of group III
in its title or abstract by 2010. A more detailed analysis of the results also shows
that within each of the three groups there are always one or two terms that take up
most of the percentages. In group I, the terms starting with comput∗and calcula∗
are the most dominant, with comput∗steadily taking over from calcula∗. For group
II, the dominant terms are conjecture∗and experiment∗. Finally, for group III, it are
algorith∗and program∗that are very dominant (by 2010, they take up about 9%)
though one can observe a steady increase also in the usage of simulation∗starting
around 1980.

3
Mathematics and Computer Science
85
So what do these results indicate? Even though further analysis on the content
or semantic level is required, combined with an extension to other databases, the
results at the very least indicate that the use of terms that are intricately tied to com-
puters (Groups I and III) show a signiﬁcant increase since the early years of digital
general-purpose computing. This increase cannot be reduced to the development of
the computer science discipline alone: a more detailed study of the distribution of
group I over the disciplines, using the MSC classiﬁcation of disciplines, shows that
for the computer science related disciplines like MSC 65 (Numerical Analysis) or
MSC 68 (computer science) there is in fact a decrease in the signiﬁcance of Group
I which starts already around 1950 and then stabilizes around 1970. This decrease
can be explained by a complementary increase in the usage of the terms algorith∗
and program∗as is shown in Fig. 3.4.
 5
 10
 15
 20
 25
 30
 35
 40
 1940
 1950
 1960
 1970
 1980
 1990
 2000
 2010
% of publications
Years
algorit*, program*
Group I
Fig. 3.4 Evolution of the frequency of the terms of group I versus that of algorith∗and
program∗in the disciplines 65∗(numerical analysis), 68∗(computer science), 90∗(operations
research, mathematical programming), 93∗(systems theory; control) and 94∗(information
and communication; circuits)
Whereas Groups I and III are quite naturally related to the computer, the results
for Group II are not. Nonetheless it cannot be neglected that the usage of terms as-
sociated with “experimental” mathematics shows a steady increase which parallels
the one for Group I.
These results are what they are: mere quantitative results which should certainly
not be overinterpreted. However, for now they do serve the purpose of testing
Lehmer’s two alternative predictions in our time: even though it is indeed true that,
in the meantime, disciplinary fences have been built there is a clear indication that
computation, calculation, algorithms, programs, conjectures and experimentation
have become more important over the years. However, these quantitative results do
not allow us to directly tackle the question of the qualitative impact of the computer
on mathematics but can at best serve as a heuristic guide to detect more carefully
the impact of the computer on mathematics.

86
L. De Mol
3.4
Computer-Assisted Explorative Mathematics:
Characteristics and Problems
How is the computer changing mathematics, if at all? This far I have looked at
this question mostly from the perspective of mathematics itself rather than from the
computer, focusing ﬁrst on the particular views of Derrick H. Lehmer and then at
some more global developments we can detect within mathematics. But what is it
exactly that the computer brings to mathematics besides extensive computational
results to be explored? What particular characteristics are there to the computer that
help to understand or clarify its (potential) effect on mathematics? In [12] I discuss
three features which I consider as characteristic to computer-assisted explorative
mathematics: human-computer interaction; the signiﬁcance of time and processes
and the internalization of mathematics into the machine.
3.4.1
Human-Computer Interaction
If there is one characteristic which is typical of computer-assisted explorative math-
ematics, it is the interaction with the computer. Of course, the history of mathemat-
ics is full of interactions between mathematicians and non-human instruments11.
The most frequently used is the pencil-and-paper method: one writes and develops
a notation in order to develop some technique, in order to communicate a result, in
order to make a computation etc. This writing act always involves a “coding” prac-
tice – the use of symbols, of drawings, of abbreviations etc. Also within computer-
assisted explorative mathematics coding practices are involved. However, whereas
in the former case, the interpreter is always a human, the interpreter in the latter
case is a non-human, viz., the computer. This affects the coding practice and hence
also the interaction. This practice requires new specialized types of knowledge and
skills which take into account the fact that one is communicating with a non-human.
First of all, one needs (a) suitable “language(s)” for the two-way communication. It
is exactly for this reason that we have seen the development of specialized software
packages like Mathematica or Maple. It is also for this reason that extensive use is
being made from visualization techniques that allow the mathematician to “digest”
the vast amount of data generated by the computer. Secondly, and perhaps more im-
portantly, one needs a good “format” to talk with the machine, viz. one needs good
algorithms that are adapted to what the machine is good at and what it is bad at. I
already provided the example of the development of pseudo-random generators, a
type of algorithms that was never really considered before in the history of math-
ematics. Lehmer talks in this context about the need for an “idiot” approach: one
needs algorithms that are executable by an “idiot” who cannot rely, for instance, on
educated guesses and hence needs to be given instructions for every possible “be-
havior”. One can, for instance, think of the SRT algorithm for division: it replaces
11 Before the ﬁrst computers, machines were already used within (applied) mathematics (e.g.
Hartree’s differential analyzer) and the practice of computer-assisted mathematics cer-
tainly has some of its roots in these practices. I will not consider such machines here.

3
Mathematics and Computer Science
87
the educated guessing we rely on for long division by a numerical table. Hartree,
another computer pioneer, understood this new way of developing algorithms for
the idiot as a real challenge for the future [22]:
[I]n programming a problem for the machine, it is necessary to take a “machine’s-eye
view” of the operating instructions, that is to look at them from the point of view of
the machine which can only follow them literally, without introducing anything not
expressed explicitly by them, and try to foresee all the unexpected things that might
occur in the course of the calculation, and to provide the machine with the means of
identifying each one and with appropriate operating instructions in each case. And this
is not so easy as it sounds; it is quite difﬁcult to put oneself in the position of doing
without any of the hints which intelligence and experience would suggest to a human
computer in such situations.
Today, part of these problems are already dealt with through developments in pro-
gramming. For instance, the compiler allows to detect a whole range of syntactical
and semantical errors by means of parsers, the symbol table and the semantic ana-
lyzer. Viz., part of these problems have already found a formal solution. However,
some of them haven’t and require the development of special more heuristic and
statistical techniques. For instance, on the hardware level, it are the laws of statistics
that govern part of how the memory is cached.
This requirement to look at a problem also from the machine’s eye has resulted
in important progress both within computer science and mathematics. In computer
science, for instance, the development of compiler techniques allow to help the pro-
grammer to specify his algorithm in the machine’s language. Within mathematics,
an obvious example are the advances being made within numerical analysis since
the rise of electronic computing: it ﬁlls the need for good and efﬁcient iterative ap-
proximation techniques that “work” for the machine. For instance, in the 50s, when
electronic memory was still very limited, new iterative algorithms were being used
that allow to reduce the size of the memory needed during computation (see e.g. [8]).
3.4.2
Internalization
In the early years of digital general-purpose computing there were two major bottle-
necks. The ﬁrst was the programming bottleneck: even though computation speed
was increased with several orders of magnitude, the lack of programming languages
and compilers meant that the setting up of a problem could take several days if not
weeks. The second bottleneck was the memory bottleneck: electronic memory was
very limited so one had to rely mostly on external memory devices like the punched
cards. If additional memory was required during computation, this reliance on me-
chanical memory seriously slowed down the computation. Viz., the speed of mem-
ory storage and retrieval wasn’t adapted at all to that of the computation.12 As these
two bottlenecks got steadily and partially resolved, it became possible to store inter-
nally into the machine more and more subroutines and the process of the so-called
“algorithmization” of knowledge could be started. Also within mathematics one can
12 Today the speed of read and write operations is still a major topic in hardware design.

88
L. De Mol
observe a steady “algorithmization” of mathematical knowledge and skills. Some fa-
mous examples are the Zeilberger-Gosper algorithm and the PSQL algorithm. The
result of this is that today we have huge libraries of mathematical subroutines at our
availability where algorithms like the Zeilberger-Gosper algorithm, can be called by
their name.
This steady process of internalization evidently goes hand-in-hand with human-
computer interaction: since more and more processes are assigned to the machine
rather than to the human, the boundaries between what is done by the “idiot” and
what is done by the “intellect” are more and more blurred. For instance, in the early
years of digital general-purpose computing the machine was concerned only with
brute-force mathematical calculations and the human did most of the exploration.
Nowadays, part of this exploration is internalized into the machine. Indeed, going
back to our simple example of the ENIAC computations of the digits of π and e,
it is clear that today, no human would bother to do the statistics on the digits. The
machine will compute the digits and inspect them before something is returned to
the human.
3.4.3
Time and Processes
One ﬁnal characteristic that I would like to consider brieﬂy here, is the signiﬁcance
of time and processes within computer-assisted mathematics.13 In a study which at-
tempts at differentiating between so-called “computer science thinking” and “clas-
sical” mathematical thinking, Knuth sampled nine mathematical text books in order
to get a ﬁrmer grip on the possible differences. His conclusions are [26]:
Computer scientists will notice [...]that two types of thinking are absent from the ex-
amples we have studied [...] In the ﬁrst place, there is almost no notion of “complexity”
or economy of operation in what we have discussed. Bishop’s mathematics is construc-
tive, but it does not have all the ingredients of an algorithm because it ignores the “cost”
of the constructions. [...] The other missing concept is related to the “assignment oper-
ation” :=, which changes values of quantities. More precisely, I would say the missing
concept is the dynamic notion of the state of a process: “How did I get here? What is
true now? What should happen next if I’m going to get to the end?” Changing states of
affairs, or snapshots of a computation, seem to be intimately related to algorithms and
algorithmic thinking. Many of the concepts of data structures [...] depend very heavily
on an ability to reason about the notion of process states, and we rely on this notion
also when studying the interaction of processes that are acting simultaneously.
Knuth’s conclusion is in a certain sense not surprising: “classical” mathematics has
a very different sense of time as compared to computer science. A computation is
executed as a dynamical process that develops and changes over time. One of the
reasons why time is really an issue within computing is the combination of very fast
computation (relative to human computation) with the fact that all computations on
13 I am in fact very much indebted to Maurice for having drawn my attention to this funda-
mental difference between mathematics and computer science in a report he wrote for my
dissertation.

3
Mathematics and Computer Science
89
a standard computer are ﬁnite both with respect to time and space. This stands in
sharp contrast with mathematics where the inﬁnite rules and computational speed
is only rarely a concern. In a report describing the state of computer science and
engineering research this difference is described as follows [1, p. 9]:
Mathematics deals with theorems, inﬁnite processes, and static relationships, while
computer science emphasizes algorithms, ﬁnitary constructions, and dynamic relation-
ships. If accepted, the frequently quoted mathematical aphorism, the system is ﬁnite,
therefore trivial, dismisses much of computer science.
This time dimension of computations becomes more explicit in the light of the fact
that most computational processes are often not reversible and this in contrast to
mathematics. As Margenstern explains [32, p. 645]:
Mathematical theories make use of reversible time; how can they exhaust nature, which
is not at all reversible? Let us note that in our discrete time of computations, time is
irreversible: it is very often extremely difﬁcult to run an algorithm backward. At the
highest level of generality it is impossible.
The introduction of the arrow of time into computations combined with the ﬁnite-
ness of the machine also affects the mathematics that is done with such computa-
tions. For instance, if one is working with iterations over the reals, the fact that one
has to work with ﬁnite approximations of these reals, combined with the fact that
one is squeezing thousands or even millions of computations in a short time span,
necessitates a study of the error propagations that may occur along the process. In
fact, it is this problem that lies at the very foundation of the study of non-linear
equations like the quadratic iterator that I mentioned before (See e.g. [37, pp.3–4]).
The processual and dynamical character of computation is reﬂected both on the
hardware as well as on the programming level. We already saw the example of the
assignment statement on the programming level in Knuth’s quote (See also Mar-
genstern:2012). That this processual character of computations needs to be reﬂected
also on the programming level, was already understood by von Neumann. Indeed,
in a series of reports which introduces the well-known ﬂowchart notation, he ex-
plains [17]:
[C]oding is not a static process of translation, but rather the technique of providing a
dynamic background to control the automatic evolution of a meaning.
In fact, as he explains elsewhere, it is exactly this dynamical nature of computation
combined with the speed by which it is executed that results in the need for logical
control [37]:
[C]ontemplate the prospect of locking twenty people for two years during which they
would be steadily performing computations. And you must give them such explicit
instructions at the time of incarceration that at the end of two years you could return
and obtain the correct result for your lengthy problem! This dramatizes the necessity
for high planning, foresight, and consideration of the logical nature of computation.
This integration of logic in the problem is a consequence of the high speed.

90
L. De Mol
On the hardware level, standard CPU design relies on a central clock signal that is
emitted to all the hardware units. Without that central pulse which is used to control
time, our current architectures would not be able to sequence and synchronize their
operations let alone be able to “remember something”.
3.5
Some (New and Open) Problems
Faced with a new practice created by the introduction of the computer, it becomes
necessary for the mathematician who wants to use a computer in his research to
reﬂect on a wide range of (new) problems or challenges that, at least, partially in-
tegrate a way of thinking that is more akin to computer science. I already indicated
some important developments within mathematics that are directly rooted in this
new practice, like for instance within the ﬁeld of numerical analysis or the study of
randomness. In what follows I will sketch some (new and open) problems that (can)
arise within a context of computer-assisted mathematics in more detail.
3.5.1
Mathematical Understanding
It is well-known that results coming from computer-assisted mathematics, espe-
cially so-called computer-assisted proofs, are not very much appreciated by a part
of the mathematical community. One important reason for this is that it is claimed
that such results do not provide any understanding of the problem resolved with
them. One famous example in this context is the computer-assisted proof of the four
color theorem. In this context, Bonsall, a mathematician at the university of Edin-
burgh stated [2, p. 14]:
It is no better to accept without veriﬁcation the word of a computer than the word of
another mathematician [...] We cannot possibly achieve what I regard as the essential
element of proof – our own personal understanding – if part of the argument is hidden
away in a box. [...] Perhaps we are seeing the birth of a new kind of computer-assisted
quasi-mathematics, but is has no place in the science of mathematics and if it is to
survive must develop its own scientiﬁc ethos – perhaps more akin to the experimental
sciences.
Another mathematician, Ian Stewart, complains not only about the fact that part of
such proofs are hidden but also about their lack of structure, viz. (Quoted from [30,
p. 41]):
[Such proofs do] not give a satisfactory explanation why the theorem is true. This is
[...] mostly because it is so apparently structureless. The answer appears as a kind of
monstrous coincidence. Why is there an unavoidable set of reducible conﬁgurations
[within the four-color problem]? The best answer at the present time is: there just is.
The proof: here it is, see for yourself. The mathematician’s search for hidden structure,
his pattern-binding [sic] urge, is frustrated.
The fact that with computer-assisted proofs, part of the proof is generated by a ma-
chine and mediated by a very lengthy code implies that, ﬁrst of all, such computer-

3
Mathematics and Computer Science
91
assisted proofs are far from being “elegant” and very lengthy and, secondly, that
part of the proof is in fact not “surveyable” by a human. Moreover, such proofs
are mostly based on a case-by-case analysis and, as such, are in need of a more
non-uniform approach which is usual business in computer science but not in math-
ematics. For instance, for the four-color theorem over 1500 cases were derived! Add
to this that such proofs are considered to be more error-prone because of their length
and the programming and hardware involved and the mathematician is left with a
feeling that this cannot be a “good” insightful proof. It is certainly true that the
understanding one gains from, say, one of the classical proofs of the Pythagorean
theorem cannot be identical to the understanding one gains from a proof that con-
tains hundreds of pages of programs and millions of computations. And it is indeed
not very insightful to “have a look” at the over 1500 cases of the original proof of
the four color theorem just as there is no insight into why there are x cases rather
that x−1 cases!
Does this mean that there is no understanding whatsoever to be found in proofs
such as that of the four color theorem? I do not think so. Rather it is my view that
they provide a different kind of understanding. Indeed, from the perspective of a
computer-assisted proof, it makes no sense to ask why a given problem reduces
to x cases and not to y but it does make sense to follow the overall structure of
such proofs as sketched in their published versions and to see how and why this
semi-algorithmic structure works. To put it in the words of Borwein, a well-known
advocate of experimental mathematics [3]: “[T]he act of programming – if well
performed – always leads to more insight about the structure of the problem.”
This problem of what kind of understanding computer-assisted proofs such as the
four-color theorem convey is a non-trivial problem which will have to be addressed
properly by the mathematical community especially if, in the future, more and more
mathematical results would be proven in this way.
3.5.2
Hidden Algorithms and Explorations
One important characteristic of (explorative) computer-assisted mathematics is that
part of the mathematics required in tackling a given problem is internalized into
the machine. I brieﬂy discuss two problems here that are partially rooted in this
characteristic.14 First of all, with current programming environments, much of the
algorithms being used are no longer explicitly programmed by the mathematician
who uses them but are instead called by their name. This means that part of the
knowledge that is used within computer-assisted mathematics is unknown to the
mathematician unless he/she has bothered him/herself with looking at the details of
the subroutines called by the main program. This becomes more problematical in the
14 Note that the above problem of mathematical understanding is also partially rooted in the
internalization of knowledge inside the machine.

92
L. De Mol
light of software that is not open source. As Joris van der Hoeven, mathematician
and developer of GNU TeXmacs and Mathemagix explains:15
As a mathematician, I am deeply convinced that only free programs are acceptable
from a scientiﬁc point of view. I see two main reasons for this:
•
A result computed by a “mathematical” system, whose source code is not public,
can not be accepted as part of a mathematical proof.
•
Just as a mathematician should be able to build theorems on top of other theorems,
it should be possible to freely modify and release algorithms of mathematical soft-
ware.
However, it is strange, and a shame, that the main mathematical programs which are
currently being used are proprietary. The main reason for this is that mathematicians
often do not consider programming as a full scientiﬁc activity. Consequently, the de-
velopment of useful software is delegated to “engineers” and the resulting programs
are used as black boxes.
In a certain sense, the reliance on algorithms without having gone through all of their
details, is comparable to referring to common or accepted mathematical knowledge
without having gone through every of its details. However, if this knowledge is sim-
ply not free to access then the challenges posed by computer-assisted proofs are
perhaps rather small when compared to those posed by non open-source mathemat-
ical software! If, in the future, computer-assisted mathematics would become more
important, this will become a major problem by itself touching upon such issues as
scientiﬁc reproducibility and patentability.
A second problem that I would like to discuss brieﬂy here concerns the idea of
letting the computer do part of the exploration. It is a well-known practice that,
when one uses a computer today, one will probably instruct it not to provide the
mathematician with all data computed but with a certain selection of them, be it by
way of visualizations, plots, statistics, etc. However, by delegating more and more of
the inspection to the machine itself one unavoidably also makes more assumptions
about the data to be inspected by the machine and one runs the risk of missing out on
some essential information. One example in this context comes from Wolfram who
was studying so-called mobile automata, a class of computational devices which
differ from cellular automata in that they do not update all cells in parallel but one
at a time. Instead of looking explicitly at the “raw” behavior of these automata,
Wolfram automated his search for a particular type of behavior [39]:
[B]eing convinced that more complicated behavior must be possible, [...] I wrote a
program that would automatically search through large numbers of mobile automata.
I set up various criteria of the search, based on how I expected mobile automata could
behave. And quite soon, I had made the program search a million mobile automata,
then ten million. But still I found nothing. So then I went back and started looking by
eye at mobile automata with large numbers of randomly chosen rules. And after some
15 Extracted from
http://www.texmacs.org/tmweb/manual/webman-about.en.html
on February 24, 2014.

3
Mathematics and Computer Science
93
time what I relayed was that with the compression scheme I was using there could be
mobile automata that would be discarded according to my search criteria, but which
nevertheless had interesting behavior.
As becomes clear from this example, by internalizing part of the exploration one is
always making certain assumptions which can result in errors. Another example is
the use of Monte Carlo methods: since the 40s this has become a standard method
in computer-assisted science in general. However, contrary to the early years of the
Monte Carlo method, one only rarely questions the statistical assumptions under-
pinning the method.
The internalization of knowledge inside of the machine implies a number of prob-
lems that are far from trivial. The bottom line with these type of problems seems to
be this: the more knowledge and assumptions that are hidden inside the machine,
the more important it becomes for the mathematician to be critical about or at least
be aware of the knowledge he/she is presupposing.
3.5.3
Finiteness, Time and Unpredictability
Perhaps one of the most important differences between mathematics and computer
science is the fact that computer science deals with computational processes that
develop over and are limited by time and space. Hence, it should not be surprising
that this feature results in several challenges for computer-assisted mathematics.
One important side-effect of the time-sensitive character of computations is their
unpredictability. As Hamming explains [20]:
One often hears the remark that computers can only do what they are told to do. True,
but that is like saying that, insofar as mathematics is deductive, once the postulates are
given all the rest is trivial. [T]he truth is that in moderately complex situations, such as
the postulates of geometry or a complicated program for a computer, it is not possible
on a practical level to foresee all of the consequences. Indeed, there is a known theorem
that there can be no program which will analyze a general program to tell how long it
will run on a machine without actually running the program.
This unpredictability is not just some theoretical problem or property. It is in fact
this unpredictability that usually brings mathematicians to the computer: it is be-
cause one cannot predict the outcome of a certain computational problem that one
needs to rely on and trust the machine’s abilities. However, many such problems,
when programmed, may contain inﬁnite loops or need an unreasonable if not in-
ﬁnite amount of time and/or space and such that this cannot be predicted by the
mathematician. This necessitates the need for developing local programming strate-
gies that are often “experimental” in nature, viz., they do not necessarily result in a
(correct) solution and often require adjustments in the light of new computational
results. For instance, Hales had to experimentally determine several constants in the
process of developing the proof of the sphere packing problem [5]:
Hales remarks for instance that “The constant 2.51 was determined experimentally to
have a number of desirable properties”, and similar experimental determinations recur

94
L. De Mol
repeatedly throughout the paper. Several of the initial decisions had to be modiﬁed in
the light of later calculations.
Another problem that comes up in this context can be illustrated with the follow-
ing example coming from my own computer-assisted research on a class of com-
putational devices known as tag systems. Also here it was the unpredictability of
the computational processes of these devices that made it necessary for me to in-
clude certain limitations into the programs used to study these devices and which
instructed the computer when it should give up on a certain tag system. For instance,
in studying the behavior of tag systems with arbitrary initial words of length 300, I
included the limitation that the program should stop if (1) after 10,000,000 compu-
tational steps the tag system had not halted or become periodic and (2) one of the
produced words became longer than 15,000,000. Initial words that resulted in more
than 10,000,000 computational steps were tentatively classiﬁed as possible immor-
tals. But what kind of derivations can one make on the basis of information that is
based only on a ﬁnite piece of information of a (possibly) inﬁnite dataset? In how
far are, for instance, the results from a statistical analysis based only on the ﬁrst part
of a computation representative for the whole computation?
These kind of problems are not only characteristic for computer-based research
but for explorative mathematics in general: experimental research on the Riemann-
zeta function, the twin prime conjectures, the Goldbach conjecture or the Collatz
problem all suffer from the problem that one has information only about a ﬁnite
subdomain of the (possibly) inﬁnite domain. Consider for instance the Collatz func-
tion C. Given an integer n0, if n0 is odd compute n1 = 3n0 + 1 if not, compute
n1 = n0/2. If n1 is odd, compute n2 = 3n1 + 1, else, compute n2 = n1/2, etc. The
Collatz conjecture states that for any ni, there is an mj such that after mj iterates
of C on ni it will end in the loop 1, 2, 4. Now, assume we want to compute C for
some very large integer. We start the computation on our computer but after weeks
of iterations we have an overﬂow. What would this tell us? Conversely, what can we
derive from the fact that the conjecture has been veriﬁed for 5 × 260 integers? It is
in such experimental context that one is in need of a “conduct of good behavior”,
a certain standard of when one has reasons to make a conjecture and when not and
one should always be extremely careful in making generalizations based on a ﬁnite
number of instances.
Another problem, which is directly related to the ﬁniteness of the computer,
comes up in the context of computations over the reals and is related to the Mandel-
brotset. Given the function:
c →c2 + c, c ∈C
the Mandelbrot set is deﬁned as:
M = {c ∈C|c →c2 →c2 + c →...remains bounded}
The set is most well-known for its intricate boundary which gives rise to very beau-
tiful visualizations. One important question, that was formulated by Roger Penrose,
is whether it is decidable for any c whether or not it belongs to the Mandelbrotset.

3
Mathematics and Computer Science
95
There are now two major competing models of computation over the reals to address
this and other related problem:
[The bit model] reﬂects the fact that computers can store only ﬁnite approximations
to real numbers. Roughly speaking, a real function f is computable in the bit model
if there is an algorithm which, given a good rational approximation to x, ﬁnds a good
rational approximation to f (x). The second approach is the algebraic approach, which
abstracts away the messiness of ﬁnite approximations and assumes that real numbers
can be represented exactly and each arithmetic operation can be performed exactly
in one step. The complexity of a computation is usually taken to be the number of
arithmetic operations (for example, additions and multiplications) performed. The al-
gebraic approach applies naturally to arbitrary rings and ﬁelds, although for modeling
scientiﬁc computation the underlying structure is usually R or C.
The second approach is most known through the work of Blum, Shub and Smale:
within their model, the Mandelbrotset turns out to be undecidable. However, the
drawback of the model is that functions which one would consider intuitively as
being decidable, like, for instance, a simple transcendental function such as ex, also
turn out to be undecidable under this model. This is not the case with the bit model.
In this latter model, the undecidability of the Mandelbrot set is still an open problem.
One interesting consequence of this model is that if a function f is uncomputable
than there is no good rational approximation of f. As such, the undecidability of the
Mandelbrot would imply that we cannot rely on our computer-generated visualiza-
tions of the set. Indeed, since any such visualization is but a ﬁnite approximation
of the “real” set, we cannot rely on it and hence also not on the observations and
conjectures that are rooted in those visualizations.
3.5.4
Unreliability
From the previous sections it becomes clear that some of the characteristics of
computing on a machine result in a number of problems within computer-assisted
mathematics that need to be taken into account in some or the other way by the
mathematician who uses the computer in his/her research. All of these different
problems contribute to a feeling that the results from computer-based research are
quite unreliable – in fact, it is often for this reason that the label “experimental”
is used. The fact that usually hundreds or thousands of lines of code are involved,
the fact that the human him/herself can often only wait and see, the fact that one
does not have access to all data generated during the process, the fact that one may
encounter truncation errors, etc indeed do not give an impression of high reliability.
Add to this that many results are “explorative” results and it becomes understandable
that many a mathematician turns his/her back to the results that are machine-aided.
On the other hand, however, one cannot deny that the computer in fact gives us
the possibility, to state it again in Lehmer’s words, “to explore the terrain that has
been staked out so freely and that something worth proving will be discovered in
the rapidly expanding universe of mathematics”. The choice is there to make for the
mathematician but if the choice is in favor of the machine it is clear that one needs

96
L. De Mol
to deal in some or the other way with these problems of unreliability that becomes
so explicit in this context. Two extreme positions have been proposed in this context
and relate back to the tensions I sketched in Sec3.1 between formalism and “ex-
perimentalism”; rigorous and non-rigorous math, etc as highlighted in the contrast
between Hilbert’s work and that of the late von Neumann.
One extreme position is represented by Doron Zeilberger, a well-known math-
ematician and strong supporter of computer-assisted mathematics. He has claimed
that, even though today [40]:
the mathematical faith is thou shalt prove everything rigorously [...] a new testament is
going to be written. Although there will always be a small group of “rigorous” old-style
mathematicians [...] who will insist that the true religion is theirs, and that the computer
is a false Messiah, they may be viewed by future mainstream mathematicians as a
fringe sect of harmless eccentrics [...] In the future, not all mathematicians will care
about absolute certainty, since there will be so many exciting new facts to discover. We
will have (both human and machine) professional theoretical mathematicians, who will
develop conceptual paradigms to make sense out of the empirical data, and who will
reap Fields medals along with (human and machine) experimental mathematicians. [...]
As absolute truth becomes more and more expensive, we would sooner or later come
to grips with the fact that few non-trivial results could be known with old-fashioned
certainty. Most likely we will wind up abandoning the task of keeping track of price
altogether, and complete the metamorphosis to non-rigorous mathematics.
This is quite a strong claim: Zeilberger is convinced that human “proofs” as cer-
tiﬁcates of truth will be abolished and replaced by a non-rigorous mathematics be-
cause few non-trivial truths that can be proven by short and human proofs will be
left. Again, it is impossible at this point to evaluate this prediction, but the idea
that mathematics would completely abandon the very idea of proof as a guarantee
of certainty, is precarious. This, however, does not mean that there will be no sit-
uations where mathematical results, which are true only to a very high degree of
certainty, will be more easily accepted. It is already a common practice to use for
instance probabilistic algorithms and, in the context of computer-assisted proofs,
the idea of corroboration, where different groups of researchers prove the same re-
sult independently from each other, has also already been suggested. For instance,
Brady remarked about his proof of the determination of the Busy Beaver winner for
four-state Turing machines [4]:16
While not all of the exploratory activities are reproducible, the runs [...] can be re-
produced, so that by utilizing the techniques described in this paper the proof can be
corroborated.
An opposite response to the challenges posed by the computer for mathematics, is
to take the side of full rigorous mathematics by means of formalized proofs with the
help of interactive proof assistants such us HOL or Isabel. The four-color theorem,
for instance, has been formalized in this way by Gonthier [18] and also Hales and
16 The Busy Beaver problem for a class of binary Turing machines with m states is the prob-
lem to determine which of these Turing machines will print out the largest number of 1s
when halting.

3
Mathematics and Computer Science
97
others are now working for some years on the so-called FlySpeck project which
aims at formalizing the computer-assisted proof of Kepler’s conjecture [19]. The
reason why people like Gonthier and Hales are working on such proofs is rooted in
the uncertainty associated with lengthy computer-assisted proofs. By means of for-
malization, it is believed that possible errors are excluded since every single logical
step is (supposedly) veriﬁed by the proof assistant [19]:
A formal proof is a proof in which every logical inference has been checked, all the
way back to the foundational axioms of mathematics. No step is skipped no matter how
obvious it may be to a mathematician. A formal proof may be less intuitive, and yet is
less susceptible to logical errors. Because of the large number of inferences involved,
a computer is used to check the steps of a formal proof.
This approach of formalized proofs however is not only applied to lengthy computer-
assisted proofs. The developments of automated veriﬁcation and proving has re-
sulted in initiatives like the Mizar project which aim at formalizing the whole of
mathematics. The motivations are the same: a believe that, partially due to the in-
troduction of the computer into mathematics, mathematical knowledge is becoming
too complex and, as a consequence, that there might be a tendency towards non-
rigouresness. In the QED manifesto, which proposes the formalization of all math-
ematics, this is stated as follows [34]:
[T]he increase of mathematical knowledge during the last two hundred years has made
the knowledge, let alone understanding, of all of even the most important mathemati-
cal results something beyond the capacity of any human. For example, few mathemati-
cians, if any, will ever understand the entirety of the recently settled structure of simple
ﬁnite groups or the proof of the four color theorem. Remarkably, however, the creation
of mathematical logic and the advance of computing technology have also provided
the means for building a computing system that represents all important mathematical
knowledge in an entirely rigorous and mechanically usable fashion. The QED system
we imagine will provide a means by which mathematicians and scientists can scan the
entirety of mathematical knowledge for relevant results and, using tools of the QED
system, build upon such results with reliability and conﬁdence but without the need
for minute comprehension of the details or even the ultimate foundations of the parts
of the system upon which they build.
Also here it seems precarious to believe that, in the future, mathematical knowledge
will become completely formalized in some computer-based system. Besides the
fact that there is a problem here of inﬁnite regress, indeed, “who will control the
controller”,17 “[i]t is a large labor-intensive undertaking to transform a traditional
proof into a formal proof” [19]. Hence, just as one can wonder whether the average
mathematician will take the side of fully non-rigorous mathematics `a la Zeilberger,
one can wonder whether he/she will take the side of fully formalized mathematics.
These two opposite alternatives, formalized and non-rigorous mathematics, are
two sides of the same medal. They are both rooted in a believe that mathematics is
becoming less reliable and rigorous, partially due to the introduction of the com-
puter. At this point, it is hard to imagine a future where either of these two extremes
17 Remark from Maurice Margenstern during MCU 2013.

98
L. De Mol
in one or the other form will really become a dominant practice. But is it really de-
sirable that we evolve to any of these two alternatives? Faced with a new situation in
mathematics, I believe that mathematics would only gain if both sides, rather than
one dominating the other, would try and complement/advance each others ﬁndings.
3.6
Some Afterthoughts
Computer science has many different historical roots and mathematics is certainly
one of them: formalization and algorithms were and are a part of the mathemat-
ical discourse and this long before the computer inspired the formation of a new
discipline known as computer science. Today we see that a converse inﬂuence is
manifesting itself: the computer is also starting to affect mathematics. Computer-
assisted exploration is resulting in a revival of so-called experimental mathematics,
a practice which is certainly not new to the history of mathematics but has been
pushed to the background for the last two centuries. It is within such practice that
algorithms and non-uniform methods rather than general theories are at play.
It is thus not surprising that explorative computer-assisted mathematics has more
in common with a computer science way of thinking than “classical” mathemat-
ics. The reason for this is not the mere fact that both are (usually) concerned with
computation, but is also rooted in the use of a physical, ﬁnite machine. From a con-
temporary perspective, the explicit focus in this paper on how the machine itself
affects mathematics may seem a bit old-fashioned. Indeed, today, there is a ten-
dency towards abstraction up to the development of interfaces which are made as
“user-friendly” as possible, hiding at the same time that one is using a technological
device. In fact, it has become rather common to see computation as something that
transcends the computer itself and can be found anywhere (See e.g. [11]). Evidently,
we are very much in need of such layers of abstraction. Amongst others, they allow
us to overcome the programming bottleneck that characterized the early machines.
However, if the addition of such layers goes hand-in-hand with forgetfulness about
the constraints imposed by the machine or, more generally, the physicality of com-
putation, on our (mathematical) thinking, one also gives away control, not only to
the machine, but also, more importantly, to the developers of these different layers
of abstraction.
The reliance on the computer implies that one needs to develop algorithms that
really work, also, from the machine’s eye and for which, as a consequence, mathe-
matical elegance is constrained by the need for procedures that have to be scientiﬁc
before becoming an art.18 It also means that a new range of problems need to be
faced by the mathematical community, problems that originate in, amongst others,
18 I am referring here to Knuth’s Turing award lecture titled “Computer programming as
an art” in which he reﬂects on the multiple ways in which “science” and “art” can be
used in the context of programming, and beyond. During that lecture he stated: “Science is
knowledge which we understand so well that we can teach it to a computer; and if we don’t
fully understand something, it is an art to deal with it. Since the notion of an algorithm or a
computer program provides us with an extremely useful test for the depth of our knowledge

3
Mathematics and Computer Science
99
the time-based character of computations which has become more explicit through
their execution on a very fast but limited machine; the internalization of procedures
inside of the machine and the fact that part of the work is, literally, out of con-
trol of the human. These problems result in two extreme positions with respect to
computer-assisted explorative mathematics and the more general observation that
mathematical knowledge is becoming too complex to guarantee rigor: on one side,
there are those who argue for the complete formalization of mathematical knowl-
edge and thus “automated rigor”, on the other side, there are those who want the
exact opposite, viz. “automated non-rigor”. Interestingly, as I argued in Sec. 3.1,
one can trace similar oppositions within the short history of computer science it-
self, which, very roughly speaking, boils down to the fact that computer science
is as much about engineering as it is about mathematics.19 The formal veriﬁca-
tion debates, with its high point in the 80s, is perhaps one of the more explicit and
harsh exempliﬁcations of this opposition. However, just as Lehmer’s two styles need
not be exclusive in mathematics, the same holds true for computer science. In fact,
within computer science, both engineering and formalization are so intricately tied
together through the physicality of computation that one can only feel regret when
these two “styles” are regarded as each others opposite. One can only hope that as
computer science matures, that the disciplinary fences that have already been built,
will not result in a complete separation of both styles. This, I believe, would only
result in an impoverishment of the ﬁeld.
References
1. Arden, B.W. (ed.): What can be automated? The computer science and engineering study.
MIT Press (1980)
2. Bonsall, F.F.: A down-to-earth view of mathematics. The American Mathematical
Monthly 89(1), 8–15 (1982)
3. Borwein, J.: Implications of experimental mathematics for the philosophy of mathemat-
ics. In: Gold, B., Simons, R.A. (eds.) Proof and Other Dilemmas: Mathematics and Phi-
losophy, pp. 33–60. Mathematical Association of America (2008)
4. Brady, A.H.: The determination of the value of Rad´o’s noncomputable function σ for
four-state Turing machines. Mathematics of Computation 40(162), 647–665 (1983)
5. Braverman, M., Cook, S.: Computing over the reals: Foundations for scientiﬁc comput-
ing. Notices of the AMS 53(3), 318–329 (2006)
6. Bullynck, M., De Mol, L.: Setting-up early computer programs: D. H. Lehmer’s ENIAC
computation. Archive for Mathematical Logic 49, 123–146 (2010)
7. Conway, J.H., Goodman-Strauss, C., Sloane, N.J.A.: Recent progress in Sphere
Packing. Contemporary Mathematics (to appear), http://comp.uark.edu/
˜strauss/papers/sphere.pdf
about any given subject, the process of going from an art to a science means that we learn
how to automate something.” [25]
19 In fact, it has been argued that there are three styles within computer science: a mathemat-
ical style characterized by theoretical work, an engineering style characterized by design
and a scientiﬁc style characterized by abstraction and modeling [10]. A discussion of this
third style lies beyond the scope of this paper.

100
L. De Mol
8. Clenshaw, C.: Polynomial approximations to elementary functions. Mathematical Tabels
and Other Aids to Computation 8(47), 143–147 (1954)
9. Corry, L.: Number crunching vs. number theory: computers and FLT, from Kummer
to SWAC (1850–1960) and beyond. Archive for the History of Exact Sciences 62,
393–455 (2008)
10. Denning, P.J., Comer, D.E., et al.: Computing as a discipline. Communications of the
ACM 32(1), 9–23 (1989)
11. Denning, P.J.: Computing as a natural science. Communications of the ACM 50(7), 13–
18 (2007)
12. De Mol, L.: The proof is in the process. A preamble for a philosophy of computer-
assisted mathematics. In: Galavotti, M.-C., et al. (eds.) New Directions in the Philosophy
of Science. Series The Philosophy of Science in a European Perspective, vol. 5, pp. 15–
34. Springer
13. Dijkstra, E.W.: On the design of Machine Independent Programming Languages, Report
MR34. Stichting Mathematisch Centrum, Amsterdam (1961)
14. Dijkstra, E.W.: Programming as a discipline of mathematical nature. The American
Mathematical Monthly 81(6), 608–612 (1974)
15. Dijkstra, E.W.: How computing science created a new mathematical style, EWD 1073
(1990)
16. Gandy, R.: The conﬂuence of ideas in 1936. In: Herken, R. (ed.) The Universal Turing
Machine, pp. 55–111. Oxford University Press, Oxford (1988)
17. Goldstine, H.H., von Neumann, J.: Planning and coding of problems for an electronic
computing instrument, vol. 2, part I, II and III (1947-1948), Report prepared for U. S.
Army Ord. Dept. under Contract W-36-034-ORD-7481
18. Gonthier, G.: Formal proof – The four color theorem. Notices of the AMS 55(11), 1382–
1393 (2008)
19. Hales, T., Harrison, J., et al.: A revision of the proof of the Kepler conjecture. Discrete
and Computational Geometry 44, 1–34 (2010)
20. Hamming, R.W.: Impact of Computers. The American Mathematical Monthly 72, 1–7
(1965)
21. Hamming, R.W.: One man’s view of computer science. Journal of the ACM 16, 3–12
(1969)
22. Hartree, D.R.: Calculating instruments and machines. University of Illinois Press (1949)
23. Hilbert, D.: Naturerkennen und Logik. Naturwissen 18, 959–963 (1930)
24. Knuth, D.: Computer science and its relation to mathematics. The American Mathemat-
ical Monthly 81(4), 323–343 (1974)
25. Knuth, D.: Computer programming as an art. Communications of the ACM 17(12), 667–
673 (1974)
26. Knuth, D.: Algorithmic thinking and mathematical thinking. The American Mathemati-
cal Monthly 92(3), 170–181 (1985)
27. Knuth, D.: Algorithmic themes. In: Duren, P., et al. (eds.) A Century of Mathematics in
America, Part I, pp. 439–445. American Mathematical Society (1988)
28. Lehmer, D.H.: Mathematical methods in large scale computing units. In: Proceedings of
Second Symposium on Large-Scale Digital Calculating Machinery, pp. 141–146. Har-
vard University Press, Cambridge (1949, 1951)
29. Lehmer, D.H.: Mechanized mathematics. Bulletin of the American Mathematical Soci-
ety 72, 739–750 (1966)
30. MacKenzie, D.: Slaying the Kraken: The sociohistory of a mathematical proof. Social
Studies of Science 29(1), 7–60 (1999)

3
Mathematics and Computer Science
101
31. Mancosu, P., Zach, R., Badesa, C.: The Development of Mathematical Logic from Rus-
sell to Tarski, 1900 - 1935. In: Haaparanta, L. (ed.) The Development of Modern Logic,
pp. 318–470. Oxford University Press, New York (2009)
32. Margenstern, M.: Comment. In: Zenil, H. (ed.) A Computable Universe, pp. 645–646.
World Scientiﬁc, Singapore (2012)
33. Post, E.: Absolutely unsolvable problems and relatively undecidable propositions —
Account of an anticipation. In: Davis, M. (ed.) The Undecidable. Basic Papers on Un-
decidable Propositions, Unsolvable Problems and Computable Functions, Raven Press,
New York (1965); Corrected Republication, pp. 340–433. Dover Publications, New York
(2004)
34. The QED Manifesto, http://www.cs.ru.nl/˜freek/qed/qed.html
35. Ulam, S., von Neumann, J.: Bulletin of the American Mathematical Society 64, 1–49
(1958)
36. von Neumann, J.: The Mathematician. In: Heywood, R.B. (ed.) The Works of the Mind,
pp. 180–196. University of Chicago Press (1947)
37. von Neumann, J.: Electronic Methods of Computation. Bulletin of the American
Academy of Arts and Sciences 1, 2–4 (1948)
38. Wegner, P.: Research paradigms in computer science. In: Proceedings of the 2nd inter-
national conference on Software Engineering, ICSE 1976, pp. 322–330 (1976)
39. Wolfram, S.: A new kind of science. Wolfram Media, Champaign (2002)
40. Zeilberger, D.: Theorem for a price. Tomorrow’s semi-rigorous mathematical culture.
The Mathematical Intelligencer 16(4), 11–18 (1994)

Chapter 4
Sampling a Two-Way Finite Automaton
Zhe Dang, Oscar H. Ibarra, and Qin Lin
Abstract. We study position sampling in a 2-way nondeterministic ﬁnite automaton
(2NFA) to measure the information dependency and information ﬂow between state
variables, based on the information-theoretic sampling technique proposed in [16].
We prove that for a 2NFA, the language generated by position sampling is regular.
We also show that for a 2NFA, we can effectively ﬁnd a vector of sampling positions
that maximizes dependency and information ﬂow in a run of the 2NFA. Finally, we
give some language properties of sampled runs of 2NFAs augmented with restricted
unbounded storage.
4.1
Introduction and Motivation
One way to understand the behavior of a software system is through observation.
That is, when the system runs, we record a sequence of values of all or part of its
state variables like PC (program counter), variable values, pointer locations, stack
frames, I/O, etc. Such a sequence, called a trace, can later be used for either off-
line or online analysis. A trace contains valuable information on dependency and
information ﬂow among the state variables, where “information ﬂow”, a jargon in
Zhe Dang
School of Computer Science and Technology,
Anhui University of Technology, Ma’anshan, China and School of Electrical Engineering and
Computer Science Washington State University, Pullman, WA 99164, USA
e-mail: zdang@eecs.wsu.edu
Oscar H. Ibarra
Department of Computer Science
University of California, Santa Barbara, CA 93106, USA
e-mail: ibarra@cs.ucsb.edu
Qin Lin
School of Computer Science and Technology,
Anhui University of Technology, Ma’anshan, China
e-mail: qinli@ahut.edu.cn
c⃝Springer International Publishing Switzerland 2015
103
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_4

104
Z. Dang, O.H. Ibarra, and Q. Lin
software security, between state variables means the information, totally [11] or
partially [12,17], ﬂows from secret variables to public ones. When conﬁdentiality is
the concern, this kind of information ﬂow is illegal, and must be constrained.
There have been a number of white-box techniques using static analysis, such as
program slicing [28,30], type analysis [2,23,25], probabilistic analysis [18,20,24],
and information-theoretic analysis [4,12,19], to ﬁnd dependency among variables.
In our recent work [16], we introduce a new information-theoretic technique that
does not depend on static analysis and can be adapted to black-boxes easily as shown
in [16].
There has been a fundamental notion shown below, proposed by Shannon [22]
and later by Chomsky and Miller [3], that computes the information quantity in a
string (word). Let L be a set of words over a given ﬁnite and non-empty alphabet Σ,
and Sn(L) be the number of words of length n in L. The information rate λ L of L is
deﬁned as
λ L = lim logSn(L)
n
.
Where the limit does not exist, we take the upper limit, which always exists for
a ﬁnite alphabet. Throughout this paper, the logarithm is in base 2. Intuitively, λ L
is the average amount of information per symbol contained in a word in L. We
emphasize that the information rate does not require any probabilistic nor statistical
arguments. Instead, it is a characteristic of the language L itself.
Based on the Shannon information rate, we introduced information dependency
and information ﬂow in [16]. We now brieﬂy state the deﬁnitions. Consider a device
M where we observe two of its state variables, x1 and x2, and record a trace, α. M, in
general, is nondeterministic. We use λ x1,x2 to denote the information rate of the lan-
guage of all such traces α. When we replace every value of x2 with a special symbol
−in every α in the language, we obtain a new language with its information rate
denoted by λ x1,−. Similarly, one can deﬁne λ −,x2. Now, information dependency
between x1 and x2 is deﬁned as
D(x1;x2) = λ x1,−+ λ −,x2 −λ x1,x2
λ x1,x2
.
(By default, 0
0 = 0.) Notice that, as in the classic Venn diagram of Shannon infor-
mation, the quantity λ x1,−+ λ−,x2 −λ x1,x2 resembles the “mutual information rate”
between x1 and x2 which characterizes the bit rate that is shared between x1 and x2
in a trace.
In addition to information dependency, one can also deﬁne information ﬂow as
F(x1;x2) = λ x1,−+ λ −,x2 −λ x1,x2
λ −,x2
.
Intuitively, this catches the amount of information that “ﬂows” from x1 to x2 in a
trace; i.e., the mutual information rate λ x1,−+ λ −,x2 −λ x1,x2 is essentially the por-
tion of information rate of x2 that comes from x1. Similarly, one can deﬁne the
information ﬂow from x2 to x1 as

4
Sampling a Two-Way Finite Automaton
105
F(x2;x1) = λ x1,−+ λ −,x2 −λ x1,x2
λ x1,−
.
The above deﬁnitions come from our previous work [16], where information
ﬂow is studied for various one-way automata. In this paper, our focus is on two-
way nondeterministic ﬁnite automata (2NFAs). Notice that, even though 2NFAs
are equivalent to their deterministic version in terms of language acceptance, the
runs of 2NFAs (which are not necessarily regular) are still difﬁcult to handle when
computing their information rate [7]. In this paper, instead of looking at the runs,
we study the sampled executions or traces, where the idea of sampling used in [16]
is generalized. We need the following fundamental result.
Theorem 1. The information rate of a regular language is computable [3].
As pointed out in [3], the information rate can actually be efﬁciently computed us-
ing a matrix algorithm. Recently, we have implemented the algorithm [26] and con-
ﬁrmed the efﬁciency when approximately computing the information rates of some
fairly large C programs [7]. It turns out that the notion itself is useful in software
analysis and testing [5,6,27].
4.2
Information Dependency and Information Flow in 2NFAs
Let M be a 2NFA. On its two-way input tape, M reads the input, say w, and per-
forms state transitions. As usual, M starts from its initial state, and when it enters
an accepting state, w is said to be accepted by M. The state sequence in the state
transition sequence witnessing the acceptance is called an accepting run. Since M is
nondeterministic, there could be more than one accepting run for the given w. We
use L(M) to denote the set of all words w accepted by M and use Lrun,M = {αw : αw
is an accepting run, w ∈L(M)} to denote the set of all accepting runs of M. Clearly,
Lrun,M is not necessarily a regular language on alphabet S (the states in M).
We show an example below of using a 2NFA to specify a security monitoring
system in a building.
Example 1. Suppose that k surveillance cameras are installed in a building. Clearly,
it could be true that all the cameras, no matter where they are installed, cannot
completely cover the entire building. Suppose that a person’s movement inside the
building is modeled as a 2NFA (e.g., every input symbol resembles a door). A nat-
ural question arises: Where shall we install the k cameras so that we can obtain the
maximal amount of information about the person?
In practice, the state space S is often expressed as the Cartesian product of the
state spaces of a number of ﬁnite state variables x1,··· ,xk, for some k. In this
way, every state s in S is a k-arity vector of values. Let A ⊆{x1,··· ,xk} be a set
of variables. Let s ↓A be the result of dropping the components in vector s that
correspond to variables not in A. Therefore, s ↓A is an |A|-arity vector. For a se-
quence α = s0 ···sn, we denote the sequence s0 ↓A ···sn ↓A as α ↓A. When α is

106
Z. Dang, O.H. Ibarra, and Q. Lin
an accepting run, α ↓A is a trace wrt A. Clearly, all such traces form a language
LA
run,M = {α ↓A: α ∈Lrun,M}.
We now give a more general deﬁnition of information dependency and informa-
tion ﬂow [16]. Let L be a set of state sequences and A,B ⊆{x1,··· ,xk} be two sets
of variables. We use λ LA, λ LB and λ LA∪B to denote the information rates of languages
LA (= {α ↓A: α ∈L}), LB, and LA∪B, respectively. The information dependency be-
tween A and B wrt L is deﬁned as
D(A;B|L) = λ LA + λLB −λ LA∪B
λ LA∪B
.
(4.1)
The information ﬂow from A to B is deﬁned as
F(A;B|L) = λ LA + λ LB −λ LA∪B
λ LB
.
(4.2)
In the deﬁnition of a trace wrt A, we require that a trace must record the values
of the variables in A at every step of the run. This is not always possible considering
the fact that the system that M speciﬁes may run at a speed much faster than the
values can be recorded. In practice, it often sufﬁces to sample a trace (e.g., record
the values once every 100 steps), and as a result, a subsequence of a trace is obtained.
We call such a subsequence as a sampled trace. Depending on the scheme used in
sampling, the information dependency and information ﬂow may or may not be
computable. Notice that the information rate on a sampled trace may not be even
a faithful approximation to the information rate on a unsampled trace. Therefore, a
decision problem arises, when given a sampling scheme, whether the scheme would
roughly maintain the bit rate of the traces of the system.
Example 2. We continue with the previous example. Suppose that each camera is
replaced with some sensors (e.g., an infrared sensor for body temperature, a motion
sensor for movement, etc.) to monitor an array of m physical characteristics. Hence,
a “run” now is the person’s behavior sampled from the k sets of sensors. Clearly, if,
from the samples, there is a high dependency between two kinds of sensors, then
it highly suggests that the two kinds of sensors can be reduced into one (i.e., a less
expensive monitoring system is sufﬁcient).
Recall that M works on a two-way input. As pointed out in [7], it is currently
an open problem whether the information rate of the set of all accepting runs of a
2NFA is computable or not, even when the input is unary. The difﬁculty is that all
such runs form a non-regular language whose information rate is known computable
only for limited cases.
We now consider a way to sample a run of a 2NFA M. In the literature, a crossing
sequence is often used for a 2NFA; i.e., to record the state of M whenever the head
is under a given position of the input. We now generalize the concept to multiple
positions. Let c = (c1,··· ,ck), for some k, be a monotonic sequence of rational
numbers with each 0 ≤ci ≤1. Let w be a non-null (i.e., |w| > 0) input to M. The
numbers pi = max(1,⌊ci|w|⌋) are the sampling positions for the input. Notice that,

4
Sampling a Two-Way Finite Automaton
107
when ci = 0 (resp. ci = 1), it is the ﬁrst (resp. last) symbol of the input. When
M runs, whenever the head is at a sampling position, the state of M is recorded.
Therefore, when a run is an accepting run, we may record a sequence of states, called
a c-sampling accepting run. We use Lrun,c(M) to denote the set of all c-sampling
accepting runs for all inputs.
We ﬁrst consider the case when k = 1 and c1 = 0 (hence, only the left end of the
two-way input is sampled). In this case, Lrun,c(M) is written as Lrun,lend(M).
Lemma 1. Let M be a 2NFA. Then, Lrun,lend(M) is a regular language.
Proof. Without loss of generality, we assume that M enters an accepting state and
returns to the left end of the input when it accepts the input. On an input w, we deﬁne
Tw to be the set of all pairs (s,s′) satisfying the following: M starts with state s at
the left end of the input w, runs on the w while the head of M is not under the left
end of the input, and eventually returns to the left end with state s′. Hence, during
the process, the head is at the left end only twice. Since the state set S of M is ﬁnite,
there are only ﬁnitely many distinct Tw’s for all w ∈L(M). We use T to denote all of
them, which can be constructed effectively. This is because, as an exercise, one can
show the following: For each T ∈T , the set {w : Tw = T} is a regular language. Now,
every such T deﬁnes a graph with directed edges (s,s′) ∈T. Clearly, a state sequence
is in Lrun,lend(M) iff, for some T ∈T , the state sequence is a walk on the graph T from
the initial state of M to an accepting state of M. Again, all such walks form a regular
language and the result follows.
⊓⊔
Lemma 1 can be generalized. We ﬁrst need some deﬁnitions.
A counter is a nonnegative integer variable that can be incremented by 1, decre-
mented by 1, or stay unchanged. In addition, a counter can be tested against 0. Let
k be a nonnegative integer. A nondeterministic k-counter machine (NCM) is a one-
way nondeterministic ﬁnite automaton, with input alphabet Σ, augmented with k
counters. For a nonnegative integer r, we use NCM(k,r) to denote the class of k-
counter machines where each counter is r-reversal-bounded; i.e., it makes at most r
alternations between non-decreasing and non-increasing modes in any computation;
e.g., the following counter value sequence
0 0 1 2 2 3 3 2 1 0 0 1 1
is of 2-reversal, where the reversals are underlined. For convenience, we sometimes
refer to a machine M in the class as an NCM(k,r). In particular, when k and r are
implicitly given, we call M as a reversal-bounded NCM. When M is deterministic,
we use ‘D’ in place of ‘N’; e.g., DCM. As usual, L(M) denotes the language that M
accepts.
Reversal-bounded NCMs have been extensively studied since their introduction
in 1978 [14]; many generalizations are identiﬁed, e.g., ones equipped with multiple
tapes, with two-way tapes, with a stack, etc. In particular, reversal-bounded NCMs
have found applications in areas like Alur and Dill’s [1] time-automata [8,9], Paun’s
[21] membrane computing systems [15], and Diophantine equations [29].

108
Z. Dang, O.H. Ibarra, and Q. Lin
Let Y be a ﬁnite set of integer variables. An atomic Presburger formula on Y is
either a linear constraint ∑y∈Y ayy < b, or a modulus constraint x ≡d c, where ay,b,c
and d are integers with 0 ≤c < d. A Presburger formula can always be constructed
from atomic Presburger formulas using ¬ and ∧. Presburger formulas are closed
under quantiﬁcation. Let S be a set of k-tuples in Nk. S is Presburger deﬁnable if
there is a Presburger formula P(y1,··· ,yk) such that the set of nonnegative integer
solutions is exactly S.
Theorem 2. Let M be a 2NFA and c be an array of rational numbers between 0 and
1 (inclusive). Then, Lrun,c(M) is (effectively) a regular language.
Proof. Without loss of generality, we assume that M enters an accepting state and
returns to the right end of the input when it accepts the input. Also without loss of
generality, we assume that c contains the two end points 0 and 1, and there are k
numbers in c. Implicitly, they represent k positions on the input word. Clearly, if
the input word w is long enough, the k sampling positions given by max(1,⌊ci|w|⌋)
must be distinct. Since there are only ﬁnitely many “short” w’s, it sufﬁces for us to
assume that the k sampling positions are indeed distinct. To ease our presentation,
we further assume that these positions are marked; i.e., when originally the input
symbol at such a position is a, in the marked input, the symbol is a marked new
symbol ˙a. By assumption, the two ends of the input symbols are also marked. On a
marked input word, a block is a subword between two neighboring marked symbols
where the two marked symbols are included; e.g., when the input word is ˙abcb ˙dca˙c,
we have two blocks: ˙abcb ˙d and ˙dca˙c. When M works on a marked input word, it
works as if the marked symbols are unmarked. Hence, marked symbols do not alter
the behaviors of M. We now consider a block, say w = ˙au˙b, where ˙a, ˙b are marked
symbols, and u is an unmarked word. Similar to the proof of Lemma 1, we deﬁne
the following sets, shown in Fig. 4.1 and described below.
(a)
(b)
(c)
(d)
Fig. 4.1 Generating regular sets when reading the block ˙au˙b
(a)[s, ˙a,stay,s′] is the set of u such that there is a run starting from state s of M as
follows. When M is reading ˙a of the block at state s, M will come back to this ˙a.
During the run, M works on u two-way. Herein, M’s read head is under ˙a only
twice (the begin and the end of the run) and never moves out of the right of u.
(b)[s, ˙a,right,s′] is the set of u such that there is a run starting from state s of M as
follows. When M is reading ˙a of the block at state s, M does not come back to
this ˙a again, and works on u two-way. The ﬁrst time when M moves out of the u,
the read head is under the marked symbol to the right of u and with state s′.

4
Sampling a Two-Way Finite Automaton
109
(c)[s, ˙b,left,s′] is the set of u such that there is a run starting from state s of M as
follows. When M is reading ˙b of the block at state s, M does not come back to
this ˙b again, and works on the u two-way. The ﬁrst time when M moves out of
the u, the read head is under the marked symbol to the left of u and with state s′.
(d)[s, ˙b,stay,s′] is the set of u such that there is a run starting from state s of M as
follows. When M is reading ˙b of the block at state s, M will come back to this
˙b. During the run, M works on the u two-way. Herein, M’s read head is under ˙b
only twice (the begin and the end of the run) and never moves out of the left of u.
Again, it is an exercise to show that every set (which could be empty) deﬁned
above is regular. We now consider a sequence of k marked symbols, say B = ˙b1··· ˙bk
and a marked word w = ˙b1u1 ···uk˙bk. We construct a table Tw which consists of all
tuples, for all states s,s′, and for all 1 ≤i < k,
• (s, ˙bi,right,s′) if ui ∈[s, ˙bi,right,s′];
• (s, ˙bi,rstay,s′) if ui ∈[s, ˙bi,rstay,s′];
• (s, ˙bi+1,left,s′) if ui ∈[s, ˙bi+1,left,s′];
• (s, ˙bi+1,lstay,s′) if ui ∈[s, ˙bi+1,lstay,s′].
Notice that Tw deﬁnes the transition table for a two-way ﬁnite automaton, also de-
noted by Tw, working on the input B (of ﬁxed length k). The initial and accepting
states of Tw are the same as those in M. Note that, by the construction of Tw,
(*) the accepting runs of M on w, sampled at positions corresponding to the marked
symbols ˙b1,··· , ˙bk, are exactly the accepting runs, denoted by run(Tw), of Tw on
word B.
Also note that there are only ﬁnitely many B’s (since its length is a constant k) and,
by deﬁnition of Tw, there are only ﬁnitely many distinct Tw’s. Using the statement in
(*), we have the following brute-force procedure to compute Lrun,c(M).
We ﬁx a B = ˙b1··· ˙bk and consider a Boolean function B which gives Boolean
values (true or false) B(s, ˙bi,right,s′), B(s, ˙bi,rstay,s′), B(s, ˙bi+1,left,s′), and
B(s, ˙bi+1,lstay,s′), for all 1 ≤i < k and all states s and s′. There are only ﬁnitely
many such distinct Boolean functions. Each such Boolean function B deﬁnes a two-
way ﬁnite automaton TB working on B, whose transitions are exactly those t that
make B(t) true. Clearly, not every B is valid.
More precisely, B is valid wrt B if there is a marked word w = ˙b1u1 ···uk˙bk such
that:
• The marked symbols in w are the sampling positions deﬁned by c. That is, for all
1 ≤i < k,
max(1,⌊ci|w|⌋) = 1 + Σi−1
j=1(|u j|+ 1).
(4.3)
• w is consistent; i.e., for all states s and s′,
– B(s, ˙bi,right,s′) iff ui ∈[s, ˙bi,right,s′];
– B(s, ˙bi,rstay,s′) iff ui ∈[s, ˙bi,rstay,s′];
– B(s, ˙bi+1,left,s′) iff ui ∈[s, ˙bi+1,left,s′];
– B(s, ˙bi+1,lstay,s′) iff ui ∈[s, ˙bi+1,lstay,s′].

110
Z. Dang, O.H. Ibarra, and Q. Lin
Using the aforementioned statement (*), we have
Lrun,c(M) =

B

B is valid
wrt B
run(TB).
(4.4)
Since each run(TB) is regular, the result follows after we show that it is decidable
whether a given B is valid wrt a given B.
We now prove that the decidability indeed holds. Closely looking at the deﬁni-
tion, B is valid wrt B iff the set, denoted by Lc, of w satisfying (4.3) and the set,
denoted by LB, of w being consistent, satisfy
Lc ∩LB ̸= /0.
(4.5)
Notice that the condition in (4.3) is deﬁnable as a Presburger formula on the lengths
of the ui’s in w (where the rational numbers in c are constants such as 3
4). Hence,
it is an exercise to construct a reversal-bounded NCM M′ to accept Lc. Also notice
that LB is a regular language. This is because, as we have mentioned earlier, sets
like [s, ˙bi,right,s′] in deﬁning the language are regular. Hence, the intersection Lc ∩
LB in (4.5) is again accepted by a reversal-bounded NCM denoted by M′′, whose
emptiness is known decidable [14].
⊓⊔
Now, we turn back to the traces of a 2NFA M. In M, the c-sampling information
dependency between A and B is deﬁned as D(A;B|Lrun,c(M)), and the c-sampling
information ﬂow from A to B is deﬁned as F(A;B|Lrun,c(M)). Clearly, from Theorem
2 and Theorem 1, both D(A;B|Lrun,c(M)) and F(A;B|Lrun,c(M)) are computable from
the given c and M.
We now consider the following maximal sampling problem using position
sampling:
Given: a 2NFA M, a variable set A in M, and a number k ≥1.
Goal: Find a k-arity vector c such that the information rate of LA
run,c(M) is maximal
(among all choices of c).
The maximal sampling problem is to ﬁnd a best set of k positions in order to maxi-
mally maintain the bit rate for the given set of (observable) state variables. Similarly,
the maximal information dependency (resp. information ﬂow) problem using posi-
tion sampling is as follows:
Given: a 2NFA M, two disjoint variable sets A and B in M, and a number k ≥1.
Goal: Find a k-arity vector c such that D(A;B|Lrun,c(M))
(resp. F(A;B|Lrun,c(M))) is maximal (among all choices of c).
Intuitively, the goal here is to ﬁnd best input positions (for the given k) in order
to observe the maximal dependency or information ﬂow. All three problems are
solvable.
Theorem 3. For 2NFAs, maximal sampling, maximal information dependency and
maximal information ﬂow problems using position sampling are solvable.
Proof. (sketch.) Let M be a 2NFA. Closely inspecting the proof of Theorem 2,
there are only ﬁnitely many distinct regular languages Lrun,c(M), as shown in (4.4),

4
Sampling a Two-Way Finite Automaton
111
whereas there are inﬁnitely many choices of sampling positions c. The result
follows.
⊓⊔
Observe that the set of sampling positions speciﬁed in c is a linear function of
the input length, while there are no nontrivial relationships between these positions.
We now consider cases where the k sampling positions are constrained. For an in-
put of length n, let 0 ≤p1,··· , pk < n be the k sampling positions on the input. Let
c(p1,··· , pk,n) be a Presburger formula to specify a sampling position constraint
satisfying the following condition that (p1,··· , pk) is a (not necessarily total) func-
tion of n:
For all n, there are no (p1,··· , pk) ̸= (p′
1,··· , p′
k) such that c(p1,··· , pk,n) and
c(p′
1,··· , p′
k,n) hold.
(The condition can be veriﬁed algorithmically for this c since the condition itself is a
closed Presburger formula.) Hence, once the input length is given, the sampling po-
sitions, if they exist, are unique, hence they depend only on the length. For instance,
such a c can be the following constraint
p1 = p2 −p1 = ··· = pk −pk−1 = n −pk −1
indicating that all sampling positions are distributed evenly on the input. For no-
tational convenience, we abuse c as a Presburger formula here so that the previous
deﬁnitions can be reused. For instance, for the 2NFA M, Lrun,c(M) still denotes the set
of all c-sampling (at positions p1,··· , pk satisfying c) accepting runs for all inputs.
We can show Theorem 2 can be generalized.
Theorem 4. Let M be a 2NFA and c be a Presburger sampling position constraint.
Then, Lrun,c(M) is a regular language.
Proof. (sketch.) Let M be a 2NFA. Similar to the proof of Theorem 3, we look
at the proof of Theorem 2: there are only ﬁnitely many distinct regular languages
Lrun,c(M), as shown in (4.4), for inﬁnitely many choices of (rational) sampling posi-
tions c. We use L1,··· ,Lm to denote these languages. In particular, all the c’s that
make Lrun,c(M) = Lq for a given q form a constraint written Q(c). For an input of
length n, let 0 ≤p1,··· , pk < n be the k sampling positions on the input. We use
pi to denote Σi−1
j=1(|u j| + 1) in (4.3) and n to replace |w|, the length of input. It
is not hard to follow from the proof of Theorem 2 that all (p1,··· ,Pk,n) satis-
fying the replaced (4.3) where the rational c satisfy the Q(c) form a Presburger
formula Pq(p1,··· , pk,n). For the given Presburger sampling position constraint
c(p1,··· , pk,n), the resulting Lrun,c(M) is simply the union of all regular languages
Lq with Pq(p1,··· , pk,n)∧c(p1,··· , pk,n) being satisﬁable. The satisﬁability is de-
cidable and the result follows.
⊓⊔
To formulate the maximal sampling problem, we further abuse the Presburger
formula c as follows. Let c(p1,··· , pk,n;t1,··· ,tm) be a Presburger formula, where
p1,··· , pk are sampling positions, and t1,··· ,tm are parameters. This c further satis-
ﬁes a condition such that, for any given t1,··· ,tm, (p1,··· , pk) is a (not necessarily

112
Z. Dang, O.H. Ibarra, and Q. Lin
total) function of n. Again, such a condition can be veriﬁed algorithmically for this
c. Notice that Lrun,c(M) depends on the values of the parameters. Now, we deﬁne the
maximal sampling problem using Presburger position sampling:
Given: a 2NFA M, a variable set A in M, and a Presburger sampling constraint c
with parameters t1,··· ,tm.
Goal: Find values for parameters t1,··· ,tm such that the information rate of
LA
run,c(M) is maximal (among all choices of the values of the parameters).
Similarly, the maximal information dependency (resp. information ﬂow) problem
using Presburger position sampling is as follows:
Given: a 2NFA M, two disjoint variable sets A and B in M, and a Presburger
sampling constraint c with parameters t1,··· ,tm.
Goal: Find values for parameters t1,··· ,tm such that D(A;B|Lrun,c(M)) (resp.
F(A;B|Lrun,c(M))) is maximal (among all choices of the values of the parameters).
Theorem 3 can be generalized.
Theorem 5. For 2NFAs, maximal sampling, maximal information dependency and
maximal information ﬂow problems using Presburger position sampling are
solvable.
Proof. (sketch.) Let M be a 2NFA. From the proof of Theorem 4, for the given
Presburger sampling position constraint c(p1,··· , pk,n;t1,··· ,tm), for each given
(t1,··· ,tm), the resulting Lrun,c(M) is simply the union of all regular languages Lq
with Pq(p1,··· , pk,n)∧c(p1,··· , pk,n;t1,··· ,tm) being satisﬁable. Notice that there
are only ﬁnitely many choices of distinct Lrun,c(M) for all (t1,··· ,tm). The results
follows.
⊓⊔
4.3
Language Properties of Sampled Runs of Some Two-Way
Inﬁnite-State Automata
In this section, we show that position sampling may result in quite complex lan-
guages when the 2NFA is augmented with unbounded storage. We start with ﬁnite
crossing-ﬁnite automata augmented with reversal-bounded counters [14]. Finite-
crossing here means that there is some ﬁxed k such that in every accepting com-
putation on any input, the number of times the input head crosses the boundary
between any two adjacent symbols of the input is at most k. Note that the number
of turns (i.e., changes in direction from left-to-right and right-to-left and vice-versa)
the input head makes on the input may be unbounded. We assume that when we are
given a ﬁnite-crossing machine, the integer k for which the machine is k-crossing is
also speciﬁed.
Let C(M) denote the crossing sequences at crossing location c, where the symbol
at location c is marked (e.g., at the beginning, end, or middle of the input). Notice
that the sampled run at this location is essentially the crossing sequence. We assume
here that the machine M “knows” the sampling position (since it is marked). The
following summarizes our results (the proofs will appear in the journal version of
this paper):

4
Sampling a Two-Way Finite Automaton
113
• It is decidable, given a two-way DFA with one reversal-bounded counter (no re-
striction on the the two-way input) M and a regular language L, whetherC(M)∩L
is empty.
• It is undecidable, given a two-way DFA with two reversal-bounded counters
(no restriction on the the two-way input) M and a regular language L, whether
C(M)∩L is empty.
• It is decidable, given a ﬁnite-crossing two-way NFA with multiple reversal-
bounded counters M and a regular language L, whether C(M)∩L is empty.
We now consider the case where the marked symbols on an input are unbounded.
Let Σ1 be an alphabet and Σ2 = {a′ | a ∈Σ1}. Thus, the symbols in Σ2 are the
“marked” symbols in Σ1. Let Σ = Σ1 ∪Σ2.
Let M be a two-way acceptor (with end markers) possibly with inﬁnite storage
over the input alphabet Σ. We assume that M on input w in Σ “ignores” the marks,
i.e., treat symbols a and a′ in the same way.
Let Mc be another acceptor over Σ (one-way or two-way, possibly with inﬁnite
storage) over Σ. This time in its computation, Mc distinguishes the marked symbols
from the unmarked symbols. Thus, e.g., on input w, Mc accepts if every other sym-
bol is marked (or the number of marked symbols is even; etc.). Intuitively, this Mc
speciﬁes where the marked symbols are on an input to M.
Let L be a language over sequences of states of M (speciﬁed by another acceptor
ML ). Let C(M) be the set of crossing sequences of states that are encountered when
M crosses a marked symbol on the input.
We can show the following: It is decidable, given a ﬁnite-crossing two-way NFA
with reversal-bounded counters M, a two-way ﬁnite-crossing NFA with reversal-
bounded counters Mc, and an NPDA ML with reversal-bounded counters accepting
L, whether C(M) ∩L is empty. (The result also holds when ML is a ﬁnite-crossing
two-way NFA with reversal-bounded counters.)
A special case of the above is the following, which can be simply observed. If M
is a one-way NFA and Mc (which speciﬁes the “marked” locations) is an NFA, then
C(M) can be accepted by a one-way NFA (and hence it is regular).
There is a well-studied acceptor, called a checking stack automaton (CSA), in-
troduced by Ginsburg et al. [10]. A CSA is a one-way NFA with a special stack.
The special stack is like a pushdown stack: it can write but not pop, but it can enter
the stack in a two-way read-only mode. However, once it enters the stack, it can no
longer write, but it can continue using the stack in a read-only mode. Recall that, for
a 2NFA M , the set (language) Lrun,M is not necessarily regular. We can show that
Lrun,M is accepted by a CSA.
Finally, consider the case when there are at most k crossing positions, for a con-
stant k. That is, every marked input accepted by Mc contains at most k marked
symbols. We can show the following results:
• If M is a two-way NFA and Mc is an NFA, then C(M) is accepted by a two-way
NFA which can be converted to a one-way NFA, so it is also regular.

114
Z. Dang, O.H. Ibarra, and Q. Lin
• If M is a one-way NCM (i.e., NFA augmented with 1-reversal counters) and Mc
is an NPDA with reversal-bounded counters, then C(M) is accepted by an NPDA
with reverse-bounded counters.
• If M and Mc are both ﬁnite-crossing NCMs, then C(M) is accepted by a ﬁnite-
crossing NCM, hence it can also be accepted by a one-way NCM [13].
4.4
Conclusions
In this paper, we have studied position sampling in a 2-way nondeterministic ﬁnite
automaton (2NFA) to measure the information dependency and information ﬂow
between state variables, based on the information-theoretic sampling technique pro-
posed in [16]. We have proved that for a 2NFA, the language generated by position
sampling is regular. We have also showed that for a 2NFA, we can effectively ﬁnd a
vector of sampling positions that maximizes dependency and information ﬂow in a
run of the 2NFA. Finally, we have summarized some language properties of sampled
runs of 2NFAs augmented with restricted unbounded storage.
In the future, we will conduct experiments and show the practical usefulness of
information rates resulting from sampling a two-way automaton.
Acknowledgment. We would like to thank Eric Wang for comments that improved
the presentation of our results.
References
1. Alur, R., Dill, D.L.: A theory of timed automata. Theoretical Computer Science 126(2),
183–235 (1994)
2. Cavadini, S.: Secure slices of insecure programs. In: Proceedings of the 2008 ACM
Symposium on Information, Computer and Communications Security, ASIACCS 2008,
pp. 112–122 (2008)
3. Chomsky, N., Miller, G.A.: Finite state languages. Information and Control 1, 91–112
(1958)
4. Clark, D., Hunt, S., Malacaria, P.: A static analysis for quantifying information ﬂow in a
simple imperative language. Journal of Computer Security 15 (2007)
5. Cui, C., Dang, Z.: Fischer: Bit rate of programs (submitted)
6. Cui, C., Dang, Z., Fischer, T., Ibarra, O.: Similarity in languages and programs. Theoret-
ical Computer Science 498, 58–75 (2013)
7. Cui, C., Dang, Z., Fischer, T.R., Ibarra, O.H.: Execution information rate for some classes
of automata. In: Dediu, A.-H., Mart´ın-Vide, C., Truthe, B. (eds.) LATA 2013. LNCS,
vol. 7810, pp. 226–237. Springer, Heidelberg (2013)
8. Dang, Z.: Pushdown timed automata: a binary reachability characterization and safety
veriﬁcation. Theoretical Computer Science 302(1-3), 93–121 (2003)
9. Dang, Z., Ibarra, O., Bultan, T., Kemmerer, R., Su, J.: Binary reachability analysis of
discrete pushdown timed automata. In: Emerson, E.A., Sistla, A.P. (eds.) CAV 2000.
LNCS, vol. 1855, pp. 69–84. Springer, Heidelberg (2000)
10. Ginsburg, S., Greibach, S.A., Harrison, M.A.: One-way stack automata. Journal of the
ACM (JACM) 14(2), 389–418 (1967)

4
Sampling a Two-Way Finite Automaton
115
11. Goguen, J.A., Meseguer, J.: Security Policies and Security Models. In: IEEE Symposium
on Security and Privacy, pp. 11–20 (1982)
12. Gray I, J.W.: Toward a mathematical foundation for information ﬂow security. In: Pro-
ceedings of the 1991 IEEE Computer Society Symposium on Research in Security and
Privacy, pp. 21–34 (May 1991)
13. Gurari, E.M., Ibarra, O.H.: The complexity of decision problems for ﬁnite-turn multi-
counter machines. Journal of Computer and System Sciences 22(2), 220–229 (1981)
14. Ibarra, O.H.: Reversal-bounded multicounter machines and their decision problems. J.
ACM 25(1), 116–133 (1978)
15. Ibarra, O.H., Dang, Z., Egecioglu, O., Saxena, G.: Characterizations of catalytic mem-
brane computing systems. In: Rovan, B., Vojt´aˇs, P. (eds.) MFCS 2003. LNCS, vol. 2747,
pp. 480–489. Springer, Heidelberg (2003)
16. Li, Q., Dang, Z.: Sampling automata and programs (submitted)
17. McLean, J.: Security models and information ﬂow. In: Proceedings of the 1990 IEEE
Computer Society Symposium on Research in Security and Privacy, pp. 180–187 (May
1990)
18. McCamant, S., Ernst, M.D.: Quantitative information ﬂow as network ﬂow capacity.
SIGPLAN Not. 43(6), 193–205 (2008)
19. McIver, A., Morgan, C.: A probabilistic approach to information hiding. In: McIver,
A., Morgan, C. (eds.) Programming Methodology. Monographs in Computer Science,
pp. 441–460. Springer, New York (2003)
20. Millen, J.K.: Covert channel capacity. In: IEEE Symposium on Security and Privacy,
Oakland, CA, pp. 60–66 (1987)
21. Paun, G.: Membrane Computing, an Introduction. Springer, Heidelberg (2000)
22. Shannon, C.E., Weaver, W.: The Mathematical Theory of Communication. University of
Illinois Press (1949)
23. Smith, G., Volpano, D.: Secure information ﬂow in a multi-threaded imperative language.
In: Proceedings of the 25th ACM SIGPLAN-SIGACT Symposium on Principles of Pro-
gramming Languages, POPL 1998, pp. 355–364. ACM, New York (1998)
24. Smith, G.: On the foundations of quantitative information ﬂow. In: de Alfaro, L. (ed.)
FOSSACS 2009. LNCS, vol. 5504, pp. 288–302. Springer, Heidelberg (2009)
25. Terauchi, T.: A type system for observational determinism. In: IEEE 21st Computer Se-
curity Foundations Symposium, CSF 2008, pp. 287–300 (June 2008)
26. Yang, L., Cui, C., Dang, Z., Fischer, T.R.: An information-theoretic complexity metric
for labeled graphs (submitted)
27. Wang, E., Cui, C., Dang, Z., Fischer, T.R., Yang, L.: Zero-knowledge blackbox testing:
where are the faults? International Journal of Foundations of Computer Science (to ap-
pear)
28. Weiser, M.: Program Slicing. IEEE Transactions on Software Engineering 10, 352–357
(1984)
29. Xie, G., Dang, Z., Ibarra, O.H.: A solvable class of quadratic diophantine equations with
applications to veriﬁcation of inﬁnite-state systems. In: Baeten, J.C.M., Lenstra, J.K.,
Parrow, J., Woeginger, G.J. (eds.) ICALP 2003. LNCS, vol. 2719, pp. 668–680. Springer,
Heidelberg (2003)
30. Xu, B., Qian, J., Zhang, X., Wu, Z., Chen, L.: A brief survey of program slicing. ACM
Sigsoft Software Engineering Notes 30, 1–36 (2005)

Chapter 5
Maurice Margenstern’s Contributions to the
Field of Small Universal Turing Machines
Turlough Neary⋆and Damien Woods⋆⋆
Abstract. On the occasion of his 65th birthday we survey some of the work of Mau-
rice Margenstern on small universal Turing machines. Margenstern has been one of
the most proliﬁc contributors to this ﬁeld, having constructed numerous small uni-
versal programs for a number of Turing machine models. These positive results are
complemented by Margenstern’s negative results, or lower bounds, on universal pro-
gram size. Finally, he has even explored the space in-between the known program
size upper and lower bounds by giving small programs that iterate the Collatz func-
tion, which suggests that proving negative results on programs of this size will be at
least as hard as resolving the Collatz problem.
5.1
Introduction
The topic of small universal Turing machines is concerned with putting upper and
lower bounds on the program size of universal Turing machines. In other words,
ﬁnding the shortest programs that are capable of universal computation and show-
ing that below this threshold such complicated behaviour is impossible. More gen-
erally, the computational complexity of small programs includes investigating the
time, space and encoding complexity costs we pay by having tiny general-purpose
programs. Simulation of small universal Turing machines and other simple univer-
Turlough Neary
Institute for Neuroinformatics, University of Z¨urich and ETH Z¨urich, Switzerland
e-mail: tneary@ini.phys.ethz.ch
Damien Woods
California Institute of Technology, Pasadena, CA 91125, USA
e-mail: woods@caltech.edu
⋆Supported by Swiss National Science Foundation grant 200021-141029.
⋆⋆Supported by the US National Aeronautics and Space Administration (NASA) grant
NNX13AJ56G, and National Science Foundation (NSF) grants 0832824 & 1317694 (The
Molecular Programming Project), CCF-1219274 and CCF-1162589.
c⃝Springer International Publishing Switzerland 2015
117
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_5

118
T. Neary and D. Woods
sal models such as Post’s tag systems [36] and the cellular automaton rule 110 [4] is
by now a standard way to prove that a large number of other models of computation,
including a variety of physically-inspired systems, are computationally universal.
Maurice Margenstern has been one of the most proliﬁc authors in the ﬁeld. He
has constructed numerous small universal Turing machines, for a variety of Turing
machine models, and has complemented this by giving lower bounds on the size of
universal Turing machine programs. As Figure 5.1 shows, although we have upper
and lower bounds on universal program size there remain a large number of inter-
mediate program sizes for which we don’t know whether or not there are universal
programs. Margenstern introduced an interesting way to explore this space of pro-
grams by giving intermediate-size programs that simulate iterations of the Collatz
function [8]. It follows that solving many questions about the long term dynamics
of these machines is at least as hard as resolving the Collatz problem.
Margenstern has been a great proponent and organiser for the ﬁeld, mainly by his
technical contributions and analysis of novel syntactical program properties other
than size, but also by other means: his survey [20] was an excellent introduction to
the ﬁeld for us, and the international conference series he started, Machine Com-
putations and Universality, has been a welcoming home for new results. In this
short note we highlight a small selection of Margenstern’s work. More of his and
others’ results can be found in a number of surveys, including those by Margen-
stern [17,19,20,25,26,32].
In Section 5.2 we discuss Margenstern’s small Collatz simulators and where they
lie in relation to the best-known upper and lower bounds on universal program size
for the standard Turing machine model. In Section 5.3 we give Margenstern’s up-
per and lower bounds results for a number of Turing machine models. Finally, in
Section 5.4 we discuss some of Margenstern’s universal machines for restrictions of
the standard Turing machine model and compare his algorithm for simulating 2-tag
systems to the original algorithm introduced by Minsky [28].
5.2
Simulating the Collatz Function with Small Turing
Machines
Historically much of the research into small universal Turing machines is concerned
with the deterministic single-tape model with the usual notion of blank symbol. We
often refer to this as the standard model. The size of a program is deﬁned as the
number of its states and symbols written as a (state, symbol) pair. Their product
gives an upper bound on the number of instructions in the program.
For the standard model, the smallest known Turing machines were given by Ro-
gozhin [38] ((2,18), (4,6), and (5,5)), Kudlek and Rogozhin [7] (3,9), and Neary
and Woods [31] ((5,5), (6,4), (9,3) and (15,2)). These machines are plotted as
circles in Figure 5.1. The halting problem has been shown to be decidable for the
following state-symbol pairs: (2,2) [6, 33], (3,2) [35], (2,3) (Pavlotskaya, unpub-
lished), (1,n) [5] and (n,1) (trivial) for n ⩾1. Thus there are no universal machines

5
Maurice Margenstern’s Contributions
119
 : Margenstern’s Collatz machines
 
: Baiocchi’s Collatz machines

: smallest known universal machines
universal curve
non-universal curve
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
states
symbols
 
 
 
     






Fig. 5.1 State-symbol plot of the smallest known universal Turing machines for the standard
model. These machines induce the universal curve. The non-universal curve shows Turing
machines that are known to have a decidable halting problem. Margenstern’s machines that
simulate iterations of the Collatz function are shown as solid diamonds.
of this size that have the property of halting exactly when the simulated Turing ma-
chine halts. These results are plotted as the “non-universal” curve in Figure 5.1.
The space between the universal and non-universal curves in Figure 5.1 contains
39 state-symbols pairs for which the universality/non-universality question remains
open. Margenstern [19, 20] found an interesting way to explore this space of open
questions. He constructed machines with state-symbol pairs of (11,2), (5,3), (4,4),
(3,6) and (2,10) that simulate iterations of the Collatz function and are shown as
solid diamonds in Figure 5.1. The Collatz function [8] is
f(x) =

3x+ 1
if x = 1 mod 2,
x/2
if x = 0 mod 2.
(5.1)
Associated with this function is the well-known question: for each x ∈N is there
an n such that f n(x) = 1? The answer is conjectured to be yes. The question has
drawn the interest of many authors and has resisted all attempts at a solution [8–10].
So for the class of machines with program size equal (or greater than) Margenstern’s
Collatz function simulators the following problem is at least as difﬁcult as solving
the Collatz conjecture: give an algorithm that takes one of these machines, an initial
conﬁguration and a target conﬁguration as input, and decides whether or not the
machine reaches the target from the initial conﬁguration.

120
T. Neary and D. Woods
Baiocchi [1] reduced the size of some of these Collatz simulators to give Turing
machines with state-symbol pairs of (10,2), (5,3), (4,4), (3,5) and (2,8). All of
Baiocchi’s machines and the (4,4), (3,6) and (2,10) machines of Margenstern use
a unary encoding and so simulate a single iteration of the Collatz function on x in
time O(x). The (11,2) and (5,3) machines of Margenstern use a binary encoding
and so simulate a single iteration of the Collatz function x in time O(logx). It would
be of interest to see if Margenstern’s efﬁcient encoding could be extended to other
state-symbol pairs as a way to explore possible trade-offs between program size and
time/space efﬁciency, even for these non-universal machines. For more on research
in this direction see [30,32,40].
5.3
Frontiers between Universality and Non-universality
Margenstern has found matching upper and lower bounds (which he calls a frontier)
on various syntactic properties of universal 2-symbol Turing machine programs. So
although we are currently unable to ﬁnd matching upper and lower bounds on the
particular syntactic property of universal program size, his work shows us that there
are some syntactic properties for which we can ﬁnd such frontiers.
One such property is the number of colours of a 2-symbol machine, deﬁned as
the number of distinct triples (α,D,δ), where α is the read symbol, D is the move
direction, and δ is the write symbol of a transition rule. Pavlotskaya [33, 34] has
shown that there are standard universal Turing machines with 3 colours and no stan-
dard universal Turing machines with 2 colours. Margenstern [13] has shown that
there are non-erasing universal Turing machines with 5 colours and no non-erasing
universal Turing machines with 4 colours.
Laterality number is another property examined by Margenstern and is deﬁned as
the minimum of the number of left-move instructions and the number of right-move
instructions of a 2-symbol Turing machine. Margenstern and Pavlotskaya [22, 33]
have shown that there are universal Turing machines with laterality number 2 and no
universal machines with laterality number 1. Margenstern [15, 18] has shown that
there are universal non-erasing Turing machines with laterality number 3 and no
universal non-erasing machines with laterality number 2. For more on these results
see [11,13–16,21].
So far in this section we have considered Turing machines that are restrictions on
the standard model. Margenstern and Pavlotskaya [24] have also considered a gener-
alisation of the standard model that consists of a (Turing machine, ﬁnite automaton)
pair. Their machine operates as follows: The Turing machine has a one-way inﬁnite
tape and operates in the usual way until it moves onto a tape cell that is blank, i.e. a
cell that did not contain a symbol from the initial input and has not previously been
visited by the tape head. When it moves onto the blank cell a symbol is written to
that cell by the ﬁnite automaton that depends on the current states of both the au-
tomaton and Turing machine. After a symbol is written by the ﬁnite automaton the
Turing machine continues to compute in the normal manner until the next blank cell

5
Maurice Margenstern’s Contributions
121
is entered. Margenstern and Pavlotskaya [23, 24] gave a 2-state, 3-symbol Turing
machine that uses only 5 instructions and is universal when coupled with a ﬁnite au-
tomaton as described above. They also showed, via a 58-page proof, that the halting
problem is decidable for such machines with 4 instructions [24], giving us another
frontier.
5.4
Restricted Turing Machines
We can suitably restrict the standard Turing machine model so that the problem of
ﬁnding universal machines with small state-symbol products becomes increasingly
difﬁcult or even impossible. For example, Margenstern [13, 16, 18, 21] and others
before him [27,29,39,41] have looked at non-erasing Turing machines, that is ma-
chines that are permitted to only overwrite blank symbols, other symbols can not
be changed. As discussed in Section 5.3, Margenstern has also given universal ma-
chines for other restrictions of the standard model and in Section 5.4.1 we look at
details of the simulation algorithms used by some of these machines.
5.4.1
Margenstern’s Tag System Simulation Algorithms
Like many authors, Margenstern used the technique of 2-tag system simulation in
some of his small universal Turing machines. Tag systems where introduced by
Post [36], and 2-tag systems were shown to be universal by Cocke and Minsky [3].
Minsky [28] constructed a 7-state, 4-symbol universal Turing machine that simu-
lates 2-tag systems and his machine’s simulation algorithm was the inspiration for
many of the small universal machines to follow [2, 7, 37, 38]. Below, we introduce
2-tag systems and describe Minsky’s simulation algorithm, and we then discuss why
Margenstern had to come up with a completely new simulation algorithm for his re-
stricted machine model. This is followed by an overview of the operation of Margen-
stern’s 2-symbol, 59-state machine with 6 left-move instructions, and his 2-symbol,
190-state machine with 3 left-move instructions, both of which simulate 2-tag sys-
tems. Margenstern gives an overview of these machines in [21], with more details
in [12,14,16].
Deﬁnition 1 (2-tag system). A tag system consists of a ﬁnite alphabet of symbols
Σ = {σ1,σ2,...σl}, a ﬁnite set of rules of the form σ →P(σ) with P(σ) ∈Σ∗, and
a deletion number β ∈N, β ⩾1. For a 2-tag system, β = 2.
A 2-tag system acts on a dataword w = σi1σi2 ...σin. The entire conﬁguration is
given by w. In a computation step, the ﬁrst two symbols σi1σi2 of w are deleted and
the rule σi1 →P(σi1) is applied by appending the word P(σi1) to the right of w:
σi1σi2σi3 ...σin ⊢σi3 ...σinP(σi1)

122
T. Neary and D. Woods
(i) Stage 1
tape head-
⟨P(σl)⟩··· b⟨P(σ2)⟩b⟨P(σ1)⟩beedeeeded
?
(ii) Stage 2
⟨P(σl)⟩··· b⟨P(σ2)⟩b/ ⟨P(σ1)⟩b/ e/e/d/eeeded
(iii) Stage 3
⟨P(σl)⟩··· b⟨P(σ2)⟩b/⟨P(σ1)⟩b/ e/e/d/e/e/e/d/ed ⟨P(σ2)⟩
(iv) End of Stage 3
⟨P(σl)⟩··· b⟨P(σ2)⟩b⟨P(σ1)⟩be/e/d/e/e/e/d/ed ⟨P(σ2)⟩
?
Fig. 5.2 An overview of Minsky’s Turing machine algorithm [28] to simulate an arbitrary
step of a tag system σ2σ 3σ1 ⊢σ1P(σ2) where σ2 is read, σ2σ3 is deleted, production P(σ2)
is appended and σi is encoded in unary as the string eid. (i) Initial conﬁguration of Stage 1
(from here, locate production P(σ2) by marking off, and thus deleting, the ﬁrst tag system
symbol σ2 encoded as eed, as well as 2 b symbols). (ii) Initial conﬁguration of Stage 2 (print
production P(σ2) and delete the second encoded tag system symbol σ3 encoded as eeed).
(iii) Initial conﬁguration of Stage 3 (unmark the 2 b symbols that were used to index the
production P(σ2)). (iv) Simulation of tag system step complete: two encoded symbols have
been deleted, a new one appended, and we are reading the symbol σ1 encoded as ed.
Figure 5.2 gives an overview of a simple algorithm for simulating a sin-
gle tag system step σ2σ3σ1 ⊢σ1P(σ2). This was introduced by Minsky [28]
and lends itself well to implementation via small programs. Figure 5.2(i) gives
the initial Turing machine tape contents. On the left side of Turing machine
tape the word ⟨P(σ l)⟩··· b⟨P(σ2)⟩b⟨P(σ1)⟩b encodes the list of productions
P(σl)···P(σ2)P(σ1), where ⟨P(σi)⟩is the encoding of P(σi) and b is a special
marker symbol that separates adjacent productions. On the right side of the tape is
the word eedeeeded which encodes the dataword σ2σ3σ1, where each symbol σi
is encoded as the sequence of Turing machine tape symbols eid.
The algorithm in Figure 5.2 has 3 stages. Stage 1 begins with the conﬁguration
shown in Figure 5.2(i) and uses the encoding eed of the leftmost symbol σ2 as a
unary index to locate the encoded production ⟨P(σ2)⟩. During this stage the tape
head repeatedly scans left and right marking off one b marker for each e in the
encoding of σ2. Stage 2 simulates the deletion of the second symbol σ3 by marking
off eeed, and then repeatedly scans left and right copying the encoded production
⟨P(σ2)⟩and printing it at the right end of the encoded dataword. Finally, Stage 3
unmarks the list of encoded productions.
Even though the algorithm given in Figure 5.2 for simulating 2-tag systems al-
lows for the construction of some of the smallest known universal Turing machines,
it still uses left and right scans of the tape in a number of different contexts (which
costs states and symbols). During Stage 1 the head scans left to mark off b’s, during
Stage 2 it scans left to copy the symbols of ⟨P(σ)⟩to be printed at the right end of
the tape, and ﬁnally in Stage 3 it scans left to restore (unmark) the original encod-
ing of the tag system program. In addition to this, the information in e, d, b, b/ and
⟨P(σ)⟩words must not be destroyed during these leftward scans which adds to the
number of left-moving transition rules required. On a 2-symbol Turing machine one
would expect the number of left-move instruction to further increase as e, d, b and b/
would be encoded as binary words. The smallest known 2-symbol universal Turing

5
Maurice Margenstern’s Contributions
123
(i) Stage 1
tape head

000b⟨P(σ 1)⟩b⟨P(σ 2)⟩b ···⟨P(σ l)⟩beedeeeded
?
(ii) Stage 2
111b/⟨P(σ1)⟩b/⟨P(σ 2)⟩b ···⟨P(σl)⟩be/e/d/e/e/e/d/ed
(iii) Stage 3
000b/⟨P(σ1)⟩b/⟨P(σ2)⟩b ···⟨P(σl)⟩be/e/d/e/e/e/d/ed⟨P(σ2)⟩
?
(iv) End of Stage 3
000b⟨P(σ1)⟩b⟨P(σ2)⟩b ···⟨P(σl)⟩be/e/d/e/e/e/d/ed⟨P(σ2)⟩
?
Fig. 5.3 Margenstern’s algorithm to simulate an arbitrary tag system step σ 2σ3σ1 ⊢
σ1P(σ2), see main text for details.
machine [31] uses 15 left-move instructions. Surprisingly, Margenstern’s managed
to give 2-symbol universal Turing machines with very few left-move instructions: a
Turing machine with 59 states and only 6 left-move instructions [21], another ma-
chine with 190 states and only 3 left-move instructions [12, 21], and a 218-state
non-erasing machine with only 3 left-move instructions [16,21]. Margenstern found
some elegant techniques to overcome a limited ability to carry information to the
left.
For Margenstern’s [21] 2-symbol, 59-state machine that uses 6 left-move instruc-
tions a simulated computation step begins with a tape of the form shown in Fig-
ure 5.3(i). Note that the order of the encoded productions in Figure 5.3 is the reverse
of that in Figure 5.2. Also, there is a special stage bit (bold 0) at the left end of the
tape. This stage bit is used to record which of Stages 1 or 2 is currently being exe-
cuted. The 6 left-move instructions are split into two disjoint sets of 3 instructions.
The ﬁrst set scans left when the stage bit should remain unchanged and the second
set scans left when the stage bit should change. Beginning with the initial conﬁgura-
tion shown in Figure 5.3(i) the tape head scans right and marks off the leftmost b on
the tape and continues its scan right (leaving all subsequent b markers unchanged)
until it marks off the leftmost e on the tape. It then scans left in the set of left-move
rules that leaves the stage bit unchanged. Following this, there is another scan to the
right to mark off another b and another e. This is repeated until a d is read during
a scan right signaling that it has ﬁnished reading the leftmost encoded symbol in
the encoded dataword. In this case it continues its scan right and deletes the next
encoded tag system symbol, before scanning left in the set of states that change the
stage bit (in this case from 0 to 1) causing the next stage (Stage 2) of the algorithm
to begin. In Stage 2 repeated rightward and leftward scans copy the encoded pro-
duction ⟨P(σ2)⟩, which was located during Stage 1, to the right end of the encoded
dataword. Each scan left occurs in the set of states that signal no change in the stage
bit until the copying process is ﬁnished. After the encoded production has been en-
tirely copied, it scans left in the set of states that signal a ﬂip in the stage bit from
1 to 0. We are now in Stage 3, which ignores the stage bit: the machine carries out
a single scan right to restore the encoded tag system program to its original value,
then scans left in the set of sets that signal no change in the stage bit. The simulation

124
T. Neary and D. Woods
of the computation step is complete and the machine goes to Stage 1. The machine
is now ready to begin simulation of the next tag system step.
Margenstern used a similar algorithm in his 2-symbol, 190-state machine with 3
left-move instructions. However, the previous technique used for updating the stage
bit cannot be used here. The reason for this is that with only 3 left-move instructions,
in this particular setup, we can send only 1 type of signal to the left. To overcome
this problem Margenstern’s machine repeatedly copies the entire conﬁguration to
the right of its current location. Before a conﬁguration is copied there are two words
found at either end of the conﬁguration, each of which records which stage is being
executed (similar to the stage bit from above). When simulating that the stage bit is
0 the word at the left end is u and the word at the right end word is v, and when sim-
ulating that the stage bit is 1 the word at the left end is c and the word at the right end
is k. After copying the entire conﬁguration to the right of the rightmost word (v or k),
this rightmost word is then positioned at left end of the newly copied conﬁguration.
During this process this word (v or k) is altered to become the new leftmost word (u
or c) that encodes the stage. Note that as the conﬁguration is copied to the right all
of the information in the conﬁguration is carried over what was originally the right-
most word and this allows it to be updated to record whether or not the encoding of
the stage bit should change. Margenstern introduces some other techniques in this
variant of his algorithm (such as the use of three simulated tape heads), which we
do not discuss here. Margenstern’s [21] non-erasing machine with 2 symbols, 218
states and only 3 left-move instructions uses a similar algorithm.
References
1. Baiocchi, C.: 3n+1, UTM e tag-system. Tech. Rep. Pubblicazione 98/38, Dipartimento
di Matematico, Universit`a di Roma (1998) (in Italian)
2. Baiocchi, C.: Three small universal Turing machines. In: Margenstern, M., Rogozhin, Y.
(eds.) MCU 2001. LNCS, vol. 2055, pp. 1–10. Springer, Heidelberg (2001)
3. Cocke, J., Minsky, M.: Universality of tag systems with P = 2. Journal of the Association
for Computing Machinery (ACM) 11(1), 15–20 (1964)
4. Cook, M.: Universality in elementary cellular automata. Complex Systems 15(1), 1–40
(2004)
5. Hermann, G.T.: The uniform halting problem for generalized one state Turing machines.
In: Proceedings of the Ninth Annual Symposium on Switching and Automata Theory
(FOCS), pp. 368–372. IEEE Computer Society Press, Schenectady (1968)
6. Kudlek, M.: Small deterministic Turing machines. Theoretical Computer Science 168(2),
241–255 (1996)
7. Kudlek, M., Rogozhin, Y.: A universal Turing machine with 3 states and 9 sym-
bols. In: Kuich, W., Rozenberg, G., Salomaa, A. (eds.) DLT 2001. LNCS, vol. 2295,
pp. 311–318. Springer, Heidelberg (2002)
8. Lagarias, J.C.: The 3x + 1 problem and its generalizations. American Mathematical
Monthly, 3–23 (1985)
9. Lagarias, J.C.: The 3x+1 problem: An annotated bibliography (1963–1999). Tech. Rep.
arXiv:math/0309224 [math.NT] (2003)
10. Lagarias, J.C.: The 3x + 1 problem: An annotated bibliography, II (2000-2009). Tech.
Rep. arXiv:math/0608208 [math.NT] (2006)

5
Maurice Margenstern’s Contributions
125
11. Margenstern, M.: Sur la fronti`ere entre machines de Turing ´a arrˆet d´ecidable et machines
de Turing universelles. Tech. Rep. 92.83, LITP Institut Blaise Pascal (1992)
12. Margenstern, M.: Machine de Turing universelle sur {0,1}, `a trois instructions gauches.
Tech. Rep. 93.36, LITP Institut Blaise Pascal (1993)
13. Margenstern, M.: Non-erasing Turing machines: A frontier between a decidable halting
problem and universality. In: ´Esik, Z. (ed.) FCT 1993. LNCS, vol. 710, pp. 375–385.
Springer, Heidelberg (1993)
14. Margenstern, M.: Une machine de Turing universelle sur {0,1}, non-effac¸ante et `a trois
instructions gauches. Tech. Rep. 94.08, LITP Institut Blaise Pascal (1994)
15. Margenstern, M.: Non-erasing Turing machines: A new frontier between a decidable
halting problem and universality. In: Baeza-Yates, R.A., Goles, E., Poblete, P.V. (eds.)
LATIN 1995. LNCS, vol. 911, pp. 386–397. Springer, Heidelberg (1995)
16. Margenstern, M.: Une machine de Turing universelle non-effac¸ante `a exactement trois
instructions gauches. CRAS, Paris 320(I), 101–106 (1995)
17. Margenstern, M.: Decidability and undecidability of the halting problem on Turing
machines, a survey. In: Adian, S., Nerode, A. (eds.) LFCS 1997. LNCS, vol. 1234,
pp. 226–236. Springer, Heidelberg (1997)
18. Margenstern, M.: The laterality problem for non-erasing Turing machines on {0,1} is
completely solved. Theoretical Informatics and Applications 31(2), 159–204 (1997)
19. Margenstern, M.: Frontier between decidability and undecidability: a survey. In:
Margenstern, M. (ed.) Machines, Computations, and Universality (MCU), vol. 1,
pp. 141–177. IUT, Metz (1998)
20. Margenstern, M.: Frontier between decidability and undecidability: a survey. Theoretical
Computer Science 231(2), 217–251 (2000)
21. Margenstern, M.: On quasi-unilateral universal Turing machines. Theoretical Computer
Science 257(1-2), 153–166 (2001)
22. Margenstern, M., Pavlotskaya, L.: Deux machines de Turing universelles ´a au plus deux
instructions gauches. CRAS, Paris 320(I), 1395–1400 (1995)
23. Margenstern, M., Pavlotskaya, L.: Vers une nouvelle approche de l’universalit´e concer-
nant les machines de Turing. Tech. Rep. 95.58, LITP Institut Blaise Pascal (1995)
24. Margenstern, M., Pavlotskaya, L.: On the optimal number of instructions for universal
Turing machines connected with a ﬁnite automaton. International Journal of Algebra and
Computation 13(2), 133–202 (2003)
25. Michel, P.: Small Turing machines and the generalized busy beaver competition. Theo-
retical Computer Science 326, 45–56 (2004)
26. Michel, P., Margenstern, M.: Generalized 3x+1 Functions and the Theory of Computa-
tion. In: The Ultimate Challenge: The 3x + 1 Problem, pp. 105–130. American Mathe-
matical Society (2010)
27. Minsky, M.: Recursive unsolvability of Post’s problem of tag and other topics in theory
of Turing machines. Annals of Mathematics 74(3), 437–455 (1961)
28. Minsky, M.: Size and structure of universal Turing machines using tag systems. In:
Recursive Function Theory, Proceedings, Symposium in Pure Mathematics, vol. 5,
pp. 229–238. American Mathematical Society, Provelence (1962)
29. Moore, E.F.: A simpliﬁed universal Turing machine. In: ACM National Meeting,
pp. 50–54. ACM Press, Toronto (1952)
30. Neary, T., Woods, D.: Small fast universal Turing machines. Theoretical Computer Sci-
ence 362(1-3), 171–195 (2006)
31. Neary, T., Woods, D.: Four small universal Turing machines. Fundamenta Informati-
cae 91(1), 123–144 (2009)

126
T. Neary and D. Woods
32. Neary, T., Woods, D.: The complexity of small universal Turing machines: a survey. In:
Bielikov´a, M., Friedrich, G., Gottlob, G., Katzenbeisser, S., Tur´an, G. (eds.) SOFSEM
2012. LNCS, vol. 7147, pp. 385–405. Springer, Heidelberg (2012)
33. Pavlotskaya, L.: Solvability of the halting problem for certain classes of Turing ma-
chines. Mathematical Notes 13(6), 537–541 (1973); Translated from Matematicheskie
Zametki 13(6), 899–909 (1973)
34. Pavlotskaya, L.: O minimal’nom chisle razlichnykh kodov vershin v grafe universal’noj
mashiny T’juringa. Disketnyj Analiz, Sbornik Trudov Instituta Matematiki SO AN
SSSR 27, 52–60 (1975); On the minimal number of distinct codes for the vertices of
the graph of a universal Turing machine (in Russian)
35. Pavlotskaya, L.: Dostatochnye uslovija razreshimosti problemy ostanovki dlja mashin
T’juring. Problemi Kibernetiki 27, 91–118 (1978); Sufﬁcient conditions for the halting
problem decidability of Turing machines (in Russian)
36. Post, E.: Formal reductions of the general combinatorial decision problem. American
Journal of Mathmatics 65(2), 197–215 (1943)
37. Robinson, R.: Minsky’s small universal Turing machine. International Journal of Math-
ematics 2(5), 551–562 (1991)
38. Rogozhin, Y.: Small universal Turing machines. Theoretical Computer Science 168(2),
215–240 (1996)
39. Wang, H.: A variant to Turing’s theory of computing machines. Journal of the Associa-
tion for Computing Machinery (ACM) 4(1), 63–92 (1957)
40. Woods, D., Neary, T.: On the time complexity of 2-tag systems and small universal Tur-
ing machines. In: 47th Annual IEEE Symposium on Foundations of Computer Science
(FOCS), pp. 439–448. IEEE, Berkeley (2006)
41. Zykin, G.P.: Remarks about a theorem of Hao Wang. Algebra i Logika 2, 33–35 (1963)
(in Russian)

Chapter 6
Constructing Reversible Turing Machines
by Reversible Logic Element with Memory
Kenichi Morita
Abstract. A reversible logic element is a primitive for composing reversible com-
puting machines. There are two kinds of such elements, i.e., one without memory,
which is commonly called a reversible logic gate, and one with memory. It is known
that, in reversible computing, a reversible logic element with memory is useful as
well as a reversible logic gate, since reversible computing machines can be con-
structed simply using such a type of elements. A rotary element (RE) is a typical
instance of a reversible logic element with 1-bit memory, whose operations can be
easily understood by an intuitive interpretation. In this survey, we discuss how RE is
implemented in an idealized reversible physical system, and how reversible Turing
machines (RTMs) can be constructed from REs. In particular, we give a new simpler
construction method of RTMs than the previous one.
6.1
Introduction
Reversible computing is a paradigm of computation that is closely related to re-
versibility in physics. A reversible logic element is a primitive for composing re-
versible logic circuits, by which reversible computing machines can be implemented.
The function of a reversible logic element is described by a one-to-one mapping.
Thus, the behavior of a reversible logic circuit can be traced backward uniquely
with respect to the time axis.
There are two types of reversible logic elements: one without memory, which is
commonly called a reversible logic gate [5,19,20], and one with memory [10,16].
The conventional design theory of logic circuits has been developed using logic
gates as primitives (but in the study of asynchronous circuits, logic elements with
memory are sometimes used [4, 6]). On the other hand, in the case of reversible
computing, logic elements with memory are also useful. The main reason is that
Kenichi Morita
Hiroshima University, Higashi-Hiroshima 739-8527, Japan
e-mail: km@hiroshima-u.ac.jp
c⃝Springer International Publishing Switzerland 2015
127
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_6

128
K. Morita
if we use an appropriate reversible logic element with memory, we can construct
various reversible computing models easily. In [10] it is shown that any reversible
Turing machine can be built using a rotary element (RE), a typical instance of a
reversible logic element with memory (RLEM). In [8], another design method of
reversible Turing machines using different kinds of RLEMs is given.
In this survey, we describe how RLEMs are deﬁned, how they are related to
physical reversibility, and how reversible Turing machines (RTMs) can be built by
them. In particular, here we give a simpler construction method of RTMs by REs
than the one given in [10].
6.2
Reversible Logic Element with Memory (RLEM)
We ﬁrst give a deﬁnition of a sequential machine of Mealy type (i.e., a ﬁnite automa-
ton with an output port as well as an input port), since a reversible logic element with
memory is deﬁned as a kind of a reversible sequential machine.
Deﬁnition 1. A sequential machine (SM) is a system deﬁned by a quadruple M =
(Q,Σ,Γ ,δ). Here, Q is a ﬁnite set of internal states, Σ and Γ are ﬁnite sets of input
and output symbols, and δ : Q × Σ →Q ×Γ is a move function. If δ is injective,
M is called a reversible sequential machine (RSM). Note that if M is reversible,
|Σ| ≤|Γ | must hold.
Deﬁnition 2. A reversible logic element with memory (RLEM) is an RSM M =
(Q,Σ,Γ ,δ) such that |Σ| = |Γ |. It is also called a |Q|-state |Γ |-symbol RLEM.
A rotary element (RE) is an instance of 2-state 4-symbol RLEM that was ﬁrst
introduced in [10]. It is deﬁned by MRE = ({H,V},{n,e,s,w},{n′,e′,s′,w′},δ RE),
where δ RE is given in Table 6.1. For example, if its present state is H and the input
is n, then the next state is V and the output is w′, and hence δ RE(H,n) = (V,w′).
Table 6.1 A tabular description of the move function δ RE of a rotary element (RE)
Input
Present state
n
e
s
w
H
V w′ H w′ V e′ H e′
V
V s′
H n′ V n′ H s′
RE has the following intuitive interpretation. It is shown by a box that contains
a rotatable bar (Figure 6.1). The direction of the bar can be either horizontal or
vertical, and they represent the states H and V, respectively. To each input (out-
put, respectively) symbol, there corresponds an input (output) line. A signal (or a
particle) goes into or goes out from the box through these lines. The bar controls
the move direction of an input signal. When no particle is coming, nothing hap-
pens on the RE. If a particle comes from the direction parallel to the bar, then it

6
Constructing Reversible Turing Machines
129
goes out from the output line of the opposite side without affecting the direction
of the bar (Figure 6.2 (a)). If a particle comes from the direction orthogonal to the
bar, then it makes a right turn, and rotates the bar by 90 degrees counterclockwise
(Figure 6.2 (b)).
-
-


6
6
?
?
n n′
e
e′
s′ s
w′
w
-
-


6
6
?
?
n n′
e
e′
s′ s
w′
w
State H
State V
Fig. 6.1 Pictorial representation of two states of a rotary element (RE)
t = 0
-
-


6
6
?
?
t = 1
-
-


6
6
?
?
t = 0
-
-


6
6
?
?
t = 1
-
-


6
6
?
?
(a)
(b)
Fig. 6.2 An intuitive interpretation on the operations of RE: (a) the parallel case, and (b) the
orthogonal case
It is shown that any RSM can be built using only REs [11], and that any reversible
Turing machine is realized as an inﬁnite circuit composed only of REs [10]. Hence,
in such a sense RE is universal.
There are inﬁnitely many RLEMs if the numbers of states and symbols are not
bounded. There are (2k)! kinds of 2-state k-symbol RLEMs. Hence, the total num-
bers of 2-state 2-, 3-, and 4-symbol RLEMs are 24, 720, and 40,320, respectively.
However, if we regard two RLEMs to be “equivalent” when one is obtained from
another by renaming input/output symbols and/or states, then the number of equiv-
alence classes decreases considerably. In [16] 2-state RLEMs are classiﬁed, and the
numbers of such equivalence classes of 2-state 2-, 3-, and 4-symbol RLEMs are 8,
24, and 82, respectively. There is also a “degenerate” 2-state RLEM that is further
equivalent to a 1-state RLEM (hence it acts as mere connecting wires for a signal)
or a 2-state RLEM having fewer symbols. The numbers of non-degenerate 2-state
2-, 3-, and 4-symbol RLEMs are 4, 14, and 55, respectively [16].
As discussed in [9] by Margenstern, it is important to know the frontier between
universality and non-universality. In [15] it is proved that for every non-degenerate
2-state k-symbol RLEM, there is a circuit composed only of it that simulates RE if
k > 2. Hence, every non-degenerate 2-state k-symbol RLEM is universal if k > 2.
On the other hand, for four non-degenerate 2-state 2-symbol RLEMs, three of them
has been proved to be non-universal [18], but it is left open for the remaining one.

130
K. Morita
6.3
Relation to Physical Reversibility
We now argue how a reversible logic element is related to physical reversibility.
Here, we use the billiard ball model (BBM) proposed by Fredkin and Toffoli [5] as
a reversible physical model. It is an idealized model of Newtonian mechanics con-
sisting of balls and reﬂectors. Computation can be carried out by balls that collide
with other balls or reﬂectors, and hence it is a kind of collision-based computing
(see [1]). It is assumed that collisions are elastic, and there is no friction. Figure 6.3
shows a realization of an interaction gate, which is a 2-input 4-output reversible
logic gate, in BBM [5].
R


R

R
x1
x2
x1·x2
x1·x2
x1·x2
x1·x2
Fig. 6.3 An interaction gate realized in the billiard ball model (BBM) [5]
It is known that we can construct RE by reversible logic gates and delay ele-
ments [14]. Hence, it is in principle possible to realize RE in BBM. But, it is not
a good method. This is because we need many reversible logic gates for construct-
ing one RE. Moreover, when we want to implement a logic gate in BBM, exact
synchronization of two or more moving balls is necessary. Fortunately, there is a
simple way of directly realizing RE in BBM shown in Figure 6.4 [12]. The RE
implemented in BBM consists of one stationary ball called a state ball, and many
reﬂectors indicated by small rectangles. A state ball is placed at the position of H or
V in Figure 6.4(a) depending on the state of the simulated RE. A moving ball called
a signal ball can be given to any one of the input lines n,e,s, and w. Assume a state
ball (a light-colored one) is at the position V, and a signal ball (a dark one) is given
to the input line s as in Figure 6.4(b). Then, the signal ball moves straight ahead
without interacting the state ball or reﬂectors, and ﬁnally goes out from the output
line n′. This simulates the case of Figure 6.2(a). Next, consider the case that a state
ball is at the position H, and a signal ball comes from the input line s (Figure 6.4(c)).
Firstly, the two balls collide at the position 1. After that, they goes along the paths
indicated by arrows. This is performed by appropriately placing reﬂectors. Then,
the two balls collide again at the position 6, and the state ball stops there. The signal
ball ﬁnally goes out from the output line e′ (Figure 6.4(c)). This simulates the case
of Figure 6.2(b). Other cases are also simulated correctly.
In this realization, the signal ball can be given to an input line at any moment and
at any speed, since the state ball is stationary till they collide. Hence, there is no
need to synchronize the input timing at all. Only the time interval between the ﬁrst
and the second collisions in Figure 6.4(c) should be adjusted exactly. In addition,
when we consider a physical realization, the state ball need not be a movable object,
but a suitable physical state that acts reversibly (such as a quantum state). Thus, the

6
Constructing Reversible Turing Machines
131
?

-
6
n
n′
e
e′
s
s′
w
w′
H
V
(a)
?

-
6
n
n′
e
e′
s
s′
w
w′
6
6
6
0,1
0
1
(b)
?

-
6
n
n′
e
e′
s
s′
w
w′
6
6
6


R

I
I
I


-
-
I
I


I
I


RR
0,1
2
3
4
5
6,7
0
1
2
3
4
5
6
7
(c)
Fig. 6.4
RE realized in BBM. (a) H and V are the places where a state ball is put. Small
rectangles indicate reﬂectors for balls. (b) The case that the state is V, and the input is s (cor-
responding to Figure 6.2(a)). (c) The case that the state is H, and the input is s (corresponding
to Figure 6.2(b)).

132
K. Morita
above idea broaden the possibility of practical realization of RE in a real system
having physical reversibility. It is noted that Mukai and Morita [17] showed any
m-state k-symbol RLEM can be realized in BBM by a systematic method if k ≤4.
6.4
Constructing Reversible Turing Machines by Rotary
Element
In this section we show any reversible Turing machine (RTM) can be realized as
a circuit composed only of REs. An RTM was ﬁrst investigated by Lecerf [7] who
showed unsolvability of their halting problem. Bennett [2,3] then studied them from
the standpoint of thermodynamics of computing, and showed its universality. In
[2] RTMs were deﬁned in the quadruple form, since an “inverse” RTM is easily
obtained from a given RTM if it is given in the quadruple form. But, here we use
an RTM in the quintuple form, since this form is convenient to make a circuit that
simulates the RTM simpler.
Deﬁnition 3. A Turing machine (TM) is deﬁned by T = (Q,S,q0,F,s0,δ), where
Q is a set of states of the ﬁnite control, S is a set of tape symbols, q0 ∈Q is an
initial state, F ⊂Q is a set of ﬁnal states, and s0 ∈S is a blank symbol. δ is a move
relation, which is a subset of (Q× S × S × {L,N,R} × Q), where L,N, and R stand
for left-shift, no-shift, and right-shift of the head. Each element of δ is a quintuple
of the form [p,s,s′,d,q]. It means if T reads the symbol s in the state p, then write
s′, shift the head to the direction d, and go to the state q.
T is called deterministic iff the following condition holds for any pair of distinct
quintuples [p1,s1,s′
1,d1,q1] and [p2,s2,s′
2,d2,q2] in δ: if p1 = p2, then s1 ̸= s2.
T is called reversible iff the following condition holds for any pair of distinct
quintuples [p1,s1,s′
1,d1,q1] and [p2,s2,s′
2,d2,q2] in δ: if q1 = q2, then s′
1 ̸= s′
2 ∧
d1 = d2. Hereafter, by RTM we mean a deterministic and reversible TM.
Example 1. Let Tparity be an RTM deﬁned as follows. Tparity = (Q,{0,1},q0,{qacc},
0,δ), where Q = {q0,q1, q2,qacc,qrej}, and δ = {[ q0,0,1,R,q1 ], [ q1,0,1,L,qacc ],
[ q1,1,0,R,q2 ], [ q2,0,1,L,qrej ], [ q2,1,0,R,q1 ]}. Tparity checks if a given unary
number n is even or odd. If it is even, Tparity halts in the accepting state qacc, other-
wise halts in the rejecting state qrej. All the symbols read by Tparity are complemented
(see Figure 6.5). It is easy to see that Tparity satisﬁes the condition for reversibility.
t = 0
6
q0
0 1 1 0 0
t = 1
6
q1
1 1 1 0 0
t = 2
6
q2
1 0 1 0 0
t = 3
6
q1
1 0 0 0 0
t = 4
6
qacc
1 0 0 1 0
Fig. 6.5 An example of a computing process of the RTM Tparity for the unary input 11

6
Constructing Reversible Turing Machines
133
In [10] it is shown that any RTM can be realized by a circuit composed only of
REs. There, two kinds of modules called a memory cell (i.e., one square of a tape),
and a ﬁnite-state control of an RTM are formalized as RSMs. Then, the two modules
are constructed by REs. Here, we show a simpler formalization and construction
method of these modules than the one given in [10]. We assume the given RTM has
only one tape that is semi-inﬁnite to the right, and uses two kinds of tape symbols 0
and 1. An RTM having multiple tapes and/or many symbols can also be constructed
in a similar manner as below.
A memory cell is an RSM that acts as one square of a tape of an RTM. Con-
necting an inﬁnite number of memory cells rightward, we can obtain a tape unit for
the RTM. The memory cell keeps a tape symbol s ∈{0,1}, and the information h
whether the head of the RTM is on this cell (h = 1) or not (h = 0). Hence, its state set
is {(h,s)|h,s ∈{0,1}}. It has ten kinds of input symbols listed in Table 6.2, which
are interpreted as instruction signals to the tape unit or response signals to the ﬁnite-
state control. For each input symbol, there is a corresponding output symbol, which
is indicated by a one with ′. Thus, it has also ten kinds of output symbols.
Table 6.2 Ten kinds of input symbols of a memory cell, and their meanings
Symbol Instruction/Response
Meaning
W0
Write 0
Instruction of writing the symbol 0 at the head position.
By this instruction, read operation is also performed.
W1
Write 1
Instruction of writing the symbol 1 at the head position.
By this instruction, read operation is also performed.
R0
Read 0
Response signal telling the read symbol is 0.
R1
Read 1
Response signal telling the read symbol is 1.
SL
Shift-left
Instruction of shift-left operation.
SLI
Shift-left immediate
Instruction of placing the head on this cell by shifting-left.
SLc
Shift-left completed
Response signal of shift-left operation.
SR
Shift-right
Instruction of shift-right operation.
SRI
Shift-right immediate Instruction of placing the head on this cell by shifting-right.
SRc
Shift-right completed Response signal of shift-right operation.
The input symbol W0 or W1 is an instruction to the tape unit for writing the
tape symbol 0 or 1, respectively, in the memory cell at the head position. After the
writing operation, the output symbol R0′ or R1′ is generated as a response signal
depending on the old tape symbol at the head position, and is sent to the ﬁnite-
state control. Hence, the write instruction performs not only the write operation but
also the read operation. Otherwise, reversibility of the memory cell does not hold.
Assume a symbol W0 or W1 is given to a memory cell. If the tape head is not at
the cell (i.e., h = 0), then it generates an output symbol W0′ or W1′, respectively,
and send it to the right-neighboring memory cell. If the tape head is at the cell (i.e.,
h = 1), then the cell sets its new tape symbol s to 0 (if W0 is given) or 1 (if W1
is given). It also generates an output symbol R0′ or R1′ depending on the old tape
symbol, and send it to the left-neighboring memory cell.

134
K. Morita
Note that, if an RTM needs to read a tape symbol, then it is performed by the
instruction W0. By this, the ﬁnite-state control obtains a response signal R0 or R1,
and the tape symbol at the head position is cleared to 0. After that, if the RTM gives
the instruction W0 or W1, then the tape symbol 0 or 1 is written at the head position,
and ﬁnally the ﬁnite-state control obtains the response signal R0.
In the previous construction of the memory cell in [10], read and write instruc-
tions are separated (where an instruction of “write complementary symbol” was
used for keeping reversibility). Due to this, “inverse-read instructions” that undo the
read operation were also necessary, because “branching” by read instruction should
be reversibly “merged” afterwards. By this, eight kinds of instruction and response
symbols for read and write operations were used. But here, by making good use of
symmetry between reading and writing, only four kinds of symbols W0, W1, R0,
and R1 are used, and thus the circuit for the memory cell is simpliﬁed considerably.
The input symbol SL is an instruction for shifting the tape head to the left. As-
sume a symbol SL is given to a memory cell. If the tape head is not at the cell (i.e.,
h = 0), then it simply sends an output symbol SL′ to the right-neighboring memory
cell. If the tape head is at the cell (i.e., h = 1), then the cell sets the new h to 0,
and sends an output symbol SLI′ to the left-neighboring memory cell. If the latter
memory cell receive an input symbol SLI, then it sets the new h to 1, and sends an
output symbol SLc′ to the left. By such a process, shift-left operation is performed
correctly. The input symbols SR, SRI, and SRc are similar to the symbols SL, SLI,
and SLc, except that an output symbol SRI′ is sent to the right-neighboring cell.
By above, the memory cell is formalized as the following RSM MC.
MC = (QC,ΣC,ΓC,δ C)
QC = {(h,s)|h,s ∈{0,1}}
ΣC = { W0, W1, R0, R1, SL, SLI, SLc, SR, SRI, SRc }
ΓC = {x′ |x ∈ΣC}
δ C((0,s),y)
= ((0,s),y′)
(y ∈ΣC −{SLI, SRI})
δ C((0,s),SLI) = ((1,s),SLc′)
δ C((0,s),SRI) = ((1,s),SRc′)
δ C((1,0),W0) = ((1,0),R0′)
δ C((1,1),W0) = ((1,0),R1′)
δ C((1,0),W1) = ((1,1),R0′)
δ C((1,1),W1) = ((1,1),R1′)
δ C((1,s),SL) = ((0,s),SLI′)
δ C((1,s),SR) = ((0,s),SRI′)
Figure 6.6 shows a circuit composed only of REs that realizes the RSM MC. Each
of the rotary elements at the positions h and s is set to state V if the value is 0, and
state H if it is 1. Connecting an inﬁnite number of copies of this circuit to the right,
we have a tape unit for a 2-symbol RTM. At the left end of the array of memory
cells, a circuit of a ﬁnite-state control explained below will be attached.
A ﬁnite-state control of an RTM can be also realized by rotary elements easily,
since it is a kind of an RSM. In this circuit, each quintuple of an RTM is executed

6
Constructing Reversible Turing Machines
135
SLc′
SLI′
SL
SRc′
SRI
SR
R1′
R0′
W1
W0
s
s
s
s
s
s
s
s
h
s
s
?
?
?
?
?
?
?
?
6
6
6
6
6
6
6
6
?

6
?
6
?

6

-


-
-

-

-
-
-

-


-

-
-

-
-
-
SLc
SLI
SL′
SRc
SRI′
SR′
R1
R0
W1′
W0′
Fig. 6.6 A memory cell implemented by REs
by producing read, write, and shift instructions consecutively, and sending them
to an array of memory cells. Firstly, giving the W0 instruction, a read operation
is performed. As a result, a completion signal R0 or R1 is obtained. Then, giving
W0 or W1 instruction, a write operation is performed, and a completion signal R0
is obtained. Finally, giving SL or SR instruction, shift operation is performed. A
circuit of a ﬁnite-state control for Tparity in Example 1 is shown in Figure 6.7.
In the circuit of a ﬁnite-state control, we lay REs in 4 rows. In Figure 6.7, three
elements in the 4th row correspond to q0,q1 and q2, respectively. They send W0
instruction to read the symbol at the head position. If they receive a signal R0 or R1,
they determine which quintuple of Tparity must be executed. Giving an instruction
W0 or W1 according to the chosen quintuple, the 4 elements of the 3rd row perform
writing and state-change. Then the elements of the 1st or 2nd rows perform a head
shift.
Figure 6.8 is a circuit that simulates the RTM Tparity in Example 1. If we give
a signal (or a particle) to the input port “Begin,” then it starts to compute. Finally,
the particle comes out from the output port “Accept” or “Reject” depending on the
parity of the given input on the tape.

136
K. Morita

-

-


-
-



-
-
-

-

-
-

-
-
6
?
6
6
?
6
6
6
?
6
6

6

q0
q1
q2
q00
q10
q11 q20
q21
q1
qacc
q2
qrej
SLc
SL′
SRc
SR′
R1
R0
W1′
W0′
Reject
Accept
Begin•
Fig. 6.7 A circuit that simulates the ﬁnite-state control of Tparity in Example 1

-

-


-
-



-
-
-

-

-
-

-
-
6
?
6
6
?
6
6
6
?
6
6

6

?
?
?
?
?
?
?
?
6
6
6
6
6
6
6
6
?

6
?
6
?

6

-


-
-

-

-
-
-

-


-

-
-

-
-
-
?
?
?
?
?
?
?
?
6
6
6
6
6
6
6
6
?

6
?
6
?

6

-


-
-

-

-
-
-

-


-

-
-

-
-
-
?
?
?
?
?
?
?
?
6
6
6
6
6
6
6
6
?

6
?
6
?

6

-


-
-

-

-
-
-

-


-

-
-

-
-
-
?
?
?
?
?
?
?
?
6
6
6
6
6
6
6
6
?

6
?
6
?

6

-


-
-

-

-
-
-

-


-

-
-

-
-
-
Reject
Accept
Begin •
• • •
Head
0
1
1
0
Fig. 6.8 A circuit made of REs that simulates the RTM Tparity in Example 1. An example of
its whole computing process is shown in 4406 ﬁgures in [13].

6
Constructing Reversible Turing Machines
137
6.5
Concluding Remarks
In this survey/tutorial paper, we discussed how a reversible logic element with mem-
ory (RLEM) is deﬁned, how it is related to physical reversibility, and how it is used
to construct reversible computing systems. In particular, reversible Turing machines
(RTMs) can be constructed rather simply as shown in Figure 6.8 using rotary ele-
ment (RE), a typical RLEM. It should be noted, if we further implement each RE by
a mechanism in the billiard ball model (BBM) as in Figure 6.4, then the whole sys-
tem of the RTM can be realized in the space of BBM. Here, we used RE to construct
RTMs. However, since it is known “all” non-degenerate 2-state RLEMs except only
four are universal [15], we can also use any one of them for composing RTMs.
Acknowledgements. This work was supported by JSPS KAKENHI Grant Number
24500017.
References
1. Adamatzky, A.: Collision-Based Computing. Springer (2002), doi:10.1007/978-1-4471-
0129-1
2. Bennett, C.H.: Logical reversibility of computation. IBM J. Res. Dev. 17, 525–532
(1973), doi:10.1147/rd.176.0525
3. Bennett, C.H.: The thermodynamics of computation—a review. Int. J. Theoret. Phys. 21,
905–940 (1982), doi:10.1007/BF02084158
4. B¨uning, H., Priese, L.: Universal asynchronous iterative arrays of Mealy automata. Acta
Informatica 13, 269–285 (1980), doi:10.1007/BF00288646
5. Fredkin, E., Toffoli, T.: Conservative logic. Int. J. Theoret. Phys. 21, 219–253 (1982),
doi:10.1007/BF01857727
6. Keller, R.: Towards a theory of universal speed-independent modules. IEEE Trans. Com-
puters C-23, 21–33 (1974), doi:10.1109/T-C.1974.223773
7. Lecerf, Y.: Machines de Turing r´eversibles — r´ecursive insolubilit´e en n ∈N de
l’´equation u = θ nu, o`u θ est un isomorphisme de codes. Comptes Rendus Hebdo-
madaires des S´eances de L’acad´emie des Sciences 257, 2597–2600 (1963)
8. Lee, J., Yang, R., Morita, K.: Design of 1-tape 2-symbol reversible Turing ma-
chines based on reversible logic elements. Theoret. Comput. Sci. 460, 78–88 (2012),
doi:10.1016/j.tcs.2012.07.027
9. Margenstern, M.: Frontier between decidability and undecidability: a survey. Theoret.
Comput. Sci. 231, 217–251 (2000), doi:10.1016/S0304-3975(99)00102-4
10. Morita, K.: A simple universal logic element and cellular automata for reversible
computing. In: Margenstern, M., Rogozhin, Y. (eds.) MCU 2001. LNCS, vol. 2055,
pp. 102–113. Springer, Heidelberg (2001)
11. Morita, K.: A new universal logic element for reversible computing. In: Martin-Vide, C.,
Mitrana, V. (eds.) Grammars and Automata for String Processing, pp. 285–294. Taylor
& Francis, London (2003), doi:10.1201/9780203009642.ch28
12. Morita, K.: Reversible computing and cellular automata — A survey. Theoret. Comput.
Sci. 395, 101–131 (2008), doi:10.1016/j.tcs.2008.01.041
13. Morita, K.: Constructing a reversible Turing machine by a rotary element, a reversible
logic element with memory. Hiroshima University Institutional Repository (2010),
http://ir.lib.hiroshima-u.ac.jp/00029224

138
K. Morita
14. Morita, K.: Reversible Computing. Kindai Kagaku-sha Co., Ltd., Tokyo (2012) ISBN
978-4-7649-0422-4 (in Japanese)
15. Morita, K., Ogiro, T., Alhazov, A., Tanizawa, T.: Non-degenerate 2-state reversible logic
elements with three or more symbols are all universal. J. Multiple-Valued Logic and Soft
Computing 18, 37–54 (2012)
16. Morita, K., Ogiro, T., Tanaka, K., Kato, H.: Classiﬁcation and universality of reversible
logic elements with one-bit memory. In: Margenstern, M. (ed.) MCU 2004. LNCS,
vol. 3354, pp. 245–256. Springer, Heidelberg (2005)
17. Mukai, Y., Morita, K.: Realizing reversible logic elements with memory in the billiard
ball model. Int. J. of Unconventional Computing 8(1), 47–59 (2012)
18. Mukai, Y., Ogiro, T., Morita, K.: Universality problems on reversible logic elements with
1-bit memory. Int. J. Unconventional Computing (to appear)
19. Toffoli, T.: Reversible computing. In: de Bakker, J.W., van Leeuwen, J. (eds.) ICALP
1980. LNCS, vol. 85, pp. 632–644. Springer, Heidelberg (1980)
20. Toffoli, T.: Bicontinuous extensions of invertible combinatorial functions. Math. Syst.
Theory 14, 12–23 (1981), doi:10.1007/BF01752388

Chapter 7
The Grossone Methodology Perspective
on Turing Machines
Yaroslav D. Sergeyev and Alfredo Garro
Abstract. This chapter discusses how the mathematical language used to describe
and to observe automatic computations inﬂuences the accuracy of the obtained re-
sults. The chapter presents results obtained by describing and observing different
kindsofTuring machines(singleand multi-tape,deterministicand non-deterministic)
through the lens of a new mathematical language named Grossone. This emerging
languageis strongly based on three methodologicalideas borrowed from Physics and
applied to Mathematics: the distinction between the object (indeed mathematical ob-
ject) of an observation and the instrument used for this observation; interrelations
holding between the object and the tool used for the observation; the accuracy of the
observation determined by the tool. In the chapter, the new results are compared to
those achievable by using traditional languages. It is shown that both languages do
not contradict each other but observe and describe the same object (Turing machines)
but with different accuracies.
7.1
Introduction
Turing machines represent one of the simple abstract computational devices that can
be used to investigate the limits of computability . In this chapter, they are consid-
ered from several points of view that emphasize the importance and the relativity of
Yaroslav D. Sergeyev
Dipartimento di Ingegneria Informatica, Modellistica, Elettronica e Sistemistica (DIMES),
Universit`a della Calabria, Rende (CS), Italy
N.I. Lobatchevsky State University, Nizhni Novgorod, Russia
Istituto di Calcolo e Reti ad Alte Prestazioni, C.N.R., Rende (CS), Italy
e-mail: yaro@si.dimes.unical.it
Alfredo Garro
Dipartimento di Ingegneria Informatica, Modellistica, Elettronica e Sistemistica (DIMES),
Universit`a della Calabria, Rende (CS), Italy
e-mail: garro@dimes.unical.it
c⃝Springer International Publishing Switzerland 2015
139
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_7

140
Y.D. Sergeyev and A. Garro
mathematical languages used to describe the Turing machines. A deep investigation
is performed on the interrelations between mechanical computations and their math-
ematical descriptions emerging when a human (the researcher) starts to describe a
Turing machine (the object of the study) by different mathematical languages (the
instruments of investigation).
In particular, we focus our attention on different kinds of Turing machines (single
and multi-tape, deterministic and non-deterministic) by organizing and discussing
the results presented in [42] and [43] so to provide a compendium of our multi-year
research on this subject.
The starting point is represented by numeral systems 1 that we use to write down
numbers, functions, models, etc. and that are among our tools of investigation of
mathematical and physical objects. It is shown that numeral systems strongly inﬂu-
ence our capabilities to describe both the mathematical and physical worlds. A new
numeral system introduced in [30,32,37]) for performing computations with inﬁnite
and inﬁnitesimal quantities is used for the observation of mathematical objects and
studying Turing machines. The new methodology is based on the principle ‘The part
is less than the whole’ introduced by Ancient Greeks (see, e.g., Euclid’s Common
Notion 5) and observed in practice. It is applied to all sets and processes (ﬁnite and
inﬁnite) and all numbers (ﬁnite, inﬁnite, and inﬁnitesimal).
In order to see the place of the new approach in the historical panorama of ideas
dealing with inﬁnite and inﬁnitesimal, see [19–21,35,36,42,43]. The new method-
ology has been successfully applied for studying a number of applications: per-
colation (see [13, 45]), Euclidean and hyperbolic geometry (see [22, 29]), fractals
(see [31,33,40,45]), numerical differentiation and optimization (see [7,34,38,48]),
ordinary differential equations (see [41]), inﬁnite series (see [35, 39, 47]), the ﬁrst
Hilbert problem (see [36]), and cellular automata (see [8]).
The rest of the chapter is structured as follows. In Section 7.2, Single and Multi-
tape Turing machines are introduced along with “classical” results concerning their
computational power and related equivalences; in Section 7.3 a brief introduction
to the new language and methodology is given whereas their exploitation for an-
alyzing and observing the different types of Turing machines is discussed in Sec-
tion 7.4. It shows that the new approach allows us to observe Turing machines with
a higher accuracy giving so the possibility to better characterize and distinguish ma-
chines which are equivalent when observed within the classical framework. Finally,
Section 7.5 concludes the chapter.
1 We are reminded that a numeral is a symbol or group of symbols that represents a number.
The difference between numerals and numbers is the same as the difference between words
and the things they refer to. A number is a concept that a numeral expresses. The same
number can be represented by different numerals. For example, the symbols ‘7’, ‘seven’,
and ‘VII’ are different numerals, but they all represent the same number.

7
A New Perspective on Turing Machines
141
7.2
Turing Machines
The Turing machine is one of the simple abstract computational devices that can be
used to model computational processes and investigate the limits of computability.
In the following subsections, deterministic Single and Multi-tape Turing machines
are described along with important classical results concerning their computational
power and related equivalences (see Section 7.2.1 and 7.2.2 respectively); ﬁnally,
non-deterministic Turing machines are introduced (see Section 7.2.3).
7.2.1
Single-Tape Turing Machines
A Turing machine (see, e.g., [12,44]) can be deﬁned as a 7-tuple
M =

Q,Γ , ¯b,Σ,q0,F,δ

,
(7.1)
where Q is a ﬁnite and not empty set of states; Γ is a ﬁnite set of symbols; ¯b ∈Γ is
a symbol called blank; Σ ⊆{Γ −¯b} is the set of input/output symbols; q0 ∈Q is the
initial state; F ⊆Q is the set of ﬁnal states; δ : {Q−F}×Γ →Q×Γ ×{R,L,N} is
a partial function called the transition function, where L means left, R means right,
and N means no move .
Speciﬁcally, the machine is supplied with: (i) a tape running through it which is
divided into cells each capable of containing a symbol γ ∈Γ , where Γ is called the
tape alphabet, and ¯b ∈Γ is the only symbol allowed to occur on the tape inﬁnitely
often; (ii) a head that can read and write symbols on the tape and move the tape left
and right one and only one cell at a time. The behavior of the machine is speciﬁed
by its transition function δ and consists of a sequence of computational steps ; in
each step the machine reads the symbol under the head and applies the transition
function that, given the current state of the machine and the symbol it is reading on
the tape, speciﬁes (if it is deﬁned for these inputs): (i) the symbol γ ∈Γ to write on
the cell of the tape under the head; (ii) the move of the tape (L for one cell left, R for
one cell right, N for no move); (iii) the next state q ∈Q of the machine.
7.2.1.1
Classical Results for Single-Tape Turing Machines
Starting from the deﬁnition of Turing machine introduced above, classical results
(see, e.g., [1]) aim at showing that different machines in terms of provided tape and
alphabet have the same computational power, i.e., they are able to execute the same
computations. In particular, two main results are reported below in an informal way.
Given a Turing machine M = {Q,Γ , ¯b,Σ,q0,F,δ}, which is supplied with
an inﬁnite tape, it is always possible to deﬁne a Turing machine M′ =
{Q′,Γ ′, ¯b,Σ′,q′
0,F′,δ ′} which is supplied with a semi-inﬁnite tape (e.g., a tape with
a left boundary) and is equivalent to M, i.e., is able to execute all the computations
of M.

142
Y.D. Sergeyev and A. Garro
Given a Turing machine M = {Q,Γ , ¯b,Σ,q0,F,δ}, it is always possible to deﬁne
a Turing machine M′ = {Q′,Γ ′, ¯b,Σ′,q′
0,F′,δ ′} with |Σ′| = 1 and Γ ′ = Σ′ ∪{¯b},
which is equivalent to M, i.e., is able to execute all the computations of M.
It should be mentioned that these results, together with the usual conclusion re-
garding the equivalences of Turing machines, can be interpreted in the following,
less obvious, way: they show that when we observe Turing machines by exploiting
the classical framework we are not able to distinguish, from the computational point
of view, Turing machines which are provided with alphabets having different num-
ber of symbols and/or different kind of tapes (inﬁnite or semi-inﬁnite) (see [42] for
a detailed discussion).
7.2.2
Multi-tape Turing Machines
Let us consider a variant of the Turing machine deﬁned in (7.1) where a machine
is equipped with multiple tapes that can be simultaneously accessed and updated
through multiple heads (one per tape). These machines can be used for a more direct
and intuitive resolution of different kind of computational problems. As an example,
in checking if a string is palindrome it can be useful to have two tapes on which
represent the input string so that the veriﬁcation can be efﬁciently performed by
reading a tape from left to right and the other one from right to left.
Moving towards a more formal deﬁnition, a k-tapes, k ≥2, Turing machine
(see [12]) can be deﬁned (cf. (7.1)) as a 7-tuple
MK =

Q,Γ , ¯b,Σ,q0,F,δ (k)
,
(7.2)
where Σ = k
i=1 Σi is given by the union of the symbols in the k input/output al-
phabets Σ1,...,Σk; Γ = Σ ∪{¯b} where ¯b is a symbol called blank; Q is a ﬁnite and
not empty set of states; q0 ∈Q is the initial state; F ⊆Q is the set of ﬁnal states;
δ (k) : {Q−F} ×Γ1 × ··· ×Γk →Q×Γ1 × ··· ×Γk × {R,L,N}k is a partial function
called the transition function, where Γi = Σi ∪{¯b},i = 1,...,k, L means left, R means
right, and N means no move .
This deﬁnition of δ (k) means that the machine executes a transition starting from
an internal state qi and with the k heads (one for each tape) above the characters
ai1,...,aik, i.e., if δ (k)(q1,ai1,...,aik) = (q j,a j1,...,a jk,zj1,...,zjk) the machine
goes in the new state q j, write on the k tapes the characters a j1,...,a jk respec-
tively, and moves each of its k heads left, right or no move, as speciﬁed by the
zjl ∈{R,L,N},l = 1,...,k.
A machine can adopt for each tape a different alphabet, in any case, for each tape,
as for the Single-tape Turing machines, the minimum portion containing characters
distinct from ¯b is usually represented. In general, a typical conﬁguration of a Multi-
tape machine consists of a read-only input tape, several read and write work tapes,
and a write-only output tape, with the input and output tapes accessible only in one
direction. In the case of a k-tapes machine, the instant conﬁguration of the machine,

7
A New Perspective on Turing Machines
143
as for the Single-tape case, must describe the internal state, the contents of the tapes
and the positions of the heads of the machine.
More formally, for a k-tapes Turing machine MK =

Q,Γ , ¯b,Σ,q0,F,δ (k)
with
Σ = k
i=1 Σi (see 7.2) a conﬁguration of the machine is given by:
q#α1 ↑β 1#α2 ↑β 2#...#αk ↑β k,
(7.3)
where q ∈Q; αi ∈ΣiΓ ∗
i ∪{ε} and β i ∈Γ ∗
i Σi ∪{¯b}. A conﬁguration is ﬁnal if q ∈F.
The starting conﬁguration usually requires the input string x on a tape, e.g., the
ﬁrst tape so that x ∈Σ∗
1, and only ¯b symbols on all the other tapes. However, it can be
useful to assume that, at the beginning of a computation, these tapes have a starting
symbol Z0 /∈Γ = k
i=1Γi. Therefore, in the initial conﬁguration the head on the ﬁrst
tape will be on the ﬁrst character of the input string x, whereas the heads on the other
tapes will observe the symbol Z0, more formally, by re-placing Γi = Σi ∪{¯b,Z0} in
all the previous deﬁnition, a conﬁguration q#α1 ↑β 1#α2 ↑β 2#...#αk ↑β k is an
initial conﬁguration if αi = ε,i = 1,...,k,β 1 ∈Σ∗
1,β i = Z0,i = 2,...,k and q = q0.
The application of the transition function δ (k) at a machine conﬁguration (c.f.
(7.3)) deﬁnes a computational step of a Multi-tape Turing machine . The set of
computational steps which bring the machine from the initial conﬁguration into a ﬁ-
nal conﬁguration deﬁnes the computation executed by the machine. As an example,
the computation of a Multi-tape Turing machine MK which computes the function
fMK(x) can be represented as follows:
q0# ↑x# ↑Z0#...# ↑Z0
→
MK q# ↑x# ↑fMK(x)# ↑¯b#...# ↑¯b
(7.4)
where q ∈F and
→
MK indicates the transition among machine conﬁgurations.
7.2.2.1
Classical Results for Multi-tape Turing Machines
It is worth noting that, although the k-tapes Turing machine can be used for a more
direct resolution of different kind of computational problems, in the classical frame-
work it has the same computational power of the Single-tape Turing machine. More
formally, given a Multi-tape Turing machine it is always possible to deﬁne a Single-
tape Turing machine which is able to fully simulate its behavior and therefore to
completely execute its computations. In particular, the Single-tape Turing machines
adopted for the simulation use a particular kind of the tape which is divided into
tracks (multi-track tape). In this way, if the tape has m tracks, the head is able to
access (for reading and/or writing) all the m characters on the tracks during a sin-
gle operation. If for the m tracks the alphabets Γ1,... Γm are adopted respectively,
the machine alphabet Γ is such that |Γ | = |Γ1 × ···×Γm| and can be deﬁned by an
injective function from the set Γ1 × ··· × Γm to the set Γ ; this function will asso-
ciate the symbol ¯b in Γ to the tuple (¯b, ¯b,..., ¯b) in Γ1 × ··· × Γm. In general, the
elements of Γ which correspond to the elements in Γ1 ×···×Γm can be indicated by
[ai1,ai2,...,aim] where ai j ∈Γj.

144
Y.D. Sergeyev and A. Garro
By adopting this notation it is possible to demonstrate that given a k-tapes Turing
machine MK = {Q,Γ , ¯b,Σ,q0,F,δ (k)} it is always possible to deﬁne a Single-tape
Turing machine which is able to simulate t computational steps of MK = in O(t2)
transitions by using an alphabet with O((2|Γ |)k) symbols (see [1]) .
The proof is based on the deﬁnition of a machine M′ = {Q′,Γ ′, ¯b,Σ′,q′
0,F′,δ ′}
with a Single-tape divided into 2k tracks (see [1]); k tracks for storing the characters
in the k tapes of MK and k tracks for signing through the marker ↓the positions of
the k heads on the k tapes of Mk. As an example, this kind of tape can represent
the content of each tapes of Mk and the position of each machine heads in its even
and odd tracks respectively. As discussed above, for obtaining a Single-tape machine
able to represent these 2k tracks, it is sufﬁcient to adopt an alphabet with the required
cardinality and deﬁne an injective function which associates a 2k-ple characters of
a cell of the multi-track tape to a symbols in this alphabet.
The
transition
function
δ (k)
of
the
k-tapes
machine
is
given
by
δ (k)(q1,ai1,...,aik) = (q j,a j1,...,a jk,zj1,...,zjk), with zj1,...,zjk ∈{R,L,N};
as a consequence the corresponding transition function δ ′ of the Single-tape
machine, for each transition speciﬁed by δ (k) must individuate the current state
and the position of the marker for each track and then write on the tracks the
required symbols, move the markers and go in another internal state. For each
computational step of MK, the machine M′ must execute a sequence of steps for
covering the portion of tapes between the two most distant markers. As in each
computational step a marker can move at most of one cell and then two markers
can move away each other at most of two cells, after t steps of MK the markers
can be at most 2t cells distant, thus if MK executes t steps, M′ executes at most:
2∑t
i=1 i = t2 +t = O(t2) steps .
Moving to the cost of the simulation in terms of the number of required characters
for the alphabet of the Single-tape machine, we recall that |Γ1| = |Σ1| + 1 and that
|Γi| = |Σi| + 2 for 2 ≤i ≤k. So by multiplying the cardinalities of these alphabets
we obtain that: |Γ ′| = 2k(|Σ1|+ 1)∏k
i=2(|Σi|+ 2) = O((2max1≤i≤k |Γi|)k).
7.2.3
Non-deterministic Turing Machines
A non-deterministic Turing machine (see [12]) can be deﬁned (cf. (7.1)) as a 7-tuple
MN =

Q,Γ , ¯b,Σ,q0,F,δ N

,
(7.5)
where Q is a ﬁnite and not empty set of states; Γ is a ﬁnite set of symbols; ¯b ∈Γ
is a symbol called blank; Σ ⊆{Γ −¯b} is the set of input/output symbols; q0 ∈Q
is the initial state; F ⊆Q is the set of ﬁnal states; δ N : {Q−F} ×Γ →P(Q×Γ ×
{R,L,N}) is a partial function called the transition function, where L means left, R
means right, and N means no move .

7
A New Perspective on Turing Machines
145
As for a deterministic Turing machine (see (7.1)), the behavior of MN is speciﬁed
by its transition function δ N and consists of a sequence of computational steps . In
each step, given the current state of the machine and the symbol it is reading on
the tape, the transition function δ N returns (if it is deﬁned for these inputs) a set of
triplets each of which speciﬁes: (i) a symbol γ ∈Γ to write on the cell of the tape
under the head; (ii) the move of the tape (L for one cell left, R for one cell right, N
for no move); (iii) the next state q ∈Q of the Machine. Thus, in each computational
step, the machine can non-deterministically execute different computations, one for
each triple returned by the transition function.
An important characteristic of a non-deterministic Turing machine (see, e.g., [1])
is its non-deterministic degree
d = ν(MN) =
max
q∈Q−F,γ∈Γ |δ N(q,γ)|
deﬁned as the maximal number of different conﬁgurations reachable in a single
computational step starting from a given conﬁguration. The behavior of the machine
can be then represented as a tree whose branches are the computations that the
machine can execute starting from the initial conﬁguration represented by the node
0 and nodes of the tree at the levels 1, 2, etc. represent subsequent conﬁgurations of
the machine.
Let us consider an example shown in Fig. 7.1 where a non-deterministic machine
MN having the non-deterministic degree d = 3 is presented. The depth of the com-
putational tree is equal to k. In this example, it is supposed that the computational
tree of MN is complete (i.e., each node has exactly d children). Then, obviously,
the computational tree of MN has dk = 3k leaf nodes.
7.2.3.1
Classical Results for Non-deterministic Turing Machines
An important result for the classic theory on Turing machines (see e.g., [1]) is that
for any non-deterministic Turing machine MN there exists an equivalent determinis-
tic Turing machine MD. Moreover, if the depth of the computational tree generated
by MN is equal to k, then for simulating MN, the deterministic machine MD will
execute at most
KMD =
k
∑
j=0
jd j = O(kdk)
computational steps.
Intuitively, for simulating MN, the deterministic Turing machine MD executes
a breadth-ﬁrst visit of the computational tree of MN. If we consider the example
from Fig. 7.1 with k = 3, then the computational tree of MN has dk = 27 leaf nodes
and dk = 27 computational paths consisting of k = 3 branches (i.e., computational
steps) . Then, the tree contains dk−1 = 9 computational paths consisting of k−1 = 2
branches and dk−2 = 3 computational paths consisting of k−2 = 1 branches . Thus,
for simulating all the possible computations of MN, i.e., for complete visiting the
computational tree of MN and considering all the possible computational paths of

146
Y.D. Sergeyev and A. Garro
Fig. 7.1 The computational tree of a non-deterministic Turing machine MN having the non-
deterministic degree d = 3
j computational steps for each 0 ⩽j ⩽k, the deterministic Turing machine MD
will execute KMD steps. In particular, if MN reaches a ﬁnal conﬁguration (e.g., it
accepts a string) in k ⩾0 steps and if MD could consider only the dk computational
paths which consist of k computational steps, it will executes at most kdk steps for
reaching this conﬁguration.
These results show an exponential growth of the time required for reaching a ﬁ-
nal conﬁguration by the deterministic Turing machine MD with respect to the time
required by the non-deterministic Turing machine MN, assuming that the time re-
quired for both machines for a single step is the same. However, in the classic theory
on Turing machines it is not known if there is a more efﬁcient simulation of MN.
In other words, it is an important and open problem of Computer Science theory to
demonstrate that it is not possible to simulate a non-deterministic Turing machine
by a deterministic Turing machine with a sub-exponential numbers of steps.

7
A New Perspective on Turing Machines
147
7.3
The Grossone Language and Methodology
In this section, we give just a brief introduction to the methodology of the new
approach [30,32] dwelling only on the issues directly related to the subject of the
chapter. This methodology will be used in Section 7.4 to study Turing machines and
to obtain some more accurate results with respect to those obtainable by using the
traditional framework [4,44] .
In order to start, let us remind that numerous trials have been done during the
centuries to evolve existing numeral systems in such a way that numerals repre-
senting inﬁnite and inﬁnitesimal numbers could be included in them (see [2, 3, 5,
17,18,25,28,46]). Since new numeral systems appear very rarely, in each concrete
historical period their signiﬁcance for Mathematics is very often underestimated
(especially by pure mathematicians). In order to illustrate their importance, let us
remind the Roman numeral system that does not allow one to express zero and neg-
ative numbers. In this system, the expression III-X is an indeterminate form. As
a result, before appearing the positional numeral system and inventing zero math-
ematicians were not able to create theorems involving zero and negative numbers
and to execute computations with them.
There exist numeral systems that are even weaker than the Roman one. They se-
riously limit their users in executing computations. Let us recall a study published
recently in Science (see [11]). It describes a primitive tribe living in Amazonia (Pi-
rah˜a). These people use a very simple numeral system for counting: one, two, many.
For Pirah˜a, all quantities larger than two are just ‘many’ and such operations as 2+2
and 2+1 give the same result, i.e., ‘many’. Using their weak numeral system Pirah˜a
are not able to see, for instance, numbers 3, 4, 5, and 6, to execute arithmetical op-
erations with them, and, in general, to say anything about these numbers because in
their language there are neither words nor concepts for that.
In the context of the present chapter, it is very important that the weakness of
Pirah˜a’s numeral system leads them to such results as
‘many’+ 1 = ‘many’,
‘many’+ 2 = ‘many’,
(7.6)
which are very familiar to us in the context of views on inﬁnity used in the traditional
calculus
∞+ 1 = ∞,
∞+ 2 = ∞.
(7.7)
The arithmetic of Pirah˜a involving the numeral ‘many’ has also a clear similarity
with the arithmetic proposed by Cantor for his Alephs2:
ℵ0 +1 = ℵ0,
ℵ0 +2 = ℵ0,
ℵ1+1 = ℵ1,
ℵ1+2 = ℵ1. (7.8)
2 This similarity becomes even more pronounced if one considers another Amazonian tribe –
Munduruk´u (see [26]) – who fail in exact arithmetic with numbers larger than 5 but are able
to compare and add large approximate numbers that are far beyond their naming range.
Particularly, they use the words ‘some, not many’ and ‘many, really many’ to distinguish
two types of large numbers using the rules that are very similar to ones used by Cantor to
operate with ℵ0 and ℵ1, respectively.

148
Y.D. Sergeyev and A. Garro
Thus, the modern mathematical numeral systems allow us to distinguish a larger
quantity of ﬁnite numbers with respect to Pirah˜a but give results that are similar to
those of Pirah˜a when we speak about inﬁnite quantities. This observation leads us to
the following idea: Probably our difﬁculties in working with inﬁnity is not connected
to the nature of inﬁnity itself but is a result of inadequate numeral systems that we
use to work with inﬁnity, more precisely, to express inﬁnite numbers.
The approach developed in [30, 32, 37] proposes a numeral system that uses the
same numerals for several different purposes for dealing with inﬁnities and in-
ﬁnitesimals: in Analysis for working with functions that can assume different in-
ﬁnite, ﬁnite, and inﬁnitesimal values (functions can also have derivatives assuming
different inﬁnite or inﬁnitesimal values); for measuring inﬁnite sets; for indicat-
ing positions of elements in ordered inﬁnite sequences ; in probability theory, etc.
(see [7,8,13,22,29,31,33–36,38–40,45,47,48]). It is important to emphasize that
the new numeral system avoids situations of the type (7.6)–(7.8) providing results
ensuring that if a is a numeral written in this system then for any a (i.e., a can be
ﬁnite, inﬁnite, or inﬁnitesimal) it follows a + 1 > a.
The new numeral system works as follows. A new inﬁnite unit of measure ex-
pressed by the numeral ①called grossone is introduced as the number of elements
of the set, N, of natural numbers. Concurrently with the introduction of grossone
in the mathematical language all other symbols (like ∞, Cantor’s ω, ℵ0,ℵ1,...,
etc.) traditionally used to deal with inﬁnities and inﬁnitesimals are excluded from
the language because grossone and other numbers constructed with its help not
only can be used instead of all of them but can be used with a higher accuracy3.
Grossone is introduced by describing its properties postulated by the Inﬁnite Unit
Axiom (see [32, 37]) added to axioms for real numbers (similarly, in order to pass
from the set, N, of natural numbers to the set, Z, of integers a new element – zero
expressed by the numeral 0 – is introduced by describing its properties) .
The new numeral ①allows us to construct different numerals expressing different
inﬁnite and inﬁnitesimal numbers and to execute computations with them. Let us
give some examples. For instance, in Analysis, indeterminate forms are not present
and, for example, the following relations hold for ①and ①−1 (that is inﬁnitesimal),
as for any other (ﬁnite, inﬁnite, or inﬁnitesimal) number expressible in the new
numeral system
0 ·①= ①·0 = 0,
①−①= 0,
①
①= 1,
①0 = 1,
1
①= 1,
0
①= 0,
(7.9)
0 ·①−1 = ①−1 ·0 = 0,
①−1 > 0,
①−2 > 0,
①−1 −①−1 = 0,
(7.10)
①−1
①−1 = 1,
①−2
①−2 = 1,
(①−1)0 = 1,
①·①−1 = 1,
①·①−2 = ①−1. (7.11)
The new approach gives the possibility to develop a new Analysis (see [35])
where functions assuming not only ﬁnite values but also inﬁnite and inﬁnitesimal
3 Analogously, when the switch from Roman numerals to the Arabic ones has been done,
numerals X, V, I, etc. have been excluded from records using Arabic numerals.

7
A New Perspective on Turing Machines
149
ones can be studied. For all of them it becomes possible to introduce a new notion
of continuity that is closer to our modern physical knowledge. Functions assuming
ﬁnite and inﬁnite values can be differentiated and integrated.
By using the new numeral system it becomes possible to measure certain inﬁnite
sets and to see, e.g., that the sets of even and odd numbers have ①/2 elements
each. The set, Z, of integers has 2①+1 elements (①positive elements, ①negative
elements, and zero). Within the countable sets and sets having cardinality of the
continuum (see [20, 36, 37]) it becomes possible to distinguish inﬁnite sets having
different number of elements expressible in the numeral system using grossone and
to see that, for instance,
①
2 < ①−1 < ①< ①+ 1 < 2①+ 1 < 2①2 −1 < 2①2 < 2①2 + 1 <
2①2 + 2 < 2①−1 < 2①< 2①+ 1 < 10①< ①①−1 < ①①< ①①+ 1.
(7.12)
Another key notion for our study of Turing machines is that of inﬁnite sequence.
Thus, before considering the notion of the Turing machine from the point of view
of the new methodology, let us explain how the notion of the inﬁnite sequence can
be viewed from the new positions.
7.3.1
Inﬁnite Sequences
Traditionally, an inﬁnite sequence {an},an ∈A, n ∈N, is deﬁned as a function hav-
ing the set of natural numbers, N, as the domain and a set A as the codomain. A
subsequence {bn} is deﬁned as a sequence {an} from which some of its elements
have been removed . In spite of the fact that the removal of the elements from {an}
can be directly observed, the traditional approach does not allow one to register, in
the case where the obtained subsequence {bn} is inﬁnite, the fact that {bn} has less
elements than the original inﬁnite sequence {an}.
Let us study what happens when the new approach is used. From the point of
view of the new methodology, an inﬁnite sequence can be considered in a dual way:
either as an object of a mathematical study or as a mathematical instrument devel-
oped by human beings to observe other objects and processes. First, let us consider
it as a mathematical object and show that the deﬁnition of inﬁnite sequences should
be done more precise within the new methodology. In the ﬁnite case, a sequence
a1,a2,...,an has n elements and we extend this deﬁnition directly to the inﬁnite
case saying that an inﬁnite sequence a1,a2,...,an has n elements where n is ex-
pressed by an inﬁnite numeral such that the operations with it satisfy the Postulate 3
of the Grossone methodology4. Then the following result (see [30,32]) holds. We
reproduce here its proof for the sake of completeness.
4 The Postulate 3 states: The part is less than the whole is applied to all numbers (ﬁnite,
inﬁnite, and inﬁnitesimal) and to all sets and processes (ﬁnite and inﬁnite), see [30].

150
Y.D. Sergeyev and A. Garro
Theorem 1. The number of elements of any inﬁnite sequence is less or equal to ①.
Proof. The new numeral system allows us to express the number of elements of
the set N as ①. Thus, due to the sequence deﬁnition given above, any sequence
having N as the domain has ①elements.
The notion of subsequence is introduced as a sequence from which some of its
elements have been removed. This means that the resulting subsequence will have
less elements than the original sequence. Thus, we obtain inﬁnite sequences having
the number of members less than grossone.
2
It becomes appropriate now to deﬁne the complete sequence as an inﬁnite se-
quence containing ①elements . For example, the sequence of natural numbers is
complete, the sequences of even and odd natural numbers are not complete because
they have ①
2 elements each (see [30,32]). Thus, the new approach imposes a more
precise description of inﬁnite sequences than the traditional one: to deﬁne a se-
quence {an} in the new language, it is not sufﬁcient just to give a formula for an, we
should determine (as it happens for sequences having a ﬁnite number of elements)
its number of elements and/or the ﬁrst and the last elements of the sequence. If the
number of the ﬁrst element is equal to one, we can use the record {an : k} where an
is, as usual, the general element of the sequence and k is the number (that can be
ﬁnite or inﬁnite) of members of the sequence; the following example clariﬁes these
concepts.
Example 1. Let us consider the following three sequences:
{an : ①} = {4,
8,
...
4(①−1),
4①};
(7.13)
{bn : ①
2 −1} = {4,
8,
...
4(①
2 −2),
4(①
2 −1)};
(7.14)
{cn : 2①
3 } = {4,
8,
...
4(2①
3 −1),
42①
3 }.
(7.15)
The three sequences have an = bn = cn = 4n but they are different because they
have different number of members. Sequence {an} has ①elements and, therefore,
is complete, {bn} has ①
2 −1, and {cn} has 2①
3 elements.
2
Let us consider now inﬁnite sequences as one of the instruments used by math-
ematicians to study the world around us and other mathematical objects and pro-
cesses. The ﬁrst immediate consequence of Theorem 1 is that any sequential
process can have at maximum ①elements. This means that a process of sequen-
tial observations of any object cannot contain more than ①steps5. We are not able
5 It is worthy to notice a deep relation of this observation to the Axiom of Choice. Since
Theorem 1 states that any sequence can have at maximum ①elements, so this fact holds
for the process of a sequential choice, as well. As a consequence, it is not possible to
choose sequentially more than ①elements from a set. This observation also emphasizes
the fact that the parallel computational paradigm is signiﬁcantly different with respect to
the sequential one because p parallel processes can choose p·①elements from a set.

7
A New Perspective on Turing Machines
151
to execute any inﬁnite process physically but we assume the existence of such a pro-
cess; moreover, only a ﬁnite number of observations of elements of the considered
inﬁnite sequence can be executed by a human who is limited by the numeral system
used for the observation. Indeed, we can observe only those members of a sequence
for which there exist the corresponding numerals in the chosen numeral system; to
better clarify this point the following example is discussed.
Example 2. Let us consider the numeral system, P, of Pirah˜a able to express only
numbers 1 and 2. If we add to P the new numeral ①, we obtain a new numeral
system (we call it P). Let us consider now a sequence of natural numbers {n : ①}.
It goes from 1 to ①(note that both numbers, 1 and ①, can be expressed by numerals
from P). However, the numeral system P is very weak and it allows us to observe
only ten numbers from the sequence {n : ①} represented by the following numerals
1,2

finite
,
...
①
2 −2, ①
2 −1, ①
2 , ①
2 + 1, ①
2 + 2



in finite
,
...
①−2,①−1,①



in finite
. (7.16)
The ﬁrst two numerals in (7.16) represent ﬁnite numbers, the remaining eight nu-
merals express inﬁnite numbers, and dots represent members of the sequence of
natural numbers that are not expressible in P and, therefore, cannot be observed if
one uses only this numeral system for this purpose.
2
In the light of the limitations concerning the process of sequential observations,
the researcher can choose how to organize the required sequence of observations
and which numeral system to use for it, deﬁning so which elements of the object
he/she can observe. This situation is exactly the same as in natural sciences: before
starting to study a physical object, a scientist chooses an instrument and its accuracy
for the study.
Example 3. Let us consider the set A={1,2,3,...,2①-1,2①} as an object of our
observation. Suppose that we want to organize the process of the sequential counting
of its elements. Then, due to Theorem 1, starting from the number 1 this process can
arrive at maximum to ①. If we consider the complete counting sequence {n : ①},
then we obtain
1,2, 3, 4, ... ①−2,①−1,①,①+1,①+2,①+3,...,2①−1,2①
↶
↶
↶
↶
↶
↶
↶



①steps
(7.17)
Analogously, if we start the process of the sequential counting from 5, the process
arrives at maximum to ①+ 4:
1,2,3,4,5 ... ①−1,①,①+1,①+2,①+3,①+4,①+5,...,2①−1,2①
↶
↶
↶
↶
↶
↶
↶



①steps
(7.18)

152
Y.D. Sergeyev and A. Garro
The corresponding complete sequence used in this case is {n + 4 : ①}. We can also
change the length of the step in the counting sequence and consider, for instance,
the complete sequence {2n −1 : ①}:
1,2,3,4, ... ①−1,①,①+1,①+2, ... 2①−3,2①−2,2①−1,2①
↶
↶
↶
↶
↶
↶
↶



①steps
(7.19)
If we use again the numeral system P, then among ﬁnite numbers it allows us to see
only number 1 because already the next number in the sequence, 3, is not expressible
in P. The last element of the sequence is 2①−1 and P allows us to observe it.
2
The introduced deﬁnition of the sequence allows us to work not only with the
ﬁrst but with any element of any sequence if the element of our interest is express-
ible in the chosen numeral system independently whether the sequence under our
study has a ﬁnite or an inﬁnite number of elements. Let us use this new deﬁnition
for studying inﬁnite sets of numerals, in particular, for calculating the number of
points at the interval [0,1) (see [30,32]). To do this we need a deﬁnition of the term
‘point’ and mathematical tools to indicate a point. If we accept (as is usually done
in modern Mathematics) that a point A belonging to the interval [0,1) is determined
by a numeral x, x ∈S, called coordinate of the point A where S is a set of numerals,
then we can indicate the point A by its coordinate x and we are able to execute the
required calculations.
It is worthwhile to emphasize that giving this deﬁnition we have not used the
usual formulation “x belongs to the set, R, of real numbers”. This has been done be-
cause we can express coordinates only by numerals and different choices of numeral
systems lead to different sets of numerals and, as a result, to different sets of num-
bers observable through the chosen numerals. In fact, we can express coordinates
only after we have ﬁxed a numeral system (our instrument of the observation) and
this choice deﬁnes which points we can observe, namely, points having coordinates
expressible by the chosen numerals. This situation is typical for natural sciences
where it is well known that instruments inﬂuence the results of observations. Re-
mind the work with a microscope: we decide the level of the precision we need and
obtain a result which is dependent on the chosen level of accuracy. If we need a
more precise or a more rough answer, we change the lens of our microscope.
We should decide now which numerals we shall use to express coordinates of
the points. After this choice we can calculate the number of numerals expressible
in the chosen numeral system and, as a result, we obtain the number of points at
the interval [0,1). Different variants (see [30, 32]) can be chosen depending on the
precision level we want to obtain. For instance, we can choose a positional numeral
system with a ﬁnite radix b that allows us to work with numerals
(0.a1a2...a(①−1)a①)b,
ai ∈{0,1,...b −2,b −1},
1 ≤i ≤①.
(7.20)

7
A New Perspective on Turing Machines
153
Then, the number of numerals (7.20) gives us the number of points within the in-
terval [0,1) that can be expressed by these numerals. Note that a number using the
positional numeral system (7.20) cannot have more than grossone digits (contrarily
to sets discussed in Example 3) because a numeral having g > ①digits would not
be observable in a sequence. In this case (g > ①) such a record becomes useless in
sequential computations because it does not allow one to identify numbers entirely
since g −①numerals remain non observed.
Theorem 2. If coordinates of points x ∈[0,1) are expressed by numerals (7.20),
then the number of the points x over [0,1) is equal to b①.
Proof. In the numerals (7.20) there is a sequence of digits, a1a2 ...a(①−1)a①, used
to express the fractional part of the number. Due to the deﬁnition of the sequence
and Theorem 1, any inﬁnite sequence can have at maximum ①elements. As a result,
there is ①positions on the right of the dot that can be ﬁlled in by one of the b digits
from the alphabet {0,1,...,b −1} that leads to b①possible combinations. Hence,
the positional numeral system using the numerals of the form (7.20) can express b①
numbers.
2
Corollary 1. The number of numerals
(a1a2a3 ...a①−2a①−1a①)b,
ai ∈{0,1,...b −2,b −1},
1 ≤i ≤①,
(7.21)
expressing integers in the positional system with a ﬁnite radix b in the alphabet
{0,1,...b −2,b −1} is equal to b①.
Proof. The proof is a straightforward consequence of Theorem 2 and is so
omitted.
2
Corollary 2. If coordinates of points x ∈(0,1) are expressed by numerals (7.20),
then the number of the points x over (0,1) is equal to b①−1.
Proof. The proof follows immediately from Theorem 2.
2
Note that Corollary 2 shows that it becomes possible now to observe and to reg-
ister the difference of the number of elements of two inﬁnite sets (the interval [0,1)
and the interval (0,1), respectively) even when only one element (the point 0, ex-
pressed by the numeral 0.00...0 with ①zero digits after the decimal point) has been
excluded from the ﬁrst set in order to obtain the second one.
7.4
Observing Turing Machines through the Lens of the
Grossone Methodology
In this Section the different types of Turing machines introduced in Section 7.2 are
analyzed and observed by using as instruments of observation the Grossone lan-
guage and methodology presented in Section 7.3 . In particular, after introducing
a distiction between physical and ideal Turing machine (see Section 7.4.1), some

154
Y.D. Sergeyev and A. Garro
results for Single-tape and Multi-tape Turing machines are summarized (see Sec-
tions 7.4.2 and 7.4.3 respectively), then a discussion about the equivalence between
Single and Multi-tape Turing machine is reported in Section 7.4.4. Finally, a com-
parison between deterministic and non-deterministic Turing machines through the
lens of the Grossone methodology is presented in Section 7.4.5.
7.4.1
Physical and Ideal Turing Machines
Before starting observing Turing machines by using the Grossone methodology, it
is useful to recall the main results showed in the previous Section: (i) a (complete)
sequence can have maximum ①elements; (ii) the elements which we are able to
observe in this sequence depend on the adopted numeral system. Moreover, a distic-
tion between physical and ideal Turing machines should be introduced. Speciﬁcally,
the machines deﬁned in Section 7.2 (e.g. the Single-Tape Turing machine of Section
7.2.1) are called ideal Turing machine, T I . Howerver, in order to study the limita-
tions of practical automatic computations, we also consider machines, T P, that can
be constructed physically. They are identical to T I but are able to work only a ﬁnite
time and can produce only ﬁnite outputs. In this Section, both kinds of machines
are analyzed from the point of view of their outputs, called by Turing ‘computable
numbers’ or ‘computable,sequences’, and from the point of view of computations
that the machines can execute .
Let us consider ﬁrst a physical machine T P and discuss about the number of
computational steps it can execute and how the obtained results then can be inter-
preted by a human observer (e.g. the researcher) . We suppose that its output is
written on the tape using an alphabet Σ containing b symbols {0,1,...b −2,b −1}
where b is a ﬁnite number (Turing uses b = 10).Thus, the output consists of a se-
quence of digits that can be viewed as a number in a positional system B with the
radix b. By deﬁnition, T P should stop after a ﬁnite number of iterations. The mag-
nitude of this value depends on the physical construction of the machine, the way
the notion ‘iteration’ has been deﬁned, etc., but in any case this number is ﬁnite.
A physical machine T P stops in two cases: (i) it has ﬁnished the execution of
its program and stops; (ii) it stops because its breakage. In both cases the output
sequence
(a1a2a3...ak−1,ak)b,
ai ∈{0,1,...b −2,b −1},
1 ≤i ≤k,
of T P has a ﬁnite length k.
If the maximal length of the output sequence that can be computed by T P is
equal to a ﬁnite number KT P, then it follows k ≤KT P. This means that there exist
problems that cannot be solved by T P if the length of the output outnumbers KT P.
If a physical machine T P has stopped after it has printed KT P symbols, then it is
not clear whether the obtained output is a solution or just a result of the depletion of
its computational resources.

7
A New Perspective on Turing Machines
155
In particular, with respect to the halting problem it follows that all algorithms
stop on T P.
In order to be able to read and to understand the output, the researcher (the user)
should know a positional numeral system U with an alphabet {0,1,...u −2,u −1}
where u ≥b. Otherwise, the output cannot be decoded by the user. Moreover, the
researcher must be able to observe a number of symbols at least equal to the maximal
length of the output sequence that can be computed by machine (i.e., KU ≥KT P).
If the situation KU < KT P holds, then this means that the user is not able to
interpret the obtained result. Thus, the number K∗= min{KU,KT P} deﬁnes the
length of the outputs that can be computed and interpreted by the user.
As a consequence, algorithms producing outputs having more than K∗positions
become less interesting from the practical point of view.
After having introduced the distinction between physical and ideal Turing ma-
chines, let us analyze and observe them through the lens of the Grossone Method-
ology. Speciﬁcally, the results obtained and discussed in [42] for deterministic and
non-deterministic Single-tape Turing machines are summarized in Section 7.4.2 and
7.4.4 respectively; whereas, Section 7.4.3 reports additional results for Multi-tape
Turing machines (see [43]).
7.4.2
Observing Single-Tape Turing Machines
As stated in Section 7.4.1, single-tape ideal Turing machines MI (see Section 7.2.1)
can produce outputs with an inﬁnite number of symbols k. However, in order to be
observable in a sequence, an output should have k ≤①(see Section 7.3). Starting
from these considerations the following theorem can be introduced.
Theorem 3. Let M be the number of all possible complete computable sequences
that can be produced by ideal single-tape Turing machines using outputs being nu-
merals in the positional numeral system B. Then it follows M ≤b①.
Proof. This result follows from the deﬁnitions of the complete sequence and the
form of numerals
(a−1a−2...a−(①−1)a−①)b, a−i ∈{0,1,...b −2,b −1}, 1 ≤i ≤①,
that are used in the positional numeral system B.
2
Corollary 3. Let us consider an ideal Turing machine MI
1 working with the alpha-
bet {0,1,2} and computing the following complete computable sequence

156
Y.D. Sergeyev and A. Garro
0,1,2,0,1,2,0,1,2, ... 0,1,2,0,1,2



①positions
.
(7.22)
Then ideal Turing machines working with the output alphabet {0,1} cannot produce
observable in a sequence outputs computing (7.22).
Since the numeral 2 does not belong to the alphabet {0,1} it should be coded by
more than one symbol. One of codiﬁcations using the minimal number of symbols
in the alphabet {0,1} necessary to code numbers 0,1,2 is {00,01,10}. Then the
output corresponding to (7.22) and computed in this codiﬁcation should be
00,01,10,00,01,10,00,01,10, ... 00,01,10,00,01,10.
(7.23)
Since the output (7.22) contains grossone positions, the output (7.23) should con-
tain 2①positions. However, in order to be observable in a sequence, (7.23) should
not have more than grossone positions. This fact completes the proof.
2
The mathematical language used by Turing did not allow one to distinguish these
two machines. Now we are able to distinguish a machine from another also when
we consider inﬁnite sequences. Turing’s results and the new ones do not contra-
dict each other. Both languages observe and describe the same object (computable
sequences) but with different accuracies.
It is not possible to describe a Turing machine (the object of the study) without
the usage of a numeral system (the instrument of the study). As a result, it becomes
not possible to speak about an absolute number of all possible Turing machines T I.
It is always necessary to speak about the number of all possible Turing machines
T I expressible in a ﬁxed numeral system (or in a group of them).
Theorem 4. The maximal number of complete computable sequences produced by
ideal Turing machines that can be enumerated in a sequence is equal to ①.
We have established that the number of complete computable sequences that can
be computed using a ﬁxed radix b is less or equal b①. However, we do not know
how many of them can be results of computations of a Turing machine. Turing es-
tablishes that their number is enumerable. In order to obtain this result, he used the
mathematical language developed by Cantor and this language did not allow him
to distinguish sets having different inﬁnite numbers of elements. The introduction
of grossone gives a possibility to execute a more precise analysis and to distinguish
within enumerable sets inﬁnite sets having different numbers of elements. For in-
stance, the set of even numbers has ①
2 elements and the set of integer numbers has
2①+ 1 elements. If the number of complete computable sequences, MT I, is larger
than ①, then there can be differen sequential processes that enumerate different se-
quences of complete computable sequences. In any case, each of these enumerating
sequential processes cannot contain more than grossone members.

7
A New Perspective on Turing Machines
157
7.4.3
Observing Multi-tape Turing Machines
Before starting to analyze the computations performed by an ideal k-tapes Turing
machine (with k ≥2) MI
K =

Q,Γ , ¯b,Σ,q0,F,δ (k)
(see (7.1), see Section 7.2.2),
it is worth to make some considerations about the process of observation itself in
the light of the Grossone methodology. As discussed above, if we want to observe
the process of computation performed by a Turing machine while it executes an
algorithm, then we have to execute observations of the machine in a sequence of
moments. In fact, it is not possible to organize a continuous observation of the ma-
chine. Any instrument used for an observation has its accuracy and there always be
a minimal period of time related to this instrument allowing one to distinguish two
different moments of time and, as a consequence, to observe (and to register) the
states of the object in these two moments. In the period of time passing between
these two moments the object remains unobservable.
Since our observations are made in a sequence, the process of observations can
have at maximum ①elements. This means that inside a computational process it is
possible to ﬁx more than grossone steps (deﬁned in a way) but it is not possible to
count them one by one in a sequence containing more than grossone elements. For
instance, in a time interval [0,1), up to b①numerals of the type (7.20) can be used
to identify moments of time but not more than grossone of them can be observed
in a sequence. Moreover, it is important to stress that any process itself, considered
independently on the researcher, is not subdivided in iterations, intermediate results,
moments of observations, etc. The structure of the language we use to describe
the process imposes what we can say about the process (see [42] for a detailed
discussion).
On the basis of the considerations made above, we should choose the accuracy
(granularity) of the process of the observation of a Turing machine; for instance we
can choose a single operation of the machine such as reading a symbol from the
tape, or moving the tape, etc. However, in order to be close as much as possible to
the traditional results, we consider an application of the transition function of the
machine as our observation granularity (see Section 7.2).
Moreover, concerning the output of the machine, we consider the symbols written
on all the k tapes of the machine by using, on each tape i, with 1 ≤i ≤k, the
alphabet Σi of the tape, containing bi symbols, plus the blank symbol (¯b). Due to the
deﬁnition of complete sequence (see Section 7.3) on each tape at least ①symbols
can be produced and observed. This means that on a tape i, after the last symbols
belonging to the tape alphabet Σi, if the sequence is not complete (i.e., if it has
less than ①symbols) we can consider a number of blank symbols (¯b) necessary
to complete the sequence. We say that we are considering a complete output of a
k-tapes Turing machine when on each tape of the machine we consider a complete
sequence of symbols belonging to Σi ∪{¯b}.
Theorem 5. Let MI
K =

Q,Γ , ¯b,Σ,q0,F,δ (k)
be an ideal k-tapes, k ≥2, Turing
machine. Then, a complete output of the machine will results in k①symbols.

158
Y.D. Sergeyev and A. Garro
Proof. Due to the deﬁnition of the complete sequence, on each tape at maximum
①symbols can be produced and observed and thus by considering a complete se-
quence on each of the k tapes of the machine the complete output of the machine
will result in k①symbols.
2
Having proved that a complete output that can be produced by a k-tapes Turing
machine results in k①symbols, it is interesting to investigate what part of the com-
plete output produced by the machine can be observed in a sequence taking into
account that it is not possible to observe in a sequence more than ①symbols (see
Section 7.3). As examples, we can decide to make in a sequence one of the follow-
ing observations: (i) ①symbols on one among the k-tapes of the machine, (ii) ①
k
symbols on each of the k-tapes of the machine; (iii) ①
2 symbols on 2 among the
k-tapes of the machine, an so on.
Theorem 6. Let MI
K =

Q,Γ , ¯b,Σ,q0,F,δ (k)
be an ideal k-tapes, k ≥2, Turing
machine. Let M be the number of all possible complete outputs that can be produced
by MI
K. Then it follows M = ∏k
i=1 (bi + 1)
①.
Proof. Due to the deﬁnition of the complete sequence, on each tape i, with 1 ≤i ≤
k, at maximum ①symbols can be produced and observed by using the bi symbols
of the alphabet Σi of the tape plus the blank symbol (¯b); as a consequence, the
number of all the possible complete sequences that can be produced and observed
on a tape i is (bi + 1)
①. A complete output of the machine is obtained by considering
a complete sequence on each of the the k-tapes of the machine, thus by considering
all the possible complete sequences that can be produced and observed on each of
the k tapes of the machine, the number M of all the possible complete outputs will
results in ∏k
i=1 (bi + 1)
①.
2
As the number M = ∏k
i=1(bi + 1)
①of complete outputs that can be produced
by MK is larger than grossone, then there can be different sequential enumerating
processes that enumerate complete outputs in different ways, in any case, each of
these enumerating sequential processes cannot contain more than grossone members
(see Section 7.3).
7.4.4
Comparing Different Multi-tape Machines and Multi and
Single-Tape Machines
In the classical framework ideal k-tape Turing machines have the same computa-
tional power of Single-tape Turing machines and given a Multi-tape Turing machine
MI
K it is always possible to deﬁne a Single-tape Turing machine which is able to
fully simulate its behavior and therefore to completely execute its computations.
As showed for Single-tape Turing machine (see [42]), the Grossone methodology
allows us to give a more accurate deﬁnition of the equivalence among different ma-
chines as it provides the possibility not only to separate different classes of inﬁnite
sets with respect to their cardinalities but also to measure the number of elements of
some of them. With reference to Multi-tape Turing machines, the Single-tape Turing

7
A New Perspective on Turing Machines
159
machines adopted for their simulation use a particular kind of tape which is divided
into tracks (multi-track tape). In this way, if the tape has m tracks, the head is able
to access (for reading and/or writing) all the m characters on the tracks during a
single operation. This tape organization leads to a straightforward deﬁnition of the
behavior of a Single-tape Turing machine able to completely execute the compu-
tations of a given Multi-tape Turing machine (see Section 7.2.2). However, the so
deﬁned Single-tape Turing machine MI, to simulate t computational steps of MI
K,
needs to execute O(t2) transitions (t2 +t in the worst case) and to use an alphabet
with 2k(|Σ1| + 1)∏k
i=2(|Σi| + 2) symbols (again see Section 7.2.2). By exploiting
the Grossone methodology is is possibile to obtain the following result that has a
higher accuracy with respect to that provided by the traditional framework.
Theorem 7. Let us consider MI
K =

Q,Γ , ¯b,Σ,q0,F,δ (k)
,a k-tapes, k ≥2, Turing
machine, where Σ = k
i=1 Σi is given by the union of the symbols in the k tape
alphabets Σ1,...,Σk and Γ = Σ ∪{¯b}. If this machine performs t computational
steps such that
t ⩽1
2(
√
4①+ 1−1),
(7.24)
then there exists MI
1 = {Q′,Γ ′, ¯b,Σ′,q′
0,F′,δ ′}, an equivalent Single-tape Turing
machine with |Γ ′| = 2k(|Σ1|+ 1)∏k
i=2(|Σi|+ 2), which is able to simulate MI
K and
can be observed in a sequence.
Proof. Let us recall that the deﬁnition of MI
1 requires for a Single-tape to be
divided into 2k tracks; k tracks for storing the characters in the k tapes of MI
K
and k tracks for signing through the marker ↓the positions of the k heads on the k
tapes of MI
k (see Section 7.2.2). The transition function δ (k) of the k-tapes machine
is given by δ (k)(q1,ai1,...,aik) = (q j,a j1,...,a jk,zj1,...,zjk), with zj1,...,zjk ∈
{R,L,N}; as a consequence the corresponding transition function δ ′ of the Single-
tape machine, for each transition speciﬁed by δ (k) must individuate the current state
and the position of the marker for each track and then write on the tracks the required
symbols, move the markers and go in another internal state. For each computational
step of MI
K, MI
1 must execute a sequence of steps for covering the portion of tapes
between the two most distant markers. As in each computational step a marker can
move at most of one cell and then two markers can move away each other at most
of two cells, after t steps of MI
K the markers can be at most 2t cells distant, thus
if MI
K executes t steps, MI
1 executes at most: 2∑t
i=1 i = t2 +t steps. In order to be
observable in a sequence the number t2 +t of steps, performed by MI
1 to simulate t
steps of MI
K, must be less than or equal to ①. Namely, it should be t2 +t ⩽①. The
fact that this inequality is satisﬁed for t ⩽1
2(
√
4①+ 1−1) completes the proof. 2

160
Y.D. Sergeyev and A. Garro
7.4.5
Comparing Deterministic and Non-deterministic Turing
Machines
Let us discuss the traditional and new results regarding the computational power of
deterministic and non-deterministic Turing machines.
Classical results show an exponential growth of the time required for reaching a
ﬁnal conﬁguration by the deterministic Turing machine MD with respect to the time
required by the non-deterministic Turing machine MN, assuming that the time re-
quired for both machines for a single step is the same. However, in the classic theory
on Turing machines it is not known if there is a more efﬁcient simulation of MN.
In other words, it is an important and open problem of Computer Science theory to
demonstrate that it is not possible to simulate a non-deterministic Turing machine
by a deterministic Turing machine with a sub-exponential numbers of steps.
Let us now return to the new mathematical language. Since the main interest to
non-deterministic Turing machines (7.5) is related to their theoretical properties,
hereinafter we start by a comparison of ideal deterministic Turing machines, T I,
with ideal non-deterministic Turing machines T IN . Physical machines T P and
T PN are considered at the end of this section. By taking into account the results of
Section 7.4.4, the proposed approach can be applied both to single and multi-tape
machines, however, single-tape machines are considered in the following.
Due to the analysis made in Section 7.4.3, we should choose the accuracy (gran-
ularity) of processes of observation of both machines, T I and T IN . In order to be
close as much as possible to the traditional results, we consider again an applica-
tion of the transition function of the machine as our observation granularity. With
respect to T IN this means that the nodes of the computational tree are observed.
With respect to T I we consider sequences of such nodes. For both cases the ini-
tial conﬁguration is not observed, i.e., we start our observations from level 1 of the
computational tree.
This choice of the observation granularity is particularly attractive due to its ac-
cordance with the traditional deﬁnitions of Turing machines (see deﬁnitions (7.1)
and (7.5)). A more ﬁne granularity of observations allowing us to follow internal
operations of the machines can be also chosen but is not so convenient. In fact, such
an accuracy would mix internal operations of the machines with operations of the
algorithm that is executed. A coarser granularity could be considered, as well. For
instance, we could deﬁne as a computational step two consecutive applications of
the transition function of the machine. However, in this case we do not observe all
the nodes of the computational tree. As a consequence, we could miss some results
of the computation as the machine could reach a ﬁnal conﬁguration before complet-
ing an observed computational step and we are not able to observe when and on
which conﬁguration the machine stopped. Then, ﬁxed the chosen level of granular-
ity the following result holds immediately.
Theorem 8. (i) With the chosen level of granularity no more than ①computational
steps of the machine T I can be observed in a sequence. (ii) In order to give possi-
bility to observe at least one computational path of the computational tree of T IN

7
A New Perspective on Turing Machines
161
Fig. 7.2 The maximum number of computational steps of the machine T I that can be
observed in a sequence
from the level 1 to the level k, the depth, k ≥1, of the computational tree cannot be
larger than grossone, i.e., k ≤①.
Proof. Both results follow from the analysis made in Section 7.3.1 and Theo-
rem 1.
2
In Figure 7.2 the ﬁrst result of Theorem 8 concerning the maximum number
of computational steps of the machine T I that can be observed in a sequence is
exempliﬁed with reference to the computational tree of the machine introduced in
Section 7.2.3.
Similarly, the second result of Theorem 8 concerning the depth of the computational
tree of T IN is exempliﬁed in Figure 7.3.
Corollary 4. Suppose that d is the non-deterministic degree of T IN and S is the
number of leaf nodes of the computational tree with a depth k representing the pos-
sible results of the computation of T IN . Then it is not possible to observe all S
possible results of the computation of T IN if the computational tree of T IN is
complete and dk >①.
Proof. For the number of leaf nodes of the tree, S, of a generic non-deterministic
Turing machine T IN the estimate S ≤dk holds. In particular, S = dk if the computa-
tional tree is complete, that is our case. On the other hand, it follows from Theorem 1
that any sequence of observations cannot have more than grossone elements. As a
consequence, the same limitation holds for the sequence of observations of the leaf
nodes of the computational tree. This means that we are not able to observe all the
possible results of the computation of our non-deterministic Turing machine T IN
if dk >①.
2

162
Y.D. Sergeyev and A. Garro
Fig. 7.3 An observable computational path of the machine T I
In Figure 7.4 the result of Corollary 4 concerning the maximum number of com-
putational results of the machine T I that can be observed in a sequence is exem-
pliﬁed with reference to the computational tree of the machine introduced in Sec-
tion 7.2.3 .
Corollary 5. Any sequence of observations of the nodes of the computational tree
of a non-deterministic Turing machine T IN cannot observe all the nodes of the tree
if the number of nodes N is such that N >①.
Proof. The corollary follows from Theorems 1, 8, and Corollary 4.
2
These results lead to the following theorem again under the same assumption
about the chosen level of granularity of observations, i.e., the nodes of the compu-
tational tree of T IN representing conﬁgurations of the machine are observed.
Theorem 9. Given a non-deterministic Turing machine T IN with a depth, k, of the
computational tree and with a non-deterministic degree d such that
d(kdk+1 −(k + 1)dk + 1)
(d −1)2
⩽①,
(7.25)

7
A New Perspective on Turing Machines
163
Fig. 7.4 Observable results of of the machine T I
then there exists an equivalent deterministic Turing machine T I which is able to
simulate T IN and can be observed.
Proof. For simulating T IN , the deterministic machine T I executes a breadth-
ﬁrst visit of the computational tree of T IN . In this computational tree, whose depth
is 1 ⩽k ⩽①, each node has, by deﬁnition, a number of children c where 0 ⩽c ⩽d.
Let us suppose that the tree is complete, i.e., each node has c = d children. In this
case the tree has dk leaf nodes and d j computational paths of length j for each
1 ⩽j ⩽k. Thus, for simulating all the possible computations of T IN , i.e., for a
complete visiting the computational tree of T IN and considering all the possible
computational paths consisting of j computational steps for each 1 ⩽j ⩽k, the
deterministic machine T I will execute
KT I =
k
∑
j=1
jd j
(7.26)
steps (note that if the computational tree of T IN is not complete, T I will execute
less than KT I). Due to Theorems 1 and 8, and Corollary 5, it follows that in order
to prove the theorem it is sufﬁcient to show that under conditions of the theorem it
follows that
KT I ⩽①.
(7.27)

164
Y.D. Sergeyev and A. Garro
To do this let us use the well known formula
k
∑
j=0
d j = dk+1 −1
d −1 ,
(7.28)
and derive both parts of (7.28) with respect to d. As the result we obtain
k
∑
j=1
jd j−1 = kdk+1 −(k + 1)dk + 1
(d −1)2
.
(7.29)
Notice now that by using (7.26) it becomes possible to represent the number KT I
as
KT I =
k
∑
j=1
jd j = d
k
∑
j=1
jd j−1.
This representation together with (7.29) allow us to write
KT I = d(kdk+1 −(k + 1)dk + 1)
(d −1)2
(7.30)
Due to assumption (7.25), it follows that (7.27) holds. This fact concludes the proof
of the theorem.
2
Corollary 6. Suppose that the length of the input sequence of symbols of a non-
deterministic Turing machine T IN is equal to a number n and T IN has a complete
computational tree with the depth k such that k = nl, i.e., polynomially depends on
the length n. Then, if the values d,n, and l satisfy the following condition
d(nldnl+1 −(nl + 1)dnl + 1)
(d −1)2
⩽①,
(7.31)
then: (i) there exists a deterministic Turing machine T I that can be observed and
able to simulate T IN ; (ii) the number, KT I, of computational steps required to a
deterministic Turing machine T I to simulate T IN for reaching a ﬁnal conﬁgura-
tion exponentially depends on n.
Proof. The ﬁrst assertion follows immediately from theorem 9. Let us prove the
second assertion. Since the computational tree of T IN is complete and has the
depth k, the corresponding deterministic Turing machine T I for simulating T IN
will execute KT I steps where KT I is from (7.27). Since condition (7.31) is satisﬁed
for T IN , we can substitute k = nl in (7.30). As the result of this substitution and
(7.31) we obtain that
KT I = d(nldnl+1 −(nl + 1)dnl + 1)
(d −1)2
⩽①,
(7.32)

7
A New Perspective on Turing Machines
165
i.e., the number of computational steps required to the deterministic Turing machine
T I to simulate the non-deterministic Turing machine T IN for reaching a ﬁnal
conﬁguration is KT I ⩽①and this number exponentially depends on the length of
the sequence of symbols provided as input to T IN .
2
Results described in this section show that the introduction of the new mathemat-
ical language including grossone allows us to perform a more subtle analysis with
respect to traditional languages and to introduce in the process of this analysis the
ﬁgure of the researcher using this language (more precisely, to emphasize the pres-
ence of the researcher in the process of the description of automatic computations).
These results show that there exist limitations for simulating non-deterministic Tur-
ing machines by deterministic ones. These limitations can be viewed now thanks to
the possibility (given because of the introduction of the new numeral ①) to observe
ﬁnal points of sequential processes for both cases of ﬁnite and inﬁnite processes.
Theorems 8, 9, and their corollaries show that the discovered limitations and
relations between deterministic and non-deterministic Turing machines have strong
links with our mathematical abilities to describe automatic computations and to con-
struct models for such descriptions. Again, as it was in the previous cases studied
in this chapter, there is no contradiction with the traditional results because both
approaches give results that are correct with respect to the languages used for the
respective descriptions of automatic computations.
We conclude this section by the note that analogous results can be obtained for
physical machines T P and T PN , as well. In the case of ideal machines, the pos-
sibility of observations was limited by the mathematical languages. In the case of
physical machines they are limited also by technical factors (we remind again the
analogy: the possibilities of observations of physicists are limited by their instru-
ments). In any given moment of time the maximal number of iterations, Kmax, that
can be executed by physical Turing machines can be determined. It depends on the
speed of the fastest machine T P available at the current level of development of
the humanity, on the capacity of its memory, on the time available for simulating
a non-deterministic machine, on the numeral systems known to human beings, etc.
Together with the development of technology this number will increase but it will
remain ﬁnite and ﬁxed in any given moment of time. As a result, theorems pre-
sented in this section can be re-written for T P and T PN by substituting grossone
with Kmax in them.
7.5
Concluding Remarks
Since the beginning of the last century, the fundamental nature of the concept of au-
tomatic computations attracted a great attention of mathematicians and computer
scientists (see [4, 14–16, 23, 24, 27, 44]). The ﬁrst studies had as their reference
context the David Hilbert programme, and as their reference language that intro-
duced by Georg Cantor [3]. These approaches lead to different mathematical mod-
els of computing machines (see [1, 6, 9]) that, surprisingly, were discovered to be
equivalent (e.g., anything computable in the λ-calculus is computable by a Turing

166
Y.D. Sergeyev and A. Garro
machine). Moreover, these results, and expecially those obtained by Alonzo Church,
Alan Turing [4,10,44] and Kurt G¨odel, gave fundamental contributions to demon-
strate that David Hilbert programme, which was based on the idea that all of the
Mathematics could be precisely axiomatized, cannot be realized.
In spite of this fact, the idea of ﬁnding an adequate set of axioms for one or
another ﬁeld of Mathematics continues to be among the most attractive goals for
contemporary mathematicians. Usually, when it is necessary to deﬁne a concept or
an object, logicians try to introduce a number of axioms describing the object in
the absolutely best way. However, it is not clear how to reach this absoluteness;
indeed, when we describe a mathematical object or a concept we are limited by
the expressive capacity of the language we use to make this description. A richer
language allows us to say more about the object and a weaker language – less.
Thus, the continuous development of the mathematical (and not only mathematical)
languages leads to a continuous necessity of a transcription and speciﬁcation of
axiomatic systems. Second, there is no guarantee that the chosen axiomatic system
deﬁnes ‘sufﬁciently well’ the required concept and a continuous comparison with
practice is required in order to check the goodness of the accepted set of axioms.
However, there cannot be again any guarantee that the new version will be the last
and deﬁnitive one. Finally, the third limitation already mentioned above has been
discovered by G¨odel in his two famous incompleteness theorems (see [10]).
Starting from these considerations, in the chapter, Single and Multi-tape Turing
machines have been described and observed through the lens of the Grossone lan-
guage and methodology . This new language, differently from the traditional one,
makes it possible to distinguish among inﬁnite sequences of different length so en-
abling a more accurate description of Single and Multi-tape Turing machines. The
possibility to express the length of an inﬁnite sequence explicitly gives the pos-
sibility to establish more accurate results regarding the equivalence of machines in
comparison with the observations that can be done by using the traditional language.
It is worth noting that the traditional results and those presented in the chapter
do not contradict one another. They are just written by using different mathematical
languages having different accuracies. Both mathematical languages observe and
describe the same objects – Turing machines – but with different accuracies. As a
result, both traditional and new results are correct with respect to the mathematical
languages used to express them and correspond to different accuracies of the obser-
vation. This fact is one of the manifestations of the relativity of mathematical results
formulated by using different mathematical languages in the same way as the usage
of a stronger lens in a microscope gives a possibility to distinguish more objects
within an object that seems to be unique when viewed by a weaker lens.
Speciﬁcally, the Grossone language has allowed us to give the deﬁnition of com-
plete output of a Turing machine, to establish when and how the output of a
machine can be observed, and to establish a more accurate relationship between

7
A New Perspective on Turing Machines
167
Multi-tape and Single-tape Turing machines as well as between deterministic and
non-deterministic ones. Future research efforts will be geared to apply the Grossone
language and methodology to the description and observation of new and emerging
computational paradigms.
References
1. Ausiello, G., D’Amore, F., Gambosi, G.: Linguaggi, modelli, complessit `a, 2nd edn.
Franco Angeli Editore, Milan (2006)
2. Benci, V., Di Nasso, M.: Numerosities of labeled sets: a new way of counting. Advances
in Mathematics 173, 50–67 (2003)
3. Cantor, G.: Contributions to the founding of the theory of transﬁnite numbers. Dover
Publications, New York (1955)
4. Church, A.: An unsolvable problem of elementary number theory. American Journal of
Mathematics 58, 345–363 (1936)
5. Conway, J.H., Guy, R.K.: The Book of Numbers. Springer, New York (1996)
6. Barry Cooper, S.: Computability Theory. Chapman Hall/CRC (2003)
7. De Cosmis, S., De Leone, R.: The use of Grossone in mathematical programming and
operations research. Applied Mathematics and Computation 218(16), 8029–8038 (2012)
8. D’Alotto, L.: Cellular automata using inﬁnite computations. Applied Mathematics and
Computation 218(16), 8077–8082 (2012)
9. Davis, M.: Computability & Unsolvability. Dover Publications, New York (1985)
10. G¨odel, K.: ¨Uber formal unentscheidbare S¨atze der Principia Mathematica und ver-
wandter Systeme. Monatshefte f¨ur Mathematik und Physik 38, 173–198 (1931)
11. Gordon, P.: Numerical cognition without words: Evidence from Amazonia. Science 306,
496–499 (2004)
12. Hopcroft, J., Ullman, J.: Introduction to Automata Theory, Languages and Computation,
1st edn. Addison-Wesley, Reading (1979)
13. Iudin, D.I.: Ya.D. Sergeyev, and M. Hayakawa. Interpretation of percolation in terms
of inﬁnity computations. Applied Mathematics and Computation 218(16), 8099–8111
(2012)
14. Kleene, S.C.: Introduction to metamathematics. D. Van Nostrand, New York (1952)
15. Kolmogorov, A.N.: On the concept of algorithm. Uspekhi Mat. Nauk 8(4), 175–176
(1953)
16. Kolmogorov, A.N., Uspensky, V.A.: On the deﬁnition of algorithm. Uspekhi Mat.
Nauk 13(4), 3–28 (1958)
17. Leibniz, G.W., Child, J.M.: The Early Mathematical Manuscripts of Leibniz. Dover Pub-
lications, New York (2005)
18. Levi-Civita, T.: Sui numeri transﬁniti. Rend. Acc. Lincei, Series 5a 113, 7–91 (1898)
19. Lolli, G.: Metamathematical investigations on the theory of Grossone. To appear in Ap-
plied Mathematics and Computation
20. Lolli, G.: Inﬁnitesimals and inﬁnites in the history of mathematics: A brief survey. Ap-
plied Mathematics and Computation 218(16), 7979–7988 (2012)
21. Margenstern, M.: Using Grossone to count the number of elements of inﬁnite sets and
the connection with bijections. p-Adic Numbers, Ultrametric Analysis and Applica-
tions 3(3), 196–204 (2011)
22. Margenstern, M.: An application of Grossone to the study of a family of tilings of the
hyperbolic plane. Applied Mathematics and Computation 218(16), 8005–8018 (2012)

168
Y.D. Sergeyev and A. Garro
23. Markov Jr., A.A., Nagorny, N.M.: Theory of Algorithms, 2nd edn. FAZIS, Moscow
(1996)
24. Mayberry, J.P.: The Foundations of Mathematics in the Theory of Sets. Cambridge Uni-
versity Press, Cambridge (2001)
25. Newton, I.: Method of Fluxions. 1671
26. Pica, P., Lemer, C., Izard, V., Dehaene, S.: Exact and approximate arithmetic in an ama-
zonian indigene group. Science 306, 499–503 (2004)
27. Post, E.: Finite combinatory processes – formulation 1. Journal of Symbolic Logic 1,
103–105 (1936)
28. Robinson, A.: Non-standard Analysis. Princeton Univ. Press, Princeton (1996)
29. Rosinger, E.E.: Microscopes and telescopes for theoretical physics: How rich locally and
large globally is the geometric straight line? Prespacetime Journal 2(4), 601–624 (2011)
30. Sergeyev, Y.D.: Arithmetic of Inﬁnity. Edizioni Orizzonti Meridionali, CS (2003)
31. Sergeyev, Y.D.: Blinking fractals and their quantitative analysis using inﬁnite and in-
ﬁnitesimal numbers. Chaos, Solitons & Fractals 33(1), 50–75 (2007)
32. Sergeyev, Y.D.: A new applied approach for executing computations with inﬁnite and
inﬁnitesimal quantities. Informatica 19(4), 567–596 (2008)
33. Sergeyev, Y.D.: Evaluating the exact inﬁnitesimal values of area of Sierpinski’s carpet
and volume of Menger’s sponge. Chaos, Solitons & Fractals 42(5), 3042–3046 (2009)
34. Sergeyev, Y.D.: Numerical computations and mathematical modelling with inﬁnite and
inﬁnitesimal numbers. Journal of Applied Mathematics and Computing 29, 177–195
(2009)
35. Sergeyev, Y.D.: Numerical point of view on Calculus for functions assuming ﬁnite, in-
ﬁnite, and inﬁnitesimal values over ﬁnite, inﬁnite, and inﬁnitesimal domains. Nonlinear
Analysis Series A: Theory, Methods & Applications 71(12), e1688–e1707 (2009)
36. Sergeyev, Y.D.: Counting systems and the First Hilbert problem. Nonlinear Analysis
Series A: Theory, Methods & Applications 72(3-4), 1701–1708 (2010)
37. Sergeyev, Y.D.: Lagrange Lecture: Methodology of numerical computations with inﬁni-
ties and inﬁnitesimals. Rendiconti del Seminario Matematico dell’Universit`a e del Po-
litecnico di Torino 68(2), 95–113 (2010)
38. Sergeyev, Y.D.: Higher order numerical differentiation on the inﬁnity computer. Opti-
mization Letters 5(4), 575–585 (2011)
39. Sergeyev, Y.D.: On accuracy of mathematical languages used to deal with the Riemann
zeta function and the Dirichlet eta function. p-Adic Numbers, Ultrametric Analysis and
Applications 3(2), 129–148 (2011)
40. Sergeyev, Y.D.: Using blinking fractals for mathematical modelling of processes of
growth in biological systems. Informatica 22(4), 559–576 (2011)
41. Sergeyev, Y.D.: Solving ordinary differential equations by working with inﬁnitesimals
numerically on the inﬁnity computer. Applied Mathematics and Computation 219(22),
10668–10681 (2013)
42. Sergeyev, Y.D., Garro, A.: Observability of Turing machines: A reﬁnement of the theory
of computation. Informatica 21(3), 425–454 (2010)
43. Sergeyev, Y.D., Garro, A.: Single-tape and Multi-tape Turing Machines through the lens
of the Grossone methodology. The Journal of Supercomputing 65(2), 645–663 (2013)
44. Turing, A.M.: On computable numbers, with an application to the entscheidungsprob-
lem. Proceedings of London Mathematical Society, Series 2 42, 230–265 (1936-1937)
45. Vita, M.C., De Bartolo, S., Fallico, C., Veltri, M.: Usage of inﬁnitesimals in the Menger’s
Sponge model of porosity. Applied Mathematics and Computation 218(16), 8187–8196
(2012)

7
A New Perspective on Turing Machines
169
46. Wallis, J.: Arithmetica inﬁnitorum (1656)
47. Zhigljavsky, A.A.: Computing sums of conditionally convergent and divergent se-
ries using the concept of Grossone. Applied Mathematics and Computation 218(16),
8064–8076 (2012)
48. ˇZilinskas, A.: On strong homogeneity of two global optimization algorithms based on
statistical models of multimodal objective functions. Applied Mathematics and Compu-
tation 218(16), 8131–8136 (2012)

Chapter 8
On Parallel Array P Systems
Linqiang Pan and Gheorghe P˘aun
Abstract. We further investigate the parallel array P systems recently introduced by
K.G. Subramanian, P. Isawasan, I. Venkat, and L. Pan. We ﬁrst make explicit several
classes of parallel array P systems (with one or more axioms, with total or maximal
parallelism, with rules of various types). In this context, some results from the paper
by Subramanian et al. mentioned above are improved. Several open problems are
formulated.
8.1
Introduction
The generality/versatility of membrane computing is already a well known fact, the
computing framework abstracted from the cell structure and functioning can cover
a large variety of processes, dealing – in particular – with a large variety of objects
processed in the compartments of membrane structures. The arrays (in general, two-
dimensional and three-dimensional ﬁgures of various types) are one of the types of
objects considered already since 2001, see [3]. A direct extension from string objects
to two-dimensional arrays was introduced in [1] and then investigated in a series of
papers.
A recent contribution to this research area is [6], where a natural counterpart
of the array P systems from [1] is considered: parallel rewriting of arrays, instead
of the sequential rewriting from [1]. Actually, the kind of parallelism investigated
in [6] is that suggested by Lindenmayer systems: all nonterminals of an array should
be rewritten in each step. A possible alternative, closer to the style of membrane
Linqiang Pan
Key Laboratory of Image Processing and Intelligent Control, School of Automation,
Huazhong University of Science and Technology, Wuhan 430074, Hubei, China
e-mail: lqpan@mail.hust.edu.cn
Gheorghe P˘aun
Institute of Mathematics of the Romanian Academy, P.O. Box 1-764,
014700 Bucures¸ti, Romania
e-mail: gpaun@us.es
c⃝Springer International Publishing Switzerland 2015
171
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_8

172
L. Pan and G. P˘aun
computing, is to consider the maximal parallelism: a multiset of rules is used which
is maximal among the multisets of applicable rules in a given moment.
In the present paper, we explicitly consider these two kinds of parallelism, and
we prove that most of the results from [6] hold true for both kinds of parallelism,
also improving those results (less membranes are used in some of them, while the
powerful priority relation is avoided in other results).
Several questions remain open; several topics for further research are formulated.
8.2
Deﬁnitions and Notations
It is useful for the reader to be familiar with basic elements of membrane computing,
e.g., from [4] (with up-dated information available at [7]), and of array grammars,
but the notions we use will be recalled below. Actually, in what concerns arrays, we
will usually use the pictorial representation, hence we need a minimal formalism
(otherwise, cumbersome if rigorously formulated).
The arrays we consider consist of ﬁnitely many symbols from a speciﬁed alphabet
V placed in the points (we call them pixels) of Z2 (the plane); the points of the
plane which are not marked with elements of V are supposed to be marked with
the blank symbol # /∈V. Given an array W over V, supp(W) denotes the set of
points in Z2 marked with symbols in V. In order to specify an array, it is usual to
specify the pixels of the support, by giving their coordinates, together with their
associated symbols from V, but, as we said above, we will pictorially represent the
arrays, indicating their non-blank pixels. These pictures should be interpreted as
arrays placed in any position of the plane (congruent, possibly to be superposed by
means of a translation).
By V ∗2 we denote the set of all two-dimensional arrays of ﬁnite support over
V, including the empty array, denoted by λ. Any subset of V ∗2 is called an array
language.
We handle the arrays by means of rewriting rules. An array rewriting rule (over
an alphabet V) is written as a usual string rewriting rule, in the form W1 →W2,
where W1,W2 are isotonic arrays overV: W1 and W2 cover the same pixels, no matter
whether they are marked with symbols in V or with #. When graphically represent-
ing an array, usually we ignore the blank pixels, but, when representing rewriting
rules, the pixels marked with # are also explicitly shown. A rule as above is used to
rewrite an array W in the natural way: a position in W is identiﬁed where W1 can be
superposed, with all pixels matching, whether or not they are marked with symbols
in V or with #, and then those pixels are replaced with W2 (the fact that W1 and W2
are isotonic ensures the fact that this replacement is possible). If the result is the
array W ′, we write W =⇒W ′. The reﬂexive and transitive closure of the relation
=⇒is denoted by =⇒∗.
Similar to string rewriting rules, the array productions can be classiﬁed according
to their form. Remember that all rules we work with are isotonic (the shapes of the
left hand side and the right hand side are identical, only the marking, by blank
or non-blank symbols, differs). Note that so far we have not distinguished between

8
On Parallel Array P Systems
173
terminal and nonterminal symbols, like in Chomsky grammars. Yet in the following,
we will work, like in [6], in a Chomsky framework, with terminal and nonterminal
symbols, and consider only two types of rules, context-free and regular ones. A
context-free rule is an isotonic one with only one non-blank pixel in its left-hand side
and only non-blank pixels in its right-hand side. A regular rule over the alphabets
T and N, N being the nonterminal one, is a rule of one of the following forms:
A # →a B, # A →B a, #
A →B
a , A
# →a
B , A →B, A →a, where A,B ∈N and
a ∈T.
Before introducing the array P systems, we recall a notion useful below: two-
dimensional right-linear grammars.
Such a grammar (e.g., see [2]) is a construct G = (Vh,Vv,Vi,T,S,Rh,Rv), where
Vh,Vv,Vi are the horizontal, vertical, and intermediate alphabets of nonterminals,
Vi ⊆Vv, T is the terminal alphabet, S ∈Vh is the axiom, Rh is the ﬁnite set of hori-
zontal rules, of the forms X →AY,X →A, for X,Y ∈Vh,A ∈Vi, and Rv is the ﬁnite
set of vertical rules, of the forms A →aB,A →a, for A,B ∈Vv,a ∈T.
A derivation in G has two phases, a horizontal one, which uses rules from Rh,
and a vertical one, which uses rules from Rv. The horizontal derivation is as usual
in a string grammar. In the vertical phase, the rules are used in parallel, downwards,
with the restriction that the terminal rules are used simultaneously for all vertical
nonterminals. Thus, in the end, a rectangle is obtained, ﬁlled with symbols in T.
The set of all rectangles generated in this way by G is denoted by L(G) and the
family of all languages of this form is denoted by 2RLG.
Two array languages which will be used below are LR, of all hollow rectangles
with the edges marked with a (one element of this language is shown in Fig. 8.1),
and LS, of all hollow squares with the edges marked with a.
a a a a a a a a a
a
a
a
a
a
a
a a a a a a a a a
Fig. 8.1 A hollow rectangle in LR
8.3
Parallel Array P Systems
We now pass to deﬁne the parallel array P systems. Such a device (of degree m ≥1)
is a construct
Π = (V,T,#,μ,F1,...,Fm,R1,...,Rm,io),
where: V is the total alphabet, T ⊆V is the terminal alphabet, # is the blank sym-
bol, μ is a membrane structure with m membranes labeled in a one-to-one way with
1,2,...,m, F1,...,Fm are ﬁnite sets of arrays over V associated with the m regions

174
L. Pan and G. P˘aun
of μ, R1,...,Rm are ﬁnite sets of array rewriting rules over V associated with the m
regions of μ; the rules have attached targets here, out, in (in general, here is omitted),
hence they are of the form W1 →W2(tar); ﬁnally, io is the label of a membrane of μ
specifying the output region.
In what follows, we only consider array P systems with regular (REG) and
context-free (CF) rules – with the symbols in V −T considered as nonterminals.
A computation in an array P system is deﬁned in the same way as in a symbol
object P system, with the following details. Every array from a compartment of the
system must be rewritten by the rules in that compartment. The rewriting is parallel,
with two types of parallelism:
(1) the total one, indicated by allP, which means that all nonterminal symbols
from the array are rewritten, and
(2) the maximal one, indicated by maxP, which means that a multiset of rules is
applied which is maximal, no further rule can be added to it. For any two rules used
simultaneously, no pixel of their left hand sides may overlap (i.e., cover the same
pixel of the rewritten array).
An important point appears here in what concerns the target indications of the
rules: in each compartment, in a step we apply a multiset of rules with the same
target indication. This is a very strong restriction, because it refers to all arrays
from the compartment. In this paper, we work under this restriction. A weaker and
somewhat more natural condition, which remains to be investigated (e.g., are the
results proved below valid also in this case?), is to impose the restriction to use rules
with the same target separately for each rewritten array (thus, separate arrays may
be rewritten by rules with different targets). Of course, the two variants coincide for
systems with only one axiom in the initial conﬁguration.
The arrays obtained by an allP or a maxP rewriting are placed in the region
indicated by the target associated with the used rules, in the way usual in membrane
computing. It is important to stress the fact that all arrays from a given compartment
travel together during a computation.
A computation is successful only if it halts, which means that it reaches a con-
ﬁguration where no rule can be applied to the current arrays. The result of a halting
computation consists of the arrays composed only of symbols from T placed inthe
region with label io in the halting conﬁguration. The set of all such arrays computed
(we also say generated) by a system Π is denoted by A(Π).
Note that a computation which produces a terminal array (hence, no rule can be
applied to it), but still can rewrite another array, is not halting; if the rewriting of one
array continues forever, no matter how many terminal arrays were produced, then
no result is obtained.
By PAPm(axk,α,β) we denote the family of all array languages A(Π) generated
by systems Π as above, with at most m membranes, at most k initial arrays in its
compartments (Σm
i=1card(Fi) ≤k), with rules of type α ∈{REG,CF}, working in
the mode β ∈{allP,maxP}. When m or k is not bounded, then it is replaced with ∗.
The following results were proved in [6] (pri indicates the use of a priority
relation on the rules):

8
On Parallel Array P Systems
175
Lemma 1 (Lemma 3 in [6]). 2RLG ⊆PAP3(ax1,CF,allP).
Lemma 2 (Lemma 4 in [6]). PAP3(ax1,CF,allP)−2RLG ̸= /0.
Lemma 3 (Theorem 3 in [6]). LR ∈PAP2(ax1,REG,allP, pri).
Lemma 4 (Theorem 4 in [6]). LS ∈PAP3(ax1,REG,allP, pri).
In what follows, we will improve the ﬁrst two lemmas in terms of the number of
membranes. Moreover, we can avoid the priority relation in the last two lemmas at
the price of using more than one axiom or using the maxP way of applying the rules.
8.4
Results
We now improve the ﬁrst two lemmas stated above.
Theorem 1. 2RLG ⊆PAP2(ax1,CF,β),β ∈{allP,maxP}.
Proof. Let G = (Vh,Vv,Vi,T,S,Th,Rv) be a two-dimensional right-linear grammar.
We construct the array P system Π indicated in Figure 8.2. The horizontal deriva-
tion is done in membrane 2. When the horizontal phase is completed, the array is
moved to the skin membrane, where the vertical phase is performed. After the use of
the terminal rules, the array is moved back into the inner membrane, where this ter-
minal array is obtained as a result of the computation in G. If the horizontal phase
does not stop, then the computation continues forever, also by means of the rules
A →A,A ∈Vi. Moreover, these rules guarantee that the maxP computations are allP
computations, too. The equality L(G) = A(Π) is clear.
Theorem 2. PAP2(ax1,CF,β)−2RLG ̸= /0, β ∈{allP,maxP}.
Proof. Let us consider the array P system from Figure 8.3. From the axiom AB,
we generate a string XnY n (A goes to the left, simultaneously with B going to the
right), then, like in a two-dimensional right-linear grammar, in the skin region we go
vertically, marking the pixels with a in the columns of X and with b in the columns
of Y. Moving back into the central membrane, a rectangle with the same number of
columns marked with a and with b is obtained. The set of all these rectangles is not
in the family 2RLG (the same example was already used in [6], too). The system
works identically in both modes allP and maxP.
Removing the priority from the other two results from [6] can be done, but only
by making use of the possibility of having two axioms in the initial conﬁguration
of the systems. One of them will generate the desired arrays, the other one will
generate “twin” arrays, which control the computation of the former arrays.
Proposition 1. LR ∈PAP2(ax2,REG,β), β ∈{allP,maxP}.

176
L. Pan and G. P˘aun
'
&
$
%
'
&
$
%
1
2
S
X# →AY, for X →AY ∈Rh
A →A, for A ∈Vi
X →A(out), for X →A ∈Rh
A →A(out), for A ∈Vi
A
# →a
B , for A →aB ∈Rv
A →a(in), for A →a ∈Rv
Fig. 8.2 The array P system from the proof of Theorem 1
'
&
$
%
'
&
$
%
1
2
AB
#A →AX
B# →YB
X →X
Y →Y
A →X(out)
B →Y(out)
X
# →a
X
Y
# →b
Y
X →a(in)
Y →b(in)
Fig. 8.3 The array P system from the proof of Theorem 2
Proof. We consider the array P system from Figure 8.4. The axioms and the rules
are written on two columns, in the left one those which lead to the desired arrays, in
the right one those handling the “twin” (control) arrays. The rules are similar, with
the ones in the right column dealing with primed nonterminal symbols.
The axiom in the left column is placed in the skin region, the one in the right
column is placed in the inner membrane. In the ﬁrst step, we move the inner axiom

8
On Parallel Array P Systems
177
to the skin membrane, while removing the subscript 0 from all symbols A,B,A′,B′.
Now we start the generation of the hollow rectangles.
The bottom horizontal line of the arrays is generated in the skin membrane, by
moving B and B′ to the right. At a given step, we switch to moving vertically, with
the arrays moved to membrane 2. We go upwards, synchronously,then the arrays are
again moved to the skin membrane, with D and D′ being the current nonterminals.
Both D and D′ go to the left and, at some moment, D is replaced with a (hence the
“left” array is terminal (but we do not know whether the rectangle is completed).
Moved again into membrane 2, the left array remains idle, while the right one can
be rewritten – and, because of the parallelism, this must be done – if (and only if) D′
has a non-marked pixel in its left. This happens if and only if the terminal array is
not a complete hollow rectangle. The symbol D′′ will go up without stopping, hence,
the computation never halts. Thus, only the halting computations produce elements
in the language LR.
A similar result can be obtained for the language of hollow squares.
Proposition 2. LS ∈PAP2(ax2,REG,β), β ∈{allP,maxP}.
Proof. We consider the array P system from Figure 8.5, again with the axioms and
the rules written on two columns, with the same signiﬁcance as above. In the ﬁrst
step, we move the axiom from the skin region to the inner membrane, while also
removing the subscript 0.
In the inner membrane we start constructing the square, from the left bottom cor-
ner, growing here the left and the bottom edges. At some moment, the two arrays are
moved to the skin membrane, where the upper and the right edges are constructed.
The work on the “left” array is terminated at the moment when the arrays are moved
to the inner membrane. We check here whether or not the square is completed. It is
not completed if and only if the nonterminal B′′ has a non-marked pixel above it. If
this is the case, the computation will continue forever, with B′′′ going to the right,
hence the computation never halts. Checking all details remains as an exercise for
the reader.
The maxP mode of using the rules is, intuitively speaking, able of “appearance
checking”, which is known to be a powerful feature of regulated grammars. This is
conﬁrmed also in our framework: we can generate the languages LR,LS by means
of array P systems with only one axiom using the maxP mode.
Proposition 3. LS ∈PAP2(ax1,REG,maxP).
Proof. We consider the array P system from Figure 8.6. In fact, this systems works
like the previous one, but now generating only one array in which the checking for
the square to have been completed is done directly in this array. The main difference
is that the maxP mode allows us to let one symbol (B′′) idle for one step. The rule
¯B# →aB′′′ guarantees that the nonterminal symbol ¯B cannot stay idle in the skin
with the target here if A′ does not change to ¯A′ at the same moment as B′ changes
to ¯B′.

178
L. Pan and G. P˘aun
'
&
$
%
'
&
$
%
1
2
A0aB0
A0 →A
B0 →B
A′
0aB′
0
A →A
B# →aB
#
A →A
a (in)
#
B →B
a (in)
#D →Da
D →a(in)
A′ →A′
B′# →aB′
#
A′ →A′
a (in)
#
B′ →B′
a (in)
#D′ →D′a
D′ →D′(in)
#
A →A
a
#
B →B
a
A →a(out)
B →D(out)
#
A′ →A′
a
#
B′ →B′
a
A′ →a(out)
B′ →D′(out)
#D′ →D′′a
#
D′′ →D′′
a
A′
0 →A′(out)
B′
0 →B′(out)
Fig. 8.4 The array P system from the proof of Proposition 1
It remains as an exercise for the reader to use similar ideas in order to prove
that LR ∈PAP2(ax1,REG,maxP}. On the other hand, we see no way to re-
place maxP with allP in these two results: LR ∈PAP2(ax1,REG,maxP) and LS ∈
PAP2(ax1,REG,maxP).

8
On Parallel Array P Systems
179
'
&
$
%
'
&
$
%
1
2
A0
a B0
A0 →A
B0 →B
#
A →A
a
B# →aB
A →A(out)
B →B(out)
#
A′ →A′
a
B′# →aB′
A′ →A′(out)
B′ →B′(out)
#
B′′ →B′′′
a
B′′′# →aB′′′
A# →aA
#
B →B
a
A# →a ¯A
B →¯B
¯A →a(in)
¯B →a(in)
A′
0
a B′
0
A′
0 →A′(in)
B′
0 →B′(in)
A′# →aA′
#
B′ →B′
a
A′# →aA′′
B′ →B′′
A′′ →a(in)
B′′ →B′′(in)
Fig. 8.5 The array P system from the proof of Proposition 2

180
L. Pan and G. P˘aun
'
&
$
%
'
&
$
%
1
2
A
a B
#
A →A
a
B# →aB
A →A′(out)
B →B′(out)
A′′ →a(out)
#
B′′ →B′′′
a
(out)
A′# →aB′′′(out)
A′# →aA′
#
B′ →B′
a
A′# →a ¯A
B′ →¯B
¯A →A′′(in)
¯B →B′′(in)
B′′ →a
B′′′# →aB′′′
¯B# →aB′′′
Fig. 8.6 The array P system from the proof of Proposition 3
8.5
Concluding Remarks
The goal of this note was, on the one hand, to make explicit the features involved
in an array P system, especially, the number of axioms and the two types of par-
allelism. We made use of them in order to improve some of the results shown
in [6], and, moreover, to replace the powerful ingredient priority relation by us-
ing more than one axiom or the more powerful variant of parallelism maxP. Fur-
ther research efforts are necessary to clarify the computing power of parallel array
P systems. For instance, we have the families PAPm(axk,α,β), for m ≥1,k = 1,
α ∈{CF,REG},β ∈{allP,maxP}. What are the relations between them? Do the
parameters m and k induce inﬁnite hierarchies? Besides CF and REG, one can also
consider variants of context-free array rules, for example erasing in contrast to non-
erasing ones or allowing the right-hand side to contain blank pixels.
A natural question is whether or not parallel array P systems are computationally
complete as the sequential ones considered in [1] are.

8
On Parallel Array P Systems
181
Rather natural is the possibility, already mentioned, to impose the use of rules
with the same target for each array, separately, thus making possible that two ar-
rays from a given region can go to different places after rewriting. A related issue
is to consider other ways to control the communication, such as the t-mode from
the grammar systems area, already investigated, for the sequential case, for array P
systems in [5].
Finally, we mention the problem of considering Lindenmayer-like array P sys-
tems, either pure (without nonterminals) or extended (with all symbols to be rewrit-
ten, but accepting only terminal arrays).
Acknowledgements. The work of the ﬁrst author was supported by the National Natural
Science Foundation of China (61033003, 91130034 and 61320106005). Many thanks to Rudi
Freund for carefully reading the initial version of the paper and for improving Proposition 3.
References
1. Ceterchi, R., Mutyam, M., P˘aun, Gh., Subramanian, K.G.: Array-rewriting P systems.
Natural Computing 2, 229–249 (2003)
2. Giammarresi, D., Restivo, A.: Two-dimensional languages. In: Rozenberg, G., Salomaa,
A. (eds.) Handbook of Formal Languages, vol. 3, pp. 215–267. Springer, Berlin (1997)
3. Krishna, S.N., Krithivasan, K., Rama, R.: P systems with picture objects. Acta Cyber-
netica 15(1), 53–74 (2001)
4. P˘aun, Gh., Rozenberg, G., Salomaa, A. (eds.): The Oxford Handbook of Membrane
Computing. Oxford University Press (2010)
5. Subramanian, K.G., Ali, R.M., Nagar, A.K., Margenstern, M.: Array P systems and t
communication. Fundamenta Informaticae 91(1), 145–159 (2009)
6. Subramanian, K.G., Isawasan, P., Venkat, I., Pan, L.: Parallel array-rewriting P systems.
Romanian Journal of Information Science and Technology (to appear)
7. The P Systems Website, http://ppage.psystems.eu.

Chapter 9
Small P Systems Deﬁning Non-semilinear Sets
Artiom Alhazov and Rudolf Freund
Abstract. We present a number of tiny P systems generating or accepting non-
semilinear sets of (vectors of) natural numbers with very small numbers of rules,
even for 1, 2, 3, 4, and 5 rules, depending on the particular model and the addi-
tional features used in these systems. Among the models we consider are P systems
with target agreement and target selection, P systems with promoters and inhibitors,
catalytic and purely catalytic P systems with normal catalysts or bi-stable catalysts.
We then improve the results for catalytic (purely catalytic) P systems: 14 rules for
generating a non-semilinear vector set and 29 rules for generating a non-semilinear
number set are sufﬁcient when allowing only the minimal number of two (three)
catalysts; only 23 rules are needed if we allow more catalysts, i.e., ﬁve (seven) cata-
lysts. Moreover, we introduce the new concept of toxic objects, objects that must not
stay idle as otherwise the computation is abandoned without yielding a result. P sys-
tems of various kinds using toxic objects allow for smaller descriptional complexity,
especially for smaller numbers of rules, as trap rules can be avoided.
9.1
Introduction
Membrane systems were introduced by Gheorghe P˘aun in [13] (and therefore called
P systems since then). Motivated by the biological functioning of molecules in cells,
from a mathematical point of view a P system can be viewed as a parallel multiset
Artiom Alhazov
Institute of Mathematics and Computer Science,
Academy of Sciences of Moldova
Academiei 5, Chis¸in˘au, MD-2028, Moldova
e-mail: artiom@math.md
Rudolf Freund
Faculty of Informatics, Vienna University of Technology
Favoritenstr. 9, 1040 Vienna, Austria
e-mail: rudi@emcc.at
c⃝Springer International Publishing Switzerland 2015
183
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_9

184
A. Alhazov and R. Freund
rewriting system. When using non-cooperative rules without any additional control,
it has the behaviour of an E0L system; yet when only taking the results when the sys-
tem halts means that the objects evolve in a context-free manner, generating a set in
PsCF, which is known (by Parikh’s theorem) to coincide with PsREG, i.e., with the
family of semilinear sets. In the accepting setup, P systems using non-cooperative
rules without any additional control are even weaker, accepting all multisets over
some subalphabet, or nothing, see [1].
In this paper we therefore are interested in different variants of P systems able to
generate or accept non-semilinear sets of natural numbers or at least sets of vectors
of natural numbers, yet with as small ingredients as possible. We assume that in
“non-trivial” computations there has to be some form of interaction between the
elements of the system (each element only containing ﬁnite information). Our main
focus is on the descriptional complexity of these P systems, i.e., on how small the
total number of rules may be, depending on the speciﬁc features of the particular
models of P systems we look at. As most of the models of P systems can be shown
to be computationally complete based on a simulation of the actions of a register
machine, even simple examples often turn out to be somehow more complicated
than the general proof.
We start with the strongest models, at least seen with respect to the number of
rules needed to accept or generate a non-semilinear set. We conclude the list of
results by P systems with two catalysts, thereby improving the result established
in [19]. Ideas how to ﬁnd good examples of catalytic P systems generating a non-
semilinear set of numbers were discussed intensively during the Fourteenth Inter-
national Conference on Membrane Computing (CMC 2013) in Chis¸in˘au, especially
by Petr Sos´ık and Rudolf Freund, based on a draft by Petr Sos´ık, with most of
them ﬁnally being included in his paper [19]. Some new observations just found
recently allowed us to reduce the number of rules again in a considerable way, see
Section 9.6.5.
In the following sections of this paper, we ﬁrst provide a lot of examples for P
systems generating or accepting a non-semilinear set of natural numbers or of vec-
tors of natural numbers with only a few rules, especially for the maximally parallel
derivation mode. Then we introduce the new concept of toxic objects which allows
us to “kill” a computation branch if we cannot ﬁnd a multiset of rules covering all
occurrences of toxic objects which then somehow become “lethal” by killing such
a computation. For all the proof techniques using a trap symbol # to “kill” a com-
putation by introducing the trap symbol # with a non-cooperative rule a →#, the
concept of toxic objects allows us to save most of the trap rules or even all of them,
thus improving the descriptional complexity of the underlying P systems.
The rest of the paper is organized as follows: We ﬁrst recall the basic deﬁni-
tions from formal language theory as well as the deﬁnitions for the most impor-
tant variants of P systems for which we will give examples generating or accepting
non-semilinear sets of (vectors of) natural numbers with only a few rules. Then
we present examples with a small number of rules for variants of accepting P sys-
tems and then for variants of generating P systems, ending up with the catalytic
and purely catalytic P systems, for which we improve previous results established

9
Small P Systems Deﬁning Non-semilinear Sets
185
in [19]. For a lot of variants, the concept of toxic objects allows us to give examples
with a smaller number of rules than that we obtained with the original variant. We
conclude the paper with a table summarizing our examples.
9.2
Deﬁnitions
In this section we ﬁrst recall the basic notions from formal language theory needed
in this paper and then the deﬁnitions of the basic variants of P systems considered
in the following sections. For more details in formal language theory we refer the
reader to the standard monographs and textbooks as [17] and for the area of reg-
ulated rewriting to [7]. All the main deﬁnitions and results for P systems can be
found in [14] and [15]; only speciﬁc notations and models not yet to be found there
will be explained in more detail in this paper, especially the new idea of “toxic ob-
jects”, which will be explained and studied in Section 9.6. For actual informations
and new developments in the area of membrane computing we refer to the P systems
webpage [20].
9.2.1
Prerequisites
The set of non-negative integers (natural numbers) is denoted by N. An alphabet V
is a ﬁnite non-empty set of abstract symbols. Given V, the free monoid generated
by V under the operation of concatenation is denoted by V ∗; the elements of V ∗are
called strings, and the empty string is denoted by λ; V ∗\ {λ} is denoted by V +.
Let {a1,··· ,an} be an arbitrary alphabet; the number of occurrences of a symbol ai
in a string x is denoted by |x|ai; the Parikh vector associated with x with respect to
a1,··· ,an is

|x|a1 ,··· ,|x|an

. The Parikh image of a language L over {a1,··· ,an}
is the set of all Parikh vectors of strings in L, and we denote it by Ps(L). For a
family of languages FL, the family of Parikh images of languages in FL is denoted
by PsFL; for families of languages over a one-letter alphabet, the corresponding sets
of non-negative integers are denoted by NFL; for an alphabet V containing exactly
d objects, the corresponding sets of Parikh vectors with d components is denoted by
NdFL, i.e., we replace Ps by Nd.
A (ﬁnite) multiset over the (ﬁnite) alphabet V, V = {a1,··· ,an}, is a mapping
f : V −→N and represented by ⟨f (a1),a1⟩···⟨f (an),an⟩or by any string x the
Parikh vector of which with respect to a1,··· ,an is (f (a1),··· , f (an)). In the fol-
lowing we will not distinguish between a vector (m1,··· ,mn), its representation by
a multiset ⟨m1,a1⟩···⟨mn,an⟩or its representation by a string x having the Parikh
vector

|x|a1 ,··· ,|x|an

= (m1,··· ,mn). Fixing the sequence of symbols a1,··· ,an
in the alphabet V in advance, the representation of the multiset ⟨m1,a1⟩···⟨mn,an⟩
by the string am1
1 ···amn
n
is unique. The family of regular, context-free, and recur-
sively enumerable string languages is denoted by REG, CF, and RE, respectively.

186
A. Alhazov and R. Freund
9.2.1.1
ET0L Systems
An ET0L system is a construct G = (V,T,w,P1,··· ,Pm) where m ≥1, V is an al-
phabet, T ⊆V is the terminal alphabet, w ∈V ∗is the axiom, and the Pi, 1 ≤i ≤m,
are ﬁnite sets (tables) of non-cooperative rules over V. In a derivation step in G, all
the symbols present in the current sentential form are rewritten using one table. The
language generated by G, denoted by L(G), consists of all terminal strings w ∈T ∗
which can be generated by a derivation in G starting from the axiom w. The fam-
ily of languages generated by ET0L systems and by ET0L systems with at most k
tables is denoted by ET0L and ETk0L, respectively.
Remark 1. According to Theorem 1.3 in chapter V of [16], for each language L ∈
ET0L there exists an ET0L system with only two tables G′ = (V ′,T,w,P′
1,P′
2) such
that L(G) = L, i.e., we have ET0L = ETk0L for all k ≥2. The main idea of the
construction is the following: Given an ET0L system G = (V,T,w,P1,··· ,Pn) with
n > 2, we use additional symbols [a,i] for each a ∈V and 1 ≤i ≤n. Then we deﬁne
V ′ = V ∪{[a,i] | a ∈V,1 ≤i ≤n},
P′
1 = {a →[a,1],[a,n] →a | a ∈V}∪{[a,i] →[a,i+ 1] | a ∈V,1 ≤i < n},
P′
2 = {[a,i] →v | a →v ∈Pi,1 ≤i ≤n}∪{a →a | a ∈V}.
The component P′
1 “colors” the symbols a ∈V as [a,i] for all of them being available
in P′
2 for a simulation of table Pi. After the use of P′
2, we again have to use P′
1 several
times to get all symbols a colored as [a, j] for all of them being available in P′
2 for a
simulation of a table Pj etc.
9.2.1.2
Register Machines
A register machine is a tuple M = (m,B,l0,lh,P), where m is the number of registers,
P is the set of instructions bijectively labeled by elements of B, l0 ∈B is the initial
label, and lh ∈B is the ﬁnal label. The instructions of M can be of the following
forms:
• l1 : (ADD(j),l2,l3), with l1 ∈B\ {lh}, l2,l3 ∈B, 1 ≤j ≤m.
Increase the value of register j by one, and non-deterministically jump to instruc-
tion l2 or l3. This instruction is usually called increment.
• l1 : (SUB(j),l2,l3), with l1 ∈B\ {lh}, l2,l3 ∈B, 1 ≤j ≤m.
If the value of register j is zero then jump to instruction l3, otherwise decrease
the value of register j by one and jump to instruction l2. The two cases of this
instruction are usually called zero-test and decrement, respectively.
• lh : HALT. Stop the execution of the register machine.
A conﬁguration of a register machine is described by the contents of each register
and by the value of the current label, which indicates the next instruction to be
executed. Computations start by executing the ﬁrst instruction of P (labeled with
l0), and terminate with reaching the HALT-instruction.

9
Small P Systems Deﬁning Non-semilinear Sets
187
Register machines provide a simple universal computational model, for example,
see [12]. In the following, we shall call a speciﬁc model of P systems computation-
ally complete or universal if and only if for any register machine M we can effec-
tively construct an equivalent P system Π of that type simulating M and yielding
the same results.
9.2.1.3
Non-semilinear Sets of Numbers and Vectors of Numbers
In most of the examples described in the following sections, we will use (variants)
of the set of natural numbers
{2n | n ≥0} = N

a2n | n ≥0

and the set of (two-dimensional) vectors of natural numbers
{(n,m) | n ≥1,n ≤m ≤2n} = Ps({(anbm) | n ≥1,n ≤m ≤2n}),
which both are known to not be semilinear.
9.2.2
P Systems
The ingredients of the basic variants of (cell-like) P systems are the membrane
structure, the objects placed in the membrane regions, and the evolution rules. The
membrane structure is a hierarchical arrangement of membranes. Each membrane
deﬁnes a region/compartment, the space between the membrane and the immedi-
ately inner membranes; the outermost membrane is called the skin membrane, the
region outside is the environment, also indicated by (the label) 0. Each membrane
can be labeled, and the label (from a set Lab) will identify both the membrane and its
region. The membrane structure can be represented by a rooted tree (with the label
of a membrane in each node and the skin in the root), but also by an expression of
correctly nested labeled parentheses. The objects (multisets) are placed in the com-
partments of the membrane structure and usually represented by strings, with the
multiplicity of a symbol corresponding to the number of occurrences of that symbol
in the string. The basic evolution rules are multiset rewriting rules of the form u →v,
where u is a multiset of objects from a given set O and v = (b1,tar1)...(bk,tark)
with bi ∈O and tari ∈{here,out,in} or tari ∈{here,out} ∪
 
in j | j ∈Lab
!
, 1 ≤
i ≤k. Using such a rule means “consuming” the objects of u and “producing” the
objects b1,...,bk of v; the target indications here, out, and in mean that an object
with the target here remains in the same region where the rule is applied, an object
with the target out is sent out of the respective membrane (in this way, objects can
also be sent to the environment, when the rule is applied in the skin region), while
an object with the target in is sent to one of the immediately inner membranes, non-
deterministically chosen, whereas with in j this inner membrane can be speciﬁed
directly. In general, we may omit the target indication here.

188
A. Alhazov and R. Freund
Yet there are a lot of other variants of rules we shall consider later; for example,
if on the right-hand side of a rule we add the symbol δ, the surrounding membrane
is dissolved whenever at least one such rule is applied, at the same moment all
objects inside this membrane (the objects of this membrane region together with the
whole inner membrane structure) are released to the surrounding membrane, and
the rules assigned to the dissolved membrane region get lost. Another option is to
add promoters p1,..., pm ∈O+ and inhibitors q1,...,qn ∈O+ to a rule and write
u →v|p1,...,pm,¬q1,...,¬qn, which rule then is only applicable if the current contents
of the membrane region includes each of the promoter multisets, but none of the
inhibitor multisets; in most cases promoters and inhibitors are rather taken to be
singleton objects than multisets. Further variants of P systems will be deﬁned later.
Formally, a (cell-like) P system is a construct
Π = (O,μ,w1,...,wm,R1,...,Rm, f)
where O is the alphabet of objects, μ is the membrane structure (with m mem-
branes), w1,...,wm are multisets of objects present in the m regions of μ at the
beginning of a computation, R1,...,Rm are ﬁnite sets of evolution rules, associ-
ated with the membrane regions of μ, and f is the label of the membrane region
from which the outputs are taken/the inputs are put in ( f = 0 indicates that the out-
put/input is taken from the environment).
If a rule u →v has at least two objects in u, then it is called cooperative, otherwise
it is called non-cooperative. In catalytic P systems we use non-cooperative as well
as catalytic rules which are of the form ca →cv, where c is a special object which
never evolves and never passes through a membrane (both these restrictions can be
relaxed), but it just assists object a to evolve to the multiset v. In a purely catalytic P
system we only allow catalytic rules. For a catalytic as well as for a purely catalytic P
system Π, in the description of Π we replace “O” by “O,C” in order to specify those
objects from O which are the catalysts in the set C. As already explained above,
cooperative and non-cooperativeas well as catalytic rules can be extended by adding
promoters and/or inhibitors, thus yielding rules of the form u →v|p1,...,pm,¬q1,...,¬qn.
All the rules deﬁned so far can be used in different derivation modes: in the
sequential mode (sequ), we apply exactly one rule in every derivation step; in the
asynchronous mode (asyn), an arbitrary number of rules is applied in parallel; in
the maximally parallel (maxpar) derivation mode, at any computation step of Π we
choose a multiset of rules from the sets R1,...,Rm in a non-deterministic way such
that no further rule can be added to it so that the obtained multiset would still be
applicable to the existing objects in the membrane regions 1,...,m.
The membranes and the objects present in the compartments of a system at a
given time form a conﬁguration; starting from a given initial conﬁguration and
using the rules as explained above, we get transitions among conﬁgurations; a
sequence of transitions forms a computation (we often also say derivation). A com-
putation is halting if and only if it reaches a conﬁguration where no rule can be
applied any more. With a halting computation we associate a result generated by
this computation, in the form of the number of objects present in membrane f in

9
Small P Systems Deﬁning Non-semilinear Sets
189
the halting conﬁguration. The set of multisets obtained as results of halting compu-
tations in Π working in the derivation mode δ ∈{sequ,asyn,maxpar} is denoted
by mLgen,δ (Π), the set of natural numbers obtained by just counting the number of
objects in the multisets of mLgen,δ (Π) by Ngen,δ (Π), and the set of (Parikh) vectors
obtained from the multisets in mLgen,δ (Π) by Psgen,δ (Π).
Yet we may also start with some additional input multiset winput over an input
alphabet Σ in membrane f, i.e., in total we there have wf winput in the initial conﬁg-
uration, and accept this input winput if and only if there exists a halting computation
with this input; the set of multisets accepted by halting computations in
Π = (O,Σ,μ,w1,...,wm,R1,...,Rm, f)
working in the derivation mode δ is denoted by mLacc,δ (Π), the corresponding sets
of natural numbers and of (Parikh) vectors are denoted by Nacc,δ (Π) and Psacc,δ (Π),
respectively.
The family of sets Yγ,δ (Π), Y ∈{N,Ps}, γ ∈{gen,acc} computed by P systems
with at most m membranes working in the derivation mode δ and with rules of type
X is denoted by Yγ,δOPm (X).
For example, it is well known (for example, see [13]) that for any m ≥1, for the
types of non-cooperative (ncoo) and cooperative (coo) rules we have
NREG = Ngen,maxparOPm (ncoo) ⊂Ngen,maxparOPm (coo) = NRE.
For γ ∈{gen,acc} and δ ∈{sequ,asyn,maxpar}, the family of sets Yγ,δ (Π), Y ∈
{N,Ps}, computed by catalytic and purely catalytic P systems with at most m mem-
branes and at most k catalysts is denoted by Yγ,δOPm (catk) and Yγ,δOPm (pcatk),
respectively; from [9] we know that, with the results being sent to the environment
(which means taking f = 0), we have
Ygen,maxparOP1 (pcat2) = Ygen,maxparOP1 (pcat3) = YRE.
If we allow a catalyst c to switch between two different states c and ¯c we call c a
bi-stable catalyst; in that way we obtain P systems with bi-stable catalysts. For γ ∈
{gen,acc} and δ ∈{sequ,asyn,maxpar}, the family of sets Yγ,δ (Π), Y ∈{N,Ps},
computed by catalytic and purely catalytic P systems with bi-stable catalysts with
at most m membranes and at most k catalysts is denoted by Yγ,δOPm (2catk) and
Yγ,δOPm (p2catk), respectively. We note that in the generative case we do not count
the catalysts in the output membrane region.
For all the variants of P systems of type X, we may consider to label all the rules
in the sets R1,...,Rm in a one-to-one manner by labels from a set H and to take a
set W containing subsets of H. Then a P system with label selection is a construct
Π = (O,μ,w1,...,wm,R1,...,Rm,H,W, f)
where Π ′ = (O,μ,w1,...,wm,R1,...,Rm, f) is a P system as deﬁned above, H is a
set of labels for the rules in the sets R1,...,Rm, and W ⊆2H. In any transition step

190
A. Alhazov and R. Freund
in Π we ﬁrst select a set of labels U ∈W and then apply a non-empty multiset R of
rules such that all the labels of these rules in R are in U (and in the case of maximal
parallelism, the set R cannot be extended by any further rule with a label from U so
that the obtained multiset of rules would still be applicable to the existing objects in
the membrane regions 1,...,m). The families of sets Y (Π), Y ∈{N,Ps}, computed
by P systems with label selection with at most m membranes and rules of type X
as well as card (W) ≤k are denoted by Yγ,δOPm (X,lsk), for any γ ∈{gen,acc} and
δ ∈{sequ,asyn,maxpar}.
For all variants of P systems using rules of some type X, we may consider systems
containing only rules of the form u →v where u ∈O and v = (b1,tar)...(bk,tar)
with bi ∈O and tar ∈{here,out,in} or tar ∈{here,out}∪
 
in j | j ∈H
!
, 1 ≤i ≤k,
i.e., in each rule there is only one target for all objects bi; if catalytic rules are
considered, then we request the rules to be of the form ca →c(b1,tar)...(bk,tar);
in both cases, for a rule u →(b1,tar)...(bk,tar) we then write u →(b1,...,bk;tar).
A P system with target selection contains only these forms of rules; moreover, in
each computation step, for each membrane region i we choose a multiset of rules
from Ri having the same target indication tar; for different membrane regions these
targets may be different; moreover, the total multiset obtained in this way must
not be empty. The families of sets Yγ,δ (Π), Y ∈{N,Ps}, computed by P systems
with target selection with at most m membranes and rules of type X are denoted by
Yγ,δOPm (X,ts), for any γ ∈{gen,acc} and δ ∈{sequ,asyn,maxpar}.
Remark 2. P systems with target selection were ﬁrst deﬁned in [10], but there the
chosen multiset of rules for each Ri had to be non-empty if possible. In this paper,
we only require the total multiset of rules, obtained by choosing multisets of rules
in each Ri with the results going to a chosen target membrane, to be non-empty. Yet
as in [10] we assume that when choosing the target in all objects are sent to just one
selected inner membrane.
We may extend rules of the form u →(b1,...,bk;tar) to rules of the form u →
(b1,...,bk;Tar) where Tar is a ﬁnite set of targets, thus obtaining P systems with
target agreement. In each computation step, for each membrane we ﬁrst choose a
target tar and then a multiset of rules of the form u →(b1,...,bk;Tar) with tar ∈
Tar – again, for different membranes these targets may be different. The families
of sets Yγ,δ (Π), Y ∈{N,Ps}, computed by P systems with target agreement with
at most m membranes and rules of type X are denoted by Yγ,δOPm (X,ta), for any
γ ∈{gen,acc} and δ ∈{sequ,asyn,maxpar}.
P systems with target agreement have the same computational power as P systems
with target selection, as proved in the following theorem, yet they allow for a more
compact description of rules as we will see in Subsection 9.4.1.
Theorem 1. For all types of rules X, any γ ∈{gen,acc}, any derivation mode δ ∈
{sequ,asyn,maxpar}, any Y ∈{N,Ps}, and any m ∈N, we have
Yγ,δOPm (X,ta) = Yγ,δOPm (X,ts).

9
Small P Systems Deﬁning Non-semilinear Sets
191
Proof. Given a P system with target selection
Π = (O,μ,w1,...,wm,R1,...,Rm, f)
we can also interpret Π as a P system
Π ′ = (O,μ,w1,...,wm,R′
1,...,R′
m, f)
with target agreement by replacing each rule u →(b1,...,bk;tar) in any of the sets
Ri, 1 ≤i ≤m, by the corresponding rule u →(b1,...,bk;{tar}) in R′
i. Obviously,
Yγ,δ (Π) = Yγ,δ (Π ′).
On the other hand, given a P system
Π ′ = (O,μ,w1,...,wm,R′
1,...,R′
m, f)
with target agreement we immediately get the corresponding P system with target
selection
Π = (O,μ,w1,...,wm,R1,...,Rm, f)
such that Yγ,δ (Π) = Yγ,δ (Π ′): for each rule u →(b1,...,bk;Tar) ∈R′
i we take all
the rules u →(b1,...,bk;tar) with tar ∈Tar into Ri.
Whereas for most of the other variants considered in this paper the so-called
ﬂattening procedure (for more details see [10]) allows for ﬁnding equivalent systems
with only one membrane, for P systems with target selection or target agreement the
membrane structure usually plays an essential rˆole.
The basic idea of membrane systems with objects moving between membrane
regions most nicely is captured by the model of symport/antiport P systems: in the
inner membranes objects can neither be generated nor deleted, but only move from
one membrane region to another one by passing through membranes; new objects
can only be taken from the environment which provides an unlimited source for
speciﬁc objects.
Formally, a symport/antiport P system is a construct
Π = (O,E,μ,w0,w1,...,wm,R1,...,Rm, f)
where O is the alphabet of objects, E ⊆O is the set of objects being available in the
environment in an unbounded number, μ is the membrane structure (with m mem-
branes), w0 is the ﬁnite multiset of objects over O \ E present in the environment
at the beginning of a computation, w1,...,wm are the multisets of objects present
in the m regions of μ at the beginning of a computation, R1,...,Rm are ﬁnite sets
of symport and/or antiport rules, associated with the membranes of μ, and f is the
label of the membrane region from which the outputs are taken/the inputs are put
in (f = 0 indicates that the output/input is taken from the environment). Every rule
is of the form (u,out;v,in) with u,v ∈O∗and uv ̸= λ; if u = λ or v = λ then this
rule is called a symport rule, otherwise it is called an antiport rule. The application
of a rule (u,out;v,in) ∈Ri means sending out u from membrane region i and taking

192
A. Alhazov and R. Freund
v into it from the surrounding membrane region. For (u,out;v,in), max{|u|,|v|} is
called its weight and |uv| is called its size; obviously, for symport rules weight and
size are the same. The families of sets Yγ,δ (Π), Y ∈{N,Ps}, γ ∈{gen,acc}, and
δ ∈{sequ,asyn,maxpar}, computed by symport/antiport P systems with at most
m membranes, symport rules with maximal weight r as well as antiport rules with
maximal weight w and maximal size s are denoted by Yγ,δOPm (symr,antiw,s).
As we will only give one very small example from the area of P systems with
active membranes, we only deﬁne a P system with active membranes and polariza-
tions as a construct
Π = (O,μ,H,P,w1,...,wm,R, f)
where O is the alphabet of objects, μ is the membrane structure consisting of m
membranes, labeled (not necessarily in a one-to-one manner) with elements of H;
P is a set of polarizations (mainly) taken to be 0, 1, and −1; w1,...,wm are strings
over O, describing the multisets of objects in the m regions of μ; f is the label of
an input/output membrane; R is a ﬁnite set of developmental rules, of the following
forms:
(1)(a) [ a →v ]e
h,
for h ∈H, e ∈P, a ∈O, v ∈O∗
(object evolution rules; associated with membranes and depending on the
label and the polarization of the membranes, but not directly involving the
membranes, in the sense that the membranes are neither taking part in the
application of these rules nor are they modiﬁed by them);
(b) a[ ]e1
h →[ b ]e2
h ,
for h ∈H, e1,e2 ∈P, a,b ∈O
(communication rules; an object is sent in through the membrane, possibly
modiﬁed by this process; also the polarization of the membrane may be
modiﬁed, but not its label);
(c) [ a ]e1
h →[ ]e2
h b,
for h ∈H, e1,e2 ∈P, a,b ∈O
(communication rules; an object is sent out through the membrane, possibly
modiﬁed by this process; also the polarization of the membrane may be
modiﬁed, but not its label);
(d) [ a ]e
h →b,
for h ∈H, e ∈P, a,b ∈O
(dissolving rules; in reaction with an object, a membrane can be dissolved,
while the object speciﬁed in the rule can be modiﬁed);
(e) [ a ]e1
h →[ b ]e2
h [ c ]e3
h ,
for h ∈H, e1,e2,e3 ∈P, a,b,c ∈O
(division rules for elementary membranes; in reaction with an object, the
membrane is divided into two membranes with the same label, possibly
of different polarizations; the object speciﬁed in the rule is replaced in the
two new membranes by possibly new objects, and the remaining objects are
duplicated).

9
Small P Systems Deﬁning Non-semilinear Sets
193
In general, we use the maximally parallel derivation mode, i.e., all objects which
can be used by a rule have to be used, but whereas the rules of type (a) are applied
in parallel, the rules of types (b), (c), (d), and (e) are used sequentially in the sense
that one membrane can be used by at most one rule of these types at a time. The
families of sets Yγ (Π), Y ∈{N,Ps}, γ ∈{gen,acc}, computed by P systems with
active membranes with at most m membranes, p polarizations, and rules of type X
are denoted by YγOPp
m (X).
Some further speciﬁc variants of P system will be explained directly in the sub-
sections of the following sections.
For any of the families of (vectors of) natural numbers Yγ,δOPm (X) we will add
subscript k at the end to indicate that only systems with at most k rules are con-
sidered, i.e., we write Yγ,δ OPm (X)k. If any of the ﬁnite parameters like m and k is
unbounded, we replace it by ∗.
9.3
Accepting P Systems
The constructions in this section accept the non-semilinear set of natural num-
bers {2n | n ≥0} ∪{0}, i.e., for each of the following P systems Π we have
mLacc,maxpar(Π) = {a2n | n ≥0} ∪{λ}. They are all based on iterated halving of
the number of input objects.
9.3.1
Using a Special Accepting and Halting Condition
In this subsection we step away from the standard halting condition and from the
standard deﬁnition of accepting inputs, which allows us to show how much these
deﬁnitions may inﬂuence the result of minimizing the total number of rules. In a
more general way, we will follow the idea of stopping a derivation when speciﬁc
objects remain idle in Subsection 9.6.2.
We consider the P system
Π1 = (O = {a},Σ = {a},μ = [ ]1,w1 = λ,R1 = {aa →a}, f = 1),
which, taken with the usual halting and accepting conditions simply accepts every-
thing, i.e., Nacc,maxpar(Π1) = {an | n ≥0} = {a}∗. We would like to mention that in
the description of the P systems exhibited in the following we will use the extended
description as for the P system Π1 given above, not only writing
Π1 = ({a},{a},[ ]1,λ,{aa →a},1),
in order to make clear which meaning all the parameters in the description have.
In fact, the main idea of the P system Π1 is to halve the number of objects in
each derivation step, trying to reach 1. Hence, we now deﬁne a computation in Π1
to immediately halt whenever some object a remains idle, but to only accept the
input if the number of objects in the ﬁnal conﬁguration is not more than 1. Using

194
A. Alhazov and R. Freund
the notation fc1 for this special variant of halting (we halt whenever no multiset
of rules exists fully covering all objects in the underlying conﬁguration) and ac-
ceptance (in the ﬁnal conﬁguration, at most 1 object is present), we get the desired
result mLacc−fc1,maxpar(Π1) = {a2n | n ≥0} ∪{λ}, i.e.,
{2n | n ≥0} ∪{0} ∈Nacc−fc1,maxparOP1(coo)1 .
This accepting P system has only one cooperative rule, but uses a special non-
standard halting condition and a special non-standard accepting condition. More-
over, it obviously is deterministic.
It is well-known that P systems with cooperative rules are computationally com-
plete, even with standard halting and accepting deﬁnitions. The smallest known uni-
versal one has 23 rules, see [5].
For the further examples of this section, we now return to the original deﬁnitions
of halting and acceptance.
9.3.2
P Systems with Input Alphabet
In many examples presented in this paper there is no need to distinguish between
the total alphabet of symbols and a subset of input symbols, which we are going to
do when considering the P system
Π2 = (O = {a,s},Σ = {a},μ = [ ]1,w1 = λ,R1, f = 1) where
R1 = {aa →a, a →s, ss →ss}.
In each derivation step, Π2 halves (part of) the objects a, renaming the rest of
objects a into s. If more than one s is produced, an inﬁnite computation is forced
due to the rule ss →ss. The only way to produce not more than one s is to always
have even multiplicity of objects a until it reaches the last one; hence, we conclude
mLacc,maxpar(Π2) = {a2n | n ≥0} ∪{λ}.
This accepting P system Π2 has 3 cooperative rules, i.e., we have
{2n | n ≥0} ∪{0} ∈Nacc,maxparOP1 (coo)3 .
It is well-known that P systems with cooperative rules are computationally com-
plete. The smallest known universal one has 23 rules, see [5].
9.3.3
P Systems with Symport/Antiport
The following symport/antiport P system accepting {a2n | n ≥0} ∪{λ} has only 3
rules and its construction is based on an idea from [11]:

9
Small P Systems Deﬁning Non-semilinear Sets
195
Π3 = (O = {a},Σ = O,E = O,μ = [ [ [ ]3 ]2 ]1,
w1 = λ,w2 = λ,w3 = aa,R1,R2,R3, f = 1),
R1 = {(aa,out;a,in)},
R2 = {(a,in)},
R3 = {(aa,out;aa,in)}.
The construction works as follows: The only way to accept the input is to halve
the number of objects in the skin membrane until this number reaches one (and
therefore rule (a,in) ∈R2 need not be used more than once); if at least two objects
come to region 2 from the skin membrane, i.e., if the rule (a,in) ∈R2 has been used
at least twice, then the computation never halts, because rule (aa,out;aa,in) ∈R3
can be used forever.
This accepting P system has 3 symport/antiport rules of weight at most 2 and size
at most 4; the size of the antiport rules may be decreased to 3 at a cost of one more
rule, i.e., by taking the following symport/antiport P system:
Π4 = (O = {a},Σ = O,E = O,μ = [ [ [ ]3 ]2 ]1,
w1 = λ,w2 = λ,w3 = a,R1,R2,R3, f = 1),
μ = [ [ [ ]3 ]2 ]1,
R1 = {(aa,out;a,in)},
R2 = {(a,in)},
R3 = {(aa,out;a,in),(a,out;aa,in)}.
In Π4, an inﬁnite loop is entered with the rules from R3 as soon as the rule (a,in)
from R2 in total has been used at least twice.
Hence, in sum we have
{2n | n ≥0} ∪{0} ∈
Nacc,maxparOP3(sym1,anti2,4)3
∩Nacc,maxparOP3 (sym1,anti2,3)4 .
In general, symport/antiport P systems are well-known to be computationally
complete with only one membrane. The smallest known number of rules is again
23, see [5].
9.4
Generating by Doubling in the Maximally Parallel Mode
This section contains models of P systems with “mass inﬂuence”, where something
can simultaneously affect (directly or indirectly) an unbounded number of copies
of speciﬁc objects (e.g., target agreement, target selection, label selection, tables,
membrane dissolution, promoters, inhibitors, active membranes with polarizations).
In that way we obtain tiny P system using massive parallelism for repeated doubling,
and using one of the effects mentioned above to halt.

196
A. Alhazov and R. Freund
9.4.1
P Systems with Target Agreement Or Target Selection
We ﬁrst consider the case of target agreement which allows for a smaller descrip-
tional complexity with only one rule:
Π6 = (O = {a},μ = [ [ ]2 ]1,w1 = a,w2 = λ,R1,R2 = /0, f = 2) with
R1 = {a →(aa,{here,in})}.
Π6 doubles the number of objects each turn that the objects stay in the skin mem-
brane choosing the target here. At some moment, all objects agree in the target des-
tination in thus moving into the inner membrane where no rule can be applied any
more. At any moment of the computation, all simultaneously produced objects must
agree in the same destination, effectively choosing between continuing the doubling
or halting.
If we resolve the rule a →(aa,{here,in}) into its two corresponding rules a →
(aa,here) and a →(aa,in), we immediately get the P system with target selection
Π7 = (O = {a},μ = [ [ ]2 ]1,w1 = a,w2 = λ,R1,R2 = /0, f = 2) with
R1 = {a →(aa,here),a →(aa,in)}.
If in both cases we allow the output to be collected in the environment, we even get
the following P systems with target agreement/selection having only one membrane:
Π8 = (O = {a},μ = [ ]1,w1 = a,R1, f = 0) with
R1 = {a →(aa,{here,out})}.
and
Π9 = (O = {a},μ = [ ]1,w1 = a,R1, f = 0) with
R1 = {a →(aa,here),a →(aa,out)}.
In sum, we therefore infer
{2n | n ≥1} ∈Ngen,maxparOP1(ncoo,ta)1 ∩Ngen,maxparOP1(ncoo,ts)2 .
9.4.2
P systems with Label Selection
Instead of selecting different targets, in the P system with label selection
Π10 = (O = {a},μ = [ ]1,w1 = a,R1,H = {1,2},W = {{1},{2}}, f = 0) with
R1 = {1 : a →(aa,here),2 : a →(aa,out)}.
we select different labels to be able to choose when to send out all objects from the
skin membrane to the environment; hence, we have
{2n | n ≥1} ∈Ngen,maxparOP1(ncoo,ls2)2 .

9
Small P Systems Deﬁning Non-semilinear Sets
197
9.4.3
P Systems with Tables
The following P system with tables of rules (tabled P system) needs only 2 rules and
is closely related to the P system with target selection described in Subsection 9.4.1.
Π11 = (O = {a},μ = [ ]1,w1 = b,R1, f = 0) with
R1 = {T1 = {a →aa,here},T2 = {a →a,out}}.
Π11 doubles the multiplicity of objects a each step as long as using table T1. At
any time, if after n ≥0 such steps the second table T2 is chosen, all objects a are
sent out to the environment and the computation halts, having generated a2n.
We also note that the second table (and thus the table feature) is not needed
under a speciﬁc variant of halting called unconditional halting which resembles
the L systems (Lindenmayer systems) mode of taking the result; P systems using
non-cooperative rules and taking the results after each computation step (i.e., with
unconditional halting) were considered in [6] and shown to characterize PsET0L.
In sum, we have obtained
{2n | n ≥0} ∈Ngen,maxparOP1(ncoo,table2)2 ∩Ngen−u,maxparOP1(ncoo)1 ,
where gen −u indicates that we take the results generated in the output membrane
after every computation step (i.e., with unconditional halting) and table2 indicates
that we are using 2 tables.
It is rather obvious that the two concepts of using tables and using labels are
pretty much the same – any possibility of choosing a speciﬁc table for each mem-
brane region corresponds with choosing a speciﬁc subset of labels for the rules in
the membrane regions; hence, we immediately infer the following results:
Theorem 2. For any k ≥2 and n ≥1,
PsET0L = PsETk0L
= Psgen,maxparOPn(ncoo,tablek)
= Psgen,maxparOPn(ncoo,lsk).
Proof. We now ﬁrst show that
PsET0L ⊆Psgen,maxparOP1 (ncoo,table2).
Given an ET0L system G = (V,T,w,P1,...,Pn), we adapt the construction given in
Remark 1 for proving PsET0L = PsET20L in order to construct a P system with 2
tables and one membrane generating the elements of Ps(L) in the environment. The
main idea is to work on “colored” variants of the symbols from V, i.e., we deﬁne
the renamings hi : V →V × {i} by hi (a) = [a,i] for all a ∈V and 1 ≤i ≤n, as well
as

198
A. Alhazov and R. Freund
Π = (O,μ = [ ]1,w1 = h1(w),R1 = {T1,T2}, f = 0),
O =

∪n
i=1hi (V)
	
∪T ∪{#},
T1 = {hi(a) →(hi+1(a),here) | a ∈V,1 ≤i < n}
∪{hn(a) →(a,out) | a ∈T}
∪{hn(a) →(#,here) | a ∈V \ T} ∪{# →(#,here)},
T2 = {hi(a) →(h1(w),here) | a →w ∈Pi,1 ≤i ≤n}.
By using the rules hi (a) →(hi+1(a),here) in table T1, we can get to the right “color”
i for simulating the application of table Pi in G by the corresponding rules hi (a) →
(h1 (w),here) from table T2 in Π. Whenever we have reached “color” n we have the
choice to either apply T2 or the ﬁnal rules hn(a) →(a,out). If G has generated a
terminal string w, then the corresponding multiset is sent out to the environment by
applying the rules hn(a) →(a,out) from table T1 and the computation in Π halts,
whereas if any of the rules hn (a) →(#,here) ∈T1 for some a ∈V \ T has to be
applied, we inevitably enter an inﬁnite loop in Π. In sum, we conclude Ps(L) =
Psgen,maxpar(Π).
Obviously, for any k ≥1 we have
Psgen,maxparOP1 (ncoo,tablek) ⊆Psgen,maxparOP1(ncoo,lsk),
as given a P system with k tables
Π = (O,μ = [ ]1,w1,R1 = {T1,...,Tk}, f = 0),
we immediately get the P system with label selection
Π ′ = (O,μ = [ ]1,w1,R1,H,W, f = 0) where
R1 = T ′
1 ∪···∪T ′
k,
T ′
i = {i : p | p ∈Ti}, 1 ≤i ≤k,
H = {1,...,k},
W = {{1},...,{k}}.
Obviously, Psgen,maxpar (Π ′) = Psgen,maxpar (Π).
By the usual “ﬂattening” procedure, see [10], we can get an equivalent P system
with label selection with only one membrane for any P system with label selection
with n membranes; hence, it only remains to show that, for any m ≥1,
Psgen,maxparOP1 (ncoo,lsm) ⊆PsET0L.
For a given P system with label selection
Π = (O,μ = [ ]1,w1,R1,H = {1,...,k},W = {U1,...,Um}, f = 0)
we construct an equivalent ET0L system G as follows:

9
Small P Systems Deﬁning Non-semilinear Sets
199
G =

O∪O′ ∪˜O∪{#},O,h(w),P0,P1,...,Pm
	
,
Pi = {X′ →h′ (b1,tar1)...h′ (bk,tark) |
l : X →(b1,tar1)...(bk,tark) ∈R1,l ∈Ui}
∪{X′ →X′ | X ∈O,¬∃l ∈Ui(l : X →(b1,tar1)...(bk,tark) ∈R1)}
∪{ ˜X →˜X | X ∈O} ∪{X →X | X ∈O} ∪{# →#}, 1 ≤i ≤m,
P0 = { ˜X →X | X ∈O} ∪{X →X | X ∈O} ∪{# →#}
∪{X′ →λ | X ∈O,
¬∃l ∈H(l : X →(b1,tar1)...(bk,tark) ∈R1 ∧∃j(l ∈Uj))}
∪{X′ →# | X ∈O,
∃l ∈H(l : X →(b1,tar1)...(bk,tark) ∈R1 ∧∃j(l ∈Uj))}.
The renaming h is deﬁned by h : O →O′ with h(X) = X′ for X ∈O; the mor-
phism h′ is deﬁned by h′ : O × {here,out} →O′ ∪˜O with h′((X,here)) = X′ and
h′ ((X,out)) = ˜X for X ∈O. With the table Ri we can simulate the application of
rules with labels from Ui. In order to obtain the ﬁnal multiset we have to apply
P0; objects ˜X for X ∈O represent objects X sent out to the environment, i.e., from
the desired result, yet we may only take them as the result in G, too, if for all the
remaining symbols Y represented as Y ′ there exists no rule with which the computa-
tion in Π could be continued, as then the nonterminal # is generated. Therefore we
conclude Ps(L(G)) = Psgen,maxpar (Π).
It is easy to see that with P systems with target agreement and P systems with
target selection we also get a characterization of PsET0L when only working with
non-cooperative rules:
Theorem 3. For any k ≥2,
Ps(ET0L) = Ps(ETk0L)
= Psgen,maxparOPk (ncoo,ts)
= Psgen,maxparOPk (ncoo,ta).
Proof. We ﬁrst show that Ps(ET0L) ⊆Psgen,maxparOP2(ncoo,ts), i.e., we again
start with an ET0L system G = (V,T,w,P1,...,Pn) and adapt the construction given
in Remark 1 in order to construct a P system Π with target selection and only 2
membranes generating the elements of Ps(L) in the innermost membrane. We now
deﬁne the renamings hi : V →V × {i} by hi (a) = [a,i] for all a ∈V and 0 ≤i ≤n.
Then we construct the following P system with target selection:
Π = (O,μ = [ [ ]2 ]1,w1 = h0 (w),w2 = λ,R1,R2, f = 2),
O =

∪n
i=0hi (V)
	
∪T ∪{#},
R1 = {hi (a) →(hi+1(a),tar) | a ∈V,0 ≤i < n,tar ∈{here,in}}
∪{hn(a) →(a,in) | a ∈T}
∪{hn(a) →(#,in) | a ∈V \ T},
R2 = {hi (a) →(h0(w),out) | a →w ∈Pi,1 ≤i ≤n}∪{# →#}.
In a non-deterministic way, with the targets here and in in the rules of R1 we
choose whether to continue with R1 or to send all objects into the inner membrane,

200
A. Alhazov and R. Freund
where one application of Pi is simulated. Only terminal multisets can be sent into the
innermost membrane without causing the system Π to enter an inﬁnite loop there
with the rule # →#. Hence, we infer Psgen,maxpar (Π) = Ps(L(G)).
Due to the proof of Theorem 1, for all n ≥1 we have
Psgen,maxparOPn(ncoo,ta) = Psgen,maxparOPn(coo,ts).
Finally, it remains to prove that
Psgen,maxparOPn (ncoo,ts) ⊆Ps(ET0L) for all n ≥1.
We only sketch the main ideas of the proof: Given a P system with target selection
Π, for each membrane i, in the simulating ET0L system G we use the symbol
[a,i] instead of the symbol a; the tables of the ET0L system then are constructed
by taking every possible variant of choosing a target for each membrane and then
simulating the rules of Π in G on the corresponding symbols [a,i]. At the end, with a
special ﬁnal table of rules Tf the contents of the ﬁnal membrane f may be recovered
as a result of a derivation in G, too, by projecting all objects [a, f] to a and at the
same time projecting all other symbols [a,i] with i ̸= f to λ if there is no rule for a
in the membrane region i of Π or else to the trap symbol #; an additional rule # →#
guarantees that we enter an inﬁnite loop if we tried to apply this ﬁnal table Tf at
the wrong moment when in Π the computation carried out so far had not yet halted.
The technical details of the proof are left to the interested reader.
9.4.4
P Systems with Membrane Dissolution
When using only non-cooperative rules we cannot obtain computational complete-
ness with the additional feature of membrane dissolution; yet, in [8] an inﬁnite hi-
erarchy with respect to the number of membranes was established using membrane
dissolution in a linear membrane structure. Now consider
Π12 = (O = {a},μ = [ [ ]2 ]1,w1 = λ,w2 = a,R1 = /0,R2, f = 1)
where R2 = {a →aa, a →aaδ}.
Π12 doubles the number of objects each turn when only using the rule a →aa
until at some moment the inner membrane may be dissolved by at least for one a
using the dissolution rule a →aaδ, thus stopping the computation.
This generative system has 2 non-cooperative rules and membrane dissolution
and computes mLgen,maxpar(Π12) = {a2n | n ≥0}, i.e.,
{2n | n ≥0} ∈Ngen,maxparOP2(ncoo,δ)2 .

9
Small P Systems Deﬁning Non-semilinear Sets
201
9.4.5
P Systems with Active Membranes Using Two Polarizations
We here use a very restricted variant of a P system with active membranes, i.e., we
have only one membrane (the skin membrane) which may take one of the polariza-
tions 0 and 1:
Π13 = (O = {a},μ = [ ]1,H = {1},P = {0,1},w1 = a,R, f = 1) with
R = {[ a →aa ]0
1, [ a ]0
1 →[ ]1
1a}.
Π13 doubles the multiplicity of objects a in each step using only the rule [ a →
aa ]0
1 until at some moment, after n ≥0 such doubling steps, (exactly) one object a is
sent out by the rule [ a ]0
1 →[ ]1
1a, changing polarization to 1; as with polarization 1
no rule can be applied any more in the skin membrane, the system halts with having
generated a2n−1. In contrast to the previous example, now exactly one symbol a is
used to stop the derivation undergoing the application of the rule [ a ]0
1 →[ ]1
1a,
whereas in the previous example the dissolution rule a →aaδ could have been
applied to an arbitrary non-zero number of copies of a.
Π13 is a P system with one active membrane, 2 polarizations and 2 rules, i.e., we
have
{2n −1 | n ≥0} ∈Ngen,maxparOP2
1 ((a),(c))2 .
P systems with active membranes using non-cooperative rules are known to be
computationally complete even with 2 polarizations and only rules of types (a) and
(c), see [3]. They are computationally complete even without polarizations, but with
creation of an unbounded number of membranes, allowing dissolution of mem-
branes and rules for bringing objects inside a membrane, see [2] and [4].
9.4.6
P Systems with Inhibitors
The following P system uses atomic inhibitors in only two rules:
Π14 = (O = {a,b},μ = [ ]1,w1 = a,R1 = {a →aa|¬b, a →bb|¬b}, f = 1).
Π14 doubles the multiplicity of objects a in each step using the rule a →aa|¬b.
At any time, some objects a may change to bb instead of aa when being evolved
by the rule a →bb|¬b, which causes the system to halt, having generated a2mb2n for
some m,n with m+ n = 2k, k ≥0, n > 0, i.e., we obtain
mLgen,maxpar(Π14) = {a2mb2n | m+ n = 2k, k ≥0, n > 0}
and therefore
{(2m,2n) | m+ n = 2k, k ≥0, n > 0} ∈Psgen,maxparOP1(ncoo,inh1)2 ;
inh1 indicates that we only need atomic inhibitors, i.e., inhibitors of weight 1.
P systems with non-cooperativerules and atomic inhibitors characterize PsET0L,
see [18].

202
A. Alhazov and R. Freund
9.4.7
P Systems with Promoters
The following P system uses one atomic promoter in only one rule:
Π15 = (O = {a,b},μ = [ ]1,w1 = ab,R1 = {a →aa|b, b →b, b →λ}, f = 1).
Π15 doubles the multiplicity of objects a in each step using the rule a →aa|b. At
any time, object b, instead of being kept by the rule b →b, may be erased by the
rule b →λ, thus causing the system to halt, having generated a2n for some n ≥1.
Π15 is a P system with 3 rules and using one atomic promoter; hence, we have
{2n | n ≥0} ∈Ngen,maxparOP1 (ncoo, pro1)3
with pro1 indicating that we only need atomic promoters, i.e., promoters of weight 1.
P systems with non-cooperativerules and atomic promoters characterize PsET0L,
see [18].
9.5
Asynchronous and Sequential P systems
The constructions in this section do not rely on maximal parallelism as those given
in the previous section. Hence, an action like doubling has to be done sequentially,
and therefore it must be controlled in some speciﬁc way. The resulting systems
elaborated in the following subsections fulﬁll their purpose equally well in the asyn-
chronous, in the sequential mode, and even in the maximally parallel mode.
9.5.1
P Systems with One Bi-stable Catalyst
The purely catalytic P system with only one bi-stable catalyst (having the two states
c and ¯c)
Π16 = (O = {a,b,b′,c, ¯c},C = {c, ¯c},μ = [ ]1,w1 = cab′,R1, f = 1) with
R1 = {cb′ →cbb, cb′ →¯cbb, ¯cb →¯cb′, ¯ca →caa}
generates mLgen,δ(Π16) = {anbm | n ≤m ≤2n,n ≥1,m ≥2}, i.e., Psgen,δ (Π16) is a
non-linear set of vectors of natural numbers, and this even holds for any derivation
mode δ ∈{sequ,asyn,maxpar}.
With the catalyst being in state c, Π16 at most doubles the joint population of
objects b and b′, and may change the state to ¯c. With the catalyst being in state ¯c,
some b may be renamed back to b′, and the state is reset to c with adding one object
a. The computation halts if and only if with c no object b′ is present.
Π16 is a purely catalytic P system with only one bi-stable catalyst and 4 catalytic
rules. Clearly, the construction works equally well in the maximally parallel, the
asynchronous, and the sequential mode. In sum, we conclude
{(n,m) | n ≤m ≤2n,n ≥1,m ≥2} ∈Psgen,δOP1 (p2cat1)4

9
Small P Systems Deﬁning Non-semilinear Sets
203
for any δ ∈{sequ,asyn,maxpar}. In general, it is not difﬁcult to see that purely
catalytic P system with only one bi-stable catalyst have sequential behavior even
under maximal parallelism. Hence, their computational power does not exceed that
of partially blind register machines (i.e., we stay within PsMAT).
9.5.2
Asynchronous and Sequential P Systems with Promoters
and Inhibitors
The P system using rules with promoters and inhibitors
Π17 = (O = {a,b,s,t},μ = [ ]1,w1 = ta,R1, f = 1) with
R1 = {a →bb|s, s →t|¬a, b →a|t, t →s|¬b, t →λ|¬b}
generates the well-known nonlinear set of natural numbers {2n | n ≥0}, as we have
mLgen,δ(Π17) = {a2n | n ≥0} for any δ ∈{sequ,asyn,maxpar}.
Π17 toggles between state s (converting objects a in pairs of objects b) and t (re-
naming b to a). The state may be changed when none of its corresponding reactants
remains. State t may be erased instead of being reset to s, which leads to halting.
This generative P system Π17 has 5 non-cooperative rules with atomic promoters
and inhibitors. We note that Π17 works equally well in the sequential, the asyn-
chronous, and the maximally parallel mode. Hence, we infer
{2n | n ≥0} ∈Ngen,δ OP1(ncoo, pro1,inh1)5
for any δ ∈{sequ,asyn,maxpar}. Moreover, Π17 may take advantage of available
parallelism. Recall that smaller solutions have been presented in the previous section
for the maximally parallel derivation mode, even using either promoters or inhibitors
only, but not both.
In general, sequential (and asynchronous) P systems with non-cooperative rules
and promoters and inhibitors are computationally complete, e.g., see [1].
9.6
P Systems with Catalysts
P systems with catalysts were already considered in the originating papers for mem-
brane systems, see [13]. In [9], two catalysts (three catalysts) were shown to be
sufﬁcient for getting computational completeness with catalytic (purely catalytic)
P systems. Whether or not one two catalysts (respectively three catalysts) might
already be enough to obtain computational completeness, is still one of the most
challenging open problems in the area of P systems. We only know that purely cat-
alytic P systems (working in the maximally parallel mode) with only one catalyst
simply correspond with sequential P systems with only one membrane, hence, to
multiset rewriting systems with context-free rules, and therefore can only generate
linear sets.

204
A. Alhazov and R. Freund
Using additional control mechanisms as, for example, priorities or promot-
ers/inhibitors, P systems with only one catalyst can be shown to be computation-
ally complete, e.g., see Chapter 4 of [15]. On the other hand, additional features for
the catalyst may be taken into account; for example, we may use bi-stable catalysts
(catalysts switching between two different states) as already considered in Subsec-
tion 9.5.1.
The ﬁrst system presented in this section continues the work using one bi-stable
catalyst: allowing non-cooperativerules, a non-semilinear set of numbers can be ob-
tained. Then we introduce a new speciﬁc variant of halting avoiding the trap symbol,
which allows us to save a lot of rules, i.e., rules involving the trap symbol. The rest
of the section deals with catalysts without states. All results of this section have one
feature in common: even under maximal parallelism, all halting computations have
a small universal bound (typically, the number of catalytic objects plus one) on the
degree of actual parallelism.
The task of generating a non-semilinear set by catalytic P systems is rather com-
plicated. Not only that we have no mass inﬂuence like we had in Section 9.4, but
we also cannot control the evolution by states like in the previous constructions.
Although catalytic P systems are known to be universal, a direct translation of a
register machine generating powers of 2 yields a rather big number of rules. Be-
low we present two optimized constructions. The ﬁrst one generates a vector set;
since the underlying register machine is partially blind (it does not have zero-test
instructions), it allows us to further simplify the simulation. The latter generates
a non-semilinear set of numbers, which relies on the simulation of two zero-test
instructions in addition to the decrement instructions of the underlying register
machine.
9.6.1
Generating a Non-semilinear Number Set with One
Bi-stable Catalyst
Whereas in Subsection 9.5.1 an accepting P systems with one bi-stable catalyst has
been established, we now turn our attention to the more difﬁcult generative case,
using the maximally parallel mode.
Π18 = (O,C = {c, ¯c},μ = [ ]1,w1 = ¯cpa,R1, f = 1) where
O = {c, ¯c} ∪{a,b} ∪{p,q,s,t} ∪{#},
R1 = {ca →¯cbb, s →t, ¯ct →cs, ct →¯cp, t →#, # →#,
¯cb →ca, p →q, cq →¯cp, ¯cq →cs, ¯cp →¯c, q →#}.
Π18 works in two phases. In phase 1, in addition to the bi-stable catalyst toggling
between c and ¯c, there is a state object present, toggling between s and t. Every two
steps, a is replaced by bb with c changing to ¯c while s changes to t; then ¯ct are reset
to cs. If objects a are no longer present, c is idle for one step, and then ct change to
¯cp, entering phase 2.
In phase 2, the state object toggles between p and q. Every two steps, b is renamed
into a with ¯c changing to c while p changes to q; then cq are reset to ¯cp. If objects b

9
Small P Systems Deﬁning Non-semilinear Sets
205
are no longer present, either ¯c is idle for one step, and then ¯cp change to cs, returning
to phase 1, or ¯c erases p thus causing the system to halt.
In both phases, if the bi-stable catalyst “chooses” a “wrong”rule, then either t
or q are left to themselves, forcing the system to enter an inﬁnite computation,
so this computation does not lead to a result any more; hence, in sum we obtain
mLgen,maxpar(Π18) = {a2n | n ≥0}.
Π18 is a purely catalytic P system with only one bi-stable catalyst and 12 rules,
hence, we have
{2n | n ≥0} ∈Ngen,maxparOP1(2cat1)12 .
The computational completeness of P systems with one bi-stable catalyst work-
ing in the maximally parallel mode was established in [2].
9.6.2
A Special Variant of Halting Avoiding the Trap Symbol
Throughout this section, a main part of the constructions (which is inevitable for
proving computational completeness, too) is the introduction of the trap symbol
# in case the derivation goes the wrong way and by the rule # →# (or c# →c#
with a catalyst c) guaranteeing the derivation never to halt. Yet all these rules can
be avoided if we apply a speciﬁc new halting strategy allowing the system to halt
without yielding a result. This somehow corresponds to the case of a Turing machine
halting its computation in a non-ﬁnal state, and as for Turing machines, where each
such computation halting in a non-ﬁnal state can be transformed into an inﬁnite
computation, in P systems usually the trap rules perform this task to yield an inﬁnite
computation.
This speciﬁc new halting strategy allowing the system to halt without yielding
a result can be deﬁned in various ways, e.g., with a special symbol (like the trap
symbol) appearing in the conﬁguration. As our main goal in this paper is to save as
many rules as possible, we want to stop one step earlier, i.e., before the trap symbol
would be introduced. Hence, we specify a speciﬁc subset of toxic objects Otox; the P
system is only allowed to continue a computation from a conﬁgurationC by using an
applicable multiset of rules covering all copies of objects from Otox occurring in C;
moreover, if there exists no multiset of applicable rules covering all toxic objects,
the whole computation having yielded the conﬁguration C is abandoned, i.e., no
results can be obtained from this computation.
This idea somehow resembles the strategy of using priorities with having the
rules involving the objects from Otox having priority over other rules. Yet this strat-
egy is both weaker and stronger compared with the strategy of priority on rules: on
one hand, we can only specify priorities with respect to the objects from Otox; on
the other hand, if not all copies of toxic objects from Otox can be covered by any
multiset of rules, we stop without getting a result, whereas in the case of priorities
any multiset of rules respecting the priority relation can be used to continue the
computation.
For any variant of P systems, we add the set of toxic objects Otox and in the
speciﬁcation of the families of sets of (vectors of) numbers generated/accepted by P

206
A. Alhazov and R. Freund
systems with toxic objects using rules of type X we add the subscript tox to O, thus
obtaining the families Yγ,maxparOtoxPm (X), for any γ ∈{gen,acc} and m ≥1.
Hence, for the P system with one bi-stable catalyst Π18 elaborated in the preced-
ing subsection, we obtain the corresponding P system Π19 with toxic objects
Π19 = (O,C = {c, ¯c},Otox,μ = [ ]1,w1 = ¯cpa,R1, f = 1) where
O = {c, ¯c} ∪{a,b} ∪{p,q,s,t},
Otox = {q,t},
R1 = {ca →¯cbb, s →t, ¯ct →cs, ct →¯cp,
¯cb →ca, p →q, cq →¯cp, ¯cq →cs, ¯cp →¯c}.
According to the arguments established in the preceding subsection, we immedi-
ately infer mLgen,maxpar(Π19) = {a2n | n ≥0}. This system now does not need more
than 9 rules, and therefore we have
{2n | n ≥0} ∈Ngen,maxparOtoxP1 (2cat1)9 .
9.6.3
General Results for [Purely] Catalytic P Systems with Toxic
Objects
Looking closer into the computational completeness proofs for catalytic P systems
given in [9], we see that the only non-cooperative rules used in the proofs given
there are rules involving the trap symbol. When going to purely catalytic P systems,
we realize that all rules involving the trap symbol can be assigned to one additional
catalyst; for example, to generate any recursively enumerable set of natural numbers
we need two catalysts for catalytic P systems and three catalysts for purely catalytic
P systems.
As the proof of the basic result
PsRE = Psgen,maxparOP1(cat2) = Psgen,maxparOP1(pcat3)
also is the basis of the construction of the [purely] catalytic P system elaborated
in Subsection 9.6.5, we ﬁrst recall the main ideas of the simulations for ADD- and
SUB-instructions of a register machine M = (d,B,l0,h,R).
For every instruction label j ∈B of the register machine to be simulated two
symbols are used to keep the two main catalysts c1,c2 busy, starting with p j ˜p j. At
the end, for the halting label h we use the two rules c1ph →c1 and c2 ˜ph →c2 to
stop the derivation in case the simulation has succeeded to be correct. The number
na stored in register a is represented by na copies of the symbol oa.
Each ADD-instruction j : (ADD(a),k,l), for a ∈{1,2,...,d}, can easily be sim-
ulated by the rules c1p j →c1oapk ˜pk, c1p j →c1oapl ˜pl, and c2 ˜p j →c2.

9
Small P Systems Deﬁning Non-semilinear Sets
207
Each SUB-instruction j : (SUB(a),k,l), only necessary to be considered for a ∈
1,2, is simulated in four steps as shown in the table listed below:
Simulation of the SUB-instruction j : (SUB(a),k,l) if
register a is not empty
register a is empty
cap j →ca ˆp j ˆp′
j
cap j →ca ¯p j ¯p′
j ¯p′′
j
c3−a ˜p j →c3−a
c3−a ˜p j →c3−a
caoa →cac′
a
ca ¯p j →ca
c3−a ˆp j →c3−a
c3−a ¯p′′
j →c3−ap′′
j
cac′
a →cac′′
a
c3−a ˆp′
j →c3−a ˆp′′
j
c3−ap′′
j →c3−ap′
j
ca ˆp′′
j →capk ˜pk
cap′
j →capl ˜pl
c3−ac′′
a →c3−a
c3−a ¯p′
j →c3−a
In addition, trap rules guarantee that in case the guess whether the contents of
register a is empty or not was wrong, the derivation enters an inﬁnite loop with the
rule # →# in the catalytic case or c3# →c3# in the purely catalytic case. These ob-
jects x for which we have such trap rules x →# in the catalytic case or c3x →c3# in
the purely catalytic case, are c′
1,c′′
1,c′
2,c′′
2 and, for every label j of a SUB-instruction
j : (SUB(a),k,l), the objects p j, p′
j, p′′
j, ˆp j, ˆp′′
j, ˜p j, ¯p j, ¯p′′
j, and for every label j of an
ADD-instruction j : (ADD(a),k,l) the objects p j, ˜p j as well. We should like to men-
tion that these trap rules for the objects p j, ˜p j coming from the ADD-instructions
j : (ADD(a),k,l) were forgotten to be included in the proof of the special Corol-
lary 8 in [9], whereas in the more general Theorem 4, due to writing down the range
of the trap rules in a different way, these trap rules for the symbols indicating an
ADD-instruction were included correctly.
The construction shown above strictly follows the proof elaborated in [9]; yet we
observe that many symbols are just needed to keep one of the two catalysts busy for
one step with being erased or else having to evolve to the trap symbol. Hence, every
symbol x for which we only have a rule cix →ciλ, i ∈{1,2}, can be replaced by
just one symbol di. In addition, for p j we now always use c1 and for ˜p j (which in
fact now is replaced by d2) we now always use c2. Finally, we can also replace all
symbols ¯p′
j by just one variable ˆd3−a.
In that way, we obtain the following tables of rules for the simulation of ADD-
instructions and SUB-instructions:
Simulation of the ADD-instruction j : (ADD(a),k,l)
c1p j →c1oapkd2, c1p j →c1oapkd2; c2d2 →c2.
The table for a SUB-instruction now contains several identical entries:

208
A. Alhazov and R. Freund
Simulation of the SUB-instruction j : (SUB(a),k,l) if
register a is not empty
register a is empty
c1p j →c1d3−a ˆp′
j
c1p j →c1da ˆd3−a ¯p′′
j
c2d2 →c2
c2d2 →c2
caoa →cac′
a
cada →ca
c3−ad3−a →c3−a
c3−a ¯p′′
j →c3−ap′′
j
cac′
a →cad3−a
c3−a ˆp′
j →c3−a ˆp′′
j
c3−ap′′
j →c3−ap′
j
ca ˆp′′
j →capkd2
cap′
j →capld2
c3−ad3−a →c3−a
c3−a ˆd3−a →c3−a
The zero-test case now can be reduced considerably to two steps:
c1p j →c1 ¯p j
c2d2 →c2
ca remains idle
c3−a ¯p j →c3−apld2
For x being an element of the following set, we have to add the trap rules c3x →
c3# in the purely catalytic case and the corresponding rules x →# in the catalytic
case:
 
#,d1,d2,c′
1,c′
2
!
∪
 
p j | j : (ADD(a),k,l)
!
∪
 
p j, ˆp′′
j, ¯p j | j : (SUB(a),k,l)
!
In the case of catalytic P systems, the only non-cooperative rules are these trap
rules, and in the case of purely catalytic P systems, the trap rules, and only those,
are associated with the third catalyst c3. If we take exactly those objects for which
such a trap rule exists as toxic objects and omit all trap rules, then we immediately
infer the following computational completeness result, where Otox indicates that we
are using toxic objects:
Theorem 4. PsRE = PsOtoxPgen,maxpar(cat2) = PsOtoxPgen,maxpar(pcat2).
In general, for all the results elaborated in [9] we obtain similar results: when
using toxic objects, then the construction for obtaining the results for catalytic and
purely catalytic P systems coincide, as for example in the preceding theorem, where
in both cases we only need two catalysts (which number currently is assumed to
be the minimal one). Moreover, the simulation becomes somehow deterministic,
as only the correct simulation paths survive; in that sense, deterministic register
machines can be simulated by deterministic [purely] catalytic P systems with toxic
objects.

9
Small P Systems Deﬁning Non-semilinear Sets
209
9.6.4
Generating a Non-Semilinear Vector Set with a [Purely]
Catalytic P System
We now construct [purely] catalytic P systems generating the non-semilinear set of
pairs of natural numbers {(n,m) | n ≤m ≤2n}. First we deﬁne the purely catalytic
P system Π20 generating {an
3am
4 | n ≤m ≤2n}.
Π20 = (O,C = {c1,c2,c3},μ = [ ]1,w1 = c1c2c3a1p1d1d2,R1, f = 1) where
O = {c1,c2,c3} ∪{a1,a2,a3,a4,d1,d2, p1, p2,#},
R1 = {c1a1 →c1, c1p2 →c1a1a4p2, c1p2 →c1a1a3a4p1, c1p2 →c1a3a4,
c2a2 →c2, c2p1 →c2a2a2p1, c2p1 →c2a2a2p2,
c1d2 →c1, c2d1 →c2, c1d1 →c1#, c2d2 →c2#,
c3p1 →c3#, c3p2 →c3#, c3# →c3#}.
This purely catalytic system has 14 rules, and with just omitting the third catalyst
we obtain the corresponding catalytic P system having 14 rules, too. But with using
toxic objects we can save some of (but in this case not all!) the trap rules, i.e., we
obtain the purely catalytic P system with toxic objects Π21 with only 11 rules:
Π21 = (O,C = {c1,c2},Otox,μ = [ ]1,w1 = c1c2a1p1d1d2,R1, f = 1) where
O = {c1,c2} ∪{a1,a2,a3,a4,d1,d2, p1, p2,#},
Otox = {p1, p2,#},
R1 = {c1a1 →c1, c1p2 →c1a1a4p2, c1p2 →c1a1a3a4p1, c1p2 →c1a3a4,
c2a2 →c2, c2p1 →c2a2a2p1, c2p1 →c2a2a2p2,
c1d2 →c1, c2d1 →c2, c1d1 →c1#, c2d2 →c2#}.
In all cases, the derivations work as follows: the objects p1, p2 work as states,
and the objects di, i ∈{1,2}, are used to check that the corresponding catalyst ci is
busy at some stage, otherwise these objects di force the system to enter an inﬁnite
loop with the trap rules or to kill the derivation.
In state p1, we decrement (the number of objects representing) register 1 by the
rule c1a1 →c1 and double their number by applying the rule c2p1 →c2a2a2p1 in
parallel. By using the rule c2p1 →c2a2a2p2 instead, we change to state p2, yet
without checking whether register 1 is already empty. In case the latter rule is used
too late, i.e., if no object a1 is present any more, then we have to use the trap rule
c1d1 →c1#.
In state p2, we decrement (the number of objects representing) register 2 by the
rule c2a2 →c2 and copy (the contents of) this register to register 1, at the same time
adding this number to register 4 by using the rule c1p2 →c1a1a4p2 in parallel. If
this rule is used until no object a2 is present any more, then we have to use the trap
rule c2d2 →c2#. If instead we use the rule c1p2 →c1a1a3a4p1, we switch back to
state 1, at the same moment incrementing register 3, yet again without checking
register a2 for being zero. By using the rule c1p2 →c1a3a4, we end this cycling
between states p1 and p2, i.e., now both p1 and p2 are not present any more, and

210
A. Alhazov and R. Freund
the two objects d1 and d2 have to be eliminated, which can be achieved by using the
two rules c1d2 →c1 and c2d1 →c2 in parallel.
At the end of a computation, even if the two objects d1 and d2 have already been
deleted, the catalysts c1 and c2 will still be active to delete all the remaining objects
a1 and a2, hence, at the end only copies of objects a3 and a4 are present any more.
In sum, we conclude that Ps({an
3am
4 | n ≤m ≤2n}) belongs to
Psgen,maxparOP1(pcat3)14 ∩Psgen,maxparOtoxP1(pcat2)11.
9.6.5
Generating Number Sets with [Purely] Catalytic P Systems
We now are going to improve the result from [19], where a catalytic P system with
54 rules was elaborated, generating the non-semilinear set of natural numbers {2n−
2n | n ≥2}. In the following, we construct a [purely] catalytic P system with 29
rules, generating the (standard) non-semilinear set of natural numbers {2n | n ≥1}.
Yet we will show even more, i.e., our construction works for any set of natural
numbers representing a function g : N →N which is computed in r3 by the following
function program (starting with r1 = b0 and r2 = r3 = 0), with r1,r2,r3 representing
three registers of a register machine, and with the parameters b0,b1,b2,b3,b4 being
natural numbers, b0 ≥1:
function g(r3):
1: if r1 > 0
then begin DEC(r1); ADD(1,r2); goto 1 end
else goto 2
orelse begin ADD(b3,r3); goto 3 end
2: if r2 > 0
then begin DEC(r2); ADD(b2,r1); ADD(b1,r3);
goto 2 end
else begin ADD(b4,r1); goto 1 end;
3: HALT
endfunction
The idea of this program is that in label 1 we copy (register) r1 to r2; the notation
DEC(r) means decrementing (register) r by one, whereas ADD(k,r) means adding
k to (register) r. As soon as register r1 is empty, we switch to label 2 or halt after
having added b3 to (the result register) r3 before. In label 2, we copy back the value
of register r2 to r1, but take it b2 times, at the same time adding b1 times the value
of register r2 to r3, and at the end, when r2 is empty, we add b4 to r1.
The structures
i: if ri > 0
then begin DEC(ri); ADD(1,r3−i); goto i end
else goto j

9
Small P Systems Deﬁning Non-semilinear Sets
211
in this function program correspond with the following instructions in a register
machine program:
i : (SUB(i),i′, j)
i′ : (ADD(3 −i),i,i)
We now describe the functions computed by speciﬁc values of the parameters
b0,b1,b2,b3,b4; thereby let fi(n), i ∈{1,2,3}, denote the value of register i after n
times, n ≥0, having gone through the loops 1 and 2, and g(n) the ﬁnal value of the
function when going through the loops 1 and 2 for n times and then performing loop
1 once more, yet exiting at the end of loop 1 to halt.
In general, we get f2(0) = f2(n) = 0 for all n ≥0 as well as the system of linear
recursions
f1(n + 1) = b2 f1(n)+ b4,
f3(n + 1) = f3(n)+ b1 f1(n)
with f1(0) = b0 and f3(0) = 0 as well as the ﬁnal result g(n) = f3(n)+ b3.
Case 1. b2 = 1:
In this case, we get the system of linear recursions
f1(n + 1) = f1(n)+ b4,
f3(n + 1) = f3(n)+ b1 f1(n).
Solving these recursions yields f1(n) = b0 + b4n and, for n ≥0,
f3(n) = f3(0)+
n−1
∑
i=0
b1 f1(i) = b1
n−1
∑
i=0
(b0 + b4i)
= b1b0n + b1b4n(n −1)/2
hence, g(0) = b3 and, for n ≥0,
g(n) = f3(n)+ b3 = (b1b4/2)n2 + (b1b0 −b1b4/2)n + b3,
i.e., a quadratic function provided b1 ̸= 0 and b4 ̸= 0.
As a speciﬁc example, for (b0,b1,b2,b3,b4) = (1,1,1,0,2) we obtain g(n) = n2.
Case 2. b2 > 1, b1 = 1, b4 = 0:
In this case, we get the linear recursions
f1(n + 1) = b2 f1(n),
f3(n + 1) = f3(n)+ f1(n)
with f1(0) = b0 and f2(0) = f3(0) = 0 as well as the ﬁnal result g(n) = f3(n)+ b3,
i.e., for n ≥0 we obtain f1(n) = b0(b2)n and
f3(n) = f3(0)+
n−1
∑
i=0
b0(b2)i = b0
n−1
∑
i=1
(b2)i = b0(((b2)n −1)/(b2 −1))

212
A. Alhazov and R. Freund
as well as
g(n) = f3(n)+ b3 = b0(((b2)n −1)/(b2 −1))+ b3.
As a speciﬁc example, for (b0,b1,b2,b3,b4) = (1,1,2,1,0) we therefore obtain
g(n) = 2n.
We now start from the constructions for simulating SUB-instructions as already
exhibited in Subsection 9.6.3. In order to get even more efﬁcient simulations, we
save the ﬁrst steps in a speciﬁc way; moreover, every ADD-instruction can be incor-
porated into the rules of the last steps of the simulations, in a similar way as this was
already done in the construction elaborated in [19]. Thus, for the function g with the
parameters b0,b1,b2,b3,b4 we construct the catalytic P system
Π22(b0,b1,b2,b3,b4) = (O,C = {c1,c2},μ = [ ]1,w1,R1, f = 1) where
O = {a1,a2,a3,c′
1,c′
2,d,#}
∪{p j, p′
j, p′′
j, ¯p j | j ∈{1,2}},
w1 = c1c2(a1)b0 p1,
R1 = R1,c ∪R1,#,
and R1,c consists of the catalytic rules contained in the following two tables:
Simulation of the instructions related with label 1 if
register 1 is not empty
register 1 is empty
c1a1 →c1c′
1
c1 remains idle
c2p1 →c2p′
1
c2 ¯p1 →c2p2
c1c′
1 →c1d
c2p′
1 →c2p′′
1
c1p′′
1 →c1p1a2 or
c1p′′
1 →c1 ¯p1a2
c2d →c2
halting:
c2 ¯p1 →c2(a3)3
Simulation of the instructions related with label 2 if
register 2 is not empty
register 1 is empty
c2a2 →c2c′
2
c2 remains idle
c1p2 →c1p′
2
c1 ¯p2 →c1p1(a1)b4
c2c′
2 →c2d
c1p′
2 →c1p′′
2
c2p′′
2 →c2p2(a1)b2(a3)b1 or
c2p′′
2 →c2 ¯p2(a1)b2(a3)b1
c1d →c1
In addition, we have to add trap rules to guarantee that in case of wrong guesses,
the derivation enters an inﬁnite loop with the rule # →# in the catalytic case (or
c3# →c3# in the purely catalytic case). The objects x for which we have such trap

9
Small P Systems Deﬁning Non-semilinear Sets
213
rules x →# in the catalytic case (or c3x →c3# in the purely catalytic case) are # and
d as well as, for j ∈{1,2}, the objects c′
j, p j, p′
j, p′′
j, ¯p j, i.e.,
R1,# = {x →# | x ∈{#,d} ∪{c′
j, p j, p′
j, p′′
j, ¯p j | j ∈{1,2}}}.
In total this yields 17 catalytic rules in R1,c and 12 trap rules in R1,#, i.e., 29 rules
in R1. Obviously, the same number of rules is obtained for the corresponding purely
catalytic P system Π ′
22(b0,b1,b2,b3,b4) where we simply have to add the third cat-
alyst c3 and replace the context-free trap rules x →# by the corresponding catalytic
trap rules c3x →c3#.
We can omit the trap rules when using toxic objects, i.e., if we take
Π ′′
22(b0,b1,b2,b3,b4) = (O′′,C = {c1,c2},Otox,μ = [ ]1,w1,R1,c, f = 1) where
O′′ = {a1,a2,a3,c′
1,c′
2,d}
∪{p j, p′
j, p′′
j, ¯p j | j ∈{1,2}},
Otox = {c′
1,c′
2,d} ∪{p j, p′
j, p′′
j, ¯p j | j ∈{1,2}},
w1 = c1c2(a1)b0 p1,
and R1,c contains the catalytic rules as listed above. This system now only contains
17 rules.
How to argue that the catalytic P system Π22(b0,b1,b2,b3,b4) and the corre-
sponding catalytic P systems Π ′
22(b0,b1,b2,b3,b4) work correctly was exhibited in
detail in [9] as well as in [19]. Yet as we have reduced the number of rules in a con-
siderable way, we have to argue for all possible cases of decrementing or zero-test
register a:
If decrementing of register a is possible, all steps have to be performed exactly
as described in the table. If decrementing fails, then in the last (third) step c3−a
must be used with the rule c3−aa3−a →c3−ac′
3−a as not both registers can be empty
during a computation in the register machine. Yet in the next step the catalyst c3−a is
busy with the program symbol pa or ¯pa, hence, with c′
3−a and one of these program
symbols competing for the same catalyst, one of these symbols will be trapped.
A successful simulation of testing register a for zero is performed in one step
leaving catalyst ca idle. In case the register is not empty, c′
a has to be generated,
and this symbol in the next step will compete for the catalyst ca with the program
symbol p3−a and thus one of these symbols will be trapped.
As explained in [19], we here also mention that when using c2 ¯p2 →c2(a3)b3
instead of c2 ¯p1 →c2p2 in order to reach a halting conﬁguration, the system does
not immediately halt, but instead, if having chosen the rule when register 1 is empty,
uses the sequences of rules c2a2 →c2c′
2, c2c′
2 →c2d, and c2d →c2 (or c1d →c1),
to clear register 2, so that in the end only the objects a3 remain besides the catalysts.
If c2 ¯p2 →c2(a3)b3 is chosen too early, then both registers may be cleared by using
the corresponding rules. The result expressed by the number of symbols a3 is not
affected, if we make such a wrong choice at the end only.
As speciﬁc examples, we therefore obtain
Ngen,maxpar(Π22(1,1,1,0,2)) = {n2 | n ≥0}

214
A. Alhazov and R. Freund
and
Ngen,maxpar(Π22(1,1,2,1,0)) = {2n | n ≥0}.
We observe that with respect to the complexity of the systems, especially con-
cerning the number of rules, there is no difference at all between the sets of natural
numbers growing in a quadratic and in an exponential way, respectively.
In sum, we conclude that all these non-linear sets of natural numbers as described
above are contained in
Ngen,maxparOP1(cat2)29 ∩Ngen,maxparOP1(pcat3)29
as well as in
Ngen,maxparOtoxP1(cat2)17 ∩Ngen,maxparOtoxP1(pcat2)17.
If we do not limit ourselves with the number of catalysts, a better solution with
respect to the number of rules is possible, i.e., for the function g with the parameters
b0,b1,b2,b3,b4 we construct the purely catalytic P system
Π23(b0,b1,b2,b3,b4) = (O,C,μ = [ ]1,w1,R1, f = 1) where
O = C ∪{a1,a2,a3, p1, p2, ph,d1,d2,d′
1,d′
2,d,d′,#}
C = {cr,Decr,cr,0Test | r ∈{1,2}} ∪{cd,cp,c#}
w1 = c1,Decrc1,0Testc2,Decrc2,0Testcdcpc#(a1)b0 p1d2d′
1d′
2dd′,
and R1 consists of the catalytic rules described in the following.
The main idea of this new construction is to use two catalysts for each register
– one for the decrement (cr,Decr) and one for the zero-test (cr,0Test). Moreover, each
SUB-instruction is simulated by two rules, one for the decrement and one for the
zero-test, just allowing the corresponding catalyst to do its work, whereas all other
catalysts are kept busy having introduced dr for cr,Decr and d′
r for cr,0Test. The catalyst
cd for the special symbol d is kept busy by d′; the symbol d is used for trapping in
case an intended decrement fails and can only be allowed to vanish in the last step.
The catalyst cp is used with the instruction labels p1, p2, and ph. The catalyst c# is
only needed for handling the trap symbol #.
For each of the two registers r ∈{1,2}, the following rules perform the decrement
and the zero-test, respectively, in case this operation is initiated by omitting dr or d′
r,
respectively, in the step before.
decrement
cr,Decrar →cr,Decr: if register r is not empty, it is decremented;
cr,Decrd →cr,Decr#: if register r is empty, the catalyst cr,Decr has to be used with
the symbol d thereby introducing the trap symbol #;
cr,Decrdr →cr,Decr: dr keeps cr,Decr busy if another instruction is to be simulated;
c#dr →c##: dr is a “toxic” object which must not stay idle;
cdd′ →cd: d′ keeps cd busy until the end;
c#d′ →c##: d′ is a “toxic” object which must not stay idle.
zero-test
cr,0Testar →cr,0Test#: if register r is not empty, a trap symbol # is
generated;

9
Small P Systems Deﬁning Non-semilinear Sets
215
cr,0Testd′
r →cr,0Test: d′
r keeps cr,0Test busy if another instruction is to be sim-
ulated; for the symbol d′
r we do not need an additional trap rule as the only
alternative is already a trap rule.
The following rules initiate the decrement or the zero-test on register 1 or 2 and
simulate the program for the function g:
(1) cpp1 →cpa2p1d2d′
1d′
2d′: decrement register 1;
cpp1 →cpp2d1d2d′
2d′: zero-test register 1;
(2) cpp2 →cp(a1)b2(a3)b1 p2d1d′
1d′
2d′: decrement register 2;
cpp2 →cp(a1)b4 p1d1d2d′
1d′: zero-test register 2;
cpp1 →cp(a3)b3 phd1d2d′
2d′: zero-test register 1 and go to halting.
(3) Whereas register 1 is already empty, now also register 2 has to be cleaned using
the instruction label ph:
cpph →cpphd1d′
1d′
2d′: decrement register 2;
cpph →cpd1d2d′
1: zero-test register 2 and eliminate ph;
cdd →cd: ﬁnally cd is allowed to eliminate d;
c## →c##: in case something goes wrong during a simulation of an instruction,
this rule keeps the P system in an inﬁnite loop.
In sum, this purley catalytic P system Π23(b0,b1,b2,b3,b4) contains only 23
rules. We can save two catalysts by using non-cooperative rules instead of the cat-
alytic rules assingned to the catalysts cp and c#, thus obtaining the catalytic P system
Π ′
23(b0,b1,b2,b3,b4). Hence, all the non-linear sets of natural numbers described
above are contained in
Ngen,maxparOP1(cat5)23 ∩Ngen,maxparOP1(pcat7)23.
Besides the trap rule c## →c##, only the rules c#dr →c##, r ∈{1,2}, and c#d′ →
c## can be omitted when considering a (purely) catalytic P system with “toxic”
objects, yet this result with 19 rules is even weaker than the previous one where we
also used less catalysts.
9.7
Conclusions
In this paper we have illustrated that many models of P systems need (rather) small
numbers n of rules to deﬁne, i.e., to accept or to generate, some speciﬁc non-
semilinear sets of vectors of natural numbers or non-semilinear sets of natural num-
bers; all the speciﬁc results obtained in this paper are summarized in Table 1.
For the catalytic P systems/purely catalytic P systems it is still one of the most
challenging questions in the area of P systems whether we really need two/three
catalysts to get computational completeness or at least to accept or generate a non-
semilinear set of (vectors of) natural numbers. Moreover, whereas some (very small)
numbers in the table above are already optimal, especially the bigger numbers might
allow for (considerable) improvements, but at the moment are the best solutions
known according to our knowledge.

216
A. Alhazov and R. Freund
Table 1 Examples for using n rules in speciﬁc models of P systems
n models
1 – deterministic cooperative rules, accepting numbers with a
special (non-standard) halting and accepting condition
– non-cooperative rules with target agreement, generating numbers
2 – non-cooperative rules with target selection, generating numbers
– non-cooperative rules with label selection, generating numbers
– non-cooperative rules with tables, generating numbers
– non-cooperative rules with membrane dissolution, generating numbers
– non-cooperative rules in active membranes with
two polarizations, generating numbers
– non-cooperative rules with one inhibitor, generating numbers
3 – non-deterministic cooperative rules, accepting numbers
– symport/antiport rules of weight ≤2 (and size ≤4), accepting numbers
– non-cooperative rules with one promoter, generating numbers
4 – purely catalytic rules with one bi-stable catalyst, generating vectors
– symport/antiport rules of weight ≤2 (and size ≤3), accepting numbers
5 – non-cooperative rules with promoters/inhibitors, generating numbers
9 – [purely] catalytic rules with one bi-stable catalyst and
toxic objects, generating numbers
11 – (purely) catalytic rules with two (three) catalysts and
toxic objects, generating vectors
12 – [purely] catalytic rules with one bi-stable catalyst, generating numbers
14 – (purely) catalytic rules with two (three) catalysts, generating vectors
17 – (purely) catalytic rules with two (three) catalysts and
toxic objects, generating numbers
23 – (purely) catalytic rules with ﬁve (seven) catalysts, generating numbers
29 – (purely) catalytic rules with two (three), generating numbers
Acknowledgements. The ﬁrst author acknowledges project STCU-5384 Models of high per-
formance computations based on biological and quantum approaches awarded by the Science
and Technology Center in the Ukraine. Both authors are very grateful to Petr Sos´ık for point-
ing out the mistake in the proof of Corollary 8 in [9].
References
1. Alhazov, A., Freund, R.: Asynchronous and Maximally Parallel Deterministic Controlled
Non-cooperative P Systems Characterize NFIN and coNFIN. In: Csuhaj-Varj´u, E., Ghe-
orghe, M., Rozenberg, G., Salomaa, A., Vaszil, Gy. (eds.) CMC 2012. LNCS, vol. 7762,
pp. 101–111. Springer, Heidelberg (2013)
2. Alhazov, A.: P Systems without Multiplicities of Symbol-Objects. Information Process-
ing Letters 100(3), 124–129 (2006)

9
Small P Systems Deﬁning Non-semilinear Sets
217
3. Alhazov, A., Freund, R., P˘aun, Gh.: Computational Completeness of P Systems with
Active Membranes and Two Polarizations. In: Margenstern, M. (ed.) MCU 2004. LNCS,
vol. 3354, pp. 82–92. Springer, Heidelberg (2005)
4. Alhazov, A., Freund, R., Riscos-N´u˜nez, A.: One and Two Polarizations, Membrane Cre-
ation and Objects Complexity in P Systems. In: Seventh International Symposium on
Symbolic and Numeric Algorithms for Scientiﬁc Computing, SYNASC 2005, pp. 385–
394. IEEE Computer Society (2005)
5. Alhazov, A., Verlan, S.: Minimization Strategies for Maximally Parallel Multiset Rewrit-
ing Systems. Theoretical Computer Science 412(17), 1581–1591 (2011)
6. Beyreder, M., Freund, R.: Membrane Systems Using Noncooperative Rules with Un-
conditional Halting. In: Corne, D.W., Frisco, P., P˘aun, Gh., Rozenberg, G., Salomaa, A.
(eds.) WMC 2008. LNCS, vol. 5391, pp. 129–136. Springer, Heidelberg (2009)
7. Dassow, J., P˘aun, Gh.: Regulated Rewriting in Formal Language Theory. Springer (1989)
8. Freund, R.: Special Variants of P Systems Inducing an Inﬁnite Hierarchy with Respect
to the Number of Membranes. Bulletin of the EATCS 75, 209–219 (2001)
9. Freund, R., Kari, L., Oswald, M., Sos´ık, P.: Computationally Universal P Systems with-
out Priorities: Two Catalysts Are Sufﬁcient. Theoretical Computer Science 330(2), 251–
266 (2005)
10. Freund, R., Leporati, A., Mauri, G., Porreca, A.E., Verlan, S., Zandron, C.: Flattening in
(Tissue) P systems. In: Alhazov, A., Cojocaru, S., Gheorghe, M., Rogozhin, Yu., Rozen-
berg, G., Salomaa, A. (eds.) CMC 2013. LNCS, vol. 8340, pp. 173–188. Springer, Hei-
delberg (2014)
11. Ibarra, O.H., Woodworth, S.: On Symport/Antiport P Systems with a Small Number
of Objects. International Journal of Computer Mathematics 83(7), 613–629, 137–152
(2006)
12. Minsky, M.L.: Computation: Finite and Inﬁnite Machines. Prentice Hall, Englewood
Cliffs (1967)
13. P˘aun, Gh.: Computing with Membranes. J. Comput. Syst. Sci. 61, 108–143 (2000); also
see TUCS Report 208 (1998), www.tucs.fi
14. P˘aun, Gh.: Membrane Computing. An Introduction. Springer (2002)
15. P˘aun, Gh., Rozenberg, G., Salomaa, A.: The Oxford Handbook of Membrane Comput-
ing, pp. 118–143. Oxford University Press (2010)
16. Rozenberg, G., Salomaa, A.: The Mathematical Theory of L Systems. Academic Press,
New York (1980)
17. Rozenberg, G., Salomaa, A. (eds.): Handbook of Formal Languages, 3 vols. Springer
(1997)
18. Sburlan, D.: Further Results on P Systems with Promoters/Inhibitors. International Jour-
nal of Foundations of Computer Science 17(1), 205–221 (2006)
19. Sos´ık, P.: A Catalytic P System with Two Catalysts Generating a Non-Semilinear Set.
Romanian Journal of Information Science and Technology 16(1), 3–9 (2013)
20. The P systems webpage, http://ppage.psystems.eu

Chapter 10
Generalized Communicating P Automata
Erzs´ebet Csuhaj-Varj´u and Gy¨orgy Vaszil
Abstract. In this paper we introduce and study generalized communicating P au-
tomata, computing devices that combine properties of classical automata and gener-
alized communicating P systems. We show that certain variants of these constructs
are able to accept any recursively enumerable language even with a small number of
cells which interact with each other using only one type of very simple communi-
cation rules, but there are rescticted types which recognize only the class of regular
languages.
10.1
Introduction
P automata are computing devices which combine properties of classical automata
and P systems. The notion of a P system or a membrane system, a computational
framework inspired by functioning and architecture of living cells, was introduced
by Gheorghe P˘aun in 1998, his seminal paper appeared in 2000, [7]. The concept of
a P automaton was introduced in [2]. Since 2000, membrane computing has become
a well-established, active scientiﬁc area. For basic information on P systems the
reader is referred to [8], for details on P automata consult [1].
The notion of a generalized communicating P system was deﬁned in [11], in
order to provide a common generalization of various purely communicating models
in P systems theory. A generalized communicating P system can be considered as a
hypergraph where each node is represented by a cell and each edge is represented
Erzs´ebet Csuhaj-Varj´u
Department of Algorithms and Their Applications, Faculty of Informatics,
E¨otv¨os Lor´and University, P´azm´any P´eter s´et´any 1/c, 1117 Budapest, Hungary
e-mail: csuhaj@inf.elte.hu
Gy¨orgy Vaszil
Department of Computer Science, Faculty of Informatics,
University of Debrecen, Kassai ´ut 26, 4028 Debrecen, Hungary
e-mail: vaszil.gyorgy@inf.unideb.hu
c⃝Springer International Publishing Switzerland 2015
219
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_10

220
E. Csuhaj-Varj´u and G. Vaszil
by a rule. Every cell contains a multiset of objects which — by communication rules
— can be communicated between the cells. The form of a communication rule is
(a,i)(b, j) →(a,k)(b,l) where a and b are objects and i, j,k,l are labels identifying
the input and the output cells. Such a rule means that an object a from cell i and
an object b from cell j move synchronously to cell k and cell l, respectively. Often,
communication rules can also be considered as interaction rules.
Depending on their form, several restrictions on communication rules (modulo
symmetry) were introduced; see [11] for these variants. When a generalized com-
municating P system has only one type of these restricted rules, then we speak of a
generalized communicating P system with minimal interaction or a minimal inter-
action P system (with the given type of rules).
Due to the simplicity of rules, the generative power of minimal interaction P sys-
tems is of particular interest and it has been studied in details. In [11] and [4] it was
proved that eight of the possible nine restricted variants (with respect to the form of
rules) are able to generate any recursively enumerable set of numbers; in the ninth
case only ﬁnite sets of singletons can be obtained. Furthermore, these systems even
with relatively small numbers of cells and simple underlying (hypergraph) architec-
tures are able to achieve this generative power. The necessary number cells to obtain
this power was signiﬁcantly reduced in [6], and in [3] it was shown that minimal in-
teraction P systems where the alphabet of objects is a singleton given with any of
the so-called parallel-shift, presence-move, chain, and join rules are able to generate
every recursively enumerable set of natural numbers.
Cells of generalized communicating P systems communicate with their environ-
ment. In this paper, we describe the behavior of generalized communicating P sys-
tems by the sequences of multisets of symbols entering some designated membranes
from the environment. That is, we consider the generalized communicating P sys-
tem as an automaton that changes its state according to the obtained input. As the
behaviour of a conventional automaton can be represented by the set of input se-
quences it accepts, the behaviour of generalized communicating P automaton can be
described by the set of words that are obtained from its accepted multiset sequences
by certain mapping. The concept of generalized communicating P automaton is a
variant of P automata, purely communicating P systems functioning similarly to
conventional automata.
We show that most of these constructs are computationally complete computing
devices, even using only rules of type split, or symport2, or join, or conditional-
uniport-in, are able to recognize any recursively enumerable language, while in the
case of antiport1 rules they recognize the elements of the class of regular languages.
10.2
Preliminaries and Deﬁnitions
We assume that the reader is familiar with the basics of formal language theory and
membrane computing; for more information, we refer to the monograph [10], and
the handbooks [9] and [8].

10
Generalized Communicating P Automata
221
In the following, we brieﬂy review the notions and the notation we will use. An
alphabet is a ﬁnite non-empty set of symbols. Given an alphabet V, we denote the
set of strings over V by V ∗. If the empty string, ε, is not included, then we use the
notation V +.
A ﬁnite multiset over an alphabet V is a mapping M : V →N where N denotes
the set of non-negative integers, and M(a) for a ∈V is said to be the multiplicity
of a in M. The multiset M can also be represented by any permutation of a string
w = aM(a1)
1
aM(a2)
2
...aM(an)
n
∈V ∗, where if M(x) ̸= 0, then there exists j, 1 ≤j ≤n,
such that x = a j. The set of all ﬁnite multisets over an alphabet V is denoted by V ∗,
the empty multiset is denoted by /0 as in the same way as the empty set. We deﬁne
the union and the difference of two multisets M3 = M1 ∪M2 and M4 = M1 \ M2
as M3(a) = M1(a) + M2(a) and M4(a) = max(M1(a) −M2(a),0), for all a ∈V,
respectively. Sometimes we also identify a set S with the multiset where all elements
of S are present with multiplicity one, so we write expressions like M2 = M1 ∪S or
M2 = M1 \ S where Mi, 1 ≤i ≤2, are multisets over some alphabet V and S ⊆V is
a set.
A generalized communicating P automaton is a P system with generalized com-
munication which is placed in an environment of objects. During its functioning,
similarly to “ordinary” P automata, it consumes multisets from the environment.
The accepted sequences of multisets are those which enter one of some designated
input membranes during a computation which ends in one of the previously deﬁned
accepting conﬁgurations.
The formal deﬁnition of a generalized communicating P automaton is as follows.
Deﬁnition 1. A generalized communicating P automaton with n ≥1 membranes is
a construct Π = (V,E,c0,P,Iin,F) where
• V is an alphabet of objects and E ⊆V is the set of objects present in an arbitrary
number of copies in the environment;
• c0 = (w0,w1,...,wn), is the initial conﬁguration of Π where wi ∈V ∗, 1 ≤i ≤n,
corresponds to the multiset of objects contained initially by cell i, and w0 ∈(V −
E)∗corresponds to the multiset of objects fromV −E initially in the environment;
• P ⊆(V ×N)2×(V ×N)2 is a ﬁnite set of interaction rules of the form (a,i)(b, j) →
(a,k)(b,l) where a,b ∈V, 0 ≤i, j,k,l ≤n. Moreover, if i = j = 0, then {a,b} ∩
(V −E) ̸= /0, that is, a ̸∈E or b ̸∈E;
• Iin ⊆{1,...n} is the set of indices of the input cells; and
• F is a ﬁnite set of n-tuples (v0,...,vn) where vi ∈V ∗or vi = #, 0 ≤i ≤n, denoting
the set of accepting conﬁgurations of Π.
The system consists of n cells containing multisets of objects over V, and there
is an additional region, numbered by 0, called the environment. The environment
contains the objects of E ⊆V in an inﬁnite number of copies. The cells interact
with each other by means of the rules in P. Such an interaction rule of the form
(a,i)(b, j) →(a,k)(b,l) may be applied if there is an object a in cell i and an object
b in cell j. As a result, the object a moves from cell i to cell k and b moves from cell
j to cell l.

222
E. Csuhaj-Varj´u and G. Vaszil
Now we deﬁne the transition relation of a generalized communicating P
automaton.
Deﬁnition 2. Let Π = (V,E,c0,P,Iin,F) be a generalized communicating P au-
tomaton. The transition mapping of Π, δ : V ∗× (V ∗)n+1 →2(V∗)n+1 is deﬁned as
follows:
For two conﬁgurations c,c′ ∈(V ∗)n+1, c′ ∈δ(u,c) if the multiset of symbols u ∈
V ∗enters any of the input cells iin ∈Iin from the environment while the conﬁguration
of Π changes from c to c′ as a result of applying a maximal subset of its rules in
parallel.
By the maximal parallel application of the rules, we assign objects to rules nonde-
terministically in such a way that after the assignment no further rule can be applied
to the remaining objects. A rule can be applied in the same step as many times as
we would like, only the number of copies of objects matters. Those objects which
are left unassigned, remain in the cell where they are and are passed to the next
conﬁguration unchanged.
Note that if both objects a,b come from the environment of the system, then at
least one of them must not be an element of E, that is, at least one of them must
not appear in the environment in an inﬁnite number of copies. This requirement
is necessary since otherwise the maximal parallel rule application could bring an
inﬁnite number of objects into the system in one step, which we would like to avoid.
The language accepted by the generalized communicating P automaton is ob-
tained by considering the multiset sequences accepted by the system.
Deﬁnition 3. The set of accepted input sequences of a generalized communicating
P automaton Π = (V,E,c0,P,Iin,F) is deﬁned by
A(Π) = {v1,...,vs | vi ∈V ∗, there are c0,c1,...cs such that
ci ∈δ(vi,ci−1), 1 ≤i ≤s, and cs ∈F}.
The ﬁnal conﬁgurations are given as (v0,...,vn) ∈F where vi, 0 ≤i ≤n, are
either multisets over V, or the symbol #. A conﬁguration (u0,...,un) ∈(V ∗)n+1 is a
ﬁnal conﬁguration, if there is a (v0,...,vn) ∈F, such that, for all i, 0 ≤i ≤n, where
vi ̸= #, ui = vi. (For all such i, where vi = #, the multiset ui ∈V ∗can be arbitrary).
The words of the accepted language are obtained by applying a mapping to the
accepted multiset sequences. In order to restrict the power of generalized communi-
cating P automata, we consider mappings which are “nonerasing” in the sense that
they do not decrease the length of the sequences they are applied to.
Deﬁnition 4. Let Π = (V,E,c0,P,Iin,F) be a generalized communicating P au-
tomaton, Σ be a ﬁnite alphabet, and f : V ∗→Σ∗be a mapping from the set of
ﬁnite multisets over V to the set of strings over Σ such that f(u) = ε if and only if
u = /0. The language accepted by Π with respect to f is deﬁned as
L(Π, f) = { f(v1)... f(vs) ∈Σ∗| v1,...,vs ∈A(Π)}.

10
Generalized Communicating P Automata
223
Following [11], we list the possible types of interaction rules (modulo symmetry)
that can be used in a generalized communicating P automaton.
LetV be an alphabet and let us consideran interaction rule (a,i)(b, j)→(a,k)(b,l)
with a,b ∈V, i, j,k,l ≥0. There are several cases:
(1) i = j = k ̸= l: the rule is of type conditional-uniport-out (uout in short), it sends
b to cell l provided that a and b are in cell i.
(2) i = k = l ̸= j: the rule is of type conditional-uniport-in (uin in short), it brings
b to cell i provided that a is also in cell i.
(3) i = j,k = l,i ̸= k: the rule is of type symport2 (sym2 in short), it moves a and b
together from cell i to k.
(4) i = l, j = k,i ̸= j: the rules is of type antiport1 (anti1 in short), as the result of
its application, a and b are exchanged in cells i and k.
(5) i = k,i ̸= j,i ̸= l, j ̸= l: the rule is of type presence-move (presence in short), it
moves the object b from cell j to l, provided that there is an object a in cell i
and i, j,l are pairwise different cells.
(6) i = j,i ̸= k,i ̸= l,k ̸= l: the rule is of type split, it sends a and b from cell i to
cells k and l, respectively.
(7) k = l,i ̸= j,k ̸= i,k ̸= j: the rule is of type join, it brings a from cell i and b from
cell j together in cell k.
(8) i = l,i ̸= j,i ̸= k and j ̸= k: the rule is of type chain, it moves a from cell i to cell
k while b is moved from cell j to cell i, i.e., to the cell where a was previously.
(9) i, j,k,l are pairwise different: the rule is of type parallel-shift (shift in short), it
moves a and b from two different cells to other two different cells.
Let L(PAn,X) denote the class of languages accepted by generalized communi-
cating P automata having n cells where n ≥1, and using only rules of type X ∈
{uout,uin,sym2,anti1, presence,split, join,chain,shift}.
In the following we will also need the notion of a k-counter machine from [5].
Deﬁnition 5. A k-counter machine for some k ∈N, is given by M = (k,Q,Σ,q0,P,F)
where Q is a set of internal states with initial state q0 ∈Q and set of accepting
states F ⊆Q; Σ is the ﬁnite input alphabet; and P is a set of instructions of the form
(x, p,(i1,ci1),...,(il,cil);q,ei1,...,eil) where x ∈Σ ∪{ε}, p,q ∈Q, ij ∈{1,...,k},
and cij ∈{> 0,= 0}, eij ∈{+1,−1,0}, 1 ≤j ≤l.
By a rule of the above form, M being in the state p may enter state q ∈Q, if the
value stored in the ijth counter corresponds to cij ∈{> 0,= 0}, 1 ≤j ≤l (that is,
the stored value is zero if cij is “= 0” or nonzero if cij is “≥0”). In case of the appli-
cation of the rule, the counters are modiﬁed according to ei1,...,eil ∈{+1,−1,0}
(denoting increment, decrement, or leaving the value unchanged). If x ∈Σ, then the
machine scanned x on the input tape, and the head moves one cell to the right; if
x = ε, then the machine performs the transition irrespectively of the scanned input
symbol, and the reading head does not move.
A conﬁguration of the k-counter machine M can be denoted by a (k + 2)-tuple
(w,q, j1,..., jk) where w ∈Σ∗is the unread part of the input word written on the
input tape, q ∈Q is the state of the machine, and ji ∈N, 1 ≤i ≤k, is the value stored

224
E. Csuhaj-Varj´u and G. Vaszil
in the ith counter of M. For two conﬁgurations, C and C′, we write C ⇒C′ if M is
capable of changing the conﬁgurationC = (xw,q, j1,..., jk) to C′ = (w,q′, j′
1,..., j′
n)
by reading a symbol x ∈Σ ∪{ε} from the input tape by applying one of its transition
rules.
A word w ∈Σ∗is accepted by the machine if starting in the initial conﬁguration
(w,q0,0,...,0), the input head eventually reaches and reads the rightmost non-blank
symbol on the input tape, and M is in an accepting state qF ∈F, that is, it reaches a
conﬁguration (ε,q f , j1,..., jk) for some ji ∈N, 1 ≤i ≤k.
Deﬁnition 6. The language accepted by the k-counter machine M is deﬁned as
L(M) = {w ∈Σ∗| where (w,q0,0,...,0) ⇒∗(ε,q f ,c1,...,ck)
for some q f ∈F}
where ⇒∗denotes the reﬂexive and transitive closure of ⇒.
In the following, we will use k-counter machines in two special forms which we
will call in this paper the ﬁrst and the second normal forms.
A k-counter machine M = (k,Q,Σ,q0,P,F) is in the ﬁrst normal form, if the
instructions of P can be grouped into pairs of the following form
• (x, p,(i,> 0);q,−1) and (x, p,(i,= 0);r,0) for some x ∈Σ ∪{ε}, p,q,r ∈Q,
i ∈{1,...,k}, or
• (ε, p,(i,> 0);q,+1) and (ε, p,(i,= 0);q,+1) for some p,q ∈Q, i ∈{1,...,k}.
These instruction pairs describe two types of changes the machine can make
in its conﬁguration in one step: First, it can check whether the value stored in a
counter is zero or not, and based on the result of the check, it can either switch to
an internal state and decrement the counter, or switch to another internal state and
leave the counter unchanged. Second, it can increment a counter irrespective of its
contents while not reading anything from the input tape. Thus, for the sake of easier
readability, we will denote the above deﬁned instruction pairs as single instructions
• (x, p;C(i)−,q,r), and
• (ε, p;C(i)+,q),
where x ∈Σ ∪{ε}, p,q,r ∈Q, i ∈{1,...,k}.
Theinstructionsareinterpreted asbefore,thus,aconﬁgurationd = (w,q,c1,...,ck)
can be changed to the conﬁguration d′ = (w′,q′,c′
1,...,c′
k), if
• there is an instruction (x, p;C(i)−,r,s) ∈P such that w′ = xw, q = p, and either
ci > 0, q′ = r, and c′
i = ci −1, or ci = 0, q′ = s, and c′
i = ci;
• there is an instruction (ε, p;C(i)+,r) ∈P such that q = p, q′ = r, and c′
i = ci +1.
In this case w′ = w.
To see that these types of machines are equivalent to the ones given in the usual
way by Deﬁnition 5, we present the following lemma.
Lemma 1. For any k-counter machine M, there is a k-counter machine M′ in the
ﬁrst normal form, such that L(M′) = L(M).

10
Generalized Communicating P Automata
225
Proof. Let M = (k,Q,Σ,q0,P,F) be a k-counter machine. First we show how to
convert the instruction set in such a way that each instruction works with one of the
counters only. Let M′′ = (k,Q′′,Σ,q0,P′′,F) be deﬁned as Q′′ = Q∪{pt,1,..., pt,k−1 |
p ∈Q, t ∈P}, and let P′′ be deﬁned as follows.
P′′ = {(x, p,(i1,ci1); pt,1,ei1) | t : (x, p,(i1,ci1),...,(il,cil);q,ei1,...,eil) ∈P} ∪
{(ε, pt,j−1,(ij,cij); pt,j,eij) | t : (x, p,(i1,ci1),...,(il,cil);q,ei1,...,eil) ∈P,
2 ≤j ≤k −1} ∪
{(ε, pt,l−1,(il,cil);q,eil) | t : (x, p,(i1,ci1),...,(il,cil);q,ei1,...,eil) ∈P}.
The instruction set P′′ contains for each instruction t ∈P which works with l ≤k
counters, a set of l instructions which imply the same change in the conﬁguration of
the machine, but in l steps instead of the one step of M.
Now from M′′, we construct M′ = (k,Q′,Σ,q0,P′,F) in the ﬁrst normal form, as
follows. Ler Q′ = Q′′ ∪{qT} ∪{pt,1, pt,2 | p ∈Q,t ∈P′′}, and for each instruction
in P′′, we introduce one or two instructions in P′ as follows.
• For t : (x, p,(i,> 0);q,−1) ∈P′′, we add (x, p;C(i)−,q,qT) to P′;
• for t : (x, p,(i,> 0);q,0) ∈P′′, we add (x, p;C(i)−, pt,1,qT) and (ε, pt,1;C(i)+,q)
to P′;
• for (x, p,(i,> 0);q,+1) ∈P′′, we add (x, p;C(i)−, pt,1,qT), (ε, pt,1;C(i)+, pt,2),
and (ε, pt,2;C(i)+,q) to P′;
• for (x, p,(i,= 0);q,0) ∈P′′, we add (x, p;C(i)−,qT ,q) to P′; and
• for (x, p,(i,= 0);q,+1) ∈P′′, we add (x, p;C(i)−,qT, pt,1) and (ε, pt,1;C(i)+,q)
to P′.
It is not difﬁcult to see why the machine M′ being in normal form is equivalent
to M.
⊓⊔
We say that a k-counter machine M = (k,Q,Σ,q0,P,F) is in the second normal
form, if P only contains instructions of the form
• (ε, p,(i,> 0);q,−1) or (ε, p,(i,= 0);r,0) for some p,q,r ∈Q, i ∈{1,...,k},
• (x, p,(i,> 0);q,+1) or (x, p,(i,= 0);q,+1) for some x ∈Σ ∪{ε}, p,q ∈Q, i ∈
{1,...,k}.
Lemma 2. For any k-counter machine M, there is a k-counter machine M′ in the
second normal form, such that L(M′) = L(M).
Proof. Let M = (k,Q,Σ,q0,P,F) be a k-counter machine. First, we construct M′′ =
(k,Q′′,Σ,q0,P′′,F) where each instruction works with one of the counters only. This
is done in the same way as in the proof of Lemma 1.
Now from M′′, we construct M′ = (k,Q′,Σ,q0,P′,F) in the second normal form,
as follows. Let Q′ = Q′′ ∪{pt,1, pt,2 | p ∈Q′′,t ∈P′′}, and for each instruction in
P′′ which does not conform to the normal form deﬁnition above, we introduce new
instructions in P′ as follows.

226
E. Csuhaj-Varj´u and G. Vaszil
• For t : (x, p,(i,> 0);q,−1) ∈P′′ with x ̸= ε, we add (x, p,(i,> 0);qt,1,+1),
(ε,qt,1,(i,> 0);qt,2,−1) and (ε,qt,2,(i,> 0);q,−1) to P′;
• for t : (x, p,(i,= 0);q,0) ∈P′′ with x ̸= ε, we add (x, p,(i,= 0);qt,1,+1), and
(ε,qt,1,(i,> 0);q,−1) to P′.
It is not difﬁcult to see that the machine M′ is in the second normal form, and it
is equivalent to M.
⊓⊔
10.3
The Power of P Automata with Generalized
Communication
First we examine the power of systems with split rules.
Theorem 3. L(PA9,split) = RE.
Proof. Let L ⊆Σ∗and let M = (k,Q,Σ,q0,P,F) be a k-counter machine in the ﬁrst
normal form accepting L. We construct a generalized communicating P automaton
Π = (V,E,c0,P′,{4},F) with nine membranes which accepts the language L(M).
Let V = E ∪{q,qr, ¯qr | q ∈Q,r ∈P}∪{X,Y,Z} where E = Σ ∪{Ci | 1 ≤i ≤k},
and let c0 = (w0,w1,...,w9) with w0 = w3 = w5 = w7 = /0, w1 = {q0,X}, w2 = {qr |
q ∈Q,r ∈P}, w4 = Q\ {q0}, w6 = {Y}, w8 = { ¯qr | q ∈Q,r ∈P}, and w9 = {Z}.
The application of a rule of M is simulated by several steps of Π. The symbol
representing the current state of the k-counter machine is in cell 1, while the values
of the counters are stored in cell 5 in the form of symbols Ci where i refers to the ith
counter, 1 ≤i ≤k.
The rule set P′ of Π is deﬁned as follows. For any r : (ε, p,C(i)+;q) ∈P we have
in P′
r1 : (p,1)(X,1) →(p,2)(X,3),
r2 : (p,2)(pr,2) →(p,4)(pr,0),
r3 : (pr,0)(Ci,0) →(pr,3)(Ci,5), r4 : (pr,3)(X,3) →(pr,4)(X,1),
r5 : (pr,4)(q,4) →(pr,2)(q,1).
For any r : (a, p,C(i)−;q,s) ∈P, a ̸= ε, we have in P′
r6 : (p,1)(X,1) →(p,2)(X,7),
r7 : (p,2)(pr,2) →(p,7)(pr,0),
r8 : (pr,0)(a,0) →(pr,5)(a,4),
r9 : (p,7)(X,7) →(p,6)(X,3),
r10 : (pr,5)(Ci,5) →(pr,3)(Ci,0), r11 : (pr,3)(X,3) →(pr,7)(X,5),
r12 : (pr,4)(q,4) →(pr,2)(q,1),
r13 : (p,5)(X,5) →(p,4)(X,1),
r14 : (p,6)(Y,6) →(p,5)(Y,7),
r15 : (pr,7)(Y,7) →(pr,4)(Y,6),
r16 : (p,5)(pr,5) →(p,8)(pr,2),
r17 : (p,8)( ¯pr,8) →(p,4)( ¯pr,9),
r18 : ( ¯pr,9)(Z,9) →( ¯pr,4)(Z,7),
r19 : ( ¯pr,4)(s,4) →( ¯pr,3)(s,1),
r20 : ( ¯pr,3)(X,3) →( ¯pr,8)(X,1), r21 : (Z,7)(Y,7) →(Z,9)(Y,6).
For any r : (ε, p,C(i)−;q,s) ∈P, we have in P′ a similar rule set as above. Instead
of r6 −r9 we have
r22 : (p,1)(X,1) →(p,2)(X,3), r23 : (p,2)(pr,2) →(p,6)(pr,5),
and the rest of the rules, r10 −r21, are the same as above.

10
Generalized Communicating P Automata
227
We show that Π simulates M.
The simulation of a rule r : (ε, p,C(i)+;q) ∈P is done by using rules of the
form r1-r5, as follows. First from cell 1 symbol p moves to cell 2 and auxiliary
symbol X moves to cell 3. In the next step, p continues its way to cell 4 and pr
from cell 2 is sent to the environment. Note that in any moment of time there is at
most one element of type pr, p ∈Q, r ∈R is in the environment and the choice of
pr determines the rule to be simulated. Then, pr and a copy of Ci enter the system,
pr moves to cell 3 and Ci to cell 5. In the next two steps pr moves to cell 4 and
then to cell 2 , X to cell 1 and symbol q from cell 4 to cell 1. Thus, the obtained
conﬁguration corresponds to the new conﬁguration of M, i.e., it changed its state
to q without reading any symbol and the number stored in counter i is increased by
one. Due to the construction of the rule set of P′ no other rules can be applied during
this phase of generation and thus the simulation is correct.
The simulation of r : (a, p,C(i)−;q,s) ∈P, a ̸= ε in Π is done by rules of the
form r6-r21, as follows: At the ﬁrst step symbol p moves from cell 1 to cell 2 and
symbol X to cell 7 (rule r6) . Then, p continues to move to cell 7 and pr is sent to
the environment (rule r7). After then pr brings a letter a from the environment that
moves to cell 4 and meantime pr enters cell 5 (rule r3). In parallel, symbol p and X
leave cell 7 and move to cells 6 and 3, respectively (rule r9). By this phase of the
generation the reading of letter a is simulated.
Then, there are two cases: cell 5 contains at least one Ci (counter i is not empty)
or there is noCi in cell 5 (counter i is empty). In the ﬁrst case, pr leaves cell 5, moves
to cell 3 and one occurrence of Ci is sent out to the environment (rule r10). Then,
pr continues its way from cell 3 to cell 7 and X from cell 3 to cell 5 (rule r11), and
in parallel, symbol p and symbol Y move from cell 6 to cells 5 and 7, respectively
(rule r14). Then, symbol pr is forwarded to cell 4 and symbol Y returns to cell 6
(rule r15), and in parallel, from cell 5, p moves to to cell 4 and X to cell 1 (rule r13).
Finally, symbols pr and q leave cell 4 and move to cells 2 and 1, respectively (rule
r12). Thus, the simulation of conﬁguration change, when M in state p reads letter a,
changes its state to q, and decrements the value of counter i by one is ﬁnished. It is
easy to see that during this phase of the generation no other rule is applicable and
the application of no other rule of M is simulated.
If no Ci can be found in cell 5, then the next applicable rule of P′ moves p and Y
from cell 6 to cells 5 and 7, respectively (rule r14). After then p leaves cell 5 to cell
8 and pr leaves cell 5 to cell 2 (rule r16). In the next step, p leaves to cell 4 and ¯pr
from cell 8 moves to cell 9 (rule r17). Then ¯pr and Z leave cell 9 and ¯pr moves to cell
4 and Z moves to cell 7 (rule r18). Then, ¯pr and s leave cell 4, ¯pr enters cell 3 and s
cell 1 (rule r19). In parallel, Z and Y from cell 7 move to cells 8 and 6, respectively
(rule r21). At the last step, ¯pr and X leave cell 3 and move to cells 8 and cell 1,
respectively (rule r20). This generation phase simulates the conﬁguration change of
M when it changes its state p to s, due to the fact that the value of counter i is zero,
i.e., it cannot be decremented. As in the previous case, it can be seen that the rules
can be applied only in the above order, and during this phase of the generation, the
application of no other rule of M can be simulated.

228
E. Csuhaj-Varj´u and G. Vaszil
Finally, we show how the simulation of a rule r : (ε, p,C(i)−;q,s) ∈P is done by
rules of Π. The procedure is very similar to the previous one, the only difference is
that instead of reading some letter a, M does not read any symbol. Thus, instead of
using rules r6-r9, we use rules r22 and r23, which result in moving p from cell 1 via
cell 2 to cell 6, symbol X form cell 1 to cell 3 and symbol pr from cell 2 to cell 5.
Then, we use rules of the form r10-r21 and the same reasoning as above.
Due to the construction of Π, only the instructions that M has and is able to
perform can be simulated. Thus, if we have F = {(/0,qX,#,...,#) | q ∈F} as the set
of accepting conﬁgurations, and f is the identity function (we deﬁne f : V ∗→V ∗
as f(a) = a for all a ∈V), then L(Π, f) contains the same words as L(M).
⊓⊔
According to the previous statement, generalized communicating P automata can
accept any recursively enumerable language with split rules. Since split rules can be
simulated by sym2 rules, Theorem 3 also implies the computational completeness
of systems with sym2 rules only. This is shown in the next theorem.
Theorem 4. L(PA∗,sym2) = RE.
Proof. Let L ⊆Σ∗be an arbitrary language and let Π be the generalized communi-
cating P automaton from the proof of Theorem 3 using split rules and the mapping
f, accepting the language L(Π, f) = L.
Notice that Π is constructed in such a way that no rule is used two or more times
in parallel, and that no rule is used in two consecutive computational steps. Under
these conditions, we can simulate split rules with symport2 rules, and thus, construct
a system Π ′ with L(Π) = L(Π ′) having only symport2 rules.
To prove the statement, we recall a construction from [4] that demonstrates how
a split rule l : (a,i)(b,i) →(a, j)(b,k) where a,b are objects, i, j,k are numbers of
cells, and l is the label of the rule, can be simulated by symport2 rules.
Let Π ′ contain for any split rule l of Π a cell 1l (for different rules different
cells), and initially let this cell contain objects Al and Bl, and let cells j and k con-
tain objects A′
l and B′
l, respectively (objects Al,Bl,A′
l,B′
l are pairwise different and
different from the objects of Π). Suppose furthermore that E contains one more
object D, different from all of the previous ones, including the objects of Π.
By [4], the following rules simulate rule l:
l1 : (a,i)(b,i) →(a,1l)(b,1l),
l2 : (a,1l)(Al,1l) →(a, j)(Al, j),
l3 : (b,1l)(Bl,1l) →(b,k)(Bl,k),
l4 : (Al, j)(A′
l, j) →(Al,0)(A′
l,0), l5 : (Al,0)(D,0) →(Al,1l)(D,1l),
l6 : (Bl,k)(B′
l,k)) →(Bl,0)(B′
l,0), l7 : (Bl,0)(D,0) →(Bl,1l)(D,1l),
l8 : (A′
l,0)(D,0) →(A′
l, j)(D, j),
l9 : (B′
l,0)(D,0) →(B′
l,k)(D,k).
It can easily be seen that rule l1 sends a and b to cell 1l. From there, a moves to
cell j and b moves to cell k, accompanied by symbols Al and Bl, respectively. After
then, by rules l4 -l9 the accompanying symbols return to cell 1l. Notice that during
this phase of functioning simulation of no other split rules can start.

10
Generalized Communicating P Automata
229
Let Π = (V,E,c0,P,Iin,F) be the system from the proof of Theorem 3. Now Π ′
is obtained from Π by adding for any rule l of Π a cell 1l, as described above, such
that initially this cell contain objects Al and Bl and let add to the initial contents of
cells j and k objects A′
l and B′
l, respectively. Let E have one more symbol D, see
above, and let us add for each rule l one more cell 2l.
Now P′ is obtained by replacing any split rule l with the set of rules above, with
the exception of l1, instead of which, the new rules
l′
1 : (a,i)(b,i) →(a,2l)(b,2l), l′′
1 : (a,2l)(b,2l) →(a,1l)(b,1l),
should be used.
If we set Iin = {2l | l ∈P}, and we have f ′ : V ∗→V ∗with f ′(ua) = (a) for
any multiset ua where u ∈V ∗, a ∈Σ, then it is not difﬁcult to see that L(Π ′, f ′) =
L(Π, f) holds.
⊓⊔
Now we turn to systems with rules of type antiport1. As shown in the next theo-
rem, they only characterize the class of regular languages.
Theorem 5. L(PA∗,anti1) = REG.
Proof. We ﬁrst show that L(PA∗,anti1) ⊆REG. Let Π = (V,E,c0,P,Iin,F) be
an arbitrary generalized communicating P automaton, Σ be a ﬁnite alphabet, and
f : V ∗→Σ∗be a mapping which satisﬁes conditions of Deﬁnition 4. Since the
application of any rule of Π means exchange of symbols between two cells (includ-
ing the environment), therefore the total number of objects inside the regions of Π
remains unchanged under its functioning. This implies that the number of conﬁgu-
rations of Π and thus the number of multisets entering the input cell (and thus the
number of their f-images) are bounded by some constants. Then, starting from the
transition mapping δ of Π, we can construct a nondeterministic ﬁnite automaton
M = (Q,Σ,δ ′,q0,F) such that L(M) = L(Π, f) holds. (For M, Q denotes the ﬁnite
nonempty set of states, Σ is the input alphabet, δ ′ is the transition mapping, q0 is
the initial state, and F is the set of accepting states; for details on deterministic and
non-deterministic ﬁnite automata consult [9].) Let C = {c0,...,cn} be the set of
conﬁgurations of Π and ¯V be the set of multisets of objects that enter the input cells
under functioning of Π. For any transition ci ∈δ(v,cj) in Π, where 0 ≤i, j ≤n,
v ∈¯V, we construct a series of transitions in δ ′ as follows: if f(v) = x1 ...xm, where
x1,...,xm ∈Σ, then [cj1] ∈δ ′(x1,[cj]), [cjk] = δ ′(xk,[cjk−1]) for 2 ≤k ≤m−1, and
[ci] = δ ′(xm,[cjm−1]). Let us deﬁne Q as the set of all symbols [cjk], where cj ∈C,
and 1 ≤k ≤max, where max is the length of the longest word f(v) entering the input
cell from the environment. Let F = {[cl] | cl ∈F} and q0 = [c0]. By the construction,
it is easy to see that L(M) = L(Π, f), which implies that L(PA∗,anti1) ⊆REG.
Next we show that the reverse inclusion also holds. Let M = (Q,Σ,δ,q0,F)
be a ﬁnite deterministic automaton. We construct a generalized communicating
P automaton Π = (V,E,c0,P,{1},F) and give a mapping f such that L(Π, f) =
L(M) holds. Let Π have only one cell and let V = {qini} ∪{[p,a,q] | p,q ∈Q,a ∈
Σ,δ(p,a) = q}. Let E = {[p,a,q] | p,q ∈Q,a ∈Σ,δ(p,a) = q}, w0 = qini, F =
{[p,a,q] | p ∈Q,q ∈F,a ∈Σ,δ(p,a) = q}.

230
E. Csuhaj-Varj´u and G. Vaszil
Let P consist of the following rules:
• (pini,1)([q0,a, p],0) →(pini,0)([q0,a, p],1), for any transition δ(q0,a) = p in
M, where a ∈Σ,
• ([p,a,q],1)([q,b,r],0) →([p,a,q],0)([q,b,r],1),
where
δ(p,a) = q
and
δ(q,b) = r are transitions following each other in M.
Let F = {[p,a,q] | p,q ∈Q,a ∈Σ,q ∈F,δ(p,a) = q}.
It is easy to see that Π simulates M. If we deﬁne f[p,a,q] = a for each p,q ∈Q
a ∈Σ where δ(p,a) = q in M, then it comes immediately that L(Π, f) = L(M)
holds.
⊓⊔
Theorem 6. L(PA8, join) = RE.
Proof. Let L ⊆Σ∗and let M = (k,Q,Σ,q0,P,F) be a k-counter machine in the ﬁrst
normal form accepting L. We construct a generalized communicating P automaton
Π = (V,E,c0,P′,{5},F) and a mapping f : V ∗→Σ∗, such that L(M) = L(Π, f).
Let V = E ∪{q,qr | q ∈Q,r ∈P} where E = Σ ∪{Z} ∪{Ci | 1 ≤i ≤k}, and let
c0 = (w0,w1,...,w7) with w0 = w3 = w5 = w6 = w7 = w8 = /0, w1 = {q0}, w2 =
Q\ {q0}, and w4 = {qr | q ∈Q,r ∈P}.
The rule set P′ of Π is deﬁned as follows. For any q ∈Q, the rule
¯rq : (q,4)(Z,0) →(q,1)(Z,1)
(10.1)
is added to P′, and we also have (Z,0)(Z,1) →(Z,8)(Z,8).
Furthermore, for any r : (ε, p,C(i)+;q) ∈P we have in P′
r1 : (p,1)(pr,4) →(p,2)(pr,2), r2 : (pr,2)(Ci,0) →(pr,3)(Ci,3),
r3 : (pr,3)(q,2) →(pr,4)(q,4).
For any r : (a, p,C(i)−;q,s) ∈P, a ̸= ε, we have in P′
r4 : (p,1)(pr,4) →(p,8)(pr,8),
r5 : (pr,8)(a,0) →(pr,5)(a,5),
r6 : (p,8)(Z,0) →(p,5)(Z,5),
r7 : (p,5)(Z,0) →(p,6)(Z,6),
r8 : (pr,5)(Ci,3) →(pr,0)(Ci,0), r9 : (p,6)(pr,5) →(p,3)(pr,3),
r10 : (p,6)(pr,0) →(p,7)(pr,7),
r11 : (pr,3)(s,2) →(pr,4)(s,4),
r12 : (pr,7)(q,2) →(pr,4)(q,4),
r13 : (p,3)(Z,0) →(p,2)(Z,2),
r14 : (p,7)(Z,0) →(p,2)(Z,2).
For any r : (ε, p,C(i)−;q,s) ∈P, we have in P′ instead of r4 −r6 the single rule
r15 : (p,1)(pr,4) →(p,5)(pr,5),
and the rest of the rules, r7 −r14, are the same as above.
We prove that Π simulates M. The application of a rule of M is simulated by
several steps of P′. The symbol representing the current state of the k-counter ma-
chine is in cell 1, while the values of the counters are stored in cell 3 in the form of
symbols Ci where i refers to the ith counter, 1 ≤i ≤k.

10
Generalized Communicating P Automata
231
The application of rule r : (ε, p,C(i)+;q) ∈P is simulated in P′ as follows: First,
symbol p moves from cell 1 and meantime pr leaves cell 4 and moves to cell 2 (rule
r1). Notice the role of pr, this symbol identiﬁes the rule to be simulated. Then, pr
brings from the environment a symbol Ci into cell 2 and both move to cell 3 (rule
r2). In the next step, pr in cell 3 and q in cell 2, both move to cell 4, where after
arrival q brings one symbol Z from the environment and both move to cell 1 (rule r3
and rule ¯rq). The obtained conﬁguration of Π corresponds to the new conﬁguration
of M, i.e., changing its state p to q without reading any symbol and increasing the
number of symbols in counter i by one. Due to the construction of the rule set of P′
no other rules can be applied during this phase of generation and the rules can only
be applied in this order.
Next we show how r : (a, p,C(i)−;q,s) ∈P, a ̸= ε is simulated by rules in P′.
First, symbols p and pr leave cells 1 and 4, respectively and both move to cell 8
(rule r4). Then pr brings a symbol a from the environment and both move to cell 5
(rule r5). At this point, the simulation of reading of letter a is ﬁnished. In parallel
with this rule application, symbol p brings a symbol Z from the environment and
both enter cell 5 (rule 6). Then, p brings one Z from the environment and the two
symbols together are forwarded to cell 6 (rule r7). There are two cases:
At the ﬁrst case, there exists a symbol Ci in cell 3, then both this symbol and
symbol pr move to the environment (rule r8), and in the next step p from cell 6 and
pr from the environment move together to cell 7 (rule r10). The procedure continues
by applying rules r12, forwarding pr to cell 4 and q from cell 2 to cell 4, then by
applying rule ¯rq, i.e., bringing one Z in from the environment and moving q and this
Z together to cell 1. In parallel with the application of ¯rq, p from cell 7 and a Z from
the environment move to cell 2 (rule r14). In this way, the simulation of the change
of the conﬁguration, when M in state p reads letter a, changes its state to q, and
decrements the value of counter i by one is ﬁnished. Due to the deﬁnition of rules of
P′, during this phase of the generation no other rule is applicable and the application
of no other rule of M can be simulated.
At the second case, there is no symbol Ci in cell 3, which implies that rule r8
cannot be applied, so pr remains in cell 5 during the computational step when p
moves from cell 5 to cell 6. Then, the only applicable rule is r9 which moves p from
cell 6 and pr from cell 5 to cell 3. From cell 3, pr moves to cell 4, simultaneously
with s, that leaves cell 2 and moves to cell 4. Then applying rule ¯rs, s from cell
4 moves to cell 1 together with a symbol Z brought in from the environment, and
meantime p from cell 3 and a symbol Z from the environment move to cell 2. This
way that conﬁguration change of M is simulated when state p changes to s, due
to that the value of counter i is zero, i.e., it cannot be decremented. It can be seen
that during this phase of the generation the application of no other rule of M can be
simulated and the rules can be applied only in the above order.
Next we show the simulation of a rule r : (ε, p,C(i)−;q,s) ∈P by rules of Π.
The procedure is almost the same as the previous one. Since M does not read any
symbol, instead of using rules r4 −r6, we use rule r15 which moves symbol p from
cell 1 and symbol pr from cell 4 to cell 5, and then we use rules of the form r7 −r14
in the same way as above.

232
E. Csuhaj-Varj´u and G. Vaszil
Due to the construction and functioning of Π no other instruction than that M has
and able to perform can be simulated. Thus, if we have F = {(/0,q,#,...,#) | q ∈F}
as the set of accepting conﬁgurations, and deﬁne f : V ∗→Σ∗with f(aZ) = a, then
L(Π, f) = L(M).
⊓⊔
Now we examine generalized communicating P automata with conditional-
uniport-in rules. To show that these systems also generate any recursively enumer-
able language, we need a bit more involved construction.
Theorem 7. L(PA8,uin) = RE.
Proof. Let L ⊆Σ∗and let M be a k-counter machine in the second normal form
accepting L. Before we start the construction of a generalized communicating P au-
tomaton simulating M, we ﬁrst show how to construct a system with four regions in
such a way that there are three symbols Ai, i = 0,1,2, appearing in the ﬁrst region
after the (3 · n + i)th steps of the computation for each n ≥0, in such a way that
they are “free” in the next step, that is, there is no rule which can be applied to the
symbol Ai present in the ﬁrst region in the (3 ·n + i+ 1)th step of the system.
Consider the alphabet {A j,cj,d j,ej | 0 ≤j ≤2} ∪{ f,g,T} and a system with
four membranes having an initial conﬁguration
(A0A2c0c1c2, d1d2A1e0e1e2, fd0, g)
(10.2)
and the following rules. First, for each j, 0 ≤j ≤2, let us have
r1,j : (cj,1)(A j,2) →(cj,1)(A j,1), r2,j : (d j,2)(A j,1) →(d j,2)(A j,2),
r3,j : (ej,2)(d j,3) →(ej,2)(d j,2), r4,j : (f,3)(d j,2) →(f,3)(d j,3),
r5,j : (g,4)(d j,2) →(g,4)(d j,4),
r6,j : (d j,4)(T,0) →(d j,4)(T,4).
To see how these rules work, consider n = j = 0 and the initial conﬁguration
shown in (10.2). Observe that A j = A0 is present in region 1, and there is no rule to
be applied to it in the ﬁrst step because there is no d0 present in the second region
(see rule r2,0). After the ﬁrst step we get the following sequence of conﬁgurations:
(A0A2c0c1c2, d1d2A1e0e1e2, fd0, g) ⇒
(A1A0c0c1c2, d2d0A2e0e1e2, fd1, g) ⇒
(10.3)
(A2A1c0c1c2, d0d1A0e0e1e2, fd2, g) ⇒
(10.4)
(A0A2c0c1c2, d1d2A1e0e1e2, fd0, g) ⇒
(10.5)
Note that there is no rule to be applied to A0 in the ﬁrst step. Then, after the ﬁrst
step, in (10.3), A1 has appeared in region 1 and there is no rule to be applied to it in
the second step. After the second step, in (10.4), A2 appears in region 1 and there is
no rule to be applied to it in the third step, resulting in (10.5) where the state of the

10
Generalized Communicating P Automata
233
system returns to its initial conﬁguration given in (10.2), and the whole process can
start again.
If we have these four membranes as part of a greater system, A j in region one
could possibly be “used” in steps 3·n+ j+1, n ≥0, by other rules in the additional
part of the greater system without having any inﬂuence on the above described pro-
cessing cycle. If, however, any of the A j symbols would be used by the additional
rules of the additional part of the system in a computational step different from
those given above, then one of the d j symbols, 0 ≤j ≤2, would have to be moved
to region 4 by the rules r5,j, and then the rules r6,j would prevent the halting of the
computation of the system.
For the construction of the generalized communicating P automaton simulating
the k-counter machine M, we will extend the above system in such a way that the
symbols A j, j = 0,1,2, appearing in the ﬁrst region will only be “used” when they
are “free”, that is, in the computational step directly after their appearance. This
way the three steps cycle described above can continue to occur during the whole
computation of the system.
Let
M = (k,Q,Σ,q0,P,F),
and
let
us
denote
for
a
rule
r ∈P
of
one
of
the
forms
(ε, p,(i,> 0);q,−1),
(ε, p,(i,= 0);q,0),
(x, p,(i,>
0);q,+1), or (x, p,(i,= 0);q,+1), x ∈Σ ∪{ε},
p,q ∈Q, i ∈{1,...,k},
the
state
p
as
State(r) ∈Q
and
the
state
q
as
NextState(r) ∈Q.
Let
us
also
deﬁne
for
any
r ∈P
the
set
NextRule(r) ⊆P
as
follows:
NextRule(r) = {r′ ∈P | NextState(r) = State(r′)}.
Now we construct Π = (V,E,c0,P′,{5},F), a generalized communicating P au-
tomaton such that L(M) = L(Π, f) for the mapping f : V ∗→V ∗with f(a) = a,
a ∈V. To construct Π, we start with a system of eight regions which is constructed
according to the ideas above in such a way, that the symbols Ai,0, Ai,1, Ai,2 appear
periodically in each region 1 ≤i ≤8, and can be “used” by other rules in the 3·n+1,
3 ·n + 2, and 3·n step of the computation, respectively, for n ≥0, in the same way
as described above. Let Vcycl and Pcycl denote the set of rules and the set of sym-
bols used by them, similar to those listed above (with the difference that instead of
A j, 0 ≤j ≤2, we have Ai,j for each 1 ≤i ≤8), and let wcycl,i, 1 ≤i ≤8 denote the
initial membrane contents of objects from Vcycl necessary for their functioning.
Let V = Vcycl ∪E ∪{r′
0}∪{r,r′ | r ∈lab(P)} where E = Σ ∪{Z,D,X}∪{Ci | 1 ≤
i ≤k} and let the initial conﬁguration be
c0 = (w0,...,w8)
with w1 = wcycl,1 ∪{r′
0}, w2 = wcycl,2 ∪{D}, w3 = wcycl,3 ∪{r | r ∈lab(P)}, w4 =
wcycl,4 ∪{r′ | r ∈lab(P)}, wi = wcycl,i for 5 ≤i ≤7, and w8 = wcycl,8 ∪{X}.
The rule set P′ is deﬁned as follows. Besides the rules in Pcycl, for all rules r ∈P
with State(r) = q0, we add to P′ the rules
(r′
0,1)(r,3) →(r′
0,1)(r,1), and (A4,1,4)(r′
0,1) →(A4,1,4)(r′
0,4),

234
E. Csuhaj-Varj´u and G. Vaszil
and for all rules r, p ∈P, such that r ∈NextRule(p), we add to P′ the rules
(p′,1)(r,3) →(p′,1)(r,1), and (A1,2,1)(r′,5) →(A1,2,1)(r′,1).
Furthermore,foranyr∈Pwherer : (x, p,(i,> 0);q,+1)orr : (x, p,(i,= 0);q,+1)
for some p,q ∈Q, i ∈{1,...,k}, and x = ε, we have in P′
r1 : (A6,1,6)(r,1) →(A6,1,6)(r,6),
r2 : (D,2)(r,6) →(D,2)(r,2),
r3 : (r,2)(Ci,0) →(r,2)(Ci,2),
r4 : (A4,1,4)(r′,1) →(A4,1,4)(r′,4),
r5 : (A5,2,5)(r′,4) →(A5,2,5)(r′,5), r6 : (r′,5)(r,2) →(r′,5)(r,5),
r7 : (A3,2,3)(r,5) →(A3,2,3)(r,3),
r8 : (X,8)(r,2) →(X,8)(r,8),
r9 : (r,8)(Z,0) →(r,8)(Z,8),
and if x = a ∈Σ (x ̸= ε), we also have r6′ : (r,5)(a,0) →(r,5)(a,5).
In addition, for any r : (ε, p,(i,> 0);q,−1) ∈P, we have in P′ the rules
r10 : (A7,1,7)(r,1) →(A7,1,7)(r,7),
r11 : (A8,1,8)(Ci,2) →(A8,1,8)(Ci,8),
r12 : (r,7)(Ci,8) →(r,7)(Ci,7),
r13 : (A4,0,4)(r,7) →(A4,0,4)(r,4),
r14 : (A3,2,3)(r′,4) →(A3,2,3)(r′,3), r15 : (r′,3)(r,4) →(r′,3)(r,3),
r16 : (A1,2,1)(r′,3) →(A1,2,1)(r′,1), r17 : (A5,2,5)(r,7) →(A5,2,5)(r,5),
r18 : (r,5)(Z,0) →(r,5)(Z,5),
r19 : (Z,5)(Z,0) →(Z,5)(Z,5),
r20 : (A2,0,2)(Ci,8) →(A2,0,2)(Ci,2), r21 : (A8,1,8)(Ci,2) →(A8,1,8)(Ci,8),
r22 : (X,8)(r,4) →(X,8)(r,8),
r23 : (r,8)(Z,0) →(r,8)(Z,8),
and for any r : (ε, p,(i,= 0);q,0) ∈P, we have in P′ the following (for the sake of
easier readability, we enumerate again also those rules, which do not depend on r,
thus, which we already presented above)
r24 : (A6,1,6)(r,1) →(A6,1,6)(r,6),
r25 : (A8,1,8)(Ci,2) →(A8,1,8)(Ci,8),
r26 : (r,6)(Ci,8) →(r,6)(Ci,6),
r27 : (A4,0,4)(r,6) →(A4,0,4)(r,4),
r28 : (A3,2,3)(r′,4) →(A3,2,3)(r′,3), r29 : (r′,3)(r,4) →(r′,3)(r,3),
r30 : (A1,2,1)(r′,3) →(A1,2,1)(r′,1), r31 : (Ci,6)(Z,0) →(Ci,6)(Z,6),
r32 : (Z,6)(Z,0) →(Z,6)(Z,6),
r33 : (A8,1,8)(Ci,2) →(A8,1,8)(Ci,8),
r34 : (X,8)(r,4) →(X,8)(r,8),
r35 : (r,8)(Z,0) →(r,8)(Z,8).
To see how this system simulates the computations of the k-counter machine M,
consider the initial conﬁguration where each region i, 1 ≤i ≤8 contains Ai,0, the
ﬁrst region r′
0, the third and the fourth regions objects r and r′, respectively, for
all r ∈P, and the eighth region the symbol X (besides the objects in wcycl,i which
we ignore in the following for the sake of simplicity). First r′
0 moves a symbol r
corresponding to a rule r ∈P with State(r) = q0 to the ﬁrst region with the rule
(r′
0,1)(r,3) →(r′
0,1)(r,1), then the system simulates r.
If r is an increment rule r : (x, p,(i,> 0);q,+1) or r : (x, p,(i,= 0);q,+1), Π
uses the rules r1,...,r9 to import an object Ci to region 2 (the multiplicity of this
object corresponds to the value stored in the ith counter of M), and if a x = a ∈Σ,
then rule r6′ reads the corresponding input symbol from the environment. After two

10
Generalized Communicating P Automata
235
Ai,0,...,Ai,2 cycles (after 6 steps), r′ appears in region one, the rule (r′,1)(s,3) →
(r′,1)(s,1) chooses the next instruction s ∈P to be simulated. The simulation of the
increment instruction is nondeterministic, thus, it might be possible that Π makes a
“wrong” nondeterministic choice, in which case the object r is moved to region 8
and the rule r9 : (r,8)(Z,0) →(r,8)(Z,8) makes sure that the computation can never
reach a halting conﬁguration.
The simulation of a decrement instruction r : (ε, p,(i,> 0);q,−1) ∈P is done
with the rules r10,...,r23. These rules remove one Ci symbol from region 2, or if
there were no Ci present, then r is moved either to region 5 or 8, in which cases the
computation may never reach a halting conﬁguration.
The zero check instruction is simulated in a similar way by the rules r24,...,r35.
If the number of Ci in region 2 is not zero, then Ci is moved to region 6, and the
computation of Π can never halt, otherwise, as in the previous cases, r′ appears in
region one after 6 steps, and the simulation of the next instruction can begin.
⊓⊔
10.4
Conclusions
As we have seen, special variants of generalized communicating P automata are
able to recognize any recursively enumerable language. Interesting questions are
how much extent the size parameters of these P automata variants can be reduced to
obtain the same computational power or, what kind of restrictions can be/should be
introduced on rules of generalized communicating P automata to describe various
language classes. Obviously, the power of generalized communicating P automata
with only rules not discussed in the paper is a topic for future research as well.
Acknowledgements. Research supported in part by the Hungarian Scientiﬁc Research Fund,
“OTKA”, grant no. K75952, and by the European Union through the T ´AMOP-4.2.2.C-
11/1/KONV-2012-0001 project which is co-ﬁnanced by the European Social Fund.
References
1. Csuhaj-Varj´u, E., Oswald, M., Vaszil, Gy.: P automata. In: [8], ch. 6, pp. 144–167 (2010)
2. Csuhaj-Varj´u, E., Vaszil, Gy.: P automata or purely communicating accepting P sys-
tems. In: P˘aun, Gh., Rozenberg, G., Salomaa, A., Zandron, C. (eds.) WMC 2002. LNCS,
vol. 2597, pp. 219–233. Springer, Heidelberg (2003)
3. Csuhaj-Varj´u, E., Vaszil, Gy., Verlan, S.: On Generalized Communicating P Systems
with One Symbol. In: Gheorghe, M., Hinze, T., P˘aun, Gh., Rozenberg, G., Salomaa, A.
(eds.) CMC 2010. LNCS, vol. 6501, pp. 160–174. Springer, Heidelberg (2010)
4. Csuhaj-Varj´u, E., Verlan, S.: On generalized communicating P systems with minimal
interaction rules. Theoretical Computer Science 412, 124–135 (2011)
5. Fischer, P.C.: Turing machines with restricted memory access. Information and Con-
trol 9, 364–379 (1966)
6. Krishna, S.N., Gheorghe, M., Dragomir, C.: Some classes of generalised communicating
P systems and simple kernel P systems. In: Bonizzoni, P., Brattka, V., L¨owe, B. (eds.)
CiE 2013. LNCS, vol. 7921, pp. 284–293. Springer, Heidelberg (2013)

236
E. Csuhaj-Varj´u and G. Vaszil
7. P˘aun, Gh.: Computing with membranes. Journal of Computer and System Sci-
ences 61(1), 108–143 (2000)
8. P˘aun, Gh., Rozenberg, G., Salomaa, A. (eds.): The Oxford Handbook of Membrane
Computing. Oxford University Press (2010)
9. Rozenberg, G., Salomaa, A. (eds.): Handbook of Formal Languages. Springer, Berlin
(1997)
10. Salomaa, A.: Formal Languages. Academic Press, New York (1973)
11. Verlan, S., Bernardini, F., Gheorghe, M., Margenstern, M.: Generalized communicating
P systems. Theoretical Computer Science 404, 170–184 (2008)

Chapter 11
Computational Models Based on Splicing
Yurii Rogozhin and Sergey Verlan
Abstract. In this paper we overview twelve different computational models that
use the splicing operation. We explain the methods used for the organization of the
computational process in this area and give examples for each considered model.
11.1
Introduction
A splicing system is a formal model of the recombinant behavior of DNA molecules
under the inﬂuence of restriction enzymes and ligases. This model introduced by T.
Head in [8] relies on the splicing operation which cuts two strings at speciﬁc points
and then recombines them crosswise. The basic model of an H system consists of
a set splicing rules which are iterated over initial strings (axioms), yielding a for-
mal language. It was shown that in the case of a ﬁnite set of rules and axioms the
generated language is strictly included in the family of regular languages (REG).
Numerous extensions of the basic variant were done in order to extend the compu-
tational power of splicing systems. In most of the cases these models are inspired by
formal models from regulated rewriting and grammar systems area and they permit
a strict increase in the computational power, often till the computational complete-
ness. An important particularity of all these variants is the fact that the splicing is
a binary operation, unlike the rewriting. This permits reacher possibilities for the
organization of the computation, which are different from their classical (rewriting)
counterparts.
Yurii Rogozhin
Institute of Mathematics and Computer Science, Academy of Sciences of Moldova,
Str. Academiei 5, Chis¸in˘au, MD-2028, Moldova
e-mail: rogozhin@math.md
Sergey Verlan
LACL, D´epartement Informatique, Universit´e Paris Est, 61,
av. G´en´eral de Gaulle, 94010 Cr´eteil, France
e-mail: verlan@u-pec.fr
c⃝Springer International Publishing Switzerland 2015
237
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_11

238
Y. Rogozhin and S. Verlan
In this paper we overview several computational models based on splicing. We
consider corresponding deﬁnitions and discuss the particularities of each variant.
For this purpose we present the generation of the language Lab = {anbn,n ≥1}
for every considered model and also give ideas how to reach the computational
completeness.
The paper is organized as follows. Section 11.2 contains the deﬁnitions and some
key results for the basic model of splicing – H systems. It also presents two pos-
sible modiﬁcations of the standard deﬁnition that use the ingredients in a different
manner: iterated splicing systems and multiset splicing systems, both having Tur-
ing machines’ power. Section 11.3 considers the enhancement of H systems with
control mechanisms from regulated rewriting: permitting/forbidding context, order
(priority) between rules, matrix, programmed, graph and time varying control. Sec-
tion 11.4 considers distributed splicing systems, i.e. systems having locations where
strings evolve locally and after that are redistributed across the system. We consider
splicing test tube systems, length-separating test tube systems, splicing P systems
and networks of splicing processors. Section 11.5 presents an original method of
reﬁning the control for a splicing system that is powerful enough to support the
necessary operations. This method is based on the fact that the splicing is a binary
operation, hence in order for a rule to be applied both stings should be present at
the same moment in the system, so varying the appearance of one of these words
the applicability of the rule is also modiﬁed. Finally, Section 11.6 presents some
concluding remarks and open questions.
11.2
Splicing Operation and H Systems
In this paper we consider the P˘aun deﬁnition of splicing operation, which is widely
used in the area. For more details on the other deﬁnitions of splicing as well as for
the biological motivation we refer to [10,22,32].
A splicing rule (over an alphabet V) is a 4-tuple (u1,u2,u3,u4) where u1,u2, u3,
u4 ∈V ∗. It is frequently written as u1#u2$u3#u4, {$,#} ̸∈V, or in two dimensions
as
u1 u2
u3 u4 . Strings u1u2 and u3u4 are called splicing sites.
We say that a word x matches rule r if x contains an occurrence of one of the two
sites of r. We also say that x and y are complementary with respect to a rule r if x
contains one site of r and y contains the other one. In this case we also say that x or
y may enter rule r. When x and y can enter a rule r = u1#u2$u3#u4, i.e., we have x =
x1u1u2x2 and y = y1u3u4y2, it is possible to deﬁne the application of r to couple x,y.
The result of this application is w and z where w = x1u1u4y2 and z = y1u3u2x2. We
also say that x and y are spliced and w and z are the result of this splicing. We write
this as follows: (x,y) ⊢r (w,z) or (x1u1|u2x2,y1u3|u4y2) ⊢r (x1u1u4y2,y1u3u2x2).

11
Computational Models Based on Splicing
239
The pair σ = (V,R) where V is an alphabet and R is a ﬁnite set of splicing rules is
called a splicing scheme or an H scheme. The diameter of a splicing rule x#y$w#z is
deﬁned as (|x|,|y|,|w|,|z|). For a splicing scheme σ = (V,R) we deﬁne the diameter
of σ as (n1,n2,n3,n4), where ni = maxx1#x2$x3#x4∈R xi, 1 ≤i ≤4.
For a splicing scheme σ = (V,R) and for a language L ⊆V ∗we deﬁne:
σ(L) def
= {w,z ∈V ∗| ∃x,y ∈L,∃r ∈R : (x,y) ⊢r (w,z)}.
The splicing operation as deﬁned above is called 2-splicing. If only w is kept
as the result, the corresponding formalism is called 1-splicing. In this paper we
consider 2-splicing operation only and we refer to [30,32] for more details.
Now we can introduce the closure of a language under splicing with respect to σ:
σ0(L) = L,
σi+1(L) = σi(L)∪σ(σi(L)), i ≥0,
σ∗(L) = ∪i≥0σi(L).
We remark that in the above deﬁnition σi(L) is not equivalent to the iteration of
the σ operator. In Section 11.2.1 we consider the iterative variant of σ.
It is known that σ∗preserves the regularity of a language:
Theorem 6. [22] Let L ⊆V ∗be a regular language and let σ = (V,R) be a splicing
scheme. Then language σ∗(L) is regular.
A Head-splicing-system [8,9], or H system, is a construct:
H = (σ,A) = ((V,R),A),
which consists of an alphabet V, a ﬁnite set A ⊆V ∗of initial words over V, the
axioms, and a ﬁnite set R ⊆V ∗×V ∗×V ∗×V ∗of splicing rules. System H is called
ﬁnite if A and R are ﬁnite sets.
The language generated by an H system H is deﬁned as L(H) def
= σ∗(A).
Thus, the language generated by H system H is the set of all words that can be
generated in H starting with A as initial words by iteratively applying splicing rules
to copies of the words already generated. The family of languages generated by H
systems with a ﬁnite set of axioms and rules is denoted by H(FIN,FIN).
Theorem 7. The following relations hold:
(1) For any H system H, L(H) ∈REG.
(2) For any L ∈REG there exists an H system H and an alphabet T such that
L = L(H)∩T ∗.
The precise characterization of the family H(FIN,FIN) is a long-standing open
problem in the area of splicing systems. There are several partial results for the cases
when the ruleset is restricted, e.g. reﬂexive. We refer to [32] for more information.
A recent result from [3] shows a necessary condition for a regular language L to be
a splicing language: L must have a constant in the Sch¨utzenbergers sense. In [11] it
is shown that it is possible to decide if a regular language is a splicing language.

240
Y. Rogozhin and S. Verlan
11.2.1
Iterated Splicing
In this subsection we consider a different deﬁnition of H systems, where iterated
splicing instead of the closure is used. Let σ = (V,R) be a splicing scheme. We
deﬁne ¯σ as the iteration of σ in an ordinary sense:
¯σ∗(L) = ∪i≥0 ¯σi(L), where ( ¯σi(L) = σ(σ(...σ



i
(L)...)).
For an H system H = (σ,A) we denote by ¯L(H) = ¯σ∗(A).
We remark that the operation ¯σ∗is different from σ∗as it keeps only the result-
ing strings of each iteration available for the next iteration. This provides a powerful
feature that permits to eliminate all strings that are not produced by a splicing op-
eration at the corresponding step. It is somehow surprising that this modiﬁcation
sufﬁces by itself to achieve the computational completeness:
Theorem 8. [16, 17] For any RE language L ⊆T ∗there is an H system H, such
that ¯L(H)∩T ∗= L.
Moreover, as it was shown in [1] there exists a universal iterated splicing H sys-
tem having only 17 splicing rules.
The study of the operation ¯σ∗is motivated by the study of time-varying dis-
tributed H (TVDH) systems, see Section 11.3.6, because it is identical to TVDH
systems with one component. In Section 11.5 we consider some more examples on
iterated splicing.
11.2.2
Multisets
Another idea used to increase the computational power of H systems is to use mul-
tisets of strings instead of sets of strings. In this case the application of a splicing
consumes one copy of each input string and produces one copy of the output strings.
We refer to [22] for the corresponding formal deﬁnitions and we remark that in such
systems it is possible to eliminate strings by consuming them. H systems with mul-
tisets are shown to be computationally complete [22].
Example 1. We present now a multiset splicing system generating the language
Lab = {anbn,n ≥1}. Let H = (V,T,A,R), where V = {a,b,c,d,Y}, T = {a,b},
A = {ab,caabbd,caZd} ∪{caY,Ybd}∞and let set R be deﬁned as follows:
1 : c#aa$ca#Zd
2 : cZ#d$Y#bd
3 : bb#d$cZ#bd
4 : c#Zd$ca#Y
5 : c#aa$ε#caZd
6 : bb#d$ccaZd#ε
It is clear that the evolution of H can be done either using the sequence of rules
1−4, repeatedly, or the sequence of rules 5,6, once. This is done by considering the

11
Computational Models Based on Splicing
241
second word that participate in splicing in one copy and by using its product as the
second string at the next step.
11.3
Controlled Splicing
The result from Theorem 6 states that a splicing closure of a ﬁnite set of strings
is regular. Making analogy to the string rewriting it is possible to consider con-
trol mechanisms for splicing like it is done in the area of regulated rewriting. In
this section we will consider counterparts for random-context, ordered, matrix, pro-
grammed, graph-controlled and time-varying grammars. Like in the case of rewrit-
ing these control mechanisms permit to strictly increase the computational power
of corresponding variants. We remark that we give only the main ideas for each
deﬁnition, full details can be consulted in [22].
11.3.1
Main Example
In the following we will consider different variants of systems based on splicing. In
order to better see their particularities we will consider a single example that will be
adapted for each type of system.
We will consider the generation of the language Lab = {anbn,n ≥1}. Consider
following splicing rules that permit to generate it.
1 : c#a$c′a#Z
2 : b#d$Z#bd′
3 : c′#a$c#Z
4 : b#d′$Z#d
5 : c#a$ε#Z′
6 : b#d$Z′#ε
Starting from a word cabd the successive application of rules 1-4 permits to in-
crease at the same time the number of symbols a and b. The application of the last
two rules permits to eliminate leading symbol c and trailing symbol d. While this is
not the only possibility to generate Lab using splicing rules, it is probably the sim-
plest one. We remark that it is important to keep end markers c and d in the string
during the ﬁrst stage (increasing the number of a’s and b’s), otherwise there is no
guarantee that the splicing happens at the ends of the string. At the second stage
it is important prohibit the application of rules 1-4, otherwise the number of a’s or
b’s can be increased in an uncontrolled manner. We also note that strings c′aZ, cZ,
Zbd′, Zd and Z′ should be available for splicing at corresponding time moments.
Now in order to perform a correct generation it should be ensured by the control
of the splicing system that the rules 1-4 are applied (in a loop) one after another, as
well as rules 5 and 6. A special attention should be done to the results of the splicing
of last two rules, as new words cZ′ and Z′d can be produced and these words can be
further eligible for rules 5 and 6.

242
Y. Rogozhin and S. Verlan
11.3.2
Rotate-and-Simulate Technique
The rotate-and-simulate technique is the basic tool for universality proofs in the
area of splicing.
Let S be a formal system based on splicing simulating a formal grammar G. Then,
for any string w ∈(N ∪T)∗of G there are strings Xw′′Bw′Y (X,Y,B /∈N ∪T,w =
w′w′′) in S. These strings are called rotational versions of w. More precisely, the
symbols X and Y ﬂank w and the symbol B marks the beginning (or ending) of the
string. It should be clear that w can be obtained from any of its rotational versions.
The system S simulates the formal grammar G as follows. For each production
u →v of G the axiom ZvY and the splicing rule λ#uY$Z#vY are in S. The produc-
tion of G is simulated as follows. The system S rotates the string Xw1uw2BY into
Xw2Bw1uY and then it applies the corresponding splicing rule. Next, the resulting
string Xw2Bw1vY is rotated into Xw1vw2BY. The rotation is made symbol by sym-
bol, i.e., the string XwaiY, ai ∈N ∪T ∪{B}, is transformed into XaiwY after some
computational steps.
The symbol by symbol rotation is often implemented as follows. The system
starts with the string XwaiY. Later, this string becomes XwYi, i.e., the presence of
ai at the right-hand end side is encoded by Yi (it is also possible to use a unary
encoding 1iY). Next, the strings Xja jwYi, 1 ≤j ≤n, are generated, where n is the
total number of symbols that may occur in the string. Afterwards, the system works
in a cycle where indices i and j are decreased. If j ̸= i, then the strings with these
indices are removed. If i = j, the string X1aiwY1 is obtained. Next, the string XaiwY
is produced. Therefore the initial string XwaiY is rotated by one symbol.
We give below an example of rules that permit to perform the rotation procedure.
1 : ε#aiY$Z#Yi
2 : X#ε$Xja j#Z
3 : ε#Yi$Z#Y ′
i−1
4 : Xi#ε$X′
i−1#Z
5 : ε#Y ′
i $Z#Yi
6 : X′
i #ε$Xi#z
7 : ε#Y ′
0$Z#Y
8 : X′
0#ε$X#Z
The rules performed in the sequence 1 −8 yield a rotation of a symbol as de-
scribed above. The needed axioms correspond to the second word in the splicing
rule.
We also note that in the examples above the computation is performed in a spe-
ciﬁc way. At each step a main word enters a splicing that changes one of its ends.
The second word used to perform the operation is predeﬁned and is part of ax-
ioms. Using rotate-and-simulate method such a strategy is sufﬁcient for obtaining
the computational completeness. We remark that most of the proofs and examples
for splicing-based systems have this property. As exception we can cite proofs using
tag systems or Turing machine simulations that are based on different ideas [14,15].

11
Computational Models Based on Splicing
243
11.3.3
Using Permitting/Forbidding Contexts
In this section we consider the adaptation of the idea of random-context control
conditions to the area of splicing. We also consider ordered splicing systems as they
are extremely close to forbidding conditions. Because splicing is more powerful
than context-free rewriting it is sufﬁcient to consider either permitting or forbidding
conditions only.
We start with splicing systems with permitting context. Such systems consider
rules of form p = (r;C1,C2), where r is a splicing rule and C1,C2 ⊆V (we remark
that in [22]C1 and C2 are ﬁnite languages overV. A rule p is applicable to the strings
w1 and w2 if and only if (1) r is applicable to w1 and w2; (2) w1 contains all letters
from C1 and (3) w2 contains all letters from C2. The language generating by such
systems is deﬁned as usual – the terminal words in the splicing closure of the set of
axioms. The family of extended H systems with permitting contexts is denoted as
EH(FIN, pFIN).
Example 2. Consider H = (V,T,A,R), with V = {a,b,c,c′,d,d′,Z,Z′}, T = {a,b},
A = {cabd,c′aZ,Zbd′,cZ,Zd,Z′} and let R be deﬁned as follows:
1 : (c#a$c′a#Z;{d}, /0)
2 : (b#d$Z#bd′;{c′}, /0)
3 : (c′#a$c#Z;{d′}, /0)
4 : (b#d′$Z#d;{c}, /0)
5 : (c#a$ε#Z′; /0, /0)
6 : (b#d$Z′#ε; /0, /0)
It can be easily seen that L(H) = Lab. The sequencing of rules 1-4 is assured by
corresponding permitting conditions that check a unique symbol at the ends of the
string that is introduced by the previous rule. Once rules 5 and 6 are applied the
above loop breaks and the only possibility to continue is to apply both rules 5 and 6.
We also remark that additional words cZ′ and Z′d do not interfere with the correct
line of the computation.
From the example above it should be clear how to construct longer sequences of
rule applications, so using the rotate-and-simulate method it can be easily shown
that EH(FIN, pFIN) = RE.
The deﬁnition of splicing systems with forbidding context is similar. The rules
are of the form p = (r;C1,C2), where r is a splicing rule and C1,C2 ⊆V (we remark
that in [22]C1 and C2 are ﬁnite languages overV. A rule p is applicable to the strings
w1 and w2 if and only if (1) r is applicable to w1 and w2; (2) w1 does not contain
any letter from C1 and (3) w2 does not contain any letter from C2. The family of
extended H systems with permitting contexts is denoted as EH(FIN, fFIN).
Example 3. We give an H system with forbidding context H = (V,T,A,R)
such that L(H) = Lab. Let V = {a,b,c,c′,c′′,d,d′,d′′,Z,Z′}, T = {a,b}, A =
{cabd,c′aZ,Zbd′,cZ,Zd,c′′Z,Zd′′,Z′} and the set R is deﬁned as follows:

244
Y. Rogozhin and S. Verlan
1 : (c#a$c′a#Z;{d′}, /0)
2 : (b#d$Z#bd′;{c,c′′}, /0)
3 : (c′#a$c#Z;{d,d′′}, /0)
4 : (b#d′$Z#d;{c′}, /0)
5a : (c#a$c′′#Z;{d′}, /0)
6a : (b#d$Z#d′′;{c,c′}, /0)
5b : (c′′#a$ε#Z′;{d}, /0)
6b : (b#d′′$Z′#ε; /0, /0)
As in example above, the sequencing 1-4 is assured by the forbidding conditions
that check that the unique symbol from the previous rule was removed. Due to the
nature of forbidding conditions, the second stage becomes more complicated as
it is impossible to enforce the presence of a symbol. For example, rule 1 will be
applicable to the string cabd as well as to the string cab, yielding an increase in
number of symbols a. To handle this case the end markers are changed to double
primed versions, meaning that the simulation enters in stage 2, and after that are
safely removed, as rules 1-4 are no more applicable.
As for permitting case it is not difﬁcult to show that EH(FIN, fFIN) = RE.
Now we consider ordered H systems that have the control deﬁned in terms of a
partial order relation (>) between rules. A rule r is applicable to w1 and w2 if there
is no other rule r′ > r such that w1 or w2 may enter this rule. The family of extended
ordered H systems is denoted as EH(FIN,ord).
Example 4. We
construct
a
ordered
H
system
H = (V,T,A,R,>)
such
that
L(H) = Lab.
Let
V
= {a,b,c,c′,c′′,d,d′,d′′,Z,Z′},
T
= {a,b},
A = {cabd,c′aZ,Zbd′,cZ,Zd,c′′Z,Zd′′,Z′} and let R be deﬁned as follows:
1 : c#a$c′a#Z
2 : b#d$Z#bd′
3 : c′#a$c#Z
4 : b#d′$Z#d
5a : c#a$c′′#Z
6a : b#d$Z#d′′
5b : c′′#a$c′′#Z
6b : b#d′′$Z#d′′
5c : c′′#a$ε#Z′
6c : b#d′′$Z′#ε
The relation > is deﬁned as follows (we denote by a > b > c two relations a > b
and b > c): 1 > 2 > 3 > 4, 5a > 6a > 5c > 6c, 5b > 2, 6b > 3.
The strategy is to keep the main loop using the order relation and to perform the
second stage as in the forbidding case. The choice to switch to between stages is
validated by dummy rules 5b and 5c that permit to inhibit the action of rules 2 and
3 if one of rules 5a or 6a was applied.
The priority control is similar to forbidding conditions, so we can easily obtain
that EH(FIN,ord) = RE.
11.3.4
Double Splicing
In this section we consider the counterpart of matrix grammars for splicing systems.
However, instead of sequences of prescribed rules, we consider the double splicing

11
Computational Models Based on Splicing
245
operation, which is composed of two splicings and requires that the result of the ﬁrst
splicing to be the input of the second one. More precisely, we deﬁne
(x,y) ⊢r1,r2 (w,z) iff (x,y) ⊢r1 (u,v) and (u,v) ⊢r2 (w,z).
We remark that in most cases (and in particular for universality proofs) it is possi-
ble to take a word y of special form that guarantees that r1 and r2 can be used in this
sequence only, hence the relation to matrix sequences can be immediately observed.
We note that it could still be interesting to investigate matrix splicing systems based
on prescribed sequences of rules, as this permits to simplify some proofs. The family
of double splicing H systems is denoted as EH(FIN,d).
Example 5. Consider the following double splicing H system H = (V,T,A,R) with
V = {a,b,c,d,Z,Z′}, T = {a,b}, A = {cabd,caZbd,Z′} and let R be deﬁned as
follows:
1 : c#a$ca#Z
2 : b#d$Z#bd
3 : c#a$ε#Z′
4 : b#d$Z′#ε
It can be easily seen that L(H) = Lab. We remark that comparing to examples
above in the case of the double splicing four rules sufﬁce, because there is a guar-
antee that both rules from the sequence are applied. We observe that the ﬁrst axiom
determines a consecutive application of rules 1 and 2, while the second axiom de-
termines the sequence 3 and 4.
Since by applying two rules it is possible to encode a chain of applications we
obtain that EH(FIN,d) = RE. We also would like to highlight the result from [2]
giving a construction of a universal double splicing H system having 5 splicing rules
only.
11.3.5
Programmed/Graph Controlled H Systems
In thissection weconsidertheadaptation oftheideaofprogrammed/graph-controlled
grammars for splicing systems. We recall that the idea of the programmed/graphcon-
trol is to associate with each rule r a set of next rules NEXT(r) and choose one of
the rules from this set as the next applicable rule. This property can be reformulated
in terms of a control graph, where nodes are states and edges correspond to rules that
could be applied being in that state. The derivation tracks the current state and after a
rule application it switches to the state that is reached by following the corresponding
edge. We remark that the variants where the initial state is ﬁxed or not are equivalent.
Below we give the deﬁnition in the graph-controlled terms, the programmed version
with the NEXT mapping could be found in [22].
More precisely, for an H system H = (V,T,A,R) consider a control multigraph
G = (N,E) with edges labeled by splicing rules from R and denote by lab(e), e ∈E

246
Y. Rogozhin and S. Verlan
the corresponding rule. Then the language generated by a graph-controlled splicing
H system is deﬁned as follows( let i0 be the initial state):
L(H) = {w ∈T ∗| (x1,y1) ⊢r1 (x2,y′
2),...,(xn,yn) ⊢rn (w,y′
n+1),
such that yi ∈A,r1 = lab(i0,s),s ∈N and
there is a path labeled by r1 ...rn in G}.
We remark that at each step the second word from the splicing is an axiom. It
could be interesting to consider a slightly different variant allowing the second word
to be also produced by a sequence of splicing rules forming a path in G. The family
languages generated by graph-controlled H systems is denoted as EH(FIN,gc).
Example 6. Consider the following graph-controlled splicing H system H =
(V,T,A,R,G,i0) with V = {a,b,c,d,Z,Z′}, A = {cabd,caZ,Zbd,Z′}, i0 = 1, T =
{a,b} and let G be deﬁned as follows:
1
1:c#a$ca#Z

3:c#a$ε#Z′

2
2:b#d$Z#bd

3
b#d$Z′#ε
 4
We remark that the set R can be deduced from G.
It can be easily seen that L(H) = Lab. We observe that like in the case of double
splicing there is no need to prime symbols c and d because the control of the system
is implementing a strict sequencing of rules 1 and 2, or 3 and 4.
Clearly, with graph-controlled splicing systems it is possible to choose an arbi-
trary chain of rules, so we obtain that EH(FIN,gc) = RE.
11.3.6
(E)TVDH
In this section we consider the adaptation of the idea of time-varying grammars to
the area of splicing. We recall that time-varying grammars can be seen as graph-
controlled grammars where the corresponding control graph has the form of a ring.
In the splicing case the deﬁnition is slightly different as one allows using words
that are simultaneously obtained to perform a splicing instead of axioms. More pre-
cisely, for an H system H = (V,T,A,R) consider a partition of R in n (not necessary
disjoint) subsets: R = R1,...,Rn. We call each Ri a component of H and n its degree.
The language generated by H is deﬁned as follows:

11
Computational Models Based on Splicing
247
L0(H) = A
Li+1(H) = σk(Li(H)), where σk = (V,Rk),
i = k
(mod n)
L(H) = ∪i≥0Li(H)∩T ∗
In the literature there are also considered enhanced time-varying distributed H
(ETVDH) systems, which differ from TVDH by the fact that σ∗
k is used at each
iteration instead of σk.
The family of languages generated by (E)TVDH systems of degree n is denoted
as (E)VDHn.
Example 7. Consider the ETVDH system of degree 4 H deﬁned as follows:
H = (V,T,A,R1,R2,R3,R4) with V = {a,b,c,c′,d,d′,Z,Z′}, T = {a,b}, A =
{cabd,c′aZ,cZ,Zd,Zbd′,Zbd,Z′} and the set of rules deﬁned as follows:
R1 ={1.1 : c#a$c′a#Z,
1.2 : c#a$ε#Z′} ∪RA,
R2 ={2.1 : b#d$Z#bd′,
2.2 : b#d$Z′#ε} ∪RA,
R3 ={3.1 : c′#a$c#Z} ∪RA,
R4 ={4.1 : b#d′$Z#d} ∪RA,
RA ={w#ε$w#ε | w ∈A and w ̸= cabd}.
We claim that L(H) = Lab. Clearly, the control of the system allows to perform
the rules 1.1, 1.2, 3.1 and 4.1 in a sequence thus increasing the number of a’s and
b’s simultaneously. Rules 1.2 and 2.2 permit to cut off endmarkers obtaining the
terminal word. We remark that the σ∗operation makes the evolution of this sys-
tem more complicated, e.g. at the second step both words cabd and c′aabd will be
present in L1(H). Moreover, both words will evolve yielding cabd, cabbd′, c′aabd
and c′aabbd′ in L2(H). Now the ﬁrst two words are eliminated as there is no rule
applicable to them in R3, so L3(H) will contain caabd and caabbd′ only. The ﬁrst
word will be eliminated, as R4 does not contain any rule applicable to it, so only
caabbd will be kept for the new cycle of iterations.
We note that in the example above the elimination of undesired strings was rel-
atively easy, because the rule cycle was long. When a smaller cycle is used, the
task becomes more difﬁcult. With ETVDH systems of degree 2 it is possible to
achieve the computational completeness, however another method for string elimi-
nation should be used [27]. More details can be found in Section 11.5. We remark
that ETVDH systems with one component are almost identical to H systems and
cannot produce more than regular languages.
Example 8. Consider the TVDH system of degree 2 H deﬁned as follows:
H = (V,T,A,R1,R2) with V = {a,b,c,c′,d,d′,Z,Z′,Z′′,A,B,C}, T = {a,b},
A = {cabd,c′aZ,Zbd′,cZ,Zd,Z′A,CZ′′,B,Z′′} and the set of rules deﬁned as
follows:

248
Y. Rogozhin and S. Verlan
R1 ={1.1 : c#a$c′a#Z, 1.2 : c#a$ε#Z′′, 1.3 : c′#a$c#Z, 1.4 : Z′#A$B#ε}∪RA,
R2 ={2.1 : b#d$Z#bd′, 2.2 : b#d$Z′#ε, 2.3 : b#d′$Z#d, 2.4 : C#Z′′$ε#B}∪RA,
RA ={w#ε$w#ε | w ∈A\{cabd,Z′′}}.
The simulation is similar to the previous example so L(H) = Lab. However, it
should be noted that using an axiom Z′ and a rule Z′#ε$Z′#ε yields to erroneous
computations as the word Z′d′ that can be obtained can false the check that the
correct sequence of rules is applied. This problem is solved by using the method of
directing molecules, see Section 11.5, by making Z′ and Z′′ to appear only at the
needed steps of the computation. This is done by rules 1.4 and 2.4.
In the case of TVDH systems of degree 1 the set of rules is applied to the result
of the previous application. This corresponds to the iteration of the splicing opera-
tion, see Section 11.2.1. It is somehow surprising that even in this case the computa-
tional completeness can be achieved. We present in Section 11.5 some details on this
result.
11.4
Distributed Splicing
In previous sections we considered models of splicing systems based on H systems.
One of the particularities of these models is that the system starts from a single ini-
tial set of axioms and at each step the current set of words is replaced by a new one,
computed according to the control of the system. In this section we consider splicing
systems based on a different idea. Namely, instead of evolving a single set of words,
a ﬁxed number of such sets (a vector) is evolved. This corresponds to a distributed
system containing some units that we call components. The computation is then di-
vided in two different steps: a computation step and a communication step. During the
computation step splicing rules are applied in each component, independently from
each other according to the underlying control. During the communication step the
contents of components is redistributed in the system according to some algorithm.
We remark that such distributed constructions cannot be expressed in terms of
single set models, because splicing is a binary operation. This means that strings
evolving in different components can interact with each other after the communica-
tion step and, moreover, this interaction cannot be predicted in the general case. In
contrast, the rewriting operation is unary so any distribution of the system makes
no sense as it is equivalent to the union of evolutions of every single axiom and the
path through the components can be decided in advance as the rewriting rule gives
enough information for doing this.
11.4.1
Communicating Distributed H Systems
The model considered in this section is the counterpart of the parallel communi-
cating grammar systems with communication by command. In the literature it is
also known under the name of splicing test tube systems. The idea is to consider

11
Computational Models Based on Splicing
249
a group of H systems, called (splicing) tubes, as components, use σ∗operation for
the computational step in each component and use input permitting ﬁlters for the
communication according to some communication graph. More precisely, a com-
municating distributed H (CDH) system of degree n is a construct
Γ = (V,T,G,(A1,R1,F1),...,(An,Rn,Fn))
where V is an alphabet, T ⊆V is the terminal alphabet, Ai are ﬁnite languages over
V, the axioms, G = ({1,...,n},E) is the communication graph of the system, Ri are
ﬁnite sets of splicing rules and Fi ⊆V ∗are called ﬁlters, 1 ≤i ≤n. We remark that
in the original deﬁnition [4] as well as in [22] ﬁlters Fi are deﬁned in terms of the
Kleene star closure of subsets of V. In the same deﬁnition the communication graph
G is a complete graph with self loops.
The conﬁguration of the system is given by the vector of languages C =
(L1,...,Ln). During the computation step, each component acts like an H system:
(L1,...,Ln) ⊢(L′
1,...,L′
n), iff L′
i = σ∗
i (Li), where σi = (V,Ri)
During the communication step the contents of each component is redistributed
among other components according to the communication graph and the permitting
ﬁlters. More precisely, if a string from component i belongs to Fj and there is an
edge (i, j) in the communication graph, then at the next step this string will belong
to component j. The strings that cannot pass any input ﬁlter of a connected node
remain in the component of their origin. We remark a subtle point here: if a string
can go to some other component(s) then each of the components will receive a
copy of that string and the initial component will not contain any copy of this string,
except the case of a communication graph with a self-loop at the corresponding node
and positive ﬁlter check. Formally, the communication step is deﬁned as follows:
(L1,...,Ln) ⊨(L′
1,...,L′
n), iff L′
i = {Lj ∩Fi | (i, j) ∈E} ∪(Li ∩(V ∗\
n
k=1
Fk))
Now a step in a CDH system is done as follows:
C =⇒C′ iff C ⊢C′′ ⊨C′.
The language generated by a CDH system Γ is
L(Γ ) = {w ∈T ∗| (A1,...,An) =⇒∗(L1,...,Ln) and w ∈L1}.
The family of languages generated by CDH systems of degree n is denoted as
CDHn.
Example 9. Consider
the
following
CDH
system
of
degree
3
Γ
=
(V,T,G,(A1,R1,F1),(A2,R2,F2),(A3,R3,F3))
with
V = {a,b,c,c′,d,d′,Z,Z′},
T = {a,b}, and let G be a complete graph with self loops. The components of the
system are deﬁned as follows.

250
Y. Rogozhin and S. Verlan
A1 ={cabd,c′aZ,Zbd′,c′′Z,Zd′′}
R1 ={1.1:c#a$c′a#Z, 1.2:b#d$Z#bd′, 1.3:c#a$c′′#Z, 1.4:b#d$Z#d′′}
F1 ={a,b,c,d}∗
A2 ={cZ,Zd}
R2 ={2.1 : c′#a$c#Z, 2.2b#d′$Z#d}
F2 ={a,b,c′,d′}∗
A3 ={cZ,Zd}
R3 ={3.1 : c′′#a$ε#Z′, 3.2 : b#d′′$Z′#ε}
F3 ={a,b,c′′,d′′}∗
We claim that L(Γ ) = Lab. After the ﬁrst computational step word c′aabbd′ is com-
municated to the second component. Then after the second computational step word
caabbd is obtained and is communicated to component 1. By continuing this loop
canbnd is obtained. At some point endmarkers can be replaced by c′′ and d′′ and the
corresponding word is communicated to component 3. In that component the end-
markers are removed and the resulting terminal string can go back to component 1,
yielding the result.
We remark that the ﬁlters are expressed in the form of star closure of a subset of
V, so they correspond to the deﬁnition from [22].
It is possible to adapt the construction from the previous example in order to sim-
ulate an arbitrary grammar using the rotate-and-simulate method. So CDH3 = RE.
Moreover, as it is shown in [2] a universal system with 8 rules can be constructed.
In the case of one component the corresponding system is identical to an H system,
so the generated language family is at most regular. In the case of two components
there are several results, depending on the type of ﬁlters used. When a complete
graph with self loops is used (like in the original deﬁnition) the computational com-
pleteness is obtained using (a) regular ﬁlters [6], (b) Kleene star closure of subsets
of V 2 [7], (c) alternation of sets of traditional ﬁlters (Kleene star closure of subsets
of V) [25]. When a complete graph without self-loops is considered, it is possible to
achieve the computational completeness with traditional ﬁlters [29].
11.4.2
Length-Separating Splicing Test Tube Systems
In this section we consider a variant of splicing test tube systems where the commu-
nication mechanism is different and it was never considered in the case of rewriting.
More precisely, context conditions on the edges of the communication graph are
replaced by numerical predicates acting on the string length. Ten types of predicates
are considered: ≤k, < k, ≥k, >, = k, ̸= k, max, ¬max, min, ¬min. For example,
the predicate < k permits to pass strings of length smaller than k, while the predicate
max permits to pass strings of maximal length. In [5] it is shown that 11 components
sufﬁce to generate any RE language.

11
Computational Models Based on Splicing
251
Example 10. Consider a system Γ having the communication graph below.
Let the axioms and rules of the system be deﬁned as follows:
A1 = {cabd,c′aZ,Zbd′},
R1 = {1 : c#a$c′a#Z,2 : b#d$Z#bd′}
A2 = {ccZ,Zdd}
R2 = {3 : c′#a$cc#Z,4 : b#d′$Z#dd}
A3 = {Z}
R3 = {5 : c#c$ε#Z,6 : d#d$Z#ε}
A4 = {Z}
R4 = {7 : cc#a$ε#Z,8 : dd#b$Z#ε}
Ai = /0,Ri = /0,4 ≤i ≤19
It can be seen that L(Γ ) = Lab. Indeed, rules 1 −6 permit to increase simultane-
ously the number of symbols a and b, while the rules 7 and 8 cut off endmarkers.
Until node 3 the right word is the longest one, after that it is the third longest one,
so it is ﬁltered as two times ¬max and one times max. The axioms are returned to
their initial places after 5 steps.
11.4.3
Splicing P Systems
In this section we consider the generalization of the idea of graph-controlled splic-
ing H systems. The idea is to consider n components containing strings and splic-
ing rules having two target indicators – the numbers of components where the
corresponding splicing results shall be moved. The language consists of words over
terminal alphabet collected in some designated node. The obtained model, called
splicing P systems, was introduced in [20] and more details can be found
in [21,23].

252
Y. Rogozhin and S. Verlan
Example 11. Consider
the
following
splicing
P
system
Π
=
(V,T,A1,A2,A3,A4,R1,R2,R3,R4,4).
V
=
{a,b,c,d,Z,Z′},
T
=
{a,b},
A1 = {cabd,caZ,Z′}, A2 = {Zbd}, A3 = {Z′}, A4 = /0
R1 ={1 : c#a$ca#Z;(2,2), 2 : c#a$ε#Z′;(3,3)}
R2 ={3 : b#d$Z#bd;(1,1)}
R3 ={4 : b#d$Z′#ε;(4,4)}
R4 =/0
We observe that the construction is almost identical to the construction from
Example 6, the difference being in the placement of axioms.
It is shown that splicing P systems are computationally complete. Two nodes
sufﬁce for that result [28]. When the intersection with terminal alphabet is not used,
the computational completeness can be obtained with four nodes [24]. In the same
article it is shown that it is also possible to achieve the computational completeness
with both targets having the same number. In this case corresponding rules can be
associated with the edges of the communication graph. Finally, we would like to
mention an interesting result from [2] showing that there exists a universal splic-
ing P system with 5 splicing rules. This result is quite important as it shows that
the splicing operation is more powerful than rewriting and it exhibits a very small
universal device different from the Turing machine.
An interesting feature of splicing P systems is that some variants allow the mod-
iﬁcation of the graph structure (creation and deletion of nodes). In this case dynam-
ical systems with dynamical structure are obtained yielding an extremely complex
behavior.
11.4.4
Networks of Splicing Processors
Another distribution idea comes by the adaptation of the idea of networks of lan-
guage processors to the area of splicing. During the communication step a single
splicing (σ operation) is used. During the communication step the strings are com-
municated to the nodes connected on the communication graph if they pass two
(regular) ﬁlters: the output ﬁlter (OF) of the source node and the input ﬁlter (IF) of
the target node. In some restricted variants regular ﬁlters are replaced by permitting
and forbidding conditions expressed in terms of a Kleene star closure of a subset of
V. We refer to [12,13] for more details.
Example 12. Consider
the
following
network
of
splicing
processors
Γ
=
(V,T,G,(A1,R1,IF1,OF1),(A2,R2,IF2,OF2),(A3,R3,IF3,OF3)),
V
=
{a,b,c,c′,d,d′,Z,Z′}, T = {a,b} and G is a complete graph with three nodes. The
components of Γ are deﬁned as follows.

11
Computational Models Based on Splicing
253
A1 ={cabd,c′aZ,Zbd′,Z′}
R1 ={1.1:c#a$c′a#Z, 1.2:b#d$Z#bd′, 1.3:c#a$ε#Z′, 1.4:b#d$Z′#ε}
IF1 =V ∗
OF1 ={a,b,c′,d′}∗
A2 ={cZ,Zd}
R2 ={2.1 : c′#a$c#Z, 2.2 : b#d′$Z#d}
IF2 =V ∗
OF2 ={a,b,c,d}∗
A3 =/0
R3 =/0
IF3 ={ab}∗
OF3 =/0
It can be easily seen that L(Γ ) = Lab. Indeed, a string cabd in the ﬁrst component
needs to be spliced two times using rules 1.1 and 1.2 (yielding c′aabbd′) in order to
be able to pass the output ﬁlter of the component.In the second component only after
both primes are removed the corresponding string (caabbd) can pass the output ﬁlter
OF2. In order to produce the result, the endmarkers are cut off using rules 1.3 and 1.4
and the resulting terminal strings can pass the input ﬁlter of the output component
3. We remark that the ﬁlters are star languages, so the example corresponds to the
simpliﬁed variant of the model.
It can be easily seen that the model of networks of splicing processors is quite pow-
erful, so the computational completeness can be easily achieved with 2 nodes [12].
11.5
Reﬁning the Control
In this section we consider a different approach that permits to reﬁne the control of a
splicing system. It exploits the idea that splicing is a binary operation, so in order to
be applied both strings participating in a splicing should be present. So, by making
the second string that can match a splicing rule to appear at speciﬁc time moments
it is possible to limit the application of the splicing rule to that speciﬁc moments.
This is possible if the control of the system permits to eliminate (or to move to a
different location in the case of distributed systems) strings from the system.
This method is well suited for computations where there is a clear distinction
between strings representing the information and strings changing the information
by splicing with above strings. More precisely, suppose that during the computa-
tion at each step i the conﬁguration Ci of the system can be split into two parts, Di
representing the data and a ﬁxed conﬁguration M containing the strings that may
be spliced with the data. Suppose also that there are no possible splicings between

254
Y. Rogozhin and S. Verlan
elements of Di, for all i. Then the method of directing molecules works by varying
in time available elements of M. This can be done by marking by a number, the
state, the corresponding strings and increasing this number, using new splicing rules,
modulo k. Therefore, the presence of a string at some step of the computation will
depend on its period k and on its initial state.
The method of directing molecules works well in combination with rotate-and-
simulate technique, because the last one ﬁts in the scheme described above. More
details can be found in [27] and [26].
We illustrate this procedure using TVDH systems of degree 1 (iterated splicing).
Example 13. Consider
TVDH
system
of
degree
1:
H = (V,T,A,R),
where
V
=
{a,b,c,d,Z,Z1,Z(1)
1 ,Z2,Z(1)
2 },
T
=
{a,b,c,d},
A
=
{abb,cZ1,Z1c,dZ(1)
2 ,ZZ(1)
1 ,Z(1)
1 Z,ZZ2} and R is deﬁned as follows:
1 : a#b$c#Z1
2 : a#b$d#Z2
3 : b#b$Z1#c
a.1.1 : c#Z1$Z#Z(1)
1
a.1.2 : c#Z(1)
1 $Z#Z1
a.2.1 : Z1#c$Z(1)
1 #Z
a.2.2 : Z(1)
1 #c$Z1#Z
a.3.1 : d#Z2$Z#Z(1)
2
a.3.2 : d#Z2$Z#Z(1)
2
First consider the evolution of words cZ1 and ZZ(1)
1
that can enter rule a.1.1. By
the deﬁnition of the system they will not be part of L1(H), which will include the
result of their splicing: words cZ(1)
1
and ZZ1. The last two strings can enter rule a.1.2
yielding cZ1 and ZZ(1)
1
in L2(H). It is not difﬁcult to see that these words will be
present in each L2k, k ≥0 (odd step), while words cZ(1)
1
and ZZ1 will be present in
each L2k+1 (even step). A similar mechanism permits to make appear words Z1c and
Z(1)
1 Z at each odd step and words dZ2 and ZZ(1)
2
at each even step. Hence, Z1c, cZ1
and dZ2 are directing molecules with the period equal to two. The ﬁrst two words
have the initial state 0, while the third word has the initial state 1.
So we can abstract our system to a system having only rules 1 −3 and words
Z1c and cZ1 present during odd steps of the computation and word dZ2 present
only during even steps. Under these assumptions it can be easily seen that L(H) =
{abb,cbb,abc,dbc}.
Example 14. In this example we construct a TVDH system H = (V,T,A,R) of de-
gree 1 generating Lab. This system is deﬁned as follows:
V = {a,b,c,c′,c′′,d,d′,d′′,Z,X,Y,K,L,M,N}∪{Zi,Z′
i | 1 ≤i ≤6}, T = {a,b},
A = {cabd,c′aZ1,cZ2,c′′Z3,Z′
4bd′,Z′
5d,Z′
6d′′}∪{ZZ′
1,ZZ′
2,ZZ′
3,Z4Z,Z5Z,Z6Z}∪
{Z2X1,1,Z3R1,ZX1,2,Z1R1}∪{Z1X2,1,Z2R2,Z3X2,2,ZR2}
The set of rules R is given below (we assume that 1 ≤p,q ≤2):

11
Computational Models Based on Splicing
255
1.1 : c#a$c′a#Z1
t.1.1.1 : c′a#Z1$Z#Z′
1
t.1.1.2 : c′a#Z′
1$Z#Z1
1.2 : c′#a$c#Z2
t.1.2.1 : c#Z2$Z#Z′
2
t.1.2.2 : c#Z′
2$Z#Z2
1.3 : c#a$c′′#Z3
t.1.3.1 : c′′#Z3$Z#Z′
3
t.1.3.2 : c#Z′
3$Z#Z3
2.1 : b#d$Z4#bd′
t.2.1.1 : Z4#bd′$Z′
4#Z
t.2.1.2 : Z′
4#bd′$Z4#Z
2.2 : b#d′$Z5#d
t.2.2.1 : Z5#d$Z′
5#Z
t.2.2.2 : Z′
5#d$Z5#Z
2.3 : b#d$Z6#d′′
t.2.3.1 : Z6#d′′$Z′
6#Z
t.2.3.2 : Z′
6#d′′$Z6#Z
1.4 : c′′#a$ε#ZX1,1
2.4 : b#d′′$ZX2,1#ε
r.1 : Z#Xp,q$Z1#Rp
r.2 : Z1#Xp,q$Z2#Rp
p,q = {1,2}
r.3 : Z2#Xp,q$Z3#Rp
r.4 : Z3#Xp,q$Z#Rp
p,q = {1,2}
We will explain each group of rules in more details. Rules having the number
1.i, (resp. 2.i) 1 ≤i ≤3 are applicable during odd (resp. even) steps only. This
becomes possible as the construction uses the method of directing molecules and
corresponding words appear with a period equal to two. Rules t.k.i.j, 1 ≤k, j ≤2,
1 ≤i ≤3 permit to implement this behavior. Rules 1.4 and 2.4 are applicable each
four steps as corresponding directing words ZXp,q, 1 ≤p,q ≤2 appear with a period
equal to four. This behavior is implemented using rules r.1 −r.4.
Using a similar idea it is possible to show that for any RE language L there exists
a TVDH system H of degree 1 (or an iterated H system) such that L(H) = L. We
also remark that the construction above has the property that Li ∩Li+1 = /0.
We note that the control using directing words was possible because in TVDH
systems it is possible to eliminate strings. In the case of splicing systems that do not
permit string elimination (like splicing test tube systems) a sink node can be used
to gather strings that are not used anymore. Another possibility is to redistribute
strings to the nodes where they cannot evolve, e.g. in the case of CDH3 systems [19]
or to make “eliminated” strings to move from a tube to another without being able to
enter a splicing rule by using the method of directing molecules, e.g. CDH2 systems
without self-loops [29].
11.6
Conclusions
In this paper we made an overview of twelve computational models based on splic-
ing. While using ideas from the area of formal language theory, the functioning of
these models is very different from their rewriting counterpart. This is due to the
properties of the splicing operation which has a contextual dependence as well as an
exchange of information not speciﬁed in advance. All these points make the study
of such systems fascinating and yield to the discovery of new ideas how to perform
and control the computation.
We remark that we did not present all existing models based on splicing. We only
mention some omissions like H systems with target languages, locally evolving H
systems, splicing grammar systems and two level distributed H systems that can be
consulted in more details in [22]. We also remark that in some works, e.g. in [31],
the splicing is considered as a unary operation. This is achieved by associating the

256
Y. Rogozhin and S. Verlan
second participating word to the deﬁnition of the splicing rule. In this case corre-
sponding models are simpler as there are less evolution possibilities. We remark that
we did not present the models from the historical perspective. More bibliographical
notes on this topic can be found in [18,22,26,32].
We would like to mention that in this paper we did not discuss descriptional pa-
rameters associated to the corresponding systems. We cite the most used parameters
in this area: the diameter of splicing rules, the number of splicing rules and axioms,
the size of the alphabet, the number of distributed components, the complexity of
ﬁlters and random conditions. It can be often observed a trade-off between all these
measures – for example the set of axioms can be in many cases reduced to a single-
ton, the trade-off being the increase of the size of the alphabet and of the number of
splicing rules [22]. Conversely, it is possible to bound the alphabet to two symbols,
the number of rules to 5 and the diameter to (9,6,6,3) paying a huge increase in the
number of axioms [2]. Many papers in the area of splicing exhibit such trade-offs,
but still a systematical study is missing.
References
1. Alhazov, A., Kogler, M., Margenstern, M., Rogozhin, Y., Verlan, S.: Small universal
TVDH and test tube systems. International Journal of Foundations of Computer Sci-
ence 22(1), 143–154 (2011)
2. Alhazov, A., Rogozhin, Y., Verlan, S.: On small universal splicing systems. International
Journal of Foundations of Computer Science 23(07), 1423–1438 (2012)
3. Bonizzoni, P., Jonoska, N.: Regular splicing languages must have a constant. In: Mauri,
G., Leporati, A. (eds.) DLT 2011. LNCS, vol. 6795, pp. 82–92. Springer, Heidelberg
(2011)
4. Csuhaj-Varj´u, E., Kari, L., P˘aun, G.: Test tube distributed systems based on splicing.
Computers and AI 15(2-3), 211–232 (1996)
5. Csuhaj-Varj´u, E., Verlan, S.: On length-separating test tube systems. Natural Comput-
ing 7(2), 167–181 (2008)
6. Freund, R., Freund, F.: Test tube systems: When two tubes are enough. In: Rozenberg,
G., Thomas, W. (eds.) Developments in Language Theory, pp. 338–350. World Scientiﬁc
(1999)
7. Frisco, P., Zandron, C.: On variants of communicating distributed H systems. Funda-
menta Informaticae 48(1), 9–20 (2001)
8. Head, T.: Formal language theory and DNA: an analysis of the generative capacity of spe-
ciﬁc recombinant behaviors. Bulletin of Mathematical Biology 49(6), 737–759 (1987)
9. Head, T.: Splicing languages generated with one sided context. In: Computing with Bio-
Molecules. Theory and Experiments, pp. 158–181 (1998)
10. Kari, L.: DNA computing: Arrival of biological mathematics. The Mathematical Intelli-
gencer 19(2), 9–22 (1997); Earlier version under the title DNA computers, tomorrow’s
reality. Bulletin of the European Association for Theoretical Computer Science (59),
256–266 (1996), http://www.csd.uwo.ca/lila/amsn.ps
11. Kari, L., Kopecki, S.: Deciding whether a regular language is generated by a splic-
ing system. In: Stefanovic, D., Turberﬁeld, A. (eds.) DNA 2012. LNCS, vol. 7433,
pp. 98–109. Springer, Heidelberg (2012)
12. Loos, R., Manea, F., Mitrana, V.: On small, reduced, and fast universal accepting net-
works of splicing processors. Theoretical Computer Science 410(4-5), 406–416 (2009)

11
Computational Models Based on Splicing
257
13. Manea, F., Mart´ın-Vide, C., Mitrana, V.: Accepting networks of splicing processors:
Complexity results. Theoretical Computer Science 371(1-2), 72–82 (2007)
14. Margenstern, M., Rogozhin, Y.: Time-varying distributed H systems of degree 2 generate
all RE languages. In: MFCS 1998 Workshop on Frontiers of Universality (1998)
15. Margenstern, M., Rogozhin, Y.: A universal time-varying distributed H system of degree
2. Biosystems 52, 73–80 (1999)
16. Margenstern, M., Rogozhin, Y.: Time-varying distributed H systems of degree 1 gen-
erate all recursively enumerable languages. In: Ito, M., P˘aun, G., Yu, S. (eds.) Words,
Semigroups, and Transductions, pp. 329–339. World Scientiﬁc (2001)
17. Margenstern, M., Rogozhin, Y., Verlan, S.: Time-varying distributed H systems with
parallel computations: the problem is solved. In: Chen, J., Reif, J.H. (eds.) DNA 2003.
LNCS, vol. 2943, pp. 48–53. Springer, Heidelberg (2004)
18. Margenstern, M., Rogozhin, Y., Verlan, S.: Time-varying distributed H systems: An
overview. Fundamenta Informaticae 64(1-4), 291–306 (2005)
19. Priese, L., Rogozhin, Y., Margenstern, M.: Finite H-systems with 3 test tubes are not
predictable. In: Altman, R., Dunker, A., Hunter, L., Klein, T. (eds.) Proceedings of Paciﬁc
Symposium on Biocomputing, 3, Kapalua, Maui, pp. 547–558. World Scientiﬁc, Hawaii
(1998)
20. P˘aun, G.: Computing with membranes. Journal of Computer and System Sciences 1(61),
108–143 (2000); Also TUCS Report No. 208 (1998)
21. P˘aun, G.: Membrane Computing. An Introduction. Springer(2002)
22. P˘aun, G., Rozenberg, G., Salomaa, A.: DNA Computing: New Computing Paradigms.
Springer (1998)
23. P˘aun, G., Rozenberg, G., Salomaa, A.: The Oxford Handbook Of Membrane Computing.
Oxford University Press (2009)
24. Verlan, S.: About splicing P systems with immediate communication and non-extended
splicing P systems. In: Mart´ın-Vide, C., Mauri, G., P˘aun, G., Rozenberg, G., Salomaa,
A. (eds.) WMC 2003. LNCS, vol. 2933, pp. 369–382. Springer, Heidelberg (2004)
25. Verlan, S.: Communicating distributed H systems with alternating ﬁlters. In: Jonoska,
N., P˘aun, G., Rozenberg, G. (eds.) Aspects of Molecular Computing. LNCS, vol. 2950,
pp. 367–384. Springer, Heidelberg (2003)
26. Verlan, S.: Head systems and applications to bioinformatics. Ph.D. thesis, University of
Metz (2004)
27. Verlan, S.: A boundary result on enhanced time-varying distributed H systems with par-
allel computations. Theoretical Computer Science 344(2-3), 226–242 (2005)
28. Verlan, S., Margenstern, M.: About splicing P systems with one membrane. Fundamenta
Informaticae 65(3), 279–290 (2005)
29. Verlan, S., Margenstern, M.: Universality of splicing test tube systems with two tubes.
Fundam. Inform. 110(1-4), 329–342 (2011)
30. Verlan, S., Zizza, R.: 1-splicing vs. 2-splicing: Separating results. In: Harju, T.,
Karhum¨aki, J. (eds.) Proceedings of WORDS 2003, 4th International Conference on
Combinatorics on Words, Turku, Finland, September 10-13, pp. 320–331. TUCS Gen-
eral Publication No. 27 (2003)
31. Zandron, C.: A model for molecular computing: Membrane systems. Ph.D. thesis, Di-
partimento di Scienze dell’Informazione, Universita’ degli Studi di Milano, Milano, Italy
(2002)
32. Zizza, R.: Splicing systems. Scholarpedia 5(7), 9397 (2010)

Chapter 12
Linear Cellular Automata and Decidability
Klaus Sutner
Abstract. We delineate the boundary between decidability and undecidability in the
context of one-dimensional cellular automata. The key tool for decidability results
are automata-theoretic methods, and in particular decision algorithms for automatic
structures, that are inherently limited to dealing with a bounded number of steps in
the evolution of a conﬁguration. Undecidability and hardness, on the other hand, are
closely related to the full orbit problem: does a given conﬁguration appear in the
orbit of another?
12.1
Linear Cellular Automata
Cellular automata arise as a natural discretization of continuous dynamical systems.
In the continuous case, we are dealing with conﬁgurations of the form, say, X :
IR →Σ , where Σ is some set of values whose precise nature is not important here.
Write C for the space of all conﬁgurations. The shift operation σa(x) = x + a acts
on C in the natural way and accounts for a change in coordinates. The dynamics
of the system are then given by a family of evolution operators τt : C →C , where
“time” t is a non-negative real. These operators form a monoid under composition:
τt ◦τs = τt+s and τ0 = Id. By discretizing the underlying space, as well as the
operators σ and τ, we naturally arrive at maps G : ΣZ →ΣZ that are continuous
in the standard product topology and invariant under the discrete shift operator σ :
ΣZ →ΣZ , σ(X)(i) = X(i + 1). This is expressed in the classical Curtis-Hedlund-
Lyndon theorem, see [18]. Hence, in a cellular automaton the global map G = Gρ
can be represented by a ﬁnite lookup table, essentially a map ρ : Σw →Σ , and is
thus amenable to analysis from the perspective of decidability and computational
complexity. It is customary to refer to this ﬁnite function as the local map or rule of
Klaus Sutner
Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh PA 15213, USA
e-mail: sutner@cs.cmu.edu
c⃝Springer International Publishing Switzerland 2015
259
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_12

260
K. Sutner
the cellular automaton. The operator monoid is generated by G and we are therefore
interested in the iterates Gt for all t ∈N.
One of the main areas of interest in the study of cellular automata is their clas-
siﬁcation with respect to the evolution of conﬁgurations, i.e., the repeated applica-
tion of the global map. This problem was expressed by Wolfram in [53] as “What
overall classiﬁcation of cellular automata behavior can be given?” While aspects
of short-term evolution such as reversibility are not excluded, it is clear from the
classiﬁcation proposed by Wolfram that he had the long-term evolution in mind,
see [52,54]. In a nutshell, the Wolfram classiﬁcation looks like so. The evolution of
a conﬁguration leads to
• Class I: homogeneous ﬁxed points,
• Class II: periodic conﬁgurations,
• Class III: chaotic, aperiodic patterns,
• Class IV: persistent, complex patterns of localized structures.
In [27, 28] Li and Packard introduce an alternative version of this hierarchy.
Again, their classiﬁcation is based on the asymptotic behavior of the automaton.
Another well-known attempt at classiﬁcation is due to Kurka, see [25, 26, 31,
47], and is based on the topological notions of equicontinuity, sensitivity to initial
conditions and expansivity. Needless to say, these notions all relate to long-term
behavior.
In [30], Margenstern gave a comprehensive description of the “frontier between
decidability and undecidability,” in a variety of computational settings. In this pa-
per we will focus on one-dimensional cellular automata and discuss an approach
to classiﬁcation that is based on model theory and computability rather than clas-
sical dynamics. Our key tool is the use of automata theory to obtain decidability
results for the ﬁrst-order theory of cellular automata, construed as ﬁrst-order struc-
tures. Using ﬁrst-order logic one can easily formalize assertions such as “the global
map is surjective,” “the system is reversible”, “the global map is k-to-1” where k is
ﬁxed, “there exists a 5-cycle” or “there are exactly 2 ﬁxed points.” Thus, we will
deal exclusively with one-dimensional cellular automata; their higher-dimensional
analogues are not amenable to these techniques as can be seen for example from
Kari’s theorem concerning the undecidability of injectivity and surjectivity of the
global map in dimension 2, see [20]. As Douglas Lind pointed out, the study of
higher-dimensional cellular automata requires one to step into “the Swamp of Un-
decidability,” see [29]. In fact, even the most basic question of whether a shift of
ﬁnite type, given by a ﬁnite collection of forbidden 2-dimensional patterns, is non-
trivial already turns out to be undecidable.
More formally, suppose a one-dimensional cellular automaton has local map ρ :
Σw →Σ . We consider the associated ﬁrst-order relational structure
Cρ = ⟨C,⟩
where Σ is a ﬁnite set, the alphabet of the cellular automaton, and w the width, C
denotes the space of conﬁgurations. We always assume that our language includes

12
Linear CA and Decidability
261
equality without further mention. Given a ﬁrst-order sentence ϕ, we want to de-
termine whether ϕ is valid over Cρ, in symbols Cρ |= ϕ. In computer science this
problem is often referred to as model checking , see [9, 19]. There are two natural
variants that are both relevant in the context of cellular automata: ﬁrst, in expression
model checking the structure is ﬁxed and we are interested in establishing the valid-
ity of a number of assertions. For example, we may be interested in understanding
properties of particularly interesting speciﬁc elementary cellular automata such as
rule 30 or rule 110, [54]. Second, in data model checking the sentence is ﬁxed and
one tries to determine validity in as many structures as possible. For example, one
might try to examine the existence of a particular ﬁnite subgraph of Cρ for all ele-
mentary cellular automata ρ. In our case, the machinery for both problems is quite
similar, though in particular for data model checking it is important to optimize the
decision procedures in order to avoid efﬁciency problems.
The reasons for representing the global map as a binary relation rather than a
function are purely technical and need not concern us here. To avoid trivial cases,
we assume that Σ has cardinality at least 2, so conﬁguration spaces are inﬁnite and
indeed uncountable, but see section 12.2 below for a natural reduction to countable
subspaces. For our purposes, there are three natural choices of the conﬁguration
space C: the bi-inﬁnite case Σζ , the one-way inﬁnite case Σω and the ﬁnite case Σn.
In the last case, n ∈N is a parameter and we are mostly concerned with the spectrum
of a ﬁrst-order sentence, see section 12.2.1 below. Correspondingly, we write Cn
ρ for
the ﬁnite structure ⟨Σn,⟩, Cω
ρ for the one-way inﬁnite structure ⟨Σω,⟩and Cζ
ρ
for the two-way inﬁnite structure ⟨Σζ ,⟩, following the terminology established
in [35, 40] for the corresponding automata. In all cases, we think of conﬁgurations
as words over a ﬁnite alphabet Σ and use appropriate ﬁnite state machines to ob-
tain decision algorithms: ordinary word automata in the ﬁnite case, B¨uchi automata
in the inﬁnite case and ζ-automata in the bi-inﬁnite case. Note that special care
is needed to deal with boundary conditions both in the ﬁnite and one-way inﬁnite
case. We point out that the notion of ﬁnite conﬁguration here refers exclusively
to conﬁgurations that are ﬁnite in the sense that they involve a ﬁnite grid of cells.
Unfortunately, it is customary in the literature to use the same terminology with
respect to inﬁnite conﬁgurations that have ﬁnite support: only ﬁnitely many cells
are in a state different from some specially designate null state. We will avoid this
usage here. At any rate, Cρ is an automatic structure in the sense of Khoussainov
and Nerode [22, 23] and generalizations in [5]. Historically, automaticity was ﬁrst
exploited in work by Gilman, Cannon, Holt, Thurston and others in study of vari-
ous types of groups, see [12]. More recently, there has been substantial progress in
elucidating the structure of automorphism groups of the inﬁnite binary tree that are
described by certain types of Mealy automata, see [4,15,16,34,50]. As we will see
in section 12.2, automaticity can be exploited in all three cases to produce decision
algorithms for the ﬁrst-order theory of these structures. Alas, the efﬁciency of the
algorithms varies considerably; in particular in the bi-inﬁnite case the corresponding
ζ-automata are algorithmically difﬁcult to handle and it is hard to push the decision
methods beyond very basic properties. One interesting technique in this connection

262
K. Sutner
is to enlarge the language by adjoining suitable automatic predicates. For example,
the predicate “differ in only ﬁnitely many places” is useful to express surjectivity of
the global map in the bi-inﬁnite case and easily seen to be automatic.
Needless to say, any full classiﬁcation of cellular automata will require stronger
logics such as monadic second-order logic or transitive closure logic. Alternatively,
we can consider augmented structures
Tρ = ⟨C,, ∗→⟩.
where ∗→denotes the transitive reﬂexive closure of , the reachability or orbit rela-
tion of the automaton. Whenever necessary, we use qualiﬁers n , ω and ζ to indicate
the type of cellular automaton under consideration. Pace Lind, this leads back into
the undecidability swamp: the ﬁrst-order theory of Tρ is not decidable in general,
though there are interesting automatic relations where the augmented structure sur-
prisingly remains automatic and can thus be handled in very much the same way as
the plain structure, see [50].
In the following section 12.2 we will describe the machinery required to produce
decision algorithms for the ﬁrst-order theory of all three kinds of cellular automata.
As we will see, there are signiﬁcant efﬁciency obstacles to overcome, in particular
in the bi-inﬁnite case. Section 12.3 then summarizes corresponding undecidabil-
ity results in connection with the long-term evolution of conﬁgurations. Lastly, we
comment on open problems and future work.
12.2
The First-Order Theory
In this section we show how to solve a very limited version of the Entscheidungsprob-
lem for one-dimensional cellular automata: the ﬁrst-order theory ofall theseautomata
is decidable. Of course, the ﬁrst-order theory of Cρ is too weak to deal with aspects
of the long-term behavior of the cellular automaton, but it easily captures elemen-
tary properties such as injectivity, surjectivity, k-to-1-ness, the existence of k-cycles
and so on. We can think of these properties as being local in the temporal sense, but
note that we can obviously construct a ﬁrst-order sentence that asserts, say, that every
conﬁguration has distance at most k from an ℓ-cycle. Or we can express the assertion
that a particular ﬁnite directed graph has an isomorphic copy somewhere in Cρ. As
we will see, all these temporally local properties are decidable, at least in principle.
12.2.1
Finite Conﬁgurations
In the ﬁnite case we are dealing with structures Cn
ρ which are clearly automatic,
uniformly in n, meaning that the same automata can be used for all these structures.

12
Linear CA and Decidability
263
As a sample decision problem, consider the standard question of testing reversibility
of the system, i.e., injectivity of the global map. The corresponding problem for bi-
inﬁnite cellular automata was handled by Amoroso and Patt [2] and appears to be
the ﬁrst clear example of a decidability result in the context of cellular automata,
following the paradigm established by Rabin and Scott [36]. To see how to tackle
injectivity in our setting, consider ﬁrst an automaton Aρ(x,y) that tests whether two
ﬁnite words X,Y ∈Σn are related by X  Y. The automaton works on words over
the alphabet Σ2 which we may consider as having two tracks:
X:Y =
x1 x2 ... xi ... xn−1 xn
y1 y2 ... yi ... yn−1 yn ∈Σ2
This two-track word X:Y ∈(Σ2)n is often referred to as the convolution of X and Y,
unfortunate but well-established terminology. For simplicity assume that the width
w = 2r+1 of the local map is odd. To test whether word y is obtained from x by uni-
form application of the local map on all ﬁnite blocks of the form xi−r,...,xi,...,xi+r,
the appropriate automaton Aρ(x,y) has state set Σ2r × Σr and the transitions are of
the form
a1,...,a2r
b1,...,br
a:b
−→
a2,...,a2r,a
b2,...,br,b
provided that ρ(a1,...,a2r,a) = b1. Here a:b indicates the 2-track letter composed
of a,b ∈Σ and labels the transition. Thus, we are dealing with a subautomaton of
the complete de Bruijn automaton over Σ2 of order 2r: we remove all the directed
edges from the full de Bruijn automaton that do not conform to ρ. An example is
shown in ﬁgure 12.1. Note that the underlying CA is the additive rule 150, as a
consequence the automaton uses the full de Bruijn graph.
A further complication is caused by the boundary conditions associated with a
ﬁnite cellular automaton. In the case of ﬁxed boundary conditions, we can augment
the de Bruijn automaton with additional initial and ﬁnal states that represent the
phantom cells. More precisely, the initial states have indegree 0 and transitions lead-
ing to the main automaton; the ﬁnal states have outdegree 0 and are reachable via
transitions leaving the main automaton. To deal with periodic boundary conditions,
these additional states have to be associated appropriately.
Injectivity is now easily expressed in our relational setting as the ﬁrst-order
formula
ϕ ≡∀x,y,z(x  z∧y  z ⇒x = y)
To convert the sentence ϕ into an automaton Aϕ we use the 3-track alphabetΓ = Σ3.
We combine two copies of the -testing automaton, Aρ(x,z) and Aρ(y,z), associ-
ated with tracks in the obvious manner. A standard product construction is used to
express logical conjunction of x  z and y  z, refer to the result as A′. Lastly, let
A be the conjunction of A′ and an automaton testing inequality x ̸= y. This last
step amounts to creating two copies of A′, with appropriate cross transitions that
verify inequality. To check injectivity we need to test the acceptance language of
A for emptiness. Clearly, this last step can be handled in time linear in the size of

264
K. Sutner
00:0
00:1
01:0
11:1
11:0
00:1
01:1
10:0
0:0
0:1
1:0
1:1
0:1
1:1
1:1
1:0
0:1
0:0
1:0
0:0
0:1
1:0
1:1
0:0
Fig. 12.1 An example of an automaton Aρ; the underlying cellular automaton here is the
additive elementary automaton number 150
A, which size is quadratic in the size of the lookup table for the local map of the
cellular automaton and we have a perfectly practical algorithm.
In the general case, the algorithm rests on the fact that regular languages form an
effective Boolean algebra. Suppose we are given a ﬁrst-order formula ϕ(x1,...,xk)
with k free variables as indicated. The decision algorithm constructs an automaton
Aϕ that accepts exactly those k-track words that satisfy the formula:
L(Aϕ) = {u1:u2:...:uk ∈Γ ⋆}Cρ |= ϕ(u1,u2,...,uk)
where Γ = Σk is the k-track version of Σ. The construction proceeds by induction
on the subformulae of ϕ. For simplicity, assume that the given formula is in preﬁx
normal form, but note that this may not be the most desirable setup for efﬁciency
reasons.
We can handle the Boolean connectives in the matrix of the formula using stan-
dard algorithms such as product machines and determinization, see [40] for a recent
description of the requisite machinery. Note that determinization here refers to the
classical Rabin-Scott algorithm [36] used for word automata; in the following sec-
tions signiﬁcantly more complicated procedures are required.
Universal quantiﬁers can be translated into existential ones via the standard trans-
formation ∀xϕ ≡¬∃x¬ϕ. Somewhat surprisingly, existential quantiﬁers are actu-
ally easier to handle than logical connectives, at least from a purely algorithmic
perspective: we can simply erase the corresponding track from the transition labels.
Note, however, that this transformation usually introduces nondeterminism, which
can cause problems at later stages if determinization is required to deal with logical

12
Linear CA and Decidability
265
negation. Unfortunately, this typically happens when dealing with universal quanti-
ﬁers in the manner described above.
A ﬁrst-order sentence has no free variables. To handle this case properly it is con-
venient to adopt the convention that (Σ0)⋆is the 2-element Boolean algebra {⊤,⊥}.
Projection yields ⊥if the set is empty and ⊤otherwise. To determine which case ob-
tains one has to test the corresponding regular language for emptiness, which can be
handled by standard path-existence algorithms in the unlabeled directed graph that
results after all quantiﬁers have been removed. At any rate, we have the following
result, see also [48].
Theorem 1. First-order logic for ﬁnite one-dimensional cellular automata is
decidable.
It should be noted that the use of product automata and determinization has the
potential effect of exponential blow-up in the size of the ﬁnite state machines in-
volved in the decision algorithm. Hence, as a practical matter, only relatively simple
formulae can be handled.
While the automaton Aϕ can be used to determine validity for speciﬁc values of
n, it is usually more interesting to exploit it to compute the spectrum of ϕ, deﬁned
as the tally language
spec(ϕ) = {0n}Cn
ρ |= ϕ ⊆0⋆
As a tally language, the spectrum can be determined by a ﬁnite state machine and
hence must be regular.
Lemma 1. Any sentence in ﬁrst-order logic has regular spectrum over the structures
Cn
ρ.
Alternatively we can think of the spectrum as a set of natural numbers, in which
case the set must be semi-linear (i.e., a ﬁnite union of linear sets). As an exam-
ple consider the elementary cellular automaton number 90, an additive automaton
whose local rule corresponds to the exclusive-or of the left and right argument.
The property “every conﬁguration has exactly 4 predecessors” here has spectrum
2N. Likewise, elementary cellular automaton number 30 has spectrum 12N for the
property “there is a 3-cycle.” See Wolfram [54] and references therein for more
background on elementary cellular automata. One particularly important case arises
when the spectrum of a sentence is universal: all ﬁnite grids have the property in
question. We can test universality of a ﬁrst-order sentence ϕ by testing universality
of the associated automaton, at least disregarding the fact that this test is PSPACE-
hard in general [14]. In general, efﬁciency is a non-negligible issue in the ﬁnite
case, but with some effort one can obtain feasible decision algorithms for interest-
ing properties.

266
K. Sutner
12.2.2
Inﬁnite Conﬁgurations
To lift our decision algorithm to inﬁnite conﬁgurations of the form X ∈Σω we need
to generalize ordinary word automata to machines operating on inﬁnite inputs. More
precisely, we can retain the ﬁnite transition systems that describe word automata, but
we have to work with a signiﬁcantly more complicated acceptance condition. To this
end,aB¨uchiautomaton B= ⟨Q,Σ,τ;I,F ⟩issaid to acceptaword X ∈Σω ifthereisa
computationB on X thatstartsatastateinI and touchesF inﬁnitely often,see[35,40].
In other words, for a computation π, let Inf(π) = { p ∈Q |
∞
∃i ∈N(pi = p)} denote
the set of recurrent states in π. Then B accepts X if there exists some computation
π on X, starting at I, such that Inf(π)∩F ̸= /0. Of course, as a ﬁnite data structure a
B¨uchi automaton is indistinguishable from an ordinary nondeterministic automaton.
The addition of automatic predicates to our language makes it possible to de-
scribe other properties of interest. For example, consider the left-shift operator L
and suppose we adjoin a predicate xLy, also denoted by L, that is interpreted as
“x is the left-shift of y.” Using the standard sloppy notation to express chains of
relations, we can then write in the extended language L(,L)
∃x,y,z,u,v(x  y  z  u ∧uLvLx)
to formalize the assertion that, for some conﬁguration X, we have G3(X) = L2(X).
Thus we can state that 3 steps in the temporal evolution correspond to a double left-
shift. Similarly we could restrict our attention to spacially periodic conﬁgurations
of a ﬁxed period. Note that chains such as x  y  z  u in the preceding formula
require an adjustment in the basic de Bruijn automaton; except for u both tracks of
a state will now contain 2r symbols.
Symbolic dynamics is concerned with the Cantor space Σω, but our use of ﬁnite
state machines to determine ﬁrst-order properties has the side effect that we are
essentially dealing with a smaller space. Deﬁne a conﬁguration to be ultimately
periodic if it is of the form vwω where v and w are ﬁnite words, w not empty. Thus,
after a ﬁnite initial segment v the conﬁguration simply repeats the block w forever.
We writeCup of this space of conﬁgurations. From the perspective of computability,
ultimately periodic conﬁgurations may seem ad hoc, but they have a perfectly good
justiﬁcation in terms of model theory:
Theorem 2. The space Cup of ultimately periodic conﬁgurations is an elementary
substructure of the full space.
This is easy to see using the standard Tarski-Vaught test, see [8, 49]. Thus, ul-
timately periodic conﬁgurations are ﬁnitary objects but are indistinguishable from
arbitrary conﬁgurations from the perspective of ﬁrst-order logic. In fact, they form
the least class of conﬁgurations that contain conﬁgurations of ﬁnite support and
form an elementary subspace. Another useful property of this subspace is that its
elements afford a natural ﬁnite description as pairs of ﬁnite words (v,w). Hence the
orbits on Cup are recursively enumerable, or r.e. for short, see [41]. As in the ﬁnite

12
Linear CA and Decidability
267
case, the collection of ω-regular languages over some alphabet forms an effective
Boolean algebra and we can lift the construction from the ﬁnite to the inﬁnite case.
Theorem 3. First-order logic for inﬁnite one-dimensional cellular automata is
decidable.
It was pointed out by O. Finkel that we can extend our language slightly by
quantiﬁers for “there exist inﬁnitely many” and “there exist r mod k many” with-
out affecting decidability, see [13]. As a consequence, we can check, say, whether
there are inﬁnitely many ﬁxed points. Alas, efﬁciency problems now become dom-
inant: it is still straightforward to determine emptiness of a B¨uchi automaton, but
the construction of the machines becomes problematic. As an example, consider the
following test related to nilpotency: a cellular automaton is nilpotent if there exists
a quiescent conﬁguration Y such that all conﬁgurations evolve to Y in ﬁnitely many
steps. A standard compactness argument shows that there has to be a ﬁxed bound n,
the nilpotency index, that limits the length of the corresponding transient:
∃y quiescent ∀x∃x1,...,xn−1 (x  x1  x2  ...  xn−1  y)
A priori, quiescence is not ﬁrst-order deﬁnable in L(), but we can easily add an
automatic predicate that singles out quiescent conﬁgurations. The matrix of the for-
mula has a simple structure, and can easily be converted into a product de Bruijn
automaton; alas, the size of this automaton is exponential in n and there is little hope
to be able to deal with the universal quantiﬁer, except for exceedingly small values
of n.
While B¨uchi automata can be represented by the exact same data structure as
a standard nondeterministic automaton, some of the attendant algorithms are sig-
niﬁcantly more complicated. In particular determinization based on Safra’s algo-
rithm [39] is notoriously difﬁcult to implement well; the upper bound of O(nn) is
known to be tight in general. Indeed, much effort has gone into ﬁnding ways around
determinization, see [24] and references there. Minimization techniques are also of
little help, since one would need to minimize nondeterministic machines: determin-
istic B¨uchi automata are strictly weaker than their nondeterministic counterparts,
so one has to resort to other devices such as Rabin and Muller automata, see [35].
The data structures associated with these types of automata can be quite large and
are difﬁcult to deal with in conversion algorithms between the various types of ma-
chines. Overall, and in contrast to the ﬁnite case, it is quite difﬁcult to test interesting
properties and more effort is needed to ﬁnd ways to design feasible decision algo-
rithms.
12.2.3
Bi-inﬁnite Conﬁgurations
The study of classical symbolic dynamics deals with another, yet more compli-
cated situation: bi-inﬁnite words X ∈Σζ . Acceptance conditions become corre-
spondingly more unwieldy and, informally, need to cover an inﬁnite past as well

268
K. Sutner
as an inﬁnite future. To this end let Inf−(π) = { p ∈Q |
∞
∃i ∈N(p−i = p)} and
Inf+(π) = { p ∈Q |
∞
∃i ∈N(pi = p)} denote the set of negatively and positively
recurrent states in π. Then A accepts a bi-inﬁnite word X if there exists some com-
putation π on X such that Inf−(π) ∩I ̸= /0 and Inf+(π) ∩F ̸= /0. While these con-
ditions are arguably more complicated, they sometimes are actually easier to deal
with than in the plain inﬁnite case. To wit, in the the basic de Bruijn automata all
states are initial and ﬁnal, so that Aρ(x,y) accepts its input if there is a bi-inﬁnite
computation: no complications arise from boundary conditions as in the ﬁnite and
inﬁnite case. This was used in [44] to give simple quadratic algorithms to test for
injectivity, surjectivity and openness of the global map.
Of course, for more complicated formulae the construction of the associated au-
tomaton overall becomes more involved. The only known approach is to split a
ζ-word into two ω-words and to use two B¨uchi automata to handle these one-way
inﬁnite input words, see again [35] for a careful discussion. Note, though, that we
are actually dealing with a whole family of pairs of B¨uchi automata, corresponding
to a representation of the ζ-regular languages in the form
L =

i∈I
UiopVi
where Uop stands for reversed ω-words obtained from U. The languages Ui and Vi
are ω-regular and the index set I is ﬁnite. As in the inﬁnite case we can identify a
collection of ultimately periodic words, this time words of the form ω uvwω where
u,v,w are ﬁnite words. Again we obtain an elementary subspace which we denote
by Cup.
Theorem 4. The space Cup of ultimately periodic conﬁgurations is an elementary
substructure of the full space of bi-inﬁnite conﬁgurations.
One can then express all the necessary Boolean operations as well as projections
in terms of this union-of-pairs representation. Alas, there are substantial efﬁciency
problems; in particular determinization becomes highly problematic. To see why,
note that the ﬁrst step in the determinization process is to make sure that that the
languages Ui are pairwise disjoint, and likewise for the Vi. This can be achieved
by exploiting the fact that ω-regular languages form and effective Boolean algebra,
but may cause an exponential blow-up in the size of the index set. In conjunction
with the complexity of determinization of B¨uchi automata this makes if typically
unfeasible to handle ﬁrst-order sentences of all but the most limited complexity.
Even pure Σ1 sentences can cause major efﬁciency problems if the matrix of the
formula is sufﬁciently large. A typical example for this kind of problem is again the
existence of “long” -chains, the automaton for the matrix grows exponentially in
k. Still, we have a decision algorithm, at least in principle.
Theorem 5. First-order logic for bi-inﬁnite one-dimensional cellular automata is
decidable.

12
Linear CA and Decidability
269
In this context it is particularly important to exploit computational shortcuts
whenever possible. For example, surjectivity is a priori the Π2 statement ∀x∃y(y 
x). The automaton corresponding to A∃y(yx) is nondeterministic and thus the test
for universality necessitated by the outermost quantiﬁer can cause exponential blow-
up. On the other hand, one can exploit Hedlund’s classical result that characterizes
surjectivity in terms of injectivity on conﬁgurations with ﬁnite support. More pre-
cisely, introduce the obviously automatic predicate “equal except for ﬁnitely many
places,” in symbols x E y. We can now express surjectivity as ∀x,y,z(x  z ∧y 
z ∧x E y ⇒x = y) in the extended language L(,E). Testing this formalization is
signiﬁcantly easier: once the automaton for the matrix of the formula has been con-
structed we simply have to solve a path-existence problem in a directed graph, a
problem easily tackled in linear time and space.
Perhaps more importantly, the choice of suitable additional predicates makes it
possible to formalize properties that fail to be ﬁrst-order in the original language
L(). As an example, consider openness of the global map, a property known to
be equivalent to the map being k-to-1 for some k. As such, the property cannot be
formalized in ﬁrst-order. However, consider the predicate x =L y if ∃n ∈Z∀i <
n(xi = yi) and likewise for x =R y. It is easy to see that both predicates can be
tested by a ﬁnite state machine. Both are weaker versions of being almost equal:
x E y ⇐⇒x =L y∧x =R y. Hence, the global map is open if, and only if,
∀x,y,z

x  z∧y  z∧(x =L y∨x =R y) ⇒x = y
	
,
following the same pattern as injectivity and surjectivity and using the language
L(,=L,=R). With a little more effort this produces an alternative proof for the
quadratic algorithms from [44].
12.3
Undecidability and Hardness
When it comes to undecidability, there is little difference between the one-way in-
ﬁnite situation and the two-way inﬁnite one, and we will focus on the latter. It is
clear from the previous sections that the framework appropriate to the study of
undecidability and computational hardness is given by the augmented structures
Tζ
ρ = ⟨C,, ∗→⟩and their ﬁnite counterparts Tn
ρ = ⟨Σn,, ∗→⟩. Moreover, since
the orbit relation
∗→involves transitive closure one can expect hardness results for
the plain Reachability Problem: for two conﬁgurations X and Y that have ﬁnitary
descriptions, is X
∗→Y? Technically we need to augment our language by appro-
priate constants to reﬂect Reachability, but we will ignore these details. The ﬁrst
observation is
Theorem 6. Reachability over Tn
ρ, uniformly in n, is PSPACE-hard.
This follows from the fact that cellular automata are easily capable of simulat-
ing linear bounded automata. On the other hand, over Tn
ρ, validity of any ﬁrst-order

270
K. Sutner
sentence is trivially decidable in polynomial space, so the validity problem is
PSPACE-complete. Thus, the problem is still decidable in principle, though the
computation may well fail to be feasible. This changes drastically when we try to
classify all ﬁnite structures at once by determining the spectrum of a sentence. In
particular, consider
FP ≡∀x∃y(x ∗→y∧y  y),
a Π2 sentence that expresses the assertion that every orbit ends in a ﬁxed point.
We can construe FP as a formalization of the ﬁrst Wolfram class, see [11, 42] for
a discussion of more general attempts to formalize Wolfram’s heuristics. To test
membership in this class we have to solve the data version of the model checking
problem. Alas, this problem is undecidable.
Theorem 7. It is undecidable whether the spectrum of the sentence FP from above
is universal. In fact, this property is Π 0
1-complete.
It should be noted that this result does not follow directly from standard results
about the computations of Turing machines: we need to deal with all possible con-
ﬁgurations of the cellular automaton, not just the ones that code stages in a com-
putation. As it turns out, similar assertions about the length of the limit cycle of all
orbits in Cn
ρ for all n are undecidable, see [43].
With a view towards the importance of Reachability, it is desirable to address
decidability questions in the augmented structures Tρ not in the full Cantor space of
conﬁgurations but in a subspace that is amenable to the methods of classical com-
putability theory as presented in, say, [37, 41]. As we have seen in sections 12.2.2
and 12.2.3, arguably the most plausible choice is the set of ultimately periodic con-
ﬁgurations. Further evidence for the naturalness of this choice is given by Cook’s
result [10] that, in the bi-inﬁnite case, there is an elementary cellular automaton
capable of universal computation. Cook’s proof of computational universality uses
ultimately periodic conﬁgurations. Interestingly, the construction seems to rest on
the ability to have two different periodic blocks, one extending to the left and the
other to the right: in the construction, the left block serves to time the computation
whereas the right block encodes the cyclic tag-system that is essential for universal-
ity. By contrast, Reachability for rule 110 is trivially decidable for conﬁgurations of
ﬁnite support.
It has since been shown by Neary and Woods that the exponential slow-down
inherent in the original construction can be avoided entirely, so that the simulation
is actually quite efﬁcient, at least from the perspective of complexity theory, see
[32, 33]. This has the consequence that it is P-complete to determine the state of a
particular cell at time t of the evolution of a ﬁnite conﬁguration under rule 110.
From now on, let us assume that the conﬁgurations in our structures, inﬁnite or
bi-inﬁnite, are always ultimately periodic. It is obvious that in both cases Reachabil-
ity is undecidable in general. Signiﬁcantly more effort shows that one can carefully
control the evolution of conﬁgurations on the cellular automaton in question so as
to make sure that the complexity of Reachability is an arbitrarily chosen recursively
enumerable degree, see [41]. Of course, at heart the construction rests on the ability

12
Linear CA and Decidability
271
of a one-dimensional cellular automaton to simulate Turing machines. Alas, there
are two substantial problems to overcome. First, hardness results for Turing ma-
chines are always phrased in terms of instantaneous descriptions, snap-shots of a
computation that actually occur. By contrast, we have to deal with all conﬁgurations
of the cellular automaton, most of which are meaningless from the perspective of
the Turing machine. Second, we need to make sure that there are no unintended sim-
ulations that could drive the degree of Reachability up to, say, r.e.-completeness. It
was shown in [42,45,46] and theorem 9 below how to cope with this problems. In
summary, we can derive the following result.
Theorem 8. The Reachability problem of an inﬁnite or bi-inﬁnite cellular automa-
ton over Cup can be chosen to be any r.e. degree.
The theorem suggests that, if one is interested in computational properties, as
opposed to the more classical dynamical systems aspects, one should consider a
more ﬁne-grained hierarchy based on the complexity of Reachability. The resulting
classiﬁcation is again highly undecidable: testing whether a cellular automaton has
Reachability problem of degree d is Σd
3 -complete. In particular, testing for compu-
tational universality is Σ0
4-complete. As it turns out, no constraints arise if we were
to choose to base our classiﬁcation on both Reachability and Conﬂuence: two con-
ﬁgurations are conﬂuent if their orbits overlap. Thus, conﬂuence is an equivalence
relation and corresponds vaguely to basins of attraction. It is clear from the deﬁni-
tions that Conﬂuence also has r.e. degree, but it is quite surprising that there is no
connection between the two: given arbitrary r.e. degrees d1 and d2 there is a cellu-
lar automaton whose Reachability and Conﬂuence problems have exactly those two
chosen degrees as their complexity. It remains to be seen how other more compli-
cated properties ﬁt into this pictures.
As is well-understood, the semi-lattice of the r.e. degrees exhibits a rather com-
plicated structure. For example, by a famous theorem of Sacks, the partial order of
the r.e. degrees is dense: whenever A <T B for two r.e. sets A and B there is a third
r.e. set such that A <T C <T B, [38]. Here <T denotes Turing reducibility. The ﬁrst
order theory of this semi-lattice is highly undecidable [17].
More complicated sentences in the ﬁrst-order logic of Tρ will, in general, yield
undecidable versions of the data model checking problem. For example, the nilpo-
tency statement ∃y∀x(x
∗→y ∧y  y) is undecidable according to [21]. By inter-
changing quantiﬁers we obtain the “ﬁxed point” sentence FP ≡∀x∃y(x ∗→y∧y  y)
from above. For ﬁnite cellular automata deciding FP is Π 0
1 -complete, but for inﬁnite
and bi-inﬁnite ones it is harder, given a slight restriction of the conﬁguration space.
For simplicity, let us only consider the bi-inﬁnite automata, the argument is entirely
similar in the one-way inﬁnite case. Choose a particular symbol $ in the alphabet
and deﬁne C$ ⊆Cup to be the class of all ultimately periodic conﬁgurations that
contain $ inﬁnitely often in both directions. In other words, in the representation
(u,v,w) both u and w contain $. This property is easy to check by a pair of B¨uchi
automata, so we are dealing with an automatic subspace.
Theorem 9. Deciding FP for an inﬁnite cellular automaton is Π0
2 -complete on C$.

272
K. Sutner
For the proof ﬁrst note that the problem lies in Π 0
2 , since we only consider ulti-
mately periodic conﬁgurations and these can be coded naturally as triples (u,v,w)
of ﬁnite words, representing ω uvwω. For hardness, recall that INF, the collection
of all indices of inﬁnite r.e. sets is well-known to be Π 0
2 -complete, see [41]. Now
e ∈INF ⇐⇒∀n∃m(n < m∧m ∈We) where n and m range over the naturals. The
construction of the cellular automaton ρe begins with a Turing machine Me with
binary tape that, on input 1n dovetails on computations m ∈We for all m > n. More
precisely, at stage s of the construction Me will perform s steps in the computation
of m ∈We for all n < m < n + s. If any of these computations converges, Me halts.
A standard simulation of the Turing machine Me by a cellular automaton will not
produce the desired results since there are conﬁgurations of the cellular automaton
that do not translate into instantaneous descriptions of Me. For example, there might
be several cells indicating state and head position of Me; in fact, there could be in-
ﬁnitely many such cells due to the periodicity of the blocks on either side. A signif-
icantly more difﬁcult problem is that a conﬁguration may syntactically look like an
instantaneous description of the Turing machine, but may actually not appear in any
computation of Me on any input 1n. For example, it might contain an inaccessible state
of the Turing machine Me, which state then launches a divergentcomputation despite
of the fact that the actual machine on input 1n would halt. Let us refer to an instan-
taneous description as admissible if it occurs during a computation on some input.
Needless to say, accessibility is not decidable in general but one can convert Me into
an equivalent Turing machine M′
e that is stable in the sense that it ultimately halts on
all inadmissible descriptions, see [45, 49] for a more detailed description. The idea
is to modify the machine so that it becomes self-verifying: retraces its steps every so
often to try to verify that the current instantaneous description is indeed admissible.
To this end, the Turing machine keeps a copy of the alleged original input throughout
the computation. Roughly, we consider instantaneous descriptions of the form
ω b#x#s#D#t #E #bω
where x, s and t are numbers written in unary and D and E are instantaneous de-
scriptions of Me, the original machine. The symbol b is the blank symbol for Me and
# is a new separator symbol. The idea is that x is the original input to Me and that
instantaneous description D occurs after s steps in the computation. The remaining
ﬁelds are used for the admissibility test, t as a counter and E as a corresponding
instantaneous description.
As to the actual cellular automaton that simulates the self-verifying Turing ma-
chine, consider the alphabet
Γ = Σ ∪Q∪Σ ∪{$}.
As usual, Q denotes the state set of the stable Turing machine and Σ and Σ are copies
of the tape alphabet modiﬁed by additional direction bits that indicate whether the
symbol is to the right or to the left of the state symbol. The special symbol $ is
the one that is required to appear inﬁnitely often in both directions and serves
the purpose of limiting the part of the conﬁguration that can code instantaneous

12
Linear CA and Decidability
273
descriptions. Any local conﬁgurations that represent syntactically incorrect descrip-
tions are ﬁxed points; for example, two adjacent symbols from Σ and Σ will stay
ﬁxed. Similarly two adjacent symbols from Q do not change. Changes do occur at
local conﬁgurations of the form Σ ×Q× Σ, except when the state in Q is the halting
state of the stable Turing machine. The special symbol $ can be converted into a
symbol in Σ ∪Q ∪Σ in the presence of the state head. As a consequence, the only
orbits free of “frozen” components are the ones that correspond to instantaneous
descriptions of M′
e, though a priori not necessarily admissible ones and not neces-
sarily single ones either. But since M′
e is stable, inadmissible descriptions will be
recognized and lead to the Turing machine halting. At the conﬁguration level we
obtain a ﬁxed point. Thus, the only situation where a conﬁguration can not evolve to
a ﬁxed point is when we are in fact simulating a divergent computation of Me. But
then e ∈INF if, and only if, the sentence FP holds over the structure Cρe.
We note that the argument relies quite heavily on the fact that we consider con-
ﬁgurations in C$, for general ultimately periodic conﬁgurations there appears to be
no way to organize the self-testing mechanism for the intermediate Turing machine.
To wit, a non-ﬁxed point computation could occur because of an ultimately periodic
conﬁguration of the cellular automaton that corresponds to an inﬁnitely instanta-
neous description of the Turing machine.
12.4
Summary
We have shown that a small part of the classiﬁcation of one-dimensional cellular
automata can be handled automatically by using ideas from model checking. Un-
fortunately, this approach leads to considerable efﬁciency problems and it is not
clear at present how far the corresponding decision algorithms can be pushed. Ef-
forts are under way to give a local classiﬁcation for elementary cellular automata
on one-way inﬁnite grids, but even in this relatively simply case there are problems
dealing with the corresponding B¨uchi automata. In particular, determinization us-
ing Safra’s classical algorithm often disrupts the decision algorithm. It is safe to
assume that the problems become virtually unsurmountable when one moves on to
bi-inﬁnite automata. One could try use compact representations such as binary de-
cision diagrams to try to cope with large state spaces [6,7], but ﬁrst experiments in
this direction have been less than compelling.
Needless to say, any plausible classiﬁcation scheme for cellular automata will
have to go further and include aspects of the long-term evolution of conﬁgura-
tions. Even in the one-dimensional case, assertions about long term evolution tend
to be undecidable and fully automatic classiﬁcation is simply impossible. On the
other hand, as shown conclusively by Cook, simulation tools can help to sharpen
one’s intuition and provide sufﬁcient insights into the behavior of a particular cel-
lular automaton to complete classiﬁcation “by hand.” This is somewhat similar to
the current situation in theorem proving and formally veriﬁed mathematics: fully
automatic tools can cover only so much ground, but with sufﬁcient intervention

274
K. Sutner
from the user, proof assistants can produce quite interesting results, see the re-
cent [3]. Some efforts in this direction can be found in [1, 51, 55]. One wonders
whether crowd-sourcing might be helpful; in astronomy, classiﬁcation of galax-
ies by large numbers of amateur volunteers has produced spectacular results, see
http://www.galaxyzoo.org/. For example, the pseudo-random behavior
of elementary cellular automaton rule 30 seems to make it a hopeless undertaking
to encode any kind of undecidability proof. Perhaps many eyes could ﬁnd enough
structure to support such an argument.
One natural challenge is to determine the degree of the full ﬁrst-order theory of
Tρ, perhaps ﬁrst over some specialized conﬁguration space such as C$. One suspects
that any level in the arithmetic hierarchy can be represented by a suitable assertion
about orbits of a cellular automaton. Note, though, that the preceding construction
differs in signiﬁcant technical details from the result in [49] that shows that Reach-
ability has arbitrary r.e. degree. In particular, a special annihilator symbol ⊥is used
there to reduce the complexity of orbits of syntactically incorrect conﬁgurations to
being decidable–the corresponding light cones of ⊥cells would break the current
argument. Hence, it is not entirely clear how to generalize this kind of argument to
longer chains of arithmetic quantiﬁers. Lastly, there is the question of the degree of
the full ﬁrst-order theory of Tρ over Cup or the full conﬁguration space.
References
1. Adamatzky, A.: Identiﬁcation of Cellular Automata. Taylor & Francis, London (1994)
2. Amoroso, S., Patt, Y.N.: Decision procedures for surjectivity and injectivity of parallel
maps for tesselation structures. Journal of Computer and Systems Sciences 6, 448–464
(1972)
3. Avigad, J., Harrison, J.: Formally veriﬁed mathematics. Comm. ACM 57(4), 66–75
(2014)
4. Bartholdi, L., Silva, P.V.: Groups deﬁned by automata. CoRR, abs/1012.1531 (2010)
5. Blumensath, A., Gr¨adel, E.: Automatic structures. In: Proc. 15th IEEE Symp. on Logic
in Computer Science, pp. 51–62. IEEE Computer Society Press (1999)
6. Bryant, R.E.: Graph-based algorithms for boolean function manipulation. IEEE Trans.
Computers C-35(8), 677–691 (1986)
7. Burch, J.R., Clarke, E.M., McMillan, K.L., Dill, D.L., Hwang, J.: Symbolic model
checking: 1020 states and beyond. Information and Computation 98(2), 142–170 (1992)
8. Chang, C.C., Keisler, H.J.: Model Theory. In: Studies in Logic and the Foundations of
Mathematics, Elsevier (1990)
9. Clarke, E., Grumberg, O., Peled, D.: Model Checking. MIT Press (2000)
10. Cook, M.: Universality in elementary cellular automata. Complex Systems 15(1), 1–40
(2004)
11. Culik, K., Yu, S.: Undecidability of CA classiﬁcation schemes. Complex Systems 2(2),
177–190 (1988)
12. Epstein, D.B.A., Cannon, J.W., Holt, D.F., Levy, S.V.F., Patterson, M.S., Thurston, W.P.:
Word Processing in Groups. Jones and Bartlett, Burlington (1992)
13. Finkel, O.: On decidability properties of one-dimensional cellular automata. Computing
Research Repository, abs/0903.4615 (2009)
14. Garey, M.R., Johnson, D.S.: Computers and Intractability. Freeman (1979)

12
Linear CA and Decidability
275
15. Grigorchuk, R., ˇSuni´c, Z.: Self-Similarity and Branching in Group Theory. In: Groups
St. Andrews 2005. London Math. Soc. Lec. Notes, vol. 339. Cambridge University Press
(2007)
16. Grigorchuk, R.R., Nekrashevich, V.V., Sushchanski, V.I.: Automata, dynamical systems
and groups. Proc. Steklov Institute of Math. 231, 128–203 (2000)
17. Harrington, L., Shelah, S.: The undecidability of the recursively enumerable degrees.
Bull. Amer. Math. Soc. 6, 79–80 (1982)
18. Hedlund, G.A.: Endomorphisms and automorphisms of the shift dynamical system.
Math. Systems Theory 3, 320–375 (1969)
19. Huth, M., Ryan, M.: Logic in Computer Science: Modelling and Reasoning about Sys-
tems, Cambridge, UP (2000)
20. Kari, J.: Reversibility of 2D cellular automata is undecidable. Physica D 45, 379–385
(1990)
21. Kari, J.: The nilpotency problem of one-dimensional cellular automata. SIAM J. Com-
put. 21(3), 571–586 (1992)
22. Khoussainov, B., Nerode, A.: Automatic presentations of structures. In: Leivant, D. (ed.)
LCC 1994. LNCS, vol. 960, pp. 367–392. Springer, Heidelberg (1995)
23. Khoussainov, B., Rubin, S.: Automatic structures: overview and future directions. J. Au-
tom. Lang. Comb. 8(2), 287–301 (2003)
24. Kupferman, O.: Avoiding determinization. In: Proc. 21st IEEE Symp. on Logic in Com-
puter Science (2006)
25. Kurka, P.: Languages, equicontinuity and attractors in cellular automata. Ergodic Th.
Dynamical Systems 17, 417–433 (1997)
26. Kurka, P.: Topological and Symbolic Dynamics. Number 11 in Cours Sp´ecialis´es. Soci-
ete Mathematique de France, Paris (2003)
27. Li, W., Packard, N.: The structure of the elementary cellular automata rule space. Com-
plex Systems 4(3), 281–297 (1990)
28. Li, W., Packard, N., Langton, C.G.: Transition phenomena in CA rule space. Physica
D 45(1-3), 77–94 (1990)
29. Lind, D.: Multi-dimensional symbolic dynamics. In: Symbolic Dynamics and its Appli-
cations. Proc. Sympos. Appl. Math., vol. 60, pp. 61–79. AMS (2004)
30. Margenstern, M.: Frontier between decidability and undecidability: a survey. TCS 231,
217–251 (2000)
31. Meyers, R.A. (ed.): Encyclopedia of Complexity and System Science. Springer, Berlin
(2009)
32. Neary, R., Woods, D.: On the time complexity of 2-tag systems and small universal turing
machines. In: FOCS, pp. 439–448. IEEE Computer Society, Washington (2006)
33. Neary, T., Woods, D.: P-completeness of cellular automaton rule 110. In: Bugliesi,
M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP 2006. LNCS, vol. 4051,
pp. 132–143. Springer, Heidelberg (2006)
34. Nekrashevych, V.: Self-Similar Groups. In: Math. Surveys and Monographs, vol. 117.
AMS (2005)
35. Perrin, D., Pin, J.-E.: Inﬁnite Words. In: Pure and Applied Math., vol. 141, Elsevier,
Amsterdam (2004)
36. Rabin, M.O., Scott, D.S.: Finite automata and their decision problems. IBM Jour. Re-
search 3(2), 114–125 (1959)
37. Rogers, H.: Theory of Recursive Functions and Effective Computability. McGraw-Hill,
New York (1967)
38. Sacks, G.E.: The recursively enumerable degrees are dense. Ann. Math. 80, 300–312
(1964)

276
K. Sutner
39. Safra, S.: On the complexity of ω-automata. In: Proc. 29th FOCS, pp. 319–327. IEEE
Computer Soc. Press, Washington (1988)
40. Sakarovitch, J.: Elements of Automata Theory. Cambridge University Press (2009)
41. Soare, R.I.: Recursively Enumerable Sets and Degrees. In: Perspectives in Mathematical
Logic. Springer, Berlin (1987)
42. Sutner, K.: A note on Culik-Yu classes. Complex Systems 3(1), 107–115 (1989)
43. Sutner, K.: Classifying circular cellular automata. Phys. D 45(1-3), 386–395 (1990)
44. Sutner, K.: De Bruijn graphs and linear cellular automata. Complex Systems 5(1),
19–30 (1991)
45. Sutner, K.: Cellular automata and intermediate degrees. Theoretical Computer Sci-
ence 296, 365–375 (2003)
46. Sutner, K.: Universality and cellular automata. In: Margenstern, M. (ed.) MCU 2004.
LNCS, vol. 3354, pp. 50–59. Springer, Heidelberg (2005)
47. Sutner, K.: Encyclopedia of Complexity and System Science, chapter Classiﬁcation of
Cellular Automata. In: Meyers [31], (2009)
48. Sutner, K.: Model checking one-dimensional cellular automata. J. Cellular Au-
tomata 4(3), 213–224 (2009)
49. Sutner, K.: Cellular automata, decidability and phasespace. Fundamenta Informati-
cae 140, 1–20 (2010)
50. Sutner, K., Lewi, K.: Iterating invertible binary transducers. In: Kutrib, M., Moreira, N.,
Reis, R. (eds.) DCFS 2012. LNCS, vol. 7386, pp. 294–306. Springer, Heidelberg (2012)
51. Vorhees, B.: Computational Analysis of One-Dimensional Cellular Automata. World
Scientiﬁc, Singapore (1996)
52. Wolfram, S.: Computation theory of cellular automata. Comm. Math. Physics 96(1),
15–57 (1984)
53. Wolfram, S.: Twenty problems in the theory of cellular automata. Physica Scripta T9,
170–183 (1985)
54. Wolfram, S.: A New Kind of Science. Wolfram Media, Champaign (2002)
55. Wuensche, A.: Classifying cellular automata automatically. Complexity 4(3), 47–66
(1999)

Chapter 13
Algorithms with Active Cells Modeled
by Cellular Automata with Write-Access (CA-w)
Rolf Hoffmann
Abstract. The CA-w model (cellular automata with write-access) was introduced
in order to simplify the description of problems with dynamic activities, like moving
agents or active particles in a grid of cells. This model allows an active cell to send
data to a neighboring cell. Thereby neighbors can be activated or deactivated, or
the movement of agents can directly be described. The usefulness of the model
is demonstrated for basic computational problems, the well-known 1-D CA trafﬁc
rule, Pascal’s triangle, Fibonacci numbers, sorting on the ring by agents, and leader
election for agents.
13.1
Introduction
A special class of distributed algorithms can be deﬁned by using active cells, also
called active particles [12] or agents, which work together in parallel in order to
ﬁnd an aimed ﬁnal global conﬁguration. In many applications only a part of all cells
in the environment are active cells, and their number may dynamically change over
time, and some cells may behave like moving agents. The question here is how
such algorithms can be described in a natural and concise way. One way is to use
the Cellular Automata (CA) model. Although such algorithms can be described by
CA, the description often becomes complicated, artiﬁcial, and redundant. The main
reason is that in CA a cell can only read information from its neighbors and cannot
change its neighbors’ states directly. Therefore the movement of agents, the change
of neighboring data, and the control of activity becomes difﬁcult to describe. The
solution presented here is to use the CA-w (local version of the GCA-w model)
instead of the CA model.
Rolf Hoffmann
Technische Universit¨at Darmstadt, FG Rechnerarchitektur,
Hochschulstr. 10, 64289 Darmstadt, Germany
e-mail: hoffmann@ra.informatik.tu-darmstadt.de
c⃝Springer International Publishing Switzerland 2015
277
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_13

278
R. Hoffmann
B
A
C
D
B
A
C
D
h1
h2
h1
h2
h3
Generation t 
Generation t+1 
Dynamic Link
h3
Fig. 13.1
A 2-D GCA-w example with three links per cell: Each cell is dynamically con-
nected to a set of global neighbors, only the activity of the center cell C is shown. The state
of C including the links hi, and the states of its neighbors can be changed (white to black) by
a local rule. Thereby information can be transferred from C to the current neighbors A,B,D,
and the activity of the neighbors may be changed. Write conﬂicts may occur, e.g. if C itself
and other cells try to update C at the same time.
The GCA-w parallel computing model (GCA with write-access) introduced in [6,
7] is an extension of the GCA (Global Cellular Automata) model [3,4]. If the neigh-
borhood of the GCA-w model is locally restricted, we will call the model CA-w
(Cellular Automata with write-access) [8].
A GCA cell can dynamically establish links to any of its global neighbors, whereas
a CA cell can only use the ﬁxed links to its local neighbors. CA and GCA do not al-
low to modify the state of a neighbor. Therefore no write-conﬂict can occur, sim-
plifying implementations in hardware and in software. However, for applications
where the amount of active cells in the whole ﬁeld is low or is varying over time,
or the locations of the active cells are changing, the GCA-w model (with global dy-
namic write-neighbors) or the CA-w model (with local dynamic write-neighbors)
are a better choice. These models allow a cell to write information to its neighbors.
This feature is very important because information can actively be transferred to a
destination, and the activity of the destination can be switched on or off. Therefore,
write-access is very useful for the description of problems with moving particles or
moving agents, or problems with dynamic activities.
Several GCA-w applications were already described in [5–7] (one-to-all com-
munication, synchronization, moving agents, different random walks of particles,
pointer inversion,sorting with pointers, trafﬁc simulation). And it was already shown
in [8] that problems with local movements (like rotor-routing and chip-ﬁring) can be
described adequately by the CA-w model. The purposeof this contribution is to show
that other interesting computational problems can easily be modeled by CA-w. In the
following two sections the properties of the GCA-w and CA-w models are reviewed.

13
Algorithms with Active Cells Modeled by Cellular Automata
279
i
j
k
r+w
i
j
r
i
j
r
w
w
r+w
w
r+w
k
i
j
r+w
r+w
p
w
r+w
(a)                                        (b)                  
(c)                                             (d)
i
j
k
r+w
i
j
r
i
j
r
w
w
r+w
w
r+w
k
i
j
r+w
r+w
p
w
r+w
(a)                                        (b)                  
(c)                                             (d)
Fig. 13.2 Some read / write situations: (a) Cell i reads from cell j and modiﬁes its own state.
(b) Cell i reads j and modiﬁes its own state and the state of its neighbor j (the basic idea of
GCA-w). (c) Cell i modiﬁes itself and is modiﬁed by cell j and k, but there occurs a conﬂict
of three write-accesses which has to be resolved. (d) Cell i modiﬁes j and vice versa. Cell p
modiﬁes cell k and p. No write conﬂicts occur (exclusive write situation).
13.1.1
Global Cellular Automata with Write-Access (GCA-w)
Fig. 13.1 shows the general idea of the GCA-w model: Each cell C is dynamically
connected via links (also called hands 1) hi to other cells, in the example to A,B,D.
Cell C updates its own state and the states of its neighbors A,B,D. Thereby the links
hi are also updated. In the following, only one link per cell is assumed, which seems
to be sufﬁcient to model most applications. In addition, in the following, the cells
will be arranged in an 1-D fashion, although n-D ﬁelds can easily be handled by
using an n-D indexing scheme.
A potential difﬁculty is that write-conﬂicts may appear. The worst case scenario
is that all cells want to write onto the same cell. In order to reduce the implemen-
tation effort to resolve conﬂicts, the GCA-w rules should be designed in such a
way that either no conﬂict will occur (which is the case for the trafﬁc rule, Sect.
13.2.1), or that the amount of conﬂicts is low or restricted, or that the conﬂicts can
be resolved within a local neighborhood.
In Fig. 13.2 some situations are depicted, showing some possible interactions
(reads and writes) between cells. In situation the cell i reads j, and modiﬁes only its
own state like in CA. In situation (b) cell i reads j, and modiﬁes its own state and
the state of its neighbor j (the basic idea of GCA-w). In situation (c) cell i modiﬁes
itself and is modiﬁed by j and k, a conﬂict with three write-accesses occurs that has
to be resolved. Situation (d) is an “exclusive-write situation”: Cells i and j exchange
data, and cell p modiﬁes cell k and p; no write conﬂicts occur.
In the following the model is formally deﬁned.
13.1.1.1
Formal Description
GCA-w = (I,Q,δ,h, f,g,e)
(1)
Index Set, unique labels identifying the cells:
I = {0,1,...,i,...,N-1}
(2)
1 If k access pointers are used, then we call the GCA-w “k−handed”; this terminology was
already used for the GCA model [3].

280
R. Hoffmann
States: Q = A∪P∪D
(3)
Active States: A = {a1,a2,...,an}
(4)
Passive States: P = {p1, p2,..., pm}
(5)
Dead States: D = {d1,d2,...,dk}
(6)
Don’t-Write-Symbol: δ
Write-Values: Qδ = Q∪{δ}
(7)
Conﬁgurations: QN
(8)
A Conﬁguration: L = (q0,q1,...,qi,...,qN−1) ∈QN,i ∈I
(9)
Neighbor’s Address (in the case of absolute addressing) h(i,q):
h : I × Q →I
(10)
Neighbor’s Address (in the case of relative addressing) hrel(i,q):
hrel : I × Q →Irel = {0,±1,±2,...,±(N-1)}
(11)
such that h(i,q) = i+ hrel 2)
(12)
Local-Rule f(i,q,q∗), where q∗= neighbor’s state:
f : I × Q× Q →Qδ
(13)
Write-Rule g(i,q,q∗):
g : I × Q× Q →Qδ
(14)
Conﬂict-Rule e(i, f,g0,g1,...,g j,...,gN−1):
e : I × QN+1
δ
→Qδ
(15)
Rule Application (synchronous updating) ∀i ∈I:
qi ←
⎧
⎪
⎨
⎪
⎩
e(i, f(i,qi,qh(i,qi)),g0
→i,..,g j
→i,..,gN-1
→i )
IF qi ∈A∧e ̸= δ
(16)
qi
IF qi ∈D∨e = δ
(17)
e(i,δ,g0
→i,...,g j=i
→i = δ,...,gN-1
→i )
IF qi ∈P∧e ̸= δ
(18)
where ∀(i, j) ∈I × I:
g j
→i =

g(j,q j,qh(j,qj))
IF h(j,q j) = i∧q j ∈A
(19)
δ
IF h(j,q j) ̸= i∨q j /∈A
(20)
The cells are arranged as a sequence ⟨ai⟩i∈I of cells, each cell is labeled by its
index i (1). The index can also be accessed by the cell itself in order to implement
non-uniform rules. A cell can be in an active state (4), in a passive state (5), or
in a dead state (6). These three classes of states are also called operational states.
Operational states are introduced in order to switch on or off the activity of the
cells, and thereby allowing to reduce the computational effort. Dead cells are totally
excluded from the computation because they remain dead forever. Passive cells do
2 All addresses are mapped onto the interval 0..N −1 by the modulo operation mod N.

13
Algorithms with Active Cells Modeled by Cellular Automata
281
not compute themselves but can be activated by other cells. In addition, passive or
dead cells can be used to deﬁne a termination condition, e.g. if all cells become
dead, or if all cells become passive.
If a cell is active, it computes all the local functions h, f,g,e. If a cell is passive,
it does not compute its local functions h, f,g, but it can be switched into another
operational state from outside, e.g., it can be changed into active. If a cell is dead, it
will stay dead forever (it can be used as a constant).
The symbol delta (“Don’t-Write”) is a special output value of the functions f,
g, and e. As a possible input of the conﬂict rule e it signals that no valid value is
received from another cell for writing. In the case that the conﬂict rule does not
receive any valid input (̸= δ), it produces δ, too. Then the cell’s state will remain
unchanged.
The address function h deﬁnes the actual neighbor (absolute address, index) in
access (read and write) (10). The neighbor’s address can also be determined by the
use of the relative addressing function hrel (11). Relative addressing is often more
adequate and more general to describe spatial relative situations. The local rule f
computes the cell’s new state if no neighbor is writing additionally. The write-rule
computes a state value that can be written to the actual neighbor. The conﬂict-rule is
dedicated to resolve the conﬂicts. It receives N+1 messages (write-values), the own
rule value f and messages g j
→i from all cells j.
Not all messages need to be valid: A non-valid δ-message is interpreted as a
“Don’t-Write”- command, meaning that a sender j does not want to write to a re-
ceiver i. A message is valid if the sender j is active and the receiver i is selected
(18). Note that δ may be produced by g, f,e. If the sender is passive or the receiver
is not selected, then δ is received by default.
Rule Application. All cells are updated synchronously in parallel. Only cells that
are not dead can be updated. If a cell is active (16) then the own rule’s value f
and the messages g j
→i from all cells j have to be taken into account. In case that a
δ–message is received, it is disregarded by the conﬂict rule e.
If a cell is passive (18) then no computation of h, f,g takes place and the default
values f = δ and g = δ are assumed, and no neighbor is selected. Although the cell
is passive, the conﬂict-rule e has to be awake because activating messages might
arrive.
At a ﬁrst glance the GCA-w model seems to be too complex because of the con-
ﬂicts and not very useful. But in many applications the complexity of the conﬂict
resolution can be reduced signiﬁcantly, e.g. if the dynamic neighborhood access pat-
terns are restricted or are known in advance, or if the rules fulﬁll the exclusive-write
condition (as in the following algorithms), or if the conﬂict resolution corresponds
to a reduction operator (e.g. summing up the inputs). The main advantage of the
GCA-w model is that it allows to describe a certain class of algorithms more natural
and concise, less redundant and energy saving (only the active cells are comput-
ing). The novel idea is to organize the computational task logically as an array of
cells with different operational states (active, passive, dead) with write-access onto
dynamically selected neighbors by the use of local rules only.

282
R. Hoffmann
h
address neighbor
read from neighbor
write to neighbor
e
e
e
f  
compute
neighbor
compute f, g
conflict
resolution
state
next state
gi-1                            gi+1
g
h
address neighbor
read from neighbor
write to neighbor
e
e
e
f  
compute
neighbor
compute f, g
conflict
resolution
state
next state
gi-1                            gi+1
g
Fig. 13.3 1-D CA-w model with one hand. The right neighbor is selected in this example.
The functions f,g are computed, the value g is sent to the right neighbor. The center cell
receives gi−1 from the left, and gi+1 from the right. These messages are taken into account
by the conﬂict resolution function e.
13.1.2
Cellular Automata with Write-Access (CA-w)
We will call the GCA-w model “CA-w” (Cellular Automata with write access) if the
neighborhood is locally restricted. As in the GCA-w model, the CA-w model may
use k “hands” to access k neighbors in parallel, then we call the CA-w k-handed.
Each hand can read from and / or write to a dynamically selected neighbor. Many
applications can be described with one hand only. The formal description of an 1-D
CA-w with one hand is the following (only the formulas differing from the ones in
Sect. 1.1 are given):
Cellular Automata with Write Access:
GCA-w = (I,Q,δ,h, f,g,e)
(1)
Neighbor’s Address (in case of absolute addressing) h(i,q):
h : I × Q →I, (i−R) ≤h′ ≤(i+ R), h = h′ mod N
(10)
Neighbor’s Address (in case of relative addressing) hrel(i,q):
hrel : I × Q →Irel = {0,±1,±2,...,±R}
(11)
such that h(i,q) = (i+ hrel) mod N
(12)
Conﬂict-Rule e(i, f,gi−R,...,gi−1,gi,gi+1,...,gi+R):
e : I × Q2R+1
δ
→Qδ
(15)
Rule Application (synchronous updating) ∀i ∈I:
qi :=
⎧
⎪
⎨
⎪
⎩
e(i, f(i,qi,qh(i,qi)),gi−R
→i ,...,gi+R
→i )
IF qi ∈A∧e ̸= δ
(16)
qi
IF qi ∈D∨e = δ
(17)
e(i,δ,gi−R
→i ,...,g j=i
→i = δ,...,gi+R
→i )
IF qi ∈P∧e ̸= δ
(18)

13
Algorithms with Active Cells Modeled by Cellular Automata
283
In this deﬁnition it is assumed that the neighborhood is symmetric, the neighbor’s
address lies within a certain radius R (10, 11), and exactly one neighbor is selected.
But in general, a set of possible neighborhoods is given (e.g. one or two cells ahead
in the moving direction of an agent), one of them being dynamically selected by a
neighborhood index. It is also possible, that the actual access-right (read, write, read
and write) differs from hand to hand (if the CA-w has multiple hands).
Inputs of the conﬂict rule (15) are the own function f and the possible messages
gi−R,...,gi+R from the neighbors. The next state qi at time t + 1 is given by the
output of e (16, 17, 18).
A more speciﬁc 1-D case (see Fig. 13.3 as an example): The radius is R = 1 and
only the left neighbor (i −1) or the right neighbor (i + 1) can be addressed. (The
access to the own cell i is excluded here because the own state is available through
the function f.) Then the deﬁnition simpliﬁes to
Conﬂict-Rule e(i, f,gi−1,gi+1):
e : I × Q3
δ →Qδ
(15)
Rule Application (synchronous updating) ∀i ∈I:
qi :=
⎧
⎪
⎨
⎪
⎩
e(i, f(i,qi,qh(i,qi)),gi−1
→i ,gi+1
→i )
IF qi ∈A∧e ̸= δ
(16)
qi
IF qi ∈D∨e = δ
(17)
e(i,δ,gi−1
→i ,gi+1
→i )
IF qi ∈P∧e ̸= δ
(18)
As result, in the 1-D CA-w model with one hand and with read/write-radius 1,
there are only three inputs of the conﬂict rule: f (own function), gi−1
→i (message
from left neighbor), and gi+1
→i (message from right neighbor). Analyzing the data
ﬂow depicted in Fig. 13.3, it can be observed that data from the neighbors within
the radius 2 can inﬂuence the new state of the own cell. Therefore the CA-w model
with radius 1 can be emulated by a CA model with radius 2.
13.1.3
Related Work
The PSA (Parallel Substitution Algorithm) model [1] of computation is a very gen-
eral and powerful model based on substitution rules. It allows also to modify a set
of arbitrary target cells (right side of the substitution) using a base and a context.
In relation to the GCA-w the base corresponds to the cell(s) under consideration,
the context corresponds to the read neighbors and the right side corresponds to the
cells which are modiﬁed (usually a part of the base). There is also a relation to the
CRCW-PRAM [9, 10] model. The PRAM model is based on a physical view with
p processors that have global memory access to physical data words whereas the
GCA-w is based on logical computing cells with local memory tailored to the ap-
plication. Another difference of the GCA-w model compared to PRAM is the direct
support of dynamic links and the rule based approach similar to the CA model.

284
R. Hoffmann
13.2
CA-w Algorithms
13.2.1
Trafﬁc Rule
Fig. 13.4
Trafﬁc Rule. (a) This table describes the new state C’ depending on the neigh-
borhood LCR. The behavior can also be described by the set(0) and set(1) operations, or
the toggle operation. (b) The graph shows how the new state is updated by set operations
(C′ ←1/0).
Fig. 13.5
Trafﬁc Rule. (a) Substitution, the pattern 10 is substituted by 01. (b) Push rule,
particles are active and are pushed to the right cell if it is empty. (c) Pull rule, empty cells are
active and pull particles from left to right.
The elementary 1-D CA rule 184, also known as trafﬁc rule, describes the moving
of particles or cars in one direction, as given by the table Fig. 13.4. In classical CA
the new state is computed by the logical functionC′ ←(C∧R)∨(L∧¯C). If the cell’s
state is 1, its new state is taken from the cell R to its right. Otherwise, its new state
is taken from the cell L to its left. This means that the rule can be implemented by a
multiplexer controlled by C: C′ ←if C then R else L.
Another way to compute the next state is to use the operations: set(value) and
δ (no operation). By default, δ is received by each cell, and in this case the cell
remains in its old state. The trafﬁc rule, described by set-operations only, is
C′ ←
⎧
⎪
⎨
⎪
⎩
1
if LC = 10
0
if CR = 10
C
otherwise (δ)
.

13
Algorithms with Active Cells Modeled by Cellular Automata
285
Depending on who is the sender of the operations, four different cases can be
distinguished.
(1) CA Cell Rule. As shown in Fig. 13.4 b, both operations (set(0), set(1)) are sent
and received by the cell itself, according to the CA principle that a cell can
change its own state only.
(2) Substitution. (Fig. 13.5 a) Both operations are enabled by a unit that can be
imagined in between of each pair of cells (A, B). Thus, wherever the pattern 10
appears it is substituted by 01 without any conﬂict.
(3) Push Rule. (Fig. 13.5 b) Only cells containing particles are considered to be
active. The value 1 is pushed (beamed, copied) by the active cell to the right
neighbor. This can be interpreted in a way where the particle moves actively to
the next free position. The operation set(1) is sent to the right neighbor, and the
operation set(0) is performed on the cell itself. Although two different values
can be accepted by a cell, there will be no conﬂict because the rule is inherently
conﬂict-free.
(4) Pull Rule. (Fig. 13.5 c) Only empty cells are considered to be active. If an empty
cell perceives a particle coming from left, it is copied (pulled) from L to C by
the operation set(1), and the particle on L is deleted by sending set(0) to it. The
pull rule is useful for applications where the particle is driven by an external
ﬁeld or force.
The push rule and pull rule show clearly that the CA-w model can be applied
successfully to describe the movement of particles in an adequate way. The substi-
tution (Fig. 13.5 a) can be shifted to the left cell, yielding the push rule, or shifted
to the right, yielding the pull rule.
A program is presented for the push rule. Only the cells that are 1 are considered
to be active.
TYPE cell = (data: BIT)
// BIT = 0/1
C: ARRAY [0 .. n-1] OF cell
self <=> C[k], right <=> C[k+1]
REPEAT
PARALLEL self WHERE (self.data = 1)
// only do for active cells
IF (right.data=0) THEN
self.data
<= 0
right.data <= 1
ENDIF
ENDPARALLEL
ENDREPEAT
A program for the pull rule is the following. Only the cells that are 0 are consid-
ered to be active.
TYPE cell = (data: BIT)
// BIT = 0/1
C: ARRAY [0 .. n-1] OF cell
self <=> C[k], left <=> C[k-1]
REPEAT
PARALLEL self WHERE (self.data = 0)
// only do for active cells
IF (left.data=1) THEN

286
R. Hoffmann
self.data <= 1
left.data <= 0
ENDIF
ENDPARALLEL
ENDREPEAT
13.2.2
Pascal’s Triangle
This well-known triangle computes the binomial coefﬁcients. It can be drawn in
different ways. Filling empty sites with zeros (Fig. 13.6 a), the computation can
easily be formulated by the cellular automata rule: (Center ←Left + Right). How-
ever, redundant ”zero” additions are performed. In order to avoid unnecessary zero
additions, the aligned representation (Fig. 13.6 b) can be used. Filling the empty
sites with zeros the computation can be described by the CA rule (Center ←Left +
Center) more efﬁciently.
Note that in a CA with n cells always (n −1) cells are updated, although in this
example the interesting information is propagated step by step from left to right.
Only in the last generation, all cells produce a useful result (except cell 0). On
average, approximately only one third of the cells are producing the coefﬁcients. Of
course, by the use of a central control, the cells can increasingly be included into
the computational process. But it is desired to control the amount of active cells by
the cells themselves in a decentralized way. The CA-w model supplies this feature.
The redundant production of zeros at the right part of a line can be avoided. In the
following program only useful computations are performed.
For simplicity, we start at time step t = 1. Initially the two cells (k = 1,2) are set
to one and only the cell (k = 1) is active. Then two actions are performed repeatedly:
(1) the rightmost active cell writes d ←1 to its right neighbor and activates it, (2)
each active cell adds the value of its left neighbor to its own value. Thereby, at every
time step, a new cell active cell is generated at the right margin.
TYPE cell = (data: integer; active: (FALSE, TRUE))
C: ARRAY [0 .. n] of cell
// example n=4
self <=> C[k], left <=> C[k-1], right <=> C[k+1]
// cell and neighbors
0
0
0
0
0
0
1
1
1
1
1
1
0
1
2
3
4
5
0
0
1
3
6
10
0
0
1
4
10
0
1
5
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
2
3
4
5
1
3
6
10
1
4
10
1
5
1
k = 0   1   2   3   4   5   
(a)
(b)
t = 0
1
2
3
4
5
0
0
0
0
0
0
1
1
1
1
1
1
0
1
2
3
4
5
0
0
1
3
6
10
0
0
1
4
10
0
1
5
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
1
2
3
4
5
0
0
1
3
6
10
0
0
1
4
10
0
1
5
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
2
3
4
5
1
3
6
10
1
4
10
1
5
1
k = 0   1   2   3   4   5   
1
1
1
1
1
1
1
2
3
4
5
1
3
6
10
1
4
10
1
5
1
k = 0   1   2   3   4   5   
(a)
(b)
t = 0
1
2
3
4
5
Fig. 13.6
Pascal’s Triangle. (a) The value ”0” is used to indicate the spaces. The value
x[k] in line t is the sum of x[k −1] + x[k + 1] of line (t −1), corresponding to the 1–D CA
rule: Center ←Left +Right). (b) Aligned representation, the result of deleting the ”0”s from
Fig. 13.6 a and shifting the remaining numbers to the left border.

13
Algorithms with Active Cells Modeled by Cellular Automata
287
INITIAL
C[0].data <= 1, C[1].data <= 1
C[1].active <= TRUE
C[/=1].active <= FALSE
ENDINITIALl
REPEAT
PARALLEL self WHERE (self.active)
// only do for active cells
self.data <= self.data + left.data
IF (self.data = 1) THEN right.data <= 1, right.active <= TRUE ENDIF
ENDPARALLEL
ENDREPEAT
13.2.3
Fibonacci Numbers
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
3
3
3
5
5
8
k = 0   1   2   3   4   5    ...
0
0
0
0
0
0
1
1
2
3
5
8
0
1
1
2
3
5
(a)
(b)
k = 0   1
t = 0
1
2
3
4
5
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
3
3
3
5
5
8
k = 0   1   2   3   4   5    ...
0
0
0
0
0
0
1
1
2
3
5
8
0
1
1
2
3
5
(a)
(b)
k = 0   1
t = 0
1
2
3
4
5
Fig. 13.7 Fibonacci Numbers. (a) The cell k = 1 is always the only active cell (shaded). The
new value at k = 0 in line t +1 is a copy of the old value at k = 1 from line t. The new value
at k = 1 in line t +1 is the sum of the old values from line t. The Fibonacci numbers appear
sequentially in column k = 1. (b) The active cell at the right edge is moving step by step to
the right (k = 1,2,...). When the active cell k is moving to the right, it writes also the sum of
its own value(k) and the value(k −1) of its left neighbor to the new position k +1.
There are several ways to describe the time evolution of the Fibonacci Numbers
by CA-w. Two possibilities are shown in Fig. 13.7. As shown in Fig. 13.7 a there
are only two cells, the right of them is the only active one. It copies its own value to
its left neighbor at k = 0, computes the sum of the value of its left neighbor and its
own value and writes it to itself.
left.data <= self.data
self.data <= self.data + left.data
In order to let the numbers appear in spatial and not in temporal order, the active
cell is moving step by step to the right (Fig. 13.7 b). It computes the sum of the value
of its left neighbor and its own value and writes it to the right neighbor, thereby
activating the right neighbor and deactivating itself.
TYPE cell = (data: integer; active: (FALSE, TRUE))
C: ARRAY [0 .. ] of cell
\clearpage{}
self <=> C[k], left <=> C[k-1], right <=> C[k+1] //active cell and neighbors
INITIAL

288
R. Hoffmann
C[0].data <= 0
C[1].data <= 1
C[1].active <= TRUE
C[/=0].active <= FALSE
ENDINITIAL
REPEAT
PARALLEL self WHERE (self.active)
// only do for active cells
self.active
<= FALSE
right.active <= TRUE
right.data
<= self.data + left.data
ENDPARALLEL
ENDREPEAT
13.2.4
Sorting on the Ring with Agents
The goal is to sort N numbers placed on a ring (1-D array of N cells with cyclic
boundary) using agents (Fig. 13.8). We will call an algorithm which is performed
by agents “agent algorithm” (a-algorithm). We may distinguish between the local
agent algorithm which is performed by each agent, and the global agent algorithm
which is performed by all agents together.
Each agent can be seen as a small processor, which often is mobile, but this is
not a necessary condition. By moving around, an agent can interact with the envi-
ronment including the other agents. We call an array of cells with agents situated
on them “Multi-agent System” (MAS). A MAS can be classiﬁed as a distributed
system [13].
The objectives and constraints for the sorting task are
• Find an agent algorithm that sorts N numbers placed on a ring.
• The algorithm shall not depend on global information.
• The algorithm shall work with any number k of agents.
• The initial direction of each agent can be arbitrary and shall not be changed
during the execution.
• The algorithm shall be designed for simultaneous synchronousupdating (obeying
to a central clock).
• The algorithm shall also work with asynchronous updating.
moves
(gets priority)
moves
moves
blocked
moves
blocked
(no priority)
conflict
move
swap
marker
moves
Fig. 13.8 Moving agents on a ring. The agents can notice the length of the ring by a marker
(dot). The agents shall sort the numbers on the ring in increasing order (from left to right
(dot)).

13
Algorithms with Active Cells Modeled by Cellular Automata
289
A   B
(a) blocked
(b) moving & sorting
(c) moving & sorting
A    B
t
t+1
order up
order down
order up
order down
no operation
no operation
Fig. 13.9 Situations and solutions. (a) Agent A is blocked by B, A performs no action. (b)
Agent is moving left or right, thereby ordering the numbers. Dark gray represents a number
that is higher then the number represented by light gray. (c) When an agent notices the marker
(end of the array to be sorted), the ordering relation is inverted for once.
(a) conflict
(b) swapping
order
order
priority left
priority right
A    C     B
(a1)
order
order
order
order
(b1)
(a2)
(b2)
(b3)
A    C     B
A    B
A    B
A    B
Fig. 13.10 Situations and solutions. (a) Conﬂict, solved by giving priority to the agent from
left (a1). (b) Swapping. Agents exchange their positions, all three solutions yield the same
result.
The basic algorithmic idea used here for the solution is odd–even sort [2]. This
algorithm processes N/2 times the two following alternating phases:
• Phase 1: all N/2 pairs of elements (d[i],d[i + 1]) for even i are compared and
transposed if they are in the wrong order.
• Phase 2: all N/2−1 pairs of elements (d[i],d[i+1]) for odd i are compared and
transposed if they are in the wrong order.
Now the idea is that each agent tries to order a pair of numbers when moving
around. When only one agent is in the system, then the algorithm is simple. Let us
assume that the agent moves rightwards (clockwise) in the ring. The agent compares
the number in front with its own number and orders them if necessary (Fig. 13.9 b).
The last element (maximum number at the end) in the sequence to be ordered is
marked by a dot. This dot speciﬁes the perimeter of the ring, it can be seen as an
implicit information about the number N of data elements being used. When the
agent detects the particular “dot”-situation (Fig. 13.9 c) when moving rightwards, it
inverts the ordering relation for once from up to down, because the number follow-
ing the dot is supposed to be smaller (the minimum at the end). In this way an agent
circulates around the ring not more than (N −1)2 times (until the sequence is sorted
(see the following simulation sequences, ﬁrst case, conﬁg: 0). It is interesting to ob-
serve the next simulation (conﬁg: 1), where the agent moves counter-clockwise and

290
R. Hoffmann
immediately reacts on the “dot” situation (Fig. 13.9 b). For this case the numbers
are sorted already after 15 time steps.
The whole designed algorithm takes into account all the solutions shown in
(Fig. 13.9 a, b, c) and (Fig. 13.10 a1, b3). It is assumed that two head-on agents
can swap their positions. The swapping case solutions (Fig. 13.10 b1, b2, b3) yield
the same result, it does not matter which one of the agents performs the data ex-
change.
In the case (b1) agent A is performing the ordering, and in the case (b2) agent
B. In the case (b3) both agents are ordering, because no data conﬂict appears (if
multiple assignments of the same data are allowed).
It should be noticed that the decision for using (a2) instead of (a1) would lead
to a slightly different behavior, but still the global sorting result is guaranteed. The
conﬂict situation (Fig. 13.10 a) could also be solved in other ways. E.g. it would
be possible that the agents on A and B swap their positions (at distance 2), thereby
ordering two or even three numbers. In addition, it would be possible that the agents
turn in certain situations, but the idea was to do without this feature in order to keep
the algorithm simple.
Here are simulations with a varying number of agents and different initial direc-
tions (< : to the left, > : to the right).
initial config:0
initial config:1
initial config:2
initial config:3
i=
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
t= 0
5> 4
3
2
1
0
5< 4
3
2
1
0
5> 4> 3> 2> 1> 0
5> 4> 3> 2> 1> 0<
t= 1
4
5> 3
2
1
0
0
4
3
2
1
5<
5> 4> 3> 2> 0
1>
5> 4> 3> 2> 0< 1>
t= 2
4
3
5> 2
1
0
0
4
3
2
1< 5
5> 4> 3> 0
2> 1>
5> 4> 3> 0< 2> 1>
t= 3
4
3
2
5> 1
0
0
4
3
1< 2
5
5> 4> 0
3> 2> 1>
5> 4> 0< 3> 2> 1>
t= 4
4
3
2
1
5> 0
0
4
1< 3
2
5
5> 0
4> 3> 2> 1>
5> 0< 4> 3> 2> 1>
t= 5
4
3
2
1
0
5>
0
1< 4
3
2
5
0
5> 4> 3> 2> 1>
0< 5> 4> 3> 2> 1>
t= 6
4> 3
2
1
0
5
0< 1
4
3
2
5
0> 5> 4> 3> 2> 1
0> 5> 4> 3> 2> 1<
t= 7
3
4> 2
1
0
5
0
1
4
3
2
5<
0> 5> 4> 3> 1
2>
0> 5> 4> 3> 1< 2>
t= 8
3
2
4> 1
0
5
0
1
4
3
2< 5
0> 5> 4> 1
3> 2>
0> 5> 4> 1< 3> 2>
t= 9
3
2
1
4> 0
5
0
1
4
2< 3
5
0> 5> 1
4> 3> 2>
0> 5> 1< 4> 3> 2>
t=10
3
2
1
0
4> 5
0
1
2< 4
3
5
0> 1
5> 4> 3> 2>
0> 1< 5> 4> 3> 2>
t=11
3
2
1
0
4
5>
0
1< 2
4
3
5
0
1> 5> 4> 3> 2>
0< 1> 5> 4> 3> 2>
t=12
3> 2
1
0
4
5
0< 1
2
4
3
5
0> 1> 5> 4> 3> 2
0> 1> 5> 4> 3> 2<
t=13
2
3> 1
0
4
5
0
1
2
4
3
5<
0> 1> 5> 4> 2
3>
0> 1> 5> 4> 2< 3>
t=14
2
1
3> 0
4
5
0
1
2
4
3< 5
0> 1> 5> 2
4> 3>
0> 1> 5> 2< 4> 3>
t=15
2
1
0
3> 4
5
0
1
2
3< 4
5
0> 1> 2
5> 4> 3>
0> 1> 2< 5> 4> 3>
t=16
2
1
0
3
4> 5
0> 1
2> 5> 4> 3>
0> 1< 2> 5> 4> 3>
t=17
2
1
0
3
4
5>
0
1> 2> 5> 4> 3>
0< 1> 2> 5> 4> 3>
t=18
2> 1
0
3
4
5
0> 1> 2> 5> 4> 3
0> 1> 2> 5> 4> 3<
t=19
1
2> 0
3
4
5
0> 1> 2> 5> 3
4>
0> 1> 2> 5> 3< 4>
t=20
1
0
2> 3
4
5
0> 1> 2> 3
5> 4>
0> 1> 2> 3< 5> 4>
t=21
1
0
2
3> 4
5
0> 1> 2
3> 5> 4>
0> 1> 2< 3> 5> 4>
t=22
1
0
2
3
4> 5
0> 1
2> 3> 5> 4>
0> 1< 2> 3> 5> 4>
t=23
1
0
2
3
4
5>
0
1> 2> 3> 5> 4>
0< 1> 2> 3> 5> 4>
t=24
1> 0
2
3
4
5
0> 1> 2> 3> 5> 4
0> 1> 2> 3> 5> 4<
t=25
0
1> 2
3
4
5
0> 1> 2> 3> 4
5>
0> 1> 2> 3> 4< 5>
initial config:4
initial config:5
initial config:7
initial config:8
i=
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
t= 0
5> 4< 3> 2< 1> 0<
5> 4> 3> 2
1
0
5> 4
3> 2
1> 0
5> 4
3< 2
1> 0
t= 1
4< 5> 2< 3> 0< 1>
5> 4> 2
3> 1
0
4
5> 2
3> 0
1>
4
5> 3< 2
0
1>
t= 2
1> 2< 5> 0< 3> 4<
5> 2
4> 1
3> 0
1> 2
5> 0
3> 4
1> 3< 5> 2
0
4
t= 3
1< 2> 0< 5> 3< 4>
2
5> 1
4> 0
3>
1
2> 0
5> 3
4>
1< 3> 2
5> 0
4
t= 4
1> 0< 2> 3< 5> 4<
2> 1
5> 0
4> 3
1> 0
2> 3
5> 4
1
2
3> 0
5> 4<
t= 5
0< 1> 2< 3> 4< 5>
1
2> 0
5> 3
4>
0
1> 2
3> 4
5
1
2
0
3> 4< 5>
t= 6
1> 0
2> 3
5> 4
1> 2
0
3< 4> 5
t= 7
0
1> 2
3> 4
5>
1
2> 0< 3
4
5>
t= 8
1> 0< 2> 3
4
5
t= 9
0< 1> 2
3> 4
5

13
Algorithms with Active Cells Modeled by Cellular Automata
291
The simulations (conﬁg: 2, 3) show that many agents cannot always solve the
task faster than one agent only.
(conﬁg: 2). For N = 6 cells, 5 agents with direction to right are used. As only
the leading agent is able to move and to exchange the numbers, this MAS is not
faster than the (conﬁg: 0)-system. Note that 6 agents, all initially with direction to
the right, are unable to solve the task because all of them are stuck.
(conﬁg: 3). 5 agents point to the right, and one to the left. In this case the agent
pointing to the left is moving to the left by swapping.
(conﬁg: 4). 6 agents are used with alternating directions. There are 3 swapping
situations, and 3 possible data exchanges. In fact, the global sorting behavior is
nearly the same as it is for the odd-even sort. Compared to odd-even sort, only
N/2 −1 steps are needed, because in every step N/2 comparisons are performed,
because even in the dot-situation two elements (with index 0 and N −1) are com-
pared, though in inverse order.
(conﬁg: 7). N/2 agents are used with the same direction and one empty cell be-
tween two successive ones. The global sorting behavior is the same as in (conﬁg: 4).
Still a question remains, namely how many agents at which initial positions and
with which initial directions are most effective in general? The precise answer re-
quires a deep formal analysis or an exhaustive evaluation, which cannot provided
here. Nevertherless we can draw some conclusions from the different simulation
cases. We can learn from (conﬁg: 5, 8), cases with N/2 agents, that an initially un-
skilled placement may lead to a longer execution time. One the other hand, a smart
initial conﬁguration may speed up the whole process. For example, if in the case
(conﬁg: 7) all the agent’s directions are changed from right to left, only 4 time steps
(N/2 −1 in general) are neded instead of 5.
It can easily be seen, that the a-algorithm works for any initial conﬁguration, ex-
cept for the one where the ring is fully packed with agents having the same direction.
In the case of asynchronous updating, the same a-algorithm (originally designed
for synchrounous updating) can also successfully perform the sorting task. The rea-
son is, that the algorithm then works sequentially at randomly changing positions
where only one agent is active at the same time. Because of the sequential updat-
ing, no conﬂict can appear. Therefore an agent may always perform the ordering.
But when using the designed synchronous a-algorithm, (i) the blocked agent in
(Fig. 13.9 a) does not perform ordering, and (ii) if the left agent has priority in
the conﬂict situation (Fig. 13.10 a1) and the right agent is active (selected by asyn-
chronous updating) then no movement and data ordering takes place (although pos-
sible). But these unsatisfying cases could be improved for asynchronous updating in
a simple way, namely in a way where an active agent compares and orders always,
and moves always if not blocked.

292
R. Hoffmann
Table 13.1 In the swapping situation, agents exchange their positions, and the states of the
agents are updated as deﬁned by the table
13.2.5
Leader Election
Initially p processes are in the same initial state (state = 0). The goal is to ﬁnd in a
decentralized way a terminal conﬁguration in which only one process is in the state
leader (state = 3) and the others are in the state lost (state = #).
Originally this problem was posed by Lelann [11], several solutions are discussed
in [13]. Here the problem shall be solved by agents which are moving around in a
ring. At the beginning there are p agents in state 0, and at the end, only one agent
shall be in state leader, and the others in state lost. We assume that agents in lost state
are still active and are always moving around. It would also be possible to deactivate
or delete such agents, then the algorithm, designed in the following, could easily be
modiﬁed.
Concerning the moving situations and reactions, the algorithm is similar to the
sorting a-algorithm presented before. The basic idea is to deﬁne one agent as lost
when two agents meet head-on (the swapping situation before). In order to force the
agents to meet, agents that perceive the marker on the ring, turn their direction.
There are four states an agent can take on:
state ∈{0,1,2,3,#} = {init, leftborder, rightborder, leader, lost}.
A   B
(a)  blocked
(b) blocked & turn   
(c)  moving
(d)  move & turn         
A    B
t
t+1
move
no operation
no operation
move&turn
move&turn
move
turn
turn
turn
turn
Fig. 13.11 Situations and solutions. (a) Agent A is blocked by B, it performs no action. (b)
Agent is blocked at the marker and turns back. (c) Agent moves. (d) Agent moves and turns
because it detects the marker.

13
Algorithms with Active Cells Modeled by Cellular Automata
293
(a)  conflict
(b)  swap
see table
A    B
see table
A    B
move
priority left
A    C     B
move
priority left
A    C     B
Fig. 13.12 Situations and solutions. (a) Conﬂict, solved by giving priority to the agent from
left. (b) Swap, agents states’ are updated according to Table. 13.1.
If an agent detects the left/right border (situations Fig. 13.11 b, d), it changes
into state 1/2. If an agent is already in state 1/2 (visited one of the borders) and
detects the other border (right/left border), it changes into state 3 (leader). When
two agents meet head-on they swap (Fig. 13.12 b), and Table 13.1 deﬁnes the new
states of the involved agents. For some cases the deﬁnitions are not mandatory, e.g.
(1,2) →(#,1) could be replaced by (1,2) →(2,#). It can be presumed that another
more simple solution can be derived from this one, by joining the two states 2, 3 into
one (representing that the agent has met any of the borders already. The presented
solution was guided by the idea that agents may change their direction in between
the borders, e.g. by random noise.
The following simulations show how the leader-election a-algorithm works.
initial config:0
initial config:4
initial config:10
initial config:12
i=
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
t= 0
0>
0> 0> 0>
0> 0< 0>
0<
0> 0> 0> 0> 0>
t= 1
0>
0> 0>
0>
#> 0>
0> 0<
0> 0> 0> 0>
2<
t= 2
0>
0>
0>
0>
#>
0> #< 0>
0> 0> 0>
0> 2<
t= 3
0>
0>
0>
2<
#> #< 0>
2<
0> 0>
0> 2< #<
t= 4
0>
0>
0> 2<
#< #>
0> 2<
0>
0> 2< #> #<
t= 5
2<
0> 2< #<
#>
#> 2< #<
0> 2< #> #< #<
t= 6
2<
2< #> #<
#>
2< #> #<
2< #> #< #> #<
t= 7
2<
2<
#< #<
#> 2< #< #<
3>
#< #> #< #<
t= 8
2<
2<
#<
#<
2< #> #< #<
t= 9
2<
3>
#<
#<
2<
#< #> #<
t=10
3>
3>
#<
#< #<
initial config:13
initial config:16
initial config:17
initial config:19
t= 0
0> 0> 0> 0> 0> 0<
0< 0< 0>
0< 0< 0> 0>
0> 0> 0> 0> 0> 0>
t= 1
0> 0> 0> 0> #< 2<
0<
0>
0<
1>
0< 0>
2<
0> 0> 0> 0> 0> 2<
t= 2
0> 0> 0> #< 0> 2<
1>
0> 0<
1> 0<
0> 2<
0> 0> 0> 0> 2< #<
t= 3
0> 0> #< 0> 2< #<
1>
#< 2<
#< 1>
2< #<
0> 0> 0> 2< #> #<
t= 4
0> #< 0> 2< #> #<
1> #<
2<
#>
1> 2< #<
0> 0> 2< #> #< #<
t= 5
#> 0> 2< #> #< #<
#< 1> 2<
#>
#< 1> #<
0> 2< #> #< #> #<
t= 6
#> 2< #> #< #> #<
#<
#< 1>
#> #< #< 3<
3> #> #< #> #< #<
t= 7
3> #> #< #> #< #<
#>
#<
3<
(conﬁg: 0). One agent is starting from the left border and keeps it initial state
until it reaches the right border. There the agent changes into state 2 and returns to
the left border where it changes into the leader state 3.
(conﬁg: 4). Three agents are moving to the right. The ﬁrst agent returns with state
2 and switches its followers into the lost state #. When reaching the left border, this
agent becomes the leader.
(conﬁg: 10). Four agents move in different directions. When agents meet head-
on, one more agent changes into the lost state. At t = 2 the rightmost agent perceives
the border and afterward moves left with state 2 until reaching the right border.

294
R. Hoffmann
(conﬁg: 12). The rightmost agent changes into state 2 and turns. When this agent
meets the other ones, they change into the lost state.
(conﬁg: 13). The system is completely ﬁlled with agents. Agent at i = 4 moves
to i = 5, turns and changes into state 2. Agent at i = 5 moves to i = 4 and changes
into state #.
(conﬁg: 19). The system is completely ﬁlled with agents. Agent at i = 5 is blocked,
therefore it turns and changes into state 2.
(conﬁg: 16). Agent at i = 1 and t = 1 moves to i = 0, turns and changes into state
1. At the end, this agent becomes the leader, winning against the other candidate in
state 2 coming from the right.
(conﬁg: 17). The initial conﬁguration is symmetric (agents at i = (1,2)/(3,4)
point to the left/right). Then, in the conﬂict and swapping situations, the agent from
the left gets priority and becomes the leader at last.
13.3
Summary
The CA-w (Cellular Automata with write access) model allows to modify not only
the cell’s own state but also the neighbor’s state. The neighbors are accessed via
dynamic links that can be modiﬁed by the cell’s rule. The classical CA model does
not allow to modify the state of a neighbor, therefore no write-conﬂict can occur.
However, for the description of applications with a varying number of active cells
and moving agents the CA-w model is a better choice because information can ac-
tively be transferred to a neighbor, and the activity of the neighbor can be switched
on or off. Compared to CA, the CA-w descriptions for such problems are more con-
cise and “natural” because the change of the system’s state is only controlled by the
active cells. Furthermore, the computational effort to simulate CA-w is minimized
because only active cells have to be computed, thereby minimizing the energy con-
sumption, too. It was shown that basic computational problems with dynamic ac-
tivities (trafﬁc rule, computing the binomial coefﬁcients and Fibonacci numbers,
sorting and leader election with moving agents) can easily be modeled by CA-w.
References
1. Achasova, S., Bandman, O., Markova, V., Piskunov, S.: Parallel Substitution Algorithms,
Theory and Applications. World Scientiﬁc (1994)
2. Haberman, N.: Parallel Neighbor Sort (or the Glory of the Induction Principle) CMU
Computer Science Report (available as Technical report AD-759 248, National Technical
Information Service, US Department of Commerce, 5285 Port Royal Rd Sprig ﬁeld VA
22151) (1972)
3. Hoffmann, R., V¨olkmann, K.-P., Waldschmidt, S.: Global Cellular Automata GCA: An
Universal Extension of the CA Model. In: Worsch, T. (ed.) ACRI Conf. Work in Progress
Session (2000)
4. Hoffmann, R., V¨olkmann, K.-P., Waldschmidt, S., Heenes, W.: GCA: Global Cellular
Automata. A Flexible Parallel Model. In: Malyshkin, V.E. (ed.) PaCT 2001. LNCS,
vol. 2127, pp. 66–73. Springer, Heidelberg (2001)

13
Algorithms with Active Cells Modeled by Cellular Automata
295
5. Hoffmann, R.: GCA-w Algorithms for Trafﬁc Simulation. Acta Physica Polonia. In:
Proc. Suppl. Summer Solistice Int. Conf. on Discr. Models of Complex Systems, Nancy
France. Polish Academy of Science, Cracow (2011)
6. Hoffmann, R.: The GCA-w Massively Parallel Model. In: Malyshkin, V. (ed.) PaCT
2009. LNCS, vol. 5698, pp. 194–206. Springer, Heidelberg (2009)
7. Hoffmann, R.: GCA-w: Global Cellular Automata with Write-Access. In: Acta Physica
Polonia Proc. Suppl. Summer Solistice Int. Conf. on Discr. Models of Complex Systems
Gdansk 2009. Polish Academy of Science, Cracow (2010)
8. Hoffmann, R.: Rotor-routing Algorithms Described by CA-w. In: Acta Physica Polonia
Proc. Suppl. Summer Solistice Int. Conf. on Discr. Models of Complex Systems Turku,
Finland 2011, Turku, Finland. Polish Academy of Science, Cracow (2012)
9. JaJa, J.: An Introduction to Parallel Algorithms. Addison-Wesley (1992)
10. Keller, J., Kessler, C., Tr¨aff, J.: Practical PRAM Programming. Wiley (2001)
11. LeLann, G.: Distributed Systems: towards a formal approach. In: Gilchrist, B. (ed.) Proc.
Information Processing, pp. 155–160. North Holland (1977)
12. Schweitzer, F.: Brownian Agents and Active Particles, Collective Dynamics in the Natu-
ral and Social Sciences. Springer Series in Synergetics. Springer (2003)
13. Tel, G.: Introduction to Distributed Algorithms. Cambridge University Press (1994)

Chapter 14
Broadcasting Automata and Patterns on Z2
Thomas Nickson and Igor Potapov
Abstract. The recently introduced Broadcasting Automata model draws inspira-
tion from a variety of sources such as Ad-Hoc radio networks, cellular automata,
neighbourhood sequences and nature, employing many of the same pattern forming
methods that can be seen in the superposition of waves and resonance. Algorithms
for the broadcasting automata model are in the same vain as those encountered in
distributed algorithms using a simple notion of waves, messages passed from au-
tomata to automata throughout the topology, to construct computations. The waves
generated by activating processes in a digital environment can be used for design-
ing a variety of wave algorithms. In this chapter we aim to study the geometrical
shapes of informational waves on integer grid generated in broadcasting automata
model as well as their potential use for metric approximation in a discrete space.
An exploration of the ability to vary the broadcasting radius of each node leads to
results of categorisations of digital discs, their form, composition, encodings and
generation. Results pertaining to the nodal patterns generated by arbitrary transmis-
sion radii on the plane are explored with a connection to broadcasting sequences and
approximation of discrete metrics of which results are given for the approximation
of astroids, a previously unachievable concave metric, through a novel application
of the aggregation of waves via a number of explored functions.
Thomas Nickson
The University of Edinburgh, Kennedy Tower,
Royal Edinburgh Hospital, Edinburgh EH10 5HF, UK
e-mail: tnickson@exseed.ed.ac.uk
Igor Potapov
University of Liverpool, Ashton Street, Ashton Building, Liverpool, L69 3BX, UK
e-mail: potapov@liverpool.ac.uk
c⃝Springer International Publishing Switzerland 2015
297
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_14

298
T. Nickson and I. Potapov
14.1
Introduction
The recently introduced model of Broadcasting Automata [19] connects many of
the techniques contained in distributed algorithms, ad-hoc radio networks, cellular
automata and neighbourhood sequences. Much like cellular automata with variably
deﬁned neighbourhoods Broadcasting Automata can be deﬁned on some form of
grid or lattice structure and have a simple computational primitives comparative
to a ﬁnite state automata with the ability to receive and send messages both from
and to those automata which are within its transmission radius. Neighbourhoods are
deﬁned in the same way as with an ad-hoc network, all those points within a certain
transmission radius, receive the message from the sender.
Algorithms for the broadcasting automata model are in the same vein as those
encountered in distributed algorithms using a simple notion of waves, messages
passed from automata to automata throughout the topology, to construct computa-
tions [20]. Wave algorithms are enhanced further with notions of composition of the
information that is carried within each wave borrowed from the physical world and
embellished with the new found computational power of the automata.
The waves generated by activating processes in a digital environment can be used
for designing a variety of wave algorithms. In fact, even very simple ﬁnite functions
for the transformation and analysis of passing information provides more complex
dynamics than classical wave effects. In [19, 20] we generalized the notion of the
standing wave which is a powerful tool for partitioning a cluster of robots on a non-
oriented grid. In contrast to classical waves where interference patterns generate
nodal lines (i.e. lines formed by points with constant values), an automata network
can have more complex patterns which are generated by periodic sequences of states
in time.
Here we take a different direction and aim to study the geometrical shapes of
informational waves on the integer grid generated in broadcasting automata model
as well as their potential use for metric approximation in a discrete space. The re-
peated transmission to nodes, within certain radii, applied to a certain network topol-
ogy, or physical layout of nodes has been studied under the name, Neighbourhood
Sequences (NS). The concept of neighbourhood sequences is of importance in a
number of practical applications and was originally applied for measuring distances
in a digital world [13]. Initially two classical digital motions (cityblock and chess-
board)1 were introduced. Based on these two types of motions periodic neighbour-
hood sequences were deﬁned in [3] by allowing arbitrary mixture of cityblock and
chessboard motions. In 2D the distances based on cityblock and chessboard neigh-
bourhood sequences deviate quite substantially from the ideal Euclidian distances,
so instead their combination that form “the octagon” was more often employed and
studied. Later the concept of neighbourhood sequences was extended to arbitrary ﬁ-
nite and inﬁnite dimensions, periodic and non-periodic sequences and then analysed
in terms of their geometric properties.
1 The cityblock motion allows movements only in horizontal and vertical directions, while
the chessboard allows to move in diagonal directions.

14
Broadcasting Automata and Patterns on Z2
299
The aggregation of two classical neighbourhood sequences based on Moore and
Von Neumann neighbourhoods(which correspond to cityblock and chessboard) was
recently proposed as an alternative method for self-organization, partitioning and
pattern formation on the non-oriented grid environment in [19]. In particular the
discrete analogs of physical standing wave phenomena were proposed to gener-
ate nodal patterns in the discrete environment by two neighbourhood sequences.
The power of the primitives was illustrated by giving distributed algorithms for the
problem of ﬁnding the centre of a digital disk of broadcasting automata. The shapes
that can be formed by neighbourhood sequences in dimension two are quite limited.
However the basic notion of neighbourhood sequences can be naturally extended
by relaxing the constraints on the initial deﬁnition of the neighbourhood in such a
way that two points are neighbors (r-neighbours) if the Euclidean distance is less
than or equal to some r, used to denote the radius of a circle. Then by Broadcast-
ing sequences we understand the periodic application of the r-neighbours distance
function.
The main result of this work is characterization of geometrical shapes that can be
generated by Broadcasting Sequences on the square lattice. The shapes of r- neigh-
borhoods correspond to Discrete Discs which are discrete convex polygons. First
we use the language of Chain Codes (i.e. the code based on 8 degrees of motion) to
describe the shape of the Discrete Discs and their Broadcasting sequences. In partic-
ular we introduce the notion of Chain code segments and Line segments to express
the shapes of the polygons corresponding to Discrete Discs. Then we characterize
the shapes of polygons produced by Broadcasting sequences and provide a linear
time algorithm for the composition of two chain codes of Discrete Discs. Based on
their composition properties we derive a number of limitations for produced poly-
gons. For example we show that there exist an inﬁnite number of gradients (of line
segments in the polygons) that cannot be produced by Broadcasting Sequences. It
also becomes clear that the set of line segments, and as such gradients, that compose
any discrete circle are closed under composition.
Moreover, we provide an alternative method for enriching the set of geometri-
cal shapes and neighbourhood sequences by aggregation of two Broadcasting Se-
quences. Initially we illustrate the idea on Moire and Anti-Moire aggregation func-
tion to produce an inﬁnite family of polygons and polygonal shapes and characterize
the gradients of their line segments. We have noticed that Anti-Moire aggregation
function can be slightly modiﬁed to provide better approximation for the Euclidean
distance on a square lattice then classical neighbourhood sequences. Finally it is
possible to observe the variety of effects that are the result of the application of an
aggregation function which are themselves shapes of some form.
14.2
Broadcasting Automata Model
One of the fundamental models of computation is the automaton. Finite state au-
tomata take as input a word, which may in some instances be referred to as a tape,
and output is limited to an accepting state which the automata is left in if it is said

300
T. Nickson and I. Potapov
to accept the word. Traditionally automata are used in cases where it is important to
transform some input in to an output, beyond the use of a single state, is the use of
Moore Machines. Such machines differ from ﬁnite state machines in that they are
able to produce an output word from an input word.
Deﬁnition 1. A Moore machine [4] is a 6-tuple, A = (Q,Σ,Λ,δ,Δ,q0) , where:
• Q is a ﬁnite set of states,
• Σ is the set of input symbols,
• Λ the set of output symbols,
• δ : Q× Σ →Q is the transition function mapping a state q ∈Q and a symbol, or
set of symbols, σ ∈Σ to a state q ∈Q,
• Δ : Q →Λ is the output function which maps a state, q ∈Q, to an output symbol,
λ ∈Λ, and
• q0 is the initial or quiescent state in which the automata starts.
It is assumed that such a machine is connected to an input tape, or word, and an
output tape, or word, where the result of the computation is read. In a situation, as
is presented in distributed systems, whereby automata are connected to each other it
is possible for the output of one automata to become the input of another automata.
Such a model is known as a network of automata and connections from one au-
tomatons output to another’s input may be represented as a directed graph, where
direction represents the output going to input from automaton to automaton.
Deﬁnition 2. A network of ﬁnite automata is a triple, (G,A,C0) , where:
• G = (V,E) is a directed graph, with vertices, V, and edges, E, which are ordered
pairs of vertices,
• A, is a Moore machine, and
• C0 is an initial conﬁguration which maps states, Q, of the automata, A, to vertices,
V, such that, C0 : V →Q.
In this model the topology is ﬁxed as speciﬁed by the construction of the graph,
G, which dictates the ﬂow of inputs and outputs from the Moore machines. Such
symbols, where a symbol is part of the input/output word of the Moore machine,
are generated as response to some input, by an automaton at vertex, v ∈V, and then
sent to all of the adjacent vertices in the graph or to a particular adjacent vertex, G,
where the automata that receive the symbols process them as they would their input
word.
A less abstract model is considered by adding more details about the communi-
cation between automata, where they are located in Euclidean space and what can
be transmitted. It will also be required to deﬁne more reﬁned models that will reﬂect
the new features and constraints of the physical environment, but are still at a high
level of abstraction.
Taking the inspiration from ad hoc wireless communication networks we intro-
duced in [19] a new model of Broadcasting Automata, which can be seen as a net-
work of ﬁnite automata with a dynamic network topology. Informally speaking,

14
Broadcasting Automata and Patterns on Z2
301
the model of Broadcasting Automata comprises nodes, which correspond to points
in space, and connectivity between nodes depends upon the distances between the
points and the strengths of the transmissions generated by the automata as may be
seen in ad-hoc radio networks. Transmission strength may vary from round to round
for any particular automaton in the space and is dictated by the state of the automa-
ton. In this model the topology, or connectivity graph, of the network of automata is
able to change at each time step based on the states of the automata.
In order to give a formal deﬁnition of the Broadcasting Automata model on a
metric space it is ﬁrst necessary to introduce the notion of a metric space and to
modify the classical notion of the Moore machine.
Deﬁnition 3. A metric space is an ordered pair, (M,d), where M is a set and d is a
metric on M such that d : M × M →R.
Here, (M,d), is any metric space, later the two dimensional euclidean space shall
be used, but this is not a necessity simply that a notion of the distance between two
points is required.
Deﬁnition 4. The Broadcasting Automaton extends the Moore Machine by intro-
ducing a set of ﬁnal states, F, along with a function, τ : Q →R, which maps the
state to a real number and represents the radius of transmission for the output sym-
bol and the output alphabet, Λ, is extended by adding an empty symbol, ε and it is
represented by an 8-tuple A = (Q,Σ,Λ,δ,Δ,τ,q0,F).
A network of Broadcasting Automata, which will also be referred to as the Broad-
casting Automata model, can now be deﬁned.
Deﬁnition 5. The Broadcasting Automata model is represented by a triple BA =
((M,d),A,C0) where:
• (M,d) is a metric space,
• A is a Broadcasting Automata, A = (Q,Σ,Λ,δ,Δ,τ,q0,F),
• C0 is the initial conﬁguration of the Broadcasting Automata model, it is a map-
ping from points, M, to states of the Broadcasting Automata, Q, such that, C0 :
M →Q.
In some cases it may be that the input and output symbols are drawn from the
same alphabet in which case, Σ = Λ.
The communication between automata is organised by message passing, where
messages are symbols from the output alphabet, Λ, of the automata, A, to all of
the automata within its transmission radius. Messages, symbols from the output
alphabet, Λ, of the automata, A, are generated and passed instantaneously at discrete
time steps, generation of message is given by the function, Δ, for the automata, A,
resulting in synchronous steps. Those automata that have received a message, for
the ﬁrst time in the computation, are said to be activated.
If several messages are transmitted to an automaton, A, it will receive only a
set of unique messages, i.e. for any multiset of transmitting messages, where the

302
T. Nickson and I. Potapov
multiset represents a number of the same message being sent, received by A, over
some number of rounds, the information about quantity of each type, within a single
round, will be lost. This simply illustrates that the automaton may not dictate a
number of the same symbol in any transition, all transitions must operate upon a set
of distinct symbols.
More formally the concept of computation and communication is captured in the
concept of conﬁgurations of the Broadcasting Automata model and its semantics
presented here.
Deﬁnition 6. The conﬁguration of the Broadcasting Automata is given by the map-
ping, c : M →Q, from points in the metric space to states. It can be noted here that
C0 is an initial conﬁguration for the Broadcasting Automata model.
Automata are updated, from conﬁguration to conﬁguration, synchronously at dis-
crete time steps. The next state of each automaton depends upon the states of all
other automata which have in their neighbourhood the automaton that is to be up-
dated. Where here the neighbourhoodsare formed by a combination of their position
in M and the range of their transmission, dictated by τ(q) for all automata.
Deﬁnition 7. The set of messages received by the automaton at a point, u,v ∈M, is
expressed by the set Γu = {Δ(c(v))|v ∈M ∧d(u,v) ≤τ(c(v))} for the metric space,
(M,d).
In Figure 14.1, it is possible to see an elucidation of message passing in the
Broadcasting Automata model through the process of constructing a digraph rep-
resentation of the broadcasting radii. The ﬁgure depicts the automatons broadcast-
ing radius by way of a dashed circle with the automaton broadcasting at that range
shown in the centre. The corresponding graph may be constructed from this by mak-
ing a directed edge in the graph from the node that is broadcasting to all nodes that
are within the broadcast range, where the range is dictated by the state, q ∈Q and
the function, τ(q) = r, where r is the broadcasting radius. The construction of such
a graph shows both the connection to the network automata model, which operates
on a similar premise with a ﬁxed graph, but also highlights how Deﬁnition 7 is con-
structed. Incoming edges are generated by connecting nodes, with the arrow from
transmitter to node, reachable from the transmitter. Naturally such a graph, in the
Broadcasting Automata model, can change over time which is shown by images
further down the page as increase time steps in Figure 14.1.
It should be noted that this set of messages can be empty. The new state of the
automaton at u is now given by applying the automaton’s transition function δ u(Γu).
This may be applied to all of the automata in the BA and as such determines the
global dynamics of the system. It can be seen that in one time step it is possible for
conﬁguration c to become conﬁguration e where, for all v ∈M, e(v) = δ v(Γu). Such
a function is called the global transition function. The set of all conﬁgurations for
BA is denoted as C.
Deﬁnition 8. The global transition function, G : C →C, represents the transition of
the system at discrete time steps such that for two conﬁgurations, c ∈C and e ∈C,
where e is the conﬁguration the time step directly after c such that, e = G(c).

14
Broadcasting Automata and Patterns on Z2
303
Fig. 14.1 The above ﬁgures show the possible evolution over time of a network of Broadcast-
ing Automata. Each ﬁgure shows, on the left, the broadcast range for the automata, depicted
by a dotted circle with the node at the centre, on the right, the same connectivity is shown in
graph form.
Deﬁnition 9. A computation is deﬁned as any series of applications of the function,
G, from an initial condition, C0, that leads to the set of all automata in the system
being in one of their accepting states from the set, F, or the computation halts such
that there exists no deﬁned relation from the current conﬁguration, c, under the
global transition function G(c).
Here a notion of reachability is given.
Deﬁnition 10. One conﬁguration, c, is said to be reachable from another, e, if from
the initial conﬁguration,C0, there exists a series of valid intermediary conﬁgurations
such that c →c′ →c′′... →e. Where c →c′ is equivalent to G(c) = c′. It can also
be stated, in a shorthand way, that where the sequence c →c′ →c′′... →e exists
reachability from c to e may be shown as c ; e.
In the above context the time complexity of an algorithm in the Broadcasting
Automata model is determined by the number of applications of the global transition
function during the computation.
Whilst in general Broadcasting Automata may be used represent any of the com-
mon network topologies (such as, linear array, ring, star, tree, near-neighbour mesh,
systolic array, completely connected, chordal ring, 3-cubes and hyper-cubes [5,8]),

304
T. Nickson and I. Potapov
through varying the location and transmission radii of the automata in the space, a
different model is employed. The model used throughout the paper is a hybrid, mix-
ing the notion of transmission radius in the pathloss geometric random graph model
and common network topologies. The method involves the physical placement of
nodes on the Euclidean plane in a lattice structure such that the distances between
points conform to the euclidean distance. Such regular lattices coincide with those
structures that can be formed by previous works [16,17,27] giving the basis of the
forms of lattice that can be studied. This paper will strictly concern itself with the
square grid lattice though it can be extended in to any of the other grid conﬁgura-
tions such as the hexagonal or triangular lattices along with many others.
The locations of the points on the lattice allow the construction of the metric
space, (M,d) as such the locations of the automata. Assuming a lattice structure
for positioning of the automata and transmission radii, R, is in accordance with the
principles of transmission used in the pathloss geometric random graph model put
forth in Ad-Hoc networks.
A variety of topologies, the lattices that can be constructed as in [16] (square,
hexagon and triangle), and the shapes of neighbourhoods generated by varying the
transmission neighbourhood can be seen in Figure 14.2 where black dots represent
the placement of automata on the plane according to the lattice here shown by the
black lines. Neighbours are those within the dashed circle and the automata at the
centre of the circle is the initial transmitter of any messages.
Fig. 14.2 Showing three differing lattices and how their neighbourhoods can be altered sim-
ply by varying the radii of the transmission neighbourhood which, here, is depicted as a dotted
circle. All automata, shown as black dots, are able to receive messages from the automata at
the centre of the circle. The three lattices depicted are (left to right) square, hexagon and
triangle.
The connectivity graph deﬁned in ad-hoc radio networks denotes those that are
able to send and receive messages with certainty however in distributed algorithms
the possibility of communication errors is not ignored. In such models messages
may be lost, duplicated, reordered or garbled where they must be detected and cor-
rected by supplementary mechanisms which are mostly referred to as protocols. For
example the common network protocol of TCP. The Broadcasting Automata model
sides with the former method of message passing. It is assumed that there will be
no error in transmission, the receipt or sending of messages across the network is

14
Broadcasting Automata and Patterns on Z2
305
guaranteed, however, consideration is made to the synchronicity of the network due
to the high sensitivity to the timing of message passing as will become clear later
when discussing pattern formation with such protocols.
To this extent we consider two variants of Broadcasting Automata: synchronous
and asynchronous (or reactive) models. In each of the variations both the transmis-
sion of a method and amount of time take for the automata to process the message
is considered to be constant. It is this constant time that will afford synchronisation
within the model. The two differing models are presented to show that there are pos-
sible complexity trade offs depending on how it is to be measured. If the size of the
alphabet is a priority, such that it must be minimised, then the synchronous model is
best, however this comes at a penalty of time complexity, where the asynchronous
model fairs better.
In the asynchronous model upon the receipt of a message, an input symbol (or
set of symbols) σ ∈Σ, the automaton, A, becomes active. The automaton may only
react to a non-empty set of messages, it may not make and epsilon transition. Once
activated the automaton, A, reacts to the symbol(s), σ, according to the conﬁguration
of the automaton and responds with the transmission of an output symbol, λ ∈Λ,
to those in its transmission radius for the current state, q ∈Q, τ(q). The processing
of the input symbols by the automaton is done in one discrete time step which is the
same period for all automata in the graph. Once active the automaton must wait for
another message in order to change its state it is not allowed epsilon transitions.
The synchronous model has a singular alphabet and as such, Σ = Λ with the
allowance of epsilon transitions. Upon the receipt of a message, an input symbol (or
set of symbols) σ ∈Σ, the automaton, A, becomes active. Once active the automa-
ton is synchronised with the rest of the graph by repeated epsilon transitions that
are representative of a constant transition via input symbol as in the asynchronous
model. As each such transition is considered a change in conﬁguration of the au-
tomaton it must take a single time step. As such this guarantees the synchronisation
of system. However the multiple alphabet can be simulated by associating different
symbols to different time steps as shall be seen later.
The following outlines, informally, the methodology of message passing used by
the two differing constructions of automata, synchronous and asynchronous.
In the synchronous model messages are passed from automaton to automaton ac-
cording to the following rules:
(1) An automaton, A, receives a message from an activating source at time, t;
(2) At time t + 1, A sends a message to all automata within its transmission radius
dependent on its state, q ∈Q, τ(q);
(3) At time t + 2, A ignores all incoming messages for this round.
In the asynchronous model the following rules can be applied:
(1) An automaton, A, receives a message σi ∈Σ from an activating source at time t;
(2) The automaton, A, broadcasts a message σ(i+1) mod |Σ| to all automata within
its transmission radius, dependent upon its state, q ∈Q, τ(q) at time t + 1;
(3) Ignore all incoming messages at time t + 2.

306
T. Nickson and I. Potapov
In both models Step 3 prevents an automaton from receiving back the message
that has just been passed to the automatons neighbours by ignoring all transmissions
received the round after transmission and ensuring that the messages are always
carried away from the initial source of transmission. In both cases this rejection of
all messages may be modelled by the addition of a state that simply does not accept
input in that state, where epsilon transitions apply, or that the transition is made to
another equivalent state independent of input received, where from here it is possible
to receive transmissions that once again affect the logic of the program.
Naturally, given these two models it is interesting to see if they are capable of
the same computations and that indeed anything that can be done in one model
can be done in the other. There are many ways with which to establish equivalence
between automata and models in general. One important technique that has been
used to show equivalence in connection with Turing machines is simulation [18].
Here a similar proposition is made with respects to establishing the equivalence of
the two models of automata that are given here, synchronous and asynchronous.
Proposition 1. [20] Both the synchronous and asynchronous models are able to
simulate the other.
It should also be noted that with some generalisation, to allow a variable radius
neighbourhood interaction which has been explored in a limited sense [11,29], the
Cellular Automata (CA) model may also be used to simulate the Broadcasting Au-
tomata model. In a grid of CA it is possible to assign states that correlate to trans-
mitters where each transmitter in the grid is assigned a unique state and all other CA
are in the quiescent state. The initial transmitters state causes each of the automata
who have the initial transmitter in their neighbourhood to change from the quiescent
state to the transmitters state plus one over some modulo. This clearly simulates the
transmission of some word over a modulo for all automata in the grid. Upon ﬁnding
a CA with one of this initial set of states within its neighbourhood, and such that
it is all encompassing in its neighbourhood, the second transmitter changes its state
which triggers a second cascade of state changes that are equivalent to those from
the ﬁrst transmitter only this time the automata must take in to account the initial
state that it is already in. This should provide an exact copy of the Broadcasting Au-
tomata model, assuming that CA is extended to allow such variable neighbourhoods.
This does not mean however that there is an exact translation of Cellular Automata
in to Broadcasting Automata and as such it is only possible to say that BA ⊆CA.
14.3
Variable Radius Broadcasting over Z2
Whilst the model provided here covers many conﬁgurations for the underlying com-
munication graph this paper is mainly considering the following case, where all
automata are place in euclidean space according to a regular arrangement equiv-
alent to that of a square lattice such that, using the Cartesian coordinate system,
for any automata in the space its nearest neighbour is at distance 1 and its next
nearest neighbour is at distance
√
2. This idea is illustrated for two dimensions in

14
Broadcasting Automata and Patterns on Z2
307
Figure 14.3. This now leads to the model of Broadcasting Automata where (M,d)
is M = Z × Z and where for ρ = (ρ0,rho1) and ρ′ = (ρ′
0,ρ′
1) the distance func-
tion is d(ρ,ρ′) =
"
(ρ0 −ρ′
0)2 + (ρ0 −ρ′
0)2, the Euclidean distance function for
ρ,ρ′ ∈M.
Fig.
14.3 A
variety
of
transmission
radii
are
shown
(l-r)
squared
radii
r2 =
{1,2,4,5,8,9,10,13,16}. Crosses represent the centre of the respective discrete disc.
Figure 14.4 shows as example the automata that receive a message when the
Euclidean space and square lattice are restricted to two dimensions but the radius
of broadcast, τ(q) for q ∈Q, is varied. The automaton at point ρ ∈M which is
the source of the transmission is shown as the circle at the centre of the surround-
ing automata on the plane, those that are black are within the transmission range
τ(q) for q ∈Q. Successive larger collections of automata coloured black show what
happens when range can be changed to alter the automata that are included in the
transmission radius, τ(q) ∈R. If the transmission radius, R, is equal to 1, as in
Figure 14.4 diagram a) then only four of the eight automata can be reached. If the
radius is made slightly larger and is equal to
√
2, it can encompass all eight au-
tomata in its neighbourhood as shown in diagram b). Such structures are identical
to the well studied neighbourhoods von Neumann and Moore (i.e. chess board and
city block) respectively and as such here it shall be considered that such construc-
tions of neighbourhoods are a generalisation of these two neighbourhoods. As we
will show later, iterative broadcasting within von Neumann and Moore neighbour-
hoods can distribute messages in the form of a diamond wave and a square wave as
shown later in Figure 14.5.
a)
b)
Fig. 14.4 Diagram a) represents the propagation pattern for a diamond wave (Von Neumann
neighbourhood) and diagram b) shows the propagation pattern for a square wave (Moore
neighbourhood).
The construction of distinct radii for the circles is deﬁned by the numbers n such
that n = x2 + y2 has a solution in non-negative integers x, y [26]. Here it can be

308
T. Nickson and I. Potapov
seen that r2 is given for convenience as n always has a convenient representation
in Z whereas it becomes cumbersome to write out either the root of the integer or
its decimal representation. For an explanation as to how these numbers relate to
the distinct discrete discs it must be noted that distinct discs are constructed such
that r2 contains a new solution in x and y. As it can be considered that the digital
disc represents all of the maximal combinations of Pythagorean triples such that
x2 + y2 ≤r2 then for one disc to differ from another there must be an increase of
r2 such that there exists a new, distinct, maximal solution for x,y ∈Z. Generating
these in the inverse direction by choosing x,y ∈Z such that it is a new maximal
combination not contained in any of the previous, smaller discrete discs allows the
construction of the list of r2 that deﬁnes the sequence of distinct discrete discs.
14.4
Neighbourhood and Broadcasting Sequences
The idea of propagating a pattern of square (r2 = 2) or diamond waves (r2 = 1), gen-
erated by repeated application of Moore or Von Neumann waves respectively, in di-
mension two are commonly known as Neighbourhood Sequences (see Figure 14.5).
Neighbourhood Sequences are an abstraction used to study certain discrete distance
metrics that are generated upon the repeated application of certain neighbourhoods
to a lattice for example the Moore neighbourhood to a square lattice. As previously
discussed such neighbourhoods as von Neumann and Moore are encompassed by
the more general model used here whereby the transmission radius is varied on a
square lattice the ﬁrst two of such transmission radii to produce distinct objects
equating to those of the von Neumann and Moore neighbourhoods.
Fig. 14.5 Wave propagation with Neighbourhood Sequences on the square grid: Moore
neighbourhood (left) and Von Neumann neighbourhood (right).
Deﬁnitions and notation concerning neighbourhood sequences as considered in
[6], and many other works, are now given here for completeness.
Let p ∈Zn where n ∈N and such that the ith coordinate of p is given by Pri(p)
for 1 ≤i ≤n.

14
Broadcasting Automata and Patterns on Z2
309
Deﬁnition 11. For M ∈Z where 0 ≤M ≤n the points p,q ∈Zn are M−Neighbours
when the following two conditions hold:
• |Pri(p)−Pri(q)| ≤1 for (1 ≤i ≤n)
• ∑n
i=1 |Pri(p)−Pri(q)| ≤M
An n-dimensional neighbourhood sequence is denoted A = (a(i))∞
i=1, ∀i ∈N
where a(i) ∈1,...,n denotes an M-neighbourhood by its value of M such that for
a(1) denotes that the next neighbourhood set of points in the sequence is those that
differ by at most one coordinate as given by Deﬁnition 11 where M = 1 in this
case the Von Neumann neighbourhood. If A is periodic then ∃l ∈N, a(i+ l) = a(i)
(i ∈N). Such periodic sequences are given as A = (a(1),a(2),...,a(l)).
Deﬁnition 12. The A-distance, d(p,q;A), of p and q is the length of the shortest
A-path(s) between them.
As the spreading of such neighbourhoods is translation invariant only an initial
point of the origin need be considered w.l.o.g.
Deﬁnition 13. The region occupied after k applications of the neighbourhood se-
quence A is denoted as Ak = {p ∈Zn : d(0, p;A) ≤k} for k ∈N.
Also, let H(Ak) be the convex hull, given in Deﬁnition 14, of Ak in Zn.
Deﬁnition 14. A convex hull is the smallest set of points that form a convex set as
given in Deﬁnition 15.
Deﬁnition 15. Convex set. A set C ⊂Rd is convex if for every two points x,y ∈C
the whole segment xy is also contained in C. In other words, for every t ∈[0,1], the
point tx+ (1 −t)y belongs to C. [21]
Discrete discs are formed by the Broadcasting Automata as they are arranged on
the plane at integral Cartesian coordinates, (Z × Z) ∈M, and as such a broadcast
to all automata in range, for point v ∈M, τ(c(v)) = r will cause a change in state
to all those automata that form a discrete disc of radius r from the point v. In the
broadcasting automata model it is assumed that there is no restriction on the radius
of transmission where it is considered that as the Von Neumann and Moore neigh-
bourhoods can be described as the ﬁrst two radii in the set of distinct discrete discs,
r2 = 1 and r2 = 2. This means that discrete discs are quite a natural extension to
the basic notion of neighbourhood sequences merely relaxing the constraints on the
initial deﬁnition of M −neighbours in the following way.
Deﬁnition 16. Two points p,q ∈Zn are r −neighbours if the Euclidean distance,
d(p,q) =
"
∑n
i=0(qi −pi)2, is less than some r, used to denote the radius of a circle,
such that d(p,q) ≤r.

310
T. Nickson and I. Potapov
Utilising the framework supplied by the work done on neighbourhood sequences
it is now possible to outline Broadcasting sequences which denote any sequence
of radii, r, such that R = (r1,r2,...,rl). Labelling those points that are reachable by
some application, Rk, of the neighbourhood sequence is another extension to the
notation. All points such that p ∈R1 are labelled 0, all points p ∈R2\R1 are labelled
as 1. More generally labels will be assigned b where k ≡b mod m and m ∈N. The
work conducted here will also be restricted to the study of Z2.
Many of the questions asked and, indeed, answered, in neighbourhoodsequences,
shall be useful in the exposition of broadcasting automata. These include whether
a certain sequence of neighbourhoods are metrical or not [23]. Indeed when using
a general form of broadcasting automata whereby alternation of the broadcasting
radius is allowed at each step as suggested here. Considering that all notions of the
construction of algorithms presented here in the broadcasting automata model rely
on the distance from the transmitter to the node, being able to assure that this dis-
tance will be consistent for all automata is essential. Also studied in neighbourhood
sequences is the possible resultant shapes which are categorised and examined for
their various properties such as their use in approximation of euclidean distances
where the isoperimetric ratio is used to compare the sequences based on the shapes
that they form on the plane [7]. This same method is used to again show an esti-
mation for euclidean distance as well as estimations for certain Lp distances, here
the astroid which will be presented in Section 14.10. The characterisation of the
shapes generated by these neighbourhood sequences yield results about the shapes
of compositions of such discrete discs on the plane and the partitions that such com-
positions form.
14.5
Geometrical Properties of Discrete Circles
The characterization of broadcasting sequences on Z2 is closely related to the study
of discrete circles on the square lattice and their discrete representation. One of the
efﬁcient methods to describe the discrete circle is a chain coding. The method of
chain coding was ﬁrst described in [10] as a way in which to encode arbitrary geo-
metric conﬁgurations where they were initially used, as they shall be here, to facili-
tate their analysis and manipulation through computational means. At the time there
were many questions about the encoding of shapes and the methodologies which
should be used to encode such shapes and chain coding presented an schema which
was simple, highly standardised and universally applicable to all continuous curves.
Whilst the deﬁnition given here differs from the deﬁnition given in [10] it only dif-
fers for convenience when discussing discrete discs in the ﬁrst octant allowing the
use of only the numeric symbols 0 and 1 instead of 0 and 7 as Freeman suggested.
This gives the following deﬁnition of chain coding that shall be used throughout.
Deﬁnition 17. From a starting point on the square lattice a chain code is a word
from the alphabet {0,1,2,3,4,5,6,7}.

14
Broadcasting Automata and Patterns on Z2
311
1
3
4
5
7
6
2
0
5
7
0
1
2
3
4
6
Fig. 14.6 (Left) Showing the eight possibilities of motion from the single point of origin,
circled, as they may be traced out on the plane (Centre) Shows the eight possible points of
motion from a central point and (Right) gives and example of an arbitrary line traced out on
the lattice with a chain code of form 70102435 originating from the circled point.
From some starting point chain codes may be used to reconstruct a shape, S, by
translating the code’s motion on to the lattice where 0 indicates positive movement
along the x −axis with increasing values moving clockwise through all 8 possible
degrees of motion on the lattice, labelled from 0 to 7 respectively. An integral shape,
S, can be encoded using an inverse method whereby the shape is traced from some
starting point and the motion from one point to the next connected point in the shape
is recorded as a chain code. For example, a straight line would be represented by the
inﬁnite repetition of a single digit such as 00000000000000..., a line that satisﬁes
the equation, x −y = 0, may be encoded 77777777... or as the example given in
Figure 14.6 and arbitrary line may be encoded numerically as 7010235.
The discrete disc is the basic descriptor of a transmission in broadcasting au-
tomata. It represents the total set of all automata that could be reached on the square
lattice after a single broadcast of radius, r2. In the following sections reasoning will
follow only from the ﬁrst octant, which can be seen in Figure 14.7, of the discrete
circle which shall now be deﬁned and gives all the information that is required to
categorise the discrete disc, its perimeter, but also has a convenient method of chain
code construction. The need only to observe the ﬁrst octant is derived from the sym-
metry that occurs such that it is possible to take a solution of the ﬁrst octant and by
successive reﬂection operations about the eight octants generate a full circle [2] un-
der the assumption, which is also made here, that the circles are centred on a integral
point on the plane.
Deﬁnition 18. A discrete circle is composed of points in the Z2 set ζ = {(x,y)|x2 +
y2 ≤r2, x,y ∈Z, r ∈R} that have in their Moore neighbourhood any point in the
complement of ζ.
In Deﬁnition 18 it is shown that r ∈R however many of these do not produce
distinct discrete circles. The set of r that includes all distinct circles are those which
form Pythagorean triples which consist of two positive integers a,b ∈N, such that
a2 + b2 = r2.
A simple way to draw the discrete circle is to divide it into octants as can be
seen in Figure 14.8. First calculate only one octant of the circle and as the rest
of the circle is ”mirrored” from the ﬁrst octant it can now be successively mir-
rored to allow the construction of the other sections of the circle. Let us divide the

312
T. Nickson and I. Potapov
1
2
3
6
7
8
5
4
Fig. 14.7 An illustration of a circle that has been divided in to eight octants with the labelling
of those octants applied accordingly.
l1
l2
l0
Fig. 14.8 An illustration of a discrete disc and its accompanying discrete circle of radius
squared 116, where the chain code for the ﬁrst octant is labelled as l0l1l2.
circle into eight octants labelled 1-8 clockwise from the y-axis. The notation
Octant(i) is used to refer to octant sector i. If an algorithm can generate the dis-
crete circle segment on one octant, it can be used to generate other octants by using
some simple transformations, such as rotation by 90◦and reﬂection around the x-
axis, y-axis, and the +45◦diagonal lines.
Some basic results are now given about chain codes of such discrete circles. It
is possible to show that in the ﬁrst octant and given the deﬁnition of chain coding
presented here only two digits, 0 and 1 are required to fully represent the discrete
circle.

14
Broadcasting Automata and Patterns on Z2
313
Lemma 1. A Chain coding w for the circle in the ﬁrst octant is a word in {0,1}∗.
Proof. First let us show that in the ﬁrst octant for any given point p0 = (x0,y0) on
the circle at the point p1 = (x0 + 1,y0 −1) must also be in the circle as |p1| < |p0|.
When p0 is in the circle and in the ﬁrst octant segment such that x < y it follows
that the magnitude of p1 is less than p0 as |p0| −|p1| = (x2
0 + y2
0) −(x2
1 + y2
1) =
(x2
0 −x2
1) + (y2
0 −y2
1) = (x2
0 −(x0 + 1)2) + (y2
0 −(y0 −1)2) = (x2
0 −x2
0 −2x0 −1) +
(y2
0 −y2
0 + 2y0 −1) = 2y0 −2x0 −2 ≥0. Since p1 = (x0 + 1,y0 −1) should be in
the circle if p0 = (x0,y0) is on the circle we have that any chain code for the circle
in the ﬁrst octant does not contain a subword “22” as it will lead to contradiction.
Moreover in any subword a single symbol “2” should be followed by “0” and can
be substituted by 1.
As only the ﬁrst octant segment is considered the solutions can be represented
using strings consisting of only 0 and 1 where 0 is positive motion along the x−axis
and 1 is positive motion along the x−axis and negative motion along the y−axis.
Indeed in the generation of such chain codes the following method shall be used
which is a modiﬁcation of the algorithm that is given in [1]. Starting with the equa-
tion for the circle x2 + y2 = r2 where it is known, by deﬁnition, that r2 ∈Z. Now
as solutions of the ﬁrst octant are required and under the assumption that, without
loss of generality, the centre of the circle is at the origin it is now possible to replace
y2 with (r′ −k)2, where r′ = ⌊r⌋, such that when k = 0 and rearranging for x2 such
that x2 = r2 −r′2 −k2 + 2r′k the solution to the circle equation gives x2 = r2 −r′2
where r′ ≤r, as such the length of the ﬁrst solution of such an equation is
√
r2 −r′2.
Successive values of k give solutions to the length in the x-axis of that particular
maximal Pythagorean triangle. All such Pythagorean triangles will give the ﬁnal
solution to the circle.
Here it is necessary to start with some terminology to say more precisely which
parts of the chain code are considered to represent speciﬁc components in the re-
sulting polygons. It will become much clearer later on why such distinctions are
required as a single side of the polygon may differ in its chain code categorisation.
The ﬁrst of these categories of chain code, the most basic, is given here.
Deﬁnition 19. A chain code segment is a subword of a chain code which is either
a word 0|s0| or 10|si|−1.
A discrete circle, u, is composed of chain code segments where each segment is
represented as ui, i.e u = u0u1u2...un. Chain code segments can be expressed using a
power notation for the number of repetitions e.g 100100 may be rewritten as (100)2
and if u = u0u1u2u3 = (00)(100)(100)(10) = (00)1(100)2(10)1.
It is possible to map the chain code, u, in Octant(1), to the others octants through
a function that maps the alphabet of the chain code {0,1}, to a new code in another
octant segment. A chain code of a circle in an Octant(n) can be constructing from
an Octant(1) ﬁrst by applying the following mapping {0,1} →{n −1 mod 8,n
mod 8}, where 0 ≤n ≤7. Then for the Octant(n), where n ≡1 mod 2, or n is odd,
the code and the mapping itself must be reversed such that {0,1} →{n mod 8,n−1

314
T. Nickson and I. Potapov
mod 8}. For the rest of this paper we will only consider chain codes in the ﬁrst
octant.
Proposition 2. A circle in the ﬁrst octant can be chain coded as follows:
0|s0|10|s1|−110|s2|−1 ...10|si|−1...10|sr′2/2|−1
where,
si = {x|r2 −r′2 + 2(i−1)r′ −(i−1)2 < x2 ≤r2 −r′2 + 2ir′ −i2,x ∈Z}
r is the radius and r′ is the ﬂoor of the radius.
Proof. It follows from [1] where such a method of chain coding is introduced.
Proposition 3. Chain code segments, following the schema
0|s0|10|s1|−110|s2|−1 ...10|si|−1...10|sr′2/2|−1
have the following constraints to their lengths in the discrete circle:
⌊si −1
2
⌋−1 ≤si−1 ≤si + 1 .
Proof. It, again, follows from [1] where this property is proved.
As previously noted terminology must be introduced to distinguish the varying
parts of the chain code and in order to talk about what they represent. In order
to characterise the discrete circle two notions are introduced, Line Segments and
Gradients of Line segments.The following deﬁnition for the gradient gives a method
for extracting such information from the series of digits that construe the line as a
word.
Deﬁnition 20. A gradient, G(u), of a chain code subword in {0,1}, u, is given by
G(u) = #1(u)
|u| , where the function #1 returns the number of 1′s found in the chain
code and |u| is the length of word u.
Line segments are in reference to actual lines that these sections of chain code
would represent on the plane. That is if one were to draw a line and then attempt
to translate this line in to a chain, under the assumption that the gradient of the line
is rational, for reasons which can be seen in the deﬁnition of a gradient, it would
be done via a periodic series of chain code segments. Taking a single period of
this chain all the information that is required to identify and reconstitute this line
to any length is now known. In this way the name may interpreted quite literally
as a segment of a line in chain coded form such that any repetition of this object
produces a line. With this in mind it will naturally be of beneﬁt to be able to know a
little more about the line that has been chain coded as a line segment.

14
Broadcasting Automata and Patterns on Z2
315
Deﬁnition 21. A Line segment, li = u ju j+1...u j+n, is the shortest contiguous sub-
sequence of chain code segments which maintain an increasing gradient such that
G(li) < G(li+1), ∀i > 0, where the last chain code segment in li, u j + n, is con-
tiguous to the ﬁrst chain code segment in the next line segment, that is, li+1 =
u j+n+1u j+n+2...u j+n+m.
1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0  1 0 1
1000
1000
100 10
10
100
1
1000
1000 100
10 1
10100
Chain Code
Chain Code
Segment
Line Segment
Fig. 14.9 Showing the the relationship between chain codes, chain code segments and line
segments
As, ultimately, all of the discrete discs that are discussed here, and, more impor-
tantly, that are possible to construct, are polygons, such polygons are composed of
line segments which represent the sides of the convex shape on the plane. Further,
when categorising the repeated broadcast of such discrete discs, later discussed as
composition, the shapes that are generated are known to be similar, in the strict ge-
ometric sense. A deﬁnition for chain codes, translating from the geometry of the
Reals, is given.
Deﬁnition 22. For any two chain codes, u,v, comprising line segments such that,
u = l0l1...lm and v = l′
0l′
1...l′
n, for m = n, given by Deﬁnition 21, are similar (geo-
metrically) if there is a constant k ∈N such that u = l′0kl′1k...l′nk.
In this way it is possible to say that two shapes are similar in a geometric sense
such that they contain the same number of distinct line segments but the number of
each of these line segments is different by some constant.
With these deﬁnitions stated it is possible to begin the categorisation of the set
of discrete circles with the ﬁrst observation which extends an already known notion
about the chain code of the discrete disc. Following [1] it is known that in the chain
code u any chain code segments with increasing lengths, such that |ui| < |ui+1| <
... < |ui+n|, may increase it by at most 1 from the length of a previous segment, i.e.
|ui+1| ≤|ui|+ 1 for all i.
By direct construction it is easy to check that a line segment can be of the form
10n or 10n10n+1. However we can show that no line segment may be composed of
more than two chain codes which increase in length by one.
Lemma 2. No line segment can be of the form 10n10n+110n+2...10n+i, where i > 1.
Proof. Let us ﬁrst show that any line segment which is part of the discrete circle can-
not be of the form 10n10n+110n+2 for any n > 1. We will prove it by contradiction.
Assume that the line segment li = 10n10n+110n+2 with the gradient 1/n+1 starting

316
T. Nickson and I. Potapov
(x,y)
(x,y)
(x,y)
li
Fig. 14.10 (Left and Centre) Showing the initial point (x,y) the chord and the point on the
chord, the solid circle. (Right) Showing a chord on a circle from (x,y)
at a point p with coordinates (x,y) and ﬁnishing at (x + 3n + 6,y −3). Therefore a
point (x + 2(n + 2),y −2) , i.e. a point which can be reached by a code 10n10n+2
from a point (x,y), is above the discrete circle. By the deﬁnition of the chordal prop-
erty of the circle any line that joins two points on the circle must bound, within the
triangle formed from the two points on the circle and its centre, points which are
again within the circle. On the other hand a point (x + 2(n + 2),y −2), shown cir-
cled in Figure 14.10, belongs to a chord between end points of a chord (x,y) and
(x+3n+6,y−3) and therefore should be within a discrete circle. So it is not possi-
ble to have a line segment with the following chain code 10n10n+110n+2. The same
argument holds for the general case 10n10n+110n+2...10n+i, where i > 1 since the
extension of the line segment still keep the point (x −2,y + 2(n + 2)) above the
discrete circle and on the other hand this point will be in the triangle formed by a
centre of the circle and two end points of the line segment since its gradient is equal
to
i+1
n(i+1)+(i+1)+i(i+1)/2 =
1
n+1+i/2 ≤
1
n+2, where i ≥2.
Further to this weak restriction, that the chain codes may not be monotonically
increasing or indeed non-decreasing, it is possible to give a more deﬁnite generali-
sation of the structure of the chain codes. This structure, which must be adhered to
for all discrete discs, is given here.
Theorem 1. Any line segments on the discrete circle with non-negative gradients
should be in one of the following forms (10n)∗, (10n)(10n+1)∗, (10n)∗(10n+1).
Proof. Following Lemma 2 we can restrict number of cases since any concatena-
tions of more than two chain codes which increase in length by one may not be part
of a line segment. The line segments are sub-words of a chain code with increas-
ing gradients, so if a subword (10n)m is surrounded by any chain code segments
10n1 and 10n2, where n1,n2 ̸= n ± 1 , then (10n)m is a line segment. In the case
were n1,n2 = n ± 1 a subword (10n)m(10n+1)k can be surrounded by only chain
codes 10n3 and 10n4, where n3 ̸= n−1 and n4 ̸= n+2 (by Lemma 2). We will show
now that the only line segments of the form (10n)m(10n+1)k can be where either
m = 1 or k = 1. The proof is similar to Lemma 2. Let us ﬁrst show that the line
segment from a point (x,y) cannot have the following chain code (10n)2(10n+1)2.
If we assume that such chain code may correspond to a line segment such that a

14
Broadcasting Automata and Patterns on Z2
317
point (x−2,y+ 2n + 3), shown in the centre diagram in Figure 14.10, is above the
chain code (i.e. our of a circle) and at the same time is on a chord between points
(x,y) and (x −4,y + 4n + 6), so should be in a discrete circle. Extending the chain
code (10n)2(10n+1)2 from the left by (10n)m1 or from the right by (10n+1)m2 for any
m1,m2 > 0 will not change the property of the point (x−4,y+4n+6). So it can be
in one of the following forms either (10n)m10n+1 or 10n(10n+1)k.
14.6
Iterative Composition
Iterative composition of discrete circles may create different polygons, which later
can also be used for forming non-convex shapes. The following deﬁnitions, and
further characterisations, shall be required in order to discuss the properties of such
composition and develop the proofs.
Deﬁnition 23. A discrete disc is composed of points in the Z2 set ζ r2 = {(x,y)|x2 +
y2 ≤r2, x,y ∈Z, r ∈R}.
Deﬁnition 24. The composition of l digital discs is deﬁned as ζ r0 ◦ζ r1 ◦...◦ζ rl for
ri ∈N, where, ζ r ◦ζ r′ = {a + b|a ∈ζ r, b ∈ζ r′} and is equivalent to H(Al) where
A = r0,r1,...,rl as given in Deﬁnition 14 and Deﬁnition 16 respectively.
Deﬁnition 25. The composition u ◦v = w represents the composition of the chain
codes in the ﬁrst octant for their respective discs, ζ u and ζ v, which results in w the
chain code for the ﬁrst octant of the resultant disc of ζ u ◦ζ v. This method can be
seen in Figure 14.11.
85
17
85o17
Fig. 14.11 Showing the composition of squared radii Disc(85)◦Disc(17) where Disc(85) is
shown as a solid line and 17’s ﬁrst composition is shown dashed. The hatched area represents
all contributions from the previous compositions.

318
T. Nickson and I. Potapov
Example 1. Iterative composition of discrete circles may create different polygons.
For example, in the following picture, Figure 14.12, there are two differing cases,
in one case we iteratively apply the digital circle with squared radius 9 resulting in
the equilateral octagon. In the second case squared radii 16 and 26 are periodically
applied starting with 16.
Fig. 14.12 The above ﬁgures illustrate (left) the constant iteration of the discrete disc of
squared radius 9 and (right) the alternation between the transmission of squared radii 16 then
26.
From these deﬁnitions it is now possible to describe a naive algorithm, and derive
its correctness, for the composition of any number of chain codes that represent
discrete discs. The result is again a convex polygon that represents the combination
of the constituent parts. Such an algorithm merely systematically checks all possible
combinations of the chain code and reports the largest that is found for that step.
Later it shall be seen that if the chain code of the discrete disc is categorised further
it is possible to give a simple linear time algorithm with a proof of correctness that
is derived from the Minkowski sum along with a combinatorial proof.
Lemma 3. Given two chain codes, u = u0u1...un and v = v0v1...vm, in form {0,1}∗
then u ◦v = w = 0|w0|10|w1|−1...10|wm+n−1|−1, such that
|wk| = max({
n
∑
i=0
|ui|+
k−n
∑
j=0
|vj| | 0 ≤n ≤k}).
Proof. The naive way of generating the composition u ◦v is for all points on the
chain code of the disc u = 0|u0|10|u1|−1...10|ui|−1...10|un|−1, to be the centre of the
disc v = 0v010|v1|−1...10|vj|−1...10|vm|−1. This is equivalent to placing the centre of
the chain code v at every point deﬁned by the chain code u generating w. The maxi-
mal point in each chain code segment (i.e if ui = 100 then the coordinate of the ﬁnal
0) need only be considered, clearly covering all others. Let ui ◦′ vj = wi+ j denote

14
Broadcasting Automata and Patterns on Z2
319
the centring v at the coordinate reached by the maximal point of ui in which we con-
sider the maximal point attained by chain code segment vj and represent a possible
length of the chain code for w at wi+ j. The maximal of all such combinations of
lengths, those for which the sum of i, j are equivalent, is required and is deﬁned as
the longest contiguous subsequence of length k from v = v0v1...vj and u = u0u1...ui
for all i, j such that i+ j = k which represents wk for 0 ≤k < m+ n i.e:
max

|u0|+ |v0|
	
= |w0|
max
#|u0|+ |v0|+ |v1|
|u0|+ |u1|+ |v0|
$
= |w1|
max
⎛
⎝
|u0|+ |v0|+ |v1|+ |v2|
|u0|+ |u1|+ |v0|+ |v1|
|u0|+ |u1|+ |u2|+ |v0|
⎞
⎠= |w2|
...
max
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
|u0|+ |v0|+ |v1|+ ...+ |vk|
|u0|+ |u1|+ |v0|+ ...+ |vk−1|
...
|u0|+ |u1|+ ...|ui|+ |v0|+ ...+ |vk−i|
...
|u0|+ |u1|+ ...+ |uk|+ |v0|
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= |wk|
...
|u0|+ |u1|+ ...+ |un|+ |v0|+ |v1|+ ...+ |vm| = |wm+n|.
Such an algorithm allows a characterisation of the composition of discrete discs.
Indeed, here, it can be shown that the composition of chain codes, which have been
shown to be a word W ∈{0,1}∗, is, again, a word, W ′ ∈{0,1}∗.
Corollary 1. Given two chain codes u, v ∈{0,1}∗. The composition u◦v ∈{0,1}∗
Proof. Each segment in u, v, is non-zero ∀i |ui| ̸= 0,|vi| ̸= 0. Its follows from
Lemma 3 that by enlarging k, for l(k + 1), the previous set of solutions is extended
by a chain code from u or v so l(k) < l(k + 1). Thus the composition will be in
{0,1}∗.
Also as a derivation from the algorithm an observation about the properties of the
composition function ◦.
Theorem 2. The composition ◦of two chain codes, u and v is commutative, i.e.
u ◦v = v◦u.
Proof. Following Lemma 3 it is possible to exchange u and v and still obtain the
same result, showing commutativity of composition of chain codes u and v. The
following two sets representing the k-th level of a new chain code are equal as well
as their maximums:

320
T. Nickson and I. Potapov
{S|S =
n
∑
i=0
|ui|+
k−n
∑
j=0
|vj|,0 ≤n ≤k} = {S|S =
n
∑
i=0
|vi|+
k−n
∑
j=0
|u j|,0 ≤n ≤k}
It is also notable that having shown commutativity chain code composition it may
be possible that it is an Abelian group, where the identity element is simply the circle
of radius 0, associativity is derived from the algorithms ordering of line segments
by gradient, which is naturally the same irrespective of the order it is carried out
in, the inverse is simply the removal of one set of line segments from a chain code.
However, it is currently unknown whether or not the operation is closed.
Further, a proof of the ordering of the line segments in the discrete circle is here
required in order to validate the proof of the composition theorem that is given as
one of the main tools and results of this section.
Lemma 4. Given line segments, li, of a form produced by the digitisation of a circle,
which are composed of chain code segments of its ﬁrst octant a = 10m−1 and b =
10m, then the ordering of their gradient for combinations of a and b is
G(b) < ... < G(ab∗) < ... < G(ab) < ... < G(a∗b) < G(a) .
Proof. The proof is a simple comparison of the gradients which shows, where |a| =
m,
1
m+1 <
n
(n−1)(m+1)+m <
n−i
(n−1−i)(m+1)+m <
n−i
(n−1−i)(m)+m+1 <
<
n
(n−1)(m)+m+1 <
n
(n−1)m+m+1, where i < n−1, for G(b) < G(abn−1) < G(abn−1−i)
< G(an−1−ib) < G(an−1b) < G(a) respectively.
Given the preceding validation about the convexity of the discrete discs it is now
possible to employ the Minkowski sum to conclude and validate the composition of
any number of discrete discs using the composition function.
Theorem 3. Composition Theorem Given two chain codes u and v which contain
line segments lu
1lu
2 ...lu
t and lv
1lv
2 ...lv
t′ with strictly increasing gradients. The chain
code of a composition u◦v can be constructed by combining line segments of u and
v and ordering them by increasing gradient.
Proof. One of the ways to prove the above statement is to use a similar result about
the Minkowski sum [25] of convex polygons in R2 and then to prove that it holds for
the ordering of the segments of the digital circles in Z2. Here, a purely combinatorial
proof is given in terms of chain codes, chain code segments and line segments.
The above statement is proved by induction. The fact that the base case for the
composition of the ﬁrst two line segments holds can be seen by directly checking
the expression from Lemma 3.
Assume that the statement of the lemma holds and the ﬁrst z line segments of
u◦v were composed from a set LS = {lu
1,lu
2,...,lu
i ,lv
1,lv
2,...,lv
j} and contains x chain
code segments. Let us also denote a set with other line segments from u and v as ¯
LS.
Without loss of generality suppose that the last line segment that has been added is
lv
j, so G(lv
j) ≥G(l′), where l′ ∈LS and G(lv
j) ≤G(l′′), where l′′ ∈¯
LS. By adding the
next line segment l′′′ which will have a gradient larger or equal than all other line

14
Broadcasting Automata and Patterns on Z2
321
segments in LS and smaller or equal than gradients of ¯
LS it must be shown that the
extended chain code of u ◦v is still correct, i.e. it satisﬁes to Lemma 3.
First of all note that adding a new part of the chain code would not change the
previous x layers. If a current maximum is in the form
|up|+ |up−1|+ ...+ |u0|+ |v0|+ |v1|+ ...+ |vq|
then the next #1(l′′′) sums will be extended by chain codes from l′′′. If l′′′ is a line
segment in u the gradient of G(lv
j) is greater then G(l′′′) and, taking into account
Lemma 4 and Theorem 1, it can be seen that
|up+1|+ |up|+ |up−1|+ ...+ |u0|+ |v0|+ |v1|+ ...+ |vq|
is a maximum within the following set
{|up+1+shift|+...+|up|+|up−1|+...+|u0|+|v0|+|v1|+...+|vq−shift|,shift ∈N}
since the shift will represent removal of a larger value from the v component and
appending the smaller value from the u component.
Repeating the procedure and extending the sum by one value it can be seen that,
again,
|up+2|+ |up+1|+ |up|+ |up−1|+ ...+ |u0|+ |v0|+ |v1|+ ...+ |vq|
is a maximum within a set
{|up+2+shift|+...+|up|+|up−1|+...+|u0|+|v0|+|v1|+...+|vq−shift|,shift ∈N}
following the same reasoning. Similar arguments can be applied for the case were
l′′′ ∈v.
Example 2. Let us illustrate the composition of two chain codes corresponding to
digital circles with squared radii 45 and 9 in the ﬁrst octant. The chain code of the
ﬁrst octant is ζ 45 = 0001 and for ζ 9 = 01 under composition this yields ζ 45 ◦ζ 9 =
000101.
From the preceding notions it is now possible to describe a linear time algorithm,
which vastly out performs the exhaustive search of all combinations that has pre-
viously been given, which is able to form the composition of the chain codes of
any two discrete circles. As Lemma 4 shows that all of the line segments will be
in a non-increasing order it is possible to construct such line segments by count-
ing the number of 0′s after each delimiting 1 combining those that increase in size
with the preceding smaller chain code segment. Having found all line segments for
both chain codes it is a simple case of, starting with the chain code with the line
segment with the largest gradient, adding that element to a new chain code which
is the composition of the initial two. Continue choosing the line segment with the
largest gradient from either of the initial chain codes until there are no elements left
in either of the original circles. The result of the algorithm is the composition of the

322
T. Nickson and I. Potapov
two circles. The following proposition gives a proof of the algorithm and the notion
that it is linear time computable.
Proposition 4. Two discrete circles chain codes can be composed into a single chain
code in a linear time.
Proof. As Lemma 4 shows that all of the line segments will be in a non-increasing
order it is possible to construct such line segments by counting the number of 0′s
after each delimiting 1 combining those that increase in size with the preceding
smaller line segment. Having found all line segments for both chain codes it is a
simple case of starting with the chain code with the line segment with the largest
gradient adding that to a new chain code which is the composition of the initial two.
It is also now possible to state a more formal deﬁnition of ‘similar’ such that it
is possible to mathematically determine whether two objects are ‘similar’, geomet-
rically. The following algorithm is given as one application of such an algorithm
and also hints at the notion of composing one discrete disc from other discrete discs
which allows an insight in to the primality of discrete discs those that cannot be
composed from any other set of discrete discs.
Theorem 4. Given a ﬁnite broadcasting sequence of radii R = (r1,r2,...,rl) and a
convex polygon P, it is decidable whether there are radii such that the chain coding
of the composition of is similar to P.
Proof. The algorithm computes all line segments, and their corresponding gradi-
ents, for the chain codes of the set of digital disks with radii in, R, and the convex
polygon, P. Each digital disk ri ∈R can be represented as a vector with k values,
where k is the cardinality of the set of gradients and each vector component corre-
sponds to a particular gradient with an integer value standing for the number of line
segments with this gradient in R. Finally we would need to solve a system of linear
Diophantine equation over positive integers to check whether there is a set of factors
for deﬁned vectors that may match a vector for P with another unknown factor.
14.7
Broadcasting Sequences and Their Limitations
Although the composition of circles gives us a large range of polygonal shapes,
an analysis of broadcasting sequences shows that there certain limitations for such
composition. It’s seen that by Lemma 2 there are only three possible forms which
the lines that comprise the discrete circle may take. Translating these in to their
respective gradients gives the following three possible gradient forms
G1 = G((10n−1)m) = m
mn = 1
n
G2 = G((10n−1)(10n)m) =
m+ 1
n + m(n + 1)

14
Broadcasting Automata and Patterns on Z2
323
G3 = G((10n−1)m(10n)) =
m+ 1
nm+ n + 1 .
The above gradients are reduced to a minimal form in order to elucidate the
exclusivity of the gradients.
Proposition 5. It is only possible to express gradients in the ﬁrst octant of a circle
in a reduced form 1
n,
a
a(n+1)−1 or
a
an+1 for a,n ∈N.
Proof. As G1 is already in a minimal form it is obvious that this can only express
those gradients, g, of the form 1
n. For G2 and G3 the following method is employed,
G2 =
m+1
n+m(n+1) = a
b, where a⊥b from which it follows that m = a −1 such that
through substitution of m in to denominator,
a
a(n+1)−1 is arrived at. Similarly for G3
by substitution the equation
a
an+1 arises.
It now becomes more clear of what cannot be expressed and the limitations in-
herent in the hulls of the broadcasting sequences.
Proposition 6. Not all rational gradients, 0 ≤g ≤1, are expressed by the lines that
comprise the discrete circle in the ﬁrst octant.
Proof. A counter example is given as proof. It is impossible to express any such
rational of the form 5
8. It is clear that it is not possible for G1 to express such a
fraction. For G2 and G3 the following sufﬁces. G2 =
a
a(n+1)−1 where a = 5 such that
5
5n+4 where there is no such n ∈N such that G2 = 5
8. Similarly for G3 =
a
an+1 where
a = 5 and
5
5n+1 there is no such n ∈N such that G3 = 5
8
It also becomes clear from the composition theorem (Theorem 3) itself that it is
not possible to generate any further gradients or new line segments through compo-
sition and in turn successive broadcasts of radii may not generate any polygons with
chain codes that include gradients not previously present.
Corollary 2. The set of line segments, and as such gradients, that compose any
discrete circle are closed under composition.
As noted here the shapes that are generatable are limited by their convexity as
well as by the gradients of the lines that compose them. In the next section some
of these limitations will be removed or relaxed using multiple broadcast sequences
and an aggregation function with which to map all values to a single value.
14.8
Reducing Restrictions through Aggregation
This section makes uses of the notion of aggregation to reduce the restrictions that
are imposed by the continual composition, or simple construction of discrete discs.
It can be shown that it is possible in certain cases, which shall be exposited both
here and in further sections where it is employed, to show the approximation of the
astroid metric.

324
T. Nickson and I. Potapov
As all points may be labelled by their distance over some arbitrary mod-
ulo value an extension to the current work is proposed whereby two differing
r −neighbourhood sequences, A and A′ are used to label the Z2 lattice from the
same point p. At any point, p′, there are now two labels such that p′ = (i, j) where
by k ≡i mod m for the sequence A and k′ ≡j mod m for the sequence A′. Here
two differing functions for the aggregation of values of i and j which deﬁne new
shapes on the lattice and in turn new metrics. The new shapes deﬁned by the lattice
are not necessarily convex.
As all of the discrete disks which make up the labellings of the lattice are com-
posed of discrete lines it is possible to analyse the effects of the combinations of
disks by considering only the intersections of the lines that make up the disks. Con-
sider a series of parallel lines expressed in the form y0 = m0x0 + c0 where ci ∈Z is
an arbitrary constant or offset, mi ∈Q is any gradient permitted by the line segments
of a discrete circle and x,y ∈Z are the usual Euclidean coordinates. These lines are
such that each successive line is of a distance wi from the last which shall imitate
the width between iterations of the discrete circles, for the ﬁrst line segment this is
equivalent to ⌊
√
r2⌋, and the discrete lines that they generate. All coordinates such
that m0x0 + c0 + kw0 ≤y0 < m0x0 + c0 + (k + 1)w0 are labelled as k0. A second set
of parallel lines differing from the ﬁrst are deﬁned as y1 = m1x1 + c1 where all co-
ordinates such that m1x1 +c1 +k1w1 ≤y1 < m1x1 +c1 +(k1 +1)w1 are labelled k1.
The intersection of these two areas is thus (k,k′). Increasing the offset of k0 and k1
to k0 + 1 and k1 + 1 naturally results in the labelling of the area of their intersection
as (k0 + 1,k1 + 1). The ordering of the tuple is not relevant to the functions that
aggregate them.
The ﬁrst of the functions here have previously been studied in [19] with regards to
geometric computations on the lattice. It is of particular interest in the Broadcasting
Automata model where the notion of waves as observed in nature are used to control
a large number of distributed automata. Being able to predict the resultant shapes
that are formed by the transmission of waves is, naturally, largely advantageous in
that it affords the ability to predict and manipulate the formations on the plane.
Such formations can then be used for a variety of computational duties such as
partitioning and geometric computation.
Two functions for aggregation will now be introduced with the relevant equations
for resultant patterning of the lattice, where m = 4 and the values of i, j ∈{0,1,2,3}.
Deﬁnition 26. The moir´e aggregation function is given in the table below.
moir´e(i,j)
=
⊕0 1 2 3
0 a b c b
1 b a b c
2 c b a b
3 b c b a

14
Broadcasting Automata and Patterns on Z2
325
Deﬁnition 27. The Anti-moir´e aggregation function can be expressed simply as
addition over modulo 4 and is given in the table below.
Anti-moir´e(i,j)
=
⊕0 1 2 3
0 a b c d
1 b c d a
2 c d a b
3 d a b c
Proposition 7. The gradients of the new lines formed by the moir´e equation can be
predicted using the equation
wo ·m1 −w1 ·m0
w0 −w1
.
Variables refer to the, line gradient, mi, and the width of the line, wi.
Proof. The form of the moir´e function is such that if the intersection of one area
is labelled as (k,k′) then the next area that is labelled similarly will be of the form
(k + 1,k′ + 1). Encoding this as the intersections of lines y = m0x+ c0 + k ·w0 with
y = m1x+ c1 + k′ ·w1 and y = m0x+ c0 + (k + 1)·w0 with y = m1x+ c1 + (k′ + 1)·
w1. All that is left is to ﬁnd the gradient of the two intersections. Solving the ﬁrst
case, x0 = c1−c0−k·w0−k′·w1
m0−m1
and y0 = m0( c1−c0−k·w0−k′·w1
m0−m1
)+c0 +k·w0 . The second,
x1 = c1−c0−(k+1)·w0−(k′+1)·w1
m0−m1
and y1 = m0( c1−c0−(k+1)·w0−(k′+1)·w1
m0−m1
) + c0 + (k + 1) ·
w0. The gradient of the two points can be found, y1−y0
x1−x0 , which results in the gradient
of the labelling applied to the lattice.
Proposition 8. The gradients of the new lines formed by the Anti-moir´e equation
can be predicted using the equation
wo ·m1 + w1 ·m0
w0 + w1
.
Variables refer to, line gradient, mi, and the width of the line, wi.
Proof. The form of the moir´e function is such that if the intersection of one area
is labelled as (k,k′) then the next area that is labelled similarly will be of the form
(k + 1,k′ + 1). Encoding this as the intersections of lines y = m0x+ c0 + k ·w0 with
y = m1x+ c1 + (k′ + 1)·w1 and y = m0x+ c0 + (k + 1)·w0 with y = m1x+ c1 + k′ ·
w1. All that is left is to ﬁnd the gradient of the two intersections. Solving the ﬁrst
case, x0 = c1−c0−k·w0−(k′+1)·w1
m0−m1
and y0 = m0( c1−c0−k·w0−(k′+1)·w1
m0−m1
)+c0 +k ·w0 . The
second, x1 = c1−c0−(k+1)·w0−k′·w1
m0−m1
and y1 = m0( c1−c0−(k+1)·w0−k′·w1
m0−m1
)+ c0 + (k + 1)·
w0. The gradient of the two points can be found, y1−y0
x1−x0 , which results in the gradient
of the labelling applied to the lattice.
Composition of this form is given as example in Figure 14.13. Here the two lines
are shown on the lattice correspond to the gradients of the two differing aggregation
functions. The following proposition is now noted.

326
T. Nickson and I. Potapov
(0,0)
0
1
2
3
3
2
1
0
0
1
2
3
2
1
(0,1)
(0,2)
(0,3)
(0,3)
(0,2)
(0,1)
(1,0)
(1,1)
(1,2)
(3,3)
(2,2)
(3,3)
(2,3)
(3,0)
(2,2)
(1,1)
(3,0)
(2,3)
(1,2)
(3,1)
(2,0)
(2,1)
(3,2)
(3,2)
(2,1)
(1,0)
(1,3)
(2,0)
(3,1)
(3,1)
(2,0)
(1,2)
(2,3)
(3,0)
(1,3)
(3,0)
(2,3)
(1,2)
(1,3)
(2,0)
(3,1)
(1,0)
(2,1)
(3,2)
(3,2)
(1,3)
(0,1)
(3,0)
(0,0)
Pattern A
Pattern B
Fig. 14.13 The two forms of aggregation via the above functions. Here pattern A gives the
line formed by the anti-moire function and the pattern b gives the line as formed by the moire
function.
Proposition 9. For any two discrete lines on the Z2 lattice it is possible to vary the
observed gradients for resultant from aggregate functions by varying the width of
the lines.
Proof. The proof is an observation of the equations for the gradients of the lines
resultant from aggregation. Observing the results of two aggregating functions,
wo·m1+w1·m0
w0+w1
and wo·m1−w1·m0
w0−w1
, any alteration to the width of the line which represents
results in a direct change in the gradient.
It is now possible to formulate the following statement which shows the increase
in expressivity that comes from using the composition of two broadcast sequences.
The number of lines formed by aggregation are not limited by the restrictions that
are demonstrated in Proposition. 5 where it is now possible to construct new lines
through the varying of widths and indeed new polygons may now be formed with
this technique such that they are non-convex.
14.9
Formation of Polygons through Aggregation
Having shown that it is possible to construct different gradients from line sections
it is natural to now observe what happens when whole circles are intersected and
the relationships that are formed by the aggregation functions that have been in-
troduced. It is noted that a diagram for the formation of polygons in case of the
moir´e aggregation function is shown in Figure 14.14. Two digital discs, those with
squared radii two and 25 such that the discs are ζ 2 and ζ 25, are composed gen-

14
Broadcasting Automata and Patterns on Z2
327
m3
m4
l0
l1
0l
’
’
Fig. 14.14 The above schematic depicts the broadcast of two discrete circles. The ﬁrst, inner,
circle (of squared radius two where ζ 2 = l0 with gradient m0) and the second discrete circle
(of squared radius 25 where ζ 25 = l′
0l′
1 with gradients m1 and m2 respectively) the outer
construction shows the resulting moir´e lines here labelled m3 and m4. Arrowed lines show
line width measurements, the respective wi, here, w0 = 1, w1 = 5, w2 = 7, w3 = 5
4 and w4 = 7
6.
erating, from the two convex polygons, and new, previously unreachable, polygon
which is non −convex.
The following examples given in Figure 14.15 also elucidate the differences be-
tween the two aggregation functions showing the non-convex polygon generated by
the two digital discs that are represented by, ζ 2 and ζ 9.
It is also possible to gain a better picture of the overall shapes produced by the
anti-moire equation by reducing the number of labels, and so the colours, further.
This is done by a process of merging certain values or reducing the values over some
modulo which in this example, Figure 14.16, is two. The reduction is given by the
following aggregation function:
Anti-moir´e-Mod2(i,j)
=
⊕0 1 2 3
0 a b b a
1 b b a a
2 b a a b
3 a a b b

328
T. Nickson and I. Potapov
Fig. 14.15 Above gives an example of aggregation of two broadcast sequences, one of the
discrete disc ζ 32 and the other ζ 36, from a central point on the diagram and with the two
aggregation functions (left) moir´e and (right) Anti-moir´e.
14.10
Approximating Lp Metrics with Broadcasting Automata
Previously, von Neumann and Moore neighbourhoods have been used to achieve
an approximation of the Euclidean metric through some mixing of the neighbour-
hoods. This has been done in many different settings such as periodic, non-periodic
combinations of the two neighbourhoods, regular and non-regular, i.e, hexagonal,
triangular grids to which different sequences, i.e applying different deﬁnitions of
neighbourhood, are applied, as well as a variety of methods for deﬁning just how
an approximation of Euclidean distance by neighbourhood sequences should be de-
ﬁned and measured, where most of these techniques discuss notions of digital cir-
cularity such as the isoperimetric ratio, perimeter comparisons, etc. In short this
has been one of the main studies with regards to neighbourhood sequences. There
is ultimately, as has been previously discussed, a large barrier to the extension of
this body of work in the approximation of the more general Lp metrics due to
the impossibility of constructing any non-convex polygon from the composition
of the two convex polygons which represent the Moore and von Neumann neigh-
bourhoods. The astroid is part of the ‘family’ of Lp metrics of which the Moore
neighbourhood, L∞, the von Neumann neighbourhood, L1, and the Euclidean met-
ric, L2 are the most well known. In general an Lp space is deﬁned by the formula
∥x ∥p= (|x1|p + |x2|p + ...+ |xn|p)
1
p such that ∥x ∥p is the p-norm.
It is in this section that a new method for the generalisation of metric approxi-
mation with broadcasting neighbourhoods shall be given and, using only two broad-
casting radii, here given in terms of r2, and the moir´e aggregating function, explore
the ability of this model to approximate the astroid and, as such, a new class of
metrics outside of the reach of neighbourhood sequences.

14
Broadcasting Automata and Patterns on Z2
329
Fig. 14.16 The above ﬁgure depicts the anti moire pattern after the merging of two sets of
two colours and initially generated by the discrete discs of squared radius 32 and 36 from the
central point on the plane.
It has previously been seen that the methodology of combining two broadcasting
sequences using an aggregating function can be used to extend the possible resulting
polygons. In this section it shall be seen that this can be employed in the solution
to a practical problem. The problem, in this case, is the approximation of Lp metric,
L2/3, which forms an astroid [30]. The astroid can be expressed by the equation,
x2/3 + y2/3 = r2/3, where x and y are those of the Cartesian plane and, as with the
equation for the circle from which this equation is generalised, the r is the ’radius’
of the astroid.
A few preliminaries with regards to the astroid which are taken from [30] will
be required to understand the methods proposed for comparing the approximation,
via broadcasting sequence and aggregation, and the actual astroid. The astroid was
discovered in Roemer in 1674 whilst searching for the best form of gear teeth. It is a
hypercycloid of four cups and can be described by a point on a circle of radius 3
4 ·a

330
T. Nickson and I. Potapov
rolling on the inside of a ﬁxed circle of radius a. The resultant shape has a perimeter,
L, of L = 6a where a is the radius of the outer ﬁxed circle and is the maximal point
reached by the astroid. The area, A, encapsulated is given by A = 3
8πa2. Finally, the
point at which the x and y coordinates are equivalent on the perimeter of the astroid
can be expressed as r
2. The equation is known to have applications in magnetism
where the Stoner-Wohlfarth astroid curve separates regions with two minima of
free energy density from those with only one energy minimum and is a geometric
representation of the model of the same name [28].
Fig. 14.17 The above ﬁgure depicts the astroid, here, rotated by 45◦(left) compared to its
best, found, approximation with broadcasting sequences (right), an aggregation of the discrete
discs of squared radius, 2 and 5.
In [14, 24] a number of methods for how well a particular neighbourhood
sequence approximates the euclidean distance are given. The authors of [24] con-
sider any sequence, including those sequences which are non-periodic, of neigh-
bourhoods with which to approximate the euclidean distance. In order to show how
well, or poorly, the sequence approximates the euclidean distance it is compared to
the euclidean circle through a variety of methods. One such method is through the
use of the isoperimetric ratio or the noncompactness ratio. Here, the attempt is to
measure P2
A where, P, is the perimeter, and, A, is the area. This measure is conjec-
tured to be minimal for the circle where it is 4π, however, this does not help when
looking at non-convex shapes, such as those produced when approximating the an
astroid, due to the measure being optimised for convex and symmetrical shapes.
Other ways of approximating the euclidean circle are suggested such as a perime-
ter based approximation and area based approximation. With respects to are based
approximation there are two techniques used. The ﬁrst is that of the inscribed circle
based approximation. This method attempts to ﬁnd a sequence that generates the
polygon, generated by the neighbourhood sequence, which is closest to the polygon

14
Broadcasting Automata and Patterns on Z2
331
having the given circle as the inscribed circle. The second of these methods is the
covering circle based approximation such that polygon must be covered by the cir-
cle. More methods are given but rely on properties distinct to the circle and so are
not discussed here.
Further, in [14], another measure of circularity is considered using three differing
methods. They formulate three approximation problems whereby the aim is to ﬁnd
the neighbourhoodsequence that minimises the error in each formulation. The prob-
lems may be informally described as the following: problem one requires ﬁnding the
neighbourhood sequence that best minimises the size of the symmetric difference
between some neighbourhood sequence at step k, given as Ak and the Euclidean cir-
cle of radius k; the second problem attempts to ﬁnd the sequence that best minimises
the complement of the neighbourhood generated by Ak with it’s smallest inscribed
circle; the third problem is a discretisation of the second problem where an Ak must
be found such that it minimises the complement with the largest discrete disc that
can be inscribed within.
It is the second of these problems from [14] that shall be explored as a method
for showing a best approximation of the astroid with combined broadcasting se-
quences. In this case the method is the most simplistic to calculate and also the
least dependent on any of the properties that are only present in the circle. As such
a more formal representation of the problem can now be posed, here, given as,
Area(H(fk(A,B))\G′
k ≤Area(H(fk′(A,B′))\G′
k′), where, fk(A,B) is the kth poly-
gon generated by the aggregation of the broadcasting sequences, A, which here will
be the discrete disc, r2 = 2, and the broadcasting sequence, B,B′, here a variable
which is to be optimised to ﬁnd the best approximation. There is a simple change to
Gk, originally used in [14] to represent the circle of radius k, to convert it to G′
k in
that G′
k = {q ∈R2 : L
2
3 (0,q) ≤k} as simple conversion of the metric from that of
the euclidean distance, L2, to the astroid distance, L
2
3 .
The complexity of calculating these compositions for the purposes of optimisa-
tion means that only experimental data shall be given here as a proof of validity
of the approximation of the concave metrics, which the astroid represents. As the
astroids require a rotation by 45◦in order to match the polygon that is generated
by the aggregation, f(A,B), as deﬁned before. In matching the point that is at the
largest euclidean distance from the origin, which for simplicity, and without loss of
generality, is considered the initial point from which all broadcasting occurs. This
point is matched to the same point on some polygon on some composition, f(A,B),
and the complement of the areas compared. The following table is produced with
this method.
The function of aggregation must also be deﬁned. Here the choice is moir´e ag-
gregation without the modulo restriction, although, such a restriction is retained in
the images for simplicity. Such a function can now be deﬁned as, f(Ai,B j) = |i−j|
for the ith and jth iteration of the sequence A and B respectively. For the function
fk(A,B) where there exists some Ai and B j such that |i−j| = k.
The min point for the L
2
3 is calculated by r
2 where r is the radius of the astroid,
the r in x
2
3 + y
2
3 = r
2
3 . The B is the second broadcasting sequence, here, given as

332
T. Nickson and I. Potapov
Table 14.1 Illustrating the experiments done with regards to the approximation of the astroid
Astroid - L
2
3
moir´e - fk(A,B)
Radius Min Area Radius Min Area B k Complement
17
8.5 340
17
11
461
5
5
121
17
8.5 340
17
13
753
9
8
413
17
8.5 340
17
13
873 10 9
533
18
9
382
18
16 1121 13 11
739
18
9
382
18
14 1033 16 11
651
18
9
382
18
14 1001 17 11
619
17
8.5 340
17
15 1041 37 13
701
17
8.5 340
17
16 1141 45 14
801
17
8.5 340
17
16 1093 61 14
753
17
8.5 340
17
16 1181 82 15
841
Table 14.2 The above diagrams show the patterns generated by the aggregation of the two
discs, where one is the disc of squared radius 2 and the other is varied, in these diagrams
(from left to right and line by line) there are discs of squared radius, 5, 9, 10 and 13
the r2 of the disc. Images for each of the resultant, aggregated images are given in
Table 14.2 and Table 14.3.

14
Broadcasting Automata and Patterns on Z2
333
Table 14.3 The above diagrams show the patterns generated by the aggregation of the two
discs, where one is the disc of squared radius 2 and the other is varied, in these diagrams
(from left to right and line by line) there are discs of squared radius, 16, 17, 37, 45, 61 and 82
Tables 14.1 and 14.4 give a series of comparisons for Area(H(fk(A,B)))\G′
k.
From this table it is possible to observe that the best approximation, from those con-
structed, though there is a trend towards worsening approximations as B increases,
that the simplest composition yields the best results. In this case this value for B is

334
T. Nickson and I. Potapov
Table 14.4 Illustrating the experiments done with regards to the approximation of the astroid
where both A and B are ﬁxed
Astroid - L
2
3
moir´e - fk(A,B)
Radius Min Area Radius Min Area B k Complement
2
1
5
2
1
13
5 1
8
5
2.5
30
5
3
65
5 2
35
8
4
75
8
5
157 5 3
82
11
5.59 143
11
7
289 5 4
146
18
9
382
18
9
461 5 5
79
21
10.5 520
21
11
673 5 6
153
24
12
679
24
13
925 5 7
246
27
13.5 859
27
15 1217 5 8
358
30
15 1060
30
17 1549 5 9
489
33
16.5 1283
33
19 1921 5 10
628
the disc generated by the squared radius r2 of 5, this disc being constructed by the
next smallest squared radius that constructs a new discrete disc. The following table
now looks, again, experimentally, at how the approximation changes as k increases.
The table notes that the approximation weakens as it increases perhaps indicating a
divergence between the astroid and the polygon generated by fK(A,B).
14.11
Pattern Formations and Periodic Structures in Z2
It is natural after observing the variety of effects that are the result of the application
of an aggregation function to look at functions which are themselves shapes of some
form. Here, functions of this form and their resultant patterns, imposed on the grid
according to their application, are given. Such functions are only restricted by their
symmetry, a result of the unordered nature of the tuples to be aggregated.
The ﬁrst function, depicted in Figure 14.18 (Left) takes the form of a discrete disc
itself, in this case one which is also represented by the discrete disc of squared radius
ﬁve. The functions here have also been increased in size and the size of the modulo
for the labels has been increased. The use of discrete discs of squared radius 8 is
important here as it constructs, in some sections of the lattice, a perfect reproduction
of the shape given in the aggregating function. The following table describes the
aggregating function and the details of the image.

14
Broadcasting Automata and Patterns on Z2
335
Fig. 14.18 (Left) Patterns generated by the aggregating function representing the discrete
disc of squared radius ﬁve and the labelling of the lattice given by two broadcasting sequences
of squared radius eight. (Right) Patterns generated by the aggregating function representing
the discrete disc of squared radius ﬁve and the labelling of the lattice given by two broadcast-
ing sequences, one of squared radius 26 and the other of squared radius 36
Array Size:
300
Centre 1:
(100,150)
Centre 2:
(200,150)
Radius 1:
8
Radius 2:
8
Modular Labelling:
(0,1,2,3,4,5,6)
Aggregation Function:
Shown right.
Figure 14.18 (Left)
⊕0 1 2 3 4 5 6
0 a a a a a a a
1 a a b b b a a
2 a b b a b b a
3 a b a a a b a
4 a b b a b b a
5 a a b b b a a
6 a a a a a a a
This reproduction of the aggregating function is not always exact. It is possible to
skew and deform the representation of the image described by altering the radii of
the circles that are used to form the underlying labelling of the lattice. The following
image, Figure 14.18 (right) gives an example of such a deformation.
Array Size:
300
Centre 1:
(100,150)
Centre 2:
(200,150)
Radius 1:
26
Radius 2:
36
Modular Labelling:
(0,1,2,3,4,5,6)
Aggregation Function:
Shown right.
Figure 14.18 (Right)
⊕0 1 2 3 4 5 6
0 a a a a a a a
1 a a b b b a a
2 a b b a b b a
3 a b a a a b a
4 a b b a b b a
5 a a b b b a a
6 a a a a a a a

336
T. Nickson and I. Potapov
The two following ﬁgures, Figure 14.19, demonstrates changes that occur when
manipulating the number of colours, changing the aggregation function and altering
the underlying tuples that generate the overall shape of the colourings.
Fig. 14.19 Patterns generated by the aggregating function represented by the table and the
labelling of the lattice given by two broadcasting sequences, (right) both of squared radius
eight (left) one of squared radius 26 and the other of squared radius 36.
Array Size:
300
Centre 1:
(100,150)
Centre 2:
(200,150)
Radius 1:
8
Radius 2:
8
Modular Labelling:
(0,1,2,3,4,5)
Aggregation Function:
Shown right.
Figure 14.19 (left)
⊕0 1 2 3 4 5
0 b b c c b b
1 b c a a c b
2 c a a a a c
3 c a a a a c
4 b c a a c b
5 b b c c b b
Array Size:
300
Centre 1:
(100,150)
Centre 2:
(200,150)
Radius 1:
26
Radius 2:
36
Modular Labelling:
(0,1,2,3,4,5)
Aggregation Function:
Shown right.
Figure 14.19 (rigth)
⊕0 1 2 3 4 5
0 b b c c b b
1 b c a a c b
2 c a a a a c
3 c a a a a c
4 b c a a c b
5 b b c c b b
Altering the aggregating function is clearly a powerful tool in pattern and poly-
gon formation. The alteration of the discs that are used as the basis of the aggrega-
tion also show that any underlying shape generate by an aggregating function can be
skewed and otherwise altered whilst retaining the gestalt representation. Methods of
manipulating the hew and scale of the shapes that are generated through some ag-
gregating function may be useful to pattern recognition and detection methods that

14
Broadcasting Automata and Patterns on Z2
337
Fig. 14.20 The above ﬁgure shows a number of distinct patterns that are here generated by
four different transmissions where two, one of squared radius 2 and the other of squared
radius 12, are placed at two separate points
are part of the Swarm Robotics cannon among others. Whilst altering the aggre-
gation function is one an interesting concept, for the purpose of pattern formation,
there is still a lot of possibility that remains when only considering the standard
moir´e function. Such as can be seen in the variety of patterns that are formed in
Figure 14.20.

338
T. Nickson and I. Potapov
References
1. Bhowmick, P., Bhattacharya, B.B.: Number-theoretic interpretation and construction of
a digital circle. Discrete Applied Mathematics 156(2), 2381–2399 (2008),
http://www.sciencedirect.com/
science/article/pii/S0166218X07004817,
doi:10.1016/j.dam.2007.10.022, ISSN 0166-218X
2. Bresenham, J.: A linear algorithm for incremental digital display of circular arcs.
Commun. ACM 20(2), 100–106 (1977),
http://doi.acm.org/10.1145/359423.359432,
doi:10.1145/359423.359432, ISSN 0001-0782
3. Chatterji, B.N., Das, P.P., Chakrabarti, P.P.: Generalized distances in digital geometry.
Information Sciences 42, 51–67 (1987)
4. Conway, J.H.: Regular Algebra and Finite Machines. Dover Books on Mathematics Se-
ries. Dover Publications (2012),
http://books.google.co.uk/books?id=1KAXc5TpEV8C,
ISBN 9780486485836
5. Duncan, R.: A survey of parallel computer architectures. Computer 23(2), 5–16 (1990),
doi:10.1109/2.44900, ISSN 0018-9162
6. Farkas, S., Bajk, J., Nagy, B.: Approximating the Euclidean circle in the square grid
using neighbourhood sequences. Pure Math. Appl (PU.M.A.) 17, 309–322 (2006), ISSN
1218-4586
7. Farkas, S., Bajak, J., Nagy, B.: Approximating the Euclidean circle in the square grid
using neighbourhood sequences. ArXiv e-prints (June 2010)
8. Feng, T.: A survey of interconnection networks. Computer 14(12), 12–27 (1981),
doi:10.1109/C-M.1981.220290, ISSN 0018-9162
9. Feynman, R.P., Leighton, R.B., Sands, M.L.: The Feynman lectures on physics. Addison-
Wesley World Student Series, vol. 1. Addison-Wesley Pub. Co. (1963),
http://books.google.co.uk/books?id=_ZUfAQAAMAAJ
10. Freeman, H.: On the encoding of arbitrary geometric conﬁgurations. IRE Transactions on
Electronic Computers, EC-10, 260–268 (1961), doi:10.1109/TEC.1961.5219197, ISSN
0367-9950
11. Gerhardt, M., Schuster, H., Tyson, J.J.: A cellular automaton model of excitable media:
Ii. curvature, dispersion, rotating waves and meandering waves. Physica D: Nonlinear
Phenomena 46(3), 392–415 (1990),
http://www.sciencedirect.com/science/
article/pii/016727899090101T,
doi:10.1016/0167-2789(90)90101-T, ISSN 0167-2789
12. Hella, L., J¨arvisalo, M., Kuusisto, A., Laurinharju, J., Lempi¨ainen, T., Luosto, K.,
Suomela, J., Virtema, J.: Weak models of distributed computing, with connections to
modal logic. CoRR, abs/1205.2051 (2012)
13. Hajdu, A.: Geometry of neighbourhood sequences. Pattern Recognition Letters 24(15),
2597–2606 (2003)
14. Hajdu, A., Hajdu, L.: Approximating the euclidean distance using non-periodic neigh-
bourhood sequences. Discrete Mathematics 283(1-3), 101–111 (2004),
http://www.sciencedirect.com/science/
article/pii/S0012365X04001116,
doi:10.1016/j.disc.2003.12.016, ISSN 0012-365X

14
Broadcasting Automata and Patterns on Z2
339
15. Kari,
J.:
Theory
of
cellular
automata:
a
survey.
Theor.
Comput.
Sci.
334,
3–33 (2005), http://dl.acm.org/citation.cfm?id=1083031.1083033,
doi:10.1016/j.tcs.2004.11.021, ISSN 0304-3975
16. Lee, G., Chong, N.Y.: A geometric approach to deploying robot swarms. Annals of Math-
ematics and Artiﬁcial Intelligence 52 257–280 (2008),
http://dl.acm.org/citation.cfm?id=1527581.1527606,
doi: 10.1007/s10472-009-9125-x, ISSN 1012-2443
17. Lee, G., Yoon, S.: A mobile sensor network forming concentric circles through local
interaction and consensus building. Journal of Robotics and Mechatronics 21, 469–477
(2009), ISSN 1883-8049
18. Linz, P.: An Introduction to Formal Languages and Automata. Theory of Computation
Series. Jones and Bartlett (2001),
http://books.google.co.uk/books?id=Cgooanwdo9AC,
ISBN 9780763714222
19. Martin, R., Nickson, T., Potapov, I.: Geometric computations by broadcasting automata
on the integer grid. In: Calude, C.S., Kari, J., Petre, I., Rozenberg, G. (eds.) UC 2011.
LNCS, vol. 6714, pp. 138–151. Springer, Heidelberg (2011)
20. Martin, R., Nickson, T., Potapov, I.: Geometric computations by broadcasting automata.
Natural Computing, 1–13 (2012),
http://dx.doi.org/10.1007/s11047-012-9330-0, doi:10.1007/s11047-
012-9330-0, ISSN 1567-7818
21. Matouˇsek, J.: Lectures on Discrete Geometry. Graduate Texts in Mathematics.
Springer (2002), http://books.google.co.uk/books?id=MzFzCZAAk8MC
ISBN 9780387953731
22. Mazoyer, J.: An overview of the ﬁring squad synchronization problem. In: Chof-
frut, C. (ed.) Automata Networks. LNCS, vol. 316, pp. 82–94. Springer, Heidelberg
(1988), http://dx.doi.org/10.1007/3-540-19444-4_16, doi:10.1007/3-
540-19444-4 16, ISBN 978-3-540-19444-6
23. Nagy, B.: Metric and non-metric distances on zn by generalized neighbour-
hood sequences. In: Proceedings of the 4th International Symposium on Image
and Signal Processing and Analysis, ISPA 2005, pp. 215–220 (September 2005),
doi:10.1109/ISPA.2005.195412
24. Nagy, B., Strand, R.: Approximating euclidean distance using distances based on
neighbourhood sequences in non-standard three-dimensional grids. In: Reulke, R.,
Eckardt, U., Flach, B., Knauer, U., Polthier, K. (eds.) IWCIA 2006. LNCS, vol. 4040,
pp. 89–100. Springer, Heidelberg (2006)
25. Schneider, R.: Convex Bodies: The Brunn-Minkowski Theory. In: Encyclopedia of
Mathematics and Its Applications. Cambridge University Press (1993),
http://books.google.co.uk/books?id=2QhT8UCKx2kC
26. Sloane, N.J.A.: The On-Line Encyclopedia of Integer Sequences (1995),
http://oeis.org/A001481A001481; Numbers that are the sum of 2 nonnega-
tive squares
27. Suzuki, I., Yamashita, M.: Distributed anonymous mobile robots: Formation of geomet-
ric patterns. SIAM Journal on Computing 28, 1347–1363 (1999)

340
T. Nickson and I. Potapov
28. Thiaville, A.: Extensions of the geometric solution of the two dimensional coherent mag-
netization rotation model. Journal of Magnetism and Magnetic Materials 182(1-2), 5–18
(1998), http://www.sciencedirect.com/science/
article/pii/S0304885397010147,
doi:10.1016/S0304-8853(97)01014-7, ISSN 0304-8853
29. Wolfram, S.: Universality and complexity in cellular automata. Physica D: Nonlinear
Phenomena 10(1-2), 1–35 (1984),
http://www.sciencedirect.com/science/
article/pii/0167278984902458,
doi:10.1016/0167-2789(84)90245-8, ISSN 0167-2789
30. Yates, R.C.: Curves and their properties. Classics in mathematics education. National
Council of Teachers of Mathematics (1974),
http://books.google.co.uk/books?id=UPs-AAAAIAAJ

Chapter 15
Real-Time Prime Generators Implemented
on Small-State Cellular Automata
Hiroshi Umeo, Kunio Miyamoto, and Yasuyuki Abe
Abstract. For a long time there was little use of prime numbers in practical ap-
plications. But nowadays, it has been known that large-scale prime numbers play
a very important role in encryption in computer security networks. In this paper,
we explore the prime generation problem on cellular automata consisting of in-
ﬁnitely many cells each with ﬁnite state memory and present two implementations
of real-time prime generators on cellular automata having smallest number of inter-
nal states, known at present. It is shown that there exists a real-time prime generator
on a 1-bit inter-cell communication cellular automaton with 25-states, which is an
improvement over a 34-state implementation given in Umeo and Kamikawa [2003].
In addition, we show that an inﬁnite prime sequence can be generated in real-time by
an eight-state cellular automaton with constant-bit communications. Both the algo-
rithms presented are based on the classical sieve of Eratosthenes, and our eight-state
implementation is an improvement over a nine-state prime generator developed by
Korec [1998]. Those two implementations on cellular automata with different com-
munication models are the smallest realizations in the number of states, at present.
15.1
Introduction
Cellular automata (CA) are considered to be a useful model of complex systems in
which an inﬁnite one-dimensional array of ﬁnite state machines (cells) updates itself
in a synchronous manner according to a uniform local rule. In the present paper, we
study the prime sequence generation problem on cellular automata. In recent years,
Hiroshi Umeo · Yasuyuki Abe
Univ. of Osaka Electro-Communication,
Neyagawa-shi, Hatsu-cho, 18-8, Osaka, 572-8530, Japan
e-mail: umeo@cyt.osakac.ac.jp
Kunio Miyamoto
Japan Advanced Institute of Science and Technology,
1-1, Asahidai, Nomi, Ishikawa, 923-1292, Japan
c⃝Springer International Publishing Switzerland 2015
341
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_15

342
H. Umeo, K. Miyamoto, and Y. Abe
C1
C2
C3
C4
Cn
Fig. 15.1 One-dimensional cellular automaton connected with 1-bit inter-cell communica-
tion links
it has been known that large-scale prime numbers play a very important role in
encryption in computer security networks.
First, we introduce the sequence generation problem on the cellular automaton
with 1-bit inter-cell communication, where each cell can communicate with its near-
est neighbor cells by sending and receiving a 1-bit information, and present a 25-
state real-time prime generator on the model. Then, we consider at each step the
same problem on an extended communication model, where each cell can send and
receive constant-bits more than one in local communications. We present an eight-
state real-time prime generation algorithm on the CA. Both algorithms are based on
the sieve of Eratosthenes. Due to the space available we only give an outline of our
two constructions.
15.2
Prime Generator on One-Bit Communication Cellular
Automata
15.2.1
One-Bit Communication Cellular Automata
A one-dimensional 1-bit inter-cell communication cellular automaton consists of an
inﬁnite array of identical ﬁnite state automata,each located at a positive integerpoint.
Each automaton is referred to as a cell. The cell at point i is denoted by Ci where
i ≥1. Each Ci, except for C1, is connected with its left and right neighbor cells via
a left and right one-way communication link, where those communication links are
indicated by right- and left-going arrows, respectively, as shown in Fig. 15.1. Each
one-way communication link can transmit only one bit at each step in each direction.
A cellular automatonwith 1-bit inter-cell communication(abbreviatedas CA1−bit)
consists of an inﬁnite array of ﬁnite state automaton A = (Q,δ), where
(1) Q is a ﬁnite set of internal states.
(2) δ is a function that deﬁnes the next state of any cell and its binary outputs to
its left and right neighbor cells such that δ: Q×{0,1}×{0,1} →Q×{0,1}×
{0,1}, where δ(p,x,y) = (q,x′,y′), p, q ∈Q, x,x′,y,y′ ∈{0,1}, has the fol-
lowing meaning: We assume that, at step t, the cell Ci is in state p and re-
ceives binary inputs x and y from its left and right communication links, respec-
tively. Then, at the next step t+1, Ci takes a state q and outputs x′ and y′ to its
left and right communication links, respectively. Note that the binary inputs to

15
Real-Time Prime Generators Implemented on Small-State Cellular Automata
343
Ci at step t are also outputs of Ci−1 and Ci+1 at step t. A quiescent state q ∈Q
has a property such that δ(q,0,0) = (q,0,0).
Thus, the CA1−bit is a special subclass of normal (i.e., conventional) cellular
automata. Let N be any normal cellular automaton with a set of states Q and a
transition function δ : Q3 →Q. The state of each cell on N depends on the previous
state of the cell and states of its nearest neighbor cells. This means that the amount
of information exchanged per step between neighboring cells is O(1) bits. Each
state in Q can be encoded with a binary sequence of length ⌈log2 |Q|⌉and then
transmitted by sending the binary sequences sequentially bit-by-bit in each direction
via each one-way communication link. The sequences are then received bit-by-bit
and decoded into their corresponding states in Q. Thus, the CA1−bit can simulate
one step of N in ⌈log2 |Q|⌉steps. This observation gives the following computational
relation between the normal CA and the CA1−bit.
Themrem 1 Mazoyer [1996], Umeo and Kamikawa [2002] Let N be any normal cellular au-
tomaton operating in T(n) steps with internal state set Q. Then, there exists a CA1−bit
that can simulate N in kT(n) steps, where k is a positive integer such that k =
⌈log2 |Q|⌉.
15.2.2
Sequence Generation Problem
We now deﬁne the sequence generation problem on the CA1−bit. Let M be a CA1−bit,
and let {tn|n = 1,2,3,...} be an inﬁnite monotonically increasing positive integer se-
quence deﬁned on natural numbers, such that tn ≥n for any n ≥1. We then have a
semi-inﬁnite array of cells, as shown in Fig. 15.1, and all cells, except for C1, are in
the quiescent state at time t = 0. In order to deﬁne the sequence generation problem,
we assume that the state set of the communication cell C1 is {0,1}. The communi-
cation cell C1 assumes a special state ”0” (zero) in Q and outputs 1 to its right com-
munication link at time t = 0 for initiation of the sequence generator. We say that M
generates a sequence {tn|n = 1,2,3,...} in k linear-time if and only if the leftmost
end cell of M falls into a special state ”1” (one) in Q at time t = ktn, where k is a posi-
tive integer. We call M a real-time generator when k = 1. In designing algorithms for
CA1−bit, for ease of understanding and description, we often use signals or waves,
instead of transition tables. A signal (wave) is an information ﬂow that is described
as a straight line in the space-time diagram. Note that any signal cannot propagate
at speed more than one cell per one step. Arisawa [1971], Fischer [1965], Korec
[1997, 1998] and Mazoyer and Terrier [1999] have considered the sequence genera-
tion problem on the cellular automata model. Umeo and Kamikawa [2002] showed
that inﬁnite non-regular sequences such as {2n|n = 1,2,3,..}, {n2|n = 1,2,3,..}
and Fibonacci sequences can be generated in real-time and the prime sequence in
twice real-time by CA1−bit. Umeo and Kamikawa [2003] showed that the prime se-
quence can be generated in real-time by a CA1−bit having 34 internal states and 107
transition rules.

344
H. Umeo, K. Miyamoto, and Y. Abe
C4
C7
C11
C16
C23
Cell Space
Time
C1
t = 0
t = 2
t = 3
t = 5
t = 7
t = 11
t = 13
t = 17
t = 19
t = 23
t = 29
t = 37
t = 31
t = 41
t = 43
t = 47
t = 53
t = 59
t = 61
t = 67
t = 71
t = 73
t = 79
e
1/3
b
c
b
b
a
a
c
c
e
1/3
e
1/3
f
1/2
f
1/2
f
1/2
a
a
a
a
a
a
a
a
a
b
b
b
b
b
b
b
b
b
a
a
a
c
c
: e-wave
: a-wave
: partition
: c-wave
: d-wave
: b-wave
: f-wave
d
1/3
d
1/3
d
1/3
UVGRU
UVGRU
UVGRU
Fig. 15.2 Space-time diagram for Korec’s 9-state real-time prime generator on constant-bit
communication model
15.2.3
Korec’s Prime Generator
Our prime generation algorithm is based on the well-known sieve of Eratosthenes.
Imagine a list of all integers greater than 2. The ﬁrst member, 2, becomes a prime
and every second member of the list is crossed out. Then, the next member of the
remainder of the list, 3, is a prime and every third member is crossed out. In Er-
atosthenes’ sieve, the procedure continues with 5, 7, 11, and so on. In our pro-
cedure, given below, for any odd integer k ≥3, every 2k-th member of the list
beginning with k2 will be crossed out, since the k-th members less than k2 (that

15
Real-Time Prime Generators Implemented on Small-State Cellular Automata
345
is, {i · k|2 ≤i ≤k −1}) and 2k-th members beginning with k2 + k (that is, it is an
even number such that {(k+2i−1)·k|i = 1,2,3,...}) should have been crossed out
in the previous stages. Thus, every k-th member beginning with k2 is successfully
crossed out in our procedure. Those integers never being crossed out are the primes.
Figure 15.2 is a space-time diagram that shows a real-time detection of every
multiples of odds. Each cross-out operation is performed by C1. We assume that
the cellular space is initially divided by the partitions such that a special mark “w”
is printed on cell C(i2+3i+4)/2, for any positive integer i ≥2. Those partitions will
be used to generate reciprocating signals for the detection of every multiples of
odds, for example, three, ﬁve, and seven, ..., . See Fig. 15.2. We denote the sub-
cellular space sandwiched by Ck and Cℓas Si, where k = (i2 + 3i + 4)/2 , ℓ=
(i2 + 7i + 14)/2, for any i ≥2, and call it the i-th partition. Note that Si contains
(2i + 3) cells, excluding both ends. How to set up the partitions in terms of 1-bit
communication is omitted. One can observe that: Let M be a CA1−bit. We assume
that the initial array of M is partitioned into ﬁnitely many blocks such that a special
symbol “w” is marked on cells C(i2+3i+4)/2, for any positive integer i ≥2. The array
M given above can generate the i-th prime at time t = i.
15.2.4
A 25-State Real-Time Prime Generator on CA1−bit
Based on the space-time diagram shown in Fig. 15.2, we construct a 25-state CA1−bit
that can generate primes in real-time. Figure 15.3 shows its transition table consist-
ing of 25 states and 86 local rules. In Figure 15.4 some snapshots on 34 cells for
real-time generation of primes are given. Small right- and left-facing black triangles,
▶and ◀, in the ﬁgure, indicate a 1-bit signal transfer in the right or left direction
between neighbor cells. The symbol in each cell shows its internal state.
Theorem 2. There exists a CA1−bit with 25-states and 86-rules that can generate
prime sequence in real-time.
15.3
Prime Generator on Constant-Bit Communication
Cellular Automata
15.3.1
Cellular Automaton with Constant-Bit Communication
A cellular automaton with constant-bit communication is a usual CA which consists
of an inﬁnite array of ﬁnite state automaton A = (Q,δ), where
(1) Q is a ﬁnite set of internal states.
(2) δ is a function that deﬁnes the next state of any cell such that δ: Q3 →Q where
δ(p,q,r) = q′, p,q,r, p′ ∈Q, has the following meaning: We assume that, at
step t, the cell Ci−1, Ci, and Ci+1 are in state p,q,r, respectively. Then, at the
next step t+1, Ci takes a state q′.

346
H. Umeo, K. Miyamoto, and Y. Abe
Current
state
Input from right link
Input
from
left link
(next state,
left output,
right output)
2
R = 0
R = 1
L = 0
L = 1
1
--
(1,0,1)
(E3,0,1)
(E3,0,1)
3
R = 0
R = 1
L = 0
L = 1
.
(.,0,0)
(F4,1,0)
(H1,1,1)
--
4
R = 0
R = 1
L = 0
L = 1
F3
(-,0,0)
(C4,1,0)
--
(F4,1,1)
5
R = 0
R = 1
L = 0
L = 1
F5
(C3,0,1)
(C2,1,0)
--
(E2,1,0)
6
R = 0
R = 1
L = 0
L = 1
B2
--
(H1,0,1)
(B1,1,0)
(B1,0,0)
7
R = 0
R = 1
L = 0
L = 1
B1
--
(E2,1,1)
(B2,0,0)
(C3,1,0)
8
R = 0
R = 1
L = 0
L = 1
E1
(E2,0,1)
(E2,0,0)
--
(E4,0,1)
9
R = 0
R = 1
L = 0
L = 1
E2
(-,1,0)
(E3,0,1)
(-,1,1)
(E2,0,1)
10
R = 0
R = 1
L = 0
L = 1
E3
(E1,0,1)
(-,0,0)
--
--
11
R = 0
R = 1
L = 0
L = 1
E4
(-,0,0)
(-,0,0)
(E4,1,1)
(E4,0,1)
12
R = 0
R = 1
L = 0
L = 1
-
(-,0,0)
(-,1,0)
(^,0,1)
(E1,0,0)
13
R = 0
R = 1
L = 0
L = 1
F1
(F2,0,0)
(F4,1,0)
(F2,1,0)
--
14
R = 0
R = 1
L = 0
L = 1
F2
(F3,0,1)
(F3,0,0)
(D3,1,1)
(F5,0,1)
15
R = 0
R = 1
L = 0
L = 1
F4
(D1,0,1)
(F5,1,1)
(-,1,1)
--
16
R = 0
R = 1
L = 0
L = 1
^
(^,0,0)
(-,1,0)
(F1,0,0)
--
17
R = 0
R = 1
L = 0
L = 1
H1
(F2,1,1)
(C1,1,0)
(B2,1,0)
(E2,1,1)
18
R = 0
R = 1
L = 0
L = 1
C3
(C4,0,1)
(C4,0,1)
(E3,0,1)
(C2,1,1)
19
R = 0
R = 1
L = 0
L = 1
C4
(C3,0,1)
(C3,0,1)
(B2,0,1)
(D3,0,0)
20
R = 0
R = 1
L = 0
L = 1
C1
(C1,1,1)
(C1,0,1)
(C1,0,1)
(C2,1,1)
21
R = 0
R = 1
L = 0
L = 1
C2
(C2,1,1)
(C2,0,1)
(D4,1,0)
(D3,0,1)
22
R = 0
R = 1
L = 0
L = 1
D1
(.,0,1)
(D2,0,1)
(D2,1,0)
(D2,1,1)
23
R = 0
R = 1
L = 0
L = 1
D2
--
--
(D1,1,0)
(D1,0,0)
24
R = 0
R = 1
L = 0
L = 1
D3
(D4,1,1)
(D4,1,1)
(D4,1,0)
(D4,1,1)
25
R = 0
R = 1
L = 0
L = 1
D4
(D4,1,0)
(C1,1,1)
(D3,1,0)
(D1,1,0)
1
R = 0
R = 1
L = 0
L = 1
0
(0,0,1)
(1,0,1)
(1,0,1)
(1,1,1)
Fig. 15.3 Transition rule set for the 25-state real-time prime generator on CA1−bit
Figure 15.5 shows a semi-inﬁnite one-dimensional cellular array (CA) consisting
of cells, denoted by C1, C2, ..., Ci, ..., i ≥1. Each cell is an identical (except the bor-
der cells) ﬁnite-state automaton. The array operates in lock-step mode in such a way
that the next state of each cell (except border cells) is determined by both its own
present state and the present states of its left and right neighbors. All cells, except
the left end cell, are initially in the quiescent state at time t = 0 with the property
that the next state of a quiescent cell with quiescent neighbors is the quiescent state
again. At time t = 0, the left end cell C1 is in a special state acting as an initiation
signal for the array. Let st
i be state of the cell Ci at time t, where t ≥0,i ≥1.

15
Real-Time Prime Generators Implemented on Small-State Cellular Automata
347
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1
0
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
1
B2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
1
B1
F2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
0
C3
F3
B2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
1
E3
C4
H1
F2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
6
0
0
B2
C1
F5
B2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
1
1
B1
C1
C2
B1
F2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
0
E3
B2
C1
D4
E2
F3
B2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9
0
0
B1
C1
D1
-
F4
H1
F2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10
0
1
B2
C1
D2
-
D1
E2
F5
B2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
1
E3
B1
C1
D1
^
D2
-
E2
B1
F2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
0
0
B2
C1
D2
^
D1
E1
-
E2
F3
B2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
13
1
1
B1
C1
D1
^
.
E2
-
-
F4
H1
F2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
0
E3
B2
C1
D2
^
.
E2
-
-
-
E2
F5
B2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15
0
0
B1
C1
D1
^
.
E3
E1
-
-
-
E2
B1
F2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
16
0
1
B2
C1
D2
^
.
E1
E4
-
-
-
-
E2
F3
B2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
1
E3
B1
C1
D1
^
.
E2
E4
E1
-
-
-
-
F4
H1
F2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
0
0
B2
C1
D2
^
.
-
E4
E4
-
-
-
-
-
E2
F5
B2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
19
1
1
B1
C1
D1
^
F4
-
-
E4
E1
-
-
-
-
-
E2
B1
F2
H1
.
.
.
.
.
.
.
.
.
.
.
.
.
20
0
E3
B2
C1
D2
-
F5
-
-
-
E4
-
-
-
-
-
-
E2
F3
B2
H1
.
.
.
.
.
.
.
.
.
.
.
.
21
0
0
B1
C1
D1
-
C3
^
-
-
-
E1
-
-
-
-
-
-
F4
H1
F2
H1
.
.
.
.
.
.
.
.
.
.
.
22
0
1
B2
C1
D2
-
C4
F1
^
-
-
E2
-
-
-
-
-
-
-
E2
F5
B2
H1
.
.
.
.
.
.
.
.
.
.
23
1
E3
B1
C1
D1
^
C3
F2
^
^
-
E3
-
-
-
-
-
-
-
-
E2
B1
F2
H1
.
.
.
.
.
.
.
.
.
24
0
0
B2
C1
D2
^
C2
D3
^
^
^
-
E1
-
-
-
-
-
-
-
-
E2
F3
B2
H1
.
.
.
.
.
.
.
.
25
0
1
B1
C1
D1
-
C2
D4
F1
^
^
^
E2
-
-
-
-
-
-
-
-
-
F4
H1
F2
H1
.
.
.
.
.
.
.
26
0
E3
B2
C1
D2
-
C2
D3
F2
^
^
^
E2
-
-
-
-
-
-
-
-
-
-
E2
F5
B2
H1
.
.
.
.
.
.
27
0
0
B1
C1
D1
^
C2
D4
F3
^
^
^
E3
E1
-
-
-
-
-
-
-
-
-
-
E2
B1
F2
H1
.
.
.
.
.
28
0
1
B2
C1
D2
^
D3
D3
-
F1
^
^
E1
E4
-
-
-
-
-
-
-
-
-
-
-
E2
F3
B2
H1
.
.
.
.
29
1
E3
B1
C1
D1
^
D4
D4
-
F2
^
^
E2
E4
E1
-
-
-
-
-
-
-
-
-
-
-
F4
H1
F2
H1
.
.
.
30
0
0
B2
C1
D2
-
C1
D3
-
F3
^
^
-
E4
E4
-
-
-
-
-
-
-
-
-
-
-
-
E2
F5
B2
H1
.
.
31
1
1
B1
C1
D1
-
C1
D4
-
-
F1
-
-
-
E4
E1
-
-
-
-
-
-
-
-
-
-
-
-
E2
B1
F2
H1
.
32
0
E3
B2
C1
D2
-
C1
D3
-
-
F4
-
-
-
-
E4
-
-
-
-
-
-
-
-
-
-
-
-
-
E2
F3
B2
H1
33
0
0
B1
C1
D1
^
C1
D4
-
-
F5
-
-
-
-
-
E1
-
-
-
-
-
-
-
-
-
-
-
-
-
F4
H1
F2
34
0
1
B2
C1
D2
^
C2
D3
-
-
C3
^
-
-
-
-
E2
-
-
-
-
-
-
-
-
-
-
-
-
-
-
E2
F5
35
0
E3
B1
C1
D1
-
C2
D4
-
-
C4
F1
^
-
-
-
E3
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
E2
36
0
0
B2
C1
D2
-
C2
D1
^
-
C3
F2
^
^
-
-
-
E1
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
37
1
1
B1
C1
D1
^
C2
D2
^
^
C4
D3
^
^
^
-
-
E2
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
38
0
E3
B2
C1
D2
^
D3
D1
^
^
D3
D4
F1
^
^
^
-
E3
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
39
0
0
B1
C1
D1
^
D4
D2
^
^
D4
D4
F2
^
^
^
^
-
E1
-
-
-
-
-
-
-
-
-
-
-
-
-
-
40
0
1
B2
C1
D2
-
C1
D1
^
-
C1
D3
F3
^
^
^
^
^
E2
-
-
-
-
-
-
-
-
-
-
-
-
-
-
41
1
E3
B1
C1
D1
-
C1
D2
-
-
C1
D4
-
F1
^
^
^
^
E2
-
-
-
-
-
-
-
-
-
-
-
-
-
-
42
0
0
B2
C1
D2
-
C1
D1
-
-
C1
D3
-
F2
^
^
^
^
E3
E1
-
-
-
-
-
-
-
-
-
-
-
-
-
43
1
1
B1
C1
D1
^
C1
D2
-
-
C1
D4
-
F3
^
^
^
^
E1
E4
-
-
-
-
-
-
-
-
-
-
-
-
-
44
0
E3
B2
C1
D2
-
C2
D1
^
-
C1
D3
-
-
F1
^
^
^
E2
E4
E1
-
-
-
-
-
-
-
-
-
-
-
-
45
0
0
B1
C1
D1
-
C2
D2
^
^
C1
D4
-
-
F2
^
^
^
-
E4
E4
-
-
-
-
-
-
-
-
-
-
-
-
46
0
1
B2
C1
D2
-
C2
D1
^
^
C2
D3
-
-
F3
^
^
-
-
-
E4
E1
-
-
-
-
-
-
-
-
-
-
-
47
1
E3
B1
C1
D1
^
C2
D2
^
-
C2
D4
-
-
-
F1
-
-
-
-
-
E4
-
-
-
-
-
-
-
-
-
-
-
48
0
0
B2
C1
D2
^
D3
D1
-
-
C2
D3
-
-
-
F4
-
-
-
-
-
-
E1
-
-
-
-
-
-
-
-
-
-
49
0
1
B1
C1
D1
^
D4
D2
-
-
C2
D4
-
-
-
F5
-
-
-
-
-
-
E2
-
-
-
-
-
-
-
-
-
-
50
0
E3
B2
C1
D2
-
C1
D1
^
-
C2
D3
-
-
-
C3
^
-
-
-
-
-
E3
-
-
-
-
-
-
-
-
-
-
51
0
0
B1
C1
D1
-
C1
D2
^
^
C2
D4
-
-
-
C4
F1
^
-
-
-
-
-
E1
-
-
-
-
-
-
-
-
-
52
0
1
B2
C1
D2
-
C1
D1
^
^
D3
D1
-
-
-
C3
F2
^
^
-
-
-
-
E2
-
-
-
-
-
-
-
-
-
53
1
E3
B1
C1
D1
^
C1
D2
^
^
D4
D2
-
-
-
C4
D3
^
^
^
-
-
-
E3
-
-
-
-
-
-
-
-
-
54
0
0
B2
C1
D2
^
C2
D1
^
-
C1
D1
^
-
-
C3
D4
F1
^
^
^
-
-
-
E1
-
-
-
-
-
-
-
-
55
0
1
B1
C1
D1
-
C2
D2
-
-
C1
D2
^
^
-
C4
D3
F2
^
^
^
^
-
-
E2
-
-
-
-
-
-
-
-
56
0
E3
B2
C1
D2
-
C2
D1
-
-
C1
D1
^
^
^
C3
D4
F3
^
^
^
^
^
-
E3
-
-
-
-
-
-
-
-
57
0
0
B1
C1
D1
^
C2
D2
-
-
C1
D2
^
^
^
C2
D3
-
F1
^
^
^
^
^
-
E1
-
-
-
-
-
-
-
58
0
1
B2
C1
D2
-
D3
D1
^
-
C1
D1
^
^
-
C2
D4
-
F2
^
^
^
^
^
^
E2
-
-
-
-
-
-
-
59
1
E3
B1
C1
D1
-
D4
D2
^
^
C1
D2
^
-
-
C2
D3
-
F3
^
^
^
^
^
^
E2
-
-
-
-
-
-
-
60
0
0
B2
C1
D2
-
C1
D1
^
^
C2
D1
-
-
-
C2
D4
-
-
F1
^
^
^
^
^
E3
E1
-
-
-
-
-
-
61
1
1
B1
C1
D1
-
C1
D2
^
-
C2
D2
-
-
-
C2
D3
-
-
F2
^
^
^
^
^
E1
E4
-
-
-
-
-
-
62
0
E3
B2
C1
D2
-
C1
D1
-
-
C2
D1
^
-
-
C2
D4
-
-
F3
^
^
^
^
^
E2
E4
E1
-
-
-
-
-
63
0
0
B1
C1
D1
^
C1
D2
-
-
C2
D2
^
^
-
C2
D3
-
-
-
F1
^
^
^
^
-
E4
E4
-
-
-
-
-
64
0
1
B2
C1
D2
^
C2
D1
^
-
C2
D1
^
^
^
C2
D4
-
-
-
F2
^
^
^
-
-
-
E4
E1
-
-
-
-
65
0
E3
B1
C1
D1
-
C2
D2
^
^
C2
D2
^
^
^
D3
D3
-
-
-
F3
^
^
-
-
-
-
-
E4
-
-
-
-
66
0
0
B2
C1
D2
-
C2
D1
^
^
D3
D1
^
^
^
D4
D4
-
-
-
-
F1
-
-
-
-
-
-
-
E1
-
-
-
67
1
1
B1
C1
D1
^
C2
D2
^
^
D4
D2
^
^
-
C1
D3
-
-
-
-
F4
-
-
-
-
-
-
-
E2
-
-
-
68
0
E3
B2
C1
D2
^
D3
D1
^
-
C1
D1
^
-
-
C1
D4
-
-
-
-
F5
-
-
-
-
-
-
-
E3
-
-
-
69
0
0
B1
C1
D1
^
D4
D2
-
-
C1
D2
-
-
-
C1
D3
-
-
-
-
C3
^
-
-
-
-
-
-
-
E1
-
-
70
0
1
B2
C1
D2
-
C1
D1
-
-
C1
D1
-
-
-
C1
D4
-
-
-
-
C4
F1
^
-
-
-
-
-
-
E2
-
-
71
1
E3
B1
C1
D1
-
C1
D2
-
-
C1
D2
-
-
-
C1
D3
-
-
-
-
C3
F2
^
^
-
-
-
-
-
E3
-
-
72
0
0
B2
C1
D2
-
C1
D1
^
-
C1
D1
^
-
-
C1
D4
-
-
-
-
C4
D3
^
^
^
-
-
-
-
-
E1
-
73
1
1
B1
C1
D1
^
C1
D2
-
^
C1
D2
^
^
-
C1
D1
^
-
-
-
C3
D4
F1
^
^
^
-
-
-
-
E2
-
74
0
E3
B2
C1
D2
^
C2
D1
-
^
C2
D1
^
^
^
C1
D2
^
^
-
-
C4
D3
F2
^
^
^
^
-
-
-
E3
-
Fig. 15.4 Snapshots of conﬁgurations for real-time prime generation on the 25-state CA1−bit

348
H. Umeo, K. Miyamoto, and Y. Abe
1
2
3
n
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.


time t = 0
Fig. 15.5 A one-dimensional cellular automaton with constant-bit communication (above)
and its initial conﬁguration at time t = 0 (below)
15.3.2
Eight-State Real-Time Prime Generator on CA with
O(1)-Bit Communication
In this section, we present an eight-state real-time prime generation algorithm on
CA. The algorithm is implemented on an eight-state CA using 305 transition rules.
Our prime generation algorithm is also based on the sieve of Eratosthenes employed
in the previous section. Its cross out technique is different from the one mentioned
above. Imagine a list of all integers greater than 2. The ﬁrst member, 2, becomes a
prime and every second member of the list is crossed out. Then, the next member
of the remainder of the list, 3, is a prime and every third member is crossed out.
In Eratosthenes’ sieve, the procedure continues with 5, 7, 11, and so on. In our
procedure, given below, for any odd integer k ≥3, every 2k-th member of the list
beginning with k2 will be crossed out, since the k-th members less than k2 (that is,
{i·k|2 ≤i ≤k−1}) and 2k-th members beginning with k2 +k (that is, it is an even
number such that {(k+2i−1)·k|i = 1,2,3,...}) should have been crossed out in the
previous stages. Thus, every k-th member beginning with k2 is successfully crossed
out in our procedure. Those integers never being crossed out are the primes.
Figure 15.6 is a space-time diagram that shows a real-time detection of odd mul-
tiples of odds.
We now outline the algorithm. Each cross-out operation is performed by C1. We
assume that the cellular space is initially divided by the partitions such that a special
mark “w” is printed on cell Ci2, for any positive integer i ≥1. Those partitions
will be used to generate reciprocating signals for the detection of odd multiples of,
for example, three, ﬁve, and seven (See Fig. 15.6). We denote a sub-cellular space
sandwiched by Ck and Cℓas Si, where k = i2 , ℓ= (i+1)2 for any i ≥1, and call it
the i-th partition. Note that Si contains (2i+2) cells, including both ends.

15
Real-Time Prime Generators Implemented on Small-State Cellular Automata
349
C4
C9
C25
C16
Cellular Space
t = 0
t = 2
Time
t = 3
t = 5
t = 7
t = 11
t =13
t =17
t = 19
t = 23
t = 29
t = 37
t = 31
t = 41
t = 43
t = 47
t = 53
t = 59
t = 61
t = 67
t = 71
t = 73
C1
S2
S3
S4
S1
: a-wave
: b-wave
: partition
.  .  .  .  .  .  .
Fig. 15.6 Space-time diagram for real-time detection of odd multiples of odds

350
H. Umeo, K. Miyamoto, and Y. Abe
.
Right State
.
0
1
V
R
L
r
/
*
Left State
.
.
.
/
.
.
L
/
/
.
0
/
0
/
.
1
.
.
.
L
/
/
V
.
1
.
L
/
/
R
R
R
r
R
R
R
r
L
.
.
/
.
0
/
r
L
r
/
.
/
.
.
L
/
*
0
Right State
.
0
1
V
R
L
r
/
*
Left State
.
.
V
0
R
0
r
.
R
0
0
1
R
1
R
1
0
V
1
r
R
0
/
V
L
V
R
V
R
r
0
/
/
0
L
*
0
0
0
1
1
0
0
1
1
Right State
.
0
1
V
R
L
r
/
*
Left State
.
V
V
1
1
1
0
R
R
V
L
L
L
0
1
.
.
/
L
L
V
1
1
1
1
1
1
.
1
R
V
V
1
1
L
V
V
1
1
1
r
V
V
1
1
/
V
.
V
1
1
*
1
0
0
0
0
V
Right State
.
0
1
V
R
L
r
/
*
Left State
.
V
0
V
V
V
1
1
0
L
L
1
R
1
L
1
V
V
1
R
V
0
1
1
L
V
1
1
r
V
1
1
/
V
1
1
1
*
R
Right State
.
0
1
V
R
L
r
/
*
Left State
.
.
.
L
L
.
.
.
/
.
0
R
V
V
1
1
1
.
.
L
L
0
.
/
V
.
.
L
R
R
r
r
V
R
L
L
L
L
/
r
r
L
L
.
/
.
L
L
r
/
*
L
Right State
.
0
1
V
R
L
r
/
*
Left State
.
.
.
/
.
L
.
/
0
R
R
r
r
1
R
R
r
R
R
r
V
R
R
r
R
R
r
R
L
r
R
V
L
V
r
R
R
r
r
R
/
.
.
.
.
*
r
Right State
.
0
1
V
R
L
r
/
*
Left State
.
.
L
/
0
R
V
1
1
1
.
0
L
L
R
/
V
V
R
R
0
R
L
V
L
L
r
V
L
L
r
/
L
/
*
/
Right State
.
0
1
V
R
L
r
/
*
Left State
.
.
L
/
.
.
L
/
/
0
0
V
1
.
.
L
/
V
.
1
.
L
/
/
R
R
0
r
R
r
r
L
.
/
.
/
r
R
R
/
.
/
.
.
L
/
*
Fig. 15.7 Transition table for the eight-state real-time prime generator
Based on the space-time diagram shown in Fig. 15.6, we construct an 8-state
CA that can generate primes in real-time. Figure 15.7 shows its transition table
consisting of 8 states and 305 local rules. In Figure 15.8 some snapshots on 50 cells
are given for real-time generation of primes.
Theorem 3. There exists a cellular automaton having 8-states and 305-rules that can
generate prime sequence in real-time.

15
Real-Time Prime Generators Implemented on Small-State Cellular Automata
351
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
1
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
1
R
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
0
/
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
1
V
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
6
0
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
1
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
0
0
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9
0
1
V
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10
0
V
L
L
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
1
L
R
R
/
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
0
R
/
V
r
r
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
13
1
1
R
1
R
r
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
0
L
L
V
.
0
L
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15
0
r
R
V
1
r
R
0
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
16
0
V
L
0
.
R
r
0
/
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
1
L
R
V
/
.
R
/
0
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
0
R
L
1
.
.
/
0
0
/
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
19
1
1
r
V
.
/
L
L
R
0
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
0
L
L
V
/
L
.
R
L
0
/
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21
0
r
R
1
L
.
0
.
L
R
0
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
22
0
V
L
1
R
.
.
.
L
L
0
/
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
1
L
r
V
.
R
.
L
.
V
R
0
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
24
0
r
L
V
.
.
R
.
.
V
.
0
/
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
25
0
1
R
V
.
.
.
R
.
V
1
R
0
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26
0
L
L
V
.
.
.
.
R
V
1
.
0
/
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
27
0
r
R
V
.
.
.
.
L
0
1
.
R
0
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
28
0
V
L
V
.
.
.
L
.
V
R
.
.
0
/
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
1
L
R
V
.
.
L
.
.
V
.
R
.
R
0
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30
0
R
L
V
.
L
.
.
.
V
.
.
R
.
0
/
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
31
1
1
R
V
L
.
.
.
.
V
.
.
.
R
R
0
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32
0
L
L
1
R
.
.
.
.
V
.
.
.
.
r
0
/
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
33
0
r
r
V
.
R
.
.
.
V
.
.
.
/
.
/
0
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
34
0
1
L
V
.
.
R
.
.
V
.
.
/
.
/
L
0
/
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
35
0
L
R
V
.
.
.
R
.
V
.
/
.
/
L
.
R
0
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
36
0
R
L
V
.
.
.
.
R
V
/
.
/
L
.
0
.
0
/
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
37
1
1
R
V
.
.
.
.
L
1
.
/
L
.
.
.
0
R
0
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
.
38
0
L
L
V
.
.
.
L
/
V
/
L
.
.
.
.
0
V
0
/
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
.
39
0
r
R
V
.
.
L
/
.
1
L
.
.
.
.
.
R
L
r
0
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
.
40
0
V
L
V
.
L
/
.
/
1
R
.
.
.
.
.
.
V
V
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
.
41
1
L
R
V
L
/
.
/
/
V
.
R
.
.
.
.
.
V
V
r
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
.
42
0
R
L
1
r
.
/
/
.
V
.
.
R
.
.
.
.
V
1
V
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
.
43
1
1
r
1
.
r
/
.
.
V
.
.
.
R
.
.
.
V
1
1
r
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
.
44
0
L
L
V
/
/
R
.
.
V
.
.
.
.
R
.
.
V
1
L
0
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
.
45
0
r
R
1
/
.
.
R
.
V
.
.
.
.
.
R
.
V
1
R
R
R
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
.
46
0
V
L
1
.
.
.
.
R
V
.
.
.
.
.
.
R
V
1
0
r
r
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
.
47
1
L
r
V
.
.
.
.
L
V
.
.
.
.
.
.
L
0
1
1
1
V
/
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
.
48
0
r
L
V
.
.
.
L
.
V
.
.
.
.
.
L
.
V
R
.
/
1
1
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
.
49
0
1
R
V
.
.
L
.
.
V
.
.
.
.
L
.
.
V
.
r
/
.
.
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
.
50
0
L
L
V
.
L
.
.
.
V
.
.
.
L
.
.
.
V
/
/
R
.
.
V
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
.
51
0
r
R
V
L
.
.
.
.
V
.
.
L
.
.
.
.
1
/
.
.
R
.
0
r
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
R
52
0
V
L
1
R
.
.
.
.
V
.
L
.
.
.
.
/
1
.
.
.
.
R
.
R
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
.
53
1
L
r
V
.
R
.
.
.
V
L
.
.
.
.
/
/
V
.
.
.
.
.
R
.
V
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
.
54
0
r
L
V
.
.
R
.
.
1
R
.
.
.
/
/
.
V
.
.
.
.
.
.
R
0
1
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
R
55
0
1
R
V
.
.
.
R
/
V
.
R
.
/
/
.
.
V
.
.
.
.
.
.
.
/
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
R
56
0
L
L
V
.
.
.
/
R
V
.
.
r
/
.
.
.
V
.
.
.
.
.
.
/
L
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
57
0
r
R
V
.
.
/
.
L
V
.
/
/
R
.
.
.
V
.
.
.
.
.
/
L
.
R
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
58
0
V
L
V
.
/
.
L
.
V
/
/
.
.
R
.
.
V
.
.
.
.
/
L
.
0
.
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
59
1
L
R
V
/
.
L
.
.
1
/
.
.
.
.
R
.
V
.
.
.
/
L
.
.
.
0
R
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
60
0
R
L
1
.
L
.
.
/
1
.
.
.
.
.
.
R
V
.
.
/
L
.
.
.
.
0
V
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
61
1
1
r
V
L
.
.
/
/
V
.
.
.
.
.
.
L
V
.
/
L
.
.
.
.
.
R
L
r
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
62
0
L
L
1
R
.
/
/
.
V
.
.
.
.
.
L
.
V
/
L
.
.
.
.
.
.
.
V
V
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
63
0
r
r
V
.
r
/
.
.
V
.
.
.
.
L
.
.
1
L
.
.
.
.
.
.
.
.
V
V
r
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
64
0
1
L
V
/
/
R
.
.
V
.
.
.
L
.
.
/
1
R
.
.
.
.
.
.
.
.
V
1
V
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
65
0
L
R
1
/
.
.
R
.
V
.
.
L
.
.
/
/
V
.
R
.
.
.
.
.
.
.
V
1
1
r
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
66
0
R
L
1
.
.
.
.
R
V
.
L
.
.
/
/
.
V
.
.
R
.
.
.
.
.
.
V
1
L
0
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
67
1
1
r
V
.
.
.
.
L
V
L
.
.
/
/
.
.
V
.
.
.
R
.
.
.
.
.
V
1
R
R
R
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
68
0
L
L
V
.
.
.
L
.
1
R
.
/
/
.
.
.
V
.
.
.
.
R
.
.
.
.
V
1
0
r
r
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
69
0
r
R
V
.
.
L
.
/
V
.
r
/
.
.
.
.
V
.
.
.
.
.
R
.
.
.
V
1
1
1
V
/
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
70
0
V
L
V
.
L
.
/
.
V
/
/
R
.
.
.
.
V
.
.
.
.
.
.
R
.
.
V
1
.
/
1
1
0
/
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Fig. 15.8 Snapshots for the eight-state real-time prime generator on 50 cells

352
H. Umeo, K. Miyamoto, and Y. Abe
15.4
Conclusions
We have studied the prime generation problem on cellular automata and presented
two constructions of real-time prime generators on cellular automata having the
smallest number of internal states, known at present. It is shown that there exists a
real-time prime generator on 1-bit inter-cell communication cellular automaton with
25-states, which is an improvement over a 34-state implementation given in Umeo
and Kamikawa [2003].
In addition, we show that the inﬁnite prime sequence can be generated in real-
time by an eight-state cellular automaton with constant-bit communications. Our
eight-state implementation is an improvement over a nine-state prime generator pre-
sented in Korec [1998]. Those two constructions on cellular automata with differ-
ent communication models are the smallest realizations in the number of states, at
present.
References
1. Arisawa, M.: On the generation of integer series by the one-dimensional iterative arrays
of ﬁnite state machines. Trans. of IECE, 71/8 54-C(8), 759–766 (1971)
2. Dyer, C.R.: One-way bounded cellular automata. Information and Control 44, 261–281
(1980)
3. Fischer, P.C.: Generation of primes by a one-dimensional real-time iterative array. J. of
ACM 12(3), 388–394 (1965)
4. Kamikawa, N., Umeo, H.: A study on sequence generation powers of small cellular
automata. SICE Journal of Control, Measurement, and System Integration 11(1), 1–8
(2011)
5. Korec, I.: Real-time generation of primes by a one-dimensional cellular automaton with
11 states. In: Privara, I., Ruˇziˇcka, P. (eds.) MFCS 1997. LNCS, vol. 1295, pp. 358–367.
Springer, Heidelberg (1997)
6. Korec, I.: Real-time generation of primes by a one-dimensional cellular automaton with 9
states. In: Margenstern, M. (ed.) Proc. of International Colloquim on Universal Machines
and Computation, MCU 1998, vol. I, pp. 101–116. IUT Metz (1998)
7. Mazoyer, J.: On optimal solutions to the ﬁring squad synchronization problem. Theoret-
ical Computer Science 168, 367–404 (1996)
8. Mazoyer, J., Terrier, V.: Signals in one-dimensional cellular automata. Theoretical Com-
puter Science 217, 53–80 (1999)
9. Smith, A.R.: Real-time language recognition by one-dimensional cellular automata. J. of
Computer and System Sciences 6, 233–253 (1972)
10. Umeo, H.: Problem solving on one-bit-communication cellular automata. In: Hoekstra,
A.G., Kroc, J., Sloot, P.M.A. (eds.) Simulating Complex Systems by Cellular Automata,
ch. 6, pp. 117–144. Springer, Heidelberg (2010)
11. Umeo, H., Kamikawa, N.: A design of real-time non-regular sequence generation algo-
rithms and their implementations on cellular automata with 1-bit inter-cell communica-
tions. Fundamenta Informaticae 52(1-3), 255–273 (2002)
12. Umeo, H., Kamikawa, N.: Real-time generation of primes by a 1-bit-communication
cellular automaton. Fundamenta Informaticae 58(3, 4), 421–435 (2003)

Chapter 16
Phyllosilicate Automata
Andrew Adamatzky
Abstract. The chapter is an overview of our ﬁnding on a novel class of regular
automata networks, the phyllosilicate automata. Phyllosilicate is a sheet of silicate
tetrahedra bound by basal oxygens. A phyllosilicate automaton is a regular network
of ﬁnite state machines, which mimics structure of the phyllosilicate. A node of a
binary state phyllosilicate automaton takes states 0 and 1. A node updates its state
in discrete time depending on a sum of states of its three (silicon nodes) or six (oxy-
gen nodes) closest neighbours. By extensive sampling of the node state transition
rule space we classify rules by main types of patterns generated by them based on
the patterns shape (convex and concave hulls, almost circularly growing patterns,
octagonal patterns, dendritic growth); and, the patterns interior (disordered, solid,
labyrinthine). We also present rules exhibiting travelling localizations attributed to
Conway’s Game of Life: gliders, oscillators, still lifes, and a glider gun.
16.1
Introduction
Phyllosilicates are parallel sheets of silicate tetrahedra, they are widely present in
nature and typically found in clay-related minerals on the Earth surface [15,16,23].
Phyllosilicates are used in the development of nano-materials, nano-wires and pat-
terned surfaces for nano-biological interfaces [18,34,41]. They are also employed,
in a form of cation-exchanged sheet silicates, as catalysts in chemical reactions [10],
e.g. nickel phyllosilicate catalysts [25,33,40].
We continue lines of enquiry into space-time dynamics of cellular automata on
non-orthogonal and aperiodic lattices, including triangular tessellations and Pen-
rose tilings and hyperbolic planes [14, 21, 28–32, 35]. We introduce and study an
automaton network — phyllosilicate automata — where connections between ﬁ-
nite automata are inspired by simpliﬁed structure of silicate sheets, lattices of con-
Andrew Adamatzky
Unconventional Computing Centre, University of the West of England, Bristol, UK
e-mail: andrew.adamatzmy@uwe.ac.uk
c⃝Springer International Publishing Switzerland 2015
353
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_16

354
A. Adamatzky
nected tetrahedra, and investigate the dynamics of perturbations on these automata
networks. In the chapter we overview results of our studies of phenomenological
automata, thus summarising our line of research published in [7–9].
Automata models studied in the paper are abstractions of silicon sheets. The au-
tomata models do not aim to compete with mainstream computational chemistry
models [38], however they do bring certain beneﬁts. Namely, an array of ﬁnite state
machines is a fast prototyping tool for an express evaluation of a space-time dynam-
ics of a spatially-extended active nonlinear medium for different local transitions
rules, and prototyping of unconventional computing devices based on the nonlinear
medium. Compare, for example, three-state cellular automata and excitable chem-
ical media. Their complexity differ by orders. A cellular automaton’s behaviour
can be expressed in a short table or set of rules while proper computer representa-
tion of an excitable medium requires dozens of partial differential equations. How-
ever, high degree of descriptional complexity does not always imply high degree
of behavioural. All types of travelling waves, including spiral and target waves
in excitable mode and wave-fragment observed mode are sufﬁciently imitated in
Greenberg-Hasting automata [22]. The automaton model of Belousov-Zhabotinsky
medium is phenomenologically equivalent to more accurate, close to physical and
chemical reality, Oregonator model [20]. By analogy, studies of phyllosilicate au-
tomata may help us to get an insight into dynamics of localizations, defects and
excitations, in the lattices of silicon tetrahedra. Thus, by developing automata mod-
els of excitation in tetrahedra sheets we contribute towards future developments of
silicate sheet based computing circuits where quanta of information are transferred
and processed by stationary and travelling localised excitations [3,6].
16.2
Phillosilicate Automata
Phyllosilicates are sheets of coordinated (SiO4)4−tetrahedra units. Each tetrahe-
dron shares its three corner basal oxygens of neighbouring tetrahedra [16, 26, 36]
(Fig. 16.1). Vacant oxygens of all tetrahedra point outside the lattice, away of tetra-
hedra; these apical oxygens are not taken into account in our present simulation.
A phyllosilicate automaton A is a two-dimensional regular network — as shown
in Fig. 16.1 — of ﬁnite state machines, or automata [7–9]. There are two types of
automata in the network: silicon automata s (centre vertex of each tetrahedron) and
oxygen automata o (corner vertices of each tetrahedron). A silicon automaton has
three neighbours. An oxygen automaton has six neighbours (Fig. 16.1).
The automata take two states, 0 (resting state) and 1 (non-resting state), and up-
date their states st and ot simultaneously and in discrete time t depending on the
states of their neighbours. Let σt(a) be a number of 1-state neighbours of automa-
ton a, 0 ≤σt(s) ≤3 and 0 ≤σt(o) ≤6. Let o and s be instances of oxygen and
silicon automata, and o and s be binary strings of seven and four symbols each; a[i]
is a symbol at ith position of string a. An automaton a, a = o or s, updates its state
by the rule:

16
Phyllosilicate Automata
355
Fig. 16.1 Phillosilicate automata. Automata representing silicon are small discs, oxygen au-
tomata are large discs. Unshared oxygen atoms of the tetrahedra are not shown.
at+1 = aat[σ(a)t],
(16.1)
where a = s or o and a = s or o. When referring to any particular rule (s0,s1,o0,o1),
where string ai determines next state of automaton being in state i, we will be us-
ing decimal representations of strings as R(dec(s0),dec(s1),dec(o0),dec(s1)). We
assume state 0 is a quiescent state: a node in state 0 will not take state 1 if all its
neighbours are 0. Sometimes we will be calling nodes in state 1 resting nodes. To
avoid rules which do not produce any patterns at all we do not study rules a0 where
a0[i] = 0 for all 0 ≤i ≤6.
For example, consider automaton A which nodes update their states as follows.
A silicon node in state 0 takes state 1 if one, two or three of its neighbours are in
state 1. The node remains in state 0 otherwise. Thus string s0 is (0111) and its binary
representation is 7. A silicon node in state 1 takes state 1 if it has two or three of
its neighbours in state, the node takes 0 otherwise: s1 = (0011), dec(s1) = 3. An
oxygen node in state 0 takes state 1 if it has two, ﬁve or six neighbours in state 1;
the node remains in state 0 otherwise: o0 = (0010011), dec(o0) = 19. An oxygen
node in state 1 takes state 0 if one, two or six of its neighbours are in state 1; the
node remains in state 1 otherwise: o0 = (1001110), dec(o0) = 78. This automaton
is represented by the rule R(7,3,19,78).
Automata are tested by random perturbations. In a rectangular array covering 39
nodes, resting nodes are assigned state 1 with probability 0.1. Statistics is collected
on a rectangular lattice of 157×157 nodes. 100K rules are generated at random. For
each rule we randomly perturb the automaton and evolve it for 100 steps, and then
evaluate conﬁgurations generated visually and using few integral parameters.

356
A. Adamatzky
μ
α





	
		
			
			
			

		
	
		
	

		
	


(a)
μ



	


	





	

	

	
μ
(b)
Fig. 16.2 100K randomly chosen rules are mapped onto μ vs α (a) and μO vs μS (b) map.
Locations of some rules illustrated in the paper are shown by arrows.

16
Phyllosilicate Automata
357
Density μa is the ratio of a number of nodes of type a in state 1 in a disc radius
100 to a total number of nodes covered by the disc; total density μ = μs + μo. Ac-
tivity α is a ratio of nodes a that at+1 ̸= at, t = 100. The rules mapped on parametric
spaces are shown in Fig. 16.2.
For each class C of rules we calculate a likelihood structure L(C) = P(ps
0,ps
1,po
0,
po
1) as follows. Given a sample of m strings, m = 200, for 0 ≤pf
z [i] ≤1 is calculated
as a number of times ’1’ appears in position i of strings fz (f = o or s and z = 0 or
1) divided by m. The higher is value 0 ≤pf
z [i] ≤1 the more likely randomly chosen
function of a given class will have 1 in position i of string fz.
16.3
Principal Morphologies
In the paper we present principle types of patterns generated by rule (16.1), classi-
ﬁed based on their shape internal morphology. There are ﬁve types of rules based
on shapes of patterns generated
• C1: octagonally shaped growth patterns,
• C2: almost circularly shaped growth patterns,
• C3: stationary patterns shaped as convex and concave hulls,
• C4: patterns exhibiting dendritic growth
• C5: patterns showing still localizations and oscillators.
and ﬁve classes of rules based on internal morphology of patterns generated
• M1: solid patterns,
• M2: labyrinthine patterns,
• M3: wave-like patterns,
• M4: disordered patterns,
• M5: still, travelling and propagating localizations.
16.3.1
Class C1
Patterns (Fig. 16.3) generated by rules of class C1 are bounded by eight half-planes
parallel to principle axes of the phyllosilicate lattice (Fig. 16.1). The pattern grow
unlimitedly on in ’inﬁnite’ lattice. Boundaries of the patterns may be well deﬁned,
e.g. conﬁgurations generated by rules R(7,13,36,96) (Fig. 16.3a), R(5,12,33,28)
(Fig. 16.3b) and R(6,13,63,76) (Fig. 16.3c) or slightly ‘fuzziﬁed’, as e.g. those
produced by rule R(3,4,41,70) (Fig. 16.3d) . Rules of this class are usually found
in the right bottom quadrant of μo–μs map (Fig. 16.2b), especially when solid do-
mains are internal morphologies. The rules occupy right top quadrant of μ–α map
in case of solid growing domains and tends towards centre of the map when patterns
generated exhibit wave-like internal morphologies (Fig. 16.2a). The rules of class C1

358
A. Adamatzky
(a) R(7,13,36,96), t = 33
(b) R(5,12,33,28), t = 33
(c) R(6,13,63,76), t = 31
(d) R(3,4,41,70), t = 29
Fig. 16.3 Conﬁgurations of A generated by rules of class C1. Snapshots of the conﬁgurations
are taken at time step t, exact values of t are shown. Silicon automata in state 1 are shown by
small black discs, oxygen automata in state 1 are shown by circles.
appear
with
probability 0.14
in
random samplings
of
rules.
Likelihood
of
the
functions
L(C) =
((0.0,0.67,0.55,0.69),
(0.53,0.42,0.58,0.47),
(0.0,1.0,0.92,0.36,0.42,0.42,0.61), (0.69,0.53,0.39,0.47,0.55,0.58,0.55))
shows that to form octagonal pattern nodes of automaton A will have the following
transition functions. A resting silicon node most likely to take state 1 if it has one
or three neighbours in state 1; a resting oxygen node very likely to take state 1 if
it has two or six neighbours in state 1. A silicon node remains in state 1 with high
probability if it has two neighbours. An oxygen node remains in state 1 more likely
if it has no neighbours in state 1.

16
Phyllosilicate Automata
359
(a) R(4,6,24,122), t = 63
(b) R(5,1,28,81), t = 68
(c) R(4,2,29,63), t = 59
(d) R(5,9,18,113), t = 56
(e) R(6,10,31,66), t = 66
(f) R(5,3,24,125), t = 54
Fig. 16.4 Conﬁgurations of A generated by rules of class C2. Snapshots of the conﬁgurations
are taken at time step t, exact values of t are shown. Silicon automata in state 1 are shown by
small black discs, oxygen automata in state 1 are shown by circles.

360
A. Adamatzky
16.3.2
Class C2
Boundaries of conﬁgurations generated by rules of class C2 by time step t are close,
in shape and position, to a circle of radius t (Fig. 16.4). Rules of C2 appear with
probability 0.21 in a random sampling of rules. The likelihood representation of the
rules L(C) = ((0.0,0.83,0.57,0.54), (0.51,0.6,0.46,0.4), (0.0,0.4,0.89,0.51,0.46,
0.57, 0.57), (0.68,0.57,0.49,0.57, 0.63,0.51,0.46)) indicates that a silicon atom is
more likely to become unresting, undergo state transition 0 →1, when it has one
neighbour in state 1. Oxygen atoms undergo transition 0 →1 when they have two
neighbours in state 1. Automata remain in state 1 more likely when they have none
or four neighbours in state 1.
Typically patterns of class C2 show a low level of activity and above average
mass, see e.g. positions of rules R(4,2,29,63) and R(5,3,24,125) on μ–α map
(Fig. 16.2a). Ratios of non-resting silicon and oxygen automata depends on internal
morphology of patterns generated (Fig. 16.2b). For example, a pattern produced by
rule R(4,2,29,63) has almost no silicon nodes in state 1, prevailing majority of
non-resting nodes are oxygen automata (Fig. 16.4b). Moreover, nodes taken state 1
remain in this state forever, thus low level of activity, and the rule’s positioning very
close to the beginning of μs axis. Labyrinthine pattern grown by rule R(5,3,24,125)
is composed of chains and rings of oxygen and silicon nodes in state 1, where a non-
resting oxygen node is always followed by a non-resting silicon atom (Fig. 16.4f).
This is why rule R(5,3,24,125) is positioned near a centre, i.e. average values, of
the plane μo–μs (Fig. 16.2b).
16.3.3
Class C3
Boundaries of the conﬁgurations generated by rules of class C3 resemble discrete
convex (Fig. 16.5a–d) and concave (Fig. 16.5ef) hulls [1, 19, 43]. The boundaries
are stationary, the patterns do not propagate further when concave or convex hulls
are formed. The rules are characterised by low density μ and activity level α, see
e.g. position of rule R(1,8,19,116) in (Fig. 16.2a). The rules of C3 are quite rate,
they appear with probability 0.051 in random samplings. As we can see in L(C) =
((0.0,0.24,0.76,0.43),(0.57,0.71,0.48,0.33),(0,0.05,0.81,0.62,0.67,0.29,0.52),
(0.91,0.67,0.71,0.71,0.33,0.48,0.57)),resting silicon automata more likely to take
state 1 if they have two neighbours in state 1, they remain in state 1 with high likeli-
hood if they have non or one neighbour in state 1. An oxygen automaton takes state
1 more likely if it has two, three or four neighbours in state 1, and it remains in state
one if there are less than four neighbours in state 1.

16
Phyllosilicate Automata
361
(a) R(1,1,20,101), t = 181
(b) R(6,0,30,70), t = 116
(c) R(7,2,29,65), t = 70
(d) R(1,8,19,116), t = 148
(e) R(3,9,17,127), t = 67
(f) R(5,12,11,29), t = 129
Fig. 16.5 Conﬁgurations of A generated by rules of class C3. Boundaries of the patterns
are stationary. Snapshots of the conﬁgurations are taken at time step t, exact values of t are
shown. Silicon automata in state 1 are shown by small black discs, oxygen automata in state
1 are shown by circles.

362
A. Adamatzky
(a) R(5,4,23,79), t = 63
(b) R(4,3,19,26), t = 81
(c) R(17,9,36,119), t = 35
(d) R(4,14,32,60), t = 36
Fig. 16.6 Conﬁgurations of A generated by rules of class C4. Snapshots of the conﬁgurations
are taken at time step t, exact values of t are shown. Silicon automata in state 1 are shown by
small black discs, oxygen automata in state 1 are shown by circles.
16.3.4
Class C4
The rules are rare, they occur with frequency 0.04 in random samplings. Patterns
generated by rules of this class show non-uniform growth rate along its wave-
fronts, reﬂected in formation of growth-cones or small dendritic protrusions. These
growth-cones grow faster than the rest of the boundary. The growth-cones can be
positioned irregularly along the boundary, see e.g. Fig. 16.6ab, or more or less
symmetrically, e.g. Fig. 16.6cd. The non-uniformity of wave-front propagation is
due to high degree of ‘non-linearity’ of cell-state transition function: numbers of
1-state neighbours responsible for a state transition can differ as much as four:
L(C) = ((0.0,0.75, 0.45,0.7), (0.3,0.6,0.5,0.25), (0.0,0.7,0.3,0.25,0.5,0.6,0.5),
(0.7,0.55,0.5, 0.55,0.7,0.5,0.45)). A silicon automaton makes the state transition
0 →1 when it has one or three neighbours in state 1, and the automaton keeps

16
Phyllosilicate Automata
363
its state 1 if there is only one non-resting neighbour. A resting oxygen automaton
switches to state 1 if there are one or ﬁve non-resting neighbours; the automaton
stays in state 1 if it has none or four 1-state neighbours.
16.3.5
Class C5
Initial random conﬁguration of 1-states shrinks to few still localisations and oscil-
lators in automata of class C5. Typical still localisations are singletons of oxygen
or silicon automata in non-resting state (Fig. 16.7a) or tiny clusters of oxygen and
silicon automata (Fig. 16.7b). The still and oscillating localizations do sometimes
assemble into chains or rings e.g. rule R(1,3,17,45) (Fig. 16.7c) or into regular two-
dimensional structures, e.g. rule R(4,6,11,95) (Fig. 16.7d). The rules of the class
are relatively common and appear with probability 0.23 in random samplings. Statis-
tics of functions from C5, L(C) = ((0.0,0.52,0.661,0.6), (0.43,0.38,0.52,0.51),
(0.0,0.01,0.15,0.46,0.55,0.49,0.49),
(0.69,0.48,0.54,0.49,0.43,0.37,
0.58)),
shows that silicon automata more likely switches from state 0 to 1 and back when
they have one, two or three neighbours in state 1. Thus transitions between 0 and 1
would be equiprobable, however the transition 1 →0 also occurs when silicon au-
tomaton has no neighbours in non-resting state. Thus, in general, initially random
pattern of non-resting automata reduces to few localizations.
16.3.6
Class M1
Node state transition rules of this morphological class can be found in all shape
based classes. Thus, rule R(7,13,36,96) generates a growing octagonal domain of
a regular lattice of oxygen automata in state 1 (Fig. 16.3a). Octagonal (Fig. 16.3a)
and near-circular (Fig. 16.4b) patterns with separate domains of non-resting oxygen
and silicon automata are produced by rules R(6,13,63,76) and R(5,1,28,81). A
regular lattice of non-resting oxygen automata grows as a near-circular pattern from
a random conﬁguration in automata governed by rule R(4,2,29,63) (Fig. 16.4c).
Regular patterns of silicon and oxygen are produced in a concave hull shaped pat-
terns, rule R(3,9,17,127) Fig. 16.5e, and dendritic patterns, rule R(5,4,23,79)
Fig. 16.6a.
16.3.7
Class M2
Labyrinthine patterns made of chains and rings of non-resting oxygen node with
passages partly ﬁlled with non-resting silicon nodes are observed in octagonal con-
ﬁgurations generated by rule R(5,12,33,28) (Fig. 16.3b). Rule R(5,3,24,125)
transforms an initially random conﬁguration into a growing labyrinthine pattern

364
A. Adamatzky
(a) R(1,13,3,126)
(b) R(6,5,11,113)
(c) R(1,3,17,45)
(d) R(4,6,11,95)
Fig. 16.7 Conﬁgurations of A generated by rules of class C5. Snapshots of the conﬁgurations
are taken at time step t, exact values of t are shown. Silicon automata in state 1 are shown by
small black discs, oxygen automata in state 1 are shown by circles.

16
Phyllosilicate Automata
365
made of chains of non-resting silicon-oxygen pairs (Fig. 16.4f). Convex and concave
halls grown by rules R(1,8,19,116) (Fig. 16.5d) and R(5,12,11,29) (Fig. 16.5f)
give a combination of oxygen only and silicon-oxygen chains. Averaging the rules,
L(C) = ((0.0,0.89, 0.44,0.78), (0.44,0.56,0.56,0.67), (0.0,0.67,0.67,0.11,0.44,
0.22, 0.44), (0.89, 0.89,0.78,1.0,0.78,0.44,0.44)), we ﬁnd that resting silicon be-
comes non-resting if it has one or three neighbours and resting oxygen takes state 1
if its two or three neighbours in state 1. A node of a chain have two neighbours and
an end-node of chain has just neighbour. This is why most labyrinthine patterns are
made of oxygen nodes.
16.3.8
Class M3
Wave-like patters are analogues of target and spiral wave-fronts in excitable non-
linear media. The wave-fronts may be travelling or stationary. The wave-like patterns
are represented by chains of automaton in non-resting states arrange circularly or
semi-circularly around initial source of perturbation. Likelihood representation of
class M3 rules is L(C) = ((0.0,0.5,0.3,0.6), (0.3,0.6,0.4,0.7), (0.0,1.0,0.9,0.7,
0.6, 0.3,0.6), (0.6,0.4, 0.3,0.5,0.7,0.2,0.5)). A resting silicon automaton is more
likely to take state 1 when two or three neighbours are in state 1; it is more likely to
remain in state 1 when three or one of its neighbours are non-resting.
16.3.9
Class M4
Rules
generating
disordered
patterns
are
quite
common,
they
appear
with
frequency
0.34
in
random
samplings.
A
frequency
representation
of
the
functions
is
L(C) =((0.0,0.6,0.77,0.45),
(0.55,0.55,0.45,
0.37),
(0.0,0.7,0.55,0.57,0.45,0.62,
0.52),
(0.7,0.52,0.47,0.5,0.5,0.4,0.5)).
The
disordered interior can be observed in patterns generated by rule R(3,4,41,70)
(Fig. 16.3d), R(5,9,18,113) (Fig. 16.4d), R(1,9,36,119) (Fig. 16.6c).
16.3.10
Class M5
Rules of this class exhibit still and travelling localizations and generators of local-
izations. Class M5 overlaps with class C5 because localizations do not have internal
morphology,their ‘internal morphology’is their shape. There are rules in M5 which
could not be found in any of the classes C. The rules of class M5 and patterns gen-
erated by them are discussed in details in Sect. 16.4.

366
A. Adamatzky
(a) t = 84
(b) t = 209
(c) t = 330
(d) scheme
Fig. 16.8 The slowest growing conﬁguration generated by rule R(7,5,31,33). (a–c) Snap-
shots of the conﬁguration, growing from initial random rectangular domain of 39×39 nodes,
recorded at 84th, 209th and 330th steps of evolution. Growth points are shown by arrows.
(d) a Scheme of the growth. Silicon automata in state 1 are shown by small black discs,
oxygen automata in state 1 are shown by circles. Automata in state 0 are shown by dots.
16.3.11
Sub-linearly Growing Patterns
Wave-like patterns and gliders are fastest growing conﬁgurations. Their linear size
increases linearly with time. What would be a slowest expanding pattern? The slow-
est we found so far emerges in automata governed by rule R(7,5,31,33), when an
initial random conﬁguration of states 1 is transformed to a convex hull shaped pat-
tern (Fig. 16.8a). The convex patterns emerged in rule R(7,5,31,33), in general, do
not grow anymore. Boundary of the pattern consists of chains of automata in state
1. Oxygen and silicon automata take state 1 in turns. In some situations, such con-
ﬁguration of non-resting oxygen and silicon atoms occurs (Fig. 16.8abc, shown by
arrow) that propagates along chains of silicon atoms and build a chain of non-resting
oxygen atoms along. This solitary conﬁguration, a growth point, always propagate
along the boundary of the convex hull pattern (Fig. 16.8c). Therefore a linear size
(along of the axis) of the pattern increases by two nodes only when the growth

16
Phyllosilicate Automata
367
Table 16.1 Parameters of localizations. For each localisation L we indicate its period p, min-
imum wmin and maximum wmax numbers of non-resting nodes, minimum wo
min and maximum
womax numbers of non-resting oxygen nodes, and minimum ws
min and maximum wsmax num-
bers of non-resting oxygen nodes. Minimal values in min columns and maximum values in
max columns are underlined.
L
p wmin wmax wo
min womax ws
min wsmax
G16
65 16 6
13
2
9
2
8
G12
68 12 7
13
2
6
2
7
G4
72 4
4
11
2
6
2
6
O5
65 5
6
10
2
8
4
8
O6
65 6
4
9
2
5
2
4
O6A
65 6
3
5
1
3
2
2
O6
68 6
3
6
1
3
2
3
O12
68 12 8
26
4
22
0
16
O2
72 2
3
4
3
3
4
4
O4
72 4
6
10
3
6
4
9
O12
72 12 2
5
1
3
1
3
point made a full turn along boundary of the pattern. Thus the size of the pattern is
bounded as O( 1
t ).
16.4
Game of Life Phyllosilicate Automata
Exploring the ’activity-density’ space in the loci with high activity and low density
we discovered three minimal — minimality in number of ’1’s in strings s and o —
rules that support gliders, R65 = R(5,2,16,65), R68 = R(5,2,16,68), and R72 =
R(5,2,16,72). The rules belong to class M5.
The rules have the same rules for state transitions of silicon nodes, s0 = (0101)
and s1 = (0010), and oxygen nodes in resting states, o0 = (0010000). Only be-
haviour of oxygen nodes in non-resting state differs: o1 = (1000001) in rule R65,
(1000100) in R68, and (1001000) in R72. Each of thee three rules R65, R68 and
R72. generate its own unique glider and several oscillators. Parameters of gliders
and oscillators are shown in Tab. 16.1. We have also discovered one still life, ob-
served in rule R68 and a glider gun, found in rule R72.
Gliders, oscillators and still lifes are characteristic patterns of Conway’s Game
of Life cellular automata. Phyllosilicate analogs of the Game of Life, or Life-like
automata, obey the following rules. Silicon node in state 0 takes state 1 only if it
has one or three neighbours in state 1. Silicon node in state 1 remains in state 1 if
it has two neighbours in state 1. Oxygen node in state 0 takes state 1 only if it has
two neighbours in state 1. An oxygen node in state 1 remains in state 1 if it has
no neighbours in state 1 or six in rule R65, four in rule R68 and three in rule R72
neighbours are in state 1.

368
A. Adamatzky
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
(g) t +6
(h) t +7
(i) t +8
(j) t +9
(k) t +10
(l) t +11
(m) t +12
(n) t +13
(o) t +14
(p) t +15
(q) t +16
Fig. 16.9 Glider G16
65 in rule R65. Glider moves west. Resting nodes are shown by dots. Non-
resting silicon nodes are shown by small black discs. Non-resting oxygen nodes are shown
by large circles.
16.4.1
Gliders
Glider G16
65 is observed in conﬁgurations produced by rule R65, see examples of
gliders travelling west and south-east in Figs. 16.9 and 16.10. Glider G16
65 is theav-
iest amongst gliders discovered (Tab. 16.1). It has longest period — 16 time steps
— and maximum weight of 13 non-resting nodes (Figs. 16.9p and 16.10p). The
glider undergoes thickening and lengthening phases. In thickening phase the glider
is larger, in terms of maximum between non-resting nodes, along a normal to its
velocity vector, see e.g. Figs. 16.9abfjko. In lengthening phase glider is larger along

16
Phyllosilicate Automata
369
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
(g) t +6
(h) t +7
(i) t +8
(j) t +9
(k) t +10
(l) t +11
(m) t +12
(n) t +13
(o) t +14
(p) t +15
(q) t +16
Fig. 16.10 Glider G16
65 in rule R65. Glider moves south-east. Resting nodes are shown by
dots. Non-resting silicon nodes are shown by small black discs. Non-resting oxygen nodes
are shown by large circles.
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
(g) t +6
(h) t +7
(i) t +8
(j) t +9
Fig. 16.11 Head on collision of gliders in rule R65, zero vertical shift. Glider travelling east
collides with glider travelling west. Resting nodes are shown by dots. Non-resting silicon
nodes are shown by small black discs. Non-resting oxygen nodes are shown by large circles.

370
A. Adamatzky
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
(g) t +6
(h) t +7
(i) t +8
(j) t +9
(k) t +10
(l) t +11
(m) t +12
Fig. 16.12 Glider G12
68 in rule R68. Glider moves down from north straight to south. Resting
nodes are shown by dots. Non-resting silicon nodes are shown by small black discs. Non-
resting oxygen nodes are shown by large circles.
parallel to its velocity vector, see e.g. Figs. 16.9dhjl. Ratio of silicon to oxygen non-
resting nodes changes during the glider’s cycle as follows: 2
4, 4
2, 2
5, 4
2, 4
5, 2
5, 6
4, 4
4, 4
5,
4
4, 4
4, 4
4, 2
6, 4
5, 8
2, 0
9, 10
3 and 2
4.
Most collisions between gliders in rule R65 lead to explosions: an unlimitedly
growing pattern is formed in the result of gliders interaction. Head on collision
with nil vertical or horizontal shift between gliders is a rare case of collision where
gliders do not explode (Fig. 16.11) but fuse into an oscillator. One glider travels east,
left non-resting pattern in Fig. 16.11a, another glider travels west, right non-resting
pattern in Fig. 16.11a. The gliders come into ‘contact’ with each other (Fig. 16.11b)
and form a transient pattern (Fig. 16.11b) of 4 non-resting silicon nodes and 10 non-
resting oxygen nodes. This pattern is then transformed into an oscillator of period
six (Fig. 16.11d–j).
Glider in rule R68 is shown in Fig. 16.12. The glider has period 12, and maxi-
mum weight 13 nodes (Tab. 16.1). Glider reﬂects along its south-north axis during
motion, in middle of its cycle, namely glider state at time step t, t = 1,2,···, is ver-
tically symmetric to the glider state at time step t + 6. The glider does not show a
pronounced thickening or lengthening but rather breathing around its centre of mass.
Ratios of non-resting silicon nodes to non-resting oxygen nodes in the glider’s cycle
are changed as follows: 7
6, 5
5, 5
2, 2
5, 6
3, 2
5, 7
6, 5
5, 5
2, 2
5, 6
3, 2
5, 7
6.
Head on collision between two gliders in R68 without offset leads to explo-
sion of both gliders. For certain offsets one glider can survive collision. Such sce-
nario is illustrated in Fig. 16.13. A glider moving north (left non-resting pattern in
Fig. 16.13a) clips a glider moving south (right non-resting pattern in Fig. 16.13a).

16
Phyllosilicate Automata
371
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
(g) t +6
(h) t +7
(i) t +8
(j) t +58
Fig. 16.13 Interaction of gliders G12
68 in rule R68. Glider moving north brushes with glider
moving south. Resting nodes are shown by dots. Non-resting silicon nodes are shown by
small black discs. Non-resting oxygen nodes are shown by large circles.

372
A. Adamatzky
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
Fig. 16.14 Glider G4
72 in rule R72. The glider travels east. Resting nodes are shown by dots.
Non-resting silicon nodes are shown by small black discs. Non-resting oxygen nodes are
shown by large circles.
(a) t
(b) t +1
(c) t +2
(d) t +3
Fig. 16.15 Glider gun in rule R72. Resting nodes are shown by dots. Non-resting silicon
nodes are shown by small black discs. Non-resting oxygen nodes are shown by large circles.
The glider moving north does not suffer any damage and continues its journey undis-
turbed. The glider moving south becomes tangled (Fig. 16.13b–d) and explodes
(Fig. 16.13e–i and j).
Glider in rule R72 is shown in Fig. 16.14. It is smallest amongst gliders discov-
ered and has smallest period. It has just four different states. Ratios of non-resting
silicons to non-resting oxygens in the glider’s cycle are 2
6, 6
5, 2
2 and 4
3.

16
Phyllosilicate Automata
373
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
Fig. 16.16 Oscillator O5
65 in rule R65. Resting nodes are shown by dots. Non-resting silicon
nodes are shown by small black discs. Non-resting oxygen nodes are shown by large circles.
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
(g) t +6
Fig. 16.17 Oscillator O6
65 in rule R65. Resting nodes are shown by dots. Non-resting silicon
nodes are shown by small black discs. Non-resting oxygen nodes are shown by large circles.
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
(g) t +6
Fig. 16.18 Oscillator O6A
65 in rule R65. Resting nodes are shown by dots. Non-resting silicon
nodes are shown by small black discs. Non-resting oxygen nodes are shown by large circles.
We did not ﬁnd glider guns in rules R65 and R68, we believe they exist though
and just wait to be found by someone being more patient. Glider gun in rule R72 is
shown in Fig. 16.15.
16.4.2
Oscillators
Oscillator is a localised compact conﬁguration of non-quiescent states which under-
goes ﬁnite growth and modiﬁcation but returns to its original state in a ﬁnite number
of steps. Oscillators are generated by all three rules, discussed in the paper, and can
be easily found by randomly perturbing the automaton lattice. Here we present a
selection of most common oscillators. Some parameters of the oscillators are shown
in Tab. 16.1. Periods of oscillators vary from 2, oscillator O2
72, to 12, oscillators
O12
68 and O12
72. Minimum weights vary from 2, oscillator O12
72 to 6, oscillator O5
65.
Maximum weights vary from 4, oscillator O2
72, to 26, oscillator O12
68.
Three typical oscillators in rule R65 are shown in Fig. 16.16, 16.17 and 16.18. Os-
cillator O5
65 changes it conﬁguration from two parallel chains of non-resting oxygen
nodes oriented north-west to south-east (Fig. 16.16a) and north-east to south-west

374
A. Adamatzky
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
(g) t +6
Fig. 16.19 Oscillator O6
68 in rule R68. Resting nodes are shown by dots. Non-resting silicon
nodes are shown by small black discs. Non-resting oxygen nodes are shown by large circles.
(Fig. 16.16c) to sparse chains of non-resting silicons with few non-resting oxygens
incorporated (Fig. 16.16bd). Oscillator O5
65 is also oscillator in rule R72.
Oscillator O6
65 switches its conﬁguration between a single chain of non-resting
silicons (Figs. 16.17ag), a single chain of non-resting oxygens (Figs. 16.17b), and
regular non-linear arrangements of non-resting nodes (Figs. 16.17c–f). Oscillator
O6A
65 is one of smallest oscillators (Fig. 16.18). It has period six. The oscillator con-
ﬁguration at time step t is symmetric — along north-east to south-west axis — to its
conﬁguration at time step t + 3.
Exemplar oscillators discovered in rule R68 are shown Figs. 16.19 and 16.20.
Oscillator O6
68 has the same conﬁguration as oscillator O6A
65, see Fig. 16.19f and
Fig. 16.18adj. Oscillator O12
68 is the largest oscillator, amongst presented in the paper
(Tab. 16.1). The oscillator has period 16, and consists of 28 non-resting nodes in its
heaviest conﬁguration (Fig. 16.20f).
Oscillator O2
72 is the smallest one (Tab. 16.1). It has just two conﬁgurations: one
consists of four non-resting silicon nodes and another of three non-resting oxygen
nodes (Fig. 16.21).
Oscillator O12
72 (Fig. 16.23) is the smallest oscillator with longest period. Its period
is 12 time steps and maximum weight is 5 non-resting nodes, see e.g. Fig. 16.23d.
When glider G4
72 collides to O12
72 there are three possible outcomes, depending on
state of the oscillator at the collision and relative positions of the glider and the os-
cillator: oscillator and glider vanish in the result of collision, both localizations ex-
plode, and glider disappear but oscillator remains intact. In Conway’s Game of Life
an eater is a still life (stationary localisation that does not change its conﬁguration)
which repairs itself after collision with a glider or any other travelling localisation,
while the colliding travelling localisation vanishes. Oscillator O12
72 is not a still life
so it can be classiﬁed as oscillator-eater.
16.4.3
Still Life
Still lifes are localizations that do not change their pattern or a position during au-
tomaton development. So far we found still lifes only in rule R68: this is a ring of
six oxygen and six silicon non-resting nodes as shown in Fig. 16.24a. This patterns
becomes an oscillator in rule R65 (Fig. 16.24b) and is transformed into a glider gun
in rule R68 (Fig. 16.15).

16
Phyllosilicate Automata
375
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
(g) t +6
(h) t +7
(i) t +8
(j) t +9
(k) t +10
(l) t +11
(m) t +12
Fig. 16.20 Oscillator O12
68 in rule R68. Resting nodes are shown by dots. Non-resting silicon
nodes are shown by small black discs. Non-resting oxygen nodes are shown by large circles.
(a) t
(b) t +1
(c) t +2
Fig. 16.21 Oscillator O2
72 in rule R72. Resting nodes are shown by dots. Non-resting silicon
nodes are shown by small black discs. Non-resting oxygen nodes are shown by large circles.
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
Fig. 16.22 Oscillator O4
72 in rule R72. Resting nodes are shown by dots. Non-resting silicon
nodes are shown by small black discs. Non-resting oxygen nodes are shown by large circles.

376
A. Adamatzky
(a) t
(b) t +1
(c) t +2
(d) t +3
(e) t +4
(f) t +5
(g) t +6
(h) t +7
(i) t +8
(j) t +9
(k) t +10
(l) t +11
(m) t +12
Fig. 16.23 Oscillator O12
72 in rule R72. Resting nodes are shown by dots. Non-resting silicon
nodes are shown by small black discs. Non-resting oxygen nodes are shown by large circles.
(a)
(b) t +1
(c) t +2
(d) t +3
(e) t +4
Fig. 16.24 Still localisation. (a) Still localisation in rule R68; in this rule the localisation
acts as eater. The localisation (a) becomes an oscillation (b–e) in rule R65. Resting nodes
are shown by dots. Non-resting silicon nodes are shown by small black discs. Non-resting
oxygen nodes are shown by large circles.
16.5
Discussion
We uncovered principle morphologies of patterns emerging in automata models of
phyllosilicates, two-dimensional regular networks of ﬁnite state machines imitating
coordination lattice and atomic bounds of silicate sheets. We classiﬁed rules based
on shape of patterns generated by the rules and internal morphologies of the pat-
terns, and approximated distribution of functions on density of patterns and activity
of cell-state transitions and maps of oxygen and silicon densities. Using shapes of
patterns as a key characteristic we selected ﬁve classes: octagonally shaped pat-
terns, almost circularly shaped, stationary patterns non-propagating beyond convex
or concave hulls, patterns exhibiting dendritic growth, and patterns showing still lo-
calizations and oscillations. Internal morphology driven taxonomy is comprised of
solid patterns, labyrinthine patterns, wave-like patterns, disordered patters; and still,
travelling and propagating localizations. The classes proposed give rather a coarse
classiﬁcation of a rule space. There are rules which represent transient patterns that
could be ﬁtted in more than one shape based or internal morphology based class;
such rules however do not dominate a rule space.

16
Phyllosilicate Automata
377
Fig. 16.25 Positions of Game of Life and phyllosilicate automata rules on ‘neighbourhood
occupancy necessary for birth’, transition from state 0 to state 1, versus ‘neighbourhood oc-
cupancy necessary for survival’, transition from state 1 to state 1.
Amongst node state transition rules of phyllosilicate automata we discovered the
rules exhibit behaviour similar to Conway’s Game of Life automata. Classical Con-
way’s Game of Life automata have rule B3/S23: a dead cell become alive, born if it
has three live neighbours, a living cell survives if it has two or three live neighbours.
In notation of Life-like automata the rules can be written as follows. All silicon
nodes update their states by rule B23/S23. Rules of oxygen nodes are B2/S06 (rule
R65), B2/S04 (rule R68) and B2/S03 (rule R72). Such notations could be somewhat
misleading. In Conway’s Game of Life a node (cell) has eight neighbours while in
phyllosilicate automata a silicon node has three neighbours and oxygen node six
neighbours. Thus, rules should be normalised by dividing number of non-resting
neighbours necessary for birth and survival by neighbourhood size. Normalised
rules are as follows. Normalised Conway’s Game of Life rule is B 3
8/S 2
8
3
8. Rule
for silicon nodes in phyllosilicate is B 2
31/S 2
3. Rules for oxygen nodes are B 2
6/S01
(R65), B 2
6/S0 4
6 (R68), and B 2
6/S0 3
6 (R72). On the map ‘neighbourhood occupancy
necessary for birth’, transition from state 0 to state 1, versus ‘neighbourhood occu-
pancy necessary for birth’ (Fig. 16.25) Game of Life rules reside in the quadrant
with below average occupancies. Occupancies necessary for silicon nodes to take
non-resting state and to remain in the non-resting reside in the quadrant of above
average occupancies. Rules for oxygen nodes are characterised by below average
occupancies necessary for survival (Fig. 16.25). Distribution of rules on this map
hints that conditions for survival are more critical than conditions for birth. Most
rules supporting gliders obey a neigbourhood occupancy between 0.3 and 0.4 while
neighbourhood occupancy for birth varies from 0 to 1.

378
A. Adamatzky
We presented three types of node-state transition rules of a regular automaton
network which mimics topology of a silicate sheet. All three rules studied support
travelling and oscillating stationary localizations. Yet only one rule exhibits still
localizations. Our search for phyllosilicate automata exhibiting Conway’s Game of
Life behaviour was extensive yet not exhaustive. Thus substantial chances remain
that glider guns could be found in rules R65 and R68, and still lifes in rules R65 and
R72.
Most interactions between localizations lead to explosions of the localizations,
when unlimitedly growing patterns of non-resting states are formed. Just few sce-
nario of collisions lead to annihilation of both or at least one of colliding local-
izations. This may somehow limit, however not totally take away, an applicability
of phyllosilicate automata in design of collision-based circuits [2]. Further studies
in the interaction of localizations are required to make a deﬁnite conclusion about
usefulness of phyllosilicate automata in the unconventional computing ﬁeld.
What are potential impacts of the results?
Phyllosilicate automata is a new species among regular automaton networks. The
phyllosilicate automata are not strictly cellular automata, because they have two
types of nodes which updates their states by two different functions. They are large-
scale regular lattice of ﬁnite state machines and thus make a valuable addition to a
family of cellular automata on non-othogonal and non-periodic lattices [14,21,35].
The phyllosilicate automata produce a rich spectrum of morphologies generated
in the automata space-time dynamics. Shape-based and morphological classiﬁca-
tion of phyllosilicate automata well matches analogous classiﬁcations developed in
two-dimensional binary cellular automata [4, 6] and automata models of excitable
media [5,13].
The phyllosilicate automata mimic structure of silicate sheets. Binary states can
be seen as analogies of atoms’ excitations and positions: state 0 corresponds to a
baseline resting atom state and state 1 represents a disturbed atom state with pos-
sible changes of inter-atomic bonds. Patterns travelling in phyllosilicate automata
imitate spatially extended defects propagating in silicate sheets. Localisations and
defects are vehicles of computation in molecular arrays [11,12]. We envisage state
transitions functions — especially those support gliders, puffers, still localisations
and oscillators — could be used as fast-prototyping tools for designing and future
manufacturing of collision-based computing devices in silicon sheets.
How gliders, oscillators and still lifes could be represented in real phyllosilicate
lattices?
We believe they will be seen as domains of localised defects, e.g. recombination-
induced defects formation [42] occurred in tetrahedra, and vacancy-impurity pairs
or isolated interstitial silicon defects [45]. The imperfection, vacancy, and self-
interstitial, substitutional and interstitial impurities are important because they are
necessary for setting up travelling localizations of e.g. a substitutional impurity atom
through the lattice. The travelling impurities cause local deformations of the phyl-
losilicate lattice, e.g. via recombination of vacancies and carriers, these deforma-
tions produce waves. Thus, for example, state ‘1’ of a node can represent a point
interstitial defect, where the host atom moves to a non-lattice position [39]. The

16
Phyllosilicate Automata
379
point defects can form travelling localizations, phenomenologically similar to glid-
ers. Such localizations of defects may not be stable, even at low temperature, and
disappear. However, duration of their life time could be sufﬁcient to execute one or
more collision-based logical gates.
A typical scenario of generating a glider on a phyllosilicate lattice would be as
follows. A locally irradiation of the lattice with 1-3 MeV electrons, gamma rays,
or fast neutrons [24] would move silicon atoms into metastable interstitial states.
Thus a negatively charged travelling vacancy is formed. Direction of this localisa-
tion travelling could be controlled by inducing lattice vibrations and affecting elec-
trons transitions. Obviously, to obtain an experimental conﬁrmation would be a very
difﬁcult task.
There is at least some analogy between gliders-supporting automata rules and
conditions for emergence of a travelling vacancy. In all glider-supporting phyllosil-
icate automata studied a silicon node excites and stays excited if it has two or three
excited neighbours. An oxygen is excited if it has two excited neighbours. As hinted
in [24] negatively charged travelling vacancies can migrate only if at least two inter-
atomic bonds are broken.
When complemented by molecular-dynamics method [27] or reaction-diffusion
computational approach [44] of simulating defects propagation, our approach can
be useful in designing close to reality computing circuits based on travelling defects
and clusters of defects. This will be a topic of future studies to ﬁnd what exact
structure of defect, localisation or dislocation [17, 37] corresponds to gliders and
still lifes.
References
1. Adamatzky, A.: Identiﬁcation of Cellular Automata. Taylor and Francis (1994)
2. Adamatzky, A. (ed.): Collision-Based Computing. Elsevier (2002)
3. Adamatzky, A., De Lacy Costello, B., Asai, T.: Reaction Diffusion Computers. Elsevier
(2005)
4. Adamatzky, A., Martinez, G.J., Mora, J.C.S.T.: Phenomenology of reaction diffusion
binary-state cellular automata. Int. J. Bifurcation and Chaos 16, 2985–3005 (2007)
5. Adamatzky, A., Chua, L.: Phenomenology of retained refractoriness: On semi-
memristive discrete media. Int. J. Bifurcation and Chaos 22, 1230036 (2012)
6. Adamatzky, A.: Reaction-Diffusion Automata. Springer (2012)
7. Adamatzky, A.: On binary-state phyllosilicate automata. Int. J. Bifurcation and Chaos
(2013)
8. Adamatzky, A.: On oscillators in phyllosilicate excitable automata. Int. J. Mod. Phys. C
24, 1350034 (2013) (10 pages)
9. Adamatzky, A.: Game of Life on Phyllosilicates: Gliders, Oscillators and Still Life.
Physics Letters A 377, 1597–1605 (2013)
10. Ballantine, J.A., Purnell, J.H., Thomas, J.M.: Sheet silicates: broad spectrum catalysts
for organic synthesis. J. Molecular Catalysis 27, 157–167 (1984)
11. Bandyopadhyay, A., Pati, R., Sahu, S., Peper, F., Fujita, D.: Massively parallel computing
on an organic molecular layer. Nature Physics 6, 369–375 (2010)
12. Adamatzky, A.: Molecular computing: Aromatic arithmetic. Nature Nature Physics 6,
325–326 (2010)

380
A. Adamatzky
13. Adamatzky, A., Chua, L.: Memristive excitable cellular automata. Int. J. Bifurcation and
Chaos 21, 3083 (2011)
14. Bays, C.: The discovery of glider guns in a Game of Life for the triangular tessellation.
J. Cellular Automata 2(4), 345–350 (2007)
15. Bergaya, F., Theng, B.G.K., Lagaly, G. (eds.): Handbook of Clay Science. Elsevier
(2006)
16. Bleam, W.: Atomic theories of phyllosilicates: Quantum chemistry, statistical mechanics,
electrostatic theory, and crystal chemistry. Reviews Geophysics 31(1), 51–73 (1993)
17. Bulatov, V.V., Justo, J.F., Wei Cai, S., Yip, A.S., Argon, T., Lenosky, M., de la Rubia,
T.D.: Parameter-free Modeling of Dislocation Motion: The Case of Silicon. Philosophi-
cal Magazine A 81, 1257–1281 (2001)
18. Carrado, K.A., Macha, S.M., Tiede, D.M.: Effects of surface functionalization and
organo-tailoring of synthetic layer silicates on the immobilization of cytochrome c.
Chem. Mater. 16, 2559–2566 (2004)
19. Clarridge, A.G., Salomaa, K.: An improved cellular automata based algorithm for the
45-Convex hull problem. Journal of Cellular Automata 5, 107–112 (2010)
20. Field, R.J., Noyes, R.M.: Oscillations in chemical systems IV. Limit cycle behavior in a
model of a real chemical reaction. J. Chem. Phys. 60, 1877–1884 (1974)
21. Goucher, A.P.: Gliders in cellular automata on Penrose tilings. J. Cellular Automata
(2012) (in Press)
22. Greenberg, J., Hastings, S.: Spatial patterns for discrete models of diffusion in excitable
media. SIAM J. Applied Math. 34, 515–523 (1978)
23. Griffen, D.T.: Silicate Crystal Chemistry. Oxford University Press (1992)
24. Janavicus, A.J., Storasta, J., Purlys, R., Mekys, A., Balakauskas, S., Norgela, Z.: Crystal
lattice and carriers hall mobility relaxation processes in Si crystal irradiated by soft X-
rays. Acta Physica Polonica 112, 55–67 (2007)
25. Lehmann, T., Wolff, T., Hamel, C., Veit, P., Garke, B., Seidel-Morgenstern, A.: Physico-
chemical characterization of Ni/MCM-41 synthesized by a template ion exchange ap-
proach. Microporous and Mesoporous Materials 151, 113–125 (2012)
26. Liebau, F.: Structural Chemistry of Silicates: Structure, Bonding, and Classiﬁcation.
Springer (1985)
27. Law, M.E., Gilmer, G.H., Jara´ız, M.: Simulation of defects and diffusion phenomena in
silicon. MRS Bulletin, 46–51 (June 2000)
28. Margenstern, M.: New Tools for Cellular Automata in the Hyperbolic Plane. J.
UCS 6(12), 1226–1252 (2000)
29. Margenstern, M.: A universal cellular automaton on the heptagrid of the hyperbolic plane
with four states. Theor. Comput. Sci. 412(1-2), 33–56 (2011)
30. Margenstern, M.: Universal Cellular Automata with Two States in the Hyperbolic Plane.
J. Cellular Automata 7(3), 259–284 (2012)
31. Margenstern, M.: Universality and the Halting Problem for Cellular Automata in Hyper-
bolic Spaces: The Side of the Halting Problem. In: Durand-Lose, J., Jonoska, N. (eds.)
UCNC 2012. LNCS, vol. 7445, pp. 12–33. Springer, Heidelberg (2012)
32. Margenstern, M.: Small Universal Cellular Automata in Hyperbolic Spaces: A Collection
of Jewels. Springer (2013)
33. McDonald, A., Scott, B., Villemure, G.: Hydrothermal preparation of nanotubular
particles of a 1:1 nickel phyllosilicate. Microporous and Mesoporous Materials 120,
263–266 (2009)
34. Monnier, A., Schuth, F., Huo, Q., Kumar, D., Margolese, D., Maxwell, R.S., Stucky,
G.D., Krishnamurty, M., Petroff, P., Firouzi, A., Janicke, M., Chmelka, B.F.: Coopera-
tive formation of inorganic-organic interfaces in the synthesis of silicate mesostructures.
Science 261, 1299–1303 (1993)

16
Phyllosilicate Automata
381
35. Owens, N., Stepney, S.: Investigations of Game of Life cellular automata rules on Pen-
rose tilings: Lifetime, ash, and oscillator Statistics. J. Cellular Automata 5, 207–225
(2010)
36. Pauling, L.: The structure of the micas and related minerals. Proc. Natl. Acad. Sci.
U.S.A. 16, 123–129 (1930)
37. Pizzagalli, L., Godet, J., Guenole, J., Brochard, S.: Dislocation cores in silicon: new
aspects from numerical simulations. Journal of Physics: Conference Series 281, 012002
(2011)
38. Richardson, I.G.: The calcium silicate hydrates. Cement and Concrete Research 38,
137–158 (2008)
39. Sokolski, M.M.: Structure and kinetics of defects in silicon. NASA TN D-4154, Wash-
ington (1967)
40. Specht, K.M., Jackson, M., Sunkel, B., Boucher, M.A.: Synthesis of a functionalized
sheet silicate derived from apophyllite and further modiﬁcation by hydrosilylation. Ap-
plied Clay Science 47, 212–216 (2010)
41. Suh, W.H., Suslick, K.S., Stucky, G.D., Suh, Y.-H.: Nanotechnology, nanotoxicology,
and neuroscience. Progress in Neurobiology 87, 133–170 (2009)
42. Tanimura, K., Tanaka, T., Itoh, N.: Creation of quasistable lattice defects by electronic
excitation in SiO2. Phys. Rev. Lett. 51, 423–426 (1983)
43. Torbey, S., Akl, S.G.: An exact solution to the two-dimensional arbitrary-threshold den-
sity classiﬁcation problem. Journal of Cellular Automata 4, 225–235 (2009)
44. Velichko, O.I., Dobrushkin, V.A., Muchynski, A.N., Tsurko, V.A., Zhuk, V.A.: Simu-
lation of coupled diffusion of impurity atoms and point defects under nonequilibrium
conditions in local domain. J. Comput Physics 178, 196–209 (2002)
45. Watkins, G.D.: Lattice vacancies and interstitials in silicon. Proc. of the US-ROC Solid
State Physics Seminar. Chinese, J. Physics 15, 92–102 (1977)
46. Watkins, G.D.E.: studies of lattice defects in semiconductors. In: Henderson, B., Hughes,
A.E. (eds.) Defects and Their Structure in Non-metallic Solids, p. 203. Plenum Press,
New York (1976)

Chapter 17
DC Programming and DCA for Challenging
Problems in Bioinformatics and Computational
Biology
Le Thi Hoai An
Abstract. Nonconvex optimization is a powerful tool in bioinformatics and compu-
tational biology. As an innovative approach to nonconvex programming, Difference
of Convex functions (DC) programming and DC Algorithms (DCA) have proved
to be efﬁcient for several problems in computational biology. The objective of this
chapter is to show that grand challenge problems in bioinformatics and computa-
tional biology can be modeled as DC programs and solved by DCA based algo-
rithms. We offer the community of researchers in computational biology promising
approaches in a uniﬁed DC programming framework to tackle challenging biolog-
ical problems such as Multiple Alignment of Sequence (MSA), Molecular confor-
mation and Phylogenetic tree reconstruction.
17.1
Introduction
Computational biology is an important part of developing emerging technologies
for the ﬁeld of biology. It is used to help sequence the human genome, create ac-
curate models of the human brain, and assist in modeling biological systems. To
know a biological system, we want to get an insight in its evolution, structure, and
function, in order to explain ultimately adaptation, diversity, and complexity of the
system. Developing mathematical and computational approaches to analyze these
and other properties of living systems is the ultimate grand challenge for bioinfor-
matics and computational biology. Optimization plays a key role in computational
biology since several issues of this domain are related to optimization problems.
During the last two decades, an increasing amount of effort has been put into non-
convex optimization to deal with many difﬁcult biological problems. The absence of
Le Thi Hoai An
Laboratory of Theoretical and Applied Computer Science (LITA EA 3097)
University of Lorraine,
Ile du Saulcy, 57045 Metz, France
e-mail: hoai-an.le-thi@univ-lorraine.fr
c⃝Springer International Publishing Switzerland 2015
383
A. Adamatzky (ed.), Automata, Universality, Computation,
Emergence, Complexity and Computation 12, DOI: 10.1007/978-3-319-09039-9_17

384
L.T. Hoai An
convexity creates a source of difﬁculties of all kinds, in particular the distinction be-
tween the local and global minima, the non-existence of veriﬁable characterizations
of global solutions, ... That causes all the computational complexity while passing
from convex to nonconvex programming. Finding a global solution of a non-convex
program, especially in the large-scale, is a graal for optimizers.
A variety of nonconvex optimization techniques have been recently developed by
researchers in optimization. Generally speaking, there are two different but comple-
mentary approaches: Global approaches such as Cutting Plane (CP), Branch and
Bound (B&B), Branch and Cut (B&C) can guarantee the globality of the solu-
tions but they are very expensive, in particular for large scale setting; and Local
approaches, on the contrary, are much faster while only local minima are avail-
able. Many current approaches are not generally effective for practical large-scale
problems. Finding efﬁcient algorithms that realize a compromise to overcome these
drawbacks is a challenge of nonconvex programming. Such algorithms must exploit
domain-speciﬁc structures of the problems being considered.
As an innovative approach to nonconvex programming, Difference of Convex
functions (DC) programming and DC Algorithms (DCA) are increasingly used by
researchers in various domains. In this chapter, within the four grand challenges in
bioinformatics and computational biology ( [61]), namely
• Protein structure prediction;
• Homology searches;
• Multiple alignment and phylogeny construction;
• Genomic sequence analysis and gene-ﬁnding;
we focus on three challenging classes of problems — Multiple Alignment of Se-
quence (MSA), Molecular conformation and Phylogenetic tree reconstruction. We
show that these problems can be modeled as DC programs and solved by DCA
based algorithms.
The chapter is organized as follows. In the next section we give a brief introduc-
tion of DC programming and DCA. The solution methods of each class of problem
are discussed in Sections 17.3, 17.4 and 17.5. Finally Section17.6 concludes the
chapter.
17.2
A Brief Introduction to DC Programming and DCA
DC programming, constitute the backbone of nonconvex programming and global
optimization, is an extension of convex programming which is sufﬁciently large to
cover almost all nonconvex optimization problems, but not too to still allow using
the arsenal of powerful tools in convex analysis and convex optimization. DC pro-
gramming and DCA were introduced by Pham Dinh Tao in their preliminary form in
1985. These theoretical and algorithmic tools are extensively developed by Le Thi
Hoai An and Pham Dinh Tao since 1993 to become now classic and increasingly
popular (see e.g. the list of reference in [28]).

17
DCA for Challenging Problems
385
17.2.1
DC Programming: Basic Notations and Background
We are working with the space X = IRn which is equipped with the canonical inner
product ⟨·,·⟩and the corresponding Euclidean norm ∥· ∥, thus the dual space Y of
X can be identiﬁed with X itself. We follow [20], [59] for deﬁnitions of usual tools
in modern convex analysis where functions could take the inﬁnite values +∞. A
function θ : X →IR∪{+∞} is said to be proper if it is not identically equal to +∞.
The effective domain of θ, denoted by dom θ, is
dom θ = {x ∈X : θ(x) < +∞}.
The indicator function χC of a nonempty closed convex C set is deﬁned by χC(x) =
0 if x ∈C,+∞otherwise. The set of all lower semicontinuous proper convex func-
tions on X is denoted by Γ0(X). Let θ ∈Γ0(X), then the conjugate function of θ,
denoted θ∗, is deﬁned by
θ ∗(y) = sup{⟨x,y⟩−θ(x) : x ∈X}.
We have θ ∗∈Γ0(Y) and θ ∗∗= θ.
Nonsmooth convex functions are handled using the concept of subdifferentials. For
θ ∈Γ0(X) and x0 ∈dom θ, ∂θ(x0) denotes the subdifferential of θ at x0, and is
deﬁned by
∂θ(x0) := {y ∈Y : θ(x) ≥θ(x0)+ ⟨x−x0,y⟩, ∀x ∈X}.
(17.1)
Each y ∈∂θ(x0) is called a subgradient of θ at x0. The subdifferential ∂θ(x0) is
a closed convex set in Y. It generalizes the derivative in the sense that θ is differ-
entiable at x0 if and only if ∂θ(x0) ≡{∇θ(x0)}. Recall the well-known properties
related to subdifferential calculus of θ ∈Γ0(X):
y0 ∈∂θ(x0) ⇔x0 ∈∂θ ∗(y0) ⇔⟨x0,y0⟩= θ(x0)+ θ∗(y0);
(17.2)
∂θ ∗(y0) = argmin{θ(x)−⟨x,y0⟩: x ∈X}.
(17.3)
A function θ ∈Γ0(X) is said to be polyhedral convex if
θ(x) = max{⟨ai,x⟩−β i : i = 1,...,m} + χC(x) ∀x ∈X,
where C is a nonempty polyhedral convex set in X.
A DC program is of the form
(Pdc) α = inf{ f(x) := g(x)−h(x) : x ∈X},
(17.4)

386
L.T. Hoai An
with g,h ∈Γ0(X). Such a function f is called a DC function, and g −h, a DC
decomposition of f, while the convex functions g and h are DC components of f.
In (Pdc) the nonconvexity comes from the concavity of the function −h (except the
case h is afﬁne since (Pdc) then is a convex program).
It should be noted that a convex constrained DC program of the form
α = inf{ f(x) := g(x)−h(x) : x ∈C},
(17.5)
can be expressed in the form (17.4) by using the indicator function on C, that is
inf{ f(x) := g(x)−h(x) : x ∈C} = inf{ϕ(x)−h(x) : x ∈X}, with ϕ:= g + χC.
Hence, throughout this paper, DC program of the form (17.4) is referred to as ”stan-
dard DC program”.
Polyhedral DC programs (Pdc) (i.e., when g or h are polyhedral convex) play a
key role in nonconvex programming (see [36, 48, 49] and references therein), and
enjoy interesting properties related to local optimality and DCA’s convergence.
The DC duality is based on the conjugate functions and the fundamental charac-
terization of a convex function θ ∈Γ0(X) as the pointwise supremum of a collection
of afﬁne minorants:
θ(x) = sup{⟨x,y⟩−θ∗(y) : y ∈Y},
∀x ∈X.
(17.6)
That associates the primal DC program (17.4) (Pdc) with its dual DC program (Ddc)
deﬁned by
(Ddc)
α = inf{h∗(y)−g∗(y) : y ∈Y},
(17.7)
and investigates their mutual relations. We observe the perfect symmetry between
primal and dual DC programs: the dual to (Ddc) is exactly (Pdc).
It is worth noting the wealth of the vector space DC(X) = Γ0(X)−Γ0(X) spanned
by the ”convex cone” Γ0(X) ( [48,49]): it contains most realistic objective functions
and is closed under operations usually considered in optimization.
The complexity of DC programs resides, of course, in the lack of veriﬁable glob-
ality conditions. Lets recall the general local optimality conditions in DC program-
ming (subdifferential’s inclusion): if x∗is a local solution of (Pdc) then
/0 ̸= ∂h(x∗) ⊂∂g(x∗).
(17.8)
The condition (17.8) is also sufﬁcient (for local optimality) in many important
classes of DC programs (see [48,49]).
A point x∗is said to be a critical point of g −h (or generalized KKT point for
(Pdc)) if
∂h(x∗)∩∂g(x∗) ̸= /0.
(17.9)
Note that, by symmetry, the dual part of (17.8) and (17.9) are trivial.

17
DCA for Challenging Problems
387
17.2.2
DCA’s Philosophy
DCA is a continuous primal dual subgradient approach based on local optimality
and duality in DC programming for solving standard DC programs (Pdc).
The key idea behind DCA is to replace in (Pdc), at the current point xk, the second
component h with its afﬁne minorant deﬁned by
hk(x) := h(xk)+ ⟨x−xk,yk⟩, yk ∈∂h(xk)
to give birth to the primal convex program of the form
(Pk)
inf{g(x)−hk(x) : x ∈X} ⇐⇒inf{g(x)−⟨x,yk⟩: x ∈X}
whose solution set is ∂g∗(yk). The next iterate xk+1 is taken in ∂g∗(yk).
Dually, a solution xk+1 of (Pk) is then used to deﬁne the dual convex program
(Dk+1) obtained from (Ddc) by replacing g∗with its afﬁne minorant
(g∗)k(y) := g∗(yk)+ ⟨y−yk,xk+1⟩
to obtain the convex program
(Dk+1) inf{h∗(y)−[g∗(yk)+⟨y−yk,xk+1⟩] : y ∈Y} ⇔inf{h∗(y)−⟨y,xk+1⟩: y ∈Y}
whose solution set is ∂h(xk+1). The next iterate yk+1 is chosen in ∂h(xk+1).
DCA scheme
Initialization: Let x0 ∈IRn be a guess, k ←0.
Repeat
• Calculate yk ∈∂h(xk)
• Calculate xk+1 ∈∂g∗(yk), which is equivalent to
xk+1 ∈argmin{g(x)−⟨x,yk⟩: x ∈IRn}
(Pk)
• k ←k + 1
Until convergence of
 
xk!
.
17.2.3
DCA’s Convergence Properties
Convergence properties of DCA and its theoretical basis can be found in ( [36, 48,
49]). For instance it is important to mention that
i) DCA is a descent method without linesearch but with global convergence: the
sequences {g(xk)−h(xk)} and {h∗(yk)−g∗(yk)} are decreasing.
ii) If the optimal value α of problem (Pdc) is ﬁnite and the inﬁnite sequences
{xk} and {yk} are bounded, then every limit point x∗(resp. y∗) of the sequence {xk}

388
L.T. Hoai An
(resp. {yk}) is a critical point of g −h (resp. h∗−g∗), i.e. ∂h(x∗)∩∂g(x∗) ̸= /0 (resp.
∂h∗(y∗)∩∂g∗(y∗) ̸= /0).
iii) DCA has a linear convergence for DC programs.
iv) DCA has a ﬁnite convergence for polyhedral DC programs.
For a complete study of DC programming and DCA the reader is referred to
[36,48–50] and the references therein.
17.2.4
Key Properties of DCA
Without going into details, let us mention the key properties of DCA.
a. Flexibility: the construction of DCA is based on g and h but not on f itself, and
there are as many DCA as there are DC decompositions. This is a crucial fact in DC
programming. It is important to study various equivalent DC forms of a DC pro-
gram, because each DC function f has inﬁnitely many DC decompositions which
have crucial implications for the qualities (speed of convergence, robustness, efﬁ-
ciency, globality of computed solutions,...) of DCA.
b. DCA is a descent method without linesearch, with global convergence (i.e. DCA
converges from an arbitrary initial point).
c. Return from DC programming to convex programming: DCA consists in an itera-
tive approximation of a DC program by a sequence of convex programs that will be
solved by appropriate convex optimization algorithms. This property is called SCA
(Successive Convex Approximation) in some recent works.
d. Versatility: With suitable DC decompositions DCA generates most standard algo-
rithms in convex and nonconvex optimization. Hence DCA offers a wide framework
for solving convex/nonconvex optimization problems. In particular DCA is a global
approach to convex programming, i.e., it converges to optimal solutions of convex
programs reformulated as DC programs. Consequently, it can be used to build efﬁ-
cient customized algorithms for solving convex programs generated by DCA.
To the best of our knowledge, DCA is actually one of the rare algorithms for
nonsmooth nonconvex programming which allow to solve large-scale DC programs.
DCA was successfully applied to a lot of different and various nonconvex optimiza-
tion problems to which it quite often gave global solutions and proved to be more
robust and more efﬁcient than related standard methods (see the list of references
in [28]).
We now show how to solve three challenging classes of biological problems by
DC programming and DCA.
17.3
Multiple Sequence Alignment
The construction of biologically plausible alignment for given families of DNA or
protein sequences is one of the major problems in computational molecular biology.
Consequently, the sequence alignment plays an important role and is a very active

17
DCA for Challenging Problems
389
area of research. The sequence of a DNA molecule can be modeled as a string
over a 4-character alphabet. The proteins are chains of amino acids which can be
represented as strings via one or three letter codes. This is very useful, for example,
in designing experiments to test and modify the function of speciﬁc proteins, in
predicting the function and structure of proteins, and in identifying new members of
protein families (see [18]).
The multiple sequence alignment problem (MSA in short) can be described as
follows. Let Σ be a ﬁnite alphabet and let Σ∗= Σ ∪{−}, where ”−” is a symbol to
represent gaps in strings. Given a set S = {S1,S2,...,Sk} of ﬁnite strings over the
alphabet Σ, where each string Si, i = 1,...,k, consists of characters si,1,...,si,li ∈Σ.
A set S∗= {S∗
1,S∗
2,...,S∗
k} of strings over the alphabet Σ∗is called an alignment of
S if the following tree properties hold:
i) all strings in S∗have the same length l;
ii) ignoring gaps, S∗
i is identical with Si,∀i = 1,...,k;
An alignment in which each sequence S∗
i has length l can be associated with
an array of k rows and l columns, where row i corresponds to S∗
i . Two characters
of distinct sequences in S are said to be aligned under S∗if they are placed into
the same column of the alignment array (corresponding to S∗). Depending on the
measure of the quality of alignment, the multiple sequence alignment problem aims
to ﬁnd the ‘best’ alignment in some sense. For example when two sequences are
aligned, in each column depending on the characters aligned we have a score and the
scores are added over columns to measure the value of the alignment. An alignment
that maximizes the value is picked up. This idea is extended to multiple sequence
alignment as well.
Notice that adding a column of ’-’ to an alignment increases its length by one. But
it should not add value as the newly added column does not contain any character
from the original alphabet. However if it does we could add more columns of this
kind increasing the value of the alignment without a limit. Therefore, if an alignment
satisﬁes iii) then its length is not larger than ∑k
i=1 li.
iii) no column of an alignment has “-” in all its rows. That is, there exits an ir
such that s∗
ir,r ̸=“-” for each r,1 ≤r ≤l.
Computational biology provides many challenging combinatorial optimization
problems, and multiple sequence alignment (MSA) problem is one of them. It has
been shown in Kececioglu [26] that the MSA problem can be transformed into the
NP-complete Maximum Weight Trace problem (MWT in short), as a natural for-
mulation of merging partial alignments to form multiple alignments. The goal is to
compute an alignment S∗whose trace has maximum weight. It is based on the con-
cept of the alignment graph G = (V,E) whose nodes V correspond to the characters
of the k input sequences and are labeled correspondingly, the edge set E connects
pairs of characters that one would like to have aligned in an alignment of the input
strings.
Several methods have been developed to the MSA problem. Dynamic program-
ming has been applied to solve this problem but the curse of dimensionality

390
L.T. Hoai An
prevented its use to solve problems with large number of sequences or if the se-
quence lengths grew in size [45]. Recently [17] has given quadratic integer pro-
gramming formulations of some problems in computational biology which includes
the MSA problem. Other efﬁcient approaches to similar problems are found in [56]
and [57] .
Kececioglu [25] introduced and studied the Maximum Weight Trace (MWT) for-
mulation of the MSA problem. Reinert et al. [58] consider the trace polytope and
give an integer programming formulation of the MWT. They study the trace poly-
tope and show some of the separation problems corresponding to some class of
facet deﬁning inequalities can be solved in polynomial time. They use polyhedral
combinatorics to develop a branch-and-cut algorithm for solving the problem. Their
computational experiments show that the problem size both in terms of number
of sequences and sequence length could be increased beyond what was feasible
with dynamic programming approaches. Polyhedral approach to sequence align-
ment problems like generalized maximum trace problem, RNA sequence alignment
problem are studied by Kececioglu et al. [27], Lenhof [42].
Le Thi et al. [40] proposed a continuous optimization approach based on DC
(Difference of Convex functions) programming and DCA (DC Algorithm) to the
MSA using the 0-1 linear formulation of the MWT. The approach is inexpensive
because it amounts to solve a few linear programs and it converges after a ﬁnite
number of iterations to an integer solution while it works in a continuous domain.
Numerical simulation experiments show the superiority of this approach using DCA
with respect to other methods.
Prestwich et al. [52] give a pseudo-boolean local search algorithm and compare
its performance with state-of-the art algorithms for solving MSA problem. However
they report longer execution time though the quality is better in some bench mark
cases. They also have a polynomial size 0 −1 formulation of the MWT problem.
In [2], Arthanari and Le Thi proposed a new integer quadratic programming for-
mulation, which directly addresses the MSA problem. The number of constraints
and variables in the problem are only of the order of kL2, where, k is the number
of sequences and L is the total length of the sequences, that is, L = ∑k
i=1 li, where
li is the length of sequence i. Based on the new quadratic formulation of the MSA
problem, an equivalent linear constrained 0−1 quadratic programming problem is
introduced. Also a linear 0−1 formulation of the MWT problem is given. This for-
mulation has in addition (2L+1)|E| constraints, where |E| is the number of edges in
the alignment graph. And thus one has a compact formulation of the MWT problem.
17.3.1
Multiple Sequence Alignment Formulated as the MWT
Problem
The notion of trace of two sequences was generalized to multiple sequences by Ke-
cecioglu [26]. This leads us to the discussion on the maximum weight trace (MWT)
problem.

17
DCA for Challenging Problems
391
Deﬁnition 1. (Alignment Graph) Let Σ be a ﬁnite alphabet. Given a set S =
{S1,S2,...,Sk} of ﬁnite strings over the alphabet Σ, where each string Si, i =
1,...,k, consists of characters si,1,...,si,li ∈Σ. Consider the graph G = (V,E),
where V is the vertex set and E is the edge set, each edge si,j represents pairs of
characters (base symbols) that one would like to have aligned in an alignment of the
input sequences. We say that an edge of the alignment graph is realized by an align-
ment if the endpoints of the edge are placed in the same column of the corresponding
alignment array.
In the alignment graph one may associate w(e), a nonnegative weight attached to
each edge, and the trace of an alignment is deﬁned by
Deﬁnition 2. (Trace of an alignment) Given an alignment ˆS corresponding to a set
of sequences S. We call the subset of edges, T ⊂E, realized by ˆS as the trace of the
alignment, and w(T) = ∑e ∈E w(e) as the weight of the trace.
The multiple sequence alignment problem can now be stated as a maximum
weight trace problem (MWT): given a set S = {S1,S2,...,Sk} of ﬁnite strings over
the ﬁnite alphabet Σ, and an alignment graph G = (V,E), with nonnegative edge
weights, ﬁnd an alignment such that its trace has maximum weight.
While MWT is known to be NP-complete [25], polyhedral combinatorics has
been successfully used in MWT by Reinert et al. [58]. We shall give some graph
preliminaries and then present the binary integer formulation of the MWT problem
as developed by them.
17.3.1.1
Graph Preliminaries
We follow standard graph theoretic notations, regarding a graph. A mixed graph is
a tuple G = (V,E,A) where V is a set of vertices, E is a set of edges and A is a
set of arcs. We use e = {vi,vj} if e is an edge and we use e = (vi,vj) to denote an
arc, specifying the direction that it is from vi to vj. A path in a mixed graph is an
alternating sequence v1,e1,v2,e2,...,vk of vertices and arcs or edges such that in
vi,ei,vi+1, the consecutive vertices form the end points of the edges or arcs, for all
i,1 ≤i ≤k. A path is called a cycle if the ﬁrst and the last vertex on the path is the
same. We use the adjective ‘mixed’ to denote paths or cycles which contain at least
one each of an arc from A and an edge from E. The number of edges in a mixed
path is called its size. In addition we use the term length of a path (cycle) to denote
the number of edges and arcs it has.
Deﬁnition 3. (Extended Alignment Graph) [58]
Given an alignment graph, G = (V,E) we consider the mixed graph (V,E,H) by
adding a set of directed arcs
H = {(si,j,si,j+1)|1 ≤i ≤k,1 ≤j ≤li},
where si,j is the vertex in V corresponding to jth base symbol in sequence Si.
We call this mixed graph the extended alignment graph (EAG).

392
L.T. Hoai An
In addition in [58] we have the deﬁnition of a critical mixed cycle.
Deﬁnition 4. (Critical Mixed Cycle) Given the extended alignment graph, G =
(V,E,H), we call a mixed cycle R critical if for all i,1 ≤i ≤k, all vertices in R∩Si
occur consecutively in R.
The important characterization of traces using critical mixed cycles is given as
Theorem 1 in [58] which is stated as Theorem 1.
Theorem 1. Let G = (V,E,H) be an EAG, let T ⊂E and let G′ = (V,T,H) be the
EAG induced by T. Then T is a trace ⇐⇒there is no critical mixed cycle in G′.
Using this theorem we are in a position to state the integer programming formu-
lation in [58].
17.3.1.2
Binary Linear Programming Formulation of the MWT Problem
Let xe be the indicator variable of an e ∈E which is in the trace T, of an alignment,
ˆS. That is, xe = 1 if e in T and 0 otherwise. Denote by |·| the cardinality of a set.
Then the MWT problem can be stated as:
(MWT) max ∑
e∈E
wexe
s.t
∑
e∈Z∩E
xe ≤|Z ∩E|−1,
(17.10)
∀critical mixed cycles Z in G
xe ∈{0,1}, ∀e ∈E.
The formulation (17.10) has exponentially many constraints and belongs to the
class of Linear constrained Binary Linear programming problems (Binary Linear
programming problems in short). In [2] a compact Binary linear programming for-
mulation of the MWT problem has been proposed which has polynomially many
constraints and variables.
17.3.1.3
On Input Alignment Graph
The edges of the alignment graph and their weights are crucial for the success of
any method applied to the MWT formulation of the MSA. So far, ﬁnding appropri-
ate edges and weights has received only little attention. The edges of the alignment
graph can be obtained from pairwise alignments as well as from other methods like
local alignment algorithms. Using the dynamic programming algorithm of Myers

17
DCA for Challenging Problems
393
and Miller ( [44]) to align each pair of sequences, we compute (n
2) = n · (n −1)/2
pairwise alignment and store them in the pairwise alignment graph. The time to
compute the input graph and memory required is O(l3n2) with l = max
i=1,...,nli ( [44]).
The edge weight we can be determined in various ways:
• Sequence identity score (SIS): For every pair of aligned sequences, the number
of character matches t1 and that of character mismatches t0 are computed (compar-
isons with gaps remain unconsidered), then the percent sequence identity score is
equal to: SIS =
t1
t1+t0 ·100; which gives a result between 0% and 100%.
• 1-0 score (1-0S): Any edge, which connects the same characters, receives the
value 1 (100%), otherwise its weight is set to 0 (0%) ( [55]).
• CLUSTAL W’s similarity matrices with window ﬁltering (CSM) ( [54]).
• T-Coffee ( [46]): Once the sequence alignments have been calculated for all
pairs of input sequences, we construct the so-called library. A library for a pairwise
alignment of two sequence consists of the set of all realized edges together with a
weighting of each edge ( [54]).
17.3.2
Integer Quadratic Formulations of the MSA Problem
Recently Greenberg [17] has suggested some integer quadratic programming mod-
els in computational biology, and among them one of the models is for the MSA
problem. This model uses a scoring function that measures the propensity for a
character in a sequence to align with a character in another sequence. In addition
a gap penalty function, and two sets of variables to indicate the opening and ex-
tending of gaps in sequences are used in this model. The model also has additional
constraints involving theses variables. In [2] a so called True Sequence Alignment
(TSA) formulation having fewer constraints and variables has been proposed.
Let any alignment be given by an array having one row for each of the k se-
quences and l columns. We call (i,r) a cell. Each cell has either a “-” or a si,j for
some j. And in any row the basic symbols in the sequence Si, si,j’s appear in cells
(i,rj), such that 1 ≤r1 < r2,...,< rli, respectively. And we have an edge (si,j,sp,q)
of the alignment graph realized if the end points of the edge appear in the same col-
umn r, that is, si,j is in cell (i,r) and sp,q is in cell (p,r) for some r. We are interested
in maximizing the sum of the weights of the edges realized by an alignment.
17.3.2.1
True Sequence Alignment Formulation
Let aijr be deﬁned such that aijr = 1 if si,j occupies the cell (i,r) of the alignment,
otherwise aijr = 0, for all i, j,r: 1 ≤i ≤k,1 ≤j ≤li and j ≤r ≤l. We next give
the quadratic integer formulation of the MSA. Since the formulation gives a 1 −1
correspondence between alignments and 0 −1 solutions to the formulation we call
this formulation True Sequence Alignment (TSA) formulation.

394
L.T. Hoai An
(TSA Formulation)
maximizel,aijr
∑
(si,j,sp,q)∈E
l
∑
r=1
w((si,j,sp,q))aijrapqr
max
i=1...k(li) ≤l ≤
k
∑
i=1
li.
(17.11)
l
integer.
(17.12)
li
∑
j=1
aijr ≤1 for all i,r.
(17.13)
l
∑
r=1
aijr = 1, for all i, j.
(17.14)
aijr −
r−1
∑
s= j−1
ai(j−1)s ≤0, for all i,r, and 2 ≤j.
(17.15)
k
∑
i=1
li
∑
j=1
aijr ≥1, for all r.
(17.16)
aijr ∈{0,1} for all i, j,r.
(17.17)
The 1 −1 correspondence with alignments and the 0 −1 solutions to the TSA
formulation is formally stated in the following theorem ( [2]).
Theorem 2. Let Σ be a ﬁnite alphabet and let ˆΣ = Σ ∪{−}, where “−” is a symbol
to represent gaps in strings. Given a set S = {S1,S2,...,Sk} of ﬁnite strings over the
alphabet Σ, every alignment of S, ˆS = { ˆS1, ˆS2,..., ˆSk} of strings over the alphabet
ˆΣ, corresponds to an integer solution to the TSA formulation and vice versa.
17.3.2.2
Linear Constrained Binary Quadratic Programming Formulation
Basing on the TSA formulation Arthanari and Le Thi [2] developed an equivalent
linear constrained 0−1 quadratic programming (Binary Quadratic programming in
short) formulation of the MSA problem. It has been shown in [2] that to ﬁnd an
optimal solution to the TSA formulation it is sufﬁcient to solve Problem[L] below,
with L = ∑k
i=1 li.

17
DCA for Challenging Problems
395
(Problem [L])
maximize
∑
(si,j,sp,q)∈E
L
∑
r=1
w((si,j,sp,q))aijrapqr
li
∑
j=1
aijr ≤1 for all i,r.
(17.18)
L
∑
r=1
aijr = 1, for all i, j.
(17.19)
aijr −
r−1
∑
s= j−1
ai(j−1)s ≤0, for all i,r, and 2 ≤j.
(17.20)
aijr ∈{0,1} for all i, j,r.
(17.21)
The following theorem connects traces with the 0 −1 solutions of the TSA
formulation.
Theorem 3. Let G = (V,E) be the graph, where V is the set of vertices correspond-
ing to the base symbols si,j, in the given sequences Si,i = 1,...,k. And an edge
e = (si,j,sp,q) is in E if and only if w(e) > 0. T ⊂E is a trace corresponding to an
alignment if and only if there is a corresponding 0−1 solution to the TSA formula-
tion with the same weight of the trace, w(T).
17.3.3
Solution Methods by DC Programming and DCA
In terms of optimization, Binary Linear programming is a special case of Binary
Quadratic programming. In this subsection we show how to apply DC programming
and DCA on the common model of three problems — the MWT problem (17.10),
the TSA formulation and Problem[L], that is the Binary Quadratic programming
problem of the form:
minimize 1
2xTCx+ cTx
(17.22)
subjectto
Ax ≤b,
(17.23)
x ∈{0,1}n
(17.24)
where C is an (n × n) symmetric matrix, c, x ∈IRn, A is an (m × n) matrix and
b ∈IRm.
Binary quadratic programming (0 −1 QP in short) problems form an important
class of combinatorial optimization with application in many areas of science, tech-
nology, and business. They are known to be NP-hard [65]. Different approaches
are developed to solve such problems (see e.g. [5], [1], [47], [32], [51]. Pardalos
and Rodgers [47] discussed computational aspects of a branch and bound algorithm.

396
L.T. Hoai An
Adams and Sherali [1] proposed a tight linearization based algorithm for solving
such problems. Le Thi and Pham Dinh [32] developed a continuous approach based
on DCA for large scale problems of this kind. More recently, an efﬁcient combined
DCA and B&B using DC/SDP relaxation for globally solving binary quadratic pro-
grams has been developed in Pham et al. [51]. A recent survey of the approaches for
solving 0 −1 QP problems appears in [8].
17.3.3.1
From Combinatorial Optimization to DC Programming:
Reformulation
Based on exact penalty techniques in concave programming [30], the 0−1 QP prob-
lem (17.22) can be equivalently reformulated as a DC program in the following way.
Let D := {x ∈IRn : Ax ≤b} be a nonempty bounded polyhedral convex set in IRn
and let J := {1,...,n} (note that if J ⊂{1,...,n} then 0 −1 QP problems becomes
Mixed 0−1 QP problems, and the results in this subsection works also in this case).
Let λ be a positive number such that C−λI is a semideﬁnite negative (s.d.n) matrix
(here I stands for the identity matrix of order n). Since x ∈{0,1}n, xi = x2
i and
therefore the 0−1 QP Problem (17.22) can be rewritten as
min

f(x) := 1
2xT(C −λI)x+ (c+ (λ/2)e)T x : x ∈D, xi ∈{0,1},∀i ∈J
+
,
(17.25)
where e denotes the vector of ones in IRn. Obviously the function f is concave.
Let K := {x ∈D : 0 ≤xi ≤1,∀i ∈J} and deﬁne p(x) = 1
2 ∑i∈J xi(1 −xi). Clearly,
p is a concave function with nonnegative values on K and
{x ∈D, xi ∈{0,1}} = {x ∈K : p(x) = 0} = {x ∈K : p(x) ≤0}.
Hence (17.25) can be reformulated as (according to Theorem 4 below)
min{f(x) : x ∈K, p(x) ≤0}
which is equivalent to, for any t > to
min{f(x)+tp(x) : x ∈K}.
(17.26)
This is a concave quadratic programming problem which is also NP-hard (but all
the variables are continuous).
Theorem 4. [30] Let K be a nonempty bounded polyhedral convex set in IRn and
f, p be ﬁnite concave on K. Assume the feasible set of (P) be nonempty and p be
nonnegative on K. Then there exists to ≥0 such that for every t > to the following
problems have the same solution sets:
(Pt)
α(t) = inf{ f(x)+tp(x) : x ∈K},
(P)
α = inf{ f(x) : x ∈K, p(x) ≤0}.

17
DCA for Challenging Problems
397
Furthermore
(i) if the vertex set of K, denoted by V(K), is contained in {x ∈K : p(x) ≤0},
then to = 0.
(ii) if V(K) is not contained in {x ∈K : p(x) ≤0}, then to ≤f(xo)−α(0)
S
for every
xo ∈K, p(xo) ≤0, where S := min{p(x) : x ∈V(K), p(x) > 0}.
17.3.3.2
A DCA Based Algorithm for Solving Continuous Concave
Quadratic Programming Problem
According to the deﬁnition of f and p, the problem (17.26) can be expressed as
min

Ft(x) := 1
2xT (C −¯λI)x+

c+ ( ¯λ/2)e
	T x : x ∈K
+
(17.27)
with ¯λ := λ +t. A natural DC formulation of this problem is
min{Ft(x) := g(x)−h(x) : x ∈IRn} with g(x) := χK(x) and h(x) := −Ft(x).
(17.28)
With this DC formulation (Pt) is a polyhedral DC program because χK is a polyhe-
dral convex function, and the general DCA scheme becomes:
yk ∈∂h(xk);
xk+1 ∈argmin

−⟨x,yk⟩: x ∈K

.
(17.29)
Besides the computation of
yk := ∇h(xk) = (C −¯λI)xk + c+ (¯λ/2)e,
the algorithm requires one linear program at each iteration. The convergence prop-
erties can be stated as follows:
Theorem 5. i) DCA generates a ﬁnite sequence x1,...,xk∗contained in V(K) such
that f(xk+1) +tp(xk+1) ≤f(xk) +tp(xk), p(xk+1) ≤p(xk) for each k , and xk∗is
local minimizer of (17.28).
ii) Let t > t1 := max

f(x)−α(0)
S
: x ∈V(K)∩{x ∈K : p(x) = 0}

. If at an iteration
q one has p(xq) = 0, then p(xk)=0 and f(xk+1) ≤f(xk) ∀k ≥q.
17.3.3.3
A Constraint Generation Algorithm Based on DCA
Let G be the extended mixed graph (V,E,H) with H = {(si,j,si,j+1)|1 ≤i ≤k,1 ≤
j ≤li}, where si,j is the vertex in V corresponding to jth base symbol in sequence
Si. Since the number of mixed cycles in a graph is an exponential function of the
number of vertices, it is very difﬁcult, even impossible, to solve (MWT) when the
number of vertices of G is large. To avoid enumerating all the critical mixed cycles

398
L.T. Hoai An
in G (i.e. all constraints of (MWT)), a constraint generation procedure based on
DCA, called DCAMSA, has been proposed in [40]. A similar procedure can also be
developed to the TSA formulation and to Problem[L].
17.4
Molecular Conformation
In recent years there has been a very active research in the molecular optimization,
especially in the protein folding framework which is one of the most important prob-
lems in biophysical chemistry. Molecular optimization problems arise also in the
study of clusters (molecular cluster problems) and of large, conﬁned ionic systems
in plasma physics. The determination of a molecular conformation can be tackled
by either minimizing a potential energy function (if the molecular structure corre-
sponds to the global minimizer of this function) or solving the distance geometry
problem ( [11], [19]) (when the molecular conformation is determined by distances
between pairs of atoms in the molecule). Both methods are concerned with global
optimization problems.
17.4.1
Molecular Conformation via Distance Geometry Problem
The exact distance geometry problem consists in ﬁnding positions x1,...,xn of n
points in IRp such that
∥xi −xj∥= δ ij,(i, j) ∈S,
(17.30)
where S is a subset of the point pairs, δ ij with (i, j) ∈S is the given distance be-
tween points i and j, and ∥· ∥denotes the Euclidean norm. Usually, a small subset
of pairwise distances is known, i.e., S is small, and in practice, lower and upper
bounds of the distances are given instead of the exact values. In the molecular con-
formation, the problem is considered in three-dimensional Euclidean space - one
has to ﬁnd positions x1,...,xn of n atoms in IR3 verifying (17.30). In general, it can
be deﬁned in arbitrary dimensions p.
It should be noted that, by the error in the theoretical or experimental data, there
may not exist a solution to this problem, then an ε-optimal solution of (17.30),
namely a conﬁguration x1,...,xn satisfying
| ∥xi −xj∥−δij |≤ε,(i, j) ∈S,
(17.31)
is useful. When only the lower and upper bounds of δ ij are given, we are faced with
the so-called general distance geometry problem which consists of ﬁnding a set of
positions x1,...,xn in IRp such that
lij ≤∥xi −xj∥≤uij,(i, j) ∈S,
(17.32)
where lij and uij are lower and upper bounds of the distance constraints,
respectively.

17
DCA for Challenging Problems
399
In Euclidean distance geometry problems, we must take into consideration the
symmetry of both the subset S (i.e. (i, j) ∈S implies (j,i) ∈S). For simplifying the
presentation, let Sw be the set deﬁned by
Sw := {(i, j) ∈S : i < j}.
In the sequel Mn,p(IR) denotes the space of real matrices of order n × p and for
X ∈Mn,p(IR), Xi (resp. Xi) is its ith row (resp. ith column). By identifying a set
of positions x1,...,xn with the matrix X (i.e. (XT)i = (Xi)T = xi for i = 1,...,n), we
can advantageously express the exact and/or general distance geometry problems in
the matrix space Mn,p(IR):
0 = min

σ(X) := 1
2 ∑
Sw
wijθij(X) :
X ∈Mn,p(IR)
,
,
(17.33)
where wij > 0 for i ̸= j and wii = 0 for all i. The pairwise potential θ ij : Mn,p(IR) −→
IR is deﬁned for problem (17.30) by either
θ ij(X) =

δ 2
i,j −∥XT
i −XT
j ∥22
(17.34)
or
θ ij(X) =

δ ij −∥XT
i −XT
j ∥
	2 ,
(17.35)
and for problem (17.32) by
θij(X) = min2

∥XT
i −XT
j ∥2 −l2
ij
l2
ij
,0
,
+ max2

∥XT
i −XT
j ∥2 −u2
ij
u2
ij
,0
,
. (17.36)
When all pairwise distances are available and a solution exists, the exact distance
geometry problem (17.30) can be solved by a polynomial time algorithm (Blumen-
thal [6], Crippen and Havel [11]). However, in practice one knows only a subset of
the distances, and it is well known (Saxe [60]) that p−dimensional distance geom-
etry problems are strongly NP-complete with p = 1 and strongly NP-hard for all
p > 1. The visible sources of difﬁculties of these problems are
• The question of the existence of a solution,
• The non-uniqueness of solutions,
• The presence of a large number of local minimizers,
• The large scale of problems that arise in practice.
Several methods have been proposed for solving the distance geometry problems
(17.30) and/or (17.32). Practically, one is often faced with very large scale problems
for which global methods like branch and bound are expensive. So it is worth inves-
tigating local approaches conjointly with global techniques such as smoothing. An
advantage of local approaches for these problems is that the optimal value is a priori
known, therefore it will be easy to recognize whether the algorithm provides a global

400
L.T. Hoai An
minimizer. For a complete list of references the reader is referred to [39]. Here we
focus on DC programming and DCA based algorithms. DC programming and DCA
were extensively studied for solving both exact distance geometry problem ( [33])
and general distance geometry problem ( [31,34,35,39]). It has been shown in these
works that the DCA can be well exploited to obtain efﬁcient algorithms for both
exact and general large scale distance geometry problems. Key issues of DCA were
carefully studied and several techniques were proposed to exploit the nice effect of
DC decompositions and of starting points.
In [33] the exact distance geometry problem was considered in the form
(EDP)
0 = min

σ(X) := 1
2 ∑
(i,j)∈Sw
wij

δ ij −∥XT
i −XT
j ∥
	2 :
X ∈Mn,p(IR)
,
for which four DC formulations were introduced.
The ﬁrst is a DC formulation of (EDP) itself which takes the form
−1
2η2
δ = min

F1(X) := 1
2η2(X)−ξ(X) :
X ∈Mn,p(IR)
+
,
(17.37)
where η and ξ are the functions deﬁned on Mn,p(IR) by
η(X) = [∑
i< j
wijd2
ij(X)]
1/2 and ξ(X) = ∑
i< j
wijδ ijdij(X).
(17.38)
It is not difﬁcult to verify that η and ξ are two seminorms in Mn,p(IR) and thus
(17.37) is a DC program.
The second is the following convex maximization problem which was proved to
be equivalent to (EDP) via the stability of Lagrangian duality ( [33]):
ω := max{ξ(X) :
X ∈U(η)}
(17.39)
with U(η) := {X ∈Mn,p(IR) : η(X) ≤1}. This problem is in fact a DC program
of the form
−ω := min{χU(η)(X)−ξ(X) :
X ∈Mn,p(IR)}.
(17.40)
The preceding DC programs (17.37) and (17.40) for (EDP) involve the l1-norm in
the deﬁnition of their objective functions. Besides, Le Thi and Pham Dinh [33] in-
troduced the nonstandard l∞and l1−l∞approach to reformulating the exact distance
geometry problem by considering the function
Φ(X) := max{Φij(X) := 1
2wij[∥XT
i −XT
j ∥−δij]2 : (i, j) ∈Sw}.
That leads to the two nonstandard DC programs using the l∞−norm and the
combined l1 - l∞norms in their formulations:

17
DCA for Challenging Problems
401
0 = min

Φ(X) :=
max
(i, j)∈Sw

Φi j(X) := 1
2wi j[∥XT
i −XT
j ∥−δ i j]2 : X ∈Mn,p(IR)
++
,
(17.41)
0 = min

Φ(X)+ ρ
2
∑
(i,j)∈Sw
wij

δ ij −∥XT
i −XT
j ∥
	2 : X ∈Mn,p(IR)
,
.
(17.42)
Regularization techniqueswerealso investigated to DCprograms(17.37)and(17.40).
Various DCA schemes were developed for solving (EDP). Besides, a two phases al-
gorithm using shortest paths between all pairs of atoms to generate the complete
dissimilarity matrix was investigated in order to compute a good starting point for
the DCAs. Many numericalsimulations of the molecular optimization problems with
up to 12567 variables were reported which prove the practical usefulness of the non-
standard nonsmooth reformulations, the globality of found solutions, the robustness,
and the efﬁciency of DCA based algorithms.
In [31], Le Thi and Pham Dinh proposed an elegant formulation to the general
distance geometry problem (17.32) which is given by
(GDP) 0 = min

∑
(i, j)∈S
wi j(∥xi −x j∥−ti j)2 : x1,...,xn ∈IR3,li j ⩽ti j ⩽ui j,(i, j) ∈S
,
.
(17.43)
This formulation corresponds to both exact and general distance geometry prob-
lems, because in the exact distance geometry problem one has lij = uij = δ ij, then
we take tij = δ ij and (17.43) becomes (EDP). Note that the standard optimization
problem (17.33), (17.36) is a DC program but the objective DC function is too com-
plex and inconvenient for DCA. The objective function of (GDP) in [31] makes it
possible to express DCA in a simple form; it actually requires matrix-vector prod-
ucts and only one Cholesky factorization, and allows one to exploit sparsity in the
large scale setting. The algorithms proposed in [31] are in fact composed of two
phases: the ﬁrst phase consists of ﬁnding a good starting point for Phase 2. It be-
gins by completing the missing distance matrix (using the shortest paths between
the pairs of atoms) and then solves the exact distance geometry problem in which
all ”distances” are known by the DCA. The second phase is the DCA applied to the
original problem. The algorithms work well for the artiﬁcial test problems where a
protein contains at most 4096 atoms and for the real world models derived from the
PDB data bank (“http:www.rcsb.org/pdb/”) with up to 4189 atoms. A disadvantage
of these methods is that, although the strategy of Phase 1 is quite suitable for DCA
to reach global solutions to the distance geometry problem, it is quite expensive (the
running time of Phase 1 is equal to that of Phase 2).
To get around this drawback, more precisely, to ﬁnd a good starting point of DCA
without using Phase 1, Le Thi [35] proposed a combined DCA-smoothing tech-
nique. Instead of (17.43) a new DC formulation with a smooth (actually inﬁnitely
differentiable) objective function was considered:

402
L.T. Hoai An
0 = min

∑
(i,j)∈S
wij(∥xi −xj∥
2−t2
ij)2 : x1,...,: xn ∈IR3,lij ⩽tij ⩽uij,(i, j) ∈S
,
.
(17.44)
Like (17.43), this formulation corresponds to both exact and general distance ge-
ometry problems. The new formulation is favorable to the use of DCA as well
as smoothing techniques via the Gaussian transform, because that the objective
function is inﬁnitely differentiable, and its Gaussian transform can be computed
explicitly. Numerical experiments show that the proposed algorithm solves these
problems more efﬁciently and reliably than the two phase DCA based method de-
veloped in [31].
As an illustrated example, we consider below the general distance geometry prob-
lem (GDP) and brieﬂy present a DC formulation and the corresponding DCA for
solving it.
By identifying an n × p matrix X with a p × n−vector, in what follows, we use
either Mn,p(IR) or IRp×n for indicating the same notation. We can identify by rows
(resp. columns) each matrix X ∈Mn,p(IR) with a row-vector (resp. column-vector)
in (IRp)n (resp. (IRn)p) by writing respectively
X ←→X = (X1,...,Xn),XT
i ∈IRp,X T ∈(IRp)n,
(17.45)
and
X ←→X = (X1,...,X p)T,
Xi ∈IRn,X ∈(IRn)p.
(17.46)
The inner product in Mn,p(IR) is deﬁned as the inner product in (IRp)n or (IRn)p.
That is
⟨X,Y⟩Mn,p(IR) = ⟨X T,YT ⟩(IRp)n =
n
∑
i=1
⟨XT
i ,Y T
i ⟩IRp =
n
∑
i=1
XiY T
i
= ⟨X ,Y⟩(IRn)p =
p
∑
k=1
⟨Xk,Y k⟩IRn =
p
∑
k=1
(Xk)TY k = Tr(XTY).
Here Tr(XTY) denotes the trace of the matrix XTY. In the sequel, for simplicity, we
shall suppress, if no possible ambiguity, the indices for the inner product and denote
by ∥·∥the corresponding Euclidean norm on Mn,p(IR). Evidently, we must choose
either representation in a convenient way.
We ﬁrst prove that problem (GDP) is a DC program and point out DC compo-
nents of the objective function of this problem. Since
−2tij∥xi −xj∥= −(∥xi −xj∥+tij)2 + ∥xi −xj∥2 +tij2,
the objective function of (GDP) can be expressed as
1
2 ∑
(i,j)∈Sw
wij∥xi −xj∥2+ 1
2 ∑
(i,j)∈Sw
wij t2
ij −1
4 ∑
(i,j)∈Sw
wij(∥xi −xj∥+tij)2.
(17.47)

17
DCA for Challenging Problems
403
Remark that by setting wij = 0 for (i, j) /∈S the constraint (i, j) ∈S can be omitted
in (GDP). Since the functions
1
2 ∑
(i,j)∈Sw
wij∥xi −xj∥2 + 1
2 ∑
(i,j)∈Sw
wijt2
ij
and
1
4

∑
(i,j)∈Sw
wij(∥xi −xj∥+tij)2
,
are convex with respect to the variables (( x1,..., xn),T ) with x1,..., xn ∈IR3 and
T = (tij) on the convex constraint set, it is clear that the function given in (17.47) is
a DC function.
The matrix spaces that we shall present below are useful for various calculations of
subgradients in DCA.
Let ϕij : Mn,n(IR) −→IR be the pairwise function deﬁned by ϕij(T) = tij, and
let η be the function deﬁned by (17.38). Problem (GDP) can be now written in the
matrix form
 0 = inf {F(X,T) := L(X,T)−K(X,T)}
s.t.
(X,T) ∈Ω := Mn,p(IR)× T ,
with
L(X,T) := 1
2η2(X)+ μ(T), μ(T) := 1
2 ∑
i< j
wij ϕ2
ij(T),
K(X,T) := 1
4 ∑
i< j
wij[dij(X)+ ϕij(T)]2,
T := {T ∈Mn,n(IR) : lij ⩽tij ⩽uij,(i, j) ∈Sw}.
(17.48)
Clearly, the function L is ﬁnite and convex on Mn,p(IR) × Mn,n(IR). Since for
every (i, j) ∈S, the function: (X,T) →dij(X)+ ϕij(T) is ﬁnite and convex on
Mn,p(IR) × Mn,n(IR) and nonnegative on Ω, the function K then is convex on Ω
too. Let χΩ be the indicator function of Ω deﬁned by χΩ(X,T) = 0 if (X,T) ∈
Ω,+∞otherwise, then problem (GDP) can be expressed in the standard form of
DC programs:

0 = inf { G(X,T)−K(X,T)}
s.t.
(X,T) ∈Mn,p(IR)× Mn,n(IR),
(17.49)
where G(X,T) := L(X,T) + χΩ(X,T) is the separable function in its variables X
and T. Before going further let us make precise the obvious relation between prob-
lems (17.32) and (17.49).
Proposition 1. (i) If a set of positions (x1,...,xn) is a solution to problem (17.32),
then the couple of matrices (X,T), with X = (x1,...,xn)T and tij = ∥xi −xj∥for
(i, j) ∈S, is a solution to (17.49).
(ii) If a couple of matrices (X,T) is a solution to (17.49), then the set of positions
(x1,...,xn) = XT is a solution to (17.32) and tij = ∥XT
i −XT
j ∥for (i, j) ∈S.

404
L.T. Hoai An
Performing this scheme by DCA thus is reduced to calculating subdifferentials
of the functions K and G∗.
(Y (k),Z(k)) ∈∂K(X(k),T (k)),(X(k+1),T (k+1)) ∈∂G∗(Y (k),Z(k)).
(17.50)
Calculations of ∂K : Remark that Problem (17.49) only involves the restriction of
K to Ω where this function is convex. Actually we shall compute the conditional
subdifferential of K with respect to the convex set Ω ( [12]) that we again denote by
∂K for simplicity, i.e., (Y 0,W 0) is a conditional subgradient of K at (X0,T 0) ∈Ω
with respect to Ω if
K(X,T) ≥K(X0,T 0)+ ⟨(X,T)−(X0,T 0),(Y 0 ,W
0)⟩for (X,T) ∈Ω.
By the very deﬁnition, it is easy to state the following inclusion for (X,T) ∈Ω
∂K(X,T) ⊃1
2 ∑
i< j
wij[dij(X)+ ϕij(T)][∂dij(X)× {0} + {0}×∂ϕij(T)].
Since
ϕij(T) = tij= ⟨T,Eij⟩Mn,n(IR),
with Eij = eieT
j (ei ∈IRn is the unit vector with value one in the ith component
and zero otherwise), ϕij is differentiable on Mn,n(IR), and ∇ϕij(T) = Eij. Hence
∂K(X,T) contains the couples (Y,Z) deﬁned by
(Y,Z) = 1
2∑
i< j
wij(dij(X)+ ϕij(T))(Y(i, j),Eij),
with Y(i, j) ∈∂dij(X),i.e.
Y = 1
2 ∑
i< j
wijdij(X)Y(i, j) + 1
2 ∑
i< j
wijϕij(T)Y(i, j),
Z = 1
2 ∑
i< j
wij(dij(X)+ ϕij(T))Eij.
Thus Z = (zij) is deﬁned by
zij =
 1
2wij(dij(X)+tij) if (i, j) ∈S,
0
if (i, j) /∈S.
For determining Y, we ﬁrst compute R := ∑i< j wijdij(X)Y(i, j). By using the row-
representation of Mn,p(IR), dij can be expressed as :

17
DCA for Challenging Problems
405
dij = ∥.∥◦φij : (IRp)n −→IRp −→IR, X −→φ ij(X) = XT
i −XT
j −→∥XT
i −XT
j ∥,
we have ( [59])
∂dij(X) = φ T
ij∂(∥.∥)(φij(X)).
Hence
Y(i, j) ∈∂dij(X) ⇔Y(i, j) = φT
ijy,
y ∈∂(∥.∥)(XT
i −XT
j ),
which implies
Y(i, j)T
k = 0 if k /∈{i, j} and
Y(i, j)T
i = −Y(i, j)T
j ∈∂(∥.∥)(XT
i −XT
j ). (17.51)
Then Y(i, j) can be chosen as
Y(i, j)i = −Y(i, j)j =

Xi−Xj
∥XT
i −XT
j ∥if Xi ̸= Xj,
0
otherwise.
(17.52)
And so
Rk := ∑
i< j
wijdij(X)Y(i, j)k = ∑
i<k
wikdik(X)Y(i,k)k + ∑
j>k
wk jdk j(X)Y(k, j)k
= ∑
i<k
wik(Xk −Xi)+ ∑
j>k
wjk(Xk −Xj) =
-
n
∑
i=1
wik
.
Xk −
n
∑
i=1
wikXi.
It follows that
R = ∑
i< j
wijdij(X)Y(i, j) = VX,
(17.53)
where V = (vij) is the n × n matrix given by
vij =
 −wij
if i ̸= j,
∑n
k=1 wik if i = j.
(17.54)
By the same way, we get
∑
i< j
wijϕij(T)Y(i, j) = C(X,T)X,
(17.55)
where C(X,T) = (cij(X,T)) is the n × n matrix valued function given by
cij(X,T) =
 −wijtijsij(X) if i ̸= j,
−∑n
k=1,k̸=i cik if i = j,
(17.56)
with
sij(X) =
 1/(∥XT
i −XT
j ∥) if Xi ̸= Xj and (i, j) ∈S,
0
otherwise.
(17.57)

406
L.T. Hoai An
Finally, we have
Y = 1
2(V +C(X,T))X.
(17.58)
It has been shown in [31] that the range of V and that of C(X,T), for a given
couple (X,T), are contained in A⊥, where
A := {X ∈Mn,p(IR) : X1 = ··· = Xn}.
Hence the sequence {Y (k)} deﬁned by (17.50) is contained in the subspace A⊥.
Computing ∂G∗. As aforementioned, the calculation of ∂G∗(Y (k),Z(k)) consists of
solving the next problem:
min{G(X,T)−⟨(Y (k),Z(k)),(X,T)⟩: (X,T) ∈Mn,p(IR)× Mn,n(IR) }.
Since the functions G is separable in its variables, the last problem can be decom-
posed into the following two problems:
min
1
2η2(X)−⟨Y(k) ,X⟩: X ∈Mn,p(IR)
+
,
(17.59)
min

μ(T)−⟨Z(k) ,T⟩: T ∈T

.
(17.60)
According to the properties of V, solving (17.59) is reduced to computing the solu-
tion of the nonsingular linear system (17.61).
#
V + 1
neeT
$
X = Y.
(17.61)
It is easy to see that the solutions of (17.60) can be explicitly determined. Indeed,
by deﬁnition of Z(k)= (z(k)
ij ) we have
μ(T)−⟨Z(k) ,T⟩= 1
2
∑
i< j,(i,j)∈S
wijt2
ij −
n
∑
i,j=1
z(k)
ij tij
= 1
2 ∑
(i,j)∈Sw
wijt2
ij −1
2 ∑
(i,j)∈Sw
wij(dij(X(k))+tij(k))tij.
The solutions to (17.60) can be explicitly computed in the following way. T ∗= (t∗
ij)
is a solution to this problem if and only if tij is arbitrary for (i, j) /∈S and
t∗
ij =
⎧
⎪
⎨
⎪
⎩
1
2(dij(X(k))+t(k)
ij ) if
lij ⩽1
2(dij(X(k))+t(k)
ij ) ⩽uij,(i, j) ∈S,
lij
if
1
2(dij(X(k))+t(k)
ij ) < lij,(i, j) ∈S,
uij
if
1
2(dij(X(k))+t(k)
ij ) > uij,(i, j) ∈S.
(17.62)

17
DCA for Challenging Problems
407
Only the components t(k+1)
ij
with (i, j) ∈S actually intervene in Problem (17.49), so
we set T (k+1) = (t(k+1)
ij
) as follows:
t(k+1)
ij
= 0 for (i, j) /∈S and t(k+1)
ij
= t∗
ij for (i, j) ∈S.
(17.63)
From the above displayed calculations, we can now provide the description of
the DCA applied to (17.49).
Description of the DCA for solving (17.49)
GDCA (DCA applied to (17.49)).
The sequence {(X(k),T (k))} with X(k) ∈A⊥is generated as follows:
Let ε > 0, and X(0) ∈A⊥\ {0},T(0) ∈T be given.
For k = 0,1,... until
lij ≤∥X(k)
i
−X(k)
j ∥≤uij, for all (i, j) ∈S
(17.64)
determine T (k+1) following (17.62), and solve the nonsingular linear system
(V + 1
neeT)X = 1
2(V +C(X(k),T (k)))X(k)
(17.65)
to obtain X(k+1).
17.4.2
Molecular Conformation by Minimizing the
Lennard-Jones Energy Function
The Lennard-Jones potential energy is deﬁned by
f(x) = f(x1,x2,...,xn) :=
n−1
∑
i=1
n
∑
j=i+1
v(∥xi −xj∥),
where n is the number of atoms in the cluster, ∥xi −xj∥stands for the Euclidean
distance between atoms xi and xj, and the Lennard-Jones pair potential v(r) is given
by:
v(r) := 1
r12 −2
r6 for r > 0.
The so called Lennard-Jones problem is to determine atomic cluster conﬁgurations
with minimum Lennard-Jones potential energy. It is one of the most studied and
signiﬁcant problems in molecular biophysics and biochemistry. Due to the non con-
vexity of the Lennard-Jones energy function and the large number of distinct local
minima of this function (about O(en2)), the Lennard-Jones problem has been con-
sidered to be a very difﬁcult and challenging global optimization problem. It has
become a benchmark for any new global optimization algorithm.

408
L.T. Hoai An
Several algorithms have been proposed during the past thirty years for solving
this problem, with signiﬁcant progresses, especially in recent years (see e.g. [3], [4],
[13] - [15], [21], [23], [43], [53], [63], [64], [37], [41]). At the current time, the low-
est energies of Lennard-Jones clusters containing up to 10000 atoms are archived.
However, global optima of the Lennar-Jones energies are known only within the
range 2 ≤n ≤147.
The Lennard-Jones potential energy can not be directly written as a DC function.
In [37,41], using some properties of composite functions and convex functions, Le
Thi and Pham Dinh transformed f into a DC function and introduced a nice DC
reformulation of the Lennard-Jones problem. Afterwards the authors developed a
DCA scheme and a two phase DCA based algorithm for the resulting DC program.
The proposed methods successfully locate the lowest known minima with a good
performance on running time. Then, putative global minima of clusters having up
to 6525 atoms are predicted with these methods. We describe shortly the approach
developed in [37,41]. First, the Lennard-Jone energy function is rewritten as
f(.) = f(x1,x2,...,xn) :=
n−1
∑
i=1
n
∑
j=i+1
w(∥xi −xj∥2),
(17.66)
where the function w is deﬁned on ]0,+∞[ by
w(r) := 1
r6 −2
r3 , r > 0.
For a given r0 ∈(0, 1
2], let W1 and W2 be the convex functions on the whole IR
given by
W1(r) :=

1
r6
if r ≥r0,
a1r + b1 if r ≤r0, , W2(r) :=

2
r3
if r ≥r0,
a2r + b2 if r ≤r0,
where
a1 := −6
r7
0
, b1 = 1
r6
0
−a1r0 = 7
r6
0
, a2 := −6
r4
0
, b2 = 2
r3
0
−a2r0 = 8
r3
0
.
Now let G and H be the functions deﬁned by
G(x1,x2,...,xn) :=
n−1
∑
i=1
n
∑
j=i+1
g(∥xi −x j∥2),: H(x1,x2,...,xn) :=
n−1
∑
i=1
n
∑
j=i+1
h(∥xi −x j∥2)
(17.67)
where
g(r) := W1(r)+ K0r, h(r) := W2(r)+ K0r, K0 := 6
r7
0
.
(17.68)
Then the following result has been proved in [37,41] :

17
DCA for Challenging Problems
409
Theorem 6. The Lennard-Jones problem is equivalent to the next DC program:
min{G(x1,x2,...,xn) −H(x1,x2,...,xn) | xi ∈IR3,i = 1,...,n}
(17.69)
in the sense that they have the same optimal value and the same set of global minima.
DCA applied on this DC program consists of computing at each iteration a sub-
gradient of H and solving a smooth convex program ( [37,41]).
17.4.3
Molecular Conformation by Minimizing the Morse Energy
Function
The Morse potential is a convenient model for the potential energy of a diatomic
molecule. The Morse potential energy is deﬁned by
M(x) = M(x1,x2,...,xn;ρ) :=
n−1
∑
i=1
n
∑
j=i+1
E(∥xi −xj∥;ρ),
where n is the number of atoms xi = (xi1,xi2,xi3) ∈IR3,( i = 1,2,...,n) in the clus-
ter, ∥xi −xj∥stands for the Euclidean distance between xi and xj, and the Morse
pair potential E(r;ρ) is deﬁned as follows: (ρ > 0 is a parameter)
E(r;ρ) := e2ρ(1−r) −2erho(1−r).
The so called Morse problem then is to determine atomic cluster conﬁgurations with
minimum Morse potential energy.
Since the Morse energy is nonconvex and the number of distinct local minima
in the potential energy surface of an n atoms Morse cluster is about O(en2), the
Morse problem is considered to be a very difﬁcult and challenging global optimiza-
tion problem. Although many algorithms have been proposed during the past thirty
years, good putative global optima are known for small size problems. Like the
Lennard-Jones problem, the Morse problem has become a benchmark for any new
global optimization algorithms.
In [38] a nonsmooth DC formulation of the Morse problem and the correspond-
ing DCA have been proposed. Preliminary computational results (for the magic se-
quence n = 13,55,...,2057) are very promising, they show the efﬁciency, globality
and robustness of DCA with respect to existing methods.
17.5
Phylogenetic Tree Reconstruction
A fundamental task in evolutionary biology is the inference of phylogenetic or an-
cestral relationships among contemporary species. Given a set of aligned sequences
(genes, proteins) from species, the goal of the phylogenetic tree reconstruction is to

410
L.T. Hoai An
reconstruct the tree which best explains the evolutionary history of this gene/protein.
Phylogenetic tree reconstruction is still a challenge today. Many concrete questions
are still unresolved (e.g. mammalian evolutionary tree). Most realistic formulations
of the problem, which take errors into account, give rise to hard computational prob-
lems. Popular phylogeny reconstruction methods can be divided into two main cat-
egories: Distance based methods (UPGMA, Neighbor Joining, Buneman trees) and
Character based methods (Maximum Parsimony, Maximum Likelihood). Besides,
there are some additional methods such as Quartets based, Disk-Covering. Experi-
mental simulations in several works (see [22] for instance) indicate that maximum
likelihood (ML) estimates of phylogenetic trees are consistently superior to parsi-
mony or distance-based method. ML is one of the most widely used techniques to
infer evolutionary histories.
Given a set of observed sequences and an underlying substitution model. ML
aims to ﬁnd the weighted tree (the tree with corresponding branch lengths) that
maximizes the likelihood function L(x,τ) jointly over the vector of branch lengths x
(continuous variables specifying the length of each edge in the tree) and the topology
τ (combinatorial variable). The likelihood of a data is the conditional probability of
producing the data, given the model parameters. Likelihood is a common optimiza-
tion criteria in numerous settings, including phylogenetic. Du to the existence of
multiple maxima, the problem of computing globally optimal ML estimates of phy-
logenetic trees is NP-hard and computationally intractable (see for example [10]).
Even if one of the two variables, x or τ, is held ﬁxed, the likelihood maximization
is difﬁcult. For a ﬁxed topology τ, and for most reasonable probabilistic models of
nucleotide substitution, the likelihood function L(x,τ) is a highly nonlinear function
of the branch lengths x. In [16] the authors proved that for a given τ the likelihood
L(x,τ) is a DC function and proposed a cutting plane method for computing the
branch lengths x by solving the problem
max
x≥0 lnL(x).
(17.70)
This algorithms works on small datasets (5 sequences). In [29] the author developed
a DCA based algorithm for (17.70) which works well on large size datasets. Hence
the use of DCA is very recommended in phylogenetic trees reconstruction.
17.6
Conclusion
We have presented DC programming and DCA for modeling and solving three chal-
lenging classes of problems in computational biology. All the problems considered
are also great challenge for the community of optimizers. These theoretical and
algorithmic tools have been outlined in an appropriate way to make them under-
standable to the reader. They highlight the distinctive features (ﬂexibility, versatility,
inexpensiveness, scalability, efﬁciency and globality) of DC programming and DCA.
It is desirable that our approaches will help researchers and practitioners tackle efﬁ-
ciently their nonconvex programs, especially in the large-scale setting.

17
DCA for Challenging Problems
411
References
1. Adams, W.P., Sherali, H.D.: A tight linearization and an algorithm for 0-1 quadratic
programming problems. Management Science 32(10), 1274–1290 (1986)
2. Arthanari, T.S., Le Thi, H.A.: New formulations of the multiple sequence alignment
problem. Optimization Letter 5(1), 27–40 (2011)
3. Barron, C., Gomez, S., Romero, D.: Lower Energy Icosahedral Atomic Cluster with
Incomplete Core. Applied Mathematics Letters 10(5), 25–28 (1997)
4. Barron, C., Gomez, S., Romero, D., Saavedra, A.: A Genetic Algorithm for Lennard-
Jones Atomic clusters. Applied Mathematics Letters 12, 85–90 (1999)
5. Billionnet, A., Elloumi, S.: Using a mixed integer quadratic programming solver for
unconstrained quadratic 0-1 problem. Math. Programming 109(1, Ser.A), 55–68 (2007)
6. Blumenthal, L.M.: Theory and Applications of Distance Geometry. Oxford University
Press (1953)
7. Cai, W., Jiang, H., Shao, X.: Global optimization of Lennard-Jones clusters by a parallel
fast annealing evolutionary algorithm. Journal of Chemical Information and Computer
Sciences 42(5), 1099–1103 (2002)
8. Caprara, A.: Constrained 0-1 quadratic programming: Basic approaches and extensions.
European Journal of Operational Research 187, 494–1503 (2008)
9. Carr, R.D., Lancia, G.: Compact vs. Exponential-size LP relaxations. Operations Re-
search Letters 30, 57–65 (2002)
10. Chor, B., Tuller, T.: Maximum likelihood of evolutionary trees is hard. In: Miyano, S.,
Mesirov, J., Kasif, S., Istrail, S., Pevzner, P.A., Waterman, M. (eds.) RECOMB 2005.
LNCS (LNBI), vol. 3500, pp. 296–310. Springer, Heidelberg (2005)
11. Cripen, G.M., Havel, T.F.: Distance Geometry and Molecular Conformation. John Wiley
& Sons (1988)
12. Demyanov, V.F., Vasilev, L.V.: Nondifferentiable optimization. Optimization Software,
Inc. Publications Division, New York (1985)
13. Deaven, D.M., Tit, N., Morris, J.M., Ho, K.M.: Structural optimization of Lennard-Jones
clusters by a genetic algorithm. Chemical Physics Letters 256(1), 195–200 (1996)
14. Deng, Y., Rivera, C.: Approximate energy minimization for large Lennard-Jones clusters.
Journal of Global Optimization 16(4), 325–341 (2000)
15. Doye. J.P.K.: Thermodynamics and the global optimization of Lennard-Jones clusters.
Journal of Chemical Physics 109(19), 8143–8153 (1998)
16. Ellis, S.E., Nayakkankuppam, M.V.: Phylogenetic analysis via DC programming. De-
partment of Mathematics and Statistics (preprint)
17. Greenberg, H.J.: Integer Quadratic Programming Models in Computational biology. Op-
erations Research Proceedings 2006, 83–95 (2007)
18. Gusﬁeld, D.: Algorithms on Strings, Trees, and Sequences. Cambridge University Press
(1997)
19. Havel, T.F.: An evaluation of computational strategies for use in the determination of
protein structure from distance geometry constraints obtained by nuclear magnetic reso-
nance. Prog. Biophys. Mol. Biol. 56, 43–78 (1991)
20. Hiriart Urruty, J.B., Lemarechal, C.: Convex Analysis and Minimization Algorithms.
Springer, Heidelberg (1993)
21. Huang, H.X., Pardalos, P.M., Shen, Z.J.: Equivalent formulations and necessary op-
timality conditions for the Lennard-Jones problem. Journal of Global Optimization
22(1-4), 97–118 (2002)
22. Huelsenbeck, J.P.: Performance of phylogenetic methods in simulation. Systematic Bi-
ology 44, C17–C48 (1995)

412
L.T. Hoai An
23. Jiang, H., Cai, W., Shao, X.: New lowest energy sequence of marksf decahedral Lennard-
Jones clusters containing up to 10,000 atoms. Journal of Physical Chemistry A 107(21),
4238–4243 (2003)
24. Kalantari, B., Rosen, J.B.: Algorithm for global minimization of linearly constrained
concave quadratic functions. Mathematics of Operations Research 12, 544–561 (1987)
25. Kececioglu, J.: The maximum weight trace problem in multiple sequence align-
ment. In: Proceedings of the 4th Symposium on Combinatorial Pattern Matching,
pp. 106–119 (1993)
26. Kececioglu, J.D.: Exact and Approximation Algorithms for DNA Sequence Reconstruc-
tion. PhD thesis, University of Arizona (1991)
27. Kececioglu, J.D., Lenhof, H.P., Mehlhorn, K., Mutzel, P., Reinert, K., Vingron, M.:
A polyhedral approach to sequence alignment problems. Discrete Applied Mathemat-
ics 104, 143–186 (2000)
28. Le Thi, H.A.: DC Programming and DCA,
http://lita.sciences.univ-metz.fr/˜lethi
29. Le Thi, H.A.: Phylogenetic tree reconstruction by a DCA based algorithm. Research
Report, LITA, University of Lorraine, 1–27 (2013)
30. Le Thi, H.A., Pham Dinh, T., Muu, L.D.: Numerical solution for optimization over the
efﬁcient set by d.c. optimization algorithm. Operations Research Letters 19, 117–128
(1996)
31. Le Thi, H.A., Pham Dinh, T.: D.C. programming approach for large scale molecular opti-
mization via the general distance geometry problem. In: Optimization in Computational
Chemistry and Molecular Biology: Local and Global Approaches. pp. 301–339. Kluwer
Academic Publishers (2000)
32. Le Thi, H.A., Pham Dinh, T.: A continuous approach for large-scale constrained
quadratic zero-one programming. Optimization (In honor of Professor ELSTER,
Founder of the Journal Optimization) 50(1-2), 93–120 (2001)
33. Le Thi, H.A., Pham Dinh, T.: Large Scale Molecular Optimization from distance ma-
trices by a d.c. optimization approach. SIAM Journal on Optimization 14(1), 77–114
(2003)
34. Le Thi, H.A., Pham Dinh, T.: A new algorithm for solving large scale molecular dis-
tance geometry problems. In: Applied Optimization: Special Issue “HighPerformance
Algorithms and Software for Nonlinear Optimization”, pp. 279–296. Kluwer Academic
Publishers (2003)
35. Le Thi, H.A.: Solving large scale molecular distance geometry problems by a smoothing
technique via the gaussian transform and d.c. programming. Journal of Global Optimiza-
tion 27(4), 375–397 (2003)
36. Le Thi, H.A., Pham Dinh, T.: The DC programming and DCA revisited with DC models
of real world nonconvex optimization problems. Annals of Operations Research 133,
23–46 (2005)
37. Le Thi, H.A., Pham Dinh, T.: A two phases DCA based algorithm for solving the
Lennard-Jones problem. Research Report, LITA, University of Metz, 1–36 (2011)
38. Le Thi, H.A., Pham Dinh, T.: Minimizing the Morse potential energy function by a DC
programming approach. Research Report, LITA, University of Lorraine, 1–30 (2012)
39. Le Thi, H.A., Pham Dinh, T.: DC programming approaches for Distance Geometry prob-
lems. In: Mucherino, A., Lavor, C., Liberti, L., Maculan, N. (eds.) Distance Geometry:
Theory, Methods and Applications. Springer (2013)
40. Le Thi, H.A., Pham Dinh, T., Belghiti, T.: DCA based algorithms for Multiple Sequence
Alignment (MSA). Central European Journal of Operations Research, 1–24 (2013)

17
DCA for Challenging Problems
413
41. Le Thi, H.A., Pham Dinh, T.: DC programming and DCA for minimizing Lennard-Jones
potential energy (submitted, 2014)
42. Lenhof, H.P., Retnert, K., Vingron, M.: A Polyhedral Approach to RNA Sequence Struc-
ture Alignmen. Journal of Computational Biology 5(3), 517–530 (1998)
43. Locatelli, M., Schoen, F.: Efﬁcient algorithms for large scale global optimization:
Lennard-Jones clusters. Computational Optimization and Applications 26(2), 173–190
(2003)
44. Myers, E., Miller, W.: Optimal alignments in linear space. Computer Applications in the
Biosciences 4(1), 11–17 (1988)
45. Notredame, C.: Recent progresses in multiple sequence alignment: a survey. Pharma-
cogenomics 3(1), 131–144 (2002)
46. Notredame, C., Higgins, D.G., Heringa, J.: T-COFFEE: A novel method for fast and
accurate multiple sequence alignment. J. Mol. Biol. 392, 205–217 (2000)
47. Pardalos, P.M., Rodgers, G.P.: Computational aspects of a branch and bound algorithm
for quadratic zero-one programming. Computing 45, 131–144 (1990)
48. Pham Dinh, T., Le Thi, H.A.: Convex analysis approach to d.c. programming: The-
ory, Algorithms and Applications, Acta Mathematica Vietnamica (dedicated to Professor
Hoang Tuy on the occasion of his 70th birthday) 22, 289–355 (1997)
49. Pham Dinh, T., Le Thi, H.A.: D.C. optimization algorithms for solving the trust region
subproblem. SIAM J. Optimization 8, 476–505 (1998)
50. Pham Dinh, T., Le Thi, H.A.: Recent advances in DC programming and DCA. In:
Nguyen, N.-T., Le-Thi, H.A. (eds.) TCCI 2013. LNCS, vol. 8342, pp. 1–37. Springer,
Heidelberg (2014)
51. Pham Dinh, T., Nguyen, C.N., Le Thi, H.A.: An efﬁcient combined DCA and B&B using
DC/SDP relaxation for globally solving binary quadratic programs. J. Global Optimiza-
tion 48(4), 595–632 (2010)
52. Prestwich, S., Higgins, D., O’Sullivan, O.: Pseudo-Boolean Multiple Sequence Align-
ment. Technical Report,TR-03-2003,, Cork Constraint Computation Centre, University
College, Cork, Ireland (2003), http://www.4c.ucc.ie/web/techreps.jsp
53. Oskolkov, N.N., Jakob, B.: A Lennard-Jones-like perspective on ﬁrst order transitions in
biological helices. Central European Journal of Physics 11(3), 357–362 (2013)
54. Thompson, J.D., Higgins, D.G., Gibson, T.J.: CLUSTAL W:Improving the sensitiv-
ity of progressive multiple sequence alignment through sequence weighting, posi-
tion speciﬁc gap penalties and weight matrix choice. Nucleic Acids Research 22(22),
4673–4680 (1994)
55. Thompson, J.D., Plewniak, F., Poch, O.: BAliBASE: A benchmark alignments database
for the evaluation of multiple sequence alignment programs. Bioinformatics 15, 87–88
(1999)
56. Rajasekaran, S., Nick, H., Pardalos, P.M., Sahni, S., Shaw, G.: Efﬁcient Algorithms for
local alignment search. Journal of Combinatorial Optimization 5(1), 117–124 (2001)
57. Rajasekaran, S., Hu, Y., Luo, J., Nick, H., Pardalos, P.M., Sahni, S., Shaw, G.: Efﬁcient
Algorithms for similarity alignment search. Journal of Combinatorial Optimization 5(1),
117–124 (2001)
58. Reinert, K., Lenhof, H., Mutzel, P., Mehlhorn, K., Kececioglu, J.D.: A branch-and-cut
algorithm for multiple sequence alignment. In: RECOMB, pp. 241–250 (1997)
59. Rockafellar, R.T.: Convex Analysis. Princeton University, Princeton (1970)
60. Saxe, J.B.: Embeddability of weighted Graphs in k-space is strongly NP-hard. In: Proc.
17 Allerton Conference in Communications, Control and Computing, pp. 480–489
(1979)

414
L.T. Hoai An
61. Searls, D.: Grand Challenges in Computational Biology. In: Salzberg, S., Searls, D.,
Kasif, S. (eds.) Computational Methods in Molecular Biology. Elsevier Science (1998)
62. Shashi, K.D., Katiyar, V.K.: Global Optimization of Lennard-Jones Potential Using
Newly Developed Real Coded Genetic Algorithms. In: Proceedings of IEEE Interna-
tional Conference on Communication Systems and Network Technologies, pp. 614–618
(2011)
63. Xiang, Y., Cheng, L., Cai, W., Shao, X.: Structural distribution of Lennard-Jones
clusters containing 562 to 1000 atoms. Journal of Physical Chemistry A 108(44),
9516–9520 (2004)
64. Xue, G.L.: Minimum Inter-Particle Distance at Global Minimizers of Lennard-Jones
Clusters. Journal of Global Optimization 11, 83–90 (1997)
65. Vavasis, S.A.: Nonlinear Optimization, Complexity Issues, Oxford University Press
(1991)

Subject Index
(≥ℓ)-creative
5, 51
G(X) 2.3
42
L = ∂M
6 53
Lp metrics
328
Sn ⊕Sp = Sn+p
4, 56
Sn ⊗Sp = Sn×p
5, 57
SmaxX
20, 56
Sn deﬁnition
3 , 49
E
9, 61
P0(N)
9, 61
P0,/0(N)
9, 61
Final
2.5.2, 64
maxX +maxY
19, 56
maxX ∈Y
18 , 55
Periodic
21, 56
Single
2.5.2, 64
k-creative
5, 51
nX
2.1, 41
Special
quad 3, 53
2-tag system
117
active
cell
277
particle
277
agent algorithm
288
algorithmization
88
alphabet
153
arithmetic hierarchy
274
array
language
172
P system
174
astroid
328
atom
13
atomic decomposition
15
automatic predicate
266
billiard ball model
130
bit model
95
Boolean connectives
264
boolean with same min
25, 69, 70
boundary conditions
263
broadcasting automata
298, 300
broadcasting sequences
310, 322
BSS model
95
Busy Beaver problem
96
cellular automata
259, 306, 341
1-bit inter-cell communication
342
CA
277
CA-w (with write-access)
278, 282
constant-bit communication
345
chain code
310
checking stack automaton
113
Chomsky grammar
173
circular sequence
20
classiﬁcation
260
Collatz problem
94, 117
complete sequence
150
computability
260
computable sequence
154
computational biology
383
computational completeness
208
concave hull
360
conﬁguration
260
bi-inﬁnite conﬁguration
267
ﬁnite conﬁguration
262
inﬁnite conﬁguration
266

416
Subject Index
conﬂuence
271
controlled splicing
241
convex hull
360
convex polygon
322
convolution
263
CPU design
90
crowd-sourcing
274
Curtis-Hedlund-Lyndon theorem
259
curve
21
DC programming
385
DCA convergence
387
de Bruijn graph
263
decidability
117, 260
decidable
262, 265, 267, 268
decision problem
263
degree
274
deterministic Turing machine
132
determinization
264
discrete circles
318
discrete disc
317
disordered patterns
365
DNA
237
don’t write symbol
281
double splicing
244
effective Boolean algebra
264
elementary substructure
266, 268
emptiness
265, 267
ENIAC
80, 88
ET0L system
197
Fibonacci numbers
287
ﬁnite state machine
261
ζ-automaton
261
B¨uchi automaton
261, 266
word automaton
261
ﬁrst-order theory
260
ﬁxed point
270
Formalized proofs
96
four color theorem
90, 97
Game of Life automata
367
glider, 366
368
collision
370
glider gun
374
global cellular automata
GCA
278
GCA-w (with write-access)
279
global map
259
good submonoid
5, 52
graph
5
t-graph
7
(2,2)-regular -
6
circular composition
10, 11
connected -
5
edge-connected -
6, 13
straight walk
7
transitions
7, 8
grid lattice
304
Grossone
148
Grossone Language
147, 153, 166
Grossone Methodology
147, 153, 166
halting problem
117
Ignorabimus
77
Inﬁnite Sequence
148, 149
information
dependency
106
ﬂow
106
initial segment
17, 55
interaction gate
130
irreversibility
89
iterated splicing
240
labyrinthine patterns
363
leader election for agents
292
Lennard-Jones potential energy
407
Life-like automata
367
linear convergence
388
local map
259
localisations
365
Mandelbrotset
94
map
8
dart
6, 8
maximal submonoid
14, 48
maximum weight trace
389
membrane computing
171
metric approximation
310, 328
metric space
301
middle square method
81
minimum generating set
2.3, 42
model checking
261
data model checking
261
expression model checking
261
molecular conformation
384, 398
Monte Carlo method
81, 93

Subject Index
417
Moore
machine
300
neighbourhood
307
Morse potential
409
multiple alignment of sequences
384
multiple sequence alignment
388
multiword
2
ambiguous -
25
connected -
23
double occurrence -
21
Gauss -
21
prime -
23
neighbourhood sequences
308
network of automata
300, 303
nilpotency
271
non-erasing Turing machine
117
non-uniformity
82, 91
nonconvex optimization
383
nondeterministic k-counter machine
107
Numeral System
140
numerical analysis
85, 87, 90
odd–even sort
289
open source
92, 98
operational state
280
oscillator
373
oxygen
354
P automaton
219
generalized communicating
221
P system
171, 187
accepting
193
asynchronuous
202
catalytic
188, 203
purely catalytic
188
sequential
202
symport/antiport
191, 194
with active membranes
192
with bi-stable catalysts
202
with label selection
196
with membrane dissolution
200
with promoters and inhibitors
201, 203
with tables
197
with target agreement
190, 196
with target selection
190, 196
with toxic objects
206
parallel rewriting
174
Pascal’s triangle
286
phyllosilicates
353
phylogenetic tree reconstruction
384, 409
point
152
prime generator
25-state real-time
345
eight-state real-time
348
Korec’s
344
program size
117
proof assistant
96
pseudo-random number generators
81, 86
PSQL algorithm
88
QED manifesto
97
quadratic iterator
81, 89
quantiﬁer
264
existential
264
inﬁnity
267
mod-counter
267
universal
264
reachability
269
recursively enumerable
266
degree
270
register machine
186
regular spectrum
265
reversible
computing
127
logic element
127
logic element with memory
128
sequential machine
128
Turing machine
128, 132
rotary element
128
sequence generation
343
sequential machine
128
shift operator
259
silicon
354
singleton
12, 47
small universal Turing machine
117
sorting on the ring
288
spectrum
265
sphere packing problem
78, 93, 97
splicing
P system
251
system
237
SRT algorithm
86
still life
374
structure
260
augmented structure
262
automatic structure
261
ﬁnite structure
261

418
Subject Index
ﬁrst-order structure
260
one-way inﬁnite structure
261
two-way inﬁnite structure
261
tag system
117
Tag systems
94
trafﬁc rule
284
true sequence alignment
393
Turing Machine
computational path
145
Computational Step
145
multi-tape
142
Transition Function
142
Turing machine
80, 96, 132, 139, 270
computational result
162
computational step
141, 143–145
ideal
154
non-deterministic
144
non-deterministic degree
145
physical
154
self-verifying
272
single-tape
141
transition function
141, 144
two-dimensional right-linear grammar
173
undecidability
117, 260, 269
undecidable
270
universal Turing machine
117
unpredictability
78, 93
unreliability
95
von Neumann neighbourhood
307
wave-like pattern
365, 366
Wolfram classiﬁcation
260
ﬁrst class
270
word
261
Zeilberger-Gosper algorithm
88

