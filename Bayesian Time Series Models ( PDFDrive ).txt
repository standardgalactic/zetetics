
BAYESIAN TIME SERIES MODELS
‘What’s going to happen next?’ Time series data hold the answers, and Bayesian methods
represent the cutting edge in learning what they have to say. This ambitious book is the
ﬁrst uniﬁed treatment of the emerging knowledge-base in Bayesian time series techniques.
Exploiting the unifying framework of probabilistic graphical models, the book covers
approximation schemes, both Monte Carlo and deterministic, and introduces switching,
multi-object, nonparametric and agent-based models in a variety of application environ-
ments. It demonstrates that the basic framework supports the rapid creation of models
tailored to speciﬁc applications and gives insight into the computational complexity of
their implementation.
The authors span traditional disciplines such as statistics and engineering and the more
recently established areas of machine learning and pattern recognition. Readers with a basic
understanding of applied probability, but no experience with time series analysis, are guided
from fundamental concepts to the state of the art in research and practice.


BAYESIAN TIME SERIES MODELS
Edited by
David Barber
University College London
A. Taylan Cemgil
Bo˘gazi¸ci University, Bebek, Istanbul
Silvia Chiappa
University of Cambridge

cambridge university press
Cambridge, New York, Melbourne, Madrid, Cape Town,
Singapore, S˜ao Paulo, Delhi, Tokyo, Mexico City
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
Information on this title: www.cambridge.org/9780521196765
c⃝Cambridge University Press 2011
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2011
Printed in the United Kingdom at the University Press, Cambridge
A catalogue record for this publication is available from the British Library
Library of Congress Cataloguing in Publication data
Bayesian time series models / edited by David Barber, A. Taylan Cemgil, Silvia Chiappa.
p.
cm.
ISBN 978-0-521-19676-5 (hardback)
1. Time-series analysis.
2. Bayesian statistical decision theory.
I. Barber, David, 1968 Nov. 9–
II. Cemgil, Ali Taylan.
III. Chiappa, Silvia.
IV. Title.
QA280.B39
2011
519.5′5–dc22
2011008051
ISBN 978-0-521-19676-5 Hardback
Cambridge University Press has no responsibility for the persistence or
accuracy of URLs for external or third-party internet websites referred to
in this publication, and does not guarantee that any content on such
websites is, or will remain, accurate or appropriate.

Contents
List of contributors
page x
Preface
xi
1
Inference and estimation in probabilistic time series models. David
Barber, A. Taylan Cemgil and Silvia Chiappa.
1
1.1
Time series
1
1.2
Markov models
3
1.3
Latent Markov models
6
1.4
Inference in latent Markov models
10
1.5
Deterministic approximate inference
15
1.6
Monte Carlo inference
21
1.7
Discussion and summary
29
I
Monte Carlo
2
Adaptive Markov chain Monte Carlo: theory and methods. Yves
Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret.
32
2.1
Introduction
32
2.2
Adaptive MCMC algorithms
34
2.3
Convergence of the marginal distribution
43
2.4
Strong law of large numbers
44
2.5
Convergence of the equi-energy sampler
46
2.6
Conclusion
47
2.A
Appendix: Proof of Section 2.5
47
3
Auxiliary particle ﬁltering: recent developments. Nick Whiteley and
Adam M. Johansen.
52
3.1
Background
52
3.2
Interpretation and implementation
56
3.3
Applications and extensions
63
3.4
Further stratifying the APF
69
3.5
Conclusions
79
4
Monte Carlo probabilistic inference for diﬀusion processes: a method-
ological framework. Omiros Papaspiliopoulos.
82
4.1
Introduction
82
4.2
Random weight continuous–discrete particle ﬁltering
84
4.3
Transition density representation for a class of diﬀusions
87

vi
Contents
4.4
Exact simulation of diﬀusions
88
4.5
Exact simulation of killed Brownian motion
90
4.6
Unbiased estimation of the transition density using series expansions
92
4.7
Discussion and directions
99
II
Deterministic approximations
5
Two problems with variational expectation maximisation for time series
models. Richard Eric Turner and Maneesh Sahani.
104
5.1
Introduction
104
5.2
The variational approach
104
5.3
Compactness of variational approximations
109
5.4
Variational approximations are biased
112
5.5
Conclusion
123
6
Approximate inference for continuous-time Markov processes. C´edric
Archambeau and Manfred Opper.
125
6.1
Introduction
125
6.2
Partly observed diﬀusion processes
126
6.3
Hidden Markov characterisation
127
6.4
The variational approximation
129
6.5
The Gaussian variational approximation
133
6.6
Diﬀusions with multiplicative noise
137
6.7
Parameter inference
137
6.8
Discussion and outlook
138
7
Expectation propagation and generalised EP methods for inference in
switching linear dynamical systems. Onno Zoeter and Tom Heskes.
141
7.1
Introduction
141
7.2
Notation and problem description
142
7.3
Assumed density ﬁltering
143
7.4
Expectation propagation
145
7.5
Free-energy minimisation
148
7.6
Generalised expectation propagation
150
7.7
Alternative backward passes
153
7.8
Experiments
155
7.9
Discussion
159
7.A
Appendix: Operations on conditional Gaussian potentials
160
7.B
Appendix: Proof of Theorem 7.1
163
8
Approximate inference in switching linear dynamical systems using
Gaussian mixtures. David Barber.
166
8.1
Introduction
166
8.2
The switching linear dynamical system
166
8.3
Gaussian sum ﬁltering
168
8.4
Expectation correction
172
8.5
Demonstration: traﬃc ﬂow
176
8.6
Comparison of smoothing techniques
177
8.7
Summary
180

Contents
vii
III
Switching models
9
Physiological monitoring with factorial switching linear dynamical
systems. John A. Quinn and Christopher K.I. Williams.
182
9.1
Introduction
182
9.2
Model
184
9.3
Novel conditions
186
9.4
Parameter estimation
188
9.5
Inference
194
9.6
Experiments
197
9.7
Summary
203
10 Analysis of changepoint models. Idris A. Eckley, Paul Fearnhead and
Rebecca Killick.
205
10.1 Introduction
205
10.2 Single changepoint models
207
10.3 Multiple changepoint models
210
10.4 Comparison of methods
215
10.5 Conclusion
221
10.A Appendix: segment parameter estimation
222
IV
Multi-object models
11 Approximate likelihood estimation of static parameters in multi-target
models. Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill.
225
11.1 Introduction
225
11.2 The multi-target model
228
11.3 A review of the PHD ﬁlter
228
11.4 Approximating the marginal likelihood
232
11.5 SMC approximation of the PHD ﬁlter and its gradient
233
11.6 Parameter estimation
238
11.7 Simulation study
239
11.8 Conclusion
243
12 Sequential inference for dynamically evolving groups of objects. Sze
Kim Pang, Simon J. Godsill, Jack Li, Franc¸ois Septier and Simon Hill.
245
12.1 Introduction
245
12.2 MCMC-particles algorithm
246
12.3 Group tracking
258
12.4 Ground target tracking
259
12.5 Group stock selection
268
12.6 Conclusions
273
12.A Appendix: Base group representation
275
13 Non-commutative harmonic analysis in multi-object tracking. Risi Kon-
dor.
277
13.1 Introduction
277
13.2 Harmonic analysis on ﬁnite groups
278
13.3 Band-limited approximations
282

viii
Contents
13.4 A hidden Markov model in Fourier space
282
13.5 Approximations in terms of marginals
288
13.6 Eﬃcient computation
290
13.7 Conclusions
293
V
Nonparametric models
14 Markov chain Monte Carlo algorithms for Gaussian processes.
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence.
295
14.1 Introduction
295
14.2 Gaussian process models
297
14.3 Non-Gaussian likelihoods and deterministic methods
299
14.4 Sampling algorithms for Gaussian process models
300
14.5 Related work and other sampling schemes
307
14.6 Demonstration on regression and classiﬁcation
308
14.7 Transcriptional regulation
309
14.8 Dealing with large datasets
312
14.9 Discussion
314
15 Nonparametric hidden Markov models. Jurgen Van Gael and Zoubin
Ghahramani.
317
15.1 Introduction
317
15.2 From HMMs to Bayesian HMMs
319
15.3 The inﬁnite hidden Markov model
321
15.4 Inference
327
15.5 Example: unsupervised part-of-speech tagging
332
15.6 Beyond the iHMM
334
15.7 Conclusions
337
15.A Appendix: Equivalence of the hierarchical Polya urn and hierarchical
Dirichlet process
338
16 Bayesian Gaussian process models for multi-sensor time series predic-
tion. Michael A. Osborne, Alex Rogers, Stephen J. Roberts, Sarvapali D.
Ramchurn and Nick R. Jennings.
341
16.1 Introduction
341
16.2 The information processing problem
342
16.3 Gaussian processes
343
16.4 Trial implementation
350
16.5 Empirical evaluation
351
16.6 Computation time
357
16.7 Related work
358
16.8 Conclusions
359
16.A Appendix
360
VI
Agent-based models
17 Optimal control theory and the linear Bellman equation. Hilbert J.
Kappen.
363
17.1 Introduction
363

Contents
ix
17.2 Discrete time control
365
17.3 Continuous time control
366
17.4 Stochastic optimal control
370
17.5 Learning
373
17.6 Path integral control
377
17.7 Approximate inference methods for control
382
17.8 Discussion
385
18 Expectation maximisation methods for solving (PO)MDPs and optimal
control problems. Marc Toussaint, Amos Storkey and Stefan Harmeling.
388
18.1 Introduction
388
18.2 Markov decision processes and likelihood maximisation
389
18.3 Expectation maximisation in mixtures of variable length dynamic
Bayesian networks
393
18.4 Application to MDPs
398
18.5 Application to POMDPs
404
18.6 Conclusion
409
18.A Appendix: Remarks
410
18.B Appendix: Pruning computations
411
Index
414

Contributors
C´edric Archambeau, Department of Computer
Science, University College London
Yves Atchad´e Department of Statistics, University of
Michigan
David Barber Department of Computer Science,
University College London
A. Taylan Cemgil Department of Computer
Engineering Bo˘gazi¸ci University
Silvia Chiappa Statistical Laboratory, University of
Cambridge
Idris A. Eckley Department of Mathematics and
Statistics, Lancaster University
Paul Fearnhead Department of Mathematics and
Statistics, Lancaster University
Gersende Fort LTCI, CNRS – Telecom ParisTech
Jurgen Van Gael Department of Engineering,
University of Cambridge
Zoubin Ghahramani Department of Engineering,
University of Cambridge
Simon J. Godsill Signal Processing Laboratory,
Department of Engineering, University of Cambridge
Stefan Harmeling MPI for Biological Cybernetics,
Department Sch¨olkopf, T¨ubingen
Tom Heskes Institute for Computing and Information
Sciences, Radboud University Nijmegen
Simon Hill Signal Processing Laboratory,
Department of Engineering, University of Cambridge
Nick R. Jennings School of Electronics and Computer
Science, University of Southampton
Adam M. Johansen Department of Statistics,
University of Warwick
Hilbert J. Kappen Donders’ Institute for
Neuroscience, Radboud University, Nijmegen
Rebecca Killick Department of Mathematics and
Statistics, Lancaster University
Risi Kondor Center for the Mathematics of
Information, California Institute of Technology
Neil D. Lawrence School of Computer Science,
University of Manchester
Jack Li Signal Processing Laboratory, Department of
Engineering, University of Cambridge
Eric Moulines LTCI, CNRS – Telecom ParisTech
Manfred Opper Technische Universit¨at Berlin,
Fakult¨at IV – Elektrotechnik und Informatik
Michael A. Osborne Department of Engineering
Science, University of Oxford
Sze Kim Pang Signal Processing Laboratory,
Department of Engineering, University of Cambridge
Omiros Papaspiliopoulos Department of Economics,
Universitat Pompeu Fabra, Barcelona
Pierre Priouret Laboratoire de Probabilit´es et
Mod`eles Al´eatoires, Universit´e P. & M. Curie, Paris
John A. Quinn Department of Computer Science,
Makerere University
Sarvapali D. Ramchurn School of Electronics and
Computer Science, University of Southampton
Magnus Rattray School of Computer Science,
University of Manchester
Stephen J. Roberts Department of Engineering
Science, University of Oxford
Alex Rogers School of Electronics and Computer
Science, University of Southampton
Maneesh Sahani Gatsby Computational
Neuroscience Unit, London
Franc¸ois Septier Institut TELECOM/ TELECOM Lille
1, France
Sumeetpal S. Singh Signal Processing Laboratory,
Department of Engineering, University of Cambridge
Amos Storkey Institute for Adaptive and Neural
Computation, University of Edinburgh
Michalis K. Titsias School of Computer Science,
University of Manchester
Marc Toussaint Technische Universit¨at Berlin
Richard Eric Turner Gatsby Computational
Neuroscience Unit, London
Nick Whiteley Statistics Group, Department of
Mathematics, University of Bristol
Christopher K. I. Williams Institute for Adaptive and
Neural Computation, University of Edinburgh
Onno Zoeter Xerox Research Centre Europe, Meylan

Preface
Probabilistic time series modelling
Time series are studied in a variety of disciplines and appear in many modern applications
such as ﬁnancial time series prediction, video-tracking, music analysis, control and genetic
sequence analysis. This widespread interest at times obscures the commonalities in the
developed models and techniques. A central aim of this book is to attempt to make modern
time series techniques, speciﬁcally those based on probabilistic modelling, accessible to a
broad range of researchers.
In order to achieve this goal, leading researchers that span the more traditional disci-
plines of statistics, control theory, engineering and signal processing, and the more recent
areas of machine learning and pattern recognition, have been brought together to discuss
advancements and developments in their respective ﬁelds. In addition, the book makes
extensive use of the graphical models framework. This framework facilitates the represen-
tation of many classical models and provides insight into the computational complexity of
their implementation. Furthermore, it enables to easily envisage new models tailored for a
particular environment. For example, the book discusses novel state space models and their
application in signal processing including condition monitoring and tracking. The book
also describes modern developments in the machine learning community applied to more
traditional areas of control theory.
The eﬀective application of probabilistic models in the real world is gaining pace,
largely through increased computational power which brings more general models into con-
sideration through carefully developed implementations. As such, developing new models
and associated approximate inference schemes is likely to remain an active area of research,
with graphical models playing an important role in facilitating communication and guiding
intuition. The book extensively discusses novel developments in approximate inference,
including both deterministic and stochastic approximations.
The structure of the book
Chapter 1 gives a general introduction to probabilistic time series and explains how
graphical models can be used to compactly represent classical models, such as the linear
dynamical system and hidden Markov model. The chapter also discusses stochastic approx-
imation schemes such as Markov chains and sequential Monte Carlo (particle ﬁltering), and
less well known deterministic approximation schemes such as variational methods.
The subsequent chapters are organised into six thematic parts: the ﬁrst two deal with
more theoretical issues related to approximate inference, while the remaining four deal with

xii
Preface
the development and application of novel models. More speciﬁcally, the parts are organised
as follows.
Monte Carlo Monte Carlo methods are important and widespread techniques for approxi-
mate inference in probabilistic models. Chapter 2 gives a comprehensive introduction
to adaptive Markov chain Monte Carlo methods. In time series models, a particularly
relevant issue is that data often arrives sequentially, for which sequential Monte Carlo
methods are appropriate. Chapter 3 gives a survey of recent developments in parti-
cle ﬁltering. Chapter 4 presents the application of Monte Carlo methods to diﬀusion
processes.
Deterministic approximations Chapter 5 discusses some characteristics of variational
approximations, highlighting important aspects and diﬃculties idiosyncratic to time
series models. Chapter 6 presents a novel deterministic approximate inference
method for continuous-time Markov processes. Chapters 7 and 8 deal with inference
in the important but computationally diﬃcult switching linear dynamical system,
and introduce speciﬁc deterministic inference schemes as improvements on classical
methods.
Switching models Switching models assume that an underlying process may change from
one parameter regime to another over time and may be used to model changes in the
environment. In Chapter 9 switching models are applied to condition monitoring, in
particular physiological monitoring. Chapter 10 reviews changepoint models, which
are restricted switching models used to detect abrupt changes in time series, and
gives insights into new applications.
Multi-object models A particularly active research area is the tracking of moving objects,
such as crowds. In Chapter 11, a detailed discussion of a tracking model and asso-
ciated stochastic inference method is given. In a similar vein, in Chapter 12 a
framework for sequentially inferring how groups dynamically evolve is presented
and applied to tracking the group behaviour of ﬁnancial stocks. In Chapter 13 a
diﬀerent theoretical approach to multi-object tracking based on non-commutative
harmonic analysis is discussed.
Nonparametric models In recent years ﬂexible nonparametric models have been a par-
ticular focus of machine learning research. In this part, their extension to time series
analysis is presented. Chapter 14 discusses sampling algorithms for Gaussian pro-
cesses. Chapter 15 presents recent developments in nonparametric hidden Markov
models, along with associated inference algorithms and applications. Chapter 16
introduces an application of a nonparametric time series model to multi-sensor time
series prediction.
Agent-based models A recent viewpoint is to treat a control problem as an inference
problem in an associated probabilistic model. This viewpoint is complementary to
classical control theory and reinforcement learning and makes use of concepts famil-
iar to probabilistic modellers, facilitating an entrance into this ﬁeld. In Chapter 17
optimal control theory and the linear Bellman equation are discussed in relation to
inference in a probabilistic model. In Chapter 18 the standard methods of learning
in probabilistic models are applied to learning control policies in Markov decision
problems.

Preface
xiii
Whom this book is for
This book will appeal to statisticians interested in modern aspects of time series analysis
in areas bordering with engineering, signal processing and machine learning and how time
series analysis is approached in those communities. For engineers and machine learners, the
book has a wealth of insights into statistical approaches, particularly dealing with sampling
techniques applied to diﬃcult time series models. To follow the book, no speciﬁc knowl-
edge of time series is required. However, readers are assumed to have a basic understanding
of applied probability.
Acknowledgements
We are particularly grateful to the following people for their advice and comments on the
book: Julien Cornebise, Mark Girolami, Andrew Golightly, Jim Griﬃn, Matt Hoﬀmanm,
Antti Honkela, Jonathan Huang, Ajay Jasra, Jens Kober, Jan Peters, Sonia Petrone, Alan
Qi, George Sermaidis, Olivier Stegle, Matt Taddy, Evangelos Theodorou, Yener Ulker.


1
Inference and estimation in probabilistic time series
models
David Barber, A. Taylan Cemgil and Silvia Chiappa
1.1
Time series
The term ‘time series’ refers to data that can be represented as a sequence. This includes
for example ﬁnancial data in which the sequence index indicates time, and genetic data
(e.g. ACATGC . . .) in which the sequence index has no temporal meaning. In this tutorial
we give an overview of discrete-time probabilistic models, which are the subject of most
chapters in this book, with continuous-time models being discussed separately in Chapters
4, 6, 11 and 17. Throughout our focus is on the basic algorithmic issues underlying time
series, rather than on surveying the wide ﬁeld of applications.
Deﬁning a probabilistic model of a time series y1:T ≡y1, . . . , yT requires the speciﬁca-
tion of a joint distribution p(y1:T).1 In general, specifying all independent entries of p(y1:T)
is infeasible without making some statistical independence assumptions. For example, in
the case of binary data, yt ∈{0, 1}, the joint distribution contains maximally 2T −1 indepen-
dent entries. Therefore, for time series of more than a few time steps, we need to introduce
simpliﬁcations in order to ensure tractability. One way to introduce statistical independence
is to use the probability of a conditioned on observed b
p(a|b) = p(a, b)
p(b) .
Replacing a with yT
and b with y1:T−1 and rearranging we obtain p(y1:T)
=
p(yT|y1:T−1)p(y1:T−1). Similarly, we can decompose p(y1:T−1) = p(yT−1|y1:T−2)p(y1:T−2). By
repeated application, we can then express the joint distribution as2
p(y1:T) =
T
Y
t=1
p(yt|y1:t−1).
This factorisation is consistent with the causal nature of time, since each factor represents
a generative model of a variable conditioned on its past. To make the speciﬁcation simpler,
we can impose conditional independence by dropping variables in each factor conditioning
set. For example, by imposing p(yt|y1:t−1) = p(yt|yt−m:t−1) we obtain the mth-order Markov
model discussed in Section 1.2.
1To simplify the notation, throughout the tutorial we use lowercase to indicate both a random variable and its
realisation.
2We use the convention that y1:t−1 = ∅if t < 2. More generally, one may write pt(yt|y1:t−1), as we generally
have a diﬀerent distribution at each time step. However, for notational simplicity we generally omit the time index.

2
David Barber, A. Taylan Cemgil and Silvia Chiappa
y1
y2
y3
y4
(a)
y1
y2
y3
y4
(b)
Figure 1.1 Belief network representations of two time series models. (a) First-order Markov model p(y1:4) =
p(y4|y3)p(y3|y2)p(y2|y1)p(y1). (b) Second-order Markov model p(y1:4) = p(y4|y3, y2)p(y3|y2, y1)p(y2|y1)p(y1).
A useful way to express statistical independence assumptions is to use a belief network
graphical model which is a directed acyclic graph3 representing the joint distribution
p(y1:N) =
N
Y
i=1
p(yi|pa (yi)) ,
where pa (yi) denotes the parents of yi, that is the variables with a directed link to yi. By
limiting the parental set of each variable we can reduce the burden of speciﬁcation. In
Fig. 1.1 we give two examples of belief networks corresponding to a ﬁrst- and second-
order Markov model respectively, see Section 1.2. For the model p(y1:4) in Fig. 1.1(a) and
binary variables yt ∈{0, 1} we need to specify only 1 + 2 + 2 + 2 = 7 entries,4 compared to
24 −1 = 15 entries in the case that no independence assumptions are made.
Inference
Inference is the task of using a distribution to answer questions of interest. For example,
given a set of observations y1:T, a common inference problem in time series analysis is the
use of the posterior distribution p(yT+1|y1:T) for the prediction of an unseen future variable
yT+1. One of the challenges in time series modelling is to develop computationally eﬃ-
cient algorithms for computing such posterior distributions by exploiting the independence
assumptions of the model.
Estimation
Estimation is the task of determining a parameter θ of a model based on observations y1:T.
This can be considered as a form of inference in which we wish to compute p(θ|y1:T).
Speciﬁcally, if p(θ) is a distribution quantifying our beliefs in the parameter values before
having seen the data, we can use Bayes’ rule to combine this prior with the observations to
form a posterior distribution
p(θ|y1:T)
|   {z   }
posterior
=
p(y1:T|θ)
|   {z   }
likelihood
p(θ)
|{z}
prior
p(y1:T)
| {z }
marginal likelihood
.
The posterior distribution is often summarised by the maximum a posteriori (MAP) point
estimate, given by the mode
θMAP = argmax
θ
p(y1:T|θ)p(θ).
3A directed graph is acyclic if, by following the direction of the arrows, a node will never be visited more than
once.
4For example, we need one speciﬁcation for p(y1 = 0), with p(y1 = 1) = 1 −p(y1 = 0) determined by
normalisation. Similarly, we need to specify two entries for p(y2|y1).

Probabilistic time series models
3
It can be computationally more convenient to use the log posterior,
θMAP = argmax
θ
log (p(y1:T|θ)p(θ)) ,
where the equivalence follows from the monotonicity of the log function. When using a
‘ﬂat prior’ p(θ) = const., the MAP solution coincides with the maximum likelihood (ML)
solution
θML = argmax
θ
p(y1:T|θ) = argmax
θ
log p(y1:T|θ).
In the following sections we introduce some popular time series models and describe
associated inference and parameter estimation routines.
1.2
Markov models
Markov models (or Markov chains) are of fundamental importance and underpin many
time series models [21]. In an mth order Markov model the joint distribution factorises as
p(y1:T) =
T
Y
t=1
p(yt|yt−m:t−1),
expressing the fact that only the previous m observations yt−m:t−1 directly inﬂuence yt. In a
time-homogeneous model, the transition probabilities p(yt|yt−m:t−1) are time-independent.
1.2.1
Estimation in discrete Markov models
In a time-homogeneous ﬁrst-order Markov model with discrete scalar observations yt ∈
{1, . . . , S }, the transition from yt−1 to yt can be parameterised using a matrix θ, that is
θ ji ≡p(yt = j|yt−1 = i, θ),
i, j ∈{1, . . . , S } .
Given observations y1:T, maximum likelihood sets this matrix according to
θML = argmax
θ
log p(y1:T|θ) = argmax
θ
X
t
log p(yt|yt−1, θ).
Under the probability constraints 0 ≤θ ji ≤1 and P
j θ ji = 1, the optimal solution is given
by the intuitive setting
θML
ji
=
nji
T −1,
where nji is the number of transitions from i to j observed in y1:T.
Alternatively, a Bayesian treatment would compute the parameter posterior distribution
p(θ|y1:T) ∝p(θ)p(y1:T|θ) = p(θ)
Y
i, j
θ
nji
ji .
In this case a convenient prior for θ is a Dirichlet distribution on each column θ: i with
hyperparameter vector α: i
p(θ) =
Y
i
DI(θ: i|α: i) =
Y
i
1
Z(α: i)
Y
j
θ
α ji−1
ji
,

4
David Barber, A. Taylan Cemgil and Silvia Chiappa
0
20
40
60
80
100
120
140
−50
0
50
100
150
200
Figure 1.2 Maximum likelihood ﬁt of a third-order AR model.
The horizontal axis represents time, whilst the vertical axis the
value of the time series. The dots represent the 100 observations
y1:100. The solid line indicates the mean predictions ⟨y⟩t , t >
100, and the dashed lines ⟨y⟩t ± √r.
where Z(α: i) =
R 1
0
Q
j θ
α ji−1
ij
dθ. The convenience of this ‘conjugate’ prior is that it gives a
parameter posterior that is also a Dirichlet distribution [15]
p(θ|y1:T) =
Y
i
DI(θ: i|α: i + n: i).
This Bayesian approach diﬀers from maximum likelihood in that it treats the parameters as
random variables and yields distributional information. This is motivated from the under-
standing that for a ﬁnite number of observations there is not necessarily a ‘single best’
parameter estimate, but rather a distribution of parameters weighted both by how well they
ﬁt the data and how well they match our prior assumptions.
1.2.2
Autoregressive models
A widely used Markov model of continuous scalar observations is the autoregressive (AR)
model [2, 4]. An mth-order AR model assumes that yt is a noisy linear combination of the
previous m observations, that is
yt = a1yt−1 + a2yt−2 + · · · + amyt−m + ϵt,
where a1:m are called the AR coeﬃcients, and ϵt is an independent noise term commonly
assumed to be zero-mean Gaussian with variance r (indicated with N(ϵt|0, r)). A so-called
generative form for the AR model with Gaussian noise is given by5
p(y1:T|y1:m) =
T
Y
t=m+1
p(yt|yt−m:t−1),
p(yt|yt−m:t−1) = N

yt

m
X
i=1
aiyt−i, r

.
Given observations y1:T, the maximum likelihood estimate for the parameters a1:m and r is
obtained by maximising with respect to a and r the log likelihood
log p(y1:T|y1:m) = −1
2r
T
X
t=m+1

yt −
m
X
i=1
aiyt−i
2 −T −m
2
log(2πr).
The optimal a1:m are given by solving the linear system
X
i
ai
T
X
t=m+1
yt−iyt−j =
T
X
t=m+1
ytyt−j
∀j,
5Note that the ﬁrst m variables are not modelled.

Probabilistic time series models
5
y1
y2
y3
y4
a1
r
(a)
a
r
−8
−6
−4
−2
0
2
4
6
10
10
10
10
10
(b)
Figure 1.3 (a) Belief network representation of a ﬁrst-order AR model with parameters a1, r (ﬁrst four time
steps). (b) Parameter prior p(a1, r) (light grey, dotted) and posterior p(a1, r|y1 = 1, y2 = −6) (black). The posterior
describes two plausible explanations of the data: (i) the noise r is low and a1 ≈−6, (ii) the noise r is high with a
set of possible values for a1 centred around zero.
which is readily solved using Gaussian elimination. The linear system has a Toeplitz form
that can be more eﬃciently solved, if required, using the Levinson-Durbin method [9]. The
optimal variance is then given by
r =
1
T −m
T
X
t=m+1

yt −
m
X
i=1
aiyt−i
2.
The case in which yt is multivariate can be handled by assuming that ai is a matrix and ϵt is
a vector. This generalisation is known as vector autoregression.
Example 1
We illustrate with a simple example how AR models can be used to estimate
trends underlying time series data. A third-order AR model was ﬁt to the set of 100 obser-
vations shown in Fig. 1.2 using maximum likelihood. A prediction for the mean ⟨y⟩t was
then recursively generated as
⟨y⟩t =
( P3
i=1 ai ⟨y⟩t−i
for
t > 100,
yt
for
t ≤100 .
As we can see (solid line in Fig. 1.2), the predicted means for time t > 100 capture an
underlying trend in the time series.
Example 2
In a MAP and Bayesian approach, a prior on the AR coeﬃcients can be used
to deﬁne physical constraints (if any) or to regularise the system. Similarly, a prior on the
variance r can be used to specify any knowledge about or constraint on the noise. As an
example, consider a Bayesian approach to a ﬁrst-order AR model in which the following
Gaussian prior for a1 and inverse Gamma prior for r are deﬁned:
p(a1) = N (a1 0, q) ,
p(r) = IG(r|ν, ν/β) = exp
"
−(ν + 1) log r −ν
βr −log Γ(ν) + ν log ν
β
#
.

6
David Barber, A. Taylan Cemgil and Silvia Chiappa
yt−1
yt
yt+1
· · ·
xt−1
xt
xt+1
· · ·
Figure 1.4 A ﬁrst-order latent Markov model. In a hidden
Markov model the latent variables x1:T are discrete and the
observed variables y1:T can be either discrete or continuous.
Assuming that a1 and r are a priori independent, the parameter posterior is given by
p(a1, r|y1:T) ∝p(a1)p(r)
T
Y
t=2
p(yt|yt−1, a1, r).
The belief network representation of this model is given in Fig. 1.3(a). For a numerical
example, consider T = 2 and observations and hyperparameters given by
y1 = 1, y2 = −6, q = 1.2, ν = 0.4, β = 100.
The parameter posterior, Fig. 1.3(b), takes the form
p(a1, r|y1:2) ∝exp
−

ν
β + y2
2
2

1
r + y1y2
a1
r −1
2

y2
1
r + 1
q
a2
1 −(ν + 3/2) log r
.
As we can see, Fig. 1.3(b), the posterior is multimodal, with each mode corresponding to a
diﬀerent interpretation: (i) The regression coeﬃcient a1 is approximately −6 and the noise
is low. This solution gives a small prediction error. (ii) Since the prior for a1 has zero mean,
an alternative interpretation is that a1 is centred around zero and the noise is high.
From this example we can make the following observations:
• Point estimates such as ML or MAP are not always representative of the solution.
• Even very simple models can lead to complicated posterior distributions.
• Variables that are independent a priori may become dependent a posteriori.
• Ambiguous data usually leads to a multimodal parameter posterior, with each mode
corresponding to one plausible explanation.
1.3
Latent Markov models
In a latent Markov model, the observations y1:T are generated by a set of unobserved or
‘latent’ variables x1:T. Typically, the latent variables are ﬁrst-order Markovian and each
observed variable yt is independent from all other variables given xt. The joint distribution
thus factorises as6
p(y1:T, x1:T) = p(x1)
T
Y
t=2
p(yt|xt)p(xt|xt−1),
where p(xt|xt−1) is called the ‘transition’ model and p(yt|xt) the ‘emission’ model. The
belief network representation of this latent Markov model is given in Fig. 1.4.
6This general form is also known as a state space model.

Probabilistic time series models
7
(a)
1
3
2
ϵ
ϵ
ϵ
1 −ϵ
1 −ϵ
1 −ϵ
(b)
Figure 1.5 (a) Robot (square) moving sporadically with probabil-
ity 1 −ϵ counter-clockwise in a circular corridor one location at a
time. Small circles denote the S possible locations. (b) The state
transition diagram for a corridor with S = 3 possible locations.
1.3.1
Discrete state latent Markov models
A well-known latent Markov model is the hidden Markov model7 (HMM) [23] in which xt
is a scalar discrete variable (xt ∈{1, . . . , S }).
Example
Consider the following toy tracking problem. A robot is moving around a cir-
cular corridor and at any time occupies one of S possible locations. At each time step t, the
robot stays where it is with probability ϵ, or moves to the next point in a counter-clockwise
direction with probability 1 −ϵ. This scenario, illustrated in Fig. 1.5, can be conveniently
represented by an S × S matrix A with elements Aji = p(xt = j|xt−1 = i). For example, for
S = 3, we have
A = ϵ

1
0
0
0
1
0
0
0
1
+ (1 −ϵ)

0
0
1
1
0
0
0
1
0
.
(1.1)
At each time step t, the robot sensors measure its position, obtaining either the correct
location with probability w or a uniformly random location with probability 1−w. This can
be expressed formally as
yt|xt ∼wδ(yt −xt) + (1 −w)U(yt|1, . . . , S ),
where δ is the Kronecker delta function and U(y|1, . . . , S ) denotes the uniform distribution
over the set of possible locations. We may parameterise p(yt|xt) using an S × S matrix C
with elements Cui = p(yt = u|xt = i). For S = 3, we have
C = w

1
0
0
0
1
0
0
0
1
+ (1 −w)
3

1
1
1
1
1
1
1
1
1
.
A typical realisation y1:T from the process deﬁned by this HMM with S = 50, ϵ = 0.5,
T = 30 and w = 0.3 is depicted in Fig. 1.6(a). We are interested in inferring the true loca-
tions of the robot from the noisy measured locations y1:T. At each time t, the true location
can be inferred from the so-called ‘ﬁltered’ posterior p(xt|y1:t) (Fig. 1.6(b)), which uses
measurements up to t; or from the so-called ‘smoothed’ posterior p(xt|y1:T) (Fig. 1.6(c)),
which uses both past and future observations and is therefore generally more accurate.
These posterior marginals are obtained using the eﬃcient inference routines outlined in
Section 1.4.
7Some authors use the terms ‘hidden Markov model’ and ‘state space model’ as synonymous [4]. We use the
term HMM in a more restricted sense to refer to a latent Markov model where x1:T are discrete. The observations
y1:T can be discrete or continuous.

8
David Barber, A. Taylan Cemgil and Silvia Chiappa
(a)
(b)
(c)
Figure 1.6 Filtering and smoothing for robot tracking using a HMM with S = 50. (a) A realisation from the
HMM example described in the text. The dots indicate the true latent locations of the robot, whilst the open
circles indicate the noisy measured locations. (b) The squares indicate the ﬁltering distribution at each time step
t, p(xt|y1:t). This probability is proportional to the grey level with black corresponding to 1 and white to 0. Note
that the posterior for the ﬁrst time steps is multimodal, therefore the true position cannot be accurately estimated.
(c) The squares indicate the smoothing distribution at each time step t, p(xt|y1:T = y1:T ). Note that, for t < T,
we estimate the position retrospectively and the uncertainty is signiﬁcantly lower when compared to the ﬁltered
estimates.
1.3.2
Continuous state latent Markov models
In continuous state latent Markov models, xt is a multivariate continuous variable, xt ∈RH.
For high-dimensional continuous xt, the set of models for which operations such as ﬁltering
and smoothing are computationally tractable is severely limited. Within this tractable class,
the linear dynamical system plays a special role, and is essentially the continuous analogue
of the HMM.
Linear dynamical systems
A linear dynamical system (LDS) on variables x1:T, y1:T has the following form:
xt = Axt−1 + ¯xt + ϵx
t ,
ϵx
t ∼N  ϵx
t 0, Q ,
x1 ∼N (x1 µ, P) ,
yt = Cxt + ¯yt + ϵy
t ,
ϵy
t ∼N

ϵy
t 0, R

,
with transition matrix A and emission matrix C. The terms ¯xt, ¯yt are often deﬁned as ¯xt =
Bzt and ¯yt = Dzt, where zt is a known input that can be used to control the system. The
complete parameter set is therefore {A, B,C, D, Q, R, µ, P}. As a generative model, the LDS
is deﬁned as
p(xt|xt−1) = N (xt Axt−1 + ¯xt, Q) ,
p(yt|xt) = N (yt Cxt + ¯yt, R) .
Example
As an example scenario that can be modelled using an LDS, consider a moving
object with position, velocity and instantaneous acceleration at time t given respectively by
qt, vt and at. A discrete time description of the object dynamics is given by Newton’s laws
(see for example [11])

Probabilistic time series models
9
−20
0
20
40
60
80
100
120
−25
−20
−15
−10
−5
0
5
(a)
−20
0
20
40
60
80
100
120
−25
−20
−15
−10
−5
0
5
(b)
−20
0
20
40
60
80
100
120
−25
−20
−15
−10
−5
0
5
(c)
Figure 1.7 Tracking an object undergoing Newtonian dynamics in a two-dimensional space, Eq. (1.2). (a) The
dots indicate the true latent positions of the object at each time t, q1,t (horizontal axis) and q2,t (vertical axis) (the
time label is not shown). The crosses indicate the noisy observations of the latent positions. (b) The circles indicate
the mean of the ﬁltered latent positions
R
qt p(qt|y1:t)dqt. (c) The circles indicate the mean of the smoothed latent
positions
R
qt p(qt|y1:T )dqt.
 qt
vt
!
|{z}
=
 I
TsI
0
I
!
|    {z    }
 qt−1
vt−1
!
|{z}
+
 1
2T 2
s I
TsI
!
| {z }
at
xt
=
A
xt−1
+
B
at,
(1.2)
where I is the 3 × 3 identity matrix and Ts is the sampling period. In tracking applications,
we are interested in inferring the true position qt and velocity vt of the object from lim-
ited noisy information. For example, in the case that we observe only the noise-corrupted
positions, we may write
p(xt|xt−1) = N (xt Axt−1 + B¯a, Q) ,
p(yt|xt) = N (yt Cxt, R) ,
where ¯a is the acceleration mean, Q = BΣB⊤with Σ being the acceleration covariance,
C = (I 0), and R is the covariance of the position noise. We can then track the position and
velocity of the object using the ﬁltered density p(xt|y1:t). An example with two-dimensional
positions is shown in Fig. 1.7.
AR model as an LDS
Many popular time series models can be cast into a LDS form. For example, the AR model
of Section 1.2.2 can be formulated as

yt
yt−1
...
yt−m+1

|      {z      }
=

a1
a2
. . .
am
1
0
0
0
0
...
0
0
0
0
1
0

|                       {z                       }

yt−1
yt−2
...
yt−m

|    {z    }
+

ϵt
0
...
0

|{z}
xt
=
A
xt−1
+
ϵx
t ,
yt =

1
0
. . .
0

|                 {z                 }
C
xt + ϵy
t ,

10
David Barber, A. Taylan Cemgil and Silvia Chiappa
where ϵx
t ∼N  ϵx
t 0, diag(r, 0, . . . , 0), ϵy
t ∼N

ϵy
t 0, 0

, the initial mean µ is set to the ﬁrst
m observations, and P = 0. This shows how to transform an mth-order Markov model into a
constrained ﬁrst-order latent Markov model. Many other related AR models and extensions
can also be cast as a latent Markov model. This is therefore a very general class of models
for which inference is of particular interest.
1.4
Inference in latent Markov models
In this section we derive computationally eﬃcient methods for computing posterior dis-
tributions in latent Markov models. We assume throughout that xt is discrete, though
the recursions hold more generally on replacing summation with integration for those
components of xt that are continuous.
1.4.1
Filtering p(xt|y1:t)
In ﬁltering,8 the aim is to compute the distribution of the latent variable xt given all
observations up to time t. This can be expressed as
p(xt|y1:t) = p(xt, y1:t)/p(y1:t).
The normalising term p(y1:t) is the likelihood, see Section 1.4.2, and α(xt) ≡p(xt, y1:t) can
be computed by a ‘forward’ recursion
α(xt) = p(yt|xt,
y1:t−1)p(xt, y1:t−1)
= p(yt|xt)
X
xt−1
p(xt|xt−1,
y1:t−1)p(xt−1, y1:t−1)
= p(yt|xt)
X
xt−1
p(xt|xt−1)α(xt−1),
(1.3)
where the cancellations follow from the conditional independence assumptions of the
model. The recursion is initialised with α(x1) = p(y1|x1)p(x1). To avoid numerical over/un-
derﬂow problems, it is advisable to work with log α(xt). If only the conditional distribution
p(xt|y1:t) is required (not the joint p(xt, y1:t)) a numerical alternative to using the logarithm
is to form a recursion for p(xt|y1:t) directly by normalising α(xt) at each stage.
1.4.2
The likelihood
The likelihood can be computed as
p(y1:t) =
X
xt
α(xt),
and
p(y1:T) =
X
xT
α(xT).
Maximum likelihood parameter learning can be carried out by the expectation maximisa-
tion algorithm, known in the HMM context as the Baum-Welch algorithm [23], see also
Section 1.5.1.
8The term ‘ﬁltering’ is somewhat a misnomer since in signal processing this term is reserved for a convolution
operation. However, for linear systems, it turns out that state estimation is a linear function of past observations
and can indeed be computed by a convolution, partially justifying the use of the term.

Probabilistic time series models
11
1.4.3
Smoothing p(x1:T|y1:T)
The smoothing distribution is the joint distribution p(x1:T|y1:T). Typically we are interested
in marginals such as p(xt|y1:T), which gives an estimate of xt based on all observations.
There are two main approaches to computing p(xt|y1:T), namely the parallel and the
sequential methods described below.
Parallel smoothing p(xt|y1:T)
In parallel smoothing, the posterior γ(xt) ≡p(xt|y1:T) is separated into contributions from
the past and future
γ(xt) ∝p(xt, y1:t)
|    {z    }
past
p(yt+1:T|xt,
y1:t)
|            {z            }
future
= α(xt)β(xt).
(1.4)
The term α(xt) is obtained from the forward recursion (1.3). The terms β(xt) can be obtained
by the following ‘backward’ recursion
β(xt) =
X
xt+1
p(yt+1|
yt+2:T,
xt, xt+1)p(yt+2:T, xt+1|xt)
=
X
xt+1
p(yt+1|xt+1)p(yt+2:T|,
xt, xt+1)p(xt+1|xt)
=
X
xt+1
p(yt+1|xt+1)p(xt+1|xt)β(xt+1),
(1.5)
with β(xT) = 1. As for ﬁltering, working in log space for β is recommended to avoid
numerical diﬃculties.9 The α −β recursions are independent and may therefore be run in
parallel. These recursions also are called the forward-backward algorithm.
Sequential smoothing p(xt|y1:T)
In sequential smoothing, we form a direct recursion for the smoothed posterior as
γ(xt) =
X
xt+1
p(xt, xt+1|y1:T) =
X
xt+1
p(xt|xt+1, y1:t,
yt+1:T)γ(xt+1),
(1.6)
with γ(xT) ∝α(xT). The term p(xt|xt+1, y1:t) is computed from ﬁltering using
p(xt|xt+1, y1:t) ∝p(xt+1|xt)α(xt),
where the proportionality constant is found by normalisation. The procedure is sequential
since we need to complete the α recursions before starting the γ recursions. This technique
is also termed the Rauch–Tung–Striebel smoother10 and is a so-called correction smoother
since it ‘corrects’ the ﬁltered results. Interestingly, this correction process uses only ﬁltered
information. That is, once the ﬁltered results have been computed, the observations y1:T are
no longer needed. One can also view the γ recursion as a form of dynamics reversal, as if
we were reversing the direction of the hidden-to-hidden arrows in the model.
9If only posterior distributions are required, one can also perform local normalisation at each stage, since only
the relative magnitude of the components of β is of importance.
10It is most common to use this terminology for the continuous latent variable case.

12
David Barber, A. Taylan Cemgil and Silvia Chiappa
Computing the pairwise marginal p(xt, xt+1|y1:T)
To implement algorithms for parameter learning, we often require terms such as
p(xt, xt+1|y1:T), see Section 1.5.1. These can be obtained from the sequential approach using
p(xt, xt+1|y1:T) = p(xt|xt+1, y1:t)p(xt+1|y1:T),
or from the parallel approach using
p(xt, xt+1|y1:T) ∝β(xt+1)p(yt+1|xt+1)p(xt+1|, xt)α(xt).
(1.7)
1.4.4
Prediction p(yt+1|y1:t)
Prediction is the problem of computing the posterior density p(yτ|y1:t) for any τ > t. For
example, the distribution of the next observation may be found using
p(yt+1|y1:t) =
X
xt+1
p(yt+1|xt+1)p(xt+1|y1:t) =
X
xt+1
p(yt+1|xt+1)
X
xt
p(xt+1|xt)p(xt|y1:t).
1.4.5
Interpolation
Interpolation is the problem of estimating a set of missing observations given past and
future data. This can be achieved using
p(yτ|y1:τ−1, yτ+1:T) ∝
X
xτ
p(yτ|xτ)p(xτ|y1:τ−1)p(yτ+1:T|xτ).
1.4.6
Most likely latent trajectory
The most likely latent trajectory that explains the observations is given by
x∗
1:T = argmax
x1:T
p(x1:T|y1:T).
In the literature x∗
1:T is also called the Viterbi path. Since y1:T is known, x∗
1:T is equivalent
to argmax
x1:T
p(x1:T, y1:T). By deﬁning δ(xt) ≡maxx1:t−1 p(x1:t, y1:t), the most likely trajectory
can be obtained with the following algorithm:
δ(x1) = p(x1, y1),
δ(xt) = p(yt|xt) max
xt−1 p(xt|xt−1)δ(xt−1) for t = 2, . . . , T,
ψ(xt) = argmax
xt−1
p(xt|xt−1)δ(xt−1) for t = 2, . . . , T,
x∗
T = argmax
xT
δ(xT),
x∗
t = ψ(x∗
t+1) for t = T −1, . . . , 1,
where the recursion for δ(xt) is obtained analogously to the recursion (1.3) by replacing the
sum with the max operator.
1.4.7
Inference in the linear dynamical system
Inference in the LDS has a long history and widespread applications ranging from tracking
and control of ballistic projectiles to decoding brain signals.11 The ﬁltered and smoothed
11The LDS and associated ﬁltering algorithm was proposed by Kalman in the late 1950s [14] based on least
squares estimates. It is interesting to note that the method also appeared almost concurrently in the Russian
literature, in a form that is surprisingly similar to the modern approach in terms of Bayes recursions [25]. Even
earlier in the 1880s, Thiele deﬁned the LDS and associated ﬁltering and smoothing recursions [16].

Probabilistic time series models
13
posterior marginals, can be computed through conditioning and marginalisation of Gaus-
sian distributions. The key results required for algebraic manipulation of Gaussians are
stated below.
Gaussian conditioning, marginalisation and linear transformation
A multivariate Gaussian distribution is deﬁned in the so-called moment form by
p(x) = N (x µ, Σ) ≡
1
√
det 2πΣ
e−1
2 (x−µ)TΣ−1(x−µ),
where µ is the mean vector, and Σ is the covariance matrix.
Consider a vector z partitioned into two subvectors x and y,
z =
 x
y
!
,
and a Gaussian distribution N (z µ, Σ) with corresponding partitioned mean and covariance
µ =
 µx
µy
!
,
Σ =
 Σxx
Σxy
Σyx
Σyy
!
, Σyx ≡ΣT
xy.
The distribution of x conditioned on y is then given by
p(x|y) = N

x µx + ΣxyΣ−1
yy

y −µy

, Σxx −ΣxyΣ−1
yy Σyx

,
(1.8)
whilst the marginal distribution of x is given by p(x) = N (x µx, Σxx).
A linear transformation y = Ax of a Gaussian random variable x, with p(x) =
N (x µ, Σ), is Gaussian with p(y) = N

y Aµ, AΣAT
.
Filtering: predictor-corrector method
For continuous x, the analogue of recursion (1.3) is12
p(xt|y1:t) ∝p(xt, yt|y1:t−1) = p(yt|xt)
Z
xt−1
p(xt|xt−1)p(xt−1|y1:t−1).
(1.9)
Since Gaussians are closed under multiplication and integration, the ﬁltered distribution is
also a Gaussian. This means that we may represent p(xt|y1:t) = N (xt ft, Ft) and the ﬁltered
recursion translates into update formulae for the mean ft and covariance Ft. One can derive
these updates by carrying out the integration in Eq. (1.9). However, this is tedious and a
shortcut is to use the linear transformation and conditioning results above. Speciﬁcally, let
⟨x|y⟩denote expectation with respect to a distribution p(x|y), and let ∆x ≡x−⟨x⟩. By using
the transition and emission models
xt = Axt−1 + ¯xt + ϵx
t ,
yt = Cxt + ¯yt + ϵy
t ,
we obtain
⟨xt|y1:t−1⟩= Aft−1 + ¯xt,
⟨yt|y1:t−1⟩= C ⟨xt|y1:t−1⟩+ ¯yt,
12With
R
x we indicate the integral with respect to the variable x.

14
David Barber, A. Taylan Cemgil and Silvia Chiappa
Algorithm 1.1 LDS forward pass. Compute the ﬁltered posteriors p(xt|y1:t) ≡N( ft, Ft) for
a LDS with parameters θt = At,Ct, Qt, Rt, ¯xt, ¯yt. The log-likelihood L = log p(y1:T) is also
returned.
{f1, F1, p1} = LDSFORWARD(0, 0, y1; θ1)
L ←log p1
for t ←2, T do
{ft, Ft, pt} = LDSFORWARD( ft−1, Ft−1, yt; θt)
L ←L + log pt
end for
function LDSFORWARD ( f, F, y; θ)
µx ←A f + ¯x
µy ←Cµx + ¯y
Σxx ←AFAT + Q
Σyy ←CΣxxCT + R
Σyx ←CΣxx
ΣT
yxΣ−1
yy is termed the Kalman gain matrix
f ′ ←µx + ΣT
yxΣ−1
yy

y −µy

updated mean
F′ ←Σxx −ΣT
yxΣ−1
yy Σyx
updated covariance
p′ ←exp

−1
2

y −µy
T Σ−1
yy

y −µy

/ pdet 2πΣyy
likelihood contribution
return f ′, F′, p′
and
D
∆xt∆xT
t |y1:t−1
E
= AFt−1AT + Q,
D
∆yt∆xT
t |y1:t−1
E
= C

AFt−1AT + Q

,
D
∆yt∆yT
t |y1:t−1
E
= C

AFt−1AT + Q

CT + R.
By conditioning p(xt, yt|y1:t−1) on yt using the formula (1.8), we obtain a Gaussian
distribution with mean ft and covariance Ft given by
ft = ⟨xt|y1:t−1⟩+
D
∆xt∆yT
t |y1:t−1
E D
∆yt∆yT
t |y1:t−1
E−1 (yt −⟨yt|y1:t−1⟩) ,
Ft =
D
∆xt∆xT
t |y1:t−1
E
−
D
∆xt∆yT
t |y1:t−1
E D
∆yt∆yT
t |y1:t−1
E−1 D
∆yt∆xT
t |y1:t−1
E
.
The resulting recursion is summarised in Algorithm 1.1, generalised to time-dependent
noise means ¯xt, ¯yt and time-dependent transition and emission noise covariances.
Algebraically the updates generate a symmetric covariance Ft although, numerically,
symmetry can be lost. This can be corrected by either including an additional symmetrisa-
tion step, or by parameterising the covariance using a square root approach [22]. A detailed
discussion regarding the numerical stability of various representations is given in [26].
Smoothing: Rauch–Tung–Striebel/correction method
For reasons of numerical stability, the most common approach to smoothing is based on
the sequential approach, for which the continuous analogue of Eq. (1.6) is
p(xt|y1:T) ∝
Z
xt+1
p(xt|y1:t,
yt+1:T, xt+1)p(xt+1|y1:T).
Due to the closure properties of Gaussians, we may assume p(xt|y1:T) = N (xt gt,Gt) and
our task is to derive update formulae for the mean gt and covariance Gt. Rather than

Probabilistic time series models
15
long-handed integration, as in the derivation of the ﬁltering updates, we can make use
of some algebraic shortcuts. We note that p(xt|y1:t, xt+1) can be found by ﬁrst computing
p(xt, xt+1|y1:t) using the linear transition model, and then conditioning p(xt, xt+1|y1:t) on
xt+1. Given that p(xt|y1:t) has mean ft and covariance Ft, we obtain
⟨xt+1|y1:t⟩= Aft,
D
∆xt∆xT
t+1|y1:t
E
= FtAT,
D
∆xt+1∆xT
t+1|y1:t
E
= AFtAT + Q.
Therefore p(xt|y1:t, xt+1) has mean
⟨xt⟩+
D
∆xt∆xT
t+1
E D
∆xt+1∆xT
t+1
E−1 (xt+1 −⟨xt+1⟩)
(1.10)
and covariance
←−Σ t ≡
D
∆xt∆xT
t
E
−
D
∆xt∆xT
t+1
E D
∆xt+1∆xT
t+1
E−1 D
∆xt+1∆xT
t
E
,
(1.11)
where the averages are conditioned on the observations y1:t. Equations (1.10) and (1.11) are
equivalent to a reverse-time linear system
xt = ←−Atxt+1 + ←−mt + ←−η t,
where
←−At ≡
D
∆xt∆xT
t+1
E D
∆xt+1∆xT
t+1
E−1 ,
←−mt ≡⟨xt⟩−
D
∆xt∆xT
t+1
E D
∆xt+1∆xT
t+1
E−1 ⟨xt+1⟩,
and ←−η t ∼N
←−η t|0, ←−Σ t

. The statistics of p(xt|y1:T) then follow from the linear transformation
gt = ←−Atgt+1 + ←−mt,
Gt = ←−AtGt+1
←−AT
t + ←−Σ t.
The recursion is summarised in Algorithm 1.2. The cross moment, which is often required
for learning, is easily obtained as follows:
D
∆xt∆xT
t+1|y1:T
E
= ←−AtGt+1 ⇒
D
xtxT
t+1|y1:T
E
= ←−AtGt+1 + gtgT
t+1.
1.4.8
Non-linear latent Markov models
The HMM and LDS are the two main tractable workhorses of probabilistic time series mod-
elling. However, they lie at opposite ends of the modelling spectrum: the HMM assumes
fully discrete latent variables, whilst the LDS assumes fully continuous latent variables
restricted to linear updates under Gaussian noise. In practice one often encounters more
complex scenarios: models requiring both continuous and discrete latent variables, contin-
uous non-linear transitions, hierarchical models with tied parameters, etc. For such cases
exact inference is typically computationally intractable and approximations are required.
This forms a rich area of research, and the topic of several chapters of this book. Below we
give a brief overview of some classical deterministic and stochastic approximate inference
techniques that have been used in the time series context.
1.5
Deterministic approximate inference
In many deterministic approximate inference methods, a computationally intractable distri-
bution is approximated with a tractable one by optimising an objective function. For exam-
ple, one may assume a family of tractable distributions q(x|θq), parameterised by θq, and

16
David Barber, A. Taylan Cemgil and Silvia Chiappa
Algorithm 1.2 LDS backward pass. Compute the smoothed posteriors p(xt|y1:T). This
requires the ﬁltered results from Algorithm 1.1.
GT ←FT, gT ←fT
for t ←T −1, 1 do
{gt,Gt} = LDSBACKWARD(gt+1,Gt+1, ft, Ft; θt)
end for
function LDSBACKWARD (g,G, f, F; θ)
µx ←A f + ¯x
Σx′x′ ←AFAT + Q
Σx′x ←AF
statistics of p(xt, xt+1|y1:t)
←−Σ ←F −ΣT
x′xΣ−1
x′x′Σx′x
dynamics reversal p(xt|xt+1, y1:t)
←−
A ←ΣT
x′xΣ−1
x′x′
←−m ←f −←−
A µx
g′ ←←−
A g + ←−m
backward propagation
G′ ←←−
A G←−
A T + ←−Σ
return g′,G′
ﬁnd the best approximation to an intractable distribution p(x) by minimising the Kullback–
Leibler (KL) divergence KL

q(x|θq)|p(x)

=
D
log(q(x|θq)/p(x))
E
q(x|θq) with respect to θq.
The optimal q(x|θq) is then used to answer inference questions. In the Bayesian context,
parameter learning is also a form of inference problem which is intractable for most mod-
els of interest. Below we describe a popular procedure for approximating the parameter
posterior based on minimising a KL divergence.
1.5.1
Variational Bayes
In Bayesian procedures, one doesn’t seek a ‘single best’ parameter estimate θ, but rather a
posterior distribution over θ given by
p(θ|y1:T) = p(y1:T|θ)p(θ)
p(y1:T)
.
In latent variable models, the marginal likelihood p(y1:T) is given by
p(y1:T) =
Z
θ
Z
x1:T
p(x1:T, y1:T|θ)p(θ).
In practice, computing the integral over both θ and x1:T can be diﬃcult. The idea in
variational Bayes (VB) (see, for example, [27]) is to seek an approximation
p(x1:T, θ|y1:T) ≈q(x1:T, θ|y1:T),
where the distribution q is restricted to the form
q(x1:T, θ|y1:T) = q(x1:T|y1:T)q(θ|y1:T).

Probabilistic time series models
17
The best distribution q in this class can be obtained by minimising the KL divergence13
KL(q(x1:T)q(θ)|p(x1:T, θ|y1:T)) = 
log q(x1:T)
q(x1:T )
+ 
log q(θ)
q(θ) −
*
log p(y1:T|x1:T, θ)p(x1:T|θ)p(θ)
p(y1:T)
+
q(x1:T )q(θ)
.
The non-negativity of the divergence results in a lower bound on the marginal likelihood
log p(y1:T) ≥−
log q(x1:T)
q(x1:T ) −
log q(θ)
q(θ)
+ 
log p(y1:T|x1:T, θ)p(x1:T|θ)p(θ)
q(x1:T )q(θ) .
In many cases of interest, this lower bound is computationally tractable. Minimising the KL
divergence with respect to q(x1:T) and q(θ) is equivalent to maximising the lower bound,
which can be achieved by iterating the following numerical updates to convergence
1. q(x1:T)new ∝exp 
log p(y1:T|x1:T, θ)p(x1:T|θ)
q(θ)
2. q(θ)new ∝p(θ) exp 
log p(y1:T|x1:T, θ)
q(x1:T ) .
If we seek a point approximation q(θ) = δ (θ −θ∗), the above simpliﬁes to
1. q(x1:T)new ∝p(y1:T|x1:T, θ)p(x1:T|θ)
2. θnew = argmax
θ
n
log p(y1:T|x1:T, θ)
q(x1:T ) + log p(θ)
o
,
giving the penalised expectation maximisation (EM) algorithm [5]. For latent Markov
models,

log p(y1:T|x1:T, θ)
q(x1:T ) =
X
t

log p(yt|xt, θ)
q(xt) +
X
t

log p(xt|xt−1, θ)
q(xt−1,xt)
so that the EM algorithm requires smoothed single and pairwise expectations [23].
1.5.2
Assumed density ﬁltering
For more complex latent Markov models than the ones described in the previous sections,
the ﬁltering recursion (1.9) is in general numerically intractable. For continuous xt and
non-linear-Gaussian transition p(xt|xt−1) the integral over xt−1 may be diﬃcult, or give rise
to a distribution that is not in the same distributional family as p(xt−1|y1:t−1). In such cases,
a useful approximation can be obtained with the assumed density ﬁltering (ADF) method,
in which the distribution obtained from the ﬁltering recursion is projected back to a chosen
family [1].
More speciﬁcally, assume that we are given an approximation q(xt−1|y1:t−1) to
p(xt−1|y1:t−1), where q(xt−1|y1:t−1) is a distribution chosen for its numerical tractability (a
Gaussian for example). Using the ﬁltering recursion (1.9), we obtain an approximation for
the ﬁltered distribution at t
˜q(xt|y1:t) ∝
Z
xt−1
p(yt|xt)p(xt|xt−1)q(xt−1|y1:t−1).
13To simplify the subsequent expressions, we omit conditioning on the observations in the approximating
distribution.

18
David Barber, A. Taylan Cemgil and Silvia Chiappa
However, in general, ˜q(xt|y1:t) will not be in the same family as q(xt−1|y1:t−1). To deal with
this we project ˜q to the family q using
q(xt|y1:t) = argmin
q(xt|y1:t)
KL(˜q(xt|y1:t)|q(xt|y1:t)) .
For q in the exponential family, this corresponds to matching the moments of q(xt|y1:t) to
those of ˜q(xt|y1:t). Assumed density ﬁltering is a widely employed approximation method
and also forms part of other methods, such as approximate smoothing methods. For
example, ADF is employed as part of the expectation correction method for approximate
smoothing in the switching LDS (see Chapter 8 of this book). Furthermore, many approx-
imation methods are based on ADF-style approaches. Below, we provide one example of
an ADF-style approximation method for a Poisson model.
Example
In this example, we discuss a model for tracking the number of objects in
a given region based on noisy observations. Similar types of models appear in applica-
tions such as population dynamics (immigration) and multi-object tracking (see Chapters 3
and 11).
Suppose that, over time, objects of a speciﬁc type appear and disappear in a given
region. At time step t−1, there are st−1 objects in the region. At the next time step t, each of
the st−1 objects survives in the region independently of other objects with probability πsur.
We denote with ¯st the number of surviving objects. Additionally, vt new objects arrive with
rate b, independent of existing objects, so that the number of objects present in the region at
time step t becomes st = ¯st + vt. By indicating with BI and PO the Binomial and Poisson
distribution respectively, the speciﬁc survive–birth process is given by
Survive
¯st|st−1 ∼BI(¯st|st−1, πsur) =
 st−1
¯st
!
π¯stsur(1 −πsur)st−1−¯st,
Birth
st = ¯st + vt,
vt ∼PO(vt|b) = bvt
vt!e−b.
Due to errors, each of the st objects is detected only with probability πdet, meaning that
some objects remain possibly undetected. We denote with ˆst the number of detected objects
among the st objects. On the other hand, there is a number et of spurious objects that
are detected (with rate c), so that we actually observe yt = ˆst + et objects. The speciﬁc
detect–observe process is given by
Detect
ˆst|st ∼BI(ˆst|st, πdet),
Observe in clutter
yt = ˆst + et,
et ∼PO(et|c).
The belief network representation of this model is given in Fig. 1.8.
The inferential goal is to estimate the true number of objects st from the ﬁltered poste-
rior p(st|y1:t). Unfortunately, the ﬁltered posterior is not a distribution in any standard form
and becomes increasingly diﬃcult to represent as we proceed in time. To deal with this,
we make use of an ADF-style approach to obtain a Poisson approximation to the ﬁltered
posterior at each time step.
If we assume that p(st−1|y1:t−1) is Poisson, then a natural way to compute p(st|y1:t)
would be to use
p(st|y1:t) ∝p(yt|st)p(st|y1:t−1).

Probabilistic time series models
19
st−1
¯st
st
ˆst
¯st+1
yt
survive
birth
detect
clutter
Figure 1.8 Belief network representation of the counting
model. The goal is to compute the ﬁltering density p(st|y1:t).
Nodes ˆst−1, yt−1 are omitted for clarity.
The ﬁrst term p(yt|st) is obtained as the sum of a Binomial and a Poisson random variable.
The second term p(st|y1:t−1) may be computed recursively from p(st−1|y1:t−1) and is Poisson
distributed (see below). Performing ADF moment matching of the non-standard p(st|y1:t)
to a Poisson distribution is however not straightforward.
A simpler alternative approach (and not generally equivalent to ﬁtting the best Poisson
distribution to p(st|y1:t) in the minimal KL divergence sense) is to project p(ˆst|y1:t) to a
Poisson distribution using moment matching and then form
p(st|y1:t) =
X
ˆst
p(st|ˆst, y1:t−1)p(ˆst|y1:t) =
X
ˆst
p(st, ˆst, y1:t−1)
p(ˆst, y1:t−1) p(ˆst|y1:t)
= p(st|y1:t−1)
X
ˆst
p(ˆst|st)
p(ˆst|y1:t−1) p(ˆst|y1:t),
(1.12)
which, as we will see, is also Poisson distributed. Before proceeding with explaining the
recursion, we state two useful results for Poisson random variables. Let s and e be Poisson
random variables with respective intensities λ and ν. Then
Superposition The sum y = s + e is Poisson distributed with intensity λ + ν.
Conditioning The distribution of s conditioned on e is given by p(s|e) = BI(s|e, λ/ν).
Using these results we can derive a recursion as follows. At time t −1 we assume
p(st−1|y1:t−1) = PO(st−1|λt−1|t−1). This gives
p(¯st|y1:t−1) =
X
st−1
p(¯st|st−1)p(st−1|y1:t−1) = PO(¯st|πsurλt−1|t−1),
where we have used the following general result derived from the conditioning property:
X
n
BI(m|n, π)PO(n|λ) = PO(m|πλ) .
From the birth process and using the superposition property we obtain
p(st|y1:t−1) = PO(st|λt|t−1),
λt|t−1 = b + πsurλt−1|t−1 .
This gives
p(ˆst|y1:t−1) =
X
st
p(ˆst|st)p(st|y1:t−1) = PO(ˆst|πdetλt|t−1) .

20
David Barber, A. Taylan Cemgil and Silvia Chiappa
0
10
20
30
40
50
60
70
80
90
100
10
20
30
40
50
Observations
True number
Filt. Estimate
Figure 1.9 Assumed density ﬁlter-
ing for object tracking. The hori-
zontal axis denotes the time index
and the vertical axis the number
of objects. The dotted lines repre-
sent one standard deviation of the
ﬁltered posterior.
From the observe in clutter process and using the superposition property we obtain the
predictive distribution
p(yt|y1:t−1) = PO(yt|λt|t−1πdet + c) .
The posterior distribution of the number of detected objects is therefore
p(ˆst|y1:t) = BI(ˆst|yt, λt|t−1πdet/(λt|t−1πdet + c)) .
A well-known Poisson approximation to the Binomial distribution based on moment
matching has intensity
λ∗= argmin
λ
KL(BI(s|y, π)|PO(s|λ)) = yπ,
so that
p(ˆst|y1:t) ≈PO(ˆst|γ),
γ ≡ytλt|t−1πdet/(λt|t−1πdet + c) .
Using Eq. (1.12) with the Poisson approximation to p(ˆst|y1:t), we obtain
p(st|y1:t) ≈PO(st|λt|t−1)
X
ˆst
BI(ˆst|st, πdet)
PO(ˆst|πdetλt|t−1)PO(ˆst|γ)
∝(λt|t−1(1 −πdet))st
st!
X
ˆst
 st
ˆst
!  
γ
λt|t−1(1 −πdet)
!ˆst
|                               {z                               }

1+
γ
λt|t−1(1−πdet)
st
,
so that
p(st|y1:t) ≈PO(st|λt|t),
λt|t = (1 −πdet)λt|t−1 + yt
πdetλt|t−1
c + πdetλt|t−1
.
Intuitively, the ﬁrst term in λt|t corresponds to the undetected objects, whilst the second
term is the Poisson approximation to the Binomial posterior that results from observing the
sum of two Poisson random variables with intensities c and πdetλt|t−1. At time t = 1, we
initialise the intensity λ1|0 to the birth intensity.
In Fig. 1.9, we show the results of the ﬁltering recursion on data generated from the
model. As we can see, the tracking performance is good even though the ﬁlter involves an
approximation.
This technique is closely related to the Poissonisation method used heavily in proba-
bilistic analysis of algorithms [20]. In Chapters 3 and 11, an extension called the probability
hypothesis density (PHD) ﬁlter to multi-object tracking is considered. Instead of tracking a
scalar intensity, an intensity function over the whole space is approximated. The PHD ﬁlter
combines ADF with approximate inference methods such as sequential Monte Carlo (see
Section 1.6).

Probabilistic time series models
21
1.5.3
Expectation propagation
In this section, we review another powerful deterministic approximation technique called
expectation propagation (EP) [19]. We present here EP in the context of approximating the
posterior p(xt|y1:T) of a continuous latent Markov model p(x1:T, y1:T), see also Chapter 7.
According to Eqs. (1.4) and (1.7), the exact single and pairwise marginals have the form
p(xt|y1:T) ∝α(xt)β(xt),
p(xt, xt+1|y1:T) ∝α(xt)p(yt+1|xt+1)p(xt+1|, xt)β(xt+1).
Starting from these equations, we can retrieve the recursions (1.3)–(1.5) for α and β by
requiring that the single marginal is consistent with the pairwise marginal, that is
p(xt+1|y1:T) =
Z
xt
p(xt, xt+1|y1:T),
α(xt+1)β(xt+1) ∝
Z
xt
α(xt)p(yt+1|xt+1)p(xt+1|, xt)β(xt+1).
Cancelling β(xt+1) from both sides we immediately retrieve the standard α recursion. One
may derive the β recursion similarly by integrating the pairwise marginal over xt+1. For
complex situations, the resulting α(xt+1) is not in the same family as α(xt), giving rise
to representational diﬃculties. As in ADF, we therefore project α(xt+1) back to a chosen
family. Whilst this is reasonably well deﬁned, since α(xt+1) represents a ﬁltered distribution,
it is unclear how to project β(xt) to a chosen family since β(xt) is not a distribution in xt. In
EP, this problem is resolved by ﬁrst deﬁning
˜q(xt+1) ∝
Z
xt
α(xt)p(yt+1|xt+1)p(xt+1|, xt)β(xt+1),
and then iterating the following updates to convergence
α(xt+1) = argmin
α(xt+1)
KL
 
˜q(xt+1)| 1
Zt+1
α(xt+1)β(xt+1)
!
,
β(xt) = argmin
β(xt)
KL
 
˜q(xt)| 1
Zt
α(xt)β(xt)
!
,
where Zt and Zt+1 are normalisation constants. In exponential family approximations, these
updates correspond to matching the moments of ˜q to the moments of α(x)β(x).
1.6
Monte Carlo inference
Many inference problems such as ﬁltering and smoothing can be considered as comput-
ing expectations with respect to a (posterior) distribution. A general numerical method for
approximating the expectation Eπ
ϕ(x) =
R
x ϕ(x)π(x) of a function ϕ of a random vari-
able x is given by sampling. Consider a procedure that draws samples from a multivariate
distribution ˆπ(x1, . . . , xN). For X =
n
x1, . . . , xNo
, the random variable
¯EX,N ≡ϕ(x1) + · · · + ϕ(xN)
N

22
David Barber, A. Taylan Cemgil and Silvia Chiappa
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
n (iteration)
π
π
π
π
ϵ = 0: Periodic chain that fails
to converge to a stationary dis-
tribution.
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
n (iteration)
π
π
π
π
ϵ = 0.1: Chain that converges
to the stationary (uniform) dis-
tribution.
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
n (iteration)
π
π
π
π
ϵ = 0.25: Chain that converges
to the stationary (uniform) dis-
tribution more quickly than for
ϵ = 0.1.
Figure 1.10 Convergence to the stationary distribution. For ϵ = 0, the state transition diagram would be
disconnected, hence the chain fails to be irreducible and therefore to converge.
has expectation
Eˆπ
h ¯EX,N
i
= 1
N
X
n
Eˆπ
φ(xn) .
If the marginal distribution of each xn is equal to the target distribution
ˆπ(xn) = π(xn),
n = 1, . . . , N
then
Eˆπ
h ¯EX,N
i
= Eπ
ϕ(x) ,
that is ¯EX,N is an unbiased estimator of Eπ
ϕ(x). If, in addition, the samples are generated
independently,
ˆπ(x1, . . . , xN) =
N
Y
n=1
ˆπ(xn),
and Eπ
ϕ(x) and Vπ[ϕ(x)] are ﬁnite, the central limit theorem guarantees that, for
suﬃciently large N, ¯EX,N is Gaussian distributed with mean and covariance
Eπ
ϕ(x) ,
Vπ[ϕ(x)]
N
.
That is the variance of the estimator drops with increasing N. These results have important
practical consequences: If we have a procedure that draws iid samples x1, . . . , xN from π,
then the sample average ¯EX,N converges rapidly to the exact expectation Eπ
ϕ(x) as N
increases and provides a ‘noisy’ but unbiased estimator for any ﬁnite N. For large N the
error behaves as N−1/2 and is independent of the dimensionality of x. The key diﬃculty,
however, is in generating independent samples from the target distribution π. Below we
discuss various Monte Carlo methods that asymptotically provide samples from the target
distribution, varying in the degree to which they generate independent samples.

Probabilistic time series models
23
1.6.1
Markov chain Monte Carlo
In Markov chain Monte Carlo (MCMC) methods, samples from a desired complex dis-
tribution π(x) are approximated with samples from a simpler distribution deﬁned by a
specially constructed time-homogeneous Markov chain. Given an initial state x1, a set of
samples x2, . . . , xN from the chain are obtained by iteratively drawing from the transition
distribution (‘kernel’) K(xn|xn−1).14 The distribution πn(xn) satisﬁes
πn(xn) =
Z
xn−1 K(xn|xn−1)πn−1(xn−1),
which we compactly write as πn = Kπn−1. The theory of Markov chains characterises the
convergence of the sequence π1, π2, . . . If the sequence converges to a distribution π, then
π (called the stationary distribution) satisﬁes π = Kπ. For an ergodic chain, namely irre-
ducible and aperiodic,15 there exists a unique stationary distribution to which the sequence
converges, irrespective of the initial state x1. To illustrate the idea, consider the Markov
chain of Section 1.3.1 in which the robot moves freely under the transition model deﬁned
in Eq. (1.1), repeated here for convenience
K = ϵ

1
0
0
0
1
0
0
0
1
+ (1 −ϵ)

0
0
1
1
0
0
0
1
0
.
The robot starts at cell 3, i.e., π1 = (π11, π12, π13) = (0, 0, 1)⊤. In Fig. 1.10, we plot the cell
probabilities of πn = Kn−1π1 as n increases for various choices of ϵ. Provided 0 < ϵ ≤1,
all chains converge to the uniform distribution π = (1/3, 1/3, 1/3), however, with diﬀering
convergence rates.
This discussion suggests that, if we can design a transition kernel K such that the asso-
ciated Markov chain is ergodic and has the target distribution π as its stationary distribution,
at least in principle we can generate samples from the Markov chain that eventually will
tend to be from π. After ignoring the initial ‘burn in’ part of the generated path as the
sequence moves to the stationary distribution, the subsequent part can be used to estimate
expectations under π. Notice, however, that the samples generated will typically be depen-
dent and therefore the variance of the estimate may not scale inversely with the number of
samples from the chain.
Metropolis–Hastings
Designing a transition kernel K for a given target π is straightforward via the approach
proposed by Metropolis [18] and later generalised by Hastings [12]. Suppose that we are
given a target density π = φ/Z, where Z is a (possibly unknown) normalisation constant.
The Metropolis–Hastings (MH) algorithm uses a proposal density q(x|x′) for generating a
candidate sample x, which is accepted with probability 0 ≤α(x|x′) ≤1 deﬁned as
α(x|x′) = min
(
1, q(x′|x)π(x)
q(x|x′)π(x′)
)
.
The MH transition kernel K has the following form
K(x|x′) = q(x|x′)α(x|x′) + δ(x −x′)ρ(x′),
14Note that the sample index is conceptually diﬀerent from the time index in a time series model; here n is the
iteration number of the sampling algorithm.
15For ﬁnite state Markov chains, irreducibility means that each state can be visited starting from any other,
while aperiodicity means that each state can be visited at any iteration n larger than some ﬁxed number.

24
David Barber, A. Taylan Cemgil and Silvia Chiappa
Algorithm 1.3 Metropolis–Hastings
1: Initialise x1 arbitrarily.
2: for n = 2, 3 . . . do
3:
Propose a candidate: xcand ∼q(xcand|xn−1).
4:
Compute acceptance probability:
α(xcand|xn−1) = min
(
1, q(xn−1|xcand)π(xcand)
q(xcand|xn−1)π(xn−1)
)
.
5:
Sample from uniform distribution: u ∼U(u|[0, 1]).
6:
if u < α then
7:
Accept candidate: xn ←xcand.
8:
else
9:
Reject candidate: xn ←xn−1.
10:
end if
11: end for
where 0 ≤ρ(x′) ≤1 is deﬁned as
ρ(x′) =
Z
(1 −α(x|x′))q(x|x′)dx.
This kernel satisﬁes the detailed balance property
K(x|x′)π(x′)
=
(q(x|x′)α(x|x′) + δ(x −x′)ρ(x′))π(x′)
=
q(x|x′) min
(
1, q(x′|x)π(x)
q(x|x′)π(x′)
)
π(x′) + δ(x −x′)ρ(x′)π(x′)
=
min q(x|x′)π(x′), q(x′|x)π(x)	 + δ(x −x′)ρ(x′)π(x′)
=
q(x′|x) min
(q(x|x′)π(x′)
q(x′|x)π(x) , 1
)
π(x) + δ(x′ −x)ρ(x)π(x)
=
K(x′|x)π(x).
By integrating both sides over x′, we obtain
π(x) =
Z
K(x|x′)π(x′)dx′,
and therefore π is a stationary distribution of K. Note that to compute the acceptance prob-
ability α we only need to evaluate φ, since the normalisation constant Z cancels out. For a
given target π and proposal q(x′|x) we now have a procedure for sampling from a Markov
chain with stationary distribution π. The procedure is detailed in Algorithm 1.3.
Gibbs sampling
The Gibbs sampler [10, 17] is a MCMC method which is suitable for sampling a multivari-
ate random variable x = (x1, . . . , xD) with joint distribution p(x). Gibbs sampling proceeds
by partitioning the set of variables x into a chosen variable xi and the rest x = (xi, x−i). The
assumption is that the conditional distributions p(xi|x−i) are tractable. One then proceeds
coordinate-wise by sampling from the conditionals as in Algorithm 1.4.
Gibbs sampling can be viewed as MH sampling with the proposal
q(x|x′) = p(xi|x′
−i)δ  x−i −x′
−i
 .

Probabilistic time series models
25
Algorithm 1.4 Gibbs sampler
1: Initialise x1 = (x1
1, . . . , x1
D) arbitrarily.
2: for n = 2, 3 . . . do
3:
xn
1 ∼p(xn
1|xn−1
2
, xn−1
3
, . . . , xn−1
D ).
4:
xn
2 ∼p(xn
2|xn
1, xn−1
3
, . . . , xn−1
D ).
...
5:
xn
D ∼p(xn
D|xn
1, xn
2, . . . , xn
D−1).
6: end for
0
5
10
15
20
25
30
35
40
45
50
0
1
2
3
4
5
6
t
y
Figure 1.11 A typical realisation from the changepoint model. The time index is indicated by t and the number
of counts by yt. The true intensities are shown with a dotted line: at time step τ = 26, the intensity drops from
λ1 = 3.2 to λ2 = 1.2.
Using this proposal results in a MH acceptance probability of 1, so that every candidate
sample is accepted. Dealing with evidence (variables in known states) is straightforward –
one sets the evidential variables into their states and samples from the remaining variables.
Example: Gibbs sampling for a changepoint model
We illustrate the Gibbs sampler on a changepoint model for count data [13]. In this model,
at each time t we observe the count of an event yt. All the counts up to an unknown time τ
are iid realisations from a Poisson distribution with intensity λ1. From time τ + 1 to T, the
counts come from a Poisson distribution with intensity λ2. We assume that the changepoint
τ is uniformly distributed over 1, . . . , T and that the intensities λ1, λ2 are Gamma distributed
G(λi|a, b) =
1
Γ(a)baλa−1
i
e−bλi,
i = 1, 2 .
This leads to the following generative model
τ
∼
U(τ|1, . . . , T),
λi ∼G(λi|a, b),
i = 1, 2,
yt
∼
( PO(yt|λ1)
1 ≤t ≤τ,
PO(yt|λ2)
τ < t ≤T.
A typical draw from this model is shown in Fig. 1.11. The inferential goal is to compute
the posterior distribution p(λ1, λ2, τ|y1:T) of the intensities and changepoint given the count
data. In this problem this posterior is actually tractable and serves to assess the quality of
the Gibbs sampling approximation.
To implement Gibbs sampling we need to compute the distribution of each variable,
conditioned on the rest. These conditionals can be conveniently derived by writing the log

26
David Barber, A. Taylan Cemgil and Silvia Chiappa
Algorithm 1.5 A Gibbs sampler for the changepoint model
1: Initialise λ1
2, τ1.
2: for n = 2, 3 . . . do
3:
λn
1 ∼p(λn
1|τn−1, y1:T) = G(a + Pτn−1
t=1 yt, τn−1 + b).
4:
λn
2 ∼p(λn
2|τn−1, y1:T) = G(a + PT
t=τn−1+1 yt, T −τn−1 + b).
5:
τn ∼p(τn|λn
1, λn
2, y1:T).
6: end for
of the joint distribution of all variables and collecting terms that depend only on the free
variable. The log of the joint distribution is given by
log p(y1:T, λ1, λ2, τ) = log
p(λ1)p(λ2)p(τ)
τ
Y
t=1
p(yt|λ1)
T
Y
t=τ+1
p(yt|λ2)
.
This gives
log p(λ1|τ,  λ2, y1:T) =
a +
τ
X
t=1
yt −1
log λ1 −(τ + b)λ1 + const.,
log p(τ|λ1, λ2, y1:T) =
τ
X
t=1
yt log λ1 +
T
X
t=τ+1
yt log λ2 + τ (λ2 −λ1) + const.,
and a similar form for log p(λ1|τ, y1:T), so that both p(λ1|τ, y1:T) and p(λ2|τ, y1:T) are Gamma
distributions. The resulting Gibbs sampler is given in Algorithm 1.5. Samples from the
obtained posterior distribution are plotted in Fig. 1.12. For this particularly simple problem,
Gibbs sampling works well, with the estimated sample marginal estimates of λ and τ close
to the values we expect based on the known parameters used to generate the data.
Figure 1.12 Gibbs samples from the posterior of the changepoint model vs. sample iteration. True values are
shown with a horizontal line. (top) Intensities λ1 and λ2. (bottom) Changepoint index τ.

Probabilistic time series models
27
Figure 1.13 Importance sampling. (a) The solid curve denotes the unnormalised target distribution φ(x) and the
dashed curve the tractable IS distribution q(x). Samples from q(x) are assumed straightforward to generate and are
plotted on the axis. (b) To account for the fact that the samples are from q and not from the target p, we need to
reweight the samples. The IS distribution q generates too many samples where p has low mass, and too few where
p has high mass. The samples in these regions are reweighted accordingly. (c) Binning the weighted samples from
q, we obtain an approximation to p such that averages with respect to this approximation will be close to averages
with respect to p.
1.6.2
Sequential Monte Carlo
The MCMC techniques described above are batch algorithms that require the availability of
all data records. These techniques are therefore unsuitable when the data needs to be pro-
cessed sequentially and can be prohibitive for long time series. In such cases, it is desirable
to use alternative methods which process the data sequentially and take a constant time per
observation. In this context, sequential Monte Carlo (SMC) techniques [6, 8] have proved
useful in many applications. These methods are based on importance sampling/resampling
which we review below.
Importance sampling
Suppose that we are interested in computing the expectation Ep
ϕ(x) with respect to a dis-
tribution p(x) = φ(x)/Z, where the non-negative function φ(x) is known but the overall
normalisation constant Z is assumed to be computationally intractable. In importance
sampling (IS), instead of sampling from the target distribution p(x), we sample from a
tractable distribution q(x) and reweight the obtained samples to form an unbiased estimator
of Ep
ϕ(x). IS is based on the realisation that we can write the expectation with respect to
p as a ratio of expectations with respect to q, that is
Ep
ϕ(x) = 1
Z
Z
ϕ(x)φ(x)
q(x)q(x) = Eq
ϕ(x)W(x)
Z
= Eq
ϕ(x)W(x)
Eq [W(x)]
,
where W(x) ≡φ(x)/q(x) is called the weight function. Thus Ep
ϕ(x) can be approximated
using samples x1, . . . , xN from q as
PN
i=1 Wiϕ(xi)/N
PN
i=1 Wi/N
,

28
David Barber, A. Taylan Cemgil and Silvia Chiappa
where Wi ≡W(xi). The samples x1, . . . , xN are also known as ‘particles’. Using normalised
weights wi ≡Wi/ PN
i′=1 Wi′, we can write the approximation as
N
X
i=1
wiϕ(xi).
An example for a bimodal distribution p(x) and unimodal distribution q(x) is given in
Fig. 1.13, showing how the weights compensate for the mismatch between q and p.
1.6.3
Resampling
Unless the IS distribution q(x) is close to the target distribution p(x), the normalised weights
will typically have signiﬁcant mass in only a single component. This issue can be partially
addressed using resampling. Given a weighted particle system PN
i=1 wiδ(x −xi), resampling
is the term for a set of methods for generating randomly a reweighted particle system of the
form 1
M
PN
i=1 niδ(x −xi). Speciﬁcally, a resampling algorithm returns an occupancy vector
n1, . . . , nN which satisﬁes ni ∈{0, 1, 2, . . . , M}, P
i ni = M. For the resampling algorithm to
produce an unbiased estimator of the original system PN
i=1 wiδ(x −xi) we require
E
h 1
M
N
X
i=1
niδ(x −xi)
i
=
N
X
i=1
1
M E[ni]δ(x −xi) .
Hence, provided E[ni] = Mwi, expectations carried out using the resampled particles will
be unbiased. It is typical (though not necessary) to set M = N. Intuitively, resampling is
a randomised pruning algorithm in which we discard particles with low weight. Unlike
a deterministic pruning algorithm, the random but unbiased nature of resampling ensures
an asymptotically consistent algorithm. For a discussion and comparison of resampling
schemes in the context of SMC see [3, 8].
1.6.4
Sequential importance sampling
We now apply IS to the latent Markov models of Section 1.3. The resulting sequential IS
methods are also known as particle ﬁlters. The goal is to estimate the posterior
p(x1:t|y1:t) = p(y1:t|x1:t)p(x1:t)
|              {z              }
φ(x1:t)
/ p(y1:t)
|{z}
Zt
,
where we assume that the normalisation term Zt is intractable. At each time t, we have
an importance distribution qt(x1:t), from which we draw samples xi
1:t with corresponding
importance weights
Wi
t = φ(xi
1:t)/qt(xi
1:t).
Without loss of generality, we can construct q sequentially
qt(x1:t) = qt(xt|x1:t−1)qt(x1:t−1).
In particle ﬁltering, one chooses a distribution q that only updates the current xt and leaves
previous samples unaﬀected. This is achieved using
qt(x1:t) = qt(xt|x1:t−1)qt−1(x1:t−1) .

Probabilistic time series models
29
The weight function Wt(x1:t) then admits a recursive formulation
Wt(x1:t) = φ(x1:t)
qt(x1:t) = p(yt|xt)p(xt|xt−1) Qt−1
τ=1 p(yτ|xτ)p(xτ|xτ−1)
qt(xt|x1:t−1) Qt−1
τ=1 qτ(xτ|x1:τ−1)
= p(yt|xt)p(xt|xt−1)
qt(xt|x1:t−1)
|               {z               }
vt
Wt−1(x1:t−1) ,
where vt is called the incremental weight. Particle ﬁltering algorithms diﬀer in their choices
for qt(xt|x1:t−1). The optimal choice (in terms of reducing the variance of the weights) is the
one step ﬁltering distribution [7]
qt(xt|x1:t−1) = p(xt|xt−1, yt).
However, sampling from this distribution is diﬃcult in practice, and simpler distributions
are therefore employed. The bootstrap ﬁlter uses the transition
qt(xt|x1:t−1) = p(xt|xt−1),
for which the incremental weight becomes vt = p(yt|xt). In this case, the IS distribution does
not make any use of the recent observation and therefore has the tendency to lose track of
the high probability regions of the posterior. Indeed, it can be shown that the variance of
the importance weights for the bootstrap ﬁlter increases in an unbounded fashion [7, 17] so
that, after a few time steps, the particle set typically loses track of the exact posterior mode.
A crucial extra step to make the algorithm work is resampling, which prunes branches with
low weights and keeps the particle set located in high probability regions. It can be shown
that, although the particles become dependent due to resampling, the estimations are still
consistent and converge to the true values as the number of particles increases to inﬁnity.
A generic particle ﬁlter is given in Algorithm 1.6 and in Fig. 1.14 we illustrate the
dynamics of the algorithm in a tracking scenario. At time step t −1 each ‘parent’ particle
generates oﬀspring candidates xt from the IS distribution. The complete set of oﬀspring is
then weighted and resampled to generate a set of particles at time t. In the ﬁgure parent
particles are linked to their surviving oﬀspring.
1.7
Discussion and summary
Probabilistic time series models enable us to reason in a consistent way about tempo-
ral events under uncertainty. The probabilistic framework is particularly appealing for its
conceptual clarity, and the use of a graphical model representation simpliﬁes the devel-
opment of the models and associated inference algorithms. The Markov independence
assumption, which states that only a limited memory of the past is needed for understand-
ing the present, plays an important role in time series models. This assumption reduces the
burden in model speciﬁcation and simpliﬁes the computation of quantities of interest.
We reviewed several classical probabilistic Markovian models such AR models, hidden
Markov models and linear dynamical systems, for which inference is tractable. We then
discussed some of the main approximate approaches for the case of intractable inference,
namely deterministic methods such as variational techniques and assumed density ﬁltering
and stochastic methods such as Monte Carlo sampling.
Many real-world time series problems are highly specialised and require novel mod-
els. The probabilistic approach, coupled with a graphical representation, facilitates the

30
David Barber, A. Taylan Cemgil and Silvia Chiappa
Algorithm 1.6 Particle ﬁlter
for i = 1, . . . , N do
Compute the IS distribution: qt(xt|xi
1:t−1).
Generate oﬀsprings: ˆxi
t ∼qt(xt|xi
1:t−1).
Evaluate importance weights
vi
t = p(yt|ˆxi
t)p(ˆxi
t|xi
t−1)
qt(ˆxi
t|xi
1:t−1)
,
Wi
t = vi
tWi
t−1.
end for
if Not Resample then
Extend particles: xi
1:t = (xi
1:t−1, ˆxi
t), i = 1, . . . , N.
else
Normalise importance weights: ˜Zt ←P
j W j
t ,
˜wt ←(W1
t , . . . , WN
t )/ ˜Zt.
Generate associations: (a(1), . . . , a(N)) ←Resample( ˜wt).
Discard or keep particles and reset weights
xi
0:t ←(xa(i)
0:t−1, ˆxa(i)
t
),
Wi
t ←˜Zt/N,
i = 1, . . . , N.
end if
0
2
4
6
8
10
12
14
16
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
y
x
Figure 1.14 Illustration of the dynamics of a particle ﬁlter with N = 4 particles. The underlying latent Markov
model corresponds to an object moving with positive velocity on the real line. The vertical axis corresponds to the
latent log-velocities x and the horizontal axis to the observed noisy positions y: the underlying velocities of the
process are shown as ‘*’, while the observed positions are shown by dotted vertical lines. The nodes of the tree
correspond to the particle positions and the sizes are proportional to normalised weights ˜w(i).
development of tailored models and helps to reason about the computational complexity
of their implementation. The ﬁeld is currently very active, with many novel developments
in modelling and inference, several of which are discussed in the remainder of this book.

Probabilistic time series models
31
Acknowledgments
Silvia Chiappa would like to thank the European Commission for
supporting her research through a Marie Curie Intra–European Fellowship.
Bibliography
[1] B. D. Anderson and J. B. Moore. Optimal
Filtering. Prentice-Hall, 1979.
[2] G. Box, G. M. Jenkins and G. Reinsel. Time
Series Analysis: Forecasting and Control.
Prentice Hall, 1994.
[3] O. Capp´e, R. Douc and E. Moulines.
Comparison of resampling schemes for particle
ﬁltering. In 4th International Symposium on
Image and Signal Processing and Analysis,
pages 64–69, 2005.
[4] O. Capp´e, E. Moulines and T. Ryd´en. Inference
in Hidden Markov Models. Springer-Verlag,
2005.
[5] A. Dempster, N. Laird and D. Rubin. Maximum
likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1–38, 1977.
[6] A. Doucet, N. de Freitas and N. J. Gordon,
editors. Sequential Monte Carlo Methods in
Practice. Springer-Verlag, 2001.
[7] A. Doucet, S. Godsill and C. Andrieu. On
sequential Monte Carlo sampling methods for
Bayesian ﬁltering. Statistics and Computing,
10(3):197–208, 2000.
[8] A. Doucet and A. M. Johansen. A tutorial on
particle ﬁltering and smoothing: ﬁfteen years
later. Handbook of Nonlinear Filtering. Oxford
University Press, 2010.
[9] J. Durbin. The ﬁtting of time series models. Rev.
Inst. Int. Stat., 28, pages 233–243, 1960.
[10] S. Geman and D. Geman. Stochastic relaxation,
Gibbs distributions and the Bayesian restoration
of images. In M. A. Fischler and O. Firschein,
editors, Readings in Computer Vision: Issues,
Problems, Principles, and Paradigms, pages
564–584. Kaufmann, 1987.
[11] F. Gustafsson. Adaptive ﬁltering and change
detection. John Wiley & Sons, 2000.
[12] W. K. Hastings. Monte Carlo sampling methods
using Markov chains and their applications.
Biometrika, 57:97–109, 1970.
[13] A. M. Johansen, L. Evers and N. Whiteley.
Monte Carlo Methods, Lecture Notes,
Department of Mathematics, Bristol University,
2008.
[14] R. E. Kalman. A new approach to linear ﬁltering
and prediction problems. Transaction of the
ASME-Journal of Basic Engineering, 35–45,
1960.
[15] S. Kotz, N. Balakrishnan and N. L. Johnson.
Continuous Multivariate Distributions, volume
1, Models and Applications. John Wiley & Sons,
2000.
[16] S. L. Lauritzen. Thiele: Pioneer in Statistics.
Oxford University Press, 2002.
[17] J. S. Liu. Monte Carlo Strategies in Scientiﬁc
Computing. Springer, 2004.
[18] N. Metropolis and S. Ulam. The Monte Carlo
method. Journal of the American Statistical
Association, 44(247):335–341, 1949.
[19] T. Minka. Expectation Propagation for
approximate Bayesian inference. PhD thesis,
MIT, 2001.
[20] M. Mitzenmacher and E. Upfal. Probability and
Computing: Randomized Algorithms and
Probabilistic Analysis. Cambridge University
Press, 2005.
[21] J. R. Norris. Markov Chains. Cambridge
University Press, 1997.
[22] P. Park and T. Kailath. New square-root
smoothing algorithms. IEEE Transactions on
Automatic Control, 41:727–732, 1996.
[23] L. R. Rabiner. A tutorial on hidden Markov
models and selected applications in speech
recognation. Proceedings of the IEEE,
77(2):257–286, 1989.
[24] H. Rauch, F. Tung and C. Striebel. Maximum
likelihood estimates of linear dynamic systems.
American Institute of Aeronautics and
Astronautics Journal, 3(8):1445–1450, 1965.
[25] R. L. Stratonovich. Application of the Markov
processes theory to optimal ﬁltering. Radio
Engineering and Electronic Physics, 5(11):1–19,
1960. Translated from Russian.
[26] M. Verhaegen and P. Van Dooren. Numerical
Aspects of Diﬀerent Implementations. IEEE
Transactions On Automatic Control,
31(10):907–917, 1986.
[27] M. Wainwright and M. I. Jordan. Graphical
models, exponential families, and variational
inference. Foundations and Trends in Machine
Learning, 1:1–305, 2008.
Contributors
David Barber, Department of Computer Science, University College London
A. Taylan Cemgil, Department of Computer Engineering, Bo˘gazic¸i University, Istanbul, Turkey
Silvia Chiappa, Statistical Laboratory, Centre for Mathematical Sciences, University of Cambridge

2
Adaptive Markov chain Monte Carlo: theory and methods
Yves Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret
2.1
Introduction
Markov chain Monte Carlo (MCMC) methods allow us to generate samples from an arbi-
trary distribution π known up to a scaling factor; see [46]. The algorithm consists in
sampling a Markov chain {Xk, k ≥0} on a state space X with transition probability P
admitting π as its unique invariant distribution.
In most MCMC algorithms known so far, the transition probability P of the Markov
chain depends on some tuning parameter θ deﬁned on some space Θ which can be either
ﬁnite dimensional or inﬁnite dimensional. The success of the MCMC procedure depends
crucially upon a proper choice of θ.
To illustrate, consider the standard Metropolis–Hastings (MH) algorithm. For simplic-
ity, we assume that π has a density also denoted by π with respect to the Lebesgue measure
on X = Rd endowed with its Borel σ-ﬁeld X. Given that the chain is at x, a candidate y is
sampled from a proposal transition density q(x, ·) and is accepted with probability α(x, y)
deﬁned as
α(x, y) = 1 ∧π(y)
π(x)
q(y, x)
q(x, y),
where a ∧b
def= min(a, b). Otherwise, the move is rejected and the Markov chain stays at its
current location x.
A commonly used choice for the proposal kernel is the symmetric increment random
walk leading to the random walk MH algorithm (hereafter SRWM), in which q(x, y) =
q(y −x) for all (x, y) ∈X × X, for some symmetric proposal density function q on X. A
possible choice of the increment distribution q is the multivariate normal with zero-mean
and covariance matrix Γ, N(0, Γ), leading to the N-SRWM algorithm. As illustrated in
Fig. 2.1 in the one-dimensional case d = 1, if the variance is either too small or too large,
then the convergence rate of the N-SRWM algorithm will be slow and any inference from
values drawn from the chain is likely to be unreliable.1
Intuitively, this may be understood as follows. If the variance is too small, then almost
all the proposed values are accepted, and the algorithm behaves almost as a random
walk. Because the diﬀerence between two successive values is small, the algorithm vis-
its the state space very slowly. On the contrary, if the variance is too large, most of the
proposed moves fall far out in the tails of the target distribution. These proposals are
often rejected and the algorithm stays at the same place. Finding a proper scale is thus
1After J. Rosenthal, this eﬀect is referred to as the Goldilocks principle.

Adaptive Markov chain Monte Carlo: theory and methods
33
0
500
1000
−3
−2
−1
0
1
2
3
0
50
100
0
0.2
0.4
0.6
0.8
1
0
500
1000
−1
−0.5
0
0.5
1
1.5
2
2.5
0
50
100
0
0.2
0.4
0.6
0.8
1
0
500
1000
−3
−2
−1
0
1
2
3
0
50
100
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Figure 2.1 The N-SRWM in one dimension.
mandatory. In [23] the authors have shown that if the target and the proposal distributions
are both Gaussian, then an appropriate choice for covariance matrix for the N-SRWM is
Γ = (2.382/d)Γπ, where Γπ is the covariance matrix of the target distribution. This scal-
ing was proven to be optimal in a large-dimensional context (d →∞); see [23], [48] and
[49, Theorem 5].
In practice this covariance matrix Γ is determined by trial and error. This hand-tuning
requires some expertise and can be time-consuming. In order to circumvent this problem,
[30] (see also [31]) have proposed a novel algorithm, referred to as adaptive Metropolis, to
update continuously Γ during the run, according to the past values of the simulations. This
algorithm can be summarised as follows. At iteration k, we estimate the sample mean and
the covariance of the draws
µk+1 = µk +
1
k + 1(Xk+1 −µk),
(2.1)
Γk+1 = Γk +
1
k + 1((Xk+1 −µk)(Xk+1 −µk)T −Γk),
k ≥0,
(2.2)
Xk+1 being simulated from the Metropolis kernel with Gaussian proposal distribution
N

0, (2.382/d)Γk

. This algorithm (and its variants) proved to be very successful in per-
forming Bayesian inference of high-dimensional time series occurring in remote sensing
and ecology; see for example [28, 29, 39].
It was recognised by [4] in an inﬂuential report that such a scheme can be cast into
the more general framework of controlled MCMC. Controlled MCMC is a speciﬁc exam-
ple of an internal adaptation setting where the θ is updated from the past history of the
chain. Other examples of internal adaptation algorithms, which do not necessarily rely on
a stochastic approximation step, are given in Section 2.2.1; see also [5] and [52].
When attempting to simulate from probability measures with multiple modes or when
the dimension of the state space is large, the Markov kernels might mix so slowly that an
internal adaptation strategy cannot always be expected to work. Other forms of adaptation
can then be considered, using one or several auxiliary processes, which are run in parallel
to the chain {Xk, k ≥0} targeting π. Because the target chain is adapted using some auxiliary
processes, we refer to this adaptation framework as external.

34
Yves Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret
The idea of running several MCMC in parallel and making them interact has been
suggested by many authors; see for example [15, 16, 34]. This simple construction has
been shown to improve the overall mixing of the chain, by allowing the badly mixing chain
to explore the state more eﬃciently with the help of the auxiliary chain.
It is possible to extend signiﬁcantly this simple idea by allowing more general interac-
tions between the auxiliary chains. In particular [36] suggested to make the auxiliary chains
interact with the whole set of past simulations. Instead of allowing us to swap only the cur-
rent states of the auxiliary chains, the current state of the target chain may be replaced with
one of the states visited by an auxiliary chain in the past. The selection of this state can
be guided by sampling in the past of the auxiliary chains, with weights depending on the
current value of the state. This class of methods is referred to as interacting MCMC. These
chains can be cast into the framework outlined above, by allowing the parameter θ to take
its value in an inﬁnite-dimensional space.
The purpose of this chapter is to review adaptive MCMC methods, emphasising the
links between internal (controlled MCMC) and external (interacting MCMC) algorithms.
The emphasis of this review is to evidence general methodology to construct convergent
adaptive MCMC algorithms and to sketch some of the results required to prove their con-
vergence. This chapter complements two recent surveys on this topic by [5] and [52] which
put more emphasis on the design of internal algorithms.
The chapter is organised as follows. In Section 2.2 the general framework of adaptive
MCMC is presented and several examples are given. In Section 2.3 we establish the con-
vergence of the marginal distribution of {Xk, k ≥0}, and in Section 2.4 we establish a strong
law of large numbers for additive functionals. Finally, in Section 2.5, we show how to apply
these results to the equi-energy sampler of [36].
Notation
In the sequel, we consider Markov chains taking values in a general state space X, equipped
with a countably generated σ-ﬁeld X. For any measure ζ on (X, X) and any ζ-integrable
function f, we set ζ(f)
def=
R
X f(x) ζ(dx). A Markov kernel P on (X, X) is a mapping from
X × X into [0, 1] such that, for each A ∈X, x 7→P(x, A) is a non-negative, bounded and
measurable function on X, and, for each x ∈X, A 7→P(x, A) is a probability on X. For
any bounded measurable function f : X →R, we denote by Pf : X →R the function
x 7→P f(x)
def=
R
P(x, dx′) f(x′). For a measure ν on (X, X), we denote by νP the measure
on (X, X) deﬁned by, for any A ∈X, νP(A)
def=
R
P(x, A) ν(dx). For a bounded function f the
supremum norm is denoted by | f|∞. For a signed measure µ on (X, X), the total variation
norm is given by ∥µ∥TV
def= supf,| f|∞≤1 |µ( f)|.
2.2
Adaptive MCMC algorithms
We let {Pθ, θ ∈Θ} be a parametric family of Markov kernels on (X, X). We consider a
process (Ω, A, P, {(Xk, θk), k ≥0}) and a ﬁltration {Fk, k ≥0} (an increasing sequence of
σ-ﬁelds summarising the history of the process). It is assumed that (i) {(Xk, θk), k ≥0} is
adapted (in the sense that for each n ≥0, (Xk, θk) is Fk-measurable) and (ii) for any non-
negative function f,
E  f(Xk+1) | Fk
 = Pθk f(Xk) =
Z
Pθk(Xk, dy) f(y)
P −a.s.

Adaptive Markov chain Monte Carlo: theory and methods
35
In words, the latter relation means that at each time instant k ≥0 the next value Xk+1 is
drawn from the transition kernel Pθk(Xk, ·). As seen in the introduction, an adaptive MCMC
algorithm is said to be internal if the next value of the parameter θk+1 is computed from
the past value of the simulations X0, . . . , Xk+1 and of the parameters θ0, . . . , θk. It is said to
be external if the next value θk+1 is computed using an auxiliary process {Yk, k ≥0} run
independently from the process {Xk, k ≥0}. More precisely, it is assumed that the process
{θk, k ≥0} is adapted to the natural ﬁltration of the process {Yk, k ≥0}, meaning that for each
k, θk is a function of the history Y0:k
def= (Y0, Y1, . . . , Yk) of the auxiliary process. In the latter
case, conditioned on the auxiliary process {Yk, k ≥0}, {Xk, k ≥0} is an inhomogeneous
Markov chain such that for any bounded measurable function f
E  f(Xk+1) | X0:k, Y0:k
 = Pθk f(Xk).
Below we give several examples of internal and external adaptive MCMC algorithms.
2.2.1
Internal adaptive algorithms
Controlled MCMC
In the controlled MCMC case, for any θ ∈Θ, the kernel Pθ has an invariant distribution
π. The parameter θk is updated according to a single step of a stochastic approximation
procedure,
θk+1 = θk + γk+1H(θk, Xk, Xk+1),
k ≥0,
(2.3)
where Xk+1 is sampled from Pθk(Xk, ·). In most cases, the function H is chosen so that
the adaptation is easy to implement, requiring only moderate amounts of extra computer
programming, and not adding a large computational overhead; see [51] and [38] for imple-
mentation details. For reasons that will become obvious below, the rate of adaptation tends
to zero as the number k of iterations goes to inﬁnity, i.e. limk→∞γk = 0. On the other hand,
P∞
k=0 γk = ∞, meaning that the sum of the parameter moves can still be inﬁnite, i.e. the
sequence {θk, k ≥0} may move at an inﬁnite distance from the initial value θ0. It is not nec-
essarily required that the parameters {θk, k ≥0} converge to some ﬁxed value. An in-depth
description of controlled MCMC algorithms is given in [5], illustrated with many examples
(some of which are given below).
Under appropriate conditions, the recursions (2.3) converge to the set of solutions of
the equation h(θ) = 0 where θ 7→h(θ) is the mean-ﬁeld associated to H deﬁned as
h(θ)
def=
Z
X
H(θ, x, x′)π(dx)Pθ(x, dx′).
The convergence of Eq. (2.3) is discussed in numerous monographs: see for example [11,
20, 37, 17, 3].
The adaptive Metropolis algorithm
Returning to the adaptive Metropolis example in the introduction, the parameter θ is equal
to the mean and the covariance matrix of the multivariate distribution, θ = (µ, Γ) ∈Θ =
(Rd, Cd
+), where Cd
+ is the cone of symmetric non-negative d × d matrices. The expression
of H is explicitly given in Eqs. (2.1) and (2.2). Assuming that
R
X |x|2π(dx) < ∞, one can
easily check that the associated mean-ﬁeld function is given by
h(θ) =
Z
X
H(θ, x)π(dx) =
h
µπ −µ, (µπ −µ)(µπ −µ)T + Γπ −Γ
i
,

36
Yves Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret
0
0.5
1
(a)
1.5
2
x10
(b)
0
0.5.
1.5
2
50
100
0
0.5
1
1.5
2
0.2
0.4
0.6
0.8
150
0
1
x10
x10
Figure 2.2 (a) Trace plot of the 20000 ﬁrst iterations of the AM algorithm. (b) Top: suboptimality factor as a
function of the number of iterations. Bottom: mean acceptance rate as a function of the number of iterations
(obtained by averaging the number of accepted moves on a sliding window of size 6000).
where µπ and Γπ denote the mean and covariance of the target distribution,
µπ
def=
Z
X
x π(dx)
and
Γπ
def=
Z
X
(x −µπ)(x −µπ)Tπ(dx).
It is easily seen that this algorithm has a unique stationary point θ⋆= (µπ, Γπ). Provided that
the step-size is appropriately chosen, a stochastic approximation procedure will typically
converge toward that stationary point; see for example [3, 2].
The behavior of the adaptive metropolis (AM) algorithm is illustrated in Fig. 2.2. The
target distribution is a multivariate Gaussian distribution N(0, Σ) with dimension d = 200.
The eigenvalues of the covariance of the target distribution are regularly spaced in the
interval [10−2, 103]. This is a challenging simulation task because the dispersion of the
eigenvalues of the target covariance is large. The proposal distribution at step k ≥2d is
Pθk(x, ·) = (1 −β)N(x, (2.38)2Γk/d) + βN(x, 0.1 Id/d),
where Γk is the current estimate of the covariance matrix given in Eq. (2.2) and β is a
positive constant (we take β = 0.05, as suggested in [47]). The initial value for Γ0 is Id. The
rationale of using such β is to avoid the algorithm being stuck with a singular covariance
matrix (in the original AM algorithm, [31] suggested to regularise the covariance matrix
by loading the diagonal; another more sophisticated solution based on projections onto
compact sets is considered in [2]).
Figure 2.2 displays the trace plot of the ﬁrst coordinate of the chain for dimension
d = 200 together with the suboptimality criterion introduced in [49], deﬁned as
bk
def= d
Pd
i=1 λ−2
i,k
Pd
i=1 λ−1
i,k
2 ,
where λi,k are the eigenvalues of the matrix Γ1/2
k Γ−1/2
π
. Usually we will have bk > 1, and the
closer bk is to 1, the better. The criterion being optimised in AM is therefore b−1
k . Since the
seminal papers by [30], there have been numerous successful applications of this scheme, in
particular to some Bayesian inverse problems in atmospheric chemistry. Some interesting
variants have been considered, the most promising one being the DRAM adaptation [29],
which adds a new component to the AM method that is called Delayed Rejection (DR)
[45]. In the DR method, instead of one proposal distribution, several proposals can be

Adaptive Markov chain Monte Carlo: theory and methods
37
used. These propositions are used in turn, until a new value is accepted or a full sweep is
done. The DR acceptance probability formulation ensures that the chain is reversible with
respect to the target distribution. In the DRAM method the DR algorithm is used together
with several diﬀerent adaptive Gaussian proposals. This helps the algorithm in two ways.
First, it enhances the adaptation by providing accepted values that make the adaptation
start earlier. Second, it allows the sampler to work better for non-Gaussian targets and with
non-linear correlations between the components; see [29].
The coerced acceptance rate algorithm
For a Metropolis algorithm, a mean acceptance rate close to zero typically reﬂects that the
scale is too large (the moves are rejected). On the contrary, a mean acceptance rate close
to 1 typically occurs when the scale is too small (the moves are almost all accepted; see
Fig. 2.1). The choice of a proper scale can be automated by controlling the expected accep-
tance probability. For simplicity, we consider only the one-dimensional SRWM, where θ is
the scale. In this case, we choose
H(x, x′)
def= α(x, x′) −α⋆
	
which is associated to the mean-ﬁeld
h(θ) =
Z
π(dx)Pθ(x, dx′)α(x, x′) −α⋆,
where the acceptance ratio is α(x, x′) = 1 ∧π(x′)/π(x). The value of α⋆can be set to 0.4 or
0.5 (the ‘optimal’ value in this context is 0.44).
The same idea applies in large-dimensional context. We may for example couple this
approach with the AM algorithm: instead of using the asymptotic (2.38)2/d factor, we
might let the algorithm determine automatically a proper scaling by controlling both the
covariance matrix of the proposal distribution and the mean acceptance rate.
To illustrate the behaviour of the algorithm, we learn simultaneously the covariance and
the scaling. The target distribution is zero-mean Gaussian with covariance drawn at random
as above in dimension d = 100. The results are displayed in Fig. 2.3.
Regional adaptation algorithms
More sophisticated adaptation techniques may be used. An obvious idea is to try to
make the adaptation ‘local’, i.e. to adapt to the local behaviour of the target density. Such
techniques have been introduced to alleviate the main weakness of the adaptive Metropo-
lis algorithm when applied to a spatially non-homogeneous target which is due to the use
of a single global covariance distribution for the proposal. Consider for example the case
where the target density is a mixture of Gaussian distributions, π = Pp
j=1 ajN(µj, Σj). Pro-
vided that the overlap between the components is weak, and the covariances Σj are widely
diﬀerent, then there does not exist a common proposal distribution which is well-ﬁtted to
sample in the regions surrounding each mode. This example suggests to tune the empirical
covariance matrices by learning the history of the past simulations in diﬀerent regions of
the state space. To be more speciﬁc, assume that there exists a partition X = Sp
j=1 Xj. Then,
according to the discussion above, it is beneﬁcial to use diﬀerent proposal distributions in
each set of the partition. We might for example use the proposal
qθ(x; x′) =
p
X
j=1
1Xj(x)φ(x′; x, Γ j),

38
Yves Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret
Figure 2.3 (a) SRWM algorithm with identity covariance matrix with optimal scaling; (b) Adaptive Metropolis
with adapted scaling; the targeted value of the mean acceptance rate is α⋆= 0.234.
where 1A(x) is the indicator of the set A, and φ(x; µ, Γ) is the density of a d-dimensional
Gaussian distribution with mean µ and covariance Γ. Here, the parameter θ collects
the covariances of the individual proposal distributions within each region. With such a
proposal, the acceptance ratio of the MH algorithm becomes
αθ(x; x′) = 1 ∧
p
X
i, j=1
π(x′)
π(x)
φ(x; x′, Γj)
φ(x′; x, Γi) 1Xi(x)1X j(x′).
To adapt the covariance matrices {Γi, i
=
1, . . . , p} we can for example use the
updates (2.1) and (2.2) within each region. To ensure a proper communication between the
regions, it is recommended to mix the adaptive kernel with a ﬁxed kernel. This technique
is investigated in [18].
There are many other possible forms of adaptive MCMC and many of these are
presented in the recent surveys by [47, 5, 52].
Adaptive independence sampler
Another interesting direction of research is to adapt the proposal distribution of an
independence sampler. In this algorithm, the proposed moves do not depend on the cur-
rent state, i.e. q(x, x′) = q(x′) where q is some probability density on X. In such a case the
acceptance ratio can be written as α(x, x′) = 1 ∧(π(x′)q(x)/π(x)q(x′)). When the proposal
q is equal to the target π, the acceptance ratio is one and the correlation between adjacent
elements of the chain is zero. Thus it is desirable to choose the proposal distribution q to
be as close as possible to the target π. In an adaptive MCMC framework, a natural strategy
is to adjust a trial proposal to obtain a new proposal that is closer to π.
A natural idea is to choose a parametric family of distributions {qθ, θ ∈Θ}, and to adapt
the parameter θ from the history of the draws. This technique is of course closely related to
the adaptive importance sampling idea; see for example [53].

Adaptive Markov chain Monte Carlo: theory and methods
39
1000
2000
3000
4000
5000
6000
Figure 2.4 Adaptive ﬁt of a mixture of three Gaussian distributions with arbitrary means and covariance using the
maximum likelihood approach developed in [2]. See plate section for colour version.
Because of its ﬂexibility and tractability, a ﬁnite mixture of Gaussian distributions is
an appealing proposal family. Recall that any continuous distribution can be approximated
arbitrarily well by a ﬁnite mixture of normal Gaussians with common covariance matrix; in
addition, a discrete mixture of Gaussians is fast to sample from, and its likelihood is easy
to calculate. In this case, the parameters θ are the mixing proportions, means and common
covariance of the component densities. Several approaches have been considered to ﬁt these
parameters. Other mixtures from the exponential family can also be considered, such as a
discrete/continuous mixture of Student’s t-distribution (see [5] for details).
In [2], the authors suggest to ﬁt these parameters using a maximum likelihood approach
or, equivalently, by maximising the cross-entropy between the proposal distribution and the
target. This algorithm shares some similarities with the so-called adaptive independence
sampler developed in [35]. In this framework, the parameters are ﬁtted using a sequential
version of the expectation maximisation (EM) algorithm [13] (several improvements on
this basic scheme are presented in [5]).
In [27], the authors proposed a principled version replacing the EM by the k-harmonic
mean, an extension of the k-means algorithm that allows for soft membership. This algo-
rithm happens to be less sensitive to convergence to local minima than the recursive EM
algorithm; in addition degeneracies of the covariance matrices of the components can be
easily prevented. An example of ﬁt is given in Fig. 2.4.
Adaptive Gibbs sampler and Metropolis-within-Gibbs sampler
Gibbs samplers are commonly used MCMC algorithms for sampling from complicated
high-dimensional probability distributions π in cases where the full conditional distri-
butions of π are easy to sample from. To introduce these algorithms, some additional
notations are required. Let (X, X) be a d-dimensional state space, X = X1 × · · · × Xd
and write Xk ∈X as Xk = (Xk,1, . . . , Xk,d). We shall use the shorthand notation Xk,−i
def=
(Xk,1, . . . , Xk,i−1, Xk,i+1, . . . , Xk,d), and similarly X−i = X1 × · · · × Xi−1 × Xi+1 × · · · × Xd. We
denote by πi(x, ·) = π(·|x−i) the conditional distribution of Xi given X−i = x−i when X ∼π.
The set {πi}d
i=1 of transition density functions is referred to as the set of full conditional
distributions.

40
Yves Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret
The random scan Gibbs sampler draws Xk given Xk−1 iteratively by ﬁrst choosing one
coordinate at random according to some selection probabilities θ
def= (θ1, . . . , θd) and then
updating that coordinate by sampling from the associated full conditional distributions.
This random scan strategy was in fact the routine suggested by [24] in their seminal Gibbs
sampling paper. Of course, alternatives to the random scan strategies are available. The
transition kernel of this MCMC is a mixture of the full conditional distributions, the mixing
weights being equal to (θ1, . . . , θd). The key condition is that the chain induced by the Gibbs
sampling scheme is ergodic and, in particular, each coordinate of X is visited inﬁnitely often
in the limit.
The random scan provides much ﬂexibility in the choice of sampling strategy. Most
implementations use equal selection probabilities. While this updating scheme may seem
fair, it is counter the intuition of visiting coordinates of X which are more variable and
thus more diﬃcult to sample more often. Some recent work has suggested that the use of
non-uniform selection probabilities may signiﬁcantly improve the sampling performance;
see for example [40, 42, 41, 19].
Therefore, for random scan Gibbs samplers, a design decision is choosing the selection
probabilities which will be used to select which coordinate to update next. An adaptive
random scan Gibbs sampler will typically adapt at each iteration the selection probabil-
ities θk (according to the past values of the simulations X0, . . . , Xk and of the selection
probabilities θ0, . . . , θk−1), then select a new component Ik+1 ∈{1, . . . , d} to update with a
probability equal to the current ﬁt of the selection probability θk, draw Xk+1,Ik+1 from the cor-
responding conditional distribution π(·|Xk,−Ik+1), and ﬁnally update the state vector by setting
Xk+1,−Ik+1 = Xk,−Ik+1 for the remaining components. Several possible ways of adapting the
selection probabilities are introduced in [41].
Another possibility consists in using a Metropolis-within-Gibbs algorithm, where
the i-th component is updated not using the full conditional distribution πi but using a
Metropolis–Hastings step with the usual MH acceptance probability for πi (see [47, 52]).
In this setting both the selection probability and the proposal distributions associated to
each individual component sampler can be adapted simultaneously. The ﬁrst attempt to
adapt a Metropolis-within-Gibbs algorithm is the Single-Component Adaptive Metropo-
lis introduced in [32]. In this algorithm, the selection probabilities are kept constant and
only the scale of the individual component proposals are adapted. Reﬁned Metropolis-
within-Gibbs involving simultaneous adaptation of the selection probabilities and of the
proposal distributions has been introduced in [54] where this methodology has been applied
successfully to high-dimensional inference for statistical genetics.
2.2.2
External adaptive algorithm
We now turn to the description of external adaptive algorithms. This class of algorithm is
currently less popular than internal adaptive algorithms since samples from an auxiliary
process are required. This increases signiﬁcantly the computational burden of the algo-
rithm. Nevertheless, the improvements in convergence speed can be so large that sampling
an auxiliary process to help the original sampler might be, in certain diﬃcult simulation
tasks, an attractive solution.
The use of an auxiliary process to learn the proposal distribution in an independent
MH algorithm has been considered in [15] and [16]. In this setting, the parameter θk is a
distribution obtained using a histogram. This approach works best in situations where the
dimension of the state space d is small, otherwise the histogram estimation becomes very
unreliable (in [16] only one- and two-dimensional examples are considered).

Adaptive Markov chain Monte Carlo: theory and methods
41
Another form of interaction has been introduced in [36]: instead of trying to learn a
well ﬁtted proposal distribution, these authors suggest to ‘swap’ the current state of the
Markov chain with a state sampled from the history of the auxiliary processes. A similar
idea has been advocated in [6]. In this setting, the ‘parameter’ θk is inﬁnite dimensional,
and these algorithms may be seen as a kind of nonparametric extension of the controlled
MCMC procedures.
All these new ideas originate from parallel tempering and simulated tempering, two
inﬂuential algorithms developed in the early 1990s to speed up the convergence of MCMC
algorithms; see [25, 43, 26]. In these approaches, the sampling algorithm moves progres-
sively to the target distribution π through a sequence of distributions which are deemed
to be easier to sample than the target distribution itself. The idea behind parallel temper-
ing algorithm by [25] is to perform parallel Metropolis sampling at diﬀerent temperatures.
Occasionally, a swap between the states of two neighbouring chains (two chains running
at adjacent temperature levels) is proposed. The acceptance probability for the swap is
computed to ensure that the joint states of all the parallel chains evolve according to the
Metropolis–Hastings rule targeting the product distribution. The objective of the parallel
tempering is to use the faster mixing of the high temperature chains to improve the mix-
ing of the low temperature chains. The simulated tempering algorithm introduced in [43]
exploits a similar idea but using a markedly diﬀerent approach. Instead of using multiple
parallel chains, this algorithm runs a single chain but augments the state of this chain by
an auxiliary variable, the temperature, that is dynamically moved up or down the temper-
ature ladder. We discuss now more precisely the equi-energy sampler, which is the most
emblematic example of external adaptive algorithm.
The equi-energy sampler
The equi-energy (EE) sampler exploits the parallel tempering idea, in the sense that the
algorithm runs several chains at diﬀerent temperatures, but allows for more general inter-
actions between states of the neighbouring chains. The idea is to replace an instantaneous
swap by a so-called equi-energy move. To avoid cumbersome notations, we assume here
that there is a single auxiliary process, but it should be stressed that the EE sampler has
been reported to work better by using multiple auxiliary processes covering a wide range
of temperatures.
Let π be the target density distribution on (X, X). For β ∈(0, 1) deﬁne the tempered
density ˜π ∝π1−β. The auxiliary process {Yn, n ≥0} is X-valued and such that its marginal
distribution converges to ˜π as n goes to inﬁnity. Let P be a transition kernel on (X, X) with
unique invariant distribution π (in most cases, P is a MH kernel).
Let ϵ ∈(0, 1) be the probability of proposing a swap between the states of two neigh-
bouring chains. Deﬁne a partition X = SK
ℓ=1 Xℓ, where Xℓare the so-called rings (a term
linked with the particular choice of the partition in [36], which is deﬁned as the level set of
the logarithm of the target distribution).
At iteration n of the algorithm, two actions may be taken:
1. With probability (1 −ϵ), we move the current state Xn according to the Markov
kernel P.
2. With probability ϵ, we propose to swap the current state Xn with a state Z drawn from
the past of the auxiliary process with weights proportional to {g(Xn, Yi), i ≤n}, where

42
Yves Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret
g(x, y)
def=
K
X
ℓ=1
1Xℓ×Xℓ(x, y).
More precisely, we propose a move Z at random within the same ring as Xn. This
move is accepted with probability α(Xn, Z), where the acceptance probability α is
deﬁned by α : X × X →[0, 1] deﬁned by
α(x, y)
def= 1 ∧

π(y)
˜π(y)
"π(x)
˜π(x)
#−1= 1 ∧πβ(y)
πβ(x).
(2.4)
More formally, let Θ be the set of the probability measures on (X, X). For any distribution
θ ∈Θ, deﬁne the Markov transition kernel
Pθ(x, ·)
def= (1 −ϵθ(x))P(x, A) + ϵθ(x)Kθ(x, A)
with
Kθ(x, A)
def=
Z
A
α(x, y)g(x, y)θ(dy)
θ[g(x, ·)]
+ 1A(x)
Z
{1 −α(x, y)} g(x, y)θ(dy)
θ[g(x, ·)] ,
where θ[g(x, ·)]
def=
R
g(x, y)θ(dy) and
ϵθ(x) = ϵ1θ[g(x,·)]>0.
The kernel K˜π can be seen as a Hastings–Metropolis kernel with proposal kernel
g(x, y)˜π(y)/
R
g(x, y)˜π(dy) and target distribution π. Hence, πP˜π = π.
Using the samples drawn from this process, a family of weighted empirical probability
distributions {θn, n ≥0} is recursively constructed as follows:
θn
def=
1
n + 1
n
X
j=0
δYj =
 
1 −
1
n + 1
!
θn−1 +
1
n + 1δYn,
where δy is the Dirac mass at y and, by convention, θ−1 = 0. Given the current value Xn and
the sequence {θk, k ≤n}, Xn+1 is obtained by sampling the kernel Pθn(Xn, ·).
Figure 2.5 illustrates the eﬀectiveness of the EE sampler. In this example the target
distribution is the two-dimensional Gaussian mixture introduced in [36, p. 1591–1592].
Figure 2.5(a) represents independent sample points from the target distribution. This distri-
bution has 20 well separated modes (most local modes are more than 15 standard deviations
away from the nearest ones) and poses a serious challenge for sampling algorithms. We test
a plain SRWM, parallel tempering and the EE sampler on this problem. For parallel temper-
ing and the EE sampler, we use 5 parallel chains with β = 0, 0.64, 0.87, 0.95, 0.98. For
the equi-energy sampling, we deﬁne 5 equi-energy rings X1 = {x ∈R2 : −log π(x) < 2},
X2 = {x ∈R2 : 2 ≤−log π(x) < 6.3}, X3 = {x ∈R2 : 6.3 ≤−log π(x) < 20},
X4 = {x ∈R2 : 20 ≤−log π(x) < 63.2} and X5 = {x ∈R2 : −log π(x) ≥63.2}. Fig-
ure 2.5(b) plots the ﬁrst 2000 iterations from the EE sampler; Fig. 2.5(c) plots the ﬁrst
2000 iterations from parallel tempering, and Fig. 2.5(d) plots the ﬁrst 2000 iterations from
a plain SRWM algorithm. The plain SRWM exhibits very poor mixing for this example.
Parallel tempering mixes better but the EE sampler mixes even faster.

Adaptive Markov chain Monte Carlo: theory and methods
43
2
4
6
8
0
2
4
6
8
10
(a)
0
2
4
6
8
0
2
4
6
8
10
(b)
0
1
2
3
4
5
0
2
4
6
8
(c)
0
2
4
6
8
10
0
2
4
6
8
10
(d)
Figure 2.5 Comparison of (b) equi-energy sampler, (c) parallel tempering and (d) a plain SRWM. (a) Target
distribution.
2.3
Convergence of the marginal distribution
There is a diﬃculty with both the internal and external adaptation procedures: as the param-
eter estimate θk depends on the whole past either of the process or the auxiliary process,
the process {Xk, k ≥0} is no longer a Markov chain and classical convergence results
do not hold. This may cause serious problems, as illustrated in this naive example. Let
X = {1, 2} and consider, for θ, t1, t2 ∈Θ = (0, 1) with t1 , t2, the following Markov
transition probability matrices
Pθ =
" 1 −θ
θ
θ
1 −θ
#
,
˜P =
" 1 −t1
t1
t2
1 −t2
#
.
For any θ ∈Θ, π = (1/2, 1/2) satisﬁes πPθ = π. However if we let θk be a given function
Ξ : X →(0, 1) of the current state, i.e. θk = Ξ(Xk) with Ξ(i) = ti, i ∈{1, 2}, one deﬁnes
a new Markov chain with transition probability ˜P. Now ˜P has [t2/(t1 + t2), t1/(t1 + t2)] as
invariant distribution. Instead of improving the mixing, the adaptation has in such a case
destroyed the convergence to π.
The ﬁrst requirement for an MCMC algorithm to be well-behaved is that the distribution
of Xn converges weakly to the target distribution π as n →∞, i.e. for any measurable
bounded function f : X →R, limn→∞E  f(Xn) = π(f). When this limit holds uniformly
for any bounded function f : X →R, this convergence of the marginal {Xn, n ≥0} is
also referred to in the literature as the ergodicity of the marginal of the adaptive MCMC
{Xn, n ≥0} (see e.g. [50]).
Ergodicity of the marginal distributions for internal adaptive algorithms has been stud-
ied, under various assumptions, by [50, 2, 7]. The analysis of external adaptive MCMC
algorithms began more recently and the theory is still less developed; see [1] and [12].
2.3.1
Main result
For any θ, θ′ ∈Θ, deﬁne
DTV(θ, θ′)
def= sup
x∈X
∥Pθ(x, ·) −Pθ′(x, ·)∥TV .
For x ∈X, θ ∈Θ and for any ϵ > 0, deﬁne
Mϵ(x, θ)
def= inf{n ≥0,
Pn
θ(x, ·) −πθ
TV ≤ϵ}.
Consider the following assumptions

44
Yves Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret
A1 For any θ ∈Θ, Pθ is a transition kernel on (X, X) that possesses a unique invariant
probability measure πθ.
A2 (Diminishing adaptation) The sequence {DTV(θn, θn−1), n ≥1} converges to zero in
probability.
A3 (Containment condition) For any ϵ > 0, the sequence {Mϵ(Xn, θn), n ≥0} is bounded
in probability, i.e. limM→∞lim supn→∞P(Mϵ(Xn, θn) ≥M) = 0.
The diminishing adaptation condition requires that the amount of change (expressed in
terms of the supremum of the total variation distance between the two successive kernels
Pθn and Pθn+1) at the nth iteration vanishes as n →∞. This does not necessarily imply
however that the parameter sequence {θn, n ≥0} converges to some ﬁxed value. Since
the user controls the updating scheme, assumption A2 can be ensured by controlling the
amount of change of the parameter value θn.
If the function H in the stochastic approximation procedure (2.3) is bounded, then the
amount of change in the parameter at the nth iteration is just O(γn) and hence goes to 0
provided that limn→∞γn = 0.
The containment condition is most easily established by checking a uniform in θ ∈Θ
ergodicity; see for example Section 2.5 and [50, 7, 10].
The central result of this section is Theorem 2.1, which allows us to compare the
marginal distribution of Xn and the distribution πθn−N for some appropriately chosen
integer N:
Theorem 2.1 Assume A1, A2 and A3. Then, for any ϵ > 0, there exists N such that
lim sup
n→∞
sup
| f|∞≤1
E  f(Xn) −πθn−N( f) ≤ϵ.
When πθ = π⋆for any θ ∈Θ, this result shows the convergence of the marginal distri-
bution to π⋆; see also [50, Theorem 13]. The fact that some of the conditions A1, A2 and
A3 are also necessary has been discussed in this case; see [9].
If the sequence {πθn, n ≥0} is known to converge to π⋆, then Theorem 2.1 implies the
following corollary.
Corollary 2.2 Assume A1, A2 and A3. Assume in addition that for some bounded
measurable function f
lim
n→∞πθn( f) = π⋆( f) ,
P −a.s.
Then, limn→∞E  f(Xn) = π⋆( f).
2.4
Strong law of large numbers
In this section, we establish a strong law of large numbers (LLN) for adaptive MCMC.
The authors in [31] were the ﬁrst to prove the consistency of Monte Carlo averages for
the algorithm described by Eqs. (2.1) and (2.2) for bounded functions, using mixingales
techniques; these results have later been extended by [8] to unbounded functions. In [2] the
authors have established the consistency and the asymptotic normality of n−1 Pn
k=1 f(Xk)
for bounded and unbounded functions for controlled MCMC algorithms associated to a

Adaptive Markov chain Monte Carlo: theory and methods
45
stochastic approximation procedure (see [7] for extensions). The authors in [50] prove a
weak law of large numbers for bounded functions for general adaptive MCMC samplers.
Finally, [6] provides a consistency result for an external adaptive sampler.
The proof is based on the so-called martingale technique (see [44, Chapter 17]). We
consider ﬁrst this approach in the simple case of a homogeneous Markov chain, i.e.
Pθ = P. In this context, this technique amounts to decomposing n−1 Pn
k=1 f(Xk) −π⋆( f)
as n−1Mn( f) + Rn( f), where {Mn( f), n ≥0} is a P-martingale (w.r.t. the natural ﬁltration)
and Rn( f) is a remainder term. The martingale is shown to converge a.s. using standard
results and the remainder term is shown to converge to 0. This decomposition is not unique
and diﬀerent choices can be found in the literature. The most usual is based on the Poisson
equation with forcing function f, namely ˆf −P ˆf = f −π⋆( f). Suﬃcient conditions for the
existence of a solution to the Poisson equation can be found in [44, Chapter 17]. In terms
of ˆf, the martingale and the remainder terms may be expressed as
Mn( f)
def=
n
X
k=1
n ˆf(Xk) −P ˆf(Xk−1)
o
,
Rn( f)
def= n−1 h
P ˆf(X0) −P ˆf(Xn)
i
.
Proposition 2.3, which follows directly from [33, Theorem 2.18], provides suﬃcient
conditions for the almost-sure convergence of n−1 Pn
k=1 f(Xk) to π⋆( f).
Proposition 2.3 Let {Xk, k ≥0} be a Markov chain with transition kernel P and invariant
distribution π⋆. Let f : X →R be a measurable function; assume that the Poisson equation
g −Pg = f −π⋆( f) with forcing function f possesses a solution denoted by ˆf. Assume in
addition that (i) |P ˆf(X0)| < ∞, P-p.s. (ii) lim supn→∞n−1|P ˆf(Xn)| = 0 P-a.s. and (iii) there
exists p ∈[1, 2] such that P
k k−pE
h
| ˆf(Xk) −P ˆf(Xk−1)|p  Fk−1
i
< +∞P-a.s. , then
lim
n→∞n−1
n
X
k=1
f(Xk) = π⋆( f) ,
P −a.s.
The method based on martingales has been successfully used to prove the strong LLN
(and central limit theorems) for diﬀerent adaptive chains (see e.g. [2, 7, 6]). A weak law of
large numbers for bounded functions is also established in [50] using a diﬀerent approach.
We develop below a general scheme of proof for the strong LLN, based on the martin-
gale approach. Let f : X →R be a measurable function. For any θ ∈Θ, let ˆfθ denote a
solution to the Poisson equation g −Pθg = f −πθ(f) with forcing function f. Consider the
following decomposition
n−1
n
X
k=1
{ f(Xk) −πθk−1( f)} = n−1Mn( f) +
2
X
i=1
Rn,i(f),
where
Mn( f)
def=
n
X
k=1
{ ˆfθk−1(Xk) −Pθk−1 ˆfθk−1(Xk−1)},
Rn,1( f)
def= n−1 
Pθ0 ˆfθ0(X0) −Pθn−1 ˆfθn−1(Xn)

,
Rn,2( f)
def= n−1
n−1
X
k=1
{Pθk ˆfθk(Xk) −Pθk−1 ˆfθk−1(Xk)}.
{Mn( f), n ≥0} is a P-martingale and Rn,i( f), i = 1, 2 are remainder terms. Conditions
for the almost-sure convergence to zero of {n−1Mn( f), n ≥0} and of the residual term

46
Yves Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret
{Rn,1( f), n ≥1} are similar to those of Proposition 2.3. The remaining term Rn,2( f) requires
additional attention.
All the ingredients to establish the LLN are summarised in the following theorem.
Theorem 2.4 Assume A1. Let f be a measurable function such that, for any θ ∈Θ, a
solution ˆfθ to the Poisson equation g −Pθg = f −πθ( f) with forcing function f exists. If
(i) |Pθ0 ˆfθ0(X0)| < ∞, P-a.s.
(ii) lim supn→∞n−1|Pθn−1 ˆfθn−1(Xn)| = 0, P-a.s.
(iii) there exists p ∈[1, 2] such that
X
k
k−p E
h
| ˆfθk−1(Xk) −Pθk−1 ˆfθk−1(Xk−1)|p  Fk−1
i
< ∞,
P −a.s.
(iv) the series P
k k−1|Pθk ˆfθk(Xk) −Pθk−1 ˆfθk−1(Xk)| is ﬁnite P-a.s.
then limn→∞n−1 Pn
k=1{f(Xk) −πθk−1( f)} = 0, P-a.s.
If πθ = π⋆for any θ ∈Θ, Theorem 2.4 implies the strong LLN for the function f. When
πθ , π⋆, we have to prove the a.s. convergence of n−1 Pn
k=1 πθk−1( f) to π⋆( f), which can
be deduced from the a.s. convergence of {πθn( f), n ≥0} to π⋆( f). Suﬃcient conditions for
unbounded functions f are considered in [22].
2.5
Convergence of the equi-energy sampler
We study the convergence of the EE sampler described in Section 2.2.2 in the case K = 1.
The case K > 1 is studied in [22, 21]. We prove the convergence of the marginals and a
strong LLN for bounded continuous functions. Extensions of the strong LLN to unbounded
functions can be found in [6] and [22].
If {Yn, n ≥0} is such that n−1 Pn
k=1 f(Yk) →˜π( f) a.s. for any bounded function f,
the empirical distributions {θn, n ≥0} converge weakly to ˜π so that, asymptotically, the
dynamic of Xn is given by P˜π. Since πP˜π = π, it is expected that π governs the distribution
of {Xn, n ≥0} asymptotically. By application of the results of Sections 2.3 and 2.4 this
intuition can now be formalised.
The following conditions are assumed:
EES1 π is a bounded positive density distribution on X.
Proposition 2.5 (a) Assume EES1. For any θ ∈Θ, the transition kernel Pθ possesses a
unique probability invariant distribution πθ. Furthermore, for all x ∈X, k ≥0,
∥Pk
θ(x, ·) −πθ∥TV ≤ρk(θ) ∥δx −πθ∥TV ,
ρ(θ)
def= 1 −ϵ
Z
πβ(x)
supX πβ θ(dx).
(b) The condition A2 holds.
EES2 For any bounded function f : X →R, limn θn( f) = ˜π( f) P-a.s.
Condition EES2 holds whenever {Yn, n ≥0} is an ergodic Markov chain with stationary
distribution ˜π. A suﬃcient condition for ergodicity is the phi-irreducibility, the aperiodicity

Adaptive Markov chain Monte Carlo: theory and methods
47
and the existence of a unique invariant distribution ˜π for the transition kernel of the chain
{Yn, n ≥0}. For example, such a Markov chain {Yn, n ≥0} can be obtained by running a
HM sampler with invariant distribution ˜π, with proposal distribution chosen in such a way
that the sampler is ergodic (see e.g. [46]).
Proposition 2.6 Assume EES1 and EES2. Then condition A3 holds.
The convergence of πθn( f) to π( f) for some bounded function f is diﬃcult; such
convergence is established (under weak additional conditions) for continuous functions
in [22].
Theorem 2.7 Assume EES1, EES2 and limn πθn( f) = π( f) for some function f. Then,
lim
n
E f(Xn) −π( f)
 = 0.
Similarly, by checking the conditions of Theorem 2.4, we may establish the following
theorem.
Theorem 2.8 Assume EES1, EES2 and limn πθn( f) = π( f) for some bounded function f.
Then limn n−1 Pn
k=1 f(Xk) = π( f) = 0 a.s.
2.6
Conclusion
The adaptive MCMC framework provides a wide class of simulation methods, allowing
the eﬃciency of the sampler to be improved, in some situations signiﬁcantly. The theory
of internal adaptive algorithms is now quite well understood and the key design constraints
to develop a successful procedure are known. Some useful applications of this method
have already been proposed for Bayesian inference for diﬃcult large-scale problems, in
particular in inverse problems for remote sensing applications and in bioinformatics. There
are still a lot of open problems. Most of the adaptation techniques known so far are global,
whereas in most problems it happens to be more sensible to adopt state-dependent (or at
least regional) adaptation. The use of the internal adaptive algorithm for hybrid Monte
Carlo is yet in its infancy; even the choice of a sensible adaptation criterion in this case is
not obvious. Here again, it is likely that the proposal should ideally be state- or regional-
dependent.
The external adaptive algorithms are very promising. The theory is less mature, the ﬁrst
complete convergence proofs (under some restrictive assumptions) have been obtained only
very recently. The potential of these methods in applications is important and yet largely
unexplored. Using the previous experiences of tempered MCMC algorithms, it is likely
that these algorithms have the potential of oﬀering signiﬁcant gains, and some encouraging
results have already been reported. The optimisation of the simulation structure is still
far from being well understood: the number of subprocesses, the amount of interactions
between the subprocesses, the choice of the target distributions for the subprocesses have
to be investigated both from the methodological and the theoretical sides.
2.A
Appendix: Proof of Section 2.5
Since K = 1, the transition kernel Pθ is given by
Pθ(x, A) = (1 −ϵ)P(x, A) + ϵ
(Z
A
α(x, y)θ(dy) + δx(A)
Z
(1 −α(x, y)) θ(dy)
)
.

48
Yves Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret
2.A.1
Proof of Proposition 2.5
(a) It is suﬃcient to prove that the Dobrushin coeﬃcient of the transition kernel Pθ is
upper bounded by ρ(θ) (see e.g. [14, Theorem 4.3.16.]). Denote by δ(Pθ) the Dobrushin
coeﬃcient. By deﬁnition, (see e.g. [14, Deﬁnition 4.3.7. and Lemma 4.3.5.])
δ(Pθ) ≤1 −inf
N
X
i=1
{Pθ(x, Ai) ∧Pθ(y, Ai)},
where the inﬁmum is taken over all (x, y) ∈X2 and all ﬁnite measurable partitions
A1, . . . , AN of X. By deﬁnition of Pθ and α(x, y) (see Eq. (2.4))
Pθ(x, A) ≥ϵ
Z
A
(
1 ∧πβ(y)
πβ(x)
)
θ(dy) ≥ϵ
Z
A
πβ(y)
(
1
πβ(y) ∧
1
πβ(x)
)
θ(dy)
≥[sup
X
πβ]−1 ϵ
Z
A
πβ(y) θ(dy) = (1 −ρ(θ)) νθ(A),
where νθ(A) ∝
R
A πβ(y) θ(dy) is a probability distribution. This implies that δ(θ) ≤ρ(θ) and
concludes the proof.
(b) By deﬁnition of the transition kernel, for any bounded function f,
Pθ f(x) −Pθ′ f(x) = ϵ
Z
α(x, y){f(y) −f(x)}  θ(dy) −θ′(dy) .
Since |α(x, y)| ≤1, sup| f|∞≤1 |Pθ f(x) −Pθ′ f(x)| ≤2ϵ sup|f|∞≤1 |θ( f) −θ′( f)|. Therefore, we
have to prove that {∥θn −θn−1∥TV, n ≥1} converges to zero in probability. This holds since
|θn( f) −θn−1( f)| ≤|f(Yn)|
n + 1 +
1
n(n + 1)
n−1
X
k=0
| f(Yk)| ≤
2
n + 1| f|∞.
2.A.2
Proof of Proposition 2.6
Let η > 0, and set γ
def= supX πβ. Under EES1, ρ(θ) ∈(0, 1) for any θ ∈Θ. By Proposi-
tion 2.5, Mη(x, θ) ≤log η/ log ρ(θ) thus proving that the sequence {Mη(Xn, θn), n ≥0} is
bounded in probability if
lim
M→+∞lim sup
n
P
 
K(θn) ≤1 −η1/M
εγ
!
= 0 ,
K(θ)
def=
Z
πβ(x)θ(dx).
Since K(θn) = (n + 1)−1 Pn
k=0 πβ(Yk) and under EES1 supX πβ = γ−1 < +∞; then, under
EES2, limn K(θn) =
R
πβ(x)˜π(dx) = Cβ almost-surely, where Cβ is such that ˜π = Cβπ1−β.
We have Cβ ≥1 and it can be assumed without loss of generality that M is large enough so
that (1 −η1/M)/(εγ) < Cβ/2. We then have
P
 
K(θn) ≤1 −η1/M
εγ
!
≤P

K(θn) −Cβ ≤−Cβ/2

≤P
K(θn) −Cβ
 ≥Cβ/2

.
Since {K(θn) −Cβ, n ≥0} converges a.s. to zero, the rhs converges to zero as n →∞, thus
concluding the proof.

Adaptive Markov chain Monte Carlo: theory and methods
49
2.A.3
Proof of Theorem 2.8
Let f such that | f|∞≤1. By Proposition 2.5, for any θ, the function ˆfθ exists on X and
| ˆfθ|∞≤2[1 −ρ(θ)]−1.
Condition (i) is satisﬁed since |Pθ0 ˆfθ0(X0)| ≤| ˆfθ0|∞≤2[1 −ρ(θ0)]−1, and ρ(θ0) ∈(0, 1).
Condition (ii) follows from |Pθn−1 ˆfθn−1(Xn)| ≤2[1 −ρ(θn−1)]−1 and the fact that
limn→∞ρ(θn) = ρ(˜π) ∈(0, 1) P-a.s. (which holds under EES2).
For p ∈(1, 2]
 ˆfθk−1(Xk) −Pθk−1 ˆfθk−1(Xk−1)
p ≤2p| ˆfθk−1|p
∞≤2p+1[1 −ρ(θk−1)]−p.
Under EES2, limn→∞ρ(θn) = ρ(˜π) ∈(0, 1) P-a.s. showing (iii).
Finally, we have (see [22])
Pθ ˆfθ −Pθ′ ˆfθ′ =
X
n≥1
n−1
X
j=0

Pj
θ −πθ

(Pθ −Pθ′)

Pn−j−1
θ′
f −πθ′( f)

+ (πθ′ −πθ) Pθ′ ˆfθ′.
For the ﬁrst term, by Proposition 2.5, there exists a constant C such that for any θ, θ′

X
n≥1
n−1
X
j=0

P j
θ −πθ

(Pθ −Pθ′)

Pn−j−1
θ′
f −πθ′( f)


≤C
X
n≥1
n−1
X
j=0
∥Pθ −Pθ′∥TV ρ(θ)j ρ(θ′)n−1−j
≤2Cϵ ∥θ −θ′∥TV [1 −ρ(θ)]−1[1 −ρ(θ′)]−1.
By deﬁnition of θk, ∥θk −θk−1∥TV = O(1/k) (see the proof of Proposition 2.5); and under
EES2, limk ρ(θk) = ρ(˜π) ∈(0, 1) a.s. This implies that the series
X
k
1
k

X
n≥1
n−1
X
j=0

Pj
θk −πθk
  Pθk −Pθk−1
 
Pn−j−1
θk−1
f −πθk−1( f)


is ﬁnite P-a.s. For the second term,
(πθ′ −πθ) Pθ′ ˆfθ′
 ≤∥πθ −πθ′∥TV | ˆfθ′|∞≤2[1 −ρ(θ′)]−1 ∥πθ −πθ′∥TV.
Combining the decomposition
πθ( f) −πθ′( f) =

Pl
θ′ f(x) −πθ′(f)

+

πθ( f) −Pl
θ f(x)

+

Pl
θ f(x) −Pl
θ′ f(x)

,
and Proposition 2.5, we have for any l ≥1, x ∈X and θ, θ′
∥πθ −πθ′∥TV ≤2
n
ρl(θ) + ρl(θ′)
o
+ sup
h,|h|∞≤1
l−1
X
j=0
Pj
θ (Pθ −Pθ′)

Pl−j−1
θ′
h(x) −πθ′(h)
 .
Since ρ(θ) ∈(0, 1) for all θ, this yields by Proposition 2.5
∥πθ −πθ′∥TV ≤
2
1 −ρ(θ)∥Pθ −Pθ′∥TV ≤
4ϵ
1 −ρ(θ)∥θ −θ′∥TV.
Using again EES2 and the property ∥θk −θk−1∥TV = O(1/k), the series
X
k
1
k
 πθk−1 −πθk
 Pθk−1 ˆfθk−1

converges P-a.s. The proof is concluded.

50
Yves Atchad´e, Gersende Fort, Eric Moulines and Pierre Priouret
Bibliography
[1] C. Andrieu, A. Jasra, A. Doucet and
P. Del Moral. On non-linear Markov chain
Monte Carlo via self-interacting approximations.
To appear, Bernoulli 2011.
[2] C. Andrieu and E. Moulines. On the ergodicity
property of some adaptive MCMC algorithms.
Annals of Applied Probability, 16(3):1462–1505,
August 2006.
[3] C. Andrieu, E. Moulines and P. Priouret.
Stability of stochastic approximation under
veriﬁable conditions. SIAM Journal on Control
and Optimization, 44(1):283–312 (electronic),
2005.
[4] C. Andrieu and C. Robert. Controlled Markov
chain Monte Carlo methods for optimal
sampling. Technical Report 125, Cahiers du
Ceremade, 2001.
[5] C. Andrieu and J. Thoms. A tutorial on adaptive
MCMC. Statistics and Computing,
18(4):343–373, 2008.
[6] Y. Atchad´e. A cautionary tale on the eﬃciency of
some adaptive Monte Carlo schemes. Technical
report, ArXiv:0901:1378v1, 2009.
[7] Y. Atchad´e and G. Fort. Limit theorems for some
adaptive MCMC algorithms with subgeometric
kernels. Bernoulli, 16(1): 116–154, 2010.
[8] Y. F. Atchad´e and J. S. Rosenthal. On adaptive
Markov chain Monte Carlo algorithms.
Bernoulli, 11(5):815–828, 2005.
[9] Y. Bai. Simultaneous drift conditions for
adaptive Markov chain Monte Carlo algorithms.
Technical report, University of Toronto,
available at www.probability.ca/jeﬀ/ftpdir/
yanbai2.pdf, 2009.
[10] Y. Bai, G. O. Roberts and J. S. Rosenthal. On the
containment condition for adaptive Markov
chain Monte Carlo algorithms. Technical report,
University of Toronto, available at www.
probability.ca/jeﬀ/, 2009.
[11] A. Benveniste, M. M´etivier and P. Priouret.
Adaptive Algorithms and Stochastic
Approximations, volume 22. Springer, 1990.
Translated from the French by Stephen S. S.
Wilson.
[12] B. Bercu, P. Del Moral and A. Doucet. A
functional central limit theorem for a class of
interacting Markov chain Monte Carlo methods.
Electronic Journal of Probability,
14(73):2130–2155, 2009.
[13] O. Capp´e and E. Moulines. On-line
expectation-maximization algorithm for latent
data models. Journal of the Royal Statistical
Society B, 71(3):593–613, 2009.
[14] O. Capp´e, E. Moulines and T. Ryd´en. Inference
in Hidden Markov Models. Springer, 2005.
[15] D. Chauveau and P. Vandekerkhove. Un
algorithme de Hastings-Metropolis avec
apprentissage s´equentiel. Comptes cendus de
l’Academie des Sciences Paris S´eries I
Mathematique, 329(2):173–176, 1999.
[16] D. Chauveau and P. Vandekerkhove. Improving
convergence of the Hastings-Metropolis
algorithm with an adaptive proposal.
Scandinavian Journal of Statistics, 29(1):13–29,
2002.
[17] H-F. Chen. Stochastic Approximation and Its
Applications, volume 64 of Nonconvex
Optimization and Its Applications. Kluwer
Academic Publishers, 2002.
[18] R. V. Craiu, J. S. Rosenthal and C. Yang. Learn
from thy neighbor: Parallel-chain adaptive
MCMC. Journal of the American Statistical
Association, 104(488):1454–1466, 2009.
[19] P. Diaconis, K. Khare and L. Saloﬀ-Coste. Gibbs
sampling, exponential families and orthogonal
polynomials (with discussion and rejoinder).
Statistical Science, 23(2):151–178, 2008.
[20] M. Duﬂo. Random Iterative Models, volume 34.
Springer, 1997. Translated from the 1990 French
original by S. S. Wilson and revised by the
author.
[21] G. Fort, E. Moulines and P. Priouret.
Convergence of interacting MCMC: central limit
theorem. Technical report, Institut
Telecom/Telecom ParisTech; CNRS/UMR 5181,
2010.
[22] G. Fort, E. Moulines and P. Priouret.
Convergence of adaptive and interacting Markov
chain Monte Carlo algorithms. Technical report,
Institut Telecom/Telecom ParisTech; CNRS/
UMR 5141, 2010.
[23] A. Gelman, G. O. Roberts and W. R. Gilks.
Eﬃcient Metropolis jumping rules. In Bayesian
Statistics, 5 (Alicante, 1994), pages 599–607,
Oxford University Press 1996.
[24] S. Geman and D. Geman. Stochastic relaxation,
Gibbs distributions and the Bayesian restoration
of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6:721–741,
1984.
[25] C. J. Geyer. Markov chain Monte Carlo
maximum likelihood. Computing Science and
Statistics: Proc. 23rd Symposium on the
Interface, Interface Foundation, Fairfax Station,
VA, pages 156–163, 1991.
[26] C. J. Geyer and E. A. Thompson. Annealing
Markov chain Monte Carlo with applications to
ancestral inference. Journal of the American
Statistical Association, 90:909–920, 1995.
[27] P. Giordani and R. Kohn. Adaptive independent
Metropolis-Hastings by fast estimation of
mixtures of normals, 2008.
[28] H. Haario, M. Laine, M. Lehtinen, E. Saksman
and J. Tamminen. Markov chain Monte Carlo

Adaptive Markov chain Monte Carlo: theory and methods
51
methods for high dimensional inversion in
remote sensing. Journal of the Royal Statistical
Society B, 66(3):591–607, 2004.
[29] H. Haario, M. Laine, A. Mira and E. Saksman.
DRAM: eﬃcient adaptive MCMC. Statistics and
Computing, 16:339–354, 2006.
[30] H. Haario, E. Saksman and J. Tamminen.
Adaptive proposal distribution for random walk
Metropolis algorithm. Computational Statistics,
14:375–395, 1999.
[31] H. Haario, E. Saksman and J. Tamminen. An
adaptive Metropolis algorithm. Bernoulli,
7:223–242, 2001.
[32] H. Haario, E. Saksman and
J. Tamminen. Componentwise adaptation for
high dimensional MCMC. Computational
Statistics, 20(2):265–273, 2005.
[33] P. Hall and C. C. Heyde. Martingale Limit
Theory and its Application. Academic Press,
New York, London, 1980.
[34] A. Jasra, D. A. Stephens and C. C. Holmes. On
population-based simulation for static inference.
Statistics and Computing, 17(3):263–279, 2007.
[35] J. Keith, D. Kroese and G. Sofronov. Adaptive
independence samplers. Statistics and
Computing, 18:409–420, 2008.
[36] S. C. Kou, Q. Zhou and W. H. Wong.
Equi-energy sampler with applications in
statistical inference and statistical mechanics.
Annals of Statistics, 34(4):1581–1619, 2006.
[37] H. J. Kushner and G. G. Yin. Stochastic
Approximation and Recursive Algorithms and
Applications, volume 35. Springer, 2nd edition,
2003.
[38] M. Laine. MCMC toolbox for Matlab, 2008.
www.helsinki.ﬁ/ mjlaine/mcmc/.
[39] M. Laine and J. Tamminen. Aerosol model
selection and uncertainty modelling by adaptive
mcmc technique. Atmospheric and Chemistry
Physics, 8:7697–7707, 2008.
[40] R. Levine. A note on markov chain Monte-Carlo
sweep strategies. Journal of Statistical
Computation and Simulation, 75(4):253–262,
2005.
[41] R. Levine and G. Casella. Optimizing random
scan Gibbs samplers. Journal of Multivariate
Analysis, 97:2071–2100, 2006.
[42] R. A. Levine, Z. Yu, W. G. Hanley and J. A.
Nitao. Implementing Random Scan Gibbs
Samplers. Computational Statistics, 20:177–196,
2005.
[43] E. Marinari and G. Parisi. Simulated tempering:
A new Monte Carlo scheme. Europhysics
Letters, 19:451–458, 1992.
[44] S. P. Meyn and R. L. Tweedie. Markov Chains
and Stochastic Stability. Cambridge University
Press, 2009.
[45] A. Mira. On Metropolis-Hastings algorithms
with delayed rejection. Metron,
LIX(3-4):231–241, 2001.
[46] C. P. Robert and G. Casella. Monte Carlo
Statistical Methods. Springer, 2nd edition, 2004.
[47] G. Roberts and J. Rosenthal. Examples of
adaptive MCMC. Journal of Computational and
Graphical Statistics 18(2):349–367, 2009.
[48] G. O. Roberts, A. Gelman and W. R. Gilks.
Weak convergence and optimal scaling of
random walk Metropolis algorithms. Annals of
Applied Probability, 7(1):110–120, 1997.
[49] G. O. Roberts and J. S. Rosenthal. Optimal
scaling for various Metropolis-Hastings
algorithms. Statistical Science, 16(4):351–367,
2001.
[50] G. O. Roberts and J. S. Rosenthal. Coupling and
ergodicity of adaptive Markov chain Monte
Carlo algorithms. Journal of Applied
Probability, 44(2):458–475, 2007.
[51] J. S. Rosenthal. AMCMC: An R interface for
adaptive MCMC. Computational Statistics and
Data Analysis, 51(12):5467–5470, 2007.
[52] J. S. Rosenthal. Optimal proposal distributions
and adaptive MCMC. In MCMC Handbook.
Chapman & Hall/CRC Press, 2009.
[53] R. Y. Rubinstein and D. P. Kroese. The
Cross-Entropy Method. Springer, 2004.
[54] E. Turro, N. Bochkina, A-M. Hein and
S. Richardson. Bgx: a Bioconductor package for
the Bayesian integrated analysis of Aﬀymetrix
Genechips. BMC Bioinformatics, 8(1):439, 2007.
Contributors
Yves Atchad´e, University of Michigan, 1085 South University, Ann Arbor, 48109, MI, USA
Gersende Fort, LTCI, CNRS–Telecom ParisTech, 46 rue Barrault, 75634 Paris Cedex 13, France
Eric Moulines, LTCI, CNRS–Telecom ParisTech, 46 rue Barrault, 75634 Paris Cedex 13, France
Pierre Priouret, LPMA, Universit´e P. & M. Curie, Boˆıte courrier 188, 75252 Paris Cedex 05, France

3
Auxiliary particle ﬁltering: recent developments
Nick Whiteley and Adam M. Johansen
3.1
Background
3.1.1
State space models
State space models (SSMs) are very popular statistical models for time series. Such models
describe the trajectory of some system of interest as an unobserved E-valued Markov chain,
known as the signal process. Let X1 ∼ν and Xn|(Xn−1 = xn−1) ∼f(·|xn−1) denote this
process. Indirect observations are available via an observation process, {Yn}n∈N. Conditional
upon Xn, Yn is independent of the remainder of the observation and signal processes, with
Yn|(Xn = xn) ∼g(·|xn).
For any sequence {zn}n∈N, we write zi: j =

zi, zi+1, ..., z j

. In numerous applications,
we are interested in estimating, recursively in time, an analytically intractable sequence of
posterior distributions {p ( x1:n| y1:n)}n∈N, of the form
p(x1:n|y1:n) ∝ν(x1)g(y1|x1)
n
Y
k=2
f(xk|xk−1)g(yk|xk).
(3.1)
A great deal has been written about inference for SSMs – see [4, 20, 22] for example –
and their counterparts in continuous time [2]. Filtering, which corresponds to computing
p(xn|y1:n) for each n, is a task of particular interest. Estimation problems in a variety of
scientiﬁc disciplines can naturally be cast as ﬁltering tasks. A canonical example arises in
engineering, where the signal process describes the location and intrinsic parameters of a
physical object, observations arise from a noisy sensor and the task is to reconstruct the
state of the object as accurately as possible, as observations arrive. Other examples arise
from the processing of biological, chemical, seismic, audio, video and ﬁnancial data. In
all these cases the SSM provides a ﬂexible and simple framework in which to describe
the relationship between a physically interpretable or abstract hidden process and observed
data. Further background information on this and related classes of models can be found in
Chapter 1 of this book.
This chapter is concerned with a class of Monte Carlo algorithms which address the
problem of ﬁltering in SSMs by approximating the distributions of interest with a set of
weighted random samples. Attention is focused on an algorithm known as the auxiliary
particle ﬁlter (APF). The APF has seen widespread use in several application areas and a
number of algorithms employing the same underlying mechanism have been developed.
Existing applications include ﬁltering in object tracking and stochastic volatility models,
[44] (in which the APF was introduced), [6]; time-varying autoregressive models for audio

Auxiliary particle ﬁltering: recent developments
53
processing, [1]; exact ﬁltering for diﬀusions, [24]; multi-object tracking, [54] and belief
propagation in graphical models [3].
The remainder of this section introduces a standard approach to the ﬁltering problem,
sequential importance resampling (SIR) and describes the APF. Section 3.2 illustrates the
strong connection between these algorithms, and provides some guidance upon implemen-
tation of the APF. Section 3.3 illustrates a number of extensions which are suggested by
these connections. Section 3.4 describes an elementary technique for variance reduction
when applying the APF to SSMs. Termed the stratiﬁed APF (sAPF), this algorithm uses
low variance sampling mechanisms to assign particles to strata of the state space. The per-
formance of the method is demonstrated in the context of a switching stochastic volatility
model using stock index returns data.
3.1.2
Particle ﬁltering
As described above, a common objective when performing inference in SSMs is the recur-
sive approximation of a sequence of posterior distributions (Eq. (3.1)). There are a small
number of situations in which these distributions can be obtained in closed form (notably
the linear-Gaussian case, which leads to the Kalman ﬁlter). However, in general it is nec-
essary to employ approximations. One of the most versatile approaches is to use sequential
Monte Carlo (SMC) methods. Whilst typically more computationally demanding than alter-
native deterministic techniques (for example see Part II of this volume), SMC methods
are very ﬂexible and have attractive theoretical properties, some of which are discussed
below.
The term particle ﬁltering is often used to describe the approximation of the optimal
ﬁltering equations using SMC techniques. Two common implementations of such algo-
rithms are described in the next two sections. The objective with all such methods is to
approximate, sequentially in time, the distribution of Xn given that Y1:n = y1:n.
The unnormalised posterior distribution p(x1:n, y1:n) given in Eq. (3.1) satisﬁes
p(x1:n, y1:n) = p(x1:n−1, y1:n−1) f(xn|xn−1)g(yn|xn).
Consequently, the posterior p (x1:n|y1:n) satisﬁes the following recursion
p(x1:n|y1:n) = p(x1:n−1|y1:n−1) f(xn|xn−1)g(yn|xn)
p(yn|y1:n−1)
,
(3.2)
where
p(yn|y1:n−1) =
Z
p(xn−1|y1:n−1) f(xn|xn−1)g(yn|xn)dxn−1:n.
In the literature, the recursion satisﬁed by the marginal distribution p(xn|y1:n) is often
presented. It is straightforward to check (by integrating out x1:n−1 in Eq. (3.2)) that
p(xn|y1:n) = g(yn|xn)p(xn|y1:n−1)
p(yn|y1:n−1)
,
(3.3)
where
p(xn|y1:n−1) =
Z
f(xn|xn−1)p(xn−1|y1:n−1)dxn−1.
(3.4)

54
Nick Whiteley and Adam M. Johansen
Recursion (3.4) is known as the prediction step and recursion (3.3) is known as the update
step. However, most particle ﬁltering methods rely on a numerical approximation of recur-
sion (3.2) and not of (3.3)–(3.4). This is the case for the majority of the algorithms described
in this chapter. One exception, which is described in more detail in Section 3.3.1, is
the marginal particle ﬁlter [38] which allows the use of an auxiliary variable technique
admitting a similar interpretation to that discussed in the context of the standard APF below.
Sequential Monte Carlo techniques propagate a collection of weighted samples, termed
particles, from one iteration to the next in such a way that they provide an approximation
of the ﬁltering distribution at each iteration. These collections of particles are used both to
approximate integrals with respect to the distributions of interest (and hence to provide esti-
mates of statistics of interest) and to approximate those distributions themselves, thereby
allowing inference at the next time step. For a more detailed explanation of these algorithms
and an illustration of how most SMC methods may be interpreted as SIR, see [22].
3.1.3
Sequential importance resampling
Sequential importance resampling is one of the simplest SMC approaches to the ﬁltering
problem. In fact, as illustrated in Algorithm 3.1, this technique can be used to sample from
essentially any sequence of distributions deﬁned on a sequence of spaces of strictly increas-
ing dimension. At its nth iteration, Algorithm 3.1 provides an approximation of πn(x1:n).
A crucial step in this algorithm is resampling. This involves duplicating particles with high
weights, discarding particles with low weights and reweighting to preserve the distribu-
tion targeted by the weighted sample. This step prevents a large amount of computational
power being wasted on samples with weights close to zero whilst retaining the consistency
of associated estimators. The simplest scheme, multinomial resampling, achieves this by
drawing N times from the empirical distribution of the weighted particle set (lower variance
alternatives are summarised in [20] and compared in [17]).
Algorithm 3.1 The generic SIR algorithm
At time 1
for i = 1 to N do
Sample X(i)
1 ∼q1(·).
Set W(i)
1 ∝
π1(X(i)
1 )
q1(X(i)
1 ).
end for
Resample
n
X(i)
1 , W(i)
1
o
to obtain
n
X′(i)
1 , 1
N
o
.
At time n ≥2
for i = 1 to N do
Set X(i)
1:n−1 = X′(i)
1:n−1.
Sample X(i)
n ∼qn(·|X(i)
n−1).
Set W(i)
n ∝
πn(X(i)
1:n)
qn(X(i)
n |X(i)
n−1)πn−1(X(i)
1:n−1).
end for
Resample
n
X(i)
1:n, W(i)
n
o
to obtain
n
X′(i)
1:n, 1
N
o
.
In a ﬁltering context, πn(x1:n) = p(x1:n|y1:n) and the expectation of some test function ϕn
with respect to the ﬁltering distribution ϕn =
R
ϕn(xn)p(xn|y1:n)dxn can be estimated using

Auxiliary particle ﬁltering: recent developments
55
bϕN
n,S IR =
N
X
i=1
W(i)
n ϕn(X(i)
n ),
where W(i)
n = wn(X(i)
n−1:n)
.PN
j=1 wn(X( j)
n−1:n) and
wn(xn−1:n) =
πn(x1:n)
qn(xn|xn−1)πn−1(x1:n−1) ∝g(yn|xn)f(xn|xn−1)
qn(xn|xn−1)
.
(3.5)
Note that Eq. (3.5) depends upon only the two most recent components of the particle
trajectory, and thus Algorithm 3.1 can be implemented with storage requirements which do
not increase over time and is suitable for online applications. In fact, SIR can be regarded
as a selection-mutation (genetic-type) algorithm constructed with a precise probabilistic
interpretation. Viewing SIR as a particle approximation of a Feynman–Kac ﬂow [13] allows
many theoretical results to be established.
This simple SIR algorithm involves resampling at every iteration. In general, this may
not be necessary. Whilst resampling permits stability of the algorithm in the long run, each
act of resampling leads to a short-term increase in estimator variance. A common strategy,
dating back at least to [39], is to resample only when the degeneracy of the importance
weights, as measured for example by the coeﬃcient of variation, exceeds some threshold.
Theoretical analyses of algorithms which resample in this manner have been presented in
[16] and [18].
It is commonly accepted that, when designing algorithms for particle ﬁltering, one
should endeavour to ensure that the variance of the importance weights is made as small
as possible. In pursuit of this objective, it is usual to attempt to employ proposal dis-
tributions which are as close as possible to the so-called optimal form, qn(xn|xn−1) ∝
f(xn|xn−1)g(yn|xn) which makes the incremental importance weight independent of xn. In
practice, it is rarely possible to sample from a distribution of the optimal form, although a
number of techniques for obtaining good approximations have been developed.
3.1.4
Auxiliary particle ﬁlters
The use of a well-chosen proposal distribution ensures that knowledge of the current obser-
vation is incorporated into the proposal mechanism and so particles are not moved blindly
into regions of the state space which are extremely unlikely in light of that observation.
However it seems wasteful to resample particles at the end of iteration n−1 prior to looking
at yn. That is, it is natural to ask whether it is possible to employ knowledge about the next
observation before resampling to ensure that particles which are likely to be compatible
with that observation have a good chance of surviving – is it possible to preserve diversity
in the particle set by taking into account the immediate future as well as the present when
carrying out selection? The APF ﬁrst proposed by [44, 45] invoked an auxiliary variable
construction in answer to this question.
The essence of this APF was that the sampling step could be modiﬁed to sample,
for each particle, an auxiliary variable, corresponding to a particle index, according to
a distribution which weights each particle in terms of its compatibility with the coming
observation. A suitable weighting is provided by some bp(yn|xn−1), an approximation of
p(yn|xn−1) =
R
g(yn|xn) f(xn|xn−1)dxn (if the latter is not available analytically). Then the
new state value is sampled as the oﬀspring of the particle indicated by this auxiliary vari-
able. It is straightforward to see that this is equivalent to resampling according to those

56
Nick Whiteley and Adam M. Johansen
weights before carrying out a standard sampling and resampling iteration. In the termi-
nology of [44], an APF which employs the exact p(yn|xn−1) and proposes according to
qn(xn|xn−1) ∝f(xn|xn−1)g(yn|xn) is called ‘fully adapted’.
A similar approach in which the auxiliary weights are combined with those of the
standard weighting was proposed in [5], which involved a single resampling during each
iteration of the algorithm (see Algorithm 3.2).
Algorithm 3.2 Auxiliary particle ﬁlter
At time 1
for i = 1 to N do
Sample X(i)
1 ∼q1(·).
Set eW(i)
1 ∝
g(y1|X(i)
1 )ν(X(i)
1 )
q1(X(i)
1 )
.
end for
At time n ≥2
for i = 1 to N do
Set W(i)
n−1 ∝eW(i)
n−1 × bp ( yn|X(i)
n−1).
end for
Resample
n
X(i)
n−1, W(i)
n−1
o
to obtain
n
X′(i)
n−1, 1
N
o
.
for i = 1 to N do
Set X(i)
n−1 = X′(i)
n−1.
Sample X(i)
n ∼qn(·|X(i)
n−1).
Set eW(i)
n ∝
g(yn|X(i)
n )f(X(i)
n |X(i)
n−1)
bp(yn|X(i)
n−1)qn(X(i)
n |X(i)
n−1).
end for
3.2
Interpretation and implementation
Whilst the APF has seen widespread use, remarkably the ﬁrst asymptotic analyses of the
algorithm have appeared very recently. These analyses provide some signiﬁcant insights
into the performance of the algorithm and emphasise some requirements that a successful
implementation must meet.
3.2.1
The APF as SIR
When one considers the APF as a sequence of weighting and sampling operations it
becomes apparent that it also has an interpretation as a mutation-selection algorithm. In
fact, with a little consideration it is possible to identify the APF as an example of an SIR
algorithm.
It was noted in [33] that the APF described in [5] corresponds to the SIR algorithm
which is obtained by setting
πn(x1:n) = bp(x1:n|y1:n+1) ∝p(x1:n|y1:n)bp(yn+1|xn).
In the SIR interpretation of the APF p(x1:n|y1:n) is not approximated directly, but rather
importance sampling is used to estimate ϕn, with (weighted) samples which target the
importance distribution πn−1(x1:n−1)qn(xn|xn−1) provided by an SIR algorithm. The resulting
estimate is given by

Auxiliary particle ﬁltering: recent developments
57
bϕN
n,APF =
N
X
i=1
eW(i)
n ϕn(X(i)
n ),
where eW(i)
n = ewn(X(i)
n−1:n)
.PN
j=1 ewn(X( j)
n−1:n) and
ewn(xn−1:n) =
p(x1:n|y1:n)
πn−1(x1:n−1)qn(xn|xn−1) ∝
g(yn|xn) f(xn|xn−1)
bp(yn|xn−1)qn(xn|xn−1).
(3.6)
Note that for a fully adapted APF, the importance weights by which estimation are made
are uniform. Only the case in which resampling is carried out once per iteration has been
considered here. Empirically this case has been preferred for many years and one would
intuitively expect it to lead to lower variance estimates. However, it would be straightfor-
ward to apply the same reasoning to the scenario in which resampling is carried out both
before and after auxiliary weighting as in the original implementations (doing this leads to
an SIR algorithm with twice as many distributions as previously but there is no diﬃculty in
constructing such an algorithm).
Theoretical consequences
One of the principal advantages of identifying the APF as a particular type of SIR algo-
rithm is that many detailed theoretical results are available for the latter class of algorithm.
Indeed, many of the results provided in [13], for example, can be applied directly to the
APF via this interpretation. Thus formal convergence results can be obtained without any
additional analysis. Via this route, a central limit theorem (CLT), for example, was shown
to hold in the case of the APF by [33]. In [19] the authors independently established a CLT
for the APF by other means. For both the APF and SIR ﬁlter, the asymptotic variance for
the CLT can be derived, as presented in the following proposition.
Proposition Under the regularity conditions given in [8, Theorem 1] or [13, Section
9.4, pp. 300–306], which prove this result for SIR algorithms (analysis of SIR and other
algorithms can also be found in [18]), we have
√
N

bϕN
n,S IR −ϕn

⇒N

0, σ2
S IR (ϕn)

,
√
N

bϕN
n,APF −ϕn

⇒N

0, σ2
APF (ϕn)

,
where ‘⇒’ denotes convergence in distribution and N

0, σ2
is the zero-mean normal of
variance σ2. Moreover, at time n = 1 we have
σ2
S IR (ϕ1) = σ2
APF (ϕ1) =
Z
p ( x1| y1)2
q1 (x1)
 ϕ1 (x1) −ϕ1
2 dx1,
whereas for n > 1
σ2
S IR (ϕn) =
Z
p(x1|y1:n)2
q1(x1)
∆ϕ1,n(x1)2dx1
+
n−1
X
k=2
Z
p ( x1:k| y1:n)2
p ( x1:k−1| y1:k−1) qk ( xk| xk−1)∆ϕk,n(x1:k)2dx1:k
+
Z
p ( x1:n| y1:n)2
p ( x1:n−1| y1:n−1) qn ( xn| xn−1)
 ϕn (x1:n) −ϕn
2 dx1:n,
(3.7)
where
∆ϕk,n(x1:k) =
Z
ϕn(x1:n)p(xk+1:n|yk+1:n, xk)dxk+1:n −ϕn

58
Nick Whiteley and Adam M. Johansen
and
σ2
APF(ϕn) =
Z
p(x1|y1:n)2
q1(x1)
∆ϕ1,n(x1)2dx1
+
n−1
X
k=2
Z
p(x1:k|y1:n)2
bp(x1:k−1|y1:k)qk(xk|xk−1)∆ϕk,n(x1:k)2dx1:k
+
Z
p(x1:n|y1:n)2
bp(x1:n−1|y1:n)qn(xn|xn−1) (ϕn(x1:n) −¯ϕn)2 dx1:n.
(3.8)
Obtaining asymptotic variance expressions in the same form for SIR and the APF allows
their comparison on a term-by-term basis. This permits some insight into their relative
performance in simple scenarios such as that considered in the following section.
It should, of course, be noted that with a slight change to the conditions (in using the
APF one must correct estimates relative to those provided by the SIR algorithm which it
corresponds to – that is, one integrates a function ewn × ϕn with respect to an SIR algorithm
targeting the auxiliary distributions in order to approximate the expectation of ϕn) essen-
tially any of the results obtained for SIR algorithms can be applied to auxiliary particle
ﬁlters in the same way. Lastly, we note that by choosing bp(yn|xn−1) = 1, one recovers from
the APF the SIR algorithm.
3.2.2
Implications for implementation
It is immediately apparent that, as SIR and the APF are essentially the same algorithm with
a diﬀerent choice of importance weights (in that the only diﬀerence in their implementa-
tion is which importance weights are used for resampling and which for estimation), very
little additional implementation eﬀort is required to develop both variants of an algorithm.
Implementation can be simpliﬁed further by employing a generic SMC library such as [31].
In real-world settings this may be worthwhile as it may not be straightforward to assess,
theoretically, which will provide better estimates at a given computational cost (even when
the APF does allow signiﬁcant reductions in variance to be obtained, it may incur a consid-
erable per-sample cost in the evaluation of a complicated approximation to the predictive
likelihood).
From an implementation point of view, perhaps the most signiﬁcant feature of this
interpretation is that it makes clear the criticality of choosing a bp(yn|xn−1) which is more
diﬀuse than p(yn|xn−1) (as a function of xn−1). For importance sampling schemes in general,
it is well known that a proposal distribution with lighter tails than the target distribution can
lead to an estimator with inﬁnite variance. In the case of the APF the proposal distribution
is deﬁned in terms of bp(yn|xn−1), with the importance weight according to which estimates
are made being Eq. (3.6). It is therefore clear that the popular choice of approximating the
predictive likelihood by the likelihood evaluated at the mode of the transition density is a
dangerous strategy. This is likely to explain the poor performance of APF algorithms based
on this idea which have appeared in the literature.
A number of other generic approaches lead to more conservative implementations. Each
of these techniques may be applicable in some circumstances:
• One simple option is to take
bp(yn|xn−1) ∝
Z
bg(yn|xn)bf(xn|xn−1)dxn,

Auxiliary particle ﬁltering: recent developments
59
with the approximations to the likelihood and transition densities being chosen to
have heavier tails than the true densities and to permit this integral to be evalu-
ated. For some models it is possible to compute the moments of p(xn, yn|xn−1) =
g(yn|xn) f(xn|xn−1) up to second order, conditional on xn−1 [48]. These can then be
used to form a Gaussian approximation of p(xn, yn|xn−1) and thus to p(yn|xn−1), with
the variance adjusted to ensure Eq. (3.6) is bounded.
• The multivariate t distribution provides a ﬂexible family of approximating distri-
butions: approximating p(yn|xn−1) with a t distribution centred at the mode but with
heavier tails than the true predictive likelihood provides a safeguard against excessive
concentration whilst remaining similar in spirit to the simple point-approximation
approach.
• In cases in which the underlying dynamic model is ergodic, the tractability of
the multivariate t distribution provides another strategy. If one approximates the
joint distribution of (Xn−1, Xn, Yn) at stationarity with a multivariate t distribution of
approximately the correct mean and correlation with tails at least as heavy as those
of the true distribution, then one can obtain the marginal distribution of (Xn−1, Yn)
under this approximation analytically – it, too, is a multivariate t distribution. Given
a multivariate t distribution for (Xn−1, Yn), the conditional density (again, under this
approximation) of yn given xn−1 is available in closed form [43].
• In the multimodal case, the situation is more complicated. It may be possible to
employ a mixture of multivariate t distributions in order to approximate complicated
distributions. In very complex settings it may not be practical to approximate the
predictive likelihood accurately.
Whilst it remains sensible to attempt to approximate the optimal (in the sense of
minimising the variance of the importance weights) transition density
qn(xn|xn−1) ∝g(yn|xn) f(xn|xn−1)
and the true predictive likelihood, it is not the case that the APF necessarily out-performs
the SIR algorithm using the same proposal even in this setting. This phenomenon is related
to the fact that the mechanism by which samples are proposed at the current iteration of the
algorithm impacts the variance of estimates made at subsequent time steps.
There are two issues to consider when assessing the asymptotic variance of estimators
provided by SIR or APF-type algorithms. Firstly, as the operation performed in both cases
is essentially importance sampling, there are likely to be particular functions which are
more accurately estimated by each of the algorithms (especially if only a few time steps are
considered). An illustrative example was provided in [33], which we discuss in more detail
below. The other issue is that the APF can only be expected to provide better estimates in
general if, for k < n, p(x1:k|y1:k+1) is closer to p(x1:k|y1:n) than p(x1:k|y1:k) is (consider the
‘importance-weight’ terms in the variance decompositions (3.7) and (3.8) in the case where
the true predictive likelihood is used). This seems likely to be true for the vast majority
of SSMs encountered in practice and so the APF is likely to yield more stable estimates
provided that a good approximation of the predictive likelihood is available.
Analysis of binary state space model
In this section we consider the application of SIR and the APF to a very simple SSM. The
performance of the two algorithms is then compared in terms of asymptotic variance. The

60
Nick Whiteley and Adam M. Johansen
simplicity of the model is such that the asymptotic variances can be evaluated easily in
terms of the model parameters, which in this case directly specify the forgetting properties
of the signal process and the amount of information provided by the observations. The
hope is that, by considering such a simple model, it is possible to gain some insight into the
relative performance of SIR and the APF.
The SSM is speciﬁed as follows:
E = {0, 1},
p(x1 = 0) = 0.5,
p(xn = xn−1) = 1 −δ,
yn ∈E,
p(yn = xn) = 1 −ε.
The test function ϕ(x1:2) := x2 was used, in the ‘full adaptation’ setting, with y1:2 = (0, 1)
q1(x1) : = p(x1|y1),
qn(xn|xn−1) : = p(xn|xn−1, yn),
bp(yn|xn) : =
Z
g(yn|xn) f(xn|xn−1)dxn.
Figure 3.1(a) illustrates the diﬀerence in variance of these two methods as obtained in
[33]. In order to understand this, it’s useful to consider the asymptotic variance of the two
estimators (which follow directly from Eqs. (3.7) and (3.8)):
σ2
S IR (ϕ) =
Z
p ( x1| y1:2)2
q1 (x1)
 Z
ϕ (x1:2) p ( x2| y2, x1) dx2 −ϕ
!2
dx1
+
Z
p ( x1:2| y1:2)2
p ( x1| y1) q2 ( x2| x1) (ϕ (x1:2) −ϕ)2 dx1:2,
σ2
APF(ϕ) =
Z
p(x1|y1:2)2
q1(x1)
 Z
ϕ(x1:2)p(x2|y2, x1)dx2 −ϕ
!2
dx1
+
Z
p(x1:2|y1:2)2
bp(x1|y1:2)q2(x2|x1) (ϕ(x1:2) −ϕ)2 dx1:2.
The ﬁrst terms of these expansions are identical; the diﬀerence between the two algo-
rithms is due entirely to the second term. The SIR term corresponds to the variance of an
importance sampling estimate of ϕ using p(x1|y1)p(x2|x1, y2) as an importance distribution
and self-normalised weights:
Z
p ( x1:2| y1:2)2
p ( x1| y1) p ( x2| x1, y2) (ϕ (x1:2) −ϕ)2 dx1:2.
The APF term corresponds to the variance of ϕ under the ﬁltering distribution
Z
p(x1:2|y1:2)2
p(x1|y1:2)p(x2|x1, y2) (ϕ(x1:2) −ϕ)2 dx1:2 =
Z
p(x1:2|y1:2) (ϕ(x1:2) −ϕ)2 dx1:2.
The latter can be treated equivalently as the variance of a self-normalised importance
sampling estimate using the target distribution as a proposal. Therefore we can appeal to
existing results on self-normalised importance sampling estimators in order to compare the
two algorithms.
It is well known [25, Theorem 3] that the optimal proposal distribution (in the sense
of minimising the variance) for self-normalised importance sampling is ∝|ϕ(x) −ϕ|π(x)

Auxiliary particle ﬁltering: recent developments
61
↓X1|X2 →
0
1
0
(1−δ)(1−ϵ)ϵ
2(1−δ)ϵ(1−ϵ)+δ((1−ϵ2)+ϵ2)
δ(1−ϵ)2
2(1−δ)ϵ(1−ϵ)+δ((1−ϵ2)+ϵ2)
1
δϵ2
2(1−δ)ϵ(1−ϵ)+δ((1−ϵ2)+ϵ2)
(1−δ)(1−ϵ)ϵ
2(1−δ)ϵ(1−ϵ)+δ((1−ϵ2)+ϵ2)
Table 3.1 Target distribution (and APF proposal).
↓X1|X2 →
0
1
0
(1−δ)ϵ(1−ϵ)
ϵ(1−δ)+δ(1−ϵ)
δ(1−ϵ)2
ϵ(1−δ)+δ(1−ϵ)
1
δϵ2
δϵ+(1−δ)(1−ϵ)
(1−δ)ϵ(1−ϵ)
δϵ+(1−δ)(1−ϵ)
Table 3.2 SIR proposal.
where ϕ is the function of interest and π is the target distribution. It is immediately apparent
that the marginal distribution of X1 under the APF proposal distribution is optimal for any
function which depends only upon X2. Thus the distribution of X1 in the SIR expression
would deﬁnitely increase the variance of any estimate if the distribution of X2 was the same
in both cases. However, the marginal distribution of X2 in the two proposal distributions is
diﬀerent and there do exist functions for which that provided by the SIR ﬁlter leads to a
lower variance.
In the case of interest here, i.e. ϕ(x1:2) = x2, we know that the APF has the optimal
marginal distribution for X1 and that the SIR algorithm produces samples with an inferior
distribution for X1. Therefore, any instances in which the SIR algorithm produces lower
variance estimates are due to the distribution of X2. For simplicity, we consider the marginal
distribution of this variable in what follows, noting that in the real scenario the distribution
of X1 will improve the APF’s performance.
The joint distribution of X1, X2 in the target (and APF proposal) is given in Table 3.1
and that for SIR in Table 3.2.
It aids interpretation to notice that ϕ = P
x2 x2p(x2|y1:2) = p(x2 = 1|y1:2). Consequently,
the optimal proposal distribution, qopt(x2) ∝p(x2|y1:2)|ϕ(x2) −ϕ| is uniform over x2
qopt(0) ∝p(x2 = 0|y1:2)|ϕ(0) −ϕ| = (1 −ϕ)ϕ,
qopt(1) ∝p(x2 = 1|y1:2)|ϕ(1) −ϕ| = ϕ(1 −ϕ).
This tells us that the marginal distribution for x2 which minimises the variance of the esti-
mate of this particular integral is uniform over its possible values. The APF generally
places more mass on the state supported by the observation than the SIR ﬁlter. Conse-
quently, the APF only produces a marginal distribution for X2 closer to this optimal form
when the prior would place the majority of its mass on the state which is not supported by
the observation. Even in this setting, the APF can improve things when we obtain unlikely
observations, but may increase the variance when the observation agrees with the prior.
Figure 3.1 illustrates that this mechanism is consistent with what is observed. Fig-
ure 3.1(a) shows the diﬀerence in estimator variance over a range of values of (δ, ϵ);
Fig. 3.1(b) and Fig. 3.1(c) respectively show the marginal probability that X2 = 1 under
the proposal distribution associated with the APF and SIR algorithm, and Fig. 3.1(d) shows
the diﬀerence in L1 distance to the optimal value for the two approaches. It is clear that the
regions in which the SIR algorithm performs well are those in which it provides a much
closer to uniform distribution over X2. Careful inspection reveals that the APF outperforms

62
Nick Whiteley and Adam M. Johansen
(a) Diﬀerence between the APF variance and the SIR
variance
(b) ˜pAPF,2(X2 = 1)
(c) ˜pSIR,2(X2 = 1)
(d) | ˜pAPF,2(X2 = 1) −1/2| −| ˜pSIR,2(X2 = 1) −1/2|
Figure 3.1 Properties of the APF and SIR in the binary, perfect-adaptation setting.
SIR slightly outside of the regions in which it more closely approximates the uniform dis-
tribution over X2. This is due to the distribution of X1 (which inﬂuences the importance
weight) as noted earlier. It should also be noted that it is when δ and ϵ are both small that
one would expect the sub-optimal nature of the SIR distribution over X1 to have the greatest
eﬀect and this is, indeed, where the APF performance is most obviously better.
More generally, one would expect much of the intuition obtained from this simple sce-
nario to apply reasonably directly in more general settings. The APF leads to samples
distributed in a way which is closer to the target distribution; it is possible that for some
test functions the ﬁnal step of the APF does not lead to an optimal marginal distribution but
this distribution is not intended to operate solely as a device for estimating an integral: it
is also used to obtain subsequent distributions and as such, tracking the sequence of target
distributions is of vital importance.
For this reason, minimising incremental variance and otherwise attempting to track
these distributions as faithfully as possible remains our preferred method for designing
APF algorithms. We also feel that, on average (with respect to observation sequences gen-
erated with respect to the true ﬁlter, say), the APF is likely to outperform SIR whenever a
good approximation to the predictive likelihood is available – especially if a broad class of
functions are to be integrated. Note, in particular, the form of the general variance decom-
position: it shows that asymptotically the APF uses distributions of the form ˆp(x1:k−1|y1:k) to
approximate p(x1:k−1|y1:n) where the SIR algorithm uses p(x1:k−1|y1:k−1). It’s approximating
the distribution well that minimises the additional variance which results from these terms,
and the APF does this better than SIR (assuming that, at least on average, p(xk−1,k|y1:k) is
a better proxy for p(xk−1:k|y1:n) than p(xk−1|y1:k−1)p(xk|xk−1, yk), which is the case for any
reasonable situation).

Auxiliary particle ﬁltering: recent developments
63
3.2.3
Other interpretations and developments
Much has been written about the APF in the decade since it was ﬁrst proposed. Some work
based upon the similarity between it and other algorithms precedes that described above.
One of the ﬁrst works to discuss the connections between SIR and the APF was [27].
Closer in spirit to the uniﬁed view presented above was [30], which showed how a number
of algorithms could be interpreted within a common framework. This framework diﬀers
slightly from that presented above, and one of the principal motivations of that approach
was the elimination of an explicit resampling step (which is often viewed as being a rather
unnatural operation in the discrete-time setting). This seems to be the ﬁrst paper to observe
that ‘the APF can be considered as an alternative formulation of the general SIR algorithm
or vice versa’. However, the slightly less standard formulation employed prevents the easy
transferral of results from SIR to the APF which was the primary purpose of [33].
A direct analysis of the particle system underlying the APF was performed recently
[19] using results obtained in [18]. This conﬁrmed the intuitive and empirical results that
resampling once per iteration leads to a lower variance estimate than resampling twice. One
principal component of this work was the determination of the auxiliary weighting function
which minimises the variance of estimates of a particular test function obtained one step
ahead of the current iterations. The ‘second stage weights’ of [19] specify the auxiliary
sequence of distributions associated with the auxiliary particle ﬁlter. The form which they
suggest is optimal for these weights is the following replacement for ˆp(yn+1|xn)
ˆtϕ(xn, yn+1) =
sZ
f(xn+1|xn)2g(yn+1|xn+1)2
q(xn+1|xn)
(ϕn+1(xn+1) −ϕn+1)2dxn+1.
Whilst this is of theoretical interest, it requires the computation of a predictive integral
which is likely to be even more diﬃcult than that required to obtain the predictive like-
lihood. In addition to the practical diﬃculties, it is not clear that it will always be wise
to employ the proposed strategy. When performing any Monte Carlo ﬁltering, the particle
set is used for two purposes at each time instant: to approximate integrals of interest and
the distribution required at the next time step. Using this form of weighting is intended to
optimise the estimate of the integral at the next time step. However, it need not lead to a
good approximation of the distribution itself. Consequently, when this weighting function
is used one may be left with a poorer approximation to the ﬁltering distribution than with
simpler approaches based upon matching only the distribution and not particular test func-
tions. In such cases, use of this approach may lead to poorer estimation in the future. It is
for precisely the same reason that customised proposal distributions tuned for a speciﬁc test
function are not generally used in particle ﬁltering, and thus a more conservative approach
with less adaptation in the proposal mechanism remains sensible.
In subsequent work [10], a criterion independent of the functions of interest was
employed to develop methods for designing adaptive algorithms based upon the auxil-
iary particle ﬁlter. This strategy seeks to minimise the Kullback–Liebler divergence or
χ2-distance between the proposal and target distributions in an adaptive manner (and is
similar in spirit to attempting to get as close to the optimal proposal as possible).
3.3
Applications and extensions
We argue that the innovation of the APF is essentially that, in sampling from a sequence
of distributions using an SIR strategy, it can be advantageous to take account of one-step-
ahead knowledge about the distributions of interest (more general information could, in

64
Nick Whiteley and Adam M. Johansen
principle, be used but it is not easy to envisage realistic scenarios in which this will be
practical). This section summarises some other applications of this principle outside of the
standard particle ﬁltering domain in which it has previously been applied.
3.3.1
Marginal particle ﬁlters
As noted above, most particle ﬁltering methods rely on a numerical approximation of
Eq. (3.2) and not of Eqs. (3.3) and (3.4) even when only the ﬁnal time marginal is of inter-
est. This is due to the diﬃculty associated with evaluating the integral which appears in
Eq. (3.4) explicitly. One possible solution to this approach, proposed in [38], is to approx-
imate these integrals using the particle set itself. Doing this increases the computational
cost considerably, but allows the algorithm to be deﬁned directly on a smaller space than
would otherwise be the case. This is of importance when approximating the derivative of
the optimal ﬁlter in online parameter estimation and optimal control applications [46, 35].
It is also possible to implement an auxiliary particle ﬁlter variant of the marginal particle
ﬁlter, taking the following form (the standard marginal particle ﬁlter is obtained by setting
the auxiliary weighting function ˆp(yn+1|xn) to a constant function):
Algorithm 3.3 Auxiliary Marginal Particle Filter
At time 1
for i = 1 to N do
Sample X(i)
1 ∼q(·).
Set eW(i)
1 ∝
ν

X(i)
1

g

y1|X(i)
1

q(X(i)
1 )
.
end for
At time n ≥2
for i = 1 to N do
Set W(i)
n−1 ∝eW(i)
n−1bp(yn|X(i)
n−1).
end for
Resample
n
X(i)
n−1, W(i)
n−1
o
to obtain
n
X′(i)
n−1, 1
N
o
.
for i = 1 to N do
Sample X(i)
n ∼q( xn| yn, X′(i)
n−1).
Set eW(i)
n ∝
g

yn|X(i)
n
 PN
j=1 W( j)
n−1 f

X(i)
n
X( j)
n−1

PN
j=1 W( j)
n−1q

X(i)
n
yn,X( j)
n−1

bp(yn|X( j)
n−1).
end for
We have presented this algorithm in a form as close as possible to that of the other algo-
rithms described here. It diﬀers in some details from the original formulation. In particular,
we do not assume that the predictive likelihood is obtained by approximating the predictive
distribution with an atom at its mode – it is not necessary to do this and as discussed in
the context of the APF there are some diﬃculties which may arise as a result of such an
approach. As with the APF, it is necessary to use an importance correction when using this
ﬁlter to approximate the ﬁltering distributions.
This approach leads to algorithms with a computational complexity which is O(N2) in
contrast to most particle ﬁlters, which are O(N) algorithms. This would ordinarily be pro-
hibitive, but it was noted in [38] that techniques widely used for the approximate solution of
N-body problems in computational physics and recently applied to statistical learning [29]

Auxiliary particle ﬁltering: recent developments
65
can be applied to this problem for a broad class of likelihood functions, thereby reducing
the complexity to O(N log N) at the cost of a small (and controllable) approximation.
3.3.2
Sequential Monte Carlo samplers
Sequential Monte Carlo samplers are a class of algorithms for sampling iteratively from a
sequence of distributions, denoted by {πn(xn)}n∈N, deﬁned upon a sequence of potentially
arbitrary spaces, {En}n∈N, [14, 15]. The approach involves the application of SIR to a clev-
erly constructed sequence of synthetic distributions which admit the distributions of interest
as marginals. It is consequently straightforward to employ the same strategy as that used
by the APF – see [32] which also illustrates that convergence results for this class of algo-
rithms follow directly. In this context it is not always clear that there is a good choice of
auxiliary distributions, although it is relatively natural in some settings.
The synthetic distributions, {eπn(x1:n)}n∈N, employed by standard SMC samplers are
deﬁned to be
eπn(x1:n) = πn(xn)
n−1
Y
p=1
Lp

xp+1, xp

,
where {Ln}n∈N is a sequence of ‘backward’ Markov kernels from En into En−1. With this
structure, an importance sample fromeπn is obtained by taking the path x1:n−1, a sample from
eπn−1, and extending it with a Markov kernel, Kn, which acts from En−1 into En, providing
samples from eπn−1 × Kn and leading to the importance weight
wn(xn−1:n) =
eπn(x1:n)
eπn−1(x1:n−1)Kn(xn−1, xn) =
πn(xn)Ln−1(xn, xn−1)
πn−1(xn−1)Kn(xn−1, xn).
(3.9)
In many applications, each πn(xn) can only be evaluated pointwise up to a normalising con-
stant and the importance weights deﬁned by Eq. (3.9) are normalised in the same manner
as in the SIR algorithm. Resampling may then be performed.
The optimal (in the sense of minimising the variance of the asymptotic importance
weights if resampling is performed at each iteration) choice of Ln−1 is
Ln−1(xn, xn−1) =
πn−1(xn−1)K(xn−1, xn)
R
πn−1(x′
n−1)K(x′
n−1, xn)dx′
n−1
,
which produces a sampler equivalent to one deﬁned only on the marginal spaces of inter-
est. In practice, it is not generally possible to use the optimal auxiliary kernels and good
approximations to this optimal form are required in order to obtain samplers with good
variance properties.
If one wishes to sample from a sequence of distributions {πn}n∈N then an alternative
to directly implementing an SMC sampler which targets this sequence of distributions,
is to employ an auxiliary sequence of distributions, {µn}n∈N, and an importance sampling
correction (with weights ewn(xn) = πn(xn)/µn(xn)) to provide estimates. This is very much in
the spirit of the APF. Such a strategy was termed auxiliary SMC (ASMC) in [32]. Like the
APF, the objective is to maintain a more diverse particle set by using as much information
as possible before, rather than after, resampling.
Resample-move: inverting sampling and resampling
As has been previously noted [15], in a setting in which every iteration shares a common
state space, En = E, and in which an MCMC kernel of invariant distribution πn is employed

66
Nick Whiteley and Adam M. Johansen
as the proposal, making use of the auxiliary kernel
Ln−1(xn, xn−1) = πn(xn−1)Kn(xn−1, xn)
πn(xn)
,
the importance weights are simply wn(xn−1, xn) = πn(xn−1)/πn−1(xn−1). In addition to its
simplicity, this expression has the interesting property that the weight is independent of the
proposed state, xn.
It is possible to interpret this approach as correcting for the discrepancy between the
previous and present distributions entirely by importance weighting with the application of
an MCMC kernel of the appropriate distribution simply serving to improve the diversity
of the sample. It is intuitively clear that one should apply the importance weighting and
resample before proposing new states in the interests of maximising sample diversity. This
has been observed previously. Indeed doing so leads to algorithms with the same struc-
ture as the resample-move (RM) particle ﬁltering algorithm [26]. By making the following
identiﬁcations
µn(xn) = πn+1(xn),
Ln−1(xn, xn−1) = µn−1(xn−1)Kn(xn−1, xn)
µn−1(xn)
= πn(xn−1)Kn(xn−1, xn)
πn(xn)
,
wn(xn−1:n) = µn(xn)
µn−1(xn) = πn+1(xn)
πn(xn) ,
ewn(xn) = µn−1(xn)/µn(xn) = πn(xn)/πn+1(xn),
it is possible to cast this approach into the form of an ASMC sampler. This for-
mal representation allows existing theoretical results to be applied to both RM and its
generalisations.
Filtering piecewise-deterministic processes
The SMC samplers framework was employed by [53] to provide ﬁltering estimates for
a class of continuous-time processes. In addition to providing an example of the class of
algorithms which are described above, this approach also illustrates that SMC samplers and
their auxiliary counterparts can provide useful extensions of SIR-type algorithms in time
series analysis.
Piecewise-deterministic processes (PDPs) are a class of stochastic processes whose
sample paths, {ζt}t≥0 evolve deterministically in continuous time between a sequence of
random times {τj} j∈N, at which the path jumps to new, random values {θ j}j∈N. The prior law
of the (τj, θj) is typically speciﬁed by a Markov kernel with density f(θn,j, τn, j|θn,j−1, τn,j−1).
Filtering for partially observed PDP models involves computing a sequence of pos-
terior distributions given observations {Yn}n∈N. In object-tracking applications, [28], the
observations may be related to the PDP trajectory by Yn = H(ζtn, Un), where Un is a noise
disturbance, H is some non-linear function and {tn}n∈N is an increasing sequence of obser-
vation times. In ﬁnancial applications such as the pricing of reinsurance [12] and options
[7], each Yn is the restriction to the interval (tn−1, tn] of a Cox process with conditional
intensity (ζt)t∈(tn−1,tn]. In general, the observation model is speciﬁed by a likelihood function
g(yn|ζ(tn−1,tn]).
The nth posterior πn(kn, θn,0:kn, τn,1;kn|y1:n), is a distribution over
En =
∞
]
k=0
{k} × Θk+1 × Tn,k,

Auxiliary particle ﬁltering: recent developments
67
where Θ ⊂Rd is a parameter space, Tn,k = {τn,1:kn : 0 ≤τn,1 < ... < τn,kn ≤tn} and U
indicates disjoint union. The posterior distribution is speciﬁed by
πn(kn, θn,0:kn, τn,1;kn|y1:n) ∝ν(θn,0)S (tn, τn,kn)
kn
Y
j=1
f(θn, j, τn,j|θn, j−1, τn, j−1)
n
Y
p=1
g(yn|ζ(tn−1,tn]),
with the convention τn,0 = 0 and where S (tn, τn,kn) is the survivor function associated
with the prior distribution over inter-jump times for the interval [0, tn]. The SMC samplers
framework is applied to approximate the distributions of interest, using a proposal kernel
consisting of a mixture of moves which extend each particle from En−1 to En by adjusting
recent jump-time/parameter pairs and adding new ones. An auxiliary scheme for ﬁltering
can be obtained by selecting the auxiliary distribution µn to be:
µn(kn, θn,0:kn, τn,1;kn) ∝Vn(θn,kn, τn,kn)πn(kn, θn,0:kn, τn,1;kn|y1:n),
where Vn(θn,kn, τn,kn) is a non-negative potential function which provides information about
yn+1. This can be done by approximating the predictive likelihood in the same manner as in
the discrete-time case, although some care is required as there may be one or more jumps
between observations and these must be considered when approximating that predictive
likelihood. This strategy was seen to perform well in [53].
3.3.3
The probability hypothesis density ﬁlter
An unusual application of ideas from the APF can be found in the area of multiple-object
tracking. This is an inference task in which one seeks to estimate, in an online manner, the
time-varying number and positions of a collection of hidden objects, given a sequence of
noisy observations. What makes this task especially diﬃcult is that it is not known which (if
any) of the observations arise from which hidden objects. In many applications, the hidden
objects are vehicles and the observations arise from sensor measurements, but many other
problems in diverse application areas such as communications engineering, biology, audio
and music processing can be cast in the same framework. Some examples are noted in [54].
See also Part IV of this volume.
In this scenario, one option is to represent the collection of hidden objects at a single
time step as a spatial Poisson process with some inhomogeneous intensity measure. The
intensity measure determines the expected number of objects within any region of the state
space. Given this representation, the problem of tracking a large collection of objects is
reduced to the problem of approximating, sequentially in time, this intensity measure. The
use of SMC methods to approximate this measure has been suggested several times and an
auxiliary-particle-ﬁlter-type implementation has recently been developed.
In principle, ﬁltering for a multi-object tracking model involves computing a sequence
of distributions with essentially the same form as Eq. (3.1). Here, E is E = U∞
k=0 Xk, where
X ⊂Rd is the state space of an individual object: each Xn = Xn,1:kn comprises a ran-
dom number, kn, of points, each in X, and can be regarded as a spatial point process (see
[42, 50, 11] for background theory). We refer the reader to Chapter 10 of this volume for
further information, but essential to the discussion below is the following concept. The ﬁrst
moment of the distribution of a point process may be speciﬁed in terms of an intensity
function, α : E →R+, so that
E[N(A)] =
Z
A
αn(x)dx,
A ∈B(X),

68
Nick Whiteley and Adam M. Johansen
where N(A) is the number of points of X which are in the set A and B(X) is the Borel
σ-algebra on X.
A simple multi-object model has the following structure. The hidden objects present
at time n −1 each survive to time n with location dependent probability pS (xn−1). Each
of the surviving objects evolves independently according to a Markov kernel with density
f(xn|xn−1). New objects appear according to a Poisson process with intensity function γ(xn).
Each of the surviving and new objects produces an observation with distribution g(y|x). In
addition to these detections, spurious observations, termed ‘clutter’, arise from an indepen-
dent Poisson process with intensity κ(y). The observation set at time n therefore consists of
a random number of points, Yn = Yn,1:Mn. Crucially, it is not known which of the points of
Yn arise from hidden objects and which are clutter.
Performing ﬁltering when E = U∞
k=0 Xk is practically very diﬃcult due to the high
and variable dimensionality of this space. The probability hypothesis density (PHD) ﬁlter,
[41], approximates the optimal ﬁlter for this problem by assuming that the state process is
a-posteriori Poisson (and hence fully characterized by its ﬁrst moment) and characterising
the intensity of that process.
For the model described above, the PHD ﬁltering scheme yields the following
prediction/update recursion for intensity functions
αn(xn) =
Z
X
f(xn|xn−1)pS (xn−1)˘αn−1(xn−1)dxn−1 + γ(xn),
(3.10)
˘αn(xn) =
mn
X
r=1
g(yn,r|xn)
Zn,r
αn(xn),
(3.11)
where for r = 1, 2, ..., mn, Zn,r =
R
E g(yn,r|x)αn(x)dx + κ(yn,r). In this notation, αn(x) and
˘αn(x) are respectively termed the predicted and updated intensities at time n. The problem
is then to compute the recursion (3.10)–(3.11) for a given observation sequence, with esti-
mates of kn and xn,1:kn made from characteristics of ˘αn. For many models this is intractable,
due to the integrals involved and because ˘αn is typically of mixture form with a number of
components which is increasing in n. Some degree of approximation is therefore required.
Sequential Monte Carlo methods may be employed to approximate the sequence of
intensity functions {˘αn(xn)}n∈N, [55, 49, 52, 34, 9]. In contrast to the case of particle ﬁlters
which approximate probability distributions, it is necessary for the collection of weighted
samples used here to characterise the total mass of the intensity function in addition to its
form. Akin to the APF, an auxiliary SMC implementation has recently been proposed in
[54]. Empirical results demonstrate that the PHD recursion is particularly well suited to an
auxiliary SMC approach. As in the APF, this involves resampling from a particle set which
has been re-weighted by a potential function.
In outline, this approach introduces an extended state space X′ = X ∪{s}, where s is
an isolated ‘source’ point which does not belong to X, then deﬁnes an intensity function
denoted βn(xn−1:n, r) on X′ × X × {1, ..., mn} as follows:
βn(xn−1:n, r) = g(yn,r|xn)
Zn,r
"
f(xn|xn−1)pS (xn−1)˘αn−1(xn−1)IX(xn−1) + γ(xn)δs(xn−1)
#
. (3.12)
Note that
˘αn(xn) =
mn
X
r=1
Z
X′ βn(xn−1:n, r)dxn−1.
(3.13)

Auxiliary particle ﬁltering: recent developments
69
The algorithm of [54] uses IS to target Eq. (3.12) and thus yields a particle approximation
of ˘αn(xn) due to Eq. (3.13).
Assume that there is available a particle approximation of ˘αn−1(xn−1), denoted by
˘αN
n−1(xn−1). N samples are drawn from some distribution qn(r) over {1, 2, ..., mn}, yielding
{R(i)
n }N
i=1. For each i, X(i)
n−1 is then drawn from
πN
n−1,R(i)(xn−1) ∝bp(yn,R(i)|xn−1)
h
˘αN
n−1(xn−1)IX(xn−1) + δs(xn−1)
i
,
(3.14)
bp(yn,r|xn−1) being an approximation of p(yn,r|xn−1), which is itself deﬁned by
p(yn,r|xn−1) = IX(xn−1)pS (xn−1)
Z
X
g(yn,r|xn)f(xn|xn−1)dxn + I{s}(xn−1)
Z
X
g(yn,r|xn)γ(xn)dxn.
For each i, X(i)
n is then drawn from a kernel qn(·|X(i)
n−1, R(i)
n ). The importance weight which
targets Eq. (3.12) is given by
ewn(xn−1:n, r) ∝g(yn,r|xn)[ f(xn|xn−1)pS (xn−1)IX(xn−1) + γ(xn)I{s}(xn−1)]
qn,r(xn|xn−1)πn−1,r(xn−1)qn(r)
,
with each normalising constant Zn,r also estimated by IS, much as in SMC algorithms for
SSMs. The result is a particle approximation of ˘αn(xn). Reference [54] also shows how to
choose qn(r) in an optimal fashion.
The connection with the APF is evident from the form of Eq. (3.14): drawing from
this auxiliary distribution involves resampling from the existing particle set re-weighted
by a potential function incorporating knowledge of the next observation. As demonstrated
empirically in [54], compared to non-auxiliary SMC implementations, this method can
result in importance weights of lower variance and more reliable estimates.
3.4
Further stratifying the APF
It is common knowledge that the use of multinomial resampling in a particle ﬁlter unnec-
essarily increases the Monte Carlo variance of the associated estimators and that the use
of residual, systematic or stratiﬁed approaches can signiﬁcantly reduce that variance [17].
This is also true in the case of the APF and one should always employ minimum variance
resampling strategies. Under some circumstances it may be possible to achieve a further
variance reduction in the APF.
Consider again the SSM from Section 3.1.1. Let (E j)M
j=1 denote a partition of E. Intro-
ducing an auxiliary stratum-indicator variable, sn = PM
j=1 jIE j(xn), we redeﬁne the SSM
on a higher-dimensional space, with the signal process being E × {1, 2, ..., M}-valued, with
transition kernel
r(xn, sn|xn−1, sn−1) = r(xn|sn, xn−1)r(sn|xn−1),
where
r(xn|sn, xn−1) ∝IEsn(xn) f(xn|xn−1),
r(sn|xn−1) =
Z
Esn
f(xn|xn−1)dxn.
The initial distribution of the extended chain is deﬁned in a similar manner and the likeli-
hood function remains unchanged. The posterior distributions for the extended model then
obey the recursion
p(x1:n, s1:n|y1:n) ∝g(yn|xn)r(xn|sn, xn−1)r(sn|xn−1)p(x1:n−1, s1:n−1|y1:n−1).
(3.15)

70
Nick Whiteley and Adam M. Johansen
Note that the marginal distribution of x1:n in Eq. (3.15) coincides with the original model.
As in the SIR interpretation of the APF, we then construct an auxiliary sequence of
distributions, {π(x1:n−1, s1:n)}n∈N, which will be targeted with an SIR algorithm, where
π(x1:n−1, s1:n) ∝bp(yn|sn, xn−1)br(sn|xn−1)p(x1:n−1, s1:n−1|y1:n−1).
(3.16)
The key feature of Eq. (3.16) is that the resampling step of the corresponding SIR algorithm
will select pairs of previous state values xn−1 and current strata sn. This assignment can
be performed with a low variance resampling mechanism. The corresponding algorithm,
which we refer to as the stratiﬁed auxiliary particle ﬁlter (sAPF) is given below.
Algorithm 3.4 Stratiﬁed auxiliary particle ﬁlter
At time 1
for i = 1 to N do
Sample X(i)
1 ∼q1(·).
Set eW(i)
1 ∝
g(y1|X(i)
1 )ν(X(i)
1 )
q1(X(i)
1 )
.
end for
At time n ≥2
for i = 1 to N do
for j = 1 to M do
Set W(i,j)
n−1 ∝eW(i)
n−1 × bp(yn|X(i)
n−1, sn = j)br(sn = j|X(i)
n−1).
end for
end for
Resample
n
X(i)
n−1, j, W(i,j)
n−1
o
(i,j)∈{1,...,N}×{1,...,M} to obtain
n
X′(i)
n−1, S (i)
n , 1
N
o
.
for i = 1 to N do
Set X(i)
n−1 = X′(i)
n−1.
Sample X(i)
n ∼qn(·|X(i)
n−1, S (i)
n ).
Set eW(i)
n ∝
g(yn|X(i)
n )f(X(i)
n |X(i)
n−1)
bp(yn|X(i)
n−1,S (i)
n )br(S (i)
n |X(i)
n−1)qn(X(i)
n |X(i)
n−1,S (i)
n ).
end for
For each i, we ﬁrst draw each X(i)
n |x(i)
n−1, s(i)
n ∼qn(·|x(i)
n−1, s(i)
n ). Then, instead of randomly
sampling a value s(i)
n+1, we evaluate one importance weight for every possible value of sn+1,
resulting in a collection of N × M weighted sample points. The resampling step of the SIR
algorithm then draws N times from the resulting distribution on {1, 2, ..., N} × {1, 2, ..., M}.
The method of [36], proposed in the context of a particular class of tracking problems can
be viewed as a special case of the proposed algorithm. However, [36] did not employ low-
variance resampling schemes, which is, as will be shown below, the key to obtaining both
a variance reduction and a decrease in computational cost.
The importance weight which targets p(x1:n, s1:n|y1:n) (i.e. the analogue of Eq. (3.6)) is
then:
ewn(xn−1:n, sn) ∝
g(yn|xn) f(xn|xn−1)
bp(yn|sn, xn−1)br(sn|xn−1)qn(xn|sn, xn−1).
This eﬀectively assigns both a parent particle and a stratum to each oﬀspring. Crucially, this
assignment can be performed with a low-variance resampling mechanism. This approach is
especially of interest in the context of switching SSMs, where the state space has a natural
partition structure by deﬁnition. We consider the application to such models below. First,

Auxiliary particle ﬁltering: recent developments
71
we consider the eﬀect of the above sampling scheme on the conditional variance of the
resulting estimates.
3.4.1
Reduction in conditional variance
The following section illustrates the principal beneﬁt of the proposed approach: a signif-
icant reduction in computational cost in those situations in which a natural stratiﬁcation
exists. This section illustrates that incorporating this additional stratiﬁcation cannot make
things worse in the sense that the variance of resulting estimators will be at most the same
as those obtained with a non-stratiﬁed variant of those estimators. For simplicity we com-
pare the performance of the sAPF to that of the APF in terms of the conditional variance
arising from a single act of resampling and assigning particles to strata.
There are several ways in which one might go about using a low-variance mechanism
in the resampling step of Algorithm 3.4. All the methods described in [17] are applicable
and in this section we consider one way of using the stratiﬁed resampling mechanism,
see [37, 23]. This method uses a form of inversion sampling to draw N samples from
the distribution deﬁned by the particle set. Inversion sampling itself involves generating
U[0, 1] random variates and passing them through the generalised inverse of the target
distribution function [47]. The stratiﬁed resampling scheme is so-named because it involves
partitioning [0, 1] into N strata of length 1/N. A single uniform variate is then drawn on
each sub-interval and passed through the inverse of the CDF, see [17] for further details.
We next consider how the stratiﬁed resampling mechanism could be used in the sAPF
and how it would be used in the regular APF. It should be noted that the scheme described
below is not the only way in which stratiﬁed resampling can be applied within the sAPF.
Indeed there are alternatives which may be of even lower variance. However, the scheme
described below is simple enough to permit direct analysis, providing some insight into
how variance reduction can be achieved.
As part of the discussion which follows, we need some notation to indicate when the
stratiﬁed resampling scheme is used and to specify the resulting random variables.
For a collection of random variables {X(i)}N
i=1, and a suitable probability distribution
µ, we use the notation {X(i)}N
i=1
ss∼µ to indicate that the samples {X(i)}N
i=1 are generated
using the stratiﬁed resampling mechanism targeting µ. Consider a collection of weighted
samples {X(i), W(i)}N
i=1 such that PN
i=1 W(i) = 1 and the associated empirical probability
distribution
N
X
i=1
W(i)δX(i)(dx).
Resampling N times from this distribution can be interpreted as generating, via some mech-
anism, a set of N ancestors, with A(i) denoting the ancestor of the ith particle so that the
resulting empirical distribution can be written as
1
N
N
X
i=1
δX(A(i))(dx),
i.e. in relation to the notation of Algorithm 3.4, X′(i) ≡X(A(i)). It will also be convenient to
specify the number of replicates of each existing sample and a cumulative count of these

72
Nick Whiteley and Adam M. Johansen
APF
sAPF
{A(i)}N
i=1
ss∼PN
i=1 W(i)δi(da).
{A(i)}N
i=1
ss∼PN
i=1 W(i)δi(da).
for i = 1 to N
for i = 1 to N
S (i) ∼PM
j=1 W( j|A(i))δ j(ds).
if Ni > 0
end for
{S (j)}
N∗
i
j=N∗
i−1+1
ss∼PM
j=1 W(j|i)δ j(ds).
end if
end for
Table 3.3 Resampling and assignment to strata for the APF and sAPF algorithms, both employing stratiﬁed
sampling. Here, the APF uses the low-variance sampling mechanism in assigning ancestors. By contrast, the
sAPF uses the low-variance mechanism in both assigning ancestors and strata.
replicates, so for i ∈{1, ..., N} we deﬁne
Ni =
N
X
j=1
I[A( j)=i],
N∗
i =
iX
j=1
Nj.
Finally, to connect with the notation of Algorithm 3.4 we also set
W(i) =
M
X
j=1
W(i, j),
W(j|i) =
W(i, j)
PM
j=1 W(i, j) ,
where the time index has been suppressed (as it is throughout this section) for clarity.
With these conventions in hand, we consider a set, {X(i), W(i)}N
i=1, of weighted samples
resulting from some iteration of an SMC algorithm and conditional upon this weighted sam-
ple set, analyse the variance arising from resampling the particles and assigning particles
to strata.
Table 3.3 shows how an algorithm which employs the stratiﬁed sampling mechanism in
both the resampling and strata-selection steps can be compared with the standard algorithm.
Figure 3.2 shows a graphical representation of the procedures.
Given the weighted sample set {X(i), W(i)}N
i=1, procedures of Table 3.3 both result in a
set of ancestor and strata indicators. For a function ϕ : {1, ..., M} × E →R we write bϕN
sAPF
and bϕN
APF for the estimators of the form
1
N
N
X
i=1
ϕ(S (i), X(A(i)))
which arise from the sAPF and the APF, respectively. The following proposition establishes
that the sAPF scheme of Table 3.3 does indeed yield a reduction in conditional variance
over the APF scheme.
Proposition 3.1 For an integrable function ϕ : {1, ..., M} × E →R and for all N,
Var{bϕN
sAPF|F } ≤Var{bϕN
APF|F },
where F = σ({X(i), W(i)}N
i=1).

Auxiliary particle ﬁltering: recent developments
73
Figure 3.2 An illustration of stratiﬁed resampling within the APF (a) and the sAPF (b) with N = 5 particles and
M = 2 strata. For the APF, each box corresponds to an existing particle; for the sAPF, each box corresponds to an
existing particle/stratum pair. In both cases, the area of each box is proportional to the corresponding weight and
a number of particles proportional to the area of each box is sampled with the appropriate parameters. In the case
of the APF the boxes have heights proportional to the weights of the particles and constant width: only the parent
particle is assigned by the low-variance sampling mechanism. In the case of the sAPF the height of the boxes
remains proportional to the weight of the particle, W(i) = P
j W(i,j), but the assignment of both parent and stratum
is performed using the low-variance sampling mechanism.
Proof The variances are ﬁrst decomposed in the following manner
Var{bϕN
sAPF|F } = E[Var{bϕN
sAPF|G}|F ] + Var{E[bϕN
sAPF|G]|F },
(3.17)
Var{bϕN
APF|F } = E[Var{bϕN
APF|G}|F ] + Var{E[bϕN
APF|G]|F },
(3.18)
where G = F ∨σ({A(i)}N
i=1). Comparison is then performed term-by-term. First consider
the conditional expectations
E[bϕN
sAPF|G] = 1
N
X
{i:Ni>0}
Ni
X
j=1
Z
j
Ni
j−1
Ni
Niϕ(Dinv
i (u), X(i))du
= 1
N
N
X
{i:Ni>0}
Ni
Z 1
0
ϕ(Dinv
i (u), X(i))du = E[bϕN
APF|G],
(3.19)
where Dinv
i
is the generalised inverse CDF associated with PM
j=1 W( j|i)δ j.

74
Nick Whiteley and Adam M. Johansen
Next consider the conditional variances. First note that for both the sAPF and APF the
{S (i)}N
i=1 are conditionally independent given {A(i)}N
i=1. Hence
Var{bϕN
sAPF|G} = 1
N2
N
X
i=1
E[[ϕ(S (i), X(A(i)))]2|G] −1
N2
N
X
i=1
[E[ϕ(S (i), X(A(i)))|G]]2
= 1
N2
N
X
i=1
Ni
Z 1
0
[ϕ(Dinv
i (u), X(i))]2du −1
N2
X
{i:Ni>0}
Ni
X
j=1

Z
j
Ni
j−1
Ni
Niϕ(Dinv
i (u), X(i))du

2
,
(3.20)
whereas for the APF
Var{bϕN
APF|G} = 1
N2
N
X
i=1
Ni
Z 1
0
[ϕ(Dinv
i (u), X(i))]2du −1
N2
N
X
i=1
Ni
"Z 1
0
ϕ(Dinv
i (u), X(i))du
#2
.
(3.21)
Applying Jensen’s inequality to the second term in Eqs. (3.20) and (3.21) shows that
Var{bϕN
sAPF|G} ≤Var{bϕN
APF|G}.
(3.22)
The result follows upon combining Eqs. (3.17), (3.18), (3.19) and (3.22).
□
It is stressed that Proposition 3.1 deals with the conditional variance, given
{X(i), W(i)}N
i=1. This gives some insight into the performance of the algorithm, but ideally
one would like to conﬁrm a reduction in the unconditional variance. In the case that resid-
ual resampling is used, it may be possible to apply similar techniques to those used in [8]
in order to establish a reduction in unconditional asymptotic variance.
3.4.2
Application to switching state space models
Switching SSMs are a particular class of models in which the state of the unobserved
process can be expressed in terms of two components, Xn = (S n, θn), with sn valued in
{1, 2, ..., M} and θn valued in some space Θ, typically a subset of Rd. The corresponding
state space is of the form
E = {1, . . . , M} × Θ =
M
]
j=1
{j} × Θ,
so the state space has a natural partition structure. Note that E j = {j} × Θ so automatically
we have sn = PM
j=1 jIE j(xn) as before.
We will focus on models of the form
p(θ1:n, s1:n|y1:n) ∝g(yn|θn)r(θn|θn−1, sn)r(sn|sn−1)p(θ1:n−1, s1:n−1|y1:n−1),
which arise in a wide variety of applications, including target tracking, [21]; audio signal
processing, [1]; and econometrics, [6]. Note that due to the structure of the model we have
r(sn|xn−1) = r(sn|sn−1).
In this model sn is a latent state, which is not observed. The model of the hidden pro-
cess (θn)n∈N can be interpreted as switching between M distinct dynamic regimes, with

Auxiliary particle ﬁltering: recent developments
75
transitions between these regimes governed a priori by the transition kernel r(sn|sn−1). This
allows a larger degree of ﬂexibility than in standard SSMs and is especially useful for
modelling time series which exhibit temporal heterogeneity.
In the conditionally linear-Gaussian case, given a trajectory s1:n it is possible to compute
p(θ1:n|y1:n, s1:n) using the Kalman ﬁlter and thus SMC algorithms for ﬁltering can be devised
in which the θ components of the state are integrated out analytically, see [21]. We do
not assume such a structure, although the methods described above are applicable in that
case. The sAPF algorithm for the speciﬁc case of switching state space models is given
below.
Algorithm 3.5 sAPF for switching state space models
At time 1
for i = 1 to N do
Sample (θ(i)
1 , S (i)
1 ) ∼q1(·).
Set eW(i)
1 ∝
g(y1|θ(i)
1 )ν(θ(i)
1 ,S (i)
1 )
q1(θ(i)
1 ,S (i)
1 )
.
end for
At time n ≥2
for i = 1 to N do
for j = 1 to M do
Set W(i, j)
n−1 ∝eW(i)
n−1 × bp(yn|θ(i)
n−1, sn = j)r(sn = j|S (i)
n−1).
end for
end for
Resample
n
θ(i)
n−1, j, W(i, j)
n−1
o
(i, j)∈{1,...,N}×{1,...,M} to obtain
n
θ′(i)
n−1, S (i)
n , 1
N
o
.
for i = 1 to N do
Set θ(i)
n−1 = θ′(i)
n−1.
Sample θ(i)
n ∼qn(·|θ(i)
n−1, S (i)
n ).
Set eW(i)
n ∝
g(yn|θ(i)
n )r(θ(i)
n |θ(i)
n−1,S (i)
n )
bp(yn|θ(i)
n−1,S (i)
n )qn(θ(i)
n |θ(i)
n−1,S (i)
n ).
end for
We next consider application of the sAPF to a Markov-switching stochastic volatility
(SV) model, as studied in [6]. Stochastic volatility models with switching regime allow
occasional discrete shifts in the parameter determining the level of the log volatility of
ﬁnancial returns. They have been advocated as a means by which to avoid overestimation
of volatility persistence, see [51] and references therein.
The log-volatility process {θn}n∈N and observations {Yn}n∈N obey the following equa-
tions
θn = φθn−1 + αsn + ζn,
Yn = ϵn exp(θn/2),
where ζn is an independent N(0, σ2
θ) random variable and ϵn is an independent N(0, 1)
random variable. The parameter φ is the persistence of volatility shocks, {αj}M
j=1 are the
log-volatility levels and sn is the latent regime indicator, so that
αsn = γ1 +
M
X
j=2
γ jI[sn≥j],

76
Nick Whiteley and Adam M. Johansen
where {γ j}M
j=1 are log-volatility increments. The prior transition kernel r(sn|sn−1) is speciﬁed
by a stochastic matrix with entry pkl being the probability of a transition from state k to
state l.
In order to construct the potential function bp(yn|θn−1, sn) and the proposal distribution
qn(θn|θn−1, sn) we employ a slight modiﬁcation of the technique proposed in [44] for stan-
dard SV models. The idea is to exploit the log-concavity of the likelihood function and form
an approximation of g(yn|θn) by taking a ﬁrst-order Taylor expansion of the log-likelihood
about the conditional mean of θn. With an abuse of notation we write ¯θn := φθn−1 +αsn. The
approximation of the likelihood is then speciﬁed by
logbg(yn|θn; θn−1, sn) = log g(yn|¯θn) + (θn −¯θn) · ∂
∂θ log g(yn|θ)
¯θn
.
(3.23)
We then choose
qn(θn|sn, θn−1) ∝bg(yn|θn; θn−1, sn)r(θn|θn−1, sn),
which is a Gaussian density, N(µqn, σ2
qn), with parameters
µqn = φθn−1 + αsn + σ2
θ
2
h
y2
n exp(−φθn−1 −αsn) −1
i
,
σ2
qn = σ2
θ.
Furthermore, we employ the following approximation of the predictive likelihood
bp(yn|θn−1, sn) ∝
Z
bg(yn|θn; θn−1, sn)r(θn|θn−1, sn)dθn
∝exp

1
2σ2
θ
(µ2
qn −(φθn−1 + αsn)2)

× exp
 
−y2
n
2 exp(−φθn−1 −αsn)(1 + φθn−1 + αsn)
!
.
The importance weight is given by
ewn(θn−1:n, sn) ∝
g(yn|θn)
bg(yn|θn; θn−1, sn)
∝exp
(
−y2
n
2
h
exp(−θn) −exp(−¯θn)[1 −(θn −¯θn)]
i)
.
Due to the fact that log g(yn|θ) is concave as a function of θ and from the deﬁnition (3.23),
the importance weight ewn(θn−1:n, sn) is bounded above.
The Bovespa Index (IBOVESPA) is an index of approximately 50 stocks traded on the
S˜ao Paulo Stock Exchange. Figure 3.3 shows weekday returns on the IBOVESPA index for
the period 1/2/97–1/15/01. As highlighted in [6], during this period there occurred several
international currency events which aﬀected Latin American markets, generating higher
levels of uncertainty and consequently higher levels of volatility. These events are listed in
Table 3.4 and are indicated by the vertical dotted lines in Fig. 3.3. This dataset was analysed
in [40], where an SMC algorithm was used to perform ﬁltering whilst simultaneously esti-
mating static parameters. We concentrate on the ﬁltering problem and set static parameters
to pre-determined values.

Auxiliary particle ﬁltering: recent developments
77
07/02/1997
Thailand devalues the Baht by as much as 20%
08/11/1997
IMF and Thailand set a rescue agreement
10/23/1997
Hong Kong’s stock index falls 10.4%
South Korean Won starts to weaken
12/02/1997
IMF and South Korea set a bailout agreement
06/01/1998
Russia’s stock market crashes
06/20/1998
IMF gives ﬁnal approval to a loan package to Russia
08/19/1998
Russia oﬃcially falls into default
10/09/1998
IMF and World Bank joint meeting to discuss the economic crisis
The Federal Reserve cuts interest rates
01/15/1999
The Brazilian government allows its currency, the Real,
to ﬂoat freely by lifting exchange controls
02/02/1999
Arminio Fraga is named President of Brazil’s Central Bank
Table 3.4 Events which impacted Latin American markets [6].
1/2/97
7/2/97
12/2/97
6/1/98
1/15/99
1/15/01
−0.2
−0.1
0
0.1
0.2
1/2/97
7/2/97
12/2/97
6/1/98
1/15/99
1/15/01
1
2
(a)
(b)
Figure 3.3 Stochastic volatility model. (a) Daily returns on the IBOVESPA index from February 1997 to January
2001. (b) MAP one-step-ahead prediction of the switching state sn. State 2 is the high-volatility regime.
The sAPF was compared to a standard APF for this model, with the latter employing
the same approximation of the likelihood in the proposal distributions, i.e.
qn(θn, sn|θn−1, sn−1) ∝bg(yn|θn; θn−1, sn)r(θn|θn−1, sn)r(sn|sn−1),
bp(yn|θn−1, sn−1) ∝
M
X
j=1
(
r(sn = j|sn−1)
Z
bg(yn|θn; θn−1, sn = j)r(θn|θn−1, sn = j)dθn
)
.
Systematic resampling was used in both algorithms.1 Based on the parameter estimates
made in [6], we set M = 2, p11 = 0.993, p22 = 0.973, α1 = −1.2, α2 = −0.9, φ = 0.85,
σ2
θ = 0.1.
For each algorithm, the variance of the minimum means square error (MMSE) ﬁltering
estimate of the log volatility was computed at each iteration, over 500 independent runs.
1Although systematic resampling does not uniformly outperform other approaches it is extremely widely
used in the applied ﬁltering literature. Although it is computationally attractive, care is required when using this
approach for the reasons documented in [17].

78
Nick Whiteley and Adam M. Johansen
APF
sAPF
N
σ2
CPU / s
σ2
CPU / s
10
0.0906
0.8394
0.0850
0.2526
20
0.0544
1.5397
0.0492
0.3558
50
0.0325
3.6665
0.0290
0.6648
100
0.0274
10.7095
0.0230
1.1801
200
0.0195
17.7621
0.0189
2.7231
500
0.0195
35.4686
0.0185
5.3206
Table 3.5 Stochastic volatility model: variance of ﬁltering estimate and average CPU time per run over 500 runs
for the IBOVESPA data.
10−1
100
101
102
0
0.05
0.1
0.15
0.2
Figure 3.4 Stochastic volatility model: variance of ﬁltering estimate vs. average CPU time in secs. over 500 runs
for the IBOVESPA data. Solid: sAPF, dash-dot: APF.
These variances were then summarised by taking their arithmetic mean and are shown in
Table 3.5.
This shows how the variance of ﬁltering estimates of the log-volatility and mean CPU
time per run for the two algorithms relate to the number of particles used. For the same
number of particles, the sAPF algorithm exhibits lower variance of ﬁltering estimates. The
results also show that, for the same number of particles, the sAPF can be computationally
cheaper than the APF. This can be explained as follows. The algorithms involve precisely
the same arithmetic operations in order to compute both the auxiliary importance weights
and the importance weights by which estimation is performed. However, in terms of ran-
dom number generation, the APF is more expensive: it uses one random variate to perform
systematic resampling, then for each particle draws S (i)
n from a distribution on {1, ..., M}
and samples θ(i)
n from qn(·|θn−1, sn). By contrast, the sAPF uses one random variate to per-
form systematic resampling (which assigns values of both X(i)
n−1 and S (i)
n ) and then for each
particle samples θ(i)
n from qn(·|θn−1, sn).
Although the cost-saving will be dependent on the programming language employed,
the results indicate that the savings can be signiﬁcant. In this case both algorithms were
implemented in MatLab, and the code was made common to both algorithms in all places
possible. The performance beneﬁt in terms of estimator variance versus CPU time is
illustrated in Fig. 3.4.
Figure 3.5 shows boxplots of the number of particles in the high-volatility regime
over 100 independent runs of each algorithm. The pairs of boxplots correspond to the
dates 3/1/99 (left), 14/1/99 (middle) and 15/1/99 (right). During this period, it can be
seen from Fig. 3.3 that an increase in volatility occurs; N = 100 particles were used
in both algorithms. The count of number of particles in the high-volatility regime was

Auxiliary particle ﬁltering: recent developments
79
APF
sAPF
0
20
40
60
80
100
APF
sAPF
0
20
40
60
80
100
APF
sAPF
0
(a)
(b)
(c)
20
40
60
80
100
Figure 3.5 Stochastic volatility model. Boxplots, over 100 runs of each algorithm, of the number of particles
in the high-volatility regime at iterations corresponding to the dates 13/1/99 (a), 14/1/99 (b) and 15/1/99 (c).
N = 100.
made immediately after resampling in the case of the sAPF and immediately after mak-
ing proposals in the case of the APF, i.e. at equivalent steps of the algorithms. Across
the three dates the sAPF exhibits lower variability than the APF and the mean number
of particles in the high-volatility regime is lower for the APF. That is, the sAPF shows
less variability in its approximation of the distribution over strata: this improved distri-
butional approximation is the underlying mechanism which leads to improved variance
properties.
Figure 3.3 shows the one-step-ahead MAP prediction of the switching state sn, using
the sAPF algorithm with N = 500 particles. Recall that sn = 2 is the high-volatility regime.
The results show that the model is able to recognise changes in the level of volatility and
these changes roughly coincide with the currency crisis events listed in Table 3.4. The
results are very similar to those obtained in [6].
3.5
Conclusions
This article has attempted to summarise the state of the art of the auxiliary particle ﬁlter.
Our intention is to provide some insight into the behaviour of the APF and its rela-
tionship with other particle-ﬁltering algorithms, in addition to summarising a number of
recent methodological extensions. One of the most signiﬁcant points is perhaps this: the
APF is simply an example of a sequential estimation procedure in which one can beneﬁt
from the early introduction of information about subsequent distributions, combined with
an importance sampling correction. In the context of time series analysis, this approach
is useful when performing ﬁltering in SSMs and the same approach can be exploited
elsewhere.

80
Nick Whiteley and Adam M. Johansen
Bibliography
[1] C. Andrieu, M. Davy and A. Doucet. Eﬃcient
particle ﬁltering for jump Markov systems.
Application to time-varying autoregressions.
IEEE Transactions on Signal Processing,
51(7):1762–1770, 2003.
[2] A. Bain and D. Crisan. Fundamentals of
Stochastic Filtering. Stochastic Modelling and
Applied Probability. Springer Verlag, 2009.
[3] M. Briers, A Doucet and S. S. Singh. Sequential
auxiliary particle belief propagation. In
Proceedings of International Conference on
Information Fusion, 2005.
[4] O. Capp´e, E. Moulines and T. Ryden. Inference
in Hidden Markov Models. Springer Verlag,
2005.
[5] J. Carpenter, P. Cliﬀord and P. Fearnhead. An
improved particle ﬁlter for non-linear problems.
IEEE Proceedings on Radar, Sonar and
Navigation, 146(1):2–7, 1999.
[6] C. M. Carvalho and H. F. Lopes.
Simulation-based sequential analysis of Markov
switching stochastic volatility models.
Computational Statistics and Data Analysis,
51:4526–4542, 2007.
[7] S. Centanni and M. Minozzo. Estimation and
ﬁltering by reversible jump MCMC for a doubly
stochastic Poisson model for
ultra-high-frequency ﬁnancial data. Statistical
Modelling, 6(2):97–118, 2006.
[8] N. Chopin. Central limit theorem for sequential
Monte Carlo methods and its applications to
Bayesian inference. Annals of Statistics,
32(6):2385–2411, 2004.
[9] D. E. Clark and J. Bell. Convergence results for
the particle PHD ﬁlter. IEEE Transactions on
Signal Processing, 54(7):2652–2661, July 2006.
[10] J. Cornebise, E. Moulines and J. Olsson.
Adaptive methods for sequential importance
sampling with application to state space models.
Statistics and Computing, 18:461–480, 2008.
[11] D. J. Daley and D. Vere-Jones. An Introduction
to the Theory of Point Processes, volume I:
Elementary Theory and Methods of Probability
and Its Applications. Springer, second edition,
2003.
[12] A. Dassios and J. Jang. Kalman-Bucy ﬁltering
for linear system driven by the Cox process with
shot noise intensity and its application to the
pricing of reinsurance contracts. Journal of
Applied Probability, 42(1):93–107, 2005.
[13] P. Del Moral. Feynman-Kac Formulae:
Genealogical and Interacting Particle Systems
with applications. Probability and Its
Applications. Springer Verlag, New York, 2004.
[14] P. Del Moral, A. Doucet and A. Jasra. Sequential
Monte Carlo methods for Bayesian
Computation. In Bayesian Statistics 8. Oxford
University Press, 2006.
[15] P. Del Moral, A. Doucet and A. Jasra. Sequential
Monte Carlo samplers. Journal of the Royal
Statistical Society Series B, 63(3):411–436,
2006.
[16] P. Del Moral, A. Doucet and A. Jasra. On
adaptive resampling procedures for sequential
Monte Carlo methods. Technical Report
HAL-INRIA RR-6700, INRIA, 2008.
[17] R. Douc, O. Capp´e and E. Moulines.
Comparison of resampling schemes for particle
ﬁlters. In Proceedings of the 4th International
Symposium on Image and Signal Processing and
Analysis, volume I, pages 64–69, 2005.
[18] R. Douc and E. Moulines. Limit theorems for
weighted samples with applications to sequential
Monte Carlo methods. Annals of Statistics,
36(5):2344–2376, 2008.
[19] R. Douc, E. Moulines and J. Olsson. Optimality
of the auxiliary particle ﬁlter. Probability and
Mathematical Statistics, 29(1):1–28, 2009.
[20] A. Doucet, N. de Freitas and N. Gordon, editors.
Sequential Monte Carlo Methods in Practice.
Statistics for Engineering and Information
Science. Springer Verlag, 2001.
[21] A. Doucet, N. Gordon and V. Krishnamurthy.
Particle ﬁlters for state estimation of jump
Markov linear systems. IEEE Transactions on
Signal Processing, 49(3):613–624, 2001.
[22] A. Doucet and A. M. Johansen. A tutorial on
particle ﬁltering and smoothing: Fifteen years
later. In D. Crisan and B. Rozovsky, editors, The
Oxford Handbook of Nonlinear Filtering. Oxford
University Press, 2011. To appear.
[23] P. Fearnhead. Sequential Monte Carlo methods
in ﬁlter theory. PhD thesis, University of Oxford,
1998.
[24] P. Fearnhead, O. Papaspiliopoulos and G. O.
Roberts. Particle ﬁlters for partially observed
diﬀusions. Journal of the Royal Statistical
Society B, 70:755–777, 2008.
[25] J. Geweke. Bayesian inference in econometric
models using Monte Carlo integration.
Econometrica, 57(6):1317–1339, 1989.
[26] W. R. Gilks and Carlo Berzuini.
RESAMPLE-MOVE ﬁltering with Cross-Model
jumps. In Doucet et al. [20], pages 117–138.
[27] S. Godsill and T. Clapp. Improvement strategies
for Monte Carlo particle ﬁlters. In Doucet et al.
[20], pages 139–158.
[28] S. J. Godsill, J. Vermaak, K-F. Ng and
J-F. Li. Models and algorithms for tracking of
manoeuvring objects using variable rate particle
ﬁlters. Proc. IEEE, April 2007.
[29] A. G. Gray and A. W. Moore. N-body problems
in statistical learning. In Advances in Neural
Information Processing Systems 13, pages
521–527. MIT Press, 2000.
[30] K. Heine. Uniﬁed framework for
sampling/importance resampling algorithms. In
Proceedings of Fusion 2005, July 25–29, 2005,
Philadelphia. 2005.

Auxiliary particle ﬁltering: recent developments
81
[31] A. M. Johansen. SMCTC: Sequential Monte
Carlo in C++. Journal of Statistical Software,
30(6):1–41, 2009.
[32] A. M. Johansen and A. Doucet. Auxiliary
variable sequential Monte Carlo methods.
Research Report 07:09, University of Bristol,
Department of Mathematics – Statistics Group,
University Walk, Bristol, BS8 1TW, UK, 2007.
[33] A. M. Johansen and A. Doucet. A note on the
auxiliary particle ﬁlter. Statistics and Probability
Letters, 78(12):1498–1504, 2008.
[34] A. M. Johansen, S. Singh, A. Doucet and B. Vo.
Convergence of the SMC implementation of the
PHD ﬁlter. Methodology and Computing in
Applied Probability, 8(2):265–291, 2006.
[35] N. Kantas. Sequential Decision Making in
General State Space Models. PhD thesis,
University of Cambridge, 2009.
[36] R. Karlsson and N. Bergman. Auxiliary particle
ﬁlters for tracking a maneuvering target. In
Proceedings of the 39th IEEE Conference on
Decision and Control, pages 3891–3895, 2000.
[37] G. Kitagawa. Monte Carlo ﬁlter and smoother
for non-Gaussian nonlinear state space models.
Journal of Computational and Graphical
Statistics, 5(1):1–25, March 1996.
[38] M. Klass, N. de Freitas and A. Doucet. Towards
practical n2 Monte Carlo: The marginal particle
ﬁlter. In Proceedings of Uncertainty in Artiﬁcial
Intelligence, 2005.
[39] J. S. Liu and R. Chen. Sequential Monte Carlo
methods for dynamic systems. Journal of the
American Statistical Association,
93(443):1032–1044, 1998.
[40] H. F. Lopes and C. M. Carvalho. Factor
stochastic volatility with time varying loadings
and Markov switching regimes. Journal of
Statistical Planning and Inference,
137:3082–3091, 2007.
[41] R. P. S. Mahler. Multitarget Bayes ﬁltering via
ﬁrst-order multitarget moments. IEEE
Transactions on Aerospace and Electronic
Systems, pages 1152–1178, October 2003.
[42] R. P. S. Mahler. Statistical
Multisource-Multitarget Information Fusion.
Artech House, 2007.
[43] S. Nadarajah and S. Kotz. Mathematical
properties of the multivariate t distribution. Acta
Applicandae Mathematicae, 89:53–84, 2005.
[44] M. K. Pitt and N. Shephard. Filtering via
simulation: Auxiliary particle ﬁlters. Journal of
the American Statistical Association,
94(446):590–599, 1999.
[45] M. K. Pitt and N. Shephard. Auxiliary variable
based particle ﬁlters. In Doucet et al. [20],
chapter 13, pages 273–293.
[46] G. Poyiadjis, A. Doucet and S.S. Singh. Particle
methods for optimal ﬁlter derivative: application
to parameter estimation. In Proceedings of IEEE
International Conference on Acoustics, Speech
and Signal Processing (ICASSP), volume 5,
pages 925–928, 2005.
[47] C. P. Robert and G. Casella. Monte Carlo
Statistical Methods. Springer Texts in Statistics.
Springer, 2004.
[48] S. Saha, P. K. Mandal, Y. Boers, H. Driessen,
and A. Bagchi. Gaussian proposal density using
moment matching in SMC methods. Statistics
and Computing, 19:203–208, 2009.
[49] H. Sidenbladh. Multi-target particle ﬁltering for
the probability hypothesis density. In
Proceedings of the International Conference on
Information Fusion, Cairns, Australia, pages
800–806, 2003.
[50] S. S. Singh, B.-N. Vo, A. Baddeley and S. Zuyev.
Filters for spatial point processes. Siam Journal
on Control and Optimization, 48(4):2275–2295,
2008.
[51] M. K. P. So, K. Lam and W. K. Li. A stochastic
volatility model with Markov switching. Journal
of Business and Economic Statistics,
16(2):244–253, 1998.
[52] B. Vo, S. Singh and A. Doucet. Sequential
Monte Carlo methods for multitarget ﬁltering
with random ﬁnite sets. IEEE Transactions on
Aerospace and Electronic Systems,
41(4):1224–1245, 2005.
[53] N. Whiteley, A. M. Johansen and S. Godsill.
Monte Carlo ﬁltering of piecewise-deterministic
processes. Journal of Computational and
Graphical Statistics, 2011. To appear.
[54] N. Whiteley, S. Singh and S. Godsill. Auxiliary
particle implementation of the probability
hypothesis density ﬁlter. IEEE Transactions on
Aerospace and Electronic Systems,
46(3):1437–1454, 2010.
[55] T. Zajic and R. P. S. Mahler. Particle-systems
implementation of the PHD multitarget tracking
ﬁlter. In Proceedings of SPIE, pages 291–299,
2003.
Contributors
Nick Whiteley, Statistics Group, Department of Mathematics, University of Bristol
Adam M. Johansen, Department of Statistics, University of Warwick

4
Monte Carlo probabilistic inference for diffusion
processes: a methodological framework
Omiros Papaspiliopoulos
4.1
Introduction
We consider statistical inference for models speciﬁed by stochastic diﬀerential equations
(SDEs). Stochastic diﬀerential equations provide a natural model for processes which at
least conceptually evolve continuously in time and have continuous sample paths. From
a more pragmatic point of view they oﬀer a ﬂexible framework for modelling irregularly
spaced time series data. As a result they are used as statistical models throughout science;
for example in ﬁnance [48, 20, 2], biology [26], molecular kinetics [29, 31]. They are
increasingly used in more mainstream statistical applications, e.g. longitudinal data anal-
ysis [49], space-time models [8] and functional data analysis, see for example [45] and
discussion therein. Speciﬁcally, an SDE for a d-dimensional process V ∈Rd is speciﬁed as
follows:
dVs = b(s, Vs) ds + σ(s, Vs) dBs,
s ∈[0, T],
(4.1)
where B is an m-dimensional standard Brownian motion, b(·, ·) : R+×Rd →Rd is called the
drift, and σ(·, · ) : R+ × Rd →Rd×m is called the diﬀusion coeﬃcient. Boundary conditions
are needed to complete the model speciﬁcation. Certain assumptions are required on b and
σ to ensure that Eq. (4.1) has a unique weak solution, see for example Theorem 5.2.1
of [38]. The unique solution is known as a diﬀusion process. It can be shown that it is a
strong Markov process, thus it shares the Markov semigroup property with the solutions
of ordinary diﬀerential equations, which are obtained in the no-noise limit σ = 0. Note
that the dimension of the driving Brownian motion can diﬀer from that of the state process.
In statistical applications an interesting possibility is to take d > m. For example, we can
model a process with diﬀerentiable sample paths by specifying an SDE on the process and
its time-derivatives. This gives rise to the so-called hypo-elliptic diﬀusion processes [44]. A
simple popular hypo-elliptic model is the integrated Brownian motion which is often used
in target tracking applications, see for example [27], and it relates to inference for unknown
regression functions [53].
The diﬀusion process can be used to model directly observed data, or it can be used to
model latent processes which relate to the observable via likelihood functions. Statistical
inference in such contexts consists of estimating unknown parameters involved in the
speciﬁcation of the drift and the diﬀusion coeﬃcient, and estimating the process itself

Monte Carlo inference for diﬀusion processes
83
when it is unobserved. We are interested in likelihood-based inference for the unknown
parameters, i.e. maximum likelihood and Bayesian methods; and in probabilistic inference
for the unobserved processes, i.e. inference according to the conditional law of the pro-
cess given the observed data, where the prior law is given by the SDE speciﬁcation (4.1).
To simplify the presentation we will refer to such estimation procedures as probabilistic
inference.
A major diﬃculty with probabilistic inference for diﬀusion processes is the intractabil-
ity of the transition density
ps,t(v, w) = Pr [Vt ∈dw | Vs = v] / dw;
t > s;
w, v ∈Rd.
(4.2)
This is due to the fact that only in very few cases can the SDE be analytically solved.
At inﬁnitely small time increments, i.e for t −s ≈0, Eq. (4.2) can be satisfactorily
approximated by a Gaussian. However, this approximation is very poor for arbitrary time
increments. Intuitively, the transition distribution for longer time increments is a non-
linear convolution of Gaussian distributions, hence it is intractable. It is known that for
ﬁxed observation frequency, quasi-maximum likelihood estimators of parameters based
on a ﬁrst-order Gaussian approximation to Eq. (4.2) are in general inconsistent [24]. On
the other hand, this diﬃculty has motivated exciting research for analytic and Monte
Carlo (MC) approximations of Eq. (4.2), see for example [1, 30, 40] and references
therein. Typically, these approaches involve systematic bias due to time and/or space
discretisations.
The methodological framework developed and reviewed in this chapter concerns
the unbiased MC estimation of the transition density, and the exact simulation of dif-
fusion processes. The former relates to auxiliary variable methods, and it builds on
a rich generic MC machinery of unbiased estimation and simulation of inﬁnite series
expansions. This machinery is employed in diverse application areas such as popula-
tion genetics and operational research. The latter is a recent signiﬁcant advance in the
numerics for diﬀusions and it is based on the so-called Wiener–Poisson factorisation of
the diﬀusion measure. It has interesting connections to exact simulation of killing times
for the Brownian motion and interacting particle systems, which are uncovered in this
chapter.
The methodological framework we develop leads to unbiased probabilistic inference
for diﬀusion processes. Our focus is more on the methodology than on its speciﬁc appli-
cation to inference. Nevertheless, for clarity we consider the so-called continuous–discrete
non-linear ﬁltering problem, see for example [14]. An overview of how to combine this
framework with standard computational algorithms such as the expectation maximisation
(EM) and Markov chain Monte Carlo (MCMC) to perform likelihood-based inference for
diﬀusions is given in [7].
The rest of the chapter is organised as follows. Section 4.2 introduces the continuous–
discrete non-linear ﬁltering problem, which serves as a motivating example. The section
introduces the idea of replacing unknown densities by positive unbiased estimators and
its interpretation as an auxiliary variable technique. Section 4.3 gives a representation
of the transition density for a class of diﬀusion processes, which is key to our frame-
work. Section 4.4 shows how to use this representation to achieve exact simulation of
diﬀusion processes. Section 4.5 provides further insights to the exact simulation by link-
ing it to the simulation of killing times of Brownian motion exploiting the connection
between the exponential distribution and the Poisson process. It also relates the construc-
tion to interacting particle systems. Section 4.6 gives a detailed account of the machinery

84
Omiros Papaspiliopoulos
involved in deriving unbiased estimators of the diﬀusion transition density. This machin-
ery is interesting outside the context of SDEs and links to the literature are provided.
Section 4.7 closes with a discussion.
4.2
Random weight continuous–discrete particle ﬁltering
The development in this section follows to some extent [21]. We consider that Eq. (4.1) is
unobserved, but partial information is available at discrete times 0 < t1 < t2 < · · · < tn
in terms of observations y1, y2, . . . , yn which are linked to the diﬀusion via a likelihood
function, f(yi|Vti). We also elicit a prior distribution on the diﬀusion initial state, say
p0(V0). Hence, we have a continuously evolving signal modelled as a diﬀusion process,
and discrete-time observations. We are interested in the recursive calculation of the so-
called ﬁltering distributions, i.e. the sequence of posterior distributions p(Vti|y1:i) which
will be denoted by πi(Vti), where by standard convention y1:i = (y1, . . . , yi). This is known
as the continuous–discrete ﬁltering problem, see for example [14]. To simplify notation in
this section we will subscribe the discrete skeleton of V by i rather than ti, i.e Vi := Vti.
Hence, we actually deal with a discrete-time Markov chain Vi, i = 0, . . . , n, observed with
noise. Hence the problem of interest can be coined as a discrete-time ﬁltering problem, as
follows.
Using marginalisation, the Bayes’ theorem and the Markov property, we obtain the
following fundamental ﬁltering recursion:
πi+1(Vi+1) ∝
Z
f(yi+1|Vi+1)pti,ti+1(Vi, Vi+1)πi(Vi)dVi.
(4.3)
Only in very speciﬁc cases can the ﬁltering distributions be characterised by a ﬁnite num-
ber of parameters which can recursively be computed. A model amenable to this type of
analysis is obtained when V is the solution of a linear SDE (e.g. the integrated Brownian)
observed with additive Gaussian error. In this case we can use the Kalman ﬁlter to do the
computations.
For non-linear models, however, the state of the art is to approximate the ﬁltering dis-
tributions using MC. The corresponding algorithms, known as particle ﬁlters (PFs) [17],
are characterised by two main steps. First, an approximation of πi by a discrete distri-
bution, denoted by πN
i , whose support is a set of N particles, {V(j)
i }N
j=1, with associated
(un-normalised probability) weights {w(j)
i }N
j=1. Substituting πN
i for πi in Eq. (4.3), yields a
(continuous density) approximation to πi+1
˜πi+1(Vi+1) ∝
N
X
j=1
w(j)
i f(yi+1|Vi+1)pti,ti+1(V( j)
i , Vi+1).
(4.4)
The aim of one iteration of the PF is to construct a further particle (discrete distribution)
approximation to ˜πi+1. The second main step of the PF is to use importance sampling to
sample from Eq. (4.4), thus obtaining a particle approximation for ˜πi+1. A general frame-
work for achieving this is given by the auxiliary particle ﬁlter of [43]. We choose a proposal
density of the form
N
X
j=1
β(j)
i qi+1(Vi+1|V( j)
i , yi+1),

Monte Carlo inference for diﬀusion processes
85
where the βis are probabilities, and the qis are probability density functions. These steps
are summarised in Algorithm 4.1. Step PF2 of the algorithm includes a decision to resam-
ple among existing particles when the variance of the proposal weights β exceeds a certain
threshold. The decision is taken using the eﬀective sample size, see for example Chapter 2
of [33]. Note that, when taking C < N and β(k)
i
= 1/N, resampling is never performed and
the approach reduces to a direct importance sampling with target πi+1 and proposals gen-
erated independently from Qi+1
k=0 qk. The (at least occasional) resampling, however, which
introduces dependence among the particles, is crucial to break the curse of dimension-
ality inherent in an importance sampling algorithm. The resulting particle ﬁlter has good
theoretical properties including consistency [12] and central limit theorems for estimates of
posterior moments [14, 10, 32], as N →∞. Under conditions relating to exponential forget-
ting of initial conditions for the signal, PF errors stabilise as n →∞[13, 32]. Additionally,
the ﬁltering distributions are obtained at computational cost O(N), and unbiased estimators
of the normalising constants (important in parameter estimation and model comparisons)
are readily available. Improvements on independent sampling in PF1 can be made: see inter
alia the stratiﬁed sampling ideas of [9].
Algorithm 4.1 Auxiliary PF for state space models.
PF0 Simulate V(j)
0
∼p0(V0), and set w(j)
0 = 1/N, for j = 1, . . . , N.
for i = 0, . . . , n −1, j = 1, . . . , N do
PF1 Calculate the eﬀective sample size of the {β(k)
i }, ES S = (PN
k=1(β(k)
i )2)−1.
If ES S < C, for some ﬁxed constant C, simulate k j
i+1 from p(k) = β(k)
i , k = 1, . . . , N
and set δ( j)
i+1 = 1; otherwise set k j
i+1 = j and δ(j)
i+1 = β(j)
i .
PF2 Simulate V( j)
i+1 from qi+1(·|V
k j
i+1
i
, yi+1).
PF3 Assign particle V(j)
i+1 a weight
w( j)
i+1 = w
(k j
i+1)
i
δ(j)
i+1 f(yi+1|V(j)
i+1) pti,ti+1(V
(k j
i+1)
i
, V( j)
i+1)
β
(k j
i+1)
i
qi+1(V(j)
i+1|V
(k j
i+1)
i
, yi+1)
.
(4.5)
end for
Algorithm 4.1 applies generally to state space time series models. However, when the
signal is a discretely sampled diﬀusion process, the PF cannot be applied due to
the intractability of the system transition density, which is necessary in the calculation
of the weights. One way to bypass this problem is to simulate the particles Vi+1 accord-
ing to the diﬀusion dynamics; then the transition density cancels out from Eq. (4.5). This
requires the exact simulation of diﬀusions, which is discussed in Section 4.4. Another pos-
sibility is to try to obtain unbiased estimators for the transition density ps,t(u, v) for arbitrary
s, t, u, v. The unbiasedness is needed to ensure that the particles are properly weighted, see
for example Section 2.5.4 of [33].
Section 4.6 shows how for each pair (u, v) and times (s, t), with s < t, to simulate auxil-
iary variables Ψ according to a distribution Q( ·; s, t, u, v), and to specify a computable func-
tion r(Ψ, s, t, u, v), with the property that E[r(Ψ, s, t, u, v)] = ps,t(u, v). Then, the so-called
random weight PF (RWPF) inserts a further step between PF2 and PF3: simulate Ψ( j)
i+1 from

86
Omiros Papaspiliopoulos
Q( ·; ti, ti+1, V
(k j
i+1)
i
, V( j)
i+1) and compute r(Ψ(j)
i+1, ti, ti+1, V
(k j
i+1)
i
, V(j)
i+1). This quantity replaces the
intractable transition density in Eq. (4.5). The RWPF is introduced in [46] and [21].
When r is positive this formulation has an interpretation as an expansion of the state
space using auxiliary variables. According to our construction, conditionally on Vi and
Vi+1, Ψi+1 is independent of Ψj and Vj for any j diﬀerent from i, i + 1. Additionally, it
follows easily from the unbiasedness and positivity of r that, for any u, r(ψ, ti, ti+1, u, v) is
a probability density function as a function of (ψ, v) with respect to the product measure
Leb(dv) × Q(dψ; , ti, ti+1, u, v), where Leb denotes the Lebesgue measure. Consider now an
alternative discrete-time Markov model with unobserved states (Vi, Ψi), i = 1, . . . , n, tran-
sition density r and observed data yi with observation density f(yi|Vi). By construction
the marginal ﬁltering distributions of Vi in this model are precisely πi. Consider an auxil-
iary PF applied to this model where we choose with probabilities β(j)
i
each of the existing
particles (V( j)
i , Ψ(j)
i ), and generate new particles in the following way: Vi+1 is proposed
from qi+1 as described before, and conditionally on this value, Ψi+1 is simulated accord-
ing to Q. Then, it can be checked that the weight assigned to each particle is precisely
that in the RWPF. Therefore, the RWPF is equivalent to an auxiliary PF on this discrete-
time model whose latent structure has been augmented with the auxiliary variables Ψi. It
is worth mentioning that the potential of using unbiased estimators of intractable densities
while retaining the ‘exactness’ of MC algorithms is being increasingly recognised. The idea
already appears in a disguised form in the auxiliary PF of [43] and explicitly in the rejec-
tion control algorithm, see for example Section 2.6.1 of [33]. The authors in [7] elaborate
on this idea to design approximation-free MCMC algorithms for probabilistic inference for
diﬀusions, [34, 35] devise novel MCMC algorithms for parameter estimation for models
with intractable normalising constants (which are functions of the parameters), [4] develop
and analyse theoretically a general class of MCMC algorithms where the target density is
replaced by an importance sampling estimator, and [3] show how to obtain exact MCMC
algorithms for state space models when the likelihood is estimated by the PF. Additionally,
[41] show that the MCEM algorithm can be adjusted using these ideas to increase mono-
tonically an objective function. The random weight idea appears also within the variance
reduction approach of [25] in the context of option pricing.
Clearly, the replacement of an importance sampling weight, say w, with an unbiased
estimator, say r, increases the variance: Var{r} = Var{w} + E[Var{r|w}], since E[r|w] = w,
provided all variances exist. The expression suggests that the random weight importance
sampler will be most eﬃcient when E[Var{r|w}] is relatively small compared to Var{w}.
In the auxiliary PF formulation given in Algorithm 4.1 the positivity of the estimators is
not necessary, since the resampling probabilities are controlled by βi. Therefore, even if the
actual weights wi are negative, the algorithm in principle can still be carried out and yield
consistent estimates of expectations over the ﬁltering distributions. Clearly, in this case
the wis lose their interpretation as un-normalised probabilities; this is further discussed in
Section 4.7. On the other hand, the generic Algorithm 4.2, proposed originally in [22],
can be applied to ensure the positivity of the unbiased estimators. Suppose that we have
N particles with true but unknown weights w( j) and for each j, let r(i,j) i = 1, 2, . . . , be a
sequence of conditionally independent unbiased estimators of w( j). The procedure yields a
random weight r(j) = Pτ
i=1 r(i,j), where τ is a stopping time which depends on the sign of
all weights. If E[τ] < ∞, then E[r(j)|w(j)] = E[τ]w( j); this follows from Wald’s identity,
see Theorem 2 of [22]. The intractable normalising constant E[τ] in the weights creates no
problems, since it is common to all particles and will be cancelled out when the particle
weights are re-normalised.

Monte Carlo inference for diﬀusion processes
87
Algorithm 4.2 Creating positive unbiased importance weights exploiting Wald’s identity.
Set i = 1, simulate r(1,j) and set r(j) = r(1, j), for all j = 1, . . . , N.
If min j{r(j)} > 0 then STOP.
i := i + 1, simulate r(i, j) and set r(j) = r(j) + r(i,j), for all j = 1, . . . , N.
4.3
Transition density representation for a class of diﬀusions
The exact simulation and unbiased estimation methods developed in the chapter critically
rely on a representation of the diﬀusion transition density. The representation relies on
certain assumptions. To simplify exposition, we will assume from now on that Eq. (4.1) is
time-homogeneous.
(A1) In the SDE (4.1), d = m, σ = I, and b is of gradient form, i.e. there exists a function
U : Rd →R (known as the potential) such that b = ∇U.
The assumptions in (A1) are easily satisﬁed when d = 1. In that case, the assumption
on b reduces to a diﬀerentiability condition. Additionally, when σ(v) is a diﬀerentiable
function of v, V can be transformed to a process with unit diﬀusion coeﬃcient, by applying
the transformation v →x =
R v 1/σ(u)du. Therefore, (A1) is restrictive only in multi-
dimensional settings. Hence, in the rest of the chapter we will consider a d-dimensional
diﬀusion process X which solves the following SDE:
dXs = ∇U(Xs) ds + dBs,
s ∈[0, T],
(4.6)
where B is a d-dimensional Brownian motion, and X0 = x. In the sequel X will also be used
to denote an arbitrary continuous path, whose meaning will be clear from the context.
Let P0 denote the law of the Brownian motion on the space of continuous paths, and let
Pb denote the probability law of X implied by Eq. (4.6). We can appeal to the Cameron–
Martin–Girsanov theorem for Itˆo processes, see for example Theorem 8.6.6 of [38], to
obtain the likelihood ratio between the two measures on the time increment [0, t]. Applying
also integration by parts facilitated by the gradient form of the drift, we obtain
dPb
dP0
|t(X) = exp
(
U(Xt) −U(x) −
Z t
0
φ(Xs)ds
)
,
(4.7)
where φ(u) := (||b(u)||2 + ∆U(u))/2, ∆is the Laplacian operator and || · || the Euclidean
norm. Let P∗
b;t,y and P∗
0;t,y denote the laws on [0, t] of X and B respectively, conditioned to
hit at time t the value y ∈Rd. A diﬀusion process conditioned to start and ﬁnish at speciﬁc
values is known as a diﬀusion bridge.
Consider the decomposition of the laws Pb and P0 into the marginal distributions at time
t and the diﬀusion bridge laws conditioned on Xt
dPb
dP0
|t(X) = p0,t(x, y)
G0,t(x, y)
dP∗
b;t,y
dP∗
0;t,y
(X),
where G0,t(x, y) is the Gaussian transition density of the dominating Brownian motion.
Then, re-arranging, we have the fundamental identity which underpins the methodological
framework we develop here
dP∗
b;t,y
dP∗
0;t,y
(X) = G0,t(x, y)
p0,t(x, y) exp
(
U(y) −U(x) −
Z t
0
φ(Xs)ds
)
.
(4.8)

88
Omiros Papaspiliopoulos
Re-arranging Eq. (4.8) and taking expectations on both sides with respect to P∗
0;t,y, we obtain
the following representation for the transition density:
p0,t(x, y) = G0,t(x, y) exp{U(y) −U(x)}EP∗
0;t,y
"
exp
(
−
Z t
0
φ(Xs)ds
)#
.
(4.9)
Therefore, we obtain the transition density as an expectation of an exponential trans-
formation of a path integral, where the expectation is taken over the Brownian bridge
measure.
The derivation of the likelihood ratio for diﬀusion bridge measures (4.8) can be made
formal, see for example Theorem 2 of [15]. On a more general level, Eq. (4.9) follows
from the basic principles of conditional expectation. In particular let (Ω, F ) be a mea-
surable space, P and Q be two probability measures on the space with Radon–Nikodym
derivative ξ = dP/dQ, and let G ⊆F be a sub-σ-algebra. Then, the derivative dP/dQ
restricted to G is E[ξ|G]. This is a very classical result which can be used to establish
the existence of conditional expectation. On the other hand, assuming the existence of
conditional expectation (using the projection approach, see for example [54]), the result
follows from the deﬁnition of conditional expectation and the tower property of iterated
conditional expectations. This basic result is instrumental in the statistical analysis of
partially observed stochastic processes; for example in [16] it is used to deﬁne an EM
algorithm for partially observed continuous-time Markov processes. To obtain Eq. (4.9)
we specify G as the σ-algebra generated by Xt and use the result in conjunction with
Eq. (4.7).
For a thorough presentation of MC identities for transition densities of diﬀusions,
treatment of the general time-inhomogeneous and multivariate case and the historical
development of these results see Sections 3 and 4 of [40].
4.4
Exact simulation of diﬀusions
The authors in [5] and [6] recognised that Eq. (4.7) suggests an algorithm for the exact
simulation of diﬀusion sample paths using rejection sampling. The algorithm is known
generally as the exact algorithm (EA) and appeared in the literature in three generations
corresponding to successive relaxations on the conditions which it requires, namely EA1
and EA2 presented in [5], and EA3 presented in [6].
For the development of the EA two further (relatively mild) assumptions are needed.
(A2) The function φ in Eq. (4.9) is lower bounded; let ℓ:= infu φ(u) > −∞.
(A3) The function ρ(y) := exp{U(y) −||y −x||2/(2t)} is integrable in y for some t and for
all x.
To avoid unnecessary notation, let us redeﬁne φ as
φ(u) = (||b(u)||2 + ∆U(u))/2 −ℓ≥0.
(4.10)
We ﬁx a time horizon t, such that (A3) holds, and consider the problem of simulating
Xt according to the solution of Eq. (4.6) given X0 = x, or equivalently according to the
transition distribution p0,t(x, y)dy. Assumption (A3) allows us to deﬁne the so-called biased
Wiener measure on the space of continuous paths on [0, t] by its Radon–Nikodym derivative
with respect to P0,

Monte Carlo inference for diﬀusion processes
89
dZ
dP0
|t(X) = exp{U(Xt)},
that is Z is obtained from P0 by biasing the marginal distribution of the latter at time t using
the potential function U. Conditionally on the end-point, the two measures are identical.
Then, by piecing everything together we have that
dPb
dZ |t(X) ∝exp
(
−
Z t
0
φ(Xs)ds
)
≤1.
(4.11)
Therefore, there exists a rejection sampling algorithm on the path space for simulating
diﬀusion sample paths (Xs, 0 ≤s ≤t) according to Pb using proposals from Z and accepting
them with probability (4.11). Nevertheless, it is far from obvious how to carry out such an
algorithm on the computer, i.e. using a ﬁnite number of steps. This can be achieved by
beneﬁting from a seemingly remarkable connection between the Brownian motion and the
Poisson process, contained in the following theorem [6].
Theorem 4.1. (Wiener–Poisson factorisation) Let L denote the law of a unit rate Poisson
process on [0, t] × [0, ∞) and deﬁne the extended law Z ⊗L with typical realisation (X, Φ),
with Φ = {(χj, ψj)} j≥1 and {ψj} non-decreasing. Deﬁne the event
Γ :=
\
j≥1
{φ(Xχ j) < ψj}.
(4.12)
Then, Pb on [0, t] is the marginal distribution of X when (X, Φ) ∼Z ⊗L | Γ.
Eﬀectively, the theorem formalises the observation that the exponential term in
Eq. (4.11) can be identiﬁed as the probability that an independent Poisson process on
[0, t]×[0, ∞) has no points under the epigraph of s →φ(Xs) for a given path X. The connec-
tion between the Poisson process and the diﬀusion measure is investigated and motivated
further in Section 4.5.
Given knowledge of the range of φ, we can appeal to the principle of retrospective
sampling [39] to provide an algorithm for the exact simulation of Xt which can be car-
ried out using a ﬁnite amount of computation. Suppose for instance that φ is also upper
bounded, that is
there exists an r < ∞such that sup
u
φ(u) < r.
(4.13)
Then, the condition posed by Eq. (4.12) is trivially satisﬁed by all points of the Poisson
process with ψ j > r, and only a ﬁnite number of comparisons have to be made to check
the condition. Additionally, since Φ is independent of X, we can ﬁrst simulate the Poisson
process on [0, t]×[0, r] and unveil X at the times χj speciﬁed by the Poisson process. When
Eq. (4.12) is satisﬁed the simulated skeleton of X (which contains Xt) is retained, otherwise
it is rejected and the procedure is repeated. This amounts to generating pairs (X, Φ) accord-
ing to Z⊗L and accepting them when (X, Φ) ∈Γ, where we have used the upper bound of φ
and retrospective sampling to check the condition using ﬁnite computation. The algorithm
is given in Algorithm 4.3, and each accepted draw Xt is a sample from the target diﬀusion
at time t. For convenience, the χ js are now ordered whereas the ψ js are not. Note also that
Step 2 simulates from the ﬁnite-dimensional distributions of Z.
When φ is unbounded, the joint simulation according to Z of X and a random box
which contains it is required. This is the EA3 which is described in detail in [6]. The extra

90
Omiros Papaspiliopoulos
Algorithm 4.3 (EA1) The exact algorithm for the simulation of Xt according to the SDE
(4.6) when Eq. (4.13) holds.
1. Generate a Poisson process 0 < χ1 < χ2 < · · · of rate r on [0, t]. Let κ be the number of
points. Generate a sequence of uniform random variables ψ j ∼Uni[0, r] , j = 1, . . . , κ.
2. Simulate Xt ∼ρ given in (A3). Simulate {Xχ1, . . . , Xχκ}, according to the Brownian
bridge started at X0 = x and ﬁnishing at Xt.
3. If ψ j > φ(Xχ j) for all j ≤κ then accept Xt; otherwise return to 2.
eﬀort needed in EA3 comes at an increased computational cost: the careful and extensive
numerical investigation in [42] suggests as a rule of thumb that EA3 is about 10 times
slower than EA1. Since EA is based on rejection sampling, when applied directly to [0, t]
the computational eﬀort necessary to yield a draw grows exponentially with t. However,
this is not the true complexity of the algorithm. The Markov property permits an imple-
mentation of the algorithm which has O(t) complexity, since the time increment [0, t] can
be split and the EA be applied sequentially. A further interesting property is that the accep-
tance probability of the EA is roughly constant when applied to intervals t/d as d increases;
this is a by-product of the gradient structure of the drift and the form of Eq. (4.11). This
argument is supported empirically in [42], who ﬁnd that EA1 has complexity O(d) in the
dimension of the target diﬀusion. On the other hand, the complexity of EA3 as a func-
tion of d is worse than linear due to maximisations needed in the implementation of the
algorithm.
4.5
Exact simulation of killed Brownian motion
The Wiener–Poisson factorisation in Theorem 4.1 appears at ﬁrst striking since it connects
the law of a diﬀusion process to that of the Brownian motion and an independent Poisson
process. However, this result is less surprising given a representation of the class of diﬀu-
sions (4.6) as killed Brownian motion; see for example Section 8.2 of [38] where also the
connections to the Feynman–Kac formula are discussed. In particular, consider an expo-
nentially distributed random variable E ∼Exp(1) independent of X, and deﬁne the killing
time T as the following function of E and X:
T = inf
(
s :
Z s
0
φ(Xs)ds = E
)
,
where φ is given in Eq. (4.10). Thus,
Pr [T > t|X ] = exp
(
−
Z t
0
φ(Xs)ds
)
.
(4.14)
Then, it is easy to see that the scheme described in Algorithm 4.4 yields an importance
sampling approximation of the law of Xt induced by the SDE (4.6). The resulting weighted
sample {(X(j)
t , w( j)
t )}N
j=1 is a particle approximation of the law of Xt. The killing step (Step 4
in Algorithm 4.4) ensures that the law of the path conditioned to be alive has a density with
respect to the Wiener measure given by the right-hand side of Eq. (4.14), and the weighting
(Step 5) is necessary to ensure that the path has density proportional to Eq. (4.7). However,
the scheme of Algorithm 4.4 is not practically implementable, since it involves an inﬁnite
amount of simulation in Step 3.

Monte Carlo inference for diﬀusion processes
91
Algorithm 4.4 Importance sampling approximation of the law of Xt by killed Brownian
motion.
1. Set j = 0.
2. while j < N do
3.
Generate E ∼Exp(1).
4.
Generate a Brownian path X started from x, and keep track of
R s
0 φ(Xs)ds. Stop when
s = t.
5.
Rejection: If
R s
0 φ(Xs)ds > E reject the path, goto 1.
6.
Weighting: If
R s
0 φ(Xs)ds < E then j := j + 1, set X(j)
t
= Xt, w(j)
t
= eU(Xt). Goto 2.
7. end while
Note that for a given X, T is the ﬁrst arrival time of a time-inhomogeneous Poisson
process with intensity φ(Xs). Assume now that Eq. (4.13) holds. Then, we can simulate T
exactly by thinning a dominating Poisson process with intensity r. Let 0 < χ1 < χ2 < · · · ,
be the time-ordered arrival times of the dominating Poisson process. Then, if each arrival
χ j is accepted with probability φ(Xχ j)/r, T is the ﬁrst accepted arrival time. Algorithm 4.5
is a modiﬁcation of Algorithm 4.4; we call it the exact killing (EK) algorithm. The result-
Algorithm 4.5 Exact killing: Exact simulation of a killed Brownian motion using thinning.
1. Set j = 0.
2. while j < N do
3.
Set χ0 = 0, i = 0
4.
Set i := i + 1, simulate χi.
5.
Simulate Xχi given Xχi−1 according to the Brownian motion dynamics. If χi > t
then simulate Xt given Xχi and Xχi−1 according to the Brownian bridge dynamics, set
j := j + 1 and X(j)
t
= Xt, w( j)
t
= eU(Xt). Goto 2.
6.
If χi < t, simulate ψi ∼Uni(0, r). If ψi > φ(Xχi), then goto 3, else goto 2.
7. end while
ing weighted sample {(X(j)
t , w(j)
t )}N
j=1 is again a particle approximation of the law of Xt
obtained by rejection (killing) and weighting, but now the procedure can be carried out
exactly using a ﬁnite number of uniform and Gaussian random variables. This is made fea-
sible precisely by the thinning of a Poisson super-process with rate r and it relies on the
assumption (4.13).
Algorithm 4.5 has intriguing connections to other exact simulation schemes for Markov
processes. For example, the thinning of a Poisson super-process is a main ingredient of the
algorithm of [23] for the exact simulation of discrete state space continuous-time Markov
chains conditioned to start and ﬁnish at speciﬁc states. Most relevant to this article, is its
direct connection with EA1 given in Algorithm 4.3. In fact, the two algorithms share exactly
the same rejection step; EK needs to weight the accepted draws, whereas EA1 by ﬁxing the
ﬁnal time t a priori, includes this bias in the dynamics of the proposal process which are
according to Z.
On the other hand, EK gives a particle approximation to the ﬂow of distributions t →
Pb|t. Since EK also relies on rejection sampling, the computational eﬀort to yield a particle
at time t increases exponentially with t. The Markov property can be exploited here as well,
by deﬁning time increments of size, δ say. If a particle is alive at time iδ but dies before
(i + 1)δ, a new path is restarted from the value it has at time iδ rather than re-starting from

92
Omiros Papaspiliopoulos
time 0. Provided that the variance of the weights wt does not increase with t (note that they
depend only on Xt rather than the whole history) the complexity of the algorithm is O(t).
One can avoid the rejections involved in EK at the expense of introducing dependence
among the simulated particles. Let N be a population of particles which move freely accord-
ing to the Brownian dynamics. To each particle j, we assign a death time T j, as before.
Once a particle dies, then a randomly chosen particle of the remaining ones duplicates
and each branch evolves conditionally independently. Again, it is easy to see that we can
construct a super-process with intensity r × N which will contain all possible death times
of all particles. We simulate iteratively these arrivals, at each arrival time χi, we pick at
random one of the existing particles, j say, and propose to kill it. To do that, we realise
its value at that time, we simulate ψi ∼Uni[0, r], and check if ψi < φ(X(j)
χi ). If this is
so we kill it and duplicate a randomly chosen one among the rest of the particles. If not,
the particle remains alive. It is clear from the lack of memory of the underlying super-
process, that at each arrival time, and after checking for killing and possibly adjusting the
population, we can forget everything that has happened and start again from the current
population of particles. To obtain an importance sample approximation for Pb|t we weight
each alive particle X(j)
t
time t with w(j)
t
= eU(Xt) weight. Hence, we can simulate exactly
the genealogy of this interacting particle system which tracks the law of the diﬀusion
process.
4.6
Unbiased estimation of the transition density using series expansions
The machinery required for producing unbiased estimators of diﬀusion transition densi-
ties is very broad in its scope and it is only mildly linked to the structure of diﬀusion
processes. The techniques we present here are intrinsically linked to the MC solution to
ﬁxed-point problems, see for example [28] for applications in population genetics, [52] in
the context of solutions of partial diﬀerential equations (PDEs), [18] for a recent contribu-
tion in the literature and references, and Section 2.5.6 of [33] for a gentle introduction to
the idea. The purpose in this section is to develop all components separately, emphasising
their generic purpose, and then piece them all together to solve the problem of interest in
this chapter. The decoupling of the techniques greatly simpliﬁes the understanding of the
ﬁnal method but also suggests possibilities for improvements. The main components of
the methodology can be identiﬁed as follows. (i) Expansion of functions into power series.
This allows the unbiased estimation of the function given unbiased estimators of its argu-
ment. The expansion of the exponential function and the so-called Poisson estimator are
treated in Section 4.6.1. Some optimality issues for the estimator are discussed and biased
alternatives mentioned. (ii) Unbiased truncation of inﬁnite series. There are various tech-
niques for the unbiased estimation of an inﬁnite sum, based either on importance sampling
or on integration by parts (eﬀectively application of Fubini’s theorem) followed by impor-
tance sampling. This is treated in Section 4.6.2. (iii) Further structure is available when the
unbiased estimator of the exponential of a path integral of a Markov process is required.
Compared to (i) the added feature is the explicit dependence of the unbiased estimators of
the argument of the function. This is explored in Section 4.6.3, which couples this mate-
rial with (ii) to yield a general class of unbiased estimators. The richer structure allows a
more insightful mathematical formulation of the problem, as one of importance sampling
in a countable union of product spaces. This point of view leads to the fourth component
of the methodology. (iv) Simulation from certain probability measures deﬁned on a count-
able union of product spaces. This is treated in Section 4.6.4, and provides the optimal

Monte Carlo inference for diﬀusion processes
93
importance sampling estimator for the problem posed in Section 4.6.3. This formalism
links directly with the so-called MC method for solving integral equation and ﬁxed-point
problems. This is outlined in Section 4.6.4. There, we argue that the power expansion idea
and the technique for solving integral equations, although related, are not equivalent. An
illustration of the estimation of the transition density of the Cox–Ingersoll–Ross diﬀusion
process, considering the unbiased estimator and various biased estimators, is presented in
Section 4.6.6.
4.6.1
Power series expansions: the exponential function and the Poisson estimator
We consider two related problems. Let X be an unknown quantity, and let ˜Xj be indepen-
dent (conditionally on X) unbiased estimators of X, i.e. E[ ˜X|X] = X, where ˜X denotes a
generic element of the sequence. We assume that the ˜Xjs have a common ﬁnite absolute
moment, E[| ˜X||X] < ∞. In many examples the ˜Xjs have the same distribution conditionally
on X. Let f be a non-linear function. Then, we are interested in estimating (a) f(X) or (b)
E[ f(X)] when X is a random variable. In fact, we are typically interested in (b), however
the argument is the same for both cases, hence we consider the two problems jointly. When
f is linear the problem is trivial. However, when f is a real analytic function there is still
the possibility of getting unbiased estimators via series expansions. We concentrate on the
case where f(x) = ex. Then, for any ﬁxed c, we have
eX = ec
∞
X
i=0
(X −c)i /i! = ec
∞
X
i=0
E

iY
j=1
( ˜Xi −c)|X
/i! = ec E

∞
X
i=0
iY
j=1
( ˜Xi −c)/i!
, (4.15)
where the product Q0
j=1 is deﬁned to be equal to 1. The role of c will be discussed later. Note
that the absolute moment assumption on the ˜X justiﬁes the third step in the above argument
by dominated convergence. Hence, the inﬁnite sum is an unbiased estimator of eX. Still,
this is not a realisable estimator. The topic of truncating unbiasedly inﬁnite sums becomes
of pivotal importance and it is discussed in the following section. At a more elementary
level, one way to yield a feasible estimator is to recognise the similarity of the expression
to an expectation of a Poisson random variable. In fact, it is easy to check directly that for
any λ > 0
eλ+c
κ
Y
i=1
˜X j −c
λ
,
κ ∼Po(λ)
(4.16)
is a realisable unbiased estimator of eX. We term Eq. (4.16) the Poisson estimator. Its
second moment is easy to work out provided that the ˜Xjs have a common second moment,
E[ ˜X2|X] < ∞:
exp
(
λ + 2c + 1
λE
h
( ˜X −c)2|X
i)
.
(4.17)
The two constants c and λ are user-speciﬁed and relate to the sign and the variance of the
estimator. For example, if ˜X is lower bounded, c can be chosen to make the Poisson esti-
mator positive, if this is desired (see for example Section 4.2). However, with two degrees
of freedom the question of optimality in terms of variance is ill-posed, as shown in the
following proposition whose proof is straightforward.

94
Omiros Papaspiliopoulos
Proposition 4.1. Optimal implementation of the Poisson estimator for estimating eX: Tak-
ing c = −λ, and λ →∞, the variance of the estimator converges monotonically to 0 and
the estimator converges to eX in mean square sense.
Working directly from Eq. (4.17) we have that, for ﬁxed c, the optimal choice for λ is
E[( ˜X −c)2|X]1/2, whereas for a given computational budget λ the optimal choice for c is
X −λ. These are not feasible estimators, but can guide good choices.
Note that a biased plug-in alternative estimator is available in this context, which is
given by exp{PN
j=1 ˜X j/N}, where N plays the role of λ in the Poisson estimator. Even
in this simple context the comparison of the two estimators in mean square error is not
obvious. We will see these two possibilities in the context of diﬀusions in Sections 4.6.3
and 4.6.6.
In most cases of interest X is a random variable and we are actually interested in esti-
mating E[eX] with respect to the law of X. The argument presented earlier can be repeated
to show that Eq. (4.16) is unbiased for this quantity, however we need the stronger condition
E[exp{E[| ˜X||X]}] < ∞
as a suﬃcient condition to justify the third step in the development. For given λ and c a
suﬃcient condition to ensure a ﬁnite second moment is
E
"
exp
(1
λ(E[ ˜X2|X] −2cX)
)#
< ∞.
The expected value of Eq. (4.17) gives the second moment. In this case we need to
average, say M, independent realisations of the estimator. Hence the computational
cost is on average λM and the choice of optimal allocation in terms of λ and M is
non-trivial.
Furthermore, c and λ can be chosen to depend on X. In [21] the authors proposed such
generalised Poisson estimators to ensure positivity of the estimators. The estimator and
its variance have the forms speciﬁed above, the conditions however which ensure their
existence have to be modiﬁed appropriately.
4.6.2
Unbiased truncation of inﬁnite series
In the previous section an estimator was given in terms of an inﬁnite sum in Eq. (4.15).
To avoid the impossible computation, we extracted an unbiased estimator of the sum by
expressing it as an expectation of a Poisson random variable. It turns out that this is just one
instance of a generic methodology for unbiased estimation of inﬁnite sums. Abstracting, let
us consider the problem of ﬁnding an unbiased estimator of
S =
∞
X
k=1
αk,
(4.18)
where we assume that the sum is ﬁnite a.s. As in the previous section, we might be inter-
ested in E[S ] when the αks are random variables, but the argument follows in a similar way.
There are (at least) three ways to obtain an unbiased estimator of Eq. (4.18), two of which
turn out to be equivalent.
Firstly, we can use importance sampling. Let βk > 0 be probabilities, i.e. P
k βk = 1.
Then αK/βK is an unbiased estimator of S , where K is simulated according to Pr[K = k] =
βk. If

Monte Carlo inference for diﬀusion processes
95
S a =
∞
X
k=1
|αk| < ∞,
(4.19)
then Jensen’s inequality shows that it is optimal to take βk = |αk|/S a.
An alternative argument to yield eﬀectively the same estimator, but useful when using
this machinery in more elaborate contexts (see for example Section 4.6.3), is to deﬁne a
sequence of ‘killing’ probabilities 0 < pk < 1, for k = 1, 2 . . .. Then, consider a discrete-
time survival process where death happens at each time k with probability pk. Let K be the
death time. Then
αK
QK−1
i=1 (1 −pi)pK
is an unbiased estimator of S . Note that Pr[K = k] = Qk−1
i=1 (1 −pi)pk. It is easy to check
that P
k
Qk−1
i=1 (1 −pi)pk ≤1. If the sum is strictly less than 1 then K = ∞has a positive
probability, which then yields an infeasible estimator. If the sum is 1, then the two estima-
tors we have discussed are equivalent and correspond to the representation of a distribution
in terms of the probabilities or the hazard function. The importance sampling estimator is
obtained by taking βk = Qk−1
i=1 (1 −pi)pk. On the other hand, for given probabilities βk, let
G be the survival function, G(k) = P∞
i=k βi. Then, taking pk = 1 −G(k)/G(k −1) yields the
second estimator.
The third estimator is based on an application of Fubini’s theorem, which can be applied
in this context under Eq. (4.19). Let again βk be probabilities with survival function G. Then
X
k
αk =
X
k
αk
G(k)G(k) =
X
k
αk
G(k)
∞
X
i=k
βi =
∞
X
k=1
∞
X
i=k
αk
G(k)βi =
∞
X
i=1
iX
k=1
αk
G(k)βi,
which suggests the following unbiased estimator of S :
K
X
k=i
αi/G(i) =
K
X
i=1
αi/
K−1
Y
i=1
(1 −pi),
where K is simulated according to Pr[K = k] = βk, and the equality follows from the
equivalent representation in terms of killing probabilities.
It should be clear that the Poisson estimator (4.16) corresponds to a very speciﬁc set-
ting where we use the importance sampling estimator with Poisson proposal probabilities
for estimating the inﬁnite expansion. It should also be clear that the other schemes we
have discussed in this section can be used to provide unbiased estimators of eX and its
expected value. These alternative estimators start with Eq. (4.15) and apply a technique for
the unbiased estimation of the inﬁnite sum.
4.6.3
Unbiased estimation of the expected value of exponential functions
of Markov process path integrals
A very interesting instance of the generic context of Section 4.6.1 is when X is a path
integral of a Markov process. With a slight abuse of notation, suppose that we are interested
in estimating
I(x, t) := E
"
exp
(Z 1
t
g(s, Xs)ds
)#
,
t ≤1,
(4.20)

96
Omiros Papaspiliopoulos
where X is a Markov process in Rd, with explicit transition density ps,t(x, y), such that
Xt = x. The upper limit of the integration can be arbitrary; here it is taken to be 1 for
notational simplicity. This problem was considered by [51] and solved with an approach
which combines the power expansions with the unbiased estimation of inﬁnite series, as
described below. Notice that the estimation problem in Eq. (4.20) is raised when consider-
ing the estimation of the transition density for the class of diﬀusion processes considered
in Section 4.3; see Eq. (4.9) where X is the Brownian bridge. The use of the estima-
tors for the estimation of diﬀusion transition densities was considered in [7], see also
Section 4.6.6.
By the standard MC integration trick, we have that (1−t)g(χ, Xχ), where χ ∼Uni(t, 1) is
conditionally on X an unbiased estimator of the exponent in Eq. (4.20). Working precisely
as in Section 4.6.1, under the suﬃcient condition
Ia(x, t) := E
"
exp
(Z 1
t
|g(s, Xs)|ds
)#
< ∞,
for all t ≤1,
we get the following inﬁnite-series unbiased estimator of Eq. (4.20):
∞
X
k=0
Z 1
t
· · ·
Z 1
un−1
Z
Rd · · ·
Z
Rd
n
Y
i=1
g(ui, xi)pui−1,ui(xi−1, xi)dxn · · · dx1dun · · · du1,
(4.21)
with the convention that x0 = x, u0 = t. This inﬁnite expansion can be treated with the
machinery of Section 4.6.2 to yield feasible unbiased estimators of Eq. (4.20). For example,
an importance sampling estimator based on Po(λ(1 −t)) probabilities and simulation of X
according to its transition density yields the Poisson estimator
e(λ+c)(1−t)
κ
Y
j=1
g(χ j, Xχj) −c
λ
,
κ ∼Po(λ(1 −t)), χj ∼Uni(t, 1).
(4.22)
Note however that with the same variables we can consider the alternative estimator based
on the application of Fubini’s theorem discussed in Section 4.6.2, or indeed use a diﬀerent
proposal distribution for the index K (e.g. the negative binomial).
The speciﬁc structure of the exponent in Eq. (4.20) (as opposed to the generic one in
Section 4.6.1) permits a mathematically richer formulation of the estimation problem. This
is done in [51] (see in particular Propositions 1, 2 and 4 of the article). This formulation
casts the estimation of Eq. (4.20) as a familiar problem in MC. Speciﬁcally, let us deﬁne
the following union of product spaces, Y := S∞
k=0 Yk where Yk = {k} × Xk+1, and in our
context X is the space [t, 1] × Rd. Let us now deﬁne the following signed measure ϕ on Y
indexed by (x, t), and given by the formulae
ϕ(k, d(t0, x0)×· · ·×d(tk, xk) ; x, t) = δ(t,x)(d(t0, x0))
k
Y
i=1
1ti[ti−1, 1]g(ti, xi)pti−1,ti(xi−1, xi)dtidxi,
where δ denotes the Dirac delta function, and 1x[A] is 1 if x ∈A and 0 otherwise. In this
formulation, Eq. (4.21) shows that Eq. (4.20) is the normalising constant of ϕ: I(x, t) =
ϕ(Y; x, t), hence we can reformulate the original problem as one of estimating a normal-
ising constant. Importance sampling is one possibility to do this by constructing measures
on Y and computing the Radon–Nikodym derivative between the two measures for the
generated samples. Provided that the normalising constant of the proposal distribution is

Monte Carlo inference for diﬀusion processes
97
known, the weight assigned to each generated sample is an unbiased estimator of ϕ(Y; x, t).
Summarising, the expansion in a power series and the explicit structure of the exponent
allow the re-formulation of estimation of Eq. (4.20) as the computation of a normalising
constant of a signed measure. The material of Section 4.6.2 together with standard MC
techniques eﬀectively gives methods for constructing proposal distributions on Y to be
used in the importance sampling. Reference [51] gives the following generic estimator
where p0(s, x) > 0 is a killing probability and qs,t(x, y) is an alternative tractable transition
density
K
Y
i=1
g(χi, Xχi)pχi−1,χi(Xχi−1, Xχi)
(1 −p0(χi−1, Xχi−1))qχi−1,χi(Xχi−1, Xχi)
1
p0(χK, XχK),
where the χis are ordered uniforms on [t, 1] and the Xχi are generated according to the tran-
sitions q. Let |ϕ| be the total variation of ϕ, thus it is obtained by replacing g with its absolute
value in the deﬁnition given above. Then, by Jensen’s inequality (as in Section 4.6.2) it
follows that the optimal proposal distribution in terms of minimising the variance of the
estimator, is |ϕ|/Ia(x, t). Simulation from probability measures in Y is treated in the next
section.
We close with the remark that alternative biased plug-in estimators (as discussed in
Section 4.6.1) are available. For example, one may consider
exp

1 −t
N
N
X
j=1
g(χj, Xχ j)

with the random elements as in Eq. (4.22). Alternative numerical approximation of the
integral in the exponent of Eq. (4.20) can also be considered. A comparison among diﬀerent
schemes is carried out in Section 4.6.6.
4.6.4
Simulation from probability measures on unions of spaces
The fourth main ingredient of the methodological framework for unbiased estimation is
linked with the simulation from the following series of measures. Consider the follow-
ing abstract problem: let γ(x) be a positive function on X; p(x, y) a transition density
(i.e. probability density in y and measurable in x), where x, y ∈X; and δx(dy) the Dirac
measure centred at x. Consider the product space Y := S∞
k=0{k} × Xk+1 with typical ele-
ment (k, x0, x1, . . . , xk) with the convention x0 = x. We have already seen this context in
Section 4.6.3, where X = [t, 1] × Rd.
We deﬁne the following positive measure on Y indexed by x ∈X:
ν(k, dx1 × · · · × dxk+1 ; x) := δx(dx0)
k
Y
i=1
p(xi−1, xi)γ(xi)dx1 · · · dxk+1.
We assume that I(x) := ν(Y ; x) < ∞, and deﬁne ˜ν(·; x) = ν(·; x)/I(x) to be the correspond-
ing probability measure on Y. Note that by deﬁnition I(x) > 1. The aim of this section
is to simulate draws from ˜ν and to show that distributions of this form provide the opti-
mal importance sampling distributions in the context of Section 4.6.3. The construction is
theoretical, since it will typically not be feasible to carry out the simulation. Nevertheless,
it provides insights on the optimal implementation of the unbiased estimators we consider
in this chapter.

98
Omiros Papaspiliopoulos
To start with note the fundamental recursion implied by the deﬁnition of the measures
and the normalising constants
I(x) = 1 +
Z
Y
I(x1)p(x, x1)γ(x1)dx1.
(4.23)
Using the same argument that led to Eq. (4.23) we can obtain the following marginal-
conditional distributions under ˜ν: ˜ν(k
=
0; x)
=
1/I(x), ˜ν(dx1, k
>
0; x)
∝
p(x, x1)γ(x1)I(x1)dx1. In the same way we obtain the general expressions
˜ν(k > i −1, dx1, . . . , dxi; x)
=
I(xi)
iY
j=1
p(xj−1, xj)γ(xj),
˜ν(dxi|x, x1, . . . , xi, k > i −1)
=
p(xi−1, xi)γ(xi)I(xi)dx1/(I(xi−1) −1),
˜ν(k = i|x, x1, . . . , xi, k > i)
=
1/I(xi).
The last two equations give the necessary structure for the simulation from ˜ν using a
Markov chain, by sequentially at each stage i ﬁrst simulating a new value xi and then decid-
ing on whether to stop the simulation. The procedure results with a string (k, x0, x1, . . . , x :
k). The problem of simulation from probability measures on Y with structure as ˜ν was
recently considered in [18] using trans-dimensional MCMC; see also the article for further
references. This problem, together with the corresponding task of estimating the normalis-
ing constant, comes up in a large number of scientiﬁc contexts. This is due to the fact that it
is intrinsically related to the numerical solution of ﬁxed-point problems. This is described
in the following section.
4.6.5
Monte Carlo for integral equations
Suppose that we are interested in the solution of the following integral equation:
I(x) = h(x) +
Z
X
p(x, y)I(y)dy,
(4.24)
where h is explicitly known for all x. This type of equation (and its discrete-valued counter-
parts) appear in a variety of problems. We have already seen an instance; I(x, t) in Eq. (4.20)
satisﬁes such an equation with h = 1. By successive substitution of I in the equation we
obtain the inﬁnite expansion
I(x) = h(x) +
∞
X
k=1
Z
Xk
k
Y
i=1
p(xi−1, xi)h(xk)dx1 × · · · dxk,
with the convention x0 = x. The analogy with the problems treated in Sections 4.6.2 and
4.6.4 is direct. This is the reason why the same machinery which is used in the solution
of the ﬁxed-point problems becomes useful in the unbiased estimation of the diﬀusion
transition density. Nevertheless, the power expansions discussed in Section 4.6.1 do not
necessarily lead to a ﬁxed-point problem. However, the techniques of Section 4.6.2 still
apply to yield unbiased estimators even in these cases.
4.6.6
Illustrating example: the CIR density
We close the section with an illustration of the methodology on the estimation of the
transition density of the so-called Cox–Ingersoll–Ross (CIR) diﬀusion [11]. This is a one-
dimensional diﬀusion with b and σ in Eq. (4.1) given by −ρ(x −µ) and σ √x respectively,

Monte Carlo inference for diﬀusion processes
99
where ρ > 0, σ > 0, µ are parameters and x ∈R+. This diﬀusion is not in the form (4.6)
but it can be transformed as described in Section 4.3. When the transformation is applied,
the transition density of the original process is linked by a change of variables to the tran-
sition density of the unit-diﬀusion-coeﬃcient process; see [7]. However, in this model,
when the process is transformed, its measure is absolutely continuous with respect to the
law of the Brownian motion conditioned to remain positive, which is known as the Bessel
process. Therefore, Eq. (4.9) holds but the expectation is taken with respect to the law
of the Bessel bridge. In our numerical results it turns out that it does not really make a
diﬀerence whether one works with the Brownian or the Bessel bridge. We consider four
estimators. First, the unbiased estimator obtained by using the Poisson estimator (4.22) to
estimate the expectation in Eq. (4.9) (using the Bessel bridge dominating measure). Sec-
ond, a biased estimator based on Riemmann approximation of the exponent in Eq. (4.9).
This is in the spirit of the plug-in estimators discussed in Section 4.6.3 but where the times
to evaluate the path are chosen deterministically. This estimator in the context of diﬀusions
was considered in [37]. Finally, we consider two estimators obtained using the discrete-time
approach of [19]. We use their estimator on the original CIR process and on the transformed
to unit-diﬀusion-coeﬃcient process. The estimator of [19] applied to the transformed pro-
cess is closely related to the estimator of [37]: the only diﬀerence is that the latter applies
numerical integration to a Cameron–Martin–Girsanov formula with the stochastic integral
eliminated using integration by parts, whereas the former applies numerical integration on
the expression which contains the stochastic integral.
The transition density of the CIR is explicitly known, hence it can be used to assess the
root mean square error of the estimators. Our simulation setup is as follows. We consider
the parameter values used in the simulation study in [19]: (ρ, µ, σ) = (0.5, 0.06, 0.15) and
starting point for the diﬀusion X0 = 0.1. We consider two ﬁnal times, a small one t = 1/252
and a large one t = 1/2, and we estimate the transition density for three diﬀerent ending
points which correspond to the 10, 50 and 90 percent quantiles of the transition distribution.
For the biased estimators we consider various values for N, the number of evaluations on
a given path, N = 2i, i = 2, 3, . . . , 8. For the Poisson estimator we choose the average
computational cost to be the same as that of the biased estimators and we take c = λ. In
each case we average M independent realisations of the estimator, where we take M = N2
following the asymptotic result of [47]. For the estimation of the root mean square error of
each estimator we average 120 independent replicates.
Figure 4.1 contains the results of the simulation, where we plot the logarithm of the
root mean square error against the logarithm of the number of evaluations per path. The
study shows the variance reduction eﬀectuated by the expression of the transition density
in Eq. (4.9). Moreover, the unbiased estimator works very well in this setup. In this chapter
we have pursued unbiasedness due to its connection with auxiliary variable methods. Nev-
ertheless, the results show that the estimator has comparable or better performance than
biased alternatives.
4.7
Discussion and directions
We have reviewed and developed a rich methodological framework for the MC assisted
probabilistic inference for diﬀusion processes. On one hand, the framework is based on
representations of the diﬀusion process which can be exploited for its exact simulation. On
the other hand, the framework relies on a generic importance sampling machinery which
has been used in various other contexts. The exact algorithm and the Poisson estimator build

100
Omiros Papaspiliopoulos
−11
−8
−6
−4
−1
−10
−8
−6
−3
−1
−12
−9
−6
−1
−6
−4
−1
−6
−4
−2
−1
2
3
4
5
2
3
4
5
−7
−4
Figure 4.1 Logarithm of the root mean square error of the estimators against the logarithm of the number of
imputed points per simulated path. The transition of the CIR process is estimated for three ending points corre-
sponding to the 10 (top), 50 (middle) and 90 (bottom) quantiles of the transition distribution. The time increment
is t = 1/252 (left) and t = 1/2 (right). (◦) Durham and Gallant without variance transformation, (△) Durham and
Gallant with variance transformation, (+) Nicolau, (x) Poisson estimator.
bridges between these two aspects, see for example the discussion in [7]. It is interesting to
understand deeper the connections; this might lead to new exact simulation algorithms out-
side the framework described in Section 4.4. A diﬀerent instance of this interplay appears
in the exact simulation from the stationary distribution of a Markov chain. There, a uni-
form ergodicity condition leads to an inﬁnite-series expansion for the stationary distribution
which can then be used for exact simulation.
The methodology for diﬀusions is based on the convenient representation of the tran-
sition density in Section 4.3, which relies on certain assumptions about the drift and the
diﬀusion coeﬃcient of the process. The conditions are strong when d > 1. On the other
hand, Theorem 3.1 of [52] establishes that the transition density of a generic diﬀusion
process solves an integral equation of the type (4.24) with h and p explicitly given. This
representation relies on diﬀerent conditions which relate to smoothness and bounded-
ness of the drift and diﬀusion coeﬃcients. Additionally, p might not be a positive kernel.
Nevertheless, this alternative representation is amenable to the type of unbiased MC esti-
mation using the tools of Section 4.6, and this has been pursued in [52] and [50]. In
current work we are exploring the possibilities of using this representation to achieve exact
simulation of the diﬀusion at ﬁxed times, and evaluating the practical usefulness of this
alternative.
The chapter has given little attention to the important question of choosing between
unbiased and biased estimators of the transition density and other diﬀusion functionals.
This question has not been seriously addressed since the primary purpose of the chap-
ter is to present in a uniﬁed manner a collection of ideas central to the construction of
unbiased MC schemes. Biased estimators of diﬀusion functionals can be easily obtained

Monte Carlo inference for diﬀusion processes
101
using the Euler or other type of approximations; see the discussion in Sections 4.6.3 and
4.6.6. It is diﬃcult to give very general statements about which type of estimator should be
preferred, particularly since exact calculation of mean square error is complicated even
in simple examples. Research in obtaining some simple general rules is underway. In
Section 4.6.6 we provide a simple comparison for the estimation of the CIR transition
density, and a much broader evaluation of competing biased and unbiased MC schemes
is in progress. On the other hand, a certain amount of empirical comparisons has been
published, see for example Sections 4.1 and 5.1 of [21] and Section 4 of [50]. Wagner
ﬁnds signiﬁcant reduction in mean square error via the application of variance reduction
techniques and recommends a combination of unbiased estimators with such techniques.
Closing this discussion, a generic argument in favour of unbiased estimation of unknown
quantities within an MC scheme is that of ‘consistency’. The RWPF of Section 4.2 pro-
vides consistent estimates of the ﬁltering distributions as N →∞. Working with biased
estimates requires that the bias is eliminated at a speciﬁc rate hence consistency is achieved
by letting both N and the amount of imputation go to inﬁnity at appropriate rates. Simi-
larly, an MCEM algorithm will typically give consistent parameter estimates as the number
of data go to inﬁnity even with ﬁxed MC eﬀort. This is not so when the MC contains
bias. The (appropriate) replacement of intractable densities by positive unbiased estimators
within an MCMC algorithm [4, 3] does not perturb the limiting distribution; this is cru-
cial since it is typically diﬃcult to quantify the amount of bias that would be introduced
otherwise.
Finally, as pointed out earlier, variance reduction techniques can be very eﬀective in
estimation of diﬀusion functionals. The biased Wiener measure proposal of Section 4.4 can
be used for this purpose; see for example [50] for implementation of such ideas. Variance
reduction methods for diﬀusions are studied for example in [36]. These possibilities within
the exact simulation framework are being currently investigated.
Acknowledgments
The author would like to acknowledge ﬁnancial support by the
Spanish government through a ‘Ramon y Cajal’ fellowship and the grants MTM2008-
06660 and MTM2009-09063, the Berlin Mathematical School for hosting him as a visiting
Professor while preparing this manuscript, Giorgos Sermaidis for useful suggestions and
Christian Robert and Randal Douc for motivating discussions on the unbiased estimation
techniques.
Bibliography
[1] Y. A¨ıt-Sahalia. Likelihood inference for
diﬀusions: a survey. In Frontiers in Statistics,
pages 369–405. Imperial College Press, 2006.
[2] Y. Ait-Sahalia and R. Kimmel. Maximum
likelihood estimation of stochastic volatility
models. Journal of Financial Economics,
83(2):413–452, 2007.
[3] C. Andrieu, A. Doucet and R. Holenstein.
Particle Markov chain Monte Carlo. Journal of
the Royal Statistical Society Series B, Statistical
Methodology, 72(3):269–342, 2010.
[4] C. Andrieu and G. O. Roberts. The
pseudo-marginal approach for eﬃcient Monte
Carlo computations. Annals of Statistics,
37(2):697–725, 2009.
[5] A. Beskos, O. Papaspiliopoulos and G. O.
Roberts. Retrospective exact simulation of
diﬀusion sample paths with applications.
Bernoulli, 12(6):1077–1098, 2006.
[6] A. Beskos, O. Papaspiliopoulos and G. O.
Roberts. A factorisation of diﬀusion measure and
ﬁnite sample path constructions. Methodology
and Computing in Applied Probability,
10(1):85–104, 2008.
[7] A. Beskos, O. Papaspiliopoulos, G. O. Roberts
and P. Fearnhead. Exact and computationally
eﬃcient likelihood-based estimation for
discretely observed diﬀusion processes. Journal
of the Royal Statistical Society B, 68(part
3):333–382, 2006.
[8] P. E. Brown, K. F. Kåresen, G. O. Roberts, and
S. Tonellato. Blur-generated non-separable
space-time models. Journal of the Royal

102
Omiros Papaspiliopoulos
Statistical Society Series B Statistical
Methodology, 62(4):847–860, 2000.
[9] J. Carpenter, P. Cliﬀord and P. Fearnhead.
Improved particle ﬁlter for nonlinear problems.
IEE Proceedings Radar, Sonar and Navigation,
146(1):2–7, 1999.
[10] N. Chopin. Central limit theorem for sequential
Monte Carlo methods and its application to
Bayesian inference. Annals of Statistics,
32(6):2385–2411, 2004.
[11] J. C. Cox, J. E. Ingersoll Jr and S. A. Ross. A
theory of the term structure of interest rates.
Econometrica: Journal of the Econometric
Society, pages 385–407, 1985.
[12] D. Crisan. Particle ﬁlters – a theoretical
perspective. In A. Doucet, N. de Freitas, and
N. Gordon, editors, Sequential Monte Carlo
Methods in Practice, pages 17–41.
Springer–Verlag, 2001.
[13] P. Del Moral and A. Guionnet. On the stability of
interacting processes with applications to
ﬁltering and genetic algorithms. Annales de
l’Institut Henri Poincar´e Probabilit´es et
Statistiques, 37:155–194, 2001.
[14] P. Del Moral and L. Miclo. Branching and
Interacting Particle Systems. Approximations of
Feymann-Kac Formulae with Application to
Non-linear Filtering, volume 1729. Springer,
2000.
[15] B. Delyon and Y. Hu. Simulation of conditioned
diﬀusion and application to parameter
estimation. Stochastic Processes and their
Applications, 116(11): 1660–1675, 2006.
[16] A. Dembo and O. Zeitouni. Parameter estimation
of partially observed continuous time stochastic
processes via the EM algorithm. Stochastic
Processes and their Applications, 23(1):91–113,
1986.
[17] A. Doucet, N. De Freitas and N. Gordon.
Sequential Monte Carlo Methods in Practice.
Springer Verlag, 2001.
[18] A. Doucet, A. M. Johansen and V. B. Tadic. On
solving integral equations using Markov Chain
Monte Carlo. Available from
www.cs.ubc.ca/ arnaud/TR.html, 2008.
[19] G. B. Durham and A. R. Gallant. Numerical
techniques for maximum likelihood estimation
of continuous-time diﬀusion processes. Journal
of Business and Economic Statistics,
20(3):297–338, 2002. With comments and a
reply by the authors.
[20] B. Eraker, M. Johannes and N. Polson. The
impact of jumps in volatility and returns. Journal
of Finance, 58(3):1269–1300, 2003.
[21] P. Fearnhead, O. Papaspiliopoulos and G. O.
Roberts. Particle ﬁlters for partially observed
diﬀusions. Journal of the Royal Statistical
Society B, 70:755–777, 2008.
[22] P. Fearnhead, O. Papaspiliopoulos, G. O. Roberts
and A. Stuart. Random weight particle ﬁltering
of continuous time processes. Journal of the
Royal Statistical Society Series B, Statistical
Methodology, 72(4):497–512, 2010.
[23] P. Fearnhead and C. Sherlock. An exact Gibbs
sampler for the Markov-modulated Poisson
process. Journal of the Royal Statistical Society
B, 68(5):767–784, 2006.
[24] D. Florens-Zmirou. Approximate discrete-time
schemes for statistics of diﬀusion processes.
Statistics, 20(4):547–557, 1989.
[25] P. Glasserman and J. Staum. Conditioning on
one-step survival for barrier options. Operations
Research, 49:923–937, 2001.
[26] A. Golightly and D. J. Wilkinson. Bayesian
sequential inference for stochastic kinetic
biochemical network models. Journal of
Computational Biology, 13:838–851, 2006.
[27] N. J. Gordon, D. J. Salmond and A. F. M. Smith.
Novel approach to nonlinear/non-Gaussian
Bayesian state estimation. IEE Proceedings F
Radar and Signal Processing, 140:107–113,
1993.
[28] R. C. Griﬃths and S. Tavar´e. Simulating
probability distributions in the coalescent.
Theoretical Population Biology, 46:131–158,
1994.
[29] I. Horenko and C. Sch¨utte. Likelihood-based
estimation of multidimensional Langevin models
and its application to biomolecular dynamics.
Multiscale Modeling and Simulation,
7(2):731–773, 2008.
[30] A. S. Hurn, Jeisman, J. I. and K. A. Lindsay.
Seeing the wood for the trees: A critical
evaluation of methods to estimate the parameters
of stochastic diﬀerential equations. Journal of
Financial Econometrics, 5(3):390–455, 2007.
[31] S. C. Kou, X. S. Xie and J. S. Liu. Bayesian
analysis of single-molecule experimental data.
Journal of the Royal Statistical Society Series C,
54(3):469–506, 2005.
[32] H. R. K¨unsch. Recursive Monte Carlo ﬁlters:
Algorithms and theoretical analysis. Annals of
Statistics, pages 1983–2021, 2005.
[33] Jun S. Liu. Monte Carlo Strategies in Scientiﬁc
Computing. Springer Series in Statistics.
Springer, 2008.
[34] J. Møller, A. N. Pettitt, R. Reeves and K. K.
Berthelsen. An eﬃcient Markov chain Monte
Carlo method for distributions with intractable
normalising constants. Biometrika,
93(2):451–458, 2006.
[35] I. A. Murray, Z. Ghahramani and D. J. C
MacKay. MCMC for doubly-intractable
distributions. In Proceedings of the 14th Annual
Conference on Uncertainty in Artiﬁcial
Intelligence pages 359–366, 2006.
[36] N. J. Newton. Variance reduction for simulated
diﬀusions. SIAM Journal on Applied
Mathematics, 54(6):1780–1805, 1994.

Monte Carlo inference for diﬀusion processes
103
[37] J. Nicolau. A new technique for simulating the
likelihood of stochastic diﬀerential equations.
Economic Journal, 5(1):91–103, 2002.
[38] B. K. Øksendal. Stochastic Diﬀerential
Equations: An Introduction With Applications.
Springer-Verlag, 1998.
[39] O. Papaspiliopoulos and G. O. Roberts.
Retrospective Markov chain Monte Carlo for
Dirichlet process hierarchical models.
Biometrika, 95:169–186, 2008.
[40] O. Papaspiliopoulos and G. O. Roberts.
Importance sampling techniques for estimation
of diﬀusion models. In SEMSTAT. Chapman and
Hall, 2009.
[41] O. Papaspiliopoulos and G. Sermaidis.
Monotonicity properties of the Monte Carlo EM
algorithm and connections with simulated
likelihood. Available from www2.
warwick.ac.uk/fac/sci/statistics/crism/
research/2007/paper07-24, 2007.
[42] S. Peluchetti and G. O. Roberts. An empirical
study of the eﬃciency of EA for diﬀusion
simulation. CRiSM Technical report 08-14,
available from www2.
warwick.ac.uk/fac/sci/statistics/crism/
research/2008/paper08-14, 2008.
[43] M. K. Pitt and N. Shephard. Filtering via
simulation: auxiliary particle ﬁlters. Journal of
the American Statistical Association,
94(446):590–599, 1999.
[44] Y. Pokern, A. M. Stuart and P. Wiberg. Parameter
estimation for partially observed hypoelliptic
diﬀusions. Journal of the Royal Statistical
Society Series B Statistical Methodology,
71(1):49–73, 2009.
[45] J. O. Ramsay, G. Hooker, D. Campbell and
J. Cao. Parameter estimation for diﬀerential
equations: a generalized smoothing approach.
Journal of the Royal Statistical Society Series B
Statistical Methodology, 69(5):741–796, 2007.
With discussions and a reply by the authors.
[46] M. Rousset and A. Doucet. Discussion of Beskos
et al. Journal of the Royal Statistical Society B,
68:374–375, 2006.
[47] O. Stramer and J. Yan. Asymptotics of an
eﬃcient Monte Carlo estimation for the
transition density of diﬀusion processes.
Methodology and Computing Applied
Probability, 9(4):483–496, 2007.
[48] S. M. Sundaresan. Continuous-time methods in
ﬁnance: A review and an assessment. Journal of
Finance, 55:1569–1622, 2000.
[49] J. M. G. Taylor, W. G. Cumberland and J. P. Sy. A
stochastic model for analysis of longitudinal
AIDS data. Journal of the American Statistical
Association, 89(427):727–736, 1994.
[50] W. Wagner. Monte Carlo evaluation of
functionals of solutions of stochastic diﬀerential
equations. Variance reduction and numerical
examples. Stochastic Analysis and Applications,
6(4):447–468, 1988.
[51] W. Wagner. Unbiased multi-step estimators for
the Monte Carlo evaluation of certain functional
integrals. Journal of Computational Physics,
79(2):336–352, 1988.
[52] W. Wagner. Unbiased Monte Carlo estimators
for functionals of weak solutions of stochastic
diﬀerential equations. Stochastics and
Stochastics Reports, 28(1):1–20, 1989.
[53] G. Wahba. Bayesian ‘conﬁdence intervals’ for
the cross-validated smoothing spline. Journal of
the Royal Statistical Society Series B,
45(1):133–150, 1983.
[54] D. Williams. Probability with Martingales.
Cambridge Mathematical Textbooks. Cambridge
University Press, 1991.
Contributor
Omiros Papaspiliopoulos, Department of Economics, Universitat Pompeu Fabra, Barcelona, Spain

5
Two problems with variational expectation maximisation
for time series models
Richard Eric Turner and Maneesh Sahani
5.1
Introduction
Variational methods are a key component of the approximate inference and learning
toolbox. These methods ﬁll an important middle ground, retaining distributional infor-
mation about uncertainty in latent variables, unlike maximum a posteriori methods, and
yet generally requiring less computational time than Markov chain Monte Carlo meth-
ods. In particular the variational expectation maximisation (vEM) and variational Bayes
algorithms, both involving variational optimisation of a free-energy, are widely used in
time series modelling. Here, we investigate the success of vEM in simple probabilistic
time series models. First we consider the inference step of vEM, and show that a con-
sequence of the well-known compactness property of variational inference is a failure
to propagate uncertainty in time, thus limiting the usefulness of the retained distribu-
tional information. In particular, the uncertainty may appear to be smallest precisely when
the approximation is poorest. Second, we consider parameter learning and analytically
reveal systematic biases in the parameters found by vEM. Surprisingly, simpler variational
approximations (such as mean-ﬁeld) can lead to less bias than more complicated structured
approximations.
5.2
The variational approach
We begin this chapter with a brief theoretical review of the variational expectation max-
imisation algorithm, before illustrating the important concepts with a simple example
in the next section. The vEM algorithm is an approximate version of the expectation
maximisation (EM) algorithm [4]. Expectation maximisation is a standard approach to
ﬁnding maximum likelihood (ML) parameters for latent variable models, including hid-
den Markov models and linear or non-linear state space models (SSMs) for time series.
The relationship between EM and vEM is revealed when EM is formulated as a vari-
ational optimisation of a free-energy [5, 11]. Consider observations collected into a
set Y, that depend on latent variables X and parameters θ. We seek to maximise the
likelihood of the parameters, log p(Y|θ). By introducing a new distribution over the
latent variables, q(X), we can form a lower bound on the log-likelihood using Jensen’s

Variational expectation maximisation for time series
105
inequality,
log p(Y|θ) = log
Z
dX p(Y, X|θ) = log
Z
dX p(Y, X|θ)q(X)
q(X)
≥
Z
dX q(X) log p(Y, X|θ)
q(X)
= F (q(X), θ).
The lower bound is called the free-energy. The free-energy is smaller than the log-
likelihood by an amount equal to the Kullback–Leibler (KL) divergence between q(X) and
the posterior distribution of the latent variables, p(X|Y, θ),
F (q(X), θ) =
Z
dX q(X) log p(X|Y, θ)p(Y|θ)
q(X)
= log p(Y|θ) −
Z
dX q(X) log
q(X)
p(X|Y, θ)
= log p(Y|θ) −KL(q(X)||p(X|Y, θ)).
This expression shows that, for ﬁxed θ, the optimum value for q is equal to
p(X|Y, θ), at which point the KL divergence vanishes and the free-energy equals
the log-likelihood. Thus, alternate maximisation of F (q, θ) with respect to q (the E-
step) and θ (the M-step) will eventually ﬁnd parameters that maximise the likelihood
locally.
The EM algorithm is widely used to ﬁnd ML parameter estimates. However, in many
models calculation of this posterior is intractable. For example, it is often impossible to
ﬁnd an analytic form for p(X|Y) because the normalising constant involves an intractable
integral. Another common source of intractability arises in models in which the number of
latent variables is very large. For instance, a model with K binary latent variables generally
requires a posterior distribution over all 2K possible states of those variables. For even
moderately large K this results in a computational intractability.
One possible method of side-stepping these intractabilities is to use the vEM approach
[8] which is to instead optimise q restricted to a class of distributions Q, within which the
minimum of the KL divergence can tractably be found1
qvEM(X) = arg min
q(X)∈Q
KL(q(X)||p(X|Y, θ)) = arg min
q(X)∈Q
Z
dX q(X) log
q(X)
p(X|Y, θ).
(5.1)
The optimal q is called the variational approximation to the posterior. Constrained optimi-
sation of q now alternates with optimisation of θ to ﬁnd a maximum of the free-energy,
though not necessarily the likelihood. The optimal parameters are taken to approximate the
ML values.
There are two main ways in which q can be restricted to a class of tractable distribu-
tions Q. The ﬁrst method is to specify a parametric form for the approximating distribution,
q(X) = qγ(X). A common choice is a Gaussian in which case the variational parameters, γ,
are the mean and the covariance. The E-Step of vEM then amounts to minimising the KL
divergence with respect to the parameters of the approximating distribution,
qvEM = arg min
γ
KL(qγ(X)||p(X|Y, θ)).
1Other variational bounds may also be used in learning (see e.g. [6]). However the term variational EM is
generally reserved for the free-energy bound that we discuss in this chapter.

106
Richard Eric Turner and Maneesh Sahani
The second method is to deﬁne the class Q to contain all distributions that factor over
disjoint sets Ci of the latent variables in the problem,
q(X) =
IY
i=1
qi(xCi).
For example, if each latent variable appears in a factor of its own, the approximation is
called mean-ﬁeld,
qMF(X) =
IY
i=1
qi(xi).
Partial factorisations, which keep some of the dependencies between variables are called
structured approximations. Generally, these methods which rely on factored classes may
be more powerful than using a pre-speciﬁed parametric form, as the optimal analytic form
of the factors may often be obtained by direct optimisation of the free-energy. To ﬁnd this
form we solve for the stationary points of a Lagrangian that combines the free-energy with
the constraint that each factor is normalised. With respect to a factor qi(xCi) we have
δ
δqi(xCi)
F (q(X), θ) −
IX
i=1
λi
 Z
dxCiqi(xCi) −1
!= 0,
where the λi are Lagrange multipliers. Taking the functional derivative, and solving, we
obtain
qi(xCi) ∝exp

log p(Y, X|θ)Q
j,i qj(xC j)

.
(5.2)
This set of equations, one for each factor, may be applied iteratively to increase the free-
energy. The procedure is guaranteed to converge, as the free-energy is convex in each of
the factors qi(xCi) [3].
5.2.1
A motivating example
Let us illustrate the EM and vEM algorithms described above by applying them to a simple
model. The same example will also serve to motivate the problems which are addressed
later in this chapter. In the model a one-dimensional observation, y, is generated by adding
a zero-mean Gaussian noise variable with variance σ2
y to a latent variable, x, itself drawn
from a zero-mean Gaussian, but with unit variance, that is
p(x) = Norm(x; 0, 1),
p(y|x, σ2
y) = Norm(y; x, σ2
y).
The model may be viewed as a very simple form of factor analysis with a one-dimensional
observation and one factor. There is only one parameter to learn: the observation
noise, σ2
y. For tutorial purposes, we consider exact maximum-likelihood learning of this
parameterfrom a single data-point. In fact, it is a simple matter to calculate the likelihood
of the observation noise, p(y|σ2
y) = Norm(y; 0, σ2
y + 1), and therefore this quantity could
be optimised directly to ﬁnd the maximum-likelihood estimate. However, an alternative
approach is to use the EM algorithm. The EM algorithm begins by initialising the observa-
tion noise. Next, in the E-Step, the approximating distribution q is updated to the posterior
distribution over the latent variables given the current value of the parameters, that is,
q(x) = p(x|y, σ2
y) = Norm
x;
y
1 + σ2y
,
σ2
y
1 + σ2y
.

Variational expectation maximisation for time series
107
Then, in the M-Step, the observation noise is updated by maximising the free-energy with
respect to the parameter, which has a closed-form solution, σ2
y = y2 −2y⟨x⟩q + ⟨x2⟩q.
The E- and M-Step updates are then iterated, and this amounts to coordinate ascent of the
free-energy (with respect to the distribution and then to the parameter) as illustrated in the
upper left panel of Fig. 5.1(a). Moreover, as the free-energy is equal to the log-likelihood
after each E-Step has been performed (see Fig. 5.1(b)), the algorithm converges to a local
optimum of the likelihood.
An alternative to exact ML learning is to use the vEM algorithm to return approximate
ML estimates. This requires the q distribution to be restricted to a particular class. As
factored approximations are not an option for this one-dimensional model, a parametric
restriction is considered. An instructive constraint is that the approximating distribution is
a Gaussian with a ﬂexible mean, but with a ﬁxed variance. In the E-Step of vEM, the mean
is set to minimise the KL divergence, which occurs when it is equal to the posterior mean.
Therefore,
qµq(x) = Norm(x; µq, σ2
q) where
µq =
y
1 + σ2y
and
σ2
q = const.
The M-Step of vEM is identical to the M-Step of EM, but the expectations are taken
with respect to the new distribution. As a result the free-energy is no longer pinned to
the log-likelihood after an E-Step and therefore vEM is not guaranteed to converge to a
local optimum of the likelihood. In fact, for the model considered here, the vEM estimate
is biased away from the maximum in the likelihood, towards regions of parameter space
where the variational bound is tightest (see Fig. 5.1). One of the main questions considered
in this chapter is to what extent such biases are a general feature of vEM.
5.2.2
Chapter organisation
The motivating example described above is a simple one, but it indicates that parameter
estimates from EM and vEM can be quite diﬀerent. However, it is unclear whether simi-
lar biases would arise for more realistic models, and in particular in those for time series.
Moreover, the example involved estimating one parameter from one observation. A com-
plete analysis should also compare EM and vEM on large datasets. After all, it is well
known that maximum-likelihood estimators can perform poorly when a large number of
parameters have to be estimated from a small dataset, thus the discrepancy between EM
and vEM noted above is not necessarily concerning. Of particular interest is the behaviour
of vEM in the limit of inﬁnite data. Maximum-likelihood estimators are often consistent,
meaning that they converge to the true parameters in this limit. Do vEM estimators inherit
this property? The motivating example indicates that a key determinant is the parameter
dependence of the tightness of the free-energy bound, given by KL(q(x)|p(y|x, θ)), and
whether this is signiﬁcant in comparison to the peak in the likelihood. The size of this
contribution to the free-energy is studied in a simple setting in Section 5.4. One intriguing
possibility is that the best approximations for learning are not necessarily those that yield
the tightest bounds, but rather those in which the tightness of the bounds depends least on
the parameters. Evidence that such an eﬀect exists is provided in Section 5.4.4 and further
investigation, in Section 5.4.6, reveals that it is fairly common.
Before analysing the biases in parameters learned using vEM, we consider the E-Step
of vEM in isolation. It is well known that variational approximations, like those derived in
the vEM E-Step, tend to be compact [10]. That is, variational approximations tend to have
a smaller entropy than the true distribution. The evidence for this folk-theorem is reviewed

108
Richard Eric Turner and Maneesh Sahani
0
0.1
0.2
0.3
0.4
0.5
µq
0.5
1
−0.55
−0.5
−0.45
−0.4
σy
2
objective
0.5
1
σy
2
−0.55
−0.5
−0.45
−0.4
exact EM
Iteration 1
Iteration 2
Iteration 10
0.5
1
−0.55
−0.5
−0.45
−0.4
σy
2
variational EM
0.5
1
σy
2
0.5
1
σy
2
(b)
(a)
Figure 5.1 Schematics of EM and vEM using the model described in the text where the observation takes the
value y = 0.4. (a) Top Left: Thin black curves are the contours of the free-energy, FEM(q, σ2
y), for exact EM
as a function of the observation noise (σ2
y, abscissa) and the mean of the posterior distribution (µq, ordinate).
The variance of the approximating distribution, σ2
q, is set to the optimal value at each point. The thick grey line
indicates the optimal choice for µq i.e. the mean of the posterior distribution p(x|y, σ2
y). Ten updates using the EM
algorithm are shown (thick black lines). Each update consists of an E-Step, which moves vertically to the optimal
setting of µq (thick grey line), and an M-Step, which moves horizontally to the optimal setting of σ2
y. By iterating
these steps the algorithm converges via coordinate ascent to the optimum of the free-energy, which is also the
optimum of the likelihood. Bottom Left: Log-likelihood of the observation noise. The value of the log-likelihood
corresponds to the contour values along the thick grey line in the plot above. Top Right: Contours of the free-
energy, FvEM(q, σ2
y), for vEM (black lines) in which the variance of the approximating distribution is ﬁxed to the
value σ2
q = 0.4. The position of the optimum has shifted to a larger value of the observation noise and the vEM
algorithm converges onto this optimum (thick black lines). Bottom Right: The optimal free-energy (thick grey
line) is a lower bound on the log-likelihood of the observation noise (thick black line). The value of the free-energy
corresponds to the contour values along the thick grey line in the plot above. (b) Top Left: Schematic showing the
ﬁrst M-Step for exact EM. After an initial E-Step the free-energy, FEM(q1, σ2
y), (thick grey line) is tight to the log-
likelihood (thick black curved line) at the current value of the parameters (indicated by the vertical black line). In
the M-Step q is ﬁxed, and the optimal parameters are found (thick vertical grey line). This corresponds to the ﬁrst
horizontal line in the top left subplot of A. Top Middle: Schematic showing the second M-Step of exact EM. After
the second E-Step, the updated free-energy, FEM(q2, σ2
y), (thick, dark grey line) is tight to the log-likelihood (thick
black line) at the current value of the parameters (indicated by the thick black vertical line). The old free-energy
is shown for reference (thick, light grey line). The result of the second M-Step is indicated by the thick vertical
grey line. Top Right: Schematic showing the free-energy, FEM(q10, σ2
y), (thick grey line) after 10 iterations. The
optimum is clearly close to that of the log-likelihood (thick black line). Bottom Left: Schematic showing the
ﬁrst M-Step for vEM. As compared with the panel above, the free-energy, FvEM(q(µq1, σ2
q1), σ2
y), (thick grey line)
is not tight to the log-likelihood (thick black line). Bottom Middle: Schematic showing the second M-Step for
vEM. Bottom Right: Schematic showing the free-energy after 10 iterations, FvEM(q(µq10, σ2
q10), σ2
y), (thick grey
line). The optimum clearly lies to the right of the optimum of the log-likelihood (thick black line). It is biased to
where the variational approximation is tightest.
in the next section with particular emphasis on the relevance to time series modelling. In
Section 5.3.3, we show that a consequence of compactness in mean-ﬁeld approximations
is a complete failure to propagate uncertainty between time steps, this makes the popular
mean-ﬁeld approximations most over-conﬁdent exactly when they are poorest.
Both the compactness and parameter learning biases are exempliﬁed using very simple
time series models, although the conclusions are likely to apply more generally.

Variational expectation maximisation for time series
109
5.3
Compactness of variational approximations
In this section we consider approximating a known distribution, p(x), with a simpler one,
q(x), by minimising the variational KL divergence, KL(q(x)||p(x)). This operation forms
the E-Step of vEM (see Eq. (5.1)) and so its behaviour has implications on how the full
algorithm behaves. Before considering a number of instructive examples, it is immediately
clear from the form of the variational KL that at any point where the true density is zero, the
approximation must also be zero (otherwise the KL divergence will be inﬁnity). A conse-
quence of this fact is that when a distribution which has two modes that are separated by a
region of zero density is approximated by a unimodal distribution, then the approximation
will model just one of the modes, rather than averaging across them. This is one example
of a general tendency for variational approximations to have a smaller entropy than the
target distribution. This section explores this so-called compactness property of variational
approximations, before considering the implications for time series modelling.
5.3.1
Approximating mixtures of Gaussians with a single Gaussian
As explained above, when the true distribution contains two modes which are separated
by an intermediate region of zero density, the approximation will be compact. However,
it is unclear what happens when the intermediate region does not dip to zero. In order to
investigate this situation, consider approximating a one-dimensional mixture of Gaussians
with a single Gaussian, where
p(x|θ) =
K
X
k=1
πkNorm(x; µk, σ2
k),
q(x) = Norm(x; µq, σ2
q).
In Fig. 5.2 a number of examples are shown for a range of diﬀerent parameter choices for
the mixture. As expected, for mixtures with two clearly deﬁned modes (right-hand col-
umn of Fig. 5.2), the approximation matches the mode with the largest variance, rather
than averaging across both of them [2]. In these cases the entropy of the approximation is
less than that of the true distribution. However, for intermediate distributions, in which the
modes are joined by a signiﬁcant bridge of probability-density, the variational approxima-
tion does average across the modes and in some cases the entropy of the approximation is
larger than the true distribution. The conclusion is that the compactness property is a useful
guide to the behaviour of variational methods when applied to highly multimodal distribu-
tions, but that there are examples when variational methods are not compact (as measured
by their entropy relative to that of the true distribution). Variational approximations com-
monly used in clustering are an example of the former [2], but variational approximations
to independent component analysis can result in the latter [12].
5.3.2
Approximating a correlated Gaussian with a factored Gaussian
The examples above indicate how compactness operates for univariate distributions, when
the approximating distribution is restricted to a particular parametric form. Next, we con-
sider approximating a bivariate distribution using the mean-ﬁeld approximation. The true
distribution is a zero-mean, correlated Gaussian distribution with principal axes oriented in
the directions e1 =
1√
2[1, 1]T and e2 =
1√
2[1, −1]T with variances σ2
1 and σ2
2 respectively
[10], that is
p(x1, x2|Σ) = Norm (x1, x2; 0, Σ) ,
Σ = σ2
1e1eT
1 + σ2
2e2eT
1 .

110
Richard Eric Turner and Maneesh Sahani
Hq−Hp = 0.00
Hq−Hp = −0.00
Hq−Hp = −0.01
Hq−Hp = −0.03
Hq−Hp = −0.03
Hq−Hp = −0.01
Hq−Hp = −0.00
Hq−Hp = −0.00
Hq−Hp = 0.03
Hq−Hp = 0.09
Hq−Hp = 0.08
Hq−Hp = 0.09
Hq−Hp = 0.1
Hq−Hp = 0.13
Hq−Hp = 0.2
Hq−Hp = −0.66
Hq−Hp = −0.51
Hq−Hp = −0.35
Hq−Hp = 0.25
Hq−Hp = 0.28
Hq−Hp = −0.69
Hq−Hp = −0.55
Hq−Hp = −0.4
Hq−Hp = −0.26
Hq−Hp = −0.11
increasing σ 2
1/σ 2
2
increasing Δ μ
Figure 5.2 Each panel shows a variational approximation to a true distribution. The true distribution is a mixture
of two Gaussians (grey line) and the approximating family is a Gaussian (black line). The parameters of the
mixture were set so that each component has equal weight (π1 = π2 = 1
2). The diﬀerence between the means of
the mixture components increases from the left column of panels (where µ1−µ2 = 0) to the right column of panels
(where µ1 −µ2 = 10). The ratio of the variances increases from the bottom row of panels (where σ2
1/σ2
2 = 1)
to the top row of panels (where σ2
1/σ2
2 = 10). The smaller of the two variances is ﬁxed, σ2
2 = 1. The bottom
left is therefore a mixture of two Gaussians with the same mean and variance, and this is another Gaussian. The
approximation is therefore exact and the entropy diﬀerence, shown at the top of each panel, is zero. In general the
entropy of the approximation can be less than (normal font) or greater than (bold font) the true entropy.
This correlated Gaussian is approximated in the mean-ﬁeld approach by a factorised dis-
tribution, q(x1, x2) = q(x1)q(x2). By considering the ﬁxed points of Eq. (5.2) the optimal
updates are found to be
q(xi) = Norm
xi; 0, 1
2
σ2
1σ2
2
σ2
1 + σ2
2
.
That is, the optimal factored approximating distribution is a spherical Gaussian that has a
precision which is equal to the diagonal elements of the precision matrix of the original
Gaussian (that is, (Σ−1)i,i). This is an example of the more general result that variational
approximations between two Gaussians match precisions, which will be seen again later
in the chapter. Consider now the behaviour of the variational approximation in the case
where the variance of the two components is very diﬀerent (e.g. σ2
1 > σ2
2). The width
of the approximating distribution becomes σ2
2/2, and therefore independent of the longer
lengthscale. In this sense the approximation is becoming compact; it matches the smallest
lengthscale in the posterior. In the next subsection, this result will be rediscovered from the
contrasting perspective of mean-ﬁeld inference in time series models.

Variational expectation maximisation for time series
111
5.3.3
Variational approximations do not propagate uncertainty
Fully factored variational approximations (so called mean-ﬁeld approximations) have been
used for inference in time series models as they are fast and yet still return estimates of
uncertainty in the latent variables [1]. Here, we show that in a simple model, the variational
iterations fail to propagate uncertainty between the factors, rendering these estimates of
uncertainty particularly inaccurate in time series models [14].
We consider a time series model with a single latent variable xt at each time step drawn
from a ﬁrst-order autoregressive prior with coeﬃcient λ and innovations variance σ2,
p(xt|xt−1) = Norm(xt; λxt−1, σ2).
(5.3)
The marginal mean of this distribution is zero and the marginal variance is σ2
∞=
σ2
1−λ2 . As
the time series of greatest interest tend to be those which exhibit strong temporal structure,
we consider models in which the autoregressive parameter λ is close to unity.2 The observed
variables yt depend only on the latent variable at the corresponding time step. The precise
form of p(yt|xt) is not important here.
If we choose a mean-ﬁeld approximating distribution which factorises over time,
q(x1:T) = QT
t=1 q(xt), the update for the latent variable at time t follows from Eq. (5.2)
q(xt) = 1
Z p(yt|xt) exp(⟨log p(xt|xt−1)p(xt+1|xt)⟩q(xt−1)q(xt+1))
= 1
Z′ p(yt|xt)Norm
 
xt;
λ
1 + λ2 (⟨xt−1⟩+ ⟨xt+1⟩) ,
σ2
1 + λ2
!
= 1
Z′ p(yt|xt)qprior(xt).
That is, the variational update is formed by combining the likelihood with a variational
prior-predictive qprior(xt) that contains the contributions from the latent variables at the
adjacent time step. This variational prior-predictive is interesting because it is identical to
the true prior-predictive when there is no uncertainty in the adjacent variables. As such,
none of the (potentially large) uncertainty in the value of the adjacent latent variables is
propagated to q(xt), and the width of the variational predictive is consequently narrower
than the width of state-conditional distribution p(xt|xt−1) (compare to Eq. (5.3)).3
Temporally factored variational methods for time series models will thus generally
recover an approximation to the posterior which is narrower than the state-conditional
distribution. As the whole point of time series models is that there are meaningful depen-
dencies in the latents, and therefore the state-conditional often has a small width, the
variational uncertainties may be tiny compared to the true marginal probabilities (see
Fig. 5.3). Thus, the mean-ﬁeld approach is not all that diﬀerent from the ‘zero-temperature
EM’ or maximum a posteriori (MAP)-based approach (in which the joint probability of
observed data and latent variables is optimised alternately with respect to the latent vari-
ables – with no distributional information – and the parameters), except that we ﬁnd the
mean of the posterior rather than a mode. In the next section, it will be shown that this
2In fact the eﬀective time scale of Eq. (5.3) is τef f = −1/ log(λ) and so a change in λ from 0.9 to 0.99 is
roughly equivalent to a change from 0.99 to 0.999. This is important when the size of the biases in the estimation
of λ are considered (Section 5.4.3).
3This problem only gets worse if the prior dynamics have longer dependencies, e.g. if p(xt|xt−1:t−τ) =
Norm(Pτ
t′=1 λt′xt−t′, σ2), in which case the variational prior-predictive has a variance
σ2
1+Pτ
t′=1 λ2
t′ .

112
Richard Eric Turner and Maneesh Sahani
−1
0
1
−1
−0.5
0
0.5
1
xt
xt−1
λ = 0.9
−1
0
1
λ = 0.99
−1
0
1
λ = 0.999
Figure 5.3 Compactness in mean-ﬁeld approximations for time series. The average true prior-predictive (black
ellipses, showing the probability density contour at one standard deviation) is shown together with the mean-ﬁeld
approximations (grey circles, also showing the probability density contour at one standard deviation), for three
settings of λ. The marginal variance of the true prior-predictive is 1. The marginal variance of the mean-ﬁeld
approximation is (1 −λ2)/(1 + λ2) which is tiny for typical values of λ in time series models. Notice that this
example is equivalent to the previous example in Section 5.3.2 involving a bivariate Gaussian when, σ2
1 =
σ2
1−λ
and σ2
2 =
σ2
1+λ.
does have some advantages over the MAP approach, notably that pathological spikes in the
likelihood can be avoided.
In conclusion, although variational methods appear to retain some information about
uncertainty, they fail to propagate this information between factors. In particular, in time
series with strong correlations between latents at adjacent times, the mean-ﬁeld variational
posterior becomes extremely concentrated, even though it is least accurate in this regime.
An ideal distributional approximation would perhaps behave in the opposite fashion,
returning larger uncertainty when it is likely to be more inaccurate.
5.4
Variational approximations are biased
In the last section we showed that variational approximations under-estimate the uncer-
tainties in inference. We will now investigate how these inaccuracies aﬀect the parameter
estimates returned by vEM. This question is important in many contexts. For example,
scientiﬁc enquiry is often concerned with the values of a parameter, to substantiate claims
like ‘natural scenes vary slowly’ or ‘natural sounds are sparse’.
What makes for a good variational approximation in this case? The instant reaction is
that the free-energy should be as tight to the log-likelihood as possible. That is, the optimal
KL divergence at each parameter setting,
KL*(θ) = arg max
q(x)
KL(q(X)||p(X|Y, θ)),
should be as small as possible for all θ. However, the conclusion from the motivating exam-
ple in Section 5.2.1, is that from the perspective of learning it is more important to be
equally tight everywhere. In other words it is more important for the KL-term to be as
parameter-independent as possible: If KL*(θ) varies strongly as a function of the param-
eters, this can shift the peaks in the free-energy away from the peaks in the likelihood,

Variational expectation maximisation for time series
113
toward the regions where the bound is tighter. This perspective explains a previous obser-
vation whereby variational Bayes typically prunes out too many components of a mixture
model [9].
We now illustrate this eﬀect in a linear SSM and show that consequences can include
mis-estimation of the time constant with which the latent variables evolve, under-estimation
of the overlap of emission weights, and unwanted pruning of emission weights. Moreover,
we show that the mean-ﬁeld approximation can actually have less severe parameter-
dependent biases than two structural approximations, and can therefore lead to better vEM
parameter estimates, even though it is less tight everywhere. We also show that the biases
in parameter estimates increase considerably with the number of parameters.
5.4.1
Deriving the learning algorithms
In the following we ﬁrst introduce an elementary SSM, for which we can ﬁnd the exact
log-likelihood, log p(y|θ). We then examine the properties of a set of diﬀerent variational
learning algorithms. This set comprises a mean-ﬁeld approximation, two diﬀerent struc-
tural approximations and zero-temperature EM. This ﬁnal approximation can be thought
of as vEM where the approximating distributions are delta functions centred at the MAP
estimates [11]. The analysis of these schemes proceeds as follows: First the optimal E-Step
updates for these approximations are derived. Second, it is shown that, as the SSM is a
simple one, the free-energies and the zero-temperature EM objective function can be writ-
ten purely in terms of the parameters. That is, maxq(x) F (θ, q(x)) and maxX log p(Y, X|θ)
have closed-form solutions, and do not require iterative updates to be computed as is usu-
ally the case. Thus, we can study the relationship between the peaks in the log-likelihood
and the peaks in the free-energies and zero-temperature EM objective function, for any
dataset. This is analogous to the lower right-hand panel of Fig. 5.1(a) for the motivating
example.
Consider an SSM which has two latent variables per time step, two time steps and two-
dimensional observations. We take the priors on the latent variables to be linear-Gaussian,
and the observations are given by summing the weighted latents at the corresponding time
step and adding Gaussian noise, that is
p(xk,1) = Norm
 
xk,1; 0,
σ2
x
1 −λ2
!
,
p(xk,2|xk,1) = Norm

xk,2; λxk,1, σ2
x

,
p(yd,t|x1,t, x2,t) = Norm
yd,t;
2
X
k=1
wd,kxk,t, σ2
y
.
This deﬁnes a joint Gaussian over the observations and latent variables. From this we can
compute the likelihood exactly by marginalisation. Deﬁning the vector of observations,
y = [y11, y21, y12, y22]T, and the matrix Md,d′ = P2
k=1 wd,kwd′,k, the likelihood is given by
p(y1:2,1:2|θ) = Norm(y; 0, Σy),
Σy = Iσ2
y +
σ2
x
1 −λ2
"
M
λM
λM
M
#
.
The posterior distribution over the latent variables is also Gaussian, and is given by
p(x|y) = Norm(µx|y, Σx|y), where the vector of latent variables is x = [x11, x21, x12, x22]T.

114
Richard Eric Turner and Maneesh Sahani
In order to ease notation, we deﬁne weight vectors and matrices
w1 =
"w11
w21
#
= |w1|
"cos(φ1)
sin(φ1)
#
,
w2 =
"w12
w22
#
= |w2|
"cos(φ2)
sin(φ2)
#
,
W =
"w11
w12
w21
w22
#
.
Then, the covariance and mean of the posterior distribution are given by
Σ−1
x|y =

|w1|2
σ2y + 1
σ2x
wT
1 w2
σ2y
−λ
σ2x
0
wT
1 w2
σ2y
|w2|2
σ2y + 1
σ2x
0
−λ
σ2x
−λ
σ2x
0
|w1|2
σ2y + 1
σ2x
wT
1 w2
σ2y
0
−λ
σ2x
wT
1 w2
σ2y
|w2|2
σ2y + 1
σ2x

,
µx|y = 1
σ2y
Σx|y
" W
0
0
W
#
y.
The posterior is correlated through time because of the linear-Gaussian prior, and correlated
across chains because of explaining away.4 The correlations through time increase as the
prior becomes ‘slower’ (|λ| increases) and less noisy (σ2
x decreases). The correlations across
chains increase as the magnitude of the weights increases (|wd|2), and the angle between
the weights (φ1 −φ2) or the observation noise (σ2
y) decreases.
We now derive the optimal E-Step for four diﬀerent approximations. The ﬁrst three
approximations, which provide uncertainty estimates, are the fully factored mean-ﬁeld
approximation (qMF), the approximation with factorisation over chains but not time (qFC),
and the approximation with factorisation over time but not chains (qFT), as shown in the
following table:
factored over time
unfactored over time
chains factored
qMF(x)
qFC(x)
chains unfactored
qFT(x)
p(x|y) = q(x)
The factorisations are therefore
qMF(x) = q(1)
MF(x11)q(2)
MF(x12)q(3)
MF(x21)q(4)
MF(x22),
qFC(x) = q(1)
FC(x11, x12)q(2)
FC(x21, x22),
qFT(x) = q(1)
FT(x11, x21)q(2)
FT(x12, x22).
The optimal E-Step updates for these three distributions are found by minimising the vari-
ational KL. Each factor is found to be Gaussian, with mean and precision that match
the corresponding elements in µx|y and Σ−1
x|y. The fourth and ﬁnal approximation is zero-
temperature EM (qMAP), for which the E-Step is given by the MAP estimate for the latent
variables for the current parameter setting. As the posterior is Gaussian, the mode and the
mean are identical and so the MAP estimates are identical to the variational values for the
means.
The next step is to compute the free-energies. In the ﬁrst three cases, the Gaussianity
of the posterior and the uncertainty-preserving variational approximations enables the KL
4Explaining away is the name given to the phenomenon in probabilistic modelling where the observation of an
eﬀect of two possible independent causes, leads to (anti-)correlation in the posterior distribution over those two
causal latent variables. Suppose that either latent may take on a value that could account for the observation. Then
if one does so, it ‘explains away’ the observed eﬀect, and the observed data no longer constrains the other. Thus,
conditioned on the observation, the distribution over each latent depends on the value of the other, even if there
was no such dependence in the prior.

Variational expectation maximisation for time series
115
divergences to be calculated analytically as
KLi

A
Y
a=1
q(a)
i (xa)||p(x|y)
= 1
2 log
QA
a=1 det

Σ(a)
i

det

Σx|y

.
That is, the KL divergence is the log-ratio of the volume of the approximation (as measured
by the matrix determinants) to the volume of the true posterior. It should be noted that the
whole point of variational methods is that this quantity is usually intractable to compute,
and that it is only because the example is very simple that it is not the case here. Using this
expression we ﬁnd
KL*
MF = 1
2 log

σ2
y + |w1|2σ2
x
2 
σ2
y + |w2|2σ2
x
2 /γ,
KL*
FC = 1
2 log

σ2
y + |w1|2σ2
x
2 −λ2σ4
y
 
σ2
y + |w2|2σ2
x
2 −λ2σ4
y

/γ,
KL*
FT = 1
2 log

σ4
x|w1|2|w2|2 sin2(φ1 −φ2) +

|w1|2 + |w2|2
σ2
xσ2
y + σ4
y
2 /γ,
(5.4)
where
γ =

|w1|2 + |w2|2
σ2
xσ2
y + σ4
x|w1|2|w2|2 sin2(φ1 −φ2) + (1 + λ2)σ4
y
2
−

λσ2
yσ2
x

|w1|2 + |w2|2
+ 2λσ4
y
2 .
In the zero-temperature EM approximation, since the KL divergence between a Gaus-
sian and a delta function is inﬁnite, the KL term is discarded and the log-joint is used as a
pseudo free-energy. To ease notation, in what follows KLi = KL*
i .
5.4.2
General properties of the bounds: a sanity check
We now verify that these results match our intuitions. For example, as the mean-ﬁeld
approximation is a subclass of the other approximations, it is always the loosest of the
bounds, KLMF > KLFC, KLFT > 0, which is borne out by the expressions. Furthermore,
qFT becomes looser than qFC when temporal correlations dominate over the correlations-
between chains. For instance, if the weights have identical magnitude, |w1| = |w2| = |w|,
then KLFT > KLFC when explaining away (EA) becomes more important than temporal
correlation (TC) in the posterior,
EA
TC < 1, where EA = | cos(φ1 −φ2)||w|2
σ2y
and TC = |λ|
σ2x
.
Moreover, qFC is equivalent to the mean-ﬁeld approximation, KLMF = KLFC, when there
are no temporal correlations, λ = 0 or σ2
x = ∞, and in this case the true posterior matches
qFT, KLFT = 0. Similarly, qFT is equivalent to the mean-ﬁeld approximation when the
observation noise is inﬁnity σ2
y = ∞, and here qFC is exact (KLFC = 0). Finally we note that,
as qFT is the only approximation which captures cross-chain correlations due to explaining
away, it is the only approximation which is dependent on the relative angle between the
weights.
Having veriﬁed that the expressions for the KL divergences appear reasonable, we
can now consider how the maxima in the log-likelihood relate to the maxima in the free-
energies. Unfortunately, there is no closed-form solution for the location of these maxima,

116
Richard Eric Turner and Maneesh Sahani
but in the simple examples which follow, the free-energies and likelihoods can be visu-
alised. In general, we will be concerned with the consistency of the variational estimators,
which means the behaviour when we have a large number of observations from the same
time series. In this case the average likelihood becomes
lim
N→∞
1
N log p(y1:N|Σy) = −1
2 log det Σy −1
2Σ−1
y
lim
N→∞
1
N
N
X
n=1
ynyT
n
= −1
2 log det Σy −1
2Σ−1
y
D
yyTE
.
When the data are drawn from the forward model,
D
yyTE
can be computed analytically.
In all cases the ML estimators are found to be consistent, and therefore equal to the true
parameters in the limit of inﬁnite data.
Although the model is simple, it has seven parameters and this means there is a great
number of possible learning scenarios, ranging from learning one parameter with the others
ﬁxed to learning all parameters at once. In the following we highlight several illustrative
examples in order to elucidate the general properties of the variational approach. First we
consider learning a single parameter (the dynamical parameter, the observation noise, the
innovations noise, the orientation of one of the weights and the magnitude of one of the
weights) with the other parameters set to their true values. This will allow us to develop
some intuition about the ways in which diﬀerent approximations lead to diﬀerent biases
in the parameter estimates. In this case, the log-likelihood and free-energies are easy to
visualise; some typical examples are shown in Fig. 5.4 and Fig. 5.5. We then consider how
the bias changes as a function of the true parameters, and observe that there is no univer-
sally preferred approximation, but instead the least biased approximation depends on the
parameter that is being learned and on the value of the true parameters. Next we will study
the bias when learning the dynamic parameter and the observation noise simultaneously,
as this provides a typical example of how the variational approach performs when multi-
ple parameters are learned. The conclusion is that the biases become signiﬁcantly larger as
more parameters are estimated.
5.4.3
Learning the dynamical parameter, λ
We begin by considering learning the dynamical parameter λ, with the other parameters
ﬁxed to their true values. In order to ensure that the eﬀects of explaining away are properly
considered the weights are set to be identical with unit magnitude (w1 = w2 and |wk|2 = 1).
As the magnitude of the dynamical parameter increases, so does the correlation in the
posterior between successive latent variables in the same chain (xk,1 and xk,2). This means
that qFT, which factorises over time, results in a looser variational bound as the magni-
tude of λ increases (KLFT increases, Eq. (5.4)). Furthermore, as the correlation between
latents in the same chain increases, (xk,1 and xk,2), so does the correlation between x11 and
x22 (because explaining away is propagated through time by the dynamics). This means,
somewhat surprisingly, that qFC which does not factorise over time, but over chains, also
becomes looser as the magnitude of λ increases. That is, KLFC also increases with the mag-
nitude of λ. In both cases, this λ-dependence in the tightness of the bound means that the
corresponding variational free-energies peak at lower values of λ than the likelihood, and
therefore both approximations yield under-estimates (see [13] for a similar result).
The mean-ﬁeld approximation suﬀers from both of the aforementioned eﬀects, and it
is therefore looser than both. However, with regard to their dependence on λ, KLMF and

Variational expectation maximisation for time series
117
−1
−0.5
0
0.5
1
−10
−5
log p(X,Y)
0.6
0.7
0.8
0.9
−10
−5
log p(X,Y)
0
2
4
6
−6
 0
 4
0
0.5
1
−5
0
5
y
2
−6
−5
−4
Free energies
log-likelihood
MF
FC
FT
−5
−4.5
−4
−3.5
Free energies
−6
−5
−4
−3
−5
−4.5
−4
−3.5
)
b
(
)a(
Figure 5.4 Biases in the free-energies for learning the dynamical parameter, λ, and the observation noise, σ2
y, of a
simple linear dynamical system. The true parameters were λ = 0.9, σ2
x = 1 −λ2 = 0.19, wT
1 = [1, 0] ,wT
2 = [1, 0]
and σ2
y = 0.43. In both columns (a) and (b), one parameter is learned and the others are set to their true values.
Column (a) shows the results of learning λ, and column (b) shows the results of learning σ2
y. Large panels show
the log-likelihood (thick black line) and the free-energies of the uncertainty preserving methods (FMF by a thick
grey line, FFC by the crosses, and FFT by the circles). Small panels show the zero-temperature EM approach
(qMAP). The maxima of these functions are indicated by the vertical lines. The maximum of the log-likelihood
lies at the true value of the parameters. The bottom two panels show a zoomed in region of the top two panels.
KLFT are equivalent. Consequently qMF and qFT recover identical values for the dynamical
parameter, even though the former is looser. Curiously, the solution from zero-temperature
EM (qMAP) is also identical to those solutions. One of the conclusions from this is that most
severe approximation need not necessarily yield the most biased parameter estimates.
5.4.4
Learning the observation noise, σ2
y, and the dynamical noise, σ2
x
Next we consider learning the observation noise σ2
y, with the other parameters ﬁxed to their
true values. Once again, in order to ensure that the eﬀects of explaining away are properly
considered we set w1 = w2 and |wk|2 = 1.
Decreasing the observation noise increases the correlation between variables at the
same time step, i.e., between x1t and x2t. This means that qFC, which factors over chains,
becomes worse as σ2
y decreases, and therefore KLFC is an increasing function of σ2
y.

118
Richard Eric Turner and Maneesh Sahani
0
50
100
150
−5.8
−5.6
−5.4
−5.2
log p(X,Y)
1−2
1
2
3
4
−6
−5
|w2|
−6.4
−6.2
−6
−5.8
−5.6
−5.4
−5.2
−5
Free Energies
log-likelihood
MF
FC
FT
−7
−6.5
−6
−5.5
)
b
(
)a(
Figure 5.5 Biases in the free-energies for learning the weights of a simple linear dynamical system. The true
parameters are λ = 0.9, σ2
x = 1 −λ2 = 0.19, wT
1 = [1, 0], wT
2 = [cos(π/8), sin(π/8)] and σ2
y = 0.3. In both
columns, (a) and (b), one parameter is learned and the others are set to their true values. (a) Learning the ori-
entation (φ2) of the second weight, wT
2 = [cos(φ2), sin(φ2)]. (b) Learning the magnitude of the second weight,
wT
2 = |w2|[cos(π/8), sin(π/8)]. Large panels show the log-likelihood (thick black line) and the free-energies of the
uncertainty preserving methods (FMF by a thick grey line, FFC by the crosses, and FFT by the circles). Small pan-
els show the zero-temperature EM approach (qMAP). The maxima of these functions are indicated by the vertical
lines. The maximum of the log-likelihood lies at the true value of the parameters.
On the other hand, as the observation process becomes less noisy the hidden states are
more precisely determined by local information, and so correlations between them in the
prior become less important. Thus qFT, which factorises over time but not over chains,
becomes tighter as σ2
y decreases, i.e. KLFT is a decreasing function of σ2
y. We have there-
fore established that KLFC and KLFT have opposite dependencies on σ2
y. As the mean-ﬁeld
approximation shares both of these eﬀects its maximum lies somewhere between the two,
depending on the settings of the parameters. This means that whilst qFT under-estimates
the observation noise, and qFC over-estimates it, the loosest approximation of the three,
the mean-ﬁeld approximation, can actually provide the best estimate, as its peak lies in
between the two. In the next section we will characterise the parameter regime over which
this occurs.
The ﬁnal approximation scheme, zero-temperature EM, behaves catastrophically when
it is used to learn the observation noise, σ2
y. This is caused by a narrow spike in the
likelihood-surface at σ2
y = 0. At this point the latent variables arrange themselves to explain
the data perfectly, and so there is no likelihood penalty (of the sort −1
2σ2y (y1,t −x1,t −x2,t)2).
In turn, this means the noise variance can be shrunk to zero which maximises the remain-
ing terms (∝−log σ2
y). The small cost picked up from violating the prior-dynamics is no
match for this inﬁnity. This is not a very useful solution from either the perspective of

Variational expectation maximisation for time series
119
learning or inference. It is a pathological example of overﬁtting:5 there is an inﬁnitesi-
mal region of the likelihood-posterior surface with an inﬁnite peak. By integrating over the
latent variables, even if only approximately in a variational method for example, such peaks
are discounted, as they are associated with negligible probability mass and so make only
a small contribution to the integral. Thus, although variational methods often do not pre-
serve as much uncertainty information as we would like, and are often biased, by recovering
means and not modes they may still provide better parameter estimates than the catastrophic
zero-temperature EM approach.
Finally we note that learning the dynamical noise σ2
x with the other parameters ﬁxed at
their true values results in a very similar situation: qFC under-estimates σ2
x, and qFT over-
estimates it, while the mean-ﬁeld approximation returns a value in between. Once again
the MAP solution suﬀers from an overﬁtting problem whereby the inferred value of σ2
x is
driven to zero. The fact that learning σ2
y and σ2
x results in similar eﬀects indicates that the
conclusions drawn from these examples are quite general.
5.4.5
Learning the magnitude and direction of one emission weight
Finally we consider learning the emission weights. In order to explicate the various factors
at work it is useful to separately consider learning the orientation of the weight vector
and its magnitude. Consider ﬁrst learning the orientation of one of the weights whilst its
magnitude, and the value of the other parameters in the model, are known and ﬁxed to
their true values (shown in Fig. 5.5). The relative orientation of the pair of weights is the
critical quantity, because this determines the amount of explaining away. If the weights are
orthogonal (φ1−φ2 = π(n+1/2)), there is no explaining away (⟨x1tx2t⟩p(x|y) = 0), and so qFC
is exact and qMF and qFT are equivalent. In contrast, if the weights are parallel (φ1−φ2 = nπ),
explaining away is maximised and qMF and qFC are at their loosest because they do not
model the dependencies between the chains. In this region qFT is also at its loosest (because
it does not capture the ‘diagonal’ correlations ⟨x11x22⟩p(x|y) and ⟨x21x12⟩p(x|y) which are
strongest here). The result is that all the approximations are biased toward settings of the
weights which are more orthogonal than the true setting. The bias in qMF, qFC and qMAP are
equal and can be substantial (see Fig. 5.5 and Fig. 5.6). The bias in qFT is somewhat less as
it captures the correlations induced by explaining away.
Finally, we consider learning the magnitude of the second weight when all other param-
eters are set to their true values (this includes the direction of the second weight). For low
magnitudes, qFC is tightest as the temporal correlations dominate over explaining away,
but for high magnitudes the situation is reversed and qFT is tightest. Consequently, qFC
under-estimates the magnitudes (often severely thereby pruning the weight entirely, see
Fig. 5.5(b)), whilst qFT over-estimates the magnitudes. As the mean-ﬁeld approximation
suﬀers from both eﬀects, its estimates lie between the two and can therefore be less biased.
Once again the MAP solution suﬀers from an over-ﬁtting problem, where the estimated
weight magnitudes blow up to inﬁnity.
5.4.6
Characterising the space of solutions
In the previous section we found examples where the mean-ﬁeld approximation was the
most unbiased (see Fig. 5.4(b) and Fig. 5.5(b)). How typical is this scenario? To answer
this question we look at the extent of the bias in parameter values returned by the four
approximate learning schemes, over a wide range of diﬀerent datasets with diﬀerent true
parameter values. As the likelihood or free-energy surfaces may be multimodal – and we are
5This is the SSM analogue to Mackay’s (2003) so-called KABOOM! problem in soft K-means.

120
Richard Eric Turner and Maneesh Sahani
(a)
0
0.2
0.4
0.6
0.8
qMF
λ
qFC
qFT
MAP
1
2
3
4
0
0.2
0.4
0.6
0.8
λ
σ y
2
1
2
3
4
σ y
2
1
2
3
4
σ y
2
1
2
3
4
σ y
2
0
1
2
3
4
−0.2
−0.4
−0.6
−0.8
−
+
+
−
−
Δ λ
Δ σ y
2
(b)
1
2
3
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
λ
σy
2
qFC
qMF
qFT
Figure 5.6 (a) Biases in inferring a single parameter as a function of σ2
y and λ. Lighter colours indicate a bias of
smaller magnitude. The bias is deﬁned as ∆Θ = ΘINF −ΘML so that over-estimation results in a positive bias.
For all points, σ2
x = 1 −λ2. The columns correspond to the four approximations. Top Row: Bias in estimating λ.
All the schemes return underestimates and so the biases are always negative. Bottom Row: Bias in estimating σ2
y.
The sign of the bias is indicated by the ‘+’ and ‘−’ symbols. (b) The best approximation for ﬁnding σ2
y indicated
by marker-type (qMF grey ﬁlled-circles, qFC black crosses and qFT black open-circles). The black solid line is
r = σ2
x/|λ|σ2
y = 1. Below r qFT is tightest, while above r qFC is tightest.
not interested here in failures of learning due to local optima – the optimal parameters were
found using three diﬀerent optimisation schemes: grid-based search; direct gradient ascent
on the free-energy; and coordinate ascent of the free-energy (or vEM). For the examples of
this section, all three methods returned identical results up to experimental error.
One typical example – the bias in inferring λ for many diﬀerent maximum-likelihood
settings of σ2
y and λ – appears in Fig. 5.6(a,top row). In each case σ2
x was set to its true value,
1 −λ2. The parameter λ is under-estimated in all cases, often substantially (e.g. for qMF,
qFT and qMAP, at high σ2
y and λ values, the bias is almost one). The bias of qFC is always
smaller than that of the other approximations, and thus in this case it is to be preferred
everywhere. However, this simple situation where one of the approximation schemes is
universally superior does not generalise when learning other parameters. For example, the
bias in inferring σ2
y is shown in Fig. 5.6(a, bottom row). As noted in the previous section,
qFC over-estimates the observation noise, whilst qFT and qMAP under-estimate it. The mean-
ﬁeld approximation combines the behaviours of qFC and qFT and therefore under-estimates
in regions where λ and σ2
y are small, and over-estimates in regions where λ and σ2
y are
large. In the intermediate region, these eﬀects cancel and this is the region in which the
mean-ﬁeld approximation is the best approximation. This is shown in Fig. 5.6(b) which
indicates the best approximation to use for inferring the observation noise in diﬀerent parts
of the space. The ﬁgure shows that the mean-ﬁeld solution is to be preferred over a fairly
large part of the space.
Next we consider biases in estimating the weight vectors (see Fig. 5.7). When learning
the vector orientations, qMF, qFC and qMAP turn out to exhibit identical biases. (Indeed, it
is generally true that if the MAP solution does not suﬀer from over-ﬁtting, then it is equal

Variational expectation maximisation for time series
121
0
30
60
90
1/16
1/4
1
4
φ1−φ2
1/16
1/4
1
4
σ 2
y
qMF, qFC, qMAP
qFT
0
0.2
0.4
0.6
0.8
λ
qMF, qFC, qMAP
qFT
0
30
60
90
1/16
1/4
1
4
qMF
σ 2
y
φ1−φ2
0
30
60
90
qFC
φ1−φ2
0
30
60
90
qFT
φ1−φ2
0
10
20
30
40
0
5
10
15
20
0.2
0.4
0.6
0.8
1
1.2
+
−
−
+
qMF
qFC
qFT
(a) Δ φ2
(b) Δ φ2
(c) Δ |w2|2
(d)
Figure 5.7 Parameter dependence of the biases in learning the weights. (a) Biases in learning the relative orienta-
tion (∆φ) of the weights as a function of the true relative orientation (φ1 −φ2) and the observation noise σ2
y. The
magnitude of the weights is unity, |wk| = 1, and the dynamical parameters are set to λ = 0.9 and σ2
x = 1 −λ2. All
of the approximations over-estimate the angular separation of the weights, but qFT is less or equally biased every-
where. (b) Biases in learning the relative orientation of the weights as a function of the true orientation (φ1 −φ2)
and the dynamical parameter, λ. The observation noise is ﬁxed to σ2
y = 0.1 and the state-noise to σ2
x = 1 −λ2. In
this case too qFT is less or equally biased everywhere. (c) Biases in learning the magnitude of the second weight
as a function of the true relative orientation (φ1 −φ2) and the observation noise. The other parameters are set to,
λ = 0.9 and σ2
x = 1 −λ2. The MAP estimate qMAP returns an inﬁnite value for the weights everywhere and is
therefore not shown. (d) The least biased approximation for ﬁnding the magnitude of the weight (indicated by
marker-type; qMF grey ﬁlled-circles, qFC black crosses and qFT black open-circles) as a function of the relative
orientation of the weights and the observation noise. Above the solid line qFC is the tighter approximation, whilst
below it qFT is the tighter approximation.
to the mean-ﬁeld approximation in these Gaussian models.) These approximations do not
model explaining away and thus they are most biased in regions where the true weights are
approximately parallel (φ1 ≈φ2). On the other hand, qFT does capture inter-chain correla-
tions, and thus is superior for any setting of the true parameters.6 When learning the weight
vector magnitudes, qFT is superior in regions where explaining away is large compared
to the temporal correlations, whilst qFC is superior in regions where temporal correlations
6It is common practice to use zero-temperature EM (qMAP) to learn weights in sparse-coding models and
then to make a detailed statistical comparison of the learned weights to biological analogues derived from experi-
ments in visual cortex. The result here – that zero-temperature EM recovers weights which are signiﬁcantly more
orthogonal than the true weights – raises concerns that this practice is seriously aﬀected by biases in the learned
weights.

122
Richard Eric Turner and Maneesh Sahani
0
0.2
0.4
0.6
0.8
qMF
λ
2
4
6
0
0.2
0.4
0.6
0.8
qFC
λ
σ y
2
qFT
2
4
6
qMAP
σ y
2
1
2
3
4
Δ σ y
2
σ y
2
Δ λ
qFC
qFT
qFC
qFT
)
b
(
)a(
Figure 5.8 Simultaneous inference of λ and σ2
y with biases shown as a function of the true settings of the param-
eters. (a) For each approximation a number of simulations are run and each is represented by an arrow. The arrow
begins at the true setting of the parameters and the tip ends at the inferred value. Ideally the arrows would be very
short, but in fact they are often very large. (b) The best uncertainty preserving approximation ({qMF, qFC, qFT}) for
ﬁnding λ (top) and σ2
y (bottom) indicated by marker-type (qMF is never superior, qFC black crosses and qFT black
open-circles). The black solid line is r = σ2
x/|λ|σ2
y = 1 and below it qFT is tightest, and above it qFC is tightest.
dominate over explaining away. However, there is a large intermediate region where the
mean-ﬁeld approximation is the least biased. Once again, the tightness of the free-energy
approximations is a poor indicator of which is the least biased.
The main conclusions from this section are that the biases in variational methods are
often severe. The examples above indicate that factorisations across time can ignore strong
temporal correlations in the data, and factorisations across chains can erroneously prune
out emission weights. Furthermore, which is the best approximation depends not only on
which parameter has to be learned, but also on the true value of the parameters. Surpris-
ingly, mean-ﬁeld approximations are often superior to structured methods when a single
parameter is estimated.
5.4.7
Simultaneous learning of pairs of parameters
So far we have considered estimating a single parameter keeping the others at their true
values. What happens when we infer pairs of parameters at once? Consider, for instance,

Variational expectation maximisation for time series
123
inferring the dynamical parameter λ and the observation noise σ2
y with σ2
x held at its true
value (see Fig. 5.8). As before, three methods are used to ﬁnd the optimal parameter set-
tings (gridding, gradient ascent and vEM). In this case, a small minority of the objective
functions is multimodal, thus the agreement between the methods depends on the initial-
isation. To avoid this ambiguity, the gradient-based methods were initialised at the values
returned from the method of gridding the space. This procedure located the global optima.
The most striking feature of Fig. 5.8(a) is that the biases are often very large (even in
regimes where the structural approximations are at their tightest). In principle, if the map-
ping between the inferred parameters and true parameters were known, it might be possible
to correct for the biases in the variational estimates. However, multiple diﬀerent settings
of the true parameters result in the same inferred parameters. Therefore it is impossible to
correct the variational estimates in this way.
Fig. 5.8(b) shows that, in contrast to the case where only one parameter is inferred at a
time, the mean-ﬁeld solution is no longer superior to the structural approximations. It also
indicates that whilst tightness is a guide for choosing the best approximation, it is not very
accurate. It is also notable that, when all three parameters are inferred together (data not
shown), the biases become even larger.
5.4.8
Discussion of the scope of the results
The examples considered in this chapter were chosen to be simple so that exact results could
be computed and visualised. It is necessary, however to analyse how the eﬀects described
here generalise to longer time series (T > 2) with more hidden variables (K > 2). Unfor-
tunately, it is generally not tractable to analyse these, more complex, scenarios. However,
increasing the length of the time series and the number of latent chains results in a posterior
distribution that has a richer correlational structure. That is, the posterior covariance matrix
has a greater number of oﬀ-diagonal terms. The variational approximations considered here
would therefore ignore larger parts of this structure, and so one might expect the KL terms
and the associated biases to be correspondingly larger.
There are many ways of assessing the performance of vEM. This chapter has focused
exclusively on the consistency of the methods and the biases in the learned parameters.
However, another relevant set of criteria come from tasks which require prediction of some
kind, for instance, to ﬁll in missing data or to denoise. How does vEM fare in this new sce-
nario? In order to answer this question it is necessary to specify the task more accurately.
Consider a task in which the ﬁrst stage involves learning the model parameters from a train-
ing set, and the second involves ﬁlling in a section of missing data in a test set using the
mean of the approximate posterior. Given the same set of parameters, all four approxima-
tions will make identical predictions for the missing section (the mean of the true posterior).
The diﬀerences between the approximations are therefore entirely dependent on the quality
of the parameters learned during the ﬁrst stage of the experiments. As the task requires
accurate learning of both the temporal dynamics (to predict the missing latent variables)
and the emission weights (to predict the missing data from the missing latent variables), all
the approximation schemes will perform poorly compared to the optimal prediction.
5.5
Conclusion
We have discussed two problems in the application of vEM to time series models. First, the
compactness property of variational inference leads to a failure in propagating posterior
uncertainty through time. Second, the dependence of the tightness of the variational lower

124
Richard Eric Turner and Maneesh Sahani
bound on the model parameters often leads to strong biases in parameter estimates. We
found that the relative bias of diﬀerent approximations depends not only on which param-
eter is sought, but also on its true value. Moreover, the tightest bound does not always
yield the smallest bias: in some cases, structured approximations are more biased than the
mean-ﬁeld approach. Variational methods, however, avoid the over-ﬁtting problem which
plagues MAP estimation. Despite the shortcomings, variational methods remain a valid,
eﬃcient alternative to computationally expensive Markov chain Monte Carlo methods.
However, the choice of the variational distribution should be complemented with an anal-
ysis of the dependency of the variational bound on the model parameters. Hopefully, these
examples will inspire new algorithms that pool diﬀerent variational approximations in order
to achieve better performance [7].
Acknowledgments
We thank David Mackay for inspiration. This research was sup-
ported by the Gatsby Charitable Foundation.
Bibliography
[1] M. J. Beal. Variational Algorithms for
approximate Bayesian Inference. PhD
thesis, University College London, May
1998.
[2] C. Bishop. Pattern Recognition and Machine
Learning. Springer, 2006.
[3] S. Boyd and L. Vandenberghe. Convex
Optimization. Cambridge University Press,
March 2004.
[4] A. P. Dempster. Maximum-likelihood from
incomplete data via the EM algorithm. Journal of
the Royal Statistical Society, 39(1):1–38, 1977.
[5] R. Hathaway. Another interpretation of the EM
algorithm for mixture distributions. Statistics
and Probability Letters, 4:53–56, 1986.
[6] T. Jaakkola and M. Jordan. Bayesian parameter
estimation via variational methods. Statistics and
Computing, 10(1):
25–37, January 2000.
[7] T. S. Jaakkola and M. I. Jordan. Improving the
mean ﬁeld approximation via the use of mixture
distributions. In Learning in Graphical Models,
pages 163–173. MIT Press, 1999.
[8] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola and
L. K. Saul. An introduction to variational
methods for graphical models. Machine
Learning, 37(2):183–233, 1999.
[9] D. J. C. MacKay. A problem with variational
free energy minimization. 2001.
[10] D. J. C. MacKay. Information Theory, Inference,
and Learning Algorithms. Cambridge
University Press, 2003. available from www.
inference.phy.cam.ac.uk/mackay/itila/.
[11] R. Neal and G. E. Hinton. A view of the EM
algorithm that justiﬁes incremental, sparse, and
other variants. In Learning in Graphical Models,
pages 355–368. Kluwer Academic Publishers,
1998.
[12] R. E. Turner, P. Berkes, M. Sahani and
D. J. C. MacKay. Counter-examples to
variational free-energy compactness folk
theorems. Technical report, University College
London, 2008.
[13] B. Wang and D. M. Titterington. Lack of
consistency of mean ﬁeld and variational Bayes
approximations for state space models. Neural
Processing Letters, 20(3):151–170, 2004.
[14] J. Winn and T. Minka. Expectation propagation
and variational message passing: a comparison
with infer.net. In Neural Information Processing
Systems Workshop: Inference in continuous and
hybrid models, 2007.
Contributors
Richard Eric Turner, Gatsby Computational Neuroscience Unit, Alexandra House, 17 Queen Square,
London WCIN 3A
Maneesh Sahani, Gatsby Computational Neuroscience Unit, Alexandra House, 17 Queen Square,
London WCIN 3A

6
Approximate inference for continuous-time Markov
processes
C´edric Archambeau and Manfred Opper
6.1
Introduction
Markov processes are probabilistic models for describing data with a sequential struc-
ture. Probably the most common example is a dynamical system, of which the state
evolves over time. For modelling purposes it is often convenient to assume that the system
states are not directly observed: each observation is a possibly incomplete, non-linear
and noisy measurement (or transformation) of the underlying hidden state. In general,
observations of the system occur only at discrete times, while the underlying system is
inherently continuous in time. Continuous-time Markov processes arise in a variety of
scientiﬁc areas such as physics, environmental modelling, ﬁnance, engineering and systems
biology.
The continuous-time evolution of the system imposes strong constraints on the model
dynamics. For example, the individual trajectories of a diﬀusion process are rough, but
the mean trajectory is a smooth function of time. Unfortunately, this information is often
under- or unexploited when devising practical systems. The main reason is that inferring
the state trajectories and the model parameters is a diﬃcult problem as trajectories are
inﬁnite-dimensional objects. Hence, a practical approach usually requires some sort of
approximation. For example, Markov chain Monte Carlo (MCMC) methods usually dis-
cretise time [41, 16, 34, 2, 20], while particle ﬁlters approximate continuous densities by
a ﬁnite number of point masses [13, 14, 15]. More recently, approaches using perfect sim-
ulation have been proposed [7, 8, 18]. The main advantage of these MCMC techniques
is that they do not require approximations of the transition density using time discretisa-
tions. Finally, a variety of approaches like extensions to the Kalman ﬁlter/smoother [38]
and moment closure methods [17] express the statistics of state variables by a ﬁnite set of
moments, for example based on Gaussian assumptions.
In this chapter we discuss a promising variational approach to the inference problem for
continuous-time Markov processes, which was introduced by [3, 4]. We will focus on dif-
fusion processes, where the system state is a continuous variable subject to a deterministic
forcing, called drift, and a stochastic noise process, called diﬀusion. However, the approach
can also be applied to other processes, such as Markov jump processes (MJPs) [31, 11, 37].
In MJPs the state trajectories are still continuous-time functions, but the system state can
only take discrete values. For diﬀusions, the approach is based on a Gaussian approxima-
tion, but as in perfect simulation MCMC, it is not based on a discrete-time approximation
of the transition density. The approximate statistics are made not ad hoc as in the case of the

126
C´edric Archambeau and Manfred Opper
Kalman ﬁlter/smoother, but introduced in such a way that the true intractable probability
measure is optimally approximated.
This chapter is organised as follows. In Section 6.2 we deﬁne partly observed diﬀusion
processes and state the inference problem. In Section 6.3 we characterise the probability
measure over state trajectories given the data and show that the resulting posterior process
is a non-stationary Markov process. In Section 6.4 we introduce the variational approxima-
tion and show how this approach can be applied to Markov processes and in particular to
diﬀusions. Note, however, that unlike in most variational approaches we do not assume any
form of factorised approximation. In Section 6.5, we consider a practical smoothing algo-
rithm based on the Gaussian variational approximation and discuss the form of the solution
in more detail. Finally, we draw conclusions in Section 6.8.
6.2
Partly observed diﬀusion processes
We will be concerned with (Itˆo) stochastic diﬀerential equations (SDEs), where the
dynamics of a state variable x(t) ∈Rd is given by
dx(t) = f(x(t))dt + D1/2(x(t)) dW(t).
(6.1)
The vector function f is called the drift. The second term describes a (in general state-
dependent) white noise process deﬁned through a positive semi-deﬁnite matrix D, called
diﬀusion matrix, and a Wiener process W(t). We can think of this process as the limit of
the discrete-time process
x(t + ∆t) −x(t) = f(x(t))∆t + D1/2(x(t))
√
∆t ϵt,
(6.2)
where ϵt is now a vector of iid Gaussian random variables. The speciﬁc scaling of the white
noise with
√
∆t gives rise to the nondiﬀerentiable trajectories of sample paths character-
istic for a diﬀusion process [23, 25, 29]. Equation (6.2) is known as the Euler–Maruyama
approximation of Eq. (6.1).
We assume the diﬀusion process is stationary, i.e. f and D are not explicit functions of
time, although this is not required. We have only access to a ﬁnite set of noisy observations
Y ≡{yi}N
i=1 of the unobserved process x(t) at times ti for i = 1, . . . , N. Conditioned on the
state x we assume that observations are independent with an observation likelihood p(y|x).
We are interested in the problem where f and D are known only up to some unknown
parameters θ. It is usually necessary to add the unknown initial state x(0) = x0 to the
parameters.
Our goals are then to learn as much as possible from the observations in order to infer
the system parameters θ, the initial state x0 and to estimate the unknown sample path x(t)
over some interval 0 ≤t ≤T. The latter task (when all observations during this time are
used) is called smoothing.
In a maximum likelihood approach (or more precisely type II maximum likelihood [6]
or evidence maximisation [28, 9]) one would solve the ﬁrst two problems by integrating out
the latent process x(t) and then maximising the marginal likelihood p(Y|x0, θ) with respect
to θ and x0. In a fully Bayesian approach, one would encode prior knowledge in a prior
density p(x0, θ) and would obtain the information about the unknowns in the posterior
density p(x0, θ|Y) ∝p(Y|x0, θ) p(x0, θ). In both cases, the computation of p(Y|x0, θ) is
essential, but in general analytically intractable.

Approximate inference for continuous-time processes
127
6.3
Hidden Markov characterisation
Let us ﬁrst assume the parameters are known. To deal with the reconstruction of a sample
path we compute pt(x|Y, x0, θ), which is the marginal posterior of the state at time t, i.e.
x(t) = x. This marginal can be computed in the same way as the marginals of standard
discrete-time hidden Markov models [33]. The only diﬀerence is that we have to deal with
continuous-time and continuous states.
Using the Markov nature of the process1 and Bayes’ rule it is not hard to show that we
can represent this posterior as a product of two factors
pt(x|Y, x0, θ) ∝p(x(t)|Y<t, x0, θ)
|              {z              }
p(F)
t
(x)
p(Y≥t|x(t))
|      {z      }
ψt(x)
,
where p(F)
t
(x) is the posterior density of x(t) = x based on the observations Y<t ≡{yi}ti<t
before time t and ψt(x) is the likelihood of the future observations Y≥t = {yi}ti≥t conditioned
on x(t) = x.
For times smaller than the next observation time p(F)
t
(x) fulﬁls the Fokker–Planck (or
Kolmogorov forward) equation [23] corresponding to the SDE deﬁned in Eq. (6.1)
( ∂
∂t + ∇⊤f −1
2Tr(∇∇⊤)D
)
p(F)
t
(x) = 0,
(6.3)
where ∇is the vector diﬀerential operator. The Fokker–Planck equation describes the
time evolution of the density p(F)
t
(x) given some earlier density, e.g. at the most recent
observation time.
The second factor is found to obey the Kolmogorov backward equation corresponding
to the SDE deﬁned in Eq. (6.1), that is
( ∂
∂t + f ⊤∇+ 1
2Tr(D∇∇⊤)
)
ψt(x) = 0.
(6.4)
This equation describes the time evolution of ψt(x), i.e. the likelihood of future observa-
tions. The knowledge of ψt(x) also gives us the desired marginal likelihood as
p(Y|x0, θ) = ψ0(x).
Equations (6.3) and (6.4) hold for times between observations. The information about
the observations enters the formalism through a set of jump conditions for p(F)
t
and ψt(x) at
the observation times. This result is known as the so-called KSP equations [26, 42, 32].
Intuitively, the occurrence of jumps can be understood as follows. Assume we are mov-
ing forward in time up to time t, where we encounter the observation y(t). The information
associated to yt is removed from ψt(x) and incorporated into p(F)
t
(x). Mathematically, the
1More speciﬁcally, we need the Chapman–Kolmogorov equation to compute pt(x|Y<t, x0, θ). By the Markov
property we have p(x(t)|x(s), x(r)) = p(x(t)|x(s)), such that
Z
dx(s) p(x(t), x(s)|x(r)) =
Z
dx(s) p(x(t)|x(s)) p(x(s)|x(r)) = p(x(t)|x(r)),
for all r ≤s ≤t. Hence, using this result recursively and then applying Bayes’ rule leads to
p(x(t)|Y<t, x0, θ) ∝p(x(t)|x0, θ) p(Y<t|x(t)).

128
C´edric Archambeau and Manfred Opper
‘prior’ p(F)
t
(x) is updated using the likelihood factor p(y(t)|x(t)) causing jumps in p(F)
t
(x)
and ψt(x) at time t
p(F)
t
(x) ←1
Z p(F)
t
(x) p(y(t)|x(t)),
(6.5)
ψt(x) ←
ψt(x)
p(y(t)|x(t)),
(6.6)
where Z is a normalising constant. Moreover, by direct diﬀerentiation of pt(x|Y, x0, θ) with
respect to time and using Eqs. (6.3) and (6.4), we ﬁnd after some calculations that the
posterior also fulﬁls the Fokker–Planck equation
( ∂
∂t + ∇⊤g −1
2Tr(∇∇T)D
)
pt(x|Y, x0, θ) = 0,
(6.7)
with a new drift deﬁned as
g(x, t) = f(x) + D(x)∇ln ψt(x).
(6.8)
This shouldn’t be too surprising because conditioning on the observations does not change
the causal structure of the process x(t). It is still a Markov process, but a non-stationary one
due to the observations. Note that there are no jumps for pt(x|Y, x0, θ) as it is equal to the
product of Eqs. (6.5) and (6.6).
Hence, the process of exact inference boils down to solving the linear partial diﬀerential
equation (PDE) (6.4) backwards in time starting with the ﬁnal condition ψT(x) and taking
the jumps ψt−
i (x) = ψti(x)p(yi|xi) into account to get the function ψt(x) from which both
the likelihood p(Y|x0, θ) and the posterior drift (6.8) are obtained. Finally, the posterior
marginals are computed by solving the linear PDE (6.7) forwards in time for some initial
condition p(F)
0 (x).
6.3.1
Example
As an analytically tractable one-dimensional example we consider the simple Wiener pro-
cess dx(t) = dW(t) starting at x(0) = 0 together with a single, noise-free observation at
t = T, i.e. x(T) = y. The forward equation
∂p(F)
t
(x)
∂t
−1
2
∂2p(F)
t
(x)
∂x2
= 0
with initial condition p(F)
0 (x) = δ(x) is solved by p(F)
t
(x) = N(0, t), while the backward
equation
∂ψt(x)
∂t
+ 1
2
∂2ψt(x)
∂x2
= 0
with end condition ψT(x) = δ(x −y) is solved by ψt(x) = N(y, T −t). The posterior density
and the posterior drift are then respectively given by
pt(x|x(T) = y, x(0) = 0) ∝p(F)
t
(x) ψt(x) = N(ty/T, t(T −t)/T),
(6.9)
g(x, t) = ∂ln ψt(x)
∂x
= y −x
T −t,
(6.10)

Approximate inference for continuous-time processes
129
(a) Sample paths and posterior drifts.
(b) Prior, likelihood and posterior densities.
Figure 6.1 Illustration of a one-dimensional diﬀusion process without drift and unit diﬀusion coeﬃcient, starting
at the origin and with a noise free observation y = 1 at t = 1. The posterior process is a Brownian bridge.
Note how the drift increases drastically when getting close to the ﬁnal time. (a) shows ﬁve sample paths with
their corresponding posterior drift functions. (b) shows the mean and variance (shaded region) of the prior, the
likelihood and the posterior marginals. Observe how the variance of the posterior pt(x) is largest in the middle of
the time interval and eventually decreases to 0 at t = 1. See plate section for colour version.
for 0 < t < T. A process with drift (6.10) is known as a Brownian bridge (see Fig. 6.1).
Inspection of Eq. (6.9) shows that any path of the process starting at the origin and diﬀusing
away will eventually go to the noise-free observation y at time T.
In general, especially in higher dimensions, the solution of the PDEs will not be ana-
lytically tractable. Also numerical methods for PDE solving [25] might become too time
consuming. Hence, we may have to consider other types of approximations. One such
possibility will be discussed next.
6.4
The variational approximation
A diﬀerent idea for solving the inference problem might be to attempt a direct computation
of the marginal likelihood or partition function Z(x0, θ)  p(Y|x0, θ). Using the Markov
property of the process x(t) we obtain
Z(x0, θ) =
Z
N
Y
i=1
{dxi p (xi|xi−1, θ) p(yi|xi)} ,
where xi is a shorthand notation for x(ti) and x0 is ﬁxed. Unfortunately, except for simple
linear SDEs, the transition density p(xi|xi−1, θ) is not known analytically. In fact it would
have to be computed by solving the Fokker–Planck equation (6.3).
Nevertheless, at least formally we can write Z as an inﬁnite-dimensional or functional
integral over paths x(t) starting at x0 using a proper weighting of the paths. Using the
Girsanov change of measure formula from stochastic calculus [29] one could write such a
path integral as
Z =
Z
dµ exp
 
−1
2
Z T
0
n
f ⊤D−1 fdt −2 f ⊤D−1dx
o!
N
Y
i=1
p(yi|xi),
where dµ denotes a Gaussian measure over paths starting at x(0) = x0 induced by the simple
linear SDE dx(t) = D1/2(x(t)) dW(t) without drift. Note that, in the case of a diagonal

130
C´edric Archambeau and Manfred Opper
diﬀusion matrix and a drift derived from a potential, the Itˆo integral
R
f ⊤D−1dx can be
transformed into an ordinary integral. These types of functional integrals play an important
role in quantum statistical physics (usually written in a slightly diﬀerent notation). Most
functional integrals cannot be solved exactly, but the variational approach of statistical
physics pioneered by Feynman, Peierls, Bogolubov and Kleinert [19, 24] gives us an idea
on how to approximate Z.
Consider some conﬁguration χ of the system of interest. In our application χ
is identiﬁed with the path x(t) in the time window [0, T]. We can represent the
probabilities over conﬁgurations in the form dp(χ) = 1
Z dµ(χ)e−H(χ), where H(χ)
=
1
2
R T
0
n
f ⊤D−1 fdt −2 f ⊤D−1dx
o
−PN
i=1 ln p(yi|xi) is the Hamiltonian, which in statistical
physics corresponds to the energy associated to the conﬁguration. To compute an approxi-
mation to the partition function Z =
R
dµ(χ) e−H(χ), we ﬁrst approximate dp(χ) by a simpler
distribution dq(χ) =
1
Z0 dµ(χ) e−H0(χ), which is deﬁned by a simpler Hamiltonian H0(χ) and
for which Z0 is tractable. Using a simple convexity argument and Jensen’s inequality, we
get an approximation to the log partition function or free-energy by the bound
−ln Z ≤−ln Z0 + ⟨H⟩−⟨H0⟩,
(6.11)
where the brackets denote expectations with respect to the measure q. Usually H0 contains
free parameters, which can be adjusted in such a way that the inequality becomes as tight
as possible by minimising the upper bound on the right-hand side.
To deﬁne a tractable variational approximation (6.11) for our inference problem, we
would use an H0 which is quadratic functional in the process x(t). This would lead to
a Gaussian measure over paths. While this is indeed possible we prefer a diﬀerent, but
equivalent formulation of the variational method, which neither needs the deﬁnition of
a Hamiltonian, nor the application of stochastic calculus. The variational method in this
formulation has been extensively applied in recent years in machine learning to problems
involving ﬁnite-dimensional latent variables [22, 30, 10].
6.4.1
The variational approximation in machine learning
Let us denote the observations by Y and assume a ﬁnite-dimensional latent variable X.
Consider some prior distribution p(X|θ) parameterised by θ and some likelihood function
p(Y|X). To approximate the intractable posterior p(X|Y, θ) ∝p(Y|X) p(X|θ) we directly
choose a simpler trial distribution q(X). The optimal q is chosen to minimise the Kullback–
Leibler (KL) divergence or relative entropy [12]
KL[q∥p] =
*
ln
q(X)
p(X|Y, θ)
+
≥0.
This inequality directly leads to the bound
−ln Z(θ) ≤−⟨ln p(Y|X)⟩+ KL[q(X)∥p(X|θ)]  F (q, θ).
(6.12)
The right-hand side of Eq. (6.12) deﬁnes the so-called variational free-energy which is an
upper bound to the marginal likelihood of the data. Hence, minimising such a bound with
respect to the parameters θ can be viewed as an approximation to the (type II) maximum
likelihood method.
One can also apply the variational method in a Bayesian setting, where we have a prior
distribution p(θ) over model parameters [27]. To approximate the posterior p(θ|Y), we set

Approximate inference for continuous-time processes
131
p(X, θ|Y) ≈q(X|θ)q(θ) and apply the variational method to the joint space of variables X
and θ. Let q(X|θ) be the distribution which minimises the variational free-energy F (q, θ) of
Eq. (6.12). We then get
q(θ) =
e−F (q,θ) p(θ)
R
e−F (q,θ) p(θ) dθ
as the best variational approximation to p(θ|Y).
6.4.2
The variational approximation for Markov processes
In the case of partly observed diﬀusion processes we are interested in the posterior measure
over latent paths, which are inﬁnite-dimensional objects. The prior measure p(χ|x0, θ) is
derived from an SDE of the form (6.1) and the posterior measure p(χ|Y, x0, θ) is computed
from Bayes’ rule
p(χ|Y, x0, θ)
p(χ|x0, θ)
=
QN
i=1 p(yi|xi)
Z(x0, θ)
,
0 ≤t ≤T.
When the exact posterior is analytically intractable, we consider a trial posterior q(χ) that
we would like to match to the true posterior by applying the variational principle. All we
need is an expression for the KL divergence. From Section 6.3, we know that the posterior
process is Markovian and obeys an SDE with the time-dependent drift (6.8), that is
dx(t) = g(x(t), t)dt + D1/2(x(t)) dW(t).
(6.13)
Consider two continuous-time diﬀusion processes having the same diﬀusion matrix
D(x),2 but diﬀerent drift functions f(x) and g(x). We call the probability measures induced
over the corresponding sample paths respectively p(χ) and q(χ). Although we could prove
the following rigorously using Girsanov’s change of measure theorem [23, 29], we will use
a simpler, more intuitive heuristic in this chapter which can also be applied to Markov jump
processes.
Let us discretise time into small intervals of length ∆t and consider discretised sample
paths X = {xk = x(tk = k∆t)}K
k=1 with their corresponding multivariate probabilities p(X|x0)
and q(X|x0). We aim to compute the KL divergence between the measures dp and dq over
some interval [0, T] as the limit of
KL q(X)∥p(X) =
Z
dX q(X|x0) ln q(X|x0)
p(X|x0)
=
K
X
k=1
Z
dxk−1 q(xk−1)
Z
dxk q(xk|xk−1) ln q(xk|xk−1)
p(xk|xk−1),
where we have used the Markov property to represent p(X|x0) and q(X|x0) respectively
as Q
k p(xk|xk−1) and Q
k q(xk|xk−1). By plugging in the speciﬁc short term behaviour (i.e.
∆t →0) of the transition probabilities, since we are dealing with diﬀusions, we obtain the
Gaussian forms
p(xk|xk−1) ∝exp
 
−1
2∆t ∥xk −xk−1 −f(xk−1)∆t∥2
D(xk−1)
!
,
q(xk|xk−1) ∝exp
 
−1
2∆t ∥xk −xk−1 −g(xk−1)∆t∥2
D(xk−1)
!
,
2It can be shown that the KL divergence diverges for diﬀerent diﬀusions matrices.

132
C´edric Archambeau and Manfred Opper
where ∥f∥2
D = f ⊤D−1 f. Following [4], a direct computation taking the limit ∆t →0 yields
KL q(X)∥p(X) = 1
2
Z T
0
dt
(Z
dqt(x) ∥g(x) −f(x)∥2
D(x)
)
,
where qt(x) is the posterior marginal at time t. Note that this result is still valid if the drift
function and the diﬀusion matrix are time dependent.
Hence, the variational free-energy in the context of diﬀusion processes can be written as
F (q, θ) = KL[q(χ)∥p(χ|θ)] −
X
i
⟨ln p(yi|xi)⟩qti,
(6.14)
where χ is a continuous sample path in the interval [0, T]. The bound (6.11) is equivalent to
the bound (6.14) for appropriate deﬁnitions of Hamiltonians H(χ) and H0(χ). The advan-
tage of the bound (6.14) is that we can directly compute the KL divergence for Markov
processes, without deﬁning H(χ) and H0(χ) explicitly. The results can also be applied to
MJPs as proposed by [31].
6.4.3
The variational problem revisited
Before discussing approximations, we will show that total minimisation of the free-energy
yields our previous result (Eq. (6.8)). For the corresponding derivation in the case of MJPs
see [36]. The free-energy can be written as
F (q, θ) =
Z T
0
dt
Z
dx qt(x)
(1
2 ∥g(x, t) −f(x)∥2
D(x) + u(x, t)
)
,
where the observations are included in the term
u(x, t) = −
X
i
ln p(yi|x) δ(t −ti).
The drift g and the marginal qt are connected by the Fokker–Planck equation
∂qt
∂t =
(
−∇⊤g + 1
2Tr(∇∇T)D
)
qt  Lgqt
as a constraint in the optimisation of qt. We can deal with this constraint by introducing a
Lagrange multiplier function λ(x, t) to obtain the following Lagrange functional:
L  F (q, θ) −
Z T
0
dt
Z
dx λ(x, t)
 ∂qt(x)
∂t
−(Lgqt)(x)
!
.
Performing independent variations of qt and g leads respectively to the following Euler–
Lagrange equations:
1
2 ∥g −f∥2
D + u +
(
g⊤∇+ 1
2Tr(D∇∇⊤)
)
λ + ∂λ
∂t = 0,
D−1(g −f) + ∇λ = 0,
where we have used integration by parts when appropriate. Deﬁning the logarithmic
transformation λ(x, t) = −ln ψt(x) and rearranging yields then the conditions
( ∂
∂t −u(x, t)
)
ψt(x) =
(
−f ⊤(x)∇−1
2Tr(D(x)∇∇⊤)
)
ψt(x),
g(x, t) = f(x) + D(x)∇ln ψt(x),
(6.15)

Approximate inference for continuous-time processes
133
for all t ∈[0, T]. By noting that u(x, t) = 0 except at the observation times, we ﬁnd that
these results are equivalent to Eqs. (6.4) and (6.8); the Dirac δ functions yield the proper
jump conditions when there are observations. Note that this derivation still holds if f and
D are time dependent.
6.5
The Gaussian variational approximation
In practice, rather than assuming the correct functional form (6.15), we will view g as
a variational function with a simpliﬁed form. The function g can then be optimised to
minimise the free-energy.
Gaussian distributions are a natural choice for approximations. For example, they have
been used frequently in statistical physics applications. For previous (ﬁnite-dimensional)
applications in machine learning see [5, 39, 21]. In the present inference case, a Gaus-
sian approximating measure over paths, that is a Gaussian process, is considered. In this
case the drift must be a linear function of the state x. We consider a drift of the form
g(x, t) = −A(t)x + b(t), where A(t) and b(t) are functions to be optimised. In addition, we
limit ourselves to the special case of a constant diﬀusion matrix D [3, 4]. The approx-
imation equally holds in the case of time-dependent diﬀusions. The more general case
of multiplicative noise processes, that is with state-dependent diﬀusion matrices, will be
discussed in Section 6.6.
Since we are dealing with a Gaussian process, the marginals qt(x) are Gaussian densi-
ties. This result represents a signiﬁcant simpliﬁcation of the calculations. First, qt(x) are
fully speciﬁed by their marginal means m(t) and their marginal covariances S (t). Sec-
ond, we don’t need to solve PDEs, but are left with simpler ordinary diﬀerential equations
(ODEs). Since SDE (6.13) is linear, we have
dm  ⟨dx⟩= (−Am + b)dt ,
dS  ⟨d((x −m)(x −m)⊤)⟩= (−AS −S A⊤)dt + Ddt + O(dt2),
where the term Ddt is obtained by applying the stochastic chain rule.3 Hence, the evolution
of m(t) and of S (t) is described by the following set of ODEs:
dm(t)
dt
= −A(t)m(t) + b(t),
(6.16)
dS (t)
dt
= −A(t)S (t) −S (t)A⊤(t) + D.
(6.17)
We can follow a similar approach as in Section 6.4.3 to optimise the Gaussian vari-
ational approximation. More speciﬁcally, we use these ODEs as a constraint during the
optimisation. Let us deﬁne e(x, t) = 1
2∥g(x, t) −f(x)∥2
D. The Lagrangian functional is now
deﬁned as
L =
Z T
0
dt ⟨e(x, t) + u(x, t)⟩qt −
Z T
0
dt λ⊤(t)
 dm(t)
dt
+ A(t)m(t) −b(t)
!
−
Z T
0
dt Tr
 
Ψ(t)
 dS (t)
dt
+ A(t)S (t) + S (t)A⊤(t) −D
!!
,
(6.18)
3This result can also be obtained by an informal derivation not relying on stochastic calculus but only using
properties of the Wiener process [3].

134
C´edric Archambeau and Manfred Opper
where λ(t) and Ψ(t) are vector and matrix Lagrange parameter functions which depend
on time only. Performing independent variations of m(t) and S (t) (which is equivalent to
performing an independent variation of qt) yields an additional set of ODEs
dλ(t)
dt
= −∇m⟨e(x, t)⟩qt + A⊤(t)λ(t),
(6.19)
dΨ(t)
dt
= −∇S ⟨e(x, t)⟩qt + Ψ(t)A(t) + A⊤(t)Ψ(t),
(6.20)
along with jump conditions at observation times
λi = λ−
i −∇m⟨u(x, t)⟩qt
t=ti,
λ−
i = lim
t↑ti λ(t),
(6.21)
Ψi = Ψ−
i −∇S ⟨u(x, t)⟩qt
t=ti,
Ψ−
i = lim
t↑ti Ψ(t).
(6.22)
Hence, the Fokker–Planck equation is replaced by Eqs. (6.16) and (6.17) in the Gaus-
sian variational approximation, while the Kolmogorov backward equation is replaced by
Eqs. (6.19) and (6.20). Based on Eqs. (6.19)–(6.22) we can devise a smoothing algorithm as
described in [3, 4]. Also, a procedure to infer θ (which parameterises f and D) is discussed
in detail in [4].
One important advantage of the Gaussian variational approach is that representations
can be based on a discretisation of ODEs instead of a direct discretisation of the SDE. The
loss of accuracy is expected to be less severe because of the smoothness of the paths [25].
Also, the approximation holds in continuous-time and is thus independent of the chosen
representations unlike most MCMC schemes [2, 20]. In contrast to these discrete-time
MCMC schemes, perfect simulation MCMC for continuous-time systems was recently
proposed [7, 8, 18]. This method is so far restricted to problems where drift terms are
derived as gradients of a potential function, which is not required in the Gaussian variational
approximation. The main similarity between the Gaussian variational approximation and
these advanced MCMC approaches is that they do not depend on a discrete-time approx-
imation of the transition density. However, the Gaussian variational approximation diﬀers
from perfect simulation in its approximation of the non-Gaussian transition density by a
(time-dependent) Gaussian one.
Thorough experimental comparisons are still required to assess the advantages and
disadvantages of the diﬀerent methods, but the Gaussian variational approximation is
likely to be computationally faster as it is not based on sampling; it only cares about the
marginal means and covariances, which can be computed eﬃciently by forward integration
(Eqs. (6.16) and (6.17)). On the other hand, perfect MCMC sampling captures the posterior
measure more accurately if run for a suﬃciently long period of time.
6.5.1
Interpretation of the solution
In this subsection we discuss the form of the Gaussian variational solution in more detail.
We perform the independent variation of A(t) and b(t), which can be viewed as performing
the independent variation of g as in Section 6.4.3. This leads to the following conditions
A(t) = −
D
∇( f ⊤)(x, t)
E
qt + 2DΨ(t),
(6.23)
b(t) = ⟨f(x, t)⟩qt + A(t)m(t) −Dλ(t),
(6.24)
for all t. In order to obtain Eq. (6.23) we used the identity ⟨f(x −m)⊤⟩= ⟨∇( f ⊤)⟩S , which
holds for any non-linear function f(·) applied to a Gaussian random variable x. The solution
(6.23)–(6.24) is closely related to a solution known as statistical linearisation [35].

Approximate inference for continuous-time processes
135
Consider a non-linear function f applied to a continuous random variable x with density
q. We are interested in the best linear approximation −Ax + b to f. Instead of directly
truncating the Taylor series of f to obtain a linear approximation, we would like to take into
account the fact that x is a random variable. Statistical linearisation takes this information
into account by taking A and b such that the linear approximation is optimal in the mean
squared sense:
A, b ←min
A,b
D
∥f(x) + Ax −b∥2E
q .
When x is a Gaussian random variable it is easy to show that the solution to this prob-
lem is given by A = −
∇(f ⊤)(x)
q and b = ⟨f(x)⟩q + Am. Comparing these expressions
to Eqs. (6.23) and (6.24), it can be observed that the variational solution reduces to the
statistical linearisation solution when the Lagrange multipliers are zero. Recalling that the
Lagrange multipliers account for the constraints and the observations, one can see that
the solution (6.23)–(6.24) is biased compared to the standard statistical linearisation solu-
tion. This bias corresponds to a correction based on future information and it is weighted by
the diﬀusion matrix. The weighting by D makes sense as the magnitude of the correction
should depend on the amount of stochasticity in the system.
6.5.2
Example
Applications of the Gaussian variational approximation to statistical inference for non-
linear SDEs can be found in [3, 4]. We illustrate the basic idea of the calculation only
for the simple, analytically tractable case of Section 6.3.1. For later use, we introduce an
extra parameter σ in the model which controls the diﬀusion coeﬃcient, i.e. we set dx(t) =
σdW(t).
We have g(x, t) = −a(t)x(t) + b(t). The evolution of the mean m(t) and variance s(t)
simplify to
dm
dt = −a(t)m(t) + b(t),
(6.25)
ds
dt = −2a(t)s(t) + σ2.
(6.26)
We model the noise free observation as the limit of a Gaussian observation centred at y and
with variance σ2
0 →0. Hence, we have
L =
Z T
0
dt
( 1
2σ2 a2(s + m2) +
1
2σ2 b2 −1
σ2 amb
)
+
Z T
0
dt
1
2σ2
0
(y2 + s + m2 −2my)δ(t −T)
−
Z T
0
dt λ
 dm
dt + am −b
!
−
Z T
0
dt ψ
 ds
dt + 2as −σ2
!
.
(6.27)

136
C´edric Archambeau and Manfred Opper
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−1
0
1
2
x(t)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−10
0
10
g(x,t) & a(t)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−5
0
5
t
pt(x)
Figure 6.2 Illustration of a one-dimensional diﬀusion process without drift and unit diﬀusion coeﬃcient, starting
at the origin and with a noise-free observation y = 1 at t = 1. The top panel shows a sample path, whilst the middle
panel shows the corresponding drift g(x, t) = −a(t)x(t) + b(t) and the variational parameter a(t). The bottom panel
shows the posterior process, which corresponds to the one obtained in Fig. 6.1.
The Euler–Lagrange equations (6.19–6.24) are then given by
dλ(t)
dt
= −a2(t)m(t)
σ2
+ a(t)b(t)
σ2
+ a(t)λ(t),
(6.28)
dψ(t)
dt
= −a2(t)
2σ2 + 2ψ(t)a(t),
(6.29)
a(t) = 2σ2ψ(t),
(6.30)
b(t) = a(t)m(t) −σ2λ(t),
(6.31)
along with the jump conditions
λ(T) = λ(T −) −m(T) −y
σ2
0
,
ψ(T) = ψ(T −) −
1
2σ2
0
.
Substitution of Eq. (6.30) into Eq. (6.29) leads to dψ
dt = 2σ2ψ2 with the end condition
1
2σ2
0 .
It follows that the solution to this ODE is given by ψ(t) =
1
2σ2(T−t)+2σ2
0 . Second, substitution
of Eq. (6.31) into Eq. (6.28) implies λ is a constant. The end condition yields λ = m(T)−y
σ2
0
.
Next, substitution of Eq. (6.31) into Eq. (6.25) leads to m(t) = −σ2λt + c with c = 0 as
m(0) = 0, such that m(T) =
yT
σ2
0/σ2+T . Hence, we obtain
a(t) =
σ2
σ2(T −t) + σ2
0
,
b(t) =
σ2y
σ2(T −t) + σ2
0
.
This leads to the same result for g(x, t) as Eq. (6.10) when σ2
0 →0. The solution is
illustrated in Fig. 6.2.

Approximate inference for continuous-time processes
137
6.6
Diﬀusions with multiplicative noise
In Section 6.5 we discussed the Gaussian variational approximation of diﬀusion processes
with a constant or a time-dependent diﬀusion matrix. However, the methodology can still
be applied to diﬀusion processes with multiplicative noise.
In some cases one can apply an explicit transformation to transform the original diﬀu-
sion process with multiplicative noise into a diﬀusion process with a unit diﬀusion matrix
[1]. The resulting drift is then expressed in terms of f and D via Itˆo’s formula. Although
this ad hoc approach is always possible when the state space is one dimensional, such a
transformation typically does not exist in the multivariate case.
In the general case of a state-dependent diﬀusion, the ODEs describing the evolution of
the mean m(t) and the covariance S (t) are deﬁned by
dm(t)
dt
= −A(t)m(t) + b(t),
(6.32)
dS (t)
dt
= −A(t)S (t) −S (t)A⊤(t) + ⟨D(x(t), t)⟩qt.
(6.33)
The only diﬀerence with Eqs. (6.16) and (6.17) is that in Eq. (6.33) the expectation of the
diﬀusion matrix appears. Hence, we can construct a Gaussian process approximation of
the posterior process using the moments of the marginals (6.32) and (6.33) as constraints.
Note, however, that A(t) and b(t) are no longer given by Eqs. (6.23) and (6.24), but have a
more complicated form.
6.7
Parameter inference
The formulation of the variational approach in terms of Eq. (6.18) is especially useful
when we would like to estimate model parameters by an approximate type II maximum
likelihood method. In this approximation, we use the free-energy F (q∗, θ) evaluated at the
optimal variational Gaussian measure q∗for given parameters θ as a proxy for the negative
log-marginal likelihood −ln Z(θ).
The optimal parameters θ∗are obtained by minimising F , which requires the computa-
tion of the gradients ∇θF (q∗, θ). Although q∗is a function of θ, this optimisation problem
is facilitated by the following argument. For each θ, we have L = F (q∗, θ) at the stationary
solution, which is also stationary with respect to marginal moments, variational parameters
and Lagrange parameters. Hence, to compute the gradients ∇θF (q∗, θ), we just have to take
the explicit gradients of L with respect to θ, while keeping all other quantities ﬁxed.
6.7.1
Example
This idea is illustrated for the simple diﬀusion example of Section 6.3.1, where we have
introduced a parameter σ to control the diﬀusion variance: dx(t) = σdW(t). We are inter-
ested in computing the derivative of the negative log-marginal likelihood of the single
observation y (at time T) with respect to σ2.
For a direct computation, let us ﬁrst note that p(F)
t
(x) = N(0, σ2t). The marginal
likelihood for y is given by
p(y|, σ2) =
Z
δ(y −x(T))p(F)
T (x)dx(T) = N(0, σ2T),

138
C´edric Archambeau and Manfred Opper
which yields
−∂ln p(y|σ2)
∂σ2
=
1
2σ2 −
y2
2σ4T .
On the other hand, diﬀerentiating Eq. (6.27) with respect to σ2 leads to
∂L
∂σ2 = −1
2σ4
Z T
0
dt

a2s + σ4λ2
+
Z T
0
dt ψ(t) =
1
2σ2 −
y2
2σ4T .
The ﬁrst equality is obtained by diﬀerentiating Eq. (6.27) and using Eq. (6.31). To get the
ﬁnal result we inserted the explicit results for a(t), λ(t) and ψ(t) obtained in Section 6.5.2
for σ2
0 →0, as well as the corresponding solution to Eq. (6.26): s(t) = σ2t
T (T −t).
6.8
Discussion and outlook
Continuous-time Markov processes, such as diﬀusion processes and Markov jump pro-
cesses, play an important role in the modelling of dynamical systems. In a variety of
applications, the state of the system is a (time-dependent) random variable of which the
realisation is not directly observed. One has only access to noisy observations taken at a
discrete set of times. The problem is then to infer from data the unknown state trajectory
and the model parameters, which deﬁne the dynamics. While it is fairly straightforward to
present a theoretical solution to these estimation problems, a practical solution in terms of
PDEs or by MCMC sampling can be time consuming. One is thus interested in eﬃcient
approximations.
In this work we described a method to ﬁt a Gaussian process to a non-Gaussian process
induced by a SDE. The method is based on the variational principle originally developed
in statistical physics and now extensively used in machine learning. It provides a practi-
cal alternative to exact methods and MCMC. Unlike previous variational approaches [43]
it is not required to discretise the sample paths, nor to factorise the posterior across time.
Although this might lead to good results when the number of observations is large com-
pared to the speed of the dynamics, this approach leads in general to poor results. For a
systematic discussion of the eﬀect of factorisation in discrete-time dynamical systems we
refer the interested reader to Chapter 5. By contrast our approximation does not assume
any form of factorisation of the posterior. Rather, we choose a posterior process within a
tractable family, namely the Gaussian family, which explicitly preserves the time depen-
dency. Moreover, the approximation holds in continuous-time such that discretisation is
only required for representation purposes.
The Gaussian variational approximation is attractive as it replaces the problem of
directly solving a SDE (or equivalently a set of PDEs) by the simpler problem of solv-
ing a set of ODEs. The variational parameters are optimised to obtain the best possible
approximation. This optimisation is done concurrently with the estimation of the model
parameters, which enable us to learn the dynamics of the system. However, the proposed
approach might be too time consuming in high-dimensional applications, such as numer-
ical weather prediction. The main reason is that the dynamics of the marginal covariance
S scales with d2, d being the state space dimension. Hence, one could envisage subopti-
mal schemes in which the variational parameters are reparameterised by a small number of
auxiliary quantities. Another potential issue is the estimation of the multivariate Gaussian
expectations, which appear in the computation of A and b, as well as the computation of

Approximate inference for continuous-time processes
139
free-energy F . In low-dimensional state spaces they can be estimated naively using sam-
pling. Alternatively, one can use quadrature methods, but most existing approaches break
down or are too slow in higher-dimensional spaces and/or for highly non-linear dynamics.
As mentioned earlier there are ongoing eﬀorts to develop computationally eﬃcient
algorithms for fully Bayesian inference in diﬀusion processes. A very promising direction
is to combine the Gaussian variational method and MCMC. One could for example develop
an MCMC algorithm which uses the variational approximating process as a proposal pro-
cess [40]. Sample paths could then be simulated using the optimal non-stationary linear
diﬀusion and ﬂexible blocking strategies would be used to further improve the mixing.
Acknowledgments
This work was partly funded by the EPSRC VISDEM project
(EP/C005848/1), as well as by the PASCAL 2 European network of excellence.
Bibliography
[1] Y. Ait-Sahalia. Closed-form likelihood
expansions for multivariate diﬀusions. Annals of
Statistics, 36:906–937, 2008.
[2] F. J. Alexander, G. L. Eyink and J. M. Restrepo.
Accelerated Monte Carlo for optimal estimation
of time series. Journal of Statistical Physics,
119:1331–1345, 2005.
[3] C. Archambeau, D. Cornford, M. Opper and
J. Shawe-Taylor. Gaussian process
approximation of stochastic diﬀerential
equations. Journal of Machine Learning
Research: Workshop and Conference
Proceedings, 1:1–16, 2007.
[4] C. Archambeau, M. Opper, Y. Shen, D. Cornford
and J. Shawe-Taylor. Variational inference for
diﬀusion processes. In J. C. Platt, D. Koller,
Y. Singer, and S. Roweis, editors, Advances in
Neural Information Processing Systems 20,
pages 17–24. MIT Press, 2008.
[5] D. Barber and C. M. Bishop. Ensemble learning
for multi-layer networks. In M. I. Jordan, M. J.
Kearns, and S. A. Solla, editors, Advances in
Neural Information Processing Systems 10,
pages 395–401. MIT Press, 1998.
[6] James O. Berger. Statistical Decision Theory and
Bayesian Analysis. Springer, 1985.
[7] A. Beskos, O. Papaspiliopoulos, G. Roberts and
P. Fearnhead. Exact and computationally
eﬃcient likelihood-based estimation for
discretely observed diﬀusion processes (with
discussion). Journal of the Royal Statistical
Society B, 68(3):333–382,
2006.
[8] A. Beskos, G. Roberts, A. Stuart and J. Voss.
MCMC methods for diﬀusion bridges.
Stochastics and Dynamics, 8(3):
319–350, 2008.
[9] C. M. Bishop. Neural Networks for Pattern
Recognition. Oxford University Press, 1995.
[10] C. M. Bishop. Pattern Recognition and Machine
Learning. Springer, 2006.
[11] I. Cohn, T. El-hay, N. Friedman and R.
Kupferman. Mean ﬁeld variational
approximation for continuous-time Bayesian
networks. In 25th International Conference on
Uncertainty in Artiﬁcial Intelligence,
pages 91–100, 2009.
[12] T. M. Cover and J. A. Thomas. Elements of
Information Theory. John Wiley & Sons, 1991.
[13] D. Crisan and T. Lyons. A particle approximation
of the solution of the Kushner-Stratonovitch
equation. Probability Theory and Related Fields,
115(4):549–578, 1999.
[14] P. Del Moral and J. Jacod. Interacting particle
ﬁltering with discrete observations. In
A. Doucet, N. de Freitas and N. Gordon, editors,
Sequential Monte Carlo Methods in Practice,
pages 43–76. MIT Press, 2001.
[15] P. Del Moral, J. Jacod and P. Protter. The Monte
Carlo method for ﬁltering with discrete-time
observations. Probability Theory and Related
Fields, 120:346–368, 2002.
[16] B. Eraker. MCMC analysis of diﬀusion models
with application to ﬁnance. Journal of Business
and Economic Statistics, 19:177–191, 2001.
[17] G. L. Eyink, J. L. Restrepo and F. J. Alexander.
A mean ﬁeld approximation in data assimilation
for nonlinear dynamics. Physica D,
194:347–368, 2004.
[18] P. Fearnhead, O. Papaspiliopoulos and G. O.
Roberts. Particle ﬁlters for partially-observed
diﬀusions. Journal of the Royal Statistical
Society B, 70:755–777, 2008.
[19] R. P. Feynman and A. R. Hibbs. Quantum
Mechanics and Path integrals. McGraw-Hill
Book Company, 1965.
[20] A. Golightly and D. J. Wilkinson. Bayesian
sequential inference for nonlinear multivariate
diﬀusions. Statistics and Computing,
16:323–338, 2006.
[21] A. Honkela and H. Valpola. Unsupervised
variational Bayesian learning of nonlinear
models. In L. Saul, Y. Weiss, and L. Bottou,
editors, Advances in Neural Information
Processing Systems 17, pages 593–600. MIT
Press, 2005.

140
C´edric Archambeau and Manfred Opper
[22] M. I. Jordan, editor. Learning in Graphical
Models. MIT Press, 1998.
[23] I. Karatzas and S. E. Schreve. Brownian Motion
and Stochastic Calculus. Springer, 1998.
[24] H. Kleinert. Path Integrals in Quantum
Mechanics, Statistics, Polymer Physics, and
Financial Markets. World Scientiﬁc, 2006.
[25] P. E. Kloeden and E. Platen. Numerical Solution
of Stochastic Diﬀerential Equations. Springer,
1999.
[26] H. J. Kushner. On the diﬀerential equations
satisﬁed by conditional probability densities of
Markov processes with applications. Journal of
SIAM, Series A: Control, 2:106–119, 1962.
[27] H. Lappalainen and J. W. Miskin. Ensemble
learning. In M. Girolami, editor, Advances in
Independent Component Analysis, pages 76–92.
Springer-Verlag, 2000.
[28] D. J. C. MacKay. Bayesian interpolation. Neural
Computation, 4(3):415–447, 1992.
[29] B. Øksendal. Stochastic Diﬀerential Equations.
Springer-Verlag, 2005.
[30] M. Opper and D. Saad, editors. Advanced Mean
Field Methods: Theory and Practice. MIT Press,
2001.
[31] M. Opper and G. Sanguinetti. Variational
inference for Markov jump processes. In J. C.
Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information
Processing Systems 20, pages 1105–1112 MIT
Press, 2008.
[32] E. Pardoux. Equations du ﬁltrage non lin´eaire, de
la pr´ediction et du lissage. Stochastics,
6:193–231, 1982.
[33] L. Rabiner. A tutorial on hidden Markov models
and selected applications in speech recognition.
Proceedings of the IEEE, 2(77):257–286, 1989.
[34] G. Roberts and O. Stramer. On inference for
partially observed non-linear diﬀusion models
using the Metropolis-Hastings algorithm.
Biometrika, 88(3): 603–621, 2001.
[35] J. B. Roberts and P. D. Spanos. Random
Vibration and Statistical Linearization. Dover
Publications, 2003.
[36] A. Ruttor, G. Sanguinetti and M. Opper.
Approximate inference for stochastic reaction
processes. In M. Rattray, N. D. Lawrence,
M. Girolami and G. Sanguinetti, editors,
Learning and Inference in Computational
Systems Biology, pages 189–205. MIT Press,
2009.
[37] G. Sanguinetti, A. Ruttor, M. Opper and
C. Archambeau. Switching regulatory models of
cellular stress response. Bioinformatics,
25(10):1280–1286, 2009.
[38] S. S¨arkk¨a. Recursive Bayesian Inference on
Stochastic Diﬀerential Equations. PhD thesis,
Helsinki University of Technology, Finland,
2006.
[39] M. Seeger. Bayesian model selection for support
vector machines, Gaussian processes and other
kernel classiﬁers. In T. G. Dietterich, S. Becker
and Z. Ghahramani, editors, Advances in Neural
Information Processing Systems 12, pages
603–609. MIT Press, 2000.
[40] Y. Shen, C. Archambeau, D. Cornford and
M. Opper. Markov Chain Monte Carlo for
inference in partially observed nonlinear
diﬀusions. In Proceedings Newton Institute for
Mathematical Sciences workshop on Inference
and Estimation in Probabilistic Time-Series
Models, pages 67–78, 2008.
[41] O. Elerian, S. Chiband and N. Shephard. Likeli-
hood inference for discretely
observed nonlinear diﬀusions. Econo-
metrika, 69(4):959–993, 2001.
[42] R. L. Stratonovich. Conditional Markov
processes. Theory of Probability and its
Applications, 5:156–178, 1960.
[43] B. Wang and D. M. Titterington. Lack of
consistency of mean ﬁeld and variational Bayes
approximations for state space models. Neural
Processing Letters, 20(3):151–170, 2004.
Contributors
C´edric Archambeau, Department of Computer Science, University College London, Gower Street,
London WCIE 6BT
Manfred Opper, Technische Universit¨at Berlin, Fakult¨at IV – Elektrotechnik und Informatik,
Franklinstr. 28/29, D-10587 Berlin, Germany

7
Expectation propagation and generalised EP methods
for inference in switching linear dynamical systems
Onno Zoeter and Tom Heskes
7.1
Introduction
Many real-world problems can be described by models that extend the classical linear
Gaussian dynamical system with (unobserved) discrete regime indicators. In such extended
models the discrete indicators dictate what transition and observation model the process
follows at a particular time. The problems of tracking and estimation in models with
manoeuvring targets [1], multiple targets [25], non-Gaussian disturbances [15], unknown
model parameters [9], failing sensors [20] and diﬀerent trends [8] are all examples of prob-
lems that have been formulated in a conditionally Gaussian state space model framework.
Since the extended model is so general it has been invented and re-invented many times in
multiple ﬁelds, and is known by many diﬀerent names, such as switching linear dynami-
cal system, conditionally Gaussian state space model, switching Kalman ﬁlter model and
hybrid model.
Although the extended model has a lot of expressive power, it is notorious for the fact
that exact estimation of posteriors is intractable. In general, exact ﬁltered, smoothed or pre-
dicted posteriors have a complexity exponential in the number of observations. Even when
only marginals on the indicator variables are required the problem remains NP-hard [19].
In this chapter we introduce a deterministic approximation scheme that is particularly
suited to ﬁnd smoothed one and two time slice posteriors. It can be seen as a symmet-
ric backward pass and iteration scheme for previously proposed assumed density ﬁltering
approaches [9].
The chapter is organised as follows. In Section 7.2 we present the general model; vari-
ants where only the transition or only the observation model switch, or where states or
observations are multi- or univariate can be treated as special cases. In Section 7.3 we
describe the assumed density ﬁltering approach that forms the basis for our approximation
scheme. In Section 7.4 we introduce the symmetric backward pass. Since both the forward
and the backward pass consist of local, greedy projections it makes sense to iterate them.
In Section 7.5 we introduce the objective that is minimised by such an iteration scheme
and give an intuition how we should interpret ﬁxed points. In Section 7.6 we provide an
extension of the basic method based on generalised expectation propagation. In Section 7.7
we describe two earlier approximate smoothing passes that are often used for conditionally
Gaussian state space models, but that in contrast to the approach presented in this chapter

142
Onno Zoeter and Tom Heskes
Figure 7.1 The belief network that encodes the conditional independencies in the SLDS. Ellipses denote Gaussian
distributed variables and rectangles denote multinomial distributed variables. Shading emphasises the fact that a
particular variable is observed.
make additional approximations in the smoothing phase. In Section 7.8 we describe exper-
iments that test the validity of the proposed method and compare it with the alternative
backward pass and state-of-the-art sampling techniques.
7.2
Notation and problem description
In a switching linear dynamical system (SLDS) it is assumed that an observed sequence
y1:T of T, d-dimensional observations is generated as noisy observations from a ﬁrst-order
Markov process. The latent space consists of a q-dimensional continuous state xt and a
discrete state st that can take on M values. Conditioned on st−1 and st, xt is a linear function
of xt−1 subjected to Gaussian white noise, that is
p(xt|xt−1, st−1, st, θ) ∼N(xt; Ast−1,stxt, Qst−1,st).
(7.1)
In the above we have used θ to denote the set of model parameters and N(.; ., .) to denote
the Gaussian probability density function. The observation model is also linear-Gaussian
and may diﬀer between settings of st−1 and st, that is
p(yt|xt, st−1, st, θ) ∼N(yt;Cst−1,stxt, Rst−1,st).
(7.2)
The discrete state follows a ﬁrst-order Markov chain in the discrete space
p(st|st−1, θ) = Πst−1→st.
(7.3)
At t = 1 we have p(s1|θ) = πs1 and p(x1|s1, θ) is a Gaussian with known parameters. The
belief network that encodes the conditional independencies implied by these equations is
presented in Fig. 7.1.
Throughout this chapter the parameters θ are assumed to be known. The interest is in
estimating the ﬁltered and smoothed one- and two-slice posteriors. If we treat zt ≡{st, xt}
as a single conditionally Gaussian (CG) distributed random variable, we obtain an indepen-
dence structure identical to the basic linear dynamical system. (Appendix 7.A introduces
the CG distribution and deﬁnes standard operations.) This might lead us to assume that
inference is easy. However, this is not the case. One way to see this is by looking at the
posterior

Expectation propagation for switching linear systems
143
p(st, xt|y1:T, θ) =
X
s1:T\t
p(xt|s1:T, y1:T, θ)p(s1:T|y1:T, θ).
(7.4)
Since the history of regime changes is unknown, we have to take into account all possible
sequences of indicator variables s1:T. The CG family is not closed under marginalisation,
so the result of summing over all possible sequences in Eq. (7.4) is of a more complex form
than a simple CG distribution: conditioned on st the posterior is a Gaussian mixture with
MT−1 components.
A second way of interpreting the exponential growth is by inspecting a recursive ﬁlter-
ing algorithm. At every time step the number of components in the exact posterior increases
by a factor M, since all the components considered in the previous slice are propagated to
and updated in the next slice by M possible models. In the next section we describe an
approximate inference algorithm where this growth is avoided by a projection at every
time step.
7.3
Assumed density ﬁltering
7.3.1
Local approximations
In the previous section we have seen that the number of components in the exact ﬁltered
posteriors increases M-fold with every new observation. An obvious, and in practice very
powerful, approximation is to ﬁrst incorporate evidence yt and then to approximate the
resulting posterior over zt by a ‘best ﬁtting’ conditional Gaussian distribution. Here ‘best
ﬁtting’ is deﬁned as the CG distribution that minimises the Kullback–Leibler (KL) diver-
gence [17] between the original and approximate distribution. The KL-divergence between
distributions ˆp(zt) and q(zt) is deﬁned as
KL ( ˆp(zt)||q(zt)) ≡
X
zt
ˆp(zt) log ˆp(zt)
q(zt)
(7.5)
and is not symmetric in ˆp and q. In Eq. (7.5) we have used the shorthand notation of P
zt for
denoting the operation of integrating over xt and summing over st, a shorthand that we will
use in the remainder of this chapter. The CG distribution ˆq(zt) closest to ˆp(zt) in the sense
of KL-divergence is the CG distribution that has the same moments as ˆp (see e.g. [27]).
That is, for each value of st, the mixture ˆp(xt|st) is approximated in ˆq(xt|st) by a single
Gaussian with the same mean and covariance as ˆp(xt|st). Motivated by these ‘collapses’ of
mixtures into single Gaussians, we introduce the notation
ˆq(zt) = Collapse ( ˆp(zt)) ≡argmin
q∈CG
KL ( ˆp(zt)||q(zt)) .
A precise deﬁnition is given in Appendix 7.A.
7.3.2
The sum-product algorithm
If the growth of complexity is avoided by a local projection in every recursion step, the com-
putational requirements of an approximate ﬁlter are linear in the number of observations
instead of exponential. This resulting approximate forward pass is referred to as assumed
density ﬁltering, or generalised pseudo Bayes 2 (GPB 2) [1], amongst others, and has been
proposed independently many times. The oldest reference we are aware of is in [9].

144
Onno Zoeter and Tom Heskes
Algorithm 7.1 Assumed density ﬁltering
ˆq1(z1) = α1(z1) ∝ψ1(z1) . For t = 2, 3, . . . T, compute approximate ﬁltered posteriors ˆqt(zt)
as follows:
1. Construct a two-slice joint ˆpt(zt−1, zt) ∝αt−1(zt−1)ψt(zt−1, zt) .
2. Marginalise to obtain a one-slice ﬁltered marginal ˆpt(zt) = P
zt−1 ˆpt(zt−1, zt) .
3. Approximate ˆpt(zt) by ˆqt(zt), the CG distribution closest to ˆpt(zt) in KL-sense
ˆqt(zt) = Collapse ( ˆpt(zt)) .
4. Set αt(zt) = ˆqt(zt).
Figure 7.2 The factor graph corresponding to a SLDS with four observations and with factors ψt deﬁned by
Eqs. (7.6) and (7.7).
In Algorithm 7.1 assumed density ﬁltering is presented in the spirit of the sum-product
algorithm [16]. The model equations (7.1), (7.2) and (7.3) enter as factors ψt, which are
potentials deﬁned as
ψt(zt−1, zt) ≡p(yt, zt|zt−1, θ) = p(yt|xt, st−1, st, θ)p(xt|xt−1, st−1, st, θ)p(st|st−1, θ), (7.6)
ψ1(z0, z1) ≡p(y1, z1|θ) = p(yt|x1, s1, θ)p(x1|s1, θ)p(s1|θ).
(7.7)
To make the similarity between the ﬁltering and smoothing pass clearer we introduce
a distinct notation for approximate one-slice marginals, ˆqt(zt) ≈p(zt|y1:T, θ), and for-
ward messages, αt(zt). The messages fulﬁl a similar role as in the regular Kalman ﬁlter.
In an exact ﬁlter, the forward messages would satisfy αt(zt) ∝p(zt|y1:t, θ), here they are
approximations of these quantities.
Figure 7.2 represents the factor graph [16] that corresponds to Algorithms 7.1 and 7.2
presented in Section 7.4. Variables are represented by ovals, factors by solid squares and
messages by arcs. Note that the variables y1:T are not depicted, they are always observed
and part of the factors.
The assumed density ﬁlter starts at t = 1. The term ˆq1(z1) is obtained by normalizing ψ1
ˆq1(z1) ∝ψ1(z1).
The posterior ˆq1(z1) is a CG distribution with ˆq1(x1|s1 = m) corresponding to the prior
p(x1|s1 = m, θ) updated in the light of observation y1 and observation model m. Similarly
ˆq1(s1 = m) is the prior probability that the system starts in regime m appropriately weighted
by the likelihood that y1 was generated by model m. Since ˆq1(z1) is still CG, there is no
need for an approximation at this point. Since the current approximation of the belief state
of z1 is only based on y1 we set α1(z1) = ˆq1(z1). The need for messages will become clear
in the next sections.

Expectation propagation for switching linear systems
145
A recursive ﬁltering step is done by making a prediction and measurement update step.
The message αt−1 is multiplied by ψt and normalised to get a belief over the states zt−1,t
ˆpt(zt−1,t) ∝αt−1(zt−1)ψt(zt−1,t).
At the start of the recursion, αt−1(zt−1) is a conditional Gaussian potential with M
modes. The belief ˆpt(zt−1,t) is a conditional Gaussian distribution with M2 components:
the M components from αt−1(zt−1) propagated and updated according to M diﬀerent
models.
The marginal ˆpt(zt) = P
zt−1 ˆpt(zt−1, zt) is not CG. Instead ˆpt(xt|st) is a mixture of Gaus-
sians with M components. If we were to use ˆpt(zt) as the new forward message, the ﬁltering
pass would give exact results, but, at the next step in the recursion in Step 1 in Algo-
rithm 7.1, the number of components in the joint would increase by a factor M, implying a
number of components that is exponential in t.
To avoid this growth ˆpt(zt) is approximated by the CG distribution closest to ˆpt(zt) in
KL-sense
ˆqt(zt) = Collapse ( ˆpt(zt)) .
As for ˆq1(z1), if we only perform a single forward pass, the approximate beliefs ˆqt(zt)
are only based on y1:t. Therefore, we set αt(zt) = ˆqt(zt).
Since the growth of complexity is prevented by the projection in Step 3 of Algorithm 7.1
the running time of assumed density ﬁltering is linear in T, the number of observations.
7.4
Expectation propagation
In this section we introduce the expectation propagation (EP) approximate smoothing
algorithm that is based on the same projection principles as the assumed density ﬁlter.
The smoother that is symmetric to the assumed density ﬁlter (i.e. does not introduce any
further approximations apart from the projection onto the conditional Gaussians) can be
understood as an expectation propagation algorithm [21].
7.4.1
Backward pass
After establishing a deterministic approximation for the forward pass in Section 7.3, it
is natural to look for an analogous backward, or smoothing, pass. Several attempts have
been made in the literature [14, 25]. These have all included extra approximations beyond
the projections onto the conditional Gaussian family; we will brieﬂy review these in
Section 7.7. Other approximations such as [10] are restricted to models with invertible
dynamics.
The smoothing pass depends on backward messages βt(zt), with t = 1, 2, . . . , T. These
messages are analogous to the backward messages in the hidden Markov model (HMM)
smoother or the smoother from the two-ﬁlter algorithm for the linear dynamical system
(LDS). In the exact case we have
βt(zt) ∝p(yt+1:T|zt, θ),
(7.8)
such that αt(zt)βt(zt) ∝p(zt|y1:T, θ), which is of the form (7.4). In the current approximation
scheme we have
αt(zt)βt(zt) ∝ˆqt(zt) ≈p(zt|y1:T, θ),
(7.9)

146
Onno Zoeter and Tom Heskes
where ˆqt(zt) is CG. In the factor graph in Fig. 7.2 this deﬁnition is depicted as follows: an
approximate posterior belief over a variable can be constructed by multiplying all messages
coming into that variable node.
A potential problem with an approximate backward pass based on βt’s is that, whereas
forward messages αt can always be normalised, the backward messages βt in general
cannot. The fact that βt messages cannot always be normalised can be understood from
Eq. (7.8); it is a correction term for zt (i.e. has zt as a conditioning term) and hence
integrating over zt does not necessarily yield a ﬁnite value.
The KL-divergence is only deﬁned on proper distributions. We therefore cannot deﬁne
a backward pass by projecting backward messages directly.
We propose to use an approximation which can be viewed as a special case of expecta-
tion propagation [21]. A key diﬀerence with the previous approaches mentioned above
is that instead of approximating messages, ﬁrst beliefs are constructed which are then
approximated. The new messages are then deduced from the approximated beliefs.
The message passing scheme is symmetric in the forward and backward pass. As in
the previous section the presentation in Algorithm 7.2 is in the spirit of the sum-product
algorithm.
It suﬃces to describe the backward recursion. The recursion starts by introducing a
message βT = 1. Then, for t ≤T, the backward message βt−1 is computed as a function
of βt, the local potential ψt and the forward message αt−1. Given αt−1, ψt and βt an
approximated two-slice posterior belief ˆpt(zt−1,t) can be computed as
ˆpt(zt−1, zt) ∝αt−1(zt−1)ψt(zt−1, zt)βt(zt).
In the factor graph in Fig. 7.2 this operation should be interpreted as follows: an approx-
imate posterior belief over the domain of the factor t can be obtained by multiplying all
incoming messages with factor ψt itself and normalising.
As in the forward pass the marginal ˆpt(zt−1) is a conditional mixture of Gaus-
sians instead of CG. However since ˆpt(zt−1) constitutes a proper distribution, the
approximation
ˆqt−1(zt−1) = Collapse ( ˆpt(zt−1))
is now well deﬁned. From the deﬁnition in Eq. (7.9), we have
ˆqt−1(zt−1) = αt−1(zt−1)βt−1(zt−1).
The message αt−1(zt−1) is kept ﬁxed in this recursion step, so the new βt−1(zt−1) follows by
a division as in Step 4 in Algorithm 7.2.
In the forward pass a new forward message αt is computed analogously as a function
of the old forward message αt−1, the local potential ψt and the backward message βt, by
constructing ˆqt(zt) and dividing by βt(zt). If we initialise all messages with 1, on the ﬁrst
forward pass the scheme is identical to the assumed density ﬁltering algorithm discussed
in Section 7.3.
A new two-slice marginal implies two new one-slice beliefs, so in Step 4 we could com-
pute two new messages instead of one. However, computing new backward (βt−1) messages
on a forward pass is redundant since these messages will be replaced on a backward pass
before they would be used. A similar reasoning goes for the computation of new forward
messages on a backward pass.

Expectation propagation for switching linear systems
147
Algorithm 7.2 Expectation propagation for the SLDS
Compute a forward pass by performing the following steps for t = 1, 2, . . . , T, with t′ ≡t,
and a backward pass by performing the same steps for t = T −1, T −2, . . . , 1, with t′ ≡
t −1. Possibly iterate forward-backward passes until convergence. At the boundaries keep
α0 = βT = 1.
1. Construct a two-slice belief ˆpt(zt−1, zt) ∝αt−1(zt−1)ψt(zt−1, zt)βt(zt) .
2. Marginalise to obtain a one-slice marginal ˆpt(zt′) = P
zt′′ ˆpt(zt−1, zt), with t′′ ≡{t −
1, t}\t′.
3. Find ˆqt′(zt′) that approximates ˆpt(zt′) best in the Kullback–Leibler sense ˆqt′(zt′) =
Collapse ( ˆpt(zt′)) .
4. Infer the new message by division αt(zt) = ˆqt(zt)
βt(zt) , βt−1(zt−1) = ˆqt−1(zt−1)
αt−1(zt−1) .
7.4.2
Iteration
If in Step 3 of Algorithm 7.2 the new one-slice marginal is not approximated, i.e. if exact
marginalisation is performed, one forward, and one backward pass would suﬃce. This can
easily be seen from the fact that the forward and backward messages can be computed
independently: since the (exact) summation is a linear operation multiplying with βt(zt) in
Step 1 and dividing again in Step 4 is redundant. In fact, the above scheme is identical to
the two-ﬁlter approach of ﬁnding smoothed estimates in a linear dynamical system if these
redundant multiplication and division operations are left out. In the current setting however,
with a ‘collapse’ operation in Step 3 that is not linear in αt−1 nor in βt (see Appendix 7.A),
the forward and backward messages do interfere. Diﬀerent backward messages βt result in
diﬀerent forward messages αt and vice versa.
Thus instead of performing one forward and backward pass, Steps 1 to 4 in Algo-
rithm 7.2 can be iterated to ﬁnd local approximations that are as consistent as possible. In
Section 7.5 we introduce an objective that can be said to be associated to such an iterative
scheme: minima of the objective correspond to ﬁxed points of the iteration.
7.4.3
Supportiveness
One issue that is not discussed in Algorithm 7.2 is supportiveness. We say that Step 4 in
Algorithm 7.2 is supported, if all the beliefs that change because of the construction of the
new messages remain normalisable. On the ﬁrst forward pass this is automatically satis-
ﬁed. Since α1 is a proper distribution and βt = 1 for all t and all ψt are proper conditional
distributions, by induction, all αt are proper distributions as well. A new message αnew
t
(zt)
changes belief ˆpnew
t+1 (zt,t+1) ∝αnew
t
(zt)ψt(zt,t+1)βt+1(zt+1), which is normalisable by construc-
tion since βt+1(zt+1) is 1 on the ﬁrst forward pass. However, in general, due to the division
in Step 4, after a message is updated neighbouring two-slice potentials are not guaranteed
to be normalisable. For example on a backward pass, after replacing βt with βnew
t
(based on
the two-slice belief ˆpnew
t+1 (zt,t+1)), the neighboring belief
ˆpnew
t
(zt−1,t) ∝αt−1(zt−1)ψt(zt−1,t)βnew
t
(zt)
may not be normalisable. The requirement is that the sum of the respective inverse covari-
ance matrices is positive deﬁnite. If a normalisability problem is detected, messages αnew
t

148
Onno Zoeter and Tom Heskes
and βnew
t−1 can be computed as damped versions of the messages α∗
t and β∗
t−1 suggested by
Step 4. We deﬁne damping as a convex combination of old and suggested messages in
canonical space (see Appendix 7.A), e.g. for a damped forward message
αnew
t
= ϵα∗
t + (1 −ϵ)αold
t .
(7.10)
In Eq. (7.10), αnew
t
, α∗
t , and αold
t
(in boldface) are the canonical parameter vectors of their
corresponding potentials. If a regular update (ϵ = 1) results in a non-normalisable potential,
a damping parameter ϵ with 0 ≤ϵ < 1 is chosen such that the resulting precision matrices
for the neighboring two-slice belief are positive deﬁnite.
7.5
Free-energy minimisation
In this section we discuss the objective that is minimised when the steps in Algorithm 7.2
are iterated. We ﬁrst show that in the exact case, the task of ﬁnding smoothed one- and two-
slice posteriors can be formulated as a minimisation problem. Although this minimisation
problem remains intractable, it forms the basis for an approximate objective discussed in
the second part of this section.
A variational distribution p(z1:T) can be introduced to get an objective F that is
sometimes referred to as a free-energy [28]
−log Z = min
p F (p) ≡min
p KL (p||p∗) −log Z.
(7.11)
In the above p∗is a shorthand for the exact posterior p(z1:T|y1:T, θ), and the minimisation is
over all proper distributions over the same domain as p∗. Since the KL-divergence is never
negative and 0 if and only if p = p∗, F has a unique minimum at which the equality in
Eq. (7.11) holds. In particular, if p forms a minimum of F its one- and two-slice marginals
will be equal to those of p∗.
In terms of the potentials ψt, the exact posterior is written as
p∗(z1:T|y1:T, θ) = 1
Z
T
Y
t=1
ψt(zt−1, zt),
with Z ≡p∗(y1:T|θ) the normalisation constant. If we plug this into F we obtain
F (p) = E(p) −S (p),
E(p) ≡−
T
X
t=1
X
zt−1,t
p(zt−1, zt) log ψt(zt−1, zt),
S (p) ≡−
X
z1:T
p(z1:T) log p(z1:T).
(7.12)
We see that the ﬁrst term E(p), the energy, factors automatically since the posterior is a
product of two-slice potentials ψt. For arbitrary p(z1:T) the second term S (p), the entropy,
does not factorise. However we can use the fact that the exact joint posterior on a chain is
of the form (see e.g. [27])
p∗(z1:T) =
QT
t=2 p∗(zt−1, zt)
QT−1
t=2 p∗(zt)
.

Expectation propagation for switching linear systems
149
We can therefore restrict the minimisation problem to range over all possible chains
parameterised by their one- and two-slice marginals
p(z1:T) =
QT
t=2 pt(zt−1, zt)
QT−1
t=2 qt(zt)
.
(7.13)
In Eq. (7.13) the one- and two-slice marginals qt(zt) and pt(zt−1, zt) should be properly
normalised distributions and such that they agree on their overlap
X
zt−1
pt(zt−1, zt) = qt(zt) =
X
zt+1
pt+1(zt, zt+1).
(7.14)
Plugging Eq. (7.13) into Eq. (7.12) we get a minimisation problem over a collection of one-
and two-slice marginals
−log Z = min
{pt,qt} F ({pt, qt})
(7.15)
≡min
{pt,qt}
(
T
X
t=1
X
zt−1,t
pt(zt−1, zt) log pt(zt−1, zt)
ψt(zt−1, zt) −
T−1
X
t=2
X
zt
qt(zt) log qt(zt)
)
,
under the constraints that the marginals are proper distributions and consistent (Eq. (7.14)).
Thus the exact posteriors can be found by a minimisation procedure in terms of (con-
strained) one- and two-slice marginals.
A problem remains of course that, as described in Section 7.2, the exact one- and
two-slice posteriors pt and qt are very complex, and in general will have exponentially
many modes. Therefore, even if we ﬁnd a scheme that could minimise F ({pt, qt}), the
results could not be computed nor stored eﬃciently. So we will approximate Eq. (7.15) by
restricting the one-slice qt(zt) marginals to be conditionally Gaussian, i.e. the conditional
posteriors p(xt|st, y1:T, θ) are approximated by a single Gaussian. The approximated, or
‘pseudo’ marginals are denoted by ˆpt and ˆqt. The resulting free-energy is given by
FEP( ˆp, ˆq) ≡
T
X
t=1
X
zt−1,t
ˆpt(zt−1, zt) log ˆpt(zt−1, zt)
ψt(zt−1, zt) −
T−1
X
t=2
X
zt
ˆqt(zt) log ˆqt(zt).
(7.16)
Restricting the marginals makes exact agreement on overlaps possible only in trivial solu-
tions. Instead the consistency requirements are weakened: overlapping two-slice marginals
only need to agree on their expectations. That is, apart from being properly normalised, the
marginals are required to have the same moments after a collapse
⟨f(zt)⟩ˆpt = ⟨f(zt)⟩ˆqt = ⟨f(zt)⟩ˆpt+1 .
(7.17)
In the above, f(zt) is the vector of suﬃcient statistics of the conditional Gaussian family
over zt (as deﬁned in Appendix 7.A) and ⟨·⟩p denotes expectation with respect to p.
The intuition and motivation behind this approximate free-energy is similar to the
assumed density ﬁltering approach. It is hoped that the collection of marginals ˆpt(zt−1,t) and
ˆqt(zt) are reasonable approximations to the exact marginals p(zt−1,t|y1:T, θ) and p(zt|y1:T, θ).
The weak consistency constraints ensure that the two possible ways of computing the
one-slice marginal ˆqt (based on ˆpt or ˆpt+1) are identical after a collapse.
Iterating the forward and backward pass as described in Section 7.4 can be interpreted
as a heuristic to ﬁnd a minimum of FEP under constraints (7.17). The following theorem
describes this relationship.

150
Onno Zoeter and Tom Heskes
Theorem 7.1 The collection of beliefs ˆpt(zt−1,t) and ˆqt(zt) form ﬁxed points of Algorithm 7.2
if and only if they are zero gradient points of FEP under the appropriate constraints.
The proof is presented in Appendix 7.B.
The relationship between the algorithm and the objective FEP is in fact somewhat
stronger. It can be shown that if the algorithm converges, the collection of beliefs cor-
respond to a local minimum of FEP under the constraints. The proof is somewhat more
involved and is given in [11].
It must be stressed that Theorem 7.1 does not claim that the algorithm always con-
verges. In hard problems the algorithm might get trapped in cycles or diverge. For such
hard problems it often helps to make smaller or damped updates (Eq. (7.10)). In practice
we observe that Algorithm 7.2 nearly always converges to a very reasonable approxima-
tion and for most ‘harder’ problems damping resolves convergence problems. However in
a thorough implementation direct minimisation of FEP may be used when Algorithm 7.2
fails to converge. A direct minimisation procedure is presented in [11]. This procedure is
a lot slower than Algorithm 7.2, so therefore the latter remains the method of choice for a
practical application.
7.6
Generalised expectation propagation
The objective (7.16) is closely related to the Bethe free-energy [28]. The Bethe free-energy
has the same form, but does not restrict the parametric family of the pseudo marginals, and
keeps strong consistency constraints. The Bethe free-energy and the associated loopy belief
propagation algorithm are used for problems where the parametric families pose no prob-
lems (are closed under product and summation), but where the conditional independence
structure contains cycles. In such cases the tree-form in Eq. (7.13) yields an approximation.
The introduction of EP as an approximation of the Bethe free-energy motivates a gener-
alised expectation propagation algorithm [12] that is analogous to the Kikuchi’s extension
of the Bethe free-energy [29]. For the SLDS this will allow us to maintain more compo-
nents in the approximate marginals and update local greedy approximations in the ﬁlter by
reﬁning them in a backward pass and iteration scheme.
In the basic EP free-energy (7.16) the minimisation is w.r.t. beliefs over two-slice
marginals, ˜pt(zt−1,t), which we refer to as outer clusters, and the overlaps of these outer
clusters, the one-slice marginals ˜qt(zt−1,t). In the so-called negative entropy,
T
X
t=2
X
zt−1,t
˜pt(zt−1,t) log ˜pt(zt−1,t) −
T−1
X
t=2
X
zt
˜qt(zt) log ˜qt(zt),
from Eq. (7.16), the outer clusters enter with a plus, the overlaps with a minus sign. These
1 and −1 powers can be interpreted as counting numbers that ensure that every variable
eﬀectively is counted once in the (approximate) entropy in Eq. (7.16). If the free-energy is
exact (i.e. no parametric choice for the beliefs, and strong consistency constraints), the local
beliefs are exact marginals, and as in Eq. (7.13), the counting numbers can be interpreted
as powers that dictate how to construct a global distribution from the pseudo marginals.
In the Kikuchi’s extension the outer clusters are taken larger. The minimisation is then
w.r.t. beliefs over outer clusters, their direct overlaps, the overlaps of the overlaps, etc. With
each belief again proper counting numbers are associated. One way to construct a valid
Kikuchi based approximation is as follows [29]. Choose outer clusters zouter(i) and asso-
ciate with them the counting number couter(i) = 1. The outer clusters should be such that all

Expectation propagation for switching linear systems
151
Figure 7.3 Cluster deﬁnitions for κ = 0 (dashed) and κ = 1 (dotted).
domains zt−1,t of the model potentials Ψt(zt−1,t) are fully contained in at least one outer clus-
ter. Then recursively deﬁne the overlaps of the outer clusters zover(i), the overlaps of the over-
laps, etc. The counting number associated with cluster γ is given by the M¨obius recursion
cγ = 1 −
X
zγ′⊃zγ
cγ′.
(7.18)
A crucial observation for the SLDS is that it makes sense to take outer clusters larger
than the cliques of a (weak) junction tree. If we do not restrict the parametric form of ˜qt(zt)
and keep exact constraints, the cluster choice in Eq. (7.13) gives exact results. However, the
restriction that ˜qt(zt) must be conditional Gaussian, and the weak consistency constraints
imply an approximation: only part of the information from the past can be passed on to
the future and vice versa. With weak constraints it is beneﬁcial to take larger outer clusters
and larger overlaps, since the weak consistency constraints are then over a larger set of
suﬃcient statistics and hence ‘stronger’.
We deﬁne symmetric extensions of the outer clusters as depicted in Fig. 7.3. The size
of the clusters is indicated by 0 ≤κ ≤
l T−2
2
m
zouter(i) = si:i+2(κ+1)−1, xi+κ,i+κ+1
	 ,
i = 1, . . . , T −κ −1,
(7.19)
zover(i) = zouter(i) ∩zouter(i+1),
i = 1, . . . , T −κ −2.
In the outer clusters only the discrete space is extended because the continuous part can be
integrated out analytically and the result stays in the conditional Gaussian family. The ﬁrst
and the last outer cluster have a slightly larger set. In addition to the set (7.19) the ﬁrst clus-
ter also contains x1:i+κ−1 and the last also xi+κ+2:T. This implies a choice where the number
of outer clusters is as small as possible at the cost of a larger continuous part in the ﬁrst
and the last cluster. A slightly diﬀerent choice would have more clusters, but only two
continuous variables in every outer cluster.
To demonstrate the construction of clusters and the computation of their associ-
ated counting numbers we will look at the case of κ = 1. Below the clusters are
shown schematically, with outer clusters on the top row, and recursively the overlaps of
overlaps, etc.
s1,2,3,4
x1,2,3
s2,3,4,5
x3,4
s3,4,5,6
x4,5
s4,5,6,7
x5,6,7
s2,3,4
x3
s3,4,5
x4
s4,5,6
x5
s3,4
s4,5
s4

152
Onno Zoeter and Tom Heskes
The outer clusters all have counting number 1. The direct overlaps each have two larger
clusters in which they are contained. Their associated counting numbers follow from
Eq. (7.18) as 1 −2 = −1. The overlaps of overlaps have ﬁve clusters in which they are
contained, their counting numbers are 1 −(3 −2) = 0. The clusters on the lowest level have
nine parents, which results in a counting number 1 −(4 −3 + 0) = 0. It is easily veriﬁed
that with κ = 0 we obtain the cluster and counting number choice of Section 7.4.
A second crucial observation for the SLDS is that the choice of outer clusters (7.19)
implies that we only have to consider outer clusters and direct overlaps, i.e. the phenomenon
that all clusters beyond the direct overlaps get an associated counting number of 0 in the
example above extends to all κ. This is a direct result of the fact that the clusters from
Eq. (7.19) form the cliques and separators in a (weak) junction tree. That is, another way
to motivate a generalisation with the cluster choice (7.19) is to replace Eq. (7.13) with
p(z1:T|y1:T, θ) =
QN
i=1 p(zouter(i)|y1:T, θ)
QN−1
j=1 p(zover(j)|y1:T, θ)
,
(7.20)
and use this choice in Eq. (7.16) to obtain an extension. In Eq. (7.20), N = T −κ−1 denotes
the number of outer clusters in the approximation. The aim then becomes to minimise
FGEP = −
N
X
i=1
X
zouter(i)
˜pi(zouter(i)) log Ψ(i)(zouter(i))
+
N
X
i=1
X
zouter(i)
˜pi(zouter(i)) log ˜pi(zouter(i))
−
N−1
X
i=1
X
zover(i)
˜qi(zover(i)) log ˜qi(zover(i)),
(7.21)
w.r.t. the potentials ˜pi(zouter(i)), and ˜qi(zover(i)). For i = 2, 3, . . . N −1, the potentials
Ψ(i)(zover(i)) are identical to the potentials ψi+κ+1(zi+κ,i+κ+1) from Eq. (7.6). At the boundaries
they are a product of potentials that are ‘left over’
Ψ(1) =
κ+2
Y
j=1
ψj(zj−1, j),
Ψ(N) =
T
Y
j=T−κ
ψj(zj−1, j),
with Ψ(1) = QT
j=1 ψj(zj−1, j) if N = 1.
The approximation in the generalised EP free-energy, FGEP, arises from the restriction
that ˜qi(zover(i)) is conditional Gaussian and from the fact that overlapping potentials are only
required to be weakly consistent

f(zover(i))
˜pi = 
f(zover(i))
˜qi = 
f(zover(i))
˜pi+1 .
The beneﬁt of the (weak) junction tree choice of outer clusters and overlaps is that we
can employ the same algorithm for the κ = 0 as for the κ > 0 case. Algorithm 7.3 can be
seen as a single-loop minimisation heuristic. Using a proof analogous to the proof for The-
orem 7.1 one can show that the algorithm can be interpreted as ﬁxed-point iteration in the
space of Lagrange multipliers that are added to Eq. (7.21) to enforce the weak consistency
constraints [30]. Just as for EP itself, convergence of Algorithm 7.3 is not guaranteed.
In Algorithm 7.3 the messages are initialised as conditional Gaussian potentials, such
that ˜q(zover(i)) = αi(zover(i))βi(zover(i)) are normalised. A straightforward initialisation would

Expectation propagation for switching linear systems
153
Algorithm 7.3 Generalised EP for the SLDS.
Compute a forward pass by performing the following steps for i = 1, 2, . . . , N −1, with
i′ ≡i, and a backward pass by performing the same steps for i = N, N −1, . . . , 2, with
i′ ≡i −1. Iterate forward-backward passes until convergence. At the boundaries keep
α0 = βN = 1.
1. Construct an outer-cluster belief
˜pi(zouter(i)) = αi−1(zover(i−1))Ψ(i)(zouter(i))βi(zover(i))
Zi
,
with Zi = P
zouter(i) αi−1(zover(i−1))Ψ(i)(zouter(i))βi(zover(i)).
2. Compute one-slice marginal ˜pi(zover(i′)) = P
zouter(i)\zover(i′) ˜pi(zouter(i)).
3. Find ˜qi′(zover(i′)) that approximates ˜pi(zover(i′)) best in Kullback–Leibler (KL) sense
˜qi′(zover(i′)) = Collapse   ˜pi(zover(i′)) .
4. Infer the new message by division
αi(zover(i)) = ˜qi(zover(i))
βi(zover(i)),
βi−1(zover(i−1)) = ˜qt−1(zover(i−1))
αi−1(zover(i−1)).
be to initialise all messages proportional to 1. If at the start all products of matching mes-
sages are normalised, we can interpret the product of local normalisations QN
i=1 Zi as an
approximation of the normalisation constant Z.
The choice of 0 ≤κ ≤
l T−2
2
m
allows a trade oﬀbetween computational complexity and
degrees of freedom in the approximation. With κ = 0, we obtain the EP/Bethe free-energy
equivalent from Section 7.4. With κ =
l T−2
2
m
there is only one cluster and we obtain a strong
junction tree, and the found posteriors are exact.
To conclude the introduction of the generalised expectation propagation algorithm it is
instructive to describe the link with tree-EP [22]. In tree-EP the approximating family p
forms a tree. The original introduction in [22] was as an extension of belief propagation,
i.e. with strong consistency constraints, without projection steps. The preceding discussion
should make it clear that all objectives with strong consistency constraints are suitable can-
didates to explore for ﬁnding approximations in more general models where only weak
consistency constraints can be worked with. Thus tree-EP can be seen as describing a large
subset of the possible GEP approximations. Since the cluster choices one obtains by choos-
ing p a tree perform quite well in practice [26] it is worthy of further study. We refer the
interested reader to [26] who, apart from experiments, also provide equivalence classes for
energies (for the strong consistency constraints case) and give further rules of thumb on
how to best pick outer clusters and counting numbers.
7.7
Alternative backward passes
7.7.1
Approximated backward messages
The smoothing pass in Algorithm 7.2 is the ﬁrst that is symmetric in the sense that no
additional approximations are introduced in the smoothing pass. This section describes

154
Onno Zoeter and Tom Heskes
two previous approaches and shows how they diﬀer from the method proposed in
Algorithm 7.2.
The forward pass in [14] is identical to the assumed density ﬁltering discussed in Sec-
tion 7.3. The backward pass diﬀers from the one proposed in Algorithm 7.2. We refer to it
as alternative backward pass (ABP) in the remainder of this chapter. The ABP is based on
the traditional Kalman smoother form (as opposed to the two-ﬁlter approach to smoothing
in EP). Instead of βt(zt) ≈p(yt+1:T|zt) messages, approximations to the smoothed posteri-
ors p(xt|st, y1:T) and p(st|y1:T) form the basis for recursion. The smoother treats the discrete
and continuous latent states separately and diﬀerently. This forces us to adapt our notation
slightly. In this section we use p(.) for (uncollapsed) distributions over two slices, ψ(.) for
the model equations (to emphasise the similarities with the factors from Section 7.3), and
q(.) for (collapsed) one-slice marginals. For compactness we do not explicitly write down
the dependence on θ.
As in the forward pass M2 modes are computed (p(xt, st, st+1|y1:T) for all instantiations
of st and st+1) and subsequently collapsed
q(xt, st|y1:T) = Collapse

X
st+1,xt+1
p(xt, xt+1, st, st+1|y1:T)
.
The ABP diﬀers from Algorithm 7.2 in the construction of p(xt, xt+1, st, st+1|y1:T). The
conditional posterior over xt is computed as follows [14]:
p(xt, xt+1|st, st+1, y1:T) = p(xt|xt+1, st, st+1, y1:t)p(xt+1|st, st+1, y1:T)
≈p(xt|xt+1, st, st+1, y1:t)q(xt+1|st+1, y1:T)
(7.22)
= p(xt, xt+1|st, st+1, y1:t)
p(xt+1|st, st+1, y1:t) q(xt+1|st+1, y1:T)
= q(xt|st, y1:t)ψ(xt+1|xt, st, st+1)
p(xt+1|st, st+1, y1:t)
q(xt+1|st+1, y1:T),
where the approximation is due to the fact that if we condition on yτ, with τ ≤t, xt+1 is not
independent of st.
The required posterior over the discrete latent state, p(st =
j, st+1 = k|y1:T), is
computed as
p(st, st+1|y1:T) = q(st+1|y1:T)p(st|st+1, y1:T)
≈q(st+1|y1:T)p(st|st+1, y1:t)
(7.23)
= q(st+1|y1:T)p(st, st+1|y1:t)
q(st+1|y1:t)
= q(st+1|y1:T)ψ(st+1|st)q(st|y1:t)
q(st+1|y1:t)
.
Note that the need for the extra approximations in Eqs. (7.22) and (7.23) comes from the
fact that the posteriors for discrete and continuous latent states are treated separately. The
posteriors are computed by conditioning on either the discrete or the continuous latent state,
i.e. only half of zt. A property of a Markov chain is that conditioning on the entire latent
state at t renders past, future and observation independent. This property is exploited in
the regular Kalman smoother and EP, but is not used in the ABP. Summarising, the ABP
smoother from [14] requires two additional approximations beyond the projection onto the
CG family. In contrast, EP requires no additional approximations.

Expectation propagation for switching linear systems
155
7.7.2
Partial smoothing
The ﬁlter in [25] is related to the ﬁlter in Section 7.3. However, instead of approximating
ﬁltered and smoothed estimates with a CG distribution it approximates them with a single
mode. The scheme computes
p(xt−1,t, st|y1:t) ∝ψ(yt, xt, st|xt−1)q(xt−1|y1:t),
(7.24)
q(xt|y1:t) = Collapse

X
st,xt−1
p(xt−1,t, st|y1:t)
,
with q(xt|y1:t) a Gaussian distribution (compared to a mixture with M modes in the ABP
and in Algorithm 7.2). This forward pass is also known as generalised pseudo Bayes 1
(GPB 1) [2]. The ﬁltering recursion is treated in [25] as if it were exact if the switches
only govern the observation model (no links from st to xt in Fig. 7.1). However, even
with such restrictions the greedy local projections result in an approximation of the exact
one- and two-slice marginals. Following the argumentations in [18] it can be seen that the
combinatoric explosion described in Section 7.2 is a property of all conditionally Gaussian
dynamic models. It is essentially caused by the unobserved continuous chain x1:T which
couples all discrete states s1:T.
In the smoothing pass of [25] the probabilities over discrete states st are not smoothed
(only approximations of ﬁltered regime posteriors are available by integrating xt−1,t in
Eq. (7.24)). The posterior over continuous states xt are computed using the standard
Kalman smoother recursion as if there are no switches.
7.8
Experiments
In this section we compare the behaviour of EP, the ABP and state-of-the-art sampling
techniques. Other interesting approximation methods include the structured mean-ﬁeld
approach from [7] and other variants of Gibbs sampling such as [24]. However, previous
studies [7, 4] have shown that these approaches are less likely to outperform the methods
we discuss here. Hence we did not include them in the description and the experiments.
In the ﬁrst part the methods are evaluated using artiﬁcially generated models and short
data sequences. For these sequences all approximations can be compared with the exact
results. In the second part we study the quality of the proposed approximation on longer
sequences. Since for these longer sequences exact results are unattainable we compare the
proposed method with Gibbs sampling.
7.8.1
Comparisons with exact posteriors
We generate models by drawing parameters from conjugate priors. The regime prior at t = 1
is multinomial. Its parameters are drawn from a uniform distribution and subsequently nor-
malised. The parameters in the rows of Πst−1→st, the regime transition probabilities, are
treated similarly. The elements of the initial state mean and the observation matrices C
are drawn from a standard normal distribution. The state transition matrices A, are also
constructed based on draws from the standard normal distribution. The covariances for the
white Gaussian noise in the transition and observation models and for the initial state, are
drawn from an inverse Wishart distribution with 10 degrees of freedom and scale matrix
0.01I.
We drew 100 models with xt ∈R3, yt ∈R2 and 2 regimes, and generated a sequence
of length 8 for each of these models. Using Eq. (7.4) we computed the exact posteriors.

156
Onno Zoeter and Tom Heskes
For each task we computed approximate one-slice smoothed posteriors using the following
methods.
EP1 Algorithm 7.2 with one forward-backward pass. The computational complexity1 of
this algorithm is O(M2T), with M the number of regimes and T the number of time
slices.
EP Algorithm 7.2 until convergence or at most 20 iterations. The computational complex-
ity is O(M2TI), with I the number of iterations.
ABP Using the approach from [14] described in Section 7.7, the associated complexity is
O(M2T).
Gibbs 1000 samples generated using the eﬃcient Gibbs sampler from [4]. In this sampler
the continuous latent states x1:T are integrated out analytically. The computational
cost is O(KMT), with K the number of samples. The ﬁrst 20 samples of the MCMC
chain are discarded. Note that with K = 1020 the computational complexity of this
approach for the current setting with T = 8 and M = 2 is higher than that of exact
computation which has associated complexity O(MT).
RBPS-M Using Rao–Blackwellised particle smoothing [6, 5]. As in the Gibbs sampler,
the performance of the particle smoother is signiﬁcantly improved by analytically
integrating out x1:T. The number of particles is set to be identical to the number of
regimes. The computational complexity is O(KMT), thus choosing the number of
particles, K, equal to M results in computational costs identical to EP1. Since the
sequences are short, relatively few resampling steps are performed, and as a result
the diversity at the start of the drawn sequences s1:T is still acceptable. Therefore
we did not implement extra ‘rejuvenation’ methods [6] to increase variance on the
smoothing pass.
RBPS-10M Using the Rao–Blackwellised particle smoother with the number of particles
10 times the number of regimes.
Ideally we would compute statistics such as KL

pexact(zt)||papprox(zt)

for every time slice
and every approximation method. However this often leads to an inﬁnite KL divergence for
sampling approaches, since it is not uncommon that for a particular time slice one of the
regimes is not represented by at least one sample. Instead we compute the mean squared
error for the posterior state mean, and the mean KL after collapsing both the exact and
approximated posteriors onto a single Gaussian per state xt
1
T
T
X
t=1
KL

Collapse(p(xt|y1:T, θ))||Collapse(papprox(xt))

,
i.e. a KL divergence that gives an indication of how well the mean and covariances of
the exact and approximated posteriors match. Figure 7.4 gives the MSE based ranks of
the algorithms we compared, an analogous ﬁgure with KL based ranks looks indistin-
guishable for this set of experiments and is not shown. Figure 7.5(a) gives a typical
result.
1The ‘big-O’ notation gives the order of the computation time disregarding constant factors which depend on
the particular implementation. The complexities of operations on Gaussian potentials depend on the dimensions
of the observations and the states, but are equal for all methods. We therefore treat these operations as constants
in the complexity description.

Expectation propagation for switching linear systems
157
123456
0
50
100
rank
EP
123456
0
50
100
rank
EP1
123456
0
50
100
rank
ABP
123456
0
50
100
rank
Gibbs
123456
0
50
100
rank
RBPS−M
123456
0
50
100
rank
RBPS−10M
Figure 7.4 Histogram of ranks of the tested algorithms on 100 artiﬁcially generated problems. Rank 1 indicates
smallest distance with exact state posterior in the MSE sense, rank 6 indicates largest distance.
0
500
1000
1500
2000
10
−15
10
−10
10
−5
10
0
Computation steps
mse
EP
ABP
Gibbs
PS
(a)
0
1000
2000
3000
4000
10
−5
10
−4
10
−3
10
−2
10
−1
Computation steps
mse
EP
Gibbs
(b)
Figure 7.5 (a) Results from a ‘typical’ task from the experiments represented in Fig. 7.4. Mean squared error
is shown versus ‘computation steps’. One iteration of EP is equivalent to M2 such computation steps, draw-
ing K samples for the Gibbs sampler or working with K particles in the particle smoother is equivalent to KM
steps. This makes the x-axis roughly proportional to CPU time. For the particle smoother, 14 diﬀerent runs with
diﬀerent values for K were used, a line connects these for clarity. (b) Results from an experiment with a long
observation sequence. Mean squared errors in the posterior state means for ﬁve independent Gibbs runs and EP
are shown. The estimate based on samples from the combined ﬁve Gibbs runs is taken as ground truth. Observe
that the distance of the deterministic approximation to the ground truth is of the same order as the individual
Gibbs runs.
From Fig. 7.4 we see that Algorithm 7.2 nearly always outperforms traditional meth-
ods. The results from Fig. 7.5(a) show that all methods give reasonable results (although
Fig. 7.5(a) only shows results for a single task, the results for the others are comparable).
Although both the particle smoother and the Gibbs sampler would give exact results in the
limit of inﬁnite K, the rate at which this is attained is slow. If we stop after a ﬁnite, but con-
siderable time, we see that for the models studied here Algorithm 7.2 gives more accurate
results, and does so in a fraction of the computation time. Comparing the results between
the ﬁrst iteration for EP with the forward-backward pass in ABP from [14], we see that the
extra approximations in the backward pass in [14] indeed have a negative eﬀect.
The results presented in this section represent properties of the set of SLDS models
associated with the generating parameters as described above. It would be worthwhile to
more carefully explore the space of SLDS models. The work by [23] gives some support
for generality. In their paper expectation propagation for the SLDS is derived as well and
experiments compare the performance of EP and a Rao–Blackwellised particle ﬁlter on a
signal detection problem. The results are similar to the ones presented here: for the sampler
to achieve results similar in quality to the EP iteration signiﬁcantly more CPU cycles are
needed.

158
Onno Zoeter and Tom Heskes
1
2
3
4
5
0
1
2
3
4
5
6
7
x 10
−4
Replicated experiments
mse
EP
Gibbs
(a)
Figure 7.6 (a) Results from ﬁve experiments similar to the experiment from Fig. 7.5(b) based on three instead of
ﬁve individual chains. For every replication the mean squared distance in the posterior state means between EP
and the ground truth, and three individual Gibbs chains and the ground truth are plotted. The ground truth is taken
to be the state means based on the combined three Gibbs chains. (b) Maximum absolute deviation between exact
and approximate single-slice posterior state mean as a function of κ. Shown are the mean, maximum and minimum
over ten replications. In all replications T = 10, so κ = 4 gives exact results. The small diﬀerences between the
mean, maximum and minimum deviations that are observed in the plot for κ = 4 are caused by diﬀerent round-oﬀ
errors in the generalised EP and the original strong junction tree implementations.
7.8.2
Comparisons with Gibbs sampling
In this section we study the quality of the approximations on sequences with 100
observations. In our experiments we compare the deterministic approximation with ﬁve
independent runs of the Gibbs sampler, drawing 2000 samples each. This corresponds
roughly to a minute computation time for the deterministic approach and a weekend for
the ﬁve runs.
Figure 7.5(b) shows a comparison of the posterior means based on the individual Gibbs
runs and the deterministic approximation and the posterior means computed from the com-
bined samples of all the ﬁve runs. As can be seen all approximations lie relatively close,
which probably indicates that all approximations give reasonably correct results.
Since for any three vectors a, b and c
(a −c)T(a −c) = 2

(a −b)T(a −b) + (b −c)T(b −c)

−(a + c −2b)T(a + c −2b)
≤2

(a −b)T(a −b) + (b −c)T(b −c)

,
we have an eﬀective bound on the mean squared error of EP. Taking a, b and c to be the EP,
Gibbs and exact posterior state means respectively, we get that the EP MSE is bounded by
two times the sum of the Gibbs MSE and the Gibbs-EP mean squared distance, that is
EEP ≤2  EGibbs + DGibbs,EP
 .
(7.25)
Thus from Eq. (7.25), and the analogous bound for the Gibbs error, we see that the dif-
ference in error between the Gibbs and EP approximation is of the order DGibbs,EP. This
Gibbs–EP mean squared distance can be read oﬀfrom Fig. 7.5(b) and is relatively small.
Figure 7.5(b) shows results from a single experiment. In Fig. 7.6(a) the results of ﬁve
replications of this experiment (based on three instead of ﬁve individual Gibbs chains)
are shown. In the ﬁve replications the mean squared distance between EP and Gibbs is
consistently small.

Expectation propagation for switching linear systems
159
The experiments in this section give empirical evidence that our proposed approxima-
tion does not break down on longer sequences. For the analogous approximate ﬁlter in a
fully discrete network (where the projection is onto a factorised distribution) Boyen and
Koller [3] show that the errors incurred by the approximation disappear at a geometric rate
due to the stochastic nature of the transition model. Intuitively, as exact and approximate
estimates are propagated through the transition model some of the ‘information’ in both is
forgotten, resulting in a smearing eﬀect which makes both predicted distributions closer.
Although the experiments in this section support a conjecture that such a proof can be
extended to the conditional Gaussian case, the required technical conditions on the model
and the proof itself remain work for future research.
7.8.3
Eﬀect of larger outer clusters
To explore the eﬀect of κ in Algorithm 7.3, we ran an experiment where a sequence of
length 10 was generated from a model drawn from the same distribution as in Section 7.8.1.
For every sequence, approximate single node posteriors ˜q(xt|y1:T, θ) were computed using
Algorithm 7.3 with κ = 0, 1, 2, 3, 4. Since the sequence is of length 10, it is also possible to
compute the exact posterior. Figure 7.6(b) shows the maximum absolute error in the single
node posterior means as a function of κ. The lines show the average over 10 replications,
the maximum encountered and the minimum.
For sequences with length 10, κ = 4 is guaranteed to give exact results. So in theory, the
lines in Fig. 7.6(b) should meet at κ = 4. The discrepancies in Fig. 7.6(b) are explained by
diﬀerent round-oﬀerrors in our implementations of Algorithm 7.3 and the strong junction
tree algorithm.
In all our experiments we have observed that, with the inference problem ﬁxed, the
accuracy in the posterior mean systematically increases when κ is increased. It should be
noted however that due to the local nature of the approximations such increases are not
guaranteed. This is completely analogous to the fact that a generalised belief propaga-
tion algorithm need not yield results that are better than the regular belief propagation
approximation on which they are based. In fact in certain applications of generalised
belief propagation where loops are ignored signiﬁcant decreases in accuracy have been
observed (see e.g. [13]). However in the generalised expectation propagation algorithm,
and particularly in its use here for the SLDS where no loops are disregarded, we would
actually expect the results we see here: keeping more components in the approximation in
general will increase the accuracy of the approximation.
7.9
Discussion
This chapter has discussed a deterministic approximation scheme for the well-known infer-
ence problem in conditionally Gaussian state space models. Whereas the complexity of
exact inference scales exponentially with the number of observations, the new approximate
method requires computation time linear in the number of observations.
The approach can be seen as a symmetric backward pass to previously proposed
assumed density ﬁltering methods and is based on the expectation propagation algorithm.
In the literature several alternative backward passes have been introduced. An important
beneﬁt of the method described in this chapter is that the underlying philosophy for the for-
ward and backward passes are the same and that, unlike the previously known methods, no
additional approximations need to be made in the backward pass. Numerical experiments

160
Onno Zoeter and Tom Heskes
suggest that removing these additional approximations leads to a signiﬁcant increase in
accuracy.
The approximation method works for the general switching linear dynamical system.
No speciﬁc assumptions such as invertibility of transition or observation models are needed.
Since both the forward and the backward passes perform greedy and local approximations
it makes sense to iterate passes to ﬁnd a ‘best’ approximation. Section 7.5 described a
variant of the Bethe free-energy that is related to such a scheme. Fixed points of iterations
of forward-backward passes correspond to extrema of this energy. The ﬁxed points have a
natural interpretation closely related to properties of the exact posterior.
An extension of the basic algorithm can be obtained based on generalised expectation
propagation. Such an extension keeps more components in the approximation. It does so
in such a way that these extended approximations from the ﬁltering pass can be improved
in a principled way in the smoothing pass. In our experiments we see that increasing the
size of the outer clusters in the approximation, and hence increasing the number of com-
ponents that are retained in the approximation, systematically increases the accuracy of the
approximation.
The main beneﬁt of the more involved approximation is expected to be for ‘diﬃcult’
models where the ambiguity in the discrete states can only be resolved based on observa-
tions from several consecutive time steps. However a formal characterisation of ‘easy’ and
‘diﬃcult’ problems currently does not exist and a systematic empirical study remains work
for future research.
In general we feel that expectation propagation and generalised expectation propaga-
tion based approximations form a promising foundation for fast and accurate inference
algorithms for the switching linear dynamical system. But several open issues remain.
The ﬁrst point is that the iteration schemes in Algorithms 7.2 and 7.3 are not guaranteed
to converge. More elaborate double-loop techniques exist [11], but these are signiﬁcantly
slower, so an appropriate adaptation of the message-passing algorithm is highly desirable.
Furthermore, the double-loop schemes do not prevent problems with supportiveness and
may even make things worse.
A second more technical point is that in the algorithm CG potentials are multiplied and
marginalised. For the multiplication canonical parameters are necessary, the marginalisa-
tion requires moments. The conversion from one form to the other implies the inversion
of covariance and precision matrices. This can result in numerical problems in a basic
implementation. There currently does not exist an implementation that is guaranteed to be
numerically stable yet avoids additional approximation in the backward pass. Chapter 8
discusses these problems in more detail and introduces an alternative.
Perhaps the most ambitious direction of future research, is that of a guarantee of the
approximation quality. The authors in [19] show that for the general switching linear
dynamical system constant factor approximations remain NP-hard. But one might hope
that either for a speciﬁc subclass of models or after observing a speciﬁc sequence of
observations a guarantee can be given.
7.A
Appendix: Operations on conditional Gaussian potentials
To allow for simple notation in the main text this appendix introduces the conditional
Gaussian (CG) distribution. A discrete variable s and a continuous variable x are jointly
CG distributed if the marginal of s is multinomial distributed and, conditioned on s, x is

Expectation propagation for switching linear systems
161
Gaussian distributed. Let x be d-dimensional and let S be the set of values s can take. In
the moment form the joint distribution reads
p(s, x) = πs(2π)−d/2|Σs|−1/2 exp
"
−1
2(x −µs)TΣ−1
s (x −µs)
#
,
with moment parameters {πs, µs, Σs + µsµT
s}, where πs is positive for all s and satisﬁes
P
s πs = 1, and Σs is a positive deﬁnite matrix. The deﬁnition of Σs + µsµT
s instead of Σs
is motivated by Eq. (7.28) below. For compact notation sets with elements dependent on s
will implicitly range over s ∈S . In canonical form the CG distribution is given by
p(s, x) = exp
"
gs + xThs −1
2xTKsx
#
,
(7.26)
with canonical parameters {gs, hs, Ks}.
The so-called link function g(·) maps canonical parameters to moment parameters
g({gs, hs, Ks}) = {πs, µs, Σs + µsµT
s}
πs = exp(gs −¯gs)
µs = K−1
s hs
Σs = K−1
s ,
with ¯gs ≡1
2 log | Ks
2π| −1
2hT
s Kshs, the part of gs that depends on hs and Ks. The link function
is unique and invertible
g−1({πs, µs, Σs + µsµT
s}) = {gs, hs, Ks}
gs = log πs −1
2 log |2πΣs| −1
2µT
sΣ−1
s µs
hs = Σ−1
s µs
Ks = Σ−1
s .
A conditional Gaussian potential is a generalisation of the above distribution in the
sense that it has the same form as in Eq. (7.26) but need not integrate to 1; Ks is restricted
to be symmetric, but need not be positive deﬁnite. If Ks is positive deﬁnite the moment
parameters are determined by g(.). In this section we will use φ(s, x; {gs, hs, Ks}) to denote
a CG potential over s and x with canonical parameters {gs, hs, Ks}.
Multiplication and division of CG potentials are the straightforward extensions of the
analogous operations for multinomial and Gaussian potentials. In canonical form
φ(s, x; {gs, hs, Ks})φ(s, x; {g′
s, h′
s, K′
s}) = φ(s, x; {gs + g′
s, hs + h′
s, Ks + K′
s}),
φ(s, x; {gs, hs, Ks})/φ(s, x; {g′
s, h′
s, K′
s}) = φ(s, x; {gs −g′
s, hs −h′
s, Ks −K′
s}).
With the above deﬁnition of multiplication we can deﬁne a unit potential
1(s, x) ≡φ(s, x; {0, 0, 0}),
which satisﬁes 1(s, x)p(s, x) = p(s, x) for all CG potentials p(s, x). We will sometimes use
the shorthand 1 for the unit potential when its domain is clear from the text.
In a similar spirit we can deﬁne multiplication and division of potentials with diﬀerent
domains. If the domain of one of the potentials (the denominator in case of division) forms
a subset of the domain of the other we can extend the smaller to match the larger and

162
Onno Zoeter and Tom Heskes
perform a regular multiplication or division as deﬁned above. The continuous domain of
the small potential is extended by adding zeros in hs and Ks at the corresponding positions.
The discrete domain is extended by replicating parameters, e.g. extending s to [s t]T we use
parameters gst = gs, hst = hs, and Kst = Ks.
Marginalisation is less straightforward for CG potentials. Integrating out continuous
dimensions is analogous to marginalisation in Gaussian potentials and is only deﬁned if
the corresponding moment parameters are deﬁned. Marginalisation is then deﬁned as con-
verting to moment form, ‘selecting’ the appropriate rows and columns from µs and Σs, and
converting back to canonical form. More problematic is the marginalisation over discrete
dimensions of the CG potential. Summing out s results in a distribution p(x) which is a
mixture of Gaussians with mixing weights p(s), i.e. the CG family is not closed under
summation.
We deﬁne weak marginalisation [18] as exact marginalisation followed by a collapse:
a projection of the exact marginal onto the CG family. The projection minimises the
Kullback–Leibler divergence KL(p||q) between p, the exact (strong) marginal and q, the
weak marginal
q(s, x) = argmin
q∈CG
KL (p||q) ≡argmin
q∈CG
X
s,x
p(s, x) log p(s, x)
q(s, x) .
This projection is only deﬁned for properly normalised distributions p and has the prop-
erty that, conditioned on s the weak marginal has the same mean and covariance as the
exact marginal. The weak marginal can be computed by moment matching (see e.g. [27]). If
p(x|s) is a mixture of Gaussians for every s with mixture weights πr|s, means µsr, and covari-
ances Σsr (e.g. the exact marginal P
r p(s, r, x) of CG distribution p(s, r, x)), the moment
matching procedure is deﬁned as
Collapse (p(s, x)) ≡p(s)N(x; µs, Σs)
µs ≡
X
r
πr|sµsr
Σs ≡
X
r
πr|s

Σsr + (µsr −µs)(µsr −µs)T
.
Contrary to exact marginalisation, this projection is not linear, and hence in general:
Collapse (p(s, x)q(x)) , Collapse (p(s, x)) q(x).
In even more compact notation, denoting with δs,m the Kronecker delta function, we
can write a CG potential as
p(s, x) = exp[νT f(s, x)],
(7.27)
with
f(s, x) ≡[δs,m δs,mxTδs,mvec(xxT)T|m ∈S ]T,
ν ≡[gs hT
s −1
2vec(Ks)T|s ∈S ]T
the suﬃcient statistics and the canonical parameters respectively. In this notation the
moment parameters follow from the canonical parameters as
g(ν) = ⟨f(s, x)⟩exp[νT f(s,x)] ≡
X
s
Z
dx f(s, x) exp[νT f(s, x)].
(7.28)

Expectation propagation for switching linear systems
163
7.B
Appendix: Proof of Theorem 7.1
In this section we present the proof of Theorem 7.1. The proof and intuition are analogous
to the result that ﬁxed points of loopy belief propagation can be mapped to extrema of the
Bethe free-energy [28].
Theorem 7.1 The collection of beliefs ˆpt(zt−1,t) and ˆqt(zt) form ﬁxed points of Algorithm 7.2
if and only if they are zero gradient points of FEP under the appropriate constraints.
Proof. The properties of the ﬁxed points of message passing follow from the description
of Algorithm 7.2. We get the CG form (7.27) of messages αt and βt and their relationship
with one- and two-slice marginals
ˆpt(zt−1,t) ∝αt−1(zt−1)ψt(zt−1,t)βt(zt),
ˆqt(zt) ∝αt(zt)βt(zt)
by construction, and consistency after a collapse
⟨f(zt)⟩ˆpt = ⟨f(zt)⟩ˆqt = ⟨f(zt)⟩ˆpt+1 ,
(7.29)
as a property of a ﬁxed point.
To identify the nature of stationary points of FEP we ﬁrst construct the Lagrangian by
adding Lagrange multipliers αt(zt) and βt(zt) for the forward and backward consistency
constraints and γt−1,t and γt for the normalisation constraints.
LEP( ˆp, ˆq, α, β, γ) =
T
X
t=1
X
zt−1,t
ˆpt(zt−1,t) log ˆpt(zt−1,t)
ψt(zt−1,t) −
T−1
X
t=2
X
zt
ˆqt(zt) log ˆqt(zt)
−
T−1
X
t=2
αt−1(zt−1)T

X
zt−1,t
ft−1(zt−1) ˆpt(zt−1,t) −
X
zt−1
ft−1(zt−1)ˆqt−1(zt−1)

−
T−1
X
t=2
βt(zt)T

X
zt−1,t
ft(zt) ˆpt(zt−1,t) −
X
zt
ft(zt)ˆqt(zt)

−
T
X
t=1
γt−1,t

X
zt−1,t
ˆpt(zt−1,t) −1
−
T−1
X
t=2
γt

X
zt
ˆqt(zt) −1
.
Note that αt(zt) and βt(zt) (in boldface to distinguish them from messages and to emphasise
that they are vectors) are vectors of canonical parameters as deﬁned in Appendix 7.A.
The stationarity conditions follow by setting the partial derivatives to 0. Taking
derivatives w.r.t. ˆpt(zt−1,t) and ˆqt(zt) gives
∂LEP
∂ˆpt(zt−1,t) = log ˆpt(zt−1,t) + 1 −log ψt(zt−1,t) −αt−1(zt−1)T ft−1(zt−1) −βt(zt)T ft(zt) −γt−1,t,
∂LEP
∂ˆqt(zt) = −log ˆqt(zt) −1 + αt(zt)T ft(zt) + βt(zt)T ft(zt) −γt.
Setting above derivatives to 0 and ﬁlling in the solutions for γt−1,t and γt (which simply
form the log of the normalisation constants) results in
ˆpt(zt−1,t) ∝eαt−1(zt−1)T ft−1(zt−1)ψt(zt−1,t)eβt(zt)T ft(zt),
ˆqt(zt) ∝eαt(zt)T ft(zt)+βt(zt)T ft(zt).

164
Onno Zoeter and Tom Heskes
The conditions
∂LEP
∂αt(zt) = 0 and
∂LEP
∂βt(zt) = 0 retrieve the forward-equals-backward constraints
(7.29).
So if we identify αt as the vector of the canonical parameters of the message αt and βt
as the vector of the canonical parameters of the message βt, we see that the conditions for
stationarity of FEP and ﬁxed points of Algorithm 7.2 are the same.
□
As can be seen from the above proof, iteration of the forward-backward passes can be
interpreted as ﬁxed-point iteration in terms of Lagrange multipliers.
Bibliography
[1] Y. Bar-Shalom and X.-R. Li. Estimation and
Tracking: Principles, Techniques, and Software.
Artech House, 1993.
[2] Y. Bar-Shalom and T. Fortmann. Tracking and
Data Association. Academic Press, 1988.
[3] X. Boyen and D. Koller. Tractable inference for
complex stochastic processes. In Proceedings of
the 14th Annual Conference on Uncertainty in
Artiﬁcial Intelligence, pages 33-42 Morgan
Kaufmann Publishers, 1998.
[4] C. Carter and R. Kohn. Markov chain Monte
Carlo in conditionally Gaussian state space
models. Biometrika, 83(3):589–601, 1996.
[5] R. Chen and J. S. Liu. Mixture Kalman ﬁlters.
Journal of the Royal Statistical Society, Series B,
62:493–508, 2000.
[6] A. Doucet, N. de Freitas, K. Murphy and
S. Russel. Rao-Blackwellized particle ﬁltering
for dynamic Bayesian networks. In Proceedings
of the 17th Annual Conference on Uncertainty in
Artiﬁcial Intelligence pages 176–183. Morgan
Kaufmann Publishers, 2001.
[7] Z. Ghahramani and G. E. Hinton. Variational
learning for switching state-space models.
Neural Computation, 12(4):963–996, 1998.
[8] J. Hamilton. A new approach to the economic
analysis of nonstationary time series and the
business cycle. Econometrica, 57(2):357–384,
1989.
[9] P. J. Harrison and C. F. Stevens. Bayesian
forecasting. Journal of the Royal Statistical
Society Society B, 38:205–247, 1976.
[10] R. E. Helmick, W. D. Blair and S. A. Hoﬀman.
Fixed-interval smoothing for Markovian
switching systems. IEEE Transactions on
Information Theory, 41:1845–1855, 1995.
[11] T. Heskes and O. Zoeter. Expectation
propagation for approximate inference in
dynamic Bayesian networks. In Proceedings of
the 18th Annual Conference on Uncertainty in
Artiﬁcial Intelligence, pages 216–223. Morgan
Kaufmann Publishers, 2002.
[12] T. Heskes and O. Zoeter. Generalized belief
propagation for approximate inference in hybrid
Bayesian networks. In C. Bishop and B. Frey,
editors, Proceedings of the Ninth International
Workshop on Artiﬁcial Intelligence and
Statistics, 2003.
[13] H. J. Kappen and W. Wiegerinck. Novel iteration
schemes for the cluster variation method. In
Advances in Neural Information Processing
Systems 14, pages 415–422. MIT Press, 2002.
[14] C.-J. Kim and C. R. Nelson. State-Space Models
with Regime Switching. MIT Press, 1999.
[15] G. Kitagawa. Monte Carlo ﬁlter and smoother
for non-Gaussian nonlinear state space models.
Journal of Computational and Graphical
Statistics, 5(1):1–25, 1996.
[16] F. R. Kschischang, B. J. Frey and H.-A. Loeliger.
Factor graphs and the sum-product algorithm.
IEEE Transactions on Information Theory,
47(2):498–519, 2001.
[17] S. Kullback and R. A. Leibler. On information
and suﬃciency. Annals of Mathematical
Statistics, 22(1):76–86, 1951.
[18] S. L. Lauritzen. Propagation of probabilities,
means, and variances in mixed graphical
association models. Journal of the American
Statistical Association, 87:1098–1108, 1992.
[19] U. Lerner and R. Parr. Inference in hybrid
networks: Theoretical limits and practical
algorithms. In Proceedings of the 17th Annual
Conference on Uncertainty in Artiﬁcial
Intelligence, pages 310–318. Morgan Kaufmann
Publishers, 2001.
[20] U. Lerner, R. Parr, D. Koller and G. Biswas.
Bayesian fault detection and diagnosis in
dynamic systems. In Proceedings of the 17th
National Conference on Artiﬁcial Intelligence,
pages 531–537, 2000.
[21] T. Minka. Expectation propagation for
approximate Bayesian inference. In Proceedings
of the 17th Annual Conference on Uncertainty in
Artiﬁcial Intelligence (UAI 2001). Morgan
Kaufmann Publishers, 2001.
[22] T. Minka and Y. Qi. Tree-structured
approximations by expectation propagation. In
Advances in Neural Information Processing
Systems, pages 193–200. MIT Press, 2004.
[23] Y. Qi and T. Minka. Expectation propagation for
signal detection in ﬂat-fading channels. IEEE
transactions on Wireless Communications,
6:348–355, 2007.
[24] N. Shephard. Partial non-Gaussian state space
models. Biometrika, 81:115–131, 1994.

Expectation propagation for switching linear systems
165
[25] R. H. Shumway and D. S. Stoﬀer. Dynamic
linear models with switching. Journal of the
American Statistical Association, 86:763–769,
1991.
[26] M. Welling, T. Minka and Y. W. Teh. Structured
region graphs: Morphing EP into GBP. In
Proceedings of the Twenty-First Conference
Annual Conference on Uncertainty in Artiﬁcial
Intelligence, pages 609–614. AUAI Press, 2005.
[27] J. Whittaker. Graphical Models in Applied
Multivariate Statistics. John Wiley and Sons,
1989.
[28] J. Yedidia, W. Freeman and Y. Weiss.
Generalized belief propagation. In Advances in
Neural Information Processing Systems 13,
pages 689–695, MIR Press, 2001.
[29] J. Yedidia, W. Freeman and Y. Weiss.
Constructing free energy approximations and
generalized belief propagation algorithms.
Technical report, MERL, 2004.
[30] O. Zoeter and T. Heskes. Change point problems
in linear dynamical systems. Journal of Machine
Learning Research, 6:1999–2026, 2005.
Contributors
Onno Zoeter, Xerox Research Centre Europe, 6 Chemin de Maupertuis, 36240 Meylan, France
Tom Heskes, Institute for Computing and Information Sciences, Radboud University Nijmegen,
Heyendaalseweg 135, 6525 AJ Nijmegen, The Netherlands

8
Approximate inference in switching linear dynamical
systems using Gaussian mixtures
David Barber
8.1
Introduction
The linear dynamical system (LDS) (see Section 1.3.2) is a standard time series model in
which a latent linear process generates the observations. Complex time series which are
not well described globally by a single LDS may be divided into segments, each modelled
by a potentially diﬀerent LDS. Such models can handle situations in which the underlying
model ‘jumps’ from one parameter setting to another. For example a single LDS might well
represent the normal ﬂows in a chemical plant. However, if there is a break in a pipeline,
the dynamics of the system changes from one set of linear ﬂow equations to another. This
scenario could be modelled by two sets of linear systems, each with diﬀerent parameters,
with a discrete latent variable at each time st ∈{normal, pipe broken} indicating which of
the LDSs is most appropriate at the current time. This is called a switching LDS (SLDS)
and used in many disciplines, from econometrics to machine learning [2, 9, 15, 13, 12, 6,
5, 19, 21, 16].
8.2
The switching linear dynamical system
At each time t, a switch variable st ∈{1, . . . , S } describes which of a set of LDSs is to be
used. The observation (or ‘visible’) variable vt ∈RV is linearly related to the hidden state
ht ∈RH by
vt = B(st)ht + ηv(st),
ηv(st) ∼N  ηv(st) ¯v(st), Σv(st) .
(8.1)
Here st describes which of the set of emission matrices B(1), . . . , B(S ) is active at time t.
The observation noise ηv(st) is drawn from one of a set of Gaussians with diﬀerent means
¯v(st) and covariances Σv(st). The transition of the continuous hidden state ht is linear,
ht = A(st)ht−1 + ηh(st),
ηh(st) ∼N

ηh(st) ¯h(st), Σh(st)

,
(8.2)
and the switch variable st selects a single transition matrix from the available set
A(1), . . . , A(S ). The Gaussian transition noise ηh(st) also depends on the switch vari-
ables. The dynamics of st itself is Markovian, with transition p(st|st−1). For the more
general‘augmented’ (aSLDS) model the switch st is dependent on both the previous st−1

Inference in switching linear systems using mixtures
167
s1
h1
v1
s2
h2
v2
s3
h3
v3
s4
h4
v4
Figure 8.1 The independence structure of the aSLDS. Square
nodes st denote discrete switch variables; ht are continu-
ous latent/hidden variables, and vt continuous observed/vis-
ible variables. The discrete state st determines which linear
dynamical system from a ﬁnite set of linear dynamical sys-
tems is operational at time t. In the SLDS links from h to s are
not normally considered.
and ht−1. The probabilistic model deﬁnes a joint distribution, see Fig. 8.1,
p(v1:T, h1:T, s1:T) =
T
Y
t=1
p(vt|ht, st)p(ht|ht−1, st)p(st|ht−1, st−1),
p(vt|ht, st) = N (vt ¯v(st) + B(st)ht, Σv(st)) ,
p(ht|ht−1, st) = N

ht ¯h(st) + A(st)ht−1, Σh(st)

.
At time t = 1, p(s1|h0, s0) denotes the prior p(s1), and p(h1|h0, s1) denotes p(h1|s1). The
SLDS can be thought of as a marriage between a hidden Markov model and an LDS. The
SLDS is also called a jump Markov model/process, switching Kalman ﬁlter, switching
linear Gaussian state space model, conditional linear Gaussian model.
8.2.1
Exact inference is computationally intractable
Performing exact ﬁltered and smoothed inference in the SLDS is intractable, scaling expo-
nentially with time, see for example [16]. As an informal explanation, consider ﬁltered
posterior inference, for which, by analogy with Section 1.4.1 the forward pass is
p(st+1, ht+1|v1:t+1) =
X
st
Z
ht
p(st+1, ht+1|st, ht, vt+1)p(st, ht|v1:t).
(8.3)
At time step 1, p(s1, h1|v1) = p(h1|s1, v1)p(s1|v1) is an indexed Gaussian. At time step 2,
due to the summation over the states s1, p(s2, h2|v1:2) is an indexed set of S Gaussians. In
general, at time t, p(st, ht|v1:t) is an indexed set of S t−1 Gaussians. Even for small t, the num-
ber of components required to exactly represent the ﬁltered distribution is computationally
intractable. Analogously, smoothing is also intractable.
The origin of the intractability of the SLDS therefore diﬀers from ‘structural intractabil-
ity’ since, in terms of the cluster variables x1:T with xt ≡(st, ht) and visible variables
v1:T, the graph of the distribution is singly connected. From a purely graph-theoretic view-
point, one would therefore envisage no diﬃculty in carrying out inference. Indeed, as we
saw above, the derivation of the ﬁltering algorithm is straightforward since the graph of
p(x1:T, v1:T) is singly connected. However, the numerical representation of the messages
requires an exponentially increasing number of terms.
In order to deal with this intractability, several approximation schemes have been intro-
duced, [8, 9, 15, 13, 12]. Here we focus on techniques which approximate the switch
conditional posteriors using a limited mixture of Gaussians. Since the exact posterior
distributions are mixtures of Gaussians, but with an exponentially large number of com-
ponents, the aim is to drop low-weight components such that the resulting limited number
of Gaussians still accurately represents the posterior.

168
David Barber
8.3
Gaussian sum ﬁltering
Equation (8.3) describes the exact ﬁltering recursion. Whilst the number of mixture com-
ponents increases exponentially with time, intuitively one would expect that there is an
eﬀective time scale over which the previous visible information is relevant. In general, the
inﬂuence of ancient observations will be much less relevant than that of recent observa-
tions. This suggests that a limited number of components in the Gaussian mixture should
suﬃce to accurately represent the ﬁltered posterior [1].
Our aim is to form a recursion for p(st, ht|v1:t) using a Gaussian mixture approxi-
mation of p(ht|st, v1:t). Given an approximation of the ﬁltered distribution p(st, ht|v1:t) ≈
q(st, ht|v1:t), the exact recursion (8.3) is approximated by
q(st+1, ht+1|v1:t+1) =
X
st
Z
ht
p(st+1, ht+1|st, ht, vt+1)q(st, ht|v1:t).
(8.4)
This approximation to the ﬁltered posterior at the next time step will contain S times more
components than at the previous time step. Therefore to prevent an exponential explosion
in mixture components we need to collapse this mixture in a suitable way. We will deal
with this once the new mixture representation for the ﬁltered posterior has been computed.
To derive the updates it is useful to break the ﬁltered approximation from Eq. (8.4) into
continuous and discrete parts
q(ht, st|v1:t) = q(ht|st, v1:t)q(st|v1:t),
(8.5)
and derive separate ﬁltered update formulae, as described below. An important remark
is that many techniques approximate p(ht|st, v1:t) using a single Gaussian. Naturally, this
gives rise to a mixture of Gaussians for p(ht|v1:t). However, in making a single Gaussian
approximation to p(ht|st, v1:t) the representation of the posterior may be poor. Our aim here
is to maintain an accurate approximation to p(ht|st, v1:t) by using a mixture of Gaussians.
8.3.1
Continuous ﬁltering
The exact representation of p(ht|st, v1:t) is a mixture with S t−1 components. To retain
computational feasibility we approximate this with a limited I-component mixture
q(ht|st, v1:t) =
IX
it=1
q(ht|it, st, v1:t)q(it|st, v1:t),
where q(ht|it, st, v1:t) is a Gaussian parameterised with mean f(it, st) and covariance F(it, st).
Strictly speaking, we should use the notation ft(it, st) since, for each time t, we have a set
of means indexed by it, st, but we drop these dependencies in the notation used here.
To ﬁnd a recursion for the approximating distribution, we ﬁrst assume that we know
the ﬁltered approximation q(ht, st|v1:t) and then propagate this forwards using the exact
dynamics. To do so consider ﬁrst the exact relation
q(ht+1|st+1, v1:t+1) =
X
st,it
q(ht+1, st, it|st+1, v1:t+1)
=
X
st,it
q(ht+1|st, it, st+1, v1:t+1)q(st, it|st+1, v1:t+1).
(8.6)

Inference in switching linear systems using mixtures
169
Wherever possible we substitute the exact dynamics and evaluate each of the two factors
above. By decomposing the update in this way the new ﬁltered approximation is of the form
of a Gaussian mixture, where q(ht+1|st, it, st+1, v1:t+1) is Gaussian and q(st, it|st+1, v1:t+1) are
the weights or mixing proportions of the components. We describe below how to com-
pute these terms explicitly. Equation (8.6) produces a new Gaussian mixture with I × S
components which we collapse back to I components at the end of the computation.
Evaluating q(ht+1|st, it, st+1, v1:t+1)
We aim to ﬁnd a ﬁltering recursion for q(ht+1|st, it, st+1, v1:t+1). Since this is conditional on
switch states and components, this corresponds to a single LDS forward step, which can be
evaluated by considering ﬁrst the joint distribution
q(ht+1, vt+1|st, it, st+1, v1:t)=
Z
ht
p(ht+1, vt+1|ht,st, it, st+1, v1:t)q(ht|st, it,
st+1, v1:t),
and subsequently conditioning on vt+1. To ease the burden on notation we assume ¯ht, ¯vt ≡0
for all t. By propagating
q(ht|v1:t, it, st) = N (ht f(it, st), F(it, st))
with the dynamics (8.1) and (8.2), we obtain that q(ht+1, vt+1|st, it, st+1, v1:t) is a Gaussian
with covariance and mean elements
Σhh = A(st+1)F(it, st)AT(st+1) + Σh(st+1),
Σvv = B(st+1)ΣhhBT(st+1) + Σv(st+1),
Σvh = B(st+1)Σhh = ΣT
hv,
µv = B(st+1)A(st+1)f(it, st),
µh = A(st+1)f(it, st).
(8.7)
These results are obtained from integrating the exact dynamics over ht, using the results in
Section 1.4.7.
To ﬁnd q(ht+1|st, it, st+1, v1:t+1) we condition q(ht+1, vt+1|st, it, st+1, v1:t) on vt+1 using the
standard Gaussian conditioning formula, Section 1.4.7, to obtain
q(ht+1|st, it, st+1, v1:t+1) = N

ht+1 µh|v, Σh|v

with
µh|v = µh + ΣhvΣ−1
vv
 vt+1 −µv
 ,
Σh|v = Σhh −ΣhvΣ−1
vv Σvh.
Evaluating the mixture weights q(st, it|st+1, v1:t+1)
The mixture weight in Eq. (8.6) can be found from
q(st, it|st+1, v1:t+1) ∝q(vt+1|it, st, st+1, v1:t)q(st+1|it, st, v1:t)q(it|st, v1:t)q(st|v1:t).
The factor q(vt+1|it, st, st+1, v1:t) is Gaussian with mean µv and covariance Σvv, as given in
Eq. (8.7). The factors q(it|st, v1:t) and q(st|v1:t) are given from the previous ﬁltered iteration.
Finally, q(st+1|it, st, v1:t) is found from (where angled brackets denote expectation)
q(st+1|it, st, v1:t) =
( ⟨p(st+1|ht, st)⟩q(ht|it,st,v1:t)
augmented SLDS,
p(st+1|st)
standard SLDS ,
(8.8)

170
David Barber
t+1
t
Figure 8.2 Gaussian sum ﬁltering. The leftmost column depicts the pre-
vious Gaussian mixture approximation q(ht, st|v1:t) for two states S = 2
(dashed and solid) and three mixture components I = 3. The area of the
oval indicates the weight of the component. There are S = 2 diﬀerent lin-
ear systems which take each of the components of the mixture into a new
ﬁltered state, the arrow indicating which dynamic system is used. After
one time step each mixture component branches into a further S com-
ponents so that the joint approximation q(ht+1, st+1|v1:t+1) contains S 2I
components (middle column). To keep the representation computationally
tractable the mixture of Gaussians for each state st+1 is collapsed back
to I components. This means that each state needs to be approximated
by a smaller I component mixture of Gaussians. There are many ways to
achieve this. A naive but computationally eﬃcient approach is to simply
ignore the lowest weight components, as depicted on the right column.
where the result above for the standard SLDS follows from the independence assumptions
present in the standard SLDS. In the aSLDS, the term in Eq. (8.8) will generally need to be
computed numerically. A simple approximation is to evaluate Eq. (8.8) at the mean value of
the distribution q(ht|it, st, v1:t). To take covariance information into account an alternative
would be to draw samples from the Gaussian q(ht|it, st, v1:t) and thus approximate the aver-
age of p(st+1|ht, st) by sampling. Note that this does not equate Gaussian sum ﬁltering with
a sequential sampling procedure, such as particle ﬁltering, Section 1.6.4. The sampling here
is exact, for which no convergence issues arise.
Closing the recursion
We are now in a position to calculate Eq. (8.6). For each setting of the variable st+1, we
have a mixture of I × S Gaussians. To prevent the number of components increasing
exponentially, we numerically collapse q(ht+1|st+1, v1:t+1) back to I Gaussians to form
q(ht+1|st+1, v1:t+1) →
IX
it+1=1
q(ht+1|it+1, st+1, v1:t+1)q(it+1|st+1, v1:t+1).
Any method of choice may be supplied to collapse a mixture to a smaller mixture. A
straightforward approach is to repeatedly merge low-weight components, as explained in
Section 8.3.4. In this way the new mixture coeﬃcients q(it+1|st+1, v1:t+1), it+1 ∈1, . . . , I,
are deﬁned. This completes the description of how to form a recursion for the continuous
ﬁltered posterior approximation q(ht+1|st+1, v1:t+1) in Eq. (8.5).
8.3.2
Discrete ﬁltering
A recursion for the switch variable distribution in Eq. (8.5) is given by
q(st+1|v1:t+1) ∝
X
it,st
q(st+1, it, st, vt+1, v1:t).
The right-hand side of the above equation is proportional to
X
st,it
q(vt+1|st+1, it, st, v1:t)q(st+1|it, st, v1:t)q(it|st, v1:t)q(st|v1:t),
for which all terms have been computed during the recursion for q(ht+1|st+1, v1:t+1). We now
have all the quantities required to compute the Gaussian sum approximation of the ﬁltering

Inference in switching linear systems using mixtures
171
Algorithm 8.1 aSLDS forward pass. Approximate the ﬁltered posterior p(st|v1:t) ≡
αt, p(ht|st, v1:t)
≡
P
it wt(it, st)N (ht ft(it, st), Ft(it, st)). Also return the approximate
log-likelihood L
≡
log p(v1:T). It are the number of components in each Gaus-
sian mixture approximation. We require I1
=
1, I2
≤
S, It
≤
S × It−1. θ(s)
=
A(s), B(s), Σh(s), Σv(s), ¯h(s), ¯v(s). See also Algorithm 1.1.
for s1 ←1 to S do
{f1(1, s1), F1(1, s1), ˆp} = LDSFORWARD(0, 0, v1; θ(s1))
α1 ←p(s1) ˆp
end for
for t ←2 to T do
for st ←1 to S do
for i ←1 to It−1, and s ←1 to S do
{µx|y(i, s), Σx|y(i, s), ˆp} = LDSFORWARD(ft−1(i, s), Ft−1(i, s), vt; θ(st))
p∗(st|i, s) ≡⟨p(st|ht−1, st−1 = s)⟩p(ht−1|it−1=i,st−1=s,v1:t−1)
p′(st, i, s) ←wt−1(i, s)p∗(st|i, s)αt−1(s) ˆp
end for
Collapse the It−1 × S mixture of Gaussians deﬁned by µx|y,Σx|y, and weights
p(i, s|st)
∝
p′(st, i, s) to a Gaussian with It components, p(ht|st, v1:t)
≈
PIt
it=1 p(it|st, v1:t)p(ht|st, it, v1:t). This deﬁnes the new means ft(it, st), covariances
Ft(it, st) and mixture weights wt(it, st) ≡p(it|st, v1:t).
Compute αt(st) ∝P
i,s p′(st, i, s)
end for
normalise αt
L ←L + log P
st,i,s p′(st, i, s)
end for
forward pass. A schematic representation of Gaussian sum ﬁltering is given in Fig. 8.2 and
the pseudo-code is presented in Algorithm 8.1.
8.3.3
The likelihood p(v1:T)
The likelihood p(v1:T) may be found from
p(v1:T) =
T−1
Y
t=0
p(vt+1|v1:t),
p(vt+1|v1:t) ≈
X
it,st,st+1
q(vt+1|it, st, st+1, v1:t)q(st+1|it, st, v1:t)q(it|st, v1:t)q(st|v1:t).
In the above expression, all terms have been computed when forming the recursion for the
ﬁltered posterior q(ht+1, st+1|v1:t+1).
8.3.4
Collapsing Gaussians
We wish to collapse a mixture of N Gaussians
p(x) =
N
X
i=1
piN  x µi, Σi

(8.9)

172
David Barber
to a mixture of K < N Gaussians. We present a simple method which has the advantage of
computational eﬃciency, but the disadvantage that no spatial information about the mixture
is used [20]. First we describe how to collapse a mixture to a single Gaussian N (x µ, Σ).
This can be achieved by setting µ and Σ to be the mean and covariance of the mixture
distribution (8.9), that is
µ =
X
i
piµi,
Σ =
X
i
pi

Σi + µiµT
i

−µµT.
To collapse to a K-component mixture we may retain the K −1 Gaussians with the largest
mixture weights and merge the remaining N−K−1 Gaussians to a single Gaussian using the
above method. Alternative heuristics such as recursively merging the two Gaussians with
the lowest mixture weights are also reasonable. More sophisticated methods which retain
some spatial information would clearly be potentially useful. The method presented in [15]
is a suitable approach which considers removing Gaussians which are spatially similar,
thereby retaining a sense of diversity over the possible solutions.
8.3.5
Relation to other methods
Gaussian sum ﬁltering can be considered a form of ‘analytical particle ﬁltering’ in which
instead of point distributions (delta functions) being propagated, Gaussians are propagated.
The collapse operation to a smaller number of Gaussians is analogous to resampling in
particle ﬁltering. Since a Gaussian is more expressive than a delta function, the Gaussian
sum ﬁlter is generally an improved approximation technique over using point particles.
See [3] for a numerical comparison. This Gaussian sum ﬁltering method is an instance of
assumed density ﬁltering, see Section 1.5.2.
8.4
Expectation correction
Approximating the smoothed posterior p(ht, st|v1:T) is more involved than ﬁltering and
requires additional approximations. For this reason smoothing is more prone to failure since
there are more assumptions that need to be satisﬁed for the approximations to hold. The
route we take here is to assume that a Gaussian sum ﬁltered approximation has been carried
out, and then approximate the γ backward pass, analogous to that of Section 1.4.3. The
exact backward pass for the SLDS reads
p(ht, st|v1:T) =
X
st+1
Z
ht+1
p(ht, st|ht+1, st+1, v1:t)p(ht+1, st+1|v1:T),
(8.10)
where p(st+1|v1:T) and p(ht+1|st+1, v1:T) are the discrete and continuous components of the
smoothed posterior at the next time step. The recursion runs backwards in time, beginning
with the initialisation p(hT, sT|v1:T) set by the ﬁltered result (at time t = T, the ﬁltered and
smoothed posteriors coincide). Apart from the fact that the number of mixture components
will increase at each step, computing the integral over ht+1 in Eq. (8.10) is problematic
since the conditional distribution term is non-Gaussian in ht+1. For this reason it is more
useful in deriving an approximate recursion to begin with the exact relation
p(st, ht|v1:T) =
X
st+1
p(st+1|v1:T)p(ht|st, st+1, v1:T)p(st|st+1, v1:T),

Inference in switching linear systems using mixtures
173
st−1
ht−1
vt−1
st
ht
vt
st+1
ht+1
vt+1
st+2
ht+2
vt+2
Figure
8.3 The
EC
backpass
approximates
p(ht+1|st+1, st, v1:T )
by
p(ht+1|st+1, v1:T ).
The
moti-
vation for this is that st inﬂuences ht+1 only indirectly
through ht. However, ht will most likely be heavily
inﬂuenced by v1:t, so that not knowing the state of st is
likely to be of secondary importance. The light shaded
node is the variable we wish to ﬁnd the posterior for.
The values of the dark shaded nodes are known, and the
dashed node indicates a known variable which is assumed
unknown in forming the approximation.
which can be expressed more directly in terms of the SLDS dynamics as
p(st, ht|v1:T) =
X
st+1
p(st+1|v1:T) ⟨p(ht|ht+1, st, st+1, v1:t,
vt+1:T)⟩p(ht+1|st,st+1,v1:T )
× ⟨p(st|ht+1, st+1, v1:T)⟩p(ht+1|st+1,v1:T ) .
In forming the recursion we assume access to the distribution p(st+1, ht+1|v1:T) from the
future time step. However, we also require the distribution p(ht+1|st, st+1, v1:T) which is
not directly known and needs to be inferred – a computationally challenging task. In the
expectation correction (EC) approach [3] one assumes the approximation (see Fig. 8.3)
p(ht+1|st, st+1, v1:T) ≈p(ht+1|st+1, v1:T),
(8.11)
resulting in an approximate recursion for the smoothed posterior
p(st, ht|v1:T) ≈
X
st+1
p(st+1|v1:T) ⟨p(ht|ht+1, st, st+1, v1:t)⟩ht+1 ⟨p(st|ht+1, st+1, v1:T)⟩ht+1 ,
where ⟨·⟩ht+1 represents averaging with respect to the distribution p(ht+1|st+1, v1:T). In car-
rying out the above approximate recursion, we will end up with a mixture of Gaussians
that grows at each time step. To avoid the exponential explosion problem, we use a ﬁnite
mixture approximation
p(ht+1, st+1|v1:T) ≈q(ht+1, st+1|v1:T) = q(ht+1|st+1, v1:T)q(st+1|v1:T),
and plug this into the recursion above. A recursion for the approximation is given by
q(ht, st|v1:T) =
X
st+1
q(st+1|v1:T) ⟨q(ht|ht+1, st, st+1, v1:t)⟩q(ht+1|st+1,v1:T )
× ⟨q(st|ht+1, st+1, v1:t)⟩q(ht+1|st+1,v1:T ) .
(8.12)
As for ﬁltering, wherever possible, we replace approximate terms by their exact counter-
parts and parameterise the posterior using
q(ht+1, st+1|v1:T) = q(ht+1|st+1, v1:T)q(st+1|v1:T).
(8.13)
To reduce the notational burden here we outline the method only for the case of using a
single component approximation in both the forward and backward passes. The extension
to using a mixture to approximate each p(ht+1|st+1, v1:T) is conceptually straightforward
and deferred to Section 8.4.5. In the single Gaussian case we assume we have a Gaussian
approximation available for
q(ht+1|st+1, v1:T) = N (ht+1 g(st+1), G(st+1)) ,
which below is ‘propagated’ backwards in time using the smoothing recursion.

174
David Barber
8.4.1
Continuous smoothing
For given st, st+1, a recursion for the smoothed continuous variable is obtained using
q(ht|st, st+1, v1:T) =
Z
ht+1
p(ht|ht+1, st, st+1, v1:t)q(ht+1|st+1, v1:T).
(8.14)
In forming the recursion, we assume that we know the distribution
q(ht+1|st+1, v1:T) = N (ht+1 g(st+1), G(st+1)) .
To compute Eq. (8.14) we then perform a single update of the LDS backward recursion,
Algorithm 1.2.
8.4.2
Discrete smoothing
The second average in Eq. (8.12) corresponds to a recursion for the discrete variable and is
given by
⟨q(st|ht+1, st+1, v1:t)⟩q(ht+1|st+1,v1:T ) ≡q(st|st+1, v1:T).
This average cannot be computed in closed form. The simplest approach is to approximate
it by evaluation at the mean, that is1
⟨q(st|ht+1, st+1v1:t)⟩q(ht+1|st+1,v1:T ) ≈q(st|ht+1, st+1, v1:t)
ht+1=⟨ht+1|st+1,v1:T ⟩,
where ⟨ht+1|st+1, v1:T⟩is the mean of ht+1 with respect to q(ht+1|st+1, v1:T). This gives the
approximation
⟨q(st|ht+1, st+1, v1:t)⟩q(ht+1|st+1,v1:T ) ≈1
Z
e−1
2 zT
t+1(st,st+1)Σ
−1(st,st+1|v1:t)zt+1(st,st+1)
√det (Σ(st, st+1|v1:t))
q(st|st+1, v1:t),
where
zt+1(st, st+1) ≡⟨ht+1|st+1, v1:T⟩−⟨ht+1|st, st+1, v1:t⟩,
and Z ensures normalisation over st; Σ(st, st+1|v1:t) is the ﬁltered covariance of ht+1 given
st, st+1 and the observations v1:t, which may be taken from Σhh in Eq. (8.7). Approximations
which take covariance information into account can also be considered, although the above
method may suﬃce in practice [3, 17].
8.4.3
Collapsing the mixture
From Sections 8.4.1 and 8.4.2 we now have all the terms to compute the approximation to
Eq. (8.12). As for the ﬁltering, however, the number of mixture components is multiplied
by S at each iteration. To prevent an exponential explosion of components, the mixture is
then collapsed to a single Gaussian
q(ht, st|v1:T) = q(ht|st, v1:T)q(st|v1:T).
The collapse to a mixture is discussed in Section 8.4.5.
1In general this approximation has the form ⟨f(x)⟩p(x) ≈f(⟨x⟩p(x)).

Inference in switching linear systems using mixtures
175
8.4.4
Relation to other methods
A classical smoothing approximation for the SLDS is generalised pseudo Bayes (GPB) [2,
12, 11]. GPB makes the following approximation
p(st|st+1, v1:T) ≈p(st|st+1, v1:t),
which depends only on the ﬁltered posterior for st and does not include any information
passing through the variable ht+1. Since
p(st|st+1, v1:t) ∝p(st+1|st)p(st|v1:t),
computing the smoothed recursion for the switch states in GPB is equivalent to running the
RTS backward pass on a hidden Markov model, independently of the backward recursion
for the continuous variables:
p(st|v1:T) =
X
st+1
p(st, st+1|v1:T) =
X
st+1
p(st|st+1, v1:T)p(st+1|v1:T)
≈
X
st+1
p(st|st+1, v1:t)p(st+1|v1:T)
=
X
st+1
p(st+1|st)p(st|v1:t)
P
st p(st+1|st)p(st|v1:t) p(st+1|v1:T).
The only information the GPB method uses to form the smoothed distribution p(st|v1:T)
from the ﬁltered distribution p(st|v1:t) is the Markov switch transition p(st+1|st). This
approximation discounts some information from the future since information passed via
the continuous variables is not taken into account. In contrast to GPB, EC preserves future
information passing through the continuous variables.
8.4.5
Using mixtures in the expectation correction backward pass
The extension to the mixture case is straightforward, based on the representation
p(ht|st, v1:T) ≈
J
X
jt=1
q( jt|st, v1:T)q(ht| jt, st, v1:T).
Analogously to the case with a single component,
q(ht, st|v1:T) =
X
it, jt+1,st+1
p(st+1|v1:T)p( jt+1|st+1, v1:T)q(ht|jt+1, st+1, it, st, v1:T)
× ⟨q(it, st|ht+1, jt+1, st+1, v1:t)⟩q(ht+1| jt+1,st+1,v1:T ) .
The average in the last line of the above equation can be tackled using the same tech-
niques as outlined in the single Gaussian case. To approximate q(ht| jt+1, st+1, it, st, v1:T) we
consider this as the marginal of the joint distribution
q(ht, ht+1|it, st, jt+1, st+1, v1:T) = q(ht|ht+1, it, st, jt+1, st+1, v1:t)q(ht+1|it, st, jt+1, st+1, v1:T).
As in the case of a single mixture, the problematic term is q(ht+1|it, st, jt+1, st+1, v1:T).
Analogously to Eq. (8.11), we make the assumption
q(ht+1|it, st, jt+1, st+1, v1:T) ≈q(ht+1| jt+1, st+1, v1:T),

176
David Barber
Algorithm 8.2 aSLDS: EC backward pass. Approximates p(st|v1:T) and p(ht|st, v1:T) ≡
PJt
jt=1 ut( jt, st)N(gt( jt, st), Gt( jt, st)) using a mixture of Gaussians. JT = IT, Jt ≤S ×It×Jt+1.
This routine needs the results from Algorithm 8.1.
GT ←FT, gT ←fT, uT ←wT
for t ←T −1 to 1 do
for s ←1 to S , s′ ←1 to S , i ←1 to It, j′ ←1 to Jt+1 do
(µ, Σ)(i, s, j′, s′) = LDSBACKWARD(gt+1( j′, s′), Gt+1( j′, s′), ft(i, s), Ft(i, s), θ(s′))
p(it, st| jt+1, st+1, v1:T)
= 
p(st = s, it = i|ht+1, st+1 = s′, jt+1 = j′, v1:t)
p(ht+1|st+1=s′, jt+1=j′,v1:T )
p(i, s, j′, s′|v1:T) ←p(st+1 = s′|v1:T)ut+1( j′, s′)p(it, st| jt+1, st+1, v1:T)
end for
for st ←1 to S do
Collapse the mixture deﬁned by weights p(it = i, st+1 = s′, jt+1 = j′|st, v1T ) ∝
p(i, st, j′, s′|v1:T), means µ(it, st, jt+1, st+1) and covariances Σ(it, st, jt+1, st+1) to a
mixture with Jt components. This deﬁnes the new means gt(jt, st), covariances
Gt( jt, st) and mixture weights ut( jt, st).
p(st|v1:T) ←P
it, j′,s′ p(it, st, j′, s′|v1:T)
end for
end for
meaning that information about the current switch state st, it is ignored. We can then form
p(ht|st, v1:T) =
X
it, jt+1,st+1
p(it, jt+1, st+1|st, v1:T)p(ht|it, st, jt+1, st+1, v1:T).
This mixture can then be collapsed to a smaller mixture to give
p(ht|st, v1:T) ≈
X
jt
q( jt|st, v1:T)q(ht| jt, v1:T).
The resulting procedure is sketched in Algorithm 8.2, including using mixtures in both
forward and backward passes.
8.5
Demonstration: traﬃc ﬂow
An illustration of modelling and inference with an SLDS is given in the network of traf-
ﬁc ﬂow problem, Fig. 8.4. Here there are four junctions a, b, c, d, and traﬃc ﬂows along
theroads in the direction indicated. Traﬃc ﬂows into the junction and then goes via diﬀer-
ent routes to d. Flow out of a junction must match the ﬂow in to a junction (up to noise).
There are traﬃc light switches at junctions a and b which, depending on their state, route
traﬃc diﬀerently along the roads. Using φ to denote ﬂow, we model the ﬂows using the
switching linear system
φa(t)
φa→d(t)
φa→b(t)
φb→d(t)
φb→c(t)
φc→d(t)

=

φa(t −1)
φa(t −1) (0.75 × I [sa(t) = 1] + 1 × I [sa(t) = 2])
φa(t −1) (0.25 × I [sa(t) = 1] + 1 × I [sa(t) = 3])
φa→b(t −1)0.5 × I [sb(t) = 1]
φa→b(t −1) (0.5 × I [sb(t) = 1] + 1 × I [sb(t) = 2])
φb→c(t −1)

Inference in switching linear systems using mixtures
177
a
b
c
d
Figure 8.4 A representation of the traﬃc ﬂow between junctions at a, b, c, d, with
traﬃc lights at a and b. If sa = 1 a →d and a →b carry 0.75 and 0.25 of the ﬂow out
of a respectively. If sa = 2 all the ﬂow from a goes through a →d; for sa = 3, all the
ﬂow goes through a →b. For sb = 1 the ﬂow out of b is split equally between b →d
and b →c. For sb = 2 all ﬂow out of b goes along b →c.
0
20
40
60
80
100
0
20
40
0
20
40
60
80
100
0
20
40
Figure 8.5 Time evolution of the traﬃc ﬂow measured at
two points in the network. Sensors measure the total ﬂow
into the network φa(t) and the total ﬂow out of the network,
φd(t) = φa→d(t) + φb→d(t) + φc→d(t). The total inﬂow at a
undergoes a random walk. Note that the ﬂow measured at d
can momentarily drop to zero if all traﬃc is routed through
a →b →c for two time steps.
By identifying the ﬂows at time t with a six-dimensional vector hidden variable ht, we can
write the above ﬂow equations as
ht = A(st)ht−1 + ηh
t
for a set of suitably deﬁned matrices A(s) indexed by the switch variable s = sa
N
sb,
which takes 3 × 2 = 6 states. We additionally include noise terms to model cars parking or
de-parking during a single time frame. The covariance Σh is diagonal with a larger variance
at the inﬂow point a to model that the total volume of traﬃc entering the system can vary.
Noisy measurements of the ﬂow into the network are taken at a
v1,t = φa(t) + ηv
1(t),
along with a noisy measurement of the total ﬂow out of the system at d
v2,t = φd(t) = φa→d(t) + φb→d(t) + φc→d(t) + ηv
2(t).
The observation model can be represented by vt = Bht+ηv
t using a constant 2×6 projection
matrix B. This is clearly a very crude model of traﬃc ﬂows since in a real system one cannot
have negative ﬂows. Nevertheless it serves to demonstrate the principles of modelling and
inference using switching models. The switch variables follow a simple Markov transition
p(st|st−1) which biases the switches to remain in the same state in preference to jumping
to another state. Given the above system and a prior which initialises all ﬂow at a, we
draw samples from the model using forward (ancestral) sampling and form the observations
v1:100, Fig. 8.5. Using only the observations and the known model structure we then attempt
to infer the latent switch variables and traﬃc ﬂows using Gaussian sum ﬁltering with I = 2
and EC with J = 1 mixture components per switch state, Fig. 8.6.
8.6
Comparison of smoothing techniques
In order to demonstrate the potential advantages of Gaussian sum approximate inference
we chose a problem that, from the viewpoint of classical signal processing, is diﬃcult, with
changes in the switches not identiﬁable by simply eyeballing a short-time Fourier trans-
form of the signal. The setup, as described in Fig. 8.7, is such that the latent trajectory h

178
David Barber
Figure 8.6 Given the observations from Fig. 8.5 we infer the ﬂows and switch states of all the latent variables. (a)
The correct latent ﬂows through time along with the switch variable state used to generate the data. (b) Filtered
ﬂows based on a I = 2 Gaussian sum forward pass approximation. Plotted are the six components of the vector
⟨ht|v1:t⟩with the posterior distribution of the sa and sb traﬃc light states p(sa
t |v1:t),p(sb
t |v1:t) plotted below. (c)
Smoothed ﬂows ⟨ht|v1:T ⟩and corresponding smoothed switch states p(st|v1:T ) using EC with J = 1.
Figure 8.7 A sample from the inference problem
for comparison of smoothing techniques. This scalar
observation data (V = 1) of T = 100 time steps
is generated from an LDS with two switch states,
S = 2: A(s) = 0.9999 ∗orth(randn(H, H)), B(s) =
randn(V, H), ¯vt ≡0, ¯h1 = 10 ∗randn(H, 1), ¯ht>1 = 0,
Σh
1 = IH, p1 = uniform. The output bias is zero. The
hidden space has dimension H = 30, with low tran-
sition noise, Σh(s) = 0.01IH, p(st+1|st) ∝1S ×S . The
output noise is relatively high, Σv(s) = 30IV.
(H = 30) is near deterministic; however, the noise in the observations (V = 1) produces
high uncertainty as to where this trajectory is, needing to track multiple hypotheses until
there is suﬃcient data to reliably reveal which of the likely hypotheses is correct. For this
reason, the ﬁltered distribution is typically strongly multimodal and methods that do not
address this perform poorly. The transition and emission matrices were sampled at random
(see Fig. 8.7 for details); each random sampling of parameters generated a problem instance
on which the rival algorithms were evaluated. We sequentially generated hidden states ht, st
and observations vt from the known SLDS and then, given only the model and the observa-
tions, the task was to infer p(ht, st | v1:T). Since the exact computation is exponential in T,
a formal evaluation of the method is infeasible. A simple alternative is to assume that the
original sample states s1:T are the ‘correct’ inferred states, and compare the most proba-
ble posterior smoothed estimates arg maxst p(st|v1:T) with the assumed correct sample st.
Performance over a set of problem instances can then be assesed. All deterministic algo-
rithms were initialised using the corresponding ﬁltered results, as was Gibbs sampling. The
running time of all algorithms was set to be roughly equal to that of EC using I = J = 4
mixture components per switch state.
8.6.1
Filtering as approximate smoothing
In Fig. 8.8 we present the accuracy of techniques that use ﬁltering p(st|v1:t) as an approx-
imation of the smoothed posterior p(st|v1:T). Many of the smoothing approaches we

Inference in switching linear systems using mixtures
179
0
25 50 75
0
200
400
600
800
1000
PF
0
25 50 75
RBPF
0
25 50 75
EP
0
25 50 75
GSFS
0
25 50 75
KimS
0
25 50 75
ECS
0
25 50 75
GSFM
0
25 50 75
KimM
0
25 50 75
ECM
0
25 50 75
Gibbs
Figure 8.8 The number of errors in estimating a binary switch p(st|v1:T ) over a time series of length T = 100, for
the problem described in Fig. 8.7. Hence 50 errors corresponds to random guessing. Plotted are histograms of the
errors over 1000 experiments. (PF) Particle ﬁlter. (RBPF) Rao–Blackwellised PF. (EP) Expectation propagation.
(GSFS) Gaussian sum ﬁltering using a single Gaussian. (KimS) Kim’s smoother, i.e. GPB, using the results from
GSFS. (ECS) Expectation correction using a single Gaussian (I = J = 1). (GSFM) GSF using a mixture of I = 4
Gaussians. (KimM) Kim’s smoother using the results from GSFM. (ECM) Expectation correction using a mixture
with I = J = 4 components. In Gibbs sampling, we use the initialisation from GSFM.
examined are initialised with ﬁltered approximations and it is therefore interesting to see
the potential improvement that each smoothing algorithm makes over the ﬁltered estimates.
In each case the histogram of errors in estimating the switch variable st for t from 1 to 50
is presented, based on a set of 1000 realisations of the observed sequence.
Particle ﬁlter
We included the particle ﬁlter (PF) for comparison with Gaussian sum ﬁl-
tering (GSF) and to demonstrate the baseline performance of using ﬁltering to approximate
the smoothed posterior. For the PF, 1000 particles were used, with Kitagawa resampling,
[14]. In this case the PF performs poorly due to the diﬃculty of representing continu-
ous high-dimensional posterior distributions (H = 30) using a limited number of point
particles and the underlying use of importance sampling in generating the samples. The
Rao–Blackwellised particle ﬁlter (RBPF) [7] attempts to alleviate the diﬃculty of sampling
in high-dimensional spaces by approximate integration over the continuous hidden vari-
able. For the RBPF, 500 particles were used, with Kitagawa resampling. We were unable
to improve on the standard PF results using this technique, most likely due to the fact that
the available RBPF implementation assumed a single Gaussian per switch state.
Gaussian sum ﬁltering
Whilst our interest is primarily in smoothing, for EC and GPB
we need to initialise the smoothing algorithm with ﬁltered estimates. Using a single Gaus-
sian (GSFS) gives reasonable results, though using a mixture with I = 4 components per
switch state (GSFM) improves on the single component case considerably, indicating the
multimodal nature of the ﬁltered posterior. Not surprisingly, the best ﬁltered results are
given using GSF, since this is better able to represent the variance in the ﬁltered posterior
than the PF sampling methods.
8.6.2
Approximate smoothing
Generalised pseudo Bayes
For this problem, Kim’s GPB method does not improve on
the Gaussian sum ﬁltered results, using either a mixture I = 4 (KimM) or a single compo-
nent I = 1 (KimS). In this case the GPB approximation of ignoring all future observations,
provided only the future switch state st+1 is known, is too severe.

180
David Barber
Expectation correction
For EC we use the mean approximation for the numerical inte-
gration of Eq. (8.13). In ECS a single Gaussian per switch state is used I = J = 1, with the
backward recursion initialised with the ﬁltered posterior. The method improves on GSFS,
though the performance is still limited due to only using a single component. When using
multiple components, I = J = 4, ECM has excellent performance, with a near perfect esti-
mation of the most likely smoothed posterior class. One should bear in mind that EC, GPB,
Gibbs and EP are initialised with the same GSF ﬁltered distributions.
Gibbs sampling
Gibbs sampling provides an alternative non-deterministic procedure [4].
For a ﬁxed state sequence s1:T, p(v1:T | s1:T) is easily computable since this is just the like-
lihood of an LDS with transitions and emissions speciﬁed by the assumed given switch
states. We therefore attempt to sample from the posterior p(s1:T | v1:T) ∝p(v1:T | s1:T) p(s1:T)
directly. We implemented a standard single-site Gibbs sampler in which all switch vari-
ables are ﬁxed, except st, and then a sample from p(st|s\t, v1:T) drawn, with the 100 update
sweeps forwards from the start to the end time and then in the reverse direction. The aver-
age of the ﬁnal 80 forward-backward sweeps was used to estimate the smoothed posterior.
Such a procedure may work well provided that the initial setting of s1:T is in a region of high
probability – otherwise sampling by such individual coordinate updates may be extremely
ineﬃcient. For this case, even if the Gibbs sampler is initialised in a reasonable way (using
the results from GSFM) the sampler is unable to deal with the strong temporal correlations
in the posterior and on average degrades the ﬁltering results.
Expectation propagation
Expectation propagation (EP) [18] has had considerable suc-
cess in approximate inference in graphical models, and has been ported to approximate
inference in the SLDS (see Chapter 7). For our particular problem, we found EP to be
numerically unstable, often struggling to converge.2 To encourage convergence, we used
the damping method in [10], performing 20 iterations with a damping factor of 0.5. Nev-
ertheless, the disappointing performance of EP is most likely due to conﬂicts resulting
from numerical instabilities introduced by the frequent conversions between moment and
canonical representations. In addition, the current implementation of EP assumes that
the posterior is well represented by a single Gaussian per switch state, and the inherent
multimodality of the posterior in this experiment may render this assumption invalid.
8.7
Summary
Exact inference in the class of switching linear dynamical systems is computationally
diﬃcult. Whilst deriving exact ﬁltering and smoothing recursions is straightforward, repre-
senting the ﬁltered and smoothed posterior for time step t requires a mixture of Gaussians
with a number of components scaling exponentially with t. This suggests the Gaussian sum
class of approximations in which a limited number of Gaussian components is retained. We
discussed how both ﬁltering and smoothing recursions can be derived for approximations
in which multiple Gaussians are assumed per switch state, extending on previous approx-
imations which assume a single Gaussian per switch state. In extreme cases we showed
how using such mixture approximations of the switch conditional posterior can improve
the accuracy of the approximation considerably. Our Gaussian sum smoothing approach,
expectation correction, can be viewed as a form of analytic particle smoothing. Rather than
2Generalised EP [21], which groups variables together improves on the results, but is still far inferior to the
EC results presented here – Onno Zoeter personal communication.

Inference in switching linear systems using mixtures
181
propagating point distributions (delta-functions), as in the particle approaches, EC prop-
agates Gaussians, which are more able to represent the variability of the distribution,
particularly in high dimensions. An important consideration in time series models is numer-
ical stability. Expectation correction uses standard forward and backward linear dynamical
system message updates and is therefore able to take advantage of well-studied methods
for numerically stable inference. Our smoothing technique has been successfully applied
to time series of length O

105
without numerical diﬃculty [17]. Code for implementing
EC is available from the author’s website and is part of the BRMLtoolbox.
Bibliography
[1] D. L. Alspach and H. W. Sorenson. Nonlinear
Bayesian estimation using Gaussian sum
approximations. IEEE Transactions on
Automatic Control, 17(4):439–448, 1972.
[2] Y. Bar-Shalom and Xiao-Rong Li. Estimation
and Tracking : Principles, Techniques and
Software. Artech House, 1998.
[3] D. Barber. Expectation correction for smoothing
in switching linear Gaussian state space models.
Journal of Machine Learning Research,
7:2515–2540, 2006.
[4] C. Carter and R. Kohn. Markov chain Monte
Carlo in conditionally Gaussian state space
models. Biometrika, 83:589–601, 1996.
[5] A. T. Cemgil, B. Kappen and D. Barber. A
generative model for music transcription. IEEE
Transactions on Audio, Speech and Language
Processing, 14(2):679 – 694, 2006.
[6] S. Chib and M. Dueker. Non-Markovian regime
switching with endogenous states and
time-varying state strengths. Econometric
Society 2004 North American Summer Meetings
600, Econometric Society, August 2004.
[7] A. Doucet, N. de Freitas, K. Murphy and
S. Russell. Rao-Blackwellised particle ﬁltering
for dynamic Bayesian networks. Uncertainty in
Artiﬁcial Intelligence, 2000.
[8] S. Fr¨uhwirth-Schnatter. Finite Mixture and
Markov Switching Models. Springer, 2006.
[9] Z. Ghahramani and G. E. Hinton. Variational
learning for switching state-space models.
Neural Computation, 12(4):963–996, 1998.
[10] T. Heskes and O. Zoeter. Expectation
propagation for approximate inference in
dynamic Bayesian networks. In A. Darwiche and
N. Friedman, editors, Uncertainty in Artiﬁcial
Intelligence, pages 216–223, 2002.
[11] C.-J. Kim. Dynamic linear models with
Markov-switching. Journal of Econometrics,
60:1–22, 1994.
[12] C.-J. Kim and C. R. Nelson. State-Space Models
with Regime Switching. MIT Press, 1999.
[13] G. Kitagawa. The two-ﬁlter formula for
smoothing and an implementation of the
Gaussian-sum smoother. Annals of the Institute
of Statistical Mathematics, 46(4):605–623, 1994.
[14] G. Kitagawa. Monte Carlo ﬁlter and smoother
for non-Gaussian nonlinear state space models.
Journal of Computational and Graphical
Statistics, 5(1):1–25, 1996.
[15] U. Lerner, R. Parr, D. Koller and G. Biswas.
Bayesian fault detection and diagnosis in
dynamic systems. In Proceedings of the
Seventeenth National Conference on Artiﬁcial
Intelligence (AIII-00), pages 531–537, 2000.
[16] U. N. Lerner. Hybrid Bayesian networks for
reasoning about complex systems. PhD thesis,
Stanford University, 2002.
[17] B. Mesot and D. Barber. Switching linear
dynamical systems for noise robust speech
recognition. IEEE Transactions of Audio, Speech
and Language Processing, 15(6):1850–1858,
2007.
[18] T. Minka. A family of algorithms for
approximate Bayesian inference. PhD thesis,
MIT Media Lab, 2001.
[19] V. Pavlovic, J. M. Rehg and J. MacCormick.
Learning switching linear models of human
motion. In Advances in Neural Information
Processing Systems (NIPS), number 13, pages
981–987, 2001. MIT Press.
[20] D. M. Titterington, A. F. M. Smith and U. E.
Makov. Statistical Analysis of Finite Mixture
Distributions. John Wiley & Sons, 1985.
[21] O. Zoeter. Monitoring non-linear and switching
dynamical systems. PhD thesis, Radboud
University Nijmegen, 2005.
Contributor
David Barber, Department of Computer Science, University College London

9
Physiological monitoring with factorial switching
linear dynamical systems
John A. Quinn and Christopher K. I. Williams
9.1
Introduction
A common way to handle non-linearity in complex time series data is to try splitting the
data up into a number of simpler segments. Sometimes we have domain knowledge to
support this piecewise modelling approach, for example in condition monitoring applica-
tions. In such problems, the evolution of some observed data is governed by a number of
hidden factors that switch between diﬀerent modes of operation. In real-world data, e.g.
from medicine, robotic control or ﬁnance, we might be interested in factors which rep-
resent pathologies, mechanical failure modes, or economic conditions respectively. Given
just the monitoring data, we are interested in recovering the state of the factors that gave rise
to it.
A good model for this type of problem is the switching linear dynamical system
(SLDS), which has been discussed in previous chapters. A latent ‘switch’ variable in this
type of model selects between diﬀerent linear-Gaussian state spaces. In this chapter we
consider a generalisation, the factorial switching linear dynamical system (FSLDS), where
instead of a single switch setting there are multiple discrete factors that collectively deter-
mine the dynamics. In practice there may be a very large number of possible factors, and
we may only have explicit knowledge of commonly occurring ones.
We illustrate how the FSLDS can be used in the physiological monitoring of premature
babies in intensive care. This application is a useful introduction because it has complex
observed data, a diverse range of factors aﬀecting the observations, and the challenge of
many ‘unknown’ factors. It also provides an opportunity to demonstrate the ways in which
domain knowledge can be incorporated into the FSLDS model. Many of the speciﬁc mod-
elling details here are also directly applicable to physiological monitoring of adults, in
intensive care and other settings.
Observations and factors
Babies born three or four months prematurely are kept, in their ﬁrst days or weeks post par-
tum, in a closely regulated environment, with a number of probes continuously collecting
physiological data such as heart rate, blood pressure, temperature and concentrations of
gases in the blood. These vital signs (literally ‘signs of life’) are used in neonatal inten-
sive care to help diagnose the condition of a baby in a critical state. The state of health

Physiological monitoring with switching linear systems
183
Channel name
Label
Core body temperature (◦C)
Core temp.
Diastolic blood pressure (mmHg)
Dia. Bp
Heart rate (bpm)
HR
Peripheral body temperature (◦C)
Periph. temp.
Saturation of oxygen in pulse (%)
SpO2
Systolic blood pressure (mmHg)
Sys. Bp
Transcutaneous partial pressure of CO2 (kPa)
TcPCO2
Transcutaneous partial pressure of O2 (kPa)
TcPO2
Incubator temperature (◦C)
Incu Temp.
Incubator humidity (%)
Incu Humidity.
Table 9.1 Physiological (upper) and environmental (lower) measurement channels in this application, with labels
used to denote them later in the chapter.
of a baby cannot be observed directly, but diﬀerent states of health are associated with
particular patterns of measurements. Given observations of the heart rate, body temperature
and so on, inferences can therefore be made about the operation of the underlying phys-
iological systems – e.g. whether they are functioning normally, or whether there seems
to be evidence of some pathology. This task is complicated by the fact that the obser-
vations depend not just on the state of a baby’s physiology but also on the operation of
the monitoring equipment. There is observation noise due to inaccuracies in the probes,
and some operations can cause the measurements to become corrupted with artefacts. The
speciﬁc data channels we consider here are listed in Table 9.1, each sampled once per
second.
Types of known, common factors which aﬀect these observations fall into two cate-
gories: artefactual and physiological. The known factors we concentrate on in this chapter
are
• Bradycardia : Temporary decrease in heart rate (physiological).
• Blood sample : Artefactual rise in systolic and diastolic blood pressure measurements
while a sample is taken (artefactual).
• Temperature probe disconnection : The core temperature probe cools to ambient
temperature (artefactual).
• Handling : Opening of the incubator, leading to a drop in incubator humidity with
increased physiological variation (artefactual).
In addition to these common factors there are many examples of physiological variation
due to rare factors, or for which no explanation is available.
Outline of the chapter
In Section 9.2 we introduce the model and compare it to other models for condition mon-
itoring. In Section 9.3 we explain how to handle novel dynamics, i.e. the presence of
unknown factors. In Section 9.4 we describe parameter estimation of the model, show-
ing how domain knowledge can be incorporated. Inference is discussed in Section 15.4 and
we demonstrate the operation of the system in Section 9.6. The description in this chapter

184
John A. Quinn and Christopher K. I. Williams
gives an overview of the application and model; for more speciﬁc implementation details
see [11, 9]. Demonstration code is also available [10].
9.2
Model
We ﬁrst recap on the SLDS before generalising to the factorial case. In such a model the
observations y1:T, yt ∈Rdy, are generated by hidden dynamics x1:T, xt ∈Rdx, according to
xt ∼N

A(st)xt−1 + d(st), Q(st)
,
yt ∼N

C(st)xt, R(st)
(9.1)
where st ∈{1, . . . , K} is a discrete variable deﬁning which of a set of K diﬀerent dynamics
is active at time t. Here A(st) is a square system matrix, d(st) is a drift vector, C(st) is the
state-observations matrix, and Q(st) and R(st) are noise covariance matrices. In this formula-
tion, all dynamical parameters can be switched between regimes. Similar models referred
to in the above literature sometimes switch only the state dynamics {A, Q}, or the obser-
vation dynamics {C, R}. Conditioned on a setting of st, the model is equivalent to a linear
dynamical system.
It is possible to factorise the switch variable, so that M factors f (1)
t
, . . . , f (M)
t
aﬀect the
observations yt. The factor f (m) can take on L(m) diﬀerent values. The state space is the cross
product of the factor variables, that is
st = f (1)
t
⊗f (2)
t
⊗· · · ⊗f (M)
t
with K = QM
m=1 L(m) being the number of settings that st can take on. The value of f (m)
t
depends on f (m)
t−1 , and the factors are a priori independent, so that
p(st | st−1) =
M
Y
m=1
p

f (m)
t
| f (m)
t−1

.
(9.2)
The joint distribution of the model is
p(s1:T, x1:T, y1:T) = p(s1)p(x1)p(y1 | x1, s1)
T
Y
t=2
p(st | st−1)p(xt | xt−1, st)p(yt | xt, st),
where s1:T denotes the sequence s1, s2, . . . , sT (similarly for x1:T and y1:T), p(xt | xt−1, st)
and p(yt | xt, st) are deﬁned in Eq. (9.1), and p(st | st−1) is deﬁned in Eq. (9.2).
By considering the factored nature of the switch setting, we have an observation term of
the form p(yt | xt, f (1)
t
, . . . , f (M)
t
). This can be parameterised in diﬀerent ways. In this work,
we specify conditional independencies between particular components of the observation
yt given the factor settings. This is explained further in Section 9.4.5. Although we make
use of prior factored dynamics in Eq. (9.2) in this work, it is very simple to generalise the
model so that this no longer holds. The inference algorithms described in Section 15.4 can
still be applied. However, the separate factors are crucial in structuring the system dynamics
and observations model.
In the physiological monitoring application, factor settings f (1)
t
, . . . , f (M)
t
represent dif-
ferent conditions (e.g. whether a probe has fallen oﬀor not, whether there is a speciﬁc
problem with the circulatory system or not). The state xt can contain estimates of the ‘true’
values of physiological properties, based on noisy, artefact-prone observations yt.

Physiological monitoring with switching linear systems
185
...
...
(a)
(b)
Figure 9.1 Belief network representations of diﬀerent factorial models, with M = 2 factors. Squares indicate
discrete variables, circles indicate continuous variables and shaded nodes indicate observed variables. (a) The
factorial HMM. (b) The factorial AR-HMM, in which each observation depends on previous values.
9.2.1
Comparison with other switching models for condition monitoring
We have assumed the existence of a discrete switch variable which indexes diﬀerent modes
of operation. In our formulation, the problem of condition monitoring is essentially to infer
the value of this switch variable over time from new data. We are particularly interested in
the class of models in which there are ﬁrst-order Markovian transitions between the switch
settings at consecutive time steps. Given the switch setting it is possible to characterise the
diﬀerent dynamic regimes on other ways, yielding alternative models for condition moni-
toring. In this section, we ﬁrst review the hidden Markov model (HMM) and autoregressive
hidden Markov model (AR-HMM), and then discuss their advantages and disadvantages for
condition monitoring with respect to the FSLDS.
A simple model for a single regime is the Gaussian distribution on yt. When this is
conditioned on a discrete, ﬁrst-order Markovian switching variable, we obtain an instance
of an HMM. This model can therefore be used for condition monitoring when the levels and
variability of diﬀerent measurement channels are signiﬁcant (though note that in general
the HMM can use any reasonable distribution on yt).
Autoregressive (AR) models are a common choice for modelling stationary time series.
Conditioning an AR model on a Markovian switching variable we obtain an autoregressive
hidden Markov model (AR-HMM), also known as a switching AR model – see e.g. [14].
This provides a model for conditions in which observations might be expected to oscillate
or decay, for example. During inference, the model can only conﬁdently switch into a
regime if the last p observations have been generated under that regime; there will be a
loss of accuracy if any of the measurement channels have dropped out in that period, for
example, or another artefactual process has aﬀected any of the readings.
The general condition monitoring problem involves independent factors which aﬀect a
system. In both of these models the switch variable can be factorised, giving the factorial
HMM [5] and the factorial AR-HMM respectively. The belief network representations for
these two constructions are shown in Fig. 9.1.
By characterising each regime as a linear dynamical system (LDS) we obtain the
FSLDS. The SLDS can be thought of as a ‘hybrid’ model, having both discrete switch
settings as in the HMM and continuous hidden state as in a linear dynamical system.
The FSLDS is similar, though with the discrete switch setting structure of the factorial
HMM. Note, however, that observations in the FHMM [5] are generated through an addi-
tive process in which each factor makes a contribution. The mechanisms used to generate

186
John A. Quinn and Christopher K. I. Williams
Artefactual state
True state
Observations
Factor 2 (physiological)
Factor 1 (artefactual)
Figure 9.2 Factorial switching linear dynamical system for physiological condition monitoring, with M = 2
factors as an example. The state is split up into two sets of variables, containing estimates of the ‘true’ physiology
and of the levels of artefactual processes.
observations under diﬀerent factor settings can in general be more complex and non-linear
than this, as in the overwriting mechanism explained in Section 9.4.5.
The FSLDS models have a number of representational advantages for condition mon-
itoring. First, we can have many dimensions of hidden state for each observed dimension.
This allows us to deal with situations in which diﬀerent elements aﬀect the observations,
as we will demonstrate in the examples later in this chapter.
In the physiological monitoring case we can construct detailed representations of the
causes underlying the observations. For instance, as we demonstrate in Sections 9.4.1
and 9.4.2, the state can be split into two groups of continuous latent variables repre-
senting respectively the ‘true’ physiology and the levels associated with diﬀerent artefac-
tual processes. In this application, factors can be physiological or artefactual processes.
Physiological factors can aﬀect any state variable, whereas artefactual processes aﬀect only
artefactual state. This formulation of the model for physiological condition monitoring is
illustrated in Fig. 9.2.
The FSLDS also gives us the ability to represent diﬀerent sources of uncertainty in
the system. We can explicitly specify the intra-class variability in the dynamics using
the parameter Q and the measurement noise using the parameter R. There is no way to
make this distinction in either of the other models, which have only one noise term per
regime. However, this ﬂexibility in the FSLDS is obtained at the cost of greater complexity,
particularly in terms of computing inferences, as we will see in Section 9.5.
9.3
Novel conditions
So far we have assumed that the monitoring data contains a limited number of regimes, for
which labelled training data is available. In real-world monitoring applications, however,
there is often such a great number of potential dynamical regimes that it might be imprac-
tical to model them all, or we might never have comprehensive knowledge of them. It can
therefore be useful to include a factor in the condition monitoring model which represents
all ‘unusual cases’.
In this section we present a method for modelling previously unseen dynamics as an
extra factor in the model, referred to as the ‘X-factor’. This represents all dynamics which
are not normal and which also do not correspond to any of the known regimes. A sequence
of data can only be said to have novelty relative to some reference, so the model is learnt

Physiological monitoring with switching linear systems
187
taking into account the parameters of the normal regime. The inclusion of this factor in
the model has two potential beneﬁts. First, it is useful to know when novel regimes are
encountered, e.g. in order to raise an alarm. Second, the X-factor provides a measure
of conﬁdence for the system. That is, when a regime is conﬁdently classiﬁed as ‘none
of the above’, we know that there is some structure in the data which is lacking in the
model.
9.3.1
The X-factor
First consider a case in which we have independent, one-dimensional observations which
normally follow a Gaussian distribution. If we expect that there will also occasionally be
spurious observations which come from a diﬀerent distribution, then a natural way to model
them is by using a wider Gaussian with the same mean. Observations close to the mean
retain a high likelihood under the original Gaussian distribution, while outliers are claimed
by the new model.
The same principle can be applied when there are a number of known distributions,
so that the model is conditionally Gaussian, y | s ∼N

µ(s), Σ(s)
. For condition moni-
toring we are interested in problems where we assume that the possible settings of s
represent a ‘normal’ mode and a number of known additional modes. In physiological
monitoring, for example, the normal mode corresponds to the times when the physiol-
ogy is stable and there is no artefactual corruption of the observed data. Additional modes
correspond e.g. to known problems with the monitoring equipment or speciﬁc patholo-
gies. We assume here that the normal regime is indexed by s = 1, and the additional
known modes by s = 2, . . . , K. In this static case, we can construct a new model, indexed
by s = ∗, for unexpected data points by inﬂating the covariance of the normal mode,
so that
Σ(∗) = ξΣ(1),
µ(∗) = µ(1),
(9.3)
where normally ξ > 1. We refer to this type of construction for unexpected observations
as an ‘X-factor’. The parameter ξ determines how far outside the normal range new data
points have to fall before they are considered ‘not normal’.
The likelihood functions for a normal class and a corresponding X-factor are shown in
Fig. 9.3(a). Clearly, data points that are far away from the normal range are more likely
to be classiﬁed as belonging to the X-factor. For condition monitoring this can be used in
conjunction with a number of known classes, as shown in Fig. 9.3(b). Here, the X-factor
y
p(y|s)
y
p(y|s)
0
1/2
f
Sy(f)
(a)
(b)
(c)
Figure 9.3 (a) Class conditional likelihoods in a static 1D model, for the normal class (solid) and the X-factor
(dashed). (b) Likelihoods of the normal class and X-factor in conjunction with other known, abnormal regimes
(shown dotted). (c) The power spectral density of a latent AR(5) process with white observation noise (solid), and
that of a corresponding X-factor process (dashed).

188
John A. Quinn and Christopher K. I. Williams
has the highest likelihood for regions which are far away from any known modes, as well
as far away from normality.
We can generalise this approach to dynamic novelty detection by adding a new fac-
tor to a trained factorial switching linear dynamical model, by inﬂating the system noise
covariance of the normal dynamics
Q(∗) = ξQ(1),
(9.4)
n
A(∗),C(∗), R(∗), d(∗)o
=
n
A(1),C(1), R(1), d(1)o
.
(9.5)
To help understand why Eqs. (9.4) and (9.5) are a dynamic generalisation of Eq. (9.3),
consider the speciﬁc case of a hidden scalar AR(p) process,
xt ∼N

p
X
k=1
αkxt−k, σ2
q
,
yt ∼N(xt, σ2
r).
The power spectral density for the hidden process xt at frequency f is given by
S x( f) =
σ2
q
1 −Pp
k=1 αke−2πifk2 ,
where −1
2 ≤f ≤1
2, assuming one observed value per unit of time. By inﬂating σ2
q (as spec-
iﬁed in Eq. (9.4)) we observe that the power is increased at each frequency. The observed
process has spectrum S y( f) = S x( f) + σ2
r. As the scale of S y( f) is determined by the mag-
nitudes of the two noise variances, inﬂating σ2
q will have the eﬀect of increasing the power
at every frequency, as illustrated in Fig. 9.3(c).
In the LDS, any sequence of x’s is jointly Gaussian. Consider the case where the state
is a scalar variable, the eigenfunctions are sinusoids and the eigenvalues are given by the
power spectrum. As increasing the system noise has the eﬀect of increasing the power at all
frequencies in the state sequence, we have a dynamical analogue of the static construction
given above.
A similar model for changes in dynamics is mentioned by [13, p. 458 and §12.4], who
suggest it as the parameterisation of an extra state in the unfactorised SLDS for modelling
large jumps in the x-process, and suggest setting ξ = 100. Their analysis in §12.4.4 shows
that this is used to model single-time-step-level changes, and not (as we are doing) sus-
tained periods of abnormality. We ﬁnd a much smaller value ξ = 1.2 to be eﬀective for our
task (larger values of ξ mean that an observation sequence must deviate further from nor-
mal dynamics to be claimed by the X-factor). A diﬀerent generative model for the X-factor
in principle would be white noise, but we ﬁnd in practice that this model is too dissimilar
to the real signal and is not eﬀective.
The nature of the measurement noise, and hence the value of the parameter R(s), is
assumed to be the same for both the normal regime and for the X-factor. Care needs to be
taken that the known factor dynamics do not have a very high variance compared to the
normal dynamics. It is clear from Fig. 9.3(b) that the X-factor will not be eﬀective if any
of the factors are wider than normality. This can be ascertained by examining the spectra
of the diﬀerent model dynamics.
9.4
Parameter estimation
In a condition monitoring problem, it is assumed that we are able to interpret at least
some of the regimes in the data – otherwise we would be less likely to have an interest

Physiological monitoring with switching linear systems
189
in monitoring them. We can therefore usually expect to obtain some labelled training data
{y1:T, s1:T}. When available, this data greatly simpliﬁes the learning process, because know-
ing the switch setting in the FSLDS makes the model equivalent to an LDS. The learning
process is therefore broken down into the training of a set of LDS models – one per switch
setting. We might choose a particular parameterisation, such as an AR model of order p hid-
den by observation noise and ﬁt parameters accordingly. Expectation maximisation (EM)
can be used to improve parameter settings given an initialisation [3]. We describe particular
methods used for parameter estimation in the physiological monitoring application which
incorporate both of these ideas.
When labelled training data is available, estimates of the factor transition probabilities
are given by P( f (m)
t
= j | f (m)
t−1 = i) =
nij+ζ
PM
k=1(nik+ζ), where ni j is the number of transitions
from factor setting i to setting j in the training data. The constant terms ζ (set to ζ = 1 in
the experiments described later in the chapter) are added to prevent any of the transition
probabilities from being zero or very small.
Some veriﬁcation of the learned model is possible by clamping the switch setting to
a certain value and studying the resulting LDS. One simple but eﬀective test is to draw
a sample sequence and check by eye whether it resembles the dynamics of training data
which is known to follow the same regime [1, §5.7]. Some insight into the quality of the
parameter settings can also be gained by considering estimation of the hidden state x in the
LDS. The Kalman ﬁlter equations yield both an innovation sequence, ˜y1:T (the diﬀerence
between the predicted and actual observations), and a speciﬁcation of the covariance of the
innovations under ideal conditions. An illuminating test is therefore to compare the actual
and ideal properties of the innovation sequence when applied to training data. In particular,
the innovations ˜yt should come from a Gaussian distribution with zero mean and a speciﬁc
covariance, and should be uncorrelated in time. We ﬁnd in practice that such tests are useful
when training FSLDS models for condition monitoring. For more details about veriﬁcation
in linear dynamical systems, see [1, §5.5].
We now show examples of learning diﬀerent aspects of the FSLDS in the physiologi-
cal monitoring setting. We begin with system identiﬁcation for the ‘normal’ physiological
dynamics (i.e. the dynamics which are observed when the baby is stable and there is no
artefactual inﬂuence) in Section 9.4.1. We then show how to model an artefactual process,
the drawing of a blood sample, in Section 9.4.2. In Section 9.4.3, we introduce the training
of a physiological factor associated with bradycardia, a speciﬁc heart problem. Learning the
X-factor parameter ξ is covered in Section 9.4.4. Finally in Section 9.4.5 we demonstrate
how to combine dynamical models into the overall factorised SLDS.
9.4.1
Learning normal dynamics: heart rate
Looking at examples of normal heart rate dynamics as in the top left and right panels of
Fig. 9.4, it can be observed ﬁrst of all that the measurements tend to ﬂuctuate around a
slowly drifting baseline. This motivates the use of a model with two hidden components:
the signal xt, and the baseline bt. These components are therefore used to represent the
true heart rate, without observation noise. The dynamics can be formulated using an AR
process, such as an AR(p1) signal that varies around an AR(p2) baseline, as given by the
following equations:
xt −bt ∼N

p1
X
k=1
αk(xt−k −bt−k), η1
,
bt ∼N

p2
X
k=1
βkbt−k, η2


190
John A. Quinn and Christopher K. I. Williams
0
1000
2000
3000
4000
150
160
170
180
Heart rate (bpm)
bt
0
1000
2000
3000
4000
160
170
180
Time (s)
xt − bt
0
1000
2000
3000
4000
−5
0
5
0
1000
2000
3000
4000
150
160
170
180
Heart rate (bpm)
bt
0
1000
2000
3000
4000
150
160
170
Time (s)
xt − bt
0
1000
2000
3000
4000
−10
0
10
Figure 9.4 In these two examples, HR measurements (top left and top right panels) are varying quickly within
normal ranges. The estimated distributions of the underlying signal (bottom left and bottom right panels) are split
into a smooth baseline process and zero-mean high-frequency component, given by applying the Kalman ﬁlter
equations with parameters learnt as in Section 9.4.1.
where η1, η2 are noise variances. For example, an AR(2) signal with AR(2) baseline has the
following state space representation:
xt =

xt
xt−1
bt
bt−1

,
A =

α1
α2
β1 −α1
β2 −α2
1
0
0
0
0
0
β1
β2
0
0
1
0

,
Q =

η1 + η2
0
0
0
0
0
0
0
0
0
η2
0
0
0
0
0

,
C = [1 0 0 0].
(9.6)
It is straightforward to adjust this construction for diﬀerent values of p1 and p2. The mea-
surements are therefore generally taken to be made up of a baseline with low-frequency
components and a signal with high-frequency components. We begin training this model
with a heuristic initialisation, in which we take sequences of training data and remove
high-frequency components by applying a symmetric 300-point moving average ﬁlter. The
resulting signal is taken to be the low-frequency baseline. The residual between the orig-
inal sequences and the moving-averaged sequences are taken to contain both stationary
high-frequency haemodynamics as well as measurement noise. These two signals can be
analysed according to standard methods and modelled as AR or integrated AR processes
(speciﬁc cases of autoregressive integrated moving average (ARIMA) processes [12]) of
arbitrary order. Heart rate sequences were found to be well modelled by an AR(2) signal
varying around an ARIMA(1,1,0) baseline. An ARIMA model is a compelling choice for
the baseline, because with a low noise term it produces a smooth drift.1 Having found this
1The ARIMA(1,1,0) model has the form (Xt −βXt−1) = α1(Xt−1 −βXt−2) + Zt where β = 1 and Zt ∼N(0, σ2
Z).
This can be expressed in un-diﬀerenced form as a non-stationary AR(2) model. In our implementation we set
β = 0.999 and with |α1| < 1 we obtain a stable AR(2) process, which helps to avoid problems with numerical
instability. This slight damping makes the baseline mean-reverting, so that the resulting signal is stationary. This
has desirable convergence properties for dropout modelling.

Physiological monitoring with switching linear systems
191
Time (s)
BS
0
50
100
150
200
250
300
20
30
40
50
60
Blood pressure (mmHg)
Sys
Dia
Time (s)
BS
0
50
100
150
200
250
300
20
30
40
50
60
Blood pressure (mmHg)
Sys
Dia
(a)
(b)
Figure 9.5 Genuine blood sample, and a sample drawn from the model with switch settings clamped to be the
same; BS indicates the factor switch setting, where black denotes f (BS)
t
= 1 (blood sample dynamics) and grey
denotes f (BS)
t
= 0 (normal dynamics).
initial setting of the model parameters, EM updates are then applied. These are similar to the
updates given in [3], though constrained so that we retain the structure in Eq. (9.6). This has
been found to be particularly useful for reﬁning the estimates of the noise terms Q and R.
Examples of the heart rate model being applied using a Kalman ﬁlter to sequences of
heart rate data are shown in Fig. 9.4, which plots the noisy observations yt (upper panels)
and estimates of the baseline bt and high frequency components xt −bt (middle and lower
panels respectively).
9.4.2
Learning artefactual dynamics: blood sampling
An arterial blood sample might be taken every few hours from each baby. This involves
diverting blood from the arterial line containing the pressure sensor, so that measurements
are entirely unrelated from the baby’s physiology. Throughout the operation a saline pump
acts against the sensor, causing an artefactual ramp in the blood pressure measurements.
The slope of the ramp is not always the same, as the rate at which saline is pumped can
vary. See Fig. 9.5(a) for an example. During this process, the systolic and diastolic blood
pressures of the baby evolve as normal but are unobserved.
For this artefactual rise, we specify a structural model in which the artefactual
measurements at evolve according to a gradient which is subject to a random walk
at ∼N

at−1 + da + ct−1, σ2
a

,
ct ∼N

ct−1, σ2
c

.
Each of these terms are scalars. Parameter da is a positive constant specifying the average
drift, which is modiﬁed by the random walk term ct. The Gaussian noise on at with variance
σ2
a models the diﬀerences in slope of blood samples taken at diﬀerent times, while the noise
on ct with variance σ2
c models the change in slope within a single blood sample operation.
During the procedure, both blood pressure readings (systolic and diastolic) are generated
by the same underlying value at.
Using a state space form we obtain the following hidden state dynamics for at, ct
xt =
" at
ct
#
, dBS =
" da
0
#
, ABS =
" 1
1
0
1
#
, QBS =
" σ2
a
0
0
σ2
c
#
.
The observations yt are two-dimensional, where yt,1 is the systolic blood pressure and yt,2
is the diastolic blood pressure. The observation model is given by
CBS =
" 1
0
1
0
#
, RBS =
" rSysBP
0
0
rDiaBP
#
.

192
John A. Quinn and Christopher K. I. Williams
Figure 9.6 State spaces for normal dynamics and for blood sampling.
The parameters are straightforward to learn from training data. Let T denote a two-
dimensional time series spanning n time steps and containing an example of the observed
systolic and diastolic blood pressures during a blood sample operation, such that T =
yt,1:2 | 1 ≤t ≤n	. If we have a set of such training sequences T1, . . . , TN then parameter
estimation proceeds as follows:
da = 1
N
N
X
i=1
slope(Ti),
σ2
a = V  slope(Ti) | 1 ≤i ≤N	,
where the slope of the measurements can be found either by simply calculating the gra-
dient slope (T) =
1
2(n−1)
(y1,1 + y1,2) −(yn,1 + yn,2) or, better, with linear regression. The
variance of the noise on ct is found using σ2
c =
1
N
PN
i=1 V (diﬀ(Ti)), where diﬀ(T) =
n 1
2
(yi,1 + yi,2) −(yi−1,1 + yi−1,2) | 2 ≤i ≤n
o
. We assume that the monitoring equipment has
the same observation noise whether a blood sample is being taken or not. Therefore RBS is
the same as for the normal dynamics, learnt as described in the previous section.
Having set the values of each of the parameters, these dynamics can be incorporated into
the full switching model. Figure 9.6 shows the structure of the full transition matrix A and
the observation matrix C, for blood sample dynamics ( f (BS) = 1) and for normal dynamics
( f (BS) = 0). During a blood sample, observations are generated only by the artefactual
state variables. State variables representing the baby’s actual systolic and diastolic blood
pressure are evolved according to the normal dynamics, but not observed (see Fig. 9.13(a)
for an illustration of hidden state inferences in this case). During normal dynamics, the
physiological state variables are observed, and at is tied to the estimate of systolic blood
pressure.
One way to check that such a model has been adequately ﬁtted is to sample from it
and verify that the results have similar characteristics to the training data. Figure 9.5(a)
shows a genuine example of measurements made while a blood sample is being taken. We
then draw a sample of the same length from the switching model, Fig. 9.5(b), using the
same initial state at t = 1 and with the switch settings clamped to be the same as for the
genuine sequence. Note that the observation noise appears greater on the simulated version
partly because iid Gaussian noise is being used to model quantisation eﬀects in the training
observations.

Physiological monitoring with switching linear systems
193
0
10
20
50
100
150
200
Heart rate (bpm)
Time (s)
0
10
20
30
50
100
150
200
0
50
0
50
100
150
200
0
50
100
120
140
160
0
20
40
50
100
150
Figure 9.7 Examples of periods of bradycardia. Heart rate is measured in beats per minute (bpm).
9.4.3
Learning physiological dynamics: bradycardia
Bradycardia is a (usually temporary) reduction in the heart rate, and brief episodes are
common for premature infants. It can have many causes, some benign and some serious.
Examples of this phenomenon are shown in Fig. 9.7. Because there is no simple physical
process governing this pattern of observations, we have less opportunity to parameterise
the model based on domain knowledge. Therefore we turn to statistical principles to learn
the dynamics and validate our chosen model.
There are several ways to model this pattern. For example, we could learn the dynamics
of the whole sequence as a single regime, or split it up into falling and rising dynamics. In
the former case, we could specify some positive second derivative, or look at the variability
of the observations. In the latter case we could use drift terms to model the fall and rise, or
use exponential decays towards diﬀerent mean values.
Bradycardic drops and subsequent rises in heart rate were found to be adequately
modelled by retraining the ARIMA(1,1,0) model for baseline heart rate dynamics. The
high-frequency heart rate dynamics were kept the same as for the stable heart rate regime.
As for the normal regime, this model learnt in terms of hidden ARIMA processes was used
as an initial setting and updated with three iterations of EM.
9.4.4
Learning the novelty threshold
Unlike the factors for which we have an interpretation, we do not assume that labelled
training data is available for learning X-factor dynamics. We therefore consider a partial
labelling of the training data y1:T, comprising of annotations for known factors and for
some representative quantity of normal dynamics. The remainder of the training data is
unlabelled, giving us a semi-supervised learning problem.
To apply the EM algorithm to the X-factor within a SLDS (non-factorised switch
setting), the M-step update to ξ is given by
˜ξ =
1
PT
t=2 p(st = ∗| y1:T, θold)
(9.7)
×
T
X
t=2

(xt −A(1)xt−1)⊤Q(1)−1(xt −A(1)xt−1)

xt,xt−1 | y1:T ,θold
p(st = ∗| y1:T, θold)
where st = ∗indexes the X-factor switch setting at time t and ⟨·⟩denotes expectation. We
describe strategies for calculating the expectation term in Sections 9.5.1 and 9.5.2. It can
be convenient to use the ﬁltering estimate ⟨·⟩xt,xt−1 | y1:t,θold as an approximation.
The parameters A(1) and Q(1) are respectively the system matrix and system noise
covariance matrix for the normal dynamical regime. Intuitively, this update expression
calculates a Z-score, considering the covariance of novel points and the covariance of

194
John A. Quinn and Christopher K. I. Williams
the normal regime. Every point is considered, and is weighted by the probability of hav-
ing been generated by the X-factor regime. Equation (9.7) does not explicitly constrain
˜ξ to be greater than 1, but with appropriate initialisation it is unlikely to violate this
condition.
The factorial case is a little more complicated due to the possibility that diﬀerent com-
binations of factors can overwrite diﬀerent channels. For example, if a bradycardia is
occurring in conjunction with some other, unknown regime, then the heart rate dynam-
ics are already well explained and should not be taken into account when re-estimating the
X-factor parameter ξ.
An alternative to Eq. (9.7) and an extension to the factorial case is given in [9, §C.4].
9.4.5
Learning the factorial model
The previous discussion assumed that we train the model conditioned on each switch setting
independently, and then combine parameters. When there are many factors this implies that
a great quantity of training data is needed. In practice, however, this requirement can be
mitigated.
Where there are several measurement channels it may be found that some factors
‘overwrite’ others. For example, if we are monitoring the physiological condition of
a patient, we might have two factors: bradycardia and incubator open. If there is a
period of bradycardia while the incubator is open, then we would see the same mea-
surements as though there was only the bradycardia. It is often possible to specify an
ordering of factors such that some overwrite measurement channels of others in this way.
This ordering speciﬁes conditional independencies in the factors and observations, such
that
f (i) overwrites f (j) on measurement channel d =⇒yt,d ⊥⊥f (j)
t
| f (i)
t
> 0,
assuming that the switch setting f (j) = 0 means that factor j is ‘inactive’ and positive inte-
gers index the active dynamics of that factor. The signiﬁcance of this is that examples of
every combination of factors do not need to be found in order to train the factorial model.
The factors can be trained independently, and then combined together by reasoning about
which channels are overwritten for each combination.
9.5
Inference
Exact inference in the SLDS is intractable, thus we need to make approximations regard-
less of whether we are doing ﬁltering or smoothing. We describe each case in turn in
Sections 9.5.1 and 9.5.2, including its relevance to this application and the techniques which
can be used to make it tractable. We then discuss modiﬁcations of the standard inference
procedures to suit this application in Sections 9.5.3 and 9.5.4.
9.5.1
Filtering
During deployment of the neonatal monitoring system we are primarily interested in ﬁlter-
ing, although ﬁxed-lag smoothing with a small time delay would also be conceivable. We
receive measurements second by second and are required to make an immediate diagnosis
based only on the history.
The time taken to calculate the exact ﬁltering distribution p(st, xt | y1:t) in the SLDS
scales exponentially with t, making it intractable. This is because the probabilities of having

Physiological monitoring with switching linear systems
195
moved between every possible combination of switch settings in times t−1 and t are needed
to calculate the posterior at time t. Hence the number of Gaussians needed to represent the
posterior exactly at each time step increases by a factor of K, the number of cross-product
switch settings. The intractability of inference in this model is proved in [7], which also
concentrates on a fault diagnosis setting.
Gaussian sum approximations have been reviewed in previous chapters of this book.
While performing the forward pass to calculate the ﬁltering distribution, at each time step
we maintain an approximation of p(xt | st, y1:t) as a mixture of I Gaussians. Calculating
the Kalman updates and likelihoods for every possible setting of st+1 will result in the
posterior p(xt+1 | st+1, y1:t+1) having KI mixture components, which can be collapsed back
into I components by matching means and variances of the distribution for each setting of
st.
Rao–Blackwellised particle ﬁltering (RBPF) [8] is another technique for approximate
inference, which exploits the conditionally linear dynamical structure of the model to try
to select particles close to the modes of the true ﬁltering distribution. A number of parti-
cles are propagated through each time step, each with a switch state st and an estimate of
the mean and variance of xt. A value for the switch state st+1 is obtained for each particle
by sampling from the transition probabilities, after which Kalman updates are performed
and a likelihood value can be calculated. Based on this likelihood, particles can be either
discarded or multiplied. Because Kalman updates are not necessarily calculated for every
possible setting of st+1, this method can give an increase in speed when there are many
factors. The fewer particles used, the greater the trade-oﬀof speed against accuracy, as it
becomes less likely that the particles can collectively track all modes of the true posterior
distribution. Rao–Blackwellised particle ﬁltering has been shown to be successful in con-
dition monitoring problems with switching linear dynamics, for example in fault detection
in mobile robots [2].
In the factorised model, the number of switch settings may be high. There may also be
some transitions with low probability. In this case, sampling from the discrete transition
prior p(st | ˆs(i)
t−1) might be problematic, as there is a chance that no particles are sampled
from certain switch settings. We modify the prior so that we sample from ˆs(i)
t
∼q(st | ˆs(i)
t−1)
where
q(st = k | st−1 = j) =
p(st = k | st−1 = j) + ζ
PM
l=1 (p(st = l | st−1 = j) + ζ)
,
which makes it more likely that particles are sampled from diﬀerent switch settings. In the
following experiments we use ζ = 0.1. If a large setting of ζ was thought to be distorting
the results, then it would be straightforward to adjust the importance weight of each particle
i by a factor of
p

ˆs(i)
t | ˆs(i)
t−1

q

ˆs(i)
t | ˆs(i)
t−1
 at each time step to compensate.
9.5.2
Smoothing
While carrying out parameter estimation, we are interested in smoothing. Given oﬄine
training data, we can aﬀord to exploit backwards information in order to reﬁne estimates of
the system parameters. The ‘E’ steps of the EM procedures cited in Section 9.4 therefore
use a backwards pass of the data – though in this case we already know the switch settings
st for all t, so we do smoothing during parameter estimation on linear dynamical systems,
not on the full switching model. It can also be interesting to see how the performance on

196
John A. Quinn and Christopher K. I. Williams
Time (s)
Heart rate (bpm)
150
300
450
145
150
155
160
165
Figure 9.8 Inference of true heart rate under a dropout, with automatic handling of missing observations. The solid
black line shows HR observations, where available, the dashed line shows the mean of the estimated distribution
of true HR, and the shaded area shows two standard deviations of the estimated distribution.
testing data changes when calculating smoothed inferences, even though this is not realistic
for deployment.
Approximate smoothed inference in the full switching model is possible with various
schemes. In the previous section we described the simplifying assumption that the true
ﬁltering distribution is a mixture of Gaussians (the Gaussian sum approximation). We can
make the same assumption about the smoothing distribution. Gaussian sum smoothing is
possible using expectation correction (see Chapter 8). Smoothed inference can also be done
with expectation propagation (see Chapter 7), or by variational approximation [3].
We now discuss some adaptations to the standard inference routines, for handling
missing observations and constraining the number of possible switch transitions.
9.5.3
Handling missing observations
Zero observations in this application denote missing values, for example where a probe has
been disconnected. It would be possible in principle to add an extra factor for each channel
to indicate a missing observation. However each extra factor with two states slows inference
down by a factor of two. To add a dropout channel for each of 10 observed channels would
slow down inference by a factor of 210.
Instead we can test at each time whether there are any dimensions in yt which are zero.
Within the inference routines, for each switch setting we then use the updated observation
matrix given by
H∗
i j =

Hij
where yj , 0
0
otherwise.
The Kalman ﬁlter updates, which are incorporated in all the inference schemes we con-
sider, have two stages: prediction (in which the variance of the estimates increases) and
correction (in which the variance decreases). The eﬀect of this modiﬁcation to the observa-
tion matrix is to cancel the correction step whenever observations are unavailable. Typical
inference results are shown in Fig. 9.8, where a period of heart rate measurements is miss-
ing and we show the estimated distribution of ‘true’ heart rate. In Section 9.4.3 we made
the normal dynamics stable and mean-reverting, so that the estimates reach an equilibrium
in this situation.
9.5.4
Constraining switch transitions
In this application, factors change switch settings slowly relative to the sampling rate. It is
therefore unlikely that more than one factor changes its setting in any one time step. We
can use this to constrain the transitions.

Physiological monitoring with switching linear systems
197
The discrete transition probabilities are deﬁned by a matrix Z, where Zi j = p(st =
j | st−1 = i), given by Eq. (9.2). We can use an updated transition matrix Z∗such that
Z∗
i j =

Zij
where 0 ≤PM
s=1 I
h
f (i)[s] = f ( j)[s]
i
≤1
0
otherwise
where I[·] is the indicator function and f (i) [s] denotes the switch setting for factor i in the
cross product switch setting s. This reduces the inference time from O(K2) to O(K log K).
9.6
Experiments
This section describes experiments used to evaluate the model for condition monitoring.
Other than the X-factor, we consider here the incubator open/handling of baby factor
(denoted ‘IO’), the blood sample factor (denoted ‘BS’), the bradycardia factor (denoted
‘BR’) and the temperature probe disconnection factor (denoted ‘TD’). We demonstrate
the operation of the transcutaneous probe recalibration factor (denoted ‘TR’), but do not
evaluate it quantitatively due to a scarcity of training data.
Some conventions in plotting the results of these experiments are adopted throughout
this section. Horizontal bars below time series plots indicate the posterior probability of a
particular factor being active, with other factors in the model marginalised out. White and
black indicate probabilities of zero and one respectively.2 In general the plots show a subset
of the observation channels and posteriors from a particular model – this is indicated in the
text.
Twenty-four-hour periods of monitoring data were obtained from ﬁfteen premature
infants in the intensive care unit at Edinburgh Royal Inﬁrmary. The babies were between 24
and 29 weeks gestation (around 3–4 months premature), and all in around their ﬁrst week
post partum.
Each of the ﬁfteen 24-hour periods was annotated by two clinical experts. At or near the
start of each period, a 30-minute section of normality was marked, indicating an example
of that baby’s current baseline dynamics. Each of the known common physiological and
artefactual patterns were also marked up.
Finally, it was noted where there were any periods of data in which there were clinically
signiﬁcant changes from the baseline dynamics not caused by any of the known patterns.
While the previous annotations were made collaboratively, the two annotators marked up
this ‘abnormal (other)’ category independently. The software package TSNet [6] was used
to record these annotations, and the recorded intervals were then exported into Matlab.
The number of intervals for each category, as well as the total and average durations, are
shown in Table 9.2. The ﬁgures for the ‘abnormal’ category were obtained by combining
the two annotations, so that the total duration is the number of points which either annotator
thought to be in this category, and the number of incidences was calculated by merging
overlapping intervals in the two annotations (two overlapping intervals are counted as a
single incidence).
The rest of this section shows the results of performing inference on this data and
comparing it to the gold standard annotations provided by the clinical experts.
2A convenient property of the models evaluated here, from the perspective of visualisation, is that the factor
posteriors tend to be close to zero or one. This is partly due to the fact that the discrete transition prior p(st|st−1)
is usually heavily weighted towards staying in the same switch setting (long dwell times).

198
John A. Quinn and Christopher K. I. Williams
Factor
Incidences
Total duration
Average duration
Incubator open
690
41 hours
3.5 mins
Abnormal (other)
605
32 hours
3.2 mins
Bradycardia
272
161 mins
35 secs
Blood sample
91
253 mins
2.8 mins
Temp. disconnection
87
572 mins
6.6 mins
TCP recalibration
11
69 mins
6.3 mins
Table 9.2 Number of incidences of diﬀerent factors, and total time for which each factor was annotated as being
active in the training data (total duration of training data 15 × 24 = 360 hours).
9.6.1
Evaluation of known factors
The dataset for evaluation consisted of ﬁfteen 24-hour periods of monitoring data (one day
of monitoring data for each of ﬁfteen babies). Evaluation was done using leave-one-out
cross validation, so that the 24-hour period from each baby was used for testing in turn,
using data from the other fourteen babies for training.
From each 24-hour period, a 30-minute section near the start containing only
normal dynamics was reserved for calibration (learning normal dynamics accord-
ing to Section 9.4.3). Testing was therefore conducted on the remaining 23 1
2 hour
periods.
The quality of the inferences made were evaluated using receiver operating character-
istic (ROC) curves for the diﬀerent inference methods. The ROC curve plots the rate of
true positives against the rate of false negatives, such that the area under the curve (AUC)
gives a balanced measure of performance even when the class sizes in the testing data are
unequal. Another useful statistic which can be obtained from the ROC curve is the equal
error rate (EER), which is the error rate for the threshold setting at which the false positive
rate is equal to the false negative rate. We give error rates, so smaller numbers are better
(some authors give 1 −EER).
Figure 9.9 shows a plot of AUC against processing time for Gaussian sum ﬁltering,
Gaussian sum smoothing using expectation correction, and RBPF with varying numbers of
particles. Figure 9.10 shows a corresponding plot for EER against processing time.
Gaussian sum ﬁltering (forward mixture size I = 1) had good performance on all
four factors. Gaussian sum smoothing (with a forward mixture size I = 1 and back-
ward mixture size J = 1) with expectation correction gave improved performance in the
inference of bradycardia, and similar or slightly improved performance for the other three
factors.
Rao–Blackwellised particle ﬁltering results were not so good, even with high numbers
of particles, though it is interesting to note that a certain level of accuracy can be achieved
with much faster inference than the other methods.
Speciﬁc examples of the operation of these models are now given. Figures 9.11–9.13
show inferences of switch settings made with the FSLDS with Gaussian sum approxima-
tion. In each case the switch settings have been accurately inferred. Figure 9.11 shows
examples of transcutaneous probe recalibration, correctly classiﬁed in conjunction with a
blood sample and a core temperature probe disconnection. In Fig. 9.11(b) the recalibra-
tion and disconnection begin at around the same time, as a nurse has handled the baby
in order to access the transcutaneous probe, causing the temperature probe to become
detached.

Physiological monitoring with switching linear systems
199
0
20
40
0.7
0.75
0.8
0.85
0.9
0.95
1
AUC
Inference time (hours)
Bradycardia
0
20
40
0.7
0.75
0.8
0.85
0.9
0.95
1
Handling
0
20
40
0.65
0.7
0.75
0.8
0.85
0.9
0.95
Temp. disconnect
0
20
40
0.7
0.75
0.8
0.85
0.9
0.95
1
Blood sample
Filtered
Smoothed
RBPF
Figure 9.9 AUC for diﬀerent inference methods on four known factors, for 360 hours of monitoring data. ‘Fil-
tered’ and ‘Smoothed’ denote the performance of Gaussian sum ﬁltering and EC smoothing respectively. RBPF
inference was done with 5, 10, 20, 50, 75, 100, 150 and 250 particles.
0
20
40
0
0.1
0.2
0.3
0.4
EER
Inference time (hours)
Bradycardia
0
20
40
0
0.1
0.2
0.3
0.4
Handling
0
20
40
0
0.1
0.2
0.3
0.4
Temp. disconnect
0
20
40
0
0.1
0.2
0.3
0.4
Blood sample
Filtered
Smoothed
RBPF
Figure 9.10 EER for diﬀerent inference methods on four known factors, with data and inference methods as in
Fig. 9.9.
Figure 9.12 shows inference of bradycardia, blood sampling and handling of the baby.
In Fig. 9.12(a) it has been possible to recognise the disturbance of heart rate at t = 800
as being caused by handling of the baby, distinguished from the bradycardia earlier when
there is no evidence of the incubator having been entered.
For the blood sample and temperature probe disconnection factors, the measurement
data bears no relation to the actual physiology, and the model should update the estimated
distribution of the true physiology in these situations accordingly. Figure 9.13 contains
examples of the inferred distribution of true physiology in data periods in which these two
artefacts occur. In each case, once the artefactual pattern has been detected, the physio-
logical estimates remain constant or decay towards a mean. As time passes since the last
reliable observation, the variance of the estimates increases towards a steady state.
9.6.2
Inference of novel dynamics
Examples of the operation of the X-factor are shown in Figs. 9.14–9.16. We employ two
models with diﬀerent sets of factors. The label ‘(1)’ on the plots denotes the FSLDS with
only one factor, the X-factor. The label ‘(5)’ denotes the FSLDS which has ﬁve factors –
the four known factors and the X-factor. Figure 9.14 shows two examples of inferred switch
settings under model (5) for periods in which there are isolated physiological disturbances.
Both the posteriors for the X-factor and the gold standard intervals for the ‘abnormal
(other)’ category are shown. The physiological disturbances in both panels are cardio-
vascular and have clearly observable eﬀects on the blood pressure and oxygen saturation
measurements.

200
John A. Quinn and Christopher K. I. Williams
BS
Time (s)
TR
0
1000
2000
3000
4000
5000
0
20
40
TcPO
(kPa)
0
10
20
TcPCO
(kPa)
20
40
60
Sys. BP
Dia. BP
(mmHg)
TD
Time (s)
TR
0
200
400
600
800
1000
1200
10
20
30
TcPO
(kPa)
0
20
40
TcPCO
(kPa)
30
35
40
Core temp.
Incu. temp.
(°C)
(a)
(b)
Figure 9.11 Inferred distributions of switch settings for two situations involving recalibration of the transcuta-
neous probe; BS denotes a blood sample, TR denotes a recalibration and TD denotes a core temperature probe
disconnection. In panel (a) the recalibration is preceded by a dropout, followed by a blood sample. Diastolic BP
is shown as a dashed line which lies below the systolic BP plot. Transcutaneous readings drop out at around
t = 1200 before the recalibration. In panel (b), the solid line shows the core temperature and the dashed line
shows incubator temperature. A core temperature probe disconnection is identiﬁed correctly, as well as the recali-
bration. Temperature measurements can occasionally drop below the incubator temperature if the probe is near to
the portals; this is accounted for in the model by the system noise term Q.
BR
Time (s)
IO
0
200
400
600
800
1000
1200
1400
1600
50
100
150
200
HR
(bpm)
60
70
80
90
Humidity
(%)
BS
Time (s)
BR
0
200
400
600
800
0
100
200
HR
(bpm)
0
50
100
Sys. BP
(mmHg)
20
40
60
Dia. BP
(mmHg)
(a)
(b)
Figure 9.12 Inferred distributions of switch settings for two further situations in which there are eﬀects due to
multiple known factors. In panel (a) there are incidences of bradycardia, after which the incubator is entered.
There is disturbance of heart rate during the period of handling, which is correctly taken to be associated with
the handling and not an example of spontaneous bradycardia. In panel (b), bradycardia and blood samples are
correctly inferred. During the blood sample, heart rate measurements (supplied by the blood pressure sensor) are
interrupted.
Time (s)
BS
0
50
100
150
200
250
Sys. BP
(mmHg)
30
40
50
60
Dia. BP
(mmHg)
20
30
40
50
Time (s)
TD
0
200
400
600
800
1000
1200
Core temp. (°C)
35
35.5
36
36.5
37
37.5
38
(a)
(b)
Figure 9.13 Inferred distributions of the true physiological state during artefactual corruption of measurements.
Panel (a) shows correct inference of the duration of a blood sample, and panel (b) shows correct inference of
a temperature probe disconnection. Measurements are plotted as a solid line, and estimates ˆxt relating to true
physiology are plotted as a dashed line with the grey shading indicating two standard deviations. In each case,
during the period in which measurements are corrupted the estimates of the true physiology are propagated with
increased uncertainty.

Physiological monitoring with switching linear systems
201
X
Time (s)
True X
0
500
1000
1500
2000
2500
30
40
50
60
Sys. BP (mmHg)
70
80
90
100
SpO  (%)
X
Time (s)
True X
0
1000
2000
3000
4000
5000
6000
30
40
50
60
Sys BP (mmHg)
85
90
95
100
SpO  (%)
(a)
(b)
Figure 9.14 Inferred switch settings for the X-factor, during periods of cardiovascular disturbance, compared to
the gold standard annotations.
In Fig. 9.14(a), the X-factor is triggered by a sudden, prolonged increase in blood pres-
sure and a desaturation, in broad agreement with the ground truth annotation. In Fig. 9.14(a)
there are two spikes in BP and shifts in saturation which are picked up by the X-factor, also
mainly in agreement with the annotation. A minor turning point in the two channels was
also picked up at around t = 2000, which was not considered signiﬁcant in the gold standard
(a false positive).
Eﬀects of introducing known factors to model (1) are shown in Fig. 9.15. In panel
(a), there are two occurrences of spontaneous bradycardia, HR making a transient drop to
around 100bpm. The X-factor alone in model (1) picks up this variation. Looking at the
inferences from model (5) for the same period, it can be seen that the bradycardia factor
provides a better match for the variation, and probability mass shifts correctly: the X-factor
is now inactive. In panel (b), a similar eﬀect occurs for a period in which a blood sample
occurs. The X-factor picks up the change in dynamics when on its own, and when all factors
are present in model (5) the probability mass shifts correctly to the blood sample factor. The
blood sample factor is a superior description of the variation, incorporating the knowledge
that the true physiology is not being observed, and so able to handle the discontinuity at
t = 900 eﬀectively.
Figure 9.16 shows examples of inferred switch settings from model (5) in which there
are occurrences of both known and unknown types of variation. In Fig. 9.16(a) a bradycar-
dia occurs in the middle of a period of elevated blood pressure and a deep drop in saturation.
The bradycardia factor is active for a period which corresponds closely to the ground truth.
X  (1)
X  (5)
Time (s)
BR  (5)
0
500
1000
1500
80
100
120
140
160
180
Heart rate (bpm)
X  (1)
X  (5)
Time (s)
BS  (5)
0
500
1000
1500
30
40
50
60
Sys. BP
(mmHg)
0
20
40
60
Dia. BP
(mmHg)
(a)
(b)
Figure 9.15 Inferred switch settings for two diﬀerent models, showing how known factors can explain away the
X-factor. Model (1) contains the X-factor only, whereas model (5) includes the X-factor and all known factors.
Panel (a) shows two instances of bradycardia, (b) shows a blood sample.

202
John A. Quinn and Christopher K. I. Williams
X
True X
BR
Time (s)
True BR
0
500
1000
1500
2000
2500
3000
100
150
200
HR
(bpm)
0
50
100
Sys. BP
(mmHg)
0
50
100
SpO
(%)
X
True X
BS
Time (s)
True BS
0
1000
2000
3000
4000
5000
20
40
60
80
Sys. BP
(mmHg)
0
20
40
60
0
1000
2000
3000
4000
50
Dia. BP
(mmHg)
(a)
(b)
X
True X
BR
Time (s)
True BR
0
200
400
600
800
1000
50
100
150
200
HR
(bpm)
40
60
80
100
SpO
(%)
X
True X
IO
Time (s)
True IO
0
200
400
600
800
1000
1200
1400
100
150
200
HR
(bpm)
60
80
100
SpO
(%)
70
80
90
Humidity
(%)
(c)
(d)
Figure 9.16 Inferred switch settings for the X-factor, in regions where other factors are active. In panel (a) a
bradycardia occurs in conjunction with a rise in blood pressure and deep desaturation. The X-factor is triggered
around the right region but is late compared to ground truth. In panel (b), unusual BP variation is correctly
classiﬁed as being due to a blood sample, followed by variation of unknown cause. Panel (c) shows bradycardia
with a desaturation picked up by the X-factor, and (d) shows the X-factor picking up disturbance after the incubator
has been entered.
The X-factor picks up the presence of a change dynamics at about the right time, but its
onset is delayed when compared to the ground truth interval. This again highlights a dif-
ﬁculty with ﬁltered inference, since at time just over 1000 it is diﬃcult to tell that this is
the beginning of a signiﬁcant change in dynamics without the beneﬁt of hindsight. In panel
(b) a blood sample is correctly picked up by the blood sample factor, while a later period
of physiological disturbance on the same measurement channels is correctly picked up by
the X-factor. Panel (c) shows another example of the bradycardia factor operating with
the X-factor, where this time the onset of the ﬁrst bradycardia is before the onset of the
X-factor. The X-factor picks up a desaturation, a common pattern which is already familiar
from panel (a). In panel (d), an interaction between the X-factor and the ‘Incubator open’
factor can be seen. From time 270 to 1000 the incubator has been opened, and all variations
including the spike in HR at t = 420 are attributed to handling of the baby. Oncethe incu-
bator appears to have been closed, further physiological disturbance is no longer explained
as an eﬀect of handling and is picked up by the X-factor.
Figure 9.17 shows the eﬀect of using smoothing to infer periods of novelty. Two periods
of novel dynamics are shown, with inference results for ﬁltering (upper) and smoothing
(lower). Filtered inferences of the X-factor tend to ‘trail oﬀ’, because without information
about future observations it is diﬃcult to tell when an unusual period of dynamics has
ended. Smoothed inferences correct this, so that the inferred period of normality correspond
more closely to periods of clinically signiﬁcant change.

Physiological monitoring with switching linear systems
203
X (Filtered)
Time (s)
X (Smoothed)
0
100
200
300
400
500
600
700
800
900
120
140
160
180
HR (bpm)
20
30
40
Sys. BP (mmHg)
Dia. BP (mmHg)
X (Filtered)
Time (s)
X (Smoothed)
0
200
400
600
800
1000
1200
1400
1600
1800
10
20
30
40
50
60
70
Sys. BP (mmHg)
Dia. BP (mmHg)
(a)
(b)
Figure 9.17 Panel (a) shows ﬁltered inference, (b) shows smoothed inference. The grey lines show diastolic blood
pressure.
9.7
Summary
This chapter has presented a general framework for inferring hidden factors from moni-
toring data, and has shown its successful application to the signiﬁcant real-world task of
monitoring the condition of a premature infant receiving intensive care. We have shown
how knowledge engineering and learning can be successfully combined in this framework.
Our formulation of an additional factor (the ‘X-factor’) allows the model to handle novel
dynamics. Experimental demonstration has shown that these methods are eﬀective when
applied to genuine monitoring data.
There are a number of directions in which this work could be continued. The set of
known factors presented here is limited, and more could usefully be added to the model
given training data. Also, experiments with the X-factor have shown that there are a signiﬁ-
cant number of non-normal regimes in the data which have not yet been formally analysed.
Future work might therefore look at learning what diﬀerent regimes are claimed by the
X-factor. This could be cast as an unsupervised or semi-supervised learning problem within
the model. Another possible avenue would be to look at the incorporation of non-linear
dynamics within the switching state space framework for physiological monitoring, using
generalised linear models for state transitions or observations.
Acknowledgments
We thank Neil McIntosh for supplying annotation of the data and
providing expert medical input on the experimental design. We also thank Birgit Wefers for
supplying additional annotation of the data, and Jim Hunter for modifying the Time Series
Workbench software for use in this research. Author JQ was funded by the premature baby
charity BLISS. The work was supported in part by the IST Programme of the European
Community, under the PASCAL Network of Excellence, IST-2002-506778.
Bibliography
[1] J. V. Candy, Model-Based Signal Processing,
Wiley-IEEE Press, 2005.
[2] N. de Freitas, R. Dearden, F. Hutter,
R. Morales-Menedez, J. Mutch and D. Poole.
Diagnosis by a waiter and a Mars explorer.
Proceedings of the IEEE, 92(3), 2004.
[3] Z. Ghahramani and G. E. Hinton. Parameter
estimation for linear dynamical systems.
Technical report, Department of Computer
Science, University of Toronto, 1996.
[4] Z. Ghahramani and G. E. Hinton. Variational
learning for switching state-space models.
Neural Computation, 12(4):963–996, 1998.
[5] Z. Ghahramani and M. Jordan. Factorial hidden
Markov models. Machine Learning, 29:245–273,
1997.
[6] J. R. W. Hunter. TSNet A distributed architecture
for time series analysis. In N. Peek and
C. Combi, editors, Intelligent Data Analysis in
bioMedicine and Pharmacology (IDAMAP
2006), pages 85–92, 2006.

204
John A. Quinn and Christopher K. I. Williams
[7] U. Lerner and R. Parr. Inference in hybrid
networks: theoretical limits and practical
algorithms. In Proceedings of the 17th Annual
Conference on Uncertainty in Artiﬁcial
Intelligence, pages 310–318, 2001.
[8] K. Murphy and S. Russell. Rao-Blackwellised
particle ﬁltering for dynamic Bayesian networks.
In A. Doucet, N. de Freitas, and N. Gordon,
editors, Sequential Monte Carlo in Practice.
Springer-Verlag, 2001.
[9] J. A. Quinn. Bayesian condition monitoring in
neonatal intensive Care. PhD thesis, University
of Edinburgh, 2007.
[10] J. A. Quinn. Neonatal condition monitoring
demonstration code. www.cit.mak.ac.
ug/staﬀ/jquinn/software.html, 2007.
[11] J. A. Quinn, C. K. I. Williams and N. McIntosh.
Factorial switching linear dynamical systems
applied to condition monitoring in neonatal
intensive care. To appear in IEEE Transactions
on Pattern Analysis and Machine Intelligence,
2009.
[12] R. H. Shumway and D. S. Stoﬀer. Time Series
Analysis and Its Applications. Springer-Verlag,
2000.
[13] M. West and J. Harrison. Bayesian Forecasting
and Dynamic Models. Springer-Verlag, 1999.
[14] P. C. Woodland. Hidden Markov models using
vector linear prediction and discriminative
output functions. In Proceedings of 1992 IEEE
International Conference on Acoustics, Speech
and Signal Processing, volume 1, pages
509–512. IEEE, 1992.
Contributors
John A. Quinn, Department of Computer Science, Faculty of Computing and IT, Makerere University,
Uganda
Christopher K. I. Williams, Institute for Adaptive and Neural Computation, School of Informatics,
University of Edinburgh

10
Analysis of changepoint models
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
10.1
Introduction
Many time series are characterised by abrupt changes in structure, such as sudden jumps
in level or volatility. We consider changepoints to be those time points which divide a
dataset into distinct homogeneous segments. In practice the number of changepoints will
not be known. The ability to detect changepoints is important for both methodological
and practical reasons including: the validation of an untested scientiﬁc hypothesis [27];
monitoring and assessment of safety critical processes [14]; and the validation of modelling
assumptions [21].
The development of inference methods for changepoint problems is by no means a
recent phenomenon, with early works including [39], [45] and [28]. Increasingly the ability
to detect changepoints quickly and accurately is of interest to a wide range of disciplines.
Recent examples of application areas include numerous bioinformatic applications [37, 15],
the detection of malware within software [51], network traﬃc analysis [35], ﬁnance [46],
climatology [32] and oceanography [34].
In this chapter we describe and compare a number of diﬀerent approaches for estimat-
ing changepoints. For a more general overview of changepoint methods, we refer interested
readers to [8] and [11]. The structure of this chapter is as follows. First we introduce the
model we focus on. We then describe methods for detecting a single changepoint and meth-
ods for detecting multiple changepoint, which will cover both frequentist and Bayesian
approaches. For multiple changepoint models the computational challenge of performing
inference is to deal with the large space of possible sets of changepoint positions. We
describe algorithms that, for the class of models we consider, can perform inference exactly
even for large datasets. In Section 10.4 we look at practical issues of implementing these
methods, and compare the diﬀerent approaches through a detailed simulation study. Our
study is based around the problem of detecting changes in the covariance structure of a time
series, and results suggest that Bayesian methods are more suitable for detection of change-
points, particularly for multiple changepoint applications. The study also demonstrates the
advantage of using exact inference methods. We end with a discussion.
10.1.1
Model and notation
Within this chapter we consider the following changepoint model. Let us assume we have
time series data, y1:n = (y1, . . . , yn). For simplicity we assume that the observation at each
time t, yt, is univariate – though extensions to multivariate data are straightforward. Our
model has a number of changepoints, m, together with their positions, τ1:m = (τ1, . . . , τm).

206
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
Each changepoint position is an integer between 1 and n −1 inclusive. We deﬁne τ0 = 0
and τm+1 = n, and assume that the changepoints are ordered so that τi < τj if and only if
i < j.
The m changepoints split the data into m+1 segments. The ith segment consists of data
yτi−1+1:τi and has associated parameters θi. We write the likelihood function as
L(m, τ1:m, θ1:m+1) = p(y1:n|m, τ1:m, θ1:m+1),
where p(·|·) denotes a general conditional density function. Finally we assume conditional
independence of data across segments, so that
p(y1:n|m, τ1:m, θ1:m+1) =
m+1
Y
i=1
p(y(τi−1+1):τi|θi).
For any segment we assume that we can calculate, either analytically or numerically, the
maximum likelihood estimator for the segment parameter. We denote this estimator by ˆθ or
ˆθi depending on the context. Thus we have
max
θ
p(y(τi−1+1):τi|θ) = p(y(τi−1+1):τi|ˆθ).
When considering this problem within a Bayesian framework, we need to introduce priors
on both the number and position of changepoints, and on the parameters for each segment.
Choice for the former will be discussed below. For the latter, we assume an exchange-
able prior structure. Thus we introduce a family of distributions p(θ|ψ), parameterised by
hyperparameters ψ. Then, conditional on ψ, we have p(θ1:m+1|ψ) = Qm+1
i=1 p(θi|ψ). Either
we specify ψ, or the model is completed through an appropriate hyperprior on ψ. Note that
the prior, p(θ|ψ), can be interpreted as describing the variability of the parameters across
segments.
For ﬁxed ψ, if we have a segment consisting of observations ys:t for s < t, then the
segment marginal likelihood is deﬁned as
Q(s, t; ψ) =
Z
p(ys:t|θ)p(θ|ψ)dθ.
(10.1)
For the Bayesian inference algorithms that we focus on, it is important that Q(s, t; ψ) can
be calculated for all s, t and ψ. For many models, this can be done analytically; whilst for
others it may be possible to calculate the marginal likelihoods numerically. In most cases,
the assumption that we can calculate Q(s, t; ψ) is equivalent to the assumption that we can
calculate the posterior distribution of the parameter associated with the segment, given the
start and end of the segment. Thus in this case, if we can calculate the posterior for the
position and number of the changepoints, then we can easily extend this to include the
segment parameters as well.
10.1.2
Example: piecewise linear regression
Assume that for each time point t we have a d-dimensional covariate zt = (z(1)
t , . . . , z(d)
t ). Our
model ﬁts a diﬀerent linear regression model within each segment. The set of parameters
for the ith segment consists of the parameters of the linear regressor and the variance of the
observations, θi = (β(1)
i , . . . , β(d)
i , σ2
i ). We assume p(y(τi−1+1):τi|θi) = Qτi
t=τi−1+1 p(yt|θi), where,
for t = τi−1 + 1, . . . , τi, Yt|θi ∼N
Pd
j=1 z(j)
t β(j)
i , σ2
i

, with N(µ, σ2) denoting a Gaussian
random variable with mean µ and variance σ2.

Analysis of changepoint models
207
(a)
(b)
(c)
(d)
Figure 10.1 Realisations of the piecewise linear regression model. (a) Change in (constant) mean. (b) Change in
variance. (c) Piecewise AR model. (d) Piecewise quadratic mean. In all cases the changepoints are at time points
100, 250 and 425. For plots (a) and (d) the underlying mean is shown.
Figure 10.1 gives example realisations from this model. Note that special cases of this
model include piecewise polynomial models, where z( j)
t
= t j−1; and, when d = 0, change-
point models for the variance of the time series. Also by letting z(j)
t
= yt−j we obtain
piecewise auto-regression models. See [41, 12] for more details of these models, and their
applications.
Conditional on knowing the segments, inference via maximum likelihood estimation
can be performed analytically.
For a Bayesian analysis, we require a prior for θi. There are computational advan-
tages in choosing the conjugate prior for this model. If we introduce hyperparameters
ψ = {a, b, η, H}, where a and b are scalars, η is a d-dimensional vector and H is a d × d
matrix, then the conjugate prior is
σ2
i |a, b ∼IG(a, b),
(β(1)
i , . . . , β(d)
i )|σ2, η, H ∼N(η, σ2H).
Here IG denotes an inverse-gamma random variable. The choice of these conjugate priors
means that, conditional on τi−1 and τi, the posterior for θi can be calculated analytically – it
is from the same inverse-gamma, multivariate normal family. Also the marginal likelihood
for a segment (Eq. (10.1)) can be calculated analytically [41].
10.2
Single changepoint models
We now describe a range of methods for detecting a single changepoint. In each case we
focus on the model introduced above and brieﬂy comment on extensions to other models.
10.2.1
Likelihood-ratio-based approach
A natural approach to detecting a single changepoint is to view it as performing a
hypothesis test. We deﬁne the null (H0) and alternative (H1) hypotheses for a change as
H0 : No changepoint, m = 0.
H1 : A single changepoint, m = 1.
We now introduce the general likelihood-ratio-based approach to test these hypothe-
ses. The potential for using a likelihood-based approach to detect changepoints was ﬁrst
proposed by [28] who derives the asymptotic distribution of the likelihood-ratio test statis-
tic for a change in the mean within a sequence of normally distributed observations. The

208
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
likelihood-based approach has also been extended to changes in mean related to other dis-
tributional forms including gamma [30], exponential [25] and binomial [29]; and also to
changes in variance within normally distributed observations by [24, 10].
Recalling our changepoint problem formulation above, we can construct a test statistic
which will decide whether a change has occurred. The likelihood-ratio method requires
calculating the maximum log-likelihood value under both null and alternative hypotheses.
For the null hypothesis the maximum log-likelihood value is log p(y1:n|ˆθ).
Under the alternative hypothesis, consider a model with a changepoint at τ, with τ ∈
{1, 2, . . . , n −1}. Then the maximum log-likelihood for a given τ (the proﬁle log-likelihood
for τ) is
Prl(τ) = log p(y1:τ|ˆθ1) + log p(y(τ+1):n|ˆθ2).
The maximum log-likelihood value under the alternative is maxτ Prl(τ). This results in the
test statistic
λ = 2

max
τ
Prl(τ) −log p(y1:n|ˆθ)

.
The test involves choosing a threshold, c, such that we reject the null hypothesis if λ > c.
If we reject the null hypothesis, which corresponds to detecting a changepoint, then we
estimate its position as ˆτ the value of τ that maximises Prl(τ).
Note that changepoint problems are not regular, so the usual asymptotic results of the
likelihood-ratio statistic do not apply. Full derivations of the asymptotic distribution for the
likelihood-ratio test of univariate and multivariate normal, gamma, binomial and poisson
distributions are provided by [11]. These can be used to give an approximate threshold for
any required signiﬁcance level.
The likelihood-ratio framework can naturally be extended to detecting changes in a
subset of the parameters; for example for the model in Section 10.1.2 we may be interested
in changes only in the regression parameters, or a speciﬁc subset of the regression param-
eters. Such problems only require a change in the calculation of the maximum likelihood
for each model, with maximisation of θ1 and θ2 being done over appropriate constraints for
the parameters.
10.2.2
Penalised likelihood approaches
The use of penalised likelihood approaches has been popular within the changepoint litera-
ture (see for example [23] or [54]). The popularity of this approach stems from parsimony
arguments. These methods more naturally extend to the multiple changepoint setting than
does the likelihood-ratio statistic approach. Below we outline a general approach for the
detection of changepoints using penalised likelihood.We begin by deﬁning the general
penalised likelihood.
Deﬁnition 10.1 Consider a model Mk, with pk parameters. Denote the parameters by Θk,
and the likelihood by L(Θk). The penalised likelihood is deﬁned to be
PL(Mk) = −2 log max L(Θk) + pkφ(n),
where φ(n) is the penalisation function, which is a non-decreasing function of the length of
the data, n.

Analysis of changepoint models
209
The value of Mk that minimises PL(Mk) is deemed the most appropriate model. Obvi-
ously the choice of model will depend on the choice of penalty function φ(n). Various
penalty functions can be considered, including Akaike’s information criterion (AIC) [1],
Schwarz information criterion (SIC) [42] and the Hannan–Quinn information criterion [26].
These criteria are deﬁned as follows:
AIC : φ(n) = 2
SIC : φ(n) = log n
Hannan–Quinn : φ(n) = 2 log log n.
Whilst the AIC is a popular penalty term, it has been shown that it asymptotically overes-
timates the correct number of parameters. On the other hand, the SIC and Hannan–Quinn
criteria both asymptotically estimate the correct number of parameters, and are therefore
generally preferred. (See [54] for details of the SIC case.)
For the changepoint problem described in Section 10.1.1, Mk corresponds to the model
with k changepoints. The associated parameter vector is Θk = (τ1:k, θ1:k+1), which has
dimension pk = k + (k + 1)dim(θ). For detecting a single changepoint the calculation of
the two penalised likelihoods corresponding to either one or no changepoint, involves a
similar likelihood maximisation step to that described in Section 10.2.1.
For estimating a single changepoint, there is a close correspondence between the
penalised likelihood and the likelihood-ratio test approaches. Both involve comparing the
maximum log-likelihood of the two models corresponding to one and no changepoint. A
changepoint is detected if the increase in log-likelihood under the one changepoint model
is greater than some threshold. The diﬀerences lie only in how this threshold is calculated.
10.2.3
Bayesian methods
To perform a Bayesian analysis we need to specify a prior probability for there being a
changepoint, Pr(M = 1), and conditional on there being a changepoint, a distribution for
its position p(τ). Note that Pr(M = 0) = 1 −Pr(M = 1).
Firstly consider the case where the hyperparameters ψ are known. In this case it is
straightforward to write down the posterior distribution in terms of marginal likelihoods,
Q(s, t), as deﬁned in Eq. (10.1). The posterior is
Pr(M = 0|y1:n) ∝Pr(M = 0)Q(1, n; ψ)
Pr(M = 1, τ|y1:n) ∝Pr(M = 1)p(τ)Q(1, τ; ψ)Q(τ + 1, n; ψ), for τ = 1, . . . , n −1.
In the case on which we focus, where the marginal likelihoods can be calculated analyti-
cally, this posterior is simple to calculate. It requires the above expressions to be evaluated
and normalised to give the posterior probabilities. This is an O(n) calculation. As men-
tioned above, in cases where we can calculate the marginal likelihood, we can normally
calculate analytically the conditional posterior for segment parameters given the start and
end of the segment. Thus we can extend the above calculation to give the joint posterior of
whether there is a changepoint, its position if there is one, and the segment parameters.
If we focus on purely detecting whether there is a changepoint, then we get
Pr(M = 1|y1:n)
Pr(M = 0|y1:n) = Pr(M = 1)
Pr(M = 0)

Pn−1
τ=1 p(τ)Q(1, τ; ψ)Q(τ + 1, n; ψ)
Q(1, n; ψ)
.
The last term on the right-hand side is called the Bayes factor. Thus the posterior ratio of
probabilities of one changepoint to no changepoint is the prior ratio multiplied by the Bayes

210
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
factor. As such the Bayes factor quantiﬁes the evidence in the data for the model with one
changepoint, as opposed to the model with no changepoint.
Note that the posterior distribution depends on ψ. In particular the choice of ψ can have
considerable eﬀect on the posterior probability for a changepoint. The reason for this is
linked to Bartlett’s paradox [5], which shows that when comparing nested models, the use
of improper priors for the parameters in the more complex model will lead to a posterior
probability of one assigned to the simpler model. Even when we do not use improper priors,
choices of ψ that correspond to vague priors for the segment parameters will tend to prefer
the simpler model, that is inferring no changepoint. We will return to this issue in the
simulation study in Section 10.4.
There are two approaches to deal with choosing ψ in the absence of prior information.
The ﬁrst is to introduce a prior on ψ and deﬁne the marginal likelihood for ψ as
ML(ψ) = Pr(M = 0)Q(1, n; ψ) +
n−1
X
τ=1
Pr(M = 1)p(τ)Q(1, τ; ψ)Q(τ + 1, n; ψ),
and let p(ψ) denote the prior. Then the marginal posterior for ψ is proportional to
p(ψ)ML(ψ), which could be explored using Markov chain Monte Carlo (MCMC). Note
that it is possible to choose an improper prior for ψ, as this is a parameter common to both
the no changepoint and one changepoint models.
Computationally simpler is to adopt an empirical Bayes approach – and use the data
to get a point estimate for ψ. For example, optimisation algorithms can be used to ﬁnd the
value of ψ that maximises ML(ψ), and then inference can be made conditional on this value
for ψ. This approach has the disadvantage of ignoring the eﬀect of uncertainty in the choice
of ψ.
We return to this issue when discussing Bayesian methods for multiple change-
point problems. Also, in Section 10.4 we look empirically at and compare the diﬀerent
approaches for dealing with no knowledge about ψ.
10.3
Multiple changepoint models
Many of the ideas for analysing single changepoint models can be adapted, at least in
theory, to the analysis of multiple changepoint models. However, the analysis of multiple
changepoint models is computationally much more challenging, as the number of possible
positions of m changepoints increases quickly with m. For example, with 1000 data points
there are just 999 possible positions of a single changepoint, but 2 × 1023 sets of possibil-
ities for 10 changepoints. Much of the focus of the following sections is on the resulting
computational challenge of detecting multiple changepoints.
We ﬁrst focus on two general search methods, which can be used to extend the
likelihood-ratio statistic approach to detecting multiple changepoints, and can be used to
eﬃciently perform the maximisation required in applying penalised likelihood methods.
We then introduce a new criterion for detecting changepoints, based on minimum descrip-
tion length, and show how the latter of these search methods can be used to ﬁnd the optimal
set of changepoints in this case. Finally we describe how to eﬃciently perform a Bayesian
analysis.
10.3.1
Binary segmentation
The binary segmentation algorithm is perhaps the most established search algorithm used
within the changepoint literature. Early applications of the binary segmentation search

Analysis of changepoint models
211
Algorithm 10.1 The generic binary segmentation algorithm to ﬁnd all possible changepoints.
Input:
A set of data of the form, (y1, y2, . . . , yn).
A test statistic Λ(·) dependent on the data.
An estimator of changepoint position ˆτ(·).
A rejection threshold C.
Initialise:
Let C = ∅, and S = {[1, n]}.
Iterate: While S , ∅
1. Choose an element of S; denote this element as [s, t].
2. If Λ(ys:t) < C, remove [s, t] from S.
3. If Λ(ys:t) ≥C then:
(a) remove [s, t] from S;
(b) calculate r = ˆτ(ys:t) + s −1, and add r to C;
(c) if r , s add [s, r] to S;
(d) if r , t −1 add [r + 1, t] to S.
Output: The set of changepoints recorded, C.
algorithm include [43] and [44]. For details on the consistency of the binary segmenta-
tion approach for estimating the true changepoint locations, τ1:m, under various conditions,
the reader is referred to the work of [49] and [48].
Binary segmentation can be used to extend any single changepoint method to multiple
changepoints. We begin by initially applying this detection method to the whole data. If
no changepoint is detected we stop, otherwise we split the data into two segments (before
and after the changepoint), and apply the detection method to each segment. If a change-
point is detected in either or both segments, we split these into further segments, and apply
the detection method to each new segment. This procedure is repeated until no further
changepoints are detected.
Generic pseudo-code for an implementation of this procedure is given in Algo-
rithm 10.1. This considers a general test statistic Λ(·), an estimator of changepoint position
ˆτ(·), and a rejection threshold C. The idea is that the test statistic is a function of data, such
as the likelihood-ratio statistic, and we detect a changepoint in data ys:t if Λ(ys:t) > C. If we
detect a changepoint, our estimate of its position, such as the maximum likelihood estimate,
is ˆτ(ys:t). Within the code, C denotes the set of detected changepoints, and S denotes a set
of segments of the data that need to be tested for a changepoint. One iteration chooses a
segment from S, and performs the test for a changepoint. For a negative result the segment
is removed from S. Otherwise a changepoint is detected and added to C, and the segment is
replaced in S by two segments deﬁned by splitting the original segment at the changepoint.
Note that in step 3(b), r is just the position of the changepoint in the original dataset, cal-
culated from ˆτ(ys:t), the position of the changepoint in the segment [s, t]. In steps 3(c) and
3(d) we only add new segments to S if they contain at least two observations: otherwise
the new segments can not contain further changepoints.
Binary segmentation is a fast algorithm that can be implemented with computational
cost O(n) where n is the length of data. However, it can be diﬃcult to choose C appro-
priately – and diﬀerent choices of C can lead to substantial diﬀerences in the estimate of
the number of changepoints. An alternative approach to detecting multiple changepoints by
recursively applying a single changepoint method is given in [31].

212
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
10.3.2
Segment neighbourhood search
References [7] and [6] consider an alternative search algorithm for changepoint detection,
namely the segment neighbourhood approach (also referred to as global segmentation). The
basic principle of this approach is to deﬁne some measure of data ﬁt, R(·), for a segment.
For inference via penalised likelihood we would set R(ys:t) to be minus the maximum log-
likelihood value for data ys:t given it comes from a single segment. That is
R(ys:t) = −log p(ys:t|ˆθ).
(10.2)
We then set a maximum number of segments, M, corresponding to at most M −1
changepoints.
The segment neighbourhood search then uses a dynamic programming algorithm to ﬁnd
the best partition of the data into m + 1 segments for m = 0, . . . , M −1. The best partition
is found by minimising the cost function Pm
i=0 R(yτi:τi+1) for a partition with changepoints at
positions τ1, τ2, . . . , τm. Thus for R(·) deﬁned in Eq. (10.2), this would give the partition of
the data with m changepoints that maximises the log-likelihood. The algorithm will output
the best partition for m = 0, . . . , M −1, and the corresponding minimum value of the cost
function, which we denote cm
1,n.
For the choice of R(·) given by Eq. (10.2), 2cm
1,n will be minus twice the log-likelihood.
So choosing m based on penalised likelihood is achieved by choosing m to minimise 2cm
1,n+
pmφ(n), where pm is the number of parameters in the model with m changepoints, and φ(n)
is the penalty function. The best partition found by the algorithm for that value of m gives
the positions of the detected changepoints.
Generic pseudo-code for this approach can be seen in Algorithm 10.2, and is based on
a dynamic programming approach described by [2]. The drawback of this approach is its
computational cost. The segment neighbourhood search is an O(n2) computation, compared
with O(n) for the binary segmentation algorithm. However this cost does result in improved
predictive performance in simulation studies [6].
Algorithm 10.2 The generic segment neighbourhood algorithm to ﬁnd up to R −1 changepoints.
Input:
A set of data of the form, (y1, y2, . . . , yn).
A measure of ﬁt R(·) dependent on the data which needs to be minimised.
An integer, M −1 specifying the maximum number of change points to ﬁnd.
Initialise:
Let n = length of data.
Calculate q1
i,j = R(yi:j) for all i, j ∈[1, n] such that i < j.
Iterate: For m = 2, . . . , M
1. Iterate step 2 for all j ∈{1, 2, . . . , n}.
2. Calculate qm
1,j = minv∈[1,j) (qm−1
1,v + q1
v+1,j).
3. Set τm,1 to be the v that minimises (qm−1
1,v + q1
v+1,n).
4. Iterate step 5 for all i ∈{2, 3, . . . , M}.
5. Let τm,i be the v that minimises (qm−i−1
1,v
+ q1
v+1,cpm,i−1).
Output: For m = 1, . . . , M: the total measure of ﬁt, qm
1,n, for m −1 changepoints and the location of the change-
points for that ﬁt, τm,1:m.

Analysis of changepoint models
213
10.3.3
Minimum description length
The authors in [12] propose the use of the minimum description length (MDL) principle to
estimate changepoints. The basic idea is that the best-ﬁtting model is the one which enables
maximum compression of the data. For a given set of changepoints we can estimate what is
called the code-length of the data. Loosely, this code-length is the amount of memory space
needed to store that data. We thus estimate the number and position of the changepoints as
the set of changepoints which have the minimum code-length. See [12] and references
therein for further background to MDL.
Our aim here is to show how ﬁnding the best set of changepoints under MDL can be
achieved using the segment neighbourhood algorithm. This guarantees ﬁnding the optimal
set of changepoints according to the MDL criterion. By comparison, [12] use a complicated
genetic algorithm to ﬁt the model.
For concreteness we will focus on the model of Section 10.1.2. In this case, up to
proportionality, the code-length for a set of m changepoints, τ1, . . . , τm, is deﬁned as
CL(m; τ1:n) = −
m+1
X
i=1
log p(y(τi−1+1):τi|ˆθi) + log(m + 1) + (m + 1) log(n) +
m+1
X
i=1
d + 1
2
log ni,
where ni = τi −τi−1 is the length of segment i, and d + 1 is the dimension of the parameter
vector associated with each segment. (See [12] for the derivation.)
Using R(ys:t) = −log p(ys:t|ˆθ) + d+1
2 log(t −s + 1), we can write the code-length as
CL(m; τ1:n) =
m+1
X
i=1
R(y(τi−1+1):τi) + log(m + 1) + (m + 1) log(n).
Thus we can use the segment neighbourhood algorithm to calculate
cm
1,n = min
τ1:m
m+1
X
i=1
R(y(τi−1+1):τi),
for m = 0, . . . , M −1. We then estimate the number of changepoints as the value m which
minimises cm
1,n + log(m + 1) + (m + 1) log(n). The segment neighbourhood algorithm also
outputs the optimal set of changepoints.
10.3.4
Bayesian methods
For a Bayesian analysis we need to specify a prior for the number and position of change-
points. There are two approaches. The ﬁrst consists in specifying a prior on the number of
changepoints, and then a prior for their position given the number of changepoints [22].
The second consists in specifying the prior for the number and position of changepoints
indirectly through a distribution for the length of each segment. The latter has computa-
tional advantages [17] and is more natural in many applications. For example, it means
that the prior does not need to be adapted based on the period of time over which the time
series is observed. It is also easier to use inferences from similar datasets, which may be of
diﬀerent length, to construct appropriate priors. We thus focus solely on this form of prior.
Formally we introduce a probability mass function, denoted g(·; ψ), to be the mass
function for the length of a segment. We allow there to be unknown parameters of this
mass function, and these will be part of the hyperparameters of the model: hence the depen-
dence on ψ. Associated with the mass function is a survivor function S (·; ψ), which satisﬁes

214
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
S (t; ψ) = P∞
i=t g(i; ψ). With this construction, the prior probability for m changepoints at
positions τ1, . . . , τm is
p(m, τ1:m|ψ) =

m
Y
i=1
g(τi −τi−1; ψ)
S (τm+1 −τm; ψ),
where as before we set τ0 = 0 and τm+1 = n. This prior corresponds to a product-partition
model [3, 4].
A common choice for the distribution of the segment lengths is the geometric dis-
tribution with parameter p. In this case g(t; ψ) = p(1 −p)t−1, S (t; ψ) = (1 −p)t−1 and
p(m, τ1:m|ψ) = pm(1−p)n−m−1. Note that this corresponds to a binomial prior on the number
of changepoints, and a conditional uniform prior on their position.
We now derive the posterior conditional on a ﬁxed value of ψ. Under the assumption
that we can calculate the segment marginal likelihoods (10.1), we can integrate out the
parameters associated with each segment to obtain the following marginal posterior for the
number and position of changepoints
p(m, τ1:m|ψ, y1:n) ∝S (τm+1 −τm; ψ)Q(τm +1, τm+1; ψ)
m
Y
i=1
g(τi −τi−1; ψ)Q(τi−1 +1, τi; ψ).
(10.3)
The normalising constant is just the marginal likelihood for ψ. As mentioned above, for
models where we can calculate the segment marginal likelihoods we can usually simulate
from the posterior distribution of the segment parameters given the changepoint positions.
Thus if we can generate samples from this posterior on the number and position of the
changepoints, it is straightforward to sample from the joint posterior of the changepoints
and the segment parameters. While MCMC [36] and reversible jump MCMC methods [22]
can be used to generate (approximate) samples from the posterior (10.3), these methods
can be computationally intensive, and lead to diﬃculties of diagnosing convergence of the
MCMC algorithm. For example the analysis of the coal-mining disaster data in [22] is
incorrect due to the MCMC algorithm not being run for long enough [17].
Instead, we describe a computationally eﬃcient algorithm that can generate iid samples
from this posterior. The algorithm has two stages. The ﬁrst is a forward pass through the
data; the second involves simulating the changepoints backwards in time. The algorithm
is thus related to the forward-backward algorithm for hidden Markov models [18] (see
Chapter 1). However the basic idea underlying this approach dates back to work by [53];
see also the methods of [3] and [38]. The version we give is suitable for online analysis of
data.
For this algorithm we introduce a variable Ct to be the position of the most recent
changepoint prior to time t. Thus Ct ∈{0, 1, . . . , t−1}, with Ct = 0 denoting no changepoint
prior to t. Note that either Ct = t −1, or Ct = Ct−1, depending on whether or not there is
a changepoint at time t −1. The forward algorithm calculates Pr(Ct = i|y1:t, ψ) for i =
0, . . . , t −1. It is based on the following recursion. For t = 2, . . . , n we have
Pr(Ct = i|y1:t, ψ) ∝Pr(Ct−1 = i|y1:t−1, ψ)
 
S (t −i; ψ)
S (t −i −1; ψ)
!  
Q(i + 1, t; ψ)
Q(i + 1, t −1; ψ)
!
, (10.4)
for i = 0, . . . , t −2; and
Pr(Ct = t −1|y1:t, ψ) ∝Q(t, t; ψ)
t−2
X
j=0
Pr(Ct−1 = j|y1:t−1, ψ)
 g(t −j −1; ψ)
S (t −j −1; ψ)
!
.
(10.5)

Analysis of changepoint models
215
Recursion (10.4) corresponds to no changepoint at time t−1, Ct = Ct−1. The ﬁnal two terms
correspond to the prior probability of this and the likelihood of the new observation given
Ct = i respectively. Recursion (10.5) corresponds to a changepoint at time t−1. In this case
Q(t, t; ψ) is the likelihood of the observation and the sum is the probability of a changepoint
at t −1 prior to observing yt. These recursions are initiated with Pr(C1 = 0|y1) = 1. For
more details of the derivation see [19]. Details of how the output from these recursions can
be used to calculate the marginal likelihood for ψ are given in [18].
The backward step generates samples from the posterior of the position and number
of changepoints. It requires that the probabilities Pr(Ct = i|y1:t) have been stored for all
t = 1, . . . , n and i = 0, . . . , t −1. To generate one sample of changepoints we ﬁrst simulate
the last changepoint from the distribution of Cn given y1:n. For a changepoint position t > 0
we can simulate the next changepoint back in time, Ct, from the conditional distribution
Pr(Ct = i|y1:n,Ct+1 = t, ψ) ∝Pr(Ct = i|y1:t, ψ)
 g(t −i; ψ)
S (t −i; ψ)
!
, for i = 1, . . . , t −1. (10.6)
(Note the event Ct+1 = t just corresponds to there being a changepoint at t.) The calculation
of this probability mass function uses the fact that conditional on a changepoint at t, the data
after this changepoint is independent of the changepoints before t. We recursively simulate
changepoints backwards in time from Eq. (10.6) until we ﬁrst simulate Ct = 0.
Full details of the forward recursion and backward simulation procedure are given in
Algorithm 10.3. In this algorithm γ(t)
i denotes Pr(Ct = i|y1:t).
The algorithm has a computational and storage cost that is quadratic in n, the number of
data points. This is because the support of Ct increases linearly with t. However, for large
t, the majority of the probabilities Pr(Ct = i|y1:t) are negligible. Hence computational and
storage savings can be made by pruning such probabilities. See [19] for a principled way
of implementing such pruning, which results in an algorithm with computational and stor-
age costs that are O(n). Pruning does introduce approximation error, but empirical results
[19] suggest that these approximations are negligible. The resulting algorithms can anal-
yse large datasets eﬃciently, see [19] and [20] for applications to genomic data. Even in
these applications, with hundreds of changepoints and n in the order of tens of thousands,
generating thousands of samples from the posterior takes a matter of seconds.
Thus we have a simple, eﬃcient and accurate method for Bayesian inference when the
hyperparameters, ψ, are known. If ψ is not known we can either introduce a prior on ψ or
estimate ψ from the data. The former is the fully Bayesian approach, but comes at a com-
putational cost. Inference will require the use of MCMC or related techniques. The above
algorithm can be used within MCMC to help mixing. However, this can be computationally
expensive as the forward recursions need to be carried out for each proposed new value
for ψ. (See [17] for suggestions on eﬃciently implementing an MCMC approach.) The
alternative is to estimate ψ from the data – for example through maximising the marginal
likelihood. Performing the maximisation is possible via a Monte Carlo expectation max-
imisation (EM) algorithm [50]. Results in [16] suggest that such an approach loses little in
terms of statistical eﬃciency, but is computationally more eﬃcient than the fully Bayesian
solution of introducing a prior on ψ.
10.4
Comparison of methods
We now compare diﬀerent changepoint methods for the problem of detecting a change
in variance. In general detecting changes in variance is more challenging than detecting

216
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
Algorithm 10.3 Algorithm for simulating from the posterior distribution of changepoint positions.
Input:
A set of data of the form, (y1, y2, . . . , yn).
A value for the hyperparameters ψ.
Survivor functions for segment lengths S (·; ψ).
A weight function W(·; ψ), such that W(ys:t; ψ) = Q(ys:t; ψ)/Q(ys:t−1; ψ) for t > s,
and W(ys; ψ) = Q(ys; ψ); where Q(·; ψ) is deﬁned in (10.1).
The number of samples from the posterior, N.
Initialise:
Let t = 2. Let γ(1)
0
= 1.
Iterate: For t = 2, . . . , n
1. For i = 0, . . . , t −2; set
γ(t)
i
= γ(t−1)
i
 
S (t −i; ψ)
S (t −i −1; ψ)
!
W(yi+1:t; ψ).
2. Set
γ(t)
t−1 = W(yt; ψ)
t−2
X
j=0
γ(t−1)
j
 S (t −j −1; ψ) −S (t −j; ψ)
S (t −j −1; ψ)
!
.
3. Normalise γ(t)s. Set A = Pt−1
i=0 γ(t)
i , and for i = 0, . . . , t −1 set γ(t)
i
= γ(t)
i /A.
Iterate: For j = 1, . . . , N
1. Simulate from the distribution with mass γ(n)
i
associated with realisation i for i = 0, . . . , n −1; denote
the realisation by t.
2. If t > 0, set Cj = {t}; otherwise Cj = ∅.
3. While t > 0 repeat steps 4 and 5.
4. Simulate from the distribution with mass proportional to
γ(t)
i
 S (t −i; ψ) −S (t −i + 1; ψ)
S (t −i; ψ)
!
,
associated with realisation i for i = 0, . . . , t −1; denote the realisation by t.
5. If t > 0, update Cj = {t, C j}.
Output: The N sets of changepoints, C1, . . . , CN.
changes in mean, and is important in applications such as ﬁnance and environmental appli-
cations [34]. As observed in [10] the detection of changes in variance has received little
attention compared to the changes in mean problem. We will look in turn at the problem of
detecting a single changepoint and multiple changepoints.
10.4.1
Single changepoint model
We ﬁrst present a simulation study which aims to compare the frequentist and Bayesian
methods for detecting a single changepoint, and to look at how speciﬁcations of the hyper-
parameter ψ can aﬀect Bayesian inference. We base our study on a speciﬁc case of the
model described in Section 10.1.2. Each data point has a normal distribution with mean 0,
but we allow for the possibility that the variance changes at a changepoint. Details of the
analytic calculations of maximum likelihood estimates, posterior distributions and marginal
likelihoods for the segments are given in the Appendix.
In particular we simulated time series consisting of 200 observations. The ﬁrst 100 data
points were iid from a standard normal distribution. The second 100 data points were iid
from a normal distribution with mean 0 and variance σ2. We considered six scenarios, each
with diﬀerent values of σ: σ2 = 1, 1.25, 1.5, 2, 3 and 4. The ﬁrst scenario corresponds to

Analysis of changepoint models
217
Figure 10.2 (a) ROC curves for the Bayesian (full-lines) and frequentist (dashed-lines) approaches. Each pair of
lines corresponds to a diﬀerent value of σ, from bottom to top: 1.25, 1.5, 2, 3 and 4. (b) Nominal false-positive
rate versus empirical false-positive rate for the likelihood-ratio method. (c) Bayes factor threshold versus empirical
false-positive rate for Bayesian method.
no changepoint, as the distribution of the data is identical for all 200 data points and is used
to estimate false-positive rates for diﬀerent methods. The remaining scenarios correspond
to increasingly large changes. We simulated 10 000 independent datasets for each scenario.
Comparison of methods
We ﬁrst looked at the performance of various methods to detect a changepoint within a
series. For detecting a changepoint, each method is based upon comparing a statistic, such
as the Bayes factor or the likelihood-ratio statistic, with a threshold value. The threshold
value aﬀects both the false-positive rate and also the proportion of true changepoints (true-
positives) detected for a given value of σ2. By varying this threshold we can plot how
the latter varies with the former, and we give the results in a so-called receiver operating
characteristic (ROC) curve. This enables us to calibrate the comparison of methods, so we
compare the true-positive rate of diﬀerent methods for a common false-positive rate.
For the Bayesian implementation, the hyperparameters ψ are the parameters of the
inverse-gamma distribution for the segment variance. Initially we set the shape parame-
ter to be 2, and the scale parameter so that the mean of the distribution was the sample
variance of the data. The ROC curve results were robust to these choices; we investigate
below the eﬀect of the choice of ψ on the performance of the Bayesian approach.
Results are shown in Fig. 10.2(a). Both the likelihood-ratio and penalised likelihood
methods (where we vary the penalty) give identical ROC curves, see Section 10.2.2, so we
plot a single curve for both these. The results show similar performance for the Bayesian
and frequentist approaches, with the Bayesian method having slightly greater power, par-
ticularly for intermediate values of σ. The intuition behind this is that for detecting change
in variance there is normally substantial uncertainty about the position of the changepoint.
The Bayes factor averages over this uncertainty, thus allowing for the accumulation of
evidence for a changepoint; whereas frequentist methods depend only on the ﬁt for the
most likely changepoint position – and as such ignore any information from other possible
changepoint locations.
Implementation of methods
The comparison above looks at the overall performance of methods via an ROC curve,
examining false and true positive rates for a range of threshold values for each method.
However, when implementing a method we need guidelines for choosing this threshold.

218
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
1e−04
1e−02
1e+00
1e+02
0.0
0.2
0.4
0.6
0.8
1.0
shape parameter
Positives
(a)
0.00
0.01
0.02
0.03
0.04
0.05
0.3
0.4
0.5
0.6
0.7
False positives
(b)
True positives
Figure 10.3 (a) Proportion of datasets with Bayes factor for no changepoint > 10, as a function of a, the shape
parameter of the inverse gamma distribution. Each line corresponds to a diﬀerent value of σ, bottom to top: 1.0,
1.25, 1.5, 2, 3 and 4. (b) ROC curve for multiple changepoint methods. Bayesian method (black full line); binary
segmentation based on likelihood-ratio test (black dotted line); binary segmentation using Bayes factor (grey
dashed line); the [31] approach for segmentation based on the likelihood-ratio test (grey full line); and penalised
likelihood (black dashed line). The square dot corresponds to MDL.
For the likelihood-ratio approach, there is clear guidance on choosing the threshold
based on asymptotic results which give nominal false-positive rates for diﬀerent threshold
values [11]. In Fig. 10.2(b) we plot empirical false-positive rates for a range of nominal
false-positive rates. For the size of data we analysed, the nominal false-positive rates over-
estimate the true false-positive rates, typically by a factor of around 2.
For comparison, we calculated the false-positive rates for the three penalised likelihood
methods introduced in Section 10.2.2. These are AIC, SIC and Hannan–Quinn. For our
data n = 200, so φ(n) = 2, 5.3 and 3.3 respectively. The false-positive rates were 70%,
4.4% and 26% in turn. This suggests that the penalty used in AIC is too small, and results
in over-detection of changepoints.
For the Bayesian approach, the test is aﬀected by (i) the prior probability of a change-
point; (ii) a threshold on the posterior probability for detecting a changepoint; and (iii)
the choice of ψ. Strictly (ii) should be speciﬁed by considering the relative cost of falsely
detecting a changepoint to missing one. The larger this is, the higher the threshold. How-
ever, in many cases it can be diﬃcult to specify this, and also often there is little prior
information to guide (i). In these cases, it is common to use general rules of thumb for the
Bayes factor [33].
In practice, the most important choice is (iii), the prior for ψ. Furthermore it can be hard
to predict the eﬀect that this choice has on the properties of the test. In particular we want
to guard against choosing values of ψ that correspond to weakly informative priors which
will lead to preference for the model for no changepoint.
To investigate the eﬀect of the choice of ψ we repeated the above analysis but for a
range of values for ψ, the parameters of the inverse gamma distribution for the variance.
In each case we chose parameters so that the mean of the inverse gamma distribution was
equal to the empirical mean and adjusted only the shape parameter, a. The choice a ≈0
corresponds to a weakly informative prior. Results are given in Fig. 10.3(a). We observe
that small and large values of a lead to the detection of a changepoint in fewer datasets. For
the Bayesian method to accurately detect changepoints we need a value of a that leads to a
prior distribution that is roughly consistent with the variation in σ across the two segments.

Analysis of changepoint models
219
As discussed in Section 10.2.3, the two approaches to choosing ψ based on the data are
to introduce a hyperprior on ψ or an empirical Bayes approach of estimating ψ by max-
imising the marginal likelihood. We tried both approaches. They provided almost identical
results, so here we give the results for the empirical Bayes approach. For a threshold value
of 10 for the Bayes factor for the model with no changepoints against the model with one,
the false-positive rate was 0.005, with, for increasing values of σ, true-positive rates of
0.02, 0.13, 0.63, 0.98 and 1.0.
How the empirical false-positive rate varies with the threshold used for the Bayes fac-
tor is shown in Fig. 10.2(c). Note that it is diﬃcult to predict the form of the relationship
beforehand. For this example, a threshold of around 2, corresponding to twice as much evi-
dence for one changepoint as opposed to no changepoints, corresponds to a false-positive
rate of 5 per cent. Note also that a threshold of 1, which corresponds to equal evidence in
the data for either one changepoint or no changepoints, has a false-positive rate much lower
than 0.5, which is what we may have predicted.
10.4.2
Multiple changepoint model
We now consider the analysis of multiple changepoint models. We aim to look at the rela-
tive performance of the diﬀerent methods and to quantify what aﬀects the power to detect
changepoints.
As in the single changepoint case, we simulated data under a model where the vari-
ance changes across segments. We simulated time series consisting of 2000 data points.
Each dataset contained 10 changepoints, which were uniformly distributed subject to the
constraint that each segment contained at least 40 observations. Within each segment, the
observations were iid draws from a normal distribution with mean 0 and common vari-
ance. The distribution of the segment variances were log-normal, and the parameters of
the log-normal distribution chosen so that 95 per cent of variances lay within the interval
[1/10, 10]. We simulated 1000 independent datasets.
The distribution of segment variances was speciﬁcally chosen to be diﬀerent from the
inverse-gamma distribution used by the Bayesian method. Also, when implementing the
Bayesian approach we assumed a geometric distribution of segment lengths and thus did
not use the information that all segments contained at least 40 observations. This avoids any
bias towards the Bayesian approach through simulating data from exactly the same class of
models that the data is analysed under.
When implementing the Bayesian method we used an empirical Bayes approach,
estimating hyperparameters based on maximising the marginal likelihood. The marginal
likelihood was maximised using a Monte Carlo EM algorithm.
Comparison of methods
Firstly we compared diﬀerent methods based on ROC curves. Making a comparison is
non-trivial as the output of Bayesian and frequentist approaches diﬀer. The former gives
posterior probabilities for changepoints at each location, while the latter returns a list of
inferred changepoint positions. The following approach was used, which gives comparison
between false and true positive rates for both methods.
For the Bayesian approach we counted a changepoint as detected if the posterior proba-
bility of a changepoint within a distance of 20 time points either side of the true position was
greater than a pre-speciﬁed threshold. For false-positives we considered non-overlapping
windows of similar size that did not contain a true changepoint. A false-positive related to

220
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
a window for which the posterior probability of a changepoint was above the threshold.
For the frequentist methods we used a similar approach. Changepoints were considered
detected if we inferred a changepoint with a distance of 20 time points of the true position.
We then considered the same non-overlapping windows which did not contain a change-
point, counting a false-positive for every window in which we inferred a changepoint.
The false-positive rate thus estimates the probability that we detect a changepoint within a
randomly chosen window that contains no changepoint.
Results are given in Fig. 10.3(b). We compared the Bayesian approach with a num-
ber of frequentist methods. The latter included penalised likelihood and MDL using the
segment neighbourhood algorithm, and binary segmentation using the likelihood-ratio test.
We also implemented binary segmentation with a test based on Bayes factors [52], and the
alternative segmentation strategy of [31], implemented with the likelihood-ratio test.
There are a number of features of the results that stand out. Firstly, the uniformly most
powerful approach is the full-Bayesian method. This approach performs particularly well
for small false-positive rates. Secondly, jointly estimating the changepoints, as in the full-
Bayesian method or the penalised likelihood approach, performs better than recursively
applying single changepoint detection methods using binary segmentation or the approach
of [31]. This supports the results of [6].
Thirdly, of the two approaches for recursively applying single changepoint methods,
that of [31] performed better than binary segmentation. This is perhaps a little surprising, as
this method is used much less in the literature. Finally we notice that although the Bayesian
method performed better in the single changepoint simulation study, there is very little
diﬀerence between the binary segmentation approach that used likelihood ratio and the one
that used Bayes factors.
While most approaches can be implemented to give ROC curves, MDL results in a
single pair of false-positive and false-negative rates. This pair lies on the penalised likeli-
hood line, and corresponds very closely to the results for penalised likelihood using SIC.
Intuitively, this similarity is not surprising as the minimisation criteria for MDL and SIC
are very similar (see Sections 10.2.2 and 10.3.3). We also note that using the AIC criteria
performed very poorly, detecting over 50 changepoints for each dataset. This suggests that
the AIC penalty is not large enough.
Factors aﬀecting power
Finally we investigated which factors aﬀect the ability to detect a changepoint. We consid-
ered two possible factors, ﬁrstly the change in variance and secondly the size of segments
either side of the changepoint. Not surprisingly, the former has an important eﬀect on the
ability to detect changepoints. In Fig. 10.4(a) we plot, for each changepoint, the posterior
probability of a changepoint against the factor by which the variance changes across the
changepoint. The former was again calculated by looking for a changepoint within a win-
dow which contains all locations at a distance of 20 or less from the changepoint position.
A change in variance by a factor of 2 has an average posterior probability of about 0.5,
while a change by a factor of 5 or more results in posterior probabilities that are very close
to 1.
We then compared power at detecting a changepoint against change in variance. To
make the comparison fair, for the Bayesian approach we detected a changepoint if the pos-
terior probability within a window was greater than a threshold. Results for the Bayesian
method, MDL and binary segmentation using the likelihood-ratio test are compared in
Fig. 10.4(b). The threshold for the Bayesian approach and for the likelihood-ratio test

Analysis of changepoint models
221
Factor
Posterior Prob
(a)
Factor
Detection Probability
(b)
Figure 10.4 (a) Plot of posterior probability of a changepoint against the factor by which the variance changes
across the changepoint for each changepoint. A smoothed estimate is given by the line. (b) Plot of power of
detecting a changepoint against the factor by which variance changes: Bayesian approach (black full line); MDL
(grey dashed line); and binary segmentation with likelihood-ratio test (black dashed line).
were chosen so that both methods had similar false-positive rates to MDL. The Bayesian
approach and MDL have similar power curves, but the Bayesian method does better at
detecting changepoints when the variance changes by a factor of between 2 and 5. The
binary segmentation approach does substantially worse than the other two methods for
changepoints across which the variance changes by a factor of 3 or more.
The size of segment had little eﬀect on the probability of detection of a changepoint.
Correlation of segment sizes against posterior probability of a changepoint was around
5 per cent. Similarly small correlation between segment size and detection of changepoints
was found for the non-Bayesian methods.
10.5
Conclusion
We have reviewed a number of ways of detecting changepoints, comparing their perfor-
mance on detecting changes in variance in a time series. Changepoint models is a large
area of research and we have not been able to cover all methods for their analysis. Exam-
ples of alternative approaches include nonparametric methods [39, 40] and methods for
online detection based on decision theory [47, 13].
The simulation results suggest that Bayesian methods are the most suitable for this
application. One aspect of a Bayesian analysis that we have not reﬂected on is that the
output is a distribution over the number and position of the changepoints. Thus Bayesian
methods have the advantage of more easily quantifying uncertainty in changepoint posi-
tions than alternative methods. Furthermore, if interest lies in estimating the underlying
segment parameters (e.g. how the variance changes over time), a Bayesian approach natu-
rally enables the uncertainty in the changepoints to be taken into account. One disadvantage
is that it is harder to summarise or represent the posterior distribution, as compared to
methods which output a set of predicted changepoints. One approach is to calculate the
most likely (so-called MAP) set of changepoints, which can often be calculated eﬃ-
ciently [9, 16]. However even here there are alternative ways of deﬁning the MAP set
of changepoints which can give diﬀerent results in practice [16].

222
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
The main issue when implementing a Bayesian analysis is the choice of priors. For
the models we consider here a computationally convenient, yet accurate, approach is to
estimate hyperparameters of the prior distributions by maximising the marginal likelihood.
This approach appears particularly suitable to multiple changepoint models where there
can be substantial information about the hyperparameters due to the variation in parameters
across the multiple segments.
When analysing multiple changepoint models, there are computational considerations
related to searching for the best set of changepoints or exploring the posterior distribu-
tion. For the class of models we focused on, both can be done exactly using either the
segment neighbourhood algorithm of Section 10.3.2 or the forward-backward algorithm
of Section 10.3.4. Simulation results suggest that using these approaches results in better
detection of changepoints than using approximate methods such as binary segmentation.
Whilst a complicated genetic algorithm is used to detect changepoints using MDL in [12],
we showed that the segment neighbourhood algorithm can be applied in this case. One
disadvantage of both the segment neighbourhood algorithm and the forward-backward
algorithm is that their computational cost is O(n2). Approximations to the latter have been
suggested in [19], which results in an accurate algorithm whose cost is O(n). One proﬁtable
area of future research would be to construct a similar approximate version of the segment
neighbourhood algorithm with O(n) computational cost. This is particularly important for
applying this approach to analysing large datasets, such as those currently being analysed
in bioinformatics.
A further disadvantage of these two algorithms is that they rely on nice properties of
the model. Changepoint models which have strong dependence across segments cannot
be analysed by either of these two algorithms. In this case alternatives, such as binary
segmentation, MCMC or genetic algorithms would need to be used to ﬁt models. However,
our recommendation is that for models with the appropriate independence properties these
two approaches should be the method of choice for ﬁtting changepoint models.
Acknowledgments
Rebecca Killick is funded by the EPSRC and Shell Research Ltd.
10.A
Appendix: segment parameter estimation
Here we give details for estimating segment parameters, conditional on the start and end of
the segment, for change in variance model used in the simulation study.
Assume throughout that the segment consists of observations ys:t = (ys, . . . , yt), for
t > s. There is a single segment parameter, the variance, which we denote by σ2. The
model assumes that within the segment we have conditionally independent observations
with yi|σ2 ∼N(0, σ2), for i = s, . . . , t. The maximum likelihood estimator of the param-
eter is ˆσ2 =
1
t−s+1
Pt
i=s y2
i . The resulting maximum log-likelihood value is p(ys:t|ˆθ) =
−n
2
n
log(2π) −log ˆσ2 −1
o
.
For the Bayesian analysis, we have an inverse-gamma prior for σ2 with hyperparame-
ters ψ = (a, b). The posterior distribution is
σ2|ys:t ∼IG
a + (t −s + 1)
2
, b + 1
2
tX
i=s
y2
i
,
with marginal likelihood
Q(s, t; ψ) = (2π)(t−s+1)/2
Γ(a + (t −s + 1)/2)ba
Γ(a)(b + 1
2
Pt
i=s y2
i )a+(t−s+1)/2) .

Analysis of changepoint models
223
Bibliography
[1] H. Akaike. A new look at the statistical model
identiﬁcation. IEEE Transactions on Automatic
Control, 19(6):716 – 723, 1974.
[2] I. E. Auger and C. E. Lawrence. Algorithms for
the optimal identiﬁcation of segment
neighborhoods. Bulletin of Mathematical
Biology, 51(1):39–54, 1989.
[3] D. Barry and J. A. Hartigan. Product partition
models for change point problems. Annals of
Statistics, 20:260–279, 1992.
[4] D. Barry and J. A. Hartigan. A Bayesian analysis
for change point problems. Journal of the
American Statistical Association, 88:309–319,
1993.
[5] M. S. Bartlett. A comment on D.V. Lindley’s
statistical paradox. Biometrika, 44:533–534,
1957.
[6] J. V. Braun, R. K. Braun and H. G. Muller.
Multiple changepoint ﬁtting via quasilikelihood,
with application to DNA sequence segmentation.
Biometrika, 87:301–314, 2000.
[7] J. V. Braun and H. G. Muller. Statistical methods
for DNA sequence segmentation. Statistical
Science, 13(2):142–162, 1998.
[8] E. Carlstein, H. G. Muller and D. Siegmund,
editors. Change-point problems. Institute of
Mathematical Statistics Lecture Notes, 1994.
[9] T. Cemgil, H. J. Kappen and D. Barber. A
generative model for music transcription. IEEE
Transactions on Audio, Speech and Language
Processing, 14:679–694, 2006.
[10] J. Chen and A. K. Gupta. Testing and locating
variance changepoints with application to stock
prices. Journal of the American Statistical
Association, 92:739 – 747, 1997.
[11] J. Chen and A. K. Gupta. Parametric Statistical
Change Point Analysis. Birkhauser, 2000.
[12] R. A Davis, T. C. M Lee and G. A.
Rodriguez-Yam. Structural break estimation for
nonstationary time series models. Journal of the
American Statistical Association,
101(473):223–239, 2006.
[13] S. Dayanik, C. Goulding and H. V. Poor.
Bayesian sequential change diagnosis.
Mathematics of Operations Research,
33:475–496, 2008.
[14] J. B. Elsner, F. N. Xu and T. H. Jagger. Detecting
shifts in hurricane rates using a Markov chain
Monte Carlo approach. Journal of Climate,
17:2652–2666, 2004.
[15] C. Erdman and J. W. Emerson. A fast Bayesian
change point analysis for the segmentation of
microarray data. Bioinformatics,
24(19):2143–2148, 2008.
[16] P. Fearnhead. Exact Bayesian curve ﬁtting and
signal segmentation. IEEE Transactions on
Signal Processing, 53:2160–2166, 2005.
[17] P. Fearnhead. Exact and eﬃcient Bayesian
inference for multiple changepoint problems.
Statistics and Computing, 16:203–213, 2006.
[18] P. Fearnhead. Computational methods for
complex stochastic systems: A review of some
alternatives to MCMC. Statistics and
Computing, 18:151–171, 2008.
[19] P. Fearnhead and Z. Liu. Online inference for
multiple changepoint problems. Journal of the
Royal Statistical Society Series B, 69:589–605,
2007.
[20] P. Fearnhead and D. Vasilieou. Bayesian analysis
of isochores. Journal of the American Statistical
Association, 485:132–141, 2009.
[21] P. Fryzlewicz and S. Subba Rao. Basta:
consistent multiscale multiple change-point
detection for piecewise-stationary ARCH
processes. (In submission), 2009.
[22] P. J. Green. Reversible jump Markov chain
Monte Carlo computation and Bayesian model
determination. Biometrika, 82:711–732, 1995.
[23] A. K. Gupta and J. Chen. Detecting changes of
mean in multidimensional normal sequences
with applications to literature and geology.
Computational Statistics, 11:211–221, 1996.
[24] A. K. Gupta and J. Tang. On testing
homogeneity of variances for Gaussian models.
Journal of Statistical Computation and
Simulation, 27:155–173, 1987.
[25] P. Haccou, E. Meelis and S. Geer. The likelihood
ratio test for the change point problem for
exponentially distributed random variables.
Stochastic Processes and Their Applications,
27:121–139, 1988.
[26] E. J. Hannan and B. G. Quinn. The
determination of the order of an autoregression.
Journal of the Royal Statistical Society, Series B,
41(2):190–195, 1979.
[27] R. Henderson and J. N. S. Matthews. An
investigation of changepoints in the annual
number of cases of haemolytic uraemic
syndrome. Applied Statistics, 42:461–471, 1993.
[28] D. V. Hinkley. Inference about the change-point
in a sequence of random variables. Biometrika,
57:1–17, 1970.
[29] D. V. Hinkley and E. A. Hinkley. Inference about
the change-point in a sequence of binomial
random variables. Biometrika, 57:477–488,
1970.
[30] D. A. Hsu. Detecting shifts of parameter in
gamma sequences with applications to stock
price and air traﬃc ﬂow analysis. Journal of the
American Statistical Association, 74:31–40,
1979.

224
Idris A. Eckley, Paul Fearnhead and Rebecca Killick
[31] C. Inclan and G. C. Tiao. Use of cumulative
sums of squares for retrospective detection of
changes of variance. Journal of the American
Statistical Association, 89(427):913–923, 1994.
[32] J. Reeves, J. Chen, X. L. Wang, R. Lund and
Q. Lu. A review and comparison of changepoint
detection techniques for climate data. Journal of
Applied Meteorology and Climatology,
6:900–915, 2007.
[33] H. Jeﬀreys. The Theory of Probability. Oxford
University Press, 1961.
[34] R. Killick, I. A. Eckley, K. Ewans and
P. Jonathan. Detection of changes in variance of
oceanographic time-series using changepoint
analysis. Ocean Engineering, 37:1120–1126,
2010.
[35] D. W. Kwon, K. Ko, M. Vannucci, A. L. N.
Reddy and S. Kim. Wavelet methods for the
detection of anomalies and their application to
network traﬃc analysis. Quality and Reliability
Engineering International, 22:953–969, 2006.
[36] M. Lavielle and E. Lebarbier. An application of
MCMC methods for the multiple change-points
problem. Signal Processing, 81:39–53, 2001.
[37] P. Lio and M. Vannucci. Wavelet change-point
prediction of transmembrane proteins.
Bioinformatics, 16(4):376–382, 2000.
[38] J. S. Liu and C. E. Lawrence. Bayesian inference
on biopolymer models. Bioinformatics,
15:38–52, 1999.
[39] E. S. Page. Continuous inspection schemes.
Biometrika, 41:100–115, 1954.
[40] A. N. Pettitt. A non-parametric approach to the
change-point problem. Applied Statistics,
28:126–135, 1979.
[41] E. Punskaya, C. Andrieu, A. Doucet and W. J.
Fitzgerald. Bayesian curve ﬁtting using MCMC
with applications to signal segmentation. IEEE
Transactions on Signal Processing, 50:747–758,
2002.
[42] G. Schwarz. Estimating the dimension of a
model. Annals of Statistics, 6:461–464, 1978.
[43] A. J. Scott and M. Knott. A cluster analysis
method for grouping means in the analysis of
variance. Biometrics, 30(3):507–512, 1974.
[44] A. Sen and M. S. Srivastava. On tests for
detecting change in mean. Annals of Statistics,
3(1):98–108, 1975.
[45] A. N. Shiryaev. On optimum methods in quickest
detection problems. Theory of Probability and its
Applications, 8:26–51, 1963.
[46] V. Spokoiny. Multiscale local change point
detection with applications to value-at-risk.
Annals of Statistics, 37:1405–1436, 2009.
[47] A. G. Tartakovsky and V. V. Veeravalli. General
asymptotic Bayesian theory of quickest change
detection. Theory of Probability and Its
Applications, 49:458–497, 2004.
[48] E. S. Venkatraman. Consistency results in
multiple change-point problems. PhD thesis,
Stanford University, 1993.
[49] L. J. Vostrikova. Detecting disorder in
multidimensional random processes. Soviet
Mathematics Doklady, 24:55–59, 1981.
[50] G. C. G. Wei and M. A. Tanner. A Monte Carlo
implementation of the EM algorithm and the
poor man’s data augmentation algorithms.
Journal of the American Statistical Association,
85(411):699–704, 1990.
[51] G. Yan, Z. Xiao and S. Eidenbenz. Catching
instant messaging worms with change-point
detection techniques. In Proceedings of the
USENIX workshop on large-scale exploits and
emergent threats, 2008.
[52] T. Y. Yang and L. Kuo. Bayesian binary
segmentation procedure for a Poisson process
with multiple changepoints. Journal of
Computational and Graphical Statistics,
10:772–785, 2001.
[53] Y. Yao. Estimation of a noisy discrete-time step
function: Bayes and empirical Bayes approaches.
Annals of Statistics, 12:1434–1447, 1984.
[54] Y. Yao. Estimating the number of change-points
via Schwarz’s criterion. Statistics and
Probability Letters, 6:181–189, 1988.
Contributors
Idris A. Eckley, Department of Mathematics and Statistics, Lancaster University, LA1 4YF
Paul Fearnhead, Department of Mathematics and Statistics, Lancaster University, LA1 4YF
Rebecca Killick, Department of Mathematics and Statistics, Lancaster University, LA1 4YF

11
Approximate likelihood estimation of static parameters
in multi-target models
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
11.1
Introduction
Target-tracking problems involve the online estimation of the state vector of an object under
surveillance, called a target, that is changing over time. The state of the target at time n,
denoted Xn, is a vector in E1 ⊂Rd1 and contains its kinematic characteristics, e.g. the
target’s position and velocity. Typically only noise-corrupted measurements of the state of
the object under surveillance are available. Speciﬁcally, the observation at time n, denoted
Yn, is a vector in E2 ⊂Rd2 and is a noisy measurement of the target’s state as acquired by
a sensor, e.g. radar. The statistical model most commonly used for the sequence of random
variables {(Xn, Yn+1)}n≥0 is the hidden Markov model (HMM):
X0 ∼µθ (·) ,
Xn|(Xn−1 = xn−1) ∼f θ(·|xn−1),
n ≥1,
(11.1)
Yn|Xn = xn ∼gθ(·|xn),
n ≥1.
(11.2)
The superscript θ on these densities (as well as on all densities introduced subsequently),
denotes the dependency of the model on a vector of parameters θ. We will assume a param-
eterisation such that θ ∈Θ ⊂Rnθ. When the target ﬁrst appears in the surveillance region,
its initial state is distributed according to the probability density µθ on E1. The change in its
state vector from time n −1 to n is determined by the Markov transition density f θ(·|xn−1).
Furthermore, the observation generated at time n is a function of the target’s state at time
n and noise, or equivalently generated according to the probability density gθ(·|xn) on E2,
and is conditionally independent of previously generated observations and state values.
This model is general enough to describe the evolution of the target and the observations it
generates in many applications; see [1, 13].
This chapter is concerned with the more complex and practically signiﬁcant problem of
tracking multiple targets simultaneously. In this case the state and observation at each time
are random ﬁnite sets ([13]):
Xn = {Xn,1, Xn,2, . . . , Xn,Kn},
Yn = {Yn,1, Yn,2, . . . , Yn,Mn},
n ≥1.
(11.3)
Each element of Xn is the state of an individual target. The number of targets Kn under
surveillance changes over time due to targets entering and leaving the surveillance region.
Some of the existing targets may not be detected by the sensor and a set of false

226
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
measurements of unknown number are also recorded due to non-target generated measure-
ments. For example, if the sensor is radar, reﬂections can be generated by ﬁxed features of
the landscape. These processes give rise to the measurement set Yn. (Note its cardinality
Mn changes with time.) An added complication usually encountered in applications is that
it is not known which observations arise from which targets (if any). The aim in multi-target
tracking is to estimate, at each time step, the time-varying state set from the entire history
of observation sets received until that time. The task of calibrating the multi-target tracking
model is also an important problem faced by the practitioner. In the multi-target model θ
includes both the parameters of the individual target model (11.1)–(11.2) and parameters
related to the surveillance environment. For example, θ may contain the variance of the
noise that corrupts the sensor measurements, the parameter of the distribution of false mea-
surements, etc. In this chapter, in order to estimate the model parameters from the data,
an approximate likelihood function is devised and then maximised. Before describing this
method, it is necessary to specify the multi-target tracking problem a little more precisely.
The state Xn evolves to Xn+1 in a Markovian fashion by a process of thinning (targets
leaving the surveillance region), displacement (Markov motion of remaining individual
targets) and augmentation of new points which correspond to new targets entering the
surveillance region. The motion of each target that has not left the surveillance region
occurs precisely according to Eq. (11.1). When a new target is introduced, its initial state is
drawn according to the probability density µθ in Eq. (11.1). If more than one new target is
introduced, then the initial states are sampled from µθ independently. The observed process
is generated from the hidden process through the same mechanisms of thinning, displace-
ment and augmentation with false measurements. (See Section 11.2 for more details.)
Displacement here implies that the individual targets, if they generate an observation at
time n, do so in accordance with the stated model in Eq. (11.2). Mathematically Xn is a
spatial point process (PP) on E1 where E1 is the state space of a single target. Likewise,
Yn is a spatial PP on E2 where E2 is the observation space in the single target-tracking
problem. False measurements and birth of new targets are, for example, assumed to be
independent spatial Poisson processes. Let y1:n = (y1, . . . , yn) denote the realisation of
observations received from time 1 to n. (Here yi denotes the realisation of Yi.) It is possi-
ble to estimate the number of targets and their individual state values from the conditional
distribution of Xn given Y1:n = y1:n, denoted p(xn|y1:n). Due to the need to process the data
online, i.e. to update the estimates every time a new observation is received, the sequence
of marginal distributions {p(xn|y1:n)}n≥0 is often sought after. For each n, p(xn|y1:n) has
support on the disjoint union U
k≥0 Ek
1 and does not admit an analytic characterisation in
general.
Multi-target tracking has long been a focus of research in the engineering literature,
primarily driven by surveillance applications, and now a standard set of tools exists for the
analysis of such problems; see for example [2, 13]. It is possible to enlarge the model (11.3)
to include the (unobserved) associations of the observations to hidden targets. For simple
sensor and individual target motion models, the posterior distribution of the unobserved
targets and associations admits an analytic characterisation. Furthermore, this posterior
distribution can be marginalised to obtain a posterior distribution over the associations
only. Even so, the support of this marginal distribution is too large for it to be stored
in practice and approximations are made. The most frequently used approach to date (in
the surveillance literature) is to approximate the posterior by retaining only its dominant
modes. A popular sub-optimal search algorithm to locate the modes is the multiple hypoth-
esis tracking (MHT) algorithm of [19]; see also [2, 13]. As more data are gathered over

Estimation of static parameters in multi-target models
227
time, which is characteristic of surveillance, the dimension of the support of the pos-
terior distribution of associations increases and searching for the modes exhaustively is
not possible. There is a large volume of work dedicated to the computational challenges
of this task, i.e., how to implement the search sequentially in time and direct it towards
‘good’ candidates, and how to store the result eﬃciently; see [2]. It is fair to say that the
MHT is complicated to implement, in fact, far more complicated than the algorithms in
this work.
Recently, the MHT algorithm has been extended by [24] to simultaneously estimate the
parameters of the multi-target model. A full Bayesian approach for estimating the model
parameters using Markov chain Monte Carlo was presented in [29] for a simpliﬁed model
which assumes the individual targets have linear Gaussian dynamics and similar Gaus-
sian assumptions hold for the observations they generate. This Gaussian scenario is highly
restrictive and cannot handle non-linear sensors, e.g. bearings measurements.
We now introduce the speciﬁc technique that we use to construct an approximation of
the marginal likelihood of observations. When the unknown number of targets and their
states at time 0 is considered to be a realisation of a Poisson PP, then it follows that for n =
1, 2, ..., the law of Xn is also Poisson (see Section 11.2 for a precise statement of this result
including the modelling assumptions). The problem of estimating the number of targets and
their individual state values at time n given the observations Y1:n is then greatly simpliﬁed
if Xn given Y1:n can be closely approximated as a Poisson PP. In the tracking literature
this problem was studied by [14]. Mahler derived an expression relating the intensity (or
the ﬁrst moment) of the conditional distribution of X1 given y1 to that of the prior of X1.
The Poisson PP is completely characterised by its ﬁrst moment and it can be shown that
the problem of ﬁnding the best Poisson approximation to the conditional distribution of X1
given y1 is equivalent to the problem of characterising its intensity; see [14]. In addition,
for the same hidden process dynamic model stated above, Mahler also derived the intensity
of the conditional distribution of X2 given y1. These results were combined to yield a ﬁlter
that propagates the intensity of the sequence of conditional densities {p(xn|y1:n)}n≥0 and is
known in the tracking literature as the probability hypothesis density (PHD) ﬁlter. Detailed
numerical studies using sequential Monte Carlo (SMC) approximations by [26, 27] (and
references therein), as well as Gaussian approximations by [25] to the PHD ﬁlter have since
demonstrated its potential as a new approach to multi-target tracking. We are not aware of
any study that has speciﬁcally characterised the error incurred in approximating Xn given
y1:n with a Poisson PP. However, in various studies, the merit of the approximation has been
conﬁrmed by comparing the estimated intensity with the true number of targets in synthetic
numerical examples. In particular, the estimates of the number of targets and their states
extracted from the propagated intensity are reasonably accurate even for diﬃcult tracking
scenarios. Recent non-target tracking applications of the PHD ﬁlter include map building
in robotics by [15] and tracking sinusoidal components in audio by [4].
Motivated by this, we explore the use of the same Poisson approximation technique to
derive an approximation of the marginal likelihood of the observed data, i.e. p(y1:T) where
T is the length of the data record. The estimate of the model parameter is then taken to be
the maximising argument (with respect to the model parameters) of this approximation of
the true marginal likelihood. A gradient ascent algorithm is used to ﬁnd the model param-
eters that maximise the likelihood. Although the approximate likelihood function is not
computable exactly, it and its gradient may be evaluated using SMC. The approximate like-
lihood function is ‘characterised’ by a sequence of non-negative functions on E1, and not
the space U
k≥0 Ek
1, and even a simple SMC implementation can be reasonably eﬃcient. (See
Section 11.4 for details.) We demonstrate in numerical examples that the approximation to

228
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
the true likelihood is reasonable as it allows us to learn the static parameters from the
data. Even for initialisation values very far from the true model parameters, the gradient
algorithm is able to converge to a vicinity of the true model parameters.
The remainder of this chapter is structured as follows. The multi-target statistical model
is deﬁned in Section 11.2. A review of the PHD ﬁlter is presented in Section 11.3 along with
several results from [21] which are needed to construct the approximation of the likelihood.
The approximation of the marginal likelihood of the multi-target tracking model is detailed
in Section 11.4. Section 11.5 describes a SMC algorithm to evaluate this approximate like-
lihood and its gradient with respect to the model parameters. Section 10.6 describes model
parameter estimaton. A simulation study which empirically assesses the performance of
the method is presented in Section 11.7.
11.2
The multi-target model
The Poisson PP features prominently in this model and we refer the reader to [10] for
an introduction to this process. To simplify the notation, only the core ingredients of the
multi-target statistical model have their dependence on θ made explicit. All other derived
quantities, while also dependent on θ, have θ omitted from their symbols.
Consider the process of unobserved points Xn = Xn,1:Kn, where each element of Xn
corresponds to one target and is a random point in the space E1. Xn evolves to Xn+1 as
follows. With probability pθ
S (x), each point of Xn survives and is displaced according to
the Markov transition density on E1, f θ(xn+1|xn), introduced in Eq. (11.1). The random
deletion and Markov motion happens independently for each point in Xn. In addition to
the surviving points of Xn, new points are ‘born’ from a Poisson process with intensity
function γθ(x). Let Xn+1 denote the PP on E1 deﬁned by the superposition of the surviving
and mutated points of Xn and the newly born points. At initialisation, X0 consists only of
‘birth’ points. Simulation from this Poisson model can be achieved by ﬁrst sampling the
cardinality according to the discrete Poisson distribution with parameter value equal to the
total mass of the intensity function. The location of the points themselves are then sampled
iid from the normalised intensity function. In the context of the model (11.1), the initial
state of each new target is drawn from the probability density γθ(x)/
R
γθ(x′)dx′.
The points of Xn+1 are observed through the following model. With probability pθ
D(x),
each point of Xn+1, e.g. xn+1, j, j ∈{1, 2, ..., Kn+1}, generates a noisy observation in the
observation space E2 through the density gθ(y|xn+1,j). This happens independently for each
point of Xn+1. Let bYn+1 denote the PP of observations originating from Xn+1. In addition to
these detected points, false measurements (or clutter points) are generated from an indepen-
dent Poisson process on E2 with intensity function κθ(y). Let Yn+1 denote the superposition
of bYn+1 and these false measurements, and let yn+1 = yn+1,1:mn+1 denote a realisation of
Yn+1 = Yn+1,1:Mn+1.
11.3
A review of the PHD ﬁlter
This section presents an alternative derivation of the PHD ﬁlter which was proposed by
[21]. The foundation of the PHD ﬁlter is a solution to a simpliﬁed inference task, which
is to characterise the posterior distribution of a hidden Poisson PP X given observations Y
generated as described in Section 11.2. We then introduce explicit time indexing and make
connections to the PHD ﬁltering recursions of [14].

Estimation of static parameters in multi-target models
229
11.3.1
Inference for partially observed Poisson processes
In this subsection we suppress dependence on the parameter θ. Let the realisation of Y be
y = {y1, . . . , ym}. In general it is not possible to characterise the distribution of X given
Y = y, denoted PX|y, in closed form; see [12] for a similar problem solved with perfect
simulation. In the case of the Poisson prior, the posterior was characterised only indirectly
by [14] by providing the formula for its probability generating functional (p.g.ﬂ.), obtained
by diﬀerentiating the joint p.g.ﬂ. of the observed and hidden process. The authors in [21]
noted that, while this is a general proof technique, it is a technical approach that does not
exploit the structure of the problem – a Poisson prior and an observed process constructed
via thinning, displacement and augmentation allows for a considerably stronger result with
a simpler proof by calling upon several well-known results concerning the Poisson PP.
Exploiting this [21] were able to provide a closed-form expression for the posterior which
is quite revealing of the ‘structure’ of the conditional process X given the observed process
Y. Corollaries of this result include the expression relating the intensity of the posterior and
prior as well as the law of the association of the points of the observed process. While the
result in [14] is only for a Poisson prior for X, [21] extends the result to a Gauss-Poisson
prior which covers the Poisson prior as a special case. The law PX|y is the foundation of the
PHD ﬁlter and its derivation presented below follows the approach of [21].
The derivation of PX|y draws upon several facts concerning a Poisson PP. The ﬁrst con-
cerns marking. Let X be a Poisson PP on E1 with realisation {x1, . . . , xn}. Let’s attach to
each xi a random mark ζi, valued in M (the mark space) and drawn from the probability
density p(·|xi). Additionally, the mark of each point xi is generated independently. Then
{(x1, ζ1), . . . , (xn, ζn)} is Poisson on E1 × M with intensity α(x)p(ζ|x)dxdζ ([10]). Con-
versely, for a Poisson PP on E1 × M with intensity v(x, ζ), given the realisation of all the
ﬁrst coordinates, {x1, x2, . . . , xn}, then the following is known about the same PP restricted
to M. There are n points and they are jointly distributed according to the following density
on Mn
p(ζ1, . . . , ζn) =
n
Y
i=1
v(xi, ζi)
R
v(xi, ζ)dζ
.
(11.4)
According to the multi-target observation model, each point xi in the realisation of
X generates an E2-valued observation with probability pD(xi). Furthermore, this happens
independently for all points in the realisation of X. At this point it is convenient to introduce
the following decomposition of X. Two point processes, bX and eX, are formed from X: bX
comprises the points of X that generate observations while eX comprises the remaining
unobserved points of X. Since bX is obtained from X by independent marking, both bX and
eX are independent Poisson with respective intensities α(x)pD(x) and α(x)(1 −pD(x)) [10,
p. 55]. (The superscript θ on pD has been omitted from the notation.)
By construction, eX is unobserved while bX is observed in noise through Y, with noise
here referring to the false measurements in Y. This decomposition sheds light on the struc-
ture of the posterior: since eX is unobserved, its law is unchanged after observing Y. As for
bX, let its posterior be PbX|y. Thus, the desired posterior PX|y is
PX|y = PeX ∗PbX|y,
where ∗denotes convolution, which follows since X is the superposition of eX and bX. All
that remains to be done is to characterise PbX|y.
Let {∆} be a one point set with ∆not belonging to either E1 or E2 and let E′
1 = E1∪
{∆}. A marked PP Z on E2 × E′
1 is constructed as follows. Each false measurement in Y is

230
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
assigned ∆as its mark. Each point in Y corresponding to a real observation is assigned as
a mark the corresponding point in bX that generated it. Let Z be this set of points formed by
marking Y. It follows that Z is a marked Poisson PP with intensity
α(x)pD(x)g(y|x)IE2×E1(y, x)dxdy + κ(y)IE2×{∆}(y, x)δ∆(dx)dy,
where IA is the indicator function of the set A and δ∆(dx) is the Dirac measure concentrated
on ∆. Given the realisation y = {y1, . . . , ym} of the ﬁrst coordinate of the process then, by
Eq. (11.4), the second coordinates are jointly distributed on (E1 ∪{∆})m with law
p(dx1, . . . , dxm) =
m
Y
i=1
α(xi)pD(xi)g(yi|xi)IE1(xi)dxi + κ(yi)I{∆}(x)δ∆(dxi)
R
E1 α(x)pD(x)g(yi|x)dx + κ(yi)
.
(11.5)
Theorem 11.1
[21, Proposition 4.1] Let X be a Poisson PP on E1 with intensity α(x)
which is observed indirectly through the PP Y on E2 and Y is generated according to the
observation model detailed in Section 11.2. The conditional distribution of X given the
realisation y = {y1, . . . , ym} of Y, PX|y, coincides with the distribution of the superposition
of the following two independent point processes:
– a Poisson PP on E1 with intensity α(x)(1 −pD(x))dx and
– the restriction to E1 of an m-point PP on E1 ∪{∆} with law given in Eq. (11.5).
The theorem may be alternatively interpreted as follows. To generate a realisation with
distribution PX|y, the following procedure may be adopted. Generate a realisation of the
m-point PP on E1 ∪{∆} with law (11.5) by simulating the ith point according to the ith
measure in the product (11.5). Discard all the points with values ∆and augment this set of
remaining points with the realisation of an independent Poisson PP with intensity α(x)(1 −
pD(x)).
Since PX|y is the law of the superposition of two independent point processes, the
following corollary is obvious.
Corollary 11.2
[21, Proposition 4.1] For a bounded real-valued measurable function
ϕ on E1,
E

X
x∈X
ϕ(x)
 Y = y
= E

X
x∈eX
ϕ(x)
+ E

X
x∈bX
ϕ(x)IE1(x)

Y = y

=
Z
E1
ϕ(x)α(x)(1 −pD(x))dx +
m
X
i=1
R
E1 ϕ(x)α(x)pD(x)g(yi|x)dx
R
E1 α(x)pD(x)g(yi|x)dx + κ(yi)
.
When ϕ(x) = IA(x) for some subset A of E1 then the term on the right is precisely the
expected number of points in the set A. The non-negative function (on E1)
α(x)(1 −pD(x)) +
m
X
i=1
α(x)pD(x)g(yi|x)
R
E1 α(x)pD(x)g(yi|x)dx + κ(yi)
is the intensity (or ﬁrst moment) of the PP with law PX|y. The intensity of the superposition
of two independent processes is the sum of the two intensities and hence the two terms
that make up the above expression. This result was ﬁrst derived, using a diﬀerent proof
technique, in [14].

Estimation of static parameters in multi-target models
231
11.3.2
The PHD Filter
The foundations of the PHD ﬁlter are the following two facts.
Let Xn−1 be a Poisson PP with intensity αn−1(xn−1). Since Xn−1 evolves to Xn by a
process of independent thinning, displacement and augmentation with an independent Pois-
son birth process, it follows that marginal distribution of Xn is also Poisson with intensity
(fact 1)
αn(xn) =
Z
E1
f θ(xn|xn−1)pθ
S (xn−1)αn−1(xn−1)dxn−1 + γθ(xn)
=: (Φαn−1)(xn) + γθ(xn).
(11.6)
This fact may be established using the Thinning, Marking and Superposition theorems for
a Poisson process; see [10]. Speciﬁcally, subjecting the realisation of the Poisson PP with
intensity αn−1 to independent thinning and displacement results in a Poisson PP. The inten-
sity of this PP is given by the ﬁrst function on the right-hand side of Eq. (11.6). Combining
the realisations of two independent Poisson point processes still results in a Poisson PP.
The intensity of the resulting process is the sum of the intensities of the processes being
combined. This explains the addition of the term γθ on the right-hand side of Eq. (11.6).
Thus, it follows that if X0 is Poisson then so is Xn for all n.
It was established in Section 11.3.1 that the distribution of X1 conditioned on a realisa-
tion of observations y1 is not Poisson. However, the best Poisson approximation to PX1|y,
in a Kullback–Leibler sense, is the Poisson PP which has the same intensity as PX1|y (fact
2); see [14, 21]. (This is a general result that applies when any PP is approximated by a
Poisson PP using the Kullback–Leibler criterion.) By Corollary 11.2, the intensity of the
best approximating Poisson PP is
α1|1(x1) =
1 −pθ
D(x1) +
X
y∈y1
pθ
D(x1)gθ(y|x1)
R
E1 pθ
D(x)gθ(y|x)α1|0(x)dx + κθ(y)
α1|0(x1)
=: (Ψ1α1|0)(x1),
where α1|0 is the intensity of X1 and is given by Eq. (11.6) with n = 1. (Note that no
observation is received at time 0.) For convenience in the following we will also write, for
each n and r = 1, 2, ..., mn,
Zn,r :=
Z
En
pθ
D(x)gθ(yn,r|x)αn|n−1(x)dx.
The subscript on the update operator Ψ1 indicates the dependence on the speciﬁc reali-
sation of the observations received at time 1. The recursive application of the above two
facts gives rise to the PHD ﬁlter. Speciﬁcally, the conditional distribution of Xn at each
time is approximated by the best ﬁtting Poisson distribution before the subsequent Bayes
prediction step. This scheme deﬁnes a speciﬁc approximation to the optimal ﬁltering recur-
sions for the multi-target model whereby at each time step only the ﬁrst moment of the
conditional distribution is propagated
αn|n−1 = (Φαn−1|n−1) + γθ,
(11.7)
αn|n = (Ψnαn|n−1).
(11.8)

232
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
In the tracking literature these equations are referred to as the PHD ﬁlter and were
ﬁrst derived by [14]. The double subscripts in Eqs. (11.7) and (11.8) imply these are
conditional intensities as opposed to the intensity in Eq. (11.6), which is the unconditional
intensity of the hidden process.
11.4
Approximating the marginal likelihood
For a block of realised observations, y1:n, according to the model of Section 11.2, we make
use of the following decomposition of the marginal likelihood:
p(y1:n) = p(y1)
n
Y
k=2
p(yk|y1:k−1)
=
Z
p(y1|x1)p(x1)dx1
n
Y
k=2
Z
p(yk|xk)p(xk|y1:k−1)dxk.
(11.9)
Using the Poisson approximation of the conditional density p(xk|y1:k−1) given by the
PHD ﬁlter, i.e. αk|k−1, it follows that the predictive likelihood p(yk|y1:k−1) is also Poisson
and easily characterised.
Proposition 11.3 Let p(xn−1|y1:n−1) be the density of a Poisson process with intensity
αn−1|n−1. Then the predictive density of the observation yn,
p(yn|y1:n−1) =
Z
p(yn|xn)p(xn|y1:n−1)dxn,
is the density of a Poisson process with intensity function given by
Z
E1
gθ(y|xn)pθ
D(xn)αn|n−1(xn)dxn + κθ(y).
(11.10)
Proof Recall from the deﬁnition of the model that, given xn−1, Xn is formed as follows.
With probability pθ
S (x), each point of xn−1 survives and mutates according to the Markov
kernel f θ(·|xn−1). This happens independently for each point of xn−1. Thus Xn consists of
the surviving and mutated points of xn−1, superposed with points from an independent birth
Poisson process with intensity γθ. From the Thinning, Marking and Superposition theorems
for a Poisson process (see [10]), and under the condition of the proposition, p(xn|y1:n−1) is
Poisson with intensity αn|n−1 as deﬁned in Eq. (11.7). The observation Yn is then formed as
follows. With probability pθ
D(x), each point of Xn is detected and generates an observation
in E2 through the probability density gθ(·|xn). Thus Yn consists of the observations origi-
nating from Xn, superposed with an independent clutter Poisson process of intensity κθ. It
follows once again from the Thinning, Marking and Superposition theorems that, under the
condition of the proposition, p(yn|y1:n−1) is Poisson with intensity given by Eq. (11.10).
□
For a realised Poisson process y = y1:k in E2 with intensity function β(y), the likelihood
is given by
p(y) = 1
k! exp
"
−
Z
β(y)dy
#
k
Y
j=1
β(y j).
(11.11)

Estimation of static parameters in multi-target models
233
Combining Eq. (11.9), Proposition 11.3 and Eq. (11.11), the log-likelihood of the observed
data may be approximated as follows:
ℓPo,n(θ) = −
n
X
k=1
Z "Z
gθ(yk|xk)pθ
D(xk)αk|k−1(xk)dxk + κθ(yk)
#
dyk
+
n
X
k=1
mk
X
r=1
log
"Z
gθ(yk,r|xk)pθ
D(xk)αk|k−1(xk)dxk + κθ(yk,r)
#
,
(11.12)
where the subscript Po on ℓindicates that this is an approximation based on the Poisson
approximations to p(xn|y1:n−1).
11.5
SMC approximation of the PHD ﬁlter and its gradient
In the majority of cases, the Poisson approximation (11.12) of the true log-likelihood
log p(y1:n) cannot be computed exactly and a numerical scheme to approximate the various
integrals therein is needed. It was noted by [25] that under certain conditions on the multi-
target model, the predicted and updated intensities are Gaussian mixtures and the recursion
(11.7)–(11.8) is analytically tractable. However, the number of components in these mix-
tures explodes over time and so [25] employed a pruning mechanism to allow practical
implementation. Extended Kalman ﬁlter and the unscented Kalman ﬁlter style determinis-
tic approximations of the intensity recursions have also been devised to cope with a more
general model. Predating these works is [27] where a SMC method to approximate the
intensity functions was devised. In this section we review this original SMC algorithm and
extend it to approximate the gradient of the intensity functions. The SMC approximation
of the PHD likelihood and its gradient is also detailed.
Sequential Monte Carlo methods have become a standard tool for computation in non-
linear optimal ﬁltering problems and in this context have been termed particle ﬁlters. We do
not give explicit details of standard particle ﬁltering algorithms here but refer the reader to
[7] and [3] for a variety of algorithms, theoretical details and applications, and [6] for a gen-
eral framework. Sequential Monte Carlo algorithms may be viewed as being constructed
from ideas of sequential importance sampling (SIS) and resampling. They recursively prop-
agate a set of weighted random samples called particles, which are used to approximate
a sequence of probability distributions. The algorithms are such that, as the number of
particles tends to inﬁnity and under weak assumptions, an integral with respect to the ran-
dom distribution deﬁned by the particle set converges to the integral with respect to the
corresponding true distribution.
A particle implementation of the PHD ﬁlter (Eqs. (11.7) and (11.8)) was proposed
simultaneously in several works; [26, 20, 30]. These implementations may be likened to the
bootstrap particle ﬁlter in the sense that their ‘proposal’ steps ignore the new observations.
An auxiliary SMC implementation that takes into account the new observations at each
time was recently proposed by [28] to minimise the variance of the incremental weight.
The particle algorithm of [27] will be the building block for the particle approximation of
Eq. (11.12) and its gradient.
Given the particle set from the previous iteration, {X(i)
n−1, W(i)
n−1}N
i=1, N samples {X(i)
n }N
i=1
are each drawn from a proposal distribution qn(·|X(i)
n−1) and the predicted importance weights
{W(i)
n|n−1}N
i=1 are computed as follows:

234
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
X(i)
n ∼qn(·|X(i)
n−1),
W(i)
n|n−1 = f θ(X(i)
n |X(i)
n−1)pθ
S (X(i)
n−1)
qn(X(i)
n |X(i)
n−1)
W(i)
n−1,
1 ≤i ≤N. (11.13)
This proposal distribution can depend on θ, e.g. by setting qn to be the transition density for
the individual targets f θ, as done in the numerical examples in Section 11.7. A collection of
L additional samples, {X(i)
n }N+L
i=N+1, dedicated to the birth term are then drawn from a proposal
distribution pn(·) and the corresponding importance weights {W(i)
n|n−1}N+L
i=N+1 are computed:
X(i)
n ∼pn(·),
W(i)
n|n−1 = 1
L
γθ(X(i)
n )
pn(X(i)
n )
,
N + 1 ≤i ≤N + L.
The distribution pn can also depend on θ and the default choice would be proportional to
γθ (provided the corresponding normalised density exists and can be sampled from easily).
The approximation to the predicted intensity αn|n−1 is
bαn|n−1(dxn) =
N+L
X
i=1
W(i)
n|n−1δX(i)
n (dxn).
This may be used to approximate integrals of the form
R
E ψ(x)αn|n−1(x)dx which appear in
the denominator of Eq. (11.8). For r ∈{1, 2, ..., mn}, this approximation is given by
b
Zn,r =
N+L
X
i=1
ψn,r(X(i)
n )W(i)
n|n−1 + κθ(yn,r),
where
ψn,r(x) = pθ
D(x)gθ(yn,r|x).
The particles are then re-weighted according to the update operator yielding a second
collection of importance weights {W(i)
n }N+L
i=1 deﬁned as follows:
W(i)
n =
1 −pθ
D(X(i)
n ) +
mn
X
r=1
ψn,r(X(i)
n )
b
Zn,r
W(i)
n|n−1.
The empirical measure deﬁned by the particle set {X(i)
n , W(i)
n }N+L
i=1
then approximates the
updated intensity αn|n:
bαn|n(dxn) :=
N+L
X
i=1
W(i)
n δX(i)
n (dxn).
(11.14)
The importance weights, with total mass PN+L
i=1 W(i)
n , are then normalised so that they sum
to 1, and after resampling N times to obtain {X′(i)
n }N
i=1, the importance weights are set to the
constant (PN+L
i=1 W(i)
n )/N. The authors in [27] also noted that the total number of particles
may be varied across iterations, perhaps guided by the total mass of the updated intensity.
Convergence results establishing the theoretical validity of the particle PHD ﬁlter have been
obtained. Convergence of expected error was established in [27], almost sure convergence
and convergence of mean-square error were established in [5] and Lp error bounds, almost
sure convergence and a central limit theorem were established in [8].

Estimation of static parameters in multi-target models
235
Because of the low dimension of αn|n (e.g. four when the state descriptor of individual
targets contains position and velocity only) even a simple SMC implementation like the
one outlined above may suﬃce. In the case when the observations are informative, and
the likelihood functions are concentrated, weight degeneracy can occur with the above
implementation and the auxiliary version of [28] has been shown to be more eﬃcient.
For the gradient of the PHD ﬁlter, we will use the method proposed in [17, 18] for
the closely related problem of computing the gradient of the log-likelihood for a HMM.
Let ∇(Φαn−1|n−1) be a pointwise approximation of ∇(Φαn−1|n−1), that is ∇(Φαn−1|n−1)(xn) ≈
∇(Φαn−1|n−1)(xn), xn ∈E1. (This pointwise approximation is constructed in a sequen-
tial manner as detailed below.) One possible construction of a particle approximation to
∇αn|n−1(xn)dxn is the following:
c
∇αn|n−1(dxn) = 1
N
N
X
i=1
∇(Φαn−1|n−1)(X(i)
n )
Qn(X(i)
n )
δX(i)
n (xn)
+ 1
L
N+L
X
i=N+1
∇γθ(X(i)
n )
pn(X(i)
n )
δX(i)
n (xn)
(11.15)
where
Qn(xn) =
1
PN+L
j=1 W(j)
n−1
N+L
X
i=1
qn(xn|X(i)
n−1)W(i)
n−1.
Note that the particle set {X(i)
n }N
i=1 in Eq. (11.13) was obtained by sampling Qn N times.
(Assuming Eq. (11.14) is resampled at every time n.) Re-write the particle approximation
to ∇αn|n−1 as
c
∇αn|n−1 =
N+L
X
i=1
δX(i)
n (xn)A(i)
n|n−1W(i)
n|n−1
where
A(i)
n|n−1 = 1
N
∇(Φαn−1|n−1)(X(i)
n )
Qn(X(i)
n )
1
W(i)
n|n−1
,
1 ≤i ≤N,
A(i)
n|n−1 = ∇γθ(X(i)
n )
γθ(X(i)
n )
,
N + 1 ≤i ≤N + L.
The pointwise approximation to ∇(Φαn−1|n−1) is
∇(Φαn−1|n−1)(xn) =
Z
E1
h
∇log f θ(xn|xn−1)
i
f θ(xn|xn−1)pθ
S (xn−1)bαn−1|n−1(dxn−1)
=
Z
E1
h
∇log pθ
S (xn−1)
i
f θ(xn|xn−1)pθ
S (xn−1)bαn−1|n−1(dxn−1)
+
Z
E1
f θ(xn|xn−1)pθ
S (xn−1)c
∇αn−1|n−1(dxn−1).

236
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
Using Eq. (11.15), the particle approximation to ∇
R
ψn,r(xn)αn|n−1(xn)dxn + ∇κθ(yr), for
r = 1, 2, . . . , mn, is
d
∇Zn,r =
Z
∇ψn,r(xn)bαn|n−1(dxn) +
Z
ψn,r(xn)c
∇αn|n−1(dxn) + ∇κθ(yr)
=
N+L
X
i=1

∇ψn,r(X(i)
n ) + ψn,r(X(i)
n )A(i)
n|n−1

W(i)
n|n−1 + ∇κθ(yr).
The particle approximation of ∇αn|n is constructed by re-weighting the particle
approximations of ∇αn|n−1 and αn|n−1:
c
∇αn|n(dxn) = −∇pθ
D(xn)bαn|n−1(dxn) +

mn
X
r=1
ψn,r(xn)
b
Zn,r
∇log ψn,r(xn) −
d
∇Zn,r
b
Zn,r

bαn|n−1(dxn)
+
1 −pθ
D(xn) +
mn
X
r=1
ψn,r(xn)
b
Zn,r
c
∇αn|n−1(dxn)
=
N+L
X
i=1
A(i)
n W(i)
n δX(i)
n (dxn)
where
A(i)
n = A(i)
n|n−1 +
−∇pθ
D(X(i)
n ) +
mn
X
r=1
ψn,r(X(i)
n )
b
Zn,r
∇log ψn,r(X(i)
n ) −
d
∇Zn,r
b
Zn,r


W(i)
n|n−1
W(i)
n
.
The SMC estimate of ℓPo (for the same θ used in the weight calculation above and
proposal distributions above) is given by
bℓPo,n(θ) = −
n
X
k=1
"Z
pθ
D(xk)bαk|k−1(dxk) +
Z
κθ(yk)dyk
#
+
n
X
k=1
mk
X
r=1
log b
Zk,r,
(11.16)
and the estimate of ∇ℓPo,n is given by
c
∇ℓPo,n(θ) = −
n
X
k=1
Z 
N+L
X
i=1

∇pθ
D(X(i)
k ) + pθ
D(X(i)
k )A(i)
k|k−1

W(i)
k|k−1 +
Z
∇κθ(yk)dyk

+
n
X
k=1
mk
X
r=1
d
∇Zk,r
b
Zk,r
.
Algorithm 11.1 summarises the proposed SMC method for computing bℓPo,n and c
∇ℓPo,n.
The computational cost of this algorithm, unlike a conventional particle ﬁlter, grows
quadratically in the number of particles N.

Estimation of static parameters in multi-target models
237
Algorithm 11.1 Particle approximation of the intensity and its gradient
At time 0
for i = 1 to N do
X(i)
0 ∼q0(·), W(i)
0 = 1
N , A(i)
0 = 0.
end for
Set ℓPo,0 = 0, c
∇ℓPo = [0, . . . , 0] (∈Rd).
At time n ≥1
Prediction step:
for i = 1 to N do
X(i)
n ∼qn(·|X(i)
n−1)
Set W(i)
n|n−1 = f θ(X(i)
n |X(i)
n−1)pθ
S (X(i)
n−1)
qn(X(i)
n |X(i)
n−1)
W(i)
n−1|n−1.
Set A(i)
n|n−1W(i)
n|n−1 = 1
N
∇(Φαn−1|n−1)(X(i)
n )
Qn(X(i)
n )
.
end for
for i = N + 1 to N + L do
X(i)
n ∼pn(·)
Set W(i)
n|n−1 = 1
L
γθ(X(i)
n )
pn(X(i)
n )
,
A(i)
n|n−1W(i)
n|n−1 = ∇γθ(X(i)
n )
pn(X(i)
n )
.
end for
for r = 1 to mn do
Set b
Zn,r = PN+L
i=1 ψn,r(X(i)
n )W(i)
n|n−1 + κθ(yn,r).
Set d
∇Zn,r = PN+L
i=1

∇ψn,r(X(i)
n ) + ψn,r(X(i)
n )A(i)
n|n−1

W(i)
n|n−1 + ∇κθ(yr).
end for
Weight step:
for i = 1 to N + L do
Set W(i)
n =
1 −pθ
D(X(i)
n ) + Pmn
r=1
ψn,r(X(i)
n )
d
Zn,r
W(i)
n|n−1.
Set A(i)
n = A(i)
n|n−1 +
−∇pθ
D(X(i)
n ) + Pmn
r=1
ψn,r(X(i)
n )
b
Zn,r
∇log ψn,r(X(i)
n ) −
[
∇Zn,r
b
Zn,r


W(i)
n|n−1
W(i)
n
.
end for
Resample
X(i)
n ,
W(i)
n
PN+L
j=1 W(j)
n

N+L
i=1
to obtain
X(i)
n ,
PN+L
j=1 W( j)
n
N

N
i=1
.
Compute likelihood and likelihood gradient:
bℓPo,n = bℓPo,n−1 −
R PN+L
i=1 pθ
D(X(i)
n )W(i)
n|n−1 −
R
κθ(yn)dyn + Pmn
r=1 log d
Zn,r
c
∇ℓPo,n = c
∇ℓPo,n−1 −
N+L
X
i=1

∇pθ
D(X(i)
n ) + pθ
D(X(i)
n )A(i)
n|n−1

W(i)
n|n−1 −
Z
∇κθ(yn)dyn +
mn
X
r=1
d
∇Zn,r
b
Zn,r
.

238
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
11.6
Parameter estimation
11.6.1
Pointwise gradient approximation
Equipped with an approximation of the true likelihood p(y1:n) and its gradient, the param-
eters of the model may be estimated with a gradient ascent algorithm. This may be done in
an oﬄine fashion once a batch of observations, say y1:T, has been received, or in an online
manner. This section discusses both these methods of estimation.
Let the true static parameter generating the sequence of observations, to be estimated
from the observed data {yn}n≥1, be θ∗. Given a record of observations {yn}T
n=1, the log-
likelihood may be maximised with the following steepest ascent algorithm. For a discrete
time index, k = 1, 2, ..., which does not coincide with the time index of the observation
sequence,
θk+1 = θk + ak+1 ∇ℓPo,T(θ)
θ=θk ,
k ≥1,
(11.17)
where {ak}k≥1 is a sequence of small positive real numbers, called the step-size sequence,
that should satisfy the following constraints: P
k ak = ∞and P
k a2
k < ∞. One possible
choice would be ak = k−ζ, 0.5 < ζ < 1 (e.g. ak = k−2/3); see [16] for background theory on
steepest ascent.
For a long observation sequence the computation of the gradient in Eq. (11.17) can
be prohibitively expensive. A more attractive alternative would be a recursive procedure
in which the data is run through once sequentially. For example, consider the following
update scheme:
θn+1 = θn + an+1 ∇log pPo(yn|y1:n−1)
θ=θn .
Upon receiving yn, θn is updated in the direction of ascent of the conditional density of this
new observation. The algorithm in the present form is not suitable for online implementa-
tion due to the need to evaluate the gradient of log pPo(yn|y1:n−1) at the current parameter
estimate. Doing so would require browsing through the entire history of observations. This
limitation is removed by computing ∇log pPo(yn|y1:n−1)
θ=θn recursively using the previ-
ous values of the parameter as well. This modiﬁcation is straightforward; see [18] for the
closely related problem of recursive maximum likelihood estimation in HMMs.
In practice, it may be beneﬁcial to start with a constant but small step-size, an = a for
some initial period n < n∗. If the step-size decreases too quickly the algorithm might get
stuck at an early stage and fail to come close to a global maximum of the likelihood.
11.6.2
Simultaneous perturbation stochastic approximation (SPSA)
It is also possible to maximise ℓPo without explicit computation of the gradient using SMC.
In particular, a ﬁnite diﬀerence (FD) approximation of the gradient may be constructed
from the noisy evaluations of ℓPo obtained using SMC. Such approaches are often termed
‘gradient-free’; see [23].
Consider the problem of maximising a real-valued function θ ∈Θ →ℓ(θ) where Θ is
an open subset of R. The ﬁrst steepest ascent algorithm based on FD approximation of the
likelihood gradient is due to [9]. This involves, for example in the two-sided case, noisy
evaluation of ℓat perturbed parameter values θ ± ∆and the subsequent approximation of
the gradient at this parameter value, ∇ℓ(θ), as follows:
c
∇ℓ(θ) =
bℓ(θ + ∆) −bℓ(θ −∆)
2∆
.

Estimation of static parameters in multi-target models
239
The method can be generalised to the case in which Θ ⊂Rd, d ≥1, by carrying out
two noisy evaluations of ℓfor each dimension of Θ, but this can become computationally
expensive when the dimension is high. An alternative is the SPSA method of [22], which
requires only two noisy evaluations of ℓ, regardless of the dimension of Θ. This method
takes its name from the fact that it involves perturbing the multiple components of the
vector θ at the same time. In the case of SPSA, the estimate of the gradient at the kth
iteration of the gradient ascent algorithm (i.e. the recursion (11.17)) is
d
∇pℓPo,T(θk) =
bℓPo,T(θk + ck∆k) −bℓPo,T(θk −ck∆k)
2ck∆k,p
,
p = 1, . . . , d,
where ∆k = [∆k,1 ∆k,2 ... ∆k,d]T is a random perturbation vector, {ck}k≥0 is a decreasing
sequence of small positive numbers and ∇pℓPo,T is the partial derivative of ℓPo,T w.r.t. the
pth component of the parameter θ. The elements of ∆k are iid, non-zero, symmetrically
distributed random variables. In this case we take them to be Bernoulli ±1 distributed, but
it should be noted that some alternative distributions, such as zero-mean Gaussian distri-
butions are theoretically invalid, see [22, Chapter 7] for further background details. The
objective function ℓPo,T(θ) is estimated using the SMC implementation of Section 11.5.
Theoretical results guaranteeing convergence of SPSA require the following conditions on
the gain sequences ([23]):
∀k, ak > 0 and ck > 0;
ak and ck →0;
∞
X
k=0
ak = ∞;
∞
X
k=0
a2
k
c2
k
< ∞.
Practical choices of these sequences can be based around the following expres-
sions, advocated and related to theoretical properties of SPSA in [23]. For non-negative
coeﬃcients a, c, A, ς, τ
ak =
a
(k + 1 + A)ς ,
ck =
c
(k + 1)τ .
The recommendation is to set ς = 0.6 and τ = 0.1 and, as a rule of thumb, to choose A to
be 10 per cent or less of the maximum number of allowed iterations of the steepest ascent
recursion.
Throughout the simulation study in Section 11.7.3, common random numbers were
used for each pair of noisy evaluations of the objective function. It has been shown by
[11] that using common random numbers in this way leads to faster convergence of the
steepest ascent algorithm. It should be noted that a number of other strategies, such as
iterate averaging and adaptive schemes involving estimation of the Hessian matrix, can
improve the performance of SPSA. These techniques are beyond the scope of this chapter
and are discussed in [23].
Simultaneous perturbation stochastic approximation for maximising ℓPo,T is sum-
marised in Algorithm 11.2.
11.7
Simulation study
11.7.1
Model
The proposed parameter estimation methods are evaluated on a multi-target model with the
following characteristics.

240
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
Algorithm 11.2 SPSA parameter estimation
At time 1
Initialise θ1.
At time k ≥1
Generate perturbation vector ∆k.
Run SMC PHD ﬁlter with θ = θk + ck∆k.
Compute bℓPo,T(θk + ck∆k) according to Eq. (11.16).
Run SMC PHD ﬁlter with θ = θk −ck∆k.
Compute bℓPo,T(θk −ck∆k) according to Eq. (11.16).
Set c
∇ℓPo,T(θk) =
bℓPo,T (θk+ck∆k)−bℓPo,T (θk−ck∆k)
2ck
h 1
∆k,1
1
∆k,2 ...
1
∆k,d
iT.
Set θk+1 = θk + akc
∇ℓPo,T(θk).
A constant velocity model is assumed for individual targets. The position of a target is
speciﬁed in two dimensions, restricted to the window [0, 100]×[0, 100]. The state of a single
target is thus speciﬁed by a four-dimensional vector Xn = [Xn(1), Xn(2), Xn(3), Xn(4)]T,
where the variables (Xn(1), Xn(3)) specify position and the variables (Xn(2), Xn(4)) specify
velocity. The state of the single target evolves over time as follows:
Xn =

1
1
0
0
0
1
0
0
0
0
1
1
0
0
0
1

Xn−1 +

Vn(1)
Vn(2)
Vn(3)
Vn(4)

,
where Vn(1) and Vn(3) are independent Gaussian random variables with mean 0 and stan-
dard deviation σxs = 0.01; Vn(2) and Vn(4) are also independent Gaussian random variables
with mean 0 and standard deviation σxv = 0.25. The state of individual targets is a vector
in E1 = [0, 100] × R×[0, 100] × R. The birth intensity is deﬁned as
γθ(·) = ΓN(·; µb, Σb),
where N(x; µb, Σb) denotes the multivariate normal density with mean µb and covariance
Σb, evaluated at x. For the numerical example,
µb =

µbx
0
µby
0

, Σb =

σ2
bs
0
0
0
0
σ2
bv
0
0
0
0
σ2
bs
0
0
0
0
σ2
bv

.
The x–y position of the target is observed in additive, isotropic Gaussian noise with
standard deviation σy. The clutter intensity κ(y) is uniform on [0, 100] × [0, 100]:
κ(y) = κI[0,100]×[0,100](y).
The probability of detection pD(x) and survival pS (x) is assumed constant over E1. The
measurements from individual targets and clutter are vectors in E2 = R2.The parameters of
the model to be inferred from the data in the numerical studies below are
θ = [σy, κ, Γ, µbx, µby, σbs, σbv, pD]T
with (σxs, σxv, pS ) assumed known.

Estimation of static parameters in multi-target models
241
11.7.2
Pointwise gradient approximation
The performance of gradient ascent, see Eq. (11.17) with Algorithm 11.1 to estimate
the derivative of the likelihood, was evaluated using the following set of parameters:
θ∗= [5, 4 × 10−4, 1, 50, 50, 5, 2, 0.9]T, i.e. the observation record was generated using these
parameter values. For an observation time of length 50, these values for the model would
generate, on average, 9 observations per time instant with 4 of them being false, and a total
of 50 targets for all the 50 time points. The number of particles used were N = 400 and
L = 400.
Figure 11.1 shows the sequence of iterates generated by Eq. (11.17) for a constant step-
size sequence, i.e. ak = a, and for an initial value chosen to be reasonably distant from θ∗
(consult the ﬁgure for the initial values). The estimated model parameters converge to a
vicinity of the true parameters but with some notable discrepancies. (Further details in the
discussion to follow.) The smoothness of the traces also indicates that the estimate of the
gradient with Algorithm 11.1 has low variance.
To characterise the distribution of the estimated parameters, the experiment was
repeated a total of 50 times for observation records of length 15, 50 and 125. In each
repetition of the experiment, the targets and observation record were generated again from
the model. The distribution of the converged value for the parameters is shown in Fig. 11.2
when the gradient is approximated using Algorithm 11.1 along with their true values as
horizontal lines. As can be seen from the box plots, the estimated model parameters do
improve with longer observation records. The results are encouraging and are a further
0
5000
10000
10
20
30
40
50
60
70
μbx, true = 50
0
5000
10000
10
20
30
40
50
60
70
μby, true = 50
0
5000
10000
0
5
10
15
20
σbs, true = 5
0
5000
10000
0
2
4
6
8
10
σbv, true = 2
0
5000
10000
0
0.2
0.4
0.6
0.8
1
μd, true = 0.9
0
5000
10000
0
1
2
3
4
5
6
x 10
−3 κ, true = 4x10−4
0
5000
10000
0
2
4
6
8
10
12
Γ, true = 1
0
5000
10000
0
5
10
15
20
σy , true = 5
Figure 11.1 Evolution of the parameter estimates generated by steepest ascent with the gradient computed using
Algorithm 11.1 (dashed) and SPSA (solid). Observation record length was 25 generated with the model outlined
at the start of Section 11.7. Only the values after every 50th step are displayed. True values of parameters are
indicated by the horizontal lines and the estimated values of the gradient based method are marked on the y-
axis. Notable discrepancies for µbx and µby would be outliers for a longer observation record; see box plots
in Fig. 11.2.

242
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
125
15
50
45
50
55
μbx
125
50
15
45
50
55
μby
125
50
15
0
5
10
σbs
125
50
15
0
1
2
3
4
5
σbv
125
50
15
0.2
0.4
0.6
0.8
1
pD
125
50
15
0
1
2
3
Γ
125
50
15
2
3
4
5
6
7
κ
125
50
15
2
4
6
8
σy
Figure 11.2 Box plots of the converged value of parameter estimates for diﬀerent observation record lengths.
Dashed horizontal lines indicate the true value of the parameters.
veriﬁcation of the merit of the Poisson approximation of the posterior in Proposition 11.3;
thus far the approximation has only been veriﬁed by comparing the estimated intensity with
the true number of targets in synthetic numerical examples. It is evident from the box plots
in Fig. 11.2 that there are small biases for some of the parameters. It is unclear if these are
due to the insuﬃcient number of particles used to approximate the intensity function or are
inherent to the approximate likelihood itself (see Eq. (11.12)). For example, the box plots
indicate a bias in the estimation of the clutter intensity; it is being over estimated. This may
be explained by the small number of particles used in the simulation. On average, for an
observation record of length 50, there were 9 targets at any particular time instant; 400 par-
ticles are not suﬃcient to follow all the modes of the intensity function (11.8) induced by
these targets. Hence the observations generated by the targets are perhaps being accounted
for by a higher clutter intensity estimate. We also remark that the converged values of the
estimates for µbx and in µby in Fig. 11.1 would be outliers for an observation record of
length 50. This can be seen from the corresponding box plots in Fig. 11.2.
11.7.3
SPSA
The SPSA scheme, Algorithm 11.2, with Algorithm 11.1 used to obtain a noisy evalua-
tion of the Poisson likelihood, was run on the same data record as the pointwise gradient
method, with the same initial conditions. A constant step-size of ak = a = 1 × 10−5 and
ck = c = 0.025 were chosen after a few pilot runs.

Estimation of static parameters in multi-target models
243
A ﬁxed step-size was chosen so as to avoid premature convergence of the algo-
rithm. After 20 000 iterations, and having reached equilibrium, the SPSA scheme resulted
in the following parameter values: µbx = 45.2, µby = 53.6, σbs = 5.22, σbv = 1.62,
κ = 3.95 × 10−4, pD = 0.746, Γ = 1.01, σy = 4.44. These values compare well with those
obtained using the pointwise method and small discrepancies are attributable to the bias
arising from the ﬁnite diﬀerence gradient approximation with ck held constant. Algorithm
11.1 used N = 1000 and L = 1000 particles.
The SPSA scheme is simpler to implement than the pointwise gradient method, but
it requires choice of both the sequences (an) and (cn) and therefore may need more man-
ual tuning in pilot runs. Also, the computational cost grows linearly with the number of
particles.
11.8
Conclusion
The problem of estimating the number of targets and their states, and calibrating the multi-
target statistical model is diﬃcult and only approximate inference techniques are feasible
([13]). The focus of this work was the problem of calibrating the model. For this purpose
an approximation of the true marginal likelihood of the observed data, i.e. p(y1:T) where T
is the ﬁnal time of the recorded data, was proposed. The estimate of the model parameters
was then taken to be the maximising argument, with respect to the model parameters, of
this approximation of the true marginal likelihood. A gradient ascent algorithm was used
to ﬁnd the model parameters that maximise the likelihood. Although the approximate like-
lihood function was not computable exactly, it and its gradient were estimated using SMC.
The approximate likelihood function was ‘characterised’ by a sequence of non-negative
functions on E1, and not the space U
k≥0 Ek
1, and even a simple SMC implementation can be
reasonably eﬃcient. However, compared to ‘standard’ SMC applications in [7], the SMC
implementation was expensive. In particular the computational cost grew quadratically with
the number of particles and this limited both the number of particles and the size of the
data records in the numerical examples. It was demonstrated in numerical examples that
the approximation to the true likelihood was reasonable as the model parameters could be
inferred by maximising it. While the results are encouraging, an important issue remains to
be addressed. We have not (even empirically) characterised the bias introduced by the like-
lihood approximation because the exact likelihood cannot be computed. A characterisation
of this bias appears to be a challenging problem.
Bibliography
[1] Y. Bar-Shalom and T. E. Fortmann. Tracking and
Data Association. Mathematics in science and
engineering. Academic Press, 1964.
[2] S. Blackman and R. Popoli, editors. Design and
Analysis of Modern Tracking Systems. Artech
House radar library. Artech House, 1999.
[3] O. Capp´e, S. J. Godsill and E. Moulines. An
overview of existing methods and recent
advances in sequential Monte Carlo.
Proceedings of the IEEE, 96(5):899 –924, 2007.
[4] D. Clark, A. T. Cemgil, P. Peeling and S. J.
Godsill. Multi-object tracking of sinusoidal
components in audio with the Gaussian mixture
probability hypothesis density ﬁlter. In Proc. of
IEEE Workshop on Applications of Signal
Processing to Audio and Acoustics, 2007.
[5] D. E. Clark and J. Bell. Convergence results for
the particle PHD ﬁlter. IEEE Transactions on
Signal Processing, 54(7):2652–2661, 2006.
[6] P. Del Moral, A. Doucet and A. Jasra. Sequential
Monte Carlo methods for Bayesian
computation. In Bayesian Statistics 8. Oxford
University Press, 2006.
[7] A. Doucet, N. de Freitas and N. Gordon, editors.
Sequential Monte Carlo Methods in Practice.
Statistics for Engineering and Information
Science. Springer Verlag, 2001.
[8] A. M. Johansen, S. Singh, A. Doucet and B. Vo.
Convergence of the SMC implementation of the

244
Sumeetpal S. Singh, Nick Whiteley and Simon J. Godsill
PHD ﬁlter. Methodology and Computing in
Applied Probability, 8(2):265–291, 2006.
[9] J. Kiefer and J. Wolfowitz. Stochastic estimation
of the maximum of a regression function. Annals
of Mathematical Statistics, 23(3):462–466, 1952.
[10] J. F. C. Kingman. Poisson Processes. Oxford
Studies in Probability. Oxford University Press,
1993.
[11] N. L. Kleinman, J. C. Spall and D. Q. Naiman.
Simulation–based optimisation with stochastic
approximation using common random numbers.
Management Science, 45(11):1571–1578, 1999.
[12] J. Lund and E. Thonnes. Perfect simulation and
inference for point processes given noisy
observations. Computational Statistics,
19(2):317–336, 2004.
[13] R. Mahler. Statistical Multisource-Multitarget
Information Fusion. Artech House, 2007.
[14] R. P. S. Mahler. Multitarget Bayes ﬁltering via
ﬁrst-order multitarget moments. IEEE
Transactions on Aerospace and Electronic
Systems, pages 1152–1178, 2003.
[15] J. Mullane, B. Vo, M. D. Adams and W. S.
Wijesoma. A PHD ﬁltering approach to robotic
mapping. In IEEE Conf. on Control, Automation,
Robotics and Vision, 2008.
[16] G. Pﬂug. Optimization of Stochastic Models: The
Interface between Simulation and Optimization.
Kluwer Academic Publishers, 1996.
[17] G. Poyiadjis, A. Doucet and S. S. Singh.
Maximum likelihood parameter estimation using
particle methods. In Proceedings of the Joint
Statistical Meeting, 2005.
[18] G. Poyiadjis, A. Doucet and S. S. Singh. Monte
Carlo for computing the score and observed
information matrix in state-space models with
applications to parameter estimation. Technical
Report CUED/
F-INFENG/TR.628, Signal Processing
Laboratory, Department of Engineering,
University of Cambridge, 2009.
[19] D. Reid. An algorithm for tracking multiple
targets. IEEE Transactions on AutomaticControl,
24:843854, 1979.
[20] H. Siddenblath. Multi-target particle ﬁltering for
the probability hypothesis density. In
Proceedings of the International Conference on
Information Fusion, Cairns, Australia, pages
800–806, 2003.
[21] S. S. Singh, B.-N. Vo, A. Baddeley and S. Zuyev.
Filters for spatial point processes. SIAM Journal
on Control and Optimization, 48:2275–2295,
2009.
[22] J. C. Spall. Multivariate stochastic
approximation using simultaneous perturbation
gradient approximation. IEEE Transations on
Automatic Control, 37(3):332–341, 1992.
[23] J. C. Spall. Introduction to Stochastic Search and
Optimization. Wiley-Interscience, 1st edition,
2003.
[24] C. B. Storlie, C. M. Lee, J. Hannig and
D. Nychka. Tracking of multiple merging and
splitting targets: A statistical perspective (with
discussion). Statistica Sinica, 19(1):152, 2009.
[25] B. Vo and W.-K. Ma. The Gaussian mixture
probability hypothesis density ﬁlter. IEEE Trans.
Signal Processing, 54(11):4091–4104, 2006.
[26] B. Vo, S. Singh and A. Doucet. Random ﬁnite
sets and sequential Monte Carlo methods in
multi-target tracking. In Proceedings of the
International Conference on Information Fusion,
Cairns, Australia, pages 792–799, 2003.
[27] B. Vo, S. Singh and A. Doucet. Sequential
Monte Carlo methods for multitarget ﬁltering
with random ﬁnite sets. IEEE Transactions on
Aerospace and Electronic Systems,
41(4):1224–1245, 2005.
[28] N. Whiteley, S. Singh and S. Godsill. Auxiliary
particle implementation of the probability
hypothesis density ﬁlter. IEEE Transactions on
Aerospace and Electronic Systems, 43(3):
1437–1454, 2010.
[29] J. W. Yoon and S. S. Singh. A Bayesian
approach to tracking in single molecule
ﬂuorescence microscopy. Technical Report
CUED/F-INFENG/TR-612, University of
Cambridge, September 2008. Working paper.
[30] T. Zajic and R. P. S. Mahler. Particle-systems
implementation of the PHD multitarget tracking
ﬁlter. In Proceedings of SPIE, pages 291–299,
2003.
Contributors
Sumeetpal S. Singh, Signal Processing Laboratory, Department of Engineering, University of
Cambridge
Nick Whiteley, Statistics Group, Department of Mathematics, University of Bristol
Simon J. Godsill, Signal Processing Laboratory, Department of Engineering, University of
Cambridge

12
Sequential inference for dynamically evolving
groups of objects
Sze Kim Pang, Simon J. Godsill, Jack Li, Fran¸cois Septier and Simon Hill
12.1
Introduction
In nature there are many examples of group behaviour arising from the action of individuals
without any apparent central coordinator, such as the highly coordinated movements of
ﬂocks of birds or schools of ﬁsh. These are among the most fascinating phenomena to be
found in nature; where the groups seem to turn and manoeuvre as a single unit, changing
direction almost instantaneously. Similarly, in man-made activities, there are many cases
of group-like behaviour, such as a group of aircraft ﬂying in formation.
There are two principal reasons why it is very helpful to model the behaviour
of groups explicitly, as opposed to treating all objects independently as in most mul-
tiple target tracking approaches. The ﬁrst is that the joint tracking of (a priori )
dependent objects within a group will lead to greater detection and tracking ability in
hostile environments with high noise and low detection probabilities. For example, in
the radar target tracking application, if several targets are in a group formation, then
some information on the positions and speeds of those targets with missing measure-
ments (due to poor detection probability) can be inferred given those targets that are
detected. Similarly, if a newly detected target appears close to an existing group, the
target can be initialised using the group velocity. Secondly, if it can be determined
automatically which targets are moving in formation as a group, it may then be pos-
sible to group objects according to intentionality, behaviour and friend or foe status.
These are all highly desirable inference outcomes for the modern tracking application.
However, little development has been reported on group tracking, with implementa-
tion problems resulting from the splitting and merging of groups hindering progress
[3].
This chapter presents a new approach to group target detection and tracking by casting
the problem in a general probabilistic framework, and modelling the group interactions of
dynamic targets and the evolving group structure over time. This permits a uniﬁed treat-
ment to various inference goals involving dynamic groupings, from group target detection
and tracking to group structure inference. The probabilistic framework is general enough
to tackle a variety of applications from target tracking in radar to group stock selection in
ﬁnance. An algorithm based on Markov chain Monte Carlo (MCMC) [16, 35] will be intro-
duced in Section 12.2 to perform inference for a sequentially evolving distribution such as
the ﬁltering distribution. The group tracking problem will be formulated in Section 12.3.
Two diﬀerent group tracking applications will be developed as well. The ﬁrst application in

246
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
Section 12.4 is concerned with tracking groups of vehicles. The second application in Sec-
tion 12.5 deals with identifying groupings of stocks over time. For each of the applications,
a group dynamical model and a group structure transition model will be developed.
12.2
MCMC-particles algorithm
Many problems in engineering and science can be formulated using dynamic state space
models. This involves describing the laws governing the changes of some hidden state S t
over time t, and the observations Zt made as a result of the hidden state. The standard
state space system can be speciﬁed by a stochastic process with Markovian transitions and
independent observations given the state. Then the joint density of the system is given by
p(S 1:t, Z1:t) = p(S 1)p(Z1|S 1)
tY
t′=2
p(Zt′|S t′)p(S t′|S t′−1),
where S t and Zt are the system state and observation at time t, p(S 1) is the initial
distribution of the state, p(S t|S t−1) is the system dynamical model and p(Zt|S t) is the
observation model. In many modern estimation problems, the state space is almost cer-
tainly non-linear and non-Gaussian. There are diﬀerent numerical methods that can provide
solutions to the Bayesian estimation problems. These include adaptive grid-based solu-
tions [9], variational methods [23] which ﬁnd the best approximating distribution based on
Kullback–Leibler divergence, and sequential Monte Carlo methods (also known as particle
ﬁlters) [22, 10, 34, 6].
Markov chain Monte Carlo is a class of Monte Carlo algorithm which can simulate
samples from a probability distribution. This essentially allows Monte Carlo integration
through the use of a Markov chain. The general form of the algorithm is known as
the Metropolis–Hastings (MH) algorithm (Algorithm 12.1). Here, S is the general state
variable, p(S ) is the target density and q(S |S m) is the proposal density. The acceptance
probability ρ(S m, S ∗) is given by
ρ(S m, S ∗) = f
 p(S ∗)q(S m|S ∗)
p(S m)q(S ∗|S m)
!
,
where we deﬁned the Metropolis–Hastings acceptance function f (x) = min(1, x).
Traditionally, MCMC is used to draw samples in a non-sequential setting. The
advantages of MCMC are that it is generally more eﬀective than particle ﬁlters in high-
dimensional systems and it is easier to design for complex distributions if it can be used
in a sequential fashion. This chapter will develop a sequential inference algorithm using
MCMC with a particle approximation of the posterior distribution.
12.2.1
Sequential MCMC
This section provides a brief overview of existing MCMC-based algorithms which operate
sequentially in time on state space problems such as those described.
Resample-move algorithm
The authors in [15] introduced the resample-move algorithm, which was one of the ﬁrst
methods to make use of MCMC in a sequential state estimation problem. The algorithm
takes the output of a standard particle ﬁlter after resampling, and applies a MCMC move to

Sequential inference for evolving groups of objects
247
Algorithm 12.1 Metropolis–Hastings (MH) algorithm
Initialise S 0. Set m = 0.
while m ⩽NMCMC do
Sample a point S ∗∼q(S |S m) and u from a uniform distribution U(u|0, 1).
if u ≤ρ(S m, S ∗) then
S m+1 = S ∗.
else
S m+1 = S m.
end if
Increment m.
end while
each particle using the posterior distribution p(S 1:t|Z1:t) as the stationary distribution. The
main idea is that moving each particle after resampling will improve the diversity of the par-
ticle population, and this can only improve the representation of the posterior distribution.
As mentioned by [6], the resample-move algorithm does not require each MCMC chain
initiated to have a burn-in period. This is because the output of the particle ﬁlter is already
a reasonable approximation to the posterior ﬁltering distribution. Given each resampled
particle S 1:t,p, the target distribution of the MCMC, Algorithm 12.1, is given by
p(S t|S 1:t−1,p, Z1:t) ∝p(Zt|S t)p(S t|S t−1,p)p(S 1:t−1,p|Z1:t−1).
With a proposal function q(S t|S m
t ), then the acceptance ratio ρ(S m
t , S ∗
t ) is given by
f
 p(S ∗
t |S 1:t−1,p, Z1:t)q(S m|S ∗)
p(S m
t |S 1:t−1,p, Z1:t)q(S ∗
t |S m
t )
!
= f
 p(Zt|S ∗
t )p(S ∗
t |S t−1,p)q(S m|S ∗)
p(Zt|S m
t )p(S m
t |S t−1,p)q(S ∗
t |S m
t )
!
.
MCMC-based particle ﬁlter
The authors in [24] provide an alternative sequential approach to using MCMC. At each
time step t, a single chain is designed to obtain the ﬁltering distribution p(S t|Z1:t). This is
achieved by using a set of unweighted particles to represent the density p(S t−1|Z1:t−1), that is
p(S t−1|Z1:t−1) ≈ˆp(S t−1|Z1:t−1) =
Np
X
p=1
1
Np
δ(S t−1 −S t−1,p),
giving
p(S t|Z1:t) ∝p(Zt|S t)
Z
p(S t|S t−1)p(S t−1|Z1:t−1)dS t−1
≈p(Zt|S t)
Z
p(S t|S t−1) ˆp(S t−1|Z1:t−1)dS t−1 = p(Zt|S t)
Np
X
p=1
1
Np
p(S t|S t−1,p).
(12.1)
Using Eq. (12.1) as the target distribution and proposal q(S t|S m
t ), the acceptance ratio
ρ(S m
t , S ∗
t ) is given by (Algorithm 12.1)
f
 p(S ∗
t |Z1:t)q(S m
t |S ∗
t )
p(S m
t |Z1:t)q(S ∗
t |S m
t )
!
= f

p(Zt|S ∗
t ) PNp
p=1
1
Np p(S ∗
t |S t−1,p)q(S m|S ∗)
p(Zt|S m
t ) PNp
p=1
1
Np p(S m
t |S t−1,p)q(S ∗|S m)
.

248
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
This approach is simpler in the sense that only MCMC is used, compared with the two-stage
procedure of the resample-move algorithm where a resampling step is necessary. However,
the computational demand of the MCMC-based particle ﬁlter can become excessive as the
number of particles increases owing to the direct Monte Carlo computation of the predictive
density at each time step (in general O(Np) per MCMC iteration).
Practical ﬁlter
The practical ﬁlter [33] is an algorithm which uses MCMC for sequential state infer-
ence and static parameter estimation. By assuming there is a particle approximation to
the smoothing distribution p(S t−k|Z1:t−1) at time t −k, the practical ﬁlter runs MCMC for
each of the particles to obtain the ﬁltered state from time t −k + 1 to t. The practical ﬁlter
makes the assumption that the lag k is large enough so that p(S t−k|Z1:t) can be approxi-
mated by p(S t−k|Z1:t−1). This is related to the approach by [8], which assumes that future
observations are independent of states in the distant past. First, it is assumed that there is a
particle approximation to p(S t−k|Z1:t−1), i.e.
p(S t−k|Z1:t−1) =
Np
X
p=1
1
Np
δ(S t−k −S t−k|t−1,p).
Here S t−k|t−1,p is used to represent the pth particle approximating the distribution
p(S t−k|Z1:t−1). This is to distinguish it from the usual notation S t−k,p which is used to
represent the pth particle from the ﬁltering distribution p(S t−k|Z1:t−k). A typical approach
to obtain p(S t−k|Z1:t−1) is to perform MCMC on the distribution p(S 1:t−1|Z1:t−1). Then
p(S t−k|Z1:t) can be approximated as follows:
p(S t−k|Z1:t) ≈p(S t−k|Z1:t−1) =
Np
X
p=1
1
Np
δ(S t−k −S t−k|t−1,p).
The lag-(k −1) smoothing distribution p(S t−k+1:t|Z1:t) is then given by
p(S t−k+1:t|Z1:t) =
Z
p(S t−k+1:t, S t−k|Z1:t)dS t−k =
Z
p(S t−k+1:t|S t−k, Z1:t)p(S t−k|Z1:t)dS t−k
≈
Np
X
p=1
1
Np
p(S t−k+1:t|S t−k|t−1,p, Z1:t).
The practical ﬁlter runs MCMC using p(S t−k+1:t|S t−k|t−1,p, Z1:t) as the target distribution
for each particle. At the end of each MCMC run after NMCMC iterations, the ﬁnal state
of the MCMC chain is S NMCMC
t−k+1:t. The S NMCMC
t−k+1 will be recorded as S t−k+1|t,p. The particle set
{S t−k+1|t,p}
Np
p=1 will then be the empirical approximation to p(S t−k+1|Z1:t). Other empirical
approximations are obtained by keeping the appropriate output from the MCMC chains.
For example, the ﬁltering distribution can be obtained by keeping S NMCMC
t
for each chain.
Particle Markov chain Monte Carlo (PMCMC)
The authors in [1] introduced a class of sequential MCMC (SMC) algorithms to design
eﬃcient high-dimensional MCMC proposal distributions. In the simplest case, a MCMC
algorithm is designed with the target distribution of p(S 1:t|Z1:t). At each iteration of the

Sequential inference for evolving groups of objects
249
MCMC, the proposal S ∗
1:t is constructed by running a SMC algorithm. By considering the
artiﬁcial joint density of all the random variables generated by the SMC algorithm, [1]
showed that the resultant acceptance ratio ρ(S m
1:t, S ∗
1:t) is given by
ρ(S m
1:t, S ∗
1:t) = f
 p(S ∗
1:t|Z1:t)q(S m
1:t|Z1:t)
p(S m
1:t|Z1:t)q(S ∗
1:t|Z1:t)
!
= f
 ˆp(Z1:t)∗
ˆp(Z1:t)m
!
,
where ˆp(Z1:t)∗and ˆp(Z1:t)m are the estimates of the marginal likelihood p(Z1:t) of the respec-
tive runs of the SMC algorithm. This approach is computationally intensive as it attempts to
draw directly from the joint density of S 1:t. For problems where p(S t|S t−1) does not admit
an analytical expression but can be sampled from, this approach is very useful.
Further developments
In this chapter, an alternative MCMC algorithm for carrying out sequential inference will
be developed. This algorithm can be considered a generalisation of some existing sequential
MCMC algorithms which allows for further extensions and insights. A special case of it
has been applied by [2] to a dynamical model with an expanding parameter space. It has
also been applied to imputing missing data from non-linear diﬀusions [20].
The approach is distinct from the resample-move scheme for particle ﬁlters where
MCMC is used to rejuvenate degenerate samples after resampling. In this algorithm, no
resampling is required. It is less computationally intensive than the MCMC-based parti-
cle ﬁlter in [24] because it avoids numerical integration of the predictive density at every
MCMC iteration. In the simplest case, the framework considered here is the MCMC imple-
mentation of the auxiliary particle ﬁlter by [32]. But the general algorithm is more ﬂexible
than that. It does not make the same approximation of the practical ﬁlter in [33], the only
approximations are the empirical representation of the posterior density using particles and
the assumption that the MCMC run has converged.
12.2.2
Outline of algorithm
Many applications are concerned with inferring the posterior ﬁltering distribution p(S t|Z1:t)
in a sequential fashion as observations arrive. Here, to develop the MCMC-particles
algorithm [30, 31], we instead consider the general joint distribution of S t and S t−1
p(S t, S t−1|Z1:t) ∝p(S t−1|Z1:t−1)p(S t|S t−1)p(Zt|S t).
(12.2)
In [2] the authors used a similar form where the target distribution for the MCMC is
π(Φt, εt−1|Dt−1, Et−1, Ft), this is equivalent with S t
= Φt, S t−1
= εt−1, and Z1:t
=
{Dt−1, Et−1, Ft}. The authors in [20] were interested in the imputation values S t of a dis-
cretely observed diﬀusion process with parameters θ. The target distribution for their
MCMC is given by p(S t, S t−1, θ|Z1:t) ∝p(S t−1, θ|Z1:t−1)p(S t, Zt|S t−1, Zt−1, θ). A MCMC
procedure will be used to draw inference from this complex distribution, i.e. (12.2) is the
target distribution. Clearly, there is no closed-form representation of the posterior distri-
bution p(S t−1|Z1:t−1) at time t −1. Instead it is approximated by an empirical distribution
based on the current particle set, ˆp(S t−1|Z1:t−1) (assumed uniformly weighted), that is
p(S t−1|Z1:t−1) ≈ˆp(S t−1|Z1:t−1) =
Np
X
p=1
1
Np
δ(S t−1 −S t−1,p),
where Np is the number of particles and p is the particle index. Then, having made many
joint draws from Eq. (12.2) using an appropriate MCMC scheme, the converged MCMC

250
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
output for variable S t can be extracted to give an updated marginalised particle approxi-
mation to p(S t|Z1:t). In this way, sequential inference can be achieved. Instead of drawing
directly from the discrete distribution, a kernel density estimate of ˆp(S t−1|Z1:t−1) can be
used [27, 20]. This can increase the diversity of the particle samples. However, a kernel
with appropriate bandwidth must be selected. In high-dimensional state spaces, it may not
be easy to select a kernel which preserves the correlation structure of the various states. In
general, the empirical approximation can be extended further back in time to t −k. This
will result in a lag-k smoothing distribution
p(S t−k:t|Z1:t) ∝p(S t−k|Z1:t−k)
tY
t′=t−k+1
p(S t′ |S t′−1)p(Zt′ |S t′ ).
The above equation can be the target distribution of an MCMC implementation, in which
it is possible to obtain an estimate of the smoothing distributions such as p(S t−k+1|Z1:t). In
this case, at each time step t, the ﬁltering distribution output p(S t|Z1:t) can be stored for use
at k time steps later.
This chapter focuses on the joint distribution (12.2) for ﬁltering applications. At the
mth MCMC iteration, a mixture sampling procedure is adopted. This involves a joint MH
proposal step with probability PJ, where both S t and S t−1 are updated jointly, as well as
individual reﬁnement Metropolis-within-Gibbs steps [35] with probability 1 −PJ, where
S t and S t−1 are updated individually. This procedure is detailed in Algorithm 12.2 and will
be referred to as the MCMC-particles algorithm. From Tierney’s results [41], the resultant
MCMC chain produced is irreducible and aperiodic. This follows from the fact the joint
proposal step can explore the joint state space of the target distribution. As with all MCMC,
the burn-in of the chain is required to ensure proper convergence. Chain thinning can be
used at the expense of running the sampler longer to give the chain more time to mix. In
the algorithm, the individual reﬁnement step for S t−1 can have low acceptance probabilities
ρ2. This is because in high-dimensional systems the empirical predictive density
ˆp(S t|Z1:t−1) ∝
Z
p(S t|S t−1) ˆp(S t−1|Z1:t−1)dS t−1 ∝
Np
X
p=1
1
Np
p(S t|S t−1,p)
may be highly disjoint. In most cases, this step can be omitted with little impact on the
algorithm as S t−1 can still be moved in the joint proposal step. This will be seen in group
tracking simulations in later chapters.
As a comparison, [2] used the individual reﬁnement step to move the current state S t
as well as the particle representation S t−1. As highlighted above, this can potentially lead
to poor mixing in high-dimensional problems due to the highly disjoint predictive density
of the particle representation. On the other hand, [20] used the joint draw only to move the
MCMC chain. This can potentially reduce the eﬀectiveness of the MCMC as reﬁnement
moves are not employed to explore the structured probabilistic space.
12.2.3
Toy example
To study the MCMC-particles algorithm we consider a linear Gaussian state space model:
S t ∼N(S t|FS t−1, QW) , S 1 ∼N(0, QW),
(12.4)
Zt ∼N(Zt|HS t, QV) ,
(12.5)
where F = 0.9 × I20, QW = 0.25 × I20, H = I20, QV = 0.5 × I20 and N(·|µ, Q) is a Gaus-
sian distribution with mean µ and covariance Q. Here, S t = [S t,1, . . . , S t,20], and Id is the

Sequential inference for evolving groups of objects
251
Algorithm 12.2 MCMC-particles algorithm
Initialise particle set {S 0,p}
Np
p=1, ˆp(S 0) = PNp
p=1
1
Np δ(S 0 −S 0,p).
for t = 1 : T do
for m = 1 : NMCMC do
Draw u ∼U(u|0, 1).
if u < PJ then
A joint proposal is made for {S m
t , S m
t−1} using a MH step with acceptance
probability
ρ1 = f

p(S ∗
t , S ∗
t−1|Z1:t)q2(S m−1
t
|S ∗
t )q1(S m−1
t−1 |S ∗
t−1)
p(S m−1
t
, S m−1
t−1 |Z1:t)q2(S ∗
t |S m−1
t
)q1(S ∗
t−1|S m−1
t−1 )
.
Accept {S m
t , S m
t−1} = {S ∗
t , S ∗
t−1} with probability ρ1
else
S m
t−1 and S m
t are reﬁned in a series of Metropolis-within-Gibbs steps.
The acceptance probability for the reﬁning step of {S ∗
t−1} is given by
ρ2 = f

p(S ∗
t−1|S m−1
t
, Z1:t)q3(S m−1
t−1 |S ∗
t−1)
p(S m−1
t−1 |S m−1
t
, Z1:t)q3(S ∗
t−1|S m−1
t−1 )
.
Accept {S m
t−1} = {S ∗
t−1} with probability ρ2.
The acceptance probability for the reﬁning step of {S ∗
t } is given by
ρ3 = f

p(S ∗
t |S m
t−1, Z1:t)q4(S m−1
t
|S ∗
t )
p(S m−1
t
|S m
t−1, Z1:t)q4(S ∗
t |S m−1
t
)
.
(12.3)
Accept {S m
t } = {S ∗
t } with probability ρ3.
end if
After a burn-in period of Nburn, keep every Nthin MCMC output S t,p = S m
t as the
new particle set for approximating p(S t|Z1:t), i.e. ˆp(S t|Z1:t) = PNp
p=1
1
Np δ(S t −S t,p).
end for
end for
d × d identity matrix. Superﬁcially, a linear Gaussian state space model does not seem
to be a particularly diﬃcult problem, since the optimal ﬁltering distribution p(S t|Z1:t) can
be obtained from Kalman ﬁltering. However for simulation-based inference, the current
challenge is still to be able to do accurate joint inference for high-dimensional systems.
The above example is not easy for simulation-based inference precisely because of its high
dimensionality of 20. Here, a comparison of simulation-based estimates with the optimal
Kalman ﬁlter output can provide better insights into the eﬀectiveness of diﬀerent algorithms
than just a comparison with the true states. This problem can also be treated as d indepen-
dent ﬁltering problems. Here, however, we treat it as a joint inference problem, using the d
independent particle ﬁlters as a baseline for comparisons. It could be argued that correlated
structures reduce the eﬀective dimensionality and are hence easier targets for ﬁltering.
12.2.4
Inference algorithm
The MCMC-particles algorithm is used to design the inference algorithm for the toy exam-
ple. For the reﬁnement stage of S t in Eq. (12.3), the 20 variables S t,i are randomly permuted
at each MCMC iteration, and updated in 5 blocks of 4 variables each. At the mth MCMC
iteration, the joint proposal as well as individual reﬁnement proposals are given below.

252
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
1. For the joint proposal {S m
t , S m
t−1}, the target distribution is p(S t, S t−1|Z1:t). The
proposal function for the joint step is given by
q1(S t−1|S m−1
t−1 ) = ˆp(S t−1|Z1:t−1),
q2(S t|S m−1
t
) = p(S t|S t−1).
Here, ˆp(S t−1|Z1:t−1) is the particle approximation to the posterior distribution
p(S t−1|Z1:t−1) at t −1; p(S t, |S t−1) is the state transition density given by Eq. (12.4).
The acceptance probability ρ1 is given by
f

p(S ∗
t , S ∗
t−1|Z1:t)q2(S m−1
t
|S ∗
t )q1(S m−1
t−1 |S ∗
t−1)
p(S m−1
t
, S m−1
t−1 |Z1:t)q2(S ∗
t |S m−1
t
)q1(S ∗
t−1|S m−1
t−1 )
= f
 p(Zt|S ∗
t )
p(Zt|S m−1
t
)
!
.
2. For the individual reﬁnement steps, the particle state S t−1 is reﬁned ﬁrst, then this is
followed by each of the random blocks S t,iB4. Here, iB4 = {i1, i2, i3, i4} represents one
of the random blocks.
(a) The target distribution for the particle reﬁnement step is p(S t−1|S m−1
t
, Z1:t). The
proposal function q3(S t−1|S m−1
t−1 ) is given by
q3(S t−1|S m−1
t−1 ) = ˆp(S t−1|Z1:t−1) .
The acceptance probability ρ2 for the particle reﬁnement is given by
f

p(S ∗
t−1|S m−1
t
, Z1:t)q3(S m−1
t−1 |S m−1
t−1 )
p(S m−1
t−1 |S m−1
t
, Z1:t)q3(S ∗
t−1|S m−1
t−1 )
= f

p(S m−1
t
|S ∗
t−1)
p(S m−1
t
|S m−1
t−1 )
.
(b) For the individual reﬁning step for each random block S t,iB4, the target distribu-
tion is given by p(S t,iB4|S
′
t,\iB4, S m
t−1, Z1:t). The proposal function q4(S t,iB4|S m−1
t,iB4 )
is given by
q4(S t,iB4|S m−1
t,iB4 ) = p(S t,iB4 |S
′
t,\iB4, S m
t−1) .
Here, S
′
t represents all the individual state variables currently in the MCMC
chain, as each random block of variables gets updated progressively. Then,
S
′
t,\iB4 is similar to S
′
t, except it excludes the iB4 variables; p(S t,iB4 |S
′
t,\iB4, S m
t−1) is
a standard multivariate conditional Gaussian distribution, owing to the original
Gaussian distribution of the transition density p(S t|S t−1) (Eq. (12.4)).
The acceptance probability ρ3 for the reﬁnement of each individual random
block is given by
f

p(S ∗
t,iB4|S
′
t,\iB4, S m
t−1, Z1:t)q4(S m−1
t,iB4 |S ∗
t,iB4)
p(S m−1
t,iB4 |S
′
t,\iB4, S m
t−1, Z1:t)q4(S ∗
t,iB4|S m−1
t,iB4 )
= f

p(Zt|S
′
t,\iB4, S ∗
t,iB4)
p(Zt|S
′
t)
.
The parameters for the inference algorithm can be found in Table 12.1.
12.2.5
Comparison with sequential importance resampling particle ﬁlter
The MCMC-particles algorithm is compared with a standard sequential importance resam-
pling (SIR) particle ﬁlter (Algorithm 12.3) [6, 34]. The prior (12.4) is used as the
importance density, i.e. q(S t|S t−1,p, Zt) = p(S t|S t−1). Residual resampling is used at the

Sequential inference for evolving groups of objects
253
Tracking parameter
Symbol
Value
Number of particles
Np
6000
Number of burn-in iterations
Nburn
600
Chain thinning
Nthin
2
Probability for joint proposal step
PJ
0.75
Table 12.1 Parameters for MCMC-particles algorithm for the toy example.
Algorithm 12.3 Sequential importance resampling particle ﬁlter
Initialise particle set {S 0,p, 1
Np }
Np
p=1, p(S 0) = PNp
p=1
1
Np δ(S 0 −S 0,p).
for t = 1 : T do
for p = 1 : Np do
Propose S t,p ∼q(S t|S t−1,p, Zt).
Calculate
wt,p = wt−1,p × p(Zt|S t,p)p(S t,p|S t−1,p)
q(S t,p|S t−1,p, Zt)
.
end for
Normalise the weights by wt,p =
wt,p
PNp
p=1 wt,p .
Calculate ˆNef f [25] using
ˆNef f =
1
PN
p=1 wt,p2 .
if ˆNe f f < Nthr then
Resample the particle set {S t,p, wt,p}
Np
p=1 to obtain a new approximating particle set
{S t,p, 1
Np }
Np
p=1, i.e. p(S t|Z1:t) = PNp
p=1
1
Np δ(S t −S t,p).
end if
end for
Tracking parameter
Symbol
Value
Number of particles
Np
22000
Eﬀective sample size threshold
Nthr
0.9Np
Table 12.2 Parameters for SIR particle ﬁlter for the toy example.
resampling stage, when the eﬀective sample size Ne f f [25] is lower than a threshold Nthr.
Table 12.2 shows the parameters used for the SIR particle ﬁlter.
Both algorithms are implemented in Matlab. The parameters for each algorithm are
chosen such that the computational time is approximately equal. The initial distribution
p(X1) is assumed known. A total of 20 runs of the comparison are conducted. For each
run, the states and observations are generated from Eqs. (12.4) and (12.5) respectively. A
Kalman ﬁlter is used to compute the optimal ﬁltering distribution. The MCMC-particles
algorithm and the SIR particle ﬁlter are used to draw samples of the joint state S t given
the observations. The estimated mean by each of the algorithms is then compared with
the Kalman mean. The root mean square error (RMSE) is computed across all 20 runs, 20
dimensions and time samples from 11 to 100.
Figures 12.1(a) and 12.1(b) show the dispersion of error of the mean estimates of the
two algorithms with respect to the optimal Kalman mean over time. The MCMC-particles

254
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Error
Time samples
SIR error dispersion (RMSE=0.212)
(a) SIR particle ﬁlter.
10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Error
Time samples
MCMC−particles error dispersion (RMSE=0.093)
(b) MCMC-particles algorithm.
Figure 12.1 Box plots of the estimation error over time with respect to Kalman mean for SIR particle ﬁlter and
MCMC-particles algorithm. It can be seen that the error dispersion is higher for SIR particle ﬁlter.
Algorithm
RMSE
Distinct particles sampled
SIR particle ﬁlter
0.212
5.07%
MCMC-particles algorithm
0.093
1.81%
SIR particle ﬁlters (independent)
0.017
65.35%
Table 12.3 This table compares the RMSE and proportion of distinct particles sampled by the SIR particle ﬁlter,
the MCMC-particles algorithm and the independent SIR particle ﬁlters.
Algorithm estimates are tighter and closer to the optimal Kalman mean. This is also con-
ﬁrmed by Fig. 12.2 (columns 1 and 4) and Table 12.3. The better performance of the
MCMC-particles algorithm in this example can probably be explained by the introduction
of the individual reﬁnement stage. This local reﬁnement allows the output of the MCMC
to better approximate the posterior distribution.
For comparison purposes, 20 independent SIR particle ﬁlters are used to track the 20
independent variables of the toy example. Each of these particle ﬁlters uses 2000 particles,
which require approximately the same computational resources overall as the joint SIR
particle ﬁlter. The average RMSE over 20 runs is 0.017 (Table 12.3), far better than both
the MCMC-particles algorithm and the joint SIR particle ﬁlter. This clearly illustrates the
challenge of doing joint inference over large dimensions. Table 12.3 also shows the average
proportion of distinct particles sampled by the respective algorithm at each time step. For
the SIR particle ﬁlter, it is the percentage of distinct particles that survived the resampling
step. For the MCMC-particles algorithm, it is the percentage of distinct particles proposed
and accepted by the algorithm. The joint SIR particle ﬁlter on average kept 5.07% distinct
particles, while the MCMC-particles algorithm kept 1.81% distinct particles. This is some-
what counter-intuitive as it is generally thought that having a larger proportion of distinct
particles sampled will provide better results. It seems that the MCMC-particles algorithm
may be more eﬃcient at producing better particles. For comparison, the average proportion
of distinct particles sampled for the independent SIR particle ﬁlters is 65.35%.
Due to the ﬂexible implementation of the MCMC-particles algorithm, advances in both
MCMC and particle ﬁlter literature can be introduced to improve the algorithm. This can
range from choosing better particles in the auxiliary particle ﬁlter [32], to using multiple
interacting chains [14] or quasi Monte Carlo methods.

Sequential inference for evolving groups of objects
255
Figure 12.2 Box plot of the estimation error with respect to the
Kalman mean, comparing all seven algorithms.
12.2.6
Comparison with other algorithms
Auxiliary particle ﬁlter
The auxilary particle ﬁlter (APF) was ﬁrst introduced by [32]. The main idea of the APF
is to perform a biased resampling procedure of the set of particles at time t −1 such that
more particles are obtained at places where there are observations at time t. This gives a
better chance of simulating particles that might have low weights at time t −1 (for example
due to observation outliers), but potentially high weights at time t. The bias induced at
the resampling stage is undone exactly at time t by appropriate correction of the particle
weights. The auxiliary particle ﬁlter is summarised in Algorithm 12.4. From the perspective
of the MCMC-particles algorithm, this ‘biased’ particle selection can be viewed as a special
form of proposal densities q1(S t−1) and q3(S t−1) in Algorithm 12.2. The original paper on
APF [32] mentioned the use of MCMC in a similar way. The equivalent proposal densities
are given by
q1(S t−1|S m−1
t−1 ) = q3(S t−1|S m−1
t−1 ) =
Np
X
p=1
Lp
PNp
p=1 Lp
δ(S t−1 −S t−1,p),
where Lp is given in Algorithm 12.4. The MCMC-particles algorithm implementation with
the above proposal densities will be called the MCMC-particles-auxiliary algorithm. The
summary is shown in Algorithm 12.5.
Multiple chains
In many inference problems, a single MCMC chain may exhibit slow mixing behaviour, i.e.
it is slow to traverse the state space occupied by the target distribution. This is frequently
due to multimodal distributions or very peaky likelihood functions. To improve mixing, one
commonly used technique is to run multiple chains of MCMC, and introduce interactions
between the chains. In [14] the authors introduced one of the earliest examples of multiple
chain MCMC by using the product space formulation.
In this section, two further extensions are made to the MCMC-particles-auxiliary
algorithm. The ﬁrst extension is to run multiple independent chains of the single chain
Algorithm 12.5. When a particle output is required, it is selected randomly from the state
of one of the chains. There is no other interaction between the chains. This extension is
summarised in Algorithm 12.6.
The second extension is to run multiple chains of Algorithm 12.5, with each of the
chains having a diﬀerent target distribution. For example, the target distribution pc(S ) for

256
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
Algorithm 12.4 Auxiliary particle ﬁlter (APF)
Initialise particle set {S 0,p, 1
Np }
Np
p=1.
for t = 1 : T do
for p = 1 : Np do
Calculate Lp = p(Zt|E(S t|S t−1,p)) .
end for
Calculate ∀p ˜wt−1,p =
wt−1,pLp
PNp
p=1 wt−1,pLp .
Resample the particle set {S t−1,p, ˜wt−1,p}
Np
p=1 to obtain {S t−1,p, 1
Lp }
Np
p=1.
for p = 1 : Np do
Propose S t,p ∼q(S t|S t−1,p, Zt).
Calculate
wt,p = 1
Lp
p(Zt|S t,p)p(S t,p|S t−1,p)
q(S t,p|S t−1,p, Zt)
.
end for
Normalise the weights by wt,p =
wt,p
PNp
p=1 wt,p .
end for
Algorithm 12.5 MCMC-particles-auxiliary algorithm
Run the MCMC-particles algorithm (Algorithm 12.2) using the proposal q1(S t−1|S m−1
t−1 ) =
q3(S t−1|S m−1
t−1 ) = PNp
p=1
Lp
PNp
p=1 Lp δ(S t−1 −S t−1,p).
Algorithm 12.6 MCMC-particles-auxiliary algorithm with independent chains
Initialise and run Nc independent chains {S c
t−1, S c
t } of the MCMC-particles-auxiliary
algorithm (Algorithm 12.5).
When an output is required, select it randomly from one of the chains, i.e. S t,p = S c,m
t
.
Tracking parameter
Symbol
Value
Number of particles
Np
6000
Number of chains
Nc
4
Number of burn-in iterations
Nburn
600
Chain thinning
Nthin
2
Probability for joint proposal step
PJ
0.75
Table 12.4 Parameters for MCMC-particles-auxiliary algorithm with independent chains for the toy example.
Tracking parameter
Symbol
Value
Number of particles
Np
6000
Number of chains
NC
4
Number of burn-in iterations
Nburn
600
Chain thinning
Nthin
2
Probability for joint proposal step
PJ
0.75
Probability for exchange move
PEx
0.40
Table 12.5 Parameters for MCMC-particles-auxiliary algorithm with likelihood tempering for the toy example.

Sequential inference for evolving groups of objects
257
Tracking parameter
Symbol
Value
Number of particles
Np
22000
Number of MCMC iterations per particle
NR
1
Table 12.6 Parameters for SIR particle ﬁlter with resample-move step for the toy example.
chain c can be a tempered version of the original distribution p(S ), pc(S ) ∝(p(S ))υc, for
υc ∈(0, 1].
The product space formulation of multiple chain MCMC is very ﬂexible. Instead of
tempering the entire distribution, one can choose to temper only the likelihood function
(see Eq. (12.7)). For the toy example, for chain c, the observation covariance is ﬂattened by
multiplying by c
1
2 in Eq. (12.6). The joint distribution of the entire MCMC (12.8) is then
given by the product of the individual tempered distributions (12.7). An exchange move is
introduced to provide mixing and information exchange across the diﬀerent chains. Algo-
rithm 12.7 summarises the MCMC-particles-auxiliary algorithm with likelihood tempering.
Algorithm 12.7 MCMC-particles-auxiliary algorithm with likelihood tempering
Initialise and run Nc chains {S c
t−1, S c
t } of the MCMC-particles-auxiliary algorithm
(Algorithm 12.5), each having a target distribution given by Eq. (12.7).
After each MCMC iteration of all the chains, an exchange move is proposed randomly
with probability PEx. Select c1 and c2 from the Nc chains randomly. The acceptance
probability for the exchange move is given by
ρ4 = f
 pc1(Zt|S c2,m
t
)pc2(Zt|S c1,m
t
)
pc1(Zt|S c1,m
t
)pc2(Zt|S c2,m
t
)
!
.
Exchange {S c1,m
t
, S c1,m
t−1 } and {S c2,m
t
, S c2,m
t−1 } with probability ρ4.
When an output is required from the algorithm, it is selected from chain 1, i.e. S t,p =
S 1,m
t
.
pc(Zt|S c
t ) = N(S c
t , 0.5c
1
2 I20),
(12.6)
pc(S c
t , S c
t−1|Z1:t) ∝p(S c
t−1|Z1:t−1)p(S c
t |S c
t−1)pc(Zt|S c
t ),
(12.7)
p(S 1
t , S 1
t−1, · · · , S Nc
t , S Nc
t−1|Z1:t) ∝
Nc
Y
c=1
pc(S c
t , S c
t−1|Z1:t).
(12.8)
Resample-move particle ﬁlter
For comparison, we also use the standard SIR particle ﬁlter with a resample-move step to
improve the diversity of the particles. This is summarised in Algorithm 12.8. Table 12.6
shows the parameters used for the SIR particle ﬁlter.
Algorithm 12.8 Sequential importance resampling particle ﬁlter with resample-move step
Initialise and run a standard SIR particle ﬁlter algorithm (Algorithm 12.3).
After the resampling step, for each particle, initialise and run NR iterations of the MCMC
reﬁnement Step 2(b) (page 252).

258
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
Comparison of results
The results from the previous seven algorithms are summarised in Table 12.7 and the error
dispersion in Fig. 12.2. In general, the MCMC-particles algorithm and its various exten-
sions perform well compared to SIR and APF on this high-dimensional toy example. The
best performance is given by the MCMC-particles-auxiliary algorithm with independent
chains, though at the expense of approximately four times the computational resources of
the single chain MCMC-particles algorithm. The SIR particle ﬁlter with resample-move
step also uses approximately ﬁve times the computational resources, but only provided
small improvement to the result.
12.3
Group tracking
In many applications, especially military ones, targets tend to travel in groups or forma-
tions. This information can be exploited to provide better detection and tracking estimates.
However, it is not easy to obtain a complete set of probabilistic models which can describe
the group dynamics and group structure changes over time. As a result, [3] stated that
despite the potential advantages of the group tracking approach, implementation problems
such as recognising groups, incorporating new members into the groups, and splitting and
merging of groups have discouraged the further development of group tracking methods.
Nevertheless, [3] also mentioned that some form of group tracking is the best approach
for the tracking environment with many closely spaced targets. Closely spaced targets fre-
quently result in merged measurements. A complete probabilistic tracking model needs to
be able to deal with this as well.
In dynamic group tracking, ﬁrst, the targets themselves are dynamic, i.e. they have
some probabilistic models specifying their evolving motion. Second, the targets’ grouping
can change over time. For example, a group of four targets can split into two groups of two
targets each. Third, the assignment of a target to a group aﬀects the probabilistic properties
of the target dynamics. Fourth, the group statistics belong to a second hidden layer. The
Algorithm
RMSE
Distinct
Joint
Particle
particles
proposal
reﬁnement
sampled
acceptance
acceptance
rate
rate
SIR particle ﬁlter
0.212
5.07%
–
–
SIR particle ﬁlters with resample-move
0.196
4.88%
–
–
Auxiliary particle ﬁlter
0.138
2.71%
–
–
MCMC-particles algorithm
0.093
1.81%
0.76%
1.51%
MCMC-particles-auxiliary algorithm
0.069
3.65%
3.28%
9.81%
MCMC-particles-auxiliary
algorithm with independent chains
0.046
8.02%
3.32%
8.04%
MCMC-particles-auxiliary
algorithm with likelihood tempering
0.055
7.38%
3.91%
8.78%
SIR particle ﬁlters (independent)
0.017
65.35%
–
–
Table 12.7 This table summarises the results of the seven diﬀerent algorithms applied to the toy example, as
well as the independent SIR particle ﬁlters. It compares the RMSE, the proportion of distinct particles sampled,
the acceptance rate for the joint proposal step (page 252 Step 1), as well as the acceptance rate for the particle
reﬁnement step (page 252 Step 2(a)).

Sequential inference for evolving groups of objects
259
target statistics belong to the ﬁrst hidden layer and the observation process usually depends
only on the targets. Finally, the number of targets is typically unknown.
We introduce a general probabilistic framework for group models [30, 31] that incor-
porates these ideas. At time t, let Xt represent all the individual targets’ states, and Gt the
group structure or conﬁguration with function similar to the cluster label used in static clus-
tering problems. Finally let Zt represent the observations (described in more detail in later
sections). The dynamic group tracking model described above can then be written as
p(X1:t,G1:t, Z1:t) = p(X1|G1)p(G1)p(Z1|X1)
×
tY
t′=2
p(Xt′ |Xt′−1,Gt′ ,Gt′−1)p(Gt′ |Gt′−1, Xt′−1)p(Zt′ |Xt′ ),
(12.9)
where the observations, targets and grouping of targets are assumed Markovian as in stan-
dard tracking problems. The main components of the group tracking model involve the
following two dynamical models, the group dynamical model p(Xt|Xt−1,Gt,Gt−1) and the
group structure transition model p(Gt|Gt−1, Xt−1).
The group dynamical model describes the motion of members in a group. It accounts
for the interactions that exist between diﬀerent members. These can be interactions that
keep a group together, as well as other repulsive mechanisms that prevent members from
occupying the same physical space. Since many physical phenomena can be well described
using continuous time stochastic processes, ideally, such models should be developed from
a continuous time basis and put into an equivalent discrete time formulation. Owing to the
linear and Gaussian form of the group dynamical model which will be presented in the
following chapter, an exact equivalent discrete time solution exists. However, many other
stochastic processes may not have analytical solutions, which means that obtaining an exact
equivalent discrete time formulation from the continuous time model may not be possible.
The group structure transition model describes the way the group membership or group
structure Gt changes over time. This process can be dependent or independent of the kine-
matic states Xt. However, to design a suitable model is not straightforward. This chapter will
explore both state-dependent and state-independent models. The general group model given
by Eq. (12.9) can be used to model a rich set of group tracking problems. The following
sections will describe two such problems.
12.4
Ground target tracking
12.4.1
Basic group dynamical model
This section develops a new group dynamical model [30, 31], conditional upon the group
structure, using stochastic diﬀerential equations (SDEs) and Itˆo stochastic calculus. The
idea here is to adopt a behavioural model in which each member of a group reacts to
the behaviour of other members of that group, typically making its velocity and position
more like that of other members in the group. This idea can conveniently be formulated
in continuous time through a multivariate SDE, which will later be placed in discrete time
without approximation error, owing to the assumed linear and Gaussian form of the model.
The acceleration of each target in a group at a particular time t is calculated based on the
positions and velocities of surrounding targets in the group. A fairly general linear structure
for this can be written, for the ith target in a group. Suppose st,i ∈ℜd is the spatial position
of the ith target (where d = 2 or 3), then
d˙st,i = −α st,i −h(st) −γ˙st,i −β ˙st,i −g(˙st)	 dt + dW s
t,i + dBs
t .
(12.10)

260
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
Figure 12.3 Illustration of the restoring forces −α(st,i −
h(st)) for a ﬁve target group. Here h(st) is taken as the
group mean position vector. Restoring forces are shown
as grey arrows, while velocities are shown as black
arrows.
Here st,i is the Cartesian position of the ith target in a group at time t, with ˙st,i the corre-
sponding velocity; Bs
t is a d-dimensioned Brownian motion common to all targets, while
W s
t,i is another d-dimensioned Brownian motion assumed to be independently generated
for each target i in the group. The term Bs
t , then, models overall randomness in the motion
of the group as a whole, while W s
t,i models the individual randomness of each target in
the group. Functions h(·) and g(·) are state dependent functions as discussed below, and
st = [st,1, · · · , st,N] and ˙st = [˙st,1, · · · , ˙st,N] are the collection of all N targets’ positions and
velocities within a particular group, respectively.1
A Newtonian dynamics interpretation of Eq. (12.10) is that there are restoring forces
which adjust at each time instant to make, on average, the target position closer to the value
h(st), the velocity closer to g(˙st) and the velocity closer to zero. These three terms com-
prise the so-called ‘drift’ function of the SDE. The inclusion of the group and individual
Brownian motion terms W s
t,i and Bs
t then serves to model the randomness expected in the
behaviour of individuals and of the group as a whole. These latter two terms form the so-
called ‘diﬀusion’ part of the SDE. Figure 12.3 gives a graphical illustration of the restoring
forces towards the average group position h(st) for a small group of objects. The whole
system, when all targets within a group are coupled with similar equations to Eq. (12.10),
forms a multivariate SDE. In order to keep the system linear, the functions h(·) and g(·) are
chosen to be linear. In the simplest case, which has been experimented with successfully,
h(·) and g(·) are the mean of the position and velocity of the group, respectively, that is
h(st) = 1
N
N
X
j=1
st, j ,
g(˙st) = 1
N
N
X
j=1
˙st, j,
where N is the group size. This form is used in the simulations. Other forms which have
been experimented with success include the leave-one-out average which simply averages
over the N −1 remaining members of the group. The parameters α, β and γ are positive,
and reﬂect the strength of the pull towards the group means. Since this is essentially a
continuous-time stochastic feedback system, the stability of the system has to be carefully
monitored to prevent unstable oscillations from developing. The ‘mean reversion’ term γ˙st,i
prevents the velocities drifting up to very large values with time. A linear SDE for the joint
target state can be written as
dXt = AXtdt + BdWt + CdBt.
(12.11)
1One single group is assumed here for the sake of notational simplicity. This is generalised straightforwardly
for more than one group later.

Sequential inference for evolving groups of objects
261
Here, S t,i = (xt,i, yt,i)2 and Xt = [xt,1, ˙xt,1, yt,1, ˙yt,1, . . . , xt,N, ˙xt,N, yt,N, ˙yt,N, xt,N]T. The matrix
A ∈ℜ4N×4N is deﬁned as
A
=

A1
A3
· · ·
A3
A3
A3
A1
...
...
...
...
...
A1
A3
A3
· · ·
· · ·
A3
A1

,
where A1 = blkdiag[A2, A2] with A2 = [0 1; (−α +
α
N ) (−β −γ +
β
N )], and A3 =
blkdiag[A4, A4] with A4 = [0 0; α
N
β
N ]. The matrices B ∈ℜ4N×2N and C ∈ℜ4N×2
are deﬁned as B
=
blkdiag[B1, . . . , B1] with B1
=
[0 0; 1 0; 0 0; 1 0] and C
=
[BT
1, BT
1, · · · , BT
1]T. The multi-dimensional Brownian motions Wt and Bt are given by
Wt = [W x
t,1, Wy
t,1, ..., W x
t,N, Wy
t,N]T ,
Bt = [Bx
t , By
t ]T,
with covariance matrices QW = diag[σ2
x, σ2
y, . . . , σ2
x, σ2
y] and QB = diag[σ2
g, σ2
g] respec-
tively. Equation (12.11) can also be equivalently written as
dXt = AXtdt + DdMt,
where D
=
[B C] and Mt is a Brownian motion with covariance matrix QM
=
diag[σ2
x, σ2
y, ..., σ2
x, σ2
y, σ2
g, σ2
g]. The above SDE can be solved exactly using Itˆo’s stochastic
calculus [29]. The transition matrix FN for N targets is deﬁned as FN = Fτ = exp(Aτ).
Here, τ is typically set to be the time between successive observations. However, more
generally, τ need not be constrained by the observation interval, but evolves with its own
dynamics. This can be done in a more ﬂexible model based on variable rate formulation
[18, 19]. The corresponding covariance matrix of the above model is given by
QN = Qτ =
Z s=t+τ
s=t
exp(A(t + τ −s))DQMDT exp(A(t + τ −s))Tds.
The covariance matrix can be obtained using matrix fraction decomposition [36, 39]. Hence
the transition density of Xt is given by
pu(Xt|Xt−1) = N (Xt FNXt−1, QN) .
Basic group dynamical model with repulsive force
A way of preventing targets from becoming too spatially close or colliding is to introduce
repulsive forces between targets if they approach each other too closely. This is similar to
models in particle physics where two of protons experience a strong repulsive force if they
come too close to one another. A direct discrete time version of a related idea has also
been implemented by [12]. Here a similar model can be developed by including piecewise
constant repulsive force. The SDE that describes the forces acting on a target in a group
can be written as
d˙st,i = −α st,i −h(st) −γ˙st,i −β ˙st,i −g(˙st) + ri
	 dt + dW s
t,i + dBs
t .
2Note that here only a planar (2D) motion is considered. The models do readily extend to 3D motion if required.

262
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
Here, ri can be any suitably chosen piecewise constant force that depends only on the states
at t −1. The distribution of Xt is then given by a normal distribution
pu(Xt|Xt−1) = N(Xt|FNXt−1 + RNH, QN).
(12.12)
The mean of Xt+τ has to take into account the extra term due to the vector of the repulsive
force terms ri for all targets, H. The matrix RN =
R s=t+τ
s=t
exp(A(t+τ−s))ds can be calculated
using direct matrix exponential expansion or via eigenvalue decomposition.
Group model with virtual leader
The above framework can model targets moving with interacting velocities and positions
over an extended period of time. It can be contrasted with models such as the bulk velocity
model [21, 34] and the virtual leader-follower model [28], in which an additional state
variable is introduced to model the bulk or group parameter. This framework can be adapted
to include a latent virtual leader parameter too. This approach to group modelling is closer
in spirit to the work of [34, 28], although again the novel spatio-temporal structure above
is adopted for individuals within the group. The SDE for the virtual leader model is
d˙st,i = {−α[st,i −vs
t ] −γ1 ˙st,i −β[˙st,i −˙vs
t ]}dt + dW s
t,i,
d˙vs
t = −γ2˙vs
t dt + dBv
t ,
where vs
t is the unobserved ‘virtual leader’. The method outlined in the previous section
can also be used to solve this model. This formulation once again leads to a simple analytic
solution owing to the linear Gaussian structure, and hence may be embedded eﬃciently into
Kalman ﬁlter or Rao–Blackwellised schemes. The properties of this virtual-leader model
are unexplored as yet, but more ﬂexible behaviour may be expected since the virtual leader
is no longer a deterministic function of the group state. For example, the virtual leader need
not be directly part of the group. Instead, it can model the destination point of a group of
targets, which they will move towards. This model is introduced here as a variant of the
basic group model, which is currently the subject of further studies by [37].
12.4.2
Bayesian model for group target tracking
First, a basic probabilistic framework will need to be established for the problem. For each
target i, its state is represented at time t by Xt,i = [xt,i ˙xt,i yt,i ˙yt,i]T and Xt = [Xt,i, · · · , Xt,Nmax]T
is the joint states of all the Nmax individual targets. As described in the previous section, Gt
is used to represent the group structure of the targets at time t.
The aim of inference is to compute the posterior probability distribution p(Xt,Gt|Z1:t).
From this, one can infer estimates and conﬁdence values for all desired quantities such as
number of targets, their positions and velocities Xt, and their group conﬁguration Gt. Note
that Xt is a variable-dimensional quantity since targets can be added or deleted from the
scene randomly over time. Of course there is no reason either theoretically or computation-
ally why a variable dimension quantity such as this should not be maintained throughout
the model as is routinely done in problems of Bayesian model choice. However, in this
chapter, this is equivalent to formulating the existence process of targets explicitly in terms
of a set of existence variables et (see [17] for a more general treatment of model selection
using the composite model space formulation). Each et,i ∈{0, 1} models the existence pro-
cess of individual targets, where et,i = 1 models an active target, and et,i = 0 models an
inactive target. In this formulation, Xt is regarded as a ﬁxed-dimensional quantity with Nmax

Sequential inference for evolving groups of objects
263
elements, with each of them being active or inactive according to et,i. This is a reasonable
framework given that practical systems have computational and storage limitations.
Assuming a Markovian state transition, the standard Bayesian ﬁltering prediction and
update steps are given by
p(Xt, et,Gt|Z1:t) = p(Zt|Xt, et,Gt)p(Xt, et,Gt|Z1:t−1)
p(Zt|Z1:t−1)
,
p(Xt, et,Gt|Z1:t−1)
=
Z
p(Xt, et,Gt|Xt−1, et−1,Gt−1)p(Xt−1, et−1,Gt−1|Z1:t−1)dXt−1det−1dGt−1,
where Z1:t = [Z1 · · · Zt] and Zt′ are all the observations collected at time t′. The transition
probability model p(Xt, et,Gt|Xt−1, et−1,Gt−1) is written as
p(Xt, et,Gt|Xt−1,et−1,Gt−1) = p(Xt, et|Gt, Xt−1, et−1)p(Gt|Xt−1, et−1,Gt−1),
where p(Gt|Xt−1, et−1,Gt−1) is the group structure transition model, which will be described
in the following section, and p(Xt, et|Gt, Xt−1, et−1) is the target dynamical model given its
group structure Gt designed such that it is unaﬀected by the group structure at time t −1.
The motion of diﬀerent groups are assumed mutually independent. Hence,
p(Xt, et|Gt, Xt−1, et−1) =
Nγ(Gt)
Y
k=1
p(Xt,Λ(k,Gt), et,Λ(k,Gt)|Xt−1,Λ(k,Gt), et−1,Λ(k,Gt)),
(12.13)
where {Xt,Λ(k,Gt), et,Λ(k,Gt)} represents the joint target states for targets that belong to group
k (see the following section for the deﬁnition of Λ(k,Gt)). Each of the densities at the
right-hand side of the above equation can be further partitioned into birth, death and update
scenarios. The group dynamic update will be given by Eq. (12.12). While the groups are
assumed to evolve a priori independently of one another, one could also envisage repulsive
or attractive force mechanisms between group centroids. This can model higher-level group
interactions and could be incorporated into the general formulation above as required.
12.4.3
State-dependent group structure transition model
The group structure transition model depends very much on the type of group targets that
are being tracked. Generally, only ‘small’ changes in group structure are expected over
short time intervals. Certain group structure changes would also be considered highly
unlikely. In some scenarios, for example, two targets that are widely separated are per-
haps unlikely to join together within one group. One possible model for such transitions is
described in the sequel.
At each time t, the group structure is represented by a variable Gt. For example, for ﬁve
targets, the group structure representation Gt = [1 1 2 2 3]T means that Targets 1 and 2 are
in Group 1, Targets 3 and 4 are in Group 2, and Target 5 is in Group 3 (see Fig. 12.4). Note
that γ5(11) = [1 1 2 2 3]T is the notation for the example of group structure above. This
representation will be called the base group representation. The complete list of the base
group representation for four and ﬁve members can be found in Appendix 12.A.
The ﬁrst target is always labelled as Group 1. A single target is also considered as a
group. Hence, the maximum number of possible groups is ﬁve. In this way, the number of

264
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
possible group structures for ﬁve targets is given by the ﬁfth Bell number B5 = 52. Bell
numbers [5] can also be viewed as the number of distinct possible ways of putting Nmax
distinguishable balls into one or more indistinguishable boxes. This is exactly equivalent to
partitioning a set of Nmax targets into the various possible groups.
The total number of groups for each group structure is given by Nγ(Gt) = max(Gt). Let
Λ(g,Gt) be the set of targets that belong to group g of group structure Gt, and let NΛ(g,Gt)
be the corresponding number of targets in the group. A transition probability p(Gt|Gt−1)
represents the probability of group structure changes from Gt−1 at time t −1 to Gt at time t.
For example, using the group structure γ5(11) in Fig. 12.4, the total number of groups
is Nγ(γ5(11)) = 3, the set of targets in Group 1 is Λ(1, γ5(11)) = {1, 2}, and it contains
NΛ(1, γ5(11)) = 2 members in total. The following is a general structure for the group
transition probability,
p(Gt|Xt−1, et−1,Gt−1) =
( PNC
if Gt = Gt−1,
(1 −PNC) ˆp(Gt|Gt−1, Xt−1, et−1)
otherwise.
Here, the group structure is expected to remain unchanged with probability PNC and to
change with probability (1−PNC). The probability of change to a structure where Gt , Gt−1
is denoted with ˆp(Gt|Gt−1, Xt−1, et−1). Specifying ˆp(Gt|Gt−1, Xt−1, et−1) will be an important
consideration in the model. In simple cases this probability can be state-independent, i.e.
ˆp(Gt|Gt−1, Xt−1, et−1) = p(Gt|Gt−1), and the probabilities can be assigned on the basis of a
metric that measures how ‘similar’ Gt is to Gt−1, with higher probabilities assigned to more
similar conﬁgurations. (Another method of designing a state-independent group structure
transition model based on the dynamic Dirichlet distribution will be described later in Sec-
tion 12.5.) In the simulations here, however, a ﬂexible state-dependent model is used in
which ˆp(Gt|Gt−1, Xt−1, et−1) = ˆp(Gt|Xt−1, et−1), i.e. it depends only on the state and exis-
tence variables; ˆp(Gt|Xt−1, et−1) determines how information from Xt−1 and et−1 is used to
guide group changes. For two targets i and j, suppose a quantity qi,j ∈[0, 1] can be used to
give an indication of how likely they are to be in the same group at time t. For each group
structure h, let ∆be the set of all pairs of targets that belong to the same group. Then, a
scoring function such as the one below can be used
s(Gt = h|Xt−1, et−1) =
Y
i, j∈∆
qi, j
Y
i, j<∆
(1 −qi, j).
The advantage of the product form of this scoring function is that it tends to favour smaller
changes in group structure. The pseudo transition density ˆp(Gt = h|Xt−1, et−1) is deﬁned as
ˆp(Gt = h|Xt−1, et−1) =
s(Gt = h|Xt−1, et−1)
P
∀h′,h′,Gt−1 s(Gt = h′|Xt−1, et−1),
while qi,j is deﬁned as
qi,j =
( PQ
if et−1,i = 0 or et−1, j = 0;
k(Xt−1,i, Xt−1, j)
otherwise.
Figure 12.4 This is an example of a group struc-
ture variable for ﬁve targets split into three
groups. Using the base group representation
(Appendix 12.A), it is also denoted as γ5(11).

Sequential inference for evolving groups of objects
265
Here, PQ is a small number between 0 and 1 (e.g. 0.02) such that it allows for an inac-
tive target to be grouped with active targets. This gives the ability to add targets to groups;
k(Xt−1,i, Xt−1,j) is a function that assigns values between 0 and 1 measuring the similar-
ity between the kinematics of the pair of active targets. Those with similar kinematics
are judged more likely to form part of the same group. The following equation shows an
example of the function, which will be used later in the simulations
k(Xt−1,i, Xt−1, j) = exp
−R2
v
2σ2
Rv
exp
−R2
d
2σ2
Rd
,
Rv = ((˙xt−1,i −˙xt−1, j)2 + (˙yt−1,i −˙yt−1,j)2)
1
2 ,
Rd = max(0, ((xt−1,i −xt−1,j)2 + (xt−1,i −xt−1,j)2)
1
2 −Dsep).
Here, Rv and Rd are the relative speed and distance of the pair of targets Xt−1,i and Xt−1, j
respectively; σRv and σRd are the corresponding standard deviations; Dsep describes the
extent of a group.
12.4.4
Observation model
The observation model for the ground vehicle tracking analysed in the next section is
based on a discretised grid obtained with the association free or track-before-detect (TBD)
approach in [34, 26]. This permits the computation of the likelihood function without ﬁrst
requiring full enumeration of the association hypotheses. Here, each grid point or pixel
observation is modelled using simpliﬁed position only Rayleigh-distributed measurements
[26]. Thresholded measurements that return 1 or 0 for each pixel are employed. The proba-
bility of detection Pd,n for n targets in a single pixel space, given that the background false
alarm probability is set at P fa, is Pd,n = Pfa
1
1+n×S NR .
12.4.5
Simulation results
Tracking scenario
The group tracking algorithm will now be used to track a set of simulation data. For the
simulation, a single discretized sensor model is used which scans a ﬁxed rectangular region
of 100 by 100 pixels, where each pixel is 50m by 50m. Thresholded measurements are used
with Pd,1 = 0.7 for a single target. The false alarm probability for each pixel is Pfa = 0.002.
The sensor returns a set of observations every 5s. The group dynamical model (12.12) is
used to generate the simulation. The scenario (see Fig. 12.5(a)) consists of two groups of
two targets moving towards each other from time step 1 to 50, and then merging to form
a combined group from time step 51 to 100. Figure 12.5(b) shows one realisation of the
observation of the tracks. The parameters used to generate the motion of the group are
shown in Table 12.8.
The ﬁltering distribution for the dynamical model coupled with the observation model
is complex and highly non-linear. Section 12.2 illustrated that the MCMC-particles algo-
rithm and its various extensions can be eﬀective in high-dimensional problems. Therefore
the MCMC-particles algorithm will be used to design the inference algorithm. Using the
probabilistic framework from previous sections, the joint distribution of S t = {Xt, et,Gt}

266
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
0
1000
2000
3000
4000
5000
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Distance (m)
Distance (m)
Direction
of motion
(a) Simulation ground truth.
10
20
30
40
50
60
70
80
90
100
0
20
40
60
80
100
x−coordinate
Pixel
10
20
30
40
50
60
70
80
90
100
0
20
40
60
80
100
Time Step
Pixel
y−coordinate
(b) Simulation observations.
Figure 12.5 These ﬁgures show the ground truth and one realisation of the observations (Pd,1 = 0.7 and Pfa =
0.002) for simulation scenario 1. In this scenario, there are two groups of two targets each merging into a single
group. See plate section for colour version.
Simulation parameter
Symbol
Value
Time interval between measurements
τ
5 s
Actual number of targets
N
4
Number of simulation time steps
100
Centroid control parameter
α
0.0006 s−2
Group velocity control parameter
β
0.050 s−1
Individual velocity control parameter
γ
0.001s−1
Individual motion noise
σx
0.2 m s−1
Group motion noise
σg
0.5 m s−1
Repulsive force constants
R1
4 m2 s−2
R2
10 m
Table 12.8 Track simulation parameters for moving ground vehicles.
and S t−1 = {Xt−1, et−1,Gt−1} is given by
p(S t, S t−1|Z1:t) = p(Xt, et,Gt, Xt−1, et−1,Gt−1|Z1:t)
∝p(Xt−1, et−1,Gt−1|Z1:t−1)p(Xt, et,Gt|Xt−1, et−1,Gt−1)p(Zt|Xt, et).
The MCMC-particles algorithm is used to detect and track the group targets. A Monte
Carlo run of Nrun = 30 sets of the observations are generated for each of the scenarios. For
each run, all the particles are initialised as inactive (‘dead’) in order to allow the algorithm
to detect all targets unaided. At each time step, NMCMC = 25 000 MCMC iterations of both
the joint and individual proposals are performed. The initial Nburn = 1000 iterations are
used for burn-in. A chain thinning of Nthin = 6 is used and Np = 4000 MCMC output are
kept as particle approximation to p(Xt, et,Gt|Z1:t). The parameters used for tracking can be
found in Table 12.9. They are selected to facilitate smoother tracks.
For four targets, Table 12.13 in Appendix 12.A lists all the possible group combinations.
Note that many of the group structures are equivalent representations under the permutation
invariance of the targets, e.g. [1 1 2 2]T = [1 2 2 1]T = [1 2 1 2]T.
The existence variable can be used to declare a legitimate target being tracked. In addi-
tion, the variance of the state estimate can also help to indicate if a good track has been
achieved. The target track declaration method used here relies on little or no permutation
switching of target labels, hence the individual tracks can be identiﬁed and colour coded. It
is easy to envisage scenarios where this might break down. A systematic way to deal with

Sequential inference for evolving groups of objects
267
Tracking parameter
Symbol
Value
Time interval between measurements
τ
5 s
Maximum number of targets
Nmax
4
Centroid control parameter
α
0.0006 s−2
Group velocity control parameter
β
0.3 s−1
Individual velocity control parameter
γ
0.001 s−1
Individual motion noise
σx,σy
0.8 m s−1
Group motion noise
σg
0.5 m s−1
Repulsive force constants
R1
6m2 s−2
R2
10 m
Grouping constant for inactive target
PQ
0.02
Probability of group structure
PNC
0.4
remains unchanged
Grouping velocity standard deviation
σRv
6 m s−1
Grouping distance standard deviation
σRd
150 m
Grouping distance extent
Dsep
150 m
Target birth probability
PB
0.3
Target death probability
PD
0.05
Probability of detection for one target
Pd,1
0.5
Probability of false alarm
P fa
0.0014
Table 12.9 Tracking parameters for simulation results.
this issue is to use a relabelling algorithm [40]. In general, the scenarios used here pose
little or no problem with the posterior estimation of summary statistics.
Results
The estimated track for one set of results is shown in Fig. 12.6(a). The MCMC-particles
algorithm can be seen to have successfully detected and tracked the four targets. The ellipse
shows the mode of the group conﬁguration and the number indicates the number of targets
in the group. Quite clearly, the algorithm is able to infer the correct group structure. This
can also be seen in Fig. 12.7(a), which shows the time evolution of the group structure
probability. The change of grouping structure from [1 1 2 2]T to [1 1 1 1]T at t = 50 can be
seen clearly in the ﬁgure.
0
1000
2000
3000
4000
5000
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Distance (m)
Distance (m)
2 2 2
2222 2
4
2 2
2 2 2 2
2
4
4
4
4
4
4
4
4
4
4
Direction
of motion
(a) Tracking results with group models.
0
1000
2000
3000
4000
5000
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Distance (m)
Distance (m)
Direction
of motion
Track lost and
false track
Track lost
(b) Tracking results without group models.
Figure 12.6 These ﬁgures show one run of the result of the group tracking algorithm compared with a tracking
result with independent targets for simulation scenario 1. The ellipse shows the mode of the group conﬁguration,
labelled with the number of objects detected in the group. The results without group models show more erratic
behaviour in estimation. See plate section for colour version.

268
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
Time Step
20
40
60
80
100
[1 1 1 1]
[1 1 1 2]
[1 1 2 1]
[1 1 2 2]
[1 1 2 3]
[1 2 1 1]
[1 2 1 2]
[1 2 1 3]
[1 2 2 1]
[1 2 2 2]
[1 2 2 3]
[1 2 3 1]
[1 2 3 2]
[1 2 3 3]
[1 2 3 4]
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
(a) Group structure probability.
10
20
30
40
50
60
70
80
90
100
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
Time Step
Average number of targets
With group tracking
Without group tracking
(b) Average number of targets.
Figure 12.7 These ﬁgures show the group structure probability over time and the average number of targets over
30 runs (Pd,1 = 0.7 and Pfa = 0.002) for simulation scenario 1. The merging of the two groups can be seen in
the changes in group structure probabilities at time step 50. The average number of targets shows the non-group
based model’s tendency to miss targets in the detection process.
Another 30 Monte Carlo sets of the observations are generated. For these data, the
MCMC-particles algorithm is used without any group tracking. The targets are treated as
independent targets and there are no group structure transition or any group based interac-
tions. However, the posterior distribution is still a joint distribution due to the observations
and repulsive force model.
Figure 12.6(b) shows the estimated track of one set of the results. From the ﬁgure, it can
be seen that the tracks are less reliably initialised, the track estimates are noisier, and the
tracks are more prone to being lost. To achieve better detection and tracking performance
in the case without group tracking, the number of MCMC iterations has to be increased
signiﬁcantly. Even then, tracks can still be lost due to the large amount of uncertainty gen-
erated when the targets are closed together and measurements are missing. This illustrates
the beneﬁt of exploiting group information to improve tracking performance. For each run,
the average number of targets NXt at each time t is estimated by
E(NXt|Z1:t) ≈1
Np
Np
X
p=1
Nmax
X
i=1
et,i,p.
Figure 12.7(b) shows the average number of targets detected for both sets of 30 Monte Carlo
runs, i.e. with and without the group tracking. The average number of correctly detected-
targets is higher for the runs with the group tracking enabled. It shows that the algorithm is
able to detect all the four targets consistently with group tracking.
12.5
Group stock selection
In ﬁnancial trading, pairs trading is an investment strategy that exploits market instru-
ments that are out of equilibrium. It is a form of market neutral statistical arbitrage.
A brief history and discussion of pairs trading can be found in [13]. Pairs trading is
a non-directional strategy that identiﬁes two ﬁnancial instruments with similar charac-
teristics whose price relationship is outside of its historical range. The strategy simply
buys one instrument and sells the other in the hope that their price relationship tends
to ﬂuctuate around its average in the short term, while remaining stable over the long
term. In [11], pairs trading is modelled by using a mean reverting Gaussian Markov

Sequential inference for evolving groups of objects
269
chain model for the spread between two securities. Predictions from the calibrated model
are then compared with subsequent observations of the spread to determine appropriate
investment decisions such as time to buy and sell. To identify suitable pairs of market
instruments a common approach is to analyse the matrix of pairwise correlations between
all instruments.
In [7] a class of Bayesian models was introduced to model structured, conditional inde-
pendence relationships in the time-varying, cross-sectional covariance matrices of multiple
time series. This can be used to model groups of stocks by specifying a static graph (or
equivalently the group structure) G representing the pairwise relationships of the multiple
time series, and directly modifying the individual entries of the covariance matrix. The
model can be extended to deal with time-varying changes to the group structure.
In this section, we introduce a new model for identifying groups of stocks that might be
suitable for a more general version of pairs trading. This will be achieved through writing
explicit SDE expressions of the stock prices with group behaviour. The main assumptions
here are that stocks in the same group have similar long-term return, they are all aﬀected
by a common group noise, and they have mean reversion tendency towards a group mean.
As this approach analyses stocks as a group, it can potentially create a portfolio of
stocks which exploit the correlation amongst them to achieve investment returns. This is
in contrast to normal portfolio construction where the aim is to reduce correlation between
stocks to achieve the same return with lower risk. It is hoped that this may result in better
risk return proﬁle, compared to just simple pairs trading, which relies only on a pair rather
than a group of stocks.
12.5.1
Group stock mean reversion model
In stock markets, it is possible for market participants to identify and exploit highly cor-
related stocks which might be priced away from the long-run return µr. This might result
in ‘forced’ changes to stock prices that bring them back into equilibrium. To model this
behaviour more explicitly,
dXt,1
Xt,1
= (µr,1 −f1(Xt))dt + σX,1dBt,1 + σX,gdBt,g,
(12.14)
where f1(Xt) is some function of the stock prices within the same group. Here, f1(Xt) mod-
els a force term which brings the stock price back to equilibrium faster by temporarily
reducing the drift component µr,1; Bt,1, Bt,2 and Bt,g are standard Brownian motions, σX,g is
the volatility of the group and σX,1 and σX,2 are the volatilities of the individual stocks.
Solving Eq. (12.14) explicitly in closed form is not easy. One possibility is to simulate it
using Euler discretisation. Inference can be done using data imputation techniques such as
those of [20]. Another possibility is to consider that from time t to t+τ, the term f1(Xt) ≈F1
is nearly constant. In this case, the following SDE can be solved exactly,
dXt,1
Xt,1
= (µr,1 −F1)dt + σX,1dBt,1 + σX,gdBt,g.
The solution of the above equation is given by
Xt+τ,1 = Xt,1e

µr,1−F1−1
2 (σ2
X,1+σ2
X,g)

τ+σX,1(Bt+τ,1−Bt,1)+σX,g(Bt+τ,g−Bt,g).
(12.15)
A set of stocks can be modelled using Eq. (12.15). There are many possible methods of
constructing f1(Xt). One method, which is successfully applied, is to use the diﬀerence

270
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
Figure 12.8 Seven simulated stock price time series with
common group volatility and group mean reversion. See
plate section for colour version.
Simulation parameter
Symbol
Value
Time interval between measurements
τ
1
365 year
Number of stocks
Nmax
7
Individual stock annual volatility
σX,i
0.20
Group annual volatility
σX,g
0.20
Group mean reversion parameter
η
1
Individual stock annual mean return
µr,i
0.10
Table 12.10 Parameters for group stock model with group volatility and group mean reversion.
between the stock price and the group average of the stock prices, that is
f1(Xt) = η

1 −
1
NXt,1
N
X
i=1
Xt,i

,
(12.16)
where η is a control parameter which determines how strongly the stock prices will tend
towards the group mean, and N is the number of stocks that belong to the same group.
Another possible method of constructing f1(Xt) is to use the exponentially weighted mean
return for the stock.
Figure 12.8 shows a set of stock prices generated using Eqs. (12.15) and (12.16) with
the parameters shown in Table 12.10. The stock prices exhibit strongly clustered behaviour.
The group mean reversion stock model will be used for the group stock selection problem
in the next section.
12.5.2
State-independent group structure model
In this section, a diﬀerent representation model for the group structure Gt is used. Assum-
ing that there are K possible groups, then for each target i an assignment variable gt,i ∈
{1, . . . , K} can be deﬁned such that it indicates which group the target is in. The new group
structure representation Gt is given by Gt = [gt,1 gt,2 · · · gt,Nmax]. It is straightforward to
map the new representation using the assignment variable to the base group representation
in Appendix 12.A, for example
Gt = [1 1 3 3 5] 7→γ5(11).
This mapping provides a way to study the changes in group structure probability. There
is a many-to-one mapping from the new representation described above to the Base Group
representation. For example, diﬀerent combinations of the group assignment variables such
as Gt = [2 1 2 1 4] map to the same base group representation γ5(11).
Now, a state-independent group structure transition model p(Gt, πt|Gt−1, πt−1) will be
developed. Here, Gt = [gt,1 gt,2 · · · gt,Nmax], where gt,i ∈{1, . . . , K} is the group assignment

Sequential inference for evolving groups of objects
271
of target i, and πt models the underlying proportion of targets in various groups at time t.
The main aim is to deﬁne a reasonable model for p(Gt, πt|Gt−1, πt−1). Consider the general
state-independent group structure transition model where
p(Gt, πt|Gt−1, πt−1) = p(πt|πt−1)p(Gt|Gt−1, πt) = p(πt|πt−1)
Nmax
Y
i=1
p(gt,i|gt−1,i, πt), (12.17)
where p(πt|πt−1) is a distribution that describes the way the underlying proportion of the
targets in various groups changes over time (this distribution will be deﬁned later in this sec-
tion, and termed the dynamic Dirichlet distribution), and p(gt,i|gt−1,i, πt) is the probability
model for the changes in group assignment, deﬁned as
p(gt,i = k|gt−1,i = l, πt) =
( PG + (1 −PG)πt,k
if gt,i = gt−1,i,
(1 −PG)πt,k
otherwise.
If πt = π0 is some ﬁxed quantity, then it is also straightforward to show that the distribution
of gt,1:Nmax over the diﬀerent groups tends to π0 after some time.
The main idea of a dynamic Dirichlet distribution is to induce reasonable dependency
between successive proportions πt over time. This can be useful in modelling time-varying
proportions of topics in a set of documents [4, 42, 38], as well as changes in group structure
or group membership over time.
Here, we use a dynamic Dirichlet distribution that has the form of a simple Dirichlet
distribution (written as Dir(·|α0) with parameters α0) as the time marginal. While there is
no particular reason for this, it is hoped that by doing so many results that exist for the
Dirichlet distribution in the literature can be used. At time t = 0, let K be the number of
groups, α0 = {α0,1, . . . , α0,K}, α0,i = α0,C/K and α0,C = PK
i=1 α0,i. The initial proportion
π0 is given by π0 ∼Dir(π0|α0). There are many possible ways of inducing dependence
between successive proportions πt. To achieve the desired grouping eﬀect, the dependence
is induced using a correlated version of the parameter vector αt
αt = ζα0,Cπt−1 + (1 −ζ)α0,
where 0 ≤ζ ≤1 is a control parameter to determine the amount of correlation that exists
between Dirichlet distributions at time t and t−1. If ζ = 1, we recover the dynamic Dirichlet
distribution in [42], if ζ = 0 we have an independent Dirichlet distribution. Hence, this
distribution will be written as
πt|πt−1 ∼Dir(πt|αt) ∼DDD(πt|πt−1, α0, ζ),
(12.18)
where DDD(·|πt−1, α0, ζ) is the dynamic Dirichlet distribution with parameters α0 and ζ.
Note that the precision parameter αt,C = PK
i=1 αt,i is constant at each time step t.
12.5.3
Bayesian model for group structure analysis
Given a set of stocks, the task is to analyse how the diﬀerent stocks are grouped together
over time. The grouping is represented by the group structure variable Gt. A simple
Bayesian model for the group stocks will be introduced in this section. For Nmax stocks,
let Zt = [Zt,1 Zt,2 · · · Zt,Nmax] be the logarithm of the stock price Xt,i given by Eq. (12.15), i.e.
Zt,i = ln(Xt,i). Then, the joint distribution can be written as
p(G1:t, π1:t, Z1:t) = p(G1, π1)p(Z1|G1)
tY
t′=2
p(Gt′, πt′|Gt′−1, πt′−1)p(Zt′|Zt′−1,Gt′),

272
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
100
200
300
400
500
600
700
−0.5
0
0.5
1
1.5
Time Samples (Day)
Stock Price (log scale)
Stock 1
Stock 2
Stock 3
Stock 4
Stock 5
Figure 12.9 Stock price time series of simulation sce-
nario 1 with two groups of two (Stocks 1 and 2) and
three stocks (Stocks 3, 4 and 5) each. See plate section
for colour version.
where the group structure transition model p(Gt, πt|Gt−1, πt−1) is the state-independent
dynamic Dirichlet distribution given by Eq. (12.17), with Gt = [gt,1 gt,2 · · · gt,Nmax]. The
group structure G1 is initialised where all the stocks are assumed independent, and each
stock price starts at Z1,i = 0.
Similarly to Eq. (12.13), diﬀerent groups of stocks are assumed to be mutually inde-
pendent, and stocks in the same group will be modelled by the group mean reversion
model(12.15). For example, if Gt indicates that ﬁve stocks are grouped into two groups
where g1 = {1, 3} and g2 = {2, 4, 5}, then the stock prices are given by
p(Zt|Zt−1,Gt) = p(Zt,g1|Zt−1,g1)p(Zt,g2|Zt−1,g2),
and the transition p(Zt,g1|Zt−1,g1) is obtained from the log-distribution of Eq. (12.15).
12.5.4
Simulation results
In this section, one simulated scenario will be used to study the ability of the group stock
models introduced in Section 12.5.3 to identify correct stock groupings. This scenario con-
sists of two groups of stocks with two and three stocks respectively. They begin as separate
groups, and after 365 days, the ﬁve stocks merge to behave as a single group. The simu-
lated stock prices are generated using the parameters in Table 12.10. Figure 12.9 shows the
simulated stock prices. The correct group structure is γ5(10) = [1 1 2 2 2]T before merging,
and γ5(1) = [1 1 1 1 1]T after merging.
Inference of the group structure {Gt, πt} is achieved using a MCMC-particles algorithm
(Algorithm 12.2). The joint distribution of S t = {Gt, πt} and S t−1 = {Gt−1, πt−1} is given by
p(Gt, πt,Gt−1, πt−1|Z1:t) ∝p(Zt|Zt−1,Gt)p(Gt, πt|Gt−1, πt−1)p(Gt−1, πt−1|Z1:t−1).
The parameters in Table 12.11 are used for the group structure inference. It is assumed that
the mean return µr,i and the volatilities σX,i and σX,g are known. A more complex model can
treat these quantities as unknowns, and infer them jointly with the group structure variables.
For analysis purposes, the output of the group structure model is mapped to the base group
representation used in the previous sections and Appendix 12.A.
Figure 12.10(a) shows the posterior distribution of the group structure p(Gt|Z1:t). From
the ﬁgure it can be seen that the correct group structures are clearly identiﬁed with a change
in group structure to a single group after 365 days. The average number of groups over time
Ng
t can be estimated from the posterior distribution of the group structure. For ﬁve stocks
and using the base group representation in Appendix 12.A, Ng
t is given as
Ng
t =
52
X
k=1
Nγ(γ5(k))p(Gt = γ5(k)|Z1:t),

Sequential inference for evolving groups of objects
273
Tracking parameter
Symbol
Value
Time interval between measurements
τ
1
365 year
Number of stocks
Nmax
5
Individual stock annual volatility
σX,i
0.20
Group annual volatility
σX,g
0.20
Group mean reversion parameter
η
1
Individual stock annual mean return
µr,i
0.10
Number of groups
K
5
Precision parameter
α0,C
35
Correlation parameter
ζ
0.95
Probability of staying in the
PG
0.96
same group
Table 12.11 Tracking parameters for group stock simulation.
Time samples (day)
Group structure
100
200
300
400
500
600
700
5
10
15
20
25
30
35
40
45
50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(a) Group structure probability.
100
200
300
400
500
600
700
0
1
2
3
4
5
6
Time samples (day)
Average number of groups
Average number of groups
One standard deviation line
(b) Average number of groups.
Figure 12.10 These ﬁgures shows the posterior distribution of the group structure and average number of groups
Ng
t for Simulation Scenario 1. There is a clear change in group structure to a single group after 365 days. This is
also indicated by a change in average group number.
where Nγ(γ5(k)) refers to the total number of groups in group structure γ5(k).
Figure 12.10(b) shows the average number of groups over time. These models can therefore
identify groupings of stocks based only on their stock price behaviour.
12.6
Conclusions
This chapter has presented a new approach to model dynamically evolving groups of
objects. This is achieved by casting the problem in a general probabilistic framework,
and modelling the group interactions of dynamic objects and the evolving group structure
over time. The approach permits a uniﬁed treatment to various inference goals involving
dynamic groupings, from group target detection and tracking to group structure inference.
A novel dynamical model for group target tracking is developed using SDEs with cou-
pled dynamics. The model, named as the basic group dynamical model, is based on the
interactions of group targets with each other. Owing to the linear and Gaussian formulation,
the group model can be solved exactly to give a Gaussian transition density for the equiv-
alent discrete time model. With appropriately chosen parameters, it can generate realistic
group tracks for ground vehicles. A complete Bayesian model for group target detection
and tracking is then developed. A general state-dependent group structure transition model
is developed to model the changes in group structure over time. The group model is then

274
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
γ4
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Target 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
Target 2
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
Target 3
1
1
2
2
2
1
1
1
2
2
2
3
3
3
3
Target 4
1
2
1
2
3
1
2
3
1
2
3
1
2
3
4
Table 12.12 Unique group representations for four targets.
used to track vehicles in a simulated scenario. It is able to successfully detect and track sim-
ulated vehicles, as well as to infer the correct group structures. The results also demonstrate
the improvement in performance through exploiting group information.
The use of complex group models in a high-dimensional setting poses signiﬁcant chal-
lenges to joint inference using simulation based methods such as sequential Monte Carlo.
This is illustrated in a simple high-dimensional toy example. To overcome this problem,
this chapter has introduced a sequential MCMC algorithm called the MCMC-particles
algorithm to approximate the posterior distribution of the high-dimensional states under
the Bayesian inference framework. This algorithm can be considered to be a generalisa-
tion of some existing sequential MCMC algorithms, which allows for further extensions
and insights. As the algorithm is based on MCMC, it is very ﬂexible and some of the
most powerful methods in Monte Carlo inference, such as parallel chain tempering and
local state exploration, can be brought to bear on sequential state estimation problems. The
results shown in a toy example demonstrate the eﬀectiveness of this method.
This chapter has also presented an alternative method to model group structure tran-
sition which is based on the dynamic Dirichlet distribution and is state independent. An
advantage of this state-independent group structure transition model is that it does not
require complete enumeration of the group structure in order to compute the transition
probabilities or to propose from the distribution. This is applied to a diﬀerent application
taken from ﬁnance. The problem here is to identify groups of stocks that might be suitable
for trading. This is achieved through writing explicit SDE expressions of the stock prices
with group behaviour. The main assumptions here are that stocks in the same group have
a similar long term return, they are all aﬀected by a common group noise, and they have
γ5
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Target 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
Target 2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
Target 3
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
1
1
1
Target 4
1
1
2
2
2
1
1
1
2
2
2
3
3
3
3
1
1
1
Target 5
1
2
1
2
3
1
2
3
1
2
3
1
2
3
4
1
2
3
γ5
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
Target 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
Target 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
Target 3
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
3
Target 4
2
2
2
3
3
3
3
1
1
1
2
2
2
3
3
3
3
1
Target 5
1
2
3
1
2
3
4
1
2
3
1
2
3
1
2
3
4
1
γ5
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
Target 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
Target 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
Target 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
Target 4
1
1
1
2
2
2
2
3
3
3
3
4
4
4
4
4
Target 5
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
5
Table 12.13 Unique group representations for ﬁve targets.

Sequential inference for evolving groups of objects
275
mean reversion tendency towards a group mean. Simulation results show that the group
stock models can identify groupings of stocks based on their price behaviour.
12.A
Appendix: Base group representation
Tables (12.12) and (12.13) list the unique group structure γ for four and ﬁve targets. For
example, γ5(11) = [1 1 2 2 3]T represents that Targets 1 and 2 are in a group, Targets 3 and
4 are in another group and Target 5 is independent and not grouped.
Bibliography
[1] C. Andrieu, A. Doucet and R. Holenstein.
Particle Markov chain Monte Carlo methods.
Journal of Royal Statistical Society Series B,
72:1–33, 2010.
[2] C. Berzuini, N. G. Best, W. R. Gilks and
C. Larizza. Dynamic conditional independence
models and Markov chain Monte Carlo methods.
Journal of the American Statistical Association,
440:1403–1412, 1997.
[3] S. S. Blackman and R. Popoli. Design and
Analysis of Modern Tracking Systems. Artech
House, 1999.
[4] D. M. Blei, A. Y. Ng and M. I. Jordan. Latent
Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022, March 2003.
[5] P. J. Cameron. Combinatorics: Topics,
Techniques, Algorithms. Cambridge University
Press, 1994.
[6] O. Capp´e, S. J. Godsill and E. Moulines. An
overview of existing methods and recent
advances in sequential Monte Carlo. Proceedings
of the IEEE, 95:899–924, May 2007.
[7] C. M. Carvalho and M. West. Dynamic
matrix-variate graphical models. Bayesian
Analysis, 2:69–98, 2007.
[8] T. C. Clapp and S. J. Godsill. Fixed-lag
smoothing using sequential importance
sampling. In J. M. Bernardo, J. O. Berger, A. P.
Dawid and A. F. M. Smith, editors, Bayesian
Statistics VI, pages 743–752. 1998.
[9] F. E. Daum and M. Krichman. Meshfree adjoint
methods for nonlinear ﬁltering. In Proceedings
of the IEEE Aerospace Conference, page 16,
2006.
[10] A. Doucet, S. J. Godsill and C. Andrieu. On
sequential Monte Carlo sampling methods for
Bayesian ﬁltering. Statistics and Computing,
10:197–208, 2000.
[11] R. J. Elliott, J. V. D. Hoek and W. P. Malcolm.
Pairs trading. Quantitative Finance, 5:271–276,
2005.
[12] M. Fallon and S. J. Godsill. Multi target acoustic
source tracking using track before detect. In
Proceedings of the IEEE Workshop on
Applications of Signal Processing to Audio and
Acoustics, pages 102–105, 2007.
[13] E. G. Gatev, W. N. Goetzmann and K. G.
Rouwenhorst. Pairs trading: performance of a
relative average arbitrage rule. NBER Working
Paper 7032, 1999.
http://www.nber.org/papers/w7032.
[14] C. Geyer. Markov chain Monte Carlo maximum
likelihood. In E. Keramigas, editor, Computing
Science and Statistics: The 23rd symposium on
the interface, pages 156–163, 1991.
[15] W. R. Gilks and C. Berzuini. Following a
moving target: Monte Carlo inference for
dynamic Bayesian models. Journal of the Royal
Statistical Society. Series B (Statistical
Methodology), 63:127–146, 2001.
[16] W. R. Gilks, S. Richardson and D. J.
Spiegelhalter. Markov Chain Monte Carlo in
Practice. Chapman and Hall/CRC, 1996.
[17] S. J. Godsill. On the relationship between
Markov chain Monte Carlo methods for model
uncertainty. Journal of Computational and
Graphical Statistics, 10(2):230–248, 2001.
[18] S. J. Godsill and J. Vermaak. Models and
algorithms for tracking using trans-dimensional
sequential Monte Carlo. In Proceedings of the
IEEE International Conference on Acoustics,
Speech and Signal Processing, volume 3, pages
976–979, 2004.
[19] S. J. Godsill, J. Vermaak, W. Ng and J. Li.
Models and algorithms for tracking of
manoeuvring objects using variable rate particle
ﬁlters. Proceedings of the IEEE, 95(5):925–952,
2007.
[20] A. Golightly and D. J. Wilkinson. Bayesian
sequential inference for nonlinear multivariate
diﬀusions. Statistics and Computing, pages
323–338, 2006.
[21] N. J. Gordon, D. J. Salmond and D. Fisher.
Bayesian target tracking after group pattern
distortion. In O. E. Drummond, editor, Signal
and Data Processing of Small Targets, volume
3163, pages 238–248. SPIE, 1997.
[22] N. J. Gordon, D. J. Salmond and A. F. M. Smith.
Novel approach to nonlinear/non-Gaussian
Bayesian state estimation. In IEEE Proceedings
of Radar and Signal Processing, volume 140,
pages 107–113, 1993.
[23] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola and
L. K. Saul. An introduction to variational

276
Sze Kim Pang, Simon J. Godsill, Jack Li et al.
methods for graphical models. In M. I. Jordan,
editor, Learning in Graphical Models, pages
183–233. MIT Press, 1999.
[24] Z. Khan, T. Balch and F. Dellaert. MCMC-based
particle ﬁltering for tracking a variable number
of interacting targets. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
27:1805–1819, 2005.
[25] A. Kong, J. S. Liu and W. H. Wong. Sequential
imputation and Bayesian missing data problems.
Journal of the Americian Statistical Association,
89:278–288, March 1994.
[26] C. Kreucher, M. Morelande, K. Kastella and
A. O. Hero. Particle ﬁltering for multitarget
detection and tracking. IEEE Transactions on
Aerospace and Electronic Systems,
41:1396–1414, 2005.
[27] J. Liu and M. West. Combined parameter and
state estimation in simulation-based ﬁltering. In
A. Doucet, N. Freitas, and N. Gordon, editors,
Sequential Monte Carlo in Practice, pages
197–217. Springer-Verlag, 2001.
[28] R. P. S. Mahler. Statistical
Multisource-Multitarget Information Fusions.
Artech House, 2007.
[29] B. Øksendal. Stochastic Diﬀerential Equations:
An Introduction with Applications (Sixth
Edition). Springer-Verlag, 2003.
[30] S. K. Pang, J. Li and S. J. Godsill. Models and
algorithms for detection and tracking of
coordinated groups. In Proceedings of the IEEE
Aerospace Conference, 2008.
[31] S. K. Pang, J. Li and S. J. Godsill. Detection and
tracking of coordinated groups. IEEE
Transactions on Aerospace and Electronic
Systems, 47(1):472–502, 2011.
[32] M. K. Pitt and N. Shephard. Filtering via
simulation: Auxiliary Particle Filter. Journal of
the American Statistical Association,
94:590–599, 1999.
[33] N. G. Polson, J. R. Stroud and P. M¨uller.
Practical ﬁltering with sequential parameter
learning. Journal of the Royal Statistical Society,
pages 413–428, 2008.
[34] B. Ristic, S. Arulampalam and N. Gordon.
Beyond the Kalman Filter – Particle Filters for
Tracking Applications. Artech House, 2004.
[35] C. P. Robert and G. Casella. Monte Carlo
Statistical Methods – Second Edition. Springer,
2004.
[36] S. Sarkka. Recursive Bayesian inference on
stochastic diﬀerential equations. PhD Thesis,
Helsinki University of Technology, 2006.
[37] F. Septier, S. K. Pang, A. Carmi and S. J.
Godsill. Tracking of coordinated groups using
marginalised MCMC-based Particle Algorithm.
In Proceedings of the IEEE Aerospace
Conference, 2009.
[38] N. Srebro and S. Roweis. Time-varying topic
models using dependent Dirichlet processes. The
University of Chicago Technical Report
UTML-TR-2005-003, March 2005.
[39] R. F. Stengel. Optimal Control and Estimation.
Dover Publications, 1994.
[40] M. Stephens. Dealing with label switching in
mixture models. Journal of the Royal Statistical
Society. Series B (Statistical Methodology),
62:795–809,
2000.
[41] L. Tierney. Markov chains for exploring
posterior distributions. Annals of Statistics,
22:1701–1786, 1994.
[42] X. Wei, J. Sun and X. Wang. Dynamic mixture
models for multiple time series. In Proceedings
of the International Joint Conference on
Artiﬁcial Intelligence, pages 2909–2914, 2007.
Contributors
Sze Kim Pang, Signal Processing Laboratory, Department of Engineering, University of Cambridge
Simon J. Godsill, Signal Processing Laboratory, Department of Engineering, University of
Cambridge
Jack Li, Signal Processing Laboratory, Department of Engineering, University of Cambridge
Franc¸ois Septier, Institut TELECOM/TELECOM Lille 1, France
Simon Hill, Signal Processing Laboratory, Department of Engineering, University of Cambridge

13
Non-commutative harmonic analysis
in multi-object tracking
Risi Kondor
13.1
Introduction
Simultaneously tracking n targets in space involves two closely coupled tasks: estimat-
ing the current positions x1, x2, . . . , xn of their tracks, and estimating the assignment
σ: {1, 2, . . . , n} →{1, 2, . . . , n} of targets to tracks. While the former is often a relatively
straightforward extension of the single target case, the latter, called identity management
or data association, is a fundamentally combinatorial problem, which is harder to ﬁt in a
computationally eﬃcient probabilistic framework.
Identity management is diﬃcult because the number of possible assignments grows
with n!. This means that for n greater than about 10 or 12, representing the distribution
p(σ) explicitly as an array of n! numbers is generally not possible.
In this chapter we discuss a solution to this problem based on the generalisation
of harmonic analysis to non-commutative groups, speciﬁcally, in our case, the group of
permutations. According to this theory, the Fourier transform of p takes the form
bp(λ) =
X
σ∈Sn
p(σ) ρλ(σ),
where Sn denotes the group of permutations of n objects, λ is a combinatorial object called
an integer partition, and ρλ is a special matrix-valued function called a representation. These
terms are deﬁned in our short primer on representation theory in Section 13.2.
What is important to note is that, since ρλ is matrix-valued, each Fourier component
bp(λ) is a matrix, not just a scalar. Apart from this surprising feature, non-commutative
Fourier transforms are very similar to their familiar commutative counterparts.
In particular, we argue that there is a well-deﬁned sense in which some of the bp(λ)
matrices are the ‘low-frequency’ components of p, and approximating p with this subset
of components is optimal. A large part of this chapter is focused on how to deﬁne such a
notion of ‘frequency’, and how to ﬁnd the corresponding Fourier components. We describe
two seemingly very diﬀerent approaches to answering this question, and ﬁnd, reassuringly,
that they give exactly the same answer.
Of course, in addition to a compact way of representing p, eﬃcient inference also
demands fast algorithms for updating p with observations. Section 13.6 gives an overview
of the fast Fourier methods that are employed for this purpose.

278
Risi Kondor
13.1.1
Related work
The generalisation of harmonic analysis to non-commutative groups is based on represen-
tation theory, which, sprouting from the pioneering work of Frobenius, Schur and others at
the turn of the twentieth century, has blossomed into one of the most prominent branches
of algebra. The symmetric group (as the group of permutations is known) occupies a cen-
tral position in this theory. For a general introduction to representation theory the reader is
referred to [19], while for information on the symmetric group and its representations we
recommend [18].
For much of the twentieth century, generalised Fourier transforms were the exclusive
domain of pure mathematicians. It was not until the 1980s that connections to statistics
and applied probability became widely recognised, thanks in particular to the work of Persi
Diaconis and his collaborators. The well-known book [3] covers a wealth of topics ranging
from ranking to card shuﬄing, and presages many of the results that we describe below, in
particular with regard to spectral analysis on permutations.
Also towards the end of the 1980s a new ﬁeld of computational mathematics started
emerging, striving to develop fast Fourier transforms for non-commutative groups. The
ﬁrst such algorithm for the symmetric group is due to Clausen [2]. Later improvements
and generalizations can be found in [15] and [16]. For an overview of this ﬁeld, including
applications, see [17].
The ﬁrst context in which non-commutative harmonic analysis appeared in machine
learning was multi-object tracking. This chapter is based on [12], where this idea was ﬁrst
introduced. Huang et al. [7] extended the model by deriving more general Fourier space
updates, and later introduced an alternative update scheme exploiting independence [5].
The journal article [6] is a tutorial quality overview of the subject.
Besides tracking, Fourier transforms on the symmetric group can also be used to con-
struct permutation invariant representations of graphs [11, 14], deﬁne characteristic kernels
on groups [4, 9], and solve hard optimisation problems [10]. An analogue of compressed
sensing for permutations is discussed in [8].
13.2
Harmonic analysis on ﬁnite groups
This section is intended as a short primer on representation theory and harmonic analysis
on groups. The reader who is strictly only interested in identity management might wish to
skip to Section 13.3 and refer back to this section as needed for the deﬁnitions of speciﬁc
terms.
A ﬁnite group G is a ﬁnite set endowed with an operation G ×G →G (usually denoted
multiplicatively) obeying the following axioms:
G1. For any x, y ∈G, xy ∈G (closure).
G2. For any x, y, z ∈G, x(yz) = (xy)z (associativity).
G3. There is a unique e ∈G, called the identity of G, such that ex = xe = x for any x ∈G.
G4. For any x ∈G, there is a corresponding element x−1∈G called the inverse of x, such
that x x−1 = x−1 x = e.
One important property that is missing from these axioms is commutativity, xy = yx. Groups
that do satisfy xy = yx for all x and y are called commutative or Abelian groups. A simple
example of a ﬁnite commutative group is Zn = {0, 1, . . . , n−1}, the group operation being
addition modulo n. The group of permutations that appears in tracking problems, however,
is not commutative.

Harmonic analysis in multi-object tracking
279
Finite groups are quite abstract objects. One way to make them a little easier to handle
is to ‘model’ them by square matrices that multiply the same way as the group elements
do. Such a system of matrices (ρ(x))x∈G obeying ρ(x)ρ(y) = ρ(xy) for all x, y ∈G is called
a representation of G. In general, we allow representation matrices to be complex valued.
Abstractly, a representation ρ is then a function ρ: G →Cdρ×dρ, where dρ is called the
degree or the dimensionality of ρ.
Once we have found one representation ρ of G, it is fairly easy to manufacture other
representations. For example, if T is an invertible dρ-dimensional matrix, then ρ′(x) =
T −1ρ(x) T is also a representation of G. Pairs of representations related to each other in this
way are said to be equivalent.
Another way to build new representations is by taking direct sums: if ρ1 and ρ2 are two
representations of G, then so is ρ1 ⊕ρ2, deﬁned
(ρ1 ⊕ρ2)(x) = ρ1(x) ⊕ρ2(x) =
 ρ1(x)
0
0
ρ2(x)
!
.
Just as ρ′ is essentially the same as ρ, ρ1 ⊕ρ2 is also not a truly novel representation.
Representations which cannot be reduced into a direct sum of smaller representations (i.e.,
for which there is no matrix T and smaller representations ρ1 and ρ2 such that ρ(x) =
T −1 (ρ1(x) ⊕ρ2(x)) T for all x ∈G) are called irreducible.
A key goal in developing the representation theory of a given ﬁnite group is to ﬁnd a
complete set of inequivalent irreducible representations. We will denote such a system
of representations by RG, and call its members irreps for short. Just as any natural number
can be expressed as a product of primes, once we have RG any representation of G can be
expressed as a direct sum of irreps from RG, possibly conjugated by some matrix T. By a
basic theorem of representation theory, if G is a ﬁnite group then RG is of ﬁnite cardinality,
and is well deﬁned in the sense that if R′
G is a diﬀerent system of irreps then there is a
bijection between RG and R′
G mapping each ρ to a ρ′ with which it is equivalent. Abelian
groups are special in that all their irreps are one-dimensional, so they can be regarded as
just scalar functions ρ: G →C.
The concept of irreps is exactly what is needed to generalise Fourier analysis to groups.
Indeed, the exponential factors e−2πikx appearing in the discrete Fourier transform
bf(k) =
X
x∈{0,...,n−1}
e−2πikx f(x)
are nothing but the irreps of Zn. This suggests that the Fourier transform on a non-
commutative ﬁnite group should be
bf(ρ) =
X
x∈G
f(x) ρ(x),
ρ ∈RG.
(13.1)
At ﬁrst sight it might seem surprising that f is a function on G, whereas bf is a sequence
of matrices. It is also strange that the Fourier components, instead of corresponding to
diﬀerent frequencies, are now indexed by irreps. In other respects, however, Eq. (13.1)
is very similar to the familiar commutative Fourier transforms. For example, we have an
inverse transform
f(x) =
1
|G |
X
ρ∈RG
dρ trρ(x)−1 bf(ρ),
(13.2)

280
Risi Kondor
and Eq. (13.1) also satisﬁes a generalised form of Parseval’s theorem (more generally
referred to as Plancherel’s theorem), stating that with respect to the appropriate matrix
norms, f 7→bf is a unitary transformation.
Another important property inherited from ordinary Fourier analysis is the convolution
theorem. On a non-commutative group, the convolution of two functions f and g is deﬁned
( f ∗g)(x) =
X
y∈G
f(xy−1) g(y).
(13.3)
The convolution theorem states that each component of the Fourier transform of f ∗g is
just the matrix product of the corresponding components of bf and bg, that is
( d
f ∗g)(ρ) = bf(ρ) · bg(ρ).
(13.4)
The translation and correlation theorems have similar non-commutative analogues.
13.2.1
The symmetric group
The mapping σ: {1, 2, . . . , n}→{1, 2, . . . , n} from targets to tracks is eﬀectively a permu-
tation of the set {1, 2, . . . , n}. The product of two permutations is usually deﬁned as their
composition, i.e., (σ2σ1)(i) = σ2(σ1(i)) for all i ∈{1, 2, . . . , n}. It is easy to check that with
respect to this notion of multiplication the n! diﬀerent permutations of {1, 2, . . . , n} form a
non-commutative ﬁnite group. This group is called the symmetric group of degree n, and
is denoted Sn.
To compute the Fourier transform of the assignment distribution p(σ), we need to study
the representation theory of the symmetric group. Fortunately, starting with the pioneering
work of the Rev Alfred Young at the turn of the twentieth century, mathematicians have
invested a great deal of eﬀort in exploring the representation theory of the symmetric group,
and have discovered a wealth of beautiful and powerful results. Some of the questions to
ask are the following: (1) How many irreps does Sn have and how shall we index them? (2)
What is the dimensionality of each irrep ρ and how shall we index the rows and columns
of ρ(σ)? (3) What are the actual ρ(σ)
i,j matrix entries? To answer these questions Young
introduced a system of combinatorial objects, which in his honour we now call Young
diagrams and Young tableaux.
A partition of n, denoted λ ⊢n, is a k-tuple λ = (λ1, λ2, . . . , λk) satisfying Pk
i=1 λi = n
and λi+1 ≤λi for i = 1, 2, . . . k−1. The Young diagram (Ferrers diagram) of λ just consists
of λ1, λ2, . . . , λk boxes laid down in consecutive left-justiﬁed rows. For example,
is the Young diagram of λ = (4, 3, 1). A Young tableau is a Young diagram in which the
boxes are bijectively ﬁlled with the numbers 1, 2, . . . , n, and a standard Young tableau is
a Young tableau in which in each row the numbers increase from left to right and in each
column they increase from top to bottom. For example,
1 2 5 8
3 4 7
6
is a standard Young tableau of shape λ = (4, 3, 1). The set of all Young tableaux of shape λ
we denote Tλ.

Harmonic analysis in multi-object tracking
281
Young discovered that there are exactly as many irreps in RSn as there are partitions
of n. Thus, instead of frequencies, in the case of the symmetric group we use partitions to
index the irreps. Even more remarkably, if we employ the correct bijection between irreps
and partitions, the dimensionality dλ := dρλ of ρλ is the same as the number of standard
tableaux of shape λ. This suggests indexing the rows and columns of ρλ(σ) by standard
tableaux: instead of talking about the (i, j)-element of the matrix ρλ(σ), where i and j
are integers, we will talk about the (t, t′)-element, where t and t′ are standard tableaux of
shape λ.
As regards deﬁning the values of the actual ρλ(σ)
t,t′ matrix entries, a number of dif-
ferent alternatives are described in the literature, of which the most convenient one for
our present purposes is Young’s orthogonal representation, which we will abbreviate as
YOR. In the following, whenever we refer to irreps of Sn, we will implicitly always be
referring to this system of irreducible representations.
To deﬁne YOR we need a more compact way to denote individual permutations than
just writing σ = [s1, s2, . . . , sn], where s1 = σ(1), . . . , sn = σ(n). The usual solution is cycle
notation. A cycle (c1, c2, . . . , ck) in σ is a sequence such that σ(c1) = c2, . . . , σ(cn−1) =
cn and σ(cn) = c1. The cycle notation for σ consists of listing its constituent cycles, for
example σ = [2, 3, 1, 5, 4] would be written (1, 2, 3) (4, 5). Clearly, this uniquely deﬁnes σ.
Any i that are ﬁxed by σ form single-element cycles by themselves, but these trivial cycles
are usually omitted from cycle notation. The cycle type µ = (µ1, µ2, . . . , µℓ) of σ is the
length of its cycles, listed in weakly decreasing order.
The notion of cycles and cycle type suggest some special classes of permutations.
The simplest permutation is the identity e, which is the unique permutation of cycle type
(1, 1, . . . , 1). Next, we have the class of transpositions, which are the permutations of cycle
type (2, 1, . . . , 1). Thus, a transposition is a single two-cycle σ = (i, j), exchanging i with
j and leaving everything else ﬁxed. Adjacent transpositions are special transpositions of
the form τi = (i, i+1).
We deﬁne YOR by giving explicit formulae for the representation matrices of adjacent
transpositions. Since any permutation can be written as a product of adjacent transpositions,
this deﬁnes YOR on the entire group. For any standard tableau t, letting τi(t) be the tableau
that we get from t by exchanging the numbers i and i + 1 in its diagram, the rule deﬁning
ρλ(τi) in YOR is the following: if τi(t) is not a standard tableau, then the column of ρλ(τi)
indexed by t is zero, except for the diagonal element ρλ(τi)
t,t = 1/dt(i, i + 1); if τi(t) is a
standard tableau, then in addition to this diagonal element, we also have a single non-zero
oﬀ-diagonal element ρλ(τi)
τk(t),t = (1−1/dt(i, i+1)2)1/2. All other matrix entries of ρλ(τi)
are zero. In the above dt(i, i+1) = ct(i+1) −ct(i), where c(j) is the column index minus the
row index of the cell where j is located in t.
Young’s orthogonal representation has a few special properties that are worth noting at
this point. First, despite having stressed that representation matrices are generally complex-
valued, the YOR matrices are, in fact, all real. It is a special property of Sn that it admits a
system of irreps which is purely real. The second property is that, as the name suggests, the
YOR matrices are orthogonal. In particular, ρλ(σ−1) = ρλ(σ)⊤. Third, as is apparent from
the deﬁnition, the ρλ(τi) matrices are extremely sparse, which will turn out to be critical
for constructing fast algorithms. Finally, and this applies to all commonly used systems
of irreps for Sn, not just YOR, the representation corresponding to the partition (n) is the
trivial representation ρ(n)(σ) = 1 for all σ ∈Sn.

282
Risi Kondor
13.3
Band-limited approximations
Combining Eq. (13.1) with the representation theory of Sn tells us that the Fourier transform
of the assignment distribution p is the sequence of matrices
bp(λ) := bp(ρλ) =
X
σ∈Sn
p(σ) ρλ(σ),
λ ⊢n.
Regarding p as a vector p ∈RSn, this can also be written componentwise as
bp(λ)
t,t′ = ⟨p, uλ
t,t′⟩,
where
uλ
t,t′ =
X
σ∈Sn
ρλ(σ)
t,t′ eσ,
and (eσ)σ∈Sn is the canonical orthonormal basis of RSn. From this point of view, the Fourier
transform is a series of projections to the subspaces
Vλ = span{ uλ
t,t′ | t, t′ ∈Tλ },
called the isotypics of RSn. By the unitarity of the Fourier transform, the isotypics are
pairwise orthogonal and together span the entire space.
The key idea of this chapter is to approximate p by its projection to some subspace W,
expressible as a sum of isotypics W =
L
λ∈Λ Vλ. The question is how we should choose W
so as to retain as much useful information about p(σ) as possible.
In the following we discuss two alternative approaches to answering this question. In
the ﬁrst approach, presented in Section 13.4, we deﬁne a Markov process governing the
evolution of p, and argue that W should be the subspace least aﬀected by stochastic noise
under this model. We ﬁnd that under very general conditions this subspace is indeed a
sum of isotypics, speciﬁcally, in the most natural model for identity management, W =
L
λ∈Λk Vλ, where Λk = { λ ⊢n | λ1 ≥n −k }. The integer parameter k plays a role akin to
the cutoﬀfrequency in low-pass ﬁltering.
In the second approach, in Section 13.5, we ask the seemingly very diﬀerent question of
what sequence of subspaces U1, U2, . . . of RSn capture the ﬁrst-order marginals p(σ(i) = j),
second-order marginals p(σ(i1) = j1, σ(i2) = j2), and so on, up to order k. Surprisingly, we
ﬁnd that the answer is again Uk =
L
λ∈Λk Vλ.
13.4
A hidden Markov model in Fourier space
Just as in tracking a single target, the natural graphical model to describe the evolution
of the assignment distribution p(σ) in identity management is a hidden Markov model.
According to this model, assuming that at time t the distribution is pt(σ), in the absence of
any observations, at time t+1 it will be
pt+1(σ′) =
X
σ∈Sn
p(σ′|σ) pt(σ),
(13.5)
where p(σ′|σ) is the probability of transitioning from assignment σ to σ′. For example, if a
pair of targets i1 (assigned to track j1) and i2 (assigned to track j2) come very close to each
other, there is some probability that due to errors in our sensing systems their assignment
might be ﬂipped. This corresponds to transitioning from σ to σ′ = τσ, where τ is the
transposition ( j1, j2).

Harmonic analysis in multi-object tracking
283
When we do have an observation O at t + 1, by Bayes’ rule the update takes on the
slightly more complicated form
pt+1(σ′) =
p(O|σ′) P
σ∈Sn p(σ′|σ) pt(σ)
P
σ′′∈Sn p(O|σ′′) P
σ∈Sn p(σ′′|σ) pt(σ).
As an example, if we observe target i at track j with probability π, then
p(O|σ′) =

π
if σ(i) = j,
(1−π)/(n−1)
if σ(i) , j.
(13.6)
Generally, in identity management we are interested in scenarios where observations are
relatively infrequent, or the noise introduced by the transition process is relatively strong.
Hence, the natural criterion for choosing the right form of band-limiting should be stability
with respect to Eq. (13.5).
13.4.1
A random walk on Sn
Equation (13.5) describes a random walk on Sn with transition matrix Pσ′,σ = p(σ′|σ). In
particular, starting from an initial distribution p0, in the absence of observations, after t time
steps the assignment distribution will be
pt = Pt p0.
(13.7)
As for random walks in general, the evolution of this process is governed by the spec-
tral structure of P. Assuming that P is symmetric and its eigenvalues are α1 ≥α2 ≥. . . ≥
αn! ≥0 with corresponding orthonormal eigenvectors v1, v2, . . . , vn! and p0 = Pn!
i=1 p(i)
0 vi,
at time t,
pt =
n!
X
i=1
αt
i p(i)
0 vi.
(13.8)
Clearly, the modes of p corresponding to low values of α will rapidly decay. To make pre-
dictions about p, we should concentrate on the more robust, high α modes. Hence, ideally,
the approximation subspace W should be spanned by these components.
In most cases we of course do not know the exact form of P. However, there are some
general considerations that can still help us ﬁnd W. First of all, it is generally reasonable to
assume that the probability of a transition σ 7→σ′ should only depend on σ′ relative to σ.
In algebraic terms, letting σ′ = τσ, p(τσ|σ) must only be a function of τ, or equivalently,
p(σ′|σ) = q(σσ−1) for some function q: Sn →R. Plugging this into Eq. (13.5) gives
pt+1(σ′) =
X
σ∈Sn
q(σ′σ−1) pt(σ),
which is exactly the convolution of pt with q, as deﬁned in Eq. (13.3). Thus, by Eq. (13.4),
in Fourier space bpt+1(λ) = bq(λ) · bpt(λ), and, in particular,
bpt(λ) = bq(λ)t · bp0(λ).
(13.9)
Thus, the Fourier transform eﬀectively block-diagonalises Eq. (13.7). From a computa-
tional point of view this is already very helpful: raising the bq(λ) matrices to the tth power
is much cheaper than doing the same to the n!-dimensional P.

284
Risi Kondor
13.4.2
Relabelling invariance
Continuing the above line of thought, q(τ) must not depend on how we choose to label the
tracks. More explicitly, if prior to a transition σ 7→τσ we relabel the tracks by permuting
their labels by some µ ∈Sn, then apply τ, and ﬁnally apply µ−1 to restore the original
labelling, then the probability of this composite transition should be the same as that of
τ, i.e.,
q(µ−1τµ) = q(τ)
∀µ ∈Sn.
(13.10)
Expressing the left-hand side by the inverse Fourier transform (13.2) and using the
orthogonality of YOR gives
q(µ−1τµ) = 1
n!
X
λ
dλ tr [ ρλ(µ−1τ−1µ) · bq(λ)]
= 1
n!
X
λ
dλ tr [ ρλ(µ−1) · ρλ(τ−1) · ρλ(µ) · bq(λ) ]
= 1
n!
X
λ
dλ tr [ ρλ(τ−1) · ρλ(µ) · bq(λ) · ρλ(µ)⊤].
It is relatively easy to see that for this to equal
q(τ) = 1
n!
X
λ
dλ tr [ ρλ(τ−1) · bq(λ) ]
for all τ and µ, we must have T ⊤bq(λ) T = bq(λ) for all orthogonal matrices T, which in
turn implies that each bq(λ) is a multiple of the identity. This result is summarised in the
following theorem.
Theorem 13.1 If the transition probabilities p(σ′|σ) = q(σ′σ−1) are relabelling-invariant
in the sense of Eq. (13.10), then the Fourier transform of q is of the form
bq(λ) = qλ Idλ,
λ ⊢n,
where (qλ)λ⊢n are scalar coeﬃcients and Idλ denotes the dλ-dimensional identity matrix.
Theorem 13.1 puts a very severe restriction on the form of q. Plugging into Eq. (13.9)
it tells us that in Fourier space the equation governing our random walk is simply
bpt(λ) = qt
λ bp0(λ).
At a more abstract level, Theorem 13.1 establishes a connection between the diﬀerent sub-
spaces of Rn corresponding to the diﬀerent Fourier components (the isotypics) and the
eigenspectrum of P.
Theorem 13.2 If p(σ′|σ) = q(σ′σ−1) is relabelling-invariant, then the eigenvectors
v1, v2, . . . , vn! of P can be re-indexed by {λ ⊢n} and i = 1, 2, . . . , d2
λ so that {vλ
i }
d2
λ
i=1 all share
the same eigenvalue qλ, and together span the isotypic Vλ.
Hence, the diﬀerent ‘modes’ of p referred to in connection with Eq. (13.8) are exactly
its Fourier components! In this sense, approximating p by retaining its high qλ Fourier com-
ponents is an optimal approximation, just as in ordinary Fourier analysis low-pass ﬁltering
is optimal in the presence of high frequency noise.

Harmonic analysis in multi-object tracking
285
13.4.3
Walks generated by transpositions
To ﬁnd which Fourier components have high qλ values, we need to be more speciﬁc about
our random walk. In particular, we make the observation that while in a given ﬁnite interval
of time many diﬀerent subsets of targets and tracks may get exchanged, in most real-world
tracking scenarios it is reasonable to assume that by making the interval between subse-
quent time steps suﬃciently short, in each interval at most a single pair of targets will get
swapped. Thus, there is no loss in restricting the set of allowable transitions to just single
transpositions. Since any two transpositions τ1 and τ2 are related by τ2 = µ−1τ1 µ for some
µ, by relabelling invariance the probability of each transposition is the same, reducing the
random walk to
p(σ′|σ) =

β
if σ′ = (i, j) · σ for some 1 ≤i < j ≤n,
1 −
n
2

β
if σ′ = σ,
0
otherwise,
governed by the single (generally small) scalar parameter β. Now, by the argument leading
to Theorem 13.1, we know that P
1≤i< j≤n ρλ((i, j)) is a multiple of the identity, in particular,
X
1≤i<j≤n
ρλ((i, j)) = 1
dλ
X
1≤i<j≤n
tr ρλ((i, j)) Idλ.
In general, the function χλ(σ) = tr ρλ(σ) is called a character of Sn, and obeys
χλ(µ−1σµ) = tr
h
ρλ(µ−1) · ρλ(σ) · ρλ(µ)
i
= tr
h
ρλ(σ) · ρλ(µ) · ρλ(µ)−1i
= tr ρλ(σ) = χλ(σ)
for any µ and σ. Hence, χλ(τ) is the same for all transpositions τ, and choosing (1, 2) as the
archetypal transposition, we can write
X
1≤i< j≤n
ρλ((i, j)) =
 n
2
! χλ((1, 2))
dλ
Idλ.
Plugging into the Fourier transform and using the fact that for the identity permutation e,
ρλ(e) = Idλ for all λ yields that
qλ = 1 −β
 n
2
!  
1 −χλ((1, 2))
dλ
!
.
This type of expression appears in various discussions of random walks over Sn, and, as
derived in [3], it may be written explicitly as
qλ = 1 −β
n
2

(1−r(λ))
where
r(λ) =
n
2
−1 X
i
λi
2

−
λ′
i
2

,
(13.11)
where λ′ is the transpose of λ, which we get by ﬂipping its rows and columns.
In general, we ﬁnd that qλ is highest for ‘ﬂat’ partitions, which have all their squares
concentrated in the ﬁrst few rows. The exact order in which qλ drops starts out as follows:
1 = q(n) ≥q(n −1,1) ≥q(n −2,2) ≥q(n −2,1,1) ≥q(n −3,3) ≥
q(n −3,2,1) ≥q(n −3,1,1,1) ≥q(n −4,4) ≥. . .
(13.12)

286
Risi Kondor
For a principled band-limited approximation to p we cut oﬀthis sequence at some point
and only retain the Fourier matrices of p up to that point. It is very attractive that we can
freely choose where that cutoﬀshould be, establishing an optimal compromise between
accuracy and computational expense.
Unfortunately, this freedom is somewhat limited by the fact that the dimensionality of
the representations (and hence, of the Fourier matrices) grows very steeply as we move
down the sequence (13.12). As we mentioned, ρ(n) is the trivial representation, which is
one-dimensional. To ﬁnd the dimensionality of the next representation, ρ(n−1,1), we must
consider all standard tableaux of shape
.
Here and in the following we draw Young diagrams as if n = 8, but it should be understood
that it is the general pattern that matters, not the exact number of boxes. In standard tableaux
of the above shape, any of the numbers 2, 3, . . . , n can occupy the single box in the second
row. Once we have chosen this number, the rest of the standard tableau is fully determined
by the ‘numbers increase from left to right and top to bottom’ rule. Hence, in total, there
are n−1 standard tableaux of this shape, so bp((n−1, 1)) is an n−1-dimensional matrix.
Similarly, standard tableaux of shapes
and
are determined by the numbers that occupy the two boxes in the second (and third) rows,
so there are O(n2) standard tableaux of each of these two shapes.
In general, the number of standard tableaux of a given shape is given by the so-called
hook rule (see, e.g., [18]), stating that
dλ =
n!
Q
b ℓ(b),
where the product extends over all boxes of the diagram, and ℓ(b) is the number of boxes to
the right of b plus the number of boxes beneath b plus one. The dimensionalities given by
this formula for the ﬁrst few partitions in the sequence (13.12) are displayed in Table 13.1.
More important than the actual dλ values in the table is the observation that in general,
dλ grows with nn−λ1. Thus, in practice, it makes sense to cut oﬀthe Fourier expansion after
(a) the ﬁrst two Fourier components bp(n), bp(n −1,1)
	; or
(b) the ﬁrst four Fourier components bp(n), bp(n −1,1), bp(n −2,2), bp(n −2,1,1)
	; or
λ
dλ
(n)
1
(n−1, 1)
n −1
(n−2, 2)
n(n−3)
2
(n−2, 1, 1)
(n−1)(n−2)
2
(n−3, 3)
n(n−1)(n−5)
6
(n−3, 2, 1)
n(n−2)(n−4)
3
λ
dλ
(n−3, 1, 1, 1)
(n−1)(n−2)(n−3)
6
(n−4, 4)
n(n−1)(n−2)(n−7)
24
(n−4, 3, 1)
n(n−1)(n−3)(n−6)
8
(n−4, 2, 2)
n(n−1)(n−4)(n−5)
12
Table 13.1 The size of the ﬁrst few irreps of Sn. For concreteness the diagrams are drawn as if n = 8.

Harmonic analysis in multi-object tracking
287
(c) the ﬁrst seven Fourier components {bp(n), bp(n −1,1), bp(n −2,2), bp(n −2,1,1), bp(n −3,3),
bp(n −3,2,1), bp(n −3,1,1,1)}.
Going beyond these ﬁrst, second and third ‘order’ Fourier matrices would involve O(n4)-
dimensional matrices, which for n in the mid teens or greater is infeasible.
13.4.4
The continuous time limit
The random walk analysis that we just presented is a somewhat simpliﬁed version of the
account given in [12]. One of the diﬀerences is that the derivations in that paper were
framed in terms of the graph Laplacian
∆σ′,σ =

−1
β p(σ′|σ)
if σ′ , σ,
1
β
P
τ,σ p(τ|σ)
if σ′ = σ,
of the weighted graph corresponding to the random walk. The transition matrix P is
expressed in terms of the graph Laplacian as P
=
I −β∆. In particular, for the
transposition-induced random walk of Section 13.4.3,
∆σ′,σ =

−1
if σ′ = (i, j) · σ for some 1 ≤i < j ≤n,
n
2

if σ′ = σ,
0
otherwise.
The eigenvalues and eigenvectors of the graph Laplacian are also referred to as the spectrum
of the corresponding graph.
In general, given a subset S of a ﬁnite group G, the graph with vertex set G in which x
and y are adjacent if and only if x−1y ∈S is called the Cayley graph of G generated by S .
Thus, by our earlier results, we have the following theorem.
Theorem 13.3 The eigenvalues of the Cayley graph of Sn generated by transpositions are
αλ =
 n
2
!  
1 −χλ((1, 2))
dλ
!
=
 n
2
!
(1 −r(λ)) ,
λ ⊢n,
where r(λ) is deﬁned as in Eq. (13.11), and each αλ is d 2
λ -fold degenerate.
In recent years spectral graph theory has become popular in machine learning in a
variety of contexts from dimensionality reduction [1], through constructing kernels [13],
to semi-supervised learning. The Laplacian of the Cayley graph establishes a connection
to this literature. A detailed account of kernels on ﬁnite groups is given in [9], and [4]
investigates the properties of kernels on groups in general.
An important advantage of the Laplacian formulation is that it lets us take the contin-
uous time limit of the random walk. Dividing the interval from t to t +1 into k equal time
steps,
pt+1 =
 
I −β∆
k
!k
pt.
In the limit k →∞, where in any ﬁnite interval of time there are an inﬁnite number of
opportunities for a given transition to take place, but the probability of it taking place in
any speciﬁc inﬁnitesimal sub-interval is inﬁnitesimally small, the expression in parentheses

288
Risi Kondor
becomes the matrix exponential e−β∆, and we arrive at the equation describing diﬀusion
on our graph,
p′
t = e−(t′−t)β∆p(t),
(13.13)
where t and t′ are now real numbers. By analogy with Eq. (13.9),
bpt′(λ) = e−αλβ(t′−t) bpt(λ),
so in Fourier space diﬀusion just amounts to rescaling the bp(λ) Fourier matrices.
In most real-world scenarios transitions happen in continuous time, so the diﬀusion
model is, in fact, more appropriate than the discrete time random walk, and this is the
model that we implemented in our experiments.
13.5
Approximations in terms of marginals
The random walk analysis of the previous section is mathematically compelling, but sheds
no light on what information is captured by the diﬀerent Fourier components. Leaving the
Fourier formalism aside for the moment, let us ask what other, more intuitive ways we
could ﬁnd an appropriate subspace W for approximating p.
One traditional approach to identity management is to just keep track of the n×n matrix
of probabilities
M(1)
j,i = Prob   target i is assigned to track j  =
X
σ(i)=j
p(σ).
Clearly, this is a very impoverished representation for p, but it does have the merit of being
fast to update. Huang et al. [6] demonstrate the limitations of such a ﬁrst-order approach on
a speciﬁc example. A more reﬁned approach is to look at the n(n−1)-dimensional matrix
of second-order marginals
M(2)
(j1,j2),(i1,i2) = Prob ( i1 is at j1 and i2 is at j2 ) =
X
σ(i1)=j1, σ(i2)=j2
p(σ),
and so on, to higher orders. In general, kth order marginals can be expressed as an inner
product
M(k)
(j1,...,jk),(i1,...,ik) = ⟨p, u(i1,...,ik),(j1,...,jk)⟩,
where u(i1,...,ik),(j1,...,jk) = P
σ(i1)= j1,...,σ(ik)=jk eσ, and representing p(σ) by M(k) corresponds to
approximating it by its projection to
Uk = span
n
u(i1,...,ik),(j1,...,jk) | i1, . . . , ik, j1, . . . , jk ∈{1, 2, . . . , n}
o
.
(13.14)
The natural question to ask is how the hierarchy of subspaces U1 ⊂U2 ⊂. . . RSn is
related to the (Vλ)λ⊢n isotypics. To answer this, the ﬁrst thing to note is that in matrix form
M(1) =
X
σ∈Sn
p(σ) P(1)(σ),
where P(1)(σ) are the usual permutation matrices
[P(1)(σ)] j,i =

1
if σ(i) = j,
0
otherwise.

Harmonic analysis in multi-object tracking
289
Similarly, the matrix of kth order marginals can be written as
M(k) =
X
σ∈Sn
p(σ) P(k)(σ),
(13.15)
where P(k) is the kth order permutation matrix
[P(k)(σ)](j1,...,jk),(i1,...,ik) =

1
if σ(i1) = j1, σ(i2) = j2, . . . , σ(ik) = jk,
0
otherwise,
which is n!/(n−k)!-dimensional.
Such generalised permutation matrices map the basis vector labelled (i1, i2, . . . , ik) to the
basis vector labelled (σ(i1), . . . , σ(ik)). It follows that P(k)(σ2)P(k)(σ1) maps (i1, i2, . . . , ik)
to (σ2σ1(i1), . . . , σ2σ1(ik)). In other words, P(k) is a representation of Sn, and hence it must
be expressible as a sum of irreps in the form
P(k)(σ) = T −1
k
hM
λ⊢n
Kk,λ
M
m=1
ρλ(σ)
i
Tk
∀σ ∈Sn,
for some appropriate choice of multiplicities Kk,λ and invertible matrix Tk (if a particular
irrep does not feature in this sum, then we just set the corresponding Kk,λ to zero). Plugging
this into Eq. (13.15) expresses M(k) directly in terms of the Fourier components of p as
M(k) = T −1
k
hM
λ⊢n
Kk,λ
M
m=1
bp(λ)
i
Tk,
implying that the subspace of kth order marginals is the sum of isotypics
Uk =
M
λ⊢n
Kk,λ≥1
Vλ.
The general answer to what the Kk,λ and T are is given by a result called James’ Submod-
ule Theorem, as explained in [6]. Stating the theorem verbatim would require introducing
additional notation and terminology. Instead, we just state its specialisation to the case of
interest to us.
Theorem 13.4 The space (13.14) of kth order marginals is the direct sum of isotypics
Uk =
M
λ⊢n
λ1≥n−k
Vλ.
Thus, the intuitive notion of approximating p by its ﬁrst, second, third, etc. order
marginals leads to exactly the same approximation as the random walk analysis did. From
this point of view, which is discussed extensively in [3, 6], and elsewhere, the signiﬁcance
of the Fourier formalism is that it provides a canonical basis for the Uk subspaces, elimi-
nating the otherwise non-trivial linear dependencies between marginals. In addition, as we
shall see in the next section, the structure of the Fourier transform is also the key to devising
fast algorithms for updating p with observations.

290
Risi Kondor
13.6
Eﬃcient computation
The previous two sections have made a strong case for approximating p in a particular
way, by discarding all but a small number of speciﬁc Fourier components. A compact way
to store p is only one half of the story, however: if any of the computations required to
run the hidden Markov model demanded full Fourier transforms, our approach would still
be infeasible. At a minimum, we need to be able to eﬃciently perform the following three
operations:
1. Rollup, which is updating p between observations by the noise model, as expressed
in Eq. (13.13).
2. Conditioning, which is the word used for updating p with observations, such as
Eq. (13.6).
3. Prediction, which typically involves returning the maximum a posteriori permuta-
tion, or computing some set of marginals, such as pi( j) = p(σ(i) = j).
Each of these operations is to be applied to the kth order band-limited approximation
described in the previous sections, consisting of the Fourier components
bp(λ),
λ ∈Λk = { λ ⊢n | λ1 ≥n −k } .
As we have seen, the largest of these matrices are O(nk)-dimensional, so the total storage
complexity is O(n2k). We assume that at time zero the correct assignment is known, and
that without loss of generality it is the identity permutation, so we initialise our model
with bp(λ) = Idλ, since ρλ(e) = Idλ. An alternative way to seed the model would be to set
bp((n)) = 1 and bp(λ) = 0 for all λ , (n), corresponding to the uniform distribution.
Of the three operations above, rollup is very easy to perform in Fourier space, since as
we have seen, it just corresponds to rescaling the individual Fourier matrices according to
bpt′(λ) = e−αλβ(t′−t) bpt(λ). This takes only O(n2k) time.
Deriving algorithms for conditioning and prediction that run similarly fast is some-
what more involved, and requires considering projections of p to yet another system of
subspaces, namely
R(i1,...,iℓ),(j1,..., jℓ) = span { eσ | σ(i1) = j1, . . . , σ(iℓ) = jℓ} , i1 < i2 < . . . < ie
if we are interested in conditioning on or predicting marginals up to order ℓ. Clearly, for
any choice of ℓand i1, . . . , iℓ,
RSn =
M
j1,...,jℓ
R(i1,...,iℓ),( j1,...,jℓ),
where the sum extends over all choices of mutually distinct j1, . . . , jℓ. Moreover,
{ σ | σ(i1) = j1, . . . σ(iℓ) = jℓ} is a structure very similar to Sn−ℓ(technically, it is an Sn−ℓ-
coset), since it is a set of (n−ℓ)! permutations that map any i which is not one of i1, . . . , iℓ
to any position j, as long as it is not j1, . . . , jℓ. This implies that each R(i1,...,iℓ),( j1,..., jℓ) sub-
space has its own Fourier transform with respect to Sn−ℓ. Our key computational trick is to
relate the individual components of these Sn−ℓ-transforms to the global Sn-transform. For
simplicity we only derive these relationships for the ‘ﬁrst-order’ subspaces Ri,j ≡R(i),(j).
The higher-order relations (ℓ> 1) can be derived by recursively applying the ﬁrst-order
ones.

Harmonic analysis in multi-object tracking
291
Identifying Sn−1 with the subgroup of permutations that ﬁx n and deﬁning ⟦a, b⟧as the
permutation
⟦a, b⟧(i) =

i+1
if i = a, . . . , b−1,
a
if i = b,
i
otherwise,
any σ satisfying σ(i) = j can be uniquely written as σ = ⟦j, n⟧τ ⟦i, n⟧−1 for some τ ∈Sn−1.
Thus, the projection of a general vector p ∈RSn to Ri, j is identiﬁed with pi,j ∈RSn−1 deﬁned
pi,j(τ) = p(⟦j, n⟧τ ⟦i, n⟧−1). Writing the full Fourier transform as
bp(λ) =
n−1
X
j=1
X
τ∈Sn−1
p(⟦j, n⟧τ ⟦i, n⟧−1) ρλ(⟦j, n⟧τ ⟦i, n⟧−1)
=
n−1
X
j=1
ρλ(⟦j, n⟧)
h X
τ∈Sn−1
p(⟦j, n⟧τ ⟦i, n⟧−1) ρλ(τ)
i
ρλ(⟦i, n⟧)−1,
the expression in brackets is seen to be almost the same as the Fourier transform of pi,j,
except that ρλ is an irrep of Sn and not of Sn−1. By complete reducibility we know that if τ
is restricted to Sn−1, then ρλ(τ) must be expressible as a sum of Sn−1-irreps, but in general
the exact form of this decomposition can be complicated. In YOR, however, it is easy to
check that the decomposition is just
ρλ(τ) =
M
λ−∈λ↓n−1
ρλ−(τ),
τ ∈Sn−1,
where λ↓n−1 denotes the set of all partitions of n−1 that can be derived from λ by removing
one box. Thus, bp can be computed from (bpi,j)n
j=1 by
bp(λ) =
n
X
j=1
ρλ(⟦j, n⟧)
h M
λ−∈λ↓n−1
bpi, j(λ−)
i
ρλ(⟦i, n⟧)⊤.
(13.16)
A short computation shows that the inverse of this transformation is
bpi, j(λ−) =
1
ndλ−
X
λ∈λ−↑n
dλ
h
ρλ(⟦j, n⟧)⊤bp(λ) ρλ(⟦i, n⟧)
i
λ−,
(13.17)
where λ−↑n denotes the set of those partitions of n that we can get by adding a single box
to λ, and [M]λ−denotes the block of M corresponding to λ−. In [12] we explain that thanks
to the special structure of YOR, these computations can be performed very fast: for kth-
order band-limited functions the complexity of Eqs. (13.16) and (13.17) is just O(n2k+2). If
we are only interested in a single projection bpi, j, then this is further reduced to O(n2k+1).
We remark that these operations are a modiﬁed form of the elementary steps in Clausen’s
FFT [2].
Conditioning on the assignment of individual targets and computing marginals can both
be expressed in terms of the forward (13.16) and backward (13.17) transforms. For exam-
ple, if at a given moment in time target i is observed to be at track j with probability π, then
by Bayes’ rule, p is to be updated to
p′(σ) = p(σ|O) =
p(O|σ) p(σ)
P
σ′∈Sn p(O|σ′) p(σ′),

292
Risi Kondor
where
p(O|σ) =

π
if σ(i) = j,
(1−π)/(n−1)
if σ(i) , j.
(13.18)
In terms of vectors this is simply p′ ∝1−π
n−1 p + πn−1
n−1 pi→j, where pi→j is the projection of
p to Ri, j. Thus, the update can be performed by computing pi→j by Eq. (13.17), rescal-
ing by the respective factors 1−π
n−1 and πn−1
n−1 , transforming back by Eq. (13.16), and ﬁnally
normalising. All this can be done in time O(n2k+1). Higher-order observations of the form
σ(i1) = j1, . . . , σ(iℓ) = jℓwould involve projecting to the corresponding R(i1,...,iℓ),(j1,..., jℓ)
subspace and would have the same time complexity.
Prediction in the simplest case consists of returning estimates of the probabilities
p(σ(i) = j). Computing these probabilities is again achieved by transforming to the Ri,j sub-
spaces. In particular, since ρ(n−1) is the trivial representation of Sn−1, the one-dimensional
Fourier component bpi,j((n −1)) = P
τ∈Sn−1 pi, j(τ) is exactly p(σ(i) = j). In computing this
single component, the sum in Eq. (13.17) need only extend over λ = (n) and (n −1), thus
p(σ(i) = j) can be computed from bp in O(n3) time. Naturally, computing p(σ(i) = j) for all
j then takes O(n4) time.
13.6.1
Truncation and positivity
In the above discussion we implicitly made the assumption that if p is initialised to be kth
order band-limited, then as it evolves in time, it will preserve this structure. This is indeed
true of the rollup update, but in the conditioning step adding the rescaled pi→j back onto
p will generally activate additional Fourier components. Thus, conditioning must involve
truncation in the Fourier domain.
To ensure that p still remains a probability distribution, we need to normalise and
enforce pointwise positivity. Normalisation is relatively easy, since, as for pi→j, the total
weight P
σ∈Sn p(σ) can be read oﬀfrom bp((n)). If this value strays from unity, all we need
to do is divide all the bp(λ) matrices by it to renormalise.
Positivity is more diﬃcult to enforce. In [12] we argued that in most cases even when
p(σ) becomes negative for some permutations, this does not seem to be a serious problem
for predicting the marginals that we are ultimately interested in. An alternative approach
introduced in [7], which seems to do somewhat better, is to use a quadratic program to
project p back onto an appropriate marginal polytope after each conditioning step.
13.6.2
Kronecker conditioning
Our fast, O(n2k+1) complexity method for conditioning in Fourier space relies heavily on
the speciﬁc form (13.18) of the likelihood in our observation model. It is interesting to ask
how the posterior might be computed in Fourier space if g(σ) = p(O|σ) was a general
function on permutations. In [6] it is shown this is related to the so called Clebsch–Gordan
decomposition, which tells us how the tensor (or Kronecker) product of two representations
decomposes into a direct sum:
C †
λ1,λ2
ρλ1(σ) ⊗ρλ2(σ) Cλ1,λ2 =
M
λ⊢n
zλ1,λ2,λ
M
i=1
ρλ(σ),
∀σ ∈Sn,
(13.19)
where the dλ1dλ2-dimensional constant matrix Cλ1,λ2, and the zλ1,λ2,λ multiplicities are uni-
versal (albeit not easily computable) constants. In particular, they show that the Fourier

Harmonic analysis in multi-object tracking
293
transform of the unnormalised posterior p′(σ) = g(σ) p(σ) is
bp ′(λ) =
1
n! dλ
X
λ1,λ2⊢n
dλ1dλ2
zλ1,λ2,λ
X
i=1
h
C †
λ1,λ2
bg(λ1) ⊗bp(λ2) Cλ1,λ2
i
λ,i,
(13.20)
where [ · ]λ,i corresponds to the ‘ith λ-block’ of the matrix in brackets according to the
decomposition (13.19).
The price to pay for the generality of this formula is its computational expense: in
contrast to the O(n2k+1) complexity of conditioning with Eq. (13.18), if we assume that g
is mth order band-limited, the complexity of computing Eq. (13.20) is O(n3k+2m). Huang
et al. [6] argue that in practice the Cλ1,λ2 matrices are sparse, which somewhat reduces this
computational burden, and manage to get empirical results using their approach for n = 11.
13.6.3
Empirical performance
Both our group and the Huang–Guestrin–Guibas group have performed experiments to
validate the Fourier approach to identity management, but side-by-side comparisons with
traditional algorithms are diﬃcult for lack of standard benchmark datasets. Our group
culled data from an online source of ﬂight paths of commercial aircraft, while Huang
et al. collected data of people moving around in a room, and later of ants in an enclosed
space.
All experiments bore out the general rule that the more Fourier components that an
algorithm can maintain, the better its predictions will be. Using the fast updates described
above our algorithms can aﬀord to maintain second-order Fourier components up to about
n = 30, and third-order components up to about n = 15. Typically, each update takes only
a few seconds on an ordinary desktop computer.
In contrast, more traditional identity management algorithms generally either store the
entire distribution explicitly, which is only feasible for n ≤12, or in some form work with
ﬁrst-order marginals. Thus, Fourier algorithms have a deﬁnite edge in the intermediate
12 < n ≤30 range.
Of course, all these statements relate to the scenario described in the introduction, where
observations are relatively rare and p becomes appreciably spread out over many permuta-
tions. If the uncertainty can be localised to a relatively small set of permutations or a subset
of the targets, then a particle ﬁlter method or the factorisation approach in [5] might be
more appropriate. For more information on the experiments the reader is referred to [12]
and [7].
13.7
Conclusions
Identity management is a hard problem because it involves inference over a combinatorial
structure, namely the group of permutations. We argued that the right way to approach this
problem is by Fourier analysis on the symmetric group.
While at ﬁrst sight the Fourier transform on the symmetric group seems like a rather
abstract mathematical construction, we have shown that the individual Fourier components
of the assignment distribution p(σ) can be interpreted in terms of both the modes of a ran-
dom walk on permutations and in terms of the natural hierarchy of marginal probabilities. In
particular, there is a sense in which certain Fourier components capture the ‘low-frequency’
information in p(σ), and estimating p(σ) in terms of these components is optimal.

294
Risi Kondor
In addition to discussing this principled way of approximating distributions over per-
mutations, we also derived algorithms for eﬃciently updating it in a Bayesian way with
observations. In general, we ﬁnd that the kth order Fourier approximation to p has space
complexity O(n2k) and time complexity O(n2k+1).
While the present chapter discussed identity management in isolation, in many real-
world settings one would want to couple such a system to some other model describing the
position of the individual targets, so that the position information can help disambiguate the
identity information and vice versa. This introduces a variety of interesting issues, which
are still the subject of research.
Acknowledgments
The author thanks Andrew Howard and Tony Jebara for their con-
tributions to [12], the original paper that this chapter is largely based on. He is also indebted
to Jonathan Huang, Carlos Guestrin and Leonidas Guibas for various discussions.
Bibliography
[1] M. Belkin and P. Niyogi. Laplacian eigenmaps
for dimensionality reduction and data
representation. In Neural Information Processing
Systems (NIPS), 2001.
[2] M. Clausen. Fast generalized Fourier transforms.
Theoretical Computer Science, 67:55–63, 1989.
[3] P. Diaconis. Group Representations in
Probability and Statistics. IMS Lecture Series.
Institute of Mathematical Statistics, 1988.
[4] K. Fukumizu, B. K. Sriperumbudur, A. Gretton
and B. Sch¨olkopf. Characteristic kernels on
groups and semigroups. In Neural Information
Processing Systems (NIPS), 2008.
[5] J. Huang, C. Guestrin, X. Jiang and L. Guibas.
Exploiting probabilistic independence for
permutations. In Proceedings of the
International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2009.
[6] J. Huang, C. Guestrin and L. Guibas. Fourier
theoretic probabilistic inference over
permutations. Journal of Machine Learning
Research, 10:997–1070, 2009.
[7] J. Huang, C. Guestrin and L. Guibas. Eﬃcient
inference for distributions on permutations. In
Neural Information Processing Systems (NIPS),
2007.
[8] S. Jagabathula and D. Shah. Inferring rankings
under constrained sensing. In Neural
Information Processing Systems (NIPS), 2008.
[9] R. Kondor. Group theoretical methods in
machine learning. PhD thesis, Columbia
University, 2008.
[10] R. Kondor. A Fourier space algorithm for
solving quadratic assignment problems. In
Proceedings of the ACM-SIAM Symposium on
Discrete Algorithms (SODA),
2010.
[11] R. Kondor and K. M. Borgwardt. The skew
spectrum of graphs. In Proceedings of the
International Conference on Machine Learning
(ICML), 2008.
[12] R. Kondor, A. Howard, and T. Jebara.
Multi-object tracking with representations of the
symmetric group. In Proceedings of the
International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS),
2007.
[13] R. Kondor and J. Laﬀerty. Diﬀusion kernels on
graphs and other discrete structures. In
Proceedings of the International Conference on
Machine Learning (ICML) 2002, 2002.
[14] R. Kondor, N. Shervashidze and K. M.
Borgwardt. The graphlet spectrum. In
Proceedings of the International Conference on
Machine Learning (ICML), 2009.
[15] D. K. Maslen. The eﬃcient computation of
Fourier transforms on the symmetric group.
Mathematics of Computation,
67(223):1121–1147, 1998.
[16] D. K. Maslen and D. N. Rockmore. Double coset
decompositions and computational harmonic
analysis on groups. Journal of Fourier Analysis
and Applications, 6(4), 2000.
[17] D. N. Rockmore. Some applications of
generalized FFTs. Proceedings of the DIMACS
workshop on groups and computation 1995,
1997.
[18] B. E. Sagan. The Symmetric Group.
Representations, combinatorial algorithms and
symmetric functions, volume 203 of Graduate
Texts in Mathematics. Springer, 2001.
[19] J.-P. Serre. Linear Representations of Finite
Groups, volume 42 of Graduate Texts in
Mathematics. Springer-Verlag,
1977.
Contributor
Risi Kondor, Center for the Mathematics of Information, California Institute of Technology, USA

14
Markov chain Monte Carlo algorithms
for Gaussian processes
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
14.1
Introduction
Gaussian processes (GPs) have a long history in statistical physics and mathematical prob-
ability. Two of the most well-studied stochastic processes, Brownian motion [12, 47] and
the Ornstein–Uhlenbeck process [43], are instances of GPs. In the context of regres-
sion and statistical learning, GPs have been used extensively in applications that arise
in geostatistics and experimental design [26, 45, 7, 40]. More recently, in the machine
learning literature, GPs have been considered as general estimation tools for solving prob-
lems such as non-linear regression and classiﬁcation [29]. In the context of machine
learning, GPs oﬀer a ﬂexible nonparametric Bayesian framework for estimating latent
functions from data and they share similarities with neural networks [23] and kernel
methods [35].
In standard GP regression, where the likelihood is Gaussian, the posterior over the latent
function (given data and hyperparameters) is described by a new GP that is obtained ana-
lytically. In all other cases, where the likelihood function is non-Gaussian, exact inference
is intractable and approximate inference methods are needed. Deterministic approximate
methods are currently widely used for inference in GP models [48, 16, 8, 29, 19, 34].
However, they are limited by an assumption that the likelihood function factorises. In addi-
tion, these methods usually treat the hyperparameters of the model (the parameters that
appear in the likelihood and the kernel function) in a non full Bayesian way by provid-
ing only point estimates. When more complex GP models are considered that may have
non-factorising and heavily parameterised likelihood functions, the development of useful
deterministic methods is much more diﬃcult. Complex GP models can arise in time series
applications, where the association of the latent function with the observed data can be
described, for instance, by a system of ordinary diﬀerential equations. An application of
this type has recently been considered in systems biology [3] where the latent function is a
transcription factor protein that inﬂuences through time the mRNA expression level of a set
of target genes [5, 32, 20]. In this chapter, we discuss Markov chain Monte Carlo (MCMC)
algorithms for inference in GP models. An advantage of MCMC over deterministic approx-
imate inference is that it provides an arbitrarily precise approximation to the posterior
distribution in the limit of long runs. Another advantage is that the sampling scheme will
often not depend on details of the likelihood function, and is therefore very generally
applicable.

296
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
In order to beneﬁt from the advantages of MCMC it is necessary to develop eﬃcient
sampling strategies. This has proved to be particularly diﬃcult in many GP applications
that involve the estimation of a smooth latent function. Given that the latent function is
represented by a discrete set of values, the posterior distribution over these function val-
ues can be highly correlated. The more discrete values used to represent the function, the
worse the problem of high correlation becomes. Therefore, simple MCMC schemes such
as Gibbs sampling can often be very ineﬃcient. In this chapter, we introduce two MCMC
algorithms for GP models that can be more eﬀective in sampling from highly correlated
posterior GPs. The ﬁrst algorithm is a block-based Metropolis–Hastings technique, where
the latent function variables are partitioned into disjoint groups corresponding to diﬀerent
function regions. The algorithm iteratively samples each function region by conditioning
on the remaining part of the function. The construction of the proposal distribution requires
the partitioning of the function points into groups. This is achieved by an adaptive process
performed in the early stage of MCMC. The block-based Metropolis–Hastings scheme can
improve upon the Gibbs sampler, but it is still not so satisfactory in dealing with highly
correlated posterior GPs. Therefore, we introduce a more advanced scheme that uses con-
trol variables. These variables are auxiliary function points which are chosen to provide
an approximate low-dimensional summary of the latent function. We consider Metropolis–
Hastings updates that ﬁrstly propose moves in the low-dimensional representation space
and then globally sample the function. The design parameters of the control variables, i.e.
their input locations, are found by minimising an objective function which is the expected
least squares error of reconstructing the function values from the control variables, where
the expectation is under the GP prior. The number of control variables required to con-
struct the proposal distribution is found automatically by an adaptive process performed
during the early iterations of the Markov chain. This sampling algorithm has previously
been presented in [42].
Furthermore, we review other sampling algorithms that have been applied to GP models
such as schemes based on variable transformation and Hybrid Monte Carlo [11]. In the
context of sampling, we also discuss the problem of inference over large datasets faced by
all GP models due to an unfavourable time complexity O(n3) where n is the number of
function values needed in the GP model.
In our experimental study, we ﬁrstly demonstrate the MCMC algorithms on regression
and classiﬁcation problems. As our main application, we consider a problem in systems
biology where we wish to estimate the concentration function of a transcription factor pro-
tein that regulates a set of genes. The relationship between the protein and the target genes
is governed by a system of ordinary diﬀerential equations in which the concentration of the
protein is an unobserved time-continuous function. Given a time series of observed gene
expression mRNA measurements and assuming a GP prior over the protein concentration,
we apply Bayesian inference using MCMC. This allows us to infer the protein concentra-
tion function together with other unknown kinetic parameters that appear in the diﬀerential
equations.
The remainder of this chapter is organised as follows. Section 14.2 gives an introduction
to GP models used in statistical learning, while Section 14.3 gives a brief overview of deter-
ministic approximate inference algorithms applied to GP models. Section 14.4 describes
sampling algorithms and Section 14.5 discusses related work. Section 14.6 demonstrates
the sampling methods on regression and classiﬁcation problems, while Section 14.7 gives a
detailed description of the application to the regulation of gene transcription. Section 14.8

Monte Carlo algorithms for Gaussian processes
297
deals with sampling methods for large GP models. The chapter concludes with a discussion
in Section 14.9.
14.2
Gaussian process models
A Gaussian process is a stochastic process, that is a set of random variables {f(x)|x ∈X},
where X is an index set, for which any ﬁnite subset follows a Gaussian distribution. To
describe a GP, we only need to specify the mean function m(x) and a covariance or kernel
function k(x, x′)
m(x) = E( f(x)),
(14.1)
k(x, x′) = E(( f(x) −m(x))(f(x′) −m(x′))),
(14.2)
where x, x′ ∈X. Gaussian processes naturally arise in the study of time-continuous stochas-
tic processes [10, 46]. In the context of statistical learning, the practical use of GPs stems
from the fact that they provide ﬂexible ways of specifying prior distributions over real-
valued functions that can be used in a Bayesian estimation framework. In this section, we
give a brief introduction to GP models in the context of statistical learning. For extensive
treatments see, for example, [29].
Suppose we wish to estimate a real-valued function f(x). We assume that x ∈RD
and D is the dimensionality of the input space. We consider a GP model as the prior over
the latent function f(x), where for simplicity the mean function m(x) is set to be equal to
zero. This prior imposes stronger preferences for certain types of functions compared to
others which are less probable. For instance, the prior may favour smooth or stationary
functions, or functions with certain lengthscales. All this is reﬂected in the choice of the
kernel k(x, x′), which essentially captures our prior beliefs about the function we wish to
estimate. The kernel k(x, x′) must be positive deﬁnite and can be chosen to fall within a
parametric family so as the values of the hyperparameters θ further specify a member in
this family. A common choice is the squared-exponential kernel
k(x, x′) = σ2
f exp
(
−1
2(x −x′)TΣ−1(x −x′)
)
,
(14.3)
where σ2
f is the kernel variance parameter and Σ is a positive deﬁnite matrix. Special cases
of this kernel are often used in practice. For instance, Σ can be chosen to be diagonal,
Σ = diag[ℓ2
1, . . . , ℓ2
D], where each diagonal element is the lengthscale parameter for a given
input dimension. This can be useful in high-dimensional input spaces, where by estimating
the lengthscales we can learn to ignore irrelevant input dimensions that are uncorrelated
with the output signal [29, 23]. The above type of kernel function deﬁnes a GP model
that generates very smooth (inﬁnitely many times diﬀerentiable) functions. This can be
particularly useful for general purpose learning problems such as those that arise in machine
learning applications. Other types of kernel function such as the Mat´ern class are often used
[1, 40, 29]. There are also operations such as addition, multiplication and convolution that
allow us to create new valid kernels from old ones.
Having chosen a GP prior over the latent function we would like to combine this with
observed data, through a Bayesian formalism, and obtain a posterior over this function.
When the data consist of noisy realisations of the latent function and the noise is Gaussian,
the above framework has an analytical solution. In particular, let (X, y) = {(xi, yi)}n
i=1 be a

298
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
set of data where xi ∈RD and yi ∈R. Each yi is produced by adding Gaussian noise to the
latent function at input xi
yi = fi + ϵi,
ϵi ∼N(0, σ2),
where fi = f(xi). This deﬁnes a Gaussian likelihood model p(y|f) = N(y|f, σ2I), where
f = ( f1, . . . , fn). The marginalisation property of GPs allows simpliﬁcation of the prior over
the latent function which initially is an inﬁnite-dimensional object. After marginalisation
of all function points not associated with the data, we obtain a n-dimensional Gaussian
distribution, p(f) = N(f|0, Kf,f ), where 0 denotes the n-dimensional zero vector and Kf, f
is the n × n covariance matrix obtained by evaluating the kernel function on the observed
inputs. Overall, the joint probability model takes the form
p(y, f) = p(y|f)p(f).
Notice that this model is nonparametric as the dimension of the (parameter) f grows linearly
with the number of data points. By applying Bayes’ rule we can obtain the posterior over f
p(f|y) =
p(y|f)p(f)
R
p(y|f)p(f)df
,
(14.4)
which can be used to obtain the prediction of any quantity of interest. For instance, the
function values f∗at any set of unseen inputs X∗are computed according to
p(f∗|y) =
Z
p(f∗|f)p(f|y)df,
(14.5)
where p(f∗|f) is the conditional GP prior given by
p(f∗|f) = N(f∗|Kf∗, f K−1
f, f f, K f∗,f∗−K f∗,f K−1
f,f K⊤
f∗, f ).
(14.6)
Here, the covariance matrix K f∗,f∗is obtained by evaluating the kernel function on the inputs
X∗and the cross-covariance matrix Kf∗, f is obtained by evaluating for X∗and X. The pre-
diction of the values y∗of the output signal corresponding to the latent points f∗is given by
p(y∗|y) =
R
p(y∗|f∗)p(f∗|y)df∗. In the regression case, where the likelihood is Gaussian, all
the above computations are analytically tractable and give rise to Gaussian distributions.
Furthermore, the posterior over the latent function can be expressed as a new GP with an
updated mean and kernel function. Thus, the counterparts of Eqs. (14.1) and (14.2) for the
posterior GP are given by
my(x) = k(x, X)(σ2I + Kf, f )−1y,
(14.7)
ky(x, x′) = k(x, x′) −k(x, X)(σ2I + Kf,f )−1k(X, x′),
(14.8)
where k(x, X) is a n-dimensional row vector of kernel function values between x and X,
while k(X, x) = k(x, X)⊤. The above functions fully specify our posterior GP and we can use
them directly to compute any quantity of interest. For instance, the mean and the covariance
matrix of the predictive Gaussian p(f∗|y) in Eq. (14.5) is simply obtained by evaluating the
above at the inputs X∗.
The posterior GP depends on the values of the kernel parameters θ as well as the likeli-
hood parameters. To make our notation explicit, we write the likelihood as p(y|f, α), with α
being the parameters of the likelihood,1 and the GP prior as p(f|θ). The quantities (α, θ) are
1For the regression case α consists only of σ2.

Monte Carlo algorithms for Gaussian processes
299
the hyperparameters of the GP model which have to be speciﬁed in order to obtain a close
ﬁt to the observed data. A common practice in machine learning is to follow an empirical
Bayes approach and choose these parameters by maximising the marginal likelihood:
p(y|α, θ) =
Z
p(y|f, α)p(f|θ)df.
When the likelihood is Gaussian this quantity is just a Gaussian distribution which can be
maximised over (α, θ) by applying a continuous optimisation method. A full Bayesian treat-
ment of the hyperparameters requires the introduction of corresponding prior distributions
and an estimation procedure based on MCMC; see Section 14.4.5 for further discussion of
this issue.
14.3
Non-Gaussian likelihoods and deterministic methods
The above framework, while ﬂexible and conceptually simple, is computationally tractable
only when the likelihood function p(y|f, α) is Gaussian. When the likelihood is non-
Gaussian, computations become intractable and quantities such as the posterior p(f|α, θ, y)
and the marginal likelihood p(y|α, θ) are not available in closed form. Clearly, the posterior
process over the latent function f(x) is not a GP any more. In such cases we need to con-
sider approximate inference methods. Before describing MCMC methods in Section 14.4,
we give a brief overview of deterministic approximate inference methods and highlight
some of their limitations.
Deterministic methods are widely used for approximate inference in GP models, espe-
cially in the machine learning community. Three diﬀerent algorithms used are the Laplace
approximation [48], the expectation propagation algorithm [22, 8, 21, 19, 36] and the
variational Gaussian approximation [27]. For instance, in binary GP classiﬁcation, the
expectation propagation algorithm seems to be accurate [19]. Deterministic methods were
also recently discussed in the statistics literature in the context of Gaussian Markov random
ﬁelds [34]. All these methods rely heavily on GP models that have a factorising likelihood
function, i.e. p(y|f, α) = Qn
i=1 p(yi|fi), where each likelihood factor p(yi|fi) depends on a
single function value fi, and there is no sharing of function points across factors. Based on
these assumptions, the conditional posterior is written in the form
p(f|α, θ, y) ∝exp

n
X
i=1
log p(yi| fi) −1
2fT K−1
f, f f
.
All alternative methods approximate this posterior by a Gaussian distribution. They diﬀer
in the way such a Gaussian is obtained. For instance, the Laplace method replaces each
factor log p(yi|fi) with a quadratic approximation, based on a Taylor series, and applies
continuous optimisation to locate the mode of p(f|α, θ, y). The expectation propagation
algorithm and the variational method also use iterative procedures, while being somehow
more advanced as they minimise some divergence between a Gaussian approximation and
the exact posterior. These methods will often be reasonably accurate especially when the
conditional posterior p(f|α, θ, y) is uni-modal. Note, however, that the marginal posterior
p(f|y) =
R
p(f|α, θ, y)p(α, θ|y)dαdθ will generally be multi modal even for the standard
regression case. The hyperparameters (α, θ) are typically estimated based on empirical
Bayes, where point estimates are obtained by maximising an approximation to the marginal
likelihood p(y|α, θ). More recently a deterministic method, the nested Laplace approxima-
tion [34], considers a full Bayesian methodology where the hyperparameters are integrated

300
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
out by applying numerical integration. However, this method can handle only a small
number of hyperparameters (less than six).
In complex GP models, with non-factorising likelihood functions, it is not clear how
to apply the current deterministic methods.2 Such a complex form of likelihood arises in
the application described in Section 14.7 that concerns inference of transcription factors in
gene regulation. This problem involves a dynamical model derived by solving a system of
ordinary diﬀerential equations (ODEs). Furthermore, in this model the number of likeli-
hood parameters α can be large (84 in one example given in Section 14.7) and it is of great
importance to estimate conﬁdence intervals for those parameters through a full Bayesian
methodology. Note that the method described in [34] that considers full Bayesian inference
is not applicable in this case, not only because it assumes a factorising likelihood but also
because it assumes a small number of hyperparameters.
Instead of using deterministic inference algorithms, we can consider stochastic methods
based on MCMC. Eﬃcient MCMC methods can reliably deal with complex GP models,
having non-factorising likelihoods, and unlike deterministic methods they beneﬁt from a
arbitrarily precise approximation to the true posterior in the limit of long runs. In the next
section we discuss MCMC algorithms.
14.4
Sampling algorithms for Gaussian process models
A major concern with the development of MCMC algorithms in GP models is how to
eﬃciently sample from the posterior conditional p(f|α, θ, y). This posterior involves a high-
dimensional random variable, consisting of function values that can be highly correlated
with one another.
In this section, we describe several sampling schemes that can simulate from p(f|α, θ, y)
given that the hyperparameters obtain some arbitrary, but ﬁxed, values. In order for our
presentation to be instructive, we start with simple schemes such as Gibbs sampling (Sec-
tion 14.4.1) and move to more advanced schemes using block-based Metropolis–Hastings
(Section 14.4.2) and control variables (Section 14.4.3). All these methods can easily be
generalised to incorporate steps that can also simulate from (α, θ) as discussed in Sec-
tion 14.4.5. To simplify our notation in the next three sections we omit reference to the
hyperparameters.
14.4.1
Gibbs sampling and independent Metropolis–Hastings
The MCMC algorithm we consider is the general Metropolis–Hastings (MH) algorithm
[30, 14]. Suppose we wish to sample from the posterior (14.4). The MH algorithm forms a
Markov chain. We initialise f(0) and we consider a proposal distribution Q(f(t+1)|f(t)) that
allows us to draw a new state given the current state. The new state is accepted with
probability min(1, A) where
A = p(y|f(t+1))p(f(t+1))
p(y|f(t))p(f(t))
Q(f(t)|f(t+1))
Q(f(t+1)|f(t)).
To apply this generic algorithm, we need to choose the proposal distribution Q. For GP
models, ﬁnding a good proposal distribution is challenging since f is high dimensional and
2This is true for the expectation propagation, variational Gaussian approximation and nested Laplace method
which seem to depend on the assumption of having a factorising likelihood. The Laplace approximation is, of
course, generally applicable.

Monte Carlo algorithms for Gaussian processes
301
the posterior distribution can be highly correlated. Despite that, there is a lot of structure in
a GP model, speciﬁcally in the prior p(f), that can greatly facilitate the selection of a good
proposal distribution.
To motivate the algorithms presented in Sections 14.4.2 and 14.4.3, we ﬁrstly discuss
two extreme options for specifying the proposal distribution Q. One simple way to choose
Q is to set it equal to the GP prior p(f) so that the proposed state is independent of the
current one. This gives us an independent MH algorithm [30]. However, sampling from
the GP prior is very ineﬃcient since it ignores the posterior structure induced by the data
leading to a low acceptance rate. Thus the Markov chain will get stuck in the same state for
thousands of iterations. On the other hand, sampling from the prior is appealing because
any generated sample satisﬁes the smoothness requirement imposed by the kernel function.
Functions drawn from the posterior GP should satisfy the same smoothness requirement as
well. It would be interesting to design proposal distributions that can possess this property
but simultaneously allow us to increase the acceptance rate.
The other extreme choice for the proposal, that has been considered in [24], is to apply
Gibbs sampling where we iteratively draw samples from each posterior conditional density
p( fi|f\i, y) with f\i = f \ fi. This scheme is feasible only when each conditional is log-
concave and the adaptive rejection sampling method [17] can be used. This will often be
the case for models with a factorising likelihood, where p( fi|f\i, y) ∝p(yi| fi)p( fi|f\i). Any
sample in the Gibbs algorithm is accepted with probability one. However, Gibbs sampling
can be extremely slow for densely discretised or sampled functions, as in the regression
problem of Fig. 14.1, where the posterior distribution over f becomes highly correlated.
To clarify this, note that the variance of the posterior conditional p( fi|f\i, y) will typically
be smaller than the variance of the conditional GP prior p( fi|f\i). However, p( fi|f\i) may
already have a tiny variance caused by the conditioning on all remaining latent function
values. The more densely sampled a function is (relative to the lengthscale of the kernel
function), the more ineﬃcient the Gibbs algorithm becomes since the variance of p( fi|f\i)
tends to zero. For the one-dimensional example in Fig. 14.1, Gibbs sampling is practically
not useful. We study this issue further in Section 14.6.
To obtain an algorithm similar to Gibbs sampling but without requiring the use of adap-
tive rejection sampling, we can consider as proposal distribution in the MH algorithm the
sequence of the conditional densities p( fi|f\i). Thus, we replace the posterior conditional
p( fi|f\i, y) with the prior conditional p( fi|f\i). We call this algorithm, which has been used
in geostatistics [9], the Gibbs-like algorithm. This algorithm can exhibit a high acceptance
rate, but it is ineﬃcient to sample from highly correlated functions for the reasons discussed
above.
A common technique used to improve the slow mixing of the Gibbs-type of algorithms
when sampling from a high-dimensional posterior distribution is to cluster the variables
into separate groups and sample all variables of a group within a single MH step based on
an appropriately deﬁned proposal distribution. Given that diﬀerent groups of variables are
weakly correlated, such a scheme can be more eﬀective. Next we describe the local region
sampling algorithm which is a way of implementing this idea for GP models.
14.4.2
Sampling using local regions
We now introduce a simple generalisation of the Gibbs-like algorithm that is more appro-
priate for sampling from smooth functions. The idea is to divide the domain of the function
into regions and sample the entire function within each region.

302
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
(a)
(b)
(c)
Figure 14.1 Illustration of the hierarchical clustering process. Panel (a) shows the variance (displayed with shaded
two standard errors bars) of the initial conditional GP prior where we condition on the right side of the function.
Since the variance is high the generated local parts of the function will not ﬁt the data often. Dividing the local
input region in (a) into two smaller groups (panels (b) and (c)) results in a decrease of the variance of the newly
formed GP conditional priors and gives an increase in the acceptance rate. However, notice that the variance of
the proposal distribution in the boundaries between diﬀerent function regions is always small. This can aﬀect the
eﬃciency of the sampling algorithm.
We wish to divide the domain of the function into local regions and sample these local
regions iteratively. Let fk denote the function points that belong to the local region k, where
k = 1, . . . , M and f1 ∪. . . ∪fM = f. New values for the region k are proposed by drawing
from the conditional GP prior p(ft+1
k |f(t)
\k ), where f\k = f \ fk, by conditioning on the remain-
ing function values; f(t+1)
k
is accepted with probability min(1, A) where A =
p(y|f(t+1)
k
,f(t)
\k )
p(y|f(t)
k ,f(t)
\k ) .
Sampling fk is iterated between all diﬀerent regions k = 1, . . . , M. Note that the terms asso-
ciated with the GP prior cancel out from the acceptance probability since sampling from
the conditional prior ensures that any proposed sample is invariant to the GP prior. Given
that the initial state f(0) is a sample from the prior, any proposed function region leads to a
possible sample drawn from the GP prior. Notice that sampling from the GP prior and the
Gibbs-like algorithm are two extreme cases of the above scheme.
To apply the algorithm, we need to partition the function values f into groups. This
process corresponds to adaption of the proposal distribution and can be carried out dur-
ing the early iterations of MCMC. An adaptive scheme can start with a small number of
clusters, so that the acceptance rate is very low, and then reﬁne the initial clusters in order
to increase the acceptance rate. Following the widely used ideas in the theory of adaptive
MCMC [15, 31, 18] (see Chapter 2) according to which desirable acceptance rates of MH
algorithms are around 1/4, we require the algorithm to sample with acceptance rate close
to that value.
More speciﬁcally, the adaption process is as follows. We obtain an initial partitioning
of the vector f by clustering the inputs X using the k-means algorithm. Then we start the
simulation and observe the local acceptance rate rk associated with the proposal p(fk|f\k).
Each rk provides information about the variance of the proposal distribution relative to the
local characteristics of the function in region k. A small rk implies that p(fk|f\k) has high
variance and most of the generated samples are outside of the support of the posterior GP;
see Fig. 14.1 for an illustrative example. Hence, when rk is small, we split the cluster k
into two clusters by locally applying the k-means algorithm using all the inputs previously
assigned to the initial cluster k. Clusters that have high acceptance rate are unchanged.
This hierarchical partitioning process is recursively repeated until all of the current clusters
exhibit a local acceptance rate larger than the predeﬁned threshold 1/4. Notice that the

Monte Carlo algorithms for Gaussian processes
303
above partitioning process can be characterised as supervised in the sense that the informa-
tion provided by the MH steps is used to decide which clusters need to be further split into
smaller groups. Figure 14.1 gives an illustration of the adaptive partitioning process in a
one-dimensional regression problem.
Once the adaption of the proposal distribution has ended, we can start sampling from
the posterior GP model. The ﬁnal form of the proposal distribution is a partition of the
vector f into M disjoint groups and the conditional GP prior is the proposal distribution for
each group.
As shown in Section 14.6, the local region algorithm improves upon the Gibbs sampler.
However, this scheme will still be ineﬃcient to sample from highly correlated posteriors
since the variance of the proposal distribution can become very small close to the bound-
aries between neighbouring function regions as illustrated in Fig. 14.1. In such cases, there
will be variables belonging to diﬀerent groups which are highly correlated with respect to
the GP prior distribution. Of course, these variables will be also highly correlated in terms
of the GP posterior. Therefore, the boundaries between function regions can cause the state
vector f(t) to move with a rather small speed when exploring the probability mass, which
will lead the Markov chain to mix poorly. In the sequel we describe a sampling algorithm
using auxiliary variables, called control points, which attempts to resolve the problems
encountered by the local region sampling method and sample more eﬃciently from highly
correlated posterior GPs.
14.4.3
Sampling using control variables
The algorithm described previously is a local sampler that samples each part of the function
by conditioning on the remaining part of the function. As discussed previously, this can
result in a slow exploration of the probability density. To resolve the problem of local
sampling we would like to sample the function in a more global sense. Below we discuss
an algorithm that achieves this by making use of auxiliary variables.
Let fc be a set of M auxiliary function values that are evaluated at inputs Xc and drawn
from the GP prior. We call fc the control variables and their meaning is analogous to the
auxiliary inducing variables used in sparse GP models [39, 28]. To compute the posterior
p(f|y) based on control variables we use the expression
p(f|y) =
Z
fc
p(f|fc, y)p(fc|y)dfc.
(14.9)
Assuming that fc is an approximate suﬃcient statistic for the parameter f, so that p(f|fc, y) ≃
p(f|fc), we can approximately sample from p(f|y) in a two-stage manner: ﬁrstly sample
the control variables from p(fc|y) and then generate f from the conditional prior p(f|fc).
This scheme can allow us to introduce an MH algorithm, where we need to specify only a
proposal distribution q(f(t+1)
c
|f(t)
c ), that will mimic sampling from p(fc|y), and always sample
f from the conditional prior p(f|fc). The whole proposal distribution takes the form
Q(f(t+1), f(t+1)
c
|f(t)
c ) = p(f(t+1)|f(t+1)
c
)q(f(t+1)
c
|f(t)
c ),
(14.10)
which is used in the MH algorithm in order to sample from the augmented poste-
rior p(f, fc|y). We should emphasise that this proposal distribution does not deﬁne an
independent Metropolis–Hastings algorithm. However, it satisﬁes a certain conditional
independence relationship according to which each proposed state (f(t+1), f(t+1)
c
) depends
only on the previous state of the control points f(t)
c and not on f(t). Figure 14.2 illustrates the

304
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
0
0.2
0.4
0.6
0.8
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
(a)
0
0.2
0.4
0.6
0.8
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
(b)
0
0.2
0.4
0.6
0.8
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
(c)
Figure 14.2 Illustration of sampling using control variables. (a) shows the current GP function f(t) with green,
the data and the current location of the control points (red circles). (b) shows the proposed new positions for
the control points (diamonds in magenta). (c) shows the proposed new function values f(t+1) drawn from the
conditional GP prior (blue dotted line). See plate section for colour version.
steps of sampling from this proposal distribution. Each proposed sample is accepted with
probability min(1, A) where A is given by
A = p(y|f(t+1))p(f(t+1)
c
)
p(y|f(t))p(f(t)
c )
q(f(t)
c |f(t+1)
c
)
q(f(t+1)
c
|f(t)
c )
,
(14.11)
where the terms involving the conditional GP prior p(f|fc) cancel out. The usefulness of
the above sampling scheme stems from the fact that the control variables can form a low-
dimensional representation of the function that does not depend much on the size of f, i.e.
on how densely the function has been discretised. The control points will tend to be less
correlated with one another since the distance between pairs of them can be large as illus-
trated in Fig. 14.2. The use of the proposal distribution in Eq. (14.10) implies that the speed
of the Markov chain, i.e. the ability to perform big moves when sampling f, will crucially
depend on how the control variables are sampled from q(f(t+1)
c
|f(t)
c ). The other part of the
proposal distribution draws an f(t+1) that interpolates smoothly between the control points.
Thus, while Gibbs-sampling will move more slowly as we keep increasing the size of f, the
sampling scheme using control variables will remain equally eﬃcient in performing big
moves. In Section 14.4.4 we describe how to select the number M of control variables and
the inputs Xc using an adaptive MCMC process. In the remainder of this section we discuss
how we set the proposal distribution q(f(t+1)
c
|f(t)
c ).
A suitable choice for q is to use a Gaussian distribution with diagonal or full covari-
ance matrix. The covariance matrix can be adapted during the burn-in phase of MCMC, for
instance using the algorithm of [18] to tune the acceptance rate. Although this scheme is
general, it has practical limitations. Firstly, tuning a full covariance matrix is time con-
suming and in our case this adaption process must be carried out simultaneously with
searching for an appropriate set of control variables. Also, since the terms involving p(fc)
do not cancel out in Eq. (14.11), using a diagonal covariance for the q distribution has
the risk of proposing control variables that may not satisfy the GP prior smoothness
requirement. To avoid these problems, we deﬁne q by using the GP prior. According to
Eq. (14.9) a suitable choice for q must mimic the sampling from the posterior p(fc|y).
Given that the control points are far apart from each other, Gibbs sampling in the control
variables space can be eﬃcient. However, iteratively sampling fci from the conditional pos-
terior p(fci|fc\i, y) ∝p(y|fc)p( fci|fc\i), where fc\i = fc \ fci is intractable for non-Gaussian

Monte Carlo algorithms for Gaussian processes
305
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3
Figure 14.3 Visualisation of iterating between control variables. The red solid line is the current f(t), the blue
line is the proposed f(t+1), the red circles are the current control variables f(t)
c
while the diamond (in magenta)
is the proposed control variable f (t+1)
ci
. The blue solid vertical line represents the distribution p( f (t+1)
ci
|f(t)
c\i) (with
two standard error bars) and the shaded area shows the eﬀective proposal p(ft+1|f(t)
c\i). See plate section for colour
version.
likelihoods.3 An attractive alternative is to use a Gibbs-like algorithm where each fci is
drawn from the conditional GP prior p(f(t+1)
ci
|f(t)
c\i) and is accepted using the MH step. More
speciﬁcally, the proposal distribution draws a new f (t+1)
ci
for a certain control variable i
from p( f (t+1)
ci
|f(t)
c\i) and generates the function f(t+1) from p(f(t+1)| f (t+1)
ci
, f(t)
c\i). The sample
( f (t+1)
ci
, f(t+1)) is accepted using the MH step. This scheme of sampling the control vari-
ables one-at-a-time and resampling f is iterated between diﬀerent control variables. A
complete iteration of the algorithm consists of a full scan over all control variables. The
acceptance probability A in Eq. (14.11) becomes the likelihood ratio and the prior smooth-
ness requirement is always satisﬁed. The detailed iteration of this sampling method is given
in Algorithm 14.1 and is illustrated in Fig. 14.3.
Although the control variables are sampled one-at-at-time, f can still be drawn with a
considerable variance which does not shrink to zero in certain regions of the input space
as happened for the local region sampling algorithm. To clarify this, note that when the
control variable fci changes, the eﬀective proposal distribution for f is
p(ft+1|f(t)
c\i) =
Z
f (t+1)
ci
p(ft+1| f (t+1)
ci
, f(t)
c\i)p( f (t+1)
ci
|f(t)
c\i)df (t+1)
ci
,
which is the conditional GP prior given all the control points apart from the current point
fci. This conditional prior can have considerable variance close to fci and in all regions that
are not close to the remaining control variables. As illustrated in Fig. 14.3, the iteration over
diﬀerent control variables allows f to be drawn with a considerable variance everywhere in
the input space whilst respecting the smoothness imposed by the GP prior.
14.4.4
Selection of the control variables
To apply the previous algorithm we need to select the number, M, of the control points and
the associated inputs Xc. Here Xc must be chosen so that knowledge of fc can determine f
with small error. The prediction of f given fc is equal to Kf,fcK−1
fc,fcfc which is the mean of the
conditional prior p(f|fc). A suitable way to search over Xc is to minimise the reconstruction
error ||f −Kf, fcK−1
fc, fcfc||2 averaged over any possible value of (f, fc):
3This is because we need to integrate out f in order to compute p(y|fc).

306
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
Algorithm 14.1 Control-points MCMC
Input: Initial state of control points f(0)
c
and f(0).
repeat
for i = 1 to M do
Sample the ith control point: f(t+1)
ci
∼p(f(t+1)
ci
|f(t)
c\i).
Sample f(t+1): f(t+1) ∼p(ft+1| f (t+1)
ci
, f(t)
c\i).
Accept or reject (f(t+1), f (t+1)
ci
) with the MH probability (likelihood ratio).
end for
until Convergence of the Markov chain is achieved.
G(Xc) =
Z
f,fc
||f −Kf, fcK−1
fc, fcfc||2p(f|fc)p(fc)dfdfc = Tr(Kf, f −Kf,fcK−1
fc,fcK⊤
f, fc).
The quantity inside the trace is the covariance of p(f|fc) and thus G(Xc) is the total variance
of this distribution. We can minimise G(Xc) w.r.t. Xc using continuous optimisation simi-
larly to the approach in [39]. Note that G(Xc) is non-negative and when it becomes zero,
p(f|fc) becomes a delta function, which means that the control variables fully determine f.
To ﬁnd the number of control points M we start with an initial small value and we follow
an adaptive MCMC strategy that increases M until the acceptance rate of the chain reaches
a certain value. More precisely, the input locations Xc of the initial set of control points is
optimised by minimising G(Xc). Then, we run the Markov chain for a small window of W
iterations (e.g. W = 100) and compute the acceptance rates ri, i = 1, . . . , M, associated with
each control variable separately. If all these rates satisfy a certain condition, the adaptive
process terminates, otherwise we add more control points and the steps above are repeated.
The condition we use is that each acceptance rate ri must obtain a value within the range
[20, 30]/M per cent so that a full iteration over all control variables will have an overall
acceptance rate (computed as r = PM
i=1 ri) around 25 per cent. This strategy was chosen
because it worked well in most problems where we applied our method and we have not
attempted so far to develop any theoretical analysis regarding the asymptotically optimal
acceptance rate.
14.4.5
Sampling the hyperparameters
Above we discussed algorithms for sampling from the conditional posterior p(f|α, θ, y)
given a ﬁxed setting of the hyperparameters (α, θ). These parameters, however, are typically
unknown and we need to estimate them by using a full Bayesian approach. In particular,
we need to assign priors to those parameters, denoted by p(α) and p(θ), and sample their
values during MCMC by adding suitable updates into all previous MH algorithms. In these
updates, we simulate from the conditional posterior distribution p(α, θ|f, y) which factorises
across α and θ, thus yielding two separate conditionals
p(α|f, y) ∝p(y|f, α)p(α),
p(θ|f) ∝p(f|θ)p(θ).
Sampling from any of these distributions is carried out by using some proposal distri-
bution, for instance a Gaussian, in the MH algorithm. The kernel hyperparameters often
take positive values and they can be sampled in the log space. In the experiments we use
Gaussian proposal distributions which are adapted during the early iterations of MCMC
in order to tune the acceptance rate. Furthermore, in the problem of transcriptional gene

Monte Carlo algorithms for Gaussian processes
307
regulation (see Section 14.7), the likelihood parameters α exhibit additional conditional
independencies and thus we can sample them independently in separate blocks. Neal [24]
uses Hybrid Monte Carlo [11] to sample the hyperparameters in GP models following his
earlier work on Bayesian neural networks [23].
An accepted state for the kernel hyperparameters requires an update of the proposal
distribution when sampling f. This holds for all algorithms, described previously, that sim-
ulate from the conditional posterior p(f|α, θ). For instance, in the algorithm using control
variables and for a newly accepted state of the hyperparameters, denoted by θ(t), the condi-
tional Gaussian p(f|fc, θ(t)) needs to be computed. This requires the estimation of the mean
vector of this Gaussian as well as the Cholesky decomposition of the covariance matrix.
Finally, we should point out that sampling the kernel hyperparameters can easily become
one of the most expensive updates during MCMC, especially when the dimension of the
vector f is large.
14.5
Related work and other sampling schemes
The MCMC algorithms described in Sections 14.4.3 and 14.4.2 use an adaptive process
which tunes the proposal distribution in order to ﬁt better the characteristics of the poste-
rior distribution. We can classify these algorithms as instances of adaptive MCMC methods.
However, our schemes are specialised to GP models. The most advanced algorithm we
presented, that uses control variables, adapts the proposal distribution by ﬁnding a set
of control variables which somehow provide an approximate low-dimensional represen-
tation of the posterior distribution. This way of adaption is rather diﬀerent to other adaptive
MCMC techniques. Perhaps the nearest technique in the literature is the principal directions
method [4].
Regarding other sampling algorithms for GP models, several other schemes seem pos-
sible and some have been considered in applications. A sampling method often considered
is based on the transformation of the vector f of function values [19]. In particular, since
much of the correlation that exists in the posterior conditional distribution p(f|α, θ, y) is
coming from the GP prior, a way to reduce this correlation is to transform f so that the
GP prior is whitened. If L is the Cholesky decomposition of the covariance matrix Kf,f of
the GP prior p(f|θ), then the transformation z = L−1f deﬁnes a new random vector that is
white with respect to the prior. Sampling in the transformed GP model can be easier as the
posterior over z can be less correlated than the posterior over f. However, since z is a high-
dimensional random variable, the use of a Gaussian proposal distribution in a random walk
MH algorithm can be ineﬃcient. This is mainly because of practical diﬃculties encoun-
tered when tuning a full covariance matrix in very high dimensional spaces. Therefore, a
more practical approach often considered [19], is to sample z based on the hybrid Monte
Carlo algorithm [11]. This method uses gradient information and has shown to be eﬀective
in sampling in high-dimensional spaces [23].
Another common approach to sample the function latent values is to construct a Gaus-
sian approximation to the posterior conditional p(f|α, θ, y) and use this as a proposal
distribution in the MH algorithm [33, 6, 44]. The authors in [44] further combine this
approximation with a transformation of the random variables and a subsequent use of
Hybrid Monte Carlo. A Gaussian approximation can be constructed, for instance, by using
one of the techniques discussed in Section 14.3. This method can be appropriate for spe-
cialised problems in which the likelihood function takes a simple factorising form and the
number of the hyperparameters is rather small. Notice that the Gaussian approximation is

308
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
obtained by ﬁxing the hyperparameters (α, θ) to certain values. However, once new val-
ues are sampled for those parameters, the Gaussian approximation can become inaccurate.
This is rather more likely to occur when the number of hyperparameters is large and vary-
ing their values can signiﬁcantly aﬀect the shape of the conditional posterior p(f|α, θ, y). To
overcome this, we could update the Gaussian approximation to accommodate the changes
made in the values of the hyperparameters. However, this scheme can be computationally
very expensive and additionally we need to make sure that such updates do not aﬀect the
convergence of the Markov chain to the correct posterior distribution.
Finally, another simple approach for sampling in a GP model is to use the underre-
laxation proposal distribution [25, 2] according to which the proposed new state f(t+1) is
produced by f(t+1) = πf(t) +
√
1 −π2u, where u is a sample drawn from the GP prior p(f)
and π ∈[−1, 1]. This procedure leaves the GP prior invariant, so that the MH acceptance
probability depends only on the likelihood ratio.
14.6
Demonstration on regression and classiﬁcation
In this section, we demonstrate the sampling algorithms on regression and classiﬁcation
problems. In the ﬁrst experiment we compare Gibbs sampling (Gibbs), sampling using
local regions (region) and sampling using control variables (control) in standard regression
problems of varied input dimensions. The performance of the algorithms can be accu-
rately assessed by computing the Kullback–Leibler (KL) divergences between the exact
Gaussian posterior p(f|y) and the Gaussians obtained by MCMC. We ﬁx the number of
training points to N = 200 and we vary the input dimension d from 1 to 10. The train-
ing inputs X were chosen randomly inside the unit hypercube [0, 1]d. This can allow us
to study the behaviour of the algorithms with respect to the amount of correlation in the
posterior GP which is proportional to how densely the function is sampled. The larger the
dimension, the sparser the function is sampled. The outputs y were chosen by randomly
producing a GP function using the squared-exponential kernel σ2
f exp(−||xm−xn||2
2ℓ2
), where
(σ2
f , ℓ2) = (1, 0.01) and then adding noise with variance σ2 = 0.09. The burn-in period was
104 iterations.4 For a certain dimension d the algorithms were initialised to the same state
obtained by randomly drawing from the GP prior. The parameters (σ2
f , ℓ2, σ2) were ﬁxed
to the values that generated the data. The experimental setup was repeated 10 times so as
to obtain conﬁdence intervals. We used thinned samples (by keeping one sample every 10
iterations) to calculate the means and covariances of the 200-dimensional posterior Gaus-
sians. Figure 14.4(a) shows the KL divergence against the number of MCMC iterations for
the ﬁve-dimensional input dataset. It seems that for 200 training points and ﬁve dimen-
sions, the function values are still highly correlated and thus Gibbs takes much longer
for the KL divergence to drop to zero. Figure 14.4(b) shows the KL divergence against
the input dimension after ﬁxing the number of iterations to be 3 × 104. Clearly Gibbs is
very ineﬃcient in low dimensions because of the highly correlated posterior. As dimension
increases and the functions become sparsely sampled, Gibbs improves and eventually the
KL divergence approaches zero. The region algorithm works better than Gibbs but in low
dimensions it also suﬀers from the problem of high correlation. For the control algorithm
we observe that the KL divergence is very close to zero for all dimensions. Note also that
as we increase the number of dimensions Gibbs eventually becomes slightly better than the
control algorithm (for d = 8 and onwards) since the function values tend to be independent
4For Gibbs we used 2 × 104 iterations since the region and control algorithms require additional iterations
during the adaption phase.

Monte Carlo algorithms for Gaussian processes
309
0
2
4
6
8
10
x 10
0
5
10
15
20
MCMC iterations
gibbs
region
control
2
4
6
8
10
0
10
20
30
40
50
60
dimension
Gibbs
region
control
2
4
6
8
10
0
10
20
30
40
50
60
70
dimension
number of control variables
corrCoef
control
0
0.05
0.1
0.15
0.2
0.25
(a)
(b)
(c)
Figure 14.4 (a) shows the evolution of the KL divergence (against the number of MCMC iterations) between the
true posterior and the empirically estimated posteriors for a ﬁve-dimensional regression dataset. (b) shows the
mean values with one-standard error bars of the KL divergence (against the input dimension) between the true
posterior and the empirically estimated posteriors. (c) plots the number of control variables used together with the
average correlation coeﬃcient of the GP prior. See plate section for colour version.
from one another. Figure 14.4(c) shows the increase in the number of control variables used
as the input dimension increases. The same plot shows the decrease of the average corre-
lation coeﬃcient of the GP prior as the input dimension increases. This is very intuitive,
since one should expect the number of control variables to increase as the function values
become more independent. In the limit when the function values are independent, there
will be no accurate low-dimensional representation of the function values and the optimal
number of control variables will tend to the number of function values sampled.
Next we consider two GP classiﬁcation problems for which exact inference is
intractable. Gaussian processes classiﬁcation involves a factorising likelihood function. For
the binary classiﬁcation problem each factor p(yi| fi) in the likelihood is deﬁned based on the
probit or logit model. Deterministic inference methods for GP classiﬁcation are widely used
in machine learning [48, 8, 21]. Among these approaches, the expectation-propagation (EP)
algorithm of [22] is found to be the most eﬃcient [19]. Our MCMC implementation con-
ﬁrms these ﬁndings since sampling using control variables gave similar classiﬁcation accu-
racy to EP. We used the Wisconsin Breast Cancer (WBC) and the Pima Indians Diabetes
(PID) binary classiﬁcation datasets. The ﬁrst consists of 683 examples (9 input dimensions)
and the second of 768 examples (8 dimensions). Twenty per cent of the examples were
used for testing in each case. The MCMC samplers were run for 5 × 104 iterations (thinned
to one sample every ﬁve iterations) after a burn-in of 104 iterations. The hyperparameters
were ﬁxed to those obtained by EP. Figure 14.5(a)–(b) shows the log-likelihood for MCMC
samples on the WBC dataset, for the Gibbs and control algorithms respectively. It can be
observed that mixing is far superior for the control algorithm and it has also converged to a
much higher likelihood. In Fig. 14.5(c) we compare the test error and the average negative
log-likelihood in the test data obtained by the two MCMC algorithms with the results from
EP. The proposed control algorithm shows similar classiﬁcation performance to EP, while
the Gibbs algorithm performs signiﬁcantly worse on both datasets.
14.7
Transcriptional regulation
We consider a small biological sub-system where a set of target genes are regulated
by one transcription factor (TF) protein. Ordinary diﬀerential equations (ODEs) can
provide a useful framework for modelling the dynamics in these biological networks

310
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
200
400
600
800
1000
−264
−262
−260
−258
−256
−254
MCMC iterations
200
400
600
800
1000
−50
−45
−40
−35
−30
MCMC iterations
gibbs contr
ep
gibbs contr
ep
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
(a)
(b)
(c)
Figure 14.5 We show results for GP classiﬁcation. Log-likelihood values are shown for MCMC samples obtained
from (a) Gibbs and (b) control applied to the WBC dataset. In (c) we show the test errors (grey bars) and the
average negative log-likelihoods (black bars) on the WBC (left) and PID (right) datasets and compare with EP.
[3, 5, 32, 20, 13]. The concentration of the TF and the gene-speciﬁc kinetic parameters
are typically unknown and need to be estimated by making use of a time series of observed
gene expression levels. We use a GP prior to model the unobserved TF activity, as proposed
in [20], and apply full Bayesian inference based on the MCMC. Next we discuss in detail
this method.
Barenco et al. introduce a linear ODE model for gene activation from TF [5] and this
approach was further extended to account for non-linear models [32, 20]. The general form
of the ODE model for transcription regulation with a single TF has the form
dy j(t)
dt
= Bj + S jg( f(t)) −Djyj(t),
(14.12)
where the changing level of a gene j’s expression, yj(t), is given by a combination of basal
transcription rate, Bj, sensitivity, S j, to its governing TF’s activity, f(t), and the decay rate
of the mRNA. The function g is typically a non-linear activation function that accounts for
phenomena such as gene activation, gene repression and saturation eﬀects. Later in this
section, we give speciﬁc examples of g functions. Notice also that the TF protein concen-
tration function f(t) takes positive values. The diﬀerential equation can be solved for yj(t)
giving
yj(t) = Bj
D j
+
 
Aj −Bj
Dj
!
e−Djt + S je−D jt
Z t
0
g( f(u))eDjudu,
where the Aj term arises from the initial condition. Due to the non-linearity of the g function
that transforms the TF, the integral in the above expression is not analytically obtained.
However, numerical integration can be used to accurately approximate the integral with
a dense grid (ui)P
i=1 of points in the time axis and evaluate the function at the grid points
fp = f(up). In this case the above equation can be written as
yj(t) = Bj
D j
+
 
Aj −Bj
Dj
!
e−Djt + S je−D jt
Pt
X
p=1
wpg( fp)eDjup,
(14.13)
where the weights wp arise from the numerical integration method used and, for example,
can be given by the composite Simpson rule. Notice that the dense grid of function values
{fp}P
p=1 does not have associated observed output data. As discussed shortly the number of

Monte Carlo algorithms for Gaussian processes
311
discrete time points in which gene expression measurements are available is much sparser
than the set of function points.
The TF concentration f(t) in the above system of ODEs is a latent function that needs
to be estimated. Additionally, the kinetic parameters of each gene α j = (Bj, Dj, S j, Aj)
are unknown and also need to be estimated. To infer these quantities we use mRNA mea-
surements (obtained from microarray experiments) of N target genes at T diﬀerent time
steps. Let yjt denote the observed gene expression level of gene j at time t and let y = {yjt}
collect together all these observations. Assuming a Gaussian noise for the observed gene
expressions the likelihood of our data has the form
p(y|f, {α j}N
j=1) =
N
Y
j=1
T
Y
t=1
p(yjt|f1≤p≤Pt, αj, σ2
j),
(14.14)
where each probability density in the above product is a Gaussian with mean given by
Eq. (14.13), f1≤p≤Pt denotes the TF values up to time t and σ2
j is a gene-speciﬁc vari-
ance parameter. Notice that there are ﬁve parameters per gene and thus overall there are
5 × N likelihood parameters. The above likelihood function is non-Gaussian due to the
non-linearity of g. Further, the above likelihood does not have a factorised form, as in
the regression and classiﬁcation cases, since an observed gene expression depends on the
protein concentration activity in all previous time points. Also note that the discretisation
of the TF in P time points corresponds to a very dense grid, while the gene expression
measurements are sparse, i.e. P ≫T.
To apply full Bayesian inference in the above model, we need to deﬁne prior distribu-
tions over all unknown quantities. The protein concentration f is a positive quantity, thus
a suitable prior is to consider a GP prior for log f. The kernel function of this GP prior is
chosen to be the squared-exponential kernel, exp(−1
2ℓ2 (t −t′)2), where the variance of this
kernel, the σ2
f in Eq. (14.3), is ﬁxed to one, which helps to avoid identiﬁability problems
when interacting with the sensitivity parameter S j. The lengthscale ℓ2 is assigned a gamma
prior. The kinetic parameters of each gene are all positive scalars and are represented in
the log space. These parameters are given vague Gaussian priors. Each noise variance σ2
j is
given a conjugate gamma prior. Sampling the GP function is done exactly as described in
Section 14.4; we have only to plug in the likelihood (14.14) in the MH step. Sampling from
the kinetic parameters is carried out using Gaussian proposal distributions with diagonal
covariance matrices that sample the positive kinetic parameters in the log space. Notice
also that the kinetics parameters α j for gene j are sampled independently from the cor-
responding parameters of all other genes. This is because the conditional p(α1, . . . , αN|f)
factorises across diﬀerent αjs. Finally each noise variance σ2
j is sampled from its gamma
conditional posterior distribution.
We now consider two experiments where we apply the algorithm that uses control vari-
ables (see Section 14.4.3) to infer the protein concentration of TFs that activate or repress
a set of target genes. The latent function in these problems is always one dimensional and
densely discretised. We ﬁrst consider the TF p53 which is a tumour repressor activated
during DNA damage. According to [5], irradiation is performed to disrupt the equilibrium
of the p53 network and the transcription of p53 target genes are then stimulated. Seven
samples of the expression levels of ﬁve target genes in three replicas are collected as the
raw time course data. The non-linear activation of the protein follows the Michaelis Menten
kinetics inspired response [3] that allows saturation eﬀects to be taken into account so the
g function in Eq. (14.12) takes the form g( f(t)) =
f(t)
γj+ f(t), where the Michaelis constant for
the jth gene, given by γ j, is an additional likelihood parameter that is inferred by MCMC.

312
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
Figure 14.6 (a) Shows the inferred TF concentration for p53; the small plot on top-right shows the ground-truth
protein concentration obtained by a Western blot experiment [5]. (b) Shows the predicted expression of a gene
obtained by the estimated ODE model; red crosses correspond to the actual gene expression measurements. (c)
Shows the estimated decay rates for all ﬁve target genes used to train the model. Grey bars display the parameters
found by MCMC and black bars the parameters found in [5] using a linear ODE model. (d) Shows the inferred
TF for LexA. Predicted expressions of two target genes are shown in (e) and (f). Error bars in all plots correspond
to 95 per cent credibility intervals.
Notice that since f(t) is positive the GP prior is placed on the log f(t). Gene expressions
for the genes are available for T = 7 diﬀerent times. To apply MCMC we discretise f
using a grid of P = 121 points. During sampling, seven control variables were needed
to obtain the desirable acceptance rate. Running time was 4 hours for 5 × 105 sampling
iterations plus 5 × 104 burn-in iterations. Acceptance rate for f after burns in was between
15%−25%. The ﬁrst row of Fig. 14.6 summarises the estimated quantities obtained from
MCMC simulation.
Next we consider the TF LexA in E.Coli that acts as a repressor. In the repression case
there is an analogous Michaelis Menten model [3] where the non-linear function g takes the
form g( f(t)) =
1
γ j+f(t). Again the GP prior is placed on the log of the TF activity. We applied
our method to the same microarray data considered by [32] where mRNA measurements
of 14 target genes are collected over six time points. The amount of LexA is reduced after
UV irradiation, decreasing for a few minutes and then recovering to its normal level. For
this dataset, the expression of the 14 genes were available for T = 6 times. Notice that
the number of likelihood parameters in this model is 14 × 6 = 84. The GP function f was
discretised using 121 points. The result for the inferred TF proﬁle along with predictions
of two target genes are shown in the second row of Fig. 14.6. Our inferred TF proﬁle and
reconstructed target gene proﬁles are similar to those obtained in [32]. However, for certain
genes, our model provides a better ﬁt to the gene proﬁle.
14.8
Dealing with large datasets
The application of GP models becomes intractable when the dimension of the vector of
function values f needed to specify the likelihood is very large. This is because we need

Monte Carlo algorithms for Gaussian processes
313
to store a large matrix of size n × n and invert this matrix (see Eqs. (14.5) and (14.6))
which scales as O(n3). For regression and classiﬁcation problems, where the dimension
of f grows linearly with the number of training examples, this is the well-known problem
of large datasets [8, 38, 37, 39, 28]. Notice that GP models become intractable for large
datasets not only in the case of non-Gaussian likelihood functions but also for the stan-
dard regression problem; observe that the posterior GP described by Eqs. (14.7) and (14.8)
requires the inversion of an n × n matrix. Next we discuss how we can deal with the prob-
lem of large datasets in the context of MCMC inference. The same problem has also been
addressed in [44].
A simple way to reduce the complexity of the GP model is to decrease the dimension of
f. In problems having factorising likelihoods, this implies that we have to ignore the large
majority of the training examples and use only a small subset of the data. A more advanced
strategy is to construct a sparse approximation based on a carefully chosen set of support
or inducing variables [8, 38, 37, 39, 28, 41]. In the context of MCMC, this framework ﬁts
naturally within the sampling scheme that uses control variables which are exactly analo-
gous to the inducing variables. One way to construct an approximate GP model that can
deal with a very large dimension of f is to modify the prior p(f). By using a set of auxiliary
control variables fc, which are function points drawn from the GP, we can write p(f) as
p(f) =
Z
p(f|fc)p(fc)dfc.
The intractable term in this expression is the conditional distribution p(f|fc) which has
an n × n full covariance matrix: Kf, f −Kf, fcK−1
fc, fcK⊤
f, fc. Clearly, we cannot simulate from
this conditional Gaussian, because of the prohibitively large full covariance matrix. There-
fore, the algorithm using control variables is not computationally tractable. To overcome
this problem, we can modify the GP prior by replacing p(f|fc) with a simpler distribu-
tion. The simplest choice is to use a delta function centred at the mean of p(f|fc), given by
K f,fcK−1
fc,fcfc. This allows us to analytically marginalise out f and obtain the joint probability
model:
q(y, fc) =
Z
p(y|f)δ(f −Kf,fcK−1
fc,fcfc)p(fc)df = p(y|Kf, fcK−1
fc,fcfc)p(fc).
This modiﬁed GP model corresponds to the projected process approximation [8, 37]. An
MCMC algorithm applied to this model requires only sampling fc. Further, notice that
the control points algorithm (see Algorithm 14.1) in this case reduces to the Gibbs-like
algorithm. A more advanced approximation to the GP prior is obtained by the sparse
pseudo-inputs GP method of [39] which is also referred to as fully independent training
conditional (FITC) in [28]. Here, q(f|fc) = Qn
i=1 p( fi|fc), where each p( fi|fc) is a marginal
conditional prior with mean K(xi, Xc)K−1
fc,fcfc and variance k(xi, xi)−K(xi, Xc)K−1
fc, fcK(Xc, xi).
This approximation keeps only the diagonal elements of the covariance matrix of p(f|fc).
The algorithm using control points can be applied exactly as described in Algorithm 14.1.
Notice that for factorising likelihoods, the step of sampling f given fc simpliﬁes to n inde-
pendent problems since the posterior p(f|fc, y) factorises across the dimensions of f, exactly
as the prior. This implies that we could also marginalise out f numerically in such case.
Extensions of the FITC approximation can be considered by representing exactly only
small blocks of the covariance matrix of p(f|fc) [28].
A diﬀerent approach for sampling in large GP models, is to follow the variational frame-
work [41, 8, 37]. In this method, the GP prior p(f) is not modiﬁed, but instead a variational
distribution is ﬁtted to the exact posterior p(f, fc|y). The variational distribution factorises
as follows

314
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
q(f, fc) = p(f|fc)φ(fc),
(14.15)
where the conditional prior p(f|fc) is one part of the variational distribution, while the other
part, φ(fc), is an unknown (generally non-Gaussian) distribution that is deﬁned optimally
through the minimisation of the KL divergence between q(f, fc) and the exact posterior
p(f, fc|y). The optimal setting for φ(fc) is given by
φ(fc) ∝p(fc) exp
(Z
p(f|fc) log p(f|y)df
)
,
(14.16)
where we assume that the integral inside the exponential can be either computed analyti-
cally or approximated accurately using some numerical integration method. For instance,
for a log Gaussian Cox model [9, 34] this integral can be obtained analytically and gen-
erally for factorising likelihoods the computations involve n independent one-dimensional
numerical integration problems. Given that we can integrate out f in Eq. (14.16), we can
then apply MCMC and sample from φ(fc) using, for instance, the Gibbs-like algorithm.
The whole representation of the variational distribution in Eq. (14.15) will have an analytic
part, the conditional prior p(f|fc), and a numerical part expressed by a set of samples drawn
from φ(fc).
14.9
Discussion
Gaussian processes allow for inference over latent functions using a Bayesian nonpara-
metric framework. In this chapter, we discussed MCMC algorithms that can be used for
inference in GP models. The more advanced algorithm that we presented uses control vari-
ables which act as approximate low-dimensional summaries of the function values that we
need to sample from. We showed that this sampling scheme can eﬃciently deal with highly
correlated posterior distributions.
Markov chain Monte Carlo allows for full Bayesian inference in the transcription factor
networks application. An important direction for future research will be scaling the models
used to much larger systems of ODEs with multiple interacting transcription factors. In
such cases the GP model becomes much more complicated and several latent functions
need to be estimated simultaneously.
Regarding deterministic versus stochastic inference, in simple GP models with factoris-
ing likelihoods and a small number of hyperparameters, deterministic methods, if further
developed, can lead to reliable inference methods. However, in more complex GP mod-
els having non-factorising likelihood functions and a large number of hyperparameters, we
believe that MCMC is currently the only reliable way of carrying out accurate full Bayesian
inference.
Acknowledgments
This work is funded by EPSRC Grant No EP/F005687/1 ‘Gaussian
Processes for Systems Identiﬁcation with Applications in Systems Biology’.
Bibliography
[1] P. Abrahamsen. A review of Gaussian random
ﬁelds and correlation functions. Technical Report
917, Norwegian Computing Center, 1997.
[2] R. P. Adams, I. Murray and D. J. C. MacKay.
The Gaussian process density sampler. In
D. Koller, D. Schuurmans, Y. Bengio and
L. Bottou, editors, Advances in Neural
Information Processing Systems 21, pages 9–16.
2009.
[3] U. Alon. An Introduction to Systems Biology:
Design Principles of Biological Circuits.
Chapman and Hall/CRC, 2006.

Monte Carlo algorithms for Gaussian processes
315
[4] C. Andrieu and J. Thoms. A tutorial on adaptive
MCMC. Statistics and Computing, 18:343–373,
2008.
[5] M. Barenco, D. Tomescu, D. Brewer, J. Callard,
R. Stark and M. Hubank. Ranked prediction of
p53 targets using hidden variable dynamic
modeling. Genome Biology, 7(3), 2006.
[6] O. F. Christensen, G. O. Roberts and Sk¨old.
Robust Markov chain Monte Carlo methods for
spatial generalized linear mixed models. Journal
of Computational and Graphical Statistics,
15:1–17, 2006.
[7] N. A. C. Cressie. Statistics for Spatial Data.
John Wiley & Sons, 1993.
[8] L. Csato and M. Opper. Sparse online Gaussian
processes. Neural Computation, 14:641–668,
2002.
[9] P. J. Diggle, J. A. Tawn and R. A. Moyeed.
Model-based Geostatistics (with discussion).
Applied Statistics, 47:299–350, 1998.
[10] J. L. Doob. Stochastic Processes. John Wiley &
Sons, 1953.
[11] S. Duane, A. D. Kennedy, B. J. Pendleton and
D. Roweth. Hybrid Monte Carlo. Physics Letters
B, 195(2):216–222, 1987.
[12] A. Einstein. On the movement of small particles
suspended in a stationary liquid by the molecular
kinetic theory of heat. Dover Publications, 1905.
[13] P. Gao, A. Honkela, N. Lawrence and
M. Rattray. Gaussian process modelling of latent
chemical species: Applications to inferring
transcription factor activities. In ECCB08, 2008.
[14] A. Gelman, J. Carlin, H. Stern and D. Rubin.
Bayesian Data Analysis. Chapman and Hall,
2004.
[15] A. Gelman, G. O. Roberts and W. R. Gilks.
Eﬃcient metropolis jumping rules. In Bayesian
statistics, 5, 1996.
[16] M. N. Gibbs and D. J. C. MacKay. Variational
Gaussian process classiﬁers. IEEE Transactions
on Neural Networks, 11(6):1458–1464, 2000.
[17] W. R. Gilks and P. Wild. Adaptive rejection
sampling for Gibbs sampling. Applied Statistics,
41(2):337–348, 1992.
[18] H. Haario, E. Saksman and J. Tamminen. An
adaptive metropolis algorithm. Bernoulli,
7:223–240, 2001.
[19] M. Kuss and C. E. Rasmussen. Assessing
approximate inference for binary Gaussian
process classiﬁcation. Journal of Machine
Learning Research, 6:1679–1704, 2005.
[20] N. D. Lawrence, G. Sanguinetti and M. Rattray.
Modelling transcriptional regulation using
Gaussian processes. In Advances in Neural
Information Processing Systems, 19. MIT Press,
2007.
[21] N. D. Lawrence, M. Seeger and R. Herbrich.
Fast sparse Gaussian process methods: the
informative vector machine. In Advances in
Neural Information Processing Systems, 13. MIT
Press, 2002.
[22] T. Minka. Expectation propagation for
approximate Bayesian inference. In UAI, pages
362–369, 2001.
[23] R. M. Neal. Bayesian Learning for Neural
Networks. Lecture Notes in Statistics 118.
Springer, 1996.
[24] R. M. Neal. Monte Carlo implementation of
Gaussian process models for Bayesian
regression and classiﬁcation. Technical report,
Dept. of Statistics, University of Toronto, 1997.
[25] R. M. Neal. Suppressing random walks in
Markov chain Monte Carlo using ordered
overrelaxation. In M. I. Jordan, editor, Learning
in Graphical Models, pages 205–225. Kluwer
Academic Publishers, 1998.
[26] A. O’Hagan. Curve ﬁtting and optimal design for
prediction. Journal of the Royal Statistical
Society, Series B, 40(1):1–42, 1978.
[27] M. Opper and C. Archambeau. The variational
Gaussian approximation revisited. Neural
Computation, 21(3), 2009.
[28] J. Qui˜nonero Candela and C. E. Rasmussen. A
unifying view of sparse approximate Gaussian
process regression. Journal of Machine Learning
Research, 6:1939–1959, 2005.
[29] C. E. Rasmussen and C. K. I. Williams.
Gaussian Processes for Machine Learning. MIT
Press, 2006.
[30] C. P. Robert and G. Casella. Monte Carlo
Statistical Methods. Springer-Verlag, 2nd
edition, 2004.
[31] G. O. Roberts, A. Gelman and W. R. Gilks.
Weak convergence and optimal scaling of
random walk metropolis algorithms. Annals of
Applied Probability, 7:110–120, 1996.
[32] S. Rogers, R. Khanin and M. Girolami. Bayesian
model-based inference of transcription factor
activity. BMC Bioinformatics, 8(2), 2006.
[33] H. Rue and L. Held. Gaussian Markov Random
Fields: Theory and Applications. Monographs
on Statistics and Applied Probability. Chapman
& Hall, 2005.
[34] H. Rue, S. Martino and N. Chopin. Approximate
Bayesian inference for latent Gaussian models
using integrated nested Laplace approximations.
Journal of the Royal Statistical Society: Series B:
Statistical Methodology, 71(2):319–392, 2009.
[35] B. Sch¨olkopf and A. Smola. Learning with
Kernels. MIT Press, 2002.
[36] M. Seeger. Bayesian Gaussian process models:
PAC-Bayesian generalisation error bounds and
sparse approximations. PhD thesis, University of
Edinburgh, July 2003.

316
Michalis K. Titsias, Magnus Rattray and Neil D. Lawrence
[37] M. Seeger, C. K. I. Williams and N. D.
Lawrence. Fast forward selection to speed up
sparse Gaussian process regression. In C.M.
Bishop and B. J. Frey, editors, Proceedings of
the Ninth International Workshop on Artiﬁcial
Intelligence. MIT Press, 2003.
[38] A. J. Smola and P. Bartlett. Sparse greedy
Gaussian process regression. In Advances in
Neural Information Processing Systems, 13. MIT
Press, 2001.
[39] E. Snelson and Z. Ghahramani. Sparse Gaussian
process using pseudo inputs. In Advances in
Neural Information Processing Systems, 13. MIT
Press, 2006.
[40] M. L. Stein. Interpolation of Spatial Data.
Springer, 1999.
[41] M. K. Titsias. Variational learning of inducing
variables in sparse Gaussian processes. In
Twelfth International Conference on Artiﬁcial
Intelligence and Statistics, JMLR: W and CP,
volume 5, pages 567–574, 2009.
[42] M. K. Titsias, N. D. Lawrence and M. Rattray.
Eﬃcient sampling for Gaussian process
inference using control variables. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou,
editors, Advances in Neural Information
Processing Systems 21, pages 1681–1688. 2009.
[43] G. E. Uhlenbeck and L. S. Ornstein. On the
theory of Brownian motion. Physics Review,
36:823–841, 1930.
[44] J. Vanhatalo and A. Vehtari. Sparse log Gaussian
processes via MCMC for spatial epidemiology.
Journal of Machine Learning Research:
Workshop and conference proceedings, 1:73–89,
2007.
[45] G. Wahba. Spline models for observational data.
Society for Industrial and Applied Mathematics,
59, 1990.
[46] M. C. Wang and G. E. Uhlenbeck. On the
Theory of the Brownian motion II. Reviews of
Modern Physics, 17(2-3):323–342, 1945.
[47] N. Wiener. Diﬀerential space. Journal of
Mathematical Physics, 2:131–174, 1923.
[48] C. K. I. Williams and D. Barber. Bayesian
classiﬁcation with Gaussian processes. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1342–1351, 1998.
Contributors
Michalis K. Titsias, School of Computer Science, University of Manchester
Magnus Rattray, School of Computer Science, University of Manchester
Neil D. Lawrence, School of Computer Science, University of Manchester

15
Nonparametric hidden Markov models
Jurgen Van Gael and Zoubin Ghahramani
15.1
Introduction
Hidden Markov models (HMMs) are a rich family of probabilistic time series mod-
els with a long and successful history of applications in natural language processing,
speech recognition, computer vision, bioinformatics, and many other areas of engineer-
ing, statistics and computer science. A deﬁning property of HMMs is that the time series
is modelled in terms of a number of discrete hidden states. Usually, the number of such
states is speciﬁed in advance by the modeller, but this limits the ﬂexibility of HMMs.
Recently, attention has turned to Bayesian methods which can automatically infer the
number of states in an HMM from data. A particularly elegant and ﬂexible approach is
to assume a countable but unbounded number of hidden states; this is the nonparametric
Bayesian approach to hidden Markov models ﬁrst introduced by Beal et al. [4] and called
the inﬁnite HMM (iHMM). In this chapter, we review the literature on Bayesian infer-
ence in HMMs, focusing on nonparametric Bayesian models. We show the equivalence
between the Polya urn interpretation of the inﬁnite HMM and the hierarchical Dirichlet
process interpretation of the iHMM in Teh et al. [35]. We describe eﬃcient inference
algorithms, including the beam sampler which uses dynamic programming. Finally, we
illustrate how to use the iHMM on a simple sequence labelling task and discuss several
extensions.
Sequential data are at the core of many statistical modelling and machine learning prob-
lems. For example, text consists of sequences of words, ﬁnancial data are often sequences
of prices, speech signals are represented as sequences of short-term power-spectra coeﬃ-
cients, proteins are sequences of amino acids, DNA are sequences of nucleotides and video
is a sequence of still images. Although it is possible to directly model the relationships
between subsequent elements of a time series, e.g. using autoregressive or n-gram mod-
els, in many cases we believe the data has some underlying hidden structure. For example,
the observed pixels in a video might correspond to objects, the power-spectra coeﬃcients
in a speech signal might correspond to phones, and the price movements of ﬁnancial
instruments might correspond to underlying economic and political events. Models that
explain sequential data in terms of such underlying hidden variables can be more inter-
pretable and have better predictive properties than models that try to directly relate observed
variables.
The hidden Markov model is an inﬂuential model for sequential data that captures such
hidden structure; [2, 8, 26]. An HMM describes a probability distribution over a sequence

318
Jurgen Van Gael and Zoubin Ghahramani
sT
sT
yT
yT
s0
s0
s1
s1
s2
s2
y1
y1
y2
y2
Figure 15.1 Belief network representation of the hidden Markov model.
of observations y1, y2, · · · , yT of length T. The HMM assumes there exists a Markov chain
denoted by s1, s2, · · · , sT where each st is in one of K possible states. The distribution
of the state at time t only depends on the states before it, through the state at time t −1
by a K by K stochastic transition matrix π, where πi j = P(st = j|st−1 = i). This is the
ﬁrst-order Markov property, which gives the HMM its middle name. Although it is straight-
forward to generalise the HMM to higher order, for simplicity we only consider ﬁrst-order
Markov models in this chapter. We refer to the variable that indexes sequences as time,
and assume discrete time steps. However, the models described are readily applicable to
sequences indexed by any other scalar variable. Generally, we do not directly observe the
Markov chain, but rather an observation yt which only depends on an observation model
F parameterised by a state-dependent parameter θst. For example, if we model an object
moving through a video using an HMM, we could assume that the position of the object
at time t (st), is only dependent on its position at time t −1. Moreover, we don’t directly
observe this position but rather we observe pixels yt whose conﬁguration is dependent
on the state at time t. We can write the probability distribution induced by the HMM as
follows:1
p(y1:T, s1:T|K, π, θ) =
T
Y
t=1
p(st|st−1)p(yt|st) =
T
Y
t=1
πst−1,stF(yt; θst).
(15.1)
Figure 15.1 shows the belief network representation of the HMM. The observation model
F can be made arbitrarily complex: in a natural language processing application [12]
used a multinomial output distribution, [19] describe how in speech recognition a Normal
distribution or mixture of Normal distributions is commonly used.
When designing HMMs a key question is how many states K to choose. If we have
prior knowledge about the underlying physical process generating the observed sequence,
and we know the number of states in that process, then we can set K to that value. For
example, HMMs have been used to model ion channel currents, where it is known that the
ion channel protein can be in some discrete number of physical conformations. In speech
recognition, we could impose the constraint that the hidden states correspond to known
phones of a language. However, in many applications the number of underlying states is
not known a priori and must be inferred from data.
In Section 15.2 we review HMM parameter learning in detail and describe several
Bayesian approaches to learning the number of states. Unfortunately, these Bayesian
approaches have both statistical and computational limitations. The main statistical limi-
tation is the assumption that a (usually small) ﬁnite number of states provides an adequate
1To make notation more convenient, we assume that for all our time series models, all latent chains start in a
dummy state that is the 1 state: e.g. for the HMM s0 = 1.

Nonparametric hidden Markov models
319
model of the sequence. In many settings, it is unlikely one can bound a priori the number
of states needed. For example, if the states correspond to political and economic circum-
stances aﬀecting ﬁnancial variables, it’s hard to say how many such discrete circumstances
are needed, and to be conﬁdent that new, as yet unobserved circumstances won’t arise in
the future. The computational limitation is that these approaches have to compare diﬀerent
ﬁnite numbers of states, and each such comparison requires some method of approximating
intractable marginal likelihoods.
The main focus in this chapter is on nonparametric Bayesian approaches to hidden
Markov modelling as introduced by [4]. Their model, known as the inﬁnite hidden Markov
model, is reviewed in Section 15.3. We show how it overcomes the statistical and computa-
tional limitations of the Bayesian approach to the HMM by deﬁning a Markov chain which
has a countably inﬁnite (i.e. unbounded) number of hidden states. For any ﬁnite observed
sequence, only a ﬁnite number of these states can be visited. Moreover, as the sequence
length is extended and new ‘circumstances’ arise, new states can be recruited from the
unbounded pool of states. In Section 15.4 we describe how to design tractable inference
algorithms that avoid the computation of intractable marginal likelihoods that plague the
Bayesian treatment of the HMM. Finally, we conclude this chapter with an application of
the iHMM in a natural language processing setting and describe various existing and new
extensions to the iHMM.
15.2
From HMMs to Bayesian HMMs
In practice we often use the HMM in a setting where the sequence y1:T is given and we want
to learn something about the hidden representation s1:T, and perhaps about the parameters
π, θ and K. The form of the observation model F is also important, but for this chapter we
assume that F is ﬁxed and any ﬂexibility in F is captured by its parameterisation through
θ. As an example of learning in HMMs, consider speech recognition: we can use an HMM
where the hidden state sequence corresponds to phones and the observations correspond
to acoustic signals. The parameters π, θ might come from a physical model of speech or
be learned from recordings of speech. Depending on how much domain knowledge is
available, we distinguish three computational questions.
• π, θ, K given. With full knowledge of the parameters π, θ and K we only need to infer
s1:T given the observations y1:T. We can apply Bayes’ rule to Eq. (15.1) to ﬁnd the
posterior distribution over s1:T
p(s1:T|K, π, θ, y1:T) = p(y1:T, s1:T|K, π, θ)
p(y1:T|K, π, θ)
∝
T
Y
t=1
p(st|st−1)p(yt|st).
The last line follows from the fact that p(y1:T|K, π, θ) is a constant that is inde-
pendent of s1:T. Computing this distribution can be done using a beautiful appli-
cation of dynamic programming which is called the forward-backward algorithm
in the context of HMMs. Reference [26] gives an excellent overview of this
algorithm.
• K given, π, θ learned. If only the number of hidden states K and observations y1:T are
known, we often want to learn the best parameters θ and π in addition to the hidden
representation s1:T. This problem is underspeciﬁed: we need a criterion to decide
what the ‘best parameters’ are. Common criteria are the maximum likelihood and

320
Jurgen Van Gael and Zoubin Ghahramani
maximum a posteriori objectives. The former ﬁnds θ, π which maximise p(y1:T|θ, π)
while the latter introduces a prior distribution for θ, π and ﬁnds the θ, π which max-
imise p(y1:T|θ, π)p(θ, π). Algorithms like expectation maximisation [7] can search for
the maximum likelihood and maximum a posteriori solutions but will generally only
ﬁnd locally optimal estimates.
• π, θ, K learned. Finally, given observations y1:T, consider the problem of discovering
a statistically meaningful value for K in addition to the hidden representation s1:T
and the other parameters π, θ. Using the maximum likelihood criterion turns out to
be a bad idea as more states always lead to a better ﬁt of the data: the nonsensical
solution where K = T and each state st has its own emission and transition param-
eters, maximises the likelihood. The Akaike Information Criterion [1] and Bayesian
Information Criterion [28] can be used to adjust the maximum likelihood estimate
by penalising the number of parameters.
A more principled approach to learning π, θ or K is to consider a fully Bayesian analysis
of the model. The Bayesian analysis considers the parameters π, θ as unknown quanti-
ties and introduces them as random variables in the model. This requires adding a prior
distribution, e.g. p(θ|H) and p(π|α), and extending the full joint distribution to
p(y1:T, s1:T, π, θ|K) = p(π|α)p(θ|H)

T
Y
t=1
p(st|st−1)p(yt|st)

= p(π|α)p(θ|H)

T
Y
t=1
πst−1,stF(yt; θst)
.
A common choice for the prior on π is to use a symmetric Dirichlet distribution on each
row: if we denote with πk the kth row of π then πk
iid∼Dirichlet(α/K, α/K, · · · , α/K) for
all k ∈{1, K}. Similarly, a common prior on θ factorises for each state k: θk
iid∼H for all
k ∈{1, K}, where θk denotes the parameter for state k; H can be any distribution but will
frequently be chosen to be conjugate to the observation model F. Figure 15.2 shows the
belief network for a Bayesian analysis of the HMM.
We can now compute the quantities p(π, θ|y1:T, α, H) or p(s1:T|y1:T, α, H) by integrat-
ing over respectively s1:T or π, θ. By integrating over unknown variables, the posterior
distribution will concentrate around parameter settings which give high probability on aver-
age. This reduces the overﬁtting problems that plague maximum likelihood approaches.
Figure 15.2 Belief network representation of the Bayesian hidden Markov model.

Nonparametric hidden Markov models
321
Moreover, in a Bayesian analysis of the HMM we can compute the marginal likelihood
or evidence p(y1:T|K) =
R
p(y1:T|K, θ, π)p(θ, π|K). This marginal likelihood can be used
for comparing, choosing or averaging over diﬀerent values of K. Unfortunately, analyti-
cally computing the marginal likelihood for an HMM is intractable. We brieﬂy review three
diﬀerent methods to deal with this intractability.
• There is a large body of literature in statistics on how to use Markov Chain Monte
Carlo (MCMC) techniques to learn the number of states in HMMs and related mod-
els. We can distinguish two main approaches: MCMC methods which estimate the
marginal likelihood explicitly and methods which switch between diﬀerent K. Exam-
ples of the former are Annealed Importance Sampling by [23] and Bridge Sampling
by [11] which have been successfully applied in practice. The disadvantage of these
methods is that it can be computationally expensive to ﬁnd an accurate estimate of the
marginal likelihood for a particular K. If one needs to run the estimation procedure
for each diﬀerent K, the computational overhead becomes high. Reversible jump
MCMC methods pioneered by [16] are a family of methods which ‘jump’ between
models of diﬀerent size. In the context of HMMs, [27] have implemented this idea
to jump between HMM models of diﬀerent K.
• A very elegant approximation to the exact marginal likelihood is the approach devel-
oped by [32]. Note that in the belief network in Fig. 15.2, if the hidden states s1:T
were observed, the parameters π and θ become independent and assuming that the
prior and likelihood are conjugate, we can compute the marginal likelihood analyti-
cally. By choosing a good state sequence, one can integrate out the other parameters
and compute an approximation to the marginal likelihood. The authors in [32] devise
a state-merging algorithm based on this idea.
• A third technique to approximate the marginal likelihood is based on variational
Bayesian (VB) inference. This computes a lower bound on the marginal like-
lihood; [20] and [3] describe VB inference algorithms that bound the marginal
likelihood of an HMM. The technique generalises EM as it doesn’t use a point esti-
mate of the parameters π, θ but rather an approximate posterior of these parameters.
Moreover, VB also generalises the idea in [32] as it doesn’t use a point estimate of
the state sequence s1:T but rather a full distribution on this quantity.
15.3
The inﬁnite hidden Markov model
Whether we choose a particular value for K or average over K, the Bayesian treatment of
HMMs requires us to deﬁne a large number of HMMs: one for each value of K. The authors
in [4] introduced a probabilistic model similar to the HMM but where K is inﬁnite and any
ﬁnite sequence s1:T only visits a ﬁnite fraction of states. This model is attractive because it
implicitly deﬁnes a distribution over the number of visited states in the HMM rather than
deﬁne K diﬀerent HMMs. The authors also describe how the distribution over s1:T can be
used to generate observations y1:T. This model is called the inﬁnite hidden Markov model
or iHMM.
In [35], the hierarchical Polya urn model in [4] was re-interpreted as a hierarchical
Dirichlet process (HDP).2 This interpretation embeds the iHMM into the Dirichlet pro-
cess formalism, leading to a deeper theoretical understanding. The HDP interpretation
2For this reason, some authors also use the name HDP-HMM for the inﬁnite HMM model.

322
Jurgen Van Gael and Zoubin Ghahramani
gives the iHMM new measure-theoretic, Chinese Restaurant Franchise and stick-breaking
representations.
In the following section we introduce the hierarchical Polya urn scheme construction of
the iHMM. The scheme makes clear the generative procedure for the hidden representation
s1:T and allows us to better understand the eﬀect of the hyperparameters. Section 15.3.2
introduces the HDP-based description of the iHMM. This interpretation will turn out to be
useful when we want to design inference methods for the iHMM.
15.3.1
A hierarchical Polya urn scheme
Polya urn schemes are a family of algorithms which deﬁne a discrete probability distri-
bution through the metaphor of ﬁlling an urn with coloured balls. For our purposes we
consider a Polya urn scheme that is parameterised by a single real number α > 0. We will
count the total amount of balls with colour i in the urn as ni. Initially the urn is empty (all
ni = 0) but at each time step, with probability
ni
α+P
i ni we add a ball with colour i to the
urn and with probability
α
α+P
i ni we add a ball with a new colour to the urn. Depending on
the colour we augment the appropriate ni variable by one. First note that if we execute this
recipe N times there will be N balls in the urn. The amount of diﬀerent colours that are
represented in the urn can be anything between 1 and N. Also, if there are a lot of balls
with colour j in the urn the probability of adding an extra ball with colour j is high. The
parameter α controls the growth of the number of colours: if α is large with respect to P
i ni
then it is very likely that a new colour will be added to the urn. A Polya urn scheme can be
interpreted as a nonparametric prior for a clustering: each datapoint corresponds to a ball
and each cluster to a colour. It is clear from our description above that in this setting the
number of clusters is a random quantity, that grows with the number of data points.
The hierarchical Polya urn model introduced in [4] describes how to extend the Polya
urn scheme in a time series setting. We review this model below and illustrate a sample
draw from the hierarchical Polya urn in Fig. 15.3.
Consider a countably inﬁnite number of Polya urns with parameter α, one for each
possible state the iHMM can be in. We refer to this set as the set of transition urns. In
addition to having coloured balls, we also colour the urns and identify both the colours of
the urns and balls with states. We draw balls from the hierarchical Polya urn and keep track
of their colours: st refers to the colour of the tth ball drawn from the hierarchical Polya urn.
For bookkeeping purposes, we also keep track of the quantity ni j: the number of balls of
colour j in the transition urn with colour i.
Initially, all urns are empty and we assume we observed a dummy ball with arbitrary
colour s0. At each time step t, we record the colour st−1 of the previously drawn ball and
then draw according to the Polya urn scheme from the urn with colour st−1. We set st to
the colour of the extra ball which we added to urn st−1. We interpret the number of balls
with colour j in the urn with colour i as being proportional to the transition probability of
a Markov chain
p(st = j|st−1 = i, α) =
nij
P
j′ nij′ + α.
(15.2)
Note that these probabilities do not sum to one: under the Polya urn scheme there is a
probability
α
P
j′ nij′+α of adding a ball with a new colour to the urn. To determine the colour
of the new ball we introduce an extra oracle Polya urn with parameter γ. We denote with
ci the number of balls in the oracle urn with colour i. When we need a ball with a new
colour, we draw a ball from the oracle urn, record its colour st and replace that ball with

Nonparametric hidden Markov models
323
two balls of the same colour back into the oracle urn and one ball with the same colour to
the transition urn with colour st−1. Formally, when we draw a ball with colour j from the
oracle urn we set cj ←cj + 1 and nst−1,j ←nst−1, j + 1. Conditional on drawing from the
oracle, the transition probability is
p(st = j|st−1 = i, γ) =

cj
P
j′ c j′+γ
if j represents an existing colour,
γ
P
j′ c j′+γ
if j represents a new colour.
In other words, when we query the oracle, with probability proportional to γ we introduce
an entirely new colour. The combination of transition and oracle urns explains why this
Blue
Red
Oracle Urn
Transition Urns
0
B
Blue
Red
Oracle Urn
Transition Urns
1
B
B
B
B
Blue
Red
Oracle Urn
Transition Urns
2
B
B
B
B
B
B
Blue
Red
Oracle Urn
Transition Urns
3
B
B
B
B
B
B
R
R
R
Blue
Red
Oracle Urn
Transition Urns
4
B
B
B
B
B
B
R
R
R
B
B
Blue
Red
Oracle Urn
Transition Urns
5
B
B
B
B
B
B
R
R
R
B
B
B
B
R
R
Figure 15.3 Hierarchical Polya urn example: we initialise (t = 0) our hierarchical Polya urn with both the transi-
tion and oracle urns empty and assume our ﬁrst dummy state carries the colour blue. In the ﬁrst step we check the
blue urn and ﬁnd that there are no balls. This means that with probability α/α = 1 we should query the oracle. In
the oracle urn, the probabilities of drawing a blue ball is γ/γ = 1 so we put a blue ball in both the blue urn and the
oracle urn. Next, we generate the state for time step 2: now, with probability 1/(1 + α) we draw a blue ball and
with probability α/(1 + α) we query the oracle urn. In our example we draw a blue ball implying that we add an
extra blue ball to the blue transition urn. For time step 3, our example shows that the oracle urn was queried (with
probability α/(2+α)) and that in the oracle a new red ball was drawn which was added to the blue transition urn as
well as to the oracle urn. This means that in time step 4 we ﬁrst query the red urn and since there are no balls in it,
with probability 1 we query the oracle urn; this time returning a blue ball moving us back to the blue urn. Finally
in step 5, a red ball is drawn from the blue transition urn without querying the oracle. The resulting sequence of
colours blue, blue, red, blue, red corresponds to the state sequence s1 = 1, s2 = 1, s3 = 2, s4 = 1, s5 = 2.

324
Jurgen Van Gael and Zoubin Ghahramani
Transition to state j
Query oracle
Transition to state j
Transition to
new state j
...
...
Figure 15.4 Hierarchical Polya urn transition scheme as a decision tree.
(a)
(b)
(c)
Figure 15.5 Sample Markov chain paths generated by the iHMM. The plots show the state sequence for an iHMM
with parameters (a) γ = 20, α = 20; (b) γ = 20, α = 2; (c) γ = 2, α = 20.
scheme is called the hierarchical Polya urn3 scheme: we stack the transition urn and oracle
urns on top of each other in a decision hierarchy illustrated in Fig. 15.4. To conclude the
hierarchical Polya urn algorithm, we interpret the st as the states of a Markov chain. It is
clear that the number of possible states is a random quantity and can grow arbitrarily large.
Figure 15.3 illustrates a draw from the hierarchical Polya urn scheme.
The hyperparameters γ and α play an important role in determining the number of
states in an iHMM; this is illustrated in Fig. 15.5. The hyperparameter γ controls how
frequently we are going to add a ball with a completely new colour to the urns, in other
words, the probability of visiting a new state. The top and bottom plots in Fig. 15.5 show
what happens when we change from γ = 20 to γ = 2: the number of states grows much
more slowly in the bottom plot than in the top plot. The hyperparameter α on the other hand
controls how frequently we query the oracle. The top and middle plots in Fig. 15.5 illustrate
the diﬀerence between α = 20 and α = 2. We see that in the middle plot, once a particular
3In [4] this was originally called a hierarchical Dirichlet process but that term has subsequently been used
by [35] to refer to the closely related Dirichlet process with base measure drawn from another Dirichlet process.

Nonparametric hidden Markov models
325
transition accumulates a lot of counts it becomes more and more likely that we take that
transition again. For example, the 4 →3 and 3 →4 transitions are taken frequently. In the
limit of α →∞, we always query the oracle and the distribution of ball colours for each
transition urn is going to be very close to the distribution implied by the oracle urn. In this
limiting case, we can think of the oracle urn as specifying the expected number of times
we visit each state which is the stationary distribution of the Markov chain.
Completing the iHMM
The hierarchical Polya urn scheme deﬁnes a nonparametric dis-
tribution on the hidden state sequence s1:T. In order to complete it to a full hidden Markov
model we need to introduce a mechanism to generate observations conditional on the state
sequence s1:T. For that we introduce
• a base distribution H from which we draw the output likelihood parameters θk for
each state k ∈{1, . . . , ∞},
• a likelihood model F which takes a state st and its parameter θst and generates a
datapoint yt.
In [4] there was an additional parameter reinforcing self transitions. This idea was
revisited by [10] as the sticky HDP-HMM. We will review this in Section 15.6.2.
15.3.2
The hierarchical Dirichlet process
The Polya urn scheme is an intuitive procedure for generating state sequences with arbi-
trarily large state space. In this section, we review the interpretation of the iHMM using the
hierarchical Dirichlet process framework by [35]. Using this representation we will be able
to more easily design inference schemes for the iHMM.
We brieﬂy review the theory of Dirichlet processes ﬁrst, but refer to [34] and [17] for
more background on the basic theory of nonparametric Bayesian methods. A draw from
a Dirichlet process G ∼DP(α, H) is a discrete distribution that puts probability mass on
a countable number of atoms in the domain of H. The atoms are iid draws from the base
distribution H and the parameter α controls how concentrated the mass of the DP is. We
can write
G =
∞
X
i=1
βiδθi,
(15.3)
where βi = ˆβi
Qi−1
l=1(1 −ˆβl) with ˆβl
iid∼Beta(1, α), θi
iid∼H and δθi is a point mass at θi. We
refer to this construction of β as the stick-breaking distribution [30], β ∼Stick(α).
The idea behind the hierarchical Dirichlet process in [35] is that the base distribution of
a DP is itself a draw from a DP.
G0|γ, H ∼DP(γ, H),
G j|α,G0
iid∼DP(α,G0).
Thus there are a set of distributions G j coupled through a common G0. This construction
has some interesting properties that make it suitable to build nonparametric hidden Markov
models. The ﬁrst important property of this construction, illustrated in Fig. 15.6, is the fact
that G0 and all G j share the same atoms. This is not too hard to see from the perspective
of drawing a G j: since G j is a draw from a DP, its atoms are draws from the DP’s base
distribution. This is G0 in our case: since G0 is discrete, G j and G0 share the same atoms.

326
Jurgen Van Gael and Zoubin Ghahramani
(a)
(b)
Figure 15.6 Visualisation of the hierarchical Dirichlet process. (a) The stick-breaking representation of the base
measure. (b) The stick-breaking representations of the child Dirichlet processes. The solid line is the base measure
H which was chosen to be Gaussian in this example.
Next, we focus on the distribution of probability mass for G0 and all G j’s. Equa-
tion (15.3) illustrates the stick-breaking representation of a DP. The theory of HDPs shows
us that the mass of the G j’s is distributed according to a stick rebreaking process. If β1:∞
represent the sticks for G0 and ˆπij
iid∼Beta(αβ j, α(1 −P j
l=1 βj)) then the jth stick for Gi is
πi j = ˆπi j
Qj−1
l=1 (1 −ˆπil), or more intuitively, the rebreaking process for any ﬁnite subset of
the sticks can be written as
πi1, πi2, . . . , πik,
∞
X
l=i+1
πil

iid∼Dirichlet
αβ1, αβ2, . . . , αβk, α(
∞
X
l=i+1
βl)
.
The previous paragraph hints at how we can use the HDP construction to build an
iHMM: we set the rows of an inﬁnite-dimensional transition matrix equal to the sticks of a
countably inﬁnite number of G j. We start a Markov chain in a dummy state s0 = 1 and then
transition according to πij. When we transition into state i, we use atom θi as the parameter
for the output distribution F. The key enabler in this scenario is the fact that all states
share the same set of likelihood model parameters θ. Figure 15.7 shows the belief network
underlying the iHMM in the HDP representation, deﬁned by
β|γ ∼Stick(γ)
πk·|α, β iid∼Stick(αβ)
∀k ∈{1 . . . ∞},
st|st−1, π ∼πst−1,·
with
s0 = 1,
θk|H iid∼H
∀k ∈{1 . . . ∞},
yt|st, θ ∼F(θst).
Although it is not immediately obvious, the hierarchical Polya urn model and hier-
archical Dirichlet process deﬁne the same distribution over the Markov chain s1:T. In
Appendix 15.A we sketch a proof for this equivalence.

Nonparametric hidden Markov models
327
Figure 15.7 The belief network for the iHMM.
15.4
Inference
When the iHMM was introduced in [4] it was presented with two diﬀerent inference algo-
rithms: a Gibbs sampler with a per iteration time complexity of O(KT 2) and an approximate
Gibbs sampler with a time complexity of O(KT); where K is the number of represented
states in that iteration. The re-interpretation of the iHMM in the HDP framework by [35]
slightly modiﬁed the approximate Gibbs sampler from [4] to make it exact, whilst retaining
the O(KT) complexity. We describe this linear time inference scheme in Section 15.4.1.
The linear time Gibbs sampler updates the hidden state at each time step conditioned
on all other states in the Markov chain. Typically, the state sequence variables in a Markov
chain are strongly correlated. As is well known, Gibbs sampling tends to mix slowly when
there are strong correlations between variables. Unfortunately, strong correlations are com-
mon when modelling sequential data: e.g. when we know a stock’s price today we have a
good guess about the stock’s price tomorrow.
A Markov chain Monte Carlo method for the Bayesian treatment of the HMM that
works around the correlation problem is reviewed in [29]. We will refer to this dynamic
programming based algorithm which resamples the whole state sequence at once, as the
forward-ﬁltering backward-sampling (FF-BS) algorithm.
Because the iHMM has a potentially inﬁnite number of states, we cannot naively apply
the FF-BS algorithm. In Section 15.4.2 we describe the beam sampler introduced by [36].
The beam sampler is an auxiliary variable MCMC algorithm which adaptively truncates
the iHMM so we can run a variant of the FF-BS algorithm. It resamples the whole Markov
chain at once and hence suﬀers less from slow mixing than the Gibbs sampling.
15.4.1
The collapsed Gibbs sampler
When we inspect the belief network in Fig. 15.7 we see that the unobserved random vari-
ables corresponding to the rows of the transition matrix πk have observed descendants: the
observations y1:T. Following the standard rules for belief networks we conclude that the πk
are conditionally dependent given y1:T. If we observe β and s1:T however, the rows of the
transition matrix become conditionally independent. We can consider each transition as a
draw from a Polya urn scheme and use our knowledge of Polya urns to resample the state
sequence. Moreover, if the output distribution F is conjugate to the HDP base distribution
H we will show how we can also analytically integrate out the likelihoods involved. One

328
Jurgen Van Gael and Zoubin Ghahramani
iteration of the collapsed Gibbs sampler will consist of two steps: resampling the states
p(s1:T|y1:T, α, β, γ, H, F) and resampling the base distribution p(β|y1:T, s1:T, α, γ, H, F).
Sampling st
If we condition on β and s1:T, the DPs for each of the transitions become independent.
Hence, for all k ∈{1, 2, . . .} we can treat all the transitions out of state k as draws from
a Polya urn. Since they form an exchangeable sequence we can resample s1:T by tak-
ing out one st at a time (essentially removing two transitions) and resampling it from the
posterior
p(st|y1:T, s−t, α, β, γ, H, F) ∝p(yt|st, s−t, y−t, H, F)p(st|s−t, α, β, γ),
where s−t denotes all states except st. The ﬁrst factor is the conditional likelihood of yt
given s1:T, y−t and H
p(yt|st, s−t, y−t, H, F) =
Z
p(yt|θst)p(θ|s−t, y−t, H)dθ,
(15.4)
where y−t denotes all observations except y−t. If the output likelihood F is conjugate to
the HDP base distribution H, this quantity can generally be analytically computed. In non-
conjugate scenarios one can use techniques in [24] to approximate this computation.
In order to compute the factor p(st|s−t, α, β, γ), we ﬁrst use the property that conditional
on β the Polya urns for each row are independent. This means that the transition in and the
transition out of st can be considered draws from a Markov exchangeable sequence (that
is, the in-transition is a draw from the exchangeable sequence out of urn st−1 and the out-
transition is a draw from the exchangeable sequence out of urn st). If we denote with n−t
i j
the number of transitions from state i to state j excluding the transitions involving time step
t, similarly let n−t
·i , n−t
i· be the number of transitions in and out of state i excluding time step
t and K be the number of distinct states in s−t then we can write
p(st = k|s−t, α, β, γ) ∝

(n−t
st−1,k + αβk)
n−t
k,st+1+αβst+1
n−t
k· +α
if k ≤K, k , st−1
(n−t
st−1,k + αβk)
n−t
k,st+1+1+αβst+1
n−t
k· +1+α
if k = st−1 = st+1
(n−t
st−1,k + αβk)
n−t
k,st+1+αβst+1
n−t
k· +1+α
if k = st−1 , st+1
αβkβst+1
if k = K + 1.
We resample the whole state sequence s1:T by resampling each st in turn, conditional on all
other states.
Sampling β
Recall that β is the distribution on the atoms for the base distribution of the HDP. If
we had kept track of the Polya urn representation of the base distribution (that is, the
counts of each ball type ci in the oracle urn) then we could resample from the posterior
β ∼Dirichlet(c1, c2, · · · , cK, γ). Although we don’t have the representation c1:K we can
introduce an auxiliary variable mij with the following interpretation: mi j denotes the num-
ber of oracle calls that returned a ball with label j when we queried the oracle from state i.

Nonparametric hidden Markov models
329
Since the total number of type j balls in the oracle urn must equal P
i mi j, if we can sample
mij then we have implicitly sampled cj = P
i mij and hence we can resample β.
First note that if the number of transitions from state i to j is non-zero (ni j > 0) then we
know that we must have queried the oracle at least once when we were in state i. Moreover,
we also know that mij ≤nij since we can query the oracle at most once for every transi-
tion. Let S ij = {s(ij1), s(ij2), · · · , s(ijnij)} be the set of states that transitioned from state i to
state j. The exchangeability of the transitions out of state i implies that the sequence S i j is
exchangeable as well. Note that mij is the number of elements in S i j that were obtained
from querying the oracle. Because S ij is an exchangeable sequence, we compute the
probability
p(si jnij = j|S
−nij
ij ) ∝
( nij −1
if we didn’t query the oracle,
αβ j
if we queried the oracle.
These equations form the conditional distribution of a Gibbs sampler whose equilibrium
distribution is the distribution of a Polya urn scheme with parameter αβj. In other words,
in order to sample mij, we sample nij elements from a Polya urn with parameter αβ j and
count the ﬁnal number of ball types.4
Complexity
We can break down the computational complexity for an iteration where the sample uses
at most K states in two diﬀerent parts. When sampling the hidden state sequence, for each
time step 1 ≤t ≤T we need to compute O(K) probabilities; this results in a O(TK)
contribution to the computational cost. On the other hand, when sampling the β variables,
for each of the K2 entries of the transition matrix, we need to sample ni j random variables.
Since P
i j ni j = T this leads to an extra O(K2 + T) complexity. In summary, one iteration of
the collapsed Gibbs sampler has an O(TK + K2) computational complexity.
Discussion
First, note that non-conjugate models can be handled using the sampling methods in [24].
An example can be found in [36] who experimented with a collapsed Gibbs sampler with
normal base distribution and Student-t distributed likelihood for a changepoint detection
problem.
As we discussed above, the collapsed Gibbs sampler suﬀers from one major limitation:
sequential data are likely to be strongly correlated. In other words, it is unlikely that the
Gibbs sampler which makes individual updates to st can cause large blocks within s1:T to
change state. We will now introduce the beam sampler which does not suﬀer from this slow
mixing behaviour by sampling the whole sequence s1:T in one go.
15.4.2
The beam sampler
As we argued above, we would like to use a method that resamples the whole state sequence
of the iHMM at once. Methods that achieve this for ﬁnite HMMs, e.g. the forward-ﬁltering
backward-sampling algorithm, eﬃciently enumerate all possible state trajectories (using
4Thanks to Emily Fox for bringing this to our attention; a previous implementation computed Stirling numbers
of the second kind which are rather expensive to compute for large-scale applications.

330
Jurgen Van Gael and Zoubin Ghahramani
dynamic programming), compute their corresponding probabilities and sample from this
set. Unfortunately, the forward-ﬁltering backward-sampling algorithm does not apply to the
iHMM because the number of states, and hence the number of potential state trajectories,
is inﬁnite.
The core idea of the beam sampling [36] is to introduce auxiliary variables u1:T such
that conditioned on u1:T the number of trajectories with positive probability is ﬁnite. Then,
dynamic programming can be used to compute the conditional probabilities of each of these
trajectories and eﬃciently sample whole state trajectories. These auxiliary variables do
not change the marginal distribution over other variables and hence MCMC sampling still
converges to the true posterior. The idea of using auxiliary variables to limit computational
cost is inspired by [38] who applied it to limit the number of components in a DP mixture
model that need to be considered during sampling.
Sampling π and θ
Since the auxiliary variable and dynamic program below need the
transition and likelihood model parameters, we can think of them as another set of auxil-
iary variables which we sample just prior to sampling u1:T. Let ni j be the number of times
state i transitions to state j in the state sequence s1:T, where i, j ∈{1 . . . K}, K is the num-
ber of distinct states in s1:T, and these states have been relabelled 1 . . . K. Merging the
inﬁnitely many states not represented in s1:T into one state, the conditional distribution of
(πk1 . . . πkK, P∞
k′=K+1 πkk′) given its Markov blanket s1:T, β, α is
Dirichlet(nk1 + αβ1 . . . nkK + αβK, α P∞
i=K+1 βi).
Each θk is independent of others conditional on s1:T, y1:T and their prior distribution
H, i.e. p(θ|s1:T, y1:T, H) = Q
k p(θk|s1:T, y1:T, H). When the base distribution H is conju-
gate to the data distribution F, each θk can generally be sampled eﬃciently. Otherwise
Metropolis–Hastings or other approaches may be applied. Note that beam sampling in the
non-conjugate case is simpler than for Gibbs sampling: we only need to sample from a
non-conjugate posterior rather than use Monte Carlo integration to compute the integral
in Eq. (15.4).
Sampling u1:T
For each time step t we introduce an auxiliary variable ut with conditional
distribution ut ∼Uniform(0, πst−1st).
Sampling s1:T
The key observation is the distribution over state sequences s1:T condi-
tional on the auxiliary variables u1:T will only have non-zero probability where πst−1st ≥ut.
Other paths are not consistent with the conditional distribution p(ut|s1:T, π). Moreover, there
are only ﬁnitely many such trajectories with this property: since πkk′ > 0 with probability 1,
ut > 0 with probability 1; given the auxiliary variable ut, note further that for each possible
value of st−1, ut partitions the set of transition probabilities out of state st−1 into two sets:
a ﬁnite set with πst−1k > ut and an inﬁnite set with πst−1k < ut. Thus we can recursively
show that for t = 1, 2 . . . T the set of trajectories s1:t with all πst′−1st′ > ut is ﬁnite. The
mathematics below makes this more clear and will also show how dynamic programming
can compute the conditional distribution over all such trajectories.
First note that the probability density for ut is
p(ut|st−1, st, π) = I(0 < ut < πst−1,st)
πst−1,st
,
(15.5)

Nonparametric hidden Markov models
331
where I(C) = 1 if condition C is true and 0 otherwise. We compute p(st|y1:t, u1:t) for all
time steps t as follows (we omit the additional conditioning variables π and θ for clarity):
p(st|y1:t, u1:t) ∝p(st, ut, yt|y1:t−1, u1:t−1),
=
X
st−1
p(yt|st)p(ut|st, st−1)p(st|st−1)p(st−1|y1:t−1, u1:t−1),
= p(yt|st)
X
st−1
I(ut < πst−1,st)p(st−1|y1:t−1, u1:t−1),
= p(yt|st)
X
st−1:ut<πst−1,st
p(st−1|y1:t−1, u1:t−1).
(15.6)
Note that we only need to compute Eq. (15.6) for the ﬁnitely many st values belonging
to some trajectory with positive probability. Furthermore, although the sum over st−1 is
technically a sum over an inﬁnite number of terms, the auxiliary variable ut truncates
this summation to the ﬁnitely many st−1’s that satisfy both constraints πst−1,st > ut and
p(st−1|y1:t−1, u1:t−1) > 0. Finally, to sample the whole trajectory s1:T, we sample sT from
p(sT|y1:T, u1:T) and perform a backward pass where we sample st given the sample for st+1:
p(st|st+1, y1:T, u1:T) ∝p(st|y1:t, u1:t)p(st+1|st, ut+1).
Sampling β
We discard the transition matrix π and sample β using the same algorithm as
the Gibbs sampler in Section 15.4.1.
Complexity
For each time step and each state assignment we need to sum over all rep-
resented previous states. If K states are represented, the worst case complexity is O(TK2).
However, the sum in Eq. (15.6) is only over previous states for which the transition prob-
ability is larger than ut. In practice this means that we might only need to sum over much
less states.
Figure 15.8 shows the empirical complexity of the beam sampler on the application
which we introduce in Section 15.5. As we can see from the plot, initially the beam sampler
considers about K2/2 transitions per state but very quickly this number decreases to about
10 per cent of K2. This means that the beam sampler will run approximately 10 times faster
than the dynamic program which considers all transitions. This increase doesn’t come for
free: by considering less potential transitions, the chain might mix slower.
Discussion
A practical point of attention is that after sampling ut, we need to make sure
that all possible transition probabilities in πst−1 > ut are represented. This might involve
extending our representation of πst−1 and θ with new states sampled from their posterior
distribution. Since there are no observations associated with these new states, the posterior
distribution equals the prior distribution.
To conclude our discussion of the beam sampler, it is useful to point out that ut need not
be sampled from the uniform distribution on [0, πst−1,st]. We can choose ut ∼πst−1stBeta(a, b)
which with the appropriate choice of a and b could bias our auxiliary variable closer to 0 or
closer to πst−1st. Smaller auxiliary variables imply that we consider more transitions in the
dynamic program, hence mixing can be faster, at a larger computational cost. Larger auxil-
iary variables imply that the dynamic program is sparser, hence mixing can be slower, at a
smaller computational cost. Note that we need to adjust the dynamic program in Eq. (15.6)

332
Jurgen Van Gael and Zoubin Ghahramani
MCMCiteration
Fraction of worst-case complexity
Figure 15.8 This plot shows an example of how eﬃcient the beam sampler is in practice. On the x-axis we plot
the iteration; on the y-axis we plot the fraction of the worst case complexity per state. Recall that if the sampler
currently represents K states, the worst-case complexity for a particular state will be K2.
slightly to take into account the density of the Beta distribution rather than the uniform
distribution.
15.5
Example: unsupervised part-of-speech tagging
Part-of-speech tagging (PoS-tagging) is the task of annotating the words in a sentence with
their appropriate part-of-speech tag: e.g. for the sentence ‘The man sat’, ‘The’ is a deter-
miner, ‘man’ is a noun and ‘sat’ is a verb. Part-of-speech-tagging is a standard component
in many linguistic processing pipelines so any improvement on its performance is likely to
impact a wide range of tasks.
Hidden Markov models are commonly used to model PoS-tags [21]: more speciﬁcally,
they are generally applied by setting the words in the corpus to correspond to the obser-
vations y1:T and the hidden representation s1:T is then interpreted as the unknown PoS-tag.
This parameterisation is particularly useful as the Markov chain allows the model to dis-
ambiguate between PoS-tags. For example in the sentence ‘The man sat’, the word ‘man’
could be a verb (as in ‘man your posts’). Because it appears after a determiner and before
a verb, it is more likely to be a noun. Most often, a PoS-tagger is trained in a supervised
fashion using a corpus of annotated sentences. Building this corpus is quite expensive;
hence [18], [14] and [12] explored the possibility of using unsupervised methods to learn
a good PoS-tagger. Using either EM, variational Bayes or MCMC methods, the authors
explored how well the states of an HMM that is trained purely unsupervised, correspond to
what humans typically consider as PoS-tags.
Choosing the number of hidden states is hard, as was carefully discussed in [18]. First
of all, there are many diﬀerent sets of PoS-tag classes of diﬀerent granularity resulting
in diﬀerent labelled corpora using diﬀerent PoS-tag classes. Secondly, even if we would
choose a particular number of PoS-classes, our HMM might decide that there is enough
statistical evidence to merge or split certain classes.
In this section we describe our experiments using the iHMM for learning the number of
PoS-tags automatically from data. We only discuss a very simple model as an illustration
of the concepts reviewed in this chapter. For a more complete overview of using the iHMM
for unsupervised PoS-tagging we refer to [37].

Nonparametric hidden Markov models
333
Our setup is to use an iHMM with γ = 5 and α = 0.8. A commonly used likelihood in
natural language applications is a simple multinomial distribution. We choose a symmet-
ric Dirichlet distribution as our base distribution H so it is conjugate to our likelihood. To
enforce a sparse output distribution for each state, we choose the symmetric Dirichlet with
parameters 0.1. We trained on section 0 of the Wall Street Journal (WSJ) part of the Penn
Treebank [22]. This dataset contains 1917 sentences with a total of 50 282 word tokens
(observations) and 7904 word types (dictionary size). We initialised the sampler with 50
states and assigned each st to one out of 50 states uniformly at random and ran the sam-
ple for 50 000 iterations. Figure 15.9 shows the evolution of the number of states for the
ﬁrst 10 000 iterations. As this plot clearly shows, the iHMM sampler seamlessly switches
between models of diﬀerent size between iterations.
As we already discussed in Section 15.4.2, when a sample of length T in the iHMM uses
K states, the beam sampler has a worst case complexity of O(TK2). However, in Fig. 15.8
we show how in our example after only 200 iterations only 10 per cent of the worst case
computations need to be done. In our example with T = 50 000 and K ∼30, one iteration
takes about 1 second on a quad core CPU.
To investigate whether the output of the iHMM-based PoS-tagger correlates with our
notion of PoS-tags, we consider the top ﬁve words for the ﬁve most common states;
see Table 15.1. The ﬁrst column, a.k.a. state 9 shows that the iHMM assigns a lot of weight
9 (8699)
12 (4592)
8 (4216)
18 (3502)
17 (3470)
of (1152)
the (1943)
, (2498)
company (93)
it (204)
to (1000)
a (944)
and (260)
year (85)
that (117)
in (829)
its (173)
’s (146)
market (52)
he (112)
and (439)
an (157)
said (136)
U.S. (46)
” (110)
for (432)
this (77)
– (95)
time (32)
they (101)
Table 15.1 This table analyses a sample after 40 000 iterations of the beam sampler. Each column in the table
describes a particular state and its ﬁve most frequent words in the sample we analysed. The top line of each
column is the state ID in the iHMM and its frequency in brackets. The following rows list the top ﬁve words with
their frequency in the sample.
Iteration
# Used states
Figure 15.9 This plot illustrates the evolution of the number of represented states in the beam sampler.

334
Jurgen Van Gael and Zoubin Ghahramani
to the words ‘of,to,in,and,for’. This corresponds well to the class of prepositions (IN in the
ground truth annotation scheme). When we look at the ground truth labelling of the corpus,
we ﬁnd that the annotators put ‘to’ in its own PoS-class (TO). The iHMM does not ﬁnd
any statistical reason to do so. Skimming over the other states we see that state 12 is a mix
of determiners (DT) and possessive pronouns (PRP$), state 8 takes care of punctuation
together with some coordinating conjunction words (CC), state 18 captures many nouns
(NN,NNS,NNP,NNPS) and state 17 captures personal pronouns (PRP).
15.6
Beyond the iHMM
In this section we discuss a number of extensions to the iHMM which we have found useful
in practice.
15.6.1
The input–output iHMM
The iHMM, like the HMM assumes that the Markov chain evolves completely
autonomously and the observations are conditionally independent given the Markov chain.
In many real scenarios, the Markov chain can be aﬀected by external factors: a robot is
driving around in an interior environment while taking pictures. The sequence of pictures
could be modelled by an iHMM where the latent chain represents the robot’s location (e.g.
a room index). If our robot doesn’t perform a random walk through the environment but
uses a particular policy, we can integrate the robot’s actions as inputs to the iHMM. We
call this model the input–output iHMM (IO-iHMM) in analogy to its ﬁnite counterpart the
IO-HMM by [5]. Fig. 15.10 illustrates the belief network of the IO-iHMM. A diﬀerent way
to think about the IO-iHMM is as a conditional sequence model: it predicts the sequence
of observations y1:T from inputs e1:T through a hidden representation s1:T. This model can
be useful for structured sequence prediction problems.
We can think of the IO-iHMM where the transition probabilities depend on the inputs et
as an iHMM with a three-dimensional transition matrix: for each input symbol e and each
previous state s, the probabilities for moving to the next state is p(st|st−1, et) = πet,st−1,st. The
vectors πe,s,· can be constructed similarly to the rows of the transition matrix in the iHMM,
by drawing πe,s,·
iid∼Stick(αβ). One eﬀect of introducing a dependency on input variables
is that the number of parameters increases. If we train a ﬁnite sequence of length T with
the iHMM and ﬁnd an eﬀective number of states equal to K then we have to learn O(K2)
Figure 15.10 Belief network of the IO-iHMM. The dotted lines denote the fact that we can choose to make either
the Markov chain or the observations (or both) dependent on a set of inputs.

Nonparametric hidden Markov models
335
transition matrix parameters. If we train the same sequence with the IO-iHMM using an
input sequence with E possible input symbols and ﬁnd K eﬀective states, then we have
to learn O(EK2) transition matrix parameters. Similarly, if we introduce a dependency of
the observation model on the input sequence, yt ∼F(θst,et), we augment the number of
parameters of the observation model with a factor of E. This suggests that either: (a) more
data will be needed to learn a model with the same capacity as the iHMM; (b) with the same
amount of data a model with smaller capacity will be learned or (c) the model complexity
must be reduced by coupling the parameters.
15.6.2
The sticky and block-diagonal iHMM
In many applications, it is useful to explicitly model the time scale of transitions between
states. For an HMM or iHMM, the weight on the diagonal of the transition matrix controls
the frequency of state transitions. The probability that we stay in state i for g time steps is
a geometric distribution p(g) = πg−1
ii
(1 −πii).
As we noted in the introduction to the iHMM, the original hierarchical Polya urn
description in [4] introduced a self-transition hyperparameter in the iHMM adding prior
probability mass to the diagonal of the transition matrix π. The authors in [10] further elab-
orated this idea in the hierarchical Dirichlet process representation of the iHMM. They
also developed a dynamic programming based inference algorithm for a truncated version
of the model. They coined it the sticky iHMM (or sticky HDP-HMM). The sticky iHMM
is particularly appropriate for segmentation problems where the number of segments is not
known a priori. Fox et al. [10] show impressive results on a speaker diarisation task.
Formally, the sticky iHMM still draws the base measure β ∼Stick but then draws the
rows of the transition matrix from the following distribution
πi j
iid∼Dirichlet(
α
α + κβ1,
α
α + κβ2, · · · , αβi + κ
α + κ , · · · ).
The eﬀect of this change is that for all states the diagonal entry carries more weight. The
parameter κ controls the switching rate or lengthscale of the process.
A more general model which allows grouping of states into a block-diagonal struc-
ture is given by the block-diagonal inﬁnite HMM in [31]. When the blocks are of size
one, this is a sticky iHMM, but larger blocks allow unsupervised clustering of states. The
block-diagional iHMM is used for unsupervised learning of view-based object models from
video data, where each block or cluster of states corresponds to an object and the model
assumptions capture the intuition that temporally contiguous video frames are more likely
to correspond to diﬀerent views of the same object than to diﬀerent objects.
Another approach to introduce more control over the self-transition probability is to
have an explicit duration model for the time spent in a particular state. This type of model
is known as a hidden semi-Markov model. Inference in these models is more costly than in
HMM’s and nonparametric versions have not yet been explored.
15.6.3
iHMM with Pitman–Yor base distribution
From the Polya urn scheme, we know that the base process (a.k.a. the oracle urn) behaves
like an average distribution over the states. In other words, the base process inﬂuences the
expected number of states visited for a given sequence length. The left plot in Fig. 15.11
plots the frequency of colours in a Polya urn for a Dirichlet process against the colour’s
rank. From this plot, we can see that the Dirichlet process is quite speciﬁc about the distri-
bution implied in the Polya urn: because the right tail of the plot drops oﬀsharply, we know

336
Jurgen Van Gael and Zoubin Ghahramani
)
b
(
(a)
Figure 15.11 In this plot we show (on a log-log scale) the frequency versus rank of colours in the Polya urn for
both the Dirichlet process (a) and the Pitman–Yor process (b). We see that the data from the Pitman–Yor process
can be ﬁtted with a straight line implying that it follows a power-law behaviour; this is not the case for the Dirichlet
process.
that the amount of colours that appear only once or twice is very small. A two-parameter
generalisation of the Dirichlet process, known as the Pitman–Yor process ([25]) introduces
more ﬂexibility in this behaviour. The right plot in Fig. 15.11 shows that the frequency-rank
for draws from a Pitman–Yor process can be more speciﬁc about the tails. More in particu-
lar, the Pitman–Yor process ﬁts a power-law distribution; [15] and [33] showed how in the
context of language modelling the Pitman–Yor distribution encodes more realistic priors.
Analogous to the Dirichlet process, the Pitman–Yor process has a stick-breaking construct-
ing: the sticks for the Pitman–Yor distribution with parameters d and α can be constructed
by drawing ˆβi
iid∼Beta(1 −d, α + id) and βi = ˆβi
Qi−1
l=1(1 −ˆβl). Note that the Pitman–Yor
process with d = 0 is exactly the Dirichlet process.
In the context of the iHMM, it is fairly straightforward to replace the Dirichlet pro-
cess base distribution by a Pitman–Yor distribution allowing the iHMM to use more
states. Although the collapsed Gibbs sampler and beam sampler work with only minor
adjustments, it is critical that the beam sampler be implemented with great care. As we
mentioned in the discussion of the beam sampler, we need to consider all transitions ij
where πij > ut. Unfortunately, the quantity πij decreases much faster for the Pitman–Yor
process stick-breaking construction than the Dirichlet process stick-breaking construction.
The advantage is that it allows for many small transition probabilities, unfortunately it also
means that the quantity P
l πil decreases very slowly. Since the beam sampler must consider
all πi j > ut, it will need to expand the transition matrix to K states so that P
l>K πil < ut and
it is sure it considered all possible states. This means when running the forward-ﬁltering
backward-sampling algorithm that we potentially end up with a very large transition matrix.
Luckily most of the entries in π will be quite small (otherwise the remaining mass P
j>K πi j
can’t decrease slowly). Hence, while expanding the transition matrix we can keep a sorted
list of matrix entries and when we run the forward-ﬁltering step, at each time step we walk
down this list until ut becomes larger than the elements in the list.
15.6.4
The auto-regressive iHMM and switching linear dynamical system
Many time series are modelled using auto-regressive (AR) models or linear dynamical
systems (LDSs) which assume that the dynamics of the process are linear. More power-
ful non-linear generalisations can be obtained by switching between a ﬁxed set of linear

Nonparametric hidden Markov models
337
Figure 15.12 The belief network for a switching linear dynamical system with a potentially inﬁnitely large state
space.
dynamics. A model where the switching is driven by an HMM is often called a switching
linear dynamical system (SLDS)5; [13] review a number of variants of the SLDS all of
whom share the property that there are a ﬁxed ﬁnite number of dynamics.
An extension of the SLDS in [9] allows for an arbitrary large number of dynamics to
be learned from data by replacing the ﬁnite Markov chain underlying the SLDS with an
iHMM. They explored two prototypical variants of the SLDS. The ﬁrst model assumes the
observations follow auto-regressive dynamics; this model is referred to as the AR-iHMM.
The second model assumes that only part of the continuous variables are observed and
the unobserved variables follow linear dynamics; this model is referred to as the inﬁnite
SLDS. The inference in [9] is implemented on a ﬁnite truncation of the nonparametric
model. In this incarnation, the model is similar to the Bayesian SLDS from [6], except that
the nonparametric model introduces an additional ‘prior on the prior’ for the rows of the
transition matrix.
Figure 15.12 illustrates the belief network for the SLDS with a potentially inﬁnitely
large state space. Inference in both models can be done eﬃciently using the beam sampler.
For the AR model, a collapsed Gibbs sampler is possible while for the LDS we are only
aware of algorithms that explicitly represent the hidden state of the LDS.
15.7
Conclusions
We reviewed both the hierarchical Polya urn construction and the hierarchical Dirichlet
process construction for the iHMM and showed a proof that both formalisations are equiv-
alent. Building on these diﬀerent representations of the iHMM, we described a collapsed
Gibbs sampler and a dynamic programming based beam sampler. Both inference methods
are only marginally harder to implement than their HMM based counterparts. We believe
this makes the iHMM an attractive solution to the problems one faces when learning the
number of states for an HMM.
As we have described, there are a number of interesting ways of extending iHMMs. We
can allow inputs and outputs, sticky self-transitions, block-structured models, Pitman–Yor
base distributions, autoregressive and switching linear structure. Given the important role
5Also known as a Markov-switching model, Markov jump system or switching state space model.

338
Jurgen Van Gael and Zoubin Ghahramani
of hidden Markov models in time series and sequence modelling, and the ﬂexibility of non-
parametric approaches, there is great potential for many future applications and extensions
of the inﬁnite hidden Markov model.
15.A
Appendix: Equivalence of the hierarchical Polya urn and hierarchical
Dirichlet process
In this appendix, we sketch a proof that the hierarchical Polya urn scheme from [4] and
the hierarchical Dirichlet process formulations of the iHMM from [35] deﬁne equivalent
distributions over the hidden state sequence s1:T. We will refer to the distribution over s1:T
deﬁned by the hierarchical Polya urn scheme as purn(s1:T) and the distribution deﬁned by
the hierarchical Dirichlet process as phdp(s1:T).
Although we didn’t introduce the Chinese Restaurant Franchise representation of the
HDP, our proof relies on this representation. We refer to [35] for a detailed description of
this representation. We ﬁrst introduce two diﬀerent sets of statistics that will be used in the
proof:
• for the Polya urn we keep track of the numbers ni j specifying the number of balls in
transition urn i of colour j and the number ci specifying the number of balls of colour
i in the oracle urn;
• for the hierarchical Dirichlet process we keep track of the number of customers in
the Chinese Restaurant Franchise (CRF) representation ([35]): mi jl is the number of
customers sitting in restaurant i at the lth table that serves dish j and bl is the number
of customers sitting at the table serving dish l in the base restaurant (or in other
words: the number of tables serving dish l in all restaurants together).
Theorem 15.1 The distribution over the sequence of states s1:T deﬁned by the hierarchical
Polya urn scheme, purn(s1:T) is the same as the distribution deﬁned by the hierarchical
Dirichlet process phdp(s1:T).
The core idea of the inductive proof is that the CRF representation keeps track of a bit
more information than the Polya urn: in the CRF, mi jl keeps track of who sits at which table
serving a particular dish, while in the Polya urn ni j only keeps track of the total number of
people who are sitting at a table serving dish j. Since the exact conﬁguration doesn’t matter
for the predictive probability p(st|s1:t−1), the two schemes deﬁne the same distribution over
the sequence s1:T. We formalise this idea in the proof sketch below.
Sketch of proof
The proof follows an inductive argument. For the base case of the induc-
tion we note that purn(s1) = phdp(s1) = 1 because initially there are no customers in the
CRF nor are there balls in the Polya urn and hence we can always initialise the ﬁrst state
to be the state with identiﬁer 1. After this assignment, note that we called the oracle once.
Hence c1 = 1 and we have set n11 = 1, analogously, m111 = 1 since there is one person
sitting in the whole CRF and he must be in restaurant 1 eating dish 1 and b1 = 1 since there
is only one table serving dish 1 in the whole CRF.
Next we describe the inductive argument: after generating state st−1, we assume bi = ci
for all i and for any conﬁguration mijl : nij = P
l mi jl. Note that we don’t assume a particular
conﬁguration for mijl but any conﬁguration that satisﬁes ni j = P
l mi jl. We now prove that
purn(st|s1:t−1) = phdp(st|s1:t−1). If we are in state st−1 under the Polya urn scheme there are

Nonparametric hidden Markov models
339
two possibilities: either we query the oracle or we don’t. We query the oracle with probabil-
ity
α
P
j nst−1, j+α and if we do so, we either go to state i with probability
ci
P
k ck+γ or to a new state
with probability
γ
P
k ck+γ. Under the HDP, the probability that we choose to sit at a new table
in restaurant st−1 is
α
P
jl mst−1,j,l+α. If we do choose to sit at a new table, we choose an existing
dish i with probability
bi
P
k bk+γ and a new dish with probability
γ
P
k bk+γ. Since choosing a
dish in the HDP representation corresponds to transition to a particular state in the iHMM,
we can see that under the induction hypothesis conditioned on the fact that we query the
oracle (or sit at a new table) the probabilities are exactly the same. Similarly, if under the
Polya urn scheme, we decide not to query the oracle, we transition to state i with probability
nst−1,i
P
j nst−1, j+α. Under the CRF, we will sit at a table serving dish i with probability
P
l mst−1,i,l
P
jl mst−1,j,l+α.
Under the induction hypothesis, these two probabilities are again the same. QED.
Acknowledgments
We would like to thank Finale Doshi-Velez, Andreas Vlachos and
S´ebastien Brati`eres for their helpful comments on the manuscript. We thank the two anony-
mous reviewers for valuable suggestions. Jurgen Van Gael is supported by a Microsoft
Research Scholarship.
Bibliography
[1] H. Akaike. A new look at the statistical model
identiﬁcation. IEEE Transactions on Automatic
Control, 19(6):716–723, 1974.
[2] L. E. Baum, T. Petrie, G. Soules and N. Weiss. A
maximization technique occurring in the
statistical analysis of probabilistic functions of
Markov chains. Annals of Mathematical
Statistics, 41(1):164–171, 1970.
[3] M. J. Beal. Variational algorithms for
approximate Bayesian inference. PhD thesis,
University of London, 2003.
[4] M. J. Beal, Z. Ghahramani and C. E. Rasmussen.
The inﬁnite hidden Markov model. In Advances
in Neural Information Processing Systems, pages
577–584, 2002.
[5] Y. Bengio and P. Frasconi. An input output
HMM architecture. In Advances in Neural
Information Processing Systems, pages 427–434,
1995.
[6] S. Chiappa. A Bayesian approach to switching
linear Gaussian state-space models for
unsupervised time-series segmentation. In
Proceedings of the International Conference on
Machine Learning and Applications, pages 3–9,
2008.
[7] A. P Dempster, N. M Laird and D. B Rubin.
Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal
Statistical Society. Series B (Methodological),
39(1):1–38, 1977.
[8] L. E. Baum and T. Petrie. Statistical inference
for probabilistic functions of ﬁnite state Markov
chains. Annals of Mathematical Statistics,
37(6):1554–1563, 1966.
[9] E. B. Fox, E. B. Sudderth, M. I. Jordan and A. S.
Willsky. Nonparametric Bayesian learning of
switching linear dynamical systems. In Advances
in Neural Information Processing Systems, pages
457–464, 2009.
[10] E. B. Fox, E. B. Sudderth, M. I. Jordan and
A. S. Willsky. An HDP-HMM for systems with
state persistence. In Proceedings of the
International Conference on Machine learning,
volume 25, Helsinki, 2008.
[11] S. Fruhwirth-Schnatter. Estimating marginal
likelihoods for mixture and Markov switching
models using bridge sampling techniques.
Econometrics Journal, 7(1):143–167, 2004.
[12] J. Gao and Johnson. M. A comparison of
Bayesian estimators for unsupervised hidden
Markov model POS taggers. In Proceedings of
the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 344–352,
2008.
[13] Z. Ghahramani and G. E. Hinton. Variational
learning for switching state-space models.
Neural Computation, 12(4):831–864, 2000.
[14] S. Goldwater and T. Griﬃths. A fully Bayesian
approach to unsupervised part-of-speech
tagging. In Proceedings of the Association for
Computational Linguistics, volume 45, page
744, 2007.
[15] S. Goldwater, T. Griﬃths and M. Johnson.
Interpolating between types and tokens by
estimating power-law generators. In Advances in
Neural Information Processing Systems, pages
459–466, 2006.
[16] P. J. Green. Reversible jump Markov chain
Monte Carlo computation and Bayesian model
determination. Biometrika, 82(4):711–732, 1995.
[17] N. Hjort, C. Holmes, P. Muller and S. Walker,
editors. Bayesian Nonparametrics. Cambridge
University Press, 2010.

340
Jurgen Van Gael and Zoubin Ghahramani
[18] M. Johnson. Why doesnt EM ﬁnd good HMM
POS-taggers. In Proceedings of the Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning, pages 296–305,
2007.
[19] D. Jurafsky and J. H. Martin. Speech and
Language Processing. Pearson Prentice Hall,
2008.
[20] D. J. C. MacKay. Ensemble learning for hidden
Markov models. Technical report, Cavendish
Laboratory, University of Cambridge, 1997.
[21] C. D. Manning and H. Sch¨utze. Foundations of
Statistical Natural Language Processing. MIT
Press.
[22] M. P. Marcus, M. A. Marcinkiewicz and B.
Santorini. Building a large annotated corpus of
English: the Penn Treebank. Computational
Linguistics, 19(2):313–330, June 1993.
[23] R. M. Neal. Annealed importance sampling.
Statistics and Computing, 11:125–139, 1998.
[24] R. M. Neal. Markov chain sampling methods for
Dirichlet process mixture models. Journal of
Computational and Graphical Statistics,
9(2):249–265, 2000.
[25] J. Pitman. Combinatorial stochastic processes,
volume 1875 of Lecture Notes in Mathematics.
Springer-Verlag, 2006.
[26] L. R. Rabiner. A tutorial on hidden Markov
models and selected applications in speech
recognition. Proceedings of the IEEE,
77(2):257–286, 1989.
[27] C. P. Robert, T. Ryden and D. M. Titterington.
Bayesian inference in hidden Markov models
through the reversible jump Markov chain Monte
Carlo method. Journal of the Royal Statistical
Society. Series B, Statistical Methodology, pages
57–75, 2000.
[28] G. Schwarz. Estimating the dimension of a
model. Annals of Statistics, 6(2):461–464, 1978.
[29] S. L. Scott. Bayesian methods for hidden
Markov models: Recursive computing in the 21st
century. Journal of the American Statistical
Association, 97(457):337–351, 2002.
[30] J. Sethuraman. A constructive deﬁnition of
Dirichlet priors. Statistica Sinica, 4:639–650,
1994.
[31] T. Stepleton, Z. Ghahramani, G. Gordon and
T-S. Lee. The block diagonal inﬁnite hidden
Markov model. In Proceedings of the
International Conference on Artiﬁcial
Intelligence and Statistics, pages 552–559, 2009.
[32] A. Stolcke and S. Omohundro. Hidden Markov
model induction by Bayesian model merging.
Advances in Neural Information Processing
Systems, 5: pages 11-18, 1993.
[33] Y. W. Teh. A hierarchical Bayesian language
model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics, pages 985–992, 2006.
[34] Y. W. Teh. Dirichlet processes. Encyclopedia of
Machine Learning, to appear.
[35] Y. W. Teh, M. I. Jordan, M. J. Beal and D. M.
Blei. Hierarchical Dirichlet processes. Journal of
the American Statistical Association,
101(476):1566–1581, 2006.
[36] J. Van Gael, Y. Saatci, Y. W. Teh and
Z. Ghahramani. Beam sampling for the inﬁnite
hidden Markov model. In Proceedings of the
International Conference on Machine Learning,
pages 1088–1095, 2008.
[37] J. Van Gael, A. Vlachos and Z. Ghahramani. The
inﬁnite HMM for unsupervised POS tagging. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages
678–687, 2009.
[38] S. G. Walker. Sampling the Dirichlet mixture
model with slices. Communications in Statistics
– Simulation and Computation, 36(1):45, 2007.
Contributors
Jurgen Van Gael, Department of Engineering, University of Cambridge
Zoubin Ghahramani, Department of Engineering, University of Cambridge

16
Bayesian Gaussian process models for multi-sensor
time series prediction
Michael A. Osborne, Alex Rogers, Stephen J. Roberts,
Sarvapali D. Ramchurn and Nick R. Jennings
16.1
Introduction
Sensor networks have recently generated a great deal of research interest within the com-
puter and physical sciences, and their use for the scientiﬁc monitoring of remote and hostile
environments is increasingly commonplace. While early sensor networks were a simple
evolution of existing automated data loggers, that collected data for later oﬄine scientiﬁc
analysis, more recent sensor networks typically make current data available through the
Internet, and thus, are increasingly being used for the real-time monitoring of environmen-
tal events such as ﬂoods or storm events (see [10] for a review of such environmental sensor
networks).
Using real-time sensor data in this manner presents many novel challenges. However,
more signiﬁcantly for us, many of the information processing tasks that would previously
have been performed oﬄine by the owner or single user of an environmental sensor network
(such as detecting faulty sensors, fusing noisy measurements from several sensors, and
deciding how frequently readings should be taken), must now be performed in real-time
on the mobile computers and PDAs carried by the multiple diﬀerent users of the system
(who may have diﬀerent goals and may be using sensor readings for very diﬀerent tasks).
Importantly, it may also be necessary to use the trends and correlations observed in previous
data to predict the value of environmental parameters into the future, or to predict the
reading of a sensor that is temporarily unavailable (e.g. due to network outages). Finally,
we note that the open nature of the network (in which additional sensors may be deployed,
and existing sensors may be removed, repositioned or updated at any time) means that
these tasks may have to be performed with only limited knowledge of the precise location,
reliability and accuracy of each sensor.
Many of the information processing tasks described above have previously been tackled
by applying principled Bayesian methodologies from the academic literature of geospa-
tial statistics and machine learning, speciﬁcally, kriging [4] and Gaussian processes [22].
However, due to the computational complexity of these approaches, to date they have
largely been used oﬄine in order to analyse and re-design existing sensor networks (e.g.
to reduce maintenance costs by removing the least informative sensors from an existing
sensor network [7], or to ﬁnd the optimum placement of a small number of sensors, after
a trial deployment of a larger number has collected data indicating their spatial correlation

342
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
[13]). Thus, there is a clear need for more computationally eﬃcient algorithms, that can be
deployed on the mobile computers and PDAs carried by our ﬁrst responders, in order to
perform this information processing in real-time.
Against this background, this chapter describes our work developing just such an
algorithm. More speciﬁcally, we present a novel iterative formulation of a Gaussian
process (GP) that uses a computationally eﬃcient implementation of Bayesian Monte
Carlo to marginalise hyperparameters, eﬃciently re-uses previous computations by fol-
lowing an online update procedure as new data sequentially arrives, and uses a prin-
cipled ‘windowing’ of data in order to maintain a reasonably sized (active) dataset.
We use our algorithm to build a probabilistic model of the environmental variables
being measured by sensors within the network, tolerant to data that may be missing,
delayed, censored and/or correlated. This model allows us to perform information pro-
cessing tasks including: modelling the accuracy of the sensor readings, predicting the
value of missing sensor readings, predicting how the monitored environmental vari-
ables will evolve in the near future, and performing active sampling by deciding when
and from which sensor to acquire readings. We validate our multi-output Gaussian pro-
cess formulation using data from networks of weather sensors, and we demonstrate its
eﬀectiveness by benchmarking it against alternative methods. Our results are promising,
and represent a step towards the deployment of real-time algorithms that use princi-
pled machine learning techniques to autonomously acquire and process data from sensor
networks.
The remainder of this chapter is organised as follows: Section 16.2 describes the
information processing problem that we face. Section 16.3 presents our Gaussian process
formulation, and Section 16.4 describes the sensor networks used to validate this formula-
tion. In Section 16.5 we present experimental results using data from these networks, and
in Section 16.6 we discuss the computational cost of our algorithm. Finally, related work is
discussed in Section 16.7, and we conclude in Section 16.8.
16.2
The information processing problem
As discussed above, we require that our algorithm be able to autonomously perform data
acquisition and information processing despite having only limited speciﬁc knowledge of
each of the sensors in their local neighbourhoods (e.g. their precise location, reliability and
accuracy). To this end, we require that the algorithm explicitly represents:
1. The uncertainty in the estimated values of environmental variables being measured,
noting that sensor readings will always incorporate some degree of measurement
noise.
2. The correlations or delays that exist between sensor readings; sensors that are close
to one another, or in similar environments, will tend to make similar readings, while
many physical processes involving moving ﬁelds (such as the movement of weather
fronts) will induce delays between sensors.
We then require that it uses this representation in order to:
1. Perform regression and prediction of environmental variables; that is, interpolate
between sensor readings to predict variables at missing sensors (i.e. sensors that have
failed or are unavailable due to network outages), and perform prediction in order to
support decision making.

Gaussian processes for time series prediction
343
2. Perform eﬃcient active sampling by selecting when to take a reading, and which
sensor to read from, such that the minimum number of sensor readings are used
to maintain the estimated uncertainty in environmental variables below a speciﬁed
threshold (or similarly, to minimise uncertainty given a constrained number of sensor
readings). Such constraints might reﬂect either the computational cost of processing
an additional reading or the energy cost associated with the sensor acquiring a new
observation.
More speciﬁcally, the problem that we face can be cast as a multivariate regression and
decision problem in which we have l = 1, . . . , L environmental variables yl ∈R of interest
(such as air temperature, wind speed or direction speciﬁed at diﬀerent sensor locations).
We assume a set of N potentially noisy sensor readings, [l1, t1], z1
, . . . , [lN, tN], zN
	, in
which we, for example, observe the value z1 for the l1th variable at time t1, whose true
unknown value is y1. Where convenient, we may group the inputs as x = [l, t]. Note that we
do not require that all the variables are observed at the same time, nor do we impose any
discretisation of our observations into regularly spaced time steps. Given this data, we are
interested in inferring the vector of values y⋆for any other vector of variables labelled by
l⋆at times t⋆.
16.3
Gaussian processes
Multivariate regression problems of the form described above have often been addressed
using multi-layer neural networks. However, Gaussian processes (GPs) are increasingly
being applied in this area. They represent a powerful way to perform Bayesian inference
about functions; we consider our environmental variables as just such a function [22]. This
function takes as inputs the variable label and time pair x and produces as output the vari-
able’s value y. In this work, we will assume that our inputs are always known (e.g. our data
is time-stamped), and will incorporate them into our background knowledge, or context,
I. A GP is then a generalised multivariate Gaussian prior distribution over the (potentially
inﬁnite number of) outputs of this function
p( y | µ, K, I ) ≜N(y; µ, K) ≜
1
√det (2πK)
exp
 
−1
2 (y −µ)T K−1 (y −µ)
!
.
It is speciﬁed by prior mean and covariance functions, which generate µ and K. We empha-
sise that we use a single GP to express a probability distribution over all our environmental
variables, the correlations amongst which are expressed by our covariance function. The
multivariate Gaussian distribution is qualiﬁed for this role due to the fact that both its
marginal and conditional distributions are themselves Gaussian. This allows us to produce
analytic posterior distributions for any variables of interest, conditioned on whatever sen-
sor readings have been observed. These distributions have both an analytic mean, which
we use as our best estimate of the output, as well as an analytic variance, which we use as
an indication of our uncertainty.
While the fundamental theory of GPs is well established (see [22] for example), there is
much scope for the development of computationally eﬃcient implementations. To this end,
in this work we present a novel online formalism of a multi-dimensional GP that allows us
to model the correlations between sensor readings, and to update this model online as new
observations are sequentially available. Space precludes a full description of this algorithm
(see [19] for further details), however, in the next sections we describe the covariance func-
tions that we use to represent correlations and delays between sensor readings, the Bayesian
Monte Carlo method that we use to marginalise the hyperparameters of these covariance

344
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
functions, and how we eﬃciently update the model as new data is received, by reusing
the results of previous computations, and applying a principled ‘windowing’ of our data
series.
16.3.1
Covariance functions
The prior mean of a GP represents whatever we expect for our function before seeing any
data. We take this as a function constant in time, such that µ([l, t]) = µl. The covariance
function of a GP speciﬁes the correlation between any pair of outputs. This can then be used
to generate a covariance matrix over our set of observations and predictants. Fortunately,
there exist a wide variety of functions that can serve in this purpose [1, 25], which can
then be combined and modiﬁed in a further multitude of ways. This gives us a great deal
of ﬂexibility in our modelling of functions, with covariance functions available to model
periodicity, delay, noise and long-term drifts.
As an example, consider a covariance given by the Hadamard product of a covari-
ance function over time alone and a covariance function over environmental variable labels
alone, such that
K([l, t], [l′, t′]) ≜Klabel(l, l′) Ktime(t −dl, t′ −dl′),
where d allows us to express the delays between environmental variables. We will often
use the completely general spherical parameterisation, s, such that
Klabel(l, l′) ≜diag (g) sTs diag (g),
(16.1)
where g gives an intuitive lengthscale for each environmental variable, and sTs is the cor-
relation matrix [20]. This parameterisation allows us to represent any potential degree of
correlation between our variables. As a less general alternative, we might instead use a term
dependent on the spatial separation between sensors.
Similarly, we can represent correlations over time with a wide variety of covariance
functions, permitting the incorporation of what domain knowledge we have. For example,
we use the additive combination of a periodic and a non-periodic disturbance term
Ktime(t, t′) ≜Ktime,1(t, t′) + Ktime,2(t, t′),
where we expect our variable to be well represented by the superposition of an oscillatory
and a non-oscillatory component. We represent both terms using the Mat´ern class [22]
(with ν = 5/2), given by
Ktime,i(t, t′) ≜h2
1 +
√
5ri + 5r2
i
3
exp

−
√
5ri

,
(16.2)
for i ∈{1, 2}, where r1 = sin π
 t−t′
w
 for periodic terms, and r2 =
 t−t′
w
 for non-periodic
terms. The Mat´ern class allows us to empirically select a degree of smoothness, given by
the choice of ν, appropriate for the functions we are trying to track. Finally, to represent
measurement noise, we further extend the covariance function to
V([l, t], [l′, t′]) ≜K([l, t], [l′, t′]) + σ2 δ([l, t] −[l′, t′]) ,
where δ(−) is the Kronecker delta and σ2 represents the variance of additive Gaussian
noise.

Gaussian processes for time series prediction
345
This choice of covariance is intended to model correlated periodic variables subject
to local disturbances which may themselves be correlated amongst variables. This general
model describes many environmental variables that are subject to some daily cycle (e.g.
the 12-hour cycle of the tide, or the 24-hour cycle seen in most temperature readings), but
we reiterate that, given diﬀerent domain knowledge, a variety of other covariance functions
can be chosen. For example, a more suitable covariance for air temperature was found to
include an additional additive covariance term over time. This allows for the possibility of
both long-term drifts in temperature occurring over the course of a week, as well as more
high-frequency, hourly changes. We might also multiply periodic terms (like Ktime,1(t, t′))
by another non-periodic term (like Ktime,2(t, t′)) to express a short-term periodicity, whose
relevance tails oﬀwith time. With such a covariance, we can model periodic patterns that
may gradually change over time.
Given these examples of the plethora of possibilities for including additional terms to
our covariance, a natural question is when to stop. The answer is, in principle, never. The
‘Occam’s razor’ action of Bayesian inference [16] will automatically lead us to select the
simplest sub-model that still explains the data. Note this is true even if our prior is ﬂat over
the model complexity.
In practice, however, the ﬂexibility of our model comes at the cost of the introduction of
a number of hyperparameters, which we collectively denote as φ. These include correlation
hyperparameters (i.e. g, s and d), along with others such as the periods and amplitudes of
each covariance term (i.e. w and h) and the noise deviation σ. The constant prior means
µ1, . . . , µM are also included as additional hyperparameters. Taking these hyperparameters
as given and using the properties of the Gaussian distribution, we are able to write our
predictive equations as
p( y⋆| z1:N, φ, I ) = N(y⋆; m⋆,C⋆) ,
(16.3)
where, collecting our inputs as x⋆≜[l⋆, t⋆] and x1:N ≜[l1:N, t1:N], we have
m⋆= µφ(x⋆) + Kφ(x⋆, x1:N)Vφ(x1:N, x1:N)−1(z1:N −µφ(x1:N)),
(16.4)
C⋆= Kφ(x⋆, x⋆) −Kφ(x⋆, x1:N)Vφ(x1:N, x1:N)−1Kφ(x1:N, x⋆).
(16.5)
16.3.2
Marginalisation
Of course, it is rare that we can be certain a priori about the values of our hyperparameters.
Rather than Eq. (16.3), we must consider
p( y⋆| z1:N, I ) =
R
dφ p( y⋆| z1:N, φ, I ) p( z1:N | φ, I ) p( φ | I )
R
dφ p( z1:N | φ, I ) p( φ | I )
,
in which we have marginalised φ. Unfortunately, both our likelihood p( z1:N | φ, I ) and
predictions p( y⋆| z1:N, φ, I ) exhibit non-trivial dependence upon φ and so our integrals are
non-analytic. As such, we resort to quadrature, which inevitably involves evaluating the
two quantities
q(φ) ≜p( y⋆| z1:N, φ, I ) ,
r(φ) ≜p( z1:N | φ, I )
at a set of η sample points φs = {φ1, . . . , φη}, giving qs ≜q(φs) and rs ≜r(φs). Of course,
this evaluation is a computationally expensive operation. Dense sampling over the space
of all possible sets of hyperparameters is clearly infeasible. Note that the more complex

346
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
I
Ψ
j ∈S
i ∈S
Qi
Rj
φi
φj
j
S
i
S
Qi
Rj
φi
φj
Figure 16.1 Belief network for marginalising hyperparameters using Bayesian Monte Carlo. Shaded nodes are
known and double-circled nodes are deterministic given all their parents. All Q nodes are correlated with one
another, as are all R nodes, and the context I is correlated with all nodes.
our model, and hence the greater the number of hyperparameters, the higher the dimension
of the hyperparameter space we must sample in. As such, the complexity of models we
can practically consider is limited by the curse of dimensionality. We can view our sparse
sampling as introducing a form of uncertainty about the functions q and r, which we can
again address using Bayesian probability theory.
To this end, we apply Bayesian Monte Carlo, and thus, assign further GP priors to q and
r [18, 21]. This choice is motivated by the fact that variables over which we have a mul-
tivariate Gaussian distribution are joint Gaussian with any projections of those variables.
Given that integration is a projection, we can hence use our computed samples qs in order
to perform Gaussian process regression about the value of integrals over q(φ), and similarly
for r. Note that the quantity we wish to perform inference about,
ψ ≜p( y⋆| q, r, z1:N, I ) =
R
q(φ⋆) r(φ⋆) p( φ⋆| I ) dφ⋆
R
r(φ⋆) p( φ⋆| I ) dφ⋆
,
possesses richer structure than that previously considered using Bayesian Monte Carlo
techniques. In our case, r(φ) appears in both our numerator and denominator integrals,
introducing correlations between the values we estimate for them. The correlation structure
of this probabilistic model is illustrated in Fig. 16.1.
In considering any problem of inference, we need to be clear about both what informa-
tion we have and which uncertain variables we are interested in. In our case, both function
values, qs and rs, and their locations, φs, represent valuable pieces of knowledge.1 As with
our convention above, we will take knowledge of sample locations φs to be implicit within
I. We respectively deﬁne m(q) and m(r) as the means for q and r conditioned on qs and rs
from Eq. (16.4), C(q) and C(r) the similarly conditional covariances from Eq. (16.5). The
ultimate quantity of our interest is then
p( y⋆| qs, rs, z1:N, I )
=
$
p( y⋆| q, r, z1:N, I ) p( ψ | q, r, I ) p( q | qs, I ) p( r | rs, I ) dψ dq dr
1As discussed by [17], traditional, frequentist Monte Carlo eﬀectively ignores the information content of φs,
leading to several unsatisfactory features.

Gaussian processes for time series prediction
347
=
$
ψ δ
ψ −
R
q⋆r⋆p( φ⋆| I ) dφ⋆
R
r⋆p( φ⋆| I ) dφ⋆
N

q; m(q),C(q)
N

r; m(r),C(r)
dψ dq dr
=
Z R
m(q)
⋆r⋆p( φ⋆| I ) dφ⋆
R
r⋆p( φ⋆| I ) dφ⋆
N

r; m(r),C(r)
dr.
Here, unfortunately, our integration over r becomes non-analytic. However, we can employ
a Laplace approximation by expanding the integrand around an assumed peak at m(r).
Before we can state its result, to each of our hyperparameters we assign a Gaussian prior
distribution (or if our hyperparameter is restricted to the positive reals, we instead assign a
Gaussian distribution to its log) given by
p( φ | I ) ≜N

φ; ν, λTλ

.
Note that the intuitive spherical parameterisation (16.1) assists with the elicitation of priors
over its hyperparameters. We then assign squared exponential covariance functions for the
GPs over both q and r, given by K(φ, φ′) ≜N

φ; φ′, wTw

.
Finally, using the further deﬁnition for i, j ∈{1, . . . , η}
Ns(i, j) ≜N
 "φi
φj
#
;
"ν
ν
#
,
"λTλ + wTw
λTλ
λTλ
λTλ + wTw
#!
,
our Laplace approximation gives us
p( y⋆| z1:N, I ) ≃qT
s ρ ,
(16.6)
where the weights of this linear combination are
ρ ≜
K(φs, φs)−1 Ns K(φs, φs)−1 rs
1T
s,1 K(φs, φs)−1 Ns K(φs, φs)−1 rs
,
(16.7)
and 1s,1 is a vector containing only ones of dimensions equal to qs. With a GP on
p( y⋆| φ, I ), each qi = p( y⋆| z1:N, φi, I ) will be a slightly diﬀerent Gaussian. Hence we
eﬀectively approximate p( y⋆| z1:N, I ) as a Gaussian (process) mixture; Bayesian Monte
Carlo returns a weighted sum of our predictions evaluated at a sample set of hyperpa-
rameters. The assignment of these weights is informed by the best use of all pertinent
information. As such, it avoids the risk of overﬁtting that occurs when applying a less
principled technique such as likelihood maximisation [16].
16.3.3
Censored observations
In the work above, we have assumed that observations of our variables of interest were
corrupted by simple Gaussian noise. However, in many contexts, we instead observe cen-
sored observations. That is, we might observe that a variable was above or below certain
thresholds, but no more. Examples are rich within the weather sensor networks considered.
Float sensors are prone to becoming lodged on sensor posts, reporting only that the water
level is below that at which it is stuck. Other observations are problematically rounded to
the nearest integer – if we observe a reading of x, we can say only that the true value was
between x −0.5 and x + 0.5. We extend our sequential algorithms to allow for such a noise
model.

348
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
More precisely, we assume that we actually observe bounds bc that constrain Gaussian-
noise-corrupted versions zc of the underlying variables of interest yc at xc. This framework
allows for imprecise censored observations. We may additionally possess observations
zd that are, as previously considered, Gaussian-noise-corrupted versions of the underly-
ing variables of interest yd at xd. Note that the noise variance for censored observations
may diﬀer from the noise variance associated with other observations. Conditioned on this
combination of censored and uncensored observations, the distribution for our variables of
interest is
p( y⋆| zd, bc, I ) =
R
dφ
R
bc dzc p( y⋆| zd, zc, φ, I ) p( zc | zd, φ, I ) p( zd | φ, I ) p( φ|I )
R
dφ
R
bc dzc p( zc | zd, φ, I ) p( zd | φ, I ) p( φ|I )
.
While we cannot determine this full, non-Gaussian distribution easily, we can analyti-
cally determine its mean and covariance. We use the abbreviations mc|d ≜m(zc |zd, φ, I)
and Cc|d
≜C(zc |zd, φ, I). To reﬂect the inﬂuence of our censored observations, the
ﬁrst required modiﬁcation to our previous results is to incorporate a new term into our
likelihoods,
r(cd)(φ) = N(zd; m(zd |φ, I) ,C(zd |φ, I))
Z
bc
dzc N zc; mc|d,Cc|d
 ,
reﬂecting this new knowledge that zc is constrained by bounds bc. This gives us the new
weights over hyperparameter samples
ρ(cd) ≜
K(φs, φs)−1 Ns K(φs, φs)−1 r(cd)
s
1T
s,1 K(φs, φs)−1 Ns K(φs, φs)−1 r(cd)
s
.
We can then write our predictive mean as
m( y⋆| zd, bc, I ) =
X
i∈s
ρ(cd)
i
 
µφi(x⋆) + Kφi(x⋆, [xc, xd])
× Vφi([xc, xd], [xc, xd])−1
"
R
bc dzc zc N( ; ,zc)mc|dCc|d
R
bc dzc N( ; ,zc)mc|dCc|d −µφi(xc)
yd −µφi(xd)
#!
,
(16.8)
noting that a censored observation is intuitively treated as an uncensored observation equal
to the conditional mean of the GP over the bounded region. We have also the predictive
covariance
C( y⋆| zd, bc, I ) =
X
i∈s
ρ(cd)
i

Kφi(x⋆, x⋆)
−Kφi(x⋆, [xc, xd])Vφi([xc, xd], [xc, xd])−1Kφi([xc, xd], x⋆)

.
(16.9)
We now have the problem of approximating the integrals
R
bc dzc N zc; mc|d,Cc|d

and
R
bc dzc zc N zc; mc|d,Cc|d
, which are non-analytic. Fortunately, there exists an eﬃ-
cient Monte Carlo algorithm [8] for exactly this purpose. This does, however, intro-
duce a practical limit to the number of censored observations we can simultaneously
consider.

Gaussian processes for time series prediction
349
16.3.4
Eﬃcient implementation
In order to evaluate Eqs. (16.4), (16.5), (16.8) and (16.9), we must determine the product
of various matrices with the inverse of the relevant covariance matrix V. The most stable
means of doing so involves the use of the Cholesky decomposition, R, of V. Performing this
Cholesky decomposition represents the most computationally expensive operation we must
perform; its cost scaling as O(N3) in the number of data points N. However, as discussed
earlier, we do not intend to use our GP with a ﬁxed set of data, but rather, within an online
algorithm that receives new observations over time. As such, we must be able to iteratively
update our predictions in as little time as possible. Fortunately, we can do so by exploiting
the special structure of our problem. When we receive new data, our V matrix is changed
only in the addition of a few new rows and columns. Hence most of the work that went
into computing its Cholesky decomposition at the last iteration can be recycled to produce
the new Cholesky decomposition (see Appendix 16.A.1 for details of the relevant rank-1
updates). Another problematic calculation required by Eqs. (16.4) and (16.8) is the com-
putation of the data-dependent term R−1(y −µ(x)), in which y −µ(x) is also only changing
due to the addition of new rows. As such, eﬃcient updating rules are also available for this
term (see Appendix 16.A.2). As such, we are able to reduce the overall cost of an update
from O(N3) to O(N2).
However, we can further increase the eﬃciency of our updates by making a judicious
assumption. In particular, experience shows that our GP requires only a very small number
of recent observations in order to produce good estimates. Indeed, most (non-periodic)
covariance functions have very light tails such that only points within a few multiples of
the time scale are at all relevant to the point of interest. Hence we seek sensible ways of
discarding information once it has been rendered ‘stale’, to reduce both memory usage and
computational requirements.
One pre-eminently reasonable measure of the value of data is the uncertainty we still
possess after learning it. In particular, we are interested in how uncertain we are about
x⋆; as given by the covariance of our Gaussian mixture (16.6). Our approach is thus to
drop our oldest data points (those which our covariance deems least relevant to the current
predictant) until this uncertainty exceeds some predetermined threshold.
Just as we were able to eﬃciently update our Cholesky factor upon the receipt of new
data, so we can downdate to remove data (see Appendix 16.A.3 for the details of this oper-
ation). This allows us to rapidly remove the least informative data, compute our uncertainty
about y⋆, and then repeat as required; the GP will retain only as much data as necessary to
achieve a pre-speciﬁed degree of accuracy. This allows a principled way of ‘windowing’
our data series.
Finally, we turn to the implementation of our marginalisation procedure. Essentially,
our approach is to maintain an ensemble of GPs, one for each hyperparameter sample,
running in parallel, each of which we update and downdate according to the proposals
above. Their predictions are then weighted and combined according to Eq. (16.6). Note that
the only computations whose computational cost grows at greater than a quadratic rate in
the number of samples, η, are the Cholesky decomposition and multiplication of covariance
matrices in Eq. (16.6), and these scale rather poorly as O(η3). To address this problem, we
take our Gaussian priors for each diﬀerent hyperparameter φ(e) ∈φ as independent. We
further take a covariance structure given by the product of terms over each hyperparameter,
the common product correlation rule (e.g. [23])
K(φ, φ′) =
Y
e
Ke

φ(e), φ′
(e)

.

350
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
If we additionally consider a simple grid of samples, such that φs is the tensor prod-
uct of a set of samples φ(e),s over each hyperparameter, then the problematic term in
Eq. (16.6) reduces to the Kronecker product of the equivalent term over each individual
hyperparameter,
K(φs, φs)−1 Ns K(φs, φs)−1 = K(φ(1),s, φ(1),s)−1Ns(φ(1),s, φ(1),s)K(φ(1),s, φ(1),s)−1
⊗K(φ(2),s, φ(2),s)−1Ns(φ(2),s, φ(2),s)K(φ(2),s, φ(2),s)−1
⊗. . .
(16.10)
This means that we only have to perform the expensive Cholesky factorisation and multi-
plication with matrices whose size equals the number of samples for each hyperparameter,
rather than on a matrix of size equal to the total number of hyperparameter samples. This
hence represents an eﬀective way to avoid the ‘curse of dimensionality’.
Applied together, these features provide us with an eﬃcient online algorithm that can
be applied in real-time as data is sequentially collected from the sensor network.
16.3.5
Active data selection
Finally, in addition to the regression and prediction problem described in Section 16.2, we
are able to use the same algorithm to perform active data selection. This is a decision prob-
lem concerning which observations should be taken. In this, we once again take a utility
that is a function of the uncertainty in our predictions given the current set of observations.
We specify a utility of negative inﬁnity if our uncertainty about any variable is greater than
a pre-speciﬁed threshold, and a ﬁxed negative utility is assigned as the cost of an observa-
tion (in general, this cost could be diﬀerent for diﬀerent sensors). Note that the uncertainty
increases monotonically in the absence of new data, and shrinks in the presence of an obser-
vation. Hence our algorithm is simply induced to make a reading whenever the uncertainty
grows beyond a pre-speciﬁed threshold.
Our algorithm can also decide which observation to make at this time, by determining
which sensor will allow it the longest period of grace until it would be forced to observe
again. This clearly minimises the number of costly observations. Note that the uncertainty
of a GP, as given by Eq. (16.5), is actually dependent only on the location of a observation,
not its actual value. Hence the uncertainty we imagine remaining after taking an observation
from a sensor can be quickly determined without having to speculate about what data we
might possibly collect. However, this is true only so long as we do not consider the impact
of new observations on our hyperparameter sample weights (16.7). Our approach is to
take the model, in particular the weights over samples, as ﬁxed2, and investigate only how
diﬀerent schedules of observations aﬀect our predictions within it. With this proviso, we
are guaranteed to maintain our uncertainty below a speciﬁed threshold, while taking as few
observations as possible.
16.4
Trial implementation
In order to empirically evaluate the information processing algorithm described in the
previous section, we have used two sensor networks.
2Our model, of course, is updated after our scheduled observations are actually made; we consider it ﬁxed
only within the context of deciding upon this scheduling.

Gaussian processes for time series prediction
351
Bramblemet.
The ﬁrst is a network of weather sensors located on the south coast
of England.3 This network consists of four sensors (named Bramblemet, Sotonmet,
Cambermet and Chimet), each of which measures a range of environmental variables
(including wind speed and direction, air temperature, sea temperature and tide height)
and makes up-to-date sensor measurements available through separate web pages (see
www.bramblemet.co.uk). The use of such weather sensors is attractive since they exhibit
challenging correlations and delays whose physical processes are well understood, and they
are subject to network outages that generate instances of missing sensor readings on which
we can evaluate our information processing algorithms.
Wannengrat.
We additionally tested our methods on a network of weather sensors
located at Wannengrat near Davos, Switzerland. Data from and more details about the
network are available at http://sensorscope.epﬂ.ch.
16.5
Empirical evaluation
In this section we empirically evaluate our information processing algorithm on real
weather data collected from the sensor networks described above. We compare our multi-
output GP formalism against conventional independent GPs in which each environmental
variable is modelled separately (i.e. correlations between these parameters are ignored).
We also perform a comparison against a Kalman ﬁlter [11, 14]. In particular, we compare
against a dynamic auto-regressive model, in the form of a state space model, giving a simple
implementation of a Kalman ﬁlter to perform sequential predictions. We also test against
the na¨ıve algorithm that simply predicts that the variable at the next time step will be equal
to that most recently observed at that sensor.
In our comparison, we present results for three diﬀerent sensor types: tide height, air
temperature and air pressure. Tide height was chosen since it demonstrates the ability of the
GP to learn and predict periodic behaviour, and more importantly, because this particular
dataset contains an interesting period in which extreme weather conditions (a Northerly
gale) cause both an unexpectedly low tide and a failure of the wireless connection between
several of the sensors and the shore that prevents our algorithm acquiring sensor readings.
Air temperature was chosen it exhibits a very diﬀerent noise and correlation structure to the
tide height measurements, and thus demonstrate that the generic approach described here is
still able to perform reliable regression and prediction. Finally, air pressure was chosen as a
demonstration of our eﬀectiveness in processing censored observations, as all air pressure
readings are subject to (the reasonably severe) rounding to the nearest Pascal.
We plot the sensor readings acquired by our algorithm (shown as markers) and the
true ﬁne-grained sensor readings (shown as bold) that were downloaded directly from the
sensor (rather than through the website) after the event. Note that the acquired readings are
a rounded version of the true data. The true data also contains segments that are missing
from the acquired readings. We additionally plot the mean and standard deviation of our
algorithm’s predictions (shown as a solid line with plus or minus a single standard deviation
shown as shading). Note that both quantities are derived by combining the GP predictions
from each hyperparameter sample set, as described in Section 16.3.2.
3The network is maintained by the Bramblemet/Chimet Support Group and funded by organisations including
the Royal National Lifeboat Institution, Solent Cruising and Racing Association and Associated British Ports.

352
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide height (m)
Bramblemet − independent GP
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide Height (m)
Bramblemet − multi-output GP
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide height (m)
Chimet − independent GP
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide Height (m)
Chimet − multi-output GP
(a)
(b)
Figure 16.2 Prediction and regression of tide height data for (a) independent and (b) multi-output Gaussian
processes.
16.5.1
Regression and prediction
Figures 16.2 and 16.3 illustrate the eﬃcacy of our GP formalism in this scenario. Note that
we present just two sensors for reasons of space, but we use readings from all four sensors
in order to perform inference. At time t, Fig. 16.2 depicts the posterior distribution of the
GP, conditioned on all observations prior to and inclusive of t. Figure 16.3 demonstrates
our algorithm’s one-step ahead predictive performance, depicting the posterior distribution
at time t conditioned on all observations prior to and inclusive of t −5 mins.
We ﬁrst consider Fig. 16.2 showing the tide predictions, and speciﬁcally, we note the
performance of our multi-output GP formalism when the Bramblemet sensor drops out at
t = 1.45 days. In this case, the independent GP quite reasonably predicts that the tide will
repeat the same periodic signal it has observed in the past. However, the GP can achieve
better results if it is allowed to beneﬁt from the knowledge of the other sensors’ readings
during this interval of missing data. Thus, in the case of the multi-output GP, by t = 1.45
days, the GP has successfully determined that the sensors are all very strongly correlated.
Hence, when it sees an unexpected low tide in the Chimet sensor data (caused by the strong
Northerly wind), these correlations lead it to infer a similarly low tide in the Bramblemet
reading. Hence, the multi-output GP produces signiﬁcantly more accurate predictions dur-
ing the missing data interval, with associated smaller error bars. Exactly the same eﬀect
is seen in the later predictions of the Chimet tide height, where the multi-output GP pre-
dictions use observations from the other sensors to better predict the high tide height at
t = 2.45 days.
Figure 16.3 shows the air temperature sensor readings where a similar eﬀect is
observed. Again, the multi-output GP is able to accurately predict the missing air
temperature readings from the Chimet sensor having learnt the correlation with other sen-
sors, despite the fact that the dataset is much noisier and the correlations between sensors
are much weaker. In this, it demonstrates a signiﬁcant improvement in performance over

Gaussian processes for time series prediction
353
(a)
(b)
Figure 16.3 One-step lookahead prediction of air temperature data for (a) a multi-output Gaussian process and
(b) a Kalman ﬁlter.
Kalman ﬁlter predictions on the same data. The root mean square errors (RMSEs) are
0.7395 ◦C for our multi-output GP, 0.9159 ◦C for the Kalman ﬁlter and 3.8200 ◦C for the
na¨ıve algorithm. Note that the Kalman ﬁlter also gives unreasonably large error bars in its
predictions.
16.5.2
Censored observations
Figure 16.4 demonstrates regression and prediction over the rounded air pressure observa-
tions from the Bramblemet censor alone. Here we can demonstrate a dramatic improvement
over Kalman ﬁlter prediction. The RMSEs are 0.3851 Pa for the GP, 3.2900 Pa for the
Kalman ﬁlter and 3.6068 Pa for the na¨ıve algorithm. Both the GP and Kalman ﬁlter have
an order of 16; that is, they store only up to the 16 most recent observations.
16.5.3
Active data selection
We now demonstrate our active data selection algorithm. Using the ﬁne-grained data
(downloaded directly from the sensors), we can simulate how our GP would have cho-
sen its observations had it been in control. Results from the active selection of observations
from all the four tide sensors are displayed in Fig. 16.5, and for three wind speed sensors
in Fig. 16.6. Again, these plots depict dynamic choices; at time t, the GP must decide when
next to observe, and from which sensor, given knowledge only of the observations recorded
prior to t, in an attempt to maintain the uncertainty in tide height below 10 cm.
Consider ﬁrst the case shown in Fig. 16.5(a), in which separate independent GPs are
used to represent each sensor. Note that a large number of observations are taken initially
as the dynamics of the sensor readings are learnt, followed by a low but constant rate of
observation.

354
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
0.5
0.8
1.1
1.4
990
995
1000
1005
Air pressure (Pa)
Time (days)
GP assuming censored observations
0.5
0.8
1.1
1.4
990
995
1000
1005
Air pressure (Pa)
Time (days)
Kalman filter
(a)
(b)
Figure 16.4 Prediction and regression of air pressure data for (a) a Gaussian process employing censored
observations and (b) a Kalman ﬁlter.
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide height (m)
Bramblemet − independent GP
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide Height (m)
Bramblemet − multi-output GP
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide height (m)
Sotonmet − independent GP
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide Height (m)
Sotonmet − multi-output GP
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide height (m)
Chimet − independent GP
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide Height (m)
Chimet − multi-output GP
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide height (m)
Cambermet − independent GP
0
0.5
1
1.5
2
2.5
3
1
2
3
4
5
Time (days)
Tide Height (m)
Cambermet − multi-output GP
(a)
(b)
Figure 16.5 Comparison of active sampling of tide data using (a) independent and (b) multi-output Gaussian
processes.

Gaussian processes for time series prediction
355
0
0.5
1
1.5
0
5
10
15
20
Time (days)
Wind speed (knots)
Bramblemet − independent GP
0
0.5
1
1.5
0
5
10
15
20
Time (days)
Wind speed (knots)
Bramblemet − multi-output GP
0
0.5
1
1.5
0
5
10
15
Time (days)
Wind speed (knots)
Sotonmet − independent GP
0
0.5
1
1.5
0
5
10
15
Time (days)
Wind Speed (knots)
Sotonmet − multi-output GP
0
0.5
1
1.5
0
5
10
15
Time (days)
Wind speed (knots)
 Chimet − independent GP
0
0.5
1
1.5
0
5
10
15
Time (days)
Wind Speed (knots)
Chimet − multi-output GP
(a)
(b)
Figure 16.6 Comparison of active sampling of wind speed using (a) independent and (b) multi-output Gaussian
processes.
In contrast, for the multi-output case shown in Fig. 16.5(b), the GP is allowed to explic-
itly represent correlations and delays between the sensors. This dataset is notable for the
slight delay of the tide heights at the Chimet and Cambermet sensors relative to the Soton-
met and Bramblemet sensors, due to the nature of tidal ﬂows in the area. Note that after
an initial learning phase as the dynamics, correlations and delays are inferred, the GP
chooses to sample predominantly from the undelayed Sotonmet and Bramblemet sensors.4
Despite no observations of the Chimet sensor being made within the time span plotted, the
resulting predictions remain remarkably accurate. Consequently only 119 observations are
required to keep the uncertainty below the speciﬁed tolerance, whereas 358 observations
were required in the independent case. This represents another clear demonstration of how
our prediction is able to beneﬁt from the readings of multiple sensors.
Figure 16.6 shows similar results for the wind speed measurements from three of the
four sensors (the Cambermet sensor being faulty during this period) where the goal was
to maintain the uncertainty in wind speed below 1.5 knots. In this case, for purposes of
clarity, the ﬁne-grained data is not shown on the plot. Note that the measurement noise
is much greater in this case, and this is reﬂected in the uncertainty in the GP predictions.
4The dynamics of the tide height at the Sotonmet sensor are more complex than the other sensors due to
the existence of a ‘young ﬂood stand’ and a ‘double high tide’ in Southampton. For this reason, the GP selects
Sotonmet as the most informative sensor and samples it most often.

356
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
Furthermore, note that while the Sotonmet and Chimet sensors exhibit a noticeable cor-
relation, Bramblemet appears to be relatively uncorrelated with both. This observation is
reﬂected in the sampling that the GP performs. The independent GPs sample the Bram-
blemet, Sotonmet and Chimet sensors 126, 120 and 121 times respectively, while over
the same period, our multi-output GP samples the same sensors 115, 88 and 81 times.
Our multi-output GP learns online that the wind speed measurements of the Sotonmet and
Chimet sensors are correlated, and then exploits this correlation in order to reduce the num-
ber of times that these sensors are sampled (inferring the wind speed at one location from
observations of another). However, there is little or no correlation between the Bramblemet
sensor and the other sensors, and thus, our multi-output GP samples Bramblemet almost as
often as the independent GPs.
Figure 16.7 shows the results of an active sampling experiment over the Wannengrat
data. Given the larger number of sensors in this dataset, it was impractical to use the arbi-
trary parameterisation (16.1), which requires a hyperparameter for every distinct pair of
sensors. Instead, we express the covariance between sensors as a function of the spatial
distance between the known sensor locations. We used the Mat´ern covariance function
(16.2) for this purpose, with the spatial distance being used to ﬁll the role of r. This spatial
0
0.3
0.6
0.9
−5
0
5
10
15
Temperature (° C)
Time (days)
Wannengrat station 4
0
0.3
0.6
0.9
−5
0
5
10
15
Temperature (° C)
Time (days)
Wannengrat station 9
0
0.3
0.6
0.9
−5
0
5
10
15
Temperature (° C)
Time (days)
Wannengrat station 13
0
0.3
0.6
0.9
−5
0
5
10
15
Temperature (° C)
Time (days)
Wannengrat station 16
0
0.3
0.6
0.9
−5
0
5
10
15
Temperature (° C)
Time (days)
Wannengrat station 29
0
0.3
0.6
0.9
0
10
20
30
Station
Time (days)
Observation times and stations
Figure 16.7 Active sampling of ambient temperatures at 16 Wannengrat sensor stations.

Gaussian processes for time series prediction
357
distance was speciﬁed by a single isotropic scale hyperparameter, which was marginalised
along with the other hyperparameters. It can be seen that there is a dramatic increase in
sampling frequency coincident with the volatile ﬂuctuations in temperature that begin at
about t = 0.7 days.
16.6
Computation time
As described earlier, a key requirement of our algorithm is computational eﬃciency, in
order that it can be used to represent multiple correlated sensors, and hence, used for
real-time information processing. Here we consider the computation times involved in
producing the results presented in the previous section. To this end, Table 16.1 tabulates
the computation times required in order to update the algorithm as a new observation is
received. This computation time represents the cost of updating the weights of Eq. (16.6)
and the Cholesky factor of V (as described in Section 16.3.4). Once this calculation has
been performed, making predictions at any point in time is extremely fast (it is simply a
matter of adding another element in z⋆).
Note that we expect the cost of computation to grow as O(N2) in the number of
stored data points. Our proposed algorithm will automatically determine the quantity
of data to store in order to achieve the desired level of accuracy. In the problems we
have studied, a few hundred points were typically suﬃcient (the largest number we
required was 750, for the multi-output wind speed data), although of course this will
depend critically on the nature of the variables under consideration. For example, our
use of a model that explicitly considers the correlations amongst sensors allows us to
eliminate much redundant information, reducing the quantity of data we are required to
store.
We also have the cost of computing Eq. (16.10), which will grow in the cube of the
number of samples in each hyperparameter. However, we consider only a ﬁxed set of
samples in each hyperparameter, and thus, Eq. (16.10) needs only be computed once,
oﬄine. In this case, our online costs are limited by the multiplication of that term by the
likelihoods rs to give the weights of Eq. (16.6), and this only grows as O(η2). Further-
more, note that this cost is independent of how the η samples are distributed amongst the
hyperparameters.
The results in Table 16.1 indicate that real-time information processing is clearly fea-
sible for the problem sizes that we have considered. In general, limiting the number of
hyperparameter samples is of critical importance to achieving practical computation. As
such, we should exploit any and all prior information that we possess about the system to
limit the volume of hyperparameter space that our GP is required to explore online. For
example, an informative prior expressing that the tidal period is likely to be around half
a day will greatly reduce the number of samples required for this hyperparameter. Simi-
larly, an oﬄine analysis of any available training data will return sharply peaked posteriors
over our hyperparameters that will further restrict the required volume to be searched over
online. For example, we represent the tidal period hyperparameter with only a single sam-
ple online, so certain does training data make us of its value. Finally, a simpler and less
ﬂexible covariance model, with fewer hyperparameters, could be chosen if computational
limitations become particularly severe. Note that the use of the completely general spher-
ical parameterisation requires a correlation hyperparameter for each pair of variables, an
approach which is clearly only feasible for moderate numbers of variables. A simple alter-
native, of course, would be to assume a covariance over variable label which is a function

358
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
Data points (N)
10
100
500
1
< 0.01
< 0.01
0.04
Hyperparameter
10
0.02
0.02
0.20
samples (η)
100
0.14
0.22
2.28
1000
1.42
2.22
29.73
Table 16.1 Required computation time (seconds) per update, over N the number of stored data points and η the
number of hyperparameter samples. Experiments performed using MATLAB on a 3.00GHz processor with 2GB
of RAM.
of the spatial separation between the sensors reading them – sensors that are physically
close are likely to be strongly correlated – in which case we would require only enough
hyperparameters to deﬁne this measure of separation. While a more complicated model
will return better predictions, a simple one or two hyperparameter covariance may supply
accuracy suﬃcient for our needs.
16.7
Related work
Gaussian process regression has a long history of use within geophysics and geospa-
tial statistics (where the process is known as kriging [4]), but has only recently been
applied within sensor networks. Examples here include the use of GPs to represent
spatial correlations between sensors in order that they may be positioned to maximise
mutual information [13], and the use of multivariate Gaussians to represent correlations
between diﬀerent sensors and sensor types for energy eﬃcient querying of a sensor
network [5].
Our work diﬀers in that we use GPs to represent temporal correlations, and represent
correlations and delays between sensors with additional hyperparameters. It is thus closely
related to other work using GPs to perform regression over multiple responses [2, 26].
However, our focus is to derive a computationally eﬃcient algorithm, and thus, we use a
number of novel computational techniques to allow the re-use of previous calculations as
new sensor observations are made. We additionally use a novel Bayesian Monte Carlo tech-
nique to marginalise the hyperparameters that describe the correlations and delays between
sensors. Finally, we use the variance of the GP’s predictions in order to perform active data
selection.
Our approach has several advantages relative to sequential state space models [9, 11].
Firstly, these models require the discretisation of the time input, representing a discarding
of potentially valuable information. Secondly, their sequential nature means they must nec-
essarily perform diﬃcult iterations in order to manage missing or late data, or to produce
long-range forecasts. In our GP approach, what observations we have are readily managed,
regardless of when they were made. Equally, the computation cost of all our predictions is
identical, irrespective of the time or place we wish to make them about. Finally, a sequen-
tial framework requires an explicit speciﬁcation of a transition model. In our approach, we
are able to learn a model from data even if our prior knowledge is negligible. The beneﬁts
of our approach are empirically supported by Fig. 16.3.
Previous work has also investigated the use of censored sensor readings within a
GP framework [6], or the similar problems involved in classiﬁcation and ordinal regres-
sion [3]. Our approach diﬀers in a number of respects. Firstly, we consider the potential

Gaussian processes for time series prediction
359
combination of both censored and uncensored observations. Our work also proposes a
principled Bayesian Monte Carlo method for adapting our models to the data, considering
the contributions from a number of samples in hyperparameter space, rather than simply
taking a single sample. Monte Carlo techniques are also used to evaluate our other integrals,
in comparison to the Laplace or expectation propagation approximations present in other
work. This allows a more accurate assessment of the likelihoods of our hyperparameter
samples and more accurate predictions.
Active data selection has been the topic of much previous research [15, 24]. Our prob-
lem, of active data selection for tracking, is simpliﬁed relative to much of that work owing
to the fact that we have a well-deﬁned point of interest. This will either be the current value
of our environmental variables, or some future value according to the degree of lookahead.
Note also that our uncertainty in this value reﬂects our underlying uncertainty about the
correct model, due to our principled marginalisation. More observations will be scheduled
if there is a signiﬁcant degree of uncertainty about the model. Previous work has also con-
sidered the intelligent discarding of data [12]. Again, our task is made relatively easier by
our well-deﬁned utility: the uncertainty in our point of interest.
16.8
Conclusions
In this chapter we addressed the need for algorithms capable of performing real-time
information processing of sensor network data, and we presented a novel computation-
ally eﬃcient formalism of a multi-output Gaussian process. Using weather data collected
from two sensor networks, we demonstrated that this formalism could eﬀectively predict
missing sensor readings caused by network outages, and could perform active sampling to
maintain estimation uncertainty below a pre-speciﬁed threshold.
There are many possibilities for further work to build upon that presented in this chap-
ter. Firstly, as a potential replacement for the ﬁxed hyperparameter samples used in this
work, we would like to investigate the use of a moving set of hyperparameter samples.
In such a scheme, both the weights and positions of samples would be adjusted accord-
ing to data received, and as the posterior distributions of these hyperparameters become
more sharply peaked, we would reduce the number of samples to further increase the
computational eﬃciency of our algorithm.
Secondly, we intend to investigate the use of correlations between diﬀerent sensor
types (rather than between diﬀerent sensors of the same type as presented here) to per-
form regression and prediction within our weather sensor network. In addition, we would
like to introduce more ﬂexible covariance functions, able to cope with abrupt changes in
the characteristics of data known as changepoints. Such covariance functions would, for
example, allow the automatic detection of changepoints due to sensor failures.
Acknowledgments
This
research
was
undertaken
as
part
of
the
ALADDIN
(Autonomous Learning Agents for Decentralised Data and Information Networks) project
and is jointly funded by a BAE Systems and EPSRC strategic partnership (EP/C548051/1).
We would like to thank B. Blaydes of the Bramblemet/Chimet Support Group, and W.
Heaps of Associated British Ports (ABP) for allowing us access to the weather sensor
network, hosting our RDF data on the sensor websites, and for providing raw sensor
data as required. We would also like to thank an anonymous reviewer for many helpful
comments.

360
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
16.A
Appendix
16.A.1
Cholesky factor update
We have a positive deﬁnite matrix, represented in block form as
"V1,1
V1,3
VT
1,3
V3,3
#
and its
Cholesky factor,
"R1,1
R1,3
0
R3,3
#
. Given a new positive deﬁnite matrix, which diﬀers from
the old only in the insertion of some new rows and columns,

V1,1
V1,2
V1,3
VT
1,2
V2,2
V2,3
VT
1,3
VT
2,3
V3,3
, we wish
to eﬃciently determine its Cholesky factor,

S 1,1
S 1,2
S 1,3
0
S 2,2
S 2,3
0
0
S 3,3
. For A triangular, we deﬁne
x = A \ b as the solution to the equations A x = b as found by the use of backwards or
forwards substitution. The following rules are readily obtained
S 1,1 = R1,1, S 1,2 = RT
1,1 \ V1,2, S 1,3 = R1,3, S 2,2 = chol(V2,2 −S T
1,2S 1,2),
S 2,3 = S T
2,2 \ (V2,3 −S T
1,2S 1,3), S 3,3 = chol(RT
3,3R3,3 −S T
2,3S 2,3) .
By setting the appropriate row and column dimensions (to zero if necessary), this allows us
to eﬃciently determine the Cholesky factor given the insertion of rows and columns in any
position.
16.A.2
Data term update
We have all terms deﬁned in Appendix 16.A.1, in addition to

Y1
Y2
Y3
and the product
"C1
C3
#
≜
"R1,1
R1,3
0
R3,3
#−T "Y1
Y3
#
. To eﬃciently determine

D1
D2
D3
≜

S 1,1
S 1,2
S 1,3
0
S 2,2
S 2,3
0
0
S 3,3

−T 
Y1
Y2
Y3
, we have
D1 = C1, D2 = S −T
2,2
 Y2 −S T
1,2 C1
, D3 = S −T
3,3
 RT
3,3 C3 −S T
2,3 D2
 .
16.A.3
Cholesky factor downdate
We have a positive deﬁnite matrix, represented in block form as

V1,1
V1,2
V1,3
VT
1,2
V2,2
V2,3
VT
1,3
VT
2,3
V3,3
and
its Cholesky factor,

S 1,1
S 1,2
S 1,3
0
S 2,2
S 2,3
0
0
S 3,3
. Given a new positive deﬁnite matrix, which diﬀers
from the old only in the deletion of some new rows and columns,
"V1,1
V1,3
VT
1,3
V3,3
#
, we wish
to eﬃciently determine its Cholesky factor
"R1,1
R1,3
0
R3,3
#
. The following rules are readily
obtained
R1,1 = S 1,1, R1,3 = S 1,3, R3,3 = chol(S T
2,3S 2,3 + S T
3,3S 3,3).

Gaussian processes for time series prediction
361
Note that the special structure of R3,3 can be exploited for the eﬃcient resolution of the
required Cholesky operation, as, for example, in the MATLAB function cholupdate [27].
By setting the appropriate row and column dimensions (to zero if necessary), this allows us
to eﬃciently determine the Cholesky factor given the deletion of rows and columns in any
position.
Bibliography
[1] P. Abrahamsen. A review of Gaussian random
ﬁelds and correlation functions. Technical
Report 917, Norwegian Computing Center, Box
114, Blindern, N-0314 Oslo, Norway, 1997. 2nd
edition.
[2] P. Boyle and M. Frean. Dependent Gaussian
processes. In Advances in Neural Information
Processing Systems 17, pages 217–224. The MIT
Press, 2005.
[3] W. Chu and Z. Ghahramani. Gaussian processes
for ordinal regression. Journal of Machine
Learning Research, 6(1):1019, 2006.
[4] N. A. C. Cressie. Statistics for Spatial Data.
John Wiley & Sons, 1991.
[5] A. Deshpande, C. Guestrin, S. Madden,
J. Hellerstein and W. Hong. Model-driven data
acquisition in sensor networks. In Proceedings of
the Thirtieth International Conference on Very
Large Data Bases (VLDB 2004), pages 588–599,
2004.
[6] E. Ertin. Gaussian process models for censored
sensor readings. In Statistical Signal Processing,
2007. SSP’07. IEEE/SP 14th Workshop on, pages
665–669, 2007.
[7] M. Fuentes, A. Chaudhuri and D. H. Holland.
Bayesian entropy for spatial sampling design of
environmental data. Environmental and
Ecological Statistics, 14:323–340, 2007.
[8] A. Genz. Numerical computation of multivariate
normal probabilities. Journal of Computational
and Graphical Statistics, 1(2):141–149, 1992.
[9] A. Girard, C. Rasmussen, J. Candela and
R. Murray-Smith. Gaussian process priors with
uncertain inputs – application to multiple-step
ahead time series forecasting. In Advances in
Neural Information Processing Systems 16. MIT
Press, 2003.
[10] J. K. Hart and K. Martinez. Environmental
Sensor Networks: A revolution in the earth
system science? Earth-Science Reviews,
78:177–191, 2006.
[11] A. H. Jazwinski. Stochastic Processes and
Filtering Theory. Academic Press, 1970.
[12] A. Kapoor and E. Horvitz. On discarding,
caching, and recalling samples in active learning.
In Uncertainty in Artiﬁcial Intelligence, 2007.
[13] A. Krause, C. Guestrin, A. Gupta and
J. Kleinberg. Near-optimal sensor placements:
maximizing information while minimizing
communication cost. In Proceedings of the Fifth
International Conference on Information
Processing in Sensor Networks (IPSN ’06),
pages 2–10, Nashville, Tennessee, USA, 2006.
[14] S. M. Lee and S. J. Roberts. Multivariate time
series forecasting in incomplete environments.
Technical Report PARG-08-03. Available at
www.robots.ox.ac.uk/∼parg/publications.html,
University of Oxford, December 2008.
[15] D. J. C. MacKay. Information-based objective
functions for active data selection. Neural
Computation, 4(4):590–604, 1992.
[16] D. J. C. MacKay. Information Theory, Inference
and Learning Algorithms. Cambridge University
Press, 2002.
[17] A. O’Hagan. Monte Carlo is fundamentally
unsound. The Statistician, 36:247–249, 1987.
[18] A. O’Hagan. Bayes-Hermite quadrature. Journal
of Statistical Planning and Inference,
29:245–260, 1991.
[19] M. Osborne and S. J. Roberts. Gaussian
processes for prediction. Technical Report
PARG-07-01. Available at
www.robots.ox.ac.uk/∼parg/publications.html,
University of Oxford, September 2007.
[20] J. Pinheiro and D. Bates. Unconstrained
parameterizations for variance-covariance
matrices. Statistics and Computing, 6:289–296,
1996.
[21] C. E. Rasmussen and Z. Ghahramani. Bayesian
Monte Carlo. In Advances in Neural Information
Processing Systems 15, pages 489–496. The MIT
Press, 2003.
[22] C. E. Rasmussen and C. K. I. Williams.
Gaussian Processes for Machine Learning. MIT
Press, 2006.
[23] M. J. Sasena. Flexibility and eﬃciency
enhancements for constrained global design
optimization with Kriging approximations. PhD
thesis, University of Michigan, 2002.
[24] S. Seo, M. Wallat, T. Graepel and K. Obermayer.
Gaussian process regression: active data
selection and test point rejection. In Neural
Networks, 2000. IJCNN 2000, Proceedings of
the IEEE-INNS-ENNS International Joint
Conference on, volume 3, 2000.
[25] M. L. Stein. Space-time covariance functions.
Journal of the American Statistical Association,
100(469):310–322, 2005.
[26] Y. W. Teh, M. Seeger and M. I. Jordan.
Semiparametric latent factor models. In
Proceedings of the Conference on Artiﬁcial
Intelligence and Statistics, pages 333–340, 2005.
[27] The MathWorks. MATLAB R2007a, 2007.

362
Michael A. Osborne, Alex Rogers, Stephen J. Roberts et al.
Contributors
Michael A. Osborne, Department of Engineering Science, University of Oxford, Oxford, OX1 3PJ
Alex Rogers, School of Electronics and Computer Science, University of Southampton,
Southampton, SO17 1BJ
Stephen J. Roberts, Department of Engineering Science, University of Oxford, Oxford, OX1 3PJ
Sarvapali D. Ramchurn, School of Electronics and Computer Science, University of Southampton,
Southampton, SO17 1BJ
Nick R. Jennings, School of Electronics and Computer Science, University of Southampton,
Southampton, SO17 1BJ

17
Optimal control theory and the linear Bellman equation
Hilbert J. Kappen
17.1
Introduction
Optimising a sequence of actions to attain some future goal is the general topic of control
theory [26, 9]. It views an agent as an automaton that seeks to maximise expected reward
(or minimise cost) over some future time period. Two typical examples that illustrate this
are motor control and foraging for food.
As an example of a motor control task, consider a human throwing a spear to kill
an animal. Throwing a spear requires the execution of a motor program that is such that
at the moment that the hand releases the spear it has the correct speed and direction to
hit the desired target. A motor program is a sequence of actions, and this sequence can
be assigned a cost that consists generally of two terms: a path cost that speciﬁes the
energy consumption to contract the muscles to execute the motor program, and an end
cost that speciﬁes whether the spear will kill the animal, just hurt it, or miss it altogether.
The optimal control solution is a sequence of motor commands that results in killing the
animal by throwing the spear with minimal physical eﬀort. If x denotes the state space
(the positions and velocities of the muscles), the optimal control solution is a function
u(x, t) that depends both on the actual state of the system at each time t and also explicitly
on time.
When an animal forages for food, it explores the environment with the objective to ﬁnd
as much food as possible in a short time window. At each time t, the animal considers the
food it expects to encounter in the period [t, t + T]. Unlike the motor control example, the
time horizon recedes into the future from the current time and the cost consists now only of
a path contribution and no end cost. Therefore, at each time the animal faces the same task,
but possibly from a diﬀerent location in the environment. The optimal control solution u(x)
is now time-independent and speciﬁes for each location in the environment x the direction
u in which the animal should move.
The general stochastic control problem is intractable to solve and requires an exponen-
tial amount of memory and computation time. The reason is that the state space needs
to be discretised and thus becomes exponentially large in the number of dimensions.
Computing the expectation values means that all states need to be visited and requires
exponentially large sums. The same intractabilities are encountered in reinforcement
learning.
In this chapter, we aim to give a pedagogical introduction to control theory. For simplic-
ity, we will ﬁrst consider in Section 17.2 the case of discrete time and discuss the dynamic
programming solution. This gives us the basic intuition about the Bellman equations in

364
Hilbert J. Kappen
continuous time that are considered later on. In Section 17.3 we consider continuous time
control problems. In this case, the optimal control problem can be solved in two ways:
using the Hamilton–Jacobi–Bellman (HJB) equation which is a partial diﬀerential equation
[2] and is the continuous time analogue of the dynamic programming method, or using the
Pontryagin minimum principle (PMP) [23] which is a variational argument and results in
a pair of ordinary diﬀerential equations. We illustrate the methods with the example of a
mass on a spring.
In Section 17.4 we generalise the control formulation to stochastic dynamics. In the
presence of noise, the PMP formalism has no obvious generalisation (see however [39]).
In contrast, the inclusion of noise in the HJB framework is mathematically quite straight-
forward. However, the numerical solution of either the deterministic or stochastic HJB
equation is in general diﬃcult due to the curse of dimensionality.
There are some stochastic control problems that can be solved eﬃciently. When the
system dynamics is linear and the cost is quadratic (LQ control), the solution is given in
terms of a number of coupled ordinary diﬀerential (Riccati) equations that can be solved
eﬃciently [26]. Linear quadratic control is useful to maintain a system such as a chemical
plant, operated around a desired point in state space, and is therefore widely applied in
engineering. However, it is a linear theory and too restricted to model the complexities of
intelligent behaviour encountered in agents or robots.
The simplest control formulation assumes that all model components (the dynamics,
the environment, the costs) are known and that the state is fully observed. Often, this is
not the case. Formulated in a Bayesian way, one may only know a probability distribution
of the current state, or over the parameters of the dynamics or the costs. This leads us to
the problem of partial observability or the problem of joint inference and control. We dis-
cuss two diﬀerent approaches to learning: adaptive control and dual control. Whereas in
the adaptive control approach the learning dynamics is exterior to the control problem, in
the dual control approach it is recognised that learning and control are interrelated and the
optimal solution for the combined learning and control problem is computed. We illustrate
the complexity of joint inference and control with a simple example. We discuss the con-
cept of certainty equivalence, which states that for particular linear quadratic problems the
inference and control problems disentangle and can be solved separately. We will discuss
these issues in Section 17.5.
Recently, we have discovered a class of continuous non-linear stochastic control prob-
lems that can be solved more eﬃciently than the general case [16, 17]. These are control
problems with a ﬁnite time horizon, where the control acts additively on the dynamics and
is in some sense proportional to the noise. The cost of the control is quadratic in the con-
trol variables. Otherwise, the path cost, end cost and the intrinsic dynamics of the system
are arbitrary. These control problems can have both time-dependent and time-independent
solutions of the type that we encountered in the examples above. For these problems, the
Bellman equation becomes a linear equation in the exponentiated cost-to-go (value func-
tion). The solution is formally written as a path integral. We discuss the path integral control
method in Section 17.6. The path integral can be interpreted as a free-energy, or as the nor-
malisation of a probabilistic time series model (Kalman ﬁlter, hidden Markov model). One
can therefore consider various well-known methods from the machine learning community
to approximate this path integral, such as the Laplace approximation and Monte Carlo sam-
pling [17], variational approximations or belief propagation [34]. In Section 17.7.2 we show
an example of an n joint arm where we compute the optimal control using the variational
approximation for large n.

Optimal control theory and the linear Bellman equation
365
Non-linear stochastic control problems display features not shared by deterministic
control problems nor by linear stochastic control. In deterministic control, only the glob-
ally optimal solution is relevant. In stochastic control, the optimal solution is typically a
weighted mixture of suboptimal solutions. The weighting depends in a non-trivial way on
the features of the problem, such as the noise and the horizon time and on the cost of each
solution. This multimodality leads to surprising behaviour in stochastic optimal control.
For instance, the optimal control can be qualitatively very diﬀerent for high and low noise
levels. In [16] it was shown that, in a stochastic environment, optimally the choice to move
to one of two targets should be delayed in time. The decision is formally accomplished by
a dynamical symmetry breaking of the cost-to-go function.
17.2
Discrete time control
We start by discussing the most simple control case, which is the ﬁnite horizon discrete
time deterministic control problem. In this case the optimal control explicitly depends on
time. See also [36] for further discussion. Consider the control of a discrete time dynamical
system
xt+1 = xt + f(t, xt, ut),
t = 0, 1, . . . , T −1,
(17.1)
where xt is an n-dimensional vector describing the state of the system and ut is an
m-dimensional vector that speciﬁes the control or action at time t. Note that Eq. (17.1)
describes a noiseless dynamics. If we specify x at t = 0 as x0 and we specify a sequence
of controls u0:T−1 = u0, u1, . . . , uT−1, we can compute future states of the system x1:T
recursively from Eq. (17.1). Deﬁne a cost function that assigns a cost to each sequence
of controls
C(x0, u0:T−1) = φ(xT) +
T−1
X
t=0
R(t, xt, ut).
(17.2)
Here R(t, x, u) is the cost associated with taking action u at time t in state x, and φ(xT) is the
cost associated with ending up in state xT at time T. The problem of optimal control is to
ﬁnd the sequence u0:T−1 that minimises C(x0, u0:T−1). The problem has a standard solution,
which is known as dynamic programming. Introduce the optimal cost-to-go
J(t, xt) = min
ut:T−1
φ(xT) +
T−1
X
s=t
R(s, xs, us)
,
t ≤T −1,
which solves the optimal control problem from an intermediate time t until the ﬁxed end
time T, starting at an arbitrary location xt. The minimum of Eq. (17.2) is given by J(0, x0).
One can recursively compute J(t, x) from J(t + 1, x) for all x in the following way. Given
J(T, x) = φ(x), then
J(t, xt) = min
ut:T−1
φ(xT) +
T−1
X
s=t
R(s, xs, us)

= min
ut
R(t, xt, ut) + min
ut+1:T−1
φ(xT) +
T−1
X
s=t+1
R(s, xs, us)



366
Hilbert J. Kappen
= min
ut (R(t, xt, ut) + J(t + 1, xt+1))
= min
ut (R(t, xt, ut) + J(t + 1, xt + f(t, xt, ut))).
(17.3)
The minimisation over the whole path u0:T−1 has reduced to a sequence of minimisations
over ut due to the Markovian nature of the problem: the future depends on the past and
vice versa only through the present. Also note that in the last line the minimisation is done
for each xt separately. The algorithm to compute the optimal control u∗
0:T−1, the optimal
trajectory x∗
1:T and the optimal cost is given by
1. Initialisation: J(T, x) = φ(x).
2. Backwards: For t = T −1, . . . , 0 and for all x compute
u∗
t (x) = arg min
u {R(t, x, u) + J(t + 1, x + f(t, x, u))},
J(t, x) = R(t, x, u∗
t ) + J(t + 1, x + f(t, x, u∗
t )).
3. Forwards: For t = 0, . . . , T −1 compute
x∗
t+1 = x∗
t + f(t, x∗
t , u∗
t (x∗
t )).
The execution of the dynamic programming algorithm is linear in the horizon time T.
17.3
Continuous time control
In the absence of noise, the optimal control problem in continuous time can be solved in
two ways: using the Pontryagin minimum principle (PMP) [23], which is a pair of ordinary
diﬀerential equations or the Hamilton–Jacobi–Bellman (HJB) equation which is a partial
diﬀerential equation [2]. The latter is very similar to the dynamic programming approach
that we have treated above. The HJB approach also allows for a straightforward exten-
sion to the noisy case. We will ﬁrst treat the HJB description and subsequently the PMP
description. For further reading see [26, 14].
17.3.1
The HJB equation
Consider the dynamical system (17.1) where we take the time increments to zero, i.e. we
replace t + 1 by t + dt with dt →0:
xt+dt = xt + f(xt, ut, t)dt.
(17.4)
In the continuous limit we will write xt = x(t). The initial state is ﬁxed to x(0) = x0 and the
ﬁnal state is free. The problem is to ﬁnd a control signal u(t), 0 < t < T, which we denote
as u(0 →T), such that
C(x0, u(0 →T)) = φ(x(T)) +
Z T
0
dτR(x(τ), u(τ), τ)
(17.5)
is minimal. Here C consists of an end cost φ(x) that gives the cost of ending in a conﬁgura-
tion x, and a path cost that is an integral over time that depends on the trajectories x(0 →T)
and u(0 →T). Equation (17.3) becomes

Optimal control theory and the linear Bellman equation
367
J(t, x) = min
u (R(t, x, u)dt + J(t + dt, x + f(x, u, t)dt)) ,
≈min
u (R(t, x, u)dt + J(t, x) + ∂tJ(t, x)dt + ∂xJ(t, x) f(x, u, t)dt) ,
−∂tJ(t, x) = min
u (R(t, x, u) + f(x, u, t)∂xJ(x, t)) ,
(17.6)
where in the second line we have used the Taylor expansion of J(t + dt, x + dx) around x, t
to ﬁrst order in dt and dx and in the third line have taken the limit dt →0. We use the
shorthand notation ∂xJ = ∂J/∂x. Equation (17.6) is a partial diﬀerential equation, known
as the Hamilton–Jacobi–Bellman (HJB) equation, that describes the evolution of J as a
function of x and t and must be solved with boundary condition J(x, T) = φ(x); ∂t and ∂x
denote partial derivatives with respect to t and x, respectively. The optimal control at the
current x, t is given by
u(x, t) = arg min
u (R(x, u, t) + f(x, u, t)∂xJ(t, x)) .
(17.7)
Note that in order to compute the optimal control at the current state x(0) at t = 0 one must
compute J(x, t) for all values of x and t.
17.3.2
Example: mass on a spring
To illustrate the optimal control principle consider a mass on a spring. The spring is at rest
at z = 0 and exerts a force proportional to F = −z towards the rest position. Using Newton’s
law F = ma with a = ¨z the acceleration and m = 1 the mass of the spring, the equation of
motion is given by ¨z = −z + u, with u an unspeciﬁed control signal with −1 < u < 1. We
want to solve the control problem: Given initial position and velocity zi and ˙zi at time 0,
ﬁnd the control path u(0 →T) such that z(T) is maximal. Introducing x1 = z, x2 = ˙z, then
˙x = Ax + Bu,
A =
 
0
1
−1
0
!
,
B =
 0
1
!
,
and x = (x1, x2)T. The problem is of the above type, with φ(x) = CT x, CT = (−1, 0),
R(x, u, t) = 0 and f(x, u, t) = Ax + Bu. Equation (17.6) takes the form
−Jt = (∂xJ)T Ax −|(∂xJ)T B|.
We try J(t, x) = ψ(t)T x + α(t). The HJB equation reduces to two ordinary diﬀerential
equations ˙ψ = −ATψ, ˙α = |ψT B|. These equations must be solved for all t, with ﬁnal
boundary conditions ψ(T) = C and α(T) = 0. Note that the optimal control in Eq. (17.7)
only requires ∂xJ(x, t), which in this case is ψ(t) and thus we do not need to solve α. The
solution for ψ is ψ1(t) = −cos(t −T), ψ2(t) = sin(t −T) for 0 < t < T. The optimal control
is u(x, t) = −sign(ψ2(t)) = −sign(sin(t −T)). As an example consider x1(0) = x2(0) = 0,
T = 2π. Then, the optimal control is
u = −1,
0 < t < π,
u = 1,
π < t < 2π.
The optimal trajectories are, for 0 < t < π, x1(t) = cos(t) −1,
x2(t) = −sin(t) and, for
π < t < 2π, x1(t) = 3 cos(t) + 1,
x2(t) = −3 sin(t). The solution is drawn in Fig. 17.1(a).
We see that in order to excite the spring to its maximal height at T, the optimal control is to
ﬁrst push the spring down for 0 < t < π and then to push the spring up between π < t < 2π,
taking maximal advantage of the intrinsic dynamics of the spring. Note that, since there is
no cost associated with the control u and u is hard limited between −1 and 1, the optimal
control is always either −1 or 1. This is known as bang-bang control.

368
Hilbert J. Kappen
(a)
(b)
Figure 17.1 (a) Optimal control of mass on a
spring such that at t = 2π the amplitude is maxi-
mal; x1 is position of the spring, x2 is velocity of
the spring. (b) Stochastic optimal control in the
case of a linear system with quadratic cost; T =
10, time discretisation dt = 0.1, ν = 0.05. The
optimal control is to steer towards the origin with
−P(t)x, where P is roughly constant until T ≈8.
Afterward control weakens because the expected
diﬀusion is proportional to the time-to-go.
17.3.3
Pontryagin minimum principle
In the previous section, we solved the optimal control problem as a partial diﬀerential
equation, with a boundary condition at the end time. The numerical solution requires a
discretisation of space and time and is computationally expensive. The solution is an opti-
mal cost-to-go function J(x, t) for all x and t. From this we compute the optimal control
sequence (17.7) and the optimal trajectory.
An alternative to the HJB approach is a variational approach that directly ﬁnds the
optimal trajectory and optimal control and bypasses the expensive computation of the cost-
to-go. This approach is known as the Pontryagin minimum principle. We can write the
optimal control problem as a constrained optimisation problem with independent variables
u(0 →T) and x(0 →T). We wish to minimise
min
u(0→T),x(0→T) φ(x(T)) +
Z T
0
dtR(x(t), u(t), t)
subject to the constraint that u(0 →T) and x(0 →T) are compatible with the dynamics
˙x = f(x, u, t)
(17.8)
and the boundary condition x(0) = x0. Here ˙x denotes the time derivative dx/dt. We can
solve the constrained optimization problem by introducing the Lagrange multiplier function
λ(t) that ensures the constraint (17.8) for all t:
C = φ(x(T)) +
Z T
0
dt R(t, x(t), u(t)) −λ(t)( f(t, x(t), u(t)) −˙x(t))
= φ(x(T)) +
Z T
0
dt[−H(t, x(t), u(t), λ(t)) + λ(t)˙x(t)],
(17.9)
−H(t, x, u, λ) = R(t, x, u) −λf(t, x, u).
The solution is found by extremising C. If we vary the action C w.r.t. to the trajectory x, the
control u and the Lagrange multiplier λ, we get
δC = φx(x(T))δx(T) +
Z T
0
dt[−Hxδx(t) −Huδu(t) + (−Hλ + ˙x(t))δλ(t) + λ(t)δ˙x(t)]
= (φx(x(T)) + λ(T)) δx(T) +
Z T
0
dt
h
(−Hx −˙λ(t))δx(t) −Huδu(t) + (−Hλ + ˙x(t))δλ(t)
i

Optimal control theory and the linear Bellman equation
369
where the subscripts x, u, λ denote partial derivatives, e.g. Hx =
∂H(t,x(t),u(t),λ(t))
∂x(t)
. In the
second line above we have used partial integration:
Z T
0
dtλ(t)δ˙x(t) =
Z T
0
dtλ(t) d
dtδx(t) = −
Z T
0
dt d
dtλ(t)δx(t) + λ(T)δx(T) −λ(0)δx(0)
and δx(0) = 0. The stationary solution (δC = 0) is obtained when the coeﬃcients of the
independent variations δx(t), δu(t), δλ(t) and δx(T) are zero. Thus,
˙λ = −Hx(t, x(t), u(t), λ(t)),
0 = Hu((t, x(t), u(t), λ(t)),
˙x = Hλ(t, x, u, λ),
λ(T) = −φx(x(T)).
(17.10)
We can solve Eq. (17.10) for u and denote the solution as u∗(t, x, λ). This solution is unique
if H is convex in u. The remaining equations are
˙x = H∗
λ(t, x, λ),
˙λ = −H∗
x(t, x, λ)
(17.11)
where we have deﬁned H∗(t, x, λ) = H(t, x, u∗(t, x, λ), λ) and with boundary conditions
x(0) = x0,
λ(T) = −φx(x(T)).
(17.12)
The solution provided by Eq. (17.11) with boundary conditions Eq. (17.12) are coupled
ordinary diﬀerential equations that describe the dynamics of x and λ over time with a
boundary condition for x at the initial time and for λ at the ﬁnal time. Compared to the HJB
equation, the complexity of solving these equations is low since only time discretisation and
no space discretisation is required. However, due to the mixed boundary conditions, ﬁnding
a solution that satisﬁes these equations is not trivial and requires sophisticated numerical
methods. The most common method for solving the PMP equations is called (multiple)
shooting [11, 13].
Equations (17.11) are also known as the so-called Hamilton equations of motion that
arise in classical mechanics, but then with initial conditions for both x and λ, [12]. In
fact, one can view control theory as a generalisation of classical mechanics. In classical
mechanics H is called the Hamiltonian. Consider the time evolution of H
˙H = Ht + Hu˙u + Hx ˙x + Hλ˙λ = Ht,
(17.13)
where we have used the dynamical equations (17.11) and (17.10). In particular, when f
and R in Eq. (17.9) do not explicitly depend on time, neither does H and Ht = 0. In this
case we see that H is a constant of motion: the control problem ﬁnds a solution such that
H(t = 0) = H(t = T).
17.3.4
Again mass on a spring
We consider again the example of the mass on a spring that we introduced in Section 17.3.2
where we had
˙x1 = x2,
˙x2 = −x1 + u,
R(x, u, t) = 0,
φ(x) = −x1.
The Hamiltonian (17.9) becomes H(t, x, u, λ) = λ1x2 + λ2(−x1 + u). Using Eq. (17.10) we
obtain u∗= −sign(λ2) and H∗(t, x, λ) = λ1x2 −λ2x1 −|λ2|. The Hamilton equations are
˙x = ∂H∗
∂λ
⇒
˙x1 = x2,
˙x2 = −x1 −sign(λ2),
˙λ = −∂H∗
∂x
⇒
˙λ1 = −λ2,
˙λ2 = λ1,
with x(t = 0) = x0 and λ(t = T) = (1, 0).

370
Hilbert J. Kappen
17.3.5
Comments
The HJB method gives a suﬃcient (and often necessary) condition for optimality. The solu-
tion of the PDE is expensive. The PMP method provides a necessary condition for optimal
control. This means that it provides candidate solutions for optimality. The PMP method
is computationally less complicated than the HJB method because it does not require dis-
cretisation of the state space. The PMP method can be used when dynamic programming
fails due to lack of smoothness of the optimal cost-to-go function.
The subject of optimal control theory in continuous space and time has been well stud-
ied in the mathematical literature and contains many complications related to the existence,
uniqueness and smoothness of the solution, particularly in the absence of noise. See [14]
for a clear discussion and further references. In the presence of noise and in particular in the
path integral framework, as we will discuss below, it seems that many of these intricacies
disappear.
17.4
Stochastic optimal control
In this section, we consider the extension of the continuous control problem to the case that
the dynamics is subject to noise and is given by a stochastic diﬀerential equation. First, we
give a very brief introduction to stochastic diﬀerential equations.
17.4.1
Stochastic diﬀerential equations
Consider the random walk on the line
xt+1 = xt + ξt,
ξt = ± √ν,
with x0 = 0. The increments ξt are iid random variables with mean zero,
D
ξ2
t
E
= ν and ν
is a constant. We can write the solution for xt in closed form as xt = Pt
i=1 ξi. Since xt is a
sum of random variables, xt becomes Gaussian in the limit of large t. We can compute the
evolution of the mean and covariance
⟨xt⟩=
tX
i=1
⟨ξi⟩= 0,
D
x2
t
E
=
tX
i, j=1
D
ξiξ j
E
=
tX
i=1
D
ξ2
i
E
+
tX
i, j=1,j,i
⟨ξi⟩
D
ξj
E
= νt.
Note that the ﬂuctuations σt =
qD
x2
t
E
= √νt increase with the square root of t. This is a
characteristic of diﬀusion, such as for instance ink in water or warm air in a room. In the
continuous time limit we deﬁne
dxt = xt+dt −xt = dξ,
(17.14)
with dξ an inﬁnitesimal mean zero Gaussian variable. In order to get the right scaling with
t we must choose
D
dξ2E
= νdt. Then in the limit of dt →0 we obtain
d
dt ⟨x⟩= lim
dt→0
 xt+dt −xt
dt

= lim
dt→0
⟨dξ⟩
dt
= 0
⇒
⟨x⟩(t) = 0,
d
dt
D
x2E
= ν
⇒
D
x2E
(t) = νt.

Optimal control theory and the linear Bellman equation
371
The conditional probability distribution of x at time t given x0 at time 0 is Gaussian and
speciﬁed by its mean and variance. Thus
ρ(x, t|x0, 0) =
1
√
2πνt
exp
 
−(x −x0)2
2νt
!
.
The process (17.14) is called a Wiener process.
17.4.2
Stochastic optimal control theory
Consider the stochastic diﬀerential equation which is a generalisation of Eq. (17.4)
dx = f(x(t), u(t), t)dt + dξ;
(17.15)
dξ is a Wiener process with
D
dξidξ j
E
= νij(t, x, u)dt and ν is a symmetric positive deﬁnite
matrix. Because the dynamics is stochastic, it is no longer the case that when x at time t
and the full control path u(t →T) are given, we know the future path x(t →T). Therefore,
we cannot minimise Eq. (17.5), but can only hope to be able to minimise its expected value
over all possible future realisations of the Wiener process
C(x0, u(0 →T)) =
*
φ(x(T)) +
Z T
0
dtR(x(t), u(t), t)
+
x0
.
(17.16)
The subscript x0 on the expectation value is to remind us that the expectation is over all
stochastic trajectories that start in x0. The solution of the control problem proceeds in a
similar manner to the deterministic case, with the only diﬀerence that we must add the
expected value over trajectories. Equation (17.3) becomes
J(t, xt) = min
ut R(t, xt, ut)dt + ⟨J(t + dt, xt+dt)⟩xt .
We must again make a Taylor expansion of J in dt and dx. However, since
D
dx2E
is of order
dt because of the Wiener process, we must expand up to order dx2:
⟨J(t + dt, xt+dt)⟩=
Z
dxt+dtN(xt+dt|xt, νdt)J(t + dt, xt+dt)
= J(t, xt) + dt∂tJ(t, xt) + ⟨dx⟩∂xJ(t, xt) + 1
2
D
dx2E
∂2
xJ(t, xt),
⟨dx⟩= f(x, u, t)dt,
D
dx2E
= ν(t, x, u)dt.
Thus we obtain the stochastic Hamilton–Jacobi–Bellman equation
−∂tJ(t, x) = min
u
 
R(t, x, u) + f(x, u, t)∂xJ(x, t) + 1
2ν(t, x, u)∂2
xJ(x, t)
!
,
(17.17)
with boundary condition J(x, T) = φ(x). Equation (17.17) reduces to the deterministic HJB
equation (17.6) in the limit ν →0.

372
Hilbert J. Kappen
17.4.3
Linear quadratic control
In the case that the dynamics is linear and the cost is quadratic one can show that the optimal
cost-to-go J is also a quadratic form and one can solve the stochastic HJB equation in terms
of ‘suﬃcient statistics’ that describe J. Here x is n-dimensional and u is p-dimensional. The
dynamics is linear
dx = [A(t)x + B(t)u + b(t)]dt +
m
X
j=1
(C j(t)x + Dj(t)u + σ j(t))dξj,
with dimensions: A = n × n, B = n × p, b = n × 1,C j = n × n, Dj = n × p, σj = n × 1 and
D
dξjdξj′
E
= δ jj′dt. The cost function is quadratic
φ(x) = 1
2 xTGx,
f0(x, u, t) = 1
2 xT Q(t)x + uTS (t)x + 1
2uTR(t)u,
with G = n × n, Q = n × n, S = p × n, R = p × p. We parameterise the optimal cost-to-go
function as
J(t, x) = 1
2 xT P(t)x + αT(t)x + β(t),
(17.18)
which should satisfy the stochastic HJB equation (17.17) with P(T) = G and α(T) =
β(T) = 0. The function P(t) is an n × n matrix, α(t) is an n-dimensional vector and β(t) is
a scalar. Substituting this form of J in Eq. (17.17), this equation contains terms quadratic,
linear and constant in x and u. We can thus do the minimisation with respect to u exactly
and the result is
u(t) = −Ψ(t)x(t) −ψ(t),
with
ˆR = R + Pm
j=1 DT
j PD j
(p × p),
ˆS = BT P + S + Pm
j=1 DT
j PC j
(p × n),
Ψ = ˆR−1 ˆS
(p × n),
ψ = ˆR−1(BTα + Pm
j=1 DT
j Pσ j)
(p × 1).
The stochastic HJB equation then decouples as three ordinary diﬀerential equations (Riccati
equations)
−˙P = PA + AT P +
m
X
j=1
CT
j PC j + Q −ˆS T ˆR−1 ˆS ,
(17.19)
−˙α = [A −B ˆR−1 ˆS ]Tα +
m
X
j=1
[C j −Dj ˆR−1 ˆS ]T Pσ j + Pb,
(17.20)
˙β = 1
2

p
ˆRψ

2
−αTb −1
2
m
X
j=1
σT
j Pσ j.
(17.21)
The way to solve these equations is to ﬁrst solve Eq. (17.19) for P(t) with end condition
P(T) = G. Use this solution in Eq. (17.20) to compute the solution for α(t) with end
condition α(T) = 0. Finally, β(s) = −
R T
s ˙βdt can be computed from Eq. (17.21).

Optimal control theory and the linear Bellman equation
373
17.4.4
Example of LQ control
Find the optimal control for the dynamics
dx = (x + u)dt + dξ,
D
dξ2E
= νdt,
with end cost φ(x) = 0 and path cost R(x, u) = 1
2(x2 + u2). The Riccati equations reduce to
−˙P = 2P + 1 −P2,
−˙α = 0,
˙β = −1
2νP,
with P(T) = α(T) = β(T) = 0 and u(x, t) = −P(t)x. We compute the solution for P and β
by numerical integration. The result is shown in Fig. 17.1(b). The optimal control is to steer
towards the origin with −P(t)x, where P is roughly constant until T ≈8. Afterwards control
weakens because the total future state cost reduces to zero when t approaches the end time.
Note that in this example the optimal control is independent of ν. It can be veriﬁed from the
Riccati equations that this is true in general for ‘non-multiplicative’ noise (C j = Dj = 0).
17.5
Learning
So far, we have assumed that all aspects that deﬁne the control problem are known. But in
many instances this is not the case. What happens if (part of) the state is not observed? For
instance, as a result of measurement error we do not know xt but only a probability distri-
bution p(xt|y0:t) given some previous observations y0:t. Or, we observe xt, but do not know
the parameters of the dynamical equation (17.15). Or, we do not know the cost/rewards
functions that appear in Eq. (17.16).
Using a Bayesian point of view, the agent can represent the uncertainty as beliefs,
i.e. probability distributions over the hidden states, parameters or rewards. The optimal
behaviour is then a trade-oﬀbetween two objectives: choosing actions that optimise the
expected future reward given the current beliefs and choosing actions that improve the
accuracy of the beliefs. In other words, the agent faces the problem of ﬁnding the right
compromise between learning and control, a problem which is known in control theory as
dual control and was originally introduced by [6] (see [7] for a recent review). In addition
to the observed state variables x, there are an additional number of variables θ that specify
the belief distributions. The dual control solution is the ordinary control solution in this
extended (x, θ) state space. The value function becomes a function of the extended state
space and the Bellman equation describes the evolution in this extended state space. Some
approaches to partially observed MDPs (POMDPs) [25, 15, 24] are an example of dual
control problems.
A typical solution to the dual control problem for a ﬁnite time horizon problem is a
control strategy that ﬁrst chooses actions that explore the state space in order to learn a
good model and use it at later times to maximise reward. In other words, the dual control
problem solves the exploration exploitation problem by making explicit assumptions about
the belief distributions. This is very reminiscent of our own life. Our life is ﬁnite and we
have only one life. Our aim is to maximise accumulated reward during our lifetime, but
in order to do so we have to allocate some resources to learning as well. It requires that
we plan our learning and the learning problem becomes an integral part of the control
problem. At t = 0, there is no knowledge of the world and thus making optimal control
actions is impossible. At t = T, learning has become useless, because we will have no
more opportunity to make use of it. So we should learn early in life and act later in life, as

374
Hilbert J. Kappen
t
learning
action
Figure 17.2 When life is ﬁnite and is executed only one time, we
should ﬁrst learn and then act.
is schematically shown in Fig. 17.2. See [3] for a further discussion. We discuss a simple
example in Section 17.5.1.
Note that reinforcement learning is typically deﬁned as an adaptive control problem
rather than a dual control problem. These approaches use beliefs that are speciﬁed in terms
of hyperparameters θ, but the optimal cost-to-go is still a function of the original state x
only. The Bellman equation is an evolution equation for J(x) where unobserved quantities
are given in terms of their expected values that depend on θ. This control problem is then
in principle solved for ﬁxed θ (although in reinforcement learning often a sample-based
approach is taken and no strict convergence is enforced): θ is adapted as a result of the sam-
ples that are collected. In this formulation, the exploration exploitation dilemma arises since
the control computation will propose actions that are only directed towards exploitation
assuming the wrong θ (its optimal value still needs to be found). As a result, the state space
is not fully explored and the updates for θ thus obtained are biased. The common heuris-
tic to improve the learning is to mix these actions with ‘exploratory actions’ that explore
the state space in directions that are not dictated by exploitation. Well-known examples
of this approach are Bayesian reinforcement learning [5] and some older methods that are
reviewed in [31]. Nevertheless, the principled solution is to explore all space, for instance
by using a dedicated exploration strategy such as proposed in the E3 algorithm [20].
In the case of ﬁnite time control problems the diﬀerence between the dual control for-
mulation and the adaptive control formulation becomes particularly clear. The dual control
formulation requires only one trial of the problem. It starts at t = 0 with its initial belief θ0
and initial state x0 and computes the optimal solution by solving the Bellman equation in
the extended state space for all intermediate times until the horizon time T. The result is a
single trajectory (x1:T, θ1:T). The adaptive control formulation requires many trials. In each
trial i, the control solution is computed by solving the Bellman equation in the ordinary
state space where the beliefs are given by θi. The result is a trajectory (x1:T, θi). Between
trials, θ is updated using samples from the previous trial(s). Thus, in the adaptive control
approach the learning problem is not solved as part of the control problem but rather in
an ‘outer loop’. The time scale for learning is unrelated to the horizon time T. In the dual
control formulation, learning must take place in a single trial and is thus tightly related to
T. Needless to say, the dual control formulation is more attractive than the adaptive control
formulation, but is computationally signiﬁcantly more costly.
17.5.1
Inference and control
As an example [10, 21], consider the LQ control problem
dx = αudt + dξ
with α unobserved and x observed. The path cost R(x, u, t), end cost φ(x) and noise variance
ν are given. Although α is unobserved, we have a means to observe α indirectly through
the sequence xt, ut, t = 0, . . .. Each time step we observe dx and u and we can thus update
our belief about α using Bayes formula
pt+dt(α|dx, u) ∝p(dx|α, u)pt(α)
(17.22)

Optimal control theory and the linear Bellman equation
375
with p(dx|α, u) a Normal distribution in dx with variance νdt and pt(α) a probability dis-
tribution that expresses our belief at time t about the values of α. The problem is that the
future information that we receive about α depends on u: if we use a large u, the term αudt
is larger than the noise term dξ and we will get reliable information about α. However,
large u values are more costly and also may drive us away from our target state x(T). Thus,
the optimal control is a balance between optimal inference and minimal control cost. The
solution is to augment the state space with parameters θt (suﬃcient statistics) that describe
pt(α) = p(α|θt) and θ0 known, which describes our initial belief in the possible values of α.
The cost that must be minimised is
C(x0, θ0, u(0 →T)) =
*
φ(x(T)) +
Z T
0
dtR(x, u, t)
+
,
where the average is with respect to the noise dξ as well as the uncertainty in α.
For simplicity, consider the example that α attains only two values α = ±1. Then
pt(α|θ) = σ(αθ), with the sigmoid function σ(x) = 1
2(1 + tanh(x)). The update equation
(17.22) implies a dynamics for θ:
dθ = u
νdx = u
ν(αudt + dξ).
With zt = (xt, θt) we obtain a standard HJB (Eq. (17.17))
−∂tJ(t, z)dt = min
u
 
R(x, u, t)dt + ⟨dz⟩z ∂zJ(z, t) + 1
2
D
dz2E
z ∂2
z J(z, t)
!
with boundary condition J(z, T) = φ(x). The expectation values appearing in this equation
are conditioned on (xt, θt) and are averages over p(α|θt) and the Gaussian noise. We com-
pute ⟨dx⟩x,θ = ¯αudt, ⟨dθ⟩x,θ =
¯αu2
ν dt,
D
dx2E
x,θ = νdt,
D
dθ2E
x,θ = u2
ν dt, ⟨dxdθ⟩= udt, with
¯α = tanh(θ) the expected value of α for a given value θ. The result is
−∂tJ = min
u
 
R(x, u, t) + ¯αu∂xJ + u2 ¯α
ν ∂θJ + 1
2ν∂2
xJ + 1
2
u2
ν ∂2
θJ + u∂x∂θJ
!
with boundary conditions J(x, θ, T) = φ(x). Thus the dual control problem (joint inference
on α and control problem on x) has become an ordinary control problem in x, θ. Quoting
[10]: ‘It seems that any systematic formulation of the adaptive control problem leads to a
meta-problem which is not adaptive.’ Note also that the dynamics for θ is non-linear (due
to the u2 term) although the original dynamics for dx was linear. The solution to this non-
linear stochastic control problem requires the solution of this PDE and was studied in [19].
An example of the optimal control solution u(x, θ, t) for x = 2 and diﬀerent θ and t is given
in Fig. 17.3. Note the ‘probing’ solution with u is much larger when α is uncertain (θ small)
than when α is certain (θ = ±∞). This exploration strategy is optimal in the dual control
formulation. In [19] we further demonstrate that exploration is achieved through symmetry
breaking in the Bellman equation; that optimal actions can be discontinuous in the beliefs
(as in Fig. 17.3); and that the optimal value function is typically non-diﬀerentiable. This
poses a challenge for the design of value function approximations for POMDPs, which
typically assumes a smooth class of functions.
17.5.2
Certainty equivalence
Although in general adaptive control is much more complex than non-adaptive control,
there exists an exception for a large class of linear quadratic problems, such as the Kalman
ﬁlter [27]. Consider the dynamics

376
Hilbert J. Kappen
Figure 17.3 Dual control solution with end cost φ(x) = x2
and path cost
R t f
t
dt′ 1
2u(t′)2 and ν = 0.5. Plot shows the devi-
ation of the control from the certain case: ut(x, θ)/ut(x, θ =
±∞) as a function of θ for diﬀerent values of t. The curves
with the larger values are for larger times-to-go.
dx = (x + u)dt + dξ,
y = x + η,
where now x is not observed, but y is observed and all other model parameters are known.
When x is observed, we assume a quadratic cost
C(xt, t, ut:T) =
* T
X
τ=t
1
2(x2
τ + u2
τ)
+
.
We denote the optimal control solution by u(x, t). When xt is not observed, we can compute
p(xt|y0:t) using Kalman ﬁltering and the optimal control minimises
CKF(y0:t, t, ut:T) =
Z
dxtp(xt|y0:t)C(xt, t, ut:T),
(17.23)
with C as above. Since p(xt|y0:t) = N(xt|µt, σ2
t ) is Gaussian, Eq. (17.23) becomes
Z
dxtC(xt, t, ut:T)N(xt|µt, σ2
t ) =
T
X
τ=t
1
2u2
τ +
T
X
τ=t
D
x2
τ
E
µt,σt ,
which is
T
X
τ=t
1
2u2
τ + 1
2(µ2
t + σ2
t ) + 1
2
Z
dxt
D
x2
t+dt
E
xt,νdt N(xt|µt, σ2
t ) + · · ·
=
T
X
τ=t
1
2u2
τ + 1
2(µ2
t + σ2
t ) + 1
2
D
x2
t+dt
E
µt,νdt + 1
2σ2
t + · · ·
= C(µt, t, ut:T) + 1
2(T −t)σ2
t .
The ﬁrst term is identical to the observed case with xt →µt. The second term does not
depend on u and thus does not aﬀect the optimal control. Thus, the optimal control for
the Kalman ﬁlter uKF(y0:t, t) computed from CKF is identical to the optimal control function
u(x, t) that is computed for the observed case C, with xt replaced by µt: uKF(y0:t, t) = u(µt, t).
This property is known as certainty equivalence [27], and implies that for these systems the
control computation and the inference computation can be done separately, without loss of
optimality.

Optimal control theory and the linear Bellman equation
377
17.6
Path integral control
As we have seen, the solution of the general stochastic optimal control problem requires
the solution of a partial diﬀerential equation. This is for many realistic applications not an
attractive option. The alternative considered often, is to approximate the problem some-
how by a linear quadratic problem which can then be solved eﬃciently using the Riccati
equations. In this section, we discuss a special class of non-linear, non-quadratic control
problems for which some progress can be made [16, 17]. For this class of problems, the
non-linear Hamilton–Jacobi–Bellman equation can be transformed into a linear equation
by a log transformation of the cost-to-go. The transformation stems back to the early days
of quantum mechanics and was ﬁrst used by Schr¨odinger to relate the Hamilton–Jacobi
formalism to the Schr¨odinger equation (a linear diﬀusion-like equation). The log transform
was ﬁrst used in the context of control theory by [8] (see also [9]).
Due to the linear description, the usual backward integration in time of the HJB equa-
tion can be replaced by computing expectation values under a forward diﬀusion process.
The computation of the expectation value requires a stochastic integration over trajectories
that can be described by a path integral. This is an integral over all trajectories starting at
x, t, weighted by exp(−S/λ), where S is the cost of the path (also known as the action) and
λ is a constant that is proportional to the noise.
The path integral formulation is well known in statistical physics and quantum mechan-
ics, and several methods exist to compute path integrals approximately. The Laplace
approximation approximates the integral by the path of minimal S . This approximation
is exact in the limit of ν →0, and the deterministic control law is recovered. In general,
the Laplace approximation may not be suﬃciently accurate. A very generic and powerful
alternative is Monte Carlo (MC) sampling. The theory naturally suggests a naive sampling
procedure, but it is also possible to devise more eﬃcient samplers, such as importance
sampling.
We illustrate the control method on two tasks: a temporal decision task, where the agent
must choose between two targets at some future time; and a simple n joint arm. The decision
task illustrates the issue of spontaneous symmetry breaking and how optimal behavior is
qualitatively diﬀerent for high and low noise. The n joint arm illustrates how the eﬃcient
approximate inference methods (the variational approximation in this case) can be used to
compute optimal controls in very high dimensional problems.
17.6.1
Path integral control
Consider the special case of Eqs. (17.15) and (17.16) where the dynamic is linear in u and
the cost is quadratic in u
dxi = fi(x, t)dt +
p
X
j=1
gij(x, t)(ujdt + dξ j),
(17.24)
R(x, u, t) = V(x, t) + 1
2uTRu
(17.25)
with R a non-negative matrix. The functions fi(x, t), gi j(x, t) and V(x, t) are arbitrary func-
tions of x and t, and
D
dξ jdξ j′
E
= νjj′dt. In other words, the system to be controlled can be
arbitrarily complex and subject to arbitrary complex costs. The control instead, is restricted
to the simple linear-quadratic form when gij = 1 and in general must act in the same
subspace as the noise. We will suppress all component notation from now on. Quantities

378
Hilbert J. Kappen
such as f, u, x, dx are vectors and R, g, ν are matrices. The stochastic HJB equation (17.17)
becomes
−∂tJ = min
u
 1
2uTRu + V + (∇J)T( f + gu) + 1
2TrνgT∇2Jg
!
.
Due to the linear-quadratic appearance of u, we can minimise with respect to u explicitly
which yields
u = −R−1gT∇J,
(17.26)
which deﬁnes the optimal control u for each x, t. The HJB equation becomes
−∂tJ = V + (∇J)T f + 1
2Tr

−gR−1gT(∇J)(∇J)T + gνgT∇2J

.
Note that after performing the minimisation with respect to u, the HJB equation has become
non-linear in J. We can, however, remove the non-linearity which will help us to solve the
HJB equation. Deﬁne ψ(x, t) through J(x, t) = −λ log ψ(x, t). We further assume that there
exists a constant λ such that the matrices R and ν satisfy: λR−1 = ν.1 This relation basically
says that directions in which control is expensive should have low noise variance. It can also
be interpreted as saying that all noise directions are controllable (in the correct proportion).
Then the HJB becomes
−∂tψ(x, t) =
 
−V
λ + f T∇+ 1
2Tr

gνgT∇2!
ψ.
(17.27)
Equation (17.27) must be solved backwards in time with ψ(x, T) = exp(−φ(x)/λ). The lin-
earity allows us to reverse the direction of computation, replacing it by a diﬀusion process,
in the following way. Let ρ(y, τ|x, t) describe a diﬀusion process for τ > t deﬁned by the
Fokker–Planck equation
∂τρ = −V
λ ρ −∇T( fρ) + 1
2Tr

∇2(gνgTρ)

(17.28)
with initial condition ρ(y, t|x, t) = δ(y −x). Note that when V = 0, Eq. (17.28) describes the
evolution of diﬀusion process (17.24) with u = 0. Deﬁne A(x, t) =
R
dyρ(y, τ|x, t)ψ(y, τ). It
is easy to see by using the equations of motion (17.27) and 17.28 that A(x, t) is independent
of τ. Evaluating A(x, t) for τ = t yields A(x, t) = ψ(x, t). Evaluating A(x, t) for τ = T yields
A(x, t) =
R
dyρ(y, T|x, t)ψ(x, T). Thus,
ψ(x, t) =
Z
dyρ(y, T|x, t) exp(−φ(y)/λ).
(17.29)
The important conclusion is that the optimal cost-to-go J(x, t) = −λ log ψ(x, t) can be
computed either by backward integration using Eq. (17.27) or by forward integration of
a diﬀusion process given by Eq. (17.28). The optimal control is given by Eq. (17.26). Both
Eq. (17.27) and Eq. (17.28) are partial diﬀerential equations and, although being linear,
still suﬀer from the curse of dimensionality. However, the great advantage of the forward
diﬀusion process is that it can be simulated using standard sampling methods which can
eﬃciently approximate these computations. In addition, as is discussed in [17], the forward
diﬀusion process ρ(y, T|x, t) can be written as a path integral and in fact (17.29) becomes a
path integral. This path integral can then be approximated using standard methods, such as
the Laplace approximation.
1Strictly, the weaker condition λg(x, t)R−1gT (x, t) = g(x, t)νgT (x, t) should hold.

Optimal control theory and the linear Bellman equation
379
Example: linear quadratic case
The class of control problems contains both additive and multiplicative cases. We give an
example of both. Consider the control problem (17.24) and (17.25) for the simplest case of
controlled free diﬀusion:
V(x, t) = 0,
f(x, t) = 0,
φ(x) = 1
2αx2.
In this case, the forward diﬀusion described by Eqs. 17.28 can be solved in closed form
and is given by a Gaussian with variance σ2 = ν(T −t):
ρ(y, T|x, t) =
1
√
2πσ
exp
 
−(y −x)2
2σ2
!
.
Since the end cost is quadratic, the optimal cost-to-go (17.29) can be computed exactly as
well. The result is
J(x, t) = νR log
 σ
σ1
!
+ 1
2
σ2
1
σ2 αx2
(17.30)
with 1/σ2
1 = 1/σ2 + α/νR. The optimal control is computed from Eq. (17.26):
u = −R−1∂xJ = −R−1 σ2
1
σ2 αx = −
αx
R + α(T −t).
We see that the control attracts x to the origin with a force that increases with t getting
closer to T. Note that the optimal control is independent of the noise ν as we also saw in
the previous LQ example in Section 17.4.4.
Example: multiplicative case
Consider as a simple example of a multiplicative case, f = 0, g = x, V = 0 in one dimension
and R = 1. Then the forward diﬀusion process reduces to
dx = x(udt + dξ)
(17.31)
and x(ti) = x0 for initial time ti. If we deﬁne y = log x then
dy = dy
dxdx + 1
2
d2y
dx2 dx2 = udt + dξ −ν
2dt
with y(ti) = log x0 and the solution in terms of y is simply a Gaussian distribution
˜ρ(y′, t|y, ti) =
1
√
2πσ
exp(−(y′ −y −(u −ν/2)(t −ti))2/2σ2)
with σ2 = (t −ti)ν. In terms of x the solution becomes
ρ(x′, t|x, ti) = 1
x′ ˜ρ(log x′, t| log x, ti).
(17.32)
The solution is shown in Fig. 17.4(a) for u = 0 and ﬁnal time t f = 0.1, 0.5 and t f = 2.
For t f = 0.5 the solution is compared with forward simulation of Eq. (17.31). Note that
the diﬀusion drifts towards the origin, which is caused by the state-dependent noise. The

380
Hilbert J. Kappen
0
1
2
3
4
5
0
0.5
1
1.5
2
2.5
x
ρ
T=0.1
T=0.5
T=2
(a)
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
1.2
t
x
(b)
0
0.2
0.4
0.6
0.8
1
−2
0
2
4
6
8
10
12
t
u
(c)
Figure 17.4 Optimal control for the one-dimensional multiplicative process (17.31) with quadratic control cost
R t f
t1 dt 1
2u(t)2 to reach a ﬁxed target x′ = 1, starting from an initial position x = 1. (a) ρ(x′, t f |x = 1, ti = 0) vs.
x′ for tf = 0.1, 0.5, 2 and shows the forward diﬀusion solution in the absence of control (17.32) which is used to
compute the optimal control solution (17.33). (b) Optimal trajectory x(t). (c) Optimal control u(t).
noise is proportional to x and therefore the conditional probability p(xsmall|xlarge) is greater
than the reverse probability p(xlarge|xsmall). This results in a net drift towards small x. From
Eq. (17.32), we can compute the optimal control. Consider the control task to steer to a
ﬁxed end point x′ from an arbitrary initial point x. Then,
ψ(x, t) = ρ(x′, t f |x, t) =
1
√
2πνT
1
x′ exp

−(log(x′) −log(x) + νT/2)2/2νT

,
J(x, t) = −ν log ψ(x, t) = ν log
√
2πνT + ν log x′ + (log(x′) −log(x) + νT/2)2/2T,
u(x, t) = −xdJ(x, t)
dx
= 1
T log
 x′
x
!
+ ν/2,
(17.33)
with T = t f −t. The ﬁrst term attracts x to x′ with strength increasing in 1/T as usual. The
second term is a constant positive drift, to counter the tendency of the uncontrolled process
to drift towards the origin. An example of the solution for a task to steer from x = 1 at t = 0
to x = 1 at t = 1 is shown in Fig. 17.4(b) and Fig. 17.4(c).
17.6.2
The diﬀusion process as a path integral
The diﬀusion equation (17.28) contains three terms. The second and third terms describe
drift f(x, t)dt and diﬀusion g(x, t)dξ as in Eq. (17.24) with u = 0. The ﬁrst term describes
a process that kills a sample trajectory with a rate V(x, t)dt/λ. This term does not conserve
the probability mass. Thus, the solution of Eq. (17.28) can be obtained by sampling the
following process
dx = f(x, t)dt + g(x, t)dξ,
x = x + dx,
with probability 1 −V(x, t)dt/λ,
xi = †,
with probability V(x, t)dt/λ.
(17.34)
We can thus obtain a sampling estimate of
ψ(x, t) =
Z
dyρ(y, T|x, t) exp(−φ(y)/λ) ≈1
N
X
i∈alive
exp(−φ(xi(T))/λ)
(17.35)
by computing N trajectories xi(t →T), i = 1, . . . , N. Each trajectory starts at the same value
x and is sampled using the dynamics (17.35). ‘Alive’ denotes the subset of trajectories that

Optimal control theory and the linear Bellman equation
381
do not get killed along the way by the † operation. The diﬀusion process can formally
be ‘solved’ as a path integral. We restrict ourselves to the simplest case gi j(x, t) = δi j.
The general case can also be written as a path integral, but is somewhat more involved.
The argument follows simply by splitting the time interval [t, T] into a large number n
of inﬁnitesimal intervals [t, t + dt]. For each small interval, ρ(y, t + dt|x, t) is a product of
a Gaussian distribution due to the drift f and diﬀusion gdξ, and the annihilation process
exp(−V(x, t)dt/λ): ρ(y, t+dt|x, t) = N(y|x+ f(x, t)dt, ν). We can then compute ρ(y, T|x, t) by
multiplying all these inﬁnitesimal transition probabilities and integrating the intermediate
variables y. The result is
ρ(y, T|x, t) =
Z
[dx]y
x exp
 
−1
λS path(x(t →T))
!
,
S path(x(t →T)) =
Z T
t
dτ1
2(˙x(τ) −f(x(τ), τ))TR(˙x(τ) −f(x(τ), τ)) +
Z T
t
dτV(x(τ), τ).
(17.36)
Combining Eq. (17.36) and Eq. (17.29), we obtain the cost-to-go as
ψ(x, t) =
Z
[dx]x exp
 
−1
λS (x(t →T))
!
,
S (x(t →T)) = S path(x(t →T)) + φ(x(T)).
(17.37)
Note that ψ has the general form of a partition sum, S is the energy of a path and λ the
temperature. The corresponding probability distribution is
p(x(t →T)|x, t) =
1
ψ(x, t) exp
 
−1
νS (x(t →T))
!
.
The function J = −λ log ψ can be interpreted as a free-energy. See [17] for details. Although
we have solved the optimal control problem formally as a path integral, we are still left
with the problem of computing the path integral. Here one can resort to various standard
methods such as Monte Carlo sampling [17] of which the naive forward sampling (17.34)
is an example. One can however, improve on this naive scheme using importance sampling
where one changes the drift term such as to minimise the annihilation of the diﬀusion by
the −V(x, t)dt/λ term.
A particularly cheap approximation is the Laplace approximation, that ﬁnds the trajec-
tory that minimises S in Eq. (17.37). This approximation is exact in the limit of λ →0
which is the noiseless limit. The Laplace approximation gives the classical path. One par-
ticular eﬀective forward importance sampling method is to use the classical path as a drift
term. We will give an example of the naive and importance forward sampling scheme below
for the double slit problem.
One can also use a variational approximation to approximate the path integral using
the variational approach for diﬀusion processes [1], or use the EP approximation [22].
An illustration of the variational approximation to a simple n joint arm is presented in
Section 17.7.2.

382
Hilbert J. Kappen
17.7
Approximate inference methods for control
17.7.1
Monte Carlo sampling
In this section, we illustrate the path integral control method for the simple example of a
double slit. The example is suﬃciently simple that we can compute the optimal control
solution in closed form. We use this example to compare the Monte Carlo and Laplace
approximations to the exact result. Consider a stochastic particle that moves with constant
velocity from t to T in the horizontal direction and where there is deﬂecting noise in the x
direction:
dx = udt + dξ.
(17.38)
The cost is given by Eq. (17.25) with φ(x) =
1
2 x2 and V(x, t1) implements a slit at an
intermediate time t1, t < t1 < T:
V(x, t1) = 0,
a < x < b,
c < x < d,
= ∞,
otherwise.
The problem is illustrated in Fig. 17.5(a) where the constant motion is in the t (horizontal)
direction and the noise and control is in the x (vertical) direction. The cost-to-go can be
solved in closed form. The result for t > t1 is a simple linear quadratic control problem for
which the solution is given by Eq. (17.30) and for t < t1 is [17]:
J(x, t) = νR log
 σ
σ1
!
+ 1
2
σ2
1
σ2 x2 −νR log 1
2 (F(b, x) −F(a, x) + F(d, x) −F(c, x))
(17.39)
F(x0, x) = Erf

r
A
2ν(x0 −B(x)
A )
,
A =
1
t1 −t +
1
R + T −t1
,
B(x) =
x
t1 −t.
The solution (17.39) is shown for t = 0 in Fig. 17.5(b). We can compute the optimal control
from Eq. (17.26).
We assess the quality of the naive MC sampling scheme, as given by Eqs. (17.34) and
(17.35) in Fig. 17.5(b) and Fig. 17.5(c). Figure 17.5(b) shows the sampling trajectories of
(a)
(b)
(c)
(d)
Figure 17.5 Double slit experiment. (a) Setup of the experiment. Particles travel from t = 0 to t = 2 under
dynamics (17.38). A slit is placed at time t = t1, blocking all particles by annihilation. Two trajectories are
shown under optimal control. (b) Naive Monte Carlo sampling trajectories to compute J(x = −1, t = 0) through
Eq. (17.35). Only trajectories that pass through a slit contribute to the estimate. (c) Comparison of naive MC
estimates with N = 100 000 trajectories and exact result for J(x, t = 0) for all x. (d) Comparison of Laplace
approximation (dotted line) and Monte Carlo importance sampling (solid jagged line) of J(x, t = 0) with exact
result (17.39) (solid smooth line). The importance sampler used N = 100 trajectories for each x. See plate section
for colour version.

Optimal control theory and the linear Bellman equation
383
the naive MC sampling procedure for one particular value of x. Note the ineﬃciency of
the sampler because most of the trajectories are killed at the inﬁnite potential at t = t1.
Figure 17.5(c) shows the accuracy of the naive MC sampling estimate of J(x, 0) for all x
between −10 and 10 using N = 100 000 trajectories. We note that the number of trajecto-
ries that are required to obtain accurate results strongly depends on the initial value of x
due to the annihilation at t = t1. As a result, low values of the cost-to-go are more easy to
sample accurately than high values. In addition, the eﬃciency of the sampling procedures
depends strongly on the noise level. For small noise, the trajectories spread less by them-
selves and it is harder to generate trajectories that do not get annihilated. In other words,
sampling becomes more accurate for high noise, which is a well-known general feature of
sampling.
The sampling is particularly diﬃcult in this example because of the inﬁnite potential
that annihilates most of the trajectories. However, similar eﬀects will be observed in general
due to the multi-modality of the Action. We can improve the importance sampling proce-
dure using the Laplace approximation (see [17]). The Laplace approximation in this case
are the two piece-wise linear trajectories that pass through one of the slits to the goal. The
Laplace approximation and the results of the importance sampler are given in Fig. 17.5(d).
The Laplace approximation is quite good for this example, in particular when one takes
into account that a constant shift in J does not aﬀect the optimal control. The MC impor-
tance sampler dramatically improves over the naive MC results in Fig. 17.5, in particular
since 1000 times less samples are used and is also signiﬁcantly better than the Laplace
approximation.
17.7.2
The variational method
We consider a particularly simple realisation of an n joint arm in two dimensions, each joint
having length 1. The location of the ith joint in the 2d plane is
xi =
iX
j=1
cos θi,
yi =
iX
j=1
sin θi
with i = 1, . . . , n. Each of the joint angles is controlled by a variable ui. The dynamics of
each joint is, for independent Gaussian noise dξi with
D
dξ2
i
E
= νdt,
dθi = uidt + dξi,
i = 1, . . . , n.
Denote by θ the vector of joint angles, and u the vector of controls. The expected cost for
control path ut:T is
C(θ, t, ut:T) =
*
φ(θ(T)) +
Z T
t
1
2uT(t) u(t)
+
,
φ(θ) = α
2

(xn(θ) −xtarget)2 + (yn(θ) −ytarget)2
,
with xtarget, ytarget the target coordinates of the end joint. Because the state-dependent path
cost V and the intrinsic dynamics of f are zero, the solution to the diﬀusion process (17.34)
that starts with the arm in the conﬁguration θ 0 is a Gaussian so that Eq. (17.29) becomes2
2This is not exactly correct because θ is a periodic variable. One should use the solution to diﬀusion on a circle
instead. We can ignore this as long as √ν(T −t) is small compared to 2π.

384
Hilbert J. Kappen
−0.5
0
0.5
1
1.5
2
2.5
−1.5
−1
−0.5
0
0.5
1
(a) t = 0.05
−0.5
0
0.5
1
1.5
2
2.5
−1.5
−1
−0.5
0
0.5
1
(b) t = 0.55
−0.5
0
0.5
1
1.5
2
2.5
−1.5
−1
−0.5
0
0.5
1
(c) t = 1.8
−0.5
0
0.5
1
1.5
2
2.5
−1.5
−1
−0.5
0
0.5
1
(d) t = 2.0
0
20
40
60
80
100
−30
−20
−10
0
10
20
30
(e) t = 0.05
0
20
40
60
80
100
−30
−20
−10
0
10
20
30
(f) t = 0.55
0
20
40
60
80
100
−30
−20
−10
0
10
20
30
(g) t = 1.8
0
20
40
60
80
100
−30
−20
−10
0
10
20
30
(h) t = 2.0
Figure 17.6 (a)–(d) Path integral control of a n = 3 joint arm. The objective is that the end joint reaches a target
location at the end time T = 2. Solid line: current joint conﬁguration in Cartesian coordinates (x, y) corresponding
to the angle state θ0 at time t. Dashed: expected joint conﬁguration computed at the horizon time T = 2 corre-
sponding to the expected angle state ⟨θ ⟩from Eq. (17.41) with θ 0 the current joint position. Target location of
the end eﬀector is at the origin, resulting in a triangular conﬁguration for the arm. As time t increases, each joint
moves to its expected target location due to the control law (17.40). At the same time the expected conﬁguration
is recomputed, resulting in a diﬀerent triangular arm conﬁguration. (e)–(h). Same, with n = 100.
ψ(θ 0, t) =
Z
dθ
 
1
√2πν(T −t)
!n
exp
−
n
X
i=1
(θi −θ0
i )2/2ν(T −t) −φ(θ)/ν
.
The control at time t for all components i is computed from Eq. (17.26) and is
ui =
1
T −t

⟨θi⟩−θ0
i

,
(17.40)
where ⟨θi⟩is the expectation value of θi computed w.r.t. the probability distribution
p(θ) =
1
ψ(θ 0, t) exp
−
n
X
i=1
(θi −θ0
i )2/2ν(T −t) −φ(θ)/ν
.
(17.41)
Thus the stochastic optimal control problem reduces the inference problem to computing
⟨θi⟩. There are several ways to compute this. One can use a simple importance sampling
scheme, where the proposal distribution is the n-dimensional Gaussian centred on θ 0 (ﬁrst
term in Eq. (17.41)) and where samples are weighted with exp(−φ(θ)/ν). This does not
work very well (results not shown). One can also use a Metropolis Hastings method with
a Gaussian proposal distribution. This works quite well (results not shown). One can also
use a very simple variational method which we will now discuss.
The expectations ⟨θ ⟩are found using a factorised Gaussian variational approximation
q(θ) = Qn
i=1 N(θi|µi, σi) to p(θ) in Eq. (17.41). We compute µi and σi by minimising the
divergence KL(q|p) ≡
log q(θ)/p(θ)
q(θ), which equals, up to a constant,
−
n
X
i=1
log
q
2πσ2
i + log ψ(θ 0, t) +
1
2ν(T −t)
n
X
i=1

σ2
i + (µi −θ0
i )2
+ 1
ν ⟨φ(θ)⟩q .
Because φ is quadratic in xn and yn and these are deﬁned in terms of sines and cosines,
the ⟨φ(θ)⟩can be computed in closed form. The computation of the variational equations

Optimal control theory and the linear Bellman equation
385
result from setting the derivative of the KL with respect to µi and σ2
i equal to zero. The
result is, for ∆x ≡⟨xn⟩−xtarget, ∆y ≡⟨yn⟩−ytarget,
µi ←θ0
i + α(T −t)

sin µie−σ2
i /2 −cos µie−σ2
i /2∆y

1
σ2
i
←1
ν
 
1
(T −t) + αe−σ2
i −α∆x cos µie−σ2
i /2 −α∆y sin µie−σ2
i /2
!
.
The converged estimate for ⟨θi⟩is µi. The problem is illustrated in Fig. 17.6.
Note that the computation of ⟨θi⟩solves the coordination problem between the diﬀerent
joints. Once ⟨θi⟩is known, each θi is steered independently to its target value ⟨θi⟩using
the control law (17.40). The computation of ⟨θi⟩in the variational approximation is very
eﬃcient and can be used to control arms with hundreds of joints.
17.8
Discussion
We have given a basic introduction to some notions in optimal deterministic and stochastic
control theory and have discussed recent work on the path integral methods for stochastic
optimal control. We would like to mention a few additional issues.
One can extend the path integral control formalism to multiple agents that jointly solve
a task. In this case the agents need to coordinate their actions not only through time, but also
among each other to maximise a common reward function. The approach is very similar
to the n-joint problem that we studied in the last section. The problem can be mapped to
a graphical model inference problem and the solution can be computed exactly using the
junction tree algorithm [37, 38] or approximately [35, 34].
There is a relation between the path integral approach discussed and the linear control
formulation proposed in [32]. In that work the discrete space and time case is considered
and it is shown, that if the immediate cost can be written as a KL divergence between the
controlled dynamics and a passive dynamics, the Bellman equation becomes linear in a
very similar way as we derived for the continuous case in Eq. (17.27). In [33] it was fur-
ther observed that the linear Bellman equation can be interpreted as a backward message
passing equation in a HMM. In [18] we have taken this analogy one step further. When
the immediate cost is a KL divergence between transition probabilities for the controlled
and passive dynamics, the total control cost is also a KL divergence between probability
distributions describing controlled trajectories and passive trajectories. Therefore, the opti-
mal control solution can be directly inferred as a Gibbs distribution. The optimal control
computation reduces to the probabilistic inference of a marginal distribution on the ﬁrst
and second time slice. This problem can be solved using eﬃcient approximate inference
methods. We also show how the path integral control problem is obtained as a special case
of this KL control formulation.
The path integral approach has recently been applied to the control of character anima-
tion [4]. In this work the linearity of the Bellman equation (17.27) and its solution (17.29) is
exploited by noting that if ψ1 and ψ2 are solutions for end costs φ1 and φ2, then ψ1 + ψ2 is a
solution to the control problem with end cost −λ log  exp(−φ1/λ) + exp(−φ2/λ). Thus, by
computing the control solution to a limited number of archetypal tasks, one can eﬃciently
obtain solutions for arbitrary combinations of these tasks. In robotics, [30, 28, 29] have
shown that the path integral method has great potential for application in robotics. They
have compared the path integral method with some state-of-the-art reinforcement learning

386
Hilbert J. Kappen
methods, showing very signiﬁcant improvements. In addition, they have successfully
implemented the path integral control method to a walking robot dog.
Acknowledgments
This work is supported in part by the Dutch Technology Founda-
tion and the BSIK/ICIS project.
Bibliography
[1] C. Archambeau, M. Opper, Y. Shen, D. Cornford
and J. Shawe-Taylor. Variational inference for
diﬀusion processes. In D. Koller and Y. Singer,
editors, Advances in Neural Information
Processing Systems 19. MIT Press, 2008.
[2] R. Bellman and R. Kalaba. Selected Papers on
Mathematical Trends in Control Theory. Dover,
1964.
[3] D. P. Bertsekas. Dynamic Programming and
Optimal Control. Second edition Athena
Scientiﬁc, 2000.
[4] M. da Silva, F. Durand and J. Popovi´c. Linear
Bellman combination for control of character
animation. In SIGGRAPH ’09: ACM
SIGGRAPH 2009 papers, pages 1–10, New
York, 2009. ACM.
[5] R. Dearden, N. Friedman and D. Andre. Model
based bayesian exploration. In Proceedings of
the Fifteenth Conference on Uncertainty in
Artiﬁcial Intelligence, pages 150–159, 1999.
[6] A. A. Feldbaum. Dual control theory. I–IV.
Automation remote control, 21–22:874–880,
1033–1039, 1–12, 109–121, 1960.
[7] N. M. Filatov and H. Unbehauen. Adaptive Dual
Control. Springer-Verlag, 2004.
[8] W. H. Fleming. Exit probabilities and optimal
stochastic control. Applied Mathematics and
Optimization, 4:329–346, 1978.
[9] W. H. Fleming and H. M. Soner. Controlled
Markov Processes and Viscosity solutions.
Springer-Verlag, 1992.
[10] J. J. Florentin. Optimal, probing, adaptive control
of a simple Bayesian system. International
Journal of Electronics, 13:165–177, 1962.
[11] G. Fraser-Andrews. A multiple-shooting
technique for optimal control. Journal of
Optimization Theory and Applications,
102:299–313, 1999.
[12] H. Goldstein. Classical Mechanics. Addison
Wesley, 1980.
[13] M. T. Heath. Scientiﬁc Computing: An
Introductory Survey. McGraw-Hill, 2002. 2nd
edition.
[14] U. J¨onsson, C. Trygger and P. ¨Ogren. Lectures
on optimal control. Unpublished, 2002.
[15] L. P. Kaelbling, M. L. Littman and A. R.
Cassandra. Planning and acting in partially
observable stochastic domains. Artiﬁcial
Intelligence, 101:99–134, 1998.
[16] H. J. Kappen. A linear theory for control of
non-linear stochastic systems. Physical Review
Letters, 95:200201, 2005.
[17] H. J. Kappen. Path integrals and symmetry
breaking for optimal control theory. Journal of
Statistical Mechanics: Theory and Experiment,
page P11011, 2005.
[18] H. J. Kappen, V. G´omez and M. Opper. Optimal
control as a graphical model inference problem.
http://arxiv.org/abs/0901.0633.
[19] H. J. Kappen and S. Tonk. Optimal exploration
as a symmetry breaking phenomenon. Technical
report, 2010.
[20] M. Kearns and S. Singh. Near-optimal
reinforcement learning in polynomial time.
Machine Learning, pages 209–232, 2002.
[21] P. R. Kumar. Optimal adaptive control of
linear-quadratic-Gaussian systems. SIAM
Journal on Control and Optimization,
21(2):163–178, 1983.
[22] T. Mensink, J. Verbeek and H. J. Kappen. EP for
Eﬃcient Stochastic Control with Obstacles. In
ECAI, pages 1–6 2010.
[23] L. S. Pontryagin, V. G. Boltyanskii, R. V.
Gamkrelidze and E. F. Mishchenko. The
Mathematical Theory of Optimal Processes.
Interscience, 1962.
[24] P. Poupart and N. Vlassis. Model-based
Bayesian reinforcement learning in partially
observable domains. In Proceedings
International Symposium on Artiﬁcial
Intelligence and Mathematics (ISAIM), 2008.
[25] E. J. Sondik. The optimal control of partially
observable Markov processes. PhD thesis,
Stanford University, 1971.
[26] R. Stengel. Optimal Control and Estimation.
Dover publications, 1993.
[27] H. Theil. A note on certainty equivalence in
dynamic planning. Econometrica, 25:346–349,
1957.
[28] E. Theodorou, J. Buchli and S. Schaal. Learning
policy improvements with path integrals. In
International Conference on Artiﬁcial
Intelligence and Statistics, 2010.
[29] E. Theodorou, J. Buchli and S. Schaal.
Reinforcement learning of motor skills in high
dimensions: a path integral approach. In
International Conference of Robotics and
Automation, 2010.

Optimal control theory and the linear Bellman equation
387
[30] E. A. Theodorou, J. Buchli and S. Schaal. Path
integral-based stochastic optimal control for
rigid body dynamics. In Adaptive Dynamic
Programming and Reinforcement Learning,
2009. ADPRL ’09. ieee symposium on, pages
219–225, 2009.
[31] S. B. Thrun. The role of exploration in learning
control. In D. A. White and D. A. Sofge, editors,
Handbook of Intelligent Control. Multiscience
Press, 1992.
[32] E. Todorov. Linearly-solvable Markov decision
problems. In B. Sch¨olkopf, J. Platt, and
T. Hoﬀman, editors, Advances in Neural
Information Processing Systems 19, pages
1369–1376. MIT Press, 2007.
[33] E. Todorov. General duality between optimal
control and estimation. In 47th IEEE Conference
on Decision and Control, pages 4286–4292,
2008.
[34] B. van den Broek, W. Wiegerinck, and H. J.
Kappen. Graphical model inference in optimal
control of stochastic multi-agent systems.
Journal of AI Research, 32:95–122, 2008.
[35] B. van den Broek, W. Wiegerinck and H. J.
Kappen. Optimal control in large stochastic
multi-agent systems. In Adaptive Agents and
Multi-Agent Systems III. Adaptation and
Multi-Agent Learning, volume 4865/2008, pages
15–26. Springer, 2008.
[36] R. Weber. Lecture notes on optimization and
control. Lecture notes of a course given autumn
2006, 2006.
[37] W. Wiegerinck, B. van den Broek and H. J.
Kappen. Stochastic optimal control in
continuous space-time multi-agent systems. In
Uncertainty in Artiﬁcial Intelligence.
Proceedings of the 22th conference, pages
528–535. Association for UAI, 2006.
[38] W. Wiegerinck, B. van den Broek and H. J.
Kappen. Optimal on-line scheduling in
stochastic multi-agent systems in continuous
space and time. In Proceedings AAMAS, page 8,
2007.
[39] J. Yong and X.Y. Zhou. Stochastic Controls.
Hamiltonian Systems and HJB Equations.
Springer, 1999.
Contributor
Hilbert J. Kappen, Donders’ Institute for Neuroscience, Radboud University, 6525 EZ Nijmegen, The
Netherlands

18
Expectation maximisation methods for solving (PO)MDPs
and optimal control problems
Marc Toussaint, Amos Storkey and Stefan Harmeling
18.1
Introduction
As this book demonstrates, the development of eﬃcient probabilistic inference techniques
has made considerable progress in recent years, in particular with respect to exploiting
the structure (e.g., factored, hierarchical or relational) of discrete and continuous problem
domains. In this chapter we show that these techniques can be used also for solving Markov
decision processes (MDPs) or partially observable MDPs (POMDPs) when formulated in
terms of a structured dynamic Bayesian network (DBN).
The problems of planning in stochastic environments and inference in state space mod-
els are closely related, in particular in view of the challenges both of them face: scaling to
large state spaces spanned by multiple state variables, or realising planning (or inference)
in continuous or mixed continuous-discrete state spaces. Both ﬁelds developed techniques
to address these problems. For instance, in the ﬁeld of planning, they include work on
factored Markov decision processes [5, 17, 9, 18], abstractions [10], and relational mod-
els of the environment [37]. On the other hand, recent advances in inference techniques
show how structure can be exploited both for exact inference as well as for making
eﬃcient approximations. Examples are message-passing algorithms (loopy belief propaga-
tion, expectation propagation), variational approaches, approximate belief representations
(particles, assumed density ﬁltering, Boyen–Koller) and arithmetic compilation (see, e.g.,
[22, 23, 7]).
In view of these similarities one may ask whether existing techniques for probabilis-
tic inference can directly be translated to solving stochastic planning problems. From a
complexity theoretic point of view, the equivalence between inference and planning is well
known (see, e.g., [19]). Inference methods have been applied before to optimal decision
making in inﬂuence diagrams [8, 25, 29]. However, contrary to MDPs, these methods
focus on a ﬁnite number of decisions and a non-stationary policy, where optimal deci-
sions are found by recursing backward starting from the last decision (see [4] and [31]
for a discussion of MDPs versus inﬂuence diagrams). More recently, [6] have used infer-
ence on abstract hidden Markov models for policy recognition, i.e., for reasoning about
executed behaviours, but do not address the problem of computing optimal policies from
such inference. Reference [2] proposed a framework which suggests a straightforward way
to translate the problem of planning to a problem of inference: A Markovian state-action
model is assumed, which is conditioned on a start state s0 and a goal state sT. Here, how-
ever, the total time T has to be ﬁxed ad hoc and the MAP action sequence that is proposed as

Expectation maximisation for (PO)MDPs and control
389
a solution is not optimal in the sense of maximising an expected future reward. The authors
in [28] introduced the same idea independently in the context of continuous state stochastic
control and called this optimistic inference control. The authors in [35] used inference to
compute plans (considering the maximal probable explanation (MPE) instead of the MAP
action sequence) but again the total time has to be ﬁxed and the plan is not optimal in the
expected return sense.
We provide a framework that translates the problem of maximising the discounted
expected future return in the inﬁnite-horizon MDP (or general DBN) into a problem of
likelihood maximisation in a related mixture of ﬁnite-time MDPs. This allows us to use
expectation maximisation (EM) for computing optimal policies, utilising arbitrary infer-
ence techniques in the E-step. We can show that this optimises the discounted expected
future return for arbitrary reward functions and without assuming an ad hoc ﬁnite total
time. The approach is generally applicable on any DBN-description of the problem when-
ever we have eﬃcient inference techniques for this structure. Dynamic Bayesian networks
allow us to consider structured representations of the environment (the world state) as well
as the agent (or multiple agents, in fact).
The next section introduces our likelihood maximisation approach for solving Markov
decision processes. This will involve the introduction of a mixture of variable length DBNs
for which we can show equivalence between likelihood maximisation and maximisation
of expected future return. Section 18.3 in detail derives an EM algorithm. Here, the key
is eﬃcient inference algorithms that handle the mixture of variable length processes. The
derived algorithms are applicable on arbitrary structured DBNs. In Section 18.4 we recon-
sider the basic MDP case, explain the relation of the EM algorithm to policy and value
iteration, and demonstrate the approach using exact inference on a discrete maze and
Gaussian belief state propagation in non-linear stochastic optimal control problems.
In Section 18.5 we consider a non-trivial DBN representation of a POMDP prob-
lem. We propose a certain model of an agent (similar to ﬁnite state controllers, FSCs)
that uses an internal memory variable as a suﬃcient representation of history which gates
a reactive policy. We use this to learn suﬃcient memory representations (e.g., counting
aisles) and primitive reactive behaviours (e.g., aisle or wall following) in some par-
tially observable maze problems. This can be seen by analogy to the classical machine
learning paradigm of learning latent variable models of data for bootstrapping interest-
ing internal representations (e.g., independent components analysis) – but here gener-
alised to the learning of latent variable models of successful behaviour. Section 18.6
will conclude this chapter and discuss existing follow-up work and future directions of
research.
18.2
Markov decision processes and likelihood maximisation
A Markov decision process ([15]) is a stochastic process on the random variables of state
st, action at, and reward rt, as deﬁned by the
initial state distribution
P(s0 = s),
transition probability
P(st+1 = s′ | at =a, st = s),
reward probability
P(rt =r | at =a, st = s),
policy
P(at =a | st = s; π) =: πas.
We assume the process to be stationary (none of these quantities explicitly depends on
time) and call the expectation R(a, s) = E{r | a, s} = P
r r P(r | a, s) the reward function. In

390
Marc Toussaint, Amos Storkey and Stefan Harmeling
a0
s0
r0
a1
s1
r1
a2
s2
r2
Figure 18.1 Dynamic Bayesian network for an MDP. The x states denote the state variables, a the actions and r
the rewards.
model-based reinforcement learning the transition and reward probabilities are estimated
from experience (see, e.g., [1]). In Section 18.6.1 we discuss follow-up work that extends
our framework to the model-free case. The random variables st and at can be discrete
or continuous whereas the reward rt is a real number. Figure 18.1 displays the dynamic
Bayesian network for an inﬁnite-horizon Markov decision process.
The free parameter of this DBN is the policy π with numbers πas ∈[0, 1] normalised
w.r.t. a. The problem we address is solving the MDP:
Deﬁnition 18.2.1. Solving an MDP means to ﬁnd a parameter π of the inﬁnite-horizon
DBN in Fig. 18.1 that maximises the expected future return Vπ = E{P∞
t=0 γt rt; π}, where
γ ∈[0, 1) is a discount factor.
The classical approach to solving MDPs is anchored in Bellman’s equation, which
simply reﬂects the recursive property of the future discounted return
∞
X
t=0
γt rt = r0 + γ
h ∞
X
t=0
γt rt+1
i
,
and consequently of its expectation conditioned on the current state,
Vπ(s) = E{
∞
X
t=0
γt rt
 s0 = s; π} =
X
s′,a
P(s′|a, s) πas [R(a, s) + γ Vπ(s′)].
Standard algorithms for computing value functions can be viewed as iterative schemes that
converge towards the Bellman equation or as directly solving this linear equation w.r.t. V
by matrix inversion.
In contrast, our general approach is to translate the problem of solving an MDP into a
problem of likelihood maximisation. There are diﬀerent approaches for such a translation.
One issue to be considered is that the quantity we want to maximise (the expected future
return) is a sum of expectations in every time slice, whereas the likelihood in Markovian
models is the product of observation likelihoods in each time slice. A ﬁrst idea for achieving
equivalence is to introduce exponentiated rewards as observation likelihoods – but that turns
out non-equivalent (see Remark (iii) in Appendix 18.A).
The authors in [34, 33] introduced an alternative based on a mixture of ﬁnite-length
processes. Intuitively, the key argument for this approach is perhaps the question Where do
I start the backward sweep? In all EM approaches we need to compute a posterior over
trajectories in the E-step so that we can update the policy in the M-step. In ﬁnite-length
Markov processes, such inference can eﬃciently be done by a forward-backward sweep
(Baum–Welch). If the process is inﬁnite it is unclear where in the future to start (anchor)
the backward sweep. However, when introducing a binary reward event there is a very

Expectation maximisation for (PO)MDPs and control
391
MIXTURE of finite−time MDPs
aT
xT
R
time prior:
P(T)
T = 0
T = 2
T = 1
a0
a0
a1
a2
a1
a0
a0
a1
a2
R
R
R
s0
s0
s1
s1
s2
s0
s0
s1
s2
Figure 18.2 Mixture of ﬁnite-time MDPs.
intuitive solution to this: Let us simply declare that the reward event occurs at some time
T in the future, without knowing what T is, and we start the backward sweep from that
time T backward. In that way we can start computing backward and forward messages in
parallel without having to estimate some horizon ad hoc, and decide when to stop if there
is suﬃcient overlap between the forward and backward propagated messages [34]. When
we choose a geometric prior P(T) = (1 −γ)γt this turns out to implement the discounting
correctly.
The mixture model is in some respects diﬀerent to the original MDP but the likelihood
of ‘observing reward’ in this mixture model is proportional to the expected future return in
the original MDP. The reasons for this choice of approach are related to inference (perform-
ing a backward pass) without pre-ﬁxing a ﬁnite time horizon T, the handling of discounting
rewards, and also to the resulting relations to standard policy and value iteration, as it will
later become more clear.
We deﬁne the mixture model as follows. Let ξ = (s0:T, a0:T) denote a state-action
sequence of length T, and let R be a random event (binary variable) with P(R | a, s)
proportional to the reward function R(a, s), for instance as in [8],
P(R | a, s) = R(a, s) −min(R)
max(R) −min(R).
Each ﬁnite-time MDP deﬁnes the joint
P(R, ξ | T; π) = P(R | aT, sT) P(aT | sT; π)
h T−1
Y
t=0
P(st+1 | at, st) P(at | st; π)
i
P(s0).
(18.1)
That is, each ﬁnite-time MDP has the same initialisation, transition and reward probabilities
as the original MDP but (i) it ends at a ﬁnite time T and (ii) it emits a single binary reward
R only at the ﬁnal time step.
Now let T be a random variable with prior P(T). The mixture of ﬁnite-time MDPs is
given by the joint
P(R, ξ, T; π) = P(R, ξ | T; π) P(T).
Note that this deﬁnes a distribution over the random variable ξ in the space of variable
length trajectories. Figure 18.2 illustrates the mixture. We ﬁnd

392
Marc Toussaint, Amos Storkey and Stefan Harmeling
r1
r2
r0
a1
a0
a2
a1
a0
a0
V = P
t γt E{rt}
P(r0:2, ξ)
P(R, ξ) = P2
T=0 P(T) P(R, ξ | T)
R
R
R
s1
s0
s0
s1
s2
s0
L = P
T P(T) P(R | T)
a2
a1
a0
s0
s1
s2
Figure 18.3 Illustration of the equivalence between the mixture and the original MDP.
Theorem 18.2.1. When introducing binary rewards R such that P(R | a, s) ∝R(a, s) and
choosing the geometric time prior P(T) = γT(1 −γ), maximising the likelihood
L(π) = P(R; π)
of observing reward in the mixture of ﬁnite-time MDPs is equivalent to solving the original
MDP.
Given the way we deﬁned the mixture, the proof is straightforward and illustrated in
Fig. 18.3.
Proof. Let H be some horizon for which we later take the limit to ∞. We can rewrite the
value function of the original MDP as
Vπ =
X
a0:H,s0:H
h
P(s0) π(a0 | s0)
H
Y
t=1
π(at | st) P(st | at-1, st-1)
ih
H
X
T=0
γTR(aT, sT)
i
=
H
X
T=0
γT
X
a0:H,s0:H
R(aT, sT)P(s0) π(a0 | s0)
H
Y
t=1
π(at | st) P(st | at-1, st-1)
=
H
X
T=0
γT
X
a0:T ,s0:T
R(aT, sT)P(s0) π(a0 | s0)
T
Y
t=1
π(at | st) P(st | at-1, st-1)
=
H
X
T=0
γT Ea0:T ,s0:T | π{R(aT, sT)}.
In the second line we pulled the summation over T to the front. Note that the second and
third line are really diﬀerent: the product is taken to the limit T instead of H since we
eliminated the variables aT+1:H, sT+1:H with the summation. The last expression has already
the form of a mixture model, where T is the mixture variable, γT is the mixture weight
(P(T) = γT(1−γ) the normalized geometric prior), and the last term is the expected reward
in the ﬁnal time slice of a ﬁnite-time MDP of length T (since the expectation is taken over
a0:T, s0:T | π).
The likelihood in our mixture model can be written as
L(π) = P(R; π)
= (1 −γ)
∞
X
T=0
γT P(R | T; π)
∝(1 −γ) Ea0:T ,s0:T | π{R(aT, sT)}
= (1 −γ) Vπ.
The proportionality stems from the deﬁnition P(R | at, st) ∝R(aT, sT) of R.
□

Expectation maximisation for (PO)MDPs and control
393
In Appendix 18.A we remark on the following points in some more detail:
(i) the interpretation of the mixture with death probabilities of the agent,
(ii) the diﬀerence between the models w.r.t. the correlation between rewards,
(iii) approaches to consider exponentiated rewards as observation likelihoods.
18.3
Expectation maximisation in mixtures of variable length dynamic
Bayesian networks
The algorithms we will derive in this section are independent from the context of MDPs.
We investigate the general case of a variable length stationary Markov process where we
have observations only at the start and the ﬁnal time. The length of the process is unknown
and we assume a mixture of variable length processes where the length prior is the geo-
metric distribution P(T) = γT(1 −γ) for γ ∈[0, 1). We ﬁrst derive EM algorithms for an
unstructured Markov process on one random variable which is then easily generalised to
structured DBNs.
18.3.1
Single variable case
Let ξ be a random variable in the domain X∗= S∞
T=0 XT+1, that is, a variable length trajec-
tory in X (in other terms, a ﬁnite but arbitrary length string over the alphabet X). We use
the notation ξ = (x0, . . . , xT) for a length-T trajectory and write |ξ| = T. We consider T a
random variable and assume an auxiliary binary random variable R depending on the ﬁnal
state xT which represents some generic observation. Speciﬁcally, we consider the joint
P(R, ξ, T; θ) = P(R | ξ; θ) P(ξ | T; θ) P(T)
= P(R | xT; θ)
h T-1
Y
t=0
P(xt+1 | xt; θ)
i
P(x0; θ) δ|ξ|T P(T).
Here, δ|ξ|T is one for |ξ| = T and zero otherwise; P(T) is a prior over the length of the
process; P(x0; θ) is the start distribution of the process; P(xt+1 | xt; θ) is the transition prob-
ability; and P(R | xT; θ) is the probability of the R-event which depends only on the ﬁnal
state. The joint is normalised since, when summing over all ξ ∈X∗with all lengths L = |ξ|,
X
ξ∈X∗
P(ξ | T; θ) =
∞
X
L=0
X
x0,..,xL
h L-1
Y
t=0
P(xt+1 | xt; θ)
i
P(x0; θ) δLT
=
X
x0,..,xT
h T-1
Y
t=0
P(xt+1 | xt; θ)
i
P(x0; θ) = 1.
For expectation maximisation (EM) we assume R is observed and we want to ﬁnd
parameters θ of the joint that maximise the likelihood P(R; θ) = P
T,ξ P(R, ξ, T; θ). The
length T of the process and the whole trajectory ξ itself are latent (non-observed) variables.
Let q(ξ, T) be a distribution over the latent variables. Consider
F(θ, q) := log P(R; θ) −D(q(ξ, T)
 P(ξ, T | R; θ))
(18.2)

394
Marc Toussaint, Amos Storkey and Stefan Harmeling
= log P(R; θ) −
X
ξ,T
q(ξ, T) log
q(ξ, T)
P(ξ, T | R; θ)
=
X
ξ,T
q(ξ, T) log P(R; θ) +
X
ξ,T
q(ξ, T) log P(ξ, T | R; θ) + H(q)
=
X
ξ,T
q(ξ, T) log P(R, ξ, T; θ) + H(q).
(18.3)
Expectation maximisation will start with an initial guess of θ, then iterate ﬁnding a q that
maximises F for ﬁxed θ in the form of Eq. (18.2), and then ﬁnding a new θ that maximises
F for ﬁxed q in the form of Eq. (18.3). Let us ﬁrst address the M-step. This will clarify
which quantities we actually need to compute in the E-step.
The M-step computes arg maxθ of
F(θ, q) =
X
ξ,T
q(ξ, T) log P(R, ξ, T; θ) + H(q)
=
X
ξ,T
q(ξ, T)
h
log P(R | xT; θ) + log P(ξ | T; θ) + log P(T)
i
+ H(q)
=
X
ξ,T
q(ξ, T)
h
log P(R | xT; θ) + log P(ξ | T; θ)
i
+ terms indep of θ
=
X
ξ
∞
X
T=0
q(ξ, T)
h
log P(R | xT; θ) +
T-1
X
t=0
log P(xt+1 | xt; θ)
i
=
X
ξ
∞
X
T=0
q(ξ, T) log P(R | xT; θ) +
X
ξ
∞
X
t=0
∞
X
T=t+1
q(ξ, T) log P(xt+1 | xt; θ)
=
X
x
h ∞
X
T=0
q(xT = x, T)
i
log P(R | xT = x; θ)
+
X
x′,x
h ∞
X
t=0
∞
X
T=t+1
q(xt+1 = x′, xt = x, T)
i
log P(xt+1 = x′ | xt = x; θ).
(18.4)
The last line uses the fact that the process and the reward are stationary (i.e., P(xt+1 = x′ | xt =
x; θ) does not explicitly depend on t and P(R | xT = x; θ) does not depend explicitly on T).
The last equation clariﬁes that the E-step actually only needs to return the quantities in the
brackets. The exact E-step is
q∗(ξ, T) = P(ξ, T | R; θold) = P(R | ξ, T; θold) P(ξ | T; θold) P(T)
P(R; θold)
.
Let us investigate the bracket terms in Eq. (18.4) for the exact E-step in more detail:
∞
X
T=0
q∗(xT, T) =
1
P(R; θold) P(R | xT; θold)
∞
X
T=0
P(xT; θold) P(T)
∞
X
t=0
∞
X
T=t+1
q∗(xt+1, xt, T) =
1
P(R; θ)
∞
X
t=0
∞
X
T=t+1
P(R | xt+1, T; θold)
× P(xt+1 | xt; θold) P(xt; θold) P(T).

Expectation maximisation for (PO)MDPs and control
395
At this point we use the property of the geometric length prior
P(T =t + τ) =
1
1 −γ P(T =t) P(T = τ)
∞
X
t=0
∞
X
T=t+1
q∗(xt+1, xt, T) =
1
P(R; θ)(1 −γ)
∞
X
t=0
∞
X
τ=1
P(R | xt+1, T =t + τ; θold)
× P(T =τ) P(xt+1 | xt; θold) P(xt; θold) P(T =t)
=
1
P(R; θ)(1 −γ)
h ∞
X
τ=1
P(R | xt+1, T =t + τ; θold) P(T =τ)
i
× P(xt+1 | xt; θold)
h ∞
X
t=0
P(xt; θold) P(T =t)
i
.
In the last line we used that P(R | xt+1, T = t + τ; θold) does not explicitly depend on t but
only on the time-to-go τ. We ﬁnally deﬁne quantities
α(xt) :=
∞
X
t=0
P(xt; θold) P(T =t)
(18.5)
β(xt+1) :=
1
1 −γ
∞
X
τ=1
P(R | xt+1, T =t + τ; θold) P(T =τ)
=
1
1 −γ
∞
X
τ=0
P(R | x′
t, T =t + τ; θold) P(T =τ + 1)
(18.6)
such that the relevant parts of F(θ, q∗) for the M-step can be written more compactly.
Eq. (18.4) now reads
F(θ, q∗) =
X
x
h
P(R | xT; θold) α(x)
i
log P(R | xT; θ)
+
X
x′,x
h
β(x′) P(x′ | x; θold) α(x)
i
log P(xt+1 | xt; θ).
(18.7)
Note, α(x) and β(x) are just quantities that are deﬁned in Eqs. (18.5) and (18.6) and useful
for the EM algorithm, but have no explicit interpretation yet. They are analogous to the
typical forward and backward messages in Baum–Welch, but diﬀerent in that there are no
observations and that they incorporate the whole mixture over variable length processes
and exploit the geometric length prior. If one likes, α can be interpreted as the last-state-
occupancy-probability averaged over the mixture of all length processes; and β can be
interpreted as (is proportional to) the probability of observing R in the future (not imme-
diately) averaged over the mixture of all length processes. The explicit M-step depends
on the kind of parameterisation of P(R | xT; θ) and P(xt+1 | xt; θ) but is straightforward to
derive from Eq. (18.7). We will derive explicit M-steps in the (PO)MDP context in the next
section.
18.3.2
Explicit E-step algorithms
In the remainder of this section we describe eﬃcient algorithms to compute α(x) and β(x)
as deﬁned in Eqs. (18.5) and (18.6). For brevity we write P ≡P(x′ | x; θold) as a matrix

396
Marc Toussaint, Amos Storkey and Stefan Harmeling
and α ≡α(x) and S ≡P(x0 = x) as vectors. The α quantity can be computed iteratively in
two ways: We deﬁne another quantity at (which directly corresponds to the typical forward
message) and from Eq. (18.5) get
at := P(xt = x) = P at-1 ,
a0 = S,
(18.8)
αh =
h
X
t=0
at P(T =t) = αh-1 + (1 −γ)γh ah ,
α0 = (1 −γ) S.
Iterating both equations together can be used to compute α approximately as αh in the limit
h →∞. Alternatively we can use
αh =
h
X
t=0
at P(T =t) = (1 −γ)
h
a0 +
h
X
t=1
at γti
= (1 −γ)
h
S +
h
X
t=1
P at-1 γti
= (1 −γ)
h
S + P
h-1
X
t=0
at γt+1i
= (1 −γ) S + γ P αh-1
(18.9)
as a direct recursive equation for αh. Analogously we have two ways to compute β (a row
vector). With R ≡P(R | xT = x) the direct computation of Eq. (18.6) is
bτ := P(R | xt = x, T =t + τ) = bτ-1 P ,
b0 = R,
(18.10)
βh =
1
1 −γ
h
X
τ=0
bτ P(T =τ + 1) = βh-1 + γh+1 bh(x) ,
β0 = γ b0.
And a second way to compute β is
βh =
1
1 −γ
h
X
τ=0
bτ P(T =τ + 1) = γb0 +
h
X
τ=1
bτ γτ+1
= γR +
h
X
τ=1
bτ-1 P γτ+1 = γR + γ
h h-1
X
τ=0
bτ γτ+1i
P
= γR + γ βh-1 P
(18.11)
in the limit h →∞. Note that, in the context of MDPs, this equation is exactly equivalent
to policy evaluation, i.e., the computation of the value function for a ﬁxed policy. We will
discuss this in more detail in Section 18.4.2.
When choosing to compute also the at and bt quantities we can use them to compute
the length posterior and likelihood,
P(R | T =t + τ) =
X
x
P(R | xt = x, T =t + τ) P(xt = x) = b⊤
τ at,
P(R) =
X
T
P(R | T) P(T),
(18.12)
P(T | R) = P(R | T) P(T)/P(R),
(18.13)
E{T | R} =
X
T
T P(T | R).

Expectation maximisation for (PO)MDPs and control
397
In particular, Eqs. (18.12) and (18.13) can be computed while iterating Eqs. (18.8) and
(18.10) and thereby provide a heuristic to choose the horizon (stopping criterion of the
iteration) on the ﬂy. On the other hand Eqs. (18.9) and (18.11) are update equations for α
and β which can be used in an incremental E-step: we can reuse the α and β of the previous
EM-iterations as an initialisation and iterate Eqs. (18.9) and (18.11) only a few steps. This
corresponds to computing parts of α and β with old parameters θold from previous EM-
iterations and only the most recent updates with the current parameters. The horizon h
implicitly increases in each EM-iteration. Algorithms 18.1 and 18.2 explicitly describe the
standard E-step and the incremental version.
Algorithm 18.1 Standard E-step
Input: vectors S, R, matrix P, scalars γ, H
Output: vectors α, β, P(T|R), scalars P(R), E{T|R}
1: initialise a = S, b = R, α = a, β = γ b, L(0) = a⊤b
2: for h = 1 to H do
3:
a ←P a
4:
L(2h −1) = γ2h−1 a⊤b
5:
b ←b P
6:
L(2h) = γ2h a⊤b
7:
α += γh a
8:
β += γh+1 b
9: end for
10: L *= 1 −γ
11: α *= 1 −γ
12: P(R) = P2H
t=0 L(t)
13: P(T =t|R) = L(t)/P(R)
14: E{T|R} = [P2H
t=0 tL(t)]/P(R)
Algorithm 18.2 Incremental E-step
Input: vectors α, β, S, R, matrix P, scalars γ, H
Output: vector α, β, scalar P(R)
1: for h = 1 to H do
2:
α ←(1 −γ) S + γ P α
3:
β ←γ [R + β P]
4: end for
5: P(R) = a⊤R
18.3.3
Structured DBN case
In the case of a structured DBN the process is deﬁned on more than one variable. Generally
the transition probability P(xt+1 | xt; θ) is then replaced by a set of factors (or conditional
probability tables) that describes the coupling between two consecutive time slices. It is
straightforward to generalise Algorithms 18.1 and 18.2 to exploit the structure in such a
DBN: In each time slice we now have several random variables s1
t , . . . , sk
t , a1
t , . . . , al
t. We
choose the notation s1
t , . . . , sk
t for a set of variables which are a separator of the Markov pro-
cess and a1
t , . . . , al
t are the remaining variables in a time slice. For exact inference we have

398
Marc Toussaint, Amos Storkey and Stefan Harmeling
to maintain quantities α(s) and β(s) over the separator clique. To exploit the DBN structure
we need to replace the transition matrix multiplications (lines 3 and 5 in Algorithm 18.1,
and lines 2 and 3 in Algorithm 18.2) with other inference techniques. The simplest solu-
tion is to use the elimination algorithm. For instance, instead of the matrix multiplication
a ←Pa (line 3 of Algorithm 18.1), we think of P as a list of factors over the variables
(st, at, st+1) that couple two time slices, a is a factor over the ‘left’ separator clique (st), we
pass the list of factors {P, a} to the elimination algorithm and query for the marginal over the
‘right’ separator clique (st+1). This yields the new assignment to a. When we choose a good
elimination order this procedure is equivalent to the Junction Tree method described in [23].
Concerning the M-step, the energy expression (18.7) generalises to
F(θ, q∗) =
X
a,s
h
P(R | a, s; θold) P(a | s; θold) α(s)
i
log P(R | a, s; θ) P(a | s; θ)
+
X
s′,a,s
h
β(s′) P(s′ | a, s; θold) P(a | s; θold) α(s)
i
log P(s′ | a, s; θ) P(a | s; θ).
(18.14)
18.4
Application to MDPs
18.4.1
Expectation maximisation with a tabular policy
A standard MDP is a DBN with random variables st and at in each time slice where the
state st is a separator. In the simplest case we parameterise the policy P(at | st; θ) using a
full CPT,
P(at =a | st = s; θ) = πas.
The energy F(θ, q∗) (Eq. (18.14)) reads (neglecting terms independent of π)
X
a,s
h
P(R|a, s) πold
as α(s)
i
log πas +
X
s′,a,s
h
β(s′) P(s′|a, s) πold
as α(s)
i
log πas.
Since πas is constrained to normalise over a for each s this energy is maximised by
πnew
as
= πold
as
h
P(R|a, s) +
X
s′
β(s′) P(s′|a, s)
i
.
(18.15)
The two terms in the brackets correspond to the expected immediate reward plus the
expected future reward as predicted by β – the brackets are exactly equivalent to the
classical Q-function.
In the case of a plain unstructured MDP we can also derive a greedy and usually faster
version of the M-step, given as
∀s : πnew
as
= δ(a, a∗(s)) ,
a∗(s) = arg max
a
h
P(R|a, s) +
X
s′
β(s′) P(s′|a, s)
i
.
(18.16)
This update corresponds to a greedy version of the previous M-step. If we were to iter-
ate Eq. (18.15) without recomputing the bracket term each time (skipping intermediate
E-steps) we would converge to this greedy M-step. Further, as we know from reinforce-
ment learning, the greedy M-step can be thought of as exploiting our knowledge that the
optimal policy must be a deterministic one (in the fully observable case).
Note however that this does not generalise to arbitrarily structured DBNs. In fact, in
the POMDP case that we investigate later we are not aware of such a greedy version of the
M-step.

Expectation maximisation for (PO)MDPs and control
399
18.4.2
Relation to policy iteration and value iteration
So far we have developed our approach for a plain unstructured MDP. It turns out that in
this case the E- and M-step are very closely related to the policy evaluation and update
steps in standard policy iteration.
We introduced the mixture of MDPs in a way such that the likelihood is proportional to
the expected future return. Hence, for the unstructured MDP, b(s) := P(R | st = s, T =t+τ; π)
as deﬁned in Eq. (18.10) is proportional to the expected reward in τ time steps in the future
conditioned on the current state. This is also the value function of the ﬁnite-time MDP of
length τ. Hence, the β(s) deﬁned in Eq. (18.6) is proportional to the value function Vπ(s) of
the original MDP. Analogously, the bracket term in the M-step (Eq. (18.15)) is proportional
to the Q-function Qπ(a, s) in the original MDP.
We conclude that the E-step in an unstructured MDP is a form of policy evaluation
since it also yields the value function. However, quite diﬀerent to traditional policy evalua-
tion, the E-step also computes α’s, i.e., probabilities to visit states given the current policy,
which may be compared to previous approaches like diverse densities (in the context of
subgoal analysis [20]) or policy search by density estimation [24]. The full E-step provides
us with posteriors over actions, states and the total time. In practice we can use the α’s
in an eﬃcient heuristic for pruning computations during inference (see Section 18.4.3 and
Appendix 18.B). Further, the E-step generalises to arbitrarily structured DBNs and thereby
goes beyond standard policy evaluation, particularly when using approximate inference
techniques like message passing or approximate belief representations.
Concerning the M-step, in the unstructured MDP the greedy M-step is identical to
the policy update in policy iteration. That means that one iteration of the EM will yield
exactly the same policy update as one iteration of policy iteration (provided one does exact
inference and exact value function computation without time horizon cutoﬀs). Again, the
M-step goes beyond a standard policy update in the generalised case. This becomes partic-
ularly apparent when in structured DBNs (e.g. the POMDP case in Section 18.5) the full
posteriors computed via inference (including forward propagated messages analogous to
α’s) are necessary for the M-step.
In summary,
Lemma 18.1. The EM algorithm on an unstructured MDP using exact inference and the
greedy M-step is equivalent to policy iteration in terms of the policy updates performed.
Interestingly, this also means they are equivalent w.r.t. convergence. (Recall that policy
iteration is guaranteed to converge to the global optimum whereas EM algorithms are only
guaranteed to converge to local optima.) The computational costs of both methods may
diﬀer depending on the implementation (see below).
Finally, the incremental E-step of Algorithm 18.2 only updates the β and α functions
by propagating them for H steps. For H = 1 and when using the greedy M-step this is
equivalent to value iteration. In a structured (but fully observable) MDP, we have the same
equivalence with structured value iteration.
18.4.3
Discrete maze examples
Eﬃciency
We ﬁrst tested the EM algorithm with standard E-step and greedy M-step on a discrete
maze of size 100×100 and compared it to standard value iteration (VI) and policy iteration
(PI). Walls of the maze are considered to be trap states (leading to unsuccessful trials) and

400
Marc Toussaint, Amos Storkey and Stefan Harmeling
(a)
(b)
k
k
k
k
Figure 18.4 (a) State visiting probability calculated by EM for some start and goal state. The radii of the dots are
proportional to P(s ∈ξ | R). (b) The probability of reaching the goal (for EM) and the value calculated for the start
state (PS) against the cost of the planning algorithms (measured by evaluations of P(s′|a, s)).
actions (north, south, east, west, stay) are highly noisy in that with a probability of 0.2
they lead to random transitions. In the experiment we chose a uniform time prior (discount
factor γ = 1), initialised π uniformly, and iterated the policy update k = 5 times. To increase
computational eﬃciency we exploited that the algorithm explicitly calculates posteriors
which can be used to prune unnecessary computations during inference as explained in
Appendix 18.B. For policy evaluation in PI we performed 100 iterations of standard value
function updates.
Figure 18.4(a) displays the posterior state visiting probabilities P(s ∈ξ | R) of the opti-
mal policy computed by the EM for a problem where a reward of 1 is given when the
goal state g is reached and the agent is initialised at a start state s. Computational costs
are measured by the number of evaluations of the environment P(s′|a, s) needed during the
planning procedure. Figure 18.4(b) displays the probability of reaching the goal P(R; π)
against these costs. Note that for EM (and PI) we can give this information only after a
complete E- and M-step cycle (policy evaluation and update) which are the discrete dots
(triangles) in the graph. The graph also displays the curve for VI, where the currently calcu-
lated value VA of the start state (which converges to P(R) for the optimal policy) is plotted
against how often VI evaluated P(s′|a, s).
In contrast to VI and PI, the inference approach takes considerable advantage of know-
ing the start state in this planning scenario: the forward propagation allows for the pruning
and the early decision on cutoﬀtimes in the E-step as described in Appendix 18.B. It
should thus not surprise and not be overstated that the EM is more eﬃcient in this spe-
ciﬁc scenario. Certainly, a similar kind of forward propagation could also be introduced
for VI or PI to achieve equal eﬃciency. Nonetheless, our approach provides a principled
way of pruning by exploiting the computation of proper posteriors. The policies computed
by all three methods are equal for states which have signiﬁcantly non-zero state visiting
probabilities.
Multimodal time posteriors
The total time T plays a special role as a random variable in our mixture model. We use
another simple experiment to illustrate this special role by considering the total time pos-
teriors. Modelling walls as trap states leads to interesting trade-oﬀs between staying away
from walls in favour of security and choosing short paths. Fig. 18.5 displays a 15 × 20
maze with three possible pathways from the start (bottom left) to the goal (bottom right)

Expectation maximisation for (PO)MDPs and control
401
(a) k = 0
P(R)=1.5e-5
(b) k = 1
P(R)=0.17
(c) k = 2
P(R)=0.44
(d) k = 3
P(R)=0.48
(e) k = 4
P(R)=0.61
(f)
K
K
K
K
K
K
K
(g)
K
K
K
K
Figure 18.5 (a)–(e) State visiting probabilities for various EM-iterations k. The start and goal states are to the
bottom left and right, respectively. The radii of the dots are proportional to P(s ∈ξ | R). (f) The diﬀerent possible
pathways lead to a multimodal time posterior P(T | R). (g) The trade-oﬀbetween the expected time to goal (mean
of P(T | R)) and the probability to reach the goal. The dots corresponds to k = 1, ..., 4 (from left to right).
state. The direct pathway is a narrow aisle clinched between two walls and thus highly
risky. The next one up requires a step through a narrow doorway. The top one is secure
but longest. The ﬁve ﬁgures illustrate the state visiting probability P(s ∈ξ | R) for ran-
dom walks (k = 0) and the policies calculated by EM for k = 1, ..., 4 iterations. Also
the success probability P(R) is indicated. Figure 18.5(f) displays the corresponding time
posteriors P(T | R) for the diﬀerent k’s. Interesting is the multimodality of these time poste-
riors in that speciﬁc environment. The multimodality in some way reﬂects the topological
properties of the environment: that there exist multiple possible pathways from the start
to the goal with diﬀerent typical lengths (maxima of the time posterior) and diﬀerent suc-
cess probabilities (area (integral) of a mode of the time posterior). Already for k = 0
the multimodality exhibits that, besides the direct pathway (of typical length ≈15), there
exist alternative, longer routes which comprise signiﬁcant success probability. One way to
exploit this insight could be to choose a new time prior for the next inference iteration that
explicitly favours these longer routes. Figure 18.5(g) nicely exhibits the trade-oﬀbetween
the expected time to goal and the probability to reach the goal.
18.4.4
Stochastic optimal control
Gaussian belief state propagation
Next we want to show that the framework naturally allows us to transfer other inference
techniques to the problem of solving MDPs. We address the problem of stochastic optimal
control in the case of a continuous state and control space. A standard inference tech-
nique in continuous state spaces is to assume Gaussian belief states as representations for
a’s and b’s and propagate forward-backward and using the unscented transform to han-
dle also non-linear transition dynamics (see [23] for an overview on inference techniques
in DBNs). Note that using Gaussian belief states implies that the eﬀective value function
(Section 18.4.2) becomes a mixture of Gaussians.

402
Marc Toussaint, Amos Storkey and Stefan Harmeling
All the equations we derived remain valid when reinterpreted for the continuous
case (summations become integrations, etc.) and the exact propagations (Eqs. (18.8) and
(18.10)) are replaced by propagations of Gaussian belief states using the unscented trans-
form. In more detail, let N(x, a, A) be the normal distribution over x with mean a and
covariance A and let N(x, a, A) be the respective non-normalised Gaussian function with
N(a, a, A) = 1. As a transition model we assume
P(x′|u, x) = N(x′, φ(u, x), Q(u)) ,
Q(u) = C + µ|u|2 I,
where φ(u, x) is a non-linear function depending on the current state x and the control signal
u, C is a constant noise covariance, and we introduced a parameter µ for an additional noise
term that is squared in the control signal. With the parameterisation at(x) = N(x, at, At) and
bτ(x) = N(x, bτ, Bτ) (note that b’s always remain non-normalised Gaussian likelihoods
during propagation), forward and backward propagation read
(at, At) = UTφ(at−1, AT−1),
(bτ, Bτ) = UTφ−1(bτ−1, Bτ−1),
where UTφ(a, A) denotes the unscented transform of a mean and covariance under a non-
linear function. In brief, this transform deterministically considers 2n + 1 points (say with
standard deviation distance to the mean) representing the Gaussian. In the forward case
(the backward case) it maps each point forward using φ (backward using φ−1), associates a
covariance Q(u) (a covariance φ′−1 Q(u) φ′−1T, where φ′−1 is the local inverse linearisation
of φ at each point) with each point, and returns the Gaussian that approximates this mixture
of Gaussians. Further, for any t and τ we have
P(R | T =t + τ) = N(at, bτ, At + Bτ),
P(xt = x | R, T =t + τ) = N(x, ctτ,Ctτ),
C−1
tτ = A−1
t
+ B−1
τ ,
ctτ = Ctτ (A−1
t
at + B−1
τ bτ).
The policy and the M-step
In general, the policy is given as an arbitrary non-linear function π : x 7→u. Clearly, we
cannot store such a function in memory. However, via the M-step the policy can always
be implicitly expressed in terms of the b-quantities of the previous E-step and numerically
evaluated at speciﬁc states x. This is particularly feasible in our case because the unscented
transform used in the belief propagation (of the next E-step) only needs to evaluate the
transition function φ (and thereby π) at some states; and we have the advantage of not
needing to approximate the function π in any way. For the M-step (Eq. (18.16)) we need to
maximise the mixture of Gaussians (see Eq. (18.6))
ˆqτ(u, x) :=
h
P(R|u, x) +
Z
x′ P(x′|u, x) β(x′)
i
,
β(x′) =
1
1 −γ
∞
X
τ=0
P(T =τ + 1) N(x′, bτ−1, Bτ−1).
We use a gradient ascent. The gradient for each component of the mixture of Gaussians is:
qτ(u, x) :=
Z
x′ P(x′|u, x) N(x′, bτ−1, Bτ−1) = |2πBτ−1|1/2 N(bτ−1, φ(u, x), Bτ−1 + Q(u)),
∂uqτ(u, x) = −qτ(u, x)
h
hT
∂uφ(u, x)

−µu

tr(A−1) −hTh
i
,

Expectation maximisation for (PO)MDPs and control
403
(a)
(b)
(c)
(d)
(e)
(f)
Figure 18.6 Learned policies (left) and forward simulation (a’s) of these policies (right) for aspheric Gaussian-
shaped targets. (a)–(b) are for the case of restricted action amplitude (the walker model). (c)–(d) are for
unconstrained amplitude (the golfer model). And (e)–(f) are for the approach to a new position under phase
space dynamics.
A := Bτ−1 + Q(u) ,
h := A−1 (φ(u, x) −b).
We perform this gradient ascent whenever we query the policy at a speciﬁc state x.
Examples
Consider a simple two-dimensional problem where the start state is distributed around zero
via a0(x) = N(x, (0, 0), .01I) and the goal region is determined by P(R | x) = N(x, (1, 1),
diag(.0001, .1)). Note that this goal region around (1, 1) is heavily skewed in that rewards
depend more on the precision along the x-dimension than the y-dimension. We ﬁrst consider
a simple control law φ(u, x) = x+0.1 u and the discount factor γ = 1. When choosing µ = 0
(no control-dependent noise), the optimal control policy will try to jump directly to the goal
(1, 1). Hence we ﬁrst consider the solution when manually constraining the norm of |u| to
be small (eﬀectively following the gradient of P(r = 1 | ut = u, xt = x; π)). Figures 18.6(a)
and 18.6(b) show the learned control policy π and the forward simulation given this policy
by displaying the covariance ellipses for a0:T(x) after k = 3 iterations. What we ﬁnd is a
control policy that reduces errors in the x-dimension more strongly than in the y-dimension,
leading to the tangential approach to the goal region. This is related to studies on redundant
control or the so-called uncontrolled manifold.
Next we can investigate what the eﬀect of control-dependent noise is without a con-
straint on the amplitude of u. Figures 18.6(c) and 18.6(d) display results (after k = 3
iterations) for µ = 1 and no additional constraints on u. The process actually resembles
a golf player: the stronger the hit, the more noise. The optimal strategy is to hit fairly hard
in the beginning, hopefully coming closer to the goal, such that later a number of smaller
and more precise hits can be made. The reason for the small control signals around the goal
region is that small steps have much more accuracy and reward expectation is already fairly
large for just the x-coordinate being close to 1.

404
Marc Toussaint, Amos Storkey and Stefan Harmeling
Finally we think of x being a phase space and consider the dynamics φ(u, x) = (x1 +
0.1x2, x2 + 0.1u) where u is the one-dimensional acceleration of the velocity x2, and x1 is
a position. This time we set the start and goal to (0, 0) and (1, 0) respectively, both with
variance 0.001 and choose µ = 10. Figures 18.6(e) and 18.6(f) display the result and show
nicely how the learned control policy approaches the new position on the x-axis by ﬁrst
gaining and then reducing velocity.
18.5
Application to POMDPs
A stationary, partially observable Markov decision process (POMDP, see e.g. [14]) is given
by four time-independent probability functions,
the initial world state distribution
P(s0 = s),
the world state transitions
P(st+1 = s′ | at =a, st = s),
the observation probabilities
P(yt =y | st = s),
the reward probabilities
P(rt =r | at =a, st = s).
These functions are considered known. We assume the world states, actions and observa-
tions (st, yt, at) are discrete random variables while the reward rt is a real number.
The POMDP only describes one ‘half’ of the process to be described as a DBN – the
other half is the agent interacting with the environment. Our point of view is that the agent
could use an arbitrary ‘internal machinery’ to decide on actions. Finite state controllers are
a simple example. However, a general DBN formulation of the agent’s internal machinery
allows us to consider much more structured ways of behaviour organisation, including fac-
torised and hierarchical internal representations (see, e.g., [30, 32]). In the remainder of this
section we investigate a policy model that is slightly diﬀerent to ﬁnite state controllers but
still rather simple. However, the approach is generally applicable to any DBN formulation
of the POMDP and the agent.
To solve a given POMDP challenge an agent needs to maintain some internal mem-
ory variable (if not the full belief state) that represents information gained from previous
observations and actions. We assume that this variable is updated depending on the current
observation and used to gate reactive policies rather than to directly emit actions. More pre-
cisely, the dynamic Bayesian network in Fig. 18.7(a) captures the POMDP and the agent
model which is deﬁned by the
initial internal memory distribution
P(b0 =b) =: νb
internal memory transition
P(bt+1 =b′ | bt =b, yt =y) =: λb′by
reactive policies
P(at =a | bt =b, yt =y) =: πaby.
Here we introduced bt as the agent’s internal memory variable. It is comparable to the
‘node state’ in ﬁnite state controllers (Fig. 18.7(b)), but diﬀers in that it does not directly
emit actions but rather gates reactive policies: for each internal memory state b the agent
uses a diﬀerent ‘mapping’ πaby (i.e. a diﬀerent reactive policy) from observations to actions.
As for the MDP case, solving the POMDP in this approach means to ﬁnd param-
eters θ = (ν, λ, π) of the DBN in Fig. 18.7 that maximise the expected future return
Vθ = E{P∞
t=0 γt rt; θ} for a discount factor γ ∈[0, 1).

Expectation maximisation for (PO)MDPs and control
405
(a)
a0
b1
y1
y0
b0
a1
y2
b2
r1
r0
r2
a2
s0
s1
s2
(b)
a0
y1
y0
a1
y2
r1
r0
r2
a2
n0
n1
n2
s0
s1
s2
Figure 18.7 (a) DBN of the POMDP and policy with internal memory bt; the time is unbounded and rewards are
emitted at every time step. (b) For comparison: the DBN of a POMDP with a standard FSC, with ‘node state’ nt.
M-step
The M-steps for the parameters can directly be derived from the free-energy in the form
(Eq. (18.14)). We have:
πnew
aby =
πold
aby
Cby
X
s
h
P(R|a, s) +
X
b′s′
β(b′, s′) λb′by P(s′|a, s)
i
P(y|s) α(b, s),
λnew
b′by =
λold
b′by
C′
by
X
s′,a,s
β(b′, s′) P(s′|a, s) πaby P(y|s) α(b, s),
νnew
b
= νold
b
C′′
b
X
x
β(b, s) P(s0 = s),
where Cby, C′
by and C′′
b are normalisation constants. Note that in these updates the α’s
(related to state visiting probabilities) play a crucial role. Also we are not aware of a greedy
version of these updates that proved eﬃcient (i.e. without immediate convergence to a local
minimum).
Complexity
Let S, B, A and Y momentarily denote the cardinalities of random variables s, b, a, y, respec-
tively. The main computational cost accumulates during a- and b-propagation; with the
separator (bt, st) both of which have complexity O(H B2 S 2). Here and below, S 2 scales
with the number of non-zero elements in the transition matrix P(s′|s) (assuming non-zero
action probabilities). We always use sparse matrix representations for transition matrices.
The number of propagations H scales with the expected time of reward (for a simple start-
goal scenario this is the expected time to goal). Sparse vector representations of α’s and
β’s further reduce the complexity depending on the topological dimensionality of P(s′|s).
The computational complexity of the M-step scales with O(AYB2S 2); in total this adds to
O((H + AY)B2S 2) for one EM iteration.
For comparison, let N denote the number of nodes in an FSC. The computation of a
policy gradient w.r.t. a single parameter of an FSC scales with O((H + AY)N2S 2) (taken

406
Marc Toussaint, Amos Storkey and Stefan Harmeling
(a)
(b)
Figure 18.8 (a) A simple scalable maze from [21], here with 330 states. The start (goal) position is marked
grey (black). The robot has ﬁve actions (north, south, east, west, stay) and his observation is a 4 bit num-
ber encoding the presence of adjacent walls. (b) Running times of EM-learning and FSC gradient ascent
that show how both methods scale with the maze size. The lines are medians, the errorbars min and max
of 10 independent runs for each of the various maze sizes. (For maze sizes beyond 1000 we display only
1 run.)
from [21], top of page 7). A fully parameterised FSC has NA + N2Y parameters, bearing a
total complexity of O((H + AY)N4S 2Y) to compute a full policy gradient.
For EM learning as well as gradient ascent, the complexity additionally multiplies with
the number k of EM iterations respectively gradient updates.
18.5.1
POMDP Experiments
Scaling
The POMDP EM algorithm has no free parameters except for the initialisations of λb′by,
πaby and νb. Roughly, we initialised νb and πaby approximately uniformly, while λb′by was
initialised in a way that favours not to switch the internal memory state, i.e., the diagonal of
the matrix λb′b· was initialised larger than the oﬀ-diagonal terms. More precisely, we ﬁrst
draw non-normalised numbers
πaby ∼1 + 0.1 U([0, 1]) ,
λb′by ∼1 + 5 δb′b + 0.1 U([0, 1]) ,
νb = 1,
(18.17)
where U([0, 1]) is the uniform distribution over [0, 1], and then normalise these parameters.
To start with, we test the scaling behaviour of our EM algorithm and compare it with
that of gradient ascent for a FSC (FSC-GA). We tried three options for coping with the
problem that the simple policy gradient in [21] ignores the normalisation constraints of the
parameters: (1) projecting the gradient on the simplex, (2) using a step-size-adaptive gra-
dient ascent (RPROP) with added soft-constraint gradients towards the simplex, (3) using
MATLAB’s gradient-based constraint optimisation method ‘fmincon’. The second option
gave the best results and we refer to those in the following. Note that our algorithm does
not have such problems: the M-step assigns correctly normalised parameters. Figure 18.8
displays the results for the simple maze considered in [21] for various maze sizes. Our pol-
icy model needs B = 2 internal memory states, the FSC N = 5 graph nodes to solve these
problems. The discount factor was chosen γ = 0.99. The results conﬁrm the diﬀerences we
noticed in the complexity analysis.

Expectation maximisation for (PO)MDPs and control
407
(a)
(b)
b = 3
b = 2
b = 1
(c)
(d)
Figure 18.9 Further mazes considered in the experiments. (c) Top left is the maze with the start (light grey), goal
(bottom right) and drain-states (dark). The other three illustrations display the internal memory state b (grey value
∝bα(b, s) for b = 1, 2, 3) at diﬀerent locations in the maze. (d) Large maze with 1516 turtle states.
Training the memory to gate primitive reactive behaviours
To exemplify the approach’s ability to learn an appropriate memory representation for a
given task we investigate further maze problems. We consider a turtle, which can move
forward, turn right or left, or wait. With probability 1−ϵ this action is successful; with
probability ϵ = 0.1 the turtle does not respond. The state space is the cross product of
positions and four possible orientations, and the observations are a 4 bit number encoding
the presence of adjacent walls relative to the turtle’s orientation. Further, whenever the
agent reaches the goal (or a zero-reward drain state, see below) it is instantly reset to the
start position.
Figures 18.9(a) and 18.9(b) display two small mazes with two speciﬁc diﬃculties: The
interior states, entries and exits (cross-shaded) of the halls in Fig. 18.9(a) all have the same
observation 0000. These halls are therefore places when the agent is likely to get lost.
For B = 2 (that is, when the latent variable b has two possible values), the turtle learns
a wall-following strategy as a basic reactive behaviour, while the internal memory is used
only at the exits and entrances to halls: for internal state b = 1 and observation y = 0000
the turtle turns left and switches to b = 2, while for b = 2 and y = 0000 the turtle goes
straight and switches back to b = 1. The maze in Fig. 18.9(b) is a binary-decision maze
and poses the problem of remembering how many junctions have passed already: To reach
the goal, the turtle has to follow aisles and at T-junctions make decisions [left, right, right,
left]. For B = 3 the algorithm ﬁnds the obvious solution: Each internal memory state is
associated with simple reactive behaviours that follows aisles and, depending on b, turns
left or right at a T-junction. A ﬁnite state controller would certainly ﬁnd a very similar

408
Marc Toussaint, Amos Storkey and Stefan Harmeling
1
100
0
66
400
iteration
freq
robot, left goal (100 runs)
(a)
1
100
0
87
600
iteration
freq
robot, right goal (100 runs)
(b)
1
100
0
89
150
700
iteration
freq
turtle, left goal (1 runs)
(c)
1
100
0
59
100
500
iteration
freq
turtle (init), bottom-left goal (1 runs)
(d)
Figure 18.10 Learning curves for the maze in Fig. 18.9(d). The expected reward interval (see footnote) is given
over the number of EM iterations. Solid: median over 100 independent trial runs (with noisy initialisations
(Eq. (18.17))), dashed and dotted: 2.5, 25, 75 and 97.5 percentiles, dotted baseline: shortest path to the goal
in the respective environment which were possible in the noise-free case. The optimal controller in our stochastic
case is necessarily above this baseline.
solution. However, in our case this solution generalises to situations when the corridors are
not straight: Figure 18.9(c) (top left) displays a maze with 30 locations (number of states
is 480), where the start state is in the top left corner and the goal state in the bottom right.
Again, the turtle has to make decisions [left, right, right, left] at T-junctions to reach the
goal, but additionally has to follow complex aisles in-between. Unlike with FSCs, our tur-
tle needs again only B = 3 internal memory states to represent the current corridor. The
shading in Fig. 18.9(c) displays the probability of visiting a location on a trajectory while
being in memory state b = 1, 2 or 3.
Finally, we investigate the maze in Fig. 18.9(d) with 379 locations (1516 turtle states).
The maze is a complex combination of corridors, rooms and junctions. On this maze we
also tested a normal robot (north, south, west, east actions with noise ϵ = 0.1), learning
curves for B = 3 for the left and rightmost goals are given in Fig. 18.10(a) and Fig. 18.6(b)
and exhibit reliable convergence.1 We also investigated single runs in the turtle case for
the leftmost goal (Fig. 18.10(c)) and the second left goal (Fig. 18.10(d) dashed line).
Again, the turtle utilises that aisle following can be implemented with a simple reactive
behaviour; the internal memory is only used for decision making in halls and at junctions.
1As a performance measure we deﬁne the expected reward interval which is directly linked to P(R). Consider
a cyclic process that receives a reward of 1 every d time steps; the expected future reward of this process is
P(R) = P∞
T=1 P(d T) = (1 −γ) P∞
T=1 γd T =
γd (1−γ)
1−γd
. Inverting this relation, we translate a given expected
future reward into an expected reward interval via d = log P(R)−log(P(R)+1−γ)
log γ
. This measure is rather intuitive: The
performance can directly be compared with the shortest path length to the goal. Note though that in the stochastic
environment even an optimal policy has an expected reward interval larger than the shortest path to goal.

Expectation maximisation for (PO)MDPs and control
409
Since the aisle following behaviour can readily be generalised to all goal settings we per-
formed another experiment: we took the ﬁnal policy πaby learned for the leftmost goal as
an initialisation for the task of ﬁnding the second left goal. Note that the start-to-goal paths
are largely disjoint. Still, the algorithm converges, particularly in the beginning, much faster
(Fig. 18.10(d) solid line), showing that such generalisation is indeed possible.
To conclude these experiments, we can summarise that the agent learned internal mem-
ory representations to switch between reactive behaviours. In the experiments they mainly
turned out to represent diﬀerent corridors. Generalisation of the reactive behaviours to new
goal situations is possible. Further, memorisation is not time bounded, e.g., independent
of the length of an aisle the turtle agent can sustain the current internal memory while
executing the reactive aisle following behaviour.
18.6
Conclusion
We introduced a framework for solving (PO)MDPs by translating the problem of maximis-
ing expected future return into a problem of likelihood maximisation. One ingredient for
this approach is the mixture of ﬁnite-time models we introduced in Section 18.2. We have
shown that this approach establishes equivalence for arbitrary reward functions, allows for
an eﬃcient inference procedure, propagating synchronously forward and backward with-
out pre-ﬁxing a ﬁnite time horizon H, and allows for the handling of discounting rewards.
We also showed that in the case of an unstructured MDP the resulting EM algorithm using
exact inference and a greedy M-step is closely related to standard policy iteration.
However, unlike policy iteration, the EM algorithm generalises to arbitrary DBNs and
the aim of this approach is to transfer the full variety of existing inference techniques to
the problem of solving (PO)MDPs. This refers especially to structured problem domains,
where DBNs allow us to consider structured representations of the environment (the world
state, e.g. factorisation or hierarchies) as well as the agent (e.g. hierarchical policies or
multiple agents). Inference techniques like variational approaches, message-passing algo-
rithms, or approximate belief representations in DBNs can be used to exploit such structure
in (PO)MDPs.
18.6.1
Follow-up and related work
We exempliﬁed the approach for exact inference on unstructured MDPs, using Gaussian
belief state propagation on a non-linear stochastic optimal control problem, and on a more
complex DBN formulation of a POMDP problem. Recently there have been a series of
papers based on or closely related to the general framework that we presented here. In [32]
we extended the approach to learning hierarchical controllers for POMDPs. In [36] we
presented a model-free reinforcement learning version of our EM approach. The authors
in [12, 13] use MCMC methods for approximate inference in this context and gener-
alise the EM algorithm for continuous MDPs [11]. Finally, [26, 16] developed similar EM
techniques in a robotics and model-free context.
An interesting issue for future research is to consider max-product BP (a generalisation
of Viterbi) for planning. In the POMDP context, further aspects to consider are: Can we
use inference techniques also to estimate the number of internal states we need to solve a
problem (cf. inﬁnite hidden Markov models [3] as a method to learn the number of hidden
states needed to model the data)? Or are there eﬃcient heuristics to add hidden states in a
DBN, e.g., analogous to how new nodes are added to bounded FSCs [27]?

410
Marc Toussaint, Amos Storkey and Stefan Harmeling
We hope that our approach lays new ground for a whole family of new, inference-based
techniques being applicable in the realm of (PO)MDPs.
Acknowledgments
M.T. is grateful to the German Research Foundation (DFG) for the
Emmy Noether fellowship TO 409/1-3.
18.A
Appendix: Remarks
(i) The mixture of ﬁnite-time MDPs may be compared to a classical interpretation of reward
discounting: Assume the agent has a probability (1 −γ) of dying after each time step. Then
the distribution over his life span is the geometric distribution P(T) = γT(1 −γ). In our
mixture of ﬁnite-time MDPs we treat each possible life-span T separately. From the agent’s
perspective, he knows that he has a ﬁnite life span T but he does not know what it is – he
lives in a mixture of possible worlds. Each ﬁnite life span is terminated by a single binary
reward (say, going to heaven or hell). The agent’s behaviour must reﬂect his uncertainty
about his life span and act by accounting for the probability that he might die now or later
on, i.e., he must ‘average’ over the mixture of possible worlds he might live in.
(ii) In the original MDP, the rewards at two diﬀerent time slices, say rt and rt+1, are
strongly correlated. The mixture of ﬁnite-time MDPs does not include such correlations
because the observations of reward at T =t and T =t + 1 are treated by separate ﬁnite-time
MDPs. However, since the expected future return Vπ is merely a sum of reward expectations
at diﬀerent time slices such correlations are irrelevant for solving the MDP and computing
optimal policies.
(iii) Do exponentiated rewards as observation likelihoods lead to equivalence? Let us
introduce modiﬁed binary reward variables ˆrt in every time slice with probabilities
P(ˆrt =1 | at, st) = eγtR(at,st) ,
R(at, st) := E{rt | at, st}.
Then
log P(ˆr0:T =1; π) = log Ea0:T ,s0:T {
T
Y
t=0
P(ˆrt =1 | at, st)}
≥Ea0:T ,s0:T {log
T
Y
t=0
P(ˆrt =1 | at, st)}
= Ea0:T ,s0:T {
T
X
t=0
γtR(at, st)}
= Vπ.
That is, maximisation of P(ˆr0:T = 1; π) is not equivalent to maximisation of Vπ. However,
this points to a formulation in terms of a Kullback–Leibler divergence minimisation
Vπ = Ea0:T ,s0:T {log
T
Y
t=0
P(ˆrt =1 | at, st)}
=
X
a0:T ,s0:T
h
P(s0) π(a0 | s0)
T
Y
t=1
π(at | st) P(st | at-1, st-1)
i
log
h
T
Y
t=0
exp{γtR(at, st)}
i
=
X
a0:T ,s0:T
p(a0:T, st:T | π) log q(a0:T, st:T),
where we deﬁned

Expectation maximisation for (PO)MDPs and control
411
Figure 18.11 Only the envelopes emanating from the start distribution (P(s))
and rewards (P(R|a, s)) contribute to the propagation. They are chopped where
they do not overlap with the other envelope after TM/2 iterations.
p(a0:T, st:T | π) = P(s0) π(a0 | s0)
T
Y
t=1
π(at | st) P(st | at-1, st-1),
q(a0:T, st:T) =
T
Y
t=0
exp{γtR(at, st)}.
The ﬁrst distribution p is the prior trajectory deﬁned by the policy π disregarding any
rewards. The second ‘distribution’ (if one normalises it) q(a0:T, s0:T) has a very simple
form, it fully factorises over time and in each time slice we have the exponentiated reward
with ‘temperature’ γ−t. If q(s0:T) is normalised, we can also write the value in terms of a
Kullback–Leibler divergence Vπ = −D(p | q) + H(p).
18.B
Appendix: Pruning computations
Consider a ﬁnite state space and assume that we ﬁxed the maximum allowed time T
by some upper limit TM (e.g., by deciding on a cutoﬀtime based on the time poste-
rior computed on the ﬂy, see below). Then there are potentially large regions of the
state space on which we may prune computations, i.e., states s for which the posterior
P(st = s | T = t + τ) = 0 for any t and τ with t + τ ≤TM. Figure 18.11 illustrates the idea.
Let us consider the a-propagation (Eq. (18.8)) ﬁrst (all statements apply conversely for the
b-propagation). For iteration time t we deﬁne a set of states
S a(t) = {s | at(s) , 0 ∧(t < TM/2 ∨bTM−t(s) , 0)}.
Under the assumption that bτ(s)=0 ⇒∀τ′ ≤τ:bτ′(s) = 0 it follows
i ∈S a(t) ⇐at(s) , 0 ∧bTM−t(s) , 0
⇐∃τ≤TM−t : at(s) , 0 ∧bτ(s) , 0
⇐⇒∃τ≤TM−t : γtτ(s) , 0.
Thus, every state that is potentially visited at time t (for which ∃τ:t+τ≤TM : γtτ(s) , 0) is
included in S a(t). We will exclude all states s < S a(t) from the a-propagation procedure
and not deliver their messages. The constraint t < TM/2 concerning the b’s was inserted in
the deﬁnition of S a(t) only because of the feasibility of computing S a(t) at iteration time t.
Initialising S a(0) = {s | P(s),0}, we can compute S a(t) recursively via
S a(t) =

S a(t−1) ∪OUT(S a(t−1))
for t < TM/2,
h
S a(t−1) ∪OUT(S a(t−1))
i
∩{s | bTM−t(s),0}
for t ≥TM/2,
where OUT(S a(t −1)) is the set of states which have non-zero probability transitions
from states in S a(t −1). Analogously, the bookkeeping for states that participate in the
b-propagation is
S b(0) ={s | P(R|a, s),0}

412
Marc Toussaint, Amos Storkey and Stefan Harmeling
S b(τ) =

S b(τ−1) ∪IN(S b(τ−1))
for τ < TM/2,
h
S b(τ−1) ∪IN(S b(τ−1))
i
∩{s | aTM−τ(s),0}
for τ ≥TM/2.
For the discount prior, we can use a time cutoﬀTM for which we expect further
contributions to be insigniﬁcant. The choice of this cutoﬀinvolves a payoﬀbetween com-
putational cost and accuracy of the E-step. Let T0 be the minimum T for which the
ﬁnite-time likelihood P(R | T; π) , 0. It is clear that the cutoﬀneeds to be greater than
T0. In the experiment in Section 18.4.3 we used an increasing schedule for the cutoﬀtime,
TM = (1 + 0.2 k) T0, depending on the iteration k of the EM algorithm to ensure that with
each iteration we become more accurate.
Bibliography
[1] C. G. Atkeson and J. C. Santamar´ıa. A
comparison of direct and model-based
reinforcement learning. In International
Conference on Robotics and Automation, 1997.
[2] H. Attias. Planning by probabilistic inference. In
Christopher M. Bishop and Brendan J. Frey,
editors, Proceedings of the 9th International
Workshop on Artiﬁcial Intelligence and
Statistics, 2003.
[3] M. J. Beal, Z. Ghahramani and C. E. Rasmussen.
The inﬁnite hidden Markov model. In
T. Dietterich, S. Becker, and Z. Ghahramani,
editors, Advances in Neural Information
Processing Systems 14. MIT Press, 2002.
[4] C. Boutilier, T. Dean, and S. Hanks. Decision
theoretic planning: structural assumptions and
computational leverage. Journal of Artiﬁcial
Intelligence Research, 11:1–94, 1999.
[5] C. Boutilier, R. Dearden and M. Goldszmidt.
Exploiting structure in policy construction. In
Proceedings of the 14th International Joint
Conference on Artiﬁcial Intelligence (IJCAI
1995), pages 1104–1111, 1995.
[6] H. Bui, S. Venkatesh and G. West. Policy
recognition in the abstract hidden Markov
models. Journal of Artiﬁcial Intelligence
Research, 17:451–499, 2002.
[7] M. Chavira, A. Darwiche, and M. Jaeger.
Compiling relational Bayesian networks for
exact inference. International Journal of
Approximate Reasoning, 42:4–20, 2006.
[8] G. F. Cooper. A method for using belief
networks as inﬂuence diagrams. In Proceedings
of the Fourth Workshop on Uncertainty in
Artiﬁcial Intelligence, pages 55–63, 1988.
[9] C. Guestrin, D. Koller, R. Parr and
S. Venkataraman. Eﬃcient solution algorithms
for factored MDPs. Journal of Artiﬁcial
Intelligence Research, 19: 399–468, 2003.
[10] M. Hauskrecht, N. Meuleau, L. P. Kaelbling,
T. Dean and C. Boutilier. Hierarchical solution
of Markov decision processes using
macro-actions. In Proceedings of Uncertainty in
Artiﬁcial Intelligence, pages 220–229, 1998.
[11] M. Hoﬀman, N. de Freitas, A. Doucet and
J. Peters. An expectation maximization
algorithm for continuous Markov decision
processes with arbitrary rewards. In Twelfth
International Conference on Artiﬁcial
Intelligence and Statistics, 2009.
[12] M. Hoﬀman, A. Doucet, N. de Freitas and
A. Jasra. Bayesian policy learning with
trans-dimensional MCMC. In Advances in
Neural Information Processing Systems 20. MIT
Press, 2008.
[13] M. Hoﬀman, H. Kueck, A. Doucet and
N. de Freitas. New inference strategies for
solving Markov decision processes using
reversible jump MCMC. In Uncertainty in
Artiﬁcial Intelligence, 2009.
[14] L. P. Kaelbling, M. L. Littman and A. R.
Cassandra. Planning and acting in partially
observable stochastic domains. Artiﬁcial
Intelligence, 101:99–134, 1998.
[15] L. P. Kaelbling, M. L. Littman and A. W. Moore.
Reinforcement learning: A survey. Journal of
Artiﬁcial Intelligence Research, 4:237–285,
1996.
[16] J. Kober and J. Peters. Policy search for motor
primitives in robotics. In D. Koller,
D. Schuurmans and Y. Bengio, editors, Advances
in Neural Information Processing Systems 21.
MIT Press, 2009.
[17] D. Koller and R. Parr. Computing factored value
functions for policies in structured MDPs. In
Proceedings of the 16th International Joint
Conference on Artiﬁcial Intelligence, pages
1332–1339, 1999.
[18] B. Kveton and M. Hauskrecht. An MCMC
approach to solving hybrid factored MDPs. In
Proceedings of the 19th International Joint
Conference on Artiﬁcial Intelligence, volume 19,
pages 1346–1351, 2005.
[19] M. L. Littman, S. M. Majercik and T. Pitassi.
Stochastic Boolean satisﬁability. Journal of
Automated Reasoning, 27(3):251–296, 2001.
[20] A. McGovern and A. G. Barto. Automatic
discovery of subgoals in reinforcement learning
using diverse density. In Proceedings of the 18th
International Conference on Machine Learning,
pages 361–368, 2001.

Expectation maximisation for (PO)MDPs and control
413
[21] N. Meuleau, L. Peshkin, K.-E. Kim and L. P.
Kaelbling. Learning ﬁnite-state controllers for
partially observable environments. In
Proceedings of Fifteenth Conference on
Uncertainity in Artiﬁcial Intelligence, pages
427–436, 1999.
[22] T. Minka. A family of algorithms for
approximate Bayesian inference. PhD thesis,
MIT, 2001.
[23] K. Murphy. Dynamic Bayesian networks:
Representation, inference and learning. PhD
Thesis, UC Berkeley, Computer Science
Division, 2002.
[24] A. Y. Ng, R. Parr and D. Koller. Policy search
via density estimation. In Advances in Neural
Information Processing Systems, pages
1022–1028, 1999.
[25] J. Pearl. Probabilistic Reasoning In Intelligent
Systems: Networks of Plausible Inference.
Morgan Kaufmann, 1988.
[26] J. Peters and S. Schaal. Reinforcement learning
by reward-weighted regression for operational
space control. In Proceedings of the
International Conference on Machine Learning,
2007.
[27] P. Poupart and C. Boutilier. Bounded ﬁnite state
controllers. In Advances in Neural Information
Processing Systems 16 (NIPS 2003), volume 16.
MIT Press, 2004.
[28] T. Raiko and M. Tornio. Learning nonlinear
state-space models for control. In Proceedings of
International Joint Conference on Neural
Networks, 2005.
[29] R. D. Shachter. Probabilistic inference and
inﬂuence diagrams. Operations Research,
36:589–605, 1988.
[30] G. Theocharous, K. Murphy and L. Kaelbling.
Representing hierarchical POMDPs as DBNs for
multi-scale robot localization. In International
Conference on Robotics and Automation, 2004.
[31] M. Toussaint. Lecture note: Inﬂuence diagrams.
http://ml.cs.tu-berlin.de/˜mtoussai/notes/, 2009.
[32] M. Toussaint, L. Charlin and P. Poupart.
Hierarchical POMDP controller optimization by
likelihood maximization. In Uncertainty in
Artiﬁcial Intelligence, 2008.
[33] M. Toussaint, S. Harmeling and A. Storkey.
Probabilistic inference for solving (PO)MDPs.
Technical Report EDI-INF-RR-0934, University
of Edinburgh, School of Informatics, 2006.
[34] M. Toussaint and A. Storkey. Probabilistic
inference for solving discrete and continuous
state Markov Decision Processes. In Proceedings
of the 23nd International Conference on
Machine Learning, pages 945–952, 2006.
[35] D. Verma and R. P. N. Rao. Goal-based imitation
as probabilistic inference over graphical models.
In Advances in Neural Information Processing
Systems, 2006.
[36] N. Vlassis and M. Toussaint. Model-free
reinforcement learning as mixture learning. In
Proceedings of the 26th International
Conference on Machine Learning, 2009.
[37] L. Zettlemoyer, H. Pasula and L. P. Kaelbling.
Learning planning rules in noisy stochastic
worlds. In Proceedings of the Twentieth National
Conference on Artiﬁcial Intelligence, 2005.
Contributors
Marc Toussaint, Technische Universit¨at Berlin, Franklinstr. 28/29 FR6-9, 10587 Berlin.
Amos Storkey, Institute for Adaptive and Neural Computation, University of Edinburgh
Stefan Harmeling, MPI for Biological Cybernetics, Dept. Sch¨olkopf, T¨ubingen.

Index
Abelian group, 278
acceptance probability, 24,
37, 246
adaptive Metropolis
algorithm, 33
Akaike’s information
criterion, 209, 320
alpha-beta recursion, 11, 390
annealed importance
sampling, 321
aperiodic chain, 23
APF, see auxiliary particle
ﬁlter
AR model, see autoregressive
model
assumed density ﬁltering, 17,
143, 388
autoregressive hidden Markov
model, 185
autoregressive model, 4, 9,
111
auxiliary particle ﬁlter, 52, 86,
249, 255
auxiliary variable, 330
backward Markov kernel, 65
band-limited approximation,
282
Bartlett’s paradox, 210
Basic group dynamical
model, 261
Baum–Welch algorithm, 10,
17, 390
Bayes factor, 209
Bayes’ rule, 127, 131, 283,
298
Bayesian information
criterion, 320
Bayesian methods, 83, 320,
341
beam sampler, 327, 329
belief network, 2, 185, 346
belief propagation, 153
Bell numbers, 264
Bellman equation, 373, 390
Bessel process, 99
Bethe free-energy, 150
binary segmentation
algorithm, 210
bootstrap ﬁlter, 29
bradycardia, 193
bridge sampling, 321
Brownian bridge, 88, 129
Brownian motion, 82, 213,
260, 295
burn-in, 247
Cameron–Martin–Girsanov
theorem, 87
Cayley graph, 287
χ2-distance, 63
censored observations, 347
central limit theorem, 22, 45,
57, 85, 234
changepoint model, 25, 205
changepoints, 359
character, 285
Chinese restaurant franchise,
322
Cholesky decomposition, 307,
349
Cholesky downdate, 349
Cholesky update, 349
CIR, see Cox–Ingersoll–Ross
diﬀusion
classical mechanics, 369
Hamilton equations, 369
Hamiltonian, 369
classiﬁcation problem, 308
Clebsch–Gordan
decomposition, 292
complete set of inequivalent
irreducible
representations, 279
condition monitoring, 182
conditionally Gaussian
state-space models,
141
conjugate prior, 4
Containment condition, 44
continuous-discrete ﬁltering
problem, 84
control theory, 8, 363
adaptive control, 374
continuous time control,
366
discrete time control, 365
dual control, 373
stochastic control, 370
controlled MCMC, 33
convolution, 229
convolution theorem, 280
correction smoothing, 14
covariance function, 344
Cox process, 66
Cox–Ingersoll–Ross (CIR)
diﬀusion, 98
cycle, 281
deterministic approximation
methods, 15, 295
assumed density ﬁltering,
17
expectation propagation, 21
variational Bayes, 16
diﬀusion bridge, 87
diﬀusion process, 82, 380
hypo-elliptic diﬀusion
processes, 82
Dirichlet distribution, 320
Dirichlet process
hierarchical Dirichlet
process, 325
Dobrushin coeﬃcient, 48
dynamic Dirichlet
distribution, 271
dynamic programming, 365
eigenvalue, 36, 262, 283
EM, see expectation
maximisation
emission model, 6
equal error rate, 198
Euler discretisation, 269
Euler–Lagrange equations,
136
Euler–Maruyama
approximation, 126
evidence, 25
exact algorithm (simulation of
diﬀusions), 88

Index
415
expectation correction, 18,
172, 196, 198
expectation maximisation, 17,
39, 83, 104, 106, 189,
332, 393
expectation propagation, 21,
145, 146, 180, 196,
299, 359, 381, 388
alternative backward pass,
154
generalised expectation
propagation, 150
extended Kalman ﬁlter, 233
factor graph, 144, 146, 278
factorial hidden Markov
model, 185
factorial switching linear
dynamical system,
182
Feynman–Kac ﬂow, 55
ﬁltering, 7, 10, 52
ﬁnancial trading, 268
ﬁnite diﬀerence
approximation, 238
Fokker–Planck equations,
127, 378
forward-backward algorithm,
11, 214
Fourier transform, 277
free-energy, 105, 130, 148,
381, 405
FSLDS, see factorial
switching linear
dynamical system
Fubini’s theorem, 95
fully independent training
conditional, 313
Gaussian process, 297, 343
Gaussian sum approximation,
171, 195
Gaussian sum ﬁlter, 171
Gaussian sum ﬁltering, 168,
198
generalised Fourier transform,
278
generalised pseudo Bayes,
143, 175
generative model, 4
genetic algorithm, 55
geospatial statistics, 341
Gibbs sampler, 39, 158, 178,
250, 301, 327
adaptive random scan
Gibbs sampler, 40
random scan Gibbs
sampler, 40
Girsanov change of measure,
129
Goldilocks principle, 32
GPB, see generalised pseudo
Bayes
gradient, 87, 235, 307
gradient ascent, 402
graph Laplacian, 287
graphical model, 2, 53, 180,
282, 385
Hadamard product, 344
Hamilton–Jacobi–Bellman
equation, 364, 367
Hamiltonian, 130
Hannan–Quinn information
criterion, 209
harmonic analysis, 278
HDP, see hierarchical
Dirichlet process
hidden Markov model, 7, 52,
104, 127, 145, 175,
185, 214, 225, 282,
317, 388
hierarchical Dirichlet process,
321
HJB, see Hamilton–Jacobi–
Bellman
equation
hybrid Monte Carlo, 47, 307
hypothesis testing, 207
identity management, 277
iHMM, see inﬁnite hidden
Markov model
importance sampling, 27, 38,
53, 84, 195, 383
importance weights, 233
independent component
analysis, 389
inﬁnite hidden Markov
model, 317, 321, 409
inﬁnite-horizon Markov
decision process, 390
inﬂuence diagrams, 388
innovation sequence, 189
input–output hidden Markov
model, 334
intensity function, 67
interacting MCMC, 34
irreducible chain, 23
irreps, see complete set of
inequivalent
irreducible
representations
isotypics, 282
Itˆo stochastic calculus, 126,
259
Jensen’s inequality, 74, 95,
130
junction tree, 151, 385, 398
k-means algorithm, 39
Kalman ﬁlter, see also linear
dynamical system
kernel function, 297
Kikuchi free-energy, 150
KL, see Kullback–Leibler
divergence
Kolmogorov backward
equation, 127
kriging, 341
Kronecker product, 292
KSP, see Kushner–
Stratonovich–Pardoux
equations
Kullback–Leibler divergence,
16, 63, 105, 107, 130,
143, 231, 246, 308,
384, 410
Kushner–Stratonovich–
Pardoux equations,
127
Lagrange multiplier, 106,
132, 152, 368
Lagrangian, 106, 132
Laplace approximation, 299,
347, 383
latent Markov model, 6
ﬁltering, 10
hidden Markov model, 7
Kalman ﬁlter, 351
linear dynamical system, 8
predictor-corrector
method, 13
Rauch–Tung–Striebel
smoothing, 14
smoothing, 11
linear dynamical system, 12,
145, 184
Kalman ﬁlter, 53, 84, 126,
144, 189, 251, 375
Rauch–Tung–Striebel
smoothing, 172
two-ﬁlter algorithm, 145
loopy belief propagation, 388
marginal likelihood, 299
Markov chain, 3, 34
aperiodic chain, 250
irreducible chain, 250
Markov chain Monte Carlo,
23, 32, 83, 104, 134,
210, 214, 295, 299,
321
acceptance probability, 246
adaptive independence
sampler, 38
adaptive MCMC, 302
adaptive Metropolis
algorithm, 35
coerced acceptance rate, 37
controlled MCMC, 35
equi-energy sampler, 41
external MCMC, 40
internal adaptive MCMC,
35
MCMC-particles
algorithm, 246
MCMC-particles-auxiliary
algorithm with
independent chains,
255
MCMC-particles-auxiliary
algorithm with

416
Index
likelihood tempering,
257
Particle Markov chain
Monte Carlo, 248
regional adaptation, 37
Markov decision process, 389
Bellman equation, 390
Markov jump process, 125
Markov kernel, 23, 34, 232
Markov model
autoregressive integrated
moving average
process, 190
Markov process, 125
Markov switching stochastic
volatility model, 75
martingale technique, 45
Mat´ern class, 297, 344
matrix exponential, 262
maximum a posteriori, 2, 104,
221, 388
maximum likelihood, 3, 83,
104, 206, 347, 392
MDP, see Markov decision
process
mean-ﬁeld, 106
Metropolis–Hastings, 23, 32,
246, 300, 330, 384
random walk
Metropolis–Hastings
algorithm, 32
Metropolis-within-Gibbs, 40
Single-Component
Adaptive Metropolis,
40
minimum description length,
213
Monte Carlo expectation
maximisation
algorithm, 86, 215
Monte Carlo methods, 21,
154, 348, 382
importance resampling, 28
importance sampling, 27
importance weights, 28
Markov chain Monte Carlo,
23
Gibbs sampling, 24
Metropolis–Hastings, 23
sequential Monte Carlo
methods, 27
sequential importance
sampling, 28
multiple hypothesis tracking
algorithm, 226
neonatal intensive care, 182
neural networks, 295, 343
Newton’s law, 367
non-commutative Fourier
transform, 277
null hypothesis, 208
Occam’s razor, 345
ordinary diﬀerential equation,
82, 133, 309, 366
Ornstein–Uhlenbeck process,
295
parallel tempering, 41
Parseval’s theorem, 280
Part-of-speech tagging, 332
partial diﬀerential equation,
92, 129, 367, 377
partially observed diﬀusion
process, 126
partially observed Markov
decision process, 373,
404
partially observed Poisson
processes, 229
particle ﬁltering, 28, 53, 84,
172, 233, 246
auxiliary particle ﬁlter, 55
partition, 280
path integral, 377, 380
PDE, see partial diﬀerential
equation
PDP, see piecewise-
deterministic
process
penalised maximum
likelihood, 208
permutation, 280
PHD ﬁlter, see probability
hypothesis density
ﬁlter
piecewise-deterministic
process, 66
Pitman–Yor process, 336
Plancherel’s theorem, 280
planning, 388
PMP, see Pontryagin
minimum principle
pointwise gradient
approximation, 238
Poisson equation, 45
Poisson estimator, 93
Poisson process, 18, 83, 89,
228
conditioning, 19
marking, 229
superposition, 19, 229, 230
thinning, 231
Poissonisation, 20
policy evaluation, 396
policy iteration, 381, 399
Polya urn, 317, 322
POMDP, see partially
observed Markov
decision process
Pontryagin minimum
principle, 364, 368
PoS-tagging, see
Part-of-speech tagging
practical ﬁlter, 248
predictor-corrector ﬁltering,
13
probability hypothesis density
ﬁlter, 20, 67, 68, 227,
231
Q-function, 399
quadrature, 345
Radon–Nikodym derivative,
88
random ﬁnite set, 225
random walk, 191, 283
Rao–Blackwellised particle
smoother, 156
receiver operating
characteristic curve,
198, 217
regression problem, 308
piecewise linear regression
model, 206
reinforcement learning, 374,
390
exploration exploitation
problem, 373
representation, 279
degree, 279
dimensionality, 279
equivalent representations,
279
irreducible representation,
279
representation theory, 278
repulsive force, 261
resample-move algorithm,
246
resample-move particle ﬁlter,
257
resampling, 28
multinomial resampling, 54
retrospective sampling, 89
reversible jump MCMC, 214,
321
Riccati equations, 372
root mean square error, 253
RTS smoothing, see
correction smoothing
Schr¨odinger equation, 377
Schwarz information
criterion, 209
segment neighbourhood
approach, 212
sensor networks, 341
sequential importance
resampling, 252
sequential Monte Carlo, 27,
53, 68, 233, 246
random weight particle
ﬁlter, 85
auxiliary particle ﬁlter, 86
bootstrap particle ﬁlter, 233
marginal particle ﬁlter, 64
Rao–Blackwellised particle
ﬁlter, 179, 195, 262

Index
417
sequential importance
resampling, 53, 54,
233
sequential importance
sampling, 233
sequential Monte Carlo
methods
particle ﬁltering, 28
sequential Monte Carlo
samplers, 65
resample-move algorithm,
65
simulated tempering, 41
simultaneous perturbation
stochastic
approximation, 238
SMC sampler, see sequential
Monte Carlo samplers
smoothing, 7, 11
spatial point process, 67, 226
spatial Poisson process, 67
spectral graph theory, 287
SPSA, see simultaneous
perturbation stochastic
approximation
state space model, see latent
Markov model
stationary distribution, 23
statistical linearisation, 135
steepest ascent algorithm, 238
stick-breaking, 322
stochastic diﬀerential
equation, 82, 126,
259, 370
stochastic Hamilton–Jacobi–
Bellman equation,
371
stochastic process, 66, 295
Brownian motion, 260
Itˆo process, 87
stock index returns, 75
stratiﬁcation, 69
stratiﬁed resampling, 71
strong junction tree, 153
strong law of large numbers,
44
strong Markov process, 82
structured mean-ﬁeld, 106,
155
sum-product algorithm, see
also belief
propagation, 144
superposition, 19
switching Kalman ﬁlter
model, 141, 167
switching linear dynamical
system, 142, 166, 167,
182, 184
switching linear dynamical
systems, 141
switching state-space model,
74, 167
symmetric group, 280
systems biology, 296
systems biology transcription
factor, 309
Taylor expansion, 367, 371
TBD, see track-before-detect
tempering, 257
tensor product, 292
time-inhomogeneous Poisson
process, 91
track-before-detect, 265
tracking, 9, 29, 195
trans-dimensional MCMC, 98
transition model, 6
transposition, 281
tree expectation propagation,
153
underrelaxation, 308
unscented Kalman ﬁlter, 233
unscented transform, 401
value iteration, 399
variational approximation,
104, 129, 196, 246,
299, 383, 388
variational Bayes, 104, 321
variational expectation
maximisation, 104,
107
vEM, see variational
expectation
maximisation
virtual leader, 262
volatility, 269
Wald’s identity, 86
Wiener process, 126, 128, 371
X-factor, 186
Young diagram, 280
Young tableau, 280
Young’s orthogonal
representation, 281

Figure 2.4 Adaptive ﬁt of a mixture of three Gaussian distributions with arbitrary means and covariance using the
maximum likelihood approach developed in [2].
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−0.5
0
0.5
1
1.5
2
x(t)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−10
−5
0
5
10
t
g(x,t)
(a) Sample paths and corresponding poste-
rior drifts.
0
5
0
–5
5
0
–5
5
0
–5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
t
(b) Prior, likelihood and posterior densities.
Figure 6.1 Illustration of a one-dimensional diﬀusion process without drift and unit diﬀusion coeﬃcient, starting
at the origin and with a noise free observation y = 1 at t = 1. The posterior process is a Brownian bridge.
Note how the drift increases drastically when getting close to the ﬁnal time. (a) shows ﬁve sample paths with
their corresponding posterior drift functions. (b) shows the mean and variance (shaded region) of the prior, the
likelihood and the posterior marginals. Observe how the variance of the posterior pt(x) is largest in the middle of
the time interval and eventually decreases to 0 at t = 1.

0
1000
2000
3000
4000
5000
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Distance (m)
Distance (m)
Direction
of motion
(a) Simulation ground truth
10
20
30
40
50
60
70
80
90
100
0
20
40
60
80
100
x−coordinate
Pixel
10
20
30
40
50
60
70
80
90
100
0
20
40
60
80
100
Time Step
Pixel
y−coordinate
(b) Simulation observations
Figure 11.5 These ﬁgures show the ground truth and one realisation of the observations (Pd,1 = 0.7 and
P fa = 0.002) for simulation scenario 1. In this scenario, there are two groups of two targets each merging into
a single group.
0
1000
2000
3000
4000
5000
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Distance (m)
Distance (m)
2 2 2
2222 2
4
2 2
2 2 2 2
2
4
4
4
4
4
4
4
4
4
4
Direction
of motion
(a) Tracking results with group models
0
1000
2000
3000
4000
5000
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Distance (m)
Distance (m)
Direction
of motion
Track lost and
false track
Track lost
(b) Tracking results without group models
Figure 11.6 These ﬁgures show one run of the result of the group tracking algorithm compared with a tracking
result with independent targets for Simulation Scenario 1. The ellipse shows the mode of the group conﬁguration,
labelled with the number of objects detected in the group. The results without group models show more erratic
behaviour in estimation.
200
400
600
800
1000
1200
1400
1600
1800
−0.5
0
0.5
1
1.5
2
2.5
Time samples (day)
Stock price (log scale)
Figure 11.8 Seven simulated stock price time series with
common group volatility and group mean reversion.

100
200
300
400
500
600
700
−0.5
0
0.5
1
1.5
Time Samples (Day)
Stock Price (log scale)
Stock 1
Stock 2
Stock 3
Stock 4
Stock 5
Figure 11.9 Stock price time series of Simulation Scenario
1 with two groups of two (Stocks 1 and 2) and three stocks
(Stocks 3, 4 and 5) each.
0
0.2
0.4
0.6
0.8
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
(a)
0
0.2
0.4
0.6
0.8
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
(b)
0
0.2
0.4
0.6
0.8
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
(c)
Figure 14.2 Illustration of sampling using control variables. (a) shows the current GP function f(t) with green,
the data and the current location of the control points (red circles). (b) shows the proposed new positions for
the control points (diamonds in magenta). (c) shows the proposed new function values f(t+1) drawn from the
conditional GP prior (blue dotted line).
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3
Figure 14.3 Visualisation of iterating between control variables. The red solid line is the current f(t), the blue
line is the proposed f(t+1), the red circles are the current control variables f(t)
c while the diamond (in magenta) is
the proposed control variable f (t+1)
ci
. The blue solid vertical line represents the distribution p( f (t+1)
ci
|f(t)
c\i) (with two
standard error bars) and the shaded area shows the eﬀective proposal p(ft+1|f(t)
c\i).

0
2
4
6
8
10
x 10
0
5
10
15
20
MCMC iterations
gibbs
region
control
2
4
6
8
10
0
10
20
30
40
50
60
dimension
Gibbs
region
control
2
4
6
8
10
0
10
20
30
40
50
60
70
dimension
number of control variables
corrCoef
control
0
0.05
0.1
0.15
0.2
(a)
(b)
(c)
Figure 14.4 (a) shows the evolution of the KL divergence (against the number of MCMC iterations) between the
true posterior and the empirically estimated posteriors for a ﬁve-dimensional regression dataset. (b) shows the
mean values with one-standard error bars of the KL divergence (against the input dimension) between the true
posterior and the empirically estimated posteriors. (c) plots the number of control variables used together with the
average correlation coeﬃcient of the GP prior.
(a)
(b)
(c)
(d)
Figure 17.5 Double slit experiment. (a) Setup of the experiment. Particles travel from t = 0 to t = 2 under
dynamics (17.38). A slit is placed at time t = t1, blocking all particles by annihilation. Two trajectories are
shown under optimal control. (b) Naive Monte Carlo sampling trajectories to compute J(x = −1, t = 0) through
Eq. (17.35). Only trajectories that pass through a slit contribute to the estimate. (c) Comparison of naive MC
estimates with N = 100 000 trajectories and exact result for J(x, t = 0) for all x. (d) Comparison of Laplace
approximation (dotted line) and Monte Carlo importance sampling (solid jagged line) of J(x, t = 0) with exact
result (17.39) (solid smooth line). The importance sampler used N = 100 trajectories for each x.

