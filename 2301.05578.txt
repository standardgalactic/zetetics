Toward General Design Principles for Generative AI Applications
JUSTIN D. WEISZ, IBM Research AI, USA
MICHAEL MULLER, IBM Research AI, USA
JESSICA HE, IBM Research AI, USA
STEPHANIE HOUDE, IBM Research AI, USA
Fig. 1. Seven principles for the design of generative AI systems. Six of these principles are presented in overlapping circles, indicating
their relationships to each other. One principle stands alone, the directive to design against potential harms that may be caused by a
generative model’s output, misuse, or other harmful effects. These principles are bounded in an environment of generative variability,
in which the outputs of a generative AI application may vary in quantity, quality, character, or other characteristics.
Generative AI technologies are growing in power, utility, and use. As generative technologies are being incorporated into mainstream
applications, there is a need for guidance on how to design those applications to foster productive and safe use. Based on recent
research on human-AI co-creation within the HCI and AI communities, we present a set of seven principles for the design of generative
AI applications. These principles are grounded in an environment of generative variability. Six principles are focused on designing
for characteristics of generative AI: multiple outcomes & imperfection; exploration & control; and mental models & explanations. In
addition, we urge designers to design against potential harms that may be caused by a generative model’s hazardous output, misuse,
or potential for human displacement. We anticipate these principles to usefully inform design decisions made in the creation of novel
human-AI applications, and we invite the community to apply, revise, and extend these principles to their own work.
CCS Concepts: • Human-centered computing →HCI design and evaluation methods; Interaction paradigms; HCI theory, concepts
and models.
Additional Key Words and Phrases: generative AI, design principles, human-centered AI, foundation models
ACM Reference Format:
Justin D. Weisz, Michael Muller, Jessica He, and Stephanie Houde. 2023. Toward General Design Principles for Generative AI
Applications. In . ACM, New York, NY, USA, 16 pages. https://doi.org/XXXXXXX.XXXXXXX
2023. Manuscript submitted to ACM
1
arXiv:2301.05578v1  [cs.HC]  13 Jan 2023

HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
Weisz et al. 2023
1
INTRODUCTION
As generative AI technologies continue to grow in power and utility, their use is becoming more mainstream. Generative
models, including LLM-based foundation models [9], are being used for applications such as general Q&A (e.g. ChatGPT1),
software engineering assistance (e.g. Copilot2), task automation (e.g. Adept3), copywriting (e.g. Jasper.ai4), and the
creation of high-fidelity artwork (e.g. DALL-E 2 [87], Stable Diffusion [90], Midjourney5). Given the explosion in
popularity of these new kinds of generative applications, there is a need for guidance on how to design those applications
to foster productive and safe use, in line with human-centered AI values [100].
Fostering productive use is a challenge, as revealed in a recent literature survey by Campero et al. [14]. They found
that many human-AI collaborative systems failed to achieve positive synergy – the notion that a human-AI team is able
to accomplish superior outcomes above either party working alone. In fact, some studies have found the opposite effect,
that human-AI teams produced inferior results to either a human or AI working alone [12, 22, 49, 53].
Fostering safe use is a challenge because of the potential risks and harms that stem from generative AI, either because
of how the model was trained (e.g. [113]) or because of how it is applied (e.g. [46, 79]).
In order to address these issues, we propose a set of design principles to aid the designers of generative AI systems.
These principles are grounded in an environment of generative variability, indicating the two properties of generative
AI systems inherently different from traditional discriminative6 AI systems: generative, because the aim of generative
AI applications is to produce artifacts as outputs, rather than determine decision boundaries as discriminative AI systems
do, and variability, indicating the fact that, for a given input, a generative system may produce a variety of possible
outputs, many of which may be valid; in the discriminative case, it is expected that the output of a model does not vary
for a given input.
We note that our principles are meant to generally apply to generative AI applications. Other sets of design principles
exist for specific kinds of generative AI applications, including Liu and Chilton [63]’s guidelines for engineering prompts
for text-to-image models, and advice about one-shot prompts for generation of texts of different kinds [25, 40, 89].
There are also more general AI-related design guidelines [1, 5, 23, 48, 57].
Six of our principles are presented as “design for...” statements, indicating the characteristics that designers should
keep in mind when making important design decisions. One is presented as a “design against...” statement, directing
designers to design against potential harms that may arise from hazardous model outputs, misuse, potential for human
displacement, or other harms we have not yet considered. The principles interact with each other in complex ways,
schematically represented via overlapping circles in Figure 1. For example, the characteristic denoted in one principle
(e.g. multiple outputs) can sometimes be leveraged as a strategy for addressing another principle (e.g. exploration).
Principles are also connected by a user’s aims, such as producing a singular artifact, seeking inspiration or creative ideas,
or learning about a domain. They are also connected by design features or attributes of a generative AI application,
such as the support for versioning, curation, or sandbox environments.
1http://chat.openai.com
2http://copilot.github.com
3http://adept.ai
4http://jasper.ai
5http://midjourney.com
6Our use of the term discriminative is to indicate that the task conducted by the AI algorithm is one of determining to which class or group a data instance
belongs; classification and clustering algorithms are examples of discriminative AI. Although our use of the term discriminative may evoke imagery
of human discrimination (e.g. via racial, religious, gender identity, genetic, or other lines), our use follows the scientific convention established in the
machine learning community (see, e.g., https://en.wikipedia.org/wiki/Discriminative_model)
2

Toward General Design Principles for Generative AI Applications
HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
Our aim for these principles is threefold: (1) to provide the designers of generative AI applications with the language
to discuss issues unique to generative AI; (2) to provide strategies and guidance to help designers make important design
decisions around how end users will interact with a generative AI application; and (3) to sensitize designers to the idea
that generative AI applications may cause a variety of harms (likely inadvertently, but possibly intentionally). We hope
these principles provide the human-AI co-creation community with a reasoned way to think through the design of
novel generative AI applications.
2
DESIGN PRINCIPLES FOR GENERATIVE AI APPLICATIONS
We developed seven design principles for generative AI applications based on recent research in the HCI and AI
communities, specifically around human-AI co-creative processes. We conducted a literature review of research studies,
guidelines, and analytic frameworks from these communities [1, 5, 23, 27, 39, 48, 57, 65, 68, 69, 81, 82, 96, 104], which
included experiments in human-AI co-creation [2, 3, 55, 64, 106, 115, 116], examinations of representative generative
applications [11, 50, 51, 64, 72, 87, 90, 92], and a review of publications in recent workshops [35, 77, 78, 114].
2.1
The Environment: Generative Variability
Generative AI technologies present unique challenges for designers of AI systems compared to discriminative AI
systems. First, generative AI is generative in nature, which means their purpose is to produce artifacts as output, rather
than decisions, labels, classifications, and/or decision boundaries. These artifacts may be comprised of different types of
media, such as text, images, audio, animations or videos. Second, the outputs of a generative AI model are variable in
nature. Whereas discriminitive AI aims for deterministic outcomes, generative AI systems may not produce the same
output for a given input each time. In fact, by design, they can produce multiple and divergent outputs for a given input,
some or all of which may be satisfactory to the user. Thus, it may be difficult for users to achieve replicable results
when working with a generative AI application.
Although the very nature of generative applications violates the common HCI principle that a system should
respond consistently to a user’s input (for critiques of this position, see [6, 10, 24, 28, 36, 79]), we take the position that
this environment in which generative applications operate – generative variability – is a core strength. Generative
applications enable users to explore or populate a “space” of possible outcomes to their query. Sometimes, this exploration
is explicit, as in the case of systems that enable latent space manipulations of an artifact. Other times, exploration of a
space occurs when a generative model produces multiple candidate outputs for a given input, such as multiple distinct
images for a given prompt [87, 90] or multiple implementations of a source code program [115, 116]. Recent studies
also show how users may improve their knowledge of a domain by working with a generative model and its variable
outputs [92, 115].
This concept of generative variability is crucially important for designers of generative AI applications to communicate
to users. Users who approach a generative AI system without understanding its probabilistic nature and its capacity to
produce varied outputs will struggle to interact with it in productive ways. The design principles we outline in the
following sections – designing for multiple outcomes & imperfection, for exploration & human control, and for mental
models & explanations – are all rooted in the notion that generative AI systems are distinct and unique because they
operate in an environment of generative variability.
3

HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
Weisz et al. 2023
2.2
Design for Multiple Outputs
Generative AI technologies such as encoder-decoder models [21, 107], generative adversarial networks [38], and
transformer models [108] are probabilistic in nature and thus are capable of producing multiple, distinct outputs for a
user’s input. Designers therefore need to understand the extent to which these multiple outputs should be visible to
users. Do users need the ability to annotate or curate? Do they need the ability to compare or contrast? How many
outputs does a user need?
Understanding the user’s task can help answer these questions. If the user’s task is one of production, in which the
ultimate goal is to produce a single, satisfying artifact, then designs that help the user filter and visualize differences
may be preferable. For example, a software engineer’s goal is often to implement a method that performs a specific
behavior. Tools such as Copilot take a user’s input, such as a method signature or documentation, and provide a singular
output. Contrarily, if the user’s task is one of exploration, then designs that help the user curate, annotate, and mutate
may be preferable. For example, a software engineer may wish to explore a space of possible test cases for a code
module. Or, an artist may wish to explore different compositions or styles to see a broad range of possibilities. Below
we discuss a set of strategies for helping design for multiple outputs.
2.2.1
Versioning. Because of the randomness involved in the generative process, as well as other user-configurable
parameters (e.g. a random seed, a temperature, or other types of user controls), it may be difficult for a user to produce
exactly the same outcome twice. As a user interacts with a generative AI application and creates a set of outputs, they
may find that they prefer earlier outputs to later ones. How can they recover or reset the state of the system to generate
such earlier outputs? One strategy is to keep track of all of these outputs, as well as the parameters that produced them,
by versioning them. Such versioning can happen manually (e.g. the user clicks a button to “save” their current working
state) or automatically.
2.2.2
Curation. When a generative model is capable of producing multiple outputs, users may need tools to curate
those outputs. Curation may include collecting, filtering, sorting, selecting, or organizing outputs (possibly from the
versioned queue) into meaningful subsets or groups, or creating prioritized lists or hierarchies of outputs according to
some subjective or objective criteria. For example, CogMol7 generates novel molecular compounds, which can be sorted
by various properties, such as their molecular weight, toxicity, or water solubility [18, 19]. In addition, the confidence
of the model in each output it produced may be a useful way to sort or rank outputs, although in some cases, model
confidence scores may not be indicative of the quality of the model’s output [3].
2.2.3
Annotation. When a generative model has produced a large number of outputs, users may desire to add marks,
decorators, or annotations to outputs of interest. These annotations may be applied to the output itself (e.g. “I like this”)
or it may be applied to a portion or subset of the output (e.g. flagging lines of source code that look problematic and
need review).
2.2.4
Visualizing Differences. In some cases, a generative model may produce a diverse set of distinct outputs, such as
images of cats that look strikingly different from each other. In other cases, a generative model may produce a set of
outputs for which it is difficult to discern differences, such as a source code translation from one language to another.
In this case, tools that aid users in visualizing the similarities and differences between multiple outputs can be useful.
Depending on the users’ goals, they may seek to find the invariant aspects across outcomes, such as identifying which
7http://covid19-mol.mybluemix.net
4

Toward General Design Principles for Generative AI Applications
HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
parts of a source code translation were the same across multiple translations, indicating a confidence in its correctness.
Or, users may prioritize the variant aspects for greater creativity and inspiration. For example, Sentient Sketchbook [59]
is a video game co-creation system that displays a number of different metrics of the maps it generates, enabling users
to compare newly-generated maps with their current map to understand how they differ.
2.3
Design for Imperfection
It is highly important for users to understand that the quality of a generative model’s outputs will vary. Users who
expect a generative AI application to produce exactly the artifact they desire will experience frustration when they
work with the system and find that it often produces imperfect artifacts. By “imperfect,” we mean that the artifact itself
may have imperfections, such as visual misrepresentations in an image, bugs or errors in source code, missing desired
elements (e.g. “an illustration of a bunny with a carrot” fails to include a carrot), violations of constraints specified in
the input prompt (e.g. “write a 10 word sentence” produces a much longer or shorter sentence), or even untruthful or
misleading answers (e.g. a summary of a scientific topic that includes non-existent references [91]). But, “imperfect”
can also mean “doesn’t satisfy the user’s desire,” such as when the user prompts a model and doesn’t get back any
satisfying outputs (e.g. the user didn’t like any of the illustrations of a bunny with a carrot). Below we discuss a set of
strategies for helping design for imperfection.
2.3.1
Multiple Outputs. Our previous design principle is also a strategy for handling imperfect outputs. If a generative
model is allowed to produce multiple outputs, the likelihood that one of those outputs is satisfying to the user is
increased. One example of this effect is in how code translation models are evaluated, via a metric called 𝑝𝑎𝑠𝑠@𝑘[56, 93].
The idea is that the model is allowed to produce 𝑘code translations for a given input, and if any of them pass a set of
unit tests, then the model is said to have produced a correct translation. In this way, generating multiple outputs serves
to mitigate the fact that the model’s most-likely output may be imperfect. However, it is left up to the user to review
the set of outputs and identify the one that is satisfactory; with multiple outputs that are very similar to each other, this
task may be difficult [116], implying the need for a way to easily visualize differences.
2.3.2
Evaluation & Identification. Given that generative models may not produce perfect (or perfectly satisfying)
outputs, they may still be able to provide users with a signal about the quality of its output, or indicate parts that require
human review. As previously discussed, a model’s per-output confidence scores may be used (with care) to indicate
the quality of a model’s output. Or, domain-specific metrics (e.g. molecular toxicity, compiler errors) may be useful
indicators to evaluate whether an artifact achieved a desirable level of quality. Thus, evaluating the quality of generated
artifacts and identifying which portions of those artifacts may contain imperfections (and thus require human review,
discussed further in Weisz et al. [115]) can be an effective way for handling imperfection.
2.3.3
Co-Creation. User experiences that allow for co-creation, in which both the user and the AI can edit a candidate
artifact, will be more effective than user experiences that assume or aim for the generative model to produce a perfect
output. Allowing users to edit a model’s outputs provides them with the opportunity to find and fix imperfections, and
ultimately achieve a satisfactory artifact. One example of this idea is Github Copilot [37], which is embedded in the
VSCode IDE. In the case when Copilot produces an imperfect block of source code, developers are able to edit it right in
context without any friction. By contrast, tools like Midjourney or Stable Diffusion only produce a gallery of images to
chose from; editing those images requires the user to shift to a different environment (e.g. Photoshop).
5

HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
Weisz et al. 2023
2.3.4
Sandbox / Playground Environment. A sandbox or playground environment ensures that when a user interacts
with a generated artifact, their interactions (such as edits, manipulations, or annotations) do not impact the larger
context or environment in which they are working. Returning to the example of Github Copilot, since it is situated inside
a developer’s IDE, code it produces is directly inserted into the working code file. Although this design choice enables
co-creation, it also poses a risk that imperfect code is injected into a production code base. A sandbox environment that
requires users to explicitly copy and paste code in order to commit it to the current working file may guard against the
accidental inclusion of imperfect outputs in a larger environment or product.
2.4
Design for Human Control
Keeping humans in control of AI systems is a core tenet of human-centered AI [98–100]. Providing users with controls
in generative applications can improve their experience by increasing their efficiency, comprehension, and ownership
of generated outcomes [64]. But, in co-creative contexts, there are multiple ways to interpret what kinds of “control”
people need. We identify three kinds of controls applicable to generative AI applications.
2.4.1
Generic Controls. One aspect of control relates to the exploration of a design space or range of possible outcomes
(as discussed in Section 2.5). Users need appropriate controls in order to drive their explorations, such as control over
the number of outputs produced from an input or the amount of variability present in the outputs. We refer to these
kinds of controls as generic controls, as they are applicable to any particular generative technology or domain. As an
example, some generative projects may involve a “lifecycle” pattern in which users benefit from seeing a great diversity
of outputs in early stages of the process in order to search for ideas, inspirations, or directions. Later stages of the project
may focus on a smaller number (or singular) output, requiring controls that specifically operate on that output. Many
generative algorithms include a user-controllable parameter called temperature. A low temperature setting produces
outcomes that are very similar to each other; conversely, a high temperature setting produces outcomes that are very
dissimilar to each other. In the “lifecycle” model, users may first set a high temperature for increased diversity, and then
reduce it when they wish to focus on a particular area of interest in the output space. This effect was observed in a
study of a music co-creation tool, in which novice users dragged temperature control sliders to the extreme ends to
explore the limits of what the AI could generate [64].
2.4.2
Technology-specific Controls. Other types of controls will depend on the particular generative technology being
employed. Encoder-decoder models, for example, often allow users to perform latent space manipulations of an artifact
in order to control semantically-meaningful attributes. For example, Liu and Chilton [62] demonstrate how semantic
sliders can be used to control attributes of 3D models of animals, such as the animal’s torso length, neck length, and
neck rotation. Transformer models use a temperature parameter to control the amount of randomness in the generation
process [110]. Natural language prompting, and the emerging discipline of prompt engineering [63], provide additional
ways to tune or tweak the outputs of large language models. We refer to these kinds of controls as technology-specific
controls, as the controls exposed to a user in a user interface will depend upon the particular generative AI technology
used in the application.
2.4.3
Domain-specific Controls. Some types of user controls will be domain-specific, dependent on the type of artifact
being produced. For example, generative models that produce molecules as output might be controlled by having
the user specify desired properties such as molecular weight or water solubility; these types of constraints might be
propagated to the model itself (e.g. expressed as a constraint in the encoder phase), or they may simply act as a filter on
6

Toward General Design Principles for Generative AI Applications
HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
the model’s output (e.g. hide anything from the user that doesn’t satisfy the constraints). In either case, the control
itself is dependent on the fact that the model is producing a specific kind of artifact, such as a molecule, and would not
logically make sense for other kinds of artifacts in other domains (e.g. how would you control the water solubility for a
text-to-image model?). Thus, we refer to these types of controls, independent of how they are implemented, as domain
specific. Other examples of domain-specific controls include the reading level of a text, the color palette or artistic style
of an image, or the run time or memory efficiency of source code.
2.5
Design for Exploration
Because users are working in an environment of generative variability, they will need some way to “explore” or “navigate”
the space of potential outputs in order to identify one (or more) that satisfies their needs. Below we discuss a set of
strategies for helping design for exploration.
2.5.1
Multiple Outputs. The ability for a generative model to produce multiple outputs (Section 2.2) is an enabler of
exploration. Returning to the bunny and carrot example, an artist may wish to explore different illustrative styles and
prompt (and re-prompt) the model for additional candidates of “a bunny with a carrot” in various kinds of styles or
configurations. Or, a developer can explore different ways to implement an algorithm by prompting (and re-prompting)
a model to produce implementations that possess different attributes (e.g. “implement this using recursion,” “implement
this using iteration,” or “implement this using memoization”). In this way, a user can get a sense of the different
possibilities the model is capable of producing.
2.5.2
Control. Depending on the specific technical architecture used by the generative application, there may be
different ways for users to control it (Section 2.4). No matter the specific mechanisms of control, providing controls to a
user provides them with the ability to interactively work with the model to explore the space of possible outputs for
their given input.
2.5.3
Sandbox / Playground Environment. A sandbox or playground environment can enable exploration by providing
a separate place in which new candidates can be explored, without interfering with a user’s main working environment.
For example, in a project using Copilot, Cheng et al. [17] suggest providing, “a sandbox mechanism to allow users to
play with the prompt in the context of their own project.”
2.5.4
Visualization. One way to help users understand the space in which they are exploring is to explicitly visualize
it for them. Kreminski et al. [55] introduce the idea of expressive range coverage analysis (ERCA) in which a user is
shown a visualization of the “range” of possible generated artifacts across a variety of metrics. Then, as users interact
with the system and produce specific artifact instances, those instances are included in the visualization to show how
much of the “range” or “space” was explored by the user.
2.6
Design for Mental Models
Users form mental models when they work with technological systems [31, 70, 95]. These models represent the user’s
understanding of how the system works and how to work with it effectively to produce the outcomes they desire. Due
to the environment of generative variability, generative AI applications will pose new challenges to users because these
applications may violate existing mental models of how computing systems behave (i.e. in a deterministic fashion).
Therefore, we recommend designing to support users in creating accurate mental models of generative AI applications
in the following ways.
7

HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
Weisz et al. 2023
2.6.1
Orientation to Generative Variability. Users may need a general introduction to the concept of generative AI.
They should understand that the system may produce multiple outputs for their query (Section 2.2), that those outputs
may contain flaws or imperfections (Section 2.3), and that their effort may be required to collaborate with the system in
order to produce desired artifacts via various kinds of controls (Section 2.4).
2.6.2
Role of the AI. Research in human-AI interaction suggests that users may view an AI application as filling a role
such as an assistant, coach, or teammate [96]. In a study of video game co-creation, Guzdial et al. [41] found participants
to ascribe roles of friend, collaborator, student, and manager to the AI system. Recent work by Ross et al. [92] examined
software engineers’ role orientations toward a programming assistant and found that people viewed the assistant with
a tool orientation, but interacted with it as if it were a social agent. Clearly establishing the role of a generative AI
application in a user’s workflow, as well as its level of autonomy (e.g. [32, 45, 84, 97]), will help users better understand
how to interact effectively with it. Designers can reason about the role of their application by answering questions such
as, is it a tool or partner? does it act proactively or does it just respond to the user? does it make changes to an artifact
directly or does it simply make recommendations for the user?
2.7
Design for Explanations
Generative AI applications will be unfamiliar and possibly unusual to many users. They will want to know what the
application can (and cannot) do, how well it works, and how to work with it effectively. Some users may even wish to
understand the technical details of how the underlying generative AI algorithms work, although these details may not
be necessary to work effectively with the model (as discussed in [115]).
In recent years, the explainable AI (XAI) community has made tremendous progress at developing techniques for
explaining how AI systems work [7, 30, 57, 58, 101]. Much of the work in XAI has focused on discriminative algorithms:
how they generally make decisions (e.g. via interpretable models [74, Chapter 5] or feature importance [74, Section 8.5],
and why they make a decision in a specific instance (e.g. via counterfactual explanations [74, Section 9.3].
Recent work in human-centered XAI (HCXAI) has emphasized designing explanations that cater to human knowledge
and human needs [30]. This work grew out of a general shift toward human-centered data science [6], in which the
import of explanations is not for a technical user (data scientist), but for an end user who might be impacted by a
machine learning model.
In the case of generative AI, recent work has begun to explore the needs for explainability. Sun et al. [106] explored
explainability needs of software engineers working with a generative AI model for various types of use cases, such
as code translation and autocompletion. They identified a number of types of questions that software engineers had
about the generative AI, its capabilities, and its limitations, indicating that explainability is an important feature for
generative AI applications. They also identified several gaps in existing explainability frameworks stemming from the
generative nature of the AI system, indicating that existing XAI techniques may not be sufficient for generative AI
applications. Thus, we make the following recommendations for how to design for explanations.
2.7.1
Calibrate Trust by Communicating Capabilities and Limitations. Because of the inherent imperfection of generative
AI outputs, users would be well-served if they understood the limitations of these systems [80, 85], allowing them
to calibrate their trust in terms of what the application can and cannot do [86]. When these kinds of imperfections
(Section 2.3) are not signaled, users of co-creative tools may mistakenly blame themselves for shortcomings of generated
artifacts in co-creative applications [64], and users in Q & A use cases can be shown deceptive misconceptions and
harmful falsehoods as objective answers [60]. One way to communicate the capabilities of a generative AI application
8

Toward General Design Principles for Generative AI Applications
HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
is to show examples of what it can do. For example, Midjourney provides a public discussion space to orient new users
and show them what other users have produced with the model. This space not only shows the outputs of the model
(e.g. images), but the textual prompts that produced the images. In this way, users can more quickly come to understand
how different prompts influence the application’s output. To communicate limitations, systems like ChatGPT contain
modal screens to inform users of the system’s limitations.
2.7.2
Use Explanations to Create and Reinforce Accurate Mental Models. Weisz et al. [115] explored how a generative
model’s confidence could be surfaced in a user interface. Working with a transformer model on a code translation
task, they developed a prototype UI that highlighted tokens in the translation that the model was not confident in.
In their user study, they found that those highlights also served as explanations for how the model worked: users
came to understand that each source code token was chosen probabilistically, and that the model had considered
other alternatives. This design transformed an algorithmic weakness (imperfect output) into a resource for users to
understand how the algorithm worked, and ultimately, to control its output (by showing users where they might need
to make changes).
2.8
Design Against Harms
The use of AI systems – including generative AI applications – may unfortunately lead to diverse forms of harms,
especially for people in vulnerable situations. Much work in AI ethics communities has identified how discriminative
AI systems may perpetuate harms such as the denial of personhood or identity [24, 52, 103]; the deprivation of liberty
or children [66, 94], and the erasure of persons, cultures, or nations through data silences [80]. We identify four types
of potential harms, some of which are unique to the generative domain, and others which represent existing risks of AI
applications that may manifest in new ways.
Our aim in this section is to sensitize designers to the potential risks and harms that generative AI systems may
pose. We do not prescribe solutions to address these risks, in part because it is an active area of research to understand
how these kinds of risks could be mitigated. Risk identification, assessment, and mitigation is a sociotechnical problem
involving computing resources, humans, and cultures. Even with our focus on the design of generative applications, an
analysis of harms that is limited to design concepts may blur into technosolutionism [61, 67, 88].
We do posit that human-centered approaches to generative AI design are a useful first step, but must be part of a larger
strategy to understand who are the direct and indirect stakeholders of a generative application [34, 44], and to work
directly with those stakeholders to identify harms, understand what are their differing priorities and value tensions [73],
and negotiate issues of culture, policy, and (yes) technology to meet these diverse challenges (e.g., [26, 29, 42]).
2.8.1
Hazardous Model Outputs. Generative AI applications may produce artifacts that cause harm. In an integrative
survey paper, Weidinger et al. [113] list six types of potential harms of large language models, three of which regard
the harms that may be caused by the model’s output:
• Discrimination, Exclusion, and Toxicity. Generative models may produce outputs that promote discrimina-
tion against certain groups, exclude certain groups from representation, or produce toxic content. Examples
include text-to-image models that fail to produce ethnically diverse outputs for a given input (e.g. a request for
images of doctors produces images of male, white doctors [20] or language models that produce inappropriate
language such as swear words, hate speech, or offensive content [1, 48].
9

HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
Weisz et al. 2023
• Information Hazards. Generative models may inadvertently leak private or sensitive information from their
training data. For example, Carlini et al. [16] found that strategically prompting GPT-2 revealed an individual’s full
name, work address, phone number, email, and fax number. Additionally, larger models may be more vulnerable
to these types of attacks [15, 16].
• Misinformation Harms. Generative models may produce inaccurate misinformation in response to a user’s
query. Lin et al. [60] found that GPT-3 can provide false answers that mimic human falsehoods and misconceptions,
such as “coughing can help stop a heart attack” or “[cold weather] tells us that global warming is a hoax”. Singhal
et al. [102] caution against the tendency of LLMs to hallucinate references, especially if consulted for medical
decisions. Albrecht et al. [4] claim that LLMs have few defenses against adversarial attacks while advising about
ethical questions. The Galactica model was found to hallucinate non-existent scientific references [43], and
Stack Overflow has banned responses sourced from ChatGPT due to their high rate of incorrect, yet plausible,
responses [109].
In addition to those harms, a generative model’s outputs may be hazardous in other ways as well.
• Deceit, Impersonation, and Manipulation. Generative algorithms can be used to create false records or
“deep fakes” (e.g., [46, 71]), to impersonate others (e.g. [105]), or to distort information into politically-altered
content [118]. In addition, they may manipulate users who believe that they are chatting with another human
rather than with an algorithm, as in the case of an unreviewed ChatGPT “experiment” in which at least 4,000
people seeking mental health support were connected to a chatbot rather than a human counselor [76].
• Copyright, Licenses, and Intellectual Property. Generative models may have been trained on data protected
by regulations such as the GDPR, which prohibits the re-use of data beyond the purposes for which it was
collected. In addition, large language models have been referred to as “stochastic parrots” due to their ability
to reproduce data that was used during their training [8]. One consequence of this effect is that the model
may produce outputs that incorporate or remix materials that are subject to copyright or intellectual property
protections [33, 47, 83]. For example, the Codex model, which produces source code as output, may (re-)produce
source code that is copyrighted or subject to a software license, or that was openly shared under a creative
commons license that prohibits commercial re-use (e.g., in a pay-to-access LLM). Thus, the use of a model’s
outputs in a project may cause that project to violate copyright protections, or subject that project to a restrictive
license (e.g. GPL). As of this writing, there is a lawsuit against GitHub, Microsoft, and OpenAI on alleged copyright
violations in the training of Codex [13].
2.8.2
Misuse. Weidinger et al. [113] describe how generative AI applications may be misused in ways unanticipated
by the creators of those systems. Examples include making disinformation cheaper and more effective, facilitating
fraud and scams, assisting code generation for cyberattacks, or conducting illegitimate surveillance and censorship.
In addition to these misuses, Houde et al. [46] also identify business misuses of generative AI applications such as
facilitating insurance fraud and fabricating evidence of a crime. Although designers may not be able to prevent users
from intentionally misusing their generative AI applications, there may be preventative measures that make sense for
a given application domain. For example, output images may be watermarked to indicate they were generated by a
particular model, blocklists may be used to disallow undesirable words in a textual prompt, or multiple people may be
required to review or approve a model’s outputs before they can be used.
10

Toward General Design Principles for Generative AI Applications
HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
2.8.3
Human Displacement. One consequence of the large-scale deployment of generative AI technologies is that they
may come to replace, rather than augment human workers. Such concerns have been raised in related areas, such as the
use of automated AI technologies in data science Wang et al. [111, 112]. Weidinger et al. [113] specifically discuss the
potential economic harms and inequalities that may arise as a consequence of widespread adoption of generative AI. If a
generative model is capable of producing high-fidelity outputs that rival (or even surpass) what can be created by human
effort, are the humans necessary anymore? Contemporary fears of human displacement by generative technologies are
beginning to manifest in mainstream media, such as in the case of illustrators’ concerns that text-to-image models
such as Stable Diffusion and Midjourney will put them out of a job [117]. We urge designers to find ways to design
generative AI applications that enhance or augment human abilities, rather than applications that aim to replace human
workers. Copilot serves as one example of a tool that clearly enhances the abilities of a software engineer: it operates
on the low-level details of a source code implementation, freeing up software engineers to focus more of their attention
on higher-level architectural and system design issues.
3
DISCUSSION
3.1
Designing for User Aims
Users of generative AI applications may have varied aims or goals in using those systems. Some users may be in
pursuit of perfecting a singular artifact, such as a method implementation in a software program. Other users may be
in pursuit of inspiration or creative ideas, such as when exploring a visual design space. As a consequence of working
with a generative AI application, users may also enhance their own learning or understanding of the domain in which
they are operating, such as when a software engineer learns something new about a programming language from the
model’s output. Each of these aims can be supported by our design principles, as well as help designers determine the
appropriate strategy for addressing the challenges posed by each principle.
To support artifact production, designers ought to carefully consider how to manage a model’s multiple, imperfect
outputs. Interfaces ought to support users in curating, annotating, and mutating artifacts to help users refine a singular
artifact. The ability to version artifacts, or show a history of artifact edits, may also be useful to enable users to revisit
discarded options or undo undesirable modifications. For cases in which users seek to produce one “ideal” artifact that
satisfies some criteria, controls that enable them to co-create with the generative tool can help them achieve their goal
more efficiently, and explanations that signal or identify imperfections can tell them how close or far they are from the
mark.
To support inspiration and creativity, designers also ought to provide adequate controls that enable users to explore
a design space of possibilities [55, 75]. Visualizations that represent the design space can also be helpful as they can
show which parts the user has vs. has not explored, enabling them to explore the novel parts of that space. Tools that
help users manage, curate, and filter the different outputs created during their explorations can be extremely helpful,
such as a digital mood board for capturing inspiring model outputs.
Finally, to support learning how to effectively interact with a generative AI application, designers ought to help users
create accurate mental models [54] through explanations [7, 30, 57, 58, 101]. Explanations can help answer general
questions such as what a generative AI application is capable or not capable of generating, how the model’s controls
impact its output, and how the model was trained and the provenance of its training data. They can also answer
questions about a specific model output, such as how confident the model was in that output, which portions of that
11

HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
Weisz et al. 2023
output might need human review or revision, how to adjust or modify the input or prompt to adjust properties of the
output, or what other options or alternatives exist for that output.
3.2
The Importance of Value-Sensitive Design in Mitigating Potential Harms
Designers need to be sensitive to the potential harms that may be caused by the rapid maturation and widespread
adoption of generative AI technologies. Although sociotechnical means for mitigating these harms have yet to be
developed, we recommend that designers use a Value Sensitive Design approach [34, 44] when reasoning about how to
design generative AI applications. By clearly identifying the different stakeholders and impacted parties of a generative
AI application, and explicitly enumerating their values, designers can make more reasoned judgments about how those
stakeholders might be impacted by hazardous model outputs, model misuse, and issues of human displacement.
4
LIMITATIONS AND FUTURE WORK
Generative AI applications are still in their infancy, and new kinds of co-creative user experiences are emerging at a
rapid pace. Thus, we consider these principles to be in their infancy as well, and it is possible that other important
design principles, strategies, and/or user aims have been overlooked. In addition, although these principles can provide
helpful guidance to designers in making specific design decisions, they need to be validated in real-world settings to
ensure their clarity and utility.
5
CONCLUSION
We present a set of seven design principles for generative AI applications. These principles are grounded in an
environment of generative variability, the key characteristics of which are that a generative AI application will generate
artifacts as outputs, and those outputs may be varied in nature (e.g. of varied quality or character). The principles
focus on designing for multiple outputs and the imperfection of those outputs, designing for exploration of a space or
range of possible outputs and maintaining human control over that exploration, and designing to establish accurate
mental models of the generative AI application via explanations. We also urge designers to design against the potential
harms that may be caused by hazardous model output (e.g. the production of inappropriate language or imagery, the
reinforcement of existing stereotypes, or a failure to inclusively represent different groups), by misuse of the model
(e.g. by creating disinformation or fabricating evidence), or by displacing human workers (e.g. by designing for the
replacement rather than the augmentation of human workers). We envision these principles to help designers make
reasoned choices as they create novel generative AI applications.
REFERENCES
[1] ACM. 2023. Words Matter: Alternatives for Charged Terminology in the Computing Profession. Retrieved 04-January-2023 from https://www.acm.
org/diversity-inclusion/words-matter
[2] Mayank Agarwal, Jorge J Barroso, Tathagata Chakraborti, Eli M Dow, Kshitij Fadnis, Borja Godoy, Madhavan Pallan, and Kartik Talamadupula.
2020. Project clai: Instrumenting the command line as a new environment for ai agents. arXiv preprint arXiv:2002.00762 (2020).
[3] Mayank Agarwal, Kartik Talamadupula, Stephanie Houde, Fernando Martinez, Michael Muller, John Richards, Steven Ross, and Justin D Weisz.
2020. Quality Estimation & Interpretability for Code Translation. In Proceedings of the NeurIPS 2020 Workshop on Computer-Assisted Programming
(NeurIPS 2020).
[4] Joshua Albrecht, Ellie Kitanidis, and Abraham J Fetterman. 2022. Despite" super-human" performance, current LLMs are unsuited for decisions
about ethics and safety. arXiv preprint arXiv:2212.06295 (2022).
[5] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori
Inkpen, et al. 2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–13.
[6] Cecilia Aragon, Shion Guha, Marina Kogan, Michael Muller, and Gina Neff. 2022. Human-Centered Data Science: An Introduction. MIT Press.
12

Toward General Design Principles for Generative AI Applications
HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
[7] Vijay Arya, Rachel KE Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C Hoffman, Stephanie Houde, Q Vera Liao, Ronny Luss,
Aleksandra Mojsilovic, et al. 2020. AI Explainability 360: An Extensible Toolkit for Understanding Data and Machine Learning Models. J. Mach.
Learn. Res. 21, 130 (2020), 1–6.
[8] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language
Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 610–623.
[9] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine
Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).
[10] Danah Boyd and Kate Crawford. 2012. Critical questions for big data: Provocations for a cultural, technological, and scholarly phenomenon.
Information, communication & society 15, 5 (2012), 662–679.
[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural
Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877–1901.
https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
[12] Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z Gajos. 2021. To trust or to think: cognitive forcing functions can reduce overreliance on AI in
AI-assisted decision-making. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–21.
[13] Matthew Butterick. 2022. GitHub Copilot Litigation. https://githubcopilotlitigation.com
[14] Andres Campero, Michelle Vaccaro, Jaeyoon Song, Haoran Wen, Abdullah Almaatouq, and Thomas W Malone. 2022. A Test for Evaluating
Performance in Human-Computer Systems. arXiv preprint arXiv:2206.12390 (2022).
[15] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying memorization across
neural language models. arXiv preprint arXiv:2202.07646 (2022).
[16] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21). 2633–2650.
[17] Ruijia Cheng, Ruotong Wang, Thomas Zimmermann, and Denae Ford. 2022. "It would work for me too": How Online Communities Shape Software
Developers’ Trust in AI-Powered Code Generation Tools. arXiv preprint arXiv:2212.03491 (2022).
[18] Vijil Chenthamarakshan, Payel Das, Samuel C Hoffman, Hendrik Strobelt, Inkit Padhi, Kar Wai Lim, Benjamin Hoover, Matteo Manica, Jannis
Born, Teodoro Laino, et al. 2020. Cogmol: target-specific and selective drug design for COVID-19 using deep generative models. arXiv preprint
arXiv:2004.01215 (2020).
[19] Vijil Chenthamarakshan, Payel Das, Inkit Padhi, Hendrik Strobelt, Kar Wai Lim, Ben Hoover, Samuel C. Hoffman, and Aleksandra Mojsilovic. 2020.
Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models. arXiv:2004.01215 [cs.LG]
[20] Jaemin Cho, Abhay Zala, and Mohit Bansal. 2022. Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers.
arXiv preprint arXiv:2202.04053 (2022).
[21] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014).
[22] Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, and Noah A Smith. 2018. Creative writing with a machine in the loop: Case studies
on slogans and stories. In 23rd International Conference on Intelligent User Interfaces. 329–340.
[23] Apple Computer. 2022. Human Interface Guidelines. https://developer.apple.com/design/human-interface-guidelines/guidelines/overview
[24] Sasha Costanza-Chock. 2020. Design justice: Community-led practices to build the worlds we need. The MIT Press.
[25] Paul Denny, Viraj Kumar, and Nasser Giacaman. 2022. Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using
Natural Language. https://arxiv.org/abs/2210.15157
[26] Norman K Denzin, Yvonna S Lincoln, Linda Tuhiwai Smith, et al. 2008. Handbook of critical and indigenous methodologies. Sage.
[27] Sebastian Deterding, Jonathan Hook, Rebecca Fiebrink, Marco Gillies, Jeremy Gow, Memo Akten, Gillian Smith, Antonios Liapis, and Kate Compton.
2017. Mixed-initiative creative interfaces. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems.
628–635.
[28] Catherine D’ignazio and Lauren F Klein. 2020. Data feminism. MIT press.
[29] Carl DiSalvo. 2022. Design as democratic inquiry: putting experimental civics into practice. MIT Press.
[30] Upol Ehsan, Philipp Wintersberger, Q Vera Liao, Elizabeth Anne Watkins, Carina Manger, Hal Daumé III, Andreas Riener, and Mark O Riedl.
2022. Human-Centered Explainable AI (HCXAI): beyond opening the black-box of AI. In CHI Conference on Human Factors in Computing Systems
Extended Abstracts. 1–7.
[31] Stephen M Fiore, Eduardo Salas, and Janis A Cannon-Bowers. 2001. Group dynamics and shared mental model development. How people evaluate
others in organizations 234 (2001).
[32] Paul M Fitts, MS Viteles, NL Barr, DR Brimhall, Glen Finch, Eric Gardner, WF Grether, WE Kellum, and SS Stevens. 1951. Human engineering for an
effective air-navigation and traffic-control system, and appendixes 1 thru 3. Technical Report. Ohio State Univ Research Foundation Columbus.
[33] Giorgio Franceschelli and Mirco Musolesi. 2022. Copyright in generative deep learning. Data & Policy 4 (2022).
[34] Batya Friedman and David G Hendry. 2019. Value sensitive design: Shaping technology with moral imagination. Mit Press.
13

HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
Weisz et al. 2023
[35] Werner Geyer, Lydia B Chilton, Justin D Weisz, and Mary Lou Maher. 2021. HAI-GEN 2021: 2nd Workshop on Human-AI Co-Creation with
Generative Models. In 26th International Conference on Intelligent User Interfaces-Companion. 15–17.
[36] Lisa Gitelman. 2013. Raw Data is an Oxymoron. MIT Press.
[37] Github. 2021. Copilot. Retrieved 03-August-2021 from https://copilot.github.com
[38] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative
adversarial networks. Commun. ACM 63, 11 (2020), 139–144.
[39] Imke Grabe, Miguel González-Duque, Sebastian Risi, and Jichen Zhu. 2022. Towards a Framework for Human-AI Interaction Patterns in Co-Creative
GAN Applications. Joint Proceedings of the ACM IUI Workshops 2022, March 2022, Helsinki, Finland (2022).
[40] Cobus Greyling. 2022. Prompt engineering, text generation and large language models. https://cobusgreyling.medium.com/prompt-engineering-
text-generation-large-language-models-3d90c527c6d5
[41] Matthew Guzdial, Nicholas Liao, Jonathan Chen, Shao-Yu Chen, Shukan Shah, Vishwa Shah, Joshua Reno, Gillian Smith, and Mark O Riedl. 2019.
Friend, collaborator, student, manager: How design of an ai-driven game level editor affects creators. In Proceedings of the 2019 CHI conference on
human factors in computing systems. 1–13.
[42] Gillian R Hayes. 2014. Knowing by doing: action research as an approach to HCI. In Ways of Knowing in HCI. Springer, 49–68.
[43] Will Douglas Heaven. 2022. Why Meta’s latest large language model survived only three days online. https://www.technologyreview.com/2022/
11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/
[44] David G Hendry, Batya Friedman, and Stephanie Ballard. 2021. Value sensitive design as a formative framework. Ethics and Information Technology
23, 1 (2021), 39–44.
[45] Eric Horvitz. 1999. Principles of Mixed-Initiative User Interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems
(Pittsburgh, Pennsylvania, USA) (CHI ’99). Association for Computing Machinery, New York, NY, USA, 159–166. https://doi.org/10.1145/302979.
303030
[46] Stephanie Houde, Vera Liao, Jacquelyn Martino, Muller Muller, David Piorkowski, John Richards, Justin D Weisz, and Yunfeng Zhang. 2020.
Business (mis)Use Cases of Generative AI. In Joint Proceedings of the Workshops on Human-AI Co-Creation with Generative Models and User-Aware
Conversational Agents co-located with 25th International Conference on Intelligent User Interfaces (IUI 2020).
[47] Kalin Hristov. 2016. Artificial intelligence and the copyright dilemma. Idea 57 (2016), 431.
[48] IBM. 2023. Racial Equity in Design. Retrieved 04-January-2023 from https://www.ibm.com/design/racial-equity-in-design/
[49] Maia Jacobs, Melanie F Pradier, Thomas H McCoy, Roy H Perlis, Finale Doshi-Velez, and Krzysztof Z Gajos. 2021. How machine-learning
recommendations influence clinician treatment selections: the example of antidepressant selection. Translational psychiatry 11, 1 (2021), 1–9.
[50] Tristan E Johnson, Youngmin Lee, Miyoung Lee, Debra L O’Connor, Mohammed K Khalil, and Xiaoxia Huang. 2007. Measuring sharedness of
team-related knowledge: Design and validation of a shared mental model instrument. Human Resource Development International 10, 4 (2007),
437–454.
[51] Benjamin Kaiser, Akos Csiszar, and Alexander Verl. 2018. Generative models for direct generation of cnc toolpaths. In 2018 25th International
Conference on Mechatronics and Machine Vision in Practice (M2VIP). IEEE, 1–6.
[52] Shalini Kantayya. 2020. Coded Bias. Retrieved 04-January-2023 from https://www.pbs.org/independentlens/documentaries/coded-bias/
[53] Bennett Kleinberg and Bruno Verschuere. 2021. How humans impair automated deception detection performance. Acta Psychologica 213 (2021),
103250.
[54] Steven Kollmansberger. 2010. Helping students build a mental model of computation. In Proceedings of the fifteenth annual conference on Innovation
and technology in computer science education. 128–131.
[55] Max Kreminski, Isaac Karth, Michael Mateas, and Noah Wardrip-Fruin. 2022. Evaluating Mixed-Initiative Creative Interfaces via Expressive Range
Coverage Analysis.. In IUI Workshops. 34–45.
[56] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S Liang. 2019. Spoc: Search-based pseudocode to
code. Advances in Neural Information Processing Systems 32 (2019).
[57] Q Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: informing design practices for explainable AI user experiences. In Proceedings
of the 2020 CHI Conference on Human Factors in Computing Systems. 1–15.
[58] Q Vera Liao, Moninder Singh, Yunfeng Zhang, and Rachel Bellamy. 2021. Introduction to explainable ai. In Extended Abstracts of the 2021 CHI
Conference on Human Factors in Computing Systems. 1–3.
[59] Antonios Liapis, Georgios N Yannakakis, Julian Togelius, et al. 2013. Sentient Sketchbook: Computer-aided game level authoring.. In FDG. 213–220.
[60] Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958
(2021).
[61] Silvia Lindtner, Shaowen Bardzell, and Jeffrey Bardzell. 2016. Reconstituting the utopian vision of making: HCI after technosolutionism. In
Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. 1390–1402.
[62] Vivian Liu and Lydia B Chilton. 2021. Neurosymbolic Generation of 3D Animal Shapes through Semantic Controls.. In IUI Workshops.
[63] Vivian Liu and Lydia B Chilton. 2022. Design Guidelines for Prompt Engineering Text-to-Image Generative Models. In CHI Conference on Human
Factors in Computing Systems. 1–23.
[64] Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry, and Carrie J Cai. 2020. Novice-AI music co-creation via AI-steering tools for deep
generative models. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1–13.
14

Toward General Design Principles for Generative AI Applications
HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
[65] Todd Lubart. 2005. How can computers be partners in the creative process: classification and commentary on the special issue. International
Journal of Human-Computer Studies 63, 4-5 (2005), 365–369.
[66] Alexandra Lyn. 2020. Risky Business: Artificial Intelligence and Risk Assessments in Sentencing and Bail Procedures in the United States. Available
at SSRN 3831441 (2020).
[67] Michael A Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co-designing checklists to understand organizational
challenges and opportunities around fairness in AI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1–14.
[68] Mary Lou Maher. 2012. Computational and collective creativity: Who’s being creative?. In ICCC. Citeseer, 67–71.
[69] Mary Lou Maher, Brian Magerko, Dan Venura, Douglas Fisher, Rogelio Cardona-rivera, Nancy Fulda, Johannes Gooth, Minwoo Lee, David Wilson,
James Kaufman, et al. 2022. A Research Plan for Integrating Generative and Cognitive AI for Human Centered, Explainable Co-Creative AI. In
ACM CHI Conference on Human Factors in Computing Systems.
[70] John E Mathieu, Tonia S Heffner, Gerald F Goodwin, Eduardo Salas, and Janis A Cannon-Bowers. 2000. The influence of shared mental models on
team process and performance. Journal of applied psychology 85, 2 (2000), 273.
[71] Edvinas Meskys, Julija Kalpokiene, Paulius Jurcys, and Aidas Liaudanskas. 2020. Regulating deep fakes: legal and ethical considerations. Journal of
Intellectual Property Law & Practice 15, 1 (2020), 24–31.
[72] Cade Metz. 2022. Meet GPT-3. It Has Learned to Code (and Blog and Argue). (Published 2020). https://www.nytimes.com/2020/11/24/science/
artificial-intelligence-ai-gpt3.html
[73] Jessica K Miller, Batya Friedman, Gavin Jancke, and Brian Gill. 2007. Value tensions in design: the value sensitive design, development, and
appropriation of a corporation’s groupware system. In Proceedings of the 2007 international ACM conference on Supporting group work. 281–290.
[74] Christoph Molnar. 2020. Interpretable machine learning. Lulu. com.
[75] Meredith Ringel Morris, Carrie J. Cai, Jess Holbrook, Chinmay Kulkarni, and Michael Terry. 2022. The Design Space of Generative Models. In
Proceedings of the NeurIPS 2022 Workshop on Human-Centered AI (NeurIPS 2022).
[76] Robert R. Morris. 2023. We provided mental health support to about 4,000 people — using GPT-3. Here’s what happened. Retrieved 07-Jan-2023 from
https://twitter.com/RobertRMorris/status/1611450197707464706
[77] Michael Muller, Plamen Agelov, Hal Daume, Q Vera Liao, Nuria Oliver, David Piorkowski, et al. 2022. HCAI@NeurIPS 2022, Human Centered AI.
In Annual Conference on Neural Information Processing Systems.
[78] Michael Muller, Lydia B Chilton, Anna Kantosalo, Charles Patrick Martin, and Greg Walsh. 2022. GenAICHI: Generative AI and HCI. In CHI
Conference on Human Factors in Computing Systems Extended Abstracts. 1–7.
[79] Michael Muller, Steven I. Ross, Stephanie Houde, Mayank Agarwal, Fernando Martinez, John T. Richards, Kartik Talamadupula, and Justin D.
Weisz. 2022. Drinking Chai with Your (AI) Programming Partner: A Design Fiction about Generative AI for Software Engineering 107-122. In
Joint Proceedings of the IUI 2022 Workshops: APEx-UI, HAI-GEN, HEALTHI, HUMANIZE, TExSS, SOCIALIZE co-located with the ACM International
Conference on Intelligent User Interfaces (IUI 2022), Virtual Event, Helsinki, Finland, March 21-22, 2022 (CEUR Workshop Proceedings, Vol. 3124), Alison
Smith-Renner and Ofra Amir (Eds.). CEUR-WS.org, 107–122.
[80] Michael Muller and Angelika Stroymayer. 2022. Forgetting Practices in the Data Sciences. In Proceedings of the 2022 CHI Conference on Human
Factors in Computing Systems. In press.
[81] Michael Muller and Justin Weisz. 2022. Extending a Human-AI Collaboration Framework with Dynamism and Sociality. In 2022 Symposium on
Human-Computer Interaction for Work. 1–12.
[82] Michael Muller, Justin D. Weisz, and Werner Geyer. 2020. Mixed initiative generative AI interfaces: An analytic framework for generative AI
applications. ICCC 2020 Workshop, The Future of Co-Creative Systems. https://computationalcreativity.net/workshops/cocreative-iccc20/papers/
Future_of_co-creative_systems_185.pdf
[83] Michael D Murray. 2022. Generative and AI Authored Artworks and Copyright Law. Available at SSRN (2022).
[84] Raja Parasuraman, Thomas B Sheridan, and Christopher D Wickens. 2000. A model for types and levels of human interaction with automation.
IEEE Transactions on systems, man, and cybernetics-Part A: Systems and Humans 30, 3 (2000), 286–297.
[85] Claudio Pinhanez. 2021. Expose Uncertainty, Instill Distrust, Avoid Explanations: Towards Ethical Guidelines for AI, in HCAI@NeurIPS 2021
workshop. https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2112.01281&sa=D
[86] Claudio Pinhanez. 2022. Breakdowns, Language Use, and Weird Errors: Past, Present, and Future of Research on Conversational Agents at BRL, in
IBM Research Cambridge Lab Guess Speaker Series.
[87] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents.
arXiv preprint arXiv:2204.06125 (2022).
[88] Anais Resseguier and Rowena Rodrigues. 2021. Ethics as attention to context: recommendations for the ethics of artificial intelligence. Open
Research Europe 1, 27 (2021), 27.
[89] Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of
the 2021 CHI Conference on Human Factors in Computing Systems. 1–7.
[90] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion
models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684–10695.
[91] Janus Rose. 2022. Facebook Pulls Its New ‘AI For Science’ Because It’s Broken and Terrible. Vice (November 2022). Retrieved 06-Jan-2023 from
https://www.vice.com/en/article/3adyw9/facebook-pulls-its-new-ai-for-science-because-its-broken-and-terrible
15

HAIGEN ’23 Workshop at IUI ’23, March 27-31, 2023, Sydney, NSW, Australia
Weisz et al. 2023
[92] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. 2023. The Programmer’s Assistant: Conversational
Interaction with a Large Language Model for Software Development. In 28th International Conference on Intelligent User Interfaces.
[93] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised Translation of Programming Languages.. In
NeurIPS.
[94] Devansh Saxena, Karla Badillo-Urquiola, Pamela J Wisniewski, and Shion Guha. 2021. A framework of high-stakes algorithmic decision-making for
the public sector developed through a case study of child-welfare. Proceedings of the ACM on Human-Computer Interaction 5, CSCW2 (2021), 1–41.
[95] Matthias Scheutz, Scott A DeLoach, and Julie A Adams. 2017. A framework for developing and using shared mental models in human-agent teams.
Journal of Cognitive Engineering and Decision Making 11, 3 (2017), 203–224.
[96] Isabella Seeber, Eva Bittner, Robert O Briggs, Triparna De Vreede, Gert-Jan De Vreede, Aaron Elkins, Ronald Maier, Alexander B Merz, Sarah
Oeste-Reiß, Nils Randrup, et al. 2020. Machines as teammates: A research agenda on AI in team collaboration. Information & management 57, 2
(2020), 103174.
[97] Thomas B Sheridan and William L Verplank. 1978. Human and computer control of undersea teleoperators. Technical Report. Massachusetts Inst of
Tech Cambridge Man-Machine Systems Lab.
[98] Ben Shneiderman. 2020. Human-centered artificial intelligence: Reliable, safe & trustworthy. International Journal of Human–Computer Interaction
36, 6 (2020), 495–504.
[99] Ben Shneiderman. 2021. Human-Centered AI. Issues in Science and Technology 37, 2 (2021), 56–61.
[100] Ben Shneiderman. 2022. Human-Centered AI. Oxford University Press.
[101] Auste Simkute, Aditi Surana, Ewa Luger, Michael Evans, and Rhianne Jones. 2022. XAI for learning: Narrowing down the digital divide between
“new” and “old” experts. In Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference. 1–6.
[102] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis,
Stephen Pfohl, et al. 2022. Large Language Models Encode Clinical Knowledge. arXiv preprint arXiv:2212.13138 (2022).
[103] Katta Spiel. 2021. ” Why are they all obsessed with Gender?”—(Non) binary Navigations through Technological Infrastructures. In Designing
Interactive Systems Conference 2021. 478–494.
[104] Angie Spoto and Natalia Oleynik. 2017. Library of Mixed-Initiative Creative Interfaces. Retrieved 19-Jun-2021 from http://mici.codingconduct.cc/
[105] Catherine Stupp. 2019. Fraudsters Used AI to Mimic CEO’s Voice in Unusual Cybercrime Case. The Wall Street Journal (August 2019). Retrieved
06-Jan-2023 from https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402
[106] Jiao Sun, Q Vera Liao, Michael Muller, Mayank Agarwal, Stephanie Houde, Kartik Talamadupula, and Justin D Weisz. 2022. Investigating
Explainability of Generative AI for Code through Scenario-based Design. In 27th International Conference on Intelligent User Interfaces. 212–228.
[107] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. Advances in neural information processing
systems 27 (2014).
[108] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is
all you need. Advances in neural information processing systems 30 (2017).
[109] James Vincent. 2022. Ai-generated answers temporarily banned on coding Q&A site stack overflow.
Retrieved 06-Jan-2023 from https:
//www.theverge.com/2022/12/5/23493932/chatgpt-ai-generated-answers-temporarily-banned-stack-overflow-llms-dangers
[110] Patrick von Platen. 2020. How to generate text: using different decoding methods for language generation with Transformers. Hugging Face Blog
(March 2020). Retrieved 06-Jan-2023 from https://huggingface.co/blog/how-to-generate
[111] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray.
2019. Human-AI collaboration in data science: Exploring data scientists’ perceptions of automated AI. Proceedings of the ACM on Human-Computer
Interaction 3, CSCW (2019), 1–24.
[112] Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. 2021. Towards mutual theory of mind in human-ai interaction: How
language reflects what students perceive about a virtual teaching assistant. In Proceedings of the 2021 CHI Conference on Human Factors in Computing
Systems. 1–14.
[113] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa
Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359 (2021).
[114] Justin D Weisz, Mary Lou Maher, Hendrik Strobelt, Lydia B Chilton, David Bau, and Werner Geyer. 2022. HAI-GEN 2022: 3rd Workshop on
Human-AI Co-Creation with Generative Models. In 27th International Conference on Intelligent User Interfaces. 4–6.
[115] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula.
2021. Perfection Not Required? Human-AI Partnerships in Code Translation. In 26th International Conference on Intelligent User Interfaces. 402–412.
[116] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, and John T Richards.
2022. Better together? an evaluation of ai-supported code translation. In 27th International Conference on Intelligent User Interfaces. 369–391.
[117] Alex Wilkins. 2022. Will AI text-to-image generators put illustrators out of a job? NewScientist (May 2022).
[118] Shuo Yang, Kai Shu, Suhang Wang, Renjie Gu, Fan Wu, and Huan Liu. 2019. Unsupervised fake news detection on social media: A generative
approach. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 5644–5651.
16

