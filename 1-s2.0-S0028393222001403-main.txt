Neuropsychologia 173 (2022) 108281
Available online 31 May 2022
0028-3932/Crown Copyright © 2022 Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
An informal reconstruction of the free-energy framework, examining the 
conceptual problems that arise 
Chris Thornton 
University of Sussex, Brighton, UK   
A R T I C L E  I N F O   
Keywords: 
Free energy 
Information 
Prediction 
Uncertainty 
Bayesian inference 
Approximate inference 
A B S T R A C T   
Recent decades have seen increasing attention given to the free-energy framework. This proposes that many of 
the phenomena we associate with life, intelligence and adaptation can be explained in terms of a single, 
mathematical precept: the free-energy principle. This is claimed to apply to the adaptive behavior of primitive 
organisms as much as it does to the physiological structures of human brains. The proposal is potentially of 
interest to theorists from multiple disciplines. But as presentations often make intensive use of mathematical 
notation, it can be hard to understand, even for experts. The present article presents an informal reconstruction, 
using schematic illustrations. Mathematical notation is largely avoided, while detail and precision are retained as 
far as possible. The specifically conceptual problems that come to notice in the reconstruction are highlighted 
and discussed.   
1. Part 1 
1.1. Introduction 
In recent years, research related to free energy has become increas­
ingly prominent (Colombo and Wright, 2018; Ramstead et al., 2020a, 
2020b; van Es, 2021; Wiese and Friston, 2021). This framework ad­
vances a remarkable proposal: that many forms of intelligent and 
adaptive behavior — among other phenomena — can be explained in 
terms of a single, precept: the free-energy principle (FEP). The proposal is 
likely to be of interest to any theorist concerned with the mind, cogni­
tion, and living things more generally. The difficulty is that it can be 
hard to understand, even for experts. As Herreror and Verschure note, 
‘for the broader audience of the uninitiated, the goal of FEP, its theo­
retical framework, and leverage may still be elusive’ (Herreros and 
Verschure, 2015, p. 218). 
Particularly challenging are the mathematical aspects of the pro­
posal. As Bogacz comments, ‘the description of mathematical details of 
the theory in these papers requires a very deep mathematical back­
ground’ (Bogacz, 2017, p. 199). In their presentation of the framework 
— itself a work involving more than 100 equations — Buckley et al. 
acknowledge that 
… [t]he mathematics involved is non-trivial and has been presented 
over different stages of evolution and using varying notations 
(Buckley et al., 2017, p. 56) 
The present article aims to address this difficulty by setting out an 
informal reconstruction of the framework, which largely avoids use of 
mathematical notation. It is not a free-energy publication of the tradi­
tional kind, or a tutorial in the conventional sense. It does not describe 
an experiment, or present a review of published critiques. The intention, 
rather, is to reconstruct the framework to a level of detail that allows an 
informed assessment to be developed, without significant use of math­
ematical formalism. 
Reconstructing a mathematically complex framework in this way is a 
challenging exercise that requires special measures to be taken. The 
approach taken here is strictly bottom-up and incremental. It is also to 
some degree historical, in that it recapitulates the key theoretical in­
novations in the order they were developed. The reconstruction is built- 
up layer by layer, with the simplest and longest-established element — 
Bayesian inference — set out first. Introduction of this basic constituent 
leads on to introducing approximate Bayesian inference, uncertainty 
reduction, and to exploring how the two are combined in free energy 
minimization. With this central core established, the focus moves on to 
the agent-level interpretation associated with the work of Karl Friston 
and colleagues. 
The reconstruction is assembled in this somewhat long-winded way 
E-mail address: c.thornton@sussex.ac.uk.  
Contents lists available at ScienceDirect 
Neuropsychologia 
journal homepage: www.elsevier.com/locate/neuropsychologia 
https://doi.org/10.1016/j.neuropsychologia.2022.108281 
Received 22 November 2021; Received in revised form 19 May 2022; Accepted 26 May 2022   

Neuropsychologia 173 (2022) 108281
2
in order to achieve the level of detail and precision normally secured 
through use of notation. The intent is that there should be no disconti­
nuities — a concept used at one level of the reconstruction should have 
been introduced previously. Content that is ambiguous should not be 
encountered. This approach also influences the evaluation of conceptual 
problems in Part 3. While difficulties affecting the free-energy frame­
work are subject to wide-ranging debate, the ones examined here are of 
a restricted type. It is specifically the conceptual problems that come to 
notice within the reconstruction that are considered. 
Also key strategically is the adoption of a ruling reference. The 
variation that exists between published presentations of the framework 
poses a problem for anyone wishing to understand it. To overcome this, 
it is useful to treat one source as definitive. For present purposes, the 
source so treated is (Buckley et al., 2017). One implication of this is that 
features of the framework introduced relatively recently are given no 
coverage. The treatment of applications is affected in similar fashion. 
Recent applications in areas such as metaphysics, intentionality and 
consciousness (Beni, 2021; van Es, 2021; Wiese and Friston, 2021) are 
given no attention.1 
The neutrality of the enterprise should also be emphasized. The aim 
is to present a reconstruction that enables an informed assessment to be 
formed, based on computational ideas rather than mathematical nota­
tion. It is not the intention to stipulate what the assessment should be. 
On the basis of what is set out, it should be possible to identify the 
specifically conceptual problems that arise, and judge whether they are 
damaging to the framework as a whole. 
The body of the paper is structured in three parts. The next section 
introduces Bayesian inference, the oldest and most fundamental con­
stituent of the framework. Following that, Section 1.3 introduces the 
concept of approximate Bayesian inference (i.e., use of approximation to 
avoid the difficulty of identifying observation probabilities). Section 1.4 
introduces the information metric of surprisal, and examines its poten­
tial use in minimizing the uncertainty of a posterior distribution. This 
leads on to identifying free energy minimization itself, and to examining 
its use in the Helmholtz machine formulation. Part 2 of the paper then 
sets out the agent-level reinterpretation of the process, also highlighting 
(Section 2.3) the role played by active inference. Part 3 addresses the 
specifically conceptual problems that come to notice in the reconstruc­
tion, and examines the origins of the framework’s exceptional 
versatility. 
1.2. Bayesian inference 
Fundamental to the free-energy framework is the long-established 
concept of Bayesian inference. (Readers familiar with this topic can 
safely skip to Section 1.3). As well as being a key element within the free- 
energy proposal, Bayesian inference is also relatively easy to explain. 
Consider the following example. Suppose that, in a box of avocados, 10 
are rotten, and we know that 50% of rotten avocados are soft to the 
touch. An interesting question then concerns an avocado that is found to 
be soft. What should we assume are the chances of it being rotten? It will 
be seen that the answer must depend on the total number of soft 
avocados. If there are 20, for example, 20 × 0.5 = 10 of these will be 
rotten. As these make up 100% of all rotten cases, it is seen there is a 
100% chance of a soft avocado being rotten. If there are only 10 soft 
avocados, these make up only half of all rotten cases, meaning there is 
only a 50% chance. 
This method of calculation can also use conditional and uncondi­
tional probabilities, in which form it is called Bayes’ rule or Bayes’ 
theorem. The method of derivation is termed Bayesian inference. The 
avocado scenario would be dealt with by stating the probability of an 
avocado being soft, the probability of it being rotten, and the conditional 
probability of it being rotten given it is soft. The conditional probability 
we’re interested in — the probability of a soft avocado being rotten — is 
then found by dividing the probability of seeing a soft, rotten avocado, 
by the probability of seeing a rotten avocado.2 The result obtained is as 
described above, except that the referenced quantities are now 
probabilities. 
An advantage of Bayesian inference is that it is completely general. It 
can be applied to any scenario involving states and observations.3 By 
combining state/observation probabilities with the conditional proba­
bility of the observation given the state, we can compute a probability 
that is usually more interesting: the probability of the state given the 
observation. This can be the basis for diagnosis in a medical scenario. 
Suppose the probability of seeing the skin condition psoriasis is 0.01, 
and the probability of seeing scaly skin — one of psoriasis’s main 
symptoms — is 0.005. If the conditional probability of psoriasis pro­
ducing scaly skin is 0.2, we can calculate the conditional probability that 
scaly skin indicates psoriasis. This is 0.01×0.2
0.005
= 0.4. The observation of 
scaly skin suggests there is a 40% chance of psoriasis. Diagnosis can 
proceed on this basis. (The figures are purely illustrative.) 
In practice, Bayesian inference is often used to find which of a set of 
states is most probable in light of given observations. The formula is 
applied to each individual case, and results are normalized to obtain an 
overall judgement. The process is given the name maximum a posteriori 
(MAP) inference, in accordance with the way quantities are labelled in 
Bayesian theory. A conditional probability derived using Bayes’ rule is 
termed a posterior probability (or just a posterior). The two uncondi­
tional probabilities are termed prior probabilities (or just priors) if they 
are provided in advance. The conditional probability of the observation 
given the state is termed the likelihood. In what follows, the product of 
the likelihood and the state probability (the numerator in Bayes’ for­
mula) will also be given a name. This will be termed the predicted 
observation probability or just the prediction. This and other names to be 
used are set out diagrammatically in Fig. 1. 
A problem often considered of interest involves the observation 
probability. Assuming this is not simply given in advance (as a prior), it 
has to be derived in some way. The difficulty is that in many situations 
there are infinitely many potential observations. Identifying the proba­
bility of a single one is then problematic at best. Even with a finite 
number of observations, calculating the probability of a single case in­
volves iterating over all situations that potentially apply, hence all 
Fig. 1. Bayes’ rule and its constituent terms, expressed in terms of state s and 
observation o. 
1 This also applies to Friston’s recent monograph A free energy principle for a 
particular physics, which attempts a theory of every “thing” that can be distin­
guished from other “things” in a statistical sense; see arxiv.org/ftp/arxiv/pa­
pers/1906/1906.10184.pdf. 
2 Given s = soft and r = rotten, this would be notated as p(r|s) = p(s|r)×p(r)
p(s)
.  
3 More accurately, it can be applied to any variables with the required 
probabilistic properties. For simplicity, this article will always use ‘state’ and 
‘observation’ respectively, in naming the conditioned and conditional variables 
used in calculating a posterior probability. 
C. Thornton                                                                                                                                                                                                                                      

Neuropsychologia 173 (2022) 108281
3
relevant likelihoods. This is the process of marginalization. Its compu­
tational complexity increases exponentially with the number of cases 
taken into account. The process is generally considered unacceptably 
costly for this reason (Bishop, 2007).4 Due to this, a way of proceeding 
that avoids the need to calculate the observation probability is generally 
considered desirable. 
1.3. Approximate Bayesian inference 
How can we calculate a posterior probability if we don’t know the 
observation probability on which it depends? For application of Bayes’ 
rule, it seems both the quantity to be divided, and the amount by which 
it is divided are required. It will be seen that the two quantities are 
closely related, however. Both specify the probability of the relevant 
observation. The denominator expresses it directly, while the numerator 
(i.e., prediction) expresses it as the product of two values: the proba­
bility of seeing a particular state, and the probability of the state pro­
ducing the observation. 
The way the value is constructed as a product has to be consistent 
with the way it is expressed as a single quantity, however. The latter 
cannot be greater than the former, for example, as a posterior greater 
than 1 would then result. There are also cases in which the two values 
are identical by definition. If the likelihood and state probability have 
the same extremal value (1 or 0), this must also be the observation 
probability. That these constraints arise is not surprising, as the obser­
vation probability depends entirely on the predicted probabilities taken 
overall (i.e., on their marginalization). The former expresses all of the 
latter in a summative way. 
This close relationship between numerator and denominator sug­
gests a solution to the problem at hand. A way of deriving posteriors can 
be envisaged, that gets around the unavailability of observation prob­
abilities by treating the numerator as a constraint. A method of this kind 
cannot deliver precise posterior probabilities — it cannot reproduce 
exact Bayesian inference. It can be the means of producing a reasonable 
approximation, however. It offers a way of performing Bayesian infer­
ence in an approximate way. The method is illustrated schematically in 
Fig. 2. 
1.4. Informational surprise 
Whether or not approximation is used, the effectiveness of Bayesian 
inference depends ultimately on the probabilities taken into account. 
They should lead to the correct state being identified in every case, and 
the identification should be made with minimum ambiguity. To achieve 
the latter, the distribution of probabilities over states should be as 
peaked as possible. To achieve the former, the peak should be in the 
right position. The worst case arises when the distribution is completely 
flat. Then there is no peak. With every state having the same posterior 
probability, the inference obtained is completely uncertain. 
The flatness of the posterior distribution is critical, then. For pur­
poses of measuring this, the free-energy framework uses a metric from 
information theory (Shannon, 1948; Shannon and Weaver, 1949). Key 
to this is recognition that probability and uncertainty have an inverse 
relationship. More of one always implies less of the other. (To illustrate: 
where an outcome is given greater probability, there is correspondingly 
less uncertainty. If, on the other hand, the outcome is given less prob­
ability, there must be correspondingly more uncertainty.) In practice, 
information theory defines the level of uncertainty associated with a 
particular probability as its negative logarithm. The level of uncertainty 
associated with probability p is defined as −
log p. This produces the 
required inverse variation. 
A way of assessing the uncertainty inherent to a posterior distribu­
tion can be obtained on this basis. As log values can be summed, a 
weighted average of the quantities arising provides what is required. 
There are some terminological pitfalls to be wary of, however. A nega­
tive log probability is often dubbed a surprise (Tribus, 1961), on the 
grounds that it quantifies the degree to which the relevant outcome 
would be surprising were it to occur. Intuitively, an outcome considered 
to have lower probability should produce a greater degree of surprise, 
and vice versa.5 Usage of this term is potentially misleading, though, as 
it implies the relevant outcome has occurred, whereas the probability 
itself does not. For this reason, the present approach will use the more 
neutral term surprisal in naming negative log-probability (cf. Weissbart 
et al., 2020). The associated misconception will also be named. Taking 
the derivation of a surprisal to imply the occurrence of an outcome will 
be termed the literal surprise fallacy. 
Using this information-theoretic definition, the uncertainty inherent 
to a distribution can be defined as a weighted average of the relevant 
surprisals6: 
∑
p
p × (−log p)
As a probabilistically weighted average this quantity can also be 
described as the expected surprisal. Shannon originally characterized it 
simply as the uncertainty (Shannon, 1949). When it was seen that the 
formula is mathematically equivalent to that used in physics to describe 
the disorder of a closed physical system, Shannon’s colleague, von 
Neumann, suggested that ‘entropy’ — the term used by physicists — 
should be adopted.7 The result is that the term entropy is now used quite 
generally to refer to average surprisal (Rioul, 2018). 
Using this formulation, the objective applying to the posterior dis­
tribution can be expressed precisely. Making the distribution as peaked 
as possible can be considered to involve minimizing its entropy. Putting 
it another way, maximizing the peakedness of the distribution is 
accomplished by minimizing its flatness, which is also its entropy. The 
first of the two over-arching objectives can be dealt with in this way — 
by requiring minimization of the posterior distribution’s entropy. 
Before moving on to consider the second objective, it is worth briefly 
examining the wider information-theoretic applications of these for­
mulations. In information theory, surprisal and entropy become the 
means of measuring quantities of information. In this case it is outcomes 
that do occur that are of interest. Where an outcome is awarded a 
probability implying a certain level of uncertainty, its occurrence has the 
effect of eliminating the uncertainty in question. Given this is 
Fig. 2. The prediction (predicted observation probability) viewed as a con­
straining influence on the observation probability. 
4 Buckley et al. note that the process involves ‘evaluating difficult integrals’ 
(Buckley et al., 2017, p. 57). In their work with the Helmholtz machine, Dayan 
et al. observe, the problem is to ‘compute expectations under the posterior 
distribution P which, in general, has exponentially many terms and cannot be 
factored into a product of simpler distributions’ (Dayan et al., 1995, p. 3). 
5 The effect can be illustrated using the avocado example. Say the chances of 
encountering a rotten avocado are extremely low (e.g., probability 0.01), the 
surprise produced by this outcome is then −
log 0.01 ≈4.6. Conversely, if the 
chances are high (probability 0.8, say), the surprise is then lower: −
log 0.8 ≈
0.2.  
6 The formula is normally simplified to −
∑
pp log p.  
7 According to legend, von Neumann advised Shannon that ‘ … no one knows 
what entropy really is, so in a debate you will always have the advantage’ 
(Tribus and McIrvine, 1971). 
C. Thornton                                                                                                                                                                                                                                      

Neuropsychologia 173 (2022) 108281
4
informative by definition, the quantification of uncertainty can be 
viewed as measuring the information produced by (or contained within) 
the outcome itself. As Shannon (1948) demonstrated, a general frame­
work for quantifying information can be developed in this way. 
Central to it is the metrical use of logarithmic quantities. With logs 
taken to base n, a surprisal is expressed in well-defined units, each being 
the uncertainty arising for n equiprobable outcomes. Rounded down, the 
surprisal is then the number of digits in a base n encoding needed to 
represent the corresponding uncertainty — it is the number of alterna­
tives that would create the uncertainty in question. Uncertainties 
become the basis for information-measurement in this way. Use of base 
2 leads to the familiar terminology of ‘bits’. With the uncertainty of an 
outcome measured using logs taken to base 2, the outcome’s information 
content corresponds to a number of digits in a binary encoding. The 
information content of the outcome can then be characterized as a 
certain number of ‘binary digits’ or ‘bits’. 
1.5. Helmholtz free energy 
What of the second over-arching objective? As well as being maxi­
mally peaked, the posterior distribution also needs to be peaked at the 
right point. Inferred states should not only be minimally uncertain, they 
should also be correct. How can these two objectives be achieved 
simultaneously? Information theory again leads to a solution. The so- 
called Kullback-Leibler (KL) divergence (Kullback and Leibler, 1951; 
Kullback, 1959; see also Buckley et al., 2017, p. 59, Eq. 6) identifies the 
informational divergence between two distributions. It measures the 
informational cost of using one distribution in place of another. Calcu­
lating the KL divergence between the posterior distribution and its true 
counterpart produces the informational cost of any incorrectness. At the 
same time, the entropy of the distribution reveals the informational cost 
of any uncertainty. The two objectives can be captured in a single cri­
terion in this way. Minimizing the overall informational cost of the 
posterior distribution — taking both factors into account — will have 
the affect of accomplishing both objectives at once. 
Historically, the idea of minimizing such costs to satisfy both pre- 
requisites stems from work that imported a comparable method from 
statistical physics. The method in question was also shown to produce 
the approximation effect of Section 1.3. Problematically, the terminol­
ogy used by physicists was imported at the same time, with the result 
that informational costs came to be characterized in terms of energies. 
In a series of publications, Geoffrey Hinton and colleagues showed 
(Hinton and Sejnowski, 1986; Zemel and Hinton, 1991; Hinton and 
Zemel, 1994; Dayan et al., 1995; Dayan and Hinton, 1996) that a 
method used in statistical physics for dealing with the states of a physical 
system can be applied to the task of optimizing approximate Bayesian 
inference. The method involved minimization of a combined quantity, 
akin to that described above, by exploitation of a physical analogy. 
Inferring the state that gives rise to an observation can be related to the 
task of determining the configuration of a physical system. As Dayan 
et al. (1995) observe, ‘if we view the alternative explanations of an 
example as alternative configurations of a physical system there is a 
precise analogy with statistical physics’ (Dayan et al., 1995, p. 2). 
The method targets reduction of uncertainty, quantified using 
negative log-probabilities. But variables are described using the termi­
nology of statistical physics. The core quantity in the framework is the 
negative log of what is here called the prediction (the expected proba­
bility of a particular observation arising from a particular state). In the 
case of state s and observation o, the quantity of interest is 
−log p(o|s)p(s)
Using information-theoretic terminology, we would describe this as 
the surprisal of the prediction.8 Adopting the terminology of statistical 
physics, Dayan et al. (1995) called it the energy of the observation, see 
Fig. 3 (In Dayan et al.‘s framework, states are called explanations, and 
observations are examples.). The quantity to be reduced is then seen to be 
the expected energy rather than the expected surprisal. 
In practice, Dayan et al.‘s method took the critical quantity to be the 
excess of the expected energy over the expected surprisal.9 Intuitively, 
this is the degree to which the uncertainty of a prediction exceeds un­
certainty about the states from which the prediction derives. This is an 
uncertainty excess that can only result from ambiguity introduced by the 
likelihoods. It is, in some sense, a form of statistical noise. In statistical 
physics, the excess is termed free energy, or more specifically Helmholtz 
free energy. From the informational perspective, it will be seen that it 
quantifies the degree to which predictions are unnecessarily uncertain. 
Free energy can be characterized as introduced uncertainty, then. More 
generally, it can be characterized as statistical noise. 
Fig. 4 presents an illustration of the calculation. Notice that it treats 
inaccuracy (expected energy) and uncertainty (expected surprisal) as 
costs in the same (informational) currency. A minimization focussing 
solely on the excess of one quantity over the other then has the effect of 
achieving two objectives at once. As James comments, ‘operationaliza­
tion of this “free energy” definition leads to a model where the problem 
of maximizing two outcomes is re-cast as maximizing one’ (James III, 
2015, p. 220). 
Notice how the approach also eliminates use of observation priors. 
As the formulation relies exclusively on state and prediction probabili­
ties, the approximation effect of Section 1.3 is realized. The known 
prediction probability allows convergence towards the unknown 
observation probability. The posterior distribution obtained is merely an 
approximation of the true posterior distribution in consequence. A result 
from Thompson (1988), however, shows that free energy calculated in 
this way will always exceed that derived using the true posterior (Dayan 
et al., 1995, p. 893). Minimizing the quantity in this way thus serves to 
satisfy all relevant goals. Introduced uncertainty is progressively elimi­
nated, while approximate posterior distributions are made to converge 
towards their true counterparts. 
1.6. The Helmholtz machine 
This way of optimizing distributions can be put to use in different 
ways. It is potentially applied to any scenario of a deductive or infer­
ential kind. It can be deployed as a way of learning patterns, for 
example. In this case, the predictive uncertainty at issue derives from 
prediction of visual data. Reducing this uncertainty improves the ac­
curacy of predictions, securing acquisition of the patterns in question. 
The method can be viewed as a form of unsupervised learning (i.e., 
learning from data alone, without use of classified examples). 
Fig. 3. The prediction surprisal named as an energy (cf. Fig. 2).  
8 In Kirby’s view the ‘energy is the surprise associated with the occurrence of 
a particular complete state’ (Kirby, 2006, p. 10).  
9 Dayan and Hinton characterize free energy as ‘the difference between the 
expected energy of an explanation [state] and the entropy [expected surprisal] 
of the probability distribution across explanations [states]’ (Dayan et al., 1995, 
p. 2). 
C. Thornton                                                                                                                                                                                                                                      

Neuropsychologia 173 (2022) 108281
5
This form of processing is illustrated by Dayan et al.‘s Helmholtz 
machine (Dayan et al., 1995; Dayan and Hinton, 1996). An unsupervised 
learning system implemented as a neural network, this aims to develop a 
generative model10 of presented patterns. With these treated as obser­
vations, minimizing free energy has the effect of inducing states that best 
serve prediction. A predictive model of the data is then derived.11 
In practice, the Helmholtz machine is somewhat more complex in 
that it develops two models simultaneously: 
A recognition model is used to infer a probability distribution over the 
underlying causes from the sensory input, and a separate generative 
model, which is also learned, is used to train the recognition model. 
(Dayan et al., 1995, p. 1, emphasis added). 
Minimizing free energy in both models encourages their alignment, 
both with each other and with presented patterns. The generative model 
develops to produce patterns that the recognition model recognizes, 
while the recognition model develops to recognize the patterns that the 
generative model produces. The generative model can be thought of as 
‘dreaming’ patterns, and the recognition model as ‘sensing’ them; the 
two phases may be described as the ‘sleep’ and ‘wake’ phase accordingly 
(Dayan and Hinton, 1996).12 
2. Part 2 
2.1. The agent-level interpretation 
The method of free-energy minimization developed by Hinton, 
Dayan and colleagues was put forward as a computationally tractable 
way of solving a problem in statistical inference.13 Subsequent to the 
dissemination of this work,14 Friston and colleagues began to explore an 
agent-focused interpretation (Friston, 2003, 2005, 2009; Friston et al., 
2006, 2009, 2011; Friston and Stephan, 2007; Hohwy et al., 2008; 
Feldman and Friston, 2010; Friston et al., 2012a, 2012b; Buckley et al., 
2017; Allen and Friston, 2018). The possibility of treating the method as 
a model of perceptual inference was examined. Its potential to provide 
the basis of a broader account of cognitive agency was also pursued. 
The reconceptualization in question has several foundations. One 
involves the inferential process itself. Bayesian inference produces the 
most probable state for a given observation. By equating the observation 
with a pattern of sensory input, and the state with a perceptual inter­
pretation, this statistical calculation can be conceived as a form of 
perceptual interpretation. A mathematical model of the perceptual act 
can be obtained in this way (van Rooij et al., 2018). As a perceptual 
interpretation and a pattern of sensory inputs are generally complex 
entities, treating each as a single variable simplifies the situation to some 
degree, however. 
A second foundation involves use of surprisals. Reducing free-energy 
in the case of a single observation involves reducing the contribution 
made by the observation surprisal. If surprisal is taken to quantify sur­
prise in a literal sense, an interpretation involving surprise avoidance can 
be framed. An agent that minimizes free-energy can be seen as refining 
its perceptual capacities so as to avoid perceptual surprises. A still 
broader generalization can also be proposed. If perceptual behavior can 
be modelled as the attempt to avoid surprises, why not cognitive 
behavior more generally? Can we not say that all behaviors of an agent 
reflect avoidance of surprise in some sense? If so, minimization of free- 
energy is potentially repositioned as a general model of cognitive 
behavior. 
One objection is definitional in nature. It will be seen that free- 
energy is not defined purely in terms of surprisal. Relative to a single 
observation, it is defined as the sum of the observation surprisal and the 
KL divergence between the true and approximate posterior distributions 
arising (see Fig. 5). Equating free-energy with surprisal is generally 
based on the way the latter bounds the former, therefore. Free-energy is 
noted to provide an upper bound on surprisal.15 
This relationship exists by definition. As the sum of two quantities 
cannot be less than either, free energy cannot be less than the obser­
vation surprisal. That free-energy ‘bounds’ surprisal is a consequence of 
the latter being one of the constituents in terms of which the former is 
defined. 
Although the two quantities still have the potential to diverge, it is 
the identification of free-energy with surprisal that largely sets the stage 
for viewing free-energy minimization as a governing principle in 
cognitive behavior. Given the assumption that surprisals quantify sur­
prise in a literal sense (see above), free-energy minimization can be 
viewed as the means of ensuring survival through the avoidance of 
surprises. 
Free-energy theorists take surprise-avoidance to be key on this basis, 
with the result that ‘minimization of expected free energy is referred to 
as minimizing surprise’ (James III, 2015, p. 220). The effect may be 
characterized in different ways, however. In (van Es, 2021, p. 316) it is 
Fig. 4. Free energy as introduced uncertainty (cf. Fig. 3).  
Fig. 5. Decomposition of free energy into surprisal and divergence.  
10 A generative model is one that includes the distribution of the modelled 
data, such that the probability of any example can be determined.  
11 Use of generative models in unsupervised learning has a long history. The k- 
means algorithm offers a simple example (MacQueen, 1967). This is a 
computationally efficient way of finding a clustering of datapoints. Processing 
is begun by randomly creating k cluster centres (called centroids), and assigning 
each datapoint to its closest centroid. The centroids — which form the gener­
ative model in this context — are then adjusted to better represent the data­
points captured. Next, the assignment of datapoints to centroids is adjusted 
using the new centroid positions. The process repeats until no further change is 
made. In the expectation-maximization (EM) algorithm of Dempster et al. 
(1977), the capture of datapoints is represented probabilistically. What is ob­
tained is then a ‘soft’ non-partitional clustering of the data (Neal and Hinton, 
1998).  
12 In Kirby’s view, ‘Helmholtz machines are artificial neural networks that, 
through many cycles of sensing and dreaming, gradually learn to make their 
dreams converge to reality, and, in the process, create a succinct model of a 
fluctuating world’ (Kirby, 2006, p. 2).  
13 The approach gave rise to ‘variational Bayes’ approaches in machine 
learning (MacKay, 1995; Winn et al., 2005).  
14 Hinton went on to play a leading role in developing deep learning, a 
mainstay of modern machine learning (Bengio, 2009; LeCun et al., 2015). 
15 Relevant to this is the assumption that reduction of free energy cannot 
eliminate surprisal completely. Buckley et al. note that, while the mathematical 
reduction of free energy ‘ … furnishes the organism with an approximation of 
surprisal it does not minimize it. Instead the organism can minimize VFE further 
by minimizing surprisal indirectly by acting on the environment and changing 
sensory input’ (Buckley et al., 2017, p. 59). 
C. Thornton                                                                                                                                                                                                                                      

Neuropsychologia 173 (2022) 108281
6
noted that ‘by minimizing free energy, the organism will indirectly also 
minimize surprise in the long run.’ Buckley and Tschantz state that ‘to 
ensure survival [agents] must simply avoid surprising situations’ 
(Buckley and Tschantz, 2019). Buckley et al. (2017), write that agents 
must ‘ … minimize the occurrence of events which are atypical (‘sur­
prising’) in their habitable environment’ (Buckley et al., 2017, p. 56). 
Friston states that the agent’s ‘model of the world (or the form of an 
agent) is optimum when it minimizes surprise’ (Friston, 2010a, p. 2), 
while Seth records that ‘organisms — in virtue of their survival — must 
avoid surprising states’ (Seth, 2014, pp. 270–271). 
Identification of survival with surprise-avoidance is not universally 
accepted, however. It is noted that natural agents often do not behave in 
surprise-averse ways (Fiorillo, 2010; Sun and Firestone, 2020). It is also 
argued 
that 
some 
forms 
of 
surprise-avoidance 
must 
be 
counter-productive for survival (Colombo and Wright, 2018). For 
example, an agent might eliminate the risk of being surprised by 
confining itself to a part of the environment where sensory stimulation is 
blocked out. In the literature, this objection is termed the ‘dark room 
problem’, on the grounds that a locale of this kind is akin to a dark room 
(Friston et al., 2012a, 2012b; Klein, 2018; Sun and Firestone, 2020; See 
also section 3.2.1.) 
Free-energy theorists support their position by introducing a second, 
closely related argument. This focuses on the effect that free-energy 
minimization has, not on individual surprisals, but on average sur­
prisal. As noted, the formula defining this is identical to that defining 
physical entropy, which is considered a measure of physical disorder. 
This leads to the idea that free-energy minimization might be the means 
by which a physical agent maintains its physical order. On this basis, it 
can be proposed that agents minimize free-energy as a consequence of 
their existence, rather than as a means of maintaining it. Coming to grips 
with this argument requires a brief examination of the second law of 
thermodynamics. 
2.2. The second law of thermodynamics 
Imagine a bathtub is filled using the hot tap alone. Initially, the 
hottest water tends to congregate near the tap. Over time, the temper­
ature evens-out, eventually becoming completely uniform. Physical 
systems that are orderly in structure and well separated from their 
environment are often found to behave in this convergent way. This 
gives rise to the second law of thermodynamics, which states that the 
entropy (homogeneity) of an isolated physical system will always evolve 
towards its maximum (Uffink, 2001). 
Since at least (Schroedinger, 1944), it has been noted that living 
things appear to violate this physical law (Gatlin, 1972). Their physical 
order seems to be maintained, rather than progressively lost (Boltz­
mann, 1974). On this basis, it may be envisaged that a fundamental 
property of living things is that they resist the increase of physical dis­
order implied by the second law. Friston and Stephan (2007, p. 421), for 
example, state that ‘biological systems and especially neuronal systems 
appear to contravene the second law of thermodynamics’, a claim that is 
seen by Bruineberg et al. (2018) to be an information-theoretic exten­
sion of (Schroedinger, 1944). 
Building on this idea, free-energy theorists develop a new justifica­
tion for viewing free-energy minimization as the governing principle in 
cognitive behavior. Since the formula defining entropy is identical to 
that defining average surprisal, minimization of free-energy can be seen 
as having the effect of minimizing entropy which, on the argument 
above (and in the case of a living agent), can be seen as an entailment of 
existence. This suggests that living agents minimize free-energy 
necessarily. 
Again there are difficulties. The argument stemming from (Schroe­
dinger, 1944) is that living things resist increase of entropy, not that they 
pursue its decrease. The process that free-energy theorists have in mind 
involves minimization. Another objection involves the distinction be­
tween sensory processing and physical constitution. The entropy 
quantified in free-energy measurement stems from observational un­
certainty, rather than physical disorder. The same formula is applied, 
but to different data. The distinction between physical and sensory en­
tropy seems important nevertheless. Free-energy theorists deal with this 
by treating sensory processing as a transition through sensory states. On 
this basis, reduction of sensory entropy can be related to reduction of 
physical entropy. 
Underlying these difficulties is a more conceptual concern. If the 
uncertainty of sensory processing is identified with the disorder of 
physical constitution, there are implications for agents that experience 
increasing uncertainty. According to the proposal, an agent experi­
encing a change of this kind must suffer increasing physical disorder. 
The prediction is that an increasingly uncertain agent will slowly 
disintegrate. Dubbed the disintegrating agent problem, this is further dis­
cussed in Section 3.2.4.16 
Setting these difficulties aside, the potential utility of the second law 
will be apparent. A new way of conceptualizing the process of free- 
energy minimization can be envisaged. Rather than seeing free-energy 
minimization as a general survival technique, it can be seen as an 
entailment of physical existence in the case of living things. Free-energy 
minimization can be viewed as fundamental in two different ways then. 
It can be supposed that this is how agents — in Friston’s words17 — 
‘avoid surprises and last longer’. Alternatively, it can be argued that 
agents are effectively compelled to operate in this way by the ‘very fact 
of their existence’ (Buckley et al., 2017, p. 57). 
2.3. Active inference 
The free-energy framework has now been constructed in rough 
outline. It is possible to move on to consider some of its more detailed 
aspects. The core of the proposal, recall, is that minimization of free- 
energy governs behavior of all kinds, in a process that can be viewed 
either as the secret to survival, or an entailment of physical existence. 
The model is seen to apply not only to internal (sensory/perceptual) 
behavior: avoidance of surprise is also considered to govern action. Free- 
energy theorists argue that action can be the means of avoiding surprises 
not only directly, but also indirectly, by changing the environment to 
make it less surprising. 
Avoiding surprises in the latter way — by modifying the environ­
ment to satisfy expectations — is termed active inference within the 
framework (Friston et al., 2017a, 2017b). Buckley et al. distinguish this 
form of behavior as follows: 
… while perception minimises VFE [free energy] by changing brain 
states to better predict sensory data, action instead acts on the 
environment to alter sensory input to better fit sensory predictions. 
(Buckley et al., 2017, p. 65) 
This inverts the traditional model of inference. Whereas prediction is 
normally evaluated against existing state, here, new state is produced to 
conform to what is predicted. Rather than inference being a purely in­
ternal activity, it becomes a potentially external activity, affecting states 
of the world. 
For purposes of integrating active inference into the free-energy 
framework, some new features have to be introduced. The key 
requirement is for an implementation mechanism. An arrangement is 
needed via which free-energy minimization can give rise to actions of 
16 Invoking thermodynamic laws in biological contexts is recognized to be 
troublesome. Morowitz (1981, p. 1) notes that ‘[t]he use of thermodynamics in 
biology has a long history rich in confusion’ (Morowitz, 1981, p. 1). In his 
opinion, such attempts have generally not been fruitful: ‘the audacious attempt 
to reveal the formal equivalence of the ideas of biological organization and 
thermodynamic order … must be judged to have failed’ (Medawar, 1984, p 
226).  
17 See Friston et al. (2012a, p. 2.). 
C. Thornton                                                                                                                                                                                                                                      

Neuropsychologia 173 (2022) 108281
7
the agent, which can then produce the requisite changes in the envi­
ronment. It might seem that something akin to a full-fledged agent/ 
environment simulation must be required, with time, actions and action 
sequences all independently coded, such that achievement of environ­
mental results can be represented and computed in a precise way.18 To a 
considerable extent, however, changes of this complexity are avoided. 
Free-energy theorists envisage an arrangement in which the current 
action of the agent is considered part of environmental state. On this 
basis, an action of the agent can be represented in a simple way, as the 
making of a prediction. Action sequences can also take a convenient 
form: they can be seen to arise when one prediction induces an action 
that creates a state which encodes a further action, in an ongoing way. 
The complication is that minimization benefits may then be offset 
arbitrarily far into the simulated future. The objective becomes mini­
mization of predicted future surprise (Friston et al., 2017a, 2017b), 
necessitating a redefinition of the minimization criterion. Minimization 
must be defined, not in terms of free-energy itself, but in terms of ex­
pected free-energy. Roughly, this is a weighted average of the 
free-energies (of observations) that are obtained when the agent pursues 
a particular sequence of actions. 
It might seem that modelling action in this way will involve differ­
entiating agent from environment. It is key to the framework, however, 
that they are representationally indivisible. The framework assumes the 
existence of just two definitional constructs: the R-density and the G- 
density. The R-density is characterized as ‘a probabilistic representation 
of what caused a particular sensation’ (Friston, 2010b, p. 128). The 
G-density, in contrast, is said to encode ‘the brain’s beliefs about envi­
ronmental causes’ of sensory input (Buckley et al., 2017, p. 61): 
… the organism needs some implicit assumptions about how 
different environmental states shape sensory input. These assump­
tions are presumed to be in the form of a … joint probability density 
between sensory data and environmental variables, the “generative 
density”, or G-density. (Buckley et al., 2017, p. 57) 
The G-density is the definitive structure. The R-density simply ex­
presses the posterior probabilities the G-density gives rise to.19 The G- 
density may be seen as encoding the agent’s ‘beliefs’ about the envi­
ronment (as above), or alternatively as its ‘model’ of the environment (e. 
g. Seth, 2014, p. 271; Buckley et al., 2017, p. 57). 
The unexpected feature of this arrangement is the lack of any inde­
pendent representation of the environment. The state of the external 
world is defined, in effect, by the agent’s beliefs. The world is as the 
agent believes it to be at all times. This fusion of agent and environment 
is not entailed by the concept of active inference, which is stated at a 
level of abstraction that makes no assumptions about defining data 
structures. In the free-energy framework, it cannot be avoided. Free- 
energy is a property of a single data structure, constituted of the likeli­
hoods and priors expressing a posterior distribution. (A structure of this 
kind is termed a posterior mapping below.) If the agent’s behavior in an 
environment is considered to be governed by minimization of free- 
energy, it is inevitable that all aspects of the process must be repre­
sented by this one structure. 
Fusion of agent and environment offers advantages, however. At a 
theoretical level, it can be seen to connect the framework to approaches 
that emphasize concepts of extension, embodiment and enaction 
(Varela et al., 1991; No¨e, 2004; Clark, 2013; Allen and Friston, 2018), 
although the validity of such connections remains controversial 
(Di Paolo et al., 2022; See also Hutto, 2018). At a more practical level, it 
offers a natural way to capture goal-directed behavior. 
With agent and environment fused in the envisaged way, a state of 
the world that has (or acquires) high surprisal will naturally elicit ac­
tions that tend to reduce that surprisal. A state or feature of the envi­
ronment that is unexpected will tend to elicit actions that render it less 
so. The result is seen to be goal-directed behavior. Buckley et al. char­
acterize the effect in the following way: 
expectations in the organism’s G-density (its “beliefs” about the 
world) cannot be met directly by perception and thus an organism 
must act to satisfy them. In effect these expectations effectively 
encode the organism’s desires on environmental dynamics. (Buckley 
et al., 2017, p. 57, p. 57) 
Fusion of agent and environment leads to a simple account of goal- 
directed behavior, then. There are some conceptual drawbacks, how­
ever. The arrangement has an element of circularity, in suggesting the 
agent takes action in accordance with its beliefs, while also changing its 
beliefs in accordance with its actions. The objective remains to improve 
predictability through reduction of free energy; but this is now under­
stood to be achieved in a way that induces actions that the agent believes 
will eliminate the free energy originally reduced. Friston et al. (2012b), 
for example, observe that the process 
… requires an agent to select or sample sensations that are predicted 
and believe that this sampling will minimize uncertainty about those 
predictions. (Friston et al., 2012b, p. 1, emphasis removed) 
The absence within the account of an independently represented 
world may give concern, however. Treating the environment as a 
product of the agent’s beliefs seems to allow no scope for genuine sur­
prises. If, at all times, the state of the environment is what the agent 
believes it to be, surprises would seem to be ruled out in principle. 
Quantities of surprisal can certainly be computed. These are just ways of 
expressing the probabilities the agent awards. But given the represen­
tational fusion of agent and environment, surprise of a genuine kind 
would seem to be unachievable. The concern is that, in this respect, the 
framework may fall foul of the literal surprise fallacy. 
3. Part 3 
3.1. Review of levels 
It is possible to step back now, and review the reconstruction that has 
been set out. Fig. 6 shows its division into six levels. The lower four 
levels involve the optimization method for approximate Bayesian 
inference specified in Dayan et al. (1995), in which calculations of 
Helmholtz free energy serve to quantify introduced uncertainty. The 
upper two levels relate to the agent-level interpretation, in which 
free-energy minimization is cast as a general principle governing agent 
physiology and behavior (e.g. Friston, 2003, 2005; Friston et al., 2006; 
Friston and Stephan, 2007; Friston, 2009). 
The most fundamental element in the reconstruction is the Bayesian 
method of inference (see Level 1). This specifies how state and obser­
vation probabilities can be combined with the conditional probability of 
the observation given the state, to derive the posterior probability of the 
state. To this basic foundation is added the concept of approximate 
Bayesian inference (Level 2). This is a way of avoiding the difficulties 
that arise if the number of potential observations is problematically 
large. The method exploits the degree to which the prediction proba­
bility (likelihood × state probability) constrains the observation 
probability. 
The next level (Level 3) introduces the informational concept of 
uncertainty, and addresses the way in which minimizing the average 
18 It is noted in (van Es, 2021, p. 316) that the concept of active inference 
‘assumes we have a probability distribution over possible action policies of the 
organism’.  
19 Friston notes (personal communication) that ‘there is only one generative 
model or G-density in play … The resulting posterior density is sometimes 
called the recognition density or R-density … the G-density is necessary to 
define the R-density … The R-density is just the approximate posterior density 
over hidden states, under a G-density.’ 
C. Thornton                                                                                                                                                                                                                                      

Neuropsychologia 173 (2022) 108281
8
uncertainty of a posterior distribution minimizes the ambiguity of the 
inferences forthcoming. Use of free energy is then introduced (Level 4). 
Attention here is given to the way this quantity supports an optimization 
which has the effect of minimizing uncertainty, maximizing accuracy, 
while also — implicitly — exploiting posterior approximation. 
The two higher levels (Level 5–6) of the reconstruction deal with the 
agent-level conception put forward by Friston and colleagues. This 
material largely focuses on the sense in which minimization of free en­
ergy can serve as a general principle governing (at least) perceptual 
inference and cognitive behavior. 
It should be emphasized that the reconstruction is far from complete 
with respect to the proposal broadly conceived. Not all aspects of the 
framework have been taken into account. It is noted that ‘free-energy 
theorists attempt an enormous variety of derivations’ (Colombo and 
Wright, 2018, p. 22). The span of the framework is vast in result. For 
present purposes, many parts of it have been passed over without 
comment, and these elements may bear on the issues raised. Acquisition 
of the G-density is a case in point. For present purposes, the assumption 
has been made, that the G-density provides the fixed constraint against 
which minimization proceeds. The G-density has been assumed to have 
the status of a construct supplied in advance. 
As Buckley et al. note, however, the formalism can be extended to 
show ‘how the brain could modify and learn the G-density’ (Buckley 
et al., 2017, p. 58). Under such an extension, a new factor is brought into 
play, and the assessment of problems might change accordingly. It is 
assumed (Buckley, personal communication) that learning of the 
G-density would also be mediated by minimization of free energy. But 
since this process requires distributions that are themselves defined by a 
G-density, this seems to create an intractable requirement for an infinite 
series of G-densities. An alternative would be to adopt the approach 
taken in the Helmholtz machine, in which the generative model is itself 
variable under minimization of free-energy. This would seem to rule out 
treating the G-density as a way of encoding the agent’s goals, however. 
Another issue overlooked involves acquisition of precisions. These are 
quantities that are proposed to modulate predictive probabilities. It is 
assumed that the probability assigned to an observation or state can be 
awarded a certain precision, and that this endows the probability with a 
certain level of confidence. A probability given a high (low) precision is 
deemed to be one for which confidence is high (low). Free-energy the­
orists envisage these quantities can also be acquired through minimi­
zation of free energy. This part of the framework has seen considerable 
development in recent years. For Buckley et al., however, the outlook 
remains unclear: 
Supposedly the appropriate regulation of precision should also 
emerge as a consequence of the minimisation of free energy … But 
how the interplay between brain states and precisions will unfold in 
an active agent involved in a complex behaviour is far from clear. 
(Buckley et al., 2017, p. 74) 
Issues involving the R-density have also been paid no attention. It is 
generally assumed that the R-density is represented as a mixture of 
Gaussian distributions. There are concerns about the generality of the 
approach. As Buckley et al. acknowledge, ‘it remains an open question 
whether representing the world in terms of Gaussian distributions is 
sufficient given the complexities of real-world sensorimotor in­
teractions’ (Buckley et al., 2017, p. 74). 
Finally, it should be stressed that the reconstruction has ignored the 
framework’s more philosophical aspects altogether. That it offers a 
fertile medium in which to rework philosophical issues seems beyond 
question (e.g. Friston et al., 2012a, 2012b; Allen and Friston, 2018; van 
Es, 2021). 
3.2. Round-up of conceptual problems 
The path to understanding the free-energy framework in a notation- 
independent way has some challenging bends along the way. The first of 
these involves the framework’s name. It will be clear by now that the 
framework has nothing to do with energy in the intuitive sense. It is easy 
to be misled, however. Publications in this area may present schematics 
which superimpose visualizations of energy flow over images of the 
brain (e.g. Friston et al., 2012b, Fig. 1). These can give the impression of 
a physical force flowing through the brain, that in some way produces the 
phenomena we would like to explain. 
In reality, the framework came to acquire its name due to a historical 
accident involving technical terms imported from statistical physics. 
Taking the negative logarithm of a probability is an effective way of 
quantifying the variability accommodated by the probability. But while 
information theorists have utilized terminologies focussed on surprise, 
physicists have preferred ones focused on energy. One result is that 
average uncertainty of a purely informational kind has come to be called 
‘entropy’. 
Generally, the conceptual problems that come to notice in the 
reconstruction involve loss of traditional distinctions. In the cognitive 
context, it is normal to maintain a distinction between agent and envi­
ronment, between prediction and interpretation, between behavior and 
perception, and between probability and surprise. Viewing free-energy 
minimization as a governing principle blurs these distinctions. Free- 
energy is a property of a single data structure, comprised of the priors 
and likelihoods implementing the G-density. It is a property of one 
posterior mapping. It is inevitable, therefore, that this one data structure 
must serve multiple roles, including that of organism, model, belief, 
Fig. 6. The six levels of the reconstruction.  
C. Thornton                                                                                                                                                                                                                                      

Neuropsychologia 173 (2022) 108281
9
action and goal. Other distinctions are blurred at the same time. A 
probability assignment is assumed to indicate that a surprising outcome 
has occurred, the environment becomes a product of the agent’s beliefs, 
and a prediction expressed through a posterior probability comes to 
represent the taking of an action. 
The general effect is to significantly reduce the complexity that 
would normally be entailed in modeling an agent/environment inter­
action. For purposes of assessment, the question is whether the effect 
achieved is that of simplification, or over-simplification. 
The conceptual problems that come to notice within the recon­
struction have been highlighted where they arise. The main items to 
consider are as follows. 
3.2.1. The disintegrating agent problem 
Under the thermodynamic argument (see Section 2.2), the uncer­
tainty of sensory observations is equated with the entropy of physical 
states. This provides an alternate way of motivating minimization of 
free-energy — it can be cast as an entailment of existence in living 
things. As noted, this rationale also gives rise to the prediction that an 
agent experience increasing uncertainty must suffer increasing physical 
disorder. The argument leads to the counter-intuitive conclusion that 
any increase in uncertainty must produce a corresponding disintegration 
of the agent. 
Were it the case that reduction of surprise affects sensory processing 
alone — rather than the agent in its entirety — this issue would not arise. 
It is key to the thermodynamic argument, however, that the agent and its 
sensory states are treated as one. The implication of disintegration 
cannot be ignored for this reason. This difficulty stems from elimination 
of the distinction between physical constitution and perceptual 
processing. 
3.2.2. The literal surprise fallacy 
It was noted in Section 1.4 that the term ‘surprise’ may be used to 
describe the negative logarithm of a probability. This usage has been 
avoided here, on the grounds that it is potentially misleading. It implies 
occurrence of the relevant outcome — and delivery of a surprise thereby 
— whereas the probability itself does not. Treating this implication as 
valid has been termed the literal surprise fallacy. 
To the extent that avoidance of surprises is central to the free-energy 
framework,20 it is potentially seen as succumbing to this fallacy. Rele­
vant to this is the degree to which over-interpreting the term ‘surprise’ is 
generally considered unwise (Thims, 2012). Shannon himself 
advises that the technical concepts used in information theory should be 
carefully 
differentiated 
from 
their 
psychological 
counterparts 
(Shannon, 1956). Buckley et al. (2017) make the same point: 
The term surprise has caused much confusion since it is distinct from 
the subjective psychological phenomenon of surprise. Instead, it is a 
measure of how atypical a sensory exchange is. (Buckley et al., 2017, 
p. 57, p. 57) 
The assumption that surprisal measurement implies avoidance of 
surprise in a literal sense remains central to the framework, however. Its 
degree of commitment to the literal surprise fallacy is relevant to the 
assessment, therefore. 
3.2.3. The missing world problem 
In light of the previous two sections, it is questionable whether free- 
energy minimization can be equated with avoidance of surprises. This 
connection can also be challenged on other grounds. Interpreting free- 
energy as a bound on surprisal allows that the relationship in a certain 
situation might be non-monotonic or even inverse. Increase of free- 
energy might correspond to decrease of surprisal. More fundamen­
tally, there is the problem of agent/environment fusion. It was seen in 
Section 2.3 that this is a mathematical inevitability within the frame­
work. As free-energy is a property of a single data structure, minimizing 
the quantity through agent/environment interaction entails that both 
agent and environment are defined by this one structure. 
The arrangement is not unproblematic, however, as it implies that 
there cannot be any difference between the way the world is, and the 
way the agent expects it to be.21 With the distinction between agent and 
environment eliminated, the situation would appear to be one in which 
genuine surprises cannot occur.22 
This problem is aggravated by inconsistencies in the way surprisals 
are said to be calculated. Surprisal is sometimes said to be a property of 
the agent’s model, and sometimes of the world. Friston asserts that 
‘surprise (surprisal or self-information) is conditioned on a model and is 
not an attribute of a sampled (frequentist) distribution’ (Friston, 2010a, 
p. 1). Conversely, Buckley et al. write that agents cannot calculate sur­
prise directly because ‘the distribution of “surprising” events is in gen­
eral unknown and unknowable’ (Buckley et al., 2017, p. 56). Friston 
concurs that this cannot be done because it ‘ … would entail knowing all 
the hidden states of the world causing sensory input’ (Friston, 2009, p. 
294). The representational fusion of agent and environment creates 
some unclarity about how surprise originates. 
3.3. The dark-room problem 
The dark-room problem has long been viewed as a key conceptual 
issue affecting the free-energy framework (Friston et al., 2012a). It will 
be seen that the problems already noted suggest a different assessment 
has to be made. To recap, the dark-room problem stems from the claim 
that avoidance of surprise ensures survival. Critics point out that sur­
vival often involves remaining alert to, and engaging with surprises, 
rather than avoiding them. This may be acknowledged by free-energy 
theorists, who may respond by hypothesizing additional arrangements 
that affect the situations in question. It may be proposed, for example, 
that the agent acquires beliefs that lead to other kinds of action being 
taken in the relevant cases. For critics, this may seem to contradict the 
claim that avoidance of surprise is the governing principle. 
I myself have supported the critic’s position on this issue (Friston 
et al., 2012a). But the picture that emerges from the reconstruction 
suggests a second look is in order. Recognizing that minimization of 
free-energy need not give rise to avoidance of surprises, it becomes 
doubtful whether this can genuinely be viewed as an entailment of the 
framework. The dark-room problem potentially remains an issue for the 
free-energy principle. While different definitions may be put forward (see 
below), 
they 
typically 
include 
an 
explicit 
commitment 
to 
surprise-avoidance of some kind. 
For similar reasons, the problem remains a potential concern for the 
predictive processing proposal (Clark, 2013, 2016). The hypothesis in this 
case is that agents minimize prediction error. Since this produces the effect 
of surprise-avoidance, the dark-room objection is potentially raised. A 
recent iteration of the debate has emerged in this way, with the criticism 
advanced by Sun and Firestone (2020), and responses offered by 
Seth et al. (2020) and Van de Cruys et al. (2020). These point to arrange­
ments that have the potential to override surprise-avoidance in particular 
cases. Seth et al. (2020, p. 682), for example, propose that ‘in order to 
minimize surprise in the future, an agent needs to be a curious, 
sensation-seeking agent in the present’, while (Van de Cruys et al., 2020, p. 
20 In the past, Friston has particularly emphasized this idea, writing that 
‘the whole point of the free-energy principle is to unify all adaptive autopoietic 
and self-organizing behavior under one simple imperative; avoid surprises and 
you will last longer.’ (Friston et al., 2012a, p. 2). 
21 Agent beliefs and environmental states are potentially represented in a way 
that makes the latter independent of the former. In this case, the problems 
highlighted would not arise.  
22 How agent/environment fusion accommodates intentional ascription more 
generally is addressed by (Ramstead et al., 2020a, 2020b). 
C. Thornton                                                                                                                                                                                                                                      

Neuropsychologia 173 (2022) 108281
10
680) emphasize how ‘optimistic predictions’ might have the effect of 
making predictable inputs unpredictable. 
Although the focus in this case is on predictive processing, the ar­
guments mirror those put forward in defence of the FEP. Responding to 
the dark-room criticism, free-energy theorists note that ‘[o]ptimisation 
of free-energy may engage different mechanisms at different timescales’ 
(Friston and Stephan, 2007, p. 436), and emphasize how this may pro­
duce prior beliefs that override the imperative to avoid surprise in 
particular situations (Friston, 2013, p. 213). Friston (2013, p. 212–213), 
for example, argues that ‘the state of a room being dark is surprising, 
because we do not expect to occupy dark rooms’. This point is made in 
more detail as follows: 
[the] resolution of the dark room problem is fairly simple: prior 
beliefs render dark rooms surprising. The existence of these beliefs is 
assured by natural selection, in the sense that agents that did not find 
dark rooms surprising would stay there indefinitely, until they die of 
dehydration or loneliness. (Friston et al., 2012b, p 1) 
There is an element of circularity in this, however (Thornton, 2010). 
It is argued that the dark-room problem is not a problem because agents 
drawn to such situations would not survive. Surprisal is claimed to 
benefit survival because it benefits survival, but as Klein notes, ‘we don’t 
explain how an organism stays alive by starting with the premise that it 
stays alive’ (Klein, 2018, p. 2544). There is also a sense in which the 
argument leads to a contradiction. If the proposal is that surprises are 
avoided in some scenarios but not others, surprise-avoidance cannot be 
the sole, governing principle. 
In Friston’s opinion (personal communication), the dark-room issue 
‘was resolved many years ago.’ For present purposes, it seems it can be 
legitimately set aside. The reconstruction reveals that the problem is 
inconsequential for the framework, regardless of its potential relevance 
to the predictive-processing proposal (or the FEP). Given any connection 
between free-energy minimization — as construed within the frame­
work — and avoidance of surprises is doubtful, the hypothesis targeted 
by the dark-room argument would appear not to be implied. 
3.3.1. The impact-characterization problem 
If what is accomplished by free-energy minimization cannot be 
related to surprise-avoidance, how should it be characterized? Within 
the reconstruction, free-energy is recognized to be statistical noise in the 
form of introduced uncertainty (see Section 1.5). A posterior mapping23 
provides the means of predicting states from observations. If the un­
certainty of observations is found to be lower than that of the states they 
predict, this is inherently suboptimal. The uncertainty at the output end 
of the mapping should be no greater than at the input end. Minimization 
of free-energy serves to eliminate unwarranted uncertainty of this kind. 
While free-energy theorists may portray the effect slightly differ­
ently, the imperative nature of the process is often emphasized. That 
free-energy should be minimized is put forward as a principle, with the 
effect linked to Bayesian inference, resistance to physical disintegration, 
or some combination of the two. Different flavours of the FEP are then 
forthcoming. In the view of Colombo and Wright, for example, the ‘free- 
energy principle states that all systems that minimize their free energy 
resist a tendency to physical disintegration’ (Colombo and Wright, 2018, 
p. 3463). Aguilera et al. (2021, p. 1) propose, on the other hand, that ‘[t] 
he free energy principle (FEP) states that any dynamical system can be 
interpreted as performing Bayesian inference upon its surrounding 
environment.’ Adopting a more intermediate position, Friston (personal 
communication) takes the principle to be that ‘all self organizing sys­
tems can be read — or interpreted — as performing approximate 
Bayesian inference by optimizing a variational (free energy) bound on 
marginal likelihood.’ 
How elimination of introduced uncertainty relates to Bayesian 
inference has been examined in Sections 1.3–5. How it relates to resis­
tance of physical disintegration has been examined in Sections 2.1–2. 
Both connections face difficulties, as noted. The proposition that the 
process is a natural imperative is unambiguously confirmed, however. 
The introduction of uncertainty in a posterior mapping is always un­
desirable, regardless of context. Minimizing such uncertainty is a 
context-free aspiration, applicable in all conceivable situations. If we 
interpret the free-energy principle in these terms, its universality cannot 
be questioned. 
This generality undoubtedly contributes to the framework’s 
remarkable adaptability. Its capacity to be applied in such a wide range 
of contexts reflects not only the casting off of contextual commitments 
— as detailed above — but also the adoption of this mathematically 
unavoidable dictum as its central tenet. Less favourably, the context-free 
nature of the proposal may impugn the Bayesian interpretation of 
impact. While the process may produce an effect of a Bayesian kind, this 
differs from what is normally desired, namely, context-sensitive infer­
ence directed towards an independently represented domain or data-set. 
Free-energy minimization produces Bayesian inference of a sort, but it is 
not the context-sensitive kind we generally seek. The explanatory value 
of the scheme may have limitations in result. 
4. Concluding comments 
An attempt has been made to reconstruct the free-energy framework 
in a way that allows an informed assessment to be derived, without 
undue reliance on mathematical notation. What the assessment should 
be is left open. The cautious approach is due in part to the variability of 
published assessments in the literature. Those originating from free- 
energy theorists tend to be positive, as we might expect. Buckley 
et al., for example, see the framework as the basis for a ‘global unified 
theory of cognition’ (Buckley et al., 2017, p. 56). Theorists interested in 
general neuroscientific models of cognition may also take a favourable 
view (e.g. Clark, 2016; Seth, 2021). Others may be more sceptical. In the 
view of Herreros and Verschure, for example, the free-energy framework 
… seems to strive toward the super power of explaining everything. 
This, however, will make it transcend the obligation of each theory to 
be testable. (Herreros and Verschure, 2015, p. 218) 
Similar in spirit is the assessment of Colombo and Wright (2018). 
These authors cast the ambitious claims originating from the framework 
in a quite negative light: 
Although these dramatic ascriptions have attracted attention in 
philosophy and the life sciences, FEP’s epistemic status remains 
opaque, along with its exact role in biological and neuroscientific 
theorizing. Conspiring against its accessibility are the varying for­
malisms and formulations of FEP, the changing scope of application, 
reliance on undefined terms and stipulative definitions, and the lack 
of clarity in the logical structure of the reasoning leading to FEP. 
(Colombo and Wright, 2018, p. 2) 
The framework is not uncontroversial, then. Ideally, the scientific 
worth of a proposal of this kind is established by evaluating its empirical 
predictions. Setting aside the issues discussed above, the key prediction 
emerging from the framework is that surprise-avoidance governs 
behavior. To what degree is this confirmed? According to some, what is 
observed in nature does not confirm this prediction in any degree. Sun 
and Fireston (2020, p. 346, emphasis added) note that ‘[h]umans and 
other animals tend not to seek such experiences’, while Fiorillo (2010) 
records that 
… animals tend to explore the least predictable sensory inputs while 
avoiding predictable inputs. (Fiorillo, 2010, p. 1, emphasis added) 
Such evidence might be seen as falsifying the framework’s central 
prediction. The potential for free-energy to be minimized at multiple 
23 The term refers to the priors and likelihoods expressing a posterior 
distribution. 
C. Thornton                                                                                                                                                                                                                                      

Neuropsychologia 173 (2022) 108281
11
timescales however (see Section 3.2.4), means that the prediction of 
surprise-avoidance has to be seen as qualified. What seems to be pre­
dicted, in actuality, is that agents will avoid surprises in some situations 
but not others. 
The prediction deriving from the thermodynamic argument also 
raises questions. This way of motivating free-energy minimization relies 
on the hypothesis that all systems that minimize their free-energy resist 
a tendency to physical disintegration (Colombo and Wright, 2018, p. 
3463). The difficulty is that, given the way free-energy is linked to 
physical entropy, the posited relationship exists necessarily. Rather than 
expressing a valid prediction, the claim seems to reduce to a tautology. 
Friston does not object to this judgment. He writes (personal 
communication) ‘I think this is absolutely right and should be empha­
sized, this is the essence of the free energy principle.’ In the past, 
however, he has also emphasized the potential value of empirical 
evaluation. Simulations and working models illustrating free-energy 
minimization can be constructed (e.g. Bastos et al., 2012), and this 
can be a way of exploring explanatory potential. It can also serve to 
reveal practical problems, e.g., the potential for the process to be trap­
ped in local minima.24 In the conclusion to the 2012 dark-room debate 
(Friston et al., 2012a, 2012b), Friston particular stressed the importance 
of systems-building work of this kind: 
This is the real challenge ahead … If, in a few years time, we do not 
have neuronally plausible accounts of all these [cognitive and 
adaptive] faculties that appeal to, and only to, free-energy minimi­
zation, then I will be surprised and will search for a better model! 
(Friston et al., 2012a, p. 6) 
It is hoped that more demonstrations of this kind will be developed in 
the future, and that their assessment can inform further developments of 
the reconstruction set out above. 
Acknowledgements 
I am grateful to Karl Friston, Adam Barrett, Manuel Baltieri, Matteo 
Colombo, Simon McGregor, Amelia Thornton and Andy Clark for helpful 
comments on an early draft of this article. I also thank Chris Buckley for 
helping me towards an understanding of the mathematics of the free- 
energy framework. 
References 
Aguilera, M., Millidge, B., Tschantz, A., Buckley, C.L., 2021. How particular is the 
physics of the free energy principle? Phys. Life Rev. 11203 arXiv preprint arXiv: 
2105.  
Allen, M., Friston, K.J., 2018. From cognitivism to autopoiesis: towards a computational 
framework for the embodied mind. Synthese 195, 2459–2482. 
Bastos, A., Usrey, W., Adams, R., Mangun, G., Fries, P., Friston, K., 2012. Canonical 
microcircuits for predictive coding. Neuron 76, 695–711. 
Bengio, Y., 2009. Learning deep architectures for AI. Found. Trend. Machine Learn. 2 (1), 
1–127. 
Beni, M.D., 2021. A critical analysis of Markovian monism. Synthese 199 (3), 
6407–6427. 
Bishop, C.M., 2007. Pattern Recognition and Machine Learning. Springer. 
Bogacz, R., 2017. A tutorial on the free-energy framework for modelling perception and 
learning. J. Math. Psychol. 76 (Part B), 198–211. 
Boltzmann, L., 1974. The Second Law of Thermodynamics (Theoretical Physics and 
Philosophical Problems). Springer-Verlag, New York.  
Bruineberg, J., Kiverstein, J., Rietveld, E., 2018. The anticipating brain is not a scientist: 
the free-energy principle from an ecological-enactive perspective. Synthese 195, 
2417–2444. 
Buckley, C., Tschantz, A., 2019. Free Energy Tutorial: what Is the Free Energy Principle 
(Tutorial Given at Okasi, 2019). 
Buckley, C.L., Kim, C.S., McGregor, S., Seth, A.K., 2017. The free energy principle for 
action and perception: a mathematical review. J. Math. Psychol. 81, 55–79. 
Clark, A., 2013. Whatever next? Predictive brains, situated agents, and the future of 
cognitive science. Behav. Brain Sci. 36, 181–253. 
Clark, A., 2016. Surfing Uncertainty: Prediction, Action, and the Embodied Mind. Oxford 
University Press, Oxford.  
Colombo, M., Wright, C., 2018. First Principles in the Life Sciences: the Free-Energy 
Principle, Organicism, and Mechanism. Synthese, p. 1. 
Dayan, P., Hinton, G., 1996. Varieties of Helmholtz machine. Neural Network. 9 (8), 
1385–1403. 
Dayan, P., Hinton, G.E., Neal, R.M., Zemel, R.S., 1995. The Helmholtz machine. Neural 
Comput. 7, 889–904. 
Dempster, A.P., Laird, N.M., Bubin, D.B., 1977. Maximum likelihood from incomplete 
data via EM algorithm. J. Roy. Stat. Soc. B 39, 1–38. 
Di Paolo, E., Thompson, E., Beer, R.D., 2022. Laying down a forking path: 
incompatibilities between enaction and the free energy principle. Philos. Mind Sci. 2 
(3). 
Feldman, H., Friston, K., 2010. Attention, uncertainty and free-energy. Front. Hum. 
Neurosci. 4 (215). 
Fiorillo, C.D., 2010. A neurocentric approach to Bayesian inference. Nat. Rev. Neurosci. 
Friston, K., 2003. Learning and inference in the brain. Neural Network. 16 (9), 
1325–1352. 
Friston, K., 2005. A theory of cortical responses. Philos. Trans. R. Soc. Lond. B Biol. Sci. 
360 (1456), 815–836. 
Friston, K., 2009. The free-energy principle: a rough guide to the brain. Trends Cognit. 
Sci. 13 (7), 279–328. 
Friston, K., 2010a. Is the free-energy principle neurocentric? Nat. Rev. Neurosci. https:// 
doi.org/10.1038/nrn2787-c2. 
Friston, K.J., 2010b. The free-energy principle: a unified brain theory? Nat. Rev. 
Neurosci. 11 (2), 127–138. 
Friston, K., 2013. Active inference and free energy. Behav. Brain Sci. 36, 212–213. 
Friston, K.J., Stephan, K.E., 2007. Free-energy and the brain. Synthese 159, 417–458. 
Friston, K.J., Kilner, J., Harrison, L., 2006. A free energy principle for the brain. 
J. Physiol. Paris 100, 70–87. 
Friston, K.J., Daunizeau, J., Kiebel, S.J., 2009. Reinforcement learning or active 
inference. PLoS One 4 (7), 1–13. 
Friston, K.J., Daunizeau, J., Kilner, J., Kiebel, S.J., 2011. Action and behavior: a free- 
energy formulation. Biol. Cybern. 102 (3), 227–260. 
Friston, K., Thornton, C., Clark, A., 2012a. Free-energy minimization and the dark room 
problem. Front. Percept. Sci. 130. 
Friston, K., Adams, R.A., Perrinet, L., Breakspear, M., 2012b. Perceptions as hypotheses: 
saccades as experiments. Front. Psychol. 3 (151). 
Friston, K.J., Lin, M., Frith, C.D., Pezzulo, G., Hobson, J.A., Ondobaka, S., 2017a. Active 
inference, curiosity and insight. Neural Comput. 29, 2633–2683. 
Friston, K., Rigoli, F., Schwartenbeck, P., Pezzulo, G., 2017b. Active inference: a process 
theory. Neural Comput. 29 (1), 1–49. 
Gatlin, L., 1972. Information Theory and the Living System. Columbia University Press, 
London.  
Herreros, I., Verschure, P.F., 2015. About the goal of a goals’ goal theory (commentary 
on Friston et al., “Active inference and epistemic value”). Cognit. Neurosci. 6 (4), 
218–219. 
Hinton, G., Sejnowski, T., 1986. Learning and relearning in Boltzmann machines. In: 
Rumelhart, McClelland, Group (Eds.), Parallel Distributed Processing: Explorations 
in the Microstructures of Cognition, vols. I and II. MIT Press, Cambridge, Mass, 
pp. 282–317. 
Hinton, G.E., Zemel, R.S., 1994. Autoencoders, minimum description length and 
Helmholtz free energy. In: Cowan, Alspector (Eds.), Advances in Neural Information 
Processing Systems, vol. 6. Morgan Kaufmann. 
Hohwy, J., Roepstorff, A., Friston, K., 2008. Predictive coding explains binocular rivalry: 
an epistemological review. Cognition 108 (3), 687–701. 
Hutto, D.D., 2018. Getting into predictive processing’s great guessing game: bootstrap 
heaven or hell? Synthese 195 (6), 2445–2458. 
James III, R.N., 2015. Exploration-exploitation: a cognitive dilemma still unresolved 
(Commentary on Friston et al., “Active inference and epistemic value”). Cognit. 
Neurosci. 6 (4), 219–220. 
Kirby, K.G., 2006. A Tutorial on Helmholtz Machines (Unpublished MS). 
Klein, C., 2018. What do predictive coders want? Synthese 195, 2541–2557. 
Kullback, S., 1959. Information Theory and Statistics. Wiley, New York.  
Kullback, S., Leibler, R.A., 1951. On information and sufficiency. Ann. Math. Stat. 22, 
79–86. 
LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature 521, 436–444. 
MacKay, D.J., 1995. Free-energy minimisation algorithm for decoding and 
cryptoanalysis. Electron. Lett. 31, 445–447. 
MacQueen, J.B., 1967. Some methods for classification and analysis of multivariate 
observations. In: Proceedings of 5th Berkeley Symposium on Mathematical Statistics 
and Probability, vol. 1. University of California Press, pp. 281–297. 
Medawar, P., 1984. Pluto’s Republic. Oxford University Press, Oxford.  
Morowitz, H.J., 1981. Rediscovering the mind. In: Hofstadter, Dennett (Eds.), The Mind’s 
I. Basic Books. 
Neal, R.M., Hinton, G.E., 1998. A new view of the EM algorithm that justifies 
incremental, sparse and other variants. In: Jordan (Ed.), Learning in Graphical 
Models. Kluwer, MA, pp. 355–368. 
No¨e, A., 2004. Action in Perception. The MIT Press, London, England.  
Ramstead, M.J.D., Kirchhoff, M.D., Friston, K.J., 2020a. A tale of two densities: active 
inference is enactive inference. Adapt. Behav. 28, 225–239. 
Ramstead, M.J., Friston, K.J., Hipmboxolito, I., 2020b. Is the free-energy principle a 
formal theory of semantics? From variational density dynamics to neural and 
phenotypic representations. Entropy 22 (8), 889. 
24 As Buckley et al. acknowledge, ‘the conditions under which gradient 
descent [in free energy] will become stuck in local minima, or fail to converge 
in an appropriate amount of time, are not well understood’ (Buckley et al., 
2017, p. 74). 
C. Thornton                                                                                                                                                                                                                                      

Neuropsychologia 173 (2022) 108281
12
Rioul, O., 2018. This Is it: A Primer on Shannon’s Entropy and Information. 
L’Information, S’eminaire Poincare, vol. XXIII, pp. 43–77. 
Schroedinger, E., 1944. What Is Life? Cambridge University Press, Cambridge.  
Seth, A.K., 2014. Response to Gu and FitzGerald: interoceptive inference: from decision- 
making to organism integrity. Trends Cognit. Sci. 18 (6). 
Seth, A., 2021. Being You: A New Science of Consciousness, Penguin. 
Seth, A.K., Millidge, B., Buckley, C.L., Tschantz, 2020. Curious inferences: reply to sun 
and firestone on the dark room problem. Trends Cognit. Sci. 24 (9), 681–683. 
Shannon, C.E., 1948. A mathematical theory of communication. Bell System Technical 
Journal 27, 379–423 and 623-656.  
Shannon, C.E., 1949. Communication theory of secrecy systems (originally published as 
”A mathematical theory of cryptography” (memorandum MM 45-110-02), bell 
laboratories, 1945). Bell Systems Technical Journal 28, 656–715. 
Shannon, C.E., 1956. The Bandwagon. Transactions on Information Theory, vol. 3. 
Institute of Electrical and Electronics Engineers. 
Shannon, C., Weaver, W., 1949. The Mathematical Theory of Communication. University 
of Illinois Press, Urbana, Illinois.  
Sun, Z., Firestone, C., 2020. The dark room problem. Trends Cognit. Sci. 24 (5), 346–348. 
Thims, L., 2012. Thermodynamics/= information theory: science’s greatest Sokal affair. 
Journal of Human Thermodynamics 8 (1), 1–120. 
Thompson, C.J., 1988. Classical Equilibrium Statistical Mechanics. Clarendon Press, 
Oxford.  
Thornton, C., 2010. Some puzzles relating to the free-energy principle: comment on 
Friston. Trends Cognit. Sci. 14 (2), 53–54. 
Tribus, M., 1961. Thermostatics and Thermodynamics. D. Van Nostrand, Princeton, NJ.  
Tribus, M., McIrvine, E.C., 1971. Energy and Information, vol. 225. Scientific American, 
pp. 179–188. 
Uffink, J., 2001. Bluff your way in the second law of thermodynamics. Studies in History 
and Philosophy of Modern Physics 32, 305–394. 
Van de Cruys, S., Friston, K., Clark, 2020. Controlled optimism: reply to Sun and 
Firestone on the dark room problem. Trends Cognit. Sci. 24 (9), 1–2. 
van Es, T., 2021. Living models or life modelled? On the use of models in the free energy 
principle. Adapt. Behav. 29 (3), 315–329. 
van Rooij, I., Wright, C.D., Kwisthout, J., Wareham, T., 2018. Rational analysis, 
intractability, and the prospects of ‘as if’-explanations. Synthese 195, 491–510. 
Varela, F.J., Thompson, E., Rosch, E., 1991. The Embodied Mind: Cognitive Science and 
Human Experience. The MIT Press, Cambridge, Massachusetts.  
Weissbart, H., Kandylaki, K.D., Reichenbach, T., 2020. Cortical tracking of surprisal 
during continuous speech comprehension. J. Cognit. Neurosci. 32 (1), 155–166. 
Wiese, W., Friston, K.J., 2021. Examining the continuity between life and mind: is there a 
continuity between autopoietic intentionality and representationality? Philosophies 
6 (1), 18. 
Winn, J., Bishop, C.M., Jaakkola, T., 2005. Variational message passing. J. Mach. Learn. 
Res. 6 (4). 
Zemel, R.S., Hinton, G.E., 1991. Discovering Viewpoint-Invariant Relationships that 
Characterize Objects. 
C. Thornton                                                                                                                                                                                                                                      

