A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
BORJAN GESHKOVSKI, CYRIL LETROUIT, YURY POLYANSKIY,
AND PHILIPPE RIGOLLET
Abstract. Transformers play a central role in the inner workings of large
language models. We develop a mathematical framework for analyzing Trans-
formers based on their interpretation as interacting particle systems, which
reveals that clusters emerge in long time. Our study explores the underlying
theory and offers new perspectives for mathematicians as well as computer
scientists.
Contents
1.
Outline
1
Part 1.
Modeling
3
2.
Interacting particle system
3
3.
Measure to measure flow map
9
Part 2.
Clustering
14
4.
A single cluster in high dimension
14
5.
A single cluster for small β
21
Part 3.
Further questions
24
6.
Dynamics on the circle
24
7.
General matrices
26
8.
Approximation and control
30
Acknowledgments
31
Appendix
31
Appendix A.
Proof of Theorem 4.7
31
Appendix B.
Proof of Theorem 5.1
34
Appendix C.
Proof of Proposition 6.1
36
References
37
1. Outline
The introduction of Transformers in 2017 by Vaswani et al. [VSP`17] marked
a significant milestone in development of neural network architectures. Central to
2020 Mathematics Subject Classification. Primary: 34D05, 34D06, 35Q83; Secondary: 52C17.
Key words and phrases. Transformers, self-attention, interacting particle systems, clustering,
gradient flows.
1
arXiv:2312.10794v1  [cs.LG]  17 Dec 2023

2
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
this contribution is self-attention, a novel mechanism which distinguishes Trans-
formers from traditional architectures, and which plays a substantial role in their
superior practical performance. In fact, this innovation has been a key catalyst
for the progress of artificial intelligence in areas such as computer vision and nat-
ural language processing, notably with the emergence of large language models.
As a result, understanding the mechanisms by which Transformers, and especially
self-attention, process data is a crucial yet largely uncharted research area.
A common characteristic of deep neural networks (DNNs) is their compositional
nature: data is processed sequentially, layer by layer, resulting in a discrete-time
dynamical system (we refer the reader to the textbook [GBC16] for a general intro-
duction). This perspective has been successfully employed to model residual neural
networks—see Section 2.1 for more details—as continuous-time dynamical systems
called neural ordinary differential equations (neural ODEs) [CRBD18, E17, HR17].
In this context, an input xp0q P Rd, say an image, is evolving according to a given
time-varying velocity field as 9xptq “ vtpxptqq over some time interval p0, Tq. As
such, a DNN can be seen as a flow map xp0q ÞÑ xpTq from Rd to Rd. Even within
the restricted class of velocity fields tvtutě0 imposed by classical DNN architectures,
such flow maps enjoy strong approximation properties as exemplified by a long line
of work on these questions [LJ18, ZGUA20, LLS22, TG22, RBZ23, CLLS23].
In this article we observe that Transformers are in fact flow maps on PpRdq,
the space of probability measures over Rd. To realize this flow map from measures
to measures, Transformers evolve a mean-field interacting particle system. More
specifically, every particle (called a token in this context) follows the flow of a vector
field which depends on the empirical measure of all particles. In turn, the continuity
equation governs the evolution of the empirical measure of particles, whose long-
time behavior is of crucial interest. In this regard, our main observation is that
particles tend to cluster under these dynamics. This phenomenon is of particular
relevance in learning tasks such as next-token prediction, wherein one seeks to map
a given input sequence (i.e., a sentence) of n tokens (i.e., words) onto a given next
token. In this case, the output measure encodes the probability distribution of the
next token, and its clustering indicates a small number of possible outcomes. Our
results indicate that the limiting distribution is actually a point mass, leaving no
room for diversity or randomness, which is at odds with practical observations.
This apparent paradox is resolved by the existence of a long-time metastable state.
As can be seen from Figures 2 and 4, the Transformer flow appears to possess two
different time-scales: in a first phase, tokens quickly form a few clusters, while in
a second (much slower) phase, through the process of pairwise merging of clusters,
all tokens finally collapse to a single point.
The goal of this manuscript is twofold. On the one hand, we aim to provide
a general and accessible framework to study Transformers from a mathematical
perspective. In particular, the structure of these interacting particle systems allows
one to draw concrete connections to established topics in mathematics, including
nonlinear transport equations, Wasserstein gradient flows, collective behavior mod-
els, and optimal configurations of points on spheres, among others. On the other
hand, we describe several promising research directions with a particular focus on
the long-time clustering phenomenon. The main results we present are new, and we
also provide what we believe are interesting open problems throughout the paper.
The rest of the paper is arranged in three parts.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
3
Part 1: Modeling. We define an idealized model of the Transformer architecture
that consists in viewing the discrete layer indices as a continuous time variable.
This abstraction is not new and parallels the one employed in classical architec-
tures such as ResNets [CRBD18, E17, HR17]. Our model focuses exclusively on
two key components of the Transformers architecture: self-attention and layer-
normalization. Layer-normalization effectively constrains particles to evolve on the
unit sphere Sd´1, whereas self-attention is the particular nonlinear coupling of the
particles done through the empirical measure. (Section 2). In turn, the empirical
measure evolves according to the continuity partial differential equation (Section 3).
We also introduce a simpler surrogate model for self-attention which has the conve-
nient property of being a Wasserstein gradient flow [AGS05] for an energy functional
that is well-studied in the context of optimal configurations of points on the sphere.
Part 2: Clustering. In this part we establish new mathematical results that indicate
clustering of tokens in the large time limit. Our main result, Theorem 4.1, indicates
that in high dimension d ě n, a set of n particles randomly initialized on Sd´1 will
cluster to a single point as t Ñ `8. We complement this result with a precise
characterization of the rate of contraction of particles into a cluster. Namely, we
describe the histogram of all inter-particle distances, and the time at which all
particles are already nearly clustered (Section 4).
We also obtain a clustering
result without assuming that the dimension d is large, in another asymptotic regime
(Section 5).
Part 3: Further questions. We propose potential avenues for future research, largely
in the form of open questions substantiated by numerical observations. We first
focus on the case d “ 2 (Section 6) and elicit a link to Kuramoto oscillators. We
briefly show in Section 7.1 how a simple and natural modification of our model
leads to non-trivial questions related to optimal configurations on the sphere. The
remaining sections explore interacting particle systems that allow for parameter
tuning of the Transformers architectures, a key feature of practical implementations.
Part 1. Modeling
We begin our discussion by presenting the mathematical model for a Transformer
(Section 2). We focus on a slightly simplified version that includes the self-attention
mechanism as well as layer normalization, but excludes additional feed-forward
layers commonly used in practice; see Section 2.3.2. This leads to a highly nonlinear
mean-field interacting particle system. In turn, this system implements, via the
continuity equation, a flow map from initial to terminal distributions of particles
that we present in Section 3.
2. Interacting particle system
Before writing down the Transformer model, we first provide a brief preliminary
discussion to clarify our methodological choice of treating the discrete layer indices
in the model as a continuous time variable in Section 2.1, echoing previous work on
ResNets. The specifics of the Transformer model are presented in Section 2.2.

4
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
2.1. Residual neural networks. One of the standard paradigms in machine
learning is that of supervised learning, where one aims to approximate an unknown
function f : Rd Ñ Rm, from data, D “ txpiq, fpxpiqquiPrNs say. This is typically
done by choosing one among an arsenal of possible parametric models, whose pa-
rameters are then fit to the data by means of minimizing some user-specified cost.
With the advent of graphical processing units (GPUs) in the realm of computer
vision [KSH12], large neural networks have become computationally accessible, re-
sulting in their popularity as one such parametric model.
Within the class of neural networks, residual neural networks (ResNets for short)
have become a staple DNN architecture since their introduction in [HZRS16]. In
their most basic form, ResNets approximate a function f at x P Rd through a
sequence of affine transformations, a component-wise nonlinearity, and skip con-
nections. Put in formulae,
(2.1)
#
xpk ` 1q “ xpkq ` wpkqσpapkqxpkq ` bpkqq
for k P t0, . . . , L ´ 1u
xp0q “ x .
Here σ is a Lipschitz function applied component-wise to the input vector, while
θp¨q “ pwp¨q, ap¨q, bp¨qq P Rdˆd ˆ Rdˆd ˆ Rd are trainable parameters.
We say
that (2.1) has L ě 1 hidden layers (or L ` 1 layers, or is of depth L). The output
of the ResNet given the i-th input, namely xipLq P Rd, is projected to Rm via a
trained transformation as to match the label fpxpiqq according to the user-specified
objective. One can also devise generalizations of (2.1), for instance in which matrix-
vector multiplications are replaced by discrete convolutions. The key element that
all these models share is that they all have skip-connections, namely, the previous
step xipkq appears explicitly in the iteration for the next one.
One upside of (2.1), which is the one of interest to our narrative, is that the layer
index k can naturally be interpreted as a time variable, motivating the continuous-
time analogue
(2.2)
#
9xptq “ wptqσpaptqxptq ` bptqq
for t P p0, Tq
xp0q “ x.
These are dubbed neural ordinary differential equations (neural ODEs). Since their
introduction in [CRBD18, E17, HR17], neural ODEs have emerged as a flexible
mathematical framework to implement and study ResNets.
2.2. The interacting particle system. Unlike ResNets, which operate on a sin-
gle input vector xp0q P Rd at a time, Transformers operate on a sequence of vectors
of length n, namely, pxip0qqiPrns P pRdqn. This perspective is rooted in natural lan-
guage processing, where each vector represents a word, and the entire sequence a
sentence or a paragraph. In particular, it allows to process words together with their
context. A sequence element xip0q P Rd is called a token, and the entire sequence
pxip0qqiPrns a prompt. We use the words “token” and “particle” interchangeably.
Practical implementations make use of layer normalization [BKH16], which
amounts to an element-wise standardization of every particle at every layer. This

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
5
effectively constrains particles to evolve on a time-varying axis-aligned ellipsoid,
that we take to be the unit sphere Sd´1 in the rest of this paper.1
A Transformer is then a flow map on pSd´1qn: the input sequence pxip0qqiPrns P
pSd´1qn is an initial condition which is evolved through the dynamics
(2.3)
9xiptq “ Pxiptq
˜
1
Zβ,iptq
n
ÿ
j“1
eβxQptqxiptq,KptqxjptqyV ptqxjptq
¸
for all i P rns and t ě 0. Here and henceforth
Pxy “ y ´ xx, yyx
denotes the projection of y P Sd´1 onto TxSd´1. The partition function Zβ,iptq ą 0
reads
(2.4)
Zβ,iptq “
n
ÿ
k“1
eβxQptqxiptq,Kptqxkptqy.
where pQp¨q, Kp¨q, V p¨qq (standing for Query, Key, and Value) are parameter ma-
trices learned from data, and β ą 0 a fixed number intrinsic to the model2, which,
can be seen as an inverse temperature using terminology from statistical physics.
Note that Qp¨q, Kp¨q need not be square.
The interacting particle system (2.3)–(2.4), a simplified version of which was
first written down in [LLH`20, DGCC21, SABP22], importantly contains the true
novelty that Transformers carry with regard to other models: the self-attention
mechanism
(2.5)
Aijptq :“ exQptqxiptq,Kptqxjptqy
Zβ,iptq
,
pi, jq P rns2,
which is the nonlinear coupling mechanism in the interacting particle system. The
n ˆ n stochastic matrix Aptq (rows are probability vectors) called the self-attention
matrix. The wording attention stems from the fact that Aijptq captures the atten-
tion given by particle i to particle j relatively to all particles ℓP rns. In particular,
a particle pays attention to its neighbors where neighborhoods are dictated by
the matrices Qptq and Kptq in (2.5). It has been observed numerically that the
probability vectors pAijp¨qqjPrns (i P rns) in a trained self-attention matrix exhibit
behavior related to the syntactic and semantic structure of sentences in natural lan-
guage processing tasks (see [VSP`17, Figures 3-5]). To illustrate our conclusions as
pedagogically as possible, throughout the paper we focus on a simplified scenario
wherein the parameter matrices pQ, K, V q are constant, and even all equal to the
identity unless stated otherwise, resulting in the dynamics
(SA)
9xiptq “ Pxiptq
˜
1
Zβ,iptq
n
ÿ
j“1
eβxxiptq,xjptqyxjptq
¸
1Layer normalization originally consisted in an entry-wise standardization of every token and
a skew via a trained matrix at every layer, leading to said axis-aligned ellipsoid.
However,
different practical implementations use different but related variants, all with the goal of en-
suring that tokens don’t diverge as to avoid rounding errors.
Considering the unit sphere is
thus a reasonable and natural modeling choice.
This is even verified empirically in the pre-
trained ALBERT XLarge v2 model described in Figure 1, and used explicitly in Mistral AI’s model
(https://github.com/mistralai/mistral-src/tree/main).
2In practical implementations the inner products are multiplied by d´ 1
2 , which along with the
typical magnitude of Q, K leads to the appearance of β.

6
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for i P rns and t ě 0 and, as before
(2.6)
Zβ,iptq “
n
ÿ
k“1
eβxxiptq,xkptqy.
The dynamics (SA) have a strong resemblance to the vast literature on nonlinear
systems arising in the modeling of collective behavior. In addition to the connection
to the classical Kuramoto model describing synchronization of oscillators [Kur75,
ABV`05] (made evident in Section 6.2), Transformers are perhaps most similar to
the Krause model [Kra00]
9xiptq “
n
ÿ
j“1
aijpxjptq ´ xiptqq,
aij “
ϕp}xi ´ xj}2q
řn
k“1 ϕp}xi ´ xk}2q.
which is non-symmetric in general (aij ‰ aji), much like (2.3). When ϕ is compactly
supported, it has been shown in [JM14] that the particles xiptq assemble in several
clusters as t Ñ `8. Other related models include those of Vicsek [VCBJ`95],
Hegselmann-Krause [HK02] and Cucker-Smale [CS07]. All these models exhibit a
clustering behavior under various assumptions (see [MT14, Tad23] and the refer-
ences therein). Yet, none of the opinion dynamics models discussed above contain
parameters appearing nonlinearly as in (SA).
The appearance of clusters in Transformers is actually corroborated by numeri-
cal experiments with pre-trained models (see Figure 1). While we focus on a much
simplified model, numerical evidence shows that the clustering phenomenon looks
qualitatively the same in the cases Q “ K “ V “ Id and generic random pQ, K, V q
(see Figures 2 and 4 for instance). We defer the interested reader directly to Sec-
tion 4; here, we continue the presentation on the modeling of different mechanisms
appearing in the Transformer architecture.
Remark 2.1 (Permutation equivariance). A function f : pSd´1qn Ñ pSd´1qn is
permutation equivariant if fpπXq “ πpf1pXq, . . . , fnpXqq for any X P pRdqn and
for any permutation π P Sn of n elements.
Otherwise put, if we permute the
input X, then the output fpXq is permuted in the same way. Given t ą 0, the
Transformer (SA), mapping pxip0qqiPrns ÞÑ pxiptqqiPrns, is permutation-equivariant
on pSd´1qn. This feature is largely untapped in classical natural language processing
applications, as the Transformer output is drawn from the distribution 1
n
řn
i“1 δxiptq
so that the ordering of the output token is lost. There exists other applications,
however where this property is used [BVE23].
2.3. Toward the complete Transformer. There are a couple of additional mech-
anisms used in practical implementations that we do not explicitly address or use
in this study. The mathematical analysis of these mechanisms remains open.
2.3.1. Multi-headed attention. Practical implementations spread out the computa-
tion of the self-attention mechanism at every t through a sequence of heads, leading
3ALBERT XLarge v2 contains all the mechanisms described in this text, namely, is a system
of the form (2.8) with 12 or 24 layers. The sequence length n is of the order of 512 or 1024,
and the tokens evolve in R4096. The dynamics are therefore high-dimensional, lending weight to
assumptions made later on (Section 4).

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
7
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 0
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 1
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 21
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 22
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 23
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 24
Figure 1. Histogram of txxiptq, xjptqyupi,jqPrns2,i‰j at different lay-
ers t in the context of the pre-trained ALBERT XLarge v2 model
([LCG`20] and https://huggingface.co/albert-xlarge-v2)3, which
has constant parameter matrices. Here we randomly selected a single
prompt, which in this context is a paragraph from a random Wikipedia
entry, and then generate the histogram of the pairwise inner products.
We see the progressive emergence of clusters all the way to the 24th
(and last) hidden layer (top), as evidenced by the growing mass at 1. If
the number of layers is increased, up to 48 say, the clustering is further
enhanced (bottom).
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 25
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 26
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 27
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 46
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 47
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
Layer 48
to the so-called multi-headed self attention. This consists in considering the follow-
ing modification to (SA):
(2.7)
9xiptq “ Pxiptq
˜ H
ÿ
h“1
n
ÿ
j“1
eβxQhxiptq,Khxjptqy
Zβ,i,hptq
Vhxjptq
¸

8
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
where Zβ,i,hptq is defined as in (2.4) for the matrices Qh and Kh. The integer H ě 1
is called the number of heads4.
The introduction of multiple heads also allows for drawing some interesting par-
allels with the literature on feed-forward neural networks, such as ResNets (2.1).
Considerable effort has been expended to understand 2-layer neural networks with
width tending to `8; more precisely, consider (2.1) with L “ 1, w P Rdˆℓ, a P Rℓˆd,
and ℓÑ `8. The infinite-width limit for Transformers is in fact very natural, as
it is realized by stacking an arbitrary large number of heads: H Ñ `8. Hence, the
same questions as for 1-hidden layer neural networks may be asked: for instance,
in the vein of [Cyb89, Bar93],
Problem 1 (Approximation). Fix d, n ě 2 and consider the 1-hidden layer Trans-
former with multi-headed self attention f H
θ : pSd´1qn Ñ pSd´1qn defined as
f H
θ px1, . . . , xnqi “ Pxi
˜ H
ÿ
h“1
n
ÿ
j“1
eβxQhxi,Khxjy
Zβ,i,h
Vhxj
¸
,
where H ě 1 and θ “ pQh, Kh, VhqhPrHs are as for (2.7). Can one approximate,
in some appropriate topology, any continuous and permutation-equivariant function
f : pSd´1qn Ñ pSd´1qn by means of some f H
θ
as H Ñ `8? The same question for
the multi-headed Transformer without layer normalization: f H
θ
: pRdqn Ñ pRdqn
defined as
f H
θ px1, . . . , xnqi “
H
ÿ
h“1
n
ÿ
j“1
eβxQhxi,Khxjy
Zβ,i,h
Vhxj,
is also open.
A universal approximation property of the above kind would then motivate
studying the training dynamics of infinite-width (i.e., infinite number of heads)
1-hidden layer Transformers, similar to what has been done for the neural network
analog in recent years [CB18, MMN18, RVE22]. None of these questions has re-
ceived a definitive answer for Transformers; see [YBR`19] for related work when
the depth is taken to infinity.
2.3.2. Feed-forward layers. The complete Transformer dynamics combines all of the
above mechanisms with a feed-forward layer. This amounts to considering dynamics
of the form
(2.8)
9xiptq “ Pxiptq
˜
wptqσ
˜ H
ÿ
h“1
n
ÿ
j“1
eβxQhxiptq,Khxjptqy
Zβ,i,hptq
Vhxjptq ` bptq
¸¸
,
where wptq, bptq and σ are as in (2.2). These layers are critical and drive the existing
results on approximation properties of Transformers [YBR`19]. Nevertheless, the
analysis of this model is beyond the scope of our current methods.
4In practical implementations, H is a divisor of d, and the query and key matrices Qh and
Kh are
d
H ˆ d rectangular. This allows for further parallelization of computations and increased
expressiveness. For mathematical purposes, we focus on working with arbitrary integers H, and
square weight matrices Qh and Kh.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
9
3. Measure to measure flow map
An important aspect of Transformers is that they are not hard-wired to take into
account the order of the input sequence, contrary to other architectures used for
natural language processing such as recurrent neural networks. In these applica-
tions, each token xip0q P Rd contains not only a word embedding wi P Rd, but also
an additional positional encoding (we postpone a discussion to Remark 3.2) which
allows tokens to also carry their position in the input sequence. Therefore, an input
sequence is perfectly encoded as a set of tokens tx1p0q, . . . , xnp0qu, or equivalently
as the empirical measure of its constituent tokens
1
n
řn
i“1 δxip0q. Recall that the
output of a transformer is also a probability measure, namely 1
n
řn
i“1 δxiptq, albeit
one that captures the likelihood of the next token. As a result, one can view Trans-
formers as flow maps between probability measures on Sd´1. To describe this flow
map, we appeal to the continuity equation, which governs precisely the evolution
of the empirical measure of particles subject to dynamics.
3.1. The continuity equation. The vector field driving the evolution of a single
particle in (SA) clearly depends on all n particles. In fact, one can equivalently
rewrite the dynamics as
(3.1)
9xiptq “ Xrµptqspxiptqq
for all i P rns and t ě 0, where
µpt, ¨q “ 1
n
n
ÿ
i“1
δxiptqp¨q
is the empirical measure, while the vector field Xrµs : Sd´1 Ñ TSd´1 reads
(3.2)
Xrµspxq “ Px
ˆ
1
Zβ,µpxq
ż
eβxx,yyy dµpyq
˙
with
(3.3)
Zβ,µpxq “
ż
eβxx,yy dµpyq.
In other words, (SA) is a mean-field interacting particle system. The evolution of
µptq is governed by the continuity equation5
(3.4)
#
Btµ ` divpXrµsµq “ 0
on Rě0 ˆ Sd´1
µ|t“0 “ µp0q
on Sd´1
satisfied in the sense of distributions.
Remark 3.1. Global existence of weak, measure-valued solutions to (3.4) for arbi-
trary initial conditions µp0q P PpSd´1q follows by arguing exactly as in [GLPR23,
Lemma A.3]. Here and henceforth, PpSd´1q stands for the set of Borel probability
measures on Sd´1.
Remark 3.2. For the sake of completeness, in this brief segue we discuss a few
ways to perform positional encoding.
The original one, proposed in [VSP`17],
proceeds as follows. Consider a sequence pwiqiPrns P pRdqn of word embeddings.
Then the positional encoding pi P Rd of the i-th word embedding is defined as
5Unless stated otherwise, ∇and div henceforth stand for the spherical gradient and divergence
respectively, and all integrals are taken over Sd´1.

10
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
ppiq2k “ sinp
k
M 2k{d q and ppiq2k`1 “ cosp
k
M 2k{d q for k P rd{2 ´ 1s, and M ą 0 is a
user-defined scalar equal to 104 in [VSP`17]. The i-th token is then defined as the
addition: xip0q “ wi `pi. Subsequent works simply use either a random6 positional
encoding (i.e., pi is just some random vector) or a trained transformation. The
addition can also be replaced with a concatenation xip0q “ rwi; pis. (See [LWLQ22,
XZ23] for details.)
Although the analysis in this paper is focused on the flow of the empirical
measure, one can also consider (3.4) for arbitrary initial probability measures
µp0q P PpSd´1q.
Both views can be linked through a mean-field limit-type re-
sult, which can be shown by making use of the Lipschitz nature of the vector
field Xrµs. The argument is classical and dates back at least to the work of Do-
brushin [Dob79]. Consider an initial empirical measure µnp0q “ 1
n
řn
i“1 δxip0q, and
suppose that the points xip0q are such that limnÑ`8 W1pµnp0q, µp0qq “ 0 for some
probability measure µp0q P PpSd´1q. (Here W1 denotes 1-Wasserstein distance–see
[Vil09] for definitions.) Consider the solutions µnptq and µptq to (3.4) with initial
data µnp0q and µp0q respectively. Dobrushin’s argument is then centered around
the estimate
W1pµnptq, µptqq ď eOp1qtW1pµnp0q, µp0qq
for any t P R, which in the case of (3.4) can be shown without much difficulty (see
[Vil01, Chapitre 4, Section 1] or [Gol16, Section 1.4.2]). This elementary mean-field
limit result has a couple of caveats. First, the time-dependence is exponential. Sec-
ond, if one assumes that the points xip0q are sampled i.i.d. according to µ0, then
W1pµnp0q, µp0qq converges to zero at rate n´
1
d´1 [Dud69, BLG14], which deterio-
rates quickly when d grows. Dimension-free convergence has been established in
some cases, for instance by a replacing the Wasserstein distance with a more careful
choice of metric as in [HHL23, Lac23] or more generally in [SFG`12]. Similarly,
the exponential time-dependence might also be improved, as recent works in the
context of flows governed by Riesz/Coulomb singular kernels, with diffusion, can
attest [RS23, GBM21] (see [LLF23] for a result in the smooth kernel case). We do
not address this question in further detail here. For more references on this well-
established topic, the reader is referred to [Vil01, Gol16, Ser20] and the references
therein.
3.2. The interaction energy. One can naturally ask whether the evolution in (3.4)
admits some quantities which are monotonic when evaluated along the flow. As it
turns out, the interaction energy
(3.5)
Eβrµs “ 1
2β
ĳ
eβxx,x1y dµpxq dµpx1q
is one such quantity. Indeed,
d
dtEβrµptqs “
ĳ
β´1eβxx,x1y dBtµpt, xq dµpt, x1q
“
ż
Xrµptqspxq ¨
ż
∇
´
β´1eβxx,x1y¯
dµpt, x1q dµpt, xq
“
ż ›››Xrµptqspxq
›››
2
Zβ,µptqpxq dµpt, xq
(3.6)
6This rationale supports the assumption that initial tokens are drawn at random, which we
make use of later on.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
11
for any t ě 0 by using integration by parts. Recalling the definition of Zβ,µpxq
in (3.3), we see that e´β ď Zβ,µpxq ď eβ for all x P Sd´1.
The identity (3.6)
therefore indicates that Eβ increases along trajectories of (3.4). (Similarly, should
V “ ´Id, the energy Eβ would decrease along trajectories.) This begs the question
of characterizing the global minima and maxima of Eβ, which is the goal of the
following result.
Proposition 3.3. Let β ą 0 and d ě 2. The unique global minimizer of Eβ over
PpSd´1q is the uniform measure7 σd. Any global maximizer of Eβ over PpSd´1q is
a Dirac mass δx˚ centered at some point x˚ P Sd´1.
This result lends credence to our nomenclature of the case V “ Id as attractive,
and V “ ´Id as repulsive. The reader should be wary however that in this result
we are minimizing or maximizing Eβ among all probability measures on Sd´1.
Should one focus solely on discrete measures, many global minima appear–these
are discussed in Section 7.1. This is one point where the particle dynamics and
the mean-field flow deviate. We now provide a brief proof of Proposition 3.3 (see
[Tan17] for a different approach).
Proof of Proposition 3.3. Let fptq “ eβt. The interaction energy then reads
Eβrµs “ 1
2
ĳ
fpxx, x1yq dµpxq dµpx1q.
The proof relies on an ultraspherical (or Gegenbauer) polynomial expansion of fptq:
fptq “
`8
ÿ
k“0
pfpk; λqk ` λ
λ
Cλ
k ptq
for t P r´1, 1s, where λ “ d´2
2 , Cλ
k are Gegenbauer polynomials, and
pfpk; λq “
Γpλ ` 1q
Γpλ ` 1
2qΓp 1
2q
1
Cλ
k p1q
ż 1
´1
fptqCλ
k ptqp1 ´ t2qλ´ 1
2 dt
where Cλ
k p1q ą 0 (see [DX13, Section 1.2]). According to [BD19, Proposition 2.2],
a necessary and sufficient condition for Proposition 3.3 to hold is to ensure that
pfpk; λq ą 0 for all k ě 1. To show this, we use the Rodrigues formula [Sze39, 4.1.72]
Cλ
k ptq “ p´1qk2k
k!
Γpk ` λqΓpk ` 2λq
ΓpλqΓp2k ` 2λq p1 ´ t2q´pλ´ 1
2 q
ˆ d
dt
˙k
p1 ´ t2qk`λ´ 1
2 ,
and the fact that Cλ
k p´tq “ p´1qkCλ
k ptq for t P r´1, 1s, which in combination with
integration by parts yield
ż 1
´1
tℓCλ
k ptqp1 ´ t2qλ´ 1
2 dt
#
ą 0
if ℓě k and ℓ´ k is even
“ 0
otherwise .
We conclude by using the power series expansion of f.
□
7That is, the Lebesgue measure on Sd´1, normalized to be a probability measure.

12
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
3.3. A Wasserstein gradient flow proxy. In view of (3.6), one could hope to
see the continuity equation (3.4) as the Wasserstein gradient flow of Eβ, or possibly
some other functional (see the seminal papers [Ott01, JKO98], and [AGS05, Vil09]
for a complete treatment). The long time asymptotics of the PDE can then be
analyzed by studying convexity properties of the underlying functional, by analogy
with gradient flows in the Euclidean case.
For (3.4) to be the Wasserstein gradient flow of Eβ, the vector field Xrµs defined
in (3.2) ought to be the gradient of the first variation δEβ of Eβ. However, notice
that Xrµs is a logarithmic derivative:
(3.7)
Xrµspxq “ ∇log
ż
β´1eβxx,yy dµpyq.
(This observation goes beyond Q “ K “ Id and V “ ˘Id as long as QJK “ V .)
And because of the lack of symmetry, it has been shown in [SABP22] that (3.7) is
not the gradient of the first variation of a functional.
One is then naturally led to look for ways to "symmetrize" (3.4). A first at-
tempt would be to remove the logarithm in (3.7), which amounts to removing the
denominator in (3.2). This is one point where working on the unit sphere is use-
ful: should one work on Rd, without layer normalization, the resulting vector field
would grow exponentially with the size of the support of the measure, rendering
a Cauchy-Lipschitz argument inapplicable. On the contrary, on Sd´1 the resulting
equation is perfectly well-posed.
Remark 3.4. Considering the Transformer dynamics on Rd, thus without layer
normalization, the authors in [SABP22] propose an alternative symmetric model:
they replace the self-attention (stochastic) matrix by a doubly stochastic one, gen-
erated from the Sinkhorn iteration. This leads to a Wasserstein gradient flow (see
[SABP22, Proposition 2]), but the resulting attention mechanism is implicitly ex-
pressed as a limit of Sinkhorn iterations. Understanding the emergence of clusters
for this model is an interesting but possibly challenging question.
In view of the above discussion, we are inclined to propose the surrogate model
(USA)
9xiptq “ Pxiptq
˜
1
n
n
ÿ
j“1
eβxxiptq,xjptqyxjptq
¸
,
which is obtained by replacing the partition function Zβ,iptq by n. As a matter of
fact, (USA) presents a remarkably similar qualitative behavior–all of the results we
show in this paper are essentially the same for both dyanmics.
The continuity equation corresponding to (USA), namely
(3.8)
$
&
%
Btµpt, xq ` div
ˆ
Px
ˆż
eβxx,x1yx1 dµpt, x1q
˙
µpt, xq
˙
“ 0
µ|t“0 “ µ0
for pt, xq P Rě0 ˆ Sd´1, can now be seen as a Wasserstein gradient flow for the
interaction energy Eβ defined in (3.5).
Lemma 3.5. Consider the interaction energy Eβ : PacpSd´1q Ñ Rě0 defined
in (3.5). Then the vector field
Xrµspxq “ Px
ˆż
eβxx,x1yx1 dµpx1q
˙

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
13
satisfies
(3.9)
Xrµspxq “ ∇δEβrµspxq
for any µ P PacpSd´1q and x P Sd´1, where δEβrµs denotes the first variation of Eβ.
We omit the proof which follow from standard Otto calculus [Vil09, Chapter 15].
Here PacpSd´1q denotes the set of probability measures which are absolutely contin-
uous with respect to the Lebesgue measure σd on Sd´1. A derivation of the gradient
flow formulation for discrete measures is provided in Remark 3.7, for which all of
the conclusions discussed in this section also hold.
We can actually write (3.9) more succinctly by recalling the definition of the
convolution of two functions on Sd´1 [DX13, Chapter 2]: for any g P L1pSd´1q and
f : r´1, 1s Ñ R such that t ÞÑ p1 ´ t2q
d´3
2 fptq is integrable,
pf ˚ gqpxq “
ż
fpxx, yyqgpyq dσdpyq.
This definition has a natural extension to the convolution of a function f (with the
above integrability) and a measure µ P PpSd´1q. We can hence rewrite
Eβrµs “ 1
2
ż
pGβ ˚ µqpxq dµpxq
where r´1, 1s Q Gβptq “ β´1eβt, and so
Xrµspxq “ ∇pGβ ˚ µqpxq.
Thus, (3.8) takes the equivalent form
(3.10)
$
&
%
Btµpt, xq ` div
´
∇
`
Gβ ˚ µpt, ¨q
˘
pxqµpt, xq
¯
“ 0
for pt, xq P Rě0 ˆ Sd´1
µ|t“0 “ µ0
for x P Sd´1.
The considerations above lead us to the following Lyapunov identity.
Lemma 3.6. The solution µ P C0pRě0; PacpSd´1qq to (3.8) satisfies
d
dtEβrµptqs “
ż ›››∇
´
Gβ ˚ µpt, ¨q
¯
pxq
›››
2
dµpt, xq
for t ě 0.
Interestingly, (3.10) is an aggregation equation, versions of which have been stud-
ied in great depth in the literature.
For instance, clustering in the spirit of an
asymptotic collapse to a single Dirac measure located at the center of mass of the
initial density µp0, ¨q has been shown for aggregation equations with singular ker-
nels in [BCM08, BLR11, CDF`11], motivated by the Patlak-Keller-Segel model of
chemotaxis. Here, one caveat (and subsequently, novelty) is that (3.10) is set on
Sd´1 which makes the analysis developed in these references difficult to adapt or
replicate.
Remark 3.7. Let us briefly sketch the particle version of the Wasserstein gradient
flow (3.8). When µptq “ 1
n
řn
i“1 δxiptq, the interaction energy (3.5) takes the form
EβpXq “
1
2βn2
n
ÿ
i“1
n
ÿ
j“1
eβxxi,xjy

14
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
where X “ px1, . . . , xnq P pSd´1qn. Denoting by ∇X the gradient associated to the
standard Riemannian metric on pSd´1qn, we get the dynamics
(3.11)
9Xptq “ n∇XEβpXptqq.
Indeed, the gradient on pSd´1qn is simply ∇“ pB1, . . . , Bnq where Bi is the gradient
in Sd´1 acting on the i-th copy in pSd´1qn. Therefore
BiEβpXptqq “
1
βn2
n
ÿ
j“1
Pxiptq
´
eβxxiptq,xjptqyβxjptq
¯
“ 1
n 9xiptq
which yields (3.11).
Part 2. Clustering
As alluded to in the introductory discussion, clustering is of particular relevance
in tasks such as next-token prediction. Therein, the output measure encodes the
probability distribution of the next token, and its clustering indicates a small num-
ber of possible outcomes. In Sections 4 and 5, we show several results which indicate
that the limiting distribution is a point mass. While it may appear that this leaves
no room for diversity or randomness, which is at odds with practical observations,
these results hold for the specific choice of parameter matrices, and apply in possi-
bly very long time horizons. Numerical experiments indicate a more subtle picture
for different parameters—for instance, there is an appearance of a long metastable
phase during which the particles coalesce in a small number of clusters, which ap-
pears consistent with behavior in pre-trained models (Figure 1). We are not able
to theoretically explain this behavior as of now.
Ultimately, the appearance of clusters is somewhat natural, since the Trans-
former dynamics are a weighted average of all particles, with the weights being
hard-wired to perform a fast selection of particles most similar to the i-th particle
being queried. This causes the emergence of leaders which attract all particles in
their vicinity. In the natural language processing interpretation, where particles
represent tokens, this further elucidates the wording attention as the mechanism of
inter-token attraction, and the amplitude of the inner product between tokens can
be seen as a measure of their semantic similarity.
4. A single cluster in high dimension
The clustering results we present in this section are restricted to the high-
dimensional regime.
We cover the case of arbitrary dimension d, when β « 0,
in Section 5. Further avenues for tackling the low-dimensional case are given in
Section 6 whereas the repulsive case V “ ´Id is discussed in Section 7.1.
4.1. Clustering when d ě n. Our first result shows the emergence of a single
cluster in high dimension and reads as follows.
Theorem 4.1. Let n ě 1 and β ą 0. Suppose d ⩾n. Consider the unique solution
pxip¨qqiPrns P C0pRě0; pSd´1qnq to the Cauchy problem8 for (SA) or (USA), corre-
sponding to an initial sequence of points pxip0qqiPrns P pSd´1qn distributed uniformly
8We refer to the initial value problem for the ODE as Cauchy problem (terminology typically
reserved for PDEs) due to the equivalence with the PDE satisfied by the empirical measure.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
15
at random. Then almost surely there exists x˚ P Sd´1 and constants C, λ ą 0 such
that
}xiptq ´ x˚} ď Ce´λt
holds for all i P rns and t ě 0.
In fact, let Q and K be arbitrary dˆd matrices. Then the same result also holds
for the solution to the corresponding Cauchy problem for (2.3) with V “ Id (or the
natural analogue of (USA) with these parameters).
This is referred to as convergence toward consensus in collective behavior models.
When d ě n and the points pxip0qqiPrns P pSd´1qn are distributed uniformly at
random, with probability one there exists9 w P Sd´1 such that xw, xip0qy ą 0 for
any i P rns. In other words, all of the initial points lie in an open hemisphere almost
surely. The proof of Theorem 4.1 thus follows as a direct corollary of the following
result, which holds for any n ě 1 and d ě 2:
Lemma 4.2 (Cone collapse). Let β ą 0 and let pxip0qqiPrns P pSd´1qn be such
that there exists w P Sd´1 for which xxip0q, wy ą 0 for any i P rns.
Consider
the unique solution pxip¨qqiPrns P C0pRě0; pSd´1qnq to the corresponding Cauchy
problem for (SA) or (USA). Then there exists x˚ P Sd´1 and constants C, λ ą 0
such that
}xiptq ´ x˚} ď Ce´λt
holds for all i P rns and t ě 0.
In fact, let Q and K be arbitrary dˆd matrices. Then the same result also holds
for the solution to the corresponding Cauchy problem for (2.3) with V “ Id (or the
natural analogue of (USA) with these parameters).
Remark 4.3. Lemma 4.2 implies that
␣
p¯xiqiPrns P pSd´1qn : ¯x1 “ . . . “ ¯xn
(
is Lya-
punov asymptotically stable as a set. In fact, it is exponentially stable.
Lemma 4.2 is reminiscent of results on interacting particle systems on the sphere
(see [CLP15, Theorem 1] for instance), and the literature on synchronization for
the Kuramoto model on the circle ([ABK`22, Lemma 2.8], [HR20, Theorem 3.1]
and Section 6.2). We often make use of the following elementary lemma.
Lemma 4.4. Let f : Rě0 Ñ R be a differentiable function such that
ż `8
0
|fptq| dt ` sup
tPRě0
ˇˇˇ 9fptq
ˇˇˇ ă `8.
Then limtÑ`8 fptq “ 0.
The proof of Lemma 4.2 is an adaptation of [CLP15, Theorem 1]. We present it
here for completeness.
Proof of Lemma 4.2. We focus on the case (USA), and set
aijptq :“ eβxxiptq,xjptqy ą 0.
The proof for (SA) is identical, and one only needs to change the coefficients aijptq
by Zβ,iptq´1eβxxiptq,xjptqy throughout. Also note that since we only make use of the
positivity of the coefficients ai,jptq throughout the proof, all arguments are readily
generalizable to the case of arbitrary dˆd matrices Q and K appearing in the inner
products.
9This weak version of Wendel’s theorem (Theorem 4.5) is easy to see directly.

16
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Step 1. Clustering. For t ě 0, consider
iptq P arg min
iPrns
xxiptq, wy.
Fix t0 ě 0. We have
ˆ d
dtxxipt0qp¨q, wy
˙ ˇˇˇ
t“t0
“
n
ÿ
j“1
aipt0qjpt0q
´
xxjpt0q, wy ´ xxipt0qpt0q, xjpt0qyxxipt0qpt0q, wy
¯
ě 0.
This is implies that all points remain within the same open hemisphere at all times
and the map
t ÞÑ rptq :“ min
iPrnsxxiptq, wy
is non-decreasing on Rě0. It is also bounded from above by 1. We may thus define
r8 :“ limtÑ`8 rptq. Note that r8 ě rp0q ą 0 by assumption. By compactness,
there exist a sequence of times ttku`8
k“1 with tk Ñ `8, and some pxiqiPrns P pSd´1qn
such that limkÑ`8 xiptkq “ xi for all i P rns. Using the definition of rptq, we also
find that
xxj, wy ě r8
for all j P rns, and by continuity, there exists i P rns such that xxi, wy “ r8. Then
lim
kÑ`8x 9xiptkq, wy “
n
ÿ
j“1
aijpxw, xjy ´ xxi, xjyxxi, wyq ě r8
n
ÿ
j“1
aijp1 ´ xxi, xjyq,
(4.1)
where we set aij :“ eβxxi,xjy ą 0. Notice that
lim
kÑ`8
ż `8
tk
x 9xipsq, wy ds “ r8 ´ lim
kÑ`8xxiptkq, wy “ 0,
and by using the equation (USA) we also find that |x:xiptq, wy| “ Ope2βq for any
t ě 0.
Therefore by Lemma 4.4, the left-hand side of (4.1) is equal to 0, and
consequently the right-hand side term as well. This implies that x1 “ . . . “ xn :“
x˚. Repeating the argument by replacing w with x˚, we see that the extraction of
a sequence ttku`8
k“1 as above is not necessary, and therefore
(4.2)
lim
tÑ`8 xiptq “ x˚
for all i P rns.
Step 2. Exponential rate. We now improve upon (4.2). Set
αptq :“ min
iPrnsxxiptq, x˚y.
From (4.2) we gather that there exists some t0 ą 0 such that αptq ě 1
2 for all t ě t0.
Also, in view of what precedes we know that x˚ lies in the convex cone generated
by the points x1ptq, . . . , xnptq for any t ą 0. Thus, there exists some η P p0, 1s such
that ηx˚ is a convex combination of the points x1ptq, . . . , xnptq, which implies that
(4.3)
x˚ “
n
ÿ
k“1
θkptqxkptq,
for some
n
ÿ
k“1
θkptq ě 1,
θkptq ě 0
@k P rns.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
17
We find
(4.4)
9αptq “ x 9xiptqptq, x˚y ě
n
ÿ
j“1
aiptqjptqp1 ´ xxiptqptq, xjptqyqαptq
On another hand,
(4.5)
min
jPrnsxxiptqptq, xjptqy ď
n
ÿ
k“1
θkptqxxiptqptq, xkptqy “ xxiptqptq, x˚y “ αptq.
Plugging (4.5) into (4.4) and using aijptq ě n´1e´2β we get
(4.6)
9αptq ě
1
2ne2β p1 ´ αptqq
for t ě t0. Applying the Grönwall inequality we get
(4.7)
1 ´ αptq ď 1
2e´
1
2neβ pt´t0q
for all t ě t0. The conclusion follows.
□
In the case d ă n, we can still apply Wendel’s theorem (recalled below) together
with Lemma 4.2 to obtain clustering to a single point with probability at least pn,d
for some explicit pn,d P p0, 1q.
Theorem 4.5 (Wendel, [Wen62]). Let d, n ě 1 be such that d ď n. Let x1, . . . , xn
be n i.i.d. uniformly distributed points on Sd´1. The probability that these points
all lie in the same hemisphere is:
P
´
Dw P Sd´1 : xxi, wy ą 0
for all
i P rns
¯
“ 2´pn´1q
d´1
ÿ
k“0
ˆn ´ 1
k
˙
.
4.2. Precise quantitative convergence in high dimension. In the regime
where n is fixed and d Ñ `8, in addition to showing the formation of a clus-
ter as in Theorem 4.1, it is possible to quantitatively describe the entire evolution
of the particles with high probability. To motivate this, on the one hand we note
that since the dynamics evolve on Sd´1, inner products are representative of the dis-
tance between points, and clustering occurs if xxiptq, xjptqy Ñ 1 for any pi, jq P rns2
as t Ñ `8. On the other hand, if d " n, n points in a generic initial sequence are
almost orthogonal by concentration of measure [Ver18, Chapter 3], and we are thus
able to compare their evolution with that of an initial sequence of truly orthogonal
ones.
We begin by describing the case of exactly orthogonal initial particles, which is
particularly simple as the dynamics are described by a single parameter.
Theorem 4.6. Let β ě 0, d, n ě 2 be arbitrary. Consider an initial sequence
pxip0qqiPrns P pSd´1qn of n pairwise orthogonal points: xxip0q, xjp0qy “ 0 for i ‰ j,
and let pxip¨qqiPrns P C0pR⩾0; pSd´1qnq denote the unique solution to the correspond-
ing Cauchy problem for (SA) (resp. for (USA)). Then the angle =pxiptq, xjptqq is
the same for all distinct i, j P rns:
=pxiptq, xjptqq “ θβptq

18
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for t ⩾0 and some θβ P C0pRě0; Tq. Furthermore, for (SA), γβptq :“ cospθβptqq
satisfies
$
’
&
’
%
9γβptq “ 2eβγβptqp1 ´ γβptqqppn ´ 1qγβptq ` 1q
eβ ` pn ´ 1qeβγβptq
for t ě 0
γβp0q “ 0 ,
(4.8)
and for (USA), we have
$
&
%
9γβptq “ 2
neβγβptqp1 ´ γβptqqppn ´ 1qγβptq ` 1q
for t ě 0
γβp0q “ 0 .
(4.9)
Here and henceforth, T “ R{2πZ denotes the one-dimensional torus. We provide
a brief proof of Theorem 4.6 just below. The following result then shows that when
d " n, t ÞÑ γβptq is a valid approximation for t ÞÑ xxiptq, xjptqy for any distinct
i, j P rns.
Theorem 4.7. Fix β ⩾0 and n ě 2. Then there exists some d˚pn, βq ě n such
that for all d ě d˚pn, βq, the following holds. Consider a sequence pxip0qqiPrns of n
i.i.d. uniformly distributed points on Sd´1, and let pxip¨qqiPrns P C0pRě0; pSd´1qnq
denote the unique solution to the corresponding Cauchy problem for (SA). Then
there exist C “ Cpn, βq ą 0 and λ “ λpn, βq ą 0, such that with probability at least
1 ´ 2n2d´1{64,
(4.10)
ˇˇˇxxiptq, xjptqy ´ γβptq
ˇˇˇ ⩽min
#
2 ¨ cpβqnt
c
log d
d
, Ce´λt
+
holds for any i ‰ j and t ě 0, where cpβq “ e10 maxt1,βu, and γβ is the unique
solution to (4.8).
Since the proof is rather lengthy, we defer it to Appendix A. It relies on combining
the stability of the flow with respect to the initial data (entailed by the Lipschitz
nature of the vector field) with concentration of measure. An analogous statement
also holds for (USA), and more details can be found in Remark A.1, whereas the
explicit values of C and λ can be found in (A.15). The upper bound in (4.10) is of
interest in regimes where d and/or t are sufficiently large as the error in (4.10) is
trivially bounded by 2.
Proof of Theorem 4.6. We split the proof in two parts. We focus on proving the
result for the dynamics (SA), since the same arguments readily apply to the dy-
namics (USA).
Part 1.
The angle θβptq. We first show there exists θ P C0pRě0; Tq such that
θptq “ =pxiptq, xjptqq for any distinct pi, jq P rns2 and t ě 0. Since the initial tokens
are orthogonal (and thus d ě n), we may consider an orthonormal basis pe1, . . . , edq
of Rd such that xip0q “ ei for i P rns. Let π : rds Ñ rds be a permutation. By
decomposing any x P Sd´1 in this basis, we define Pπ : Sd´1 Ñ Sd´1 as
Pπ
˜ n
ÿ
i“1
aiei
¸
“
n
ÿ
i“1
aieπpiq.
Setting yiptq “ Pπpxiptqq for i P rns, we see that yiptq solves (SA) with initial
condition yip0q “ Pπpxip0qq. But pxπp1qptq, . . . , xπpnqptqq is a solution of (SA) by

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
19
permutation equivariance, and it has the same initial condition since Pπpxip0qq “
xπpiqp0q. Consequently, we deduce that Pπpxiptqq “ xπpiqptq for any t ě 0 and any
i P rds. Hence
xxiptq, xjptqy “ xPπpxiptqq, Pπpxjptqqy “ xxπpiqptq, xπpjqptqy
which concludes the proof.
Part 2.
The curve γβptq. By virtue of the orthogonality assumption we have
γβp0q “ cospθβp0qq “ 0. To prove that γβptq satisfies (4.8) for the case of (SA),
recall that
Pxiptqpxjptqq “ xjptq ´ xxiptq, xjptqy xiptq.
Then for k ‰ i,
9γβptq “ 2x 9xiptq, xkptqy
“
n
ÿ
j“1
ˆ
eβxxiptq,xjptqy
řn
ℓ“1 eβxxiptq,xℓptqy
˙
pxxjptq, xkptqy ´ xxiptq, xjptqyxxiptq, xkptqyq .
Since the denominator in the above expression is equal to pn ´ 1qeβγβptq ` eβ, we
end up with
9γβptq “
2eβγβptq
pn ´ 1qeβγβptq ` eβ
n
ÿ
j“1
´
xxjptq, xkptqy ´ xxiptq, xjptqyxxiptq, xkptqy
¯
“
2eβγβptq
pn ´ 1qeβγβptq ` eβ p1 ´ γβptq2 ` pn ´ 2qpγβptq ´ γβptq2qq,
as desired.
□
4.3. Metastability and a phase transition. An interesting byproduct of The-
orem 4.6 and Theorem 4.7 is the fact that they provide an accurate approximation
of the exact phase transition curve delimiting the clustering and non-clustering
regimes, in terms of t and β.
To be more precise, given an initial sequence
pxip0qqiPrns P pSd´1qn of random points distributed independently according to
the uniform distribution on Sd´1, and for any fixed 0 ă δ ! 1, we define the phase
transition curve as the boundary
Γd,δ “ B
"
t, β ě 0: t “ arg inf
sě0
´
Ppxx1psq, x2psqy ě 1 ´ δq “ 1 ´ 2n2d´ 1
64
¯*
where pxip¨qqiPrns denotes the solution to the corresponding Cauchy problem for (SA).
(Here the choice of the first two particles instead of a random distinct pair is jus-
tified due to permutation equivariance.) Theorem 4.7 then gives the intuition that
over compact subsets of pRě0q2, Γd,δ should be well-approximated by
(4.11)
Γ8,δ “
!
t, β ě 0: γβptq “ 1 ´ δ
)
.
This is clearly seen in Figure 2, along with the fact that the resolution of this
approximation increases with d Ñ `8.
Figure 2 appears to contain more information than what we may gather from
Theorem 4.1, Theorem 4.6 and Theorem 4.7. In particular, for small d, we see the
appearance of a zone (white/light blue in Figure 2) of parameters pt, βq for which
the probability of particles being clustered is positive, but not close to one.
A
careful inspection of this region reveals that points are grouped in a finite number

20
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
d “ 2
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
d “ 8
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
0.0
0.2
0.4
0.6
0.8
1.0
d “ 32
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
d “ 128
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
d “ 512
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
0.0
0.2
0.4
0.6
0.8
1.0
d “ 1024
Figure 2. Plots of the probability that randomly initialized particles
following (SA) cluster to a single point as a function of t and β: we graph
the function pt, βq ÞÑ Ppx1p0q,...,xnp0qq„σd ptxx1ptq, x2ptqy ě 1 ´ δuq, which
is equal to pt, βq ÞÑ Ppx1p0q,...,xnp0qq„σd,i‰j fixed ptxx1ptq, x2ptqy ě 1 ´ δuq
by permutation equivariance. We compute this function by generating
the average of the histogram of txxiptq, xjptqy ě 1 ´ δ : pi, jq P rns2, i ‰
ju over 210 different realizations of initial sequences. Here, δ “ 10´3,
n “ 32, while d varies. We see that the curve Γ8,δ defined in (4.11)
approximates the actual phase transition with increasing accuracy as d
grows, as implied by Theorem 4.7.
of clusters; see Figure 3.
The presence of such a zone indicates the emergence
of a long-time metastable state where points are clustered into several groups but
eventually relax to a single cluster in long-time. This two-time-scale phenomenon
is illustrated in Figure 3 and prompts us to formulate the following question.
Problem 2. Do the dynamics enter a transient metastable state, in the sense that
for β " 1, all particles stay in the vicinity of m ă n clusters for long periods of
time, before they all collapse to the final cluster tx˚u?
There have been important steps towards a systematic theory of metastabil-
ity for gradient flows, with applications to nonlinear parabolic equations–typically
reaction-diffusion equations such as the Allen-Cahn or Cahn-Hilliard equations
[OR07, KO02]. While these tools to not readily apply to the current setup, they
form an important starting point to answer this question.
Finally, one may naturally ask whether the clustering and phase diagram conclu-
sions persist when the parameter matrices pQ, K, V q are significantly more general:
some illustrations10 are given in Figure 4.
10See github.com/borjanG/2023-transformers-rotf for additional figures which indicate that
this phenomenon appears to hold in even more generality.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
21
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
t = 0.0
t = 18.0
t = 30.0
t = 0.0
t = 18.0
t = 30.0
Figure 3. We zoom in on the phase diagram (Figure 2) for the dy-
namics on the circle: d “ 2. For β “ 4, 9, we also display a trajectory
of (SA) for a randomly drawn initial condition at times t “ 2.5, 18, 30.
We see that the particles settle at 2 clusters when β “ 4 (bottom right)
and 3 clusters when β “ 9 (top right), for a duration of time. This
reflects our metastability claim for large β in the low-dimensional case.
The regime β ! 1 (a single cluster emerges) is covered in Section 5.
Problem 3. Can the conclusions of Theorem 4.6–Theorem 4.7 be generalized to
the case of random matrices pQ, K, V q?
5. A single cluster for small β
Our first attempt to remove the assumption d " 1 consists in looking at extreme
choices of β. The case β “ `8 is of little interest since all particles are fixed by
the evolution. We therefore first focus on the case β “ 0, before moving to the case
β ! 1 by a perturbation argument.
5.1. The case β “ 0. For β “ 0, both (SA) and (USA) read as
(5.1)
9xiptq “ Pxiptq
˜
1
n
n
ÿ
j“1
xjptq
¸
,
t ⩾0.
The following result shows that generically over the initial points, a single cluster
emerges. It complements a known convergence result ([FL19, Theorem 2]) for (5.1).
In [FL19, Theorem 2], the authors show convergence to an antipodal configuration,
in the sense that n ´ 1 particles converge to some x˚ P Sd´1, with the last particle
converging to ´x˚. Moreover, once convergence is shown to hold, it holds with
an exponential rate. Mimicking the proof strategy of [BCM15, Theorem 2.2] and
[HKR18, Theorem 3.2], we sharpen this result by showing that the appearance of
an antipodal particle is non-generic over the choice of initial conditions.
Theorem 5.1. Let d, n ě 2. For Lebesgue almost any initial sequence pxip0qqiPrns P
pSd´1qn, there exists some point x˚ P Sd´1 such that the unique solution pxip¨qqiPrns P

22
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
Q, K, V in real Ginibre ensemble
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
0.0
0.2
0.4
0.6
0.8
1.0
QJK Wigner, V ľ 0 GOE
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
Q, K in real Ginibre ensemble, V “ QJK
0
5
10
15
20
25
30
t
1
2
3
4
5
6
7
8
9
0.0
0.2
0.4
0.6
0.8
1.0
QJK Wigner, V “ QJK
Figure 4. Phase diagrams (see Figure 2 for explanations) for some
choices of random matrices pQ, K, V q; here d “ 128, n “ 32. Sharp
phase transitions as well as metastable regions appear in all cases. In
the "gradient flow" case QJK “ V , there is a remarkable resemblance
to the well-understood case QJK “ V “ Id, which we are not able to
explain for the moment.
C0pRě0; pSd´1qnq to the corresponding Cauchy problem for (5.1) satisfies
lim
tÑ`8 xiptq “ x˚
for any i P rns.
We refer the interested reader to Appendix B for the proof.
5.2. The case β ! 1. Theorem 5.1 has some implications for small but positive β,
something which is already seen in Figure 2 and Figure 3. This is essentially due
to the fact that, formally,
9xiptq “ Pxiptq
˜
1
n
n
ÿ
j“1
xjptq
¸
` Opβq
for β ! 1. So, during a time ! β´1, the particles do not feel the influence of the
remainder Opβq and behave as in the regime β “ 0. This motivates

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
23
Theorem 5.2. Fix d, n ě 2. For β ě 0, let Sβ Ă pSd´1qn be the subset consisting of
all initial sequences for which the associated solutions to (SA) (or (USA)) converge
to one cluster as t Ñ `8. Then
lim
βÑ0 PpSβq “ 1.
Proof. We focus on the dynamics (SA), but the proof is in fact identical in the case
of (USA).
For α ą 0, we say that a set formed from n points z1, . . . , zn P pSd´1qn is α–
clustered if for any i, j P rns, xzi, zjy ą α holds. Observe that if tz1, . . . , znu is
α–clustered for some α ě 0, then the solution to the Cauchy problem for (SA) (for
arbitrary β ě 0) with this sequence as initial condition converges to a single cluster,
since w “ z1 satisfies the assumption in Lemma 4.2.
Now, for any integer m ě 1, we denote by Sm
0 Ă S0 the set of initial sequences
x1p0q, . . . , xnp0q in pSd´1qn for which the solution px0
i p¨qqiPrns to the associated
Cauchy problem for (5.1) is 3
4–clustered at time t “ m, namely
(5.2)
xx0
i pmq, x0
jpmqy ą 3
4
holds for all i, j P rns. We see that Sm
0
is an open set for any integer m ě 1.
Moreover, Sm
0 Ă Sm`1
0
according to the proof of Lemma 4.2, and Ť`8
m“1 Sm
0 “ S0.
This implies that
(5.3)
lim
mÑ`8 PpSm
0 q “ 1.
We now show that the solution to (SA) is near that of (5.1), starting from the same
initial condition, when β is small. Using the Duhamel formula, we find
xβ
i ptq ´ x0
i ptq “
ż t
0
n
ÿ
j“1
˜
eβxxβ
i psq,xβ
j psqy
řn
k“1 eβxxβ
i psq,xβ
kpsqy
¸
Pxβ
i psqpxβ
j psqq ds
´
ż t
0
1
n
n
ÿ
j“1
Px0
i psqpx0
jpsqq ds
“
ż t
0
n
ÿ
j“1
ˆ 1
n ` O
ˆβ
n
˙˙
Pxβ
i psqpxβ
j psqq ds
´
ż t
0
1
n
n
ÿ
j“1
Px0
i psqpx0
jpsqq ds,
where we used that all particles lie on Sd´1 for all times. Employing Grönwall, we
deduce
›››xβ
i ptq ´ x0
i ptq
››› ď Opβqe3t
(5.4)
for all t ě 0, β ě 0 and i P rns. Due to (5.4), there exists some βm ą 0 such that
for any β P r0, βms,
(5.5)
›››xβ
i pmq ´ x0
i pmq
››› ď 1
8.
For this to hold, we clearly need βm Ñ 0 as m Ñ `8. Combining (5.2) and (5.5),
we gather that for any initial condition in Sm
0 , the solution pxβ
i p¨qqiPrns to the

24
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
corresponding Cauchy problem for (SA) is
1
2–clustered at time t “ m, namely
satisfies
xxβ
i pmq, xβ
j pmqy ą 1
2
for all i, j P rns and β P r0, βms. Thus Sm
0
Ă Sβ for any β P r0, βms by virtue of
Lemma 4.2, which together with (5.3) concludes the proof.
□
One can naturally ask
Problem 4. Does PpSβq “ 1 hold for any β ě 0?
Part 3. Further questions
We conclude this manuscript by discussing several avenues of research that can
lead to a finer understanding of the clustering phenomenon and generalizations of
our results, and which, we believe, are of independent mathematical interest.
6. Dynamics on the circle
We study the dynamics (SA) and (USA) in the special case d “ 2, namely on
the unit circle S1 Ă R2. This model, parametrized by angles and related to the
celebrated Kuramoto model, is of independent interest and deserves a complete
mathematical analysis.
6.1. Angular equations. On the circle S1, all particles xiptq P S1 are of course
completely characterized by the angle θiptq P T: xiptq “ cospθiptqqe1 ` sinpθiptqqe2
where e1 “ p1, 0q and e2 “ p0, 1q P R2.
We focus on the dynamics (USA) for
simplicity. For any i P rns and t ě 0, we may derive the equation satisfied by θiptq
from cospθiptqq “ xxiptq, e1y: differentiating in t and plugging into (USA) we obtain
9θiptq “ ´
n´1
sinpθiptqq
˜ n
ÿ
j“1
eβxxiptq,xjptqy”
xxjptq, e1y ´ xxiptq, xjptqyxxiptq, e1y
ı¸
where we used the definition of the projection (if θiptq “ 0 for some t, we differ-
entiate the equality sinpθiptqq “ xxiptq, e2y instead, which also leads to (6.1) in the
end). Observing that
xxiptq, xjptqy “ cospθiptq ´ θjptqq,
we find
9θiptq “ ´
n´1
sinpθiptqq
˜ n
ÿ
j“1
eβ cospθiptq´θjptqq”
cospθjptqq ´ cospθiptq ´ θjptqq cospθiptqq
ı¸
.
Using elementary trigonometry, we conclude that
(6.1)
9θiptq “ ´ 1
n
n
ÿ
j“1
eβ cospθiptq´θjptqq sinpθiptq ´ θjptqq.
The case β “ 0 is exactly the Kuramoto model recalled in Section 6.2. Suppose for
the time being that β ą 0. Defining the function hβ : T Ñ Rě0 as
hβpθq “ eβ cospθq,

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
25
we have effectively deduced that the empirical measure of the angles, νptq “
1
n
řn
j“1 δθjptq, which is a measure on the torus T, is a solution to the continuity
equation
Btνptq ` BθpXrνptqsνptqq “ 0,
on Rě0 ˆ T,
where
Xrνspθq “ 1
β
´
h1
β ˚ ν
¯
pθq.
When the particles xiptq follow (SA), one readily checks that the same continuity
equation is satisfied but rather with the field
Xrνspθq “ 1
β
ˆh1
β ˚ ν
hβ ˚ ν
˙
pθq.
6.2. The Kuramoto model. As mentioned above, when β “ 0, (6.1) is a partic-
ular case of the Kuramoto model [Kur75]:
(6.2)
9θiptq “ ωi ` K
n
n
ÿ
j“1
sinpθjptq ´ θiptqq,
where K ą 0 is a prescribed coupling constant, and ωi P T are the intrinsic natural
frequencies of the oscillators θiptq. It is known that for sufficiently small coupling
strength K, the oscillators θiptq in the Kuramoto model (6.2) do not synchro-
nize in long time. It is also known that when K exceeds some critical threshold
value, a phase transition occurs, leading to the synchronization of a fraction of
the oscillators.
If K is chosen very large, there is total synchronization of the
oscillators in long time. For more on the mathematical aspects of the Kuramoto
model, we refer the reader to the review papers [Str00, ABV`05, HKPZ16] (see also
[CCH`14, Chi15, FGVG16, DFGV18, HR20, TSS20, ABK`22] for a non-exhaustive
list of other recent mathematical results on the subject).
When all the frequencies ωi are equal to some given frequency, ω P R say, after
a change of variable of the form θiptq Ð θiptq ´ ωt, the dynamics in (6.2) become
the gradient flow
9θptq “ n∇Fpθq
where the energy F : Tn Ñ Rě0 reads
(6.3)
Fpθq “ K
2n2
n
ÿ
i“1
n
ÿ
j“1
cospθi ´ θjq.
The oscillators can be viewed as attempting to maximize this energy. The energy
F is maximized when all the oscillators are synchronized, that is, θi “ θ˚ for some
θ˚ P T and for all i P rns. As the dynamics follow a gradient system, the equilibrium
states are the critical points of the energy, namely those satisfying ∇Fpθq “ 0. The
local maxima of F correspond to equilibrium states θ that are physically achievable,
since small perturbations thereof return the system back to θ.
Some authors consider a variant of the Kuramoto model where the oscillators
are interacting according to the edges of a graph. In other words, the coefficients
Aij of the graph’s adjacency matrix are inserted in the sum in (6.3) as weights,
and the dynamics are then the corresponding gradient flow. A recent line of work
culminating with [ABK`22] has established that synchronization occurs with high
probability for Erdős–Rényi graphs with parameter p, for every p right above the
connectivity threshold.

26
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Coming back to our dynamics (6.1), we notice that it can also be written as a
gradient flow on Tn:
9θptq “ n∇Eβpθptqq,
for the interaction energy Eβ : Tn Ñ Rě0 defined as
(6.4)
Eβpθq “
1
2βn2
n
ÿ
i“1
n
ÿ
j“1
eβ cospθi´θjq,
which is maximized when θi “ θ˚ for some θ˚ P T and for all i P rns. In the spirit
of [LXB19], we suggest the following open problem—we recall that a critical point
is called a strict saddle point of Eβ if the Hessian of Eβ at these points has at least
one positive eigenvalue.
Problem 5. With the exception of the global maxima, are all critical points of Eβ
strict saddle points?
By classical arguments, recalled in Appendix B, a positive answer to Problem
5 would imply that for all initial conditions except a set of measure zero, all θiptq
converge under the dynamics (6.1) to a common limit as t Ñ `8.
In the proof of Theorem 5.1 we already demonstrate that for some small enough
β0 “ β0pnq ą 0, and for all β P r0, β0q, the energy Eβ indeed satisfies the property
in Problem 5. The same actually holds for β P rβ1, `8q for some β1 “ β1pnq ą 0:
Proposition 6.1. Let d “ 2 and n ě 2. There exists some β1 “ β1pnq ą 0 such
that for all β ą β1, any critical point of the function Eβ defined in (6.4) is a strict
saddle point with exception of the global maxima θ1 “ ¨ ¨ ¨ “ θn.
The proof may be found in Appendix C; in fact, we also gather that β1pnq ě Cn2
for some C ą 0.
Extensions of the Kuramoto model of the form
(6.5)
9θiptq “ ωi ` K
n
n
ÿ
j“1
hpθjptq ´ θiptqq,
for a general non-linearity h : T Ñ R, which contains both (6.2) and our model (6.1)
as particular cases, have already been studied in the physics literature. For instance,
we refer the reader to [Dai92] (see also [ABV`05, page 158]), where many heuristics
are proposed to address the behavior of solutions to these dynamics. We are not
aware of mathematical results for (6.1)–besides Proposition 6.1–if β is not close to
0. We nevertheless have some hope that handling the dynamics (6.1) is easier than
dealing with (6.5) for a general h; for instance, we have
hβpθq “ eβ cospθq “
ÿ
kPZ
Ikpβqeikθ
where Ikpβq are the modified Bessel function of the first kind, whose properties
have been extensively studied.
7. General matrices
Figure 4 hints at the likelihood of the clustering phenomenon being significantly
more general than just the case Q “ K “ V “ Id. However, extending our proofs
to more general parameter matrices does not appear to be straightforward and is
an open problem. Here we discuss a couple of particular cases (without excluding
other approaches).

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
27
7.1. The repulsive case. As seen from Lemma 3.6, in the repulsive case V “ ´Id
the interaction energy Eβ decreases along trajectories. Recall that the unique global
minimum of Eβ over PpSd´1q is the uniform distribution (Proposition 3.3).
In
contrast, we explain in this section that many different configurations of n points
may yield global minima for Eβ when minimized over empirical measures with n
atoms.
We thus focus on minimizing Eβ over the set PnpSd´1q of empirical measures,
namely sums of n Dirac masses. Rewriting Eβ as
Eβrµs “ e2β
2β
ĳ
e´β}x´x1}2 dµpxq dµpx1q,
it turns out that minimizing Eβ over PnpSd´1q is precisely the problem of find-
ing optimal configurations of points on Sd´1, which has direct links to the sphere
packing problem [CK07, CKM`22] and coding theory [DGS91]. For µ P PnpSd´1q,
we can equivalently rewrite Eβ in terms of the set of support points C Ă Sd´1,
#C “ n:
Eβrµs “ HβrCs “ e2β
2n2β
ÿ
x,x1PC
e´β}x´x1}2.
In [CK07], Cohn and Kumar characterize the global minima C of Hβ. To state
their result, we need the following definition.
Definition 7.1. Let n ě 2. A set of points C “ tx1, . . . , xnu Ă Sd´1 is called a
spherical t-design if
ż
ppxq dσdpxq “ 1
n
n
ÿ
i“1
ppxiq
for all polynomials p of d variables, of total degree at most t. The set of points C is
called a sharp configuration if there are m distinct inner products between pairwise
distinct points in C, for some m ą 1, and if it is a spherical p2m ´ 1q-design.
The following result is a special case of [CK07, Theorem 1.2].
Theorem 7.2 ([CK07]). Let n ě 2. Any global minimum of Hβ among C Ă Sd´1,
#C “ n is either a sharp configuration, or the vertices of a 600-cell11.
The set of sharp configurations is not known for all regimes of n, d or m (the
largest m such that the configuration is a spherical m-design). A list of known
examples is provided in [CK07, Table 1]: it consists of vertices of full-dimensional
polytopes (specifically, regular polytopes whose faces are simplices), or particular
derivations of the E8 root lattice in R8 and the Leech lattice in R24. We defer
the reader to [CK07] and the illustrative experimental paper [BBC`09] for further
detail. A complete picture of the long time behavior of Transformers in the repulsive
case remains open.
7.2. Pure self-attention. An alternative avenue for conducting such an analysis
which has shown to be particularly fruitful consists in removing the projector Px,
leading to
(7.1)
9xiptq “
1
Zβ,iptq
n
ÿ
j“1
eβxQxiptq,KxjptqyV xjptq
11A 600-cell is a particular 4-dimensional convex polytope with n “ 120 vertices.

28
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for all i P rns and t P Rě0. In fact, in [GLPR23] we analyze precisely these dynamics,
and show different clustering results depending on the spectral properties of the
matrix V . We briefly summarize our findings in what follows.
7.2.1. A review of [GLPR23]. For most choices of value matrices V , without rescal-
ing time, most particles diverge to ˘8 and no particular pattern emerges. To make
a very rough analogy, (7.1) "looks like" 9xiptq “ V xiptq (which amounts to having
Pijptq “ δij instead of (2.5)), whose solutions are given by xiptq “ etV xip0q. To
discern the formation of clusters, we introduce the rescaling12
(7.2)
ziptq “ e´tV xiptq,
which are solutions to
(7.3)
9ziptq “
1
Zβ,iptq
n
ÿ
j“1
eβxQetV ziptq,KetV zjptqyV pzjptq ´ ziptqq
for i P rns and t ě 0, where
Zβ,iptq “
n
ÿ
k“1
eβxQetV ziptq,KetV zkptqy,
whereas the initial condition remains the same, namely xip0q “ zip0q. It is crucial
to notice that the coefficients Aijptq (see (2.5)) of the self-attention matrix for the
rescaled particles ziptq are the same as those for the original particles xiptq. The
weight Aijptq indicates the strength of the attraction of ziptq by zjptq. In [GLPR23]
we show that the rescaled particles ziptq cluster toward well-characterized geometric
objects as t Ñ `8 for various choices of matrices pQ, K, V q.
Our results are
summarized in Table 1 below, whose first two lines are discussed thereafter.
V
K and Q
Limit geometry
Result in [GLPR23]
V “ Id
QJK ą 0
vertices of convex polytope
Theorem 3.1
λ1pV q ą 0, simple
xQφ1, Kφ1y ą 0
union of 3 parallel hyperplanes
Theorem 4.1
V paranormal
QJK ą 0
polytope ˆ subspaces
Theorem 5.1
V “ ´Id
QJK “ Id
single cluster at origin˚
Theorem C.5
Table 1. Summary of the clustering results of [GLPR23]. ˚All results
except for the case V “ ´Id hold for the time-scaled dynamics (7.3).
When V “ Id, outside from exceptional situations, all particles cluster to vertices
of some convex polytope. Indeed, since the velocity 9ziptq is a convex combination
of the attractions zjptq ´ ziptq, the convex hull Kptq of the ziptq shrinks and thus
converges to some convex polytope. The vertices of the latter attract all particles
as t Ñ `8. When the eigenvalue with largest real part of V , denoted by λ1pV q,
is simple and positive, the rescaled particles ziptq cluster on hyperplanes which are
parallel to the direct sum of the eigenspaces of the remaining eigenvalues. Roughly
speaking, the coordinates of the points ziptq along the eigenvector of V correspond-
ing to λ1pV q quickly dominate the matrix coefficients Pijptq in (7.3) due to the
factors etV zjptq. For more results and insights regarding clustering on Rd, we refer
12The rescaling (7.2) should be seen as a surrogate for layer normalization.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
29
the reader to [GLPR23]. We nonetheless leave the reader with the following general
question:
Problem 6. Is it possible to extend the clustering results of Table 1 to other cases
of pQ, K, V q? What are the resulting limit shapes?
7.2.2. Singular dynamics. We mention another intriguing question, whose answer
would allow for a transparent geometric understanding of clustering for (7.3). Let
pQ, K, V q be given d ˆ d matrices. For β ą 0, we consider the system of coupled
ODEs
(7.4)
9ziptq “
1
Zβ,iptq
n
ÿ
j“1
eβxQziptq,KzjptqyV pzjptq ´ ziptqq,
where once again
Zβ,iptq “
n
ÿ
k“1
eβxQziptq,Kzkptqy.
For any T ą 0, and any fixed initial condition pzip0qqiPrns P pRdqn, as β Ñ `8, we
expect that the solution to (7.4) converges uniformly on r0, Ts to a solution of
(7.5)
9ziptq “
1
|Ciptq|
ÿ
jPCiptq
V pzjptq ´ ziptqq
where
(7.6)
Ciptq “
!
j P rns: xQziptq, Kzjptqy ě xQziptq, Kzkptqy
for all k P rns
)
.
However, defining a notion of solution to (7.5)–(7.6) is not straightforward, as
illustrated by the following example.
Example 7.3. Suppose d “ 2, n “ 3.
Let Q “ K “ V “ Id and z1p0q “
p1, 1q, z2p0q “ p´1, 1q, z3p0q “ p0, 0q. Consider the evolution of these particles
through (7.5)–(7.6). The points z1ptq and z2ptq do not move, because it is easily
seen that Ciptq “ tiu for i P t1, 2u. On the other hand, the point z3ptq can be chosen
to solve either of three equations: 9z3ptq “ z1ptq ´ z3ptq, or 9z3ptq “ z2ptq ´ z3ptq, or
even 9z3ptq “ 1
2pz1ptq ` z2ptqq ´ z3ptq. In any of these cases, both (7.5) and (7.6)
remain satisfied for almost every t ě 0.
It is possible to prove the existence of solutions to (7.5)–(7.6) defined in the sense
of Filippov13: for this, we can either use a time-discretization of (7.5)–(7.6), or use a
convergence argument for solutions to (7.4) as β Ñ `8. Uniqueness however does
not hold, as illustrated by Example 7.3. This naturally leads us to the following
question:
Problem 7. Is it possible to establish a selection principle (similar to viscosity or
entropy solutions) for solutions to (7.5)–(7.6) which allows to restore uniqueness?
In the affirmative, is it possible to revisit the clustering results of [GLPR23] and
Problem 6 in the setting of (7.5)–(7.6)?
We believe that (7.5)–(7.6) is also an original model for collective behavior. There
are some similarities in spirit with methods arising in consensus based optimization
(CBO for short), [PTTM17, CJLZ21]. With CBO methods, one wishes to minimize
13We thank Enrique Zuazua for this suggestion.

30
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
a smooth and bounded, but otherwise arbitrary function f : Rd Ñ R by making
use of the Laplace method
lim
βÑ`8
ˆ
´ 1
β log
ż
Rd e´βfpxq dρpxq
˙
“
inf
xPsupppρq fpxq,
which holds for any fixed ρ P PacpRdq.
This is accomplished by considering a
McKean-Vlasov particle system of the form
dxiptq “ ´λpxiptq´vfqHϵpfpxiptqq´fpvrµnptqsqq dt`
?
2σ|xiptq´vrµnptqs| dWiptq
for fixed β ą 0, with drift parameter λ ą 0 and noise parameter σ ě 0; Hϵ is a
particular smoothed Heaviside function, and µnptq is the empirical measure of the
particles. The point vrµs P Rd is a weighted average of the particles:
vrµs “
1
Zβ,µ
ż
Rd e´βfpxqx dµpxq
where Zβ,µ “
ş
Rd e´βfpxq dµpxq. Morally speaking, particles which are near a min-
imum of f have a larger weight.
The drift term is a gradient relaxation (for a
quadratic potential) towards the current weighted average position of the batch of
particles. The diffusion term is an exploration term whose strength is proportional
to the distance of the particle from the current weighted average. Results of con-
vergence to a global minimizer do exist, under various smallness assumptions on
the initial distribution of the particles, and assumptions on the relative size of the
coefficients. They rely on the analysis of the associated Fokker-Planck equation,
see [CJLZ21, CD22], and also [FHPS21] for the analog on Sd´1. We point out that
similarities are mainly in spirit—these results and analysis are inapplicable to our
setting because there is no analog for fpxq. Nonetheless, they do raise the following
interesting question:
Problem 8. What can be said about the long time limit of Transformers with a
noise/diffusion term of strength σ ą 0?
The question is of interest for any of the Transformers models presented in what
precedes.
8. Approximation and control
Understanding the expressivity, namely the ability of a neural network to re-
produce any map in a given class (by tuning its parameters), is essential. Two
closely related notions reflect the expressivity of neural networks: interpolation—
the property of exactly matching arbitrarily many input and target samples—and
(universal) approximation—the property of approximating input-target functional
relationships in an appropriate topology. We refer the reader to [CLLS23] for a
primer on the relationship between these two notions in the contex of deep neural
networks.
For discrete-time Transformers, universal approximation has been shown to hold
in [YBR`19], making use of a variant of the architecture with translate parameters
and letting the number of layers go to infinity; see also [ADTK23] and the review
[JLLW23].
In the context of flow maps (from Rd to Rd), it is now well understood that
interpolation and approximation reflect the controllability properties of the system.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
31
The transfer of control theoretical techniques to the understanding of expressiv-
ity has borne fruit, both in terms of controllability results [AS22, CLT20, TG22,
LLS22, RBZ23, VR23, CLLS23] and optimal control insights [LCT18, GZ22]. We
are however not aware of control-theoretical results in which arbitrarily many input
measures ought to be mapped to as many output measures, as would be the case
for Transformers.
Acknowledgments
We thank Sébastien Bubeck, Matthew Rosenzweig, Sylvia Serfaty, Kimi Sun,
and Rui Sun for discussions.
Appendix
Appendix A. Proof of Theorem 4.7
Proof. We focus on the dynamics (SA), since the proof for (USA) follows from very
similar computations.
Step 1.
The flow map is Lipschitz. We begin by showing that the trajectories
satisfy a Lipschitz property with respect to the initial data.
To this end, let
pxip¨qqiPrns P C0pRě0; pSd´1qnq and pyip¨qqiPrns P C0pRě0; pSd´1qnq be two solutions
to the Cauchy problem for (SA) associated to data pxip0qqiPrns and pyip0qqiPrns
respectively. For any i P rns and t ě 0, we have
xiptq ´ yiptq “ xip0q ´ yip0q
`
ż t
0
n
ÿ
j“1
ˆ
eβxxipsq,xjpsqy
řn
k“1 eβxxipsq,xkpsqy
˙ `
xjpsq ´ xxipsq, xjpsqyxipsq
˘
ds
´
ż t
0
n
ÿ
j“1
ˆ
eβxyipsq,yjpsqy
řn
k“1 eβxyipsq,ykpsqy
˙ `
yjpsq ´ xyipsq, yjpsqyyipsq
˘
ds.
(A.1)
We see that
›››››
ż t
0
n
ÿ
j“1
ˆ
eβxxipsq,xjpsqy
řn
k“1 eβxxipsq,xkpsqy
˙
pxjpsq ´ yjpsqq ds
››››› ď
ż t
0
max
jPrns }xjpsq ´ yjpsq} ds.
(A.2)
On another hand, since the softmax function with a parameter β is β–Lipschitz
(with respect to the Euclidean norm), we also get
›››››
ż t
0
n
ÿ
j“1
ˆ
eβxxipsq,xjpsqy
řn
k“1 eβxxipsq,xkpsqy ´
eβxyipsq,yjpsqy
řn
k“1 eβxyipsq,ykpsqy
˙
yjpsq ds
›››››
ď βn
1
2
ż t
0
˜ n
ÿ
j“1
”
xxipsq, xjpsqy ´ xyipsq, yjpsqy
ı2
¸ 1
2
ds
ď 2βn
ż t
0
max
jPrns }xjpsq ´ yjpsq} ds.
(A.3)

32
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Using (A.2), (A.3) and arguing similarly for the remaining terms in (A.1), we deduce
that
}xiptq ´ yiptq} ď }xip0q ´ yip0q} ` 10 maxt1, βun
ż t
0
max
jPrns }xjpsq ´ yjpsq} ds.
Maximizing over i P rns and applying the Grönwall inequality yields
(A.4)
max
jPrns }xjptq ´ yjptq} ď cpβqnt max
jPrns }xjp0q ´ yjp0q} ,
for any i P rns and t ě 0.
Step 2.
Almost orthogonality. Let x1p0q, . . . , xnp0q P Sd´1 be the random i.i.d.
initial points. We prove that with high probability, there exist n pairwise orthogonal
points y1p0q, . . . , ynp0q P Sd´1, such that for any i P rns,
(A.5)
}xip0q ´ yip0q} ď
c
log d
d
.
To this end, we take y1p0q “ x1p0q and then construct the other points yip0q by
induction. Assume that y1p0q, . . . , yip0q are constructed for some i P rns, using only
knowledge about the points x1p0q, . . . , xip0q. Then by Lévy’s concentration of mea-
sure, since xi`1p0q is independent from x1p0q, . . . , xip0q and uniformly distributed
on Sd´1,
P
˜#
dist
`
xi`1p0q, spanty1p0q, . . . , yip0quK˘
ď
c
log d
d
+¸
ě 1 ´ 4id´1{64,
for some universal constants c, C ą 0. Using the union bound, we gather that the
event
A0 “ t(A.5) is satisfied for any i P rnsu
has probability at least p0 “ 1 ´ 2n2d´1{64. We now consider the event
A “ A0 X tTheorem 4.1 is satisfiedu
which, since d ě n and thus the second event has probability 1, also holds with
probability at least p0 “ 1 ´ 2n2d´1{64. For the remainder of the proof, we assume
that A is satisfied.
Step 3. Proof of (4.10). Let pyip¨qqiPrns P C0pRě0; pSd´1qnq denote the unique solu-
tion to the Cauchy problem for (SA) corresponding to the initial datum pyip0qqiPrns.
A combination of (A.4) and (A.5) yields
(A.6)
}xiptq ´ yiptq} ď cpβqnt
c
log d
d
for any i P rns and t ě 0, under A. Combining (A.6) with Theorem 4.6 we obtain
(A.7)
ˇˇˇxxiptq, xjptqy ´ γβptq
ˇˇˇ ď 2cpβqnt
c
log d
d
for any i ‰ j and t ě 0, under A.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
33
We turn to the proof of the second part of (4.10). For this, we prove that for
large times t, both γβptq and xxiptq, xjptqy are necessarily close to 1. We first show
that
(A.8)
1 ´ γβptq ď 1
2 exp
¨
˝
n2eβ
2
´
n ` e
β
2
¯ ´
nt
n ` e
β
2
˛
‚
for any t ě 0. To this end, we notice that t ÞÑ γβptq is increasing and thus γβptq ě 0,
as well as 9γβptq ě
1
neβ as long as γβptq ď 1
2. Therefore,
γβ
ˆneβ
2
˙
ě 1
2.
We deduce that for t ě neβ
2 ,
9γβptq ě np1 ´ γβptqq
n ` e
β
2
.
Integrating this inequality from neβ
2
to t, we obtain (A.8). We now set d˚pn, βq ě n
such that
(A.9)
d
log d ě 16cpβq2
γβp 1
nq2
holds for any d ě d˚pn, βq. According to Lemma 4.2, since A is satisfied, there
exists x˚ P Sd´1 such that xiptq Ñ x˚ for any i P rns as t Ñ `8. We set
αptq :“ min
iPrnsxxiptq, x˚y,
and prove that
(A.10)
1 ´ αptq ď exp
˜
1 ´ γβ
` 1
n
˘
t
2ne2β
¸
.
To this end, let us first prove that
(A.11)
α
ˆ 1
n
˙
ě 1
2γβ
ˆ 1
n
˙
.
From Step 2 in the proof of Lemma 4.2, we gather that x˚ lies in the convex cone
generated by the points x1ptq, . . . , xnptq for any t ą 0, and so the decomposition
(4.3) holds. Taking the inner product of xip 1
nq with the decomposition (4.3) at time
t “ 1
n, we get
α
ˆ 1
n
˙
ě
min
pi,jqPrns2
B
xi
ˆ 1
n
˙
, xj
ˆ 1
n
˙F
ě γβ
ˆ 1
n
˙
´ 2cpβq
c
logpdq
d
ě 1
2γβ
ˆ 1
n
˙
,
where the second inequality comes from (A.6) evaluated at time t “
1
n, and the
last inequality comes from (A.9).
This is precisely (A.11).
Using the notation
aijptq “ Zβ,iptq´1eβxxiptq,xjptqy as in the proof of Lemma 4.2, we now find
(A.12)
9αptq “ x 9xiptqptq, x˚y ě
n
ÿ
j“1
aiptqjptqp1 ´ xxiptqptq, xjptqyqαptq

34
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for one of the indices iptq P rns achieving the minimum in the definition of αptq.
Combining this with (A.11), we gather that αptq ě αp 1
nq for t ě 1
n. But
(A.13)
min
jPrnsxxiptqptq, xjptqy ď
n
ÿ
k“1
θkptqxxiptqptq, xkptqy “ xxiptqptq, x˚y “ αptq.
Plugging (A.13) into (A.12) and using aijptq ě n´1e´2β we get
(A.14)
9αptq ě
1
ne2β α
ˆ 1
n
˙
p1 ´ αptqq
for t ě
1
n. Integrating (A.14) from
1
n to t, we get (A.10). We therefore deduce
from (A.10) that
xxiptq, xjptqy ě 1 ´ exp
ˆ1 ´ γβp 1
nqt
2ne2β
˙
holds for any distinct i, j P rns. Together with (A.8), we then get
(A.15)
ˇˇˇxxiptq, xjptqy ´ γβptq
ˇˇˇ ď exp
ˆ1 ´ γβp 1
nqt
2ne2β
˙
` 1
2 exp
˜
n2eβ
2pn ` e
β
2 q
´
nt
n ` e
β
2
¸
.
Finally, combining (A.7) and (A.15) we obtain (4.10).
□
Remark A.1. An analogous statement to Theorem 4.7 holds for (USA), where γβ
would rather be the unique solution to (4.9). More concretely, Step 1 in the proof
is only slightly changed–the constant one obtains in the analogue of (4.10) is rather
cpβqnt with cpβq “ e10βe2β. Step 2 remains unchanged. In Step 3, (A.8) is replaced
by γβp n
2 q ě 1
2 and
1 ´ γβptq ď 1
2 exp
´
´e
β
2
´
t ´ n
2
¯¯
.
The rest of the proof then remains essentially unchanged.
Appendix B. Proof of Theorem 5.1
The proof of Theorem 5.1 relies on standard arguments from dynamical sys-
tems, upon noticing that the evolution (5.1) is a gradient flow for the energy
E0 : pSd´1qn Ñ R defined as
E0px1, . . . , xnq “ ´ 1
n
n
ÿ
i“1
n
ÿ
j“1
xxi, xjy.
Since the dynamics are the gradient flow of a real-analytic functional on the compact
real-analytic manifold pSd´1qn, the celebrated Łojasiewicz theorem [Loj63], in the
form given by [HKR18, Corollary 5.1]–which is valid in the context of general
compact Riemannian manifolds–, implies that for any initial condition X P pSd´1qn,
the solution ΦtpXq P pSd´1qn converges to some critical point X˚ P pSd´1qn of E0
as t Ñ `8.
We recall that a strict saddle point of E0 is a critical point of E0 at which the
Hessian of E0 has at least one strictly negative eigenvalue. Theorem 5.1 then follows
by combining the following couple of lemmas with the Łojasiewicz theorem.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
35
Lemma B.1. Let M be a compact Riemannian manifold and let f : M Ñ R be a
smooth function. The set of initial conditions X0 P M for which the gradient flow
(B.1)
# 9Xptq “ ´∇fpXptqq
Xp0q “ X0
converges to a strict saddle point of f is of volume zero.
Lemma B.2. Any critical point px1, . . . , xnq P pSd´1qn of E0 which is not a global
minimum, namely such that x1 “ . . . “ xn, is a strict saddle point.
Proof of Lemma B.1. Let us denote by ΦtpX0q :“ Xptq, t ě 0 the solution to (B.1).
We denote by S Ă M the set of strict saddle points of f, and by A Ă M the set
of initial conditions X0 P M for which ΦtpX0q converges to a strict saddle point of
f as t Ñ `8. For any y P S, we denote by By a ball in which the local center-
stable manifold W sc
locpyq exists (see [Shu13], Theorem III.7 and Exercise III.3 for the
adaptation to flows). Using compactness, we may write the union of these balls as a
countable union Ť
kPI Byk (where I is countable and yk P M for k P I). If X0 P A,
there exists some t0 ě 0 and k P I such that ΦtpX0q P Byk for all t ě t0. From
the center-stable manifold theorem ([Shu13], Theorem III.7 and Exercise III.3) we
gather that ΦtpX0q P W sc
locpykq for t ě t0, hence X0 P Φ´tpW sc
locpykqq for all t ě t0.
The dimension of W sc
locpykq is at most dimpMq ´ 1, thus it has zero volume. Since
Φt is a diffeomorphism on a compact manifold, Φ´t preserves null-sets and hence
Φ´tpW sc
locpykqq has zero volume for all t ě 0. Therefore A, which satisfies
A Ă
ď
kPI
ď
ℓPN
Φ´ℓpW sc
locpykqq
has volume zero.
□
Proof of Lemma B.2. We extend the proof idea of [Tay12, Theorem 4.1] as follows.
Let px1, . . . , xnq P pSd´1qn be a critical point of E0, and assume that the points xi
are not all equal to eachother.
Step 1. We first prove that there exists a set of indices S Ă rns such that
(B.2)
ÿ
iPS
ÿ
jPSc
xxi, xjy ă 0.
To this end, define
m :“
n
ÿ
j“1
xj,
and consider two cases. If m ‰ 0, then we deduce from ∇E0px1, . . . , xnq “ 0 that
for any j P rns, xj is collinear with m. Thus xj “ ˘x1 for any j P rns. Setting
S “ tj P rns: xj “ `x1u,
we can see that (B.2) holds, unless S “ rns which has been excluded. Now suppose
that m “ 0. Then by expanding xm, xiy “ 0, we find that for any i P rns
´1 “
n
ÿ
j“2
xxj, x1y ,
holds, which again implies (B.2) with S “ t1u.

36
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Step 2. In this second step we look to deduce from (B.2) that px1, . . . , xnq is a strict
saddle point. Consider an arbitrary non-zero skew-symmetric matrix V and define
the perturbation
xiptq “
#
xi
i R S
etV xi
i P S.
Set E0ptq “ E0px1ptq, . . . , xnptqq. Note that we have
E0ptq “ const. ´ 2
n
ÿ
iPS
ÿ
jPSc
xxiptq, xjy ,
where we grouped time-independent terms into the constant (recall that etV is an
orthogonal matrix, since skew-symmetric matrices are the Lie algebra of SOpdq).
Thus
E1
0ptq “ ´ 2
n
ÿ
iPS
ÿ
jPSc
x 9xiptq, xjy
E2
0ptq “ ´ 2
n
ÿ
iPS
ÿ
jPSc
x :xiptq, xjy .
Since px1, . . . , xnq is a critical point of E0, we have E1
0p0q “ 0. On the other hand,
since :xip0q “ V 2xi we have
E2
0p0q “ ´ 2
n
ÿ
iPS
ÿ
jPSc
xV 2xi, xjy .
We claim that given (B.2), there must exist some skew-symmetric matrix V such
that E2
0p0q ă 0. Indeed, if d is even, then we just take V as the block-diagonal
matrix with repeated block
„
0
1
´1
0
ȷ
,
so that V 2 “ ´Id. If d is odd, we can represent
´Id “
1
d ´ 1
dÿ
j“1
V 2
j ,
where Vj is the same block-diagonal matrix, with the exception that the j-th block
is a 1 ˆ 1 zero-matrix.
If each Vj were to yield E2
0p0q ě 0, then it would vio-
late (B.2). Thus, E2
0p0q ă 0 for some well-chosen skew-symmetric V , which proves
that px1, . . . , xnq is a strict saddle point.
□
Appendix C. Proof of Proposition 6.1
Proof. Let pθ1, . . . , θnq P Tn be a critical point such that all eigenvalues of the
Hessian of Eβ are non-positive. We intend to show that if β is sufficiently large,
then necessarily θ1 “ ¨ ¨ ¨ “ θn. To that end, note that the non-positivity of the
Hessian of Eβ implies in particular that for any subset of indices S Ă rns, we must
have
(C.1)
ÿ
iPS
ÿ
jPS
BθiBθjEβpθ1, . . . , θnq ď 0 .

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
37
Notice that for any i, j P rns,
BθiEβpθ1, . . . , θnq “ 1
n2
ÿ
m‰i
´ sinpθi ´ θmqeβ cospθi´θmq
and
BθiBθjEβpθ1, . . . , θnq “ 1
n2 ¨
#
gpθi ´ θjq,
i ‰ j
´ ř
m‰i gpθi ´ θmq,
i “ j ,
where we set gpxq :“
1
n2 pcospxq ´ β sin2pxqqeβ cospxq. Plugging this expression back
into (C.1) and simplifying, we obtain
(C.2)
ÿ
iPS
ÿ
jPSc
gpθi ´ θjq ě 0 .
Let us now define τ ˚
β be the unique solution on r0, π
2 q of the equation
β sin2pτq “ cospτq .
Note that τ ˚
β is a monotonically decreasing function of β, and τ ˚
β “
1`op1q
?β
as
β Ñ `8.
The importance of τ ˚
β is in implying the following property of the
function g: for any τ R r´τ ˚
β , τ ˚
β s, we must have that gpτq ă 0. We arrive at the
following conclusion: it must be that for any proper subset S Ă rns there exists, by
virtue of (C.2), some index j P Sc whose distance to S is strictly smaller than τ ˚
β .
So now let us start with S “ t1u and grow S inductively by adding those points j
at distance ă τ ˚
β at each induction step. If β is large enough so that
pn ´ 1qτ ˚
β ă π ,
then in the process of adding points we have travelled a total arc-length ă π. Thus
it must be that the collection of points θ1, . . . , θn is strictly contained inside a half-
circle of angular width ă π. By Lemma 4.2 we know that there can be no critical
points of Eβ that are strictly inside some halfspace, unless that critical point is
trivial: θ1 “ ¨ ¨ ¨ “ θn. This completes the proof.
□
References
[ABK`22]
Pedro Abdalla, Afonso S Bandeira, Martin Kassabov, Victor Souza, Steven H Stro-
gatz, and Alex Townsend. Expander graphs are globally synchronising. arXiv preprint
arXiv:2210.12788, 2022.
[ABV`05]
Juan A Acebrón, Luis L Bonilla, Conrad J Pérez Vicente, Félix Ritort, and Renato
Spigler. The Kuramoto model: A simple paradigm for synchronization phenomena.
Reviews of Modern Physics, 77(1):137, 2005.
[ADTK23]
Silas Alberti, Niclas Dern, Laura Thesing, and Gitta Kutyniok. Sumformer: Universal
Approximation for Efficient Transformers. In Topological, Algebraic and Geometric
Learning Workshops 2023, pages 72–86. PMLR, 2023.
[AGS05]
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient flows: in metric spaces
and in the space of probability measures. Springer Science & Business Media, 2005.
[AS22]
Andrei Agrachev and Andrey Sarychev. Control on the manifolds of mappings with
a view to the deep learning. Journal of Dynamical and Control Systems, 28(4):989–
1008, 2022.
[Bar93]
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal
function. IEEE Transactions on Information Theory, 39(3):930–945, 1993.
[BBC`09]
Brandon Ballinger, Grigoriy Blekherman, Henry Cohn, Noah Giansiracusa, Eliza-
beth Kelly, and Achill Schürmann. Experimental study of energy-minimizing point
configurations on spheres. Experimental Mathematics, 18(3):257–283, 2009.

38
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
[BCM08]
Adrien Blanchet, José A Carrillo, and Nader Masmoudi. Infinite time aggregation for
the critical Patlak-Keller-Segel model in R2. Communications on Pure and Applied
Mathematics, 61(10):1449–1481, 2008.
[BCM15]
Dario Benedetto, Emanuele Caglioti, and Umberto Montemagno. On the complete
phase synchronization for the Kuramoto model in the mean-field limit. Communica-
tions in Mathematical Sciences, 13(7):1775–1786, 2015.
[BD19]
Dmitriy Bilyk and Feng Dai. Geodesic distance Riesz energy on the sphere. Transac-
tions of the American Mathematical Society, 372(5):3141–3166, 2019.
[BKH16]
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv
preprint arXiv:1607.06450, 2016.
[BLG14]
Emmanuel Boissard and Thibaut Le Gouic. On the mean speed of convergence of
empirical and occupation measures in Wasserstein distance. In Annales de l’IHP
Probabilités et statistiques, volume 50, pages 539–563, 2014.
[BLR11]
Andrea L Bertozzi, Thomas Laurent, and Jesús Rosado. Lp theory for the multidi-
mensional aggregation equation. Communications on Pure and Applied Mathematics,
64(1):45–83, 2011.
[BVE23]
Nicholas M Boffi and Eric Vanden-Eijnden. Deep learning probability flows and en-
tropy production rates in active matter. arXiv preprint arXiv:2309.12991, 2023.
[CB18]
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for
over-parameterized models using optimal transport. Advances in Neural Information
Processing Systems, 31, 2018.
[CCH`14]
José A Carrillo, Young-Pil Choi, Seung-Yeal Ha, Moon-Jin Kang, and Yongduck Kim.
Contractivity of transport distances for the kinetic Kuramoto equation. Journal of
Statistical Physics, 156(2):395–415, 2014.
[CD22]
Louis-Pierre Chaintron and Antoine Diez. Propagation of chaos: a review of models,
methods and applications. II. Applications. 2022.
[CDF`11]
J. A. Carrillo, M. DiFrancesco, A. Figalli, T. Laurent, and D. Slepčev. Global-in-time
weak measure solutions and finite-time aggregation for nonlocal interaction equations.
Duke Mathematical Journal, 156(2):229 – 271, 2011.
[Chi15]
Hayato Chiba. A proof of the Kuramoto conjecture for a bifurcation structure of
the infinite-dimensional Kuramoto model. Ergodic Theory and Dynamical Systems,
35(3):762–834, 2015.
[CJLZ21]
José A Carrillo, Shi Jin, Lei Li, and Yuhua Zhu. A consensus-based global opti-
mization method for high dimensional machine learning problems. ESAIM: Control,
Optimisation and Calculus of Variations, 27:S5, 2021.
[CK07]
Henry Cohn and Abhinav Kumar. Universally optimal distribution of points on
spheres. Journal of the American Mathematical Society, 20(1):99–148, 2007.
[CKM`22] Henry Cohn, Abhinav Kumar, Stephen Miller, Danylo Radchenko, and Maryna Via-
zovska. Universal optimality of the E8 and Leech lattices and interpolation formulas.
Annals of Mathematics, 196(3):983–1082, 2022.
[CLLS23]
Jingpu Cheng, Qianxiao Li, Ting Lin, and Zuowei Shen. Interpolation, approximation
and controllability of deep neural networks. arXiv preprint arXiv:2309.06015, 2023.
[CLP15]
Marco Caponigro, Anna Chiara Lai, and Benedetto Piccoli. A nonlinear model of
opinion formation on the sphere. Discrete & Continuous Dynamical Systems-A,
35(9):4241–4268, 2015.
[CLT20]
Christa Cuchiero, Martin Larsson, and Josef Teichmann. Deep neural networks,
generic universal interpolation, and controlled ODEs. SIAM Journal on Mathematics
of Data Science, 2(3):901–919, 2020.
[CRBD18]
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural
ordinary differential equations. Advances in Neural Information Processing Systems,
31, 2018.
[CS07]
Felipe Cucker and Steve Smale. Emergent behavior in flocks. IEEE Transactions on
Automatic Control, 52(5):852–862, 2007.
[Cyb89]
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathe-
matics of Control, Signals and Systems, 2(4):303–314, 1989.
[Dai92]
Hiroaki Daido. Order function and macroscopic mutual entrainment in uniformly
coupled limit-cycle oscillators. Progress of Theoretical Physics, 88(6):1213–1218, 1992.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
39
[DFGV18]
Helge Dietert, Bastien Fernandez, and David Gérard-Varet. Landau damping to par-
tially locked states in the Kuramoto model. Communications on Pure and Applied
Mathematics, 71(5):953–993, 2018.
[DGCC21]
Subhabrata Dutta, Tanya Gautam, Soumen Chakrabarti, and Tanmoy Chakraborty.
Redesigning the transformer architecture with insights from multi-particle dynamical
systems. Advances in Neural Information Processing Systems, 34:5531–5544, 2021.
[DGS91]
Philippe Delsarte, Jean-Marie Goethals, and Johan Jacob Seidel. Spherical codes and
designs. In Geometry and Combinatorics, pages 68–93. Elsevier, 1991.
[Dob79]
Roland
L’vovich
Dobrushin.
Vlasov
equations.
Funktsional’nyi
Analiz
i
ego
Prilozheniya, 13(2):48–58, 1979.
[Dud69]
R. M. Dudley. The Speed of Mean Glivenko-Cantelli Convergence. The Annals of
Mathematical Statistics, 40(1):40 – 50, 1969.
[DX13]
Feng Dai and Yuan Xu. Approximation theory and harmonic analysis on spheres and
balls. Springer, 2013.
[E17]
Weinan E. A proposal on machine learning via dynamical systems. Communications
in Mathematics and Statistics, 1(5):1–11, 2017.
[FGVG16]
Bastien Fernandez, David Gérard-Varet, and Giambattista Giacomin. Landau damp-
ing in the Kuramoto model. In Annales Henri Poincaré, volume 17, pages 1793–1823.
Springer, 2016.
[FHPS21]
Massimo Fornasier, Hui Huang, Lorenzo Pareschi, and Philippe Sünnen. Consensus-
based optimization on the sphere: Convergence to global minimizers and machine
learning. The Journal of Machine Learning Research, 22(1):10722–10776, 2021.
[FL19]
Amic Frouvelle and Jian-Guo Liu. Long-time dynamics for a simple aggregation
equation on the sphere. In Stochastic Dynamics Out of Equilibrium: Institut Henri
Poincaré, Paris, France, 2017, pages 457–479. Springer, 2019.
[GBC16]
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press,
Cambridge, MA, 2016.
[GBM21]
Arnaud Guillin, Pierre Le Bris, and Pierre Monmarché. Uniform in time propagation
of chaos for the 2d vortex model and other singular stochastic systems. arXiv preprint
arXiv:2108.08675, 2021.
[GLPR23]
Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. The emer-
gence of clusters in self-attention dynamics. arXiv preprint arXiv:2305.05465, 2023.
[Gol16]
François Golse. On the dynamics of large particle systems in the mean field limit.
Macroscopic and large scale phenomena: coarse graining, mean field limits and er-
godicity, pages 1–144, 2016.
[GZ22]
Borjan Geshkovski and Enrique Zuazua. Turnpike in optimal control of PDEs,
ResNets, and beyond. Acta Numerica, 31:135–263, 2022.
[HHL23]
Jiequn Han, Ruimeng Hu, and Jihao Long. A class of dimension-free metrics for
the convergence of empirical measures. Stochastic Processes and their Applications,
164:242–287, 2023.
[HK02]
Rainer Hegselmann and Ulrich Krause. Opinion dynamics and bounded confidence:
models, analysis and simulation. Journal of Artifical Societies and Social Simulation
(JASSS), 5(3), 2002.
[HKPZ16]
Seung-Yeal Ha, Dongnam Ko, Jinyeong Park, and Xiongtao Zhang. Collective syn-
chronization of classical and quantum oscillators. EMS Surveys in Mathematical Sci-
ences, 3(2):209–267, 2016.
[HKR18]
Seung-Yeal Ha, Dongnam Ko, and Sang Woo Ryoo. On the relaxation dynamics
of Lohe oscillators on some Riemannian manifolds. Journal of Statistical Physics,
172:1427–1478, 2018.
[HR17]
Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. In-
verse problems, 34(1), 2017.
[HR20]
Seung-Yeal Ha and Seung-Yeon Ryoo. Asymptotic phase-locking dynamics and crit-
ical coupling strength for the Kuramoto model. Communications in Mathematical
Physics, 377(2):811–857, 2020.
[HZRS16]
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep
residual networks. In Computer Vision–ECCV 2016:
14th European Conference,
Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages
630–645. Springer, 2016.

40
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
[JKO98]
Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of
the Fokker–Planck equation. SIAM Journal on Mathematical Analysis, 29(1):1–17,
1998.
[JLLW23]
Haotian Jiang, Qianxiao Li, Zhong Li, and Shida Wang. A brief survey on the ap-
proximation theory for sequence modelling. arXiv preprint arXiv:2302.13752, 2023.
[JM14]
Pierre-Emmanuel Jabin and Sebastien Motsch. Clustering and asymptotic behavior
in opinion formation. Journal of Differential Equations, 257(11):4165–4187, 2014.
[KO02]
Robert V Kohn and Felix Otto. Upper bounds on coarsening rates. Communications
in Mathematical Physics, 229(3):375–395, 2002.
[Kra00]
Ulrich Krause. A discrete nonlinear and non-autonomous model of consensus. In
Communications in Difference Equations: Proceedings of the Fourth International
Conference on Difference Equations, page 227. CRC Press, 2000.
[KSH12]
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with
deep convolutional neural networks. Advances in Neural Information Processing Sys-
tems, 25, 2012.
[Kur75]
Yoshiki Kuramoto. Self-entrainment of a population of coupled non-linear oscillators.
In International Symposium on Mathematical Problems in Theoretical Physics: Jan-
uary 23–29, 1975, Kyoto University, Kyoto/Japan, pages 420–422. Springer, 1975.
[Lac23]
Daniel Lacker. Hierarchies, entropy, and quantitative propagation of chaos for mean
field diffusions. Probability and Mathematical Physics, 4(2):377–432, 2023.
[LCG`20]
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
and Radu Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language
Representations. In International Conference on Learning Representations, 2020.
[LCT18]
Qianxiao Li, Long Chen, and Cheng Tai. Maximum principle based algorithms for
deep learning. Journal of Machine Learning Research, 18:1–29, 2018.
[LJ18]
Hongzhou Lin and Stefanie Jegelka. Resnet with one-neuron hidden layers is a uni-
versal approximator. Advances in Neural Information Processing Systems, 31, 2018.
[LLF23]
Daniel Lacker and Luc Le Flem. Sharp uniform-in-time propagation of chaos. Proba-
bility Theory and Related Fields, pages 1–38, 2023.
[LLH`20]
Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and
Tie-Yan Liu. Understanding and improving transformer from a multi-particle dynamic
system point of view. In International Conference on Learning Representations, 2020.
[LLS22]
Qianxiao Li, Ting Lin, and Zuowei Shen. Deep learning via dynamical systems: An ap-
proximation perspective. Journal of the European Mathematical Society, 25(5):1671–
1709, 2022.
[Loj63]
Stanislaw Lojasiewicz. Une propriété topologique des sous-ensembles analytiques
réels. Les équations aux dérivées partielles, 117:87–89, 1963.
[LWLQ22]
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transform-
ers. AI Open, 2022.
[LXB19]
Shuyang Ling, Ruitu Xu, and Afonso S Bandeira. On the landscape of synchronization
networks: A perspective from nonconvex optimization. SIAM Journal on Optimiza-
tion, 29(3):1879–1907, 2019.
[MMN18]
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the land-
scape of two-layer neural networks. Proceedings of the National Academy of Sciences,
115(33):E7665–E7671, 2018.
[MT14]
Sebastien Motsch and Eitan Tadmor. Heterophilious dynamics enhances consensus.
SIAM Review, 56(4):577–621, 2014.
[OR07]
Felix Otto and Maria G Reznikoff. Slow motion of gradient flows. Journal of Differ-
ential Equations, 237(2):372–420, 2007.
[Ott01]
Felix Otto. The geometry of dissipative evolution equations: the porous medium
equation. Communications in Partial Differential Equations, 26(1-2):101–174, 2001.
[PTTM17]
René Pinnau, Claudia Totzeck, Oliver Tse, and Stephan Martin. A consensus-based
model for global optimization and its mean-field limit. Mathematical Models and
Methods in Applied Sciences, 27(01):183–204, 2017.
[RBZ23]
Domenec Ruiz-Balet and Enrique Zuazua. Neural ODE control for classification, ap-
proximation, and transport. SIAM Review, 65(3):735–773, 2023.

A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS
41
[RS23]
Matthew Rosenzweig and Sylvia Serfaty. Global-in-time mean-field convergence for
singular Riesz-type diffusive flows. The Annals of Applied Probability, 33(2):954–998,
2023.
[RVE22]
Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural
networks: An interacting particle system approach. Communications on Pure and
Applied Mathematics, 75(9):1889–1935, 2022.
[SABP22]
Michael E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyré. Sinkformers:
Transformers with doubly stochastic attention. In International Conference on Arti-
ficial Intelligence and Statistics, pages 3515–3530. PMLR, 2022.
[Ser20]
Sylvia Serfaty. Mean field limit for Coulomb-type flows. Duke Mathematical Journal,
169(15), 2020.
[SFG`12]
Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf,
and Gert R. G. Lanckriet. On the empirical estimation of integral probability metrics.
Electronic Journal of Statistics, 6:1550 – 1599, 2012.
[Shu13]
Michael Shub. Global stability of dynamical systems. Springer Science & Business
Media, 2013.
[Str00]
Steven H Strogatz. From Kuramoto to Crawford: exploring the onset of synchroniza-
tion in populations of coupled oscillators. Physica D: Nonlinear Phenomena, 143(1-
4):1–20, 2000.
[Sze39]
Gabor Szegö. Orthogonal polynomials, volume 23. American Mathematical Soc., 1939.
[Tad23]
Eitan Tadmor. Swarming: hydrodynamic alignment with pressure. Bulletin of the
American Mathematical Society, 60(3):285–325, 2023.
[Tan17]
Yan Shuo Tan. Energy optimization for distributions on the sphere and improvement
to the Welch bounds. Electronic Communications in Probability, 22(none):1 – 12,
2017.
[Tay12]
Richard Taylor. There is no non-zero stable fixed point for dense networks in the
homogeneous Kuramoto model. Journal of Physics A: Mathematical and Theoretical,
45(5):055102, 2012.
[TG22]
Paulo Tabuada and Bahman Gharesifard. Universal approximation power of deep
residual neural networks through the lens of control. IEEE Transactions on Automatic
Control, 2022.
[TSS20]
Alex Townsend, Michael Stillman, and Steven H Strogatz. Dense networks that do
not synchronize and sparse ones that do. Chaos: An Interdisciplinary Journal of
Nonlinear Science, 30(8), 2020.
[VCBJ`95] Tamás Vicsek, András Czirók, Eshel Ben-Jacob, Inon Cohen, and Ofer Shochet. Novel
type of phase transition in a system of self-driven particles. Physical Review Letters,
75(6):1226, 1995.
[Ver18]
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications
in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cam-
bridge University Press, 2018.
[Vil01]
Cédric Villani. Limite de champ moyen. Cours de DEA, 2002:49, 2001.
[Vil09]
Cédric Villani. Optimal transport: old and new, volume 338. Springer, 2009.
[VR23]
Tanya Veeravalli and Maxim Raginsky. Nonlinear controllability and function repre-
sentation by neural stochastic differential equations. In Learning for Dynamics and
Control Conference, pages 838–850. PMLR, 2023.
[VSP`17]
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in
Neural Information Processing Systems, 30, 2017.
[Wen62]
James G Wendel. A problem in geometric probability. Mathematica Scandinavica,
11(1):109–111, 1962.
[XZ23]
Tong Xiao and Jingbo Zhu. Introduction to Transformers: an NLP Perspective. arXiv
preprint arXiv:2311.17633, 2023.
[YBR`19]
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv
Kumar. Are transformers universal approximators of sequence-to-sequence functions?
arXiv preprint arXiv:1912.10077, 2019.
[ZGUA20]
Han Zhang, Xi Gao, Jacob Unterman, and Tom Arodz. Approximation capabilities
of neural ODEs and invertible residual networks. In International Conference on
Machine Learning, pages 11086–11095. PMLR, 2020.

42
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Department of Mathematics, Massachusetts Institute of Technology, 77 Mas-
sachusetts Ave, 02139 Cambridge MA, USA
Email address: borjan@mit.edu
CNRS & Université Paris-Saclay, Laboratoire de mathématiques d’Orsay, 307 rue
Michel Magat, Bâtiment 307, 91400 Orsay, France
Email address: cyril.letrouit@universite-paris-saclay.fr
Department of EECS, Massachusetts Institute of Technology, 77 Massachusetts
Ave, 02139 Cambridge MA, USA
Email address: yp@mit.edu
Department of Mathematics, Massachusetts Institute of Technology, 77 Mas-
sachusetts Ave, 02139 Cambridge MA, USA
Email address: rigollet@math.mit.edu

