Bayesian Nonparametric Learning of Complex Dynamical
Phenomena
by
Emily B. Fox
S.B., Electrical Engineering, Massachusetts Institute of Technology, 2004
M.Eng., Elect. Eng. and Comp. Sci., Massachusetts Institute of Technology, 2005
E.E., Electrical Engineering, Massachusetts Institute of Technology, 2008
Submitted to the Department of Electrical Engineering and Computer Science
in partial fulﬁllment of the requirements for the degree of
Doctor of Philosophy
in Electrical Engineering and Computer Science
at the Massachusetts Institute of Technology
September 2009
c⃝2009 Massachusetts Institute of Technology
All Rights Reserved.
Signature of Author:
Department of Electrical Engineering and Computer Science
July 31, 2009
Certiﬁed by:
Alan S. Willsky
Edwin Sibley Webster Professor of Electrical Engineering and Computer Science
Thesis Co-Supervisor
Certiﬁed by:
John W. Fisher III
Principal Research Scientist
Thesis Co-Supervisor
Accepted by:
Terry P. Orlando
Professor of Electrical Engineering and Computer Science
Chair, Committee for Graduate Students

2

Bayesian Nonparametric Learning of Complex Dynamical
Phenomena
by Emily B. Fox
To be submitted to the Department of Electrical Engineering
and Computer Science
in Partial Fulﬁllment of the Requirements for the Degree of
Doctor of Philosophy in Electrical Engineering and Computer Science
Abstract
The complexity of many dynamical phenomena precludes the use of linear models
for which exact analytic techniques are available. However, inference on standard non-
linear models quickly becomes intractable. In some cases, Markov switching processes,
with switches between a set of simpler models, are employed to describe the observed
dynamics. Such models typically rely on pre-specifying the number of Markov modes.
In this thesis, we instead take a Bayesian nonparametric approach in deﬁning a prior on
the model parameters that allows for ﬂexibility in the complexity of the learned model
and for development of eﬃcient inference algorithms.
We start by considering dynamical phenomena that can be well-modeled as a hidden
discrete Markov process, but in which there is uncertainty about the cardinality of the
state space. The standard ﬁnite state hidden Markov model (HMM) has been widely
applied in speech recognition, digital communications, and bioinformatics, amongst
other ﬁelds.
Through the use of the hierarchical Dirichlet process (HDP), one can
examine an HMM with an unbounded number of possible states. We revisit this HDP-
HMM and develop a generalization of the model, the sticky HDP-HMM, that allows
more robust learning of smoothly varying state dynamics through a learned bias to-
wards self-transitions. We show that this sticky HDP-HMM not only better segments
data according to the underlying state sequence, but also improves the predictive per-
formance of the learned model. Additionally, the sticky HDP-HMM enables learning
more complex, multimodal emission distributions. We demonstrate the utility of the
sticky HDP-HMM on the NIST speaker diarization database, segmenting audio ﬁles
into speaker labels while simultaneously identifying the number of speakers present.
Although the HDP-HMM and its sticky extension are very ﬂexible time series mod-
els, they make a strong Markovian assumption that observations are conditionally inde-
pendent given the discrete HMM state. This assumption is often insuﬃcient for captur-
ing the temporal dependencies of the observations in real data. To address this issue,
we develop extensions of the sticky HDP-HMM for learning two classes of switching

4
dynamical processes: the switching linear dynamical system (SLDS) and the switching
vector autoregressive (SVAR) process. These conditionally linear dynamical models can
describe a wide range of complex dynamical phenomena from the stochastic volatility
of ﬁnancial time series to the dance of honey bees, two examples we use to show the
power and ﬂexibility of our Bayesian nonparametric approach. For all of the presented
models, we develop eﬃcient Gibbs sampling algorithms employing a truncated approx-
imation to the HDP that allows incorporation of dynamic programming techniques,
greatly improving mixing rates.
In many applications, one would like to discover and model dynamical behaviors
which are shared among several related time series. By jointly modeling such sequences,
we may more robustly estimate representative dynamic models, and also uncover in-
teresting relationships among activities. In the latter part of this thesis, we consider
a Bayesian nonparametric approach to this problem by harnessing the beta process to
allow each time series to have inﬁnitely many potential behaviors, while encouraging
sharing of behaviors amongst the time series. For this model, we develop an eﬃcient
and exact Markov chain Monte Carlo (MCMC) inference algorithm. In particular, we
exploit the ﬁnite dynamical system induced by a ﬁxed set of behaviors to eﬃciently
compute acceptance probabilities, and reversible jump birth and death proposals to
explore new behaviors. We present results on unsupervised segmentation of data from
the CMU motion capture database.
Thesis Supervisors:
Alan S. Willsky
Professor of Electrical Engineering and Computer Science
John W. Fisher III
Principal Research Scientist

Acknowledgments
Everything should be made as simple as possible, but not simpler.
attributed to Albert Einstein
Aerodynamically the bumblebee shouldn’t be able to ﬂy,
but the bumblebee doesn’t know that so it goes on ﬂying anyway.
Mary Kay Ash
This thesis marks the culmination of an intense though incredibly gratifying journey at
MIT that started nearly a decade ago. I look fondly upon my years as an undergraduate
student at the Institution, but it was my time as a graduate student that was the most
formative and rewarding. Academically, this is in large part due to the interactions
I had with my advisor, Professor Alan Willsky. Alan’s incredible breadth and depth
of knowledge have been an inspiration to me and of great importance in shaping the
research contained in this thesis. No matter how many times I ventured away from
the group’s core areas, Alan was always right there still actively (and energetically!)
following and providing context for the ideas. My co-advisor, Dr. John Fisher, has
provided, in addition to many good laughs and distractions from work, illumination
into my research through many insightful questions; he was also readily available to
answer all of my many questions.
In addition to my interactions with Alan and John, the other students in the
Stochastic Systems Group (SSG), both past and present, have played a pivotal role
in my graduate studies. My long-time oﬃcemates—Kush Varshney, Pat Kreidl, and
Jason Williams—provided stimulating conversations and tolerated my incessant inter-
ruptions. My new oﬃcemate, Matt Johnson, has quickly ﬁlled those shoes since Pat
and Jason graduated. We have had many interesting discussions on Bayesian statistics
and I look forward to continued collaborations. I also want to thank Myung Jin Choi,
Venkat Chandrasekaran, Vincent Tan, and Ying Liu for enlivening SSG with Friday
poker night and other group events. Along those lines, I thank members of CSAIL,
such as Mike Siracusa, Gerald Dalley, Wanmei Ou, and Thomas Yeo, for sharing in
“vulturing” trips and our ensuing lunch conversations. I am particularly indebted to
Mike Siracusa who, in addition to many illuminating discussions on Markov switching
processes, went above and beyond in helping me with computing issues. We made a
fabulous, and seemingly automatic, grouplet and SSG seminar pairing.
I must highlight Erik Sudderth, a former SSG member, for the exceptional guidance
and mentoring he has provided me during my graduate studies. I can attribute my
5

6
ACKNOWLEDGMENTS
exposure to Bayesian nonparametrics to Erik, who examined such methods during the
latter part of his thesis. Although our collaborations started only after he left MIT, Erik
has contributed signiﬁcantly to the work presented herein. He has taught me a great
deal about persistence (no pun intended regarding Chapter 3) and thorough analysis
of results. My thesis committee, Professor Munzer Dahleh and Princeton University’s
Professor David Blei, also provided thoughtful suggestions that continue to guide my
research and help make the work understandable to both the System Identiﬁcation and
Machine Learning communities. In addition to Alan’s courses on recursive estimation
and stochastic systems, Munzer’s exceptional instruction of the course on dynamic
systems and control was pivotal in building my foundation for studying the dynamical
models presented in this thesis.
I have also had the honor of working with Professor Michael Jordan at UC Berkeley.
During my many visits to Berkeley, and through a massive number of emails, this bi-
coastal collaboration has provided me with invaluable guidance on my work in Bayesian
nonparametrics and insight into the ﬁelds of Machine Learning and Statistics. Addi-
tionally, I am deeply indebted to Mike for his extensive editing of papers that comprise
a good portion of this thesis. Another contributing factor outside of MIT’s campus was
my time interning at MIT Lincoln Laboratory, speciﬁcally working on target tracking,
that set me on this hunt for ﬂexible descriptions of Markov switching models. Without
the inspiration of that application, and discussions with Keh Ping Duhn, David Choi,
and Daniel Rudoy, I likely would not have taken the path I did.
On a more personal note, I would like to thank my family for their support during my
nine-year adventure 3,000 miles away from home. I would especially like to acknowledge
my mom who has always supported my pursuits, however oﬀbeat and incomprehensible
they were to her (e.g., ice hockey, pole vaulting, and needless to say, anything having
to do with math.)
My stepdad, who has a Ph.D. in chemistry, has been a refuge
at home in understanding logical reasoning while my dad has taught me the value of
adventure and optimism. My siblings, Ben and Nathan, have each been there for me in
incredible ways. I also must thank all of my friends, especially Melanie Rudoy1 and Erin
Aylward, for their never-ending support, encouragement, and distractions. Finally, this
endeavor would have been inﬁnitely more challenging without the love and support of
Wes McKinney2.
1Yes, there are two Rudoys in one acknowledgments section.
2For him, I must thank Jim Munkres who taught the topology course in which we met.

Contents
Abstract
3
Acknowledgments
5
List of Figures
11
List of Algorithms
15
List of Tables
17
Notational Conventions
19
1
Introduction
23
1.1
Thesis Organization and Overview of Methods and Contributions . . . .
26
1.1.1
Chapter 2: Background
. . . . . . . . . . . . . . . . . . . . . . .
27
1.1.2
Chapter 3: The Sticky HDP-HMM . . . . . . . . . . . . . . . . .
27
1.1.3
Chapter 4: Bayesian Nonparametric Learning of SLDS . . . . . .
28
1.1.4
Chapter 5:
Sharing Features among Dynamical Systems with
Beta Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
1.1.5
Chapter 6: Contributions and Recommendations . . . . . . . . .
31
1.1.6
Appendices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
2
Background
33
2.1
The Bayesian Framework
. . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.1.1
Modeling via Exchangeability . . . . . . . . . . . . . . . . . . . .
34
2.2
Exponential Families . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.2.1
Properties of the Canonical Exponential Family . . . . . . . . . .
42
2.2.2
Interpretation as Linearly Constrained Maximum Entropy Dis-
tribution
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
2.2.3
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
2.3
Suﬃcient Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.4
Incorporating Prior Knowledge . . . . . . . . . . . . . . . . . . . . . . .
49
7

8
CONTENTS
2.4.1
Conjugate Priors . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
2.4.2
Multinomial Observations . . . . . . . . . . . . . . . . . . . . . .
52
2.4.3
Gaussian Observations . . . . . . . . . . . . . . . . . . . . . . . .
53
2.4.4
Multivariate Linear Regression Model
. . . . . . . . . . . . . . .
55
2.5
Graphical Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.5.1
A Brief Overview . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.5.2
Directed Graphical Models
. . . . . . . . . . . . . . . . . . . . .
58
2.5.3
Undirected Graphical Models . . . . . . . . . . . . . . . . . . . .
60
2.5.4
Belief Propagation . . . . . . . . . . . . . . . . . . . . . . . . . .
62
2.6
Hidden Markov Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
2.6.1
Forward-Backward Algorithm . . . . . . . . . . . . . . . . . . . .
67
2.6.2
Viterbi Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . .
69
2.7
State Space Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
2.7.1
Standard Discrete-Time Linear-Gaussian State Space Formulation
71
2.7.2
Vector Autoregressive Processes . . . . . . . . . . . . . . . . . . .
72
2.7.3
Switching Linear Dynamic Systems . . . . . . . . . . . . . . . . .
72
2.7.4
Stochastic Realization Theory . . . . . . . . . . . . . . . . . . . .
73
2.7.5
Kalman Filtering and Smoothing . . . . . . . . . . . . . . . . . .
76
2.8
Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . .
80
2.8.1
Monte Carlo Integration . . . . . . . . . . . . . . . . . . . . . . .
80
2.8.2
The Metropolis-Hastings Algorithm
. . . . . . . . . . . . . . . .
81
2.8.3
Gibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
2.8.4
Auxiliary, Blocked, and Collapsed Gibbs Samplers . . . . . . . .
86
2.9
Bayesian Nonparametric Methods . . . . . . . . . . . . . . . . . . . . . .
91
2.9.1
Dirichlet Processes . . . . . . . . . . . . . . . . . . . . . . . . . .
92
2.9.2
Dirichlet Process Mixture Models . . . . . . . . . . . . . . . . . .
96
2.9.3
Hierarchical Dirichlet Processes . . . . . . . . . . . . . . . . . . .
98
2.9.4
Beta Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3
The Sticky HDP-HMM
107
3.1
The HDP-HMM and Its Sticky Extension . . . . . . . . . . . . . . . . . 109
3.1.1
Chinese Restaurant Franchise with Loyal Customers . . . . . . . 111
3.1.2
Sampling via Direct Assignments . . . . . . . . . . . . . . . . . . 114
3.1.3
Blocked Sampling of State Sequences . . . . . . . . . . . . . . . . 115
3.1.4
Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
3.2
Experiments with Synthetic Data . . . . . . . . . . . . . . . . . . . . . . 117
3.2.1
Gaussian Emissions
. . . . . . . . . . . . . . . . . . . . . . . . . 119
3.2.2
Multinomial Emissions . . . . . . . . . . . . . . . . . . . . . . . . 124
3.2.3
Comparison to Independent Sparse Dirichlet Prior . . . . . . . . 125
3.3
Multimodal Emission Densities . . . . . . . . . . . . . . . . . . . . . . . 126
3.3.1
Direct Assignment Sampler . . . . . . . . . . . . . . . . . . . . . 127
3.3.2
Blocked Sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . 128

CONTENTS
9
3.4
Assessing the Multimodal Emissions Model
. . . . . . . . . . . . . . . . 128
3.4.1
Mixture of Gaussian Emissions . . . . . . . . . . . . . . . . . . . 128
3.5
Speaker Diarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
3.6
Discussion and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . 137
4
Bayesian Nonparametric Learning of SLDS
141
4.1
The HDP-SLDS and HDP-AR-HMM Models
. . . . . . . . . . . . . . . 143
4.1.1
Posterior Inference of Dynamic Parameters
. . . . . . . . . . . . 145
Conjugate Prior on {A(k), Σ(k)} . . . . . . . . . . . . . . . . . . . 147
Alternative Prior — Automatic Relevance Determination
. . . . 147
Measurement Noise Posterior . . . . . . . . . . . . . . . . . . . . 152
4.1.2
Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
Sampling Dynamic Parameters {A(k), Σ(k)} . . . . . . . . . . . . 153
Sampling Measurement Noise R (HDP-SLDS only) . . . . . . . . 154
Block Sampling z1:T
. . . . . . . . . . . . . . . . . . . . . . . . . 154
Block Sampling x1:T (HDP-SLDS only) . . . . . . . . . . . . . . 154
Sequentially Sampling z1:T (HDP-SLDS only) . . . . . . . . . . . 155
4.2
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
4.2.1
MNIW prior
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
4.2.2
ARD prior
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
4.2.3
Dancing Honey Bees . . . . . . . . . . . . . . . . . . . . . . . . . 163
4.3
Model Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
4.3.1
Shared Dynamic Matrix, Switching Driving Noise . . . . . . . . . 169
4.3.2
Fixed Dynamic Matrix, Switching Driving Noise
. . . . . . . . . 174
4.4
Discussion and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . 181
5
Sharing Features among Dynamical Systems with Beta Processes
183
5.1
Describing Multiple Time Series with Beta Processes . . . . . . . . . . . 184
5.2
MCMC Methods for Posterior Inference . . . . . . . . . . . . . . . . . . 186
5.2.1
Sampling binary feature assignments . . . . . . . . . . . . . . . . 187
5.2.2
Sampling dynamic parameters and transition variables . . . . . . 191
5.2.3
Sampling the IBP and Dirichlet transition hyperparameters . . . 192
5.3
Synthetic Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
5.4
Motion Capture Experiments . . . . . . . . . . . . . . . . . . . . . . . . 198
5.5
Discussion and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . 203
6
Contributions and Recommendations
205
6.1
Summary of Methods and Contributions . . . . . . . . . . . . . . . . . . 205
6.2
Suggestions for Future Research . . . . . . . . . . . . . . . . . . . . . . . 207
6.2.1
Inference on Large-Scale Data . . . . . . . . . . . . . . . . . . . . 207
6.2.2
Alternative Dynamic Structures
. . . . . . . . . . . . . . . . . . 208
6.2.3
Bayesian Nonparametric Variable-Order Markov Models . . . . . 209
6.2.4
Alternatives to Global Clustering . . . . . . . . . . . . . . . . . . 210

10
CONTENTS
6.2.5
Asymptotic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 210
A Sticky HDP-HMM Direct Assignment Sampler
213
A.1 Sticky HDP-HMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
A.1.1
Sampling zt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
A.1.2
Sampling β . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
A.1.3
Jointly Sampling mjk, wjt, and ¯mjk
. . . . . . . . . . . . . . . . 218
A.2 Sticky HDP-HMM with DP emissions
. . . . . . . . . . . . . . . . . . . 220
B Sticky HDP-HMM Blocked Sampler
223
B.1
Sampling β, π, and ψ
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
B.2
Sampling z1:T for the Sticky HDP-HMM . . . . . . . . . . . . . . . . . . 224
B.3
Sampling (z1:T , s1:T ) for the Sticky HDP-HMM with DP emissions . . . 224
B.4
Sampling θ
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
B.4.1
Non-Conjugate Base Measures
. . . . . . . . . . . . . . . . . . . 225
C Hyperparameters
227
C.1 Posterior of (α + κ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
C.2 Posterior of γ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
C.3 Posterior of σ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
C.4 Posterior of ρ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
D HDP-SLDS and HDP-AR-HMM Message Passing
233
D.1 Mode Sequence Message Passing for Blocked Sampling . . . . . . . . . . 233
D.2 State Sequence Message Passing for Blocked Sampling . . . . . . . . . . 234
D.3 Mode Sequence Message Passing for Sequential Sampling
. . . . . . . . 237
E Derivation of Maneuvering Target Tracking Sampler
243
E.1
Chinese Restaurant Franchise . . . . . . . . . . . . . . . . . . . . . . . . 244
E.2
Normal-Inverse-Wishart Posterior Update . . . . . . . . . . . . . . . . . 244
E.3
Marginalization by Message Passing
. . . . . . . . . . . . . . . . . . . . 245
E.4
Combining Messages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
E.5
Joining Distributions that Depend on ut . . . . . . . . . . . . . . . . . . 248
E.6
Resulting (ut, zt) Sampling Distributions . . . . . . . . . . . . . . . . . . 248
F Dynamic Parameter Posteriors
251
F.1
Conjugate Prior — MNIW
. . . . . . . . . . . . . . . . . . . . . . . . . 251
F.2
Non-Conjugate Independent Priors on A(k), Σ(k), and µ(k) . . . . . . . . 254
F.2.1
Normal Prior on A(k)
. . . . . . . . . . . . . . . . . . . . . . . . 254
F.2.2
Inverse Wishart Prior on Σ(k) . . . . . . . . . . . . . . . . . . . . 255
F.2.3
Normal Prior on µ(k) . . . . . . . . . . . . . . . . . . . . . . . . . 255
Bibliography
257

List of Figures
1.1
Examples of data we examine in the thesis.
. . . . . . . . . . . . . . . .
25
1.2
Motion capture skeleton plots for six examples of jumping jacks.
. . . .
26
2.1
Histograms of inferred parameters from coin ﬂipping and P´olya urn ex-
periments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.2
Bayes ball algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
2.3
Graphical representation of Markov blanket. . . . . . . . . . . . . . . . .
60
2.4
Hierarchical Bayesian model of exchangeable random variables. . . . . .
60
2.5
Moralization of two directed graphical models.
. . . . . . . . . . . . . .
61
2.6
Example tree graphical models. . . . . . . . . . . . . . . . . . . . . . . .
63
2.7
Graphical representation of two belief propagation scheduling schemes. .
65
2.8
Graphical representation of a hidden Markov model (HMM).
. . . . . .
66
2.9
Lattice representation of an HMM state sequence.
. . . . . . . . . . . .
67
2.10 Graphical models for the switching vector autoregressive (VAR) process
and switching linear dynamical system (SLDS). . . . . . . . . . . . . . .
74
2.11 Graphical model of a ﬁnite mixture model.
. . . . . . . . . . . . . . . .
88
2.12 Dirichlet process mixture model graphs. . . . . . . . . . . . . . . . . . .
97
2.13 Graphical model of Chinese restaurant franchise. . . . . . . . . . . . . . 100
2.14 Depiction of a Chinese restaurant franchise with two restaurants. . . . . 101
2.15 A draw from a beta process, and associated Bernoulli realizations, along
with a realization from the Indian buﬀet process. . . . . . . . . . . . . . 103
3.1
Demonstration of rapid transitions in HDP-HMM state sequences.
. . . 108
3.2
Sticky HDP-HMM graphical models. . . . . . . . . . . . . . . . . . . . . 110
3.3
Graphical model of Chinese restaurant franchise with loyal customers. . 112
3.4
Illustration of dish-choosing process for the Chinese restaurant with loyal
customers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.5
Demonstration of sequential HDP-HMM Gibbs sampler splitting tempo-
rally separated examples of the same state.
. . . . . . . . . . . . . . . . 115
3.6
Synthetic three-state HMM observation sequence and resulting sticky vs.
non-sticky HDP-HMM performance. . . . . . . . . . . . . . . . . . . . . 119
11

12
LIST OF FIGURES
3.7
Comparison of perfromance of the blocked and sequential HDP-HMM
Gibbs samplers on the three-state HMM synthetic observation sequence. 120
3.8
Performance of beam sampling on the three-state HMM synthetic data
example. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
3.9
Fast state-switching synthetic data along with sticky vs.
non-sticky
HDP-HMM segmentation performance.
. . . . . . . . . . . . . . . . . . 123
3.10 Multinomial synthetic data along with sticky vs. non-sticky HDP-HMM
segmentation results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
3.11 State transition diagram for a nine-state HMM. . . . . . . . . . . . . . . 126
3.12 Sticky HDP-HMM results for the nine-state HMM example, as compared
to a model with an independent sparse Dirichlet prior. . . . . . . . . . . 127
3.13 Sticky vs. non-sticky HDP-HMM performance on data generated from a
ﬁve-state HMM with mixture of Gaussian emissions. . . . . . . . . . . . 131
3.14 Block diagram of preprocessing of speaker diaraization data. . . . . . . . 133
3.15 For each of the 21 meetings, comparison of diarizations using sticky vs.
original HDP-HMM with DP emissions. . . . . . . . . . . . . . . . . . . 134
3.16 Example diarization for the NIST 20051102-1323 meeting. . . . . . . . . 135
3.17 Example diarization for the VT 20050304-1300 meeting. . . . . . . . . . 137
3.18 Chart comparing the DERs of the sticky and original HDP-HMM with
DP emissions to those of ICSI for each of the 21 meetings. . . . . . . . . 138
3.19 Trace plots of log-likelihood and Hamming distance error for 10 chains
over 100,000 Gibbs iterations for the NIST 20051102-1323 meeting. . . . 139
4.1
Graphical models of the HDP-SLDS and an order two HDP-AR-HMM.
144
4.2
Block diagram of one iteration of the Gibbs sampler for the HDP-SLDS
and HDP-AR-HMM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
4.3
Depiction of HDP-SLDS sampling stages on a set of graphical models. . 153
4.4
Depiction of HDP-AR-HMM sampling stages on a set of graphical models154
4.5
Plots of three synthetic data sequences generated from switching linear
dynamical processes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
4.6
Synthetic data results for the three sequences using the HDP-SLDS,
HDP-AR-HMM, and HDP-HMM.
. . . . . . . . . . . . . . . . . . . . . 161
4.7
Synthetic data generated from an SLDS with a sparse dynamical matrix,
and results comparing the HDP-SLDS with an ARD vs. MNIW prior. . 163
4.8
Trajectories of six honey bees dance sequences. . . . . . . . . . . . . . . 164
4.9
Change-point detection performance of the HDP-AR-HMM on the six
honey bee dance sequences as compared to the method of Xuan and
Murphy [188]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
4.10 Segmentation performance of the HDP-AR-HMM on the six honey bee
dance sequences.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
4.11 Honey bee head angle measurements for the six dances.
. . . . . . . . . 167
4.12 Inferred ARD hyperparameters for the learned honey bee dance modes.
168

LIST OF FIGURES
13
4.13 IBOVESPA stock index daily returns from 01/03/1997 to 01/16/2001. . 170
4.14 IBOVESPA stock index change-point detection performance for two vari-
ants of the HDP-SLDS.
. . . . . . . . . . . . . . . . . . . . . . . . . . . 173
4.15 HDP-SLDS maneuvering target tracking results for synthetic data of a
target following a sinusoidal trajectory. . . . . . . . . . . . . . . . . . . . 180
4.16 HDP-SLDS maneuvering target tracking results for synthetic data of a
target controlled by a step function input on acceleration. . . . . . . . . 181
5.1
Graphical model of the IBP-AR-HMM.
. . . . . . . . . . . . . . . . . . 186
5.2
Synthetic data for 5 switching AR(1) time series, and associated true
and IBP-AR-HMM learned feature matrices.
. . . . . . . . . . . . . . . 197
5.3
Hamming distance quantiles comparing the segmentation performance of
the HDP-AR-HMM to the IBP-AR-HMM on a synthetic data example.
199
5.4
Motion capture skeleton plots for IBP-AR-HMM learned segmentations
of six exercise routine videos. . . . . . . . . . . . . . . . . . . . . . . . . 200
5.5
Comparison of the IBP-AR-HMM MoCap segmentation performance to
HMM and Gaussian mixture model approaches. . . . . . . . . . . . . . . 201
5.6
Learned MoCap feature matrices from the IBP-AR-HMM, HMM, and
Gaussian mixture model approaches. . . . . . . . . . . . . . . . . . . . . 202

14
LIST OF FIGURES

List of Algorithms
1
Viterbi hidden Markov model decoding. . . . . . . . . . . . . . . . . . .
70
2
Kalman ﬁlter recursion for an LTI system. . . . . . . . . . . . . . . . . .
76
3
Stable forward information form Kalman ﬁlter recursion. . . . . . . . . .
79
4
Metropolis-Hastings algorithm. . . . . . . . . . . . . . . . . . . . . . . .
81
5
Multi-stage Gibbs sampling algorithm. . . . . . . . . . . . . . . . . . . .
84
6
Two-stage Gibbs sampling algorithm.
. . . . . . . . . . . . . . . . . . .
86
7
Completion Gibbs sampler for a ﬁnite mixture model.
. . . . . . . . . .
90
8
Collapsed Gibbs sampler for a ﬁnite mixture model.
. . . . . . . . . . .
91
9
Direct assignment collapsed Gibbs sampler for the sticky HDP-HMM.
. 116
10
Blocked Gibbs sampler for the sticky HDP-HMM.
. . . . . . . . . . . . 118
11
Direct assignment collapsed Gibbs sampler for the sticky HDP-HMM
with DP emissions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
12
Blocked Gibbs sampler for the sticky HDP-HMM with DP emissions. . . 130
13
HDP-SLDS and HDP-AR-HMM Gibbs sampler.
. . . . . . . . . . . . . 157
14
Blocked mode-sequence sampler for HDP-AR-HMM or HDP-SLDS.
. . 158
15
Parameter sampling using MNIW prior. . . . . . . . . . . . . . . . . . . 158
16
Parameter sampling using ARD prior. . . . . . . . . . . . . . . . . . . . 159
17
IBP-AR-HMM MCMC sampler.
. . . . . . . . . . . . . . . . . . . . . . 195
18
IBP-AR-HMM auxiliary variable sampler for updating transition and
dynamic parameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
19
Numerically stable form of the backwards Kalman information ﬁlter. . . 237
20
Numerically stable form of the forward Kalman information ﬁlter.
. . . 239
15

16
LIST OF ALGORITHMS

List of Tables
3.1
Overall DERs for the sticky and original HDP-HMM with DP emissions. 136
4.1
Median label accuracy of the HDP-AR-HMM compared to accuracy of
the approach of Oh et al. [129]. . . . . . . . . . . . . . . . . . . . . . . . 167
4.2
Table of 10 key world events aﬀecting the IBOVESPA stock index from
01/03/1997 to 01/16/2001.
. . . . . . . . . . . . . . . . . . . . . . . . . 171
4.3
Summary of two variants of the HDP-SLDS for detecting changes in
volatility of a stock index. . . . . . . . . . . . . . . . . . . . . . . . . . . 172
17

18
LIST OF TABLES

Notational Conventions
Symbol
Deﬁnition
General Notation
Z+
the set of positive integers
R
the set of reals
x1:t
the sequence {x1, . . . , xt}
x\t
the sequence {x1, . . . , xt−1, xt+1, . . . , xT }, where T is largest
possible index
x·b
P
a xab
xa·
P
b xab
x··
P
b
P
a xab
| · |
cardinality of a set
δ(k, j)
the discrete Kronecker delta
δθ
measure concentrated at θ
E[·]
expectation of a random variable
DP(α, H)
Dirichlet process distribution with concentration parameter
α and base measure H
Dir(α1, . . . , αK)
K-dimensional ﬁnite Dirichlet distribution with parameters
α1, . . . , αK
Ber(p)
Bernoulli distribution with parameter p
GEM(γ)
stick-breaking distribution with parameter γ
19

20
NOTATIONAL CONVENTIONS
Symbol
Deﬁnition
Hierarchical Dirichlet Process and
Chinese Restaurant Franchise with Loyal Customers
yji
ith observation within jth group
zji
index of mixture component that generated observation yji
θji
(non-unique) parameter associated with observation yji
θ∗
jt
(non-unique) parameter, or dish, served at table t in restau-
rant j
θ∗∗
k
kth unique global parameter of the mixture model
tji
table assignment for observation, or customer, yji
¯kjt
considered dish assignment for table t in restaurant j
kjt
served dish assignment for table t in restaurant j
¯kj
the set of all considered dish assignments in restaurant j
kj
the set of all served dish assignments in restaurant j
wjt
override variable for table t in restaurant j
˜njt
number of customers at table t in restaurant j
¯mjk
number of tables in restaurant j that considered dish k
mjk
number of tables in restaurant j that were served dish k
Tj
number of currently occupied tables in restaurant j
¯K
number of unique dishes considered in the franchise
K
number of unique dishes served in the franchise
Sticky HDP-HMM
yt
observation from the hidden Markov model at time t
zt
state of the Markov chain at time t
njk
number of transitions from state j to state k in z1:T
n−t
jk
number of transitions from state j to state k in z1:T , not
counting the transitions zt−1 →zt or zt →zt+1
κ
self-transition parameter
ρ
self-transition proportion parameter κ/(α + κ)
with DP emissions
st
index of mixture component that generated observation yt
n′
jk
number of transitions from state j to state k in z1:T
n−t
jk
number of transitions from state j to state k in z1:T , not
counting the transitions zt−1 →zt or zt →zt+1
K′
j
number of currently instantiated mixture components for
state j’s emission distribution

NOTATIONAL CONVENTIONS
21
Symbol
Deﬁnition
HDP-SLDS and HDP-AR-HMM
VAR(r)
order r vector autoregressive process
SLDS
switching linear dynamical system
A(k)
i
ith lag matrix of the kth VAR process
A(k)
dynamic matrix for kth dynamical mode
For HDP-AR-HMM, contains lag matrices A(k)
i
Σ(k)
process noise covariance of kth dynamical mode
C
measurement matrix
R
measurement noise covariance
zt
dynamical mode index at time t
xt
continuous-valued state vector at time t
yt
observation vector at time t
ψt
pseudo-observation vector at time t
¯ψt
lag pseudo-observation vector at time t
d
dimension of the observations yt
n
dimension of the latent state xt
Sℓ
set of indices for which elements a(k)
ij
of A(k) are distributed
with ARD parameter α(k)
ℓ

22
NOTATIONAL CONVENTIONS
Symbol
Deﬁnition
IBP-AR-HMM
z(i)
t
dynamical mode index for object i at time t
y(i)
t
observation vector for object i at time t
fi
feature vector for object i containing elements fik
η(i)
jk
transition variables for object i
Ak
dynamic matrix for kth dynamical mode
Σk
process noise covariance of kth dynamical mode
π(i)
j
jth feature-constrained transition distribution for object i
Normalizes η(i)
jk over indices determined by fi
K+
total number of instantiated dynamical modes
K−i
+
number of instantiated dynamical modes not considering
those used by object i
f−i
feature vector for object i containing only the components of
fi shared by other objects
f+i
feature vector for object i containing the feature indices of fi
unique to object i
θ+
dynamic parameters θk = {Ak, Σk} for features unique to
object i
η+
transition variables η(i)
jk associated with features unique to
object i
ni
number of features unique to object i

Chapter 1
Introduction
T
HE study of dynamical phenomena is pervasive in ﬁelds as diverse as bioinformat-
ics, econometrics, and systems and control. For example, within bioinformatics one
might be interested in modeling recombination hotspots and ancestral haplotypes. In
econometrics, classical time series include daily returns of a stock index, the exchange
rate of a currency, or interest rate. Systems and controls applications are plentiful,
ranging from robotics to modeling the dynamics of aircraft. Within these ﬁelds, there
has been an explosion of data of increasingly complex phenomena, resulting in a push
toward building more intricate time series models and developing eﬃcient inference
techniques. The challenges these datasets pose result from a convergence of factors:
the size of the datasets demand examination of time series analysis techniques that
scale eﬀectively with the dimensionality of the data while the complexity of the dy-
namics precludes the use of standard linear dynamical models for which exact inference
techniques exist.
A small subset of time series data, such as the trajectory of a ballistic missile, can
be described by a single dynamical model that is well-deﬁned through knowledge of
the underlying physics of the object we are observing. Slightly more complicated time
series, like a maneuvering passenger aircraft, can be described as switching between a
small set of dynamical models. However, many of the dynamical processes we encounter
are too complex for such modeling schemes. For example, describing human motion
requires the formulation of a model that represents the large number of degrees of
freedom provided by the many human joints. High performance aircraft or the dance
of honey bees [129] are other examples of dynamical systems with patterned, but very
intricate motions. In this thesis, we consider methods for learning dynamical models for
time series with complex and uncertain behavior patterns. Speciﬁcally, we address how
Bayesian nonparametric methods can be used to provide a ﬂexible and computationally
eﬃcient structure for learning and inference of these complex systems.
Although the true underlying dynamics of the phenomena of interest are generally
nonlinear, they can often be eﬀectively modeled as switches among a set of conditionally
linear dynamical modes. These switching linear dynamical systems (SLDS) have been
used to describe, for example, human motion [133, 140], ﬁnancial time series [27, 94, 154],
and maneuvering targets [43, 145]. Within the control community, these models are of-
ten referred to as Markov jump-linear systems (MJLS). The diﬀerent linear dynamical
23

24
CHAPTER 1.
INTRODUCTION
modes account for changes the phenomena exhibit: a person changes from walking to
running; a country undergoes a recession, a central bank intervention, or some national
or global event; an aircraft makes an evasive maneuver. Classical methods for infer-
ring the latent state of the switching dynamical process rely on deﬁning a ﬁxed, ﬁnite
set of models with known parameterizations and switching behaviors. In the case of
identifying switching dynamical processes, the ﬁeld consists of only a ﬁxed number of
directions: either relying on knowledge of the number of dynamical regimes and es-
timating the model parameters from the data, or relying on simplifying assumptions
such as deterministic dynamics when the number of models is not known. Further de-
tails are discussed in Chapter 4. Alternatively, emerging methods within the ﬁeld of
Bayesian nonparametrics, speciﬁcally hierarchical extensions of the Dirichlet process,
oﬀer promise in learning stochastic switching dynamical models with the ﬂexibility of
incorporating new dynamical modes as new behaviors are observed. Furthermore, by
casting the problem of system identiﬁcation within the Bayesian framework, one can
leverage the extensive theory and methodologies of this ﬁeld.
The clustering properties induced by the Dirichlet process prior have been exploited
in many standard mixture modeling applications. Hierarchical layerings of Dirichlet pro-
cesses, such as the hierarchical Dirichlet process (HDP) [162] and the nested Dirichlet
process [143], as well as generalizations of the Dirichlet process, such as the Pitman-Yor
process [72, 137], have proven useful in a variety of ﬁelds including genomics [187], doc-
ument modeling [19], natural language processing [58, 160], and computer vision [158].
Originally developed for static estimation problems, a burgeoning trend is realizing the
signiﬁcant impact these methods can have on time series analysis, an impact which cuts
through the boundaries between machine learning, statistics, and dynamics and control.
One perspective of this analysis has been the development of Dirichlet process priors on
stochastically evolving distributions such as the dependent Dirichlet process [61, 111]
and the kernel stick-breaking process [39]. For example, imagine one has recordings
of a unknown collection of neurons. Due to either changing recording conditions or
changes within the neuron itself, the waveforms observed may vary with time. In such
cases, one would like to allow the model parameters to stochastically evolve [48]. Other
uses of these processes include the study of how a response density changes with pre-
dictors [39], or time-varying document topic modeling [156] in which the popularity of
various topics within a given domain evolve with time.
The complex time series we analyze in this thesis, however, have more patterned
behaviors that we would like to capture through models that allow repeated returns to
a set of simpler dynamical models. In such cases, instead of examining stochastically
evolving distributions as in the dependent Dirichlet process, we would like to nonpara-
metrically model the stationary transition distributions of a discrete-time Markov pro-
cess. That is, we would like to allow for switching processes with an unbounded number
of possible Markov states. A ﬁrst attempt at such a model is the hierarchical Dirich-
let process hidden Markov model (HDP-HMM) [11, 162]. One of our contributions in
this thesis—the sticky HDP-HMM—provides improved control over the number of hid-

25
John
Jane
Bob
John
B
o
b
J
i
l
l
John
Jane
Bob
John
B
o
b
J
i
l
l
John
Jane
Bob
John
B
o
b
J
i
l
l
(a)
(b)
Figure 1.1. Two examples of data we examine in the thesis. (a) A speech signal from which we aim
to infer the number of speakers and a segmentation of the audio into speaker labels; (b) A honey bee in
the beehive, performing a set of three dances indicated by the arrows: turn right, turn left, and waggle.
In this scenario, our goal is to discover these dances and to estimate dynamical models to describe
them.
den Markov model modes inferred by better capturing the temporal mode persistence
present in many real datasets. As a motivating example for the sticky HDP-HMM,
consider the problem of speaker diarization [185], to which we return in Chapter 3.
Here, an audio recording is made of a meeting involving multiple human participants
and the problem is to segment the recording into time intervals associated with indi-
vidual speakers. See Fig. 1.1(a). Segmentation is to be accomplished without a priori
knowledge of the number of speakers involved in the meeting; moreover, one does not
assume a priori knowledge of the speech patterns of particular individuals. For this
application, we show that producing state-of-the-art diarizations using the HDP-HMM
requires the sticky extension to properly account for the fact that a person currently
speaking is likely to continue speaking.
Both the HDP-HMM and its sticky extension make a strong Markovian assumption
that observations are conditionally independent given the mode. Such an assumption is
inappropriate for many of the datasets we examine. For example, consider the problem
of segmenting the dance of a honey bee into the turn right, turn left, and waggle dances
depicted in Fig. 1.1(b) [129]. (See Chapter 4 for explanation.) In such a scenario, even
conditioned on the dance mode, the observations of the honey bee position are highly
correlated and thus the overall dance cannot be well approximated by a hidden Markov
model. Motivated by such applications, in this thesis we also examine a Bayesian non-
parametric approach for learning SLDS, thereby capturing a broader class of dynamical
phenomena exhibiting more complex temporal dependencies.
While the Dirichlet process targets inferring a small set of representative dynamical
modes, there is still a question about the dimensionality of the parametrization for
the conditionally linear dynamical models. In the presence of limited data, one would
like to reduce the number of parameters that must be estimated. Additionally, ﬁnding
the minimal such dimension still yielding a model adequately describing the observed
dynamics can provide insight into properties of the underlying dynamical phenomenon.

26
CHAPTER 1.
INTRODUCTION
−15
−10
−5
0
5
10
−2 0 2 4
5
10
15
20
25
30
x
z
y
−15
−10
−5
0
5
10
−4−2 0 2
0
5
10
15
20
25
30
x
z
y
−10
−5
0
5
10
−4−2 0 2
5
10
15
20
25
30
x
z
y
−10
−5
0
5
10
0
5
10
5
10
15
20
25
30
x
z
y
−10
−5
0
5
10
−5
0
5
5
10
15
20
25
30
x
z
y
−10
−5
0
5
10
−2 0 2 4
5
10
15
20
25
30
x
z
y
Figure 1.2. Motion capture skeleton plots for six examples of jumping jacks, each from a diﬀerent
motion capture movie. Skeleton rendering done by modiﬁcations to Neil Lawrence’s Matlab MoCap
toolbox [105].
To jointly address these issues, we propose a method of inducing sparsity in the temporal
dependency structure among variables.
In the problems discussed so far, we have assumed that we are interested in the
dynamics of a single time series. However, in many applications one is presented with
numerous realizations of related phenomena. One example we consider in Chapter 5 is
that of motion capture data depicting multiple people performing a set of related tasks.
In such cases, one would like to discover and model dynamical behaviors which are
shared among the multiple, related time series. For example, in the motion capture data
one might be interested in grouping all instances of jumping jacks from a collection of
videos, as displayed in Fig. 1.2. The beneﬁts of such joint modeling are twofold: we may
more robustly estimate representative dynamic models in the presence of limited data,
and we may also uncover interesting relationships among the time series. Our proposed
method relates the set of dynamical behaviors each object exhibits through utilization
of a beta process prior [67, 165]. This speciﬁc choice of a Bayesian nonparametric prior
allows ﬂexibility in the number of total and sequence-speciﬁc behaviors, and encourages
the time series to share similar subsets of the large set of possible behaviors.
■1.1 Thesis Organization and Overview of Methods and Contributions
We now provide an overview of the contributions of each chapter, including method-
ologies and results, as well as an overview of the chapter structure. The introductory
paragraphs of each chapter provide more detailed outlines.
The overarching theme of the thesis is the proposal of methods for Bayesian nonpara-
metric learning of time series exhibiting complex dynamics that can be approximated
as switches among conditionally linear dynamical modes. For each of the Bayesian non-
parametric time series models that we present, we leverage the simple Markov struc-
ture and the induced conditionally linear dynamics to develop eﬃcient inference tech-
niques. Throughout this thesis, we provide numerous demonstrations that our proposed
Bayesian nonparametric framework provides ﬂexible and eﬃcient methods for learning
simple representative models of dynamical phenomena from limited noisy observations.

Sec. 1.1.
Thesis Organization and Overview of Methods and Contributions
27
■1.1.1 Chapter 2: Background
We begin by reviewing many of the statistical concepts that are utilized throughout
this thesis. The chapter starts by motivating the Bayesian, and more speciﬁcally the
Bayesian nonparametric, approach by presenting the de Finetti theorem.
We then
describe exponential families of probability distributions and suﬃcient statistics. To-
gether, these concepts enable examination of prior distributions, namely conjugate prior
distributions, that lead to eﬃcient inference techniques built upon in this thesis. We
present an analysis of a class of likelihood models and associated conjugate priors used
extensively in our models. The chapter then moves to discussing the graphical model
representation of joint probability distributions. We provide an introduction to graph-
ical models, with an emphasis on the directed chains and their associated inference
techniques that provide the basis for the models we consider and are fundamental to
our derivations. For the more general models we consider in this thesis, exact inference
is infeasible and we rely on the Markov chain Monte Carlo techniques outlined in this
chapter.
We conclude the chapter with background material on the stochastic pro-
cesses we use in developing our Bayesian nonparametric models: the Dirichlet process,
its hierarchical extension, and the beta process.
■1.1.2 Chapter 3: The Sticky HDP-HMM
Accounting for Temporal Mode Persistence
The existing Bayesian nonparametric approach to learning hidden Markov models
(HMMs)—the HDP-HMM [162]—utilizes the hierarchical Dirichlet process (HDP) to
allow for an unbounded HMM mode space. However, as we thoroughly analyze in this
chapter, the HDP-HMM inadequately captures the temporal mode persistence present
in many real datasets such as the speaker diarization application described previously.
To address this issue, we augment the model with a bias towards self-transitions and
show that in our scenarios of interest this sticky HDP-HMM leads to both improved
segmentation performance as well as increased predictive power. Earlier papers have
also proposed self-transition parameters for HMMs with inﬁnite mode spaces [11, 186],
but did not formulate general solutions that integrate fully with Bayesian nonparamet-
ric inference.
One of the main contributions of this chapter is the derivation of an
exact Gibbs sampling technique that allows for a learned bias towards self-transitions
instead of relying on ﬁxing this sticky parameter. As such, the model still allows for
fast-switching dynamics if they are present in the data.
Eﬃcient Inference Leveraging Dynamic Programming
The direct assignment Gibbs sampler [162] developed for inference in the hierarchical
Dirichlet process was also proposed as the sampler for the HDP-HMM. This direct
assignment sampler marginalizes the HMM transition distributions and sequentially
samples the mode sequence. However, as we demonstrate in this chapter, sequential
sampling of a mode sequence with strong correlations leads to very slow mixing rates.

28
CHAPTER 1.
INTRODUCTION
This problem is exacerbated in the case of the sticky HDP-HMM in which the temporal
dependencies encoded in the prior are even stronger than in the HDP-HMM of Teh
et al. [162]. We instead consider a truncated approximation to the sticky HDP-HMM
and develop a sampler that harnesses eﬃcient dynamic programming techniques to
block sample the HMM mode sequence. Speciﬁcally, we utilize a variant of the forward-
backward algorithm [139]. Such sampling techniques have been proposed for the ﬁnite
HMM [148], with analysis showing that blocked sampling requires more computation
time but leads to faster mixing rates than a direct sampler.
Learning Multimodal Emissions and Application to Speaker Diarization
Having developed the sticky HDP-HMM framework that accounts for temporal mode
persistence, one can examine extending the model to account for multimodal emission
distributions. Speciﬁcally, we consider Bayesian nonparametric learning of the emission
distributions by treating each as a mixture of Gaussians with a Dirichlet process prior.
The sticky HDP-HMM’s bias towards generating sequences of observations from the
same latent HMM mode allows the model to disambiguate the underlying emission
distribution.
In contrast, a similar extension of the HDP-HMM of Teh et al. [162]
to allow multimodal emissions exhibits considerable uncertainty in the choice between
rapidly switching amongst HMM modes with single Gaussian emissions or creating
persistent HMM modes and associating multiple Gaussian emission components. As a
motivating example, we consider the problem of speaker diarization and demonstrate
that the sticky HDP-HMM provides state-of-the-art speaker diarizations. We show that
such performance relies on the augmented model’s ability to capture mode persistence
and multimodal emissions.
Chapter Outline
The chapter begins with a review of the HDP-HMM of Teh et al. [162], as well as
a demonstration that this model inadequately captures the temporal mode persistence
present in many real datasets. We then describe our proposed sticky HDP-HMM frame-
work and how one may place a prior on this self-transition bias parameter and infer it
from the data. Both the direct assignment sampler of Teh et al. [162] and the blocked
sampler we develop utilizing the truncated sticky HDP-HMM are subsequently out-
lined. The second half of the chapter focuses on extending the sticky HDP-HMM to
allow for Bayesian nonparametric learning of multimodal emission distributions. We
conclude with an analysis of the NIST speaker diarization database [126].
■1.1.3 Chapter 4: Bayesian Nonparametric Learning of SLDS
Extending the Sticky HDP-HMM to Models with Conditionally Linear Dynamics
The fourth chapter extends the sticky HDP-HMM model of Chapter 3 to scenarios in
which a Markov switching model with conditionally linear dynamics provides a better
approximation to the observed dynamics than the HMM’s assumption of conditionally

Sec. 1.1.
Thesis Organization and Overview of Methods and Contributions
29
independent observations. We consider two such models: the switching linear dynamical
system (SLDS) and switching vector autoregressive (VAR) process and refer to our
Bayesian nonparametric versions of these models as the HDP-SLDS and HDP-AR-
HMM, respectively. The basic formulation we present uses a conjugate matrix-normal
inverse-Wishart (MNIW) [183] prior on the set of dynamic parameters assuming a ﬁxed
model order (i.e., dimension of the SLDS continuous state vector or the autoregressive
order.) For the HDP-SLDS and HDP-AR-HMM, we examine a set of synthetic datasets
demonstrating our ability to learn switching dynamical models with varying numbers
of dynamical regimes. We also examine our ability to segment a sequence of honey bee
dances (see Fig. 1.1(b)) and to detect changes in volatility of the IBOVESPA stock
index, showing performance competitive with alternative methods and consistent with
domain expert analysis.
Sparsity Inducing Priors for Model Order Identiﬁcation
A more complete system identiﬁcation of the switching dynamical models that we con-
sider would also involve learning the model order. Although our HDP-SLDS and HDP-
AR-HMM formulations assume that, respectively, the underlying state dimension or
autoregressive order are ﬁxed, we propose using automatic relevance determination
(ARD) [9, 112, 124] as a sparsity-inducing prior in place of the conjugate MNIW prior.
We speciﬁcally encourage mode-speciﬁc sparsity in the dynamic parameters in a struc-
tured manner that leads to insight into components of the ﬁxed-dimension state vector
or ﬁxed set of autoregressive components that do not contribute to the underlying dy-
namics of the observed phenomenon. In addition to such insights, the sparsity-inducing
prior leads to improved parameter estimation in the presence of limited data. We apply
this model to a sequence of the honey bee dances, and demonstrate that the turning
dances are well-modeled by a single autoregressive component while the waggle dance
relies on two components.
Eﬃcient Inference Leveraging Kalman Filtering
Just as we harnessed dynamic programming techniques in the truncated sticky HDP-
HMM blocked Gibbs sampler, for the HDP-SLDS we can leverage the conditionally
linear dynamics induced by a ﬁxed mode sequence and incorporate eﬃcient Kalman
ﬁlter computations to block-sample the latent state sequence.
Such block sampling
of the state sequence was proposed for the ﬁnite SLDS in [25].
A later paper [26]
analyzed the beneﬁts of an alternative sampler that sequentially samples the dynamical
mode sequence, analytically marginalizing the state sequence. We propose a sampler
that iterates between block sampling of the mode and state sequences, occasionally
interleaving a step of sequentially sampling the mode sequence.

30
CHAPTER 1.
INTRODUCTION
Chapter Outline
Chapter 4 begins with a description of our proposed HDP-SLDS and HDP-AR-HMM
dynamical models. We then describe two possible priors for the dynamic parameters:
the MNIW prior and the sparsity-inducing ARD prior. We outline our Gibbs sampling
algorithm for both the HDP-SLDS and HDP-AR-HMM under these two choices of
priors. Simulations on synthetic data and a sequence of honey bee dances demonstrate
that the developed HDP-SLDS and HDP-AR-HMM are able to infer both the number of
dynamical modes and the underlying model order. We conclude by presenting variants
of these models that are commonly found in application areas such as econometrics and
target tracking. For the latter application, an alternative sampler harnessing the speciﬁc
structure of the model is also presented. We present results for the model variants on
the IBOVESPA stock index, and synthetic maneuvering target tracking data.
■1.1.4 Chapter 5: Sharing Features among Dynamical Systems with Beta
Processes
Transferring Knowledge Among Multiple Related Time Series
The ﬁnal main chapter of the thesis focuses on methods of transferring knowledge
between multiple related time series. We assume that each of the time series can be
modeled according to the switching dynamical processes of Chapters 3 and 4. We then
envision a large library of behaviors, with each time series exhibiting a subset of these
behaviors.
Speciﬁcally, we examine the beta process [67, 165] as a method of tying
together the set of behaviors associated with the time series. This process encourages
sharing in the chosen behaviors while allowing time-series-speciﬁc variability.
One could imagine an alternative architecture based upon the hierarchical Dirichlet
process, similar to the model considered in Chapter 4.
Speciﬁcally, consider a set
of HDP-SLDS’s tied together by sharing the same set of transition distributions and
dynamic parameters. Such a model would assume that each time series was performing
exactly the same set of behaviors, and switching between them in the same manner.
In addition to allowing each time series to choose a unique subset of the full set of
behaviors, our proposed model using the beta process prior also enables multiple time
series to select the same set of behaviors, but to switch between them in a unique
manner. To test our proposed model, we analyze a set of exercise routine videos from
the Carnegie Melon University (CMU) motion capture database [169] and demonstrate
that we are indeed able to identify common motion behaviors. A beneﬁt of our Bayesian
nonparametric approach is that we are also able to discover motions unique to a given
video.
Birth-Death RJ-MCMC for Non-Conjugate IBP Models
The model we introduce does not allow for conjugate analysis, and previous samplers
for the non-conjugate case either relied on approximations [59] or proposals from the
prior [117] that result in low acceptance rates in high-dimensional applications.
In

Sec. 1.1.
Thesis Organization and Overview of Methods and Contributions
31
contrast, we develop a Markov chain Monte Carlo (MCMC) sampler that uses reversible
jump [60] birth and death proposals to explore the incorporation of new behaviors, and
exploits the ﬁnite dynamical system induced by a ﬁxed set of behaviors to eﬃciently
compute acceptance probabilities.
Chapter Outline
We start by describing how the beta process may be used as a prior for relating the
switching dynamical processes of Chapters 3 and 4. Having established the generative
model, we describe an MCMC inference algorithm that allows for eﬃcient exploration
of the set of possible behaviors. We conclude the chapter with empirical results on a
set of synthetic data, and on data from the CMU motion capture database.
■1.1.5 Chapter 6: Contributions and Recommendations
We conclude by surveying the contributions of this thesis, and highlights of directions
for future research. Each chapter concludes with a lengthy discussion of areas of future
research.
In this chapter we simply abstract and jointly examine common themes
appearing throughout the thesis.
■1.1.6 Appendices
For readability and clarity of the main concepts of the thesis, the majority of derivations
are placed in a series of appendices appearing at the end of the thesis. These derivations
focus on determining the conditional distributions and message passing schemes used
in our MCMC samplers, and rely heavily upon the background material presented in
Chapter 2.

32
CHAPTER 1.
INTRODUCTION

Chapter 2
Background
I
N this background chapter, we review the statistical methodologies upon which our
contributions are based. We begin in Sec. 2.1 by motivating the Bayesian framework
through a discussion of exchangeability and de Finetti’s theorem, which can be viewed
as a justiﬁcation for the use of prior distributions. We then describe exponential families
of probability distributions and suﬃcient statistics in Sec. 2.2 and Sec. 2.3, respectively.
Together, these concepts enable examination of prior distributions, namely conjugate
prior distributions, that lead to eﬃcient inference techniques, as discussed in Sec. 2.4.
In Sec. 2.5, we turn to discussing the graphical model representation of joint prob-
ability distributions that allows for the development of eﬃcient inference techniques.
We ﬁrst provide an introduction to graphical models, with an emphasis on the directed
chains and their associated inference techniques that provide the basis for the mod-
els we consider and are fundamental to our derivations. In Sec. 2.6 and Sec. 2.7, we
speciﬁcally consider two such simple directed chain graphical models that are the basic
building blocks for the more complex models we consider in this thesis: the hidden
Markov model and the state space model. For each of these models, we provide an
interpretation of their associated classical inference techniques in terms of the general
graphical model framework described in Sec. 2.5.
For the models we consider in this thesis, exact inference is infeasible and we rely on
Markov chain Monte Carlo techniques that are outlined in Sec. 2.8. Finally, we conclude
in Sec. 2.9 by providing background material on the stochastic processes we use in
developing our Bayesian nonparametric models: the Dirichlet process, its hierarchical
extension, and the beta process.
■2.1 The Bayesian Framework
In this section we provide a brief motivation for the Bayesian approach and establish
some concepts that reappear throughout this thesis. The overarching goal of the thesis
is then to examine the ﬂexibility a Bayesian approach can provide in the case of learning
dynamical systems.
33

34
CHAPTER 2.
BACKGROUND
■2.1.1 Modeling via Exchangeability
The concept of exchangeability is central to many statistical approaches, and may be
viewed as critical in motivating Bayesian statistics. Let us assume that we are aggregat-
ing data in an attempt to make predictions about future values of the random process
we are observing. If we were to make the strong assumption of the data being indepen-
dent, we would treat every new data point individually without using past observations
to predict future observations since:
p(y1, . . . , yn) =
n
Y
i=1
p(yi)
(2.1)
implies that
p(yn+1, . . . , ym | y1, . . . , yn) = p(yn+1, . . . , ym).
(2.2)
A weaker assumption that often better describes the data we encounter is that of
exchangeability, which states that the order we encounter the data is inconsequential.
Deﬁnition 2.1.1. A sequence of random variables y1, y2, . . . , yn is said to be ﬁnitely
exchangeable if
y1, y2, . . . , yn
D= yπ(1), yπ(2), . . . , yπ(n)
(2.3)
for every permutation π on {1, . . . , n}. Here, we use the notation D= to mean equality
in distribution.
From this deﬁnition, we see that independence implies exchangeability, but not vice
versa.
We are often in settings where data is continually accumulated, or in which
ﬁxing an upper bound n is challenging. We would thus like to formalize a notion of
exchangeability for inﬁnite sequences.
Deﬁnition 2.1.2. A sequence y1, y2, . . . is said to inﬁnitely exchangeable if every ﬁnite
subsequence is ﬁnite exchangeable [15].
As is demonstrated in Bernardo and Smith [15], not every ﬁnitely exchangeable
sequence can be embedded in an inﬁnitely exchangeable sequence.
Example 2.1.1. As an example of inﬁnite exchangeability, consider an urn with b black
balls and w white balls. Draw a ball at random from the urn and replace that ball along
with n balls of the same color. Continue repeating this procedure inﬁnitely many times.
Such an urn is typically referred to as a P´olya urn. Let yi = 1 if the ith draw from the
urn produces a black ball, and yi = 0 otherwise. Then,
p(1, 1, 0, 1) =
b
b + w
b + n
b + w + n
w
b + w + 2n
b + 2n
b + w + 3n
=
b
b + w
w
b + w + n
b + n
b + w + 2n
b + 2n
b + w + 3n
= p(1, 0, 1, 1).

Sec. 2.1.
The Bayesian Framework
35
The denominator is the same for all possible sequences since n balls are added at every
draw regardless of the color of the drawn ball. The sequence of terms in the numerator
simply depends upon how many previous times a black or white ball was drawn, not the
speciﬁc order. Using this argument, one can prove that every ﬁnite subsequence of data
generated from this urn procedure are exchangeable under this model. However, we can
clearly see that the data are not independent, nor even a Markov process.
Exchangeability has simplifying implications for inference since we can simply ignore
the order in which the data arrive. Sometimes, however, exchangeability is too strong of
an assumption. Relaxations include considering partially exchangeable data where some
auxiliary information partitions the data into exchangeable sets. For example, consider
a person ﬂipping two biased coins, one on even throws and the other on odd throws. The
data are exchangeable within the set of odd or even tosses if these labels are provided.
There are many possible extensions and variations on the standard exchangeability
model; however, the end goal is to group data into exchangeable, and thus relatively
simple, blocks for which inference is more tractable.
A very important result arising from the assumption of exchangeable data is what
is typically referred to as de Finetti’s theorem. This theorem states that an inﬁnite
sequence of random variables y1, y2, . . . is exchangeable if and only if there exists a
random probability measure ν with respect to which y1, y2, . . . are conditionally i.i.d.
with distribution ν. Furthermore, this random measure can be viewed as the limit-
ing empirical measure. De Finetti actually proved this in the case of binary random
variables de Finetti [33], with the more general extension to arbitrary real-valued ex-
changeable sequences made by Hewitt and Savage [66] and Ryll-Nardzewski [146].
Theorem 2.1.1. If y1, y2, . . . is an inﬁnitely exchangeable sequence of binary random
variables with probability measure P, then there exists a distribution function Q on [0, 1]
such that for all n
p(y1, . . . , yn) =
Z 1
0
n
Y
i=1
ϑyi(1 −ϑ)1−yidQ(ϑ),
(2.4)
where p(y1, . . . , yn) is the joint probability mass function deﬁned by measure P. Fur-
thermore, Q is the distribution function of the limiting empirical frequency:1
θ a.s.
=
lim
n→∞
1
n
n
X
i=1
yi,
θ ∼Q.
(2.5)
Proof. Originally presented in [33]. See Bernardo and Smith [15] and Heath and Sud-
derth [65] for a proof in more modern terms.
■
1The notation x ∼F indicates that the random variable x is drawn from a distribution F. We use
bar notation x | F ∼F to specify conditioned upon random variables, such as a random distribution.

36
CHAPTER 2.
BACKGROUND
This theorem can be interpreted as saying that if y1, y2, . . . is an inﬁnitely exchange-
able binary sequence, then it is as if the elements of this sequence are independent
Bernoulli random variables with probability of success θ, where θ has distribution Q.
Furthermore, one can interpret Q as our belief about the limiting empirical frequency
of ones in the data.
From de Finetti’s theorem, we see the motivation for the Bayesian perspective of the
parameter yielding the observations i.i.d. as a random quantity with some distribution
Q, rather than as a ﬁxed and unknown quantity. We now state the more general form
of the de Finetti theorem.
Theorem 2.1.2. If y1, y2, . . . is an inﬁnitely exchangeable sequence of real-valued ran-
dom variables with probability measure P, then there exists a probability measure µ
deﬁned on the space of all probability measures P(R) on R such that2
P(y1 ∈A1, . . . , yn ∈An) =
Z
P(R)
n
Y
i=1
V(Ai)µ(dV)
(2.6)
Furthermore, µ is the law of a probability measure ν, where ν is almost surely deﬁned
by the limiting empirical measure. Namely,
ν(B) a.s.
=
lim
n→∞
1
n
n
X
i=1
IB(yi),
ν ∼µ.
(2.7)
where B ranges over all elements of the Borel σ-algebra. The measure µ is often referred
to as the de Finetti measure.
Proof. See Hewitt and Savage [66] and Ryll-Nardzewski [146].
■
From a generative perspective, the theorem states that if y1, y2, . . . are inﬁnitely
exchangeable, then there exists a measure µ on measures such that:
ν ∼µ
yi | ν i.i.d.
∼ν.
(2.8)
When we take the sets Ai to be (−∞, yi], we obtain a form of the above theorem in
terms of the random distribution functions F associated with the random measures ν.
Example 2.1.2. As an informal presentation to provide some intuition for this the-
orem, let us return to the case of binary random variables. Assume the phenomenon
we are observing are realizations from a game, though we do not know the game being
played. Instead, we are simply observers of the outcomes of the game. For example,
2Here, we use V as the variable of integration when integrating with respect to the probability
measure µ. We then use ν as the random measure with law µ.

Sec. 2.1.
The Bayesian Framework
37
assume the observed phenomenon are ﬂips of a coin with probability p of heads. The
limiting empirical frequency of heads (i.e., 1’s) in inﬁnitely many draws will be p almost
surely. The de Finetti theorem then implies that the de Finetti measure µ is degenerate
on
ν = pδ1 + (1 −p)δ0
because every such inﬁnite sequence of ﬂips of that coin results in the same empirical
measure. See Fig. 2.1(a). Here, we use δi to be a measure concentrated at i.
On the other hand, assume we are observing draws from a P´olya urn starting with
b black balls and w white balls, adding n balls per round, as in Example 2.1.1. From
this example, we know that the data are exchangeable. We observe an inﬁnite binary
sequence which gives us the following empirical measure:
ν1 = θ1δ1 + (1 −θ1)δ0.
We are provided with inﬁnitely many such sequences from an urn in the (b, w) start-
ing conﬁguration (i.e., inﬁnitely many realizations from this game.) For each inﬁnite
sequence, we build the empirical measure
νi = θiδ1 + (1 −θi)δ0
i = 1, 2, . . . .
De Finetti tells us that these νi are instantiations of the random measure ν. In essence,
we can empirically build up the de Finetti measure µ by examining the inﬁnite collection
of empirical measures. Let us instead examine the distribution Q on θ. One can show
that Q is a Beta(b/n, w/n) distribution (see Sec. 2.4.2), as empirically demonstrated in
Fig. 2.1(b), implying that the generative process is
θ ∼Beta(b/n, w/n)
yi | θ i.i.d.
∼Ber(θ),
where Ber denotes the Bernoulli distribution. This process is referred to as a Beta-
Bernoulli process. There are many other such games for generating inﬁnitely exchange-
able binary sequences that we could be observing, each corresponding to a diﬀerent de
Finetti measure.
As the observer of these sequences, de Finetti simply tells us that
there exists a random probability measure which yields the data i.i.d.; we would need
to observe inﬁnitely many sequences to actually reconstruct the distribution on this
probability measure associated with the underlying game.
We have seen in Theorem 2.1.1 that for inﬁnitely exchangeable binary sequences,
there exists a random probability measure ν that concentrates on {0, 1} implying that
this measure can be uniquely described by a single parameter θ. One can straight-
forwardly extend the argument in Theorem 2.1.1 to inﬁnitely exchangeable sequences
taking values in {1, . . . , K}; here, the random measure yielding the data i.i.d. concen-
trates on {1, . . . , K} and is thus uniquely deﬁned by a (K −1)-dimensional parameter

38
CHAPTER 2.
BACKGROUND
0
0.2
0.4
0.6
0.8
1
0
50
100
150
200
250
300
350
400
p
Counts
0
0.2
0.4
0.6
0.8
1
0
50
100
150
200
250
Theta
Counts
(a)
(b)
Figure 2.1.
(a) Histogram of the empirical estimates of the probability of heads, p, in the coin-
ﬂipping experiment from 10,000 trials. Each trial’s estimate is based on 100,000 observations. The red
line indicates the true probability of heads. (b) Histogram of the empirical estimates of the parameter
θ that yields the exchangeable observations drawn from a P´olya urn i.i.d.. The histogram is the result
of 10,000 trials from an urn starting with 10 black balls and 6 white balls, and replacing 2 balls at every
draw from the urn. Each trial’s estimate of θ is produced based on 1,000 observations. The red line
indicates a Beta(10/2, 6/2) distribution.
θ = {θ1, . . . , θK−1} [15]. Analogous to the examples presented in Example 2.1.2, pos-
sible underlying games include rolling a K-sided weighted die or drawing from an urn
with K diﬀerent colored balls. When moving to inﬁnitely exchangeable sequences tak-
ing values in the reals, the random probability measures ν can be arbitrarily complex
and are, in general, deﬁned by inﬁnitely many parameters (i.e., ν is a generic element of
P(R).) Some special cases exist in which the parametrization remains ﬁnite. For exam-
ple, if ν is almost surely a Gaussian distribution, the parametrization solely consists of
a mean and variance. The more general case in which θ may be an inﬁnite-dimensional
parameter motivates the development of Bayesian nonparametric methods, some of
which we explore in this thesis. For example, the Dirichlet process of Sec 2.9.1 deﬁnes
a distribution on probability measures that concentrate at a countably inﬁnite number
of elements of the reals (or the more general spaces we consider in Sec. 2.9.1.)
When we limit ourselves to the more restrictive class of ﬁnite-dimensional θ (e.g.,
Bernoulli, multinomial, Gaussian random variables), we can invoke the following corol-
laries.
Corollary 2.1.1. Assuming the required densities exist, and assuming the conditions
of Theorem 2.1.2 hold, then there exists a distribution function Q such that the joint
density of y1, . . . , yn is of the form
p(y1, . . . , yn) =
Z
Θ
n
Y
i=1
p(yi | ϑ)dQ(ϑ),
(2.9)
with p(· | ϑ) representing the density function corresponding to the ﬁnite-dimensional

Sec. 2.1.
The Bayesian Framework
39
parameter ϑ ∈Θ.
From the above corollary, it is simple to see how the de Finetti theorem motivates
the concept of a prior distribution Q(·) and a likelihood function p(y | ·).
Corollary 2.1.2. Given that the conditions of Corollary 2.1.1 hold, then the predictive
density is given by
p(ym+1, . . . , yn | y1, . . . , ym) =
Z
Θ
n
Y
i=m+1
p(yi | ϑ)dQ(ϑ | y1, . . . , ym),
(2.10)
where
dQ(θ | y1, . . . , ym) =
Qm
i=1 p(yi | θ)dQ(θ)
R
Θ
Qm
i=1 p(yi | ϑ)dQ(ϑ).
(2.11)
Proof. The result follows directly from employing
p(ym+1, . . . , yn | y1, . . . , ym) = p(y1, . . . , yn)
p(y1, . . . , ym),
along with Corollary 2.1.1.
■
From the form of the predictive density in Eq. (2.10), we see that our view of
the existence of an underlying random parameter θ yielding the data i.i.d. has not
changed. Instead, we have simply updated our prior belief Q(θ) into a posterior belief
Q(θ | y1, . . . , ym) through an application of Bayes rule:
p(θ | y) =
p(y | θ)p(θ)
R
Θ p(y | ϑ)p(ϑ)dϑ = p(y | θ)p(θ)
p(y)
.
(2.12)
Here, we have written the rule in its simplest form assuming that a density on θ exists
in addition to the conditional density on y. Although one can view the computation
of the predictive distribution in Eq. (2.10) as the objective in Bayesian statistics, we
will often limit our discussion to the process of forming the posterior distribution in
Eq. (2.11) from the prior by incorporating observations, since this is a fundamental step
in examining the predictive distribution.
From a practical perspective, we never have an inﬁnite sequence of observations
from which to characterize our prior distribution. Furthermore, even if we had such a
quantity, the probability measure that the de Finetti theorem would suggest as yielding
the data i.i.d. might be arbitrarily complex. Thus, we are left with two competing
pragmatic choices in deﬁning our prior:
(1) Tractable inference,
(2) Modeling ﬂexibility.

40
CHAPTER 2.
BACKGROUND
The issue of tractable inference often motivates the use of conjugate priors, as discussed
in Sec. 2.4. The goal of ﬂexibility in our models leads to the study of Bayesian non-
parametric methods. A brief introduction to some speciﬁc classes of nonparametric
methods that maintain computational tractability is presented in Sec. 2.9.1-Sec. 2.9.4.
Another key aspect of the Bayesian framework we have established is in charac-
terizing a model, or likelihood distribution, p(y | θ) for how our data are generated
conditioned a parameter value θ. This choice, too, is often motivated by practical con-
siderations that are typically coupled with those of choosing a prior distribution. We
do not develop a full analysis of model selection in this thesis, but begin the explo-
ration in Sec. 2.2. As practitioners, we do not actually know the underlying generative
process, but we can use a combination of our insight on the process (e.g., we know we
are observing heights from a given population and heights tend to be well-modeled as
Gaussian) and our adherence to computational limitations to deﬁne a model.
■2.2 Exponential Families
Exponential families represent a fundamental class of distributions in statistics. They
arise as the answer to numerous, albeit related, questions. Within the Bayesian frame-
work: For what class of models does there exist a prior that leads to computationally
tractable inference [15, 141]? Frequentists arrive at the exponential family when ask-
ing: If there exists an eﬃcient estimator, can we describe the class of models from
which the data could have been generated [87, 184]? Common to both domains: What
distribution is maximally random while being consistent with a set of moment con-
straints [15, 79, 116]?
Deﬁnition 2.2.1. A parametrized family of distributions PΘ = {Pθ} is a k-parameter
exponential family with natural parameter η(·) = [η1(·), . . . , ηk(·)]T , natural statistic
t(·) = [t1(·), . . . , tk(·)]T , and base distribution q(·) ∝eβ(·) if each member Pθ of the
family has a density of the form
p(y | θ) = exp{ηT (θ)t(y) −α(θ) + β(y)}
(2.13)
= exp
( k
X
i=1
ηi(θ)ti(y) −α(θ) + β(y)
)
(2.14)
with respect to a dominating measure3 µ. Here, y4 is a point in the sample space Y,
which represents the support of the density.
The function α(·) is referred to as the
log-partition function and ensures that the probability density integrates to 1. We will
denote this family by E(θ; η(·), t(·), β(·)).
3The dominating measure is the assumed measure on the considered measurable space, and as such
provides the measure with respect to which the Radon-Nikodym derivative is taken when deﬁning
densities (amongst other measure-theoretic operations one could examine).
4We use the notation y rather than y to indicate that this quantity is allowed to be vector valued.

Sec. 2.2.
Exponential Families
41
The set of admissible parameter values, or the natural parameter space, for which a
constant α(θ) exists are those such that
Z
exp
( k
X
i=1
ηi(θ)ti(y) + β(y)
)
dy < ∞.
(2.15)
We could generalize Eq. (2.15) and the results to follow for a given measure µ rather
than the assumed Lebesgue (or where appropriate, counting) measure. However, we
will omit this level of mathematical formality.
It is common to restrict oneself to examining families of distributions whose support,
i.e., the set of y such that p(y | θ) > 0, does not depend upon θ.
Deﬁnition 2.2.2. An exponential family E(θ; η(·), t(·), β(·)) is called regular if the
support of each member of the family does not depend upon the value of the parameter
θ.
Another form of exponential families that deserves a special name is when the
density of each member of the family depends linearly on the parameters, i.e., η(θ) =
[θ1, . . . , θk].
Deﬁnition 2.2.3. A canonical exponential family is one which depends linearly on the
parameter θ:
p(y | θ) = exp{θT t −α(θ) + β(y)}
(2.16)
= exp
( k
X
i=1
θiti(y) −α(θ) + β(y)
)
.
(2.17)
We will denote the canonical exponential family by E(θ; I(·), t(·), β(·)).
One, in theory, can always consider an exponential family in its canonical form by
deﬁning a family Pη with the parameters as the possibly nonlinear mapping η(θ) ≜
[η1, . . . , ηk] and the log-partition function α(·) appropriately redeﬁned.
In practice,
however, it might be challenging to ﬁnd the set of admissible values of η and the form
of the log-partition function. Note that some references, such as Bernardo and Smith
[15], use the term canonical to refer to exponential families that also depend linearly
on the data.
Deﬁnition 2.2.4. For data y distributed according to p(y | θ), a parameter θ is termed
unidentiﬁable on the basis of y if there exists θ1 ̸= θ2 such that Pθ1 = Pθ2.
Lemma 2.2.1. If the set of natural statistics [t1(·), . . . , tk(·)] are linearly dependent,
then the parameters [η1, . . . , ηk] are unidentiﬁable from the data y.

42
CHAPTER 2.
BACKGROUND
Proof. Assume, without loss of generality, that tk(y) = ctk−1(y) for some constant c.
Take η′
i = ηi for i = 1, . . . , k −2, η′
k−1 = ηk−1 + cηk, and η′
k = 0. Then,
p(y | η) = exp
( k
X
i=1
ηiti(y) −α(θ) + β(y)
)
= exp
(k−2
X
i=1
ηiti(y) + (ηk−1 + cηk)tk−1(y) −α(θ) + β(y)
)
= exp
(k−2
X
i=1
η′
iti(y) + η′
k−1tk−1(y) −α(θ) + β(y)
)
= p(y | η′).
■
From the above, we see that whenever there are linearly dependent natural statistics,
we can ﬁnd an equivalent, reduced-order exponential family. We will assume that we
always restrict ourselves to such reduced-order models. Note that the same issue arises
if the components of the natural parameter η(θ) are linearly dependent functions of θ.
Deﬁnition 2.2.5. A minimal exponential family is one in which there does not exist a
non-zero vector a = [a1, . . . , ak] such that
k
X
i=1
aiti(y)
(2.18)
is equal to a constant.
■2.2.1 Properties of the Canonical Exponential Family
The following theorem leads to a number of useful properties of the exponential family,
speciﬁcally, the moment-generating property of the log-partition function.
Theorem 2.2.1. For any integrable function f(·) and any θ in the set of natural
parameters, the integral
Z
f(y) exp
nX
θiti(y) −α(θ) + β(y)
o
dy
(2.19)
is continuous and has derivatives of all orders with respect to the parameters θ.
Proof. See Barndorﬀ-Nielsen [8], amongst other texts.
■

Sec. 2.2.
Exponential Families
43
Corollary 2.2.1. The expected value and covariance of the natural statistics ti(y) are
related to derivatives of the log-partition function by
Eθ[ti(y)] = ∂
∂θi
α(θ)
(2.20)
and
cov(ti(y), tj(y)) =
∂2
∂θj∂θi
α(θ),
(2.21)
respectively.
Proof. We apply Theorem 2.2.1 to the following identity, arising from the unit integra-
bility of the density p(· | θ):
Z
exp
nX
θiti(y) −α(θ) + β(y)
o
dy = 1.
Taking the derivative with respect to θi,
Z
ti(y) exp
nX
θiti(y) −α(θ) + β(y)
o
dy = ∂
∂θi
α(θ).
(2.22)
The ﬁrst equality of the corollary results from noting that the left-hand side of Eq. (2.22)
is the expected value of ti(y) under the given exponential family. Diﬀerentiating again
with respect to θj yields
Z 
ti(y) −∂
∂θi
α(θ)
 
tj(y) −∂
∂θj
α(θ)

exp
nX
θiti(y) −α(θ) + β(y)
o
dy
−
∂2
∂θj∂θi
α(θ)
Z
exp
nX
θiti(y) −α(θ) + β(y)
o
dy = 0.
(2.23)
Identifying the ﬁrst line of Eq. (2.23) as cov(ti(y), tj(y)) using the fact that
∂
∂θj α(θ) =
Eθ[tj(y)], and identifying the integral of the second line as 1 completes the proof.
■
The above implies that instead of computing potentially complicated integrals, we
can ﬁnd the moments of a distribution by calculating the derivatives of the log-partition
function. We note, however, that ﬁnding the log-partition function is often a challenge
in and of itself. Another important implication of the above result is that since ∇2α(·)
is a positive semi-deﬁnite covariance matrix, α(θ) is a convex function in θ. For minimal
exponential families, ∇2α(·) must be positive deﬁnite, implying strict convexity. Such
interpretations of α(·) have important implications for the geometry of exponential
families that are exploited in variational approaches [176].

44
CHAPTER 2.
BACKGROUND
■2.2.2 Interpretation as Linearly Constrained Maximum Entropy Distribu-
tion
As alluded to at the beginning of this section, the exponential family can be derived
as the maximally random distribution subject to a set of linear constraints. To derive
this result, and to formalize our deﬁnition of randomness, we need to rely on some
information-theoretic concepts. See Cover and Thomas [31] for a more detailed explo-
ration of these terms.
Fundamental Quantities of Information Theory
Shannon’s measure of entropy conveys the uncertainty of a discrete random variable y
taking values within a ﬁnite space Y:
H(y) = −
X
y∈Y
p(y) log p(y),
(2.24)
where p(y) is the associated probability mass function deﬁning the law of y. If the log
is base 2, the units of this measure is in bits while for base e the units are nats. From
this deﬁnition, one can easily prove that
0 ≤H(y) ≤log |Y|.
(2.25)
One can extend the idea of entropy to jointly random variables (x, y) ∼p(x, y), x ∈X,
in which case the joint entropy is deﬁned as
H(x, y) = −
X
x∈X
X
y∈Y
p(x, y) log p(x, y),
(2.26)
One can similarly deﬁne the conditional entropy of a random variable y given x:
H(y | x) = −
X
x∈X
X
y∈Y
p(x, y) log p(y | x).
(2.27)
Using standard manipulations, one can show that the joint entropy H(x, y) is simply
the sum of the entropy of x, H(x), and the conditional entropy of y given x, H(y | x),
which has a nice interpretation in terms of conservation of uncertainty. The change in
entropy of a random variable y after an observation x is given by the mutual information
I(y; x) =
X
x∈X
X
y∈Y
p(x, y) log p(x, y)
p(x)p(y)
(2.28)
= −
X
x∈X
X
y∈Y
p(x, y)(log p(y) −log p(y | x))
(2.29)
= H(y) −H(y | x).
(2.30)

Sec. 2.2.
Exponential Families
45
The above deﬁnitions can be extended to continuous random variables by considering
diﬀerential entropy
h(y) = −
Z
Y
p(y) log p(y)dy,
(2.31)
and diﬀerential conditional entropy
h(y | x) = −
Z
X
Z
Y
p(x, y) log p(y | x)dydx.
(2.32)
However, although discrete entropy is a non-negative quantity, diﬀerential entropy does
not have this property.
Finally, we deﬁne a measure of the distance between two densities p and q. The
relative entropy or Kullback-Leibler (KL) divergence is given by:
D(p||q) =
Z
Y
p(y) log p(y)
q(y)dy.
(2.33)
Note that because KL divergence is not symmetric, it is not actually a distance metric.
From this deﬁnition, we see that mutual information can be interpreted as the KL
divergence between a joint distribution of (x, y) and the distribution assuming they are
independent random variables:
I(y; x) = D(p(x, y)||p(x)p(y)).
(2.34)
Here, the mutual information is deﬁned in terms of diﬀerential entropy.
Projections onto Exponential Families
Let us deﬁne a linear family of distributions for a random variable y as
Lt = {p : Ep[tk(y)] = µk,
k = 1, . . . , K}.
(2.35)
This family is termed linear since for all p1, p2 ∈Lt, and for all λ ∈[0, 1], pλ =
λp1 + (1 −λ)p2 ∈Lt.
Theorem 2.2.2. Let t(·) = {t1(·), . . . , tK(·)} be a set of functions deﬁned on Y and
{µ1, µ2, . . . , µK} be a set of arbitrary constants. Deﬁne the linear family
Lt = {p : Ep[tk(y)] = µk,
k = 1, . . . , K},
(2.36)
and consider the element of this family, p∗, which satisﬁes
p∗= arg min
p∈Lt D(p||q),
(2.37)
where the support of q contains that of p. Then p∗belongs to the exponential family
Et =
(
p : p(y) = exp
 K
X
k=1
λktk(y) −α(λ) + log q(y)
!)
.
(2.38)

46
CHAPTER 2.
BACKGROUND
Proof. See Bernardo and Smith [15] for a proof. The basic idea is to minimize the
Lagrange function consisting of the KL divergence and a set of Lagrange multipliers that
enforce the linear constraints, as well as the constraint that p∗must be a valid density.
These Lagrange multipliers end up as the natural parameters λ of the exponential
family.
■
If we take q(y) ∝1 for all y ∈Y (an improper distribution in the case when Y is not
ﬁnite), then p∗has the interpretation as the maximum entropy distribution that satisﬁes
a set of moment constraints. As an example, the maximum entropy distribution over
the real line subject to a second moment constraint is a zero-mean Gaussian distribution
with variance given by that constraint.
■2.2.3 Examples
Many well-known classes of distributions can be cast within the framework of an expo-
nential family. We now present a set of examples of such manipulations.
Bernoulli
p(y | θ) = θy(1 −θ)1−y
θye−θ
y!
y ∈{0, 1}
(2.39)
ln p(y | θ) = y ln θ + (1 −y) ln(1 −θ)
(2.40)
= ln

θ
1 −θ

|
{z
}
η(θ)
y
|{z}
t(y)
+ ln(1 −θ)
|
{z
}
α(θ)
(2.41)
Geometric
p(y | θ) = (1 −θ)θy
y ∈{0, 1, 2, . . . }
(2.42)
ln p(y | θ) = ln(θ)
| {z }
η(θ)
y
|{z}
t(y)
+ ln(1 −θ)
|
{z
}
α(θ)
(2.43)
Poisson
p(y | θ) = θye−θ
y!
y ∈{0, 1, 2, . . . }
(2.44)
ln p(y | θ) = ln(θ)
| {z }
η(θ)
y
|{z}
t(y)
−
θ
|{z}
α(θ)
−ln y
|{z}
β(y)
(2.45)
Exponential
p(y | θ) = θe−θy
y > 0
(2.46)
ln p(y | θ) = −θy
|{z}
η(θ)t(y)
+ ln θ
|{z}
α(θ)
(2.47)

Sec. 2.3.
Suﬃcient Statistics
47
■2.3 Suﬃcient Statistics
For the exponential family, we have seen that the densities only depend on the data
through the natural statistics t(y) and the base distributions q(y) ∝exp{β(y)}. This
leads one to ask under what conditions are inferences using transformations of the data,
or statistics, the same as if we had used the data itself. One might additionally ask
what set of models yield a compact set of statistics, summarizing an arbitrarily large
set of data, that are suﬃcient for the inferences we wish to make. In the following, we
establish a formal framework for this data-processing concept.
Deﬁnition 2.3.1. Given a sequence of random variables y1, y2, . . . , with yj ∈Yj and
probability measure P, a sequence of statistics t1, t2, . . . , with each function tj deﬁned
on the product space Y1 × · · · × Yj, is said to be predictive suﬃcient for y1, y2, . . . if
p(yi1, . . . , yik | y1, . . . , yj) = p(yi1, . . . , yik | tj)
∀j, k
(2.48)
where {i1, . . . , ik} are a set of indices not seen in {1, . . . , j}.
Here, p(· | ·) is the
conditional density induced by the measure P.
That is, given tj = tj(y1, . . . , yj), the values of the data y1, . . . , yj do not further
contribute to the prediction of future values of data.
Deﬁnition 2.3.2. Given an exchangeable5 sequence of random variables y1, y2, . . . ,
each with sample space Y, the sequence of statistics t1, t2, . . . , with each function tj
deﬁned on the product space Yj, is said to be parametric suﬃcient for y1, y2, . . . if
dQ(θ | y1, . . . , yn) = dQ(θ | tn)
∀n ≥1,
(2.49)
for any dQ(θ) such that
p(y1, . . . , yn) =
Z
n
Y
i=1
p(yi | θ)dQ(θ).
(2.50)
Informally, this deﬁnition of suﬃciency implies that, given exchangeable data, pos-
terior inference using a parametric suﬃcient statistic results in the same analysis as
using the data itself. The following theorem provides a connection between a statistic
being suﬃcient for prediction and for posterior inference.
Theorem 2.3.1. Given an exchangeable sequence of random variables y1, y2, . . . , each
with sample space Y, the sequence of statistics t1, t2, . . . , with tj deﬁned on the product
space Yj, is predictive suﬃcient if, and only if, it is parametric suﬃcient.
Proof. See Sec. 4.5 of Bernardo and Smith [15] for a heuristic proof.
■
5Unless otherwise noted, for an inﬁnite sequence of random variables we use the phrases exchangeable
and inﬁnitely exchangeable interchangeably.

48
CHAPTER 2.
BACKGROUND
The following theorem identiﬁes the structure in the probability model that leads
to the existence of parametric suﬃcient statistics, thus providing insight into how to
propose and test statistics for such suﬃciency.
Theorem 2.3.2 (Neyman factorization criterion). The sequence of statistics t1, t2, . . .
is parametric suﬃcient for an inﬁnitely exchangeable sequence of random variables
y1, y2, . . . if and only if the joint density for y1, . . . , ym can be factored as
p(y1, . . . , ym | θ) = hm(tm, θ)g(y1, . . . , ym),
m ≥1,
(2.51)
for some functions hm ≥0 and g > 0.
Proof. This proof follows that provided in Sec. 4.5 of [15]. Assume such a factorization
exists. Then, for any dQ(θ) we may write
dQ(θ | y1, . . . , ym) =
p(y1, . . . , ym | θ)dQ(θ)
R
Θ p(y1, . . . , ym | ϑ)dQ(ϑ) =
hm(tm, θ)dQ(θ)
R
Θ hm(tm, ϑ)dQ(ϑ).
The righthand equality depends on the data y1, . . . , ym solely through the statistic tm,
and thus, dQ(θ | y1, . . . , ym) = dQ(θ | tm).
Conversely, assume that tm is a parametric suﬃcient statistic. Then,
p(y1, . . . , ym | θ)dQ(θ)
p(y1, . . . , ym)
= dQ(θ | y1, . . . , ym)
= dQ(θ | tm) = p(tm | θ)dQ(θ)
p(tm)
.
The result follows by identifying that this must imply
p(y1, . . . , ym | θ) = hm(tm, θ)g(y1, . . . , ym)
for some hm ≥0, g > 0.
■
From the Neyman factorization criterion, and from the fact that we can write the
likelihood of N i.i.d. observations from a k-parameter exponential family as
p(y1, . . . , yN | θ) = exp
(
η(θ)T
N
X
n=1
t(yn) −Nα(θ) +
N
X
n=1
β(yn)
)
(2.52)
= exp
(
η(θ)T
N
X
n=1
t(yn) −Nα(θ)
)
exp
( N
X
n=1
β(yn)
)
,
(2.53)
we see that sn(y1, . . . , yn) = {n, Pn
i=1 t1(yi), . . . , Pn
i=1 tk(yi)}, n = 1, 2, . . . , is a se-
quence of suﬃcient statistics.
Furthermore, the Pitman-Koopman-Darmois theorem [80, 141] states that a prob-
ability model admits a suﬃcient statistic whose dimension remains bounded as the
sample size increases if and only if it is an exponential family model. The ﬁrst proof
of this result is due to Darmois [32] (in French), with two versions in English produced
independently by Pitman [136] and Koopman [98], each using slightly diﬀerent technical
conditions.

Sec. 2.4.
Incorporating Prior Knowledge
49
■2.4 Incorporating Prior Knowledge
Within the Bayesian framework, motivated by the concepts presented in Sec. 2.1.1,
one is interested in incorporating a prior distribution on the latent model parameter θ
in order to make predictions about future data. Assuming the associated conditional
densities exist, as we will throughout this section, and given N i.i.d. observations, this
predictive likelihood is given by:
p(y | y1, . . . , yN, λ) =
Z
Θ
p(y | ϑ)p(ϑ | y1, . . . , yN, λ)dϑ.
(2.54)
Here, we take the prior distribution itself to be contained within a family PΛ parame-
terized by a set of hyperparameters λ ∈Λ. The hyperparameters are not fundamental
to the objective of our inference, and can simply be viewed as tuning parameters. As an
intermediary step in the process of predictive analysis, one might simply be interested
in examining the posterior density on θ:
p(θ | y, λ) =
p(y | θ)p(θ | λ)
R
Θ p(y | ϑ)p(ϑ | λ)dϑ
(2.55)
There are many perspectives on how one should choose a prior distribution on
the latent parameter θ. A subjective Bayesian would argue that one should choose
a distribution that encodes our subjective prior belief about the values of θ. On the
other hand, objective Bayesians aim to remain agnostic and employ a prior distribution
that is maximally uninformative, allowing the data to speak most loudly. Such goals
often lead to the use of “ﬂat” priors (e.g., limiting forms of the conjugate families
discussed in Sec. 2.4.1), but with sometimes unintended implications [13].
A more
coherent framework for developing objective priors is that of reference analysis, ﬁrst
introduced by Bernardo [14] and further developed by Berger and Bernardo [12]6. A
reference prior is one that—constrained within a class of candidate priors—maximizes
the uncertainty about θ relative to the knowledge that could be gained about θ from
repeated observations from the model. For any suﬃciently regular prior p(θ), as the
number of observations tends to inﬁnity, the posterior of θ concentrates about its true
value. Thus, the limiting mutual information
lim
k→∞
Z
Yk
Z
Θ
p(ϑ, y1, . . . , yk) log p(ϑ, y1, . . . , yk)
p(ϑ)p(y1, . . . , yk)dϑdy1:k,
(2.56)
or equivalently, the average divergence between the prior and the posterior:
lim
k→∞
Z
Yk p(y1, ..., yk)
Z
Θ
p(ϑ | y1, . . . , yk) log p(ϑ | y1, . . . , yk)
p(ϑ)
dϑdy1:k
(2.57)
provides a measure of the amount of missing information about θ after receiving in-
ﬁnitely many observations from the model.
A reference prior aims to choose from
6See also [13] for a comprehensive survey.

50
CHAPTER 2.
BACKGROUND
within a speciﬁed class the prior that maximizes this missing information. Thus, refer-
ence priors only depend on the asymptotic behavior of the assumed model7. In ﬁnite
parameter spaces (i.e., |Θ| = K, K < ∞), the reference prior reduces to the prior that
maximizes the entropy within the class of candidate priors, as proposed by Jaynes [79].
For one-dimensional location and scale families, such as the family of univariate Gaus-
sian distributions parameterized by a mean (location parameter) and variance (scale
parameter), the reference prior is a constant for the location parameter (i.e., improper)
and equivalent to Jeﬀreys prior [81] for the scale parameter. For more complex models,
however, derivation of reference priors relies on numerical techniques. In addition, even
once a reference prior is derived, posterior inference can be challenging.
An alternative approach, largely considered a pragmatic choice, is that of conju-
gate priors. In many cases, these priors do indeed encode substantial information that
can strongly inﬂuence the analysis of θ.8
As we see in the following sections, the
parametrization of these conjugate priors can be viewed as adding pseudo-observations
when the model class is in the regular exponential family. Thus, choices of hyperparam-
eters that add few pseudo-observations are often viewed (with the caveats mentioned
above) as weakly informative while maintaining the computational beneﬁts we describe
in Sec. 2.4.1. For the subjective Bayesian, the choice of a conjugate prior is also a prag-
matic one, and the hyperparameters allow for a simple method of tuning the distribution
to aspects of their prior belief.
■2.4.1 Conjugate Priors
The use of conjugate priors is often motivated by practical considerations. Namely,
conjugate priors allow for a computationally tractable mechanism for incorporating
new data into the posterior distribution of the parameter θ. For an arbitrary family
PΛ of prior distributions, with p(θ | λ) ∈PΛ, the integral of Eq. (2.54) and in the
denominator of Eq. (2.55) may be intractable. If, however, p(y | θ)p(θ | λ) remains in
the family PΛ where every element of PΛ has some known functional form, then the
normalization constant is automatically determined by the deﬁnition of the distributions
in that family. This motivates the following deﬁnition of conjugacy.
Deﬁnition 2.4.1. A family PΛ of prior distributions on θ ∈Θ is said to be conjugate
to a model class PΘ, with p(y | θ) ∈PΘ, if the posterior remains in the family of prior
distributions:
p(θ | y, λ) ∈PΛ
(2.58)
7This statement assumes the model provides conditionally independent observations given θ.
If
instead there were dependencies in the observations, such as the time-series models we consider in this
thesis, the reference prior might be a function of the sample size.
8We note, however, that there are special cases in which the conjugate prior and reference prior
coincide. For example, these priors coincide when θ represents the mean of a Gaussian, and that mean
is subject to second moment constraints.
In this case, both the reference and conjugate priors are
Gaussian, which can be derived utilizing the constrained maximum entropy results of Sec. 2.2.2 and
noting the equivalence of the reference prior and the maximum entropy prior for location families.

Sec. 2.4.
Incorporating Prior Knowledge
51
for all possible observations y ∈Y, likelihoods p(· | θ) ∈PΘ, and priors p(· | λ) ∈PΛ.9
Since we could simply take PΛ to be the set of all distributions, this deﬁnition alone
does not lead to the tractable inference we seek to deﬁne. Instead, one may consider
the likelihood in terms of a suﬃcient statistics t(·). From the deﬁnition of suﬃciency,
we have
p(θ | y, λ) = p(θ | t(y), λ) ∝p(t(y) | θ)p(θ | λ)
(2.59)
If t(y) is of ﬁxed, ﬁnite dimension independent of that of y (i.e., the number of data
points), the family of prior probability distributions which satisfy
p(t(y) | θ)p(θ | λ) ∝p(θ | λ′)
(2.60)
will lead to tractable inference. From this stricter deﬁnition, the Pitman-Koopman-
Darmois theorem [80] described at the end of Sec. 2.3 implies that the class of likelihoods
for which a conjugate prior family exists are those belonging to the exponential family
(regular or non-regular).
Conjugate Prior to the Regular Exponential Family
Given a model which is member of a regular exponential family, we may easily construct
the corresponding conjugate prior. Namely, for any member of E(θ; η(·), t(·), β(·)), we
can write the likelihood as:
p(y | θ) = exp{η(θ)T t(y) −α(θ) + β(y)}
(2.61)
Given a set of N i.i.d. observations, we have:
p(y1, . . . , yN | θ) = exp
(
η(θ)T
N
X
n=1
t(yn) −Nα(θ) +
N
X
n=1
β(yn)
)
(2.62)
= exp
(
η(θ)T
N
X
n=1
t(yn) −Nα(θ)
)
exp
( N
X
n=1
β(yn)
)
(2.63)
If we choose PΛ such that
PΛ = {p(· | λ) | p(θ | λ) ∝exp{tT
0 η(θ) −N0α(θ)}}
(2.64)
with λ = {t0, N0}, then
p(θ | y1, . . . yN, λ) ∝exp{t′T η(θ) −N ′α(θ)}
(2.65)
= p(θ | λ′) ∈PΛ
(2.66)
9Occasionally, we write p(y | θ) to be explicit about the domain of the distribution, whereas here
we write p(· | θ) to be clear that the distribution is a function of its argument for ﬁxed θ, not a number
resulting from an evaluation at y.

52
CHAPTER 2.
BACKGROUND
with λ′ = {t′, N ′} where
t′ = t0 +
N
X
n=1
t(yn)
(2.67)
N ′ = N0 + N.
(2.68)
We note that the conjugate prior is itself in the exponential family. Namely, the prior is
in the canonical family E(t0; I(·), η(·), −N0α(·)). As evidenced by Eq. (2.67)-Eq. (2.68),
we see that conjugate priors have the additional beneﬁt of being interpretable as simply
adding N0 pseudo-observations with a total suﬃcient statistic t0.
The likelihood of the data can then be written in terms of the normalizing constant
of a member of the exponential family:
p(y1, . . . , yN | λ) =
Z
Θ
p(ϑ | λ)p(y1, . . . , yN | ϑ)dϑ
=
Z
Θ
exp
(
η(ϑ)T
N
X
n=1
t(yn) −Nα(ϑ) +
N
X
n=1
β(y)
)
exp{tT
0 η(ϑ) −γ(λ) −N0α(ϑ)}dϑ
= exp
(
−γ(λ) +
N
X
n=1
β(y)
)
exp{t′T η(ϑ) −N ′α(ϑ)}dϑ
= exp
(
γ(λ′) −γ(λ) +
N
X
n=1
β(y)
)
,
(2.69)
where we use γ(·) to denote the log-partition function of the conjugate prior family PΛ,
and the last equality follows from identifying the integral over Θ as integrating over an
unnormalized member of PΛ with parameter λ′ = {t′, N ′}.
In the following sections, we brieﬂy outline some of the probability density and mass
functions, and the associated conjugate analysis that we utilize throughout this thesis.
All of these results may be derived using a combination of the results presented in
Sec. 2.4.1 along with manipulations similar to those in Sec. 2.2.3.
■2.4.2 Multinomial Observations
Multinomial Likelihood Distribution
Consider a random variable y on a ﬁnite sample space Y = {1, . . . , K}. Let the probabil-
ity mass function be denoted by π = [π1, . . . , πK]. The multinomial distribution [16, 51]
describes the probability of a string of N observations of y taking on values y1, . . . , yn:
p(y1, . . . , yN|π) =
N!
Q
k Nk!
Y
k
πNk
k ,
Nk ≜
X
n
δ(yn, k).
(2.70)
We use the notation δ(j, k) to indicate the discrete Kronecker delta. When K = 2, this
distribution is referred to as the binomial distribution.

Sec. 2.4.
Incorporating Prior Knowledge
53
Dirichlet Prior Distribution
The K-dimensional Dirichlet distribution [51] is the conjugate prior for the class of
K-dimensional multinomial distributions and is uniquely deﬁned by a set of hyperpa-
rameters α = [α1, . . . , αK]. The distribution has the following form:
p(π|α) = Γ(P
k αk)
Q
k Γ(αk)
Y
k
παk−1
k
,
αk > 0,
(2.71)
with Γ(·) representing the standard Gamma function. We denote this distribution by
Dir(α1, . . . , αK). When K = 2, this distribution is referred to as the beta distribution,
which we denote by Beta(α1, α2).
The ﬁrst moment of the Dirichlet distribution is
given by:
E[πi] =
αi
P
j αj
.
(2.72)
Conjugate Posterior and Predictions
The conjugacy of the Dirichlet distribution implies that, conditioned on N multinomial
observations y1, . . . , yN, the posterior distribution of π is also Dirichlet:
p(π|y1, . . . , yN, α) ∝p(π|α)p(y1, . . . , yN|π)
(2.73)
∝
K
Y
k+1
παk+Nk−1
k
∝Dir(α1 + N1, . . . , αK + NK).
(2.74)
Using the normalizing constant of the Dirichlet distribution, and substituting into
Eq. (2.54), one can derive the predictive likelihood to be:
p(y = k|y1, . . . , yN, α) = Nk + αk
N + α0
,
α0 ≜
K
X
k=1
αk.
(2.75)
■2.4.3 Gaussian Observations
Gaussian Likelihood Distribution
A Gaussian or normal distribution [51] is parameterized by a mean vector µ and
covariance matrix Σ. This distribution often arises in the natural world and can provide
a useful description of continuous-valued random variables that concentrate about a
given value and have constrained variability. The distribution is deﬁned over a sample
space Y = Rd and is written as
p(y|µ, Σ) =
1
(2π)d/2|Σ|1/2 exp

−1
2(y −µ)T Σ−1(y −µ)

.
(2.76)
We denote this Gaussian distribution by N(µ, Σ) or N(y; µ, Σ) to be explicit about
the domain.

54
CHAPTER 2.
BACKGROUND
Known Covariance: Normal Prior Distribution
For ﬁxed covariance Σ, the normal distribution is the conjugate prior on the mean
parameter µ. In the following, we assume a N(µ0, Σ0) prior for this parameter10.
Known Mean: Inverse-Wishart Prior Distribution
When only the covariance Σ is uncertain, the conjugate prior is the inverse-Wishart
distribution [51]. The d-dimensional inverse-Wishart distribution, with covariance pa-
rameter ∆and ν degrees of freedom, is given by
p(Σ|ν, ∆) =
|ν∆|
ν
2 |Σ|
ν+d+1
2
2
νd
2 π
d(d−1)
4
Qk
i=1 Γ
  ν+1−i
2
 exp

−1
2tr(ν∆Σ−1)

.
(2.77)
We denote this distributions by IW(ν, ∆). The ﬁrst moment is given by:
E[Σ] =
ν∆
ν −d −1.
(2.78)
Normal-Inverse-Wishart Prior Distribution
When both the mean and covariance are uncertain, the normal-inverse-Wishart dis-
tribution [51] is conjugate. This distribution deﬁnes a conditionally normal prior on
the mean, µ | Σ ∼N(ϑ, Σ/κ), and an inverse-Wishart distribution on the covariance,
Σ ∼IW(ν, ∆). The joint prior distribution is then deﬁned as
p(µ, Σ|κ, ϑ, ν, ∆) ∝|Σ|
ν+d+1
2
exp

−1
2tr(ν∆Σ−1) −κ
2(µ −ϑ)T Σ−1(µ −ϑ)

.
(2.79)
We will use the notation NIW(κ, ϑ, ν, ∆) to represent this distribution11.
Conjugate Posteriors and Predictions
Consider N Gaussian observations y1, . . . , yN with yi ∼N(µ, Σ). We will outline the
posterior distributions for each of the three cases listed above. More explicit details can
be found in Gelman et al. [51].
For known covariance Σ, the posterior distribution on the mean µ is given by an
updated normal distribution:
p(µ | y1, . . . , yN, Σ, µ0, Σ0)
= N
 
(Σ−1
0
+ Σ−1)−1(Σ−1
0 µ0 + Σ−1
N
X
i=1
yi), (Σ−1
0
+ Σ−1)−1
!
.
(2.80)
10In the limit as prior precision tends to zero (i.e., |Σ−1
0 | →0), the reference prior p(µ) ∝constant is
obtained.
11In the limit as κ →0, ν →−1, and |∆| →0, the often proposed “noninformative” multivariate
Jeﬀreys prior is obtained: p(µ, Σ) ∝|Σ|−(d+1)/2. Note, however, in the multivariate case, this is not
equivalent to the reference prior [13].

Sec. 2.4.
Incorporating Prior Knowledge
55
For known mean µ, the posterior distribution on the covariance Σ is given by an
updated inverse-Wishart distribution:
p(Σ|y1, . . . , yN, µ, ν, ∆) = IW
 
ν + N, ∆+ (1/ν)
N
X
i=1
(yi −µ)(yi −µ)T
!
.
(2.81)
Finally, when both the mean µ and covariance Σ are uncertain, the posterior dis-
tribution is given by an updated normal inverse-Wishart distribution:
p(µ, Σ|y1, . . . , yN, κ, ϑ, ν, ∆) = NIW(¯κ, ¯ϑ, ¯ν, ¯∆),
(2.82)
where the hyperparameter update equations are:
¯κ = κ + N
(2.83)
¯κ¯ϑ = κϑ +
N
X
n=1
yn
(2.84)
¯ν = ν + N
(2.85)
¯ν ¯∆= ν∆+
N
X
n=1
ynyT
n + κϑϑT −¯κ¯ϑ¯ϑT .
(2.86)
For the scenario where both µ and Σ are uncertain, and a conjugate normal inverse-
Wishart prior is placed on these parameters, the predictive likelihood is given by a
multivariate Student-t distribution [51]:
p(y | y1, . . . , yN, κ, ϑ, ν, ∆) = t¯ν−d+1

¯ϑ,
(¯κ + 1)¯ν
¯κ(¯ν −d + 1)
¯∆

,
(2.87)
where a standard multivariate Student-t distribution tν(ϑ, ν∆) is given by:
p(θ) =
Γ((ν + d)/2)
Γ(ν/2)νd/2πd/2 |ν∆|−1/2

1 + 1
ν (θ −ϑ)T (ν∆)−1(θ −ϑ)
(ν+d)/2
.
(2.88)
When ¯ν > (d + 1), the posterior density can be approximated by a moment-matched
Gaussian:
p(y|y1, . . . , yN, κ, ϑ, ν, ∆) ≈N

y; ¯ϑ,
(¯κ + 1)¯ν
¯κ(¯ν −d + 1)
¯∆

.
(2.89)
For analysis on the accuracy of this approximation, see [157, Section 2.2].
■2.4.4 Multivariate Linear Regression Model
Gaussian Likelihood Distribution
The normal multivariate linear regression model is one in which the observations, or
responses, yi ∈Rd can be described as a linear combination of a set of known regressors

56
CHAPTER 2.
BACKGROUND
xi ∈Rn with errors accounted for by additive Gaussian noise:
yi = xi1a1 + · · · + xinan + ei
ei ∼N(0, Σ)
(2.90)
We may combine a set of N response vectors into a matrix Y =
h
y1
· · ·
yN
i
, the re-
gressors into a matrix X =
h
x1
· · ·
xN
i
, and the noise terms into E =
h
e1
· · ·
eN
i
and compactly write:
Y = AX + E,
(2.91)
where A =
h
a1
· · ·
aN
i
is referred to as the design matrix.
Known Covariance: Matrix-Normal Prior Distribution
When the noise covariance Σ is know, the conjugate prior on the design matrix A is the
matrix-normal distribution [183]. A matrix A ∈Rd×m has a matrix-normal distribution
MN (A; M, V, K) if
p(A) =
|K|
d
2
|2πV |
m
2 e−1
2 tr((A−M)T V −1(A−M)K),
(2.92)
Equivalently,
p(vec(A)) = N(vec(M), K−1 ⊗V ),
(2.93)
where ⊗denotes the Kronecker product. From this, we see that M is the mean matrix,
and V and K−1 are related to the covariance along the rows and columns of A.
Matrix-Normal Inverse-Wishart Prior Distribution
The conjugate prior on the set of parameters A and Σ is the matrix-normal inverse-
Wishart prior. This distribution places a conditionally matrix-normal prior on A given
Σ,
A | Σ ∼MN (A; M, K, Σ)
(2.94)
and an inverse-Wishart prior on Σ,
Σ ∼IW(ν, ∆).
(2.95)
Conjugate Posteriors and Predictions
Let D = {X, Y }. The posterior distribution of {A, Σ} decomposes as
p(A, Σ | D) = p(A | Σ, D)p(Σ | D).
(2.96)

Sec. 2.5.
Graphical Models
57
The resulting posterior of A is derived in Appendix F.1 to be
p(A | Σ, D) = MN
 A; SyxS−1
xx , Σ−1, Sxx

,
(2.97)
with
Sxx = XXT + K
Syx = Y XT + MK
Syy = Y Y T + MKMT .
(2.98)
The marginal posterior of Σ is given by:
p(Σ | D) = IW
 ν + N, ∆+ Sy|x

,
(2.99)
where Sy|x = Syy −SyxS−1
xx ST
yx.
■2.5 Graphical Models
Probabilistic graphical models provide a framework for compactly encoding the con-
ditional probabilistic dependency structure of a set of random variables. For surveys
of these models and their associated inference algorithms, see [85, 103, 157, 176], with
seminal work by Pearl [134]. The framework of graphical models has allowed for the
development of many eﬃcient inference techniques such as belief propagation [104, 134],
and for advances in variational methods [176]. Such developments have provided an
ability to analyze large-scale datasets, which would not be feasible without harnessing
the sparsity in the parametrization of the full model. Additionally, the generic formu-
lation of the graphical model inference algorithms enables transfer of advances in one
domain to other domains in a straightforward manner. For example, many classical
models such as the hidden Markov model (HMM) [139] and state space model can be
formulated within the graphical model framework; the inference algorithms developed
speciﬁcally for these models—like the forward-backward algorithm [139], Viterbi decod-
ing [42], and Kalman ﬁltering [90]—can be derived as special cases of generic graphical
model inference algorithms. The development of inference algorithms for the Bayesian
nonparametric extensions of these models that we examine in this thesis is considerably
simpliﬁed by representing the models within the graphical model framework.
■2.5.1 A Brief Overview
A graph G = (V, E) consists of a set of nodes V representing the random variables of
the model and edges E containing elements (i, j) ∈E which connect a unique pair of
nodes i, j ∈V. For an undirected graph, the element (i, j) ∈E if and only if (j, i) ∈E,
which of course need not be true in a directed graph. Pictorially, a node is typically
represented by a circle, an undirected edge by a line, and a directed edge by an arrow
with the tail originating at the parent node and the head ending at the child node. See
Fig. 2.5. We primarily restrict our attention to directed graphs, since these graphs are
the most appropriate for describing the dynamical models we consider in this thesis.

58
CHAPTER 2.
BACKGROUND
■2.5.2 Directed Graphical Models
Let Γ(j) denote the set of parent nodes to a node i. This set is deﬁned by
Γ(j) = {i ∈V | (i, j) ∈E}.
(2.100)
A leaf node is one that has no children while a root node has no parents. For a directed
graph, the joint density decomposes as the product of the conditional densities for each
node i given its parents Γ(i):
p(xV) =
Y
i∈V
p(xi | xΓ(i)),
(2.101)
where we use the notation xA to denote the set {xi | i ∈A}. For an acyclic graph
(i.e., one without a directed cycle going from some node i and returning to node i,)
one can verify that Eq. (2.101) deﬁnes a valid joint density. Namely, to verify that
the density integrates to 1, one can marginalize over nodes starting at leaf nodes and
ending at root nodes. For a directed graph, the sparsity of the model parametrization
is deﬁned in terms of the relative ratio of nodes to parent nodes. As we will see, there
is not as signiﬁcant a reduction in the representational complexity and computational
complexity of inference if each node has many parents.
Whereas the joint distribution is easy to deﬁne from a directed graphical model, the
conditional independence statements encoded by the graph are somewhat challenging
to directly infer. Consider the graphical model of Fig. 2.2(d). Without conditioning on
y, random variables x and z are independent:
p(x, y, z) = p(x)p(z)p(y | x, z) ⇒p(x, z) = p(x)p(z).
(2.102)
However, these variables are not conditionally independent given y:
p(x, z|y) ∝p(x, y, z) = p(x)p(z)p(y | x, z)
̸= p(x | y)p(z | y).
(2.103)
This phenomenon is referred to as explaining away, Berkson’s paradox, or selection
bias. For example, imagine that x represented whether or not an earthquake occurred,
z whether a burglar is trying to get into the car, and y the car alarm. Earthquakes and
car robberies might be independent a priori, but upon conditioning on the car alarm
being triggered, an increase in the probability of an earthquake results in a decrease
in the probability of a burglary since the earthquake “explains away” the fact that the
alarm was triggered. For general directed graphical models, instead of writing down the
joint distribution and deriving whether the conditional independence statement is true,
one can employ an algorithm called Bayes ball [150]. One can use such an algorithm
to verify the conditional independence statements that appear in the derivations in the
appendices of this thesis.
The algorithm provides a set of eight scenarios, depicted
in Fig. 2.2, consisting of the eight possible three-node chains one can encounter in a

Sec. 2.5.
Graphical Models
59
x
y
z
x
y
z
x
y
z
x
y
z
x
y
z
x
y
z
x
y
z
x
y
z
(a)
(b)
(c)
(d)
Figure 2.2.
Pictorial representation of the Bayes ball algorithm for determining the independence
statements in a directed graphical model. There are four possible three node combinations depicted by
the graphs of (a)-(d). For each of these structures, we examine the case of marginal independence of x
and z (top) or conditional independence of x and z (bottom) given an observation y (gray node). If a
ball starting at one of the x or z nodes can pass to the other, as indicated by the straight arrows, then
those two nodes are not (conditionally/marginally) independent. If the ball bounces back, as indicated
by a set of walls and curved arrows, then the nodes are (conditionally/marginally) independent. These
rules can be linked together in various combinations to examine larger graphical models.
directed graph based on directionality of the edges and whether or not the intermediary
node is an evidence node (i.e., observed). Some of the junction scenarios are bestowed
with a set of walls that deﬂect the Bayes ball. Two random variables xi and xj associated
with nodes i and j are then deemed conditionally dependent given the random variables
xVk associated with a set of evidence nodes Vk (which may be the empty set) if a
ball starting at one node can traverse the graph to the other node based on the rules
summarized in Fig. 2.2; the random variables are conditionally independent otherwise.
Another method of determining some statements of conditional independence, and ones
extremely useful for the inference algorithms we develop, is described in the following.
Markov Blanket
For a directed graph, a node is conditionally independent of all other nodes in the graph
given its Markov blanket which consists of the node’s parents, children, and coparents.
The coparents of a given node are deﬁned as those nodes that have a child in common
with the given node. The Markov blanket concept is depicted in Fig. 2.3.
Mixture Models and Exchangeability
The version of the de Finetti theorem in Corollary 2.1.1, assuming the distribution Q
has a parameterized density q(· | λ), implies the following hierarchical Bayesian model:
p(y1, . . . , yn, θ | λ) = q(θ | λ)
n
Y
i=1
p(yi | θ),
(2.104)
which, based on Eq. (2.101), has a directed graphical representation shown in Fig. 2.4.
This ﬁgure contains both an explicit representation of the graphical model, as well as an
equivalent representation using plate notation to compactly represent the n observations

60
CHAPTER 2.
BACKGROUND
xi
Figure 2.3. Markov blanket for xt consisting of the node’s parents, coparents, and children. The node
xt is then conditionally independent of all other nodes in the graph given its Markov blanket.
λ
λ
θ
θ
y1
y2
y3
y4
yn
yi
n
Figure 2.4. Graphical representation of the hierarchical Bayesian model of n exchangeable random
variables implied by de Finetti’s theorem. Each observation is an independent draw from a density
parameterized by θ, which itself has a prior distribution with hyperparameters λ. Left: An explicit
representation of the graphical model.
Right: A compact representation using a plate to denote n
replicates of the observations yi.
yi. The fact that this set of random variables is yielded conditionally i.i.d. given θ can
be directly veriﬁed from the graphical model by using the Markov blanket concept or
the Bayes ball algorithm.
■2.5.3 Undirected Graphical Models
Many inference algorithms for directed graphical models rely on ﬁrst converting the
graph to an undirected form.
This conversion process, referred to as moralization,
“marries” any coparents by connecting them with an undirected edge. Each directed
edge is then converted into an undirected edge.
See Fig. 2.5.
In the following, we
provide a very brief sketch of the theory of undirected graphical models that we employ
in subsequent sections.
Undirected graphical models, or Markov random ﬁelds (MRF), are typically used
when there is no causal structure to the data, as in images, which instead have spatial
dependencies. Whereas the directed graphical model is easily derived from the factor-

Sec. 2.5.
Graphical Models
61
x1
x2
x3
x4
x5
x1
x2
x3
x4
x5
x1
x2
x3
x4
x5
(a)
(b)
(c)
Figure 2.5.
(a)-(b) Two directed graphical models that result in the same moralized (undirected)
graphical model shown in (c).
ization of the joint distribution, the form of an undirected model is typically formed
from a set of conditional independence statements. For an undirected graphical model,
if Vi, Vj and Vk are three disjoint sets of nodes, and if every path from a node in Vi to
a node in Vk passes through Vj, then Vj is called a separator. If the following holds for
every possible choice of such sets:
p(xVi, xVk | xVj) = p(xVi | xVj)p(xVk, | xVj),
(2.105)
then the set of random variables xV = {xi} is said to be globally Markov with respect to
the undirected graph G. Eq. (2.105) implies that each node i in an undirected graphical
model is conditionally independent of all other nodes given its set of neighbors Γ(i):
p(xi | xV\i) = p(xi | xΓ(i)),
(2.106)
where V\i denotes the set of all nodes except for node i, and Γ(i) is deﬁned just as in
Eq. (2.100) using the undirected set of edges E. This local Markov property can be used
to derive the Markov blanket property of a directed graph since the neighborhood of
a node in a moralized graph will solely contain the children, parents, and coparents of
the node in the directed graph.
It is important to note that in the conversion of a directed graph to its undirected
form, all of the conditional independence statements of the undirected graph hold for the
model of the directed graph. The converse is not necessarily true since the mapping is
many to one. Take, for example, the graphs of Fig. 2.5. The directed graph of Fig. 2.5(a)
encodes a model with x1 and x2 marginally independent; however, in the moralized
graph this result cannot be directly deduced from the graphical model and instead
depends upon the parametrization. For example, the directed graph of Fig. 2.5(c) does
not (necessarily) have x1 and x2 independent and has the same undirected graphical
representation. For the basic V-structure of Fig. 2.2(d), there is no undirected graph
that encodes the same set of independence statements. Conversely, for an undirected
graph consisting of four nodes x1, x2, x3, x4 connected in a four-cycle (i.e., each node
shares an edge with exactly two other nodes), none of the 16 possible directed graph
structures capture the same conditional independence statements. For tree-structured

62
CHAPTER 2.
BACKGROUND
directed graphical models, in which the moralized graph does not contain any loops,
the set of conditional independence statements for both graphs is identical, implying
that undirected graph inference exploits all possible conditional independencies.
In
Sec. 2.5.4, we present an eﬃcient inference algorithm for undirected, tree-structured
graphical models that harnesses the conditional independence statements implied by
the graphical model. Because these statements for a moralized directed tree are the
same as those for the directed tree, the undirected inference is equivalent to inference
in the directed tree and leverages all possible eﬃciencies. The majority of algorithms
developed in this thesis simplify to iterative inferences on tree-structured graphs.
Given a general undirected graph G, the characterization of a joint distribution
satisfying the speciﬁed Markov properties is not as straightforward as in the directed
case.
However, the Hammersley-Cliﬀord theorem [21] provides some insight.
Let C
denote the set of cliques in an undirected graph G, where a clique is deﬁned as a fully
connected subset of nodes. If a distribution can be factorized in terms of non-negative
potential functions ψc(·) deﬁned on the cliques:
p(xV) ∝
Y
c∈C
ψc(xc),
(2.107)
then the distribution is Markov with respect to G. Conversely, any strictly positive
density, p(x) > 0 for all x, which is Markov with respect to G has such a factorized
representation. Note that the full characterization of the joint distribution, a necessary
step for many inference tasks, can be quite challenging since it relies on computing a
normalization constant or partition function from an arbitrarily complicated product
of potential functions.
In some applications, it is useful to examine a pairwise Markov random ﬁeld repre-
sentation in which the clique potentials are deﬁned on the graph’s edges:
p(xV) ∝
Y
(i,j)∈E
ψij(xi, xj)
Y
i∈V
ψk(xi).
(2.108)
■2.5.4 Belief Propagation
For most graphical models we encounter in applications, the joint state space X is
too large to explicitly characterize, and thus simple inference tasks can pose signiﬁcant
challenges. For example, consider a graphical model with N nodes each taking one of K
possible values. The joint state space of such a graph is |X| = KN. Naive computation
of the posterior marginal based on a set of observations y,
p(xi | y) =
Z
XV\i
p(xV | y)dxV\i,
(2.109)
requires a sum containing KN−1 terms in the case of the K-valued graphical model.
For tree-structured graphical models, however, such global inference tasks can be
exactly and eﬃciently computed by a recursion of local computations. This belief propa-
gation algorithm harnesses the fact that in an undirected tree (such as the one depicted

Sec. 2.5.
Graphical Models
63
Figure 2.6. Left: A tree graphical model with node xi dividing the tree into disjoint subgraphs. Right:
A simple tree graph for illustrating the concepts underlying belief propagation.
in Fig. 2.6(left)), any given node separates the tree into disjoint—and thus condition-
ally independent—subgraphs given the value on the separating node. Computations
performed within the subgraphs can then be combined to form the desired posterior
marginal of the chosen node.
For the graph of Fig. 2.6(right), the joint distribution can be factorized as follows:
p(x) ∝ψ12(x1, x2)ψ23(x2, x3)ψ24(x2, x4)ψ1(x1)ψ2(x2)ψ3(x3)ψ4(x4).
(2.110)
Then, computation of the marginal p(x1) can be accomplished by combining local com-
putations resulting from distributing the integrals over the terms of the product in
Eq. (2.110):
p(x1) ∝ψ1(x1)
Z
X2
ψ12(x1, x2)ψ2(x2)
Z
X3
ψ23(x2, x3)ψ3(x3)dx3

|
{z
}
m32(x2)
·
Z
X4
ψ24(x2, x4)ψ4(x4)dx4

|
{z
}
m42(x2)
dx2.
(2.111)
Here, we have deﬁned a message mij(xj) as the result of a local integration over xi that
results in a function in terms of xj. In the above example, we would also deﬁne
m21(x1) ∝
Z
X2
ψ12(x1, x2)ψ2(x2)m32(x2)m42(x2)dx2.
(2.112)
More generally, assume we additionally have a set of evidence nodes representing a
set of observations y = {yi} that are conditioned upon during inference. We assume a
structure in which the neighborhood associated with each observation yi solely contains

64
CHAPTER 2.
BACKGROUND
node i (i.e., that of xi), and deﬁne a generic message from node j to node i as
mji(xi) =
Z
Xj

ψi(xi, yi)ψij(xi, xj)
Y
k∈Γ(j)\i
mkj(xj)

dxj.
(2.113)
That is, each outgoing message from node j is a function of |Γ(j)|−1 incoming messages
to node j. The initial messages at leaf nodes are simply given by:
mji(xi) =
Z
Xj
ψi(xi, yi)ψij(xi, xj)dxj.
(2.114)
Then, one can show that after passing all of the messages, the desired marginal can be
computed as
p(xi | y) = 1
Z ψi(xi, yi)
Y
j∈Γ(i)
mji(xi),
(2.115)
with
Z =
Z
Xi
ψi(xi, yi)
Y
j∈Γ(i)
mji(xi)dxi.
(2.116)
See [157] for a more complete derivation, and for references to classical literature. Note
that tractable propagation of messages and computation of the normalization constant
in Eq. (2.116) relies on restricted forms such as discrete or Gaussian MRFs. Otherwise,
one can consider a discretization of the continuous beliefs or one of many approximate
inference schemes such as the Monte Carlo techniques we outline in Sec. 2.8.
A node can send a valid message to a neighboring node only when it has received
valid messages from each of its other neighbors. As such, one needs to implement a
schedule when running belief propagation. One possible choice is a serial scheme in
which a single node is selected as the root of the tree.
Then, messages are passed
from the leaves to the root, followed by a pass from the root back to the leaves. See
Fig. 2.7(top). Alternatively, one can use a synchronous parallel update where every
node sends a message whenever it has received all |Γ(j)| −1 incoming messages. This
schedule starts with all leaf node passing messages, as depicted in Fig. 2.7(bottom).
Finally, a parallel scheme involving message passing from all nodes at every iteration
is also provably correct. After L such iterations, the local marginal estimates will have
incorporated information from all nodes within a distance L [2]. Thus, the algorithm
converges after a number of iterations equal to the diameter of the tree. Typically, the
messages are initialized to be uniform over Xi in the case of a discrete-valued MRF. The
parallel scheme has obvious advantages over the alternative schedules in a distributed
implementation; in a serial implementation, such a schedule is typically ineﬃcient but
is simple to code.

Sec. 2.5.
Graphical Models
65
x1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x5
x6
Figure 2.7. Graphical representation of the serial (top) and synchronous (bottom) belief propagation
scheduling schemes. Arrows indicate a message being passed. For the serial schedule, the node x1 was
selected as the root node.
In terms of computational costs, the belief propagation algorithm can lead to a dra-
matic improvement over alternative approaches for computing marginals at all nodes.
For example, if each node can take on K possible values, and we have N nodes in total, a
brute force approach to calculating the set of marginals requires O(NKN−1) operations
(KN−1 operations for computing the marginal at each of N nodes.) A naive application
of simply passing integrals through the product of the pairwise potentials in Eq. (2.108)
requires O(N(N −1)K2) operations (for each of N nodes, integrate over N −1 nodes
with two nodes per clique.) The belief propagation algorithm simply requires O(NK2)
operations by eﬃciently reusing messages.
Note that for graphs with cycles, a single node does not necessarily partition the
graph into disjoint sets, and thus Eq. (2.115) is not valid.
The junction tree algo-
rithm [104, 151] allows for exact inference in arbitrary graphs by running belief propa-
gation on the tree formed from the maximal cliques of a triangulated graph. However,
these cliques can be quite large, leading to computation intractability. In such cases, the
parallel message update form of belief propagation algorithm is often applied directly to
graphs with loops, and is termed loopy belief propagation. For graphs with large loops,
the inconsistencies or frustrations that can arise in more tightly coupled loops are less
pronounced and loopy belief propagation can yield good performance. In the Gaussian
MRF case, if loopy belief propagation converges, then it provides correct node means
(but in general gives incorrect node variances) [181]. For convergence results in discrete
and Gaussian MRFs, see [71, 113] for more details.
Many classical models, such as the hidden Markov model (HMM) or linear-Gaussian
state space model, have hand-tailored inference algorithms, such as the forward-backward

66
CHAPTER 2.
BACKGROUND
y1
y2
y3
y4
yn
z1
z2
z3
z4
zn
Figure 2.8.
Graphical representation of a hidden Markov model (HMM) over n time steps. The
latent, discrete-valued Markov process zt captures the temporal dependencies in the observations yt.
algorithm and Kalman ﬁltering and smoothing, that can be described within the more
general framework of inference on a graphical model. In Sec. 2.6-2.7, we examine the
HMM and state space models in detail and explore these connections in inference algo-
rithms.
■2.6 Hidden Markov Model
The hidden Markov model, or HMM, is a class of doubly stochastic processes based
on an underlying, discrete-valued state sequence that is modeled as Markovian [139].
Conditioned on this state sequence, the model assumes that the observations, which may
be discrete or continuous valued, are independent. The HMM has proven a powerful
model in many applied ﬁelds including speech recognition [82, 88, 139], computational
biology [100, 101, 155], machine translation [127, 128], cryptanalysis [92] and ﬁnance [17].
Let zt denote the state of the Markov chain at time t and πj the state-speciﬁc
transition distribution for state j. Then, the Markovian structure on the state sequence
dictates that for all t > 1
zt | zt−1 ∼πzt−1
(2.117)
The state at the ﬁrst time step is distributed according to an initial transition distri-
bution π0:
z1 ∼π0.
(2.118)
Given the state zt, the observation yt is conditionally independent of the observations
and states at other time steps. The observation is simply generated as
yt | zt ∼F(θzt)
(2.119)
for an indexed family of distributions F(·) where θi are the emission parameters for
state i. Assuming there exists a density associated with F(·), the resulting joint density
for n observations is then given by:
p(z1:n, y1:n) = π0(z1)p(y1 | z1)
n
Y
t=2
p(zt | zt−1)p(yt | zt),
(2.120)

Sec. 2.6.
Hidden Markov Model
67
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Time
State
1
2
3
K
1
2
3
π1
π2
π3
π11
π12 π13
π1K
π21
π22
π23
π2K
π31
π32 π33
π3K
1 2
3
K
1 2
3
K
1 2
3
K
. . .
Z+
Z+
Z+
. . .
. . .
. . .
Figure 2.9. Left: Lattice representation of an HMM state sequence sample path. Each circle represents
one of the K possible HMM states at various time steps. The highlighted circles indicate the selected
states, and the arrows represent the set of possible transitions from that state to each of the K possible
next states. The weights of these arrows convey the relative probability of the transitions encoded by
that state-speciﬁc transition distributions πj. Right: Corresponding transition distributions π1, π2, and
π3 for the lattice example.
from which we can infer a directed graphical model representation shown in Fig. 2.8.
One can use the Bayes ball algorithm of Fig. 2.2 on this HMM graphical model to verify
that an observation yt is indeed conditionally independent of all other observations and
states when given the state zt.
One can view a sample path of the state sequence as a walk through a state versus
time lattice, such as the one depicted in Fig. 2.9. A similar diagram representing all
possible transitions is often referred to as a trellis diagram.
■2.6.1 Forward-Backward Algorithm
The forward-backward algorithm [139] provides an eﬃcient message-passing scheme for
computing node marginals of interest for problems of ﬁltering p(zn | y1, . . . , yn), pre-
diction p(zn+m | y1, . . . , yn), and smoothing p(zn | y1, . . . , yN), N > n. This classical
algorithm has straightforward connections with the belief propagation algorithm of
Sec. 2.5.4. Following Rabiner [139], we deﬁne a set of forward messages
αn(zn) ≜p(y1, . . . , yn, zn)
(2.121)
and backward messages
βn(zn) ≜p(yn+1, . . . , yN | zn).
(2.122)
For the problem of ﬁltering, we simply need the forward messages since
p(zn | y1, . . . , yn) =
αn(zn)
P
z αn(z).
(2.123)

68
CHAPTER 2.
BACKGROUND
Similarly, for prediction we can utilize the Markov structure of the underlying chain to
derive that
p(zn+m | y1, . . . , yn) =
P
zn+m−1 p(zn+m | zn+m−1) · · · P
zn p(zn+1 | zn)αn(zn)
P
z αn(z)
, (2.124)
which we show is equivalent to propagating the forward message without incorporating
the missing observations yn+1, . . . , yn+m. The problem of smoothing, on the other hand,
utilizes both the forward and backward messages:
p(zn | y1, . . . , yN) = p(y1, . . . , yN | zn)p(zn)
p(y1, . . . , yN)
(2.125)
= p(y1, . . . , yn | zn)p(yn+1, . . . , yN | zn)p(zn)
p(y1, . . . , yN)
(2.126)
=
αn(zn)βn(zn)
P
z αm(z)βm(z),
(2.127)
for any m.
We derive the recursions for these forward and backward messages by harnessing the
conditional independencies implied by the graph of Fig. 2.8. The recursions presented
in this section are utilized by many of the inference algorithms described throughout
the thesis and derived in the appendices. For the forward message,
αn+1(zn+1) = p(yn+1 | zn+1)
X
zn
p(y1, . . . , yn | zn)p(zn+1 | zn)p(zn)
(2.128)
= p(yn+1 | zn+1)
X
zn
αn(zn)p(zn+1 | zn).
(2.129)
The backward recursion is similarly derived as
βn(zn) =
X
zn+1
p(yn+1 | zn+1)p(yn+2, . . . , yN | zn+1)p(zn+1 | zn)
(2.130)
=
X
zn+1
p(yn+1 | zn+1)p(zn+1 | zn)βn+1(zn+1).
(2.131)
The forward initial condition and backward ﬁnal condition are given by:
α1(z1) = p(y1, z1) = p(y1 | z1)π0(z1)
(2.132)
βN(zN) = 1.
(2.133)
To relate the forward-backward algorithm to belief propagation, we need to convert
the directed graph of Fig. 2.8 to its undirected form. In this case, the conversion simply
means exchanging directed edges for undirected ones. The relationship between the
algorithms is then readily apparent. Speciﬁcally, consider node zn with neighbors zn−1

Sec. 2.6.
Hidden Markov Model
69
and zn+1, and evidence node yn. Choosing a sequential node ordering starting at z1 for
the message-passing scheme, Eq. (2.115) simpliﬁes to:
p(zn | y1, . . . , yN) = 1
Z
X
zn
p(yn | zn)mn−1,n(zn)mn+1,n(zn),
(2.134)
with
mn−1,n(zn) =
X
zn−1
p(yn−1 | zn−1)p(zn | zn−1)mn−2,n−1(zn−1)
(2.135)
mn+1,n(zn) =
X
zn+1
p(yn+1 | zn+1)p(zn+1 | zn)mn+2,n+1(zn+1).
(2.136)
From the above, we can make the connection:
αn(zn) = p(yn | zn)mn−1,n(zn)
βn(zn) = mn+1,n(zn).
(2.137)
The prediction algorithm of Eq. (2.124) is trivial to derive within the belief propa-
gation framework. Consider an HMM graphical model up to time n and then append
a length m Markov chain with nodes xn+1, . . . , xn+m. The standard belief propagation
algorithm deﬁned on this graph is then equivalent to the method described above.
■2.6.2 Viterbi Algorithm
Given a set of HMM parameters, one might be curious about the most likely state se-
quence to have generated an observation sequence y1, . . . , yN. The Viterbi algorithm [42]
provides an eﬃcient dynamic programming approach to computing this MAP sequence:
ˆz = max
z1,...,zN π0(z1)p(y1 | z1)
N
Y
n=2
p(zn | zn−1)p(yn | zn)
(2.138)
=
min
z1,...,zN
"
−log π0(z1) −log p(y1 | z1) +
N
X
n=2
−log p(zn | zn−1) −log p(yn | zn)
#
.
Note that choosing the MAP sequence is not necessarily equivalent to choosing the
maximum node marginal independently at each node:
ˆzn = max p(zn | y1, . . . , yN).
(2.139)
Actually, such a maximum node marginal sequence may not even be a feasible sequence
for the HMM.
The Viterbi algorithm works on the dynamic programming principle that the min-
imum cost path to zn = k is equivalent to the minimum cost path to node zn−1 plus
the cost of a transition from zn−1 to zn = k (and the cost incurred by observation yn

Compute the MAP hidden Markov model state sequence ˆz1, . . . , ˆzN as follows:
1. Initialize minimum path sum to state z1 = k for each k ∈{1, . . . , K}:
S1(z1 = k) = −log π0(z1 = k) −log p(y1 | z1 = k)
2. For n = 2, . . . , N, and for each k ∈{1, . . . , K}, calculate the minimum path sum
to state zn = k:
Sn(zn = k) = −log p(yn | zn = k) + min
zn−1 {Sn−1(zn−1) −log p(zn = k | zn−1)}
and let
z∗
n−1(zn) = arg min
zn−1 {Sn−1(zn−1) −log p(zn = k | zn−1)}
3. Compute
min
z1,...,zN −log p(z1, . . . , zN | y1, . . . , yN) = min
zN SN(zN)
and set
ˆzN = arg min
zN SN(zN)
4. Iteratively set, for n ∈{N −1, . . . , 1}.
ˆzn = z∗
n(ˆzn+1)
Algorithm 1. Viterbi hidden Markov model decoding.

Sec. 2.7.
State Space Models
71
given zn = k.) These costs can be represented on edges and nodes in the trellis diagram
of Fig. 2.9. The MAP state sequence is then determined starting at node zN and re-
constructing the optimal path backwards in the trellis based on the stored calculations.
The details of the Viterbi algorithm are outlined in Algorithm 1.
Viterbi decoding reduces the computation cost to O(K2N) operations instead of
the brute force O(KN) operations. Algebraically, the Viterbi algorithm is very closely
related to the max-product (or min-sum) algorithm that operates by distributing the
maximization (or minimization) operators over the elements of the product (or sum)
in Eq. (2.138).
The max-product algorithm is equivalent to the belief propagation
recursion, except for replacing the integrals with maximizations.
■2.7 State Space Models
A state space model provides a general framework for analyzing many continuous-
valued dynamical phenomena.
The model consists of an underlying state xt ∈Rn
driven by a set of deterministic control inputs ut ∈Rm. The latent process produces a
set of observations yt ∈Rd. A stochastic state space model additionally incorporates
mutually independent and white process noise and measurement noise terms et and
wt, respectively. We assume these noise processes are zero-mean with covariances Σt
and Rt, respectively. The process noise term can be used to account for disturbances
or uncertainties in the assumed dynamical model, while the measurement noise term
models noisy observation mechanisms.
■2.7.1 Standard Discrete-Time Linear-Gaussian State Space Formulation
A discrete-time linear time-invariant (LTI) state space model is given by:
xt+1 = Axt + But + et
yt = Cxt + Dut + wt.
(2.140)
The time invariance of the model describes the fact that the parameters {A, B, C, D}
deﬁning the linear state space model do not depend on the time index t. The terms et
and wt are noise processes which satisfy:
E





x0
ei
wi






x0
ej
wj



T 
=


P0
0
0
0
Σδij
Sδij
0
ST δij
Rδij

.
(2.141)
This formulation ensures that the state sequence x1:T forms a continuous-valued, discrete-
time wide-sense Markov process [89]. Note, however, that y1:T is not marginally wide-
sense Markov although the joint process (xt, yt) is. When et and wt are Gaussian noise
processes, implying that the second order statistics fully characterize the stochastic
process, the state sequence forms a strict-sense Markov process: the state xt yields the
past, x1:t−1, and the future, xt+1:T , conditionally independent. Neither time-invariance

72
CHAPTER 2.
BACKGROUND
nor linear dynamics is necessary for the strict-sense, discrete-time Markov process re-
sult.
In this thesis, we typically assume an uncontrolled (i.e., ut = 0) model:
xt+1 = Axt + et
yt = Cxt + wt.
(2.142)
The graphical model for this process is equivalent to that of the hidden Markov
model depicted in Fig. 2.8.
■2.7.2 Vector Autoregressive Processes
Many dynamical processes can be modeled as autoregressive (AR). That is, the observa-
tions are a noisy linear combination of some ﬁnite set of past observations plus additive
white noise.
An order r vector AR process, denoted by VAR(r), with observations
yt ∈Rd, can be deﬁned as
yt =
r
X
i=1
Aiyt−i + et
et ∼N(0, Σ).
(2.143)
Here, the observations depend linearly on the previous r observation vectors. We refer
to {A1, . . . , Ar} as the set of lag matrices. Every VAR(r) process can be described in
state space form by, for example, the following transformation:
xt =


A1
A2
. . .
Ar
I
0
. . .
0
...
...
...
...
0
. . .
I
0


xt−1 +


I
0
...
0


et
yt =
h
I
0
. . .
0
i
xt.
(2.144)
Note that there are many such equivalent minimal state space representations that
result in the same input-output relationship [34, 110], where minimality implies that
there does not exist a realization with lower state dimension (see Sec. 2.7.4 for further
details). On the other hand, not every state space model may be expressed as a VAR(r)
process for ﬁnite r [6]. We can thus conclude that considering a class of state space
models with state dimension r · d and arbitrary dynamic matrix A subsumes the class
of VAR(r) processes.
■2.7.3 Switching Linear Dynamic Systems
Many complex dynamical phenomena cannot be adequately described by a single lin-
ear dynamical model. However, the dynamics can often be approximated as switches
between a set of linear systems in some probabilistic fashion based on an underlying,
discrete-valued mode of the system. This class of hybrid systems is commonly referred to
as a jump-linear system. When one takes the latent mode of the system to be a discrete-
time Markov process, this model is typically referred to as a Markov jump-linear system

Sec. 2.7.
State Space Models
73
(MJLS) [30] or switching linear dynamic system (SLDS). Switched aﬃne and piecewise
aﬃne (PWA) models, which we do not consider in this thesis, alternatively take the
mode to be a function of the continuous state [130].
The SLDS we consider in this thesis can be described by:
zt ∼πzt−1
xt = A(zt)xt−1 + et(zt)
yt = Cxt + wt,
(2.145)
where zt represents the mode of the system at time t, and is deﬁned by a discrete-valued
Markov process with transition distributions πj. Here, we assume the process noise is
mode-speciﬁc:
et(zt) ∼N(0, Σ(zt))
(2.146)
while the measurement mechanism is not. This assumption could be modiﬁed to allow
for both a mode-speciﬁc measurement matrix C(zt) and noise wt(zt) ∼N(0, R(zt)).
However, such a choice is not always necessary nor appropriate for certain applica-
tions and can have implications on the identiﬁability of the model, as is discussed in
Chapter 4.
We similarly deﬁne a switching VAR(r) process by
zt ∼πzt−1
yt =
r
X
i=1
A(zt)
i
yt−i + et(zt).
(2.147)
Note that the underlying state dynamics of the SLDS are equivalent to a switching
VAR(1) process.
Both the SLDS and the switching VAR process can be viewed as extensions of the
standard HMM where instead of having conditionally independent observations given
the mode sequence, the system has conditionally linear dynamics. See Fig. 2.10 for
graphical model representations, and compare to that of the HMM in Fig. 2.8.
■2.7.4 Stochastic Realization Theory
The models we have described so far assume that the dynamic parameters are known
and speciﬁed. The ﬁeld of stochastic realization theory addresses the issue of construct-
ing a model that realizes a stochastic process with a given set of statistical properties.
One example of this is determining from a set of zero-mean, wide-sense stationary ob-
servations whether there exists an LTI state space model driven by white noise that
produces the same second order moments, and if so, ﬁnding such a model. A ques-
tion then arises as to the dimension of the underlying state of such a model, and more
speciﬁcally, ﬁnding the minimal such dimension. The theory is developed assuming
that the statistics of the process are available. In practice, however, applying the ideas

74
CHAPTER 2.
BACKGROUND
y1
y2
y3
y4
yn
z1
z2
z3
z4
zn
y1
y2
y3
y4
yn
z1
z2
z3
z4
zn
x1
x2
x3
x4
xn
(a)
(b)
Figure 2.10.
Graphical models for the (a) switching vector autoregressive (VAR) process and (b)
switching linear dynamical system (SLDS) over n time steps.
For both models, a discrete-valued
Markov process zt dictates the linear dynamical model at time t. For an order r switching VAR process
(shown here for r = 2), the conditionally linear dynamics are completely determined by the previous r
observations. The SLDS instead relies on a latent, continuous-valued Markov state xt to capture the
history of the dynamical process.
of stochastic realization relies on approximations based on estimates of a ﬁnite set of
these statistics (e.g., correlations over a ﬁxed number of lags) from a ﬁnite set of data.
Such practices fall under the category of system identiﬁcation. Model order reduction,
on the other hand, addresses the problem of ﬁnding a lower order approximation to a
given state space model. In the following, we review some of the methodologies rele-
vant to the models we consider. A more detailed presentation of this material can be
found in [107]. We restrict our attention here to linear dynamical systems, with a brief
overview of stochastic realization for the SLDS presented in Chapter 4.
Assume we are given a set of random variables y−∞:∞. Let
Y+ =
h
yt
yt+1
yt+2
. . .
i
(2.148)
Y+ =
h
yt−1
yt−2
yt−3
. . .
i
.
(2.149)
Eq. (2.142) implies that the state of any stochastic realization of this process must yield
the past and present conditionally independent:
p(y−∞,∞| xt) = p(Y−| xt)p(Y+ | xt).
(2.150)
Exploiting this Markov property, it can be shown that the size of the minimal state
dimension that fully characterizes the second-order statistics of Y+ and Y−must be
exactly the dimension of their cross-covariance:
H ≡E[Y+Y T
−] =


Λ(1)
Λ(2)
Λ(3)
. . .
Λ(2)
Λ(3)
Λ(4)
. . .
Λ(3)
Λ(4)
Λ(5)
. . .
...
...
...
...


.
(2.151)

Sec. 2.7.
State Space Models
75
If we have a realization of the form given in Eq. (2.142), we see that
Λ(τ) ≡E[yt+τyT
t ] =
(
CPxCT + R,
τ = 0;
CAτPxCT ≡CAτ−1G,
τ > 0,
(2.152)
where Px is the steady-state covariance satisfying (assuming A stable),
E[xtxT
t ] ≡Px = APxAT + Σ,
(2.153)
and we may rewrite the Hankel matrix as
H =


C
CA
· · ·
CAd−1


h
G
AG
· · ·
Ad−1G
i
≡OR.
(2.154)
Noting that O and R correspond to extended observability and reachability matrices,
respectively, for the system triplet (A, C, G), we may utilize the results from determin-
istic realization theory. These results inform us that a realization is minimal if and only
if (A, C) is observable and (A, G) is reachable. Such a minimal realization is unique
up to a change of basis in the coordinates of the state. Namely, we can deﬁne a set of
equivalent minimal realizations as:
M(A, C, G) = {(CT, T −1AT, T −1G) | T invertible similarity matrix}.
(2.155)
For any such (A, C, G) to be a stochastic realization, it must also satisfy the following
set of positive real equations:
Px = APxAT + Σ ⪰0
(2.156)
Λ(0) = CPxCT + R ⪰0
(2.157)
G = APxCT .
(2.158)
The Kalman-Yakubovich-Popov (or positive real) lemma [91, 189] states that there ex-
ists a Px satisfying the positive real equations if and only if the covariance Λ(τ) is a
positive semideﬁnite function.
In practice, one does not have an inﬁnite collection of correlations {Λ(τ)}∞
τ=1, but
rather a ﬁnite set of observations y1:T or correlations {Λ(τ)}T
τ=1 from which one wants
to produce a realization of the state space model. This necessarily leads to approximate
methods since the above rank and factorization methods cannot be exactly applied to a
ﬁnite Hankel matrix using the covariance estimates produced by y1:T . Instead, there is a
wealth of literature that attempts to ﬁnd a low-rank approximation to the ﬁnite Hankel
matrix, such as through principle component analysis or more sophisticated balanced
truncation techniques that aim to be coordinate-invariant. Note that such realizations
need not satisfy the positive real equations and care must be taken to ensure that the
state space realization is valid.

76
CHAPTER 2.
BACKGROUND
■2.7.5 Kalman Filtering and Smoothing
The Kalman ﬁlter [90] provides a recursive algorithm for estimating the underlying
state of a linear-Gaussian state space model given a set of observations and ﬁxed model
parameters. Classically, the recursion is derived by exploiting the orthogonality prin-
ciples that arise from preprocessing the observations with a whitening ﬁlter, and then
building the linear least-squares estimate of the state from this white sequence. For a
detailed derivation, and for numerous properties and extensions of the standard Kalman
ﬁlter, see [89]. For the purposes of this background chapter, we simply present the al-
gorithmic outline and then build the connection with a message passing formulation.
Speciﬁcally, consider the state space model of Eq. (2.142). In Algorithm 2, we outline
the standard Kalman ﬁlter for computing the linear least-squares estimate ˆxt|t of xt
given y1, . . . , yt.
1. Initialize ﬁlter with
P0|0 = P0
ˆx0|0 = 0
2. Working forwards in time, for each t ∈{1, . . . , T}:
(a) Compute
Kt = Pt|t−1C(CPt|t−1C + R)−1
(b) Predict
ˆxt|t−1 = Aˆxt−1|t−1
Pt|t−1 = APt−1|t−1AT + Σ
(c) Update
ˆxt|t = ˆxt|t−1 + Kt(yt −C ˆxt|t−1)
Pt|t = Pt|t−1 −KtCPt|t−1
Algorithm 2. Kalman ﬁlter recursion for an LTI system.
Other inference tasks for which closed-form solutions exist include:
ﬁxed-point
smoothing to form an estimator ˆxt∗|T for t∗ﬁxed and T > t∗; ﬁxed-lag smoothing
to form ˆxT−k|T for k ﬁxed; and ﬁxed-interval smoothing to form ˆxt|T for T ﬁxed and for
all t < T. For each of these problems, there are multiple possible algorithmic solutions.
See [89] for more details.
Relationship to Belief Propagation
As with the forward-backward algorithm, the Kalman ﬁlter and the Rauch-Tung-
Striebel variant of the Kalman smoother can be related to the belief propagation algo-

Sec. 2.7.
State Space Models
77
rithm of Sec. 2.5.4. Similar derivations to the ones presented in this section will prove
useful in the derivations in Appendices D and E.
For any state space model (i.e., model with a graphical representation as in Fig. 2.8),
the following recursions exist:
p(xt | y1, . . . , yt) = p(y1, . . . , yt | xt)
p(y1, . . . , yt)
∝p(yt | xt)p(xt | y1, . . . , yt−1)
(2.159)
p(xt+1 | y1, . . . , yt) =
Z
Xt
p(xt+1, xt | y1, . . . , yt)dxt
(2.160)
∝
Z
Xt
p(xt+1 | xt)p(xt | y1, . . . , yt)dxt.
(2.161)
Assuming a linear-Gaussian state space model as in Eq. (2.142), these operations can
be evaluated in closed form and simply correspond to operations on Gaussian mean and
covariance parameters. Eq. (2.159) yields the update step of the Kalman ﬁlter, while
Eq. (2.161), commonly referred to as the Chapman-Kolmogorov equation, yields the
predict step. We will utilize these generic formulations in our derivation of the Kalman
message passing algorithm.
We start by deﬁning a forward message in a similar manner to that of the HMM
forward-backward algorithm:
αt+1(xt+1) =
Z
Xt
p(xt+1 | xt)αt(xt)dxt

· p(yt+1 | xt+1).
(2.162)
From Eq. (2.161), we can directly infer that, as with the HMM,
αt(xt) ∝p(xt | y1, . . . , yt).
(2.163)
Assume αt(xt) ∼N −1(xt; θf
t|t, Λf
t|t), where N −1(θ, Λ) denotes a Gaussian N(µ, Σ)
in information form with Λ = Σ−1 and θ = Σ−1µ. We may write the integrand of
Eq. (2.162) in the following quadratic form:
p(xt+1|xt) ∝exp

−1
2(xt+1 −Axt)T Σ−1(xt+1 −Axt)

(2.164)
∝exp


−1
2
"
xt+1
xt
#T "
Σ−1
−Σ−1A
−AT Σ−1
AT Σ−1A
# "
xt+1
xt
#


αt(xt) ∝exp

−1
2(xt −θf
t|t)T Λf
t|t(xt −θf
t|t)

(2.165)
∝exp


−1
2
"
xt+1
xt
#T "
0
0
0
Λf
t|t
# "
xt+1
xt
#
+
"
xt+1
xt
#T "
0
θf
t|t
#

.

78
CHAPTER 2.
BACKGROUND
Combining these terms, the integrand is given by:
p(xt+1|xt)αt(xt) ∝exp

−1
2(xt −θf
t|t)T Λf
t|t(xt −θf
t|t)

(2.166)
∝exp

−1
2
"
xt+1
xt
#T "
Σ−1
−Σ−1A
−ATΣ−1 AT Σ−1A + Λf
t|t
# "
xt+1
xt
#
+
"
xt+1
xt
#T "
0
θf
t|t
# 
.
Marginalizing over xt+1 using the standard Gaussian marginalization identity
Z
X2
N −1
 "
x1
x2
#
;
"
θ1
θ2
#
,
"
Λ11
Λ12
Λ21
Λ22
#!
dx2 = N −1(x1; θ1 −Λ12Λ−1
22 θ2, Λ11 −Λ12Λ−1
22 Λ21),
we obtain:
Z
Xt
p(xt+1 | xt)αt(xt)dxt ∝N −1(xt+1; θt,t+1, Λt,t+1),
(2.167)
where
θt,t+1 = Σ−1A(AT Σ−1A + Λf
t|t)−1θf
t|t
Λt,t+1 = Σ−1 −Σ−1A(AT Σ−1A + Λf
t|t)−1AT Σ−1.
(2.168)
We can write our likelihood term as:
p(yt+1|xt+1) ∝exp

−1
2(yt+1 −Cxt+1)T R−1(yt+1 −Cxt+1)

(2.169)
∝exp

−1
2xT
t+1CTR−1Cxt+1 + xT
t+1CT R−1yt+1

(2.170)
To combine these terms, we simply add the information parameters:
αt+1(xt+1) ∝exp

−1
2xT
t (Λt,t+1 + CT R−1C)xt + xT
t (θt,t+1 + CTR−1yt+1)

.
(2.171)
Thus,
θf
t+1|t+1 = θt,t+1 + CT R−1yt+1
= Σ−1A(AT Σ−1A + Λf
t|t)−1θf
t|t + CTR−1yt+1
Λf
t+1|t+1 = Λt,t+1 + CTR−1C
= Σ−1 −Σ−1A(AT Σ−1A + Λf
t|t)−1AT Σ−1 + CTR−1C,
(2.172)

Sec. 2.7.
State Space Models
79
1. Initialize ﬁlter with
Λf
0|0 = P −1
0
θf
0|0 = 0
2. Working forwards in time, for each t ∈{1, . . . , T}:
(a) Compute
Mt = A−T Λf
t|tA−1
Jt = Mt(Mt + Σ−1)−1
Lt = I −Jt.
(b) Predict
Λt−1,t = Lt−1Mt−1LT
t−1 + Jt−1Σ−1JT
t−1
θt−1,t = Lt−1A−T θf
t−1|t−1
(c) Update
Λf
t|t = Λt−1,t + CTR−1C
θf
t|t = θt−1,t + CT R−1yt
Algorithm 3. Stable forward information form Kalman ﬁlter recursion.
which is equivalent to a standard update-to-update Kalman ﬁlter in information form
with ˆxt|t = (Λf
t|t)−1θf
t|t and Pt|t = (Λf
t|t)−1. An equivalent form of this recursion (as-
suming A is invertible) is given by Algorithm 3.
We now examine the backward recursion. As in the HMM forward-backward algo-
rithm, let
βt(xt) = p(yt+1, . . . , yT | xt),
(2.173)
and recursively deﬁne
βt(xt) ∝
Z
Xt+1
p(xt+1 | xt)p(yt+1 | xt+1)βt+1(xt+1)dxt+1.
(2.174)
Assuming βT (xt) ∼N −1(xT ; θb
T|T+1, Λf
T|T+1), an analogous derivation to that of the

80
CHAPTER 2.
BACKGROUND
forward recursion provides the following backwards recursion:
θb
t−1|t = AT Σ−1(Σ−1 + CT R−1C + Λb
t|t+1)−1(CT R−1yt + θb
t|t+1)
Λb
t−1|t = AT Σ−1A −AT Σ−1(Σ−1 + CTR−1C + Λb
t|t+1)−1Σ−1A
(2.175)
As in the HMM forward-backward algorithm, the posterior marginal is computed
by combining the forward and backward messages, and then normalizing:
p(xt | y1, . . . , yT ) ∝αt(xt)βt(xt).
(2.176)
Replacing αt(xt) and βt(xt) by their deﬁnitions in terms of the information parameters
θf
t|t, Λf
t|t, θb
t|t+1, Λb
t|t+1, we have:
p(xt | y1, . . . , yT ) ∝N −1(xt; θf
t|t, Λf
t|t)N −1(xt; θb
t|t+1, Λb
t|t+1)
(2.177)
∝N −1(xt; θf
t|t + θb
t|t+1, Λf
t|t + Λb
t|t+1).
(2.178)
The connections between Kalman ﬁltering and smoothing and belief propagation follow
exactly as they did for the HMM. See Eq. (2.137), replacing zn with xt and using the
deﬁnitions of αt(·) and βt(·) above.
■2.8 Markov Chain Monte Carlo
As we have seen in Sec. 2.1, Bayesian inference (e.g., prediction or computation of
posterior parameter estimates) relies on integration with respect to some potentially
high-dimensional probability distribution12. We will generically denote this distribution
by π. Except in the simplest cases, such integrals cannot be computed in closed form.
Markov chain Monte Carlo (MCMC) methods [57, 142] provide a class of algorithms
that produce estimates of the desired integral based on iterative sampling, combining
Monte Carlo integration with samples from a specially constructed Markov chain. The
key feature of these methods is that the sampling procedure does not rely on sampling
from the distribution π, which is assumed to have an arbitrarily complex form.
■2.8.1 Monte Carlo Integration
The ﬁrst step in understanding Monte Carlo integration involves formulating the desired
integral as an expectation under the distribution π:
Z
X
f(x)π(x)dx = Eπ[f(x)].
(2.179)
The Strong Law of Large Numbers [46] informs us that the sample average based on
a set of independent samples xi ∼π, i = 1, . . . , n, converges almost surely to the
12Frequentists rely on integration for inference as well, and the techniques described in this section
are equally well-suited to such problems. However, per the theme of this thesis, we will focus on the
Bayesian framework.

Sec. 2.8.
Markov Chain Monte Carlo
81
true expectation under π. Thus, we may consider the following approximation, which
becomes arbitrarily precise for n suﬃciently large:
Eπ[f(x)] ≈1
n
n
X
i=1
f(xi).
(2.180)
The assumption of having i.i.d. samples xi can be relaxed by examination of ergodic
theory [142].
The focus of MCMC methods is to develop an ergodic Markov chain
with stationary distribution π, which we refer to as the target distribution, such that a
sample path from this chain can be used to form the above estimate.
■2.8.2 The Metropolis-Hastings Algorithm
The Metropolis-Hastings algorithm provides a generic method for constructing an er-
godic Markov chain, relying solely on deﬁning a valid proposal distribution q(· | ·) and
evaluation of the target distribution π up to a normalization constant. It is assumed
evaluating π(x) is easy, but sampling from this distribution is challenging. The weak
conditions the proposal distribution must satisfy are described in Eq. (2.189)-Eq. (2.191)
to follow. The Metropolis-Hastings algorithm is outlined in Algorithm 4.
Given a previous sample x(t−1):
1. Sample x′ ∼q(x′ | x(t−1)).
2. Determine the acceptance probability:
ρ(x′ | x(t−1)) = min
(
π(x′)q(x(t−1) | x′)
π(x(t−1))q(x′ | x(t−1)), 1
)
.
3. Sample
x(t) ∼ρ(x′ | x(t−1))δx′ +

1 −ρ(x′ | x(t−1))

δx(t−1),
where δx is a Dirac mass at x.
Algorithm 4. Metropolis-Hastings algorithm.
The acceptance probability ρ(y | x) is deﬁned only when π(x) > 0. However, as
long as π(x(0)) > 0, the chain deﬁned in Algorithm 4 will have π(x(t)) > 0 for all t. We
use the convention that ρ(y | x) is 0 if both π(x) and π(y) are zero.
To analyze the properties of the Markov chain deﬁned by the Metropolis-Hastings
algorithm, it is useful to examine a condition known as detailed balance.
Proposition 2.8.1. Let K(y | x) = p(xn+1 = y | xn = x) be the transition distribution
or transition kernel for a given Markov chain. If K(y | x) satisﬁes detailed balance:
K(y | x)π(x) = K(x | y)π(y),
(2.181)

82
CHAPTER 2.
BACKGROUND
then the chain deﬁned by this transition kernel has stationary distribution π. A Markov
chain satisfying detailed balance is said to be reversible with respect to π.
Proof. Given a chain satisfying detailed balance,
Z
K(y | x)π(x)dx =
Z
K(x | y)π(y)dx = π(y)
Z
K(x | y)dx = π(y),
(2.182)
implying that π is indeed a stationary distribution of the Markov chain.
■
It is straightforward to show that the transition kernel deﬁned by Algorithm 4
satisﬁes detailed balance. With probability ρ(y | x), the chain transitions from x to a
sample y ∼q(y | x); otherwise, the chain transitions back to x. Thus, the kernel is a
weighted mixture of the proposal distribution and a Dirac mass at x:
K(y | x) = ρ(y | x)q(y | x) +

1 −
Z
ρ(z | x)q(z | x)dz

δx.
(2.183)
To check the detailed balance condition of Eq. (2.181), we analyze each term of the
transition kernel separately. The Dirac mass satisﬁes the following equality trivially:

1 −
Z
ρ(z | x)q(z | x)dz

δxπ(x) =

1 −
Z
ρ(z | y)q(z | y)dz

δyπ(y).
(2.184)
We derive the equivalence of the other term in the resulting detailed balance equation
as:
ρ(y | x)q(y | x)π(x) =
(
q(y | x)π(x),
q(y | x)π(x) < q(x | y)π(y);
q(x | y)π(y),
otherwise.
(2.185)
= min (q(y | x)π(x), q(x | y)π(y))
(2.186)
= min (q(x | y)π(y), q(y | x)π(x))
(2.187)
= ρ(x | y)q(x | y)π(y).
(2.188)
Therefore, as long as
[
x∈supp π
supp q(· | x) ⊃supp π,
(2.189)
the chain deﬁned by the Metropolis-Hastings algorithm (Algorithm 4) will satisfy de-
tailed balance and thus deﬁne a Markov chain with π a stationary distribution. To prove
that the Markov chain indeed converges to π (i.e., π is the unique invariant distribution
for this chain and this distribution is reached from all initial states), we invoke some
mild conditions under which the chain is both aperiodic and Harris recurrent [142].
Jointly, these conditions imply ergodicity.

Sec. 2.8.
Markov Chain Monte Carlo
83
A suﬃcient condition for the Metropolis-Hastings Markov chain to be aperiodic is
for events x(t) = x(t−1) to occur with some positive probability. That is,
P[π(x(t−1))q(y | x(t−1)) ≤π(y)q(x(t−1) | y)] < 1.
(2.190)
Furthermore, if
q(y | x) > 0
∀(x, y) ∈X × X,
(2.191)
then the Metropolis-Hastings Markov chain is irreducible. It can be shown (see Lemma
7.3 of [142]) that an irreducible Metropolis-Hastings chain is also Harris recurrent. Thus,
any Metropolis-Hastings algorithm deﬁned with a proposal distribution that satisﬁes
the conditions of Eq. (2.189)-Eq. (2.191) will eventually produce samples from the
stationary distribution π and
lim
T→∞
1
T
T
X
t=1
f(x(t)) =
Z
X
f(x)π(x)dx
a.e. −π.
(2.192)
Discussion on the rate of convergence to the stationary distribution can be found in [57,
142]. In general, this burn-in period is challenging to quantify, except by conservative
bounds, and is especially challenging to assess in high-dimensional spaces. Convergence
can be greatly aﬀected by the initialization of the Markov chain, and in practice, it is
common to run multiple chains from diﬀerent initializations [50]. Multimodal target
distributions with low valleys between the modes can lead to poorly mixing chains that
stay in one region of the state space for long periods of time.
Cleverly engineered
proposal distributions, such as through tempering [125], can play a signiﬁcant role in
the success of a sampling algorithm.
■2.8.3 Gibbs Sampling
The Gibbs sampler [57, 142] is a special case of the Metropolis-Hastings algorithm in
which the proposed sample is always accepted. The Gibbs sampler for n random vari-
ables (x1, x2, . . . , xn) is summarized in Algorithm 5, from which we see that in order
to sample from the full joint distribution on n random variables, it is suﬃcient to
iteratively sample from each of the possibly univariate conditional distributions. As
discussed in Sec. 2.5.2, a node in a directed graph is conditionally independent of all
other nodes given its Markov blanket.
Therefore, in the case of sparse graphs, the
conditional density from which we are sampling is dependent only on a small subset of
the other sampled nodes. We note that, as opposed to Metropolis-Hastings, the Gibbs
sampler requires knowledge of the full conditional distributions and an ability to sample
from them. Additionally, this algorithm is only applicable to models with at least two
random variables.

84
CHAPTER 2.
BACKGROUND
Given a previous sample x(t−1) = (x(t−1)
1
, . . . , x(t−1)
n
), generate:
1. x(t)
1
∼p1(x1 | x(t−1)
2
, . . . , x(t−1)
n
).
2. x(t)
2
∼p2(x2 | x(t)
1 , x(t−1)
2
, . . . , x(t−1)
n
).
...
n. x(t)
n ∼pn(xn | x(t)
1 , . . . , x(t)
n−1).
Algorithm 5. Multi-stage Gibbs sampling algorithm.
To ensure a reversible chain13, which leads to a Central Limit Theorem result for
the estimator of Eq. (2.180) [142, 167], the reversible Gibbs sampler performs a sweep
at every iteration from x1 to xn followed by a sweep in the reverse ordering back to
x1. Another variant on the standard Gibbs sampler of Algorithm 5, as proposed by Liu
et al. [109], is to choose a random ordering for a single sweep—such an algorithm can
lead to improved rates of convergence.
To make the connection between Gibbs sampling and Metropolis-Hastings, consider
the proposal distribution at the ith step of the sampler in Algorithm 5:
qi(x′ | x) = pi(x′
i | x1, . . . , xi−1, xi+1, . . . , xn)
· δ(x1,...,xi−1,xi+1,...,xn)(x′
1, . . . , x′
i−1, x′
i+1, . . . , x′
n),
(2.193)
where x = (x1, . . . , xn) and x′ = (x′
1, . . . , x′
n). That is, sample x′
i from its Markov
kernel and set each x′
j, j ̸= i, equal to xj. For this proposal, the acceptance probability
is given by:
ρi(x′ | x) = min
π(x′)qi(x | x′)
π(x)qi(x′ | x) , 1

= min
pi(x′
i | x1, . . . , xi−1, xi+1, . . . , xn)pi(xi | x1, . . . , xi−1, xi+1, . . . , xn)
pi(xi | x1, . . . , xi−1, xi+1, . . . , xn)pi(x′
i | x1, . . . , xi−1, xi+1, . . . , xn), 1

= 1,
implying that every proposed sample is accepted. Thus, one can interpret the full set of
n steps of Algorithm 5 as a composition of n Metropolis-Hastings steps with Markovian
kernels in which each proposal has acceptance probability equal to 1. If one were to
treat all n steps of Algorithm 5 as a particular Metropolis-Hastings algorithm, the global
acceptance probability of x′ = (x′
1, . . . , x′
n) is typically not equal to 1. However, with
13A non-reversible Markov chain implies that the chain does not satisfy the detailed balance condition
of Eq. (2.181). However, since detailed balance is merely a suﬃcient condition for π to be a stationary
distribution, this does not preclude π from being the stationary distribution of the Gibbs chain.

Sec. 2.8.
Markov Chain Monte Carlo
85
the direct form of Algorithm 5 in which each step is a Metropolis-Hastings proposal
that is accepted (rather than modifying the algorithm to be a proposal distribution for
a sample x′), the convergence properties must be assessed diﬀerently than they were for
the Metropolis-Hastings algorithm. In particular, each transition step does not satisfy
the suﬃcient conditions of Eq. (2.190)-Eq. (2.191) so the resulting Markov chain is not
necessarily irreducible. (Actually, each individual Markov kernel is never irreducible as
it is constrained to a lower dimensional subspace.)
The following proposition states that, based on a condition of ergodicity, the Markov
chain deﬁned by Algorithm 5 indeed has stationary distribution π, as desired. We then
provide a suﬃcient condition for ergodicity.
Proposition 2.8.2. If (x(t)) is ergodic, then π is the stationary distribution of the
chain deﬁned in Algorithm 5.
Proof. The kernel of the chain (x(t)) is
K(x′ | x) = p1(x′
1 | x2, . . . , xn)p2(x′
2 | x′
1, x3, . . . , xn) · · · pn(x′
n | x′
1, . . . , x′
n−1). (2.194)
Using this kernel, one can show that
P(x′ ∈A) =
Z
IA(x′)K(x′ | x)π(x)dx′dx
=
Z
A
π(x′)dx′,
implying that π is the stationary distribution. See [142] for further details.
■
We now state a condition for the transition kernel deﬁned by Algorithm 5, which,
if satisﬁed, implies the ergodicity of the chain. Weaker conditions based on positivity
constraints on the transition kernel exist (see Theorem 10.8 of [142]), but are more
challenging to verify.
Proposition 2.8.3. If the transition kernel associated with Algorithm 5 (see Eq. (2.194))
is absolutely continuous with respect to the dominating measure, the resulting chain is
Harris recurrent.
Proof. For a proof of this result, see [167].
■
If one of the Gibbs steps is replaced with a Metropolis-Hastings step (i.e., a hy-
brid sampler), absolute continuity is lost and further analysis is necessary to conclude
convergence of the resulting chain. Another important consideration is the fact that
the developed Gibbs sampler does not apply to changing numbers of parameters since
such changes imply irreducibility of the resulting chain.
In such variable-dimension
cases, one can instead appeal to reversible jump MCMC [60]. Both a hybrid sampler
and reversible jump MCMC are employed in an algorithm developed in Chapter 5 (see
Sec. 5.2 in particular.) However, the analysis of these techniques is beyond the scope

86
CHAPTER 2.
BACKGROUND
of this background chapter, and we refer the interested reader to Chapters 10 and 11
of [142].
We conclude by noting that a two-stage Gibbs sampler (n = 2) has special conver-
gence properties that do not apply in the general case of Algorithm 5. Some of these
special properties arise from the fact that in the two-stage sampler, each subchain is
also Markov allowing for component-wise study, which does not carry over to the more
general case. For this two-stage sampler, instead of using the notation x1, . . . , xn as
before, we use x and y to denote the two random variables of the model. An outline of
the two-stage sampler is presented in Algorithm 6.
Given a previous sample x(t−1):
1. Sample y(t) ∼py|x(y | x(t−1)).
2. Sample x(t) ∼px|y(x | y(t)).
Algorithm 6. Two-stage Gibbs sampling algorithm.
From the construction of the sampler in Algorithm 6, it is clear that (x(t), y(t))
forms a Markov chain.
Interestingly, so does each subsequence (x(t)) and (y(t)), as
previously suggested. The transition kernel for the sequence of random variables (x(t)),
for example, is
K(x′ | x) =
Z
px|y(x′ | y)py|x(y | x)dy,
(2.195)
and the marginal distribution px(·) is indeed the stationary distribution of this chain:
px(x′) =
Z
px|y(x′ | y)py(y)dy
=
Z
px|y(x′ | y)
Z
py|x(y | x)px(x)dxdy
=
Z Z
px|y(x′ | y)py|x(y | x)dy

px(x)dx
=
Z
K(x′ | x)px(x)dx.
(2.196)
Based on such results, interleaving Markov chain results and the duality principle [36]
apply.
See Robert and Casella [142] for more details and for a full analysis of the
convergence properties of the two-stage Gibbs sampler.
■2.8.4 Auxiliary, Blocked, and Collapsed Gibbs Samplers
In Sec. 2.8.3, we have assumed that it is feasible to sample from the full conditional
distributions of the variables of interest, and we have assumed that this sampling has

Sec. 2.8.
Markov Chain Monte Carlo
87
occurred by dividing the n random variables into n sampling stages. In this section, we
explore several Gibbs sampling variants: auxiliary, blocked, and collapsed. In the aux-
iliary variable sampler, a set of auxiliary variables, which are not the random variables
of interest in the inference, are added to the sampling procedure in order to enable
closed-form sampling of the variables of interest. In some cases, one can improve the
eﬃciencies of the sampler by block sampling multiple random variables jointly. Finally,
collapsed Gibbs sampling involves the analytic marginalization of random variables
from the model and then sampling the remaining variables from the reduced-order con-
ditional distributions (assumed to maintain a simple, analytic form.) Each of these
methods is summarized below.
Auxiliary Variable Sampling
There are some cases in which augmenting the random variables of interest x with
auxiliary variables or completion variables14 allows for closed form conditional distri-
butions for the augmented set y = {x, z}. Note that although (y(t)) forms a Markov
chain (by construction), the subchain (x(t)) need not. However, the subchain (x(t)) still
converges to the the marginal distribution px(·) (see Theorem 10.6 of [142].) In the
standard mixture model we explore in Example 2.8.1, the completion variables z have a
physical interpretation as the mixture components that allow for Gibbs sampling of the
mixture weights {πk} and mixture parameters {θk}. Estimation of the mixture weights
and parameters can then be performed by simply discarding the completion variables.
Often, the auxiliary variables are only added for a subset of the sampling stages,
and then discarded at other stages. That is, the auxiliary variables are sampled based
on the current MCMC conﬁguration of the variables of interest, then some subset of
the variables of interest are sampled based on the sampled auxiliary variables. Finally,
the auxiliary variables are discarded when sampling the other variables of interest that
do not depend upon the auxiliary variables. Such sampling algorithms are developed
in this thesis, for example, in Sec. 5.2 and Appendix C.1.
Blocked Gibbs Sampling
There are also many scenarios in which jointly sampling variables can lead to statistical
eﬃciencies. Such blocked Gibbs sampling is especially useful when subsets of variables
are very strongly dependent. In such cases, large moves in the joint probability space by
standard coordinate-by-coordinate sampling may require stepping through deep valleys
in the posterior distribution that can be avoided with blocked sampling of variables.
Collapsed Gibbs Sampling
Finally, in some cases it is possible to analytically marginalize nuisance parameters
from the model and solely sample the variables of interest. In other cases, the variables
14The choice of terminology often depends upon whether the variables added have a physical inter-
pretation in the model.

88
CHAPTER 2.
BACKGROUND
Figure 2.11. Graphical model of a ﬁnite mixture model in which the model parameters are deﬁned
with mixture weights π | γ ∼Dir(γ/K, . . . , γ/K) and emission parameters θk ∼H, λ ∼H(λ). For
each of the N observations yi, a cluster assignment variable zi ∈{1, . . . , K} is sampled as zi | π ∼π,
determining the mixture component for generating observations yi | {θk}, zi ∼F(θzi).
marginalized are actually the variables of interest, and sampling occurs on a chain from
which estimates of the desired variables can be formed. See Example 2.8.1. Analytical
marginalization of variables in a Gibbs sampler, often referred to as collapsed Gibbs
sampling, can aid in improving the mixing rate, especially when the marginalized ran-
dom variables are high-dimensional as this can dramatically reduce the dimensionality
of the search space.
However, there are scenarios in which such marginalization introduces dependencies
between the remaining random variables that can actually lead to slower mixing rates.
For example, let us consider the case of the hidden Markov model (HMM) described
in Sec. 2.6. Given a sampled set of transition distributions πj and emission param-
eters θj, one can employ a variant of the forward-backward message passing scheme
to block-sample the entire state sequence z1:T (see Chapter 3.) On the other hand, if
one chooses to marginalize the transition distributions (assuming a conjugate Dirichlet
prior), then the state sequence no longer forms a simple Markov chain and thus block
sampling is no longer feasible—one must instead rely on sequentially sampling the state
zt conditioned on the state at all other time steps z1, . . . , zt−1, zt+1, . . . , zT . Since the
temporal correlations in an HMM can be quite strong, such sequential sampling can
lead to very slow mixing rates. These mixing rate issues are examined in much further
depth in Chapter 3, with a general empirical conclusion that block sampling of strongly
dependent variables leads to more signiﬁcant improvements in rates of convergence than
marginalization.
Example 2.8.1. Consider the ﬁnite mixture model of Fig. 2.11 in which each cluster
assignment variable zi ∈{1, . . . , K} indicates the mixture component associated with
observations yi. The model is deﬁned by a set of mixture weights π distributed as
π | γ ∼Dir(γ/K, . . . , γ/K)
and emission parameters θk drawn as
θk | H, λ ∼H(λ)
k = 1, . . . , K.

Sec. 2.8.
Markov Chain Monte Carlo
89
Assume we have N observations. The generative model then dictates that each for each
i ∈{1, . . . , N}, we draw:
zi | π ∼π
yi | {θk}K
k=1, zi ∼F(θzi).
In what follows, we assume the distribution F(θk) has an associated conditional density
f(· | θk).
Let us assume that our goal is to infer the set of model parameters consisting of the
mixture weights π = [π1, . . . , πK] and the emission parameters θ = {θk}K
k=1. One cannot
simply employ a Gibbs sampler on these parameters since there do not exist closed-form
conditional distributions p(π | θ, y1, . . . , yN) and p(θ | π, y1, . . . , yN).
Instead, one
could consider a completion or auxiliary variable Gibbs sampler in which the cluster
assignment variables z1, . . . , zN are additionally sampled.
Let z1:N = {z1, . . . , zN} and z\i = {z1, . . . , zi−1, zi+1, . . . , zN}. Then, one can sam-
ple each zi from
p(zi = k | z\i, y1:N, π, θ) = p(zi = k | yi, π, θ)
∝πkf(yi | θk),
with the ﬁrst equality following from the Markov properties implied by the graphical
model in Fig. 2.11. Conditioned on the cluster assignment variables z1:N, the mixture
weights π and emission parameters θk are mutually independent. The mixture weights
can be sampled from the posterior Dirichlet distribution (see Eq. (2.74)):
p(π | z1:N, γ) = Dir(N1 + γ/K, . . . , NK + γ/K)
Nk =
N
X
i=1
δ(zi, k),
(2.197)
and the parameters θk from their associated posterior (depending upon the form of the
prior H(λ)):
p(θk | {yi | zi = k}, λ).
(2.198)
Here, we have used the fact that the full conditional distributions for π and each θk
simply depend upon the sampled values of the random variables contained within the
Markov blanket for that random variable’s node. The resulting completion Gibbs sampler
is outlined in Algorithm 7.
Alternatively, assuming the base measure H(λ) is conjugate to the likelihood model
F,15 one could analytically marginalize the mixture weights π and emission parameters
θ and solely sample the cluster assignment variables zi. Based on a set of Gibbs samples
of z1:N, one could then estimate a set of model parameters (the variables of interest)
from the distributions given in Eq. (2.197) and Eq. (2.198).
15Conjugacy of the prior on π to the multinomial observations zi is already established by our choice
of a Dirichlet prior.

90
CHAPTER 2.
BACKGROUND
Given mixture weights π(t−1) and emission parameters {θ(t−1)
k
}K
k=1 from the previous
Gibbs iteration, sample a new set of model parameters as follows:
1. For each i ∈{1, . . . , N}, independently assign observation yi to one of the K
clusters by sampling the cluster assignment variable zi as:
z(t)
i
∼1
Zi
K
X
k=1
π(t−1)
k
f(yi | θ(t−1)
k
)δ(zi, k),
Zi =
K
X
k=1
π(t−1)
k
f(yi | θ(t−1)
k
)
2. Sample a new set of mixture weights:
π(t) ∼Dir(N1 + γ/K, . . . , NK + γ/K),
Nk =
N
X
i=1
δ(z(t)
i , k)
3. For each cluster k ∈{1, . . . , K}, independently sample new parameters from the
conditional distribution implied by the observations currently assigned to that
cluster:
θ(t)
k
∼p(θk | {yi | z(t)
i
= k}, λ)
Algorithm 7.
Completion Gibbs sampler for the ﬁnite mixture model shown in Fig. 2.11.
Each
iteration resamples the cluster assignment variables for each of the N observations, and uses these
sampled values to resample a set of mixture weights and emission parameters.
Integrating over π and θ, the Markov structure16 of the graph in Fig. 2.11 implies
a posterior distribution on the cluster assignment variables that decomposes as:
p(zi | z\i, y1:N, γ, λ) ∝p(zi | z\i, γ)p(yi | z1:N, y\i, λ).
(2.199)
Based on the Dirichlet prior, the predictive distribution of Eq. (2.75) informs us that:
p(zi = k | z\i, γ) = Nk−i + γ/K
N −1 + γ
N −i
k
=
X
j̸=i
δ(zj, k).
(2.200)
When considering zi = k, the likelihood term of Eq. (2.199) simpliﬁes to:
p(yi | zi = k, z\i, y\i, λ) = p(yi | {yj | zj = k, j ̸= i}, λ).
(2.201)
Because H(λ) is chosen conjugate to F, Eq. (2.201) can be analytically determined.
The resulting collapsed Gibbs sampler is outlined in Algorithm 8.
16Marginalization of π and θ induces dependencies between the zi and yi not present in Fig. 2.11.
However, the Markov structure of the graph in Fig. 2.11 can be exploiting during the integration over
π and θ to produce the decomposition of Eq. (2.199).

Sec. 2.9.
Bayesian Nonparametric Methods
91
Given a previous set of cluster assignment variables π(t−1), sequentially sample new
assignments as follows:
1. Set z1:N = z(t−1)
1:N .
2. Sample a random permutation τ(·) of the integers {1, . . . , N}.
3. For each i ∈{τ(1), . . . , τ(N)}, resample zi as follows:
(a) For each k ∈{1, . . . , K}, determine the predictive likelihood of observation yi
based on an assignment to cluster k:
fk(yi) = p(yi | {yj | zj = k, j ̸= i}, λ).
This likelihood can be computed from a set of cached suﬃcient statistics
based on the results presented in Sec. 2.4.1.
(b) Sample a new cluster assignment zi as:
zi ∼1
Zi
K
X
k=1
(N −i
k
+ γ/K)fk(yi)δ(zi, k),
Zi =
K
X
k=1
(N −i
k
+ γ/K)fk(yi),
where N −i
k
is deﬁned as in Eq. (2.200).
(c) Update cached suﬃcient statistics to reﬂect the assignment of yi to cluster zi.
4. Set z(t)
1:T = z1:T .
Algorithm 8.
Collapsed Gibbs sampler for the ﬁnite mixture model shown in Fig. 2.11. Each it-
eration resamples the cluster assignment variables for each of the N observations, having analytically
marginalized the mixture weights and emission parameters.
■2.9 Bayesian Nonparametric Methods
As motivated by the discussion of de Finetti’s theorem (Theorem 2.1.2) in Sec. 2.1, it
is theoretically desirable to consider models that are not limited to ﬁnite parameteriza-
tions, and in so doing one must deﬁne prior distributions on these inﬁnite-dimensional
objects. Bayesian nonparametric methods avoid the often restrictive assumptions of
parametric models by deﬁning distributions on function spaces such as that of proba-
bility measures. If suitably designed, these methods allow for eﬃcient, data-driven pos-
terior inference. For a review of Bayesian nonparametric inference, see [120, 157, 178].
In the following sections, we brieﬂy describe some classes of Bayesian nonparametric
methods: the Dirichlet process, its hierarchical extension, and the beta process.

92
CHAPTER 2.
BACKGROUND
■2.9.1 Dirichlet Processes
A Dirichlet process (DP) is a distribution on probability measures on a measurable
space Θ. This stochastic process17 is uniquely deﬁned by a base measure H on Θ and a
concentration parameter γ; we denote it by DP(γ, H). The Dirichlet process is formally
deﬁned by the distributions induced on ﬁnite partitions of Θ.
Theorem 2.9.1. Let H be a probability distribution on a measurable space Θ, and γ a
positive scalar. Consider a ﬁnite partition {A1, . . . , AK} of Θ:
K
[
k=1
Ak = Θ
Aj ∩Ak = ∅, j ̸= k.
(2.202)
A random probability measure G0 on Θ is a draw from a Dirichlet process if its measure
on every ﬁnite partition follows a Dirichlet distribution:
(G0(A1), . . . , G0(AK)) | γ, H ∼Dir(γH(A1), . . . , γH(AK)).
(2.203)
For each such base measure H and concentration parameter γ, there exists a unique
stochastic process satisfying the above conditions, which we denote by DP(γ, H).
Proof. The proof of the existence of the Dirichlet process was initially provided by Fer-
guson [41], who invoked Kolmogorov’s consistency conditions to establish the existence
of the Dirichlet process as a stochastic process with Dirichlet marginals. A more con-
structive deﬁnition of the Dirichlet process was given by Sethuraman [149].
■
Using Eq. (2.72) along with Eq. (2.203), it is straightforward to establish that for
any A ⊂Θ,
E[G0(A) | H] = H(A)
G0 | H, γ ∼DP(γ, H) .
(2.204)
Based on an observation θ′ ∼G0 that falls within an element Ak of a given partition
{A1, . . . , AK}, one can use the Dirichlet posterior analysis results of Eq. (2.74) to show
that
(G0(A1), . . . , G0(AK)) | θ′, H, γ ∼Dir(γH(A1), . . . , γH(Ak) + 1, . . . , γH(AK)).
(2.205)
Here, we note that the observation θ′ only aﬀects the Dirichlet parameter of the arbi-
trarily small partition element Ak in which it is contained. Formalizing such an analysis,
17In elementary probability theory, random variables are functions whose range is R, whereas more
advanced probability theory allows random variables to range over more general spaces (e.g., function
spaces, spaces of probability measures, etc.). Stochastic process theory describes these more general
random objects.

Sec. 2.9.
Bayesian Nonparametric Methods
93
Ferguson [41] showed that a set of independent observations θ′
1, . . . , θ′
N, θ′
i ∼G0, leads
to a posterior distribution
G0 | θ′
1, . . . , θ′
N, H, γ ∼DP
 
γ + N,
γ
γ + N H +
1
γ + N
N
X
i=1
δθ′
i
!
,
(2.206)
where δθ denotes a unit-mass measure concentrated at θ. From Eq. (2.204), we see that
for any A ⊂Θ,
E[G0(A) | θ′
1, . . . , θ′
N, H, γ] =
γ
γ + N H(A) +
1
γ + N
N
X
i=1
δθ′
i(A),
(2.207)
implying that,
lim
N→∞E[G0(A) | θ′
1, . . . , θ′
N, H, γ] =
∞
X
k=1
βkδθk(A),
(2.208)
where {θk}∞
k=1 are the unique values in the set of observations {θ′
i}∞
i=1, and βk is the
limiting empirical frequency of θk. Assuming the posterior concentrates about its mean,
Eq. (2.208) implies that a realization from a Dirichlet process is discrete with probability
one.
Sethuraman [149] provides a formal proof of the discreteness of the Dirichlet
process random measures G0, and connects the weights βk of this atomic measure with
a constructive procedure.
Stick-Breaking Construction
Consider a probability mass function (pmf) {βk}∞
k=1 on a countably inﬁnite set, where
the discrete probabilities are deﬁned as follows:
vk | γ ∼Beta(1, γ)
k = 1, 2, . . .
βk = vk
k−1
Y
ℓ=1
(1 −vℓ)
k = 1, 2, . . . .
(2.209)
In eﬀect, we have divided a unit-length stick into lengths given by the weights βk: The
kth weight is a random proportion vk of the remaining stick after the previous (k −1)
weights have been deﬁned. This stick-breaking construction is generally denoted by
β ∼GEM(γ). Sethuraman [149] showed that with probability one, a random draw
G0 ∼DP(γ, H) can be expressed as
G0 =
∞
X
k=1
βkδθk
θk | H ∼H,
k = 1, 2, . . . ,
(2.210)
From this deﬁnition, we see that the Dirichlet process actually deﬁnes a distribution
over discrete probability measures. The stick-breaking construction also gives us insight

94
CHAPTER 2.
BACKGROUND
into how the concentration parameter γ controls the relative proportion of the weights
βk.
Alternative stick-breaking processes have been studied for cases in which the weights
vk are drawn from a more general Beta(ak, bk) distribution [72, 74]. When considering
a two-parameter (a, b) family with ak = 1 −a and bk = b + ka, one arrives at the
Poisson-Dirichlet, or Pitman-Yor, process [137]. This process has heavier-tailed weight
distributions than the Dirichlet process that have proven useful in applications such as
natural language processing [58, 160], in which word frequencies closely follow a power-
law.
P´olya Urn Predictions
The Dirichlet process has a number of properties which make inference based on this
nonparametric prior computationally tractable. Once again, consider a set of observa-
tions {θ′
i}N
i=1
θ′
i | G0 ∼G0.
(2.211)
Because probability measures drawn from a Dirichlet process are discrete, there is a
strictly positive probability of multiple observations θ′
i taking identical values within the
set {θk}∞
k=1, with θk deﬁned as in Eq. (2.210). Blackwell and MacQueen [18] introduced
a P´olya urn representation of the θ′
i that results from integrating over the underlying
random measure G0 (distributed as in Eq. (2.204)):
θ′
i | θ′
1, . . . , θ′
i−1 ∼
γ
γ + i −1H +
i−1
X
j=1
1
γ + i −1δθ′
j
(2.212)
∼
γ
γ + i −1H +
K
X
k=1
Nk
γ + i −1δθk.
(2.213)
The second line is an equivalent representation of the ﬁrst, but in terms of the unique set
of parameter {θk}∞
k=1, with Nk denoting the number of times each of these parameters
was observed in the set {θ′
i}N
i=1.
A formal argument is presented in [18]. We can informally begin to justify Eq. (2.213)
by once again considering Eq. (2.207). We ﬁrst rewrite this expectation in terms of the
unique parameters θk
E[G0(A) | θ′
1, . . . , θ′
N, H, γ] =
γ
γ + N H(A) +
1
γ + N
∞
X
k=1
Nkδθk(A).
(2.214)
Taking A to be the singleton set {θk}, Eq. (2.214) implies that the marginal posterior
probability of θ′
N+1 = θk for all k such that Nk > 0 is proportional to Nk, the number
of times this parameter was previously observed. New parameter values are observed
with probability proportional to γ.
The representation of Eq. (2.213) can be used to sample observations from a Dirichlet
process without explicitly constructing the random probability measure G0 ∼DP(γ, H).

Sec. 2.9.
Bayesian Nonparametric Methods
95
Chinese Restaurant Process
For each value θ′
i, let zi be an indicator random variable that picks out the unique value
θk such that
θ′
i = θzi.
(2.215)
Eq. (2.213) implies the following predictive distribution on the indicator random vari-
ables:
p(zN+1 = z | z1, . . . , zN, γ) =
γ
N + γ δ(z, K + 1) +
1
N + γ
K
X
k=1
Nkδ(z, k),
(2.216)
where Nk = PN
i=1 δ(zi, k) is the number of indicator random variables taking the value
k, and K + 1 is a previously unseen value.
The distribution on partitions induced by the sequence of conditional distributions
in Eq. (2.216) is commonly referred to as the Chinese restaurant process. The analogy,
which is useful in developing various generalizations of the Dirichlet process we consider
in this thesis, is as follows. Take θ′
i to be a customer entering a restaurant with inﬁnitely
many tables, each serving a unique dish θk. Each arriving customer chooses a table,
indicated by zi, in proportion to how many customers are currently sitting at that table.
With some positive probability proportional to γ, the customer starts a new, previously
unoccupied table K +1. From the Chinese restaurant process, we see that the Dirichlet
process has a reinforcement property that leads to a clustering at the values θk.
Number of Unique Observed Values
From Eq. (2.216) we see that when
zi | β ∼β
β | γ ∼GEM(γ),
(2.217)
we can integrate out β to determine a closed-form predictive distribution for zi. We
can also ﬁnd the distribution of the number of unique values of zi (i.e., the number
of occupied tables in the Chinese restaurant process) resulting from N draws from the
measure β. Letting K denote the number of unique values of {z1, . . . , zN}, Antoniak
[5] derives this distribution to be:
p(K | N, γ) =
Γ(γ)
Γ(γ + N)s(N, K)γK,
(2.218)
where s(n, m) are unsigned Stirling numbers of the ﬁrst kind [1].
Using Eq. (2.218), Antoniak [5] also observes that
E[K | N, γ] ≈γ log
γ + N
γ

(2.219)
implying that the number of occupied tables in the Chinese restaurant process ap-
proaches (almost surely) γ log(N) as N →∞.

96
CHAPTER 2.
BACKGROUND
■2.9.2 Dirichlet Process Mixture Models
The Dirichlet process is commonly used as a prior on the parameters of a mixture
model with a random number of components. Such a model is called a Dirichlet process
mixture model and is depicted as a graphical model in Fig. 2.12(a)-(b). To generate
observations, we choose
θ′
i | G0 ∼G0
yi | θ′
i ∼F(θ′
i)
(2.220)
for an indexed family of distributions F(·). This sampling process is also often described
in terms of the indicator random variables zi; in particular, we have
zi | β ∼β
yi | {θk}∞
k=1, zi ∼F(θzi).
(2.221)
The parameter with which an observation is associated implicitly partitions or clusters
the data. In addition, the Chinese restaurant process representation indicates that the
Dirichlet process provides a prior that makes it more likely to associate an observation
with a parameter to which other observations have already been associated. This re-
inforcement property is essential for inferring ﬁnite, compact mixture models. It can
be shown under mild conditions that if the data were generated by a ﬁnite mixture,
then the Dirichlet process posterior is guaranteed to converge (in distribution) to that
ﬁnite set of mixture parameters [75]. See Sec. 6.2.5 for further discussion of the vari-
ous asymptotic guarantees that have been established for models employing Dirichlet
process priors.
Limit of Finite Mixture Models
We can also obtain the Dirichlet process mixture model as the limit of a sequence of
ﬁnite mixture models, such as the one analyzed in Example 2.8.1. Let us assume that
there are L components in a ﬁnite mixture model and we place a ﬁnite-dimensional,
symmetric Dirichlet prior on these mixture weights:
β | γ ∼Dir(γ/L, . . . , γ/L).
(2.222)
Let GL
0 = PL
k=1 βkδθk. Then, it can be shown [74, 76] that for every measurable function
f integrable with respect to the measure H, this ﬁnite distribution GL
0 converges weakly
to a countably inﬁnite distribution G0 distributed according to a Dirichlet process. That
is,
Z
θ
f(θ)dGL
0 (θ) D→
Z
θ
f(θ)dG0(θ),
(2.223)
as L →∞for G0 ∼DP(γ, H). One can begin to justify this result by considering
the K-component mixture model of Example 2.8.1 and taking the limit as K →∞of

Sec. 2.9.
Bayesian Nonparametric Methods
97
(a)
(b)
(c)
(d)
Figure 2.12.
Dirichlet process (left) and hierarchical Dirichlet process (right) mixture models rep-
resented in two diﬀerent ways as graphical models.
(a) Indicator variable representation in which
β|γ ∼GEM(γ), θk|H, λ ∼H(λ), zi|β ∼β, and yi|{θk}∞
k=1, zi ∼F(θzi). (b) Alternative representation
with G0|H, γ ∼DP(γ, H), θ′
i|G0 ∼G0, and yi|θ′
i ∼F(θ′
i). (c) Indicator variable representation in
which β|γ ∼GEM(γ), πk|α, β ∼DP(α, β), θk|H, λ ∼H(λ), zji|πj ∼πj, and yji|{θk}∞
k=1, zji ∼F(θzji).
(d) Alternative representation with G0|H, γ ∼DP(γ, H), Gj|G0 ∼DP(α, G0), θ′
ji|Gj ∼Gj, and
yji|θ′
ji ∼F(θ′
ji). The “plate” notation is used to compactly represent replication [162].
Eq. (2.200), resulting in:
p(zi = k | z\i, γ) =
Nk−i
N −1 + γ
(2.224)
for each instantiated cluster k. The probability of generating a new cluster is given the
remaining mass γ/(N −1 + γ). Comparing these probabilities with those deﬁned by
Eq. (2.216) (using exchangeability to treat zi as if it were the last observation), we see
the equivalence of both predictive distributions.
In some scenarios, such as one we examine in Chapter 3, it is desirable to maintain a
ﬁnite approximation to the Dirichlet process mixture model. One approach to producing
such a ﬁnite approximation is simply to terminate the stick-breaking construction after
some portion of the stick has already been broken and assign the remaining weight to a
single component. This approximation is referred to as the truncated Dirichlet process.
Another method, motivated by the convergence guarantee of Eq. (2.223), is to consider
the degree L weak limit approximation to the Dirichlet process [76],
GEML(α) ≜Dir(α/L, . . . , α/L),
(2.225)
where L is a number that exceeds the total number of expected mixture components.
Both of these approximations, which are presented in [74, 76], encourage the learning of
models with fewer than L components while allowing the generation of new components,
upper bounded by L, as new data are observed. The two choices of approximations are
compared in [102], and little to no practical diﬀerences are found.

98
CHAPTER 2.
BACKGROUND
■2.9.3 Hierarchical Dirichlet Processes
There are many scenarios in which groups of data are thought to be produced by related,
yet distinct, generative processes. For example, take a sensor network monitoring an
environment where time-varying conditions may inﬂuence the quality of the data. Data
collected under certain conditions should be grouped and described by a similar, but
diﬀerent model from that of other data.
The hierarchical Dirichlet process (HDP)
[162] extends the Dirichlet process to such scenarios by taking a hierarchical Bayesian
approach: the group-speciﬁc distributions Gj, with
Gj | G0, α ∼DP(α, G0) ,
(2.226)
are tied together via a global base measure G0, which is itself given a Dirichlet process
prior:
G0 | H, γ ∼DP(γ, H) ,
(2.227)
As given by Eq. (2.204), for every A ⊂Θ,
E[Gj(A) | G0] = G0(A).
(2.228)
In this sense, we can interpret G0 as an “average” distribution across all groups. Below,
we demonstrate that this speciﬁc choice of hierarchy implies that atoms are shared not
only within groups, but also between groups, as desired. The HDP is depicted as a
graphical model in Fig. 2.12(c)-(d).
Stick-Breaking Representation
Let {yj1, . . . , yjNj} be the set of observations in group j.
We assume there are J
such groups of data. Then, replacing each Dirichlet process random measure with its
associated stick-breaking representation (see Eq. (2.210)), the generative model can be
written as:
G0 = P∞
k=1 βkδθk
β | γ ∼GEM(γ)
θk | H, λ ∼H(λ)
k = 1, 2, . . .
Gj = P∞
t=1 ˜πjtδθ∗
jt
˜πj | α ∼GEM(α)
j = 1, . . . , J
θ∗
jt | G0 ∼G0
t = 1, 2, . . .
θ′
ji | Gj ∼Gj
yji | θ′
ji ∼F(θ′
ji)
j = 1, . . . , J
i = 1, . . . , Nj.
(2.229)
See Fig. 2.12(d).
From this formulation, we clearly see how placing a Dirichlet process prior on G0
creates a shared (and unbounded) support for each of the group-speciﬁc distributions

Sec. 2.9.
Bayesian Nonparametric Methods
99
Gj. Namely, each group-speciﬁc set of support points θ∗
jt are drawn from the collection
of atomic masses of G0. Thus, there exists non-zero probability that diﬀerent Gj share
support points.
If G0 were instead absolutely continuous with respect to Lebesgue
measure, there would be zero probability of the group-speciﬁc distributions having
overlapping support.
Chinese Restaurant Franchise and the associated Table-Dish Representation
Teh et al. [162] have described the marginal probabilities obtained from integrating over
the random measures G0 and Gj. They show that these marginals can be described
in terms of a Chinese restaurant franchise (CRF) that is an analog of the Chinese
restaurant process. The CRF is comprised of J restaurants, each corresponding to an
HDP group, and an inﬁnite buﬀet line of dishes common to all restaurants. The process
of seating customers at tables, however, is restaurant speciﬁc. To build up to this CRF,
and to lay the foundation for modiﬁcations we make in Chapter 3, we ﬁrst present the
generative process in terms of indicator random variables being drawn from the stick-
breaking measures β and ˜πj leading to a table-dish representation of the HDP. We then
marginalize these random measures to obtain the CRF.
More formally, we introduce indicator variables tji and kjt to represent table and
dish assignments. There are J restaurants (groups), each with inﬁnitely many tables
(clusters) at which customers (observations) sit. Each customer is pre-assigned to a
given restaurant determined by that customer’s group j. The table assignment for the
ith customer in the j restaurant is chosen as tji ∼˜πj, and each table is assigned a dish
(parameter) via kjt ∼β. One can think of β as a set of ratings for the dishes served in
the buﬀet line. Observation yji is then generated by global parameter
θ′
ji = θ∗
jtji = θkjtji.
(2.230)
The generative model for this table-dish representation is summarized below and is
depicted as a graphical model in Fig. 2.13:
kjt | β ∼β
tji | πj ∼˜πj
yji | {θk}∞
k=1, {kjt}∞
t=1, tji ∼F(θkjtji).
(2.231)
Marginalizing over the stick-breaking measures ˜πj and β yields the following pre-
dictive distributions that describe the CRF:
p(tji | tj1, . . . , tji−1, α) ∝
Tj
X
t=1
˜njtδ(tji, t) + αδ(tji, Tj + 1)
p(kjt | k1, k2, . . . , kj−1, kj1, . . . , kjt−1, γ) ∝
K
X
k=1
m·kδ(kjt, k) + γδ(kjt, K + 1),
(2.232)

100
CHAPTER 2.
BACKGROUND
Figure 2.13. Graph of Chinese restaurant franchise (CRF). Customers yji sit at table tji|˜πj ∼˜πj.
The ﬁrst customer at each table chooses a dish kjt|β ∼β.
where m·k = P
j mjk and kj = {kj1, . . . , kjTj}. Here, ˜njt denotes the number of cus-
tomers in restaurant j sitting at table t, mjk the number of tables in restaurant j
serving dish k, Tj the number of currently occupied tables in restaurant j, and K the
total number of unique dishes being served in the franchise. We note that m·k is a
pooling of the number of tables serving dish k in each of the individual restaurants,
from which we see the sharing induced by the deﬁned hierarchical model.
Eq. (2.232) implies that upon entering the jth restaurant in the CRF, customer
yji sits at currently occupied tables tji with probability proportional to the number of
currently seated customers, or starts a new table Tj + 1 with probability proportional
to α.
Whenever a customer is the ﬁrst customer to sit at a table in any of the J
restaurants, that customer goes to the buﬀet line and picks a dish kjt for their table,
choosing the dish with probability proportional to the number of times that dish has
been picked previously by any table in the franchise, or ordering a new dish θK+1 with
probability proportional to γ. The intuition behind this predictive distribution is that
integrating over the dish ratings β results in customers making decisions based on the
observed popularity of the dishes.
Compressed Indicator Variable Representation
Since each distribution Gj is drawn from a Dirichlet process with a discrete base measure
G0, multiple θ∗
jt may take an identical value θk for multiple unique values of t, implying
that multiple tables in the same restaurant may be served the same dish, as depicted
in Fig. 2.14. We can write Gj as a function of these unique dishes [162]:
Gj =
∞
X
k=1
πjkδθk,
πj | α, β ∼DP(α, β) ,
θk | H ∼H,
(2.233)

Sec. 2.9.
Bayesian Nonparametric Methods
101
y11
y17
y12
y13
y14
y15
y16
y21
y27
y23
y25
y22
y26
y24
θ1
θ2
θ3
θ11 θ1
=
θ12 θ1
=
θ13 θ3
=
θ21 θ2
=
θ22 θ3
=
θ23 θ3
=
j=1
j=2
k11 1
=
k12 1
=
k13 3
=
k21 2
=
k22 3
=
k23 3
=
*
*
*
*
*
*
Figure 2.14.
Chinese restaurant franchise (CRF) with J = 2 restaurants. The currently occupied
tables each choose a dish θ∗
jt|Gj ∼Gj, where Gj|G0, α ∼DP(α, G0) is a discrete probability measure so
that multiple tables may serve the same dish. Since G1 has overlapping support with G2, parameters
(i.e., dishes) are shared between restaurants.
where πj now deﬁnes a restaurant-speciﬁc distribution over dishes served rather than
over tables, with
πjk =
X
t|kjt=k
˜πjt.
(2.234)
Let zji be the indicator random variable for the unique dish eaten by customer yji,
so that zji = kjtji. A third equivalent representation of the generative model is in terms
of these indicator random variables:
πj | α, β ∼DP(α, β)
zji | πj ∼πj
yji | {θk}, zji ∼F(θzji),
(2.235)
and is shown in Fig. 2.12(c).
Limit of Finite Mixture Models
As with the Dirichlet process, the HDP mixture model has an interpretation as the
limit of a ﬁnite mixture model. Placing a ﬁnite Dirichlet prior on β induces a ﬁnite
Dirichlet prior on πj (using Eq. (2.203) and the fact that πj ∼DP(α, β)):
β | γ ∼Dir(γ/L, . . . , γ/L)
(2.236)
πj | α, β ∼Dir(αβ1, . . . , αβL).
As L →∞, this model converges in distribution to the HDP mixture model [162].

102
CHAPTER 2.
BACKGROUND
■2.9.4 Beta Process
In Sec. 2.9.1, we described how the Dirichlet process, and its hierarchical extension,
are useful in clustering applications (i.e., when it is assumed that the collection of
observations are partitioned into a discrete set of classes, each described by a single
parameter.) However, in many applications it is more appropriate to associate each
observation with a binary feature vector indicating the set of parameters that describe
the observation. For the clustering application, this vector would simply have a single
1 in the location corresponding to the index of the observation’s cluster.
When given a large collection of observations, each described by multiple features, it
is useful to consider a featural model that induces sparsity in the feature space by encour-
aging sharing of features among the observations. Analogous to the Dirichlet process
inducing the Chinese restaurant process (CRP) clustering model with an unbounded
number of clusters, we explain how a diﬀerent stochastic process—the beta process—
underlies the Indian buﬀet process (IBP) [62] featural model with an unbounded number
of possible features18. Here, instead of associating a customer with a single dish as in
the CRP, each customer of the IBP chooses a collection of dishes. And, just as the
CRP encouraged the use of a sparse subset of the inﬁnite collection of possible clusters,
the IBP encourages the use of a sparse subset of the inﬁnite feature space.
The Beta Process - Bernoulli Process Featural Model
The beta process [67, 161] is a stochastic process within the class of completely random
measures [95, 96]; that is, evaluating a draw from a beta process over disjoint sets results
in measures that are independent random variables. The deﬁnition of a completely
random measure implies that the realizations are discrete, and thus described by a
weighted collection of atoms, just as in the case of the Dirichlet process. We note,
however, that Dirichlet process does not produce completely random measures since
the weights of its realizations are constrained to sum to 1 (i.e., they are probability
measures), inducing dependencies between the measures over disjoint sets. One can
instead show [95] that Dirichlet process realizations are obtained by normalizing the
completely random measures generated by the gamma process.
Consider a probability space Θ, and let B0 denote a ﬁnite base measure on Θ with
total mass B0(Θ) = α. Supposing ﬁrst that B0 is absolutely continuous with respect to
the dominating measure, we deﬁne the following L´evy measure [93, 106] on the product
space [0, 1] × Θ:
ν(dω, dθ) = cω−1(1 −ω)c−1dωB0(dθ)
(2.237)
Here, c > 0 is a concentration parameter; we denote such a beta process by BP(c, B0).
18Speciﬁcally, the beta process is the de Finetti mixing distribution underlying the Indian buﬀet pro-
cess (IBP), just as the Dirichlet process is the de Finitti distribution underlying the Chinese restaurant
process. Historically, the IBP was introduced ﬁrst by Griﬃths and Ghahramani [62], who noted the
exchangeability of the feature vectors. From Theorem 2.1.2, exchangeability implies there must exist
an underlying measure yielding the feature vectors i.i.d.. As derived by Thibaux and Jordan [165], this
measure is distributed according to the beta process.

Sec. 2.9.
Bayesian Nonparametric Methods
103
0
1
2
0
50
w
a
r
D
θ
θ
B
ω
θk
k
X
X
1
50
θk
Customer
Dish
10
20
30
40
50
20
40
60
80
100
(a)
(b)
Figure 2.15.
(a) Top: A draw B from a beta process is shown in blue, with the corresponding
cumulative distribution in red. Bottom: 50 draws Xi from a Bernoulli process using the beta process
realization. Each blue dot corresponds to a coin-ﬂip at that atom in B that came up heads. (b) An
image of a feature matrix associated with a realization from an Indian buﬀet process with α = 10. Each
row corresponding to a diﬀerent customer, and each column a diﬀerent dish. White indicates a chosen
feature.
A draw B ∼BP(c, B0) is then described by
B =
∞
X
k=1
ωkδθk,
(2.238)
where (ω1, θ1), (ω2, θ2), . . . are the set of atoms in a realization of a non-homogeneous
Poisson process with rate measure ν. This set is necessarily inﬁnite, as ν has inﬁnite
mass. However, because ν is σ-ﬁnite, Campbell’s theorem [96] guarantees that for α
ﬁnite, B has ﬁnite expected measure.
For a base measure B0 containing atoms, the deﬁnition of the beta process measure
B must be altered. Let qk ∈(0, 1) denote the mass of the kth atom. A sample B ∼
BP(c, B0) necessarily contains this atom, with associated weight
ωk ∼Beta(cqk, c(1 −qk)).
(2.239)
The overall realization B is then the sum of independent contributions from the con-
tinuous and discrete components of B0. For an example realization and its associated
cumulative distribution, see Fig. 2.15.
The beta process is conjugate to a class of so-called Bernoulli processes [165], de-
noted by BeP(B). A realization
Xi | B ∼BeP(B),
(2.240)
with B an atomic measure (i.e., having a representation as in Eq. (2.238)), is a collection
of unit mass atoms on Θ located at some subset of the atoms in B. In particular, for

104
CHAPTER 2.
BACKGROUND
each atom θk in Eq. (2.238), we independently sample19
fik ∼Bernoulli(ωk)
(2.241)
and then set
Xi =
X
k
fikδθk.
(2.242)
Example realizations of Xi ∼BeP(B), with B a draw from a beta process, are shown
in Fig. 2.15(a).
For continuous measures B, we draw L ∼Poisson(B(Θ)) and then independently
sample a set of L atoms θℓ∼B(Θ)−1B. The Bernoulli realization is then given by:
Xi =
L
X
ℓ=1
δθℓ.
(2.243)
In many applications, we interpret the atom locations θk as a shared set of global
features.
A Bernoulli process realization Xi then determines the subset of features
allocated to object i:
B | B0, c ∼BP(c, B0)
Xi | B ∼BeP(B),
i = 1, . . . , N.
(2.244)
Because beta process priors are conjugate to the Bernoulli process [165], the posterior
distribution given N samples Xi ∼BeP(B) is a beta process with updated parameters:
B | X1, . . . , XN, B0, c ∼BP
 
c + N,
c
c + N B0 +
1
c + N
N
X
i=1
Xi
!
(2.245)
= BP
 
c + N,
c
c + N B0 +
K+
X
k=1
mk
c + N δθk
!
(2.246)
Here, mk denotes the number of objects Xi that select the kth feature θk. For simplicity,
we have reordered the feature indices to list ﬁrst the K+ features used by at least one
object.
The Indian Buﬀet Process
Computationally, Bernoulli process realizations Xi are often summarized by an inﬁnite
vector of binary indicator variables fi = [fi1, fi2, . . .], where fik = 1 if and only if
object i exhibits feature k. As shown by Thibaux and Jordan [165], marginalizing over
19One can visualize this process as walking along the atoms of a discrete measure B and, at each
atom θk, ﬂipping a coin with probability of heads given by ωk.

Sec. 2.9.
Bayesian Nonparametric Methods
105
the beta process measure B, and taking c = 1, provides a predictive distribution on
indicators equivalent to the Indian buﬀet process (IBP) of Griﬃths and Ghahramani
[62].
The IBP is a culinary analogy inspired by the Chinese restaurant process, which is
itself the predictive distribution on partitions induced by the Dirichlet process [162].
The Indian buﬀet consists of an inﬁnitely long buﬀet line of dishes, or features. The ﬁrst
arriving customer, or object, chooses Poisson(α) dishes. Each subsequent customer i
selects a previously tasted dish k with probability mk/i proportional to the number of
previous customers mk to sample it, and also samples Poisson(α/i) new dishes. The
image of a feature matrix realization from an IBP with α = 10 is shown in Fig. 2.15(b).
Each row corresponding to a diﬀerent customer, and each column a diﬀerent dish.
White indicates a chosen feature.
To derive the IBP from the beta process formulation described above, we note that
the probability Xi contains feature θk after having observed X1, . . . , Xi−1 is equal to
the expected mass of that atom:
p(fik = 1 | X1, . . . , Xi−1) = EB|X1,...,Xi−1[p(fik = 1 | B)] = EB|X1,...,Xi−1[ωk],
(2.247)
where our notation EB[·] means to take the expectation with respect to the distribution
of B. Using the posterior distribution deﬁned in Eq. (2.246), we consider the discrete
and continuous portions of the base measure separately. The discrete component is a
collection of atoms at locations θ1, . . . , θK+, each with weight
qk =
mk
c + i −1,
(2.248)
where K+ is the number of unique atoms present in X1, . . . , Xi−1. For each of the
currently instantiated features k ∈{1, . . . , K+}, we have
ωk ∼Beta((c + i −1)qk, (c + i −1)(1 −qk))
(2.249)
such that the expected weight is simply qk, implying that the ith object chooses one
of the currently instantiated features with probability proportional to the number of
objects that already chose that feature, mk. We now consider the continuous portion
of the base measure,
c
c + i −1B0.
(2.250)
The Poisson process deﬁned by this rate function generates
Poisson

c
c + i −1B0(Θ)

= Poisson

c
c + i −1α

(2.251)
new atoms in Xi that do not appear in X1, . . . , Xi−1. Following this argument, the ﬁrst
object simply chooses Poisson(α) features. If we specialize this process to c = 1, we
arrive at the Indian buﬀet process of Griﬃths and Ghahramani [62].
Just as with the Dirichlet process, hierarchical extensions [165] and stick-breaking
constructions [163] of the Indian buﬀet process have been developed. However, we will
not utilize such constructions in this thesis, so we omit the details of these processes.

106
CHAPTER 2.
BACKGROUND

Chapter 3
The Sticky HDP-HMM
H
IDDEN Markov models (HMMs) have been a major success story in many applied
ﬁelds; they provide core statistical inference procedures in areas as diverse as
speech recognition, genomics, structural biology, machine translation, cryptanalysis and
ﬁnance.
Even after four decades of work on HMMs, however, signiﬁcant problems
remain. One lingering issue is the choice of the cardinality of the hidden state space.
While standard parametric model selection methods can be adapted to the HMM,
there is little understanding of the strengths and weaknesses of such methods in this
setting, and practical applications of HMMs often ﬁx the number of states using ad hoc
approaches.
Recently, Teh et. al. [162] presented a Bayesian nonparametric approach to HMMs
in which a stochastic process, the hierarchical Dirichlet process (HDP), deﬁnes a prior
distribution on transition matrices over countably inﬁnite state spaces. The resulting
HDP-HMM is amenable to full Bayesian inference; in particular it is possible to compute
and sample from posterior distributions over the number of model states. Moreover,
this posterior distribution can be integrated over when making predictions, eﬀectively
averaging over models of varying complexity. The HDP-HMM has shown promise in a
variety of applications, including visual scene recognition [97], music synthesis [68], and
the modeling of genetic recombination [186] and gene expression [10].
One serious limitation of the standard HDP-HMM is that it inadequately models
the temporal persistence of states. This problem arises in classical ﬁnite HMMs as well,
where semi-Markovian models are often proposed as solutions. However, the problem is
exacerbated in the nonparametric setting, in which the Bayesian bias towards simpler
models is insuﬃcient to prevent the HDP-HMM from giving high posterior probability
to models with unrealistically rapid switching. As demonstrated in Fig. 3.1, HDP-HMM
sampling algorithms often create redundant states and rapidly switch among them.
To illustrate the seriousness of this issue, let us consider a challenging application
that we revisit in Sec. 3.4.
The problem of speaker diarization involves segmenting
an audio recording into time intervals associated with individual speakers [185]. This
application seems like a natural ﬁt for the HDP-HMM, as the number of true speakers
is typically unknown, and may grow as more data are observed. However, this is not a
setting in which model averaging is the goal; rather, it is critical to infer the number
of speakers as well as the transitions among speakers. As we show in Sec. 3.4, the
107

108
CHAPTER 3.
THE STICKY HDP-HMM
0
200
400
600
800
1000
0
5
10
15
20
Time
Observations
0
200
400
600
800
1000
0
1
2
3
4
5
6
7
8
9
Time
True Mode Sequence
(a)
(b)
0
200
400
600
800
1000
0
1
2
3
4
5
6
7
8
9
Time
Estimated Mode Sequence
0
200
400
600
800
1000
0
1
2
3
4
5
6
7
8
9
Time
Estimated Mode Sequence
(c)
(d)
Figure 3.1.
(a) Multinomial observation sequence; (b) true state sequence; (c)-(d) estimated state
sequence after 30,000 Gibbs iterations for the original and sticky HDP-HMM, respectively, with errors
indicated in red.
Without an extra self–transition bias, the HDP-HMM rapidly transitions among
redundant states.
HDP-HMM’s tendency to rapidly switch among redundant states leads to poor speaker
diarization performance.
In contrast, the methods we develop in Sec. 3.1 provide a general solution to the
problem of state persistence in HDP-HMMs, and, along with other model extensions,
yield a state-of-the-art speaker diarization method. The success on this challenging
dataset is a profound demonstration of the eﬃcacy of our methods for practical appli-
cations. The approach is easily stated—we simply augment the HDP-HMM to include
a parameter for self-transition bias, and place a separate prior on this parameter. The
challenge is to execute this idea coherently in a Bayesian nonparametric framework.
Earlier papers have also proposed self-transition parameters for HMMs with inﬁnite
state spaces [11, 186], but did not formulate general solutions that integrate fully with
Bayesian nonparametric inference.
Another goal of this chapter, which we explore in Sec. 3.3, is to develop a more
fully nonparametric version of the HDP-HMM in which the emission distribution (the
conditional distribution of observations given states) as well as the transition distribu-
tion is treated nonparametrically. This is again motivated by applications—classical
applications of HMMs often ﬁnd it necessary to use ﬁnite Gaussian mixtures as emis-
sion distributions in order to cope with multimodality. In the nonparametric setting

Sec. 3.1.
The HDP-HMM and Its Sticky Extension
109
it is natural to replace these ﬁnite mixtures with Dirichlet process mixtures (or with
hierarchical Dirichlet process mixtures so as to tie emission distributions across states).
Unfortunately, this idea is not viable in practice, because of the tendency of the HDP-
HMM to rapidly switch between redundant states.
By incorporating an additional
self-transition bias, however, it is possible to make use of Dirichlet process mixtures for
the emission distributions.
An important reason for the popularity of the classical HMM is its computational
tractability. In particular, marginal probabilities and samples can be obtained from the
HMM via an eﬃcient dynamic programming algorithm known as the forward-backward
algorithm (see Sec. 2.6.1). In Sec. 3.1.3 and Sec. 3.3.2, we show that this algorithm
also plays an important role in computationally eﬃcient inference for our general-
ized HDP-HMM. In particular, we develop a blocked Gibbs sampler which leverages
forward–backward recursions to jointly resample the state and emission assignments
for all observations.
■3.1 The HDP-HMM and Its Sticky Extension
Recall the hidden Markov model, or HMM, of Sec. 2.6. Once again, let zt denote the
state of the Markov chain at time t and πj the state-speciﬁc transition distribution for
state j. The HDP described in Sec. 2.9.3 can be used to develop an HMM with an
inﬁnite state space—the HDP-HMM [162]. Conceptually, we envision a doubly-inﬁnite
transition matrix, with each row corresponding to a Chinese restaurant of the metaphor
introduced in Sec. 2.9.1. That is, the groups in the HDP formalism here correspond to
states, and each Chinese restaurant deﬁnes a distribution on next states. The Chinese
restaurant franchise (CRF) of Sec. 2.9.3 links these next-state distributions.
Thus,
in this application of the HDP, the group-speciﬁc distribution, πj, is a state-speciﬁc
transition distribution and, due to the inﬁnite state space, there are inﬁnitely many
such groups. Since zt ∼πzt−1, we see that zt−1 indexes the group to which yt is assigned
(i.e., all observations with zt−1 = j are assigned to group j). Just as with the HMM,
the current state zt then indexes the parameter θzt used to generate observation yt. The
generative model is summarized below, and is graphically depicted in Fig. 3.2(a).
β | γ ∼GEM(γ)
πj | β, α ∼DP(α, β)
j = 1, 2, . . .
θj | H, λ ∼H(λ)
j = 1, 2, . . .
zt|{πj}∞
j=1, zt−1 ∼πzt−1
t = 1, . . . , T
yt|{θj}∞
j=1, zt ∼F(θzt)
t = 1, . . . , T,
(3.1)
where we recall that GEM(·) denotes the stick-breaking construction (see Sec. 2.9.1).
By deﬁning πj ∼DP(α, β), the HDP prior encourages states to have similar transi-
tion distributions. Namely, utilizing Eq. (2.204), the state-speciﬁc transition distribu-

110
CHAPTER 3.
THE STICKY HDP-HMM
(a)
(b)
Figure
3.2.
(a) Graphical representation of the sticky HDP-HMM. The state evolves as
zt+1|{πk}∞
k=1, zt ∼πzt, where πk|α, κ, β ∼DP(α + κ, (αβ + κδk)/(α + κ)) and β|γ ∼GEM(γ), and
observations are generated as yt|{θk}∞
k=1, zt ∼F(θzt). The original HDP-HMM has κ = 0. (b) Sticky
HDP-HMM with DP emissions, where st indexes the state-speciﬁc mixture component generating ob-
servation yt. The DP prior dictates that st|{ψk}∞
k=1, zt ∼ψzt for ψk|σ ∼GEM(σ). The jth Gaussian
component of the kth mixture density is parameterized by θk,j so yt|{θk,j}∞
k,j=1, zt, st ∼F(θzt,st).
tions are identical in expectation1:
E[πjk | β] = βk.
(3.2)
Thus, the state-speciﬁc transition distributions πj share sparsity in the state space, as
induced by β. That is, the set of probable states visited from state i is related to that
of state j. However, as we see from Eq. (3.2), the HDP-HMM does not diﬀerentiate
self–transitions from moves between diﬀerent states. When modeling data with state
persistence, the ﬂexible nature of the HDP-HMM prior allows for state sequences with
unrealistically fast dynamics to have large posterior probability.
For example, with
multinomial emissions, a good explanation of the data is to divide diﬀerent observation
values into unique states and then rapidly switch between them (see Fig. 3.1). In such
cases, many models with redundant states may have large posterior probability, thus
impeding identiﬁcation of a compact dynamical model which best explains the obser-
vations. The problem is compounded by the fact that once this alternating pattern
has been instantiated by the sampler, its persistence is then reinforced by the proper-
ties of the Chinese restaurant franchise, thus slowing mixing rates. Furthermore, this
fragmentation of data into redundant states can reduce predictive performance, as is
discussed in Sec. 3.2. In many applications, one would like to be able to incorporate
prior knowledge that slow, smoothly varying dynamics are more likely.
To address these issues, we propose to instead sample transition distributions πj as
1In addition, the mean of these distributions, β, has, in expectation, a monotonically decreasing set
of weights due to properties of its stick-breaking construction (see Sec. 2.9.1).

Sec. 3.1.
The HDP-HMM and Its Sticky Extension
111
follows:
β | γ ∼GEM(γ)
πj | α, κ, β ∼DP

α + κ, αβ + κδj
α + κ

.
(3.3)
Here, (αβ + κδj) indicates that an amount κ > 0 is added to the jth component of αβ.
Informally, what we are doing is increasing the expected probability of self-transition
by an amount proportional to κ. Speciﬁcally, once again using Eq. (2.204), we see that
the expected set of weights for transition distribution πj is a convex combination of
those deﬁned by β and state-speciﬁc weight deﬁned by κ:
E[πjk | β, κ] =
α
α + κβk +
κ
α + κδ(j, k).
(3.4)
More formally, over a ﬁnite partition (Z1, . . . , ZK) of the positive integers Z+, the deﬁ-
nition of the Dirichlet process in Theorem 2.9.1 dictates that the prior on the measure
πj adds an amount κ only to the arbitrarily small partition containing j, corresponding
to a self-transition. That is,
(πj(Z1), . . . , πj(ZK)) | α, β ∼Dir(αβ(Z1) + κδj(Z1), . . . , αβ(ZK) + κδj(ZK))
(3.5)
When κ = 0 the original HDP-HMM of Teh et al. [162] is recovered. Because positive
κ values increase the prior probability E[πjj | β] of self–transitions, we refer to this
extension as the sticky HDP-HMM. See Fig. 3.2(a).
The κ parameter is reminiscent of the self-transition bias parameter of the inﬁnite
HMM [11], a precursor of the HDP-HMM. The inﬁnite HMM employs a two-level urn
model. The top-level process places a probability on transitions to existing states in
proportion to how many times these transitions have been seen, with an added bias
towards a self-transition even if this has not previously occurred. With some remaining
probability an oracle is called, representing the second-level urn. This oracle chooses an
existing state in proportion to how many times the oracle previously chose that state,
regardless of the state transition involved, or chooses a previously unvisited state. The
oracle is included so that newly instantiated states may be visited from all currently
instantiated states. While this urn model is an appealing description of probabilities
on transitions, the lack of an underlying random measure makes it diﬃcult to specify a
coherent Bayesian inference procedure, and indeed the inﬁnite HMM of Beal et al. [11]
relies on a heuristic approximation to a Gibbs sampler. The full connection between
HMMs on an inﬁnite state space and an underlying Bayesian nonparametric prior,
as well as the development of a coherent inference algorithm, was made in [162], but
without the inclusion of a self-transition parameter (and hence with the potential pitfalls
mentioned previously.)
■3.1.1 Chinese Restaurant Franchise with Loyal Customers
We extend the Chinese restaurant metaphor of Sec. 2.9.1 and Sec. 2.9.3 to the sticky
HDP-HMM, where our franchise now has restaurants with loyal customers. In addition

112
CHAPTER 3.
THE STICKY HDP-HMM
1 
2 
2 
3 
1 
2 
f(1) = 01 
f(2) = 11 
f(3) = 21 
f(4) = 22 
f(5) = 31 
f(6) = 12 
y1
y2
y3
y4
y5
y6
(a)
(b)
Figure 3.3. (a) Graph of CRF with loyal customers. Customers yji sit at table tji|˜πj ∼˜πj. Each table
considers a dish ¯kjt|β ∼β, but override variables wjt|α, κ ∼Ber(κ/(α + κ)) can force the served dish
kjt to be j. (b) Mapping of time indices to CRF restaurant indices, with the state sequence labeled
with a ﬁxed set of assignments z1:6 = [1, 2, 2, 3, 1, 2]. For example, y4 is seated in restaurant j = 2
(since z3 = 2 implies z4 ∼π2), and is the second customer to be seated in that restaurant. As such, y4
is assigned a restaurant index y22. Observation y1 is assigned to a special initial restaurant j = 0 due
to the fact that z1 is drawn from an initial distribution π0.
to providing intuition for the predictive distribution on assignment variables, developing
this metaphor aids in constructing the Gibbs samplers of Sec. 3.1.2 and Sec. 3.1.3. In
the CRF with loyal customers, each restaurant in the franchise has a specialty dish with
the same index as that of the restaurant. Although this dish is served elsewhere, it is
more popular in the dish’s namesake restaurant. We see this increased popularity in
the specialty dish from the fact that a table’s dish is now drawn from the modiﬁed dish
ratings, namely,
kjt | α, κ, β ∼αβ + κδj
α + κ
.
(3.6)
Speciﬁcally, we note that each restaurant has a set of restaurant-speciﬁc ratings of the
buﬀet line that redistributes the shared ratings β so that there is more weight on the
house-specialty dish.
Recall that while customers in the CRF of the HDP are pre-partitioned into restau-
rants based on the ﬁxed group assignments, in the HDP-HMM the value of the state
zt−1 determines the group assignment (and thus restaurant) of customer yt. Therefore,
we will describe a generative process that ﬁrst assigns customers to restaurants and
then assigns customers to tables and dishes. We will refer to zt as the parent and zt+1
as the child. The parent enters a restaurant j determined by its parent (the grand-
parent), zt−1 = j. We assume there is a bijective mapping f : t →ji of time indices
t to restaurant/customer indices ji. See Fig. 3.3(b) for an example. The parent then
chooses a table tji ∼˜πj and that table is served a dish indexed by kjt. Noting that
zt = zji = kjtji (i.e., the value of the state is the dish index), the increased popularity of

Sec. 3.1.
The HDP-HMM and Its Sticky Extension
113
the house specialty dish implies that children are more likely to eat in the same restau-
rant as their parent and, in turn, more likely to eat the restaurant’s specialty dish. This
develops family loyalty to a given restaurant in the franchise. However, if the parent
chooses a dish other than the house specialty, the child will then go to the restaurant
where this dish is the specialty and will in turn be more likely to eat this dish, too.
One might say that for the sticky HDP-HMM, children have similar tastebuds to their
parents and will always go the restaurant that prepares their parent’s dish best. Often,
this keeps many generations eating in the same restaurant.
The inference algorithm for the sticky HDP-HMM, which is derived in Sec. 3.1.2, is
simpliﬁed if we introduce a set of auxiliary random variables ¯kjt and wjt as follows:
¯kjt | β ∼β
wjt | α, κ ∼Ber

κ
α + κ

≜Ber(ρ)
(3.7)
kjt | ¯kjt, wjt =
( ¯kjt,
wjt = 0;
j,
wjt = 1,
where Ber(ρ) represents the Bernoulli distribution with parameter ρ. Here, we have
deﬁned a self-transition parameter ρ = κ/(α + κ). The table ﬁrst chooses a dish ¯kjt
without taking the restaurant’s specialty into consideration (i.e., the original CRF).
With some probability, this considered dish is overridden (perhaps by a waiter’s sug-
gestion) and the table is served the specialty dish j. Thus, kjt represents the served
dish. We refer to wjt as the override variable. For the original HDP-HMM, when κ = 0,
the considered dish is always the served dish since wjt = 0 for all tables. This generative
process is depicted in Fig. 3.4(a). Our inference algorithm, described in Sec. 3.1.2, aims
to infer these variables conditioned on knowledge of the served dishes kjt. For example,
if the served dish of table t in restaurant j is indexed by j, the house specialty, the origin
of this dish may either have been from considering ¯kjt = j or having been overridden
by wjt = 1. See Fig. 3.4(b).
A table-dish representation of the sticky HDP-HMM, analogous to that of Fig. 2.13
for the HDP, is shown in Fig. 3.3(a). Although not explicitly represented in this graph,
the sticky HDP-HMM still induces a Markov structure on the indicator random variables
zt, which, based on the value of the parent state zt−1, are mapped to a group-speciﬁc
index ji. This process is depicted in Fig. 3.3(b).
One can derive a distribution on
partitions by marginalizing over the stick-breaking distributed measures ˜πj and β, just
as in the HDP (see Eq. (2.232)). The CRF with loyal customers is then described by:
p(tji | tj1, . . . , tji−1, α, κ) ∝
Tj
X
t=1
˜njtδ(tji, t) + (α + κ)δ(tji, Tj + 1)
(3.8)
p(¯kjt | ¯k1, . . . , ¯kj−1, ¯kj1, . . . , ¯kjt−1, γ) ∝
¯
K
X
k=1
¯m·kδ(¯kjt, k) + γδ(¯kjt, ¯K + 1),

114
CHAPTER 3.
THE STICKY HDP-HMM
(a) Generative
(b) Inference
Figure 3.4. (a) Generative model of considered dish indices ¯kjt (top) being converted to served dish
indices kjt (bottom) via override variables wjt. (b) Perspective from the point of view of an inference
algorithm that must infer ¯kjt and wjt given kjt. If kjt ̸= j, then the override variable wjt is automatically
0 implying that ¯kjt = kjt, as indicated by the jagged arrow. If instead kjt = j, then this could have
arisen from the considered dish ¯kjt being overridden (wjt = 1) or not (wjt = 0).
These scenarios
are indicated by the dashed arrow. If the considered dish was not overridden, then ¯kjt = kjt = j.
Otherwise, ¯kjt could have taken any value, as denoted by the question mark.
where ¯mjk is the number of tables in restaurant j that considered dish k, and ¯K the
number of unique considered dishes in the franchise.
The distributions on wjt and
kjt remain as before, so that considered dishes are sometimes overridden by the house
specialty.
■3.1.2 Sampling via Direct Assignments
In this section we present an inference algorithm for the sticky HDP-HMM of Sec. 3.1
and Fig. 3.2(a) that is a modiﬁed version of the direct assignment collapsed Gibbs
sampler of [162]. This sampler circumvents the complicated bookkeeping of the CRF by
sampling indicator random variables directly. That is, the sampler uses the condensed
indicator variable representation of the HDP instead of the table-dish representation
(see Sec. 2.9.3). The resulting sticky HDP-HMM direct assignment Gibbs sampler is
outlined in Algorithm 9, with the full derivation presented in Appendix A.
The basic idea is that we marginalize over the inﬁnite set of state-speciﬁc transition
distributions πk and parameters θk, and sequentially sample the state zt given all other
state assignments z\t, the observations y1:T , and the global transition distribution β. A
variant of the Chinese restaurant process gives us the prior probability of an assignment
of zt to a value k based on how many times we have seen other transitions from the
previous state value zt−1 to k and k to the next state value zt+1. We denote the number
of transitions from j to k in z1:T by njk. As presented in Algorithm 9, this conditional
distribution is dependent upon whether either or both of the transitions zt−1 to k and k
to zt+1 correspond to a self-transition, most strongly when κ > 0. The prior probability
of an assignment of zt to state k is then weighted by the likelihood of the observation
yt given all other observations assigned to state k.

Sec. 3.1.
The HDP-HMM and Its Sticky Extension
115
0
200
400
600
800
1000
−15
−10
−5
0
5
10
Time
Observations
0
200
400
600
800
1000
0
2
4
6
8
10
12
Time
Estimated Mode Sequence
(a)
(b)
Figure 3.5. Plots showing the sequential Gibbs sampler splitting temporally separated examples of the
same true state into multiple estimated states. (a) Observation sequence. (b) Example of the estimated
HMM state sequence (blue) at Gibbs iteration 1000 overlayed on the true HMM state sequence (red).
Here, we see that a single true state is divided into multiple estimated states, each with high probability
of self-transition.
Given a sample of the state sequence z1:T , we can represent the posterior distribution
of the global transition distribution β via a set of auxiliary random variables ¯mjk, mjk,
and wjt, which correspond to the jth restaurant-speciﬁc set of table counts for each
considered dish and served dish, and override variables of the CRF with loyal customers,
respectively. The Gibbs sampler of Algorithm 9 iterates between sequential sampling
of the state zt for each individual value of t given β and z\t, sampling of the auxiliary
variables ¯mjk, mjk, and wjt given z1:T and β, and sampling of β given these auxiliary
variables.
■3.1.3 Blocked Sampling of State Sequences
The HDP-HMM sequential, direct assignment sampler of Algorithm 9 can exhibit slow
mixing rates since global state sequence changes are forced to occur coordinate by co-
ordinate.
This phenomenon is explored in [148] for the ﬁnite HMM. Although the
sticky HDP-HMM reduces the posterior uncertainty caused by fast state-switching ex-
planations of the data, the self-transition bias can cause two continuous and temporally
separated sets of observations of a given state to be assigned to two distinct states. See
Fig. 3.5 for an example. If this occurs, the high probability of self-transition makes it
challenging for the sequential sampler to assign those two examples to a common state.
Alternatively, we propose using a variant of the HMM forward-backward procedure
described in Sec. 2.6.1 to harness the Markovian structure and jointly sample the state
sequence z1:T given the observations y1:T , transition probabilities πk, and parameters θk.
To take advantage of this procedure, we now must sample the previously marginalized
transition distributions and model parameters. In practice, this requires approximating
the countably inﬁnite transition distributions using one of the approaches outlined in
Sec. 2.9.2. For this chapter, we choose to use the weak limit approximation because of

Given the previous state assignments z(n−1)
1:T
and global transition distribution β(n−1):
1. Set z1:T = z(n−1)
1:T
and β = β(n−1). For each t ∈{1, . . . , T}, sequentially
(a) Decrement nzt−1zt and nztzt+1 and remove yt from the cached statistics for
the current assignment zt = k:
(ˆµk, ˆΣk) ←(ˆµk, ˆΣk) ⊖yt
ˆνk ←ˆνk −1
(b) For each of the K currently instantiated states, determine
fk(yt) = (αβk + nzt−1k)
αβzt+1 + nkzt+1 + κδ(k, zt+1)
α + nk· + κ

tˆνk(yt; ˆµk, ˆΣk)
for zt−1 ̸= k, otherwise see Eq. (A.10). Also determine probability fK+1(yt)
of a new state K + 1.
(c) Sample the new state assignment zt:
zt ∼
K
X
k=1
fk(yt)δ(zt, k) + fK+1(yt)δ(zt, K + 1)
If zt = K + 1, increment K and transform β as follows. Sample b ∼Beta(1, γ)
and assign βK ←bβ˜k and β˜k ←(1 −b)β˜k, where β˜k = P∞
k=K+1 βk.
(d) Increment nzt−1zt and nztzt+1 and add yt to the cached statistics for the new
assignment zt = k:
(ˆµk, ˆΣk) ←(ˆµk, ˆΣk) ⊕yt
ˆνk ←ˆνk + 1
2. Fix z(n)
1:T = z1:T . If there exists a j such that nj· = 0 and n·j = 0, remove j and
decrement K.
3. Sample auxiliary variables m, w, and ¯
m as follows:
(a) For each (j, k) ∈{1, . . . , K}2, set mjk = 0 and n = 0. For each customer in
restaurant j eating dish k, that is for n = 1, . . . , njk, sample
x ∼Ber

αβk + κδ(j, k)
n + αβk + κδ(j, k)

Increment n, and if x = 1 increment mjk.
(b) For each j ∈{1, . . . , K}, sample the number of override variables in
restaurant j:
wj· ∼Binomial
 mjj, ρ(ρ + βj(1 −ρ))−1
,
Set the number of informative tables in restaurant j considering dish k to:
¯mjk =
(
mjk,
j ̸= k;
mjj −wj·,
j = k.
4. Sample the global transition distribution from
β(n) ∼Dir( ¯m·1, . . . , ¯m·K, γ)
5. Optionally, resample hyperparameters γ, α, and κ as described in Appendix C.
Algorithm 9. Direct assignment collapsed Gibbs sampler for the sticky HDP-HMM. The al-
gorithm for the HDP-HMM follows directly by setting κ = 0.
Here, we assume Gaussian
observations with a normal-inverse-Wishart prior on the parameters of these distributions (see
Appendix A). The ⊕and ⊖operators update cached mean and covariance statistics as assign-
ments are added or removed from a given component.

Sec. 3.2.
Experiments with Synthetic Data
117
its simplicity and computational eﬃciency. That is,
β | γ ∼Dir(γ/L, . . . , γ/L)
πj | α, κ, β ∼Dir(αβ1, . . . , αβj + κ, . . . , αβL),
(3.9)
where L is the chosen truncation level.
The Gibbs sampler using blocked resampling of z1:T is outlined in Algorithm 10,
with derivations found in Appendix B. A similar sampler has been used for inference in
HDP hidden Markov trees [97]. However, this work did not consider the complications
introduced by multimodal emissions, which we explore in Sec. 3.3. Recently, a slice sam-
pler, referred to as beam sampling [170], has been developed for the HDP-HMM. This
sampler harnesses the eﬃciencies of the forward-backward algorithm without having to
ﬁx a truncation level for the HDP. However, as we elaborate upon in Sec. 3.2.1, this
sampler can suﬀer from slower mixing rates than our blocked sampler, which utilizes a
ﬁxed-order, weak limit truncation of the HDP-HMM.
■3.1.4 Hyperparameters
We treat the hyperparameters in the sticky HDP-HMM as unknown quantities and
perform full Bayesian inference over these quantities. This emphasizes the role of the
data in determining the number of occupied states and the degree of self-transition bias.
Our derivation of sampling updates for the hyperparameters of the sticky HDP-HMM
is presented in Appendix C; it roughly follows that of the original HDP-HMM [162].
A key step which simpliﬁes our inference procedure is to note that since we have the
deterministic relationships
α = (1 −ρ)(α + κ)
κ = ρ(α + κ),
(3.10)
we can treat ρ and α + κ as our hyperparameters and sample these values instead of
sampling α and κ directly.
■3.2 Experiments with Synthetic Data
In this section, we explore the performance of the sticky HDP-HMM relative to the
original model (i.e., with the self-transition bias κ = 0) in a series of experiments with
synthetic data. We judge performance according to two metrics: our ability to accu-
rately segment the data according to the underlying state sequence, and the predictive
likelihood of held-out data under the inferred model. We additionally empirically assess
the improvements in mixing rate achieved by using the blocked sampler of Sec. 3.1.3.
The results of Sec. 3.2.1 primarily demonstrate that the sticky HDP-HMM more rapidly
ﬁnds good segmentations of data with state persistence; in Sec. 3.2.2, the diﬀerence in
overall modeling power becomes apparent.

Given a previous set of state-speciﬁc transition probabilities π(n−1), the global
transition distribution β(n−1), and emission parameters θ(n−1):
1. Set π = π(n−1) and θ = θ(n−1). Working sequentially backwards in time,
calculate messages mt,t−1(k) :
(a) For each k ∈{1, . . . , L}, initialize messages to
mT+1,T(k) = 1
(b) For each t ∈{T −1, . . . , 1} and for each k ∈{1, . . . , L}, compute
mt,t−1(k) =
L
X
j=1
πk(j)N(yt; µj, Σj)mt+1,t(j)
2. Sample state assignments z1:T working sequentially forward in time, starting
with njk = 0 and Yk = ∅for each (j, k) ∈{1, . . . , L}2:
(a) For each k ∈{1, . . . , L}, compute the probability
fk(yt) = πzt−1(k)N(yt; µk, Σk)mt+1,t(k)
(b) Sample a state assignment zt:
zt ∼
L
X
k=1
fk(yt)δ(zt, k)
(c) Increment nzt−1zt and add yt to the cached statistics for the new assignment
zt = k:
Yk ←Yk ⊕yt
3. Sample the auxiliary variables m, w, and ¯
m as in step 3 of Algorithm 9.
4. Update the global transition distribution by sampling
β ∼Dir(γ/L + ¯m·1, . . . , γ/L + ¯m·L)
5. For each k ∈{1, . . . , L}, sample a new transition distribution and emission
parameter based on the sampled state assignments
πk
∼
Dir(αβ1 + nk1, . . . , αβk + κ + nkk, . . . , αβL + nkL)
θk
∼
p(θ | λ, Yk)
See Appendix B.4.1 for details on resampling θk.
6. Fix π(n) = π, β(n) = β, and θ(n) = θ.
7. Optionally, resample hyperparameters γ, α, and κ as described in Appendix C.
Algorithm 10. Blocked Gibbs sampler for the sticky HDP-HMM. The algorithm for the original
HDP-HMM follows directly by setting κ = 0. Here, we assume Gaussian observations with an
independent Gaussian prior on the mean and inverse-Wishart (IW) prior on the covariance
(see Appendix B.4.1). The set Yk is comprised of the statistics obtained from the observations
assigned to state k that are necessary for updating the parameter θk = {µk, Σk}. The ⊕operator
updates these cached statistics as a new assignment is made.

Sec. 3.2.
Experiments with Synthetic Data
119
0
200
400
600
800
1000
−20
−15
−10
−5
0
5
10
15
Time
Observations
0.7
0.75
0.8
0.85
0.9
0.95
1
0
1000
2000
3000
4000
5000
6000
7000
8000
rho
Counts
(a)
(b)
0
0.5
1
1.5
2
2.5
3
x 10
4
0
0.05
0.1
0.15
0.2
0.25
Iteration
Normalized Hamming Distance
0
0.5
1
1.5
2
2.5
3
x 10
4
0
0.05
0.1
0.15
0.2
0.25
Iteration
Normalized Hamming Distance
(c)
(d)
Figure 3.6. (a) Observation sequence (blue) and true state sequence (red) for a three-state HMM with
state persistence. (b) Histogram of the inferred self-transition proportion parameter, ρ, for the sticky
HDP-HMM blocked sampler. (c) Hamming distance over 30,000 Gibbs samples from three chains using
the sticky HDP-HMM blocked sampler. (d) Similar plot for the original HDP-HMM.
■3.2.1 Gaussian Emissions
We begin our analysis of the sticky HDP-HMM performance by examining a set of
simulated data generated from an HMM with Gaussian emissions. The ﬁrst dataset is
generated from an HMM with a high probability of self-transition. The second dataset
is from an HMM with a high probability of leaving the current state. In this scenario,
our goal is to demonstrate that the sticky HDP-HMM is still able to capture rapid
dynamics by inferring a small probability of self-transition.
For all of the experiments with simulated data, we used weakly informative hy-
perpriors. We placed a Gamma(1, 0.01) prior on the concentration parameters γ and
(α + κ). The self-transition proportion parameter ρ was given a Beta(10, 1) prior. The
parameters of the base measure were set from the data, as will be described for each
scenario.
State Persistence
The data for the high persistence case were generated from a three-state HMM with a
0.98 probability of self-transition and equal probability of transitions to the other two
states. The observation and true state sequences for the state persistence scenario are

120
CHAPTER 3.
THE STICKY HDP-HMM
200
400
600
800
1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iteration
Normalized Hamming Distance
200
400
600
800
1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iteration
Normalized Hamming Distance
(a)
(b)
200
400
600
800
1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iteration
Normalized Hamming Distance
200
400
600
800
1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iteration
Normalized Hamming Distance
(c)
(d)
Figure 3.7.
For the observation sequence of Fig. 3.6, the median (solid blue) and 10th and 90th
quantiles (dashed red) of Hamming distance between the true and estimated state sequences over the
ﬁrst 1,000 Gibbs samples from 200 chains are shown for the (a) sticky HDP-HMM direct assignment
sampler, (b) original HDP-HMM direct assignment sampler, (c) sticky HDP-HMM blocked sampler,
and (d) original HDP-HMM blocked sampler.
shown in Fig. 3.6(a). We placed a normal inverse-Wishart (NIW) prior (see Sec. 2.4.3)
on the space of mean and variance parameters and set the hyperparameters as: 0.01
pseudocounts, mean equal to the empirical mean, three degrees of freedom, and scale
matrix equal to 0.75 times the empirical variance. We used this conjugate base measure
so that we may directly compare the performance of the blocked and direct assignment
samplers of Algorithms 9 and 10. For the blocked sampler, we used a truncation level
of L = 20.
In Fig. 3.7(a)-(d), we plot the 10th, 50th, and 90th quantiles of the Hamming distance
between the true and estimated state sequences over the 1000 Gibbs iterations using
the direct assignment and blocked samplers on the sticky and original HDP-HMM
models. To calculate the Hamming distance, we used the Munkres algorithm [121] to
map the randomly chosen indices of the estimated state sequence to the set of indices
that maximize the overlap with the true sequence.
From these plots, we see that the burn-in rate of the blocked sampler using the sticky
HDP-HMM is signiﬁcantly faster than that of any other sampler-model combination.
As expected, the sticky HDP-HMM with the sequential, direct assignment sampler
(Algorithm 9) gets stuck in state sequence assignments from which it is hard to move

Sec. 3.2.
Experiments with Synthetic Data
121
200
400
600
800
1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iteration
Normalized Hamming Distance
0
0.5
1
1.5
2
2.5
3
x 10
4
0
0.05
0.1
0.15
0.2
0.25
Iteration
Normalized Hamming Distance
0
10
20
30
40
50
0
2
4
6
8
10
12
Leff
Log Counts
(a)
(b)
(c)
Figure 3.8. For the beam sampler, we plot: (a) the median (solid blue) and 10th and 90th quantiles
(dashed red) of the Hamming distance between the true and estimated state sequences over the ﬁrst
1,000 Gibbs samples from 200 chains, and (b) the Hamming distance over 30,000 Gibbs samples from
three chains. (c) Histogram of the eﬀective beam sampler truncation level, Leff, over the 30,000 Gibbs
iterations from the three chains (blue) compared to the ﬁxed truncation level, L = 20, used above (red).
away, as conveyed by the ﬂatness of the Hamming error versus iteration number plot in
Fig. 3.7(a). For example, the estimated state sequence of Fig. 3.5(b) might have similar
parameters associated with states 3, 7, 10 and 11 so that the likelihood is in essence
the same as if these states were grouped, but this sequence has a large error in terms of
Hamming distance and it would take many iterations to move away from this assign-
ment. Incorporating the blocked sampler with the original HDP-HMM improves the
Hamming distance performance relative to the sequential, direct assignment sampler for
both the original and sticky HDP-HMM; however, the burn-in rate is still substantially
slower than that of the blocked sampler on the sticky model (Algorithm 10).
Recently, a beam sampling algorithm [170] has been proposed which adapts slice
sampling methods [142] to the HDP-HMM. This approach uses a set of auxiliary slice
variables, one for each observation, to eﬀectively truncate the number of state transi-
tions that must be considered at every Gibbs sampling iteration. Dynamic programming
methods can then be used to jointly resample state assignments. The beam sampler
was inspired by a related approach for DP mixture models [177], which is conceptu-
ally similar to retrospective sampling methods [131]. In comparison to our ﬁxed-order,
weak limit truncation of the HDP-HMM, the beam sampler provides an asymptoti-
cally exact algorithm. However, the beam sampler can be slow to mix relative to our
blocked sampler on the ﬁxed, truncated model (see Fig. 3.8 for a comparison on the
high persistence dataset examined above.) The issue is that in order to consider a tran-
sition which has low prior probability, one needs a correspondingly rare slice variable
sample at that time. Thus, even if the likelihood cues are strong, to be able to con-
sider state sequences with several low-prior-probability transitions, one needs to wait
for several rare events to occur when drawing slice variables. By considering the full,
exponentially large set of paths in the truncated state space, we avoid this problem.
Of course, the trade-oﬀbetween the computational cost of the blocked sampler on the
ﬁxed, truncated model (O(TL2)) and the slower mixing rate of the beam sampler yields

122
CHAPTER 3.
THE STICKY HDP-HMM
an application-dependent sampler choice.
The Hamming distance plots of Fig. 3.8(a) and (b), when compared to those of
Fig. 3.6 and Fig. 3.7, depict the substantially slower mixing rate of the beam sampler
than the blocked sampler. However, the theoretical computational beneﬁt of the beam
sampler can be seen in Fig. 3.8(c). In this plot, we present a histogram of the eﬀective
truncation level, Leff, used over the 30,000 Gibbs iterations on three chains.
We
computed this eﬀective truncation level by summing over the number of state transitions
considered during a full sweep of sampling z1:T and then dividing this number by the
length of the dataset, T, and taking the square root. On a more technical note, our ﬁxed,
truncated model allows for more vectorization of the code than the beam sampler. Thus,
in practice, the diﬀerence in computation time between the samplers is signiﬁcantly less
than the O(L2/L2
eff) factor obtained by counting state transitions.
From this point onwards, we present results only from blocked sampling since we
have seen the clear advantages of this method over the sequential, direct assignment
sampler.
Fast State-Switching
In order to warrant the general use of the sticky model, one would like to know that the
incorporated sticky parameter does not preclude learning models with fast dynamics. To
this end, we explore the performance of the sticky HDP-HMM on data generated from
a model with a high probability of switching between states. Speciﬁcally, we generated
observations from a four-state HMM with the following transition probability matrix:


0.4
0.4
0.1
0.1
0.4
0.4
0.1
0.1
0.1
0.1
0.4
0.4
0.1
0.1
0.4
0.4

.
(3.11)
We once again used a truncation level L = 20.
Since we are restricting ourselves
to the blocked Gibbs sampler, it is no longer necessary to use a conjugate base mea-
sure2. Instead, we placed an independent Gaussian prior on the mean parameter and an
inverse-Wishart prior on the variance parameter. Since we can no longer jointly sample
the mean and variance from their posterior, our sampler now relies on iterating between
sampling the mean given the variance, and the variance given the mean, each of which
yields closed-form posteriors. See Appendix B.4.1 for more details. For the Gaussian
prior, we set the mean and variance hyperparameters to be equal to the empirical mean
and variance of the entire dataset. The inverse-Wishart hyperparameters were set such
that the expected variance is equal to 0.75 times that of the entire dataset, with three
degreee of freedom.
The results depicted in Fig. 3.9 conﬁrm that by inferring a small probability of self-
transition, the sticky HDP-HMM is indeed able to capture fast HMM dynamics, and just
2Conjugate base measures can impose restrictive assumptions, such as the scaling of the variance
with the mean in the case of the NIW prior.

Sec. 3.2.
Experiments with Synthetic Data
123
0
200
400
600
800
1000
−40
−30
−20
−10
0
10
20
30
Time
Observations
0
0.2
0.4
0.6
0.8
1
0
500
1000
1500
2000
2500
3000
3500
4000
rho
Counts
(a)
(b)
0
0.5
1
1.5
2
2.5
3
x 10
4
0
0.05
0.1
0.15
0.2
Iteration
Normalized Hamming Distance
0
0.5
1
1.5
2
2.5
3
x 10
4
0
0.05
0.1
0.15
0.2
Iteration
Normalized Hamming Distance
(c)
(d)
200
400
600
800
1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iteration
Normalized Hamming Distance
200
400
600
800
1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iteration
Normalized Hamming Distance
(e)
(f)
Figure 3.9.
(a) Observation sequence (blue) and true state sequence (red) for a four-state HMM
with fast state switching.
(b) Histogram of the inferred self-transition parameter, ρ, for the sticky
HDP-HMM blocked sampler. (c) Hamming distance over 30,000 Gibbs samples from three chains are
shown for the sticky HDP-HMM blocked sampler. (d) Similar plot for the original HDP-HMM. The
median (solid blue) and 10th and 90th quantiles (dashed red) of Hamming distance between the true
and estimated state sequences over the ﬁrst 1,000 Gibbs samples from 200 chains are shown for the (e)
sticky HDP-HMM and (f) original HDP-HMM using the blocked sampler.
as quickly as the original HDP-HMM (although with higher variability.) Speciﬁcally, we
see that the histogram of the self-transition proportion parameter ρ for this dataset (see
Fig. 3.9(b)) is centered around a value close to the true probability of self-transition,
which is substantially lower than the mean value of this parameter on the data with
high persistence (Fig. 3.6(b).)

124
CHAPTER 3.
THE STICKY HDP-HMM
■3.2.2 Multinomial Emissions
In Fig. 3.6(c)-(d) and Fig. 3.9(c)-(d), we display the Hamming distance associated with
three chains over 30,000 Gibbs iterations for both the sticky and original HDP-HMM
using blocked sampling on the data of Sec. 3.2.1.
From these plots, we do not see
the diﬀerences between the models that were present at burn-in (see Fig. 3.7). The
diﬀerence in modeling power, rather than simply burn-in rate, between the sticky and
original HDP-HMM is more pronounced when we consider multinomial emissions. This
is because the multinomial observations are embedded in a discrete topological space
in which there is no concept of similarity between non-identical observation values. In
contrast, Gaussian emissions have a continuous range of values in Rn with a clear notion
of closeness between observations under the Lebesgue measure, aiding in grouping
observations under a single HMM state’s Gaussian emission distribution, even in the
absence of a self-transition bias.
To demonstrate the increased posterior uncertainty with discrete observations, we
generated data from a ﬁve-state HMM with multinomial emissions with a 0.98 proba-
bility of self-transition and equal probability of transitions to the other four states. The
vocabulary, or range of possible observation values, was set to 20. The observation and
true state sequences are shown in Fig. 3.10(a). We placed a symmetric Dirichlet prior
on the parameters of the multinomial distribution, with the Dirichlet hyperparameters
equal to 2 (i.e., Dir(2, . . . , 2).)
From Fig. 3.10, we see that even after burn-in, many fast-switching state sequences
have signiﬁcant posterior probability under the non-sticky model leading to sweeps
through regions of larger Hamming distance error.
A qualitative plot of one such
inferred sequence after 30,000 Gibbs iterations is shown in Fig. 3.1(c). Such sequences
have negligible posterior probability under the sticky HDP-HMM formulation.
In some applications, such as the speaker diarization problem that is explored in
Sec. 3.5, one cares about the inferred segmentation of the data into a set of state labels.
In this case, the advantage of incorporating the sticky parameter is clear. However, it
is often the case that the metric of interest is the predictive power of the learned model,
not the accuracy of the inferred state sequence. To study performance under this metric,
we simulated 10 test sequences using the same parameters that generated the training
sequence. We then computed the likelihood of each of the test sequences under the set
of parameters inferred at every 100th Gibbs iteration from iterations 10,000 to 30,000.
This likelihood was computed by running the forward-backward algorithm described
in Sec. 2.6.1. We plot these results as a histogram in Fig. 3.10(b). From this plot,
we see that the fragmentation of data into redundant HMM states can also degrade
the predictive performance of the inferred model. Thus, the sticky parameter plays an
important role in the Bayesian nonparametric learning of HMMs even in terms of model
averaging and predictive power.

Sec. 3.2.
Experiments with Synthetic Data
125
0
200
400
600
800
1000
−5
0
5
10
15
20
Time
Observations
−3050
−3000
−2950
−2900
−2850
−2800
0
50
100
150
200
250
Log−likelihood
Counts
 
 
Sticky HDP−HMM
Original HDP−HMM
(a)
(b)
0
0.5
1
1.5
2
2.5
3
x 10
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iteration
Normalized Hamming Distance
0
0.5
1
1.5
2
2.5
3
x 10
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iteration
Normalized Hamming Distance
(c)
(d)
Figure 3.10.
(a) Observation sequence (blue) and true state sequence (red) for a ﬁve-state HMM
with multinomial observations. (b) Histogram of the predictive probability of test sequences using the
inferred parameters sampled every 100th iteration from Gibbs iterations 10,000 to 30,000 for the sticky
and original HDP-HMM. The Hamming distances over 30,000 Gibbs samples from three chains are
shown for the (b) sticky HDP-HMM and (c) original HDP-HMM.
■3.2.3 Comparison to Independent Sparse Dirichlet Prior
We have alluded to the fact that the shared sparsity of the HDP-HMM induced by β
is essential for inferring sparse representations of the data. Although this is clear from
the perspective of the prior model, or equivalently the generative process, it is not im-
mediately obvious how much this hierarchical Bayesian constraint helps us in posterior
inference. Once we are in the realm of considering a ﬁxed, truncated approximation
to the HDP-HMM, one might propose an alternate model in which we simply place
a sparse Dirichlet prior, Dir(α/L, . . . , α/L) with α/L < 1, independently on each row
of the transition matrix. This is equivalent to setting β = [1/L, . . . , 1/L] in the trun-
cated HDP-HMM, which can also be achieved by letting the hyperparameter γ tend to
inﬁnity. Indeed, when the data do not exhibit shared sparsity or when the likelihood
cues are suﬃciently strong, the independent sparse Dirichlet prior model can perform
as well as the truncated HDP-HMM. However, in scenarios such as the one depicted in
Fig. 3.11, we see substantial diﬀerences in performance by considering the HDP-HMM,
as well as the inclusion of the sticky parameter. We explore the relative performance of
the HDP-HMM and sparse Dirichlet prior model, with and without the sticky param-
eter, on such a Markov model with multinomial emissions on a vocabulary of size 20.

126
CHAPTER 3.
THE STICKY HDP-HMM
0.6
0.7
0.7
0.7
0.2
0.2
0.2
0.05
0.05
0.05
Figure 3.11.
State transition diagram for a nine-state HMM with one main state (labeled 1) and
eight sub-states (labeled 2 to 9.) All states have a signiﬁcant probability of self-transition. From the
main state, all other states are equally likely. From a sub-state, the most likely non-self-transition is a
transition is back to the main state. However, all sub-states have a small probability of transitioning
to another sub-state, as indicated by the dashed arcs.
We placed a Dir(0.1, . . . , 0.1) prior on the parameters of the multinomial distribution.
For the sparse Dirichlet prior model, we assumed a state space of size 50, which is the
same as the truncation level we chose for the HDP-HMM (i.e., L = 50). The results are
presented in Fig. 3.12. From these plots, we see that the hierarchical Bayesian approach
of the HDP-HMM does, in fact, improve the learning of a model with shared sparsity.
The HDP-HMM consistently infers fewer HMM states and more representative model
parameters. As a result, the HDP-HMM has higher predictive likelihood on test data,
with an additional beneﬁt gained from using the sticky parameter.
Note that the results of Fig. 3.12(g) also motivate the use of the sticky parameter
in the more classical setting of a ﬁnite HMM with a standard Dirichlet sparsity prior.
A motivating example of the use of sparse Dirichlet priors for ﬁnite HMMs is presented
in [83].
■3.3 Multimodal Emission Densities
In many application domains, the data associated with each hidden state may have a
complex, multimodal distribution. We propose to model such emission distributions
nonparametrically, using a DP mixture of Gaussians. This formulation is related to the
nested DP [143], which uses a Dirichlet process to partition data into groups, and then
models each group via a Dirichlet process mixture. The bias towards self-transitions
allows us to distinguish between the underlying HDP-HMM states. If the model were
free to both rapidly switch between HDP-HMM states and associate multiple Gaussians
per state, there would be considerable posterior uncertainty. Thus, as we demonstrate
in Sec. 3.4, the sticky parameter is essential in eﬀectively learning such models.
We augment the HDP-HMM state zt with a term st indexing the mixture component
of the zth
t
emission density. For each HDP-HMM state, there is a unique stick-breaking
measure ψk ∼GEM(σ) deﬁning the mixture weights of the kth emission density so that

Sec. 3.3.
Multimodal Emission Densities
127
0
200
400
600
800
1000
−5
0
5
10
15
20
Time
Observations
(a)
2
4
6
8
1
2
3
4
5
6
7
8
9
2
4
6
8
1
2
3
4
5
6
7
8
9
5
10
15
20
25
30
5
10
15
20
25
30
(b)
(c)
(d)
0.2
0.4
0.6
0.8
1
0
500
1000
1500
Normalized Hamming Distance
Counts
 
 
sticky HDP−HMM
HDP−HMM
sticky sparse Dir
sparse Dir
0
10
20
30
40
50
60
0
500
1000
1500
2000
2500
3000
Number of Inferred States
Counts
 
 
sticky HDP−HMM
HDP−HMM
sticky sparse Dir
sparse Dir
−2800
−2700
−2600
−2500
−2400
−2300
0
50
100
150
200
250
300
350
400
Log−Likelihood
Counts
 
 
sticky HDP−HMM
HDP−HMM
sticky sparse Dir
sparse Dir
(e)
(f)
(g)
Figure 3.12. (a) Observation sequence (blue) and true state sequence (red) for a nine-state HMM with
multinomial observations. (b) The true transition probability matrix (TPM). (c)-(d) The inferred TPM
at the 30,000th Gibbs iteration for the sticky HDP-HMM and sticky sparse Dirichlet model, respectively,
only examining those states with more than 1% of the assignments. For the HDP-HMM and sparse
Dirichlet model, with and without the sticky parameter, we plot: (e) the Hamming distance error over
10,000 Gibbs iterations, (f) the inferred number of states with more than 1% of the assignments, and (g)
the predictive probability of test sequences using the inferred parameters sampled every 100th iteration
from Gibbs iterations 5,000 to 10,000.
st ∼ψzt. Given the augmented state (zt, st), the observation yt is generated by the
Gaussian component with parameter θzt,st. Note that both the HDP-HMM state index
and mixture component index are allowed to take values in a countably inﬁnite set.
The generative model is summarized below, and is graphically depicted in Fig. 3.2(b).
β | γ ∼GEM(γ)
πj | β, α ∼DP(α, β)
ψj | σ ∼GEM(σ)
j = 1, 2, . . .
θk,j | H, λ ∼H(λ)
k, j = 1, 2, . . .
zt|{πj}∞
j=1, zt−1 ∼πzt−1
st | {ψj}∞
j=1, zt ∼ψzt
t = 1, . . . , T
yt|{θk,j}∞
k,j=1, zt, st ∼F(θzt,st)
t = 1, . . . , T.
(3.12)
■3.3.1 Direct Assignment Sampler
Many of the steps of the direct assignment sampler for the sticky HDP-HMM with
DP emissions remains the same as for the regular sticky HDP-HMM (Algorithm 9).
Speciﬁcally, the sampling of the global transition distribution β, the table counts mjk
and ¯mjk, and the override variables wjt are unchanged. The diﬀerence arises in how we
sample the augmented state (zt, st).

128
CHAPTER 3.
THE STICKY HDP-HMM
The joint distribution on the augmented state, having marginalized the transition
distributions πk and emission mixture weights ψk, is given by
p(zt = k, st = j | z\t, s\t, y1:T , β, α, σ, κ, λ) = p(st = j | zt = k, z\t, s\t, y1:T , σ, λ)
p(zt = k | z\t, s\t, y1:T , β, α, κ, λ).
(3.13)
We then block-sample (zt, st) by ﬁrst sampling zt, followed by st conditioned on the
sampled value of zt. The term p(st = j | zt = k, z\t, s\t, y1:T , σ, λ) relies on how many
observations are currently assigned to the jth mixture component of state k, which we
denote by n′
kj. These conditional distributions are derived in Appendix A.2 and the
resulting Gibbs sampler is outlined in Algorithm 11.
■3.3.2 Blocked Sampler
To implement blocked resampling of (z1:T , s1:T ), we use weak limit approximations to
both the HDP-HMM and DP emissions, approximated to levels L and L′, respectively.
The posterior distributions for β and πk remain unchanged from the sticky HDP-HMM;
that of ψk is given by
ψk | z1:T , s1:T , σ ∼Dir(σ/L′ + n′
k1, . . . , σ/L′ + n′
kL′).
(3.14)
The procedure for sampling the augmented state sequence (z1:T , s1:T ) is derived in
Appendix B.3. The resulting blocked Gibbs sampler for the sticky HDP-HMM with
DP emissions is outlined in Algorithm 12.
■3.4 Assessing the Multimodal Emissions Model
In this section, we evaluate the ability of the sticky HDP-HMM to infer multimodal
emission distributions relative to the model without the sticky parameter. We then
apply this extended sticky HDP-HMM to the speaker diarization task.
■3.4.1 Mixture of Gaussian Emissions
To test the model of Sec. 3.3, we generated data from a ﬁve-state HMM, where the
number of Gaussian mixture components for each emission distribution was chosen
randomly from a uniform distribution on {1, 2, . . . , 10}. Each component of the mix-
ture was equally weighted and the probability of self-transition was set to 0.98, with
equal probabilities of transitions to the other states.
The large probability of self-
transition is what disambiguates this process from one with many more HMM states,
each with a single Gaussian emission distribution. The resulting observation and true
state sequences are shown in Fig. 3.13(a) and (b).
We once again used a non-conjugate base measure and placed a Gaussian prior on
the mean parameter and an independent inverse-Wishart prior on the variance param-
eter of each Gaussian mixture component. The hyperparameters for these distributions

Given a previous set of augmented state assignments (z(n−1)
1:T
, s(n−1)
1:T
) and the global
transition distribution β(n−1):
1. Set (z1:T , s1:T ) = (z(n−1)
1:T
, s(n−1)
1:T
) and β = β(n−1). For each t ∈{1, . . . , T},
(a) Decrement nzt−1zt, nztzt+1, and n′
ztst and remove yt from the cached statistics
for the current assignment (zt, st) = (k, j):
(ˆµk,j, ˆΣk,j) ←(ˆµk,j, ˆΣk,j) ⊖yt
ˆνk,j ←ˆνk,j −1
(b) For each of the K currently instantiated HDP-HMM states, compute
i. The predictive conditional distribution for each of the K′
k currently
instantiated mixture components associated with this HDP-HMM state
f ′
k,j(yt)
=
 
n′
kj
σ + n′
k·
!
tˆνk,j(yt; ˆµk,j, ˆΣk,j)
and for a new mixture component K′
k + 1
f ′
k,K′
k+1(yt)
=

σ
σ + n′
k·

tˆν0(yt; ˆµ0, ˆΣ0).
ii. The predictive conditional distribution of the HDP-HMM state without
knowledge of the current mixture component
fk(yt) = (αβk + nzt−1k)
αβzt+1 + nkzt+1 + κδ(k, zt+1)
α + nk· + κ
 

K′
k
X
j=1
f ′
k,j(yt) + f ′
k,K′
k+1(yt)


for zt−1 ̸= k, otherwise see Appendix A.2. Repeat this procedure for a
new HDP-HMM state K + 1 with K′
K+1 initialized to 0.
(c) Sample the new augmented state assignment (zt, st) by ﬁrst sampling zt:
zt
∼
K
X
k=1
fk(yt)δ(zt, k) + fK+1(yt)δ(zt, K + 1).
Then, conditioned on a new assignment zt = k, sample st:
st
∼
K′
k
X
j=1
f ′
k,j(yt)δ(st, j) + f ′
k,K′
k+1(yt)δ(st, K′
k + 1).
If k = K + 1, increment K and transform β as follows. Sample b ∼Beta(1, γ)
and assign βK ←bβ˜k and β˜k ←(1−b)β˜k. If st = K′
k + 1, then increment K′
k.
(d) Increment nzt−1zt, nztzt+1, and n′
ztst and add yt to the cached statistics for
the new assignment (zt, st) = (k, j):
(ˆµk,j, ˆΣk,j) ←(ˆµk,j, ˆΣk,j) ⊕yt
ˆνk,j ←ˆνk,j + 1
2. Fix (z(n)
1:T , s(n)
1:T ) = (z1:T , s1:T ). If there exists a k such that nk· = 0 and n·k = 0,
remove k and decrement K. Similarly, if there is a (k, j) such that n′
kj = 0 then
remove j and decrement K′
k.
3. Sample auxiliary variables m, w, and ¯
m as in step 3 of Algorithm 9.
4. Sample the global transition distribution β(n) as in step 4 of Algorithm 9.
5. Optionally, resample hyperparameters σ, γ, α, and κ as described in App. C.
Algorithm 11. Direct assignment collapsed Gibbs sampler for the sticky HDP-HMM with DP
emissions.

Given a previous set of state-speciﬁc transition probabilities π(n−1), emission mixture
weights ψ(n−1), global transition distribution β(n−1), and emission parameters θ(n−1):
1. Set π = π(n−1), ψ = ψ(n−1) and θ = θ(n−1). Working sequentially backwards in
time, calculate messages mt,t−1(k) :
(a) For each k ∈{1, . . . , L}, initialize messages to
mT+1,T(k) = 1
(b) For each t ∈{T −1, . . . , 1} and for each k ∈{1, . . . , L}, compute
mt,t−1(k) =
L
X
i=1
L′
X
ℓ=1
πk(i)ψi(ℓ)N(yt+1; µi,ℓ, Σi,ℓ)mt+1,t(i)
2. Sample augmented state assignments (z1:T , s1:T ) working sequentially forward in
time. Start with nik = 0, n′
kj = 0, and Yk,j = ∅for (i, k) ∈{1, . . . , L}2 and
(k, j) ∈{1, . . . , L} × {1, . . . , L′}.
(a) For each (k, j) ∈{1, . . . , L} × {1, . . . , L′}, compute the probability
fk,j(yt) = πzt−1(k)ψk(j)N(yt; µk,j, Σk,j)mt+1,t(k)
(b) Sample an augmented state assignment (zt, st):
(zt, st) ∼
L
X
k=1
L′
X
j=1
fk,j(yt)δ(zt, k)δ(st, j)
(c) Increment nzt−1zt and n′
ztst and add yt to the cached statistics for the new
assignment (zt, st) = (k, j):
Yk,j ←Yk,j ⊕yt
3. Sample the auxiliary variables m, w, and ¯
m as in step 3 of Algorithm 9.
4. Update the global transition distribution β as in step 4 of Algorithm 10.
5. For each k ∈{1, . . . , L},
(a) Sample a new transition distribution πk and emission mixture weights ψk:
πk
∼
Dir(αβ1 + nk1, . . . , αβk + κ + nkk, . . . , αβL + nkL)
ψk
∼
Dir(σ/L′ + n′
k1, . . . , σ/L′ + n′
kL′)
(b) For each j ∈{1, . . . , L′}, sample the parameters associated with the jth
mixture component of the kth emission distribution:
θk,j
∼
p(θ | λ, Yk,j)
See Appendix B.4.1 for details on resampling θk,j.
6. Fix π(n) = π, ψ(n) = ψ, β(n) = β, and θ(n) = θ.
7. Optionally, resample hyperparameters σ, γ, α, and κ as described in App. C.
Algorithm 12. Blocked Gibbs sampler for the sticky HDP-HMM with DP emissions. Here,
we use an independent Gaussian prior on the mean and inverse-Wishart (IW) prior on the
covariance (see Appendix B.4.1). The set Yk,j is comprised of the statistics obtained from the
observations assigned to augmented state (k, j) that are necessary for updating the parameter
θk,j = {µk,j, Σk,j}. The ⊕operator updates these cached statistics as a new assignment is
made.

Sec. 3.4.
Assessing the Multimodal Emissions Model
131
0
200
400
600
800
1000
−20
−15
−10
−5
0
5
10
15
Time
Observations
−3100
−3000
−2900
−2800
−2700
−2600
0
50
100
150
200
250
Log−likelihood
Counts
 
 
Sticky HDP−HMM
Original HDP−HMM
(a)
(b)
0
0.5
1
1.5
2
2.5
3
x 10
4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Iteration
Normalized Hamming Distance
0
0.5
1
1.5
2
2.5
3
x 10
4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Iteration
Normalized Hamming Distance
(c)
(d)
Figure 3.13. (a) Observation sequence (blue) and true state sequence (red) for a ﬁve-state HMM with
mixture of Gaussian observations. The Hamming distance over 30,000 Gibbs samples from three chains
are shown for the (b) sticky HDP-HMM and (c) original HDP-HMM, both with DP emissions. (d)
Histogram of the predictive probability of test sequences using the inferred parameters sampled every
100th iteration from Gibbs iterations 10,000 to 30,000 for the sticky and original HDP-HMM.
were set from the data in the same manner as in the fast-switching scenario. Consis-
tent with the sticky HDP-HMM concentration parameters γ and (α + κ), we placed a
weakly informative Gamma(1, 0.01) prior on the concentration parameter σ of the DP
emissions. All results are for the blocked sampler with truncation levels L = L′ = 20.
In Fig. 3.13, we compare the performance of the sticky HDP-HMM with DP emis-
sions to that of the original HDP-HMM with DP emissions (i.e., DP emissions, but no
bias towards self-transitions.) As with the multinomial observations, when the distance
between observations does not directly factor into the grouping of observations into
HMM states, there is a considerable amount of posterior uncertainty in the underlying
HMM state. Even after 30,000 Gibbs samples, there are still state sequence sample
paths with very rapid dynamics. The result of this fragmentation into redundant states
is a slight reduction in predictive performance on test sequences, as in the multinomial
emission case. See Fig. 3.13(b).

132
CHAPTER 3.
THE STICKY HDP-HMM
■3.5 Speaker Diarization
The speaker diarization task involves segmenting an audio recording into speaker-
homogeneous regions, while simultaneously identifying the number of speakers.
We
tested the performance of the sticky HDP-HMM with DP emissions3 for this task on
the data distributed by NIST as part of the Rich Transcription 2004-2007 meeting
recognition evaluations [126]. We used the ﬁrst 19 Mel Frequency Cepstral Coeﬃcients
(MFCCs), computed over a 30ms window every 10ms, as our feature vector. When
working with this dataset, we discovered that: (1) the high frequency content of these
features contained little discriminative information, and (2) without a minimum speaker
duration, the sticky HDP-HMM inferred within speaker dynamics in addition to global
speaker changes. To jointly address these issues, we deﬁned the observations as aver-
ages over 250ms, non–overlapping blocks. A minimum speaker duration of 500ms was
set by associating two of these observations with each hidden state. To regularize the
learning in this high-dimensional space, we also tied the covariances of within–state
mixture components (i.e., each speaker–speciﬁc mixture component was forced to have
identical covariance structure.) See Fig. 3.14 for a block diagram of the processing of
audio into diarizations.
As in Sec. 3.4.1, we used a non-conjugate prior on the mean and covariance param-
eters of the Gaussian components of the mixture emissions. Speciﬁcally, we placed a
normal prior on the mean parameter with mean equal to the empirical mean and covari-
ance equal to 0.75 times the empirical covariance, and an inverse-Wishart prior on the
covariance parameter with 1000 degrees of freedom and expected covariance equal to
the empirical covariance. For the concentration parameters, we placed a Gamma(12, 2)
prior on γ, a Gamma(6, 1) prior on α + κ, and a Gamma(1, 0.5) prior on σ. The self-
transition proportion parameter ρ was given a Beta(500, 5) prior. For each of the 21
meetings, we ran 10 chains of the blocked Gibbs sampler of Algorithm 12 for 10,000
iterations for both the original and sticky HDP-HMM with DP emissions.
For the NIST speaker diarization evaluations, the goal is to produce a single segmen-
tation for each meeting. Due to the label-switching issue (i.e., under our exchangeable
prior, labels are arbitrary entities that do not necessarily remain consistent over Gibbs
iterations), we cannot simply integrate over multiple Gibbs sampled state sequences.
We propose two solutions to this problem. The ﬁrst is to simply choose from a ﬁxed
set of Gibbs samples the one that produces the largest likelihood given the estimated
parameters (marginalizing over state sequences), and then produce the corresponding
Viterbi state sequence. This heuristic, however, is sensitive to over-ﬁtting and will, in
general, be biased towards solutions with more states.
An alternative, and more robust metric is what we refer to as the minimum ex-
pected Hamming distance. We ﬁrst choose a large reference set R of state sequences
produced by the Gibbs sampler and a possibly smaller set of test sequences T . Then,
3Although not presented here, the sticky HDP-HMM with single Gaussian emissions performed
signiﬁcantly worse. It is well-established within the speech community that speaker-speciﬁc emissions
are multimodal, and thus models typically employ mixture of Gaussian emissions [185].

Sec. 3.5.
Speaker Diarization
133
Compute
MFCCs
Raw 
Audio
30 ms
Speech/
Non-Speech
Detector
Sticky 
HDP-HMM
25
25
Insert
Non-Speech
25
25
z -1
z -1
z -1
…
+
+
+
…
z -1
z -1
z -1
…
+
+
+
…
Average over 25 MFCCs
Diarizations of the audio
Figure 3.14. Speaker diaraization block diagram. Raw audio is processed into Mel Frequency Cepstral
Coeﬃcients (MFCCs) over 30 ms windows every 10 ms. The MFCCs are then separated into speech
and non-speech feature sequences.
The results of these two steps are provided by ICSI using the
algorithms in [185]. We then average non-overlapping blocks of 25 speech features which are passed to
the sticky HDP-HMM with DP emissions model. The state sequences produced by the Gibbs sampler
of Algorithm 12 are upsampled and the non-speech sequence inserted to produce the ﬁnal diarizations.
for each sequence i in the test set T , we compute the empirical mean Hamming dis-
tance between the test sequence and the sequences in the reference set R; we denote
this empirical mean by ˆHi. We then choose the test sequence j∗that minimizes this
expected Hamming distance. That is,
j∗= arg min
i∈T
ˆHi.
The empirical mean Hamming distance ˆHi is a label-invariant loss function since it does
not rely on labels remaining consistent across samples—we simply compute
ˆHi =
1
|R|
X
j∈R
Hamm(i, j),
where Hamm(i, j) is the Hamming distance between sequences i and j after ﬁnding
the optimal permutation of the labels in test sequence i to those in reference sequence
j. At a high level, this method for choosing state sequence samples aims to produce
segmentations of the data that are typical samples from the posterior. Asymptotically,
this approach will minimize a posterior expected risk. Jasra et al. [78] provides an
overview of some related techniques to address the label-switching issue. Although we

134
CHAPTER 3.
THE STICKY HDP-HMM
0
10
20
30
40
50
0
10
20
30
40
50
Sticky DERs
Non−Sticky DERs
0
10
20
30
40
50
0
10
20
30
40
50
Sticky DERs
Non−Sticky DERs
0
10
20
30
40
50
0
10
20
30
40
50
Sticky DERs
Non−Sticky DERs
(a)
(b)
(c)
0
10
20
30
40
50
0
10
20
30
40
50
Sticky DERs
ICSI DERs
0
10
20
30
40
50
0
10
20
30
40
50
Sticky DERs
ICSI DERs
0
10
20
30
40
50
0
10
20
30
40
50
Sticky DERs
ICSI DERs
(d)
(e)
(f)
Figure 3.15. (a)-(c) For each of the 21 meetings, comparison of diarizations using sticky vs. original
HDP-HMM with DP emissions. In (a) we plot the DERs corresponding to the Viterbi state sequence
using the parameters inferred at Gibbs iteration 10,000 that maximize the likelihood, and in (b) the
DERs using the state sequences that minimize the expected Hamming distance. Plot (c) is the same
as (b), except for running the 10 chains for meeting 16 out to 50,000 iterations. (d)-(f) Comparison of
the sticky HDP-HMM with DP emissions to the ICSI errors under the same conditions.
could have chosen any label-invariant loss function to minimize, we chose the Hamming
distance metric because it is closely related to the oﬃcial NIST diarization error rate
(DER) that is calculated during the evaluations. The ﬁnal metric by which the speaker
diarization algorithms are judged is the overall DER, a weighted average over the set
of meetings based on the length of each meeting.
In Fig. 3.15(a), we report the DER of the chain with the largest likelihood given
the parameters estimated at the 10, 000th Gibbs iteration for each of the 21 meetings,
comparing the sticky and original HDP-HMM with DP emissions. The sticky model’s
temporal smoothing provides substantial performance gains. Although not depicted
here, the likelihoods given the parameter estimates under the original HDP-HMM are
almost always higher than those under the sticky model. This phenomenon is due to
the fact that without the sticky parameter, the HDP-HMM over-segments the data and
thus produces parameter estimates more ﬁnely tuned to the data resulting in higher
likelihoods. Since the original HDP-HMM is contained within the class of sticky models
(i.e., when κ = 0), there is some probability that state sequences similar to those under
the original model will eventually arise using the sticky model. Thus, the likelihood
metric is not very robust as one would expect the performance under the sticky model

Sec. 3.5.
Speaker Diarization
135
True speaker label
Time
0
25000
50000
75000
0
2
4
6
8
10
True speaker label
Time
0
10000
20000
0
2
4
6
8
10
(a)
(b)
Estimated speaker label
Time
0
10000
20000
0
2
4
6
8
10
Estimated speaker label
Time
0
10000
20000
0
2
4
6
8
10
(c)
(d)
Figure 3.16. (a) True state sequence for the NIST 20051102-1323 meeting (meeting 16), with labels 9
and 10 indicating times of overlapping- and non- speech, respectively, missed by the speech/non-speech
preprocessor. (b) True state sequence with the overlapping- and non- speech time steps removed. (c)-(d)
Plotted only over the time-steps as in (b), the state sequences inferred by the sticky HDP-HMM with
DP emissions at Gibbs iteration 10,000 chosen using the most likely and minimum expected Hamming
distance metrics, respectively. Incorrect labels are shown in red.
to degrade given enough Gibbs chains and/or iterations. In Fig. 3.15(b), we instead
report the DER of the chain whose state sequence estimate at Gibbs iteration 10,000
minimizes the expected Hamming distance to the sequences estimated every 100 Gibbs
iteration, discarding the ﬁrst 5,000 iterations. Due to the slow mixing rate of the chains
in this application, we additionally discard samples whose normalized log-likelihood
is below 0.1 units of the maximum at Gibbs iteration 10,000. For the sticky HDP-
HMM, this results in using an average of 270 samples per meeting and, for the original
HDP-HMM, 464 samples. The disparity in these numbers arises from the fact that, as
further discussed in Sec. 3.6, the sticky model has more chains that have not mixed
by 5,000 iterations. From this ﬁgure, we see that the sticky model still signiﬁcantly
outperforms the original HDP-HMM, implying that most state sequences produced by
the original model are worse, not just the one corresponding to the most-likely sample.
One noticeable exception to this trend is the NIST 20051102-1323 meeting (meeting 16).
For the sticky model, the state sequence using the maximum likelihood metric had very
low DER (see Fig. 3.16(c)); however, there were many chains that merged speakers
and produced segmentations similar to the one in Fig. 3.16(d), resulting in such a

136
CHAPTER 3.
THE STICKY HDP-HMM
Overall DERs (%)
Min Hamming
Max Likelihood
2-Best
5-Best
Sticky HDP-HMM
19.01 (17.84)
19.37
16.97
14.61
Non-Sticky HDP-HMM
23.91
25.91
23.67
21.06
Table 3.1. Overall DERs for the sticky and original HDP-HMM with DP emissions using the minimum
expected Hamming distance and maximum likelihood metrics for choosing state sequences at Gibbs
iteration 10,000. For the maximum likelihood criterion, we show the best overall DER if we consider
the top two or top ﬁve most-likely candidates. The number in the parentheses is the performance when
running meeting 16 for 50,000 Gibbs iterations. The overall ICSI DER is 18.37%.
sequence minimizing the expected Hamming distance. See Sec. 3.6 for a discussion on
the issue of merged speakers. Running meeting 16 for 50,000 Gibbs iterations improved
the performance, as depicted by the revised results in Fig. 3.15(c). We summarize our
overall performance in Table 3.1, and note that (when using the 50,000 Gibbs iterations
for meeting 16) we obtain an overall DER of 17.84% using the sticky HDP-HMM versus
the 23.91% of the original HDP-HMM model.
As a further comparison, the ICSI team’s algorithm [185], by far the best per-
former at the 2007 competition, has an overall DER of 18.37%.
The ICSI team’s
algorithm uses agglomerative clustering, and requires signiﬁcant tuning of parameters
on representative training data. In contrast, our hyperparameters are automatically set
meeting-by-meeting, as outlined at the beginning of this section. An additional bene-
ﬁt of the sticky HDP-HMM over the ICSI approach is the fact that there is inherent
posterior uncertainty in this task, and by taking a Bayesian approach we are able to
provide several interpretations. When considering the best per-meeting DER for the
ﬁve most likely samples, our overall DER drops to 14.61% (see Table 3.1). Meeting
VT 20050304-1300 meeting (meeting 18) is an example for which providing multiple
solutions is helpful. For this meeting, multiple chains provided segmentations similar
to the one depicted in Fig. 3.17(b) with speaker 2 split into two inferred speakers. Ex-
amining the observations from this meeting, speaker 2 looks noticeably diﬀerent over
time. However, another chain produced a segmentation which correctly grouped all
of the times corresponding to this speaker (see Fig. 3.17(a)). Although not useful in
the NIST evaluations, providing multiple segmentations could prove of importance in
practice.
To ensure a fair comparison on this dataset, we use the same speech/non-speech
pre-processing as ICSI, so that the diﬀerences in our performance are due to changes
in the identiﬁed speakers. Non-speech refers to time when nobody is speaking. The
pre-processing step of removing non-speech observations is important in ensuring that
the learned acoustic models are not corrupted by non-speech information. As depicted
in Fig. 3.18, both our performance and that of ICSI depend signiﬁcantly on the quality
of this pre-processing step. In Fig. 3.18(a), we compare the meeting-by-meeting DERs
of the sticky HDP-HMM, original HDP-HMM, and ICSI algorithm, and in Fig. 3.18(b)
we plot the fraction of post-processed data that still contains overlapping- and non-

Sec. 3.6.
Discussion and Future Work
137
True speaker label
Time
0
10000
20000
30000
40000
50000
0
2
4
6
8
10
(a)
Estimated speaker label
Time
0
10000
20000
30000
40000
50000
0
2
4
6
8
10
Estimated speaker label
Time
0
10000
20000
30000
40000
50000
0
2
4
6
8
10
(b)
(c)
Figure 3.17.
(a) True state sequence for the VT 20050304-1300 meeting (meeting 18), with labels
9 and 10 indicating times of overlapping- and non- speech, respectively, missed by the speech/non-
speech preprocessor. (b) State sequence inferred by the sticky HDP-HMM with DP emissions at Gibbs
iteration 10,000 chosen using the most likely metric. The corresponding DER is 20.48%. (c) State
sequence at Gibbs iteration 10,000 for the chain ranked ﬁfth according to the likelihood metric. The
corresponding DER is 4.81%. Compare to ICSI’s DER of 22.00% for this meeting. .
speech4. It is clear from Fig. 3.18(a) that the sticky HDP-HMM with DP emissions
provides performance comparable to that of the ICSI algorithm while the original HDP-
HMM with DP emissions performs signiﬁcantly worse. Overall, the results presented
in this section demonstrate that the sticky HDP-HMM with DP emissions provides an
elegant and empirically eﬀective speaker diarization method.
■3.6 Discussion and Future Work
We have examined some of the limitations of the HDP-HMM presented in [162], and
demonstrated the considerable beneﬁts of a sticky HDP-HMM in which a separate
parameter captures state persistence. By developing a hierarchical Bayesian approach
in which a prior is placed on this state persistence parameter, this sticky HDP-HMM
provides a general solution that remains capable of capturing fast dynamics. One of
the main contributions of the work presented in this chapter is fully integrating this
4Not shown in this plot is the amount of actual speech removed by the speech/non-speech pre-
processor.

138
CHAPTER 3.
THE STICKY HDP-HMM
0
5
10
15
20
0
10
20
30
40
50
60
Meeting
DER
 
 
Sticky
Non−Sticky
ICSI
0
5
10
15
20
0
0.1
0.2
0.3
0.4
0.5
0.6
Overlapping− and Non−Speech
Meeting
(a)
(b)
Figure 3.18. (a) Chart comparing the DERs of the sticky and original HDP-HMM with DP emissions
to those of ICSI for each of the 21 meetings. Here, we chose the state sequence at the 10, 000th Gibbs
iteration that minimizes the expected Hamming distance. For meeting 16 using the sticky HDP-HMM
with DP emissions, we chose between state sequences at Gibbs iteration 50,000. (b) Plot of the fraction
of overlapping- or non- speech in the post-processed data for each of the 21 meetings.
prior within the Bayesian nonparametric framework. For concise presentation, these
derivations were extracted to the appendices.
We have also shown that this sticky HDP-HMM allows a fully Bayesian nonpara-
metric treatment of multimodal emissions, disambiguated by its bias towards self-
transitions.
Accommodating multimodal emissions is a necessary step in describing
the data found in many real-world applications, such as the speaker diarization task of
Sec. 3.5. Without providing any application-speciﬁc information, the sticky HDP-HMM
with DP emissions yields a speaker diarization algorithm strongly competitive with the
current state of the art. This application, along with extensive testing on synthetic
data, clearly demonstrate the practical importance of our extensions.
Finally, we presented eﬃcient sampling techniques with mixing rates that improve
on the state of the art by harnessing the Markovian structure of the HDP-HMM.
Speciﬁcally, we proposed employing a truncated approximation to the HDP and block-
sampling the state sequence using a variant of the forward-backward algorithm. Al-
though the blocked samplers of Algorithms 10 and 12 yield substantially improved
mixing rates over the sequential, direct assignment samplers of Algorithms 9 and 11,
there are still some pitfalls to these sampling methods.
One issue is that for each
new considered state, the parameter sampled from the prior distribution must better
explain the data than the parameters associated with other states that have already
been informed by the data. In high-dimensional applications, and in cases where state-
speciﬁc emission distributions are not clearly distinguishable, this method for adding
new states poses a signiﬁcant challenge. The data in the speaker diarization task is

Sec. 3.6.
Discussion and Future Work
139
0
2
4
6
8
10
x 10
4
−1.6
−1.595
−1.59
−1.585
−1.58
x 10
5
Iteration
Log Likelihood
0
2
4
6
8
10
x 10
4
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Hamming Distance
(a)
(b)
Figure 3.19.
Trace plots of (a) log-likelihood and (b) Hamming distance error for 10 chains over
100,000 Gibbs iterations for the NIST 20051102-1323 meeting (meeting 16).
both high-dimensional and often has only marginally distinguishable speakers, leading
to extremely slow mixing rates, as indicated by the trace plots in Fig. 3.19 of various
indicators such as Hamming distance and log-likelihood for 100,000 Gibbs iterations of
meeting 16. Many of our errors in this application can be attributed to merged speakers,
as depicted in Fig. 3.16(d). On such large datasets, the computation cost of running
hundreds of thousands of Gibbs iterations presents an insurmountable barrier. A direc-
tion for future work is to develop split-merge algorithms for the HDP and HDP-HMM
similar to those developed in [77] for the DP mixture model.
A limitation of the HMM in general is that the observations are assumed condition-
ally independent and identically distributed given the state sequence. This assumption
is often insuﬃcient in capturing the complex temporal dependencies exhibited in real-
world data. In the following chapter, we consider Bayesian nonparametric versions of
models better suited to such applications, such as the switching linear dynamical system
(SLDS) and switching vector autoregressive (VAR) process. A ﬁrst attempt at develop-
ing such models is presented in [44]. An inspiration for the sticky HDP-HMM actually
came from considering the original HDP-HMM as a prior for an SLDS. In such sce-
narios in which one does not have direct observations of the underlying state sequence,
the issues arising from not properly capturing state persistence are exacerbated. The
sticky HDP-HMM presented in this chapter provides a more robust building block for
developing more complicated Bayesian nonparametric dynamical models.

140
CHAPTER 3.
THE STICKY HDP-HMM

Chapter 4
Bayesian Nonparametric Learning of
SLDS
L
INEAR dynamical systems (LDSs) are useful in describing dynamical phenomena as
diverse as human motion [133, 140], ﬁnancial time-series [27, 94, 154], maneuvering
targets [43, 145], and the dance of honey bees [129]. However, such phenomena often
exhibit structural changes over time, and the LDS models which describe them must
also change. For example, a ballistic missile makes an evasive maneuver; a country
experiences a recession, a central bank intervention, or some national or global event;
a honey bee changes from a waggle to a turn right dance. Some of these changes will
appear frequently, while others are only rarely observed. In addition, there is always
the possibility of a new, previously unseen dynamical behavior. These considerations
motivate us to develop a Bayesian nonparametric approach for learning switching LDS
(SLDS) models. We also consider a special case of the SLDS—the switching vector
autoregressive (VAR) model—in which direct observations of the underlying dynamical
process are assumed available. Although a special case of the general linear systems
framework, autoregressive models have simplifying properties that often make them a
practical choice in applications.
One can view the SLDS, and the simpler switching VAR process, as an extension of
hidden Markov models (HMMs) in which each HMM state, or mode, is associated with a
linear dynamical process. While the HMM makes a strong Markovian assumption that
observations are conditionally independent given the mode, the SLDS and switching
VAR processes are able to capture more complex temporal dependencies often present
in real data. Most existing methods for learning SLDS and switching VAR processes rely
on either ﬁxing the number of HMM modes, such as in [129], or considering a change-
point detection formulation where each inferred change is to a new, previously unseen
dynamical mode, such as in [188]. In this chapter we show how one can remain agnostic
about the number of dynamical modes while still allowing for returns to previously
exhibited dynamical behaviors.
As we examined in Chapter 3, the hierarchical Dirichlet process (HDP) can be used
as a prior on the parameters of HMMs with unknown mode space cardinality [11, 162].
We proposed a variant on the HDP-HMM—the sticky HDP-HMM —that provides im-
141

142
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
proved control over the number of modes inferred; we demonstrate that such control
is crucial for the problems we examine in this chapter. Our Bayesian nonparametric
approach for learning switching dynamical processes extends the sticky HDP-HMM
formulation to learn an unknown number of persistent dynamical modes and thereby
capture a wider range of temporal dependencies. We then explore a method for learning
which components of the underlying state vector contribute to the dynamics of each
mode by employing automatic relevance determination (ARD) [9, 112, 124]. The result-
ing model allows for learning realizations of SLDS that switch between an unknown
number of dynamical modes with possibly varying state dimensions, or switching VAR
processes with varying autoregressive orders.
Paoletti et al. [130] provide a survey of recent approaches to identiﬁcation of switch-
ing dynamical models. The typical identiﬁcation problem for these switching models,
in the most general sense, involves learning: (i) the number of dynamical modes, (ii)
the model order, and (iii) the associated dynamic parameters. Most approaches assume
that the model order is the same for each dynamical mode. For noiseless switching VAR
processes, Vidal et al. [174] present an exact algebraic approach. However, the method
relies on ﬁxing the maximal mode space cardinality and autoregressive order, which
is assumed shared among modes. Additionally, extensions to the noisy case rely on
heuristics. Psaradakis and Spagnolo [138] alternatively consider a penalized likelihood
approach to identiﬁcation of stochastic switching VAR processes.
For SLDS, identiﬁcation is signiﬁcantly more challenging, and methods typically
rely on simplifying assumptions such as deterministic dynamics or knowledge of the
mode space. If one knew the mode sequence, then one could partition the data ac-
cording to the underlying mode sequence and then employ the techniques described
in Sec. 2.7.4 for identiﬁcation of single LDS. However, when addressing the issue of
stochastic realization or system identiﬁcation of SLDS, we assume that the mode se-
quence is a latent variable of our model. Huang et al. [70] present an approach that
embeds the input/output data in a higher-dimensional space and ﬁnds the switching
times by segmenting the data into distinct subspaces [173]. This algebraic approach
assumes deterministic dynamics and claims robustness to moderate amounts of noise.
Kotsalis et al. [99] develop a balanced truncation algorithm for SLDS assuming the
switches between modes are i.i.d. within a ﬁxed, ﬁnite set; the authors also present a
method for model-order reduction of HMMs1. In [135], a realization theory is presented
for what the authors refer to as generalized jump-Markov linear systems (GJMLS) in
which the dynamic matrix depends both on the previous mode and current mode. The
authors mention that it is unclear whether a similar theory can be developed for the
standard SLDS we consider in this chapter. Finally, when the number of dynamical
modes is assumed known, Ghahramani and Hinton [52] present a variational approach
to segmenting the data into the linear dynamical regimes and learning the associated
dynamic parameters2.
1The problem of identiﬁcation of HMMs is thoroughly analyzed in [4].
2The formulation of Ghahramani and Hinton [52] uses a mixture of experts SLDS representation in

Sec. 4.1.
The HDP-SLDS and HDP-AR-HMM Models
143
Many questions of observability and identiﬁability of SLDS in the absence of noise
are addressed in [172]. Speciﬁcally, a set of suﬃcient conditions are provided for the
initial continuous state x0 and discrete mode sequence z1:T to be observable given ﬁxed
model parameters. The authors argue that if both the dimension d of the continuous
state and the number of possible modes K are unconstrained, there is an inﬁnite set of
systems that realize the same set of observations y1:T while diﬀering in {x0, z1:T }. Even
when limiting one of these two degrees of freedom, the problem of realization is ill-posed.
In this chapter, we circumvent having to limit both d and K, and having to check the
detailed conditions provided in [172], by taking a Bayesian approach. One can interpret
the control literature on non-identiﬁable systems as arising in classical statistics when
multiple models have equivalent likelihood3. In the Bayesian approach that we adopt, it
is the speciﬁc prior we place on the parameters that allows us to distinguish between the
set of these equivalent models. Our choice of prior penalizes more complicated models,
both in terms of the number of modes and the state dimension describing each mode.
Thus, instead of placing hard constraints on the model, we simply increase the posterior
probability of simpler explanations of the data. As opposed to a penalized likelihood
approach using Akaike’s information criterion (AIC) [3] or the Bayesian information
criterion (BIC) [147], our approach provides a model complexity penalty in a purely
Bayesian manner.
In summary, previous methods for identiﬁcation of the SLDS we consider in this
thesis rely on assuming either: (i) deterministic dynamics, (ii) a ﬁxed number of dy-
namical modes, or (iii) non-Bayesian penalties on model complexity. The approach we
present herein aims to address identiﬁcation of mode space cardinality and model order
of SLDS and switching VAR processes within a Bayesian nonparametric framework.
We also demonstrate that allowing for variable order models provides insight into the
structure of the underlying phenomenon.
■4.1 The HDP-SLDS and HDP-AR-HMM Models
For greater modeling ﬂexibility within the Bayesian framework, we take a nonparametric
approach in deﬁning the mode space of our switching dynamical processes. Speciﬁcally,
we develop extensions of the sticky HDP-HMM of Chapter 3 for both the SLDS and
switching VAR models described in Sec. 2.7.3. For the SLDS, we consider conditionally-
dependent emissions of which only noisy observations are available (see Fig. 4.1(b).) We
refer to this model as the HDP-SLDS. The switching VAR(r) process, with r denoting
the autoregressive order, can similarly be posed using an HDP-HMM in which the
observations are modeled as conditionally VAR(r). This model is referred to as the
which M diﬀerent continuous-valued state sequences evolve independently with linear dynamics and
the Markovian dynamical mode, taking values in 1, . . . , M, selects which state sequence is observed at
a given time.
3The deﬁnition of identiﬁability in the control literature diﬀers from that in statistics, though with
obvious relations. There is also a concept of Bayesian non-identiﬁability in which multiple parameters
have the same posterior probability. See [15, 49].

144
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
(a)
(b)
Figure 4.1.
Grapical models of sticky HDP-HMM prior on switching (a) VAR(2) and (b) SLDS
processes with the mode evolving as zt+1|{πk}∞
k=1, zt ∼πzt for πk|α, κ, β ∼DP(α+κ, (αβ+κδk)/(α+κ)).
Here, β | γ ∼GEM(γ) and θk | H, λ ∼H(λ). The dynamical processes are as in Eq. (4.1).
HDP-AR-HMM and is depicted in Fig. 4.1(a). The generative processes for these two
models are summarized as follows:
HDP-AR-HMM
HDP-SLDS
Mode dynamics
zt ∼πzt−1
zt ∼πzt−1
Observation dynamics
yt = Pr
i=1 A(zt)
i
yt−i + et(zt)
xt = A(zt)xt−1 + et(zt)
yt = Cxt + wt
(4.1)
Here, πj is as deﬁned in the sticky HDP-HMM, namely,
πj | α, κ, β ∼DP

α + κ, αβ + κδj
α + κ

.
(4.2)
The additive noise processes are deﬁned as
et(k) ∼N(0, Σ(k))
wt ∼N(0, R).
(4.3)
For the HDP-SLDS, we place a priors on the dynamic parameters {A(k), Σ(k)} and
on measurement noise R and infer their posterior from the data. We do, however, ﬁx
the measurement matrix, C, for reasons of identiﬁability.
As given by Eq. (2.155),
there exists a set of equivalent systems in terms of the input-output relationship. Let
˜C ∈Rd×n, n ≥d, be the measurement matrix associated with a dynamical system
deﬁned by ˜A and ˜G, where we recall from Sec. 2.7.4 that ˜G = ˜A ˜Px ˜CT with ˜Px the
steady-state covariance matrix. Assume that ˜C has full row rank. Then, without loss
of generality, we may consider C = [Id 0] since there exists an invertible transformation
T such that the triplet
(A = T −1 ˜AT, C = ˜CT = [Id 0], G = T −1 ˜G)
(4.4)

Sec. 4.1.
The HDP-SLDS and HDP-AR-HMM Models
145
is in the equivalence class M( ˜A, ˜C, ˜G) as deﬁned in Eq. (2.155). Since the measurement
matrix is shared for all modes of the HDP-SLDS, the similarity transformation T is
identical for all modes, implying that the change-in-basis of our state vector remains
consistent over time. Our choice of the number of columns of zeros in C is, in essence,
a choice of model order, and one which we address in Sec. 4.1.1.
For the HDP-AR-HMM, we similarly place a prior on the dynamic parameters,
which in this case consist of {A(k)
1 , . . . , A(k)
r , Σ(k)}.
The speciﬁc choices of priors we explore, and the resulting posterior distributions
conditioned on a set of observations, are described in Sec. 4.1.1. The Gibbs sampling
inference scheme is derived in Sec. 4.1.2, and iterates between the following steps for
the HDP-SLDS:
1. Sample the state sequence x1:T given the mode sequence z1:T and SLDS parameters
{A(k), Σ(k), R}.
2. Sample the mode sequence z1:T given the state sequence x1:T , HMM parameters
{πk}, and dynamic parameters {A(k), Σ(k)}.
3. Sample the HMM parameters {πk} and SLDS parameters {A(k), Σ(k), R} given the
sequences, z1:T , x1:T , and y1:T .
The sampler for the HDP-AR-HMM reuses many steps of the HDP-SLDS sampler,
except for directly sampling the mode sequence z1:T based on the observations y1:T
(whereas the HDP-SLDS relied on a hidden, sampled state sequence x1:T for this step.)
Then, resampling the dynamic parameters simply uses the sequences z1:T and y1:T . In
order to make the connections between the samplers for the HDP-SLDS and HDP-AR-
HMM explicit in the following sections, we introduce the concept of pseudo-observations
ψ1:T . For the HDP-AR-HMM, the pseudo-observations are simply the original obser-
vations y1:T . However, for the HDP-SLDS, the pseudo-observations are the sampled
state sequence x1:T . A block diagram depicting the connections between these samplers
is shown in Fig. 4.2.
■4.1.1 Posterior Inference of Dynamic Parameters
In this section we focus on developing a prior to regularize the learning of the dynamic
parameters (and measurement noise) conditioned on a ﬁxed mode assignment z1:T . The
joint learning of the number of modes and resampling the mode sequence z1:T follows
as a straightforward extension of the methods described for the sticky HDP-HMM in
Chapter 3.
To analyze the posterior distribution of the dynamic parameters, it is useful to ﬁrst
rewrite the dynamic equation for both the HDP-SLDS and HDP-AR-HMM generically
in terms of the pseudo-observations ψt in the following manner:
ψt = A(k) ¯
ψt−1 + et,
(4.5)

146
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
!"#$"%&'())*
+(,-)"
.)/01
+(,-)"
!"#$%&"%
.)/01
+(,-)"
2+&',(&"
,/3")
-(4(,"&"4+
56+"47(&'/%+
!"#$'($!))
!"#$
%&"%
8+"$3/9/6+"47(&'/%+
:/3")
-(4(,"&"4+
Figure 4.2.
Block diagram of one iteration of the Gibbs sampler for the HDP-SLDS and HDP-
AR-HMM. Assume there exists a previous sample of model parameters. Based on the model type, the
appropriate switch closes. If HDP-SLDS, there is an extra stage of sampling the latent, continuous
state sequence x1:T .
This sequence then becomes the pseudo-observations.
If HDP-AR-HMM, the
pseudo-observations are simply the original observations y1:T . The pseudo-observations are then used
to block sample the mode sequence z1:T . Subsequently, a new set of model parameters are sampled
conditioned on the mode sequence and pseudo-observations.
where we utilize the following deﬁnitions:
HDP-AR-HMM
HDP-SLDS
Dynamic matrix
A(k) = [A(k)
1
. . . A(k)
r ] ∈Rd×(d∗r)
A(k) = A(k) ∈Rn×n
Pseudo-observations
ψt = yt
ψt = xt
Lag pseudo-observations
¯
ψt = [yT
t−1 . . . yT
t−r]T
¯
ψt = xt−1.
(4.6)
For the HDP-AR-HMM, we have simply written the dynamic equation of Eq. (4.1) in
matrix form by concatenating the lag matrices into a single matrix A(k) and forming a
lag observation vector ¯
ψt comprised of a series of previous observation vectors. For the
HDP-SLDS, however, we see that inferences based on pseudo-observations actually rely
on a ﬁxed, known state sequence x1:T . Methods for resampling this state sequence are
discussed in Sec. 4.1.2. For this section (for the HDP-SLDS), we assume such a sample
of the state sequence (and hence {ψt, ¯
ψt}) is available so that Eq. (4.5) applies equally
well to both the HDP-SLDS and the HDP-AR-HMM.
Conditioned on the mode sequence, one may partition this dynamic sequence into K
diﬀerent linear regression problems, where K = |{z1, . . . , zT }|. That is, for each mode
k, we may form a matrix Ψ(k) with Nk columns consisting of the ψt with zt = k. Then,
Ψ(k) = A(k) ¯Ψ(k) + E(k),
(4.7)
where ¯Ψ(k) is a matrix of the associated ¯
ψt−1, and E(k) the associated noise vectors.

Sec. 4.1.
The HDP-SLDS and HDP-AR-HMM Models
147
Conjugate Prior on {A(k), Σ(k)}
The matrix-normal inverse-Wishart (MNIW) prior (see Sec. 2.4.4) is conjugate to the
likelihood model deﬁned in Eq. (4.7) for the parameter set {A(k), Σ(k)}. Although this
prior is typically used for inferring the parameters of a single linear regression problem,
it is equally applicable to our scenario since the linear regression problems of Eq. (4.7)
are independent conditioned on the mode sequence z1:T .
We note that although the MNIW prior does not enforce stability constraints on
each mode, this prior is still a reasonable choice since each mode need not have stable
dynamics for the SLDS to be stable [30], and conditioned on data from a stable mode,
the posterior distribution will likely be sharply peaked around stable dynamic matrices.
Let D(k) = {Ψ(k), ¯Ψ(k)}. The posterior distribution of the dynamic parameters for
the kth mode decomposes as
p(A(k), Σ(k) | D(k)) = p(A(k) | Σ(k), D(k))p(Σ(k) | D(k)).
(4.8)
The resulting posterior of A(k) is derived in Appendix F.1 to be
p(A(k) | Σ(k), D(k)) = MN

A(k); S(k)
ψ ¯ψS−(k)
¯ψ ¯ψ , Σ−(k), S(k)
¯ψ ¯ψ

,
(4.9)
with B−(k) denoting (B(k))−1 for a given matrix B, and
S(k)
¯ψ ¯ψ = ¯Ψ(k) ¯Ψ(k)T + K
S(k)
ψ ¯ψ = Ψ(k) ¯Ψ(k)T + MK
S(k)
ψψ = Ψ(k)Ψ(k)T + MKMT ,
(4.10)
where M and K are the parameters deﬁning the matrix-normal portion of the MNIW
prior, as in Eq. (2.94).
Assuming an inverse-Wishart prior IW(n0, S0) on Σ(k), with S0 the scale matrix
and n0 the degrees of freedom, Eq. (2.99) gives us the marginal posterior of Σ(k):
p(Σ(k) | D(k)) = IW

Nk + n0, S(k)
ψ| ¯ψ + S0

,
(4.11)
where S(k)
ψ| ¯ψ = S(k)
ψψ −S(k)
ψ ¯ψS−(k)
¯ψ ¯ψ S(k)T
ψ ¯ψ
and and Nk = |{t | zt = k, t = 1, . . . , T}|.
Alternative Prior — Automatic Relevance Determination
The MNIW prior leads to full A(k) matrices, which (i) becomes problematic as the
model order grows in the presence of limited data; and (ii) does not provide a method for
identifying irrelevant model components (i.e. state components in the case of the HDP-
SLDS or lag components in the case of the HDP-AR-HMM.) To jointly address these
issues, we alternatively consider automatic relevance determination (ARD) [9, 112, 124],
which encourages driving components of the model parameters to zero if their presence
is not supported by the data.

148
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
For the HDP-SLDS, we harness the concepts of ARD by placing independent, zero-
mean, spherically symmetric Gaussian priors on the columns of the dynamic matrix
A(k):
p(A(k)|α(k)) =
n
Y
j=1
N

a(k)
j ; 0, α−(k)
j
In

.
(4.12)
Each precision parameter α(k)
j
is given a Gamma(a, b) prior. The zero-mean Gaussian
prior penalizes non-zero columns of the dynamic matrix by an amount determined by
the precision parameters. Iterative estimation of these hyperparameters α(k)
j
and the
dynamic matrix A(k) leads to α(k)
j
becoming large for columns whose evidence in the
data is insuﬃcient for overcoming the penalty induced by the prior. Having α(k)
j
→∞
drives a(k)
j
→0, implying that the jth state component does not contribute to the
dynamics of the kth mode. Thus, examining the set of large α(k)
j
provides insight into
the order of that mode. Looking at the kth dynamical mode alone, having a(k)
j
= 0
implies that the realization of that mode is not minimal since the associated Hankel
matrix H of Eq. (2.154) has reduced rank. However, the overall SLDS realization may
still be minimal.
In order to ensure that our choice of C = [Id 0] does not interfere with learning a
sparse realization if one exists, we must constrain the class of dynamical phenomenon
we may analyze. Imagine a realization of an LDS with
˜A =
"
0.8
0
0.2
0
#
,
˜C =
h
1
1
i
.
Then, the transformation to C = [1 0] using
T =
"
0.5
1
0.5
−1
#
leads to
A = T −1 ˜AT =
"
0.5
1
0.15
0.3
#
So, for this example, ﬁxing C = [1 0] would not lead to learning a sparse dynamical
matrix A. Thus, in this chapter we restrict ourselves to ARD modeling of dynamical
phenomena that satisfy the following criterion.
Criterion 4.1.1. If for some realization R a mode k has a(k)
j
= 0, then that realization
must have cj = 0, where cj is the jth column of C. That is, for all possible realizations,
the set of observed state vector components is a subset of those relevant to all modes.
We assume, without loss of generality, that the states are ordered such that C = [C0 0]
(i.e., the observed components are the ﬁrst components of the state vector.)

Sec. 4.1.
The HDP-SLDS and HDP-AR-HMM Models
149
For example, if we have a 3-mode SLDS realization R with
A(1) =
h
a(1)
1
a(1)
2
a(1)
3
0
0
i
A(2) =
h
a(2)
1
a(2)
2
0
a(2)
4
0
i
A(3) =
h
a(3)
1
a(3)
2
a(3)
3
0
a(3)
5
i
,
(4.13)
then the observation matrix must be of the form C =
h
c1
c2
0
0
0
i
to satisfy
Criterion 4.1.1.
This criterion is suﬃcient, though not necessary, for maintaining the sparsity within
each A(k) while still ﬁxing C = [Id 0]. That is, given there exists a realization R1 of
our dynamical phenomena that satisﬁes Criterion 4.1.1, the transformation T to an
equivalent realization R2 with C = [Id 0] will maintain the sparsity structure seen in
R1, which we aim to infer with the ARD prior. The above criterion is reasonable for
many applications, as we often have observations only of components of the state vector
that are essential to all modes, but some modes may have additional components that
aﬀect the dynamics, but are not directly observed. If there does not exist a realization
R satisfying Criterion 4.1.1, we may instead consider a more general model where the
measurement equation is mode-speciﬁc and we place a prior on C(k) instead of ﬁxing
this matrix. However, this model leads to identiﬁability issues that are considerably
less pronounced in the above case.
The ARD prior may also be used to learn variable-order switching VAR processes.
Here, the goal is to “turn oﬀ” entire lag blocks A(k)
i
(whereas in the HDP-SLDS we were
interested in eliminating columns of the dynamic matrix.) Instead of placing indepen-
dent Gaussian priors on each column of A(k) as we did in Eq. (4.12), we decompose the
prior over the lag blocks A(k)
i
:
p(A(k)|α(k)) =
r
Y
i=1
N

vec(A(k)
i
); 0, α−(k)
i
Id2

.
(4.14)
Since each element of a given lag block A(k)
i
is distributed according to the same precision
parameter α(k)
i
, if that parameter becomes large the entire lag block will tend to zero.
Example 4.1.1. For an order three HDP-AR-HMM with observations yt ∈R2, the
dynamic matrix is of the form
A(k) =




|
|
a(k)
1
a(k)
2
|
|




|
|
a(k)
3
a(k)
4
|
|




|
|
a(k)
5
a(k)
6
|
|





150
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
and is distributed as
p(A(k)|α(k)) =
r
Y
i=1
N








a(k)
1,2i−1
a(k)
2,2i−1
a(k)
1,2i
a(k)
2,2i


;


0
0
0
0

,


α−(k)
i
0
0
0
0
α−(k)
i
0
0
0
0
α−(k)
i
0
0
0
0
α−(k)
i








.
In order to examine the posterior distribution on the dynamic matrix A(k), it is
useful to consider the Gaussian induced by Eq. (4.12) and Eq. (4.14) on a vectorization
of A(k). We ﬁrst provide two examples of this transformation, and then a general form
followed by a derivation of the posterior distribution.
Example 4.1.2. Let us consider an HDP-SLDS in which xt ∈R3 such that
A(k) =


|
|
|
a(k)
1
a(k)
2
a(k)
3
|
|
|

.
Then, the distribution induced on a vectorization of A(k) is given by:
p
h
a(k)T
1
a(k)T
2
a(k)T
3
iT
| α(k)

= N




















0
0
0
0
0
0
0
0
0


,


α−(k)
1
0
0
0
0
0
0
0
0
0
α−(k)
1
0
0
0
0
0
0
0
0
0
α−(k)
1
0
0
0
0
0
0
0
0
0
α−(k)
2
0
0
0
0
0
0
0
0
0
α−(k)
2
0
0
0
0
0
0
0
0
0
α−(k)
2
0
0
0
0
0
0
0
0
0
α−(k)
3
0
0
0
0
0
0
0
0
0
α−(k)
3
0
0
0
0
0
0
0
0
0
α−(k)
3




















Returning to the HDP-AR-HMM example of Example 4.1.1, the induced distribution

Sec. 4.1.
The HDP-SLDS and HDP-AR-HMM Models
151
is as follows:
p
h
a(k)T
1
a(k)T
2
. . .
a(k)T
5
a(k)T
6
iT
| α(k)

= N





















0
0
0
0
...
0
0
0
0


,


α−(k)
1
0
0
0
. . .
0
0
0
0
0
α−(k)
1
0
0
. . .
0
0
0
0
0
0
α−(k)
1
0
. . .
0
0
0
0
0
0
0
α−(k)
1
. . .
0
0
0
0
0
0
0
0
...
0
0
0
0
0
0
0
0
. . .
α−(k)
3
0
0
0
0
0
0
0
. . .
0
α−(k)
3
0
0
0
0
0
0
. . .
0
0
α−(k)
3
0
0
0
0
0
. . .
0
0
0
α−(k)
3





















More generally, our ARD prior on A(k) is equivalent to a N(0, Σ(k)
0 ) prior on
vec(A(k)), where
Σ(k)
0
= diag

α(k)
1 , . . . , α(k)
1 , . . . , α(k)
m , . . . , α(k)
m
−1
.
(4.15)
Here, m = n for the HDP-SLDS with n replicates of each α(k)
i
, and m = r for the HDP-
AR-HMM with d2 replicates of α(k)
i
. (Recall that n is the dimension of the HDP-SLDS
state vector xt, r the autoregressive order of the HDP-AR-HMM, and d the dimension
of the observations yt.) To examine the posterior distribution of A(k), we note that we
may rewrite the state equation as,
ψt+1 =
h
¯
ψt,1Iℓ
¯
ψt,2Iℓ
· · ·
¯
ψt,ℓ∗rIℓ
i
vec(A(k)) + et+1(k)
∀t|zt = k
≜˜Ψtvec(A(k)) + et+1(k),
(4.16)
where ℓ= n for the HDP-SLDS and ℓ= d for the HDP-AR-HMM. Using Eq. (4.16), in
Appendix F.2 we derive the posterior distribution as
p(vec(A(k)) | D(k), Σ(k), α(k))
= N −1
 X
t|zt=k
˜ΨT
t−1Σ−(k)ψt, Σ−(k)
0
+
X
t|zt=k
˜ΨT
t−1Σ−(k) ˜Ψt−1

.
(4.17)
Here, N −1(ϑ, Λ) represents a Gaussian N(µ, Σ) with information parameters ϑ = Σ−1µ
and Λ = Σ−1.
Given A(k), and recalling that each precision parameter is gamma

152
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
distributed, the posterior of α(k)
ℓ
is given by
p(α(k)
ℓ
| A(k)) = Gamma

a + |Sℓ|
2 , b +
P
(i,j)∈Sℓa(k)2
ij
2

.
(4.18)
The set Sℓcontains the indices for which a(k)
ij
has prior precision α(k)
ℓ, as illustrated in
the following example.
Example 4.1.3. Returning to the models of Examples 4.1.1- 4.1.2, for the HDP-SLDS
the sets Sℓare deﬁned as
S1 = {a(k)
11 , a(k)
21 , a(k)
31 },
S2 = {a(k)
12 , a(k)
22 , a(k)
32 },
S3 = {a(k)
13 , a(k)
32 , a(k)
33 }.
For the HDP-AR-HMM, we have
S1 = {a(k)
11 , a(k)
21 , a(k)
12 , a(k)
22 },
S2 = {a(k)
13 , a(k)
23 , a(k)
14 , a(k)
24 },
S3 = {a(k)
15 , a(k)
25 , a(k)
16 , a(k)
26 }.
Note that in this model, regardless of the number of observations yt, the size of
Sℓ(i.e., the number of a(k)
ij
used to inform the posterior distribution) remains the
same. Thus, the gamma prior is an informative prior and the choice of a and b should
depend upon the cardinality of Sℓ. For the HDP-SLDS, this cardinality is given by
the maximal state dimension n, and for the HDP-AR-HMM, by the square of the
observation dimensionality d2.
We then place a separate inverse-Wishart prior IW(n0, S0) on Σ(k) and look at the
posterior given A(k):
p(Σ(k) | D(k), A(k)) = IW

Nk + n0, S(k)
ψ| ¯ψ + S0

,
(4.19)
where here, as opposed to in Eq. (4.11), we deﬁne
S(k)
ψ| ¯ψ =
X
t|zt=k
(ψt −A(k) ¯
ψt−1)(ψt −A(k) ¯
ψt−1)T .
(4.20)
Measurement Noise Posterior
For the HDP-SLDS, we additionally place an IW(r0, R0) prior on the measurement noise
covariance R, which we assume is shared between modes. The posterior distribution is
given by
p(R | y1:T , x1:T ) = IW(T + r0, SR + R0),
(4.21)
where SR = PT
t=1(yt −Cxt)(yt −Cxt)T . If we wished to consider a model with mode-
speciﬁc measurement noise, we would simply partition the data according to the mode
sequence and examine the posterior:
p(R(k) | y1:T , x1:T , z1:T ) = IW

Nk + r0, S(k)
R + R0

∀k,
(4.22)

Sec. 4.1.
The HDP-SLDS and HDP-AR-HMM Models
153
(a)
(b)
(c)
Figure 4.3. Depiction of the HDP-SLDS sampling stages. (a) Sampling of transition and dynamic
parameters β, π, and θ conditioned on a sampled state sequence x1:T and mode sequence z1:T . (b)
Block sampling of the mode sequence z1:T conditioned on the sampled state sequence x1:T , transition
distributions π, and dynamic parameters θ. (c) Block sampling of state sequence x1:T conditioned
on the sampled mode sequence z1:T , dynamic parameters θ, and observations y1:T . Hyperparameters
κ,α, and γ are additionally sampled, but omitted here for simplicity. This animation also ignores the
optional stage of sequentially sampling z1:T marginalizing x1:T .
where S(k)
R
= P
t|zt=k(yt −Cxt)(yt −Cxt)T . The derivations in the appendices con-
sider this more general case, while the subsequent sections of this chapter maintain the
assumption that the measurement mechanism is independent of the dynamical regime.
■4.1.2 Gibbs Sampler
For inference in the HDP-AR-HMM, we use a Gibbs sampler that iterates between
sampling the mode sequence, z1:T , and the set of dynamic and sticky HDP-HMM pa-
rameters.
The sampler for the HDP-SLDS is identical with the additional step of
sampling the state sequence, x1:T , and conditioning on this sequence when resampling
dynamic parameters and the mode sequence. Periodically, we interleave a step that
sequentially samples the mode sequence z1:T marginalizing over the state sequence x1:T
in a similar vain to that of Carter and Kohn [26]. We describe the sampler in terms
of the pseudo-observations ψt, as deﬁned by Eq. (4.5), in order to clearly specify the
sections of the sampler shared by both the HDP-AR-HMM and HDP-SLDS. Refer to
Fig. 4.2 and Figs. 4.3-4.4.
Sampling Dynamic Parameters {A(k), Σ(k)}
Conditioned on the mode sequence, z1:T , and the pseudo-observations, ψ1:T , we can
sample the dynamic parameters θ = {A(k), Σ(k)} from the posterior densities of Sec. 4.1.1.
For the ARD prior, we then sample α(k) given A(k). In practice we iterate multiple
times between sampling α(k) given A(k) and A(k) given α(k) before moving to the next
sampling stage.

154
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
(a)
(b)
Figure 4.4. Depiction of the HDP-AR-HMM sampling stages. (a) Sampling of transition and dynamic
parameters β, π, and θ conditioned on the observations y1:T and a sampled mode sequence z1:T . (b)
Block sampling of the mode sequence z1:T conditioned on the observations y1:T , transition distributions
π, and dynamic parameters θ. Hyperparameters κ,α, and γ are additionally sampled, but omitted here
for simplicity.
Sampling Measurement Noise R (HDP-SLDS only)
For the HDP-SLDS, we additionally sample the measurement noise covariance R con-
ditioned on the sampled state sequence x1:T . In this case, we use the notation θ to rep-
resent the set of dynamic parameters and the measurement noise covariance. Namely,
θ = {A(k), Σ(k), R}.
Block Sampling z1:T
As shown in Chapter 3, the mixing rate of the Gibbs sampler for the HDP-HMM can
be dramatically improved by using a truncated approximation to the HDP, such as the
weak limit approximation, and jointly sampling the mode sequence using a variant of the
forward-backward algorithm. In the case of our switching dynamical systems, we must
account for the direct correlations in the observations in our likelihood computation.
The variant of the forward-backward algorithm we use here then involves computing
backward messages mt+1,t(zt) ∝p(ψt+1:T |zt, ¯
ψt, π, θ) followed by recursively sampling
each zt conditioned on zt−1 from
p(zt | zt−1, ψ1:T , π, θ) ∝p(zt | πzt−1)p(ψt | ¯
ψt−1, A(zt), Σ(zt))mt+1,t(zt).
(4.23)
Joint sampling of the mode sequence is especially important when the observations
are directly correlated via a dynamical process since this correlation further slows the
mixing rate of the sampler of Teh et al. [162]. Note that as with the sticky HDP-HMM,
using an order L weak limit approximation to the HDP still encourages the use of a
sparse subset of the L possible dynamical modes.
Block Sampling x1:T (HDP-SLDS only)
Conditioned on the mode sequence z1:T and the set of SLDS parameters θ, our dy-
namical process simpliﬁes to a time-varying linear dynamical system.
We can then

Sec. 4.1.
The HDP-SLDS and HDP-AR-HMM Models
155
block sample x1:T by ﬁrst running a backward Kalman ﬁlter to compute mt+1,t(xt) ∝
p(yt+1:T |xt, zt+1:T , θ) and then recursively sampling each xt conditioned on xt−1 from
p(xt | xt−1, y1:T , z1:T , θ) ∝p(xt | xt−1, A(zt), Σ(zt))p(yt | xt, R)mt+1,t(xt).
(4.24)
The messages are given in information form by mt,t−1(xt−1) ∝N −1(xt−1; ϑt,t−1, Λt,t−1),
where the information parameters are recursively deﬁned as
ϑt,t−1 = A(zt)T Σ−(zt)˜Λt(CT R−1yt + ϑt+1,t)
Λt,t−1 = A(zt)T Σ−(zt)A(zt) −A(zt)T Σ−(zt)˜ΛtΣ−(zt)A(zt),
(4.25)
with ˜Λt = (Σ−(zt) + CTR−1C + Λt+1,t)−1. See Appendix D for a derivation and for a
more numerically stable version of this recursion.
Sequentially Sampling z1:T (HDP-SLDS only)
For the HDP-SLDS, iterating between the previous sampling stages can lead to slow
mixing rates since the mode sequence is sampled conditioned on a sample of the state
sequence. For high-dimensional state spaces Rn, this problem is exacerbated. Instead,
one can analytically marginalize the state sequence and sequentially sample the mode
sequence from p(zt | z\t, y1:T , π, θ).4
This marginalization is accomplished by once
again harnessing the fact that conditioned on the mode sequence, our model reduces
to a time-varying linear dynamical system. When sampling zt and conditioning on the
mode sequence at all other time steps, we can run a forward Kalman ﬁlter to marginalize
the state sequence x1:t−2 producing p(xt−1 | y1:t−1, z1:t−1, θ), and a backward ﬁlter to
marginalize xt+1:T producing p(yt+1:T | xt, zt+1:T , θ). Then, for each possible value of
zt, we combine these forward and backward messages with the local likelihood p(yt | xt)
and local dynamic p(xt | xt−1, θ, zt = k) and marginalize over xt and xt−1 resulting in
the likelihood of the observation sequence y1:T as a function of zt. This likelihood is
combined with the prior probability of transitioning from zt−1 to zt = k and from zt = k
to zt+1. The resulting distribution is given by (see Appendix D.3 for full derivations):
p(zt = k | z\t, y1:T , π, θ) ∝πzt−1(k)πk(zt+1)
|Λ(k)
t |1/2
|Λ(k)
t
+ Λb
t|t|1/2 exp

−1
2ϑ(k)T
t
Λ−(k)
t
ϑ(k)
t
+ 1
2(ϑ(k)
t
+ ϑb
t|t)T (Λ(k)
t
+ Λb
t|t)−1(ϑ(k)
t
+ ϑb
t|t)

(4.26)
with
Λ(k)
t
= (Σ(k) + A(zt)Λ−f
t−1|t−1A(zt)T )−1
ϑ(k)
t
= (Σ(k) + A(zt)Λ−f
t−1|t−1A(zt)T )−1A(zt)Λ−f
t−1|t−1ϑf
t−1|t−1.
(4.27)
4Note that the instantiated parameter values are used in this sequential sampling, and the resampling
of these parameters relies on the sampled state sequence so that one cannot simply iterate between
sampling the mode sequence and parameters.

156
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
Here, ϑf
t|t and Λf
t|t are the updated information parameters for a forward running
Kalman ﬁlter, deﬁned recursively as
Λf
t|t = CT R−1C + Σ−(zt) −Σ−(zt)A(zt)(A(zt)T Σ−(zt)A(zt) + Λf
t−1|t−1)−1A(zt)T Σ−(zt)
ϑf
t|t = CT R−1yt + Σ−(zt)A(zt)(A(zt)T Σ−(zt)A(zt) + Λf
t−1|t−1)−1ϑf
t−1|t−1.
(4.28)
Note that a sequential node ordering for this sampling step allows for eﬃcient up-
dates to the recursively deﬁned ﬁlter parameters. However, this sequential sampling is
still computationally intensive, so our Gibbs sampler iterates between blocked sampling
of the state and mode sequences many times before interleaving a sequential mode se-
quence sampling step. The resulting Gibbs sampler is outlined in Algorithm 13, with
speciﬁcations for the MNIW and ARD priors provided in Algorithm 15 and Algo-
rithm 16, respectively.
■4.2 Results
■4.2.1 MNIW prior
We begin by analyzing the relative modeling power of the HDP-VAR(1)-HMM5, HDP-
VAR(2)-HMM, and HDP-SLDS using the MNIW prior on three sets of test data dis-
played in Fig. 4.5. We compare to a baseline sticky HDP-HMM using ﬁrst diﬀerence
observations, imitating a HDP-VAR(1)-HMM with A(k) = I for all k. In Fig. 4.6 we
display Hamming distance errors that are calculated by choosing the optimal mapping
of indices maximizing overlap between the true and estimated mode sequences.
For all of the scenarios, we set the MNIW hyperparameters from statistics of the
data in the following way. We start by assuming the mean matrix M is 0, and setting
K = Im. This choice centers the mass of the matrix-normal distribution around stable
dynamic matrices while allowing for considerable variability in the matrix values. The
inverse-Wishart portion of the prior is given n0 = m + 2 degrees of freedom, which is
the smallest integer setting6 that maintains a proper prior. Recall that smaller degrees
of freedom implies a broader prior distribution.
For the HDP-AR-HMM, the scale matrix S0 is set to 0.75 times the empirical
covariance of the entire dataset.
Setting the prior directly from the data can help
move the mass of the distribution to reasonable values of the parameter space. Ideally,
one would like to account for the uncertainty in A(k) when setting the distribution of
Σ(k). However, this presents a challenge since the mode-speciﬁc covariance Σ(k) factors
into the the matrix-normal prior on A(k). Simply analyzing the observations yt as if
A(k) = 0 for all k is made as a practical choice; our use of small degrees of freedom
aids in mitigating the aﬀects of this ad-hoc assumption. The fact that the covariance
computed from a pooling of all of the data overestimates the mode-speciﬁc covariance
motivates our slight downscaling (by a factor of 0.75) of the estimate.
5Here we use the notation HDP-VAR(r)-HMM to refer to a HDP-AR-HMM with autoregressive
order r and vector observations.
6Note that it is not required to set the degrees of freedom to an integer.

Given a previous set of mode-speciﬁc transition probabilities π(n−1), the global
transition distribution β(n−1), and dynamic parameters θ(n−1):
1. Set π = π(n−1), β = β(n−1), and θ = θ(n−1).
2. If HDP-SLDS,
(a) For each t ∈{1, . . . , T}, compute {ϑf
t|t, Λf
t|t} as in Algorithm 20.
(b) For each t ∈{T, . . . , 1},
i. Compute {ϑb
t|t, Λb
t|t} as in Algorithm 19
ii. For each k ∈{1, . . . , K}, compute {ϑ(k)
t , Λ(k)
t } as in Eq. (4.27) and set
fk(y1:T ) = |Λ(k)
t |1/2|Λ(k)
t
+ Λb
t|t|−1/2
exp

−1
2ϑ(k)T
t
Λ−(k)
t
ϑ(k)
t
+ 1
2(ϑ(k)
t
+ ϑb
t|t)T (Λ(k)
t
+ Λb
t|t)−1(ϑ(k)
t
+ ϑb
t|t)

.
iii. Sample a mode assignment
zt ∼
L
X
k=1
πzt−1(k)πk(zt+1)fk(y1:T )δ(zt, k).
(c) Working sequentially forward in time sample
xt ∼N(xt; (Σ−(zt) + Λb
t|t)−1(Σ−(zt)A(zt)xt−1 + ϑb
t|t), (Σ−(zt) + Λb
t|t)−1).
(d) Set pseudo-observations ψ1:T = x1:T .
3. If HDP-AR-HMM, set pseudo-observations ψ1:T = y1:T.
4. Block sample z1:T given transition distributions π, dynamic parameters θ, and
pseudo-observations ψ1:T as in Algorithm 14.
5. Update the global transition distribution β (utilizing auxiliary variables m, w,
and ¯
m), mode-speciﬁc transition distributions πk, and hyperparameters α, γ,
and κ as in Algorithm 10 of Sec. 3.1.3.
6. For each k ∈{1, . . . , L}, sample dynamic parameters (A(k), Σ(k)) given the
pseudo-observations ψ1:T and mode sequence z1:T as in Algorithm 15 for the
MNIW prior and Algorithm 16 for the ARD prior.
7. If HDP-SLDS, also sample the measurement noise covariance
R
∼
IW
 
T + r0,
T
X
t=1
(yt −Cxt)(yt −Cxt)T + R0
!
.
8. Fix π(n) = π, β(n) = β, and θ(n) = θ.
Algorithm 13. HDP-SLDS and HDP-AR-HMM Gibbs sampler.

Given a set of mode-speciﬁc transition probabilities π, dynamic parameters θ, and
pseudo-observations ψ1:T :
1. Calculate messages mt,t−1(k), initialized to mT+1,T(k) = 1, and the sample mode
sequence z1:T :
(a) For each t ∈{T, . . . , 1} and k ∈{1, . . . , L}, compute
mt,t−1(k) =
L
X
j=1
πk(j)N
 
ψt;
r
X
i=1
A(j)
i ψt−i, Σ(j)
!
mt+1,t(j)
(b) Working sequentially forward in time, starting with transitions counts
njk = 0:
i. For each k ∈{1, . . . , L}, compute the probability
fk(ψt) = N
 
yt;
r
X
i=1
A(k)
i
ψt−i, Σ(k)
!
mt+1,t(k)
ii. Sample a mode assignment zt as follows and increment nzt−1zt:
zt ∼
L
X
k=1
πzt−1(k)fk(ψt)δ(zt, k)
Note that the likelihoods can be precomputed for each k ∈{1, . . . , L}.
Algorithm 14. Blocked mode-sequence sampler for HDP-AR-HMM or HDP-SLDS.
Given pseudo-observations ψ1:T and mode sequence z1:T , for each k ∈{1, . . . , K}:
1. Construct Ψ(k) and ¯Ψ(k) as in Eq. (4.7).
2. Compute suﬃcient statistics using pseudo-observations ψt associated with zt = k:
S(k)
¯ψ ¯ψ = ¯Ψ(k) ¯Ψ(k)T + K
S(k)
ψ ¯ψ = Ψ(k) ¯Ψ(k)T + MK
S(k)
ψψ = Ψ(k)Ψ(k)T + MKMT .
3. Sample dynamic parameters:
Σ(k)
∼
IW

Nk + n0, S(k)
ψ| ¯ψ + S0

A(k) | Σ(k)
∼
MN

A(k); S(k)
ψ ¯ψS−(k)
¯ψ ¯ψ , Σ−(k), S(k)
¯ψ ¯ψ

.
Algorithm 15. Parameter sampling using MNIW prior.

Given pseudo-observations ψ1:T , mode sequence z1:T , and a previous set of dynamic
parameters (A(k), Σ(k), α(k)), for each k ∈{1, . . . , K}:
1. Construct ˜Ψt as in Eq. (4.16).
2. Iterate multiple times between the following steps:
(a) Construct Σ(k)
0
given α(k) as in Eq. (4.15) and sample the dynamic matrix:
vec(A(k)) | Σ(k), α(k)
∼N −1
 X
t|zt=k
˜ΨT
t−1Σ−(k)ψt, Σ−(k)
0
+
X
t|zt=k
˜ΨT
t−1Σ−(k) ˜Ψt−1

.
(b) For each ℓ∈{1, . . . , m}, with m = n for the SLDS and m = r for the
switching VAR, sample ARD precision parameters:
α(k)
ℓ
| A(k) ∼Gamma

a + |Sℓ|
2 , b +
P
(i,j)∈Sℓa(k)2
ij
2

.
(c) Compute suﬃcient statistic:
S(k)
ψ| ¯ψ =
X
t|zt=k
(ψt −A(k) ¯
ψt−1)(ψt −A(k) ¯
ψt−1)T
and sample process noise covariance:
Σ(k) | A(k)
∼
IW

Nk + n0, S(k)
ψ| ¯ψ + S0

.
Algorithm 16. Parameter sampling using ARD prior.

160
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
When setting the HDP-SLDS inverse-Wishart prior on Σ(k), taking A(k) = 0 still
leaves us with ambiguity between the contribution from the measurement and process
noise terms in driving the covariance of the observations yt. In addition, for an HDP-
SLDS with xt ∈Rn and yt ∈Rd, n > d (i.e., larger state dimension than observation
dimension,) we need a method for setting the mean of the extra n −d dimensions of
the process noise covariance. Thus, for the HDP-SLDS we use the following heuristic:
we set the upper d × d lefthand quadrant of the scale matrix S0 to be 0.675 times the
empirical covariance; the n −d × n −d lower righthand quadrant is set to be diagonal
with determinant equal to that of the upper righthand d × d block. We then set the
inverse-Wishart prior on the measurement noise to have r0 = d + 2 degrees of freedom
and a scale matrix equal to 0.075 times the empirical covariance. Although we have
chosen this speciﬁc heuristic for setting the hyperparameters of the MNIW prior, we
have found that the results are fairly robust to various settings.
As in Chapter 3, we place a Gamma(a, b) prior on the sticky HDP-HMM concen-
tration parameters α + κ and γ, and a Beta(c, d) prior on the self-transition proportion
parameter ρ = κ/(α+κ). We once again choose the weakly informative setting of a = 1,
b = 0.01, c = 10, and d = 1.
For the ﬁrst scenario (Fig. 4.5(a)), the data were generated from a ﬁve-mode switch-
ing VAR(1) process with a 0.98 probability of self-transition and equally likely transi-
tions to the other modes. The same mode-transition structure was used in the subse-
quent two scenarios, as well. The three switching linear dynamical models provide com-
parable performance since both the HDP-VAR(2)-HMM and HDP-SLDS with C = I3
contain the class of HDP-VAR(1)-HMMs. In the second scenario (Fig. 4.5(b)), the data
were generated from a 3-mode switching AR(2) process. The HDP-AR(2)-HMM has
signiﬁcantly better performance than the HDP-AR(1)-HMM while the performance of
the HDP-SLDS with C = [1 0] performs similarly, but has greater posterior variability
because the HDP-AR(2)-HMM model family is smaller. Note that the HDP-SLDS sam-
pler is slower to mix since the hidden, continuous state is also sampled. The data in the
third scenario (Fig. 4.5(c)) were generated from a three-mode SLDS model with C = I3.
Here, we clearly see that neither the HDP-VAR(1)-HMM nor HDP-VAR(2)-HMM is
equivalent to the HDP-SLDS. Note that all of the switching models yielded signiﬁcant
improvements relative to the baseline sticky HDP-HMM. This input representation is
more eﬀective than using raw observations for HDP-HMM learning, but still much less
eﬀective than richer models which switch among learned LDS. Together, these results
demonstrate both the diﬀerences between our models as well as the models’ ability to
learn switching processes with varying numbers of modes.
■4.2.2 ARD prior
We now compare the utility of the ARD prior to the MNIW prior using the HDP-SLDS
model when the true underlying dynamical modes have sparse dependencies relative

Sec. 4.2.
Results
161
0
500
1000
1500
2000
−2
0
2
4
6
8
10
12
14
Time
0
200
400
600
800
1000
0
2
4
6
8
10
12
14
16
Time
0
200
400
600
800
1000
0
50
100
150
200
Time
(a)
(b)
(c)
Figure 4.5. (a) Observation sequence (blue, green, red) and associated mode sequence (magenta) for:
(a) 5-mode switching VAR(1) process, (b) 3-mode switching AR(2) process, and (c) 3-mode SLDS.
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
(a)
(b)
(c)
(d)
Figure 4.6.
Each row corresponds to an observation sequence of Fig. 4.5: (top) 5-mode switching
VAR(1) process, (middle) 3-mode switching AR(2) process, and (bottom) 3-mode SLDS. The associated
10th, 50th, and 90th Hamming distance quantiles over 100 trials are shown for the (b) HDP-VAR(1)-
HMM, (c) HDP-VAR(2)-HMM, (d) HDP-SLDS with C = I (top and bottom) and C = [1 0] (middle),
and (e) sticky HDP-HMM using ﬁrst diﬀerence observations.

162
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
to the assumed model order7. We generated data from a two-mode SLDS with 0.98
probability of self-transition and the following dynamic parameters:
A(1) =


0.8
−0.2
0
−0.2
0.8
0
0
0
0


A(2) =


−0.2
0
0.8
0.8
0
−0.2
0
0
0

,
with C = [I2 0], Σ(1) = Σ(2) = I3, and R = I2. The ﬁrst dynamical process can be
equivalently described by just the ﬁrst and second state components since the third
component is simply white noise that does not contribute to the state dynamics and is
not directly (or indirectly) observed. For the second dynamical process, the third state
component is once again a white noise process, but does contribute to the dynamics
of the ﬁrst and second state components. However, we can equivalently represent the
dynamics of this mode as:
x1,t = −0.2x1,t−1 + ˜e1,t
x2,t = 0.8x1,t−1 + ˜e2,t
where ˜et is a noise term deﬁned by the original process noise and x3,t, and can be shown
to be white. We also notice that the second state component solely relies on the ﬁrst
component of the lagged state vector. Thus, one could rewrite the second dynamical
mode as a linear dynamical system with
˜A(2) =


−0.2
0
0
0.8
0
0
0
0
0


and process noise covariance appropriately redeﬁned.
Notice that this SLDS does not satisfy Criterion 4.1.1 since the second column of
A(2) is zero while the second column of C is not. That is, the second state component
does not contribute to the dynamics of the second mode, but is directly observed.
Nevertheless, because the realization is in our canonical form with C = [I2 0], we still
expect to recover the a(2)
2
= a(2)
3
= 0 sparsity structure. We set the parameters of the
Gamma(a, b) prior on the ARD precisions as a = |Sℓ| and b = a/1000, where we recall
the deﬁnition of Sℓfrom Eq. (4.18). This speciﬁcation ﬁxes the mean of the prior to
1000 while aiming to provide a prior that is equally informative for various choices of
model order (i.e., sizes |Sℓ|).
In Fig. 4.7, we see that even in this low-dimensional example, the ARD provides
superior mode-sequence estimates, as well as a mechanism for identifying non-dynamical
state components. The histograms of the inferred α(k) are shown in Fig. 4.7(d)-(e).
From the clear separation between the sampled dynamic range of α(1)
3
and (α(1)
1 , α(1)
2 ),
7That is, the HDP-SLDS may have dynamical regimes reliant on lower state dimensions, or the
HDP-AR-HMM may have modes described by lower order VAR processes.

Sec. 4.2.
Results
163
0
500
1000
1500
2000
−20
−10
0
10
20
30
Time
Observations
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
(a)
(b)
(c)
0
500
1000
1500
2000
0
20
40
60
80
100
Value
Counts
 
 
ARD hyper: x1
ARD hyper: x2
ARD hyper: x3
0
500
1000
1500
2000
0
20
40
60
80
100
Value
Counts
 
 
ARD hyper: x1
ARD hyper: x2
ARD hyper: x3
(d)
(e)
Figure 4.7. (a) Observation sequence (green, blue) and mode sequence (magenta) of a 2-mode SLDS,
where the ﬁrst mode can be realized by the ﬁrst two state components and the second mode solely by
the ﬁrst. The associated 10th, 50th, and 90th Hamming distance quantiles over 100 trials are shown for
the (b) MNIW and (c) ARD prior. (d)-(e) Histograms of inferred ARD precisions associated with the
ﬁrst and second dynamical modes, respectively, at the 5000th Gibbs iteration. Larger values correspond
to non-dynamical components.
and between that of (α(2)
2 , α(2)
3 ) and α(2)
1 , we see that we are able to correctly identify
dynamical systems with a(1)
3
= 0 and a(2)
2
= a(2)
3
= 0.
■4.2.3 Dancing Honey Bees
Honey bees perform a set of dances within the beehive in order to communicate the
location of food sources. Speciﬁcally, they switch between a set of waggle, turn-right, and
turn-left dances. During the waggle dance, the bee walks roughly in a straight line while
rapidly shaking its body from left to right. The turning dances simply involve the bee
turning in a clockwise or counterclockwise direction. These turning dances often start
at the endpoint of a waggle dance and form a C-like shape that typically returns the bee
to the starting location of the waggle dance. We display six such sequences of honey bee
dances in Fig. 4.8. The data consist of measurements yt = [cos(θt)
sin(θt) xt yt]T ,
where (xt, yt) denotes the 2D coordinates of the bee’s body and θt its head angle8.
Both Oh et al. [129] and Xuan and Murphy [188] used switching dynamical models to
8The data for the six honey bee dance sequences, along with ground truth labels, are available at
http://www.cc.gatech.edu/ borg/ijcv psslds/.

164
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
(1)
(2)
(3)
(4)
(5)
(6)
Figure 4.8. Trajectories of the dancing honey bees for sequences 1 to 6, colored by waggle (red), turn
right (blue), and turn left (green) dances.
analyze these honey bee dances. We wish to analyze the performance of our Bayesian
nonparametric variants of these models in segmenting the six sequences into the dance
labels displayed in Fig. 4.8.
MNIW Prior — Unsupervised
We start by testing the HDP-VAR(1)-HMM using a MNIW prior. (Note that we did
not see performance gains by considering the HDP-SLDS, so we omit showing results
for that architecture.) We set the MNIW prior with mean matrix M = 0, K = 0.1∗Im,
degrees of freedom n0 = 6, and scale matrix S0 set to 0.75 times the empirical covariance
of a pre-processed observation sequence.
The pre-processing involves centering the
position observations around 0 and scaling each component of yt to be within the same
dynamic range. As in the synthetic data examples, we set the Gamma(a, b) priors on
the concentration parameters α+κ and γ to have a = 1 and b = 0.01, and the Beta(c, d)
prior on ρ to have c = 10 and d = 1.
We compare our results to those of Xuan and Murphy [188], who used a change-
point detection technique for inference on this dataset. As shown in Fig. 4.9, our model
achieves a superior segmentation compared to the change-point formulation in almost
all cases, while also identifying modes which reoccur over time.
Oh et al. [129] also presented an analysis of the honey bee data, using an SLDS
with a ﬁxed number of modes. Unfortunately, that analysis is not directly comparable
to ours, because Oh et al. [129] used their SLDS in a supervised formulation in which
the ground truth labels for all but one of the sequences are employed in the inference
of the labels for the remaining held-out sequence, and in which the kernels used in the
MCMC procedure depend on the ground truth labels. (The authors also considered a

Sec. 4.2.
Results
165
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Detection Rate
False Alarm Rate
 
 
HDP−VAR−HMM, unsupervised
HDP−VAR−HMM, supervised
Change−point formulation
Viterbi sequence
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Detection Rate
False Alarm Rate
 
 
HDP−VAR−HMM, unsupervised
HDP−VAR−HMM, supervised
Change−point formulation
Viterbi sequence
(a)
(b)
Figure 4.9. ROC curves for the unsupervised HDP-VAR-HMM, partially supervised HDP-VAR-HMM,
and change-point formulation of Xuan and Murphy [188] using the Viterbi sequence for segmenting
datasets (a) 1-3 and (b) 4-6.
“parameterized segmental SLDS (PS-SLDS),” which makes use of domain knowledge
speciﬁc to honey bee dancing and requires additional supervision during the learning
process.) Nonetheless, in Table 4.1 we report the performance of these methods as well
as the median performance (over 100 trials) of the unsupervised HDP-VAR(1)-HMM
in order to provide a sense of the level of performance achievable without detailed,
manual supervision. As seen in Table 4.1, the HDP-VAR(1)-HMM yields very good
performance on sequences 4 to 6 in terms of the learned segmentation and number of
modes (see Fig. 4.10); the performance approaches that of the supervised method.
For sequences 1 to 3—which are much less regular than sequences 4 to 6—the per-
formance of the unsupervised procedure is substantially worse. In Fig. 4.11, we see
the extreme variation in head angle during the waggle dances of sequences 1 to 3.9 As
noted by Oh, the tracking results based on the vision-based tracker are noisier for these
sequences and the patterns of switching between dance modes is more irregular. This
dramatically aﬀects our performance since we do not use domain-speciﬁc information.
Indeed, our learned segmentations consistently identify turn-right and turn-left modes,
but often create a new, sequence-speciﬁc waggle dance mode. Many of our errors can be
attributed to creating multiple waggle dance modes within a sequence. Overall, how-
ever, we are able to achieve reasonably good segmentations without having to manually
input domain-speciﬁc knowledge.
MNIW Prior — Partially Supervised
The discrepancy in performance between our results and the supervised approach of Oh
et al. [129] motivated us to also consider a partially supervised variant of the HDP-
VAR(1)-HMM in which we ﬁx the ground truth mode sequences for ﬁve out of six of
9From Fig. 4.11, we also see that even in sequences 4 to 6, the ground truth labeling appear to be
inaccurate at times. Speciﬁcally, certain time steps are labeled as waggle dances (red) that look more
typical of a turning dance (green, blue).

166
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
200
400
600
800
0
0.1
0.2
0.3
0.4
0.5
Iteration
Normalized Hamming Distance
200
400
600
800
0
0.1
0.2
0.3
0.4
0.5
Iteration
Normalized Hamming Distance
200
400
600
800
0
0.1
0.2
0.3
0.4
0.5
Iteration
Normalized Hamming Distance
(a)
(b)
(c)
0
200
400
600
1
2
3
4
Estimated mode
Time
0
200
400
600
800
1
2
3
Estimated mode
Time
0
200
400
600
1
2
3
4
Estimated mode
Time
(d)
(e)
(f)
Figure 4.10. (a)-(c) The 10th, 50th, and 90th Hamming distance quantiles over 100 trials are shown
for sequences 4, 5, and 6, respectively. (d)-(f) Estimated mode sequences representing the median error
for sequences 4, 5, and 6 at the 200th Gibbs iteration, with errors indicated in red.
the sequences, and jointly infer both a combined set of dynamic parameters and the
left-out mode sequence. This is equivalent to informing the prior distributions with the
data from the ﬁve ﬁxed sequences, and using these updated posterior distributions as
the prior distributions for the held-out sequence. As we see in Table 4.1, this partially
supervised approach considerably improved performance for these three sequences, es-
pecially sequences 2 and 3. Here, we hand-aligned sequences so that the waggle dances
tended to have head angle measurements centered about π/2 radians. Aligning the
waggle dances is possible by looking at the high frequency portions of the head angle
measurements. Additionally, the pre-processing of the unsupervised approach is not ap-
propriate here as the scalings and shiftings are dance-speciﬁc, and such transformations
modify the associated switching VAR(1) model. Instead, to account for the varying
frames of reference (i.e., point of origin for each bee body) we allowed for a mean µ(k)
on the process noise, and placed an independent N(0, Σ0) prior on this parameter. We
set Σ0 to 0.75 times the scale matrix S0 of the inverse-Wishart prior on Σ(k). Since we
are not shifting and scaling the observations, we set the scale matrix S0 to 0.75 times the
empirical covariance of the ﬁrst diﬀerence observations. We also use n0 = 10 degrees
of freedom, making the distribution around the expected covariance tighter than in the
unsupervised case. Examining ﬁrst diﬀerences is appropriate since the bee’s dynam-

Sec. 4.2.
Results
167
200
400
600
800
1000
−1
−0.5
0
0.5
1
Time
Sine of Head Angle
200
400
600
800
1000
−1
−0.5
0
0.5
1
Time
Sine of Head Angle
100
200
300
400
500
600
−1
−0.5
0
0.5
1
Time
Sine of Head Angle
(1)
(2)
(3)
100
200
300
400
500
600
700
−1
−0.5
0
0.5
1
Time
Sine of Head Angle
200
400
600
800
−1
−0.5
0
0.5
1
Time
Sine of Head Angle
100
200
300
400
500
600
−1
−0.5
0
0.5
1
Time
Sine of Head Angle
(4)
(5)
(6)
Figure 4.11. Measurements of the sine of the bee’s head angle for the six honey bee dance sequences,
colored by the ground truth dance label sequence.
Sequence
1
2
3
4
5
6
HDP-VAR(1)-HMM unsupervised
45.0
42.7
47.3
88.1
92.5
88.2
HDP-VAR(1)-HMM partially supervised
55.0
86.3
81.7
89.0
92.4
89.6
SLDS DD-MCMC
74.0
86.1
81.3
93.4
90.2
90.4
PS-SLDS DD-MCMC
75.9
92.4
83.1
93.4
90.4
91.0
Table 4.1. Median label accuracy of the HDP-VAR(1)-HMM using unsupervised and partially super-
vised Gibbs sampling, compared to accuracy of the supervised PS-SLDS and SLDS procedures, where
the latter algorithms were based on a supervised MCMC procedure (DD-MCMC) [129].
ics are better approximated as a random walk than as i.i.d. observations. Using raw
observations in the unsupervised approach creates a larger expected covariance matrix
making the prior on the dynamic matrix less informative, which is useful in the absence
of other labeled data.
ARD Prior
Using the cleaner sequences 4 to 6, we investigate the honey bee dance’s variable order
structure by assuming a higher order switching VAR model and employing the ARD
prior. Namely, we choose an HDP-VAR(2)-HMM and use the same approach to setting
the hyperparameters as in Sec. 4.2.2. Although not depicted here, the Hamming dis-
tance plots for the HDP-VAR(2)-HMM with the ARD prior are indistinguishable from

168
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
0
200
400
600
800
1000
0
10
20
30
40
50
Value
Counts
 
 
ARD hyper: lag 1
ARD hyper: lag 2
0
200
400
600
800
1000
0
10
20
30
40
50
Value
Counts
 
 
ARD hyper: lag 1
ARD hyper: lag 2
0
10
20
30
40
0
2
4
6
8
10
12
Value
Counts
 
 
ARD hyper: lag 1
ARD hyper: lag 2
0
200
400
600
800
1000
0
20
40
60
80
100
Value
Counts
 
 
ARD hyper: lag 1
ARD hyper: lag 2
0
200
400
600
800
1000
0
20
40
60
80
100
Value
Counts
 
 
ARD hyper: lag 1
ARD hyper: lag 2
0
10
20
30
40
0
5
10
15
20
25
30
35
Value
Counts
 
 
ARD hyper: lag 1
ARD hyper: lag 2
0
200
400
600
800
1000
0
10
20
30
40
50
60
70
80
Value
Counts
 
 
ARD hyper: lag 1
ARD hyper: lag 2
0
200
400
600
800
1000
0
10
20
30
40
50
60
70
80
Value
Counts
 
 
ARD hyper: lag 1
ARD hyper: lag 2
0
10
20
30
40
0
5
10
15
20
Value
Counts
 
 
ARD hyper: lag 1
ARD hyper: lag 2
(a)
(b)
(c)
Figure 4.12.
(a)-(c) Histograms of the inferred ARD hyperparameters for the learned turn right,
turn left, and waggle dance modes, respectively, at the 400th Gibbs iteration for the trials with Ham-
ming distance below the median. Larger values correspond to unnecessary lag components. Note the
horizontal axis scale in column (c).
those of Fig. 4.10(a)-(c) using the HDP-VAR(1)-HMM with the MNIW prior. Thus,
the information in the ﬁrst lag component is suﬃcient for the segmentation problem.
However, the ARD prior informs us of the variable-order nature of this switching dy-
namical process. From Fig. 4.12(a)-(c), we see that the turning dances simply rely on
the ﬁrst lag component while the waggle dance relies on both lag components. To verify
these results, we provided the data and ground truth labels to MATLAB’s lpc10 im-
plementation of Levinson’s algorithm, which indicated that the turning dances are well
approximated by an order 1 process, while the waggle dance relies on an order 2 model.
Thus, our learned orders for the three dances match what is indicated by Levinson’s
algorithm on ground-truth segmented data.
10lpc computes AR coeﬃcients for scalar data, so we analyzed each component of the observation
vector independently. The order was consistent across these components.

Sec. 4.3.
Model Variants
169
■4.3 Model Variants
There are many variants of the general SLDS and switching VAR models that are
pervasive in the literature.
One important example is when the dynamic matrix is
shared between modes; here, the dynamics are instead distinguished based on a switch-
ing mean, such as the Markov switching stochastic volatility (MSSV) model. In the
maneuvering target tracking community, it is often further assumed that the dynamic
matrix is shared and known (due to the understood physics of the target). We explore
both of these variants in the following sections.
■4.3.1 Shared Dynamic Matrix, Switching Driving Noise
In many applications, the dynamics of the switching process can be described by a
shared linear dynamical system matrix A; the dynamics within a given mode are then
determined by some external force acting upon this LDS, and it is how this force is
exerted that is mode-speciﬁc. The general form for such an SLDS is given by:
zt ∼πzt−1
xt = Axt−1 + et(zt)
yt = Cxt + wt,
(4.29)
where
et(k) ∼N(µ(k), Σ(k))
wt ∼N(0, R).
(4.30)
In this scenario, the data are generated from one dynamic matrix, A, and multiple
process noise covariance matrices, Σ(k). Thus, one cannot place a MNIW prior jointly
on these parameters (conditioned on µ(k)) due to the coupling of the parameters in this
prior. We instead consider independent priors on A, Σ(k), and µ(k). We will refer to the
choice of a normal prior on A, inverse-Wishart prior on Σ(k), and normal prior on µ(k)
as the N-IW-N prior. See Appendix F.2 for details on deriving the resulting posterior
distributions given these independent priors. The appendix derives the distributions
assuming both mode-speciﬁc process noise parameters {µ(k), Σ(k)} and a mode-speciﬁc
dynamic matrix A(k). However, the derivations for a shared dynamic matrix A follow
directly by considering data from all time steps, not just those with zt = k.
Stochastic Volatility
An example of an SLDS in a similar form to that of Eq. (4.29) is the Markov switch-
ing stochastic volatility (MSSV) model. Hamilton [63] provides the seminal work in
proposing Markov-switching autoregressive models for econometric modeling; applica-
tions include modeling GNP [63] and interest rates [47]. Kim [94] extends such models
to the SLDS framework. The standard stochastic volatility model [152] is extended to
account for regime switching by So et al. [154], resulting in the MSSV. The MSSV as-
sumes that the log-volatilities follow an AR(1) process with a Markov switching mean.

170
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
1/3/97 7/2/97
6/1/98
1/15/99
1/13/00
Date
Figure 4.13. IBOVESPA stock index daily returns from 01/03/1997 to 01/16/2001.
This underlying process is observed via conditionally independent and normally dis-
tributed daily returns. Speciﬁcally, let yt represent, for example, the daily returns of
a stock index. The state xt is then given the interpretation of log-volatilities and the
resulting state space is given by [27]:
zt ∼πzt−1
xt = axt−1 + et(zt)
yt = ut(xt),
(4.31)
where
et(k) ∼N(µ(k), σ2)
ut(xt) ∼N(0, exp(xt)).
(4.32)
Here, only the mean of the process noise is mode-speciﬁc. Note, however, that the
measurement equation is non-linear in the state xt. Carvalho and Lopes [27] employ
a particle ﬁltering approach to cope with these non-linearities. In [154], the MSSV is
instead modeled in the log-squared-daily-returns domain such that
log(y2
t ) = xt + wt
(4.33)
where wt is additive, non-Gaussian noise. This noise is sometimes approximated by
a moment-matched Gaussian [64], while So et al. [154] use a mixture of Gaussians
approximation. The MSSV is then typically bestowed a ﬁxed set of two or three regimes
of volatility.
We test two variants of the HDP-SLDS on the IBOVESPA stock index (Sao Paulo
Stock Exchange) over the period of 01/03/1997 to 01/16/2001, during which ten key
world events are cited in [27] as aﬀecting the emerging Brazilian market during this
time period. The daily returns are displayed in Fig. 4.13 and the key world event are

Sec. 4.3.
Model Variants
171
Date
Event
07/02/1997
Thailand devalues the Baht by as much as 20%
08/11/1997
IMF and Thailand set a rescue agreement
10/23/1997
Hong Kongs stock index falls 10.4%
South Korea won starts to weaken
12/02/1997
IMF and South Korea set a bailout agreement
06/01/1998
Russias stock market crashes
06/20/1998
IMF gives ﬁnal approval to a loan package to Russia
08/19/1998
Russia oﬃcially falls into default
10/09/1998
IMF and World Bank joint meeting to discuss global economic crisis
The Fed cuts interest rates
01/15/1999
The Brazilian government allows its currency, the Real,
to ﬂoat freely by lifting exchange controls
02/02/1999
Arminio Fraga is named President of Brazils Central Bank
Table 4.2.
Table of 10 key world events aﬀecting the IBOVESPA stock index (Sao Paulo Stock
Exchange) over the period of 01/03/1997 to 01/16/2001, as cited by Carvalho and Lopes [27].
summarized in Table 4.2 and shown in the plots of Fig. 4.14. Use of this dataset was
motivated by the work of Carvalho and Lopes [27], in which a two-mode MSSV model
is assumed.
The ﬁrst variant of the HDP-SLDS we consider, which we refer to as Model A, is
simply the HDP-SLDS of Eq. (4.1) operating on the raw daily returns. The second
variant, referred to as Model B, has a clearer interpretation as an MSSV model. Specif-
ically, we consider a model similar to that of Eq. (4.29) and operate on log-squared
daily returns. The diﬀerence between Model B and the form of the model in Eq. (4.29)
is that we use a DP mixture of Gaussian measurement noise model instead of the single
Gaussian model since this representation better matches the standard MSSV model.
We truncate the measurement noise DP mixture to 10 components. Both models are
assumed to have a one-dimensional underlying state vector (i.e., xt ∈R.) See Table 4.3
for a summary of Model A and Model B.
For the IBOVESPA experiments, we used the same hyperparameter settings for the
prior distributions on the concentration parameters α, γ, and κ as in Sec. 4.2.1-4.2.3.
The prior distributions on the dynamic parameters were once again set from statistics
of the data.
For Model A, where we are considering raw observations and a MNIW prior, we
ﬁrst pre-processed the data in the same manner as the honey bee data by centering the
observations around 0 and scaling the data to be roughly within a [−10, 10] dynamic
range. We then set the MNIW prior with M = 0, K = 1, and n0 = 3 degrees of freedom.
The expected process noise covariance was set to 0.75 times the empirical covariance of
the data. The IW prior on the measurement noise covariance, R, was given r0 = 100

172
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
Model A
Model B
Mode dynamics
zt ∼πzt−1
zt ∼πzt−1
Observation dynamics
xt = A(zt)xt−1 + et(zt)
xt = Axt−1 + et(zt)
yt = Cxt + wt
yt = Cxt + wt
Noise distributions
et(k) ∼N(0, Σ(k))
et(k) ∼N(µ(k), Σ(k))
wt ∼N(0, R)
wt ∼P∞
ℓ=1 ωℓN(0, Rℓ),
ω ∼GEM(σr), Rℓ∼IW(nr, Sr)
Observation type
Daily returns
Log-squared daily returns
Table 4.3. Summary of two variants of the HDP-SLDS for detecting changes in volatility of a stock
index.
degrees of freedom and an expected covariance of 25. Our sampler initializes parameters
from the prior, and for Model A we found it useful to set the prior around large values of
R in order to avoid initial samples chattering between dynamical regimes caused by the
state sequence having to account for the noise in the observations. After accounting for
the residuals of the data in the posterior distribution, we typically learned R ≈10. For
the HDP-AR(r)-HMM’s to which we compare in Fig. 4.14, we use M = 0, K = 10 ∗Ir,
and n0 = 3. Since the additional measurement noise was helpful in the Model A HDP-
SLDS, we allowed for larger noise terms in the HDP-VAR(r)-HMM’s as well. Namely,
we set the expected covariance of the noise process to be equal to the sum of our
expected process noise and measurement noise in the HDP-SLDS case (i.e., 0.75 times
the empirical covariance of the data plus 25.)
For Model B, we rely on the N-IW-N prior described in Sec. 4.3.1. Since we are
allowing for a mean on the process noise and dealing with log-squared daily returns, we
do not perform any pre-processing of the data. For the normal prior on the dynamic
parameter a, we set the mean to 0 and the covariance to 0.75 times the empirical covari-
ance of the observations. This matches with the moments on a deﬁned by our MNIW
prior when ﬁxing the process noise covariance to its expected value (see Eq. (2.93)).
Our normal prior on µ(k) is also given a 0 mean with covariance equal to 0.75 times
the empirical covariance. The IW prior on the process noise Σ(k) was given 3 degrees
of freedom and an expected value of 0.75 times the empirical covariance. Finally, each
component of the mixture-of-Gaussian measurement noise was given an IW prior with
3 degrees of freedom and an expected value of 5 ∗π2, which matches with the moment-
matching technique of Harvey et al. [64].
For the HDP-AR(r)-HMM’s to which we
compare in Fig. 4.14, we once again place a zero-mean normal prior on the dynamic
parameter a with covariance set to the expected noise covariance, which in this case is
equal to 0.75 times the empirical covariance plus 5 ∗π2 (using the same rationale as in
Model A.) This IW prior on the noise parameter is given 3 degrees of freedom. As with
the Model B HDP-SLDS, the mean parameter µ(k) is given a normal, zero-mean prior
with covariance equal to 0.75 times the empirical covariance.
The posterior probability of an HDP-SLDS inferred change point for both Model A

Sec. 4.3.
Model Variants
173
Model A
0
200
400
600
800
1000
0
0.2
0.4
0.6
0.8
1
Probability of Change Point
Day Index
0
200
400
600
800
1000
0
0.2
0.4
0.6
0.8
1
Probability of Change Point
Day Index
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False Alarm Rate
Detection Rate
 
 
HDP−SLDS
HDP−SLDS, non−sticky
HDP−AR(1)−HMM
HDP−AR(2)−HMM
(a)
(b)
(c)
Model B
0
200
400
600
800
1000
0
0.2
0.4
0.6
0.8
1
Probability of Change Point
Day Index
0
200
400
600
800
1000
0
0.2
0.4
0.6
0.8
1
Probability of Change Point
Day Index
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False Alarm Rate
Detection Rate
 
 
HDP−SLDS
HDP−SLDS, non−sticky
HDP−AR(1)−HMM
HDP−AR(2)−HMM
(e)
(f)
(g)
Figure 4.14. (a) Plot of the estimated probability of a change point on each day using 3,000 Gibbs
samples for the HDP-SLDS of Eq. (4.1) on raw daily return measurements. The 10 key events are
indicated with red lines.
(b) Similar plot for the non-sticky HDP-SLDS with no bias towards self-
transitions. (c) ROC curves for the HDP-SLDS, non-sticky HDP-SLDS, HDP-AR(1)-HMM, and HDP-
AR(2)-HMM. (e)-(g) Analogous plots for the HDP-SLDS matched to the MSSV by using a shared
dynamic matrix and allowing a mean on the mode-speciﬁc process noise and a mixture of Gaussian
measurement noise model. For this matched model, we use log-squared daily returns.
and Model B is shown in Fig. 4.14(a) and Fig. 4.14(d), respectively. In Fig. 4.14(b) and
Fig. 4.14(e), we display the corresponding plots for non-sticky variants of these HDP-
SLDS models (i.e., with κ = 0 so that there is no bias towards mode self-transitions.)
The Model A HDP-SLDS is able to infer very similar change points to those presented
in [27]. Interestingly, the HDP-SLDS consistently identiﬁes three regimes of volatility
versus the assumed two-mode model of Carvalho and Lopes [27]. Without the sticky
extension, the non-sticky model variant over-segments the data and rapidly switches
between redundant states leading to many inferred change points that do not align with
any world event. The Model B HDP-SLDS performs similarly; the non-sticky variant
here once again rapidly switches between states, but not as frequently as in the Model
A case. Overall, although the state of the Model B HDP-SLDS has the interpretation
of log-volatilities, we see that the Model A HDP-SLDS can also capture regime-changes
in the dynamics of this stock index.

174
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
In Fig. 4.14(c) and Fig. 4.14(f), the overall change-point detection performance of
the Model A and Model B HDP-SLDS are compared to that of the HDP-AR(1)-HMM,
HDP-AR(2)-HMM, and non-sticky HDP-SLDS. The ROC curves shown in these plots
are calculated by windowing the time axis and taking the maximum probability of a
change point in each window. These probabilities are then used as the conﬁdence of
a change point in that window. For both Model A and Model B, we clearly see the
advantage of using an SLDS model combined with the sticky HDP-HMM prior on the
mode sequence.
■4.3.2 Fixed Dynamic Matrix, Switching Driving Noise
There are some cases in which the dynamical model is well-deﬁned through knowledge
of the physics of the system being observed, such as simple kinematic motion. More
complicated motions can typically be modeled using the same ﬁxed dynamical model,
but using a more complex description of the driving force. Returning to the model of
Eq. (2.140), a generic LDS driven by an unknown control input ut can be represented
as:
xt = Axt−1 + But + vt
yt = Cxt + Dut + wt,
(4.34)
where vt ∼N(0, Q) and wt ∼N(0, R). It is often appropriate to assume D = 0, as we
do herein.
Maneuvering Target Tracking
The methods for modeling a maneuvering target can be primarily classiﬁed into three
categories: (1) methods which approximate the non-random but unobserved control
input sequence u1:T as a deterministic unknown, (2) methods which model u1:T as a
random process, and (3) methods which use a set of dynamic systems to model typical
target trajectories. For a thorough survey of maneuvering target tracking, see [144, 145].
Inference on systems modeled with deterministic unknown inputs is computationally
complex, and thus modeling the control input as a random process is a common sim-
plifying assumption. The most basic of these models is to take the control to be white
noise. The constant velocity (CV) and constant acceleration (CA) models, with random
walks on velocity and acceleration, respectively, are contained within this class. An im-
mediate extension is to model the control input as a zero-mean Markov process [153].
A more realistic model is to assume that the input is a stochastic process with both
temporal correlation and a time-varying, non-zero mean, such as a switching Markov
process. Classical approaches, such as [119], rely on a ﬁxing a ﬁnite set of mean values,
typically a discretization of the acceleration space, and setting the probability of jump-
ing between these modes. One can describe such a model of noisy, jump-mean control

Sec. 4.3.
Model Variants
175
inputs by:
zt ∼πzt−1
xt = Axt−1 + But(zt) + vt
yt = Cxt + wt,
(4.35)
where
ut(k) ∼N(µ(k), Σ(k))
vt ∼N(0, Q)
wt ∼N(0, R).
(4.36)
Equivalently, the state dynamics can be described as
xt = Axt−1 + et(zt)
(4.37)
et(k) ∼N(Bµ(k), BΣ(k)BT + Q).
(4.38)
This model can be captured by our HDP-SLDS formulation of Eq. (4.29) with a ﬁxed
dynamic matrix and mode-speciﬁc, non-zero mean process noise. Such a formulation
can be viewed as an extension of the work by Caron et al. [24] in which the exogenous
input is modeled as an independent noise process (i.e., no Markov structure on zt)
generated from a DP mixture model.
An alternative formulation for capturing maneuvering target dynamics is that of
multiple models, which describes the targets maneuvers as switches between a set of
dynamic models, e.g. CV and CA. These dynamic models can capture more coherent
maneuver behavior than random processes and are well-suited to applications where
target dynamics have well-deﬁned system models with known parametrization, such as
tracking civilian air traﬃc, tactical ballistic missiles, etc. However, for tracking hostile
or noncooperative targets, such as evasive manned aircraft, the strong maneuverabil-
ity and unpredictable behavior are challenging for multiple model methods which rely
heavily on prior knowledge for deﬁning the models and mode-switching probabilities. In
this section, we examine the ability of the HDP-SLDS to track a highly maneuverable
target. Here, we assume that the dynamic matrix A is well-modeled by either a CV
or CA model, but the control input process is challenging to deﬁne. In scenarios for
which a good representative set of kinematic models have not been developed, such as
in tracking targets like ships and ground targets [145], the more general HDP-SLDS
that additionally learns a set of mode-speciﬁc dynamic matrices may be better suited.
An Alternative Sampling Scheme
In some applications, the control input might be much lower-dimensional than the state.
Harnessing the knowledge of the dynamics of the system and sampling the control in-
put instead of the state sequence can lead to dramatic improvements in the mixing rate
of the Gibbs sampler. One can sequentially block-sample (zt, ut), marginalizing over
the state sequence x1:T , the transition distributions π, and the dynamic parameters
θ = {µ(k), Σ(k)}. As with the direct assignment sticky HDP-HMM sampler of Algo-
rithm 9, this sampler relies on instantiating the global transition distribution β. We

176
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
jointly sample (zt, ut) because a change in assignment of the mode zt may induce a
signiﬁcant change in the distribution over the input ut; sampling these variables inde-
pendently could result in diﬀerent local modes between which it is very challenging to
move. For this sampler, we assume that the measurement noise covariance R is known.
Alternatively, one could interleave a step of sampling the state sequence x1:T given z1:T
and u1:T , and then sample R conditioned on this state sequence.
The derivation of the conditional distribution of (zt, ut) is outlined below, with
details in Appendix E. We speciﬁcally examine a sequential node ordering for the Gibbs
sampler to allow for simple updates, as will become clear in the following derivations.
Let θ denote the ﬁxed parameters {A, Q, R}. We begin by writing
p(ut, zt | z\t, u\t, y1:T , θ, β, α, κ, λ)
= p(zt | z\t, u\t, y1:T , θ, β, α, κ, λ)p(ut | z1:T , u\t, y1:T , θ, λ).
(4.39)
As derived in Appendix E, we can write the conditional density of ut for each candidate
zt as,
p(ut | zt = k, z\t, u\t, y1:T , θ, λ) ∝p(ut | {uτ|zτ = k, τ ̸= t}, λ)p(y1:T | u1:T , θ)
∝N −1(ut; ˆΣ−1
k ˆµk + ϑt, ˆΣ−1
k
+ Λt).
(4.40)
Given a normal inverse-Wishart (NIW) prior on {µ(k), Σ(k)}, the posterior distribution
of ut given all other control inputs u\t is a Student-t distribution (see Eq. (2.87)),
which we approximate by a moment-matched Gaussian N(ˆµk, ˆΣk) (see Eq. (2.89)).
The information parameters ϑt and Λt arise from marginalization of the state sequence
by once again harnessing conditionally linear dynamics. Speciﬁcally, conditioning on the
control input sequence simpliﬁes the SLDS to an LDS with a deterministic control input
u1:T . Thus, conditioning on u1:t−1,t+1:T allows us to marginalize the state sequence in
the following manner. We run a forward Kalman ﬁlter to pass a message from t −2 to
t −1, which is updated by the local likelihood at t −1. A backward ﬁlter is also run
to pass a message from t + 1 to t, which is updated by the local likelihood at t. These
updated messages are combined with the local dynamic p(xt | xt−1, ut, θ) and then
marginalized over xt and xt−1, resulting in the likelihood of the observation sequence
y1:T as a function of ut, the variable of interest. Because the sampler conditions on
control inputs, the ﬁlter for this time-invariant system can be eﬃciently implemented by
pre-computing the error covariances and then solely computing local Kalman updates
at every time step. Of note is that the computational complexity is linear in the training
sequence length, as well as the number of currently instantiated maneuver modes.
We similarly derive the distribution on zt as
p(zt | z\t, u\t, y1:T , θ, β, α, κ, λ) ∝p(zt = k | z\t, β, α, κ)Ck,
(4.41)

Sec. 4.3.
Model Variants
177
where p(zt = k | z\t, β, α, κ) is as in Eq. (A.10) for the sticky HDP-HMM and
Ck =
|ˆΣ−1
k |1/2
|ˆΣ−1
k
+ Λt|1/2
exp

−1
2 ˆµT
k ˆΣ−1
k ˆµk + 1
2(ˆΣ−1
k ˆµk + ϑt)T (ˆΣ−1
k
+ Λt)−1(ˆΣ−1
k ˆµk + ϑt)

.
(4.42)
The derivation of this constant is very similar to the constant that arises in the sequential
sampling of zt described in Sec. 4.1.2.
Results
We compare the performance of our Bayesian nonparametric target tracking algorithm
to that of a multiple model algorithm commonly used within the target-tracking com-
munity. Due to the exponentially growing mode-sequence hypothesis space with time,
online multiple model inference methods rely on various algorithms for reducing the
hypothesis space by using a cooperation strategy, which includes pruning, merging, and
selection. Thus, these methods are theoretically suboptimal, but work well in certain
situations. For each mode hypothesis, a conditional ﬁlter is run and the state estimates
are then fused. Overall, the multiple model method requires model-set determination,
a cooperation strategy, conditional ﬁltering, and output processing. The standard in
state-of-the-art multiple model inference algorithms, which we use in the following re-
sults, is the interacting multiple model (IMM) method [20, 115, 145].
For our Bayesian nonparametric approach, we consider the HDP-SLDS of Eq. (4.35)
and take the ﬁxed dynamic and control matrices to be:11
A =


1 ∆T
1
2∆T 2
0
1
∆T
0
0
1

B =


1
2∆T 2
∆T
0

.
(4.43)
Here, the state consists of the x-direction position, velocity, and acceleration and we
assume that we only have noisy observations of the position of the target such that
C = [1 0 0].
We compare this HDP-SLDS to a multiple model formulation using the standard
constant velocity (CV) and constant acceleration (CA) coordinate-uncoupled maneuver
models, with the state being x-direction position and velocity in the case of the CV
model and x-direction position, velocity, and acceleration in the case of the CA model.
The multiple model state space equations are,
xt = A(zt)xt−1 + et(zt)
yt = C(zt)xt + wt.
(4.44)
11For this scenario, we take ut to be the control input integrated into the system over the time
window t −1 to t. This parallels the IMM dynamics to which we compare our performance.

178
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
The system matrices for these two models are given by
A(CV ) =
"
1
∆T
0
1
#
C(CV ) =
h
1
0
i
Σ(CV ) = qCV
"
1
3∆T 3
1
2∆T 2
1
2∆T 2
∆T
#
(4.45)
A(CA) =


1
∆T
1
2∆T 2
0
1
∆T
0
0
1


C(CA) =
h
1
0
0
i
Σ(CA) = qCA


1
20∆T 5
1
8∆T 4
1
6∆T 3
1
8∆T 4
1
3∆T 3
1
2∆T 2
1
6∆T 3
1
2∆T 2
∆T

.
(4.46)
The IMM inference algorithm requires the deﬁnition of a transition matrix P deﬁn-
ing the probability Pij of transitioning to model j given a current model i. We take
P =
"
pii
1 −pii
1 −pii
pii
#
,
(4.47)
since we have no prior bias towards the CA model versus the CV model. Additionally,
we assume that initially both models are equally likely.
It is important to understand the diﬀerences between the CV-CA multiple model
formulation and our HDP-SLDS. The CV-CA multiple model formulation attempts to
open up the bandwidth for maneuvers by incorporating a random walk on acceleration.
However, such a formulation cannot capture the abrupt jumps in acceleration charac-
teristic of highly maneuverable targets, whereas our formulation does. Speciﬁcally, the
HDP-SLDS model we have deﬁned takes the acceleration to be a Markov jump-mean
process with a random walk noise component. Here, the parameter µ(zt) represents
the mean of this process at time t while Σ(zt) allows for mode-speciﬁc variation of the
control input realization ut. If we learn a mode with a mean of zero, our model reduces
to that of the CA model (i.e. zero-mean random walk on acceleration.) When, in ad-
dition, the learned covariance Σ(zt) and ﬁxed process noise covariance Q are small, this
model adequately describes a non-maneuvering target. However, by having the ﬂexibil-
ity of learning modes with non-zero means, our model can account for fast changes in
acceleration.
To compare the performance of the HDP-SLDS to that of the CV-CA IMM, we
generated two types of simulated observations of position versus time. The ﬁrst sequence
(Scenario A) is a noisy version of a modulated sinusoid starting at a random phase point

Sec. 4.3.
Model Variants
179
with measurement noise variance R = 5 ∗105. The underlying position sequence has
continuous derivatives so that velocity and acceleration vary smoothly.
The second
sequence (Scenario B) was a noisy step function generated from a three-mode Markov
jump-mean model with12 R = 5 ∗109. The three modes of the model were deﬁned
by means {−50, 0, 50} and variances {5, 1, 5}, respectively.
The probability of self-
transition was set to 0.99 while transitions to the other modes were equally likely. By
considering both smooth and abrupt changes in acceleration, we show the ﬂexibility of
the proposed HDP-SLDS model.
We set the HDP-SLDS and CV-CA IMM model parameters in the following manner.
We use an initial error covariance P0 = 100 ∗I3 and step size ∆T = 1. For the CV-
CA IMM, we take qCA = qCV = 10, while for the HDP-SLDS, we use a small noise
covariance Q = 0.01∗I3 in order to encourage u1:T to capture the statistical properties
of the input process. For the HDP-SLDS, we place a conjugate NIW(0.001, 0, 50, 1)
prior on the parameters {µ(k), Σ(k)}.
In the following set of results we present two methods of using the samples provided
by the HDP-SLDS inference algorithm. One method involves learning the control in-
put sequence u1:T from the observation sequence y1:T and then calculating Kalman
smoothed state estimates given the learned input sequence. We take our estimate of
u1:T to simply be the average over 100 Gibbs samples. We refer to this method as the
HDP-SLDS smoother. The batch processing of data used by the HDP-SLDS smoother is
impractical in many applications. Therefore, we also present an oﬄine-training online-
tracking approach to learning a set of dynamic models that can be used within the
IMM framework. Speciﬁcally, we run the HDP-SLDS sampler on training data until
it is well-mixed and then examine a set of 10 samples of (u1:T , z1:T ). From each of
these samples, we infer a set of parameters {µ(k), Σ(k)} and transition densities πk. The
resulting HDP-SLDS-learned IMMs consist of CA dynamic models with diﬀerent noise
processes, both in terms of mean and covariance as determined by {µ(k), Σ(k)}, and
by the transition probabilities πk. The results we present for this method are state
estimates averaged over the 10 parallel HDP-SLDS-learned IMMs, where the models
were trained on either sinusoidal data with random phase shifts for Scenario A, or
from observation sequences generated from random step function input sequences on
acceleration for Scenario B.
In Fig. 4.15(d) and Fig. 4.16(d) we plot the performance of the CV-CA IMM as
a function of the transition probability parameter pii. We see that the IMM exhibits
strong model sensitivity to pii, while the HDP-SLDS does not depend on presetting this
parameter. In the experiments for Scenarios A and B (shown in Fig. 4.15 and Fig. 4.16,
respectively), we ﬁx pii = 0.95 in order to consider a single “good” IMM for both of
these scenarios.
In Fig. 4.15(a) and Fig. 4.16(a), we show the track estimates of position versus
time for the CV-CA IMM, HDP-SLDS-learned IMMs, and HDP-SLDS smoother, as
12The large measurement noise setting is due to the large scale of the position observations depicted
in 4.16.

180
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
Scenario A
50
100
150
200
250
300
−6000
−4000
−2000
0
2000
4000
6000
Time
Track estimates
50
100
150
200
250
300
0
500
1000
1500
2000
2500
3000
Time
L2 position error
(a)
(b)
1
2
3
4
5
6
7
8
9
10
0
50
100
150
200
250
300
350
400
450
Number of clusters
Counts over 1,000 iterations
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
1
2
3
4
5
x 10
5
Probability of self transition
Total L2 error
(c)
(d)
Figure 4.15.
Plots of (a) observation sequence (gray) with track estimates and (b) associated L2
error for the CV-CA IMM (blue,—), HDP-SLDS learned IMMs (red,- -), and HDP-SLDS smoother
(green,· · · ) on a modulated sinusoid (Scenario A). (c) Histogram of number of HDP-SLDS maneuver
modes over 1,000 Gibbs iterations on the training sequence for the learned IMMs. (d) Total L2 error
versus self transition probability pii depicting the model sensitivity of the IMM as compared to the
HDP-SLDS learned IMMs or HDP-SLDS smoother, which do not depend on presetting this parameter.
well as the noisy observations. The associated average L2 position errors versus time
averaged over 10 measurement realizations of the true target trajectory are plotted in
Fig. 4.15(b) and Fig. 4.16(b). These plots show the performance gain of the HDP-SLDS
methods over the CV-CA IMM. Relative to the CV-CA IMM performance, the HDP-
SLDS-learned IMMs have a 42% average decrease in total L2 error in the modulated
sinusoid case and 52% decrease in the step function case while the HDP-SLDS smoother
has decreases of 78% and 75%.
One can analyze the complexity of the inferred HDP-SLDS model by looking at the
number of maneuver modes to which a signiﬁcant number of observations are assigned.
We histogram those modes with more than 5% of the assignments over 1,000 Gibbs
iterations in Fig. 4.15(c) and Fig. 4.16(c). When the true control inputs are drawn
from a small ﬁnite set, as in the step function scenario, the HDP-SLDS describes the
data with fewer model components than the more complicated modulated sinusoid
scenario. These results emphasize the ﬂexibility of the HDP-SLDS approach.

Sec. 4.4.
Discussion and Future Work
181
Scenario B
50
100
150
200
250
300
350
400
450
500
−2
0
2
4
6
8
10
12
14
x 10
5
Time
Track estimates
50
100
150
200
250
300
350
400
450
500
0
1
2
3
4
5
6
7x 10
4
Time
L2 position error
(a)
(b)
1
2
3
4
5
6
7
8
9
10
0
50
100
150
200
250
300
Number of clusters
Counts over 1,000 iterations
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
2
4
6
8
10
12
x 10
7
Probability of self transition
Total L2 error
(c)
(d)
Figure 4.16. Analogous plots to those of Fig. 4.15, but for a step function control input (Scenario B).
■4.4 Discussion and Future Work
In this chapter, we have addressed the problem of learning switching linear dynamical
models with an unknown number of modes for describing complex dynamical phenom-
ena.
We presented a Bayesian nonparametric approach and demonstrated both the
utility and versatility of the developed HDP-SLDS and HDP-AR-HMM on real appli-
cations. Using the same parameter settings, although diﬀerent model choices, in one
case we are able to learn changes in the volatility of the IBOVESPA stock exchange
while in another case we learn segmentations of data into waggle, turn-right, and turn-
left honey bee dances. We also described a method of applying automatic relevance
determination (ARD) as a sparsity-inducing prior, leading to ﬂexible and scalable dy-
namical models that allow for identiﬁcation of variable order structure. The utility of
this model was demonstrated on synthetic data and the honey bee dance sequences.
Following the presentation of the generic HDP-SLDS and HDP-AR-HMM models,
we considered adaptations to speciﬁc forms often examined in the literature. Speciﬁ-
cally, we developed Bayesian nonparametric variants of the Markov switching stochas-
tic volatility model and a standard multiple model target tracking formulation. For
the target tracking application, we derived a collapsed Gibbs sampler that eﬃciently
computes smoothed state estimates from noisy observation sequences by sampling the

182
CHAPTER 4.
BAYESIAN NONPARAMETRIC LEARNING OF SLDS
lower-dimensional control input sequence. We showed that the parameters inferred by
this sampler can be utilized in an online IMM ﬁlter and demonstrated signiﬁcant gains
over the ﬁxed model set commonly used in tracking applications.
The batch processing of the Gibbs samplers derived herein may be impractical
and oﬄine-training online-tracking infeasible for certain applications. Due both to the
nonlinear dynamics and uncertainty in model parameters, exact recursive estimation is
infeasible. We can leverage the conditionally linear dynamics and use Rao-Blackwellized
particle ﬁltering (RBPF) [29], an eﬃcient sequential importance sampler (SIS). Here,
the particles represent samples from p(φ1:t | y1:t), where φt = {A(zt), Σ(zt)}. Estimation
of p(x1:t | φ1:t, y1:t) can then be computed using Kalman ﬁltering. However, particle
ﬁlters can suﬀer from a progressively impoverished particle representation, especially
in the case of static parameter estimation. In our scenario, both the hyperparameters
and the HDP distribution over parameters φt are static. Thus, we could modify the SIS
methods, in a similar vain to resample-move [56] which interleaves Gibbs and particle
ﬁlter steps, so that potential new values for static parameters continue to be explored
over time. Another approach for an online implementation is that of decayed MCMC
ﬁltering [114]. The decayed MCMC algorithm is similar to standard MCMC methods
except instead of uniformly sampling the state variables the algorithm concentrates
sampling activity to the recent past, since these states are the most relevant to the cur-
rent state. Decayed MCMC is guaranteed to converge to the true marginal distribution
given an appropriate decay function, and has provable rates of convergence.
Another direction of future research is to develop stronger sparsity inducing priors.
The ARD prior provides a simple quadratic, or L2, penalty on the columns of A.
Alternatively, one could examine the class of spike and slab priors [28, 73, 182], which
place an additional spike of probability mass concentrated around the random variable
being exactly zero, and have been successfully applied to both regression and factor
analysis.
Other examples include the Laplace prior, corresponding to the Lasso L1
penalty [166], and the class of scale mixture of Gaussian priors presented in [23].
Overall, the formulation we developed herein represents a ﬂexible, Bayesian non-
parametric model for describing nonlinear dynamical phenomena and discovering simple
underlying structures to describe time series. In the next chapter, we explore how to
transfer knowledge between multiple, related time series.

Chapter 5
Sharing Features among Dynamical
Systems with Beta Processes
I
N many applications, one would like to discover and model dynamical behaviors which
are shared among several related time series. For example, consider video or motion
capture data depicting multiple people performing a number of related tasks. By jointly
modeling such sequences, we may more robustly estimate representative dynamic mod-
els, and also uncover interesting relationships among activities. We speciﬁcally focus on
time series where behaviors can be individually modeled via temporally independent or
linear dynamical systems, and where transitions between behaviors are approximately
Markovian. Examples of such Markov jump processes include the hidden Markov model
(HMM), switching vector autoregressive (VAR) process, and switching linear dynam-
ical system (SLDS). These models have proven useful in such diverse ﬁelds as speech
recognition, econometrics, remote target tracking, and human motion capture. We have
presented Bayesian nonparametric approaches to learning such models in Chapters 3
and 4, and examined some of these applications. In this chapter, our approach envisions
a large library of behaviors, and each time series or object exhibits a subset of these
behaviors. We then seek a framework for discovering the set of dynamic behaviors,
or features, that each object exhibits. We particularly aim to allow ﬂexibility in the
number of total and sequence-speciﬁc behaviors, and encourage objects to share similar
subsets of the large set of possible behaviors.
One can represent the set of behaviors an object exhibits via an associated list of
features. A standard featural representation for N objects, with a library of K features,
employs an N × K binary matrix F = {fik}.
Setting fik = 1 implies that object i
exhibits feature k. Our desiderata motivate a Bayesian nonparametric approach based
on the beta process [67], which allows for inﬁnitely many potential features. As shown
by Thibaux and Jordan [165], integrating over the latent beta process random measure
induces a predictive distribution on features equivalent to the Indian buﬀet process
(IBP) of Griﬃths and Ghahramani [62]. The beta process, and its connection with the
IBP, are reviewed in Sec. 2.9.4. Given a sampled feature set, our model reduces to a
collection of Bayesian HMMs (or SLDS) with partially shared parameters.
One approach to a Bayesian nonparametric representation of multiple time series
183

184
CHAPTER 5.
SHARING FEATURES AMONG DYNAMICAL SYSTEMS WITH BETA PROCESSES
would be to consider an extension of the sticky HDP-HMM and HDP-SLDS models of
Chapters 3 and 4, respectively, in which the time series are tied together with the same
set of transition and emission parameters. However, such an HDP-HMM or HDP-SLDS
does not select a subset of behaviors for a given time series, but restrictively assumes
that all time series share the same set of behaviors and switch among them in exactly
the same manner. Another recent approach to Bayesian nonparametric modeling of
Markov-switching time series is the inﬁnite factorial HMM [171]. The inﬁnite factorial
HMM also does not solve our problem; it instead models a single time series using an
inﬁnite set of latent features, which evolve via independent Markovian dynamics. Our
work focuses on modeling multiple time series and on capturing dynamical modes that
are shared among the series.
Our results are obtained via an eﬃcient and exact Markov chain Monte Carlo
(MCMC) inference algorithm. In particular, we exploit the ﬁnite dynamical system
induced by a ﬁxed set of features to eﬃciently compute acceptance probabilities, and
reversible jump birth and death proposals to explore new features. We validate our
sampling algorithm using several synthetic datasets, and also demonstrate promising
unsupervised segmentation of data from the CMU motion capture database [169].
■5.1 Describing Multiple Time Series with Beta Processes
Assume we have a set of N objects, each of whose dynamics is described by a switching
vector autoregressive (VAR) process, with switches occurring according to a discrete-
time Markov process. For a review of switching VAR processes, refer to Sec. 2.7.3. As
we have seen in Chapter 4, such autoregressive HMMs (AR-HMMs) provide a simpler,
but often equally eﬀective, alternative to SLDS. Let y(i)
t
represent the observation vector
of the ith object at time t, and z(i)
t
the latent dynamical mode. Assuming an order r
switching VAR process, denoted by VAR(r), we have
z(i)
t
∼π(i)
z(i)
t−1
y(i)
t
=
r
X
j=1
Aj,z(i)
t y(i)
t−j + e(i)
t (z(i)
t ) ≜Az(i)
t ˜y(i)
t
+ e(i)
t (z(i)
t ),
(5.1)
where e(i)
t (k) ∼N(0, Σk), Ak =
h
A1,k
. . .
Ar,k
i
, and ˜y(i)
t
=
h
y(i)T
t−1
. . .
y(i)T
t−r
iT
.
The standard HMM with Gaussian emissions arises as a special case of this model when
Ak = 0 for all k. We refer to these VAR processes, with parameters θk = {Ak, Σk}, as
behaviors, and use a beta process prior to couple the dynamic behaviors exhibited by
diﬀerent objects or sequences.
Let fi be a vector of binary indicator variables, where fik denotes whether object i
exhibits behavior k. As in Sec. 2.9.4, we can deﬁne this feature vector by utilizing the

Sec. 5.1.
Describing Multiple Time Series with Beta Processes
185
beta process prior in the following speciﬁcation:
B | B0 ∼BP(1, B0)
Xi | B ∼BeP(B),
i = 1, . . . , N,
(5.2)
where fik are the weights associated with the Bernoulli process realizations
Xi =
X
k
fikδθk.
(5.3)
As discussed in Sec. 2.9.4, marginalization of the latent beta process random measure
B induces a predictive distribution on the features fik equivalent to the IBP.
Given fi, we deﬁne a feature-constrained transition distribution π(i) = {π(i)
k }, which
governs the ith object’s transitions among its set of dynamic behaviors. In particular,
for each object i we deﬁne a doubly inﬁnite collection of gamma-distributed random
variables:
η(i)
jk | γ, κ ∼Gamma(γ + κδ(j, k), 1)
(5.4)
Here, δ(j, k) indicates the Kronecker delta function. We denote this collection of transi-
tion variables by η(i), and use them to deﬁne object-speciﬁc, feature-constrained tran-
sition distributions:
π(i)
j
=
h
η(i)
j1
η(i)
j2
. . .
i
⊗fi
P
k|fik=1 η(i)
jk
(5.5)
Here, ⊗denotes the element-wise vector product. This construction deﬁnes π(i)
j
over
the full set of positive integers, but assigns positive mass only at indices k where fik = 1,
constraining the object to solely transition amongst the dynamical behaviors indicated
by its feature vector.
The preceding generative process can equivalently be represented via a sample ˜π(i)
j
from a ﬁnite Dirichlet distribution of dimension Ki = P
k fik, containing the non-zero
entries of π(i)
j :
˜π(i)
j
| fi, γ, κ ∼Dir([γ, . . . , γ, γ + κ, γ, . . . γ])
(5.6)
The κ hyperparameter places extra expected mass on the component of ˜π(i)
j
correspond-
ing to a self-transition π(i)
jj , analogously to the sticky hyperparameter of Chapter 3. We
also use the representation
π(i)
j
| fi, γ, κ ∼Dir([γ, . . . , γ, γ + κ, γ, . . . ] ⊗fi),
(5.7)
implying π(i)
j
=
h
π(i)
j1
π(i)
j2
. . .
i
, with only a ﬁnite number of non-zero entries π(i)
jk .
This representation is really an abuse of notation since the Dirichlet distribution is

186
CHAPTER 5.
SHARING FEATURES AMONG DYNAMICAL SYSTEMS WITH BETA PROCESSES
. . .
. . .
N
∞
ω
θk
k
z1
(i)
z2
(i)
z3
(i)
zT
(i)
i
y1
(i)
y2
(i)
y3
(i)
yT
(i)
i
z1
fi
(i)
π
γ
κ
∞
B0
Figure
5.1.
Graphical model of the IBP-AR-HMM. The beta process distributed measure
B | B0 ∼BP(1, B0) is represented by its masses ωk and locations θk, as in Eq. (2.238).
The fea-
tures are then conditionally independent draws fik | ωk ∼Bernoulli(ωk), and are used to deﬁne feature-
constrained transition distributions π(i)
j
| fi, γ, κ ∼Dir([γ, . . . , γ, γ + κ, γ, . . . ] ⊗fi).
The switching
VAR dynamics are as in Eq. (5.1).
not deﬁned for inﬁnitely many parameters. In reality, we are simply examining a Ki-
dimensional Dirichlet distribution as in Eq. (5.6). However, the notation of Eq. (5.7)
is useful in reminding the reader that the indices of ˜π(i)
j
deﬁned by Eq. (5.6) are not
over 1 to Ki, but rather over the Ki values of k such that fik = 1. Additionally, this
notation is useful for concise representations of the posterior distribution.
We refer to the model described in this section as the IBP autoregressive HMM
(IBP-AR-HMM), with a graphical model representation presented in Fig. 5.1.
■5.2 MCMC Methods for Posterior Inference
In this section, we develop an MCMC method which alternates between resampling
binary feature assignments given observations and dynamic parameters, and resam-
pling dynamic parameters given observations and features.
The sampler interleaves
Metropolis-Hastings and Gibbs sampling updates, which are sometimes simpliﬁed by
appropriate auxiliary variables. We leverage the fact that ﬁxed feature assignments
instantiate a set of ﬁnite AR-HMMs, for which dynamic programming can be used
to eﬃciently compute marginal likelihoods. To resample the potentially inﬁnite set of
object-speciﬁc features, we introduce a new approach employing incremental “birth”
and “death” proposals, improving on previous exact samplers for IBP models in the
non-conjugate case [117].

Sec. 5.2.
MCMC Methods for Posterior Inference
187
■5.2.1 Sampling binary feature assignments
Let F −ik denote the set of all binary feature indicators excluding fik, and K−i
+ be the
number of behaviors used by all of the other objects1. For notational simplicity, we
assume that these behaviors are indexed by {1, . . . , K−i
+ }. The IBP prior diﬀerenti-
ates between features, or behaviors, that other objects have already selected and those
unique to the current object. Thus, we examine each of these cases separately. See
Ex. 5.2.1 for an example illustration of the steps below.
Shared features
Given the ith object’s observation sequence y(i)
1:Ti, transition variables η(i) = η(i)
1:K−i
+ ,1:K−i
+ ,
and shared dynamic parameters θ1:K−i
+ , the feature indicators fik for currently used
features k ∈{1, . . . , K−i
+ } have the following posterior distribution:
p(fik | F −ik, y(i)
1:Ti, η(i), θ1:K−i
+ , α) ∝p(fik | F −ik, α)p(y(i)
1:Ti | fi, η(i), θ1:K−i
+ )
(5.8)
Here, the IBP prior described in Sec. 2.9.4 implies that p(fik = 1 | F −ik, α) = m−i
k /N,
where m−i
k
denotes the number of objects other than object i that exhibit behavior k.
In evaluating this expression, we have exploited the exchangeability of the IBP [62],
which follows directly from the beta process construction [165].
For binary random variables, Metropolis-Hastings proposals can mix faster [45] and
have greater statistical eﬃciency [108] than standard Gibbs samplers. To update fik
given F −ik, we thus use the posterior of Eq. (5.8) to evaluate a Metropolis-Hastings
proposal which ﬂips fik to the complement ¯f of its current value f:
fik ∼ρ( ¯f | f)δ(fik, ¯f) + (1 −ρ( ¯f | f))δ(fik, f)
ρ( ¯f | f) = min
(p(fik = ¯f | F −ik, y(i)
1:Ti, η(i), θ1:K−i
+ , α)
p(fik = f | F −ik, y(i)
1:Ti, η(i), θ1:K−i
+ , α)
, 1
)
.
(5.9)
To compute observation likelihoods, we combine fi and η(i) to construct feature-
constrained transition distributions π(i)
j
as in Eq. (5.5), and apply a variant of the
sum-product message passing algorithm of Sec. 2.6.1 for AR-HMMs that accounts for
the direct correlations in the observations determined by the autoregressive process.
Unique features
An alternative approach is needed to resample the Poisson(α/N) “unique” features as-
sociated only with object i. Let K+ = K−i
+ + ni, where ni is the number of unique
features chosen, and deﬁne f−i = fi,1:K−i
+ and f+i = fi,K−i
+ +1:K+. The posterior distri-
bution over ni is then given by
1Some of the K−i
+
features may also be used by object i, but only those not unique to that object.
See Example 5.2.1.

188
CHAPTER 5.
SHARING FEATURES AMONG DYNAMICAL SYSTEMS WITH BETA PROCESSES
p(ni | fi, y(i)
1:Ti, η(i), θ1:K−i
+ , α) ∝( α
N )nie−α
N
ni!
ZZ
p(y(i)
1:Ti | f−i, f+i = 1, η(i), η+, θ1:K−i
+ , θ+) dB0(θ+)dH(η+),
(5.10)
where H is the gamma prior on transition variables η(i)
jk , and we recall that B0 is the
base measure of the beta process. The set θ+ = θK−i
+ +1:K+ consists of the parameters of
unique features, and η+ the transition parameters η(i)
jk to or from unique features j, k ∈
{K−i
+ + 1 : K+}. Exact evaluation of this integral is intractable due to dependencies
induced by the AR-HMMs.
One early approach to approximate Gibbs sampling in non-conjugate IBP models
relies on a ﬁnite truncation of the limiting Bernoulli process interpretation of the Poisson
distribution [59].
That is, drawing ni ∼Poisson(α/N) distribution is equivalent to
setting ni equal to the number of successes in inﬁnitely many Bernoulli trials, each with
probability of success
lim
K→∞
α/K
α/K + N .
(5.11)
G¨or¨ur et al. [59] truncate this process and instead considers K∗Bernoulli trials with
probability (α/K∗)/(α/K∗+ N).
Meeds et al. [117] instead consider independent
Metropolis proposals which replace the existing unique features by ni ∼Poisson(α/N)
new features, with corresponding parameters θ+ drawn from the prior.
For high-
dimensional models like that considered in this chapter, however, such moves have
extremely low acceptance rates.
We instead develop a birth and death reversible jump MCMC sampler [60], which
proposes to either add a single new feature, or eliminate one of the existing features in
f+i. Our proposal distribution factors as follows:
q(f ′
+i, θ′
+, η′
+ | f+i, θ+, η+) = qf(f ′
+i | f+i)qθ(θ′
+ | f ′
+i, f+i, θ+)qη(η′
+ | f ′
+i, f+i, η+)
(5.12)
Let ni = P
kf+ik.
The feature proposal qf(· | ·) encodes the probabilities of birth
and death moves: A new feature is created with probability 0.5, and each of the ni
existing features is deleted with probability 0.5/ni. This set of possible proposals leads
to considering transitions from ni to n′
i unique features, with n′
i = ni + 1 in the case of
a birth proposal, or n′
i = ni −1 in the case of a proposed feature death. Note that if
the proposal from the distribution deﬁned in Eq. (5.12) is rejected, we maintain n′
i = ni
unique features. For parameters, we deﬁne our proposal using the generative model:
qθ(θ′
+ | f′
+i, f+i, θ+) =
(
b0(θ′
+,ni+1) Qni
k=1 δθ+,k(θ′
+,k),
birth of feature ni + 1;
Q
k̸=ℓδθ+,k(θ′
+,k),
death of feature ℓ.
(5.13)

Sec. 5.2.
MCMC Methods for Posterior Inference
189
That is, for a birth proposal, a new parameter θ′
+,ni+1 is drawn from the prior and
all other parameters remain the same. For a death proposal of feature j, we simply
eliminate that parameter from the model. Here, b0 is the density associated with α−1B0.
The distribution qη(· | ·) is deﬁned similarly, but using the gamma prior on transition
variables of Eq. (5.4).
The Metropolis-Hastings acceptance probability is then given by
ρ(f ′
+i, θ′
+, η′
+ | f+i, θ+, η+) = min{r(f ′
+i, θ′
+, η′
+ | f+i, θ+, η+), 1},
(5.14)
with the acceptance ratio r(· | ·) derived as follows. Let us ﬁrst consider a birth move in
which we propose a transition from ni to ni+1 unique features for object i. As dictated
by Eq. (5.13), the ﬁrst ni proposed components of θ′
+ and η′
+ are equal to the previous
parameters associated with those ni features. Namely, θ′
+,k = θ+,k and η′
+,k = η+,k for
all k ∈{1, . . . , ni}. The diﬀerence between the proposed and previous parameters arises
from the fact that θ′
+ and η′
+ contain an additional component θ′
+,ni+1 and η′
+,ni+1,
respectively, drawn from the prior distributions on these parameter spaces. Then,
r(f ′
+i, θ′
+, η′
+ | f+i, θ+, η+)
=
p(f′
+i, θ′
+, η′
+ | f−i, y(i)
1:Ti, θ1:K−i
+ , η(i))
p(f+i, θ+, η+ | f−i, y(i)
1:Ti, θ1:K−i
+ , η(i))
· q(f+i, θ+, η+ | f ′
+i, θ′
+, η′
+)
q(f ′
+i, θ′
+, η′
+ | f+i, θ+, η+)
(5.15)
=
p(y(i)
1:Ti | [f−i f′
+i], θ1:K−i
+ , θ′
+, η(i), η′
+)p(f ′
+i)p(θ′
+)p(η′
+)
p(y(i)
1:Ti | [f−i f+i], θ1:K−i
+ , θ+, η(i), η+)p(f+i)p(θ+)p(η+)
· qf(f+i | f ′
+i)qθ(θ+ | f+i, f ′
+i, θ′
+)qη(η+ | f+i, f ′
+i, η′
+)
qf(f ′
+i | f+i)qθ(θ′
+ | f ′
+i, f+i, θ+)qη(η′
+ | f ′
+i, f+i, η+)
(5.16)
Noting that each component of the parameter vector θ+ and η+ is drawn i.i.d., and
plugging in the appropriate deﬁnitions for the proposal distributions, we have
r(f ′
+i, θ′
+, η′
+ | f+i, θ+, η+)
=
p(y(i)
1:Ti | [f−i f ′
+i], θ1:K−i
+ , θ′
+, η(i), η′
+)Poisson(ni + 1; α/N) Qni+1
k=1 p(θ′
+,k)p(η′
+,k)
p(y(i)
1:Ti | [f−i f+i], θ1:K−i
+ , θ+, η(i), η+)Poisson(ni; α/N) Qni
k=1 p(θ+,k)p(η+,k)
·
qf(ni ←ni + 1) Qni
k=1 δθ′
+,k(θ+,k)δη′
+,k(η+,k)
qf(ni + 1 ←ni)p(θ′
+,ni+1)p(η′
+,ni+1) Qni
k=1 δθ+,k(θ′
+,k)δη+,k(η′
+,k).
(5.17)
We use the notation qf(k ←j) to denote the proposal probability of transitioning from
j to k unique features. Using the fact that θ′
+,k = θ+,k ∈θ1:K+ and η′
+,k = η+,k ∈η(i)

190
CHAPTER 5.
SHARING FEATURES AMONG DYNAMICAL SYSTEMS WITH BETA PROCESSES
for all k ∈{1, . . . , ni}, we can simplify the acceptance ratio to:
p(y(i)
1:Ti | [f−i f ′
+i], θ1:K+, θ′
+,ni+1, η(i), η′
+,ni+1)Poisson(ni + 1; α/N)qf(ni ←ni + 1)
p(y(i)
1:Ti | [f−i f+i], θ1:K+, η(i))Poisson(ni; α/N)qf(ni + 1 ←ni)
.
(5.18)
The derivation of the acceptance ratio for a death move follows similarly. We compactly
represent the acceptance ratio for either a birth or death move as
p(y(i)
1:Ti | [f−i f ′
+i], θ1:K+, θ′
+, η(i), η′
+) Poisson(n′
i | α/N) qf(f+i | f′
+i)
p(y(i)
1:Ti | [f−i f+i], θ1:K+, η(i)) Poisson(ni | α/N) qf(f ′
+i | f+i)
,
(5.19)
where we recall that n′
i = P
kf ′
+ik. Because our birth and death proposals do not modify
the values of existing parameters, the Jacobian term normally arising in reversible jump
MCMC algorithms simply equals one.
Example 5.2.1. Assume we have a set of four objects, and that we have ﬁnished
resampling the features associated with objects 1 and 2.
We also have the previous
sampled feature vectors for objects 3 and 4.
Let us consider the case in which this
feature matrix is given by:
F =


1
1
0
1
|
0
0
0
1
0
1
1
|
0
0
0
1
1
1
0
|
1
1
1
0
1
0
1
|
0
0
0


Then, when we resample the features for object 3 (i.e., the third row of the feature
matrix), we have K−i
+
= 4 and K+ = 7.
The separation between the set of shared
and unique features for object 3 is indicated by the dashed vertical line. For the shared
features k = 1, 2, 3, 4, we resample f3k according to the Metropolis-Hastings proposal
given Eq. (5.9). For simplicity, let us assume that these features remain as they were in
the previous MCMC iteration. That is, the shared feature vector is f−i =
h
1
1
1
0
i
.
After resampling the shared features, we consider the features unique to object 3. In
this case, we have f+i =
h
1
1
1
i
as the unique feature vector. The feature portion
of our proposal distribution, qf(· | ·), deﬁnes the probability of the following possible
moves:
f+i =
h
1
1
1
i
→f ′
+i =















h
1
1
1
1
i
,
birth of feature 8;
h
1
1
0
i
,
death of feature 7;
h
1
0
1
i
,
death of feature 6;
h
0
1
1
i
,
death of feature 5.

Sec. 5.2.
MCMC Methods for Posterior Inference
191
The birth move is proposed with probability 0.5 while each of the death moves has prob-
ability 0.5/3. The associated parameter moves are given by:
θ+ = {θ+,1, θ+,2, θ+,3} →θ′
+ =









{θ+,1, θ+,2, θ+,3, θ′
+,4},
birth of feature 8;
{θ+,1, θ+,2},
death of feature 7;
{θ+,1, θ+,3},
death of feature 6;
{θ+,2, θ+,3},
death of feature 5,
with θ′
+,4 a draw from the MNIW prior. We also recall that θ+ = {θ+,1, θ+,2, θ+,3} =
{θ5, θ6, θ7}, and in a birth move we set θ8 = θ′
+,4. The transition parameter moves are
similarly deﬁned.
■5.2.2 Sampling dynamic parameters and transition variables
Posterior updates to transition variables η(i) and shared dynamic parameters θk are
greatly simpliﬁed if we instantiate the mode sequences z(i)
1:Ti for each object i. We treat
these mode sequences as auxiliary variables. Namely, these variables are sampled given
the current MCMC conﬁguration. We then resample the model parameters conditioned
on the sampled auxiliary variables, after which the auxiliary variables are discarded for
subsequent updates of feature assignments fi.
Mode sequences z(i)
1:Ti
Given feature-constrained transition distributions π(i) and dynamic parameters {θk},
along with the observation sequence y(i)
1:Ti, we block sample the mode sequence z(i)
1:Ti by
computing backward messages mt+1,t(z(i)
t ) ∝p(y(i)
t+1:Ti | z(i)
t , ˜y(i)
t , π(i), {θk}), and then
recursively sampling each z(i)
t :
z(i)
t
| z(i)
t−1, y(i)
1:Ti, π(i), {θk} ∼π(i)
z(i)
t−1
(z(i)
t )N
 y(i)
t ; Az(i)
t ˜y(i)
t , Σz(i)
t

mt+1,t(z(i)
t ).
(5.20)
This backward message-passing, forward-sampling scheme is identical to that derived
for the HDP-AR-HMM in Sec. 4.1.2, but utilizing the parameters and observations
speciﬁc to object i.
Transition distributions π(i)
j
We use the fact that Dirichlet priors are conjugate to multinomial observations z(i)
1:T
(see Sec. 2.4.2) to derive that the posterior of π(i)
j
is
π(i)
j
| fi, z(i)
1:T , γ, κ ∼Dir([γ + n(i)
j1 , . . . , γ + n(i)
jj−1, γ + κ + n(i)
jj , γ + n(i)
jj+1, . . . ] ⊗fi).
(5.21)

192
CHAPTER 5.
SHARING FEATURES AMONG DYNAMICAL SYSTEMS WITH BETA PROCESSES
Here, n(i)
jk are the number of transitions from mode j to k in z(i)
1:T . Since the mode
sequence z(i)
1:T was generated from feature-constrained transition distributions, n(i)
jk will
be zero for any k such that fik = 0. Using the deﬁnition of π(i)
j
in Eq. (5.5), one can
equivalently deﬁne a sample from the posterior of Eq. (5.21) by solely updating η(i)
jk for
instantiated features:
η(i)
jk | z(i)
1:T , γ, κ ∼Gamma(γ + κδ(j, k) + n(i)
jk, 1),
k ∈{ℓ| fiℓ= 1}.
(5.22)
Dynamic parameters {Ak, Σk}
We now turn to posterior updates for dynamic parameters. As with the HDP-AR-HMM
of Chapter 4, we place a conjugate matrix normal inverse-Wishart (MNIW) prior on
{Ak, Σk}, comprised of an inverse-Wishart prior IW(n0, S0) on Σk and a matrix-normal
prior MN
 Ak; M, Σ−1
k , K

on Ak given Σk. We consider the following suﬃcient statis-
tics based on the sets Y k = {y(i)
t
| z(i)
t
= k} and ˜Y k = {˜y(i)
t
| z(i)
t
= k} of observations
and lagged observations, respectively, associated with behavior k:
S(k)
˜y˜y =
X
(t,i)|z(i)
t
=k
˜y(i)
t ˜y(i)T
t
+ K
S(k)
y˜y =
X
(t,i)|z(i)
t
=k
y(i)
t ˜y(i)T
t
+ MK
S(k)
yy =
X
(t,i)|z(i)
t
=k
y(i)
t y(i)T
t
+ MKM T
S(k)
y|˜y = S(k)
yy −S(k)
y˜y S−(k)
˜y˜y
S(k)T
˜y˜y
.
(5.23)
It is through this pooling of observations across objects that we achieve enhanced learn-
ing of shared behaviors, especially in the presence of limited data. Analogous to the
derivation outlined in Sec. 4.1.1, the posterior can then be shown to equal
Ak | Σk, Y k ∼MN

Ak; S(k)
y˜y S−(k)
˜y˜y
, Σ−1
k , S(k)
˜y˜y

Σk | Y k ∼IW

|Y k| + n0, S(k)
y|˜y + S0

.
(5.24)
■5.2.3 Sampling the IBP and Dirichlet transition hyperparameters
We additionally place priors on the Dirichlet hyperparameters γ and κ, as well as the
IBP parameter α.
IBP hyperparameter α
Let F = {f i}. As derived in [62], p(F | α) can be expressed as
p(F | α) ∝αK+ exp

−α
N
X
n=1
1
n

,
(5.25)

Sec. 5.2.
MCMC Methods for Posterior Inference
193
where, as before, K+ is the number of unique features activated in F . As in [59], we
place a conjugate Gamma(aα, bα) prior on α, which leads to the following posterior
distribution:
p(α | F , aα, bα) ∝p(F | α)p(α | aα, bα)
∝αK+ exp
 
−α
N
X
n=1
1
n
!
αaα−1 exp(−bαα)
Γ(α)
∝αaα+K+−1 exp
 
−α
 
bα +
N
X
n=1
1
n
!!
= Gamma

aα + K+, bα +
N
X
n=1
1
n

(5.26)
Transition hyperparameters γ and κ
Transition hyperparameters are assigned similar priors γ ∼Gamma(aγ, bγ) and κ ∼
Gamma(aκ, bκ). Because the generative process of Eq. (5.4) is non-conjugate, we rely
on Metropolis-Hastings steps which iteratively resample γ given κ, and κ given γ. Each
sub-step uses a gamma proposal distribution qγ(· | ·) or qκ(· | ·), respectively, with
ﬁxed variance σ2
γ or σ2
κ, and mean equal to the current hyperparameter value. Since a
Gamma(a, b) distribution has mean a/b and variance a/b2, these proposal distribution
settings are accomplished by taking
qγ(· | γ) = Gamma
γ2
σ2γ
, γ
σ2γ

qκ(· | κ) = Gamma
κ2
σ2κ
, κ
σ2κ

.
(5.27)
To update γ given κ, the acceptance probability is min{r(γ′ | γ), 1}, where the
acceptance ratio is given by:
r(γ′ | γ) = p(γ′ | κ, π, F )q(γ | γ′)
p(γ | κ, π, F )q(γ′ | γ) = p(π | γ′, κ, F )p(γ′)q(γ | γ′)
p(π | γ, κ, F )p(γ)q(γ′ | γ) ,
(5.28)
where we omit the hyperparameters aγ, bγ, and σ2
γ for simplicity of notation. Recalling
the deﬁnition of ˜π(i)
j
from Eq. (5.6) and that Ki = P
k fik, the likelihood term may be
written as
p(π | γ, κ, F ) =
Y
i
Ki
Y
k=1
p(˜π(i)
k
| γ, κ, fi)
(5.29)
=
Y
i
Ki
Y
k=1



Γ(γKi + κ)
QKi−1
j=1 Γ(γ)

Γ(γ + κ)
Ki
Y
j=1
˜π(i)γ+κδ(k,j)−1
kj


.
(5.30)

194
CHAPTER 5.
SHARING FEATURES AMONG DYNAMICAL SYSTEMS WITH BETA PROCESSES
The ratio of the prior distributions reduces to
p(γ′ | aα, bα)
p(γ | aα, bα) = γ′aγ−1 exp{−γ′bγ}
γaγ−1 exp{−γbγ} =
γ′
γ
aγ−1
exp{−(γ′ −γ)bγ}.
(5.31)
Letting ϑ = γ2/σ2
γ and ϑ′ = γ′2/σ2
γ, the ratio of the proposal distributions reduces to
q(γ | γ′, σ2
γ)
q(γ′ | γ, σ2γ) =
(γ′/σ2
γ)ϑ′
Γ(ϑ′)
γϑ′−1 exp{−γ γ′
σ2γ }
(γ/σ2γ)ϑ
Γ(ϑ) γ′ϑ−1 exp{−γ′ γ
σ2γ }
= Γ(ϑ)γϑ′−ϑ−1
Γ(ϑ′)γ′ϑ−ϑ′−1 σ2(ϑ−ϑ′)
γ
.
(5.32)
Deﬁning f(γ) ≜p(π | γ, κ, F ), our acceptance ratio can be compactly written as
r(γ′ | γ) = f(γ′)Γ(ϑ)γϑ′−ϑ−aγ
f(γ)Γ(ϑ′)γ′ϑ−ϑ′−aγ exp{−(γ′ −γ)bγ}σ2(ϑ−ϑ′)
γ
.
(5.33)
The Metropolis-Hastings sub-step for resampling κ given γ follows similarly. In this
case, however, the likelihood terms simpliﬁes to
f(κ) ≜
Y
i
Γ(γKi + κ)Ki
Γ(γ + κ)Ki
Ki
Y
j=1
˜π(i)γ+κ−1
jj
∝p(π | γ, κ, F ).
(5.34)
The resulting MCMC sampler for the IBP-AR-HMM is summarized in Algorithm 17.2
■5.3 Synthetic Experiments
To test the ability of the IBP-AR-HMM to discover shared dynamics, we generated ﬁve
time series that switched between AR(1) models
y(i)
t
= az(i)
t y(i)
t−1 + e(i)
t (z(i)
t )
(5.35)
with ak ∈{−0.8, −0.6, −0.4, −0.2, 0, 0.2, 0.4, 0.6, 0.8} and process noise covariance Σk
drawn from an IW(3, 0.5) prior.
The object-speciﬁc features, shown in Fig. 5.2(b),
were sampled from a truncated IBP [62] using α = 10 and then used to generate the
observation sequences of Fig. 5.2(a) (colored by the true mode sequences). Each row of
the feature matrix corresponds to one of the ﬁve time series, and the columns represent
the diﬀerent autoregressive models with a white square indicating that a given time
series uses that dynamical regime. Here, the columns are ordered so that the ﬁrst feature
corresponds to an autoregressive model deﬁned by a1, and the ninth feature corresponds
to that of a9. The resulting feature matrix estimated over 10,000 MCMC samples is
shown in Fig. 5.2(c). Each of the 10,000 estimated feature matrices is produced from
an MCMC sample of the mode sequences that are ﬁrst mapped to the ground truth
2Note that Algorithm 18 is embedded within Algorithm 17

Given a previous set of object-speciﬁc transition variables {η(i)}(n−1), the dynamic
parameters {Ak, Σk}(n−1), and features F (n−1):
1. Set {η(i)} = {η(i)}(n−1), {Ak, Σk} = {Ak, Σk}(n−1), and F = F (n−1).
2. From the feature matrix F , create count vector m = [ m1
m2
. . .
mK+ ],
with mk representing the number of objects possessing feature k.
3. For each i ∈{1, . . . , N}, sample features as follows:
(a) Set m−i = m −fi, and reorder columns of F so that the K−i
+ columns with
m−i
k > 0 appear ﬁrst. Appropriately relabel indices of {Ak, Σk} and {η(i)}.
(b) For each shared feature k ∈{1, . . . , K−i
+ }, set f = fik and:
i. Consider fik ∈{0, 1} and:
A. Create feature-constrained transition distributions:
π(i)
j
∝[ η(i)
j1
η(i)
j2
. . .
η(i)
jK+
] ⊗fi
B. Compute likelihood ℓfik

y(i)
1:Ti

≜p

y(i)
1:Ti | π(i), {Ak, Σk}

using a
variant of the sum-product algorithm described in Sec. 2.6.1.
ii. Compute
ρ∗=
m−i
k
N −m−i
k
·
ℓ1

y(i)
1:Ti

ℓ0

y(i)
1:Ti
 and set ρ( ¯f | f) =
(
min{ρ∗, 1},
f = 0;
min{1/ρ∗, 1},
f = 1.
iii. Sample fik ∼ρ( ¯f | f)δ(fik, ¯f) + (1 −ρ( ¯f | f))δ(fik, f).
(c) Let fi’ = fi and calculate the number of unique features ni = K+ −K−i
+ .
i. Propose a birth or death move, each with probability 0.5.
• Birth: sample {θ′
+,ni+1, η+,ni+1} from their priors and set f ′
i,ni+1 = 1,
n′
i = ni + 1.
• Death: sample ℓ∼uniform[K−i
+ + 1 : K+] and set f ′
iℓ= 0, n′
i = ni −1.
ii. Compute likelihoods ℓfi

y(i)
1:Ti

and ℓf′
i

y(i)
1:Ti

of data under the
previous and proposed models, respectively.
iii. Keep (ζ = 1) or discard (ζ = 0) proposed model by sampling:
ζ ∼Ber(ρ)
ρ = min



ℓfi

y(i)
1:Ti

Poisson(n′
i | α
N )qf(ni ←n′
i)
ℓf′
i

y(i)
1:Ti

Poisson(ni | α
N )qf(n′
i ←ni)
, 1


.
(d) Set m = m−i + fi. Remove columns for which mk = 0, and appropriately
redeﬁne the dynamic parameters {Ak, Σk} and transition variables {η(i)}.
4. Resample dynamic parameters {Ak, Σk} and transition variables {η(i)} using the
auxiliary variable sampler of Algorithm 18.
5. Fix {η(i)}(n) = {η(i)}, {Ak, Σk}(n) = {Ak, Σk}, and F (n) = F .
Algorithm 17. IBP-AR-HMM MCMC sampler.

Given the feature-restricted transition distributions π(i) and dynamic parameters
{Ak, Σk}, update the parameters as follows:
1. For each i ∈{1, . . . , N}:
(a) Block sample z(i)
1:Ti as follows:
i. For each k ∈{1, . . . , K+}, initialize messages to m(i)
T+1,T (k) = 1.
ii. For each t ∈{Ti, . . . , 1} and k ∈{1, . . . , K+}, compute
m(i)
t,t−1(k) =
K
X
j=1
π(i)
k (j)N

y(i)
t ; Aj˜y(i)
t , Σj

m(i)
t+1,t(j).
iii. Working sequentially forward in time, and starting with transitions
counts n(i)
jk = 0:
A. Sample a mode assignment z(i)
t
as:
z(i)
t
∼
K+
X
k=1
π(i)
z(i)
t−1
(k)N

y(i)
t ; Ak˜y(i)
t , Σk

m(i)
t+1,t(k)δ

z(i)
t , k

.
B. Increment n(i)
z(i)
t−1z(i)
t
.
Note that π(i)
j (k) is zero for any k such that fik = 0, implying that z(i)
t
= k
will never be sampled (as desired). Considering all K+ indices simply allows
for eﬃcient matrix implementation.
(b) For each (j, k) ∈{1, . . . , K+} × {1, . . . , K+}, sample
η(i)
jk | γ ∼Gamma(1, γ + κδ(j, k) + n(i)
jk).
2. For each k ∈{1, . . . , K+}:
(a) Form Y k = {y(i)
t |z(i)
t
= k} and ˜Y k = {˜y(i)
t |z(i)
t
= k} and compute S(k)
˜y˜y , S(k)
y˜y ,
S(k)
yy , and S(k)
y|˜y as in Eq. (5.23).
(b) Sample dynamic parameters:
Σk ∼IW
 N
X
i=1
n(i)
k· + n0, S(k)
y|˜y + S0
!
Ak | Σk ∼MN

Ak; S(k)
y˜y S−(k)
˜y˜y
, Σ−1
k , S(k)
˜y˜y

.
Algorithm 18.
IBP-AR-HMM auxiliary variable sampler for updating transition and dynamic pa-
rameters.

Sec. 5.3.
Synthetic Experiments
197
0
200
400
600
800
1000
−5
0
5
10
15
20
25
Time
Observations
Object 1
Object 2
Object 3
Object 4
Object 5
(a)
2
4
6
8
10
12
14
16
18
20
1
2
3
4
5
2
4
6
8
10
12
14
16
18
20
1
2
3
4
5
(b)
(c)
Figure 5.2. (a) Observation sequences for each of 5 switching AR(1) time series colored by true mode
sequence, and oﬀset for clarity. Images of the (b) true feature matrix of the ﬁve objects and (c) estimated
feature matrix averaged over 10,000 MCMC samples taken from 100 trials every 10th sample. Each row
corresponds to a diﬀerent object, and each column a diﬀerent autoregressive model. White indicates
active features. Although the true model is deﬁned by only 9 possible dynamical modes, we show 20
columns in order to display the “tail” of the IBP-AR-HMM estimated matrix resulting from samples
that incorporated additional dynamical modes (events that have positive probability of occurring, as
deﬁned by the IBP prior.) The estimated feature matrices are produced from mode sequences mapped
to the ground truth labels according to the minimum Hamming distance metric, and selecting modes
with more than 2% of the object’s observations.
labels according to the minimum Hamming distance metric. We then only maintain
inferred modes with more than 2% of the object’s observations. Comparing to the true
feature matrix, we see that our model is indeed able to discover most of the underlying
latent structure of the time series despite the challenging setting deﬁned by the close
autoregressive coeﬃcients. The most commonly missed feature is the use of a4 by the
ﬁfth time series. This ﬁfth time series is the top-most displayed in Fig. 5.2(a), and the
dynamical mode deﬁned by a4 is shown in green. We see that this mode is used very
infrequently, making it challenging to distinguish. Due to the nonparametric nature
of the model, we also see a “tail” in the estimated matrix because of the (infrequent)
incorporation of additional dynamical modes.

198
CHAPTER 5.
SHARING FEATURES AMONG DYNAMICAL SYSTEMS WITH BETA PROCESSES
One might propose, as an alternative to the IBP-AR-HMM, the use of an architec-
ture based on the hierarchical Dirichlet process of [162]; speciﬁcally we could use the
HDP-AR-HMMs of Chapter 4 tied together with a shared set of transition and dynamic
parameters. To demonstrate the diﬀerence between these models, we generated data for
three switching AR(1) processes. The ﬁrst two objects, with four times the data points
of the third, switched between dynamical modes deﬁned by ak ∈{−0.8, −0.4, 0.8} and
the third object used ak ∈{−0.3, 0.8}.
The results shown in Fig. 5.3 indicate that
the multiple HDP-AR-HMM model, which assumes all objects share exactly the same
transition matrices and dynamic parameters, typically describes the third object using
ak ∈{−0.4, 0.8} since this assignment better matches the parameters deﬁned by the
other (lengthy) time series. This common grouping of two distinct dynamical modes
leads to the large median and 90th Hamming distance quantiles shown in Fig. 5.3(b).
The IBP-AR-HMM, on the other hand, is better able to distinguish these dynamical
modes (see Fig. 5.3(c)) since the penalty in not sharing a behavior is only in the feature
matrix; once a unique feature is chosen, it does not matter how the object chooses to
use it. Example segmentations representative of the median Hamming distance error
are shown in Fig. 5.3(d)-(e). These results illustrate that the IBP-based feature model
emphasizes choosing behaviors rather than assuming all objects are performing minor
variations of the same dynamics.
For the experiments above, we placed a Gamma(1, 1) prior on α and γ, and a
Gamma(100, 1) prior on κ. The gamma proposals used σ2
γ = 1 and σ2
κ = 100 while the
MNIW prior was given M = 0, K = 0.1 ∗Id, n0 = d + 2, and S0 set to 0.75 times
the empirical variance of the joint set of ﬁrst diﬀerence observations. At initialization,
each time series was segmented into ﬁve contiguous blocks, with feature labels unique
to that sequence.
■5.4 Motion Capture Experiments
The linear dynamical system is a common model for describing simple human mo-
tion [69], and the more complicated SLDS has been successfully applied to the problem
of human motion synthesis, classiﬁcation, and visual tracking [132, 133].
Other ap-
proaches develop non-linear dynamical models using Gaussian processes [179] or based
on a collection of binary latent features [159]. However, there has been little eﬀort in
jointly segmenting and identifying common dynamic behaviors amongst a set of multiple
motion capture (MoCap) recordings of people performing various tasks. The IBP-AR-
HMM provides an ideal way of handling this problem. One beneﬁt of the proposed
model, versus the standard SLDS, is that it does not rely on manually specifying the
set of possible behaviors. As an illustrative example, we examined a set of six CMU
MoCap exercise routines [169], three from Subject 13 and three from Subject 14. Each
of these routines used some combination of the following motion categories: running
in place, jumping jacks, arm circles, side twists, knee raises, squats, punching, up and
down, two variants of toe touches, arch over, and a reach out stretch.

0
500
1000
1500
2000
−20
−10
0
10
20
30
40
Time
Observations
Object 1
Object 2
Object 3
(a)
200
400
600
800
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
200
400
600
800
0
0.1
0.2
0.3
0.4
0.5
0.6
Iteration
Normalized Hamming Distance
(b)
(c)
Time
0
1000
2000
Time
0
1000
2000
Time
0
250
500
Object: 1
Object: 2
Object: 3
True
Estimated
Time
0
1000
2000
Time
0
1000
2000
Time
0
250
500
Object: 1
Object: 2
Object: 3
True
Estimated
(d)
(e)
Figure 5.3. (a) Observation sequences for each of 3 switching AR(1) time series colored by true mode
sequence, and oﬀset for clarity. The ﬁrst and second sequences are four times as long as the third.
(b)-(c) Focusing solely on the third time series, the median (solid blue) and 10th and 90th quantiles
(dashed red) of Hamming distance between the true and estimated mode sequence over 1000 trials are
displayed for the multiple HDP-AR-HMM model and the IBP-AR-HMM, respectively. (d)-(e) Examples
of typical segmentations into behavior modes for the three objects at MCMC iteration 1000 for the two
models. The top and bottom panels display the estimated and true sequences, respectively, and the
color coding corresponds exactly to that of (a). For example, object 3 switches between two modes
colored by cyan and maroon.

200
CHAPTER 5.
SHARING FEATURES AMONG DYNAMICAL SYSTEMS WITH BETA PROCESSES
−8
−6
−4
−2
0
2
−4−2 0 2 4 6
5
10
15
20
25
30
x
z
y
−15
−10
−5
0
5
10
−5
0
5
5
10
15
20
25
30
x
z
y
−6 −4 −2 0
2
4
−2 0 2 4 6 8
5
10
15
20
25
30
x
z
y
−5
0
5
−5
0
5
5
10
15
20
25
30
x
z
y
−10
−5
0
5
10
−5
0
5
5
10
15
20
25
z
x
y
−10
−5
0
5
−5
0
5
10
15
5
10
15
20
25
z
x
y
−10
−5
0
5
0
2
4
6
0
5
10
15
20
25
x
z
y
−10
−5
0
5
0
2
4
6
0
5
10
15
20
25
x
z
y
−10
−5
0
5
5
10
15
5
10
15
20
25
x
z
y
−10
−5
0
5
−5
0
5
5
10
15
20
25
x
z
y
−10
−5
0
5
−5
0
5
10
5
10
15
20
25
z
x
y
−10
−5
0
5
−5
0
5
10
5
10
15
20
25
30
z
x
y
−10
−5
0
5
−10
−5
0
5
10
15
20
25
z
x
y
−10
−5
0
5
−10
−5
0
5
5
10
15
20
25
30
z
x
y
−15
−10
−5
0
5
−5
0
5
10
5
10
15
20
25
z
x
y
−10
−5
0
5
−5
0
5
10
5
10
15
20
25
z
x
y
−10
−5
0
5
10
−10
−5
0
5
5
10
15
20
25
30
35
z
x
y
−15
−10
−5
0
5
10
−10
−5
0
5
10
15
0
5
10
15
20
25
x
z
y
−15
−10
−5
0
5
10
−2 0 2 4
5
10
15
20
25
30
x
z
y
−15
−10
−5
0
5
10
−4−2 0 2
0
5
10
15
20
25
30
x
z
y
−10
−5
0
5
10
−4−2 0 2
5
10
15
20
25
30
x
z
y
−10
−5
0
5
10
0
5
10
5
10
15
20
25
30
x
z
y
−10
−5
0
5
10
−5
0
5
5
10
15
20
25
30
x
z
y
−10
−5
0
5
10
−2 0 2 4
5
10
15
20
25
30
x
z
y
−10
0
10
−5
0
5
10
5
10
15
20
25
30
x
z
y
−15
−10
−5
0
5
10
0
5
10
5
10
15
20
25
30
x
z
y
−15
−10
−5
0
5
10
0
5
10
5
10
15
20
25
30
x
z
y
−15
−10
−5
0
5
10
0
5
10
5
10
15
20
25
30
x
z
y
−20
−10
0
10
−10
−5
0
5
10
0
5
10
15
20
25
x
z
y
−15
−10
−5
0
5
10
−15
−10
−5
0
5
10
0
5
10
15
20
25
x
z
y
−10
−5
0
5
10
−5
0
5
10
0
5
10
15
20
25
x
z
y
−5
0
5
10
−15
−10
−5
0
5
5
10
15
20
25
x
z
y
−10
−5
0
5
−5
0
5
5
10
15
20
25
30
z
x
y
−5
0
5
10
−10
−5
0
5
5
10
15
20
25
x
z
y
−15
−10
−5
0
5
10
15
−5
0
5
10
15
20
5
10
15
20
25
30
z
x
y
Figure 5.4. Each skeleton plot displays the trajectory of a learned contiguous segment of more than
2 seconds. To reduce the number of plots, we preprocessed the data to bridge segments separated by
fewer than 300 msec. The boxes group segments categorized under the same behavior label, with the
color indicating the true behavior label (allowing for analysis of split behaviors). Skeleton rendering
done by modiﬁcations to Neil Lawrence’s Matlab MoCap toolbox [105].
From the set of 62 position and joint angles, we selected the following set of 12
measurements deemed most informative for the gross motor behaviors we wish to cap-
ture: one body torso position, two waist angles, one neck angle, one set of right and
left shoulder angles, the right and left elbow angles, one set of right and left hip angles,
and one set of right and left ankle angles. As with the speaker diarization application
of Sec. 3.5, we block average and downsample the data. The CMU MoCap data is
recorded at a rate of at 120 frames per second, and we use a window size of 12 in our
preprocessing. We additionally scale each component of the observation vector so that
the empirical variance on the concatenated set of ﬁrst diﬀerence measurements is 1.
Using these measurements, the prior distributions were set exactly as in the synthetic
data experiments except the scale matrix, S0, of the MNIW prior which was set to
5 · I12 (i.e., ﬁve times the empirical covariance of the preprocessed ﬁrst diﬀerence ob-
servations, and maintaining only the diagonal.) This setting allows more variability in
the observed behaviors. We ran 25 chains of the sampler for 20,000 iterations and then
examined the chain whose segmentation minimized the expected Hamming distance to
the set of segmentations from all chains over iterations 15,000 to 20,000. This method
of choosing an MCMC sample is described in more detail in Sec. 3.5.
The resulting MCMC sample is displayed in Fig. 5.4. Each skeleton plot depicts the

Sec. 5.4.
Motion Capture Experiments
201
5
10
15
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Number of Clusters/States
Normalized Hamming Distance
 
 
GMM
GMM 1st diff
HMM
HMM 1st diff
IBP−AR−HMM
Figure 5.5. Hamming distance versus number of GMM clusters / HMM states on raw observations
(blue/green) and ﬁrst-diﬀerence observations (red/cyan), with the IBP-AR-HMM segmentation (black,
horizontal dashed) and true feature count (magenta, vertical dashed) shown for comparison. Results
are for the most-likely of 10 EM initializations using Kevin Murphy’s HMM Matlab toolbox [123].
trajectory of a learned contiguous segment of more than two seconds, and boxes group
segments categorized under the same behavior label by our algorithm. The color of the
box indicates the true behavior label. From this plot we can infer that although some
true behaviors are split into two or more categories by our algorithm, the IBP-AR-
HMM shows a clear ability to ﬁnd common motions. Speciﬁcally, the IBP-AR-HMM
has successfully identiﬁed and grouped examples of jumping jacks (magenta), side twists
(bright blue), arm circles (dark purple), squats (orange), and various motion behaviors
that appeared in only one movie (bottom left four skeleton plots.) The split behaviors
shown in green and yellow correspond to the true motion categories of knee raises and
running, respectively, and the splits can be attributed to the two subjects performing
the same motion in a distinct manner. For the knee raises, one subject performed the
exercise while slightly twisting the upper in a counter-motion to the raised knee (top
three examples) while the other subject had signiﬁcant side-to-side upper body motion
(middle three examples). For the running motion category, the splits also tended to
correspond to varying upper body motion such as running with hands in or out of sync
with knees. One example (bottom right) was the subject performing a lower-body run
partially mixed with an upper-body jumping jack/arm ﬂapping motion (an obviously
confused test subject.) See Sec. 5.5 for further discussion of the IBP-AR-HMM splitting
phenomenon.
We compare our MoCap performance to the Gaussian mixture model (GMM) method
of Barbiˇc et al. [7] using expectation maximization (EM) initialized with k-means. Barbiˇc
et al. [7] also present an approach based on probabilistic principle component analy-
sis (PCA), but this method focuses primarily on change-point detection rather than
behavior clustering.
As further comparisons, we look at a GMM on ﬁrst diﬀerence
observations, and an HMM on both data sets. In Fig. 5.5, we analyze the ability of

202
CHAPTER 5.
SHARING FEATURES AMONG DYNAMICAL SYSTEMS WITH BETA PROCESSES
2
4
6
8
10
12
14
16
18
20
1
2
3
4
5
6
2
4
6
8
10
12
14
16
18
20
1
2
3
4
5
6
2
4
6
8
10
12
14
16
18
20
1
2
3
4
5
6
2
4
6
8
10
12
14
16
18
20
1
2
3
4
5
6
Figure 5.6.
Feature matrices associated with the true MoCap sequences (top-left), IBP-AR-HMM
estimated sequences over iterations 15,000 to 20,000 (top-right), and MAP assignment of the GMM
(bottom-left) and HMM (bottom-right) using ﬁrst-diﬀerence observations and 12 clusters/states.
the IBP-AR-HMM, as compared to the deﬁned GMMs and HMMs, in providing ac-
curate labelings of the individual frames of the six movie clips3. Speciﬁcally, we plot
the Hamming distance between the true and estimated frame labels versus the number
of GMM clusters and HMM states, using the most-likely of 10 initializations of EM.
We also plot the Hamming distance corresponding the IBP-AR-HMM MCMC sample
depicted in Fig. 5.4, demonstrating that the IBP-AR-HMM provides more accurate
frame labels than any of these alternative approaches over a wide range of mixture
model settings. The estimated feature matrices for the IBP-AR-HMM and the GMM
and HMM on ﬁrst diﬀerence observations are shown in Fig. 5.6. The ﬁgure displays the
matrix associated with the MAP label estimate in the case of the GMM and HMM, and
an estimate based on MCMC samples from iterations 15,000 to 20,000 for the IBP-AR-
HMM. For the GMM and HMM, we consider the case when the number of Gaussian
mixture components or the number of HMM states is set to the true number of behav-
iors, namely 12. By pooling all of the data, the GMM and HMM approaches assume
that each object exhibits the same structure; the results of this assumption can be seen
in the strong bands of white implying sharing of behavior between the time series. The
IBP-AR-HMM estimated feature matrix, on the other hand, provides a much better
match to the true matrix by allowing for sequence-speciﬁc variability. For example,
3The ability to accurately label the frames of a large set of movies is useful for tasks such as querying
an extensive MoCap database (such as that of CMU) without relying on manual labeling of the movies.

Sec. 5.5.
Discussion and Future Work
203
this ability is indicated by the special structure of features in the upper right portion of
the true feature matrix that is mostly captured in the IBP-AR-HMM estimated feature
matrix, but is not present in those of the GMM or HMM. We do, however, note a few
IBP-AR-HMM merged and split behaviors. Overall, we see that in addition to pro-
ducing more accurate segmentations of the MoCap data, the IBP-AR-HMM provides a
superior ability to discover the shared feature structure.
■5.5 Discussion and Future Work
Utilizing the beta process, we developed a coherent Bayesian nonparametric framework
for discovering dynamical features common to multiple time series. This formulation
allows for object-speciﬁc variability in how the dynamical behaviors are used.
We
additionally developed a novel exact sampling algorithm for non-conjugate IBP models.
The utility of our IBP-AR-HMM was demonstrated both on synthetic data, and on a
set of MoCap sequences where we showed performance exceeding that of alternative
methods. Although we focused on switching VAR processes, our approach could be
equally well applied to HMMs, and to a wide range of other switching dynamical systems
such as the SLDS of Chapter 4.
One area of future work is to develop split-merge proposals to further improve mixing
rates in high-dimensions. Although the block initialization of the time series helps with
the issue of splitting merged behaviors (analogous to the issues with splitting merged
speakers discussed in Chapter 3), it does not fully solve the problem and cannot be
relied upon in datasets with more irregular switching patterns than the MoCap data
we considered. Additionally, splitting a single true behavior into multiple estimated
behaviors often occurred, and is related to the issue of splitting a single true speaker
in Chapter 3. The root of the splitting issue is two-fold. One is due to the mixing
rate of the sampler. The second, unlike in the case of merging behaviors, is due to
modeling issues. Our model assumes that the dynamic behavior parameters (i.e., the
VAR process parameters) are identical between time series and do not change over time.
This assumption can be problematic in grouping related dynamic behaviors, and might
be addressed via hierarchical models of behaviors or by ideas similar to those of the
dependent Dirchlet process [61, 111] that allows for time-varying parameters.
Overall, the MoCap results appeared to be fairly robust to examples of only slightly
dissimilar behaviors (e.g., squatting to diﬀerent levels, twisting at diﬀerent rates, etc.)
However, in cases such as the running motion where only portions of the body moved
in the same way while others did not, we tended to split the behavior group. This ob-
servation motivates examination of local partition processes [37, 38] rather than global
partition processes. That is, our current model assumes that the grouping of obser-
vations into behavior categories occurs along all components of the observation vector
rather than just a portion (e.g., lower body measurements.) Allowing for greater ﬂex-
ibility in the grouping of observation vectors becomes increasingly important in high
dimensions.

204
CHAPTER 5.
SHARING FEATURES AMONG DYNAMICAL SYSTEMS WITH BETA PROCESSES

Chapter 6
Contributions and
Recommendations
W
E begin with a summary of the overriding themes and principal contributions
presented in the preceding chapters. Throughout the course of these chapters,
we developed a ﬂexible Bayesian framework for learning Markov switching processes
to describe a wide variety of datasets; these models provide the foundation for many
other interesting extensions and analyses, which we highlight following our summary of
contributions.
■6.1 Summary of Methods and Contributions
As we have demonstrated in this thesis, many complex dynamical phenomena, such as
human motion, the dance of honey bees, trends in stock indices, and conference audio
can be modeled via Markov switching processes. Due to uncertainty in the underlying
dynamics of these phenomena, it is often challenging to determine how many modes
should be used to describe the observed behaviors. By taking a Bayesian nonparametric
approach, we are able to make fewer assumptions about the underlying dynamics than
are required by a parametric approach, allowing the data to drive the complexity of the
inferred model.
The most basic of the Markov switching processes we considered in this thesis is
the hidden Markov model (HMM), which assumes that the data can be modeled as
independent given an underlying discrete-valued Markov sequence. In Chapter 3, we
examine a Bayesian nonparametric approach to learning HMMs with an unknown num-
ber of modes1, and demonstrate that the current state of the art—the HDP-HMM—
inadequately captures the temporal mode persistence present in our datasets of in-
terest. We then show how one can augment the model with a learned bias towards
self-transitions, resulting in what we refer to as the sticky HDP-HMM. One of the main
contributions of this chapter is the formulation of this augmented model in a manner
that integrates fully with Bayesian nonparametric inference. As a motivating example,
1Although Chapter 3 uses the terminology state, we switch to the mode terminology here to avoid
confusion with the latent continuous-valued state sequence introduced in Chapter 4.
205

206
CHAPTER 6.
CONTRIBUTIONS AND RECOMMENDATIONS
we consider the problem of speaker diarization in which the goal is to segment conference
audio into a set of speaker labels in the presence of an unknown number of speakers. For
this application, it is extremely unlikely that there are rapid transitions from speaker to
speaker, and we show that capturing this temporal mode persistence is key in obtaining
state-of-the-art speaker diarizations. Another key aspect of this application is the fact
that the speaker-speciﬁc emissions are very complex and multimodal. This motivates
our examination of a sticky HDP-HMM with Dirichlet process emissions; we show that
the sticky parameter is essential in being able to identify such a model. In Chapter 3,
we additionally present a blocked Gibbs sampler, relying on a truncated approximation
to the sticky HDP-HMM, that leads to dramatic improvements in mixing rates over the
previously proposed collapsed sequential Gibbs sampler.
Motivated by applications such as the dance of a honey bee, in Chapter 4 we develop
Bayesian nonparametric approaches to learning more complex Markov switching pro-
cesses in which each mode is endowed with conditionally linear dynamics (in contrast
to the HMM’s assumption of conditionally independent observations.) We consider two
such Markov jump linear processes: the switching linear dynamic system (SLDS) and
switching vector autoregressive (VAR) process. We refer to our Bayesian nonparamet-
ric versions of these models as the HDP-SLDS and HDP-AR-HMM, respectively. One
of the challenges of these models is deﬁning an appropriate prior on the dynamic pa-
rameters. We analyze a couple of possibilities including the conjugate matrix-normal
inverse-Wishart prior (MNIW) and a sparsity-inducing automatic relevance determina-
tion (ARD) prior. Both of these priors require knowledge of the underlying model order
(i.e., VAR order or latent state dimension); however, by employing the ARD prior so
as to encourage sparsity in a very structured manner, we are able to make inferences
about possible variable-order structure.
Finally, in Chapter 5 we turn to the case in which one has multiple related time
series and would like to transfer knowledge among them. By jointly modeling the data,
we aim to improve parameter estimates and ﬁnd interesting relationships among the
sequences. Following the theme of this thesis, we consider methods that allow each
sequence to have an unknown number of dynamical behaviors. We show that simply
tying together the parameters of multiple HDP-AR-HMMs is inadequate for our goals
since such a model assumes each time-series is performing the same set of behaviors in
the same manner. Instead of the global clustering induced by the Dirichlet process, we
demonstrate that a featural representation induced by the beta process (referred to as
the Indian buﬀet process or IBP) is more appropriate, encouraging sharing of behaviors
among objects while still allowing time-series-speciﬁc variability. To perform inference
on the resulting IBP-AR-HMM, we introduce a new method of sampling unique features
in the non-conjugate IBP case based on a birth-death proposal. The overall MCMC
sampler harnesses many eﬃciencies arising from the fact that based on a ﬁxed set of
features, the model reduces to a collection of ﬁnite switching VAR processes.

Sec. 6.2.
Suggestions for Future Research
207
■6.2 Suggestions for Future Research
We conclude by discussing a variety of open research directions suggested by our ap-
proaches to Bayesian nonparametric learning of Markov switching processes. Each of
the preceding chapters has presented a lengthy description of possible avenues for fu-
ture research as the culminating section of the chapter. In the following we primarily
point to important aspects from these sections, and highlight common themes that have
appeared. We additionally present a few new concepts not previously discussed.
■6.2.1 Inference on Large-Scale Data
Split-Merge Proposals
In Sec. 3.6 and Sec. 5.5, we discuss a mixing rate issue common to both the sticky
HDP-HMM and IBP-AR-HMM when examining high-dimensional datasets. Namely,
if our samplers merge either two true HMM modes (in the case of the sticky HDP-
HMM) or two true behavior categories (in the case of the IBP-AR-HMM), splitting
this mode requires sampling a parameter from the high-dimensional prior distribution
that better describes the data than currently instantiated parameters that have been
informed by the data. Such occurrences are rare, leading to very slow mixing rates.
Here, we clearly see the tradeoﬀbetween: marginalizing the model parameters and
sequentially sampling the mode sequence, introducing strong temporal dependencies;
and block-sampling the mode sequence, but requiring instantiation of parameters that
must be sampled from high-dimensional priors. Clever initializations of the sampler
(e.g., a block initialization that is expected to oversegment the mode sequence) can
help with producing reasonable mode sequence samples by trapping the sampler within
a preferred mode of the posterior, but still do not lead to fast mixing over the entire
support of the posterior.
In such cases, developing split-merge proposals similar to
those developed for the Dirichlet process mixture model by Jain and Neal [77] could
improve exploration between various modes of the posterior.
Variational Approaches
Although Gibbs sampling provides theoretical guarantees of accuracy, mixing rates
on large datasets can often be slow, as described in the preceding section, and are
in general diﬃcult to characterize. Alternatively, variational inference [175] provides
a fast, deterministic approximation to posteriors with an optimization criterion that
can be easily utilized to assess convergence. The goal of the variational approach is
to minimize the Kullbeck-Leibler (KL) divergence between the variational distribution
and the posterior distribution with respect to the free parameter.
This problem is
generally intractable and is then “relaxed”, yielding a simpliﬁed optimization problem
that depends on multiple free parameters that are iteratively optimized. Variational
schemes have been developed for the HDP [164] as well as linear dynamic systems [9].
However, the nonparametric nature of the dynamic models considered in this thesis
raises new conceptual challenges in extending these previous works.

208
CHAPTER 6.
CONTRIBUTIONS AND RECOMMENDATIONS
Online Learning
Another topic of keen interest in time series analysis is online estimation. The motiva-
tion for considering such algorithms is two-fold. First, and primarily, some applications
require inferences to be made sequentially as data arrive. Another motivation arises
from the fact that the batch processing algorithms developed in this thesis may be im-
practical for long time series datasets. Although the complexity of eﬃcient algorithms
like the forward-backward algorithm grow only linearly with the length of the dataset
(versus quadratically with the dimension of the mode space), incorporating information
from distant data may not be well motivated. Due both to the nonlinear dynamics and
uncertainty in model parameters, exact recursive estimation is infeasible. As discussed
in Sec. 4.4, we can instead leverage the conditionally linear dynamics to develop eﬃcient
sequential importance sampler techniques. The standard issue of a progressively im-
poverished particle representation, especially in the case of static parameter estimation,
introduces challenges that are interesting to explore. See Sec. 4.4 for further details.
Lower Dimensional Analysis
For some of the applications we examine, it is possible that the dynamics of interest
evolve on a lower-dimensional manifold. Methods for discovering such lower-dimensional
descriptions of high-dimensional time-series is an open area of research. A ﬁrst step at a
lower-order description of the data would be to consider some dimensionality-reduction
technique such as principal component analysis (PCA) [84]. We attempted PCA analy-
sis of the speaker diarization data, but did not obtain promising results. Such techniques
have been applied to motion capture data [7], and it would be interesting to see if such
an approach would eliminate the step of hand-selecting gross motor measurements from
the full 62-dimensional observation vector in the task presented in Sec. 5.4.
■6.2.2 Alternative Dynamic Structures
Semi-Markov Models
To address the temporal mode persistence issue in the HDP-HMM, we augmented the
model with a bias on self-transitions. Although such an approach is reasonable and
eﬀective in many scenarios, maintaining the simplicity of the standard HMM structure,
in some applications the dynamics are probably better described as semi-Markov [46].
That is, each mode is endowed with a duration distribution and the generative process
dictates that upon entering a mode, a sojourn time within that mode is chosen from
this duration distribution. At the culmination of the chosen sojourn time, the next
mode is chosen from a transition distribution that solely depends upon the previous
mode.
When the duration distribution is chosen to be geometric, the semi-Markov
formulation simpliﬁes to that of a discrete-time Markov process.
Various forms of
hidden semi-Markov models (HSMM), where observations are of a latent semi-Markov
mode sequence, have been proposed in the literature for ﬁnite mode spaces and are
reviewed by Murphy [122]. Considering an HSMM with an unbounded mode space is

Sec. 6.2.
Suggestions for Future Research
209
a tangible extension of the HDP-HMM, made even more intriguing when considering
each mode’s duration distribution in a Bayesian nonparametric framework.
Time-Inhomogeneous Processes
The Markov switching processes we have considered in this thesis have assumed that
the parameters associated with each mode, including the transition distributions and
emission parameters, remain the same over time. An interesting direction for future
research is to consider methods for relaxing this assumption and instead allowing for
time-inhomogeneous processes. Take, for example, a person continually transitioning
between an apriori unknown set of gaits (e.g., “run”,“walk”,“jog”). The parameters
describing these gaits, and the relative frequency of each gait, may evolve in time. As
alluded to in Sec. 5.5, one could develop extensions of the dependent Dirchlet process
(DDP) [61, 111] to such tasks.
Previous applications of the DDP include modeling
changes in the ﬁring of neurons [48] and document topic drift [156].
■6.2.3 Bayesian Nonparametric Variable-Order Markov Models
In Sec. 4.1.1 we explored a method for inferring variable-order switching VAR process
by employing a sparsity-inducing prior on a ﬁxed-dimensional parameter space.
As
mentioned in Sec. 4.4, the ARD prior we used simply provides a quadratic penalty on
non-zero model parameters, and one could instead consider stronger sparsity-inducing
priors like the class of spike and slab priors [28, 73, 182]. Such priors might lead to
clearer analysis of non-dynamical model components since positive prior mass is placed
on these components being exactly zero.
A possibly more intellectually gratifying direction for future research in this area
is to consider a Bayesian nonparametric approach to learning variable-order models
in which the maximal model order is unbounded. This idea is related to the inﬁnite
Markov model (iMM) [118] that uses prediction suﬃx trees (PST) to adaptively choose
a Markov order, but with the signiﬁcantly more challenging problem of continuous
observations. A suﬃx tree stores strings of symbols at the nodes of a tree, and labels
edges with the unique symbols. Each node of a PST is additionally associated with a
distribution over the next symbol. A PST of depth n can be used for variable-order
Markov modeling with a maximum order n [22].
Such models are often utilized in
statistical language domains where contexts of varying length, such as “United States
of”, make certain words, such as “America”, more likely. The iMM allows for inﬁnite
depth in an eﬃcient manner by utilizing a stick-breaking process. However, this model
relies on a discrete state space for repetition of observed strings. This will not occur,
almost surely, with VAR processes.
A promising ﬁrst step is to consider variable-
order moving average processes where we cluster on the input sequences and allow for
additional noise. Extensions to VAR models is a challenging next step.

210
CHAPTER 6.
CONTRIBUTIONS AND RECOMMENDATIONS
■6.2.4 Alternatives to Global Clustering
All of the models presented in this thesis (i.e., the sticky HDP-HMM of Chapter 3, the
HDP-SLDS and HDP-AR-HMM of Chapter 4, and the IBP-AR-HMM of Chapter 5)
assume that the association of an observation with a latent dynamical mode occurs
based on every dimension of the observation vector. However, as we saw in the motion
capture task of Sec. 5.4, some subjects shared behaviors solely in the trajectories of
their lower or upper body.
These diﬀerences resulted in splitting related behaviors
into separate behavior categories. Motivated by this eﬀect, instead of clustering based
on global commonalities of dynamic parameters, one could consider clustering on local
commonalities.
As discussed in Sec. 5.5, this idea is related to the local partition process (LPP)
of [37]. The LPP assumes that the high-dimensional parameter vector describing one
subject’s collection of observations might be related to that of another subject, but
only along certain dimensions of the parameter vector. A Dirichlet process prior on
the parameter space would, on the other hand, imply that if two subjects share any
component of the parameter vector, they share all components; this is unlikely to occur
in high-dimensional applications. The formulation of [37] speciﬁcally considers a random
eﬀects model in the which the parameter vector is then referred to as a random eﬀects
vector. The LPP uses a combination of global and local clustering to create an eﬀect
where assigning two subjects to the same global cluster makes it more likely for the
subjects to have identical values for multiple elements of their random eﬀects vector.
The closely related kernel partition process (KPP) [38] assumes that there is some
natural ordering or location associated with the elements of the random eﬀects vector,
and incorporates the idea that if two subjects share a given component of their random
eﬀects vectors, they are more likely to share “close by” elements as well.
It would
be interesting to examine how to extend such processes to time-series models like the
IBP-AR-HMM. For the motion capture data, one could assume that the coordinates
of the parameter space are related via a distance metric based on the human body’s
geometry.
■6.2.5 Asymptotic Analysis
One element absent from this thesis is any sort of asymptotic analysis. As mentioned
in Sec. 2.9.1, assuming the data are generated by a ﬁnite mixture, the Dirichlet pro-
cess posterior is guaranteed to converge (in distribution) to that ﬁnite set of mixture
parameters [75]. In addition, for target distributions with suﬃciently small tail prob-
abilities, Dirichlet process mixtures of Gaussians provide strongly consistent density
estimates [54, 55]. Posterior consistency results also considering fat-tailed distributions
like the Cauchy are presented in [168]. In [53], rates of convergence are established
based on an assumption of a twice continuously diﬀerentiable target density. This pa-
per also provides an overview of other convergence results based on various smoothness
conditions.

Sec. 6.2.
Suggestions for Future Research
211
The posterior consistency results above focused on density estimation using Dirichlet
process mixtures of normals.
In more general settings, the asymptotic behavior is
more challenging to assess. Diaconis and Freedman [35] provide a scenario involving
estimation of a Gaussian distributed location parameter based on independent, noisy
observations where the noise distribution is unknown and given a Dirichlet process prior.
Here, consistent frequentist estimates exist [86], but it is demonstrated that a heavy-
tailed Student-t base measure may produce an inconsistent Bayes estimate.
Other
examples, such as the Robins-Ritov paradox [180], validate that care must be taken
in analyzing the results produced by Bayesian nonparametric methods.
Developing
theoretical asymptotic guarantees for the Bayesian nonparametric models presented
herein provides an important and challenging direction for future research.

212
CHAPTER 6.
CONTRIBUTIONS AND RECOMMENDATIONS

Appendix A
Sticky HDP-HMM Direct
Assignment Sampler
This appendix provides the derivations for the sequential, direct assignment Gibbs
samplers outlined in Algorithms 9 and 11. Throughout this appendix, we will refer to
the random variables in the graph of Fig. 2.13(b). For these derivations we include
the κ term of the sticky HDP-HMM; the derivations for the original HDP-HMM follow
directly by setting κ = 0.
■A.1 Sticky HDP-HMM
To derive the direct assignment sampler for the sticky HDP-HMM, we ﬁrst assume that
we sample: table assignments for each customer, tji; served dish assignments for each
table, kjt; considered dish assignments, ¯kjt; dish override variables, wjt; and the global
mixture weights, β. Because of the properties of the HDP, and more speciﬁcally the
stick-breaking distribution, we are able to marginalize the group-speciﬁc distributions
˜πj and parameters θk and still have closed-form distributions from which to sample
(since exchangeability implies that we may treat every table and dish as if it were the
last, as in Eq. (3.8).) The assumption of having tji and kjt is a stronger assumption
than that of having zji since zji can be uniquely determined from tji and kjt, though
not vice versa. We proceed to show that directly sampling zji instead of tji and kjt is
suﬃcient when the auxiliary variables mjk, ¯mjk, and wjt are additionally sampled.
■A.1.1 Sampling zt
Recall the generative process of Eq. (3.1) using the sticky variant of the model given
by Eq. (3.3). Using the conditional independence statements encoded in the graph of
213

214
APPENDIX A. STICKY HDP-HMM DIRECT ASSIGNMENT SAMPLER
Fig. 3.2(a), the posterior distribution of zt factors as:
p(zt = k | z\t, y1:T, β, α, κ) ∝
Z
π
Y
i
p(πi | α, β, κ)
Y
τ
p(zτ | πzτ−1)dπ
Z Y
k
p(θk | λ)
Y
τ
p(yτ | θzτ )dθ
∝p(zt = k | z\t, β, α, κ)p(yt | y\t, zt = k, z\t, λ).
(A.1)
The term p(zt = k | z\t, β, α, κ), which arises from integration over π, is a variant of
the Chinese restaurant franchise prior, while p(yt | y\t, zt=k, z\t, λ) is the likelihood of
an assignment zt = k having marginalized the parameter θk.
The conditional distribution p(zt = k | z\t, β, α, κ) of Eq. (A.1) can be written as:
p(zt = k | z\t, β, α, κ) ∝
Z
π
p(zt+1 | πk)p(zt = k | πzt−1)
Y
i
(p(πi | α, β, κ)
Y
τ|zτ−1=i,τ̸=t,t+1
p(zτ | πi))dπ
∝
Z
π
p(zt+1 | πk)p(zt = k | πzt−1)
(A.2)
Y
i
p(πi | {zτ | zτ−1 = i, τ ̸= t, t + 1}, β, α, κ)dπ.
Let zt−1 = j. If k ̸= j, that is, assuming a change in state value at time t, then
p(zt = k | z\t, β, α, κ)
∝
Z
πk
p(zt+1 | πk)p(πk | {zτ | zτ−1 = k, τ ̸= t, t + 1}, β, α, κ)dπk
Z
πj
p(zt = k | πj)p(πj | {zτ | zτ−1 = j, τ ̸= t, t + 1}, β, α, κ)dπj
∝p(zt+1 | {zτ | zτ−1 = k, τ ̸= t, t + 1}, β, α, κ)
(A.3)
p(zt = k | {zτ | zτ−1 = j, τ ̸= t, t + 1}, β, α, κ).
When considering the probability of a self-transition (i.e., k = j), we have
p(zt = j | z\t, β, α, κ) ∝
Z
πj
p(zt+1 | πj)p(zt = j | πj)
p(πj | {zτ | zτ−1 = k, τ ̸= t, t + 1}, β, α, κ)dπj
∝p(zt = j, zt+1 | {zτ | zτ−1 = k, τ ̸= t, t + 1}, β, α, κ).
(A.4)
These predictive distributions can be derived by standard results arising from having
placed a Dirichlet prior on the parameters deﬁning these multinomial observations zτ.

Sec. A.1.
Sticky HDP-HMM
215
The ﬁnite Dirichlet prior is induced by considering the ﬁnite partition {1, . . . , K, A˜k}
of Z+, where A˜k = {K + 1, K + 2, . . . } is the set of unrepresented state values in z\t.
The properties of the Dirichlet process (see Theorem 2.9.1) dictate that on this ﬁnite
partition, we have the following form for the group-speciﬁc transition distributions:
πj | α, β ∼Dir(αβ1, . . . , αβj + κ, . . . , αβK, αβ˜k),
(A.5)
where β˜k = P∞
i=K+1 βi. Using this prior, we derive the distribution of a generic set of
observations generated from a single transition distribution πi given the hyperparame-
ters α, β, and κ:
p({zτ | zτ−1 = i} | β, α, κ) =
Z
πi
p(πi | β, α, κ)p({zτ | zτ−1 = i} | πi)dπi
=
Z
πi
Γ(P
k αβk + κδ(k, i))
Q
k Γ(αβk + κδ(k, i))
K+1
Y
k=1
παβk+κδ(k,i)−1
jk
K+1
Y
k=1
πnjk
jk dπi
= Γ(P
k αβk + κδ(k, i))
Q
k Γ(αβk + κδ(k, i))
Q
k Γ(αβk + κδ(k, i) + njk)
Γ(P
k αβk + κδ(k, i) + njk)
=
Γ(α + κ)
Γ(α + κ + ni·)
Y
k
Γ(αβk + κδ(k, i) + njk)
Γ(αβk + κδ(k, i))
,
(A.6)
where we make a slight abuse of notation in taking βK+1 = β˜k. We use Eq. (A.6) to
determine that the ﬁrst component of Eq. (A.3) is
p(zt = k | {zτ | zτ−1 = j, τ ̸= t, t + 1}, β, α, κ)
= p({zτ | zτ−1 = j, τ ̸= t + 1, zt = k} | β, α, κ)
p({zτ | zτ−1 = j, τ ̸= t, t + 1} | β, α, κ)
=
Γ(α + κ + n−t
j· )
Γ(α + n−t
j· + 1)
Γ(αβk + κ + n−t
jk + 1)
Γ(αβk + n−t
jk )
=
αβk + n−t
jk
α + n−t
j·
.
(A.7)
Here, n−t
jk denotes the number of transitions from state j to k not counting the transition
from zt−1 to zt or from zt to zt+1. Similarly, the second component of Eq. (A.3) is derived
to be
p(zt+1 = ℓ| {zτ | zτ−1 = k, τ ̸= t, t + 1}, β, α, κ) = αβℓ+ κδ(ℓ, k) + n−t
kℓ
α + κ + n−t
k·
,
(A.8)

216
APPENDIX A. STICKY HDP-HMM DIRECT ASSIGNMENT SAMPLER
For k = j, the distribution of Eq. (A.4) reduces to
p(zt = j, zt+1 | {zτ | zτ−1 = j, τ ̸= t, t + 1}, β, α, κ)
=
p({zτ | zτ−1 = j} | β, α, κ)
p({zτ | zτ−1 = j, τ ̸= t, t + 1} | β, α, κ)
=





Γ(α+κ+n−t
j· )
Γ(α+κ+n−t
j· +2)
Γ(αβj+κ+n−t
jj +1)
Γ(αβj+κ+n−t
jj )
Γ(αβℓ+n−t
jℓ+1)
Γ(αβℓ+n−t
jℓ) ,
zt+1 = ℓ, ℓ̸= j;
Γ(α+κ+n−t
j· )
Γ(α+κ+n−t
j· +2)
Γ(αβj+κ+n−t
jj +2)
Γ(αβj+κ+n−t
jj ) ,
zt+1 = j;
=





(αβj+κ+n−t
jj )(αβℓ+n−t
jℓ)
(α+κ+n−t
j· +1)(α+κ+n−t
j· ),
zt+1 = ℓ, ℓ̸= j;
(αβj+κ+n−t
jj +1)(αβj+κ+n−t
jj )
(α+κ+n−t
j· +1)(α+κ+n−t
j· )
,
zt+1 = j;
=
(αβj + κ + n−t
jj )(αβℓ+ n−t
jℓ+ (κ + 1)δ(j, ℓ))
(α + κ + n−t
j· )(α + κ + n−t
j· + 1)
.
(A.9)
Combining these cases, the prior predictive distribution of zt is:
p(zt = k | z\t, β, α, κ)
∝









(αβk + n−t
zt−1k + κδ(zt−1, k))

αβzt+1+n−t
kzt+1+κδ(k,zt+1)+δ(zt−1,k)δ(k,zt+1)
α+n−t
k· +κ+δ(zt−1,k)

k ∈{1, . . . , K}
α2β˜kβzt+1
α+κ
k = K + 1.
(A.10)
The conditional distribution of the observation yt given an assignment zt = k and
given all other observations yτ, having marginalized out θk, can be written as follows:
p(yt | y\t, zt = k, z\t, λ)
∝
Z
p(yt | θk)p(θk | λ)
Y
τ|zτ =k,τ̸=t
p(yτ | θk)dθk
∝
Z
p(yt | θk)p(θk | {yτ | zτ = k, τ ̸= t}, λ)dθk
∝
p(yt | {yτ | zτ = k, τ ̸= t}, λ).
(A.11)
There exists a closed-form distribution for this likelihood if we consider a conjugate
distribution on the parameter space Θ.
Assuming our emission distributions are Gaussian with unknown mean and covari-
ance parameters, the conjugate prior is the normal-inverse-Wishart distribution (see
Sec. 2.4.3), which we denote by NIW(ζ, ϑ, ν, ∆). Here, λ = {ζ, ϑ, ν, ∆}. As described
in Sec. 2.4.3, via conjugacy, the posterior distribution of θk = {µk, Σk} given a set
of Gaussian observations yt ∼N(µk, Σk) is distributed as an updated normal-inverse-

Sec. A.1.
Sticky HDP-HMM
217
Wishart NIW(¯ζk, ¯ϑk, ¯νk, ¯∆k), where
¯ζk = ζ + |{ys | zs = k, s ̸= t}| ≜ζ + |Yk|
¯νk = ν + |Yk|
¯ζk ¯ϑk = ζϑ +
X
ys∈Yk
ys
¯νk ¯∆k = ν∆+
X
ys∈Yk
ysyT
s + ζϑϑT −¯ζk ¯ϑk ¯ϑT
k .
Marginalizing θk induces a multivariate Student-t predictive distribution for yt as given
by Eq. (2.87):
p(yt | {yτ | zτ = k, τ ̸= t}, ζ, ϑ, ν, ∆) = t¯νk−d−1

yt; ¯ϑk,
(¯ζk + 1)¯νk
¯ζk(¯νk −d −1)
¯∆k

≜tˆνk(yt; ˆµk, ˆΣk).
(A.12)
■A.1.2 Sampling β
Let ¯K be the number of unique dishes considered. We note that for the sticky HDP-
HMM, every served dish had to be considered in some restaurant. The only scenario
in which this would not be the case is if for some dish j, every table served dish j
arose from an override decision. However, overrides resulting in dish j being served
can only occur in restaurant j, and this restaurant would not exist if dish j was not
considered (and thus served) in some other restaurant. Therefore, each served dish had
to be considered by at least one table in the franchise. On the other hand, there may
be some dishes considered that were never served. From this, we conclude that ¯K ≥K.
We will assume that the K served dishes are indexed in {1, . . . , K} and any considered,
but not served, dish is indexed in {K + 1, K + 2, . . . }. For the sake of inference, we will
see in the following section that ¯K never exceeds K, the number of unique considered
dishes, implying that ¯K = K.
Take a ﬁnite partition {θ1, θ2, . . . , θ ¯
K, Θ˜k} of the parameter space Θ, where Θ˜k =
Θ\ S ¯
K
k=1{θk} is the set of all currently unrepresented parameters.
By deﬁnition of
the Dirichlet process (once again using Theorem 2.9.1 combined with the fact that
G0 ∼DP(γ, H)), G0 has the following distribution on this ﬁnite partition:
(G0(θ1), . . . , G0(θ ¯
K), G0(Θ˜k)) | γ, H ∼Dir(γH(θ1), . . . , γH(θ ¯
K), γH(Θ˜k))
∼Dir(0, . . . , 0, γ),
(A.13)
where we have used the fact that H is absolutely continuous with respect to Lebesgue
measure.
For every currently instantiated table t, the considered dish assignment variable
¯kjt associates the table-speciﬁc considered dish θ∗
jt with one among the unique set of
dishes {θ1, . . . , θ ¯
K}. Recalling that ¯mjk denotes how many of the tables in restaurant j

218
APPENDIX A. STICKY HDP-HMM DIRECT ASSIGNMENT SAMPLER
considered dish θk, we see that we have ¯m·k observations θ∗
jt ∼G0 in the franchise that
fall within the single-element cell {θk}. By the properties of the Dirichlet distribution,
speciﬁcally as given by Eq. (2.74), the posterior of G0 is
(G0(θ1), . . . , G0(θ ¯
K), G0(Θ˜k))|θ∗, γ ∼Dir( ¯m·1, . . . , ¯m· ¯
K, γ).
(A.14)
Since (G0(θ1), . . . , G0(θ ¯
K), G0(Θ˜k)) is by deﬁnition equal to (β1, . . . , β ¯
K, β˜k), and from
the conditional independencies illustrated in Fig. 2.13, the desired posterior of β is
(β1, . . . , β ¯
K, β˜k) | t, k, ¯k, w, y1:T , γ ∼Dir( ¯m·1, . . . , ¯m· ¯
K, γ),
(A.15)
where here we deﬁne β˜k = P∞
k= ¯K+1 βk. From the above, we see that { ¯m·k} ¯
K
k=1 is a set
of suﬃcient statistics for resampling β deﬁned on this partition. Thus, it is suﬃcient to
sample ¯mjk instead of tji and kjt, when given the state index zt. The sampling of ¯mjk,
as well as the resampling of hyperparameters (see Appendix C), is greatly simpliﬁed by
additionally sampling auxiliary variables mjk and wjt, corresponding to the number of
tables in restaurant j that were served dish k and the corresponding override variables.
■A.1.3 Jointly Sampling mjk, wjt, and ¯mjk
We jointly sample the auxiliary variables mjk, wjt, and ¯mjk from
p(m, w, ¯
m | z1:T , β, α, κ) = p( ¯
m | m, w, z1:T , β, α, κ)
p(w | m, z1:T , β, α, κ)p(m | z1:T , β, α, κ).
(A.16)
We start by examining p(m | z1:T , β, α, κ). Having the state index assignments z1:T
eﬀectively partitions the data (customers) into both restaurants and dishes, though the
table assignments are unknown since multiple tables can be served the same dish. Thus,
sampling mjk is in eﬀect equivalent to sampling table assignments for each customer
after knowing the dish assignment. This conditional distribution is given by:
p(tji = t | kjt = k, t−ji, k−jt, y1:T , β, α, κ)
∝p(tji | tj1, . . . , tji−1, tji+1, . . . , tjTj, α, κ)p(kjt = k | β, α, κ)
∝
(
˜n−ji
jt ,
t ∈{1, . . . , Tj};
αβk + κδ(k, j),
t = Tj + 1,
(A.17)
where ˜n−ji
jt
is the number of customers sitting at table t in restaurant j, not counting
yji.
Similarly, t−ji are the table assignments for all customers except yji and k−jt
are the dish assignments for all tables except table t in restaurant j. We recall from
Sec. 3.1.1 that Tj is the number of currently occupied tables in restaurant j. The form of
Eq. (A.17) implies that a customer’s table assignment conditioned on a dish assignment
k follows a Dirichlet process with concentration parameter αβk + κδ(k, j). That is,
tji | kjtji = k, t−ji, k−jtji, y1:T , β, α, κ ∼˜π′,
˜π′ ∼GEM(αβk + κδ(k, j)).
(A.18)

Sec. A.1.
Sticky HDP-HMM
219
Then, Eq. (2.218) provides the form for the distribution over the number of unique
components (i.e., tables) generated by sampling njk times from this stick-breaking dis-
tributed measure, where we note that for the HDP-HMM njk is the number of customers
in restaurant j eating dish k:
p(mjk = m | njk, β, α, κ) =
Γ(αβk + κδ(k, j))
Γ(αβk + κδ(k, j) + njk)s(njk, m)(αβk + κδ(k, j))m.
(A.19)
For large njk, it is often more eﬃcient to sample mjk by simulating the table assignments
of the Chinese restaurant, as described by Eq. (A.17), rather than having to compute
a large array of Stirling numbers.
We now derive the conditional distribution for the override variables wjt.
The
table counts provide that mjk tables are serving dish k in restaurant j. If k ̸= j, we
automatically have mjk tables with wjt = 0 since the served dish is not the house
specialty. Otherwise, for each of the mjj tables t serving dish kjt = j, we start by
assuming we know the considered dish index ¯kjt, from which inference of the override
parameter is trivial. We then marginalize over all possible values of this index:
p(wjt | kjt = j, β, ρ =
¯
K
X
¯kjt=1
p(¯kjt, wjt | kjt = j, β) + p(¯kjt = ¯K + 1, wjt | kjt = j, β)
∝
¯
K
X
¯kjt=1
p(kjt = j | ¯kjt, wjt)p(¯kjt | β)p(wjt | ρ)
+ p(kjt = j | ¯kjt = ¯K + 1, wjt)p(¯kjt = ¯K + 1 | β)p(wjt | ρ)
∝
(
βj(1 −ρ),
wjt = 0;
ρ,
wjt = 1,
(A.20)
where ρ =
κ
α+κ is the prior probability that wjt = 1. This distribution implies that
having observed a served dish kjt = j makes it more likely that the considered dish ¯kjt
was overridden via choosing wjt = 1 than the prior suggests. This is justiﬁed by the
fact that if wjt = 1, the considered dish ¯kjt could have taken any value and the served
dish would still be kjt = j. The only other explanation of the observation kjt = j
is that the dish was not overridden, namely wjt = 0 occurring with prior probability
(1 −ρ), and the table considered a dish ¯kjt = j, occurring with probability βj. These
events are independent, resulting in the above distribution. We draw mjj i.i.d. samples
of wjt from Eq. (A.20), with the total number of dish overrides in restaurant j given
by wj· = P
t wjt. The sum of these Bernoulli random variables results in a binomial
random variable.
Given mjk for all j and k and wjt for each of these instantiated tables, we can now
deterministically compute ¯mjk, the number of tables that considered ordering dish k

220
APPENDIX A. STICKY HDP-HMM DIRECT ASSIGNMENT SAMPLER
in restaurant j. Any table that was overridden is an uninformative observation for the
posterior of ¯mjk so that
¯mjk =
(
mjk,
j ̸= k;
mjj −wj·,
j = k.
(A.21)
Note that we are able to subtract oﬀthe sum of the override variables within a restau-
rant, wj·, since the only time wjt = 1 is if table t is served dish j. From Eq. (A.21), we
see that ¯K = K.
■A.2 Sticky HDP-HMM with DP emissions
In this section we derive the predictive distribution of the augmented state (zt, st) of
the sticky HDP-HMM with DP emissions of Sec. 3.3. We use the chain rule to write:
p(zt = k, st = j | z\t, s\t, y1:T , β, α, σ, κ, λ)
= p(st = j | zt = k, z\t, s\t, y1:T , σ, λ)
p(zt = k | z\t, s\t, y1:T , β, α, κ, λ).
(A.22)
We can examine each term of this distribution by once again considering the joint distri-
bution over all variables in the graph of Fig. 3.2(b) and integrating over the appropriate
parameters. For the conditional distribution of zt = k when not given st, this amounts
to:
p(zt = k | z\t, s\t, y1:T , β, α, κ, λ) ∝
Z
π
Y
j
p(πj | α, β, κ)
Y
τ
p(zτ | πzτ−1)dπ
X
st
Z
ψ
Y
j
p(ψj | σ)
Y
τ
p(sτ | ψzτ )dψ
Z Y
i,ℓ
p(θi,ℓ| λ)
Y
τ
p(yτ | θzτ ,sτ)dθ
∝p(zt = k | z\t, β, α, κ)
(A.23)
X
st
p(st | {sτ | zτ = k, τ ̸= t}, σ)p(yt | {yτ | zτ = k, st, τ ̸= t}, λ).
The term p(zt = k | z\t, β, α, κ) is as in Eq. (A.10), while
p(st = j | {sτ | zτ = k, τ ̸= t}, σ) =





n
′−t
kj
σ+n
′−t
k·
,
j ∈{1, . . . , K′
k};
σ
σ+n
′−t
k·
,
j = K′
k + 1,
(A.24)
which is the predictive distribution of the indicator random variables of the DP mixture
model associated with zt = k. This can be derived directly from the Chinese restaurant
process predictive distribution of Eq. (2.216). Here, n′−t
kj is the number of observations

Sec. A.2.
Sticky HDP-HMM with DP emissions
221
yτ with (zτ = k, sτ = j) for τ ̸= t, and K′
k is the number of currently instantiated
mixture components for the kth emission density.
We similarly derive the conditional distribution of an assignment st = j given zt = k
as:
p(st = j | zt = k, z\t, s\t, y1:T , σ, λ) ∝p(st = j | {sτ | zτ = k, τ ̸= t}, σ)
p(yt | {yτ | zτ = k, st = j, τ ̸= t}, λ).
(A.25)
The likelihood component of these distributions,
p(yt | {yτ | zτ = k, st = j, τ ̸= t}, λ),
(A.26)
is derived in the same fashion as Eq. (A.12) where now we only consider the observations
yτ that are assigned to HDP-HMM state zτ = k and mixture component sτ = k.

222
APPENDIX A. STICKY HDP-HMM DIRECT ASSIGNMENT SAMPLER

Appendix B
Sticky HDP-HMM Blocked Sampler
In this appendix, we present the derivation of the blocked Gibbs samplers outlined in
Algorithms 10 and 12.
■B.1 Sampling β, π, and ψ
The order L weak limit approximation to the Dirichlet process mixture model gives us
the following form for the prior distribution on the global weights β (see Eq. (2.225)):
β | γ ∼Dir(γ/L, . . . , γ/L).
(B.1)
Using the sticky HPD-HMM generative model of Eq. (3.3), Theorem 2.9.1 informs us
that on this ﬁnite partition, the prior distribution over the transition distributions is
Dirichlet with parametrization:
πj | α, κ, β ∼Dir(αβ1, . . . , αβj + κ, . . . , αβL).
(B.2)
Recalling that ¯kjt ∼β and zt ∼πzt−1, the standard Dirichlet-multinomial conjugacy
results of Eq. (2.74) imply that the posterior distributions are given by:
β | ¯
m, γ ∼Dir(γ/L + ¯m·1, . . . , γ/L + ¯m·L)
(B.3)
πj | z1:T , α, β ∼Dir(αβ1 + nj1, . . . , αβj + κ + njj, . . . , αβL + njL),
where we recall that njk is the number of j to k transitions in the state sequence z1:T
and ¯mjk is the number of tables in restaurant j that considered dish k. The sampling
of the auxiliary variables ¯mjk is as in Appendix A.
For the sticky HDP-HMM with DP emissions of Sec. 3.3, an order L′ weak limit
approximation to the DP prior on the emission parameters yields the following posterior
distribution on the mixture weights ψk:
ψk | z1:T , s1:T , σ ∼Dir(σ/L′ + n′
k1, . . . , σ/L′ + n′
kL′),
(B.4)
where n′
kℓis the number of observations assigned to the ℓth mixture component of the
kth HMM state.
223

224
APPENDIX B. STICKY HDP-HMM BLOCKED SAMPLER
■B.2 Sampling z1:T for the Sticky HDP-HMM
To derive the forward-backward procedure for jointly sampling z1:T given y1:T for the
sticky HDP-HMM, we ﬁrst note that
p(z1:T | y1:T, π, θ) = p(zT | zT−1, y1:T , π, θ)p(zT−1 | zT−2, y1:T , π, θ)
· · · p(z2 | z1, y1:T , π, θ)p(z1 | y1:T , π, θ).
Thus, we may ﬁrst sample z1 from p(z1 | y1:T , π, β, θ), then condition on this value to
sample z2 from p(z2 | z1, y1:T , π, θ), and so on. The conditional distribution of z1 is
derived as:
p(z1 | y1:T , π, θ) ∝p(z1)p(y1 | θz1)
X
z2:T
Y
t
p(zt | πzt−1)p(yt | θzt)
∝p(z1)p(y1 | θz1)
X
z2
p(z2 | πz1)p(y2 | θz2)m3,2(z2)
∝p(z1)p(y1 | θz1)m2,1(z1),
(B.5)
where mt,t−1(zt−1) is the backward message passed from zt to zt−1 and for an HMM is
recursively deﬁned by (see Sec. 2.6.1):
mt,t−1(zt−1) ∝
( P
zt p(zt | πzt−1)p(yt | θzt)mt+1,t(zt),
t ≤T;
1,
t = T + 1;
∝p(yt:T | zt−1, π, θ).
(B.6)
The general conditional distribution of zt is:
p(zt | zt−1, y1:T , π, θ) ∝p(zt | πzt−1)p(yt | θzt)mt+1,t(zt).
(B.7)
So, to block sample z1:T , we pass messages backwards and then recursively sample zt
forwards (i.e., for t = 1, . . . , T) from the distributions deﬁned in Eq. (B.7) and Eq. (B.5).
■B.3 Sampling (z1:T, s1:T) for the Sticky HDP-HMM with DP emissions
We now examine how to sample the augmented state (zt, st) of the sticky HDP-HMM
with DP emissions. The conditional distribution of (zt, st) for the forward-backward
procedure is derived as:
p(zt, st | zt−1, y1:T , π, ψ, θ) ∝p(zt | πzt−1)p(st | ψzt)p(yt | θzt,st)mt+1,t(zt).
(B.8)
Since the Markovian structure is only on the zt component of the augmented state, the
backward message mt,t−1(zt−1) from (zt, st) to (zt−1, st−1) is solely a function of zt−1.
These messages are given by (see Sec. 2.6.1):
mt,t−1(zt−1)
∝
( P
zt
P
st p(zt | πzt−1)p(st | ψzt)p(yt | θzt,st)mt+1,t(zt),
t ≤T;
1,
t = T + 1.
(B.9)

Sec. B.4.
Sampling θ
225
More speciﬁcally, since each component j of the kth state-speciﬁc emission distribution
is a Gaussian with parameters θj,k = {µk,j, Σk,j}, we have:
p(zt = k, st = j | zt−1, y1:T , π, ψ, θ) ∝πzt−1(k)ψk(j)N(yt; µk,j, Σk,j)mt+1,t(k)
mt+1,t(k) =
L
X
i=1
L′
X
ℓ=1
πk(i)ψi(ℓ)N(yt+1; µi,ℓ, Σi,ℓ)mt+2,t+1(i)
mT+1,T (k) = 1
k = 1, . . . , L.
(B.10)
■B.4 Sampling θ
Depending on the form of the emission distribution and base measure on the parameter
space Θ, we sample parameters for each of the currently instantiated states from the
updated posterior distribution. For the sticky HDP-HMM, this distribution is:
θj | z1:T , y1:T , λ ∼p(θ | {yt | zt = j}, λ).
(B.11)
For the sticky HDP-HMM with DP emissions, the posterior distribution for each Gaus-
sian’s mean and covariance, θk,j, is determined by the observations assigned to this
component, namely,
θk,j | z1:T , s1:T , y1:T , λ ∼p(θ | {yt | (zt = k, st = j)}, λ).
(B.12)
■B.4.1 Non-Conjugate Base Measures
Since the blocked sampler instantiates the parameters θk, rather than marginalizing
them as in the direct assignment sampler, we can place a non-conjugate base measure
on the parameter space Θ. Take, for example, the case of single Gaussian emission
distributions where the parameters are the means and covariances of these distributions.
Here, θk = {µk, Σk}. In this situation, one may place a Gaussian prior N(µ0, Σ0) on
the mean µk and an inverse-Wishart IW(ν, ∆) prior on the covariance Σk.
Conditioned on the state sequence z1:T , we may examine the set of observations
assigned to state k, which we denote by Yk = {yt | zt = k}. The posterior distribu-
tions over the mean and covariance parameters of that state are then derived from the
standard inverse-Wishart and Gaussian posterior distributions of Sec. 2.4.3 to be:
Σk | µk
∼
IW(¯νk ¯∆k, ¯νk)
(B.13)
µk | Σk
∼
N(¯µk, ¯Σk),

226
APPENDIX B. STICKY HDP-HMM BLOCKED SAMPLER
where,
¯νk
=
ν + |Yk|
¯νk ¯∆k
=
ν∆+
X
t∈Yk
(yt −µk)(yt −µk)′
¯Σk
=
(Σ−1
0
+ |Yk|Σ−1
k )−1
¯µk
=
¯Σk

Σ−1
0 µ0 + Σk
X
t∈Yk
yt

.
The sampler alternates between sampling µk given Σk and Σk given µk several times
before moving on to the next stage in the sampling algorithm. The equations for the
sticky HDP-HMM with DP emissions follow directly by considering Yk,j = {yt | zt =
k, st = j} when resampling parameter θk,j = {µk,j, Σk,j}.

Appendix C
Hyperparameters
In this appendix we present the derivations of the conditional distributions for the
hyperparameters of the sticky HDP-HMM of Chapter 3. These hyperparameters include
α, κ, γ, σ, and λ, where λ is considered ﬁxed. Many of these derivations follow directly
from those presented in [40, 162].
We parameterize our model by (α+κ) and ρ = κ/(α+κ); this simpliﬁes the resulting
sampler. We place Gamma(a, b) priors on each of the concentration parameters (α+κ),
γ, and σ, and a Beta(c, d) prior on ρ. The a and b parameters of the gamma hyperprior
may diﬀer for each of the concentration parameters. In the following sections, we derive
the resulting posterior distribution of these hyperparameters.
■C.1 Posterior of (α + κ)
Let us assume that there are J restaurants in the franchise at a given iteration of
the sampler. Note that for the HDP-HMM, the number of restaurants is equal to the
number of unique states in z1:T 1. As depicted in Fig. 2.13(b), the generative model
dictates that for each restaurant j we have ˜πj ∼GEM(α+κ), and a table assignment is
determined for each customer by tji ∼˜πj. In total there are nj· draws from this stick-
breaking measure over table assignments resulting in mj· unique tables. By Eq. (2.218)
and using the fact that the restaurants are mutually conditionally independent, we may
write:
p(α + κ | m1·, . . . , mJ·, n1·, . . . , nJ·) ∝p(α + κ)p(m1·, . . . , mJ· | α + κ, n1·, . . . , nJ·)
∝p(α + κ)
J
Y
j=1
p(mj· | α + κ, nj·) ∝p(α + κ)
J
Y
j=1
s(nj·, mj·)(α + κ)mj·
Γ(α + κ)
Γ(α + κ + nj·)
∝p(α + κ)(α + κ)m··
J
Y
j=1
Γ(α + κ)
Γ(α + κ + nj·).
(C.1)
1One must account for the fact that the initial state is drawn from a special initial distribution, and
the value of zT does not create a new restaurant.
227

228
APPENDIX C. HYPERPARAMETERS
Using the fact that the gamma function has the property Γ(z+1) = zΓ(z) and is related
to the beta function via β(x, y) = Γ(x)Γ(y)/Γ(x + y), we rewrite this distribution as
p(α + κ | m1·, . . . , mJ·, n1·, . . . , nJ·)
∝p(α + κ)(α + κ)m··
J
Y
j=1
(α + κ + nj·)β(α + κ + 1, nj·)
(α + κ)Γ(nj·)
= p(α + κ)(α + κ)m··
J
Y
j=1

1 +
nj·
α + κ
 Z 1
0
rα+κ
j
(1 −rj)nj·−1drj,
(C.2)
where the second equality arises from the fact that β(x, y) =
R 1
0 tx−1(1 −t)y−1dt. We
introduce a set of auxiliary random variables r = {r1, . . . , rJ}, where each rj ∈[0, 1].
Now, we augment the posterior with these auxiliary variables as follows:
p(α + κ, r | m1·, . . . , mJ·, n1·, . . . , nJ·)
∝p(α + κ)(α + κ)m··
J
Y
j=1

1 +
nj·
α + κ

rα+κ
j
(1 −rj)nj·−1
∝(α + κ)a+m··−1e−(α+κ)b
J
Y
j=1

1 +
nj·
α + κ

rα+κ
j
(1 −rj)nj·−1
= (α + κ)a+m··−1e−(α+κ)b
J
Y
j=1
X
sj∈{0,1}
 nj·
α + κ
sj
rα+κ
j
(1 −rj)nj·−1. (C.3)
Here, we have used the fact that we placed a Gamma(a, b) prior on (α + κ). We add
another set of auxiliary variables s = {s1, . . . , sJ}, with each sj ∈{0, 1}, to further
simplify this distribution. The joint distribution over (α + κ), r, and s is given by
p(α + κ, r, s | m1·, . . . , mJ·, n1·, . . . , nJ·)
∝(α + κ)a+m··−1e−(α+κ)b
J
Y
j=1
 nj·
α + κ
sj
rα+κ
j
(1 −rj)nj·−1.
(C.4)
The conditional distribution of α + κ given the auxiliary variables is:
p(α + κ | r, s, m1·, . . . , mJ·, n1·, . . . , nJ·)
∝(α + κ)a+m··−1−PJ
j=1 sje−(α+κ)(b−PJ
j=1 log rj)
= Gamma(a + m·· −
J
X
j=1
sj, b −
J
X
j=1
log rj),
(C.5)

Sec. C.2.
Posterior of γ
229
while the auxiliary variables have conditional distributions:
p(rj | α + κ, r\j, s, m1·, . . . , mJ·, n1·, . . . , nJ·)
∝
rα+κ
j
(1 −rj)nj·−1
=
Beta(α + κ + 1, nj·)
(C.6)
p(sj | α + κ, r, s\j, m1·, . . . , mJ·, n1·, . . . , nJ·)
∝
 nj·
α + κ
sj
=
Ber

nj·
nj· + α + κ

.
(C.7)
■C.2 Posterior of γ
We may similarly derive the conditional distribution of γ. The generative model de-
picted in Fig. 2.13(b) dictates that β ∼GEM(γ) and that each table t considers ordering
a dish ¯kjt ∼β. From Eq. (A.21), we see that the sampled value ¯mj· represents the total
number of tables in restaurant j where the considered dish ¯kjt was the served dish kjt
(i.e., the number of tables with considered dishes that were not overridden.) Thus, ¯m··
is the total number of informative draws from β. If K is the number of unique served
dishes, which can be inferred from z1:T , then the number of unique considered dishes
at the informative tables is:
¯K =
K
X
k=1
1( ¯m·k > 0) = K −
K
X
k=1
1( ¯m·k = 0 and mkk > 0).
(C.8)
We use the notation 1(A) to represent an indicator random variable that is 1 if the
event A occurs and 0 otherwise.
The only case where ¯K is not equivalent to K is
if every instance of a served dish k arose from an override in restaurant k and this
dish was never considered in any other restaurant. That is, there were no informative
considerations of dish k, implying ¯m·k = 0, while dish k was served in restaurant k,
implying mkk > 0 so that k is counted in K. This is equivalent to counting how many
dishes k had an informative table consider ordering dish k, regardless of the restaurant.
We may now use Eq. (2.218) to form the conditional distribution on γ:
p(γ | ¯K, ¯m··)
∝
p(γ)p( ¯K | γ, ¯m··)
∝
p(γ)s( ¯m··, ¯K)γ
¯
K
Γ(γ)
Γ(γ + ¯m··)
∝
p(γ)γ
¯
K (γ + ¯m··)β(γ + 1, ¯m··)
γΓ( ¯m··)
∝
p(γ)γ
¯
K−1(γ + ¯m··)
Z 1
0
ηγ(1 −η) ¯m··−1dη.
(C.9)

230
APPENDIX C. HYPERPARAMETERS
As before, we introduce an auxiliary random variable η ∈[0, 1] so that the joint distri-
bution over γ and η can be written as
p(γ, η | ¯K, ¯m··)
∝
p(γ)γ
¯
K−1(γ + ¯m··)ηγ(1 −η) ¯m··−1
∝
γa+ ¯
K−2(γ + ¯m··)e−γ(b−log η)(1 −η) ¯m··−1.
(C.10)
Here, we have used the fact that there is a Gamma(a, b) prior on γ. We may add an
indicator random variable ζ ∈{0, 1} as we did in Eq. (C.4), such that
p(γ, η, ζ | ¯K, ¯m··)
∝
γa+ ¯
K−1
 ¯m··
γ
ζ
e−γ(b−log η)(1 −η) ¯m··−1.
(C.11)
The resulting conditional distributions are given by:
p(γ | η, ζ, ¯K, ¯m··)
∝
γa+ ¯
K−1−ζe−γ(b−log η)
=
Gamma(a + ¯K −ζ, b −log η)
(C.12)
p(η | γ, ζ, ¯K, ¯m··)
∝
ηγ(1 −η) ¯m··−1 = Beta(γ + 1, ¯m··)
(C.13)
p(ζ | γ, η, ¯K, ¯m··)
∝
 ¯m··
γ
ζ
= Ber

¯m··
¯m·· + γ

.
(C.14)
Alternatively, we can directly identify Eq (C.10) as leading to a conditional distribution
on γ that is a simple mixture of two Gamma distributions:
p(γ | η, ¯K, ¯m··)
∝
γa+ ¯
K−2(γ + ¯m··)e−γ(b−log η)
∝
π ¯mGamma(a + ¯K, b −log η)
(C.15)
+(1 −π ¯m)Gamma(a + ¯K −1, b −log η)
p(η | γ, ¯K, ¯m··)
∝
ηγ(1 −η) ¯m··−1 = Beta(γ + 1, ¯m··),
(C.16)
where
π ¯m =
a + ¯K −1
¯m··(b −log η).
(C.17)
The distribution in Eq. (C.3) would lead to a much more complicated mixture of gamma
distributions. The addition of auxiliary variables sj greatly simpliﬁes the interpretation
of the distribution.
■C.3 Posterior of σ
The derivation of the conditional distribution on σ is similar to that of (α + κ) in
that we have J distributions ψj ∼GEM(σ).
The state-speciﬁc mixture component
index is generated as st ∼ψzt implying that we have nj· total draws from ψj, one
for each occurrence of zt = j. Let K′
j be the number of unique mixture components

Sec. C.4.
Posterior of ρ
231
associated with these draws from ψj. Then, after adding auxiliary variables r′ and s′,
the conditional distributions of σ and these auxiliary variables are:
p(σ | r′, s′, K′
1·, . . . , K′
J·, n1·, . . . , nJ·)
∝(σ)a+K′
··−1−PJ
j=1 s′
je−(σ)(b−PJ
j=1 log r′
j)
(C.18)
p(r′
j | σ, r′
\j, s′, K′
1·, . . . , K′
J·, n1·, . . . , nJ·)
∝
r
′σ
j (1 −r′
j)nj·−1
(C.19)
p(s′
j | σ, r′, s′
\j, K′
1·, . . . , K′
J·, n1·, . . . , nJ·)
∝
nj·
σ
s′
j .
(C.20)
In practice, it is useful to alternate between sampling the auxiliary variables and
concentration parameters α, γ, and σ for several iterations before moving to sampling
the other variables of this model.
■C.4 Posterior of ρ
Finally, we derive the conditional distribution of ρ. We have m·· = P
k m·k total draws
of wjt ∼Ber(ρ), with P
j wj· the number of Bernoulli successes. Here, each success
represents a table’s considered dish being overridden by the house specialty dish. Using
these facts, and the Beta(c, d) prior on ρ, we have
p(ρ | w)
∝
p(w | ρ)p(ρ)
∝
 
m··
P
j wj·
!
ρ
P
j wj·(1 −ρ)m··−P
j wj· Γ(c + d)
Γ(c)Γ(d)ρc−1(1 −ρ)d−1
∝
ρ
P
j wj·+c−1(1 −ρ)m··−P
j wj·+d−1
∝
Beta

X
j
wj· + c, m·· −
X
j
wj· + d

.
(C.21)

232
APPENDIX C. HYPERPARAMETERS

Appendix D
HDP-SLDS and HDP-AR-HMM
Message Passing
In this appendix, we explore the computation of the backwards message passing and
forward sampling scheme used for generating samples of the mode sequence z1:T and
state sequence x1:T in the HDP-AR-HMM and HDP-SLDS models of Chapter 4.
■D.1 Mode Sequence Message Passing for Blocked Sampling
Consider a switching VAR(r) process. To derive the forward-backward procedure for
jointly sampling the mode sequence z1:T given observations y1:T , plus r initial obser-
vations y1−r:0, we ﬁrst note that the chain rule and Markov structure allows us to
decompose the joint distribution as follows:
p(z1:T | y1−r:T , π, θ) = p(zT | zT−1, y1−r:T , π, θ)p(zT−1 | zT−2, y1−r:T, π, θ)
· · · p(z2 | z1, y1−r:T , π, θ)p(z1 | y1−r:T, π, θ).
(D.1)
Thus, we may ﬁrst sample z1 from p(z1 | y1−r:T, π, θ), then condition on this value to
sample z2 from p(z2 | z1, y1−r:T , π, θ), and so on. The conditional distribution of z1 is
derived as:
p(z1 | y1−r:T , π, θ) ∝p(z1)p(y1 | θz1, y1−r:0)
X
z2:T
Y
t
p(zt | πzt−1)p(yt | θzt, yt−r:t−1)
∝p(z1)p(y1 | θz1, y1−r:0)
X
z2
p(z2 | πz1)p(y2 | θz2, y2−r:1)m3,2(z2)
∝p(z1)p(y1 | θz1, y1−r:0)m2,1(z1),
(D.2)
where mt,t−1(zt−1) is the backward message passed from zt to zt−1 and is recursively
deﬁned by:
mt,t−1(zt−1) ∝
( P
zt p(zt | πzt−1)p(yt | θzt, yt−r:t−1)mt+1,t(zt),
t ≤T;
1,
t = T + 1.
(D.3)
233

234
APPENDIX D. HDP-SLDS AND HDP-AR-HMM MESSAGE PASSING
The general conditional distribution of zt is:
p(zt | zt−1, y1−r:T , π, θ) ∝p(zt | πzt−1)p(yt | θzt, yt−r:t−1)mt+1,t(zt).
(D.4)
For the HDP-AR-HMM, these distributions are given by:
p(zt = k | zt−1, y1−r:T , π, θ) ∝πzt−1(k)N(yt;
r
X
i=1
A(k)
i
yt−i, Σ(k))mt+1,t(k)
mt+1,t(k) =
L
X
j=1
πk(j)N(yt+1;
r
X
i=1
A(j)
i yt−i, Σ(j))mt+2,t+1(j)
mT+1,T (k) = 1
k = 1, . . . , L.
(D.5)
■D.2 State Sequence Message Passing for Blocked Sampling
A similar sampling scheme is used for generating samples of the state sequence x1:T .
Although we now have a continuous state space, the computation of the backwards
messages mt+1,t(xt) is still analytically feasible since we are working with Gaussian
densities. Assume, mt+1,t(xt) ∝N −1(xt; θt+1,t, Λt+1,t), where N −1(x; θ, Λ) denotes a
Gaussian distribution on x in information form with mean µ = Λ−1θ and covariance Σ =
Λ−1. Given a ﬁxed mode sequence z1:T , we simply have a time-varying linear dynamic
system. Using similar derivations to those of Sec. 2.7.5, the backwards messages for the
HDP-SLDS can be recursively deﬁned by
mt,t−1(xt−1) ∝
Z
Xt
p(xt|xt−1, zt)p(yt|xt)mt+1,t(xt)dxt.
(D.6)
For this model, the state transition density of Eq. (D.6) can be expressed as
p(xt|xt−1, zt) ∝exp

−1
2(xt −A(zt)xt−1 −µ(zt))T Σ−(zt)(xt −A(zt)xt−1 −µ(zt))

(D.7)
∝exp

−1
2
"
xt−1
xt
#T "
A(zt)T Σ−(zt)A(zt)
−A(zt)T Σ−(zt)
−Σ−(zt)A(zt)
Σ−(zt)
# "
xt−1
xt
#
+
"
xt−1
xt
#T "
−A(zt)T Σ−(zt)µ(zt)
Σ−(zt)µ(zt)
# 
.

Sec. D.2.
State Sequence Message Passing for Blocked Sampling
235
We can similarly write the likelihood in exponentiated quadratic form
p(yt|xt) ∝exp

−1
2(yt −Cxt)T R−1(yt −Cxt)

(D.8)
∝exp

−1
2
"
xt−1
xt
#T "
0
0
0
CTR−1C
# "
xt−1
xt
#
+
"
xt−1
xt
#T "
0
CTR−1yt
# 
,
as well as the messages
mt+1,t(xt) ∝exp

−1
2xT
t Λt+1,txt + xT
t θt+1,t

(D.9)
∝exp

−1
2
"
xt−1
xt
#T "
0
0
0
Λt+1,t
# "
xt−1
xt
#
+
"
xt−1
xt
#T "
0
θt+1,t
# 
.
The product of these quadratics is given by:
p(xt|xt−1, zt)p(yt|xt)mt+1,t(xt) ∝
exp

−1
2
"
xt−1
xt
#T "
A(zt)T Σ−(zt)A
−A(zt)T Σ−(zt)
−Σ−(zt)A(zt)
Σ−(zt) + CTR−1C + Λt+1,t
# "
xt−1
xt
#
+
"
xt−1
xt
#T "
−A(zt)T Σ−(zt)µ(zt)
CT R−1yt + Σ−(zt)µ(zt) + θt+1,t
# 
(D.10)
Using standard Gaussian marginalization identities we integrate over xt to get,
mt,t−1(xt−1) ∝N −1(xt−1; θt,t−1, Λt,t−1),
(D.11)
where,
θt,t−1 = −A(zt)T Σ−(zt)µ(zt) + A(zt)T Σ−(zt)(Σ−(zt) + CT R−1C + Λt+1,t)−1
· (CT R−1yt + Σ−(zt)µ(zt) + θt+1,t)
Λt,t−1 = A(zt)T Σ−(zt)A(zt) −A(zt)T Σ−(zt)(Σ−(zt) + CT R−1C + Λt+1,t)−1Σ−(zt)A(zt).
(D.12)
The backwards message passing recursion is initialized with mT+1,T ∼N −1(xT ; 0, 0).
Let,
Λb
t|t = CTR−1C + Λt+1,t
θb
t|t = CTR−1yt + θt+1,t.
(D.13)

236
APPENDIX D. HDP-SLDS AND HDP-AR-HMM MESSAGE PASSING
Then we can deﬁne the following recursion, which we note is equivalent to a backwards
running Kalman ﬁlter in information form,
Λb
t−1|t−1 = CTR−1C + A(zt)T Σ−(zt)A(zt)
(D.14)
−A(zt)T Σ−(zt)(Σ−(zt) + CT R−1C + Λt+1,t)−1Σ−(zt)A(zt)
= CTR−1C + A(zt)T Σ−(zt)A(zt) −A(zt)T Σ−(zt)(Σ−(zt) + Λb
t|t)−1Σ−(zt)A(zt)
θb
t−1|t−1 = CTR−1yt−1 −A(zt)T Σ−(zt)µ(zt) + A(zt)T Σ−(zt)(Σ−(zt) + CT R−1C + Λt+1,t)−1
· (CT R−1yt + Σ−(zt)µ(zt) + θt+1,t)
= CTR−1yt−1 −A(zt)T Σ−(zt)µ(zt)
+ A(zt)T Σ−(zt)(Σ−(zt) + Λb
t|t)−1(θb
t|t + Σ−(zt)µ(zt))
We initialize at time T with
Λb
T|T = CT R−1C
θb
T|T = CT R−1yT
(D.15)
An equivalent, but more numerically stable recursion is summarized in Algorithm 19.
After computing the messages mt+1,t(xt) backwards in time, we sample the state
sequence x1:T working forwards in time. As with the discrete mode sequence, one can
decompose the posterior distribution of the state sequence as
p(x1:T | y1:T , z1:T , θ) = p(xT | xT−1, y1:T , z1:T , θ)p(xT−1 | xT−2, y1:T , z1:T , θ)
· · · p(x2 | x1, y1:T , z1:T , θ)p(x1 | y1:T, z1:T , θ).
(D.16)
where
p(xt | xt−1, y1:T, z1:T , θ) ∝p(xt | xt−1, A(zt), Σ(zt), µ(zt))p(yt | xt, R)mt+1,t(xt).
(D.17)
For the HDP-SLDS, the product of these distributions is equivalent to
p(xt | xt−1, y1:T , z1:T , θ) ∝N(xt; A(zt)xt−1 + µ(zt), Σ(zt))N(yt; Cxt, R)mt+1,t(xt)
∝N(xt; A(zt)xt−1 + µ(zt), Σ(zt))N −1(xt; θb
t|t, Λb
t|t)
∝N −1(xt; Σ−(zt)(A(zt)xt−1 + µ(zt)) + θb
t|t, Σ−(zt) + Λb
t|t),
(D.18)
which is a simple Gaussian distribution so that the normalization constant is easily
computed. Speciﬁcally, for each t ∈{1, . . . , T} we sample xt from
xt ∼N(xt; (Σ−(zt) + Λb
t|t)−1(Σ−(zt)A(zt)xt−1 + Σ−(zt)µ(zt) + θb
t|t), (Σ−(zt) + Λb
t|t)−1).
(D.19)

Sec. D.3.
Mode Sequence Message Passing for Sequential Sampling
237
1. Initialize ﬁlter with
Λb
T|T = CTR−1C
θb
T|T = CTR−1yT
2. Working backwards in time, for each t ∈{T −1, . . . , 1}:
(a) Compute
˜Jt+1 = Λb
t+1|t+1(Λb
t+1|t+1 + Σ−(zt+1))−1
˜Lt+1 = I −˜Jt+1.
(b) Predict
Λt+1,t = A(zt+1)T (˜Lt+1Λb
t+1|t+1 ˜LT
t+1 + ˜Jt+1Σ−(zt+1) ˜JT
t+1)A(zt+1)
θt+1,t = A(zt+1)T ˜Lt+1(θb
t+1|t+1 −Λb
t+1|t+1µ(zt+1))
(c) Update
Λb
t|t = Λt+1,t + CTR−1C
θb
t|t = θt+1,t + CTR−1yt
3. Set
Λb
0|0 = Λ1,0
θb
0|0 = θ1,0
Algorithm 19. Numerically stable form of the backwards Kalman information ﬁlter.
■D.3 Mode Sequence Message Passing for Sequential Sampling
A similar sampling scheme to Carter and Kohn [26] is used for generating samples of
the mode sequence z1:T having marginalized over the state sequence x1:T . Speciﬁcally,
we sample zt from:
p(zt = k | z\t, y1:T , π, θ) ∝p(zt = k | z\t, π)p(y1:T | zt = k, z\t)
∝πzt−1(k)πk(zt+1)p(y1:T | zt = k, z\t).
(D.20)
We omit the dependency on π and θ for compactness. To compute the likelihood for
each zt, we combine forward and backward messages along with the local dynamics and

238
APPENDIX D. HDP-SLDS AND HDP-AR-HMM MESSAGE PASSING
measurements as follows:
p(y1:T | zt = k, z\t) ∝
Z
Xt−1
Z
Xt
mt−2,t−1(xt−1)p(yt−1 | xt−1)p(xt|xt−1, zt = k)
p(yt|xt)mt+1,t(xt)dxtdxt−1
(D.21)
∝
Z
Xt
Z
Xt−1
mt−2,t−1(xt−1)p(yt−1 | xt−1)p(xt|xt−1, zt = k)dxt−1
p(yt|xt)mt+1,t(xt)dxt,
(D.22)
where the backwards messages are deﬁned as in Appendix D.2 and the forward messages
by:
mt−1,t(xt) ∝
Z
Xt−1
p(xt|xt−1, zt)p(yt−1|xt−1)mt−2,t−1(xt−1)dxt−1.
(D.23)
To derive the forward message passing recursions, assume that
mt−2,t−1(xt−1) ∝N −1(xt−1; θt−2,t−1, Λt−2,t−1)
(D.24)
and zt is known. The terms of the integrand of Eq. (D.23) can be written as:
p(xt|xt−1, zt) = N(xt; A(zt)xt−1 + µ(zt), Σ(zt))
(D.25)
∝exp

−1
2
"
xt
xt−1
#T "
Σ−(zt)
−Σ−(zt)A(zt)
−A(zt)T Σ−(zt)
A(zt)T Σ−(zt)A(zt)
# "
xt
xt−1
#
+
"
xt
xt−1
#T "
Σ−(zt)µ(zt)
−A(zt)T Σ−(zt)µ(zt)
# 
mt−2,t−1(xt−1)p(yt−1|xt−1) ∝N(xt−1; Λ−f
t−1|t−1θf
t−1|t−1, Λf
t−1|t−1)
(D.26)
∝exp

−1
2
"
xt
xt−1
#T "
0
0
0
Λf
t−1|t−1
# "
xt
xt−1
#
+
"
xt
xt−1
#T "
0
θf
t−1|t−1
# 
,
where, similar to the backwards recursions, we have made the following deﬁnitions
θf
t|t = θt−1,t + CT R−1yt
Λf
t|t = Λt−1,t + CTR−1C.
(D.27)
Combining these distributions and integrating over xt−1, we have
mt−1,t(xt) ∝N −1(xt; θt−1,t, Λt−1,t)
(D.28)

Sec. D.3.
Mode Sequence Message Passing for Sequential Sampling
239
with
θt−1,t = Σ−(zt)µ(zt)
(D.29)
+ Σ−(zt)A(zt)(A(zt)T Σ−(zt)A(zt) + Λf
t−1|t−1)−1(θf
t−1|t−1 −A(zt)T Σ−(zt)µ(zt))
Λt−1,t = Σ−(zt) −Σ−(zt)A(zt)(A(zt)T Σ−(zt)A(zt) + Λf
t−1|t−1)−1A(zt)T Σ−(zt),
or equivalently,
θt−1,t = Λt−1,t(µ(zt) + A(zt)Λ−f
t−1|t−1θf
t−1|t−1)
Λt−1,t = (Σ(zt) + A(zt)Λ−f
t−1|t−1A(zt)T )−1.
(D.30)
Assuming x0 ∼N(0, P0), we initialize at time t = 0 to
θ−1,0 = 0
Λ−1,0 = P −1
0 .
(D.31)
An equivalent, but more numerically stable recursion is summarized in Algorithm 20.
However, this algorithm relies on the dynamic matrix A(k) being invertible.
1. Initialize ﬁlter with
Λb
0|0 = P0
θb
0|0 = 0
2. Working forwards in time, for each t ∈{1, . . . , T}:
(a) Compute
Mt = A−(zt+1)T Λ−f
t|t A−(zt+1)
Jt = Mt(Mt + Σ−(zt+1))−1
Lt = I −Jt.
(b) Predict
Λt−1,t = Lt−1Mt−1LT
t−1 + Jt−1Σ−(zt)JT
t−1
θt−1,t = Lt−1A−(zt)T (θf
t−1|t−1 + θf
t−1|t−1A−(zt)µ(zt))
(c) Update
Λf
t|t = Λt−1,t + CTR−1C
θf
t|t = θt−1,t + CTR−1yt
Algorithm 20. Numerically stable form of the forward Kalman information ﬁlter.
We now return to the computation of the likelihood of Eq. (D.22). We note that
the integral over xt−1 is equivalent to computing the message mt−1,t(xt) using zt = k.

240
APPENDIX D. HDP-SLDS AND HDP-AR-HMM MESSAGE PASSING
However, we have to be careful that any constants that were previously ignored in this
message passing are not a function of zt. For the meantime, let us assume that there
exists such a constant and let us denote this special message by
mt−1,t(xt; zt) ∝c(zt)N −1(xt; θt−1,t(zt), Λt−1,t(zt)).
(D.32)
Then, the likelihood can be written as
p(y1:T | zt = k, z\t) ∝
Z
Xt
mt−1,t(xt; zt = k)p(yt|xt)mt+1,t(xt)dxt
(D.33)
∝
Z
Xt
c(k)N −1(xt; θt−1,t(k), Λt−1,t(k))N −1(xt; θb
t|t, Λb
t|t)dxt
(D.34)
Combining the information parameters, and maintaining the term in the normalizing
constant that is a function of k, this is equivalent to
p(y1:T | zt = k, z\t) ∝c(k)|Λt−1,t(k)|1/2 exp

−1
2θt−1,t(k)T Λt−1,t(k)−1θt−1,t(k)

Z
Xt
exp

−1
2xT
t (Λt−1,t(k) + Λb
t|t)xt + xT
t (θt−1,t(k) + θb
t|t)

dxt
(D.35)
To compute this integral, we write the integrand in terms of a Gaussian distribution
times a constant. The integral is then simply that constant term:
p(y1:T | zt = k, z\t) ∝c(k)|Λt−1,t(k)|1/2 exp

−1
2θt−1,t(k)T Λt−1,t(k)−1θt−1,t(k)

|Λt−1,t(k) + Λb
t|t|−1/2 exp
1
2(θt−1,t(k) + θb
t|t)T (Λt−1,t(k) + Λb
t|t)−1(θt−1,t(k) + θb
t|t)

Z
Xt
N −1(xt; θt−1,t(k) + θb
t|t, Λt−1,t(k) + Λb
t|t)dxt
∝c(k)
|Λt−1,t(k)|1/2
|Λt−1,t(k) + Λb
t|t|1/2
exp

−1
2θt−1,t(k)T Λt−1,t(k)−1θt−1,t(k)
+ 1
2(θt−1,t(k) + θb
t|t)T (Λt−1,t(k) + Λb
t|t)−1(θt−1,t(k) + θb
t|t)

Thus,
p(zt = k | z\t, y1:T , π, θ) ∝πzt−1(k)πk(zt+1)c(k)|Λ(k)
t
|1/2|Λ(k)
t
+ Λb
t|t|−1/2
exp

−1
2θ(k)T
t
Λ−(k)
t
θ(k)
t
+ 1
2(θ(k)
t
+ θb
t|t)T (Λ(k)
t
+ Λb
t|t)−1(θ(k)
t
+ θb
t|t)

(D.36)

Sec. D.3.
Mode Sequence Message Passing for Sequential Sampling
241
We now show that c(zt) is not a function zt. The only place where the previously
ignored dependency on zt arises is from p(xt | xt−1, zt). Namely,
p(xt | xt−1, zt) = exp(−1
2µ(zt)T Σ−(zt)µ(zt))
|Σ(zt)|1/2
· exponential1
= c1(zt) · exponential1
(D.37)
where exponential1 is the exponentiated quadratic of Eq. (D.25). Then, when compute
the message mt−1,t(xt; zt) we update the previous message mt−2,t−1(xt−1) by incor-
porating the local likelihood p(yt−1 | xt−1) and then propagating the state estimate
with p(xt | xt−1, zt) and integrating over xt−1. Namely, we combine the distribution of
Eq. (D.37) with the exponentiated quadratic of Eq. (D.26) and integrate over xt−1:
mt−1,t(xt; zt) ∝c1(zt)
Z
Xt−1
exponential1 · exponential2dxt−1,
(D.38)
where exponential2 is the exponentiated quadratic of Eq. (D.26).
Since mt−2,t−1(xt−1) ∝p(xt−1 | y1:t−2, z1:t−1), and the Markov properties of the
state space model dictate
p(xt−1 | y1:t−1, z1:t−1) = p(yt−1|xt−1)p(xt−1 | y1:t−2, z1:t−1)
∝p(yt−1|xt−1)mt−2,t−1(xt−1),
(D.39)
then
p(xt−1 | y1:t−1, z1:t−1) = c2 · exponential2.
We note that the normalizing constant c2 is not a function of zt since we have only
considered zτ for τ < t.
Once again exploiting the conditional independencies induced by the Markov struc-
ture of our state space model, and plugging in Eq. (D.37) and Eq. (D.40),
p(xt, xt−1 | y1:t−1, z1:t) = p(xt−1 | xt−1, zt)p(xt−1 | y1:t−1, z1:t−1)
= (c1(zt) · exponential1)(c2 · exponential2)
= c1(zt)c2 · exponential1 · exponential2.
(D.40)
Plugging this results into Eq. (D.38), we have
mt−1,t(xt; zt) ∝c1(zt)
Z
Xt−1
1
c1(zt)c2
p(xt, xt−1 | y1:t−1, z1:t)dxt−1
∝1
c2
p(xt | y1:t−1, z1:t).
(D.41)
Comparing Eq. (D.41) to Eq. (D.32), and noting that
p(xt | y1:t−1, z1:t) = N −1(xt; θt−1,t(zt), Λt−1,t(zt)),

242
APPENDIX D. HDP-SLDS AND HDP-AR-HMM MESSAGE PASSING
we see that c(zt) = 1
c2 and is thus not a function of zt.
Algebraically, we could derive this result as follows.
mt−1,t(xt; zt) ∝c1(zt)
Z
Xt−1
exponential1 · exponential2dxt−1
(D.42)
= c1(zt)c3(zt)
Z
Xt−1
N
 "
xt−1
xt
#
; Λ(zt)−1θ(zt), Λ(zt)
!
dxt−1,
where θ(zt) and Λ(zt) are the information parameters determined by combining the
functional forms of exponential1 and exponential2, and
c1(zt)c3(zt) = exp{−1
2µ(zt)T Σ−(zt)µ(zt)}
|Σ(zt)|1/2
exp{1
2θ(zt)T Λ(zt)−1θ(zt)}
|Λ(zt)|1/2
.
(D.43)
Computing these terms in parts, and using standard linear algebra properties of block
matrices,
|Λ(zt)| = |Σ−(zt)||(A(zt)T Σ−(zt)A(zt) + Λf
t−1|t−1) −A(zt)T Σ−(zt)A(zt)|
= |Σ−(zt)||Λf
t−1|t−1|
(D.44)
Λ(zt)−1 =
"
(Σ−(zt) −Σ−(zt)A(zt) ˜Λ(zt)−1A(zt)T Σ−(zt))−1
A(zt)Λf
t−1|t−1
Λf
t−1|t−1A(zt)T
(˜Λ(zt) −A(zt)T Σ−(zt)A(zt))−1
#
=
"
Σ(zt) + A(zt)Λ−f
t−1|t−1A(zt)T
A(zt)Λf
t−1|t−1
Λf
t−1|t−1A(zt)T
Λ−f
t−1|t−1
#
,
(D.45)
where ˜Λ(zt) = (A(zt)T Σ−(zt)A(zt) + Λf
t−1|t−1) and we have used the matrix inversion
lemma in obtaining the last equality. Using this form of Λ(zt)−1, we readily obtain
θ(zt)T Λ(zt)−1θ(zt) = µ(zt)Σ−(zt)µ(zt) + θfT
t−1|t−1Λ−f
t−1|t−1θf
t−1|t−1.
(D.46)
Thus,
c1(zt)c3(zt) =
exp{1
2θfT
t−1|t−1Λ−f
t−1|t−1θf
t−1|t−1}
|Λf
t−1|t−1|1/2
,
(D.47)
which does not depend upon the value of zt.

Appendix E
Derivation of Maneuvering Target
Tracking Sampler
In this Appendix we derive the maneuvering target tracking (MTT) sampler outlined
in Sec. 4.3.2. Many of the derivations follow directly from those of Appendice A and D,
and we refer to sections of that appendix as appropriate. Recall the MTT model:
zt ∼πzt−1
xt = Axt−1 + But(zt) + vt
yt = Cxt + wt,
(E.1)
where
ut(k) ∼N(µ(k), Σ(k))
vt ∼N(0, Q)
wt ∼N(0, R).
(E.2)
As described in Sec. 4.3.2, we are interested in jointly sampling the control input
and dynamical mode (ut, zt), marginalizing over the state sequence x1:T , the transition
distributions π, and the dynamic parameters θ = {µ(k), Σ(k)}.
One can factor the
desired conditional distribution factorizes as,
p(ut, zt|z\t, u\t, y1:T , β, α, κ, λ) = p(zt|z\t, u\t, y1:T , β, α, κ, λ)p(ut|z1:T , u\t, y1:T, λ).
(E.3)
The distribution in Eq.(E.3) is a hybrid distribution: each discrete value of the dynam-
ical mode indicator variable zt corresponds to a diﬀerent continuous distribution on
the control input ut. We analyze each of the conditional distributions of Eq. (E.3) by
considering the joint distribution on all of the model parameters, and then marginaliz-
ing x1:T , π, and θk. (Note that marginalization over θj for j ̸= k simply results in a
constant.)
p(zt = k|z\t, u\t, y1:T , β, α, κ, λ) ∝
Z
π
Y
j
p(πj|β, α, κ)
Y
τ
p(zτ|πzτ−1)dπ
Z
Ut
Z
p(θk | λ)
Y
τ|zτ =k
p(uτ|θk)dθk
Z
X
Y
τ
p(xτ|xτ−1, uτ)p(yτ|xτ)dx1:T dut.
(E.4)
243

244
APPENDIX E. DERIVATION OF MANEUVERING TARGET TRACKING SAMPLER
Similarly, we can write the conditional density of ut for each candidate zt as,
p(ut|zt = k, z\t, u\t, y1:T , λ) ∝
Z
p(θk | λ)
Y
τ|zτ =k
p(uτ|θk)dθk
Z
X
Y
τ
p(xτ|xτ−1, uτ)p(yτ|xτ)dx1:T .
(E.5)
In the following sections, we evaluate each of these integrals in turn.
■E.1 Chinese Restaurant Franchise
The integration over π appearing in the ﬁrst line of Eq. (E.4) results in exactly the
same predictive distribution as Eq. (A.10) of the sticky HDP-HMM. Namely,
p(zt = k | z\t, β, α, κ)
∝









(αβk + n−t
zt−1k + κδ(zt−1, k))

αβzt+1+n−t
kzt+1+κδ(k,zt+1)+δ(zt−1,k)δ(k,zt+1)
α+n−t
k· +κ+δ(zt−1,k)

k ∈{1, . . . , K}
α2β˜kβzt+1
α+κ
k = K + 1.
(E.6)
■E.2 Normal-Inverse-Wishart Posterior Update
The marginalization of θk, appearing both in Eq. (E.4) and Eq. (E.5), can be rewritten
as follows:
Z
p(θk|λ)
Y
τ|zτ =k
p(uτ|θk)dθk =
Z
p(ut|θk)p(θk|λ)
Y
τ|zτ =k,τ̸=t
p(uτ|θk)dθk
∝
Z
p(ut|θk)p(θk|{uτ|zτ = k, τ ̸= t}, λ)dθk
= p(ut|{uτ|zτ = k, τ ̸= t}, λ).
(E.7)
Here, the set {uτ|zτ = k, τ ̸= t} denotes all the observations uτ other than ut that were
drawn from the Gaussian parameterized by θk. When θk has a normal-inverse-Wishart
prior NIW(κ, ϑ, ν, ∆), we use the results of Sec. 2.4.3 to derive that
p(ut|{uτ|zτ = k, τ ̸= t}, κ, ϑ, ν, ∆) ≃N

ut; ¯ϑ,
(¯κ + 1)¯ν
¯κ(¯ν −d −1)
¯∆

≜N(ut; ˆµk, ˆΣk),
(E.8)

Sec. E.3.
Marginalization by Message Passing
245
where
¯κ = κ + |{us|zs = k, s ̸= t}|
¯ν = ν + |{us|zs = k, s ̸= t}|
¯κ¯ϑ = κϑ +
X
us∈{us|zs=k,s̸=t}
us
¯ν ¯∆= ν∆+
X
us∈{us|zs=k,s̸=t}
usuT
s + κϑϑT −¯κ¯ϑ¯ϑT
(E.9)
Here, we are using the moment-matched Gaussian approximation to the Student-t pre-
dictive distribution for ut induced by marginalizing θk.
■E.3 Marginalization by Message Passing
When considering the control input ut and conditioning on the values of all uτ, τ ̸= t,
the marginalization over all states x1:T can be equated to a message passing scheme
that relies on the conditionally linear dynamical system induced by ﬁxing uτ, τ ̸= t.
Speciﬁcally,
Z
X
Y
τ
p(xτ|xτ−1, uτ)p(yτ|xτ)dx
∝
Z
Xt−1
Z
Xt
mt−1,t−2(xt−1)p(yt−1|xt−1)p(xt|xt−1, ut)p(yt|xt)mt,t+1(xt)dxtdxt−1
∝p(y1:T |ut; u\t),
(E.10)
where we recall the deﬁnitions of the forward messages mt−1,t(xt) and backward mes-
sages mt+1,t(xt) from Appendix D.2. For our MTT model of Eq. (E.1), however, instead
of accounting for a process noise mean µ(zτ ) at time τ in the ﬁltering equations, we
must account for the control input uτ. Conditioning on uτ, one can equate Buτ with
a process noise mean, and thus we simply replace µ(zτ ) with Buτ in the ﬁltering equa-
tions of Appendix D.2. Similarly, we replace the process noise covariance term Σ(zτ)
with our process noise covariance Q. (Note that although uτ(zτ) ∼N(µ(zτ ), Σ(zτ )), we
condition on the value uτ so that the MTT parameters {µ(zτ ), Σ(zτ )} do not factor into
the message passing equations.)
■E.4 Combining Messages
To compute the likelihood of Eq. (E.10), we take the ﬁltered estimates of xt−1 and
xt, combine them with the local dynamics and local likelihood, and marginalize over
xt−1 and xt. To aid in this computation, we consider the exponentiated quadratic form
of each term in the integrand of Eq. (E.10). We then join these terms and use stan-
dard Gaussian integration formulas to arrive at the desired likelihood. The derivation
of this likelihood greatly parallels that for the sequential mode sequence sampler of
Appendix D.3.

246
APPENDIX E. DERIVATION OF MANEUVERING TARGET TRACKING SAMPLER
Recall the forward ﬁlter recursions of Appendix D.2 in terms of information param-
eters
{θt−1,t, Λt−1,t, θf
t|t, Λf
t|t},
and the backward ﬁlter recursions in terms of
{θt+1,t, Λt+1,t, θb
t|t, Λb
t|t}.
Replace µ(zt) with But and Σ(zt) with Q where appropriate.
We many then write
mt,t+1(xt) updated with the likelihood p(yt−1|xt−1) in exponentiated quadratic form
as:
mt−1,t−2(xt−1)p(yt−1|xt−1)
∝exp

−1
2
"
xt−1
xt
#T "
CT R−1C + Λt−1,t−2
0
0
0
# "
xt−1
xt
#
+
"
xt−1
xt
#T "
CT R−1yt−1 + θt−1,t−2
0
# 
.
The local dynamics can similarly be written as
p(xt|xt−1, ut) ∝exp

−1
2


ut
xt−1
xt


T 

BTQ−1B
BTQ−1A
−BTQ−1
AT Q−1B
AT Q−1A
−ATQ−1
−Q−1B
−Q−1A
Q−1




ut
xt−1
xt


+


ut
xt−1
xt


T 

0
0
0



.
Finally, the backward message mt,t+1(xt) updated with the likelihood p(yt|xt) can be
written as
p(yt|xt)mt,t+1(xt) ∝exp

−1
2
"
xt−1
xt
#T "
0
0
0
CT R−1C + Λt,t+1
# "
xt−1
xt
#
+
"
xt−1
xt
#T "
0
CT R−1yt + θt,t+1
# 
.
Using the deﬁnitions
Λb
t|t = CTR−1C + Λt+1,t
θb
t|t = CTR−1yt + θt+1,t
Λf
t|t = CTR−1C + Λt−1,t
θf
t|t = CTR−1yt + θt−1,t,

Sec. E.4.
Combining Messages
247
we may express the entire integrand as
mt−1,t−2(xt−1)p(yt−1|xt−1)p(xt|xt−1, ut)p(yt|xt)mt,t+1(xt) ∝
exp

−1
2


ut
xt−1
xt


T 

BT Q−1B
BT Q−1A
−BTQ−1
AT Q−1B
AT Q−1A + Λf
t−1|t−1
−AT Q−1
−Q−1B
−Q−1A
Q−1 + Λb
t|t




ut
xt−1
xt


+


ut
xt−1
xt


T 

0
θf
t−1|t−1
θb
t|t



Integrating over xt, we obtain an expression proportional to
N −1
 "
uT
t
xt−1
#
; θ
 "
ut
xt−1
#!
, Λ
 "
ut
xt−1
#!!
,
with
Λ
 "
ut
xt−1
#!
=
"
BT Q−1B
BTQ−1A
AT Q−1B
AT Q−1A + Λf
t−1|t−1
#
−
"
BT Q−1
AT Q−1
#
(Q−1 + Λb
t|t)−1 h
Q−1B
Q−1A
i
=
"
BT Σ−1
t B
BTΣ−1
t A
AT Σ−1
t B
AT Σ−1
t A
#
θ
 "
ut
xt−1
#!
=
"
0
θf
t−1|t−1
#
+
"
BT Q−1
AT Q−1
#
(Q−1 + Λb
t|t)−1θb
t|t
=
"
BT Q−1K−1
t
θb
t|t
θf
t−1|t−1 + AT Q−1K−1
t
θb
t|t
#
.
Here, we have deﬁned
Σt = Q−1 + Q−1(Q−1 + Λb
t|t)−1Q−1 = Q−1 + Q−1K−1
t
Q−1.
Finally, integrating over xt−1 yields an expression proportional to
N −1(uT
t ; θ(ut), Λ(ut)),
with
Λ(ut) = BTΣ−1
t B −BTΣ−1
t A(AT Σ−1
t A + Λf
t−1|t−1)−1AT Σ−1
t B
θ(ut) = BTQ−1K−1
t
θb
t|t
−BTΣ−1
t A(AT Σ−1
t A + Λf
t−1|t−1)−1(θf
t−1|t−1 + AT Q−1K−1
t
θb
t|t).

248
APPENDIX E. DERIVATION OF MANEUVERING TARGET TRACKING SAMPLER
■E.5 Joining Distributions that Depend on ut
We have derived two terms which depend on ut: a prior and a likelihood. Normally,
one would consider p(ut|θk) the prior on ut. However, through marginalization of this
parameter, we induced dependencies between the control inputs uτ and all the uτ that
were drawn from a distribution parameterized by θk inform us of the distribution over
ut. Therefore, we treat p(ut|{uτ|zτ = k, τ ̸= t}) as a prior distribution on ut. The
likelihood function p(y1:T |ut; u\t) describes the likelihood of an observation sequence
y1:T given the input sequence u1:T , containing the random variable is ut.
We multiply the prior distribution by the likelihood function to get the following
quadratic expression:
p(ut|{uτ|zτ = k, τ ̸= t})p(y1:T |ut; u\t)
∝
1
(2π)N/2|ˆΣk|1/2 exp

−1
2(ut −ˆµk)T ˆΣ−1
k (ut −ˆµk)
−1
2(ut −Λ(ut)−1θ(ut))T Λ(ut)(ut −Λty−1θ(ut))

=
1
(2π)N/2|ˆΣk|1/2 exp

−1
2

uT
t (ˆΣ−1
k
+ Λ(ut))ut −2uT
t (ˆΣ−1
k ˆµk
+ θ(ut)) + ˆµT
k ˆΣ−1
k ˆµk + θ(ut)T Λ(ut)−1θ(ut)

= (2π)N/2|(ˆΣ−1
k
+ Λ(ut))−1|1/2
(2π)N/2|ˆΣk|1/2
exp

−1
2

ˆµT
k ˆΣ−1
k ˆµk + θ(ut)T Λ(ut)−1θ(ut)
−(ˆΣ−1
k ˆµk + θ(ut))T (ˆΣ−1
k
+ Λ(ut))−1(ˆΣ−1
k ˆµk + θ(ut))

· N(ut; (ˆΣ−1
k
+ Λ(ut))−1(ˆΣ−1
k ˆµk + θ(ut)), (ˆΣ−1
k
+ Λ(ut))−1)
≜Ck · N(ut; (ˆΣ−1
k
+ Λ(ut))−1(ˆΣ−1
k ˆµk + θ(ut)), (ˆΣ−1
k
+ Λ(ut))−1),
(E.11)
where we note that the deﬁned constant Ck is a function of zt = k, but not of ut.
■E.6 Resulting (ut, zt) Sampling Distributions
We write Eq. (E.4) and Eq. (E.5) in terms of the derived distributions:
p(zt = k|z\t, u\t, y1:T , β, α, κ, λ) ∝p(zt = k | z\t, β, α, κ)
Z
Ut
p(ut|{uτ|zτ = k, τ ̸= t})p(y1:T |ut; u\t)dut,
(E.12)
p(ut|zt = k, z\t, u\t, y1:T , λ) ∝p(ut|{uτ|zτ = k, τ ̸= t})p(y1:T |ut; u\t).
(E.13)

Sec. E.6.
Resulting (ut, zt) Sampling Distributions
249
Thus, the distribution over zt, marginalizing ut, is given by
p(zt = k|z\t, u\t, y1:T , β, α, κ, λ)
∝p(zt = k | z\t, β, α, κ)
Z
Ut
Ck · N(ut; (ˆΣ−1
k
+ Λ(ut))−1(ˆΣ−1
k ˆµk + θ(ut)), (ˆΣ−1
k
+ Λ(ut))−1)dut
∝Ck · p(zt = k | z\t, β, α, κ).
(E.14)
and the distribution over ut (for zt = k ﬁxed) is
p(ut|zt = k, z\t, u\t, y1:T , λ)
= N(ut; (ˆΣ−1
k
+ Λ(ut))−1(ˆΣ−1
k ˆµk + θ(ut)), (ˆΣ−1
k
+ Λ(ut))−1).
(E.15)

250
APPENDIX E. DERIVATION OF MANEUVERING TARGET TRACKING SAMPLER

Appendix F
Dynamic Parameter Posteriors
In this appendix, we derive the posterior distribution over the dynamic parameters of
a switching VAR(r) process deﬁned as follows:
yt =
r
X
i=1
A(zt)
i
yt−i + et(zt)
et(k) ∼N(µ(k), Σ(k)),
(F.1)
where zt indexes the mode-speciﬁc VAR(r) process at time t. Assume that the state
sequence {z1, . . . , zT } is known and we wish to compute the posterior distribution of
the kth mode’s VAR(r) parameters A(k)
i
for i = 1, . . . , r and Σ(k). Let {t1, . . . , tNk} =
{t|zt = k}. Then, we may write
h
yt1
yt2
. . .
ytNk
i
=
h
A(k)
1
A(k)
2
. . .
A(k)
r
i


yt1−1
yt2−1
. . .
ytNk −1
yt1−2
yt2−2
. . .
ytNk −2
...
yt1−r
yt2−r
. . .
ytNk −r


+
h
et1
et2
. . .
etNk
i
.
(F.2)
We deﬁne the following notation for Eq. (F.2):
Y(k) = A(k) ¯Y(k) + E(k),
(F.3)
and let D(k) = {Y(k), ¯Y(k)}. In the following sections, we consider two possible priors
on the dynamic parameter. In Appendix F.1, we assume that µ(k) is 0 for all k and
consider the conjugate matrix-normal inverse-Wishart (MNIW) prior for {A(k), Σ(k)}.
In Appendix F.2, we consider the more general form of Eq. (F.1) and take independent
priors on A(k), Σ(k), and µ(k).
■F.1 Conjugate Prior — MNIW
To show conjugacy, we place a MNIW prior on the dynamic parameters {A(k), Σ(k)}
and show that the posterior remains MNIW given a set of data from the model of
251

252
APPENDIX F. DYNAMIC PARAMETER POSTERIORS
Eq. (F.1) (assuming µ(k) = 0). The MNIW prior is given by placing a matrix-normal
prior MN
 A(k); M, Σ(k), K

on A(k) given Σ(k) (see Eq. (2.94)):
p(A(k) | Σ(k)) =
|K|d/2
|2πΣ(k)|m/2 exp

−1
2tr((A −M)T Σ−(k)(A −M)K)

(F.4)
and an inverse-Wishart prior IW(n0, S0) on Σ(k) (see Eq. (2.95)):
p(Σ(k)) = |S0|n0/2|Σ(k)|−(d+n0+1)/2
2n0d/2Γd(n0/2)
exp

−1
2tr(Σ−(k)S0)

(F.5)
where Γd(·) is the multivariate gamma function and B−(k) denotes (B(k))−1 for some
matrix B.
We ﬁrst analyze the likelihood of the data, D(k), given the kth mode’s dynamic
parameters, {A(k), Σ(k)}. Starting with the fact that each observation vector, yt, is
conditionally Gaussian given the lag observations, ¯yt = [yT
t−1 . . . yT
t−r]T , we have
p(D(k)|A(k), Σ(k)) =
1
|2πΣ(k)|Nk/2 exp
 
−1
2
X
i
(yti −A(k)¯yti)T Σ−(k)(yti −A(k)¯yti)
!
=
1
|2πΣ(k)|Nk/2 exp

−1
2tr(Σ−(k)(Y(k) −A(k) ¯Y(k))(Y(k) −A(k) ¯Y(k))T )

=
1
|2πΣ(k)|Nk/2 exp

−1
2tr((Y(k) −A(k) ¯Y(k))T Σ−(k)(Y(k) −A(k) ¯Y(k))I)

= MN

Y(k); A(k) ¯Y(k), Σ(k), I

.
(F.6)
To derive the posterior of the dynamic parameters, it is useful to ﬁrst compute
p(D(k), A(k) | Σ(k)) = p(D(k) | A(k), Σ(k))p(A(k) | Σ(k)).
(F.7)
Using the fact that both the likelihood p(D(k) | A(k), Σ(k)) and the prior p(A(k) | Σ(k))
are matrix-normally distributed sharing a common parameter Σ(k), we have
log p(D(k), A(k) | Σ(k)) + C
= −1
2tr((Y(k) −A(k) ¯Y(k))T Σ−(k)(Y(k) −A(k) ¯Y(k))
+ (A(k) −M)T Σ−(k)(A(k) −M)K)
= −1
2tr(Σ−(k){(Y(k) −A(k) ¯Y(k))(Y(k) −A(k) ¯Y(k))T
+ (A(k) −M)K(A(k) −M)T })
= −1
2tr(Σ−(k){A(k)S(k)
¯y¯y A(k)T −2S(k)
y¯y A(k)T + S(k)
yy })
= −1
2tr(Σ−(k){(A(k) −S(k)
y¯y S−(k)
¯y¯y
)S(k)
¯y¯y (A(k) −S(k)
y¯y S−(k)
¯y¯y
)T + S(k)
y|¯y}), (F.8)

Sec. F.1.
Conjugate Prior — MNIW
253
where we have used the deﬁnitions:
C = −log
1
|2πΣ(k)|Nk/2
|K|d/2
|2πΣ(k)|rNk/2
S(k)
y|¯y = S(k)
yy −S(k)
y¯y S−(k)
¯y¯y
S(k)T
y¯y
,
S(k)
¯y¯y = ¯Y(k) ¯Y(k)T + K
S(k)
y¯y = Y(k) ¯Y(k)T + MK
S(k)
yy = Y(k)Y(k)T + MKMT .
Conditioning on the noise covariance Σ(k), we see that the dynamic matrix posterior
is given by:
p(A(k) | D(k), Σ(k)) ∝exp

−1
2tr((A(k) −S(k)
y¯y S−(k)
¯y¯y
)T Σ−(k)(A(k) −S(k)
y¯y S−(k)
¯y¯y
)S(k)
¯y¯y )

= MN

A(k); S(k)
y¯y S−(k)
¯y¯y
, Σ−(k), S(k)
¯y¯y

.
(F.9)
Marginalizing Eq. (F.8) over the dynamic matrix A(k), we derive
p(D(k) | Σ(k)) =
Z
A(k) p(D(k), A(k) | Σ(k))dA(k)
=
Z
A(k)
|Kd/2|
|2πΣ(k)|Nk/2|2πΣ(k)|rNk/2
exp

−1
2tr(Σ−(k)(A(k) −S(k)
y¯y S−(k)
¯y¯y
)S(k)
¯y¯y (A(k) −S(k)
y¯y S−(k)
¯y¯y
)T )

exp

−1
2tr(Σ−(k)S(k)
y|¯y)

dA(k)
=
|K|d/2
|2πΣ(k)|Nk/2 exp

−1
2tr(Σ−(k)S(k)
y|¯y)

Z
A(k)
1
|S(k)
¯y¯y |d/2 MN

A(k); S(k)
y¯y S−(k)
¯y¯y
, Σ−(k), S(k)
¯y¯y

dA(k)
=
|K|d/2
|2πΣ(k)|Nk/2|S(k)
¯y¯y |d/2 exp

−1
2tr(Σ−(k)S(k)
y|¯y)

,
(F.10)
which leads us to our ﬁnal result of the covariance having an inverse-Wishart marginal
posterior distribution:
p(Σ(k) | D(k)) ∝p(D(k) | Σ(k))p(Σ(k))
∝
|K|d/2
|2πΣ(k)|Nk/2|S(k)
¯y¯y |d/2
exp

−1
2tr(Σ−(k)S(k)
y|¯y)

|Σ(k)|−(d+n0+1)/2 exp

−1
2tr(Σ−(k)S0)

∝|Σ(k)y|−(d+Nk+n0+1)/2 exp

−1
2tr(Σ−(k)(S(k)
y|¯y + S0))

= IW(Nk + n0, S(k)
y|¯y + S0).
(F.11)

254
APPENDIX F. DYNAMIC PARAMETER POSTERIORS
■F.2 Non-Conjugate Independent Priors on A(k), Σ(k), and µ(k)
In this section, we provide the derivations for the posterior distributions of A(k), Σ(k),
and µ(k) when each of these parameters is given an independent prior.
■F.2.1 Normal Prior on A(k)
Assume we place a Gaussian prior, N(µA, ΣA), on the vectorization of the matrix A(k),
which we denote by vec(A(k)). To examine the posterior distribution, we ﬁrst aim to
write the data as a linear function of vec(A(k)). We may rewrite Eq. (F.1) as
yt = A(k)


yt−1
yt−2
...
yt−r


+ et
∀t|zt = k
≜A(k)¯yt + et(k).
(F.12)
Recalling that r is the autoregressive order and d the dimension of the observation
vector yt, we can equivalently represent the above as
yt = et(k)
+


¯yt,1
¯yt,2
· · ·
¯yt,d∗r
0
0
· · ·
0
0
0
· · ·
0
0
0
· · ·
0
¯yt,1
¯yt,2
· · ·
¯yt,d∗r
0
0
· · ·
0
...
...
...
...
...
...
...
...
...
...
...
...
0
0
· · ·
0
0
0
· · ·
0
¯yt,1
¯yt,2
· · ·
¯yt,d∗r




a(k)
1,1
a(k)
1,2
...
a(k)
1,d∗r
a(k)
2,1
a(k)
2,2
...
a(k)
d,d∗r


=
h
¯yt,1Id
¯yt,2Id
· · ·
¯yt,d∗rId
i
vec(A(k)) + et(k) ≜¯Y tvec(A) + et(k).
(F.13)
Here, the columns of ¯yt are permutations of those of the matrix in the ﬁrst line such
that we may write yt as a function of vec(A(k)). Noting that et(k) ∼N(µ(k), Σ(k)),
log p(D(k), A(k) | Σ(k), µ(k))
= C −1
2
X
t|zt=k
(yt −µ(k) −¯Y tvec(A(k)))T Σ−(k)(yt −µ(k) −¯Y tvec(A(k)))
−1
2(vec(A(k)) −mA)T Σ−1
A (vec(A(k)) −mA),
(F.14)

Sec. F.2.
Non-Conjugate Independent Priors on A(k), Σ(k), and µ(k)
255
which can be rewritten as,
log p(D(k), A(k) | Σ(k), µ(k)) = C −1
2vec(A(k))T

Σ−1
A +
X
t|zt=k
¯Y T
t Σ−(k) ¯Y t

vec(A(k))
+ vec(A(k))T

Σ−1
A mA +
X
t|zt=k
¯Y T
t Σ−(k)(yt −µ(k))


−1
2mT
AΣ−1
A mA −1
2
X
t|zt=k
(yt −µ(k))T Σ−(k)(yt −µ(k))
(F.15)
Conditioning on the data, we arrive at the desired posterior distribution
log p(A(k) | D(k), Σ(k), µ(k)) = C −1
2

vec(A(k))T (Σ−1
A +
X
t|zt=k
¯Y T
t Σ−(k) ¯Y t)vec(A(k))
−2vec(A(k))T (Σ−1
A mA +
X
t|zt=k
¯Y T
t Σ−(k)(yt −µ(k)))

= N −1

Σ−1
A mA +
X
t|zt=k
¯Y T
t Σ−(k)(yt −µ(k)), Σ−1
A +
X
t|zt=k
¯Y T
t Σ−(k) ¯Y t


(F.16)
■F.2.2 Inverse Wishart Prior on Σ(k)
We place an inverse-Wishart prior, IW(n0, S0), on Σ(k).
Let Nk = |{t|zt = k, t =
1, 2, . . . , T}|. Conditioned on A(k) and µ(k), the standard conjugate prior results pre-
sented in Sec. 2.4.3 imply that the posterior of Σ(k) is:
p(Σ(k) | D(k), A(k), µ(k)) =
IW

Nk + n0, S +
X
t|zt=k
(yt −A(k)¯yt −µ(k))(yt −A(k)¯yt −µ(k))T

.
(F.17)
■F.2.3 Normal Prior on µ(k)
Finally, we place a Gaussian prior, N(µ0, Σ0), on µ(k). Conditioned on A(k) and Σ(k),
the results of Sec. 2.4.3 provide that the posterior of µ(k) is:
p(µ(k) | D(k), A(k), Σ(k)) =
N −1

µ(k); Σ−1
0 µ0 + Σ−(k) X
t|zt=k
(yt −A(k)¯yt), Σ−1
0
+ NkΣ−(k)

.
(F.18)

256
APPENDIX F. DYNAMIC PARAMETER POSTERIORS
We iterate between sampling A(k), Σ(k), and µ(k) many times before moving on to the
next step of the Gibbs sampler.

Bibliography
[1] M. Abramowitz and I. Stegun. Handbook of Mathematical Functions with Formu-
las, Graphs, and Mathematical Tables, chapter 24.1.3, page 824. Models of Neural
Networks, III. Dover, 9 edition, 1972.
[2] S.M. Aji and R.J. McEliece. The generalized distributive law. IEEE Transactions
on Information Theory, 46(2):325–343, 2000.
[3] H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions
on Automatic Control, 19(6):716–723, 1974.
[4] B.D.O. Anderson. The realization problem for hidden Markov models. Mathe-
matics of Control, Signals, and Systems, 12:80–120, 1999.
[5] C.E. Antoniak.
Mixtures of Dirichlet processes with applications to Bayesian
nonparametric problems. The Annals of Statistics, 2(6):1152–1174, 1974.
[6] M. Aoki and A. Havenner. State space modeling of multiple time series. Econo-
metric Reviews, 10(1):1–59, 1991.
[7] J. Barbiˇc, A. Safonova, J.-Y. Pan, C. Faloutsos, J.K. Hodgins, and N.S. Pol-
lard. Segmenting motion capture data into distinct behaviors. In Proc. Graphics
Interface, pages 185–194, 2004.
[8] O. Barndorﬀ-Nielsen. Information and Exponential Families. John Wiley, 1978.
[9] M.J. Beal. Variational Algorithms for Approximate Bayesian Inference. Ph.D.
thesis, University College London, London, UK, 2003.
[10] M.J. Beal and P. Krishnamurthy. Gene expression time course clustering with
countably inﬁnite hidden Markov models. In Proc. Conference on Uncertainty in
Artiﬁcial Intelligence, 2006.
[11] M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. The inﬁnite hidden Markov
model. In Advances in Neural Information Processing Systems, volume 14, pages
577–584, 2002.
257

258
BIBLIOGRAPHY
[12] J.O. Berger and J.M. Bernardo.
On the development of reference priors.
In
J.M. Bernardo, J.O. Berger, D.V. Lindley, and A.F.M. Smith, editors, Bayesian
Statistics 4, pages 35–60. Oxford University Press, 1992.
[13] J.M. Bernardo. Reference analysis. In D.K. Dey and C.R. Rao, editors, Handbook
of Statistics 25, pages 17–90. Elsevier, 2005.
[14] J.M. Bernardo. Reference posterior distributions for Bayesian inference. Journal
of the Royal Statistical Society B, 41:113–147, 1979.
[15] J.M. Bernardo and A.F.M. Smith. Bayesian Theory. Wiley, 2000.
[16] D.P. Bertsekas and J.N. Tsitsikilis. Introduction to Probability. Kluwer Academic,
1996.
[17] R. Bhar and S. Hamori. Hidden Markov models: applications to ﬁnancial eco-
nomics. Kluwer Academic Publishers, 2004.
[18] D. Blackwell and J.B. MacQueen. Ferguson distributions via Polya urn schemes.
The Annals of Statistics, 1(2):353–355, 1973.
[19] D. Blei, T. Griﬃths, M. Jordan, and J. Tenenbaum. Hierarchical topic models
and the nested Chinese restaurant process. In Advances in Neural Information
Processing Systems, volume 16, 2004.
[20] H. Blom and Y. Bar-Shalom. The interacting multiple model algorithm for sys-
tems with Markovian switching coeﬃcients. IEEE Transactions on Automatic
Control, 33(8):780–783, 1988.
[21] P. Br´emaud. Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues.
Springer-Verlag, New York, 1999.
[22] P. Buhlmann and J.W. Abraham. Variable length Markov chains. The Annals of
Statistics, 27(2):480–513, 1999.
[23] F. Caron and A. Doucet. Sparse Bayesian nonparametric regression. In Proc.
International Conference on Machine Learning, July 2008.
[24] F. Caron, M. Davy, A. Doucet, E. Duﬂos, and P. Vanheeghe. Bayesian infer-
ence for dynamic models with Dirichlet process mixtures. In Proc. International
Conference on Information Fusion, July 2006.
[25] C.K. Carter and R. Kohn. On Gibbs sampling for state space models. Biometrika,
81(3):541–553, 1994.
[26] C.K. Carter and R. Kohn. Markov chain Monte Carlo in conditionally Gaussian
state space models. Biometrika, 83:589–601, 3 1996.

BIBLIOGRAPHY
259
[27] C.M. Carvalho and H.F. Lopes. Simulation-based sequential analysis of Markov
switching stochastic volatility models. Computational Statistics & Data Analysis,
51:4526–4542, 9 2007.
[28] C.M. Carvalho, J.E. Lucas, Q. Wang, J. Chang, J.R. Nevins, and M. West. High-
dimensional sparse factor modelling - Applications in gene expression genomics.
Journal of the American Statistical Association, 103, 2008.
[29] G. Casella and C. Robert. Rao-Blackwellisation of sampling schemes. Biometrika,
83(1):81–94, 1996.
[30] O.L.V. Costa, M.V. Fragoso, and R.P. Marques. Discrete-Time Markov Jump
Linear Systems. Springer, 2005.
[31] T.M. Cover and J.A. Thomas. Elements of Information Theory. John Wiley &
Sons, 2006.
[32] G. Darmois. Sur les lois de probabilite a estimation exhaustive. C. R. Acad. Sci.
Paris, 260:1265–1266, 1935.
[33] B. de Finetti. Funzione Caratteristica Di un Fenomeno Aleatorio, pages 251–
299. 6. Memorie, Classe di Scienze Fisiche, Mathematiche e Naturali. Academia
Nazionale dei Lincei, 1931.
[34] P. de Jonga and J. Penzerb. The ARMA model in state space form. Statistics &
Probability Letters, 70:119–125, 2004.
[35] P. Diaconis and D. Freedman. On the consistency of Bayes estimates. The Annals
of Statistics, pages 1–26, 1986.
[36] J. Diebolt and C.P. Robert. Estimation of ﬁnite mixture distributions through
Bayesian sampling. Journal of the Royal Statistical Society. Series B, pages 363–
375, 1994.
[37] D. Dunson.
Nonparametric Bayes local partition models for random eﬀects.
Biometrika, 96(2):249–262, 2009.
[38] D. Dunson. Multivariate kernel partition process mixtures. To appear in Statistica
Sinica.
[39] D. Dunson and J.-H. Park. Kernel stick-breaking processes. Biometrika, 95(2):
307–323, 2008.
[40] M.D. Escobar and M. West.
Bayesian density estimation and inference using
mixtures. Journal of the American Statistical Association, 90(430):577–588, 1995.
[41] T.S. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals
of Statistics, 1(2):209–230, 1973.

260
BIBLIOGRAPHY
[42] G.D. Forney. The Viterbi algorithm. Proceedings of the IEEE, 61(3):268–278,
1973.
[43] E.B. Fox, E.B. Sudderth, and A.S. Willsky. Hierarchical Dirichlet processes for
tracking maneuvering targets. In Proc. International Conference on Information
Fusion, July 2007.
[44] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. Nonparametric Bayesian
learning of switching dynamical systems.
In Advances in Neural Information
Processing Systems, volume 21, pages 457–464, 2009.
[45] A. Frigessi, P. Di Stefano, C.R. Hwang, and S.J. Sheu. Convergence rates of the
Gibbs sampler, the Metropolis algorithm and other single-site updating dynamics.
Journal of the Royal Statistical Society, Series B, pages 205–219, 1993.
[46] R.G. Gallager. Discrete Stochastic Processes. Athena Scientiﬁc, 2002.
[47] R. Garcia and P. Perron. An analysis of real interest rate under regime shifts.
Review of Economics & Statistics, 78(1):111–125, 1996.
[48] J. Gasthaus, F. Wood, D. Gorur, and Y.W. Teh. Dependent Dirichlet process
spike sorting. In Advances in Neural Information Processing Systems, volume 21,
pages 497–504, 2009.
[49] A.E. Gelfand and S. Sahu. Gibbs sampling, identiﬁability and improper priors
in generalized linear mixed models. Journal American Statistical Association, 94:
247–253, 1999.
[50] A. Gelman and D.B. Rubin. Inference from iterative simulation using multiple
sequences. Statistical Science, pages 457–472, 1992.
[51] A. Gelman, J.B. Carlin, H.S. Stern, and D.B. Rubin. Bayesian Data Analysis.
Chapman & Hall, 2004.
[52] Z. Ghahramani and G.E. Hinton. Variational learning for switching state-space
models. Neural Computation, 12(4):831–864, 2000.
[53] S. Ghosal and A. van der Vaart. Posterior convergence rates of Dirichlet mixtures
at smooth densities. Annals of statistics, 35(2):697, 2007.
[54] S. Ghosal, J.K. Ghosh, and R.V. Ramamoorthi. Posterior consistency of Dirichlet
mixtures in density estimation. Annals of Statistics, pages 143–158, 1999.
[55] J.K. Ghosh and R.V. Ramamoorthi. Bayesian nonparametrics. Springer-Verlag,
2003.

BIBLIOGRAPHY
261
[56] W.R. Gilks and C. Berzuini. Following a moving target-Monte Carlo inference for
dynamic Bayesian models. Journal of the Royal Statistical Society, 63(1):127–146,
2002.
[57] W.R. Gilks, S. Richardson, and D.J. Spiegelhalter, editors. Markov Chain Monte
Carlo in Practice. Chapman & Hall, 1996.
[58] S. Goldwater, T. Griﬃths, and M. Johnson. Interpolating between types and
tokens by estimating power-law generators. In Advances in Neural Information
Processing Systems, volume 18, pages 459–466, 2006.
[59] D. G¨or¨ur, F. J¨akel, and C.E. Rasmussen. A choice model with inﬁnitely many
latent features.
In Proc. International Conference on Machine learning, June
2006.
[60] P.J. Green.
Reversible jump Markov chain Monte Carlo computation and
Bayesian model determination. Biometrika, 82(4):711–732, 1995.
[61] J.E. Griﬃn and M.F.J. Steel. Order-based dependent Dirichlet processes. Journal
of the American Statistical Association, 101:179–194, 2006.
[62] T.L. Griﬃths and Z. Ghahramani.
Inﬁnite latent feature models and the In-
dian buﬀet process. Gatsby Computational Neuroscience Unit, Technical Report
#2005-001, 2005.
[63] J.D. Hamilton. A new approach to the economic analysis of nonstationary time
series and the business cycle. Econometrica, 57(2):357–384, 1989.
[64] A.C. Harvey, E. Ruiz, and N. Shephard. Multivariate stochastic variance models.
Review of Economic Studies, 61:247–264, 1994.
[65] D. Heath and W. Sudderth. De Finetti’s theorem on exchangeable variables. The
American Statistician, 30(4):188–189, 1976.
[66] E. Hewitt and L.J. Savage. Symmetric measures on cartesian products. Transac-
tions of the American Mathematical Society, 80(2):470–501, 1955.
[67] N.L. Hjort. Nonparametric Bayes estimators based on beta processes in models
for life history data. The Annals of Statistics, pages 1259–1294, 1990.
[68] M. Hoﬀman, P. Cook, and D. Blei. Data-driven recomposition using the hierar-
chical Dirichlet process hidden Markov model. In Proc. International Computer
Music Conference, 2008.
[69] E. Hsu, K. Pulli, and J. Popovi´c. Style translation for human motion. In SIG-
GRAPH, pages 1082–1089, 2005.

262
BIBLIOGRAPHY
[70] K. Huang, A. Wagner, and Y. Ma. Identiﬁcation of hybrid linear time-invariant
systems via subspace embedding and segmentation SES. In Proc. IEEE Confer-
ence on Decision and Control, December 2004.
[71] A.T. Ihler, J.W. Fisher III, and A.S. Willsky. Loopy belief propagation: Conver-
gence and eﬀects of message errors. Journal of Machine Learning Research, 6:
905–936, 2005.
[72] H. Ishwaran and L. James. Gibbs sampling methods for stick-breaking priors.
Journal of the American Statistical Association, 96(453):161–173, 2001.
[73] H. Ishwaran and J.S. Rao. Spike and slab variable selection: Frequentist and
Bayesian strategies. The Annals of Statistics, 33(2):730–773, 2005.
[74] H. Ishwaran and M. Zarepour. Markov chain Monte Carlo in approximate Dirich-
let and beta two–parameter process hierarchical models. Biometrika, 87(2):371–
390, 2000.
[75] H. Ishwaran and M. Zarepour. Dirichlet prior sieves in ﬁnite normal mixtures.
Statistica Sinica, 12:941–963, 2002.
[76] H. Ishwaran and M. Zarepour. Exact and approximate sum–representations for
the Dirichlet process. Canadian Journal of Statistics, 30:269–283, 2002.
[77] S. Jain and R.M. Neal. A split-merge markov chain monte carlo procedure for
the dirichlet process mixture model. Journal of Computational and Graphical
Statistics, 13:158–182, 2004.
[78] A. Jasra, C.C. Holmes, and D.A. Stephens. Markov chain Monte Carlo methods
and the label switching problem in Bayesian mixture modeling. Statistical Science,
20(1):50–67, 2005.
[79] E.T. Jaynes. Prior probabilities. IEEE Transactions on Systems, Science and
Cybernetics, 4:227–291, 1968.
[80] H. Jeﬀreys. Theory of probability. Oxford University Press, 1998.
[81] H. Jeﬀreys. An invariant form for the prior probability in estimation problems.
Proceedings of the Royal Society of London. Series A, Mathematical and Physical
Sciences, pages 453–461, 1946.
[82] F. Jelinek. Statistical methods for speech recognition. MIT Press, 1998.
[83] M. Johnson. Why doesn’t EM ﬁnd good HMM POS-taggers. In Proc. Joint Con-
ference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, 2007.
[84] I.T. Jolliﬀe. Principal component analysis. Springer, 2002.

BIBLIOGRAPHY
263
[85] M.I. Jordan. Graphical models. Statistical Science (Special Issue on Bayesian
Statistics), 19:140–155, 2004.
[86] M.I. Jordan. Dirichlet processes, Chinese restaurant processes and all that. In
Tutorial presentation at the NIPS Conference, 2005.
[87] V.M. Joshi. On the attainment of the Cram´er-Rao lower bound. The Annals of
Statistics, pages 998–1002, 1976.
[88] B.H. Juang and L.R. Rabiner. Hidden Markov models for speech recognition.
Technometrics, pages 251–272, 1991.
[89] T. Kailath, A.H. Sayed, and B. Hassibi, editors. Linear Estimation. Prentice
Hall, 2000.
[90] R.E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal
of Basic Engineering, 82(1):35–45, 1960.
[91] R.E. Kalman. Lyapunov functions for the problem of Lure in automatic control.
Proceedings of the National Academy of Sciences of the United States of America,
49(2):201–205, 1963.
[92] C. Karlof and D. Wagner. Hidden Markov model cryptanalysis. Lecture Notes in
Computer Science, pages 17–34, 2003.
[93] A.Y. Khinchine. Korrelationstheorie der station¨aren stochastiscen prozesse. Math-
ematische Annalen, 109:604–615, 1934.
[94] C.-J. Kim. Dynamic linear models with Markov-switching. Journal of Economet-
rics, 60:1–22, 1994.
[95] J.F.C. Kingman. Completely random measures. Paciﬁc Journal of Mathematics,
21(1):59–78, 1967.
[96] J.F.C. Kingman. Poisson processes. Oxford University Press, 1993.
[97] J.J. Kivinen, E.B. Sudderth, and M.I. Jordan. Learning multiscale representations
of natural scenes using Dirichlet processes. In Proc. International Conference on
Computer Vision, pages 1–8, 2007.
[98] B. Koopman. On distributions admitting a suﬃcient statistic. Transactions of
the American Mathematical Society, 39:399–409, 1936.
[99] G. Kotsalis, A. Megretski, and M.A. Dahleh. Model reduction of discrete-time
Markov jump linear systems. In Proc. American Control Conference, June 2006.
[100] A. Krogh, M. Brown, I.S. Mian, K. Sjolander, and D. Haussler. Hidden Markov
models in computational biology. applications to protein modeling. Journal of
Molecular Biology, 235(5):1501–1531, 1994.

264
BIBLIOGRAPHY
[101] A. Krogh, B.`E. Larsson, G. Von Heijne, and E.L.L. Sonnhammer. Predicting
transmembrane protein topology with a hidden Markov model: application to
complete genomes. Journal of Molecular Biology, 305(3):567–580, 2001.
[102] K. Kurihara, M. Welling, and Y.W. Teh. Collapsed variational Dirichlet process
mixture models. In Proc. International Joint Conferences on Artiﬁcial Intelli-
gence, 2007.
[103] S.L. Lauritzen. Graphical models. Oxford University Press, USA, 1996.
[104] S.L. Lauritzen and D.J. Spiegelhalter. Local computations with probabilities on
graphical structures and their application to expert systems. Journal of the Royal
Statistical Society, Series B, 50(2):157–224, 1988.
[105] N.
Lawrence.
MATLAB
motion
capture
toolbox.
http://www.cs.man.ac.uk/ neill/mocap/.
[106] P. L´evy, editor. Th´eorie de l’addition des variables al´eatoires. Gauthiers-Villars,
1937.
[107] A. Lindquist and G. Picci. Geometric methods for state space identiﬁcation. In
S. Bittanti and G. Picci, editors, Identiﬁcation, Adaptation, Learning, pages 1–69.
Springer, 1994.
[108] J.S. Liu.
Peskun’s theorem and a modiﬁed discrete-state Gibbs sampler.
Biometrika, 83(3):681–682, 1996.
[109] J.S. Liu, W.H. Wong, and A. Kong. Covariance structure and convergence rate
of the Gibbs sampler with various scans. Journal of the Royal Statistical Society,
Series B, pages 157–169, 1995.
[110] H. L¨utkepohl, editor.
New Introduction to Multiple Time Series Analysis.
Springer, 2005.
[111] S.N. MacEachern. Dependent nonparametric processes. In Proc. Bayesian Statis-
tistical Science Section, pages 50–55, 1998.
[112] D.J.C. MacKay. Bayesian methods for backprop networks, chapter 6, pages 211–
254. Models of Neural Networks, III. Springer, 1994.
[113] D.M. Malioutov, J.K. Johnson, and A.S. Willsky. Walk-sums and belief propaga-
tion in Gaussian graphical models. The Journal of Machine Learning Research,
7:2031–2064, 2006.
[114] B. Marthi, H. Pasula, S. Russell, and Y. Peres. Decayed MCMC ﬁltering. In
Proc. Uncertainty in Artiﬁcial Intelligence, August 2002.

BIBLIOGRAPHY
265
[115] E. Mazor, A. Averbuch, Y. Bar-Shalom, and J. Dayan.
Interacting multiple
model methods in target tracking: A survey. IEEE Transactions on Aerospace
and Electronic Systems, 34(1):103–123, 1998.
[116] L.R. Mead and N. Papanicolaou. Maximum entropy in the problem of moments.
Journal of Mathematical Physics, 25:2404, 1984.
[117] E. Meeds, Z. Ghahramani, R.M. Neal, and S.T. Roweis. Modeling dyadic data
with binary latent factors. Advances in Neural Information Processing Systems,
19:977–984, 2007.
[118] D. Mochihashi and E. Sumita. The inﬁnite Markov model. In Advances in Neural
Information Processing Systems, volume 20, pages 1017–1024, 2008.
[119] R.L. Moose, H.F. VanLandingham, and D.H. McCabe. Modeling and estimation
of tracking maneuvering targets. IEEE Transactions on Aerospace and Electronic
Systems, 15(3):448–456, 1979.
[120] P. M¨uller and F.A. Quintana. Nonparametric Bayesian data analysis. Statistical
Science, 19(1):95–110, 2004.
[121] J. Munkres. Algorithms for the assignment and transportation problems. Journal
of the Society of Industrial and Applied Mathematics, 5(1):32–38, 1957.
[122] K.P. Murphy.
Hidden semi-Markov models (HSMMs).
Informal Notes,
http://www.cs.ubc.ca/ murphyk/Papers/segment.pdf, 2002.
[123] K.P.
Murphy.
Hidden
Markov
model
(HMM)
toolbox
for
MATLAB.
http://www.cs.ubc.ca/ murphyk/Software/HMM/hmm.html.
[124] R.M. Neal, editor. Bayesian Learning for Neural Networks, volume 118 of Lecture
Notes in Statistics. Springer, 1996.
[125] R.M. Neal. Sampling from multimodal distributions using tempered transitions.
Statistics and Computing, 6(4):353–366, 1996.
[126] NIST. Rich transcriptions database. http://www.nist.gov/speech/tests/rt/, 2007.
[127] F.J. Och and H. Ney. A comparison of alignment models for statistical machine
translation. In Proc. Conference on Computational Linguistics, volume 2, pages
1086–1090, 2000.
[128] F.J. Och and H. Ney. The alignment template approach to statistical machine
translation. Computational Linguistics, 30(4):417–449, 2004.
[129] S. Oh, J. Rehg, T. Balch, and F. Dellaert. Learning and inferring motion pat-
terns using parametric segmental switching linear dynamic systems. International
Journal of Computer Vision, 77(1–3):103–124, 2008.

266
BIBLIOGRAPHY
[130] S. Paoletti, A. Juloski, G. Ferrari-Trecate, and R. Vidal. Identiﬁcation of hybrid
systems: A tutorial. European Journal of Control, 2–3:242–260, 2007.
[131] O. Papaspiliopoulos and G.O. Roberts. Retrospective Markov chain Monte Carlo
methods for Dirichlet process hierarchical models. Biometrika, 95:169–186, 2008.
[132] V. Pavlovi´c, J.M. Rehg, T.J. Cham, and K.P. Murphy.
A dynamic Bayesian
network approach to ﬁgure tracking using learned dynamic models.
In Proc.
International Conference on Computer Vision, September 1999.
[133] V. Pavlovi´c, J.M. Rehg, and J. MacCormick.
Learning switching linear mod-
els of human motion. In Advances in Neural Information Processing Systems,
volume 13, 2001.
[134] J. Pearl.
Probabilistic reasoning in intelligent systems: Networks of plausible
inference. Morgan-Kaufmann, San Mateo, CA, 1988.
[135] M. Petreczky and R. Vidal. Realization theory of stochastic jump-Markov linear
systems. In Proc. IEEE Conference on Decision and Control, December 2007.
[136] E.J.G. Pitman.
Suﬃcient statistics and intrinsic accuracy.
Proceedings of the
Cambridge Philosophical Society, 32:567–579, 1936.
[137] J. Pitman and M. Yor. The two-parameter Poisson-Dirichlet distribution derived
from a stable subordinator. Annals of Probability, 25:855–900, 1997.
[138] Z. Psaradakis and N. Spagnolo. Joint determination of the state dimension and
autoregressive order for models with Markov regime switching. Journal of Time
Series Analysis, 27:753–766, 2006.
[139] L.R. Rabiner. A tutorial on hidden Markov models and selected applications in
speech recognition. Proceedings of the IEEE, 77(2):257–286, 1989.
[140] L. Ren, A. Patrick, A. Efros, J. Hodgins, and J. Rehg. A data-driven approach
to quantifying natural human motion. In SIGGRAPH, August 2005.
[141] C.P. Robert. The Bayesian choice. Springer, 2007.
[142] C.P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, 2005.
[143] A. Rodriguez, D.B. Dunson, and A.E. Gelfand. The nested Dirichlet process.
Institute of Statistics and Decision Sciences, Duke University, Technical Report
#06-19., July 2006.
[144] X. Rong Li and V. Jilkov. Survey of maneuvering target tracking. Part I: Dynamic
models. IEEE Transactions on Aerospace and Electronic Systems, 39(4):1333–
1364, 2003.

BIBLIOGRAPHY
267
[145] X. Rong Li and V. Jilkov.
Survey of maneuvering target tracking. Part V:
Multiple-model methods. IEEE Transactions on Aerospace and Electronic Sys-
tems, 41(4):1255–1321, 2005.
[146] C. Ryll-Nardzewski.
On stationary sequences of random variables and the de
Finettis equivalence. Colloq. Math., 4:149–156, 1957.
[147] G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, pages
461–464, 1978.
[148] S.L. Scott. Bayesian methods for hidden Markov models: Recursive computing
in the 21st century.
Journal of the American Statistical Association, 97(457):
337–351, 2002.
[149] J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4:
639–650, 1994.
[150] R.D. Shachter.
Bayes-ball: The rational pastime (for determining irrelevance
and requisite information in belief networks and inﬂuence diagrams). In Proc.
Conference on Uncertainty in Artiﬁcial Intelligence, pages 480–487, 1998.
[151] G.R. Shafer and P.P. Shenoy. Probability propagation. Annals of Mathematics
and Artiﬁcial Intelligence, 2(1):327–351, 1990.
[152] N. Shephard.
Statistical aspects of ARCH and stochastic volatility.
In D.R.
Cox, D.V. Hinkley, and O.E. Barndorﬀ-Nielsen, editors, Time Series Models in
Econometrics, Finance and Other Fields, pages 1–67. Chapman & Hall, 1996.
[153] R.A. Singer. Estimating optimal tracking ﬁlter performance for manned maneu-
vering targets. IEEE Transactions on Aerospace and Electronic Systems, 4(6):
473–483, 1970.
[154] M.K.P So, K. Lam, and W.K. Li. A stochastic volatility model with Markov
switching. Journal of Business & Economic Statistics, 16(2):244–253, 1998.
[155] E.L.L. Sonnhammer, G. von Heijne, and A. Krogh.
A hidden Markov model
for predicting transmembrane helices in protein sequences. In Proc. International
Conference on Intelligent Systems for Molecular Biology, volume 6, pages 175–82,
1998.
[156] N. Srebro and S. Roweis. Time-varying topic models using dependent Dirichlet
processes. UTML, TR #2005-003, March 2005.
[157] E.B. Sudderth. Graphical Models for Visual Object Recognition and Tracking.
Ph.D. thesis, MIT, Cambridge, MA, 2006.

268
BIBLIOGRAPHY
[158] E.B. Sudderth, A. Torralba, W.T. Freeman, and A.S. Willsky. Describing visual
scenes using transformed objects and parts. International Journal of Computer
Vision, 77:244–253, 2008.
[159] G.W. Taylor, G.E. Hinton, and S.T. Roweis.
Modeling human motion using
binary latent variables. Advances in Neural Information Processing Systems, 19:
1345–1352, 2007.
[160] Y.W. Teh. A hierarchical Bayesian language model based on Pitman-Yor pro-
cesses. In Proc. 21st International Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computational Linguistics, 2006.
[161] Y.W. Teh and M.I. Jordan. Hierarchical bayesian nonparametric models with
applications.
In N. Hjort, C. Holmes, P. M¨uller, and S. Walker, editors, To
appear in Bayesian Nonparametrics in Practice. Cambridge University Press.
[162] Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. Hierarchical Dirichlet processes.
Journal of the American Statistical Association, 101(476):1566–1581, 2006.
[163] Y.W. Teh, D. Gorur, and Z. Ghahramani. Stick-breaking construction for the
Indian buﬀet process. In Proc. International Conference on Artiﬁcial Intelligence
and Statistics, volume 11, 2007.
[164] Y.W. Teh, K. Kurihara, and M. Welling. Collapsed variational inference for HDP.
In Advances in Neural Information Processing Systems, volume 20, pages 1481–
1488, 2008.
[165] R. Thibaux and M.I. Jordan. Hierarchical beta processes and the Indian buﬀet
process. In Proc. International Conference on Artiﬁcial Intelligence and Statistics,
volume 11, 2007.
[166] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the
Royal Statistical Society B, 58(1):267–288, 1996.
[167] L. Tierney. Markov chains for exploring posterior distributions. the Annals of
Statistics, pages 1701–1728, 1994.
[168] S. Tokdar. Posterior consistency of Dirichlet location-scale mixture of normals in
density estimation and regression. Sankhya: The Indian Journal of Statistics, 67:
90–110, 2006.
[169] Carnegie
Melon
University.
Graphics
lab
motion
capture
database.
http://mocap.cs.cmu.edu/.
[170] J. Van Gael, Y. Saatci, Y.W. Teh, and Z. Ghahramani. Beam sampling for the
inﬁnite hidden Markov model.
In Proc. International Conference on Machine
Learning, July 2008.

BIBLIOGRAPHY
269
[171] J. Van Gael, Y.W. Teh, and Z. Ghahramani. The inﬁnite factorial hidden Markov
model. In Advances in Neural Information Processing Systems, volume 21, pages
1697–1704, 2009.
[172] R. Vidal, A. Chiuso, , and S. Soatto. Observability and identiﬁability of jump
linear systems. In Proc. IEEE Conference on Decision and Control, December
2002.
[173] R. Vidal, Y. Ma, and S. Sastry.
Generalized principal component analysis
(GPCA): Subspace clustering by polynomial factorization, diﬀerentiation, and
division. UC Berkeley, Technical Report UCB/ERL., August 2003.
[174] R. Vidal, S. Soatto, Y. Ma, and S. Sastry. An algebraic geometric approach to
the identiﬁcation of a class of linear hybrid systems. In Proc. IEEE Conference
on Decision and Control, December 2003.
[175] M.J. Wainwright and M.I. Jordan. Graphical models, exponential families, and
variational inference.
UC Berkeley, Dept. of Statistics, TR #649, September
2003.
[176] M.J. Wainwright and M.I. Jordan. Graphical models, exponential families, and
variational inference. Foundations and Trends in Machine Learning, 1(1–2):1–305,
2008.
[177] S.G. Walker. Sampling the Dirichlet mixture model with slices. Communications
in Statistics–Simulation and Computation, 36:45–54, 2007.
[178] S.G. Walker, P.L. Damien, Purushottam W., and A.F.M. Smith. Bayesian non-
parametric inference for random distributions and related functions. Journal of
the Royal Statistical Society, Series B, 61(3):485–527, 1999.
[179] J.M. Wang, D.J. Fleet, and A. Hertzmann. Gaussian process dynamical models for
human motion. IEEE Transactions on Pattern Analysis and Machine Intelligence,
30(2):283–298, 2008.
[180] L. Wasserman. Asymptotic properties of nonparametric Bayesian procedures. In
D. Dey, P. M´’uller, and D. Sinha, editors, Practical Nonparametric and Semipara-
metric Bayesian Statistics, pages 293–304. Springer-Verlag, 1998.
[181] Y. Weiss and W.T. Freeman. Correctness of belief propagation in Gaussian graph-
ical models of arbitrary topology. Neural Computation, 13(10):2173–2200, 2001.
[182] M. West. Bayesian factor regression models in the “large p, small n” paradigm.
Bayesian Statistics, 7:723–732, 2003.
[183] M. West and J. Harrison. Bayesian Forecasting and Dynamic Models. Springer,
1997.

270
BIBLIOGRAPHY
[184] R.A. Wijsman. On the attainment of the Cram´er-Rao lower bound. The Annals
of Statistics, pages 538–542, 1973.
[185] C. Wooters and M. Huijbregts.
The ICSI RT07s speaker diarization system.
Lecture Notes in Computer Science, 2007.
[186] E.P. Xing and K-A Sohn. Hidden Markov Dirichlet process: Modeling genetic
inference in open ancestral space. Bayesian Analysis, 2(3):501–528, 2007.
[187] E.P. Xing, M.I. Jordan, and R. Sharan. Bayesian haplotype inference via the
Dirichlet process. Journal of Computational Biology, 14(3):267–284, 2007.
[188] X. Xuan and K. Murphy. Modeling changing dependency structure in multivariate
time series. In Proc. International Conference on Machine Learning, June 2007.
[189] V.A. Yakubovich. Solutions of certain matrix inequalities in automatic control
theory. Dokl. Akad. Nauk USSR, 143(3):1304–1307, 1962.

