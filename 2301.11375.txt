Neural networks learn to magnify areas near decision boundaries
Jacob A. Zavatone-Veth,1, 2, ∗Sheng Yang,3 Julian A. Rubinﬁen,4 and Cengiz Pehlevan3, 2, 5, †
1Department of Physics, Harvard University, Cambridge, Massachusetts 02138, USA
2Center for Brain Science, Harvard University, Cambridge, Massachusetts 02138, USA
3John A. Paulson School of Engineering and Applied Sciences,
Harvard University, Cambridge, Massachusetts 02138, USA
4Department of Physics, Yale University, New Haven, Connecticut 06511, USA
5Kempner Institute for the Study of Natural and Artiﬁcial Intelligence,
Harvard University, Cambridge, Massachusetts 02138, USA
(Dated: May 18, 2023)
We study how training molds the Riemannian geometry induced by neural network feature maps.
At inﬁnite width, neural networks with random parameters induce highly symmetric metrics on
input space. Feature learning in networks trained to perform classiﬁcation tasks magniﬁes local
areas along decision boundaries. These changes are consistent with previously proposed geometric
approaches for hand-tuning of kernel methods to improve generalization.
I.
INTRODUCTION
The physical and digital worlds possess rich geometric structure. If endowed with appropriate inductive biases,
machine learning algorithms can leverage these regularities to learn eﬃciently. However, it is unclear how one should
uncover the geometric inductive biases relevant for a particular task. The conventional approach to this problem is to
hand-design algorithms to embed certain geometric priors [1], but little attention has been given to an alternative
possibility: Can we uncover useful inductive biases by studying the geometry learned by existing deep neural network
models, which are known to be highly performant [2–4]? Previous works have explored some aspects of the geometry
induced by neural networks with random parameters [5–11], but we lack a rigorous understanding of data-dependent
changes in representational geometry over training.
In this work, we explore the possibility that neural networks learn to enhance local input disciminability over the
course of training. Concretely, we formalize and test the following hypothesis:
Hypothesis 1. Deep neural networks trained to perform supervised classiﬁcation tasks using standard gradient-based
methods learn to magnify areas near decision boundaries.
This hypothesis is inspired by a series of inﬂuential papers published around the turn of the 21st century by Amari
and Wu. They proposed that the generalization performance of support vector machine (SVM) classiﬁers on small-scale
tasks could be improved by transforming the kernel to expand the Riemannian volume element near decision boundaries
[12–14]. This proposal was based on the idea that this local magniﬁcation of areas improves class discriminability
[8, 12, 15].
Our speciﬁc contributions are:
• In §IV, we study general properties of the metric induced by shallow fully-connected neural networks. In §IV B, we
compute the volume element and curvature of the metric induced by inﬁnitely wide shallow networks with Gaussian
weights and smooth activation functions, showing that it is spherically symmetric. These results provide a baseline
for our tests of Hypothesis 1.
• In §V, we empirically show that training shallow networks on simple classiﬁcation tasks expands the volume element
along decision boundaries, consistent with Hypothesis 1. In §VI A and VI B, we provide evidence that deep residual
networks trained on more complex tasks behave similarly.
• Finally, in §VI C, we demonstrate how our approach can be applied to the self-supervised learning method Barlow
Twins, showing how area expansion can emerge even without supervised training.
In total, our results provide a preliminary picture of how feature learning shapes the geometry induced by neural
network feature maps. These observations open new avenues for investigating when this richly nonlinear form of
feature learning is required for good generalization in deep networks.
∗jzavatoneveth@g.harvard.edu
† cpehlevan@seas.harvard.edu
arXiv:2301.11375v2  [cs.LG]  17 May 2023

2
II.
PRELIMINARIES
To allow us to formalize Hypothesis 1, we begin by introducing the basic idea of the Riemannian geometry of feature
space representations. Our setup and notation largely follow Burges [15], which in turn follows the conventions of
Dodson and Poston [16]. We use the Einstein summation convention; summation over all repeated indices is implied.
A.
Feature embeddings as Riemannian manifolds
Consider d-dimensional data living in some submanifold D ⊆Rd. Let the feature map Φ : Rd →H be a map from
Rd to some separable Hilbert space H of possibly inﬁnite dimension n, with Φ(D) = M ⊆H. We index input space
dimensions by Greek letters µ, ν, ρ, . . . ∈[d] and feature space dimensions by Latin letters i, j, k, . . . ∈[n]. Assume that
Φ is Cℓfor ℓ≥3, and is everywhere of rank r = min{d, n}. If r = d, then M is a d-dimensional Cℓmanifold immersed
in H. If ℓ= ∞, then M is a smooth manifold. In contrast, if r < d, then M is a d-dimensional Cℓmanifold submersed
in H. The ﬂat metric on H can then be pulled back to M, with components
gµν = ∂µΦi∂νΦi,
(1)
where we write ∂µ ≡∂/∂xµ. If we deﬁne the feature kernel k(x, y) = Φi(x)Φi(y) for x, y ∈D, then the resulting
metric can be written in terms of the kernel as gµν = (1/2)∂xµ∂xνk(x, x) −[∂yµ∂yνk(x, y)]y=x. This formula applies
even if n = ∞, giving the metric induced by the feature embedding associated to a suitable Mercer kernel [15].
If r = d and the pullback metric gµν is full rank, then (M, g) is a d-dimensional Riemannian manifold [15, 16].
However, if the pullback gµν is a degenerate metric, as must be the case if r < d, then (M, g) is a singular semi-
Riemannian manifold [11, 17]. In this case, if we let ∼be the equivalence relation deﬁned by identifying points with
vanishing pseudodistance, the quotient (M/ ∼, g) is a Riemannian manifold [11]. Unless noted otherwise, our results
will focus on the non-singular case. We denote the matrix inverse of the metric tensor by gµν, and we raise and lower
input space indices using the metric.
B.
Volume element and curvature
With this setup, (M, g) is a Riemannian manifold; hence, we have at our disposal a powerful toolkit with which we
may study its geometry. We will focus on two geometric properties of (M, g). First, the volume element is given by
dV = √det g ddx, where the factor √det g measures how local areas in input space are magniﬁed by the feature map
[12, 15, 16]. It is this measure of volume which we will use in testing Hypothesis 1. Second, we consider the intrinsic
curvature of the manifold, which is characterized by the Riemann tensor Rµ
ναβ [16]. If Rµ
ναβ = 0, then the manifold is
intrinsically ﬂat. As a tractable measure, we focus on the Ricci curvature scalar R = gβνRα
ναβ, which measures the
deviation of the volume of an inﬁnitesimal geodesic ball in the manifold from that in ﬂat space [16]. In the singular
case, we can compute the volume element on M/ ∼at a given point by taking the square root of the product of the
non-zero eigenvalues of the degenerate metric gµν at that point [11]. However, the curvature in this case is generally
not straightforward to compute; we will therefore leave this issue for future work.
C.
Shallow neural network feature maps
In this work, we consider feature maps given by the hidden layer representations of neural networks [2, 7, 8, 10, 11, 18–
21]. In our theoretical investigations, we focus on shallow fully-connected neural networks, which have feature maps of
the form Φj(x) = n−1/2φ(wj · x + bj) for weights wj, biases bj, and an activation function φ, where we abbreviate
w · x = wµxµ. In this case, the feature space dimension n is equal to the number of hidden units, i.e., the width of the
hidden layer. We scale the components of the feature map by n−1/2 such that the associated kernel k(x, y) = Φi(x)Φi(y)
and metric have the form of averages over hidden units, and therefore should be well-behaved at large widths [18, 19].
We will assume that φ is Ck for k ≥3, so that this feature map satisﬁes the smoothness conditions required in the
setup above. We will also assume that the activation function and weight vectors are such that the Jacobian ∂µΦj
is full-rank, i.e., is of rank min{d, n}. Then, the shallow network feature map satisﬁes the required conditions for
the feature embedding to be a (possibly singular) Riemannian manifold. These conditions extend directly to deep
fully-connected networks formed by composing shallow feature maps [10, 11].

3
III.
RELATED WORKS
Having established the geometric preliminaries of §II, we can give a more complete overview of related works. As
introduced above, our hypothesis for how the Riemannian geometry of neural network representations changes during
training is directly inspired by the work of Amari and Wu [12]. In a series of works predating the deep learning
revolution [12–14], they proposed to modify the kernel of an SVM as ˜k(x, y) = h(x)h(y)k(x, y) for some positive
scalar function h(x) chosen such that the magniﬁcation factor √det g is large near the SVM’s decision boundary.
Concretely, they proposed to ﬁt an SVM with some base kernel k, choose h(x) = P
v∈SV(k) exp[−∥x −v∥2/2τ 2] for τ
a bandwidth parameter and SV(k) the set of support vectors for k, and then ﬁt an SVM with the modiﬁed kernel ˜k.
Here, ∥· ∥denotes the Euclidean norm. This process could then be iterated, yielding a sequence of modiﬁed kernels.
They found that this hand-designed form of iterative feature learning could improve generalization performance on a
set of small-scale tasks [12–14].
The geometry induced by common kernels was investigated by Burges [15], who established a broad range of technical
results. He showed that translation-invariant kernels of the form k(x, y) = k(∥x −y∥2) yield ﬂat, constant metrics, and
gave a detailed characterization of polynomial kernels k(x, y) = (x · y)q. Cho and Saul [8] subsequently analyzed the
geometry induced by arc-cosine kernels, i.e., the feature kernels of inﬁnitely-wide shallow neural networks with threshold-
power law activation functions φ(x) = max{0, x}q and random parameters [7]. Our results on inﬁnitely-wide networks
for general smooth activation functions build on these works. More recent works have studied the representational
geometry of deep networks with random Gaussian parameters in the limit of large width and depth [5, 6], tying into a
broader line of research on inﬁnite-width limits in which inference and prediction is captured by a kernel machine
[9, 18–26]. Our results on the representational geometry of wide shallow networks with smooth activation functions
build on these ideas, particularly those relating activation function derivatives to input discriminability [5, 9, 22, 27].
Particularly closely related to our work are several recent papers that aim to study the curvature of neural network
representations. Hauser and Ray [10], Benfenati and Marta [11] discuss formal principles of Riemannian geometry in
deep neural networks, but do not characterize how training shapes the geometry. Kaul and Lall [28] aimed to study the
curvature of metrics induced by the outputs of pretrained classiﬁers. However, their work is limited by the fact that
they estimate input-space derivatives using inexact ﬁnite diﬀerences under the strong assumption that the input data is
conﬁned to a known smooth submanifold of Rd. In very recent work, Benfenati and Marta [29] have used the geometry
induced by the full input-output mapping to reconstruct iso-response curves of deep networks. In contrast, our work
focuses on hidden representations, and seeks to characterize the representational manifolds themselves. Finally, several
recent works have studied the Riemannian geometry of the latent representations of deep generative models [30–32].
IV.
REPRESENTATIONAL GEOMETRY OF SHALLOW NEURAL NETWORK FEATURE MAPS
A.
Finite-width networks
We begin by studying general properties of the metrics induced by shallow neural networks. We ﬁrst consider
ﬁnite-width networks with ﬁxed weights, assuming that n ≥d. Writing zj = wj · x + bj for the preactivation of the
j-th hidden unit, the general formula (1) for the metric yields
gµν = 1
nφ′(zj)2wjµwjν.
(2)
This metric has the useful property that ∂αgµν is symmetric under permutation of its indices, hence the formula for
the Riemann tensor simpliﬁes substantially (Appendix A). We show in Appendix B that the determinant of the metric
can be expanded as a sum over d-tuples of hidden units:
det g =
1
ndd!M 2
j1···jdφ′(zj1)2 · · · φ′(zjd)2,
(3)
where Mj1···jd = det([wjii]1≤i≤d) is the minor of the weight matrix obtained by selecting units j1, . . . , jd. For the
error function φ(x) = erf(x/
√
2), det g expands as a superposition of Gaussian bump functions, one for each tuple
of hidden units (B.55). This is reminiscent of Amari and Wu’s approach, which yields a Gaussian contribution to
√det g from each support vector (§III). We can also derive similar expansions for the Riemann tensor and Ricci scalar.
The resulting expressions are rather unwieldy, so we give their general forms only in Appendix B 3. However, in
two dimensions the situation simpliﬁes, as the Riemann tensor is completely determined by the Ricci scalar [16, 33]

4
n=50
n=100
n=250
n=500
n=1000
n=∞
det(g)1/2
erf
0
0.4
R
x2
-8
2
a
║x║
0
2
det(g)1/2
0
12
R
-1
0
b
1
║x║
0
2
1
║x║
0
2
1
║x║
0
2
1
0
-2
-4
x2
erf
-6
0.1
0.2
0.3
8
4
-0.2
-0.4
-0.6
-0.8
n=50
n=100
n=250
n=500
n=1000
n=∞
FIG. 1. Convergence of geometric quantities for ﬁnite-width networks with Gaussian random parameters to the inﬁnite-width
limit. a. The magniﬁcation factor √det g (left) and Ricci scalar R (right) as functions of the input norm ∥x∥for networks
with φ(x) = erf(x/
√
2). Empirical results for ﬁnite networks, computed using (3) and (B.15) are shown in blue, with solid
lines showing the mean and shaded patches the standard deviation over 25 realizations of random Gaussian parameters. In all
cases, σ = ζ = 1. The inﬁnite-width result is shown as a black dashed line. b. As in a, but for normalized quadratic activation
functions φ(x) = x2/
√
3.
(Appendix B 1). In this case, we have the compact expression
(det g)2R = −3
n3 M 2
jkMijMikφ′(zi)2φ′(zj)φ′(zk)φ′′(zj)φ′′(zk).
(4)
This shows that in d = 2 the curvature acquires contributions from each triple of distinct hidden units, hence if
n = 2 we have R = 0. This follows from the fact that the feature map is in this case a change of coordinates on the
two-dimensional manifold [16].
B.
Geometry of inﬁnite shallow networks
We now characterize the metric induced by inﬁnite-width networks (n →∞) with Gaussian weights wj ∼i.i.d.
N(0, σ2Id) and biases bj ∼i.i.d. N(0, ζ2), as commonly chosen at initialization [2, 5, 20, 21, 23, 24].
For such
networks, the hidden layer representation is described by the neural network Gaussian process (NNGP) kernel [18–21]:
k(x, y) = limn→∞n−1Φ(x) · Φ(y) = Ew,b[φ(w · x + b)φ(w · y + b)]. For networks in the lazy regime, this kernel
completely describes the representation after training [24, 26]. In Appendix C, we show that the metric associated
with the NNGP kernel can be written as
gµν = eΩ(∥x∥2)[δµν + 2Ω′(∥x∥2)xµxν],
(5)
where the function Ω(∥x∥2) is deﬁned via eΩ(∥x∥2) = σ2E[φ′(z)2] for z ∼N(0, σ2∥x∥2 + ζ2). Therefore, like the
metrics induced by other dot-product kernels, the NNGP metric has the form of a projection [15]. Such metrics have
determinant
det g = eΩd(1 + 2∥x∥2Ω′)
(6)
and Ricci scalar
R = −3(d −1)e−Ω(Ω′)2∥x∥2
(1 + 2∥x∥2Ω′)2

d + 2 + 2∥x∥2

(d −2)Ω′ + 2Ω′′
Ω′

.
(7)
Thus, all geometric quantities are spherically symmetric, depending only on ∥x∥2. Thanks to the assumption of
independent Gaussian weights, the geometric quantities associated to the shallow Neural Tangent Kernel and to the
deep NNGP will share this spherical symmetry (Appendix D) [20, 21, 23, 24]. This generalizes the results of Cho and
Saul [8] for threshold-power law functions to arbitrary smooth activation functions. The relation between Gaussian
norms of φ′ and input discriminability indicated by this result is consistent with previous studies [5, 9, 22, 27]. In
short, unless the task depends only on the input norm, the geometry of inﬁnite-width networks will not be linked to
the task structure.

5
FIG. 2. Evolution of the volume element over training in a network with a single hidden layer of 20 hidden units trained to
classify points separated by a sinusoidal boundary y = 3
5 sin(7x −1). Training datapoints were sampled uniformly from [−1, +1]2.
Red lines indicate the decision boundaries of the network. See Appendix G 1 for experimental details and visualizations at other
widths.
In Appendix C 2, we evaluate the geometric quantities of the NNGP for certain analytically tractable activation
functions. The resulting expressions for √det g and R are rather lengthy, so we discuss only their qualitative behavior
here. For the error function φ(x) = erf(x/
√
2), R is negative for all d > 1, and both R and √det g are monotonically
decreasing functions of ∥x∥for all ζ and σ. For monomials φ(x) ∝xq for integer q > 1, √det g is a monotonically
increasing function of ∥x∥2, while R is again non-positive. However, in this case the behavior of R depends on whether
or not bias terms are present: if ζ = 0, then R is a non-decreasing function of ∥x∥2 that diverges towards −∞
as ∥x∥2 ↓0, while if ζ > 0, R may be non-monotonic in ∥x∥2. In Figure 1, we illustrate this behavior, and show
convergence of the empirical geometry of ﬁnite networks with random Gaussian parameters to the inﬁnite-width
results.
V.
CHANGES IN SHALLOW NETWORK GEOMETRY DURING TRAINING
We now consider how the geometry of the pullback metric changes during training in networks that learn features,
that is, outside of the lazy/kernel regime. Changes in the volume element and curvature during gradient descent
training are challenging to study analytically, because feature-learning networks with solvable dynamics—deep linear
networks [34]—trivially yield ﬂat, constant metrics. One could attempt to solve for the metric’s dynamics through
time in inﬁnite-width networks parameterized such that they learn features [24, 26], but we will not do so here. We
can make slightly more analytical progress for Bayesian neural networks at large but ﬁnite width by leveraging recent
results on perturbative feature-learning corrections to the NNGP kernel [25, 35] to compute corrections to the posterior
mean of the volume element (Appendix E). This gives some intuition for how interpolating a single point shapes the
network’s global representational geometry, showing that the training point’s inﬂuence is maximal for points parallel
to it. Yet, the settings we can address are far removed from realistic network architectures and datasets. Thus, given
the intractability of studying changes in geometry analytically, we resort to numerical experiments. For details of our
numerical methods, see Appendix G.
A.
Changes in representational geometry for networks trained on two-dimensional toy tasks
To build intuition, we ﬁrst consider networks trained on simple two-dimensional tasks, for which we can directly
visualize the input space. We ﬁrst consider a toy binary classiﬁcation task with sinusoidal boundary, inspired by the
task considered in the original work of Amari and Wu [12]. We train networks with sigmoidal activation functions
of varying widths to perform this task, and visualize the resulting geometry over the course of training in Figures 2
and G.5. At initialization, the peaks in the volume element lack a clear relation to the structure of the task, with
approximate rotational symmetry at large widths as we would expect from §IV B. As the network’s decision boundary
is gradually molded to conform to the true boundary, the volume element develops peaks in the same vicinity. At all
widths, the ﬁnal volume elements are largest near the peaks of the sinusoidal decision boundary. At small widths, the
shape of the sinusoidal curve is not well-resolved, but at large widths there is a clear peak in the close neighborhood of
the decision boundary. This result is consistent with Hypothesis 1. In Appendix G, Figure G.6, we plot the Ricci scalar
for these trained networks. Even for these small networks, the curvature computation is computationally expensive

6
FIG. 3. Top panel: log10(√det g) induced at interpolated images between 7 and 6 by a single-hidden-layer fully-connected
network trained to classify MNIST digits. Bottom panel: Digit class predictions and log10(√det g) for the plane spanned by
MNIST digits 7, 6, and 1 at the ﬁnal training epoch (200) . Sample images are visualized at the endpoints and midpoint for
each set. Each line is colored by its prediction at the interpolated region and end points. As training progresses, the volume
elements bulge in the middle (near the decision boundary) and taper oﬀwhen travelling towards endpoints. See Appendix G 2
for experimental details and Figure G.8 for images interpolated between other digits.
and numerically challenging. Over training, it evolves dynamically, with task-adapted structure visible at the end of
training. However, the patterns here are harder to interpret than those in the volume element.
B.
Changes in geometry for shallow networks trained to classify MNIST digits
We now provide evidence that a similar phenomenon is present in networks trained to classify MNIST images. We
give details of these networks in Appendix G 2; note that all reach above 95% train and test accuracy within 200
epochs. Because of the computational complexity of estimating the curvature—the Riemann tensor has d2(d2 −1)/12
independent components [16, 33]—and its numerical sensitivity (Appendix G 1), we do not attempt to estimate it for
this high-dimensional task.
In Figure 3, we plot the induced volume element at synthetic images generated by linearly interpolating between two
input images (see Appendix G for details). We emphasize that linear interpolation in pixel space of course does not
respect the structure of the image data, and results in unrealistic images. However, this approach has the advantage
of being straightforward, and also illustrates how small Euclidean perturbations are expanded by the feature map
[36]. At initialization, the volume element varies without clear structure along the interpolated path. However, as
training progresses, areas near the center of the path, which roughly aligns with the decision boundary, are expanded,
while those near the endpoints deﬁned by true training examples remain relatively small. This is again consistent with
Hypothesis 1. We provide additional visualizations of this behavior in Appendix G 2.
To gain an understanding of the structure of the volume element beyond one-dimensional slices, in Figure 3 we also
plot its value in the plane spanned by three randomly-selected example images, at points interpolated linearly within
their convex hull. Here, we only show the end of training; in Appendix G 2 we show how the volume element in this
plane changes over the course of training. The edges of the resulting ternary plot are one-dimensional slices like those
shown in the top row of Figure 3, and we observe consistent expansion of the volume element along these paths. The
volume element becomes large near the centroid of the triangle, where multiple decision boundaries intersect.

7
0
10
20
30
40
50
60
1120
1100
1080
1060
1040
1020
1000
980
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1200
1180
1160
1140
1120
1100
1080
1060
log10 volume element
Epoch 50
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1240
1220
1200
1180
1160
1140
1120
log10 volume element
Epoch 200
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
FIG. 4. Top panel: log10(√det g) induced at interpolated images between a horse and a frog by ResNet34 with GELU activation
trained to classify CIFAR-10 images. Bottom panel: Digits classiﬁcation of a horse, a frog, and a car. The volume element
is the largest at the intersection of several binary decision boundaries, and smallest within each of the decision region. The
one-dimensional slices along the edges of each ternary plot are consistent with the top panel. See Appendix G 3 for experimental
details, Figure G.13 for linear interpolation and plane spanned by other classes, and how the plane evolves during training.
VI.
BEYOND SHALLOW LEARNING
A.
Deep residual networks with smooth activation functions
Thus far, we have focused on the geometry of the feature maps of single-hidden-layer neural networks. However,
these analyses can also be applied to deeper networks, regarding the representation at each hidden layer as deﬁning
a feature map [10, 11]. As a simple version of this, in Supplemental Figure G.7 we consider a network with three
fully-connected hidden layers trained on the sinusoid task. The metrics induced by the feature maps of all three hidden
layers all show the same qualitative behavior as we observed in the shallow case in Figure 2: areas near the decision
boundary are magniﬁed. As one progresses deeper into the network, the contrast between regions of low and high
magniﬁcation factor increases.
As a more realistic example, we consider deep residual networks (ResNets) [37] trained to classify the CIFAR-10 image
dataset [38]. To make the feature map diﬀerentiable, we replace the rectiﬁed linear unit (ReLU) activation functions
used in standard ResNets with Gaussian error linear units (GELUs) [39]. With this modiﬁcation, we achieve comparable
test accuracy (92%) with a ResNet-34—the largest model we can consider given computational constraints—to that
obtained with ReLUs (Appendix G 3). Importantly, the feature map deﬁned by the input-to-ﬁnal-hidden-layer mapping
of a ResNet-34 gives a submersion of CIFAR-10, as the input images have 32 × 32 × 3 = 3072 pixels, while the
ﬁnal hidden layer has 512 units. Empirically, we ﬁnd that the Jacobian of this mapping is full-rank (Figure G.14);
we therefore consider the volume element on (M/ ∼, g) deﬁned by the product of the non-zero eigenvalues of the
degenerate pullback metric (§II, Appendix G 3).
In Figure 4, we visualize the resulting geometry in the same way we did for networks trained on MNIST, along 1-D
interpolated slices and in a 2-D interpolated plane (see Appendix G 3 for details and additional ﬁgures). In both 1-D
and 2-D slices, we see a clear trend of large volume elements near decision boundaries, as we observed for shallow
networks. In Figure G.19, we show that these networks also expand areas near incorrect decision boundaries, but
do not expand areas along slices between three correctly-classiﬁed points of the same class. Thus, even in this more
realistic setting, we observe shaping of geometry over training that appears consistent with Hypothesis 1.

8
0
10
20
30
40
50
60
1040
1030
1020
1010
1000
990
980
970
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1140
1120
1100
1080
1060
1040
log10 volume element
Epoch 50
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1225
1200
1175
1150
1125
1100
1075
log10 volume element
Epoch 200
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
FIG. 5. Top panel: log10(√det g) induced at interpolated images between a horse and a frog by ResNet34 with ReLU activation
trained to classify CIFAR-10 images. Bottom panel: Digits classiﬁcation of a horse, a frog, and a car. The volume element is the
largest at the intersection of several binary decision boundaries, and smallest within each of the decision region. See Appendix
G 3 for details of these experiments and additional ﬁgures.
B.
Deep ReLU networks
Because of the smoothness conditions required by the deﬁnition of the pullback metric and the requirement that
(M, g) be a diﬀerentiable manifold [10, 11], the approach pursued in the preceding sections does not apply directly
to networks with ReLU activation functions, which are not diﬀerentiable. Deep ReLU networks are continuous
piecewise-linear maps, with many distinct activation regions [40, 41]. Within each region, the corresponding linear
feature map will induce a ﬂat metric on the input space, but the magniﬁcation factor will vary from region to region.
Therefore, though the overall framework of the preceeding sections does not apply in the ReLU setting, we can still
visualize this variation in the piecewise-constant magniﬁcation factor. In Figure 5, we replicate Figure 4 for a ResNet34
with ReLU rather than GELU activations, and observe qualitatively similar expansion of the volume element. We
provide additional visualizations of ReLU ResNets in Appendix G 3. Therefore, the qualitative picture of Hypothesis 1
can be extended beyond the globally smooth setting.
C.
Self-supervised learning with Barlow Twins
Though Hypothesis 1 focuses on supervised training, the same geometric analysis can be performed for any feature
map, irrespective of the training procedure. To demonstrate the broader utility of visualizing the induced volume
element, we consider ResNet feature maps trained with the self-supervised learning (SSL) method Barlow Twins [42].
In Figure 6 and Appendix G 4, we show that we observe expansion of areas near the decision boundaries of a linear
probe trained on top of this feature map, consistent with what we saw for supervised ResNets. In Appendix G 4, we
show in contrast that a clear pattern of expansion is not visible for ResNets trained with the alternative SSL method
SimCLR [43]. We hypothesize that this diﬀerence results from SimCLR’s normalization of the feature map, which may
make treating the embedding space as Euclidean inappropriate. These results illustrate the broader potential of our
approach to give new insights into how diﬀerent SSL procedures induce diﬀerent geometry, suggesting avenues for
future investigation.

9
0
10
20
30
40
50
60
4080
4060
4040
4020
4000
3980
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1640
1620
1600
1580
log10 volume element
Epoch 200
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1900
1880
1860
1840
log10 volume element
Epoch 1000
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
FIG. 6. Top panel: log10(√det g) induced at interpolated images between a horse and a frog by Barlow Twins with a ResNet34
backbone with GELU activation. Bottom panel: Digits classiﬁcation of a horse, a frog, and a car, prediction given by a multiclass
logistic regression on the features by the ResNet backbone. The volume element is the largest at the intersection of several
binary decision boundaries, and smallest within each of the decision region. See Appendix G 4 for details of these experiments
and additional ﬁgures.
VII.
DISCUSSION
To conclude, we have shown that training on simple tasks shapes the Riemannian geometry induced by neural
network representations by magnifying areas along decision boundaries [12–14]. These results are relevant to the broad
goal of leveraging non-Euclidean geometry in deep learning, but they diﬀer from many past approaches in that we seek
to characterize what geometric structure is learned rather than hand-engineering the optimal geometry for a given
task [1, 44]. Perhaps the most important limitation of our work is the fact that we focus either on toy tasks with
two-dimensional input domains, or on low-dimensional slices through high-dimensional domains. This is a fundamental
limitation of how we have attempted to visualize the geometry. We are also restricted by computational constraints
(Appendix G 3); to characterize the geometry of state-of-the-art network architectures, more eﬃcient and numerically
stable algorithms for computing these quantities must be developed.
An important question that we leave open for future work is whether expanding areas near decision boundaries
generically improves classiﬁer generalization, consistent with Amari and Wu [12]’s original motivations. Indeed, it is
easy to imagine a scenario in which the geometry is overﬁt, and the trained network becomes too sensitive to small
changes in the input. This possibility is consistent with prior work on the sensitivity of deep networks [36], and with
the related phenomenon of adversarial vulnerability [45, 46]. In recent work, Radhakrishnan et al. [4] have proposed
a method for learning data-adaptive kernels, and show that for some datasets this method generalizes better than
fully-trained deep networks. Their method produces kernels that induce ﬂat, constant metrics (Appendix F). As we
have shown here that deep networks generically learn to expand areas near decision boundaries, it will be interesting
to investigate when this ﬂexible, nonlinear form of feature learning is necessary for generalization, and when it can be
harmful.
One possible application of our ideas is to the problem of semantic data deduplication. In contemporaneous work,
Abbas et al. [47] have proposed a data pruning method that identiﬁes related examples based on their embeddings
under a pretrained feature map. Their method proceeds in two steps: ﬁrst, they cluster examples using k-means
based on the Euclidean distances between their embeddings, and then they eliminate examples within each cluster
by identifying pairs whose embeddings have Euclidean cosine similarity above some threshold. They show that this

10
procedure can substantially reduce the size of large image and text datasets, and that models trained on the pruned
datasets display superior performance. By considering local distances between points in embedding space, their
method is closely related to a ﬁnite-diﬀerence approximation of the distance as measured by the induced metric of the
pretrained feature map. We therefore propose that the Riemannian viewpoint taken here could allow both for deeper
understanding of existing deduplication methods and for the design of novel algorithms that are both principled and
interpretable.
Our results are also applicable to the general problem of how to analyze and compare neural network representations
[48, 49]. As illustrated by our SSL experiments, one could compute and plot the volume element induced by a
feature map even when one does not have access to explicit class labels. This could allow one to study pre-trained
networks for which one does not have access to the training classes, and perhaps even diﬀerentiable approximations to
biological neural networks [50]. Exploring the rich geometry induced by these networks is an exciting avenue for future
investigation.
ACKNOWLEDGMENTS
We thank Blake Bordelon, Matthew Farrell, Anindita Maiti, Carlos Ponce, Sabarish Sainathan, James B. Simon,
and Binxu Wang for useful discussions and comments on earlier versions of our manuscript. JAZ-V and CP were
supported by NSF Award DMS-2134157. The computations in this paper were run on the FASRC Cannon cluster
supported by the FAS Division of Science Research Computing Group at Harvard University.
[1] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veliˇckovi´c, Geometric deep learning: Grids, groups, graphs, geodesics, and
gauges, arXiv preprint arXiv:2104.13478 (2021).
[2] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature 521, 436 (2015).
[3] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, Understanding deep learning (still) requires rethinking
generalization, Communications of the ACM 64, 107 (2021), arXiv:1611.03530.
[4] A. Radhakrishnan, D. Beaglehole, P. Pandit, and M. Belkin, Feature learning in neural networks and kernel machines that
recursively learn features, arXiv 10.48550/ARXIV.2212.13881 (2022).
[5] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli, Exponential expressivity in deep neural networks through
transient chaos, in Advances in Neural Information Processing Systems, Vol. 29, edited by D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett (Curran Associates, Inc., 2016) arXiv:1606.05340.
[6] S.-i. Amari, R. Karakida, and M. Oizumi, Statistical neurodynamics of deep networks: Geometry of signal spaces, Nonlinear
Theory and Its Applications, IEICE 10, 322 (2019), arXiv:1808.07169.
[7] Y. Cho and L. K. Saul, Kernel methods for deep learning, in Advances in Neural Information Processing Systems, Vol. 22,
edited by Y. Bengio, D. Schuurmans, J. Laﬀerty, C. Williams, and A. Culotta (Curran Associates, Inc., 2009).
[8] Y. Cho and L. K. Saul, Analysis and extension of arc-cosine kernels for large margin classiﬁcation, arXiv preprint
arXiv:1112.3712 10.48550/ARXIV.1112.3712 (2011), arXiv:1112.3712.
[9] J. A. Zavatone-Veth and C. Pehlevan, On neural network kernels and the storage capacity problem, Neural Computation
34, 1136 (2022), arXiv:2201.04669.
[10] M. Hauser and A. Ray, Principles of Riemannian geometry in neural networks, in Advances in Neural Information Processing
Systems, Vol. 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(Curran Associates, Inc., 2017).
[11] A. Benfenati and A. Marta, A singular Riemannian geometry approach to deep neural networks: I. Theoretical foundations,
Neural Networks 158, 331 (2023).
[12] S.-i. Amari and S. Wu, Improving support vector machine classiﬁers by modifying kernel functions, Neural Networks 12,
783 (1999).
[13] S. Wu and S.-I. Amari, Conformal transformation of kernel functions: A data-dependent way to improve support vector
machine classiﬁers, Neural Processing Letters 15, 59 (2002).
[14] P. Williams, S. Li, J. Feng, and S. Wu, A geometrical method to improve performance of the support vector machine, IEEE
Transactions on Neural Networks 18, 942 (2007).
[15] C. J. C. Burges, Geometry and invariance in kernel based methods, in Advances in Kernel Methods: Support Vector
Learning, edited by B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola (MIT Press, Cambridge, MA, USA, 1999) p. 89–116.
[16] C. T. J. Dodson and T. Poston, Tensor Geometry: The Geometric Viewpoint and its Uses (Springer Berlin Heidelberg,
Berlin, Heidelberg, 1991).
[17] D. N. Kupeli, Singular semi-Riemannian geometry, Vol. 366 (Springer Science & Business Media, 2013).
[18] C. K. Williams, Computing with inﬁnite networks, Advances in Neural Information Processing Systems , 295 (1997).
[19] R. M. Neal, Priors for inﬁnite networks, in Bayesian Learning for Neural Networks (Springer, 1996) pp. 29–53.

11
[20] J. Lee, J. Sohl-Dickstein, J. Pennington, R. Novak, S. Schoenholz, and Y. Bahri, Deep neural networks as Gaussian processes,
in International Conference on Learning Representations (2018).
[21] A. G. d. G. Matthews, J. Hron, M. Rowland, R. E. Turner, and Z. Ghahramani, Gaussian process behaviour in wide deep
neural networks, in International Conference on Learning Representations (2018).
[22] A. Daniely, R. Frostig, and Y. Singer, Toward deeper understanding of neural networks: The power of initialization and a
dual view on expressivity, in Advances in Neural Information Processing Systems, Vol. 29, edited by D. Lee, M. Sugiyama,
U. Luxburg, I. Guyon, and R. Garnett (Curran Associates, Inc., 2016).
[23] G. Yang, Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence,
and neural tangent kernel derivation, arXiv preprint arXiv:1902.04760 (2019).
[24] G. Yang and E. J. Hu, Tensor Programs IV: Feature learning in inﬁnite-width neural networks, in Proceedings of the 38th
International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol. 139, edited by M. Meila
and T. Zhang (PMLR, 2021) pp. 11727–11737.
[25] J. A. Zavatone-Veth, A. Canatar, B. S. Ruben, and C. Pehlevan, Asymptotics of representation learning in ﬁnite Bayesian
neural networks, in Advances in Neural Information Processing Systems, Vol. 34, edited by M. Ranzato, A. Beygelzimer,
Y. Dauphin, P. Liang, and J. W. Vaughan (Curran Associates, Inc., 2021) pp. 24765–24777.
[26] B. Bordelon and C. Pehlevan, Self-consistent dynamical ﬁeld theory of kernel evolution in wide neural networks, in Advances
in Neural Information Processing Systems, Vol. 35, edited by A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho (2022).
[27] J. A. Zavatone-Veth and C. Pehlevan, Activation function dependence of the storage capacity of treelike neural networks,
Phys. Rev. E 103, L020301 (2021).
[28] P. Kaul and B. Lall, Riemannian curvature of deep neural networks, IEEE Transactions on Neural Networks and Learning
Systems 31, 1410 (2020).
[29] A. Benfenati and A. Marta, A singular Riemannian geometry approach to deep neural networks: II. Reconstruction of 1-D
equivalence classes, Neural Networks 158, 344 (2023).
[30] H. Shao, A. Kumar, and P. Thomas Fletcher, The Riemannian geometry of deep generative models, in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops (2018).
[31] L. Kuhnel, T. Fletcher, S. Joshi, and S. Sommer, Latent space non-linear statistics, arXiv 10.48550/ARXIV.1805.07632
(2018).
[32] B. Wang and C. R. Ponce, A geometric analysis of deep generative image models and its applications, in International
Conference on Learning Representations (2021).
[33] C. W. Misner, K. S. Thorne, and J. A. Wheeler, Gravitation (Princeton University Press, 2017).
[34] A. M. Saxe, J. L. McClelland, and S. Ganguli, Exact solutions to the nonlinear dynamics of learning in deep linear neural
networks, arXiv preprint arXiv:1312.6120 (2013), arXiv:1312.6120.
[35] D. A. Roberts, S. Yaida, and B. Hanin, The Principles of Deep Learning Theory: An Eﬀective Theory Approach to
Understanding Neural Networks (Cambridge University Press, 2022) arXiv:2106.10165.
[36] R. Novak, Y. Bahri, D. A. Abolaﬁa, J. Pennington, and J. Sohl-Dickstein, Sensitivity and generalization in neural networks:
an empirical study, in International Conference on Learning Representations (2018).
[37] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in 2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2016) pp. 770–778.
[38] A. Krizhevsky, Learning multiple layers of features from tiny images, Tech. Rep. (University of Toronto, 2009).
[39] D. Hendrycks and K. Gimpel, Gaussian error linear units (GELUs), arXiv 10.48550/ARXIV.1606.08415 (2016).
[40] B. Hanin and D. Rolnick, Complexity of linear regions in deep networks, in Proceedings of the 36th International Conference
on Machine Learning, Proceedings of Machine Learning Research, Vol. 97, edited by K. Chaudhuri and R. Salakhutdinov
(PMLR, 2019) pp. 2596–2604.
[41] B. Hanin and D. Rolnick, Deep ReLU networks have surprisingly few activation patterns, in Advances in Neural Information
Processing Systems, Vol. 32, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett
(Curran Associates, Inc., 2019).
[42] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny, Barlow twins: Self-supervised learning via redundancy reduction, in
Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol.
139, edited by M. Meila and T. Zhang (PMLR, 2021) pp. 12310–12320.
[43] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, A simple framework for contrastive learning of visual representations, in
Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol.
119, edited by H. D. III and A. Singh (PMLR, 2020) pp. 1597–1607.
[44] M. Weber, M. Zaheer, A. S. Rawat, A. K. Menon, and S. Kumar, Robust large-margin learning in hyperbolic space, in
Advances in Neural Information Processing Systems, Vol. 33, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan,
and H. Lin (Curran Associates, Inc., 2020) pp. 17863–17873.
[45] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, Intriguing properties of neural
networks, arXiv 10.48550/ARXIV.1312.6199 (2013).
[46] I.
J.
Goodfellow,
J.
Shlens,
and
C.
Szegedy,
Explaining
and
harnessing
adversarial
examples,
arXiv
10.48550/ARXIV.1412.6572 (2014).
[47] A. Abbas, K. Tirumala, D. Simig, S. Ganguli, and A. S. Morcos, Semdedup: Data-eﬃcient learning at web-scale through
semantic deduplication, arXiv 10.48550/arXiv.2303.09540 (2023), 2303.09540 [cs.LG].
[48] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton, Similarity of neural network representations revisited, in Proceedings of
the 36th International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol. 97, edited by
K. Chaudhuri and R. Salakhutdinov (PMLR, 2019) pp. 3519–3529.

12
[49] A. H. Williams, E. Kunz, S. Kornblith, and S. Linderman, Generalized shape metrics on neural representations, in Advances
in Neural Information Processing Systems, Vol. 34, edited by M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W.
Vaughan (Curran Associates, Inc., 2021) pp. 4738–4750.
[50] B. Wang and C. R. Ponce, Tuning landscapes of the ventral stream, Cell Reports 41, 111595 (2022).
[51] R. Penrose, The road to reality : a complete guide to the laws of the universe, 1st ed. (A.A. Knopf, New York, 2005).
[52] R. Price, A useful theorem for nonlinear devices having Gaussian inputs, IRE Transactions on Information Theory 4, 69
(1958).
[53] D. Saad and S. A. Solla, Exact solution for on-line learning in multilayer neural networks, Phys. Rev. Lett. 74, 4337 (1995).
[54] DLMF, NIST Digital Library of Mathematical Functions, http://dlmf.nist.gov/, Release 1.1.1 of 2021-03-15 (2021), f. W. J.
Olver, A. B. Olde Daalhuis, D. W. Lozier, B. I. Schneider, R. F. Boisvert, C. W. Clark, B. R. Miller, B. V. Saunders, H. S.
Cohl, and M. A. McClain, eds.
[55] J. A. Zavatone-Veth, W. L. Tong, and C. Pehlevan, Contrasting random and learned features in deep Bayesian linear
regression, Phys. Rev. E 105, 064118 (2022).
[56] J. B. Simon, M. Knutins, L. Ziyin, D. Geisz, A. J. Fetterman, and J. Albrecht, On the stepwise nature of self-supervised
learning, in Proceedings of the 40th International Conference on Machine Learning (2023).
[57] M. Kochurov, R. Karimov, and S. Kozlukov, Geoopt: Riemannian optimization in PyTorch, arXiv preprint arXiv:2005.02819
(2020).
[58] N. Miolane, N. Guigui, A. Le Brigant, J. Mathe, B. Hou, Y. Thanwerdas, S. Heyder, O. Peltre, N. Koep, H. Zaatiti, et al.,
Geomstats: a Python package for Riemannian geometry in machine learning, Journal of Machine Learning Research 21, 1
(2020).
[59] G. Mishne, Z. Wan, Y. Wang, and S. Yang, The numerical stability of hyperbolic representation learning, arXiv preprint
arXiv:2211.00181 (2022).
[60] Y. LeCun, C. Cortes, and C. Burges, MNIST handwritten digit database, ATT Labs [Online] 2 (2010).
[61] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,
A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, PyTorch:
An imperative style, high-performance deep learning library, in Advances in Neural Information Processing Systems 32
(Curran Associates, Inc., 2019) pp. 8024–8035.
[62] K. Liu, W. Yang, P. Yang, and F. Ducau, Train CIFAR10 with PyTorch, Github (2021).

S1
Appendix A: Simpliﬁcation of the Riemann tensor for a general shallow network
In this section, we show how the general form of the Riemann tensor can be simpliﬁed for metrics of the form
considered here. As elsewhere, our conventions follow Dodson and Poston [16]. Consider a metric of the general form
gµν = Ew,b[φ′(w · x + b)2wµwν],
(A.1)
where we do not assume that the distribution of the weights and biases is Gaussian. For such a metric, we have
∂αgµν = 2Ew,b[φ′(w · x + b)φ′′(w · x + b)wαwµwν],
(A.2)
which is symmetric under permutation of its indices. Therefore, the Christoﬀel symbols of the second kind reduce to
Γα
βγ = 1
2gαµ(∂βgγµ −∂µgβγ + ∂γgµβ)
(A.3)
= 1
2gαµ∂βgγµ.
(A.4)
The (3, 1) Riemann tensor is then
Rµ
ναβ = ∂αΓµ
βν −∂βΓµ
αν + Γρ
ανΓµ
βρ −Γρ
βνΓµ
αρ
(A.5)
= 1
2 [∂α(gµρ∂βgνρ) −∂β(gµρ∂αgνρ)]
+ 1
4

(gρλ∂αgνλ)(gµσ∂βgρσ) −(gρλ∂βgνλ)(gµσ∂αgρσ)

(A.6)
= 1
2 [∂αgµρ∂βgνρ −∂βgµρ∂αgνρ + gµρ(∂α∂βgνρ −∂β∂αgνρ)]
+ 1
4

−∂αgνλ∂βgµλ + ∂βgνλ∂αgµλ
(A.7)
= 3
4(∂αgµρ∂βgνρ −∂βgµρ∂αgνρ),
(A.8)
where we have used the fact that partial derivatives commute and recalled the matrix calculus identity
∂αgµν = −gµρgνλ∂αgρλ.
(A.9)
Then, the (4, 0) Riemann tensor is
Rµναβ = gµλRλ
ναβ
(A.10)
= −3
4gρλ(∂αgµρ∂βgνλ −∂βgµρ∂αgνλ)
(A.11)
which, given the permutation symmetry of the derivatives of the metric, can be re-expressed as
Rµναβ = −3
4gρλ(∂ρgµα∂λgνβ −∂ρgµβ∂λgνα).
(A.12)
It is then easy to see that the simpliﬁed formula for the Riemann tensor has the expected symmetry properties under
index permutation:
Rµναβ = −Rµνβα
(A.13)
Rµναβ = −Rνµαβ
(A.14)
Rµναβ = +Rαβµν
(A.15)
and satisﬁes the Bianchi identity
Rµναβ + Rµαβν + Rµβνα = 0.
(A.16)
Finally, the Ricci scalar is
R = gβνRα
ναβ
(A.17)
= −3
4gµαgνβgρλ(∂αgµρ∂βgνλ −∂βgµρ∂αgνλ)
(A.18)
= −3
4gρλ(∂αgαρ∂βgβλ −∂βgαρ∂αgβλ).
(A.19)

S2
Appendix B: Expansion of geometric quantities for a shallow network with ﬁxed weights
In this appendix, we derive formulas for the geometric quantities of a ﬁnite-width shallow network with ﬁxed weights.
Our starting point is the metric (2)
gµν = 1
nφ′(zj)2wjµwjν,
(B.1)
where zj = wj · x + bj is the preactivation of the j-th hidden unit.
1.
Direct derivations for 2D inputs
As a warm-up, we ﬁrst derive the geometric quantities for two-dimensional inputs (d = 2), using simple explicit
formulas for the determinant and inverse of the metric. These derivations have the same content as those in following
sections for general input dimension, but are more straightforward. In this case, we will explicitly write out summations
over hidden units, as we will need to exclude certain index combinations. As the metric is a 2 × 2 symmetric matrix,
we have immediately that
det g = g11g22 −g2
12
(B.2)
= 1
n2
n
X
j,k=1
φ′(zj)2φ′(zk)2(w2
j1w2
k2 −wj1wj2wk1wk2)
(B.3)
= 1
n2
X
k̸=j
φ′(zj)2φ′(zk)2(w2
j1w2
k2 −wj1wj2wk1wk2)
(B.4)
= 1
n2
X
k<j
M 2
jkφ′(zj)2φ′(zk)2
(B.5)
=
1
2n2
X
j,k
M 2
jkφ′(zj)2φ′(zk)2,
(B.6)
where we have deﬁned
Mjk = det

wj1 wj2
wk1 wk2

= wj1wk2 −wj2wk1.
(B.7)
This shows explicitly that the metric is invertible if and only if at least one pair of weight vectors is linearly independent,
as one would intuitively expect.
Moreover, we of course have
gµν =
1
det g

g22
−g12
−g12
g11

.
(B.8)
As we are working in two dimensions, the Riemann tensor has only one independent component, and is entirely
determined by the Ricci scalar [16]:
Rµναβ = R
2 (gµαgνβ −gµβgνα).
(B.9)
Given the permutation symmetry of ∂αgµν, we can combine the results of Appendix A with the simple formula for gµν
to obtain
R =
3
2(det g)2

g11(∂1g22∂2g12 −∂2g22∂1g12)
+ g12(∂1g11∂2g22 −∂2g11∂1g22)
+ g22(∂1g12∂2g11 −∂2g12∂1g11)

(B.10)

S3
In general, we have
∂αgνρ∂βgλγ −∂βgνρ∂αgλγ = 4
n2
X
k̸=j
φ′(zj)φ′′(zj)φ′(zk)φ′′(zk)
× (wjαwkβ −wjβwkα)wjνwjρwkλwkγ
(B.11)
hence, using the fact that Mjj = 0, we have
(det g)2
3
R = 2
n3
n
X
i,j,k=1
Mjkφ′(zi)2φ′(zj)φ′(zk)φ′′(zj)φ′′(zk)
× (w2
i1w2
j2wk1wk2 + wi1wi2w2
j1w2
k2 + w2
i2wj1wj2w2
k1)
(B.12)
As Mkj = −Mjk, we can antisymmetrize the term in the round brackets in those indices, yielding
(det g)2
3
R = 1
n3
n
X
i,j,k=1
Mjkφ′(zi)2φ′(zj)φ′(zk)φ′′(zj)φ′′(zk)
×

(w2
i1w2
j2wk1wk2 + wi1wi2w2
j1w2
k2 + w2
i2wj1wj2w2
k1) −(j ↔k)

.
(B.13)
With a bit of algebra, we have
(w2
i1w2
j2wk1wk2 + wi1wi2w2
j1w2
k2 + w2
i2wj1wj2w2
k1) −(j ↔k) = −MjkMijMik.
(B.14)
Therefore,
R = −
3
n3(det g)2
n
X
i,j,k=1
M 2
jkMijMikφ′(zi)2φ′(zj)φ′(zk)φ′′(zj)φ′′(zk).
(B.15)
As Mii = 0, the non-vanishing contributions to the sum are now triples of distinct indices. We remark that the index i
is singled out in this expression. If n = 2, the Ricci scalar, and thus the Riemann tensor, vanishes identically. This
follows from the fact that in this case the feature map is a change of coordinates on the input space [16, 33]. If n = 3,
we have the relatively simple formula
R =
2
9(det g)2 M12M23M31φ′(z1)φ′(z2)φ′(z3)
×

M23φ′(z1)φ′′(z2)φ′′(z3) −M13φ′(z2)φ′′(z1)φ′′(z3) + M12φ′(z3)φ′′(z1)φ′′(z2)

.
(B.16)
2.
The volume element
We now consider the volume element for general input dimension d. We use the Leibniz formula for determinants in
terms of the Levi-Civita symbol ϵµ1···µd [33, 51]:
det g = ϵµ1···µdg1µ1 · · · gdµd
(B.17)
= 1
d!ϵµ1···µdϵν1···νdgµ1ν1 · · · gµdνd.
(B.18)
This gives
det g = 1
d!ϵµ1···µdϵν1···νdgµ1ν1 · · · gµdνd
(B.19)
=
1
ndd!ϵµ1···µdϵν1···νdφ′(zj1)2 · · · φ′(zjd)2wj1µ1wj1ν1 · · · wjdµdwjdνd
(B.20)
=
1
ndd!φ′(zj1)2 · · · φ′(zjd)2(ϵµ1···µdwj1µ1 · · · wjdµd)(ϵν1···νdwj1ν1 · · · wjdνd)
(B.21)
=
1
ndd!M 2
j1···jdφ′(zj1)2 · · · φ′(zjd)2,
(B.22)

S4
where
Mj1···jd = ϵµ1···µdwj1µ1 · · · wjdµd
(B.23)
= det



wj11 · · · wj1d
...
...
...
wjd1 · · · wjdd



(B.24)
is the minor of the weight matrix obtained by selecting rows j1, . . . , jd. For d = 2, this result agrees with that which
we obtained in Appendix B 1.
3.
The Riemann tensor and Ricci scalar
To compute the curvature for general input dimension, we need the inverse of the metric, which can be expanded
using the Levi-Civita symbol as [51]
gµν =
1
(d −1)! det g ϵµµ2···µdϵνν2···νdgµ2ν2 · · · gµdνd.
(B.25)
Then, applying the results of Appendix A for
∂αgµν = 2
nφ′(zj)φ′′(zj)wjαwjµwjν,
(B.26)
the (4, 0) Riemann tensor is
Rµναβ = −3
4gρλ(∂ρgµα∂λgνβ −∂ρgµβ∂λgνα)
(B.27)
= −
3
nd+1(d −1)! det g φ′(zj2)2 · · · φ′(zjd)2φ′(zi)φ′′(zi)φ′(zk)φ′′(zk)
× ϵρµ2···µdϵλν2···νdwj2µ2wj2ν2 · · · wjdµdwjdνd
× (wiρwiµwiαwkλwkνwkβ −wiρwiµwiβwkλwkνwkα)
(B.28)
= −
3
nd+1(d −1)! det g φ′(zj2)2 · · · φ′(zjd)2φ′(zi)φ′′(zi)φ′(zk)φ′′(zk)
× (ϵρµ2···µdwiρwj2µ2 · · · wjdµd)(ϵλν2···νdwkλwj2ν2 · · · wjdνd)
× (wiµwiαwkνwkβ −wiµwiβwkνwkα)
(B.29)
= −
3
nd+1(d −1)! det g φ′(zj2)2 · · · φ′(zjd)2φ′(zi)φ′′(zi)φ′(zk)φ′′(zk)
× Mij2···jdMkj2···jdwiµwkν(wiαwkβ −wiβwkα).
(B.30)
Raising one index, the (3, 1) Riemann tensor is
Rλ
ναβ = gλµRµναβ
(B.31)
= −
3
n2[nd−1(d −1)! det g]2
× φ′(zl2)2 · · · φ′(zld)2φ′(zj2)2 · · · φ′(zjd)2φ′(zi)φ′′(zi)φ′(zk)φ′′(zk)
× Mij2···jdMkj2···jdMil2···ldϵλν2···νdwl2ν2 · · · wldνdwkν(wiαwkβ −wiβwkα)
(B.32)
hence the Ricci tensor is
Rνβ = Rλ
νλβ
(B.33)
= −
3
n2[nd−1(d −1)! det g]2 φ′(zj2)2 · · · φ′(zjd)2φ′(zl2)2 · · · φ′(zld)2
× φ′(zi)φ′′(zi)φ′(zk)φ′′(zk)
× Mij2···jdMkj2···jdMil2···ldwkν(Mil2···ldwkβ −wiβMkl2···ld).
(B.34)

S5
Finally, the Ricci scalar is
R = gνβRνβ
(B.35)
= −
3
n2[nd−1(d −1)! det g]3
× φ′(zi)φ′′(zi)φ′(zj)φ′′(zj)φ′(zk2)2 · · · φ′(zkd)2φ′(zl2)2 · · · φ′(zld)2φ′(zm2)2 · · · φ′(zmd)2
× Mik2···kdMjk2···kd(M 2
il2···ldM 2
jm2···md −Mil2···ldMjl2···ldMim2···mdMjm2···md).
(B.36)
We now observe that the quantity outside the round brackets is symmetric under interchanging lµ ↔mµ, hence we
may symmetrize the quantity in the round brackets, which, as
(M 2
il2···ldM 2
jm2···md −Mil2···ldMjl2···ldMim2···mdMjm2···md) + (lµ ↔mµ)
(B.37)
= M 2
il2···ldM 2
jm2···md + M 2
im1···mdM 2
jl2···ld −2Mil2···ldMjl2···ldMim2···mdMjm2···md
(B.38)
= (Mil2···ldMjm2···md −Mim2···mdMjl2···ld)2,
(B.39)
yields
R = −
3
2n2[nd−1(d −1)! det g]3 φ′(zi)φ′′(zi)φ′(zj)φ′′(zj)φ′(zk2)2 · · · φ′(zkd)2Mik2···kdMjk2···kd
× φ′(zl2)2 · · · φ′(zld)2φ′(zm2)2 · · · φ′(zmd)2(Mil2···ldMjm2···md −Mim2···mdMjl2···ld)2.
(B.40)
If d = 2, we can show by direct computation that
MilMjm −MimMjl = MijMlm,
(B.41)
hence this result simpliﬁes to
R = −
3
2n5[det g]3 φ′(zi)φ′′(zi)φ′(zj)φ′′(zj)φ′(zk)2MikMjkM 2
ij
× φ′(zl)2φ′(zm)2M 2
lm
(B.42)
= −
3
n3(det g)2 φ′(zi)φ′′(zi)φ′(zj)φ′′(zj)φ′(zk)2MikMjkM 2
ij,
(B.43)
which recovers the formula (B.15) we obtained in Appendix B 1.
4.
Example: error function activations
In this section, we perform explicit computations for error function activations φ(x) = erf(x/
√
2). In this case,
φ′(x) =
p
2/π exp(−x2/2), so
det g =
1
ndd!M 2
j1···jdφ′(zj1)2 · · · φ′(zjd)2
(B.44)
= 1
d!
 2
πn
d
M 2
j1···jd exp[−(z2
j1 + · · · + z2
jd)].
(B.45)
Each contribution to this sum is a Gaussian bump, which we write as
exp[−(z2
j1 + · · · + z2
jd)] = exp [−(Qj1···jd)µν[xµ −(cj1···jd)µ][xν −(cj1···jd)ν]]
(B.46)
for a d × d precision matrix Qj1···jd and a center point cj1···jd. Expanding out the sum of squares in the exponential,
we have
z2
j1 + · · · + z2
jd = (wj1µxµ + bj1)2 + · · · + (wjdµxµ + bjd)2
(B.47)
= (wj1µwj1ν + · · · + wjdµwjdν)xµxν
+ 2(bj1wj1µ + · · · + bjdwjdµ)xµ
+ (b2
j1 + · · · + b2
jd),
(B.48)

S6
from which we can see that the precision matrix is
(Qj1···jd)µν = wj1µwj1ν + · · · + wjdµwjdν,
(B.49)
while the center point is given by (cj1···jd)µ = −(Q−1
j1···jd)µν(bj1wj1ν + · · · + bjdwjdν). Using the Leibniz formula for
determinants (B.17), we have
det Qj1···jd = 1
d!ϵµ1···µdϵν1···νd(Qj1···jd)µ1ν1 · · · (Qj1···jd)µdνd
(B.50)
= 1
d!
d
X
i1,··· ,id=1
ϵµ1···µdϵν1···νdwji1µ1wji1ν1 · · · wjidµdwjidνd
(B.51)
= 1
d!
d
X
i1,··· ,id=1
(ϵµ1···µdwji1µ1 · · · wjidµd)(ϵν1···νdwji1ν1 · · · wjidνd)
(B.52)
= 1
d!
d
X
i1,··· ,id=1
(ϵji1···jid)2M 2
j1···jd
(B.53)
= M 2
j1···jd,
(B.54)
hence we may write
det g = 1
d!
 2
πn
d
det(Qj1···jd) exp

−(Qj1···jd)µν[xµ −(cj1···jd)µ][xν −(cj1···jd)ν]

.
(B.55)
If all the bias terms are zero, then the bump must be centered at the origin.
If the bias terms do not vanish, then the center point is
(cj1···jd)µ
= −(Q−1
j1···jd)µν(bj1wj1ν + · · · + bjdwjdν)
(B.56)
= −
1
(d −1)! det Qj1···jd
d
X
i1,··· ,id=1
ϵµµ2···µdϵνν2···νdwji2µ2wji2ν2 · · · wjidµdwjidνdbji1wji1µ
(B.57)
= −
1
(d −1)! det Qj1···jd
Mj1···jd
d
X
i1,··· ,id=1
ϵνν2···νdwji2ν2 · · · wjidνdbji1ϵji1···jid
(B.58)
= −
1
(d −1)!Mj1···jd
d
X
i1,··· ,id=1
ϵνν2···νdϵji1···jidbji1wji2ν2 · · · wjidνd
(B.59)
= −
1
(d −1)!Mj1···jd
ϵνν2···νdBj1···jd,ν2···νd
(B.60)
where we let
Bj1···jd,ν2···νd = det





bj1 wj1ν2 · · · wj1νd
bj2 wj2ν2 · · · wj2νd
...
...
...
...
bjd wjdν2 · · · wjdνd




.
(B.61)
In general, this is not particularly useful.
In the special case of two-dimensional inputs, we have
det g =
 2
πn
2 X
j<k
det(Qjk) exp

−(Qjk)µν[xµ −(cjk)µ][xν −(cjk)ν]

.
(B.62)
for center
cjk =
1
Mjk

−(bjwk2 −bkwj2)
bjwk1 −bkwj1

(B.63)

S7
-2
-1
0
1
2
-2
-1
0
1
2
x1
x2
det(g), b=0
0.0096
0.0192
0.0288
0.0384
0.0480
0.0576
0.0672
0.0768
0.0864
0.0960
-2
-1
0
1
2
-2
-1
0
1
2
x1
x2
R, b=0
-8.55
-6.65
-4.75
-2.85
-0.95
0.95
2.85
4.75
6.65
8.55
-2
-1
0
1
2
-2
-1
0
1
2
x1
x2
det(g), b=1
0.0032
0.0064
0.0096
0.0128
0.0160
0.0192
0.0224
0.0256
0.0288
0.0320
-2
-1
0
1
2
-2
-1
0
1
2
x1
x2
R, b=1
-8.55
-6.65
-4.75
-2.85
-0.95
0.95
2.85
4.75
6.65
8.55
FIG. B.1. Volume element (left) and Ricci scalar R (right) for erf networks with three hidden units on the unit circle and bias
zero (top) or one (bottom). See text for full description of the setup.
and precision matrix
Qjk =

w2
j1 + w2
k1
wj1wj2 + wk1wk2
wj1wj2 + wk1wk2
w2
i2 + w2
j2

.
(B.64)
As φ′′(x) = −
p
2/πx exp(−x2/2), the Ricci curvature has a similar expansion in terms of Gaussian bumps, but the
bumps are now modulated by products of preactivations.
As an illustrative example, consider an erf network with three hidden units, with biases uniformly equal to b and
weight matrix
W =


1
0
−1/2
√
3/2
−1/2 −
√
3/2

.
(B.65)
In this case, there are three unique pairs of weights that contribute to the volume element: 12, 23, and 13. We can
easily see that M12 = M23 = −M13 = +
√
3/2, and then that the bump centers are at
c12 = b
 −1
−
√
3

,
c23 = b

+2
0

,
and
c13 = b
 −1
+
√
3

.
(B.66)
with precision matrices
Q12 =

5/4
−
√
3/4
−
√
3/4
5/4

,
Q12 =

1/2
0
0
3/2

,
and
Q12 =

5/4
√
3/4
√
3/4
5/4

.
(B.67)
We can also explicitly write out
det g =
1
3π2 e−3
2 (∥x∥2+2b2) h
e(x1+b)2 + e
1
4 (x1+
√
3x2−2b)2 + e
1
4 (x1−
√
3x2−2b)2i
.
(B.68)

S8
Therefore, the volume element has a three-fold rotational symmetry for b > 0, and six-fold symmetry for b = 0.
Considering the Ricci scalar, we can use the explicit formula obtained for three hidden units in Appendix B 1 to work
out that
R = −
1
π3(det g)2 (∥x∥2 −4b2)e−3
2 (∥x∥2+2b2)
(B.69)
= −9π
4
(∥x∥2 −4b2)e
3
2 (∥x∥2+2b2)
h
e(x1+b)2 + e
1
4 (x1+
√
3x2−2b)2 + e
1
4 (x1−
√
3x2−2b)2i2 .
(B.70)
Again, the Ricci scalar has six-fold symmetry if b = 0, and three-fold symmetry if b > 0. We visualize this behavior in
Figure B.1.
Appendix C: Derivation of geometric quantities at inﬁnite width
In this section, we derive the geometric quantities for the inﬁnite-width metric (or, equivalently, the average
ﬁnite-width metric) at initialization:
gµν = Ew∼N(0,σ2Id),b∼N(0,ζ2)[φ′(w · x + b)2wµwν].
(C.1)
For the remainder of this section, we will simply write the expectation over w ∼N(0, σ2Id) and b ∼N(0, ζ2) as E[·].
We let
z ≡w · x + b,
(C.2)
which has an induced N(0, σ2∥x∥2 + ζ2) distribution. We remark that it is easy to show that (C.1) is the metric
induced by the NNGP kernel
k(x, y) = E[φ(w · x + b)φ(w · y + b)]
(C.3)
using the formula [15]
gµν = 1
2
∂2
∂xµ∂xν
k(x, x) −

∂2
∂yµ∂yν
k(x, y)

y=x
(C.4)
for a suﬃciently smooth activation function. We note also that here the diﬀerentiability conditions may be relaxed to
weak diﬀerentiability conditions [22, 25].
Applying Stein’s lemma twice, we have
gµν = E[φ′(z)2wµwν]
(C.5)
= σ2E[φ′(z)2]δµν + 2σ2E[φ′(z)φ′′(z)wν]xµ
(C.6)
= σ2E[φ′(z)2]δµν + 2σ4E[φ′′(z)2 + φ′(z)φ′′′(z)]xµxν.
(C.7)
Then, we can see that the metric is of a special form. Noting that E[φ′(z)2] ≥0 and that
σ2E[φ′′(z)2 + φ′(z)φ′′′(z)] = σ2
d
d(σ2∥x∥2 + ζ2)E[φ′(z)2]
(C.8)
=
d
d∥x∥2 E[φ′(z)2]
(C.9)
by Price’s theorem [52] and the chain rule, we may write
gµν = eΩ(∥x∥2)[δµν + 2Ω′(∥x∥2)xµxν],
(C.10)
where we have deﬁned the function Ω(∥x∥2) by
exp Ω(∥x∥2) ≡σ2E[φ′(z)2].
(C.11)

S9
1.
Geometric quantities for metrics of the form induced by the shallow NNGP kernel
Motivated by the metric induced by the shallow NNGP kernel, we consider metrics of the general form
gµν = eΩ(∥x∥2)[δµν + 2Ω′(∥x∥2)xµxν],
(C.12)
where Ωis a smooth function with derivative Ω′. For brevity, we will henceforth suppress the argument of Ω.
Such metrics have determinant
det g = edΩ(1 + 2∥x∥2Ω′)
(C.13)
by the matrix determinant lemma, and inverse
gµν = e−Ω

δµν −
2Ω′
1 + 2∥x∥2Ω′ xµxν

(C.14)
by the Sherman-Morrison formula. It is also easy to see that the eigenvalues of the metric at any given point x are
eΩ(1 + 2∥x∥2Ω′) with corresponding eigenvector x/∥x∥, and eΩwith multiplicity d −1, with eigenvectors lying in the
null space of x.
We now consider the Riemann tensor. For such metrics, we have
∂αgµν = 2eΩΩ′(xαδµν + xµδαν + xνδαµ) + 4eΩ[Ω′′ + (Ω′)2]xαxµxν,
(C.15)
which is symmetric under permutation of its indices. Then, we may use the simpliﬁed formula for the (4, 0) Riemann
tensor obtained in Appendix A, which yields
Rµναβ = −
3eΩ(Ω′)2
1 + 2∥x∥2Ω′

∥x∥2δµαδνβ +

1 + 2∥x∥2 Ω′′
Ω′

(xνxβδµα + xµxαδνβ) −(α ↔β)

(C.16)
after a straightforward computation, where we have noted that
gρλxρ = e−Ω
1
1 + 2∥x∥2Ω′ xλ
(C.17)
and
gρλxρxλ = e−Ω
∥x∥2
1 + 2∥x∥2Ω′
(C.18)
We can then compute the Ricci scalar
R = gµαgνβRµναβ
(C.19)
= −
3eΩ(Ω′)2
1 + 2∥x∥2Ω′

∥x∥2(gααgββ −gαβgβα)
+ 2

1 + 2∥x∥2 Ω′′
Ω′

(gααgνβxνxβ −gµαgµβxαxβ)

(C.20)
which, as
gααgββ −gαβgβα = e−2Ω

d −2
2∥x∥2Ω′
1 + 2∥x∥2Ω′

(d −1)
(C.21)
and
gααgνβxνxβ −gµαgµβxαxβ = e−2Ω
∥x∥2
1 + 2∥x∥2Ω′ (d −1)
(C.22)
yields
R = −3(d −1)e−Ω(Ω′)2∥x∥2
(1 + 2∥x∥2Ω′)2

d + 2 + 2∥x∥2

(d −2)Ω′ + 2Ω′′
Ω′

.
(C.23)

S10
2.
Examples
As an analytically-tractable example, we consider the error function φ(x) = erf(x/
√
2). For such networks, the
NNGP kernel is
k(x, y) = 2
π arcsin
σ2x · y + ζ2
p
(1 + σ2∥x∥2 + ζ2)(1 + σ2∥y∥2 + ζ2)
,
(C.24)
which is easy to prove using the integral representation of the error function [53]. In this case, we have the simple
result φ′(x) =
p
2/π exp(−x2/2), hence we can easily compute
E[φ′(z)2] =
2
π
p
1 + 2(σ2∥x∥2 + ζ2)
.
(C.25)
This yields
Ω(∥x∥2) = −1
2 log[1 + 2(σ2∥x∥2 + ζ2)] + log 2σ2
π
(C.26)
hence we easily obtain the volume element
p
det g =
2σ2
π
d/2
p
2ζ2 + 1
[1 + 2(σ2∥x∥2 + ζ2)](d+2)/4
(C.27)
and the Ricci scalar
R = −
3π(d −1)(d + 2)σ2∥x∥2
2(2ζ2 + 1)
p
1 + 2(σ2∥x∥2 + ζ2)
.
(C.28)
In this case, it is easy to see that R is negative for all d > 1 and that it is a monotonically decreasing function of ∥x∥,
hence curvature becomes increasingly negative with increasing radius.
Another illustrative example is the monomial φ(x) = xq/
p
(2q −1)!! for integer q ≥1, normalized such that
k(x, x) =
1
(2q −1)!!E[z2q] = (σ2∥x∥2 + ζ2)q.
(C.29)
Though this is not required to obtain the metric, an explicit formula for the NNGP kernel for two distinct inputs can
be obtained using the Mehler expansion of the bivariate Gaussian density [22, 27], or by direct computation using
Isserlis’ theorem [25]. Following the ﬁrst approach, we expand the kernel as
k(x, y) =
1
(2q −1)!!Ew,b[(w · x + b)q(w · y + b)q]
(C.30)
= [(σ2∥x∥2 + ζ2)(σ2∥y∥2 + ζ2)]q/2
1
(2q −1)!!E[uqvq],
(C.31)
where we have

u
v

∼N

0
0

,

1 ρ
ρ 1

(C.32)
for
ρ =
σ2x · y + ζ2
p
(σ2∥x∥2 + ζ2)(σ2∥y∥2 + ζ2)
.
(C.33)
Then, using the Mehler expansion, we have
E[uqvq] =
∞
X
k=0
ρk
k! Et∼N(0,1)[Hek(t)tq]2
(C.34)

S11
where Hek(t) is the k-th probabilist’s Hermite polynomial. Using the inversion formula
tq = q!
j q
2
k
X
m=0
1
2mm!(q −2m)!Heq−2m(t)
(C.35)
and the orthogonality relation
Et∼N(0,1)[Hek(t)Heq−2m(t)] = (q −2m)!δk,q−2m,
(C.36)
we have
Et∼N(0,1)[Hek(t)tq] = q!
j q
2
k
X
m=0
1
2mm!δk,q−2m.
(C.37)
Let us ﬁrst consider the case in which q is even. Let q = 2ℓ. Then, only terms with even k contribute, and, writing
k = 2j, we have
E[uqvq] =
∞
X
j=0
ρ2j
(2j)!
"
(2ℓ)!
ℓ
X
m=0
1
2mm!δj,ℓ−m
#2
(C.38)
=
ℓ
X
j=0
ρ2j
(2j)!

(2ℓ)!
2ℓ−j(ℓ−j)!
2
(C.39)
= [(2ℓ−1)!!]2
2F1

−ℓ, −ℓ; 1
2; ρ2

,
(C.40)
where 2F1 is the Gauss hypergeometric function [54]. Now consider the case in which q is odd. Letting q = 2ℓ+ 1,
only terms with odd k = 2j + 1 contribute, and we have
E[uqvq] =
∞
X
j=0
ρ2j+1
(2j + 1)!
"
(2ℓ+ 1)!
ℓ
X
m=0
1
2mm!δj,ℓ−m
#
(C.41)
=
ℓ
X
j=0
ρ2j+1
(2j + 1)!
 (2ℓ+ 1)!
2ℓ−j(ℓ−j)!
2
(C.42)
= [(2ℓ+ 1)!!]2ρ 2F1

−ℓ, −ℓ, 3
2, ρ2

.
(C.43)
Combining these results, we obtain an expansion for the kernel.
For these activation functions, we have
E[φ′(z)2] =
q2
2q −1(σ2∥x∥2 + ζ2)q−1,
(C.44)
yielding the volume element
p
det g =
s
1 + 2(q −1)
σ2∥x∥2
σ2∥x∥2 + ζ2
q2σ2(σ2∥x∥2 + ζ2)q−1
2q −1
d/2
(C.45)
and the Ricci scalar
R = −3(d −1)(q −1)2(2q −1)σ2∥x∥2[(d + 2)ζ2 + (d −2)(2q −1)σ2∥x∥2]
q2(σ2∥x∥2 + ζ2)q[(2q −1)σ2∥x∥2 + ζ2]2
.
(C.46)
If ζ = 0, this simpliﬁes substantially to
p
det g = qd(2q −1)(1−d)/2σdq∥x∥(q−1)d
(C.47)

S12
q=2
q=3
q=4
q=5
q=6
q=7
q=8
q=9
q=10
0.2
0.4
0.6
0.8
1.0
|x|
-1.5
-1.0
-0.5
R; d=2
d=2
d=3
d=4
d=5
d=6
d=7
d=8
d=9
d=10
0.5
1.0
1.5
2.0
|x|
-20
-15
-10
-5
R; q=2
FIG. C.1. Ricci curvature scalar R (C.46) as a function of input modulus ∥x∥for monomial activation function NNGPs of
varying degree q and input dimension d. At left, we show the eﬀect of varying the degree q (with lighter shades of red indicated
higher degrees) for ﬁxed dimension d = 2. At right, we show the eﬀect of varying the dimension d (with lighter shades of purple
indicating higher degrees) for ﬁxed degree q = 2. In all cases, the weight and bias variances are ﬁxed to unity, i.e., σ2 = ζ2 = 1.
and
R

ζ=0
= −3(d −1)(d −2)(q −1)2
q2(σ2∥x∥2)q
.
(C.48)
For all ζ ≥0, all dimensions d ≥1, and all q > 1, √det g is a monotone increasing function of ∥x∥2.
The Ricci curvature is somewhat more complicated. First, we can see that R = 0 if q = 1 or d = 1, which we would
expect. We can then restrict our attention to d > 1 and q > 1. If ζ = 0, R = 0 if d = 2 and R < 0 for all d > 2, but,
unlike for the error function, |R| is monotonically decreasing with ∥x∥. We now consider ζ > 0. By diﬀerentiation, we
have
∂R
∂(σ2∥x∥2) ∝(d −2)(2q −1)2q(σ2∥x∥2)3 + (2q −1)[(2q −1)d + 6]ζ2(σ2∥x∥2)2
−[8 + q(d −14)]ζ4(σ2∥x∥2) −(d + 2)ζ6,
(C.49)
where the implied constant of proportionality is strictly positive. This suggests that R is non-monotonic, with an
initial decrease followed by a gradual increase towards zero as ∥x∥→∞. We illustrate this behavior across degrees
q and input dimensions d in Figure C.1. In d = 2, we have the simpliﬁcation that the equation ∂R/∂(σ2∥x∥2) = 0
is quadratic rather than cubic, and we ﬁnd easily that ∂R/∂(σ2∥x∥2) < 0 if σ2∥x∥2 < C, ∂R/∂(σ2∥x∥2) = 0 if
σ2∥x∥2 = C, and ∂R/∂(σ2∥x∥2) > 0 if σ2∥x∥2 > C, where the threshold value is determined by
[2q2 + q −1]C2 + (3q −2)ζ2C −ζ4 = 0,
(C.50)
hence
C =
p
17q2 −8q −3q + 2
2(2q2 + q −1)
ζ2.
(C.51)
For q = 2, this gives
√
C ≃0.42ζ, which is consistent with our numerical results in Figure 1.
Appendix D: Comparing the shallow Neural Tangent Kernel to the NNGP
In this section, we compare the shallow NTK to the shallow NNGP. For a shallow network
f(x) =
1
√n
n
X
j=1
vjφ(wj · x + bj),
(D.1)

S13
the empirical NTK is
Θe(x, y) = 1
n
n
X
j=1
φ(wj · x + bj)φ(wj · y + bj)
+ 1
n
n
X
j=1
v2
j φ′(wj · x + bj)φ′(wj · y + bj)(1 + x · y)
(D.2)
and, taking w ∼N(0, σ2Id), b ∼N(0, ζ2), and v ∼N(0, ξ2In), the inﬁnite-width NTK is
Θ(x, y) = E[φ(w · x + b)φ(w · y + b)] + ξ2E[φ′(w · x + b)φ′(w · y + b)](1 + x · y)
(D.3)
where the remaining expectations are taken over w and b.
Writing z ≡w · x + b, we have
∂2
∂xµ∂xν
Θ(x, x)
=
∂2
∂xµ∂xν

E[φ(z)2] + ξ2E[φ′(z)2](1 + ∥x∥2)

(D.4)
=
∂
∂xµ

2E[φ(z)φ′(z)wν] + 2ξ2E[φ′(z)φ′′(z)wν](1 + ∥x∥2) + 2ξ2E[φ′(z)2]xν

(D.5)
= 2E[{φ′(z)2 + φ(z)φ′′(z)}wµwν] + 2ξ2E[{φ′′(z)2 + φ′(z)φ′′′(z)}wµwν](1 + ∥x∥2)
+ 4ξ2E[φ′(z)φ′′(z)wν]xµ + 4ξ2E[φ′(z)φ′′(z)wµ]xν + 2ξ2E[φ′(z)2]δµν
(D.6)
while
∂2
∂yµ∂yν
Θ(x, y)
=
∂2
∂yµ∂yν

E[φ(w · x + b)φ(w · y + b)] + ξ2E[φ′(w · x + b)φ′(w · y + b)](1 + x · y)

(D.7)
=
∂
∂yµ

E[φ(w · x + b)φ′(w · y + b)wν] + ξ2E[φ′(w · x + b)φ′′(w · y + b)wν](1 + x · y)
+ ξ2E[φ′(w · x + b)φ′(w · y + b)]xν

(D.8)
= E[φ(w · x + b)φ′′(w · y + b)wµwν] + ξ2E[φ′(w · x + b)φ′′′(w · y + b)wµwν](1 + x · y)
+ ξ2E[φ′(w · x + b)φ′′(w · y + b)wν]xµ + ξ2E[φ′(w · x + b)φ′′(w · y + b)wµ]xν
(D.9)
hence

∂2
∂yµ∂yν
Θ(x, y)

y=x
= E[φ(z)φ′′(z)wµwν] + ξ2E[φ′(z)φ′′′(z)wµwν](1 + ∥x∥2)
+ ξ2E[φ′(z)φ′′(z)wν]xµ + ξ2E[φ′(z)φ′′(z)wµ]xµ.
(D.10)
Therefore, we have
gµν = 1
2
∂2
∂xµ∂xν
Θ(x, x) −

∂2
∂yµ∂yν
Θ(x, y)

y=x
(D.11)
= E[φ′(z)2wµwν] + ξ2E[φ′′(z)2wµwν](1 + ∥x∥2)
+ ξ2E[φ′(z)φ′′(z)wν]xµ + ξ2E[φ′(z)φ′′(z)wµ]xν + ξ2E[φ′(z)2]δµν.
(D.12)
By Stein’s lemma, as in the NNGP case,
E[φ′(z)2wµwν] = σ2E[φ′(z)2]δµν + 2σ4E[φ′′(z)2 + φ′(z)φ′′′(z)]xµxν
(D.13)

S14
and
E[φ′′(z)2wµwν] = σ2E[φ′′(z)2]δµν + 2σ4E[φ′′′(z)2 + φ′′(z)φ′′′′(z)]xµxν
(D.14)
while
E[φ′(z)φ′′(z)wν] = σ2E[φ′′(z)2 + φ′(z)φ′′′(z)]xν,
(D.15)
and therefore
gµν
= E[φ′(z)2wµwν] + ξ2E[φ′′(z)2wµwν](1 + ∥x∥2)
+ ξ2E[φ′(z)φ′′(z)wν]xµ + ξ2E[φ′(z)φ′′(z)wµ]xν + ξ2E[φ′(z)2]δµν
(D.16)
=

(σ2 + ξ2)E[φ′(z)2] + σ2ξ2E[φ′′(z)2](1 + ∥x∥2)

δµν
+ 2σ2

(σ2 + ξ2)E[φ′′(z)2 + φ′(z)φ′′′(z)] + σ2ξ2E[φ′′′(z)2 + φ′′(z)φ′′′′(z)](1 + ∥x∥2)

xµxν
(D.17)
This metric is not quite of the special form of the NNGP metric, as
d
d∥x∥2

(σ2 + ξ2)E[φ′(z)2] + σ2ξ2E[φ′′(z)2](1 + ∥x∥2)

= σ2

(σ2 + ξ2)E[φ′′(z)2 + φ′(z)φ′′′(z)] + σ2ξ2E[φ′′′(z)2 + φ′′(z)φ′′′′(z)](1 + ∥x∥2)

+ σ2ξ2E[φ′′(z)2]
(D.18)
by Price’s theorem, hence we have
gµν = ω(∥x∥2)δµν + 2

ω′(∥x∥2) −σ2ξ2E[φ′′(z)2]

xµxν
(D.19)
for
ω(∥x∥2) = (σ2 + ξ2)E[φ′(z)2] + σ2ξ2E[φ′′(z)2](1 + ∥x∥2)
(D.20)
Even though the geometric quantities associated with this metric are not as easy to compute as those for the NNGP
kernel, we can see that it shares similar symmetries. In particular, it still takes the form of a projection, and the
volume element will depend on the input only through its norm.
Appendix E: Perturbative ﬁnite-width corrections in shallow Bayesian neural networks
In this section, we describe how one may compute perturbative corrections to geometric quantities in shallow
Bayesian neural networks at large but ﬁnite width [25, 35]. For simplicity, we will focus on corrections to the volume
element. We will not go through the straightforward but tedious exercise of computing perturbative corrections to the
Riemann tensor and Ricci scalar [33]. We will follow the notation and formalism of Zavatone-Veth et al. [25]; we could
equivalently use the formalism of Roberts et al. [35].
We begin by considering the eﬀect of a general perturbation to the metric that preserves the index-permutation
symmetry of its derivatives. We write the metric tensor as gµν = ¯gµν + hµν for some background metric ¯gµν and a
small perturbation hµν. We assume that both ∂α¯gµν and ∂αhµν are completely symmetric under permutation of their
indices. By Jacobi’s formula for the variation of a determinant, we have
det g = [1 + ¯gµνhµν + O(h2)] det ¯g,
(E.1)
hence the volume element expands as
p
det g =

1 + 1
2 ¯gµνhµν + O(h2)
 p
det ¯g.
(E.2)

S15
1.
Metric perturbations for a wide Bayesian neural network
We now consider the concrete setting of a Bayesian neural network with a single hidden layer of large but ﬁnite
width n and m-dimensional output,
f(x; W, V) =
1
√n
n
X
i=1
viφ(wi · x).
(E.3)
We ﬁx isotropic standard Gaussian priors over the weights, and, for a training dataset {(xa, ya)}p
a=1 of p examples,
choose an isotropic Gaussian likelihood of inverse variance β:
p({(xa, ya)}p
a=1 | W, V) ∝exp
 
−β
2
p
X
a=1
∥f(xa; W, V) −ya∥2
2
!
.
(E.4)
We denote expectation with respect to the resulting Bayes posterior by ⟨·⟩. Our choice of unit-variance priors is made
without much loss of generality, as changing the prior variance does not change the qualitative structure of perturbative
feature learning [25].
Using the nomenclature of Zavatone-Veth et al. [25] and Roberts et al. [35], the metric
gµν = 1
n
n
X
i=1
φ′(wj · x)2wiµwiν
(E.5)
is a hidden layer observable, with inﬁnite-width limit given by the metric ¯gµν associated with the NNGP kernel of the
network,
¯gµν = EWgµν = Ew∼N(0,Id)[φ′(w · x)2wµwν]
(E.6)
where EW denotes expectation with respect to the prior distribution. Then, we may apply the result of Zavatone-Veth
et al. [25] for the perturbative expansion of posterior moments of gµν at large width. To state the result, we must ﬁrst
introduce some notation. Let
k(x, y) = 1
n
n
X
i=1
φ(wi · x)φ(wi · y)
(E.7)
be the hidden layer kernel, and
¯k(x, y) = Ew∼N(0,Id)[φ(w · x)φ(w · y)]
(E.8)
be its inﬁnite-width limit, i.e., the NNGP kernel. Then, deﬁne the p × p matrix
Ψ ≡Γ−1GyyΓ−1 −Γ−1,
(E.9)
where the p × p matrix Γ is deﬁned as
Γab ≡¯k(xa, xb) + β−1δab
(E.10)
and Gyy is the normalized target Gram matrix,
(Gyy)ab = 1
mya · yb.
(E.11)
Then, the result of Zavatone-Veth et al. [25] yields the perturbative expansion
⟨gµν⟩= ¯gµν + 1
2m
p
X
a,b=1
Ψab covw[k(xa, xb), gµν] + O
 1
n2

(E.12)
where the required posterior covariance can be explicitly expressed as
n covw[k(xa, xb), gµν] = Ew∼N(0,Id)[φ(za)φ(zb)φ′(z)2wµwν] −¯k(xa, xb)¯gµν,
(E.13)

S16
where we write za ≡w · xa and z ≡w · x. This formula alternatively follows from applying the result of Zavatone-Veth
et al. [25] for the asymptotics of the mean kernel, and then using the formula for the metric in terms of derivatives of
the kernel [15].
In general, this expression is somewhat unwieldy, and the integrals are challenging to evaluate in closed form.
However, the situation simpliﬁes dramatically if we train with only a single datapoint, and focus on the zero-temperature
limit β →∞. In this case, we can set m = 1 with very little loss of generality. We then have the simpliﬁed expression
⟨gµν⟩= ¯gµν + 1
2

y2
a
Ew[φ(za)2] −1
 covw[k(xa, xa), gµν]
Ew[φ(za)2]
+ O
 1
n2

(E.14)
hence, to the order of interest, the volume element expands as
⟨√det g⟩
√det ¯g
=
p
det⟨g⟩
√det ¯g + O
 1
n2

(E.15)
= 1 + 1
4n

y2
a
Ew[φ(za)2] −1

(χ −d) + O
 1
n2

,
(E.16)
where we have deﬁned
χ ≡¯gµνEw[φ(za)2φ′(z)2wµwν]
Ew[φ(za)2]
(E.17)
and used the facts that ¯gµν¯gµν = d and ¯k(xa, xa) = Ew[φ(za)2].
From Appendix C, we know that
¯gµν = e−Ω

δµν −
2Ω′
1 + 2∥x∥2Ω′ xµxν

(E.18)
for Ω(∥x∥2) deﬁned by
exp Ω(∥x∥2) ≡Ew[φ′(z)2],
(E.19)
so we have
χ =
1
eΩEw[φ(za)2]Ew[φ(za)2φ′(z)2wµwνδµν]
−
1
eΩEw[φ(za)2]
2Ω′
1 + 2∥x∥2Ω′ Ew[φ(za)2φ′(z)2z2]
(E.20)
Using Stein’s lemma, we have
Ew[φ(za)2φ′(z)2wµwνδµν] = dEw[φ(za)2φ′(z)2]
+ 2Ew[φ(za)φ′(za)zaφ′(z)2 + φ(za)2φ′(z)φ′′(z)z].
(E.21)
Importantly, this setting can be applied to a simple classiﬁcation task. Consider training a network on two points of
the XOR task, (1, 0) 7→0 and (0, 1) 7→1
2.
A tractable example: monomial activation functions
We now specialize to the case of φ(x) = xq/
p
(2q −1)!!, in which all of the required expectations can be evaluated
analytically. In this case, we have the explicit formula
exp Ω(∥x∥2) = E[φ′(z)2] =
q2
2q −1∥x∥2q−2.
(E.22)
Noting that
Ew[φ(za)φ′(za)zaφ′(z)2] = qEw[φ(za)2φ′(z)2]
(E.23)

S17
and
Ew[φ(za)2φ′(z)φ′′(z)z] = (q −1)Ew[φ(za)2φ′(z)2],
(E.24)
we have
Ew[φ(za)2φ′(z)2wµwνδµν] = [d + 2(2q −1)]Ew[φ(za)2φ′(z)2]
(E.25)
=
q2
[(2q −1)!!]2 [d + 2(2q −1)]Ew[z2q
a z2q−2]
(E.26)
and, similarly,
Ew[φ(za)2φ′(z)2z2] =
q2
[(2q −1)!!]2 Ew[z2q
a z2q].
(E.27)
Then, using the formula for eΩand the fact that Ew[φ(za)2] = ∥xa∥2q, we have
χ(ρ2) =
(2q −1)
[(2q −1)!!]2 [d + 2(2q −1)]Ew[u2q
a u2q−2] −
2(q −1)
[(2q −1)!!]2 Ew[u2q
a u2q]
(E.28)
where we have let

ua
u

=

za/∥xa∥
z/∥x∥

∼N

0
0

,

1 ρ
ρ 1

(E.29)
for
ρ =
xa · x
∥xa∥∥x∥.
(E.30)
In general, we have
1
[(2q −1)!!]2 Ew[u2q
a u2q] = 2F1

−q, −q; 1
2; ρ2

(E.31)
and
2q −1
[(2q −1)!!]2 Ew[u2q
a u2q−2] = 2F1

1 −q, −q; 1
2; ρ2

(E.32)
in terms of the Gauss hypergeometric function [54]. Thus, we have
χ(ρ2) = [d + 2(2q −1)]2F1

1 −q, −q; 1
2; ρ2

−2(q −1)2F1

−q, −q; 1
2; ρ2

.
(E.33)
Each of these hypergeometric functions is a polynomial with all-positive coeﬃcients in ρ2, and both evaluate to
unity when ρ = 0 [54]. At small order, we have
χ

q=1
−d = 2
(E.34)
for linear networks and
χ

q=2
−d = 4
4
3ρ4 + (d + 2)ρ2 + 1

(E.35)
for quadratic networks. Then, at ρ = 0, one can easily show that
χ(ρ = 0) = d + 2q
(E.36)
and, as ρ increases from zero to one, χ increases monotonically (for all q > 1; for q = 1 it is constant) to
lim
ρ↑1 χ(1) = (2(2q −1) −1)!!
[(2q −1)!!]2
[(2q −1)d + 2q]
(E.37)

S18
q=1
q=2
q=3
q=4
q=5
q=6
q=7
q=8
q=9
q=10
0.0
0.2
0.4
0.6
0.8
1.0
ρ
0.0
0.2
0.4
0.6
0.8
1.0
χ(ρ)/χ(1); d=2
q=1
q=2
q=3
q=4
q=5
q=6
q=7
q=8
q=9
q=10
0.0
0.2
0.4
0.6
0.8
1.0
ρ
0.0
0.2
0.4
0.6
0.8
1.0
χ(ρ)/χ(1); d=100
FIG. E.1. Normalized perturbative expansion factor χ(ρ2)/χ(ρ = 1) as a function of overlap ρ for Bayesian MLPs with monomial
activation functions of varying degree q (with larger q indicated by lighter shades of red) in d = 2 (left) and d = 100 (right). See
main text for details.
using formulas for hypergeometric functions of unit argument [54]. Thus, χ(ρ2) −d is strictly positive for all ρ, d,
and q. We plot χ(ρ2)/χ(1) for varying degrees in d = 2 and d = 100 in Figure E.1 to illustrate the increasing relative
separation between ρ(0) and ρ(1) with increasing d and q.
Collecting our results, the general expansion (E.15) for the magniﬁcation factor simpliﬁes to
⟨√det g⟩
√det ¯g
= 1 + 1
4n

y2
a
∥xa∥2q −1

(χ(ρ2) −d) + O
 1
n2

.
(E.38)
As χ(ρ2) > d for all ρ, we can see that if ∥xa∥2q > y2
a, the leading correction compresses areas, while if ∥xa∥2q < y2
a, it
expands them. As χ(ρ2) −d is monotonically increasing in the overlap ρ =
xa·x
∥xa∥∥x∥, the expansion or contraction is
minimal for points orthogonal to the training example xa, and maximal for points parallel to the training example.
We remark that the dependence on the scale of xa relative to ya parallels the conditions under which generalization
error decreases with increasing width in deep linear networks [25, 55]: with unit-variance priors, increasing width is
beneﬁcial if
y2
a
∥xa∥2 > 1
(E.39)
as shown in Zavatone-Veth et al. [25]. This is a simple consequence of the fact that the sign of the leading perturbative
correction is determined by the same quantity. We note that, under the prior, as v ∼N(0, In),
Ew,v[f(xa)2] = Ew[φ(za)2]
(E.40)
= ∥xa∥2q,
(E.41)
so this is a comparison of the variance of the function output under the prior to the target magnitude.
As an example, consider training a network on one point of the XOR task: (1, 1) 7→0. In this case, ∥xa∥2q > y2
a, so
the volume element will be contracted everywhere, maximally along the line x1 = x2 and minimally orthogonal to this
line.
Appendix F: The geometry of recently-proposed kernel learning algorithms
In this appendix, we analyze the changes in representational geometry resulting from kernel learning algorithms
recently proposed by Radhakrishnan et al. [4] and Simon et al. [56].
1.
The supervised kernel learning algorithm of Radhakrishnan et al.
We ﬁrst discuss the method for iterative supervised kernel learning proposed by Radhakrishnan et al. [4]. Their
method starts with a translation-invariant kernel of the general form
kM(x, y) = h(∥x −y∥2
M),
(F.1)

S19
where h : R≥0 →R≥0 is a suitably smooth scalar function and
∥x −y∥2
M = (x −y)⊤M(x −y)
(F.2)
is the squared Mahalanobis distance between x and y for a constant positive-semideﬁnite symmetric matrix M.
Then, for a dataset {(xa, ya)}p
a=1, they initialize M = Id, ﬁt a kernel machine
fM(x) =
p
X
a=1
ya(K−1
M )abkM(xb, x)
(F.3)
for (KM)ab = kM(xa, xb) the kernel Gram matrix, update
Mµν ←1
p
p
X
a=1
∂fM
∂xµ
(xa)∂fM
∂xν
(xa)
(F.4)
to the expected gradient outer product, and repeat.
For a smoothed Laplace kernel, Radhakrishnan et al. [4] show that this method achieves accuracy on simple image
(CelebA) and tabular datasets that is competitive with fully-connected neural networks. They also link the expected
gradient outer product to the ﬁrst-layer weight matrix W1 ∈Rn×d of a fully-connected network, showing that
W⊤
1 W1 ≃M.
But, using the formula [15]
gµν = 1
2
∂2
∂xµ∂xν
k(x, x) −

∂2
∂yµ∂yν
k(x, y)

y=x
,
(F.5)
the kernel (F.1) induces a metric
gµν = 1
2
∂2
∂xµ∂xν
h(0) −

∂2
∂yµ∂yν
h[(x −y)⊤M(x −y)]

y=x
(F.6)
= −2h′(0)Mµν.
(F.7)
on the input space. This generalizes the result of Burges [15] for M = Id to general M. This metric is constant over
the entire input space, and is therefore ﬂat [16]. Thus, Radhakrishnan et al. [4]’s method achieves strong performance
on certain tasks despite the fact that it results in kernels that always induce ﬂat metrics.
In a footnote, Radhakrishnan et al. [4] comment that their method can be extended to general, non-translation-
invariant base kernels k(x, y) by deﬁning the transformed kernel
kM(x, y) = k(M1/2x, M1/2y)
(F.8)
for a symmetric positive-deﬁnite matrix M with symmetric positive-deﬁnite square root M1/2. However, they do not
test the performance of the algorithm for non-translation-invariant base kernels. In this more general case, the metric
gM induced by kM has components
(gM)µν = 1
2
∂2
∂xµ∂xν
k(M1/2x, M1/2x) −

∂2
∂yµ∂yν
k(M1/2x, M1/2y)]

y=x
(F.9)
= (M 1/2)µρ(M 1/2)νλgµλ,
(F.10)
where gµλ is the metric induced by the base kernel k(x, y), evaluated at the point (M1/2x, M1/2y). Using the
multiplicative property of the determinant, we thus have
det gM = (det M) det g.
(F.11)
Therefore, even in its most general form this kernel learning algorithm aﬀects the volume element only through an
overall rescaling and the global change of coordinates x 7→M1/2x.

S20
2.
The self-supervised kernel learning algorithm of Simon et al.
We now consider the self-supervised kernel learning algorithm proposed in very recent work by Simon et al. [56].
This method starts with a base kernel
k(x, y)
(F.12)
and a dataset of positive pairs {(xa, x′
a)}p
a=1. Deﬁne the p × p kernel matrix KXX by (KXX )ab = k(xa, xb), and deﬁne
KXX ′, KX ′X , KX ′X ′ analogously. Let
˜K =

KXX
KXX ′
KX ′X KX ′X ′

∈R2p×2p
(F.13)
be the combined kernel, and let
Z = 1
2n

KXX ′KXX
K2
XX ′
KX ′X ′KXX KX ′X ′KXX ′

+ (transpose) ∈R2p×2p.
(F.14)
Deﬁne the symmetric matrix
KΓ = ˜K−1/2Z ˜K−1/2 ∈R2p×2p,
(F.15)
where we interpret the inverses as pseudoinverses if necessary. Finally, for some integer d, let
K≤d
Γ
(F.16)
be the matrix formed by discarding all but the top d eigenvalues of KΓ, and let (K≤d
Γ )+ be its pesudoinverse. That is,
if Kγ has an orthogonal eigendecomposition KΓ = U diag(γ1, . . . , γ2p)U⊤for ordered eigenvalues γ1 ≥γ2 ≥· · · ≥γ2p,
then K≤d
Γ
= U diag(γ1, . . . , γd, 0, . . . , 0)U⊤. Here, we neglect the possibility of degenerate eigenvalues for convenience,
and assume that KΓ has at least d nonzero eigenvalues. In terms of this eigendecomposition, we have (K≤d
Γ )+ =
U diag(1/γ1, . . . , 1/γd, 0, . . . , 0)U⊤. Then, their method returns a modiﬁed kernel
kss(x, y) =

KxX
KxX ′
⊤
˜K−1/2(K≤d
Γ )+ ˜K−1/2

KyX
KyX ′

,
(F.17)
where we deﬁne the p-dimensional vectors KxX and KxX ′ by (KxX )a = k(x, xa) and (KxX ′)a = k(x, x′
a), respectively.
For brevity, we deﬁne the 2p × 2p spatially constant matrix
Q = ˜K−1/2(K≤d
Γ )+ ˜K−1/2,
(F.18)
such that
kss(x, y) =

KxX
KxX ′
⊤
Q

KyX
KyX ′

.
(F.19)
The kernel kss induces a metric
(gss)µν = 1
2
∂2
∂xµ∂xν
kss(x, x) −

∂2
∂yµ∂yν
kss(x, y)

y=x
(F.20)
=

∂xµKxX
∂xµKxX ′
⊤
Q

∂xνKxX
∂xνKxX ′

,
(F.21)
which for general base kernels will diﬀer substantially from that induced by the base kernel.
Appendix G: Numerical methods and supplemental ﬁgures
1.
XOR and sinusoidal tasks
As an especially simple toy problem, we begin by training neural networks to perform a standard XOR classiﬁcation
task. Single-hidden-layer fully-connected networks with Sigmoid non-linearities are initialized with widths [2, w, 2]
where w = 2 and trained on a dataset consisting of the four points
{(−1, −1), (−1, 1), (1, −1), (1, 1)}
(G.1)

S21
FIG. G.1. Evolution of the volume element over training in a network trained to perform an XOR classiﬁcation task (single
hidden layer, two hidden units). Red lines indicate the decision boundaries of the network. See Appendix G 1 for experimental
details and visualizations for higher hidden dimensions. Note that for two hidden unit case, the curvature is identically zero.
with respective labels {0, 1, 1, 0}. Networks are trained via stochastic gradient descent (learning rate 0.02, momentum
0.9, and weight decay 10−4) with cross entropy-loss for 2000 epochs. As is standard practice in geometric representation
learning [57, 58], in all tests we adopt 64-bit ﬂoating point precision (float64) to minimize instabilities [59].
In addition to architectures with two hidden units, we also attempted higher dimensions. The redundancy in this
case gives rise to vastly diﬀerent patterns from the architecture with a width of 2. Figure G.2, Figure G.3, and
Figure G.4 visualize the training dynamics of Ricci curvature, volume element, and prediction (decision boundary) for
w ∈{10, 100, 250, 500} hidden units using the Sigmoid non-linearity. As we increase the width of the hidden layer,
both Ricci and volume elements get spherically symmetric as expected, since in the limit the volume element and
curvature only depends on the norm of the query point (Appendix C).
For a slightly more complex toy problem, we train neural networks to classify points according to a sinusoidal
decision boundary. Two-hidden-layer fully-connected networks are initialized with widths [2,8,8,2] and trained on a
dataset consisting of uniformly random sampled 400 points (x, y) ∈[−1, 1] × [−1, 1] with labels
(
1
y > 3
5 sin (7x −1)
0
y ≤3
5 sin (7x −1)
(G.2)
Networks are trained via stochastic gradient descent (learning rate 0.05, momentum 0.9, and zero weight decay) with
cross-entropy loss for 10,000 epochs.
In both cases, we calculate the volume element and Ricci scalar induced by the network at 1,600 points evenly
spaced in [−1.5, 1.5] × [−1.5, 1.5] periodically throughout training (the magnitudes of these two quantities at each of
the 1,600 points are plotted as heat maps in Figures G.1 and 2). The metric we consider is the one induced by the map
from input space to the ﬁrst hidden layer of the network (in the case of XOR, the single hidden layer), i.e., the feature
map Φj(x) = n−1/2φ(wj · x + bj) for weights wj, biases bj, and activation function φ. The metric is then calculated as
gµν = ∂µΦi∂νΦi,
(G.3)
The volume element induced by the network is then dV (x) =
p
det gµν(x), which we can compute directly using the
explicit formula (B.6) from Appendix B. We use the explicit formula (B.15) to compute the Ricci scalar. We avoid
all-purpose automatic-diﬀerentiation-based curvature computation, since we have empirically observed that automatic
diﬀerentiation leads to a consistent overestimation of the Ricci curvature quantities due to numerical issues.

S22
FIG. G.2. Ricci curvature for XOR classiﬁcation, with an architecture [2, w, 2] for w = 10 (ﬁrst row), w = 100 (second row),
w = 250 (third row), and w = 500 (forth row) hidden units across diﬀerent epochs. As the number of hidden units increase, the
curvature structure grows increasingly regularized and spherically symmetric.

S23
FIG. G.3. Volume element for XOR classiﬁcation, with an architecture [2, w, 2] for w = 10 (ﬁrst row), w = 100 (second row),
w = 250 (third row), and w = 500 (forth row) hidden units across diﬀerent epochs. The volume element structures likewise
become spherically symmetric as the number of hidden units increase.

S24
FIG. G.4. Prediction for XOR classiﬁcation, with architecture [2, w, 2] for w = 10 (ﬁrst row), w = 100 (second row), w = 250
(third row), and w = 500 (forth row) hidden units across diﬀerent epochs. As the hidden units increase, the prediction boundary
converges to XOR quicker.

S25
FIG. G.5. Evolution of the volume element over training in a network with with architecture [2, w, 2] for w = 5 (ﬁrst row),
w = 20 (second row), and w = 250 (third row) hidden units across diﬀerent epochs trained to classify points separated by
a sinusoidal boundary y = 3
5 sin(7x −1). Red lines indicate the decision boundaries of the network. See Appendix G 1 for
experimental details and visualizations at other widths.

S26
FIG. G.6. Evolution of the Ricci curvature over training in a network trained to classify points separated by a sinusoidal
boundary (single hidden layer with 5 hidden units (top), 20 hidden units (mid), and 250 hidden units (bottom)), clipped between
-100 and 100 for visualization purposes. Red lines indicate the decision boundaries of the network. More hidden units oﬀer
smoother curvature transition when traversing the boundary, though the pattern presented here is less illustrative than the
volume element in Figure 2.

S27
FIG. G.7. Evolution of the volume element over training in an architecture of [2, 8, 8, 8, 2] trained to classify points separated
by a sinusoidal boundary. From left to right, each panel correspond to the feature map induced by the ﬁrst, second, and third
hidden layers. Red lines indicate the decision boundaries of the network. Note that the learning is performed predominantly at
the ﬁrst layer, and later layers oﬀer a better demarcation by polarizing the volume elements at regions near and away from the
decision boundaries. See Appendix G 1 for experimental details. More hidden units oﬀer better approximation to the sinusoid
curve.

S28
2.
Shallow networks trained to classify MNIST digits
We next compute the metric induced on input space by shallow networks trained to classify MNIST digits [60].
Single-hidden-layer fully-connected networks are initialized with widths [784, 2000, 10], representing a modest 2.6-fold
representational expansion, and trained on the 60000 28 × 28 pixel handwritten digit images. We perform a 75/25
train test split, and no preprocessing is made on either training or testing set. Batches of 1000 images and their labels
from the training set (numbers 0 −9) are fed to the network for 200 epochs; the networks are trained via the Adam
optimizer (learning rate 0.001, weight decay 10−4) with negative log-likelihood loss. At the end of 200 epochs both
training and testing accuracy exceed 95%. The metric induced by the trained network at a series of input images is
then computed with autograd, e.g., PyTorch [61]. Here, as in Appendix G 1, we use float64 precision [57–59].
We give two styles of visualization: linear interpolation and plane spanning. For linear interpolation, the images we
consider are images yi interpolated between two test images x1, x2 as follows: y(t) = (1 −t)x1 + tx2 for t ∈[0, 1];
for plane spanning, the images are all in the plane spanned by three random test samples assigned at the edge of a
unit equilateral triangle, so that each edge of the triangle correspond to the linear interpolation as previously noted.
Concretely, we consider y(t1, t2, t3) = t1x1 + t2x2 + t3x3 for {t1, t2, t3 ∈[0, 1] : t1 + t2 + t3 = 1}. Eigenvalues of the
metric matrix gµν tend to become small as training progresses, and so, due to the high dimensionality of the input
space, the metric
p
det gµν becomes minuscule and diﬃcult to compute within machine precision. Therefore, instead
of
p
det gµν, we compute its logarithm (for eﬃciency, in production, we compute the log of the singular values of the
Jacobians ∂µΦi so as to avoid the complexity of large matrix multiplication in ﬁguring out the metric). We ﬁnd that
this log volume element consistently grows (relatively) large at input images near the decision boundary, as shown in
Figure 3 for the linear interpolation and the plane spanning.
We conclude this experiment by commenting on the numerical stability of the computations described above. In
addition to the geometric quantities, we also examine the numerical singularity of the metrics. Figure G.12 visualizes
the full eigenvalue spectrum of at the anchor points corresponding to Figure G.10. As training progresses, the decay
becomes slower initially, but expedites at respective tails, potentially as a manipulation by the network to contract local
volumes compared to the boundary towards a better generalization, whose exact mechanism should be subject to further
scrutiny. Importantly, note that all eigenvalues are of reasonable log scale in the float64 paradigm within 200 training
epochs. Unfortunately, this may not hold when we increase the training epochs or perturb other hyperparameters. The
tail eigenvalues would become too small even in the float64 range (smaller than 10−320, perilously close to the smallest
positive number a float64 object can hold), and subsequent arithmetic operations break down. This close-to-singular
behavior of the metric is ticklish and inevitable, and the naive solution of imposing threshold below which eigenvalues
are discarded for volume element computation may risk not observing the central message of this paper that the
volume element is large at the boundary since only parts of the eigenspectrum is taken into consideration.

S29
FIG. G.8. log(√det g) induced at interpolated images between 1 and 5 (top row) and 1 and 6 (bottom row) by networks trained
to classify MNIST digits. Sample images are visualized at the endpoints and midpoint for each set. Each line is colored by its
prediction at the interpolated region and end points. As training progresses, the volume elements bulge in the middle (near
decision boundary) and taper oﬀat both endpoints. See Appendix G 2 for experimental details.

S30
FIG. G.9. Digit predictions and log(√det g) for the hyperplane spanned by three randomly sampled training point (5, 6, and 7)
across diﬀerent epochs.

S31
FIG. G.10. Digit predictions and log(√det g) for the hyperplane spanned by three randomly sampled training point (5, 6, and
1) across diﬀerent epochs.

S32
FIG. G.11. Digit predictions and log(√det g) for the hyperplane spanned by three randomly sampled training point (7, 6, and
1) across diﬀerent epochs.

S33
0
100
200
300
400
500
600
700
800
50
40
30
20
10
0
log(
i )
log(
i ) at 5
epoch
0
50
200
0
100
200
300
400
500
600
700
800
50
40
30
20
10
0
log(
i )
log(
i ) at 6
epoch
0
50
200
0
100
200
300
400
500
600
700
800
40
35
30
25
20
15
10
5
0
log(
i )
log(
i ) at 1
epoch
0
50
200
FIG. G.12. The base-10 logarithms of the square roots of non-zero eigenvalues λi of the metric g at anchor points 5, 6, and 1
corresponding to Figure G.10

S34
3.
ResNets trained on CIFAR-10
Finally, we experiment on a deep network trained to classify CIFAR-10 images. CIFAR-10 contains 60000 train
and 10000 test images, each of a size 3 × 32 × 32 [38]. 10 classes of images cover plane, car, bird, cat, deer, dog, frog,
horse, ship, and truck, with an equal distribution in each class. Some preprocessing is made to boost performance: for
training set, we pad each image for 4 pixels and crop at random places to keep it of size 32 × 32, randomly horizontally
ﬂip images with probability 0.5, and translate each channel by subtracting (0.4914, 0.4822, 0.4465) and scale by
dividing (0.2023, 0.1994, 0.2010); for testing, we only perform the translation and scaling [62]. We use ResNet34 [37]
with GELU activation functions [39], trained with SGD using a learning rate 0.01, weight decay 10−4, and momentum
0.9. Batches of 1024 images are fed to train for 200 epochs. Our ResNet code was adapted from the publicly-available
implementation by Liu et al. [62], distributed under an MIT License. At the ﬁnal epoch the training accuracy reaches
above 99% and testing accuracy around 92%.
The images we consider are generated from samples in the preprocessed testing set and interpolated in the same
fashion as in the MNIST dataset (Appendix G 2). The geometric quantity considered here is likewise log(√det g)
and is computed using autograd. For demonstration purposes, although geometric quantities are computed on
preprocessed images, the images in Figures throughout this paper are their unpreprocessed counterparts. In general,
the message in CIFAR-10 experiment is consistent with our ﬁndings that along decision boundaries we observe large
volume elements but is less clear: The linear interpolation plot in Figure G.13 demonstrate similar behaviors as in
its counterpart in Figure G.8; Figure G.15, Figure G.16, and Figure G.17 each visualizes the convex hull anchored
by diﬀerent combinations of classes and demonstrates much more convoluted decision boundaries in the interpolated
space. Likewise, the volume element enlarges when traversing the decision boundaries and stays small within a class
prediction region.
In addition to the convex hull visualization, we provide the aﬃne hull (i.e. the region extrapolated from the anchor
points) in Figure G.18 along with the entropy of the softmax outputs by ResNet34. The entropy here is computed
with log10 to scale values between 0 and 1 for this 10 class classiﬁcation task. Note that places with high entropy
(more uncertainty) not only delineate the decision boundaries but also correspond to places with large volume element.
Moreover, we also show that volume element does not expand in regions away from the decision boundaries. We
illustrate this phenomenon in Figure G.19 by sampling images from the same class to span the plane and conclude
that the volume element expansion is only pronounced when there is an explicit decision boundary, regardless of it
being a correct one.
The same pattern is observed for the same achitecture but with ReLU activation. Even though ReLU is not
diﬀerentiable, it still yields consistent prediction as in the GELU cases where the volume element expands near
the decision boundaries. These are demonstrated by the linear interpolation in Figure G.20 and convex hull plane
visualization in Figure G.22, G.23, G.24 and aﬃne hull plane visualization in Figure G.25. Examination of the eigen
spectrum in Figure G.21 does not ﬂag any numerical issues in computations
We conclude this section by commenting on the memory consumption. Note that for this experiment only, we
enforce float32 out of memory concerns. This fortunately does not pose a numerical challenge since all eigenvalues
are in a reasonable range (shown in Figure G.14), bounded away from the smallest positive number float32 can hold
(10−38). However, the memory issue eﬀectively constraints the choice of our model, since the exact log volume element
at a single training sample computed through autograd for deeper network (e.g. ResNet50, ResNet 101) requires more
than 80GB, exceeding the largest memory conﬁguration of any single publicly available GPU as of the time of writing
(NVIDIA A100). To enable the study of larger models, it would be useful to have an approximation for the volume
element with tractable memory footprint in the future.

S35
0
10
20
30
40
50
60
930
925
920
915
910
905
900
895
890
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1080
1060
1040
1020
1000
980
960
log10 volume element
Epoch 50
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1160
1140
1120
1100
1080
1060
1040
log10 volume element
Epoch 200
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1020
1000
980
960
940
920
900
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1060
1040
1020
1000
980
960
940
log10 volume element
Epoch 50
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1140
1120
1100
1080
1060
1040
1020
log10 volume element
Epoch 200
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
FIG. G.13. log(√det g) induced at interpolated images between a car and a dog (top row) and between a car and a frog (bottom
row) by ResNet34 trained to classify CIFAR-10 digits. Sample images are visualized at the endpoints and midpoint for each set.
Each line is colored by its prediction at the interpolated region and end points. As training progresses, the volume elements
bulge in the middle (near decision boundary) and taper oﬀat both endpoints. See Appendix G 3 for experimental details.
0
100
200
300
400
500
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
log(
i )
log(
i ) at dog
epoch
0
50
200
0
100
200
300
400
500
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
log(
i )
log(
i ) at frog
epoch
0
50
200
0
100
200
300
400
500
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
log(
i )
log(
i ) at car
epoch
0
50
200
FIG. G.14. The base-10 logarithms of square roots of the eigenvalues λi of the metric g at the anchor points in Figure G.15:
dog (left), frog (mid), and car (right). As training proceeds, the spectrum is shifted downward and consequently the volume
element decreases at these points.

S36
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.3
0.4
0.5
0.6
0.7
0.8
0.9
entropy (log10)
1150
1100
1050
1000
950
900
850
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
entropy (log10)
1140
1110
1080
1050
1020
990
960
log10 volume element
Epoch 50
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
0.5
entropy (log10)
1200
1160
1120
1080
1040
1000
log10 volume element
Epoch 200
FIG. G.15. Digit predictions, log10(entropy), and log10(√det g) for the hyperplane spanned by three randomly sampled training
point a dog, a frog, and a car across diﬀerent epochs.

S37
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.5
0.6
0.7
0.8
0.9
entropy (log10)
1050
1020
990
960
930
900
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
entropy (log10)
1180
1140
1100
1060
1020
980
940
log10 volume element
Epoch 50
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
0.5
entropy (log10)
1280
1240
1200
1160
1120
1080
1040
1000
log10 volume element
Epoch 200
FIG. G.16. Digit predictions, log10(entropy), and log10(√det g) for the hyperplane spanned by three randomly sampled training
point a dog, a frog, and a horse across diﬀerent epochs.

S38
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.3
0.4
0.5
0.6
0.7
0.8
entropy (log10)
1140
1100
1060
1020
980
940
900
860
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
entropy (log10)
1200
1160
1120
1080
1040
1000
960
log10 volume element
Epoch 50
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
0.5
entropy (log10)
1220
1180
1140
1100
1060
1020
log10 volume element
Epoch 200
FIG. G.17. Digit predictions, log10(entropy), and log10(√det g) for the hyperplane spanned by three randomly sampled training
point a horse, a frog, and a car across diﬀerent epochs.

S39
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.3
0.4
0.5
0.6
0.7
0.8
0.9
entropy (log10)
1100
1050
1000
950
900
850
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
entropy (log10)
1200
1150
1100
1050
1000
log10 volume element
Epoch 50
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
0.5
0.6
entropy (log10)
1300
1250
1200
1150
1100
1050
log10 volume element
Epoch 200
FIG. G.18. Digit predictions and log(√det g) for the hyperplane spanned by three randomly sampled training point a dog, a
frog, and a car across diﬀerent epochs. The entire aﬃne hull (instead of the convex hull) is visualized. The middle panel is the
entropy of the softmax-ed probabilities of the ResNet34 outputs. Places with high entropy demarcate the decision boundary as
well as regions with relatively large volume element as expected, though less clear in the latter case.

S40
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
2
4
6
8
entropy (log10)
1e
6
1220
1200
1180
1160
1140
1120
1100
log10 volume element
Epoch 200
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
entropy (log10)
1300
1260
1220
1180
1140
1100
1060
log10 volume element
Epoch 200
FIG. G.19. Regions away from decision boundaries do not have a clear volume element pattern: we randomly select three
ﬁgures from the same class (top: frogs, bottom: planes) and perform the plane extrapolation. We visualize digit predictions,
log10(entropy), and log10(√det g) at the end of training for both cases. The top graph predicts frog universally with slight
volume element variation across the landscape; the bottom graph has an incorrect prediction of plane (treating it as a ship), and
creates an artifact of a decision boundary, which explains the vol element expansion at near that region.

S41
0
10
20
30
40
50
60
940
930
920
910
900
890
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1160
1140
1120
1100
1080
1060
log10 volume element
Epoch 50
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1200
1180
1160
1140
1120
1100
1080
log10 volume element
Epoch 200
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
960
950
940
930
920
910
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1160
1140
1120
1100
1080
1060
1040
1020
log10 volume element
Epoch 50
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1200
1180
1160
1140
1120
1100
log10 volume element
Epoch 200
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
FIG. G.20. log(√det g) induced at interpolated images between a car and a dog (top row) and between a car and a frog (bottom
row) by ResNet34 with ReLU activation trained to classify CIFAR-10 digits. Sample images are visualized at the endpoints and
midpoint for each set. Each line is colored by its prediction at the interpolated region and end points. As training progresses,
the volume elements bulge in the middle (near decision boundary) and taper oﬀat both endpoints. See Appendix G 3 for
experimental details.
0
100
200
300
400
500
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
log(
i )
log(
i ) at dog
epoch
0
50
200
500
0
100
200
300
400
500
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
log(
i )
log(
i ) at frog
epoch
0
50
200
500
0
100
200
300
400
500
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
log(
i )
log(
i ) at car
epoch
0
50
200
500
FIG. G.21. The base-10 logarithms of square roots of the eigenvalues λi of the metric g at the anchor points in Figure G.22:
dog (left), frog (mid), and car (right). As training proceeds, the spectrum is shifted downward and consequently the volume
element decreases at these points.

S42
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.4
0.5
0.6
0.7
0.8
0.9
entropy (log10)
1080
1040
1000
960
920
880
840
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
entropy (log10)
1160
1120
1080
1040
1000
960
920
log10 volume element
Epoch 50
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
0.5
0.6
entropy (log10)
1220
1180
1140
1100
1060
1020
980
log10 volume element
Epoch 200
FIG. G.22. Same as Figure G.15 but with ReLU activation. Digit predictions, log10(entropy), and log10(√det g) for the
hyperplane spanned by three randomly sampled training point a dog, a frog, and a car across diﬀerent epochs.

S43
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.4
0.5
0.6
0.7
0.8
0.9
entropy (log10)
1005
975
945
915
885
855
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
entropy (log10)
1220
1180
1140
1100
1060
1020
980
log10 volume element
Epoch 50
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
entropy (log10)
1300
1250
1200
1150
1100
1050
1000
log10 volume element
Epoch 200
FIG. G.23. Same as Figure G.16 but iwth ReLU activation. Digit predictions, log10(entropy), and log10(√det g) for the
hyperplane spanned by three randomly sampled training point a dog, a frog, and a horse across diﬀerent epochs.

S44
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.4
0.5
0.6
0.7
0.8
entropy (log10)
1060
1020
980
940
900
860
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
entropy (log10)
1200
1160
1120
1080
1040
1000
960
log10 volume element
Epoch 50
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
entropy (log10)
1240
1200
1160
1120
1080
1040
1000
log10 volume element
Epoch 200
FIG. G.24. Same as Figure G.17 but with ReLU activation. Digit predictions, log10(entropy), and log10(√det g) for the
hyperplane spanned by three randomly sampled training point a horse, a frog, and a car across diﬀerent epochs.

S45
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.4
0.5
0.6
0.7
0.8
0.9
entropy (log10)
1050
1000
950
900
850
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
entropy (log10)
1250
1200
1150
1100
1050
1000
950
log10 volume element
Epoch 50
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
entropy (log10)
1300
1250
1200
1150
1100
1050
1000
log10 volume element
Epoch 200
FIG. G.25. Same as Figure G.18 but with ReLU activation. Predictions and log(√det g) for the hyperplane spanned by three
randomly sampled training point a dog, a frog, and a car across diﬀerent epochs. The entire aﬃne hull (instead of the convex
hull) is visualized. The middle panel is the entropy of the softmax-ed probabilities of the ResNet34 outputs. Places with high
entropy demarcate the decision boundary as well as regions with relatively large volume element as expected, though less clear
in the latter case.

S46
0
10
20
30
40
50
60
4000
3900
3800
3700
3600
3500
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1550
1525
1500
1475
1450
1425
1400
1375
log10 volume element
Epoch 200
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1825
1800
1775
1750
1725
1700
1675
1650
log10 volume element
Epoch 1000
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
2850
2800
2750
2700
2650
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1720
1700
1680
1660
1640
1620
1600
1580
1560
log10 volume element
Epoch 200
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0
10
20
30
40
50
60
1950
1925
1900
1875
1850
1825
1800
1775
log10 volume element
Epoch 1000
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
FIG. G.26. log(√det g) induced at interpolated images between a car and a dog (top row) and between a car and a frog (bottom
row) by Barlow Twins with ResNet34 backbone and a GELU activation. Sample images are visualized at the endpoints and
midpoint for each set. Each line is colored by its prediction at the interpolated region and end points. As training progresses,
the volume elements bulge in the middle (near decision boundary) and taper oﬀat both endpoints. See Appendix G 4 for
experimental details.
4.
Self-supervised learning on CIFAR-10: Barlow Twins and SimCLR
Our hypothesis likewise extends to self-supervised learning frameworks. We employ the Barlow Twins architecture
[42] with a ResNet34 backbone, GELU activation, and a single-layer projector of dimension 256. We train the network
for 1000 epochs with SGD optimizer, a learning rate of 0.01, weight decay 10−4, and a batch size of 1024. In the
following we report the model performance at initialization, 200 epochs, and 1000 epochs respectively.
Data augmentation is done slightly diﬀerently from training end-to-end ResNet34 as above and we follow exactly the
same procedure as in [42]: random crop to 32 by 32 images, random horizontal ﬂip with probability 0.5, a color jitter
with brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, and probability 0.8, random grayscale with probability
0.2, a gaussian blur with probability 1.0 and 0.1 for two augmented inputs, solarization with probability 0 and 0.2 for
two augmented inputs, and ﬁnally a normalization by subtracting [0.485, 0.456, 0.406] and dividing by [0.229, 0.224,
0.225] channelwise.
Unlike supervised training, in the self-supervised framework the network is not exposed to labels during training.
The predictor is separately trained using a multiclass logistic regression with an l2 regularization of 1 on features
obtained by the ResNet34 backbone at diﬀerent snapshots of the training epochs. For ResNet34 in particular each
image is represented by a vector of dimension 512. At the end of 1000 epochs, a multiclass logistic regression is able to
reach 84% accuracy on training set and 70% accuracy on testing set. We acknowledge that the performance can be
further improved if we use a deeper backbone (e.g. ResNet 50) or a more expressive projector appended to the ResNet
backbone. Figure G.26 shows the linear interpolation between two samples and Figure G.29, G.28, G.27 show the
convex hull generated by three samples. Likewise, we also display the eigenspectrum in Figure G.30, which does not
indicate any numerical issues.
One intriguing phenomenon is that we observe the opposite behavior for the contrastive learning model SimCLR
[43]. The volume element is now dipping at decision boundaries. We suspect that this is due to SimCLR’s explicit
requirement that the embeddings lie on a unit sphere. As a result, it is possible that pulling back the Euclidean
metric from embedding space is no longer appropriate, and one should instead pull back the ﬂat metric on the sphere.
Investigating this phenomenon could be an interesting topic for future work. One demonstration of this phenomenon
is displayed in ﬁgure G.32, G.34, G.33.

S47
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.86
0.88
0.90
0.92
0.94
0.96
0.98
entropy (log10)
4200
4000
3800
3600
3400
3200
3000
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
entropy (log10)
1650
1620
1590
1560
1530
1500
1470
log10 volume element
Epoch 200
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
entropy (log10)
1920
1890
1860
1830
1800
1770
1740
1710
log10 volume element
Epoch 1000
FIG. G.27. Digit predictions, log10(entropy), and log10(√det g) for the hyperplane spanned by three randomly sampled training
point a horse, a frog, and a car across diﬀerent epochs for Barlow Twins with ResNet34 backbone using GELU activation.

S48
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.98425
0.98450
0.98475
0.98500
0.98525
0.98550
0.98575
0.98600
0.98625
entropy (log10)
4160
4080
4000
3920
3840
3760
3680
3600
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
0.7
entropy (log10)
1640
1600
1560
1520
1480
1440
1400
log10 volume element
Epoch 200
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
entropy (log10)
1875
1845
1815
1785
1755
1725
1695
log10 volume element
Epoch 1000
FIG. G.28. Digit predictions, log10(entropy), and log10(√det g) for the hyperplane spanned by three randomly sampled training
point a dog, a frog, and a horse across diﬀerent epochs for Barlow Twins with ResNet34 backbone using GELU activation.

S49
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.86
0.88
0.90
0.92
0.94
0.96
0.98
entropy (log10)
4200
4000
3800
3600
3400
3200
3000
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
entropy (log10)
1640
1600
1560
1520
1480
1440
1400
log10 volume element
Epoch 200
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.2
0.4
0.6
0.8
entropy (log10)
1860
1830
1800
1770
1740
1710
1680
log10 volume element
Epoch 1000
FIG. G.29. Digit predictions, log10(entropy), and log10(√det g) for the hyperplane spanned by three randomly sampled training
point a dog, a frog, and a car across diﬀerent epochs for Barlow Twins with ResNet34 backbone using GELU activation.

S50
0
100
200
300
400
500
10
8
6
4
2
0
log(
i )
log(
i ) at dog
epoch
0
50
200
500
1000
0
100
200
300
400
500
10
8
6
4
2
0
log(
i )
log(
i ) at frog
epoch
0
50
200
500
1000
0
100
200
300
400
500
10
8
6
4
2
0
log(
i )
log(
i ) at car
epoch
0
50
200
500
1000
FIG. G.30. The base-10 logarithms of square roots of the eigenvalues λi of the metric g at the anchor points in Figure G.29: dog
(left), frog (mid), and car (right) for Barlow Twins with ResNet34 backbone using GELU activation. As training proceeds, the
spectrum is initially shifted upward and then shifted downward and consequently the volume element decreases at these points.

S51
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.65
0.70
0.75
0.80
0.85
0.90
0.95
entropy (log10)
4000
3800
3600
3400
3200
3000
log10 volume element
Epoch 0
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.2
0.4
0.6
0.8
entropy (log10)
1800
1750
1700
1650
1600
1550
1500
1450
1400
log10 volume element
Epoch 200
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.2
0.4
0.6
0.8
entropy (log10)
2000
1950
1900
1850
1800
1750
1700
log10 volume element
Epoch 1000
FIG. G.31. Digit predictions, log10(entropy), and log10(√det g) for the aﬃne hull spanned by three randomly sampled training
point a dog, a frog, and a car across diﬀerent epochs for Barlow Twins with ResNet34 backbone using GELU activation. This
corresponds to Figure G.29.

S52
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
entropy (log10)
19.75
19.25
18.75
18.25
17.75
17.25
16.75
16.25
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
entropy (log10)
9.5
9.0
8.5
8.0
7.5
7.0
6.5
log10 volume element
Epoch 200
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
entropy (log10)
8.0
7.6
7.2
6.8
6.4
6.0
5.6
log10 volume element
Epoch 1000
FIG. G.32. Digit predictions, log10(entropy), and log10(√det g) for the hyperplane spanned by three randomly sampled training
point a horse, a frog, and a car across diﬀerent epochs for SimCLR with ResNet34 backbone using GELU activation. This
contradicts our predictions since the volume elements dip at decision boundaries, possibly due to the inappropriateness of
approximating a sphere using a linear interpolation.

S53
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.75
0.80
0.85
0.90
0.95
entropy (log10)
12.2
11.8
11.4
11.0
10.6
10.2
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
0.7
entropy (log10)
1.2
0.8
0.4
0.0
0.4
0.8
1.2
log10 volume element
Epoch 200
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
entropy (log10)
3.6
3.2
2.8
2.4
2.0
1.6
1.2
log10 volume element
Epoch 1000
FIG. G.33. Digit predictions, log10(entropy), and log10(√det g) for the hyperplane spanned by three randomly sampled training
point a dog, a frog, and a horse across diﬀerent epochs for SimCLR with ResNet34 backbone using GELU activation. This
contradicts our predictions since the volume elements dip at decision boundaries, possibly due to the inappropriateness of
approximating a sphere using a linear interpolation.

S54
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.75
0.80
0.85
0.90
0.95
entropy (log10)
12.45
12.15
11.85
11.55
11.25
10.95
log10 volume element
Epoch 0
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
entropy (log10)
1.5
1.0
0.5
0.0
0.5
1.0
log10 volume element
Epoch 200
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
0.2
0.4
0.6
0.8
class prediction
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
0.2
0.4
0.6
0.8
entropy (log10)
3.6
3.2
2.8
2.4
2.0
1.6
log10 volume element
Epoch 1000
FIG. G.34. Digit predictions, log10(entropy), and log10(√det g) for the hyperplane spanned by three randomly sampled training
point a dog, a frog, and a car across diﬀerent epochs for SimCLR with ResNet34 backbone using GELU activation. This
contradicts our predictions since the volume elements dip at decision boundaries, possibly due to the inappropriateness of
approximating a sphere using a linear interpolation.

S55
5.
Data and code availability
All datasets used in this work are either programmatically generated (Appendix G 1) or publicly available (Appendices
G 2, G 3, and G 4; LeCun et al. [60] and Krizhevsky [38]). PyTorch [61] code to train all models and generate ﬁgures
is available on GitHub at https://github.com/Pehlevan-Group/nn_curvature.
As noted above, our ResNet
implementation is adapted from Liu et al. [62]’s MIT-licensed implementation.
All models were trained on the Harvard Cannon HPC cluster equipped with NVIDIA SMX4-A100-80GB GPUs
(https://www.rc.fas.harvard.edu/).

