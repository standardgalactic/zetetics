On the Universality of Graph Neural Networks
on Large Random Graphs
Nicolas Keriven∗
Alberto Bietti†
Samuel Vaiter‡
Abstract
We study the approximation power of Graph Neural Networks (GNNs) on latent position random
graphs. In the large graph limit, GNNs are known to converge to certain “continuous” models known
as c-GNNs, which directly enables a study of their approximation power on random graph models.
In the absence of input node features however, just as GNNs are limited by the Weisfeiler-Lehman
isomorphism test, c-GNNs will be severely limited on simple random graph models. For instance,
they will fail to distinguish the communities of a well-separated Stochastic Block Model (SBM) with
constant degree function. Thus, we consider recently proposed architectures that augment GNNs with
unique node identiﬁers, referred to as Structural GNNs here (SGNNs). We study the convergence
of SGNNs to their continuous counterpart (c-SGNNs) in the large random graph limit, under new
conditions on the node identiﬁers. We then show that c-SGNNs are strictly more powerful than
c-GNNs in the continuous limit, and prove their universality on several random graph models of
interest, including most SBMs and a large class of random geometric graphs. Our results cover both
permutation-invariant and permutation-equivariant architectures.
1
Introduction
Graph Neural Networks (GNNs) are deep architectures deﬁned over graph data that have garnered a lot
of attention in recent years. They represent the state-of-the-art in many graph Machine Learning (graph
ML) problems, and have been successfully applied to e.g. node clustering [5], semi-supervised learning
[21], quantum chemistry [14], and so on. See [3, 39, 15, 18] for reviews.
As the universality of Multi-Layers Perceptrons (MLP) is one of the foundational theorems in deep learning
– that is, any continuous function can be approximated arbitrarily well by an MLP – in the last few years
the approximation power of GNNs has been a topic of great interest. In the absence of special node
features, i.e. when one has only access to the graph structure, the crux of the problem has been proven
to be the capacity of GNNs to solve the graph isomorphism problem, that is, deciding when two graphs
are permutations of each other or not (a diﬃcult problem for which no polynomial algorithm is known
[2]). Indeed, this property is directly linked to the approximation power of GNNs [6, 1]. In this light, the
landmark paper [40] proves that classical GNNs are at best as powerful as the famous Weisfeiler-Lehman
(WL) test [38] for graph isomorphism. Since then, several works [40, 28, 6] have derived new architectures,
for instance involving high-order tensors [28], with improved discriminative power equivalent to “higher-
order” variants of the WL test. In another line of works, several recent papers have advocated the use of
unique node identiﬁers [25, 24], with strategies to preserve the permutation-equivariance/invariance of
GNNs coined Structural Message Passing (SMP) in [36] (see Sec. 2.3). We call these models Structural
GNN (SGNN) here. SGNNs have been proved to be strictly more powerful than the WL test in [36], and
even universal on graphs with bounded degrees, however for powerful layers that cannot be implemented
in practice.
∗CNRS & GIPSA-lab.
11 rue des Math´ematiques, 38400 St-Martin-d’Her`es, France.
firstname.name@gipsa-lab.
grenoble-inp.fr
†NYU Center for Data Science, New York, USA. firstname.name@nyu.edu
‡CNRS & IMB, Universit´e de Bourgogne. 9 avenue Alain Savary, 21000 Dijon, France. SV is partly supported by ANR
JCJC GraVa (ANR-18-CE40-0005). firstname.name@u-bourgogne.fr
1
arXiv:2105.13099v2  [stat.ML]  28 May 2021

When the size of the graphs grows, the notion of graph isomorphism becomes somewhat moot: large graphs
might share properties (number of well-connected communities, and so on) but are never isomorphic to
each other. GNNs have nevertheless proven successful in identifying their large-scale structures, e.g. for
node clustering [5]. Several papers have therefore used tools from random graph theory and graphons to
study the behavior of GNNs in the large-graph limit. In [19, 34], GNNs are shown to converge to limiting
“continuous” architectures (coined c-GNNs in [19]). A few works have studied the discriminative power of
c-GNNs on graphons [27], however, analogously to how the WL test will fail on regular graphs, c-GNNs
are severely limited on graph models with almost-constant degree function (Fig. 1), and the question is
still largely open.
Figure 1: Illustration of Prop. 7 (Sec. 5.3). On an
SBM with constant degree function, a GNN (top) might
overﬁt the training set, but converges to a constant c-
GNN. On the contrary, there exists a c-SGNN (bottom)
that perfectly separates the communities. Details can
be found in App. E.
Contribution and outline.
In this paper, we
study the convergence of SGNNs on large random
graphs towards “c-SGNNs”, and analyze the approx-
imation power of c-GNNs and c-SGNNs. After some
preliminary results in Sec. 2, we study the conver-
gence of SGNNs in Sec 3. In Sec. 4 and 5, we show
that c-SGNNs are strictly more powerful than c-
GNNs in both permutation-invariant and equivariant
case. We then prove the universality of c-SGNNs on
several popular models of random graphs, including
Stochastic Block Models (SBMs) and radial kernels.
For instance, we show in Sec. 5.3 that c-SGNNs can
perfectly identify the communities of most SBMs,
including some for which any c-GNN provably fails
(Fig. 1).
Related work.
The approximation power of GNNs
(on ﬁnite graphs) has been a topic of great interest
in recent years, and we do not attempt to make an
exhaustive list of all results here.
The landmark
paper [40] showed that permutation-invariant GNNs
were at best as powerful as the WL test, and later models [28, 6, 13] were constructed to be as powerful as
higher-order WL tests, using for instance higher-order tensors [28]. The link between the graph isomorphism
problem and function approximation was made in [6, 20] and extended in [1]. Some architectures were
proven to be universal [29, 20] but involve tensors of unbounded order. When graphs are equipped
with unique node identiﬁers, the approximation power of GNNs can be signiﬁcantly improved if relaxing
permutation-invariance/equivariance [25, 24]. Recent works have proposed methods to restore these
properties [36]. As mentioned above, the resulting SGNNs are indeed more powerful than the WL test
[36], and even universal on graphs with bounded degrees when allowing powerful layers. To our knowledge,
the approximation power of SGNNs as they are implemented in practice is still open. We treat of their
continuous limit in this paper.
Fewer results can be found in the large-graph limit. Beyond the graph-isomorphism paradigm, authors
have studied the capacity of GNNs to count graph substructures [4, 10] or identify various graph properties
[12, 41]. Several works have studied the large-graphs limit of GNNs [34, 27, 23, 19], assuming random graph
or graphon models. The degree function has been proven to be a crucial element for the discriminative
power of c-GNNs, and they will not be able to distinguish graphons with close degree functions [27]. Here
we show how c-SGNNs allow to overcome these limitations, and provide the ﬁrst universality theorems for
(S)GNNs in the continuous limit. Finally, we note that universal architectures exist on measures [8, 43],
which can be seen as limits of point clouds, but, similar to the discrete case [42, 29, 20], the graph case
may be signiﬁcantly harder to study.
2

2
Preliminaries
In this section, we group notations on random graphs and GNNs, present the convergence result of [19]
slightly adapted to our context, and introduce the SGNNs of [36]. An undirected graph G with n nodes is
represented by a symmetric adjacency matrix A ∈Rn×n. It is isomorphic to all graphs with adjacency
matrices σAσ⊤, for any permutation matrix σ ∈{0, 1}n×n. Matrix rows (resp. columns) are denoted by
Ai,: (resp. A:,i). The norm ∥·∥is the operator norm for matrices and Euclidean norm for vectors. For
a compact metric space X, we denote by C(X, R) the space of bounded continuous functions X →R,
equipped with ∥f∥∞= supx |f(x)|. For a multivariate function f ∈C(X, Rd), fi denotes its ith coordinate,
and ∥f∥∞= (P
i ∥fi∥2
∞)1/2.
2.1
Random graph model
We consider “latent position” random graphs, which include several popular models such as SBMs or
graphons [26]. The nodes are associated with unobserved latent random variables xi drawn i.i.d. For the
edges, we will examine two cases: an “ideal” one with deterministic weighted edges, and a more “realistic”
one where the edges are randomly drawn independently. In the literature, the latter is often considered as
an idealized framework to study the behavior of various algorithms [37, 33].
Let (X, mX ) be a compact metric space that is not a singleton, and assume that the ε-covering numbers1
of X scale as O
 ε−dX 
for some dimension dX . We denote by W the set of symmetric bivariate functions
in C(X × X, [0, 1]) that are LW -Lipschitz in each variable, and by P the set of probability distributions
over X equipped with the total variation norm ∥·∥TV. A graph with n nodes is generated according to a
random graph model (W, P) ∈W × P as follows:
x1, . . . , xn
i.i.d
∼P,
aij =
(
W(xi, xj)
deterministic edges case
α−1
n Bernoulli(αnW(xi, xj))
random edges case
where αn is the sparsity level of the graph in the random edges case, which we assume to be known for
simplicity. When αn ∼1, the graph is said to be dense, when αn ∼1
n the graph is sparse, and when
α ∼log n
n
the graph is relatively sparse. Note that we have normalized the Bernoulli edges by 1/αn such
that Eaij = W(xi, xj) conditionally on xi, xj. When X is ﬁnite, the model is called an SBM (see Sec. 4.3).
Like ﬁnite graphs, random graph models can be isomorphic to one another [19, 26]. In this paper, similar
to [19] we consider that, for any bijection ϕ : X →X, the model (W, P) is isomorphic to (Wϕ, ϕ−1
♯P),
where Wϕ(x, y) = W(ϕ(x), ϕ(y)) and f♯P is the pushforward of P (the distribution of f(X)). Indeed, it
is easy to see that both produce exactly the same distribution over graphs.
2.2
Graph Neural Networks
Following [19, 9], in this paper we consider the so-called “spectral” version of GNNs, which include several
message-passing models for certain aggregation functions. We consider polynomial ﬁlters h deﬁned as
h(A) = P
k βkAk for a matrix or operator A. In practice, the order of the ﬁlters is always ﬁnite, but our
results are valid for inﬁnite-order ﬁlters (assuming that the sum always converges for simplicity). We
consider any activation function ρ : R →R which satisﬁes ρ(0) = 0 and |ρ(x) −ρ(y)| ⩽|x −y| for which
the universality theorem of MLPs applies [31], e.g. ReLU.
Spectral GNNs are deﬁned by successive ﬁltering of a graph signal. Given an input signal Z(0) ∈Rn×d0,
at each layer ℓ= 0, . . . , M −1:
Z(ℓ+1)
:,j
= ρ
Pdℓ
i=1 h(ℓ)
ij
  1
nA

Z(ℓ)
:,i + b(ℓ)
j 1n

∈Rn
j = 1, . . . , dℓ+1 ,
(1)
where h(ℓ)
ij (A) = P
k β(ℓ)
ijkAk are trainable graph ﬁlters, b(ℓ)
j
∈R are trainable additive biases and ρ is
applied pointwise. We note the normalization A/n that will be necessary for convergence. GNNs exist in
1the number of balls of radius ε required to cover X
3

two main versions: so-called “permutation-invariant” GNNs ¯Φ output a single vector for the entire graph,
while “permutation-equivariant” GNNs Φ output a graph signal:
ΦA(Z(0)) = g
 Z(M)
∈Rn×dout
¯ΦA(Z(0)) = g

1
n
Pn
i=1 Z(M)
i,:

∈Rdout ,
(2)
where g : RdM →Rdout is an MLP applied row-wise in the equivariant case. By construction, such
GNNs are indeed equivariant or invariant to node permutation: for all permutation matrices σ, we have
ΦσAσ⊤(σZ(0)) = σΦA(Z(0)) and ¯ΦσAσ⊤(σZ(0)) = ¯ΦA(Z(0)).
Continuous GNNs.
In [19], the authors show that, in the large random graphs limit, GNNs converge
to the following “continuous” models (coined c-GNNs), which propagate functions over the latent
space f (ℓ) ∈C(X, Rdℓ) instead of graph signals. Given an input function f (0) ∈C(X, Rd0):
f (ℓ+1)
j
= ρ
Pdℓ
i=1 h(ℓ)
ij (TW,P )f (ℓ)
i
+ b(ℓ)
j

,
where TW,P is the operator TW,P f =
R
W(·, x)f(x)dP(x).
Then similar to (2) the permutation-
equivariant/invariant versions of c-GNNs are deﬁned as:
ΦW,P (f (0)) = g ◦f (M),
¯ΦW,P (f (0)) = g
 R
f (M)(x)dP(x)

.
Remark that ΦW,P (f (0)) is itself a function in C(X, Rdout) while ¯ΦW,P (f (0)) ∈Rdout is a vector. Moreover,
by virtue of the polynomial ﬁlters, the four architectures ΦA, ¯ΦA, ΦW,P , ¯ΦW,P have the exact same set of
parameters. Like in the discrete case, one can prove that c-GNN are equivariant or invariant to isomorphisms
of random graph models [19]: for all bijection ϕ : X →X, we have ΦWϕ,ϕ−1
♯
P (f ◦ϕ) = ΦW,P (f) ◦ϕ and
¯ΦWϕ,ϕ−1
♯
P (f ◦ϕ) = ¯ΦW,P (f).
Let us now turn to convergence of GNNs to c-GNNs. The following result is adapted2 from [19]. While the
outputs of permutation-invariant GNNs and c-GNNs are vectors in Rdout that can be directly compared, the
output graph signal of a permutation-equivariant GNN is compared with a sampling of the output function
of the corresponding c-GNN: for a graph signal Z ∈Rn×d, a function f : X →Rd and X = {xi}n
i=1, we
deﬁne the (square root of the) mean-square error as MSEX(Z, f) = ( 1
n
Pn
i=1 ∥Zi −f(xi)∥2
2)1/2. The proof
of Theorem 1 with all multiplicative constants is given in App. A.1. Recall that dX is the “dimension” of
X.
Theorem 1. Assume G is drawn from (W, P) and has latent variables X. Fix ρ, ν > 0.
• In the deterministic edges case: with probability 1 −ρ, for all Z(0):
MSEX(ΦA(Z(0)), ΦW,P (f (0))) ⩽C · MSEX(Z(0), f (0)) + R1(n)
(3)
for some constant C and R1(n) = O
p
(dX + log(1/ρ))/n

.
• In the random edges case: assume that the sparsity level is αn ≳n−1 log n. There is a constant
Cν such that, with probability 1 −ρ −n−ν, for all Z(0):
MSEX(ΦA(Z(0)), ΦW,P (f (0))) ⩽C · MSEX(Z(0), f (0)) + R1(n) + R2(n)
(4)
where R2(n) = O
 Cν/√αnn

.
• In the permutation-invariant case: The exact same results hold for
¯ΦA(Z(0)) −¯ΦW,P (f (0))

instead of the MSE on the left-hand-side, with an added error term R3(n) = O
p
log(1/ρ)/n

.
2The authors in [19] proved this for the normalized Laplacian, which allows bypassing the knowledge of αn. Here we use
the adjacency matrix, for our later results on approximation power.
4

By the theorem above, a GNN converges to its continuous counterpart if Z(0) is (close to) a sampling of a
function f (0) at the latent variables. This is directly assumed in [19]. In the present paper however, we do
not suppose that input node features are available. While several strategies have been proposed in the
literature, a popular baseline is to simply take constant input Z(0) = 1n (which, by a multiplication by
A on the ﬁrst layer, is also equivalent to inputing the degrees as in [5] for instance). In this case, there
is convergence to a c-GNN with f (0) = 1. For simplicity in the rest of the paper we drop the notation
“(1)” and write ΦA = ΦA(1n), ΦW,P = ΦW,P (1), and so on. It is known that GNNs are limited on regular
graphs: for permutation-invariant GNNs, the WL test cannot distinguish regular graphs of the same order,
and for permutation-equivariant GNNs, ΦA(1n) is constant over the nodes. Similarly for c-GNN, the
degree function
R
W(·, x)dP(x) is key in the discriminative power of c-GNNs [27], and if it is constant,
then ΦW,P (1) is a constant function (see Fig. 1).
2.3
SGNN: GNN with unique node identiﬁers
To remedy the absence of input node features, in [36] the authors propose an architecture with unique node
identiﬁers while still respecting permutation invariance/equivariance. More precisely, they ﬁrst choose an
arbitrary ordering of the nodes q = 1, . . . , n, apply a GNN to each one-hot vector eq = [0, . . . , 1, . . . , 0],
and restore equivariance to permutation by a ﬁnal pooling. For later purpose of convergence, we generalize
this strategy to any collection Eq(A) ∈Rn×d0 that satisﬁes:
Eq(σAσ⊤) = σEσ−1(q)(A) .
(5)
where by an abuse of notation σ(q) designates the permutation function applied to index q. For instance,
it can be any ﬁltering of one-hot vector Eq(A) = h(A)eq. After the GNN ΦA, a pooling is applied to
restore equivariance, then a second GNN Φ′
A that is either invariant or equivariant:
ΨA = Φ′
A

1
n
P
q ΦA (Eq(A))

∈Rn×d′
out,
¯ΨA = ¯Φ′
A

1
n
P
q ΦA (Eq(A))

∈Rd′
out
(6)
In [36], these architectures, called SMPs, are interpreted as doing message-passing over matrices, and can
use more general pooling and aggregation functions. In a sense, what we call SGNN are “spectral” versions
of SMP, but are essentially the same idea. It is not diﬃcult to see that SGNNs satisfy: ΨσAσ⊤= σΨA
and ¯ΨσAσ⊤= ¯ΨA.
3
Convergence of SGNNs on large random graphs
In this section, we extend Theorem 1 to SGNNs. To deﬁne continuous SGNNs, we consider a bivariate
input function ηW,P : X ×X →Rd0 such that: the mapping (W, P) 7→ηW,P is continuous3, for any (W, P),
ηW,P is Cη-bounded and Lη-Lipschitz in each variable, and similar to (5) is respects the following:
ηWϕ,ϕ−1
♯
P (x, y) = ηW,P (ϕ(x), ϕ(y)) ,
(7)
for all bijections ϕ : X →X. For instance, ηW,P = W or any ﬁlter ηW,P (x, y) = [h(TW,P )W(·, y)](x)
satisfy these conditions. A c-SGNN is then deﬁned as:
ΨW,P = Φ′
W,P
 R
ΦW,P (ηW,P (·, x))dP(x)

,
(8)
for the equivariant case, or similarly ¯ΨW,P = ¯Φ′
W,P
 R
ΦW,P (ηW,P (·, x))dP(x)

for the invariant case.
Again, using the properties of c-GNNs, it is easy to see that c-SGNNs respect random graphs isomorphism:
ΨWϕ,ϕ−1
♯
P = ΨW,P ◦ϕ and ¯ΨWϕ,ϕ−1
♯
P = ¯ΨW,P . We are now ready to extend Theorem 1. The proof and
multiplicative constants are in App. A.2.
Theorem 2. Assume G is drawn from (W, P) with latent variables X. Fix ρ, ν > 0.
3for the norm ∥·∥∞+ ∥·∥TV on W × P
5

• In the deterministic edges case: with probability 1 −ρ:
MSEX(ΨA, ΨW,P ) ⩽C′ · sup
q MSEX(Eq(A), η(·, xq)) + R′
1(n)
(9)
for some constant C′ and R′
1(n) = O
p
(dX + log(1/ρ))/n

.
• In the random edges case: assume that the sparsity level is αn ≳n−1 log n. There is a constant
Cν such that, with probability 1 −ρ −n−ν:
MSEX(ΨA, ΨW,P ) ⩽C′ · sup
q MSEX(Eq(A), η(·, xq)) + R′
1(n) + R′
2(n)
(10)
where R′
2(n) = O
 Cν/√αnn

.
• In the permutation-invariant case: The exact same results hold for
¯ΨA −¯ΨW,P
 instead of the
MSE, with an added error term R′
3(n) = O
p
log(1/ρ)/n

.
Hence, we obtain convergence when the input signal Eq(A) is close to being a sampling of a function ηW,P
at xq. The choice of input signal/function is therefore crucial. Let us examine a few strategies.
One-hot vectors.
If one chooses one-hot vectors Eq = eq as in [36], then we can see that the SGNN
converges to the continuous architecture with input ηW,P = 0, since MSEX(eq, 0) →0. Since ΨW,P is
nothing more than a traditional c-GNN ΦW,P with constant input in that case, this is not a satisfying
choice in terms of approximation power.
One-hop ﬁltering.
If we choose to “ﬁlter” eq once and take Eq(A) = Aeq, the natural continuous
equivalent is ηW,P = W. Such a strategy only works for deterministic edges: indeed,
MSEX(Aeq, W(·, xq)) =
 n−1 Pn
i=1(aiq −W(xi, xq))21/2
(
= 0
with deterministic edges
≈
p
n−1 P
i V ar(aiq) ∼α−1
with random edges, w.h.p.
where the last line comes from a simple application of Hoeﬀding’s inequality. Hence, in the case of random
edges, the MSE does not vanish and typically diverges for non-dense graphs (Fig. 2).
101
102
103
n
10
3
MSE
One-hop
Two-hop
Theory
Figure 2: Diﬀerence between the out-
puts of some ¯ΨA in the random edges
case with αn ∼n−1/3 and in the deter-
ministic edges case, which converges to
¯ΨW,P . Convergence is observed for two-
hop ﬁltering only. The theoretical rate
of Prop. 1 is slightly pessimistic. Details
can be found in App. E.
Two-hop ﬁltering.
We can therefore choose to ﬁlter twice and
consider ηW,P (x, y) = TW,P [W(·, y)](x), that is, Eq(A) = A2eq/n.
We have the following result.
Proposition 1. In the random edges case, with probability 1 −ρ,
we have for all q:
MSEX

A2eq
n , TW,P [W(·, xq)](·)

≲O
√
log(n/ρ)
αn
√n

.
In the deterministic edges case, the rate is O (1/√n).
Convergence is guaranteed when √log n/(αn
√n) →0, which is
stronger than relative sparsity. The diﬀerence between one-hop and
two-hop ﬁltering is illustrated in Fig. 2. Other strategies may also
be examined, and we leave this for future work. For instance, based
on Theorem 5 in App. D, a strategy based on the eigenvectors of
A could lead to convergence even in the relatively sparse case. In
the rest of the paper, we explicitly write when our results are valid
for one- or two-hop ﬁltering.
6

4
Approximation power: permutation-invariant case
In this section, we study the approximation power of continuous architectures in the permutation-invariant
case. We seek to characterize the functions W × P →R that can be well-approximated by a c-GNN or
c-SGNN. We derive a generic criterion for universality arising from the Stone-Weierstrass theorem, before
proving that c-SGNNs are indeed strictly more powerful than c-GNNs. Finally, we give several examples
of models for which c-SGNNs are universal.
4.1
Generic result with Stone-Weierstrass theorem
A classical tool to prove universality of neural nets in the literature is the Stone-Weierstrass theorem
[17, 20], which states that an algebra of functions that separates points on a compact space is dense in
the set of continuous functions. Although NNs are typically not an algebra since they are not closed by
multiplication, one of the original proofs of universality for MLPs solves this by a clever trick [17] which
we recall in Lemma 1 in App. D. A direct application results in the following proposition.
Proposition 2. Let M ⊂W × P be a compact subset of W × P. c-SGNNs (resp. c-GNNs) are dense
in C(M, R) if and only if: for all (W, P) ̸= (W ′, P ′) ∈M, there is a c-SGNN (resp. a c-GNN) such that
¯ΨW,P ̸= ¯ΨW ′,P ′ (resp. ¯ΦW,P ̸= ¯ΦW ′,P ′).
Note that, by construction of the permutation-invariant c-(S)GNNs, universality is only possible when M
does not contain two isomorphic versions of the same random graph model. Equivalently, M may be a
larger set quotiented by random graph isomorphism.
4.2
c-SGNNs are more powerful than c-GNNs
SGNNs were proven to be strictly more powerful than the WL test, and therefore than GNNs, in [36]. In
the theorem below, we check that this strict inclusion holds for their continuous limits.
Theorem 3. The set of functions of the form (W, P) →¯ΦW,P is strictly included in the set of functions
(W, P) →¯ΨW,P , for both one- and two-hop input ﬁltering.
This theorem is proven by constructing two models that are distinguished by a c-SGNN but not by any
c-GNN. We note that there might be subsets M ⊂W × P that do not contain such models, and therefore
on which c-GNNs and c-SGNNs have the same approximation power.
4.3
Examples
While a generic universality theorem on random graphs seems to be out-of-reach for the moment, we
examine several interesting examples, and pave the way for future extensions. We focus on c-SGNNs,
but, sometimes, may not conclude on the power of c-GNNs: we could not prove that they are universal,
but were not able to ﬁnd a counter-example either. For simplicity, and since our purpose here is mainly
illustrative, we mostly focus on one-hop input ﬁltering.
Stochastic Block Models.
SBMs [16] are classical models to emulate graphs with communities. In
our settings, SBMs with K communities can be obtained with a ﬁnite latent space |X| = K, typically
X = {1, . . . , K}. The kernel W(k, k′) = Wkk′ can be represented as a matrix W ∈SK, where SK is the
set of symmetric matrices in [0, 1]K×K, and the distribution P(k) = Pk as a vector P ∈∆K−1, where
∆K−1 = {P ∈[0, 1]K, P
k Pk = 1} is the (K–1)-dimensional simplex.
In the following proposition, we ﬁx P and examine universality with respect to W. In this case, continuous
GNNs are actually quite similar to discrete ones on matrices W, except that the probability vector P also
intervenes in the computation. While GNNs on ﬁnite graphs can only be universal when using high-order
tensors [29, 20] due to invariance to graph-isomorphism, here P can help to disambiguate this constraint.
We will say that P ∈∆K−1 is incoherent if: for signs s ∈{−1, 0, 1}k, having PK
k=1 skPk = 0 implies
7

s = 0. That is, no probability is an exact sum or diﬀerence of the others. We note that a vector drawn
uniformly on ∆K−1 is incoherent with probability 1. For incoherent probability vectors, we can show
universality of c-SGNNs. Moreover, this is actually a case where we can prove that c-GNNs are, in turn,
not universal.
Proposition 3. For one-hop input ﬁltering, if P is incoherent the space of functions W →¯ΨW,P is dense
in C(SK, R). Moreover, there exists P incoherent and W ̸= W ′ such that, for any c-GNN, ¯ΦW,P = ¯ΦW ′,P .
Additive kernel.
Let us now ﬁx W and examine universality with respect to P. A classical theorem
on symmetric continuous functions [42] states that any W can be arbitrarily well approximated as
W(x, y) ≈u(v(x) + v(y)) for some functions u, v. Inspired by this result, a kernel will be said to be
additive if it can (exactly) be written as W(x, y) = u(v(x) + v(y)), and injectively additive if both
u, v are continuous and injective. We prove universality in the unidimensional case below.
Proposition 4. Let X ⊂R, and ˜P be any compact subset of P. Assume W is injectively additive with
Im(v) ⊂R. For one-hop ﬁltering, the space of functions P →¯ΨW,P is dense in C( ˜P, R).
It is easy to see that injectively additive kernels include all SBMs for which Wij ̸= Wij′ when j ̸= j′, so
Prop. 4 completes Prop. 3. However, unlike additive kernels, injectively additive kernels are a priori not
universal approximators of symmetric continuous functions: this result [42] is only valid when Im(v) can
be multidimensional. But it is known for instance that there is no continuous injective map from [0, 1]2 to
[0, 1], so if Im(v) = [0, 1]2, u cannot be both continuous and injective.
Radial kernel.
We conclude this section with an important class of kernels, radial kernels W(x, y) =
w(∥x −y∥) for some function w : R+ →[0, 1]. They include the popular Gaussian kernel and so-called
ε-graphs. Below, we give an example in one dimension, for which c-SGNNs are universal on symmetric
distributions. The case of non-symmetric distributions seems more involved and we leave it for future
investigations.
Proposition 5. Assume that X = [−1, 1] and W(x, y) = w(|x −y|) where w is continuous and injective.
Let ˜P ⊂P be any compact set of symmetric distributions. For one-hop input ﬁltering, the space of
functions P →¯ΨW,P is dense in C( ˜P, R).
5
Approximation power: permutation-equivariant case
In the equivariant case, recall that the outputs of c-(S)GNNs are functions on X. The “traditional” notion
of universality is to evaluate the approximation power of mappings (W, P) →F, where F is some space of
equivariant functions, as is done for the discrete case in [20, 1]. However, a potentially simpler and more
relevant notion here is to ﬁx (W, P), and directly examine the properties of the space of functions X →R
represented by c-(S)GNNs, that is, the space of functions {ΨW,P : X →R} for all possible c-SGNNs ΨW,P
(and similar for ΦW,P ). Indeed, this directly answers such questions as: given an SBM, does there exist a
c-GNN that can labels the communities (Fig. 1)? Or: given the structure of a mesh, what functions can
be computed on it, e.g. for segmentation?
5.1
Generic result with Stone-Weierstrass theorem
As in the invariant case, the Stone-Weierstrass theorem yields a generic separation condition.
Proposition 6. Let (W, P) be ﬁxed. Then c-SGNNs (resp. c-GNNs) are dense in C(X, R) iﬀ: for all x ̸=
x′ ∈X, there is a c-SGNN (resp. a c-GNN) such that ΨW,P (x) ̸= ΨW,P (x′) (resp. ΦW,P (x) ̸= ΦW,P (x′)).
If (W, P) are such that c-(S)GNNs satisfy some symmetry or invariance (see for instance Prop. 9), then
the space X can be quotiented to obtain universality among functions satisfying these constraints.
8

Figure 3: Illustration of Prop. 9 on Gaussian kernel, but with random edges and two-hop input ﬁltering. The
x-axis is the latent variables in X = [−1, 1]. The y-axis is the output of a SGNN trained to approximate some
function f (red curve). On the left, both distribution P and f are symmetric, c-SGNNs are universal in that case.
In the center, P is symmetric but not f, and the training expectedly fails since the limit c-SGNN is symmetric.
On the right, P and f are non-symmetric, and universality holds again. Details can be found in App. E.
5.2
c-SGNNs are more powerful than c-GNNs
Using a proof similar to Theorem 3, we can then prove that c-SGNNs are indeed strictly more powerful
than c-GNNs for some (W, P).
Theorem 4. For both one- and two-hop input ﬁltering, the following holds. For any (W, P), the set of
functions of the form ΦW,P is included in the set of functions ΨW,P , and there exist (W, P) such that the
inclusion is strict.
Again, we note that, for some random graph models (W, P), c-SGNNs and c-GNNs might have the same
approximation power.
5.3
Examples
We treat the same examples as before, with the addition of two-hop ﬁltering for SBMs and radial kernels
on the d-dimensional sphere.
SBM.
Universality in the SBM case corresponds to being able to distinguish communities, that is,
ΨW,P : X →R returns a diﬀerent value for each element of the latent space X. In the following result,
we assume that W is invertible, and prove the result for both one- and two-hop ﬁltering, meaning that
c-SGNNs can indeed distinguish communities of SBMs with random edges under some mild conditions.
On the other hand, c-GNNs may fail on such models.
Proposition 7. Let P be incoherent, and W be invertible. For both one- and two-hop input ﬁltering,
c-SGNNs are dense in C(X, R). Moreover, there exist (W, P) satisfying the conditions above such that any
c-GNN ΦW,P is a constant function.
This proposition is illustrated in Fig. 1: on an SBM with constant degree function, any GNN converges to
a c-GNN with constant output, while a SGNN with two-hop input ﬁltering can be close to a c-SGNN that
perfectly separates the communities.
Additive kernel.
As in the invariant case, injectively additive kernels lead to universality.
Proposition 8. Assume W is injectively additive, ﬁx any P. Then, for one-hop input ﬁltering, c-SGNNs
are dense in C(X, R).
9

Radial kernel.
Unlike the invariant case (Prop. 5) which was limited to symmetric distributions, we
treat both symmetric and non-symmetric case: when P is symmetric, then so is ΨW,P (by permutation-
equivariance), and we have universality among symmetric functions. When P is non-symmetric, we have
universality among all functions. See Fig. 3 for an illustration.
Proposition 9. Consider X = [−1, 1], W(x, y) = w(|x−y|) a radial kernel with an invertible, continuous
w, and P with a piecewise continuous density such that EP X = 0. Then, for one-hop input ﬁltering: if
P is symmetric, c-SGNNs are dense in the space of symmetric functions in C(X, R), and if P is not
symmetric, c-SGNNs are dense in C(X, R).
Finally, we look at radial kernels on the d-dimensional sphere X = Sd−1 = {x ∈Rd | ∥x∥= 1}, an important
example sometimes referred to as random geometric graphs [30]. In this case, the kernel only depends on
the dot product W(x, y) = w(x⊤y). Denoting by dτ the uniform measure on Sd−1, it is known [11, 7]
that functions in L2(dτ) can be uniquely decomposed as f(x) = P
k⩾0 fk(x) = P
k⩾0
PN(d,k)
j=1
ak,jYk,j(x)
where Yk,j are spherical harmonics, that is, homogeneous harmonic polynomials of degree k which form an
orthonormal basis of L2(dτ). We will say that such a function is injectively decomposed if the mapping
x →[fk(x)]k⩾0 from Sd−1 to ℓ2(R) is injective. Note that generically this is veriﬁed if fk is non-zero for
more than d −1 distinct values of k > 0, as this corresponds to solving an over-determined system of
polynomial equations, but there may be degenerate situations where this is not enough. The proof of
the following proposition is based on the well-known Legendre/Gegenbauer polynomial decomposition of
spherical harmonics [7] (see App. C.5).
Proposition 10. Assume that X = Sd−1, that W(x, y) = w(x⊤y) with continuous invertible w : [−1, 1] →
[0, 1], and that P = fdτ has a density f which is injectively decomposed. Then for one-hop input ﬁltering
c-SGNNs are dense in C(X, R).
6
Conclusion and outlooks
It is known that permutation-invariant GNNs fail to distinguish regular graphs of the same order, and
permutation-equivariant GNNs return constant output on regular graphs. Similarly, their continuous
counterparts suﬀer from the same ﬂaw on random graph with constant or almost-constant degree function
[27]. However, we showed that the recently proposed SGNNs converge to continuous architectures which,
like in the discrete world, are strictly more powerful than c-GNNs. Moreover, we proved that both
permutation-invariant and permutation-equivariant c-SGNNs are universal on many random graph models
of interest, including a large class of SBMs and random geometric graphs.
We believe that our work opens many possibilities for future investigations. We examined very simple
strategies for choosing the inputs Eq(A) of the SGNN, but more complex, spectral-based choices could
exhibit better convergence properties and approximation power. We showed universality in speciﬁc random
graph models of interest, but deriving a more generic criterion is still an open question. More directly,
most of our examples illustrate the one-dimensional case X ⊂R, and a generalization to multidimensional
latent spaces would be an important step forward. Besides SGNNs, architectures that include high-order
tensors [28] (sometimes called FGNN [1]) are known to be more powerful than the WL test. Conditions for
their convergence on large graphs are still open, in particular since high-order tensors lead to high-order
operators that may be diﬃcult to manipulate. Finally, we remark that directly estimating the latent
variables xi is a classical task in statistics, for which conditions of success have been derived for various
approaches, e.g. for Spectral Clustering [22]. Comparing them with (S)GNNs is an important path for
future work.
References
[1] Wa¨ıss Azizian and Marc Lelarge. Characterizing the expressive power of invariant and equivariant
graph neural networks. In International Conference on Learning Representations (ICLR), 2021.
10

[2] L´aszl´o Babai.
Graph isomorphism in quasipolynomial time.
Proceedings of the Annual ACM
Symposium on Theory of Computing, 2016.
[3] Michael M. Bronstein, Joan Bruna, Yann Lecun, Arthur Szlam, and Pierre Vandergheynst. Geometric
Deep Learning: Going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.
[4] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count
substructures? In Advances in Neural Information and Processing Systems (NeurIPS), 2020.
[5] Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural
networks. In International Conference on Learning Representations (ICLR), 2019.
[6] Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with GNNs. In Advances in Neural Information
Processing System (NeurIPS), pages 1–19, 2019.
[7] Feng Dai and Yuan Xu. Approximation Theory and Harmonic Analysis on Spheres and Balls.
Springer, 2013.
[8] Gwendoline de Bie, Gabriel Peyr´e, and Marco Cuturi. Stochastic deep networks. In International
Conference on Machine Learning, 2018.
[9] Micha¨el Deﬀerrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional Neural Networks on
Graphs with Fast Localized Spectral Filtering. In Advances in Neural Information and Processing
Systems (NIPS), 2016.
[10] Nima Dehmamy, Albert-L`aszl`o Barab`asi, and Rose Yu. Understanding the representation power of
graph neural networks in learning graph topology. In Advances in Neural Information and Processing
Systems (NeurIPS), 2019.
[11] Costas Efthimiou and Christopher Frye. Spherical harmonics in p dimensions. World Scientiﬁc, 2014.
[12] Vikas K. Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks. In International Conference on Machine Learning (ICML), pages 1–22, 2020.
[13] Floris Geerts. The expressive power of kth-order invariant graph networks. arXiv:2007.12035, 2020.
[14] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
Message Passing for Quantum Chemistry. In International Conference on Machine Learning (ICML),
pages 1–14, 2017.
[15] William L. Hamilton. Graph Representation Learning. Morgan & Claypool, 2020.
[16] Paul W Holland. Stochastic blockmodels: First steps. Social Networks, 5(2):109–137, 1983.
[17] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer Feedforward Networks are
Universal Approximators. Neural Networks, 2:359–366, 1989.
[18] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. Neural
Information Processing Systems (NeurIPS), 2020.
[19] Nicolas Keriven, Alberto Bietti, and Samuel Vaiter. Convergence and Stability of Graph Convolutional
Networks on Large Random Graphs. In Advances in Neural Information and Processing Systems
(NeurIPS), pages 1–26, 2020.
[20] Nicolas Keriven and Gabriel Peyr´e. Universal Invariant and Equivariant Graph Neural Networks. In
Advances in Neural Information Processing Systems (NeurIPS), pages 1–19, 2019.
11

[21] Thomas N Kipf and Max Welling. Semi-Supervised Learning with Graph Convolutional Networks.
In International Conference on Learning Representations (ICLR), 2017.
[22] Jing Lei and Alessandro Rinaldo. Consistency of spectral clustering in stochastic block models.
Annals of Statistics, 43(1):215–237, 2015.
[23] Ron Levie, Michael M. Bronstein, and Gitta Kutyniok. Transferability of spectral graph convolutional
neural networks. arXiv:1907.12972, pages 1–41, 2019.
[24] Andreas Loukas. How hard is to distinguish graphs with graph neural networks? In Advances in
Neural Information and Processing Systems (NeurIPS), 2020.
[25] Andreas Loukas. What graph neural networks cannot learn: depth vs width. In ICLR, 2020.
[26] L´aszl´o Lov´asz. Large networks and graph limits. Colloquium Publications, 60, 2012.
[27] Abram Magner, Mayank Baranwal, and Alfred O. Hero. The Power of Graph Convolutional Networks
to Distinguish Random Graph Models. arXiv:1910.12954, pages 1–27, 2019.
[28] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably Powerful Graph
Networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 1–12, 2019.
[29] Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the Universality of Invariant
Networks. In International Conference on Machine Learning (ICML), 2019.
[30] Mathew Penrose. Random Geometric Graphs. Oxford University Press, 2008.
[31] Allan Pinkus.
Approximation theory of the MLP model in neural networks.
Acta Numerica,
8(May):143–195, 1999.
[32] Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In Advances in Neural Information Processing Systems (NIPS), 2009.
[33] Lorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators. Journal
of Machine Learning Research, 11:905–934, 2010.
[34] Luana Ruiz, Luiz F.O. Chamon, and Alejandro Ribeiro. Graphon neural networks and the trans-
ferability of graph neural networks. In Advances in Neural Information and Processing Systems
(NeurIPS), 2020.
[35] Roman Vershynin. High-dimensional probability: An introduction with applications in data science.
Cambridge University Press, 2018.
[36] Cl´ement Vignac, Andreas Loukas, and Pascal Frossard. Building powerful and equivariant graph
neural networks with message-passing. In Advances in Neural Information and Processing Systems
(NeurIPS), 2020.
[37] Ulrike Von Luxburg, Mikhail Belkin, and Olivier Bousquet. Consistency of spectral clustering. Annals
of Statistics, 36(2):555–586, 2008.
[38] B Yu Weisfeiler and A A Leman. The Reduction of a Graph to Canonical Form and the Algebra
Which Appears Therein. Nti, 2(9):12–16, 1968.
[39] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
Comprehensive Survey on Graph Neural Networks. IEEE Transactions on Neural Networks and
Learning Systems, pages 1–21, 2020.
[40] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural
Networks? In ICLR, pages 1–15, 2019.
12

[41] Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, and Haggai Maron. On Size Generalization
in Graph Neural Networks. arXiv:2010.08853, 2020.
[42] Manzil Zaheer, Satwik Kottur, Siamak Ravanbhakhsh, Barnab´as P´oczos, Ruslan Salakhutdinov, and
Alexander J. Smola. Deep sets. Advances in Neural Information Processing Systems (NeurIPS),
2017-December(ii):3392–3402, 2017.
[43] Aaron Zweig and Joan Bruna. A functional perspective on learning symmetric functions with neural
networks. arXiv, pages 1–19, 2020.
13

A
Convergence
Let us start with some notations. Given a GNN Φ, we deﬁne some bounds on its parameters that will be used
in the multiplicative constants of the theorem. Recall that the ﬁlters are written h(ℓ)
ij (λ) = P∞
k=0 β(ℓ)
ijkλk. We
deﬁne B(ℓ)
k
=

β(ℓ)
ijk

ji ∈Rdℓ+1×dℓthe matrix containing the order-k coeﬃcients, and by B(ℓ)
k,|·| =
β(ℓ)
ijk


ji
the same matrix with absolute value on all coeﬃcients. Recall that ∥·∥is the operator norm for matrices.
We deﬁne the following bounds:
H(ℓ)
2
=
X
k
B(ℓ)
k

H(ℓ)
∂,2 =
X
k
B(ℓ)
k
 k
H(ℓ)
∞=
B(ℓ)
0,|·|
 +
X
k⩾1
B(ℓ)
k

H(ℓ)
∂,∞=
X
k
B(ℓ)
k
 k
p
log k
which all converge by our assumptions on the βk. We may also denote H2 by HL2(P ) for convenience but
this quantity does not depend on P. Note that, only for H∞, we use the spectral norm of the matrix
B0,|·| with non-negative coeﬃcients, which is suboptimal compared to using B0. This is due to a part of
our analysis where we do not operate in a Hilbert space but only in a Banach space B(X), see Lemma 4.
We also deﬁne
b(ℓ) =
qP
j(b(ℓ)
j )2 to measure the norm of the bias.
Given X = {x1, . . . , xn} and any dimension d, we denote by SX the sampling operator acting on functions
f : X →Rd deﬁned by SXf
def.
= [f(x1), . . . , f(xn)] ∈Rn×d. We have
 1
√nSXf

F ⩽∥f∥∞. Finally, given
X and W, we deﬁne W(X)
def.
= (W(xi, xj))ij ∈Rn×n, and remark that W (X)
n
◦SX = SX ◦TW,X. In the
deterministic edges case, the adjacency matrix A is directly W(X). In the random edges case, A has
expectation W(X) (conditionally on X).
A.1
Convergence of GNNs: proof of Theorem 1
This proof is a variant of [19]. We prove Theorem 1 with the following error terms:
R1(n) =
C1
√dX + C2
r
log
 P
ℓdℓ
ρ

√n
R2(n) = CνC3
√αnn
R3(n) = C4
r
log(1/ρ)
n
,
(11)
where R2 is present in the random edges case and R3 in the permutation-invariant case, and the following
constants:
C = Lg
M−1
Y
ℓ=0
H(ℓ)
2
C1 =
M−1
X
ℓ=0
C(ℓ)H(ℓ)
∂,∞
C2 = C1(1 + DX LW )
C3 =
M−1
X
ℓ=0
C(ℓ)H(ℓ)
∂,2
C4 = C(M)
where the MLP g is Lg-Lipschitz and DX = supx,x′∈X mX (x, x′) is the diameter of X, and
C(ℓ) = Lg
 M−1
Y
s=ℓ+1
H(s)
2
!  
Cf
ℓ−1
Y
s=0
H(s)
∞+
ℓ−1
X
s=0
b(s)
ℓ−1
Y
p=s+1
H(p)
∞
!
with the conventions that an empty product is 1 and an empty sum is 0.
We begin the proof by the equivariant case, the invariant case will simply use an additional concentration
14

inequality. We have
MSEX

ΦA(Z(0)), ΦW,P (f)

=
1
√n
ΦA(Z(0)) −SXΦW,P (f (0))

F
⩽Lg
1
√n
Z(M) −SXf (M)
F .
We therefore seek to bound that last term. Deﬁne the notation
∆(ℓ) =
1
√n
v
u
u
tX
j

X
i

h(ℓ)
ij
A
n

SXf (ℓ)
i
−SXh(ℓ)
ij (TW,P )f (ℓ)
i

2
.
Then, using the Lipschitzness of ρ, Lemma 4 with ∥A/n∥⩽1, and the fact that SX ◦ρ = ρ ◦SX, we have
Z(ℓ+1) −SXf (ℓ+1)
F
=

X
j
ρ
 dℓ
X
i=1
h(ℓ)
ij
A
n

Z(ℓ)
:,i + b(ℓ)
j 1n
!
−SXρ
 dℓ
X
i=1
h(ℓ)
ij (TW,P )f (ℓ)
i
+ b(ℓ)
j
!
2

1
2
=

X
j
ρ
 dℓ
X
i=1
h(ℓ)
ij
A
n

z(ℓ)
i
+ b(ℓ)
j 1n
!
−ρ
 
SX
 dℓ
X
i=1
h(ℓ)
ij (TW,P )f (ℓ)
i
+ b(ℓ)
j
!!
2

1
2
⩽

X
j

dℓ
X
i=1
h(ℓ)
ij
A
n

Z(ℓ)
:,i −SXh(ℓ)
ij (TW,P )f (ℓ)
i

2

1
2
⩽

X
j

dℓ
X
i=1
h(ℓ)
ij
A
n
 
Z(ℓ)
:,i −SXf (ℓ)
i

2

1
2
+

X
j

dℓ
X
i=1
h(ℓ)
ij
A
n

SXf (ℓ)
i
−SXh(ℓ)
ij (TW,P )f (ℓ)
i

2

1
2
⩽H(ℓ)
2
Z(ℓ) −SXf (ℓ)
F + √n∆(ℓ).
A recursion shows that, for all Z(0):
MSEX

ΦA(Z(0)), ΦW,P (f)

⩽Lg
M−1
X
ℓ=0
∆(ℓ)
M−1
Y
s=ℓ+1
H(s)
2
+ Lg
 M−1
Y
ℓ=0
H(ℓ)
2
!
MSEX(Z(0), f (0)).
(12)
We now bound all ∆(ℓ) with high probability. Recall that W (X)
n
◦SX = SX ◦TW,X, and that we have
15

 SX
√nf
 ⩽∥f∥∞and ∥TW,X∥∞⩽1. By Lemma 4 we have
∆(ℓ) ⩽
v
u
u
tX
j

X
i

h(ℓ)
ij
A
n

−h(ℓ)
ij
W(X)
n
 SX
√nf (ℓ)
i

2
+
v
u
u
tX
j

X
i
SX
√n

h(ℓ)
ij (TW,X) −h(ℓ)
ij (TW,P )

f (ℓ)
i

2
⩽H(ℓ)
∂,2

A −W(X)
n

f (ℓ)
∞
+
X
k
∥Bk∥
v
u
u
u
t
X
i
 k−1
X
p=0
(TW,X −TW,P )T k−1−p
W,P
f (ℓ)
i

∞
!2
.
(13)
The ﬁrst term in (13) is 0 in the deterministic edges case. In the random edges case, it is handled with
a recent concentration inequality for Bernoulli matrices [22], recalled in Theorem 5 in App. D. Since
αn ≳log n
n , for any ν, there is a constant Cν such that, with probability 1 −n−ν on the random edges
(conditionally on X),
 A−W (X)
n
 ⩽
Cν
√αnn. By the law of total probability, it is valid with joint probability
1 −n−ν on both X and the random edges.
We now bound the second term in (13). Deﬁne ρk =
Cρ
(k+1)2 P
ℓdℓwith C such that P
kℓdℓρk = ρ/4 (even
when the ﬁlters are not of ﬁnite order). Using an application of Dudley’s inequality detailed in Lemma 3,
applied with U(x, y) = W(x, y)f(y) which is bounded by ∥f∥∞and has Lipschitz constant LW ∥f∥∞in
the ﬁrst variable, and a union bound, we obtain with probability 1 −ρ/4 that: for all i, ℓ, k, we have
(TW,X −TW,P )T k
W,P f (ℓ)
i

∞≲
1
√n
f ℓ
i

∞
p
dX + (1 + DX LW )
q
log ρ−1
k

.
Coming back to the second term of (13), with probability 1 −ρ/4:
X
k
∥Bk∥
v
u
u
tX
i
 k−1
X
p=0
(TW,X −TW,P )T k−1−p
W,P
f (ℓ)
i

∞
2
!
≲
√dX + (1 + DX LW )
q
log
P
ℓdℓ
ρ

√n
X
k
∥Bk∥k
p
log k
sX
i
f (ℓ)
i

2
∞
⩽
√dX + (1 + DX LW )
q
log
P
ℓdℓ
ρ

√n
H(ℓ)
∂,∞
f (ℓ)
∞.
At the end of the day we obtain that with probability 1 −ρ, for all ℓ:
∆(ℓ) ∝
f (ℓ)
∞




H(ℓ)
∂,2
√αnn +
H(ℓ)
∂,∞
√
d + (1 + DX LW )
q
log
P
ℓdℓ
ρ

√n



.
We then use Lemma 5 to bound
f (ℓ)
∞and conclude.
16

For the invariant case, we have
¯ΦA(Z(0)) −¯ΦW,P (f (0))
 ⩽MSEX(ΦA(Z(0)), ΦW,P (f (0)))
+ Lg

1
n
n
X
i=1
f (M)(xi) −
Z
f (M)(x)dP(x)

We use a vector Hoeﬀding’s inequality [32, Lemma 4] and a bound on
f (M)
∞(Lemma 5) to conclude.
A.2
Convergence of SGNNs
We prove Theorem 2 with the same form of error terms (11) where Ri is replaced by R′
i with modiﬁed
multiplicative constants C′
i. Here we will have:
C′ = DLg
M−1
Y
ℓ=0
H(ℓ)
2
C′
1 = DLΦ +
M−1
X
ℓ=0
H′(ℓ)
∂,∞C′(ℓ) +
M−1
X
ℓ=0
H(ℓ)
∂,∞C(ℓ)
C′
2 = (1 + DX LW )
M−1
X
ℓ=0
H′(ℓ)
∂,∞C′(ℓ) + DLΦDX + DCΦ +
M−1
X
ℓ=0
H(ℓ)
∂,∞(C(ℓ) + DX L(ℓ))
C′
3 =
M−1
X
ℓ=0
H′(ℓ)
∂,2 C′(ℓ) + H(ℓ)
∂,2C(ℓ),
C′
4 = C′(M)
where H′(ℓ)
⋆
is like H(ℓ)
⋆
but for the weights in Φ′, the ﬁnal-layer MLP of Φ′ is denoted by g′ with a
Lipschitz constant Lg′, and:
D = Lg′
M−1
Y
ℓ=0
H′(ℓ)
2
C′(ℓ) = Lg′
 M−1
Y
s=ℓ+1
H′(s)
2
!  
CΦ
ℓ−1
Y
s=0
H′(s)
∞+
ℓ−1
X
s=0
b′(s)
ℓ−1
Y
p=s+1
H′(p)
∞
!
C(ℓ) = DLg
 M−1
Y
s=ℓ+1
H(s)
2
!
˜C(ℓ)
∞
L(ℓ) = DLg
 M−1
Y
s=ℓ+1
H(s)
2
!  
LW ˜C(ℓ)
∞+
p
dℓLη
ℓ−1
Y
s=0
H(s)
∞
!
CΦ = ∥g(0)∥+ Lg ˜C(M)
∞
LΦ = Lg
 
Lη
M−1
Y
ℓ=0
B(ℓ)
0
 + LW
M−1
X
ℓ=0
 M−1
Y
s=ℓ+1
B(s)
0

!
˜C(ℓ)
2
!
with
˜C(ℓ)
⋆
= Cη
ℓ−1
Y
s=0
H(s)
⋆
+
ℓ−1
X
s=0
b(s)
ℓ−1
Y
p=s+1
H(p)
⋆
for ⋆∈{2, ∞} .
We start by applying Theorem 1 on the outer GNN Φ′. Since the result is uniformly valid over all input
17

of the GNN Z(0) with probability 1 −ρ:
MSEX(ΨA, ΨW,P ) ⩽DMSEX
 
1
n
X
q
ΦA(Eq(A)),
Z
ΦW,P (η(·, x))dP(x)
!
+ R′(n)
(14)
where, from Theorem 1, D = Lg′ QM−1
ℓ=0 H′(ℓ)
2
is C1 but for the weights in Φ′, and R′(n) is the error term
formed by summing various R′
i(n), taking into account that by Lemma 5 the function inputed in Φ′ is
bounded by CΦ.
We must therefore bound the ﬁrst term in (14). We write
MSEX
 
1
n
X
q
ΦA(Eq(A)),
Z
ΦW,P (η(·, x))dP(x)
!
⩽MSEX
 
1
n
X
q
ΦA(Eq(A)), 1
n
X
q
ΦW,P (η(·, xq))
!
+ MSEX
 
1
n
X
q
ΦW,P (η(·, xq)),
Z
ΦW,P (η(·, x))dP(x)
!
⩽sup
q MSEX (ΦA(Eq(A)), ΦW,P (η(·, xq)))
+

1
n
X
q
ΦW,P (η(·, xq)) −
Z
ΦW,P (η(·, x))dP(x)

∞
.
(15)
Let us start with the second term. By Lemma 5 and the Lipschitzness of g, U(x, y)
def.
= ΦW,P (η(·, y))(x) is
CΦ-bounded and LΦ-Lipschitz with respect to x.
Hence, applying Lemma 3: with probability 1 −ρ,

1
n
X
q
ΦW,P (η(·, xq)) −
Z
ΦW,P (η(·, x))dP(x)

∞
≲LΦ
√dX + (LΦDX + CΦ)
p
log(1/ρ)
√n
.
(16)
For the ﬁrst term in (15), we introduce some notations. We denote by f (ℓ)
i
: X × X →R the bivariate
function propagated at each layer of the inner part of the c-SGNN, as:
f (0)
0
= η
f (ℓ+1)
j
= ρ
 X
i
h(ℓ)
ij (TW,P )f (ℓ)
i
+ b(ℓ)
j
!
(17)
where TW,P is here to be understood as an operator on C(X×X) deﬁned by TW,P [f](x, y) =
R
W(x, z)f(z, y)dP(z).
With these notations, ΦW,P (η(·, xq)) = g(f (M)(·, xq)). Note that we still have ∥TW,P ∥∞⩽1 for this
version. We perform the computation as in the proof of Theorem 1 in (12) to obtain:
sup
q MSEX (ΦA(Eq(A)), ΦW,P (η(·, xq)))
⩽Lg
M−1
X
ℓ=0
sup
q ∆(ℓ)
q
M−1
Y
s=ℓ+1
H(s)
2
+ Lg
 M−1
Y
ℓ=0
H(ℓ)
2
!
sup
q MSEX(Eq(A), η(·, xq))
(18)
with
∆(ℓ)
q
=
1
√n
v
u
u
tX
j

X
i

h(ℓ)
ij
A
n

SXf (ℓ)
i
(·, xq) −SXh(ℓ)
ij (TW,P )[f (ℓ)
i
(·, xq)]

2
.
(19)
18

Then, again we decompose
sup
q ∆(ℓ)
q
⩽H(ℓ)
∂,2

A −W(X)
n

f (ℓ)
∞
+
X
k
∥Bk∥
v
u
u
u
t
X
i
 k−1
X
p=0
(TW,X −TW,P )T k−1−p
W,P
[f (ℓ)
i
]

∞
!2
(20)
where we recall here that f (ℓ)
i
is a bivariate function.
Again, the ﬁrst term is 0 in the deterministic edges case, and otherwise by Theorem 5 we have
 A−W (X)
n
 ⩽
Cν/√αnn with probability 1 −n−ν, and by Lemma 6 we have
f (ℓ)
∞⩽Cη
ℓ−1
Y
s=0
H(s)
∞+
ℓ−1
X
s=0
b(s)
ℓ−1
Y
p=s+1
H(p)
∞.
Fix k, ℓ, i for now. We will apply Lemma 3 with U : (X × X) × X →R deﬁned as U((x, x′), y) =
W(x, y)f(y, x′) for f(y, x′) = T k
W,P [f (ℓ)
i
(·, x′)](y). Since ∥TW,P ∥∞⩽1 we have ∥f∥∞⩽
f (ℓ)
i

∞. Then,
T k
W,P [f (ℓ)
i
(·, x′)] −T k
W,P [f (ℓ)
i
(·, x′′)]

∞=
T k
W,P [f (ℓ)
i
(·, x′) −f (ℓ)
i
(·, x′′)]

∞
⩽
f (ℓ)
i
(·, x′) −f (ℓ)
i
(·, x′′)

∞
⩽LηmX (x, x′)
ℓ−1
Y
s=0
H(s)
∞
by Lemma 6. Hence U is bounded by
f (ℓ)
i

∞and L(ℓ)
i -Lipschitz with respect to (x, x′), with
L(ℓ)
i
= LW
f (ℓ)
i

∞+ Lη
ℓ−1
Y
s=0
H(s)
∞.
(21)
Finally, note that X × X is compact with covering numbers proportional to ε−2d. Hence by Lemma 3 and
a union bound, again deﬁning ρk as in the proof of Theorem 1 such that P
ikℓρk = ρ: with probability
1 −ρ, we have simultaneously for all i, k, ℓ:
sup
x
(TW,X −TW,P )T k
W,P f (ℓ)
i
(·, x)

∞
≲
1
√n
f (ℓ)
i

∞
p
dX + (
f (ℓ)
i

∞+ DX L(ℓ)
i )
q
log ρ−1
k

.
Hence, as in the previous proof:
X
k
∥Bk∥
v
u
u
tX
i
 k−1
X
p=0
(TW,X −TW,P )T k−1−p
W,P
f (ℓ)
i

∞
2
!
⩽
f (ℓ)
∞
√dX + (
f (ℓ)
∞+ DX L(ℓ))
q
log
P
ℓdℓ
ρ

H(ℓ)
∂,∞
√n
where L(ℓ) = (P
i(L(ℓ)
i )2)
1
2 .
19

A.3
Proof of Prop. 1
The error can be written as
1
√n
A2eq/n −SXTW,P (W(·, xq))

2 ⩽
1
√n
A2eq/n −W(X)2eq/n

2
+ ∥(TW,X −TW,P )W(·, xq)∥∞.
Using chaining as in the previous section, we have
sup
x ∥(TW,X −TW,P )W(·, x)∥∞≲
1
√n
p
dX + (1 + DX LW )
p
log 1/ρ

.
The ﬁrst term is 0 in the deterministic edges case, and otherwise:
1
√n
A2eq/n −W(X)2eq/n

2 =


1
n
X
i

1
n
X
j
aijajq −1
n
X
j
W(xi, xj)W(xj, xq)


2


1
2
⩽


1
n
X
i̸=q

1
n
X
j
aijajq −W(xi, xj)W(xj, xq)


2


1
2
+
1
√n

1
n
X
j
a2
jq −1
n
X
j
W(xj, xq)2


2
.
Now, by Bernstein inequality with
V ar(aijajq) ⩽E(a2
ija2
jq) = α−2
n E(aijajq) = α−2
n W(xi, xj)W(xj, xq)
for i ̸= q and a union bound, with proba 1 −δ, we have:

1
n
X
j
aijajq −1
n
X
j
W(xi, xj)W(xj, xq)

≲
p
log(n/δ)
αn
√n
for all q and i ̸= q.
Since
 1
n
P
j a2
jq −1
n
P
j W(xj, xq)2 ⩽2, we have
1
√n sup
q
A2eq/n −SXTW (W(·, xq))

2 ≲
p
log(n/δ)
αn
√n
+
√dX + (1 + DX LW )
p
log 1/ρ
√n
(22)
B
Approximation power: invariant case
B.1
Application of Stone-Weierstrass
Proof of Prop. 2. We do the proof for cSGNNws, it is exactly similar for cGNNs.
This is a direct application of Lemma 1: for any two cSGNNs ¯Ψ : W × P →Rd, ¯Ψ′ : W × P →Rd′, their
concatenation [¯Ψ, ¯Ψ′] : W × P →Rd+d′ is also a cSGNN (if they do not use the same input transforms
η, η′, one can concatenate η′′ = [η, η′]), and for any MLP g, g ◦¯Ψ is also a cSGNN.
One must just check that cSGNNs are continuous with respect to ∥·∥∞+ ∥·∥TV on W × P:
• (W, P) 7→η is continuous by assumption ;
20

• for any fW,P ∈C(X, Rd) continuously indexed by (W, P),
∥TW,P [fW,P ] −TW ′,P ′[fW ′,P ′]∥∞⩽∥fW,P ∥∞(∥W −W ′∥∞+ ∥P −P ′∥TV)
+ ∥fW,P −fW ′,P ′∥∞
and similarly for
R
fW,P dP −
R
fW ′,P ′dP ′ ;
• the non-linearity ρ is Lipschitz.
B.2
cSGNNs are more powerful than cGNNs
Proof of Theorem 3. By construction, cGNNs are included in cSGNNs, since one can take Φ = 0 as the
input GNN before pooling in (6).
To prove strict inclusion, we will construct two models (W, P), (W ′, P ′) such that, for any cGNN we
have ¯ΦW,P = ¯ΦW ′,P ′, but there exists a cSGNN such that ¯ΨW,P = ¯ΨW ′,P ′. We do the proof in the
random edges case with two-hop input ﬁltering ηW,P = TW,P (W), since such cSGNNs can of course also
be constructed in the deterministic edges case.
Since X is not a singleton, one can can single out two arbitrary elements x, x′ and take P as a sum of two
Diracs over them, which is equivalent to considering that X = {x, x′} (since any invariant architecture
involves a ﬁnal integration by P, it is useless to consider W outside of the support of P). This results in a
two-community SBM, for which P can be represented as a 2-vector on the simplex and W as a 2-by-2
symmetric matrix. We then consider a family of SBMs indexed by γ ∈[0, 1]:
P =
1/3
2/3

,
Wγ =
 γ
1−γ
2
1−γ
2
1+γ
4

.
It is not hard to see that TWγ,P [1] = 1/3·1 for any γ. Therefore, for any cGNN Φ, the function propagated
inside its layers is always constant, and does not depend on γ. That is, ¯ΦWγ,P = ¯ΦW0,P for any γ. On
the other hand, consider the following SGNN:
¯ΨW,P =
Z
x
Z
y
f(TW (W(·, y)))dP(y)dP(x)
where f is an MLP. By the universality theorem, f can approximate x →x2, for which we obtain:
¯ΨWγ,P ≈1/16 ∗γ4 −1/12 ∗γ3 + 1/24 ∗γ2 −1/108 ∗γ + 17/1296.
This is not a constant function, so we can always ﬁnd γ, γ′ such that ¯ΨWγ,P ̸= ¯ΨWγ′,P , which concludes
the proof.
B.3
SBMs
Proof of Prop. 3. We apply Prop. 2. We ﬁx P as an incoherent vector in the k-simplex, and deﬁne
M = {(W, P) : W ∈Sk([0, 1])} which is indeed compact. It therefore suﬃces to show that cSGNNs
separates points in M.
We proceed by contraposition: assume that W, W ′ are such that ¯ΨW,P = ¯ΨW ′,P for any cSGNN Ψ. We
must show that necessarily W = W ′. We look at cSGNNs of the form
ΨW,P =
Z
f1
Z
f0 (W(x, y)) dP(y)

dP(x)
=
X
i
Pif1(
X
j
Pjf0(Wij)) =
X
i
Pif1(
X
j
Pjf0(W ′
ij))
21

where f0, f1 are MLPs.
By the universality theorem, they can approximate any continuous func-
tion.
Pick any f0.
Then f1 can be chosen as to take only values in {0, 1} on the discrete set
{P
j Pjf0(Wij), P
j Pjf0(W ′
ij)}i of size 2K. Moreover, if there was an index i0 such that P
j Pjf0(Wi0j) ̸=
P
j Pjf0(W ′
i0j), f1 can be chosen to give diﬀerent values on them. Then, deﬁning si = f1(P
j Pjf0(Wij))−
f1(P
j Pjf0(W ′
ij)) ∈{−1, 0, 1}, we have both si0 ̸= 0 and
X
i
Pisi =
X
i
Pi

f1(
X
j
Pjf0(Wij)) −f1(
X
j
Pjf0(W ′
ij))

= 0
which contradicts the incoherence of P. So, for all f0 and i, we have P
j Pjf0(Wij) = P
j Pjf0(W ′
ij). By
the exact same reasoning on f0, we obtain that for all i, j, Wij = W ′
ij, which concludes the proof.
For the failure of c-GNNs, the proof is immediate using the example SBM in the proof of Theorem 3,
since P = [1/3, 2/3] is indeed incoherent.
B.4
Decomposed kernel
Proof of Prop. 4. Applying Prop. 2 with M = {W} × ˜P, it suﬃces to show that cSGNNs separate the
distributions in ˜P. By contraposition, assume that P, P ′ ∈˜P are such that ¯ΨW,P = ¯ΨW,P ′ for any cSGNN,
and we want to prove that necessarily P = P ′.
We look at cSGNN of the form P 7→
R
f1(
R
f0(W(x, y))dP(y))dP(x), where f0, f1 are MLPs, that can
approximate any continuous functions by the universality theorem. Since u is continuous and injective,
it is well-known that it has a continuous inverse on its image. Hence f0 can be chosen to approximate
f0 ≈u−1. By choosing f1 to approximate x →xk, we obtain that:
Z
(v(x) + EP v)k dP(x) =
Z
(v(x) + EP ′v)k dP ′(x) .
Taking k = 1 we obtain that EP v = EP ′v, and by an easy recursion we have EP vk = EP ′vk for all
k. Since v is invertible and polynomial functions are universal approximators on compacts one can
write v−1(x) = P
k akxk and x = P
k akv(x)k, such that EP Xk = EP ′Xk. Again, by the universality of
polynomial functions, EP f = EP ′f for any continuous function, which is well-known to be equivalent to
P = P ′ and concludes the proof.
B.5
Radial kernel
Proof of Prop. 5. We proceed as in the proof of Prop. 4 above: assuming P, P ′ ∈˜P are such that
¯ΨW,P = ¯ΨW,P ′ for any cSGNN, we want to prove that necessarily P = P ′. We look at cSGNN of the form
P 7→
R
f1(
R
f0(W(x, y))dP(y))dP(x), where f0, f1 are MLPs. Since w is injective f0 can approximate
(x →x2) ◦w−1. By choosing f1 to approximate x →xk, and since P, P ′ are centered we obtain
Z  x2 + EP X2k dP(x) =
Z  x2 + EP ′X2k dP ′(x) .
Taking k = 1 we have EP X2 = EP ′X2, and by an easy recursion EP X2k = EP ′X2k for all k. Since P, P ′
have 0 odd-order moments, EP Xk = EP ′Xk for all k, from which we can conclude P = P ′ as in the
previous proof.
C
Approximation power: equivariant case
C.1
Application of Stone-Weierstrass
Proof of Prop. 6. As the proof of Prop. 2, this is a direct application of Lemma 1: the set of cSGNNs is
closed by concatenation and composition with an MLP, X is compact, and any equivariant cSGNN in
continuous since by assumption W, ηW,P and ρ are.
22

C.2
cSGNNs are more powerful than cGNNs
Proof of Theorem. 4. As in the proof of Theorem 3 in App. B.2, non-strict inclusion is immediate. To
prove strict inclusion, as in the proof of Theorem 3 we consider again the same 2-community SBM but for
γ = 1/2:
P =
1/3
2/3

,
W =
1/2
1/4
1/4
3/8

.
Again any c-GNN would return a constant function ΦW,P (1) = ΦW,P (2), while if we consider the following
c-SGNN for two-hop ﬁltering:
ΨW,P =
Z
f(TW (W(·, y)))dP(y)
with f an MLP that approximates x →x2, we obtain ΨW,P (1) ≈1/8 and ΨW,P (2) ≈11/96, hence a
non-constant function.
C.3
SBMs
Proof of Prop. 7. We treat the two-hop ﬁltering case, since they are included in one-hop architectures.
By Prop. 6, we must prove the separation of elements of X, which here are discrete community labels
X = {1, . . . , K}. Fix P ∈∆K−1 incoherent and W ∈SK invertible. Assume that k, k′ are two communities
such that ΨW,P (k) = ΨW,P (k′) for all Ψ with two-hop input ﬁltering. We want to show that necessarily
k = k′. By assumption, we have:
X
i
f

X
j
WkjWjiPj

Pi =
X
i
f

X
j
Wk′jWjiPj

Pi
for any MLP f. As in the proof of Prop. 3 in App. B.3, f can approximate a function that is {0, 1}-
valued on its inputs such that, if there is an index i such that P
j WkjWjiPj ̸= P
j WkjWjiPj, then the
incoherency of P is contradicted. Hence, for all i, P
j WkjWjiPj = P
j WkjWjiPj, or in other words:
W · (P ⊙(Wk,: −Wk′,:)) = 0.
Since W is invertible and P has only non-zero coordinates (by incoherency), we obtain Wk,: = Wk′,:. Since
W is invertible it has necessarily distinct columns, so k = k′, which concludes the proof.
For the failure of c-GNNs we use the example of the proof of Theorem. 4, for which P is incoherent, W is
invertible, but any c-GNN is constant.
C.4
Additive kernel
Proof of Prop. 8. Again we apply Prop. 6. Assume that x, x′ are such that ΨW,P (x) = ΨW,P (x′) for all
one-hop c-SGNN. In particular,
Z
f (u(v(x) + v(y))) dP(y) =
Z
f (u(v(x′) + v(y))) dP(y)
for all MLP f. By taking f = u−1, we obtain v(x) = v(x′), which leads to x = x′ by assumption of
injectivity and concludes the proof.
C.5
Radial kernel
Proof of Prop. 9. Again we apply Prop. 6. Let x, x′ such that ΨW,P (x) = ΨW,P (x′) for all c-SGNNs. We
want to prove that: if P is symmetric, then x = x′ or x = −x′ (i.e. we quotient [−1, 1] by symmetry), and
if P is not symmetric, then necessarily x = x′.
By assumption
R
f(W(x, y))dP(y) =
R
f(W(x′, y))dP(y) for all MLP f.
23

If P is symmetric.
By choosing f = (·)2 ◦w−1, we have
0 = E(x −X)2 −E(x′ −X)2 = x2 + 2xEX + EX2 −(x′)2 −2x′EX −EX2 = x2 −(x′)2
which is indeed x′ = x or x′ = −x.
If P is not symmetric.
By the previous reasoning, we still have x′ = x or x′ = −x, however, we must
now show that the case x′ = −x is not possible. By contradiction, assume x′ = −x (and x ̸= 0). Denote
Mk = EP Xk the kth moment of P. By Lemma 2, P is symmetric iﬀM2k+1 = 0 for all k. We are going
to show that this is the case by recursion: that is true for k = 0 by assumption, and if M2ℓ+1 = 0 for all
ℓ⩽k −1, by taking f0(t) = t2k+2:
0 = E(x −X)2(k+1) −E(x + X)2(k+1) =
2(k+1)
X
ℓ=0
2(k + 1)
ℓ

xℓ(−1)ℓM2(k+1)−ℓ
−


2(k+1)
X
ℓ=0
2(k + 1)
ℓ

xℓM2(k+1)−ℓ


=
k+1
X
ℓ=1
2(k + 1)
2ℓ−1

x2ℓ−1M2(k+1−ℓ)+1
= 2(k + 1)xM2k+1
and therefore M2k+1 = 0, and P is symmetric, which is a contradiction. Therefore, necessarily x′ = x,
which completes the proof.
Proof of Prop. 10. Again we apply Prop. 6. Let x, x′ ∈Sd−1 such that ΨW,P (x) = ΨW,P (x′) for all
c-SGNNs. We want to prove that x = x′. In particular,
Z
g(w(x⊤y))dP(y) =
Z
g(w(x′⊤y))dP(y),
(23)
for all MLP g.
Recall that we have assumed that that P has a density f decomposed as f(x) = P
k⩾0 fk(x) =
P
k⩾0
PN(d,k)
j=1
ak,jYk,j(x), where Yk,j are spherical harmonics.
Let Pk denote the Legendre/Gegenbauer polynomial of degree k, which satisﬁes the addition formula
Pk(x⊤y) =
1
N(d, k)
N(d,k)
X
j=1
Yk,j(x)Yk,j(y).
Then, taking g = N(d, k)Pk ◦w−1, note that we have
Z
g(w(x⊤y))dP(y) = N(d, k)
Z
Pk(x⊤y)f(y)dτ(y)
=
X
j
Yk,j(x)⟨f, Yk,j⟩L2(dτ)
= fk(x).
Thus, (23) implies fk(x) = fk(x′) for all k. By assumption of injectivity of x →[fk(x)]k, necessarily
x = x′, which concludes the proof.
24

D
Additional material
Lemma 1. Let (X, d) be a compact metric space, F ⊂∪d⩾1C(X, Rd) be a subspace of continuous
multivariate functions on X that is closed by concatenation, that is, f, f ′ ∈F ⇒[f, f ′] ∈F. Deﬁne Fρ =

g ◦f | f ∈F, g : Rd →R is a MLP with non-linearity ρ
	
, where ρ is not polynomial. If F separates
points, that is, ∀x ̸= x′, ∃f ∈F, f(x) ̸= f(x′), then FMLP is dense in C(X, R) for the supremum norm.
Proof. The proof uses the classical Stone-Weierstrass theorem: an algebra of continuous functions that
separates points is dense in the space of continuous functions (for the supremum norm).
The main point is to check that Fρ is an algebra.
It is closed by linear combination: for all g, g′
MLPs, there is a g′′ such that g ◦f + g′ ◦f ′ = g′′ ◦[f, f ′] and F is closed by concatenation. Closure
by multiplication is not true in general, however, following [17], this is true when ρ = cos: since
cos(a) cos(b) = 1
2(cos(a + b) −cos(a −b)), we have: for g(x) = P
i ai cos(b⊤
i x + ci) and similarly g′,
(g ◦f) · (g′ ◦f ′) =
X
ij
aia′
j cos
 b⊤
i f(x) + ci

cos
 (b′
j)⊤f ′(x) + c′
j

=
X
ij
aia′
j
1
2

cos
 [bi, bj]⊤[f, f ′](x) + ci + c′
j

−cos
 [bi, −bj]⊤[f, f ′](x) + ci −c′
j
 
= g′′ ◦[f, f ′]
for a certain MLP g′′.
Hence Fcos is an algebra. Moreover, it separates points: for x ̸= x′, by hypothesis there is a f ∈F such
that f(x) ̸= f(x′), and by the universality theorem applied to MLPs, this is also true for some g ◦f.
To conclude the proof, we note that, by the universality theorem of MLPs, cos itself can be approached
by a MLP with any non-polynomial non-linearity ρ, so that Fρ is dense in Fcos.
Lemma 2. A piecewise continuous function p on [−1, 1] is symmetric iﬀ
R
t2k+1p(t)dt = 0 for all k.
Proof. Recall that the Legendre polynomials Lk of degree k are such that: a) they form an orthogonal
basis of piecewise continuous functions on [−1, 1] for L2, b) respect parity Lk(−t) = (−1)kLk(t), c)
involves only monomials of the same parity tk−2p, p = 0, . . . , ⌊k
2⌋.
By considering the decomposition p = P
k(
R
Lkp)Lk, it is immediate that p is symmetric iﬀ
R
L2k+1p = 0
for all k, which is the same as
R
t2k+1p(t)dt = 0 for all k.
Lemma 3 (Chaining). Let (X, mX ) be a compact metric space with diameter DX and covering numbers
N(X, mX , ε) ∝ε−dX , and Y a measurable space. Consider a bivariate measurable function U : X ×Y →R
that is uniformly CU-bounded, and LU-Lipschitz in the ﬁrst variable. Let y1, . . . , yn be drawn i.i.d from a
distribution P on Y. Then, with probability at least 1 −ρ,

1
n
X
i
η(·, yi) −
Z
η(·, y)dP(y)

∞
≲LU
√dX + (LUDX + CU)
p
log(1/ρ)
√n
.
Proof. For any x ∈X, deﬁne
Yx = 1
n
X
i
U(x, xi) −
Z
U(x, y)dP(y).
25

Since |Yx| ⩽2CU, for any ﬁxed x0 ∈X, by Hoeﬀding’s inequality we have: with probability at least 1 −ρ,
|Yx0| ≲CU
r
log(1/ρ)
n
.
Now we have 
1
n
X
i
U(·, xi) −
Z
U(·, x)dP(x)

∞
= sup
x∈X
|Yx| ⩽
sup
x,x′∈X
|Yx −Yx′| + |Yx0| .
The second term is bounded by the inequality above. For the ﬁrst term, we are going to use Dudley’s
inequality “tail bound” version [35, Thm 8.1.6]. We ﬁrst check the sub-gaussian increments of the process
Yx. The sub-gaussian norm ∥·∥ψ2 is deﬁned in [35, Def. 2.5.6]. For any x, x′ ∈X, we have
∥Yx −Yx′∥ψ2 ≲

1
n
X
i
U(x, yi) −U(x′, yi)

ψ2
≲1
n
 n
X
i=1
∥U(x, yi) −U(x′, yi)∥2
ψ2
! 1
2
≲1
n

n ∥U(x, ·) −U(x′, ·)∥2
∞
 1
2
⩽LU
√nmX (x, x′)
where we have used, from [35], Lemma 2.6.8 for the ﬁrst line, Prop. 2.6.1 for the second, Example 2.5.8
for the third, and the Lipschitz property of U for the last.
Now, we apply Dudley’s inequality [35, Thm 8.1.6] to obtain that with probability 1 −ρ,
sup
x,x′∈X
|Yx −Yx′| ≲LU
√n
Z 1
0
p
log N(X, d, ε)dε + DX
p
log(1/ρ)

≲LU
√
d + DX
p
log(1/ρ)
√n
.
Combining with the decomposition above yields the desired result.
Lemma 4 (Variant of Lemma 6 in [19]). Let (E, ∥·∥E) be a Banach space and (H, ∥·∥H) be a separable
Hilbert space. Let L, L′ be two bounded operators on E, and S : E →H be a linear operator such that
∥S∥H→E ⩽1. For 1 ⩽i ⩽d and 1 ⩽j ⩽d′, let hij = P
k βijkλk be a collection of analytic ﬁlters, with
Bk = (βijk)ji ∈Rd′×d the matrix of order-k coeﬃcients, with operator norm ∥Bk∥. Let x1, . . . , xd ∈E be
a collection of points. Then:
v
u
u
tX
j
S
X
i
hij(L)xi

2
H
⩽
 X
k
∥Bk∥
Lk
! sX
i
∥xi∥2
E
(24)
and
v
u
u
tX
j
S
X
i
(hij(L) −hij(L′))xi

2
H
⩽
X
k
∥Bk∥
v
u
u
tX
i
 k−1
X
ℓ=0
∥Lℓ∥∥(L −L′)(L′)k−1−ℓxi∥E
!2
.
(25)
Now, if ∥Lx∥E ⩽∥Sx∥H for some Hilbert space H, then
v
u
u
tX
j

X
i
hij(L)xi

2
E
⩽

B0,|·|
 +
X
k⩾1
∥Bk∥
Lk−1


sX
i
∥xi∥2
E.
(26)
26

Proof. The results (24) and (25) are directly from Lemma 6 in [19]. The result (26) is obtained by
observing that
v
u
u
tX
j

X
i
hij(L)xi

2
E
=
v
u
u
tX
j

X
ik
βijkLkxi

2
E
⩽
X
k
v
u
u
tX
j

X
i
βijkLkxi

2
E
⩽
v
u
u
tX
j

X
i
βij0xi

2
E
+
X
k⩾1
v
u
u
tX
j
S
X
i
βijkLk−1xi

2
H
.
We apply (24) on the second term and on the ﬁrst:
v
u
u
tX
j

X
i
βij0xi

2
E
⩽
v
u
u
tX
j
 X
i
|βij0| ∥xi∥E
!2
⩽
B0,|·|

sX
i
∥xi∥2
E.
Lemma 5 (Properties of c-GNNs). Apply a c-GNN to a random graph model. Denote by f (ℓ) the function
at each layer. Then we have
f (ℓ)
∗⩽∥f∥∗
ℓ−1
Y
s=0
H(s)
∗
+
ℓ−1
X
s=0
b(s)
ℓ−1
Y
p=s+1
H(p)
∗
(27)
where ∗indicates L2(P) or ∞.
Moreover, for x, x′ ∈X, we have
f (ℓ)(x) −f (ℓ)(x′)
 ⩽
 ℓ−1
Y
s=0
B(s)
0

! f (0)(x) −f (0)(x′)

+ LW dX (x, x′)
ℓ−1
X
s=0
 ℓ−1
Y
p=s+1
B(p)
0

!  f (0)
L2(P )
s
Y
p=0
H(p)
2
+
s−1
X
p=0
b(p)
s
Y
r=p+1
H(r)
2
!
.
Proof. For j ⩽dℓ, using Lemma 4, the Lipschitzness of ρ and the easy fact that ∥TW,P f∥∞⩽∥f∥L2(P ).
we write
f (ℓ)
∗⩽
v
u
u
u
t
X
j

dℓ−1
X
i=1
h(ℓ−1)
ij
(TW,P )f (ℓ−1)
i
+ b(ℓ−1)
j

2
∗
⩽
v
u
u
u
t
X
j

dℓ−1
X
i=1
h(ℓ−1)
ij
(TW,P )f (ℓ−1)
i

2
∗
+
b(ℓ−1)
⩽H(ℓ−1)
∗
f (ℓ−1)
∗+
b(ℓ−1) .
An easy recursion gives the result.
Now,
f (ℓ)(x) −f (ℓ)(x′)
 ⩽
v
u
u
u
t
X
j


dℓ−1
X
i=1
h(ℓ−1)
ij
(TW,P )f (ℓ−1)
i
(x) −h(ℓ−1)
ij
(TW,P )f (ℓ−1)
i
(x′)


2
=
X
k
B(ℓ−1)
k
h
T k
W,P f (ℓ−1)
i
(x) −T k
W,P f (ℓ−1)
i
(x′)
idℓ−1
i=1

27

and since |TW,P f(x) −TW,P f(x′)| ⩽LW dX (x, x′) ∥f∥L2(P ) and ∥TW,P ∥L2(P ) ⩽1 by Schwartz inequality,
f (ℓ)(x) −f (ℓ)(x′)
 ⩽
B(ℓ−1)
0

f (ℓ−1)(x) −f (ℓ−1)(x′)
 + H(ℓ−1)
2
f (ℓ−1)
L2(P ) LW dX (x, x′).
Again we obtain the result by recursion.
Lemma 6. (Properties of c-SGNNs) Denote by f (ℓ) the bivariate functions propagated in the inner part
of a c-SGNN. We have
f (ℓ)
∞⩽Cη
ℓ−1
Y
s=0
H(s)
∞+
ℓ−1
X
s=0
b(s)
ℓ−1
Y
p=s+1
H(p)
∞.
Moreover,
f (ℓ)(·, x) −f (ℓ)(·, x′)

∞⩽LηdX (x, x′)
ℓ−1
Y
s=0
H(s)
∞.
Proof. The ﬁrst inequality is proved exactly as Lemma 5, noting that ∥TW,P ∥∞even for bivariate functions
and ∥η∥∞⩽Cη.
Then we have
f (ℓ)(·, x) −f (ℓ)(·, x′)

∞⩽
v
u
u
u
t
X
j

dℓ−1
X
i=1
h(ℓ−1)
ij
(TW,P )
h
f (ℓ−1)
i
(·, x) −f (ℓ−1)
i
(·, x′)
i

2
∞
⩽H(ℓ−1)
∞
f (ℓ−1)(·, x) −f (ℓ−1)(·, x′)

∞.
Theorem 5 ([22]). Let A be a n × n symmetric matrix with independent Bernoulli entries aij ∼αnpij.
Assume that αn ≳log n
n . Then, for all ν > 0, there is a constant Cν such that, for all n, with probability
at least 1 −n−ν:
1
n

A
αn
−P
 ⩽
Cν
√αnn.
(28)
E
Details of numerical experiments
The code is available at https://github.com/nkeriven/random-graph-gnn.
Figure 1.
In this ﬁgure, we consider a 2-communities SBM with incoherent P, invertible W, but constant
degree function. We use dense random edges with αn = 1. We train a permutation-equivariant GNN
and a two-hop ﬁltering SGNN on 5 random graphs with n = 80 nodes, output dimension dout = K, with
cross-entropy loss and the Adam optimizer. The displayed graph signal corresponds to the ﬁrst dimension
of the log-softmax of the ouput. The test graph has n = 300 nodes. The graph ﬁlters have order 1, such
that we actually manipulate the message-passing version of GNNs. The GNN has M = 5 hidden layers
with internal dimension dℓ= 250 (except d0 = 1 and dout = 2) and is trained for 2000 epochs. Each of
the GNNs constituting the SGNN has M = 2 hidden layers with dimension dℓ= 50 and is trained for
1000 epochs.
Figure 2.
We compare one-hop and two-hop input ﬁltering for a simple permutation-invariant SGNN,
between the deterministic edges case and the random edges case. We know that the deterministic edges
case converges to the c-SGNN in all settings, and we test if the random edges case converge to the
deterministic one. We average over 50 random graphs with Gaussian kernel and a range of n’s with
αn ∼n−1/3, such that Prop. 1 applies in the two-hop case. The dominating term in the theoretical rate is
O (1/(αn
√n)) from Prop. 1.
28

Figure 3.
Here we consider X = [−1, 1] with Gaussian kernel, and either a symmetric P (uniform) or a
non-symmetric but centered P (here a well-adjusted aﬃne by part function). We use random edges with
αn ∼n−1/3. We train a SGNN with two-hop input ﬁltering to approximate either a symmetric function
x →cos(5x) or a non-symmetric one x →sin(5x) with a simple square loss. We use 5 training graphs of
size n = 150 and display a test graph with size n = 400.
29

