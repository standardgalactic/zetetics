

Coherent Stress Testing 


Coherent Stress Testing 
A Bayesian Approach to the Analysis of Financial Stress 
Riccardo Rebonato 
A John Wiley and Sons, Ltd., Publication 

This edition ﬁrst published 2010 
© 2010 Riccardo Rebonato 
Registered ofﬁce 
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom 
For details of our global editorial ofﬁces, for customer services and for information about how to apply for 
permission to reuse the copyright material in this book please see our website at www.wiley.com. 
The right of the author to be identiﬁed as the author of this work has been asserted in accordance with the 
Copyright, Designs and Patents Act 1988. 
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, 
in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted 
by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher. 
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not 
be available in electronic books. 
Designations used by companies to distinguish their products are often claimed as trademarks. All brand names 
and product names used in this book are trade names, service marks, trademarks or registered trademarks of 
their respective owners. The publisher is not associated with any product or vendor mentioned in this book. 
This publication is designed to provide accurate and authoritative information in regard to the subject matter 
covered. It is sold on the understanding that the publisher is not engaged in rendering professional services. If 
professional advice or other expert assistance is required, the services of a competent professional should be 
sought. 
Library of Congress Cataloging-in-Publication Data 
Rebonato, Riccardo. 
Coherent stress testing : a Bayesian approach / Riccardo Rebonato. 
p. cm. 
ISBN 978-0-470-66601-2 
1. 
Risk management. 2. 
Probabilities. 3. 
Bayesian statistical decision theory. 
I. Title. 
HD61.R42 2010 
658.15′ 501519542 – dc22 
2010005778 
A catalogue record for this book is available from the British Library. 
ISBN 978-0-470-66601-2 
Typeset in 10/12 Times-Roman by Laserwords Private Limited, Chennai, India 
Printed in Great Britain by CPI Antony Rowe, Chippenham, Wiltshire 

To my parents 
To my wife 
To my son 


Contents 
Acknowledgements 
xi 
1 Introduction 
1 
1.1 
Why We Need Stress Testing 
1 
1.2 
Plan of the Book 
5 
1.3 
Suggestions for Further Reading 
6 
I Data, Models and Reality 
7 
2 Risk and Uncertainty – or, Why Stress Testing is Not Enough 
9 
2.1 
The Limits of Quantitative Risk Analysis 
9 
2.2 
Risk or Uncertainty? 
10 
2.3 
Suggested Reading 
13 
3 The Role of Models in Risk Management and Stress Testing 
15 
3.1 
How Did We Get Here? 
16 
3.2 
Statement of the Two Theses of this Chapter 
17 
3.3 
Defence of the First Thesis (Centrality of Models) 
18 
3.3.1 Models as Indispensable Interpretative Tools 
18 
3.3.2 The Plurality-of-Models View 
21 
3.4 
Defence of the Second Thesis (Coordination) 
23 
3.4.1 Traders as Agents 
23 
3.4.2 Agency Brings About Coordination 
25 
3.4.3 From Coordination to Positive Feedback 
26 
3.5 
The Role of Stress and Scenario Analysis 
27 
3.6 
Suggestions for Further Reading 
29 
4 What Kind of Probability Do We Need in Risk Management? 
31 
4.1 
Frequentist versus Subjective Probability 
31 
4.2 
Tail Co-dependence 
36 
4.3 
From Structural Models to Co-dependence 
38 
4.4 
Association or Causation? 
39 
4.5 
Suggestions for Further Reading 
42 
vii 

viii 
CONTENTS 
II The Probabilistic Tools and Concepts 
45 
5 Probability with Boolean Variables I: Marginal and Conditional 
Probabilities 
47 
5.1 
The Set-up and What We are Trying to Achieve 
47 
5.2 
(Marginal) Probabilities 
50 
5.3 
Deterministic Causal Relationship 
53 
5.4 
Conditional Probabilities 
55 
5.5 
Time Ordering and Causation 
56 
5.6 
An Important Consequence: Bayes’ Theorem 
57 
5.7 
Independence 
58 
5.8 
Two Worked-Out Examples 
59 
5.8.1 Dangerous Running 
59 
5.8.2 Rare and Even More Dangerous Diseases 
61 
5.9 
Marginal and Conditional Probabilities: A Very Important Link 
62 
5.10 
k
Interpreting and Generalizing the Factors xi 
65 
5.11 Conditional Probability Maps 
67 
6 Probability with Boolean Variables II: Joint Probabilities 
71 
6.1 
Conditioning on More Than One Event 
71 
6.2 
Joint Probabilities 
73 
6.3 
A Remark on Notation 
75 
6.4 
From the Joint to the Marginal and the Conditional Probabilities 
76 
6.5 
From the Joint Distribution to Event Correlation 
77 
6.6 
From the Conditional and Marginal to the Joint Probabilities? 
83 
6.7 
Putting Independence to Work 
84 
6.8 
Conditional Independence 
86 
6.9 
Obtaining Joint Probabilities with Conditional Independence 
88 
6.10 At a Glance 
89 
6.11 Summary 
90 
6.12 Suggestions for Further Reading 
90 
7 Creating Probability Bounds 
93 
7.1 
The Lay of the Land 
93 
7.2 
Bounds on Joint Probabilities 
93 
7.3 
How Tight are these Bounds in Practice? 
96 
8 Bayesian Nets I: An Introduction 
99 
8.1 
Bayesian Nets: An Informal Deﬁnition 
99 
8.2 
Deﬁning the Structure of Bayesian Nets 
101 
8.3 
More About Conditional Independence 
104 
8.4 
What Goes in the Conditional Probability Tables? 
106 
8.5 
Useful Relationships 
107 
8.6 
A Worked-Out Example 
109 

CONTENTS 
ix 
8.7 
A Systematic Approach 
111 
8.8 
What Can We Do with Bayesian Nets? 
113 
8.8.1 Unravelling the Causal Structure 
113 
8.8.2 Estimating the Joint Probabilities 
114 
8.9 
Suggestions for Further Reading 
115 
9 Bayesian Nets II: Constructing Probability Tables 
117 
9.1 
Statement of the Problem 
117 
9.2 
Marginal Probabilities – First Approach 
118 
9.2.1 Starting from a Fixed Probability 
119 
9.2.2 Starting from a Fixed Magnitude of the Move 
120 
9.3 
Marginal Probabilities – Second Approach 
120 
9.4 
Handling Events of Different Probability 
122 
9.5 
Conditional Probabilities: A Reasonable Starting Point 
123 
9.6 
Conditional Probabilities: Checks and Constraints 
125 
9.6.1 Necessary Conditions 
125 
9.6.2 Triplet Conditions 
126 
9.6.3 Independence 
127 
9.6.4 Deterministic Causation 
127 
9.6.5 Incompatibility of Events 
128 
9.7 
Internal Compatibility of Conditional Probabilities: The Need for 
a Systematic Approach 
129 
III Applications 
131 
10 Obtaining a Coherent Solution I: Linear Programming 
133 
10.1 Plan of the Work Ahead 
133 
10.2 Coherent Solution with Conditional Probabilities Only 
135 
10.3 The Methodology in Practice: First Pass 
141 
10.4 The CPU Cost of the Approach 
144 
10.5 Illustration of the Linear Programming Technique 
144 
10.6 What Can We Do with this Information? 
149 
10.6.1 Extracting Information with Conditional Probabilities Only 
149 
10.6.2 Extracting Information with Conditional 
and Marginal Probabilities 
151 
11 Obtaining a Coherent Solution II: Bayesian Nets 
155 
11.1 Solution with Marginal and n-conditioned Probabilities 
156 
11.1.1 Generalizing the Results 
164 
11.2 An ‘Automatic’ Prescription to Build Joint Probabilities 
165 
11.3 What Can We Do with this Information? 
167 
11.3.1 Risk-Adjusting Returns 
168 

x 
CONTENTS 
IV Making It Work In Practice 
171 
12 Overcoming Our Cognitive Biases 
173 
12.1 Cognitive Shortcomings and Bounded Rationality 
174 
12.1.1 How Pervasive are Cognitive Shortcomings? 
175 
12.1.2 The Social Context 
175 
12.1.3 Adaptiveness 
176 
12.2 Representativeness 
178 
12.3 Quantiﬁcation of the Representativeness Bias 
181 
12.4 Causal/Diagnostic and Positive/Negative Biases 
182 
12.5 Conclusions 
184 
12.6 Suggestions for Further Reading 
185 
13 Selecting and Combining Stress Scenarios 
187 
13.1 Bottom Up or Top Down? 
187 
13.2 Relative Strengths and Weaknesses of the Two Approaches 
187 
13.3 Possible Approaches to a Top-Down Analysis 
190 
13.4 Sanity Checks 
191 
13.5 How to Combine Stresses – Handling the Dimensionality Curse 
192 
13.6 Combining the Macro and Bottom-Up Approaches 
194 
14 Governance 
197 
14.1 The Institutional Aspects of Stress Testing 
197 
14.1.1 Transparency and Ease of Use 
197 
14.1.2 Challenge by Non-specialists 
198 
14.1.3 Checks for Completeness 
198 
14.1.4 Interactions among Different Specialists 
199 
14.1.5 Auditability of the Process and of the Results 
201 
14.2 Lines of Criticism 
201 
14.2.1 The Role of Subjective Inputs 
202 
14.2.2 The Complexity of the Stress-testing Process 
203 
Appendix A Simple Introduction to Linear Programming 
205 
A.1 
Plan of the Appendix 
205 
A.2 
Linear Programming – A Refresher 
205 
A.3 
The Simplex Method 
208 
References 
213 
Index 
217 

Acknowledgements 
It gives me great pleasure to acknowledge the support I received while writing this book. 
Dr Keating provided very insightful comments and pointed out some ‘loose thinking’. I 
am very grateful for that. 
Many of my colleagues have helped me a lot, by challenging my thoughts, pointing 
out what did and did not work, and suggesting how I could improve the approach. I am 
sure that I will forget many, but I certainly extend my thanks to Dr Gary Dunn, Dr Ron 
Keating, Dr Ronnie Barnes, Mr Daniel Burns, Dr Michael Smith, Mr Paul Fairhurst, Dr 
Jeremy Broughton, Dr Ed Hayes, Mr Craig Schor, Dr Tom Connor and, for his unﬂinching 
criticism, Dr Stephen Laughton. Above all, however, I thank Dr Jan Kwiatkowski, who 
proposed an earlier version of the Linear Programming approach that I describe in Chapter 
10. His contribution has extended well beyond this technical suggestion, as over the years 
he has become my Bayesian mentor. 
I have greatly beneﬁted from discussions that I have had with regulators, at the Boston 
Fed, at the FSA and at the MSA. I am grateful for the time they spent discussing my ideas 
on stress testing. 
I have presented parts of the material in this book at several conferences in the US and 
in Europe, and therefore I have received extremely useful and insightful comments from 
many delegates. The book would be much the poorer without their suggestions: thank you. 
At John Wiley & Sons, Ltd, Caitlin Cornish ﬁrst and then Pete Baker have shown from 
the start great enthusiasm for the project. I am very grateful for this. 
Last but not least, my wife and my parents have given me continuous support and 
encouragement. In a way, it is to them that I owe my greatest debt of gratitude. 
Despite all this help, I am sure that there are still many mistakes and imperfections in 
the book. I am fully and solely responsible for these. 
xi 


Chapter 1 
Introduction 
[Under uncertainty] there is no scientiﬁc basis on which to form any calculable probability 
whatever. We simply don’t know. Nevertheless, the necessity for action and for decision 
compels us as practical men to do our best to overlook this awkward fact and to behave 
exactly as we should if we had behind us [. . .] a series of prospective advantages and 
disadvantages, each multiplied by its appropriate probability waiting to be summed . . . . 
Robertson (1936) 
1.1 Why We Need Stress Testing 
Why a book about stress testing? And why a book about stress testing now ? Stress testing 
has been part of the risk manager’s toolkit for decades.1 What justiﬁes the renewed interest 
from practitioners and regulators2 for a risk management tool that, truth be told, has always 
been the poor relation in the family of analytical techniques to control risk?3 And why has 
stress testing so far been regarded as a second-class citizen? 
Understanding the reason for the renewed interest is simple: the ﬁnancial crisis of 
2007–2008–2009 has shown with painful clarity the limitations of the purely statistical 
techniques (such as Value at Risk (VaR) or Economic Capital) that were supposed to pro-
vide the cornerstones of the ﬁnancial ediﬁce. In the year and a half starting with July 2007, 
events of once-in-many-thousand-years rarity kept on occurring with disconcerting regular-
ity. Only a risk manager of Stalinist dogmatism could have lived through these events and 
‘kept the faith’. Clearly, something more – or, rather, something different – had to be done. 
But what? And what analytical tools should we employ to ﬁx the problem? 
1See, e.g., Sorge (2004) for a review of stress testing methodologies put together before the sub-prime crisis, 
or Alexander and Sheedy (2008) for a recent approach ﬁrmly rooted in the statistical analysis of past data. 
Stress testing is dealt with, often in a rather cursory manner, in most standard texts on risk management. I 
assume in this book that the reader is broadly familiar with current stress-testing practice. By quoting from the 
work of Aragones, Blanco and Dowd (2001), I explain in this chapter a common shortcoming of these approaches. 
Indeed, this shortcoming is one of the motivations of the approach I propose. 
2See, e.g., BIS (2009). 
3“Those who can, do, and those who can’t, do stress testing”, a proud risk quant told me less than 
two years ago. 
1 

2 
CHAPTER 1 INTRODUCTION 
‘Stress testing’ has become the stock answer to these questions. But the unease and 
suspicion with which this technique has been regarded has not melted away. The frog has 
not been kissed (yet) into a handsome prince. The current attitude seems one of resigned 
acceptance of a faute-de-mieux measure of risk: a far cry from an enthusiastic embrace of 
a new and powerful analytical tool. Two cheers, the mood seems to be, for stress testing. 
Can we do better? And why has stress testing been regarded as such an ungainly frog in 
the ﬁrst place? 
If by stress testing we mean the assessment of very severe ﬁnancial losses arrived at 
without heavy reliance on statistical techniques, but by deploying instead a large dose of 
subjective judgement, some answers to the latter question are not difﬁcult to see. Rather 
than paraphrasing, I would like to quote extensively from an article by Aragones, Blanco 
and Dowd (2001), who put their ﬁngers exactly on the problem: 
. . . traditional stress testing is done on a stand-alone basis, and the results of stress tests 
are evaluated side-by-side with the results of traditional market risk (or VaR) models. 
This creates problems for risk managers, who then have to choose which set of risk 
exposures to ‘believe’. [R]isk managers often don’t know whether to believe their stress 
test results, because the stress tests exercises give them no idea of how likely or unlikely 
stress-test  scenarios  might  be . . . .  
And again: 
A related problem is that the results of stress tests are difﬁcult to interpret because they 
give us no idea of the probabilities of the events concerned, and in the absence of such 
information we often don’t know what to do with them. Suppose for instance that stress 
testing reveals that our ﬁrm will go bust under a particular scenario. Should we act on 
this information? The only answer is that we can’t say. If the scenario is very likely, 
we would be very unwise not to act on it. But if the scenario was extremely unlikely, 
then it becomes almost irrelevant, because we would not usually expect management to 
take expensive precautions against events that may be too improbable to worry about. 
So the extent to which our results matter or not depends on unknown probabilities. As 
Berkowitz [1999] nicely puts it, this absence of probabilities puts ‘stress testing in a 
statistical purgatory. We have some loss numbers, but who is to say whether we should 
be concerned about them?’ 
The result of this state of affairs is not pretty: we are left with 
. . . two sets of separate risk estimates – probabilistic estimates (e.g., such as VaR), and 
the loss estimates produced by stress tests – and no way of combining them. How can 
we combine a probabilistic risk estimate with an estimate that such-and-such a loss will 
occur if such-and-such happens? The answer, of course, is that we can’t. We therefore 
have to work with these estimates more or less independently of each other, and the best 
we can do is use one set of estimates to check for prospective losses that the other might 
have underrated or missed . . . 
In modern ﬁnance, risk and reward are supposed to be two sides of the same coin. Risk 
is ‘priced’ in terms of expected return by assigning probabilities4 to outcomes. But when 
4Note that I did not say ‘assigning objective probabilities known and agreed upon by all the market participants’. 
See Shefrin (2008) for a discussion of asset pricing under heterogeneous beliefs about probabilities. See also the 
discussion in Chapter 3. 

1.1 WHY WE NEED STRESS TESTING 
3 
it comes to extreme events, absent of any probabilistic assessment, we don’t know how to 
‘price’ the outcomes of stress testing. And if our conﬁdence in assigning a probability to 
extremely rare events has been terminally shaken by the recent market events,5 the state of 
impasse seems inescapable. 
Perhaps there is hope – and it is exactly this ray of hope that this book pursues. First of 
all, ‘probabilistic statement’ need not be equated with ‘frequentist (i.e., purely data-driven) 
probabilistic statement’. As I discuss in Chapter 4, there is a different way of looking at 
probability that takes into account, but is not limited to, the pure analysis of data. I maintain 
(and I have argued at length elsewhere6) that the subjective view of probability is every 
bit as ‘respectable’ as the purely-data-driven (frequentist) one. I also believe that it is much 
better suited to the needs of risk management. 
This view, while not mainstream, is not particularly new, especially in the context of 
ﬁnancial risk management – see, e.g., Berkowitz (1999). However, the subjective approach 
brings about an insidious problem. It is all well and good to assign subjective probabilities 
to stand-alone events. But, if we want to escape from Berkowitz’s purgatory, we will have 
to do more. We will have to combine different stress scenarios, with different subjective 
probabilities, into an overall coherent, albeit approximate, stress loss number at a given 
conﬁdence level (or, perhaps, into a whole stress loss distribution). How is one to do that? 
How is one to provide subjectively these co-dependences – and tail co-dependences to boot? 
How is one to ensure that the subjectively-assigned probabilities are reasonable, let alone 
feasible (i.e., mathematically possible and self-consistent)? 
This book offers two routes to escape this purgatorial dilemma. The ﬁrst is the acknowl-
edgement that the risk manager can only make sense of data on the basis of a model (or 
of competing models) of reality. A risk manager, for instance, should have a conception 
of the direction of causation between different events: does a dramatic fall in equity prices 
‘cause’ an increase in equity implied volatilities? Or is it an increase in implied volatil-
ity that ‘causes’ a dramatic fall in equity prices? The answer, at least in this case, may 
seem obvious. Unfortunately, correlations, and even conditional probabilities, contain no 
information about the direction of causation. Yet, this information about causation, even if 
imperfect, is powerful. It is ignored in the frequentist approach at a great loss for the risk 
manager. Speaking about the sciences in general, Pearl (2009) points out that there is ‘no 
greater impediment to scientiﬁc progress than the prevailing practice of focusing all of our 
mathematical resources on statistical and probabilistic inferences’. I believe that exactly the 
same applies in the area of quantitative risk management. 
If one is prepared to ‘stick one’s neck out’ and make some reasonable assumptions about 
the direction of the arrow of causation, just like Dante one can begin to glimpse some light 
ﬁltering through the thick trees of the selva oscura. In the case of stress testing, the route 
to salvation is via the provision of information that is not ‘contained in the data’. 
Sure enough, even if one can provide this extra information not all is plain sailing. Organ-
izing one’s understanding about how the world might work into a coherent and tractable 
analytical probabilistic framework is not an easy task. Fortunately, if one is prepared to 
make some reasonable approximations, there are powerful and intuitive techniques that can 
offer great help in building plausible and mathematically self-consistent joint distributions 
5Of course, there are techniques such as Extreme Value Theory (EVT) that promise to give us a quantitative 
probabilistic glimpse of extremely rare events. I discuss in Chapter 4 my reservations about this and related 
techniques. 
6See Rebonato (2007). 

4 
CHAPTER 1 INTRODUCTION 
of the stress losses that have been identiﬁed. These technical tools (Bayesian networks and 
Linear Programming) have been well known for a long time, but their application to risk 
management problems, and to stress testing in particular, has been hesitant at best. This is 
a pity, because I believe that they are not only powerful and particularly well suited to the 
problem at hand, but also extremely intuitively appealing. And in Section 2.2 of the next 
chapter I will highlight how important appeal to intuition can be if the recommendations 
of the risk managers are to be acted upon (as opposed to ‘conﬁned to a stress report’). 
Once we accept that we can, approximately but meaningfully, associate stress events with 
a probabilistic assessment of their likelihood, the questions that opened this chapter begin to 
ﬁnd a compelling answer. We need stress testing, and we need stress testing now , because 
the purely-data-based statistical techniques we have been using have proven unequal to the 
task when it really mattered. Perhaps the real question should have been instead: ‘How can 
we do without stress testing?’ 
Of course, there is a lot more to risk management than predicting the probability of 
losses large and small. But, even if we look at the management of ﬁnancial risk through 
the highly reductive prism of analysing the likelihood of losses, there still is no one single 
goal for the risk manager. For instance, estimating the kind of proﬁt-and-loss variability 
that can be expected on a weekly or monthly basis has value and importance. Ensuring 
that a business line or trading desk effectively ‘diversiﬁes’ the revenue stream from other 
existing lines of activity under normal market conditions is also obviously important. So 
is estimating the income variability or the degree of diversiﬁcation that can be expected 
from a portfolio of businesses over a business cycle. And recent events have shown the 
importance of ensuring that a set of business activities do not endanger the survival of a 
ﬁnancial institution even under exceptional market conditions. These are all important goals 
for a risk manager. But it would be extraordinary if the same analytical tools could allow 
the risk manager to handle all these problems – problems, that is, whose solution hinges on 
the estimation of probabilities of events that should occur, on average, from once every few 
weeks to once in several decades. This is where stress testing comes in. Stress testing picks 
up the baton from VaR and other data-driven statistical techniques as the time horizons 
become longer and longer and the risk manager wants to explore the impact of events that 
are not present in her dataset – or, perhaps, that have never occurred before.7 
As I explain in Chapter 4, stress testing, by its very nature, can rely much less on a 
frequentist concept of probability, and almost has to interpret probability in a subjective 
sense. In Bayesian terms, as the time horizon lengthens and the severity of the events 
increases, the ‘prior’ acquires a greater and greater weight, and the likelihood function a 
smaller and smaller one.8 In my opinion, this is a strength, not a weakness, of stress testing. 
It is also, however, the aspect of the project I propose that requires most careful handling. 
Frequentist probability may make little sense when it comes to stress testing, but this does 
not mean that probability tout court has no place in stress testing. If anything, it is stress 
testing without any notion of probability that, as Aragones, Blanco and Dowd remind us, is 
of limited use. The challenge taken up in this book is to provide the missing link between 
7Is Extreme Value Theory not supposed to provide information about the far tails of a distribution? It may, 
but only if the underlying phenomenon is time-stationary over sufﬁciently long time periods (decades if we try to 
estimate once-in-a-decade events). Also, EVT is silent about the causal mechanism that produces the tail events. 
There is no way of telling whether, conditional on today’s state of the world, the probability of the tail event 
predicted by the EVT applies. See Chapter 4 and Sections 4.2 to 4.4 in particular. 
8For readers not familiar with Bayesian analysis, I shall explain what I mean by this in Chapter 4. 

1.2 PLAN OF THE BOOK 
5 
stress events and their approximate likelihood – as explained, an essential prerequisite for 
action9 – without inappropriately resorting to purely frequentist methods. 
The enterprise I have brieﬂy sketched therefore gives us some hope of bringing stress 
losses within the same conceptual framework as the more mundane losses analysed by 
VaR-like techniques. The approach I suggest in this book bridges the gap between the 
probabilities that a risk manager can, with some effort, provide (marginal, and simple 
conditioned probabilities) and the probabilities that she requires (the joint probabilities). It 
does so by exploiting to the fullest the risk manager’s understanding of the causal links 
between the individual stress events. By employing the causal, rather than associative, 
language, it resonates with our intuition and works with, not against, our cognitive grain. 
The approach I suggest is therefore intended to give us guidance as to whether and when 
we should really worry, and to suggest how to act accordingly. It gives, in short, tools to 
ensure that the stress losses are approximately but consistently ‘priced’. Hopefully, all of 
this might give us a tool for managing ﬁnancial risk more effectively than we have been 
able to do so far. 
This is what this book is about, and this is why, I think, it is important. 
1.2 Plan of the Book 
This book is structured in four parts. The ﬁrst, which contains virtually no equations, puts 
stress testing and probabilistic assessments of rare ﬁnancial events in their context. The 
second part presents the quantitative ideas and techniques required for the task. Here lots 
of formulae will be found. The third part deals with the quantitative applications of the 
concepts introduced in Part II. The fourth and last part deals with practical implementation 
issues, and equations therefore disappear from sight again. 
Let me explain in some detail what is covered in these four parts. 
After the optimistic note with which I closed the previous section, in Chapter 2 I move 
swiftly to dampen the reader’s enthusiasm, by arguing that stress testing is not the solution 
to all our risk management problems. In particular, I make the important distinction, too 
often forgotten, between risk and uncertainty and explain what this entails for stress analysis. 
With these caveats out of the way, I argue that the expert knowledge of the risk manager 
is essential in constructing, using and associating probabilities to stress events. This expert 
knowledge (and the ‘models of reality’ that underpin it) constitutes the link between the past 
data and the possible future outcomes. In Chapter 3 I therefore try to explain the role played 
by competing interpretative models of reality in helping the risk manager to ‘conceive of the 
unconceivable’. Chapter 3 is therefore intended to put into context the speciﬁc suggestions 
about stress testing that I provide in the rest of the book. 
In Chapter 4 I describe the different types of probability (frequentist and subjective) 
that can be used for risk management, and discuss which ‘type of probability’ is better 
suited to different analytical tasks. The chapter closes with an important distinction between 
associative and causal descriptions. This distinction is at the basis of the efﬁcient elicitation 
of conditional probabilities, and of the Bayesian-net approach described later in the book. 
In Part II I lay the quantitative foundations required for the applications presented in 
the rest of the book. Some of the concepts are elementary, others are less well-known. In 
9‘Some of the things I vaguely apprehend are, like the end of the world, uninsurable risks, and it’s useless to 
worry about them’ – Keynes, quoted in Skidelsky (2009), page 71. 

6 
CHAPTER 1 INTRODUCTION 
order to give a uniﬁed treatment I deal with both the elementary concepts (Chapter 5) and 
the somewhat-more-advanced ones (Chapter 6) using the same conceptual framework and 
formalism. Venn diagrams will play a major role throughout the book. Chapter 7 shows 
how very useful bounds on joint probabilities can be obtained by specifying marginals and 
(some) singly-conditioned probabilities. Chapter 8 introduces Bayesian nets, and Chapter 9 
explains how to build the conditional probability tables required to use them. This concludes 
the tool-gathering part of the book. (A simple introduction to Linear Programming can be 
found in Appendix Chapter 15.) 
Part III is then devoted to the application of the conceptual tools and techniques presented 
in Part II. This is achieved by introducing two different possible systematic approaches to 
stress testing, of different ambition and scope, which are described in Chapters 10 and 11. 
Finally, in Part IV I address more practical questions: how we can try to overcome the 
difﬁculties and the cognitive biases that stand in the way of providing reasonable conditional 
probabilities (Chapter 12); how we can structure our chain of stress events (Chapter 13); 
and how we can embed the suggestions of the book into a viable approach in a real ﬁnancial 
institution (as opposed to a classroom exercise). Doing so requires taking into account the 
reality of its governance structure, its reporting lines and the need for independence of a 
well-functioning risk-management function (Chapter 14). 
I have prepared Parts II and III with exercises. I have done so not because I see this 
book necessarily as a text for a formal course, but because I ﬁrmly believe that, in order to 
really understand new quantitative techniques, there is no substitute for getting one’s hands 
dirty and actually working out some problems in full. 
1.3 Suggestions for Further Reading 
Stress testing is the subject of a seemingly endless number of white, grey and variously-
coloured consultation papers by the BIS and other international bodies. At the time of 
writing, the latest such paper I am aware of is BIS (2009), but, no doubt, by the time this 
book reaches the shelves many new versions will have appeared. Good sources of up-to-date 
references are the publication sites of the BIS, the IIF and the IMF. 

Part I 
Data, Models and Reality 


Chapter 2 
Risk and Uncertainty – or, Why 
Stress Testing is Not Enough 
In the introductory chapter I made my pitch as to why stress testing is important and why 
I believe that the approach I propose can show us the way out of Berkowitz’s (1999) 
purgatory. I don’t want to convey the impression, however, that stress testing can be the 
answer to all our risk management questions. The problem, I think, does not lie with the 
speciﬁc approach I suggest in this book – ﬂawed as this may be – but is of a fundamental 
nature. To present a balanced picture, I must therefore share two important reservations. 
2.1 The Limits of Quantitative Risk Analysis 
The ﬁrst reservation is that the quantitative assessment of risk (and I include stress testing in 
this category) is an important part of risk management, but it is far from being its beginning 
and end. Many commentators and risk ‘gurus’ have stressed the inadequacies of the current 
quantitative techniques. The point is taken. But even if the best quantitative assessment of 
risk were available, a lot more would be required to translate this insight into effective risk 
management. The purpose of analysis is to inform action. Within a complex organization, 
effective action can only take place in what I call a favourable institutional environment.1 So, 
in a favourable institutional environment the output of the quantitative analysis is ﬁrst esca-
lated, and then understood and challenged by senior management. This is now well accepted. 
But there is a lot more, and this ‘a lot more’ has very little to do with quantitative risk anal-
ysis. The organizational set-up, for instance, must be such that conﬂicts of interest are mini-
mized (in the real world they can never be totally eliminated). Or, the agency problems that 
bedevil any large organization, and ﬁnancial institutions in primis, must be understood and 
addressed in a satisfactory manner. And again: an effective way must be found to align the 
interests of the private decision makers of a systemically-relevant institution such as a large 
1A strong governance structure is a common expression with a similar meaning. I prefer to speak of institutional 
environment, which encompasses such intangibles as a ‘robust risk management culture’, because even the strongest 
governance structure can fail if it does not dovetail constructively with the underlying culture of the organization. 
9 

10 
CHAPTER 2 RISK AND UNCERTAINTY 
bank with those of the regulators – and, more to the point, of society at large. And the list 
can go on. 
VaR & Co have received so much criticism that it sometimes seems that if we had the 
right analytical tools, all our risk management problems would be solved. If only that were 
true! The institutional environment in which the risk management decisions are made is 
where the heart of risk management lies. Yes, the quantitative analysis of risk is part of this 
‘institutional environment’ – and perhaps an important one – but it remains, at best, a start. 
2.2 Risk or Uncertainty? 
My second reservation is about our ability to specify probabilities (frequentist, subjective or 
otherwise) for extremely rare events when the underlying phenomenon is the behaviour of 
markets and of the economy. As the reader will appreciate, I make in this book ‘minimal’ 
probabilistic requirements, often asking the risk manager to estimate no more than the order 
of magnitude of the likelihood of an event. Nowhere in my book will the reader ﬁnd the 
demand to estimate the 99.975th percentile of the loss distribution at a one-year horizon.2 
But even my more limited and modest task may be asking too much. Let me explain why 
I think this may be the case. 
One of the applications of stress testing that has been recently put forth is for regulatory 
capital. Regulatory capital has to do with the viability of a bank as a going concern – the 
time horizon is, effectively, ‘forever’. I do not know what ‘forever’ means in ﬁnance, but 
certainly it must mean more than two, four or even ten years. When the horizon of required 
survivability becomes so long, I am not sure that, for matters ﬁnancial, we truly have the 
ability to associate probabilities, however approximate, to future events. Perhaps Keynesian 
(or Knightian) uncertainty provides a better conceptual framework. 
What is the difference? ‘Risk’ and ‘uncertainty’ are today used interchangeably in the 
risk-management literature, but a careful distinction used to be drawn between the two 
concepts: the word ‘risk’ should be used in those situations where we know for sure the 
probabilities attaching to future events (and, needless to say, we know exactly what the 
possible future events may be). We should instead speak of uncertainty when we have no 
such probabilistic knowledge (but we still know what may hit us tomorrow). Indeed, as far 
back as in the early 1920s Knight (1921) was writing 
. . . the practical difference between [. . .] risk and uncertainty, is that in the former the 
distribution of the outcome in a group of instances is known (either through calculation 
a priori  or from the statistics of past experience), while in the case of uncertainty, this 
is not true, the reason being in general that it is impossible to form a group of instances, 
because the situation dealt with is in a high degree unique . . . .  
This distinction was kept alive for several decades. For instance, as game theory became 
in the post-war years an increasingly important technique in the economist’s toolkit, Luce 
and Raiffa (1957) were still clearly pointing out that the two concepts yield very different 
2This is the mythical percentile that appears in the most uncompromising versions of the Economic Capital 
Project. See Rebonato (2007) for a discussion. I do not use the word ‘mythical’ lightly, because the 99.975th 
percentile of the one-year loss distribution brings into play losses of such magnitude that should, on average, have 
been incurred only once since the days of Homer. 

2.2 RISK OR UNCERTAINTY? 
11 
types of results and ‘solutions’ and devote in their textbook on game theory different chapters 
to the two categories. Yet, the boundaries between the two concepts have become increas-
ingly blurred, and the two words are now frequently used interchangeably. So much so that 
the current prevailing view in economics has become that all probabilities are known (or 
at least knowable), and that the economy therefore becomes ‘computable’, in the sense that 
there are no ‘unknown unknowns’. Current mainstream economics ﬁrmly endorses a risk-
based, not uncertainty-based, view of the world. In the neo-classical synthesis ‘uncertainty 
plays a minimal role in the decision making of economic agents, since rational utility-
maximizing individuals are [assumed] capable of virtually eliminating uncertainty with the 
historical information at hand’.3 
The consequences of this distinction are well presented by Skidelsky (2009) in his 
discussion of the Keynesian view of probability: 
Classical economists believe implicitly, and Neoclassical economists explicitly, that mar-
ket participants have complete knowledge of all probability distributions over future 
events. This is equivalent to say that they face only measurable risk . . . 
In basing the calculation of regulatory (or economic) capital on the full 
knowledge – down to the highest percentiles – of ‘all probability distributions over 
future events’, the regulators have implicitly embraced the neo-classical view: that is, that 
when it comes to matters ﬁnancial, human beings are always faced with risk, not with 
uncertainty, even when they are dealing with events of such rarity and magnitude that they 
could bring a bank to its knees. 
Why has the concept of risk prevailed over uncertainty, despite the rather extreme 
assumptions about human cognitive abilities (and the world itself!) that it implies? From 
an academic perspective, unfortunately, dealing with uncertainty brings about rather ‘unex-
citing’ analytical results, often based on minimax solutions: we disregard probability com-
pletely, and we arrange our actions so as to minimize the damage if the worst (however 
unlikely) materializes. There is no great ediﬁce of economic thought that can be built on 
such dull foundations. Succinctly put, ‘in conditions of uncertainty, economic reasoning 
would be of no value’.4 This is not a good recipe for exciting papers or for getting a 
tenure-track position at a prestigious university. 
Matters are different when it comes to risk – i.e., when we assume that we can know 
the probabilities of future events perfectly. Dealing with risk rather than uncertainty allows 
us to speak about trade-offs and non-trivial optimality,5 and opens the door to much more 
exciting analytical results, such as expected utility maximization, portfolio diversiﬁcation, 
rational expectations, the efﬁcient-markets hypothesis, etc. – in short, to modern ﬁnance. 
No wonder risk has won hands down over uncertainty. 
In addition, from a practical perspective, speaking of risk provides an illusion of quan-
tiﬁability and precision that regulators like because of the supposed ‘objectivity’ of the rules 
it brings about. 
3Greer (2000), page 33, quoted in Davidson (2009). To be fair, there is still active research in the area of 
uncertainty–such as, for instance, Kreps, Epstein, Wang and others. One of their more interesting results is that 
under uncertainty prices need not be unique. I am grateful to Dr Keating for pointing this out. 
4Lucas (1977, page 15), quoted in Davidson (2009, page 35). 
5Minimax solutions are, of course, also optimal, but in a rather uninteresting manner. 

12 
CHAPTER 2 RISK AND UNCERTAINTY 
But the fact that one set of results is more ‘sexy’, more fun to obtain and more handy to 
use than the other does not necessarily make that set more true – or more useful. The real 
question should be: leaving aside which approach is more fun to work with, is the problem at 
hand better described by a risk- or an uncertainty-based approach? If we are talking about the 
magnitude of weekly, monthly and perhaps even yearly losses, I believe that the risk (i.e., the 
known-probability-based) framework can yield very useful results. Traditional (frequentist) 
statistical techniques such as VaR do provide, in this domain, useful information. But what 
about much rarer events? 
Stress testing tries to graft onto this substratum of frequentist information a less precise, 
but still probabilistic, assessment of risk. I think that this is useful, but I truly do not know 
how ‘far in the tail’ even the subjective approach can be pushed. When we deal with the 
cataclysmic events that can put at risk the whole ﬁnancial system, I do harbour serious 
doubts as to whether stress testing – and stress testing for capital allocation purposes in 
particular – may not be already trying to stretch well into the Knightian-uncertainty domain. 
Would an intelligent risk manager have been able in late 2006 to give even an order-of-
magnitude assessment of the probabilities attaching to the events that were to unfold in the 
next 24 months? As an attempt to help the risk manager with such a difﬁcult task, I suggest 
in the next chapter a conceptual approach that may give some guidance in ‘conceiving the 
unconceivable’. But the task remains daunting. 
These considerations notwithstanding, I have taken a leaf out of Keynes’ book and, as 
the quote that opens this work recommends, I have decided to follow a pragmatic approach, 
and to make use of a pared-down probabilistic approach. This is because, if private ﬁnancial 
institutions and regulators want to associate capital to stress events – and this appears to be 
the choice of the moment – under the current conceptual framework some form of prob-
abilistic assessment of losses is unavoidable. Indeed, imposing a link between capital and 
probability of losses constitutes one of the cornerstones of the current regulatory paradigm. 
I have some doubts about the wisdom of this approach,6 but, on the other hand, I have no 
better suggestions. I have therefore taken the approach that, given the present probability-
related way of looking at capital, some link between large losses and the probability of their 
occurrence should be provided. We should therefore ask the risk manager to make use of 
those types of probabilities (frequentist or subjective) that are better suited to the problem 
at hand. And we should then set the terms of the problem in such a way that the ‘difﬁcult’ 
probabilities (the joint distributions) are obtained or approximated from more cognitively 
resonant quantities (such as marginal or conditional probabilities) using as much information 
as possible about ‘how the world works’. 
The approach I propose is therefore radically different from the frequentist VaR and Eco-
nomic Capital methods, which ‘go for the jugular’ and attempt instead the direct estimation 
of the king probabilistic quantity, i.e., of the joint distribution. I have taken the approach, 
in short, that one should use different tools for different problems (on the basis that ‘when 
your only tool is a hammer, every problem begins to look like a nail’), and that it is better 
to be approximately right than precisely wrong. 
6One might wonder, for instance, whether regulators, rather than private ﬁrms, should apply a minimax approach 
to regulation. 

2.3 SUGGESTED READING 
13 
2.3 Suggested Reading 
For a clear and very readable discussion of the difference between uncertainty and risk 
in decision making, a good starting point is Luce and Raiffa (1957). An explanation of 
what uncertainty (rather than risk) implies for asset prices and economic prediction can be 
found in Skidelsky (2009), who discusses the issue in the context of Keynes’ views on 
the matter. 


Chapter 3 
The Role of Models in Risk  
Management and Stress Testing 
Experience brings out the impossibility of learning anything from facts till they are 
examined and interpreted by reason; and teaches that the most reckless and treacherous 
of theorists is he who professes to let fact and ﬁgures speak for themselves. 
Alfred Marshall quoted in Friedman and Jacobson Schwartz (1963 [2008]) 
In this chapter I deal with two distinct but related topics that have a direct bearing on stress 
testing.1 These topics are of a more general and abstract nature than the material covered 
in the rest of the book. They are, however, every bit as important. 
The ﬁrst topic deals with the role of models in arriving at an understanding of ﬁnancial 
phenomena and attempting to predict future ﬁnancial events. A crucial distinction I will 
make is between reduced-form and micro-speciﬁed (structural) models. My claim is that, 
when it comes to stress testing, micro-speciﬁed models are much better suited to the task. 
Indeed, I will try to explain why the much more commonly used reduced-form models can 
be particularly misleading in the case of stress testing. 
The second topic deals with coordination and positive feedback mechanisms in ﬁnancial 
markets. I believe this feature is very important, and helps our understanding of why, in 
some important circumstances, prices appear to move away from any reasonable under-
standing of ‘fundamentals’. The relevance of this ‘run-away behaviour’ for stress testing 
is self-evident. Important as it is, however, the understanding of coordination and feed-
back mechanisms is not as crucial to my overall argument as the understanding of the role 
played by models in ﬁnancial predictions. So, if the reader is not convinced by my sec-
ond thesis presented in this chapter, but accepts the ﬁrst, the overall gist of the book will 
remain valid. If I fail to convince the reader of the validity of the ﬁrst argument, how-
ever, then I will be unlikely to make a convincing case for the main approach proposed in 
this book. 
1Some parts of this chapter have been adapted from Rebonato (2009) and Rebonato (2010). 
15 

16 
CHAPTER 3 MODELS IN RISK MANAGEMENT AND STRESS TESTING 
3.1 How Did We Get Here? 
As the saying goes, we learn from our mistakes. If this is true, after the events that unfolded 
during the 2007–2009 ﬁnancial crisis the risk management profession should be comprised 
of some of the wisest individuals on the planet. How did we end up becoming so wise? 
And what have we learnt? More concretely: if any of the numerous new risk management 
ideas that are now been touted as the ‘new best practice’ had been put in place in, say, late 
2005, would things have turned out any better? If not, what should we do instead? 
Great upheavals urge a radical rethink of the way we make sense of reality. Finance 
and economics are no exceptions. The Great Depression of the 1930s gave impetus to the 
transition from classical to Keynesian economics. The economic woes of the late 1960s and 
of the 1970s urged the switch from Keynesianism to neo-classical economics. Similarly, 
the events of the late 2000s are bringing to the fore a radical rethinking of the neo-classical 
economics orthodoxy – its reliance, for instance, on the ability of rational investors and 
private-sector ﬁrms to self-regulate economic activities.2 
As a consequence, the economic profession is in a state of ‘restlessness’, and vigorous 
criticism to the orthodox way of thinking is now coming not from mavericks ‘out on the 
fringes’ or from conspiracy theorists, but from mainstream, establishment economists and 
central bankers: 
The modern risk management paradigm held sway for decades. The whole intellectual 
ediﬁce, however, collapsed in the summer of last year [2007].3 
Change is in the air. For this reason it is particularly important to put the current thinking 
about quantitative risk management in its intellectual context. 
We did not get where we are by accident or ‘from scratch’, i.e., by thinking in the 
abstract: ‘how should we analyse ﬁnancial risk?’ The logical underpinning of quantitative 
risk management as it has been practised in the run-up to the crisis is intimately enmeshed 
with the prevailing (neo-classical) conceptual framework of ﬁnancial economics. Take, for 
instance, one of the cornerstones of the neo-classical ﬁnance ediﬁce, Markowitz’s portfolio 
diversiﬁcation. It shares obvious conceptual similarities with the risk management statistics, 
such as VaR or Economic Capital, that underpin the estimation of regulatory capital: indeed, 
‘diversiﬁcation’ is a key word both in Markowitz’s portfolio theory and in contemporary 
risk management. But efﬁcient asset allocation and modern risk management share a far 
deeper intellectual legacy: the idea, that is, that Rational Man, equipped with a Perfect 
Computing Machine, is supposed to be able to estimate all the statistical properties of 
return distributions (in the case of Economic Capital down to the highest percentiles), and 
to use this information to make ﬁnancial decisions under risk. 
As the quote above reminds us, this way of looking at ﬁnancial risk has held sway 
for decades. But if we are at a juncture when the very foundations of the ediﬁce of neo-
classical ﬁnance are being questioned – and I believe we are – then our way of looking at 
the management of ﬁnancial risk must be revisited as well. That we are at such a turning 
2See, e.g., Skidelsky (2009), Introduction. 
3Former Chairman Alan Greenspan, Congressional testimony, 23 October 2008. 

3.2 STATEMENT OF THE TWO THESES OF THIS CHAPTER 
17 
point is perhaps most clearly shown (again) by the words of the chastened high priest of 
the Rational Investor school, Chairman Alan Greenspan (2009): 
The extraordinary risk-management discipline that developed out of the writings of the 
University of Chicago’s Harry Markowitz in the 1950s produced insights that won several 
Nobel prizes in economics. It was widely embraced not only by academia but also by a 
large majority of ﬁnance professionals and global regulators. But in August 2007, the risk 
management structure cracked. All the sophisticated mathematics and computer wizardy 
essentially rested on one central premise: that the enlightened self-interest of owners and 
managers of ﬁnancial institutions would lead them to maintain a sufﬁcient buffer against 
insolvency by actively monitoring their forms’ capital and risk positions. 
It is silly to claim that we got where we are because the assumptions that underpin this 
way of looking at ﬁnancial problems are ‘wrong’. Model assumptions are always wrong, and 
to use McKenzie’s (2006) metaphor, a model is not a passive mirror of an external reality. 
When it comes to neo-classical ﬁnance, perhaps we have all been beguiled by the elegance 
of the construction, and we have begun taking the scaffolding (i.e., the assumptions) too 
seriously and literally. But, in the end, it is far too simplistic to claim that the root of all 
evil were ‘unrealistic assumptions’ or ‘the blind faith in the normal distribution’ – whose 
failure to describe returns is, by the way, one of the most uncontroversial and universally 
accepted facts in mainstream ﬁnancial econometrics. When it comes to quantitative risk 
analysis, something more fundamental is at play. We do not need another model. What is 
required is a richer way of thinking about how models interact with ﬁnancial reality. 
3.2 Statement of the Two Theses of this Chapter 
What does it mean that we require a new way of thinking about how models interact with 
ﬁnancial reality? As I mentioned in the opening paragraphs of this chapter, I present two 
distinct arguments. Each argument can, in turn, be split into sub-theses, which for clarity I 
present below. 
Argument 1: Centrality of Models 
1. We must recognize that data (e.g., empirical return distributions) do not speak by them-
selves, but only make sense in the context of models of reality. Without a plausible 
(not necessarily a ‘true’) generative model, data analysis is blind. 
2. This does not mean, however, that we should look for the Holy Grail of the correct 
interpretative model of reality. Especially when it comes to quantitative risk analysis , 
the search for a unique ‘true’ model (i.e., a unique ‘correct’ mapping from new 
information to price changes) may be misguided, futile and, at times, even harmful. 
3. It is instead more fruitful to entertain the possibility of the coexistence of a plurality 
of plausible interpretative models of reality, ranging from the fully ﬂedged, rigorously 
articulated, mathematically formalized and microstructurally founded models of the 
neo-classical synthesis, to the gloriﬁed rules of thumb used by traders. 

18 
CHAPTER 3 MODELS IN RISK MANAGEMENT AND STRESS TESTING 
4. Each of these models can be, and is, adopted or abandoned by market participants 
and analysts in an unpredictable fashion.4 
The second argument requires the validity of the ﬁrst, and builds on it as follows. 
Argument 2: Importance of Coordination 
1. Given the simultaneous coexistence of a variety of interpretative models of reality, and 
given the inability by traders to maintain a ‘conviction’ position for very long – the 
more so, the more turbulent the market conditions – market participants often ﬁnd it 
advantageous to engage in a game of coordination. 
2. In this game of coordination traders often adjust their positions not so much on the 
basis of their independent analysis of fundamentals, but by taking into account what 
other market players will do. 
3. The coordinated actions of many traders can give rise to positive-feedback mecha-
nisms, which can cause ‘wild’ price moves. 
4. These coordination and feedback mechanisms may be active at several levels in the 
market and in the economy at large: from private investors to proprietary traders, from 
institutional investors to central banks, etc. 
3.3 Defence of the First Thesis (Centrality of Models) 
I now handle in order the points into which I sub-divided Argument 1. 
3.3.1 Models as Indispensable Interpretative Tools 
Quantitative risk management in general, and stress testing in particular, are nowadays 
typically approached as an exercise in statistical analysis of return distributions. These, 
in turn, are derived from historical records (time series) of risk factors. The assumption 
behind this approach is that the answers to all our quantitative risk management questions 
can be found just by ‘looking at the data’. Non-parametric approaches to Extreme Value 
Theory are the logical conclusion of this approach, and constitute its natural extension to 
the domain of stress testing. 
Now, much too much has been written about whether the normal distribution provides 
an adequate description of ﬁnancial returns (for many applications, it clearly does not), and 
about which alternative statistical distributions may be up to the task. Let us leave aside 
the fact that, given a ﬁnite amount of relevant data, choosing among various distributional 
alternatives is a statistically very difﬁcult task.5 My claim is that, in itself , knowledge that 
our existing data are better ﬁtted by, say, a Stretched Exponential or a Power Law than by 
a Gaussian distribution is of little help, especially when dealing with tail behaviour . When 
it comes to the real ‘black swans’, i.e., to the events that can create havoc of systemic 
dimension, I maintain that this type of analysis is of very little use. 
4McKenzie’s (2006) engine is particularly apposite here, as a changed model implies a change in causality. 
5See, for instance, the discussion in Malvergne and Sornette (2006), Chapter 2. 

3.3 DEFENCE OF THE FIRST THESIS (CENTRALITY OF MODELS) 
19 
This is a bold claim, because these more exotic distributions are often invoked as the 
solution to the quantitative analysis of the extremely rare event. To see how I can justify my 
assertion I must explain the difference between reduced-form and micro-structural models. 
In general, reduced-form models abstract from the detailed mechanisms that generate the 
phenomenon at hand, and rely on observed regularities to ﬁt to the data the model ‘free 
parameters’. In physics, for instance, the very complex quantum interactions among the 
electrons and between the electrons and the nuclei in a solid can be ‘reduced’ to a simple 
model of balls linked by harmonic springs. The strength of the spring is then ﬁtted to some 
observable properties of the material (for instance, its elastic properties, or the slope at the 
origin of the phonon dispersion curve). 
The ﬁtting part of this approach is crucial to our discussion: physicists know that the 
springs do not really give a true micro-description of the solid, but force a link with physi-
cal reality by imposing the correspondence between a certain property of the idealized ‘toy 
model’ and the real solid under study. To go beyond this level of model reduction physicists 
then attempt to explain the strength of the spring on the basis of a theory of electron interac-
tions. This is where ab initio band structure calculations take over from empirical potentials. 
When they engage in band-structure calculations physicists give a micro-speciﬁed descrip-
tion of the spring. Sure enough, band structure calculations (despite the grand-sounding ab 
initio label) also conceal their own reduced-form models (for instance, the pseudo-potentials 
that describe the electron-nucleon interactions). 
The important point here is not that micro-speciﬁed models are necessarily ‘better’ than 
reduced-form ones (the elastic properties of a solid, for instance, are often better accounted 
for by the simpler spring model). What micro-speciﬁed models do is that they try to explain 
what reduced-form models only describe. They therefore provide a level of understanding 
that the often-better-ﬁtting reduced-form models cannot provide. And, above all, micro-
speciﬁed models give a clear indication of when the validity of a certain toy model stops. 
As a result, reduced-form models are loved by engineers, who have to deal with chal-
lenging but well-deﬁned and ‘within-the-paradigm’ problems, and superciliously regarded 
by pure physicists, who strive for explanation rather than description. 
One more observation: in physics – and, indeed, in many of the social sciences such 
as economics6 – the interaction between micro-speciﬁed and reduced-form models is very 
much like a series of Russian matryoshka dolls: a new structural model will typically intro-
duce higher-level reduced-form models in its formulation. A structural model explains, that 
is, the exogenous features of the reduced-form model ‘underneath’ (the ‘springs’), but does 
so by introducing a higher-level reduced-form description of reality. This is in turn phe-
nomenologically ﬁtted to some ﬁner observable quantities of the phenomenon under study. 
So, the spring model is a structural model with respect to continuum mechanics (which 
dispenses altogether with the concept of atoms), and is a reduced-form model with respect 
to ab initio band structure calculations. 
The real question, therefore, is not so much whether a model is micro-structurally 
founded or ‘reduced-form’ in the absolute, but at what level we stop explaining and we 
begin describing. 
When we look at current quantitative risk management in this light, its most extraor-
dinary feature is where it locates itself in this spectrum between full model reduction and 
6In economics there has been in the post-war years a concerted effort to provide micro-foundations to earlier 
economic descriptions that made use of ‘macro’ quantities. For instance, the domain of macroeconomics has 
progressively been eroded in the process – how proﬁtably and successfully is a different story. 

20 
CHAPTER 3 MODELS IN RISK MANAGEMENT AND STRESS TESTING 
complete micro-speciﬁcation: it is the most hard-core reduced-form model that we can 
possibly conceive! The only superstructure we allow to be built on data is the distribution 
of returns, with its coefﬁcients as the free-ﬁtting parameters of the reduced-form model (i.e., 
the equivalent of the strength of the springs in the model of solid). To pursue the metaphor 
of the study of solids, it is as if, in order to understand the elastic properties of materials, 
we simply recorded the mechanical behaviour of glass, wood, steel, diamond, grass and 
molybdenum under tension, looked for a suitable statistical distribution able to ﬁt these 
disparate behaviours and we stopped there. 
When the implicit assumption is made that we should ﬁt very accurately the existing data 
to very sophisticated distributions, without concerning ourselves at all with the generative 
mechanisms of the distributions themselves, we have given up any attempt to explain.7 We 
claim that pure description is enough. Dispensing in such a radical way with any explanation 
is, in a way, the ultimate reduced-form model. 
Unfortunately when one uses reduced-form models (just because they bypass a speciﬁ-
cation of how the data were generated in the ﬁrst place), one has no way of telling when 
yesterday’s parametrization ceases to be of use. The ﬁtting of statistical distributions (no 
matter how sophisticated) to past data therefore shares all the advantages and the unavoid-
able shortcomings of reduced-form models: without an understanding of how the exceptional 
returns in the beautifully-ﬁtted fat tail have been generated, there is no way of telling if 
yesterday’s parametrization is still valid today. And, when it matters, it invariably is not. 
This is well echoed in the words of Allison and Zelikow (1999) in their seminal Essence 
of Decision: 
Our ﬁrst proposition is that [. . .] bundles of [. . .] related assumptions constitute basic 
frames of references or conceptual models in terms of which analysts and ordinary 
lay persons ask and answer the questions: What happened? Why did it happen? What 
will happen? Assumptions like these are central to the activities of explanation and 
prediction.8 
In passing, believing that the history of price changes ‘speaks by itself’ and does 
not require any underlying model of reality shares the same intellectual foundations and 
credibility as chartism.9 I fear that it is just as useful. Once again, imperfect, but plausi-
ble, micro-speciﬁed (‘structural’) models are inﬁnitely more powerful in making ﬁnancial 
7There is an even closer parallel to the interatomic potentials in solids. Once the limitations of the simple (but 
powerful) spring-like model became apparent, one line of research became focused on creating more complex, 
but still reduced-form, models based on less-and-less-elegant variations on the simple spring theme. By and large, 
this proved to be a dead-alley: the improvement in understanding was marginal, and the increase in complexity 
substantial. 
The parallel with quantitative risk management here is remaining wedded to the same level of model reduction 
(all the information is in some return distribution), but tinkering with the exact shape of the distribution (say, 
Levy rather than Gaussian), without trying to explain why and under which circumstances a Levy distribution may 
emerge. 
8Introduction, page 4. 
9An obvious distinction must be made here: chartism attempts to predict the ﬁrst moment of the return 
distribution (‘where prices go’). Risk management concerns itself with the second (or higher) moments. The ﬁrst 
task is immensely more difﬁcult, and, with the possible exception of high-frequency data, there is virtually no 
evidence that this prediction can be successful. Estimating the second moment, on the other hand, can be done 
much more successfully. The second moment of a distribution, however, does not contain all the information we 
may want to know from a risk persective. If our interest is in extreme events, reliance on past data alone without 
a model to tell us when the data are relevant has little chance to fare any better than chartism. 

3.3 DEFENCE OF THE FIRST THESIS (CENTRALITY OF MODELS) 
21 
decisions because they give a possible explanation of why we should worry today . As we  
shall see, these simple explanatory models of reality play a pivotal role in the approach I 
propose in this book. 
There is another reason why relying on the ﬁt to an unconditional return distribution, 
no matter how accurate, does not help us very much. Even if we had ascertained beyond 
reasonable statistical doubt that, say, a Levy distribution with a characteristic exponent of 
exactly 1.75 gave the best description of our data, in itself this would be of little practical 
use. Yes, with this piece of information the probabilities of occurrence of extreme events 
would certainly be much higher (and probably more realistically accounted for) than if we 
had used a more ‘tame’ distribution. But, even if it were attainable and ‘true’, such an 
unconditional estimate (an estimate, that is, unrelated to present market conditions) would 
be of very little use to the risk manager. This is because, absent a model of reality, there is 
no way of telling whether the same information is conditionally relevant – i.e., is relevant 
to today’s world. Systematically buying insurance against extreme adverse events is far 
too expensive to be contemplated as an everyday strategy. And as any trader knows well, 
buying insurance on an ongoing basis against tail events will (more than) negate the excess 
return she expects to reap from entering into the risky trades in the ﬁrst place. 
What we need, again, are perhaps imperfect, but plausible, models of reality that tell us 
when it may be worthwhile to ‘use our silver bullets’ and spend money on protection, or 
simply to reduce the size of our exposure. What a useful quantitative risk analysis should 
provide is the conditional assessment that today, given the particular market conditions 
we are in and given our understanding of how the world works , certain extreme events are 
more or less likely to occur than they normally are. A risk manager can only carry out 
this analysis if she has at her disposal an imperfect but plausible structural model that links 
market information to outcomes. 
In the preceding paragraphs I have repeatedly mentioned ‘models’ (in the plural). This 
naturally brings me to my second point. 
3.3.2 The Plurality-of-Models View 
I stated in points 2 and 3 of Argument 1 that a plurality of interpretative models coexist ‘in 
the market’. This statement may seem commonsensical and uncontroversial, but it is not 
part of the economics orthodoxy (the rational-expectations and efﬁcient-market hypotheses 
in particular). If we accept the neo-classical view of rationality, in fact, according to Muth 
(1961) in the presence of risk 
[rational] expectations are formed on the basis of all the available information concerning 
the variable being predicted. Furthermore [. . .] individuals use available information 
intelligently; that is, they understand the way in which the variables they observe will 
affect the variable they are going to predict. 
As a consequence ‘expectations are essentially the same as the predictions of the relevant 
economic theory’ (Muth, 1961). There is, in effect, a ‘communism of models’, whereby ‘all 
agents inside the model, the econometrician and God all share the same model’.10 
10Interview with Thomas Sargent by Evans and Honkapohja (2005) quoted in Frydman and Goldberg (2007). 
It is worthwhile noting in passing that a strand of criticism of this view of rationality does not bring into play the 

22 
CHAPTER 3 MODELS IN RISK MANAGEMENT AND STRESS TESTING 
The logical inﬂexibility of some aspects of the neo-classical paradigm is difﬁcult to 
fathom for the educated layperson, or, indeed, for academics in other disciplines. For 
instance, most people would ﬁnd it only natural that, even if presented with the same 
information, different intelligent and educated individuals – indeed, even different experts 
in the ﬁeld – will in general reach different conclusions in matters of economic relevance 
(say, ‘Is the economy about to enter a recession?’). Yet heterogeneity of prior beliefs given 
the same background information is difﬁcult to reconcile with the hyper-rational, super-
Bayesian view of Homo Economicus. Consider the following quote by Bernard Dumas, 
Professor of Finance at the University of Lausanne, in his review of Shefrin (2008): 
Judging from the large volume of trade in the ﬁnancial markets and the astounding 
volatility of prices, one has to accept the idea that investors hold divergent and fast 
ﬂuctuating beliefs. For this to make sense, I see only two possible hypothesis. [Either] 
both individual and professional investors receive a lot of information – some of it public 
but a lot more private – on which they act. Or they all receive similar information, but 
each one interprets this information somewhat differently.11 
Let us pause to consider the full meaning of this quote. The suggestions that different 
individuals, given the same information, may process it somewhat differently, and reach 
somewhat different conclusions does not strike the average educated person as unreasonable. 
Perhaps what most would consider outlandish is actually maintaining the opposite. And even 
staunch market critics would consider heterogeneity of opinions more plausible than a degree 
of insider trading so pervasive as to constitute, effectively, the backbone of ﬁnancial markets 
(as the expression ‘some of it public but a lot more private’ implies). Yet, this is not the 
prevailing model of neo-classical ﬁnancial economics, according to which, for instance, the 
volume of trading remains a puzzle (‘Why trading at all, if everyone adjusts prices in the 
same manner?’, to paraphrase the argument in Bookstaber (2008)). So, the view that rational 
individuals may have different beliefs and reach different conclusions can be taken to be 
an ‘arresting, although by no means mainstream, hypothesis’ (Dumas, 2008). And Hersh 
Shefrin, the Mario L. Bellotti Professor of Finance at the Leavy School of Business, Santa 
Clara University, is hailed by Dumas for proposing such an apparently commonsensical 
view as a ‘maverick and a pioneer’. 
This hyper-rational view can be, and has been, criticized,12 but at least it has an impec-
cable, if slightly otherworldly, logical coherence. The same cannot be said of current 
quantitative risk management, which has borrowed freely but haphazardly from the neo-
classical toolkit. In common with ﬁnance theory, the statistical distribution of returns has 
for instance become central to quantitative risk management – if anything it has assumed a 
status of almost religious idolatry. However, the debatable (but at least logically coherent) 
link between these returns distributions and decisions via expected utility maximization has 
been forgotten, or bastardized beyond recognition, in risk management practice. Somewhere 
along the line, the implicit assumption has been smuggled in that, once obtained, impos-
sibly accurate statistical distributions of returns would, effectively, ‘speak by themselves’ 
cognitive limitations of human beings, but questions the ability to project into the future ﬁrm knowledge about 
preferences, technological advances, etc. This school of thought is referred to as Imperfect Knowledge Economics. 
Cognitive biases do enter the description – see, e.g., Frydman and Goldberg (2007), Chapter 5 – but the over-
extension of ‘perfect knowledge’ economics would apply even in the absence of these cognitive biases. 
11Bernard Dumas, endorsing Shefrin (2008), my emphasis. 
12See, e.g., Frydman and Goldberg (2007), Shefrin (2000), Shefrin (2008) and Shleifer (2000). 

3.4 DEFENCE OF THE SECOND THESIS (COORDINATION) 
23 
and self-evidently point to the correct risk management choice. At best, crude, ad hoc, 
logically dubious and cognitively unappealing rules of thumb have been offered as crutches 
to justify the decision-making process. Magic percentiles – the higher the better – were sup-
posed to constitute the be-all-and-end-all of risk management. (See Rebonato (2007) for a 
non-technical discussion of these points.) 
Lack of logical coherence is regrettable. However, I am not at all advocating that the 
way to improve quantitative risk management is to integrate the statistical data analysis 
with a single correct model of reality (be it the neo-classical one or any of the many alter-
natives that are being, and have been, proposed). Quite the opposite: I maintain that for 
risk analysis purposes what is more productive is the awareness of the different plausible 
interpretative models of reality, and of how different agents, by embracing these different 
plausible models, may react to new information. The relevance of this approach to risk 
management stems from the observation that – pace the neo-classical picture of Homo Eco-
nomicus – market participants do freely and eclectically move between different explanatory 
paradigms, and adjust their actions accordingly. Risk managers can only hope to keep track 
of this ever-shifting explanatory landscape if they themselves display a similar degree of 
mental ﬂexibility and eclecticism. Again, the words of Allison and Zelikow (1999) are 
particularly appropriate: 
Concepts and theories, especially ones that do real work, become accepted, conventional 
and efﬁcient for communicating answers. [. . .] [W]hen one family of simpliﬁcations 
becomes convenient and compelling, it is even more essential to have at hand one or 
more simple but competitive conceptual frameworks to help remind the questioner and 
the answerer what is omitted. They open minds a little wider and keep them open a 
little longer. Alternative conceptual frameworks [. . .] are essential as a reminder of the 
distortions and limitation of whatever conceptual framework one employs.13 
3.4 Defence of the Second Thesis (Coordination) 
In my ﬁrst thesis I argued that models are indispensable tools, not a luxury, if we want to 
make sense of reality. And that, at any one time, a number of models will in general offer 
themselves as plausible explanations of a given state of affairs. If correct, the second part of 
my ﬁrst thesis has a direct bearing on the argument about coordination that I present below. 
3.4.1 Traders as Agents 
What is the link between a plurality of models and coordination? 
If only one model of reality were the correct one (and if it were known to all the perfectly 
rational agents in the economy), a need for coordination would not arise.14 Yes, everybody 
would analyse data in the same way and reach the same conclusions about economic events. 
But coordination (indeed, identity) of action would be a necessary by-product of this com-
munism of models (shared, remember, by the market agent, the econometrician and God), 
not a conscious goal. 
13Allison and Zelikow (1999), Introduction, page 8. 
14Admittedly, for the result to hold, everyone should share not only the same model and information, but also 
the same endowment and the same risk aversion. 

24 
CHAPTER 3 MODELS IN RISK MANAGEMENT AND STRESS TESTING 
Matters immediately look very different if many models can be considered to be a plau-
sible description of reality by reasonable and well-informed market agents. In the presence 
of this model uncertainty, perhaps the greatest danger for any one trader is not so much to 
be ‘wrong’ (whatever that may mean), but to be out of step with the way fellow traders will 
interpret reality. This is because most market participants are agents and not principals, and 
their ‘staying power’ (i.e., the length of time over which they will be able to convince the 
ultimate providers of capital or liquidity not to worry about the ‘temporary mark-to-market 
losses’) is very short.15 
This is poorly accounted for by ﬁnancial economics, which typically looks at market 
participants as principals who invest their own money with a clear understanding of funda-
mental value and an Olympic disregard for the temporary vagaries of prices. Such a picture, 
however, does not chime well even with casual observation of market dynamics. In times of 
severe market turmoil even a ‘conviction trade’ – put in place by an agent whose principal 
does not share the same knowledge16 and the same payoff proﬁle – can last much less than a 
week. In these days of balance sheet and capital constraints, of skittish risk managers and of 
worried regulators, the life of a pseudo-arbitrageur caught on the wrong side of the consensus 
interpretation of a market event may not be brutish and nasty, but is certainly short. 
As a suggestive example of a recent event that ﬁts well with this interpretation, consider 
what happened to the Bear Stern share price in the immediate wake of the ‘Thursday rescue’ 
by the Fed. Recall that on 14 March 2007, the investment bank Bear Sterns was running 
out of cash, and the possibility of a systemically disastrous bankruptcy was looming. To 
avert this occurrence the Fed’s ﬁrst course of action was to make funding available to Bear 
Sterns through JP Morgan. The measure was complex, unprecedented and the press releases 
that accompanied it far from crystal clear.17 
Now, we must remember that there was no precedent for the rescue plan, and that 
there was a high degree of uncertainty about its mechanics and about the validity and 
enforceability of the various guarantees. Under these circumstances, it would be reasonable 
to expect that different traders would interpret the scant available information in a variety of 
possible ways. If these interpretations were translated into ‘independent’ trades one would 
expect that, after the netting off of the bullish and bearish trades, a relatively modest 
movement up or down in the share price due to the residual un-netted views would result. 
But, given the complexity of the matter and the lack of any precedents one would have 
expected that ‘wait-and-see’ would have been the better part of valour. 
So, unless the extraordinary news of the day lent themselves to an unequivocal interpre-
tation by the vast majority of market participants (which they obviously did not), a view of 
the independent, uncoordinated traders would not suggest an opening price move shooting 
up or down in the ﬁrst half hour of the trading day. Plausibly, one could expect some ini-
tial price movement (due to the predominance of positive or negative interpretations of the 
ambiguous facts). And one would also not be surprised to see this initial price change during 
the day, as more and more traders began to look at the information in a different light. 
But this is not at all what happened. The stock price action on Friday morning saw an 
initial euphoria and relief rally that brought the share price up by 10% in less than an hour. 
So, despite the uncertainty of the situation, at the opening of the trading day a vast majority 
15See Shleifer (2000). 
16This is what Shleifer (2000) calls the unfortunate divorce of ‘brawn and brains’. This is one of the explanations 
put forth by critics of unfettered market efﬁciency for the observed existence of limits to (pseudo)-arbitrage. 
17For a detailed and illuminating account, see Cohen (2009), Chapter 6, page 70 and passim in particular. 

3.4 DEFENCE OF THE SECOND THESIS (COORDINATION) 
25 
of traders seemed to analyse the scant available information by giving the same positive 
interpretation to the actions of the Fed. But just a few hours later on the same day the share 
price was down 50%.18 
The interesting fact throughout these roller-coaster price gyrations is that no meaningful 
piece of information regarding Bear Sterns materialized during the course of the 60% peak-
to-trough price drop. The price action, with a sudden rise at the opening, a brief period of 
range trading and a rapid decline to a completely different equilibrium is difﬁcult to reconcile 
with a view of different market agents independently expressing their views on the basis of 
their competing models of reality. An explanation, instead, that invokes coordination among 
traders can account for these price moves, and many similar events, much more naturally 
and convincingly. It is to this explanation that I therefore turn. 
3.4.2 Agency Brings About Coordination 
Leaving this example aside, the view of trading that I propose is consistent with a coordina-
tion game played by the market participants. A thumb-nail sketch of what I suggest would 
start from the observation that these market participants are predominantly agents. This 
agency position reduces their ‘staying power’, and can force them out of a position (at great 
loss) before they can be proven right. This (i.e., a premature forced termination of a trade) 
is the state of affairs an agent trader will try most to avoid. Hence the desire to coordinate. 
Let us look at this in a bit more detail. As information arrives, traders have to anal-
yse it and process it in a very short time. As a ﬁrst step they will tend to categorize 
this information in one of the ‘market modes’ they recognize – the ‘inﬂationary story’, the 
‘repeat-of-the-Great-Depression story’, the ‘emerging-market-decoupling story’, etc. Now, 
a principal investor should just try to pick the right ‘story’ – this is trading based on funda-
mentals. But we have seen that the worst situation for an individual agent trader is not so 
much to be ‘wrong’, but to give a dissonant categorization of the new piece of information. 
As a consequence, she will have interest in coordinating her categorization with that of the 
other market participants.19 This coordination becomes more important, the greater the state 
of turmoil of the market – and, therefore, the smaller the ‘leeway’ providers of capital and 
liquidity are prepared to give to the traders who invest money on their behalf. 
A view that has a lot in common with the one I have sketched above (but that focuses on 
a similar coordination enacted by principals – i.e., ultimate investors – rather than agents) 
has been recently formalized by Ozdenoren and Yuan (2008),20 who point out that 
. . . investors have an incentive to coordinate, which may generate self-fulﬁlling beliefs 
and multiple equilibria. Using insights from global games, we pin down investors’ 
beliefs, analyze equilibrium prices, and show that strong feedback leads to higher excess 
volatility. .  ..21 
18The [i]nitial reaction seemed to be very positive [. . .] Why it went south I don’t know. I can tell you I had 
analysts calling me on Friday morning. They couldn’t ﬁgure out why the stock was getting pummeled this way, 
because they were saying, ‘I don’t get it. It seems like a good thing. Why is the market reacting like this? It seems 
like a good thing . . .’ (Mr Molinaro reported in Cohen (2009), page 71). 
19The second safest thing for a trader, the saying goes, after being right on one’s own, is to be wrong with 
everyone else. 
20Agency relationships, of course, only exacerbate the dynamics highlighted by Ozdenoren and Yuan (2008). 
21My emphasis. 

26 
CHAPTER 3 MODELS IN RISK MANAGEMENT AND STRESS TESTING 
And of course, Keynes wrote about this as far back as the 1930s, when he compared 
investment decisions to a competition run at the time by a newspaper. In those political-
correctness-unaware days, a parade of pretty faces was shown in an evening paper (Akelrof 
and Shiller, 2009), and a prize was promised to the readers who would pick not the most 
attractive face, but the one chosen by the majority of readers as the most attractive. In my  
reading of what happened, the equity traders on the fateful Thursday of the attempted Bear 
rescue were doing exactly this: they were not trying to choose the prettiest face, but the 
face that the majority of other traders would vote as the prettiest. I believe that, especially 
in situations of market turmoil, the beauty-parade-of-models metaphor is more relevant 
today than ever. 
Another comment about coordination: different agents (hedge fund managers, proprietary 
traders of banks, asset managers, pension fund trustees, etc.) have different horizons. Here 
by horizon I do not necessarily mean the investment horizon. Rather, I am referring to 
the time frame over which a losing trading position or investment can be allowed to run 
before the ultimate providers of capital pull out the money, stop-loss limits are breached, 
lenders sell collateral or regulators require defensive actions. In general, the quieter and 
more benign the market conditions, the longer this horizon will be. In periods of market 
turmoil, however, when uncertainty about value quickly increases, fewer and fewer agents 
are trusted with running a ‘fundamentally sound’ but losing position, and even the ultimate 
principals with the longest staying power throw in the towel. 
When this happens the need for coordination ceases to be limited to the speculative fringes 
of the investment landscape, but becomes a pervasive feature of the economy. During the 
winter of 2007 and throughout the ﬁrst half 2008, for instance, many traders felt that the 
prices of some securities had fallen ‘too much’ with respect to any plausible fundamental,22 
but were reluctant to try to ‘catch a falling knife’. In these situations of generalized uncer-
tainty nothing is more lethal than being a contrarian too soon, and coordination becomes an 
imperative. 
3.4.3 From Coordination to Positive Feedback 
Why is coordination so important from the point of view of risk analysis in general, and of 
stress testing in particular? Because it can produce positive feedback mechanisms, herding 
behaviour, cascades and, ultimately, even bubbles (see, e.g., Chamley, (2004)). Coordina-
tion, and feedback in particular, can bring about ‘the emergent properties of interacting 
systems – for want of a better term, organized complexity’ (Miller and Page, 2007). As 
Miller and Page point out 
[w]e often see unanticipated statistical regularities emerging in complex systems. These 
regularities go beyond the usual bounds covered by the Central Limit Theorem [. . .] 
Agent intention can [. . .] alter the patterns that emerge in complex systems. [. . .] As we 
give the agents even more strategic ability, we often see elaborate dances of strategies, 
with good and bad epochs, cycles, and crashes. [. . .] In systems characterized by the 
Central Limit Theorem, interactions cancel each other out and result in a smooth bell 
22At one point, for instance, the prices of some highly structured securities implied mortgage default rates for 
Alt-A borrowers of 75%. 

3.5 THE ROLE OF STRESS AND SCENARIO ANALYSIS 
27 
curve. In complex systems, interactions reinforce one another and result in behaviour 
that is very different from the norm.23 
These behavioural features may therefore leave their signature in the distribution of asset 
prices in the shape of fat tails even if the statistical distribution of new information were 
perfectly Gaussian – which, in all probability, it is not in the ﬁrst place. Furthermore, all 
these dynamics are clearly exacerbated by the presence of leverage in the ﬁnancial system. 
Coordination plus leverage can therefore produce cumulative price moves of magnitudes 
that are well in excess of what would apply if every agent acted independently using her 
own ‘model’. When this happens the distribution of returns can easily become populated by 
the ‘ten-standard-deviation’ events at whose occurrence newspapers never cease to marvel. 
In neo-classical ﬁnance the existence of irrational bubbles is difﬁcult to explain.24 And 
certainly, one must resist the temptation to see bubble-creating mechanisms in every aspect 
of market dynamics. After all, the ‘market’ is made up of a variety of players, each with 
different goals, institutional constraints, horizons and, yes, ‘models’. In most circumstances, 
the occurrence of a sequence of coordination-driven run-away price moves can be broken 
by the springing into action of a different class of traders, investors or hedgers. However, 
as discussed above, there are situations when this may not happen. It is when these natural 
circuit breakers fail that danger looms. 
3.5 The Role of Stress and Scenario Analysis 
If these views are correct, how should we carry out the quantitative analysis of ﬁnancial risk? 
To go back to the Keynes beauty parade, one proﬁtable approach is to try to gauge which 
of today’s ‘pretty faces’ are in the running to be chosen by most readers tomorrow – or, to 
abandon the metaphor, to be aware of today’s plausible interpretative models of reality, so 
as to be prepared when the one that will prevail tomorrow will assert itself as the ‘winner’, 
and when possible, to ask the question: what is today’s ‘dominant story’? 
The relevance of this to stress testing is clear. 
First, we must recognize that thinking the improbable-but-plausible is at the core of 
the art of stress testing. Being too wedded to one view of the world – in a way, being 
too ‘logically coherent’ – can make us blind to very plausible alternatives. During periods 
of turmoil, events impossible according to one view of the world can happen twice a 
week. And neglecting the possible effect of coordination can make us underestimate the 
magnitude – and sometimes the suddenness – of price swings. 
Second, an understanding of the reason for the coordination of traders’ actions and of 
when and where this coordination might occur is essential if the risk manager wants to be 
able to identify the ‘vulnerabilities’ of the portfolio under her watch. And as we shall see in 
Chapter 13, being cognizant of the ‘pressure points’ of a portfolio is extremely important, 
especially if a bottom-up approach to stress testing is adopted. 
My analysis therefore calls for a very important role for expert subjective judgement. 
Stress testing and scenario analysis become two of the practical tools to translate these 
23Chapter 4, pages 49–50, my emphasis. 
24Leyland and Gennote’s work, quoted in McKenzie (2006), posits rational investors who know or do not 
know the re-hedging plan of portfolio insurers. 

28 
CHAPTER 3 MODELS IN RISK MANAGEMENT AND STRESS TESTING 
subjective inputs into a coherent representation of the vulnerabilities of a portfolio. Scenario 
analyses and combinations of stress events provide us with a tool to describe and, when 
appropriate, quantify our uncertainty about how future events will unfold (their likely mag-
nitude, whether they are plausible or not, whether they are likely to occur together). They 
therefore become the natural tools to take into account the multiplicity of possible inter-
pretative models of reality that I have discussed above, and to express the risk managers’ 
understanding of the feedback dynamics that may apply to her portfolio. 
This is all well and good at a very abstract level. But how do my suggestions ﬁt with the 
organizational reality of a ﬁnancial institution, and of a regulated one in particular? To be of 
practical relevance, any suggestion as to how we may control ﬁnancial risk must lend itself 
to being embedded in the organizational and governance structure of a ﬁnancial institution, 
of which risk management is only one part, and often far from the most important one. 
One important aspect of what I called above the organizational structure is the creation 
and dissemination of risk management information: who gets to see what and at what point 
in time. Inevitably, as information percolates up the organizational chain, it becomes more 
and more condensed, synthetic and stylized. Key risk indicators tend to take the place of 
discursive narrative. Once we are at Management Committee level or above we are fully 
in PowerPoint land. If any suggestion about stress testing is to be taken seriously, it must 
take this reality into account. It must, for instance, lend itself to the preservation of intuitive 
and cognitively appealing information even after the successive distillations that produce 
a board report have been carried out. The approach that I suggest in this book takes full 
account of these needs, and its outputs can be not only presented, but also challenged and 
discussed, at all levels in the organization. See in this respect my discussion in Chapter 14. 
Admittedly, introducing competing models and the need for expert judgement in the risk 
analysis takes away ‘objectivity’ from the process (even if this objectivity was probably 
never there in the ﬁrst place). And formula-based approaches, for all their limitations, do 
have the positive features of allowing standardization, reproducibility and comparability 
across ﬁrms. So, even as they recognize the limitations of traditional quantitative measures 
of risk, neither private ﬁrms nor regulators can afford to do away with them. But it is the 
balance between the quantitative and the subjective that could be proﬁtably altered. This, 
of course, will require greater engagement, by regulators and by senior managers, with the 
more subjective aspects of risk management. This is certainly a very demanding process. I 
will discuss in Chapter 14 if (and, if so, how) this may be done in practice. 
Luckily, not everything becomes more difﬁcult when one shifts the weight away from 
statistical measures of risk towards ‘expert judgement’. Many of the senior decision-makers 
of ﬁnancial institutions have neither the technical ability nor the inclination to challenge 
the mathematical assumptions of a statistical model. They are, however, both perfectly 
capable and, given their likely professional background, often naturally inclined to question 
the subjective reasoning process that leads to a well-thought-out stress scenario. Unlike the 
statistical assessment of a probability or a percentile, a stress scenario is ultimately a ‘story’, 
with its resonance in our understanding of what has happened in the past and of whether 
and why it may or may not happen again today. Ultimately it has a ‘narrative’. And when 
it comes to inﬂuencing decisions and prompting action, the power of a ‘story’ should never 
be underestimated. A ‘plausible model of reality’ is exactly that, a ‘story’ that connects a 
variety of visible and readily understandable inputs to more or less extreme outcomes. The 
cognitive resonance of this approach may make a big difference when it comes to informing 
behaviour and, perhaps, to deciding to use the silver bullets of protection I referred to above. 

3.6 SUGGESTIONS FOR FURTHER READING 
29 
I think it is useful to present in this context an extended passage from Casebeer (2008): 
A research program by Mark Johnson, George Lakoff, (Lakoff and Johnson, 1980), Giles 
Fauconnier and Mark Turner (2002) has explored reasoning by metaphor and analogy in 
a rich research program; they and others conclude that our most complex mental tasks 
are usually carried out not by ‘classical mechanics’ of rational actor theory (where stories 
really have no place in the details) but rather by a set of abilities that enable us to make 
analogies and map metaphors, which forms the core of human cognition. Exploration 
into the ‘storytelling man’ is a research program that combines metaphor and analogy 
into an examination of the powerful grip narrative has on human cognition; narratives 
can restructure our mental spaces in ways that profoundly affect our reasoning ability. 
I would not want to overemphasize the ‘emotional’ and ‘meta-logical’ aspects of a well-
designed stress testing programme. After all, I intend to present in this book tools for a sober 
and rational analysis of ﬁnancial risk, not for an ‘intuitive approach to risk management’. 
And indeed, I will stress the importance of strict logical coherence in our stress assessments. 
However, the ultimate goal of a risk manager is not to produce risk reports, but to inﬂuence 
the behaviour of the decision-makers in a ﬁnancial institution and to ‘obtain results’. If this 
is the goal, assuming that the recipients of the information are hyper-rational agents capable 
of digesting and analysing reams of data irrespective of the way they are presented is not 
a very useful starting point. If ‘stories’ have the power to inﬂuence behaviour more than 
the dry presentation of information, this should not be ignored. And if stress-based analysis 
shares some important features of ‘story telling’, so much the better. 
The regulators have become aware of the need to think outside the narrow statistical, 
model-based approach that they had until recently so strongly emphasized, and are engaging 
with the ﬁnancial industry in a far-reaching re-thinking of their stress-testing and scenario-
analysis framework.25 As I have had the privilege of being personally involved in this 
process, I can conﬁdently say that the regulators are genuinely open to new ways of look-
ing at risk – including more subjective ones. As their remit requires, they will not dictate 
how individual ﬁrms will carry out these new tasks, but they will opine on the quality and 
acceptability of the outcomes. My suggestion is that, in their assessments of the industry 
efforts, they may want to consider the quality of the process by which the ‘stressful out-
comes’ produced by the banks have been arrived at. Have these results been obtained by 
looking at ﬁnancial reality via the narrow prism of a single ‘best model’? Or have they 
taken into account the plurality of plausible models of reality and the coordination among 
market players that this can generate. Just as importantly, they may want to consider how 
these initiatives ﬁt in practice in the organizational structure of the regulated institutions: 
who gets to see these results, what kind of challenge and questioning they generate, what 
actions are prompted. 
I believe that the approach I describe in this book can be so implemented as to satisfy 
these complex requirements. 
3.6 Suggestions for Further Reading 
Some of the themes developed in this chapter can be found in Rebonato (2009). A very good 
reference about coordination and herding behaviour in general, and with speciﬁc references 
25See, e.g., BIS (2009). 

30 
CHAPTER 3 MODELS IN RISK MANAGEMENT AND STRESS TESTING 
to ﬁnance, is Chamley (2004). Two recent articles that cover topics of coordination in 
ﬁnancial markets are Guarino and Cipriani (2008) and Ozdenoren and Yuan (2008). 
The emergence of complex coordination from the actions of agents is the subject of 
the trendy discipline of ‘complexity studies’. The grandfather of the approach is probably 
Nobel-prize winner Thomas Shelling, whose Micromotives and Macrobehaviour (1978) 
remains a classic. The reader should then approach the more recent literature with some 
circumspection, because the quality is rather uneven. Miller and Page (2007) provide a 
sober, non-technical introduction. 
Heterogeneity of beliefs and its impact on asset prices are treated very well in Shefrin 
(2008), who uses concepts from behavioural ﬁnance to recast the traditional asset pricing 
paradigm. Of particular interest is the treatment of the stochastic discount factor in the 
presence of heterogeneous beliefs. The treatment by Shefrin is to be commended for its 
clarity. Buraschi (quoted in Rebonato (2009)) provides another interesting perspective on 
the importance of heterogeneous beliefs in the case of derivatives markets. 
A cogent critique of the neo-classical view that market agents can identify precisely 
the set of variables that drive ﬁnancial outcomes is given by the Imperfect-Knowledge 
Economics (IKE) school. The book by Frydman and Goldberg (2007) is excellent and 
thought-provoking. Also see Frydman and Goldberg (2009) for a perspective on the impli-
cations for the scope of optimal regulation. 
For a discussion of the limits of arbitrage in the presence of principal/agent relationships, 
and of the consequences of this ‘divorce of brawn and brains’ for asset pricing, Shleifer 
(2000) provides an excellent discussion. 
Soros (1987) and Soros (2008) offer a perspective about feedback mechanisms in ﬁnancial 
markets that is related to the one presented in this chapter. Not surprisingly, the insights about 
the workings of ﬁnancial markets afforded by George Soros deserve very careful reading. 

Chapter 4 
What Kind of Probability 
Do We Need in Risk 
Management? 
. . . between one out of three and even . . . 
The odds of nuclear confrontation between Russia and the USA, as stated by 
President J. F. Kennedy during the Cuban missile crisis, quoted in Allison and Zelikow 
(1999) – certainly not a frequentist statement, and yet one of which President Kennedy 
was able to make very proﬁtable use. 
4.1 Frequentist versus Subjective Probability 
In common speech we use the same word ‘probability’ to cover a variety of meanings. Take 
for instance the following three sentences: 
The probability of this coin landing ‘heads’ is 50%. 
There is a high probability that Mr Burns killed his wife. 
The probability of a woman becoming President of the United States within ten years 
is less than 10%. 
The same word (‘probability’) has been used in the three sentences, but clearly we do 
not really mean quite the same ‘thing’ in the three instances. For instance, try to modify 
the ﬁrst and last sentences as follows: 
The probability of this coin landing ‘heads’ is 50.002 ± 0.001%. 
The probability of a woman becoming President of the United States within ten years 
is less than 10.034 ± 0.001%. 
In the ﬁrst case the sentence makes perfect sense. The statement may be correct or 
not, and we may want to ask a lot of questions about how carefully the experiment was 
designed – for instance, how many times the coin was tossed, how we ensured that the 
31 

32 
CHAPTER 4 PROBABILITY IN RISK MANAGEMENT 
conditions were exactly the same, how we established the conﬁdence interval, etc. But in 
principle there is nothing in the ﬁrst sentence that offends our common sense. 
The second sentence, however, leaves us at best perplexed. Perhaps we even ﬁnd it 
nonsensical. We are not sure how anybody could possibly have arrived at this statement. 
And even if we were told that the statement is exactly true (whatever ‘true’ in this context 
means), we would not quite know what use to make of all that incredible precision.1 
The coin-tossing probability represents a text-book case of frequentist probability. The 
President-of-the-US probability is a classical example of subjective probability. It is related 
to, and sometimes interchangeably referred to as, probability as degree of (rational) belief. 
The philosophical debate on frequentist versus subjective probability has been raging for 
almost a century, and I will not even begin to address it.2 Much as I will try to remain as 
‘impartial’ as possible, I must however admit up-front that I ﬁnd the subjective (Bayesian) 
view of probability much more convincing than the frequentist one. To be more precise, I 
like to think in terms of a continuum of settings, from almost perfectly frequentist to almost 
completely subjective. In Bayesian terms, the exact positioning on this spectrum depends 
on the relative importance of the prior and the likelihood functions – where by ‘importance’ 
I mean how peaked these functions are. If you are familiar with Bayesian probability you 
may agree or disagree with my view, but it will at least make perfect sense to you. If you 
are not, I will not try to introduce the terms of the debate. Should anybody be interested in 
my views on the matter, Rebonato (2007) has an extensive but non-technical discussion of 
the topic. 
For the purpose of our discussion, this deﬁnition of frequentist probability, or rather of 
its domain of applicability, will serve us well (Jaynes, 2003): 
The traditional frequentist methods [. . .] are usable and useful in many particularly sim-
ple, idealized problems; but they represent the most proscribed special case of probability 
theory, because they presuppose conditions (independent repetition of a ‘random experi-
ment’ but no relevant prior information) that are hardly ever met in real problems. This 
approach is quite inadequate for the current needs of science. 
In my opinion the frequentist approach is unfortunately just as inadequate for the current 
needs of quantitative risk analysis. Yet, whenever we employ statistical techniques such as 
VaR, we implicitly endorse a purely frequentist view of probability. We assume that all the 
information of relevance is ‘in the data’ (in the likelihood function). We imply that we have 
no knowledge whatsoever about how the world works (leaving aside technical details, we 
assume that our prior is inﬁnitely diffuse). We believe, in short, that we are tossing coins, 
and not making informed guesses about the gender of the next President of the United States. 
1For instance, if someone told you that the probability of this coin landing heads is truly 50.002 ± 0.001%, 
leaving aside your moral scruples, you would immediately know how to make a lot of money setting up a casino 
table called ﬂip-the-coin. Your precise knowledge, if not shared by the rest of the population, could allow you to 
reap untold riches. But what extra proﬁt could you make from the knowledge that the probability that the next 
President of the United States will be a woman is 10.034 ± 0.001%, rather than 10%, given that the event will 
happen only once? 
2I do take sides in this debate, but in this very charged topic I try always to keep in mind the following quote 
by Ted Rosencrantz (in Rebonato, 2008): 
Here’s one useful rule of thumb: there are very few ﬁelds in which every single practitioner is an 
idiot. If your understanding of a ﬁeld is such that all practitioners of it must be idiots, then probably 
you’re not understanding it correctly. 

4.1 FREQUENTIST VERSUS SUBJECTIVE PROBABILITY 
33 
Is this reasonable? If we cast our minds to real-life risk management problems, when can 
we expect coin-tossing-like probabilities to be appropriate? First of all, for the frequentist 
approach to make sense we must be dealing with repeated draws under identical conditions 
from exactly the same price distribution – see again the quote above by Jaynes. To have 
access to this distribution we must have ‘relevant’ data, i.e., data that pertain to the same 
market conditions and institutional arrangements that we encounter today. And if we want 
to make very precise statements about the underlying distribution – e.g., if we want to say 
something about very high percentiles – we must also have lots of relevant data. 
By the way, when we use statistical tool such as VaR, we proclaim ourselves innocent of 
any knowledge about how the world works (the ultimate hypotheses-non-ﬁngo approach), 
but, as we choose the length of our dataset, we claim that we know exactly which portion of 
the past is relevant to predicting the future. Very often, for instance, we claim that, say, the 
last 200 days are perfectly and equally relevant to predicting what will happen tomorrow, 
but that what happened 201 days ago bears no relevance whatsoever to our predictions.3 
Sometimes our weighting scheme is not so ‘digital’, but how do we decide about weights 
and ‘decay constants’ without a prior understanding of the relevance of past events? I do 
not really understand how this is possible, but let me gloss over this point. 
Let us rejoin the main thrust of the argument. First, as we have seen, for the frequentist 
approach to be valid, I must draw from the same distribution under identical conditions – the 
world must not have changed a bit over my sampling period and the projection horizon. 
Second, if I want to talk about rare events, I need lots of relevant data. But these two joint 
requirements impose a limit on the horizon over which I can project my forecast. This is 
simply because, in an environment as dynamic and ever-changing as ﬁnancial markets, I do 
not have many independent draws obtained under the same conditions as the ones prevailing 
today. So, with data collected in the last few months or years I can perhaps estimate 
with some degree of conﬁdence the tails of daily or weekly, monthly and perhaps yearly 
distributions of price changes. But I cannot reasonably expect to say anything reasonable 
about the 99.975th percentile of the one-year distribution (as a ‘fundamentalist’ interpretation 
of the Economic Capital project implies). 
Now, there are two routes to gathering lots of data. We can collect them at a very high 
frequency or we can look a long way back into the past. What about the latter option? Can 
this help? This may well increase the quantity of our data, but are we gathering ‘relevant’ 
data? Are we really looking at tosses of the same coin? How conﬁdent are we that data 
collected 50, 10, 5 or perhaps even 2 years ago are relevant to predicting what will happen 
tomorrow? If I want to say something about a 1-in-20-year widening in the spread of 
CDSs, it does not help me that 20 years ago CDSs barely existed. More generally, ﬁnancial 
innovation – one of the hallmarks of ﬁnancial markets in the last decades – has not simply 
added new instruments to the market place. It has greatly changed the ‘workings’ of the 
markets as well. In the ‘good old days’ of CDO tranche issuance, for instance, CDSs became 
traded in higher volumes than ever before to hedge the exposure to the various tranches 
retained by investment banks. This created what I have called elsewhere ‘bipolar liquidity’: 
i.e., great liquidity, and tight bid-offer spreads, in normal market conditions; and a sudden 
3At the time of writing, some banks use regulatory VaR with a data window of 250 days, and others of 500 
days. They both do so with the blessing of the regulators. For the former set of banks the events that followed 
Lehman’s default have just, and very suddenly, disappeared from their risk radar screens. For the latter set, these 
events will remain fully relevant for one more year, and then they will also disappear into oblivion. 

34 
CHAPTER 4 PROBABILITY IN RISK MANAGEMENT 
drying up of liquidity, with gaping bid-offer spreads, whenever a squall hits the market.4 
Clearly, information about the price movements of CDS spreads before the ‘invention’ of 
CDO tranches would be of very little use in predicting spread movements in the heyday 
of the synthetic CDO market. (Note again how it is non-data information that allows me 
to express an opinion – right or wrong as it might be – about the relevance of different 
patches of the past to the statistical problem at hand.) 
So, going a long way back into the past may not be such a good idea, because that distant 
past may not be relevant to today. But what about collecting data at higher frequency? Would 
this give plenty of relevant (because it is recent) data? 
Unfortunately, also here there are problems. To begin with, very often the frequency 
of data collection is beyond the control of the observer. Firms default when they want, 
for instance, and hurricanes arrive when they please. Sometimes we do have the ability to 
increase the frequency of our data collection (with FX data we can gather minute-by-minute, 
or even tick-by-tick, information). However, it is not obvious that the statistical properties 
of the distributions for the longer forecasting horizons we are probably interested in can be 
‘safely’ obtained from the high-frequency data. Of course, if returns were perfectly indepen-
dent, there would be well-established techniques (convolution) to obtain, in theory, one-year 
distributions from one-hour distributions. In principle, this would be possible exactly and 
not as an approximation: under independence of returns the tail of the one-hour distribution 
built with thousands and thousands of observations would contain all the information we 
need to estimate the tail of the one-day, one-week or even one-year distributions. Unfor-
tunately, empirical analysis shows5 that, because of the absence of serial independence of 
returns, this is not the case, and longer-horizon distributions cannot be ‘guessed’ simply by 
collecting lots of high-frequency data.6 
So, when we try to estimate the probability of occurrence of very rare events, we are 
caught between the rock of the relevance of very ancient data and the hard place of the 
frequency of data collection: look too far back in the past, and the data may refer to an 
altogether different world; collect data too frequently (assuming that you can), and you may 
be uncovering the microstructure of short-scale phenomena that, as a risk manager, you are 
unlikely to be interested in. 
This places some hard limits on the applicability of frequentist statistical methods to the 
estimation of the probability of rare events in general, and of high percentiles in particular. 
Let us look at the different aspects of the problem one by one: 
• Whenever I speak of the probability of, say, a given price move I must always specify 
over which time period this move applies. A move in long-term yields by 200 basis 
points may be extraordinary for a one-day horizon, but commonplace over a year. 
• The longer this projection horizon (the ‘holding period’), the fewer the independent 
(non-overlapping) segments of history of the same length I have at my disposal. With 
four years’ worth of (relevant) data, I have about 1000 independent observations of 
daily returns; but only 16 observations of quarterly returns. 
4See Rebonato (2009, 2010). 
5See, e.g., Figure 2.2 in Malvergne and Sornette (2006), and the discussion that accompanies the ﬁgure. 
6‘[A]ssessing extreme risks at large timescales (1 or 10 days) by simple convolution of the distribution of 
returns at time scales of 1 or 5 minutes leads to crude approximations and to dramatic underestimation of the 
amount of risk really incurred’ (Malvergne and Sornette, 2006, page 36). 

4.1 FREQUENTIST VERSUS SUBJECTIVE PROBABILITY 
35 
• Even if I can increase the quantity of data by looking farther back in the past, as I do 
so I become less and less conﬁdent that what I have gathered is relevant to today’s 
conditions. 
• If I try to gather data at higher frequency than the prediction horizon I am interested 
in – assuming that I can – I have no conﬁdence that the results from the high-frequency 
data can be extrapolated to the regime I am interested in. The more so, the more I am 
interested in the tails. 
• The number of independent and relevant pieces of information therefore gives me an 
idea of the ‘rarity’ of events I can say something meaningful about from a frequentist 
point of view: with the 1000 daily relevant data points mentioned above, I can hope 
to say something (very imprecisely) about the 99.9th percentile of the underlying 
distribution of daily returns.  But I will  only be able to talk (again  very imprecisely) 
about the 93.75th percentile of the distribution of quarterly returns – and about the 
75th percentile of the distribution of yearly returns. 
This information is meaningful and useful. Unfortunately, it does not even begin to 
address the needs of capital allocation: what I can measure has little direct relevance to 
prudential capital; what has relevance for prudential capital, I cannot measure (at least 
within a frequentist framework). 
What can we do then? Are we lost in the mist of Knightian uncertainty? Perhaps by 
making use of a subjective approach we can push our boat one or two more orders of 
probabilistic magnitude beyond the safe frequentist shores. As I said in Chapter 1, I believe 
that beyond that there is a domain of extremely severe events for which no probabilistic 
assessment (subjective or frequentist) makes sense. But between the mundane losses for 
which frequentist techniques work reasonably well and Armageddon the hope is that perhaps 
we can patch together some reasonable additional probabilistic information. 
In my opinion this must come from the subjective input of market participants. This 
subjective input is, of course, informed by what happened in the past – where else can our 
beliefs be grounded in? But our understanding of how the world works could make us think, 
for instance, that events in 1930–1931 may be more relevant to the economic climate of 
2009–2010 than, say, what happened in the 1960s, or even in 2006. Or, again, I don’t know 
whether the ‘money glut’ or the ‘savings glut’ explanation of what went wrong in 2007 
(and of what we need to get out of the mess) is more correct. As a risk manager my task is 
not to take sides between neo-classical and neo-Keynesian economists, but to acknowledge 
that both views are plausible, and that the outcome produced by both views of the world 
(say, about future inﬂation, or the occurrence of more banking failures) should be given a 
non-negligible subjective probability in my scenarios. See the discussion in Chapter 3 about 
competing models of reality. 
Sometimes markets provide their own subjective assessment of the likelihood of future 
events. Implied volatilities, for instance, can sometimes respond to new events much more 
quickly and intelligently than the best GARCH model is able to. Indeed, if you believe 
in efﬁcient markets, market prices provide the best estimate of future everything. Now, 
in the wake of the recent crisis few theories have taken more serious knocks than the 
efﬁcient-market hypothesis. However, only a fool would totally neglect the ‘implied’ infor-
mation contained in spreads, volatilities, prices, etc. Disentangling the risk premium from 
expectations – say, in credit spreads – is, of course, not easy; but, again, when spreads move 

36 
CHAPTER 4 PROBABILITY IN RISK MANAGEMENT 
from 200 to 1200 basis points, the question of whether risk aversion accounts for 40%, 50% 
or 60% of the increase is really neither here nor there. 
In short, asking for ‘expert judgement’ means asking the risk manager to quote the odds 
for a bet for which she does not know whether she will be the bookie or the punter. If the 
nature of the bet is such that frequentist methods are appropriate (if the bet is of the coin-
tossing variety), it would be crazy for the risk manager not to make large use of frequentist 
techniques. If, however, the bet is of the President-of-the-US variety, and I were forced to 
advertise my odds, I would look at many sources of information, but not at the frequency 
of past women Presidents in the US. 
Providing ‘expert judgement’ is not easy, and there is a large literature that highlights 
the systematic biases we make when we make forecasts. I deal with this topic (i.e., how to 
be aware of human decisional biases and how to try to correct for them) in Chapter 12. But, 
again, we should not be discouraged by the difﬁculty and the unfamiliarity of the task. After 
all, if our probabilistic assessments turn out to be within the correct order of magnitude, 
given the rarity of the events at hand, it would still be a great feat. 
4.2 Tail Co-dependence 
All I have said so far pertains to the estimation of the probability of occurrence of a single 
rare event. However, the events we are interested in when we deal with stress testing arise 
from the interaction of a series of concomitant factors. Say ‘interaction’ and ‘stochastic 
variables’, and correlation is the word that springs to mind. Indeed, the concept of cor-
relation is very popular, but it has very strong limitations. For our purposes, its greatest 
shortcoming is that it is in no way synonymous with co-dependence. (By the way, even 
co-dependence is not the be-all-and-end-all of our understanding of how different variables 
interact, because it is still ultimately rooted in an associative, rather than causal, description 
of complex phenomena.) Let us take matters in order, however, and try to understand ﬁrst 
why correlation is so popular. 
Let us start from its nicest features. Correlation is one of the few measures of dependence 
that can be nicely decomposed into a simple pairwise statistic. When we are dealing with 
many variables and we talk about correlation we are implicitly saying that each variable 
inﬂuences, and is inﬂuenced by, each of the other variables in a simple pairwise fashion. This 
is computationally, and cognitively, great, but it should not make us believe for a second that 
the mathematical structure we are imposing on the phenomenon at hand should be reﬂected 
in the way the world actually works. Whether A is positively or negatively correlated with 
B will often depend on what C does: so, for instance, a loosening of monetary policy may 
or may not have an effect on inﬂation depending on the level of unemployment or the 
demanding for borrowing. 
Even when a pairwise dependence does an adequate job at describing the phenomenon at 
hand, there is no a priori reason to believe that the strength of this pairwise co-dependence 
should be independent of the magnitude of the moves in the two risk factors. We can see 
why this may be the case by means of a little toy model that, for all its simplicity, contains 
in a nutshell some very interesting implications. 
As everybody remembers during a ﬁnancial shock, and forgets shortly thereafter, in 
periods of extreme market turbulence correlations among assets become more ‘polarized’, 
tending towards +1 or  −1. We can take this as a descriptive datum, to store in our data 

4.2 TAIL CO-DEPENDENCE 
37 
bank of curious pieces of information about the ﬁnancial markets. Or we may try to create a 
simple structural model to explain this phenomenon. The simple model may go a bit like this. 
We assume that the observed changes in asset prices are produced by the changes in 
a relatively small number of driving factors – plus the usual idiosyncratic component. Let 
us call ‘loadings’ the sensitivities of the price return processes to the various factors (say, 
news about company earnings, GDP growth, etc.) that determine the changes in the vector 
of stock prices. 
Now, suppose that there is a common factor (say, liquidity) that affects the returns 
on asset prices alongside all the other factors. In this toy model normal liquidity times 
are characterized by run-of-the mill realizations of the liquidity shock, and liquidity crises 
correspond to much larger draws from the liquidity shock distribution. For instance, we could 
model this by allowing the liquidity shock to be drawn from a jump-diffusion process, or 
from a more general and interesting Levy process, but nothing essential hangs on this. 
Depending on the relative magnitude of the ‘loadings’ on the various factors on the one 
hand and of the idiosyncratic terms on the other, the correlation among assets due to the 
common liquidity factor could be very small when liquidity times are normal – i.e., when 
the draws of the liquidity stochastic process are of non-exceptional size. However, periods 
of liquidity crisis are characterized by exceptionally large shocks to this common liquidity 
factor. When this happens the liquidity factor can account for the lion’s share of the moves 
of all the assets. The assets become much more correlated then in normal times. This is the 
ﬁrst feature that our little model can explain. 
Let us see what else we could do. It would also be plausible to build into the model 
the feature that different assets should have negative or positive loadings onto the liquidity 
factor (think, for instance, of Treasuries and emerging market bonds). These modelling 
choices would then cause risky assets to move together (south), and risk-less or safe assets 
(e.g., Treasury bonds and highly-rated supranational paper) to appreciate. This could give 
rise to the observed ‘close to +1 or  −1 correlations’ in extreme market conditions. We have 
explained another feature. 
One can be a little cleverer. The market perception of what constitutes a liquid asset can 
change with the severity of the liquidity crisis. As the events of 1998 show, for instance, 
in periods of extreme stress on-the-run Treasuries can become noticeably more in demand 
than almost identical off-the-run issues. So, the magnitude, and perhaps even the sign, of 
the loadings onto the liquidity factor could themselves be a function of the magnitude of 
the liquidity shock. We have managed to explain why co-dependence may be a function of 
the magnitude of the liquidity shock. Imagine what an interesting and complex empirical 
copula the data produced by this little model could generate.7 
For all this simplicity, this example already shows that a very complex co-dependence 
structure can be easily ‘implied’ by a very simple structural model. Suppose now that we 
examine the data produced by this toy model, but with no knowledge of how they had been 
generated. If our analysis were purely descriptive and relied exclusively on looking at the 
data,8 we could still, of course, calculate a correlation matrix from the output generated by 
this model, but the number we would come up with would not be particularly meaningful. 
Indeed, no single correlation number, large or small, would be adequate – and the interesting 
7Keep in mind this simple example, and its last ‘twist’ in particular, for when we come to the associative-
versus-causal description of phenomena presented in the last section of this chapter. 
8In Section 4.4, I describe a purely-data-driven approach as ‘associative’. 

38 
CHAPTER 4 PROBABILITY IN RISK MANAGEMENT 
one, i.e., the correlation that applies during periods of stress, will be present in our dataset 
only in a handful of points. The vast majority of the data will then refer to the ‘uninteresting’ 
(tame) correlation, and our overall estimate will be close to meaningless. 
The discussion above suggests that correlations allow us to move from marginal to joint 
distributions only for one very special (and tame) class of distributions, namely the elliptical 
ones. Elliptical distributions are nice to work with. Unfortunately, computational expediency 
has a very limited impact on the way the world actually works, especially in the tails. If 
what we are interested in are joint probabilities, the ‘correct’ way to go from marginal 
to joint distribution is, of course, via copulas (see, e.g., Nelsen (1999) or McNeil, Frey 
and Embrechts (2005)). But the interesting differences between most copulas are in the tail 
behaviour, where, again, the relevant data points are rare, and often (when ‘ancient’) of 
dubious relevance to today’s conditions.9 
There is more. As we have seen, correlations do not allow us in general to go from 
marginal to joint probabilities. And as for copulas, theoretically they provide the ‘correct’ 
solution to this problem, but in the tails they give rise to some pretty fundamental calibration 
problems. But let us leave all these caveats aside and assume that, somehow, we have 
managed to conjoin our marginals in the cleverest possible way and have gained access to 
the joint probabilities. I will argue below that, devoid of a model ‘of how things work’, 
even these supposedly all-informative joint probabilities fall short of providing a cognitively 
resonant tool for risk management and stress testing. This brings me to the distinction 
between associative and causal descriptions of reality that I deal with in Section 4.4. 
4.3 From Structural Models to Co-dependence 
I have argued in the ﬁrst part of this chapter that a frequentist view of probability – a view, 
that is, that purely relies on data and makes little or no use of an underlying structural model 
(however simple) – is unlikely to be of much use in assessing the (marginal) probability of 
stand-alone extreme events. This does not mean, however, that nothing can be done: and 
this is exactly where subjective probabilities come in. 
Subjective marginal (stand-alone) probabilities for stress events can proﬁtably combine 
data information with our models of how the world works. If we try to ascertain probabilities 
of events that are too rare, the task remains near impossible – this is the Keynesian uncer-
tainty domain. But there is a useful range of still-remote-but-not-so-extremely-rare events 
about whose stand-alone probability of occurrence something can be said. Yes, I argue in 
this book that it pays to be humble and modest in our approach. Humility and modesty, 
however, should not prevent us from providing some order-of-magnitude estimates for the 
probability of their occurrence. Doing much more may just be unrealistic (or even hubristic). 
But doing less may mean throwing away some very useful pieces of information. In the 
approach I propose I will therefore often ask the risk manager to estimate as best she can 
at least the order of magnitude of the marginal probabilities. 
9Indeed, I employed copulas a few years ago with a student of mine (Shamrakov, 2006) in order to explore 
the dependence between credit spreads and equity prices for a large number of ﬁrms, of different initial credit 
ratings. As we were interested in tail co-dependence I recall that, if we had had to rely purely on the data, the 
choice of one copula over another would have been close to arbitrary. Once again, what provided great help was 
supplementing the raw data with a simple, Merton-like structural model. The choice of this model, needless to 
say, introduced an important subjective element to our data analysis. 

4.4 ASSOCIATION OR CAUSATION? 
39 
What can we do then? I believe that we may be able to go a long way by integrating 
stand-alone probabilities with simple models of reality, such as, for instance, the liquidity 
toy model sketched above. The point of the rather lengthy description given above of this 
little model was not its speciﬁcs, but the realization that a simple understanding of how the 
world may work can take us a long way towards implying a co-dependence structure when 
its estimation from the raw data, for any of the reasons above, is not feasible. This is why 
I will present in the rest of the book ways to integrate some carefully chosen (marginal or 
conditional) probabilities for the stand-alone events with our ‘world models’. 
As a by-product I will obtain joint probabilities (or bounds for them). These probabilities 
will allow us to escape Berkowitz’s purgatory, and to treat the output of stress testing on a 
comparable basis with the output from VaR & Co. If we want to use stress testing to calcu-
late capital, having an idea of the order of magnitude of the various stress losses will prove 
invaluable. However, obtaining joint probabilities should not be seen as our end result. Build-
ing, using, reﬁning and, when necessary, throwing away a causal structure between the vari-
ables at play (a model) will provide a much more powerful tool for risk management action. 
Of course, this means that, in Bayesian language, I advocate a far greater role in my 
approach for my prior than is allowed for by the statistical analysis typically used in a risk 
management context. A greater relevance of the prior in turn means that we have to resort 
to a (more) subjective interpretation of probability. Yes, this does also mean a departure 
from ‘objectivity’, but this should be welcomed and not regretted, as pure objectivity in any 
scientiﬁc endeavour is, in my opinion, a pious myth.10 
The rest of this book will therefore unapologetically require the risk manager to provide 
her subjective probabilities where appropriate – but, far more importantly, her tentative 
understanding of how the world works. Doing so in a logically coherent manner is not easy. 
Bayesian nets will provide my tool of choice in carrying out this programme. 
Bayesian nets are often interpreted as computational devices to obtain in a parsimonious 
and ‘frugal’ manner a difﬁcult set of quantities (the joint probabilities), from simpler building 
blocks (marginal and conditional probabilities). However, looking at Bayesian nets simply 
as a handy book-keeping device to obtain joint probabilities would be a missed opportunity. 
Such a reductive view neglects the distinction between associative and causal descriptions 
of a complex phenomenon. It is to the discussion of this important aspect that I therefore 
now turn. 
4.4 Association or Causation? 
A purely probabilistic approach to risk management (and, for that matter, to the analysis 
of any complex phenomenon) places all the emphasis on the association among variables, 
rather than on the causal links among them. This is unsatisfactory on several accounts. 
First, assigning links among variables on a causal rather than associative basis is cog-
nitively much easier and more ‘natural’. To see what this means, consider the following 
example, modiﬁed from Pearl (2009). Suppose that the variable in whose prediction we 
are interested is whether a garden pavement will be slippery or not. The other variables in 
10In Plight of the Fortune Tellers (2007) I describe a case where lack of one’s prior would induce a Martian to 
make a grossly wrong guess about the biasedness of a coin even in a simple coin-tossing experiment. So important 
is this example that I toyed with the idea of calling that book Coin-Tossing Martians and the Next President of the 
United States . Fortunately, everyone, from my editor to my wife, disabused me of the idea. 

40 
CHAPTER 4 PROBABILITY IN RISK MANAGEMENT 
this baby problem are the season, whether it rained or not, whether the pavement is wet 
and whether a sprinkler was on or off. We could take a purely associative approach, and 
build all the probability tables required. As we shall see in Chapters 8 and 9, these would 
require the speciﬁcation of stand-alone probabilities (such as probability of rain tomorrow), 
and conditional probabilities. These, in turn, could be very simple and ‘natural’ (such as the 
probability of rain tomorrow, given that it is autumn), or very complex and ‘awkward’ ones 
(such as the probability that the sprinkler is off, given that it is autumn, that the pavement is 
not slippery and that it has been raining). Assigning some of these probabilities can in some 
cases feel like answering a well-posed question, but in other cases we seem to be ‘working 
against the cognitive grain’, and we are faced with difﬁcult, puzzling or even paradoxical 
probabilistic assignments. 
From the point of view of associational relationships, there is nothing wrong with what 
we are trying to do (ﬁlling the conditional probability tables) – in particular, there is nothing 
to distinguish the ‘natural’ from the ‘awkward’ assignments. Now, if we are using a purely 
frequentist approach, we just collect data and calculate all the conditional probabilities from 
our dataset. But what if we have to provide subjective probabilities? If this is the task at 
hand, our cognitive aptitude at dealing with some questions rather than others does make 
a big difference. What is happening here? Why are we ﬁnding some questions easier to 
answer than others? 
The root of the problem lies in the fact some of the assignments we are requested to 
make invoke a causal link among variables, and others are linked by a subtler (‘diagnostic’) 
kind of relationship. It so happens that our mind works much more effectively in a causal 
rather in an associative mode.11 When we try to answer difﬁcult questions by taking a purely 
probabilistic (associative) approach we are therefore not doing ourselves any favours. The 
results of this analysis will also be cognitively very difﬁcult to use. 
‘Awkward’ and ‘natural’ conditional probabilities do not only make a difference when 
we assign them subjectively. The associative link between variables ‘contained’ in a purely 
probabilistic description can make our interpretation difﬁcult, and this can be true even 
when we have plentiful and relevant data. If understanding , rather than just analysing, the 
output of complicated analysis is our ﬁnal goal, this cannot be satisfactory. 
Fortunately, an associative approach is not the only possible one. In the sprinkler example, 
we can follow a different approach and create a simple causal model of ‘garden reality’, 
whereby 
• the season of the year affects the probability of rain and of the sprinkler being on or 
off (but is not affected by either);12 
• the status of the sprinkler (on or off) and whether it rained or not affects the probability 
of the pavement being wet (but sprinkler status and occurrence of rain are not caused 
by the wetness of the pavement); 
• the wetness of the pavement affects its slipperiness (but the slipperiness does not 
‘cause’ wetness). 
11See Pearl (2009), Chapter 1, and references therein on this point. 
12Of course, whether it is summer or autumn is not affected by the pavement being wet or not. However, 
knowledge that the pavement is wet does change my probability assessment of the season being autumn. When 
we work from the season (autumn) to the probability of the pavement being wet we are working in a causal mode. 
When we work backwards from the state of the pavement we are working in a diagnostic mode. See the discussion 
in Chapter 12. 

4.4 ASSOCIATION OR CAUSATION? 
41 
All of a sudden, given this model , certain conditional probabilities make a lot more 
sense, and appear a lot more ‘natural’ than others. For instance, the conditional probability 
of the pavement being wet given the season of the year (via the mediated ‘causes’ of rain 
and sprinkler) appears ‘natural’ and cognitively sensible to assign. In particular, it appears 
natural and sensible in a sense that the conditional probability of the season being springtime, 
given that the pavement is slippery but not wet, is not. Yet, if we take a purely associational 
approach, there is nothing to distinguish between the two probabilities – there is nothing 
to indicate, that is, that the former probability naturally falls out from a causal appreciation 
of how the world works, while the latter requires our mind to work ‘backwards’.13 As Pearl 
(2009) eloquently puts it: 
[I]f conditional independence judgements are by-products of stored causal relationships, 
then tapping and representing those relationships directly [is] a more natural and more 
reliable way of expressing what we know or believe about the world. This is indeed the 
philosophy behind causal Bayesian nets. 
The second reason why causal models (and causal interpretations of Bayesian nets) are 
more powerful than associative ones is that simple modiﬁcations of the causal link among 
variables can give rise to large and ‘mysterious’ changes in association (and in measures 
of association, such as correlation) and in joint probabilities. Suppose, for instance, that the 
world changes a little (and, by the way, when it comes to the ﬁnancial world, it tends to 
change a lot rather than a little). In our small garden world, suppose that we have installed 
a clever sprinkler, which does not come on if it is already raining. 
It is very easy to encode this change in our chain of causal links among variables (we 
shall see how to do it with Bayesian nets in Chapter 8, but the reader can go with the ﬂow 
and imagine the gist of the construction). However, if we look at the problem from a purely 
associative point of view, i.e., just by looking at the changes in marginal, conditional and 
joint probabilities before and after the installation of the clever sprinkler, we can be very 
puzzled. After the installation of the clever sprinkler, we would observe, for instance, that 
some of these probabilities have changed a lot, and others not at all. But why these and not 
those? In general we will observe that the probabilities have changed in a way that, if we 
do not know what caused what, can be both complex and difﬁcult to make sense of – this 
is what I meant above when I used the term ‘mysterious’. 
Pearl (2009) addresses a closely related point when he writes: 
[C]ausal models (assuming they are correct) are much more informative than proba-
bility models. A joint probability tells us how probable events and how probabilities 
would change with subsequent observations, but a causal model also tells us how these 
probabilities would change as a result of external interventions.14 
All of this is of great relevance when it comes to our attempts to understand the ﬁnancial 
world for risk management purposes in general, and for stress testing in particular. The 
frequentist approach that has been at the core of the reigning paradigm has studiously 
eschewed all causal information. Association without causes has been the modelling structure 
13As we shall see, ‘backwards’ in this context means in a diagnostic rather than a causal mode. 
See Chapter 12. 
14Pearl (2009), page 32, my emphasis. 

42 
CHAPTER 4 PROBABILITY IN RISK MANAGEMENT 
of choice: think, for instance, of how the calculation of the VaR statistics works when 
the historical simulation method is employed. Pure juxtaposition (association) without any 
overarching structure lies at the core of its modus operandi. This is the price that has been 
paid in the pursuit of objectivity – and a very high price indeed it has been. 
Now, when the relationships among variables are stable and we can observe them for a 
long time,15 the associative approach can be inefﬁcient, but we can still have some hope of 
saying something about the variables of interest (in our garden world, whether the pavement 
is slippery). But when it comes to stress testing, the paucity of the relevant observations 
and the mutability of the causal relationship between them makes an approach based on 
association (i.e., on probabilities without causal links) close to hopeless. This is why the 
provision of causal information is so fundamentally important for stress testing. 
The link between causal descriptions and the interpretative models of reality discussed 
in the previous chapter should be evident. From Chapter 3 to Chapter 4 my argument has 
begun to ‘rotate’ from a purely discursive and qualitative approach to the somewhat more 
precise observations about the differences between the causal and probabilistic relationships 
I have just presented. In the rest of the book the ‘rotation’ of my argument from the 
discursive and qualitative to the quantitative will gather pace in Part II. This is because 
some mathematical formalization and a reasonable degree of precision are indispensable to 
turning a ‘good idea’ into an operative blueprint for action. However, the importance of a 
qualitative understanding of the issues at play should never be underestimated, and should 
remain with the reader throughout the book. 
So, for instance, a lot of effort will be devoted to showing how to go from conditional 
and marginal probabilities to the joint distribution. Indeed, I will often refer to this latter 
quantity as the probabilistic Holy Grail. However, one should never lose sight of the fact 
that causal links are primary, richer, more ﬂexible and more informative than probabilities 
(joint or otherwise). And that joint probabilities may well be ‘king’, but only from a purely 
associative perspective. 
4.5 Suggestions for Further Reading 
So vast is the literature on the Bayesian versus frequentist debate, that it is virtually impos-
sible to give a comprehensive list of ‘essential readings’. For an eloquent, impassioned and 
uncompromising defence of the Bayesian view of the world the reader can refer to Jaynes 
(2003). A reviewer hailed the book as ‘the most important contribution to the problem of 
induction since Aristotle’. As another book in my list of suggested readings (Williamson, 
2005) has been given a virtually identical accolade, we must be living in a particularly 
productive decade. Hyperbole aside, Jaynes’ book is very clear and certainly inspiring, but, 
at times, almost strident. For a more balanced view, see Poirier (1995). 
Moving to more technical topics, see Malvergne and Sornette (2006) for a good discussion 
of the difﬁculty in ﬁtting tail distributions. An excellent and concise introduction to Extreme 
Value Theory and copula theory can be found in McNeil, Frey and Embrechts (2005). For 
its combination of concision, clarity and precision this is probably the best book I am aware 
of about quantitative risk management. A good discussion of parametric, non-parametric 
15In the language of the previous sections, this means when the data are relevant and plentiful. 

4.5 SUGGESTIONS FOR FURTHER READING 
43 
and semi-parametric approaches to Extreme Value Theory can be found in Malvergne and 
Sornette (2006), especially in their Chapter 2. 
The important distinction between association and causation is discussed very well in 
Pearl (2009). 
Finally, I deal in more detail, but still in a non-technical manner, with some of the topics 
of this chapter in Rebonato (2007). 


Part II 
The Probabilistic Tools and 
Concepts 


Chapter 5 
Probability with Boolean 
Variables I: Marginal and 
Conditional Probabilities 
5.1 The Set-up and What We are Trying to Achieve 
This chapter does not attempt to provide a systematic introduction to probability theory. The 
reader is expected to be already familiar with the fundamentals.1 My tasks are to explain in 
the simplest possible way a selected number of topics and ideas – some elementary, some 
slightly more advanced – that we shall use in Part III; to present the notation that will be 
used in the rest of the book; to introduce some graphical tools (Venn diagrams) that are very 
useful for thinking about conditional probabilities; and to give the simplest self-consistent 
framework that can allow the reader to think coherently about marginal, conditional and 
joint probabilities. I have made no attempt to either rigour or generality.2 
After reading this and the next chapter the reader should be able to understand why, if 
she had the joint probability distribution among n variables, she would know everything 
she can probabilistically say about them; and also why obtaining this quantity is so difﬁcult. 
Readers who have followed the qualitative discussion about co-dependence that closed the 
previous chapter should already have an idea of why that is the case. This difﬁculty will 
motivate the topic of Chapters 6 to 8, which can be summarized as follows: under what 
conditions can we make the task of obtaining the joint distribution more tractable – and, 
more speciﬁcally, sufﬁciently tractable to be of use in stress testing. 
1See Section 4.5 (Suggestions for Further Reading) for some recommendations about introductory and 
intermediate-level texts in probability theory well suited to my treatment. 
2For instance, I will only deal with a ﬁnite number of variables, and I will allow them to assume only one of 
two possible values. Some of the extensions (such as allowing for more than two possible realizations) would be 
simple enough others would take us on a long detour. 
47 

48 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
When it comes to scenarios, very often a very important question is: what is the 
probability of this scenario happening? Let me make the question a bit more meaningful.3 
Consider ﬁrst a precise scenario (e.g., an upward move of a yield curve by 50 basis points 
or more).4 I call this scenario event E. I can then ask the question: ‘What is the probability 
that this event will happen over the next week?’ After the observation period has elapsed 
(in this case, one week) we will know for sure whether the event happened or not, but 
‘today’ there is some degree of uncertainty – however small – about the outcome. If this 
is the case, E is a two-valued Boolean random variable, where ‘Boolean’ means that it 
can assume values ‘true’ (T ) or ‘false’ (F ), and two-valued means just what it says. It is 
important to understand that the random variable we are talking about does not assume a 
numerical value (say, 50 basis points), only a logical value (T or F ). Strictly speaking, if the 
move in the yield curve turned out to be 49 or 51 basis points, then event E did not occur. 
That is why we will have to be careful when deﬁning the events. More about this later. 
A large part of this book will deal with assigning probabilities (marginal, conditional, 
joint) to events, that is, to two-valued Boolean variables (or, more shortly, Boolean variables 
or even variables when no ambiguity can arise). The treatment below is tailored to this task. 
Before getting started it is useful to have a map of the road ahead for the present and the 
next chapter. As I said, the probabilistic Holy Grail is gaining access to the joint distribution. 
Unfortunately, even all the marginal and (singly-conditioned) probabilities do not allow us 
in general to gain access to this quantity. They will not allow us, that is, to answer some 
questions such as: ‘What is the probability of the ﬁrst two scenarios materializing but the 
third and fourth not coming true?’ Ideally, this would be just the information we would 
like to have because Nature will not produce scenarios (if she does) one at a time, but in 
a combination of her liking. If we want to associate some capital to these joint events, 
or more simply, if we want to have an idea as to whether we should worry or not, we 
would like to have at least an order-of-magnitude idea of the probability of occurrence 
of the joint scenarios Nature will put in front of us. Being able to answer – even in an 
approximate manner – questions such as the one above requires knowledge of quantities 
informationally much richer, but correspondingly more difﬁcult to obtain, than marginal or 
singly-conditional probabilities, that is, of the full joint distribution. 
I cannot stress how difﬁcult it is to estimate a joint distribution when the events we 
are looking at are very rare. Even estimating the stand-alone probability of one scenario in 
isolation is a daunting task, which for truly ‘stress’ events, I do not believe can be accom-
plished with better-than-order-of-magnitude precision. But what is a well-nigh impossible 
task is estimating purely from ﬁnancial time series information about tail co-dependencies. In 
Chapter 4 I explained why this is the case. Only the knowledge of these tail co-dependencies 
(not correlations!5) would allow us to go from the marginal to the joint distribution. 
Should we throw up our hands in despair then? Perhaps not. If we are prepared to add to 
the purely frequentist information our understanding of how the world works (or might work) 
we can turn an impossible task into a very difﬁcult one. Where can this ‘extra information’ 
3We will have to be a lot more precise than this (see Section 9.2). 
4Note that saying ‘an upward move of a yield curve by 50 basis points’ does not make a lot of sense, or 
is not what we probably meant in the ﬁrst place. If we think of ‘50’ as a real number, the probability of ‘an 
upward move of a yield curve by exactly 50 basis points’ is zero. Even if we recognize that basis points move 
in discrete increments, a move by exactly 50 basis points (rather than 49 or 51) is not what we probably had in 
mind. Specifying clearly the event is important. For the moment, however, we just go with the ﬂow. 
5As I pointed out in Chapter 4, correlations allow us to go from the marginals to the joint distribution only 
in the case of elliptical distributions. In general, there is no reason to believe that, especially in the tails, the 
dependence among n variables should neatly decompose into pairwise dependencies. 

5.1 THE SET-UP AND WHAT WE ARE TRYING TO ACHIEVE 
49 
come from? Correlations (and, for that matter, also conditional probabilities) convey no 
information about causal relationship.6 But surely we often have some ideas of what causes 
what. A dramatic fall in the S&P is more likely to cause an increase of equity volatility 
than the other way around. (Of course, some other cause, such as the default of a major 
bank counterparty, may cause both the S&P to fall and equity volatility to increase – but 
also in this case we have an idea of what caused what). 
Bayesian nets (or Bayesian networks, or acyclical directed graphs if you prefer difﬁcult 
words) are a very powerful and intuitive technique to systematize our knowledge about 
causal relationships among variables and translate it directly into probabilistic information. 
It may all sound very abstract now, but I will show in Part III how to turn these lofty 
words into a concrete action plan. 
So, here is the game plan: 
• Our ﬁnal goal is to gain access to the joint distribution. 
• Getting there directly is too hard. 
• Therefore we start from marginal (stand-alone) probabilities and conditional probabil-
ities of very low order.7 
• We realize that these are in general not enough to get us what we want (the joint 
distribution). 
• We therefore inject information about how we expect the world to work. 
• Given this extra information, the tool that gives us hope to get from what we can 
laboriously estimate to the joint distribution we truly need are Bayesian nets. 
• The simpler the structure we allow for our Bayesian net, the fewer (and simpler8) the  
conditional probabilities we will have to supply. 
• When we feel that providing some conditional probabilities is too difﬁcult, and our 
causal structure does not pin down the joint probabilities uniquely, there still are some 
very useful bounds that can be found. 
6One may say that same-time across-asset correlations convey no information about causal relationships, but 
time lagged time series do. Indeed, there is a whole ﬁeld of econometrics (Granger causality) devoted to uncovering 
causality from lagged time series. See, e.g., Granger (1969) and Hacker and Hatemi-J (2006) for an extension 
to non-normal variates and for a more modern treatment that uses the unit-root approach to time series analysis. 
In principle, the idea is simple: suppose we have two time series of changes, x and y, and that we want to 
explore whether x Granger-causes y. First, we do a regression of y on lagged values of y. If some lag values 
are found to be signiﬁcant, we carry out further regressions on various lags of x. These lags are added to the 
regression if they are signiﬁcant on a stand-alone basis, and if they increase the explanatory power of the model. 
There are two problems with this, one of a practical and one of a theoretical nature. The practical problem is 
that ﬁnancial markets tend to adjust so quickly that, unless one deals with high-frequency data, it is difﬁcult to 
ﬁnd stable, robust and convincingly non-zero time-lagged correlations. 
The second problem is that, even if we could detect such a lagged correlation, without a model of reality there 
would be no telling whether some other cause (not captured in our data series) might not have caused the leading 
variable, and so, indirectly caused both the leading and the lagged indicator. 
7The probability of A given that I know that B has happened (denoted by P (A|B)) is  a  singly -conditioned 
probability, because I am availing myself of the knowledge that only one event (B in this case) has happened. 
The probability of A given that I know that B and C have happened (denoted by P (A|B, C)) is  a  doubly -
conditioned probability, because now I know that two events (in this case B and C) have happened. 
I call conditional probabilities of very low order at-most-doubly-conditioned probabilities. 
8Where by ‘simpler’ I mean ‘of low order’ in the sense explained in the footnote above. 

50 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
• An example of the type of ‘external information’ that we will have to supply is about 
independence (a very strong and, in ﬁnance, very rare condition) and conditional 
independence. These are the scissors that will allow us to prune the branches of the 
Bayesian net and, hopefully, give us access to the joint distribution. 
• When we have this, we have everything. 
5.2 (Marginal) Probabilities 
Consider an event E. Following the excellent and simple treatment by Moore (2001), I am 
going to deﬁne the (marginal) probability of E being true (P (E  = T ), or more simply, 
P (E)) as the ‘fraction of the possible worlds in which event E occurs’.9 As Moore says: 
We could at this point spend two hours on the philosophy of this. 
But we won’t. 
I will instead give a graphical representation of this probability. Recall that E is a Boolean 
variable. Therefore the ‘fraction of the possible worlds in which event E occurs’ means the 
‘fraction of the possible worlds in which Boolean variable E assumes the value T ’. We can 
visualize this as shown in Figure 5.1. There are two sets: one denoted by E = T , the other 
by E = F . The set E = F is made up only of the cells shaded in grey. The grey cells do 
not ‘extend under’ the white cells. There is nothing to be read in the fact that the set E = T 
is all ‘inside’ set E = F . Exactly the same information would be conveyed by Figure 5.2. 
The only thing that matters is the relative size of the two sets. 
Recalling the deﬁnition of the probability of E (being true) given above, we can have a 
graphical representation of how likely E is as follows: 
{E = T }
P (E)  = 
(5.1)
{E = T ∪ E = F } 
In the expression above the use of curly brackets {A} denotes the size (in some units) of a 
set A. If you are drawing your Venn diagrams in Excel, the size can be taken to be equal 
to (rather than approximated by) the number of cells in each domain. 
E = TRUE 
E = FALSE 
Figure 5.1 The ‘fractions of the possible worlds’ where event E is true. 
9Why ‘marginal’? Because marginal distributions are obtained by ‘integrating out’ the variables one is not 
interested in. For a two-dimensional case, where the discretized joint distribution can be represented as a table, 
the marginal distribution appears as a sum over rows or columns at the margin of the table – hence the name. 

5.2 (MARGINAL) PROBABILITIES 
51 
E = TRUE 
E = FALSE 
Figure 5.2 An equivalent representation of the ‘fractions of the possible worlds’ where event E 
is true. 
To make things simpler, I am now going to normalize the union of set E = T and set 
E = F to 1: 
{E = T ∪ E = F } = 1 
(5.2) 
We can simplify our notation further as follows: instead of denoting the set of ‘worlds’ 
where E is true by E = T , I will simply denote it by E. What shall I do with the set of 
‘worlds’ where E is not true? I will denote this set by E˜. 
Let us now link the size of the sets we have associated with different events with their 
probability of occurrence. 
To start with, if we are certain that E will occur, its (ex-ante) probability is 1. If we 
are certain that it will not happen, its (ex-ante) probability is 0. It is then reasonable to 
introduce the following axioms: 
Axiom 1 
For any event E. 0  ≤ P (E)  ≤ 1 
Axiom 2 
For any event E. if  {E = T } = 1, then P (E)  = 1 
Axiom 3 
For any event E. if  {E = T } = 0, then P (E)  = 0 
We need another axiom that involves two events, E and F. Consider Figure 5.3. To save 
ink, the set E now denotes what I called above set E = T . The two sets intersect, that is, 
there are possible worlds in which both E and F happen. The probability of both events 
happening, denoted by P (E ∩ F), is given  by  
P (E ∩ F) = {E ∩ F} 
(5.3) 
(Remember that the size of the whole set of all the possible worlds has been normalized 
to 1.) As we shall see, this intersection set is central to the treatment that will follow, but 
for the moment we still have to gather a few tools. 
I can now introduce the last axiom, which relates to the probability of either E or F, 
denoted P (E  ∪ F), happening: 
Axiom 4 For any couple of events E and F, P (E  ∪ F) = P (E) + P (F) − P (E  ∩ F) 
If we look at Figure 5.4, this is ‘obvious’: if I added P (E) and P (F) I would be double-
counting the area {E = T ∩ F = T }, which is therefore duly subtracted. Recall, however, 

52 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
E 
F 
E∩F 
Figure 5.3 The area of overlap denotes the intersection of sets E and F. 
E ∩ F 
Figure 5.4 ‘Explaining’ the axiom P(E ∪ F) = P(E) + P(F) − P(E ∩ F). 
that an axiom, however ‘plausible’ or ‘obvious’, is just that: an axiom. We must always 
make the graphical depiction reﬂect the axioms, not the other way round. 
In particular, from Axiom 4 it follows that if two events E and F can never happen 
together – i.e., if they are incompatible – then the probability of either event happening is 
given by: 
P (E  ∪ F) = P (E) + P (F) 
(incompatibility) 
(5.4) 
But this also implies (again by Axiom 4) that 
P (E ∩ F) = {E ∩ F} = 0 
(5.5) 
Therefore the graphical representation must look something like what is shown in Figure 5.5: 
the two sets must have no points in common: when either event happens, the other one 
never does. 

F
5.3 DETERMINISTIC CAUSAL RELATIONSHIP 
53 
E 
Figure 5.5 The sets E and F when the two events are incompatible. 
Exercise 1 
Prove using the axioms above or Venn diagrams that, for any E, P (E)  + 
P (  ˜E) = 1. 
5.3 Deterministic Causal Relationship 
In order to get some familiarity with these deﬁnitions and graphical constructions, let us ask 
ourselves the question: how would we draw the case where there is a perfect, deterministic 
causal relationship10 between E and F, in the sense that E always and invariably causes 
F 11? If  E deterministically causes F, then there are no possible worlds where E happened 
and F did not happen. Now, F may be caused by other events as well. So, to begin with, 
{E} must be smaller than or equal in size to {F}. (Why?) But we can say more. The set 
E must be totally contained within the set F: a point of E not contained in F means a 
possible world where E can happen but F does not. But, if E deterministically causes F, 
this cannot occur. So, the picture must look something like Figure 5.6. By the way, given 
the way this picture has been drawn there are many more causes (although not necessarily 
deterministic causes) for F other than E. Event E may be the only deterministic cause. 
A very important caveat: if E deterministically causes F, then the set E must be totally 
contained within the set F, but the converse is not true; that is, it is not true that if the set E 
10We often say things such as: ‘Smoking causes lung cancer’. But this is not an instance of deterministic 
causation; not because there are other causes of lung cancer, but because some smokers do not get lung cancer. 
11I deﬁne the term ‘deterministically cause’ to identify the situation when the occurrence of one event is 
invariably associated with the occurrence of another event. The deﬁnition is a bit misleading, because perhaps A 
does not cause B in our intuitive undertstanding of the word ‘cause’, in that there may be some other event that 
causes both A and B. For instance, we may say that the default of a major bank counterparty ‘causes’ spreads to 
widen. But a nuclear bomb dropped on Wall Street would cause both a major counterparty to fail, and spreads to 
widen. Even if there are no states of the world where a major bank counterparty fails without spreads widening, 
the former event may not necessarily be the cause of the latter. Causation is an extremely complex topic, and 
philosophers have been at it for about 2500 years. Whenever I speak of causation in this book I steer clear of 
the subtle but important caveats that come with a proper treatment of causation. To make clear that I am dealing 
with ‘causation-lite’ sometimes I write ‘causation’. Most of the time I do not feel this is necessary, but the reader 
should keep this caveat in mind. 
Thinking clearly about what causes what, and about proximate and prime causes, is very important in order 
to organize stress tests in a coherent manner. This is one of the areas where Bayesian nets can help us. See 
Williamson (2005) for a treatment that deals head on with the topic of ‘proper’ causation, and Pearl (2009) for a 
good discussion of the topic. 

 
 
54 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
E 
F 
Figure 5.6 The sets E and F when E deterministically causes E. 
is totally contained within the set F, then there must be deterministic causation from E to F. 
To see why this is the case, consider an event, G. When G happens, sometimes, but not 
always, F happens. But F happens, when it does, only when G happens. Then the set F is 
completely contained in the set G, but we would certainly not like to say that ‘F caused G’. 
To see more clearly what this means in terms of stress events, suppose that G represents 
the occurrence of a run on a major bank. Let F be the event ‘The Fed takes over a major 
bank’. Assume, for the sake of argument, that the Fed will be moved to nationalize a major 
bank only if there has been a run on this bank, but will not necessarily do so. Then, clearly, 
the set F is a proper subset of the set G, but we would not want to say that the rescue 
operation by the Fed (F) has caused the run on the bank (G)! So, the condition F ⊆G is 
a necessary but not sufﬁcient condition for deterministic causation.12 
Exercise 2 
How would you depict by means of Venn diagrams the situation where F has 
two deterministic causes, either E or G, and  E and G can never happen together? 
Exercise 3 
How would you draw the situation where F has two deterministic causes, 
either E or G, and  E deterministically causes G? 
The two exercises above begin to give us some familiarity with the graphical tools that we 
will use in Parts II and III. As we shall see, these pictures will help our reasoning immensely. 
To see their power even more clearly, try to prove the following theorem either using the 
axioms 1 to 4 above, or simply by drawing a Venn diagram and putting the pieces together: 
Exercise 4 
For any E and F, prove that 
P (E)  = P (E  ∩F) + P
E ∩F˜
If in solving the exercise above you found the mental colour-and-cut-and-paste-the-sets 
approach easier than the algebraic one (as I do), you should be convinced that Venn diagrams 
will help us in situations where the reasoning can become quite subtle. They will help us, 
12The notation F ⊆G indicates that the set F is a (not necessarily proper) subset of G. 

5.4 CONDITIONAL PROBABILITIES 
55 
that is, to think logically. This is exactly what we are going to need when we begin to deal 
with several scenarios. 
5.4 Conditional Probabilities 
Let us look again at Figure 5.3, which I have drawn again for ease of reference in Figure 5.7. 
The part of the drawing where most of the action takes place is in the intersection set, E ∩ F. 
Its area represents the fraction of possible worlds where E and F happen together. Since 
the set of all possible worlds has been normalized to 1, the area of this intersection set must 
be smaller than or equal to 1: 
{E ∩ F} ≤ 1 
(5.6) 
and, obviously, 
{E ∩ F} ≤{E} 
(5.7) 
{E ∩ F} ≤{F} 
(5.8) 
Now, given the normalization of the ‘size of the whole universe’ to 1, the probability of 
both E and F happening is clearly given by 
P (E  ∩ F) = {E ∩ F} 
(5.9) 
But what about the probability of both E and F happening given that we know that, say, F 
has happened? When we ask this question, the new universe we are interested in becomes 
the set of possible worlds for which F = T . This new reference universe gives the new 
normalization yardstick, and in this new metric we look at the fraction of possible worlds in 
which E is also true. We give a special name to this ratio, namely the conditional probability 
of E given F , which we denote by P (E|F): 
P (E ∩ F)
P (E|F) ≡{E ∩ F} = 
(5.10)
{F} 
P (F) 
E 
F 
E∩F 
Figure 5.7 The intersection set, E ∩ F, which must be normalized either by the size of E or by the 
size of F. 

 
 
56 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
Conversely and symmetrically, the probability of both E and F happening given that we 
know that E has happened is given by 
P (E ∩ F)
P (F|E) ≡{E ∩F} = 
(5.11)
{E} 
P (E) 
Note how the numerators ({E ∩F}, P (E ∩ F)) do not change: it is only the ‘normalization 
factor’ in the denominator that tells us that our ‘reference universe’, that is, what we assume 
to know to have occurred, has changed. 
The following table summarizes what we have deﬁned so far: 
numerator denominator 
ratio 
meaning 
{E ∩F}
{E}
{E ∩F} 
P (F|E)
{E} 
(5.12) 
{E ∩F}
{F}
{E ∩F} 
P (E|F) 
{F} 
{E ∩F}
{E ∪E˜} 
{E ∩F} 
P (E ∩ F)
1 
Given the deﬁnition above it is an easy task to show that 
P (E|F) + P
E˜|F = 1 
(5.13) 
Exercise 5 Prove Equation (5.13) using Venn diagrams. 
5.5 Time Ordering and Causation 
One important observation is in order. There is in general no notion of causality in the 
concept of conditional probability, and there is no notion of temporal sequence either. In 
general, it is not right to think of the probability of E given F as the probability that E is 
caused (deterministically or otherwise) by F. Nor is it right to think that ‘ﬁrst F happened, 
and now we are going to see the probability that E might follow’. If one looks at the 
Venn diagram in Figure 5.7, it is clear that it contains no time information, and that the 
‘conditioning’ only refers to a different normalization, not to time ordering or causation. 
So, strictly speaking, instead of saying ‘What is the probability of E happening given 
that F has happened?’ – which leads us astray with an unwarranted whiff of temporal 
sequence – one should say ‘What can I say about the probability of E happening given that 
I know that F has happened ?’ For our purposes, the best way to think about conditional 
probabilities is the following: the quantity P (E|F) gives us an indication of whether, and 
by how much, we are helped in predicting E given that we know that F has happened. 
Knowledge that a major bank counterparty has failed helps us (enormously) in predicting 
whether credit spreads have widened. Knowing that the summer–winter gas spread has 
widened does not help us in predicting whether a major bank counterparty has failed. In 
the former case, the conditional probability is (close to) 1; in the latter it is (close to) the 
unconditional probability (see Section 5.7). 

5.6 AN IMPORTANT CONSEQUENCE: BAYES’ THEOREM 
57 
In order to introduce causation and time ordering we need extra information that does 
not come from the Venn diagrams we have drawn so far. This information comes from 
our understanding (possibly faulty) of how the world works. This is where Bayesian nets 
come in. Causation and time ordering (what causes what, and what follows from what) are 
extremely important for stress testing. This should already give us an indication as to why 
probabilities (even conditional ones) – and, as we shall see, correlations – do not take us 
very far in this direction: marginal probabilities, conditional probabilities and correlations 
are the tools of the risk analyst who has no idea whatsoever of how the world works, that 
is, of the direction of causation. As we shall see, between a full understanding of how the 
world works, and no understanding whatsoever, there is the vast and useful grey area of 
partial and imperfect information that most humans inhabit. Again, this is where Bayesian 
nets come in handy – not by adding extra information that we do not have, but by helping 
us in thinking straight. 
5.6 An Important Consequence: Bayes’ Theorem 
A trivial rearrangement of the above equations can give us the single most important, and 
useful, result in this book. From 
P (E ∩ F)
P (E|F) ≡{E ∩ F} = 
(5.14)
{F} 
P (F) 
it follows that 
P (E|F) P (F) = P (E ∩ F) 
(5.15) 
Of course, we must also have 
P (E ∩ F)
P (F|E) ≡{E ∩ F} = 
(5.16)
{E} 
P (E) 
and 
P (F|E) P (E) = P (E ∩ F) 
(5.17) 
Since the right-hand side of Equations (5.15) and (5.17) are the same (the size, {E ∩ F}, of  
the intersection set does not change!), the same must be true of the left-hand side, and so: 
P (E|F) P (F) = P (F|E) P (E) 
Bayes’ theorem 
(5.18) 
Simple as it is, this result is so important that it deserves a special name: Bayes’ theorem.13 
In words: the probability of E given F times the probability of F must be equal to the 
probability of F given E times the probability of E. 
13As mentioned above, there is a, shall we say, lively debate between Bayesians and frequentists about the 
‘correct’ meaning of probability. This has nothing to do with Bayes’ theorem, which even the most dyed-in-the-
wool frequentist will accept as a perfectly good theorem. Frequentists and Bayesians make use of Bayes’ theorem 
to the same degree and with the same (100%) conﬁdence. 

 
 
  
58 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
The theorem just established allows us to derive another relationship of which we will 
make extensive use. Consider the following two quantities: 
P (E|F) P (F) = P (E ∩ F) 
(5.19) 
and 
 
 
  
P E| ˜F P 
˜F = P (E ∩ ˜F) 
(5.20) 
A glance at a Venn diagram immediately convinces us that 
P (E ∩ F) + P (E ∩ ˜F) = P (E)  
(5.21) 
Therefore 
P (E)  = P (E|F) P (F) + P
E|F˜
P
F˜
(5.22) 
5.7 Independence 
Two events, E and F, are said to be independent if14 
P (E|F) = P (E) 
(5.23) 
Given the discussion above, this means that knowledge of whether F has happened does 
not help at all in assessing the probability of E happening. Think again of the failure of 
a major bank counterparty and the summer–winter gas spread. This way of looking at 
conditional probabilities is particularly helpful if we interpret probability as degree of belief 
(see Jaynes (2003)). 
Again, paraphrasing Moore (2001), I could write a whole chapter about this. But I won’t. 
Let us see instead, using Bayes’ theorem again, what independence implies about 
P (F|E): 
P (E|F) P (F) = P (F|E) P (E) 
= P (E) P (F) = P (F|E) P (E) 
= P (F) = P (F|E) 
(independence) 
(5.24) 
where the second line follows from Equation (5.23), and in the last line the common term 
P (E) has been cancelled on both sides of the equation. This tells us that, if knowledge 
of whether F has happened does not help us at all in assessing the probability of E 
happening, knowledge of whether E has happened also does not help at all in assessing 
the probability of F happening. 
14To be meaningful, the independence condition P (E|F) = P (E) also requires that P (F)  ̸= 0. I will assume 
that this is always the case. 

5.8 TWO WORKED-OUT EXAMPLES 
59 
Here is another simple but powerful result that holds under independence: 
P (E  ∩ F)
P (E| F) =
= P (E) ===⇒ 
P (F) 
P (E  ∩ F) = P (E) P (F) 
(independence) 
(5.25) 
In words: when E and F are independent, the probability of both E and F happening is 
just given by the product of the stand-alone probabilities of E and F happening. 
Exercise 6 
Prove that, if P (E| F) = P (E), then P (E  = x ∩ F = y) = P (E  = x) 
P (F = y) for x, y = T , F . 
A word of caution: the concept of independence is as powerful as it is treacherous (see 
Williams (2001), Chapter 4, for a good discussion of the strength of the approach and of 
the pitfalls that can trip up the unaware). 
5.8 Two Worked-Out Examples 
Understanding a mathematical result is one thing (a very important one). Getting an intu-
itive feel for the same result is another (and every bit as important). I will present in this 
section two examples that highlight the subtleties involved when dealing with conditional 
probabilities, especially when some of the probabilities in question are very small. 
5.8.1 Dangerous Running 
I begin with a simple example. Let the event E be ‘I’ll go for a run today’ and the event 
F be ‘I’ll be hit by a car today’, where we can agree that ‘I’ll be hit by a car today’ means 
‘I’ll be hit by a car today ﬁrst thing in the morning’.15 Then Bayes’ theorem says that 
(the probability of my going for a run today, given that I will be hit by a car today) 
times 
(the probability of my being hit by a car today) 
must be equal to 
(the probability of my being hit by a car today, given that I have gone for a run today) 
times 
(the probability of my going for a run today) 
15This speciﬁcation simpliﬁes the problem in that we do not have to worry about situations in which I ﬁrst 
went for a run and then was hit by a car. 

60 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
Does this make intuitive sense? With (I hope) self-explanatory notation, let us rewrite this 
more compactly as 
P (run|hit)P (hit) = P (hit|run) P (run) 
(5.26) 
To dispel any causation and temporal fallacies, think of the following two questions 
• ‘If you know for sure that last Tuesday I went for a run, what would you say the 
probability is that I was hit by a car upon stepping out of my house that morning?’ 
• ‘If you know for sure that last Tuesday I was hit by a car upon stepping out of my 
house ﬁrst thing in the morning, what would you say the probability is that on that 
day I went for a run?’ 
The answers, of course, are P (hit|run) and P (run|hit), respectively. 
At ﬁrst blush one may say that P (run|hit) should be about as large as P (hit|run), and  
that both probabilities should be very (and similarly) low: after all, given that I have been 
hit by a car, it seems unlikely that I will have gone running; and given that I have gone 
running, it seems similarly unlikely that I had been hit by a car ﬁrst thing in the morning. 
But is this right? 
Rearranging Equation (5.26) one gets 
P (run|hit) 
P (run) 
= 
(5.27)
P (hit|run) 
P (hit) 
Now, I like to go running about twice a week. So P (run) ≃ 2/7 ≃ 0.28. I do not know my 
probability of being run over by a car today, but being since I have not been run over in my 
life yet, I can have a stab at estimating this probability as P (run) ≃ 1/(50 ∗ 365) ≃ 0.00005, 
where I have massively rounded (up, of course) my age. Then the ratio P (run) is given by 
P(hit) 
P (run) 
0.28 
≃
≃ 5000 
(5.28)
P (hit) 
0.00005 
So much for the right-hand side. But this ratio must hold also for the left-hand side: 
P (run|hit) ≃ 5000 
(5.29)
P (hit|run) 
or 
P (run|hit) ≃ 5000 P (hit|run) 
This shows that the probability of running today given that I have been hit by a car is about 
5000 times greater than the probability of being hit by a car today given that I have gone 
for run. Probably not what most of us would have guessed. Where does this counterintuitive 
result come from? 

5.8 TWO WORKED-OUT EXAMPLES 
61 
It comes from the fact that the probability of being hit by a car is so low to start with. 
Plausibly, I will go for a run despite being hit by a car if I have been lucky enough to 
sustain little damage, or if I have hurt my hand, not my legs. But the fact that I have 
still gone for a run only tells me something about the fact that my injury was mild or 
localized: the probability of being hit in the ﬁrst place remains very small. The probability 
of running is of course affected by my having been hit by a car, but still there is a chance 
of, say, 5% that I will be able to run today even if I have suffered a car accident. So, 
the rather high probability of running today (28%) is only diminished by a factor of about 
20 (1/0.05). Therefore, the probability of going for a run today given that I have been hit 
by a car is much, much higher than the probability of having been hit by a car ﬁrst thing 
in the morning, given that I have gone for a run today. What I have touched upon here 
is the problem of representativeness (or, rather, of the neglect of baseline frequencies) in 
estimating probabilities. I will have a lot more to say about this in Chapter 12. 
5.8.2 Rare and Even More Dangerous Diseases 
This is an old story, but still worth telling. A friend of yours is afraid that she may have 
contracted a rare but deadly disease (Wrong-Bayesitis, WB in the following for brevity). 
The condition affects one person in 50 000. Your friend therefore undertakes a medical test 
to ascertain whether she is infected. The test has a high accuracy rate (95%). After two days 
of anxious waiting, the test results arrive, and they are unfortunately positive. Your friend 
calls you in understandable distress. Can you offer words of comfort? 
Given the high accuracy of the test, the situation at ﬁrst glance does not look too encour-
aging for your friend. However, let us see whether Bayes’ theorem can help our reasoning. 
I denote by WB the event ‘your friend suffers from Wrong-Bayesitis’ and by test the event 
‘the test comes back positive’. From the information provided we know that the frequency 
1
of occurrence of WB in the population is 50 000 = 0.00002. Equating in this case frequency 
with probability we have 
1 
P (WB) =
= 0.00002 
(5.30)
50 000 
We also know that, if a person does have WB, then the test will detect this with 95% 
accuracy. Therefore 
P (test|WB) = 0.95 
(5.31) 
What we need is the probability that a person is affected by WB, given that the test has 
come back positive: we are looking for P (WB|test). So, by Bayes’ theorem we have 
P (WB|test) = P (test|WB)P (WB) 
(5.32)
P (test) 
The only quantity that we do not know is P (test), but we can have a very good stab 
at it. Suppose that 100 people take the test. As the test is 95% accurate, 5 (almost cer-
tainly) healthy people out of 100 will receive alarming but false news from the hospital. So 

62 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
P (test) ≃ 0.05.16 Now we have all  the information  we need:  
P (WB|test) = P (test|WB)P (WB) 
0.95 ∗ 0.00002 
=
= 0.0038 
(5.33)
P (test) 
0.05 
Good news: your friend has a chance of a little more than a third of 1% of being affected 
by WB!17 You and your friend can open a celebratory bottle of good wine to the memory 
of Reverend Bayes. 
Exercise 7 
The result above is not quite correct, as footnote 16 suggests. Can you derive 
the correct result? 
Where does this counterintuitive result come from? From the fact that the deadly Wrong-
Bayesitis is so rare (if only!) that almost all of the positive test results are false positives. 
Obviously, if no-one in the world had WB any more because it has been fully eradicated, 
but a test with 95% accuracy were available to detect it, 5 people out of 100 would receive 
a positive test result in the mail, but the probability of any of them, and of your friend in 
particular, having WB would still be zero. 
I have spent some time discussing the two examples above because they clearly make 
two important points. First, how easy it is to make big mistakes when thinking about very 
small probabilities – the type of probabilities, that is, that we are likely to encounter in 
stress testing. Second, how much conditional probabilities in general, and Bayes’ theorem 
in particular, can help us to think straight in these difﬁcult situations. 
5.9 Marginal and Conditional Probabilities: A Very 
Important Link 
This section explores a very important link between marginal and conditional probabilities. 
Mathematically, the results are trivial. However, I cannot stress how valuable the cognitive 
help provided by this link turns out to be. 
One can ask the following question: given that (I know that) event Ek has happened, has 
the marginal probability, P (Ei), of events Ei, i ̸= k, increased or decreased? In other terms: 
P (Ei) ≶?P (Ei|Ek) 
(5.34) 
16This is not quite right, because out of the 100 people 0.002 people will actually have the disease. 
17Needless to say, if you are a frequentist this sentence is meaningless: your friend either has the illness or 
she does not. 
If you are a frequentist the only way for you to comfort your friend is by saying something like the following: 
‘My dear friend, you either have the illness, in which case the probability of you being infected is 1; or you 
don’t, in which the case the probability is 0. Unfortunately, given the information at hand, I can’t tell you which 
probability applies to you. I will be able to tell for sure, of course, after the incubation time has elapsed. But as 
for today, the only thing that I can tell you is that, if every person in the population were tested, the fraction who 
test positive and do not have the disease is (100 − 0.38)%. In this sense you can be 99.62% conﬁdent that you do 
not have the disease.’ 
This is probably why frequentists do not have many friends. 
Bayesians, instead, who interpret probability as a degree of belief, have no problem in saying: ‘Dear friend, 
despite the outcome of the test, the probability of you having WB is 0.38%. Tonight, go out and celebrate with a 
nice bottle of Amarone.’ 
This is probably why Bayesians are so popular and have so many friends. 
And, as you can see, I am totally unbiased between a frequentist and Bayesian view of the world. 

  
5.9 MARGINAL AND CONDITIONAL PROBABILITIES: A VERY IMPORTANT LINK 63 
k
k
This can be expressed as a multiplier, xi . So,  x is deﬁned to be 
i 
P (Ei |Ek) ≡ P (Ei) xk 
(5.35)
i 
k
Exercise 8 What is the multiplier xi in the case of independence? 
k
Exercise 9 
What is the multiplier xi in the case of deterministic causation? 
k
Exercise 10 
What is the multiplier xi in the case of incompatibility? 
Now, we also know from Bayes’ theorem that 
P (Ei |Ek) P (Ek) = P (Ek|Ei) P (Ei) 
(5.36) 
and therefore 
P (Ei |Ek) 
P (Ek |Ei) 
= 
(5.37)
P (Ei) 
P (Ek) 
But, given Equation (5.35), 
P (Ei) xk
P (Ei |Ek) 
i
k
=
= x 
(5.38)
P (Ei) 
P (Ei) 
i 
and, from Equation (5.37), 
P (Ei |Ek) 
P (Ek|Ei)
k
i 
x =
=
= x 
(5.39)
i 
P (Ei) 
P (Ek) 
k 
and therefore 
k
i 
x = x 
(5.40)
i
k 
In words: by how much the probability of event i increases/decreases given the occurrence 
of event k must be equal to how much the probability of event k increases/decreases given 
k
the occurrence of event i. So,  xi 
must be a real symmetric matrix. 
We should pause to think about this result, which is both ‘obvious’ and very deep. 
Consider two events, such as the default of China on its internationally issued debt and 
a sharp widening in credit spreads for commercial mortgage-backed securities (CMBSs). 
How can we think about the interrelationships between these two events? More precisely, 
how can we assign the associated conditional probabilities? 
To begin with, the stand-alone probability of China defaulting is currently much, much 
lower than the probability of a widening in CMBS spreads. For argument’s sake, let us 
set P (China) = 10−5 and P (CMBS) = 10−3. Now, if China were to default, it would be 
almost certain that a generalized turmoil will hit all ﬁnancial markets, and that CMBS 

64 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
spreads, together with all manner of different spreads, would widen. I would therefore 
greatly increase the marginal probability P (CMBS) and set, say, P (CMBS|China) = 10−1. 
China
This means that xCMBS = 100. 
But let us look at the problem from the opposite angle, that is, conditioning on the 
widening of the CMBS spreads rather than on China’s default. We may be tempted to say 
that a widening of spreads will have very little effect, if any, on the probability of default of 
China, and that therefore P (China|CMBS) ≃ P (China). This would imply that x CMBS ≃ 1.
China 
But by Equation (5.40) this cannot happen: we must have 
China 
CMBS 
(5.41)
xCMBS = xChina 
Therefore, it seems that we are forced to say either that the default of China has no effect 
on CMBS spreads (which I ﬁnd difﬁcult to believe), or that the effect of the widening of the 
CMBS spreads has a major effect on the probability of China’s default. What went wrong? 
Nothing, really – or, rather, what is wrong is the misleading use of the word ‘effect’. Recall 
that conditional probabilities by themselves convey no causal information. So, we should not 
think of CMBS spreads ‘causing’ China’s default. Rather, we should phrase the problem as 
follows: given that we know that CMBS spreads have widened, how would we revise our 
assessment of the stand-alone probability of China’s default? As spreads may have widened 
exactly because China defaulted (besides, of course, for many other reasons) increasing our 
assessment of the probability that China defaulted no longer seems so unreasonable. Note 
that we are conditioning on the widening of the CMBS spreads, but, in this explanation, the 
cause of the widening was China’s default. Conditioning does not mean causing. See Section 
12.4 for a discussion of causal versus diagnostic interpretation of conditional probabilities. 
Are we happy, however, with increasing the probability of default by as much as a factor 
of 100? Yes, we are, because the unconditional probability was so low to start with (10−5)! 
So, we are saying that an extremely remote event has become much more likely, but is still 
very remote. 
The consistency check provided by Equation (5.40) is invaluable in helping us to think 
straight. And, as this example shows, often there is a ‘direction of conditioning’ that helps 
our reasoning (typically, but not always, the easiest direction is when we condition on 
k
i
the rarer event). Given the symmetry in xi = xk , the risk manager should therefore always 
tackle the conditional probability that she ﬁnds more ‘natural’ to work with. This, of course, 
is closely related to two topics to which I have given great emphasis in this book: one has to 
do with the superiority of causal as against association-based links between variables, which 
I discussed in Sections 3.3 and 3.4; and the other with the cognitive biases (such as the 
neglect of base-line frequency and the diagnostic/causal bias) that I discuss in Chapter 12. 
Note also that, if all the events in question are ‘stress’ (and hence ‘rare’) events, then 
any conditional probability (say, P (Ei |Ek)) larger than about 10% must be greater than the 
unconditional probability P (Ei). So, the conditional probabilities P (Ei |Ek) fall roughly 
in three categories: 
• Those that relate to pairs of events for which occurrence of event Ek makes little 
k
difference to the probability of observing event Ei . Then xi ≃ 1. 
• Those that relate to pairs of events for which the occurrence of stress scenario Ek makes 
k
the occurrence of event Ei signiﬁcantly less likely. Then xi < 1, and P (Ei |Ek) ≪ 1 

k
5.10 INTERPRETING AND GENERALIZING THE FACTORS x 
65
i 
(because P (Ei) was small to start with). In the limit, if the occurrence of event Ek 
k
makes the occurrence of event event Ei impossible, then P (Ei |Ek) = 0 and  x = 0.
i 
• Those that relate to pairs of events for which the occurrence of stress scenario Ek 
makes the occurrence of event Ei signiﬁcantly more likely. If we truly believe that 
the conditional probabilities are of the order of, say, 10% to 20% or greater, then, 
by the same token (i.e., because every stress event Ei is rare, and P (Ei) ≪ 1), the 
k
multiplicative factor must be much greater than 1: xi ≫ 1. 
To avoid spurious precision one could therefore restrict one’s attention to the following 
cases:18 
• The marginal probability of event Ei is strongly increased by the occurrence of event 
k
Ek : say,  xi = 10 to 50. 
• The marginal probability of event Ei is somewhat increased by the occurrence of event 
k
Ek : say,  xi = 2 to  5.  
• The marginal probability of event Ei is left unchanged by the occurrence of event Ek : 
k
say, xi ≃ 1. 
• The marginal probability of event Ei is somewhat decreased by the occurrence of 
k
event Ek : say,  xi = 0.5 to 0.2. 
• The marginal probability of event Ei is strongly decreased by the occurrence of event 
k
Ek : say,  xi = 0.02 to 0.1. 
5.10 Interpreting and Generalizing the Factors x 
k
Here is another interpretation and some simple generalizations of the factors xi that can be 
of great assistance in providing conditional probabilities. 
Start from 
P (Ei)P (Ek)
 
 
k
P (Ei, Ek) = P (Ei |Ek)P (Ek) =  
x 
(5.42)
i
Independence 
P (Ei)P (Ek)
Note that  
 
 is just the joint probability that would result if the two events 
Independence 
k
were independent. Therefore the multiplicative factor xi can be thought of as giving the 
‘correction from independence’: 
P (Ei, Ek)
k
i 
x = x = 
(5.43)
i
k 
P (Ei)P (Ek) 
We can use these factors xk also to help our intuition when it comes to providing 
i 
doubly-conditioned probabilities. Consider in fact the joint probability P (Ei, Ej, Ek). We  
k
18I present a different approach to assigning the factors xi in Chapter 9, in Equation (9.13) and passim. 
k
i 

	 

 
66 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
can always write 
P (Ei, Ej, Ek) = P (Ei|Ej, Ek)P (Ej, Ek) 
= P (Ei|Ej, Ek)P (Ej|Ek)P (Ek) 
(5.44) 
If we implicitly deﬁne xi
jk by the relationship 
P (Ei|Ej, Ek) = P (Ei)xjk 
(5.45)
i 
then we obtain 
P (Ei)P (Ej)P (Ek) 
jk k
P (Ei, Ej, Ek) =  
 
 x
x 
(5.46)
i
j
Independence 
k
In passing we note that the ‘correction-from-independence’ term is now xi
jkxj . But, more 
interestingly, we can repeat the same procedure for P (Ek, Ei, Ej) and P (Ej, Ei, Ek). 
Since, of course, P (Ei, Ej, Ek) = P (Ek, Ei, Ej) = P (Ej, Ei, Ek), it is a simple matter to 
prove that 
jk
j 
ij
j 
ik
k 
x
x = x x = xj x 
(5.47)
i
k
k
i 
i 
and therefore 
jk 
j 
x
x
i
i 
= 
(5.48)
ij 
j 
x
x
k
k 
ik 
j
xj 
xi 
= 
(5.49)
ij 
k 
xk 
xi 
ik 
j
xj 
xi 
= 
(5.50)
ij 
k 
xk 
xi 
These relationships are very useful. They tell us that, if we can specify all the marginal 
and singly -conditioned probabilities (or, equivalently, all the singly-conditioned factors 
xi
j), then we can always obtain the ratios of all the doubly-conditioned probabilities. As 
assigning doubly-conditioned probabilities can be very difﬁcult, we should be grateful for 
any help we can get along the way. 
k
But there is (a bit) more. Deﬁne xij by the relationship 
P (Ei, Ej|Ek) = P (Ei, Ej)xk 
(5.51)
ij 
Then, by using the same decomposition of the joint probabilities, one can show that 
j 
ik 
x = x 
(5.52)
ik 
j 

  
  
  
  
  
  
  
5.11 CONDITIONAL PROBABILITY MAPS 
67 
This relationship directly mirrors the symmetry of the fundamental identity 
i
k 
x = x 
(5.53)
k
i 
that we derived above.19 Mathematically the result is trivial, but it can help us in assigning 
the probabilities because it allows us to work with and not against our ‘cognitive grain’; that 
is, in the direction (causal rather than diagnostic) that we ﬁnd more natural. For instance, the 
reader should ponder the implications of Equation (5.52) by assigning a concrete interpre-
tation in terms of actual stress events to the indices i, j and k, and, again, convince herself 
of how obvious-yet-surprising these results are. Try, for instance, i = {China defaults}, 
j = {implied equity volatilities rise by 5%}, k  = {yield curve steepens by 50bp}. 
Exercise 11 
Prove Equation (5.52). 
We shall make more use of all these multiplicative factors in Chapter 9, where I discuss 
how to assign the conditional probabilities needed for the analysis. In that chapter we 
will also make more precise the constraint on the conditional probability imposed by the 
condition ‘Ei is strongly increased by the occurrence of event Ek’. 
5.11 Conditional Probability Maps 
We can enhance our intuition further by creating a visual display of the whole set of the 
k
conditional probabilities – or, rather, of quantities related to the xi factors deﬁned above. 
k
k
To do so, we begin by taking logs of xi and deﬁning bi
k = ln x . Then the case when 
i 
the probability of event Ei is left unchanged by the occurrence of event Ek corresponds to 
xk = 1, and therefore to bi
k = ln xk = 0. If we interpret ‘strongly increased’ as ‘increased 
i
i 
by a factor of at least 10’ and ‘strongly decreased’ as ‘decreased by a factor of at least 
k
k
10’, the corresponding ln xi 
would be ln x 
≥2.3 and  ln  xk ≤−2.3, respectively. A 
i
i 
decrease/increase of the unconditional probability by a factor of 2 would then correspond 
to ln x k 
i ≥0.69 and ln x k 
i ≤−0.69, respectively. See the following table: 
  
x k 
i 
bk 
i = ln x k 
i 
100 
4.605 
20 
2.996 
10 
2.303 
2 
0.693 
1 
0 
1/2 
−0.693 
1/10 
−2.303 
1/20 
−2.996 
1/100 
−4.605 
The quantities bi
k range in value from −∞to ln max[P(E
1 
i),P (Ej )]. 
19In a way, once obtained, this result is ‘obvious’: we can always think of the joint event (i, k) as some event, 
j 
ik 
l
say, l. Then the relationship xik = xj simply restates the well-known symmetry result xl
j = xj . 

  
 
 
 
 
 
 
 
68 
CHAPTER 5 MARGINAL AND CONDITIONAL PROBABILITIES 
What we would like to do is to give a graphical representation of the degree of ‘stress 
diversiﬁcation’. However, if we use the ‘raw’ quantities bi
k , these can assume large positive 
or negative values (in the limit, all the way to −∞). To give a more compact representation, 
we therefore deﬁne 
θk = arctan bi
k 
(5.54)
i 
π
Then each point θi
k will be smaller in absolute value than 2 . The positioning of point θk 
i 
on the segment 
π 
1 
(5.55)
− 2 , arctan ln max[P (Ei), P (Ej)] 
therefore gives information about how much event Ek decreases or increases (in log space) 
the stand-alone marginal probability P (Ei): a point θi
k with a value close to − π or

 
2 
arctan ln max[P(E
1 
i),P (Ej )] , respectively, shows that knowledge that event Ek has happened 
increases (decreases) a lot in our assessment of the stand-alone marginal probability P (Ei). 
More importantly, the clustering of the points towards the upper or the lower bounds indi-
cates the degree of event diversiﬁcation relative to event Ei : clustering towards the upper 
bound arctan ln max[P(E
1 
i),P (Ej )] 
indicates high concentration; clustering towards the 
lower bound (− π 
2 ) suggests that the events are unlikely to occur together. 
Figures 5.8, 5.9 and 5.10 show this graphical representation in the case of a highly 
concentrated, a neutral and a highly diversiﬁed set of events, given that event Ek has 
happened. 
We can also obtain a diversiﬁcation index, dk , by summing the quantities θi
k over the 
various events Ei : 
dk = 
θi
k 
(5.56) 
i 
This diversiﬁcation index tells us to what extent the conditional losses from events Ei̸=k 
associated with event Ek tend to ‘cluster’ or ‘avoid each other’. 
Diversification Chart 
9 
7 
5 
3 
1 
−2.0000 
−1.5000 
−1.0000 
−0.5000 
0.0000 
0.5000 
1.0000 
1.5000 
Figure 5.8 The concentration graph for a case of high concentration. 

5.11 CONDITIONAL PROBABILITY MAPS 
69 
Diversification Chart 
9 
7 
5 
3 
1 
−1.5000 
−1.0000 
−0.5000 
0.0000 
0.5000 
1.0000 
1.5000 
Figure 5.9 The concentration graph for a case of good diversiﬁcation. 
Diversification Chart 
1 
3 
5 
7 
9 
−2.0000 
−1.5000 
−1.0000 
−0.5000 
0.0000 
0.5000 
1.0000 
1.5000 
Figure 5.10 The concentration graph for a case of very high diversiﬁcation. 
Finally, an overall diversiﬁcation index, D, for all the events can be obtained by summing 
over all the events Ek : 
 
 
D = 
θ k 
i = 
dk 
(5.57) 
i,k 
k 
A positive, zero or negative diversiﬁcation index suggests concentration, total independence 
or diversiﬁcation, respectively. 
I note in closing that as presented here the diversiﬁcation index and the graphical display 
do not take into account the magnitude of the stress losses associated with the different 
events. So these indices only convey information about the relationships (clustering or 
avoidance) among the various events, not among the various losses. In the language of 
Chapters 8 and 9, they only depend on the topology of the underlying Bayesian net and on 
the associated conditional probability tables. They will therefore remain stable even if the 
exact size of positions changes over time. But we are getting ahead of ourselves. 


Chapter 6 
Probability with Boolean 
Variables II: Joint Probabilities 
The previous chapter dealt with the links between conditional and marginal probabilities. 
This chapter deals with a related topic, which is a central part of my treatment: the link 
between marginal and conditional probabilities on the one hand, and joint probabilities on the 
other. By the end of the chapter it should be clear why availability of the joint probabilities 
is the Holy Grail – and almost as difﬁcult to attain. 
One important remark is in order: most treatments of probability start from joint proba-
bilities and derive from these conditional probabilities. Following Pearl (2009), I follow the 
opposite route, which leads from marginal and conditional to joint probabilities. This is not 
just a matter presentation. I believe that, as human beings, we ﬁnd it very difﬁcult to think 
in terms of joint probabilities, and are better suited to making cognitive use of conditional 
relationships. My view in this respect is unashamedly Bayesian. As Pearl (2009) points out: 
Bayesian philosophers see the conditional relationship as more basic than that of joint 
events – that is, more compatible with the organization of human knowledge. In  this  view,  
B serves as a pointer to a context or frame of knowledge, and A|B stands for an event 
A in the context speciﬁed by B [. . .] Consequently, empirical knowledge invariably will 
be encoded in conditional probability statements, whereas belief in joint events (if it 
is ever needed) will be computed from those statements via the product P(A, B) = 
P(A|B)P(B).1 
This is exactly the approach I have taken in this book, and needless to say, there are 
close links between ‘pointers to a frame of knowledge’ and the interpretative models of 
reality discussed in Chapter 3. 
6.1 Conditioning on More Than One Event 
The treatment of the previous chapter can be easily generalized to the case where con-
ditioning is over more than one event. For instance, if we condition on (knowledge of) 
1Pearl (2009), page 4, my emphasis. 
71 

 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
72 
CHAPTER 6 JOINT PROBABILITIES 
two events having happened, the conditional probability of event Ei , given events Ej and 
Ek, P  (Ei, |Ej ∩ Ek), is  given  by  
Ei ∩ Ej ∩ Ek 
P (Ei, |Ej ∩ Ek) = 

 
(6.1) 
Ej ∩ Ek 
This expression will become useful when we establish the connection between the condi-
tional and joint probabilities. As mentioned in footnote 7 of Chapter 5, I refer to a conditional 
probability such as the one in Equation (6.1) as a doubly-conditioned probability . The exten-
sion of the deﬁnition to n-conditioned probabilities is obvious. 
Exercise 12 
Convince yourself of the reasonableness of the deﬁnition above by drawing 
the associated Venn diagrams. 
We can also easily provide an extension of Equation (5.13): 
P (Ei, |Ej ∩ Ek) + P (E˜i, |Ej ∩ Ek) = 1 
(6.2) 
From Equation (6.1) we can then immediately derive 
P Ei ∩ Ej ∩ Ek = P (Ei, |Ej ∩ Ek)P Ej ∩ Ek 
(6.3) 
Now, recall the deﬁnition of P (Ej, |Ek): 
P Ej ∩ Ek 
P (Ej, |Ek) = 
(6.4)
P (Ek) 
Therefore 
P Ei ∩ Ej ∩ Ek = P (Ei, |Ej ∩ Ek)P (Ej, |Ek)P (Ek) 
(6.5) 
This is the ﬁrst example of an extremely important set of relationships, i.e., those relation-
ships that allow us to ‘break up’ the probability of an n-ple intersection, P Ei ∩ Ej ∩ Ek , 
into conditionals and marginals of order n − 1 or lower. Once we have introduced joint 
probabilities (see the next section) the importance of this will be evident. 
Exercise 13 
Generalize the result to the case of n Boolean variables. 
Clearly, this decomposition is not unique. For instance 
P Ei ∩ Ej ∩ Ek = P Ek ∩ Ej ∩ Ei 
(6.6) 
and therefore 
P Ek ∩ Ej ∩ Ei = P (Ek, |Ej ∩ Ei)P (Ej, |Ei)P (Ei) 
(6.7) 

 
 
 
 
6.2 JOINT PROBABILITIES 
73 
Equating the right-hand sides of Equations (6.5) and (6.7) we have 
P (Ei, |Ej ∩ Ek)P (Ej, |Ek)P (Ek) = P (Ek, |Ej ∩ Ei)P (Ej, |Ei)P (Ei) 
(6.8) 
Since, of course, 
P (Ek, |Ej ∩ Ei) = P (Ek, |Ej, Ei) 
(6.9) 
and 
P Ei ∩ Ej ∩ Ek = P Ei, Ej, Ek 
we ﬁnally have 
P (Ei, |Ej, Ek)P (Ej, |Ek)P (Ek) = P (Ek, |Ej, Ei)P (Ej, |Ei)P (Ei) 
(6.10) 
and 
P (Ei, |Ej, Ek) 
P (Ej, |Ei) P (Ei) 
= 
(6.11)
P (Ek, |Ej, Ei) 
P (Ej, |Ek) P (Ek) 
or 
P (Ei, |Ej, Ek) = P (Ej, |Ei)P (Ei) P (Ek, |Ej, Ei) 
(6.12)
P (Ej, |Ek)P (Ek) 
This relationship gives us a link between one doubly-conditioned probability 
(P (Ei, |Ej, Ek)) in terms of another (P (Ek, |Ej, Ei)), and marginal and singly-conditioned 
probabilities (P (Ei), P (Ek), P (Ej, |Ei) and P (Ej, |Ek)). This can be of help when we 
deal with Bayesian nets and we need to assign doubly-conditioned probabilities – always 
a difﬁcult task. Sometimes it may be easier to have a good intuition for one such 
doubly-conditioned probability (in this case P (Ek, |Ej, Ei)) rather than the other. Then 
Equation (6.12) can provide help in determining the less ‘intuitive’ one. 
6.2 Joint Probabilities 
We are now ready to introduce the most powerful probabilistic piece of information at our 
disposal: the joint distribution. Unfortunately, we will have to get some of notation out of 
the way ﬁrst. 
First, we now allow for n events (to be a bit more precise, n two-valued Boolean random 
variables), E1, E2, . . .  , En. A joint event is any combination of T and F realizations for 
the n elementary events. I will denote the ith joint event by Ji . For two-valued Boolean 
random variables, there will be 2n such joint events. We can therefore label them all with 
an index i = 1, 2, . . .  , 2n. Let us suppose that we have done so. We can now associate a 

 
74 
CHAPTER 6 JOINT PROBABILITIES 
probability p(i) to each joint event. If we normalize the size of the whole universe to 1, 
each joint probability is therefore given by the size of the intersection set: 
Ji = {E1 = x, E2 = y, . . .  , En = z} 
(6.13) 
where x, y and z assume the values T or F , as appropriate for event Ji . 
It is essential to stress that no two joint events can happen at the same time. With 
unfortunate terminology, all joint events Ji are therefore disjointed. Therefore, the following 
requirements (which we can derive from the probability axioms presented above) must 
hold true: 
p(i) ≥ 0,
i
 
= 1, 2, . . .  , 2n 
(6.14) 
2n 
p(i) = 1 
(6.15) 
i=1 
For n = 3 we can represent all of this very easily as follows: 
E1 
E2 
E3 
J1 
1 
0 
0 
p(1) 
J2 
0 
1 
0 
p(2) 
J3 
0 
0 
1 
p(3) 
J4 
1 
1 
0 
p(4) 
(6.16) 
J5 
1 
0 
1 
p(5) 
J6 
0 
1 
1 
p(6) 
J7 
1 
1 
1 
p(7) 
J8 
0 
0 
0 
p(8) 
The entries 1, 0, 0 in the ﬁrst row (which is assigned to joint event J1) indicate that for this 
joint event the ﬁrst event happened, and the second and third did not. The same applies for 
the other rows. 
Note the last two rows. The last one (J8) is associated with the joint event ‘none of the 
three elementary events occurred’. The second-to-last one is associated with the joint event 
‘all of the three elementary events happened’. If our elementary events are to be given the 
interpretation of rare stress events, p(8) should be ‘large’ (close to 1), and p(7) very, very 
small (smaller than or equal to the smallest stand-alone probability P (Ei)). These kinds of 
checks will become very important when we move on to the practical applications of the 
above to stress testing, but for the moment any set of values p(i) compatible with Equations 
(6.14) and (6.15) will sufﬁce. 
The set of probabilities [p(i)] is called the joint distribution for the n Boolean random 
variables . 
We can now readily understand the links between the joint probabilities and the various 
possible intersections. Look again at the table that forms Equation (6.16). The size of the 
set E1 = T is given by 
{E1 = T } = P (E1) 
= p(1) + p(4) + p(5) + p(7) 
(6.17) 

6.3 A REMARK ON NOTATION 
75 
The size of the  set  {E1 = T ∩ E2 = F } is given by 
{E1 = T ∩ E2 = F } = P (E1 ∩˜E2) 
= p(1) + p(5) 
(6.18) 
The size of  {E1 = T ∩ E2 = F ∩ E3 = F } is given by 
{E1 = T ∩ E2 = F ∩ E3 = F } = P (E1 ∩˜E2 ∩˜E3) 
= p(1) 
(6.19) 
In general, therefore, each of the probabilities of the joint events Ji can be written as the 
probability of appropriate intersections of sets of the elementary events: 
p(1) = P (E1 ∩E˜2 ∩E˜3) 
(6.20) 
p(2) = P (E˜1 ∩E2 ∩E˜3) 
(6.21) 
p(3) = P (E˜1 ∩E˜2 ∩E3) 
(6.22) 
p(4) = P (E1 ∩E2 ∩E˜3) 
(6.23) 
. . .  
p(8) = P (E˜1 ∩E˜2 ∩E˜3) 
(6.24) 
I said above that once we know the joint probabilities we can obtain any probabilistic 
information about the underlying Boolean variables. I therefore now have to show how to 
do this. But before doing so, however, it is high time to clean up the notation. 
6.3 A Remark on Notation 
Deﬁning notation midway through my treatment may strike the reader as odd. However, I 
wanted to introduce all the concepts needed for our analysis ﬁrst, rather than introducing at 
the start the notation for mysterious quantities that have not yet been deﬁned. 
So, let me begin with the joint probabilities I have just deﬁned. As I have shown, 
the joint probabilities are just the probabilities of all the various possible intersections. 
Depending on the context, I will therefore denote joint probabilities in any of the following 
equivalent ways: 
• p(i): The probability of joint event Ji . Joint events are in turn given by the intersec-
tion of a particular combination of realizations for events Er , Es, . . .  , Et , such as,  for  
instance, Jk = [E1 = T ∩E2 = F , . . .  , ∩En = T ]. This notation for the joint proba-
bility is compact, but not very transparent when we are interested in the occurrences 
of the underlying events. 
• P (E1 = T , E2 = F , . . .  , En = T ): This more cumbersome notation has the advan-
tage of being totally transparent, because it shows in full the particular set of joint 
realizations (T or F ), for the n elementary events, E1 = T , E2 = F , . . .  , En = T . 

 
 
76 
CHAPTER 6 JOINT PROBABILITIES 
• P (E1 = T ∩E2 = F ∩. . .  ∩En = T ): The same as above, emphasizing that the joint 
probability is the probability of the intersection of the realizations for the n elementary 
events, E1 = T , E2 = F , . . .  , En = T . 
• P
E1 ∩E˜2 ∩. . .  ∩En : The same as above, using the notation E˜2 to denote E2 = F 
and E1 to denote E1 = T . This is the most delicate piece of notation, because now 
E1 can mean either the Boolean random variable E1 or its TRUE realization. So if 
I write P (E1 ∩E2 ∩. . .  ∩En) I could be referring to a generic joint probability, or 
to the particular joint probability where all the events turned out to be TRUE. The 
context should make the meaning clear. 
• P (E1, E˜2, . . .  , En): The same as above, when there is no need to emphasize the 
intersections. 
• P (E1 = x ∩E2 = y ∩. . .  ∩En = z) or P (E1 = x, E2 = y, . . .  , En = z) with x, y, 
z = T , F to denote all the joint probabilities associated with the various possible 
realizations for the Boolean variables E1, E2, . . .  , En. 
I will always denote marginal probabilities by P (Ek), where  P (Ek) means the probability 
of event Ek being true. When I want to denote the marginal probability of event Ek being 
false, I will write P (E˜k) or P (Ek = F ). 
As for conditional probabilities, the notation P (Ej |Ek) suffers from the same ambi-
guity highlighted above in the case of joint probabilities: I could be referring to the 
generic conditional probability P (Ej |Ek) or I could mean the speciﬁc conditional prob-
ability P (Ej = T |Ek = T ). Context, as usual, is everything. 
˜
When I mean, say, P (Ej = T |Ek = F ), to save ink I will often write P (Ej |Ek). 
And, ﬁnally, the notation P (Ej = x|Ek = y) with x, y = T , F will have the collective 
meaning of denoting all the possible conditional probabilities, exactly as in the case of joint 
probabilities. 
Depending on the context, one notation can be more suggestive or transparent than the 
other, and I will therefore use them interchangeably. 
6.4 From the Joint to the Marginal and the Conditional 
Probabilities 
With the notation clearly deﬁned, I return to the main thrust of the presentation. Our next 
task is to obtain the probabilities of the individual events, P (Ek), k = 1, 2, . . .  ,  n from 
the joint distribution – in practice we will often try to go in the opposite direction, but we 
worry about this later. To illustrate the concepts in the simplest case, let us consider the 
case of n = 3. 
We start from P (E1). Since all the joint events Ji are disjoint, the probability of event E1 
occurring is simply given by the sum of the probabilities p(i) associated with joint events 
Ji where E1 occurs. A glance at Equation (6.16) shows that this is equal to 
P (E1) = p(1) + p(4) + p(5) + p(7) 
(6.25) 
We can, of course, proceed similarly for E2 and E3. 

6.5 FROM THE JOINT DISTRIBUTION TO EVENT CORRELATION 
77 
So much for marginal probabilities. What about the probability that both events E2 and 
E3 happen? Again, we can read it directly from Equation (6.16) above: 
P (E2 ∩ E3) = p(6) + p(7) 
(6.26) 
Of course, we also know that 
P (E2 ∩ E3) = {E2 ∩ E3} 
(6.27) 
We can check this immediately from Equation (6.16), because event E2 is true (E2 = T ) 
in joint events J2, J4, J6 and J7, and event event E3 is true (E3 = T ) in joint events J3, 
J5, J6 and J7. So, the only two joint events that have events E2 and E3 in common are 
J6 and J7. 
Exercise 14 
Using the information in Equation (6.16) derive an expression for 
P (E2 ∪ E3). 
Of course, if we know how to calculate from the joint probabilities the probability of 
the intersection between any two sets, we are only a small step away from calculating 
any conditional probability in terms of the joint probabilities. To see this, consider, say, 
P (E2|E3). We know that 
P (E2 ∩ E3)
P (E2|E3) = {E2 ∩ E3} = 
(6.28)
{E3} 
P (E3) 
The probability of event E3 is immediately obtained as above: 
P (E3) = p(3) + p(5) + p(6) + p(7) 
(6.29) 
and therefore 
p(6) + p(7)
P (E2|E3) = 
(6.30)
p(3) + p(5) + p(6) + p(7) 
Exercise 15 
Generalize the expression just derived to any j and k, and check that the 
expressions thus obtained satisfy the conditions for representing possible probabilities (i.e., 
they are all greater or equal to 0 and smaller or equal to 1, and they all add up to 1). 
6.5 From the Joint Distribution to Event Correlation 
As I said, from the knowledge of the joint distribution we can derive everything that we may 
statistically desire. And everything means everything, not just the marginal and conditional 
probabilities. Let me show, for instance, how to determine the event correlation between 
any two events. The result will be more than an exercise in manipulating joint probabilities, 
because it will have intrinsic interest when we deal with stress testing. 

 
 
 
 
   
 
78 
CHAPTER 6 JOINT PROBABILITIES 
When we deal with stochastic processes (i.e., roughly speaking, random variables indexed 
by time) we have a simple intuitive understanding of what correlation ‘means’: after nor-
malizing for the possibly different variance, the correlation coefﬁcient gives an indication 
of the extent to which the two variables ‘move together’. What interpretation should we 
give to the correlation coefﬁcient in the case of Boolean variables? Let me give a formal 
deﬁnition ﬁrst, and we will see the intuitive meaning after obtaining the result. 
As a ﬁrst step it is useful to associate with each event Ei a random variable, 1i , i.e.,  the  
indicator function that assumes value 1 if Ei is true, and zero otherwise: 
1i = 1 if  Ei = T 
1i = 0 if  Ei = F 
(6.31) 
Then a natural deﬁnition of correlation ρij is 
E 1i − 1i 
1j − 1j
ρij = 
 

 
(6.32) 
var (1i) var 1j 
where the symbols E[·], var(·) and 1i denote the expectation operator,2 the variance operator 
and the expected value of the indicator function 1i , respectively: 
1i = E [1i ] 
(6.33) 
Looking back at Equation (6.16) it is easy to calculate the expected value of 1i . Take, for 
instance, i = 1. In this case the expectation will be equal to the sum of the products of the 
value of the indicator function 11 for each joint event Jk , times the probability of the joint 
event, p(k). For  i = 1 this gives  
11 
= 1 ∗ p(1) + 0 ∗ p(2) + 0 ∗ p(3) + 1 ∗ p(4) + 1 ∗ p(5) + 1 ∗ p(7) + 0 ∗ p(8) 
= 1 ∗ p(1) + 1 ∗ p(4) + 1 ∗ p(5) + 1 ∗ p(7) 
= p(1) + p(4) + p(5) + p(7) = P (E1) 
(6.34) 
Therefore 
1i = E [(1i)] = P (Ei) for any i 
(6.35) 
We can therefore rewrite Equation (6.32) as 
E (1i − P (Ei)) 1j − P (Ej) 
ρij = 
 

 
(6.36) 
var (1i) var 1j 
2The expectation is taken, of course, over the joint probabilities [p(i)]. 

 
 
 
 
6.5 FROM THE JOINT DISTRIBUTION TO EVENT CORRELATION 
79 
To calculate the numerator, I can construct the following table: 
E1 
E2 
E3 
J1 
1 − P (E1) 0 − P (E2) 0 − P (E3)
p(1) 
J2 
0 − P (E1) 1 − P (E2) 0 − P (E3)
p(2) 
J3 
0 − P (E1) 0 − P (E2) 1 − P (E3)
p(3) 
J4 
1 − P (E1) 1 − P (E2) 0 − P (E3)
p(4) 
(6.37) 
J5 
1 − P (E1) 0 − P (E2) 1 − P (E3)
p(5) 
J6 
0 − P (E1) 1 − P (E2) 1 − P (E3)
p(6) 
J7 
1 − P (E1) 1 − P (E2) 1 − P (E3)
p(7) 
J8 
0 − P (E1) 0 − P (E2) 0 − P (E3)
p(8) 
Now, each entry of the table has been expressed in a way that is suitable for 
direct use in Equation (6.36): suppose, for instance, that I want to calculate 
E [(11 − P (E1)) (12 − P (E2))]. All I have to do is to look  at  the ﬁrst two  columns of  
Equation (6.37) and obtain: 
[(1 − P (E1)) (0 − P (E2))] p(1) + 
[(0 − P (E1)) (1 − P (E2))] p(2) + 
[(0 − P (E1)) (0 − P (E2))] p(3) + 
[(1 − P (E1)) (1 − P (E2))] p(4) + 
[(1 − P (E1)) (0 − P (E2))] p(5) + 
[(0 − P (E1)) (1 − P (E2))] p(6) + 
[(1 − P (E1)) (1 − P (E2))] p(7) + 
[(0 − P (E1)) (0 − P (E2))] p(8) 
(6.38) 
where, of course, P (E1) and P (E2) have been derived as above. 
So much for the numerator. Let us move to the denominator. The variance of the indicator 
function 1i is given by 
(1i)2
var [1i ] = E 
− E [(1i)]2 
(6.39) 
Now, 
2
E [(1i)]2 = P (Ei)
(6.40) 
But, because we have stipulated that the indicator function can only take values 1 or 0, 
(1i)2 = (1i) 
(6.41) 
It therefore follows that 
E (1i)2 = E [(1i)] = P (Ei) 
(6.42) 

 
 
 
 
 
 
80 
CHAPTER 6 JOINT PROBABILITIES 
So, we obtain 
var [1i ] = P (Ei) − P (Ei)2 
(6.43) 
Reassuringly, as P (Ei) is always smaller or equal to 1, the variance will always be positive. 
Exercise 16 
Comment on the variance in the case when P (Ei) = 1. Does the result make 
sense? What does it indicate? 
As a ﬁnal result I can rewrite Equation (6.36) as: 

   
 
E 1i − 1i 
1j − 1j
ρij = 
   
 
(6.44) 
P (Ei) − P (Ei)2 
P (Ej) − P (Ej)2 
E (1i − P (Ei)) 1j − P (Ej) 
= 
   
 
(6.45) 
P (Ei) − P (Ei)2 
P (Ej) − P (Ej)2 
with the numerator given by Equation (6.39-1). Needless to say, the joint probabilities enter 
expression (6.44) via the expectation operator, E [·]. 
So much for an algebraic expression for the correlation. What intuitive meaning can we 
ascribe to it? Consider a joint event, say, J6. If in joint event J6, elementary event Ei is true 
(occurs), the indicator function will have value 1 and the quantity 1i − P (Ei) will be greater 
than or equal to 0. Conversely, if the elementary event Ei does not occur, the same quantity 
1i − P (Ei) will be smaller than or equal to 0. The same reasoning applies to the elementary 
event Ej . So, if both event Ei and event Ej occur in joint event J6, the numerator will 
be positive (positive number times positive number). The numerator will also be positive if 
neither event occurs (negative number times negative number). The numerator will instead 
be negative when one event occurs and the other does not in joint event J6. This positive 
or negative number will then be weighted by the probability of joint event J6, p(6). 
So, the correlation coefﬁcient tells us the following: when the indicator function for one 
event is greater than its expected value (i.e., than the marginal probability for that event, 
P (Ei)) and the indicator function for the other event is greater than its expected value 
(the probability for that event, P (Ej)), we have a positive contribution to the correlation 
coefﬁcient (weighted by p(6)). The opposite occurs when the indicator function for one 
event is greater than its expected value but the indicator function for the other event is 
smaller than its expected value. The correlation coefﬁcient therefore gives a measure of the 
concordance of occurrence of two events. It gives a quantitative answer to the question: 
‘Are the two events likely or unlikely to happen together, after taking into account their 
stand-alone probability of occurrence?’ 
Following Tzani and Polychronakos (2008), we can gain further insight by analysing in 
greater detail Equation (6.44): 
E (1i − P (Ei)) 1j − P (Ej) 
ρij = 
   
 
(6.46) 
P (Ei) − P (Ei)2 
P (Ej) − P (Ej)2 

 
 
 
 
 
 
 
 
  
 
 
  
 
6.5 FROM THE JOINT DISTRIBUTION TO EVENT CORRELATION 
81 
Let us look at the term 
E (1i − P (Ei)) 1j − P (Ej) 
This can be rewritten as 
E (1i − P (Ei)) 1j − P (Ej) 
= E 1i 1j − P (Ei)E 1j − P (Ej)E [1i ] + P (Ei)P (Ej) 
= E 1i 1j − P (Ei)P (Ej) 
(6.47) 
where the last line follows because 
E 1j = P (Ej) 
As for the denominator, note that 
P (Ei) ∗ (1 − P (Ei)) = P (Ei) − P (Ei)2 
= P (Ei)Q(Ei) 
with 
Q(Ei) ≡ 1 − P (Ei) 
Therefore Equation (6.44) can be rewritten as: 
 
 
 
ρij = 
E (1i − P (Ei )) 1j − P (Ej )
 
P (Ei) − P (Ei )2    
P (Ej ) − P (Ej )2  
 
 
ρij = 
E 1i 1j − P (Ei )P (Ej )
 
P (Ei )Q(Ei )P (Ej )Q(Ej ) 
(6.48) 
Note that 
 
 
E 1i 1j = P (Ei = T , Ej = T ) 
(6.49) 
Following again Tzani and Polychronakos (2008), from this result we can write for any two 
events Ei , Ej : 
P (Ei, Ej) − P (Ei)P (Ej) = ρij 
P (Ei)Q(Ei)P (Ej)Q(Ej) 
(6.50) 
where 
P (Ei, Ej) = P (Ei = T , Ej = T ) 
(6.51) 

 
 
 
 
 
 
	 
 

 
	  
 

 
 
 
	 
 

 
 
 
82 
CHAPTER 6 JOINT PROBABILITIES 
and 
P (Ei, E˜j) = P (Ei = T , Ej = F ) 
Also, it is always true that 
P (Ei, E˜j) + P (Ei, Ej) = P (Ei) 
(6.52) 
P (E˜i, Ej) + P (Ei, Ej) = P (Ej) 
(6.53) 
P (E˜i, E˜j) + P (Ei, E˜j) + P (E˜i, Ej) + P (Ei, Ej) = 1 
(6.54) 
From this we can readily obtain the following: 
P (E˜i, E˜j) = Q(Ei)Q(Ej) + ρij 
P (Ei)Q(Ei)P (Ej)Q(Ej) 
(6.55) 
P (E˜i, Ej) = Q(Ei)P (Ej) − ρij 
P (Ei)Q(Ei)P (Ej)Q(Ej) 
(6.56) 
P (Ei, E˜j) = P (Ei)Q(Ej) − ρij 
P (Ei)Q(Ei)P (Ej)Q(Ej) 
(6.57) 
P (Ei, Ej) = P (Ei)P (Ej) + ρij 
P (Ei)Q(Ei)P (Ej)Q(Ej) 
(6.58) 
As these probabilities must all be non-negative, there are constraints on the correlations 
ρij , such as  
ρij ≤ min 
P (Ei)Q(Ej) 
P (Ej)Q(Ei) 
, 
(6.59)
P (Ej)Q(Ei) 
P (Ei)Q(Ej) 
P (Ei)P (Ej) 
Q(Ej)Q(Ei)
ρij ≥ max − 
, − 
(6.60)
Q(Ej)Q(Ei) 
P (Ei)P (Ej) 
These conditions are distinct from the conditions of positivity of the eigenvalues of ρij : 
there are correlation matrices (which, as such, have positive eigenvalues) for which the 
condition above is not satisﬁed; and there are quantities such that the same inequality is 
satisﬁed, but that do not constitute a correlation matrix. However, if Equation (6.59) holds 
as an equality, 
P (Ei)Q(Ej) 
P (Ej)Q(Ei)
ρij = min 
, 
(6.61)
P (Ej)Q(Ei) 
P (Ei)Q(Ej) 
then an associated set of event probabilities exists and is unique. (See Tzani and Polychron-
akos (2008) for a simple proof.) This means that, for any given set of event probabilities, 
there is an absolute maximum in the associated event correlations. 
Exercise 17 
Derive the constraints on the correlations ρij 
coming from the condition 
that the probabilities must be smaller than or equal to 1. 

6.6 FROM THE CONDITIONAL AND MARGINAL TO THE JOINT PROBABILITIES? 83 
6.6 From the Conditional and Marginal to the Joint 
Probabilities? 
We have seen how to go from the joint probabilities, p(i), to the conditional, P (Ek|Ej), 
or marginal probabilities, P (Ek). Can we go in the opposite direction? If, in other words, I 
gave you all the stand-alone probabilities, P (Ek), and all the (singly-conditioned) conditional 
probabilities, P (Ek|Ej), would you be able to reconstruct the joint probability distribution? 
Unfortunately, the answer is, in general, ‘no’. Let us see why this is the case. 
Suppose that we have somehow obtained all the singly-conditional probabilities, 
P (Ek|Ej), and marginal probabilities, P (Ek), for  n Boolean variables: j, k = 1, 2, . . .  , n. 
Do we have enough information to deduce the joint probabilities, p(i), i = 1, 2, . . .  , 2n? 
Well, we have n 2 − n conditional probabilities (n 2 because P (Ek|Ej) ̸= P (Ej |Ek) and 
−n because the main diagonal is just ﬁlled with n 1s.) Knowledge of the marginal 
probabilities adds another n pieces of information. So, we seem to have in all n 2 quantities. 
However, once the marginal probabilities are given, the singly-conditioned probabilities 
are not independent, because they are linked by Bayes’ theorem: given P (Ej |Ek) (and 
the marginals P (Ej) and P (Ek)) then P (Ek|Ej) is fully speciﬁed. Therefore we only 
have n 2−n pieces of independent information from the singly-conditioned probabilities, 
2 
plus n from the marginals. Speciﬁcation of the joint probabilities requires 2n − 1
2n 
numbers (where the −1 comes from the condition 
i=1 p(i) = 1). The following table 
shows the number of unknowns against the number of known quantities for various 
values of n: 
2−n 
n 
n 
2 
+ n 2n − 1 
2
3
 
3
 
3
6
 
7
 
4
10
 
15
 
(6.62) 
5
15
 
31
 
6
21
 
63
 
7 28 
127 
As one can see, when there are more than two Boolean random variables, the information 
in the marginal and singly-conditional probabilities is in general not enough to determine 
uniquely the joint distribution. 
Having said that, there are situations where we can derive the joint probabilities from 
the marginal and the singly-conditioned probabilities: this can happen when our knowledge 
of the problem suggests that there may be independence (absolute or conditional) between 
some of the variables. This is dealt with in the following two sections. Before moving to 
that task it is important to point out that there are other very interesting situations: we 
may not have enough information to specify fully the joint probabilities, but it is surprising 
how powerful even limited information about marginal and once-conditioned probabilities 
can be in producing very useful bounds for joint distributions or more highly-conditioned 
probabilities. This aspect is dealt with in Chapter 7. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
84 
CHAPTER 6 JOINT PROBABILITIES 
6.7 Putting Independence to Work 
I deﬁned in Section 5.7 the concept of independence between two events. Let us see how, 
and when, we can make use of this extremely powerful tool in order to calculate joint 
probabilities. Suppose that we have decided that two events, E1 and E2, are independent, 
and that they have marginal probabilities of P (E1) = 0.6 and  P (E2) = 0.3.3 Recall that, 
in general, 
P (E1|E2) + P
E˜1|E2 = 1 
(6.63) 
and that 
P (E1 = x ∩E2 = y ∩E3 = z) 
= P (E1 = x|E2 = y ∩E3 = z) ∗P (E2 = y ∩E3 = z) 
(6.64) 
with x, y, z = T , F . Also, recall that in the case of independence 
P (E1 = x ∩E2 = y) = P (E1 = x)P (E2 = y) 
(6.65) 
for x, y = T , F . If independence applies, the joint probabilities for E1 and E2 are then 
easily obtained: 
P (E1 ∩E2) = P (E1)P (E2) = 0.6 ∗0.3 = 0.18 
(6.66) 
P
E1 ∩E˜2 = P (E1)P (E˜2) = 0.6 ∗(1 −0.3) = 0.42 
(6.67) 
P
E˜1 ∩E2 = P (E˜1)P (E2) = (1 −0.6) ∗0.3 = 0.12 
(6.68) 
P
E˜1 ∩E˜2 = P (E˜1)P (E˜2) = (1 −0.6) ∗(1 −0.3) = 0.28 
(6.69) 
Now assume that, on the basis of our knowledge of how the world works, we have decided 
that 
P (E3|E1 ∩E2) = 0.05 
(6.70) 
P
E3|E1 ∩E˜2 = 0.10 
(6.71) 
P
E3|E˜1 ∩E2 = 0.10 
(6.72) 
˜
P
E3|E1 ∩E˜2 = 0.20 
(6.73) 
How can we build the joint probabilities P (E1 = x ∩E2 = y ∩E3 = z) for x, y, z = T , 
F from this information? 
3The following example has been adapted from Moore (2001). 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
6.7 PUTTING INDEPENDENCE TO WORK 
85 
To begin with let us make use of Equation (6.63) to obtain the following: 
P
E˜3|E1 ∩E2 = 1 −P (E3|E1 ∩E2) = 0.95 
(6.74) 
˜
P
E3|E1 ∩E˜2 = 1 −P
E3|E1 ∩E˜2 = 0.90 
(6.75) 
˜
P
E3|E˜1 ∩E2 = 1 −P
E3|E˜1 ∩E2 = 0.90 
(6.76) 
˜
˜
˜
P
E3|E1 ∩E˜2 = 1 −P
E3|E1 ∩E˜2 = 0.80 
(6.77) 
Next, we write in full the expression P (E1 = x ∩E2 = y ∩E3 = z) for all the cases of 
interest making use of Equation (6.64) (and remembering, of course, that, e.g., P (Ei ∩ 
Ej ∩Ek) = P (Ek ∩Ei ∩Ej), for any permutation of i, j, k): 
P (E3 ∩E1 ∩E2) = P (E3|E1 ∩E2) ∗P (E1 ∩E2) = 0.05 ∗0.18 = 0.009 
(6.78) 
P
E˜3 ∩E1 ∩E2 = P
E˜3|E1 ∩E2 ∗P (E1 ∩E2) = 0.95 ∗0.18 = 0.171 
(6.79) 
P
E3 ∩E˜1 ∩E2 = P
E3|E˜1 ∩E2 ∗P
E˜1 ∩E2 = 0.10 ∗0.12 = 0.012 
(6.80) 
˜
P
E˜3 ∩E˜1 ∩E2 = P
E˜3|E1 ∩E2 ∗P
E˜1 ∩E2 = 0.90 ∗0.12 = 0.108 
(6.81) 
P
E3 ∩E1 ∩E˜2 = P
E3|E1 ∩E˜2 ∗P
E1 ∩E˜2 = 0.10 ∗0.42 = 0.042 
(6.82) 
˜
P
E˜3 ∩E1 ∩E˜2 = P
E3|E1 ∩E˜2 ∗P
E1 ∩E˜2 = 0.90 ∗0.42 = 0.378 
(6.83) 
˜
˜
P
E3 ∩E˜1 ∩E˜2 = P
E3|E1 ∩E˜2 ∗P
E1 ∩E˜2 = 0.20 ∗0.28 = 0.056 
(6.84) 
˜
˜
˜
P
E˜3 ∩E˜1 ∩E˜2 = P
E3|E1 ∩E˜2 ∗P
E1 ∩E˜2 = 0.80 ∗0.28 = 0.224 
(6.85) 
If we redeﬁne as follows: 
E3 
E1 
E2 
J1
1 
1 
1 
p(1) 
J2
0 
1 
1 
p(2) 
J3
1 
0 
1 
p(3) 
J4
0 
0 
1 
p(4) 
(6.86) 
J5
1 
1 
0 
p(5) 
J6
0 
1 
0 
p(6) 
J7
1 
0 
0 
p(7) 
J8
0 
0 
0 
p(8) 

86 
CHAPTER 6 JOINT PROBABILITIES 
we then have 
p(1) = 0.009 
(6.87) 
p(2) = 0.171 
(6.88) 
p(3) = 0.012 
(6.89) 
p(4) = 0.108 
(6.90) 
p(5) = 0.042 
(6.91) 
p(6) = 0.378 
(6.92) 
p(7) = 0.056 
(6.93) 
p(8) = 0.224 
(6.94) 
that is, thanks to the assumption of independence, we have determined the 8 − 1 joint 
probabilities from two marginal probabilities and four conditional probabilities. (The −1
2n 
of course comes from the normalization condition 
i=1 p(i) = 1.) Reassuringly, all these 
probabilities nicely add up to 1. And we did it by assigning six numbers instead of seven! 
You may not be overwhelmed, but the savings become substantial as soon as we begin to 
deal with a larger number of variables. 
Clearly, there is no ‘universal trick’ that can conjure information out of thin air. The 
success of a strategy like the one above is predicated on the nature of the problem; that is, 
on whether it is indeed possible to identify independence between some of the variables. 
As we shall see in the following, even when full independence is not there we can still be 
helped by invoking the related property of conditional independence. The general point does 
not change, however: whether conditional independence can or cannot be invoked depends 
on the nature of the problem, not on our desire to make the task at hand more manageable. 
6.8 Conditional Independence 
As we saw above, when we can safely assume that two variables are independent the 
probabilistic problem simpliﬁes considerably. Full independence, however, is a very strong 
condition, rarely met when one deals with ﬁnancial variables. Conditional independence 
gives us a slightly less powerful, but more easily justiﬁable, tool to simplify the problem 
at hand. Let us see what conditional independence means. In order to help intuition we 
abandon for a moment the world of two-valued Boolean variables, and deal instead with 
Brownian processes, with which most readers are likely to be familiar. 
So, consider for a moment the processes for two asset prices: 
dS1 = µ1dt + σ1
1dw + σ1
2dz1 
(6.95)
S1 
dS2 = µ2dt + σ2
1dw + σ2
2dz2 
(6.96)
S2 

6.8 CONDITIONAL INDEPENDENCE 
87 
Suppose that the Brownian shock dw represents a shock to a common factor (if S1 and 
S2 are equity prices, dw could be a shock to the same equity index of which they are 
components; if they represent the assets of two ﬁrms in a Merton-like model, dw could be a 
shock to the economy; etc.). Clearly, the processes for S1 and S2 are not independent – and 
the greater σ 1 is with respect to σi 
2, the more the two processes will be correlated. However, 
i 
let us assume that 
E [dz1dz2] = 0 
(6.97) 
that is, the idiosyncratic shocks dz1 and dz2 are uncorrelated.4 But, if this is the case, 
conditional on a particular shock, d ˜w, having been realized , the variables S1 and S2 become 
independent: once we know the sign and magnitude of the common shock, dw˜ , knowledge 
of what ‘else’ happened to S1 because of the shock dz1 will not help us at all in predicting 
what S2 does. Even if the unconditional correlation between S1 and S2 were 99.9%, they 
would still remain conditionally independent. 
In this book I do not deal with Brownian processes, but since many readers are likely to 
come from a derivatives or stochastic calculus background, the simple example above can 
give an intuitive understanding of the concept of conditional independence, which I now 
introduce for the case of Boolean variables. 
Given three events, E1, E2 and E3, E1 and E2 are said to be conditionally independent 
given E3 if5 
P (E1|E2, E3) = P (E1|E3) 
(6.98) 
and 
 
 
 
 
P E1|E2, ˜E3 = P E1| ˜E3 
(6.99) 
As Moore (2001) points out, the simplest way to think about conditional independence is 
the following: 
Given knowledge of whether E3 has occurred or not, knowledge of whether E2 has 
happened or not would not help in predicting whether E1 has happened or not. 
With more general notation, if E1 and E2 are conditionally independent given E3, then 
P (E1 = x|E2 = y ∩ E3 = z) = P (E1 = x|E3 = z) 
(6.100) 
where x, y and z, as usual, take on the values x, y, z = T , F . 
4Since we are dealing in this example with a Brownian process, we can interchangeably speak of lack of 
correlation and independence. This is not correct in general (i.e., for non-elliptic distributions). 
5For the deﬁnition of conditional independence to make sense, we must, of course, also require that 
P (E2, E3) ̸= 0. In principle this requires knowledge of the joint probabilities, P (E2, E3), which in general we do 
not have – remember that our plan is to build the joint probabilities from the marginals and conditionals. However, 
the only thing we need to know about P (E2, E3) is that this probability should be non-zero, i.e., that the two 
elementary events, E2, E3, can happen at the same time. 

 
 
 
 
88 
CHAPTER 6 JOINT PROBABILITIES 
Let us see what conditional independence implies. 
We can start from an expression for the joint probability P (E1, E2, E3) in the case when 
E1 and E2 are conditionally independent given E3, that is,  when  P (E1| E2, E3) = P (E1| E3): 
P (E1, E2, E3) = P (E1| E2, E3)P (E2, E3) 
= P (E1| E2, E3)P (E2| E3)P (E3) 
= P (E1| E3)P (E2| E3)P (E3) 
(6.101) 
where the last line follows from the assumed conditional independence. 
However, since P (E1, E2, E3) = P (E2, E1, E3), I can also write 
P (E1, E2, E3) = P (E2, E1, E3) = P (E2| E1, E3)P (E1, E3) 
= P (E2| E1, E3)P (E1| E3)P (E3) 
(6.102) 
Equating Equations (6.101) and (6.102) therefore gives 
P (E1| E2, E3) = P (E1| E3) ⇐⇒ P (E2| E1, E3) = P (E2| E3) 
(6.103) 
In words: if conditioning the probability of E1 on E2 and E3 is the same as conditioning 
the probability of E1 just on E3, then conditioning the probability of E2 on E1 and E3 is 
equivalent to conditioning the probability of E2 just on E3. 
The reader can proﬁtably compare Equation (6.103) with Equations (5.23) and (5.24). 
6.9 Obtaining Joint Probabilities with Conditional 
Independence 
Let us use an example to see how the above can help us in obtaining joint probabilities. 
Consider again three events, E1, E2 and E3, and assume that E1 and E2 are conditionally 
independent given E3. Suppose that we have exogenously established the following:6 
P (E3) = 0.6 
(6.104) 
P (E2| E3) = 0.085 
(6.105) 
P
E2| E˜3 = 0.17 
(6.106) 
P (E1| E3) = 0.3 
(6.107) 
P
E1| E˜3 = 0.6 
(6.108) 
This information and the assumption that E1 and E2 are conditionally independent given E3 
comes from our knowledge of ‘how the world works’. The task, as usual, is to determine 
6This example is also adapted from Moore (2001). 

 
 
 
 
6.10 AT A GLANCE 
89 
the joint probabilities, P (E1 = x ∩ E2 = y ∩ E3 = z), with x, y, z = T , F . The tools we 
are going to use are given by Equations (6.98) to (6.100). 
We start from P (E1, E2, E3): 
P (E1, E2, E3) = P (E1|E2, E3)P (E2, E3) 
= P (E1|E3)P (E2, E3) 
= P (E1|E3)P (E2|E3)P (E3) 
= 0.3 ∗ 0.085 ∗ 0.6 = 0.0153 
(6.109) 
where the left-hand side on the second line follows from the assumption of conditional 
independence. We build another joint probability, say, P (E˜1, E2, E˜3): 
˜
˜
˜
P (E˜1, E2, E3) = P (E˜1|E2, E3)P (E2, E3) 
˜
˜
= P (E˜1|E3)P (E2, E3) 
˜
˜
= P (E˜1|E3)P (E2|E3)P (E˜3) 
(6.110) 
= 0.4 ∗ 0.17 ∗ 0.4 = 0.0272 
(6.111) 
where we have used the fact that 
˜
P
E1|E˜3 + P
E1|E˜3 = 1 
(6.112) 
and, again, the conditional independence. 
Exercise 18 
Complete the derivation of the remaining joint probabilities. 
Exercise 19 
Draw a Venn diagram for three events A, B and C when P (A|B, C) = 
P (A|B). 
Conditional independence will be one of the most powerful tools at our disposal when 
we deal with Bayesian networks. 
6.10 At a Glance 
Here are the relationships we are going to use in the next chapter, neatly collected in 
one place: 
P (B)  = P (B|A)P (A) + P (B|A)P ( ˜
˜
A) 
(6.113) 
P (A  ∩ B)
P (A|B) = 
(6.114)
P (B)  
P (A  ∩ B) = P (A|B)P (B) 
(6.115) 
P (A  ∩ B ∩ C) = P (A|B ∩ C) P (B|C)P (C) = P (C|B ∩ A) P (B|A)P (A) 
(6.116) 
P (A)  + P (  ˜A) = 1 
(6.117) 
P (A|B) + P (A˜|B) = 1 
(6.118) 

90 
CHAPTER 6 JOINT PROBABILITIES 
6.11 Summary 
In this chapter I ﬁrst cleaned up the notation. I then gave the deﬁnition of stand-alone 
(marginal) probabilities for the two-valued Boolean-valued variables (true-or-false events) 
that we are going to use to model stress events. Finally, for the same variables I intro-
duced the concepts of conditional and joint probabilities, and of absolute and conditional 
independence. All the rest has been a manipulation of these fundamental concepts. 
The most important message from this chapter is that, once we have the joint probabilities, 
we have everything else. The converse, however, is in general not true. The joint probabilities 
are king: they contain all the (probabilistic) information we can possibly want. 
Unfortunately it is very difﬁcult to assign or estimate these joint probabilities: there are 
very many of them, and it is difﬁcult even for an expert risk manager to make an informed 
guess. (Would you like to venture a guess, or to estimate from your data, the joint probability 
of the S&P falling by 20% and of the US yield Treasury curve not steepening by at least 
60 bp, and of the winter/summer gas spread falling by more than 40%?) 
Therefore, a lot of non-data information (which we often translate in absolute and con-
ditional independence) must go into the construction of the joint probabilities if we want to 
derive them from quantities for which we have a better intuitive feel, and which are easier to 
estimate from our data. These easier-to-estimate quantities are the marginal and conditional 
probabilities. As I have shown above, unfortunately these quantities rarely deﬁne the whole 
joint distribution – unless, that is, we are prepared to provide very-highly-conditioned prob-
abilities, or to make strong assumptions about the nature of the dependencies among the 
variables at play. Bayesian nets provide a useful tool to organize and visualize the infor-
mation contained in the marginal and conditional probabilities, and to help us in thinking 
correctly. (I discuss this topic in Chapters 8 and 9.) The power of Bayesian nets is remark-
able, and their strengths come via the exploitation of conditional independence and the 
simpliﬁcation of the causal structure among the underlying variables. This simpliﬁcation, 
as I will show, reduces the degree of conditioning of the probabilities required to obtain 
exactly the joint distribution. 
Sometimes, however, the risk manager may feel too daunted by the prospect of supplying 
even these relatively-lowly-conditioned probabilities. However, I show in the next chapter 
that even limited knowledge of the marginal probabilities and a few singly-conditioned 
probabilities can create very tight bounds for the joint probabilities we want to determine. 
It is to this task that I therefore turn. 
6.12 Suggestions for Further Reading 
The approach I have followed in this chapter mirrors closely that chosen by Moore (2001). 
The conceptual framework of probability is very well presented in Williams (2001). His 
book is as demanding as it is rewarding, and is particularly useful for the reader who wants 
to understand conditional probabilities well, because it takes conditional probabilities as one 
of its central themes. 
A book at a mathematically much simpler level, but with proper care paid to the 
logical aspects of the problems at hand, and of conditional probability in particular, is 

6.12 SUGGESTIONS FOR FURTHER READING 
91 
Hacking (2001). For a gentle introduction to Williams’ book, I recommend Stirzacker (1999). 
Stirzacker’s emphasis on Venn diagrams is particularly consonant with the approach taken 
in this book. He also provides a brief introduction to networks (trees and graphs), which, 
however, only touches on some of the aspects needed for the Bayesian network approach 
presented in the following chapters. 


Chapter 7 
Creating Probability Bounds 
7.1 The Lay of the Land 
As far as probabilities are concerned, our ﬁnal goal is obtaining joint probabilities.1 For n 
variables, we have seen that to do so we have to specify (n − 1)-conditioned and marginal 
probabilities: see, for instance, Equation (6.12) in Chapter 6. I will derive more, and more 
handy, expressions in the following pages (i.e., expressions in terms of different conditioned 
and marginal probabilities), but the general result does not change. For n greater than about 3, 
the task would therefore seem hopeless. 
Our strategy to escape this curse of dimensionality will be two-fold: ﬁrst we will so 
restrict the type of causal links among our variables as to require at-most-doubly-conditioned 
probabilities. Conditional independence will then be the indispensable tool to obtain the joint 
probabilities from these at-most-doubly-conditioned ones. 
Even assigning doubly-conditioned probabilities on the basis of expert judgement may be 
too much for the cognitive abilities of human beings, let alone risk managers. Fortunately, I 
show in this chapter how, from the knowledge of marginal and singly-conditioned probabil-
ities, it is possible to obtain usefully tight bounds on the joint probabilities that we are after. 
And if the risk manager feels bold enough to provide just one or two doubly-conditioned 
probabilities, the bounds on the joint probabilities become very tight indeed. 
Having bounds rather than point estimates is not necessarily a bad thing, because they 
can give us an idea of the uncertainty around our stress estimates. I deal with this topic in 
Part III. My current task is to derive these bounds. 
7.2 Bounds on Joint Probabilities 
So, if I have  n Boolean variables, in order to determine uniquely the associated joint prob-
abilities, I need to specify all the independent (n − 1)-conditioned probabilities. Consider, 
1This ‘ﬁnal’ goal should be considered ﬁnal only from a probabilistic point of view. As I discussed at the 
end of Chapter 4, there is more to life, and to quantitative risk management, than determining joint probabilities 
(which underpin an associative view of events). Causal links are richer and more powerful. 
93 

94 
CHAPTER 7 CREATING PROBABILITY BOUNDS 
for instance, n = 3: 
P (A,  B,  C)  = P (A|B, C)P (B, C) = 
P (A|B, C)P (B|C)P (C) 
(7.1) 
In general 
P (A,  B,  C,  . . ., Z)  = P (A|B, C, . . ., Z)P (B, C, . . ., Z)  
(7.2) 
and so on. When Bayesian nets are introduced I will show that, by restricting our analysis 
to a subset of the most general nets, we can reduce the problem to, say, providing at most 
doubly-conditioned probabilities. But I have also pointed out that, in practice, even this can 
become a daunting task. (See also Chapter 12.) Therefore, In this section I draw directly 
from work by Moskowitz and Sarin (1983) to show that very useful bounds can be set 
on the joint probabilities even if only the marginal and the singly-conditioned probabilities 
are provided – and, sometimes, even if only the marginals are available. Following their 
treatment the reasoning goes as follows. 
Consider again the following table: 
E1 
E2 
E3 
J1 
1 
1 
1 
p(1) 
J2 
0 
1 
1 
p(2) 
J3 
1 
0 
1 
p(3) 
J4 
0 
0 
1 
p(4) 
(7.3) 
J5 
1 
1 
0 
p(5) 
J6 
0 
1 
0 
p(6) 
J7 
1 
0 
0 
p(7) 
J8 
0 
0 
0 
p(8) 
Deﬁne by ai the column vector containing 1s or 0s for event i. Its  jth element will be 
denoted by ai
j . So, for instance, 
⎤
⎡ 
a1 = 
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
1 
0 
1 
0 
1 
0 
1 
0 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
(7.4) 
3
and a1 = 1 and  a 8 = 0 for any i.
i 
Now, denote by p the column vector of components p(k) = pk, containing the n joint 
probabilities, and pT its transpose. Let, as usual, P (Ei) be the marginal probability and 

	 

 
7.2 BOUNDS ON JOINT PROBABILITIES 
95 
P (Ei, Ej)  and P (Ei.Ej, Ek) be the joint probabilities. Then, given these deﬁnitions it is 
straightforward to prove that, for n = 3, 
P (Ei) = p T · ai for i = 1, 2, 3 
(7.5) 
P (Ei, Ej) = p T · (ai ∗ aj) for i = 1, 2 
(7.6) 
P (Ei, Ej, Ek) = p T · (ai ∗ aj ∗ ak) for i = 1 
(7.7) 
where the symbol ‘·’ denotes the row-by-line product between two vectors, and ai ∗ aj indi-
cates the column vector whose elements are given by the element-by-element multiplication 
of the vectors ai and aj. 
Exercise 20 
Prove the three relationships above. 
The generalization of the results to the case of n variables above is obvious: 
P (Ei) = p T · ai for i = 1, . . ., n  
(7.8) 
P (Ei, Ej) = p T · (ai ∗ aj) for i = 1, . . ., n  − 1, j > i 
(7.9) 
P (Ei, Ej, Ek) = p T · (ai ∗ aj ∗ ak) for i = 1, . . ., n  − 2, k > j > i 
(7.10) 
. . .  
P (E1, E2, E3, . . ., En) = p T · (a1 ∗ a2 ∗ a3 ∗ . . .  ∗ an) 
(7.11) 
In addition we have the usual normalization condition: 
2n
 
pi = 1 
(7.12) 
i=1 
As we have seen, the fullest speciﬁcation of the joint probabilities can be given in terms 
of (n − 1)-conditional and marginal probabilities. Suppose, however, that the risk manager 
only felt able to supply a subset of the conditions above, say, the normalization condition, 
the marginals and the singly-conditioned probabilities, P (Ei|Ej). (Of course, providing the 
marginals and the singly-conditioned probabilities is equivalent to supplying the two-event 
joint probabilities, as, for any j and k, P (j, k)  = P (j|k)P (k)). Call this subset X: 
2n 
 
X = 
pi = 1∪ 
i=1 
P (Ei) = p T · ai for i = 1, . . ., n  ∪ 
P (Ei|Ej)P (Ej) = p T · (ai ∗ aj) for i = 1, . . ., n  − 1, j > i 

96 
CHAPTER 7 CREATING PROBABILITY BOUNDS 
Consider now the j th joint probability, pj . We can determine an upper or a lower bound 
for this probability by solving the simple Linear Programming2 problems 
Minimize pj subject to p ∈ X 
(7.13) 
and 
Maximize pj subject to p ∈ X 
(7.14) 
where the expression p ∈ X means that the joint probabilities p must belong to the constraint 
set X. Of course, the more elements in the set X, the tighter the bounds we will obtain. The 
good news, however, is that even if the set X is rather small (i.e., even if the risk manager 
only feels conﬁdent to provide relatively few probabilities), the bounds can still be usefully 
tight. (See the examples below.) 
Following the same approach, we can ask a lot of related and useful questions. For 
instance, as Moskowitz and Sarin (1983) point out, we can ask whether one joint scenario, 
say, Jr , has a higher probability than another joint scenario, say, Js . We can answer the 
question by deﬁning the variable pr − ps , and checking whether the solution to 
Minimize pr − ps subject to p ∈ X 
(7.15) 
gives an answer greater than 0. 
7.3 How Tight are these Bounds in Practice? 
Bounds are all well and good, but to be useful they have to be reasonably tight. The good 
news is that even very sparse information can produce highly useful bounds. I ﬁrst present 
the results obtained by Moskowitz and Sarin (1983), and then the results obtained for an 
example that is of greater relevance to the low probabilities found in stress testing. 
Moskowitz and Sarin looked at a 3-event and a 4-event case. For each one, ﬁrst they 
randomly sampled from the uniform, U[0, 1], distribution, but the assignments of the num-
bers thus drawn to the marginal or conditional probabilities were made by ensuring internal 
consistency (i.e., by ensuring that the numbers chosen could be marginal and conditional 
probabilities – see Chapter 11). As a next step, Moskowitz and Sarin considered two cases: 
either they availed themselves of the knowledge just of marginals or they assumed that 
they had at their disposal both the marginal and the singly-conditioned probabilities. Using 
the Linear Programming approach described above they then determined the bounds to the 
joint probabilities. Finally, they repeated this procedure 100 times for each of the four cases 
(3- or 4-event, marginals only or marginals plus conditionals). Their results were as follows: 
• 3-event case, only marginal probabilities provided: in 95% of the cases the difference 
between upper and lower bound for the joint probabilities was ≤ 0.1641. 
2I provide a simple introduction to Linear Programming in the Appendix in Chapter 15. 

7.3 HOW TIGHT ARE THESE BOUNDS IN PRACTICE? 
97 
• 3-event case, marginals and conditionals provided: in 95% of the cases the difference 
between upper and lower bound for the joint probabilities was ≤ 0.0567. 
• 4-event case, only marginal probabilities provided: in 95% of the cases the difference 
between upper and lower bound for the joint probabilities was ≤ 0.058. 
• 4-event case, marginals and conditionals provided: in 95% of the cases the difference 
between upper and lower bound for the joint probabilities was ≤ 0.0073. 
Two observations. The ﬁrst is obvious: supplying also the conditional probabilities makes 
the bounds tighter. The second observation is not so obvious: increasing the number of 
variables tightens the bounds. The intuition behind this second result is that new vari-
ables bring in more and more hyperplanes of acceptability, which progressively limit the 
solution space. 
So much for tests with the probabilities drawn from the uniform, U[0, 1], distribution. 
What about the situation when the probabilities are of magnitudes more likely to be met 
in stress-testing situations? To answer this question, look at the case of four variables with 
joint probabilities given by 
⎤
⎡ 
E1 
E2 
E3 
E4 
p(i) 
1 
0 
1 
1 
0.001369 
0 
1 
1 
1 
0.000006 
1 
1 
1 
1 
0.000028 
0 
0 
1 
1 
0.000016 
1 
0 
0 
1 
0.000013 
0 
1 
0 
1 
0.000034 
1 
1 
0 
1 
0.000010 
0 
0 
0 
1 
0.000123 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
(7.16) 
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
J1 
J2 
J3 
J4 
J5 
J6 
J7 
J8 
J9 
1 
0 
1 
0 
0.000529 
J10 
0 
1 
1 
0 
0.000248 
J11 
1 
1 
1 
0 
0.000980 
J12 
0 
0 
1 
0 
0.000023 
J13 
1 
0 
0 
0 
0.000081 
J14 
0 
1 
0 
0 
0.000104 
J15 
1 
1 
0 
0 
0.000090 
J16 
0 
0 
0 
0 
0.996345 
and marginal probabilities given by 
P (E1) = 0.00310 
(7.17) 
P (E2) = 0.00150 
(7.18) 
P (E3) = 0.00320 
(7.19) 
P (E4) = 0.00160 
(7.20) 
We assume that the risk manager only feels conﬁdent enough to provide the marginal 
probabilities. Normally we should be able to do better than this, but we want to see to what 

98 
CHAPTER 7 CREATING PROBABILITY BOUNDS 
extent even very limited information can impose useful bounds on the joint probabilities. 
Of course, the table of joint probabilities in Equation (7.16) is not assumed to be known by 
the risk manager – it is only presented to allow the reader to ‘peek behind the curtains’. 
Now, Linear Programming is a beautiful tool, but rather opaque. For this super-simple 
example I am therefore going to take a more transparent approach. Here is one intuitive way 
to look at the problem: the four marginal probabilities pin down any four joint probabilities. 
To make my algebra easy I have chosen these four joint probabilities to be associated with 
events J1, J9, J10 and J12. 
Exercise 21 
For the net depicted in Figure 11.1 (page 157), derive the expression for 
p(1), p(9), p(10) and p(12) in terms of the pre-assigned marginal probabilities P (Ei), 
i = 1, . . ., n, and other joint probabilities. 
The constraint from the normalization condition then pins down another joint probability, 
which I have chosen to be the one associated with J16. I can then move the remaining 
joint probabilities – p(2), p(3), p(4), p(5), p(6), p(7), p(8), p(11), p(13), p(14) and 
p(15) – from their true value (in correspondence of which there obviously exists a solution) 
until any of the other joint probabilities becomes either negative or greater than 1. 
It is then not difﬁcult, following the approach described above, to show the following: 
• 0 ≤ p(2) ≤ 0.00024 
• 0.000005 ≤ p(3) ≤ 0.00026 
• 0 ≤ p(4) ≤ 0.00003 
• 0 ≤ p(5) ≤ 0.0013 
• 0.000014 ≤ p(6) ≤ 0.00024 
• 0 ≤ p(7) ≤ 0.00024 
• 0 ≤ p(8) ≤ 0.0013 
• 0.000960 ≤ p(11) ≤ 0.00118 
• 0.000060 ≤ p(13) ≤ 0.00060 
• 0.000081 ≤ p(14) ≤ 0.00034 
• 0.000079 ≤ p(15) ≤ 0.00033 
For our applications, of course, these bounds must be looked at in the context of stress 
events. From this point of view a bound like the one for p(4) is indeed tight and useful; the 
one for, say, p(8), less so. Recall, however, that no conditional probabilities were provided 
for this analysis. 
These results are very encouraging, and should be used in conjunction with the Bayesian-
net treatment presented in the following chapters. 

Chapter 8 
Bayesian Nets I: An Introduction 
8.1 Bayesian Nets: An Informal Deﬁnition 
Before we get started, let me make my game plan as clear as possible. What we want to get 
are joint probabilities. We have seen that assigning marginal and conditional probabilities 
of low order is in general not enough to pin down the joint probabilities we want (see 
Section 6.6). But we do not think that complex, highly-conditioned probabilities can be 
reliably assigned by a risk manager. At most we are ready to trust order-of-magnitude 
estimates of marginal and simple, lowly-conditioned probabilities. What can we do? 
To get out of this impasse we need extra information, which is provided by the causal 
links among the variables in our problems. Bayesian nets are the tools that will assist us 
with the necessary book-keeping, and more importantly, will help us to think constructively 
about the problem. The rest of the chapters will clarify what ‘thinking constructively’ means. 
For our purposes, Bayesian nets (or networks) can be deﬁned as directed, acyclical 
graphs where each node (or vertex) is associated with a two-valued (Boolean) random 
variable and with a list of numbers called a conditional probability table. Let us see what 
this means in detail. 
I will discuss conditional probability tables later on in Section 8.4. Let me therefore 
start to explain what directed, acyclical graphs are. A graph is a set of points (the nodes 
that represent random variables) connected by lines (arcs or edges). For the applications 
of interest to us, the edges are directed – they have an arrow that goes from one node 
to another. And for our applications the directed graphs we work with are acyclical. This 
means that, starting from any node, we cannot go back to the same node following the 
arrows that connect the other nodes. 
A few more pieces of terminology: ﬁrst, if an arrow points from node A to node B, node A 
is said to be the parent of node B (the child ) (see Figure 8.1). Then, the parents, the parents 
of the parents of a node, their parents, the parents of these and so on, are collectively called 
the ancestors. Similarly, the children, the children of children, their children, the children 
of these, and so on are called the descendants . 
99 

100 
CHAPTER 8 BAYESIAN NETS I: AN INTRODUCTION 
A 
F 
B 
C 
D 
E 
Figure 8.1 In this net, A causes B and C, F causes C, B and C causes D and C causes E. 
The power of Bayesian nets comes from the fact that we can interpret the existence of 
an arrow between two nodes as representing a causal link between the associated random 
variables. This is where we begin to enrich the associative approach to uncertain events 
afforded by a purely probabilistic treatment. We do so by injecting information about how 
the world works. I have discussed in Chapter 3 and at the end of Chapter 4 how important 
and powerful this approach can be. 
What do we mean by ‘causal link’? As a bare minimum, when we say that there is a 
causal link between A and B we are saying that knowledge of A (the parent) will help 
us in our assessment of the probability of B (the child) happening.1 This is always true. 
However, there is more to a causal link than an alteration of their marginal probabilities. 
For instance, knowing that on a given day it was raining changes my probability assessment 
that that day was in autumn. Conversely, knowing that it is autumn changes my assessment 
of the marginal probability that it will rain tomorrow. Both marginal (prior) probabilities are 
modiﬁed by knowledge of occurrence of the other event. However, we do not think that a 
rainy day ‘causes’ the season to become autumn. A causal relationship is linked to a model 
of how the world works. Availability of such a model helps our understanding, tells us what 
is essential and what is accidental, and alerts us to when the world has changed – and when, 
therefore, we should abandon the model.2 It is no surprise, then, that when we can establish 
a causal link, our minds ﬁnd assigning conditional probabilities much easier and ‘natural’.3 
This last feature of causal links is particularly useful because, as we shall see, in order 
to use Bayesian nets we have to associate to each node either a marginal probability or a 
conditional probability table. Once this information is given, Bayesian nets provide a full 
graphical (and, via the tables, numerical) representation of the joint distribution. But the 
construction of these conditional probability tables can be carried out much more efﬁciently 
if we are aware, and fully exploit, the causal structure of the underlying phenomenon. If 
you need convincing about this, you may want to revisit the wet pavement example (see 
also Pearl (2009)) presented in Chapter 4, Section 4.4. 
This point is important, so let me clarify it a bit. What do I mean when I say that under-
standing of the causal structure of the problem can make the construction of the Bayesian 
nets more efﬁcient? Bayesian nets are made up of nodes, lines, arrows and conditional 
probability tables . Not surprisingly, these are populated by conditional probabilities. Now, 
different conditional probabilities (say, P (A|B) rather than P (B|A)) may well be equiva-
lent from the mathematical point of view (and are linked, once the marginals are given, by 
1This is just a necessary condition for causation. See the discussion below. 
2It is for this reason that the availability of alternative models of reality discussed in Chapter 3 is so important. 
3Truth be told, sometimes causal links can make us over estimate the revision of the stand-alone probability. 
See the discussion in Chapter 12. 

8.2 DEFINING THE STRUCTURE OF BAYESIAN NETS 
101 
Bayes’ theorem). However, very often they are not at all as easy to assign (cognitively 
resonant) during the process of expert elicitation. Choosing the causal rather than diagnostic 
‘direction’ (see Section 12.4) can make an enormous difference in producing a good-quality 
conditional–probability table. We shall see many examples of this in the following. 
Keeping these considerations in mind, in order to make use of Bayesian nets we have to 
be able to answer a few questions: 
1. How do we ‘map’ the causal relationships among the variables into the structure (arcs 
and arrows) or the net? I’ll answer these questions in the next section. 
2. What goes in the conditional probability tables? This is dealt with in Section 8.4 
3. Assuming that the conditional probability tables have been given, how do we go 
from these to the joint probabilities? I’ll also deal with this topic in this chapter 
(Section 8.7). 
4. Now that we know what we should put in the conditional probability tables, how do 
we actually populate their entries? How do we assign the conditional probabilities 
that are required? I deal with this question in Chapters 9 and 10. 
Once all these questions have been answered the structure of the Bayesian net and the 
inputs required to use it will be in place, and I will show in Chapter 11 how to put all the 
pieces together and obtain a solution (a set of joint probabilities) in the general case. 
8.2 Deﬁning the Structure of Bayesian Nets 
How do we build Bayesian nets? More precisely, how do we translate the causal links 
among the variables into the topology of a Bayesian net? 
First, we must identify the variables relevant to our application (in our context, to our 
joint scenario). Each variable will typically be a stand-alone stress event. Each event can 
either happen (in which case the variable has value T ) or not (in which case it has value F ). 
Next, we associate each variable to a node. 
As a next step we draw arcs between the nodes. We do so keeping in mind that 
whenever we draw an arc (a line) between two nodes we mean that there is a causal link 
between the two variables. 
Now, wherever we ﬁnd a line we put an arrow at one end or the other. The causal link 
goes in the direction of the arrow: from the parent to the child . 
Causality is a very complex topic. Philosophers have been discussing it for about 2500 
years, and they are not quite done yet. I am therefore understandably reluctant to enter the 
fray. Still, it is important to make a few, uncontroversial, points. 
First of all, a necessary condition for the existence of a causal link is that knowledge 
of whether the parent Boolean random variable is true or false should help us in telling 
whether the child Boolean random variable is true or false. It may help us a lot (perhaps 
completely, in the case of deterministic causation), or very little, but this is a different 
matter.4 I stress that also knowledge of whether a child is true or false can help in deciding 
4For pragmatic reasons, very often we will set very low degrees of causation (very low degrees of inﬂuence) 
to zero in order to simplify our life, but this is (yet) another matter. 

102 
CHAPTER 8 BAYESIAN NETS I: AN INTRODUCTION 
whether the parent was true or false. Therefore the criterion I gave above (help in prediction) 
is a necessary but not sufﬁcient condition for causation. To be clear: if knowledge of A does 
not help us at all in predicting whether B is true or false, then A cannot possibly be a cause 
for B, but the reverse is not true: A (a sudden large increase in equity implied volatilities) 
may help us in determining whether B (an equity market crash) occurred, but may be the 
consequence not the cause of B. 
Having settled this point about causation, we can begin to look in turn at our variables 
(our elementary stress events). Given our knowledge of the world, there will be a set of 
variables that have no parents (those we believe are caused by no other variables in our set). 
We will draw these primary parents in such a way that no arrows point into them. They 
will only have arrows originating away from them. In Figure 8.1, variables A and F are the 
only primary parents: either one or the other directly or indirectly inﬂuences all the other 
variables, but they are not inﬂuenced by any of them (and they do not inﬂuence each other). 
Note already how the graphical depiction naturally reﬂects this concept, and therefore aids 
our intuition. 
One more comment about these Ur-parents in Figure 8.1, nodes A and F. We  have  
decided to draw no connecting lines between them, and they have no common parents. 
This means that we believe that they are statistically independent : P (A|F) = P (A)  and 
P (F|A) = P (F). 
We have seen that the existence of an arrow indicates causal relationship from parent to 
child. If the causal relationship is deterministic this is signiﬁed by putting 0s or 1s, as appro-
priate, in the conditional probability table – this is what we called above incompatibility 
and deterministic causation. This is a rather trivial result. What is far more interesting is that 
the conditional probability tables we associate with nodes can allow us to express far more 
nuanced relationships between variables than incompatibility or deterministic causation. In 
particular, our understanding of the nature of the causal relationships among the variables 
will enable us to embed in the structure of the net not only absolute, but also conditional, 
independence. How can this be done? 
Simply by exploiting the fact that, once we have drawn all the nodes and arrows in a 
Bayesian net, given the parents of node X, node X is conditionally independent of all the 
nodes that are not its own descendants . So, in Figure 8.1, conditional on C and B, node 
D is conditionally independent of A and E. It is this condition that dramatically reduces 
the entries in the conditional probability table required to compute the joint probabilities. 
At this stage I will say only that this is the case because providing probabilities of D 
conditioned on events other than B or C would be informationally redundant once the 
quantities P (D  = x|B = y, C = z) have been provided. This may not be totally clear at 
this stage, and I will discuss this important point at length in this chapter. 
These ideas can be made more precise by introducing the concept of d-separation.5 Here 
is how it works. Let Y, Z and X be sets of nodes. Then the set of nodes X is said to 
d-separate sets Y and Z if every undirected path from a node in set Y to a node in set Z 
is blocked by X. It is then easy to show that, if X d-separates Y and Z, then Y and Z are 
conditionally independent given X. 
5The d in d-separation stands for ‘direction-dependent’. 

8.2 DEFINING THE STRUCTURE OF BAYESIAN NETS 
103 
Y 
A 
F 
X 
B 
C 
Z 
D 
E 
Figure 8.2 In this net, the block X d-separates blocks Z and Y. 
Let us look at what this means in practice with reference to Figure 8.2. Let Y = {A&F}, 
Z = {D&E} and X = {B&C}. Then, by the deﬁnition above X d-separates Y and Z. This 
means that any node in Z is conditionally independent of any node in Y given X.6 
Therefore, when we build Bayesian nets we make the fundamental assumption that, 
conditional on its parents , any variable is independent of all other variables apart from its 
descendants. Looking back at Figure 8.1, this means that, say, D clearly depends on A, F, 
B and C, but, once we know B and C, there is no residual A- and  F-dependence on D: 
knowledge of A and F does not help us in predicting D any more than just knowledge of 
B and C does. This also means that there is no path-dependence in our net : the probability 
of D happening does not depend on whether C was caused by A or F. 
This condition of path independence is very closely linked to the Markov condition, 
and plays an essential role in the developments below. Why Markov? For a time-ordered 
process, Xi, we commonly call it a Markov process if 
P (Xk|Xk−1, Xk−2, . . .  , Xk−n) = P (Xk|Xk−1) 
(8.1) 
that is, if knowledge of the realization of the process at times earlier than k −1 does not 
help the time-k −1 prediction of its realization at time k. It is easy to see the analogy in 
the case of Bayesian nets: ancestors other than parents are equivalent to ‘earlier events’ that 
do not affect the probability of the current realization.7 
Conditional independence is so central to Bayesian nets that I discuss this point in more 
detail in the next section. 
Exercise 22 We know from Section 6.8 (Equation (6.103) in particular) that, if P (E|D, C) 
= P (E|C) then P (D|E, C) = P (D|C). Convince yourself that this is borne out by the 
topology of our Bayesian net. 
6In providing the criterion for conditional independence based on d-separation I have cheated a bit, because I 
have not really deﬁned what I mean when I say that the set Z ‘blocks off’ the sets X and Y. 
Explaining ‘blocking’ in a precise manner is actually very easy, but rather tedious, and I trust that the reader 
can go with the ﬂow. If desired, a precise deﬁnition can be found in Pearl (2009). 
7In the Bayesian nets we work with in this book there is no time dimension. It is possible to interpret the 
arrows as conveying time ordering, but this route is not taken in the present treatment. 

104 
CHAPTER 8 BAYESIAN NETS I: AN INTRODUCTION 
Exercise 23 Looking at Figure 8.1, are variables A and E correlated? What about B and 
C? A and F? 
8.3 More About Conditional Independence 
When I introduced d-separation I stated that, conditional on its parents, any variable is 
independent of all other variables apart from its descendants . What does this last qualiﬁca-
tion mean? In general a node is not conditionally independent of its descendants given its 
parents. Looking at Figure 8.3, this means that D is conditionally independent of A and F 
given B and C (which together d-separate {A&F} from {D&E}): 
P (D|A, F, B, C) = P (D|B, C) 
(8.2) 
but it is not conditionally independent of G: 
P (D|A, F, B, C, G) ̸= P (D|B, C) 
(8.3) 
This is because knowledge of whether G is true or false does help in deciding whether 
its parent occurred or not: if D is an equity market crash, and G a sharp increase in 
equity implied volatilities (‘caused’ by the equity market crash), observing that the sharp 
rise in volatility has not occurred can help us (a lot) in deciding whether the equity crash 
occurred – probably not. 
Therefore, even if a node only has, say, two parents, we cannot say that, in general, there 
can be at-most-doubly-conditioned probabilities associated with it. Since what we want to 
obtain are joint probabilities this seems to create a big problem: was the strategy advertised 
in the ﬁrst paragraph of this chapter not predicated on the need to provide only simple, low-
conditioned probabilities? Fortunately, when we consider joint probabilities we can always 
order the variables in such a way that the conditional probability tables required to build 
them only contain at-most-m-conditioned probabilities, where m is the maximum number of 
parents a node can have. As a consequence, we can still always build the joint distribution 
by supplying at-most-m-conditioned probabilities. 
Let us see how this can be done. To begin, note that any Bayesian net (which is made 
up, remember, of a directed, acyclical graph) induces a partial ordering among its variables. 
A 
F 
B 
C 
D 
E 
G 
Figure 8.3 In this net, D is conditionally independent of A and F given B and C, but is not 
conditionally independent of G. 

8.3 MORE ABOUT CONDITIONAL INDEPENDENCE 
105 
Inducing an ordering means establishing a rule by means of which, given A and B, we can 
always say whether A ≥ B or B ≥ A. For acyclical graphs this ordering can be achieved 
by deﬁning 
X ≤ Y if and only if X is a descendant of Y 
(8.4) 
So, for the net in Figure 8.3, G ≤ D ≤ B ≤ A. 
Now, we can always break down a joint probability in such a way that the conditional 
probability for a node has only non-descendants in the condition. It is thanks to this property 
that in a Bayesian net with at most m parents per node the conditional probability tables 
necessary to build the joint probabilities only contain at-most-m-conditioned probabilities. 
Let us see how this works. Looking at Figure 8.3 again, consider the joint probability 
P (A,  B,  C,  D,  E,  F,  G). Thanks to the ordering deﬁned above, I can rearrange the variables 
from ‘smallest’ to ‘largest’ as 
P (A,  B,  C,  D,  E,  F,  G)  = P (G,  D,  E,  B,  C,  F,  A)  
(8.5) 
Therefore 
P (G,  D,  E,  B,  C,  F,  A)  = P (G|D, E, B, C, F, A)P (D, E, B, C, F, A) 
= P (G|D)P (D, E, B, C, F, A) = P (G|D)P (D|E, B, C, F, A)P (E, B, C, F, A) 
= P (G|D)P (D|B, C)P (E|B, C, F, A)P (B, C, F, A) 
= P (G|D)P (D|B, C)P (E|C)P (B, C, F, A) 
= P (G|D)P (D|B, C)P (E|C)P (B|C, F, A)P (C, F, A) 
= P (G|D)P (D|B, C)P (E|C)P (B|A)P  (C, F, A)  
= P (G|D)P (D|B, C)P (E|C)P (B|A)P (C|F, A)P (F, A) 
= P (G|D)P (D|B, C)P (E|C)P (B|A)P (C|F, A)P (A)P (F) 
(8.6) 
This example shows that, in order to calculate the joint probability P (A,  B,  C,  D,  E,  F,  G), 
we have used at-most-doubly-conditioned probabilities despite the fact that D is not condi-
tionally independent of its descendant G, given  B and C. 
The reader can easily prove to herself that this property holds true in general, and that 
there was nothing special about the topology of the net in Figure 8.3 that allowed us to 
derive this result. 
For short I will therefore often say in the following that, for a Bayesian net with at 
most m parents we will need at-most-m-conditioned probabilities to build all the joint 
probabilities, without specifying that this holds after a suitable rearrangement of the 
variables in ascending order. 
A last observation is worth repeating: in the chain of equalities above I have set 
P (B|C, A) = P (B|A). This is not because C does not help us in predicting whether B 
happened – in general it does! For instance, now let A be an equity market crash, B a sharp  
increase in implied volatilities and C a widening of credit spreads. Knowing that credit 
spreads have widened (C = T ) does help us in adjusting our unconditional probability, 

 
 
106 
CHAPTER 8 BAYESIAN NETS I: AN INTRODUCTION 
P (B), of a sharp increase in implied volatilities: 
P (B)  ̸= P (B|C) 
(8.7) 
However, once we are told whether the equity market crash (the common parent) happened 
or not , there is no extra information left in C! The additional information about B (the 
increase in implied volatilities) that we could extract from knowledge of the occurrence 
or otherwise of the credit spread widening, C, was only ‘coming from’ our knowledge of 
whether the equity market crash, A, had occurred or not. Therefore, indeed, 
P (B|A, C) = P (B|A) 
(8.8) 
This is all we need to know to translate our understanding of the causal links among the 
variables at play into a topological structure for the net. The topology is not just made up 
of the nodes, lines and arrows. It also establishes an ordering among the variables and tells 
us which entries of the conditional probability tables are required (or, rather, which entries 
convey non-redundant information to build the joint probabilities). We have not been able 
to say anything yet about the strength of the causal links. To do this we have to provide 
conditional probability tables. There is one class of probability table per structure of Bayesian 
net – where by structure I mean order of conditioning and number of entries in the table. It 
is the actual numerical numbers we put in the tables that quantify the strength of the links. 
8.4 What Goes in the Conditional Probability Tables? 
In general, given n variables with an unspeciﬁed causal structure among them, a priori 
there is information in all the conditional probabilities, that is, in very complex quantities 
such as, say, P (E1, E2, . . .  , Ej |Ek, El, . . .  , En). However, once the topology of the net has 
been assigned (i.e., once we have speciﬁed the causal structure among the variables) only 
a very small subset of the whole universe of conditional probabilities will be needed. We 
now examine in detail and systematically what goes in the tables. 
We can start from the earliest ancestors.8 If a node has no parents (such as node A 
in Figure 8.1), we assign a marginal probability to the event associated with the random 
variable of that node. For the earliest ancestors, this is all we need. 
We next consider any of the descendants of A, not necessarily a child of A. Let  us  
consider, for instance, node D. A conditional probability table for D lists all the conditional 
probabilities of D being true or false given its parent(s) being true or false. So, for instance, 
the conditional probability table for node D in Figure 8.1 is given by 
P (D|C ∩B) P (D|C˜ ∩B) 
B) P (D|C ∩˜
(8.9)
P (D|C ∩˜
˜
B) 
8In the actual computational we start from the opposite end, i.e., from the children. However, in order to see 
what the conditional probability tables should contain it is better to start from the opposite end of the net. 

 




 
8.5 USEFUL RELATIONSHIPS 
107 
plus its ‘mirror image’ 
D|C˜ ∩B) 
D|
P (
P (
D|C ∩B) 
D|
P (
(8.10)
C ∩˜B) P (
C˜ ∩˜B) 
This entails a very substantial reduction of the non-zero conditional probabilities connected 
with node D: for instance, P (D|A, F) is certainly not zero, and in itself, carries information. 
But, given the topology of a particular net, we know which conditional probabilities carry 
the minimal information required to build the joint probabilities. Therefore we do not have 
to assign the conditional probability P (D|A, F) in order to build the joint probabilities. The 
point is that the entry P (D|A, F) will carry no extra information that we cannot recover 
once the conditional probabilities in the tables in Equations (8.9) and (8.10) have been given 
(and, of course, the marginal probability for A).9 
We proceed in the same manner for all the descendants of the ancestors. In general each 
node will contain the marginal probability for that node, and conditioned probabilities of 
order as high as the number of direct parents into that node. And, of course, the conditioning 
will then only be on the parents of each node. 
8.5 Useful Relationships 
We saw in Section 8.3 that the ability to rearrange the variables of a joint probability accord-
ing to their ordering was crucial in reducing the highest order of conditionality required to 
obtain any joint probability. This is but one of the relationships that we will use in order to 
ﬁll efﬁciently the conditional probability matrices. 
Here is a useful list of simple relationships that we will use over and over again when 
we tackle real applications. 
1. Breaking down the joint 
P (E1 = x ∩E2 = y ∩. . .  En = z) 
= P (E1 = x|E2 = y ∩. . .  En = z)P (E2 = y ∩. . .  En = z) 
(8.11) 
with, as usual, x, y, . . .  , z  = T , F . 
Lightening the notation, we can present the breaking-down-the-joint rule as follows: 
P (E1, E2, . . .  , En) = P (E1|E2, . . .  , En)P (E2, . . .  , En) 
= P (E1|E2, . . .  , En)P (E2|. . .  , En)P (E3, . . .  , En) = . . .  
= P (E1|E2, . . .  , En)P (E2|. . .  , En)P (E3|. . .  , En). . .  , P (En−1|En)P (En) (8.12) 
9In case the reader is daunted by the number of entries required to populate the table, let me make clear 
straightaway that most of these entries can be derived from the applications of Bayes’ theorem, and from the 
closure relationships. 

108 
CHAPTER 8 BAYESIAN NETS I: AN INTRODUCTION 
So, given an n-dimensional joint distribution we can always break it down into the product of 
an n − 1-conditioned probability, an n − 2-conditioned probability, . . . , a singly-conditioned 
probability and a marginal probability. 
2. Order of conditioning 
As discussed above, if we have n two-valued Boolean variables, and each can have at most 
m parents, then the speciﬁcation of all the n-dimensional joint probabilities never requires 
anything more complex than m-conditioned probabilities. 
3. Commutativity 
We can reorder variables without crossing a conditionality sign (|·| ) any way we like: 
P (E1 = x| En = y, . . .  , E2 = z) = P (E1 = x| E2 = y, . . .  , En = z) 
(8.13) 
P (E1 = x, E2 = y, . . .  , En = z) = P (En = x, En− 1 = y, . . .  , E1 = z) 
(8.14) 
P (E1 = x, E2 = y, . . .  , Ek = w| En = z) = P (Ek = x, Ek− 1 = y, . . .  , E1 = w| En = z) 
(8.15) 
Trivial at it may seem this condition is extremely useful, because it allows us to rearrange 
variables in a way that the conditional probability for a given node only has non-descendants 
in the breakdown of the joint above. Putting the breakdown rule and the commutativity 
property together we can always write the joint probabilities in terms of the entries in the 
conditional probability tables. 
4. Closure 
P (E1 = x| E2 = y ∩ . . .  En = z) + P (E˜1 = x| E2 = y ∩ . . .  En = z) = 1 
(8.16) 
5. Conditional independence 
If the probability of E1 given E2 is equal to the probability of E1, then the probability of 
E2 given E1 is equal to the probability of E2: 
P (E1| E2) = P (E1) ⇐⇒ P (E2| E1) = P (E2) 
(8.17) 
Similarly for double conditioning: 
P (E1| E2, E3) = P (E1| E3) ⇐⇒ P (E2| E1, E3) = P (E2| E3) 
(8.18) 
In words: if conditioning the probability of E1 on E2 and E3 is the same as conditioning 
the probability of E1 just on E3, then conditioning the probability of E2 on E1 and E3 is 
equivalent to conditioning the probability of E2 just on E3. 

8.6 A WORKED-OUT EXAMPLE 
109 
6. Splitting of the marginal 
For any E1 and E2 
˜
P (E1|E2)P (E2) + P (E1|E2)P (E˜2) = P (E1) 
(8.19) 
Exercise 24 
Prove the equality above using a Venn diagram. 
8.6 A Worked-Out Example 
All of this may seem a bit abstract, and so I now show how it works in practice with a very 
stylized example.10 We will look at more interesting cases in Chapter 11. 
Consider the net in Figure 8.4. What information has been embedded in the net? We 
believe that A and B are ‘primary variables’, in the sense that knowledge of no other 
variable would help us making a prediction about either A or B. We believe that D depends 
on B but not on A. We also think that C depends both on A and B, and that, once we know 
C, we do not need to know anything else in order to predict E. 
The probability tables must therefore contain the following information: 
• for node A, the marginal probability, P (A); 
• for node B, the marginal probability, P (B); 
• for node C, the marginal probability, P (C), and the conditional probability table 
P (C  = x|A = y ∩ B = z) 
for x, y, z = T , F 
(8.20) 
• for node D, the marginal probability, P (D), and the conditional probability table 
P (D  = x|B = y) 
for x, y = T , F 
(8.21) 
A 
B 
C 
D 
E 
Figure 8.4 Which variables display zero correlation? 
10This example has been adapted from Moore (2001). 

110 
CHAPTER 8 BAYESIAN NETS I: AN INTRODUCTION 
• for node E, the marginal probability, P (E), and the conditional probability table 
P (E  = x|C = y) 
for x, y = T , F 
(8.22) 
Remembering the closure property above (P (E1 = x|E2 = y ∩. . .  En = z) + P (E˜1 = x|
E2 = y ∩. . .  En = z) = 1), one can easily see that all of the above amounts to just 13 num-
bers (5 marginal probabilities; 4 entries for the conditional probability table for node C; 
2 entries for the conditional probability table for node D; and 2 entries for the condi-
tional probability table for node E). This will allow us to determine the 25 −1 = 31 joint 
probabilities (where the −1 comes, as usual, from the normalization condition). 
How exactly can we do this? Let us start from one particular entry, say, P (E  ∩˜D ∩ 
C ∩B˜ ∩A), that is, the joint probability that events E, C and A happened and events D 
and B did not happen. Note, to begin with, that I have made use of the commutativity 
condition (8.13) to rearrange the order of the variables in (what will turn out to be) the most 
convenient order. Then, by Equation (8.11) we can write: 
D ∩C ∩B˜ ∩A) = P (E|D ∩C ∩B˜ ∩A)P ( ˜
P (E  ∩˜
˜
D ∩C ∩B˜ ∩A) 
(8.23) 
We look at the ﬁrst term on the right-hand side. The way we have drawn our Bayesian net 
means that once I know C I know everything I need to know to predict E. (Of course, A 
and B indirectly inﬂuence E, but once A and B have inﬂuenced C, there is no residual 
dependence left – this is the Markov condition at play.) Therefore 
˜
P (E|D ∩C ∩B˜ ∩A) = P (E|C) 
(8.24) 
and Equation (8.23) simpliﬁes to: 
P (E  ∩D˜ ∩C ∩B˜ ∩A) = P (E|D ∩C ∩B˜ ∩A)P ( ˜
˜
D ∩C ∩B˜ ∩A) 
P (E|C)P ( ˜D ∩C ∩B˜ ∩A) 
(8.25) 
The quantity P (E|C) is known from our conditional probability table for E. So, we are left 
with the task of decomposing P (  ˜D ∩C ∩B˜ ∩A). We can play the same trick again: 
D ∩C ∩B˜ ∩A) = P (  ˜
P (  ˜
D|C ∩B˜ ∩A)P (C ∩B˜ ∩A) 
(8.26) 
Let us focus on P (  ˜D|C ∩B˜ ∩A). Looking at our Bayesian net it is clear that D only 
depends on B. Therefore 
˜
P (D˜ |C ∩B˜ ∩A) = P (D˜ |B) 
(8.27) 
˜
From the conditional probability table for D we have P (D|B). But we know that 
˜
˜
˜
D|B) = 1 ===⇒P (  ˜
P (D|B) + P (  ˜ ˜
D|B) = 1 −P (D|B) 
(8.28) 

8.7 A SYSTEMATIC APPROACH 
111 
and therefore 
D ∩C ∩B˜ ∩A) = P (  ˜
D|B)P (C ∩B˜ ∩A) (8.29)
˜
P (  ˜
D|C ∩B˜ ∩A)P (C ∩B˜ ∩A) = P (  ˜
Exercise 25 
Complete the example above to show that 
P (E  ∩˜D ∩C ∩˜B ∩A) 
 
 
  
= P (E|C) P (  ˜D| ˜B)P C| ˜B ∩A P 
˜B P (A) 
(8.30) 
Exercise 26 Looking at Figure 8.4 state which variables have zero correlation. 
8.7 A Systematic Approach 
For the size of Bayesian nets that I propose to use for stress-testing purposes (i.e., ﬁve-to-
ten well chosen variables) a ‘manual’ approach such as the one highlighted in the previous 
section may sometimes be sufﬁcient. The procedure, however, soon becomes too tedious 
and time-consuming. This section provides an algorithm for a systematic derivation of the 
joint probabilities from the (marginal or conditional) probability tables. 
By extending the line of reasoning we followed in the worked-out example presented 
in the previous section, we can write a general prescription for going from a Bayesian 
net and its associated conditional probability tables to the joint probabilities as follows. 
To do so I will make a minor modiﬁcation to the notation used so far: instead of writing, 
say, P (E1 = w ∩E2 = x ∩. . .  ∩En−1 = y ∩En = z) with w, x, y, z = T , F , I will write 
P (E1 = e1 ∩E2 = e2 ∩. . .  ∩En−1 = en−1 ∩En = en), ei = T , F . This is simply because 
I may otherwise run out of letters of the alphabet, or lack a simple way to indicate the 
previous or following letter. 
For n variables (and n nodes) a generic joint probability entry will therefore have the 
following form: 
P (E1 = e1 ∩E2 = e2 ∩. . .  ∩En−1 = en−1 ∩En = en) 
(8.31) 
where the values, e1, e2, . . ., en−1, en, which the variables E1, E2, . . .  , En−1, En can assume, 
are T and F . If we have labelled ancestors and descendants in ascending order, the ﬁrst 
step is to reorder the joint probability entry that we want to evaluate from descendants to 
ancestors: 
P (E1 = e1 ∩E2 = e2 ∩. . .  ∩En−1 = en−1 ∩En = en) 
= P (En = en ∩En−1 = en−1 ∩. . .  ∩E2 = e2 ∩E1 = e1) 
(8.32) 
Next we decompose as follows: 
P (En = en ∩En−1 = en−1 ∩. . .  ∩E2 = e2 ∩E1 = e1) 
∗ 
= P (En = en|En−1 = en−1 ∩. . .  ∩E2 = e2 ∩E1 = e1) 
P (En−1 = en−1 ∩. . .  ∩E2 = e2 ∩E1 = e1) 

 
 
112 
CHAPTER 8 BAYESIAN NETS I: AN INTRODUCTION 
Let us consider the term in the third line: 
P (En−1 = en−1 ∩ . . .  ∩ E2 = e2 ∩ E1 = e1) 
(8.33) 
It is easy to see that the same strategy can be employed, progressively reducing the joint 
entry to a product of conditional and marginal probabilities: 
P (En−1 = en−1 ∩ . . .  ∩ E2 = e2 ∩ E1 = e1) 
= P (En−1 = en−1|En−2 = en−2 ∩ . . .  ∩ E2 = e2 ∩ E1 = e1) ∗ 
P (En−2 = en−2 ∩ . . .  ∩ E2 = e2 ∩ E1 = e1) 
This is just the break-down-the-joint property in action. Continuing with the same approach, 
one can generalize to 
P (E1 = e1 ∩ E2 = e2 ∩ . . .  ∩ En−1 = en−1 ∩ En = en) 
n
= 
P (Ei = ei |Ei−1 = ei−1 ∩ . . .  ∩ E1 = e1) 
(8.34) 
i=1 
or, more synthetically, 
P (E1 = e1 ∩ E2 = e2 ∩ . . .  ∩ En−1 = en−1 ∩ En = en) 
n
= 
P (Ei = ei |Parents of Ei) 
(8.35) 
i=1 
where the last line follows from the Markov assumption (and what this implies about 
conditional independence). 
One important observation: the joint probabilities can be ‘solved’ by the formula above 
once the marginal and conditional probability tables are provided. But where can the risk 
manager get these from? In particular, from where can the risk manager get a beast such as: 
P (En−1 = en−1|En−2 = en−2 ∩ . . .  ∩ E2 = e2 ∩ E1 = e1) 
(8.36) 
even for a relatively tame case of, say, ﬁve or six variables? I will discuss in detail 
how to assign conditional probabilities in Chapter 11, but we can already see that it is 
a super-human feat to expect the risk manager to prescribe the conditional probability of 
event E6, given that, say, event E5 has happened, events E4 and E3 have not happened, 
event E2 has happened and event E1 has not happened. As I mentioned above, the answer 
to this question comes at the stage of the speciﬁcation of the causal links among the 
variables. Much as the world might be a subtly complex place, we may want to restrict 
the universe of possible Bayesian (directed, acyclical) nets associated with n variables to 
a simpler ‘approximating’ subspace – see the discussion in the next section, which looks 
at the approximation problem from a different, and complementary, angle. For, instance, 
we may want to restrict our attention to children who can have at most two parents. 

 
 
 
 
 
8.8 WHAT CAN WE DO WITH BAYESIAN NETS? 
113 
This simpliﬁes the problem enormously, and brings it (close) to the cognitive abilities of 
someone as smart as a risk manager. As promised, more about this aspect in Chapter 11. 
Exercise 27 Prove that, if a child can have at most two parents, then the conditional proba-
bilities terms of the form P (En−1 = en−1|En−2 = en−2 ∩ . . .  ∩ E2 = e2 ∩ E1 = e1) contain 
at most two events after the conditioning sign (|). 
8.8 What Can We Do with Bayesian Nets? 
Bayesian nets have a variety of applications. From the point of view of stress testing, they 
can be looked at from two different angles. 
8.8.1 Unravelling the Causal Structure 
If we had the joint probabilities, Bayesian nets could help us in clarifying which causal 
structure the underlying variables are likely to display. The idea is conceptually simple – if, 
at times, computationally complex. See Williamson (2005) for an excellent discussion. Let 
us denote the set of the true joint probabilities by [p(i)].11 As I said, when we look at 
Bayesian nets from this angle, we assume that we already have the joint probabilities. For 
computational tractability we then restrict our attention to a subset of Bayesian nets simpler 
than the most general possible one. For instance, we may restrict our search to Bayesian nets 
with nodes with at most two parents. The set of these nets, call it S2, is clearly a subset of the 
full set of possible Bayesian nets, B. Any possible net in S2 (equipped with its conditional 
probability tables) will produce joint probabilities, [pS2 (i)]. When we try to unravel the 
causal structure, we want to establish the links between the nodes in our subset S2 in such a 
way that, in some sense, the probabilities [pS2 (i)] are as close as possible to the probabilities 
[p(i)]. The terms ‘links’ refers to the topology of the Bayesian nets (which nodes are 
connected and in which direction the arrows point) and to the associated probability tables. In 
a way the topology of the net and the entries in the associated conditional probability tables 
are the degrees of freedom that we use to make our approximate joint probabilities, [pS2 (i)], 
as close as possible to the true joint probabilities, [p(i)]. To give a precise meaning to the 
expression ‘as close as possible’ we have to deﬁne the distance between the two distributions, 
D pS, p  . This is often taken to be given by the cross-entropy measure between the two 
sets of probabilities (see Williamson (2005)):12 
D 
 
pS2 , p  = 
 
−p(i) ln p(i) 
(8.37) 
pS2 (i) 
What we want to achieve is the minimization of this distance. Doing this ‘properly’ is 
computationally very hard. There are, however, computational techniques, such as greedy 
algorithms, that provide good approximating joint probabilities, [pS2 (i)], constructed using 
11For simplicity, I neglect the possible difference between the true probabilities, and the probabilities that have 
been statistically estimated. 


12As is well known, the cross-entropy distance is not a true distance because of its lack of symmetry: D f, g ̸= 
D g, f . However, this measure shares the other two important properties of a distance, i.e., it is non-negative 
and is equal to zero if and only if f = g. 

114 
CHAPTER 8 BAYESIAN NETS I: AN INTRODUCTION 
Bayesian nets in S2 to the target [p(i)]. Again, Williamson (2005) has a very good 
discussion. 
Could we use this approach for stress testing? Probably not. My starting point in this 
book has been that establishing the tail co-dependence for rare and extremely rare events 
using frequentist methods is just too hard. This being the case, we do not have direct 
access to the joint distribution. A brave risk manager could perhaps proceed as follows. 
She could estimate the joint probabilities from the data at her disposal, hoping that the 
correlation (not co-dependence!) structure she estimates will be valid in the tails as well. 
She would then have access to [p(i)]. She could then use the techniques alluded to above 
to understand the causal structure among the variables in her problem. This means that she 
would be able to construct the Bayesian net in the chosen subset Sn that best accounts for the 
estimated probabilities [p(i)]. In itself this is extremely valuable. Doing so would also allow 
the risk manager to ‘stress’ the Bayesian net, say, by altering the conditional probability 
tables in such a way to reﬂect her understanding of the weaknesses of the estimated co-
dependencies. This has a certain appeal, but as soon as the risk manager begins to tinker 
with the conditional probability tables, whatever optimality the net so painfully constructed 
might have enjoyed is lost. 
Tempting as it is, I will not pursue this route in this book. The determined reader is 
strongly encouraged to read Williamson (2005), especially Chapter 4. 
8.8.2 Estimating the Joint Probabilities 
The line I take in this book is different, in that I attempt to estimate the joint probabilities 
given the Bayesian net. The treatment above is tailored to this task. The approach is, in 
a way, complementary to the one above, which assumed that we know everything about 
the joint probabilities, and nothing about the causal structure that generated them. For our 
problem, which leads from the marginal and conditional to the joint probabilities, we must 
throw at the problem everything we have: our understanding of which data are relevant to 
the problem at hand; our data-based (frequentist) information; our subjective estimates for 
the conditional probabilities; and our understanding of how the world might work. Or, more 
realistically and more in line with the thoughts expressed in Chapter 3, our awareness about 
how little we know about how the world works, and our understanding that different models 
of reality may be similarly plausible. Perhaps we may end up with two or three plausible 
Bayesian nets (and joint probabilities). Not a bad thing in itself. 
In the following chapters I will explain more precisely how all these various components 
can be gathered. In particular, I assumed in this chapter that somehow we have at our 
disposal the conditional probabilities needed to ﬁll the node-speciﬁc tables. But where do 
these come from? I discuss this in the next chapter. 
One more important point: human intuition can be faulty when we deal with probabilistic 
matters in general, and with conditional probabilities in particular. Even when we have put in 
our best efforts it is therefore likely that the conditional probability matrix we have built will 
not be a ‘feasible’ one. In general we have to clean it, that is, create the feasible conditional 
matrix closest to the one we devised. This is dealt with in Chapters 9 and 10. Some of 
the tools we require are narrowly technical ones – for example, some rudiments of Linear 
Programming. However, I will show that a less systematic set of ‘pre-cleansing’ checks can 
both simplify the task and enhance our understanding of the problem. The reader who is 
familiar with Linear Programming can safely skip the pedestrian review in the Appendix in 

8.9 SUGGESTIONS FOR FURTHER READING 
115 
Chapter 15, which is simply included to make the book self-contained. All readers should 
read carefully, however, Chapter 9. 
8.9 Suggestions for Further Reading 
The literature on Bayesian nets is very large, but only touches risk management in passing. 
In my treatment I have presented the simplest introduction for the task at hand. By doing 
so, I have left by the wayside many important deﬁnitions and concepts. My most glaring 
omission has been a proper discussion of d-separation. I have trusted that the reader can 
ﬁnd the link between d-separation and conditional independence plausible, just by looking 
at which paths are blocked by certain nodes. One of the great advantages of Bayesian nets, 
after all, is their intuitional appeal, and I have decided to exploit this feature to the fullest. 
I have also avoided the very interesting and deep conceptual issues about causation and 
probability. For those readers who may want a more satisfactory treatment of these aspects, 
here are some suggestions. 
A very good conceptual treatment of Bayesian nets in general can be found in Williamson 
(2005). Much of the emphasis of the book is unfortunately from the probabilities (which 
the author assumes exogenously available) to the discovery of the underlying net (and 
hence causal structure). This is the opposite direction to the one taken in this book. As a 
consequence, much of the numerical work (e.g., greedy algorithms, simulated annealing, 
etc.) has little relevance to our applications. 
Pearl (2009) provides an excellent discussion of causality. His introductory chapter about 
probability, graphs and causal models uses a similar framework to the one employed in this 
book, and provides additional material (e.g., a deeper treatment of the d-separation crite-
rion) that can be useful for more advanced applications. In keeping with the treatment I 
have followed, it focuses on the much simpler setting of a ﬁnite number of discrete vari-
ables. The book makes clear the link between probability and causation. Pearl’s approach, 
which is deeply Bayesian in nature, also establishes a profound link between mathemati-
cal expressions (such as Bayes’ theorem) and cognitive structures (see, e.g., his remarks 
on page 5). 
For readers who want to delve much more deeply and rigorously into Bayesian networks, 
Neapolitan (2003) provides a very good and ‘solid’ reference – although not an easy-going 
one: one important theorem is stated over one page, and proved over ﬁve! 


Chapter 9 
Bayesian Nets II: Constructing 
Probability Tables 
9.1 Statement of the Problem 
The discussion in the previous chapters makes clear that there are three conceptual steps in 
building a Bayesian net: 
1. Identiﬁcation of the relevant variables. 
2. Speciﬁcation of a set of causal relationships among them (subject to the Markov 
condition) – this step deﬁnes the topology of the Bayesian net. 
3. Provision of the marginal and conditional probability tables.1 
In this chapter we deal with the third topic, that is, we answer the questions: how can 
we build the conditional probability tables needed to obtain the joint probabilities? What 
information can we use for the task? 
The tables contain marginal and conditional probabilities. As far as the marginal proba-
bilities are concerned, the task is in practice far from trivial, but at least it is mathematically 
relatively simple: as long as we ensure that 
0 ≤ P (Ei) ≤ 1 
for any i 
(9.1) 
we can at least rest assured that we have not introduced any logical impossibilities into 
our problem. Far more care will be required when we specify the conditional probabilities, 
because of the many possible logical pitfalls into which it is possible – indeed, easy – to 
fall. In this chapter I present a series of conditions and sanity checks that the risk manager 
1For brevity, in this chapter I often call conditional probability tables the collection of the conditional and 
marginal probabilities associated with each node. 
117 

118 
CHAPTER 9 BAYESIAN NETS II: CONSTRUCTING PROBABILITY TABLES 
can – and should – perform ‘by hand’. In the next chapter I will then show how to carry out 
more sophisticated and ‘automatic’ checks (and corrections) for the conditional probability 
tables using the Linear Programming technique. 
If these latter checks are more sophisticated and ‘automatic’, why should the risk manager 
bother with the manually intensive ones? Because they provide a powerful indication of 
whether we have been thinking correctly about the problem at hand. Finding preliminary 
inconsistencies forces us to review our assumptions about how the world works, and to ask 
ourselves whether we have coherently translated this assumed knowledge about the world 
into probabilistic statements. I cannot stress how important and how useful this part of the 
process is. It is not an exaggeration to say that the risk manager may well learn more about 
the structure and vulnerabilities of the portfolio under her watch through this process of 
sanity checking than by looking at the ﬁnal outcome. 
So, let us look in some detail at how to assign the entries of the conditional probability 
table, starting from the marginal probabilities (Sections 9.2 and 9.3 present two different 
suggestions as to how this can be done), and then moving on to the conditional probabilities 
(Sections 9.5 and 9.6). 
9.2 Marginal Probabilities – First Approach 
In order to arrive at marginal probabilities, I recommend that each risk manager should 
begin by looking at a given asset class (e.g., equities, IR, ABSs, etc.), and identifying the 
‘vulnerabilities’ of the portfolio. These may come from large concentrations, from very 
volatile positions (currencies that may depeg) or from a combination of the two. Ideally, 
the vulnerability should be due to a set of tightly linked and well-identiﬁable risk factors. 
Once the portfolio vulnerabilities have been identiﬁed, for the next step there are four 
quantities to be speciﬁed: 
• the holding period; 
• the magnitude of the adverse move in the risk factor; 
• the magnitude of loss incurred; 
• the probability of an adverse move of a given magnitude over the chosen holding 
period. 
Obviously, these four quantities cannot be assigned independently of each other. But, 
strictly speaking, not even two of the quantities above cannot be arbitrarily assigned. We 
will see below why this is the case. 
Remember that we are dealing with Boolean variables (‘events’), which can be true 
or false. So, we must assign a probability to each variable being true or false, that is, a 
probability of occurrence to each event. We can do this by identifying the event with a loss 
of a given magnitude. So, when we look at the problem from this angle, the event could be 
‘We lose $100m from our position in 10-year Treasuries’. The event is true if the loss is 
incurred. When we look at the problem in this way, the ﬁnal goal is therefore to be able to 
generate, for each vulnerable position, a pair of numbers, the magnitude of the associated 
loss and its probability – as deﬁned more precisely below. 

9.2 MARGINAL PROBABILITIES – FIRST APPROACH 
119 
9.2.1 Starting from a Fixed Probability 
One possible way to proceed is as follows: 
1. For each position choose a holding period. This should be linked to the ‘liquidation 
period’, i.e., to the time it would take to exit the position. This exit time is clearly 
linked to the size of the position, but it also depends on market conditions. Since 
we are looking at stress events, one can assume conditions of market distress. How 
severe? We will come back to this point (possibly in an iterative fashion), but for the 
moment we go with the ﬂow. So, the risk manager should establish the approximate 
length of the liquidation period of the vulnerable position, given its size and assuming 
that the market is in a condition of stress. To avoid, as usual, spurious precision it 
is useful to group the holding periods into standard buckets: say, 2 days, a week, a 
fortnight, a month, a quarter, etc.2 
2. The risk manager can then choose for each vulnerable position an order-of-magnitude 
probability, say, 10−1, 10−2, 10−3, 10−4, etc. It is reasonable, but not always nec-
essarily the best choice, to choose the same approximate probability for all the 
vulnerable positions. Speaking of probability raises the question: Probability of what? 
This becomes clear when the size of the adverse move is brought into play. 
3. The risk manager can then ask the question: by how much can the risk factor(s) 
that affect a given vulnerable position move in an adverse manner with a probability 
not lower than the chosen one? For instance: suppose the vulnerable position is a 
long position in the S&P and the probability level chosen is 10−3. Then the question 
should be: what is the size of a fall in the S&P over the holding period such that the 
probability of exceeding that fall is 10−3? 
4. Given the size of the fall (derived from the probability level and the holding period) 
the loss can now be calculated (approximately or exactly, if by full revaluation). Note 
that, since we have estimated the minimum magnitude of the move for the given 
probability level, the loss associated with that move is a lower bound for the loss that 
could be sustained.3 By deﬁnition this quantity is therefore very closely related to the 
value-at-risk statistic (VaR) at the appropriate conﬁdence level for that position and the 
relevant risk factors. The estimation of the move in the risk factor need not (but can, 
if the risk manager thinks it is appropriate) be arrived at using a frequentist approach. 
I said that a reasonable starting point is for the risk manager to choose the same probability 
level for all the moves associated with the portfolio vulnerabilities. Sometimes, however, 
this may not be a good idea. Why may a risk manager want to choose different levels of 
2Different liquidation periods can be accommodated as long as one makes the assumption that the different 
traders are not allowed to ‘reload the gun’ once the simultaneous unwinding of all the risk positions has begun. 
The ﬁnal result of the calculation, i.e., the conditional expected loss, therefore implictly refers to the loss that 
can be expected to be incurred if all the traders began unwinding their positions at the ﬁrst occurrence of the stress 
event (whichever this may be), and were not allowed to take on more risky positions (‘reload the gun’) until the 
position with the longest unwinding period has been liquidated. 
3I am implictly assuming linear risk positions. Severe non-linearities would require a more precise speciﬁcation 
of the range of moves, and of the associated probabilities. 

120 
CHAPTER 9 BAYESIAN NETS II: CONSTRUCTING PROBABILITY TABLES 
probability? Suppose that a vulnerable position is a short gamma position. At the chosen 
level of probability the move in the risk factor may be not large enough to produce a sizeable 
loss. But for a bigger move (associated, of course, with a lower probability of exceedance) 
the losses could become truly massive. In this situation, it would be useful to capture the 
lower-probability event. 
The attentive reader will have spotted that the four steps outlined above imply a degree of 
circularity in the reasoning. We started by choosing a holding period, which we set equal to 
the liquidation period. To determine the liquidation period we looked, of course, at the size 
of the vulnerable position. But we also assumed ‘stressed market conditions’. How stressed? 
We could not know that when we chose the holding period, because we had not worked 
out the magnitude of the move. If we ﬁx the probability, the magnitude of the price move 
is determined, but only at this point can we tell how stressed the market condition is – and 
therefore what the liquidation period should be. Since a quantity such as the liquidation 
period cannot be known with any precision, in practice this rarely causes problems, but it is 
useful to carry out a self-consistency check here. If necessary, the probability level should 
be changed, so as to bring about a magnitude in the adverse move roughly in line with the 
length of the liquidation period. This is another reason why the ﬁnal probabilities associated 
with the various vulnerable positions may not end up being all the same. 
9.2.2 Starting from a Fixed Magnitude of the Move 
Even if we continue to consider the occurrence of a given loss from a set of positions as 
‘the event’, the approach just described is not the only possible one: 
1. Step 1 (choice of a holding period) is exactly as above. 
2. The risk manager chooses the magnitude(s) of the move(s) in the risk factor(s). A 
reasonable choice is a severe, large, but plausible move. 
3. Since the risk manager now already knows the magnitude of the move (because she 
has just chosen it), she can already estimate the magnitude of the loss. 
4. Finally the risk manager should try to estimate the probability of a move at least as 
large as the one chosen in step 2 over the liquidation (holding) period determined 
in Step 1. 
This approach may seem more natural, but note that the mild circularity in the approach 
has not disappeared. We have chosen the holding period ﬁrst, without knowing the proba-
bility of the adverse move. The two items should be at least roughly consistent. Again, a 
sanity check (or, if needed, an iterative process) may be useful. 
9.3 Marginal Probabilities – Second Approach 
The approach to assigning marginal probabilities that I have described above implicitly takes 
the loss itself as the event. Saying ‘loss’, however, is not enough. I must also specify from 
which (set of) positions this loss originates. It is for this reason that in the example I gave 
above I deﬁned my event as ‘We lose $100m from our position in 10-year Treasuries ’. 

9.3 MARGINAL PROBABILITIES – SECOND APPROACH 
121 
There is a problem with this approach. Suppose that I am long 10-year Treasuries. Then 
the loss will be incurred if the 10-year point of the yield curve increases. But if my ‘event’ 
is the loss itself, I cannot distinguish between different causes that may have given rise to 
the same rise in the yield curve: perhaps it happened because Bernanke suddenly raised 
rates; or perhaps because China began selling US Treasuries. If we are (very) smart we can 
perhaps specify a marginal probability that ‘knows about’ all the possible causes that have 
given rise to the increase in the yield curve. But when it comes to assigning the conditional 
probabilities knowing whether the increase in rates was due to Bernanke or to China will 
make a big difference. 
To circumvent this problem I propose below a set of deﬁnitions that can work more 
efﬁciently. 
I start by taking the word ‘event’ as primitive – that is, I do not try to deﬁne this term. 
Two examples of an event could be: ‘Bernanke raises rates by 100 basis points’, or ‘Moody 
downgrades the sovereign debt of Italy’. Then I deﬁne the following. 
• Probability of an event : This is the probability of the event happening between now 
and time T (the occurrence horizon). We could take T = 2 days.  
• Risk positions : These are the risk positions directly affected by the occurrence of the 
event. (Say, a position in 10-year Treasuries with a PV01 of $2m/bp.) 
• Unwinding horizon: This is the time that it will take the trader(s) to exit the risk 
positions in an efﬁcient manner given that the event has occurred during the occurrence 
horizon (i.e., at any time between time 0 and time T ). 
• Risk factors : These are the risk factors that affect the risk positions. 
• Moves in the risk factors : these are the expected moves over the unwinding horizon 
in the risk factor(s) as a consequence of the occurrence of the event. For our example, 
we say that the event (‘Bernanke raises rates by 100 basis points’) will cause the 
10-year yield to increase by 60 basis points. 
• Event-related loss : This is the loss incurred because of the moves in the risk factors. 
In our case, the loss would be $120m ($2m/bp × 60bp). 
In the deﬁnitions above, I use italic for words and expressions when ﬁrst deﬁned and 
bold when used as deﬁned. So, the expression moves in the risk factors in the deﬁnition 
of event-related loss, means the expected moves over the unwinding horizon in the risk 
factor(s) as a consequence of the occurrence of the event. 
Note the difference between the occurrence horizon and the unwinding horizon. The ﬁrst 
is the ‘surprise time’ or the time between now and when I can begin to take action. It may 
be one day, or two or three days in the case of weekends. The second is the time it will take 
me to get out of the position ‘in an efﬁcient manner’ given that the event has happened. 
The qualiﬁer ‘in an efﬁcient manner’ reﬂects the fact that no trader, even if determined to 
exit a position, will unwind it immediately at any cost, but will try to strike a compromise 
between exiting quickly and allowing the market to ‘digest’ her inventory. 
Note also that, when I came to deﬁne the moves in the risk factors, I had to speak of 
expected moves. This is because I do not know by how much the 10-year point of the curve 

122 
CHAPTER 9 BAYESIAN NETS II: CONSTRUCTING PROBABILITY TABLES 
will move if Bernanke raises the short rate by 100 basis points.4 In the example above, 60 
basis points is my estimate of the expected increase in rates. Using the expected increase is 
reasonable, but not the only possible choice. If I were (very) brave, instead of ‘the expected 
move’ I could try to say something like ‘the XXth percentile move in the 10-year point of the 
curve given that the event has happened’. However, this is something far more precise than I 
will ever feel able to express. If I wanted to be conservative, I could perhaps say something 
like ‘a very adverse, but plausible, move in the 10-year point of the curve given that the event 
has happened’. This already sounds a bit better, but precision, as usual, is not the issue here. 
9.4 Handling Events of Different Probability 
The discussion above has highlighted that there are several reasons why we may want 
to associate probabilities of very different magnitude to the vulnerable positions we have 
identiﬁed. As a result of this, we need a way to handle events that, while being rare, can be 
of substantially different degrees of ‘rarity’. To do this we can ﬁrst categorize the marginal 
probabilities of each event into groups (‘blocks’) of roughly equal magnitude: 
P (Ei ) ≃ η ∗ kmi 
(9.2) 
where i denotes the event and η is a ‘small’ reference probability, say, 10−2 . 
Taking logs (in the chosen basis, k) 
logk [P (Ei )] = log [η] + mi 
(9.3) 
A choice for the basis, k, for the logarithm of ten is reasonable: working in basis two (or 
using natural logarithms) would require too much knowledge about the marginal proba-
bilities that we can reasonably expect to have for stress events. Basis ﬁve may be more 
reasonable, but we do not have a great intuition about increases in probability by a factor 
of ﬁve. In the future I will therefore always assume a basis of ten unless I say otherwise, 
but nothing hangs on the choice. 
It is then useful to set 
mi ∈ [−M, −M + 1, . . .  , −1, 0, 1, . . .  , M  − 1, M] 
(9.4) 
For practical applications (and base ten) a reasonable value for M is 1 or 2. So, with the ﬁrst 
choice we partition the rare events into three groups, those of a reference small probability 
(for which mi is 0), those with probability ten times larger (for which mi is 1) and those 
with probabilities ten times smaller (for which mi is −1). Similarly for M = 2. 
So, to each event, Ei , we associate an exponent, mi , which determines its (approximate) 
marginal probability, P (Ei). What we are trying to do here is to group events into blocks of 
similar (and similarly small) probability. Both the widening of credit spreads by 300 bp and 
the default of a major bank counterparty are events of low probabilities (and, potentially, 
4This is the drawback of not taking the loss as the event. On the other hand, when we took the loss as the event 
we had to speak of losses of, say, ‘$100m or greater ’. As the attentive reader will notice, deﬁnitional problems 
can be moved around, but they rarely wholly disappear. 

 
 
 
 
 
 
 
 
 
 
 
 
9.5 CONDITIONAL PROBABILITIES: A REASONABLE STARTING POINT 
123 
interesting stress events); but the default of a major bank counterparty is almost certainly 
much less likely than the widening of credit spreads. (Do you agree? Why – or why not?) 
These two events should therefore belong to different probability blocks. 
When we move to conditional probabilities we shall see how this exponent grouping 
comes in very handy. 
9.5 Conditional Probabilities: A Reasonable Starting Point 
If we want a reasonable starting point in our construction of a conditional probability matrix, 
and we are not too sure where to begin, we can always remember that 
P Ei |Ej 
P (Ei)

 = 

 
(9.5) 
P Ej |Ei 
P Ej 
Therefore, for all events in probability blocks i and j a reasonable starting point could be 
P Ei |Ej

 = 10(mi −mj) 
(9.6) 
P Ej |Ei 
From the relationship above it is also obvious that the marginal and conditional probabilities 
cannot be assigned independently of each other. For n events there are n compatibility 
relationships 
P Ei |Ej 

 

 
P (Ei) = 

 P Ej = 10(mi −mj)P Ej 
(9.7) 
P Ej |Ei 
If this sanity check does not hold, we should rethink how we have estimated either our 
marginal or our conditional probabilities, or both. 
k
To make progress recall the deﬁnition of the quantity xi that I gave in Chapter 5, 
Sections 5.9 and 5.10: 
P Ei |Ej ≡ P (Ei) xj 
(9.8)
i 
This quantity gives by how much we can modify (increase or decrease) the probability of 
event Ei , given our knowledge that event Ej has happened. 
Now, recall that we are dealing with stress events, and that their marginal probability is 
low to start with. This imposes serious constraints on the conditional probabilities. Let us 
look into this more precisely. 
First of all, the most common mistake in assigning conditional probabilities is to express 
the intuitive idea that knowledge that event Ej has happened has no inﬂuence on our estimate 
of the occurrence of event Ej by assigning P Ei |Ej = 0. This mistake is common for 
risk managers who are accustomed to working with correlation matrix. But, of course, as 
we saw before saying that P Ei |Ej = 0 means that event Ei cannot occur, given that we 
know that event Ej has occurred. This is just the sort of incompatibility condition that we 
discussed in Chapter 5. 

 
 
	 

 
124 
CHAPTER 9 BAYESIAN NETS II: CONSTRUCTING PROBABILITY TABLES 
So, if we want to say that events Ei and Ej are independent we must set 
P Ei | Ej ≡ P (Ei) 
(9.9) 
and therefore 
xj = 1 
(9.10) 
i 
If we want to say that knowledge that event Ej has happened decreases our estimate of the 
probability of occurrence of event Ei , then we have to set  
j 
x < 1 
(9.11) 
i 
And, of course, xi
j can be reduced all the way to 0, which, as discussed above, corresponds 
to the case of incompatibility. 
If we want to say that knowledge that event Ej has happened increases our estimate of 
the probability of occurrence of event Ei , then we have to set  
j 
x > 1 
(9.12) 
i 
By how much can we increase xi
j ? All probabilities (conditional or otherwise) must be 
smaller or equal to one. Therefore 

 
1
j
P Ei | Ej ≡ P (Ei) xj ≤ 1 ===⇒ x ≤ 

 
(9.13)
i
i 
max P (Ei), P (Ej) 
So 
1 
0 ≤ xj ≤ 

 
(9.14)
i 
max P (Ei), P (Ej) 
As explained in Chapter 5, it is difﬁcult to give a precise estimate of how much knowl-
edge that event Ej has happened increases or decreases our estimate of the probability of 
occurrence of event Ei . It can therefore be useful to require that xi
j should assume one of 
a limited range of possible values, say: 
1
j 
x ∈ ε, 0.1, 1, 10, (1 − ε) 

 
(9.15)
i 
max P (Ei), P (Ej) 
where ε is a very small, but strictly positive, quantity.5 Note that, by doing so, we are 
refraining from imposing either absolute incompatibility or deterministic causation. The 
courageous risk manager can, of course, always set ε = 0.6 
5In the denominator, we have max[P (Ei), P (Ej)] rather than P (Ei) in order to preserve the symmetry of the 
matrix xj (which is a mathematical requirement), and to ensure that neither conditional probability will be greater 
i 
than one (also a mathematical requirement). 
6In order to facilitate the cognitive task of the risk manager, she could be asked whether, in her opinion, the 
occurrence of event Ej (i) makes event Ei almost certain; (ii) has virtually no impact on the probability of the 

 
 
 
 
 
 
9.6 CONDITIONAL PROBABILITIES: CHECKS AND CONSTRAINTS 
125 
This approach to setting conditional probabilities is extremely useful in avoiding 
logical pitfalls – and these pitfalls are particularly insidious when we deal with very small 
marginal probabilities. 

 
There is another important advantage. The matrix of elements P Ei |Ej is, of course, not 
symmetric. The risk manager must therefore assign n 2 − n elements, linked by n 2 − n /2 
Bayes condition. But the matrix of elements xi
j is symmetric and the risk manager must 
j
only assign n 2 − n /2 elements. The quantities xi are fewer in number, and cognitively 
much easier to assign – the latter property deriving from the fact that they automatically do 
the Bayesian book-keeping for us, and help in correcting for the baseline bias that I will 
discuss in Chapter 12 (in particular, in Section 12.4). 
9.6 Conditional Probabilities: Checks and Constraints 
In this section I assume that the marginal and conditional probabilities have already been 
estimated and assigned to the appropriate probability buckets as described above. I will also 
assume that we have chosen the precision in our probability estimates (the width of our 
log bucket) literally to be one order of magnitude, that is, a factor of ten. If you feel more 
courageous, and you feel you can specify marginal probabilities of very rare events much 
more precisely, ﬁrst of all I would like to hear how you do it, but then there is nothing 
stopping you from substituting your chosen basis k for my humble ten. 
What we want to achieve in the rest of this section is to provide as much help as possible 
in ensuring that the conditional probabilities we have assigned are not inconsistent. This 
help will come from the mathematical requirements we must impose if we want the numbers 
we assign to be bona ﬁde potential probabilities, and, in some cases, from our supposed 
knowledge about the problem at hand (e.g., from our assumption about independence, etc.). 
9.6.1 Necessary Conditions 
Needless to say, for any two events, Ei and Ej , it must be that 
0 ≤ P Ei |Ej ≤ 1 
(9.16) 
This seems too obvious to be worth mentioning. But consider the following events and 
tentatively assigned probabilities:7 
• event E1: equity market crash by 20% or more; 
• event E2: widening of AA credit spreads by 100 bp or more; 
• P (E1) = 0.09%; 
• P (E2) = 0.05%; 
• P (E2|E1) = 70%. 
1
occurrence of Ei ; or (iii) makes the occurrence of event Ei almost impossible. Factors xi
j of (1 − ε) max[P(Ei ),P (Ej )], 
1 or  ε would be associated with options (i), (ii) or (iii), respectively. 
7This example has been adapted from Moskowitz and Sarin (1983). 

 
 
 
 
 
 
 
 
 
 
126 
CHAPTER 9 BAYESIAN NETS II: CONSTRUCTING PROBABILITY TABLES 
Prima facie these assignments may seem plausible: we may or may not agree with the 
risk manager’s assessment, but there does not seem to be anything ‘impossible’ in the 
numbers provided. A moment’s reﬂection, however, shows that we have already violated 
the ‘obvious’ constraint coming from Bayes’ theorem (9.16): because 
P (E2| E1)P (E1) = P (E1| E2)P (E2) 
(9.17) 
it must be that 
P (E1) 
0.0009 
P (E1| E2) = P (E2| E1) 
= 0.7 ∗
= 1.26 
(9.18)
P (E2) 
0.0005 
But this cannot be, because a conditional probability, qua probability, must be smaller or 
equal to 1. Therefore a useful entry-level check should be that 
P (E2)
P (E1| E2) ≤ 
(9.19)
P (E1) 
This is useful, but with a little more effort, we can do better. 
9.6.2 Triplet Conditions 
We have events Ei for i = 1, 2, . . .  , n. The starting point is the usual, that is, some rear-
rangement of Bayes’ theorem, this time in the following form: 
P Ei | Ej 
P (Ei)

 = 

 
(9.20) 
P Ej | Ei 
P Ej 
Using the fact exploited above that all conditional probabilities must be non-negative and 
smaller or equal to one, we can easily determine the following constraint (Constraint 
Triplets) among the conditional and marginal probabilities: 
 

 
 P (Ei) 

 P (Ei) P (Ek)
P Ei | Ej = P Ej | Ei 

 = P Ej | Ei 

 ===⇒ 
P Ej 
P (Ek) P Ej 
= j ̸
1 ≥ P Ej | Ei 
 P (Ei | Ek) P  Ek | Ej  ≥ 0 for  i ̸
= k 
(9.21)
P (Ek | Ei) P Ej | Ek 
where, again, the double inequality in the second line follows because the ﬁrst term in the 
ﬁrst line (P Ei | Ej ) must  be a  bona ﬁde probability. 
We have six combinations of P Ei | Ej 
for i ̸ = j . Constraint Triplets  shows that, 
given the conditional probabilities of ﬁve combinations, the sixth (P Ei | Ej ) is uniquely 
determined (and, of course, must be smaller than or equal to 1 and non-negative). A simple 
routine can easily be written that returns all the inconsistent triplets, and allows the risk 
manager to make changes accordingly. 

 
 
 
 
 
 
 
 
9.6 CONDITIONAL PROBABILITIES: CHECKS AND CONSTRAINTS 
127 
We note in passing that setting P Ei |Ej = P Ej |Ei would ensure that Constraint 
Triplets is automatically satisﬁed for any triplet. But this is in general not a good place to 
start – the probability of a general equity market crash given a widening in the credit spread 
of ﬁrm X can be very different from the probability of a widening in the credit spread of 
ﬁrm X given a general equity market crash. 
9.6.3 Independence 
The sanity checks introduced so far apply to any form of dependence among the under-
lying Boolean variables. We now want to see what additional conditions we can establish 
if we are prepared to say something about the causal links among the variables – or lack 
thereof. We want to see, in other words, what we can say about the conditional proba-
bilities in the case of independence, conditional independence, deterministic causation and 
incompatibility. 
If you think that events Ei and Ej are independent it follows that 
 
 
P Ei |Ej = P (Ei ) 
(independence) 
Then 
P 
 
Ei , Ej 
 
= P (Ei ) P 
 
Ej 
 
≃ 10−(mi +mj ) 
(9.22) 
and 
P 
 
Ei |Ej 
 
≃ 10−mi , P  
 
Ej |Ei 
 
≃ 10−mj 
In the case of independence the risk manager should therefore make herself happy that the 
singly-conditioned probabilities are the same as the marginal probabilities, and that the joint 
probability, P (Ei, Ej), is just given by the product of the two marginals. 
9.6.4 Deterministic Causation 
If you think that A deterministically causes B, then P (B)  ≥ P (A). This must be true, 
because every time A happens B happens as well, but there may be other events that cause 
B. So, as a ﬁrst check, one should make sure that the grouping into blocks is compatible 
with this. If not, then either our assumed causation or our initial grouping into blocks must 
be wrong. Needless to say, imposing deterministic causation is a very strong constraint, 
which should not be imposed lightly. 
If we are really conﬁdent about our understanding of the causal links problem at hand, 
then in the case of deterministic causation 
P Ei |Ej = 1 
(9.23) 
So 

 
P Ej
P Ej |Ei =
≃ 10(mj −mi) with mi ≥ mj 
(9.24)
P (Ei) 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
128 
CHAPTER 9 BAYESIAN NETS II: CONSTRUCTING PROBABILITY TABLES 
This condition looks identical to Equation (9.22). Note, however, that we now have a 
restriction (mi ≥ mj ) on the relative magnitude of the exponents that was absent in the case 
of independence. 
So, a good way to check the reasonableness of deterministic causal links is to look at the 
events as they have been grouped into blocks. An event Ei can be considered as a potential 
deterministic cause for event Ej only if mi ≤ mj . 
What else does deterministic causation imply about conditional probabilities? Consider 
again the Venn diagram in the case of causation when i causes j . It readily follows that 
P Ei ∩ Ej 
P Ei ∩ Ej

 
≤ 
(9.25) 
P Ej 
P (Ei) 
But therefore a necessary condition for causation (i causes j ) is that  
P Ej |Ei ≤ P Ei |Ej 
(9.26) 
9.6.5 Incompatibility of Events 
If you believe that Ei and Ej are incompatible, then 
P Ei |Ej = P Ej |Ei = 0 
Just like deterministic causation, incompatibility is a very strong statement. Again, you must 
feel very sure about your knowledge of how the world works to invoke incompatibility. I 
recommend that, to avoid closing doors that should be left gently ajar, it is always advisable 
to set 
P Ei |Ej = P Ej |Ei = 10−K with K ≫ 1 
(9.27) 
If, however, you really want to impose the requirement that two events, Ei and Ej , should 
be mutually incompatible, there are severe constraints on the other conditional probabilities. 
First, let us make use of the identity 
P Ei ∪ Ej |Ek = P (Ei |Ek) + P Ej |Ek − P Ei ∩ Ej |Ek 
(9.28) 
(where ∪ and ∩ indicate union and intersection, respectively). 
Exercise 28 Prove 
the 
relationship 
P Ei ∪ Ej |Ek = P (Ei |Ek) + P Ej |Ek − 
P Ei ∩ Ej |Ek making use of Venn diagrams. 
Now, if P Ei |Ej = P Ej |Ei = 0, it follows that P Ei ∪ Ej |Ek = P (Ei |Ek) + 
P Ej |Ek . 
Exercise 29 Making use of Venn diagrams again, prove the relationship P Ei ∪ Ej |Ek 
= P (Ei |Ek) + P Ej |Ek for incompatible Ei and Ej . 

9.7 INTERNAL COMPATIBILITY OF CONDITIONAL PROBABILITIES 
129 
For all other events Ek , k ̸= i, j , from Equation (9.28) it must therefore hold that 
 
 
P Ei ∩ Ej |Ek = 0 
(9.29) 
Therefore (Constraint Zeros) 
 
 
 
 
P (Ei |Ek ) + P Ej |Ek = P Ei ∪ Ej |Ek ≤ 1 
(9.30) 
A simple routine can be written to identify violations of this inequality for any zero con-
ditional probability in the conditional probability matrix that the risk manager may want to 
assign. 
9.7 Internal Compatibility of Conditional Probabilities: 
The Need for a Systematic Approach 
The sanity checks I have suggested in the previous section can ‘cure’ the most glaring incom-
patibilities and impossibilities. If we have assigned the conditional probabilities making use 
of the quantities xi
j it is more likely that our starting point for the conditional probability 
matrix was not too bad to begin with. Even so, if the conditional probabilities we have 
assigned were not obtained from the statistical analysis of data, but came from our expert 
subjective judgement, it is virtually unavoidable that, even after the cleansing above, the 
resulting conditional probability matrix will still contain subtler embedded logical impossi-
bilities. These subtler ‘impossibilities’ are virtually impossible to spot ‘by the naked eye’. 
The next chapter will therefore deal with a systematic method to ‘correct’ the conditional 
probability matrix supplied by the risk manager. In short, for a conditional probability matrix 
to be admissible, we will have to ensure that there exists a set of marginal probabilities 
from which these conditionals can all be simultaneously derived. When this is not the case 
the algorithm provided in the next chapter will return the conditional probability matrix 
‘closest’ (in some sense to be deﬁned) to the one chosen by the risk manager. 
Note that it would be a mistake to dispense with the sanity checks described in this 
chapter, and fast forward directly to the systematic solution that I describe in Chapter 10. 
This ‘automatic’ procedure will, in fact, return the closest feasible solution to the proposed 
one, but ‘closest’ will only be truly ‘close’ to our starting point if this was reasonable and 
well thought-out in the ﬁrst place. 


Part III 
Applications 


Chapter 10 
Obtaining a Coherent Solution I: 
Linear Programming 
10.1 Plan of the Work Ahead 
The two chapters in Part III show how to apply the concepts and techniques introduced in 
Part II to stress-testing problems. 
In Chapter 9 I discussed a series of ‘manual’ sanity checks to ensure the consistency 
of the marginal and conditional probabilities assigned by the risk manager. By themselves, 
these checks do not in general guarantee that the set of assigned probabilities will be 
self-consistent, but they do serve two important purposes. 
First, they ensure that the search for the consistent solution we undertake in this chapter 
will start from a plausible set of initial conditions. As a result, we can hope that the 
‘optimal’ solution returned by the Linear Programming algorithm will be close to what the 
risk manager actually ‘meant’. 
Second, and more important, the sanity checks force the risk manager to question the 
assumptions made, and to reason more clearly about assumed dependencies. I cannot stress 
strongly enough the importance and usefulness of this part of the process. 
In this chapter I assume that all this groundwork has been carried out, and I will present 
the ﬁrst systematic solution to the programme started in the previous chapters – where by 
‘solution’ I mean a set of internally consistent conditional probabilities as close as possible to 
the risk manager’s intuition. I will also discuss what we can do with this information – that 
is, what information about the problem at hand we can extract from the solution. 
In the next chapter I will then show what we can do if the risk manager feels able 
to provide the full set of marginal and conditional probabilities, that is, the conditional 
probability tables associated with each mode. These conditional probability tables can yield 
much richer fruits if we are prepared to add to them our tentative understanding of how the 
world works – that is, of the causal links among the chosen variables. See, in this respect, 
the discussion in Sections 4.3 and 4.4. 
133 

134 CHAPTER 10 OBTAINING A COHERENT SOLUTION I: LINEAR PROGRAMMING 
Which approach is better? It depends on how conﬁdent the risk manager feels about 
the probabilistic and causal infromation she must supply. At one end of the conﬁdence 
spectrum the risk manager may think that the marginal probabilities are, for very rare 
events, too difﬁcult to estimate, and that she can only venture a guess at 
• singly-conditioned probabilities; 
• the relative likelihood of the stand-alone events. 
If that is the case, then the approach presented in this chapter is the suggested route. The 
main task faced by the risk manager will then be to ensure consistencies among the set of 
conditional probabilities provided. 
At the opposite end of the conﬁdence spectrum, the risk manager may feel conﬁdent 
(bold enough?) to provide 
• the marginal probabilities of all the individual stress events (or at least their orders of 
magnitude); 
• the associated n-conditioned probabilities; 
• the causal links among the variables at play. 
When the risk manager feels conﬁdent that she can provide this degree of information, 
the approach presented in Chapter 11 is much more powerful, and yields richer results. This 
approach is directly based on the Bayesian-nets technology (and on the bounding techniques 
described in Chapter 7). 
How conﬁdent can a risk manager be? Clearly, it is not realistic to expect anybody to 
be able to provide even an informed guess for a quantity such as, say, P (E1 = x1|E2 = 
x2 ∩ E3 = x3 ∩ . . .  ∩ En = xn), with xi = T , F , for  n larger than, say, 3 (i.e., for more-
than-doubly-conditioned probabilities). However, if we are prepared to restrict the set of 
possible Bayesian nets underlying the problem (i.e., if we are prepared to simplify the 
causal structure among the underlying variables), dramatic simpliﬁcations can result. When 
these simpliﬁcations are acceptable, the full joint probability structure can be determined. 
This will be an approximation to the ‘true’ and latent set of joint probabilities. If we 
have been clever, it will be a close approximation to the true joint probabilities chosen 
from the set of joint probabilities that can be derived by the simpliﬁed Bayesian net struc-
ture that we have allowed. I will use some concrete examples to show how this can be 
achieved. 
Between these two extremes (total ignorance of marginal probabilities and full 
knowledge of the marginal, n-conditioned probabilities and causal links required by the 
Bayesian net structure that we have chosen), there lies a full spectrum of intermediate 
states of probabilistic knowledge. Sometimes – actually, quite often – the risk manager 
will feel able to assign most marginal probability, many singly-conditioned probabilities 
and perhaps a handful of doubly-conditioned probabilities. In this situation the bounds 
determined in Chapter 7 can be extremely useful. 

10.2 COHERENT SOLUTION WITH CONDITIONAL PROBABILITIES ONLY 
135 
It must be stressed that, as long as we are well aware of what we can and what we cannot 
know, all these different degrees of knowledge provide useful risk-management information. 
Richer information is not always necessarily better, though – especially if one forgets the 
assumptions that have gone into obtaining the results. Even the simple identiﬁcation of the 
stand-alone stress scenarios and of the associated losses with no probabilistic information 
at all can serve a very important and useful role – say, of raising awareness. Admittedly, it 
may well be difﬁcult to associate capital to this probability-free information, but awareness 
and qualitative appreciation of danger is a valuable part of managing risk. 
If, on the other hand, we feel that we can build the required joint probabilities – and we 
are conﬁdent that, given the rarity of the events, they ‘make sense’ – we have the fullest 
possible probabilistic information about the stress events at hand. With this information 
we can estimate all sorts of risk statistics associated with these stress events (expected 
loss, variance of loss, higher moments of the loss distribution, Value at Risk, Conditional 
Expected Shortfall, you name it). 
And, of course, if we believe that we know how to associate capital (regulatory or 
economic) to losses at a given percentile level, we can calculate internal or regulatory 
capital, estimate return after adjusting for tail risk (tail-risk-adjusted return) and do all kinds 
of marvellous things. This is particularly relevant in these post-subprime-crisis days, because 
some banks have begun to build their Economic Capital directly from analysis of stress 
events, rather than by reading impossibly high percentiles of proﬁt-and-loss distributions 
obtained using frequentist methods. 
If, in addition to joint probabilties, we can offer a possible set of causal links among 
the stand-alone events, our understanding will be greatly enhanced. Moving from an asso-
ciative to a causal description allows us to ‘understand’ the problem in a much deeper and 
cognitively more resonant manner. It also allows us to deal with changes in the causal 
dependence much more easily and constructively. See again the dicussion in Section 4.4, 
and the example about the sprinkler and wet pavement in particular. 
These are indeed marvellous things, but we should never get carried away and convince 
ourselves that we can know more than we actually can. As Keynes said: Sometimes, ‘we 
simply don’t know’.1 
10.2 Coherent Solution with Conditional Probabilities Only 
In this section I formulate the problem at hand (ﬁnding a set of self-consistent conditional 
probabilities) in a way that lends itself to be solved by Linear Programming. The reader 
can ﬁnd a simple description of the Linear Programming technique in the Appendix in 
Chapter 15. 
We assume that the risk manager has at least provided a ‘cleansed’ matrix of singly-
conditional probabilities – where by ‘cleansed’ I mean that all the manual sanity checks have 
been carried out. This is the minimal requirement to obtain some useful quantitative output 
from the analysis. The approach presented below can also be used if the risk manager can 
provide some or all of the marginal probabilities and some of the more-highly conditioned 
1Robertson (1936): see the quote that opens Chapter 1. 

 
 
 
 
 
 
136 CHAPTER 10 OBTAINING A COHERENT SOLUTION I: LINEAR PROGRAMMING 
probabilities. If she can, correspondingly richer and more comprehensive output can be 
generated (see Sections 9.3 and 9.4). 
In all these cases, our goal is then to ensure that the set of conditional (and, if applicable, 
marginal) probabilities provided are internally consistent. By this I mean that there should 
exist at least one set of joint probabilities from which the exogenously supplied conditional 
(or, if applicable, marginal) probabilities could have been derived. 
The general methodology consists in checking the exogenously assigned probabilities for 
consistency, and until consistency is proven we allow the conditional probabilities to be 
moved within a widening range of values. The speciﬁc tool we employ for the task is the 
(Revised) Simplex Method applied to Linear Programming described in the Appendix. At 
each iteration the algorithm suggests a consistent solution, and at any time this solution can 
be accepted (even if it is outside the current range). 
The formulation is as follows.2 To each of the stress events we associate an indicator 
variable, Ii, i  = 1, 2, . . .  , N  , where  Ii = 1 if event Ei occurs, and Ii = 0 otherwise. As  
we know, for two-valued Boolean random functions, there are 2N mutually exclusive and 
exhaustive joint events corresponding to the distinct combinations of the set of indicators 
Ii s. We signify any given combination of Ii s by the vector I , and the set of all I s by  I. To  
each vector I (i.e., to each possible combination of occurrence or non-occurrence for the 
events Ei ) we associate a probability p[I ]. 

 
In order for any set of proposed conditional probabilities, P Ei |Ej , to be  consistent 
there must exist (at least) one set of p[I ] of joint probabilities with 
p[I ] = 1 
(10.1) 
I ∈I 
1 ≤ p[I ] ≥ 0 
(10.2) 
and such that the corresponding conditional probabilities (for conditioning on a single event) 
are given by 
P 
 
Ei |Ej 
 
= P Ei ∩ Ej 
I :{Ii =1,Ij =1} p[I ] 

 
= 
 
(10.3) 
P Ej 
I :{Ij =1} p[I ] 
Similarly, any set of doubly-conditioned probabilities, P Ei |Ej, Ek , are said to be con-
sistent if there exists (at least) one set of p[I ] of joint probabilities satisfying the same 
conditions (10.1) and (10.2) and such that the corresponding conditional probabilities (for 
double-conditioning) are given by 
 
 

 
P 
 
Ei |Ej, Ek 
 
= P Ei  ∩ Ej ∩ Ek 
I :{Ii =1,Ij =1.,Ik =1} p[I ] 
 
= 
 
(10.4) 
P Ej ∩ Ek 
I :{Ij =1,Ik =1} p[I ] 
Finally, we can also deﬁne marginal probabilities to be consistent if they can be derived 
from joint probabilities satisfying the same conditions (10.1) and (10.2) and such that they 
2The following treatment is based on Kwiatkowski and Rebonato (2010). 

 
 
 
 
 
 
 
 
 
 
10.2 COHERENT SOLUTION WITH CONDITIONAL PROBABILITIES ONLY 
137 
can be expressed in terms of the joint probabilities p[I ] as follows: 
P (Ei) = 
p[I ] 
(10.5) 
I :{ Ii = 1} 
For the sake of clarity, it is useful to reproduce again the matrix shown in Section 6.2. For 
n = 3 the matrix has the following representation: 
E1 
E2 
E3 
J1 
1 
0 
0 
p(1) 
J2 
0 
1 
0 
p(2) 
J3 
0 
0 
1 
p(3) 
J4 
1 
1 
0 
p(4) 
(10.6) 
J5 
1 
0 
1 
p(5) 
J6 
0 
1 
1 
p(6) 
J7 
1 
1 
1 
p(7) 
J8 
0 
0 
0 
p(8) 
So, an expression such as 
I :{ I2= 1} p[I ] indicates the sum of all the probabilities for 
which I2 = 1, i.e., it gives P (E2). In our case this means p(2) + p(4) + p(6) + p(7). An  
expression such as 
I :{ I1= 1,Ij = 2} p[I ] indicates the sum of all the probabilities for which 
both I1 = 1 and I2 = 1 (i.e.,  P (E1 ∩ E2)). For the matrix above this means p(4) + p(7). 
So, the conditional probability P (E1| E2) is given by 
I :{ I1= 1,Ij = 2} p[I ] 
p(4) + p(7)
P (E1| E2) = 
 
= 
(10.7) 
I :{ I2= 1} p[I ] 
p(2) + p(4) + p(6) + p(7) 
Let us go back to the optimization problem. Any proposed set of marginal and/or condi-
tional probabilities proposed on the basis of expert subjective judgement by a risk manager 
is very unlikely to be internally consistent. We therefore allow the marginal and conditional 
probabilities to lie anywhere between upper and lower limits, P ± (· ), as follows: 
P ± (Ei) for the marginal probabilities 
(10.8) 
P ± Ei | Ej 
for the conditional probabilities 
(10.9) 
P ± Ei | Ej, Ek for the doubly-conditioned probabilities 
(10.10) 
These upper and lower limits reﬂect a conﬁdence tolerance, and can be speciﬁed by the risk 
manager. They will be used to introduce the inequality constraints needed for the Linear 
Programming approach we intend to employ. Let us look again at Equation (10.5): 
P (Ei) = 
p[I ] ===⇒ P (Ei) − 
p[I ] = 0 
(10.11) 
I :{ Ii = 1} 
I :{ Ii = 1} 

 
 
 
 
 
 
  
 
 
 
 
 
 
  
 
 
  
 
 
 
 
 
 
138 CHAPTER 10 OBTAINING A COHERENT SOLUTION I: LINEAR PROGRAMMING 
From our deﬁnition of upper and lower limits, it therefore follows that, for each event Ei , 
P + (Ei) − 
p[I ] ≥ 0 
(10.12) 
I :{Ii =1} 
P − (Ei) − 
p[I ] ≤ 0 
(10.13) 
I :{Ii =1} 
What we are saying here is that, on the basis of our expert judgement, we have assigned to 
event Ei a marginal probability P (Ei). We are not too sure about this value, however, and 
we would not be too unhappy if the marginal probability were anywhere between P − (Ei) 
and P + (Ei). 
We can follow the same approach for the conditional probabilities. Look back at 
Equation (10.3): 
P 
 
Ei |Ej 
 
= P Ei ∩ Ej 
I :{Ii =1,Ij =1} p[I ] 

 
= 
 
(10.14) 
P Ej 
I :{Ij =1} p[I ] 
We can rearrange it as 
P Ei |Ej 
p[I ] = 
p[I ] 
(10.15) 
I :{Ij =1} 
I :{Ii =1,Ij =1} 
Replacing the upper and lower limits, P ± Ei |Ej , for  P Ei |Ej we therefore have for 
each i and j , the two inequalities (linear constraints) 
P − Ei |Ej 
P [I ] − 
P [I ] ≤ 0 
(10.16) 
I :{Ij =1} 
I :{Ii =1,Ij =1} 
and 
P + Ei |Ej 
P [I ] − 
P [I ] ≥ 0 
(10.17) 
I :{Ij =1} 
I :{Ii =1,Ij =1} 
Finally, we can do the same with the doubly-conditioned probabilities. From 

 
I :{Ii =1,Ij =1,Ik =1} p[I ] 
P Ei |Ej, Ek = 
 
(10.18) 
I :{Ij =1,Ik =1} p[I ] 
we rearrange as 
P Ei |Ej, Ek 
p[I ] = 
p[I ] 
(10.19) 
I :{Ij =1,Ik =1} 
I :{Ii =1,Ij =1,Ik =1} 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
  
 
 
 
 
 
 
 
 
 
 
10.2 COHERENT SOLUTION WITH CONDITIONAL PROBABILITIES ONLY 
139 
and obtain3 
P + Ei |Ej, Ek 
p[I ] − 
p[I ] ≥0 
(10.20) 
I :{Ij =1,Ik =1} 
I :{Ii =1,Ij =1,Ik =1} 
P −Ei |Ej, Ek 
p[I ] − 
p[I ] ≤0 
(10.21) 
I :{Ij =1,Ik =1} 
I :{Ii =1,Ij =1,Ik =1} 
Again, the same interpretation applies: the risk manager may not be too sure of the 
precise value for the singly- or doubly-conditioned probabilities she has assigned, but she 
would not be too unhappy if they turned out to lie anywhere between P −Ei |Ej 
and 
P + Ei |Ej or between P − Ei |Ej, Ek and P + Ei |Ej, Ek , for the singly- or doubly-
conditioned probabilities, respectively. 
Following the standard Linear Programming approach sketched in the Appendix, we can 
±
±
now deﬁne non-negative ‘slack’ variables, πi 
±, πi|j and πi|j,k : 
π −= −P −(Ei) + 
p[I ] 
(10.22) 
i 
I :{Ii =1} 
π + = +P + (Ei) − 
p[I ] 
(10.23) 
i 
I :{Ii =1} 
−
πi|j = −P −Ei |Ej 
p[I ] + 
p[I ] 
(10.24) 
I :{Ij =1} 
I :{Ii =1,Ij =1} 
π + = P + Ei |Ej 
p[I ] − 
p[I ] 
(10.25) 
i|j 
I :{Ij =1} 
I :{Ii =1,Ij =1} 
−
πi|j,k = −P −Ei |Ej, Ek 
p[I ] + 
p[I ] 
(10.26) 
I :{Ij =1,Ik =1} 
I :{Ii =1,Ij =1,Ik =1} 
π +
= P + Ei |Ej, Ek 
p[I ] − 
p[I ] 
(10.27) 
i|j,k 
I :{Ij =1,Ik =1} 
I :{Ii =1,Ij =1,Ik =1} 
Given our deﬁnitions, the slack variables are indeed all positive, as they should be: 
±
±
πi 
±, πi|j, πi|j,k ≥0 for any i, j, k 
(10.28) 
3The careful reader will have noticed that the risk manager is now also required to provide the quantity 
 
! p[I ], which is the joint probability of occurrence of two events. This seems to be asking for more 
I : Ij =1,Ik =1 
than I have so far advertised. However, we are dealing with the joint probability of occurrence of two events, say 
P (A,  B), which can always be written as 
P (A,  B)  = P (A|B)P (B) 
Therefore the risk manager still does not have to provide anything more complex than singly-conditioned and 
marginal probabilities. 

 
 
 
 
 
140 CHAPTER 10 OBTAINING A COHERENT SOLUTION I: LINEAR PROGRAMMING 
We refer to any set of joint probabilities {p[I ]} that obey Equations (10.1) and (10.2) as 
a ‘feasible’ solution. We call any set of joint probabilities {p[I ]} that in addition obey the 
constraint Equations (10.12), (10.13), (10.16), (10.17), (10.20) and (10.21) as a ‘coherent ’ 
solution. If we can ﬁnd a coherent solution then the constraints assigned by the risk manager 
are consistent, otherwise they are not. 
In order to ﬁnd a coherent solution, we use Phase 1 of the Revised Simplex Method (see, 
e.g., Press et al. (1996)) that I outline in the Appendix. This involves deﬁning non-negative 
artiﬁcial variables to give an initial ‘basic’ solution,4 and minimizing the sum – which we 
shall call z – of the artiﬁcial variables subject to the given constraints. If the minimum value 
of z – let us call it zmin – is greater than zero, then we have not found a coherent solution, 
that is, not all the constraint equations are satisﬁed. However, the solution we have found 
is still feasible, which means that the probabilities {p[I ]} are non-negative and add up to 1. 
When zmin is greater than zero our optimal solution will be, in some sense, as ‘close’ as we 
can get to a coherent solution. The risk manager can then either accept the resulting ‘close’ 
solution or widen the limits, P ± (Ei), P ± Ei |Ej and P ± Ei |Ej, Ek , within which the 
marginal and conditional probabilities are allowed to lie. 
It is worth noting that, if no marginal probabilities are provided, any feasible solution 
gives rise to an inﬁnite number of related solutions. Indeed, if we denote by 0 the vector 
I whose components are all zero, by p [0] the (joint) probability that none of the events 
happens, by p [I ] a generic vector of joint probabilities, and by popt [I ] a given feasible 
solution, then any solution such that 
1 + α 
p [0] = 1 + αpopt [0] popt [0] 
(10.29) 
and 
1 
p [I ] = 1 + αpopt [0] popt [I ] for I ̸= 0 
(10.30) 
is also feasible for −1 ≤ α. Scaling all the individual probabilities (except p [0]) by the  
same proportion clearly does not affect the conditional probabilities. This means that the 
solution can be associated with unconditional probabilities that are arbitrarily small or large 
(provided, of course, that none of them is greater than 1). 
Note also that, given any proposed solution with p[I ] ≥ 0 for any I ∈ I, we can always 
ﬁnd a feasible solution (i.e., the additional constraint 
I ∈I p[I ] = 1 can always be satisﬁed) 
simply by dividing each p[I ] by the  sum of the  p[I ]s, and this will not affect the conditional 
probabilities. 
4Recall that a ‘basic’ solution is one in which all but M of the variables are set to zero (where M is the number 
of constraints – excluding the non-negativity constraints). At each iteration, each of the basic variables and the 
objective function (z in our case) are expressed as afﬁne functions of the non-basic variables. See the worked-out 
example in Section 10.5. 

10.3 THE METHODOLOGY IN PRACTICE: FIRST PASS 
141 
10.3 The Methodology in Practice: First Pass 
Before delving into the implementation of the Linear Programming approach, let me give 
a ﬂavour of how I intend the approach to work. I will assume for the moment that all the 
conditional probabilities assigned in this section satisfy the internally consistency require-
ments, and are therefore bona ﬁde probabilities.5 
Suppose that we have n positions, π1, π2, . . .  , πn, which have been identiﬁed as ‘large’ 
by the desk risk managers. Let E1, E2, . . .  , En , i = 1, 2, . . .  , 2n, denote the signiﬁcant 
events that would give rise to large proﬁts or losses arising from our positions {πi } at 
a given point in time. Let Gi and Li , i = 1, 2, . . .  n, be the largest plausible proﬁts and 
losses, respectively, associated with position πi if event Ei occurs. (I am using G – as in 
‘Gain’ – rather than P – for Proﬁt – because I am trying to avoid confusion with P for 
probability.)6 So, to ﬁx ideas, position 1, π1, could be a large long equity position in the 
S&P. There are two events associated with position 1, E1 and E2: E1 could be a 1987-like 
equity market crash; E2 could be a large market rally. The rise and fall need not be of the 
same magnitude. Associated with event E1 there is a proﬁt, G1, and a loss, L1. Both proﬁt  
G1 and loss L1 are a function of the event E1 (the equity market crash) and of the position 
π1: G1 = G1 (E1, π1), L1 = L1 (E1, π1). Similarly, there is a proﬁt, G2, and a loss, L2, 
associated with the equity rally. 
I want to stress that these events encompass not only the large losses, but also the 
large possible gains arising from the most signiﬁcant positions {πi }. Of course, for a given 
position, the expected proﬁt or loss need not be just the same number with the opposite sign. 
This can be because the moves in the underlying are not symmetric (equity markets ‘crash’ in 
different ways than they spike, volatilities do not fall in the same way as they suddenly rise, 
etc.,) or because the underlying position may be not linear (e.g., a short-gamma position). 
The notation used in the following assumes that, say, a yield curve steepening and a yield 
curve ﬂattening are two distinct events. If our portfolio contains, say, a steepener position, 
then the loss for the steepener scenario would be zero, and the gain for ﬂattening scenario 
would be zero. In short, given a large position, the associated events will, in general, be the 
largest plausible moves that give rise to proﬁts or losses . 
As mentioned in the introductory section, we assume that the risk manager can express 
an informed opinion about the probability, P (Ej |Ei), of occurrence of event Ej conditional 
on event Ei having occurred. For instance, if event Ei were a major equity market crash, 
the event Ej associated with a drop in equity volatilities would have a very low probability; 
however, the event Ek associated with widening in credit spreads could have a signiﬁcant 
probability. In the following, to avoid spurious precision, these conditional probabilities can 
initially be grouped into buckets, as suggested in Section 9.4. 
5The treatment that follows requires a somewhat more complex set-up, in that I will deal with scenarios that give 
rise to large losses and proﬁts. This is much more satisfactory, because we can better handle ‘diversiﬁcation’ and 
‘proxy hedging’. Conceptually, there are few differences. In practice, however, the approach requires considerably 
more care, and the conditional probability tables are more complex. After this section I will never deal again with 
the case of proﬁts and losses. 
6Ideally, these should be the proﬁts or losses that our portfolio could incur over and above the hypothetical 
proﬁts or losses that our VaR engine, run with the longest time series and volatility rescaling, produces . 

142 CHAPTER 10 OBTAINING A COHERENT SOLUTION I: LINEAR PROGRAMMING 
Therefore, given n large positions, there will be in general 2n events, with a proﬁt, Pi , 
and a loss, Gi , associated with each. 
Consider now the [2n × 1] matrix y, deﬁned by 
y = p · E 
⎤
⎡ 
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
y1 
y2 
. . .  
. . .  
y2n−1 
y2n 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
⎤
⎡ 
= 
0
0 
P (E2|E1)
P
 
(E2|E1) . . .  P (E2n|E1)
P
 
(E2n|E1) 
P (E1|E2)
P
 
(E1|E2) 0  
0  
. . .  P (E2n|E2)
P
 
(E2n|E2)
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
= 
0
0 
⎤
⎡ 
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
L1 
G1 
L2 
G2 
Ln 
Gn 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
(10.31)
× 
Consider, say, the second entry of the matrix y. Given the deﬁnition above it is given by 
y2 = (L1 + G1) P (E1|E2) + 0 + (L3 + G3) 
× P (E3|E2) + . . .  (Ln + Gn) P (E2n|E2) 
(10.32) 
This can be interpreted as the (conditional) expectation of the proﬁts and losses incurred by 
our portfolio due to positions other than position 2 if the second scenario event materializes. 
The total stress loss, SL2, if the second event scenario materializes is therefore given by 
the sum of y2 and the loss associated with the second scenario: 
SL2 = L2 + y2 
(10.33) 

10.3 THE METHODOLOGY IN PRACTICE: FIRST PASS 
143 
As Equation (10.31) shows, the matrix equation above can be written more concisely as 
y = P · E 
⎤
⎡ y1 
y2 
. . .  
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
= 
. . .  
y2n−1 
y2n 
0 
⎤
⎡ 
P (E2|E1)
P
 
(E3|E1) . . .  . . .  P (En−1|E1)
P
 
(En|E1) 
|E2) 
0 
P (E3|E2) . . .  . . .  P (En−1|E2)
P
 
(En|
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
P (E1
E2) 
0 
0
= 
0 
P (E1|En−1) 
0 
P (En|En−1) 
P (E1|En)
 P
 
(E2 En)
 P
 
(En−1 En) 0
|
|
⎡
⎤ 
L1 + G1 
L2 + G2 
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
(10.34)
× 
Ln + Gn 
A simple example can clarify the notation and the approach. Let event 1 be an equity 
market crash (S&P). (Event 2 can be an equity market rally, but we do not consider this in 
this example.) Let event 3 be a ﬂattening of the US$ yield curve and event 4 a steepening of 
the US$ yield curve. We assume that we are long the S&P and have a yield curve steepener 
on. Given an equity market crash we have a conditional probability of a curve steepening of 
the US$ curve, P (E4|E1), of 80%, and a conditional probability, P (E3|E1), of a ﬂattening 
of 20% (the two probabilities only happen to add up to 1 by chance). 
Since we have a long equity position and a yield curve steepener we then have the 
following: 
• a gain,  G4, and a loss, L4, of, say, $100m and $0m, respectively, associated with event 
4 (the steepening of the curve) – L4 is 0 because there is no loss from our position (a 
steepener) if event 4 happens, i.e., if the curve indeed steepens; 

144 CHAPTER 10 OBTAINING A COHERENT SOLUTION I: LINEAR PROGRAMMING 
• a gain,  G3, and a loss, L3, of, say, $0m and $100m, respectively, associated with event 
3 (the ﬂattening of the curve) – it is G3 that is now 0 because there is no gain from 
our position (a steepener) if event 3 happens, i.e., if the curve ﬂattens; 
• a stand-alone loss, L1, of, say, $200m on the S&P position if the equity market crash 
occurs. 
The conditional losses associated with event 1 (equity market crash) are given by 
y1 = P (E3|E1)L3 + P (E3|E1)G3 + P (E4|E1)L4 + P (E4|E1)G4 
= P (E3|E1)L3 + P (E3|E1)(0) + P (E4|E1)(0) + P (E4|E1)G4 
(10.35) 
The conditional expected stress loss 1, SL1, associated with the equity market crash is 
then given by 
SL1 = L1 + (G3 + L3) P (E3|E1) + (G4 + L4) P (E4|E1) 
= L1 + (0 + L3) P (E3|E1) + (G4 + 0) P (E4|E1) 
= −$200m −$100m ∗0.2 + $100m ∗0.7 = −$150m 
(10.36) 
It must be stressed again that the proﬁts and losses L2, G2, L3 and G3 do not depend 
on event 1 (the equity market crash), and are unconditional (marginal) quantities. The link 
with the equity market crash comes only via the conditional probabilities, P (E4|E1) and 
P (E3|E1). 
As our intuition suggests, in the example above the steepening position mitigates the 
stand-alone loss in the naked equity position, reducing it from $200m to $150m. 
10.4 The CPU Cost of the Approach 
The Simplex solution described above was ﬁrst prototyped in VBA; for 12 scenarios this 
was taking of the order of 5–10 minutes to run. Coding it in C++ made the routines run 
10 to 20 times faster. 
Preliminary analysis suggests that each additional scenario will increase the run time by 
a factor of about three; considering that the analysis is likely to be required infrequently 
(say, on a weekly basis), the computation time should not be a problem with up to, say, 
15–20 scenarios. In fact we are more likely to run into memory-space problems ﬁrst. In 
any case, the main difﬁculties when dealing with 15–20 scenarios are of a cognitive nature, 
that is, they lie in the speciﬁcation of the required conditional probability tables. I would 
not recommend working with more than about ten scenarios. 
10.5 Illustration of the Linear Programming Technique 
I now move to illustrating the precise workings of the Linear Programming technique 
described above in cleansing and ﬁxing an exogenously assigned conditional probability 

10.5 ILLUSTRATION OF THE LINEAR PROGRAMMING TECHNIQUE 
145 
matrix. In order to illustrate the procedure in practice I present in this section the results of 
a very simple ﬁctitious case study. I assume, in particular, that the risk manager has only 
assigned singly-conditioned probabilities. 
We have four scenarios, A, B, C and D, and we suppose that the risk manager supplied 
the following conditional probabilities:7 
⎤
⎡ 
⎢⎢⎢⎢⎣ 
A
B
C
D 
A 1
0.60 0.50 0.60 
B 
0.80
1 
0.22 0.20 
C 0.50 0.20
1 
0.00 
D 0.40 0.40 0.00 1 
⎥⎥⎥⎥⎦
(10.37) 
where, for example, the number 0.50 in the ﬁrst row represents P (C|A). 
We ﬁrst observe that P (C|A) + P (D|A) = 1.1. But, from the matrix, C and D are 
mutually exclusive events (P (C|D) = P (D|C) = 0). Therefore we need to reduce either 
or both of P (C|A) and P (D|A) so that their sum is ≤ 1. In fact we would make the sum 
strictly less than 1 to avoid making the overly strong statement that if A occurs then C or 
D must occur. 
Suppose that the risk manager therefore sets P (C|A) = 0.4 and  P (D|A) = 0.5. If we 
now look at the triplet {A, B, D}, we  have  
0.80 0.40 
P (A|D) = P (D|A) P (A|B) P (B|D) = 0.5 
= 1.333 
(10.38)
P (B|A) P (D|B) 
0.60 0.20 
So the ﬁve probabilities to the right of the ﬁrst ‘=’ sign are clearly inconsistent. To ﬁx the 
problem, the risk manager could set P (D|B) = 0.3, for example, which gives P (A|D) = 
0.89. Our revised matrix therefore becomes: 
⎤
⎡ 
⎢⎢⎢⎢⎣ 
A
B
C
D 
A 1
0.60 0.40 0.50 
B 
0.80
1 
0.22 0.30 
C 0.50 0.20
1 
0.00 
D 0.89 0.40 0.00 1 
⎥⎥⎥⎥⎦ 
(10.39) 
which shows no obvious inconsistencies. Note, however, that to ‘ﬁx the problem’ the 
risk manager has had to change substantially her original estimate of P (A|D) (from 
0.40 to 0.89). The plausibility of this change should be questioned. Was the original 
suggestion ill-thought-out? Or is the ‘ﬁx’ forcing assumptions on the risk manager she is 
not comfortable with? We view this as an important and useful part of the process (the 
‘auditable decision process’). 
We now move to the next phase, to ensure consistency of the plausible and ‘cleansed’ 
matrix. Suppose that the risk manager allows each of the conditional probabilities8 to lie 
7The conditional-probability matrix presented here is not particularly realistic from a stress-testing perspective. 
(Do you agree? Why?) It has only been provided to illustrate the numerical aspects of the methodology. Note, in 
particular, the assumed incompatibility between events C and D. 
8Except, of course, those that are 1 or 0. 

 
 
 
 
 
 
 
 
  
 
 
 
 
146 CHAPTER 10 OBTAINING A COHERENT SOLUTION I: LINEAR PROGRAMMING 
within a range 
P Ei |Ej 1 (1 − δ) ≤ P Ei |Ej 
≤ P Ei |Ej 
+ δ 1 − P Ei |Ej 
(10.40)
2
1 
1 
where the P Ei |Ej s and  P Ei |Ej 2s are taken from the revised matrix above (the sub-
1
scripts 1 and 2 indicate the ﬁrst or second iteration of the procedure, respectively). Note 
that at this stage we are not yet necessarily dealing with bona ﬁde, internally consistent 
conditional probabilities. Taking δ = 0.01 gives lower and upper bounds: 
⎤
⎡ 
⎢⎢⎢⎢⎣ 
A
B
C
D 
A 1
0.59 0.40 0.50 
B 
0.79
1 
0.22 0.30 
C 0.50 0.20
1 
0.00 
D 0.88 0.40 0.00 1 
⎥⎥⎥⎥⎦ 
(10.41) 
and 
⎤
⎡ 
⎢⎢⎢⎢⎣ 
A
B
C
D 
A 1
0.60 0.41 0.51 
B 
0.80
1 
0.23 0.31 
C 0.51 0.20
1 
0.00 
D 0.89 0.41 0.00 1 
⎥⎥⎥⎥⎦ 
(10.42) 
respectively.9 
We now deﬁne the 16 indicator vectors and their corresponding probabilities: 
⎤
⎡ 
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
I 8 
I 9 
I 10 
I 11 
I 12 
I 13 
I 14 
I 15 
Indicator A
B
C
D
 Prob 
I 0 
0
0
0
0 
p(0) 
I 1 
0
0
0
1 
p(1) 
I 2 
0
0
1
0 
p(2) 
I 3 
0
0
1
1 
p(3) 
I 4 
0
1
0
0 
p(4) 
I 5 
0
1
0
1 
p(5) 
I 6 
0
1
1
0 
p(6) 
I 7 
0
1
1
1 
p(7) 
p(8) 
p(9) 
p(10) 
p(11) 
p(12) 
p(13) 
p(14) 
p(15) 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
(10.43) 
1 
1 
1 
1 
1 
1 
1 
1 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
1 
0 
1 
0 
1 
0 
1 
9Note that the incompatibility constraint has remained ‘hard’. 

 
  
 
 
  
 
 
  
 
10.5 ILLUSTRATION OF THE LINEAR PROGRAMMING TECHNIQUE 
147 
Recall that indicator variables represent the occurrence of combinations of scenarios. 
Thus, for example, the indicator variable I 3 represents the outcome that scenarios C and D 
occur and scenarios A and B do not occur, and p(3) is the corresponding joint probability 
(to be solved for). 
We have 12 constraints of the form: 
P −Ei |Ej 
p[I ] − 
p[I ] ≤0 
(10.44) 
I :{Ii =1} 
I :{Ii =1,Ij =1} 
and 12 of the  form  
P + Ei |Ej 
p[I ] − 
p[I ] ≥0 
(10.45) 
I :{Ii =1} 
I :{Ii =1,Ij =1} 
For example, corresponding to P [D|A] we have 
P [D|A] ∗{p(8) + p(9) + p(10) + p(11) + p(12) + p(13) + p(14) + p(15)} 
−{p(9) + p(11) + p(13) + p(15)} ≤0 
(10.46) 
that is, 
0.5 ∗{p(8) + p(9) + p(10) + p(11) + p(12) + p(13) + p(14) + p(15)} 
−{p(9) + p(11) + p(13) + p(15)} ≤0 
(10.47) 
where the term 0.5 comes from P [D|A] = 0.5. 
Recall that we deal with inequality constraints by introducing slack variables, and turning 
the inequality into equality constraints. We do this now by introducing the slack variable 
−
πD|A, – see Equation (10.22): 
−
πi|j = −P −Ei |Ej 
P [I ] + 
P [I ] 
(10.48) 
I :{Ij =1} 
I :{Ii =1,Ij =1} 
This gives 
P [D|A] ∗{p(8) + p(10) + p(12) + p(14) −p(9) −p(11) −p(13) −p(15)} 
−
+ πD|A = 0 
(10.49) 
that is, 
0.5 ∗{p(8) + p(10) + p(12) + p(14) −p(9) −p(11) −p(13) −p(15)} 
−
+ πD|A = 0 
(10.50) 
where π− 
A is the ‘slack’ variable corresponding to inequality constraint (10.46). 
D|

148 CHAPTER 10 OBTAINING A COHERENT SOLUTION I: LINEAR PROGRAMMING 
Trivially, all the constraints, as formulated, can be satisﬁed by setting p(0) = 1 (the prob-
ability of no scenarios occurring is one), and all other variables to zero. Clearly, however, in 
this case the conditional probabilities are not deﬁned and we do not have an economically 
interesting solution. Therefore we introduce non-negative ‘artiﬁcial’ variables, aX|Y , of the  
following kind: 
aD|A = 0.5 {P (8) + P (10) + P (12) + P (14) − P (9) − P (11) − P (13) − P (15)} 
−
+ πD|A 
(10.51) 
We create all the possible artiﬁcial variables aX|Y , for  X, Y = A, B, C, D. In order to get 
the algorithm started we initialize all of these to some arbitrarily small value, ε. 
Our initial (infeasible) solution is given by setting all the artiﬁcial variables to ε, and all 
other variables to zero. 
As explained above, we need now to express the artiﬁcial variables in terms of the 
currently zero variables.10 This can be done by summing the right-hand sides of all the 
equations exempliﬁed by Equation (10.51). We are ﬁnally in a position to use the Revised 
Simplex Algorithm to minimize this expression. 
This gives the following solution: 
⎤
⎡ 
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 
Indicator A
B
C
D
 K 
I 2 
0
0
1
0
1.606 
I 1 
0
0
0
1
0.336 
I 14 
1
1
1
0
0.702 
I 9 
1
0
0
1
0.865 
I 13 
1 
1 
0 
1 
0.946 
I 12 
1 
1 
0 
0 
0.537 
I 4 
0 
1 
0 
0 
0.664 
I 10 
1 
0 
1 
0 
0.734 
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ 
(10.52) 
where K is a quantity proportional to the joint probabilities and all other joint probabilities, 
except p(0), have been set to zero. (Recall that in our starting conditional probability matrix 
we had set, perhaps unwisely, P (C|D) = P (D|C) = 0. This being the case, looking back 
at the table in Equation (10.43), one can see that the joint probabilities p(3), p(7), p(11), 
p (15) have to be zero.) 
Exercise 30 
Why? 
10Recall that this is what is required for the Revised Simplex Algorithm: at each iteration we have a set of 
‘basic’ (in general non-zero) variables, and ‘non-basic’ (zero-valued) variables. The value of each of the basic 
variables and the value of the ‘objective function’ (the expression to be optimized) are expressed as functions of 
the non-basic variables only. See, e.g., Press et al. (1996). 

 
10.6 WHAT CAN WE DO WITH THIS INFORMATION? 
149 
The corresponding solution in terms of conditional probabilities is 
⎤
⎡ 
⎢⎢⎢⎢⎢⎣ 
Trial Solution A 
B 
C 
D 
A 
1.00 0.58 0.38 0.48 
B 
0.77 1.00 0.25 0.33 
C 
0.47 0.23 1.00 0.00 
D 
0.84 0.44 0.00 1.00 
⎥⎥⎥⎥⎥⎦ 
(10.53) 
For ease of comparison I show below the conditional probability matrix from which we 
started: 
⎡
⎤ 
⎢⎢⎢⎢⎢⎣ 
Initial Guess A 
B 
C 
D 
A 
1 
0.60 0.40 0.50 
B 
0.80 1 
0.22 0.30 
C 
0.50 0.20
1 
0.00 
D 
0.89 0.40 0.00 1 
⎥⎥⎥⎥⎥⎦ 
(10.54) 
Comparing these two tables we see that the largest violation of the constraints is for P [A|D], 
where the above solution lies 0.04 below its lower limit. We could accept this solution, 
implicitly making ad hoc changes to the tolerance limits (δ = 0.01) we had assigned at the 
start (i.e., in this case setting δ = 0.04). Or we could increase δ from 0.01, while keeping 
it below 0.04, and re-optimize.11 
10.6 What Can We Do with this Information? 
After the manual cleansing ‘sanity checks’ suggested in Section 9.4 and the systematic 
procedure just presented, we ﬁnally have a logically-coherent matrix of conditional prob-
abilities. It may not be quite the one we had assigned to start with, but it should be very 
close to it. What can we do with it? 
10.6.1 Extracting Information with Conditional Probabilities Only 
Even if the risk manager has not felt conﬁdent to provide any marginal probabilities, we 
can still obtain quantities of interest. Here are a few examples of what we can do. 
To begin with, to each event, Ei , we can associate a conditional expected loss, Li : 
Li = Li + 
P (Ej |Ei)Lj 
(10.55) 
j ̸=i 
This is the loss we can expect, given that event Ei has occurred . It  is  made  up  of  two  
components: the stand-alone loss from events Ei , Li , that will occur with certainty if event 
11Perhaps if we had set a less ambitious tolerance δ, say  δ = 0.02, we may ﬁnd a solution closer to our initial 
guess than the one we just obtained. 

150 CHAPTER 10 OBTAINING A COHERENT SOLUTION I: LINEAR PROGRAMMING 
Ei occurs, plus the sum from all the losses from the other events, Lj , each weighted by its 
conditional probability of occurrence.12 
By looking at all the events in turn and comparing their conditional expected losses, 
we can ascertain which individual events are expected to be associated with the largest 
conditional expected losses. Arguably, these are the events on which the risk manager 
should focus her attention most closely. Limited as it may be, this information is simple, 
transparent and lends itself to questioning by traders and senior management. Something 
not to be sniffed at. 
Note that we still cannot say anything about which joint events produce the largest 
expected losses, because of the indeterminacy in the joint probabilities that I highlighted 
above. The conditional expected loss conveys more information than the stand-alone loss, 
but less information than the loss associated with a joint event and the probability associated 
with that loss. 
We can do more. To start with, if one wanted to link regulatory capital, RC, to the  
conditional expected losses, these largest losses would also be a natural starting point. A 
plausible suggestion could be 
 
Li 
 
RC = max 
(10.56) 
A further simple piece of analysis that can be carried out once we have the set of expected 
conditional losses is the following. After running the analysis for several weeks, the risk 
manager can begin to record the number of times that the maximum conditional loss is 
associated with event E1, E2, . . .  , En. She can then build a simple histogram. If the same 
event, say Ek , were always, or predominantly, associated with the maximum conditional 
loss, this would suggest a portfolio with a very clear and persistent risk concentration. As 
the analysis is repeated week after week, it is also useful to track the time series of the 
top-most expected conditional losses. 
There is more that we can do with the minimal input of the conditional probability matrix. 
Let us for simplicity assume now that, if event Ej does not occur, then the loss associated 
with that event will not be incurred at all (Lj = 0). Recall also that 
P (Ej |Ei) + P (  
Ej |Ei) = 1 
(10.57) 
Now, consider events Ei and Ej . Conditional on event Ei having occurred, event Ej 
will therefore contribute a loss Lj with probability P (Ej |Ei), and no loss with probability 
P (  
Ej |Ei). Of course, the contribution to the expected conditional loss from event Ej given 
that event Ei has occurred, Lj |i , is then just 
Lj |i = P (Ej |Ei)Lj + P (  
Ej |Ei) ∗ 0 = P (Ej |Ei)Lj 
(10.58) 
as we saw before. We can therefore redeﬁne 
 
 
Li = Li + 
P (Ej |Ei)Lj = Li + 
Lj |i 
(10.59) 
=i
j ̸
j ̸
=i 
12The generalization to the case when offsetting proﬁt-making positions are taken into account is straightfor-
ward. 

 
10.6 WHAT CAN WE DO WITH THIS INFORMATION? 
151 
And if we decide by convention to assign 
P (Ej |Ej) = 1 ∀j 
(10.60) 
then we can write this even more compactly as 
Li = 
Lj |i 
(10.61) 
j 
The variance of loss from event Ej conditional on event Ei having occurred, varj |i , is then 

2 

2 
varj |i = Lj − Lj |i 
P (Ej |Ei) + 0 − Lj |i 
P (  
Ej |Ei) 

2 

2 
= Lj − Lj |i 
P (Ej |Ei) + Lj |i 
P (  
Ej |Ei) 
 
2 

2 
 
= Lj − Lj |i 
P (Ej |Ei) + Lj |i 
1 − P (Ej |Ei) 
(10.62) 
So, given the occurrence of event Ei , we can associate a conditional variance to each event 
Ej ̸= Ei . For a given event Ei we have n such conditional variances, and so the conditional 
variance matrix can be arranged in an n × n matrix. 
All of this sounds (and is) interesting and informative, but we must be careful to attribute 
the correct meaning to this quantity, and to understand clearly what this variance does and 
does not capture. We are dealing with two-valued Boolean random variables, which can 
either occur or not. Given that one event has occurred, there is therefore no variability 
whatsoever in the magnitude of the associated loss.13 Also, we have not allowed for any 
uncertainty in the conditional probabilities. So, the variance just deﬁned is only a function 
of the magnitude of the conditional loss associated with event Ei and of its conditional 
probability of occurrence given that the same event Ei has happened. All the uncertainty 
comes from whether the loss happens or not, and from the conditional probability of the loss. 
This is an interesting quantity in itself, but probably not quite what we would like to 
have. In order to get a measure of uncertainty closer to what we are more likely to be 
interested in, we now have to bring in infromation about the marginal probabilities, P (Ei). 
10.6.2 Extracting Information with Conditional and Marginal 
Probabilities 
As I mentioned at the beginning of the treatment, I recommend avoiding spurious precision 
by assigning conditional probabilities to coarse probability buckets. Also, it is much easier 
k
to work with the multiplicative factors xi (see Sections 5.9, 5.10 and 9.3), rather than 
1
directly with the conditional probabilities. Multiplicative factors of 0 or P(Ei) are left to 
the courageous, and for the reasons explained in Sections 5.9 and 9.3, I prefer to work 
with minimum and maximum factors of ε and (1 − ε) max{P(E
1 
i),P (Ej )} . Of course, if the 
k
marginal probabilities are given, each multiplicative factor xi uniquely determines the 
13Of course, in principle, we could remedy this by dealing with multi-valued Boolean variables (see, e.g., 
Moore (2001)). Given how difﬁcult it is to assign the associated probabilities I have not pursued this route in this 
book. 

152 CHAPTER 10 OBTAINING A COHERENT SOLUTION I: LINEAR PROGRAMMING 
associated probability P (Ei |Ek). Therefore, once the xk factors have been determined, one 
i 
can equivalently talk of coarse conditional probability buckets. 
For a given set of these coarse conditional probability buckets, the risk manager can then 
do the following: 
• Take in turn each of the ﬁrst-pass conditional probabilities assigned by the risk manager 
before the Linear Programming procedure described above. 
• Randomly re-allocate each conditional probability to the same bucket or halfway 
between the assigned bucket and the bucket above or halfway between the assigned 
14 So, for instance, if the multiplicative 
1 
3 .
bucket and the bucket below with probability 
k
factor xi 
1 
3
had been assigned to be, say, 1, with probability 
it could be left unchanged, 
or moved to 5 or moved to 0.2. 
• Now the usual cleansing of the conditional probabilities can be carried out (via the Lin-
ear Programming technique described above), and the conditional losses re-calculated. 
• This process can be repeated many times and the highest conditional loss can be 
recorded for each iteration. 
• A distribution of maximum conditional losses can ﬁnally be constructed. 
An example of the distribution obtained using this procedure is given in Figure 10.1. 
Despite the nice-looking picture, there is nothing too ‘scientiﬁc’ about this distribution, 
k
because in obtaining it the multiplicative factors xi were moved by a commonsensical but 
arbitrary amount (see the second bullet point above). Nonetheless, this heuristic procedure 
can give an idea of the sensitivity of the results to one of the most delicate inputs of the 
whole procedure, that is, the assignment of the conditional probabilities. What it does not 
address is the variation in the losses due to uncertainty in the occurrence of the different 
events. Can we say something about what we really care about? 
Histogram 
0 
200 
400 
600 
800 
Frequency 
1210
1251
1291
1331
1372
1412
1452
1493
1533
1574
1614
1654
1695 
Bin 
Figure 10.1 The dispersion of the maximum conditional expected loss with uncertainty in the input 
parameters. 
14There is, of course, nothing magical about choosing exactly halfway. Perhaps a quarter of the way may work 
better. The choice, of course, depends on the coarseness of the original buckets (i.e., on the chosen basis of the 
logarithm). 

  
  
 
10.6 WHAT CAN WE DO WITH THIS INFORMATION? 
153 
To see what we can do, we go back to the quantities deﬁned above: 
Lj |i = P (Ej |Ei )Lj 
(10.63) 
and 
 
Li = 
j 
Lj |i 
(10.64) 
If the risk manager feels that she can assign marginal probabilities, P (Ei), to the various 
events Ei , then the total expected loss, L, can be calculated as 
⎡ 
⎤
⎡
⎤ 
L = 
⎣ 
P (Ej |Ei)Lj ⎦ P (Ei) = 
⎣ 
Lj |i ⎦ P (Ei) = 
LiP (Ei) 
(10.65) 
i
j 
i
j
i 
This quantity is another possible candidate for regulatory capital, RC. The variance of the 
distribution can then be obtained in a straightforward manner. 
There are, of course, many more useful things that one can do with the quantities that 
we have estimated. Constructing time series, for instance, of conditional expected losses, 
expected stress losses, variances and so on, is as simple as it is useful. I do not have to 
tell experienced risk managers what to do here – although I provide a few suggestions in 
Chapter 14. 
If the risk manager feels conﬁdent enough to provide enough causal information to 
estimate (bounds for) the joint probabilities of stress events, then a much richer analysis is 
possible. I deal with this situation at the end of the next chapter. 


Chapter 11 
Obtaining a Coherent Solution II: 
Bayesian Nets 
In this chapter I assume that the risk manager has provided the following: 
• the structure of the Bayesian net (nodes, arcs and arrows); 
• the associated conditional probability tables, i.e., the marginal and conditional proba-
bilities for the various stress events, to the desired order. 
I also assume that the entries of the conditional probability tables have already been 
cleansed via the sanity-checks-plus-Linear-Programming approach described in the previ-
ous chapter. Alternatively – or, rather, as a complement – the bounds obtained using the 
approach by Moskowitz and Sarin (1983) described in Chapter 7 may have been used. 
Once the risk manager has at her disposal the required probability tables, the ‘solution’ 
of the problem (i.e., the derivation of the joint probabilities) becomes mechanical and is 
directly obtained via the Bayesian-nets technology, using the properties of Bayesian nets 
and of conditional and joint probabilities highlighted in Chapters 8 and 9. In this chapter 
we shall see how to do this in practice. 
To make the approach feasible, some simpliﬁcations are required. In general, given n 
variables, the risk manager is supposed to be able to specify (n − 1)-conditioned probabil-
ities. But it is not realistic to expect that the risk manager will be able to provide even an 
informed guess for all the required conditional probabilities for n larger than, say, 3 (i.e., 
for more-than-doubly-conditioned probabilities) – and even this is a big ask. As discussed, 
one possible way out of this impasse is to restrict the set of possible Bayesian nets underly-
ing the problem. This ultimately means simplifying the underlying causal structure among 
the variables at hand, and should not therefore be undertaken lightly. When this ‘structural 
pruning’ can be done in a sensible way, however, dramatic simpliﬁcations can be derived 
and the full joint probability structure can be determined with relative ease. 
Because of this pruning, the joint probabilities that we determine using this method 
will be an approximation to the joint probabilities that would have been obtained if God 
had whispered in our ears all the required conditional probabilities. The quality of the 
155 

156 
CHAPTER 11 OBTAINING A COHERENT SOLUTION II: BAYESIAN NETS 
approximation will depend on how much of the underlying causal structure our ruthless 
pruning will have retained. Williamson (2005), as usual, provides a good discussion of 
these subtle points. 
11.1 Solution with Marginal and n-conditioned 
Probabilities 
In light of the above, we therefore assume that we have restricted the class of underlying 
Bayesian nets to, say, nets where each node has at most two parents. In the terminology of 
Chapter 8 (see Section 8.8.2 in particular), this set will be denoted by S2. In this case the 
marginal probability tables required to specify the net in its entirety consist of the marginal 
probabilities associated with each node and with at-most-doubly-conditioned probabilities. 
In order to understand how the assumptions made can help us in obtaining the joint 
probabilities, let us consider a simple Bayesian net, such as the one depicted in Figure 11.1. 
We have four Boolean variables (four stress events), connected by the causal links we have 
speciﬁed in the ﬁgure. 
To help our intuition, I am going to assign an interpretation to the four Boolean 
variables. So, event A could be an equity market crash, event B the default of a large bank 
counterparty, event C a large widening of credit spreads and event D a large increase in 
equity implied volatilities. 
In our simple model of the world we assume that the rise in the implied volatilities of 
equities is directly ‘caused’ by the equity market crash, and by no other variable. We also 
assume that the widening of the credit spreads can be ‘caused’ either by the default of a 
major bank counterparty or by the equity market crash. Finally we have made the assumption 
that the default of the bank counterparty and the equity market crash are independent. This 
is debatable to say the least, but realism is not what I have strived for with this example, 
and the probabilities that I associate with the various events are totally ‘made up’. 
Exercise 31 How would you modify the Bayesian net in such a way that the default of a 
large counterparty is now assumed to cause an equity market crash, and the resulting net 
remains in S2? 
The set-up I have chosen implies that events C and D are conditionally independent: 
once we know whether event A (the equity market crash) has happened or not, the increases 
in implied volatilities and the widening in credit spreads become independent, in the sense 
that knowledge that either has happened does not help us in predicting whether the other 
has occurred. I stress, however, that this is only true once I know whether the equity market 
crash has occurred or not. 
Exercise 32 Assign a different interpretation to the events represented by the four nodes 
A, B, C and D, which is approximately compatible with the dependence structure in 
Figure 11.1. 
We have four two-valued Boolean random variables. The associated joint probabilities are 
therefore 24 = 16. If the risk manager wanted to assign them directly, instead of constructing 
them from simpler building blocks, not only would she have to specify 15 numbers, but 

11.1 SOLUTION WITH MARGINAL AND n-CONDITIONED PROBABILITIES 
157 
A 
B 
MargA 
0.01 
MargNotA 
0.99 
MargD 
0.015 
MargNotD 
0.985 
CondDA 
0.6 
CondAD 
0.4 
CondNotDA 
0.4 
CondNotAD 
0.6 
CondDNotA 0.009091 
CondANotD 0.004061 
CondNotANotD 0.995939 
D 
C 
A 
D 
Joint(AD) 
C 
Joint(ADC) 
1 
0 
0.004 
1 
0.0012 
MargC 
0.02 
0 
1 
0.009 
1 
0.0002 
MargNotC 
0.98 
1 
1 
0.006 
1 
0.0018 
CondCA 
0.3 
0 
0 
0.981 
1 
0.0168 
CondNotCA 
0.7 
1 
0 
0.004 
0 
0.0028 
CondAC 
0.15 
0 
1 
0.009 
0 
0.0088 
CondNotAC 
0.85 
1 
1 
0.006 
0 
0.0042 
CondCNotA 0.017172 
0 
0 
0.981 
0 
0.9642 
CondNotCNotA 0.982828 
1.000000 
A 
D 
Joint(AD) 
MargB 
0.002 
1 
0 
0.0040 
Conditional Independence 
MargNotB 
0.998 
0 
1 
0.0090 
P(C = x|A =y,D = z) = P(C = x|A = y) 
CondCB 
0.95 
1 
1 
0.0060 
x,y,z = T,F 
CondNotCB 
0.05 
0 
0 
0.9810 
CondBC 
0.095 
1 
CondNotBC 
0.905 
CondBNotC 0.000102 
CondNotBNotC 0.999898 
A 
D 
C 
B 
Joint(ADCB) 
1 
0 
1 
0.0012 
1 
0.000114 
0 
1 
1 
0.0002 
1 
1.4682E-05 
1 
1 
1 
0.0018 
1 
0.000171 
0 
0 
1 
0.0168 
1 
0.00160032 
1 
0 
0 
0.0028 
1 
2.8571E-07 
0 
1 
0 
0.0088 
1 
9.026E-07 
1 
1 
0 
0.0042 
1 
4.2857E-07 
0 
0 
0 
0.9642 
1 
9.8383E-05 
1 
0 
1 
0.0012 
0 
0.001086 
0 
1 
1 
0.0002 
0 
0.00013986 
1 
1 
1 
0.0018 
0 
0.001629 
0 
0 
1 
0.0168 
0 
0.01524514 
1 
0 
0 
0.0028 
0 
0.00279971 
0 
1 
0 
0.0088 
0 
0.00884455 
1 
1 
0 
0.0042 
0 
0.00419957 
0 
0 
0 
0.9642 
0 
0.96405616 
1 
Figure 11.1 Construction of the conditional probability tables associated with the simple A, B, C, 
D net. 
she would also have to dream up joint probabilities for pretty complex joint events, such 
as the probability of the equity market crash not happening and the default of a major bank 
counterparty happening and the widening in credit spreads happening and the increase in 
implied volatilities not happening. 
I will show that, given the causal structure we have posited among the four variables 
(reﬂected by the topology of the associated Bayesian net), the task is not only simpler in 
terms of the number of quantities that have to be speciﬁed, but also cognitively feasible. 

158 
CHAPTER 11 OBTAINING A COHERENT SOLUTION II: BAYESIAN NETS 
The task is still far from trivial, but a normal human being, armed with decent data and 
a plausible model of reality, can at least hope to come up with something that she could 
justify and defend. From the mere book-keeping point of view, the risk manager will have 
to specify seven quantities instead of 15. 
It is useful to attack a tree starting from its last descendants. I will therefore start from 
node D (the increase in equity implied volatilities). First of all the risk manager will have 
to specify D’s stand-alone marginal probability of occurrence, P (D). In this case I have 
arbitrarily chosen this to be 0.015. In Figure 11.1 I have followed the convention that 
quantities assigned by the risk manager are in bold type, and all other (derived) quantities 
in normal (non-bold) type. 
My ﬁrst goal is to begin by producing a subset of the full set of joint probabilities, that 
is, the joint probabilities between event D and its parents – in our case, its only parent, 
event A (the equity market crash). So, I want to populate the following table: 
A
D
 Joint(A, D) 
J1 
1 
0 
p(1) 
J2 
0 
1 
p(2) 
(11.1) 
J3 
1 
1 
p(3) 
J4 
0 
0 
p(4) 
Let us start from the easiest step: we know that, for any event D, P (D)  + P (  ˜D) = 1, 
we have 
P (  ˜D) = 1 − P (D)  = 1 − 0.015 = 0.985 
(11.2) 
In Figure 11.1, this quantity has been labelled, with hopefully self-explanatory notation, 
MargNotA. As it is a derived quantity, it is not in bold type. 
The next quantity the risk manager is expected to supply is the conditional probability 
of D (the rise in implied volatilities) given A (the equity market crash). I have given this 
quantity, P (D|A), the value of 0.60. It is shown in Figure 11.1 as the (bold typeface) 
quantity CondDA. 
What about P (A|D)? We can easily derive this from the marginal probabilities, P (A)  
and P (D)  – that we have already – from the exogenously assigned conditional probability, 
P (D|A), and from Bayes’ theorem. Let us build the easy quantities ﬁrst: 
P (  ˜A) = 1 − P (A)  = 1 − 0.01 = 0.99 
(11.3) 
Then, from 
P (D|A)P (A) = P (A|D)P (D) 
(11.4) 
we have 
P (A)  
0.01 
P (A|D) = P (D|A) 
= 0.60 
= 0.4 
(11.5) 
P (D)  
0.015 
In Figure 11.1 this is the non-bold quantity CondAD. 

11.1 SOLUTION WITH MARGINAL AND n-CONDITIONED PROBABILITIES 
159 
A simple but important observation: once the two marginal probabilities P (A)  and P (D)  
have been assigned, from a mathematical point of view supplying P (A|D) is totally equiv-
alent to supplying P (D|A), because the four quantities are linked by Bayes’ theorem. 
Mathematical ease, however, is not the same as cognitive ease. Any risk manager will ﬁnd 
it far easier and more ‘natural’ to assign the probability of equity implied volatility increas-
ing given an equity market crash, as opposed to the probability of an equity market crash 
given that implied volatilities have risen. This is because the human mind works better in 
the causal than in the diagnostic mode (see also Chapter 12 on this point). For practical 
applications it is therefore essential to work in the causal direction as much as possible, and 
to let Bayes’ theorem do the rest of the work for us. This is particularly true if the marginal 
probabilities of the two events are very different. 
We are now going to use the relationship 
P (A|D) + P (A˜|D) = 1 
(11.6) 
which holds true for any A and D, to obtain 
P (A˜|D) = 1 − P (A|D) = 0.6 
(11.7) 
This is the (non-bold) quantity CondNotAD in Figure 11.1. 
We can also build P (D˜ |A) immediately as 
P (  ˜D|A) = 1 − P (D|A) = 0.4 
˜
What about P (A|D)? Bayes’ theorem helps us again: 
P (  ˜D|A)P (A) = P (A| ˜D)P ( ˜D) 
(11.8) 
which implies 
P (A| ˜D) = P (  ˜D|A) P (A)  
P (  ˜D) 
= 0.4 0.01 
0.985 ≃ 0.00406 
(11.9) 
Exercise 33 
Equation (11.9) states the following: given that implied volatilities have not 
increased sharply, ( ˜D), the probability that an equity market crash has occurred is very 
low indeed – less than half as large as the marginal (stand-alone) probability of the same 
crash. Are you happy with this derived conclusion? It seems reasonable to me. Does it seem 
reasonable to you? Why? If not, why not? 
˜
˜
˜
Finally we need P (A˜|D) which is easy: it comes from P (A|D) + P (A˜|D) = 1 and gives 
˜
˜
P (A˜|D) = 1 − P (A|D) ≃ 0.99594 
(11.10) 
This equation says that, given that implied volatilities have not risen, the fact that we have 
not had an equity market crash is a virtual certainty. Also, this conclusion passes my personal 
plausibility test. 

160 
CHAPTER 11 OBTAINING A COHERENT SOLUTION II: BAYESIAN NETS 
Now we have all the ingredients to build our ﬁrst sub-joint-probability matrix. Let us start 
from the ﬁrst entry of the joint probability table, P (A  = T , D  = F ), which for simplicity 
of notation we write as P (A,  ˜D): 
D) = P (A|D)P ( ˜
P (A,  ˜
˜
D) = 0.00406 ∗ 0.985 = 0.004 
(11.11) 
All the other joint probabilities can be obtained using the formal relationship 
P (A  = x, D = y) = P (A  = x|D = y)P (D = y) 
(11.12) 
for x, y = T , F . Note that we have arranged the events in the joint probability following the 
ordering induced by the topology of our Bayesian net. So we have written P (A  = x, D = y) 
rather than P (D  = x, A = y). See the discussion in Section 8.3. 
Thanks to this ordering we already have all the building blocks (i.e., the marginal prob-
A), P (D)  and P (  ˜
abilities P (A), P (  ˜
D) and the conditional probabilities P (A|D), P (D|A), 
P (A˜|D), P (D˜ |A), P (A|D), P (D|A), P (A˜|D), P (D˜ |A)) needed for ﬁlling in all the ele-
˜
˜
˜
˜
ments required for the joint probability table: 
A
D
 Joint(A, D) 
J1
1 
0 
p(1) = 0.004 
J2
0 
1 
p(2) = 0.009 
(11.13) 
J3
1 
1 
p(3) = 0.006 
J4
0 
0 
p(4) = 0.981 
Note that the probability of an equity market crash occurring and implied volatilities not 
rising (0.004) is much smaller than the stand-alone probability of an equity market crash 
(0.01). It is also smaller than the probability of both the equity market crash and the rise in 
implied volatility occurring (0.006). Sometimes, therefore, the probability of the occurrence 
of two very rare events can be higher than the probability of occurrence of just one of the 
two rare events. I am happy with this result – by which I mean that it makes intuitive sense 
to me. Are you happy as well? By the way, what should the conditional probability P (D|A) 
A
have been for this result not to apply? And what would have this implied about the factor xD? 
As a next step we can now move to building the joint probability matrix between variables 
A, D and C: 
A
D
 Joint(A, D) 
C 
Joint(A, D, C)  
J1 
1 
0 
p(1) = 0.004 1 
p(1) 
J2 
0 
1 
p(2) = 0.009 1 
p(2) 
J3 
0 
1 
p(3) = 0.006 1 
p(3) 
J4 
1 
1 
p(4) = 0.981 1 
p(4) 
(11.14) 
J5 
1 
0 
p(1) = 0.004 0 
p(5) 
J6 
0 
1 
p(2) = 0.009 0 
p(6) 
J7 
1 
1 
p(3) = 0.006 0 
p(7) 
J8 
0 
0 
p(4) = 0.981 0 
p(8) 

11.1 SOLUTION WITH MARGINAL AND n-CONDITIONED PROBABILITIES 
161 
The strategy should by now be clear. We are going to need joint probabilities such as, for 
instance, 
D ∩C) = P (C  ∩A ∩˜
p(1) = P (A  ∩˜
D) 
(11.15) 
where I have used the commutative property to obtain the last equality (see Section 8.5). 
We rewrite this as 
D) = P (C|A ∩˜
D) 
(11.16)
P (C  ∩A ∩˜
D)P (A ∩˜
The term P (A  ∩˜D) has already been obtained from the joint probability table between A 
and D. We just need the term P (C|A ∩˜D). But, given the causal links posited in our view 
of the world, and reﬂected in the topological structure of the Bayesian net we have drawn, 
we know that 
P (C|A ∩˜D) = P (C|A) 
(11.17) 
Note that we are not saying that C (the widening in credit spreads) and D (the increase 
in implied volatilities) are independent – they are not! What we are saying is that, once 
we know A (the occurrence or otherwise of the equity market crash, which is the common 
‘cause’ for C and D),1 knowledge of C would not help us in predicting D, and  vice versa. 
To proceed, the risk manager has to provide two more quantities, the marginal probability, 
P (C), of  C happening (MargC in Figure 11.1) and the conditional probability P (C|A). 
When these quantities are given, the next step is just a matter of applying Bayes’ theorem, 
and the closure relationships (again, see Section 8.5) 
P (X|Y) + P (  ˜X|Y) = 1 
(11.18) 
P (X|Y ∩Z) + P (  ˜X|Y ∩Z) = 1 
(11.19) 
in order to obtain all the remaining conditional probabilities we need, namely: P (C˜|A), 
˜
˜
P (A|C), P (A˜|C), P (C|A) and P (C˜|A). Note that, in order to do so, we will have to 
invoke again the conditional independence conditions 
P (C  = x|A = y ∩D = z) = P (C  = x|A = y) 
(11.20) 
for x, y = T , F . 
˜
˜
Exercise 34 
Derive P (C˜|A), P (A|C), P (A˜|C), P (C|A) and P (C˜|A). 
1Note carefully that we are not saying that A, the equity market crash, is the only cause for C, the widening 
in the credit spreads. In our simpliﬁed world, credit spreads can increase also because of the default of a major 
bank counterparty, event B. 

162 
CHAPTER 11 OBTAINING A COHERENT SOLUTION II: BAYESIAN NETS 
When we have all the conditional probabilities we need, we are one step away from 
building the joint probabilities between A, D and C. We simply have to use the relationships 
P (C  = x ∩ A = y ∩ D = z) = P (C  = x|A = y ∩ D = z)P (A = y ∩ D = z) 
(11.21) 
for x, y, z = T , F . Once we have done so, we can build the joint probability table shown 
in Figure 11.1, and labelled Joint(ADC). The numbers turn out to be as follows: 
A
D
 Joint(A, D) 
C 
Joint(A, D, C)  
J1 
1 
0 
p(1) = 0.004 1 
p(1) = 0.0012 
J2 
0 
1 
p(2) = 0.009 1 
p(2) = 0.0002 
J3 
1 
1 
p(3) = 0.006 1 
p(3) = 0.0018 
J4 
0 
0 
p(4) = 0.981 1 
p(4) = 0.0168 
(11.22) 
J5 
1 
0 
p(1) = 0.004 0 
p(5) = 0.0028 
J6 
0 
1 
p(2) = 0.009 0 
p(6) = 0.0088 
J7 
1 
1 
p(3) = 0.006 0 
p(7) = 0.0042 
J8 
0 
0 
p(4) = 0.981 0 
p(8) = 0.9642 
Apart from adding up to 1 (which they do) and being all positive (which they are), do these 
numbers make sense? First of all, by far the most likely joint event, J8, is that no stress  
event at all occurs. With a probability of 96.42% none of the stress events we have chosen 
will materialize. As we are dealing with rare events, this makes sense. 
Note, however, that despite the rarity of the underlying events, the joint event, J3, ‘all  
events happen at the same time’ (i.e., we have the failure of a major bank counterparty, 
and an equity market crash and an increase in implied volatilities) is not the most unlikely 
event, because it has a probability, p(3), of 0.18%, which is greater, for instance, than the 
probability of joint event J2 (there is an increase in implied volatilities and in spreads, but 
no equity market crash): p(2) = 0.02%. This is both good and bad. The fact that the joint 
event, J3, ‘all events happen at the same time’ does not have the lowest probability makes 
sense: we have implied, via our causal links and via the conditional probabilities we have 
assigned, either a reinforcing or a neutral inﬂuence of each variable on its descendants. In 
our example, when there is a dependence, the conditional probabilities, P (X|Y), are  always  
higher than the marginal probabilities, P (X). 
What we may not like as much is that the lowest probability joint event is J2, ‘we have 
an increase in implied volatilities and in credit spreads without an equity market crash’. 
What we have ‘hard-wired’ in our Bayesian net and in the probabilities we have assigned is 
the fact that having an increase in equity implied volatilities without an equity market crash 
is extremely unlikely. So, despite the fact that an equity market crash and a sharp increase 
in implied volatilities are rare events to start with, the occurrence of the crash without the 
increase in volatility is even rarer. 
If you do not ﬁnd this conclusion plausible, what would you have to change in the prob-
ability inputs to obtain an outcome more consonant with your intuition? This is an example 
of the analysis that should always be carried out when ﬁlling in conditional probability 
tables and examining the partial joint probabilities built along the way. 
Let us go back to the building of the probabilities. Completing the full joint probability 
table for the four variables is now easy. We juxtapose two copies of the joint probability 
table labelled Joint(A, D, C) in Figure 11.1; we insert an extra column that has the ﬁrst 

 
 
 
 
 
 
 
 
 
 
11.1 SOLUTION WITH MARGINAL AND n-CONDITIONED PROBABILITIES 
163 
eight elements corresponding to event B (the default of the bank counterparty) happening, 
and the remaining eight entries corresponding to P (  ˜B). 
Next we write 
P (A  = w, B = x, C = y, D = z) = P (B  = w, A = x, D = y, C = z) 
= P (B  = w, A = x, D = y, C = z) 
= P (B  = w|A = x, D = y, C = z)P (A = x, D = y, C = z) 
(11.23) 
for w, x, y, z = T , F . Then, given the structure of the Bayesian net we have built, we can 
write 
P (B  = w|A = x, D = y, C = z) = P (B  = w|C = z) 
(11.24) 
As a next step, the risk manager will have to assign the marginal probability of the default of 
a major bank counterparty (P (B)) and the conditional probability of credit spreads widening 
given the bank default, P (C|B). Note that, logically, the risk manager could equivalently 
have assigned P (B|C), but given the direction of the causal link, specifying one conditional 
probability is much more ‘natural’ than specifying the other. See the discussion in Section 
4.4 and also the distinction drawn in Section 12.4 between causal and diagnostic logical rela-
tionships, and the associated biases. Note also that we have assigned a very high conditional 
probability to a widening of credit spreads given the default of a major bank counterparty. 
Given our experience of the events that followed the default of Lehman, this is a reasonable 
assumption. The conditional probability of a bank counterparty given a widening of credit 
spreads should, however, be much smaller – as it is – because, we hope, there should be 
many more ‘causes’ for a widening in spread than for the default of a major counterparty. 
Given Bayes’ theorem, this can only come about if the marginal probability of B (the bank 
default) is much smaller than the marginal probability of credit spreads widening. This is 
reﬂected in the numbers I have chosen in Figure 11.1. See in this respect the discussion 
in Chapter 12 about representativeness and the neglect of the base-rate frequencies. 
The last step then comes from multiplying the joint probabilities for A, D, C by the 
conditional probabilities, P (B  = w|C = z). This produces the 16 joint probabilities shown 
in Figure 11.1. 
One may be tempted to say that we are done. Actually, the most interesting part of the 
exercise is only just beginning. We are now in a position, in fact, to interrogate the results 
and see whether they agree with our intuition. If they do not (barring any mistakes) the 
only causes for the discrepancy can be our faulty assignment of marginal probabilities our 
faulty assignment of conditional probabilities or our faulty intuition. For instance, it pays 
to compare these joint probabilities with the eight joint probabilities calculated for A, D 
and C. The probability that neither A nor D nor C happen, P A, ˜
˜ D, C˜ , is 0.96415. But 
˜ D, ˜
the probability that neither A nor D nor C nor B happen, P A, ˜ C, B˜ , is 0.96405. 
The two probabilities are almost identical. On the other hand, the probability of neither 
˜
˜ D, C˜
or
A nor D happening, P A, D˜ , is 0.9810, considerably different from P A, ˜
˜ D, ˜
P A, ˜ C, B˜ . Why is that the case? What in our probability assignments produces this 
result? Are we happy with what we have implicitly said about the way the world works? 

164 
CHAPTER 11 OBTAINING A COHERENT SOLUTION II: BAYESIAN NETS 
The power of Bayesian nets comes from the ability it affords to mix powerful intuitive 
pieces of information (e.g., statements that there should be many more ‘causes’ for a widen-
ing in spread than for the default in a major counterparty, or that given the bank’s default, a 
widening in credit spreads is almost a dead certainty) with logical book-keeping and ‘auto-
matic production’ of conditional and joint probabilities for combinations of events about 
which we may have little intuition. This double reason – enhancement of intuition and logi-
cal book-keeping – is what makes Bayesian nets, in my opinion, so suitable for stress testing. 
11.1.1 Generalizing the Results 
The approach followed to ‘solve for’ the joint probabilities in the case depicted in Figure 11.1 
gives a good ﬂavour of the general strategy to tackle these problems. For pedagogical reasons 
the example just tackled is particularly apposite because we had to deal at most with singly-
conditioned probabilities. This was despite the fact that we allowed for nets belonging to 
S2, i.e., with two parents. In general, these nets can give rise to the need to specify doubly-
conditioned probabilities. Consider, for instance, the Bayesian net in Figure 11.2. Let us 
look at the last descendant, event E, and assume that the risk manager has assigned the 
marginal probability P (E). What conditional probabilities will she have to assign? Let us 
try to follow the same approach as before. It is clear that now the lowest-level set of joint 
probabilities will involve three, not two, variables: D, C and E. After re-ordering, we will 
be left with an expressions of the following type: 
P (E  = x, C = y, D = z) = P (E  = x|C = y, D = z)P (C = y, D = z) 
(11.25) 
for x, y, z = T , F . We cannot invoke conditional independence to reduce the degree of 
conditioning any further. The last term in Equation (11.25) can be broken down, say, as 
P (C  = y, D = z) = P (C  = y|D = z)P (D = z) 
But there is no way to reduce the term P (E  = x|C = y, E = z) into some combination 
of singly-conditioned or marginal probabilities. Given what we have decided to say about 
the way the world works,2 the risk manager will therefore have to specify conditional 
probabilities such as, for instance, 
P (E|C, D) 
(11.26) 
A 
B 
D
C 
E 
Figure 11.2 An example of a net where each node has at most two parents. 
2Or, more precisely, I should say: given what we have been prepared to say about the simplest, yet reasonable, 
approximation about how the world works. 

11.2 AN ‘AUTOMATIC’ PRESCRIPTION TO BUILD JOINT PROBABILITIES 
165 
Then, as usual, we can use the closure relationship to write 
P (E˜|C, D) = 1 − P (E|C, D) 
(11.27) 
One can then recall the extension of Bayes’ theorem we discussed in Chapter 5: from 
P (A|B)P (B) = P (B|A) P (A) 
(11.28) 
we obtain 
P (A|B ∩ C)P (B ∩ C) = P (B|A ∩ C) P (A ∩ C) 
(11.29) 
Applied to Equation (11.26) this gives 
P (E|C, D)P (C, D) = P (C|E, D) P (E,  D)  
(11.30) 
and therefore, once the risk manager has speciﬁed P (E|C, D), she can write 
P (C, D) 
P (C|E, D) = P (E|C, D) 
(11.31)
P (E,  D)  
with P (C, D) and P (E,  D)  decomposable as 
P (C,  D)  = P (C|D)P (D) 
(11.32) 
P (E,  D)  = P (E|D)P (D) 
(11.33) 
The good news is that, given the limited topological complexity of S2 Bayesian nets (i.e., 
Bayesian nets with at most two parents), matters do not get any more complex. The risk man-
ager may need a good amount of patience, but will not be required to provide estimates or 
guesses more complex than the ones we have encountered in this section. Trebly-conditioned 
conditional probabilities, for instance, will never be required. 
There is more: as I showed in Chapter 7, even if the risk manager feels unable to 
provide doubly-conditioned probabilities, very useful bounds can still be obtained for the 
joint probabilities just by providing the marginals and (some of) the singly-conditioned 
probabilities. The analytical tool for obtaining this result is, again, Linear Programming. 
But, apart from the technical aspect, the tightness of the bounds on the joint probabilities 
provides invaluable help in the elicitation phase of the programme. 
11.2 An ‘Automatic’ Prescription to Build Joint 
Probabilities 
What we have done so far also gives a clear blueprint for ‘automating’ the procedure to build 
a set of joint probabilities. First we have to identify the dependence structure embedded in 

166 
CHAPTER 11 OBTAINING A COHERENT SOLUTION II: BAYESIAN NETS 
the Bayesian net we want to employ. Then we have to characterize its topological structure 
in a way that can be understood by a computer program. This can be accomplished via 
the g matrix. For a directed Bayesian network that links n Boolean two-valued random 
variables, this is an n × n matrix that contains 1s and 0s. The entries with 1s are those that 
correspond to causation links. Those with 0s signify that there is no link between the two 
variables. By convention, the diagonal is ﬁlled with 1s. So, for instance, for the Bayesian 
net in Figure 11.1, the g matrix has the following form: 
⎤
⎡ 
g = 
⎢⎢⎢⎢⎣ 
A
B
C
D
 
A 
1
0
0
0 
B 
0
1
0
0 
C 
1
1
1
0 
D 1
0
0
1 
⎥⎥⎥⎥⎦ 
(11.34) 
I have also chosen the convention that the causal link goes from row entries to column 
entries. So a 1 in the cell (row = C, column = A) means that C ‘causes’ A – or, more 
precisely, that there is a directed edge (arrow) from A to C.3 Note that, given the matrix g 
above, one could build the net in Figure 11.1: just draw on a piece of paper as many dots 
as variables; whenever you ﬁnd a 1 in the matrix, draw a directed arrow between the two 
variables, remembering the column/row convention. Done. 
Now our computer knows about the topological structure of the net we have chosen. If 
you want to be clever, you can note that, for S2 Bayesian nets, the matrix will be very 
sparse (i.e., it will be mainly populated by 0s). One could therefore store information in 
more efﬁcient ways, but for the size of nets we are going to deal with, size should never 
be a problem. 
Exercise 35 At most, how many non-zero elements will there be in an [n × n] g matrix 
obtained from a Bayesian net belonging to S2? 
Exercise 36 Build the g matrix for the net in Figure 11.2. 
The next step is to take as inputs the quantities that the risk manager must assign, that 
is, the marginal and conditional probabilities. The risk manager should always specify the 
marginal and conditional probabilities in the ‘order’ that makes more sense to her. So, for 
instance, typically P (X)  will be more natural to specify than P (  ˜X). And very often there is a 
natural direction of causality that makes either P (A|B) or P (B|A) more ‘intuitive’ to assign. 
For instance, specifying the conditional probability of equity implied volatilities increasing 
given an equity market crash can be easier than assigning the conditional probability of an 
equity market crash given an increase in implied volatilities. Of course, the two are linked 
via the respective marginal probabilities (and Bayes’ theorem). See the diagnostic/causal 
distinction drawn in Section 12.4. 
As a next step, one starts from the lowest descendants in the tree, and determines the joint 
probabilities for the associated variables, using the primitive and derived marginal and con-
ditional probabilities for the variables at hand, much as we have done in the examples above. 
3Obviously, if the network is non-directed, the g matrix is symmetrical. Having directed edges, which is 
necessary to speak of causation, breaks the symmetry. 

11.3 WHAT CAN WE DO WITH THIS INFORMATION? 
167 
Once the joint probabilities for all the lowest-level variables have been determined, one 
moves one step up the generations. The new joint probabilities (i.e., the joint probabilities 
at the next level up) will be determined using the lower-level joint probabilities and the 
marginal and conditional probabilities connecting the new and the old variables – making 
use, of course, of the central tool at our disposal to keep the problem manageable: conditional 
independence. 
A simple book-keeping device for doing so is to repeat twice the 2k lower-level joint 
probabilities determined at the previous level, and juxtapose next to them 2k 1s and 2k 0s, 
corresponding to the possible values of the indicator values of the new variable encountered 
at the higher level. This is exactly what we have done in Figure 11.1. Of course, if moving 
to the higher level brings in two more variables, then we will have to repeat four times 
the 2k lower-level joint probabilities, and associate them with the appropriate combinations 
of 1s and 0s for the indicator functions of the two new variables. Since we are limiting 
ourselves to S2,  this  is  as hard  as it  gets.  
Proceeding this way we can mechanically work our way all the way to the root(s) of the 
tree, and determine all the joint probabilities that we need. The precise equations that have 
to be coded are given in Section 8.7. 
I stress again that the difﬁculty is in the speciﬁcation of the inputs to the problem, 
not in the construction of the joint probabilities. Even if the risk manager may feel a bit 
daunted by the construction task the ﬁrst time around, the process is totally mechanical, and 
relatively ease to code, as long as one is not too concerned about numerical efﬁciency. As I 
recommend working with no more than about ten (well-chosen) stress events, storage, CPU 
time and computational efﬁciency are not of great concern. All the ‘thinking time’ should 
be devoted to assigning the marginal and conditional probabilities, in making critical use of 
the various sanity checks, and in scrutinizing carefully the intermediate conditional and joint 
probabilities. If something looks ‘strange’ in any of the intermediate or ﬁnal outputs, our 
estimates were faulty, our intuition about how the world was supposed to work was faulty, 
or both. This is what the risk manager should worry about, not the mechanical book-keeping 
to obtain the various joint and marginal probabilities. The next chapter therefore deals with 
providing the risk manager with all the help she can get in the providing good, sensible 
inputs to the problem. 
11.3 What Can We Do with this Information? 
I suggested at the end of the previous chapter the type of analysis that can be carried out 
using conditional probabilities only. As discussed, it was useful, but limited. 
We now assume that we have obtained the full joint distribution – as advertised, the 
probabilistic Holy Grail. What can we do with it? 
Assume that we have captured all the important stress events, which is clearly unrealistic 
but we can use this assumption as a starting point. Also, we do not address the issue of how 
the statistical information about stress losses can be aggregated with the (typically much 
smaller) losses captured by techniques such as VaR.4 The problem is not a small one, but 
4A good idea is to deﬁne the losses associated with each event in the stress-testing exercise as the extra loss 
over and above the largest VaR loss generated by the same risk factor. If the risk factor is not captured by VaR at 
all – in regulatory parlance, if it is a Risk Not In VaR the extra loss is the full stress loss. 
Easier said than done. 







168 
CHAPTER 11 OBTAINING A COHERENT SOLUTION II: BAYESIAN NETS 
we can either be conservative and simply add (as the regulators are likely to require), or be 
optimistic and aggregate, perhaps via the square-root-of-sum-of-squares rule. 
So, let us keep these caveats in mind, and see what we can do with the information we 
have obtained. 
First of all we can associate a loss to each of the joint events. Of course, to do so, we will 
need some mapping from event space to proﬁts or losses. This mapping can be as simple 
as multiplying a basis point move by a PV01,5 or it may be a full revaluation. The details 
are immaterial for the purpose of this discussion. 
Since we have the probability of each joint event, we can calculate the total expected 
loss, L. It is just given by 
2n 
 
L = 
L(Jj)p(j ) 
(11.35) 
j =1 
where n is the total number of stress events and the expression L(Ji) emphasizes that the 
ith loss is a function of the joint realization Ji of elementary events. 
Since we have the full distribution we can now calculate the variance, or any moment we 
may have conﬁdence in, of the loss distribution. Note that this variance has a different (and 
richer) meaning than the variance I deﬁned in the case of conditional expected losses. See 
the discussion in Section 10.6. It still does not address the question of variance of output 
losses coming from the uncertainty in the model parameters (e.g., subjective probabilities 
and the direction of causation). Again, an obvious adaptation of the heuristic technique 
discussed in Section 10.6 can be of help. 
Needless to say, we still have the conditional expected loss given that any of the ele-
mentary events have occurred. But, if we want, now we can also calculate the conditional 
expected loss given that any combination of elementary events has happened. 
We can also estimate the probability that none of our events have happened: we call this 
P (E1 ∩ E∩, . . .  , ∩En). Then  1  − P (E1, E2, . . .  En) is the probability that we will incur a 
loss at least as large as the smallest loss L(Ji). This loss is by deﬁnition our stress-loss-
percentile (stress VaR). We can then also integrate the tail losses beyond the VaR level to 
obtain a Conditional Expected Shortfall (CES) statistic. 
11.3.1 Risk-Adjusting Returns 
There is more than can be done. Very often investment decisions are made on the basis 
of the RAROC (risk-adjusted-return-on-capital) criterion,6 expressed as a ratio of the 
expected return from a given initiative over the associated capital.7 If the capital in 
question is the regulatory capital, then the deﬁnition is unambiguous. But if it is ‘economic’ 
5Present Value of a Basis Point. 
6There are deep corporate ﬁnance questions as to whether, and if so why, this rule should be employed. I 
elegantly sidestep these issues here, and pragmatically assume that a RAROC-type criterion is used in practice, 
whatever its theoretical attractiveness – or lack thereof. 
7The expected return is to be understood as net of expected losses, costs and taxes. Note in passing that, as 
Matten (2000) points out, given the deﬁnition just provided one should speak of return on risk-adjusted capital 
(RORAC), rather than risk-adjusted return on capital (RAROC), because it is the denominator (the capital) that 
is risk-adjusted, not the numerator (the expected return). The distinction has more-than-semantic relevance, but I 
shall not pursue this topic. 

 
 
 
 
 
 
11.3 WHAT CAN WE DO WITH THIS INFORMATION? 
169 
capital – understood as the capital a ﬁrm would prudentially hold even in the absence of 
any regulatory constraints – then its calculation becomes more problematic. To the extent 
that capital should be risk-based, it should be some monotonic function of risk. To capture 
the idea of risk in the quantity that goes in the denominator, it has been common to use 
the VaR statistic: 
Expected Return j
RAROCj ∝ 
(11.36)
VaRj 
where the index j labels the particular business line, or the investment initiative. A con-
ceptually more attractive alternative is 
Expected Return j
RAROCCES ∝ 
(11.37)
j 
CESj 
However, both these approaches neglect the fact that truly tail events – such as liquidity, 
and, in general, events not contained in the data historical base – are poorly captured by 
VaR or CES, if at all. An analysis purely based on return-over-VaR ‘efﬁciency’ would 
therefore favour lines of business with leptokurtic proﬁt-and-loss proﬁles – the fatter the 
tails for the same variance, the greater their apparent attractiveness. But plausible utility 
functions, and common sense, suggest that, for a given variance, we should prefer less 
fat-tailed distributions. 
There is a further practical consideration that suggests that using VaR8 in the denominator 
may not be such a good idea. When the trading-book capital used to be almost exclusively a 
function of the VaR, then, with some heroic distributional assumptions, this provided a sim-
ple alignment of ‘economic’ and regulatory capital for the trading book. After the ﬁnancial 
crisis of 2007–2009 we live in a different world, and regulators are increasingly requir-
ing that capital should also cover stress and other events not captured by VaR. Therefore 
including in the denominator a measure of stress risk not only makes the measure more 
intellectually appealing, but also better aligns this statistic with the likely capital charge. One 
can therefore complement the naive VaR measure above with a ‘Stress RAROC’ metric: 
Expected Return j
Stress RAROCj ∝ 
(11.38)
[Stress-related risk statistic]j 
where the term Stress-related risk statistic refers to a suitable proxy for stress losses asso-
ciated with business line j. 
Neither statistic (using VaR or stress as the denominator) tells the whole story, or contains 
‘the truth’. Should we be concerned more about ‘normal’ or ‘tail’ losses’? How unlikely 
must a loss become before we stop worrying about it? Ultimately, our preferences are 
primary, and utility functions should be calibrated to revealed preferences, not the other 
8By the way, when I say ‘VaR’, do I refer to the stand-alone or the marginal VaR? Conceptually, the marginal 
VaR seems to make more sense, but only to the extent that the business or investment mix coming from the other 
areas of activity is relatively stable, and ‘structural’. 
Furthermore, marginal calculations can be very noisy, especially if the new investment or business line is much 
smaller than the exisiting portfolio: the more so, the higher the percentile. 

170 
CHAPTER 11 OBTAINING A COHERENT SOLUTION II: BAYESIAN NETS 
way around. However, we cannot express preferences about risky prospects (‘lotteries’) 
unless cognitively resonant points of the risk/reward proﬁle are presented to us. So, if a 
given business line shows a more attractive ratio of expected return to ‘risk’ both when VaR 
and when expected stress losses are taken as a proxy for risk, then choosing between the 
two looks like a no-brainer. But if the order is reversed, our preferences should be carefully 
interrogated. 
Finally, what is the Stress-related risk statistic I refer to above? There is no unique 
answer: the expected stress loss, the stress VaR, the stress CES and so on can all be taken 
to be suitable proxies. In the wake of recent events, regulators are trying to make the 
capital charge more truly risk-sensitive. If this were achieved by the inclusion of a capital 
component arising from subjective stress losses, there would be great appeal in employing 
for internal decision-making whatever measure the regulators choose. If, however, the capital 
were linked to, say, the 99.975th percentile of a one-year loss distribution, then there would 
probably arise a chasm between the regulatory charge and what institutions would be really 
looking at. See Rebonato (2007) for more on this point. 

Part IV 
Making It Work In Practice 


Chapter 12 
Overcoming Our Cognitive 
Biases 
Conditional probabilities lie at the heart of the approach I have suggested in this book. The 
success or failure of what I propose therefore largely depends on how well we can provide 
good estimates of these quantities. This is not an easy task. Therefore, in this chapter I deal 
with some of the problems that stand in the way of coming up with reasonable conditional 
probabilities. These problems are of a cognitive nature, that is, they can be traced back to 
systematic errors human beings tend to make when dealing with probabilities in general, 
and with probabilities of remote events in particular. 
There is little question that these systematic biases are real, and that human beings are 
imperfect Bayesian machines. How imperfect, as we shall see in the next section, is still 
up for debate. But, looking on the bright side, there is little to suggest that these biases 
are so ‘hard-wired’ that they cannot be overcome. Indeed, the ﬁrst step to take in order to 
avoid falling prey of these cognitive shortcomings is being aware of their existence. This is 
therefore the ﬁrst goal of this chapter. 
Awareness of these biases is one thing, but we can hope for something more. Indeed, 
several studies suggest that we can learn to think better (in this context, that we can learn to 
‘think Bayesian’).1 I therefore discuss in some detail the origin of some of the most common 
biases (for instance, representativeness), with the hope that self-awareness combined with 
training can improve our ability to suggest reasonable conditional probabilities. 
So, what is the problem? In the last few decades psychologists and behavioural ﬁnance 
scientists have highlighted important and systematic deviations in the reasoning process dis-
played by most human beings with respect to the normative standard provided by Bayesian 
rationality. These ﬁndings have had, and are having, a profound effect on the theory and 
practice of ﬁnance and economics, and are currently the subject of lively debate. They go 
straight to the core of the neo-classical set-up, questioning as they do the status of cen-
tral theories such as the Efﬁcient Market Hypothesis and the Rational Expectation Theory 
in economics, and the Rational Agent Model in political science. Bounded rationality is 
1See, in particular, Nisbett (2009) and references therein, and Grifﬁths and Tenenbaum (2005). 
173 

174 
CHAPTER 12 OVERCOMING OUR COGNITIVE BIASES 
the term often used to describe these systematic deviations from Bayesian rationality. For 
reasons that I explain in the next section I ﬁnd the term potentially misleading, and I prefer 
to speak of cognitive biases. Understanding the difference between bounded rationality and 
cognitive deﬁciencies is important in order to appreciate what needs ﬁxing and what does 
not. I therefore turn to this distinction in the next section. 
12.1 Cognitive Shortcomings and Bounded Rationality 
In the neo-classical theory the view of the economic agent as a rational Bayesian actor 
constitutes both the normative and the descriptive standard. I have discussed in Chapter 
3 how inﬂexible the straight-jacket imposed by this view of rationality can be. All ortho-
doxies, however, create dissenters, and indeed two distinct schools began to emerge almost 
half a century ago just as the neo-classical consensus was ﬁrming up. The ﬁrst was the 
bounded rationality’ school (Simon 1956, 1957 and Selten 1991, 1998). The second was the 
‘irrationality’, ‘behavioural ﬁnance’ or ‘cognitive biases’ school pioneered by Kahneman, 
Tversky and others in the 1970s (see, e.g., Kahneman and Tversky (1972a, 1972b, 1973, 
1979a, 1979b)). The two schools have a lot in common and are often confused, but there 
are very important differences between the two, which are of relevance to our discussion.2 
Starting from the similarities: both schools point out that individuals do not employ the 
‘rational’ Bayesian cognitive processes posited (either literally or in an as-if sense) by the 
neo-classical school. The behavioural ﬁnance school points to the systematic ‘errors’ or 
‘biases’ (representativeness, anchoring, overconﬁdence, hot-hand fallacy, gambler’s fallacy, 
etc.) stemming from the failure to employ the Bayesian methods of belief update. These 
arguments are very well known, and there is no need to dwell upon them here at length.3 
The less-well-known bounded-rationality view also concedes that human beings do not 
analyse difﬁcult problems or reach decisions by optimizing some target function or by using 
Bayesian methods. However, it differs from the behavioural school in claiming that human 
beings often employ ‘fast and frugal’ heuristics (in the form of rough-and-ready general 
rules) that produce results often almost as good as if the full optimization approach had been 
employed . 
To understand how this comes about, I brieﬂy examine three areas of disagreement 
between the two schools that are of relevance for our discussion: the ﬁrst is the prevalence 
of these cognitive shortcomings; the second is the different emphasis on the social context; 
and the third refers to the adaptive value of the imperfect heuristics employed in the decision-
making process. 
2There is yet another interpretation of bounded rationality that developed into an opposite direction. This line 
of thought recognizes that gathering all the information required fully to analyse a problem is time consuming and 
entails a cost – to ﬁx ideas, think of gathering information about buying a new home. In this school of thought 
(Stigler, 1961), the gathering of extra information – with presumably declining marginal beneﬁt to the quality 
of the choice outcome – is stopped when the search costs exceed the beneﬁts. The problem now becomes one 
of optimization under constraint (a stopping problem). Although appealing, the approach moves in the opposite 
direction to that of the current bounded-rationality school, in that it implies an even more complex optimization, 
and possibly an inﬁnity of meta-optimizations that take into account incorporating in the cost-beneﬁt analysis the 
cost of gathering information, of analysing the cost of gathering information, of analysing the cost of the cost 
of gathering information, etc. If anything, ‘wholly unbounded rationality’ would seem a better description of this 
modus operandi. I will not pursue this line of inquiry in the discussion that follows. 
3See Section 12.6 for references on this topic. 

12.1 COGNITIVE SHORTCOMINGS AND BOUNDED RATIONALITY 
175 
12.1.1 How Pervasive are Cognitive Shortcomings? 
The ﬁrst point of disagreement between the two schools is the degree of pervasiveness of 
human cognitive deﬁciencies. Proponents of the bounded rationality school emphatically 
remind us that most of the time we do not make decisional mistakes, and point out 
that ‘myriad ways in which human observers act as optimal Bayesian actors’ (Knill and 
Pouget, 2004) have been observed. And, as Brighton and Gigerenzer (2008) remind us, 
‘[t]he strength of this viewpoint – the implied ability of the cognitive system to “act 
Bayesian” – rests on the [wide] range of settings in which this ﬁnding holds . . .’. In a 
similar vein, Gigerenzer and Hoffrage (1995) argue that base-rate neglect can be perfectly 
rational ‘when information is represented in terms of natural frequencies rather than 
probabilities’. So, as a partly tongue-in-cheek remark, we should perhaps ask ourselves 
whether we may not be victims of the representativeness heuristic ourselves when we 
(over)estimate the importance and pervasiveness of our cognitive failures in the face of a 
large body of evidence pointing to cognitive successes. 
12.1.2 The Social Context 
When it comes to judging the effectiveness of a way of reaching decisions, the bounded-
rationality approach places strong emphasis on the context (environment) where the choice 
is made.4 The metaphor often used is that of scissors, one blade of which is the cognitive 
limitations and the other the ‘structure of the environment’: 
. . . a great deal can be learned about rational decision making [. . .] by taking account of 
the fact that the environments to which it must adapt possess properties that allow [. . .] 
simpliﬁcations [of the problem at hand]. 
Simon (1956) 
As Gigerenzer and Selten (2002) point out . . . [s]tudying only one blade is not enough; 
it takes both for the scissors to cut . . . . So, spectacular cognitive failures due to the use 
of heuristics may only be such in a laboratory environment (often a ‘set-up’ by a clever 
experimenter to highlight and magnify the failure), but may be much smaller, or indeed 
disappear, in real-life situations. To see how this can happen in practice, take a look at the 
following example. 
A friend of yours, Alice, wants to buy a new TV set.5 She has dutifully read the consumer 
report in Best-Buy TV 6 and concluded that, given her budget and requirements, brand X 
is the best choice. Just before her purchase, however, she meets a friend, John, who, upon 
hearing about her choice, expresses strong reservations: ‘I have bought this brand myself, 
and in three months I have had nothing but problems: ﬁrst the remote control broke down; 
4For instance, anchoring – i.e., being inﬂuenced in one’s decision by an ‘irrelevant’ piece of pre-existing 
information – can have very different outcomes depending on whether the ‘irrelevant information’ is planted in 
the laboratory by an experimenter, or naturally occurs in the environment. In the latter case, it is much less likely 
to be truly irrelevant, and the adaptive value of being inﬂuenced by it may well be positive. See Gintis (2009), 
Chapter 3, for a related discussion. 
5This example has been adapted from Schwartz (2004). 
6I am not aware of the existence of any publication called Best-Buy TV . If any such publication exists, I am 
obviously not referring to it. 

176 
CHAPTER 12 OVERCOMING OUR COGNITIVE BIASES 
then the DVD connection stopped working; then it deleted the programmes I had recorded. 
And the colours are sooo ﬂat . . .’.  
Logically, this one extra piece of information should alter minimally, if at all, Alice’s 
resolve to buy brand X. After all, Best-Buy TV presumably conducted hundreds of tests 
under very different conditions, and the statistical weight of this body of evidence far 
outweighs the single, albeit ﬁrst-hand, opinion of Alice’s friend. In a laboratory setting, 
giving undue weights to John’s opinion is therefore a statistical mistake, and if Alice 
changes her choice as a result, her decision will be labelled by the cognitive psychologist 
as having being biased by one of the many decisional biases, say, salience. In the real 
world, however, is Alice’s decision not to buy brand X decision really so ‘wrong’ – or, 
perhaps, one should say, so maladaptive? 
Now, Best-Buy TV may well present itself as an impartial tester of TV brands but how 
do we know that it is so impartial after all? Does a board member of X TV perhaps also sit 
on the board of the publishing company that controls Best-Buy TV ? And does Best-Buy TV 
accept advertising of TV brands on its pages? If so, is brand X a major advertiser for Best-
Buy TV ? On the other hand, Alice has known John for ages as an honest, straight-talking 
friend, and he certainly does not work for TV makers Y . Unless Alice is prepared to do a lot 
of extra homework – e.g., to assess the actual impartiality of Best-Buy TV , or to ascertain 
whether John may now be working for TV makers Z – is her choice not to buy brand X 
after all so irrational? Is her distrust of a ‘professional’ rater necessarily so misplaced? 
Moving from TVs to matters of more direct ﬁnancial relevance, how many stocks were 
rated ‘strong buy’ by stock analysts just before the bursting of the tech bubble? How 
many AAA-rated CDOs defaulted within months of their stellar rating being granted? How 
‘irrational’ would it have been to disregard the rating-based information on the basis of some 
piece of evidence – admittedly anecdotal and statically-insigniﬁcant – provided by someone 
who looked in detail at the structure of one CDO of ABSs? 
12.1.3 Adaptiveness 
This brings me to the third difference between the bounded-rationality and the cognitive-bias 
(behavioural–ﬁnance) schools. This difference stems from the evolutionary and adaptive 
perspective that is embraced by the bounded-rationality approach: the heuristics employed 
in reaching decisions may well be fast and frugal, this school says, but they must also be 
reasonably good for their evolutionary development to have occurred. If this were not the 
case, we would, after all, be an evolutionary paradox. 
Bounded rationalists therefore place their emphasis on how close the outcomes produced 
by the fast and frugal heuristics actually are to the outcomes of the optimization process, 
not on how crass the mistakes are. So, the bounded-rationality school tends to see as 
an essential part in the architecture of the human mind [. . .] a collection of fast and frugal 
heuristics, each of which is well designed for solving a speciﬁc class of inference tasks. 
This adaptive toolbox contains strategies for estimation, comparison, and categorization, 
to name a few. 
Martignon (2002) 
When it comes to comparing the performance of the fast and frugal adaptive toolbox 
with the Bayesian normative standard, Martignon (2002) ﬁnds that it is ‘remarkably 

12.1 COGNITIVE SHORTCOMINGS AND BOUNDED RATIONALITY 
177 
accurate, often outperforming multiple regression, the normative benchmark in the class 
of linear models. Even compared with Bayesian models, the differences in outcomes are 
seldom large’. 
In a similar vein, Grifftiths and Tenenbaum (2005) report related results pointing out that 
predictions of non-experts can come remarkably close to the Bayesian normative standard in 
a surprisingly wide range of forecasting settings. Their ﬁndings are of particular relevance 
to our discussion because they focus on the implicit estimation of conditional probabilities.7 
It is also very relevant for us that, when failures in their experiments were observed, these 
could be explained by imperfect factual knowledge, rather than deﬁciency in reasoning. 
Grifﬁths and Tenenbaum (2005) show that once better factual information was provided, 
a better conditional estimate was obtained . This provides a ﬁrst, simple suggestion to the 
risk manager faced with the task of assigning a difﬁcult conditional probability: do your 
homework well. Learn, that is, the ‘facts’ about the problem at hand: question the causal 
links, and, yes, do look very carefully at the frequentist information at hand. 
This point is reinforced by Jolls and Sunstein (2005), who point out that, given some 
legally relevant facts and evidence, subjects were ﬁrst observed systematically to overesti-
mate the probability of success in a trial. The reason for these overestimates can be readily 
traced to some of the cognitive biases (such as overconﬁdence) highlighted in the behavioural 
ﬁnance literature. The important observation, however, is that these estimates were substan-
tially corrected by a number of ‘cognitive exercises’, for example, by suggesting that they 
take the other side’s view into account.8 So, here is another way to improve our probabilistic 
assessments: look at the problem from different angles (the different interpretative models 
of reality I discuss in Chapter 3). 
The discussion above is important in the context of expert-judgement-based stress testing. 
It has two distinct implications: the ﬁrst is that, pervasive as these cognitive deﬁciencies 
may be, it is to some extent possible to overcome them – for example, by providing better 
information or being aware of our tendency to fall into systematic mistakes. The second is 
that, as the bounded-rationality school points out, we may not be perfect Bayesian machines, 
but the probabilistic tasks before us are not beyond our cognitive reach: the ‘fast and frugal 
7Grifftiths and Tenenbaum (2005) ask their subjects to answer questions such as: a friend is reading to you 
a line from her favourite poem, and she tells you that it was line 5 of that poem. What would your guess be of 
the total length of the (unknown) poem? Or: if you met an 18-year-old man, what would your prediction be for 
his residual lifespan? Or, again, you walk into a kitchen and observe from the timer above the oven that a cake 
has been cooking for 35 minutes. What would you guess for the total baking time? Finally, suppose that someone 
told you that a Pharaoh who reigned in Egypt around 4000 BCE had been ruling for 11 years. What would you 
predict about the duration of his reign? 
All these questions have a similar logical structure: they assume that you have some prior knowledge about 
a given topic: the typical length of reign of a Pharaoh, the life expectancy of a man, the typical baking time of 
a cake, etc. This, in Bayesian terms, is the prior. Then some evidence is provided about the speciﬁc problem at 
hand: e.g. the age (18) of the man, the time the cake has already been in the oven (40 minutes), etc. This is the 
‘evidence’. The subject is then required to revise the prior information on the basis of the evidence. To do so she 
will (implicitly) make use of the conditional probability of each possible answer given the evidence provided. 
The high degree of success in answering this set of questions indicates not only that we can handle difﬁcult 
probabilistic tasks, but also that we can be surprisingly good at estimating conditional probabilities . 
8More precisely, Jolls and Sunstein (2005) refer to the work by Babcock et al. (1995) ‘on the tendency of 
litigants to evaluate likely outcomes, as well as questions of fairness, in ways that systematically serve their own 
interests. [. . .] Babcock, Loewenstein and Issacharoff (1997) ﬁnd, however, that in an experimental setting, this 
self-serving bias may be eradicated by requiring litigants to consider the weaknesses in their case or reasons that 
the judge might rule against them. In these circumstances, individuals in the plaintiff’s and defendant’s roles have 
similar views on likely trial outcomes and fair settlements.’ 

178 
CHAPTER 12 OVERCOMING OUR COGNITIVE BIASES 
heuristics’ that we employ in solving these problems are very often not so bad after all, 
even when a full analytical solution of the problem may not be possible or may be beyond 
our abilities. 
If the best cure against these biases is awareness of their existence and of their exact 
nature, it is useful to look at them in some more detail. For this reason, I look at represen-
tativeness in the next section. 
12.2 Representativeness 
The simplest way to understand what the representativeness bias entails is through an 
example, which goes as follows. 
Here is the description of a ﬁctitious student, Tom W, taken from Shefrin (2008) who 
quotes and discusses original work from Kahneman and Tversky (1973):9 
Tom W. is of high intelligence, although lacking in true creativity. He has a need for order 
and clarity, and for neat and tidy systems in which every detail ﬁnds its appropriate place. 
His writing is rather dull and mechanic, occasionally enlivened by somewhat corny puns 
and by ﬂashes of imagination of the sci-ﬁ type. He has a strong drive for competence. 
He seems to have little feeling and little sympathy for other people and does not enjoy 
interacting with others. Self-centred, he nonetheless has a deep moral sense. 
Consider now the following nine ﬁelds of academic specialization: 
• business administration; 
• computer science; 
• engineering; 
• humanities and education; 
• law; 
• library science; 
• medicine; 
• physical and life sciences; 
• social science and social work. 
As a ﬁrst step some subjects10 were asked to give their estimates of the relative frequency 
of students in these nine areas of specialization. This means that they were asked to give 
their estimates of the probability that a randomly chosen student belonged to any of the 
nine ﬁelds. These are the subjects’ prior beliefs. We shall denote them by P (F), where  F 
stands for ﬁeld (of specialization). 
9Shefrin (2008), Chapter 2, page 19. 
10The original study had a between-subject design. See Kahneman and Tversky (1973) for details – the exact 
experimental set-up matters little for our discussion. In my presentation, I have simpliﬁed the experiment in several 
respects. 

12.2 REPRESENTATIVENESS 
179 
Other subjects where then asked to state their opinion of how likely it was that the 
description (D) given above related to students in each of the nine ﬁelds. In Bayesian 
terms, this is just P (D|F) – the probability of the description being relevant, given the ﬁeld. 
So, when F = Engineering, P (D|F) gives the probability ‘that an engineering student 
shares the features in Tom’s description’.11 It provides, in other words, ‘a measure of how 
representative Tom’s description is of an engineering student’.12 As for P (D), of course, 
it gives the probability that a randomly chosen student should share Tom W’s character 
description. The quantity P(F) then gives the ratio of engineering students who share Tom 
P(D) 
W’s description. What can we say about P (F|D), that is, about the probability that, given 
the description above, Tom is an engineering student? Before reading further, how would 
you answer this question? 
Now, from Bayes’ theorem we know that 
P (F)  
P (F|D) = P (D|F)
(12.1)
P (D)  
The subjects in Kahneman and Tversky’s experiments estimated correctly that engineering 
students are ‘rare’ compared to other students. But they overestimated the probability that a 
student ﬁtting the description above would be an engineering student. Why? Because they 
did not take into account in their estimate how rare engineering students are in the ﬁrst 
place, and how this should be reﬂected in the ratio P(F) .13 Even if every single engineering 
P(D) 
student ﬁtted Tom W’s description (i.e., even if the term P (D|F) were exactly equal to 
1) – which is obviously an unrealistic generalization – the probability that a student ﬁtting 
the description will be an engineering student (P (F|D)) would still be low because the 
fraction of engineering students (P (F)) is so low to start with. The reader may want to 
revisit the example about the deadly disease Wrong-Bayesitis discussed in Section 5.8.2. 
Also in that case it was representativeness that was likely to lead us astray. 
The conclusion we can draw from this example is therefore the following. When one 
event is of much lower probability than the other, we often fail to make full use of 
this information, that is, we ignore the base-rate differences in probabilities between the 
two events. For this reason the representativeness bias is often also referred to as the 
base-rate bias. 
Let us see how representativeness can lead us to provide a faulty assessment of a con-
ditional probability in a ﬁnancial stress-testing context. Suppose that we have to provide 
the probability, P (BF|SW), of the failure of a major bank counterparty, BF, given a very 
large and sudden widening in credit spreads, SW. From Bayes’ theorem we know that 
P (BF)  
P (BF|SW) = P (SW|BF)
(12.2)
P (SW)  
Once again, we can safely assume that, if a major bank fails, credit spreads will widen 
suddenly and massively – for the purpose of this argument we could even assume that 
11Shefrin (2008), page 20. 
12Shefrin (2008), page 21. 
13It is worthwhile pointing out that the term P (D)  (how likely it is that a random student will ﬁt Tom’s 
description) does not change across different ﬁelds, and therefore, even if wrongly estimated, it would not affect 
the ratios of the across-ﬁeld estimates. 

180 
CHAPTER 12 OVERCOMING OUR COGNITIVE BIASES 
the widening will occur with certainty: P (SW|BF) = 1. But the probability of a Lehman-
style bank failure should (today, at least) be very, very low – much lower than a large and 
sudden widening in credit spreads. Therefore the ratio P(BF) should also be very, very low, 
P(SW) 
reﬂecting the fact that there are many more causes for a large and sudden widening in credit 
spreads than the failure of a major bank counterparty. As a consequence, the conditional 
probability P (BF|SW) should be not too different from P(BF) :
P(SW)
P (BF)  
P (BF|SW) ≃ 
(12.3)
P (SW)  
So, the ﬁrst message from this discussion is that, in order to provide good estimates of 
conditional probabilities, it is very useful to have an idea of the relative probabilities of the 
stand-alone events. We do not need the precise probabilities: just an understanding of which 
event is more likely is already of great help. 
The second message is that we should be particularly careful when we deal with events 
of very different probabilities. The ‘trick’ at the root of the Wrong-Bayesitis paradox (see 
Section 5.8.2) lies exactly in the large difference in probability of the two ‘rare’ events, 
contracting the deadly illness and receiving a wrong positive test result. It is for this reason 
that assigning conditional probabilities by means of the multiplicative factors introduced in 
Sections 5.9 and 5.10 can be so useful. In my experience, when risk managers are asked 
k
to ﬁll conditional probability matrices using the factors xi , the resulting matrix tends to be 
much closer to being a coherent matrix14 than if the conditional probabilities are directly 
assigned. 
Lastly, the ﬁnancial example of the default of a major bank and of the widening of 
credit spreads suggested that when we expect a causation-like link (and I am using the 
word ‘causation’ with circumspection), a good starting point for the conditional probability 
is the ratio of the two marginals, as shown in Equation (12.3). But we can do better than 
that. Causal links, as I have stressed throughout this book,15 are extremely important and 
cognitively helpful. Consider Bayes’ theorem again: 
P (S)P (S|C) = P (C)P (C|S) 
(12.4) 
Give to the variable S the meaning ‘the credit spreads of CMBSs have widened’ and to C 
the meaning ‘China has defaulted’ as we did in Section 5.9. Now, would you rather assign 
P (S|C) (the probability of CMBS spreads widening given that China has defaulted) or 
P (C|S) (the probability that China has defaulted given that CMBS spreads have widened)? 
If the two marginal probabilities are given, from a mathematical point of view either task is 
just as simple (or difﬁcult) as the other.16 Not so from a cognitive point of view. Most people 
ﬁnd that assigning P (S|C) is much easier than estimating P (C|S). This is because we can 
see a clear causal link between the default of China, the mayhem that would ensue, and the 
14By coherent, I mean a conditional probability matrix that can be derived from a set of bona ﬁde joint 
probabilities. 
15See Sections 4.3 and 4.4 and Pearl (2009), Chapter 1. 
16Also if we had frequentist information about the two events (which is obviously not the case with China 
defaulting), assessing from the data one conditional probability is just as easy as giving the other, but the fact would 
not change that we can ‘work’ with one piece of information more easily than with the other. Again, association 
is less cognitively powerful than causation. 

 
 
 
 
" 
# 
" 
# 
12.3 QUANTIFICATION OF THE REPRESENTATIVENESS BIAS 
181 
widening of credit spreads. But we do not perceive (correctly, I believe) the widening of 
CMBS credit spreads as causing the default of China. At most, their widening may be an 
indication (a diagnosis) that China may have defaulted. For this reason, assigning P (C|S) 
requires us to ‘work against our cognitive grain’. I will touch on this point again later in this 
chapter when I deal with the causal and diagnostic interpretation of a conditional probability. 
But for the moment here is another suggestion to avoid cognitive errors: always assign 
conditional probabilities in the direction that you ﬁnd more cognitively ‘natural’ – which is 
almost invariably the causal direction – and let Bayes’ theorem do the rest of the work.17 
12.3 Quantiﬁcation of the Representativeness Bias 
Behavioural ﬁnance is often accused of providing so many and so different explanations 
for human cognitive differences that virtually anything can be ex post explained, but very 
little ex ante predicted. In the case of representativeness, however, Bayes’ theorem gives 
us a way to identify and quantify systematic deviations from Bayesian rationality due to 
representativeness. Consider once again 
P (A)  
P (A|B) = P (B|A) 
(12.5)
P (B)  
and take logs of both sides: 
P (A)  
ln[P (A|B)] = ln[P (B|A)] + ln 
(12.6)
P (B)  
Trivially this can be rewritten 
P (A)  
ln[P (A|B)] = α0 ln[P (B|A)] + α1 ln 
(12.7)
P (B)  
with α0 = 1 and  α1 = 1. Equation (12.7) gives the weights that a rational (Bayesian) 
P(A) 
agent should attribute to her log-prior18 (ln P(B) ) and to the log-likelihood function 
(ln[P (B|A)]) in forming an opinion about the posterior. However, from experiments like 
the one reported above one can test whether either empirical coefﬁcient turns out to be 
signiﬁcantly different from 1. The result of this regression analysis lends itself to a ready 
interpretation. If a population of subjects gave too little importance to the base-line frequen-
P(A) 
cies (ln P(B) – representativeness again) this would show up in a coefﬁcient α1 smaller 
than 1. If, instead, the subjects gave too much importance to the likelihood function term 
(ln[P (B|A)]), the coefﬁcient α0 would end up being signiﬁcantly larger than 1. The test, 
17As I will discuss, we ﬁnd assigning causally-interpreted conditional probabilities perhaps too easy. This can 
give rise to another bias. See Section 12.4. 
18The log of the prior is actually given by ln[P (A)], but the term − ln[P (B)] simply provides a constant offset 
that does not affect the conclusion. Indeed, Bayes’ theorem is often written as 
P (A|B) ∝ P (B|A)P (A) 

182 
CHAPTER 12 OVERCOMING OUR COGNITIVE BIASES 
therefore, does not only tell us whether the estimates are Bayesian or not, but also what the 
root cause of the deviation is. 
Although the actual experimental results have been more complex than sketched above 
(see Grether (1980)), the hypothesis of α0 = α1 = 1 has been rejected in many experiments. 
Representativeness in the sense explained above tends to be the most convincing explanation 
for the deviation from the Bayesian prescription. 
12.4 Causal/Diagnostic and Positive/Negative Biases 
Representativeness is not the only bias of relevance when subjectively estimating conditional 
probabilities. Moskowitz and Sarin (1983) look at the problem of elicitation of conditional 
probabilities in situations when ‘the conditional probability of an event A given that some 
other event B is known to have occurred, P (A| B), can only be assessed subjectively by an 
“expert”’ (Moskowitz and Sarin, 1983, page 735). Their approach is of relevance to our study 
because they make clear that in their work conditional probability is a basic measurement 
that cannot be computed and has to be elicited from the subject’. These authors go on 
to claim that the systematic errors made even by experts in their subjective estimation of 
conditional probabilities ‘result from systematic perceptual and cognitive biases in experts’ 
responses. Moreover, even statistically mature experts are highly susceptible to these errors’ 
(Moskowitz and Sarin, 1983, page 736). 
What are these systematic errors due to? As we have seen, representativeness is an 
important, but not the only, cause for systematic biases. We can get an understanding of 
these additional biases by looking at a conditional probability P (A| B) as a revision of 
the marginal probability P (A)  given knowledge that B has occurred. This is just the idea 
behind the factors xB we introduced in Section 5.9. We know that conditional probabil-
A 
ities convey no information about causality or temporal sequence (see the discussion in 
Section 5.5). However, given our ‘models of how the world works’ we often tend to organ-
ize in our minds questions about conditional probabilities in terms of causation. There is 
nothing wrong with this: conditional probabilities do not pin down causation, but causation 
certainly affects conditional probabilities. Now, causes can work in two directions (a feature 
that, as we have seen, Bayesian networks exploit): when asked to estimate the conditional 
probability P (A| B) we tend to think (perhaps unwarrantedly) that either B caused A or that 
A caused B. Using obvious terminology, with respect to the conditional probability P (A| B) 
the ﬁrst case is referred to as causal and the second as diagnostic. Tversky and Kahneman 
(1979) ﬁnd that, if the subject’s ‘picture of the world’ is such that she believes that B causes 
A, (B ===⇒ A), then the elicited P (A| B) turns out to be too high (in the testable sense 
discussed in Section 12.3) to be consistent with Bayes’ theorem. The opposite is true if the 
relationship between A and B is perceived to be of the diagnostic type (A ===⇒ B): ‘A 
causal relation leads to a more substantial revision in conditional probability.’ Furthermore 
‘[i]ndividuals [. . .] ﬁnd it easier to assess P (A| B) if B is causal and have more conﬁdence 
in their assessments’.19 
In a similar vein, Tversky and Kahneman (1982) observe that ‘we expect an asymmetry 
in inference regarding two variables whenever the ﬁrst appears to explain the second better 
than the second explains the ﬁrst’. Pursuing their argument, suppose that we deﬁne as 
19Tversky and Kahneman (1979), pages 740–741, quoting results in Kahneman and Tversky (1973). 

12.4 CAUSAL/DIAGNOSTIC AND POSITIVE/NEGATIVE BIASES 
183 
‘tall’ and ‘fat’ those individuals in a population who are above the 90th percentile of the 
weight and height distribution, respectively. Now, if asked to venture a guess, would you 
say that the probability of a person being fat given that we know she is tall is greater or 
smaller than the probability of a person being tall given that we know she is fat? Pause 
to think for a second. Now, if like most people who are asked the question you answered 
P (f  at| tall) > P (tall| f at), you have been a victim of the diagnostic/causal bias. Indeed, 
as the base-rate frequencies of tall and fat people in the population are by construction the 
same in this example, it must be that P (f  at| tall) = P (tall| f at).20 The ‘mistake’ is due 
to the fact that we tend to think of being tall as a possible cause for being fat, but we do 
not think that being fat ‘causes’ a person to be tall. 
To see the relevance of the diagnostic/causal bias, the reader should consider carefully 
again the example about China’s default and the widening in CMBS spreads presented in 
Section 5.9. (I mean it!) As we think that the default of China ‘causes’ the widening of 
the spreads, we are likely to overestimate the conditional probability P (CMBS| China) 
(when we reason in the causal direction) and underestimate the conditional probability 
P (China| CMBS) (when we think in the diagnostic direction). That is why the symmetry 
China 
constraint x 
= xCMBS is of such help in overcoming the causal/diagnostic bias. 
CMBS 
China 
The representativeness and the causal/diagnostic biases are far from being the only pitfalls 
in making assessments of conditional probabilities. For instance, an overestimation of the 
conditional probability is also often observed when the relationship between the variables is 
B
perceived to be positive (increase in the marginal probability, xA > 1) rather than negative 
B
(increase in the marginal probability, xA < 1). So, for positive relationships the revision of 
the marginal probability tends to be too large and too small for negative relationships. Also 
in this case, awareness of the problem is the ﬁrst remedy to ﬁx our cognitive shortcomings. 
A moment’s thought suggests a link between the causal and the representativeness bias. 
Indeed, when we assume a strong causal link between A and B (in the sense that B ===⇒ A), 
the quantity P (A| B) will tend to 1 (see also the discussion in Sections 5.3 and 12.2). But 
then in the expression 
P (B)  
P (B)  
P (B| A) = P (A| B)
≃ 
(12.8)
P (A)  
P (A) 
what we are left with is only the ratio P(B) 
P(A) . This is just the situation where neglect of the 
baseline probabilities (P (A)  and P (B)) can have a serious effect on our estimate of the 
conditional probability P (B| A), the  more  so  when  P (A)  and P (B)  are very different – think 
of Wrong-Bayesitis again. 
Finally, in the actual practice of risk management in general and of stress testing in par-
ticular, anchoring21 is another cognitive bias that can stand in the way of a good assessment 
20If this is not obvious to you, I am afraid you really should read Sections 5.6 to 5.9 again. 
21Anchoring is described in the cognitive psychology literature as the tendency to be inﬂuenced by arbitrary 
‘reference points’ that bear no relevance to the problem at hand. 
As an example of anchoring, consider the following. In many countries, there are laws that require that any 
credit card statement should specify a minimum repayment amount. The idea behind the law is reasonable enough: 
it attempts to stop borrowers from just forgetting about what they owe, and accumulating amounts of debt that they 
will not be able to service. However, the same laws are not as prescriptive about how large the actual minimum 
repayment amount should be – and, as we have all certainly noticed, the repayment amounts suggested by the 
credit card companies are always a very small fraction of the debt incurred during the month. 
This is no accident, as a study by Professor Neil Stewart, a psychologist at Nottingham University, suggests 
(Stewart, 2008). His intuition was that the suggested amount would turn out to be yet another of the many examples 

184 
CHAPTER 12 OVERCOMING OUR COGNITIVE BIASES 
of conditional probabilities. Anchoring works in this case when ‘last week’s estimate’ of a 
conditional probability ends up inﬂuencing today’s estimate, even if the market conditions 
may have changed, or new information has arrived in the meantime. In other words, the 
stale estimate can affect the current estimate just because ‘it is there’.22 Therefore, even 
if the availability of the latest estimates for the conditional probability matrices certainly 
makes the updating task less daunting and time-consuming, it is a good idea to restart ‘with 
a clean slate’ every time the construction of the matrix is undertaken. 
As we have seen, the potential pitfalls are many. Luckily, Moskowitz and Sarin (1983) 
point out that, whatever the original causes of these cognitive biases, even if little or no 
instructions are given to experimental subjects, the availability and careful examination 
of the joint probabilities can signiﬁcantly improve their initial estimates of conditional 
probabilities. This is particularly helpful for our applications, because the joint probability 
table is just the ﬁnal output of the Bayesian-network approach recommended above. The 
reader is therefore invited to revise, once this output has been obtained, the original estimates 
of the conditional probabilities in the light of the joint probabilities. A simple example of 
this type of analysis was provided in Chapter 11. 
12.5 Conclusions 
The conclusions of this chapter are the following. 
First, human beings are certainly not perfect Bayesian computing machines. In particular 
we often seem to make complex probabilistic assessment by means of ‘fast and frugal 
heuristics’ (in plain English, rules of thumb). However, these fast and frugal heuristics are 
frequently more effective than it would be reasonable to expect. Furthermore, lack of correct 
factual evidence, rather than an innate inability to ‘think Bayesian’, is often at the root of the 
probabilistic mistakes (see, in particular, the Pharaohs’ example in Grifﬁths and Tenenbaum 
(2005)). 
Second, the fact remains that there are still signiﬁcant and systematic cognitive biases. 
There is no sure-proof way to overcome these biases, but some measures do help, such as 
the following: 
• retaining an awareness of the nature of the bias; 
• examining carefully all the information about the problem at hand, including, of course, 
the frequentist information; 
of ‘irrational anchoring’: i.e., a spurious and logically irrelevant reference point that really has nothing to do with 
the choice at hand, yet tends to inﬂuence it in a predictable way. 
To substantiate his hunch, Professor Stewart presented 413 subjects with the credit card bills for the same total 
amount of £435.67. The statements were identical in all respects apart from the fact that half speciﬁed a minimum 
repayment amount of £5.42 (little more than 1% of the outstanding amount owed), while the other half did not 
specify any amount at all. 
Now, per se, the minimum repayment amount should have no inﬂuence on the decision of a rational borrower 
as to whether a larger amount should be repaid – and, if so, how much larger. Yet, Professor Stewart showed that, 
among the borrowers who did not pay their bill in full, those who had been presented with a repayment amount 
wanted to pay 43% less than those who had been shown any amount at all. As The Economist points out, ‘. . . in 
the real world, this would roughly double interest charges’. Not bad for a few lines’ work. 
22I am grateful to Dr Patrick deFontnouvelle for pointing this out during the course of a presentation I gave at 
the Federal Reserve Board of Boston in November 2009. 

12.6 SUGGESTIONS FOR FURTHER READING 
185 
• cross-checking of the conditional probabilities against the joint probability matrix; 
• paying explicit attention to the baseline frequencies; 
• working in the causal rather than diagnostic ‘direction’; 
k
• assigning the multiplicative factors xi rather than the conditional probabilities directly. 
These have all been shown to be useful in improving the ‘undoctored’ estimates. 
Third, as studies in many areas of developmental and cognitive psychology show, what 
we can and cannot do – what we are good and bad at – is not always an intrinsic datum, 
but strongly depends on whether we believe, or are made to believe, that we are good 
or bad at these tasks. As the work by Professor Nisbet (2009) shows, this is true even for 
mathematical abilities. There is a danger, therefore, that dwelling too much on our cognitive 
deﬁciencies may become a self-fulﬁlling prophecy. We must always remember, after all, 
that if we were fundamentally unable to produce conditional estimates, our very existence 
would be an extraordinary evolutionary paradox. 
12.6 Suggestions for Further Reading 
The distinction between bounded rationality and the cognitive biases of the behavioural 
ﬁnance school is well discussed in Gigerenzer and Selten (20002) and references therein. 
In particular, an inﬂuential analysis of bounded rationality in terms of information costs 
dates back to Stigler (1961). Note, however, that in Stigler’s analysis the economic agent 
not only remains unaffected by cognitive deﬁciencies, but also optimizes in her search for 
a preferred outcome (i.e., she does not use the fast and frugal heuristics posited by the 
bounded rationality school). The only difference with Homo Economicus is that Stigler’s 
agent faces information costs, which (rationally) induce her to abandon the optimization 
process at a given point. 
The literature on ‘true’ cognitive biases is enormous and has been accumulating for 
several decades. Much as it is still regarded as ‘alternative’, it must be remembered that 
Kahneman was awarded a Nobel Prize for his and Tversky’s work. As a starting point, a 
good reference book is Kahneman, Slovic and Tversky (1982). A lighter and more engaging 
(but less precise) treatment can be found in Shefrin (2000). Kahneman and Tversky (2000) 
provide an excellent collection of perspectives on cognitive biases with a focus on decision-
making and economic value. Chater and Oaksford (2008), and the contribution by Brighton 
and Gigerenzer it contains, provide a good Bayesian perspective of cognitive mechanisms. 
Several papers quoted in Shefrin (2008) deal with the original studies on representative-
ness. They all make for very interesting reading. Grether (1980) deals with the quantiﬁcation 
of the representativeness bias. All these papers deserve careful study to appreciate the nature 
of the problem. Sheﬁrn (2008) also discusses at length heterogeneity of opinions among 
economic agents. 


Chapter 13 
Selecting and Combining 
Stress Scenarios 
13.1 Bottom Up or Top Down? 
Using the cleverest tricks to establish the marginal, conditional and even joint probabilities 
among events will do us very little good unless we have picked our stress scenarios in an 
intelligent way. There are two main schools of thought on how to do this: the bottom-up 
and the top-down approach. 
The names are self explanatory: with the top-down approach, one selects broad ‘macro’ 
events of relevance to the economy (e.g., increase in unemployment), to the global geo-
political landscape (e.g., tension between China and Taiwan), or of more general character 
(e.g., outbreak of a major epidemic). The links between these events and the positions on 
the books of the ﬁnancial institution are then worked out, typically via positing interme-
diate sub-events and using some model from events to losses: for instance, an increase in 
unemployment may cause a rise in mortgage delinquencies, which will reduce the value of 
mortgage-related assets via a pricing model. As a last step in the process, the associated 
losses (or, depending on the positions, possibly gains) are estimated. 
The bottom-up approach starts instead from what I call the vulnerabilities of the portfolio, 
that is, from a detailed knowledge of ‘where it hurts’. These vulnerabilities will generally 
arise either from concentrations (e.g., to names or sectors in the banking book), from large 
relative-value positions (long and short similar positions), or from high leverage – or, need-
less to say, from combinations of the above. The portfolios are then stressed around their 
‘pressure points’ to the desired level of severity. 
13.2 Relative Strengths and Weaknesses of the 
Two Approaches 
The top-down (or ‘macro’) approach has an immediate intuitive appeal, and a sort of 
‘West-Wing-like’ glamour. Typically, behind the construction of the scenarios there is a 
187 

188 
CHAPTER 13 SELECTING AND COMBINING STRESS SCENARIOS 
structural model of what drives the economy – even better if there are several competing 
models. The top-down approach therefore builds directly into the formulation of the prob-
lem the causal links between the various macro drivers. In this respect, it lends itself very 
well to the Bayesian-net approach that I described in Parts II and III. 
Despite its appeal, several practical problems begin to crop up as soon as one begins to 
put into practice a top-down approach. Let me review them in detail. 
First of all, rarely do ‘macro’ events lead to unambiguously identiﬁable ﬁnancial out-
comes. As a consequence, the event tree can soon become very ‘bushy’. A major ﬁnancial 
crisis may weaken or strengthen the dollar depending on whether the ‘market psychology’ 
(or ‘sentiment’1) of the moment sees the dollar as a haven currency, or the overvalued 
currency of an imports-addicted nation. 
To give an idea of the practical difﬁculties associated with a top-down approach, one 
example springs to mind. In the summer of 2008, as the ﬁnancial crisis was reaching its 
severest point and oil was hovering around $140/barrel, the possibility was mooted of a 
preemptive air strike by the Israeli air force against Iranian nuclear power plants (as if we 
didn’t have enough problems at the time . . .). I was talking in those days with the head of 
trading of a commodities ﬁrm. I double-checked with him that such an air strike would 
send oil prices through the roof. His only-half-tongue-in-cheek answer was: ‘An air strike 
will certainly do either of two things to oil prices: either they will go up a lot. Or they will 
plummet.’ 
The trader’s thinking, of course, was that the air strike would either have a negative 
impact on the world oil supply or tip the economic sentiment resolutely towards doom and 
gloom, thereby depressing future projections of oil demand. The point of this anecdote is 
that even in this simplest of scenarios, the event tree begins to branch very soon (at the root); 
and almost any of the following branches carries a similar degree of uncertainty (what will 
the dollar do, for instance?). If our vulnerable positions are in oil (or if we are long dollars), 
handling this chain of dependencies may be easy enough. But what if our vulnerabilities are, 
say, in a large inventory of hybrid Adjustable Rate Mortgages (ARMs)? Before reaching 
the ‘sub-event’ that actually affects the vulnerable position in our books so many uncertain 
bifurcations in the event tree will have to be crossed that our conﬁdence in being able to 
handle the associated probabilities can be rather shaken. 
A second reason why a bottom-up approach may be better suited for positions held in 
the trading book is that relative-value trades can be extremely difﬁcult to capture using 
macro scenarios. Think of a large butterﬂy position on a yield curve: say, paying the ﬁxed 
rate in 9 years, receiving ﬁxed in 10 years and paying ﬁxed in 11 years in JGB (Japanese 
Government Bonds). (This position, by the way, is not fanciful, because around 1999–2000 
the trading dynamics of the JGB futures contract were creating distortions exactly around 
this part of the Japanese yield curve; the distortions, in turn, were attracting the attention 
of relative-value traders who were taking leveraged bets that the resulting kink in the 
yield curve would revert to normality.) Which ‘macro’ scenario could possibly stress these 
particular positions? How do you go from ‘Earthquake in Southern California’ or from 
‘Unexpected rise in non-farm payroll’ to the ‘Japanese butterﬂy’ – as the position was 
called at the time? How many steps does it take to get from A to B (or, perhaps, to Z)? Is 
there a path connecting the macro event and the potential loss? 
1See, e.g., Akerlof and Shiller (2009), Shefrin (2008) and Skidelsky (2009) for the importance of sentiment 
(‘animal spirits’) on ﬁnancial prices. 

13.2 RELATIVE STRENGTHS AND WEAKNESSES OF THE TWO APPROACHES 
189 
Here is a third problem with top-down approaches. If we want to assign marginal prob-
abilities to the root events in a macro tree, we are sometimes faced with a task far more 
difﬁcult than assigning marginal probabilities for ﬁnancial price moves. I have stressed that 
a purely frequentist answer for the probability of a price move of a given (large) magnitude 
should always be regarded with healthy suspicion. But at least we have in the raw data 
a starting point, something to fall back on if everything else fails. How do we begin to 
assign even an order-of-magnitude probability to an event like ‘Increased tension in the 
South China Sea’? In comparison, estimating an approximate probability for an event such 
as ‘S&P falls by 20%’ looks like child’s play. In Keynesian language, we are dealing in 
the case of most geopolitical events with pure uncertainty, not with risk. 
Sure enough, the problem does not magically disappear with the bottom-up approach. 
We still have to come up with marginal and conditional probabilities. The trick, however, is 
to choose our events as ‘close’ as possible to the pressure points of the portfolio, so that we 
can reach the signiﬁcant losses we are interested in as small a number of steps as possible. 
Then the task of assigning the various probabilities can be made more manageable. 
A fourth feature of macro approaches to stress testing that can limit their applicability 
for trading-book positions has to do with the holding period. When one speciﬁes events like 
‘sharp rise in unemployment’, ‘onset of deﬂation’, ‘generalized fall in house prices’, etc., 
one is referring to time periods ranging from a few months to a few years. For instance, 
the capital adequacy test run by the Federal Reserve Bank of New York early in 2009 for 
‘Fed-19 banks’,2 asked for cumulative losses over a two-year period given a base-line and an 
adverse scenario for unemployment, house prices and GDP growth. (See Hirtle, Schuermann 
and Stirch (2009) for an excellent example of a thoughtful exercise in stress testing at a 
macro level.) Such a time frame is very relevant to stress the structural or illiquid positions 
of a bank, but makes little sense for trading-book positions (assuming, of course, that what 
has been placed in the trading book belongs there . . .). 
It is therefore fair to say that a top-down or macro approach can be well suited to stressing 
the resilience and survivability of a ﬁnancial institution over time periods ranging from a 
few months to a few years, but does not readily lend itself to the analysis of trading-book 
positions. 
Of course, there are also disadvantages with the bottom-up approach. The ﬁrst is a certain 
degree of ad-hockery: since we start right from ‘where it hurts’ we may be missing the bigger 
picture. Trees loom very large in this approach, sometimes hiding the forest. For instance, 
some portfolio-related ‘events’ have common underlying causes that do not ﬁgure in our 
set of Boolean variables (say, a sudden and generalized deterioration in market liquidity, 
an increase in risk aversion or a widespread pulling-back from over-structured products). In 
principle the effect of these ‘latent causes’ could be captured by our conditional probability 
tables, but doing so can be difﬁcult. 
Also, when we employ a bottom-up approach, we are more likely to assign proximate 
rather than primary causes: we are closer to a reduced-form approach than to a fully-
speciﬁed model. This can be good or bad, depending on how ‘phenomenological’ we want 
our approach to end up being. 
So, when it comes to top-down or bottom-up, there is no such thing as an approach that 
works best under all circumstances. Questions of organizational resilience, analyses of the 
2Why 19? The ‘Fed-19 banks’ were chosen as the banks that, at the point in time of the test, had a balance 
sheet of at least $100bn. 

190 
CHAPTER 13 SELECTING AND COMBINING STRESS SCENARIOS 
impact on proﬁtability of regulatory or macro-economic events, studies of the performance 
under adverse conditions of buy-and-hold and banking-book portfolios are all probably 
better handled by a top-down approach. The shorter the holding period of the positions that 
are being stressed, the more compelling the case becomes for a bottom-up approach. 
13.3 Possible Approaches to a Top-Down Analysis 
If we decide that a top-down approach is our preferred route, we can do worse than looking 
carefully at what has been done for decades in areas such as foreign policy analysis and 
security planning. Much as I dislike the word ‘paradigm’, one useful approach goes under 
the name of creating strategic paradigms. For instance, in a security-planning context Davis 
and Sweeney (1999) identify several plausible, coherent geopolitical developments (which 
they call ‘paradigms’) and build their analyses around a number of coherent sets of events, 
rather than a series of isolated ‘stresses’. Even if their focus is very different from that of 
a ﬁnancial institution, it pays to look at their approach in some detail. Speciﬁcally, they 
analyse the following: 
• Paradigm A: Coalition of Opposing States : the coalescence of a loose coalition of 
states opposed to the United States, led by a strong China and a weaker Russia (Davis 
and Sweeney, 1999, page 225). 
• Paradigm B: Multipolarity : ‘the United States does not face a bifurcated international 
system, but rather confronts a setting in which ﬁve or six great powers have emerged’ 
(Davis and Sweeney, 1999, page 238). 
• Paradigm C: Weak Unipolarity : ‘the international system could develop in such a 
way that the United States remains the world’s sole great power. This system would 
emerge [. . .] from the inability of other great-power contenders to develop the means 
or inclination to claim status as a major pole’ (Davis and Sweeney, 1999, page 255). 
• Paradigm D: Chaos : ‘the United States is left as the sole superpower’ and ‘the major 
threat to global security comes from the breakdown of the state in several regions of 
the world’ (Davis and Sweeney, 1999, page 274). 
Why did I relate the approach by Davis and Sweeney (1999) in such detail? To underline 
the fact that, associated with each paradigm, there is a coherent story of how this posited 
development comes about, and of how it would affect different international players (Europe, 
China, the Middle East, etc.). Each paradigm therefore weaves together a set of linked 
interactions. Under the assumption underpinning each paradigm the question is asked of what 
the most likely response of each power block might be. Any of the available models, such 
as, say, the Rational Actor Model (see, e.g., Allison and Zelikov (1999)) or the institutional 
model (see, e.g., Davis and Sweeney (1999)), then provide an indication of what the response 
of the various players might be given the posited scenario. 
In a ﬁnancial setting, a paradigm is therefore made up of a story and model, not of a 
single event. ‘Sharp rise in inﬂation’, in this context is not a paradigm, but a stand-alone 
scenario. There are many states of the world in which a sharp rise of inﬂation may occur, 
and they may have very little in common. A sharp rise in inﬂation, for instance, can come 

13.4 SANITY CHECKS 
191 
about from an economy forging ahead on all cylinders in a situation of overloose monetary 
policy. But it may also arise as the consequence of overdone quantitative easing by central 
banks, carried out in order to pull an economy out of a deep recession. Clearly, the ‘other 
events’ (such as unemployment, corporate defaults, mortgage arrears, etc.) in the ‘paradigm’ 
will be very different in the two cases, at least in the short-to-medium term. 
Once again, it is difﬁcult to overemphasize the importance of simple structural models, 
imperfect as these may be. 
13.4 Sanity Checks 
Whichever approach is used, how do we make sure that we do not leave something obvi-
ous behind? When it comes to stress testing, there is no ‘completeness theorem’ to help 
us – there is no guarantee, that is, that we have not overlooked some major events. We can, 
however, employ a number of sanity checks. 
The ﬁrst and most obvious one is to compare the outcome of our stress analysis (i.e., 
the losses it suggests) against the worst losses our current portfolio would incur under 
the changes in risk factors ‘contained’ in the longest data series at our disposal. It is not 
necessarily the case that the loss produced by the most adverse historical change in risk 
factors (given today’s portfolio) must be a lower bound for the subjective loss. Perhaps we 
are conﬁdent enough to argue that the conditions under which these most adverse historical 
changes occurred cannot possibly repeat themselves today. For instance, perhaps we can 
argue that the South-East Asia currency crisis of 1997–1998 cannot occur in, say, 2010 
because the same countries are no longer relying on ‘hot’ foreign investment, and now sit 
on huge foreign reserves. Perhaps we can argue that a 1987-like equity market crash cannot 
occur because portfolio insurance is no longer as widespread. This is perfectly reasonable, 
since, after all, altering frequentist probabilities (or, indeed, possibility of occurrence) of 
events using subjective judgement is a two-way street: it can tell us that some events are 
more or less likely than they have been in the past (or, in the limit, impossible). It must 
be conceded, however, that from a prudential point of view only a very courageous risk 
manager would completely discard a chain of events that already happened in the past. 
A second, weaker, sanity check is the following. We can compare the subjective loss we 
have produced with the largest historical loss actually incurred (with a possibly very different 
portfolio) by our ﬁnancial institution. Here the warning sign is much weaker, and it is much 
easier to argue that we should not worry: for instance, if a bank exited a line of business 
after spectacular losses, those losses have very tangential relevance to today’s conditions. 
The decision as to whether past losses are relevant to today’s condition can become more 
nuanced if, for instance, we have today similar positions as when the loss was incurred, 
but we have put ‘protection in place’ (say, by buying CDSs or equity puts). Arguably, we 
could claim that, given this bought protection, the historical losses are no longer relevant. 
We would do well, however, to ask how conﬁdent we can be that the bought protection 
will still be there when we need it – as any purchaser of monoline protection knows all too 
well after circa 2007. 
In short, also in this case there are no hard-and-fast rules, or unambiguous ﬂoors. 
However, bringing to light, analysing and comparing these reference losses with the sub-
jective ones is a valuable and instructive process in itself. It is also a process that can 
provide a simple audit trail that elementary due diligence has been followed in arriving at 

192 
CHAPTER 13 SELECTING AND COMBINING STRESS SCENARIOS 
the subjective losses. This is particularly important if the outcome of the subjective analysis 
is to be used for internal limits or for regulatory capital purposes. 
Finally, we should always compare our subjective probabilities with the same quantities 
obtained using a purely frequentist approach. As I have said, in ﬁnancial matters I consider 
subjective probabilities more reliable and useful. But I also believe that these subjective 
probabilities always rest on the twin pillars of data and models of reality. I have argued long 
enough that data by themselves are mute. However, models unsupported by data are vacuous. 
Frequentist data are not the ultimate answers, but it would be foolish to neglect them. 
13.5 How to Combine Stresses – Handling the 
Dimensionality Curse 
In this book I have argued that, to be of value, stress testing must be an exercise that 
combines in a coherent picture the various severe adverse events that can affect a book, a 
business or an institution. Simply producing a list of ‘scary scenarios’ is of limited use. Not 
surprisingly, when stress testing is simply the presentation of a number of unrelated adverse 
events, senior management are unlikely to engage and give it much attention. The words 
by Aragones, Blanco and Dowd (2001) and Berkowitz (1999) quoted in the opening pages 
of Chapter 1 give a clear explanation of why this is the case. 
Weaving the various scenarios and stress events into a more coherent whole is therefore 
highly desirable. But, especially when one employs the bottom-up approach suggested above, 
one is still often faced with a very large number of events. Even if we limit our attention to 
a subset of the possible Bayesian nets that may conceivably connect them (say, to what we 
called in Section 8.8.1 the S2 subspace), the speciﬁcation of the marginal and conditional 
probabilities can still become too difﬁcult a task if the number of variables exceeds a dozen. 
For the avoidance of doubt, I must stress again that the difﬁculties I am referring to are not 
of computational nature – CPU is, these days, cheap and plentiful enough. What is time-
demanding and cognitively challenging is the thoughtful, yet parsimonious, determination 
of the dependencies, and of the conditional probability tables (or matrices) that underpin 
the approach. The ‘sanity checks’ that I strongly advocate (such as the ‘triplet constraint’ 
described in Section 9.6.2) fulﬁl the important role of forcing the risk manager (and the 
trader, and, we hope, the senior manager) to think hard about how the different events 
‘hang together’. But they do not ‘come cheap’. Having being involved in the process, I 
have found the exercise of creating, checking and revising the conditional probability tables 
and matrices one of the intellectually most demanding tasks a risk manager can undertake. 
There is little hope of carrying this out in a meaningful manner for more than about ten 
events. This being the case, how do we choose these events? How do we carry out the cruel 
‘pruning’ without which the exercise is doomed to failure? 
I can only offer some pragmatic suggestions here, which fall well short of a systematic 
approach to dimensionality reduction. I have nonetheless found them sufﬁciently useful 
in practice to deserve some attention. As the dimensionality problem tends to arise most 
frequently in the bottom-up approach, my treatment is tailored to this mode of operation. 
Let us suppose that the activities of a bank are divided into business lines, which we will 
call in the following Equities (E), Foreign Exchange (FX), Commodities (C), Credit Trading 
(CT ), Mortgages (M) and Fixed Income (FI). The risk manager responsible for, say, 
FI, will have determined – perhaps, but not necessarily, using the bottom-up approach – a 

13.5 HANDLING THE DIMENSIONALITY CURSE 
193 
handful of stressful events for the ﬁxed-income book under her watch. Using the techniques 
described above (Linear Programming and Bayesian nets) she will also have determined 
the associated conditional expected losses, Li(F I), where the index i labels the FI -related 
events. Note that this can be accomplished even if the simple Linear Programming approach, 
which requires no marginal probabilities, has been followed. She can then rank the ﬁxed-
income-related conditional expected losses thus obtained from largest to smallest. At this 
stage no probabilistic statement about their occurrence has been made. 
The other risk managers will have similarly determined, and ranked, their conditional 
expected losses, LiC(C), LiE(E), LiM(M), LiFX(F X) and LiCT (CT ). The indices 
iC, iE, . . ., iCT label the losses from commodities, equities, . . ., credit trading, and each 
index runs from 1 to the number, nC, nE, . . ., nCT , of vulnerabilities (and hence losses) in 
the various portfolios. 
Once this is done, the losses from all the business lines can be put together and sorted 
from largest to smallest. The risk managers can then set a materiality cut-off, or more 
pragmatically, choose the maximum number of variables they feel they can handle. Suppose 
that n events (Boolean random variables) are selected, with n ≲ 10. To each event (i.e., to 
the realization of each Boolean variable) there is associated a conditional expected loss 
that ‘takes into account’ the interaction of the other stress events in the same asset class. 
So, for instance, the event with the largest expected conditional loss could ‘come from’ 
the ﬁxed-income area and it could correspond, say, to the steepening of the yield curve. 
Its associated expected conditional loss will of course also ‘contain information’ about the 
other ﬁxed-income-related stress events (say, a widening of swap/government spreads, the 
narrowing of the OIS/LIBOR spread, etc.). 
The risk manager can now begin to build the Bayesian network as described above using 
the n chosen events with the largest conditional losses as variables. The variable associated 
with each node will be the primary event associated with each of the top n losses. So, for 
instance, one event, coming from the ﬁxed-income area, could be a yield curve steepening. 
How shall the conditional probability table then be constructed? Since the losses are 
conditional expected losses, these are the losses that on average one can expect to make if 
the associated primary event (say, the yield curve steepening) occurs. 
Let us collapse the full distribution of conditional losses associated with each primary 
event to a Dirac-δ distribution centred at its expectation. What we are saying is that, con-
ditional on a given primary event happening, we will make the conditional loss associated 
with it with certainty. 
This is crude but useful. In fact, if we are prepared to make this heroic assumption we 
can then associate to the expected conditional loss, Li(X) (where X denotes the generic 
business line), the same probability of occurrence as the event that gives rise to Li(X). 
It bears repeating that with this approximation what one is saying is that, conditional on 
event EXi having occurred, the loss incurred will be Li(X) with certainty. I must stress, 
however, that this is not the loss due to the stand-alone occurrence of the associated stress 
event, but it is a loss that ‘knows about’ the interaction (concentration, diversiﬁcation, etc.) 
with the other losses in the same asset class . 
Of course, even if up to this point the risk managers had only provided conditional 
probabilities, to make progress the marginal probabilities for the primary events will now 
have to be supplied. The risk managers, however, will only have to supply, say, ﬁve to 
ten such estimates. The number of the conditional probability tables required by the pruned 
Bayesian net will also be correspondingly much, much smaller than if we had retained the 

194 
CHAPTER 13 SELECTING AND COMBINING STRESS SCENARIOS 
losses from all the asset classes. And as for what is in the conditional probability tables 
themselves, the maximum order of conditioning, of course, will depend on the assumption 
about the ‘connectivity’, k, of the Bayesian net in the subset Sk that we want to build. 
The advantage of this approach is substantial. If each risk manager has started from, 
say, eight stress scenarios for her own asset class, a brute-force approach with no ‘pruning’ 
would require about 50 marginal probabilities, and dozens of conditional probability tables. 
Questionable as the assumptions behind the approach described above might be, they can 
make the difference between being able to implement the approach in a real-life situation 
or not. 
Of course, such a huge simpliﬁcation does not come for free. There is, to start with, the 
collapse to a δ-distribution of the full possible distribution of conditional losses associated 
with a given stress event to worry about. More subtly, but, possibly more importantly, there 
is no guarantee that some stress event that was ‘just below the cut-off’ might not, if retained, 
have given rise to a more adverse interaction with other scenarios. 
13.6 Combining the Macro and Bottom-Up Approaches 
So far I have presented the bottom-up and the top-down approaches as incompatible. But 
this need not be the case. I present in this section an example of how they can be made to 
talk to each other. 
Let us consider the case where we have identiﬁed ﬁve vulnerabilities for our portfolio: 
• default of China; 
• equity market crash; 
• widening in spreads of CMBSs; 
• generalized widening of credit spreads; 
• sharp increase in short rates. 
Suppose that we follow the prescription for building a Bayesian net outlined in 
Chapters 9 and 11. The ﬁrst thing we have to do is to draw boxes with the names of 
the events on a sheet of paper. This is the easy bit. See Figure 13.1, where the different 
variables have been placed pretty much randomly, that is, without any thought as to the 
possible causal links we may end up drawing. 
China defaults 
Short rates up 
Equity market crash 
CMBS spreads widen 
Credit spreads widen 
Figure 13.1 Building a Bayesian net: placing the variables. 

13.6 COMBINING THE MACRO AND BOTTOM-UP APPROACHES 
195 
Now we have to draw the arrows that connect the boxes. This is where the thinking 
begins. 
The problem is that some events can be reasonably thought of as causes of others – say 
China’s default would presumably ‘cause’ an equity market crash. But for some other events 
what causes what depends on how macro events in the real world are actually unfolding. 
For instance, an unexpected rise in short rates could cause a fall in equity prices. However, 
this may not be the case if the rise in rates came on the back of particularly good economic 
news – see, for instance, what happened to equity prices worldwide when the Australian 
Central Bank unexpectedly raised rates in October 2009. 
It pays therefore to draw a small set of causal links (directed arrows) among the variables 
on the basis of a small number of macro scenarios. This means building a different Bayesian 
net for each of the underlying macro scenarios. The interesting feature of the approach is 
that the macro scenarios in question could be those chosen for a top-down stress-testing 
exercise for the ﬁrm as a whole. This approach therefore goes some way towards combining 
macro-driven with vulnerability-driven approaches to stress testing. 
Let us see how this can work out in practice. In one macro scenario it is an Asian crisis 
that triggers mayhem in the markets. The variable associated with the event ‘China default’ 
could therefore be a primary ancestor in our Bayesian net. If this is the case, the widening 
in CMBS spreads and the generalized widening of credit spreads can probably be conﬂated 
into one single event. The sharp increase in short rates (which in this scenario is now very 
unlikely) would then probably be another primary variable (i.e., a variable without parents). 
The Bayesian net may therefore look something like Figure 13.2. 
Another scenario could be driven by, say, a speciﬁc deterioration in the commercial real 
estate area. In this macro scenario it is unlikely that a rise in short rates could be due to 
‘good news’. So it is reasonable to posit a negative causal link between the rise in rates and 
equity prices. In this case a plausible Bayesian net could look something like Figure 13.3. 
It is easy to see how the reasoning could be extended. The macro scenarios I have 
sketched for this baby example are, of course, over-stylized. In reality comprehensive 
‘paradigms’ like the ones presented in Section 13.3 above should be carefully devised. 
As each ‘paradigm’ could be one of the ﬁrm-wide macro stress tests, we have a way to 
associate the ‘banking book’ losses coming from the top-down approach and the ‘trading 
book’ losses coming from the bottom-up approach. 
China defaults 
Short rates up 
Equity market crash 
CMBS and credit spreads widen 
Figure 13.2 Building a Bayesian net: assigning the causal link among the variables. 
China defaults 
Short rates up 
CMBS spreads widen 
Equity market crash 
Credit spreads widen 
Figure 13.3 A Bayesian net reﬂecting the causal links discussed in the text. 

196 
CHAPTER 13 SELECTING AND COMBINING STRESS SCENARIOS 
Now that we have drawn the arrows (one set of arrows for each macro scenario), the next 
task is to ﬁll in the conditional probability tables. Everything works as explained above, 
but needless to say, the entries in these tables are macro-scenario-dependent. For instance, 
the marginal probability of short rates rising could be very different in the case of an Asian 
crisis or of inﬂationary pressures appearing in Western economies. 
The ﬁnal result of this exercise would be a set of joint probabilities for each macro sce-
nario. The analysis can then proceed as usual. We have to be careful, though: what we have 
obtained are now sets of joint probabilities, each conditional on one macro scenario coming 
true. Perhaps we may want to build a Bayesian net linking the macro scenarios, produce 
the associated conditional probability table and obtain a ‘true’ overall joint probability dis-
tribution. Appealing as this approach may be from a conceptual point of view, the practical 
difﬁculties in carrying out this exercise can make brave risk managers tremble with fear. I 
would suggest that just analysing the outputs from the various macro scenarios (with the 
attached probability of joint-event losses) is a very valuable exercise in itself. Going back 
to the capital adequacy stress test carried out by the Federal Reserve Bank in early 2009, 
this is exactly what it did: it produced a base-line and an adverse macro scenario, asked 
the banks to give their quantitative feedback for both scenarios and critically examined the 
results from the various banks originating from the two scenarios.3 
If a way to associate capital to stress losses is what we are looking for, it does not take 
too much imagination to dream up some prudential worse-off rule. 
3Incidentally, here is another similarity between the exercise carried out by the Federal Reserve Bank and 
the approach I recommend in this book: the ‘adverse’ scenario of the Fed was deﬁned as a one-in-ten event 
(Schuermann, 2009, personal communication). Note the broad-brush, order-of-magnitude speciﬁcation of the prob-
ability level, and the conscious effort to avoid spurious precision. 

Chapter 14 
Governance 
14.1 The Institutional Aspects of Stress Testing 
Stress testing does not take place in a vacuum. Even the best ideas about stress testing will 
come to little use unless the way it is carried out is consonant with the risk-management 
practices of the institutions that make use of it. To be effective, stress testing must lead 
to decisions and, when appropriate, to action. For this to occur, the overall stress-testing 
process must meet a large number of institutional requirements, such as the following: 
• transparency and ease of use; 
• challenge by non-specialists; 
• checks for completeness; 
• interactions among different specialists; 
• auditability of the process and of the results; 
• sensitivity of the outputs to the inputs and the underlying assumptions must be made 
transparent. 
This list is far from exhaustive, but already gives an idea of the institutional complexity 
that a well-thought-out stress-testing programme entails. Let me explain what I mean by 
these one-liners in greater detail. 
14.1.1 Transparency and Ease of Use 
The outcomes of the stress-testing exercise must be presentable in a way that engages the 
attention of all its users, from expert risk managers and traders to non-specialist senior 
managers. The output of stress testing will typically serve different functions for different 
users: it can be used for information, for discussion, for risk control (e.g., via limits) and 
197 

198 
CHAPTER 14 GOVERNANCE 
to inform action (e.g., reduction of positions or purchase of protection). As one moves 
up the decisional chain of a ﬁnancial institution, the information provided must become 
progressively more synthetic, concise and easier to question at a qualitative level. When 
the risk manager communicates the results of the stress test to senior executives, the details 
of the construction of the conditional probability tables may have to be removed from 
sight like a piece of scaffolding that is removed once the building job is done. I say 
‘removed’, but not discarded. At every step any user must be able to question and challenge 
the assumptions. Traders, for instance, may question a particular choice for a marginal or 
conditional probability (even if they may not phrase their challenge quite in these terms). 
When this happens, a risk manager had better have done her homework well if she wants 
to retain her credibility (see the next section). 
14.1.2 Challenge by Non-specialists 
One of the appealing features of the approach I propose is that non-specialists, and in 
particular, intelligent users with little or no quantitative background, can challenge the 
choices made along the way. A non-specialist should, for instance, be able to ask: ‘Why 
did you assume that this event depends on these two events, not on these?’ (By the way, 
‘for the sake of simplicity’ is an excellent answer, as long as it is made clear that this is 
the case.) The same non-specialist should feel able, and encouraged by the way the results 
are presented, to ask: ‘Why is the probability of credit spreads widening given an equity 
market crash 80%?’ The input of a non-quantitative but experienced trader is in this respect 
invaluable. 
The assumptions that underpin the approach presented in this book are more transparent 
than, say, the choice of the number of degrees of freedom in a t -Student distribution, or the 
exact type of copula function chosen. This is an advantage of the approach, and as such, it 
should be exploited. 
14.1.3 Checks for Completeness 
Most stress-testing approaches start from some criterion to decide whether a given candidate 
qualiﬁes or not as a stress event to be analysed: for instance, the criterion may be based 
on the magnitude of the potential loss. No matter how thorough the risk manager strives 
to be, no list of possible stress events will ever be ‘complete’ – nor is it obvious what 
‘complete’ could mean in this context. Nonetheless, the risk manager must ensure that she 
has not left something obvious by the wayside. This is far more common than one might 
think. Risk managers seem to be both constitutionally inclined and condemned always to 
prepare for yesterday’s war. The endless arrays of Maginot lines built by risk managers 
(and regulators, bankers, etc.) in the ﬁnancial theatres of operations are a testament both to 
Fortune’s wickedness, and to the limitation of human imagination. 
In order to try to contain the damage – and the embarrassment – some criteria should 
therefore be speciﬁed to ensure that major stress events have not been ‘forgotten’. I men-
tioned several of these sanity checks in Section 13.4, but it is a good idea to go brieﬂy over 
them again. 

14.1 THE INSTITUTIONAL ASPECTS OF STRESS TESTING 
199 
A good starting point is to consider the worst historical hypothetical loss1 in the longest 
database at the risk manager’s disposal, and compare it with the outcome from the stress 
testing exercise. The risk manager may well conclude that today’s conditions are such that 
the historical loss cannot possibly repeat itself today, but the question of why this is the 
case should, at the very least, be asked. 
Similarly, the largest losses incurred by a ﬁnancial institution in the recent and not-so-
recent past should be carefully examined, and the question asked whether they could occur 
tomorrow again. Perhaps the offending risk positions and concentrations have been cut; 
perhaps today’s conditions are totally different; perhaps hedging positions have been put in 
place. But, just for peace of mind, all these ‘perhaps’ should be carefully examined. 
Another valuable sanity check is simply asking traders and risk managers about potential 
losses and portfolio vulnerabilities in areas not under their daily professional watch. I have 
always found it amazing how penetrating traders’ risk insight can be when they look not at 
their own but at their colleagues’ positions. 
Major historical blow-ups (from Metallgesellshaft to LTCM to Amaranth) should be 
carefully examined, especially when some of the strategies (from swap spread carry trades, 
to various cash/futures bases, to summer/winter gas spreads) have a tendency to be put 
in place, under different guises, over and over again: history does not repeat itself, but it 
certainly rhymes. 
Many institutions routinely run stress scenarios against standardized stress events (such 
as: rigid move of all yield curves by 50 basis points, equity market crash by 30%, oil 
spike by 60%, etc.). These standardized stress events may not be particularly realistic (all 
yield curves have never moved in parallel by 50 basis points), and they may be unable to 
uncover the vulnerability of a complex portfolio, especially when this is peppered by highly 
leveraged relative-value positions. Despite all of this, the losses associated with these ready-
made stresses should still be looked at and compared carefully with the output of the more 
realistic and sophisticated approach proposed above. There may be very good reasons why a 
more focused stress approach is ‘superior’, but these reasons must be identiﬁed and clearly 
articulated. 
Finally, as discussed in the previous chapter, it would be foolish to dispense with the 
analysis of all the frequentist information the risk manager can gather. 
Even when all of this checking has been done, still there is no guarantee that everything 
has been caught in the net of our stress-event identiﬁcation procedure. However, if the risk 
manager has done her job properly, all these sanity checks should throw up either milder 
losses than the ones identiﬁed by the risk manager, or the same ones. Even in this case, the 
time spent carrying out the sanity checks will not have been wasted, because the outcomes 
can provide reassurance that a certain degree of ‘due diligence’ has been carried out in 
selecting the stress events. This can be of particular relevance when it comes to convincing 
a regulator or an audit committee that a robust and justiﬁable process has been followed. 
14.1.4 Interactions among Different Specialists 
For the process described in this book to work well, the interaction of professionals with 
very different skills and types of experience are required. Let me list a few: 
1By hypothetical loss I mean a loss that would be incurred given today’s positions if the combination of 
changes in risk factors that happened on one particular day in the past occurred today. 

200 
CHAPTER 14 GOVERNANCE 
• The identiﬁcation of relevant stress events will typically be carried out by traders and 
front-line risk managers. A strong understanding of market dynamics (understood as 
‘how the market works’) is the most relevant skill here. 
• The association of the magnitude of the loss with the probability level will have 
to be carried out by risk managers and quantitative analysts who have both a good 
understanding of statistical estimation techniques, and a strong grasp of how relevant 
past data may be to today’s conditions. 
• After this piece of analysis has been carried out, and the magnitude and type of the 
various losses has become apparent, the selection of the most salient stress events we 
want to retain and work with (what I called elsewhere the ‘pruning’ process) can take 
place. Risk managers with a good understanding of the capital and liquidity position 
of the institution they work for are best placed to carry out this task. 
• Once we know what the relevant stress events are (or, rather, which ones we can handle 
given our cognitive and computational constraints) a parsimonious network of causal 
links among them must be established. Quantitative analysts, traders and front-ofﬁce 
risk managers must interact here. 
• The elicitation of the minimal set of marginal and conditional probabilities required 
to ﬁll in the conditional probability tables requires careful analysis and discussion by 
the same parties. 
• Obtaining the joint probabilities is, in a sense, a mechanical process, which could, in 
theory, be delegated to pure quantitative analysts. However, as I showed in Chapters 7 
to 9, this is a very interactive process, during which the various derived probabilities 
(marginal via the completion property, conditional by repeated use of the Bayes’ 
theorem, and joint by a combination of the above) should be questioned at each stage. 
• Also the various ‘sanity checks’ (triplet conditions, etc.) should invite debate, ques-
tioning and a back-and-forth process of progressive adjustment. 
• Finally we have the output. In a way, this is just the beginning of the process. Different 
ultimate users will look at the results in a different light. Traders may want to compare 
the outputs of this process with their qualitative understanding of their (and their 
colleagues’) books. Risk managers will have a tool to check whether the positions 
taken on by the ﬁnancial institution are congruent with the risk appetite articulated 
by the board. Of course, if limits or guidelines have been applied to the outcomes of 
the stress test, these must be monitored and enforced. Senior management must be 
presented with a synthetic and intuitive view of the results, possibly accompanied by 
an economic ‘story’ of how the stress losses may come about. In doing so, the detailed 
workings of the process, especially the most technical ones, will almost certainly be 
‘removed from sight’. It is important, at the same time, to convey an idea of which 
broad assumptions have gone into the analysis, and of the limitation of the approach. 
Given the intuitive appeal of the approach, it is possible that senior managers, and 
even board members, may be inclined to question some of the building blocks, such 
as: ‘Why has this and not that causal link been chosen?’, or ‘Why has the probability 
of X happening given that Y has happened been assumed to be 40%?’ For this debate 

14.2 LINES OF CRITICISM 
201 
to happen, information must be presented in an intuitively appealing and information-
preserving manner. 
The process is complex, and a lot of time must be devoted to it. Refreshingly, the ‘time’ 
in question is mainly thinking time, rather than CPU time. The important thing to point out 
is that there is only so much delegation of responsibilities to specialists and quants that can 
take place, if we want the process to remain effective. 
This should come as no surprise. I believe that the output from a purely statistical 
approach such as VaR must be complemented by a tool, such as stress testing, which 
is different in nature but of similar breadth and import. If this is the case, it would be 
unreasonable to expect that a handful of clever people working quietly in a forgotten corner 
of the bank could provide all the answers required. If we think of the size of the capital 
expenditure budget and of the recurring payroll costs required to run the VaR programme 
for a bank, we cannot expect that a realistic alternative can be achieved at little cost and 
with negligible effort. 
14.1.5 Auditability of the Process and of the Results 
The stress-testing programme I have proposed in this book can have many applications, 
both internal and external to an institution. Regulators are still discussing exactly which 
form the capital add-ons beyond VaR will take, but they will certainly associate some form 
of stress testing to the new additional trading-book capital. At the time of writing, regulators 
are ﬁrmly leaving the responsibility of coming up with a workable stress-testing framework 
to the industry. Statistical (frequentist) approaches, especially when it comes to rare events, 
have many drawbacks (and these have been recognized by the regulators), but at least 
they have the advantage of an apparent ease of standardization. If the approach suggested 
in this book is going to be used to calculate some form of capital add-on the regulators 
must understandably be able to make themselves happy that the process is robust, and that 
a high-quality challenge by an independent function has occurred at every crucial stage 
of the process. The subjectivity of the probability inputs is one source of the strength of 
the approach and gives it the potential for being much more powerful than a traditional 
frequentist approach. The same subjectivity, however, can also be its weakness from the 
point of view of an external ‘observer’. To allay the understandable scepticism of regulators, 
they must be reassured not so much of the ‘correctness’ of the inputs, but of the fact that 
they are unbiased, and that once chosen, they will not become subject to internal pressure 
by the front ofﬁce. There are ways to ensure that this is the case, but the importance of 
this aspect cannot be overemphasized. I touched above on the need for transparency if the 
results are to be used effectively within an institution. An extra dimension of transparency 
must therefore be enforced if regulators are to endorse the process for capital purposes. 
14.2 Lines of Criticism 
There are several grounds on which the approach proposed in this book could be criticized. 
Having spoken to many risk managers, the main strands of criticism can be grouped under 
two broad rubrics: complexity and subjectivity. Given the particularly prominent role played 
by subjective expert judgement in the process I have described in this book, I will start from 
this aspect. 

202 
CHAPTER 14 GOVERNANCE 
14.2.1 The Role of Subjective Inputs 
. . . [t]he subjectivist states his judgements, whereas the objectivist sweeps them under 
the carpet by calling assumptions knowledge, and [. . .] basks in the glorious objectivity 
of science. 
Good (1973) 
One of the recurrent objections that are raised to the approach I have presented is that ‘it 
is too subjective’. Surprisingly, sometimes this objection is raised even by risk managers, 
who should be the ‘experts’ from whom the judgement is sought, and who, presumably, 
should feel empowered by an approach that calls for abilities not possessed by a ‘shift-F9 
monkey’. 
It is true that the approach above calls for subjective judgement. Several clariﬁcations 
are, however, in order. 
First of all, every approach has a surprisingly large degree of subjectivity embedded 
into its execution. In this respect, the main difference between traditional techniques and 
the approach I propose here are the transparency and auditability of its subjective aspects, 
the ease with which its underlying assumptions can be understood, and the simplicity with 
which sensitivity analysis to its inputs can be carried out. 
Let me start from my ﬁrst claim, that is, that every approach to stress testing (and indeed 
to the quantiﬁcation of risk) has an embedded element of subjectivity. Consider, for instance, 
the case of the VaR statistic calculated using a historical simulation approach. Surely, almost 
nothing has been left to the judgement of the risk manager. The answer is ‘all in the data’. 
Or is it? 
To begin with, how has the length of the data window been chosen? If, as frequently hap-
pens, we use a ﬁnite window with equal weights (say, of two years) we are implicitly saying 
that every single day in this two-year period ‘counts’ exactly as much as yesterday – where 
‘counts as much as yesterday’ means here that it has exactly the same statistical relevance 
for the prediction of tomorrow’s price moves. So, what happened exactly two years ago has 
the same ‘salience’ as what happened yesterday, but what happened two years and one day 
ago has no relevance at all. To understand the relevance of this very subjective (but rarely 
questioned) choice, the reader can reﬂect on the following: at the time of this writing the 
default of Lehman happened almost exactly one year ago. For those banks that, with the 
blessing of the regulators, calculate – and report! – their VaR on the basis of a one-year win-
dow, the institutional memory of the most dramatic ﬁnancial event of the last 50 years will 
be lost next Monday. For those banks who instead use a two-year window (and also do so 
with the blessing of the regulators), the VaR analysed for internal risk management purposes 
and reported for external information will be affected by completely different events. 
Surely, the solution here is to use non-constant weights. I am all in favour of using well-
chosen weights. But the question is: what do we mean by ‘well-chosen’? Is time proximity 
the only criterion of relevance? If not, then a subjective choice of which ‘patches’ of past 
history are relevant and which are not must surely be introduced. Even if past events are 
simply given exponentially decaying weights, how the decay constant is chosen can make 
an enormous (and opaque!) difference. The popular decay factor of 0.94, for instance, gives 
approximately half as much weight to events that happened 12 days ago as to yesterday’s 
occurrence, and weighs events that happened 30 days ago little more than 15%. 

14.2 LINES OF CRITICISM 
203 
Let us look at another topic that greatly affects ‘objective’ risk measures, yet is often 
swept under the carpet: the issue of mapping. Suppose that we are calculating VaR for a 
currency that has never de-pegged, but that we fear may do so in the near future. Clearly, the 
time series history for that particular currency will not give us a reasonable answer: a strict 
frequentist can only look at the available, de-pegging-free history, and shrug her shoulders. 
The standard response from less doctrinaire risk managers who still like their answers to 
come straight from distributions and percentiles is to ‘map’ the currency in question to 
the history of another ‘similar’ currency that did default. But what constitutes a similar 
currency? Is geographical proximity the main relevant criterion – that is, if the currency 
we are interested in is a South-East Asian should we necessarily look for another currency 
in this part of the world? Or is the state of current account balance of the country more 
relevant? More subtly: suppose that the particular currency we are interested in did de-peg 
in the past, say, during the 1997–1998 crisis, but is now running a massive current account 
surplus and has curbed the inﬂux of hot investment money? Are we happy in using that patch 
of history for our guess about what might happen tomorrow, just because it is in our dataset? 
The case of the currency de-pegging is, of course, a limiting one. But similar mapping 
choices are ubiquitous, because we cannot just have a time series for every single price, 
rate, volatility or risk factor. Of course, there is nothing wrong about this – these are just 
the hard choices that a reasonable risk manager or statistician will always have to make. It 
is the ‘objectivity’ pretence that is difﬁcult to defend. Subjectivity plays an important role 
in all statistical analysis, and in statistical analysis applied to risk management more than 
anywhere else. The more productive approach is to acknowledge and make as transparent 
as possible the inevitable choices that invariably have to be made. Sunlight is also in this 
context the best disinfectant. 
And there are far more subjective inputs that come into the determination of the so-called 
‘objective’ risk measures – and that are routinely ignored or forgotten at disclosure time. To 
begin with, the choice of percentile in the case of the VaR or CES statistics is as arbitrary as 
it is important.2 Once the percentile has been chosen, the method of calculation itself (Monte 
Carlo, historical simulation, variance-covariance, etc.) can also affect the results greatly. If 
more sophisticated techniques such as Extreme Value Theory or Copula Theory are used, 
the precise choice of the copula or of the tail behaviour and their precise parametrization 
can make enormous differences, especially when it comes to estimating the probability of 
very rare events. And, of course, this list is not exhaustive. 
In short, the general point is threefold: subjectivity is unavoidable; in statistical risk 
analysis, the effect of subjective choices is always larger than you would like to think; 
it is better to have ‘transparent’ and ‘auditable’ subjectivity than invisible model choices 
embedded deep down in the core of the calculation or process. 
14.2.2 The Complexity of the Stress-testing Process 
The second most frequent objection that I hear about the approach I have proposed in this 
book is that it is too complex. There is no denying that it cannot be made to work in an 
2Bank analysts tend to compare the ‘objective’ VaR inputs of banks who report VaR at different percentile 
levels by scaling up or down using the theoretical ratio of the corresponding percentiles in the case of the Gaussian 
distribution. This is reasonable, because the analysts cannot, of course, know the true tail behaviour of the proﬁt-
and-loss distribution of the various banks. However, this simple procedure can easily understate by 25–40% the 
estimate of the VaR at the 99 percentile if the reported number was evaluated at the 95 percentile. 

204 
CHAPTER 14 GOVERNANCE 
afternoon, or in a week. It requires coordination among different functions (see my discussion 
in Section 14.1.4 above); it makes high demands on the thinking time of the risk managers; 
and it requires familiarity with quantitative techniques (such as Linear Programming and 
Bayesian networks), which are unlikely to be in the standard toolkit of the average risk 
3
manager.
All of this is true. But, as I argued above, it would be truly extraordinary if we could 
overcome all the shortcomings the traditional statistical approach to risk quantiﬁcation with 
a simple ﬁx that can be added on as an afterthought at little effort and less cost. Solu-
tions to (or improvements on) complex problems rarely are simple and even more rarely 
come cheap. 
Despite the relatively high level of commitment and resources required by the full imple-
mentation of the stress-testing programme I have suggested in this book, there is one 
redeeming feature: every stage of the process brings about incremental improvements to 
our understanding of the risk position of a ﬁrm. Even the entry-level identiﬁcation of port-
folio vulnerabilities, with no probabilistic assessment whatsoever, has great value. So has 
the assignment of events to probability buckets (however we may choose to do so); the 
choice of the conditional probabilities; the teasing out of the causal link among the various 
events; the pruning of the Bayesian net; the analysis of the stand-alone expected conditional 
losses; etc. I do believe that these features in themselves (which I collectively refer to as the 
‘polyvalence’ of the stress-testing programme I have proposed) can make it a worthwhile 
investment of resources. 
In closing, I do not expect for a second that what I have proposed in this book is the 
deﬁnitive answer to the stress-testing problem, let alone to the current challenges of ﬁnancial 
risk management. I just hope that it can provide material for discussion and improvements, 
and suggest a possible path to exit Berkowitz’s statistical purgatory in which stress testing 
has so far been mired. 
3By the way, these techniques may be less familiar, but they are far simpler, and require far less mathematical 
sophistication, than, say, stochastic calculus or measure theory. 

Appendix 
A Simple Introduction to Linear 
Programming 
A.1 Plan of the Appendix 
The task dealt with in the chapters in Part III is to obtain a set of self-consistent conditional 
probabilities as close as possible to the exogenous speciﬁcation for the same quantities 
provided by the risk manager. One attractive way to ensure consistency is via Linear Pro-
gramming. Since this technique is perhaps not as well known in the risk management 
community as, say, stochastic calculus or Extreme Value Theory, I present in this Appendix 
a brief and concise introduction. The main ideas behind the Linear Programming approach 
are very simple, but one of the main obstacles lies in the notation and terminology, which 
I will therefore try to present as clearly as possible with the speciﬁc application at hand 
clearly in mind. 
A.2 Linear Programming – A Refresher 
Linear Programming deals with constrained optimization problems1 when 
• the objective function, f (i.e., the function to be maximized or minimized), is linear 
in the control variables, x1, x2, . . .  , xn; and  
• the constraints among the control variables are linear inequalities. 
The inequality constraints therefore restrict by (hyper)-planes the domain of the control 
variables. 
1The treatment presented in this section closely mirrors Kreiszig (1993), Chapter 12. 
205 

206 
APPENDIX 
The best way to understand the ideas behind Linear Programming and to introduce the 
terminology is to consider a concrete example. The following numerical example is taken 
directly from Kreiszig (1993). 
Let the function to be maximized be given by 
f (x1, x2) = 29x1 + 45x2 
(A.1) 
and let the linear inequalities be 
2x1 + 8x2 ≤ 60 
(A.2) 
4x1 + 4x2 ≤ 60 
(A.3) 
x1 ≥ 0
 (A.4)
x2 ≥ 0
 (A.5)
Let us focus on the ﬁrst inequality (Equation (A.2)), which we can rewrite as 
60 − 2x1 − 8x2 ≥ 0
 (A.6)
So, if we deﬁne x3 to be the quantity 
x3 = 60 − 2x1 − 8x2 ≥ 0
 (A.7)
by construction it must be non-negative. It serves the purpose of turning an inequality such 
as Equation (A.2) into an equality: 
2x1 + 8x2 + x3 = 60 
(A.8) 
plus a positivity constraint: 
x3 ≥ 0
 (A.9)
A variable such as x3 is called a slack variable. 
Repeating the same procedure for constraint (A.3), and introducing slack variable x4, we  
can write 
2x1 + 8x2 + x3 = 60 
(A.10) 
4x1 + 4x2 + x4 = 60 
(A.11) 
x1 ≥ 0 
(A.12) 
x2 ≥ 0 
(A.13) 
x3 ≥ 0 
(A.14) 
x4 ≥ 0 
(A.15) 

A.2 LINEAR PROGRAMMING – A REFRESHER 
207 
with 
x4 = 60 −4x1 −4x2 
(A.16) 
More generally, consider an objective function, f , 
f = f (c1x1 + c2x2 + · · · + cnxn) 
(A.17) 
that must be maximized subject to the constraints 
a11x1 + · · · + a1nxn = b1 
(A.18) 
a21x1 + · · · + a2nxn = b2 
. . .  
am1x1 + · · · + amnxn = bm 
xi ≥ 0 
for i = 1, 2, . . .  , n. (Note that the index n includes the slack variables, for which the coef-
ﬁcients c in the argument of the function f are zero, and m is the number of ‘original’ 
constraints – that is, the number of constraints excluding the positivity constraints.) Then 
we can give the following deﬁnitions. 
Deﬁnition 1 
A solution (i.e., an n-tuple x1, x2, . . .  , xn) that satisﬁes constraints (A.18) is 
called a feasible solution. 
Deﬁnition 2 
An optimal solution is a feasible solution that maximizes function f . 
Deﬁnition 3 
A basic feasible solution is a feasible solution for which at least n −m of 
the variables x1, x2, . . .  , xn are 0. 
Note that we have deﬁned a basic feasible solution as a solution which satisﬁes the 
constraints, which maximizes f and for which at least n −m of the variables are 0 . We  
are not saying that all solutions that satisfy the constraints and maximize the function f 
are basic. The following fundamental theorem of Linear Programming, however, reassures 
ourselves of the following: 
Theorem 1 
Given the Linear Programming problem in Equations (A.17) and (A.18), some 
optimal solution is also a basic feasible solution. 
The power of the theorem lies in the following observation. In general, a Linear Program-
ming problem may have many optimal solutions (i.e., n-tuples x1, x2, . . .  , xn that maximize 
f and satisfy the constraints), but they may not all be basic feasible solutions. The funda-
mental theorem assures us that, even if we restrict our search to the basic solutions, we will 
never miss an optimal solution. 

208 
APPENDIX 
Restricting the search to basic feasible solutions simpliﬁes the task, but it still remains 
far from trivial. There is a not-so-small cottage industry devoted to providing search 
methodologies of different complexity and efﬁciency. For our purposes the Simplex Method 
(introduced as far back as 1948) will serve us well enough. 
A.3 The Simplex Method 
The Simplex Method proceeds iteratively, by identifying a basic feasible solution to start 
with (we know that the optimal solution will belong to this set), and then performing three 
operations for each step: 
1. We check whether the basic feasible solution is optimal (i.e., satisﬁes the constraints 
and maximizes the target function, f ). If it does, we stop. 
2. If it does not, we ﬁnd a better basic feasible solution. 
3. We transition to that better solution and go back to 1. 
Let us see in detail how this works, by considering again the super-simple problem we 
looked at before: maximize 
f (x1, x2) = 29x1 + 45x2 
(A.19) 
subject to 
2x1 + 8x2 + x3 = 60 
(A.20) 
4x1 + 4x2 + x4 = 60 
(A.21) 
x1 ≥ 0 
(A.22) 
x2 ≥ 0 
(A.23) 
x3 ≥ 0 
(A.24) 
x4 ≥ 0 
(A.25) 
with 
x3 = 60 − 2x1 − 8x2 
(A.26) 
x4 = 60 − 4x1 − 4x2 
(A.27) 
To carry out the ﬁrst step (i.e., ﬁnding a basic feasible solution) we divide the four variables, 
x1, x2, x3, x4, into two groups by selecting m = 2 variables as basic variables. Then, by 
deﬁnition of basic feasible solution, the other two variables must be zero at the basic 
feasible solution. We shall call these latter variables non-basic variables .2 
2Some texts refer to these variables as right-hand variables. It is not a very good idea to identify some variables 
on the basis of an arbitrary decision of where we are going to position them on the page. I will not therefore follow 
this convention, but the reader should be aware of the terminology when reading other books on the subject. 

A.3 THE SIMPLEX METHOD 
209 
For our problem we start by arbitrarily choosing x3 and x4 as basic variables. With some 
foresight we had already solved for them in Equations (A.26) and (A.27), otherwise we 
would have to do so now. 
As a next step we set to zero the non-basic variables, x1 and x2. When we do so,  we  
ﬁnd from Equations (A.26) and (A.27): 
x3 = 60 
(A.28) 
x4 = 60 
(A.29) 
As both x3 and x4 are greater or equal to zero, the solution we have found is indeed a 
feasible solution (i.e., the constraints are satisﬁed). Since we have chosen the basic variables 
arbitrarily, this will not always be the case. If our choice of basic variables had given a 
negative value once we had solved for either of them, we would have had to choose different 
basic variables to start the procedure. 
We now test for optimality, that is, we check whether the constraint-satisfying solution 
we have found also maximizes f . Let us look at the objective function as a function of the 
non-basic variables: 
f (x1, x2) = 29x1 + 45x2 
(A.30) 
(the basic variables are at zero). Note that the two coefﬁcients are positive. Therefore 
the solution is not optimal because I could increase the value of the objective function by 
increasing both x1 and x2. This is allowed, because the constraints are simply x1 ≥ 0, x2 ≥ 0. 
But this also tells us when we should stop in our search for an optimal solution: we 
are done when the coefﬁcients of the objective function are all negative or zero. Then we 
know that we are at an optimal solution, because we cannot increase the objective function 
by increasing the nonbasic variables (remember that they must be non-negative). We can 
therefore express the following condition for optimality: a basic feasible solution is an 
optimal solution if the coefﬁcients of the non-basic variables in the objective function are 
negative or zero. 
We now move to the second step: ﬁnding a better basic feasible solution. We have to 
try another basic feasible solution, that is, we must go to a point where another variable 
xi is zero. Remember why we are doing this: because, thanks to the fundamental theorem 
above, we know that some optimal solution is also a basic feasible solution. We can, in 
other words, restrict our search to basic solutions. Unfortunately the ﬁrst basic solution we 
tried was not an optimal one. We must perform what is called an exchange of variables. 
We proceed as follows. 
We want to carry out the switch of basic/non-basic variables that will give us the biggest 
‘bang for the buck’, i.e., that will allow us to increase the target function as much as 
possible. We start by choosing a non-basic variable, xNB, that has a positive coefﬁcient. For 
our problem, this could be x1: 
xNB = x1 
(A.31) 
We set the other non-basic variables (in our case x2) at zero. We determine the largest 
increase, xNB, in  xNB that leaves all the previous basic variables (i.e., x3 and x4) non-
negative. We call f the corresponding increase in the objective function, f . So, in our 

210 
APPENDIX 
case we start from 
x3 = 60 − 2x1 − 8x2 
(A.32) 
x4 = 60 − 4x1 − 4x2 
(A.33) 
We set the basic variables (i.e., x3 and x4) to zero to give 
0 = 60 − 2x1 − 8x2 
(A.34) 
0 = 60 − 4x1 − 4x2 
(A.35) 
Since we have chosen  xNB = x1, we set to zero the other non-basic variable: 
0 = 60 − 2x1 
(A.36) 
0 = 60 − 4x1 
(A.37) 
Then, from 0 = 60 − 2x1 we get 
x1 = 30 ===⇒ f = 29x1 = 870 
(A.38) 
So, x1 = 30 is the maximum increase we can give to x1 because of the constraint imposed 
by Equation 0 = 60 − 2x1. 
Similarly, from 0 = 60 − 4x1 we get 
x1 = 15 ===⇒ f = 29x1 = 435 
(A.39) 
We see that the objective function would increase more if we gave to x1 the increment 
x1 = 30, but if we did so, the quantity 60 − 4x1 would become negative. Therefore the 
maximum increment we can give to x1 is x1 = 15. The following equation prevents any 
increase in x1 greater than x1 = 15: 
x4 = 60 − 4x1 − 4x2 
(A.40) 
We therefore underline the variable x1 in Equation (A.40). 
We must do the same for the other non-basic variable with a positive coefﬁcient, that is, 
in our case, with x2. This  gives  
xNB = x2 
(A.41) 
We set the other non-basic variables (now x1) at zero. Then 
x3 = 60 − 2x1 − 8x2 
(A.42) 
x4 = 60 − 4x1 − 4x2 
(A.43) 

A.3 THE SIMPLEX METHOD 
211 
Setting the basic variables and the other non-basic variable to zero now gives 
0 = 60 − 8x2 
(A.44) 
0 = 60 − 4x2 
(A.45) 
Therefore, x2 = 7.5 from Equation (A.44) and x2 = 15 from Equation (A.44). This gives 
in one case 
x2 = 7.5 ===⇒ f = 45x2 = 337.5 
(A.46) 
and in the other 
x2 = 15 ===⇒ f = 45x2 = 675 
(A.47) 
Therefore, much as we would like to be able to increase the objective function all the way 
up to 675, we are ‘blocked’ by 
x3 = 60 − 2x1 − 8x2 
(A.48) 
Consider now the two equations with underlined variables: 
x3 = 60 − 2x1 − 8x2 
f = 337.5 
(A.49) 
x4 = 60 − 4x1 − 4x2 
f = 435 
(A.50) 
We want to exchange the role of one basic and one non-basic variable. Which non-basic 
variable should we choose? The one that gave the greatest increment to f . Since  f = 435 
for Equation (A.50) and f = 337.5 for Equation (A.49), our switch will be between x1 
(which now becomes basic) and x4. Solving for x1 in Equation (A.50) and substituting in 
Equation (A.49) we get 
1 
x1 = 15 − x2 − 4 x4 
(A.51) 
1 
x3 = 30 − 6x2 + 2 x4 
(A.52) 
The same three operations are now performed again with the new set of basic and 
non-basic variables. If the reader is patient enough, she will ﬁnd that the new basic solu-
tion is still non-optimal, because not all the coefﬁcients are negative. But she will also 
observe that now only one coefﬁcient will be positive, and therefore there will be no 
need to make a choice of which variables should be switched between being basic and 
non-basic. 

212 
APPENDIX 
The switch procedure continues until all the coefﬁcients are negative, at which point the 
basic solution will be an optimal basic solution. By the fundamental theorem above, we 
are done. 
Obviously, I do not recommend carrying out this three-step-tango by hand for more 
than three variables. There are readily available pieces of source code that can be 
adapted for the purpose (see, e.g., Press et al. (1992)), but it is always useful to have 
an idea of what is inside the black box. Providing this idea was the purpose behind this 
Appendix. 

References 
Akerlof, G. A. and Shiller, R. J. (2009) Animal Spirits – How Human Psychology Drives the Econ-
omy, and Why It Matters for Global Capitalism, Princeton University Press, Princeton, NJ and 
Oxford. 
Alexander, C. and Sheedy, E. (2008) Model-Based Stress Tests: Linking Stress Tests to VaR for 
Market Risk , MAFC Research Paper No. 33, Macquire University, Applied Finance Centre. 
Allison, G. and Zelikow, P. (1999) Essence of Decision, 2nd edn, Addison Wesley Longman, New 
York. 
Aragones, J. R., Blanco, C. and Dowd, K. (2001) Incorporating Stress Testing Into Market Risk 
Modelling, Insitutional Investor , Spring, 44–49. 
Babcock, L., Loewenstein, G., Issacharoff, S. and Camerer, C. (1995) Biased Judgments of Fairness 
in Bargaining, American Economic Review , 85, 1337–1343. 
Babcock, L., Loewenstein, G. and Issacharoff, S. (1997) Creating Convergence: Debiasing Biased 
Litigants, Law and Social Inquiry , 22, 913–925. 
Berkowitz, J. (1999) A Coherent Framework for Stress-Testing, Journal of Risk , 2(2), 5–15. 
BIS (Bank of International Settlements) (2009) Principles for Sound Stress Testing Practices and 
Supervision, consultative paper, January 2009 (latest version, May 2009). 
Bookstaber, R. (2008) A Demon of Our Own Design, John Wiley & Sons, Ltd, Chichester, UK. 
Brighton, H. and Gigerenzer, G. (2008) Bayesian Brains and Cognitive Mechanisms: Harmony or 
Dissonance? in The Probabilistic Mind – Prospects for Bayesian Cognitive Science, N. Chater  
and M. Oaksford (eds), Oxford Univeristy Press, Oxford, UK, Chapter 9. 
Casebeer, W. D. (2008) The Stories Markets Tell – Affordances for Ethical Behaviour In Free 
Exchange, in  Moral Markets – The Critical Role of Values in the Economy ,  P. J. Zak  (ed.)  
Princeton University Press, Princeton, NJ and Oxford, Chapter 1. 
Chamley, C. P. (2004) Rational Herds: Economic Models of Social Learning, Cambridge University 
Press, Cambridge, UK. 
Chater, N. and Oaksford, M. (eds) (2008) The Probabilistic Mind – Prospects for Bayesian Cognitive 
Science, Oxford Univeristy Press, Oxford. 
Cohen, W. D. (2009) House of Cards – How Wall Street Gamblers Broke Capitalism, Allen Lane, 
London. 
Davidson, P. (2009) John Maynard Keynes , Palgrave Macmillan, Houndmills, Hampshire, UK. 
Davis, J. K. and Sweeney, M. J. (1999) Strategic Paradigms 2025: US Security Planning for a New 
Era, Institute for Foreign Policy Analysis, Cambridge, MA. 
Dumas, B. (2008) Endorsement on back cover of Shefrin (2008). 
Evans, G. W. and Honkapohja, S. (2005) An Interview with Thomas G Sargent, Macroeconomic 
Dynamics , 9, 561–583. 
Fauconnier, G. and Turner, M. (2002) The Way We Think: Conceptual Blending and the Mind’s 
Hidden Complexities, Basic Books, New York. 
213 

214 
REFERENCES 
Friedman, M. and Jacobson Schartz, A. (1963 [2008]) The Great Contraction, 1929–1933 , Prince-
ton University Press, Princeton, New Jersey. 
Frydman, R. and Goldberg M. D. (2007) Imperfect Knowledge Economics: Exchange Rates and 
Risk , Princeton University Press, Princeton, NJ. 
Frydman, R. and Goldberg, M. D. (2009) Financial Markets and the State: Long Swings, Risk 
and the Scope of Regulation, Capitalism and Society, 4(2), available at: http://www.bepress. 
com/cas/vol4/iss2/art2, accessed 20 November 2009, Berkeley Electronic Press. 
Gigerenzer, G. and Hoffrage, U. (1995) How to Improve Bayesian Reasoning Without Instructions. 
Frequency Formats, Psychological Review , 102, 684–704. 
Gigerenzer, G. and Selten, R. (2002) Rethinking Rationality , in  Bounded Rationality – The Adaptive 
Toolbox , G. Gigerenzer and R, Selten (eds), MIT Press, Cambridge, MA. 
Gigerenzer, G. and Selten, R. (eds) (2002) Bounded Rationality – The Adaptive Toolbox , MIT Press, 
Cambridge, MA. 
Good, I. J. (1973) The probabilistic explication of evidence, surprise, causality, explanation, and 
utility, in Foundations of Statistical Inference, V. P. Godambe and D. A. Sprott (eds), Holt, 
Rinehart and Winston, Toronto. 
Granger, C. W. J. (1969) Investigating Causal Relations by Econometric Models and Cross-Spectral 
Methods, Econometrica, 37(3), 424–438. 
Greenspan, A. (2008) Congressional Testimony, 23 October 2008. 
Greenspan, A. (2009) We Need a Better Cushion Against Risk, Financial Times, p. 11, 27 March. 
Greer, W. N. (2000) Ethics and Uncertainty, Elgar, Cheltenham, UK. 
Grether, D. (1980) Bayes Rule as a Descriptive Model: The Representativeness Heuristic, Quartertly 
Journal of Economics, 95, 537–557. 
Grifﬁths, T. L. and Tenenbaum, J. B. (2005) Optimal Predictions in Everyday Cognition, Psycho-
logical Science, 17(9), 767–773. 
Guarino, A. and Cipriani, M. (2008) Herd Behaviour in Financial Markets: An Experiment with 
Financial Market Professionals. IMF Working Paper 08/141, International Monetary Fund. 
Gintis, H. (2009) The Bounds of Reason: Game Theory and the Uniﬁcation of the Behavioral Sci-
ences, Princeton University Press, Princeton, NY. 
Hirtle, B., Schuermann, T. and Stiroh, K. (2009) Macroprudential Supervision of Financial Institu-
tions: Lessons from the SCAP, Federal Reserve Bank of New York Staff Reports, Staff Report 
no. 409, November 2009, available at http://newyorkfed.org/research/staff_reports/sr409.html 
(accessed 5 February 2009). 
Hacker, S. R. and Hatemi-J, A. (2006) Tests for Causality between Integrated Variables Using 
Asymptotic and Bootstrap Distributions: Theory and Applications, Applied Economics , 38(13), 
1489–1500. 
Hacking, I. (2001) An Introduction to Inductive Logic and Probability, Cambridge University Press, 
Cambridge, UK. 
Jaynes, E. T. (2003) Probability Theory: The Logic of Science, Cambridge University Press, Cam-
bridge, UK. 
Jolls, C. and Sunstein, C. (2005) Debiasing Through Law , University of Chicago, John M Olin 
School of Law and Economics Working Paper No. 225, second series. 
Kahneman, D., Slovic, P. and Tversky, A. (eds) (1982) Judgement under Uncertainty: Heuristics 
and Biases , Cambridge University Press, Cambridge, UK. 
Kahneman, D. and Tversky, A. (1972a) On Prediction and Judgement, ORI Research Monograph, 
12(4). 
Kahneman, D. and Tversky, A. (1972b) Subjective Probability: A Judgement of Representativeness, 
Cognitive Psychology, 3, 430–454. 
Kahneman, D. and Tversky, A. (1973) On the Psychology of Prediction, Psychological Review , 
80, 237–251. 

215 
REFERENCES 
Kahneman, D. and Tversky, A. (1979a) Intuitive Prediction: Biases and Corrective Procedures, 
TIMS Studies in Management Science, 12, 313–327. 
Kahneman, D. and Tversky, A. (1979b) Prospect Theory: An Analysis of Decision Under Risk, 
Econometrica, 47, 263–291. 
Kahneman, D. and Tversky, A. (eds) (2000) Choices, Values, and Frames, Cambridge University 
Press, Cambridge, UK. 
Knill, D. C. and Pouget, A. (2004) The Bayesian Brain: The Role of Uncertainty in Neural Coding 
and Computation, Trends in Neuroscience, 27(12), 712–719. 
Knight, F. N. (1921) Risk, Uncertainty and Proﬁt , Houghton Mifﬂin, New York. 
Kreiszig, E. (1993) Advanced Engineering Mathematics, 7th edn, John Wiley and Sons, Ltd, Chich-
ester, UK. 
Kwiatkowski, J. and Rebonato, R. (2010). A Coherent Aggregation Framework for Stress Testing 
and Scenario Analysis, submitted to Applied Mathematical Finance. 
Lakoff, G. and Johnson, M. (1980) Metaphors We Live By, University of Chicago Press, Chicago, 
IL. 
Lucas, R. E. (1977) Understanding Business Cycles, in Stabilization of the Domestic and Interna-
tional Economy , K. Brunner and A. H. Meltzer (eds), Carnegie Mellon Conference on Public 
Policy, 5, North-Holland, Amsterdam. 
Luce, D. R. and Raiffa, H. (1957) Games and Decisions , Dover Publications, New York. 
Malvergne, Y. and Sornette, D. (2006) Extreme Financial Risk: From Dependence to Risk Manage-
ment , Springer Verlag, New York. 
Martignon, L. (2002) Comparing Fast and Frugal Heuristics and Optimal Models, in Bounded 
Rationality – The Adaptive Toolbox , G. Gigerenzer and R. Selten (eds), MIT Press, Cambridge, 
MA. 
Matten, C. (2000) Managing Bank Capital: Capital Allocation and Performance Measurement , 2nd 
edn, John Wiley and Sons, Ltd, Chichester, UK. 
McKenzie, K. (2006) An Engine, not a Camera, MIT Press, Cambridge, MA. 
McNeil, A., Frey, R. and Embrechts, P. (2005) Quantitative Risk Management , Princeton University 
Press, Princeton, NJ. 
Miller, J. H. and Page, S. E. (2007) Complex Adaptive Systems – An Introduction to Computational 
Models of Social Life, Princeton University Press, Princeton, NJ. 
Moore A. (2001) Bayes Nets for Representing and Reasoning About Uncertainty , Carnegie Mellon 
University, http://www.autonlab.org/tutorials/bayesnet09.pdf, last accessed 18 August 2009. 
Moskowitz, H. and Sarin, R. K. (1983) Improving the Consistency of Conditional Probability 
Assessment for Forecasting and Decision Making, Management Science, 29(6), 735–749. 
Muth, J. F. (1961) Rational Expectations and the Theory of Price Movements, Econometrica, 29, 
315–335, quoted in Frydman, R. and Goldberg, M. D. (2007) Imperfect Knowledge Economics: 
Exchange Rates and Risk , Princeton University Press, Princeton, NJ. 
Neapolitan, R. E. (2003) Learning Bayesian Networks, Prentice Hall, Upper Saddle River, NJ. 
Nelsen, R. B. (1999) An Introduction to Copulas, Lecture Notes in Statistics, Springer Verlag, New 
York. 
Nisbett, R. (2009) Intelligence and How to Get It – Why Schools and Cultures Count , W.W. Norton, 
New York and London. 
Ozdenoren, E. and Yuan, K. (2008), Feedback Effects and Asset Prices, Journal of Finance, 
LXIII(4), 1939–1975. 
Pearl, J, (2009), Causality , 2nd edn, Cambridge University Press, Cambridge, UK. 
Poirier, D. (1995), Intermediate Statistics and Econometrics, MIT Press, Cambridge, MA. 
Press, W. H., Teukolski, S. A., Vetterling, W. T. and Flannery, B. P. (1992) Numerical Recipes in 
Fortran 77 – The Art of Scientiﬁc Computing , 2nd edn, Cambridge University Press, Cambridge, 
UK. 

216 
REFERENCES 
Rebonato, R. (2007) The Plight of the Fortune Tellers – Why We Must Manage Financial Risk 
Differently , Princeton University Press, Princeton, NJ. 
Rebonato, R. (2009) What Models Do We Need for Risk Management?, in QFinance – The Ultimate 
Resource, Bloomsbury, London, 228–231. 
Rebonato, R. (2010) Post-Crisis Financial Risk Management: Some Suggestions, Journal of Risk 
Management in Financial Institutions, 3(2), 1–8. 
Robertson, D. H. (1936) Some Notes on Mr. Keynes’ General Theory of Employment, The Quar-
terly Journal of Economics , 51(1), 168–191. 
Sargent, T. J. (2005), interviewed in Evans and Honkapohja (2005). 
Schwartz, B. (2004) The Paradox of Choice, Harper Collins, New York. 
Schuermann, T. (2009) Personal Communication. 
Selten, R. (1991) Evolution, Learning and Economic Behaviour, Games and Economic Behaviour , 
3, 3–24. 
Selten, R. (1998) Features of Experimentally Observed Bounded Rationality, European Economic 
Review , 42, 413–436. 
Shamrakov, L. (2006) Joint Dynamics of Credit Spreads and Equity Prices , MSc Thesis, Oxford 
University. 
Shefrin, H. (2000) Beyond Greed and Fear – Understanding Behvioural Finance and the Psychology 
of Investing , Harvard Business School Press, Harvard, MA. 
Shefrin, H. (2008) A Behavioural Approach to Asset Pricing , 2nd edn, Academic Press, Oxford, 
UK. 
Shelling, T. (1978) Micromotives and Macrobehaviour , Norton, New York and London. 
Shleifer A. (2000) Inefﬁcient Markets – An Introduction to Behavioural Finance, Clarendon Lec-
tures in Economics, Oxford University Press, Oxford, UK. 
Simon, H. A. (1956) Rational Choice and the Structure of Environments, Psychological Review , 
63, 129–138. 
Simon, H. A. (1957) Models of Man, John Wiley & Sons Inc., New York. 
Skidelsky, R. (2009) Keynes – The Return of the Master , Public Affairs, Perseus Books, New York. 
Sorge, M. (2004) Stress-Testing Financial Systems: An Overview of Current Methodologies, Mon-
etary and Economics Department, BIS Working Paper No. 165, December 2004, 1–41. 
Soros, G. (1987) The Alchemy of Finance, John Wiley & Sons Inc., New York. 
Soros, G. (2008) The New Paradigm for Financial Markets: The Credit Crisis of 2008 and What It 
Means , Public Affairs, New York. 
Stewart, N. (2008) quoted in The Economist , 13 December 2008, ‘Credit Cards – A Nudge in the 
Wrong Direction’, 88. 
Stigler, G. J. (1961) The Economics of Information, Journal of Political Economy , 69, 191–214. 
Stirzacker D. (1999) Probability and Random Variables – A Beginner’s Guide, Cambridge Univer-
sity Press, Cambridge, UK. 
Tversky, A. and Kahneman, D. (1979) Causal Schemata in Judgments under Uncertainty, in 
Progress in Social Psychology, M. Fishbein (ed.), Lawrence Erlbaum Associates, Hillsdale, 
Chapter 8. 
Tversky, A. and Kahneman, D. (1982) Evidential Impact of Base Rates, in Judgement under 
Uncertainty: Heuristics and Biases , D. Kahneman, P. Slovic, and A. Tversky (eds), Cambridge 
University Press, Cambridge, UK, Chapter 10. 
Tzani, R. and Polychronakos, A. P. (2008) Correlation Breakdown, Copula Credit Models and 
Arbitrage, GARP Risk Review , December, 27–37. 
Williams, D. (2001) Weighing the Odds – A Course in Probability and Statistics, Cambridge Uni-
versity Press, Cambridge, UK. 
Williamson, J. (2005) Bayesian Nets and Causality – Philosophical and Computational Founda-
tions , Oxford University Press, Oxford, UK. 

Index 
ABSs 118, 176 
acyclical directed graphs 
see also Bayesian networks 
concepts 49, 99–115 
terminology 99–101 
adaptiveness concepts, bounded rationality 
176–8, 184–5 
adjustable-rate mortgages (ARMs) 188 
advanced quantitative foundations 5–6, 47, 
71–91 
agents 9–10, 18, 21–2, 23–30, 173–4, 185 
see also asset managers; banks; pension fund 
trustees; traders 
coordination thesis 18, 25–7, 29–30 
principal/agent relationships 24–6, 30 
traders 18, 23–7 
types 26 
Akerlof, G.A. 188 
algorithms, Bayesian networks 111–15 
Allison, G. 20, 23, 190 
Amaranth 199 
ancestors, graph concepts 99–115, 158–70, 
194–6 
anchoring bias 
concepts 174–5, 183–4 
deﬁnition 183–4 
appendix 205–12 
applications 1–5, 9, 27–30, 39, 41, 49, 53, 57, 
90, 94, 98, 99, 101, 113–15, 131–70 
Aragones, J.R. 1, 2, 4–5, 192 
arbitrage 24, 30 
arcs/edges, graph concepts 99–115, 155–70, 
194–6 
Aristotle 42 
ARMs see adjustable-rate mortgages 
arrows 
see also Bayesian networks 
graph concepts 99–115, 155–70, 194–6 
asset classes 118–29, 193–4 
asset managers 26, 30 
association 
causation 63–5 
concepts 5, 37, 39–43, 63–5, 93–8, 100–15, 
118–29, 151–3, 180–1, 187–96 
critique 39–40 
audits, stress tests 197, 199, 201, 203 
Australian Central Bank 195 
axioms, concepts 51–3 
‘backwards’ contexts 40–2 
banking-book portfolios 190 
banks 10–11, 16–17, 24–6, 33–4, 49, 53–4, 
56–7, 58–9, 121–3, 135, 150–3, 156–70, 
179–81, 189–96, 201 
dimensionality problems 192–4 
failures 24–6, 33–4, 49, 53–4, 56–7, 58–9, 
121–3, 156–70, 179–81, 191, 194–6 
regulatory capital 10–11, 16–17, 24, 26, 135, 
150–3, 167–70, 189–90, 196, 201 
typical activities 192–3 
base-rate bias see representativeness bias 
basic variables, linear programming concepts 
114–15, 140, 148, 207–12 
Bayes’ Theorem 4, 22, 32–43, 57–62, 63–9, 71, 
83–91, 101–15, 126–9, 158–70, 173–85, 
200 
see also conditional probabilities; joint 
probabilities; subjective probabilities 
concepts 57–62, 63–9, 83, 101–15, 126–9, 
158–9, 163–7, 200 
deﬁnition 57–8, 63 
Bayesian networks 
see also conditional probability tables; 
two-valued Boolean variables 
algorithms 111–15 
bottom-up approaches to stress scenarios 
188–96 
breaking-down-the-joint rule 107–11, 112–15 
closure rule 108–11, 161–5 
coherent solutions 155–70 
commutativity rule 108–11, 161–70 
concepts 4, 5, 6, 39, 41, 49–50, 53, 57, 69, 
73–91, 94, 98, 99–115, 117–29, 134–5, 
155–70, 182, 188–96, 204 
217 

218 
INDEX 
Bayesian networks (continued ) 
construction methods 101–6, 117–29, 155–64, 
193–6 
deﬁnition 39, 49, 99–104 
examples 109–11 
generalized results 164–5 
independence 102–15, 123–9, 156–70 
order-of-conditioning rule 108–11, 160–70 
partial ordering of variables 104–9, 164–70 
path-dependencies 103 
questions to consider 101 
splitting-of-the-marginal rule 109–11 
structural issues 100–4 
‘structural pruning’ 155–6, 192–4, 200, 204 
systematic approach 111–13, 129 
terminology 99–101 
top-down approaches to stress scenarios 
188–96 
useful relationships 107–9 
uses 39, 41, 49, 53, 57, 90, 94, 98, 99, 101, 
113–15, 155–70, 188–96 
Bear Sterns 24, 26 
beauty-parade-of-models metaphor 26, 27–8 
behavioural ﬁnance 
see also cognitive biases 
concepts 26–7, 29–30, 173–85 
Berkowitz, J. 3, 9, 39, 192, 204 
Bernanke 121–2 
biases see cognitive biases 
bid-offer spreads 33–4 
bipolar liquidity 33–4 
Blanco,  C. 1, 2, 4–5,  192  
bona ﬁde internally consistent conditional 
probabilities 133–7, 141, 146 
bonds 37–8, 188, 192–4 
see also ﬁxed-income securities 
Boolean variables, concepts 47–69, 71–91, 
101–15, 118–29, 136, 151, 156–70, 
189–90, 193–4 
bottom-up approaches to stress scenarios 
banking activities 192–4 
Bayesian networks 188–96 
concepts 27–8, 187–96 
critique 187–90 
deﬁnition 187 
dimensionality problems 192–4 
top-down uses 194–6 
bounded rationality 
see also cognitive biases; rationality 
adaptiveness concepts 176–8, 184–5 
concepts 173–8, 184–5 
decision-making 174, 184–5 
deﬁnition 173–4 
fast-and-frugal adaptive toolbox 176–8, 
184–5 
schools of thought 174 
scissors metaphor 175–6 
social context 174–6 
bounds 
concepts 83, 93–8, 134–53, 155–6, 165 
deﬁnition 93–6 
stress tests 96–8, 134–53, 155–6, 165 
tightness considerations 96–8 
breaking-down-the-joint rule, Bayesian networks 
107–11, 112–15 
Brighton, H. 175, 185 
Brownian motion 86–91 
bubbles 26–7 
buckets, conditional probabilities 119–25, 
141–51 
butterﬂies 188 
buy-and-hold conditions 190 
C++ 144 
capital 
see also regulatory capital 
adequacy requirements 189–90, 196 
RAROC 168–70 
regulatory capital 10–12, 16–17, 24, 26, 135, 
150–3, 167–70, 189–90, 196, 201 
carry trades 199 
Casebeer, W.D. 29 
causal/diagnostic biases, concepts 101, 159, 
163–70, 182–5 
causation 
association 63–5 
complexity problems 101–2, 200–1, 203–4 
concepts 3–5, 18, 37, 49–69, 90, 93, 100–15, 
117–29, 133–4, 153, 155–70, 177–82, 
188–96, 204 
conditioning contrasts 64–5 
deﬁnition 100–1 
time ordering 56–69 
unravelling contemplations 113–15 
CDOs 33–4, 176 
CDSs 33–4, 191 
central limit theorem 26–7 
centrality-of-models thesis, concepts 17–23 
CES see conditional expected shortfall 
challenges, non-specialist challenges 197, 198 
chartism 20–1 
Chater, N. 185 
children, graph concepts 99–115, 158–70, 194–6 
China 63–5, 121, 180–3, 187, 189, 190, 194–6 
‘cleansed’ matrices 
see also sanity checks 
concepts 135–6, 144–53, 155–6 
closure rule, Bayesian networks 108–11, 161–5 
CMBSs see commercial mortgage-backed 
securities 
co-dependencies 
concepts 3–4, 36–43, 48–9, 114–15 

INDEX 
219 
critique 36 
structural models 38–9 
cognitive biases 
see also bounded rationality; 
representativeness. . . 
anchoring bias 174–5, 183–4 
causal/diagnostic biases 101, 159, 163–70, 
182–5 
concepts 11, 22–3, 36, 61, 100–1, 124–5, 
159, 173–85 
frequentist probability 177–84 
information 22–3, 177–8, 183–4 
multi-viewpoint perspectives 177–8 
positive/negative biases 182–4 
cognitive exercises 177–8 
cognitive shortcomings 
bounded rationality 174–8, 185 
self-fulﬁlling prophecy 185 
coherent conditional probability matrices 180–1 
coherent solutions 133–70, 190–1 
Bayesian networks 155–70 
linear programming 133–53, 165 
coin-tossing experiments 31–3, 36, 39 
commercial mortgage-backed securities (CMBSs) 
63–5, 180–3, 194–6 
commodities, banking activities 192–4 
commutativity rule, Bayesian networks 108–11, 
161–70 
competing models 17, 21–30, 35 
completeness checks, stress tests 197, 198–9 
complex adaptive systems 26–7, 30 
complexity problems, stress tests 101–2, 200–1, 
203–4 
concentration graphs 68–9 
conditional expected shortfall (CES), concepts 
135, 168–70, 203 
conditional independence 
see also Bayesian networks 
concepts 50, 83, 86–91, 99–115, 156–70, 201 
d-separation 115 
joint probabilities 88–91, 156–70 
conditional probabilities 
see also Bayes’ Theorem; doubly. . .; singly. . .; 
Venn diagrams 
buckets 119–25, 141–51 
causal links 100–1, 113–15, 117–29, 133–4, 
153, 155–70, 188–96, 204 
checks 117–18, 125–9, 133–53, 155–6, 
198–9 
cognitive biases 173–85 
coherent solutions 135–53 
concepts 3–6, 12, 40–1, 42–3, 47–69, 71–91, 
93–8, 99–115, 117–29, 133–53, 155–70, 
184, 192–4 
dangerous-running example 59–61 
deﬁnition 49, 55–6, 76, 89–90 
examples 59–62 
joint probabilities 49–50, 65–7, 71–2, 73–4, 
76–7, 83–91, 93–8, 105–15, 117–29, 
134–53, 155–70, 184 
marginal probabilities 62–9, 71–91, 93–8, 
117–29, 133–53, 156–70 
multiplicative factors 62–9, 120–2, 151–3, 
163–70 
predictions 3–6, 177–82 
rare-and-even-more-dangerous-diseases 
example 61–2 
conditional probability maps, concepts 67–9 
conditional probability tables 
see also Bayesian networks 
automatic (linear programming) checks 118, 
129, 133–53, 155–6, 164, 165–70, 200 
checks 117–18, 125–9, 133–53, 155–6, 
191–2, 198–9 
concepts 40–1, 67–9, 74–5, 94–8, 99–115, 
117–29, 133–53, 155–70, 180–1, 184, 
198 
construction methods 101–7, 117–29, 135–6, 
144–53, 155–64, 193–6, 198 
deterministic causal relationships 124–9 
event deﬁnitions 120–2 
events of different probabilities 122–3 
incompatibility 123–9 
independence 123–9 
manual checks 117–18, 125–9, 133–4, 
135–6, 149, 155–6, 163–4, 200 
marginal probabilities 117–22, 133–53, 
155–70 
necessary conditions 125–6 
populating considerations 101, 106–7, 117–29, 
135–6, 144–53, 155–64, 192–4, 200 
practical uses 149–53, 167–70 
reasonable starting point in construction 
123–9 
systematic approach 129, 133–53, 155–70 
triplet conditions 126–7, 192–3, 200 
conditioning concepts 64–5, 71–91, 95–8, 164 
conﬂicts of interest 9–10 
consistent doubly-conditioned probabilities, 
concepts 136–53 
consistent marginal probabilities, concepts 
136–53 
constrained optimization problems 
see also linear programming 
concepts 133–53, 205–12 
constraint triplets, concepts 126–7, 192–3, 200 
control variables, linear programming concepts 
205–12 
coordination thesis 
see also herding behaviour 
concepts 15, 18, 23–7, 29–30 
copulas 37–8, 42–3, 198, 203 

220 
INDEX 
correlations 
concepts 3–4, 36–43, 48–9, 57–69, 77–91, 
114–15, 123–9 
critique 36–7, 48 
counterparty risk 49, 53–4, 63–5, 121–3, 
156–70, 179–81, 194–6 
CPU times 144, 167, 192, 201 
credit cards, minimum repayment amounts 
183–4 
credit derivatives 33–4, 191 
credit spreads 35–6, 38, 56–7, 63–5, 105–6, 
122–3, 125–9, 156–70, 179–81, 193–6, 
198–9 
credit trading, banking activities 192–4 
credit-rating agencies 121 
cultural issues 9–10 
see also institutional environments 
revenue streams 4 
doubly-conditioned probabilities 
concepts 49, 65–6, 72–91, 93–8, 108–11, 
134–9, 155–6, 165 
deﬁnition 49, 72 
Dowd, K. 1, 2, 4–5, 192 
due diligence 191–2, 199 
Dumas, Bernard 22 
ease-of-use requirements, governance 197–8 
economic agents, cognitive biases 174, 185 
economic capital, concepts 1, 10, 12, 16–17, 33, 
135, 168–70 
economic value, cognitive biases 185 
economics 5, 10, 11, 12, 13, 16, 19–20, 187–96 
efﬁcient-market hypothesis, critique 21–2, 173–4 
elementary quantitative foundations 5–6, 47–69, 
168–70 
elliptical distributions 38, 48, 87 
d-separation, concepts 115 
dangerous-running example, conditional 
probabilities 59–61 
data-driven views see frequentist probability 
emerging markets 25 
emotions 29, 188 
equities 
indices 49, 87–91, 104, 119, 143–4 
Davis, J.K. 190 
market crashes 1–2, 12, 16, 33–7, 104–6, 
de-pegging 203 
125–9, 141–53, 156–70, 194–6, 198, 199 
decaying weights 202–3 
market rallies 143–4 
decision-making processes, concepts 10–13, 
prices 3, 24–5, 27–8, 30, 37, 38–9, 49, 
20–30, 174–8, 184–5 
86–91, 104–6, 118–29, 141–53, 156–70,
default risk 33–5, 49, 53–4, 63–5, 121–3, 
187–96 
156–70, 179–81, 190–1, 194–6 
Europe, strategic paradigms 190–1 
deﬂation 189 
event correlations, concepts 77–91 
degrees of belief 22–30, 48, 58–69 
events 
see also Bayes. . .  
see also Boolean variables 
derivatives 3, 33–4, 35–6, 87–91, 104–6, 
Bayesian networks 101–15, 156–70, 188–96 
156–66, 191–2, 199 
deﬁnitions 120–2 
see also ﬁnancial innovations; futures; options; 
extreme events 3–6, 12, 19–30, 34–43, 
swaps 
48–69, 86–91, 104–6, 118–29, 135–53, 
descendants, graph concepts 99–115, 158–70, 
194–6 
deterministic causal relationships 
concepts 53–5, 63–9, 124–9 
conditional probability tables 124–9 
diagnostic/causal biases, concepts 101, 159, 
163–70, 182–5 
dimensionality problems 
see also populating considerations 
concepts 192–4, 200 
Dirac distributions 193–4 
directed edges, graph concepts 99–115 
discursive narratives 28, 42 
diseases, rare-and-even-more-dangerous-diseases 
example 61–2 
distributions 18–30, 33–4, 37–8, 193–4, 198, 
203 
see also tail. . . 
diversiﬁcation 4, 16–17, 68–9, 141 
charts 68–9 
156–70, 187–96, 198–204 
multiple events 49, 71–91, 95–8, 156–70 
selection/combination of stress scenarios 
187–96, 198–9 
Excel 50 
expectation operator, joint probability concepts 
78–91 
expected losses 
concepts 135, 141–53, 167–70, 191–4, 
199–204 
regulatory capital 150–3, 167–70, 196 
uncertainty 151–3, 168–70 
expected returns, concepts 2–6, 135, 168–70 
expected utility maximization theory 22–3 
experts 
see also specialists 
judgement-based stress testing 28–30, 177–8, 
189, 201–4 
predictions 21–2, 27–9, 35–6, 90, 93, 100–1, 
177–8, 182–3, 201–4 

INDEX 
221 
exponential distributions 18–19, 21 
external information 49–69 
extreme events 3–6, 12, 19–30, 34–43, 48–69, 
86–91, 104–6, 118–29, 135–53, 156–70, 
187–96, 198–204 
extreme value theory (EVT) 3, 18–19, 42–3, 
203, 205 
see also risk management 
failing banks 24–6, 33–4, 49, 53–4, 56–7, 
58–9, 121–3, 156–70, 179–81, 191, 194–6 
fast-and-frugal adaptive toolbox, concepts 176–8, 
184–5 
feasible solutions, linear programming concepts 
114–15, 140, 148, 207–12 
the Fed 24, 54, 189, 196 
ﬁnancial crisis from 2007 1–2, 12, 16, 33–7, 188 
ﬁnancial innovations 33–4, 87, 191–2 
see also derivatives 
ﬁnancial markets 1–2, 12, 15, 16, 18, 25–7, 
33–7, 63–5, 104–6, 125–9, 141–53, 
156–70, 176, 187–96 
cognitive biases 176 
crashes 1–2, 12, 16, 33–7, 104–6, 125–9, 
141–53, 156–70, 194–6, 198, 199 
ﬁnancial crisis from 2007 1–2, 12, 16, 33–7, 
188 
positive feedback mechanisms 15, 18, 25–7 
ﬁxed probabilities, conditional probability tables 
118–20 
ﬁxed-income securities 
see also bonds; yield curves 
banking activities 192–4 
foreign policy analysis 190–1 
‘fractions of the possible worlds’ 50–69 
frequentist probability 3–6, 10, 12, 31–43, 48–9, 
57, 62, 119–29, 135, 177–84, 189, 192, 
199, 201–4 
see also Value at Risk 
cognitive biases 177–84 
critique 3–5, 12, 32–3, 41–2, 189, 201–4 
deﬁnition 31–2 
examples 31–2 
God 21, 23, 155–6 
Goldberg, M.D. 21, 22, 30 
Good, I.J. 202 
governance 9, 28, 149–51, 167–70, 190–3, 
197–204 
see also institutional environments 
aspects 197 
audits 197, 199, 201, 203 
completeness checks 197, 198–9 
concepts 6, 197–204 
ease-of-use requirements 197–8 
input/output sensitivities 197, 200–3 
non-specialist challenges 197, 198 
specialist interactions 197, 198, 199–201 
transparency 75–6, 98, 150, 197–8, 201, 
203 
graphs 
concepts 68–9, 99–115 
deﬁnition 99 
The Great Depression of the 1930s 16, 25 
greedy algorithms 115 
Greenspan, Alan 16–17 
Grifﬁths, T.L. 177, 184 
Hacking, I. 91 
hedging 26, 33–4, 141 
herding behaviour 26–7, 29–30 
see also coordination. . . 
heterogeneity of beliefs, concepts 22–30, 58–9 
heuristics 152–3, 174–85 
histograms 150 
historical losses 191–2, 199, 202–3 
holding periods 118–29, 189–90 
Homer 10 
hot-hand fallacy bias 174 
house prices 189 
see also mortgages 
hypothetical losses, completeness checks 198–9 
idiosyncratic shocks 87–91 
imperfect heuristics, cognitive biases 174–85 
imperfect-knowledge economics (IKE), concepts 
Frydman, R. 21, 22, 30 
futures 199 
FX 
banking activities 192–4 
de-pegging 203 
gambler’s fallacy 174 
game theory 10–11, 26 
gamma 120, 141 
GARCH model 35 
garden pavements 39–42, 100, 135 
Gaussian distributions 18–20, 27, 203 
Gigerenzer, G. 175, 185 
20–30 
implied volatilities 
concepts 3, 35–6, 104–6, 156–66 
uses 35–6, 104–6, 156–66 
incompatibility 52–3, 63–9, 123–9 
independence 
Bayesian networks 102–15, 123–9, 156–70 
concepts 50, 58–69, 83–91, 123–9, 156–70, 
201 
conditional probability tables 123–9 
deﬁnition 58–9, 84–5 
joint probabilities 83–91, 156–70 
indicator function, joint probability concepts 
78–91 

222 
INDEX 
indispensable interpretative tools, models 5, 17, 
18–21, 23, 42, 71 
inequality constraints, linear programming 
concepts 137–53, 205–12 
inﬂation 189, 190–1, 196 
information 22–3, 25–7, 33–4, 50, 149–53, 
167–70, 177–8, 183–4 
cognitive biases 22–3, 177–8, 183–4 
coordination thesis 25–7 
input/output sensitivities, governance 197, 200–3 
institutional environments 6, 9–10, 12, 27–9, 
167–70, 190–2, 197–204 
see also governance 
insurance 21 
interest rates 48–9, 90, 118, 121–2, 141–4, 
uses 167–70, 196 
joint probability tables 74–5, 78–9, 83, 85–6, 
94, 97, 117–29, 136–53, 160–70, 184, 185 
JP Morgan 24 
Kahneman, D. 174, 178, 182–3, 185 
Kennedy, President J.F. 31 
key risk indicators, critique 28 
Keynes, John Maynard 5, 10, 11, 12, 13, 16, 26, 
27, 35, 38, 135, 189 
Knight, F.N. 10–11, 12, 35 
knowledge 20–30, 71, 134–53, 202–4 
Kreiszig, E. 205–6 
Kwiatkowski, J. 136 
188–9, 193–6 
learning from mistakes 16, 36–7 
internally consistent conditional probabilities 
Lehman-style bank failure 33, 163, 180, 202 
133–7, 141, 146 
Levy distributions 20–1, 37 
interpretative tools, models 5, 17, 18–21, 23, 
LIBOR 193 
42, 71 
likelihood functions, concepts 3–5 
intersections, joint probability concepts 72–91 
linear inequality constraints, concept 137–53,
intuitive techniques 3–5, 28, 62, 73–91, 98, 
205–12 
114–15, 123–9, 133, 144, 162–70, 187–8, 
linear programming 
193–4, 199 
see also subjective probabilities 
Iran, Israel 188 
irrationality school 174 
Israel, Iran 188 
Italy 121 
Japan 188 
Jaynes, E.T. 32–3, 42, 58 
joint distribution for the n Boolean random 
variables, deﬁnition 74–5 
joint probabilities 5–6, 12, 38, 41–3, 47–8, 
49–50, 65–9, 71–91, 93–8, 100–15, 
117–29, 134–53, 155–70, 180–1, 184, 185, 
187, 196 
see also Bayesian networks 
‘automatic’ build prescription 165–70 
bottom-up/top-down approaches to stress 
scenarios 196 
bounds 83, 93–8, 153, 165 
conditional independence 88–91, 156–70 
conditional probabilities 49–50, 65–7, 71–2, 
73–4, 76–7, 83–91, 93–8, 105–15, 
117–29, 134–53, 155–70, 184 
deﬁnition 73–6, 89–90 
estimates 114–15, 155–70 
event correlations 77–91, 123–9 
goals 47–8, 49, 93, 99, 155–70, 196 
independence 83–91, 156–70 
marginal probabilities 76–7, 83–91, 93–8, 
117–29, 156–70 
notation 73–6, 160–1 
RAROC 168–70 
see also risk management 
basic variables 114–15, 140, 148, 207–12 
coherent solutions 133–53, 165 
concepts 4, 6, 96–8, 114–15, 118, 129, 
133–53, 165, 193–4, 204, 205–12 
conditional programming tables 118, 129, 
133–53 
deﬁnition 205–8 
feasible solutions 114–15, 140, 148, 207–12 
illustrations of the coherent solution technique 
141–9 
inequality constraints 137–53, 205–12 
non-basic variables 140, 148, 208–12 
objective function 140, 148, 205–12 
simplex method 136, 140–8, 208–12 
slack variables 139, 147–9, 206–12 
source code 144, 167, 212 
theorem 207–8 
lines 
see also Bayesian networks 
graph concepts 99–115, 194–6 
liquidation periods 119–20 
liquidity 33–4, 37–8, 119–20 
loadings, price returns 37–43 
log-likelihood function 181–2 
loss estimates 
see also expected losses; stress tests 
concepts 2–6, 118–29, 134, 135, 141–53, 
167–70, 193–6 
historical losses 191–2, 199, 202–3 
magnitude of associated losses 118–29, 134, 
196, 200–1 
‘lotteries’ 170 

INDEX 
223 
LTCM 199 
micro-structural models 15, 17, 19–30, 189–91 
Luce, D.R. 10–11, 13 
plurality-of-models view 17, 21–30 
lung cancer 53 
reality 5, 15–30, 39–43, 100–15, 144–53, 
158–70 
McNeil, A. 42 
macro approaches to stress scenarios see 
top-down approaches. . . 
Maginot lines, risk management 198–9 
magnitude of associated losses 118–29, 134, 196, 
200–1 
Malvergne, Y. 34, 42–3 
management reports 6, 28, 197–204 
see also governance 
marginal probabilities 5, 12, 38, 42–3, 47–69, 
71–91, 93–8, 100–15, 117–29, 133–53, 
155–70, 180, 182–3, 189, 200 
concepts 47–69, 71–91, 93–8, 100–15, 
117–29, 133–53, 155–70, 189, 200 
conditional probabilities 62–9, 71–91, 93–8, 
117–29, 133–53, 155–70 
conditional probability tables 117–22, 133–53, 
155–70 
deﬁnition 50–3, 76, 89–90, 95 
joint probabilities 76–7, 83–91, 93–8, 
117–29, 156–70 
multiplicative factors 62–9, 120–2, 151–3, 
163–70 
Markov process 103, 110–12, 117 
Markowitz, Harry 16–17 
Marshall, Alfred 15 
Martignon, L. 176–7 
measure theory 204 
Merton-like model 87 
Metallgeshaft 199 
micro-structural models 
concepts 15, 17, 19–30, 189–91 
deﬁnition 19 
Middle East 188–91 
Miller, J.H. 26–7, 30 
minimax solutions 11 
minimum repayment amounts, credit cards 
183–4 
mistakes 
learning from mistakes 16, 36–7 
small probabilities 61–2 
models 
see also predictions; techniques 
beauty-parade-of-models metaphor 26, 27–8 
centrality-of-models thesis 17–23 
competing models 17, 21–30, 35 
concepts 5, 15–30, 38–43, 100–15, 144–53, 
158–70, 189–96, 197–204 
coordination thesis 15, 18, 23–7, 29–30 
deﬁnition 17 
indispensable interpretative tools 5, 17, 18–21, 
23, 42, 71 
reduced-form models 15, 19–30, 189–90 
two theses 17–30 
types 15–21, 38–43, 189–90, 197–204 
monetary policies 190–1 
Monte Carlo simulations 203 
Moody 121 
Moore, A. 50–1, 58, 84, 87–8, 90, 109, 151 
mortgages 63–5, 180–3, 187–8, 191, 192–4 
banking activities 192–4 
delinquencies 187–8, 191 
mortgage-backed securities 63–5, 180–3, 187 
Moskowitz, H. 94, 96, 125, 155, 182, 184 
moves in the risk factors, concepts 121–9 
multi-viewpoint perspectives, cognitive biases 
177–8 
multiple events, conditioning 71–91, 95–8, 164 
multiplicative factors 62–9, 120–2, 151–3, 
163–70 
Muth, J.F. 21–2 
mythology 10 
natural disasters 188 
Neapolitan, R.E. 115 
necessary conditions, conditional probability 
tables 125–6 
negative/positive biases, concepts 182–4 
neo-classical theory 11–12, 16, 21–2, 27, 30, 35, 
173–4 
networks 91 
see also Bayesian. . .  
nodes 
see also Bayesian networks 
graph concepts 99–115, 155–70, 194–6 
non-basic variables, linear programming concepts 
140, 148, 208–12 
non-experts, predictions 177 
non-farm payroll statistics 188 
non-specialist challenges, stress tests 197, 198 
non-trivial optimality 11 
normal market conditions 4–5 
normalization factors 55–7, 73–91, 95–8 
normative standard 177–85 
notation 47–69, 73–7, 160–1 
joint probabilities 73–6, 160–1 
transparency 75–6 
objective function, linear programming concepts 
140, 148, 205–12 
objectivity, concepts 39–43, 202–3 
occurrence horizon, concepts 121–9 
oil prices 188, 199 
OIS/LIBOR spread 193 

224 
INDEX 
optimization 
see also bounded rationality; linear 
programming 
concepts 11, 133–53, 173–8, 185, 205–12 
options 
implied volatilities 3, 35–6, 104–6, 156–66 
put options 191–2 
order-of-conditioning rule, Bayesian networks 
108–11, 160–70 
organizational resilience 189–90 
overconﬁdence bias, concepts 135, 174, 177 
overview of the book 4–6 
Ozdenoren, E. 25–6, 30 
Page, S.E. 26–7, 30 
paradigms 
see also models 
concepts 190–1, 195–6 
predictions 2–6, 21–2, 27–9, 35–6, 90, 93, 
100–1, 177–83, 201–4 
see also models 
conditional probabilities 3–6, 177–82 
experts 21–2, 27–9, 35–6, 90, 93, 100–1, 
177–8, 182–3, 201–4 
present value of a basis point 168 
pressure points of portfolios 27–8, 118–29, 
187–96, 199, 204 
prices 3, 24–5, 27–8, 30, 37, 38–9, 49, 86–91, 
104–6, 118–29, 141–53, 156–70, 187–96 
pricing concepts 2–6, 30, 37 
primary variables, Bayesian networks 102–15, 
193–4 
principal/agent relationships 24–6, 30 
printing money see quantitative easing 
probabilities 2–6, 10–13, 31–43, 47–69, 71–91, 
93–8, 100–15, 117–29, 134–53, 155–70, 
parents, graph concepts 99–115, 158–70, 194–6 
173–85, 187, 196 
partial ordering of variables in Bayesian networks 
see also Boolean. . .; conditional. . .; 
104–9, 164–70 
frequentist. . .; joint. . .; marginal. . .; 
path-dependencies, Bayesian networks 103 
subjective. . . 
pavements 39–42, 100, 135 
cognitive biases 61–2, 100–1, 159, 173–85 
payoffs 24–30 
concepts 31–43, 47–69, 71–91, 115, 117–29, 
Pearl, J. 39–41, 43, 53, 71, 100, 103, 115 
173–85 
pension fund trustees 26 
deﬁnition 31–2, 49–53, 74–6, 89–90 
percentile levels, VaR 203 
doubly-conditioned probabilities 49, 65–6, 
Pharaohs of Egypt 177, 184 
72–3, 93–8, 108–11, 134–9, 155–6, 165 
physics 19 
notation 47–69, 73–7, 160–1 
plausible models of reality 
small probabilities 61–2, 96–8, 120–9, 
166–70 
see also stress tests 
concepts 27–30, 100–15, 144–53, 158–70 
types 31–43, 47–9, 71–3 
plurality-of-models view, concepts 17, 21–30 
probability bounds see bounds 
‘pointers to a frame of knowledge’ 71 
probability as degree of (rational) belief see 
see also joint probabilities 
subjective probabilities 
Poirier, D. 42 
proﬁt/loss events 
Polychronakos, A.P. 80–2 
see also loss. . .  
‘polyvalence’ features of stress testing 
linear programming methodology 141–53 
programmes 204 
‘proxy hedging’ 141 
populating considerations 
‘pruning’, Bayesian networks 155–6, 192–4, 
see also dimensionality problems 
conditional probability tables 101, 106–7, 
117–29, 135–6, 144–53, 155–64, 192–4, 
200 
portfolios 16–17, 27–8, 118–29, 141–53, 
187–96, 199–204 
bottom-up approaches to stress scenarios 27–8, 
187–96 
diversiﬁcation concepts 16–17 
pressure points 27–8, 118–29, 187–96, 199, 
204 
positive feedback mechanisms, ﬁnancial markets 
15, 18, 25–7 
positive/negative biases, concepts 182–4 
power law 18–19 
practical uses 149–53, 167–70, 188–96 
200, 204 
put options 191–2 
see also options 
qualitative approaches, concepts 42–3, 198 
quantiﬁcation of the representativeness bias 
181–2, 185 
quantitative easing 191 
quantitative risk analysis 5, 9–10, 16, 18–30, 
31–43, 197–204 
see also stress tests 
quants 201 
Raiffa, H. 10–11, 13 
rare-and-even-more-dangerous-diseases example, 
conditional probabilities 61–2 

INDEX 
225 
rare/extreme events, concepts 3–6, 12, 19–30, 
34–43, 48–69, 86–91, 104–6, 118–29, 
135–53, 156–70, 187–96, 198–204 
RAROC see risk-adjusted-return-on-capital 
rational agent model, critique 21–2, 29, 173–4, 
190–1 
rational expectation theory, critique 21–2, 29, 
173–4 
rationality 
see also bounded. . . 
concepts 16, 21–2, 28–9, 32–3, 173–8, 190–1 
critique 21–2, 29, 173–4 
reality, models 5, 15–30, 39–43, 100–15, 
144–53, 158–70 
reason 15–30 
reduced-form models 
concepts 15, 19–30, 189–90 
deﬁnition 19 
references 213–16 
regression analysis 177, 181–2 
regulatory capital 
see also capital. . . 
concepts 10–12, 16–17, 24, 26, 135, 150–3, 
167–70, 189–90, 196, 201 
conditional expected losses 150–3, 167–70 
relative-value trades 187–96 
see also bottom-up approaches. . . 
reporting lines, concepts 6, 28, 197–204 
representativeness bias 
see also cognitive biases 
concepts 61, 125, 173, 174, 175, 178–82, 185 
deﬁnition 178–9 
quantiﬁcation 181–2, 185 
Wrong-Bayesitis 61, 179, 180 
return distributions 18–30, 33–4, 37–8 
return on risk-adjusted capital (RORAC) 168 
return-over-VaR 168–70 
reward, risk 2–6, 168–70 
risk 
aversion 36 
concentration 150–1, 199 
concepts 2–6, 9–13, 35–6, 118–29, 168–70 
deﬁnition 10–11 
factors 121–9 
positions 121–9 
premiums 35–6 
RAROC 168–70 
reward 2–6, 168–70 
risk management 
see also economic capital; extreme value 
theory; linear programming; stochastic 
calculus; stress tests; Value at Risk 
concepts 1–6, 9–13, 15–30, 31–43, 90–1, 
93–8, 115, 117–29, 133–53, 155–70, 
192–6, 197–204, 205 
dimensionality problems 192–4, 200 
Maginot lines 198–9 
models 5, 15–30, 158–70 
subjective probabilities 2–6, 10, 12, 27–9, 
31–43, 177–8, 188–92, 201–4 
techniques 1–6, 28, 62, 73–91, 98, 114–15, 
117–29, 133–53, 155–70, 187–8, 193–6, 
197–204, 205–12 
risk-adjusted-return-on-capital (RAROC), 
concepts 168–70 
Robertson, D.H. 1, 135 
RORAC see return on risk-adjusted capital 
Rosencrantz, Ted 32 
rules of thumb see fast-and-frugal adaptive 
toolbox 
‘run-away behaviour’ 15 
running, dangerous-running example 59–61 
Russia 190 
S&P 49, 90, 119, 143–4 
salience bias 176, 200 
sanity checks 
concepts 117–29, 133–4, 135–6, 149, 155–6, 
191–2, 198–9 
triplet conditions 126–7, 192–3, 200 
types 191–2, 198–9 
Sarin, R.K. 94, 96, 125, 155, 182, 184 
satisﬁcing 174 
see also bounded rationality 
scary scenarios, limited uses 192 
scenario analysis 
bottom-up approaches to stress scenarios 27–8, 
187–96 
concepts 2–3, 27–30, 47–69, 96–8, 141–53, 
187–96 
selection/combination of stress scenarios 
187–96, 198–9 
‘story’ analogy 28–9, 145–53, 190–1, 200–1 
strategic paradigms 190–1, 195–6 
top-down approaches to stress scenarios 
187–96 
uses 27–9, 141, 187–96 
Schuermann, T 196 
scissors metaphor, bounded rationality 175–6 
search costs, bounded rationality 174, 185 
security planning 190–1 
selection/combination of stress scenarios 187–96, 
198–9 
self-awareness skills, cognitive biases 173–85 
self-fulﬁlling prophecy 25–6, 185 
Selten, R. 174–5, 185 
sentiment effects on prices 188 
‘sexy’ results 12 
Shefrin, H. 2, 22, 30, 178–9, 185, 188 
Shelling, Thomas 30 
Shiller, R.J. 188 
Shleifer, A. 30 

226 
INDEX 
short rates 194–6 
‘silver bullets’ of protection 21, 28–9 
Simon, H.A. 174–5 
simplex method, linear programming concepts 
136, 140–8, 208–12 
simulate annealing 115 
singly-conditioned probabilities, concepts 48–9, 
66, 73–91, 93–8, 108–11, 134–45, 164–5 
‘size of the whole universe’ 55–69, 73–91 
Skidelsky, R. 5, 11, 13, 16, 188 
skills of professionals 199–201 
slack variables, linear programming concepts 139, 
147–9, 206–12 
small probabilities 61–2, 96–8, 120–9, 166–70 
social context, cognitive biases 174–85 
Sornette, D. 34, 42–3 
Soros, G. 30 
source code, linear programming 144, 167, 212 
South-East Asia currency crisis of 1997–1998 
191, 203 
specialists 
see also experts 
governance 197, 198, 199–201 
skills 199–201 
splitting-of-the-marginal rule, Bayesian networks 
109–11 
sprinklers 39–41, 100, 135 
square-root-of-sum-of-squares rule 168 
stand-alone probabilities 5, 12, 38–40, 42–3, 
47–69, 74–91, 100, 134–53, 158–70, 180, 
182–3, 190–1, 193–4, 204 
see also marginal. . .  
Stewart, Neil 184 
Stigler, G.J. 174, 185 
Stirzacker, D. 91 
stochastic calculus 30, 78, 87, 204, 205 
see also risk management 
stochastic discount factors 30 
stochastic processes 
concepts 78, 87, 204 
deﬁnition 78 
stochastic variables 36 
see also correlations 
stocks 
see also equities 
technology bubble 176 
‘story’ analogies 28–9, 145–53, 190–1, 200–1 
see also models; paradigms 
audits 197, 199, 201, 203 
Bayesian networks 111–15, 155–70 
bottom-up approaches 27–8, 187–96 
bounds 96–8, 134–53, 155–6, 165 
coherent solutions 133–70 
commitment expectations 204 
completeness checks 197, 198–9 
complexity problems 101–2, 200–1, 203–4 
critique 1–3, 5, 9–13, 201–4 
deﬁnition 2, 4, 12, 27 
dimensionality problems 192–4, 200 
ease-of-use requirements 197–8 
expert-judgement-based stress testing 28–30, 
177–8, 189, 201–4 
governance 9, 28, 149–51, 167–70, 190–3, 
197–204 
input/output sensitivities 197, 200–3 
non-specialist challenges 197, 198 
‘polyvalence’ features of stress testing 
programmes 204 
sanity checks 117–29, 133–4, 135–6, 149, 
155–6, 191–2, 198–9 
selection/combination of stress scenarios 
187–96, 198–9 
specialist interactions 197, 198, 199–201 
systematic approach 111–13, 129, 133–70 
top-down approaches 187–96 
transparency 75–6, 98, 150, 197–8, 201, 203 
uses 1–5, 9, 27–30, 53 
stress-related risk statistic, concepts 169–70 
stretched exponential distributions 18–19 
structural issues, Bayesian networks 100–4 
‘structural pruning’, Bayesian networks 155–6, 
192–4, 200, 204 
subjective probabilities 
see also intuitive techniques; stress tests 
concepts 2–6, 10, 12, 27–9, 31–43, 177–8, 
188–92, 201–4 
critique 3, 188–92, 201–4 
deﬁnition 31–2 
examples 31–2 
summer–winter gas spreads 56, 58–9, 90, 199 
sunlight 203 
swaps 33–4, 191, 193–4, 199 
Sweeney, M.J. 190 
systematic approach 
Bayesian networks 111–13, 129 
conditional probability tables 129, 133–53, 
strategic paradigms, concepts 190–1, 195–6 
stress tests 1–6, 9–13, 15–17, 27–30, 38–43, 
47–8, 53, 64–5, 77–8, 96–8, 111–15, 
155–70 
stress tests 111–13, 129, 133–70 
systematic biases, concepts 36, 173–85 
122–3, 133–70, 174–5, 177–8, 183–4, 
187–96, 197–204 
see also subjective probabilities 
anchoring bias 174–5, 183–4 
applications 1–5, 9, 27–30, 53, 131–70 
t-Student distributions 198 
tails 
co-dependencies 3–4, 36–43, 48–9, 114–15, 
135 

INDEX 
227 
concepts 3–4, 18–30, 33–4, 36–43, 48–9, 
135, 168–70, 203 
Taiwan 187 
techniques 1–6, 28, 62, 73–91, 98, 114–15, 
117–29, 133–53, 155–70, 187–8, 193–6, 
197–204, 205–12 
see also Bayesian. . .; intuitive. . .; linear 
programming; models 
technology bubble 176 
Tenenbaum, J.B. 177, 184 
‘thinking constructively’ 99–115 
tightness of bounds 96–8 
time horizons 4–6, 10–11, 34–43, 49–50, 
59–69, 78, 189–90, 200–3 
time ordering, concepts 56–69 
time series 18–30, 33–4, 48–69, 141, 150–1, 
153, 202–4 
see also frequentist. . . 
top-down approaches to stress scenarios 
Bayesian networks 188–96 
bottom-up uses 194–6 
concepts 187–96 
critique 187–90 
deﬁnition 187 
possible approaches 190–1, 194–6 
toy models 19–20, 37, 39–40 
trade-offs 11 
traders 15, 18, 23–7, 29–30, 188–96, 197–204 
see also risk management 
agents 18, 23–7 
coordination thesis 15, 18, 23–7, 29–30 
training, cognitive biases 173–85 
transparency 75–6, 98, 150, 197–8, 201, 203 
see also governance 
Treasuries 37–8, 90, 118–29 
trees 91, 158–70, 188–96 
triplet conditions 126–7, 192–3, 200 
see also sanity checks 
Tversky, A. 174, 178, 182–3, 185 
two theses, models 17–30 
two-valued Boolean variables 48–69, 73–91, 
99–115, 118–29, 136, 151, 166–70 
see also Bayesian networks; Boolean. . . 
Tzani, R. 80–2 
uncertainty 
concepts 10–13, 35, 38–9, 93–8, 151–3, 
168–70 
deﬁnition 10–11 
expected losses 151–3, 168–70 
unemployment levels 187, 189, 191 
‘unknown unknowns’ 11 
unravelling contemplations, causation 113–15 
unwinding horizon, concepts 121–9 
US 24, 49, 54, 90, 119, 143–4, 189–91 
utility functions 22–3, 169–70 
Value at Risk (VaR) 1, 2, 4–5, 10, 16, 32–3, 39, 
42, 119–20, 135, 141, 167–70, 201–3 
variance 78–91, 135, 151–3, 168–70, 203 
variance-covariance calculations 203 
VBA 144 
Venn diagrams 6, 47, 50–69, 72–91, 109, 128–9 
see also conditional probabilities 
volatilities 3, 25–6, 35–6, 49–50, 104–6, 
118–29, 141–53, 156–66, 203 
see also implied. . . 
weightings 202–3 
‘West-Wing-like’ glamour of the top-down 
approach to stress scenarios 187–8 
wet pavements 39–42, 100, 135 
wholly unbounded rationality, concepts 174 
Williams, D. 59, 90–1 
Williamson, J. 42, 113–15, 156 
Wrong-Bayesitis (WB) 61, 179, 180 
yield curves 48–9, 90, 121–2, 141–4, 188–9, 
193–4, 199 
see also bonds; ﬁxed-income securities 
Yuan, K. 25–6,  30  
Zelikow, P. 20, 23, 190 

