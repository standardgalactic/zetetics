On the Power of Convolution Augmented Transformer
Mingchen Li
Xuechen Zhang
University of Michigan
{milii,zxuechen}@umich.edu
Yixiao Huang
UC Berkeley
yixiaoh@berkeley.edu
Samet Oymak
University of Michigan
oymak@umich.edu
Abstract
The transformer architecture has catalyzed revolutionary advances in language
modeling. However, recent architectural recipes, such as state-space models, have
bridged the performance gap. Motivated by this, we examine the benefits of
Convolution-Augmented Transformer (CAT) for recall, copying, and length gener-
alization tasks. CAT incorporates convolutional filters in the K/Q/V embeddings
of an attention layer. Through CAT, we show that the locality of the convolution
synergizes with the global view of the attention. Unlike comparable architectures,
such as Mamba or transformer, CAT can provably solve the associative recall
(AR) and copying tasks using a single layer while also enjoying guaranteed length
generalization. We also establish computational tradeoffs between convolution and
attention by characterizing how convolution can mitigate the need for full attention
by summarizing the context window and creating salient summary tokens to attend.
Evaluations on real datasets corroborate our findings and demonstrate that CAT
and its variations indeed enhance the language modeling performance.
32
64
128 256 512 1024
0.00
0.25
0.50
0.75
1.00
 
CAT, 1 layer
Attention, 2 layers
Attention, no PE
Mamba, 2 layers
Train Length
Test Length
Accuracy
512
1K
2K
4K
8K
16K
0
100
200
300
400
500
600
CAT, RoPE
MH-CAT, RoPE
Pythia, RoPE
CAT, no PE
MH-CAT, no PE
Train Length
Test Length
Perplexity
Figure 1: Evaluations on synthetic and real data. The models are trained on 128 and 2,048 context length
(vertical dashed lines) and tested on varying context lengths respectively. Left figure: We conduct synthetic
experiments on the Associative Recall task and contrast 1-layer CAT with 2-layers of alternative architectures.
The embedding dimension is 128. We find that CAT is the only model that solves AR with length generalization
in line with our theory (also see Fig. 6). Right figure: Evaluations on language modeling where we train CAT
models by equipping Pythia with short convolutions (window size 21). Convolution allows the model to pretrain
without positional encoding and further improves perplexity when combined with RoPE. Importantly, it also
generalizes to longer context lengths more robustly with or without RoPE. For length generalization, we used
YaRN [36] which incorporates position interpolation [8] (for RoPE only) and temperature scaling (see Sec. 6.2).
1
Introduction
The attention mechanism is the central component of the transformer architecture [47] which empow-
ers modern large language models. Through the self-attention layer, all pairs of tokens get to interact
with each other which equips the model with a global view of the context window. On the other hand,
without positional-encoding (PE), self-attention lacks locality. For instance, without PE or causal
masking, self-attention layer is permutation-equivariant and does not distinguish between nearby
vs distant tokens. In contrast, convolution operator is a well-established primitive that facilitates
arXiv:2407.05591v1  [cs.LG]  8 Jul 2024

ùëã= {ùë•!, ùë•", ùë•#}
ùëä$
ùëä%
ùëä&
ùëÑ= (ùëã‚àóùêπ$)ùëä$
ùêæ= (ùëã‚àóùêπ%)ùëä%
ùëâ= (ùëã‚àóùêπ&)ùëä&
ùêπ$
ùêπ%
ùêπ&
Attention ùëÑ, ùêæ, ùëâ
=  Softmax ùëÑùêæ'
ùëë%
 V
Convolutions
64
128
256
512
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
Associative Recall Comparison
Figure 2: Left figure: Illustration of the Convolution-Augmentated Attention (CAT) block, where separate
filters are applied to the K/Q/V embeddings, before self-attention (see Sec. 3.1 for details). Right figure:
Performance of 1-layer CAT models trained on multi-query AR (MQAR, see Sec. 3.2.1 for details) tasks with
model embedding dimension 64 and varying sequence length. The LinCAT replaces the standard attention in
CAT with linear attention. We observe that the CAT model outperforms the baseline models across all sequence
lengths with only 1 layer compared to 2 layers baselines.
local feature aggregation based on relative positions and provides a natural alternative to PE. While
convolution has enjoyed major success in vision during the last three decades, its explicit use in
language modeling is relatively recent [11]. On the other hand, there is a growing recent interest in
using convolution-based blocks in language models: For instance, state-space models (SSM) [17]
and linear RNNs [33] are efficient parameterizations of long convolutional filters. These models
have enjoyed significant success in long-range sequence modeling as they provide fast inference and
parallelizable training. On the other hand, purely convolutional architectures are known to suffer from
recall capability as they lack the global view of the context window [2]. These insights motivated a
recent push toward hybrid architectures [12, 2, 35, 1] that combine the strengths of both attention and
convolution-like approaches, including short convolutional filters, SSMs, linear RNNs, or Mamba.
In this work, we explore the synergy between attention and convolution which reveals new theoretical
principles that inform hybrid architecture design. Specifically, we introduce an intuitive hybrid
architecture called Convolution-Augmented Transformer (CAT)1. CAT incorporates convolutional
filters to the K/Q/V embeddings of the attention layer as depicted on the left hand side of Figure
2. We explore the capabilities of the CAT layer through mechanistic tasks including associative
recall (AR), selective copying [15, 22], and length generalization. For instance, AR is a fundamental
task motivated from the associative memory in cognitive science [4]. This task underpins critical
applications such as bigram retrieval, where a specific sequence, such as ‚ÄòRings‚Äô following ‚ÄòThe Lord
of the‚Äô, must be correctly retrieved. It is also a generalization of the induction head task [32] and
known to be crucial for LLM functionality and mechanistic understanding [32, 14, 2, 31, 38].
We theoretically and empirically show that, within the CAT layer, attention and convolution exhibit
strong synergy and complementarity to solve these mechanistic tasks while enjoying length gener-
alization benefits. As a concrete example, the left side of Figure 1 displays the AR performance
for various test-time sequence lengths. As the sequence length grows, we observe two distinct
failure modes: Mamba‚Äôs accuracy degrades due to its finite state dimension whereas attention-only
models degrade due to the length extension bottlenecks of PE. In contrast, CAT maintains perfect
accuracy and length generalization because attention and convolution patch these failure modes in a
complementary fashion. Overall, we make the following contributions:
‚Ä¢ We propose the convolution-augmented attention layer and prove that it can solve the N-gram
AR (NAR) and Selective Copying tasks using a single layer (Theorems 1 and 4). Comparison to
alternatives (Mamba, Based, attention, linear attention) reveals that CAT can uniquely solve NAR
with length generalization.
‚Ä¢ To explain this, we establish a length generalization result on the loss landscape (Theorem 2):
Under mild assumptions, all CAT models that solve AR for a particular context length provably
generalize to all other context lengths.
‚Ä¢ We evaluate CAT on real data and demonstrate that even 1-dimensional short convolutions notice-
ably aids language modeling: In line with theory, convolution enables the model to train stably
1The transformer architecture consists of attention and MLP layers. For theoretical analysis and synthetic
experiments, we will entirely focus on the Convolution Augmented Attention layer described in Fig. 2. For this
reason, we will use the CAT acronym to refer to both Convolution-Augmented Transformer and Attention.
2

without PE and improves length generalization. We also develop a multihead version of CAT
which yields further accuracy improvements (see Table 2).
‚Ä¢ We show that long convolutions, such as SSMs, bring the benefit of context summarization and
mitigates the need for dense attention: We describe the Landmark CAT model (following Landmark
Attention [30]) which first creates landmark/summary tokens through convolution and then attends
on these landmarks to efficiently locate the most relevant subsets of the input sequence (Sec. 5).
Focusing on the AR problem, we characterize fundamental tradeoffs between the embedding
dimension, amount of summarization, and the sparsity of attention. Through these, we show that
the use of long convolutions can provably enable the success of sparse/cheaper attention.
2
Related Works
Convolution-like sequence models. Gated-convolutions [11] and state-space models, such as
S4 [17], utilize long convolutions to reduce the computational demands associated with attention
mechanisms. Performance enhancements have also been achieved through novel filter parametrization
techniques [18, 16]. Despite these innovations, challenges in Multi-query Associative Recall (MQAR)
prompted the development of input-dependent convolution techniques. Notable developments in
this area include, Liquid S4 [19], Mamba [15, 10] and [48, 24] where convolution filters are directly
parametrized by inputs and include correlation terms between input tokens to enhance state mixing.
[26] empirically explores the reason underlying the success of convolutional models.
Expressivity, recall, length generalization. Recent works [2, 21, 1, 14] explore the limitations
of purely convolutional models, including Mamba, and demonstrate that, these models inherently
lack the capability to solve recall problems unless they have large state dimensions (i.e. memory).
[21] also provides a construction for 2-layer self-attention to solve AR with length generalization.
Interestingly, this construction uses Hard Alibi, which is a variation of Alibi PE [39] that utilize
explicit linear biases in attention. Their Hard Alibi restricts the attention layer to focus on and
aggregate only the recent N tokens. In this regard, this construction is related to our short convolution.
On the other hand, while this work is constructive, we also prove that CAT has good loss landscape
and all CAT solutions to AR provably length generalize. It has also been observed that PE can hurt
length generalization and reasoning. In fact, [23] has found NoPE to be viable. On the other hand, in
our real data evaluations, we have found pure NoPE to be highly brittle as it either fails to converge
or optimization is unreasonably slow. Our AR experiments also corroborate that NoPE by itself is
indeed not a viable strategy.
Hybrid architectures. There is a growing interest in integrating different language modeling prim-
itives to obtain best-of-all-world designs. To this end, mechanistic tasks such as AR, copying,
induction head, and in-context learning have been important to demystify the functionalities of lan-
guage models [32, 35] and have been utilized to guide architecture design [1, 38]. Gating mechanisms
have been integrated within convolutional frameworks to enhance the model‚Äôs selectivity. Models
employing gating functions, have shown substantial improvements in AR tasks [14, 37]. Additionally,
recent innovations on hybrid architecture, such as BaseConv [1, 2], GLA [49], MambaFormer [35],
and [27, 28, 40] have provided more effective solutions to AR tasks. This comprehensive foundation
of hybrid architectures informs our exploration into the convolution-attention synergy.
3
Problem Setup
3.1
Convolutional-Augmented Attention
Let us first introduce helpful notation. Id is the identity matrix of size d. Di denotes the causal delay
filter that shifts a signal x i-timesteps forward i.e. (x ‚àóDi) j = x j‚àíi. For an integer n ‚â•1, we denote
the set {0, . . . , n ‚àí1} by [n]. We use lower-case and upper-case bold letters (e.g., m, M) to represent
vectors and matrices, respectively. mi denotes the i-th entry of a vector m.
Below, we introduce the Convolution-Augmented Attention layer, which incorporates learnable filters
into the K/Q/V embeddings. Let X = [x0 . . . xL‚àí1]‚ä§‚ààRL√ód denote the input to the layer containing
L tokens with embedding dimension d. Let F ‚ààRW denote the convolutional filter with temporal
length W. We examine two convolution types which handle multi-head attention in different ways:
‚Ä¢ 1D per-head convolution: For each attention head, we have a distinct 1D filter F ‚ààRW. F is
applied temporally to each of the d embedding dimensions. This results in F ‚àóX where (F ‚àóX)i =
P
j‚àà[W] F jxi‚àíj, with F j being the j-th entry of F.
3

‚Ä¢ Multi-head convolution: Suppose we have H sequences ¬ØX = [X1, . . . , XH] ‚ààRL√ód√óH each
corresponding to one of the H attention heads. We use a filter ¬ØF = [F1, . . . , FH] ‚ààRW√óH√óH. Each Fi
is convolved with ¬ØX to obtain the i-th head‚Äôs output of size L √ó d.
Observe that both convolution types are identical when there is a single attention head. However,
multi-head convolution is more expressive because it mixes the attention heads. In Section 6, we
will also examine a variation of multi-head convolution where we mix the attention maps rather than
embeddings. The architecture of CAT is illustrated in Fig. 2 and is formally defined as follows:
Definition 1 (Convolution-Augmented Attention (CAT)). A CAT layer incorporates learnable
convolutional filters to the key/query/value embeddings. For a single-head CAT, the key embeddings
are given by K = (X ‚àóFk)Wk with weights Fk,Wk (same for query and value embeddings).
3.2
Mechanistic Tasks for Language Modeling
To proceed, we describe the Associative Recall and Selective Copying tasks that will help us
mechanistically study CAT. Table 1 provides an illustration of these tasks which are adapted from the
sequence modeling literature [15, 1, 38, 32].
Definition 2 (Associative Recall Problem). Consider a discrete input sequence X = [x0, x1, . . . , xL‚àí1],
with tokens drawn from a vocabulary V of size |V|. The AR problem is defined as follows: Suppose
that there is a unique index i (0 ‚â§i < L ‚àí1) such that xi = xL‚àí1. A model f successfully solves the
AR problem if f(X) = xi+1 for all inputs X. In this problem, xi becomes the key, xi+1 is the associated
value, and the last token xL‚àí1 is the query.
Building on the AR problem, we introduce its N-gram variation: The model needs to identify the
copy of the last N tokens in the context window and return the associated value.
Definition 3 (N-gram AR Problem). Consider a discrete input sequence X = [x0, x1, . . . , xL‚àí1], with
tokens drawn from a vocabulary V of size V|. Let X{i, j} = [xi, xi+1, . . . , xj] denote the subsequence
of X from index i to j. The N-gram associative recall (NAR) problem is formulated as follows: for
X{L‚àíN,L‚àí1} (which are the last N tokens), there exists a unique index i (0 ‚â§i < L ‚àíN) such that
X{i,i+N‚àí1} = X{L‚àíN,L‚àí1}. A model f solves NAR if f(X) = xi+N for all inputs X.
Selective copying (SC) task is originally introduced by [22] and it is utilized by the recent Mamba
[15] and Griffin [12] papers to assess their model‚Äôs approximation capabilities. In SC, given an input
sequence X containing noisy tokens, the model should denoise X and return the signal tokens within.
Definition 4 (Selective Copying). Consider a vocabulary V composed of a set of signal tokens S, a
set of noise tokens N, and special token ‚ä•i.e. V = S ‚à™N ‚à™{‚ä•}. Let X be a sequence whose tokens
are drawn from S ‚à™N and let XS be the sub-sequence of X that includes all signal tokens in order. f
solves selective copying over S if it autoregressively outputs XS following the prompt [X ‚ä•] for all
inputs X. f solves unique selective copying if it outputs all unique tokens of XS in order for all X.
3.2.1
Multi-Query Associative Recall
In this section, we introduce the multi-query versions of the AR and NAR tasks, abbreviated as
MQAR and MQNAR, respectively. In the multi-query (MQ) setting, a model receives multiple
queries in a single input and must generate corresponding outputs in a single forward pass, at varying
Table 1: Illustrative examples of synthetic tasks. In all AR-based tasks, keys and queries are highlighted in red
and the values in green. For NAR tasks, parentheses denote N-gram queries; note that the parentheses are not
part of the input. In SC tasks, signal tokens are in green and noise tokens in gray, and the model begins output
when ‚ä•appears in the sequence.
Input
Query
Output
Single Query
AR
a 2 c 1
a
2
NAR
(a b) 2 (b a) q (a a) 4
b a
q
SC
a [n] [n] c [n] k
‚ä•
a c k
Multi Query
AR
a 2 c 1
c a
1 2
NAR
(a b) 2 (b a) q (a a) 4
(b a) (a a)
q 4
4

positions in the sequence. This approach was first introduced in [1], which demonstrated that while
the Mamba model successfully addresses single-query AR tasks, it struggles with MQAR when
operating with a limited model dimension. This highlights the increased complexity of MQAR tasks
where models need to memorize more sequence information and recall queries at different positions.
Definition 5 (Multi-Query Associative Recall (MQAR)). Consider a discrete input sequence
X = [x0, x1, . . . , xL‚àí1] with tokens drawn from a vocabulary V. Let X{i, j} = [xi, . . . , xj] denote
a subsequence of X from index i to j. The multi-query N-gram associative recall (MQNAR) problem
is defined as follows: for every N-gram query Qk = Xk‚àíN+1...k, N ‚â§k < L, determine if there exists a
N ‚â§j < k such that X{j‚àíN+1,j} = Qk. If so, output the value xj+1 as the result, else output a special
token to indicate no match is found. A model f solves MQNAR if it outputs the correct values for all
N-gram queries and all inputs X. The standard MQAR problem [1] is a special instance of MQNAR
by setting N = 1.
Table 1 provides examples of the synthetic tasks we consider in this work. Specifically, we conduct AR
and NAR experiment on their multi-queiry variants to evaluate the model‚Äôs ability to recall multiple
queries. For the selective copying task, we generate the output auto-regressively by predicting the
signal tokens in the input sequence after the special token ‚ä•.
4
Provable Benefits of Convolution-Augmented Attention
Before diving into the theoretical results, we make a few clarifying remarks. We assume that all token
embeddings have unit ‚Ñì2 norm. Secondly, a CAT layer maps each query to a vector-valued output
f(X) ‚ààRd. To sample the discrete output token, we will simply return the nearest neighbor in the
vocabulary of token embeddings. For associative recall problems, we will use a single head attention
layer with weights Wq,Wk are chosen as suitably scaled identity matrices. With this choice, attention
essentially implements a nearest neighbor retrieval. It suffices for the theory thanks to the simple
nature of the AR problem where we wish to identify the replica of a query within the context window.
In general, we can easily contrive natural generalizations of AR and Selective Copy problems that
necessitate a more sophisticated attention mechanism (see [38]). One such generalization is, given
query q, we wish to retrieve a general key k (possibly k , q) and return the value associated with k.
N-gram AR. Our first result shows that a single CAT layer can solve the NAR problem under fairly
general conditions.
Theorem 1 (Solving NAR). Let F ‚ààRN be a causal 1-D convolutional filter of length N and
norm(X) normalize the rows of a matrix to unit ‚Ñì2 norm. Consider a single CAT layer f(X) =
(XvWv)‚ä§S(XkWkW‚ä§
q q) where q is the final token of Xq and Xq = norm(X ‚àóFq) ‚ààRL√ód (same for
Xk,). Set Fq = F and Wk = Wq = ‚àöcId. Use either
‚Ä¢ Value delay: Fk = Fq, Fv = D‚àí1 and Wv = 2Id or,
‚Ä¢ Key delay: Fk = D1 ‚àóFq, Fv = D0 and Wv = Id
Let Œµ > 0 be the minimum ‚Ñì2 distance between two distinct tokens embeddings. For almost all choices
of F, there is a scalar c0 > 0 depending on F such that, setting c = c0 log(4L/Œµ), CAT layer solves
the NAR problem of Def. 3 for all input sequences up to length L.
As a corollary, using a simple 1-D convolutional filter on the key embeddings solves the AR problem.
Corollary 1 (1-D CAT solves AR). Consider a CAT layer employing 1-D convolution on key
embeddings with the delay filter Fk = D1 = [0 1 0 . . . 0] and Fq = Fv = D0. This model solves AR.
Length generalization. Our next result shows that the global minima of CAT provably exhibit length
generalization, thereby shedding light on the empirical benefits of CAT in Figure 1. Concretely, even
if we train CAT to solve AR for a fixed context length, the AR capability will generalize to all other
context lengths. This result is distinct from Theorem 1 because it establishes length generalization
for all CAT models that approximately solve the AR problem for a context length, rather than
constructing one such solution. The proof is provided in Section C.2.
Theorem 2 (Length generalization). Let Fv ‚ààR2W+1
+
be a convolutional filter from time t = ‚àíW
to t = W where W ‚â§L ‚àí1. Consider a CAT layer of the form f(X) = X‚ä§
v S(XWxL‚àí1) where
5

X ‚ààRL√ód, Xv = X ‚àóFv ‚ààRL√ód and xL‚àí1 is the last token of X and W = WkW‚ä§
q . Suppose that token
embeddings have unit norm. Consider any model f = (W, Fv) that can solve the AR problem defined
in Def. 2 up to Œµ-accuracy on all sequences of length L ‚â•3. That is, for all (X, y) where query xL‚àí1
repeats twice and y being the associated value token, we have ‚à•y ‚àíf(X)‚à•‚Ñì2 ‚â§Œµ. Define the minimum
embedding distance within vocabulary V as ‚àÜ= (1 ‚àímaxa,b‚ààV(a‚ä§b)2)1/2 and assume that ‚àÜ> 0.
There are absolute constants R0, R > 0 such that, if Œµ0 := Œµ/‚àÜ‚â§R0/L, we have that
‚Ä¢ The filter obeys ‚à•F ‚àíD‚àí1‚à•‚Ñì1 ‚â§LŒµ0, which is in line with Theorem 1.
‚Ä¢ Let X be an input sequence of length L‚Ä≤ following Def. 2. Let s‚ãÜ(X) ‚ààRL‚Ä≤ be the ‚Äúgolden
attention map‚Äù with entries equal to 1/2 at the positions of the query xL‚Ä≤‚àí1 and 0 otherwise. For
all such X, the attention map of f obeys ‚à•S(XWxL‚Ä≤‚àí1) ‚àís‚ãÜ(X)‚à•‚Ñì1 ‚â§L‚Ä≤Œµ0.
‚Ä¢ For all X of length L‚Ä≤ following Def. 2, we have that ‚à•y ‚àíf(X)‚à•‚Ñì2 ‚â§RL‚Ä≤Œµ0.
Here it worths noting that all CAT models that approximately solve the AR problem ends up learning
convolution and attention weights that are consistent with the constructive result of Theorem 1. This
simple ‚Äúuniversal solution‚Äù is in contrast to attention-only models where length generalization not
only requires standard positional encoding but also additional adjustments to extend the context
window of PE [36, 8].
Additionally, in Appendix C.3, we generalize the length generalization result to the N-gram AR
problem under slightly stronger assumptions, which is specified in Assumption 1. The reader is
referred to Proposition 3. Besides showcasing the value of convolution-attention hybrids, these results
also motivate future research into the optimization landscape: Under what conditions gradient methods
provably converge to generalizable CAT models, namely those described in Theorem 2? Answers to
such questions could build on the recent optimization theoretic results on the transformer/attention
models [45, 44, 13, 34, 25, 31, 20, 3, 29, 9] and extend them to hybrid designs.
Selective Copy. Our next result shows that, 1-layer CAT model can solve the unique selective copy
problem. That is, it can provably generate all signal tokens in the correct order as long as the input
contains each distinct signal token at most once. Corroborating this, our experiments demonstrate
that 1-layer CAT performs on par with or better than alternative architectural choices. The proof is
deferred to Section C.4.
Theorem 3 (Selective Copy). Consider the setting of Def. 4. There is a 1-layer CAT using exponential-
decay query-convolution (i.e. Fq,i = œÅi) and d = |S| + 3 dimensional token embeddings such that, it
outputs all signal tokens in order for all inputs where signal tokens appear uniquely.
Selective Copy problem is distinct from AR in the sense that, it requires a global view of the
token positions as the model has to distinguish the order of the distinct signal tokens within the
context window. In Theorem 4, we actually describe two ways to achieve this (see appendix for
the details): The first option is using an infinitely long convolution Fq,i = œÅi which admits a simple
parameterization as a state-space model [17]. We show that this convolution choice can aggregate
all signal tokens in the query embedding while distinguishing their order. This also partly explains
how Mamba/SSMs are equally effective in solving Selective Copying. An alternative construction
is using a short convolution together with a simple positional encoding. Here, convolution equips
the query with local context (specifically the summary of the signal tokens generated so far) and PE
provides the global context on the locations of remaining signal tokens. This synergy of PE and short
convolution is in line with our real language modeling experiments where CAT with PE outperforms
CAT without PE in terms of perplexity as well as length generalization.
5
Benefits of Long Convolution for Enabling Sparse-Attention
So far we have discussed the benefits of short convolutions to equip transformer with local context to
solve AR and its variations. During this discussion, we have used dense attention which has exact
recall capabilities thanks to its ability to scan the full context window. In this section, we ask the
following: Can convolution also help mitigate the need for dense attention? Intuitively, we should be
able to tradeoff the accuracy of attention computation with computation. Here, we describe how long
convolutions can enable this by effectively summarizing the context window so that we can identify
where to attend in (extremely) long-context settings.
6

ùëã= {ùë•!, ùë•", ùë•#, ùë•$, ùë•%, ‚Ä¶ , ùë•&}
ùëä'
ùêæ= (ùëã‚àóùêπ')ùëä'
ùêπ'
Convolution
Augmentation
Landmark
Attention
ùêæ= {ùëò!, ùëò", ùëò#, ùëò$, ùëò%, ‚Ä¶ , ùëò&}
Sample Rate B=2
Hard Attention
ùêæ!! = {ùëò", ùëò#, ùëò$, ùëò%}
ùëû&
0.1
0.5
1.2
0.2
ùëè= arg max ùêæ!!ùëû&
Sample Rate B=2
Retrieved Block b=2
Local Attention
ùë¶& = Softmax ùë≤'()ùëû& ùëΩ
Retrieved 
block
ùêæ'() = {ùëò*, ùëò$, ùëò+}
Long Convolution
Figure 3: Illustration of the Landmark CAT. We first apply
long convolution on the input sequence and subsample it to
obtain landmark tokens representing individual blocks. Hard
Attention computes the similarity between the query and land-
marks to retrieve the most relevant block. Local Attention
concatenates the retrieved block with the final block contain-
ing the query and computes the output token.
Specifically, we will prove that, long con-
volutions (such as SSMs) allow us to uti-
lize sparse attention while retaining (high-
probability) recall guarantees. These find-
ings complement the recent research that
establish the recall limitations of purely re-
current models [2, 1]. Our theory will also
shed light on the mechanics of landmark
attention [30]. While [30] does not rely on
convolution, we will describe how convolu-
tion can generate landmark tokens by sum-
marizing/hashing the chunks of the context
window, and attention can efficiently solve
recall by attending only to these summary
tokens.
Landmark
Convolutional
Attention
(LCAT): Figure 3 describes the LCAT
block that apply on input sequence X. Let
Fk ‚ààRL be the convolutional filter on keys,
B be the sampling rate, and ¬ØL = ‚åàL/B‚åâ.
Setting K = (X ‚àóFk)Wk ‚ààRL√ód, we obtain
Kss ‚ààR ¬ØL√ód by sampling K at every B
tokens. Additionally, define Xi to be the ith block of X of size B spanning tokens (i ‚àí1)B + 1 to iB.
Let V = (Fv ‚àóX)Wv denote the value embeddings. For a query qi for i ‚àà[L], the LCAT layer outputs:
(1) Hard Attention:
b = arg max
j,‚åài/B‚åâKssqi
(LCAT)
(2) Local Attention:
y = S(Klocqi)Vl
where
Kloc = concat(K‚åài/B‚åâ, Kb).
Above, hard attention phase aims to retrieve the correct block associated to the query. This block is
merged with the local block ‚åài/B‚åâthat contains the query itself similar to sliding window attention.
We then apply dense local attention on the concatenated blocks Kloc.
Computational complexity of LCAT: For a fixed query, (LCAT) requires O(d(L/B + B)) computa-
tions. This is in contrast to O(dL) computations of vanilla attention. Choosing a suitable block size
(e.g. B = O(
‚àö
L)), this model should save up to √ó
‚àö
L in computational savings. Importantly, our
theory will highlight the interplay between the embedding dimension d and the allowable acceleration
by characterizing the exact performance of (LCAT) under a random context model.
Definition 6 (Random Context Model). The query token xL occurs twice in the sequence and has
unit ‚Ñì2 norm. All other tokens of X are IID and drawn with IID N(0, œÉ2/d) entries.
The following proposition shows that, (LCAT) will solve AR if and only if
d
2B log ¬ØL ‚â•1 + o(1).
Proposition 1. Recall ¬ØL = ‚åàL/B‚åâis the number of blocks. Let Wv = 2Id, Fv = D‚àí1, and Wk = Wq =
‚àöc ¬∑ Id with c ‚Üí‚àû. Set key convolution as Fk,i = 1 for 0 ‚â§i < B and zero otherwise.
(A) If d ‚â•2œÉ2B(
p
log ¬ØL +t)2, then (LCAT) solves AR for fixed xL with probability at least 1‚àí3e‚àít2/4.
(B) Conversely, for any Œµ > 0 there is CŒµ > 0 as follows: If ¬ØL ‚â•CŒµ and d ‚â§2œÉ2B(
p
(1 ‚àíŒµ) log ¬ØL‚àít)2,
then (LCAT) fails to solve AR with the same probability.
(C) Finally, suppose we wish to solve AR uniformly for all queries xL over a subspace S . This
succeeds with the same probability whenever d ‚â•2œÉ2B(
p
log ¬ØL + ‚àödim(S ) + t)2.
Figure 4 corroborates the predictive accuracy of Proposition 1: As the block size increases, the
embedding dimension to maintain success of AR grows approximately linearly. One can expand on
this proposition in two directions. Firstly, a fundamental bottleneck in (LCAT) is the requirement
d ‚â≥B log ¬ØL. This arises from a memory-recall tradeoff [2, 21] as we are summarizing the information
of block Xi of length B through its landmark token. However, once this requirement is satisfied, the
model can identify the correct block in O( ¬ØL) cost. To avoid paying the additional O(B) cost of local
attention, we could apply the LCAT approach hierarchically within the selected block to reduce the
compute cost to d( ¬ØL + log B) per token. The dominant term d ¬ØL captures the recall capacity of the
7

LCAT model: Consistent with our theorem and lower bounds of [2], for AR to succeed, we need
recall_capacity = d ¬ØL ‚â•L = required_memory
101
102
103
Block size B
103
104
105
Embedding d
Theory (single query)
10%-50% error range
Theory (uniform, dim=5)
Theory (uniform, dim=20)
Figure 4: Behavior of the embedding dimension as a
function of block size for context length L = 220 ‚âà1
million (noise level œÉ2 = 1). Shaded region highlights
te range of d that exhibits 10%-50% empirical success.
Proposition 1 accurately captures the empirical behavior.
For the success of uniform AR, we need larger d as the
dimension of the query space S grows.
Secondly, Proposition (1) chooses a particular
long convolution where landmarks become the
mean of the input tokens within the block. In
practice, we can use a state-space model [16] to
parameterize convolution efficiently. A particu-
lar SSM choice of state dimension 1 is simply
using exponential smoothing. This yields the
following SSM variant of Proposition 1.
Proposition 2. Consider the setting of Propo-
sition 1 with the exponential smoothing filter
Fi = œÅi for i ‚â•0. Set œÅ = e‚àí1/B so that œÅB = e‚àí1.
Suppose d ‚â•50B(
p
log ¬ØL + t)2. Then, (LCAT)
solves AR with probability at least 1 ‚àí3e‚àít2/4.
Above, we fixed the decay rate œÅ for exposition
purposes. More generally, any œÅ choice with an
effective context size of O(B) would result in
similar guarantee.
6
Experiments
6.1
Model Evaluation on N-gram AR and Length Generalization Capability
For the synthetic experiments on associative recall problems, we employ the CAT architecture as
detailed in Section 3.1. We utilize convolution kernels with a width of W = 3 and explore model
embedding sizes of d = 32, 64, and 128 across MQAR and MQNAR problems to assess the impact
of model dimension on performance. In addition to the standard attention mechanism, we introduce a
perturbation strategy by implementing linear attention on the convoluted Q, K, and V embeddings,
referred to as LinCAT. We adhere strictly to the parameters set by [1]. Our experimental setup and
code are available on GitHub2. More detailed information on the training setup can be found in
Section A including the data generation and hyperparameters. For reporting results, we conduct each
experiment three times and present the maximum accuracy achieved across these runs, aligning with
the methodologies of [1] and [2].
As illustrated in Fig. 5, the CAT model consistently outperforms all baseline models across a range of
sequence lengths and model dimensions. Notably, both Mamba and Based models exhibit improved
performance as the model dimension increases, particularly with shorter sequence lengths. This
improvement is due to the memory-recall tradeoff [2] where models store and recall sequence
information more as their dimensionalities expand. In contrast, thanks to the short convolution, the
single-layer CAT model maintains 100% accuracy across all experimental settings, aligned with our
theorem 1. Interestingly, aside from CAT, Mamba is the only model demonstrating the potential to
effectively address the MQAR task within a single-layer network architecture. We will discuss this
observation in further detail in Section B.
Evaluation of Length Generalization. In Fig. 6, we train models with 128 sequence length (the
vertical red dashed line) and evaluate their performance on varying sequence lengths from 32 to
1,024. Fig. 6 shows the results of length generalization, which is aligned with our Theorem 2: CAT
models maintain 100% accuracy while all other models exhibit a sharp decline in performance as
the sequence length increases. This decrease is due to the increased demand of recall which requires
the model to store and retrieve more information as the sequence length grows. The CAT model,
however, is able to maintain its performance by leveraging the convolutional filters to shift the context
and retrieve the necessary information. Remarkably, in Fig. 5, we observe non-monotonic accuracy
behavior for Mamba and Attention-only models as a function of sequence length. This is due to the
2https://github.com/umich-sota/CAT
8

64
128
256
512
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
512
Sequence Length
64
128
256
512
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
64
128
256
512
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
512
Sequence Length
64
128
256
512
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
Figure 5: Evaluation of models on MQAR and MQNAR tasks with varying model dimensions and sequence
lengths. Model dimensions are 32, 64, 128 for each column of the figures, from left to right. Top: Models
trained on the MQAR setup. Bottom: Models trained on the MQNAR setup. Note that CAT models employ a
single-layer architecture, whereas all other models utilize two layers. Refer to Section 6.1 for detailed setup
descriptions.
32
64
128
256
512 1024
Test Length
0.00
0.25
0.50
0.75
1.00
Accuracy
 
32
64
128
256
512 1024
Test Length
 
32
64
128
256
512 1024
Test Length
 
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
32
64
128
256
512 1024
Test Length
0.00
0.25
0.50
0.75
1.00
Accuracy
 
32
64
128
256
512 1024
Test Length
 
32
64
128
256
512 1024
Test Length
 
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
Figure 6: Evaluation of models on length generalization. Model dimensions are 32, 64, 128 for each column of
the figures, from left to right. The models are trained with sequence length 128 (vertical red dashed lines) and
tested on varying test length. Top: Models trained on the MQAR. Bottom: Models trained on the MQNAR.
Note that CAT models establish length generalization aligned with Theorem 2 .
fact that these models are more sensitive and harder to optimize in AR problems. In Fig 6, we used a
denser hyperparameter grid and more trials to ensure smoother curves with better reproducibility.
6.2
Evaluations on Language Modeling
Based on the outcomes from the synthetic experiments, we further explore the efficacy of the CAT
model in real-world NLP tasks by integrating a 1D CAT structure into the Pythia [5] framework. We
pretrain the modified 370M-parameter model on the SlimPajama [42] dataset, involving 15 billion
tokens. We then assess the model on a variety of downstream zero-shot tasks, including Wikitext,
Lambada, Piqa, Hella, Winogrande, Arc-E, and Arc-C, a methodology commonly used in the field to
evaluate generalization capabilities across diverse tasks [5, 15, 1, 2]. The findings are compiled in
Table 2.
9

Table 2: Experiment results for model pretraining. ‚àóare results from [49], which uses a same dataset and
training procedure as ours. We use the same hyperparameters as [49] for fair comparison. For perplexity, lower
is better, and for accuracy, higher is better. The average accuracy in last column is calculated by averaging the
accuracy across all tasks but excluding the perplexity tasks. The best and second best results are highlighted in
boldface and underline, respectively.
Model
Wikitext
ppl‚Üì
Lambada_std
ppl‚Üì
Lambada_openai
ppl‚Üì
Lambada_std
acc‚Üë
Lambada_openai
acc‚Üë
Piqa
acc‚Üë
Hella
acc_norm‚Üë
Winogrande
acc‚Üë
Arc-E
acc‚Üë
Arc-C
acc_norm‚Üë
Avg
Acc‚Üë
Pythia
27.410
74.663
34.023
0.281
0.343
0.651
0.355
0.529
0.443
0.235
0.405
CAT, no PE
29.216
86.318
42.260
0.266
0.321
0.640
0.339
0.515
0.436
0.237
0.393
CAT, RoPE
26.776
65.423
38.557
0.288
0.341
0.654
0.362
0.507
0.461
0.239
0.407
MH-CAT, no PE
27.417
58.959
32.822
0.296
0.355
0.644
0.352
0.531
0.460
0.240
0.411
MH-CAT, RoPE
25.858
47.593
28.273
0.330
0.377
0.662
0.376
0.512
0.466
0.231
0.422
TF++ [46]‚àó
28.390
NA
42.690
NA
0.310
0.633
0.340
0.504
0.445
0.242
NA
Mamba [15]‚àó
28.390
NA
39.660
NA
0.306
0.650
0.354
0.501
0.463
0.236
NA
GLA [49]‚àó
28.650
NA
43.350
NA
0.303
0.648
0.345
0.514
0.451
0.227
NA
In this series of experiments, the CAT model is trained in two variants: one incorporating rotary
positional embedding [43] (PE) and another without positional embedding (noPE). We observe
that the CAT model with PE not only consistently outperforms the Pythia model but also achieves
performance better than state-of-the-art models, including Mamba [15], TF++ [46], and GLA [49].
Notably, the CAT model secures a superior perplexity gain compared to the standard model while
maintaining a similar level of parameters.
Regarding the noPE variant, training a Pythia model without positional encoding leads directly
to divergence and extremely large losses during training, affirming the critical role of positional
encoding in enabling standard transformer models to learn and converge. Intriguingly, despite the
absence of positional encoding, the CAT model still performs competitively with the leading models.
This suggests that the convolutional structure in the CAT model effectively captures positional
information within the data. We conjecture that the short convolutions provide positional information
for neighboring tokens, while the deep multi-layer network structure hierarchically aggregates this
information to establish long-range positional information.
This observation aligns with our synthetic experiment results, where the CAT model demonstrated
the capability to handle the AR task without positional encoding. These insights indicate that the
convolutional structure could potentially replace positional encoding, which might benefit length
extrapolation and generalization in the model. This offers a promising direction for further model
design and optimization in the field of NLP.
‚Ä¢Length Generalization Figure 1 presents the results from a length generalization experiment
with the CAT model, in which we trained the model on sequences of length 2,048 and assessed its
zero-shot performance on the Wikitext dataset across varying test sequence lengths. As a baseline
in our analysis, we implemented position interpolation (PI) [8] and YaRN [36] tempreture scaling
on RoPE models, including CAT/MH-CAT RoPE, to facilitate length generalization. The results
indicate that among the three RoPE models examined, the CAT model consistently demonstrates
excellent performance across all test sequence lengths. In contrast, the Pythia model exhibits a sharp
decline in performance as the sequence length increases. We suggest that is due to the additional
positional embeddings introduced by PI that was absent during the training phase. Despite this,
CAT models proficiently manage the relative positioning of tokens (especially overcome the new
positional embeddings by leveraging convolution information), which significantly boosts its ability
for length generalization. Additionally, the CAT model without PE is superior to the Pythia model
with RoPE, suggesting the effectiveness of the convolutional structure within the CAT model in
capturing essential positional data in length extrapolation.
6.3
Model Evaluation on Selective Copying
Fig. 7 displays the selective copying results for 1-layer and 2-layer models. We train these models
across a variety of model dimensions and sequence lengths. The models are required to copy 16
signal tokens from the input sequence and output them in the correct order. We observe that all
2-layer models perform well and show overlapping results, except for LinCAT. Among the 1-layer
models, CAT and Mamba achieve nearly 100% accuracy, while the performance of other models is
lower. These results are consistent with Theorem 4 and demonstrate that the 1-layer CAT model can
solve the selective copying problem without repetitions.
10

64
128
256
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
Sequence Length
64
128
256
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 1 layer
Based, 1 layer
Mamba, 1 layer
64
128
256
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
Sequence Length
64
128
256
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
Figure 7: Evaluation of models on selective copying tasks with varying model dimensions and sequence lengths.
Model dimensions are 32, 64, 128 for each column of the figures, from left to right. Top: Models trained on
1-layer architectures. Bottom: Models trained on 2-layer architectures. Note on 1-layer experiment, CAT and
Mamba achieve nearly 100% and their curves are overlapped.
7
Discussion
In this work, we have examined the synergy between the attention and convolution mechanisms
by introducing Convolution-Augmented Attention where K/Q/V embeddings are equipped with
convolution. We have shown that CAT layer enjoys strong theoretical guarantees when it comes to
AR and copying tasks with length generalization and also described insightful tradeoffs between the
need for attention and convolution. Importantly, real experiments confirm the benefit of CAT model
both in accuracy and in length generalization. Ultimately, we believe this work as well as the related
recent literature [15, 2, 38, 10] contributes to stronger design principles for the next generation of
(hybrid) architectures.
Limitations and future work. This work has a few shortcomings. We have only focused on
pretraining. However, Fig. 1 shows the potential of CAT in finetuning as a future direction. While
K/Q convolution helps in theoretical constructions for N-gram AR, in real experiments, they don‚Äôt
provide noticeable performance benefits. We suspect that K/Q convolution might be diluting the
attention scores and incorporating normalization or better parameterization can address this issue. An
important parameterization to explore is replacing the short convolutions within CAT with SSMs.
Finally, Section 5 introduced Landmark CAT as a sparse attention strategy. It would be interesting to
evaluate this proposal on real language modeling tasks.
Acknowledgements
This work was supported in part by the National Science Foundation grants CCF-2046816, CCF-
2403075, the Office of Naval Research award N000142412289, and gifts by Open Philanthropy and
Google Research.
References
[1] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri
Rudra, and Christopher R√©. Zoology: Measuring and improving recall in efficient language
models. arXiv:2312.04927, 2023.
[2] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley,
James Zou, Atri Rudra, and Christopher R√©. Simple linear attention language models balance
the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024.
11

[3] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin
token selection in attention mechanism. Advances in Neural Information Processing Systems,
36:48314‚Äì48362, 2023.
[4] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast
weights to attend to the recent past. Advances in neural information processing systems, 29,
2016.
[5] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O‚ÄôBrien,
Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward
Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In
International Conference on Machine Learning, pages 2397‚Äì2430. PMLR, 2023.
[6] Emmanuel Candes and Terence Tao. Near optimal signal recovery from random projections:
Universal encoding strategies?, 2006.
[7] Emmanuel J Candes. The restricted isometry property and its implications for compressed
sensing. Comptes rendus. Mathematique, 346(9-10):589‚Äì592, 2008.
[8] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context
window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595,
2023.
[9] Liam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, and Sanjay Shakkottai.
In-context learning with transformers: Softmax attention adapts to function lipschitzness. arXiv
preprint arXiv:2402.11639, 2024.
[10] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms
through structured state space duality. arXiv preprint arXiv:2405.21060, 2024.
[11] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with
gated convolutional networks. In International conference on machine learning, pages 933‚Äì941.
PMLR, 2017.
[12] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru,
Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin:
Mixing gated linear recurrences with local attention for efficient language models. arXiv
preprint arXiv:2402.19427, 2024.
[13] Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis.
On the
optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680,
2023.
[14] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R√©.
Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint
arXiv:2212.14052, 2022.
[15] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.
arXiv preprint arXiv:2312.00752, 2023.
[16] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R√©. On the parameterization and
initialization of diagonal state space models. Advances in Neural Information Processing
Systems, 35:35971‚Äì35983, 2022.
[17] Albert Gu, Karan Goel, and Christopher R√©. Efficiently modeling long sequences with structured
state spaces. arXiv preprint arXiv:2111.00396, 2021.
[18] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured
state spaces. Advances in Neural Information Processing Systems, 35:22982‚Äì22994, 2022.
[19] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and
Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022.
[20] M Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, and Samet Oymak. From
self-attention to markov models: Unveiling the dynamics of generative transformers. arXiv
preprint arXiv:2402.13512, 2024.
[21] Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. Repeat after me:
Transformers are better than state space models at copying. arXiv preprint arXiv:2402.01032,
2024.
12

[22] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and
Yoshua Bengio. Gated orthogonal recurrent units: On learning to forget. Neural computation,
31(4):765‚Äì783, 2019.
[23] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva
Reddy. The impact of positional encoding on length generalization in transformers. Advances
in Neural Information Processing Systems, 36, 2024.
[24] Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. Time-parameterized convo-
lutional neural networks for irregularly sampled time series. arXiv preprint arXiv:2308.03210,
2023.
[25] Yingcong Li, Yixiao Huang, Muhammed E Ildiz, Ankit Singh Rawat, and Samet Oymak.
Mechanics of next token prediction with self-attention. In International Conference on Artificial
Intelligence and Statistics, pages 685‚Äì693. PMLR, 2024.
[26] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolu-
tional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022.
[27] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May,
Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and
inference with unlimited context length. arXiv preprint arXiv:2404.08801, 2024.
[28] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan
May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint
arXiv:2209.10655, 2022.
[29] Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji
Kim, and Michael Gastpar. Attention with markov: A framework for principled analysis of
transformers via markov chains. arXiv preprint arXiv:2402.04161, 2024.
[30] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context
length for transformers. NeurIPS, 2023.
[31] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with
gradient descent. arXiv preprint arXiv:2402.14735, 2024.
[32] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and
induction heads. arXiv preprint arXiv:2209.11895, 2022.
[33] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan
Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In
International Conference on Machine Learning, pages 26670‚Äì26698. PMLR, 2023.
[34] Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the
role of attention in prompt-tuning. In International Conference on Machine Learning, pages
26724‚Äì26768. PMLR, 2023.
[35] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak,
Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative
study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024.
[36] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context
window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.
[37] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher R√©. Hyena hierarchy: Towards larger convolutional
language models. In International Conference on Machine Learning, pages 28043‚Äì28078.
PMLR, 2023.
[38] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj√∂rn Deiseroth, Kristian
Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R√©, et al. Mechanistic design
and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024.
[39] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases
enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.
[40] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba:
Simple hybrid state space models for efficient unlimited context language modeling. arXiv
preprint arXiv:2406.07522, 2024.
13

[41] David Slepian. The one-sided barrier problem for gaussian noise. Bell System Technical Journal,
41(2):463‚Äì501, 1962.
[42] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan
Dey. Slimpajama: A 627b token cleaned and deduplicated version of redpajama, 2023.
[43] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
[44] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Trans-
formers as support vector machines. arXiv preprint arXiv:2308.16898, 2023.
[45] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du.
Joma: De-
mystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint
arXiv:2310.00535, 2023.
[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
[48] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally
parameterized convolutions for efficient inference. Advances in neural information processing
systems, 32, 2019.
[49] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear
attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.
14

64
128
256
512
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
512
Sequence Length
64
128
256
512
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 1 layer
Based, 1 layer
Mamba, 1 layer
Figure 8: Performance of 1-layer models on MQAR tasks with varying model dimension and
sequence length. Noted that all models are trained using 1-layer architecture.
A
Detailed Experiment Setup
A.1
Associative Recall Experiments
We first introduce the training setup for the synthetic experiments. In our MQAR and MQNAR
experiments, we create a dataset with a vocabulary size of 8,092 to ensure that the vocabulary
replicates the scope of real language data. The dataset is constructed as described in Sec. 3.2.1, with
varying sequence lengths L of 64, 128, and 256, and 512. Specifically, we formulate the dataset in
the form of key-value pairs accompanied by multiple queries where the keys are unique within each
sequence. For each example, we initially select k keys from the vocabulary without replacement and
subsequently draw the values from the remaining vocabulary. We then randomly shuffle the keys
and associated values to form the input sequence. The number of queries is set to match k, ensuring
each key in the sequence is queried. It should be noted that while the keys are unique within a single
example, they may be repeated across different examples. For sequence lengths of L = 64, 128,
256, and 512, we set k = 16, 32, 64, and 128 respectively, indicating that the number of keys and
queries scales with the sequence length, thus increasing the task complexity. We generate 100,000
training examples and 3,000 testing examples for each of the sequence lengths. For NAR experiment,
we primarily focus on N = 2 to evaluate the performance. We construct the dataset similarly to
the MQAR task with sequence lengths of 64, 128, and 256. Consequently, the number of keys and
queries is reduced to k = 10, 20, 40 respectively, to accommodate the larger N. We generate 200,000
training examples and 3,000 testing examples for each sequence length.
For the training, we adhere strictly to the parameters set by [1], and their experimental setup and code,
using learning rate sweep among 0.001, 0.01, 0.1 and train the model for 64 epoches. The maximum
accuracy achieved across these learning rate is reported.
We remark that for the length generalization experiments, we sweep the learning rate among
0.001, 0.003, 0.01, 0.03, 0.1 and report the maximum accuracy over 5 runs to ensure the robust-
ness and reproducibility of the results.
A.2
Language Modeling Experiments
For the language modeling experiments, we exactly follow the setup from [49]. For the length
generalization experiment, we train the model on sequences of length 2,048 and assess its zero-shot
performance on the Wikitext dataset across varying test sequence lengths.
B
Additional Experiments
We conduct additional Experiments, Fig. 8 and 9 shows the result of 1-layer models on 1-gram and
2-gram MQNAR tasks with varying hidden sizes and sequence lengths. The model dimension is set
to 32, 64, and 128 for each column of the figures, from left to right. All other models perform much
worse compare to their 2-layer counterparts. Fig. 10 and 11 show the length generalization results
of 1-layer models on 1-gram and 2-gram MQNAR tasks. The results are consistent with the 2-layer
models.
15

64
128
256
512
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
512
Sequence Length
64
128
256
512
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 1 layer
Based, 1 layer
Mamba, 1 layer
Figure 9: Performance of 1-layer CAT models on 2-gram MQNAR tasks with varying hidden sizes
and sequence length. All models are trained using 1-layer architecture.
32
64
128
256
512 1024
Test Length
0.00
0.25
0.50
0.75
1.00
Accuracy
 
32
64
128
256
512 1024
Test Length
 
32
64
128
256
512 1024
Test Length
 
CAT, 1 layer
LinCAT, 1 layer
Attention, 1 layer
Based, 1 layer
Mamba, 1 layer
Figure 10: 1-gram Length generalization
C
Proofs on Associative Recall and Selective Copying
C.1
Proof of Theorem 1
Proof. Given an N-gram Z ‚ààRN√ód, let us define s(Z) = norm(P
i‚àà[N] FN‚àíizi) to be its signature.
We will first show that for almost all F, each N-gram admits a unique signature. To see this, let
A, Z ‚ààRN√ód be two distinct N-grams. Let us write the difference between their signatures as a
correlation coefficient. Set s‚Ä≤(Z) = P
i‚àà[N] FN‚àíizi. Note that if s(A) = s(Z), we would have the
following function of F that arises from correlation coefficient as zero:
gA,Z(F) = (s‚Ä≤(Z)‚ä§s‚Ä≤(A))2 ‚àí‚à•s‚Ä≤(Z)‚à•2
‚Ñì2‚à•s‚Ä≤(A)‚à•2
‚Ñì2.
Now, observe that g is a fourth-order polynomial of the entries of F ‚ààRN and we can expand g(F)
further as follows
g(F) = (
X
i‚àà[N]
X
j‚àà[N]
FN‚àíiFN‚àíja‚ä§
i z j)2 ‚àí‚à•
X
i‚àà[N]
FN‚àíiai‚à•2
‚Ñì2‚à•
X
i‚àà[N]
FN‚àíizi‚à•2
‚Ñì2.
(1)
Above, let ci be the coefficient of the fourth moment term F4
N‚àíi. Note that
ci = (a‚ä§
i zi)2 ‚àí‚à•ai‚à•2
‚Ñì2‚à•zi‚à•2
‚Ñì2.
Since A , Z, there exists i ‚àà[N] such that ai , zi. This implies that ci , 0 and g(F) is a nonzero
polynomial. As a result, g(F) , 0 almost everywhere implying the same for s(Z) , s(A). Since there
32
64
128
256
512 1024
Test Length
0.00
0.25
0.50
0.75
1.00
Accuracy
 
32
64
128
256
512 1024
Test Length
 
32
64
128
256
512 1024
Test Length
 
CAT, 1 layer
LinCAT, 1 layer
Attention, 1 layer
Based, 1 layer
Mamba, 1 layer
Figure 11: 2-gram Length generalization
16

are finitely many N-grams, repeating the same argument for all N-gram pairs, we find that all N-gram
signatures are unique for almost all F.
Next, suppose we have an F resulting in unique signatures. We will prove the ability of CAT layer to
solve the N-AR problem. Consider an arbitrary sequence X and denote the last N tokens by Z. Let
X‚àó= norm(X ‚àóF) be the convolved sequence and let q be the final token of X‚àó. By assumption, q
repeats exactly twice in the sequence. Let Œ± be the position of the q in the sequence. By definition, the
target token v = xŒ±+1. Let Ii ‚ààRL be the indicator function that has 1 at position i and 0 everywhere
else. Since all N-grams are unique and their signatures have unit norm, we have that
lim
c‚Üí‚àûS(cX‚àóq) = s‚àó(X) := IL + IŒ±
2
.
(2)
Above we use the standard fact that softmax will saturate at the top entry as the inverse-temperature
goes to infinity. For the purposes of length generalization, we provide the precise temperature
requirement. Let a, b be two vectors in the normalized N-gram token set SN (the set of tokens
obtained after convolving with F). Over all such a, b, define the minimum cosine distance to be
‚àÜ= 1 ‚àímax
a,b‚ààSN a‚ä§b.
Given sequence X‚àó, using the worst case likelihood ratios of e‚àí‚àÜbetween the two q-tokens vs the
remaining L ‚àí2 non-q N-grams tokens, for any X, we have that
‚à•map(X, c) ‚àís‚àó(X)‚à•‚Ñì1 = ‚à•S(cX‚àóq) ‚àís‚àó(X)‚à•‚Ñì1 ‚â§
2(L ‚àí2)e‚àíc‚àÜ
2 + (L ‚àí2)e‚àíc‚àÜ.
(3)
To make the right hand side ‚â§Œµ/2 for all (admissible) sequences X of length at most L, we need
2(L ‚àí2)e‚àíc‚àÜ‚â§Œµ which implies c ‚â•‚àÜ‚àí1 log( 2(L‚àí2)
Œµ
).
Value delay. For value delay, we will use (3) as key and query embeddings use the same filter. Let
Xv = 2 ¬∑ X ‚àóD‚àí1. Using the fact that rows of Xv are unit norm, for c ‚â•‚àÜ‚àí1 log( 2(L‚àí2)
Œµ
)
‚à•X‚ä§
v map(X, c) ‚àíX‚ä§
v s‚àó(X)‚à•‚Ñì2 ‚â§2‚à•S(cX‚àóq) ‚àís‚àó(X)‚à•‚Ñì1 ‚â§Œµ.
Next, note that
X‚ä§
v s‚àó(X) = X‚ä§
v (IL + IŒ±
2
) = vŒ± + vL
2
.
Now observe that, thanks to ‚àí1 delay, vŒ±
=
2xŒ±+1 and vL
=
2xL+1
=
0 resulting in
limc‚Üí‚àûX‚ä§
v map(X, c) = xŒ±+1. Combining the above results, we find that ‚à•V‚ä§map(X, c) ‚àív‚à•‚Ñì2 ‚â§Œµ for
all X.
Key delay. In this scenario, we are delaying X‚àóforward by one. Because of this, we have Xk = X‚àó‚àóD1
and, within Xk, q appears in positions Œ± + 1 and L + 1. Since the latter is out of bounds, repeating the
argument (3) and defining map(X, c) := S(cXkq), for any sequence X, we find that
‚à•S(cXkq) ‚àíIŒ±+1‚à•‚Ñì1 ‚â§
2(L ‚àí1)e‚àíc‚àÜ
1 + (L ‚àí1)e‚àíc‚àÜ.
Similarly, the right hand side is upper bounded by Œµ, whenever c ‚â•‚àÜ‚àí1 log( 2(L‚àí1)
Œµ
).
To conclude, using the fact that tokens are unit norm and the target value vector is v = xŒ±+1, for any
X, we obtain
‚à•X‚ä§map(X, c) ‚àív‚à•‚Ñì2 ‚â§‚à•S(cX‚àóq) ‚àíIŒ±+1‚à•‚Ñì1 ‚â§Œµ,
completing the proof that ‚à•X‚ä§map(X, c) ‚àív‚à•‚Ñì2 for all X of length at most L.
Concluding the proof of the theorem statement. So far, we have concluded that, for all input
sequences X, CAT layer output guarantees ‚à•f(X) ‚àív‚à•‚Ñì2 < Œµ0 where v is the target value token and
Œµ0 is under our control by choosing c = ‚àÜ‚àí1 log(2L/Œµ0). Since we assume the minimum distance
between distinct token embeddings are Œµ, to accurately and uniquely decode the target v, we choose
Œµ0 = Œµ/2 and apply nearest neighbor on f(X) to conclude.
‚ñ°
17

C.2
Proof of Theorem 2
In this section, we will use the shorthand F to denote the value filter Fv for notational simplicity.
Recall that R0 > 0 is an absolute constant throughout the proof. Finally, the constant R used in
Theorem 2‚Äôs statement will be subsumed within the O(¬∑) notation below.
Lemma 1. Consider the same setting in Theorem 2. For any f = (W, F) that can solve the AR
problem defined in Def. 2 up to Œµ-accuracy on all sequences of length L ‚â•3, if Œµ0 := Œµ/‚àÜ‚â§1/8, we
have that
‚à•F ‚àíD‚àí1‚à•‚Ñì1 ‚â§O(WŒµ0(1 + LŒµ0) + LŒµ0) ‚â§O(LŒµ0(1 + WŒµ0))
(4)
‚à•F‚â•0‚à•‚Ñì1 =
W
X
i=0
Fi ‚â§O(Œµ0(1 + LŒµ0))
(5)
where we use O(¬∑) notation to denote an upper bound up to a constant i.e. for some absolute r > 0,
O(x) ‚â§r ¬∑ x. Moreover, let q be a token within vocabulary V and v be the top query not equal to q
that maximizes the similarity v‚ä§Wq i.e. v = arg maxx‚ààV,x,q x‚ä§Wq, we have
oq = sv/sq ‚â§Œì =
Œµ0
1 ‚àí4Œµ0
= O(Œµ0)
(6)
where sq and sv are the softmax values for q and v.
Proof. Throughout, we assume Œµ ‚â§‚àÜ
8 and ‚àÜ> 0 where ‚àÜis the minimum embedding distance, i.e.,
‚àÜ= (1 ‚àímaxa,b‚ààV(a‚ä§b)2)1/2. Before proceeding, we first note that, without losing generality, we
can assume L ‚â•W + 1. The reason is that, if L ‚â§W, left or right end of the convolutional filter will
never interact with features. Thus, we simply set them to zero, truncating the filter. Define sequence
Xi ‚ààRL√ód where xi
L‚àí1 = q, xi
i = q and xi
j = v for all j , i. Let Zi = F ‚àóXi. Let si = S(XiWq) and
sq = si
L‚àí1 and sv = (1 ‚àí2sq)/(L ‚àí2). Here sq and sv are the softmax values for q and v respectively.
Additionally, observe that
sv/sq = exp((v ‚àíq)‚ä§Wq).
Finally, let I = [L] ‚àí{L ‚àí1, i} and recalling value sequence Zi, note that
f(Xi) = sv
X
j‚ààI
zi
j + sq(zi
i + zi
L).
By assumption, we also have that
‚à•v ‚àíf(Xi)‚à•‚Ñì2 ‚â§Œµ
for
i < L ‚àí2,
‚à•q ‚àíf(XL‚àí2)‚à•‚Ñì2 ‚â§Œµ.
(7)
We will leverage these inequalities to prove the statement of the theorem. Let œÅ = œÅ(q, v) = q‚ä§v be
the correlation between q, v. Define v‚ä•=
q‚àíœÅv
‚à•q‚àíœÅv‚à•‚Ñì2 . Observe that convolution output has the form
f(Xi) = Œ±v + Œ≤q for some Œ± = Œ±i, Œ≤ = Œ≤i > 0. For i < L ‚àí2, we have that
Œµ ‚â•‚à•v ‚àíf(Xi)‚à•‚Ñì2 ‚â•|(v‚ä•)‚ä§(v ‚àíf(Xi))| ‚â•Œ≤|(v‚ä•)‚ä§q| ‚â•Œ≤
q
1 ‚àíœÅ2.
Recalling that the minimum embedding distance is defined as ‚àÜ=
p
1 ‚àímaxq,v œÅ2(q, v) ‚â§1 and
setting Œµ0 = Œµ/‚àÜ, this implies that
Œ≤i ‚â§Œµ0 := Œµ/‚àÜ
for
i < L ‚àí2,
Œ±L‚àí2 ‚â§Œµ0 := Œµ/‚àÜ.
(8)
Additionally, writing Œµ ‚â•|v‚ä§(v ‚àíf(Xi))| = |1 ‚àíŒ±i ‚àíŒ≤iv‚ä§q| for i < L ‚àí2 and using |v‚ä§q| ‚â§1, we can
deduce
Œ±i ‚â•1 ‚àí(1 + 1/‚àÜ)Œµ ‚â•1 ‚àí2Œµ0
for
i < L ‚àí2,
Œ≤L‚àí2 ‚â•1 ‚àí(1 + 1/‚àÜ)Œµ ‚â•1 ‚àí2Œµ0
(9)
Œ±i ‚â§1 + (1 + 1/‚àÜ)Œµ ‚â§1 + 2Œµ0
for
i < L ‚àí2,
Œ≤L‚àí2 ‚â§1 + (1 + 1/‚àÜ)Œµ ‚â§1 + 2Œµ0.
(10)
We note that when L = W + 1, the problem only has a subtle difference, which we discuss at the end.
Case 1: L ‚â•W + 2. For i = 0 and i = L ‚àí2, the coefficients Œ±i, Œ≤i can be written in terms of
convolution as
Œ≤0 = 2sqF0 + sv
X
i,0
Fi
(11)
Œ≤L‚àí2 = sq(2F0 + F‚àí1 + F1) + sv[2
X
i<0
Fi ‚àí(F‚àí1 + F‚àíW)].
(12)
18

Let ¬ØF1 = F‚àí1 + F1. Observing 2 P
i<0 Fi ‚àí(F‚àí1 + F‚àíW) ‚â§2(P
i,0 Fi), we can write
1 ‚àí(1 + 1/‚àÜ)Œµ ‚â§Œ≤L‚àí2 ‚â§sq ¬ØF1 + 2Œ≤0 ‚â§sq ¬ØF1 + 2Œµ/‚àÜ.
(13)
Combining these implies sq ¬ØF1 ‚â•1 ‚àí4Œµ0. Also, we know the trivial bound sq ¬ØF1 ‚â§Œ≤L‚àí2 ‚â§1 + 2Œµ0.
Thus, we obtain
1 + 2Œµ0 ‚â•sq ¬ØF1 ‚â•1 ‚àí4Œµ0.
To proceed, we wish to prove that sv is small. From (11), we have that sv ¬ØF1 ‚â§Œµ0. Consequently, we
have that
sv
sq
‚â§Œì =
Œµ0
1 ‚àí4Œµ0
.
Using 2sq + (L ‚àí2)sv = 1, we get
1 = 2sq + (L ‚àí2)sv ‚â§(2 + (L ‚àí2)Œì)sq =‚áísq ‚â•
1
2 + (L ‚àí2)Œì =
1 ‚àí4Œµ0
2 + (L ‚àí10)Œµ0
.
Since sq ‚â§1/2 (due to query repeating twice), this also implies that
2(1 + LŒµ0)(1 + 2Œµ0)
1 ‚àí4Œµ0
‚â•2(1 + 2Œµ0)(1 + (L/2 ‚àí5)Œµ0)
1 ‚àí4Œµ0
‚â•¬ØF1 ‚â•2(1 ‚àí4Œµ0).
Using above, in essence, so far we have established that | ¬ØF1 ‚àí2| ‚â§O(LŒµ0) and sv/sq ‚â§O(Œµ0). Both
statements hold whenever Œµ0 ‚â§1/8 (e.g. so that 1/(1 ‚àí4Œµ0) ‚â§1 + O(Œµ0)). The primary remaining
item in the proof is establishing |Fi| ‚â§O(Œµ0) for all i , ‚àí1.
To prove this, we utilize the following observations: First, by keeping track of the contributions of
the last two q vectors on Œ±L‚àí2, we observe that
sq
W
X
i=1
Fi ‚â§Œ±L‚àí2 ‚â§Œµ0.
This implies PW
i=1 Fi ‚â§Œµ0/sq ‚â§Œµ0
2+(L‚àí10)Œµ0
1‚àí4Œµ0
= O(Œµ0(1+ LŒµ0)). We similarly find F0 ‚â§Œµ0/2sq through
(11). Finally, since F1 ‚â§O(Œµ0(1 + LŒµ0)) ‚â§O(LŒµ0), we also find the critical bound
|F‚àí1 ‚àí2| ‚â§O(LŒµ0).
Finally, we wish to bound P
i‚â§‚àí2 Fi. To do so, we can bound the contribution of the first q vector on
Œ≤i as follows. For any W ‚â•j ‚â•2, letting i = L ‚àí1 ‚àíj, we have that
Œµ0 ‚â•Œ≤i ‚â•sqF‚àíj =‚áíF‚àíj ‚â§Œµ0
2 + (L ‚àí10)Œµ0
1 ‚àí4Œµ0
= O(Œµ0(1 + LŒµ0)).
Aggregating these, we have found the advertised bounds:
‚à•F ‚àíD‚àí1‚à•‚Ñì1 ‚â§O(WŒµ0(1 + LŒµ0) + LŒµ0) ‚â§O(LŒµ0(1 + WŒµ0))
(14)
‚à•F‚â•0‚à•‚Ñì1 =
W
X
i=0
Fi ‚â§O(Œµ0(1 + LŒµ0))
(15)
oq = sv/sq ‚â§Œì =
Œµ0
1 ‚àí4Œµ0
= O(Œµ0)
(16)
where v is chosen to be the most similar token in terms of attention probabilities. Note that, the bound
on left entries of F that retrieves the past values is tighter than the right entries.
Case 2: L = W + 1. In this scenario, the main difference is we have the following estimates rather
than (11)
Œ≤0 = sq(2F0 + FW + F‚àíW) + sv
X
i,0,|i|<W
Fi
(17)
Œ≤L‚àí2 = sq(2F0 + ¬ØF1) + sv[2
X
i<0
Fi ‚àí(F‚àí1 + F‚àíW)].
(18)
19

So we can‚Äôt immediately use the estimate provided right below (13) because of the missing F‚àíW sv
term. On the other hand, considering X1 and contribution of the first v token on Œ≤1, we have that
svF‚àíW ‚â§Œ≤1 ‚â§Œµ0. As a result, we can instead use the fact that Œ≤0 + Œ≤1 ‚â§O(Œµ0) and the fact that
2sqF0 + sv(2
X
i<0
Fi ‚àí(F‚àí1 + F‚àíW)) ‚â§2(Œ≤0 + Œ≤1)
so that we have again established |1 ‚àísq ¬ØF1| ‚â§O(Œµ0) and can proceed similarly.
‚ñ°
Now that we have established the fine-grained control of the filter and attention map with Lemma 1,
we can conclude with length generalization.
Proof of Theorem 1. Given a query q and a sequence of length L‚Ä≤, let us define sq similarly (i.e. at-
tention probability that falls on the q token) and study the attention output. Let q appear at i for the
first time, v be the token following q, and I = [L‚Ä≤] ‚àí{i, L‚Ä≤ ‚àí1}. Let a = S(XWq) ‚ààRL‚Ä≤ be softmax
scores with ai = aL‚Ä≤‚àí1 = sq. We write
f(X) =
X
j‚ààI
ajz j + sq(zi + zL‚Ä≤‚àí1).
where z j = PW
i=‚àíW Fixj‚àíi To proceed, let R be a universal constant and Œû = RLŒµ0(1 + WŒµ0) so that
‚à•F‚à•‚Ñì1 ‚â§2 + Œû from (14) in Lemma 1. Then we get ‚à•zj‚à•‚Ñì2 ‚â§‚à•F‚à•‚Ñì1 ‚â§2 + Œû for all j ‚àà[L‚Ä≤]. Secondly,
due to right-clipped convolution we have ‚à•zL‚Ä≤‚àí1‚à•‚Ñì2 ‚â§‚à•PW
i=0 Fi‚à•‚Ñì1 ‚â§Œû and thanks to value retrieval at
i‚Äôth position, we get
‚à•zi ‚àí2v‚à•‚Ñì2 ‚â§|F‚àí1 ‚àí2|‚à•v‚à•‚Ñì2 + |
X
j,‚àí1
F j| ‚â§Œû
(19)
Next, observe that aj/sq ‚â§sv/sq ‚â§Œì =
Œµ0
1‚àí4Œµ0 for all j ‚ààI and that 2sq + P
j‚ààI aj = 1, consequently,
for some constant R0 > 0,
1
2 ‚â•sq ‚â•
1
2 + (L‚Ä≤ ‚àí2)Œì =
1 ‚àí4Œµ0
2 + (L‚Ä≤ ‚àí10)Œµ0
=‚áí|2sq ‚àí1| ‚â§R0L‚Ä≤Œµ0.
and
X
j‚ààI
aj = 1 ‚àí2sq ‚â§R0L‚Ä≤Œµ0.
Aggregating these, we find that
‚à•f(X) ‚àív‚à•‚Ñì2
(a)
‚â§‚à•
X
j‚ààI
ajz j‚à•‚Ñì2 + ‚à•sq(zi + zL‚Ä≤‚àí1) ‚àí2sqv‚à•‚Ñì2 + |2sq ‚àí1|‚à•v‚à•‚Ñì2
(20)
(b)
‚â§|
X
j‚ààI
a j|(2 + Œû) + ‚à•sq(zi + zL‚Ä≤‚àí1) ‚àí2sqv‚à•‚Ñì2 + |2sq ‚àí1|
(21)
‚â§R0(2 + Œû)L‚Ä≤Œµ0 + Œû + R0L‚Ä≤Œµ0
(22)
‚â§3R0L‚Ä≤Œµ0 + Œû + R0ŒûL‚Ä≤Œµ0
(23)
‚â§3Œµ0
 R0L‚Ä≤ + RL(1 + WŒµ0)(1 + R0L‚Ä≤Œµ0)
(24)
where (a) follows triangle inequality and (b) follows Cauchy-Schwarz inequality. Let c0, c1 be
absolute constants to be determined. Assuming WŒµ0 ‚â§O(1) (i.e. bounded by constant), we have that
‚à•f(X) ‚àív‚à•‚Ñì2 ‚â§c0Œµ0(L‚Ä≤ + L + LL‚Ä≤Œµ0)
where c0 ‚â•3 max{R0, R(1 + WŒµ0), R0R(1 + WŒµ0)}. Assuming the stronger bound LŒµ0 ‚â§O(1) and
c1 ‚â•c0(1 + L/L‚Ä≤ + LŒµ0), we have that
‚à•f(X) ‚àív‚à•‚Ñì2 ‚â§c1Œµ0L‚Ä≤
This concludes the advertised proof.
‚ñ°
20

C.3
Proving Length Generalization for N-gram AR (Proposition 3)
In this section we use Fv = F for the filter applied on value token and Fq = Fk = ¬ØF for filters on
query and keys.
Assumption 1. Recall that V is the vocabulary from which the token embeddings are drawn. We
have the following two assumptions to make the output f(X) more tractable:
a) The filter weights are bounded and obey ‚à•F‚à•‚Ñì1 ‚â§1. Besides, assuming that ‚àÜ= 1 ‚àí
maxa,b‚ààV,a,b b‚ä§a > 0
b) Any subset of 2N tokens within the vocabulary V is linearly independent.
Note that Assumption 1.b is essentially a restricted isometry property condition on the embedding
matrix induced by the vocabulary V. Specifically, if embeddings are randomly chosen, as soon as
the embedding dimension obeys d ‚â≥O(N log |V|
N ), this assumption will hold with high probability
[7, 6]. In the following analysis, we will leverage either one of the assumptions to establish the length
generalization result.
Lemma 2. Suppose Assumption 1.b holds. Let B be any subset of 2N tokens within V and U :=
{uj| j ‚àà[|U|]} be the orthonormal tokens obtained after applying the Gram-Schmidt process on B
where bj = P j
l=0 Œ≤j,lul. Then we have 0 < Œ¥ = min j‚àà[|B|] |Œ≤j, j| ‚â§1.
Proof. First note that Œ≤j,l = b‚ä§
j ul ‚â§1 for any j, l ‚àà[|B|] and Œ≤0,0 = 1. Then Œ¥ = minj‚àà[|B|] |Œ≤j,j| ‚â§1.
Moreover, we can prove Œ¥ > 0 by contradiction. Assuming there exists j ‚â•1 such that Œ≤j, j = 0. This
indicates that bj can represented as a linear combination of the previously orthogonalized vectors
{u0, . . . , u j‚àí1}. In other words, bj lies entirely in the span of these previous vectors. This contradicts
the fact that tokens in B are linearly independent. As a result we have Œ¥ > 0.
‚ñ°
Proposition 3. Let ¬ØF ‚ààRN be a 1-D causal convolutional filter and F ‚ààR2W+1
+
be a 1-D convolutional
filter from time t = ‚àíW to t = W where W ‚â§L ‚àíN. Suppose that token embeddings have unit
norm. Consider the same CAT Layer f(X) = (XvWv)‚ä§S(XkWkW‚ä§
q q) defined in Theorem 1 where
q is the final token of Xq and Xq = norm(X ‚àóFq) ‚ààRL√ód (same for Xk, Xv). We set Fq = Fk = ¬ØF,
W = WkW‚ä§
q , and Wv = 2Id Consider any f = (W, F) that can solve the N-AR problem up to
Œµ-accuracy on all sequences of length L ‚â•O(N). That is, for all (X, y) where N-gram Z occurs
within X exactly twice and y being the associated value token that follows the first occurrence of
Z, we have ‚à•y ‚àíf(X)‚à•‚Ñì2 ‚â§Œµ. Let B is any subset of 2N tokens within vocabulary V and Œ≤j, j be the
corresponding projection coefficients defined in Lemma 2. Assume either Assumption 1.a or 1.b holds
and define
Œµ0 =
Ô£±Ô£¥Ô£≤Ô£¥Ô£≥
Œµ/‚àÜ,
‚àÜ= 1 ‚àímaxa,b‚ààV,a,b b‚ä§a
under Assumption 1.a
Œµ e2N/Œ¥
Œ¥ ,
Œ¥ = min j‚àà[|B|] |Œ≤j, j|
under Assumption 1.b
For almost all choices of ¬ØF, there are absolute constants R0, R > 0 such that, if Œµ0 ‚â§R0/L, we have
that
‚Ä¢ ‚à•F ‚àíD‚àí1‚à•‚Ñì1 ‚â§LŒµ0
‚Ä¢ Let s‚ãÜ‚ààRL‚Ä≤ be a vector with entries equal to 1/2 at the positions of query q in Xq and 0
otherwise. For all inputs X of arbitrary length L‚Ä≤, attention map obeys ‚à•S(XkWq) ‚àís‚ãÜ‚à•‚Ñì1 ‚â§
L‚Ä≤Œµ0.
‚Ä¢ For all N-AR sequences X of arbitrary length L‚Ä≤, we have that ‚à•y ‚àíf(X)‚à•‚Ñì2 ‚â§RL‚Ä≤Œµ0.
Lemma 3. Consider the same setting in Prop. 3, for any f = (W, F) that can solve the N-AR problem
defined in Def. 3 up to Œµ-accuracy on all sequences of length L ‚â•O(N). There are absolute constants
R0 > 0 such that, if Œµ0 ‚â§R0/N, we have that
‚à•F ‚àíD‚àí1‚à•‚Ñì1 ‚â§O(NŒµ0(1 + LŒµ0) + NŒµ0) ‚â§O(LŒµ0(1 + NŒµ0))
(25)
‚à•F‚â•0‚à•‚Ñì1 =
W
X
i=0
Fi ‚â§O(NŒµ0(1 + LŒµ0))
(26)
21

where we use O(¬∑) notation to denote an upper bound up to a constant i.e. for some absolute r > 0,
O(x) ‚â§r ¬∑ x. Moreover, we consider N-gram Zq ‚ààRN√ód that ends with a token q‚Ä≤, which can be any
token from the vocabulary BN. Let q be the final token of norm(Zq ‚àó¬ØF) and v be the top query not
equal to q that maximizes the similarity v‚ä§Wq. i.e. v = arg maxx‚ààBN,x,q x‚ä§Wq, we have
oq = sv
sq
‚â§Œì =
O(Œµ0)
1 ‚àíO(NŒµ0) ‚â§O(NŒµ0)
(27)
where sq and sv are the softmax values for q and v.
Proof. Following the proof of Thoerem 1, suppose that we have an ¬ØF that results in unique signatures.
We argue that the length generalization fails when W > L ‚àíN, which is explained at the end.
Throughout, we assume that W = L ‚àíN. When W < L ‚àíN, it is equivalent to the setting where
W = L ‚àíN and F j = 0 for W + 1 ‚â§|j| ‚â§L ‚àíN. Denote the corresponding N-gram that results in q
and v after convolving with ¬ØF be Zq = [q0, q1, . . . , qN‚àí1] ‚ààRN√ód and Zv = [v0, v1, . . . , vN‚àí1] ‚ààRN√ód
respectively, i.e., q = norm(Zq‚àó¬ØF) and v = norm(Zv‚àó¬ØF). Zq and Zv are unique due to the assumption
on ¬ØF. For brevity, let q‚Ä≤ = qN‚àí1, v‚Ä≤ = vN‚àí1 and Zk
v‚Ä≤ = [v‚Ä≤, v‚Ä≤, . . . , v‚Ä≤] ‚ààRk√ód, Z‚Ä≤
q = [q1, . . . , qN‚àí1] ‚àà
R(N‚àí1)√ód, where q0 is removed from Zq.
Xi,k =
h
ZN‚àí1+i
v‚Ä≤
Zv
Zq
ZN‚àí1+k
v‚Ä≤
Z‚Ä≤
q
q0
ZN‚àí1
v‚Ä≤
Zni,k
v‚Ä≤
Zv
Zq
i
‚ààRL√ód
(28)
¬ØXi,k =
h
ZN‚àí1+i
v‚Ä≤
Zv
Zq
q0
ZN‚àí1+k
v‚Ä≤
Z‚Ä≤
q
ZN‚àí1
v‚Ä≤
Zni,k
v‚Ä≤
Zv
Zq
i
‚ààRL√ód
(29)
where ni,k = L ‚àí8N + 3 ‚àíi ‚àík ‚â•0, and this naturally introduces a lower bound for L, i.e., L ‚â•O(N)
and upper bounds for both i and k. Note that Xi,k and ¬ØXi,k have different labels. By assumption, we
have that
‚à•v‚Ä≤ ‚àíf(Xi,k)‚à•‚Ñì2 ‚â§Œµ,
‚à•q0 ‚àíf( ¬ØXi,k)‚à•‚Ñì2 ‚â§Œµ.
(30)
Let si,k = S(Xi,kWq), ¬Øsi,k = S( ¬ØXi,kWq). Define the probability of selecting the j-th entry of Xi,k and
¬ØXi,k as si,k
j and ¬Øsi,k
j and selecting the token q and v as sq, sv. Here we omit i, k for sq and sv since it‚Äôs
invariant to the values of i, k. Additionally, observe that
sv/sq = exp((v ‚àíq)‚ä§Wq) and (L ‚àí2)sv + 2sq ‚â•1
where the inequality comes from the fact that v = arg maxx‚ààBN,x,q x‚ä§Wq.
We will lever-
age these inequalities to prove the statement of the theorem. Define the vocabulary set B =
{v0, v1, . . . , vN‚àí1, q0, q1, . . . , qN‚àí1} which includes all tokens in Xi,k and ¬ØXi,k. Note that the vocab-
ulary B is a subset of tokens chosen from V, i.e., B ‚äÜV and the vocabulary size |B| is at most
2N, i.e., |B| := K ‚â§2N. Observe that convolution output has the form f(Xi,k) = P
j‚àà[|B|] mi,k
j bj and
f( ¬ØXi,k) = P
j‚àà[|B|] ¬Ømi,k
j b j where {mi,k
j , ¬Ømi,k
j }j‚àà[|B|] are non-negative coefficients due to the assumption that
entries in ¬ØF and softmax probabilities si,k and ¬Øsi,k are non-negative. In particular, we are interested
in mi,k
q , ¬Ømi,k
q and mi,k
v , ¬Ømi,k
v , which correspond to the coefficients of token q0 and v‚Ä≤. To proceed, we
leverage Assumption 1 to bound the coefficients:
When Assumption 1.a holds. By expanding the coefficients, we get
X
j‚àà[|B|]
mi,k
j =
X
j‚àà[|B|]
X
t‚àà[L]
Fvt ‚àíjsi,k
t
‚â§‚à•F‚à•‚Ñì1
X
t‚àà[L]
si,k
t
‚â§1
(31)
Combining this with the fact that
Œµ ‚â•‚à•v‚Ä≤ ‚àíf(Xi,k)‚à•‚Ñì2 ‚â•|v‚Ä≤‚ä§(v‚Ä≤ ‚àíf(Xi,k))| ‚â•|
X
j‚àà[|B|],bj,v‚Ä≤
v‚Ä≤‚ä§bjmi,k
j + mi,k
v ‚àí1|
(32)
, we have
Œµ ‚â•
X
j‚àà[|B|],bj,v‚Ä≤
(1 ‚àív‚Ä≤‚ä§bj)mi,k
j ‚â•(1 ‚àív‚Ä≤‚ä§bj)mi,k
j
for any
j ‚àà{j | j ‚àà[|B|], bj , v‚Ä≤}
(33)
‚Üími,k
j ‚â§Œµ/‚àÜ:= Œµ0
for any
j ‚àà{j | j ‚àà[|B|], bj , v‚Ä≤}
(34)
22

where ‚àÜ= 1 ‚àímaxa,b‚ààB,a,b b‚ä§a > 0. In terms of mi,k
v , we apply Triangle Inequality on (32) and (34):
|1 ‚àími,k
v | ‚â§|
X
j‚àà[|B|],bj,v‚Ä≤
v‚Ä≤‚ä§bjmi,k
j + mi,k
v ‚àí1| + |
X
j‚àà[|B|],b j,v‚Ä≤
v‚Ä≤‚ä§b jmi,k
j |
(35)
‚â§Œµ + 2NŒµ0 ‚â§(2N + 1)Œµ0
(36)
Similarly for ¬ØXi,k we have
¬Ømi,k
j ‚â§Œµ0
for any
j ‚àà{j | j ‚àà[|B|], b j , q0},
|1 ‚àí¬Ømi,k
q | ‚â§O(NŒµ0)
(37)
When Assumption 1.b holds. Based on the linear independence property, we can apply the
Gram‚ÄìSchmidt process to transform the tokens in B to orthonormal tokens U = {uj| j ‚àà[|U|]}
where bj = P j
l=0 Œ≤j,lul where Œ≤j,l = b‚ä§
j ul. Since the order of tokens in U does not matter, we can
set u0 = v‚Ä≤. Then for any j ‚â•1, uj is orthogonal to v‚Ä≤ and bi for all i < j. Consider the case of Xi,k
whose label is v‚Ä≤, utilizing the orthogonality we get
Œµ ‚â•‚à•v‚Ä≤ ‚àíf(Xi,k)‚à•‚Ñì2 ‚â•|u‚ä§
j (v‚Ä≤ ‚àíf(Xi,k))| ‚â•|
|B|‚àí1
X
l= j
mi,k
l u‚ä§
j bl|
(38)
Using backward induction, we can then bound mi,k
j for 1 ‚â§j ‚â§K‚àí1. First consider j = |B|‚àí1 = K‚àí1.
Then we have:
Œµ ‚â•|mK‚àí1u‚ä§
K‚àí1bK‚àí1| = |mK‚àí1Œ≤K‚àí1,K‚àí1| ‚â•|mK‚àí1Œ¥|
(39)
where Œ¥ = min j‚àà[|B|] |Œ≤j, j| = minj‚àà[|B|] |b‚ä§
j u j|. Following Lemma 2 we have 0 < Œ¥ ‚â§1. As a result we
get mK‚àí1 ‚â§Œµ/Œ¥. Next we prove that if j ‚â•1 and mi,k
l
‚â§Œµ (1+1/Œ¥)K‚àíl‚àí1
Œ¥
for j < l ‚â§K‚àí1, mj ‚â§Œµ (1+1/Œ¥)K‚àíj‚àí1
Œ¥
.
When 1 ‚â§j ‚â§K ‚àí2, from equation (38) we can derive
Œµ ‚â•|
K‚àí1
X
l=j
mi,k
l u‚ä§
j bl| = |
K‚àí1
X
l=j+1
mi,k
l u‚ä§
j bl + mju‚ä§
j bj|
(40)
For the first term we have
|
K‚àí1
X
i=j+1
miu‚ä§
j bi| ‚â§
K‚àí1
X
i=j+1
mi ‚â§
K‚àíj‚àí2
X
i=0
Œµ(1 + 1/Œ¥)i
Œ¥
= Œµ (1 + 1/Œ¥)K‚àíj‚àí1 ‚àí1
Using Triangle Inequality we get
|mju‚ä§
j bj| ‚â§Œµ(1 + 1/Œ¥)K‚àíj‚àí1 ‚Üímj ‚â§Œµ(1 + 1/Œ¥)K‚àíj‚àí1
Œ¥
‚â§Œµe
K‚àíj‚àí1
Œ¥
Œ¥
‚â§Œµe2N/Œ¥
Œ¥
(41)
We can hereby bound mi,k
j for any j ‚àà{j | bj ‚ààB, bj , v‚Ä≤}. Let Œµ1 := Œµ e2N/Œ¥
Œ¥ , we have
mi,k
j ‚â§Œµ1
for any
j ‚àà[|B|], bj , v‚Ä≤
(42)
Additionally, writing Œµ ‚â•|v‚Ä≤‚ä§(v‚Ä≤ ‚àíf(Xi,k))| = |1 ‚àími,k
v ‚àív‚Ä≤‚ä§P
j‚àà[|B|],bj,v‚Ä≤ mi,k
j bj| and using |v‚Ä≤‚ä§bj| ‚â§1
for any bj ‚ààB, we can deduce
|1 ‚àími,k
v | ‚â§Œµ +
K‚àí1
X
i=1
mi
(43)
‚â§Œµ +
K‚àí2
X
i=0
Œµ(1 + 1/Œ¥)i
Œ¥
(44)
‚â§Œµ(1 + 1/Œ¥)K‚àí1 ‚â§Œµ1
(45)
Similarly for ¬ØXi,k, we have
¬Ømi,k
j ‚â§Œµ1
for any
j ‚àà[|B|], b j , q0,
|1 ‚àí¬Ømi,k
q | ‚â§Œµ1
(46)
23

To summarize, using Assumption 1, we can have an upper bound on {mi,k
j , ¬Ømi,k
j } j‚àà[|B|]:
mi,k
j ‚â§Œµ0
for any
j ‚àà{j | j ‚àà[|B|], b j , v‚Ä≤},
|1 ‚àími,k
v | ‚â§O(NŒµ0)
(47)
¬Ømi,k
j ‚â§Œµ0
for any
j ‚àà{j | j ‚àà[|B|], b j , q0},
|1 ‚àí¬Ømi,k
q | ‚â§O(NŒµ0)
(48)
where
Œµ0 :=
Ô£±Ô£¥Ô£≤Ô£¥Ô£≥
Œµ/‚àÜ,
‚àÜ= 1 ‚àímaxa,b‚ààB,a,b b‚ä§a > 0
under Assumption 1.a
Œµ e2N/Œ¥
Œ¥ ,
Œ¥ = min j‚àà[|B|] |Œ≤j, j|
under Assumption 1.b
(49)
We proceed by comparing mi,k
q and ¬Ømi,k
q :
mi,k
q = 2sq
 F‚àíL+4N+i‚àí2 + F‚àí2N+1‚àík + 2FN‚àí1 + FL‚àí5N‚àíi‚àík + FL‚àí2N‚àíi

(50)
+ 2
X
j‚àà[L]‚àí{3N‚àí2+i,L‚àí1}
si,k
j (F‚àíL+N+ j + F‚àí5N‚àíi‚àík+j+3 + F‚àí2N‚àíi+ j+1)
(51)
¬Ømi,k
q = 2sq
 F‚àíL+4N+i‚àí2 + F‚àí1 + 2FN‚àí1 + FL‚àí3N‚àíi + FL‚àí2N‚àíi

(52)
+ 2
X
j‚àà[L]‚àí{3N‚àí2+i,L‚àí1}
¬Øsi,k
j (F‚àíL+N+j + F‚àí3N‚àíi+ j+1 + F‚àí2N‚àíi+j+1)
(53)
Observing that
‚Ä¢ si,k
j = ¬Øsi,k
j for j ‚àà[3N ‚àí1 + i] and 6N ‚àí3 + i + k ‚â§j ‚â§L ‚àí1
‚Ä¢ si,k
j+1 = ¬Øsi,k
j for 4N ‚àí1 + i + k ‚â§j ‚â§5N ‚àí3 + i + k
‚Ä¢ si,k
j+(2N‚àí1) = ¬Øsi,k
j for 5N ‚àí2 + i + k1 ‚â§j ‚â§6N ‚àí4 + i + k
‚Ä¢ si1,k1
j+2N‚àí2+k1+i1‚àíi2 = ¬Øsi2,k2
j
for 3N ‚àí1 + i2 ‚â§j ‚â§4N ‚àí2 + i2
Utilizing these observations, we have
X
j‚àà[3N‚àí1+i]
2¬Øsi,k
j (F‚àíL+N+j + F‚àí3N‚àíi+ j+1 + F‚àí2N‚àíi+j+1) ‚â§mi,k
q + mi+N,k
q
X
j‚àà[6N‚àí3+i+k,L‚àí1]
2¬Øsi,k
j (F‚àíL+N+j + F‚àí3N‚àíi+ j+1 + F‚àí2N‚àíi+j+1) ‚â§mi,k
q + mi,k‚àíN
q
X
4N‚àí1+i+k‚â§j‚â§5N‚àí3+i+k
2¬Øsi,k
j (F‚àíL+N+j + F‚àí3N‚àíi+ j+1 + F‚àí2N‚àíi+j+1) ‚â§mi+1,k
q
+ mi‚àíN+1,k
q
+ mi,k+1
q
X
5N‚àí2+i+k‚â§j‚â§6N‚àí4+i+k
2¬Øsi,k
j (F‚àíL+N+j + F‚àí3N‚àíi+ j+1 + F‚àí2N‚àíi+j+1) ‚â§mi+(N‚àí1),k
q
+ mi+(2N‚àí1),k
q
+ mi,k+(2N‚àí1)
q
X
3N‚àí1+i‚â§j‚â§4N‚àí2+i
2¬Øsi,k
j (F‚àíL+N+j + F‚àí3N‚àíi+ j+1 + F‚àí2N‚àíi+j+1) ‚â§mi‚àí(2N‚àí2),k
q
+
X
3N‚àí1+i‚â§j‚â§4N‚àí2+i
¬Øsi,k
j (F‚àí3N‚àíi+j+1 + F‚àí2N‚àíi+j+1)
(a)
‚â§mi‚àí(2N‚àí2),k
q
+ sv
X
l‚àà[2N]
Fl
(b)
‚â§mi‚àí(2N‚àí2),k
q
+
X
j‚àà{j | bj=¬Øxi,k
l ,l‚àà[2N]}
¬Øm1,k
j
where (a) comes from v = arg maxx‚ààBN,x,q x‚ä§Wq and (b) comes from the attention from the first v to
itself and its previous 2N ‚àí1 terms. Let i = 2N ‚àí2, k = N, combining the inequalities above, we get
1 ‚àíO(NŒµ0) ‚â§¬Øm2N‚àí2,N
q
‚â§2sq(F‚àí1 + FL‚àí5N+2) + m2N‚àí2,N
q
+ m3N‚àí2,N
q
+ ... + m0,N
q
+
X
j‚àà{j | bj=¬Øxi,k
l ,l‚àà[2N]}
¬Øm1,N
j
(54)
‚â§2sq(F‚àí1 + FL‚àí5N+2) + O(NŒµ0)
(55)
24

Combining these we get sq(F‚àí1 + FL‚àí5N+2) ‚â•1/2 ‚àíO(NŒµ0). Moreover, from (52), we have sq(F‚àí1 +
FL‚àí5N+2) ‚â§¬Ømi,k
q /2 ‚â§1/2 + O(NŒµ0), which results in
|sq(F‚àí1 + FL‚àí5N+2) ‚àí1/2| ‚â§O(NŒµ0)
(56)
Based on this, we wish to show that sv is small. Substituting l ‚àà{4N ‚àí4, L ‚àíN} into (51), we have
sv(F‚àí1 + FL‚àí5N+2) ‚â§m2N‚àí2,N
q
‚â§Œµ0, which implies that
sv
sq
‚â§Œì =
O(Œµ0)
1/2 ‚àíO(NŒµ0)
(57)
Using 2sq + (L ‚àí2)sv ‚â•1, we have
1 ‚â§2sq + (L ‚àí2)sv ‚â§(2 + (L ‚àí2)Œì)sq =‚áísq ‚â•
1
2 + (L ‚àí2)Œì ‚â•
1 ‚àíO(NŒµ0)
2 + (L ‚àí2N ‚àí2)O(Œµ0)
(58)
Combining this with 2sq ‚â§1, we get
(1 + LO(Œµ0))(1 + O(NŒµ0))
1 ‚àíO(NŒµ0)
‚â•(1 + O(NŒµ0))(1 + (L/2 ‚àíN ‚àí1)O(Œµ0))
1 ‚àíO(NŒµ0)
(59)
‚â•F‚àí1 + FL‚àí5N+2 ‚â•1 ‚àíO(NŒµ0)
(60)
At this stage, we have already proved that when NŒµ0 ‚â§O(1), |F‚àí1 + FL‚àí5N+2 ‚àí1| ‚â§O(LŒµ0) and
sv/sq ‚â§O(Œµ0). The primary remaining proof is to prove |Fl| < O(Œµ0) for all l , ‚àí1. Since ¬Ømi,k
j ‚â§Œµ0
for b j , q0, by tracking the contribution of the last q on ¬Ømi,k
j , we have
sq
X
0‚â§l‚â§L‚àíN,l<{N‚àí1,L‚àí2N,L‚àí3N}
Fl ‚â§
X
j‚àà{j | bj‚ààB,bj,q0}
¬Øm0,0
j
‚â§O(NŒµ0)
(61)
sq
X
l‚àà{L‚àí2N,L‚àí3N}
Fl ‚â§
X
j‚àà{ j | bj‚àà{q‚Ä≤,v‚Ä≤}}
¬Øm1,0
j
‚â§Œµ0
(62)
sqFN‚àí1 ‚â§m0,0
q
‚â§Œµ0
(63)
Combining these three inequalities and W
=
L ‚àíN, we get PW
l=0 Fl
‚â§
O(NŒµ0)/sq
‚â§
O(NŒµ0) 2+(L‚àí2N‚àí2)O(Œµ0)
1‚àíO(NŒµ0)
‚â§O(NŒµ0(1+LŒµ0)). Additionally, since FL‚àí5N+2 ‚â§O(NŒµ0(1+LŒµ0)) ‚â§O(NLŒµ0),
we get
|F‚àí1 ‚àí1| < O(NLŒµ0)
(64)
Finally, we want to bound P
‚àíL+N‚â§j‚â§‚àí2 F j. By tracking the contribution of the first q to ¬Øm0,0
j
for any
j ‚àà{j | bj ‚ààB, bj , q0}, we have sqFl ‚â§¬Øm0,0
j
where b j = xi,k
‚àíl+3N‚àí2 for any ‚àíL + 3N ‚àí1 ‚â§l ‚â§‚àí2.
Summing over l we get,
sq
X
‚àíL+3N‚àí1‚â§l‚â§‚àí2,l,W‚àí4N+3
Fl ‚â§
X
j‚àà{ j | bj‚ààB,bj,q0}
¬Øm0,0
j
‚â§O(NŒµ0)
(65)
Note that F‚àíL+4N‚àí2 touches q0 for ¬ØX0,0 so we need to handle it separately. We can consider ¬Øm1,0
j
instead and get
sqF‚àíL+4N‚àí2 ‚â§¬Øm1,0
L‚àíN+1 ‚â§Œµ0
(66)
For P
‚àíL+N‚â§j‚â§‚àíL+3N‚àí2 F j, we consider the following example:
ÀÜXi,k =
h
Zq
v0
ZN‚àí1+i
v‚Ä≤
Zv
ZN‚àí1+k
v‚Ä≤
Z‚Ä≤
q
ZN‚àí1
v‚Ä≤
Zni,k
v‚Ä≤
Zq
i
‚ààRL√ód
(67)
Similarly we define ÀÜsi,k = S( ÀÜXi,kWq) and the probability that selects q and v as ÀÜsq and ÀÜsv. Note that
ÀÜsv/ÀÜsq = sv/sq ‚â§O(Œµ0) and (L‚àí2)ÀÜsv+ ÀÜsq ‚â•1. We can then obtain the exact lower bound as sq for ÀÜsq, i.e.,
ÀÜsq ‚â•
1‚àíO(NŒµ0)
2+(L‚àí2N‚àí2)O(Œµ0). Note that the output of f( ÀÜXi,k) can also be written as f( ÀÜXi,k) = P
bj‚ààB ÀÜmi,k
j bj where
{ ÀÜmi,k
j }b j‚ààB is the corresponding coefficients. Moving forward, by tracking the attendance of the first q
to the last 2N ‚àí1 terms, we have ÀÜsqFl ‚â§ÀÜm0,0
j
where bj = ÀÜxi,k
‚àíl+i+N‚àí1 for any ‚àíL+ N ‚â§j ‚â§‚àíL+3N ‚àí2.
25

Repeating the same argument based on Assumption 1, we get ÀÜm0,0
j
‚â§Œµ0 for any j ‚àà{j|bj ‚ààB, b j , v0},
which leads to
ÀÜsq
X
‚àíL+N‚â§l‚â§‚àíL+3N‚àí2
Fl ‚â§ÀÜsq
X
j‚àà{ j | bj‚ààB,bj,v0}
ÀÜm0,0
j
‚â§O(NŒµ0)
(68)
Combining (65), (66) and (68), we have
X
‚àíL+N‚â§l‚â§‚àí2
Fl ‚â§O(NŒµ0)(1/ÀÜsq + 1/sq) ‚â§O(NŒµ0)2 + (L ‚àí2N ‚àí2)O(Œµ0)
1 ‚àíO(NŒµ0)
‚â§O(NŒµ0(1 + LŒµ0))
(69)
In summary, we get
‚à•F ‚àíD‚àí1‚à•‚Ñì1 ‚â§O(NŒµ0(1 + LŒµ0) + NŒµ0) ‚â§O(LŒµ0(1 + NŒµ0))
(70)
‚à•F‚â•0‚à•‚Ñì1 =
W
X
i=0
Fi ‚â§O(NŒµ0(1 + LŒµ0))
(71)
oq = sv
sq
‚â§Œì =
O(Œµ0)
1 ‚àíO(NŒµ0) ‚â§O(NŒµ0)
(72)
where v is chosen to be the most similar tokens to q in terms of attention probabilities. Now
we discuss the scenario where W > L ‚àíN. First note that the output f(X) can be written as
f(X) = 2 P
l‚àà[L] sl
PW
j=‚àíW F jxl‚àíj where sl is the softmax value at l-th position. We are interested in
sq in particular, which is proven to converge to 1/2 when Œµ diminishes. Also, note that the smallest
possible index of q is N ‚àí1 since it‚Äôs the last token of an N-gram. Then, when W > L ‚àíN, the left
end of the convolutional filter never interacts with sq since the index of xi‚àíj is out of bound, i.e.,
i ‚àíj = N ‚àí1 + W > L ‚àí1
‚ñ°
Using the results from Lemma 3, we can establish the length generalization on N-AR task.
Proof of Proposition 3. Given a sequence X of length L‚Ä≤, let q be the last token of Xq = Xk =
norm(X ‚àó¬ØF) and we define sq as the attention probability that selects q. Assume the first occurrence
of q in Xk is i and q‚Ä≤ = xi. By definition, the target vector v is the token following q‚Ä≤ in X, i.e.,
v = xi+1. Let I = [L‚Ä≤] ‚àí{i, L‚Ä≤ ‚àí1}. Let a = S(XkWq) ‚ààRL‚Ä≤ be the softmax probabilities where
ai = aL‚Ä≤‚àí1 = sq
f(X) =
X
j‚ààI
ajzj + sq(zi + zL‚Ä≤‚àí1)
(73)
where zj = 2 PW
i=‚àíW Fix j‚àíi. We define R as a universal constant and Œû = RLŒµ0(1 + NŒµ0) such that
‚à•F‚à•‚Ñì1 ‚â§1 + Œû from Lemma 3. Then we get ‚à•z j‚à•‚Ñì2 ‚â§2‚à•F‚à•‚Ñì1 ‚â§2(1 + Œû) for all j ‚àà[L‚Ä≤]. Note that
aj/sq ‚â§sv/sq = Œì =
O(Œµ0)
1‚àíO(NŒµ0) for all j ‚ààI and that 2sq + P
j‚ààI a j = 1. As a result, there exists some
constant R0 > 0 such that
1
2 ‚â•sq ‚â•
1
2 + (L‚Ä≤ ‚àí2)Œì =
1 ‚àíO(NŒµ0)
2 + (L‚Ä≤ ‚àí2N ‚àí2)O(Œµ0) =‚áí|2sq ‚àí1| ‚â§R0L‚Ä≤Œµ0
(74)
and
X
j‚ààI
aj = 1 ‚àí2sq ‚â§R0L‚Ä≤Œµ0
(75)
Moreover, due to right-clipped convolution, we have ‚à•zL‚Ä≤‚àí1‚à•‚Ñì2 = 2‚à•PW
i=0 Fi‚à•‚Ñì1 ‚â§2Œû. Next, according
to the value retrieval at i-th position, we have
‚à•zi ‚àí2v‚à•‚Ñì2 ‚â§|2F‚àí1 ‚àí2|‚à•v‚à•‚Ñì2 + 2|
X
j,‚àí1
F j| ‚â§2Œû
(76)
26

Utilizing these findings above, we get
‚à•f(X) ‚àív‚à•‚Ñì2 ‚â§‚à•
X
j‚ààI
a jzj‚à•‚Ñì2 + ‚à•sq(zi + zL‚Ä≤‚àí1) ‚àí2sqv‚à•‚Ñì2 + |2sq ‚àí1|‚à•v‚à•‚Ñì2
(77)
‚â§|
X
j‚ààI
aj| max
j
‚à•zj‚à•‚Ñì2 + sq(‚à•zi ‚àí2v‚à•‚Ñì2 + ‚à•zL‚Ä≤‚àí1‚à•‚Ñì2) + |2sq ‚àí1|
(78)
‚â§2R0L‚Ä≤Œµ0(1 + Œû) + 2Œû + R0L‚Ä≤Œµ0
(79)
‚â§3R0L‚Ä≤Œµ0 + 2Œû + 2R0ŒûL‚Ä≤Œµ0
(80)
‚â§3Œµ0(R0L‚Ä≤ + 2RL(1 + NŒµ0)(1 + R0L‚Ä≤Œµ0))
(81)
Let c0, c1 be absolute constants to be determined. Assuming NŒµ0 ‚â§O(1) (i.e. bounded by constant),
we have that
‚à•f(X) ‚àív‚à•‚Ñì2 ‚â§c0Œµ0(L‚Ä≤ + L + LL‚Ä≤Œµ0)
where c0 ‚â•3 max{R0, 2R(1 + NŒµ0), 2R0R(1 + NŒµ0)}. Assuming the stronger bound LŒµ0 ‚â§O(1) and
c1 ‚â•c0(1 + L/L‚Ä≤ + LŒµ0), we have that
‚à•f(X) ‚àív‚à•‚Ñì2 ‚â§c1Œµ0L‚Ä≤
This concludes the advertised results.
‚ñ°
C.4
Proof of Theorem 4
Below we state a generalization of Theorem 4 which distinguishes two scenarios: Short convolution
with PE and Long convolutions with no PE.
Theorem 4 (Selective Copy). Consider the setting of Def. 4. There is a 1-layer CAT f using
exponential-decay query-convolution Fq as follows:
‚Ä¢ Suppose Fq is infinitely long (namely parameterized as an SSM with state matrix A = œÅ for
some decay parameter œÅ < 1). Then, using d = |S| + 3 dimensional token embeddings, f
solves unique selective copying.
‚Ä¢ Suppose Fq ‚ààRN and input sequences contain at most N signal tokens. Using d = |S| + 4
and 1-D positional encodings, f solves unique selective copying.
Proof. Let T be the maximum context length the model encounters. Specifically, T = L + N + 1
where L is the maximum length of the input sequence X that precedes the special token ‚ä•and N
is the maximum number of signal tokens in X. Recall that the cardinality of the signal vocabulary
S is allowed to be larger than N and we resume generation until outputting all signal tokens. Let
Z = [X ‚ä•zL+2 . . . zt] denote the current input sequence where [X ‚ä•] is the initial input that kickstarts
decoding. Denote boldface Z, X to be the embedded sequences of Z, X. We use query convolution
thus the CAT model is given by f(Z) = nearest_token_embedding(Z‚ä§S(ZWz‚àó
t )) where Z‚àó= Fq ‚àóZ
is the convolved sequence and z‚àó
t is the last token of the convolved query sequence for L + 1 ‚â§t ‚â§T.
We set convolution to be Fq,i = œÅi for 0 ‚â§i < W for a suitable œÅ ‚â§1 to be determined where W is
the window size of the convolution. This choice aggregates the current token and the W ‚àí1 most
recent tokens and allows for all all-ones filter as a special case. For the first statement of the theorem
W = ‚àûwhereas for the second statement W = N.
The choice of token embeddings. We construct the token embeddings as follows:
‚Ä¢ Token embedding of the ith token has the form xi = [x‚Ä≤
i, si, pi]. Here
‚Äì Base embedding. x‚Ä≤
i is the base embedding vector associated to the discrete token
value xi. We choose these x‚Ä≤
i embeddings to have unit Euclidean norm.
‚Äì Signal indicator. si ‚ààR is an indicator of whether the token is a signal token or not.
We set si = 1 for signal tokens and the ‚ä•token and si = 0 for noise tokens.
‚Äì Position encoding. pi ‚ààR is the positional encoding of the i‚Äôth token. We simply set
pi = i/T where T = L + N + 1. pi is only required for short convolution i.e. when
W = N.
‚Ä¢ The base embeddings of noise tokens N are orthogonal to that of signal tokens and ‚ä•token.
27

‚Ä¢ The base embeddings of signal tokens S and ‚ä•are also orthogonal to each other.
Let Dnoisy be the dimension of the subspace spanned by the base embeddings of noise tokens. We
can choose Dnoisy = 1 by setting all base embeddings for the noise tokens to be identical. The signal
tokens and ‚ä•token occupies an orthogonal |S| + 1 dimensional subspace. Together, this recovers the
embedding dimensions advertised in the theorem statement, namely
‚Ä¢ With positional encoding and W = N: We need an embedding dimension of d = |S| +
Dnoisy + 3 ‚â•|S| + 4 where two additional dimension is due to si and pi.
‚Ä¢ Without positional encoding and W = ‚àû: We need an embedding dimension of d =
|S| + Dnoisy + 2 ‚â•|S| + 3 where the additional dimension is due to si.
‚Ä¢ Construction of the CAT model. We construct a one layer CAT model with the following criteria
in the order of priority:
1. The model should always select signal tokens.
2. The model should select a signal token not previously selected.
3. The model should select the farthest signal token from the current/last token (i.e. generates
signal tokens that are closer to the start of the sequence).
To satisfy the three criteria above, we pick the attention weights W as follows when W = N:
W =
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚àíŒ±IN+1+Dnoisy
0
0
0
Œ≤
0
0
0
‚àíŒ∏
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª.
(82)
The choice for W = ‚àûis same except that we do not have the positional encoding coefficient Œ∏.
Recall that we also choose the convolutional filter as Fq,i = œÅi for 0 ‚â§i < W for œÅ < 1. Specifically,
we choose œÅ = 2‚àí1/T so that œÅT = 1/2. This choice guarantees that œÅi ‚àíœÅi+1 ‚â•c/T for all 0 ‚â§i < T
for some absolute constant c > 0.
We will accomplish the proof inductively. Suppose that X contains N‚Ä≤ unique signal tokens and that
until time t for some L + 1 ‚â§t ‚â§L + N‚Ä≤ + 1, the model outputs the correct sequence of t‚Ä≤ = t ‚àíL ‚àí1
unique signal tokens. We will prove that it will accurately output the next signal token in line with
suitable choice of Œ±, Œ≤, Œ∏. To this end, we state the following lemma regarding the output of the
query-convolution z‚àó
t .
Note that z‚àó
t = Pt
i=1 œÅizt‚àíi. Recall that zL+1 to zt are unique correctly-ordered signal tokens where we
set z0 = ‚ä•. Denote the rest of the N‚Ä≤ ‚àít‚Ä≤ signal tokens with correct order by q1 to qN‚Ä≤‚àít‚Ä≤. Here q1 is
the left most such token in X and the token we wish to output next. We can write z‚àó
t in terms of signal
tokens and noise tokens as follows:
z‚àó
t =
t‚Ä≤+1
X
i=1
bizt‚àíi + n +
N‚Ä≤‚àít‚Ä≤
X
j=1
ajq j,
(83)
where we set bi := œÅi. Here the first term Pt‚Ä≤+1
i=1 bizt‚àíi is due to last t‚Ä≤ + 1 tokens (including ‚ä•) that
are already generated. The n term denotes the aggregated contribution of the noise tokens to the
convolution. PN‚Ä≤‚àít‚Ä≤
j=1 a jqj is the contributions of the signal tokens that are yet to be generated. Crucially
note that,
‚Ä¢ If W = ‚àû, ai is strictly increasing because convolution coefficients Fq,j are strictly decreasing
(with a gap lower bounded by c/T).
‚Ä¢ Whether W = N or W = ‚àû, bi = œÅi is strictly decreasing and bt‚Ä≤+1 ‚â•aN‚Ä≤‚àít‚Ä≤ + c/T. That is,
the contribution of any token already generated is larger than any token that is yet to be
generated.
28

Let us write z‚àó
t = [z‚Ä≤‚àó
t s p]. Note that s ‚â•1/2 because ‚ä•token is involved in the convolution and
œÅT = 1/2. Similarly, if we employ PE, we have that p ‚â•(L + 1)/2T ‚â•1/4 for the same reason. Given
a token xi = [x‚Ä≤
i si pi], through (82), we have that
scorei = x‚ä§
i Wz‚àó
t = ‚àíŒ± 
z‚Ä≤‚àó
t , x‚Ä≤
i
 + Œ≤ssi ‚àíŒ∏ppi.
(84)
We now proceed with the proof which relies on choosing Œ±, Œ≤, Œ∏ > 0 in a suitable fashion. Specifically,
we will choose their relative ratios Œ≤/Œ±, Œ±/Œ∏ suitably to ensure the desired token q1 receives the
highest score. After ensuring this, we can suitably scale up Œ±, Œ≤, Œ∏ in a proportional fashion, which
will also scale up the scores of each token. Thanks to softmax attention, this will ensure that the
model precisely retrieves the token with the highest score.
Scenario 1: W = ‚àû. In this scenario, we don‚Äôt use PE, thus, effectively Œ∏ = 0. We need to satisfy
aforementioned criteria: First, we want the highest score to be a signal token. We will guarantee this
by observing si = 0 for noise tokens, s > 0 and by setting Œ≤/Œ± ‚â´1. Second, we want the highest
score to be q1, the left most signal token that has not been output yet. Now, since W = ‚àû, q1 receives
the lowest coefficient of a1 in (83). Using orthogonality and unit Euclidean norm, this implies that
D
z‚Ä≤‚àó
t , q‚Ä≤
1
E
= a1. In contrast, any other signal token has a larger inner product by at least c/T. Choosing
Œ± = 1 (and then suitably scaling it up together with Œ≤), this implies that, q1 is indeed the token with
the highest score that will be generated next.
Scenario 2: W = N and we employ PE. We again follow the score decomposition (84). Observe that
D
z‚Ä≤‚àó
t , x‚Ä≤
i
E
, ssi, ppi are all bounded by 1 in absolute value. Thus, by controlling the relative ratios of
the scalar knobs Œ≤ > Œ± > Œ∏ = 1, we can enforce the three criteria listed above. Recall that q1 denotes
the next signal token we wish to output next. We will prove that q1 achieves the strictly highest score
amoung the tokens of Z. To proceed, set Œ≤/Œ± ‚â´1 and Œ±/Œ∏ ‚â´1.
‚Ä¢ Since Œ≤ dominates Œ± and Œ∏, following the same argument in Scenario 1, noise tokens will
have strictly lower scores than signal tokens, thus cannot be generated next.
‚Ä¢ Following (84), the signal tokens have a score contribution of ‚àíŒ± ¬∑ bi or ‚àíŒ± ¬∑ aj from the
inner product term
D
z‚Ä≤‚àó
t , x‚Ä≤
i
E
. Here bi denoted the coefficient of a generated signal token
whereas aj denoted the coefficient of a missing signal token. Next recall from (83) that
bi ‚â•c/T > 0 and bi ‚â•aj + c/T thanks to the Fq choice. Since Œ± dominates Œ∏, this implies
that the generated signal tokens have strictly less score than the missing signal tokens.
‚Ä¢ Finally, we wish to show that q1 has the highest score among missing signal tokens. First,
recall from (83) that a1 is the smallest coefficient among the missing signal tokens. As a
result, it achieves the largest inner product score ‚àíŒ±¬∑
D
z‚Ä≤‚àó
t , x‚Ä≤
i
E
. To complete the proof, we use
positional encoding to break any score ties. Since q1 is the left most missing signal token,
any other missing signal token will achieve a strictly worse position encoding score ‚àíŒ∏ppi
as p ‚â•1/4, Œ∏ = 1, and pi = i/T is strictly increasing. This guarantees that q1 achieves the
strictly highest score as desired.
To summarize, by choosing suitable Œ≤ ‚â´Œ± ‚â´Œ∏ = 1 and proportionally scaling up Œ±, Œ≤, Œ∏ sufficiently,
we conclude with the proof.
‚ñ°
D
Proofs for Section 5 ‚Äì Convolution-Attention Tradeoff
D.1
Proof of Proposition 1
The key to the proof is establishing the detection threshold of the correct block ID Œ≤ in (LCAT)
i.e. we wish to guarantee b = Œ≤. Once correct block is retrieved, the rest of the argument is
identical to AR over dense attention as we retrieve the correct blocks. Observe that, we have ¬ØL ‚àí1
blocks in total (not counting the local/final block). Note that zi ‚àºN(0, œÉ2BId/d) for i , Œ≤ and
zŒ≤ ‚àºN(xL, œÉ2(B ‚àí1)Id/d).
Set gi = z‚ä§
i xL ¬∑ ‚àöd/B for i < ¬ØL and gŒ≤. Observe that gi‚Äôs and gŒ≤ are independent random variables.
Additionally, gi,Œ≤ ‚àºN(0, œÉ2), gŒ≤ ‚àºN( ‚àöd/B, (1 ‚àí1/B)œÉ2). Let gmax = maxi,Œ≤ gi. We have the
29

following gaussian concentration inequalities
P(gmax ‚â•œÉ(
p
2 log L‚Ä≤ + t)) ‚â§e‚àít2/2
(85)
P(gŒ≤ ‚â§
p
d/B ‚àíœÉt) ‚â§e‚àít2/2.
(86)
Combining these three, we find that, with probability 1‚àí2e‚àít2/2, whenever ‚àöd/B ‚â•œÉ(
p
2 log L‚Ä≤+2t),
we have that
gŒ≤ > gmax = max
i,b gi.
This condition is implied by d ‚â•œÉ2B(
p
2 log ¬ØL+2t)2. Applying change of variable on t, we conclude
with the result.
Retrieving the value token. Once the correct block is identified, (query, value) pair is retrieved by
applying full softmax attention with W = cI with c ‚Üí‚àûwithin the selected two blocks. Recall that
local attention retrieves the query and the choice of convolutional filter will return the value ahead of
the query. To guarantee this, we only need to prove that xL also has the largest correlation to itself
within the two selected blocks we apply local attention. To this aim, we similarly use the fact that
correlations between xL and the other tokens in the selected blocks are IID N(0, œÉ2/d) variables.
There are at most 2B ‚àí2 such other tokens. Consequently, the maximum local correlation gloc
max obeys
P(gloc
max ‚â•œÉ(
p
2 log(2B) + t)/
‚àö
d) ‚â§e‚àít2/2. We wish to guarantee that gloc
max < 1. This holds with
1 ‚àíe‚àít2/2 probability whenever d ‚â•œÉ2(
p
2 log(2B) + t)2. This latter condition is implied by the
original condition because
‚àö
B(
p
2 log ¬ØL + 2t) ‚â•
p
2B log 2 + 2t ‚â•
p
2 log(2B) + t. Union bounding,
we end up with a success probability of at least 1 ‚àí3e‚àít2/4.
Next, we wish to show the converse result. We recall that as the expectation of supremum of K IID
N(0, 1) become (1 ¬± o(1))
p
2 log K as K grows to infinity. Thus, for sufficiently large ¬ØL ‚â•CŒµ, we
have that E[gmax] ‚â•
p
(2 ‚àíŒµ) log ¬ØL. Consequently, we can write the reversed inequalities
P(gmax ‚â§œÉ(
q
(2 ‚àíŒµ) log ¬ØL ‚àít)) ‚â§e‚àít2/2
(87)
P(gŒ≤ ‚â•
p
d/B + œÉt) ‚â§e‚àít2/2.
(88)
Combining these, we conclude with the advertised reverse inequality. As a result, with the same
probability, we fail to identify the block containing the target query/value pair.
The next subsection proves the uniform AR guarantees via an application of Slepian‚Äôs lemma.
D.2
Proof of Uniform Associative Recall via Slepian‚Äôs Lemma (Proposition 1 continued)
Slepian‚Äôs Lemma [41] is an important gaussian comparison inequality. A specific variation is the
following result that holds for a random gaussian matrix. We first introduce the Gaussian width
definition that is important for assessing the complexity of a geometric set in a high-dimensional
space.
Definition 7 (Gaussian width). Let S ‚ààRd and g ‚àºN(0, Id). Gaussian width of S is defined as
œâ(S ) = E[supx‚ààS x‚ä§g]
Proposition 4 (Slepian‚Äôs Lemma). Let X ‚ààRn√ód be a matrix with IID N(0, 1) entries. Let g ‚àº
N(0, In) and h ‚àºN(0, Id). Given sets A ‚ààRn, B ‚ààRd, we have that
E[supa‚ààA,b‚ààBa‚ä§X‚ä§b] ‚â§E[supa‚ààA,b‚ààBa‚ä§g‚à•b‚à•‚Ñì2 + b‚ä§h‚à•a‚à•‚Ñì2].
We have the following application of Slepian‚Äôs lemma.
Lemma 4. Let X ‚ààRL√ód be a matrix with IID N(0, 1) entries. Let g ‚àºN(0, Id) be an independent
vector. Fix a subset of unit sphere S ‚ààRd. With probability 1 ‚àíe‚àít2/2, we have that
sup
Œ≤‚ààS
‚à•XŒ≤‚à•‚Ñì‚àû‚â§
‚àö
2(
p
log L + œâ(S ) + t).
Proof. Define the augmented matrix X‚Ä≤ =
"
X
‚àíg
#
. Define the set A ‚ààRL+1 such that a ‚ààA has the
following form. a has two nonzero entries both of which are equal to 1. Additionally, last entry is
30

nonzero i.e. aL+1 = 1. Using ‚à•a‚à•‚Ñì2 =
‚àö
2 and ‚à•Œ≤‚à•‚Ñì2 = 1, we now apply Slepian‚Äôs lemma as follows
E[supa‚ààA,Œ≤‚ààS a‚ä§X‚ä§Œ≤] ‚â§E[supa‚ààA,b‚ààBa‚ä§g‚à•Œ≤‚à•‚Ñì2 + Œ≤‚ä§h‚à•a‚à•‚Ñì2]
(89)
‚â§E[‚à•g‚à•‚Ñì‚àû+
‚àö
2 sup
Œ≤‚ààS
Œ≤‚ä§h]
(90)
‚â§
‚àö
2(
p
log L + œâ(S )).
(91)
To proceed, observe that a‚ä§X‚ä§Œ≤ is a
‚àö
2-Lipschitz function of X. This implies the statement of the
lemma.
‚ñ°
Proposition 5. Consider the setting of Proposition 1. Suppose we wish to solve AR for the worst
query drawn from a set S which is subset of the unit sphere. If d ‚â•2œÉ2B(
p
log ¬ØL+œâ(S )+t)2, (LCAT)
solves AR with probability at least 1 ‚àí2e‚àít2/2 for all xL ‚ààS .
Proof. The proof follows the steps of Section D.1 with the following distinction. Note that, to
determine the correct block, we now need to do a worst case analysis. Namely, let z‚Ä≤
Œ≤ = xL ‚àízŒ≤,
z‚Ä≤
i = zi for i , Œ≤, and set Z‚Ä≤ =
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
z‚Ä≤
1...
z ¬ØL‚àí1
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª. Also let Z = [z1 . . . zŒ≤‚àí1 zŒ≤+1 . . . z ¬ØL‚àí1]. The accurate
detection of the block Œ≤ coincides with the following event
inf
xL‚ààS ‚à•ZxL‚à•‚Ñì‚àû‚àíz‚ä§
Œ≤ xL > 0.
Using z‚ä§xL = 1 ‚àíz‚Ä≤‚ä§xL and defining the set A to be the set of all vectors with exactly two 1s with
one of the 1 appearing at position Œ≤, the above event can alternatively be written as
sup
a‚ààA,xL‚ààS
a‚ä§Z‚Ä≤xL < 1.
Now applying Lemma 4 on the left hand side, we find that, with probability 1 ‚àíe‚àít2/4,
sup
a‚ààA,xL‚ààS
a‚ä§Z‚Ä≤xL ‚â§
r
2B
d œÉ(
p
log L + œâ(S ) + t)
Consequently, whenever d > 2œÉ2B(log L+œâ(S )+t)2, we conclude with the result. Note that, when S
is an r-dimensional subspace, we plug in the well-known bound œâ(S ) ‚â§‚àör. Finally, we need to union
bound this event with the event that the query token can be identified through local attention by letting
Wk = Wq = ‚àöcI and c ‚Üí‚àû. To do so, we apply Lemma 4 over the 2B‚àí2 non-query tokens. Denoting
these tokens by Xloc ‚ààRd√ó(2B‚àí2), we have that P(Xlocq ‚â•
p
2œÉ2/d ¬∑ (
p
log(2B) + œâ(S ) + t)) ‚â§e‚àít2/2.
Consequently, Xlocq < q‚ä§q = 1 as soon as the same condition d ‚â•2œÉ2B(
p
log ¬ØL + œâ(S ) + t)2 holds.
This introduces an additional e‚àít2/4 probability of failure.
‚ñ°
D.3
Proof of Proposition 2
We essentially follow the proof of Proposition (1). The only differences are that, the variance
calculations, comparison of block correlations, and signal-to-noise ratio bounds will all slightly
change due to exponential smoothing impacting the full context window. To proceed, let us observe
the following scenarios for a block ID 1 ‚â§i < ¬ØL:
‚Ä¢ Scenario 1: i < Œ≤. In this scenario, zi is exponentially-smoothed sum of IID vectors with
N(0, 1) entries. Recalling œÅ = e‚àí1/B, the variance œÉ2
z of entries of zi is upper bounded by
œÉ2
z =
‚àû
X
i=0
œÅ2i =
œÉ2
1 ‚àíœÅ2 ‚â§1.2B.
(92)
Here, we used the fact that for B = 1, the bound holds and for B ‚â•2, we have that
œÅ2 = e‚àí2/B ‚â§1 ‚àí1
B. The latter implies 1 ‚àíœÅ2 ‚â•1/B and 1/(1 ‚àíœÅ2) ‚â§B.
The above bound on œÉ2
z implies that, setting gi = z‚ä§
i xL ¬∑ ‚àöd/B, we have that gi ‚àºN(0, œÉ2
i )
with œÉ2
i ‚â§1.2œÉ2.
31

‚Ä¢ Scenario 2: i = Œ≤. In this scenario, the variance upper bound œÉ2
i above is still applicable. The
key is to estimate and lower bound the mean component similar to the proof of Proposition
(1). Let the query token appear in the kth position of block Œ≤ for k ‚àà[B]. Define p = (k‚àí1)/B.
Observe that
E[gŒ≤] = E[z‚ä§
Œ≤ xL ¬∑
p
d/B] = e‚àíp p
d/B.
‚Ä¢ Scenario 3: i > Œ≤. This is essentially same as Scenario 2, because, thanks to the exponential
smoothing, the signal token from block Œ≤ will propagate to future zi‚Äôs. The coefficient of the
propagation satisfies
E[gi] = E[z‚ä§
i xL ¬∑
p
d/B] = e‚àí(p+i‚àíŒ≤) p
d/B.
Now that we have gathered these three scenarios, we can define gmax = maxi,Œ≤ gi ‚àíE[gi]) similar
to above. gmax is a supremum of independent Gaussians of bounded variance controlled by (92).
Through this, we have that
P(gmax ‚â•1.6œÉ(
q
log ¬ØL + t)) ‚â§e‚àít2
(93)
P(gŒ≤ ‚àíE[gŒ≤] ‚â§1.6œÉt) ‚â§e‚àít2.
(94)
Secondly, for i , Œ≤, using p ‚â§1, we have that
E[gŒ≤] ‚àíE[gi] ‚â•(e‚àíp ‚àíe‚àí(p+i‚àíŒ≤))
p
d/B ‚â•(e‚àí1 ‚àíe‚àí2)
p
d/B ‚â•0.23
p
d/B.
Consequently, we require 0.23 ‚àöd/B > 1.6œÉ(
p
log ¬ØL + 2t). Using œÑ = t/2, this is guaranteed by
d ‚â•50BœÉ2(
p
log ¬ØL + œÑ)2 with probability at least 1 ‚àí2e‚àíœÑ2/4. Once the correct block is identified,
(query, value) pair is retrieved by applying dense softmax attention with W = cI with c ‚Üí‚àûover the
selected blocks thanks to the choice of convolutional filter. This argument is identical to ‚ÄúRetrieving
the value token‚Äù proof in Section D.1 and introduces an additional e‚àíœÑ2/2 probability of failure in the
union bound.
32

