BEATS
: Audio Pre-Training with Acoustic
Tokenizers
Sanyuan Chen∗Yu Wu† Chengyi Wang Shujie Liu Daniel Tompkins
Zhuo Chen Furu Wei
{t-schen, yuwu1, t-chewang, shujliu, datompki, zhuc, fuwei}@microsoft.com
Microsoft
Abstract
The massive growth of self-supervised learning (SSL) has been witnessed in
language, vision, speech, and audio domains over the past few years. While
discrete label prediction is widely adopted for other modalities, the state-of-the-art
audio SSL models still employ reconstruction loss for pre-training. Compared with
reconstruction loss, semantic-rich discrete label prediction encourages the SSL
model to abstract the high-level audio semantics and discard the redundant details
as in human perception. However, a semantic-rich acoustic tokenizer for general
audio pre-training is usually not straightforward to obtain, due to the continuous
property of audio and unavailable phoneme sequences like speech. To tackle this
challenge, we propose BEATS, an iterative audio pre-training framework to learn
Bidirectional Encoder representation from Audio Transformers, where an acoustic
tokenizer and an audio SSL model are optimized by iterations. In the ﬁrst iteration,
we use random projection as the acoustic tokenizer to train an audio SSL model
in a mask and label prediction manner. Then, we train an acoustic tokenizer for
the next iteration by distilling the semantic knowledge from the pre-trained or
ﬁne-tuned audio SSL model. The iteration is repeated with the hope of mutual
promotion of the acoustic tokenizer and audio SSL model. The experimental
results demonstrate our acoustic tokenizers can generate discrete labels with rich
audio semantics and our audio SSL models achieve state-of-the-art results across
various audio classiﬁcation benchmarks, even outperforming previous models that
use more training data and model parameters signiﬁcantly. Speciﬁcally, we set a
new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without
using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained
models are available at https://aka.ms/beats.
1
Introduction
Recent years have witnessed great success in self-supervised learning (SSL) for speech and audio
processing. The speech SSL models, such as Wav2vec 2.0 [Baevski et al., 2020], HuBERT [Hsu et al.,
2021], BigSSL [Zhang et al., 2022], WavLM [Chen et al., 2022b], and data2vec [Baevski et al., 2022],
show prominent performance across various speech processing tasks, especially in low-resource
scenarios. Different from speech, audio typically contains wide variations of environmental events,
including human voices, nature sounds, musical beats, etc, which brings great challenges to general
audio modeling. To this end, audio SSL models, such as SS-AST [Gong et al., 2022a] and Audio-
MAE [Xu et al., 2022], are proposed for general audio classiﬁcation applications, demonstrating that
SSL learns robust auditory representations not only for speech but also for non-speech signals.
Until now, state-of-the-art (SOTA) audio SSL models [Xu et al., 2022, Chong et al., 2022] employ an
acoustic feature reconstruction loss as the pre-training objective instead of the discrete label prediction
∗This work is done during the internship at MSRA. † Corresponding author
arXiv:2212.09058v1  [eess.AS]  18 Dec 2022

pre-training task as in SSL models of speech [Hsu et al., 2021, Chen et al., 2022b], vision [Bao et al.,
2021, Peng et al., 2022, Wang et al., 2022b] and language [Devlin et al., 2019, Liu et al., 2019, Lan
et al., 2019]. However, it was generally believed that the reconstruction loss only accounts for the
correctness of low-level time-frequency features but neglects high-level audio semantic abstraction
[Ramesh et al., 2021, Bao et al., 2021]. The discrete label prediction would be a potentially better
audio pre-training objective than reconstruction for the following reasons.
Firstly, from the bionics aspect, humans understand audio by extracting and clustering the high-level
semantics instead of focusing on the low-level time-frequency details. For example, humans can
easily recognize the sound of any dog barking by capturing and classifying the semantic patterns,
even though the dog has never barked in the same scenario with the same tone before. By mimicking
the semantics extracting and clustering through the discrete label prediction pre-training objective,
the audio SSL model is expected to learn the same understanding and generalization skills as humans.
Secondly, from the aspect of modeling efﬁciency, the reconstruction loss may waste the audio model
parameter capacity and pre-training resources on predicting the semantic irrelevant information,
which has little beneﬁt to the general audio understanding tasks. In comparison, the discrete label
prediction objective can improve the audio modeling efﬁciency by providing semantic-rich tokens
as the pre-training targets and encouraging the model to discard the redundant details, resulting in a
superior audio understanding capability with a lower pre-training recourse cost.
Thirdly, the audio SSL pre-training with the discrete label prediction objective advances the uniﬁcation
of language, vision, speech, and audio pre-training. Instead of designing the pre-training task for each
modality, this uniﬁcation enables the possibility of building a foundation model across modalities
with a single pre-training task, i.e. discrete label prediction. With the capability of modeling various
forms of information, the general-purpose foundation model can be transferred and employed to
handle a wide range of practical tasks.
Despite these advantages and great successes in various domains, the application of discrete label
prediction in general audio processing remains challenging for two reasons. Firstly, as the audio
signal is continuous and the same acoustic event might have various durations in different occasions,
it is not straightforward to directly split the audio into semantically meaningful tokens as in language
processing [Devlin et al., 2019]. On the other hand, different from speech, the general audio
signals contain excessively larger data variations, including various non-speech acoustic events
and environmental sounds, where the commonly used speech tokenizer for phoneme information
extraction [Hsu et al., 2021] can not be directly applied.
Figure 1: Iterative audio pre-training of
BEATS.
To tackle these challenges, in this work we propose
BEATS, short for Bidirectional Encoder representation
from Audio Transformers, in which an acoustic tokenizer
and an audio SSL model are optimized through an itera-
tive audio pre-training framework. The training pipeline
is illustrated in Figure 1. In each iteration, we ﬁrst use
the acoustic tokenizer to generate the discrete labels of
the unlabeled audio, and use them to optimize the audio
SSL model with a mask and discrete label prediction loss.
After convergence, the audio SSL model acts as a teacher
to guide the acoustic tokenizer to learn audio semantics
with knowledge distillation [Hinton et al., 2015]. In this
alternating update learning process, the acoustic tokenizer
and the audio SSL model can beneﬁt from each other. The procedure is repeated until convergence.
Speciﬁcally, in the ﬁrst iteration, we use a random-projection acoustic tokenizer to generate discrete
labels as a cold start. In addition, we could ﬁne-tune the audio SSL model with a little supervised
data, and use the ﬁne-tuned model as the teacher for acoustic tokenizer training. A ﬁne-tuned model
learns semantic knowledge not only from SSL but supervised learning, which can further improve the
tokenizer quality. We believe the proposed pre-training framework encourages our audio SSL model
to learn relevant semantic information from iterations. Our pre-training framework is also compatible
with any masked audio prediction model, regardless of what backbone network is used.
We employ the vanilla ViT model [Dosovitskiy et al., 2021] as the backbone of our audio SSL models
without heavy structure engineering, and apply the speed-up technique proposed in He et al. [2022].
Given the discrete labels generated by the acoustic tokenizer, we mask 75% of the input sequence and
2

let the model predict the corresponding discrete labels on mask regions. We follow Xu et al. [2022] to
ﬁne-tune the audio SSL model across various audio tasks. Experimental results show that our BEATS
pre-trained models have superior performance compared with previous works across six audio and
speech classiﬁcation tasks. We achieve SOTA audio understanding performance on AudioSet-2M,
and outperform the previous SOTA results by a large margin (48.6 v.s. 47.4 for single model and 50.6
v.s. 49.6 for ensemble models) with much fewer model parameters and training data. On ESC-50, our
BEATS achieved 25% relative error rate reduction over the previous SOTA performance. We further
demonstrate the effectiveness of our proposed acoustic tokenizers, where the generated discrete labels
are robust to random disturbances and well aligned with audio semantics.
Our contributions include the following: 1) We propose an iterative audio pre-training framework,
which opens the door to audio pre-training with a discrete label prediction loss and shows better
performance than with reconstruction loss. It uniﬁes the pre-training for speech and audio, which
sheds light on the foundation model building for both speech and audio. 2) We provide effective
acoustic tokenizers to quantize continuous audio features into semantic-rich discrete labels, facilitating
future work of audio pre-training and multi-modality pre-training. 3) We achieve SOTA results on
several audio and speech understanding benchmarks. The models and codes are released 2 to facilitate
future research.
2
Related Work
Recently, audio pre-training has achieved great success in audio understanding tasks. The existing
audio pre-training methods include supervised pre-training and self-supervised pre-training.
Supervised audio pre-training.
The supervised audio pre-training methods either leverage out-of-
domain supervised data (e.g. ImageNet [Deng et al., 2009]) or in-domain supervised audio data (e.g.
AudioSet [Gemmeke et al., 2017]) for pre-training. As for the out-of-domain supervised pre-training,
PSLA [Gong et al., 2021b] proposes to use an ImageNet supervised pre-trained EfﬁcientNet [Tan and
Le, 2019] model for the audio model initialization and ﬁne-tunes the model on the audio classiﬁcation
tasks, which leads to signiﬁcant accuracy improvement. Instead of CNNs [LeCun et al.], AST [Gong
et al., 2021a], PaSST [Koutini et al., 2021], MBT [Nagrani et al., 2021] and HTS-AT [Chen et al.,
2022a] employ Transformer-based architectures [Vaswani et al., 2017] as the backbone, such as
ViT [Dosovitskiy et al., 2021] and Swin Transformer [Liu et al., 2021], and obtain superior audio
classiﬁcation performance.
As for the in-domain supervised pre-training, inspired by the vision pre-training method CLIP
[Radford et al., 2021], CLAP [Elizalde et al., 2022] proposes a contrastive language-audio pre-
training task to learn the text-enhanced audio representations with supervised audio and text pairs.
Instead of pre-training from scratch, Wav2clip [Wu et al., 2022] and Audioclip [Guzhov et al., 2022]
leverage the CLIP pre-trained model and learn an additional audio encoder with the supervised pairs
of audio and class labels from AudioSet. In addition, to push the performance for audio classiﬁcation
tasks with scarce data, some previous works [Kong et al., 2020, Verbitskiy et al., 2022, Gong et al.,
2021a, Chen et al., 2022a, Koutini et al., 2021, Xu et al., 2022] report the results on ESC-50 (1.6K
training samples) with an additional round of supervised pre-training on the AudioSet dataset (2M
training samples). Despite the promising classiﬁcation results, these methods strongly rely on a great
amount of supervised data, which is complex and expensive in practice.
Self-supervised audio pre-training.
In comparison, the self-supervised pre-training methods only
require large-scale unlabeled data, which can be easily get from the Internet. The self-supervised
audio pre-training methods typically learn the audio representations with the contrastive learning or
reconstruction objective. LIM [Ravanelli and Bengio, 2018], COLA [Saeed et al., 2021] and Fonseca
et al. [2021] adopt the contrastive learning framework for audio pre-training, where the positive
samples are the augmented clips from the same audio, and the negative ones are sampled from the
different audios. Instead of taking only the raw waveform or the acoustic feature as the input, CLAR
[Al-Tahan and Mohsenzadeh, 2021] proposes several data augmentation methods on both of them
for more effective contrastive learning. Wang and Oord [2021] also propose a contrastive learning
framework with different formats of audio samples by maximizing the agreement between the raw
waveform and its acoustic feature.
2https://aka.ms/beats
3

As for the reconstruction pre-training objective, inspired by Word2Vec [Mikolov et al., 2013] in NLP,
Audio2Vec [Tagliasacchi et al., 2020] proposes the CBoW task to reconstruct the acoustic feature
of an audio clip of pre-determined duration based on past and future clips, and the skip-gram task
to predict the past and future clips based on the middle audio clip. BYOL-A [Niizumi et al., 2021]
adopts the siamese architecture as BYOL [Grill et al., 2020], and learns to encode the robust audio
representations that are invariant to different audio augmentation methods with the mean square
error (MSE) loss and exponential moving average (EMA) optimization strategy. SSAST [Gong
et al., 2022a] proposes a patch-based self-supervised learning method to pre-train AST [Gong et al.,
2021a] with both the reconstruction and contrastive loss, and obtains comparable performance to the
supervised pre-training methods. Inspired by the success of the recent visual pre-training method
MAE [He et al., 2022], MSM-MAE [Niizumi et al., 2022], MaskSpec [Chong et al., 2022], MAE-AST
[Baade et al., 2022] and Audio-MAE [Xu et al., 2022] learn the audio representations following the
Transformer-based encoder-decoder design and reconstruction pre-training task in MAE, where a
decoder is trained to reconstruct the masked patches based on the encoded representations of the
unmasked ones. Until now, the MAE-style reconstruction pre-training methods show the best audio
understanding performance on various audio classiﬁcation tasks.
In addition, Audio2Vec [Tagliasacchi et al., 2020] proposes the TemporalGap pre-training task
to estimate the absolute time distance between two audio clips, which is however inferior to the
reconstruction tasks. Carr et al. [2021] introduces a permutation-based self-supervised pre-training
method, where the model is trained to reorder the shufﬂed patches of an input acoustic feature, and
leverage differentiable ranking to enable end-to-end model pre-training. Unlike the previous methods,
in this work, we explore the self-supervised audio pre-training method with the masked discrete label
prediction objective for the ﬁrst time.
Audio and speech tokenizer.
Various tokenizers have been proposed for learning discrete rep-
resentations on audio and speech tasks. Dieleman et al. [2018] propose a hierarchical VQ-VAE
based model to learn audio discrete representations for music generation tasks. HuBERT [Hsu et al.,
2021] generates discrete labels with the iterative hidden state clustering method for speech SSL task,
where the hidden state is extracted from the last round speech SSL model. Chiu et al. [2022] claim
a random-projection tokenizer is adequate for a large speech SSL model pre-training. Our work is
the ﬁrst to train an acoustic tokenizer with the supervision of the last round SSL model, which is
different from the previous auto-encoding and ad-hoc clustering methods.
3
BEATS
3.1
Iterative Audio Pre-training
Figure 1 shows the overall pipeline of our iterative audio pre-training framework of BEATS, where
an acoustic tokenizer (Section 3.2) and an audio SSL model (Section 3.3) are optimized by iterations.
In each iteration, given the unlabeled audio, we use the acoustic tokenizer to generate the discrete
labels, and use them to train the audio SSL model with a mask and discrete label prediction loss.
After model convergence, we use the audio SSL model as the teacher to train a new acoustic tokenizer
with knowledge distillation for the next iteration of audio SSL model training.
Speciﬁcally, given an audio clip as the input, we ﬁrst extract the corresponding acoustic features,
split them into regular grid patches, and further ﬂatten them to the patch sequence X = {xt}T
t=1. For
the audio SSL model training, we use the acoustic tokenizer to quantize the patch sequence X to the
patch-level discrete labels ˆZ = {ˆzt}T
t=1 as the masked prediction targets. For the acoustic tokenizer
training, we leverage the audio SSL model to encode the patch sequence X and extract the output
sequence ˆO = {ˆot}T
t=1 as the knowledge distillation targets.
Note that we could leverage either a pre-trained audio SSL model or a ﬁne-tuned audio SSL model
as the teacher for acoustic tokenizer training. A ﬁne-tuned model learns semantic knowledge not
only from self-supervised pre-training but supervised ﬁne-tuning, making it a better teacher for audio
semantics distillation. With this alternating update learning process, the acoustic tokenizer beneﬁts
from the semantic-rich knowledge encoded by the audio SSL model, while the audio SSL model
beneﬁts from semantic-rich discrete labels generated by the acoustic tokenizer. The procedure is
repeated until convergence.
4

Figure 2: Acoustic tokenizers for discrete label generation.
3.2
Acoustic Tokenizers
The acoustic tokenizers are used to generate the discrete labels for each iteration of BEATS pre-
training. In the ﬁrst iteration, given the teacher model is unavailable, we employ a Random-Projection
Tokenizer (Section 3.2.1) to cluster the continuous acoustic features into discrete labels as a cold start.
Starting from the second iteration, we train a Self-Distilled Tokenizer (Section 3.2.2) to generate the
reﬁned discrete labels with the semantic-aware knowledge distilled from the pre-trained/ﬁne-tuned
audio SSL model obtained in the last iteration.
3.2.1
Cold Start: Random-Projection Tokenizer
For the ﬁrst iteration of BEATS pre-training, we apply the random-projection tokenizer [Chiu et al.,
2022] to generate the patch-level discrete labels for each input audio.
As shown in the left part of Figure 2, the random-projection tokenizer includes a linear projection
layer and a set of codebook embeddings, which are kept frozen after random initialization. Each
patch of the input feature is ﬁrst projected with the linear layer, then ﬁnds the nearest neighbor vector
among the codebook embeddings, where the index of the nearest neighbor is deﬁned as the discrete
label.
Speciﬁcally, given the patch sequence extracted from the input audio X = {xt}T
t=1, we ﬁrst project
xt to the vector Wxt with a randomly initialized projection layer W. Then we look up the nearest
neighbor vector of each projected vector Wxt from a set of random initialized vectors V = {vi}K
i=1,
where K is the codebook size, and deﬁne the discrete label of t-th patch as the index of the nearest
neighbor vector:
ˆzt = arg min
i
||vi −Wxt||2
2
(1)
3.2.2
Iteration: Self-Distilled Tokenizer
From the second iteration of BEATS pre-training, we leverage the last iteration audio SSL model
as the teacher, which can be either a pre-trained model or a ﬁne-tuned model, to teach the current
iteration tokenizer learning. We call it the self-distilled tokenizer to generate the patch-level discrete
labels for each input audio.
5

As shown in the right part of Figure 2, the self-distilled tokenizer ﬁrst uses a Transformer-based
tokenizer encoder to convert the input patches to discrete labels with a set of learnable codebook
embeddings. Then, a Transformer-based tokenizer estimator is trained to predict the output of a
teacher model with the discrete labels and codebook embeddings as the input. With knowledge
distillation as the training target, the tokenized discrete labels are optimized to contain more semantic-
rich knowledge from the teacher and less redundant information of the input audio.
Speciﬁcally, we ﬁrst feed the input patches X = {xt}T
t=1 to a 12-layer Transformer encoder and
obtain the encoded vector sequence E = {et}T
t=1. Then, for each encoded vector et, we conduct the
quantization by ﬁnding the nearest neighbor vector vˆzt from the codebook embeddings V = {vi}K
i=1:
ˆzt = arg min
i
||ℓ2(vi) −ℓ2(et)||2
2,
(2)
where ℓ2 normalization is used to improve the codebook utilization [Yu et al., 2021, Peng et al.,
2022]. With the quantized vector sequence Eq = {vˆzt}T
t=1 as the input, we use a 3-layer Transformer
estimator to predict the last layer output of the teacher model {ˆot}T
t=1.
To deal with the non-differentiable problem of the vector quantization (Equation 2), following Van
Den Oord et al. [2017], we apply the straight-through gradients mechanism, where the gradients are
directly copied from the quantized vector sequence Eq to the encoded vector sequence E during the
backward process.
The overall training objective of the self-distilled tokenizer is deﬁned as the cosine similarity between
the output sequence of the tokenizer estimator {ot}T
t=1 and the output sequence of the teacher model
{ˆot}T
t=1, along with the mean squared error between the encoded vector sequence E = {et}T
t=1 and
the quantized vector sequence Eq = {vˆzt}T
t=1:
max
X
X∈D
T
X
t=1
cos(ot, ˆot) −||sg[ℓ2(et)] −ℓ2(vˆzt)||2
2 −||ℓ2(et) −sg[ℓ2(vˆzt)]||2
2,
(3)
where D denotes the pre-training datasets, cos(·, ·) and sg[·] are the cosine similarity and the stopgra-
dient operator, respectively. We employ the exponential moving average [Van Den Oord et al., 2017]
for codebook embedding optimization for more stable tokenizer training [Peng et al., 2022].
During inference, we discard the tokenizer estimator, and leverage the pre-trained tokenizer encoder
and codebook embeddings to convert each input audio X = {xt}T
t=1 to patch-level discrete labels
ˆZ = {ˆzt}T
t=1, as in Equation 2.
3.3
Audio SSL Model
3.3.1
Backbone
Following the previous works [Gong et al., 2021a, 2022a, Xu et al., 2022], we employ the ViT
structure [Dosovitskiy et al., 2021] as the backbone network, which consists of a linear projection
layer and a stack of Transformer encoder layers.
Given the patch sequence extracted from the input audio X = {xt}T
t=1, we ﬁrst convert them to the
patch embeddings E = {et}T
t=1 with a linear projection network. Then, we feed the patch embeddings
to the Transformer encoder layers, and obtain the encoded patch representations R = {rt}T
t=1. The
Transformer is equipped with a convolution-based relative position embedding layer at the bottom,
and the gated relative position bias [Chi et al., 2022] for better position information encoding. We
also employ the DeepNorm [Wang et al., 2022a] for more stable pre-training.
3.3.2
Pre-Training
We propose a Masked Audio Modeling (MAM) task for the audio SSL model pre-training, as shown
in the left part of Figure 3. Different from the previous audio pre-training methods, where the model
is optimized to reconstruct the input acoustic feature, our model is optimized to predict the patch-level
discrete labels generated by the acoustic tokenizers (Section 3.2) with a Transformer-based label
predictor.
Speciﬁcally, given the input patch sequence X = {xt}T
t=1 and the corresponding target discrete
acoustic labels ˆZ = {ˆzt}T
t=1, we randomly mask 75% of the input patches, where the masked
6

Figure 3: Overview of audio SSL model pre-training and ﬁne-tuning.
positions are denoted as M = {1, . . . , T}0.75T . Then, we feed the unmasked patch sequence
XU = {xt : t ∈M}T
t=1 to the ViT encoder, and obtain the encoded representations RU = {rt : t ∈
M}T
t=1. Finally, we feed the combination of the non-masked patch representations and the masked
patch features {rt : t ∈M}T
t=1 ∪{0 : t ̸∈M}T
t=1 to the label predictor to predict the discrete
acoustic labels Z = {zt}T
t=1. It should be noted here that only feeding the non-masked patches into
the encoder could signiﬁcantly speed up the training process while providing slight improvement
across downstream tasks [Xu et al., 2022].
The pre-training objective of MAM is the cross entropy loss which maximizes the log-likelihood of
the correct acoustic labels in the masked positions given the unmasked patch sequences.
LMAM = −
X
t∈M
log p(ˆzt|XU)
(4)
3.3.3
Fine-Tuning
During audio SSL model ﬁne-tuning, we discard the label predictor, and append a task-speciﬁc linear
classiﬁer upon the ViT encoder to generate the labels for the downstream classiﬁcation tasks, as
shown in the right part of Figure 3.
Speciﬁcally, we ﬁrst random mask the input acoustic feature in the time and frequency dimension
as spec-augmentation [Park et al., 2019], then split and ﬂat it to the patch sequence X = {xt}T
t=1.
Unlike pre-training, we feed the whole patch sequence X to the ViT encoder, and obtain the encoded
representations R = {rt}T
t=1. Finally, we use a linear classiﬁer to calculate the category probabilities
as p(C) = Softmax(MeanPool(WcR)), where Softmax, MeanPool and Wc denote the softmax
operation, mean-pooling layer and the linear projection, respectively.
We employ the cross entropy loss as the ﬁne-tuning objective for the single label classiﬁcation tasks,
and the binary cross entropy loss for the multi-label classiﬁcation tasks or the mixup augmentation
[Zhang et al., 2017] is employed.
4
Experiment
4.1
Datasets
We pre-train our BEATS tokenizers and audio SSL models on the full training set of the AudioSet
dataset, and evaluate our pre-trained audio SSL models on six downstream tasks, including three
7

audio classiﬁcation tasks (AS-2M, AS-20K and ESC-50) and three speech classiﬁcation tasks (KS1,
KS2 and ER).
AudioSet (AS-2M and AS-20K) [Gemmeke et al., 2017] is a large-scale audio classiﬁcation dataset.
It contains over 2 million 10-second YouTube clips annotated with 527 audio event classes, where
each clip could be annotated with multiple audio event classes. It is ofﬁcially subdivided into three
partitions, including a class-wise balanced set (22,176 clips), a class-wise unbalanced set (2,042,985
clips), and an eval set (20,383 clips). Due to the constant change in YouTube video availability (e.g.,
videos being removed or taken down), we downloaded and processed 20,666, 1,919,153, and 18,987
clips for the balanced, unbalanced, and eval sets, respectively, which is consistent with the previous
works [Baade et al., 2022].
Following the previous works, we use the combination of the 21K balanced and the 1.9M unbalanced
training audios for ﬁne-tuning in the AS-2M task, and only the 21K balanced training audios for
ﬁne-tuning in the AS-20K task. We evaluate our models on the 19K eval set with the mean average
precision (mAP) evaluation metric.
Environmental Sound Classiﬁcation (ESC-50) [Piczak, 2015] is an audio classiﬁcation dataset
that contains 2,000 5-second environmental sound recordings annotated with 50 classes. Each sound
recording is only annotated with one class. We follow the 5-fold cross-validation evaluation setting
as the previous works and report the classiﬁcation accuracy as the evaluation metric.
Speech Commands V2 (KS2) [Warden, 2018] is a keyword spotting dataset that contains 105,829
1-second spoken word clips annotated with 35 common word classes. It is ofﬁcially subdivided into
the training, validation, and testing set that contains 84,843, 9,981, and 11,005 audio clips respectively.
We report classiﬁcation accuracy as the evaluation metric.
Speech Commands V1 (KS1) [Warden, 2018] task uses the same dataset as KS2, but only contains
10 classes of keywords, 1 silence class, and 1 unknown class that includes all the other 20 common
speech commands. We use the standard data and split provided in SUPERB benchmark [wen Yang
et al., 2021] to report classiﬁcation accuracy for a fair comparison with the previous works.
IEMOCAP (ER) [Busso et al., 2008] is an emotion recognition dataset that contains about 12 hours
of emotional speech clips annotated with four classes. we use the 5-fold cross-validation evaluation
setting as SUPERB benchmark [wen Yang et al., 2021] and report classiﬁcation accuracy as the
evaluation metric.
4.2
Implementation Details
Backbone
The BEATS models have 12 Transformer encoder layers, 768-dimensional hidden states,
and 8 attention heads, resulting in 90M parameters. We keep the model size similar to the previous
SOTA audio pre-trained models [Xu et al., 2022, Chong et al., 2022] for a fair comparison of the
pre-training methods.
Acoustic feature
Following [Gong et al., 2021a, 2022a], we convert the sample rate of each raw
waveform to 16,000, and extract the 128-dimensional Mel-ﬁlter bank features with a 25ms Povey
window that shifts every 10 ms as the acoustic feature. The acoustic feature is normalized to the mean
value of 0 and standard deviation of 0.5 following the previous works. We split each acoustic feature
into the 16 × 16 patches, and further ﬂat them to the patch sequence as the input of our BEATS
tokenizers and models.
Model and tokenizer training
We pre-train the BEATS models on AS-2M dataset for three
iterations and denote them as BEATSiter1, BEATSiter2, BEATSiter3, BEATSiter3+.
The BEATSiter1 is pre-trained with the discrete labels generated by a random-projection tokenizer
(Section 3.2.1). Starting from the second iteration, we train a self-distilled tokenizer (Section 3.2.2)
to generate the discrete labels for the pre-training of BEATSiter2 and BEATSiter3 with the pre-trained
BEATSiter1 and BEATSiter2 as the teacher, respectively. Different from BEATSiter3, the self-distilled
tokenizer for BEATSiter3+ pre-training takes the supervised ﬁne-tuned BEATSiter2 as the teacher
model and learns to estimate the classiﬁcation logits of the input audios. Compared with the other
BEATS models, the BEATSiter3+ not only make use of the downstream supervised data during
ﬁne-tuning but also in pre-training.
8

Table 1: Comparing with the SOTA single models on audio and speech classiﬁcation tasks. IN, AS,
and LS denote the ImageNet, AudioSet, and LibriSpeech datasets, respectively. TA and TI denote
the 128K text-audio pairs and 400M text-image pairs for CLAP and CLIP pre-training, respectively.
The evaluation metrics are mAP for AS-2M/AS-20K and accuracy for ESC-50/KS1/KS2/ER. We
compared the best single models from each previous work. We gray-out the models and results with
additional supervised training on the external datasets. ∗The results reported following the SUPERB
policy [wen Yang et al., 2021], where pre-trained models are kept frozen during ﬁne-tuning.
Model
# Param
Data
Audio
Speech
AS-2M
AS-20K
ESC-50
KS1
KS2
ER
No Pre-Training
PANN [Kong et al., 2020]
81M
-
43.1
27.8
83.3
-
61.8
-
ERANN [Verbitskiy et al., 2022]
55M
-
45.0
-
89.2
-
-
-
Out-of-domain Supervised Pre-Training
PSLA [Gong et al., 2021b]
14M
IN
44.4
31.9
-
-
96.3
-
AST [Gong et al., 2021a]
86M
IN
45.9
34.7
88.7
95.5
98.1
56.0
MBT [Nagrani et al., 2021]
86M
IN-21K
44.3
31.3
-
-
-
-
PaSST [Koutini et al., 2021]
86M
IN
47.1
-
-
-
-
-
HTS-AT [Chen et al., 2022a]
31M
IN
47.1
-
-
-
98.0
-
Wav2CLIP [Wu et al., 2022]
74M
TI+AS
-
-
86.0
-
-
-
AudioCLIP [Guzhov et al., 2022]
93M
TI+AS
25.9
-
96.7
-
-
-
In-domain Supervised Pre-Training
PANN [Kong et al., 2020]
81M
AS
-
-
94.7
-
-
-
ERANN [Verbitskiy et al., 2022]
55M
AS
-
-
96.1
-
-
-
AST [Gong et al., 2021a]
86M
IN+AS
45.9
-
95.6
-
97.9
-
PaSST [Koutini et al., 2021]
86M
IN+AS
47.1
-
96.8
-
-
-
HTS-AT [Chen et al., 2022a]
31M
IN+AS
47.1
-
97.0
-
-
-
CLAP [Elizalde et al., 2022]
190.8M
TA
-
-
96.7
-
96.8
-
Audio-MAE [Xu et al., 2022]
86M
AS
-
-
97.4
-
-
-
Self-Supervised Pre-Training
Wav2vec [Schneider et al., 2019]
33M
LS
-
-
-
96.2
-
59.8
Wav2vec 2.0 [Baevski et al., 2020]
95M
LS
-
-
-
96.2∗
-
63.4∗
SS-AST [Gong et al., 2022a]
89M
AS+LS
-
31.0
88.8
96.0
98.0
59.6
MSM-MAE [Niizumi et al., 2022]
86M
AS
-
-
85.6
-
87.3
-
MaskSpec [Chong et al., 2022]
86M
AS
47.1
32.3
89.6
-
97.7
-
MAE-AST [Baade et al., 2022]
86M
AS+LS
-
30.6
90.0
95.8
97.9
59.8
Audio-MAE [Xu et al., 2022]
86M
AS
47.3
37.1
94.1
96.9
98.3
-
data2vec [Baevski et al., 2022]
94M
AS
-
34.5
-
-
-
-
Audio-MAE Large [Xu et al., 2022]
304M
AS
47.4
37.6
-
-
-
-
CAV-MAE [Gong et al., 2022b]
86M
AS+IN
44.9
34.2
-
-
-
-
Ours
BEATSiter1
90M
AS
47.9
36.0
94.0
98.0
98.3
65.9
BEATSiter2
90M
AS
48.1
38.3
95.1
97.7
98.3
66.1
BEATSiter3
90M
AS
48.0
38.3
95.6
97.7
98.3
64.5
BEATSiter3+
90M
AS
48.6
38.9
98.1
98.1
98.1
65.0
We pre-train all the BEATS models for 400k steps with a batch size of 5.6K seconds and a 5e-4 peak
learning rate. The codebook of all the tokenizers contains 1024 embeddings with 256 dimensions.
The self-distilled tokenizer with a self-supervised model as the teacher is trained for 400k steps
with a batch size of 1.4K seconds and a 5e-5 peak learning rate. The self-distilled tokenizer with a
supervised model as the teacher is trained for 400k steps with a batch size of 1.4K seconds and a 5e-4
peak learning rate. Please see Appendix A.1 for the detailed hyperparameter settings.
4.3
Comparing with the SOTA Single Models
Table 1 shows the comparison of the single-model performance of our BEATS pre-trained models
and the previous SOTA models. For a fair comparison with the previous self-supervised pre-training
methods, we report the BEATSiter3+ ﬁne-tuning results on AS-2M and AS-20K with the models
that are pre-trained with the same supervised dataset as ﬁne-tuning. On the other tasks, we report
the BEATSiter3+ ﬁne-tuning results with the model that is pre-trained with the AS-2M supervised
dataset, and compare them with the previous supervised pre-training methods. Following [Xu et al.,
2022, Gong et al., 2021a], we report the BEATSiter3+ ﬁne-tuning result on ESC-50 with additional
supervised training on AS-2M.
9

Table 2: Comparing different BEATS tokenizers on audio classiﬁcation tasks. SSL Data and SL
Data denote the training data used for self-supervised learning and supervised learning, respectively.
∗We use AS-2M supervised data during pre-training and AS-20K supervised data during ﬁne-tuning.
†Here, We report the ESC-50 results without additional supervised pre-training on AS-2M for a fair
comparison of different tokenizers.
Model
Tokenizer Type
Tokenizer Teacher
SSL Data
SL Data
AS-2M
AS-20K
ESC-50
BEATSiter1
Random-Projection
N/A
AS
-
47.9
36.0
94
BEATSiter2
Self-Distilled
BEATSiter1
AS
-
48.1
38.3
95.1
BEATSiter3
Self-Distilled
BEATSiter2
AS
-
48.0
38.3
95.6
BEATSiter3+
Self-Distilled
BEATSiter2 ﬁne-tuned on AS-20K
AS
AS-20K
48.0
38.9
96.2
BEATSiter3+
Self-Distilled
BEATSiter2 ﬁne-tuned on AS-2M
AS
AS
48.6
41.8∗
97.1†
Overall, BEATS achieve the best performance across all six audio and speech classiﬁcation tasks.
BEATSiter3+ set a new SOTA single-model audio understanding performance on AS-2M and AS-20K,
and outperform the previous SOTA results by a large margin (48.6 v.s. 47.4 on AS-2M, and 38.9
v.s. 37.6 on AS-20K) with much fewer model parameters (90M v.s. 304M). Notably, BEATS also
signiﬁcantly outperform all the previous models that use more out-of-domain or in-domain data
for supervised or self-supervised pre-training. On ESC-50, BEATS successfully reduce the SOTA
classiﬁcation error rate from 5.9% to 4.4% without any external supervised data, and from 2.6% to
1.9% with external AS-2M supervised data.
As shown in the table, our ﬁrst iteration model BEATSiter1 which uses a random-projection tokenizer
for label generation can already obtain better performance than previous works on ﬁve out of six
tasks (AS-2M, ESC-50, KS1, KS2, and ER), which demonstrates the superiority of the discrete label
prediction loss comparing to the reconstruction loss. Pre-trained with the reﬁned labels generated
by a self-distilled tokenizer, BEATSiter2 can achieve further performance improvements, especially
on the audio classiﬁcation tasks. With SSL on AS-2M, BEATSiter1 learns to encode the high-level
audio representations with semantic-aware knowledge. Taking BEATSiter1 as the teacher model, the
self-distilled tokenizer is optimized to reﬁne the labels with more audio-related semantics, resulting
in the more powerful audio modeling ability of BEATSiter2.
As for the third iteration of BEATS pre-training, we can ﬁnd that BEATSiter3 obtains similar
performance as BEATSiter2, indicating our self-distilled tokenizer is robust to difference SSL teacher
models, and our BEATS iterative pre-training procedure is capable of fast convergence in only
a few iterations. Furthermore, if we use the ﬁne-tuned BEATSiter2 models as the teacher model,
the BEATSiter3+ can bring signiﬁcant performance gains on both AS-2M and AS-20K tasks, and
outperform all the previous SOTA models by a large margin. By leveraging the supervised ﬁne-tuning
data in our iterative training pipeline, both the acoustic tokenizer and the audio SSL model learn more
task-speciﬁc semantic knowledge from each other, which would effectively promote BEATSiter3+
performance on the downstream understanding tasks.
4.4
Comparing Different BEATS Tokenizers
Table 2 shows the detailed performance comparison of different BEATS tokenizers. We can ﬁnd
that the self-distilled tokenizer shows remarkable superiority compared with the random-projection
tokenizer, especially in the task with scarce data. It is because the random-projection tokenizer with
a simple feature clustering process is insufﬁcient to provide the labels with the high-level audio
semantic abstraction, while the self-distilled tokenizer is able to distill the semantic knowledge from
a well pre-trained audio SSL model to the generated discrete labels.
In addition, the results show that the performance of the self-distilled tokenizer is insensitive to
different self-supervised teachers (e.g. BEATS models) but sensitive to different supervised teachers
(e.g. the ﬁne-tuned BEATS models). The self-distilled tokenizer guided by BEATSiter1 obtains
similar performance as the tokenizer guided by BEATSiter2. The self-distilled tokenizer guided
by the AS-2M ﬁne-tuned BEATSiter2 model can achieve the best performance on all three audio
classiﬁcation tasks.
10

(a) Reconstruction
(b) BEATSiter3
(c) BEATSiter3+
Figure 4: Comparing the pre-training targets of different SSL models with audio samples from ESC-50.
We visualize the acoustic features for reconstruction-based SSL models, the representations quantized
by the tokenizer with a self-supervised pre-trained teacher for BEATSiter3, and the representations
quantized by the tokenizer with a supervised ﬁne-tuned teacher for BEATSiter3+.
Table 3: Comparing with the SOTA ensemble models on AS-2M.
Model
SL Data
AS-2M
PSLA [Gong et al., 2021b]
IN+AS
47.4
AST [Gong et al., 2021a]
IN+AS
48.5
HTS-AT [Chen et al., 2022a]
IN+AS
48.7
PaSST [Koutini et al., 2021]
IN+AS
49.6
BEATS (5 models)
AS
50.4
BEATS (10 models)
AS
50.6
4.5
Comparing Different Pre-Training Targets via Visualization
Figure 4 shows the comparison of the pre-training targets of different SSL models with audio samples
from the ESC-50 dataset. Speciﬁcally, ﬁgure 4(a) demonstrates the acoustic features which are
the pre-training targets for reconstruction-based SSL models. Figure 4(b) and 4(c) illustrate the
pre-training targets of BEATSiter3 and BEATSiter3+, which are demonstrated with the quantized
representations encoded by the acoustic tokenizers with a self-supervised teacher (i.e. BEATSiter2)
and a supervised teacher (i.e. the ﬁne-tuned BEATSiter2), respectively. We reduced the feature
dimension to 2-D by T-SNE [Van der Maaten and Hinton, 2008] for better visualization.
As the standard evaluation setting, we divide the data into a 1.6K training set and a 0.4K valid set.
We use the training set for BEATSiter3+ pre-training, and the valid set for visualization. We randomly
select ten audio samples with different classiﬁcation labels from the valid set, then add some random
disturbance on the waveform with RIR 3 reverberations and DNS noises [Reddy et al., 2021]. The
points with different colors denote the audios with different classiﬁcation labels, and the points with
the same color denote different disturbances to the same audio.
As shown in the ﬁgures, the pre-training targets of reconstruction-based SSL models are very sensitive
to random disturbances on the waveform. The acoustic feature of the same audio with different
disturbances can be far apart, and the acoustic feature with different labels can be closely spaced. It
indicates the pre-training targets of reconstruction-based SSL models mainly contain low-level time-
frequency features and lack high-level audio semantic abstractions. In comparison, the pre-training
targets of BEATS models are much more robust to the random variations. With the self-supervised
pre-trained model as the teacher, the acoustic tokenizer learns to cluster the audio samples with the
same semantic content and get rid of the background reverberations and noises. With the supervised
ﬁne-tuned model as the teacher, the acoustic tokenizer can successfully capture high-level semantics
of audio regardless of the low-level details of redundancy, and generate semantic-rich discrete tokens
for more effective BEATS model pre-training.
3https://www.openslr.org/28/
11

4.6
Comparing with the SOTA Ensemble Models
Table 3 shows the comparison of the ensemble-model performance of our BEATS pre-trained models
and the previous SOTA models on AS-2M. We ﬁrst ensemble all the ﬁve AS-2M ﬁne-tuned BEATS
models that are listed in Table 2, and denote it as BEATS (5 models). As shown in the table, without
using any external supervised data (e.g. ImageNet), our BEATS (5 models) signiﬁcantly outperforms
the previous best ensemble models by 0.8 mAP. Then, we rerun the AS-2M ﬁne-tuning of the ﬁve
BEATS SSL models with a learning rate of 5e-5 for 100k training steps, and ensemble all the ten
AS-2M ﬁne-tuned models. The BEATS (10 models) can further improve the ensemble results and
achieve 50.6 SOTA mAP performance.
5
Conclusion
In this paper, we propose BEATS, an iterative audio pre-training framework a self-supervised
model for audio representation learning. Different from the previous audio SSL models that employ
reconstruction loss as the pre-training objective, we present a self-distilled tokenizer to convert
continuous audio signals into discrete labels, enabling the classic mask and discrete label prediction
pre-training. BEATS achieve superior performance across six audio and speech classiﬁcation tasks
and set new state-of-the-art results on AudioSet-2M and ESC-50 benchmarks. Further analysis via
visualization illustrates the pre-training targets of BEATS models are more robust to disturbances
and aligned with the semantics than reconstruction-based audio SSL models, which indicates the
effectiveness of the self-distilled tokenizer and accounts for the superiority of the proposed audio
pre-training framework.
In the future, we would like to scale up the model size and pre-training data to further push the limits
of audio classiﬁcation. In addition, it is interesting to study the multi-modality ﬁeld by combining
audio with vision and language.
References
Haider Al-Tahan and Yalda Mohsenzadeh. Clar: Contrastive learning of auditory representations. In
International Conference on Artiﬁcial Intelligence and Statistics, pages 2530–2538. PMLR, 2021.
Alan Baade, Puyuan Peng, and David Harwath. Mae-ast: Masked autoencoding audio spectrogram
transformer. arXiv preprint arXiv:2203.16691, 2022.
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework
for self-supervised learning of speech representations. Advances in Neural Information Processing
Systems, 33:12449–12460, 2020.
Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec:
A general framework for self-supervised learning in speech, vision and language. arXiv preprint
arXiv:2202.03555, 2022.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers.
In International Conference on Learning Representations, 2021.
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jean-
nette N Chang, Sungbok Lee, and Shrikanth S Narayanan. Iemocap: Interactive emotional dyadic
motion capture database. Language resources and evaluation, 42(4):335–359, 2008.
Andrew N Carr, Quentin Berthet, Mathieu Blondel, Olivier Teboul, and Neil Zeghidour. Self-
supervised learning of audio representations from permutations with differentiable ranking. IEEE
Signal Processing Letters, 28:708–712, 2021.
Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Hts-at:
A hierarchical token-semantic audio transformer for sound classiﬁcation and detection. In ICASSP
2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pages 646–650. IEEE, 2022a.
12

Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki
Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training
for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):
1505–1518, 2022b.
Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma, Bo Zheng, Saksham Singhal, Payal Bajaj, Xia
Song, Xian-Ling Mao, He-Yan Huang, et al. Xlm-e: Cross-lingual language model pre-training
via electra. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 6170–6182, 2022.
Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. Self-supervised learning
with random-projection quantizer for speech recognition. arXiv preprint arXiv:2202.01855, 2022.
Dading Chong, Helin Wang, Peilin Zhou, and Qingcheng Zeng. Masked spectrogram prediction for
self-supervised audio pre-training. arXiv preprint arXiv:2204.12768, 2022.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pages 248–255. Ieee, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.
Sander Dieleman, Aaron van den Oord, and Karen Simonyan. The challenge of realistic music
generation: modelling raw audio at scale. Advances in Neural Information Processing Systems, 31,
2018.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In
9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May
3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.
Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap: Learning
audio concepts from natural language supervision. arXiv preprint arXiv:2206.04769, 2022.
Eduardo Fonseca, Diego Ortego, Kevin McGuinness, Noel E O’Connor, and Xavier Serra. Unsuper-
vised contrastive learning of sound event representations. In ICASSP 2021-2021 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 371–375. IEEE, 2021.
Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing
Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for
audio events. In 2017 IEEE international conference on acoustics, speech and signal processing
(ICASSP), pages 776–780. IEEE, 2017.
Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio spectrogram transformer. arXiv preprint
arXiv:2104.01778, 2021a.
Yuan Gong, Yu-An Chung, and James Glass. Psla: Improving audio tagging with pretraining,
sampling, labeling, and aggregation. IEEE/ACM Transactions on Audio, Speech, and Language
Processing, 29:3292–3306, 2021b.
Yuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass. Ssast: Self-supervised audio spectrogram
transformer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 36, pages
10699–10709, 2022a.
Yuan Gong, Andrew Rouditchenko, Alexander H Liu, David Harwath, Leonid Karlinsky, Hilde
Kuehne, and James Glass.
Contrastive audio-visual masked autoencoder.
arXiv preprint
arXiv:2210.07839, 2022b.
13

Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural
information processing systems, 33:21271–21284, 2020.
Andrey Guzhov, Federico Raue, Jörn Hees, and Andreas Dengel. Audioclip: Extending clip to image,
text and audio. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 976–980. IEEE, 2022.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
autoencoders are scalable vision learners.
In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 16000–16009, 2022.
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2(7), 2015.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,
and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked
prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing,
29:3451–3460, 2021.
Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley. Panns:
Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transac-
tions on Audio, Speech, and Language Processing, 28:2880–2894, 2020.
Khaled Koutini, Jan Schlüter, Hamid Eghbal-zadeh, and Gerhard Widmer. Efﬁcient training of audio
transformers with patchout. arXiv preprint arXiv:2110.05069, 2021.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942, 2019.
Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 10012–10022, 2021.
Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781, 2013.
Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention
bottlenecks for multimodal fusion. Advances in Neural Information Processing Systems, 34:
14200–14213, 2021.
Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, and Kunio Kashino. Byol for
audio: Self-supervised learning for general-purpose audio representation. In 2021 International
Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2021.
Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, and Kunio Kashino. Masked
spectrogram modeling using masked autoencoders for learning general-purpose audio representa-
tion. arXiv preprint arXiv:2204.12260, 2022.
Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and
Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition.
arXiv preprint arXiv:1904.08779, 2019.
14

Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. BEiT v2: Masked image modeling
with vector-quantized visual tokenizers. 2022.
Karol J Piczak. Esc: Dataset for environmental sound classiﬁcation. In Proceedings of the 23rd ACM
international conference on Multimedia, pages 1015–1018, 2015.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International Conference on Machine Learning,
pages 8748–8763. PMLR, 2021.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine
Learning, pages 8821–8831. PMLR, 2021.
Mirco Ravanelli and Yoshua Bengio. Learning speaker representations with mutual information.
arXiv preprint arXiv:1812.00271, 2018.
Chandan KA Reddy, Harishchandra Dubey, Kazuhito Koishida, Arun Nair, Vishak Gopal, Ross
Cutler, Sebastian Braun, Hannes Gamper, Robert Aichner, and Sriram Srinivasan. Interspeech
2021 deep noise suppression challenge. arXiv preprint arXiv:2101.01902, 2021.
Aaqib Saeed, David Grangier, and Neil Zeghidour. Contrastive learning of general-purpose audio
representations. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 3875–3879. IEEE, 2021.
Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised
pre-training for speech recognition. arXiv preprint arXiv:1904.05862, 2019.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine
learning research, 15(1):1929–1958, 2014.
Marco Tagliasacchi, Beat Gfeller, Félix de Chaumont Quitry, and Dominik Roblek. Pre-training
audio representations with self-supervision. IEEE Signal Processing Letters, 27:600–604, 2020.
Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.
In International conference on machine learning, pages 6105–6114. PMLR, 2019.
Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in
neural information processing systems, 30, 2017.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems, 30, 2017.
Sergey Verbitskiy, Vladimir Berikov, and Viacheslav Vyshegorodtsev. Eranns: Efﬁcient residual
audio neural networks for audio pattern recognition. Pattern Recognition Letters, 161:38–44, 2022.
Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet:
Scaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022a.
Luyu Wang and Aaron van den Oord. Multi-format contrastive learning of audio representations.
arXiv preprint arXiv:2103.06508, 2021.
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,
Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language:
Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022b.
Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint
arXiv:1804.03209, 2018.
15

Shu wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin,
Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng,
Ko tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman
Mohamed, and Hung yi Lee. SUPERB: Speech Processing Universal PERformance Benchmark.
In Proc. Interspeech 2021, pages 1194–1198, 2021. doi: 10.21437/Interspeech.2021-1775.
Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2clip: Learning
robust audio representations from clip. In ICASSP 2022-2022 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pages 4563–4567. IEEE, 2022.
Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, Christoph
Feichtenhofer, et al. Masked autoencoders that listen. arXiv preprint arXiv:2207.06405, 2022.
Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong
Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan.
arXiv preprint arXiv:2110.04627, 2021.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017.
Yu Zhang, Daniel S Park, Wei Han, James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu,
Yanping Huang, Shibo Wang, et al. Bigssl: Exploring the frontier of large-scale semi-supervised
learning for automatic speech recognition. IEEE Journal of Selected Topics in Signal Processing,
16(6):1519–1532, 2022.
A
Appendix
A.1
Hyperparamter Settings
Table 4 shows the detailed hyperparameters that are used for BEATS acoustic tokenizer training,
audio SSL model pre-training and ﬁne-tuning, which are adapted from the previous works [Xu et al.,
2022, Chen et al., 2022b, Peng et al., 2022].
Hyperparameters
Tokenizer Training
Model Pre-Training
Model Fine-Tuning
SSL Teacher
SL Teacher
AS-2M
AS-2M
AS-20K
ESC
KS1
KS2
ER
Optimizer
AdamW [Loshchilov and Hutter, 2017]
Optimizer Momentum
β1 = 0.9, β2 = 0.98
Weight decay
0.01
Learning Rate Schedule
Linear Decay
Cosine
Steps
400K
50K
80K
Warmup epochs
32K
5K
8K
GPU
8
16
16
4
Batch size (s)
1.4K
5.6K
6.4K
800
300
100
300
Layer-wise learning rate decay
1.0
1.0
0.6
0.3
0.2
0.3
1.0
Peak learning rate
5e-5
5e-4
5e-4
1e-4
3e-5
1e-4
3e-5
Weighted Sampling



 *

Dropout [Srivastava et al., 2014]
0.1
0.0
Layer Dropout
0.0
0.1
Roll Augmentation




SpecAug [Park et al., 2019]
N/A
0.3
0.2
0.3
0.15
Mixup [Zhang et al., 2017]
N/A
0.0
0.8
0.0
0.8
0.0
Multilabel
N/A





Loss Function
CosineSimilarity
CE
BCE
CE
BCE
CE
Dataset Mean for Normalization
15.41663
11.72215
11.43905
11.41045
12.0889
Dataset Std for Normalization
6.55582
10.60431
5.64913
5.67857
4.29147
Table 4: Hyperparameters of BEATS acoustic tokenizer training, audio SSL model pre-training and
ﬁne-tuning. *We balance each class to 50% of the size of the unknown class for each training epoch.
16

