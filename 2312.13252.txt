Zero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model
Saurabh Saxena†, Junhwa Hur‡, Charles Herrmann‡, Deqing Sun‡, David J. Fleet†*
†Google DeepMind
‡Google Research
{srbs,junhwahur,irwinherrmann,deqingsun,davidfleet}@google.com
Abstract
While methods for monocular depth estimation have
made significant strides on standard benchmarks, zero-shot
metric depth estimation remains unsolved. Challenges in-
clude the joint modeling of indoor and outdoor scenes,
which often exhibit significantly different distributions of
RGB and depth, and the depth-scale ambiguity due to un-
known camera intrinsics. Recent work [5] proposed a spe-
cialized multi-head architecture for jointly modeling indoor
and outdoor scenes. In contrast, we advocate a generic,
task-agnostic diffusion model, with several advancements
such as log-scale depth parameterization to enable joint
modeling of indoor and outdoor scenes, conditioning on the
field-of-view (FOV) to handle scale ambiguity and syntheti-
cally augmenting FOV during training to generalize beyond
the limited camera intrinsics in training datasets. Further-
more, by employing a more diverse training mixture than
is common, and an efficient diffusion parameterization, our
method, Diffusion for Metric Depth (DMD) achieves a 25%
reduction in relative error (REL) on zero-shot indoor and
33% reduction on zero-shot outdoor datasets over the cur-
rent SOTA [5] using only a small number of denoising steps.
For an overview see diffusion-vision.github.io/dmd
1. Introduction
Monocular estimation of metric depth in general environ-
ments, while useful for applications such as mobile robotics
and autonomous driving, has proven elusive. The two main
barriers stem from (1) the large differences in RGB and
depth distributions one finds in indoor and outdoor datasets,
and (2) intrinsic scale ambiguity in images when one lacks
knowledge of camera intrinsics. Not surprisingly, most cur-
rent models for monocular depth are either specific to in-
door or outdoor scenes, or, if trained for both, they estimate
scale-invariant depth.
Current models for metric depth are often trained solely
on indoor or outdoor scenes, primarily on a single dataset
captured with fixed camera intrinsics (e.g., with an RGBD
*DF is also affiliated with the University of Toronto and the Vector
Institute.
0.0
0.2
0.4
0.6
0.8
Relative depth error (REL
)
NYU*
SunRGBD
ibims-1
DIODE
Indoor
Hypersim
-6.5%
-26.0%
-36.6%
-12.1%
-24.1%
Indoors
DMD (ours)
ZoeDepth
0.0
0.2
0.4
0.6
0.8
Relative depth error (REL
)
KITTI*
VKITTI 2
DDAD
DIML
Outdoor
DIODE
Outdoor
-7.0%
-12.4%
-21.7%
-70.4%
-26.9%
Oudoors
DMD (ours)
ZoeDepth
Figure 1.
Relative depth error for DMD (ours) compared to
ZoeDepth (SOTA) on eight zero-shot and two in-distribution (∗)
benchmarks. DMD outperforms ZoeDepth by a substantial mar-
gin on all benchmarks.
camera, or, with RGB+LIDAR for outdoor scenes). Such
models avoid challenges caused by different indoor and out-
door depth distributions, but at the cost of generality. They
also overfit to the camera intrinsics of the training dataset
and do not generalize well to out of distribution data.
The predominant approach to jointly modeling indoor
and outdoor data is to estimate scale- and shift-invariant
depth, rather than metric depth (e.g., MiDaS [34]). By nor-
malizing the depth distributions, one brings indoor and out-
door depth distributions closer, and also avoids the prob-
lems of scale ambiguities in the presence of variable cam-
era intrinsics. Recently there has been growing interest in
bridging these different approaches, training joint indoor-
outdoor models that estimate metric depth. To cope with
both indoor and outdoor domains, ZoeDepth [5] adds two
heads to MiDaS [34], one for each domain, to convert from
scale-invariant depth to metric depth.
In this paper we advocate denoising diffusion models for
zero-shot metric depth estimation, with the aid of several
key innovations to obtain state of the art performance. In
particular, field-of-view (FOV) augmentation is used dur-
ing training to improve generalization to different camera
intrinsics, and FOV conditioning in training and inference
helps to resolve intrinsic scale ambiguities, providing a fur-
1
arXiv:2312.13252v1  [cs.CV]  20 Dec 2023

ther boost in performance. Representing depth in the log
domain allocates model capacity in a more balanced way for
indoor and outdoor scenes, improving indoor performance.
Finally, we find that v-parameterization in neural network
denoising greatly increases inference speed. The resulting
model, dubbed DMD (Diffusion for Metric Depth), outper-
forms recently proposed metric depth model, ZoeDepth [5].
In particular, DMD yields much lower relative depth er-
ror than ZoeDepth on all eight out-of-distribution datasets
in [5] when fine-tuned on the same data. Expanding the
training dataset leads to further improvement in perfor-
mance (Fig. 1).
To summarize, we make the following contributions:
• We introduce DMD, a simple yet effective method for
zero-shot metric depth estimation on general scenes.
• We propose synthetically augmenting the FOV for im-
proved generalization, FOV conditioning to resolve
depth-scale ambiguity and representing depth in log-scale
to better utilize the model’s representation capacity.
• DMD establishes a SOTA on zero-shot metric depth,
achieving 25% and 33% lower relative error than
ZoeDepth on indoor and outdoor datasets,
respec-
tively, while being efficient, owing to the use of v-
parameterization for diffusion.
2. Related Work
Monocular depth (fine-tuned and evaluated in-domain).
Given the challenges of learning a joint indoor-outdoor
model, most approaches have restricted models to target a
single dataset (either indoor or outdoor) with fixed intrin-
sics. In this setting, great progress has been made with ad-
vancements in specialized architectures [15, 16] such as the
use of binning [1, 3, 8, 18, 30] or loss functions [15, 28]
that are suited for this task. [2] proposed combining multi-
ple training datasets with variable intrinsics by normalizing
the images to the same camera intrinsic.
Joint indoor-outdoor models.
To train joint indoor-
outdoor models, one can mitigate the difficulty of learn-
ing diverse scene statistics by estimating scale- and shift-
invariant depth instead.
MiDaS [34] trains their model
on diverse indoor-outdoor datasets and demonstrates good
generalization to various unseen datasets. However, they
do not provide metric depth.
DPT [35] leverages this
for pre-training and further fine-tunes separately for metric
depth on NYU and KITTI. ZoeDepth [5] proposes adding
a mixture-of-experts head, supervised by scene type, on top
of a similarly pre-trained model, thereby handling indoor
and outdoor scenes. In contrast, our model, DMD, uses a
relatively generic framework, without domain-specific ar-
chitectural components.
Intrinsics-conditioned monocular depth. Incorporating
camera intrinsics for depth estimation has been briefly ex-
plored in previous work [17, 22]. They argue that intrinsic-
conditioning allows one to train on multiple datasets with
varying intrinsics, but this is only demonstrated with small-
scale experiments. Similar to our method, ZeroDepth [21]
introduces an intrinsic-conditioned metric-scale depth esti-
mator that is trained on large-scale training datasets.
While conditioning on the input field-of-view, we intro-
duce a novel field-of-view augmentation scheme that aug-
ments training data by cropping or uncropping to simulate
diverse FOVs and provide a large scale study on zero-shot
generalization to both indoor and outdoor domains as well
as robustness to diverse camera intrinsics.
Diffusion for vision. Denoising diffusion models [23, 43]
have recently emerged as a powerful class of generative
models. Although initially proposed for natural image gen-
eration [13, 24, 32, 38], they have recently been shown to be
effective for several computer vision tasks such as semantic
segmentation [26], panoptic segmentation [9], optical flow
[40] and monocular depth estimation [14, 26, 40]. Ours is
the first demonstration that diffusion models can also sup-
port state-of-the-art zero-shot metric depth estimation for
general indoor or outdoor scenes.
3. Diffusion for Metric Depth (DMD)
In what follows we describe DMD (Diffusion for Metric
Depth) and the design decisions to solve these issues. In
particular, we cast monocular depth estimation as a gener-
ative RGB-to-depth translation task using denoising diffu-
sion. To this end we introduce several technical innovations
to conventional diffusion models and training procedures to
accommodate zero-shot, metric depth.
3.1. Diffusion models
Diffusion models are probabilistic models that assume a for-
ward process that gradually transforms a target distribution
into a tractable noise distribution. A learned neural denoiser
is trained to reverse this process, iteratively converting a
noise sample to a sample from the target distribution. They
have been shown to be remarkably effective with images
and video, and they have recently begun to see use for dense
vision tasks like segmentation, tracking, optical flow, and
depth estimation. They are attractive as they exhibit strong
performance on regression tasks, capturing posterior uncer-
tainty, without task specific architectures, loss functions and
training procedures.
For DMD we build on the task-agnostic Efficient U-Net
architecture from DDVM [40].
While DDVM used the
ϵ-parameterization for training the neural depth denoiser,
here instead we use the v-parameterization [39]. We find
that the v-parameterization yields remarkably efficient in-
ference, using as few as one or two refinement steps, with-
out requiring progressive distillation [39].
2

Under the v-parameterization, the denoising network is
given a noisy target image (in our case a depth map), zt =
αtx+σtϵ, where x is the noiseless target input (depth map),
ϵ ∼N(0, I), t ∼U(0, 1), σ2
t = 1−α2
t, and αt > 0 is com-
puted with a pre-determined noise schedule, and the denois-
ing network predicts v ≡αtϵ−σtx. From the output of the
denoising network, i.e., vθ(zt, y, t), where y is an optional
conditioning signal (RGB image in this case), one obtains
an estimate of x at step t, i.e., ˆxt = αtzt −σtvθ(zt, y, t),
and the corresponding estimate of the noise, denoted ˆϵt.
Under this parameterization, with a conventional L2 norm,
the training objective is based on the expected ‘truncated
SNR weighting’ loss, i.e., max(∥x −ˆxt∥2
2, ∥ϵ −ˆϵt∥2
2) [39].
Motivated by the superior performance of the L1 loss in
training DDVM [40] compared to the L2, we similarly em-
ploy a L1 loss for DMD as well, yielding the objective
Ex,y,t,ϵ [max(∥x −ˆxt∥1, ∥ϵ −ˆϵt∥1)] .
(1)
3.2. Joint indoor-outdoor modelling
Training a joint indoor-outdoor model can be difficult be-
cause of the large differences in depth distributions one
finds in indoor and outdoor scenes. Much of the available
indoor training data have depths up to 10m, while outdoor
scenes include ground truth depths up to 80m.
Further,
training data is often lacking the variation in camera intrin-
sics needed for robustness to images from different cam-
eras. Rather, many datasets are captured with a fixed cam-
era. To mitigate these issues we propose three innovations,
namely, the use of log depth, field of view augmentation,
and field of view conditioning.
Log depth. Diffusion models usually model data distribu-
tion in [-1, 1]. One might convert metric depth to this range
with linear scaling, i.e.,
dlin = normalize(dr/dmax) ,
(2)
where dr is the raw depth in meters, dmax is often taken to
be 80m, to accommodate the usual outdoor depth range, and
normalize(d) = clip(2∗d−1, −1, 1). This, however, allo-
cates little representation capacity to indoor scenes (where
depths are usually less than 10m).
Instead, we can allocate more representation capacity to
indoor scenes with log-scaled depth (dlog) as the target for
inference, i.e.,
dlog = normalize
 log(dr/dmin)
log(dmax/dmin)

,
(3)
where dmin and dmax denote the minimum and maximum
supported depths (e.g., 0.5m and 80m). Empirically, we
find log scaling to be remarkably beneficial.
Field-of-view augmentation.
Because datasets for depth
estimation often have little or no variation in the field of
view, it is easy for models to over-fit and thus generalize
poorly to images with different camera intrinsics. To en-
courage models to generalize well to different fields of view,
we propose to augment training data by cropping or uncrop-
ping to simulate diverse FOVs. While cropping is straight-
forward, for uncropping it is unclear how best to pad the
enlarged image. Our preliminary experiments used gener-
ative uncropping with Palette [37], however, we found that
padding the RGB image with Gaussian noise (mean-zero,
variance 1) works as well, is simpler and more efficient.
For missing ground truth depth with uncropping aug-
mentation, we adopt the approach in [40], using a combi-
nation of near-neighbor in-filling and step-unrolled denois-
ing during training. It is shown in [40] that this technique
is effective in coping with the inherent distribution shift be-
tween training and testing when ground truth data are noisy
or incomplete.
Field-of-view conditioning. Metric depth estimation from
a single image is ill-posed in the presence of unknown cam-
era intrinsics, as depth scales inversely with the field-of-
view. While one might hope that diversifying camera in-
trinsics through FOV augmentation will help generalization
to different cameras, we and others [48] observe that it is
not sufficient in itself. FOV augmentations do help simu-
late some variation in camera intrinsics, but variations in
other factors, like focal length, are hard to simulate.
As a conditioning signal, to help disambiguate depth
scale, we use tan(θ/2), where θ is the vertical FOV. We
explored conditioning on the horizontal FOV as well, but
that did not improve results substantially.
4. Experiments
4.1. Training data
DDVM [40] showed that using large amounts of diverse
training data is important for generic models with no task-
specific inductive biases. Here we follow the training strat-
egy proposed in [40], initializing the model with unsuper-
vised pre-training on ImageNet [12] and Places365 [52],
with tasks proposed in [37]. This is followed by supervised
pre-training on ScanNet [11], SceneNet-RGBD [31], and
Waymo [45]. Unlike [40], we also include the DIML In-
door [10] dataset for more diversity. No FOV augmentation
or conditioning is used for this pre-training stage.
For the final training stage we train on a mixture of NYU
[42], Taskonomy [51], KITTI [19] and nuScenes [7]. At
this stage we apply FOV augmentation to NYU, KITTI and
nuScenes, but not Taskonomy as it is large and has sub-
stantial FOV diversity and also add the FOV conditioning.
Finally, to enable fair comparisons with the current SOTA
ZoeDepth [5] model, we also train a model solely from
NYU and KITTI, like ZoeDepth.
3

Image
Ground Truth
ZoeDepth [5]
DMD (ours)
DIODE
Indoor
Hypersim
ibims-1
NYU
SunRGBD
Figure 2. Qualitative comparison between our method and ZoeDepth [5] on indoor scenes. Unlike ZoeDepth, our method estimates depths
at more accurate scale over diverse datasets.
SUN RGB-D
iBims-1 Benchmark
DIODE Indoor
HyperSim
Method
δ1 ↑
REL ↓RMSE ↓
δ1 ↑
REL ↓RMSE ↓
δ1 ↑
REL ↓RMSE ↓
δ1 ↑
REL ↓RMSE ↓
BTS [29]
0.740
0.172
0.515
0.538
0.231
0.919
0.210
0.418
1.905
0.225
0.476
6.404
AdaBins [3]
0.771
0.159
0.476
0.555
0.212
0.901
0.174
0.443
1.963
0.221
0.483
6.546
LocalBins [4]
0.777
0.156
0.470
0.558
0.211
0.880
0.229
0.412
1.853
0.234
0.468
6.362
NeWCRFs [50]
0.798
0.151
0.424
0.548
0.206
0.861
0.187
0.404
1.867
0.255
0.442
6.017
ZoeD-M12-NK [5]
0.856
0.123
0.356
0.615
0.186
0.777
0.386
0.331
1.598
0.274
0.419
5.830
DMD-NK
0.914
0.109
0.306
0.801
0.130
0.563
0.402
0.298
1.407
0.356
0.382
5.527
DMD-MIX
0.930
0.091
0.275
0.859
0.118
0.447
0.380
0.291
1.292
0.497
0.318
4.394
Table 1. Zero-shot results on unseen indoor datasets (evaluated at pixels with GT depth less 8m for SUN RGB-D, 10m for iBims-1 and
DIODE Indoor, and 80m for Hypersim).
Best and second-best results are highlighted. Trained on the same data (NYU and KITTI),
DMD-NK outperforms ZoeD-M12-NK. With more data (Taskonomy and nuScenes), DMD-MIX outperforms by a much larger margin.
4

Image
Ground Truth
ZoeDepth [5]
DMD (ours)
DDAD
DIML
Outdoor
DIODE
Outdoor
KITTI
Virtual
KITTI 2
Figure 3. Qualitative comparison between DMD and ZoeDepth [5] on outdoor scenes. Compared with ZoeDepth [5], our method is able
to estimate a more accurate depth scale.
Virtual KITTI 2
DDAD
DIML Outdoor
DIODE Outdoor
Method
δ1 ↑
REL ↓RMSE ↓
δ1 ↑
REL ↓RMSE ↓
δ1 ↑
REL ↓RMSE ↓
δ1 ↑
REL ↓RMSE ↓
BTS [29]
0.831
0.115
5.368
0.805
0.147
7.550
0.016
1.785
5.908
0.171
0.837
10.48
AdaBins [3]
0.826
0.122
5.420
0.766
0.154
8.560
0.013
1.941
6.272
0.161
0.863
10.35
LocalBins [4]
0.810
0.127
5.981
0.777
0.151
8.139
0.016
1.820
6.706
0.170
0.821
10.27
NeWCRFs [50]
0.829
0.117
5.691
0.874
0.119
6.183
0.010
1.918
6.283
0.176
0.854
9.228
ZoeD-M12-NK [5]
0.850
0.105
5.095
0.824
0.138
7.225
0.292
0.641
3.610
0.208
0.757
7.569
DMD-NK
0.872
0.093
4.828
0.842
0.122
6.740
0.544
0.300
2.522
0.162
0.627
9.577
DMD-MIX
0.890
0.092
4.387
0.907
0.108
5.365
0.602
0.190
2.089
0.187
0.553
8.943
Table 2. Zero-shot results on four unseen outdoor datasets.
Best and second-best results are highlighted. Following pre-training,
DMD-NK is trained on NYUv2 and KITTI, while DMD-MIX adds Taxonomy and NuScenes for training. Using the same fine-tuning data,
DMD-NK outperforms ZoeD-M12-NK on all benchmarks except DIODE Outdoor where it outperforms on REL but is behind on other
metrics. DMD-MIX shows further performance improvements over DMD-NK with the expanded training mixture.
Indoors
Outdoors
Experiment
NYU SunRGBD DIODE Indoor ibims-1 Hypersim KITTI DIML Outdoor DIODE Outdoor Virtual KITTI 2 DDAD
REL
Linear scaling
0.082
0.108
0.324
0.146
0.398
0.056
0.467
0.630
0.092
0.122
Log scaling
0.076
0.109
0.298
0.130
0.382
0.055
0.300
0.628
0.093
0.122
RMS Linear scaling
0.340
0.314
1.526
0.612
5.693
2.516
3.126
10.129
4.788
6.288
Log scaling
0.313
0.306
1.407
0.563
5.527
2.527
2.522
9.577
4.828
6.740
Table 3. Ablation showing that log depth improves quantitative performance on indoor datasets which is understandable since log-scaling
increases the share of representation capacity allocated to shallow depths.
5

Image
Ground Truth
Prediction with linear-scaling
Prediction with log-scaling
Figure 4. Linearly scaling depth leads to noisy predictions for images with shallow depth. See Section 3.2 for more details. Predicting
depth in a log-scale fixes this. Note that here we use a max depth of 5 meters for better visualization.
Image
Ground Truth
DMD-NK
DMD-MIX
ibims-1
DDAD
Figure 5. Qualitative comparison between DMD-NK (fine-tuned on NYU and KITTI) and DMD-MIX (fine-tuned on KITTI, NYU,
nuScenes, and Taskonomy). DMD-MIX further improves depth scale estimation as well as fine details on depth boundaries.
Indoors
Outdoors
In-distribution
OOD
Figure 6. Plots showing the effect of perturbing the FOV during
inference. Optimal performance is at or near the true FOV. Perfor-
mance degrades with larger perturbation.
4.2. Design choices
Denoiser Architecture. We adopt the modifications of the
Efficient U-Net [38] proposed in DDVM [40], with one fur-
ther modification to support FOV conditioning. The FOV
embedding, like the timestep embedding, is constructed by
first building a sin-cos positional embedding [47] followed
by linear projection. The sum of these two embeddings is
used to modulate different layers of the denoiser backbone
using FiLM [33] layers. Following [5] we train at a resolu-
tion of 384×512. The predicted depth maps are resized to
the ground-truth resolution for evaluation, following prior
work [5]. Other training hyper-parameters such as the batch
size and optimizer details are like those in [40].
Augmentations. In addition to the FOV augmentation (Sec.
3.2) we use random horizontal flip augmentation, like many
prior works.
Sampler. We use the DDPM [23] sampler with eight de-
noising steps for indoor datasets. For outdoor datasets we
find that two denoising steps suffice. We report results using
a mean of eight samples, following [40]; as shown in Table
6, sample averaging leads to improved performance.
Evaluation. We adopt the evaluation protocol of ZoeDepth
[5]. We report in-distribution performance on the NYU [42]
and KITTI [19] datasets, and generalization performance on
eight unseen datasets [5], namely, SunRGBD [44], iBims-
1 [44], DIODE Indoor [46], Hypersim [36] for indoors,
and Virtual KITTI 2 [6], DDAD [20], DIML Outdoor [10],
6

Method
NYU
KITTI
δ1↑
δ2↑
δ3 ↑
REL ↓
RMS ↓
log10 ↓
δ1↑
δ2↑
δ3↑
REL ↓
Sq-rel ↓
RMS ↓
RMS log ↓
Domain-specific models:
BTS [29]
0.885
0.978
0.994
0.110
0.392
0.047
0.956
0.993
0.998
0.059
0.245
2.756
0.096
DPT [35]
0.904
0.988
0.998
0.110
0.357
0.045
0.959
0.995
0.999
0.062
–
2.573
0.092
AdaBins [3]
0.903
0.984
0.997
0.103
0.364
0.044
0.964
0.995
0.999
0.058
0.190
2.360
0.088
NeWCRFs [50]
0.922
0.992
0.998
0.095
0.334
0.041
0.974
0.997
0.999
0.052
0.155
2.129
0.079
BinsFormer [30]
0.925
0.989
0.997
0.094
0.330
0.040
0.974
0.997
0.999
0.052
0.151
2.098
0.079
PixelFormer [1]
0.929
0.991
0.998
0.090
0.322
0.039
0.976
0.997
0.999
0.051
0.149
2.081
0.077
IEBins [41]
0.936
0.992
0.998
0.087
0.314
0.038
0.978
0.998
0.999
0.050
0.142
2.011
0.075
MIM [49]
0.949
0.994
0.999
0.083
0.287
0.035
0.977
0.998
1.000
0.050
0.139
1.966
0.075
DDVM [40]
0.946
0.987
0.996
0.074
0.315
0.032
0.965
0.994
0.998
0.055
0.292
2.613
0.089
Joint indoor-outdoor models:
ZoeD-M12-NK
0.953
0.995
0.999
0.077
0.277
0.033
0.966
0.993
0.996
0.057
0.204
2.362
0.087
DMD-NK
0.944
0.986
0.996
0.076
0.313
0.033
0.964
0.994
0.999
0.055
0.219
2.527
0.087
DMD-MIX
0.953
0.989
0.996
0.072
0.296
0.031
0.967
0.995
0.999
0.053
0.203
2.411
0.084
Table 4. In-domain results showing better relative error than ZoeDepth for our models on both the NYU and KITTI datasets. Best results
(amongst indoor-outdoor models only) are bolded. For reference, we provide results for models trained separately for the indoor and
outdoor domains showing that our results are competitive despite being a more general model.
Indoors
Outdoors
Metric Experiment
NYU SunRGBD DIODE Indoor ibims-1 Hypersim KITTI DIML Outdoor DIODE Outdoor Virtual KITTI 2 DDAD
REL
No FOV cond
0.081
0.116
0.316
0.18
0.400
0.057
1.257
0.613
0.100
0.121
With FOV cond
0.076
0.109
0.298
0.130
0.382
0.055
0.300
0.628
0.093
0.122
RMS
No FOV cond
0.319
0.325
1.474
0.712
5.196
2.574
5.382
8.582
5.021
6.826
With FOV cond
0.313
0.306
1.407
0.563
5.527
2.527
2.522
9.577
4.828
6.740
Table 5. Depth errors for models trained with and without field-of-view conditioning. Results shows that FOV conditioning provides a
substantial boost in performance. DIML Outdoor benefits the most, which is understandable given its large FOV, for which generalization
is a major challenge for simple FOV augmentation.
DIODE Outdoor [46] for outdoors. We closely follow the
evaluation protocol, including depth ranges and cropping,
used in [5] and report results using the standard error and
accuracy metrics that are used in literature.
4.3. Results
Zero-shot. Tables 1 and 2 report zero-shot performance
on eight OOD datasets. DMD-NK permits fair compari-
son with ZoeDepth, as both are fine-tuned on NYU and
KITTI. With these datasets, DMD outperforms ZoeDepth
across all but DIODE Outdoor, where DMD-NK outper-
forms ZoeDepth on relative error but not RMSE and δ1.
Importantly, but perhaps not surprisingly, further perfor-
mance improvements are obtained by fine-tuning on larger
amounts of data.
Our variant trained on a mixture with
KITTI, NYU, nuScenes, and Taskonomy, dubbed DMD-
MIX, generalizes much better on OOD test data, establish-
ing an even stronger new state of the art.
Figures 2 and 3 visualize qualitative comparison be-
tween DMD and ZoeDepth on indoor and outdoor datasets
respectively. Our method captures more accurate metric-
scale depth with both indoor and outdoor scenes. Fig. 5
illustrates the qualitative differences in depth maps from
DMD-NK and DMD-MIX. By virtue of training on a larger
dataset, DMD-MIX significantly improves the depth scale
and fine-grained depth details near object boundaries.
In-distribution.
Table 4 reports results on the KITTI
and NYU datasets.
On KITTI, DMD-MIX outperforms
ZoeDepth on all metrics except RMSE where it is com-
petitive.
On NYU DMD-MIX substantially outperforms
ZoeDepth on relative error and is competitive on other met-
rics. Interestingly, DMD-MIX outperforms DMD-NK on
in-distribution data.
4.4. Ablations
We next consider several ablations to test different compo-
nents of the model, all with DMD-NK for expedience.
Log vs linearly scaled depth. Table 3 shows that parame-
terizing depth in log scale (Sec. 3.2) improves quantitative
performance. As expected, this is beneficial for datasets of
indoor scenes and also for datasets of outdoor scenes with
shallower depths, like DIML Outdoor and DIODE Outdoor.
Furthermore, as shown in Fig. 4, using linear-scaling leads
to noise artifacts in the depth estimates for indoor scenes
7

Indoors
Outdoors
Metric # samples NYU SunRGBD DIODE Indoor ibims-1 Hypersim KITTI DIML Outdoor DIODE Outdoor Virtual KITTI 2 DDAD
REL
1
0.077
0.110
0.304
0.135
0.383
0.055
0.311
0.634
0.093
0.122
8
0.076
0.109
0.298
0.130
0.382
0.055
0.300
0.628
0.093
0.122
RMS
1
0.321
0.311
1.426
0.579
5.521
2.544
2.572
9.602
4.842
6.755
8
0.313
0.306
1.407
0.563
5.527
2.527
2.522
9.577
4.828
6.740
Table 6. Averaging multiple samples leads to small but consistent improvement on both REL and RMS.
Indoors
Outdoors
Metric Experiment
NYU SunRGBD DIODE Indoor ibims-1 Hypersim KITTI DIML Outdoor DIODE Outdoor Virtual KITTI 2 DDAD
REL
No FOV aug or cond
0.074
0.124
0.337
0.180
0.479
0.055
1.399
0.615
0.095
0.116
W/ FOV aug and cond 0.076
0.109
0.298
0.130
0.382
0.055
0.300
0.628
0.093
0.122
RMS
No FOV aug or cond
0.310
0.348
1.535
0.722
5.247
2.597
5.919
8.529
4.874
6.476
W/ FOV aug and cond 0.313
0.306
1.407
0.563
5.527
2.527
2.522
9.577
4.828
6.740
Table 7. Ablation showing that training without FOV augmentations and conditioning hurts generalization to out-of-domain data due to
overfitting on the training data intrinsics.
REL ↓
RMSE ↓
NYU KITTI
NYU KITTI
ZoeDepth Auto Router 0.102
0.075
0.377
2.584
Ours no fov aug / cond
0.074
0.055
0.310
2.597
Table 8. Comparison against ZoeDepth without scene type su-
pervision. ZoeDepth performance degrades significantly when the
scene type (indoor or outdoor) is not provided. DMD learns well
without such supervision.
which is resolved with log-depth parameterization.
Field-of-view conditioning. Table 5 shows that FOV con-
ditioning achieves the best performance. Fig. 6 perturbs the
conditioning FOV signal during inference, showing that op-
timal performance occurs at or close to the true FOV.
No FOV augmentation or conditioning. ZoeDepth found
that without scene-type supervision for the experts (i.e.,
Auto Router), ZoeDepth’s performance degrades, even for
in-domain data. To compare against ZoeDepth in this set-
ting, we fine-tune a model on NYU and KITTI without FOV
augmentations or conditioning.
Interestingly, DMD per-
forms relatively well in this setting for in-domain data (Ta-
ble 8). Nevertheless, as shown in Table 7, OOD perfor-
mance is better with FOV augmentation and conditioning.
ϵ vs v diffusion parameterization. Inference latency is
a concern with diffusion models for vision. DDVM [40],
for example, uses 128 denoising steps for depth estima-
tion which can be prohibitive. We find that using the v-
parameterization dramatically reduces the number of de-
noising steps required for good performance. As shown in
Table 9, ϵ-parameterization requires 64 denoising steps to
match the performance of a model with v-parameterization
using only 1 denoising step. Intuitively, v-parameterization
NYU
KITTI
Num denoising steps
ϵ
v
ϵ
v
1
2.374 0.077 0.596 0.056
4
1.484 0.075 0.406 0.055
16
0.409 0.074 0.141 0.055
64
0.077 0.074 0.056 0.055
Table 9. Comparison of relative error on NYU and KITTI for mod-
els trained with ϵ- and v- parameterization. Both are fine-tuned on
NYU and KITTI without FOV augmentation or conditioning.
ensures that the model accurately recovers the signal at both
ends of the noise schedule, unlike ϵ-parameterization where
estimating the noise is easy for low SNR inputs.
5. Conclusion
We propose a generic diffusion-based monocular metric
depth generator with no task-specific inductive biases and
no specialized architectures for handling diverse indoor and
outdoor scenes. Our log-scale depth parameterization ade-
quately allocates representation capacity to different depth
ranges. We advocate augmenting the FOV of training data
through simple cropping/uncropping to enable generaliza-
tion to fields-of-view beyond those in the training datasets
and show that simply uncropping with noise padding is ef-
fective for simulating a larger FOV. We find that condition-
ing on the FOV is essential for disambiguating depth-scale.
We further propose a new fine-tuning dataset mixture that
dramatically improves performance. With these innovations
combined, we establish a new state-of-the-art outperform-
ing the prior work of ZoeDepth across all zero-shot and in-
domain datasets by a substantial margin.
8

Acknowledgements
We thank Jon Shlens, Kevin Swersky, Shekoofeh Azizi and
Cristina Vasconcelos for their detailed feedback and help
with figures. We also thank Ting Chen, Daniel Watson,
Forrester Cole and others in Google DeepMind and Google
Research for helpful discussions.
References
[1] Ashutosh Agarwal and Chetan Arora. Attention Attention
Everywhere: Monocular depth prediction with skip atten-
tion. In WACV, 2023. 2, 7
[2] Manuel L´opez Antequera, Pau Gargallo, Markus Hofinger,
Samuel Rota Bul`o, Yubin Kuang, and Peter Kontschieder.
Mapillary planet-scale depth dataset. In ECCV, pages 589–
604, 2020. 2
[3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
AdaBins: Depth estimation using adaptive bins. In CVPR,
pages 4009–4018, 2021. 2, 4, 5, 7
[4] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
LocalBins: Improving depth estimation by learning local dis-
tributions. In ECCV, pages 480–496, 2022. 4, 5
[5] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter
Wonka, and Matthias M¨uller. ZoeDepth: Zero-shot trans-
fer by combining relative and metric depth. arXiv preprint
arXiv:2302.12288, 2023. 1, 2, 3, 4, 5, 6, 7, 11, 12
[6] Yohann Cabon, Naila Murray, and Martin Humenberger. Vir-
tual KITTI 2, 2020. 6
[7] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom.
nuScenes: A multi-
modal dataset for autonomous driving. In CVPR, 2020. 3
[8] Yuanzhouhan Cao, Zifeng Wu, and Chunhua Shen.
Esti-
mating depth from monocular images as classification using
deep fully convolutional residual networks. IEEE T-CSVT,
28(11):3174–3182, 2017. 2
[9] Ting Chen, Lala Li, Saurabh Saxena, Geoffrey Hinton, and
David J. Fleet. A generalist framework for panoptic segmen-
tation of images and videos. In ICCV, 2023. 2
[10] Jaehoon
Cho,
Dongbo
Min,
Youngjung
Kim,
and
Kwanghoon Sohn.
DIML/CVL RGB-D dataset:
2M
RGB-D images of natural indoor and outdoor scenes. arXiv
preprint arXiv:2110.11590, 2021. 3, 6
[11] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. ScanNet:
Richly-annotated 3D reconstructions of indoor scenes.
In
CVPR, 2017. 3
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. ImageNet: A large-scale hierarchical image
database. In CVPR, pages 248–255, 2009. 3
[13] Prafulla Dhariwal and Alex Nichol. Diffusion models beat
GANs on image synthesis. In NeurIPS, 2022. 2
[14] Yiqun Duan, Xianda Guo, and Zheng Zhu. DiffusionDepth:
Diffusion denoising approach for monocular depth estima-
tion. arXiv preprint arXiv:2303.05021, 2023. 2
[15] David Eigen and Rob Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale convo-
lutional architecture. In ICCV, pages 2650–2658, 2015. 2
[16] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In NIPS, 2014. 2
[17] Jose M. Facil, Benjamin Ummenhofer, Huizhong Zhou, Luis
Montesano, Thomas Brox, and Javier Civera. CAM-Convs:
Camera-Aware Multi-Scale Convolutions for Single-View
Depth. In CVPR, 2019. 2
[18] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression net-
work for monocular depth estimation. In CVPR, pages 2002–
2011, 2018. 2
[19] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets Robotics: The KITTI dataset. The
International Journal of Robotics Research, 32(11):1231–
1237, 2013. 3, 6
[20] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raven-
tos, and Adrien Gaidon.
3D packing for self-supervised
monocular depth estimation. In CVPR, 2020. 6
[21] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares, Ambrus,,
and Adrien Gaidon. Towards zero-shot scale-aware monoc-
ular depth estimation. In ICCV, pages 9233–9243, 2023. 2
[22] Lei He, Guanghui Wang, and Zhanyi Hu. Learning depth
from single images with deep neural network embedding fo-
cal length. IEEE Transactions on Image Processing, 27(9):
4676–4689, 2018. 2
[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif-
fusion Probabilistic Models. NeurIPS, 2020. 2, 6
[24] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high fidelity image generation. JMLR, 2022. 2
[25] Yannick Hold-Geoffroy, Kalyan Sunkavalli, Jonathan Eisen-
mann, Matt Fisher, Emiliano Gambaretto, Sunil Hadap, and
Jean-Francois Lalonde. A perceptual measure for deep sin-
gle image camera calibration. In CVPR. IEEE, 2018. 12
[26] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu,
Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. DDP:
Diffusion model for dense visual prediction. In ICCV, 2023.
2
[27] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver
Wang, Kevin Blackburn-Matzen, Matthew Sticha, and
David F. Fouhey. Perspective fields for single image cam-
era calibration. In CVPR, 2023. 12
[28] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks. In 3DV, pages
239–248, 2016. 2
[29] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong
Suh. From big to small: Multi-scale local planar guidance
for monocular depth estimation. arXiv:1907.10326, 2019. 4,
5, 7
[30] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang.
BinsFormer: Revisiting adaptive bins for monocular depth
estimation. arxiv.2204.00987, 2022. 2, 7
9

[31] John McCormac, Ankur Handa, Stefan Leutenegger, and
Andrew J. Davison. SceneNet RGB-D: Can 5M synthetic
images beat generic imagenet pre-training on indoor seg-
mentation? In ICCV, 2017. 3
[32] Alex Nichol and Prafulla Dhariwal. Improved denoising dif-
fusion probabilistic models. In ICML, 2021. 2
[33] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-
moulin, and Aaron Courville. FiLM: Visual Reasoning with
a General Conditioning Layer . In AAAI, 2018. 6
[34] Ren´e Ranftl,
Katrin Lasinger,
David Hafner,
Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE TPAMI, 44(3):1623–1637, 2020. 1, 2
[35] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction.
In ICCV, pages
12179–12188, 2021. 2, 7
[36] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit
Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,
and Joshua M. Susskind. Hypersim: A photorealistic syn-
thetic dataset for holistic indoor scene understanding.
In
ICCV, 2021. 6
[37] Chitwan Saharia, William Chan, Huiwen Chang, Chris A.
Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mo-
hammad Norouzi. Palette: Image-to-Image Diffusion Mod-
els. In SIGGRAPH, 2022. 3, 11
[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J.
Fleet, and Mohammad Norouzi.
Photorealistic Text-to-
Image Diffusion Models with Deep Language Understand-
ing. In NeurIPS, 2022. 2, 6
[39] Tim Salimans and Jonathan Ho. Progressive distillation for
fast sampling of diffusion models. In ICLR, 2022. 2, 3
[40] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek
Kar, Mohammad Norouzi, Deqing Sun, and David J. Fleet.
The surprising effectiveness of diffusion models for optical
flow and monocular depth estimation. In NeurIPS, 2023. 2,
3, 6, 7, 8
[41] Shuwei Shao, Zhongcai Pei, Xingming Wu, Zhong Liu, Wei-
hai Chen, and Zhengguo Li. IEBins: Iterative elastic bins for
monocular depth estimation. In NeurIPS, 2023. 7
[42] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus.
Indoor segmentation and support inference from
RGBD images. In ECCV, pages 746–760, 2012. 3, 6
[43] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics.
In ICML, pages 2256–
2265, 2015. 2
[44] Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao.
Sun RGB-D: A RGB-D scene understanding benchmark
suite. In CVPR, pages 567–576, 2015. 6
[45] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han,
Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger,
Maxim Krivokon, Amy Gao, Aditya Joshi, Sheng Zhao,
Shuyang Cheng, Yu Zhang, Jonathon Shlens, Zhifeng Chen,
and Dragomir Anguelov. Scalability in perception for au-
tonomous driving: Waymo open dataset. In CVPR, 2020.
3
[46] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,
Haochen Wang, Falcon Z. Dai, Andrea F. Daniele, Moham-
madreza Mostajabi, Steven Basart, Matthew R. Walter, and
Gregory Shakhnarovich. DIODE: A Dense Indoor and Out-
door DEpth Dataset. CoRR, abs/1908.00463, 2019. 6, 7
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NIPS, 2017. 6
[48] Chengrui Wei, Meng Yang, Lei He, and Nanning Zheng. FS-
Depth: Focal-and-scale depth estimation from a single image
in unseen indoor scene. arXiv preprint arXiv:2307.14624,
2023. 3
[49] Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han
Hu, and Yue Cao. Revealing the dark secrets of masked im-
age modeling. In CVPR, 2023. 7
[50] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and
Ping Tan. Neural window fully-connected CRFs for monoc-
ular depth estimation. In CVPR, pages 3916–3925, 2022. 4,
5, 7
[51] Amir Zamir, Alexander Sax, William Shen, Leonidas
Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:
Disentangling task transfer learning. In CVPR, 2018. 3
[52] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE transactions on pattern analysis
and machine intelligence, 40(6):1452–1464, 2017. 3
10

Image
Ground Truth
ZoeDepth [5]
DMD (ours)
DIODE
Indoor
Hypersim
ibims-1
NYU
SunRGBD
Figure 7. Qualitative comparison between our method and ZoeDepth [5] on indoor scenes. Compared with ZoeDepth, our method estimates
depths at more accurate scale over diverse datasets.
A. Additional qualitative examples
In Fig. 7 and Fig. 8, we provide additional qualitative com-
parison between DMD and ZoeDepth on indoor and out-
door datasets respectively. Our method consistently cap-
tures more accurate metric-scale depth in various indoor and
outdoor scenes.
B. Handling unknown field-of-view
While RGB camera intrinsics are available for most prac-
tical uses of monocular depth estimators (e.g. cell phones,
robot platforms or self-driving cars), they may sometimes
be unknown (e.g. internet images or generative imagery).
One solution to handle the unknown FOV would be to esti-
mate the camera intrinsics from the RGB image.
To test this, we train a simple model for predicting the
vertical field-of-view (FOV). The model consists of the en-
coder of a pre-trained Palette [37] model followed by a spa-
tial average pooling layer and a linear head that predicts a
scalar. It is trained to regress to tan(θ/2), where θ is the
vertical FOV, using a L1 loss. We train on a mix of the NYU
and KITTI datasets employing the same FOV augmentation
strategy as used for DMD-NK.
Table 10 compares the depth estimation performance of
our models when using the estimated FOV versus the true
FOV. Despite the simplistic design of our FOV estimator,
the depth estimation performance when using the estimated
FOV is competitive to that when using the true FOV, except
for the DIML outdoor dataset, where the large error in FOV
estimation leads to a much worse relative error for depth.
11

Image
Ground Truth
ZoeDepth [5]
DMD (ours)
DDAD
DIML
Outdoor
DIODE
Outdoor
KITTI
Virtual
KITTI 2
Figure 8. Qualitative comparison between DMD and ZoeDepth [5] on outdoor scenes. Compared with ZoeDepth [5], our method is able
to estimate a more accurate depth scale.
Indoors
Outdoors
Experiment
NYU SunRGBD DIODE Indoor ibims-1 Hypersim KITTI DIML Outdoor DIODE Outdoor Virtual KITTI 2 DDAD
DMD-NK true FOV
0.076
0.109
0.298
0.130
0.382
0.055
0.300
0.628
0.093
0.122
DMD-NK est. FOV
0.077
0.124
0.302
0.140
0.407
0.062
1.490
0.639
0.114
0.132
DMD-MIX true FOV
0.072
0.091
0.291
0.118
0.318
0.053
0.190
0.553
0.092
0.108
DMD-MIX est. FOV
0.072
0.136
0.291
0.114
0.361
0.061
1.049
0.560
0.131
0.119
FOV est. error (degrees) 1.520
5.114
0.735
2.339
4.224
0.842
27.998
3.459
4.142
4.384
Table 10. Comparison of relative error (REL) using an estimated (est.) FOV versus the true FOV. Even with a simple FOV estimator the
depth estimation results using an estimated FOV are competitive to those with the true FOV, except for DIML Outdoor where our FOV
estimator struggles to predict correct FOV as shown by the large error in FOV estimation.
This might be due to the significantly larger FOV of DIML
outdoor, compared to other outdoor datasets, which could
be challenging for our simple FOV regressor to generalize
to. We hypothesize that incorporating more sophisticated
camera intrinsic prediction models, such as those proposed
by [25, 27], could lead to further improvements in FOV esti-
mation accuracy, and thereby better metric depth estimates.
However, we defer a thorough investigation of this approach
to future work.
C. Additional training details
We perform the first stage of supervised training for 1.5M
steps with a learning rate of 1 × 10−4 and the second (final)
stage for 50k steps with a learning rate of 3 × 10−5. For the
FOV augmentation in the second stage we randomly uni-
formly sample a scale in [0.8, 1.5] to crop/uncrop the image
and depth maps.
12

